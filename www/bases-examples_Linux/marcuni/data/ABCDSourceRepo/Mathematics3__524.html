<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title></title>
</head>
<body><div class="page"><p/>
</div>
<div class="page"><p/>
<p>Mathematical Physics</p>
<p/>
</div>
<div class="page"><p/>
<p>Sadri Hassani
</p>
<p>Mathematical
Physics
</p>
<p>AModern Introduction to
Its Foundations
</p>
<p>Second Edition</p>
<p/>
</div>
<div class="page"><p/>
<p>Sadri Hassani
Department of Physics
Illinois State University
Normal, Illinois, USA
</p>
<p>ISBN 978-3-319-01194-3 ISBN 978-3-319-01195-0 (eBook)
DOI 10.1007/978-3-319-01195-0
Springer Cham Heidelberg New York Dordrecht London
</p>
<p>Library of Congress Control Number: 2013945405
</p>
<p>&copy; Springer International Publishing Switzerland 1999, 2013
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole
or part of the material is concerned, specifically the rights of translation, reprinting, reuse of
illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way,
and transmission or information storage and retrieval, electronic adaptation, computer software,
or by similar or dissimilar methodology now known or hereafter developed. Exempted from this
legal reservation are brief excerpts in connection with reviews or scholarly analysis or material
supplied specifically for the purpose of being entered and executed on a computer system, for
exclusive use by the purchaser of the work. Duplication of this publication or parts thereof is
permitted only under the provisions of the Copyright Law of the Publisher&rsquo;s location, in its
current version, and permission for use must always be obtained from Springer. Permissions
for use may be obtained through RightsLink at the Copyright Clearance Center. Violations are
liable to prosecution under the respective Copyright Law.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a specific statement, that such names are
exempt from the relevant protective laws and regulations and therefore free for general use.
While the advice and information in this book are believed to be true and accurate at the date of
publication, neither the authors nor the editors nor the publisher can accept any legal responsi-
bility for any errors or omissions that may be made. The publisher makes no warranty, express
or implied, with respect to the material contained herein.
</p>
<p>Printed on acid-free paper
</p>
<p>Springer is part of Springer Science+Business Media (www.springer.com)</p>
<p/>
<div class="annotation"><a href="http://www.springer.com">http://www.springer.com</a></div>
<div class="annotation"><a href="http://www.springer.com/mycopy">http://www.springer.com/mycopy</a></div>
</div>
<div class="page"><p/>
<p>To my wife, Sarah,
and to my children,
Dane Arash and Daisy Bita</p>
<p/>
</div>
<div class="page"><p/>
<p>Preface to Second Edition
</p>
<p>Based on my own experience of teaching from the first edition, and more im-
portantly based on the comments of the adopters and readers, I have made
some significant changes to the new edition of the book: Part I is substan-
tially rewritten, Part VIII has been changed to incorporate Clifford algebras,
Part IX now includes the representation of Clifford algebras, and the new
Part X discusses the important topic of fiber bundles.
</p>
<p>I felt that a short section on algebra did not do justice to such an im-
portant topic. Therefore, I expanded it into a comprehensive chapter dealing
with the basic properties of algebras and their classification. This required a
rewriting of the chapter on operator algebras, including the introduction of a
section on the representation of algebras in general. The chapter on spectral
decomposition underwent a complete overhaul, as a result of which the topic
is now more cohesive and the proofs more rigorous and illuminating. This
entailed separate treatments of the spectral decomposition theorem for real
and complex vector spaces.
</p>
<p>The inner product of relativity is non-Euclidean. Therefore, in the discus-
sion of tensors, I have explicitly expanded on the indefinite inner products
and introduced a brief discussion of the subspaces of a non-Euclidean (the
so-called semi-Riemannian or pseudo-Riemannian) vector space. This inner
product, combined with the notion of algebra, leads naturally to Clifford al-
gebras, the topic of the second chapter of Part VIII. Motivating the subject
by introducing the Dirac equation, the chapter discusses the general prop-
erties of Clifford algebras in some detail and completely classifies the Clif-
ford algebras Cνμ(R), the generalization of the algebra C
</p>
<p>1
3(R), the Clifford
</p>
<p>algebra of the Minkowski space. The representation of Clifford algebras,
including a treatment of spinors, is taken up in Part IX, after a discussion of
the representation of Lie Groups and Lie algebras.
</p>
<p>Fiber bundles have become a significant part of the lore of fundamen-
tal theoretical physics. The natural setting of gauge theories, essential in
describing electroweak and strong interactions, is fiber bundles. Moreover,
differential geometry, indispensable in the treatment of gravity, is most ele-
gantly treated in terms of fiber bundles. Chapter 34 introduces fiber bundles
and their complementary notion of connection, and the curvature form aris-
ing from the latter. Chapter 35 on gauge theories makes contact with physics
and shows how connection is related to potentials and curvature to fields. It
also constructs the most general gauge-invariant Lagrangian, including its
local expression (the expression involving coordinate charts introduced on
the underlying manifold), which is the form used by physicists. In Chap. 36,
</p>
<p>vii</p>
<p/>
</div>
<div class="page"><p/>
<p>viii Preface to Second Edition
</p>
<p>by introducing vector bundles and linear connections, the stage becomes
ready for the introduction of curvature tensor and torsion, two major play-
ers in differential geometry. This approach to differential geometry via fiber
bundles is, in my opinion, the most elegant and intuitive approach, which
avoids the ad hoc introduction of covariant derivative. Continuing with dif-
ferential geometry, Chap. 37 incorporates the notion of inner product and
metric into it, coming up with the metric connection, so essential in the gen-
eral theory of relativity.
</p>
<p>All these changes and additions required certain omissions. I was careful
not to break the continuity and rigor of the book when omitting topics. Since
none of the discussions of numerical analysis was used anywhere else in the
book, these were the first casualties. A few mathematical treatments that
were too dry, technical, and not inspiring were also removed from the new
edition. However, I provided references in which the reader can find these
missing details. The only casualty of this kind of omission was the discus-
sion leading to the spectral decomposition theorem for compact operators in
Chap. 17.
</p>
<p>Aside from the above changes, I have also altered the style of the book
considerably. Now all mathematical statements&mdash;theorems, propositions,
corollaries, definitions, remarks, etc.&mdash;and examples are numbered consec-
utively without regard to their types. This makes finding those statements
or examples considerably easier. I have also placed important mathemat-
ical statements in boxes which are more visible as they have dark back-
grounds. Additionally, I have increased the number of marginal notes, and
added many more entries to the index.
</p>
<p>Many readers and adopters provided invaluable feedback, both in spot-
ting typos and in clarifying vague and even erroneous statements of the
book. I would like to acknowledge the contribution of the following peo-
ple to the correction of errors and the clarification of concepts: Sylvio An-
drade, Salar Baher, Rafael Benguria, Jim Bogan, Jorun Bomert, John Chaf-
fer, Demetris Charalambous, Robert Gooding, Paul Haines, Carl Helrich,
Ray Jensen, Jin-Wook Jung, David Kastor, Fred Keil, Mike Lieber, Art Lind,
Gary Miller, John Morgan, Thomas Schaefer, Hossein Shojaie, Shreenivas
Somayaji, Werner Timmermann, Johan Wild, Bradley Wogsland, and Fang
Wu. As much as I tried to keep a record of individuals who gave me feed-
back on the first edition, fourteen years is a long time, and I may have omit-
ted some names from the list above. To those people, I sincerely apologize.
Needless to say, any remaining errors in this new edition is solely my re-
sponsibility, and as always, I&rsquo;ll greatly appreciate it if the readers continue
pointing them out to me.
</p>
<p>I consulted the following three excellent books to a great extent for the
addition and/or changes in the second edition:
</p>
<p>Greub, W., Linear Algebra, 4th ed., Springer-Verlag, Berlin, 1975.
Greub, W., Multilinear Algebra, 2nd ed., Springer-Verlag, Berlin, 1978.
Kobayashi, S., and K. Nomizu, Foundations of Differential Geometry,
vol. 1, Wiley, New York, 1963.
</p>
<p>Maury Solomon, my editor at Springer, was immeasurably patient and
cooperative on a project that has been long overdue. Aldo Rampioni has</p>
<p/>
</div>
<div class="page"><p/>
<p>Preface to Second Edition ix
</p>
<p>been extremely helpful and cooperative as he took over the editorship of
the project. My sincere thanks go to both of them. Finally, I would like to
thank my wife Sarah for her unwavering forbearance and encouragement
throughout the long-drawn-out writing of the new edition.
</p>
<p>Sadri HassaniNormal, IL, USA
November, 2012</p>
<p/>
</div>
<div class="page"><p/>
<p>Preface to First Edition
</p>
<p>&ldquo;Ich kann es nun einmal nicht lassen, in diesem Drama von Mathematik und
Physik&mdash;die sich im Dunkeln befruchten, aber von Angesicht zu Angesicht so
gerne einander verkennen und verleugnen&mdash;die Rolle des (wie ich gen&uuml;gsam er-
fuhr, oft unerw&uuml;nschten) Boten zu spielen.&rdquo;
</p>
<p>Hermann Weyl
</p>
<p>It is said that mathematics is the language of Nature. If so, then physics
is its poetry. Nature started to whisper into our ears when Egyptians and
Babylonians were compelled to invent and use mathematics in their day-
to-day activities. The faint geometric and arithmetical pidgin of over four
thousand years ago, suitable for rudimentary conversations with nature as
applied to simple landscaping, has turned into a sophisticated language in
which the heart of matter is articulated.
</p>
<p>The interplay between mathematics and physics needs no emphasis.
What may need to be emphasized is that mathematics is not merely a tool
with which the presentation of physics is facilitated, but the only medium
in which physics can survive. Just as language is the means by which hu-
mans can express their thoughts and without which they lose their unique
identity, mathematics is the only language through which physics can ex-
press itself and without which it loses its identity. And just as language is
perfected due to its constant usage, mathematics develops in the most dra-
matic way because of its usage in physics. The quotation by Weyl above,
an approximation to whose translation is &ldquo;In this drama of mathematics and
physics&mdash;which fertilize each other in the dark, but which prefer to deny and
misconstrue each other face to face&mdash;I cannot, however, resist playing the
role of a messenger, albeit, as I have abundantly learned, often an unwel-
come one,&rdquo; is a perfect description of the natural intimacy between what
mathematicians and physicists do, and the unnatural estrangement between
the two camps. Some of the most beautiful mathematics has been motivated
by physics (differential equations by Newtonian mechanics, differential ge-
ometry by general relativity, and operator theory by quantum mechanics),
and some of the most fundamental physics has been expressed in the most
beautiful poetry of mathematics (mechanics in symplectic geometry, and
fundamental forces in Lie group theory).
</p>
<p>I do not want to give the impression that mathematics and physics cannot
develop independently. On the contrary, it is precisely the independence of
each discipline that reinforces not only itself, but the other discipline as
well&mdash;just as the study of the grammar of a language improves its usage and
vice versa. However, the most effective means by which the two camps can
</p>
<p>xi</p>
<p/>
</div>
<div class="page"><p/>
<p>xii Preface to First Edition
</p>
<p>accomplish great success is through an intense dialogue. Fortunately, with
the advent of gauge and string theories of particle physics, such a dialogue
has been reestablished between physics and mathematics after a relatively
long lull.
</p>
<p>Level and Philosophy of Presentation
</p>
<p>This is a book for physics students interested in the mathematics they use.
It is also a book for mathematics students who wish to see some of the ab-
stract ideas with which they are familiar come alive in an applied setting.
The level of presentation is that of an advanced undergraduate or beginning
graduate course (or sequence of courses) traditionally called &ldquo;Mathematical
Methods of Physics&rdquo; or some variation thereof. Unlike most existing math-
ematical physics books intended for the same audience, which are usually
lexicographic collections of facts about the diagonalization of matrices, ten-
sor analysis, Legendre polynomials, contour integration, etc., with little em-
phasis on formal and systematic development of topics, this book attempts
to strike a balance between formalism and application, between the abstract
and the concrete.
</p>
<p>I have tried to include as much of the essential formalism as is neces-
sary to render the book optimally coherent and self-contained. This entails
stating and proving a large number of theorems, propositions, lemmas, and
corollaries. The benefit of such an approach is that the student will recog-
nize clearly both the power and the limitation of a mathematical idea used
in physics. There is a tendency on the part of the novice to universalize the
mathematical methods and ideas encountered in physics courses because the
limitations of these methods and ideas are not clearly pointed out.
</p>
<p>There is a great deal of freedom in the topics and the level of presentation
that instructors can choose from this book. My experience has shown that
Parts I, II, III, Chap. 12, selected sections of Chap. 13, and selected sections
or examples of Chap. 19 (or a large subset of all this) will be a reasonable
course content for advanced undergraduates. If one adds Chaps. 14 and 20,
as well as selected topics from Chaps. 21 and 22, one can design a course
suitable for first-year graduate students. By judicious choice of topics from
Parts VII and VIII, the instructor can bring the content of the course to a
more modern setting. Depending on the sophistication of the students, this
can be done either in the first year or the second year of graduate school.
</p>
<p>Features
</p>
<p>To better understand theorems, propositions, and so forth, students need to
see them in action. There are over 350 worked-out examples and over 850
problems (many with detailed hints) in this book, providing a vast arena in
which students can watch the formalism unfold. The philosophy underly-
ing this abundance can be summarized as &ldquo;An example is worth a thousand
words of explanation.&rdquo; Thus, whenever a statement is intrinsically vague or</p>
<p/>
</div>
<div class="page"><p/>
<p>Preface to First Edition xiii
</p>
<p>hard to grasp, worked-out examples and/or problems with hints are provided
to clarify it. The inclusion of such a large number of examples is the means
by which the balance between formalism and application has been achieved.
However, although applications are essential in understanding mathemati-
cal physics, they are only one side of the coin. The theorems, propositions,
lemmas, and corollaries, being highly condensed versions of knowledge, are
equally important.
</p>
<p>A conspicuous feature of the book, which is not emphasized in other
comparable books, is the attempt to exhibit&mdash;as much as it is useful and
applicable&mdash;interrelationships among various topics covered. Thus, the un-
derlying theme of a vector space (which, in my opinion, is the most primitive
concept at this level of presentation) recurs throughout the book and alerts
the reader to the connection between various seemingly unrelated topics.
</p>
<p>Another useful feature is the presentation of the historical setting in
which men and women of mathematics and physics worked. I have gone
against the trend of the &ldquo;ahistoricism&rdquo; of mathematicians and physicists by
summarizing the life stories of the people behind the ideas. Many a time,
the anecdotes and the historical circumstances in which a mathematical or
physical idea takes form can go a long way toward helping us understand
and appreciate the idea, especially if the interaction among&mdash;and the contri-
butions of&mdash;all those having a share in the creation of the idea is pointed out,
and the historical continuity of the development of the idea is emphasized.
</p>
<p>To facilitate reference to them, all mathematical statements (definitions,
theorems, propositions, lemmas, corollaries, and examples) have been num-
bered consecutively within each section and are preceded by the section
number. For example, 4.2.9 Definition indicates the ninth mathematical
statement (which happens to be a definition) in Sect. 4.2. The end of a proof
is marked by an empty square ✷, and that of an example by a filled square �,
placed at the right margin of each.
</p>
<p>Finally, a comprehensive index, a large number of marginal notes, and
many explanatory underbraced and overbraced comments in equations fa-
cilitate the use and comprehension of the book. In this respect, the book is
also useful as a reference.
</p>
<p>Organization and Topical Coverage
</p>
<p>Aside from Chap. 0, which is a collection of purely mathematical concepts,
the book is divided into eight parts. Part I, consisting of the first four chap-
ters, is devoted to a thorough study of finite-dimensional vector spaces and
linear operators defined on them. As the unifying theme of the book, vector
spaces demand careful analysis, and Part I provides this in the more accessi-
ble setting of finite dimension in a language that is conveniently generalized
to the more relevant infinite dimensions, the subject of the next part.
</p>
<p>Following a brief discussion of the technical difficulties associated with
infinity, Part II is devoted to the two main infinite-dimensional vector spaces
of mathematical physics: the classical orthogonal polynomials, and Fourier
series and transform.</p>
<p/>
</div>
<div class="page"><p/>
<p>xiv Preface to First Edition
</p>
<p>Complex variables appear in Part III. Chapter 9 deals with basic proper-
ties of complex functions, complex series, and their convergence. Chapter 10
discusses the calculus of residues and its application to the evaluation of def-
inite integrals. Chapter 11 deals with more advanced topics such as multi-
valued functions, analytic continuation, and the method of steepest descent.
</p>
<p>Part IV treats mainly ordinary differential equations. Chapter 12 shows
how ordinary differential equations of second order arise in physical prob-
lems, and Chap. 13 consists of a formal discussion of these differential equa-
tions as well as methods of solving them numerically. Chapter 14 brings in
the power of complex analysis to a treatment of the hypergeometric dif-
ferential equation. The last chapter of this part deals with the solution of
differential equations using integral transforms.
</p>
<p>Part V starts with a formal chapter on the theory of operator and their
spectral decomposition in Chap. 16. Chapter 17 focuses on a specific type
of operator, namely the integral operators and their corresponding integral
equations. The formalism and applications of Sturm-Liouville theory appear
in Chaps. 18 and 19, respectively.
</p>
<p>The entire Part VI is devoted to a discussion of Green&rsquo;s functions. Chap-
ter 20 introduces these functions for ordinary differential equations, while
Chaps. 21 and 22 discuss the Green&rsquo;s functions in an m-dimensional Eu-
clidean space. Some of the derivations in these last two chapters are new
and, as far as I know, unavailable anywhere else.
</p>
<p>Parts VII and VIII contain a thorough discussion of Lie groups and their
applications. The concept of group is introduced in Chap. 23. The theory of
group representation, with an eye on its application in quantum mechanics,
is discussed in the next chapter. Chapters 25 and 26 concentrate on tensor
algebra and tensor analysis on manifolds. In Part VIII, the concepts of group
and manifold are brought together in the context of Lie groups. Chapter 27
discusses Lie groups and their algebras as well as their representations, with
special emphasis on their application in physics. Chapter 28 is on differential
geometry including a brief introduction to general relativity. Lie&rsquo;s original
motivation for constructing the groups that bear his name is discussed in
Chap. 29 in the context of a systematic treatment of differential equations
using their symmetry groups. The book ends in a chapter that blends many of
the ideas developed throughout the previous parts in order to treat variational
problems and their symmetries. It also provides a most fitting example of the
claim made at the beginning of this preface and one of the most beautiful
results of mathematical physics: Noether&rsquo;s theorem on the relation between
symmetries and conservation laws.
</p>
<p>Acknowledgments
</p>
<p>It gives me great pleasure to thank all those who contributed to the mak-
ing of this book. George Rutherford was kind enough to volunteer for the
difficult task of condensing hundreds of pages of biography into tens of
extremely informative pages. Without his help this unique and valuable fea-
ture of the book would have been next to impossible to achieve. I thank him
wholeheartedly. Rainer Grobe and Qichang Su helped me with my rusty</p>
<p/>
</div>
<div class="page"><p/>
<p>Preface to First Edition xv
</p>
<p>computational skills. (R.G. also helped me with my rusty German!) Many
colleagues outside my department gave valuable comments and stimulating
words of encouragement on the earlier version of the book. I would like to
record my appreciation to Neil Rasband for reading part of the manuscript
and commenting on it. Special thanks go to Tom von Foerster, senior editor
of physics and mathematics at Springer-Verlag, not only for his patience
and support, but also for the extreme care he took in reading the entire
manuscript and giving me invaluable advice as a result. Needless to say,
the ultimate responsibility for the content of the book rests on me. Last but
not least, I thank my wife, Sarah, my son, Dane, and my daughter, Daisy, for
the time taken away from them while I was writing the book, and for their
support during the long and arduous writing process.
</p>
<p>Many excellent textbooks, too numerous to cite individually here, have
influenced the writing of this book. The following, however, are noteworthy
for both their excellence and the amount of their influence:
</p>
<p>Birkhoff, G., and G.-C. Rota, Ordinary Differential Equations, 3rd ed.,
New York, Wiley, 1978.
Bishop, R., and S. Goldberg, Tensor Analysis on Manifolds, New York,
Dover, 1980.
Dennery, P., and A. Krzywicki, Mathematics for Physicists, New York,
Harper &amp; Row, 1967.
Halmos, P., Finite-Dimensional Vector Spaces, 2nd ed., Princeton, Van
Nostrand, 1958.
Hamermesh, M., Group Theory and its Application to Physical Prob-
lems, Dover, New York, 1989.
Olver, P., Application of Lie Groups to Differential Equations, New
York, Springer-Verlag, 1986.
</p>
<p>Unless otherwise indicated, all biographical sketches have been taken
from the following three sources:
</p>
<p>Gillispie, C., ed., Dictionary of Scientific Biography, Charles Scribner&rsquo;s,
New York, 1970.
Simmons, G., Calculus Gems, New York, McGraw-Hill, 1992.
History of Mathematics archive at www-groups.dcs.st-and.ac.uk:80.
</p>
<p>I would greatly appreciate any comments and suggestions for improve-
ments. Although extreme care was taken to correct all the misprints, the
mere volume of the book makes it very likely that I have missed some (per-
haps many) of them. I shall be most grateful to those readers kind enough to
bring to my attention any remaining mistakes, typographical or otherwise.
Please feel free to contact me.
</p>
<p>Sadri Hassani
Campus Box 4560
Department of Physics
Illinois State University
Normal, IL 61790-4560, USA
e-mail: hassani@entropy.phy.ilstu.edu</p>
<p/>
<div class="annotation"><a href="http://www-groups.dcs.st-and.ac.uk:80">http://www-groups.dcs.st-and.ac.uk:80</a></div>
<div class="annotation"><a href="mailto:hassani@entropy.phy.ilstu.edu">mailto:hassani@entropy.phy.ilstu.edu</a></div>
</div>
<div class="page"><p/>
<p>xvi Preface to First Edition
</p>
<p>It is my pleasure to thank all those readers who pointed out typograph-
ical mistakes and suggested a few clarifying changes. With the exception
of a couple that required substantial revision, I have incorporated all the
corrections and suggestions in this second printing.</p>
<p/>
</div>
<div class="page"><p/>
<p>Note to the Reader
</p>
<p>Mathematics and physics are like the game of chess (or, for that matter, like
any game)&mdash;you will learn only by &ldquo;playing&rdquo; them. No amount of reading
about the game will make you a master. In this book you will find a large
number of examples and problems. Go through as many examples as pos-
sible, and try to reproduce them. Pay particular attention to sentences like
&ldquo;The reader may check . . . &rdquo; or &ldquo;It is straightforward to show . . . &rdquo;. These
are red flags warning you that for a good understanding of the material
at hand, you need to provide the missing steps. The problems often fill in
missing steps as well; and in this respect they are essential for a thorough
understanding of the book. Do not get discouraged if you cannot get to the
solution of a problem at your first attempt. If you start from the beginning
and think about each problem hard enough, you will get to the solution, and
you will see that the subsequent problems will not be as difficult.
</p>
<p>The extensive index makes the specific topics about which you may be
interested to learn easily accessible. Often the marginal notes will help you
easily locate the index entry you are after.
</p>
<p>I have included a large collection of biographical sketches of mathemat-
ical physicists of the past. These are truly inspiring stories, and I encourage
you to read them. They let you see that even under excruciating circum-
stances, the human mind can work miracles. You will discover how these
remarkable individuals overcame the political, social, and economic condi-
tions of their time to let us get a faint glimpse of the truth. They are our true
heroes.
</p>
<p>xvii</p>
<p/>
</div>
<div class="page"><p/>
<p>Contents
</p>
<p>1 Mathematical Preliminaries . . . . . . . . . . . . . . . . . . . 1
1.1 Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
</p>
<p>1.1.1 Equivalence Relations . . . . . . . . . . . . . . . . 3
1.2 Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.3 Metric Spaces . . . . . . . . . . . . . . . . . . . . . . . . 8
1.4 Cardinality . . . . . . . . . . . . . . . . . . . . . . . . . . 10
1.5 Mathematical Induction . . . . . . . . . . . . . . . . . . . 12
1.6 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
</p>
<p>Part I Finite-Dimensional Vector Spaces
</p>
<p>2 Vectors and Linear Maps . . . . . . . . . . . . . . . . . . . . 19
2.1 Vector Spaces . . . . . . . . . . . . . . . . . . . . . . . . 19
</p>
<p>2.1.1 Subspaces . . . . . . . . . . . . . . . . . . . . . . 22
2.1.2 Factor Space . . . . . . . . . . . . . . . . . . . . . 24
2.1.3 Direct Sums . . . . . . . . . . . . . . . . . . . . . 25
2.1.4 Tensor Product of Vector Spaces . . . . . . . . . . 28
</p>
<p>2.2 Inner Product . . . . . . . . . . . . . . . . . . . . . . . . . 29
2.2.1 Orthogonality . . . . . . . . . . . . . . . . . . . . 32
2.2.2 The Gram-Schmidt Process . . . . . . . . . . . . . 33
2.2.3 The Schwarz Inequality . . . . . . . . . . . . . . . 35
2.2.4 Length of a Vector . . . . . . . . . . . . . . . . . . 36
</p>
<p>2.3 Linear Maps . . . . . . . . . . . . . . . . . . . . . . . . . 38
2.3.1 Kernel of a Linear Map . . . . . . . . . . . . . . . 41
2.3.2 Linear Isomorphism . . . . . . . . . . . . . . . . . 43
</p>
<p>2.4 Complex Structures . . . . . . . . . . . . . . . . . . . . . 45
2.5 Linear Functionals . . . . . . . . . . . . . . . . . . . . . . 48
2.6 Multilinear Maps . . . . . . . . . . . . . . . . . . . . . . . 53
</p>
<p>2.6.1 Determinant of a Linear Operator . . . . . . . . . . 55
2.6.2 Classical Adjoint . . . . . . . . . . . . . . . . . . . 56
</p>
<p>2.7 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
</p>
<p>3 Algebras . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
3.1 From Vector Space to Algebra . . . . . . . . . . . . . . . . 63
</p>
<p>3.1.1 General Properties . . . . . . . . . . . . . . . . . . 64
3.1.2 Homomorphisms . . . . . . . . . . . . . . . . . . . 70
</p>
<p>3.2 Ideals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
3.2.1 Factor Algebras . . . . . . . . . . . . . . . . . . . 77
</p>
<p>xix</p>
<p/>
</div>
<div class="page"><p/>
<p>xx Contents
</p>
<p>3.3 Total Matrix Algebra . . . . . . . . . . . . . . . . . . . . . 78
3.4 Derivation of an Algebra . . . . . . . . . . . . . . . . . . . 80
3.5 Decomposition of Algebras . . . . . . . . . . . . . . . . . 83
</p>
<p>3.5.1 The Radical . . . . . . . . . . . . . . . . . . . . . 84
3.5.2 Semi-simple Algebras . . . . . . . . . . . . . . . . 88
3.5.3 Classification of Simple Algebras . . . . . . . . . . 92
</p>
<p>3.6 Polynomial Algebra . . . . . . . . . . . . . . . . . . . . . 95
3.7 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
</p>
<p>4 Operator Algebra . . . . . . . . . . . . . . . . . . . . . . . . . 101
4.1 Algebra of End(V) . . . . . . . . . . . . . . . . . . . . . . 101
</p>
<p>4.1.1 Polynomials of Operators . . . . . . . . . . . . . . 102
4.1.2 Functions of Operators . . . . . . . . . . . . . . . 104
4.1.3 Commutators . . . . . . . . . . . . . . . . . . . . . 106
</p>
<p>4.2 Derivatives of Operators . . . . . . . . . . . . . . . . . . . 107
4.3 Conjugation of Operators . . . . . . . . . . . . . . . . . . 113
</p>
<p>4.3.1 Hermitian Operators . . . . . . . . . . . . . . . . . 114
4.3.2 Unitary Operators . . . . . . . . . . . . . . . . . . 118
</p>
<p>4.4 Idempotents . . . . . . . . . . . . . . . . . . . . . . . . . 119
4.4.1 Projection Operators . . . . . . . . . . . . . . . . . 120
</p>
<p>4.5 Representation of Algebras . . . . . . . . . . . . . . . . . 125
4.6 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
</p>
<p>5 Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
5.1 Representing Vectors and Operators . . . . . . . . . . . . . 137
5.2 Operations on Matrices . . . . . . . . . . . . . . . . . . . 142
5.3 Orthonormal Bases . . . . . . . . . . . . . . . . . . . . . . 146
5.4 Change of Basis . . . . . . . . . . . . . . . . . . . . . . . 148
5.5 Determinant of a Matrix . . . . . . . . . . . . . . . . . . . 151
</p>
<p>5.5.1 Matrix of the Classical Adjoint . . . . . . . . . . . 152
5.5.2 Inverse of a Matrix . . . . . . . . . . . . . . . . . . 155
5.5.3 Dual Determinant Function . . . . . . . . . . . . . 158
</p>
<p>5.6 The Trace . . . . . . . . . . . . . . . . . . . . . . . . . . 160
5.7 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
</p>
<p>6 Spectral Decomposition . . . . . . . . . . . . . . . . . . . . . 169
6.1 Invariant Subspaces . . . . . . . . . . . . . . . . . . . . . 169
6.2 Eigenvalues and Eigenvectors . . . . . . . . . . . . . . . . 172
6.3 Upper-Triangular Representations . . . . . . . . . . . . . . 175
6.4 Complex Spectral Decomposition . . . . . . . . . . . . . . 177
</p>
<p>6.4.1 Simultaneous Diagonalization . . . . . . . . . . . . 185
6.5 Functions of Operators . . . . . . . . . . . . . . . . . . . . 188
6.6 Real Spectral Decomposition . . . . . . . . . . . . . . . . 191
</p>
<p>6.6.1 The Case of Symmetric Operators . . . . . . . . . . 193
6.6.2 The Case of Real Normal Operators . . . . . . . . . 198
</p>
<p>6.7 Polar Decomposition . . . . . . . . . . . . . . . . . . . . . 205
6.8 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . 208</p>
<p/>
</div>
<div class="page"><p/>
<p>Contents xxi
</p>
<p>Part II Infinite-Dimensional Vector Spaces
</p>
<p>7 Hilbert Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . 215
7.1 The Question of Convergence . . . . . . . . . . . . . . . . 215
7.2 The Space of Square-Integrable Functions . . . . . . . . . 221
</p>
<p>7.2.1 Orthogonal Polynomials . . . . . . . . . . . . . . . 222
7.2.2 Orthogonal Polynomials and Least Squares . . . . . 225
</p>
<p>7.3 Continuous Index . . . . . . . . . . . . . . . . . . . . . . 227
7.4 Generalized Functions . . . . . . . . . . . . . . . . . . . . 233
7.5 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
</p>
<p>8 Classical Orthogonal Polynomials . . . . . . . . . . . . . . . . 241
8.1 General Properties . . . . . . . . . . . . . . . . . . . . . . 241
8.2 Classification . . . . . . . . . . . . . . . . . . . . . . . . . 244
8.3 Recurrence Relations . . . . . . . . . . . . . . . . . . . . 245
8.4 Details of Specific Examples . . . . . . . . . . . . . . . . 248
</p>
<p>8.4.1 Hermite Polynomials . . . . . . . . . . . . . . . . 248
8.4.2 Laguerre Polynomials . . . . . . . . . . . . . . . . 249
8.4.3 Legendre Polynomials . . . . . . . . . . . . . . . . 250
8.4.4 Other Classical Orthogonal Polynomials . . . . . . 252
</p>
<p>8.5 Expansion in Terms of Orthogonal Polynomials . . . . . . 254
8.6 Generating Functions . . . . . . . . . . . . . . . . . . . . 257
8.7 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . 258
</p>
<p>9 Fourier Analysis . . . . . . . . . . . . . . . . . . . . . . . . . 265
9.1 Fourier Series . . . . . . . . . . . . . . . . . . . . . . . . 265
</p>
<p>9.1.1 The Gibbs Phenomenon . . . . . . . . . . . . . . . 273
9.1.2 Fourier Series in Higher Dimensions . . . . . . . . 275
</p>
<p>9.2 Fourier Transform . . . . . . . . . . . . . . . . . . . . . . 276
9.2.1 Fourier Transforms and Derivatives . . . . . . . . . 284
9.2.2 The Discrete Fourier Transform . . . . . . . . . . . 286
9.2.3 Fourier Transform of a Distribution . . . . . . . . . 287
</p>
<p>9.3 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . 288
</p>
<p>Part III Complex Analysis
</p>
<p>10 Complex Calculus . . . . . . . . . . . . . . . . . . . . . . . . 295
10.1 Complex Functions . . . . . . . . . . . . . . . . . . . . . 295
10.2 Analytic Functions . . . . . . . . . . . . . . . . . . . . . . 297
10.3 Conformal Maps . . . . . . . . . . . . . . . . . . . . . . . 304
10.4 Integration of Complex Functions . . . . . . . . . . . . . . 309
10.5 Derivatives as Integrals . . . . . . . . . . . . . . . . . . . 315
10.6 Infinite Complex Series . . . . . . . . . . . . . . . . . . . 319
</p>
<p>10.6.1 Properties of Series . . . . . . . . . . . . . . . . . 319
10.6.2 Taylor and Laurent Series . . . . . . . . . . . . . . 321
</p>
<p>10.7 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . 330
</p>
<p>11 Calculus of Residues . . . . . . . . . . . . . . . . . . . . . . . 339
11.1 Residues . . . . . . . . . . . . . . . . . . . . . . . . . . . 339
11.2 Classification of Isolated Singularities . . . . . . . . . . . 342
11.3 Evaluation of Definite Integrals . . . . . . . . . . . . . . . 344</p>
<p/>
</div>
<div class="page"><p/>
<p>xxii Contents
</p>
<p>11.3.1 Integrals of Rational Functions . . . . . . . . . . . 345
11.3.2 Products of Rational and Trigonometric Functions . 348
11.3.3 Functions of Trigonometric Functions . . . . . . . 350
11.3.4 Some Other Integrals . . . . . . . . . . . . . . . . 352
11.3.5 Principal Value of an Integral . . . . . . . . . . . . 354
</p>
<p>11.4 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . 359
</p>
<p>12 Advanced Topics . . . . . . . . . . . . . . . . . . . . . . . . . 363
12.1 Meromorphic Functions . . . . . . . . . . . . . . . . . . . 363
12.2 Multivalued Functions . . . . . . . . . . . . . . . . . . . . 365
</p>
<p>12.2.1 Riemann Surfaces . . . . . . . . . . . . . . . . . . 366
12.3 Analytic Continuation . . . . . . . . . . . . . . . . . . . . 372
</p>
<p>12.3.1 The Schwarz Reflection Principle . . . . . . . . . . 374
12.3.2 Dispersion Relations . . . . . . . . . . . . . . . . . 376
</p>
<p>12.4 The Gamma and Beta Functions . . . . . . . . . . . . . . . 378
12.5 Method of Steepest Descent . . . . . . . . . . . . . . . . . 382
12.6 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . 388
</p>
<p>Part IV Differential Equations
</p>
<p>13 Separation of Variables in Spherical Coordinates . . . . . . . 395
13.1 PDEs of Mathematical Physics . . . . . . . . . . . . . . . 395
13.2 Separation of the Angular Part . . . . . . . . . . . . . . . . 398
13.3 Construction of Eigenvalues of L2 . . . . . . . . . . . . . . 401
13.4 Eigenvectors of L2: Spherical Harmonics . . . . . . . . . . 406
</p>
<p>13.4.1 Expansion of Angular Functions . . . . . . . . . . 411
13.4.2 Addition Theorem for Spherical Harmonics . . . . 412
</p>
<p>13.5 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . 413
</p>
<p>14 Second-Order Linear Differential Equations . . . . . . . . . . 417
14.1 General Properties of ODEs . . . . . . . . . . . . . . . . . 417
14.2 Existence/Uniqueness for First-Order DEs . . . . . . . . . 419
14.3 General Properties of SOLDEs . . . . . . . . . . . . . . . 421
14.4 The Wronskian . . . . . . . . . . . . . . . . . . . . . . . . 425
</p>
<p>14.4.1 A Second Solution to the HSOLDE . . . . . . . . . 426
14.4.2 The General Solution to an ISOLDE . . . . . . . . 428
14.4.3 Separation and Comparison Theorems . . . . . . . 430
</p>
<p>14.5 Adjoint Differential Operators . . . . . . . . . . . . . . . . 433
14.6 Power-Series Solutions of SOLDEs . . . . . . . . . . . . . 436
</p>
<p>14.6.1 Frobenius Method of Undetermined Coefficients . . 439
14.6.2 Quantum Harmonic Oscillator . . . . . . . . . . . . 444
</p>
<p>14.7 SOLDEs with Constant Coefficients . . . . . . . . . . . . 446
14.8 The WKB Method . . . . . . . . . . . . . . . . . . . . . . 450
</p>
<p>14.8.1 Classical Limit of the Schr&ouml;dinger Equation . . . . 452
14.9 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . 453
</p>
<p>15 Complex Analysis of SOLDEs . . . . . . . . . . . . . . . . . . 459
15.1 Analytic Properties of Complex DEs . . . . . . . . . . . . 460
</p>
<p>15.1.1 Complex FOLDEs . . . . . . . . . . . . . . . . . . 460
15.1.2 The Circuit Matrix . . . . . . . . . . . . . . . . . . 462</p>
<p/>
</div>
<div class="page"><p/>
<p>Contents xxiii
</p>
<p>15.2 Complex SOLDEs . . . . . . . . . . . . . . . . . . . . . . 463
15.3 Fuchsian Differential Equations . . . . . . . . . . . . . . . 469
15.4 The Hypergeometric Function . . . . . . . . . . . . . . . . 473
15.5 Confluent Hypergeometric Functions . . . . . . . . . . . . 478
</p>
<p>15.5.1 Hydrogen-Like Atoms . . . . . . . . . . . . . . . . 480
15.5.2 Bessel Functions . . . . . . . . . . . . . . . . . . . 482
</p>
<p>15.6 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . 485
</p>
<p>16 Integral Transforms and Differential Equations . . . . . . . . 493
16.1 Integral Representation of the Hypergeometric Function . . 494
</p>
<p>16.1.1 Integral Representation of the Confluent
Hypergeometric Function . . . . . . . . . . . . . . 497
</p>
<p>16.2 Integral Representation of Bessel Functions . . . . . . . . . 498
16.2.1 Asymptotic Behavior of Bessel Functions . . . . . 502
</p>
<p>16.3 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . 505
</p>
<p>Part V Operators on Hilbert Spaces
</p>
<p>17 Introductory Operator Theory . . . . . . . . . . . . . . . . . 511
17.1 From Abstract to Integral and Differential Operators . . . . 511
17.2 Bounded Operators in Hilbert Spaces . . . . . . . . . . . . 513
</p>
<p>17.2.1 Adjoints of Bounded Operators . . . . . . . . . . . 517
17.3 Spectra of Linear Operators . . . . . . . . . . . . . . . . . 517
17.4 Compact Sets . . . . . . . . . . . . . . . . . . . . . . . . 519
</p>
<p>17.4.1 Compactness and Infinite Sequences . . . . . . . . 521
17.5 Compact Operators . . . . . . . . . . . . . . . . . . . . . 523
</p>
<p>17.5.1 Spectrum of Compact Operators . . . . . . . . . . 527
17.6 Spectral Theorem for Compact Operators . . . . . . . . . . 527
</p>
<p>17.6.1 Compact Hermitian Operator . . . . . . . . . . . . 529
17.6.2 Compact Normal Operator . . . . . . . . . . . . . 531
</p>
<p>17.7 Resolvents . . . . . . . . . . . . . . . . . . . . . . . . . . 534
17.8 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . 539
</p>
<p>18 Integral Equations . . . . . . . . . . . . . . . . . . . . . . . . 543
18.1 Classification . . . . . . . . . . . . . . . . . . . . . . . . . 543
18.2 Fredholm Integral Equations . . . . . . . . . . . . . . . . . 549
</p>
<p>18.2.1 Hermitian Kernel . . . . . . . . . . . . . . . . . . 552
18.2.2 Degenerate Kernels . . . . . . . . . . . . . . . . . 556
</p>
<p>18.3 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . 560
</p>
<p>19 Sturm-Liouville Systems . . . . . . . . . . . . . . . . . . . . . 563
19.1 Compact-Resolvent Unbounded Operators . . . . . . . . . 563
19.2 Sturm-Liouville Systems and SOLDEs . . . . . . . . . . . 569
19.3 Asymptotic Behavior . . . . . . . . . . . . . . . . . . . . 573
</p>
<p>19.3.1 Large Eigenvalues . . . . . . . . . . . . . . . . . . 573
19.3.2 Large Argument . . . . . . . . . . . . . . . . . . . 577
</p>
<p>19.4 Expansions in Terms of Eigenfunctions . . . . . . . . . . . 577
19.5 Separation in Cartesian Coordinates . . . . . . . . . . . . . 579
</p>
<p>19.5.1 Rectangular Conducting Box . . . . . . . . . . . . 579
19.5.2 Heat Conduction in a Rectangular Plate . . . . . . . 581</p>
<p/>
</div>
<div class="page"><p/>
<p>xxiv Contents
</p>
<p>19.5.3 Quantum Particle in a Box . . . . . . . . . . . . . . 582
19.5.4 Wave Guides . . . . . . . . . . . . . . . . . . . . . 584
</p>
<p>19.6 Separation in Cylindrical Coordinates . . . . . . . . . . . . 586
19.6.1 Conducting Cylindrical Can . . . . . . . . . . . . . 586
19.6.2 Cylindrical Wave Guide . . . . . . . . . . . . . . . 588
19.6.3 Current Distribution in a Circular Wire . . . . . . . 589
</p>
<p>19.7 Separation in Spherical Coordinates . . . . . . . . . . . . . 590
19.7.1 Radial Part of Laplace&rsquo;s Equation . . . . . . . . . . 591
19.7.2 Helmholtz Equation in Spherical Coordinates . . . 593
19.7.3 Quantum Particle in a Hard Sphere . . . . . . . . . 593
19.7.4 Plane Wave Expansion . . . . . . . . . . . . . . . . 594
</p>
<p>19.8 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . 595
</p>
<p>Part VI Green&rsquo;s Functions
</p>
<p>20 Green&rsquo;s Functions in One Dimension . . . . . . . . . . . . . . 605
20.1 Calculation of Some Green&rsquo;s Functions . . . . . . . . . . . 606
20.2 Formal Considerations . . . . . . . . . . . . . . . . . . . . 610
</p>
<p>20.2.1 Second-Order Linear DOs . . . . . . . . . . . . . . 614
20.2.2 Self-adjoint SOLDOs . . . . . . . . . . . . . . . . 616
</p>
<p>20.3 Green&rsquo;s Functions for SOLDOs . . . . . . . . . . . . . . . 617
20.3.1 Properties of Green&rsquo;s Functions . . . . . . . . . . . 619
20.3.2 Construction and Uniqueness of Green&rsquo;s Functions . 621
20.3.3 Inhomogeneous BCs . . . . . . . . . . . . . . . . . 626
</p>
<p>20.4 Eigenfunction Expansion . . . . . . . . . . . . . . . . . . 630
20.5 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . 632
</p>
<p>21 Multidimensional Green&rsquo;s Functions: Formalism . . . . . . . 635
21.1 Properties of Partial Differential Equations . . . . . . . . . 635
</p>
<p>21.1.1 Characteristic Hypersurfaces . . . . . . . . . . . . 636
21.1.2 Second-Order PDEs in m Dimensions . . . . . . . 640
</p>
<p>21.2 Multidimensional GFs and Delta Functions . . . . . . . . . 643
21.2.1 Spherical Coordinates in m Dimensions . . . . . . 645
21.2.2 Green&rsquo;s Function for the Laplacian . . . . . . . . . 647
</p>
<p>21.3 Formal Development . . . . . . . . . . . . . . . . . . . . . 648
21.3.1 General Properties . . . . . . . . . . . . . . . . . . 648
21.3.2 Fundamental (Singular) Solutions . . . . . . . . . . 649
</p>
<p>21.4 Integral Equations and GFs . . . . . . . . . . . . . . . . . 652
21.5 Perturbation Theory . . . . . . . . . . . . . . . . . . . . . 655
</p>
<p>21.5.1 The Nondegenerate Case . . . . . . . . . . . . . . 659
21.5.2 The Degenerate Case . . . . . . . . . . . . . . . . 660
</p>
<p>21.6 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . 661
</p>
<p>22 Multidimensional Green&rsquo;s Functions: Applications . . . . . . 665
22.1 Elliptic Equations . . . . . . . . . . . . . . . . . . . . . . 665
</p>
<p>22.1.1 The Dirichlet Boundary Value Problem . . . . . . . 665
22.1.2 The Neumann Boundary Value Problem . . . . . . 671
</p>
<p>22.2 Parabolic Equations . . . . . . . . . . . . . . . . . . . . . 673
22.3 Hyperbolic Equations . . . . . . . . . . . . . . . . . . . . 678
22.4 The Fourier Transform Technique . . . . . . . . . . . . . . 680</p>
<p/>
</div>
<div class="page"><p/>
<p>Contents xxv
</p>
<p>22.4.1 GF for the m-Dimensional Laplacian . . . . . . . . 681
22.4.2 GF for the m-Dimensional Helmholtz Operator . . . 682
22.4.3 GF for the m-Dimensional Diffusion Operator . . . 684
22.4.4 GF for the m-Dimensional Wave Equation . . . . . 685
</p>
<p>22.5 The Eigenfunction Expansion Technique . . . . . . . . . . 688
22.6 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . 693
</p>
<p>Part VII Groups and Their Representations
</p>
<p>23 Group Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . 701
23.1 Groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . 702
23.2 Subgroups . . . . . . . . . . . . . . . . . . . . . . . . . . 705
</p>
<p>23.2.1 Direct Products . . . . . . . . . . . . . . . . . . . 712
23.3 Group Action . . . . . . . . . . . . . . . . . . . . . . . . 713
23.4 The Symmetric Group Sn . . . . . . . . . . . . . . . . . . 715
23.5 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . 720
</p>
<p>24 Representation of Groups . . . . . . . . . . . . . . . . . . . . 725
24.1 Definitions and Examples . . . . . . . . . . . . . . . . . . 725
24.2 Irreducible Representations . . . . . . . . . . . . . . . . . 728
24.3 Orthogonality Properties . . . . . . . . . . . . . . . . . . . 732
24.4 Analysis of Representations . . . . . . . . . . . . . . . . . 737
24.5 Group Algebra . . . . . . . . . . . . . . . . . . . . . . . . 740
</p>
<p>24.5.1 Group Algebra and Representations . . . . . . . . . 740
24.6 Relationship of Characters to Those of a Subgroup . . . . . 743
24.7 Irreducible Basis Functions . . . . . . . . . . . . . . . . . 746
24.8 Tensor Product of Representations . . . . . . . . . . . . . 750
</p>
<p>24.8.1 Clebsch-Gordan Decomposition . . . . . . . . . . . 753
24.8.2 Irreducible Tensor Operators . . . . . . . . . . . . 756
</p>
<p>24.9 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . 758
</p>
<p>25 Representations of the Symmetric Group . . . . . . . . . . . 761
25.1 Analytic Construction . . . . . . . . . . . . . . . . . . . . 761
25.2 Graphical Construction . . . . . . . . . . . . . . . . . . . 764
25.3 Graphical Construction of Characters . . . . . . . . . . . . 767
25.4 Young Operators . . . . . . . . . . . . . . . . . . . . . . . 771
25.5 Products of Representations of Sn . . . . . . . . . . . . . . 774
25.6 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . 776
</p>
<p>Part VIII Tensors and Manifolds
</p>
<p>26 Tensors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 781
26.1 Tensors as Multilinear Maps . . . . . . . . . . . . . . . . . 782
26.2 Symmetries of Tensors . . . . . . . . . . . . . . . . . . . . 789
26.3 Exterior Algebra . . . . . . . . . . . . . . . . . . . . . . . 794
</p>
<p>26.3.1 Orientation . . . . . . . . . . . . . . . . . . . . . . 800
26.4 Symplectic Vector Spaces . . . . . . . . . . . . . . . . . . 801
26.5 Inner Product Revisited . . . . . . . . . . . . . . . . . . . 804
</p>
<p>26.5.1 Subspaces . . . . . . . . . . . . . . . . . . . . . . 809
26.5.2 Orthonormal Basis . . . . . . . . . . . . . . . . . . 812</p>
<p/>
</div>
<div class="page"><p/>
<p>xxvi Contents
</p>
<p>26.5.3 Inner Product on Λp(V,U) . . . . . . . . . . . . . 819
26.6 The Hodge Star Operator . . . . . . . . . . . . . . . . . . 820
26.7 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . 823
</p>
<p>27 Clifford Algebras . . . . . . . . . . . . . . . . . . . . . . . . . 829
27.1 Construction of Clifford Algebras . . . . . . . . . . . . . . 830
</p>
<p>27.1.1 The Dirac Equation . . . . . . . . . . . . . . . . . 832
27.2 General Properties of the Clifford Algebra . . . . . . . . . 834
</p>
<p>27.2.1 Homomorphism with Other Algebras . . . . . . . . 837
27.2.2 The Canonical Element . . . . . . . . . . . . . . . 838
27.2.3 Center and Anticenter . . . . . . . . . . . . . . . . 839
27.2.4 Isomorphisms . . . . . . . . . . . . . . . . . . . . 842
</p>
<p>27.3 General Classification of Clifford Algebras . . . . . . . . . 843
27.4 The Clifford Algebras Cνμ(R) . . . . . . . . . . . . . . . . 846
</p>
<p>27.4.1 Classification of C0n(R) and C
n
0(R) . . . . . . . . . 849
</p>
<p>27.4.2 Classification of Cνμ(R) . . . . . . . . . . . . . . . 851
27.4.3 The Algebra C13(R) . . . . . . . . . . . . . . . . . 852
</p>
<p>27.5 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . 856
</p>
<p>28 Analysis of Tensors . . . . . . . . . . . . . . . . . . . . . . . . 859
28.1 Differentiable Manifolds . . . . . . . . . . . . . . . . . . . 859
28.2 Curves and Tangent Vectors . . . . . . . . . . . . . . . . . 866
28.3 Differential of a Map . . . . . . . . . . . . . . . . . . . . 872
28.4 Tensor Fields on Manifolds . . . . . . . . . . . . . . . . . 876
</p>
<p>28.4.1 Vector Fields . . . . . . . . . . . . . . . . . . . . . 877
28.4.2 Tensor Fields . . . . . . . . . . . . . . . . . . . . . 882
</p>
<p>28.5 Exterior Calculus . . . . . . . . . . . . . . . . . . . . . . 888
28.6 Integration on Manifolds . . . . . . . . . . . . . . . . . . . 897
28.7 Symplectic Geometry . . . . . . . . . . . . . . . . . . . . 901
28.8 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . 909
</p>
<p>Part IX Lie Groups and Their Applications
</p>
<p>29 Lie Groups and Lie Algebras . . . . . . . . . . . . . . . . . . 915
29.1 Lie Groups and Their Algebras . . . . . . . . . . . . . . . 915
</p>
<p>29.1.1 Group Action . . . . . . . . . . . . . . . . . . . . 917
29.1.2 Lie Algebra of a Lie Group . . . . . . . . . . . . . 920
29.1.3 Invariant Forms . . . . . . . . . . . . . . . . . . . 927
29.1.4 Infinitesimal Action . . . . . . . . . . . . . . . . . 928
29.1.5 Integration on Lie Groups . . . . . . . . . . . . . . 935
</p>
<p>29.2 An Outline of Lie Algebra Theory . . . . . . . . . . . . . . 936
29.2.1 The Lie Algebras o(p,n&minus; p) and p(p,n&minus; p) . . . 940
29.2.2 Operations on Lie Algebras . . . . . . . . . . . . . 944
</p>
<p>29.3 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . 948
</p>
<p>30 Representation of Lie Groups and Lie Algebras . . . . . . . . 953
30.1 Representation of Compact Lie Groups . . . . . . . . . . . 953
30.2 Representation of the General Linear Group . . . . . . . . 963
30.3 Representation of Lie Algebras . . . . . . . . . . . . . . . 966
</p>
<p>30.3.1 Representation of Subgroups of GL(V) . . . . . . . 967</p>
<p/>
</div>
<div class="page"><p/>
<p>Contents xxvii
</p>
<p>30.3.2 Casimir Operators . . . . . . . . . . . . . . . . . . 969
30.3.3 Representation of so(3) and so(3,1) . . . . . . . . 972
30.3.4 Representation of the Poincar&eacute; Algebra . . . . . . . 975
</p>
<p>30.4 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . 983
</p>
<p>31 Representation of Clifford Algebras . . . . . . . . . . . . . . 987
31.1 The Clifford Group . . . . . . . . . . . . . . . . . . . . . 987
31.2 Spinors . . . . . . . . . . . . . . . . . . . . . . . . . . . . 995
</p>
<p>31.2.1 Pauli Spin Matrices and Spinors . . . . . . . . . . . 997
31.2.2 Spinors for Cνμ(R) . . . . . . . . . . . . . . . . . . 1001
31.2.3 C13(R) Revisited . . . . . . . . . . . . . . . . . . . 1004
</p>
<p>31.3 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . 1006
</p>
<p>32 Lie Groups and Differential Equations . . . . . . . . . . . . . 1009
32.1 Symmetries of Algebraic Equations . . . . . . . . . . . . . 1009
32.2 Symmetry Groups of Differential Equations . . . . . . . . 1014
</p>
<p>32.2.1 Prolongation of Functions . . . . . . . . . . . . . . 1017
32.2.2 Prolongation of Groups . . . . . . . . . . . . . . . 1021
32.2.3 Prolongation of Vector Fields . . . . . . . . . . . . 1022
</p>
<p>32.3 The Central Theorems . . . . . . . . . . . . . . . . . . . . 1024
32.4 Application to Some Known PDEs . . . . . . . . . . . . . 1029
</p>
<p>32.4.1 The Heat Equation . . . . . . . . . . . . . . . . . . 1030
32.4.2 The Wave Equation . . . . . . . . . . . . . . . . . 1034
</p>
<p>32.5 Application to ODEs . . . . . . . . . . . . . . . . . . . . . 1037
32.5.1 First-Order ODEs . . . . . . . . . . . . . . . . . . 1037
32.5.2 Higher-Order ODEs . . . . . . . . . . . . . . . . . 1039
32.5.3 DEs with Multiparameter Symmetries . . . . . . . 1040
</p>
<p>32.6 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . 1043
</p>
<p>33 Calculus of Variations, Symmetries, and Conservation Laws . 1047
33.1 The Calculus of Variations . . . . . . . . . . . . . . . . . . 1047
</p>
<p>33.1.1 Derivative for Hilbert Spaces . . . . . . . . . . . . 1047
33.1.2 Functional Derivative . . . . . . . . . . . . . . . . 1050
33.1.3 Variational Problems . . . . . . . . . . . . . . . . . 1053
33.1.4 Divergence and Null Lagrangians . . . . . . . . . . 1060
</p>
<p>33.2 Symmetry Groups of Variational Problems . . . . . . . . . 1062
33.3 Conservation Laws and Noether&rsquo;s Theorem . . . . . . . . . 1065
33.4 Application to Classical Field Theory . . . . . . . . . . . . 1069
33.5 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . 1073
</p>
<p>Part X Fiber Bundles
</p>
<p>34 Fiber Bundles and Connections . . . . . . . . . . . . . . . . . 1079
34.1 Principal Fiber Bundles . . . . . . . . . . . . . . . . . . . 1079
</p>
<p>34.1.1 Associated Bundles . . . . . . . . . . . . . . . . . 1084
34.2 Connections in a PFB . . . . . . . . . . . . . . . . . . . . 1086
</p>
<p>34.2.1 Local Expression for a Connection . . . . . . . . . 1087
34.2.2 Parallelism . . . . . . . . . . . . . . . . . . . . . . 1089
</p>
<p>34.3 Curvature Form . . . . . . . . . . . . . . . . . . . . . . . 1091
34.3.1 Flat Connections . . . . . . . . . . . . . . . . . . . 1095</p>
<p/>
</div>
<div class="page"><p/>
<p>xxviii Contents
</p>
<p>34.3.2 Matrix Structure Group . . . . . . . . . . . . . . . 1096
34.4 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . 1097
</p>
<p>35 Gauge Theories . . . . . . . . . . . . . . . . . . . . . . . . . . 1099
35.1 Gauge Potentials and Fields . . . . . . . . . . . . . . . . . 1099
</p>
<p>35.1.1 Particle Fields . . . . . . . . . . . . . . . . . . . . 1101
35.1.2 Gauge Transformation . . . . . . . . . . . . . . . . 1102
</p>
<p>35.2 Gauge-Invariant Lagrangians . . . . . . . . . . . . . . . . 1105
35.3 Construction of Gauge-Invariant Lagrangians . . . . . . . . 1107
35.4 Local Equations . . . . . . . . . . . . . . . . . . . . . . . 1112
35.5 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . 1115
</p>
<p>36 Differential Geometry . . . . . . . . . . . . . . . . . . . . . . 1117
36.1 Connections in a Vector Bundle . . . . . . . . . . . . . . . 1117
36.2 Linear Connections . . . . . . . . . . . . . . . . . . . . . 1120
</p>
<p>36.2.1 Covariant Derivative of Tensor Fields . . . . . . . . 1123
36.2.2 From Forms on P to Tensor Fields on M . . . . . . 1125
36.2.3 Component Expressions . . . . . . . . . . . . . . . 1128
36.2.4 General Basis . . . . . . . . . . . . . . . . . . . . 1132
</p>
<p>36.3 Geodesics . . . . . . . . . . . . . . . . . . . . . . . . . . 1137
36.3.1 Riemann Normal Coordinates . . . . . . . . . . . . 1138
</p>
<p>36.4 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . 1140
</p>
<p>37 Riemannian Geometry . . . . . . . . . . . . . . . . . . . . . . 1143
37.1 The Metric Connection . . . . . . . . . . . . . . . . . . . 1143
</p>
<p>37.1.1 Orthogonal Bases . . . . . . . . . . . . . . . . . . 1148
37.2 Isometries and Killing Vector Fields . . . . . . . . . . . . . 1155
37.3 Geodesic Deviation and Curvature . . . . . . . . . . . . . 1159
</p>
<p>37.3.1 Newtonian Gravity . . . . . . . . . . . . . . . . . . 1161
37.4 General Theory of Relativity . . . . . . . . . . . . . . . . 1163
</p>
<p>37.4.1 Einstein&rsquo;s Equation . . . . . . . . . . . . . . . . . 1163
37.4.2 Static Spherically Symmetric Solutions . . . . . . . 1167
37.4.3 Schwarzschild Geodesics . . . . . . . . . . . . . . 1169
</p>
<p>37.5 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . 1174
</p>
<p>References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1179
</p>
<p>Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1181</p>
<p/>
</div>
<div class="page"><p/>
<p>List of Symbols
</p>
<p>General
&forall; for all (values of)
&exist; there exists (a values of)
iff if and only if
A&equiv; B A is identical to (equivalent to, defined as) B
Set Theory
&isin; set theoretic membership sign: &ldquo;belongs to&rdquo;
�&isin; set theoretic exclusion sign: &ldquo;does not belong to&rdquo;
&sub; subset sign
&sube; subset with the possibility of equality emphasized
&empty; empty set
&cup; union of sets
&cap; intersection of sets
&sim;A complement of the set A
A&times;B set of ordered pairs (a, b) with a &isin;A and b &isin; B
⊲⊳ equivalence relation
❏a❑ equivalence class of a
idX identity map of the set X
N the set of natural (non-negative integer) numbers
Z the set of integers
Q the set of rational numbers
R the set of real numbers
R+ the set of positive real numbers
C the set of complex numbers
H the set of quaternions
f :A&rarr; B map f from set A to set B
x �&rarr; f (x) x is mapped to f (x) via the map f
g ◦ f composition of the two maps f and g
Vector Spaces
V a generic vector space
|a〉 a generic vector labeled a
Cn, Rn complex (real) n-tuples
Mm&times;n m&times; n matrices
C&infin; absolutely convergent complex series
Pc[t] polynomials in t with complex coefficients
Pr [t] polynomials in t with real coefficients
Pcn[t] complex polynomials of degree n or less
</p>
<p>xxix</p>
<p/>
</div>
<div class="page"><p/>
<p>xxx List of Symbols
</p>
<p>C0(a, b) continuous functions on real interval (a, b)
Cn(a, b) n-times differentiable functions on real interval (a, b)
C&infin;(a, b) infinitely differentiable functions on real interval (a, b)
Span{S} the span of a subset S of a vector space
〈a | b〉 inner product of vectors |a〉 and |b〉
‖a‖ norm of the vector |a〉
&oplus; direct sum of vectors or vector spaces&oplus;n
</p>
<p>k=1 direct sum of n vectors or vector spaces
V&lowast; dual of the vector space V
〈a,ααα〉 pairing of the vector |a〉 with its dual ααα
Algebras
A a generic algebra
L(V) the algebra of linear maps of the vector space V; same as
</p>
<p>End(V)
End(V) the algebra of linear maps of the vector space V; same as
</p>
<p>L(V)
</p>
<p>&oplus; direct sum of algebras
&oplus;V direct sum of algebras only as vector spaces
&otimes; tensor product of algebras
M(F) total matrix algebra over the field F
C(V) Clifford algebra of the inner-product space V
&or; Clifford product symbol
Groups
Sn symmetric group; group of permutations of 1,2, . . . , n
GL(V) general linear group of the vector space V
GL(n,C) general linear group of the vector space Cn
</p>
<p>GL(n,R) general linear group of the vector space Rn
</p>
<p>SL(V) special linear group; subgroup of GL(V) with unit
determinant
</p>
<p>O(n) orthogonal group; group of orthogonal n&times; n matrices
SO(n) special orthogonal group; subgroup of O(n) with unit
</p>
<p>determinant
U(n) unitary group; group of unitary n&times; n matrices
SU(n) special unitary group; subgroup of U(n) with unit
</p>
<p>determinant
g Lie algebra of the Lie group G
&sim;= isomorphism of groups, vector spaces, and algebras
Tensors
Trs (V) set of tensors of type (r, s) in vector space V
Sr(V) set of symmetric tensors of type (r,0) in vector space V
Λp(V) set of p-forms in vector space V
Λp(V&lowast;) set of p-vectors in vector space V
&and; wedge (exterior) product symbol
Λp(V,U) set of p-forms in V with values in vector space U
F&infin;(P ) set of infinitely differentiable functions at point P of a
</p>
<p>manifold
TP (M) set of vectors tangent to manifold M at point P &isin;M</p>
<p/>
</div>
<div class="page"><p/>
<p>List of Symbols xxxi
</p>
<p>T (M) tangent bundle of the manifold M
T &lowast;(M) cotangent bundle of the manifold M
X(M) set of vector fields on the manifold M
exp(tX) flow of the vector field X parametrized by t
T rs (M) set of tensor fields of type (r, s) in the manifold M</p>
<p/>
</div>
<div class="page"><p/>
<p>1Mathematical Preliminaries
</p>
<p>This introductory chapter gathers together some of the most basic tools and
notions that are used throughout the book. It also introduces some common
vocabulary and notations used in modern mathematical physics literature.
Readers familiar with such concepts as sets, maps, equivalence relations,
and metric spaces may wish to skip this chapter.
</p>
<p>1.1 Sets
</p>
<p>Modern mathematics starts with the basic (and undefinable) concept of set.
We think of a set as a structureless family, or collection, of objects. We
speak, for example, of the set of students in a college, of men in a city, of
women working for a corporation, of vectors in space, of points in a plane,
</p>
<p>concept of set
</p>
<p>elaborated
or of events in the continuum of space-time. Each member a of a set A is
called an element of that set. This relation is denoted by a &isin;A (read &ldquo;a is an
element of A&rdquo; or &ldquo;a belongs to A&rdquo; ), and its negation by a /&isin;A. Sometimes
a is called a point of the set A to emphasize a geometric connotation.
</p>
<p>A set is usually designated by enumeration of its elements between
braces. For example, {2,4,6,8} represents the set consisting of the first
four even natural numbers; {0,&plusmn;1,&plusmn;2,&plusmn;3, . . . } is the set of all integers;
{1, x, x2, x3, . . . } is the set of all nonnegative powers of x; and {1, i,&minus;1,&minus;i}
is the set of the four complex fourth roots of unity. In many cases, a set is
defined by a (mathematical) statement that holds for all of its elements. Such
a set is generally denoted by {x | P(x)} and read &ldquo;the set of all x&rsquo;s such that
P(x) is true.&rdquo; The foregoing examples of sets can be written alternatively as
follows:
</p>
<p>{n | n is even and 1 &lt; n&lt; 9}
{&plusmn;n | n is a natural number}
{
y | y = xn and n is a natural number
</p>
<p>}
</p>
<p>{
z | z4 = 1 and z is a complex number
</p>
<p>}
.
</p>
<p>S. Hassani, Mathematical Physics, DOI 10.1007/978-3-319-01195-0_1,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>1</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-01195-0_1">http://dx.doi.org/10.1007/978-3-319-01195-0_1</a></div>
</div>
<div class="page"><p/>
<p>2 1 Mathematical Preliminaries
</p>
<p>In a frequently used shorthand notation, the last two sets can be abbrevi-
ated as {xn | n &ge; 0 and n is an integer} and {z &isin; C | z4 = 1}. Similarly, the
unit circle can be denoted by {z &isin; C | |z| = 1}, the closed interval [a, b] as
{x | a &le; x &le; b}, the open interval (a, b) as {x | a &lt; x &lt; b}, and the set of
all nonnegative powers of x as {xn}&infin;n=0 or {xn}n&isin;N, where N is the set of
natural numbers (i.e., nonnegative integers). This last notation will be used
frequently in this book. A set with a single element is called a singleton.singleton
</p>
<p>If a &isin;A whenever a &isin; B , we say that B is a subset of A and write B &sub;A
or A&sup; B . If B &sub; A and A&sub; B , then A= B . If B &sub; A and A �= B , then B
is called a proper subset of A. The set defined by {a | a �= a} is called the(proper) subset
empty set and is denoted by &empty;. Clearly, &empty; contains no elements and is aempty set
subset of any arbitrary set. The collection of all subsets (including &empty;) of a
set A is denoted by 2A. The reason for this notation is that the number of
subsets of a set containing n elements is 2n when n is finite (Problem 1.1).
</p>
<p>If A and B are sets, their union, denoted by A &cup;B , is the set containing
all elements that belong to A or B or both. The intersection of the sets Aunion, intersection,
</p>
<p>complement and B , denoted by A &cap; B , is the set containing all elements belonging to
both A and B . If {Bα}α&isin;I is a collection of sets,1 we denote their union by⋃
</p>
<p>α&isin;I Bα and their intersection by
⋂
</p>
<p>α&isin;I Bα .
The complement of a set A is denoted by &sim;A and defined as
</p>
<p>&sim;A&equiv; {a | a /&isin;A}.
</p>
<p>The complement of B in A (or their difference) is
</p>
<p>A&sim; B &equiv; {a | a &isin;A and a /&isin; B}.
</p>
<p>In any application of set theory there is an underlying universal set
whose subsets are the objects of study. This universal set is usually clearuniversal set
from the context. For example, in the study of the properties of integers, the
set of integers, denoted by Z, is the universal set. The set of reals, R, is the
universal set in real analysis, and the set of complex numbers, C, is the uni-
versal set in complex analysis. To emphasize the presence of a universal set
X, one can write X &sim;A instead of &sim;A.
</p>
<p>From two given sets A and B , it is possible to form the Cartesian prod-Cartesian product
uct of A and B , denoted by A&times;B , which is the set of ordered pairs (a, b),ordered pairs
where a &isin;A and b &isin; B . This is expressed in set-theoretic notation as
</p>
<p>A&times;B =
{
(a, b) | a &isin;A and b &isin; B
</p>
<p>}
.
</p>
<p>We can generalize this to an arbitrary number of sets. If A1,A2, . . . ,An are
sets, then the Cartesian product of these sets is
</p>
<p>A1 &times;A2 &times; &middot; &middot; &middot; &times;An =
{
(a1, a2, . . . , an) | ai &isin;Ai
</p>
<p>}
,
</p>
<p>1Here I is an index set&mdash;or a counting set&mdash;with its typical element denoted by α. In
most cases, I is the set of (nonnegative) integers, but, in principle, it can be any set, for
example, the set of real numbers.</p>
<p/>
</div>
<div class="page"><p/>
<p>1.1 Sets 3
</p>
<p>which is a set of ordered n-tuples. If A1 = A2 = &middot; &middot; &middot; = An = A, then we
write An instead of A&times;A&times; &middot; &middot; &middot; &times;A, and
</p>
<p>An =
{
(a1, a2, . . . , an) | ai &isin;A
</p>
<p>}
.
</p>
<p>The most familiar example of a Cartesian product occurs when A = R.
Then R2 is the set of pairs (x1, x2) with x1, x2 &isin;R. This is simply the points
in the plane. Similarly, R3 is the set of triplets (x1, x2, x3), or the points in
space, and Rn = {(x1, x2, . . . , xn)|xi &isin;R} is the set of real n-tuples.
</p>
<p>1.1.1 Equivalence Relations
</p>
<p>There are many instances in which the elements of a set are naturally
grouped together. For example, all vector potentials that differ by the gra-
dient of a scalar function can be grouped together because they all give the
same magnetic field. Similarly, all quantum state functions (of unit &ldquo;length&rdquo;)
that differ by a multiplicative complex number of unit length can be grouped
together because they all represent the same physical state. The abstraction
of these ideas is summarized in the following definition.
</p>
<p>Definition 1.1.1 Let A be a set. A relation on A is a comparison test be- relation and equivalence
relationtween members of ordered pairs of elements of A. If the pair (a, b) &isin;A&times;A
</p>
<p>passes this test, we write a ⊲ b and read &ldquo;a is related to b&rdquo;. An equivalence
relation on A is a relation that has the following properties:
</p>
<p>a ⊲ a &forall;a &isin;A, (reflexivity)
a ⊲ b&rArr; b ⊲ a a, b &isin;A, (symmetry)
a ⊲ b, and b ⊲ c&rArr; a ⊲ c a, b, c &isin;A, (transivity).
</p>
<p>When a ⊲ b, we say that &ldquo;a is equivalent to b&rdquo;. The set ❏a❑= {b &isin;A | b ⊲ a}
of all elements that are equivalent to a is called the equivalence class of a. equivalence class
</p>
<p>The reader may verify the following property of equivalence relations.
</p>
<p>Proposition 1.1.2 If ⊲ is an equivalence relation on A and a, b &isin; A, then
either ❏a❑&cap; ❏b❑= &empty; or ❏a❑= ❏b❑.
</p>
<p>Therefore, a&prime; &isin; ❏a❑ implies that ❏a&prime;❑= ❏a❑. In other words, any element
of an equivalence class can be chosen to be a representative of that class. representative of an
</p>
<p>equivalence classBecause of the symmetry of equivalence relations, sometimes we denote
them by ⊲⊳.
</p>
<p>Example 1.1.3 Let A be the set of human beings. Let a ⊲ b be interpreted
as &ldquo;a is older than b.&rdquo; Then clearly, ⊲ is a relation but not an equivalence
relation. On the other hand, if we interpret a ⊲ b as &ldquo;a and b live in the
same city,&rdquo; then ⊲ is an equivalence relation, as the reader may check. The
equivalence class of a is the population of that city.</p>
<p/>
</div>
<div class="page"><p/>
<p>4 1 Mathematical Preliminaries
</p>
<p>Let V be the set of vector potentials. Write A ⊲ A&prime; if A &minus; A&prime; = &nabla;f for
some function f . The reader may verify that ⊲ is an equivalence relation,
and that ❏A❑ is the set of all vector potentials giving rise to the same mag-
netic field.
</p>
<p>Let the underlying set be Z&times; (Z&sim; {0}). Say &ldquo;(a, b) is related to (c, d)&rdquo; if
ad = bc. Then this relation is an equivalence relation. Furthermore, ❏(a, b)❑
can be identified as the ratio a/b.
</p>
<p>Definition 1.1.4 Let A be a set and {Bα} a collection of subsets of A. We
say that {Bα} is a partition of A, or {Bα} partitions A, if the Bα&rsquo;s arepartition of a set
disjoint, i.e., have no element in common, and
</p>
<p>⋃
α Bα =A.
</p>
<p>Now consider the collection {❏a❑ | a &isin;A} of all equivalence classes of A.
These classes are disjoint, and evidently their union covers all of A. There-
fore, the collection of equivalence classes of A is a partition of A. This
collection is denoted by A/ ⊲⊳ and is called the quotient set or factor set ofquotient set
A under the equivalence relation ⊲⊳.
</p>
<p>Example 1.1.5 Let the underlying set be R3. Define an equivalence relation
on R3 by saying that P1 &isin; R3 and P2 &isin; R3 are equivalent if they lie on the
same line passing through the origin. Then R3/ ⊲⊳ is the set of all lines in
space passing through the origin. If we choose the unit vector with positive
third coordinate along a given line as the representative of that line, then
R3/ ⊲⊳, called the projective space associated with R3, is almost (but notprojective space
quite) the same as the upper unit hemisphere. The difference is that any two
points on the edge of the hemisphere which lie on the same diameter ought
to be identified as the same to turn it into the projective space.
</p>
<p>On the set Z of integers, define a relation by writing m ⊲ n for m,n &isin; Z
if m&minus; n is divisible by k, where k is a fixed integer. Then ⊲ is not only a
relation, but an equivalence relation. In this case, we have
</p>
<p>Z/⊲ =
{
❏0❑, ❏1❑, . . . , ❏k &minus; 1❑
</p>
<p>}
,
</p>
<p>as the reader is urged to verify.
For the equivalence relation defined on Z&times; (Z&sim; {0}) of Example 1.1.3,
</p>
<p>the set (Z &times; (Z &sim; {0}))/ ⊲⊳ can be identified with Q, the set of rational
numbers.
</p>
<p>1.2 Maps
</p>
<p>To communicate between sets, one introduces the concept of a map. A map
</p>
<p>f from a set X to a set Y , denoted by f : X &rarr; Y or X f&minus;&rarr; Y , is a corre-map, domain, codomain,
image spondence between elements of X and those of Y in which all the elements
</p>
<p>of X participate, and each element of X corresponds to only one element of
Y (see Fig. 1.1). If y &isin; Y is the element that corresponds to x &isin; X via the
map f , we write
</p>
<p>y = f (x) or x �&rarr; f (x) or x f�&minus;&rarr; y</p>
<p/>
</div>
<div class="page"><p/>
<p>1.2 Maps 5
</p>
<p>Fig. 1.1 The map f maps all of the set X onto a subset of Y . The shaded area in Y is
f (X), the range of f
</p>
<p>and call f (x) the image of x under f . Thus, by the definition of map, x &isin;
X can have only one image. The set X is called the domain, and Y the
codomain or the target space. Two maps f : X &rarr; Y and g : X &rarr; Y are
said to be equal if f (x)= g(x) for all x &isin;X. function
</p>
<p>Definition 1.2.1 A map whose codomain is the set of real numbers R or the
set of complex numbers C is commonly called a function.
</p>
<p>A special map that applies to all sets A is idA :A&rarr;A, called the identity
map of A, and defined by identity map
</p>
<p>idA(a)= a &forall;a &isin;A.
</p>
<p>The graph Γf of a map f :A&rarr; B is a subset of A&times;B defined by graph of a map
</p>
<p>Γf =
{(
a,f (a)
</p>
<p>)
| a &isin;A
</p>
<p>}
&sub;A&times;B.
</p>
<p>This general definition reduces to the ordinary graphs encountered in alge-
bra and calculus where A= B =R and A&times;B is the xy-plane.
</p>
<p>If A is a subset of X, we call f (A) = {f (x) | x &isin; A} the image
of A. Similarly, if B &sub; f (X), we call f&minus;1(B) = {x &isin; X | f (x) &isin; B} the
inverse image, or preimage, of B . In words, f&minus;1(B) consists of all ele- preimage
ments in X whose images are in B &sub; Y . If B consists of a single element b,
then f&minus;1(b) = {x &isin; X | f (x) = b} consists of all elements of X that are
mapped to b. Note that it is possible for many points of X to have the same
image in Y . The subset f (X) of the codomain of a map f is called the range
of f (see Fig. 1.1).
</p>
<p>If f : X &rarr; Y and g : Y &rarr; W , then the mapping h : X &rarr; W given by
h(x) = g(f (x)) is called the composition of f and g, and is denoted by composition of two
</p>
<p>mapsh= g ◦ f (see Fig. 1.2).2 It is easy to verify that
</p>
<p>f ◦ idX = f = idY ◦f.
</p>
<p>If f (x1) = f (x2) implies that x1 = x2, we call f injective, or one-to- injection, surjection, and
bijection, or 1-1
</p>
<p>correspondence
</p>
<p>one (denoted 1-1). For an injective map only one element of X corresponds
to an element of Y . If f (X) = Y , the mapping is said to be surjective, or
</p>
<p>2Note the importance of the order in which the composition is written. The reverse order
may not even exist.</p>
<p/>
</div>
<div class="page"><p/>
<p>6 1 Mathematical Preliminaries
</p>
<p>Fig. 1.2 The composition of two maps is another map
</p>
<p>onto. A map that is both injective and surjective is said to be bijective, or
to be a one-to-one correspondence. Two sets that are in one-to-one corre-
spondence, have, by definition, the same number of elements. If f :X&rarr; Y
is a bijection from X onto Y , then for each y &isin; Y there is one and only one
element x in X for which f (x)= y. Thus, there is a mapping f&minus;1 : Y &rarr;X
given by f&minus;1(y) = x, where x is the unique element such that f (x) = y.
This mapping is called the inverse of f . The inverse of f is also identifiedinverse of a map
as the map that satisfies f ◦ f&minus;1 = idY and f&minus;1 ◦ f = idX . For example,
one can easily verify that ln&minus;1 = exp and exp&minus;1 = ln, because ln(ex) = x
and elnx = x.
</p>
<p>Given a map f :X&rarr; Y , we can define a relation ⊲⊳ on X by saying x1 ⊲⊳
x2 if f (x1)= f (x2). The reader may check that this is in fact an equivalence
relation. The equivalence classes are subsets of X all of whose elements
map to the same point in Y . In fact, ❏x❑ = f&minus;1(f (x)). Corresponding to
f , there is a map f̃ :X/⊲⊳&rarr; Y , called quotient map or factor map, given
by f̃ (❏x❑)= f (x). This map is injective because if f̃ (❏x1❑)= f̃ (❏x2❑), thenquotient or factor map
f (x1)= f (x2), so x1 and x2 belong to the same equivalence class; therefore,
❏x1❑= ❏x2❑. It follows that
</p>
<p>Proposition 1.2.2 The map f̃ :X/⊲⊳&rarr; f (X) is bijective.
</p>
<p>If f and g are both bijections with inverses f&minus;1 and g&minus;1, respectively,
then g ◦ f also has an inverse, and verifying that (g ◦ f )&minus;1 = f&minus;1 ◦ g&minus;1 is
straightforward.
</p>
<p>Example 1.2.3 As an example of the preimage of a set, consider the sine
and cosine functions: sin :R&rarr;R and cos :R&rarr;R. Then it should be clear
that
</p>
<p>sin&minus;1 0 = {nπ}&infin;n=&minus;&infin;, cos&minus;1 0 =
{
π
</p>
<p>2
+ nπ
</p>
<p>}&infin;
</p>
<p>n=&minus;&infin;
.
</p>
<p>Similarly, sin&minus;1[0, 12 ], the preimage of the closed interval [0, 12 ] &sub; R, con-
sists of all the intervals on the x-axis marked by heavy line segments in
Fig. 1.3, i.e., all the points whose sine lies between 0 and 12 .
</p>
<p>Example 1.2.4 Let X be any set on which an equivalence relation ⊲⊳ is
defined. Then there is a natural map π , called projection π : X &rarr; X/ ⊲⊳projection</p>
<p/>
</div>
<div class="page"><p/>
<p>1.2 Maps 7
</p>
<p>Fig. 1.3 The union of all the intervals on the x-axis marked by heavy line segments is
sin&minus;1[0, 12 ]
</p>
<p>given by π(x)= ❏x❑. This map is obviously surjective, but not injective, as
π(y)= π(x) if y ⊲⊳ x. It becomes injective only if the equivalence relation
becomes the identity map: ⊲⊳ = idX . Then the map becomes bijective, and
we write X &sim;=X/idX .
</p>
<p>Example 1.2.5 As further examples of maps, we consider functions f :
R &rarr; R studied in calculus. The two functions f : R &rarr; R and g : R &rarr;
(&minus;1,+1) given, respectively, by f (x)= x3 and g(x)= tanhx are bijective.
The latter function, by the way, shows that there are as many points in the
whole real line as there are in the interval (&minus;1,+1). If we denote the set injectivity and
</p>
<p>surjectivity depend on
</p>
<p>the domain and
</p>
<p>codomain
</p>
<p>of positive real numbers by R+, then the function f : R &rarr; R+ given by
f (x) = x2 is surjective but not injective (both x and &minus;x map to x2). The
function g : R+ &rarr; R given by the same rule, g(x) = x2, is injective but
not surjective. On the other hand, h : R+ &rarr; R+ again given by h(x) = x2
is bijective, but u : R &rarr; R given by the same rule is neither injective nor
surjective.
</p>
<p>Let Mn&times;n denote the set of n&times; n real matrices. Define a function det :
Mn&times;n &rarr;R by det(A)= det A. This function is clearly surjective (why?) but
not injective. The set of all matrices whose determinant is 1 is det&minus;1(1).
Such matrices occur frequently in physical applications.
</p>
<p>Another example of interest is f :C&rarr;R given by f (z)= |z|. This func-
tion is also neither injective nor surjective. Here f&minus;1(1) is the unit circle, unit circle
the circle of radius 1 in the complex plane. It is clear that f (C)= {0} &cup;R+.
Furthermore, f induces an equivalence relation on C: z1 ⊲⊳ z2 if z1 and z2
belong to the same circle. Then C/ ⊲⊳ is the set of circles centered at the ori-
gin of the complex plane and f̃ :C/⊲⊳&rarr; {0} &cup;R+ is bijective, associating
each circle to its radius.
</p>
<p>The domain of a map can be a Cartesian product of a set, as in f : X &times;
X&rarr; Y . Two specific cases are worthy of mention. The first is when Y =R.
An example of this case is the dot product on vectors. Thus, if X is the set
of vectors in space, we can define f (a,b)= a &middot; b. The second case is when
Y =X. Then f is called a binary operation on X, whereby an element in binary operation
X is associated with two elements in X. For instance, let X = Z, the set of
all integers; then the function f : Z&times; Z&rarr; Z defined by f (m,n) = mn is</p>
<p/>
</div>
<div class="page"><p/>
<p>8 1 Mathematical Preliminaries
</p>
<p>the binary operation of multiplication of integers. Similarly, g :R&times;R&rarr;R
given by g(x, y)= x+y is the binary operation of addition of real numbers.
</p>
<p>1.3 Metric Spaces
</p>
<p>Although sets are at the root of modern mathematics, by themselves they
are only of formal and abstract interest. To make sets useful, it is necessary
to introduce some structures on them. There are two general procedures for
the implementation of such structures. These are the abstractions of the two
major branches of mathematics&mdash;algebra and analysis.
</p>
<p>We can turn a set into an algebraic structure by introducing a binary op-
eration on it. For example, a vector space consists, among other things, of
the binary operation of vector addition. A group is, among other things, a
set together with the binary operation of &ldquo;multiplication&rdquo;. There are many
other examples of algebraic systems, and they constitute the rich subject of
algebra.
</p>
<p>When analysis, the other branch of mathematics, is abstracted using the
concept of sets, it leads to topology, in which the concept of continuity plays
a central role. This is also a rich subject with far-reaching implications and
applications. We shall not go into any details of these two areas of math-
ematics. Although some algebraic systems will be discussed and the ideas
of limit and continuity will be used in the sequel, this will be done in an
intuitive fashion, by introducing and employing the concepts when they are
needed. On the other hand, some general concepts will be introduced when
they require minimum prerequisites. One of these is a metric space:
</p>
<p>Definition 1.3.1 A metric space is a set X together with a real-valued func-
tion d :X&times;X&rarr;R such thatmetric space defined
(a) d(x, y)&ge; 0 &forall;x, y, and d(x, y)= 0 iff x = y.
(b) d(x, y)= d(y, x) (symmetry).
(c) d(x, y)&le; d(x, z)+ d(z, y) (the triangle inequality).
</p>
<p>It is worthwhile to point out that X is a completely arbitrary set and
needs no other structure. In this respect, Definition 1.3.1 is very broad and
encompasses many different situations, as the following examples will show.
Before examining the examples, note that the function d defined above is the
abstraction of the notion of distance: (a) says that the distance between any
two points is always nonnegative and is zero only if the two points coincide;
(b) says that the distance between two points does not change if the two
points are interchanged; (c) states the known fact that the sum of the lengths
of two sides of a triangle is always greater than or equal to the length of the
third side.
</p>
<p>The fact that the distance between two points of a set is positive and real
is a property of a Euclidean metric space. In relativity, on the other hand,Euclidean and
</p>
<p>Minkowskian metric
</p>
<p>spaces
</p>
<p>one has to deal with the possibility of a Minkowskian metric space for which
distance (squared) is negative.</p>
<p/>
</div>
<div class="page"><p/>
<p>1.3 Metric Spaces 9
</p>
<p>Example 1.3.2 Here are some examples of metric spaces:
</p>
<p>1. Let X =Q, the set of rational numbers, and define d(x, y)= |x &minus; y|.
2. Let X =R, and again define d(x, y)= |x &minus; y|.
3. Let X consist of the points on the surface of a sphere. We can define
</p>
<p>two distance functions on X. Let d1(P,Q) be the length of the chord
joining P and Q on the sphere. We can also define another metric,
d2(P,Q), as the length of the arc of the great circle passing through
points P and Q on the surface of the sphere. It is not hard to convince
oneself that d1 and d2 satisfy all the properties of a metric function.
Note that for d2, if two of the three points are the poles of the sphere,
then the triangle inequality becomes an equality.
</p>
<p>4. Let C0[a, b] denote the set of continuous real-valued functions on the
closed interval [a, b]. We can define d(f,g)=
</p>
<p>&int; b
a
|f (x)&minus; g(x)|dx for
</p>
<p>f,g &isin; C0(a, b).
5. Let CB(a, b) denote the set of bounded continuous real-valued func-
</p>
<p>tions on the closed interval [a, b]. We then define
</p>
<p>d(f,g)= max
x&isin;[a,b]
</p>
<p>{∣∣f (x)&minus; g(x)
∣∣} for f,g &isin; CB(a, b).
</p>
<p>This notation says: Take the absolute value of the difference in f and
g at all x in the interval [a, b] and then pick the maximum of all these
values.
</p>
<p>The metric function creates a natural setting in which to test the &ldquo;close-
ness&rdquo; of points in a metric space. One occasion on which the idea of close-
ness becomes essential is in the study of a sequence. A sequence is a map- sequence defined
ping s :N&rarr;X from the set of natural numbers N into the metric space X.
Such a mapping associates with a positive integer n a point s(n) of the met-
ric space X. It is customary to write sn (or xn to match the symbol X) instead
of s(n) and to enumerate the values of the function by writing {xn}&infin;n=1.
</p>
<p>Knowledge of the behavior of a sequence for large values of n is of funda-
mental importance. In particular, it is important to know whether a sequence
approaches a finite value as n increases. convergence defined
</p>
<p>Definition 1.3.3 Suppose that for some x and for any positive real num-
ber ǫ, there exists a natural number N such that d(xn, x) &lt; ǫ whenever
n &gt; N . Then we say that the sequence {xn}&infin;n=1 converges to x and write
limn&rarr;&infin; d(xn, x)= 0 or d(xn, x)&rarr; 0 or simply xn &rarr; x.
</p>
<p>It may not be possible to test directly for the convergence of a given
sequence because this requires a knowledge of the limit point x. However,
it is possible to do the next best thing&mdash;to see whether the points of the
sequence get closer and closer as n gets larger and larger.
</p>
<p>Definition 1.3.4 A Cauchy sequence is a sequence for which Cauchy sequence
</p>
<p>lim
m,n&rarr;&infin;
</p>
<p>d(xm, xn)= 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>10 1 Mathematical Preliminaries
</p>
<p>Fig. 1.4 The distance between the elements of a Cauchy sequence gets smaller and
smaller
</p>
<p>Figure 1.4 shows a Cauchy sequence.
We can test directly whether or not a sequence is Cauchy. However, the
</p>
<p>fact that a sequence is Cauchy does not guarantee that it converges. For
example, let the metric space be the set of rational numbers Q with the
metric function d(x, y)= |x&minus;y|, and consider the sequence {xn}&infin;n=1 where
xn =
</p>
<p>&sum;n
k=1(&minus;1)k+1/k. It is clear that xn is a rational number for any n.
</p>
<p>Problem 1.7 shows how to prove that |xm &minus; xn| &rarr; 0. Thus, the sequence is
Cauchy. However, it is probably known to the reader that limn&rarr;&infin; xn = ln 2,
which is not a rational number.
</p>
<p>Definition 1.3.5 A metric space in which every Cauchy sequence con-
verges is called a complete metric space.complete metric space
</p>
<p>Complete metric spaces play a crucial role in modern analysis. The pre-
ceding example shows that Q is not a complete metric space. However, if
the limit points of all Cauchy sequences are added to Q, the resulting space
becomes complete. This complete space is, of course, the real number sys-
tem R. It turns out that any incomplete metric space can be &ldquo;enlarged&rdquo; to a
complete metric space.
</p>
<p>1.4 Cardinality
</p>
<p>The process of counting is a one-to-one comparison of one set with another.
If two sets are in one-to-one correspondence, they are said to have the same
cardinality. Two sets with the same cardinality essentially have the samecardinality
&ldquo;number&rdquo; of elements. The set Fn = {1,2, . . . , n} is finite and has cardinal-
ity n. Any set from which there is a bijection onto Fn is said to be finite with
n elements.
</p>
<p>Historical Notes
</p>
<p>Although some steps had been taken before him in the direction of a definitive theory of
sets, the creator of the theory of sets is considered to be Georg Cantor (1845&ndash;1918), who
was born in Russia of Danish-Jewish parentage but moved to Germany with his parents.
His father urged him to study engineering, and Cantor entered the University of Berlin in
1863 with that intention. There he came under the influence of Weierstrass and turned to</p>
<p/>
</div>
<div class="page"><p/>
<p>1.4 Cardinality 11
</p>
<p>pure mathematics. He became Privatdozent at Halle in 1869 and professor in 1879. When
he was twenty-nine he published his first revolutionary paper on the theory of infinite sets
in the Journal f&uuml;r Mathematik. Although some of its propositions were deemed faulty
by the older mathematicians, its overall originality and brilliance attracted attention. He
continued to publish papers on the theory of sets and on transfinite numbers until 1897.
One of Cantor&rsquo;s main concerns was to differentiate among infinite sets by &ldquo;size&rdquo; and,
likeBolzano before him, he decided that one-to-one correspondence should be the basic
principle. In his correspondence with Dedekind in 1873, Cantor posed the question of
whether the set of real numbers can be put into one-to-one correspondence with the inte-
gers, and some weeks later he answered in the negative. He gave two proofs. The first is
more complicated than the second, which is the one most often used today. In 1874 Can-
tor occupied himself with the equivalence of the points of a line and the points of Rn and
sought to prove that a one-to-one correspondence between these two sets was impossible.
Three years later he proved that there is such a correspondence. He wrote to Dedekind,
&ldquo;I see it but I do not believe it.&rdquo; He later showed that given any set, it is always possible
</p>
<p>Georg Cantor 1845&ndash;1918
</p>
<p>to create a new set, the set of subsets of the given set, whose cardinal number is larger
than that of the given set. For the natural numbers N, whose cardinality is denoted by &alefsym;0,
the cardinal number of the set of subsets is denoted by 2&alefsym;0 . Cantor proved that 2&alefsym;0 = c,
where c is the cardinal number of the continuum; i.e., the set of real numbers.
Cantor&rsquo;s work, which resolved age-old problems and reversed much previous thought,
could hardly be expected to receive immediate acceptance. His ideas on transfinite ordi-
nal and cardinal numbers aroused the hostility of the powerful Leopold Kronecker, who
attacked Cantor&rsquo;s theory savagely over more than a decade, repeatedly preventing Can-
tor from obtaining a more prominent appointment in Berlin. Though Kronecker died in
1891, his attacks left mathematicians suspicious of Cantor&rsquo;s work. Poincar&eacute; referred to
set theory as an interesting &ldquo;pathological case.&rdquo; He also predicted that &ldquo;Later generations
will regard [Cantor&rsquo;s] Mengenlehre as a disease from which one has recovered.&rdquo; At one
time Cantor suffered a nervous breakdown, but resumed work in 1887.
Many prominent mathematicians, however, were impressed by the uses to which the new
theory had already been put in analysis, measure theory, and topology. Hilbert spread
Cantor&rsquo;s ideas in Germany, and in 1926 said, &ldquo;No one shall expel us from the paradise
which Cantor created for us.&rdquo; He praised Cantor&rsquo;s transfinite arithmetic as &ldquo;the most as-
tonishing product of mathematical thought, one of the most beautiful realizations of hu-
man activity in the domain of the purely intelligible.&rdquo; Bertrand Russell described Cantor&rsquo;s
work as &ldquo;probably the greatest of which the age can boast.&rdquo; The subsequent utility of Can-
tor&rsquo;s work in formalizing mathematics&mdash;a movement largely led by Hilbert&mdash;seems at
odds with Cantor&rsquo;s Platonic view that the greater importance of his work was in its impli-
cations for metaphysics and theology. That his work could be so seamlessly diverted from
the goals intended by its creator is strong testimony to its objectivity and craftsmanship.
</p>
<p>Now consider the set of natural numbers N= {1,2,3, . . . }. If there exists
a bijection between a set A and N, then A is said to be countably infinite. countably infinite
Some examples of countably infinite sets are the set of all integers, the set
of even natural numbers, the set of odd natural numbers, the set of all prime
numbers, and the set of energy levels of the bound states of a hydrogen atom.
</p>
<p>It may seem surprising that a subset (such as the set of all even numbers)
can be put into one-to-one correspondence with the full set (the set of all
natural numbers); however, this is a property shared by all infinite sets. In
fact, sometimes infinite sets are defined as those sets that are in one-to-one
correspondence with at least one of their proper subsets. It is also astonish-
ing to discover that there are as many rational numbers as there are natural
numbers. After all, there are infinitely many rational numbers just in the
interval (0,1)&mdash;or between any two distinct real numbers!3
</p>
<p>3The proof involves writing m/n as the mnth entry in an &infin;&times;&infin; matrix and starting
the &ldquo;count&rdquo; with the (1,1) entry, going to the right to (1,2), then diagonally to (2,1),</p>
<p/>
</div>
<div class="page"><p/>
<p>12 1 Mathematical Preliminaries
</p>
<p>Fig. 1.5 The Cantor set after one, two, three, and four &ldquo;dissections&rdquo;
</p>
<p>Sets that are neither finite nor countably infinite are said to be uncount-uncountable sets
able. In some sense they are &ldquo;more infinite&rdquo; than any countable set. Ex-
amples of uncountable sets are the points in the interval (&minus;1,+1), the real
numbers, the points in a plane, and the points in space. It can be shown
that these sets have the same cardinality: There are as many points in
three-dimensional space&mdash;the whole universe&mdash;as there are in the interval
(&minus;1,+1) or in any other finite interval.
</p>
<p>Cardinality is a very intricate mathematical notion with many surprising
results. Consider the interval [0,1]. Remove the open interval ( 13 , 23 ) from
its middle (leaving the points 13 and
</p>
<p>2
3 behind). From the remaining portion,Cantor set constructed
</p>
<p>[0, 13 ]&cup;[ 23 ,1], remove the two middle thirds; the remaining portion will then
be [
</p>
<p>0,
1
</p>
<p>9
</p>
<p>]
&cup;
[
</p>
<p>2
</p>
<p>9
,
</p>
<p>1
</p>
<p>3
</p>
<p>]
&cup;
[
</p>
<p>2
</p>
<p>3
,
</p>
<p>7
</p>
<p>9
</p>
<p>]
&cup;
[
</p>
<p>8
</p>
<p>9
,1
</p>
<p>]
</p>
<p>(see Fig. 1.5). Do this indefinitely. What is the cardinality of the remaining
set, which is called the Cantor set? Intuitively we expect hardly anything to
be left. We might persuade ourselves into accepting the fact that the number
of points remaining is at most infinite but countable. The surprising fact is
that the cardinality is that of the continuum! Thus, after removal of infinitely
many middle thirds, the set that remains has as many points as the original
set!
</p>
<p>1.5 Mathematical Induction
</p>
<p>Many a time it is desirable to make a mathematical statement that is true
for all natural numbers. For example, we may want to establish a formula
involving an integer parameter that will hold for all positive integers. One
encounters this situation when, after experimenting with the first few posi-
tive integers, one recognizes a pattern and discovers a formula, and wants to
make sure that the formula holds for all natural numbers. For this purpose,
one uses mathematical induction. The essence of mathematical induction
is stated as follows:induction principle
</p>
<p>then down to (3,1), then diagonally up, and so on. Obviously the set is countable and it
exhausts all rational numbers. In fact, the process double counts some of the entries.</p>
<p/>
</div>
<div class="page"><p/>
<p>1.5 Mathematical Induction 13
</p>
<p>Proposition 1.5.1 Suppose that there is associated with each natural num-
ber (positive integer) n a statement Sn. Then Sn is true for every positive
integer provided the following two conditions hold:
</p>
<p>1. S1 is true.
2. If Sm is true for some given positive integer m, then Sm+1 is also true.
</p>
<p>Example 1.5.2 We illustrate the use of mathematical induction by proving
the binomial theorem: binomial theorem
</p>
<p>(a + b)m =
m&sum;
</p>
<p>k=0
</p>
<p>(
m
</p>
<p>k
</p>
<p>)
am&minus;kbk =
</p>
<p>m&sum;
</p>
<p>k=0
</p>
<p>m!
k!(m&minus; k)!a
</p>
<p>m&minus;kbk
</p>
<p>= am +mam&minus;1b+ m(m&minus; 1)
2! a
</p>
<p>m&minus;2b2 + &middot; &middot; &middot; +mabm&minus;1 + bm,
(1.1)
</p>
<p>where we have used the notation
</p>
<p>(
m
</p>
<p>k
</p>
<p>)
&equiv; m!
</p>
<p>k!(m&minus; k)! . (1.2)
</p>
<p>The mathematical statement Sm is Eq. (1.1). We note that S1 is trivially true:
(a + b)1 = a1 + b1. Now we assume that Sm is true and show that Sm+1 is
also true. This means starting with Eq. (1.1) and showing that
</p>
<p>(a + b)m+1 =
m+1&sum;
</p>
<p>k=0
</p>
<p>(
m+ 1
k
</p>
<p>)
am+1&minus;kbk.
</p>
<p>Then the induction principle ensures that the statement (equation) holds for
all positive integers. Multiply both sides of Eq. (1.1) by a + b to obtain
</p>
<p>(a + b)m+1 =
m&sum;
</p>
<p>k=0
</p>
<p>(
m
</p>
<p>k
</p>
<p>)
am&minus;k+1bk +
</p>
<p>m&sum;
</p>
<p>k=0
</p>
<p>(
m
</p>
<p>k
</p>
<p>)
am&minus;kbk+1.
</p>
<p>Now separate the k = 0 term from the first sum and the k =m term from the
second sum:
</p>
<p>(a + b)m+1 = am+1 +
m&sum;
</p>
<p>k=1
</p>
<p>(
m
</p>
<p>k
</p>
<p>)
am&minus;k+1bk +
</p>
<p>m&minus;1&sum;
</p>
<p>k=0
</p>
<p>(
m
</p>
<p>k
</p>
<p>)
am&minus;kbk+1
</p>
<p>︸ ︷︷ ︸
let k = j &minus; 1 in this sum
</p>
<p>+bm+1
</p>
<p>= am+1 +
m&sum;
</p>
<p>k=1
</p>
<p>(
m
</p>
<p>k
</p>
<p>)
am&minus;k+1bk
</p>
<p>+
m&sum;
</p>
<p>j=1
</p>
<p>(
m
</p>
<p>j &minus; 1
</p>
<p>)
am&minus;j+1bj + bm+1.</p>
<p/>
</div>
<div class="page"><p/>
<p>14 1 Mathematical Preliminaries
</p>
<p>The second sum in the last line involves j . Since this is a dummy index,
we can substitute any symbol we please. The choice k is especially useful
because then we can unite the two summations. This gives
</p>
<p>(a + b)m+1 = am+1 +
m&sum;
</p>
<p>k=1
</p>
<p>{(
m
</p>
<p>k
</p>
<p>)
+
(
</p>
<p>m
</p>
<p>k &minus; 1
</p>
<p>)}
am&minus;k+1bk + bm+1.
</p>
<p>If we now use (
m+ 1
k
</p>
<p>)
=
(
m
</p>
<p>k
</p>
<p>)
+
(
</p>
<p>m
</p>
<p>k &minus; 1
</p>
<p>)
,
</p>
<p>which the reader can easily verify, we finally obtain
</p>
<p>(a + b)m+1 = am+1 +
m&sum;
</p>
<p>k=1
</p>
<p>(
m+ 1
k
</p>
<p>)
am&minus;k+1bk + bm+1
</p>
<p>=
m+1&sum;
</p>
<p>k=0
</p>
<p>(
m+ 1
k
</p>
<p>)
am&minus;k+1bk.
</p>
<p>This complete the proof.
</p>
<p>Mathematical induction is also used in defining quantities involving inte-
gers. Such definitions are called inductive definitions. For example, induc-inductive definitions
tive definition is used in defining powers: a1 = a and am = am&minus;1a.
</p>
<p>1.6 Problems
</p>
<p>1.1 Show that the number of subsets of a set containing n elements is 2n.
</p>
<p>1.2 Let A, B , and C be sets in a universal set U . Show that
</p>
<p>(a) A&sub; B and B &sub; C implies A&sub; C.
(b) A&sub; B iff A&cap;B =A iff A&cup;B = B .
(c) A&sub; B and B &sub; C implies (A&cup;B)&sub; C.
(d) A&cup;B = (A&sim; B)&cup; (A&cap;B)&cup; (B &sim;A).
Hint: To show the equality of two sets, show that each set is a subset of the
other.
</p>
<p>1.3 For each n &isin;N, let
</p>
<p>In =
{
x | |x &minus; 1|&lt; n and |x + 1|&gt; 1
</p>
<p>n
</p>
<p>}
.
</p>
<p>Find
⋃
</p>
<p>n In and
⋂
</p>
<p>n In.
</p>
<p>1.4 Show that a&prime; &isin; ❏a❑ implies that ❏a&prime;❑= ❏a❑.
</p>
<p>1.5 Can you define a binary operation of &ldquo;multiplication&rdquo; on the set of vec-
tors in space? What about vectors in the plane? In each case, write the com-
ponents of the product in terms of the components of the two vectors.</p>
<p/>
</div>
<div class="page"><p/>
<p>1.6 Problems 15
</p>
<p>1.6 Show that (f ◦ g)&minus;1 = g&minus;1 ◦ f&minus;1 when f and g are both bijections.
</p>
<p>1.7 We show that the sequence {xn}&infin;n=1, where xn =
&sum;n
</p>
<p>k=1(&minus;1)k+1/k, is
Cauchy. Without loss of generality, assume that n &gt; m and n&minus;m is even
(the case of odd n&minus;m can be handled similarly).
(a) Show that
</p>
<p>xn &minus; xm = (&minus;1)m
n&minus;m&sum;
</p>
<p>j=1
</p>
<p>(&minus;1)j
j +m.
</p>
<p>(b) Separate the even and odd parts of the sum and show that
</p>
<p>xn &minus; xm = (&minus;1)m
{
&minus;
</p>
<p>(n&minus;m)/2&sum;
</p>
<p>k=1
</p>
<p>1
</p>
<p>2k&minus; 1 +m +
(n&minus;m)/2&sum;
</p>
<p>k=1
</p>
<p>1
</p>
<p>2k+m
</p>
<p>}
.
</p>
<p>(c) Add the two sums to obtain a single sum, showing that
</p>
<p>xn &minus; xm =&minus;(&minus;1)m
{
(n&minus;m)/2&sum;
</p>
<p>k=1
</p>
<p>1
</p>
<p>(2k+m)(2k +m&minus; 1)
</p>
<p>}
,
</p>
<p>and that
</p>
<p>|xn &minus; xm| &le;
(n&minus;m)/2&sum;
</p>
<p>k=1
</p>
<p>1
</p>
<p>(2k +m&minus; 1)2
</p>
<p>= 1
(1 +m)2 +
</p>
<p>(n&minus;m)/2&sum;
</p>
<p>k=2
</p>
<p>1
</p>
<p>(2k +m&minus; 1)2 .
</p>
<p>(d) Convince yourself that
&int; s
</p>
<p>1 f (x)dx &ge;
&sum;s
</p>
<p>k=2 f (k) for any continuous
function f (x), and apply it to part (c) to get
</p>
<p>|xn &minus; xm| &le;
1
</p>
<p>(1 +m)2 +
&int; (n&minus;m)/2
</p>
<p>1
</p>
<p>1
</p>
<p>(2x +m&minus; 1)2 dx
</p>
<p>= 1
n
+ 1
</p>
<p>(1 +m)2 &minus;
1
</p>
<p>2
</p>
<p>(
1
</p>
<p>n&minus; 1 &minus;
1
</p>
<p>m+ 1
</p>
<p>)
.
</p>
<p>Each term on the last line goes to zero independently as m and n go to
infinity.
</p>
<p>1.8 Find a bijection f :N&rarr; Z. Hint: Find an f which maps even integers
onto the positive integers and odd integers onto the negative integers.
</p>
<p>1.9 Take any two open intervals (a, b) and (c, d), and show that there are
as many points in the first as there are in the second, regardless of the size of
the intervals. Hint: Find a (linear) algebraic relation between points of the
two intervals.</p>
<p/>
</div>
<div class="page"><p/>
<p>16 1 Mathematical Preliminaries
</p>
<p>1.10 Use mathematical induction to derive the Leibniz rule for differenti-Leibniz rule
ating a product:
</p>
<p>dn
</p>
<p>dxn
(f &middot; g)=
</p>
<p>n&sum;
</p>
<p>k=0
</p>
<p>(
n
</p>
<p>k
</p>
<p>)
dkf
</p>
<p>dxk
</p>
<p>dn&minus;kg
dxn&minus;k
</p>
<p>.
</p>
<p>1.11 Use mathematical induction to derive the following results:
</p>
<p>n&sum;
</p>
<p>k=0
rk = r
</p>
<p>n+1 &minus; 1
r &minus; 1 ,
</p>
<p>n&sum;
</p>
<p>k=0
k = n(n+ 1)
</p>
<p>2
.</p>
<p/>
</div>
<div class="page"><p/>
<p>Part I
</p>
<p>Finite-Dimensional Vector Spaces</p>
<p/>
</div>
<div class="page"><p/>
<p>2Vectors and Linear Maps
</p>
<p>The familiar two- and three-dimensional vectors can easily be generalized
to higher dimensions. Representing vectors by their components, one can
conceive of vectors having N components. This is the most immediate gen-
eralization of vectors in the plane and in space, and such vectors are called
N -dimensional Cartesian vectors. Cartesian vectors are limited in two re- Cartesian vectors
spects: Their components are real, and their dimensionality is finite. Some
applications in physics require the removal of one or both of these limi-
tations. It is therefore convenient to study vectors stripped of any dimen-
sionality or reality of components. Such properties become consequences of
more fundamental definitions. Although we will be concentrating on finite-
dimensional vector spaces in this part of the book, many of the concepts and
examples introduced here apply to infinite-dimensional spaces as well.
</p>
<p>2.1 Vector Spaces
</p>
<p>Let us begin with the definition of an abstract (complex) vector space.1
</p>
<p>Definition 2.1.1 A vector space V over C is a set of objects denoted by
|a〉, |b〉, |x〉, and so on, called vectors, with the following properties: vector space defined
</p>
<p>1. To every pair of vectors |a〉 and |b〉 in V there corresponds a vector
|a〉 + |b〉, also in V, called the sum of |a〉 and |b〉, such that
(a) |a〉 + |b〉 = |b〉 + |a〉,
(b) |a〉 + (|b〉 + |c〉)= (|a〉 + |b〉)+ |c〉,
(c) There exists a unique vector |0〉 &isin; V, called the zero vector, such
</p>
<p>that |a〉 + |0〉 = |a〉 for every vector |a〉,
(d) To every vector |a〉 &isin; V there corresponds a unique vector &minus;|a〉 &isin;
</p>
<p>V such that |a〉 + (&minus;|a〉)= |0〉.
</p>
<p>1Keep in mind that C is the set of complex numbers and R the set of reals.
</p>
<p>S. Hassani, Mathematical Physics, DOI 10.1007/978-3-319-01195-0_2,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>19</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-01195-0_2">http://dx.doi.org/10.1007/978-3-319-01195-0_2</a></div>
</div>
<div class="page"><p/>
<p>20 2 Vectors and Linear Maps
</p>
<p>2. To every complex number2 α&mdash;also called a scalar&mdash;and every vector
|a〉 there corresponds a vector α|a〉 in V such thatscalars are numbers
(a) α(β|a〉)= (αβ)|a〉,
(b) 1|a〉 = |a〉.
</p>
<p>3. Multiplication involving vectors and scalars is distributive:
(a) α(|a〉 + |b〉)= α|a〉 + α|b〉.
(b) (α + β)|a〉 = α|a〉 + β|a〉.
</p>
<p>The bra, 〈 |, and ket, | 〉, notation for vectors, invented by Dirac, is veryDirac&rsquo;s bra and ket
notation useful when dealing with complex vector spaces. However, it is somewhat
</p>
<p>clumsy for certain topics such as norm and metrics and will therefore be
abandoned in those discussions.
</p>
<p>The vector space defined above is also called a complex vector space. Itcomplex versus real
vector space is possible to replace C with R&mdash;the set of real numbers&mdash;in which case the
</p>
<p>resulting space will be called a real vector space.
Real and complex numbers are prototypes of a mathematical structure
</p>
<p>called field. A field F is a set of objects with two binary operations called ad-concept of field
summarized dition and multiplication. Multiplication distributes over addition, and each
</p>
<p>operation has an identity. The identity for addition is denoted by 0 and is
called additive identity. The identity for multiplication is denoted by 1 and
is called multiplicative identity. Furthermore, every element α &isin; F has an
additive inverse &minus;α, and every element except the additive identity has a
multiplicative inverse α&minus;1.
</p>
<p>Example 2.1.2 (Some vector spaces)
</p>
<p>1. R is a vector space over the field of real numbers.
2. C is a vector space over the field of real numbers.
3. C is a vector space over the complex numbers.
4. Let V=R and let the field of scalars be C. This is not a vector space,
</p>
<p>because property 2 of Definition 2.1.1 is not satisfied: A complex
number times a real number is not a real number and therefore does
not belong to V.
</p>
<p>5. The set of &ldquo;arrows&rdquo; in the plane (or in space) forms a vector space
over R under the parallelogram law of addition of planar (or spatial)
vectors.
</p>
<p>6. Let Pc[t] be the set of all polynomials with complex coefficients in
a variable t . Then Pc[t] is a vector space under the ordinary addition
of polynomials and the multiplication of a polynomial by a complex
number. In this case the zero vector is the zero polynomial.
</p>
<p>7. For a given positive integer n, let Pcn[t] be the set of all polynomials
with complex coefficients of degree less than or equal to n. Again it
is easy to verify that Pcn[t] is a vector space under the usual addition
</p>
<p>2Complex numbers, particularly when they are treated as variables, are usually denoted
by z, and we shall adhere to this convention in Part III. However, in the discussion of
vector spaces, we have found it more convenient to use lower case Greek letters to denote
complex numbers as scalars.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.1 Vector Spaces 21
</p>
<p>of polynomials and their multiplication by complex scalars. In partic-
ular, the sum of two polynomials of degree less than or equal to n is
also a polynomial of degree less than or equal to n, and multiplying
a polynomial with complex coefficients by a complex number gives
another polynomial of the same type. Here the zero polynomial is the
zero vector.
</p>
<p>8. The set Prn[t] of polynomials of degree less than or equal to n with
real coefficients is a vector space over the reals, but it is not a vector
space over the complex numbers.
</p>
<p>9. Let Cn consist of all complex n-tuples such as |a〉 = (α1, α2, . . . , αn)
and |b〉 = (β1, β2, . . . , βn). Let η be a complex number. Then we de-
fine
</p>
<p>|a〉 + |b〉 = (α1 + β1, α2 + β2, . . . , αn + βn),
η|a〉 = (ηα1, ηα2, . . . , ηαn),
|0〉 = (0,0, . . . ,0),
</p>
<p>&minus;|a〉 = (&minus;α1,&minus;α2, . . . ,&minus;αn).
</p>
<p>It is easy to verify that Cn is a vector space over the complex numbers. n-dimensional complex
coordinate spaceIt is called the n-dimensional complex coordinate space.
</p>
<p>10. The set of all real n-tuples Rn is a vector space over the real num-
bers under the operations similar to that of Cn. It is called the n-
dimensional real coordinate space, or Cartesian n-space. It is not n-dimensional real
</p>
<p>coordinate space, or
</p>
<p>Cartesian n-space
</p>
<p>a vector space over the complex numbers.
11. The set of all complex matrices with m rows and n columns Mm&times;n is
</p>
<p>a vector space under the usual addition of matrices and multiplication
by complex numbers. The zero vector is the m &times; n matrix with all
entries equal to zero.
</p>
<p>12. Let C&infin; be the set of all complex sequences |a〉 = {αi}&infin;i=1 such that&sum;&infin;
i=1 |αi |2 &lt;&infin;. One can show that with addition and scalar multipli-
</p>
<p>cation defined componentwise, C&infin; is a vector space over the complex
numbers.
</p>
<p>13. The set of all complex-valued functions of a single real variable that
are continuous in the real interval (a, b) is a vector space over the
complex numbers.
</p>
<p>14. The set Cn(a, b) of all real-valued functions of a single real variable
defined on (a, b) that possess continuous derivatives of all orders up
to n forms a vector space over the reals.
</p>
<p>15. The set C&infin;(a, b) of all real-valued functions on (a, b) of a single real
variable that possess derivatives of all orders forms a vector space over
the reals.
</p>
<p>It is clear from the example above that a vector space depends as much
on the nature of the vectors as on the nature of the scalars.
</p>
<p>Definition 2.1.3 The vectors |a1〉, |a2〉, . . . , |an〉, are said to be linearly in- linear independence and
linear combination of
</p>
<p>vectors defined
</p>
<p>dependent if for αi &isin; C, the relation
&sum;n
</p>
<p>i=1 αi |ai〉 = 0 implies αi = 0 for
all i. The sum
</p>
<p>&sum;n
i=1 αi |ai〉 is called a linear combination of {|ai〉}ni=1.</p>
<p/>
</div>
<div class="page"><p/>
<p>22 2 Vectors and Linear Maps
</p>
<p>2.1.1 Subspaces
</p>
<p>Given a vector space V, one can consider a collection W of vectors in V,
i.e., a subset of V. Because W is a subset, it contains vectors, but there is no
guarantee that it contains the linear combination of those vectors. We now
investigate conditions under which it does.
</p>
<p>Definition 2.1.4 A subspace W of a vector space V is a nonempty subsetsubspace
of V with the property that if |a〉, |b〉 &isin;W, then α|a〉 + β|b〉 also belongs to
W for all α,β &isin;C.
</p>
<p>The reader may verify that a subspace is a vector space in its own right,The intersection of two
subspaces is also a
</p>
<p>subspace.
and that the intersection of two subspaces is also a subspace.
</p>
<p>Example 2.1.5 The following are subspaces of some of the vector spaces
considered in Example 2.1.2. The reader is urged to verify the validity of
each case.
</p>
<p>&bull; The &ldquo;space&rdquo; of real numbers is a subspace of C over the reals.
&bull; R is not a subspace of C over the complex numbers, because as ex-
</p>
<p>plained in Example 2.1.2, R cannot be a vector space over the complex
numbers.
</p>
<p>&bull; The set of all vectors along a given line going through the origin is a
subspace of arrows in the plane (or space) over R.
</p>
<p>&bull; Pcn[t] is a subspace of Pc[t].
&bull; Cn&minus;1 is a subspace of Cn when Cn&minus;1 is identified with all complex n-
</p>
<p>tuples with zero last entry. In general, Cm is a subspace of Cn for m&lt; n
when Cm is identified with all n-tuples whose last n&minus;m elements are
zero.
</p>
<p>&bull; Mr&times;s is a subspace of Mm&times;n for r &le; m and s &le; n. Here, we identify
an r &times; s matrix with an m&times; n matrix whose last m&minus; r rows and n&minus; s
columns are all zero.
</p>
<p>&bull; Pcm[t] is a subspace of Pcn[t] for m&lt; n.
&bull; Prm[t] is a subspace of Prn[t] for m&lt; n. Note that both Prn[t] and Prm[t]
</p>
<p>are vector spaces over the reals only.
&bull; Rm is a subspace of Rn for m &lt; n. Therefore, R2, the plane, is a sub-
</p>
<p>space of R3, the Euclidean space. Also, R1 &equiv; R is a subspace of both
the plane R2 and the Euclidean space R3.
</p>
<p>&bull; Let a be along the x-axis (a subspace of R2) and b along the y-axis (also
a subspace of R2). Then a + b is neither along the x-axis nor along theunion of two subspaces
</p>
<p>is not a subspace y-axis. This shows that the union of two subspaces is not generally a
subspace.
</p>
<p>Theorem 2.1.6 If S is any nonempty set of vectors in a vector space V, then
the set WS of all linear combinations of vectors in S is a subspace of V. Wespan of a subset of a
</p>
<p>vector space say that WS is the span of S, or that S spans WS , or that WS is spanned
by S. WS is often denoted by Span{S}.
</p>
<p>The proof of Theorem 2.1.6 is left as Problem 2.6.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.1 Vector Spaces 23
</p>
<p>Definition 2.1.7 A basis of a vector space V is a set B of linearly inde- basis defined
</p>
<p>pendent vectors that spans all of V. A vector space that has a finite basis is
called finite-dimensional; otherwise, it is infinite-dimensional.
</p>
<p>The definition of the dimensionality of a vector space based on a single
basis makes sense because of the following theorem which we state without
proof (see [Axle 96, page 31]):
</p>
<p>Theorem 2.1.8 All bases of a given finite-dimensional vector space have
the same number of linearly independent vectors.
</p>
<p>Definition 2.1.9 The cardinality of a basis of a vector space V is called the
dimension of V and denoted by dimV. To emphasize its dependence on the
scalars, dimCV and dimRV are also used. A vector space of dimension N
is sometimes denoted by VN .
</p>
<p>If |a〉 is a vector in an N -dimensional vector space V and B = {|ai〉}Ni=1 a
basis in that space, then by the definition of a basis, there exists a unique (see
Problem 2.4) set of scalars {α1, α2, . . . , αn} such that |a〉 =
</p>
<p>&sum;N
i=1 αi |ai〉.
</p>
<p>The set {αi}Ni=1 is called the components of |a〉 in the basis B . components of a vector
in a basis
</p>
<p>Example 2.1.10 The following are bases for the vector spaces given in Ex-
ample 2.1.2.
</p>
<p>&bull; The number 1 (or any nonzero real number) is a basis for R, which is
therefore one-dimensional.
</p>
<p>&bull; The numbers 1 and i =
&radic;
&minus;1 (or any pair of distinct nonzero complex
</p>
<p>numbers) are basis vectors for the vector space C over R. Thus, this
space is two-dimensional.
</p>
<p>&bull; The number 1 (or any nonzero complex number) is a basis for C over
C, and the space is one-dimensional. Note that although the vectors are
the same as in the preceding item, changing the nature of the scalars
changes the dimensionality of the space.
</p>
<p>&bull; The set {êx, êy, êz} of the unit vectors in the directions of the three axes
forms a basis in space. The space is three-dimensional.
</p>
<p>&bull; A basis of Pc[t] can be formed by the monomials 1, t, t2, . . . . It is clear
that this space is infinite-dimensional.
</p>
<p>&bull; A basis of Cn is given by ê1, ê2, . . . , ên, where êj is an n-tuple that has
a 1 at the j th position and zeros everywhere else. This basis is called
the standard basis of Cn. Clearly, the space has n dimensions. standard basis of C
</p>
<p>n
</p>
<p>&bull; A basis of Mm&times;n is given by e11,e12, . . . ,eij , . . . ,emn, where eij is the
m&times;n matrix with zeros everywhere except at the intersection of the ith
row and j th column, where it has a one.
</p>
<p>&bull; A set consisting of the monomials 1, t, t2, . . . , tn forms a basis of Pcn[t].
Thus, this space is (n+ 1)-dimensional.
</p>
<p>&bull; The standard basis of Cn is a basis of Rn as well. It is also called the
standard basis of Rn. Thus, Rn is n-dimensional.</p>
<p/>
</div>
<div class="page"><p/>
<p>24 2 Vectors and Linear Maps
</p>
<p>&bull; If we assume that a &lt; 0 &lt; b, then the set of monomials 1, x, x2, . . .
forms a basis for C&infin;(a, b), because, by Taylor&rsquo;s theorem, any function
belonging to C&infin;(a, b) can be expanded in an infinite power series about
x = 0. Thus, this space is infinite-dimensional.
</p>
<p>Remark 2.1.1 Given a space V with a basis B = {|ai〉}ni=1, the span of any
m vectors (m &lt; n) of B is an m-dimensional subspace of V.
</p>
<p>2.1.2 Factor Space
</p>
<p>Let W be a subspace of the vector space V, and define a relation on V as
follows. If |a〉 &isin; V and |b〉 &isin; V, then we say that |a〉 is related to |b〉, and
write |a〉 ⊲⊳ |b〉 if |a〉&minus;|b〉 is in W. It is easy to show that ⊲⊳ is an equivalence
relation. Denote the equivalence class of |a〉 by ❏a❑, and the factor set (or
quotient set) {❏a❑||a〉 &isin; V} by V/W. We turn the factor set into a factor
space by defining the combined addition of vectors and their multiplication
by scalars as follows:
</p>
<p>α❏a❑+ β❏b❑= ❏αa + βb❑ (2.1)
</p>
<p>where ❏αa + βb❑ is the equivalence class of α|a〉 + β|b〉. For this equation
to make sense, it must be independent of the choice of the representatives of
the classes. If ❏a&prime;❑ = ❏a❑ and ❏b&prime;❑ = ❏b❑, then is it true that ❏αa&prime; + βb&prime;❑ =
❏αa + βb❑? For this to happen, we must have
</p>
<p>(
α
∣∣a&prime;
</p>
<p>&rang;
+ β
</p>
<p>∣∣b&prime;
&rang;)
&minus;
(
α|a〉 + β|b〉
</p>
<p>)
&isin;W.
</p>
<p>Now, since |a&prime;〉 &isin; ❏a❑, we must have |a&prime;〉 = |a〉 + |w1〉 for some |w1〉 &isin;W.
Similarly, |b&prime;〉 = |b〉 + |w2〉. Therefore,
</p>
<p>(
α
∣∣a&prime;
</p>
<p>&rang;
+ β
</p>
<p>∣∣b&prime;
&rang;)
&minus;
(
α|a〉 + β|b〉
</p>
<p>)
= α|w1〉 + β|w2〉
</p>
<p>and the right-hand side is in W because W is a subspace.
Sometimes ❏a❑ is written as |a〉+W. With this notation comes the equal-
</p>
<p>ities
</p>
<p>|w〉 +W=W, W+W=W, αW=W, αW+ βW=W,
</p>
<p>which abbreviate the obvious fact that the sum of two vectors in W is a
vector in W, the product of a scalar and a vector in W is a vector in W, and
the linear combination of two vectors in W is a vector in W.
</p>
<p>How do we find a basis for V/W? Let {|ai〉} be a basis for W. Extend it
to a basis {|ai〉, |bj 〉} for V. Then, {❏bj ❑} form a basis for V/W. Indeed, let
❏a❑ &isin; V/W. Then, since |a〉 is in V, we have
</p>
<p>❏a❑&equiv; |a〉 +W=
&isin;W︷ ︸︸ ︷&sum;
</p>
<p>i
</p>
<p>αi |ai〉+
&sum;
</p>
<p>j
</p>
<p>βj |bj 〉 +W</p>
<p/>
</div>
<div class="page"><p/>
<p>2.1 Vector Spaces 25
</p>
<p>=
&sum;
</p>
<p>j
</p>
<p>βj |bj 〉 +W &rArr; ❏a❑=
�&sum;
</p>
<p>j
</p>
<p>βj |bj 〉
③
=
&sum;
</p>
<p>j
</p>
<p>βj ❏bj ❑.
</p>
<p>Thus, {❏bj ❑} span V/W. To form a basis, they also have to be linearly inde-
pendent. So, suppose that
</p>
<p>&sum;
j βj ❏bj ❑= ❏0❑. This means that
</p>
<p>&sum;
</p>
<p>j
</p>
<p>βj |bj 〉 +W= |0〉 +W=W &rArr;
&sum;
</p>
<p>j
</p>
<p>βj |bj 〉 &isin;W.
</p>
<p>So the last sum must be a linear combination of {|ai〉}:
&sum;
</p>
<p>j
</p>
<p>βj |bj 〉 =
&sum;
</p>
<p>i
</p>
<p>αi |aj 〉 or
&sum;
</p>
<p>j
</p>
<p>βj |bj 〉 &minus;
&sum;
</p>
<p>i
</p>
<p>αi |aj 〉 = 0.
</p>
<p>This is a zero linear combination of the basis vectors of V. Therefore, all co-
efficients, including all βj must be zero. One consequence of the argument
above is (with obvious notation)
</p>
<p>dim(V/W)= dimV&minus; dimW (2.2)
</p>
<p>2.1.3 Direct Sums
</p>
<p>Sometimes it is possible, and convenient, to break up a vector space into spe-
cial (disjoint) subspaces. For instance, the study of the motion of a particle
in R3 under the influence of a central force field is facilitated by decompos-
ing the motion into its projections onto the direction of angular momentum
and onto a plane perpendicular to the angular momentum. This corresponds
to decomposing a vector in space into a vector, say in the xy-plane and a
vector along the z-axis. We can generalize this to any vector space, but first
some notation: Let U and W be subspaces of a vector space V. Denote by Sum of two subspaces
</p>
<p>definedU+W the collection of all vectors in V that can be written as a sum of two
vectors, one in U and one in W. It is easy to show that U+W is a subspace
of V.
</p>
<p>Example 2.1.11 Let U be the xy-plane and W the yz-plane. These are both
subspaces of R3, and so is U+W. In fact, U+W=R3, because given any
vector (x, y, z) in R3, we can write it as
</p>
<p>(x, y, z)=
(
x,
</p>
<p>1
</p>
<p>2
y,0
</p>
<p>)
</p>
<p>︸ ︷︷ ︸
&isin;U
</p>
<p>+
(
</p>
<p>0,
1
</p>
<p>2
y, z
</p>
<p>)
</p>
<p>︸ ︷︷ ︸
&isin;W
</p>
<p>.
</p>
<p>This decomposition is not unique: We could also write (x, y, z) =
(x, 13y,0)+ (0, 23y, z), and a host of other relations.
</p>
<p>Definition 2.1.12 Let U and W be subspaces of a vector space V such that direct sum U&oplus;W
defined
</p>
<p>V=U+W and U&cap;W= {|0〉}. Then we say that V is the direct sum of U
and W and write V=U&oplus;W.</p>
<p/>
</div>
<div class="page"><p/>
<p>26 2 Vectors and Linear Maps
</p>
<p>Proposition 2.1.13 Let U and W be subspaces of V such that V=U+W.
Then V = U &oplus; W if and only if any nonzero vector in V can be writtenuniqueness of direct sum
uniquely as a vector in U plus a vector in W.
</p>
<p>Proof Assume V= U&oplus;W, and let |v〉 &isin; V be written as a sum of a vector
in U and a vector in W in two different ways:
</p>
<p>|v〉 = |u〉 + |w〉 =
∣∣u&prime;
</p>
<p>&rang;
+
∣∣w&prime;
</p>
<p>&rang;
&hArr; |u〉 &minus;
</p>
<p>∣∣u&prime;
&rang;
=
∣∣w&prime;
</p>
<p>&rang;
&minus; |w〉.
</p>
<p>The LHS is in U. Since it is equal to the RHS&mdash;which is in W&mdash;it must be
in W as well. Therefore, the LHS must equal zero, as must the RHS. Thus,
|u〉 = |u&prime;〉, |w&prime;〉 = |w〉, and there is only one way that |v〉 can be written as a
sum of a vector in U and a vector in W.
</p>
<p>Conversely, suppose that any vector in V can be written uniquely as a
vector in U and a vector in W. If |a〉 &isin; U and also |a〉 &isin;W, then one can
write
</p>
<p>|a〉 = 1
3
|a〉
</p>
<p>︸︷︷︸
in U
</p>
<p>+ 2
3
|a〉
</p>
<p>︸︷︷︸
in W
</p>
<p>= 1
4
|a〉
</p>
<p>︸︷︷︸
in U
</p>
<p>+ 3
4
|a〉
</p>
<p>︸︷︷︸
in W
</p>
<p>.
</p>
<p>Hence |a〉 can be written in two different ways. By the uniqueness assump-
tion |a〉 cannot be nonzero. Therefore, the only vector common to both U
and W is the zero vector. This implies that V=U&oplus;W. �
</p>
<p>More generally, we have the following situation:
</p>
<p>Definition 2.1.14 Let {Ui}ri=1 be subspaces of V such that
</p>
<p>V=U1 + &middot; &middot; &middot; +Ur and Ui &cap;Uj =
{
|0〉
</p>
<p>}
for all i, j = 1, . . . , r.
</p>
<p>Then we say that V is the direct sum of {Ui}ri=1 and write
</p>
<p>V=U1 &oplus; &middot; &middot; &middot; &oplus;Ur =
r&oplus;
</p>
<p>i=1
Ui .
</p>
<p>Let W = U1 &oplus; &middot; &middot; &middot; &oplus; Us be a direct sum of s subspaces (they need not
span the entire V). Write W as W = U1 &oplus;W&prime;, with W&prime; = U2 &oplus; &middot; &middot; &middot; &oplus; Us .
Let {|ui〉}si=1 be nonzero vectors with |ui〉 &isin;Ui and suppose that
</p>
<p>α1|u1〉 + α2|u2〉 + &middot; &middot; &middot; + αs |us〉 = |0〉, (2.3)
</p>
<p>or
</p>
<p>α1|u1〉 + α
∣∣w&prime;
</p>
<p>&rang;
= |0〉 &rArr; α1|u1〉 = &minus;α
</p>
<p>∣∣w&prime;
&rang;
,
</p>
<p>with |w&prime;〉 &isin;W&prime;. Since α1|u1〉 &isin;U1 from the left-hand side, and α1|u1〉 &isin;W&prime;
from the right-hand side, we must have α1|u1〉 = |0〉. Hence, α1 = 0 because
|u1〉 �= |0〉. Equation (2.3) now becomes
</p>
<p>α2|u2〉 + α3|u3〉 + &middot; &middot; &middot; + αs |us〉 = |0〉.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.1 Vector Spaces 27
</p>
<p>Write this as
</p>
<p>α2|u2〉 + β
∣∣w&prime;&prime;
</p>
<p>&rang;
= |0〉 &rArr; α2|u2〉 = &minus;β
</p>
<p>∣∣w&prime;&prime;
&rang;
,
</p>
<p>where W&prime; =U2 &oplus;W&prime;&prime; with W&prime;&prime; =U3 &oplus; &middot; &middot; &middot; &oplus;Us and |w&prime;&prime;〉 &isin;W&prime;&prime;. An argu-
ment similar to above shows that α2 = 0. Continuing in this way, we have
</p>
<p>Proposition 2.1.15 The vectors in different subspaces of Definition 2.1.14
are linearly independent.
</p>
<p>Proposition 2.1.16 Let U be a subspace of V. Then there exist a subspace
W of V such that V=U&oplus;W.
</p>
<p>Proof Let {|ui〉}mi=1 be a basis of U. Extend this basis to a basis {|ui〉}Ni=1
of V. Then W= Span{|uj 〉}Nj=m+1. �
</p>
<p>Example 2.1.17 Let U be the xy-plane and W the z-axis. These are both
subspaces of R3, and so is U+W. Furthermore, it is clear that U+W=R3,
because given any vector (x, y, z) in R3, we can write it as
</p>
<p>(x, y, z)= (x, y,0)︸ ︷︷ ︸
&isin;U
</p>
<p>+ (0,0, z)︸ ︷︷ ︸
&isin;W
</p>
<p>.
</p>
<p>This decomposition is obviously unique. Therefore, R3 =U&oplus;W.
</p>
<p>Proposition 2.1.18 If V=U&oplus;W, then dimV= dimU+ dimW. dimensions in a direct
sum
</p>
<p>Proof Let {|ui〉}mi=1 be a basis for U and {|wi〉}ki=1 a basis for W. Then it
is easily verified that {|u1〉, |u2〉, . . . , |um〉, |w1〉, |w2〉, . . . , |wk〉} is a basis
for V. The details are left as an exercise. �
</p>
<p>Let U and V be any two vector spaces over R or C. Consider the Cartesian
product W &equiv; U&times; V of their underlying set. Define a scalar multiplication
and a sum on W by
</p>
<p>α
(
|u〉, |v〉
</p>
<p>)
=
(
α|u〉, α|v〉
</p>
<p>)
(
|u1〉, |v1〉
</p>
<p>)
+
(
|u2〉, |v2〉
</p>
<p>)
=
(
|u1〉 + |u2〉, |v1〉 + |v2〉
</p>
<p>)
.
</p>
<p>(2.4)
</p>
<p>With |0〉W = (|0〉U , |0〉V ), W becomes a vector space. Furthermore, if we
identify U and V with vectors of the form (|u〉, |0〉V ) and (|0〉U , |v〉), respec-
tively, then U and V become subspaces of W. If a vector |w〉 &isin;W belongs
to both U and V, then it can be written as both (|u〉, |0〉V ) and (|0〉U , |v〉),
i.e., (|u〉, |0〉V ) = (|0〉U , |v〉). But this can happen only if |u〉 = |0〉U and
|v〉 = |0〉V , or |w〉 = |0〉W . Thus, the only common vector in U and V is the
zero vector. Therefore,
</p>
<p>Proposition 2.1.19 Let U and V be any two vector spaces over R or C.
Then their Cartesian product W&equiv; U&times; V together with the operations de-</p>
<p/>
</div>
<div class="page"><p/>
<p>28 2 Vectors and Linear Maps
</p>
<p>fined in Eq. (2.4) becomes a vector space. Furthermore, W = U&oplus; V if U
and V are identified with vectors of the form (|u〉, |0〉V ) and (|0〉U , |v〉),
respectively.
</p>
<p>Let {|ai〉}Mi=1 be a basis of U and {|bj 〉}Nj=1 a basis of V. Define the vectors
{|ck〉}M+Nk=1 in W=U&oplus;V by
</p>
<p>|ck〉 =
(
|ak〉, |0〉V
</p>
<p>)
if 1 &le; k &le;M
</p>
<p>|ck〉 =
(
|0〉U , |bk&minus;M 〉
</p>
<p>)
if M + 1 &le; k &le;M +N.
</p>
<p>(2.5)
</p>
<p>Then {|ck〉}M+Nk=1 are linearly independent. In fact,
</p>
<p>M+N&sum;
</p>
<p>k=1
γk|ck〉 = |0〉W iff
</p>
<p>M&sum;
</p>
<p>k=1
γk
(
|ak〉, |0〉V
</p>
<p>)
+
</p>
<p>N&sum;
</p>
<p>j=1
γM+j
</p>
<p>(
|0〉U , |bj 〉
</p>
<p>)
=
(
|0〉U , |0〉V
</p>
<p>)
,
</p>
<p>or
(
</p>
<p>M&sum;
</p>
<p>k=1
γk|ak〉, |0〉V
</p>
<p>)
+
(
|0〉U ,
</p>
<p>N&sum;
</p>
<p>j=1
γM+j |bj 〉
</p>
<p>)
=
(
|0〉U , |0〉V
</p>
<p>)
,
</p>
<p>or
(
</p>
<p>M&sum;
</p>
<p>k=1
γk|ak〉,
</p>
<p>N&sum;
</p>
<p>j=1
γM+j |bj 〉
</p>
<p>)
=
(
|0〉U , |0〉V
</p>
<p>)
,
</p>
<p>or
</p>
<p>M&sum;
</p>
<p>k=1
γk|ak〉 = |0〉U and
</p>
<p>N&sum;
</p>
<p>j=1
γM+j |bj 〉 = |0〉V .
</p>
<p>Linear independence of {|ai〉}Mi=1 and {|bj 〉}Nj=1 imply that γk = 0 for 1 &le;
k &le;M +N .
</p>
<p>It is not hard to show that W = Span{|ck〉}M+Nk=1 . Hence, we have the
following
</p>
<p>Theorem 2.1.20 Let {|ai〉}Mi=1 be a basis of U and {|bj 〉}Nj=1 a basis of V.
The set of vectors {|ck〉}M+Nk=1 defined by Eq. (2.5) form a basis of the direct
sum W=U&oplus;V. In particular, W has dimension M +N .
</p>
<p>2.1.4 Tensor Product of Vector Spaces
</p>
<p>Direct sum is one way of constructing a new vector space out of two. There
is another procedure. Let U and V be vector spaces. On their Cartesian prod-</p>
<p/>
</div>
<div class="page"><p/>
<p>2.2 Inner Product 29
</p>
<p>uct, impose the scalar product and bilinearity conditions:
</p>
<p>α
(
|u〉, |v〉
</p>
<p>)
=
(
α|u〉, |v〉
</p>
<p>)
=
(
|u〉, α|v〉
</p>
<p>)
(
α1|u1〉 + α2|u2〉, |v〉
</p>
<p>)
= α1
</p>
<p>(
|u1〉, |v〉
</p>
<p>)
+ α2
</p>
<p>(
|u2〉, |v〉
</p>
<p>)
(
|u〉, β1|v1〉 + β2|v2〉
</p>
<p>)
= β1
</p>
<p>(
|u〉, |v1〉
</p>
<p>)
+ β2
</p>
<p>(
|u〉, |v2〉
</p>
<p>)
.
</p>
<p>(2.6)
</p>
<p>These properties turn U&times;V into a vector space called the tensor product of
U and V and denoted by U&otimes;V.3 The vectors in the tensor product space are
denoted by |u〉 &otimes; |v〉, (or occasionally by |uv〉). If {|ai〉}Mi=1 and {|bj 〉}Nj=1
are bases in U and V, respectively, and
</p>
<p>|u〉 =
M&sum;
</p>
<p>i=1
αi |ai〉 and |v〉 =
</p>
<p>N&sum;
</p>
<p>j=1
βj |bj 〉,
</p>
<p>then Eq. (2.6) yields
</p>
<p>|u〉 &otimes; |v〉 =
(
</p>
<p>M&sum;
</p>
<p>i=1
αi |ai〉
</p>
<p>)
&otimes;
(
</p>
<p>N&sum;
</p>
<p>j=1
βj |bj 〉
</p>
<p>)
=
</p>
<p>M&sum;
</p>
<p>i=1
</p>
<p>N&sum;
</p>
<p>j=1
αiβj |ai〉 &otimes; |bj 〉.
</p>
<p>Therefore, {|ai〉&otimes; |bj 〉} is a basis of U&otimes;V and dim(U&otimes;V)= dimUdimV.
From (2.6), we have
</p>
<p>|0〉U &otimes; |v〉 =
(
|u〉 &minus; |u〉
</p>
<p>)
&otimes; |v〉 = |u〉 &otimes; |v〉 &minus; |u〉 &otimes; |v〉 = |0〉U&otimes;V
</p>
<p>Similarly, |u〉 &otimes; |0〉V = |0〉U&otimes;V .
</p>
<p>2.2 Inner Product
</p>
<p>A vector space, as given by Definition 2.1.1, is too general and structureless
to be of much physical interest. One useful structure introduced on a vector
space is a scalar product. Recall that the scalar (dot) product of vectors in
the plane or in space is a rule that associates with two vectors a and b, a real
number. This association, denoted symbolically by g : V &times; V &rarr; R, with
g(a,b)= a &middot; b, is symmetric: g(a,b)= g(b,a), is linear in the first (and by
symmetry, in the second) factor:4
</p>
<p>g(αa + βb, c)= αg(a, c)+ βg(b, c) or (αa + βb) &middot; c = αa &middot; c + βb &middot; c,
</p>
<p>gives the &ldquo;length&rdquo; of a vector: |a|2 = g(a,a) = a &middot; a &ge; 0, and ensures that
the only vector with zero length5 is the zero vector: g(a,a)= 0 if and only
if a = 0.
</p>
<p>3A detailed discussion of tensor products and tensors in general is given in Chap. 26.
4A function that is linear in both of its arguments is called a bilinear function.
5In our present discussion, we are avoiding situations in which a nonzero vector can have
zero &ldquo;length&rdquo;. Such occasions arise in relativity, and we shall discuss them in Part VIII.</p>
<p/>
</div>
<div class="page"><p/>
<p>30 2 Vectors and Linear Maps
</p>
<p>We want to generalize these properties to abstract vector spaces for which
the scalars are complex numbers. A verbatim generalization of the forego-
ing properties, however, leads to a contradiction. Using the linearity in both
arguments and a nonzero |a〉, we obtain
</p>
<p>g
(
i|a〉, i|a〉
</p>
<p>)
= i2g
</p>
<p>(
|a〉, |a〉
</p>
<p>)
=&minus;g
</p>
<p>(
|a〉, |a〉
</p>
<p>)
. (2.7)
</p>
<p>Either the right-hand side (RHS) or left-hand side (LHS) of this equation
must be negative! But this is inconsistent with the positivity of the &ldquo;length&rdquo;
of a vector, which requires g(|a〉, |a〉) to be positive for all nonzero vectors,
including i|a〉. The source of the problem is the linearity in both arguments.
If we can change this property in such a way that one of the i&rsquo;s in Eq. (2.7)
comes out complex-conjugated, the problem may go away. This requires lin-
earity in one argument and complex-conjugate linearity in the other. Which
argument is to be complex-conjugate linear is a matter of convention. We
choose the first argument to be so.6 We thus have
</p>
<p>g
(
α|a〉 + β|b〉, |c〉
</p>
<p>)
= α&lowast;g
</p>
<p>(
|a〉, |c〉
</p>
<p>)
+ β&lowast;g
</p>
<p>(
|b〉, |c〉
</p>
<p>)
,
</p>
<p>where α&lowast; denotes the complex conjugate. Consistency then requires us
to change the symmetry property as well. In fact, we must demand that
g(|a〉, |b〉)= (g(|b〉, |a〉))&lowast;, from which the reality of g(|a〉, |a〉)&mdash;a neces-
sary condition for its positivity&mdash;follows immediately.
</p>
<p>The question of the existence of an inner product on a vector space is a
deep problem in higher analysis. Generally, if an inner product exists, there
may be many ways to introduce one on a vector space. However, as we
shall see in Sect. 2.2.4, a finite-dimensional vector space always has an inner
product and this inner product is unique.7 So, for all practical purposes we
can speak of the inner product on a finite-dimensional vector space, and asDirac &ldquo;bra,&rdquo; 〈 |, and
</p>
<p>&ldquo;ket&rdquo; | 〉, notation is used
for inner products.
</p>
<p>with the two- and three-dimensional cases, we can omit the letter g and use
a notation that involves only the vectors. There are several such notations in
use, but the one that will be employed in this book is the Dirac bra(c)ket
notation, whereby g(|a〉, |b〉) is denoted by 〈a|b〉. Using this notation, we
have
</p>
<p>Definition 2.2.1 The inner product of two vectors, |a〉 and |b〉, in a vector
space V is a complex number, 〈a|b〉 &isin;C, such thatinner product defined
</p>
<p>1. 〈a|b〉 = 〈b|a〉&lowast;
2. 〈a|(β|b〉 + γ |c〉)= β〈a|b〉 + γ 〈a|c〉
3. 〈a|a〉 &ge; 0, and 〈a|a〉 = 0 if and only if |a〉 = |0〉.
The last relation is called the positive definite property of the inner prod-positive definite, or
</p>
<p>Euclidean inner product
</p>
<p>6In some books, particularly in the mathematical literature, the second argument is chosen
to be conjugate linear.
7This uniqueness holds up to a certain equivalence of inner products that we shall not get
into here.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.2 Inner Product 31
</p>
<p>uct.8 A positive definite real inner product is also called a Euclidean inner
product, otherwise it is called pseudo-Euclidean.
</p>
<p>Note that linearity in the first argument is absent in the definition above,
because, as explained earlier, it would be inconsistent with the first property,
which expresses the &ldquo;symmetry&rdquo; of the inner product. The extra operation of
complex conjugation renders the true linearity in the first argument impos-
sible. Because of this complex conjugation, the inner product on a complex
vector space is not truly bilinear; it is commonly called sesquilinear or her-
mitian. sesquilinear or hermitian
</p>
<p>inner productA shorthand notation will be useful when dealing with the inner product
of a linear combination of vectors.
</p>
<p>Box 2.2.2 We write the LHS of the second equation in the definition
above as 〈a|βb+ γ c〉.
</p>
<p>This has the advantage of treating a linear combination as a single vector.
The second property then states that if the complex scalars happen to be in
a ket, they &ldquo;split out&rdquo; unaffected:
</p>
<p>〈a|βb+ γ c〉 = β〈a|b〉 + γ 〈a|c〉. (2.8)
</p>
<p>On the other hand, if the complex scalars happen to be in the first factor (the
bra), then they should be conjugated when they are &ldquo;split out&rdquo;:
</p>
<p>〈βb+ γ c|a〉 = β&lowast;〈b|a〉 + γ &lowast;〈c|a〉. (2.9)
</p>
<p>A vector space V on which an inner product is defined is called an inner
product space. As mentioned above, a finite-dimensional vector space can
always be turned into an inner product space.
</p>
<p>Example 2.2.3 In this example, we introduce some of the most common
inner products. The reader is urged to verify that in all cases, we indeed
have an inner product.
</p>
<p>&bull; Let |a〉, |b〉 &isin;Cn, with |a〉= (α1, α2, . . . , αn) and |b〉= (β1, β2, . . . , βn),
and define an inner product on Cn as
</p>
<p>natural inner product
</p>
<p>for Cn
</p>
<p>〈a|b〉 &equiv; α&lowast;1β1 + α&lowast;2β2 + &middot; &middot; &middot; + α&lowast;nβn =
n&sum;
</p>
<p>i=1
α&lowast;i βi .
</p>
<p>That this product satisfies all the required properties of an inner product
is easily checked. For example, if |b〉 = |a〉, we obtain 〈a|a〉 = |α1|2 +
|α2|2 + &middot; &middot; &middot; + |αn|2, which is clearly nonnegative.
</p>
<p>8The positive definiteness must be relaxed in the space-time of relativity theory, in which
nonzero vectors can have zero &ldquo;length&rdquo;.</p>
<p/>
</div>
<div class="page"><p/>
<p>32 2 Vectors and Linear Maps
</p>
<p>&bull; Similarly, for |a〉, |b〉 &isin; Rn the same definition (without the complex
conjugation) satisfies all the properties of an inner product.
</p>
<p>&bull; For |a〉, |b〉 &isin; C&infin; the natural inner product is defined as 〈a|b〉 =&sum;&infin;
i=1 α
</p>
<p>&lowast;
i βi . The question of the convergence of this infinite sum is the
</p>
<p>subject of Problem 2.18.
&bull; Let x(t), y(t) &isin; Pc[t], the space of all polynomials in t with complex
</p>
<p>coefficients. Define
weight function of an
</p>
<p>inner product defined in
</p>
<p>terms of integrals
</p>
<p>〈x|y〉 &equiv;
&int; b
</p>
<p>a
</p>
<p>w(t)x&lowast;(t)y(t) dt, (2.10)
</p>
<p>where a and b are real numbers&mdash;or infinity&mdash;for which the integral
exists, and w(t), called the weight function, is a real-valued, continu-
ous function that is always strictly positive in the interval (a, b). Then
Eq. (2.10) defines an inner product. Depending on the weight function
w(t), there can be many different inner products defined on the infinite-
dimensional space Pc[t].
</p>
<p>&bull; Let f,g &isin;C(a, b) and define their inner product by
</p>
<p>〈f |g〉 &equiv;
&int; b
</p>
<p>a
</p>
<p>w(x)f &lowast;(x)g(x) dx.
</p>
<p>It is easily shown that 〈f |g〉 satisfies all the requirements of the inner
product if, as in the previous case, the weight function w(x) is always
positive in the interval (a, b). This is called the standard inner product
on C(a, b).
</p>
<p>natural inner product for
</p>
<p>complex functions
</p>
<p>2.2.1 Orthogonality
</p>
<p>The vectors of analytic geometry and calculus are often expressed in terms
of unit vectors along the axes, i.e., vectors that are of unit length and per-
pendicular to one another. Such vectors are also important in abstract inner
product spaces.
</p>
<p>Definition 2.2.4 Vectors |a〉, |b〉 &isin; V are orthogonal if 〈a|b〉 = 0. A nor-orthogonality defined
mal vector, or normalized vector, |e〉 is one for which 〈e|e〉 = 1. A basis
B = {|ei〉}Ni=1 in an N -dimensional vector space V is an orthonormal basisorthonormal basis
if
</p>
<p>〈ei |ej 〉 = δij &equiv;
{
</p>
<p>1 if i = j,
0 if i �= j,
</p>
<p>(2.11)
</p>
<p>where δij , defined by the last equality, is called the Kronecker delta.Kronecker delta
</p>
<p>Example 2.2.5 Let U and V be inner product vector spaces. Let W =
U &oplus; V. Then an inner product can be defined on W in terms of those on
U and V. In fact, it can be easily shown that if |wi〉 = (|ui〉, |vi〉), i = 1,2,
then
</p>
<p>〈w1|w2〉 = 〈u1|u2〉 + 〈v1|v2〉 (2.12)</p>
<p/>
</div>
<div class="page"><p/>
<p>2.2 Inner Product 33
</p>
<p>Fig. 2.1 The essence of the Gram&ndash;Schmidt process is neatly illustrated by the process
in two dimensions. This figure, depicts the stages of the construction of two orthonormal
vectors
</p>
<p>defines an inner product on W. Moreover, with the identification
</p>
<p>U=
{(
|u〉, |0〉V
</p>
<p>)
| |u〉 &isin;U
</p>
<p>}
and V=
</p>
<p>{(
|0〉U , |v〉
</p>
<p>)
| |v〉 &isin; V
</p>
<p>}
,
</p>
<p>any vector in U is orthogonal to any vector in V.
</p>
<p>Example 2.2.6 Here are examples of orthonormal bases:
</p>
<p>&bull; The standard basis of Rn (or Cn)
</p>
<p>|e1〉 = (1,0, . . . ,0), |e2〉 = (0,1, . . . ,0), . . . , |en〉 = (0,0, . . . ,1)
</p>
<p>is orthonormal under the usual inner product of those spaces.
&bull; Let |ek〉 = eikx/
</p>
<p>&radic;
2π be functions in C(0,2π) with w(x)= 1. Then
</p>
<p>〈ek|ek〉 =
1
</p>
<p>2π
</p>
<p>&int; 2π
</p>
<p>0
e&minus;ikxeikx dx = 1,
</p>
<p>and for l �= k,
</p>
<p>〈el |ek〉 =
1
</p>
<p>2π
</p>
<p>&int; 2π
</p>
<p>0
e&minus;ilxeikx dx = 1
</p>
<p>2π
</p>
<p>&int; 2π
</p>
<p>0
ei(k&minus;l)x dx = 0.
</p>
<p>Thus, 〈el |ek〉 = δlk .
</p>
<p>2.2.2 The Gram-Schmidt Process
</p>
<p>It is always possible to convert&mdash;by taking appropriate linear combinations&mdash;
any basis in V into an orthonormal basis. A process by which this may
be accomplished is called Gram&ndash;Schmidt orthonormalization. Consider
a basis B = {|ai〉}Ni=1. We intend to take linear combinations of |ai〉
in such a way that the resulting vectors are orthonormal. First, we let
|e1〉 = |a1〉/
</p>
<p>&radic;〈a1|a1〉 and note that 〈e1|e1〉 = 1. If we subtract from |a2〉 The Gram&ndash;Schmidt
process explainedits projection along |e1〉, we obtain a vector that is orthogonal to |e1〉 (see
</p>
<p>Fig. 2.1).</p>
<p/>
</div>
<div class="page"><p/>
<p>34 2 Vectors and Linear Maps
</p>
<p>Fig. 2.2 Once the orthonormal vectors in the plane of two vectors are obtained, the third
orthonormal vector is easily constructed
</p>
<p>Calling the resulting vector |e&prime;2〉, we have |e&prime;2〉 = |a2〉&minus;〈e1|a2〉|e1〉, which
can be written more symmetrically as |e&prime;2〉 = |a2〉 &minus; |e1〉〈e1|a2〉. Clearly,
this vector is orthogonal to |e1〉. In order to normalize |e&prime;2〉, we divide it
by
</p>
<p>&radic;
〈e&prime;2|e&prime;2〉. Then |e2〉 = |e&prime;2〉/
</p>
<p>&radic;
〈e&prime;2|e&prime;2〉 will be a normal vector orthogonal
</p>
<p>to |e1〉. Subtracting from |a3〉 its projections along the first and second unit
vectors obtained so far will give the vector
</p>
<p>∣∣e&prime;3
&rang;
= |a3〉 &minus; |e1〉〈e1|a3〉 &minus; |e2〉〈e2|a3〉 = |a3〉 &minus;
</p>
<p>2&sum;
</p>
<p>i=1
|ei〉〈ei |a3〉,
</p>
<p>which is orthogonal to both |e1〉 and |e2〉 (see Fig. 2.2):
</p>
<p>〈e1|e&prime;3〉 = 〈e1|a3〉 &minus;
=1︷ ︸︸ ︷
</p>
<p>〈e1|e1〉〈e1|a3〉 &minus;
=0︷ ︸︸ ︷
</p>
<p>〈e1|e2〉〈e2|a3〉 = 0.
</p>
<p>Similarly, 〈e2|e&prime;3〉 = 0.
</p>
<p>Historical Notes
</p>
<p>Erhard Schmidt (1876&ndash;1959) obtained his doctorate under the supervision of David
Hilbert. His main interest was in integral equations and Hilbert spaces. He is the
&ldquo;Schmidt&rdquo; of the Gram&ndash;Schmidt orthogonalization process, which takes a basis of
a space and constructs an orthonormal one from it. (Laplace had presented a special case
of this process long before Gram or Schmidt.)
In 1908 Schmidt worked on infinitely many equations in infinitely many unknowns, in-
troducing various geometric notations and terms that are still in use for describing spaces
of functions. Schmidt&rsquo;s ideas were to lead to the geometry of Hilbert spaces. This was
</p>
<p>Erhard Schmidt
</p>
<p>1876&ndash;1959
</p>
<p>motivated by the study of integral equations (see Chap. 18) and an attempt at their ab-
straction.
Earlier, Hilbert regarded a function as given by its Fourier coefficients. These satisfy
the condition that
</p>
<p>&sum;&infin;
k=1 a
</p>
<p>2
k is finite. He introduced sequences of real numbers {xn}
</p>
<p>such that
&sum;&infin;
</p>
<p>n=1 x
2
n is finite. Riesz and Fischer showed that there is a one-to-one cor-
</p>
<p>respondence between square-integrable functions and square-summable sequences of
their Fourier coefficients. In 1907 Schmidt and Fr&eacute;chet showed that a consistent theory
could be obtained if the square-summable sequences were regarded as the coordinates
of points in an infinite-dimensional space that is a generalization of n-dimensional Eu-
clidean space. Thus functions can be regarded as points of a space, now called a Hilbert
space.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.2 Inner Product 35
</p>
<p>In general, if we have calculated m orthonormal vectors |e1〉, . . . , |em〉,
with m&lt;N , then we can find the next one using the following relations:
</p>
<p>∣∣e&prime;m+1
&rang;
= |am+1〉 &minus;
</p>
<p>m&sum;
</p>
<p>i=1
|ei〉〈ei |am+1〉,
</p>
<p>|em+1〉 =
|e&prime;m+1〉&radic;
</p>
<p>〈e&prime;m+1|e&prime;m+1〉
.
</p>
<p>(2.13)
</p>
<p>Even though we have been discussing finite-dimensional vector spaces, the
process of Eq. (2.13) can continue for infinite-dimensions as well. The
reader is asked to pay attention to the fact that, at each stage of the Gram&ndash;
Schmidt process, one is taking linear combinations of the original vectors.
</p>
<p>2.2.3 The Schwarz Inequality
</p>
<p>Let us now consider an important inequality that is valid in both finite and
infinite dimensions and whose restriction to two and three dimensions is
equivalent to the fact that the cosine of the angle between two vectors is
always less than one.
</p>
<p>Theorem 2.2.7 For any pair of vectors |a〉, |b〉 in an inner product space V,
the Schwarz inequality holds: 〈a|a〉〈b|b〉 &ge; |〈a|b〉|2. Equality holds when Schwarz inequality
|a〉 is proportional to |b〉.
</p>
<p>Proof Let |c〉 = |b〉&minus; (〈a|b〉/〈a|a〉)|a〉, and note that 〈a|c〉 = 0. Write |b〉 =
(〈a|b〉/〈a|a〉)|a〉 + |c〉 and take the inner product of |b〉 with itself:
</p>
<p>〈b|b〉 =
∣∣∣∣
〈a|b〉
〈a|a〉
</p>
<p>∣∣∣∣
2
</p>
<p>〈a|a〉 + 〈c|c〉 = |〈a|b〉|
2
</p>
<p>〈a|a〉 + 〈c|c〉.
</p>
<p>Since 〈c|c〉 &ge; 0, we have
</p>
<p>〈b|b〉 &ge; |〈a|b〉|
2
</p>
<p>〈a|a〉 &rArr; 〈a|a〉〈b|b〉 &ge;
∣∣〈a|b〉
</p>
<p>∣∣2.
</p>
<p>Equality holds iff 〈c|c〉 = 0, i.e., iff |c〉 = 0. From the definition of |c〉, we
conclude that for the equality to hold, |a〉 and |b〉 must be proportional. �
</p>
<p>Notice the power of abstraction: We have derived the Schwarz inequality
solely from the basic assumptions of inner product spaces independent of
the specific nature of the inner product. Therefore, we do not have to prove
the Schwarz inequality every time we encounter a new inner product space.
</p>
<p>Historical Notes
</p>
<p>Karl Herman Amandus Schwarz (1843&ndash;1921) the son of an architect, was born in
what is now Sobiecin, Poland. After gymnasium, Schwarz studied chemistry in Berlin for
a time before switching to mathematics, receiving his doctorate in 1864. He was greatly
influenced by the reigning mathematicians in Germany at the time, especially Kummer</p>
<p/>
</div>
<div class="page"><p/>
<p>36 2 Vectors and Linear Maps
</p>
<p>and Weierstrass. The lecture notes that Schwarz took while attending Weierstrass&rsquo;s lec-
</p>
<p>Karl Herman Amandus
</p>
<p>Schwarz 1843&ndash;1921
</p>
<p>tures on the integral calculus still exist. Schwarz received an initial appointment at Halle
and later appointments in Zurich and G&ouml;ttingen before being named as Weierstrass&rsquo;s suc-
cessor at Berlin in 1892. These later years, filled with students and lectures, were not
Schwarz&rsquo;s most productive, but his early papers assure his place in mathematics history.
Schwarz&rsquo;s favorite tool was geometry, which he soon turned to the study of analysis. He
conclusively proved some of Riemann&rsquo;s results that had been previously (and justifiably)
challenged. The primary result in question was the assertion that every simply connected
region in the plane could be conformally mapped onto a circular area. From this effort
came several well-known results now associated with Schwarz&rsquo;s name, including the prin-
ciple of reflection and Schwarz&rsquo;s lemma. He also worked on surfaces of minimal area, the
branch of geometry beloved by all who dabble with soap bubbles.
Schwarz&rsquo;s most important work, for the occasion of Weierstrass&rsquo;s seventieth birthday,
again dealt with minimal area, specifically whether a minimal surface yields a minimal
area. Along the way, Schwarz demonstrated second variation in a multiple integral, con-
structed a function using successive approximation, and demonstrated the existence of a
&ldquo;least&rdquo; eigenvalue for certain differential equations. This work also contained the most
famous inequality in mathematics, which bears his name.
Schwarz&rsquo;s success obviously stemmed from a matching of his aptitude and training to the
mathematical problems of the day. One of his traits, however, could be viewed as either
positive or negative&mdash;his habit of treating all problems, whether trivial or monumental,
with the same level of attention to detail. This might also at least partly explain the decline
in productivity in Schwarz&rsquo;s later years.
Schwarz had interests outside mathematics, although his marriage was a mathematical
one, since he married Kummer&rsquo;s daughter. Outside mathematics he was the captain of the
local voluntary fire brigade, and he assisted the stationmaster at the local railway station
by closing the doors of the trains!
</p>
<p>2.2.4 Length of a Vector
</p>
<p>In dealing with objects such as directed line segments in the plane or in
space, the intuitive idea of the length of a vector is used to define the dot
product. However, sometimes it is more convenient to introduce the inner
product first and then define the length, as we shall do now.
</p>
<p>Definition 2.2.8 The norm, or length, of a vector |a〉 in an inner productnorm of a vector defined
space is denoted by ‖a‖ and defined as ‖a‖ &equiv;&radic;〈a|a〉. We use the notation
‖αa + βb‖ for the norm of the vector α|a〉 + β|b〉.
</p>
<p>One can easily show that the norm has the following properties:
</p>
<p>1. The norm of the zero vector is zero: ‖0‖ = 0.
2. ‖a‖ &ge; 0, and ‖a‖ = 0 if and only if |a〉 = |0〉.
3. ‖αa‖ = |α|‖a‖ for any9 complex α.
4. ‖a + b‖ &le; ‖a‖ + ‖b‖. This property is called the triangle inequality.triangle inequality
</p>
<p>Any function on a vector space satisfying the four properties above is
called a norm, and the vector space on which a norm is defined is called a
normed linear space. One does not need an inner product to have a norm.normed linear space
</p>
<p>One can introduce the idea of the &ldquo;distance&rdquo; between two vectors in
a normed linear space. The distance between |a〉 and |b〉&mdash;denoted by
d(a, b)&mdash;is simply the norm of their difference: d(a, b) &equiv; ‖a &minus; b‖. It can
</p>
<p>natural distance in a
</p>
<p>normed linear space
</p>
<p>9The first property follows from this by letting α = 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.2 Inner Product 37
</p>
<p>be readily shown that this has all the properties one expects of the distance
(or metric) function introduced in Chap. 1. However, one does not need a
normed space to define distance. For example, as explained in Chap. 1, one
can define the distance between two points on the surface of a sphere, but the
addition of two points on a sphere&mdash;a necessary operation for vector space
structure&mdash;is not defined. Thus the points on a sphere form a metric space,
but not a vector space.
</p>
<p>Inner product spaces are automatically normed spaces, but the converse
is not, in general, true: There are normed spaces, i.e., spaces satisfying prop-
erties 1&ndash;4 above that cannot be promoted to inner product spaces. However,
if the norm satisfies the parallelogram law,
</p>
<p>parallelogram law
</p>
<p>‖a + b‖2 + ‖a &minus; b‖2 = 2‖a‖2 + 2‖b‖2, (2.14)
</p>
<p>then one can define
</p>
<p>〈a|b〉 &equiv; 1
4
</p>
<p>{
‖a + b‖2 &minus; ‖a &minus; b‖2 &minus; i
</p>
<p>(
‖a + ib‖2 &minus; ‖a &minus; ib‖2
</p>
<p>)}
(2.15)
</p>
<p>and show that it is indeed an inner product. In fact, we have (see [Frie 82,
pp. 203&ndash;204] for a proof) the following theorem.
</p>
<p>Theorem 2.2.9 A normed linear space is an inner product space if and only
if the norm satisfies the parallelogram law.
</p>
<p>Now consider any N -dimensional vector space V. Choose a basis
{|ai〉}Ni=1 in V, and for any vector |a〉 whose components are {αi}Ni=1 in
this basis, define
</p>
<p>‖a‖2 &equiv;
N&sum;
</p>
<p>i=1
|αi |2.
</p>
<p>The reader may check that this defines a norm, and that the norm satisfies
the parallelogram law. From Theorem 2.2.9 we have the following:
</p>
<p>Theorem 2.2.10 Every finite-dimensional vector space can be turned into
an inner product space.
</p>
<p>Example 2.2.11 Let the space be Cn. The natural inner product of Cn gives
Cn has many different
</p>
<p>distance functions
</p>
<p>rise to a norm, which, for the vector |a〉 = (α1, α2, . . . , αn) is
</p>
<p>‖a‖ =
&radic;
〈a|a〉 =
</p>
<p>&radic;&radic;&radic;&radic;
n&sum;
</p>
<p>i=1
|αi |2.
</p>
<p>This norm yields the following distance between |a〉 and |b〉 = (β1, β2,
. . . , βn):
</p>
<p>d(a, b)= ‖a &minus; b‖ =
&radic;
〈a &minus; b|a &minus; b〉 =
</p>
<p>&radic;&radic;&radic;&radic;
n&sum;
</p>
<p>i=1
|αi &minus; βi |2.</p>
<p/>
</div>
<div class="page"><p/>
<p>38 2 Vectors and Linear Maps
</p>
<p>One can define other norms, such as ‖a‖1 &equiv;
&sum;n
</p>
<p>i=1 |αi |, which has all the
required properties of a norm, and leads to the distance
</p>
<p>d1(a, b)= ‖a &minus; b‖1 =
n&sum;
</p>
<p>i=1
|αi &minus; βi |.
</p>
<p>Another norm defined on Cn is given by
</p>
<p>‖a‖p &equiv;
(
</p>
<p>n&sum;
</p>
<p>i=1
|αi |p
</p>
<p>)1/p
,
</p>
<p>where p is a positive integer. It is proved in higher mathematical analysis
that ‖ &middot; ‖p has all the properties of a norm. (The nontrivial part of the proof
is to verify the triangle inequality.) The associated distance is
</p>
<p>dp(a, b)= ‖a &minus; b‖p =
(
</p>
<p>n&sum;
</p>
<p>i=1
|αi &minus; βi |p
</p>
<p>)1/p
.
</p>
<p>The other two norms introduced above are special cases, for p = 2 and
p = 1.
</p>
<p>2.3 Linear Maps
</p>
<p>We have made progress in enriching vector spaces with structures such as
norms and inner products. However, this enrichment, although important,
will be of little value if it is imprisoned in a single vector space. We would
like to give vector space properties freedom of movement, so they can go
from one space to another. The vehicle that carries these properties is a lin-
ear map or linear transformation which is the subject of this section. First
it is instructive to review the concept of a map (discussed in Chap. 1) by
considering some examples relevant to the present discussion.
</p>
<p>Example 2.3.1 The following are a few familiar examples of mappings.
</p>
<p>1. Let f :R&rarr;R be given by f (x)= x2.
2. Let g :R2 &rarr;R be given by g(x, y)= x2 + y2 &minus; 4.
3. Let F : R2 &rarr; C be given by F(x, y) = U(x,y) + iV (x, y), where
</p>
<p>U :R2 &rarr;R and V :R2 &rarr;R.
4. Let T :R&rarr;R2 be given by T (t)= (t + 3,2t &minus; 5).
5. Motion of a point particle in space can be considered as a mapping
</p>
<p>M : [a, b] &rarr; R3, where [a, b] is an interval of the real line. For each
t &isin; [a, b], we define M(t) = (x(t), y(t), z(t)), where x(t), y(t), and
z(t) are real-valued functions of t . If we identify t with time, which is
assumed to have a value in the interval [a, b], then M(t) describes the
path of the particle as a function of time, and a and b are the beginning
and the end of the motion, respectively.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.3 Linear Maps 39
</p>
<p>Let us consider an arbitrary mapping F : V &rarr; W from a vector space
V to another vector space W. It is assumed that the two vector spaces are
over the same scalars, say C. Consider |a〉 and |b〉 in V and |x〉 and |y〉 in W
such that F(|a〉)= |x〉 and F(|b〉)= |y〉. In general, F does not preserve the
vector space structure. That is, the image of a linear combination of vectors
is not the same as the linear combination of the images:
</p>
<p>F
(
α|a〉 + β|b〉
</p>
<p>)
�= αF
</p>
<p>(
|x〉
</p>
<p>)
+ βF
</p>
<p>(
|y〉
</p>
<p>)
.
</p>
<p>This is the case for all the mappings of Example 2.3.1. There are many appli-
cations in which the preservation of the vector space structure (preservation
of the linear combination) is desired.
</p>
<p>Definition 2.3.2 A linear map (or transformation) from the complex vec-
linear map (or
</p>
<p>transformation), linear
</p>
<p>operator,
</p>
<p>endomorphism
tor space V to the complex vector space W is a mapping T : V&rarr;W such
that
</p>
<p>T
(
α|a〉 + β|b〉
</p>
<p>)
= αT
</p>
<p>(
|a〉
</p>
<p>)
+ βT
</p>
<p>(
|b〉
</p>
<p>)
&forall;|a〉, |b〉 &isin; V and α,β &isin;C.
</p>
<p>A linear transformation T : V &rarr; V is called an endomorphism of V or a
linear operator on V. The action of a linear transformation on a vector is
written without the parentheses: T(|a〉)&equiv; T|a〉.
</p>
<p>The same definition applies to real vector spaces. Note that the defini-
tion demands that both vector spaces have the same set of scalars: The
same scalars must multiply vectors in V on the LHS and those in W on
the RHS.
</p>
<p>The set of linear maps from V to W is denoted by L(V,W), and this
set happens to be a vector space. The zero transformation, 0, is defined
to take every vector in V to the zero vector of W. The sum of two linear
transformations T and U is the linear transformation T + U, whose action
on a vector |a〉 &isin; V is defined to be (T + U)|a〉 &equiv; T|a〉 + U|a〉. Similarly,
define αT by (αT)|a〉 &equiv; α(T|a〉) = αT|a〉. The set of endomorphisms of V
is denoted by L(V) or End(V) rather than L(V,V). We summarize these
</p>
<p>L(V,W) is a vector
</p>
<p>space
</p>
<p>observations in
</p>
<p>Box 2.3.3 L(V,W) is a vector space. In particular, so is the set of
endomorphisms of a single vector space L(V)&equiv; End(V)&equiv;L(V,V).
</p>
<p>Definition 2.3.4 Let V and U be inner product spaces. A linear map T :
V&rarr;U is called an isometric map if10
</p>
<p>〈Ta|Tb〉 = 〈a|b〉, &forall;|a〉, |b〉 &isin; V.
</p>
<p>10It is convenient here to use the notation |Ta〉 for T|a〉. This would then allow us to write
the dual (see below) of the vector as 〈Ta|, emphasizing that it is indeed the bra associated
with T|a〉.</p>
<p/>
</div>
<div class="page"><p/>
<p>40 2 Vectors and Linear Maps
</p>
<p>If U= V, then T is called a linear isometry or simply an isometry of V. It is
common to call an isometry of a complex (real) V a unitary (orthogonal)
</p>
<p>isometry
</p>
<p>operator
</p>
<p>Example 2.3.5 The following are some examples of linear operators in var-
ious vector spaces. The proofs of linearity are simple in all cases and are left
as exercises for the reader.
</p>
<p>1. Let V be a one-dimensional space (e.g., V=C). Then any linear endo-
morphism T of V is of the form T|x〉 = α|x〉 with α a scalar. In partic-
ular, if T is an isometry, then |α|2 = 1. If V= R and T is an isometry,
then T|x〉 = &plusmn;|x〉.
</p>
<p>2. Let π be a permutation (shuffling) of the integers {1,2, . . . , n}. If |x〉 =
(η1, η2, . . . , ηn) is a vector in Cn, we can write
</p>
<p>Aπ |x〉 = (ηπ(1), ηπ(2), . . . , ηπ(n)).
</p>
<p>Then Aπ is a linear operator.
3. For any |x〉 &isin; Pc[t], with x(t) =&sum;nk=0 αktk , write |y〉 = D|x〉, where
</p>
<p>|y〉 is defined as y(t)=&sum;nk=1 kαktk&minus;1. Then D is a linear operator, the
derivative operator.derivative operator
</p>
<p>4. For every |x〉 &isin; Pc[t], with x(t)=&sum;nk=0 αktk , write |y〉 = S|x〉, where
|y〉 &isin; Pc[t] is defined as y(t) = &sum;nk=0[αk/(k + 1)]tk+1. Then S is a
linear operator, the integration operator.integration operator
</p>
<p>5. Let Cn(a, b) be the set of real-valued functions defined in the inter-
val [a, b] whose first n derivatives exist and are continuous. For any
|f 〉 &isin; Cn(a, b) define |u〉 = G|f 〉, with u(t) = g(t)f (t) and g(t) a
fixed function in Cn(a, b). Then G is linear. In particular, the oper-
ation of multiplying by t , whose operator is denoted by T, is lin-
ear.
</p>
<p>An immediate consequence of Definition 2.3.2 is the following:
</p>
<p>Box 2.3.6 Two linear transformations T : V &rarr; W and U : V &rarr; W
are equal if and only if T|ai〉 = U|ai〉 for all |ai〉 in some basis of V.
Thus, a linear transformation is uniquely determined by its action on
some basis of its domain space.
</p>
<p>The equality in this box is simply the set-theoretic equality of maps dis-
cussed in Chap. 1.
</p>
<p>The equality of operators can also be established by other, more conve-
nient, methods when an inner product is defined on the vector space. The
following two theorems contain the essence of these alternatives.
</p>
<p>Theorem 2.3.7 An endomorphism T of an inner product space is 0 if and
only if 〈b|T|a〉 &equiv; 〈b|Ta〉 = 0 for all |a〉 and |b〉.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.3 Linear Maps 41
</p>
<p>Proof Clearly, if T= 0 then 〈b|T|a〉 = 0. Conversely, if 〈b|T|a〉 = 0 for all
|a〉 and |b〉, then, choosing |b〉 = T|a〉 = |Ta〉, we obtain
</p>
<p>〈Ta|Ta〉 = 0 &forall;|a〉 &hArr; T|a〉 = 0 &forall;|a〉 &hArr; T= 0
</p>
<p>by positive definiteness of the inner product. �
</p>
<p>Theorem 2.3.8 A linear operator T on an inner product space is 0 if and
only if 〈a|T|a〉 = 0 for all |a〉.
</p>
<p>Proof Obviously, if T = 0, then 〈a|T|a〉 = 0. Conversely, choose a vector
α|a〉+β|b〉, sandwich T between this vector and its bra, and rearrange terms
to obtain what is known as the polarization identity polarization identity
</p>
<p>α&lowast;β〈a|T|b〉 + αβ&lowast;〈b|T|a〉 = 〈αa + βb|T|αa + βb〉
&minus; |α|2〈a|T|a〉 &minus; |β|2〈b|T|b〉.
</p>
<p>According to the assumption of the theorem, the RHS is zero. Thus, if we
let α = β = 1 we obtain 〈a|T|b〉 + 〈b|T|a〉 = 0. Similarly, with α = 1 and
β = i we get i〈a|T|b〉&minus; i〈b|T|a〉 = 0. These two equations give 〈a|T|b〉 = 0
for all |a〉, |b〉. By Theorem 2.3.7, T= 0. �
</p>
<p>To show that two operators U and T on an inner product space are equal,
one can either have them act on an arbitrary vector and show that they give
the same result, or one verifies that U&minus; T is the zero operator by means of
one of the theorems above. Equivalently, one shows that 〈a|T|b〉 = 〈a|U|b〉
or 〈a|T|a〉 = 〈a|U|a〉 for all |a〉, |b〉.
</p>
<p>2.3.1 Kernel of a Linear Map
</p>
<p>It follows immediately from Definition 2.3.2 that the image of the zero vec-
tor in V is the zero vector in W. This is not true for a general mapping, but
it is necessarily true for a linear mapping. As the zero vector of V is mapped
onto the zero vector of W, other vectors of V may also be dragged along. In
fact, we have the following theorem.
</p>
<p>Theorem 2.3.9 The set of vectors in V that are mapped onto the zero vector
of W under the linear transformation T : V &rarr; W form a subspace of V
called the kernel, or null space, of T and denoted by kerT. kernel of a linear
</p>
<p>transformation
</p>
<p>Proof The proof is left as an exercise. �
</p>
<p>The dimension of kerT is also called the nullity of V.
nullityThe proof of the following is also left as an exercise.
</p>
<p>Theorem 2.3.10 The range T(V) of a linear map T : V&rarr;W is a subspace rank of a linear
transformationof W. The dimension of T(V) is called the rank of T.</p>
<p/>
</div>
<div class="page"><p/>
<p>42 2 Vectors and Linear Maps
</p>
<p>Theorem 2.3.11 A linear transformation is 1&ndash;1 (injective) iff its kernel is
zero.
</p>
<p>Proof The &ldquo;only if&rdquo; part is trivial. For the &ldquo;if&rdquo; part, suppose T|a1〉 = T|a2〉;
then linearity of T implies that T(|a1〉 &minus; |a2〉) = 0. Since kerT = 0,11 we
must have |a1〉 = |a2〉. �
</p>
<p>Theorem 2.3.12 A linear isometric map is injective.
</p>
<p>Proof Let T : V&rarr;U be a linear isometry. Let |a〉 &isin; kerT, then
</p>
<p>〈a|a〉 = 〈Ta|Ta〉 = 〈0|0〉 = 0.
</p>
<p>Therefore, |a〉 = |0〉. By Theorem 2.3.11, T is injective. �
</p>
<p>Suppose we start with a basis of kerT and add enough linearly inde-
pendent vectors to it to get a basis for V. Without loss of generality, let us
assume that the first n vectors in this basis form a basis of kerT. So let
B = {|a1〉, |a2〉, . . . , |aN 〉} be a basis for V and B &prime; = {|a1〉, |a2〉, . . . , |an〉} be
a basis for kerT. Here N = dimV and n = dim kerT. It is straightforward
to show that {T|an+1〉, . . . ,T|aN 〉} is a basis for T(V). We therefore have the
following result (see also the end of this subsection).
</p>
<p>Theorem 2.3.13 Let T : V&rarr;W be a linear transformation. Then12
dimension theorem
</p>
<p>dimV= dim kerT+ dimT(V)
</p>
<p>This theorem is called the dimension theorem. One of its consequences
is that an injective endomorphism is automatically surjective, and vice versa:
</p>
<p>Proposition 2.3.14 An endomorphism of a finite-dimensional vector space
is bijective if it is either injective or surjective.
</p>
<p>The dimension theorem is obviously valid only for finite-dimensional
vector spaces. In particular, neither surjectivity nor injectivity implies bijec-
tivity for infinite-dimensional vector spaces.
</p>
<p>Example 2.3.15 Let us try to find the kernel of T :R4 &rarr;R3 given by
</p>
<p>T(x1, x2, x3, x4)
</p>
<p>= (2x1 + x2 + x3 &minus; x4, x1 + x2 + 2x3 + 2x4, x1 &minus; x3 &minus; 3x4).
</p>
<p>11Since kerT is a set, we should write the equality as kerT= {|0〉}, or at least as kerT=
|0〉. However, when there is no danger of confusion, we set {|0〉} = |0〉 = 0.
12Recall that the dimension of a vector space depends on the scalars used in that space.
Although we are dealing with two different vector spaces here, since they are both over
the same set of scalars (complex or real), no confusion in the concept of dimension arises.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.3 Linear Maps 43
</p>
<p>We must look for (x1, x2, x3, x4) such that T(x1, x2, x3, x4)= (0,0,0), or
</p>
<p>2x1 + x2 + x3 &minus; x4 = 0,
x1 + x2 + 2x3 + 2x4 = 0,
</p>
<p>x1 &minus; x3 &minus; 3x4 = 0.
</p>
<p>The &ldquo;solution&rdquo; to these equations is x1 = x3 + 3x4 and x2 = &minus;3x3 &minus; 5x4.
Thus, to be in kerT, a vector in R4 must be of the form
</p>
<p>(x3 + 3x4,&minus;3x3 &minus; 5x4, x3, x4)= x3(1,&minus;3,1,0)+ x4(3,&minus;5,0,1),
</p>
<p>where x3 and x4 are arbitrary real numbers. It follows that kerT consists
of vectors that can be written as linear combinations of the two linearly
independent vectors (1,&minus;3,1,0) and (3,&minus;5,0,1). Therefore, dim kerT =
2. Theorem 2.3.13 then says that dimT(V) = 2; that is, the range of T is
two-dimensional. This becomes clear when one notes that
</p>
<p>T(x1, x2, x3, x4)
</p>
<p>= (2x1 + x2 + x3 &minus; x4)(1,0,1)+ (x1 + x2 + 2x3 + 2x4)(0,1,&minus;1),
</p>
<p>and therefore T(x1, x2, x3, x4), an arbitrary vector in the range of T, is a
linear combination of only two linearly independent vectors, (1,0,1) and
(0,1,&minus;1).
</p>
<p>2.3.2 Linear Isomorphism
</p>
<p>In many cases, two vector spaces may &ldquo;look&rdquo; different, while in reality they
are very much the same. For example, the set of complex numbers C is a
two-dimensional vector space over the reals, as is R2. Although we call the
vectors of these two spaces by different names, they have very similar prop-
erties. This notion of &ldquo;similarity&rdquo; is made precise in the following definition.
</p>
<p>Definition 2.3.16 A vector space V is said to be isomorphic to another
isomorphism and
</p>
<p>automorphism
vector space W, and written V &sim;= W, if there exists a bijective linear map
T : V&rarr;W. Then T is called an isomorphism.13 A bijective linear map of
V onto itself is called an automorphism of V. An automorphism is also
called an invertible linear map. The set of automorphisms of V is denoted
by GL(V).
</p>
<p>An immediate consequence of the injectivity of an isometry and Propo-
sition 2.3.14 is the following:
</p>
<p>13The word &ldquo;isomorphism&rdquo;, as we shall see, is used in conjunction with many algebraic
structures. To distinguish them, qualifiers need to be used. In the present context, we speak
of linear isomorphism. We shall use qualifiers when necessary. However, the context
usually makes the meaning of isomorphism clear.</p>
<p/>
</div>
<div class="page"><p/>
<p>44 2 Vectors and Linear Maps
</p>
<p>Proposition 2.3.17 An isometry of a finite-dimensional vector space is an
automorphism of that vector space.
</p>
<p>For all practical purposes, two isomorphic vector spaces are different
manifestations of the &ldquo;same&rdquo; vector space. In the example discussed above,
the correspondence T :C&rarr;R2, with T(x+ iy)= (x, y), establishes an iso-
morphism between the two vector spaces. It should be emphasized that only
as vector spaces are C and R2 isomorphic. If we go beyond the vector space
structures, the two sets are quite different. For example, C has a natural mul-
tiplication for its elements, but R2 does not. The following three theorems
give a working criterion for isomorphism. The proofs are simple and left to
the reader.
</p>
<p>Theorem 2.3.18 A linear surjective map T : V&rarr;W is an isomorphism if
and only if its nullity is zero.
</p>
<p>Theorem 2.3.19 An injective linear transformation T : V&rarr;W carries lin-
early independent sets of vectors onto linearly independent sets of vectors.
</p>
<p>Theorem 2.3.20 Two finite-dimensional vector spaces are isomorphic if
and only if they have the same dimension.
</p>
<p>A consequence of Theorem 2.3.20 is that all N -dimensional vector
only twoN -dimensional
</p>
<p>vector spaces
spaces over R are isomorphic to RN and all complex N -dimensional vector
spaces are isomorphic to CN . So, for all practical purposes, we have only
two N -dimensional vector spaces, RN and CN .
</p>
<p>Suppose that V = V1 &oplus; V2 and that T is an automorphism of V which
leaves V1 invariant, i.e., T(V1)= V1. Then T leaves V2 invariant as well. To
see this, first note that if V= V1 &oplus;V2 and V= V1 &oplus;V&prime;2, then V2 = V&prime;2. This
can be readily established by looking at a basis of V obtained by extending
a basis of V1. Now note that since T(V)= V and T(V1)= V1, we must have
</p>
<p>V1 &oplus;V2 = V= T(V)= T(V1 &oplus;V2)= T(V1)&oplus; T(V2)= V1 &oplus; T(V2).
</p>
<p>Hence, by the argument above, T(V2) = V2. We summarize the discussion
as follows:
</p>
<p>Proposition 2.3.21 If V = V1 &oplus; V2, then an automorphism of V which
leaves one of the summands invariant leaves the other invariant as well.
</p>
<p>Example 2.3.22 (Another proof of the dimension theorem) Let T, V, and
W be as in Theorem 2.3.13. Let T&prime; : V/kerT&rarr; T(V) be a linear map defined
as follows. If ❏a❑ is represented by |a〉, then T&prime;(❏a❑)= T|a〉. First, we have
to show that this map is well defined, i.e., that if ❏a&prime;❑= ❏a❑, then T&prime;(❏a&prime;❑)=
T|a〉. But this is trivially true, because ❏a&prime;❑= ❏a❑ implies that |a&prime;〉 = |a〉+|z〉
with |z〉 &isin; kerT. So,
</p>
<p>T&prime;
(�
a&prime;
②)
</p>
<p>&equiv; T
∣∣a&prime;
</p>
<p>&rang;
= T
</p>
<p>(
|a〉 + |z〉
</p>
<p>)
= T
</p>
<p>(
|a〉
</p>
<p>)
+ T
</p>
<p>(
|z〉
</p>
<p>)
︸ ︷︷ ︸
=|0〉
</p>
<p>= T
(
|a〉
</p>
<p>)
.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.4 Complex Structures 45
</p>
<p>One can also easily show that T&prime; is linear.
We now show that T&prime; is an isomorphism. Suppose that |x〉 &isin; T(V). Then
</p>
<p>there is |y〉 &isin; V such that |x〉 = T|y〉 = T&prime;(❏y❑). This shows that T&prime; is surjec-
tive. To show that it is injective, let T&prime;(❏y❑) = T&prime;(❏x❑); then T|y〉 = T|x〉 or
T(|y〉&minus; |x〉)= 0. This shows that |y〉&minus; |x〉 &isin; kerT, i.e., ❏y❑= ❏x❑. This iso-
morphism implies that dim(V/kerT)= dimT(V). Equation (2.2) now yields
the result of the dimension theorem.
</p>
<p>The result of the preceding example can be generalized as follows
</p>
<p>Theorem 2.3.23 Let V and W be vector spaces and T : V &rarr; W a linear
map. Let U be a subspace of V. Define T&prime; : V/U&rarr; T(V) by T&prime;(❏a❑)= T|a〉,
where |a〉 is assumed to represent ❏a❑. Then T&prime; is a well defined isomor-
phism.
</p>
<p>Let U, V, and W be complex vector spaces. Consider the linear map
</p>
<p>T : (U&oplus;V)&otimes;W&rarr; (U&otimes;W)&oplus; (V&otimes;W)
</p>
<p>given by
</p>
<p>T
((
|u〉 + |v〉
</p>
<p>)
&otimes; |w〉
</p>
<p>)
= |u〉 &otimes; |w〉 + |v〉 &otimes; |w〉.
</p>
<p>It is trivial to show that T is an isomorphism. We thus have
</p>
<p>(U&oplus;V)&otimes;W&sim;= (U&otimes;W)&oplus; (V&otimes;W). (2.16)
</p>
<p>From the fact that dim(U&otimes;V)= dimUdimV, we have
</p>
<p>U&otimes;V&sim;= V&otimes;U. (2.17)
</p>
<p>Moreover, since dimC= 1 we have dim(C&otimes;V)= dimV. Hence,
</p>
<p>C&otimes;V&sim;= V&otimes;C&sim;= V. (2.18)
</p>
<p>Similarly
</p>
<p>R&otimes;V&sim;= V&otimes;R&sim;= V. (2.19)
for a real vector space V.
</p>
<p>2.4 Complex Structures
</p>
<p>Thus far in our treatment of vector spaces, we have avoided changing the
nature of scalars. When we declared that a vector space was complex, we
kept the scalars of that vector space complex, and if we used real numbers
in that vector space, they were treated as a subset of complex numbers.
</p>
<p>In this section, we explore the possibility of changing the scalars, and the
corresponding changes in the other structures of the vector space that may
ensue. The interesting case is changing the reals to complex numbers.</p>
<p/>
</div>
<div class="page"><p/>
<p>46 2 Vectors and Linear Maps
</p>
<p>In the discussion of changing the scalars, as well as other formal treat-
ments of other topics, it is convenient to generalize the concept of inner
products. While the notion of positive definiteness is crucial for the physical
applications of an inner product, for certain other considerations, it is too
restrictive. So, we relax that requirement and define our inner product anew.
However, except in this subsection,
</p>
<p>Box 2.4.1 Unless otherwise indicated, all complex inner products
are assumed to be sesquilinear as in Definition 2.2.1.
</p>
<p>Definition 2.4.2 Let F be either C or R. An inner product on an F-linear
complex bilinear inner
</p>
<p>product
space V is a map g : V&times;V&rarr; F with the following properties:
</p>
<p>(a) symmetry: g
(
|a〉, |b〉
</p>
<p>)
= g
</p>
<p>(
|b〉, |a〉
</p>
<p>)
;
</p>
<p>(b) bilinearity: g
(
|x〉, α|a〉 + β|b〉
</p>
<p>)
= αg
</p>
<p>(
|x〉, |a〉
</p>
<p>)
+ βg
</p>
<p>(
|x〉, |b〉
</p>
<p>)
,
</p>
<p>g
(
α|a〉 + β|b〉, |x〉
</p>
<p>)
= αg
</p>
<p>(
|a〉, |x〉
</p>
<p>)
+ βg
</p>
<p>(
|b〉, |x〉
</p>
<p>)
;
</p>
<p>(c) nondegeneracy: g
(
|x〉, |a〉
</p>
<p>)
= 0 &forall;|x〉 &isin; V &rArr; |a〉 = |0〉;
</p>
<p>with α,β &isin; F and |a〉, |b〉, |x〉 &isin; V.
</p>
<p>Non-degeneracy can be restated by saying that for any nonzero |a〉 &isin; V,
there is at least one vector |x〉 &isin; V such that g(|x〉, |a〉) �= 0. It is the state-
ment of the fact that the only vector orthogonal to all vectors of an inner
product space is the zero vector.
</p>
<p>Once again we use the Dirac bra and ket notation for the inner product.
However, to distinguish it from the previous inner product, we subscript the
notation with F. Thus the three properties in the definition above are denoted
by
</p>
<p>(a) symmetry: 〈a|b〉F = 〈b|a〉F;
(b) bilinearity: 〈x|αa + βb〉F = α〈x|a〉F + β〈x|b〉F,
</p>
<p>〈αa + βb|x〉F = α〈a|x〉F + β〈b|x〉F;
(c) non-degeneracy: 〈x|a〉F = 0 &forall;|x〉 &isin; V &rArr; |a〉 = |0〉.
</p>
<p>(2.20)
</p>
<p>Note that 〈 | 〉F = 〈 | 〉 when F=R.
</p>
<p>Definition 2.4.3 The adjoint of an operator A &isin; End(V), denoted by AT, is
defined by
</p>
<p>adjoint, self-adjoint,
</p>
<p>skew 〈Aa|b〉F = 〈a|ATb〉F or 〈a|AT|b〉F = 〈b|A|a〉F.
</p>
<p>An operator A is called self-adjoint if AT = A, and skew if AT =&minus;A.
</p>
<p>From this definition and the non-degeneracy of 〈 | 〉F it follows that
(
AT
</p>
<p>)T = A. (2.21)</p>
<p/>
</div>
<div class="page"><p/>
<p>2.4 Complex Structures 47
</p>
<p>Proposition 2.4.4 An operator A &isin; End(V) is skew iff 〈x|Ax〉F &equiv;
〈x|A|x〉F = 0 for all |x〉 &isin; V.
</p>
<p>Proof If A is skew, then
</p>
<p>〈x|A|x〉F = 〈x|AT|x〉F =&minus;〈x|A|x〉F &rArr; 〈x|A|x〉F = 0.
</p>
<p>Conversely, suppose that 〈x|A|x〉F = 0 for all |x〉 &isin; V, then for nonzero
α,β &isin; F and nonzero |a〉, |b〉 &isin; V,
</p>
<p>0 = 〈αa + βb|A|αa + βb〉F
= α2 〈a|A|a〉F︸ ︷︷ ︸
</p>
<p>=0
</p>
<p>+αβ〈a|A|b〉F + αβ〈b|A|a〉F + β2 〈b|A|b〉F︸ ︷︷ ︸
=0
</p>
<p>= αβ
(
〈b|A|a〉F + 〈b|AT|a〉F
</p>
<p>)
.
</p>
<p>Since αβ �= 0, we must have 〈b|(A+AT)|a〉F=0 for all nonzero |a〉, |b〉 &isin; V.
By non-degeneracy of the inner product, (A+AT)|a〉 = |0〉. Since this is true
for all |a〉 &isin; V, we must have AT =&minus;A. �
</p>
<p>Comparing this proposition with Theorem 2.3.8 shows how strong a re-
striction the positive definiteness imposes on the inner product.
</p>
<p>Definition 2.4.5 A complex structure J on a real vector space V is a linear
operator which satisfies J2 =&minus;1 and 〈Ja|Jb〉 = 〈a|b〉 for all |a〉, |b〉 &isin; V.
</p>
<p>Proposition 2.4.6 The complex structure J is skew.
</p>
<p>Proof Let |a〉 &isin; V and |b〉 = J|a〉. Then recalling that 〈 | 〉R = 〈 | 〉, on the one
hand,
</p>
<p>〈a|Ja〉 = 〈a|b〉 = 〈Ja|Jb〉 = 〈Ja|J2a〉 = &minus;〈Ja|a〉.
On the other hand,
</p>
<p>complex structure
</p>
<p>〈a|Ja〉 = 〈a|b〉 = 〈b|a〉 = 〈Ja|a〉.
</p>
<p>These two equations show that 〈a|Ja〉 = 0 for all |a〉 &isin; V. Hence, by Propo-
sition 2.4.4, J is skew. �
</p>
<p>Let |a〉 be any vector in the N -dimensional real inner product space.
Normalize |a〉 to get the unit vector |e1〉. By Propositions 2.4.4 and 2.4.6,
J|e1〉 is orthogonal to |e1〉. Normalize J|e1〉 to get |e2〉. If N &gt; 2, let |e3〉 be
any unit vector orthogonal to |e1〉 and |e2〉. Then |a3〉 &equiv; J|e3〉 is obviously
orthogonal to |e3〉. We claim that it is also orthogonal to both |e1〉 and |e2〉:
</p>
<p>〈e1|a3〉 = 〈Je1|Ja3〉 = 〈Je1|J2e3〉
= &minus;〈Je1|e3〉 = &minus;〈e2|e3〉 = 0
</p>
<p>〈e2|a3〉 = 〈Je1|Je3〉 = 〈e1|e3〉 = 0.
</p>
<p>Continuing this process, we can prove the following:</p>
<p/>
</div>
<div class="page"><p/>
<p>48 2 Vectors and Linear Maps
</p>
<p>Theorem 2.4.7 The vectors {|ei〉, J|ei〉}mi=1 withN = 2m form an orthonor-
mal basis for the real vector space V with inner product 〈 | 〉R = 〈 | 〉. In
particular, V must be even-dimensional for it to have a complex structure J.
</p>
<p>Definition 2.4.8 If V is a real vector space, then C&otimes; V, together with the
complex multiplication rule
</p>
<p>α
(
β &otimes; |a〉
</p>
<p>)
= (αβ)&otimes; |a〉, α,β &isin;C,
</p>
<p>is a complex vector space called the complexification of V and denoted
complexification by VC. In particular, (Rn)C &equiv;C&otimes;Rn &sim;=Cn.
</p>
<p>Note that dimCVC = dimRV and dimRVC = 2 dimRV. In fact, if
{|ak〉}Nk=1 is a basis of V, then it is also a basis of VC as a complex vec-
tor space, while {|ak〉, i|ak〉}Nk=1 is a basis of VC as a real vector space.
</p>
<p>After complexifying a real vector space V with inner product 〈 | 〉R = 〈 | 〉,
we can define an inner product on it which is sesquilinear (or hermitian) as
follows
</p>
<p>〈α&otimes; a|β &otimes; b〉 &equiv; ᾱβ〈a|b〉.
</p>
<p>It is left to the reader to show that this inner product satisfies all the proper-
ties given in Definition 2.2.1.
</p>
<p>To complexify a real vector space V, we have to &ldquo;multiply&rdquo; it by the set
of complex numbers: VC = C&otimes; V. As a result, we get a real vector space
of twice the original dimension. Is there a reverse process, a &ldquo;division&rdquo; of a
(necessarily even-dimensional) real vector space? That is, is there a way of
getting a complex vector space of half complex dimension, starting with an
even-dimensional real vector space?
</p>
<p>Let V be a 2m-dimensional real vector space. Let J be a complex structure
on V, and {|ei〉, J|ei〉}mi=1 a basis of V. On the subspace V1 &equiv; Span{|ei〉}mi=1,
define the multiplication by a complex number by
</p>
<p>(α + iβ)&otimes; |v1〉 &equiv; (α1+ βJ)|v1〉, α,β &isin;R, |v1〉 &isin; V1. (2.22)
</p>
<p>It is straightforward to show that this process turns the 2m-dimensional real
vector space V into the m-dimensional complex vector space VC1 .
</p>
<p>2.5 Linear Functionals
</p>
<p>An important example of a linear transformation occurs when the second
vector space, W, happens to be the set of scalars, C or R, in which case the
linear transformation is called a linear functional. The set of linear func-
</p>
<p>linear functional tionals L(V,C)&mdash;or L(V,R) if V is a real vector space&mdash;is denoted by V&lowast;
</p>
<p>dual vector space V&lowast; and is called the dual space of V.
</p>
<p>Example 2.5.1 Here are some examples of linear functionals:</p>
<p/>
</div>
<div class="page"><p/>
<p>2.5 Linear Functionals 49
</p>
<p>(a) Let |a〉 = (α1, α2, . . . , αn) be in Cn. Define φ :Cn &rarr;C by
</p>
<p>φ
(
|a〉
</p>
<p>)
=
</p>
<p>n&sum;
</p>
<p>k=1
αk.
</p>
<p>Then it is easy to show that φ is a linear functional.
(b) Let μij denote the elements of an m &times; n matrix M. Define ω :
</p>
<p>Mm&times;n &rarr;C by
</p>
<p>ω(M)=
m&sum;
</p>
<p>i=1
</p>
<p>n&sum;
</p>
<p>j=1
μij .
</p>
<p>Then it is easy to show that ω is a linear functional.
(c) Let μij denote the elements of an n&times;n matrix M. Define θ :Mn&times;n &rarr;
</p>
<p>C by
</p>
<p>θ(M)=
n&sum;
</p>
<p>j=1
μjj ,
</p>
<p>the sum of the diagonal elements of M. Then it is routine to show that
θ is a linear functional.
</p>
<p>(d) Define the operator int : C0(a, b)&rarr;R by
integration is a linear
</p>
<p>functional on the space
</p>
<p>of continuous functionsint(f )=
&int; b
</p>
<p>a
</p>
<p>f (t) dt.
</p>
<p>Then int is a linear functional on the vector space C0(a, b).
(e) Let V be a complex inner product space. Fix |a〉 &isin; V, and let γ a : V&rarr;
</p>
<p>C be defined by
</p>
<p>γ a
(
|b〉
</p>
<p>)
= 〈a|b〉.
</p>
<p>Then one can show that γ a is a linear functional.
(f) Let {|a1〉, |a2〉, . . . , |am〉} be an arbitrary finite set of vectors in V, and
</p>
<p>{φ1,φ2, . . . ,φm} an arbitrary set of linear functionals on V. Let
</p>
<p>A&equiv;
m&sum;
</p>
<p>k=1
|ak〉φk &isin; End(V)
</p>
<p>be defined by
</p>
<p>A|x〉 =
m&sum;
</p>
<p>k=1
|ak〉φk
</p>
<p>(
|x〉
</p>
<p>)
=
</p>
<p>m&sum;
</p>
<p>k=1
φk
</p>
<p>(
|x〉
</p>
<p>)
|ak〉.
</p>
<p>Then A is a linear operator on V.
</p>
<p>An example of linear isomorphism is that between a vector space and
its dual space, which we discuss now. Consider an N -dimensional vector
space with a basis B = {|a1〉, |a2〉, . . . , |aN 〉}. For any given set of N scalars,</p>
<p/>
</div>
<div class="page"><p/>
<p>50 2 Vectors and Linear Maps
</p>
<p>{α1, α2, . . . , αN }, define the linear functional φα by φα|ai〉 = αi . When φα
acts on any arbitrary vector |b〉 =&sum;Ni=1 βi |ai〉 in V, the result is
</p>
<p>φα|b〉 = φα
(
</p>
<p>N&sum;
</p>
<p>i=1
βi |ai〉
</p>
<p>)
=
</p>
<p>N&sum;
</p>
<p>i=1
βiφα|ai〉 =
</p>
<p>N&sum;
</p>
<p>i=1
βiαi . (2.23)
</p>
<p>This expression suggests that |b〉 can be represented as a column vector with
entries β1, β2, . . . , βN and φα as a row vector with entries α1, α2, . . . , αN .
Then φα|b〉 is merely the matrix product14 of the row vector (on the left)
and the column vector (on the right).
</p>
<p>φα is uniquely determined by the set {α1, α2, . . . , αN }. In other words,
corresponding to every set of N scalars there exists a unique linear func-
tional. This leads us to a particular set of functionals, φ1,φ2, . . . ,φN corre-
sponding, respectively, to the sets of scalars {1,0,0, . . . ,0}, {0,1,0, . . . ,0},
. . . , {0,0,0, . . . ,1}. This means that
</p>
<p>Every set ofN scalars
</p>
<p>defines a linear
</p>
<p>functional.
φ1|a1〉 = 1 and φ1|aj 〉 = 0 for j �= 1,
φ2|a2〉 = 1 and φ2|aj 〉 = 0 for j �= 2,
</p>
<p>...
...
</p>
<p>...
</p>
<p>φN |aN 〉 = 1 and φN |aj 〉 = 0 for j �=N,
</p>
<p>or that
</p>
<p>φi |aj 〉 = δij , (2.24)
where δij is the Kronecker delta.
</p>
<p>The functionals of Eq. (2.24) form a basis of the dual space V&lowast;. To show
this, consider an arbitrary γ &isin; V&lowast;, which is uniquely determined by its action
on the vectors in a basis B = {|a1〉, |a2〉, . . . , |aN 〉}. Let γ |ai〉 = γi &isin; C.
Then we claim that γ =&sum;Ni=1 γiφi . In fact, consider an arbitrary vector |a〉
in V with components (α1, α2, . . . , αN ) with respect to B . Then, on the one
hand,
</p>
<p>γ |a〉 = γ
(
</p>
<p>N&sum;
</p>
<p>i=1
αi |ai〉
</p>
<p>)
=
</p>
<p>N&sum;
</p>
<p>i=1
αiγ |ai〉 =
</p>
<p>N&sum;
</p>
<p>i=1
αiγi .
</p>
<p>On the other hand,
</p>
<p>(
N&sum;
</p>
<p>i=1
γiφi
</p>
<p>)
|a〉 =
</p>
<p>(
N&sum;
</p>
<p>i=1
γiφi
</p>
<p>)(
N&sum;
</p>
<p>j=1
αj |aj 〉
</p>
<p>)
</p>
<p>=
N&sum;
</p>
<p>i=1
γi
</p>
<p>N&sum;
</p>
<p>j=1
αjφi |aj 〉 =
</p>
<p>N&sum;
</p>
<p>i=1
γi
</p>
<p>N&sum;
</p>
<p>j=1
αj δij =
</p>
<p>N&sum;
</p>
<p>i=1
γiαi .
</p>
<p>14Matrices will be taken up in Chap. 5. Here, we assume only a nodding familiarity with
elementary matrix operations.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.5 Linear Functionals 51
</p>
<p>Since the actions of γ and
&sum;N
</p>
<p>i=1 γiφi yield equal results for arbitrary |a〉,
we conclude that γ =&sum;Ni=1 γiφi , i.e., {φi}Ni=1 span V&lowast;. Thus, we have the
following result.
</p>
<p>Theorem 2.5.2 For every basis B = {|aj 〉}Nj=1 in V, there corresponds a
unique basis B&lowast; = {φi}Ni=1 in V&lowast; with the property that φi |aj 〉 = δij .
</p>
<p>By this theorem the dual space of an N -dimensional vector space is also
N -dimensional, and thus isomorphic to it. The basis B&lowast; is called the dual
</p>
<p>dual basisbasis of B . A corollary to Theorem 2.5.2 is that to every vector in V there
corresponds a unique linear functional in V&lowast;. This can be seen by noting that
every vector |a〉 is uniquely determined by its components (α1, α2, . . . , αN )
in a basis B . The unique linear functional φa corresponding to |a〉, also
called the dual of |a〉, is simply &sum;Ni=1 αiφi , with φi &isin; B&lowast;.
</p>
<p>Definition 2.5.3 An annihilator of |a〉 &isin; V is a linear functional φ &isin; V&lowast;
such that φ|a〉 = 0. Let W be a subspace of V. The set of linear functionals
in V&lowast; that annihilate all vectors in W is denoted by W0.
</p>
<p>The reader may check that W0 is a subspace of V&lowast;. Moreover, if we
extend a basis {|ai〉}ki=1 of W to a basis B = {|ai〉}Ni=1 of V, then we can
</p>
<p>annihilator of a vector
</p>
<p>and a subspace
</p>
<p>show that the functionals {φj }Nj=k+1, chosen from the basis B&lowast; = {φj }Nj=1
dual to B , span W0. It then follows that
</p>
<p>dimV= dimW+ dimW0. (2.25)
</p>
<p>We shall have occasions to use annihilators later on when we discuss sym-
plectic geometry.
</p>
<p>We have &ldquo;dualed&rdquo; a vector, a basis, and a complete vector space. The
only object remaining is a linear transformation.
</p>
<p>Definition 2.5.4 Let T : V&rarr;U be a linear map. Define T&lowast; :U&lowast; &rarr; V&lowast; by15
</p>
<p>dual, or pull back, of a
</p>
<p>linear transformation
</p>
<p>[
T&lowast;(γ )
</p>
<p>]
|a〉 = γ
</p>
<p>(
T|a〉
</p>
<p>)
&forall;|a〉 &isin; V, γ &isin;U&lowast;,
</p>
<p>T&lowast; is called the dual or pullback, of T.
</p>
<p>One can readily verify that T&lowast; &isin; L(U&lowast;,V&lowast;), i.e., that T&lowast; is a linear oper-
ator on U&lowast;. Some of the mapping properties of T&lowast; are tied to those of T. To
see this we first consider the kernel of T&lowast;. Clearly, γ is in the kernel of T&lowast; if
and only if γ annihilates all vectors of the form T|a〉, i.e., all vectors in T(V).
It follows that γ is in T(V)0. In particular, if T is surjective, T(V)=U, and γ
annihilates all vectors in U, i.e., it is the zero linear functional. We conclude
that kerT&lowast; = 0, and therefore, T&lowast; is injective. Similarly, one can show that
if T is injective, then T&lowast; is surjective. We summarize the discussion above:
</p>
<p>15Do not confuse this &ldquo;*&rdquo; with complex conjugation.</p>
<p/>
</div>
<div class="page"><p/>
<p>52 2 Vectors and Linear Maps
</p>
<p>Proposition 2.5.5 Let T be a linear transformation and T&lowast; its pull back.
Then kerT&lowast; = T(V)0. If T is surjective (injective), then T&lowast; is injective (sur-
jective). In particular, T&lowast; is an isomorphism if T is.
</p>
<p>It is useful to make a connection between the inner product and linear
functionals. To do this, consider a basis {|a1〉, |a2〉, . . . , |aN 〉} and let αi =
〈a|ai〉. As noted earlier, the set of scalars {αi}Ni=1 defines a unique linear
functional γ a (see Example 2.5.1) such that γ a|ai〉 = αi . Since 〈a|ai〉 is
also equal to αi , it is natural to identify γ a with the symbol 〈a|, and write
γ a �&rarr; 〈a|.
</p>
<p>duals and inner products
</p>
<p>It is also convenient to introduce the notation16
</p>
<p>(
|a〉
</p>
<p>)&dagger; &equiv; 〈a|, (2.26)
</p>
<p>where the symbol &dagger; means &ldquo;dual, or dagger of&rdquo;. Now we ask: How does
this dagger operation act on a linear combination of vectors? Let |c〉 =
</p>
<p>dagger of a linear
</p>
<p>combination of vectors
α|a〉 + β|b〉 and take the inner product of |c〉 with an arbitrary vector |x〉
using linearity in the second factor: 〈x|c〉 = α〈x|a〉+β〈x|b〉. Now complex
conjugate both sides and use the (sesqui)symmetry of the inner product:
</p>
<p>(LHS)&lowast; = 〈x|c〉&lowast; = 〈c|x〉,
(RHS)&lowast; = α&lowast;〈x|a〉&lowast; + β&lowast;〈x|b〉&lowast; = α&lowast;〈a|x〉 + β&lowast;〈b|x〉
</p>
<p>=
(
α&lowast;〈a| + β&lowast;〈b|
</p>
<p>)
|x〉.
</p>
<p>Since this is true for all |x〉, we must have (|c〉)&dagger; &equiv; 〈c| = α&lowast;〈a| + β&lowast;〈b|.
Therefore, in a duality &ldquo;operation&rdquo; the complex scalars must be conjugated.
So, we have
</p>
<p>(
α|a〉 + β|b〉
</p>
<p>)&dagger; = α&lowast;〈a| + β&lowast;〈b|. (2.27)
Thus, unlike the association |a〉 �&rarr; γ a which is linear, the association γ a �&rarr;
〈a| is not linear, but sesquilinear:
</p>
<p>γ αa+βb �&rarr; α&lowast;〈a| + β&lowast;〈b|.
</p>
<p>It is convenient to represent |a〉 &isin;Cn as a column vector
</p>
<p>|a〉 =
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>α1
α2
...
</p>
<p>αn
</p>
<p>⎞
⎟⎟⎟⎠ .
</p>
<p>Then the definition of the complex inner product suggests that the dual of
|a〉 must be represented as a row vector with complex conjugate entries:
</p>
<p>〈a| =
(
α&lowast;1 α
</p>
<p>&lowast;
2 . . . α
</p>
<p>&lowast;
n
</p>
<p>)
, (2.28)
</p>
<p>16The significance of this notation will become clear in Sect. 4.3.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.6 Multilinear Maps 53
</p>
<p>and the inner product can be written as the (matrix) product
</p>
<p>〈a|b〉 =
(
α&lowast;1 α
</p>
<p>&lowast;
2 &middot; &middot; &middot; α&lowast;n
</p>
<p>)
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>β1
β2
...
</p>
<p>βn
</p>
<p>⎞
⎟⎟⎟⎠=
</p>
<p>n&sum;
</p>
<p>i=1
α&lowast;i βi .
</p>
<p>Compare (2.28) with the
</p>
<p>comments after (2.23).
</p>
<p>The complex
</p>
<p>conjugation in (2.28) is
</p>
<p>the result of the
</p>
<p>sesquilinearity of the
</p>
<p>association |a〉&harr; 〈a|.
</p>
<p>2.6 Multilinear Maps
</p>
<p>There is a very useful generalization of the linear functionals that becomes
essential in the treatment of tensors later in the book. However, a limited
version of its application is used in the discussion of determinants, which
we shall start here.
</p>
<p>Definition 2.6.1 Let V and U be vector spaces. Let Vp denote the p-fold
Cartesian product of V. A p-linear map from V to U is a map θ : Vp &rarr; U p-linear map
which is linear with respect to each of its arguments:
</p>
<p>θ
(
|a1〉, . . . , α|aj 〉 + β|bj 〉, . . . , |ap〉
</p>
<p>)
</p>
<p>= αθ
(
|a1〉, . . . , |aj 〉, . . . , |ap〉
</p>
<p>)
+ βθ
</p>
<p>(
|a1〉, . . . , |bj 〉, . . . , |ap〉
</p>
<p>)
.
</p>
<p>A p-linear map from V to C or R is called a p-linear function in V. p-linear function
</p>
<p>As an example, let {φi}pi=1 be linear functionals on V. Define θ by
</p>
<p>θ
(
|a1〉, . . . , |ap〉
</p>
<p>)
= φ1
</p>
<p>(
|a1〉
</p>
<p>)
. . .φp
</p>
<p>(
|ap〉
</p>
<p>)
, |ai〉 &isin; V.
</p>
<p>Clearly θ is p-linear.
Let σ denote a permutation of 1,2, . . . , p. Define the p-linear map σω
</p>
<p>by
</p>
<p>σω
(
|a1〉, . . . , |ap〉
</p>
<p>)
= ω
</p>
<p>(
|aσ(1)〉, . . . , |aσ(p)〉
</p>
<p>)
</p>
<p>Definition 2.6.2 A p-linear map ω from V to U is skew-symmetric if σω= skew-symmetric p-linear
mapǫσ &middot;ω, i.e., if
</p>
<p>ω
(
|aσ(1)〉, . . . , |aσ(p)〉
</p>
<p>)
= ǫσω
</p>
<p>(
|a1〉, . . . , |ap〉
</p>
<p>)
</p>
<p>where ǫσ is the sign of σ , which is +1 if σ is even and &minus;1 if it is odd. The
set of p-linear skew-symmetric maps from V to U is denoted by Λp(V,U).
The set of p-linear skew-symmetric functions in V is denoted by Λp(V).
</p>
<p>The permutation sign ǫσ is sometimes written as
</p>
<p>ǫσ = ǫσ(1)σ (2)...σ (p) &equiv; ǫi1i2...ip , (2.29)
</p>
<p>where ik &equiv; σ(k).</p>
<p/>
</div>
<div class="page"><p/>
<p>54 2 Vectors and Linear Maps
</p>
<p>Any p-linear map can be turned into a skew-symmetric p-linear map. In
fact, if θ is a p-linear map, then
</p>
<p>ω&equiv;
&sum;
</p>
<p>π
</p>
<p>ǫπ &middot; πθ (2.30)
</p>
<p>is skew-symmetric:
</p>
<p>σω= σ
&sum;
</p>
<p>π
</p>
<p>ǫπ &middot; πθ =
&sum;
</p>
<p>π
</p>
<p>ǫπ &middot; (σπ)θ = (ǫσ )2
&sum;
</p>
<p>π
</p>
<p>ǫπ &middot; (σπ)θ
</p>
<p>= ǫσ
&sum;
</p>
<p>π
</p>
<p>(ǫσ ǫπ ) &middot; (σπ)θ = ǫσ
&sum;
</p>
<p>σπ
</p>
<p>ǫσπ &middot; (σπ)θ = ǫσ &middot;ω,
</p>
<p>where we have used the fact that the sign of the product is the product of the
signs of two permutations, and if
</p>
<p>&sum;
π sums over all permutations, then so
</p>
<p>does
&sum;
</p>
<p>σπ .
The following theorem can be proved using properties of permutations:
</p>
<p>Theorem 2.6.3 Let ω &isin;Λp(V,U). Then the following statements are equiv-
alent:
</p>
<p>1. ω(|a1〉, . . . , |ap〉)= 0 whenever |ai〉 = |aj 〉 for some pair i �= j .
2. ω(|aσ(1)〉, . . . , |aσ(p)〉) = ǫσω(|a1〉, . . . , |ap〉), for any permutation σ
</p>
<p>of 1,2, . . . , p, and any |a1〉, . . . , |ap〉 in V.
3. ω(|a1〉, . . . , |ap〉)= 0 whenever {|ak〉}pk=1 are linearly dependent.
</p>
<p>Proposition 2.6.4 Let N = dimV and ω &isin; ΛN (V,U). Then ω is deter-
mined uniquely by its value on a basis of V. In particular, if ω vanishes
on a basis, then ω= 0.
</p>
<p>Proof Let {|ek〉}Nk=1 be a basis of V. Let {|aj 〉}Nj=1 be any set of vectors in V
and write |aj 〉 =
</p>
<p>&sum;N
k=1 αjk|ek〉 for j = 1, . . . ,N . Then
</p>
<p>ω
(
|a1〉, . . . , |aN 〉
</p>
<p>)
=
</p>
<p>N&sum;
</p>
<p>k1...kN=1
α1k1 . . . αNkNω
</p>
<p>(
|ek1〉, . . . , |ekN 〉
</p>
<p>)
</p>
<p>&equiv;
&sum;
</p>
<p>π
</p>
<p>α1π(1) . . . αNπ(N)ω
(
|eπ(1)〉, . . . , |eπ(N)〉
</p>
<p>)
</p>
<p>=
(&sum;
</p>
<p>π
</p>
<p>ǫπα1π(1) . . . αNπ(N)
</p>
<p>)
ω
(
|e1〉, . . . , |eN 〉
</p>
<p>)
.
</p>
<p>Since the term in parentheses is a constant, we are done. �
</p>
<p>Definition 2.6.5 A skew symmetric N -linear function in V, i.e., a member
Determinant function of ΛN (V) is called a determinant function in V.
</p>
<p>Let B = {|ek〉}Nk=1 be a basis of V and B&lowast; = {ǫj }Nj=1 a basis of V&lowast;, dual
to B . For any set of N vectors {|ak〉}Nk=1 in V, define the N -linear function
θ by
</p>
<p>θ
(
|a1〉, . . . , |aN 〉
</p>
<p>)
= ǫ1
</p>
<p>(
|a1〉
</p>
<p>)
. . . ǫN
</p>
<p>(
|aN 〉
</p>
<p>)
,</p>
<p/>
</div>
<div class="page"><p/>
<p>2.6 Multilinear Maps 55
</p>
<p>and note that
</p>
<p>πθ
(
|e1〉, . . . , |eN 〉
</p>
<p>)
&equiv; θ
</p>
<p>(
|eπ(1)〉, . . . , |eπ(N)〉
</p>
<p>)
= διπ ,
</p>
<p>where ι is the identity permutation and διπ = 1 if π = ι and διπ = 0 if π �= ι.
Now let � be defined by �&equiv;&sum;π ǫπ &middot;πθ . Then, by Eq. (2.30), � &isin;ΛN (V),
i.e., � is a determinant function. Furthermore,
</p>
<p>�
(
|e1〉, . . . , |eN 〉
</p>
<p>)
=
&sum;
</p>
<p>π
</p>
<p>ǫπ &middot; πθ
(
|e1〉, . . . , |eN 〉
</p>
<p>)
=
&sum;
</p>
<p>π
</p>
<p>ǫπδιπ = ǫι = 1
</p>
<p>Therefore, we have the following:
</p>
<p>Box 2.6.6 In every finite-dimensional vector space, there are deter-
minant functions which are not identically zero.
</p>
<p>Proposition 2.6.7 Let ω &isin;ΛN (V,U). Let� be a fixed nonzero determinant
function in V. Then ω determines a unique |u�〉 &isin;U such that
</p>
<p>ω
(
|v1〉, . . . , |vN 〉
</p>
<p>)
=�
</p>
<p>(
|v1〉, . . . , |vN 〉
</p>
<p>)
&middot; |u�〉.
</p>
<p>Proof Let {|vk〉}Nk=1 be a basis of V such that �(|v1〉, . . . , |vN 〉) �= 0. By
dividing one of the vectors (or �) by a constant, we can assume that
�(|v1〉, . . . , |vN 〉) = 1. Denote ω(|v1〉, . . . , |vN 〉) by |u�〉. Now note that
ω &minus; � &middot; |u�〉 yields zero on the basis {|vk〉}Nk=1. By Proposition 2.6.4, it
must be identically zero. �
</p>
<p>Corollary 2.6.8 Let � be a fixed nonzero determinant function in V. Then
every determinant function is a scalar multiple of �.
</p>
<p>Proof Let U be C or R in Proposition 2.6.7. �
</p>
<p>Proposition 2.6.9 Let � be a determinant function in the N -dimensional
vector space V. Let |v〉 and {|vk〉}Nk=1 be vectors in V. Then
N&sum;
</p>
<p>j=1
(&minus;1)j&minus;1�
</p>
<p>(
|v〉, |v1〉, . . . , |̂vj 〉, . . . , |vN 〉
</p>
<p>)
&middot; |vj 〉 =�
</p>
<p>(
|v1〉, . . . , |vN 〉
</p>
<p>)
&middot; |v〉
</p>
<p>where a hat on a vector means that particular vector is missing.
</p>
<p>Proof See Problem 2.37. �
</p>
<p>2.6.1 Determinant of a Linear Operator
</p>
<p>Let A be a linear operator on an N -dimensional vector space V. Choose a
nonzero determinant function �. For a basis {|vi〉}Ni=1 define the function
�A by
</p>
<p>�A
(
|v1〉, . . . , |vN 〉
</p>
<p>)
&equiv;�
</p>
<p>(
A|v1〉, . . . ,A|vN 〉
</p>
<p>)
. (2.31)</p>
<p/>
</div>
<div class="page"><p/>
<p>56 2 Vectors and Linear Maps
</p>
<p>Clearly, �A is also a determinant function. By Corollary 2.6.8, it is a mul-
tiple of �. So, �A = α�. Furthermore, it is independent of the nonzero
determinant function chosen, because if �&prime; is another nonzero determinant
function, then again by Corollary 2.6.8, �&prime; = λ�, and
</p>
<p>�&prime;A = λ�A = λα�= α�&prime;.
</p>
<p>This means that α is determined only by A, independent of the nonzero
determinant function and the basis chosen.
</p>
<p>determinant of an
</p>
<p>operator defined
</p>
<p>Definition 2.6.10 Let A &isin; End(V). Let � be a nonzero determinant
function in V, and let �A be as in Eq. (2.31). Then
</p>
<p>�A = detA &middot;� (2.32)
</p>
<p>defines the determinant of A.
</p>
<p>Using Eq. (2.32), we have the following theorem whose proof is left as
Problem 2.38:
</p>
<p>Theorem 2.6.11 The determinant of a linear operator A has the following
properties:
</p>
<p>1. If A= λ1, then detA= λN .
2. A is invertible iff detA �= 0.
3. det(A ◦ B)= detAdetB.
</p>
<p>2.6.2 Classical Adjoint
</p>
<p>Let V be an N -dimensional vector space, � a determinant function in V,
and A &isin; End(V). For |v〉, |vi〉 &isin; V, define Φ : VN &rarr; End(V) by
</p>
<p>Φ
(
|v1〉, . . . , |vN 〉
</p>
<p>)
|v〉
</p>
<p>=
N&sum;
</p>
<p>j=1
(&minus;1)j&minus;1�
</p>
<p>(
|v〉,A|v1〉, . . . , Â|vj 〉, . . . ,A|vN 〉
</p>
<p>)
&middot; |vj 〉.
</p>
<p>Clearly Φ is skew-symmetric. Therefore, by Proposition 2.6.7, there is a
unique linear operator&mdash;call it ad(A)&mdash;such that
</p>
<p>Φ
(
|v1〉, . . . , |vN 〉
</p>
<p>)
=�
</p>
<p>(
|v1〉, . . . , |vN 〉
</p>
<p>)
&middot; ad(A),
</p>
<p>i.e.,
</p>
<p>N&sum;
</p>
<p>j=1
(&minus;1)j&minus;1�
</p>
<p>(
|v〉,A|v1〉, . . . , Â|vj 〉, . . . ,A|vN 〉
</p>
<p>)
&middot; |vj 〉
</p>
<p>=�
(
|v1〉, . . . , |vN 〉
</p>
<p>)
&middot; ad(A)|v〉. (2.33)</p>
<p/>
</div>
<div class="page"><p/>
<p>2.7 Problems 57
</p>
<p>This equation shows that ad(A) is independent of the determinant function
chosen, and is called the classical adjoint of A.
</p>
<p>classical adjoint of an
</p>
<p>operator
</p>
<p>Proposition 2.6.12 The classical adjoint satisfies the following relations:
</p>
<p>ad(A) ◦A= detA &middot; 1= A ◦ ad(A) (2.34)
</p>
<p>where 1 is the unit operator.
</p>
<p>Proof Replace |v〉 with A|v〉 in Eq. (2.33) to obtain
N&sum;
</p>
<p>j=1
(&minus;1)j&minus;1�
</p>
<p>(
A|v〉,A|v1〉, . . . , Â|vj 〉, . . . ,A|vN 〉
</p>
<p>)
&middot; |vj 〉
</p>
<p>=�
(
|v1〉, . . . , |vN 〉
</p>
<p>)
ad(A) ◦A|v〉.
</p>
<p>Then, the left-hand side can be written as
</p>
<p>LHS = detA &middot;
N&sum;
</p>
<p>j=1
(&minus;1)j&minus;1�
</p>
<p>(
|v〉, |v1〉, . . . , |̂vj 〉, . . . , |vN 〉
</p>
<p>)
&middot; |vj 〉
</p>
<p>= detA &middot;�
(
|v1〉, . . . , |vN 〉
</p>
<p>)
&middot; |v〉,
</p>
<p>where the last equality follows from Proposition 2.6.9. Noting that |v〉 is
arbitrary, the first equality of the proposition follows.
</p>
<p>To obtain the second equality, apply A to (2.33). Then by Proposi-
tion 2.6.9, the left-hand side becomes
</p>
<p>LHS =
N&sum;
</p>
<p>j=1
(&minus;1)j&minus;1�
</p>
<p>(
|v〉,A|v1〉, . . . , Â|vj 〉, . . . ,A|vN 〉
</p>
<p>)
&middot;A|vj 〉
</p>
<p>=�
(
A|v1〉, . . . ,A|vN 〉
</p>
<p>)
&middot; |v〉 = detA &middot;�
</p>
<p>(
|v1〉, . . . , |vN 〉
</p>
<p>)
&middot; |v〉,
</p>
<p>and the right-hand side becomes
</p>
<p>RHS =�
(
|v1〉, . . . , |vN 〉
</p>
<p>)
&middot;A ◦ ad(A)|v〉.
</p>
<p>Since the two sides hold for arbitrary |v〉, the second equality of the propo-
sition follows. �
</p>
<p>Corollary 2.6.13 If detA �= 0, then A is invertible and
</p>
<p>A&minus;1 = 1
detA
</p>
<p>&middot; ad(A).
</p>
<p>2.7 Problems
</p>
<p>2.1 Let R+ denote the set of positive real numbers. Define the &ldquo;sum&rdquo; of two
elements of R+ to be their usual product, and define scalar multiplication by
elements of R as being given by r &middot; p = pr where r &isin; R and p &isin; R+. With
these operations, show that R+ is a vector space over R.</p>
<p/>
</div>
<div class="page"><p/>
<p>58 2 Vectors and Linear Maps
</p>
<p>2.2 Show that the intersection of two subspaces is also a subspace.
</p>
<p>2.3 For each of the following subsets of R3 determine whether it is a
subspace of R3:
</p>
<p>(a) {(x, y, z) &isin;R3|x + y &minus; 2z= 0};
(b) {(x, y, z) &isin;R3|x + y &minus; 2z= 3};
(c) {(x, y, z) &isin;R3|xyz= 0}.
</p>
<p>2.4 Prove that the components of a vector in a given basis are unique.
</p>
<p>2.5 Show that the following vectors form a basis for Cn (or Rn).
</p>
<p>|a1〉 =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>1
1
...
</p>
<p>1
1
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠
, |a2〉 =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>1
1
...
</p>
<p>1
0
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠
, . . . , |an〉 =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>1
0
...
</p>
<p>0
0
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠
.
</p>
<p>2.6 Prove Theorem 2.1.6.
</p>
<p>2.7 Let W be a subspace of R5 defined by
</p>
<p>W=
{
(x1, . . . , x5) &isin;R5 | x1 = 3x2 + x3, x2 = x5, and x4 = 2x3
</p>
<p>}
.
</p>
<p>Find a basis for W.
</p>
<p>2.8 Let U1 and U2 be subspaces of V. Show that
</p>
<p>(a) dim(U1+U2)= dimU1+dimU2&minus;dim(U1&cap;U2). Hint: Let {|ai〉}mi=1
be a basis of U1 &cap; U2. Extend this to {{|ai〉}mi=1, {|bi〉}ki=1}, a basis
for U1, and to {{|ai〉}mi=1, {|ci〉}li=1}, a basis for U2. Now show that
{{|ai〉}mi=1, {|bi〉}ki=1, {|ci〉}li=1} is a basis for U1 +U2.
</p>
<p>(b) If U1 +U2 = V and dimU1 + dimU2 = dimV, then V=U1 &oplus;U2.
(c) If dimU1 + dimU2 &gt; dimV, then U1 &cap;U2 �= {0}.
</p>
<p>2.9 Show that the vectors defined in Eq. (2.5) span W=U&oplus;V.
</p>
<p>2.10 Show that the inner product of any vector with |0〉 is zero.
</p>
<p>2.11 Find a0, b0, b1, c0, c1, and c2 such that the polynomials a0, b0 + b1t ,
and c0+c1t+c2t2 are mutually orthonormal in the interval [0,1]. The inner
product is as defined for polynomials in Example 2.2.3 with w(t)= 1.
</p>
<p>2.12 Given the linearly independent vectors x(t)= tn, for n= 0,1,2, . . . in
Pc[t], use the Gram&ndash;Schmidt process to find the orthonormal polynomials
e0(t), e1(t), and e2(t)
</p>
<p>(a) when the inner product is defined as 〈x|y〉 =
&int; 1
&minus;1 x
</p>
<p>&lowast;(t)y(t) dt .</p>
<p/>
</div>
<div class="page"><p/>
<p>2.7 Problems 59
</p>
<p>(b) when the inner product is defined with a nontrivial weight function:
</p>
<p>〈x|y〉 =
&int; &infin;
</p>
<p>&minus;&infin;
e&minus;t
</p>
<p>2
x&lowast;(t)y(t) dt.
</p>
<p>Hint: Use the following result:
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
e&minus;t
</p>
<p>2
tn dt =
</p>
<p>⎧
⎪⎨
⎪⎩
</p>
<p>&radic;
π if n= 0,
</p>
<p>0 if n is odd,&radic;
π
</p>
<p>1&middot;3&middot;5&middot;&middot;&middot;(n&minus;1)
2n/2
</p>
<p>if n is even.
</p>
<p>2.13 (a) Use the Gram&ndash;Schmidt process to find an orthonormal set of vec-
tors out of (1,&minus;1,1), (&minus;1,0,1), and (2,&minus;1,2).
</p>
<p>(b) Are these three vectors linearly independent? If not, find a zero linear
combination of them by using part (a).
</p>
<p>2.14 (a) Use the Gram&ndash;Schmidt process to find an orthonormal set of vec-
tors out of (1,&minus;1,2), (&minus;2,1,&minus;1), and (&minus;1,&minus;1,4).
</p>
<p>(b) Are these three vectors linearly independent? If not, find a zero linear
combination of them by using part (a).
</p>
<p>2.15 Show that
&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>(
t10 &minus; t6 + 5t4 &minus; 5
</p>
<p>)
e&minus;t
</p>
<p>4
dt
</p>
<p>&le;
&radic;&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>(
t4 &minus; 1
</p>
<p>)2
e&minus;t4 dt
</p>
<p>&radic;&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>(
t6 + 5
</p>
<p>)2
e&minus;t4 dt.
</p>
<p>Hint: Define an appropriate inner product and use the Schwarz inequality.
</p>
<p>2.16 Show that
&int; &infin;
</p>
<p>&minus;&infin;
dx
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
dy
</p>
<p>(
x5 &minus; x3 + 2x2 &minus; 2
</p>
<p>)(
y5 &minus; y3 + 2y2 &minus; 2
</p>
<p>)
e&minus;(x
</p>
<p>4+y4)
</p>
<p>&le;
&int; &infin;
</p>
<p>&minus;&infin;
dx
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
dy
</p>
<p>(
x4 &minus; 2x2 + 1
</p>
<p>)(
y6 + 4y3 + 4
</p>
<p>)
e&minus;(x
</p>
<p>4+y4).
</p>
<p>Hint: Define an appropriate inner product and use the Schwarz inequality.
</p>
<p>2.17 Show that for any set of n complex numbers α1, α2, . . . , αn, we have
</p>
<p>|α1 + α2 + &middot; &middot; &middot; + αn|2 &le; n
(
|α1|2 + |α2|2 + &middot; &middot; &middot; + |αn|2
</p>
<p>)
.
</p>
<p>Hint: Apply the Schwarz inequality to (1,1, . . . ,1) and (α1, α2, . . . , αn).
</p>
<p>2.18 Using the Schwarz inequality show that if {αi}&infin;i=1 and {βi}&infin;i=1 are in
C&infin;, then
</p>
<p>&sum;&infin;
i=1 α
</p>
<p>&lowast;
i βi is convergent.
</p>
<p>2.19 Show that T :R2 &rarr;R3 given by T(x, y)= (x2 + y2, x+ y,2x&minus; y) is
not a linear mapping.</p>
<p/>
</div>
<div class="page"><p/>
<p>60 2 Vectors and Linear Maps
</p>
<p>2.20 Verify that all the transformations of Example 2.3.5 are linear.
</p>
<p>2.21 Let π be the permutation that takes (1,2,3) to (3,1,2). Find
</p>
<p>Aπ |ei〉, i = 1,2,3,
</p>
<p>where {|ei〉}3i=1 is the standard basis of R3 (or C3), and Aπ is as defined in
Example 2.3.5.
</p>
<p>2.22 Show that if T &isin; L(C,C), then there exists α &isin; C such that T|a〉 =
α|a〉 for all |a〉 &isin;C.
</p>
<p>2.23 Show that if {|ai〉}ni=1 spans V and T &isin;L(V,W), then {T|ai〉}ni=1 spans
T(V). In particular, if T is surjective, then {T|ai〉}ni=1 spans W.
</p>
<p>2.24 Give an example of a function f :R2 &rarr;R such that
</p>
<p>f
(
α|a〉
</p>
<p>)
= αf
</p>
<p>(
|a〉
</p>
<p>)
&forall;α &isin;R and |a〉 &isin;R2
</p>
<p>but f is not linear. Hint: Consider a homogeneous function of degree 1.
</p>
<p>2.25 Show that the following transformations are linear:
</p>
<p>(a) V is C over the reals and C|z〉 = |z&lowast;〉. Is C linear if instead of real
numbers, complex numbers are used as scalars?
</p>
<p>(b) V is Pc[t] and T|x(t)〉 = |x(t + 1)〉 &minus; |x(t)〉.
</p>
<p>2.26 Verify that the kernel of a transformation T : V&rarr;W is a subspace of
V, and that T(V) is a subspace of W.
</p>
<p>2.27 Let V and W be finite dimensional vector spaces. Show that if T &isin;
L(V,W) is surjective, then dimW &le; dimV .
</p>
<p>2.28 Suppose that V is finite dimensional and T &isin; L(V,W) is not zero.
Prove that there exists a subspace U of V such that kerT &cap; U = {0} and
T(V)= T(U).
</p>
<p>2.29 Using Theorem 2.3.11, prove Theorem 2.3.18.
</p>
<p>2.30 Using Theorem 2.3.11, prove Theorem 2.3.19.
</p>
<p>2.31 Let BV = {|ai〉}Ni=1 be a basis for V and BW = {|bi〉}Ni=1 a basis for W.
Define the linear transformation T|ai〉 = |bi〉, i = 1,2, . . . ,N . Now prove
Theorem 2.3.20 by showing that T is an isomorphism.
</p>
<p>2.32 Show that (AT)T = A for the adjoint given in Definition 2.4.3.
</p>
<p>2.33 Show that W0 is a subspace of V&lowast; and
</p>
<p>dimV= dimW+ dimW0.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.7 Problems 61
</p>
<p>2.34 Show that every vector in the N -dimensional vector space V&lowast; has
N &minus; 1 linearly independent annihilators. Stated differently, show that a lin-
ear functional maps N &minus; 1 linearly independent vectors to zero.
</p>
<p>2.35 Show that T and T&lowast; have the same rank. In particular, show that if T is
injective, then T&lowast; is surjective. Hint: Use the dimension theorem for T and
T&lowast; and Eq. (2.25).
</p>
<p>2.36 Prove Theorem 2.6.3.
</p>
<p>2.37 Prove Proposition 2.6.9. Hint: First show that you get zero on both
sides if {|vk〉}Nk=1 are linearly dependent. Next assume their linear indepen-
dence and choose them as a basis, write |v〉 in terms of them, and note that
</p>
<p>�
(
|v〉, |v1〉, . . . , |̂vj 〉, . . . , |vN 〉
</p>
<p>)
= 0
</p>
<p>unless i = j .
</p>
<p>2.38 Prove Theorem 2.6.11. Hint: For the second part of the theorem, use
the fact that an invertible A maps linearly independent sets of vectors onto
linearly independent sets.</p>
<p/>
</div>
<div class="page"><p/>
<p>3Algebras
</p>
<p>In many physical applications, a vector space V has a natural &ldquo;product&rdquo;,
i.e., a binary operation V&times;V&rarr; V, which we call multiplication. The prime
example of such a vector space is the vector space of matrices. It is therefore
useful to consider vector spaces for which such a product exists.
</p>
<p>3.1 From Vector Space to Algebra
</p>
<p>In this section, we define an algebra, give some familiar examples of alge-
bras, and discuss some of their basic properties.
</p>
<p>Definition 3.1.1 An algebra A over C (or R) is a vector space over C
algebra defined
</p>
<p>(or R), together with a binary operation A &times; A &rarr; A, called multiplica-
tion. The image of (a,b) &isin; A&times;A under this mapping1 is denoted by ab,
and it satisfies the following two relations
</p>
<p>a(βb + γ c)= βab + γ ac
(βb + γ c)a = βba + γ ca
</p>
<p>for all a,b, c &isin; A and β,γ &isin; C (or R). The dimension of the vector space
is called the dimension of the algebra. The algebra is called associative if
</p>
<p>dimension of the
</p>
<p>algebra; associativity;
</p>
<p>commutativity; identity;
</p>
<p>and right and left
</p>
<p>inverses
</p>
<p>the product satisfies a(bc)= (ab)c and commutative if it satisfies ab = ba.
An algebra with identity is an algebra that has an element 1 satisfying a1 =
1a = a. An element b of an algebra with identity is said to be a left inverse
of a if ba = 1. Right inverse is defined similarly. The identity is also called
unit, and an algebra with identity is also called a unital algebra.
</p>
<p>It is sometimes necessary to use a different notation for the identity of an
algebra. This happens especially when we are discussing several algebras at
the same time. A common notation other than 1 is e.
</p>
<p>1We shall, for the most part, abandon the Dirac bra-and-ket notation in this chapter due
to its clumsiness; instead we use boldface roman letters to denote vectors.
</p>
<p>S. Hassani, Mathematical Physics, DOI 10.1007/978-3-319-01195-0_3,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>63</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-01195-0_3">http://dx.doi.org/10.1007/978-3-319-01195-0_3</a></div>
</div>
<div class="page"><p/>
<p>64 3 Algebras
</p>
<p>3.1.1 General Properties
</p>
<p>Taking β = 1 =&minus;γ and b = c in the definition above leads immediately to
</p>
<p>a0 = 0a = 0 &forall;a &isin;A.
</p>
<p>The identity of an algebra is unique. If there were two identities 1 and e,
then 1e = e, because 1 is the identity, and 1e = 1, because e is the identity.
</p>
<p>If A is an associative algebra and a &isin; A has both a left inverse b and a
right inverse c, then the two are equal:
</p>
<p>bac = (ba)c = 1c = c,
bac = b(ac)= b1 = b.
</p>
<p>Therefore, in an associative algebra, we talk of an inverse without specifying
right or left. Furthermore, it is trivial to show that the (two-sided) inverse is
unique. Hence, we have
</p>
<p>Theorem 3.1.2 LetA be an associative algebra with identity. If a &isin;A has a
right and a left inverse, then they are equal and this single inverse is unique.
We denote it by a&minus;1. If a and b are invertible, then ab is also invertible, and
</p>
<p>(ab)&minus;1 = b&minus;1a&minus;1.
</p>
<p>The proof of the last statement is straightforward.
</p>
<p>Definition 3.1.3 Let A be an algebra and A&prime; a linear subspace of A. If A&prime;
subalgebra of an algebra
</p>
<p>is closed under multiplication, i.e., if ab &isin;A&prime; whenever a &isin;A&prime; and b &isin;A&prime;,
then A&prime; is called a subalgebra of A.
</p>
<p>Clearly, a subalgebra of an associative (commutative) algebra is also as-
sociative (commutative).
</p>
<p>Let A be an associative algebra and S a subset of A. The subalgebra
generated by S is the collection of all linear combinations of
</p>
<p>subalgebra generated by
</p>
<p>a subset s1s2 . . . sk, si &isin; S.
</p>
<p>If S consists of a single element s, then the subalgebra generated by s is the
set of polynomials in s.
</p>
<p>Example 3.1.4 Let A be a unital algebra, then the vector space
C is a subalgebra of any
</p>
<p>unital algebra. Span{1} = {α1 | α &isin;C}
</p>
<p>is a subalgebra of A. Since Span{1} is indistinguishable from C, we some-
times say that C is a subalgebra of A.
</p>
<p>Definition 3.1.5 Let A be an algebra. The set of elements of A which com-
mute with all elements of A is called the center of A and denoted by Z(A).center of an algebra</p>
<p/>
</div>
<div class="page"><p/>
<p>3.1 From Vector Space to Algebra 65
</p>
<p>Table 3.1 The multiplication table for S
</p>
<p>e0 e1 e2 e3
e0 e0 e1 e2 e3
e1 e1 e0 e3 e2
e2 e2 &minus;e3 &minus;e0 e1
e3 e3 &minus;e2 &minus;e1 e0
</p>
<p>Z(A) is easily shown to be a subspace of A, and if A is associative, then
Z(A) is a subalgebra of A.
</p>
<p>Definition 3.1.6 A unital algebra A is called central if Z(A)= Span{1}. central algebra
</p>
<p>Example 3.1.7 Consider the algebra S with basis {ei}3i=0 and multiplica-
tion table given in Table 3.1, where for purely aesthetic reasons the identity
has been denoted by e0.
</p>
<p>We want to see which elements belong to the center. Let a &isin; Z(S). Then
for any arbitrary element b &isin; S, we must have ab= ba. Let
</p>
<p>a=
3&sum;
</p>
<p>i=0
αiei and b=
</p>
<p>3&sum;
</p>
<p>i=0
βiei .
</p>
<p>Then a straightforward calculation shows that
</p>
<p>ab= (α0β0 + α1β1 &minus; α2β2 + α3β3)e0
+ (α0β1 + α1β0 + α2β3 &minus; α3β2)e1
+ (α0β2 + α1β3 + α2β0 &minus; α3β1)e2
+ (α0β3 + α1β2 &minus; α2β1 + α3β0)e3,
</p>
<p>with a similar expression for ba, in which αs and βs are switched. It is easy
to show that the two expressions are equal if and only if
</p>
<p>α2β3 = α3β2 and α1β3 = α3β1.
</p>
<p>This can hold for arbitrary b only if α1 = α2 = α3 = 0, with α0 arbitrary.
Therefore, a &isin; Z(S) if and only if a is a multiple of e0, i.e., if and only if
a &isin; Span{e0}. Therefore, S is central.
</p>
<p>Let A and B be subsets of an algebra A. We denote by AB the set of
elements in A which can be written as the sum of products of an element
in A by an element in B:
</p>
<p>AB &equiv;
{
</p>
<p>x &isin;A | x =
&sum;
</p>
<p>k
</p>
<p>akbk, ak &isin;A,bk &isin; B
}
. (3.1)
</p>
<p>In particular,
</p>
<p>A
2 &equiv;
</p>
<p>{
x &isin;A | x =
</p>
<p>&sum;
</p>
<p>k
</p>
<p>akbk, ak,bk &isin;A
}
</p>
<p>(3.2)</p>
<p/>
</div>
<div class="page"><p/>
<p>66 3 Algebras
</p>
<p>is called the derived algebra of A.
derived algebra
</p>
<p>Definition 3.1.8 Given any algebra A, in which (a,b) �&rarr; ab, we can obtain
Algebra opposite toA
</p>
<p>a second algebra Aop in which (a,b) �&rarr; ba. We write
</p>
<p>(ab)op = ba
</p>
<p>and call Aop the algebra opposite to A.
</p>
<p>It is obvious that if A is associative, so is Aop, and if A is commutative,
then Aop =A.
</p>
<p>Example 3.1.9 Here are some examples of algebra:
</p>
<p>&bull; Define the following product on R2:
</p>
<p>(x1, x2)(y1, y2)= (x1y1 &minus; x2y2, x1y2 + x2y1).
</p>
<p>The reader is urged to verify that this product turns R2 into a commuta-
tive algebra.
</p>
<p>&bull; Similarly, the vector (cross) product on R3 turns it into a nonassociative,
noncommutative algebra.
</p>
<p>&bull; The paradigm of all algebras is the matrix algebra whose binary oper-
ation is ordinary multiplication of n&times; n matrices. This algebra is asso-
ciative but not commutative.
</p>
<p>&bull; Let A be the set of n&times;n matrices. Define the binary operation, denoted
by &bull;, as
</p>
<p>A &bull; B &equiv; AB &minus; BA, (3.3)
where the RHS is ordinary matrix multiplication. The reader may check
that A together with this operation becomes a nonassociative, noncom-
mutative algebra.
</p>
<p>&bull; Let A be the set of n&times; n upper triangular matrices, i.e., matrices all of
whose elements below the diagonal are zero. With ordinary matrix mul-
tiplication, this set turns into an associative, noncommutative algebra,
as the reader can verify.
</p>
<p>&bull; Let A be the set of n&times; n upper triangular matrices. Define the binary
operation as in Eq. (3.3). The reader may check that A together with
this operation becomes a nonassociative, noncommutative algebra. The
derived algebra A2 of A is the set of n &times; n strictly upper triangular
matrices, i.e., upper triangular matrices whose diagonal elements are
all zero.
</p>
<p>&bull; We have already established that the set of linear transformations
L(V,W) from V to W is a vector space. Let us attempt to define a
multiplication as well. The best candidate is the composition of linear
transformations. If T : V&rarr;U and S :U&rarr;W are linear operators, then
the composition S◦T : V&rarr;W is also a linear operator, as can easily be
verified. This product, however, is not defined on a single vector space,
but is such that it takes an element in L(V,U) and another element in</p>
<p/>
</div>
<div class="page"><p/>
<p>3.1 From Vector Space to Algebra 67
</p>
<p>a second vector space L(U,W) to give an element in yet another vec-
tor space L(V,W). An algebra requires a single vector space. We can
accomplish this by letting V= U=W. Then the three spaces of linear
transformations collapse to the single space L(V,V), the set of endo-
morphisms of V, which we have abbreviated as L(V) or End(V) and to
which T, S, ST&equiv; S ◦ T, and TS&equiv; T ◦ S belong.
</p>
<p>&bull; All the examples above are finite-dimensional algebras. An example
of an infinite-dimensional algebra is Cr(a, b), the vector space of real-
valued functions defined on a real interval (a, b), which have derivatives
up to order r . The multiplication is defined pointwise: If f &isin; Cr (a, b)
and g &isin; Cr(a, b), then
</p>
<p>(fg)(t)&equiv; f (t)g(t) &forall;t &isin; (a, b).
</p>
<p>This algebra is commutative and associative, and has the identity ele-
ment f (t)= 1.
</p>
<p>&bull; Another example of an infinite dimensional algebra is the algebra of
polynomials.2 This algebra is a commutative and associative algebra
with identity.
</p>
<p>Definition 3.1.10 Let A and B be algebras. Then the vector direct sum
A&oplus;B becomes an algebra direct sum if we define the following product
</p>
<p>(a1 &oplus; b1)(a2 &oplus; b2)= (a1a2 &oplus; b1b2)
</p>
<p>on A&oplus;B.
</p>
<p>algebra direct sum
</p>
<p>Note that if an element a is in A, then it can be represented by a &oplus; 0
as an element of A&oplus; B. Similarly, an element b in B can be represented
by 0&oplus; b. Thus the product of any element in A with any element in B is
zero, i.e., AB = BA = {0}. As we shall see later, this condition becomes
necessary if a given algebra is to be the direct sum of its subalgebras.
</p>
<p>In order for a&oplus; b to be in the center of A&oplus;B, we must have
</p>
<p>(a&oplus; b)(x&oplus; y)= (x&oplus; y)(a&oplus; b),
</p>
<p>or
</p>
<p>ax&oplus; by= xa&oplus; yb or (ax&minus; xa)&oplus; (by&minus; yb)= 0,
for all x &isin;A and y &isin;B. For this to hold, we must have
</p>
<p>ax&minus; xa= 0 and by&minus; yb= 0,
</p>
<p>i.e., that a &isin; Z(A) and b &isin; Z(B). Hence,
</p>
<p>Z(A&oplus;B)= Z(A)&oplus;Z(B). (3.4)
</p>
<p>Definition 3.1.11 Let A and B be algebras. Then the vector space tensor algebra tensor product
</p>
<p>2It should be clear that the algebra of polynomials cannot be finite dimensional.</p>
<p/>
</div>
<div class="page"><p/>
<p>68 3 Algebras
</p>
<p>product A&otimes;B becomes an algebra tensor product if we define the product
</p>
<p>(a1 &otimes; b1)(a2 &otimes; b2)= a1a2 &otimes; b1b2
on A&otimes;B. Because of the isomorphism, A&otimes;B&sim;=B&otimes;A, we demand that
a&otimes; b= b&otimes; a for all a &isin;A and b &isin;B.
</p>
<p>The last condition of the definition becomes an important requirement
when we write a given algebra A as the tensor product of two of its subal-
gebras B and C. In such a case, &otimes; coincides with the multiplication in A,
and the condition becomes the requirement that all elements of B commute
with all elements of C, i.e., BC= CB.
</p>
<p>Definition 3.1.12 Given an algebra A and a basis B = {ei}Ni=1 for the un-
derlying vector space, one can write
</p>
<p>structure constants of an
</p>
<p>algebra
eiej =
</p>
<p>N&sum;
</p>
<p>k=1
ckijek, c
</p>
<p>k
ij &isin;C. (3.5)
</p>
<p>The complex numbers ckij , the components of the vector eiej in the basis B ,
are called the structure constants of A.
</p>
<p>The structure constants determine the product of any two vectors once
they are expressed in terms of the basis vectors of B . Conversely, given any
N -dimensional vector space V, one can turn it into an algebra by choosing a
basis and a set of N3 numbers {ckij } and defining the product of basis vectors
by Eq. (3.5).
</p>
<p>Example 3.1.13 Let the structure constants in algebras A and B be
{akij }Mi,j,k=1 and {blmn}Nl,m,n=1 in their bases {ei}Mi=1 and {fn}Nn=1, respectively.
So that
</p>
<p>eiej =
M&sum;
</p>
<p>i,j=1
akijek and fmfn =
</p>
<p>N&sum;
</p>
<p>m,n=1
blmnfl .
</p>
<p>Construct the MN dimensional algebra C by defining its structure constants
as cklim,jn = akijblmn in a basis {vkl}
</p>
<p>M,N
k,l=1, so that
</p>
<p>vimvjn =
M&sum;
</p>
<p>i,j=1
</p>
<p>N&sum;
</p>
<p>m,n=1
cklim,jnvkl =
</p>
<p>M&sum;
</p>
<p>i,j=1
</p>
<p>N&sum;
</p>
<p>m,n=1
akijb
</p>
<p>l
mnvkl .
</p>
<p>This algebra is isomorphic to the algebra A&otimes;B. In fact, if we identify vkl
on the right-hand side as ek &otimes; fl , then
</p>
<p>vimvjn =
M&sum;
</p>
<p>i,j=1
</p>
<p>N&sum;
</p>
<p>m,n=1
cklim,jnek &otimes; fl =
</p>
<p>M&sum;
</p>
<p>i,j=1
</p>
<p>N&sum;
</p>
<p>m,n=1
akijb
</p>
<p>l
mnek &otimes; fl
</p>
<p>=
(
</p>
<p>M&sum;
</p>
<p>i,j=1
akijek
</p>
<p>)
&otimes;
(
</p>
<p>N&sum;
</p>
<p>m,n=1
blmnfl
</p>
<p>)
= (eiej )&otimes; (fmfn),</p>
<p/>
</div>
<div class="page"><p/>
<p>3.1 From Vector Space to Algebra 69
</p>
<p>which is consistent with vim &equiv; ei &otimes; fm and vjn &equiv; ej &otimes; fn, and the rule of
multiplication of the tensor product of two algebras.
</p>
<p>Definition 3.1.14 A unital algebra all of whose nonzero elements have in-
verses is called a division algebra. division algebra
</p>
<p>Example 3.1.15 Let {e1, e2} be a basis of R2. Let the structure constants
be
</p>
<p>c111 =&minus;c122 = c212 = c221 = 1
</p>
<p>c112 =&minus;c121 = c211 = c222 = 0,
</p>
<p>i.e., let
</p>
<p>e21 =&minus;e22 = e1, e1e2 = e2e1 = e2.
Then, it is easy to prove that the algebra so constructed is just C. All that
needs to be done is to identify e1 with 1 and e2 with
</p>
<p>&radic;
&minus;1. Clearly, C is a
</p>
<p>division algebra.
</p>
<p>Example 3.1.16 In the standard basis {ei}3i=0 of R4, choose the structure
constants as follows:
</p>
<p>e20 =&minus;e21 =&minus;e22 =&minus;e23 = e0,
e0ei = eie0 = ei for i = 1,2,3,
</p>
<p>eiej =
3&sum;
</p>
<p>k=1
ǫijkek for i, j = 1,2,3, i �= j,
</p>
<p>where ǫijk is completely antisymmetric in all its indices (therefore vanishing
if any two of its indices are equal) and ǫ123 = 1. The reader may verify that
these relations turn R4 into an associative, but noncommutative, algebra.
This algebra is called the algebra of quaternions and denoted by H. In
this context, e0 is usually denoted by 1, and e1, e2, and e3 by i, j , and k,
respectively, and one writes q = x + iy + jz+ kw for an element of H. It
then becomes evident that H is a generalization of C. In analogy with C, x
is called the real part of q , and (y, z,w) the pure part of q . Similarly, the
conjugate of q is q&lowast; = x &minus; iy &minus; jz&minus; kw.
</p>
<p>algebra of quaternions
It is convenient to write q = x0+x, where x is a three-dimensional vector.
</p>
<p>Then q&lowast; = x0 &minus; x. Furthermore, one can show that, with q = x0 + x and
p = y0 + y,
</p>
<p>qp = x0y0 &minus; x &middot; y︸ ︷︷ ︸
real part of qp
</p>
<p>+x0y + y0x + x &times; y︸ ︷︷ ︸
pure part of qp
</p>
<p>. (3.6)
</p>
<p>Changing x to &minus;x and y to &minus;y in the expression above, one gets
</p>
<p>q&lowast;p&lowast; = x0y0 &minus; x &middot; y &minus; x0y &minus; y0x + x &times; y,
</p>
<p>which is not equal to (qp)&lowast;. However, it is easy to show that (qp)&lowast; = p&lowast;q&lowast;.</p>
<p/>
</div>
<div class="page"><p/>
<p>70 3 Algebras
</p>
<p>Substituting q&lowast; for p in (3.6), we get qq&lowast; = x20 + |x|2. The absolute
value of q , denoted by |q| is&mdash;similar to the absolute value of a complex
number&mdash;given by |q| = &radic;qq&lowast;. If q �= 0, then q&lowast;/(x20 + |x|2) is the inverse
of q . Thus, the algebra of quaternions is a division algebra.
</p>
<p>It is not hard to show that
</p>
<p>qn =
[n]/2&sum;
</p>
<p>k=0
(&minus;1)k
</p>
<p>(
n
</p>
<p>2k
</p>
<p>)
xn&minus;2k0 |x|2k +
</p>
<p>[n]/2&sum;
</p>
<p>k=0
(&minus;1)k
</p>
<p>(
n
</p>
<p>2k+ 1
</p>
<p>)
xn&minus;2k&minus;10 |x|2kx,
</p>
<p>(3.7)
where [n] = n if n is even and [n] = n&minus; 1 if n is odd.
</p>
<p>In order for a&otimes; b to be in the center of A&otimes;B, we must have
</p>
<p>(a&otimes; b)(x&otimes; y)= (x&otimes; y)(a&otimes; b),
</p>
<p>or
</p>
<p>ax&otimes; by= xa&otimes; yb
for all x &isin;A and y &isin;B. For this to hold, we must have
</p>
<p>ax= xa and by= yb,
</p>
<p>i.e., that a &isin; Z(A) and b &isin; Z(B). Hence,
</p>
<p>Z(A&otimes;B)= Z(A)&otimes;Z(B). (3.8)
</p>
<p>Let A be an associative algebra. A subset S &sub; A is called the genera-
tor of A if every element of A can be expressed as a linear combination
</p>
<p>Generator of an algebra
of the products of elements in S. A basis of the vector space A is clearly a
generator of A. However, it is not the smallest generator, because it may be
possible to obtain the entire basis vectors by multiplying a subset of them.
For example, (R3,&times;), the algebra of vectors under cross product, has the
basis {êx, êy, êz}, but {êx, êy}&mdash;or any other pair of unit vectors&mdash;is a gen-
erator because êz = êx &times; êy .
</p>
<p>3.1.2 Homomorphisms
</p>
<p>The linear transformations connecting vector spaces can be modified
slightly to accommodate the binary operation of multiplication of the corre-
sponding algebras:
</p>
<p>Definition 3.1.17 Let A and B be algebras. A linear map3 φ : A &rarr; B
is called an algebra homomorphism if φ(ab) = φ(a)φ(b). An injec-
tive, surjective, or bijective algebra homomorphism is called, respectively,
a monomorphism, an epimorphism, or an isomorphism. An isomorphism
of an algebra onto itself is called an automorphism.
</p>
<p>Homomorphism,
</p>
<p>monomorphism,
</p>
<p>epimorphism, and
</p>
<p>isomorphism of algebras
</p>
<p>3It is more common to use φ,ψ etc. instead of T,U, etc. for linear maps of algebras.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.1 From Vector Space to Algebra 71
</p>
<p>Example 3.1.18 Let A be R3, and B the set of 3 &times; 3 matrices of the form
</p>
<p>A =
</p>
<p>⎛
⎝
</p>
<p>0 a1 &minus;a2
&minus;a1 0 a3
a2 &minus;a3 0
</p>
<p>⎞
⎠ .
</p>
<p>Then the map φ :A&rarr;B defined by
</p>
<p>φ(a)= φ(a1, a2, a3)=
</p>
<p>⎛
⎝
</p>
<p>0 a1 &minus;a2
&minus;a1 0 a3
a2 &minus;a3 0
</p>
<p>⎞
⎠
</p>
<p>can be shown to be a linear isomorphism. Let the cross product be the binary
operation on A, turning it into an algebra. For B, define the binary operation
of Eq. (3.3). The reader may check that, with these operations, φ is extended
to an algebra isomorphism.
</p>
<p>Proposition 3.1.19 Let A and B be algebras. Let {ei} be a basis of A and
φ :A&rarr;B a linear transformation. Then φ is an algebra homomorphism if
and only if
</p>
<p>φ(eiej )= φ(ei)φ(ej ).
</p>
<p>Proof If a =&sum;i αiei and b =
&sum;
</p>
<p>j βj ej , then
</p>
<p>φ(ab)= φ
((&sum;
</p>
<p>i
</p>
<p>αiei
</p>
<p>)(&sum;
</p>
<p>j
</p>
<p>βj ej
</p>
<p>))
= φ
</p>
<p>(&sum;
</p>
<p>i
</p>
<p>αi
&sum;
</p>
<p>j
</p>
<p>βj eiej
</p>
<p>)
</p>
<p>=
&sum;
</p>
<p>i
</p>
<p>αi
&sum;
</p>
<p>j
</p>
<p>βjφ(eiej )=
&sum;
</p>
<p>i
</p>
<p>αi
&sum;
</p>
<p>j
</p>
<p>βjφ(ei)φ(ej )
</p>
<p>=
&sum;
</p>
<p>i
</p>
<p>αiφ(ei)
&sum;
</p>
<p>j
</p>
<p>βjφ(ej )= φ
(&sum;
</p>
<p>i
</p>
<p>αiei
</p>
<p>)
φ
</p>
<p>(&sum;
</p>
<p>j
</p>
<p>βj ej
</p>
<p>)
</p>
<p>= φ(a)φ(b).
</p>
<p>The converse is trivial. �
</p>
<p>Example 3.1.20 Let A and B be algebras and φ : A &rarr; B a homomor-
phism. Theorem 2.3.10 ensures that φ(A) is a subspace of B. Now let
b1,b2 &isin; φ(A). Then there exist a1,a2 &isin; A such that b1 = φ(a1) and
b2 = φ(a2). Furthermore,
</p>
<p>b1b2 = φ(a1)φ(a2)= φ(a1a2), &rArr; b1b2 &isin; φ(A).
</p>
<p>Hence, φ(A) is a subalgebra of B.
</p>
<p>Example 3.1.21 Let A be a real algebra with identity 1. Let φ :R&rarr;A be R (or C) is a subalgebra
of any unital algebra.the linear mapping given by φ(α) = α1. Considering R as an algebra over
</p>
<p>itself, we have
</p>
<p>φ(αβ)= αβ1 = (α1)(β1)= φ(α)φ(β).</p>
<p/>
</div>
<div class="page"><p/>
<p>72 3 Algebras
</p>
<p>This shows that φ is an algebra homomorphism. Furthermore,
</p>
<p>φ(α1)= φ(α2) &rArr; α11 = α21 &rArr; (α1 &minus; α2)1 = 0 &rArr; α1 = α2.
</p>
<p>Hence, φ is a monomorphism. Therefore, we can identify R with φ(R), and
consider R as a subalgebra of A. This is the same conclusion we arrived at
in Example 3.1.4.
</p>
<p>Definition 3.1.22 Let A and B be unital algebras. A homomorphism φ :
A&rarr;B is called unital if φ(1A)= 1B .
</p>
<p>One can show the following:
</p>
<p>Proposition 3.1.23 Let A and B be unital algebras. If φ : A &rarr; B is an
epimorphism, then φ is unital.
</p>
<p>Example 3.1.9 introduced the algebra L(V) of endomorphisms (opera-
tors) on V. This algebra has an identity 1 which maps every vector to itself.
</p>
<p>Definition 3.1.24 An endomorphism ω of V whose square is 1 is called an
involution
</p>
<p>involution.
</p>
<p>In particular, 1 &isin; End(V) is an involution. If ω1 and ω2 are involutions
such that ω1 ◦ω2 = ω2 ◦ω1, then ω1 ◦ω2 is also an involution.
</p>
<p>For an algebra, we require that an involution be a homomorphism, not
just a linear map. Let A be an algebra and let H(A) denote the set of homo-
</p>
<p>Involutions do not affect
</p>
<p>the identity of the
</p>
<p>algebra.
</p>
<p>morphisms of A. An involution ω &isin; H(A) satisfies ω ◦ ω = ι &isin; H(A), of
course.4 Now, if A has an identity e, then ω(e) must be equal to e. Indeed,
let ω(e)= a, then, since ω ◦ω= ι, we must have ω(a)= e and
</p>
<p>ω(ea)= ω(e)ω(a)= ω(e)e = ω(e)
</p>
<p>applying ω to both sides, we get ea = e. This can happen only if a = e.
</p>
<p>Theorem 3.1.25 Let U and V be two isomorphic vector spaces. Then the
algebras L(U) and L(V) are isomorphic as algebras.
</p>
<p>Proof Let φ : U&rarr; V be a vector-space isomorphism. Define Φ : L(U)&rarr;
L(V) by
</p>
<p>Φ(T)= φ ◦ T ◦ φ&minus;1.
It is easy to show that Φ is an algebra isomorphism. �
</p>
<p>A consequence of this theorem and Theorem 2.3.20 is that L(V), the al-
gebra of the linear transformations of any real vector space V, is isomorphic
to L(RN ), where N is the dimension of V. Similarly, L(V) is isomorphic to
L(CN ) if V is an N -dimensional complex vector space.
</p>
<p>4In keeping with our notation, we use ι for the identity homomorphism of the algebra A.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.2 Ideals 73
</p>
<p>3.2 Ideals
</p>
<p>Subalgebras are subspaces which are stable under multiplication of their
elements; i.e., the product of elements of a subalgebra do not leave the sub-
algebra. Of more importance in algebra theory are those subspaces which
are stable under multiplication of its elements by the entire algebra.
</p>
<p>Definition 3.2.1 Let A be an algebra. A subspace B of A is called a left
ideal of A if it contains ab for all a &isin; A and b &isin; B. Using Eq. (3.1), we
write this as AB&sub;B. A right ideal is defined similarly with BA&sub;B. A
two-sided ideal, or simply an ideal, is a subspace that is both a left ideal
and a right ideal.
</p>
<p>left, right, and two-sided
</p>
<p>ideals
</p>
<p>It is clear from the definition that an ideal is automatically a subalge-
bra, and that the only ideal of a unital algebra containing the identity, or an
</p>
<p>an ideal is a subalgebra
invertible element, is the algebra itself.
</p>
<p>Example 3.2.2 Let A be an associative algebra and a &isin;A. Let L(a) be the
left and right
</p>
<p>annihilators
set of elements x &isin;A such that xa = 0. For any x &isin;L(a) and any y &isin;A, we
have
</p>
<p>(yx)a = y(xa)= 0,
i.e., yx &isin;L(a). So, L(a) is a left ideal in A. It is called the left annihilator
of a. Similarly, one can construct R(a), the right annihilator of a.
</p>
<p>Example 3.2.3 Let Cr(a, b) be the algebra of all r times differentiable real-
valued functions on an interval (a, b) (see Example 3.1.9). The set of func-
tions that vanish at a given fixed point c &isin; (a, b) constitutes an ideal in
Cr (a, b). Since the algebra is commutative, the ideal is two-sided.
</p>
<p>More generally, let Mn be the (noncommutative) algebra of matrices with
entries fij &isin; Cr (a, b). Then the set of matrices whose entries vanish at a
given fixed point c &isin; (a, b) constitutes a two-sided ideal in Mn.
</p>
<p>Let A and B be algebras and φ : A &rarr; B a homomorphism. By Theo-
rem 2.3.9, kerφ is a subspace of A. Now let x &isin; kerφ and a &isin;A. Then
</p>
<p>φ(xa)= φ(x)φ(a)= 0φ(a)= 0,
</p>
<p>i.e., xa &isin; kerφ. This shows that kerφ is a right ideal in A. Similarly, one can
show that kerφ is a left ideal in A.
</p>
<p>Theorem 3.2.4 Let φ : A &rarr; B be a homomorphism of algebras.
Then kerφ is a (two-sided) ideal of A.
</p>
<p>One can easily construct left ideals for an associative algebra A: Take ideals generated by an
element of an
</p>
<p>associative algebra
</p>
<p>any element x &isin;A and consider the set
</p>
<p>Ax &equiv; {ax | a &isin;A}.</p>
<p/>
</div>
<div class="page"><p/>
<p>74 3 Algebras
</p>
<p>The reader may check that Ax is a left ideal. Similarly, xA is a right ideal,
and the set
</p>
<p>AxA&equiv; {axb | a,b &isin;A}
</p>
<p>is a two-sided ideal. These are all called left, right, and two-sided ideals
generated by x.
</p>
<p>Definition 3.2.5 A left (right, two-sided) ideal M of an algebra A is calledminimal left, right, and
two-sided ideals minimal if every left (right, two-sided) ideal of A contained in M coincides
</p>
<p>with M.
</p>
<p>Theorem 3.2.6 Let L be a left ideal of A. Then the following statements
are equivalent:
</p>
<p>(a) L is a minimal left ideal.
(b) Ax=L for all x &isin;L.
(c) Lx=L for all x &isin;L.
Similar conditions hold for a minimal right ideal.
</p>
<p>Proof The proof follows directly from the definition of ideals and minimal
ideals. �
</p>
<p>Theorem 3.2.7 Let A and B be algebras, φ :A&rarr;B an epimorphism, and
L a (minimal) left ideal of A. Then φ(L) is a (minimal) left ideal of B. In
particular, any automorphism of an algebra is an isomorphism among its
minimal ideals.
</p>
<p>Proof Let b be any element of B and y any element of φ(L). Then there
exist elements a and x of A and L, respectively, such that b = φ(a) and
y= φ(x). Furthermore,
</p>
<p>by= φ(a)φ(x)= φ(ax) &isin; φ(L)
</p>
<p>because ax &isin;L. Hence, φ(L) is an ideal in B.
Now suppose L is minimal. To show that φ(L) is minimal, we use (b) of
</p>
<p>Theorem 3.2.6. Since φ is an epimorphism, we have B= φ(A). Therefore,
let u &isin; φ(L). Then there exists t &isin;L such that u= φ(t) and
</p>
<p>Bu= φ(A)φ(t)= φ(At)= φ(L).
</p>
<p>The last statement of the theorem follows from the fact that kerφ is an
ideal of A. �
</p>
<p>Definition 3.2.8 A is the direct sum of its subalgebras B and C if A=B&oplus;algebra direct sums
C as a vector space and BC = CB = {0}. B and C are called components
of A. Obviously, an algebra can have several components. An algebra is
called reducible if it is the direct sum of subalgebras.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.2 Ideals 75
</p>
<p>Table 3.2 The multiplication table for S
</p>
<p>f1 f2 f3 f4
f1 f1 f2 0 0
</p>
<p>f2 0 0 f2 f1
f3 0 0 f3 f4
f4 f4 f3 0 0
</p>
<p>As we saw in Definition 3.1.10, the condition BC = CB = {0} is nec-
essary if B and C are to be naturally identified as B &oplus; {0} and C &oplus; {0},
respectively.
</p>
<p>Proposition 3.2.9 A central algebra is not reducible.
</p>
<p>Proof Suppose that the (necessarily unital) central algebra A is reducible.
Then the identity has components in each of the subalgebras of which A
is composed. Clearly, these components are linearly independent and all
belong to the center. This is a contradiction. �
</p>
<p>Example 3.2.10 Consider S, the algebra introduced in Example 3.1.7. Con-
struct a new basis {fi}4i=1 as follows:5
</p>
<p>f1 =
1
</p>
<p>2
(e0 + e3), f2 =
</p>
<p>1
</p>
<p>2
(e1 &minus; e2),
</p>
<p>f3 =
1
</p>
<p>2
(e0 &minus; e3), f4 =
</p>
<p>1
</p>
<p>2
(e1 + e2).
</p>
<p>(3.9)
</p>
<p>The multiplication table for S in terms of the new basis vectors is given in
Table 3.2, as the reader may verify.
</p>
<p>Multiplying both sides of the identity e0 = f1+ f3 by an arbitrary element
of S, we see that any such element can be written as a vector in the left ideal
L1 &equiv; Sf1 plus a vector in the left ideal L3 &equiv; Sf3. Any vector in L1 can be
written as a product of some vector in S and f1. Let a =
</p>
<p>&sum;4
i=1 αifi be an
</p>
<p>arbitrary element of S. Then any vector in L1 is of the form
</p>
<p>af1 = (α1f1 + α2f2 + α3f3 + α4f4)f1 = α1f1 + α4f4,
</p>
<p>i.e., that f1 and f4 span L1. Similarly, f2 and f3 span L3. It follows that
L1 &cap;L3 = {0}. Therefore, we have
</p>
<p>S=L1 &oplus;V L3, L1 = Span{f1, f4}, L3 = Span{f2, f3},
</p>
<p>where &oplus;V indicates a vector space direct sum. Note that there is no contra-
diction between this direct sum decomposition and the fact that S is central
because the direct sum above is not an algebra direct sum since L1L3 �= {0}.
</p>
<p>5The reader is advised to show that {fi}4i=1 is a linearly independent set of vectors.</p>
<p/>
</div>
<div class="page"><p/>
<p>76 3 Algebras
</p>
<p>Let x= γ1f1 + γ4f4 be an arbitrary nonzero element of L1. Then clearly,
Sx&sube; L1. To show that L1 &sube; Sx, let y= β1f1 + β4f4 be in L1. Can we find
z &isin; S such that y= zx? Let z=&sum;4i=1 ηifi and note that
</p>
<p>zx= (η1f1 + η2f2 + η3f3 + η4f4)(γ1f1 + γ4f4)
= (η1γ1 + η2γ4)f1 + (η3γ4 + η4γ1)f4.
</p>
<p>We are looking for a set of η&rsquo;s satisfying
</p>
<p>η1γ1 + η2γ4 = β1 and η3γ4 + η4γ1 = β4.
</p>
<p>If γ1 �= 0, then η1 = β1/γ1, η2 = 0 = η3, η4 = β4/γ1 yields a solution for z.
If γ4 �= 0, then η2 = β1/γ4, η1 = 0 = η4, η3 = β4/γ4 yields a solution for z.
Therefore, L1 = Sx, and by Theorem 3.2.6, L1 is minimal. Similarly, L3 is
also minimal.
</p>
<p>If A=B&oplus; C, then multiplying both sides on the right by B, we get
</p>
<p>AB=BB&oplus; CB=BB&oplus; {0} =BB&sub;B,
</p>
<p>showing that B is a left ideal of A. Likewise, multiplying on the left leads
to the fact that B is a right ideal of A. Thus it is an ideal of A. Similarly, C
is an ideal of A. Moreover, since the subalgebras do not share any nonzero
elements, any other ideal of A must be contained in the subalgebras. We
thus have
</p>
<p>Proposition 3.2.11 IfA is the direct sum of algebras, then each component
(or the direct sum of several components) is an ideal ofA. Furthermore, any
other ideal of A is contained entirely in one of the components.
</p>
<p>Algebras which have no proper ideals are important in the classificationsimple algebra
of all algebras.
</p>
<p>Definition 3.2.12 An algebra A is called simple if its only ideals are
A and {0}.
</p>
<p>Recall that by ideal we mean two-sided ideal. Therefore, a simple algebra
can have proper left ideals and proper right ideals. In fact, the following
example illustrates this point.
</p>
<p>Example 3.2.13 Let&rsquo;s go back to algebra S of Example 3.2.10, where we
saw that S=L1 &oplus;V L3 in which L1 and L3 are minimal left ideals and &oplus;V
indicates direct sum of vector spaces. Can S have a proper two-sided ideal?
Let I be such an ideal and let a &isin; I be nonzero. By the decomposition of
S, a = a1 + a3 with a1 &isin; L1 and a3 &isin; L3, at least one of which must be
nonzero. Suppose a1 �= 0. Then Sa1 is a nonzero left ideal which is con-
tained in L1. Since L1 is minimal, Sa1 =L1. Since f1 &isin;L1 there must exist</p>
<p/>
</div>
<div class="page"><p/>
<p>3.2 Ideals 77
</p>
<p>b &isin; S such that ba1 = f1, and hence,
</p>
<p>ba= ba1 + ba3 = f1 + ba3.
</p>
<p>Multiplying both sides on the right by f1 and noting that f21 = f1 and L3f1 =
{0} by the multiplication table of Example 3.2.10, we obtain baf1 = f1.
Since I is a two-sided ideal and a &isin; I, baf1 &isin; I, and therefore, f1 &isin; I.
</p>
<p>The equality Sa1 = L1, also implies that there exists c &isin; S such that
ca1 = f4, and hence,
</p>
<p>ca= ca1 + ca3 = f4 + ca3.
</p>
<p>Multiplying both sides on the right by f1 and noting that f4f1 = f4 and
L3f1 = {0}, we obtain caf1 = f4. Since I is a two-sided ideal, we must have
f4 &isin; I. Since f1f2 = f2 and f4f2 = f3, all the basis vectors are in I. Hence,
I = S. The case where a3 �= 0 leads to the same conclusion. Therefore, S
has no proper ideal, i.e., S is simple.
</p>
<p>An immediate consequence of Definition 3.2.12 and Theorem 3.2.4 is
</p>
<p>Proposition 3.2.14 A nontrivial homomorphism of a simple algebraA with
any other algebra B is necessarily injective.
</p>
<p>Proof For any φ :A&rarr;B, the kernel of φ is an ideal of A. Since A has no
proper ideal, kerφ = A or kerφ = {0}. If φ is nontrivial, then kerφ = {0},
i.e., φ is injective. �
</p>
<p>3.2.1 Factor Algebras
</p>
<p>Let A be an algebra and B a subspace of A. Section 2.1.2 showed how to
construct the factor space A/B. Can this space be turned into an algebra?
Let ❏a❑ and ❏a&prime;❑ be in A/B. Then the natural product rule for making A/B
an algebra is
</p>
<p>❏a❑❏a&prime;❑= ❏aa&prime;❑. (3.10)
Under what conditions does this multiplication make sense? Since ❏a❑ =
❏a + b❑ and ❏a&prime;❑= ❏a&prime; + b&prime;❑ for all b,b&prime; &isin;B, for (3.10) to make sense, we
must have
</p>
<p>(a + b)
(
a&prime; + b&prime;
</p>
<p>)
= aa&prime; + b&prime;&prime;
</p>
<p>for some b&prime;&prime; in B. Taking a = 0 = a&prime; yields bb&prime; = b&prime;&prime;. This means that B
must be a subalgebra of A. Taking a&prime; = 0 yields ab&prime; + bb&prime; = b&prime;&prime; for all
a &isin; A, b,b&prime; &isin; B and some b&prime;&prime; &isin; B. This means that B must be a left ideal
of A. Similarly, by setting a = 0 we conclude that B must be a right ideal of
A. We thus have
</p>
<p>Proposition 3.2.15 Let A be an algebra and B a subspace of A. Then
the factor space A/B can be turned into an algebra with multiplication
❏a❑❏a&prime;❑= ❏aa&prime;❑, if and only ifB is an ideal inA. The algebra so constructed
is called the factor algebra of A with respect to the ideal B. factor algebra</p>
<p/>
</div>
<div class="page"><p/>
<p>78 3 Algebras
</p>
<p>Example 3.2.16 Let A and B be algebras and φ : A&rarr; B an algebra ho-
momorphism. Example 3.1.20 and Theorem 3.2.4 showed that φ(A) is a
subalgebra of B and kerφ is an ideal in A. Now consider the linear map
φ̄ : A/kerφ &rarr; φ(A) defined in Example 2.3.22 by φ̄(❏a❑) = φ(a). It is
straightforward to show that φ̄ is an algebra homomorphism. Using this and
Example 2.3.22 where it was shown that φ̄ is a linear isomorphism, we
conclude that φ̄ is an algebra isomorphism.
</p>
<p>3.3 Total Matrix Algebra
</p>
<p>Consider the vector space of n&times;n matrices with its standard basis {eij }ni,j=1,
where eij has a 1 at the ij th position and zero everywhere else. This means
that (eij )lk = δilδjk , and
</p>
<p>(eij ekl)mn =
n&sum;
</p>
<p>r=1
(eij )mr(ekl)rn
</p>
<p>=
n&sum;
</p>
<p>r=1
δimδjrδkrδln = δimδjkδln = δjk(eil)mn,
</p>
<p>or
</p>
<p>eijekl = δjkeil .
The structure constants are cmnij,kl = δimδjkδln. Note that one needs a double
index to label these constants.
</p>
<p>The abstract algebra whose basis is {eij }ni,j=1 with multiplication rules
and structure constants given above is called the total matrix algebra. Lettotal matrix algebra
</p>
<p>F&otimes;Mn orMn(F) F denote either R or C. Then the total matrix algebra over F is denoted by
F&otimes;Mn or Mn(F). It is an associative algebra isomorphic with the real or
complex matrix algebra, but its elements are not necessarily n&times; n matrices.
When the dimension of the matrices is not specified, one writes simply F&otimes;
M or M(F).
</p>
<p>We now construct a left ideal of this algebra. Take epq and multiply it on
the left by
</p>
<p>&sum;n
i,j=1 αij eij , a general element of Mn(F). This yields
</p>
<p>(
n&sum;
</p>
<p>i,j=1
αij eij
</p>
<p>)
epq =
</p>
<p>n&sum;
</p>
<p>i,j=1
αij eijepq =
</p>
<p>n&sum;
</p>
<p>i,j=1
αij δjpeiq =
</p>
<p>n&sum;
</p>
<p>i=1
αipeiq ,
</p>
<p>which corresponds to a matrix all of whose columns are zero except the qth
column. Let L be the set of all such matrices. Multiplying an element of L
by a general matrix
</p>
<p>&sum;n
l,m=1 βlmelm, we obtain
</p>
<p>6
</p>
<p>(
n&sum;
</p>
<p>l,m=1
βlmelm
</p>
<p>)(
n&sum;
</p>
<p>i=1
γieiq
</p>
<p>)
=
</p>
<p>n&sum;
</p>
<p>i,l,m=1
βlmγielmeiq =
</p>
<p>n&sum;
</p>
<p>i,l,m=1
βlmγiδmielq
</p>
<p>6The index p has no significance in the final answer because all the epq with varying p
but a fixed q generate the same matrices.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.3 Total Matrix Algebra 79
</p>
<p>=
n&sum;
</p>
<p>l,m=1
βlmγmelq =
</p>
<p>n&sum;
</p>
<p>l=1
</p>
<p>(
n&sum;
</p>
<p>m=1
βlmγm
</p>
<p>)
</p>
<p>︸ ︷︷ ︸
&equiv;ηl
</p>
<p>elq
</p>
<p>=
n&sum;
</p>
<p>l=1
ηlelq .
</p>
<p>It follows that L is a left ideal. Furthermore, the very construction of L
implies that it satisfies condition (b) of Theorem 3.2.6. Had we multiplied
epq on the right, we would have obtained a right ideal consisting of matrices
all of whose rows equaled zero except the pth row; and this right ideal would
satisfy condition (b) of Theorem 3.2.6 for right minimal ideals. We thus have
</p>
<p>Theorem 3.3.1 The minimal left (right) ideals of R&otimes;M or C&otimes;M
consist of matrices with all their columns (rows) zero except one.
</p>
<p>Multiplying epq on the left and the right by a pair of arbitrary matrices,
the reader can easily show that one recovers the entire total matrix algebra.
This indicates that the algebra has no proper two-sided ideal. Example 3.3.3
below finds the center of Mn(F) to be Span{1n}, where 1n is the identity of
Mn(F). We thus have
</p>
<p>Theorem 3.3.2 The total matrix algebra Mn(F) is central simple.
</p>
<p>Example 3.3.3 Let a=&sum;ni,j=1 αij eij be in the center of F&otimes;Mn. Then finding the center of
F&otimes;Mn
</p>
<p>aekl =
n&sum;
</p>
<p>i,j=1
αij eijekl =
</p>
<p>n&sum;
</p>
<p>i,j=1
αij δjkeil =
</p>
<p>n&sum;
</p>
<p>i=1
αikeil
</p>
<p>ekla=
n&sum;
</p>
<p>i,j=1
eklαijeij =
</p>
<p>n&sum;
</p>
<p>i,j=1
αij δilekj =
</p>
<p>n&sum;
</p>
<p>j=1
αlj ekj .
</p>
<p>For these two expressions to be equal, we must have
</p>
<p>n&sum;
</p>
<p>i=1
(αikeil &minus; αlieki)= 0.
</p>
<p>By letting l = k in the sum above and invoking the linear independence
of eij , we conclude that αik = 0 if i �= k. Therefore, a must be a diagonal
matrix. Write a = &sum;nk=1 λkekk and let b =
</p>
<p>&sum;n
i,j=1 βij eij be an arbitrary
</p>
<p>element of F&otimes;Mn. Then
</p>
<p>ab=
n&sum;
</p>
<p>i,j,k=1
λkβij ekkeij =
</p>
<p>n&sum;
</p>
<p>i,j,k=1
λkβij δikekj =
</p>
<p>n&sum;
</p>
<p>i,j=1
λiβijeij</p>
<p/>
</div>
<div class="page"><p/>
<p>80 3 Algebras
</p>
<p>ba=
n&sum;
</p>
<p>i,j,k=1
λkβijeij ekk =
</p>
<p>n&sum;
</p>
<p>i,j,k=1
λkβij δjkeik =
</p>
<p>n&sum;
</p>
<p>i,j=1
λjβij eij .
</p>
<p>Again, because of the linear independence of eij , for these two expressions
to be equal, we must have λjβij = λiβij for all i and j and all βij . The only
way this can happen is for λi to be equal to λj for all i and j . It follows that
a= λ1n, where 1n =
</p>
<p>&sum;n
k=1 ekk is the identity element of Mn(F). Therefore,
</p>
<p>Mn(F) is central.
</p>
<p>3.4 Derivation of an Algebra
</p>
<p>The last two items in Example 3.1.9 have a feature that turns out to be of
great significance in all algebras, the product rule for differentiation.
</p>
<p>Definition 3.4.1 A vector space endomorphism D : A &rarr; A is called a
derivation on A if it has the additional propertyderivation
</p>
<p>D(ab)=
[
D(a)
</p>
<p>]
b + a
</p>
<p>[
D(b)
</p>
<p>]
.
</p>
<p>Example 3.4.2 Let Cr (a, b) be as in Example 3.1.9, and let D be ordinary
differentiation: D : f �&rarr; f &prime; where f &prime; is the derivative of f . Then ordinary
differentiation rules show that D is a derivation of the algebra Cr (a, b).
</p>
<p>Example 3.4.3 Consider the algebra of n&times; n matrices with multiplication
as defined in Eq. (3.3). Let A be a fixed matrix, and define the linear trans-
formation
</p>
<p>DA(B)= A &bull; B.
Then we note that
</p>
<p>DA(B &bull; C)= A &bull; (B &bull; C)= A(B &bull; C)&minus; (B &bull; C)A
= A(BC &minus; CB)&minus; (BC &minus; CB)A
= ABC &minus; ACB &minus; BCA + CBA.
</p>
<p>On the other hand,
</p>
<p>(DAB) &bull; C + B &bull; (DAC)= (A &bull; B) &bull; C + B &bull; (A &bull; C)
= (AB &minus; BA) &bull; C + B &bull; (AC &minus; CA)
= (AB &minus; BA)C &minus; C(AB &minus; BA)+ B(AC &minus; CA)
&minus; (AC &minus; CA)B
</p>
<p>= ABC + CBA &minus; BCA &minus; ACB.
</p>
<p>So, DA is a derivation on A.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.4 Derivation of an Algebra 81
</p>
<p>Theorem 3.4.4 Let {ei}Ni=1 be a basis of the algebraA. Then a vector space
endomorphism D :A&rarr;A is a derivation on A iff
</p>
<p>D(eiej )=D(ei) &middot; ej + ei &middot;D(ej ) for i, j = 1,2, . . . ,N.
</p>
<p>Proof The simple proof is left as an exercise for the reader. �
</p>
<p>If A has an identity e, then D(e)= 0, because
D(e)=D(ee)=D(e)e + eD(e)= 2D(e).
</p>
<p>This shows that e &isin; kerD. In general, one can show that kerD is a subalgebra
of A.
</p>
<p>Proposition 3.4.5 Every derivation D satisfies the Leibniz formula Leibniz formula
</p>
<p>Dn(ab)=
n&sum;
</p>
<p>k=0
</p>
<p>(
n
</p>
<p>k
</p>
<p>)
Dk(a) &middot;Dn&minus;k(b). (3.11)
</p>
<p>Proof The proof by mathematical induction is very similar to the proof of
the binomial theorem of Example 1.5.2. The details are left as an exercise
for the reader. �
</p>
<p>Derivations of A, being endomorphisms of the vector space A, are sub-
sets of End(A). If D1 and D2 are derivations, then it is straightforward to
show that any linear combination αD1 + βD2 is also a derivation. Thus, the
set of derivations D(A) on an algebra A forms a vector space [a subspace
of End(A)]. Do they form a subalgebra of End(A)? Is D1D2 a derivation?
Let&rsquo;s find out!
</p>
<p>D1D2(ab)=D1
([
D2(a)
</p>
<p>]
b + a
</p>
<p>[
D2(b)
</p>
<p>])
</p>
<p>=
[
D1D2(a)
</p>
<p>]
b +D2(a)D1(b)+D1(a)D2(b)+ a
</p>
<p>[
D1D2(b)
</p>
<p>]
.
</p>
<p>So, the product of two derivations is not a derivation, because of the two
terms in the middle. However, since these terms are symmetric in their sub-
scripts, we can subtract them away by taking the difference D1D2 &minus; D2D1.
The question is whether the result will be a derivation. Switching the order
of the subscripts, we obtain
</p>
<p>D2D1(ab)=
[
D2D1(a)
</p>
<p>]
b +D1(a)D2(b)+D2(a)D1(b)+ a
</p>
<p>[
D2D1(b)
</p>
<p>]
.
</p>
<p>Subtracting this from the previous expression yields
</p>
<p>(D1D2 &minus;D2D1)(ab)
=
[
D1D2(a)
</p>
<p>]
b + a
</p>
<p>[
D1D2(b)
</p>
<p>]
&minus;
[
D2D1(a)
</p>
<p>]
b &minus; a
</p>
<p>[
D2D1(b)
</p>
<p>]
</p>
<p>=
[
(D1D2 &minus;D2D1)(a)
</p>
<p>]
b + a
</p>
<p>[
(D1D2 &minus;D2D1)(b)
</p>
<p>]
.
</p>
<p>Thus, if we define a new product
</p>
<p>D1 &bull;D2 &equiv; D1D2 &minus;D2D1, (3.12)
then D(A) becomes an algebra.</p>
<p/>
</div>
<div class="page"><p/>
<p>82 3 Algebras
</p>
<p>Theorem 3.4.6 The set D(A) of derivations of A forms an algebra, the
derivation algebra of A under the product (3.12).derivation algebra
</p>
<p>Definition 3.4.7 Let A and B be algebras, and φ : A &rarr; B a homomor-
phism. Then D :A&rarr;B is called a φ-derivation ifφ-derivation
</p>
<p>D(a1a2)= D(a1)φ(a2)+ φ(a1)D(a2), a1,a2 &isin;A.
</p>
<p>Example 3.4.8 As an example, let DA be a derivation in A. Then D= φ ◦
DA is a φ-derivation, because
</p>
<p>φ ◦DA(a1a2)= φ
[
DA(a1)a2 + a1DA(a2)
</p>
<p>]
</p>
<p>= φ
[
DA(a1)
</p>
<p>]
φ(a2)+ φ(a1)φ
</p>
<p>[
DA(a2)
</p>
<p>]
</p>
<p>= φ ◦DA(a1)φ(a2)+ φ(a1)φ ◦DA(a2).
</p>
<p>Similarly, if DB is a derivation in B, then DB ◦ φ is a φ-derivation.
More specifically, let A be the algebra Cr(a, b) of r-time differentiable
</p>
<p>functions, and B be the algebra R of real numbers. Let φc : Cr(a, b)&rarr; R
be the evaluation at a fixed point c &isin; (a, b), so that φc(f ) = f (c). If Dc :
Cr (a, b)&rarr; R is defined as Dc(f ) = f &prime;(c), then one can readily show that
Dc is a φc-derivation.
</p>
<p>Definition 3.4.9 Let A be an algebra with identity and ω an involution
of A. A linear transformation ��� &isin; L(A) is called an antiderivation of Aantiderivation
with respect to ω if
</p>
<p>���(a1a2)=���(a1) &middot; a2 +ω(a1) &middot;���(a2).
</p>
<p>In particular, a derivation is an antiderivation with respect to to the identity.
</p>
<p>As in the case of the derivation, one can show that ker��� is a subalgebra
of A, ���(e) = 0 if A has an identity e, and ��� is determined entirely by its
action on the generators of A.
</p>
<p>Theorem 3.4.10 Let ���1 and ���2 be antiderivations with respect to two in-
volutions ω1 and ω2. Suppose that ω1 ◦ω2 = ω2 ◦ω1. Furthermore assume
that
</p>
<p>ω1���2 =&plusmn;���2ω1 and ω2���1 =&plusmn;���1ω2.
Then ���1���2 ∓ ���2���1 is an antiderivation with respect to the involution
ω1 ◦ω2.
</p>
<p>Proof The proof consists of evaluating���1���2∓���2���1 using Definition 3.4.9
for ���1 and ���2. We leave the straightforward proof for the reader. �
</p>
<p>Some particular cases of this theorem are of interest:
</p>
<p>&bull; Let ��� be an antiderivation with respect to ω and D a derivation such
that ωD=Dω. Then D���&minus;���D is an antiderivation with respect to ω.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.5 Decomposition of Algebras 83
</p>
<p>&bull; Let ���1 and ���2 be antiderivations with respect to the same involution ω
such that ω���i =&minus;���iω for i = 1,2. Then���1���2+���2���1 is a derivation.
</p>
<p>&bull; A particular example of the second case is when ��� is an antiderivation
with respect to an involution ω such that ω��� = &minus;���ω. Then ���2 is a
derivation.
</p>
<p>3.5 Decomposition of Algebras
</p>
<p>In Sect. 2.1.3, we decomposed a vector space into smaller vector spaces.
The decomposition of algebras into &ldquo;smaller&rdquo; algebras is also useful. In this
section we investigate properties and conditions which allow such a decom-
position. All algebras in this section are assumed to be associative.
</p>
<p>Definition 3.5.1 A nonzero element a &isin; A is called nilpotent if ak = 0
for some positive integer k. The smallest such integer is called the index
of a. A subalgebra B of A is called nil if all elements of B are nilpotent.
B is called nilpotent of index ν if Bν = {0} and Bν&minus;1 �= {0}.7 A nonzero
element P &isin;A is called idempotent if P2 = P.
</p>
<p>nilpotent, index, nil, and
</p>
<p>idempotent
</p>
<p>Proposition 3.5.2 The identity element is the only idempotent in a division
algebra.
</p>
<p>Proof The proof is trivial. �
</p>
<p>If P is an idempotent, then Pk = P for any positive integer k. Therefore,
a nilpotent subalgebra cannot contain an idempotent.
</p>
<p>The following theorem, whose rather technical proof can be found in
[Bly 90, p. 191], is very useful:
</p>
<p>Theorem 3.5.3 A nil ideal is nilpotent.
</p>
<p>Example 3.5.4 The set of n&times; n upper triangular matrices is a subalgebra
of the algebra of n&times;n matrices, because the product of two upper triangular
matrices is an upper triangular matrix, as can be easily verified.
</p>
<p>A strictly upper triangular matrix is nilpotent. Let&rsquo;s illustrate this for a
4 &times; 4 matrix. With
</p>
<p>A =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>0 a12 a13 a14
0 0 a23 a24
0 0 0 a34
0 0 0 0
</p>
<p>⎞
⎟⎟⎠ ,
</p>
<p>7Recall that Bk is the collection of products a1 . . .ak of elements in B.</p>
<p/>
</div>
<div class="page"><p/>
<p>84 3 Algebras
</p>
<p>it is easily seen that
</p>
<p>A2 =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>0 0 a12a23 a12a24 + a13a34
0 0 0 a23a34
0 0 0 0
0 0 0 0
</p>
<p>⎞
⎟⎟⎠ ,
</p>
<p>A3 =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>0 0 0 a12a23a34
0 0 0 0
0 0 0 0
0 0 0 0
</p>
<p>⎞
⎟⎟⎠ ,
</p>
<p>and
</p>
<p>A4 =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>0 0 0 0
0 0 0 0
0 0 0 0
0 0 0 0
</p>
<p>⎞
⎟⎟⎠ .
</p>
<p>Thus, the strictly upper triangular 4&times; 4 matrices are nilpotent of index 4. In
fact, one can show that the subalgebra of the strictly upper triangular 4 &times; 4
matrices has index 4.
</p>
<p>The reader can convince him/herself that strictly upper triangular n&times; n
matrices are nilpotent of index n, and that the subalgebra of the strictly upper
triangular n&times; n matrices is nilpotent of index n.
</p>
<p>3.5.1 The Radical
</p>
<p>Nilpotent subalgebras play a fundamental role in the classification of alge-
bras. It is remarkable that all the left, right, and two-sided nilpotent ideals of
an algebra are contained is a single nilpotent ideal, which we shall explore
now.
</p>
<p>Lemma 3.5.5 Let L and M be two nilpotent left (right) ideals of the alge-
bra A. Let λ and μ be the indices of L and M, respectively. Then L+M is
a left (right) ideal of A of index at most λ+μ&minus; 1.
</p>
<p>Proof We prove the Lemma for left ideals. Clearly, L+M is a left ideal.
Any element of L +M raised to the kth power can be written as a linear
combination of elements of the form a1a2 . . .ak with ai belonging to either
L or M. Suppose that l terms of this product are in L and m terms in M.
Let j be the largest integer such that aj &isin; L. Starting with aj move to the
left until you reach another element of L, say ar . All the terms ar+1 to aj&minus;1
are in M. Since L is a left ideal,
</p>
<p>ar+1 . . .aj&minus;1︸ ︷︷ ︸
&isin;A
</p>
<p>aj &equiv; a&prime;j &isin;L.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.5 Decomposition of Algebras 85
</p>
<p>This contracts the product arar+1 . . .aj&minus;1aj to ara&prime;j with both factors in L.
Continuing this process, we obtain
</p>
<p>a1a2 . . .ak = b1b2 . . .blc, bi &isin;L, c &isin;M.
</p>
<p>Similarly,
</p>
<p>a1a2 . . .ak = c1c2 . . . cmb, b &isin;L, ci &isin;M.
Since k = l+m, if k = μ+λ&minus;1, then (μ&minus;m)+(λ&minus; l)= 1. This shows that
if m&lt; μ, then l &ge; λ and if l &lt; λ, then m&ge; μ. In either case, a1 . . .ak = 0,
by one of the last two equations above. Hence, L+M is nilpotent with an
index of at most μ+ λ&minus; 1. The proof for the right ideals is identical to this
proof. �
</p>
<p>Lemma 3.5.6 Let L be a nilpotent left ideal of the algebraA. Then the sum
I=L+LA is a nilpotent two-sided ideal.
</p>
<p>Proof Since L is a left ideal, AL&sube;L. Therefore,
</p>
<p>AI=AL+ALA&sube;L+LA= I,
</p>
<p>showing that I is a left ideal. On the other hand,
</p>
<p>IA=LA+LAA&sube;LA+LA=LA&sub; I
</p>
<p>showing that I is a right ideal.
Now consider a product of k elements of LA:
</p>
<p>l1a1l2a2 . . . lkak = l1l&prime;2l&prime;3 . . . l&prime;kak, lj &isin;L, aj &isin;A
</p>
<p>where l&prime;i &equiv; ai&minus;1li &isin; L. This shows that if k is equal to the index of L, then
the product is zero and hence, LA is nilpotent. Note that since some of the
a&rsquo;s may be in L, the index of LA is at most equal to the index of L. Invoking
Lemma 3.5.5 completes the proof. �
</p>
<p>The preceding two lemmas were introduced for the following:
</p>
<p>Theorem 3.5.7 There exists a unique nilpotent ideal in A which contains
every nilpotent left, right, and two-sided ideal of A.
</p>
<p>Proof Let N be a nilpotent ideal of maximum dimension. Let M be any
nilpotent ideal. By Lemma 3.5.5, N+M is both a left and a right nilpotent
ideal, hence, a nilpotent ideal. By assumption N +M &sub; N, and therefore,
M &sub; N, proving that N contains all ideals. If there were another maximal
ideal N&prime;, then N&prime; &sub; N and N &sub; N&prime;, implying that N&prime; = N, and that N is
unique.
</p>
<p>If L is a left nilpotent ideal, then by Lemma 3.5.6, L&sub; I=L+LA&sub;N,
because I is an ideal. Thus, N contains all the nilpotent left ideals. Similarly,
N contains all the nilpotent right ideals. �</p>
<p/>
</div>
<div class="page"><p/>
<p>86 3 Algebras
</p>
<p>Definition 3.5.8 The unique maximal ideal of an algebra A guar-
anteed by Theorem 3.5.7 is called the radical of A and denoted by
Rad(A).
</p>
<p>We have seen that a nilpotent algebra cannot contain an idempotent. In
</p>
<p>radical of an algebra
</p>
<p>fact, the reverse implication is also true. To show that, we need the following
</p>
<p>Lemma 3.5.9 Suppose that A contains an element a such that Aak =
Aak&minus;1 for some positive integer k. Then A contains an idempotent.
</p>
<p>Proof Let B&equiv;Aak&minus;1. Then B is a left ideal of A satisfying Ba=B. Mul-
tiplying both sides by a, we see that
</p>
<p>Ba2 =Ba=B, Ba3 =Ba=B,
</p>
<p>and Bak =B. But ak &isin;B because B&equiv;Aak&minus;1. Thus, with b= ak , we get
Bb=B. This means that there must exist an element P &isin;B such that Pb=
b, or (P2 &minus; P)b= 0. By Problem 3.32, P2 = P. Hence, B, and therefore A
has an idempotent. �
</p>
<p>Proposition 3.5.10 An algebra is nilpotent if and only if it contains no
idempotent.
</p>
<p>Proof The &ldquo;only if&rdquo; part was shown after Definition 3.5.1. We now show
that if A has no idempotent, then it must be nilpotent. To begin, we note
that in general, Aa&sube;A, and therefore, Aak &sube;Aak&minus;1 for all k. If A has no
idempotent, then the equality is ruled out by Lemma 3.5.9. Hence, Aak &sub;
Aak&minus;1. This being true for all k, we have
</p>
<p>A&sup;Aa&sup;Aa2 &sup; &middot; &middot; &middot; &sup;Aak &sup; &middot; &middot; &middot; .
</p>
<p>Since A has a finite dimension, there must exist an integer r such that Aar =
{0} for all a &isin;A. In particular, ar+1 = 0 for all a &isin;A. This shows that A is
nil, and by Theorem 3.5.3, nilpotent. �
</p>
<p>Let P be an idempotent of A. Consider L(P), the left annihilator of P (see
Example 3.2.2), and note that (a&minus; aP) &isin;L(P) for any a &isin;A. Furthermore,
if a &isin; PL(P), then a= Px for some x &isin;L(P). Thus, a has the property that
Pa= a and aP= 0.
</p>
<p>Similarly, consider R(P), the right annihilator of P, and note that (a &minus;
Pa) &isin;R(P) for any a &isin;A. Furthermore, if a &isin;R(P)P, then a= xP for some
x &isin;R(P). Thus, a has the property that aP= a and Pa= 0.
</p>
<p>Let I(P)=L(P)&cap;R(P). Then, clearly I(P) is a two-sided ideal consist-
ing of elements a &isin;A such that aP= Pa= 0. To these, we add the subalge-
bra PAP, whose elements a can be shown to have the property Pa= aP= a.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.5 Decomposition of Algebras 87
</p>
<p>We thus have
</p>
<p>PAP= {a &isin;A | Pa= aP= a},
PL(P)= {a &isin;A | Pa= a, aP= 0},
R(P)P= {a &isin;A | aP= a, Pa= 0},
I(P)= {a &isin;A | aP= Pa= 0},
</p>
<p>(3.13)
</p>
<p>and the following
</p>
<p>Theorem 3.5.11 LetA be any algebra with an idempotent P. Then we have
the Peirce decomposition of A: Peirce decomposition
</p>
<p>A= PAP&oplus;V PL(P)&oplus;V R(P)P&oplus;V I(P),
</p>
<p>where &oplus;V indicates a vector space direct sum, and each factor is a subal-
gebra.
</p>
<p>Proof By Eq. (3.13), each summand is actually an algebra. Furthermore, it
is not hard to show that the only vector common to any two of the summands
is the zero vector. Thus the sum is indeed a direct sum of subspaces. Next
note that for any a &isin;A,
</p>
<p>a= PaP+ P (a&minus; aP)︸ ︷︷ ︸
&isin;L(P)
</p>
<p>+ (a&minus; Pa)︸ ︷︷ ︸
&isin;R(P)
</p>
<p>P+ (a&minus; Pa&minus; aP+ PaP)︸ ︷︷ ︸
&isin;I(P)
</p>
<p>Problem 3.33 provides the details of the proof. �
</p>
<p>Definition 3.5.12 An element a &isin; A is orthogonal to an idempotent P if principal idempotent
and elements
</p>
<p>orthogonal to an
</p>
<p>idempotent
</p>
<p>aP = Pa = 0. Thus I(P) houses such elements. An idempotent P is called
principal if I(P) contains no idempotent.
</p>
<p>Let P0 be an idempotent. If it is not principal, then I(P0) contains an
idempotent q. Let P1 = P0 +q. Then using the fact that P0q= qP0 = 0, we
can show that P1 is an idempotent and that
</p>
<p>P1P0 = P0P1 = P0 and P1q= qP1 = q. (3.14)
</p>
<p>If x &isin; I(P1), then xP1 = P1x = 0, and the first equation in (3.14) gives
xP0 = P0x = 0, i.e., x &isin; I(P0), demonstrating that I(P1) &sube; I(P0). Since
q &isin; I(P0), but q �&isin; I(P1), I(P1) is a proper subset of I(P0). If I(P1) is not
principal, then I(P1) contains an idempotent r. Let P2 = P1 + r. Then P2 is
an idempotent and, as before, I(P2) is a proper subset of I(P1). We continue
this process and obtain
</p>
<p>I(P0)&sup; I(P1)&sup; I(P2)&sup; &middot; &middot; &middot; &sup; I(Pk)&sup; &middot; &middot; &middot; .
</p>
<p>However, we cannot continue this chain indefinitely, because I(P0) has finite
dimension. This means that there is a positive integer n such that I(Pn) has
no idempotent, i.e., Pn is principal. We have just proved</p>
<p/>
</div>
<div class="page"><p/>
<p>88 3 Algebras
</p>
<p>Proposition 3.5.13 Every algebra that is not nilpotent has a principal
idempotent.
</p>
<p>Definition 3.5.14 An idempotent is primitive if it is not the sum of twoprimitive idempotent
orthogonal idempotents.
</p>
<p>Proposition 3.5.15 P is primitive if and only if it is the only idempotent of
PAP.
</p>
<p>Proof Suppose that P is not primitive. Then there are orthogonal idempo-
tents P1 and P2 such that P= P1+P2. It is easy to show that PPi = PiP= Pi
for i = 1,2. Hence, by the first equation in (3.13), Pi &isin; PAP, and P is not
the only idempotent of PAP.
</p>
<p>Conversely, suppose that P is not the only idempotent in PAP, so that
PAP contains another idempotent, say P&prime;. Then by the first equation in
(3.13), PP&prime; = P&prime;P= P&prime;. This shows that
</p>
<p>(
P&minus; P&prime;
</p>
<p>)
P&prime; = P&prime;
</p>
<p>(
P&minus; P&prime;
</p>
<p>)
= 0 and
</p>
<p>(
P&minus; P&prime;
</p>
<p>)
P= P
</p>
<p>(
P&minus; P&prime;
</p>
<p>)
= P&minus; P&prime;,
</p>
<p>i.e., that (P &minus; P&prime;) &isin; PAP and it is orthogonal to P&prime;. Furthermore, P =
(P&minus; P&prime;)+ P&prime;, i.e., P is the sum of two primitive idempotents, and thus not
primitive. �
</p>
<p>Let P be an idempotent that is not primitive. Write P= P1 +Q, with P1
and Q orthogonal. If either of the two, say Q, is not primitive, write it as
Q= P2 + P3, with P2 and P3 orthogonal. By Problem 3.34, the set {Pi}3i=1
are mutually orthogonal idempotents and P= P1+P2+P3. We can continue
this process until all Pis are primitive. Therefore, we have
</p>
<p>Theorem 3.5.16 Every idempotent of an algebraA can be expressed as the
sum of a finite number of mutually orthogonal primitive idempotents.
</p>
<p>3.5.2 Semi-simple Algebras
</p>
<p>Algebras which have no nilpotent ideals play an important role in the clas-
sification of algebras.semi-simple algebras
</p>
<p>Definition 3.5.17 An algebra whose radical is zero is called semi-
simple.
</p>
<p>Since Rad(A) contains all nilpotent left, right, and two-sided ideals of an
algebra, if A is semi-simple, it can have no nilpotent left, right, or two-sided
ideals.
</p>
<p>Proposition 3.5.18 A simple algebra is semi-simple.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.5 Decomposition of Algebras 89
</p>
<p>Proof If the simple algebra A is not semi-simple, then it has a nilpotent
ideal. Since the only ideal is A itself, we must show that A is not nilpo-
tent. Assume otherwise, and note that A2 is a proper ideal of A, because if
A2 =A, then Ak =A for any k. This contradicts our assumption that A is
nilpotent. Since the only ideals of A are A and {0}, we must have A2 = {0}.
It then follows that any proper subspace of A is trivially a nonzero proper
ideal of A, which cannot happen because of the simplicity of A. �
</p>
<p>Lemma 3.5.19 If A is semi-simple and P is any principal idempotent in A,
then A= PAP.
</p>
<p>Proof Since A is not nilpotent, it has a principal idempotent P by Proposi-
tion 3.5.13. Since P is principal, I(P) of Theorem 3.5.11 contains no idem-
potent and by Proposition 3.5.10 must be nilpotent. Since A has no nilpotent
ideal, I(P)= {0}. Now note that R(P)L(P) of Theorem 3.5.11 consists of all
elements annihilated by both the right and left multiplication by P. There-
fore, R(P)L(P) is a subset of I(P). Hence, R(P)L(P)= {0}. This shows that
if r &isin; R(P) and l &isin; L(P), then rl = 0. On the other hand, for any l &isin; L(P)
and r &isin;R(P), we have
</p>
<p>(lr)2 = l (rl)︸︷︷︸
=0
</p>
<p>r= 0.
</p>
<p>It follows that the ideal L(P)R(P) (see Problem 3.10) is nil of index 2,
and by Theorem 3.5.3, it is nilpotent. The semi-simplicity of A implies that
L(P)R(P)= {0}. Multiplying the Peirce decomposition on the left by L(P),
and using these results and the fact that L(P)P= {0}, we obtain
</p>
<p>L(P)A=L(P)R(P)P= {0}.
</p>
<p>In particular L(P)L(P)= {0}, and thus L(P) is nilpotent, hence zero. Sim-
ilarly, R(P) is also zero. Therefore, the Peirce decomposition of A reduces
to the first term. �
</p>
<p>Theorem 3.5.20 A semi-simple algebra A is necessarily unital. Further- semi-simple algebras are
unitalmore, the unit is the only principal idempotent of A.
</p>
<p>Proof Let P be a principal idempotent of A. If b &isin;A, then by Lemma 3.5.19
b &isin; PAP, and b= PaP for some a &isin;A. Therefore,
</p>
<p>Pb= P2aP= PaP= b
</p>
<p>bP= PaP2 = PaP= b.
</p>
<p>Since this holds for all b &isin;A, we conclude that P is the identity of A. �
</p>
<p>Idempotents preserve the semi-simplicity of algebras in the following
sense:
</p>
<p>Proposition 3.5.21 If A is semi-simple, then PAP is also semi-simple for
any idempotent P &isin;A.</p>
<p/>
</div>
<div class="page"><p/>
<p>90 3 Algebras
</p>
<p>Proof Let N= Rad(PAP) and x &isin;N&sub; PAP. Construct the left ideal Ax in
A and note that by Eq. (3.13), xP= Px= x. Then we have the following set
identities:
</p>
<p>(Ax)ν+1 =AxAx . . .AxAx=AxPAPx . . .PAPxPAPx
=Ax(PAPx)ν .
</p>
<p>Since N is an ideal in PAP, we have PAPx &sub; N, and if ν is the index of
N, then (PAPx)ν = {0}. Thus, Ax is nilpotent. Since A is semi-simple, we
must have Ax = {0}. Thus, for any nonzero a &isin; A, ax = 0. In particular,
Px= x= 0. Since x was an arbitrary element of Rad(PAP), we must have
Rad(PAP)= {0}. Hence, PAP is semi-simple. �
</p>
<p>Proposition 3.5.22 Let A be a semi-simple algebra and P an idem-
potent in A. Then PAP is a division algebra if and only if P is primi-
tive.
</p>
<p>Proof Suppose that PAP is a division algebra. By Proposition 3.5.2, identity
is the only idempotent of PAP. But P is the identity of PAP. Hence, P is the
only idempotent of PAP, and by Proposition 3.5.15 P is primitive.
</p>
<p>Conversely, assume that P is primitive. Let x &isin; PAP be nonzero. The
left ideal L&equiv; (PAP)x cannot be nilpotent because PAP is semi-simple by
Proposition 3.5.21. Hence, it must contain an idempotent by Proposition
3.5.13. But an idempotent in L is an idempotent in PAP. Proposition 3.5.15
identifies P as the sole idempotent in PAP, and thus, in L. As an element of
L, we can write P as P= ax with a &isin; PAP. Since, P is the identity in PAP,
x has an inverse. It follows that any element in PAP has an inverse. Thus it
is a division algebra. �
</p>
<p>It is intuitively obvious that a simple algebra is somehow more funda-
mental than a semi-simple algebra. We have seen that a simple algebra
is semi-simple. But the converse is of course not true. If simple algebras
are more fundamental, then semi-simple algebras should be &ldquo;built up&rdquo; from
simple ones. To see this we first need some preliminaries.
</p>
<p>Lemma 3.5.23 If A has an ideal B with unit 1B , then A = B &oplus; I(1B),
where I(1B) is the ideal in the Peirce decomposition of A.
</p>
<p>Proof Since 1B is an idempotent8 of A, we can write the following Peirce
decomposition:
</p>
<p>A= 1BA1B &oplus;V 1BL(1B)&oplus;V R(1B)1B &oplus;V I(1B)&equiv; S(1B)&oplus;V I(1B)
</p>
<p>8Note that 1B is not the identity of A. It satisfies x1B = 1Bx= x only if x &isin;B.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.5 Decomposition of Algebras 91
</p>
<p>where S(1B) = 1BA1B &oplus;V 1BL(1B) &oplus;V R(1B)1B . Since, B is an ideal,
each component of S(1B) is a subset of B, and therefore, S(1B) &sube; B. If
b &isin; B, then b &isin; A, and by the above decomposition, b = b1 + b2, with
b1 &isin; S(1B) and b2 &isin; I(1B). Multiplying both sides by 1B , we get
</p>
<p>b1B = b11B + b21B or b= b1
</p>
<p>because 1B is the identity in B and I(1B) is orthogonal to 1B . It follows that
b &isin; S(1B) and, therefore, B &sube; S(1B). Hence, B = S(1B) and A = B&oplus;V
I(1B). Since I(1B)B=BI(1B)= {0}, we can change &oplus;V to &oplus;. �
</p>
<p>Lemma 3.5.24 A nonzero ideal of a semi-simple algebra is semi-simple.
</p>
<p>Proof Let A be a semi-simple algebra and B be a nonzero ideal of A. Then
BRad(B)B&sub; Rad(B) because Rad(B) is an ideal in B. Furthermore, since
B is an ideal in A, AB&sub;B and BA&sub;B. It follow that A(BRad(B)B)A=
(AB)Rad(B)(BA) &sub; BRad(B)B, i.e., that BRad(B)B is an ideal in A.
Furthermore, it is nilpotent because it is contained in Rad(B). Semi-
simplicity of A implies that BRad(B)B = {0}. Since Rad(B) &sub; B,
ARad(B)A&sub;B, and ARad(B)ARad(B)ARad(B)A&sub;BRad(B)B. Now
note that
</p>
<p>(
ARad(B)A
</p>
<p>)3 =ARad(B)AARad(B)AARad(B)A
&sub;ARad(B)ARad(B)ARad(B)A
&sub;BRad(B)B= {0},
</p>
<p>indicating that ARad(B)A is nilpotent. Since it is an ideal in A, and A
is semi-simple, ARad(B)A = {0}, and since A has an identity by Theo-
rem 3.5.20, Rad(B)= {0}, and B is semi-simple. �
</p>
<p>Theorem 3.5.25 An algebra is semi-simple iff it is the direct sum of
simple algebras.
</p>
<p>Proof If the algebra A is the direct sum of simple algebras, then by Propo-
sition 3.2.11, the only ideals of A are either direct sums of the components
or contained in them. In either case, these ideals cannot be nilpotent because
a simple algebra is semi-simple. Therefore, A is semi-simple.
</p>
<p>Conversely, assume that A is semi-simple. If it has no proper ideal, then
it is simple and therefore semi-simple, and we are done. So, suppose B is
a proper nonzero ideal of A. By Lemma 3.5.24 B is semi-simple, and by
Theorem 3.5.20 B has a unit 1B . Invoking Lemma 3.5.23, we can write
A=B&oplus; I(1B). If either of the two components is not simple, we continue
the process. �
</p>
<p>Theorem 3.5.26 The reduction of a semi-simple algebra to simple subal-
gebras is unique up to an ordering of the components.</p>
<p/>
</div>
<div class="page"><p/>
<p>92 3 Algebras
</p>
<p>Proof Let A = A1 &oplus; &middot; &middot; &middot; &oplus; Ar with Ai simple. The unit of A is a sum of
the units of the components: 1= 11 + &middot; &middot; &middot; + 1r . Let A=A&prime;1 &oplus; &middot; &middot; &middot; &oplus;A&prime;s be
another reduction. Multiply both sides of the identity decomposition on the
left by A&prime;j to obtain
</p>
<p>A
&prime;
j =A&prime;j11 + &middot; &middot; &middot; +A&prime;j1r &equiv;A&prime;j1 + &middot; &middot; &middot; +A&prime;jr =
</p>
<p>r&sum;
</p>
<p>i=1
A
</p>
<p>&prime;
ji .
</p>
<p>Since 1i &isin;Ai , and Ai is an ideal of A, A&prime;ji &sub;Ai . Since Ai are disjoint, A&prime;ji
are disjoint. Since A&prime;j is an ideal, A
</p>
<p>&prime;
j1i is an algebra as can be easily verified.
</p>
<p>Furthermore, since 1i1k = 0 for i �= k, the sum is a direct sum of algebras.
Hence, by Proposition 3.2.11, A&prime;ji is an ideal, and since it is a subset of
Ai , it is a subideal of Ai . The simplicity of Ai implies that A&prime;ji = Ai or
A
</p>
<p>&prime;
ji = {0}. Since A&prime;j is simple, only one of its components is nonzero, and
</p>
<p>it is one of the Ai . �
</p>
<p>3.5.3 Classification of Simple Algebras
</p>
<p>Theorems 3.5.25 and 3.5.26 classify all the semi-simple algebras, i.e., al-
gebras with zero radicals, in terms of simple algebras. Can a general al-
gebra be written as its radical and a semi-simple algebra? It turns out
that an algebra A with nonzero radical Rad(A) is the direct sum A =
Rad(A)&oplus; (A/Rad(A)), i.e., the radical plus the factor algebra modulo the
radical. Since, in A/Rad(A), the radical has been &ldquo;factored out&rdquo; of A, the
quotient is indeed semi-simple. This result is known as Wedderburn prin-
cipal structure theorem, and reduces the study of all algebras to that of
</p>
<p>Wedderburn principal
</p>
<p>structure theorem
</p>
<p>simple algebras. Simple algebras can be further decomposed (for a proof,
see [Benn 87, pp. 330&ndash;332]):
</p>
<p>Wedderburn
</p>
<p>decomposition theorem
</p>
<p>Theorem 3.5.27 (Wedderburn decomposition) An algebra A is sim-
ple if and only if
</p>
<p>A&sim;=D&otimes;Mn &sim;=Mn(D),
where D is a division algebra and Mn(D) is a total matrix algebra
overD for some non-negative integer n. D andMn(D) are unique up
to a similarity transformation.
</p>
<p>Denote by Zn the center of Mn. Since Mn is central, by Theorem 3.3.2,
Zn = Span{1n}. On the other hand, Eq. (3.8) gives
</p>
<p>Z(A)= Z(D)&otimes;Zn &sim;= Z(D), (3.15)
</p>
<p>which is a relation that determines D from a knowledge of the center of the
algebra A.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.5 Decomposition of Algebras 93
</p>
<p>Proposition 3.5.28 The only division algebra over C is C itself.
</p>
<p>Proof Let D be a division algebra over C and x a nonzero element of D.
Since D is finite-dimensional, there must exist a polynomial in x such that
(why?)
</p>
<p>f (x)= xn + αn&minus;1xn&minus;1 + &middot; &middot; &middot; + α1x+ α01= 0.
</p>
<p>Let n be the smallest integer such that this holds. By the fundamental theo-
rem of algebra (see Sect. 10.5), f (x) has at least one root λ. Then we have
</p>
<p>f (x)= (x&minus; λ1)g(x)= 0.
</p>
<p>Now, g(x) has degree at most n &minus; 1 and by assumption cannot be zero.
Hence, it has an inverse because D is a division algebra. Therefore, x&minus;λ1=
0, and every element of D is a multiple of 1. This completes the proof. �
</p>
<p>Proposition 3.5.28 and Theorem 3.5.27, plus the fact that Mn(C) is cen-
tral (Theorem 3.3.2) give the following:
</p>
<p>Theorem 3.5.29 Any simple algebra A over C is isomorphic to
Mn(C) for some n, and therefore A is necessarily central simple.
</p>
<p>The centrality of a complex algebra can also be deduced from Eq. (3.15)
and Proposition 3.5.28.
</p>
<p>There is a theorem in abstract algebra, called the Frobenius Theorem, Frobenius Theorem
which states that the only division algebras over R are R, C, and H, and
since the tensor product of two division algebras is a division algebra, also
C&otimes;H.9 Furthermore, the center of C is the entire C, because it is a com-
mutative algebra. On the other hand, H is central, i.e., its center is the span
of its identity (reader, please verify), therefore, isomorphic to R.
</p>
<p>Now consider a simple algebra A over R. If A is central, i.e., if Z(A)=
R, then Eq. (3.15) yields
</p>
<p>R&sim;= Z(D) &rArr; D=R or H.
</p>
<p>If Z(A)=C, then
</p>
<p>C&sim;= Z(D) &rArr; D=C or C&otimes;H.
</p>
<p>These results, plus the theorems of Frobenius and Wedderburn yield
</p>
<p>9Since C is a subalgebra of H, the tensor product is actually redundant. However, in
the classification of the Clifford algebras discussed later in the book, C is sometimes
explicitly factored out.</p>
<p/>
</div>
<div class="page"><p/>
<p>94 3 Algebras
</p>
<p>Theorem 3.5.30 Any simple algebra A over R is isomorphic to D&otimes;
Mn for some n. If the center of A is isomorphic to C, thenD is either
C or C&otimes;H. If A is central (i.e., its center is isomorphic to R), then
D is R or H.
</p>
<p>We conclude our discussion of the decomposition of an algebra by a fur-
ther characterization of a simple algebra and the connection between primi-
tive idempotents andminimal left ideals.
</p>
<p>Definition 3.5.31 Two idempotents P and P&prime; of an algebra A are called
similar if there exists an invertible element s &isin;A such that P&prime; = sPs&minus;1.similar idempotents
</p>
<p>The proof of the following theorem can be found in [Benn 87, pp. 332&ndash;
334]:
</p>
<p>Theorem 3.5.32 If P is an idempotent of a simple algebra A, then thererank of an idempotent of
a simple algebra exist mutually orthogonal primitive idempotents {Pi}ri=1 such that P =&sum;r
</p>
<p>i=1 Pi . The integer r is unique and is called the rank of P. Two idem-
potents are similar if and only if they have the same rank.
</p>
<p>Theorem 3.5.33 Let P be a primitive idempotent of a semi-simple
algebra A. Then AP (respectively PA) is a minimal left (respectively
right) ideal of A.
</p>
<p>Proof Since a semi-simple algebra is a direct sum of simple algebras each
independent of the others, without loss of generality, we can assume that
A is simple. Suppose L &equiv; AP is not minimal. Then L contains a nonzero
left ideal L1 of A. Since A is (semi-)simple, L1 is not nilpotent. Hence by
Proposition 3.5.10 it contains an idempotent P1. If P1 = P, then
</p>
<p>L=AP=AP1 &sube;L1,
</p>
<p>and therefore L= L1, and we are done. So suppose that P1 �= P. Then, by
Theorem 3.5.16
</p>
<p>P1 =Q1 + &middot; &middot; &middot; +Qr ,
where Qi are all primitive and orthogonal to each other. Since Q1 and P have
rank 1, by Theorem 3.5.32 they are similar, i.e., there exists an invertible
element s &isin;A such that P= sQ1s&minus;1. So, by choosing sP1s&minus;1 instead of P1
if we have to,10 we can assume that Q1 = P. Then
</p>
<p>P1 = P+Q2 + &middot; &middot; &middot; +Qr ,
</p>
<p>10This is equivalent to replacing L with sLs&minus;1, which is allowed by Theorem 3.2.7 and
the non-uniqueness clause of Theorem 3.5.27.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.6 Polynomial Algebra 95
</p>
<p>and P is orthogonal to all the Qi . Multiplying both sides on the left by P, we
get PP1 = P and
</p>
<p>L=AP=APP1 &sube;L1,
implying that L=L1. The case of a right ideal follows similarly. �
</p>
<p>3.6 Polynomial Algebra
</p>
<p>Let A be an associative algebra with identity 1. For any fixed element a &isin;A,
consider the set P[a] of elements of the algebra of the form
</p>
<p>p(a)&equiv;
&infin;&sum;
</p>
<p>k=0
αkak, αk &isin;C,
</p>
<p>in which only a finite number of the terms in the sum are nonzero. These are
clearly polynomials in a for which addition and multiplication is defined as
usual.
</p>
<p>Definition 3.6.1 Let A be an associative algebra with identity 1. For any leading coefficient,
monic, degree,
</p>
<p>monomial
</p>
<p>fixed element a &isin; A, the set P[a] is a commutative algebra with identity
called the polynomial algebra generated by a. The coefficient of the highest
power of a in p(a) = &sum;&infin;k=0 αkak is called the leading coefficient of p,
and α0 is called the scalar term. A polynomial with leading coefficient 1
is called monic. The highest power of a in p is called the degree of p
and denoted by degp. A nonzero polynomial of the form αnan is called a
monomial of degree n.
</p>
<p>It is clear that {ak}&infin;k=0 is a basis of the polynomial algebra P[a].
If p(a)&equiv;&sum;&infin;k=0 αkak and q(a)&equiv;
</p>
<p>&sum;&infin;
j=0 βja
</p>
<p>j , then
</p>
<p>(p+ q)(a)=
&infin;&sum;
</p>
<p>k=0
(αk + βk)ak,
</p>
<p>(pq)(a)=
&infin;&sum;
</p>
<p>i=0
γiai, where γi =
</p>
<p>&sum;
</p>
<p>j+k=i
αkβj .
</p>
<p>Consider two nonzero polynomials p(a) and q(a). Then obviously
</p>
<p>deg(p+ q)&le; max(degp,degq),
deg(pq)= degp+ degq.
</p>
<p>(3.16)
</p>
<p>Definition 3.6.2 The linear map d : P[a]&rarr; P[a] defined by differentiation map
</p>
<p>dak = kak&minus;1, k &ge; 1
</p>
<p>da0 &equiv; d1 = 0
</p>
<p>is called the differentiation map in P[a].</p>
<p/>
</div>
<div class="page"><p/>
<p>96 3 Algebras
</p>
<p>Theorem 3.6.3 The differentiation map d is a derivation of P[a]. We denote
d(p) by p&prime;.
</p>
<p>Proof The simple proof is left as Problem 3.35. �
</p>
<p>Let p and q be two polynomials. Then d(pq)= d(p)q + pd(q), and in
particular
</p>
<p>d
(
q2
</p>
<p>)
= 2qd(q),
</p>
<p>and in general
</p>
<p>d
(
qk
</p>
<p>)
= kqk&minus;1d(q), k &ge; 1 and d
</p>
<p>(
q0
</p>
<p>)
= 0.
</p>
<p>Because q is an element of A, it can generate a polynomial in itself. We can
construct, for example p(q), by replacing a with q:
</p>
<p>p(q)=
&infin;&sum;
</p>
<p>k=0
αkq
</p>
<p>k.
</p>
<p>Then, it is straightforward to show that (see Problem 3.36)
</p>
<p>d
(
p(q)
</p>
<p>)
= p&prime;(q) &middot; q &prime; (3.17)
</p>
<p>This is the chain rule for the differentiation of polynomials.chain rule
</p>
<p>Definition 3.6.4 The polynomial dr(p) is called the rth derivative of p
and denoted by p(r). We extend the notation by defining p(0) = p.
</p>
<p>It is clear that p(r) = 0 if r &gt; deg(p).
Consider the monomial an, and note that
</p>
<p>dr
(
an
)
= n!
</p>
<p>(n&minus; r)!a
n&minus;r or an&minus;r = (n&minus; r)!
</p>
<p>n! d
r
(
an
)
.
</p>
<p>Now use the binomial theorem to write
</p>
<p>(a + b)n =
n&sum;
</p>
<p>r=0
</p>
<p>(
n
</p>
<p>r
</p>
<p>)
an&minus;r &middot; br =
</p>
<p>n&sum;
</p>
<p>r=0
</p>
<p>1
</p>
<p>r!d
r
(
an
)
&middot; br .
</p>
<p>The left-hand side is an arbitrary term of the polynomial p(a + b). There-
fore, taking linear combination of such terms, we haveTaylor formula
</p>
<p>p(a + b)=
n&sum;
</p>
<p>r=0
</p>
<p>p(r)(a)
r! &middot; b
</p>
<p>r . (3.18)
</p>
<p>This is called the Taylor formula for p.
A root of the polynomial p(a)=&sum;nk=0 ηkak of degree n is a scalar λ &isin;C
</p>
<p>such that p(λ) = &sum;nk=0 ηkλk = 0. The fundamental theorem of algebra11
states that C is algebraically closed, meaning that any polynomial with co-
efficients in C can be factored out into a product of polynomials of degree
</p>
<p>11A proof of the theorem can be found in Sect. 10.5.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.7 Problems 97
</p>
<p>one with coefficients in C:
</p>
<p>p(a)= ηn(a &minus; λ11)k1 . . . (a &minus; λs1)ks , (3.19)
</p>
<p>where ηn �= 0, {λi}si=1 are the distinct complex roots of the polynomial, ki ,
</p>
<p>C is algebraically closed
</p>
<p>called the multiplicity of λi , is a nonnegative integer, and
&sum;s
</p>
<p>i=1 ki = n. multiplicity of a root
As the simple example p(a) = a2 + 1 suggests, R is not algebraically
</p>
<p>closed. Nevertheless, a real polynomial can still be factored out into prod-
ucts of polynomials of degree 1 and 2 with real coefficients. To show this,
first note that if λ is a complex root of a real polynomial, then its complex
conjugate λ̄ is also a root. This follows from taking the complex conjugate
of
</p>
<p>&sum;n
k=0 ηkλ
</p>
<p>k = 0 and noting that η̄k = ηk for real ηk . Furthermore, λ and λ̄
must have the same multiplicity, otherwise the unmatched factors produce a
polynomial with complex coefficients, which, when multiplied out with the
rest of the factors, produce some complex coefficients for p(a).
</p>
<p>Next, multiply each factor in Eq. (3.19) containing a complex root by its
complex conjugate. So, if λm = γm + iξm, then
</p>
<p>(a &minus; λm1)km(a &minus; λ̄m1)km = (a &minus; γm1 &minus; iξm1)km(a &minus; γm1 + iξm1)km
</p>
<p>=
(
a2 &minus; 2γma + γ 2m1 + ξ2m1
</p>
<p>)km
</p>
<p>&equiv;
(
a2 + αma + βm1
</p>
<p>)km , α2m &lt; 4βm.
</p>
<p>The inequality ensures that ξm �= 0, i.e., that the root is not real. We have
just proved the following:
</p>
<p>Theorem 3.6.5 A real polynomial p(a) =&sum;nk=0 ηkak of degree n has the
following factorization:
</p>
<p>p(a)= ηn
r&prod;
</p>
<p>i=1
(a &minus; λi1)ki
</p>
<p>R&prod;
</p>
<p>j=1
</p>
<p>(
a2 + αja + βj1
</p>
<p>)Kj , α2j &lt; 4βj ,
</p>
<p>where λi, αj , βj &isin; R, ki,Kj &isin; N, λi are all distinct, the pairs (αj , βj ) are
all distinct, and 2
</p>
<p>&sum;R
j=1 Kj +
</p>
<p>&sum;r
i=1 ki = n.
</p>
<p>Corollary 3.6.6 A real polynomial of odd degree has at least one real root.
</p>
<p>3.7 Problems
</p>
<p>3.1 Show that
</p>
<p>(a) the product on R2 defined by
</p>
<p>(x1, x2)(y1, y2)= (x1y1 &minus; x2y2, x1y2 + x2y1)
</p>
<p>turns R2 into an associative and commutative algebra, and
(b) the cross product on R3 turns it into a nonassociative, noncommutative
</p>
<p>algebra.</p>
<p/>
</div>
<div class="page"><p/>
<p>98 3 Algebras
</p>
<p>3.2 Show that the center of an algebra is a subspace of that algebra. If the
algebra is associative, then its center is a subalgebra.
</p>
<p>3.3 Prove that A2, the derived algebra of A, is indeed an algebra.
</p>
<p>3.4 Prove that the set A of n &times; n matrices, with the product defined by
Eq. (3.3), form a nonassociative noncommutative algebra.
</p>
<p>3.5 Prove that the set A of n&times; n upper triangular matrices, with the prod-
uct defined by ordinary multiplication of matrices is an associative non-
commutative algebra. Show that the same set with multiplication defined by
Eq. (3.3), is a nonassociative noncommutative algebra, and that the derived
algebra A2 &equiv; B is the set of strictly upper triangular matrices. What is the
derived algebra B2 of B?
</p>
<p>3.6 Prove Proposition 3.1.23.
</p>
<p>3.7 Let ω &isin;L(V) be defined by ω(a)=&minus;a for all a &isin; V. Is ω an involution
of V? Now suppose that V is an algebra. Is ω so defined an involution of the
algebra V? Recall that an involution of an algebra must be a homomorphism
of that algebra.
</p>
<p>3.8 Show that no proper left (right) ideal of an algebra with identity can
contain an element that has a left (right) inverse.
</p>
<p>3.9 Let A be an associative algebra, and x &isin;A. Show that Ax is a left ideal,
xA is a right ideal, and AxA is a two-sided ideal.
</p>
<p>3.10 Let L be a left ideal and R a right ideal. Show that LR is a two-sided
ideal.
</p>
<p>3.11 Show that Φ of Theorem 3.1.25 is an algebra isomorphism.
</p>
<p>3.12 Show that the linear transformation of Example 3.1.18 is an isomor-
phism of the two algebras A and B.
</p>
<p>3.13 Let A be an algebra with identity 1A and φ an epimorphism of A onto
another algebra B. Show that φ(1A) is the identity of B.
</p>
<p>3.14 Show that the derived algebra of A is an ideal in A.
</p>
<p>3.15 Show that the algebra of quaternions is central.
</p>
<p>3.16 Write down all the structure constants for the algebra of quaternions.
Show that this algebra is associative.
</p>
<p>3.17 Show that a quaternion is pure iff its square is a nonpositive real num-
ber.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.7 Problems 99
</p>
<p>3.18 Let p and q be two quaternions. Show that
</p>
<p>(a) (pq)&lowast; = q&lowast;p&lowast;,
(b) q &isin;R iff q&lowast; = q , and q &isin;R3 iff q&lowast; =&minus;q , and
(c) qq&lowast; = q&lowast;q is a nonnegative real number.
</p>
<p>3.19 Prove Eq. (3.7).
</p>
<p>3.20 Show that φ̄ of Example 3.2.16 is an algebra homomorphism.
</p>
<p>3.21 Prove Theorem 3.3.2.
</p>
<p>3.22 The algebra A has a basis {1, e} with e2 = 1.
(a) Show that {f1, f2} with f1 = 12 (1+ e) and f2 = 12 (1&minus; e) is also a basis.
(b) Show that A=L1 &oplus;V L2, where Li =Afi , i = 1,2 and &oplus;V indicates
</p>
<p>a vector space direct sum.
(c) Show that L1 an L2 are actually two-sided ideals and that L1L2 = {0}.
</p>
<p>Therefore, A=L1 &oplus;L2.
(d) Multiply an arbitrary element of Li , i = 1,2, by an arbitrary element
</p>
<p>of A to show that Li = Span{fi}, i = 1,2. Thus, Li &sim;= R, i = 1,2, or
A=R&oplus;R.
</p>
<p>3.23 If A is an algebra and D is a derivation in A, prove that both the center
Z(A) and the derived algebra A2 are stable under D, i.e., if a &isin; Z(A) then
D(a) &isin; Z(A), and if a &isin;A2 then D(a) &isin;A2.
</p>
<p>3.24 Let D :A&rarr;A be a derivation. Show that kerD is a subalgebra of A.
</p>
<p>3.25 Show that a linear combination of two derivations is a derivation.
</p>
<p>3.26 Fix a vector a &isin;R3 and define the linear transformation Da :R3 &rarr;R3
by Da(b)= a&times;b. Show that Da is a derivation of R3 with the cross product
as multiplication.
</p>
<p>3.27 Show that D defined on Cr (a, b) by D(f )= f &prime;(c), where a &lt; c &lt; b,
is a φc-derivation if φc is defined as the evaluation map φc(f )= f (c).
</p>
<p>3.28 Let��� &isin; End(A) be an antiderivation of A with respect to ω. Show that
ker��� is a subalgebra of A and ���(e)= 0 if A has an identity.
</p>
<p>3.29 Derive the Leibniz formula (3.11).
</p>
<p>3.30 Prove Theorem 3.4.10.
</p>
<p>3.31 Show that the algebra of the strictly upper triangular n&times; n matrices is
nilpotent of index n.
</p>
<p>3.32 Let b be a fixed element of an algebra B. Consider the linear trans-
formation Tb :B&rarr;B given by Tb(x)= xb. Using the dimension theorem,
show that if Bb=B, then kerTb = 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>100 3 Algebras
</p>
<p>3.33 Let A be an algebra with an idempotent P. Show that PAP consists of
elements a such that aP= Pa= a. For the subspaces of Theorem 3.5.11, let
A1 &equiv; PAP, A2 &equiv; PL(P), A3 &equiv; R(P)P, and A4 &equiv; I(P). Show that {Ai}3i=1
are subalgebras of A and that Ai &cap;Aj = {0}, but AiAj �= {0} for all i �= j ,
i, j = 1, . . . ,4. Thus, Peirce decomposition is a vector space direct sum, but
not an algebra direct sum.
</p>
<p>3.34 Let p and q be orthogonal idempotents. Suppose that q = q1 + q2,
where q1 and q2 are orthogonal idempotents. Show that qqi = qiq = qi
for i = 1,2. Using this result, show that pqi = qip= 0 for i = 1,2.
</p>
<p>3.35 Use the basis {ak}&infin;k=0 of P[a] and apply Theorem 3.4.4 on it to show
that the differentiation map of Definition 3.6.2 is a derivation.
</p>
<p>3.36 Derive the chain rule (3.17).</p>
<p/>
</div>
<div class="page"><p/>
<p>4Operator Algebra
</p>
<p>Chapter 3 introduced algebras, i.e., vector spaces in which one can multiply
two vectors to obtain a third vector. In this chapter, we want to investigate
the algebra of linear transformations.
</p>
<p>4.1 Algebra of End(V)
</p>
<p>The product in the algebra of the endomorphisms End(V) of a vector space
V is defined as the composition of maps. In addition to the zero element,
which is present in all algebras, End(V) has an identity element, 1, which
satisfies the relation 1|a〉 = |a〉 for all |a〉 &isin; V. Thus, End(V) is a unital
algebra. With 1 in our possession, we can ask whether it is possible to find an
operator T&minus;1 with the property that T&minus;1T= TT&minus;1 = 1. Generally speaking,
only bijective mappings have inverses. Therefore, only automorphisms of a
vector space are invertible.
</p>
<p>Example 4.1.1 Let the linear operator T :R3 &rarr;R3 be defined by
</p>
<p>T(x1, x2, x3)= (x1 + x2, x2 + x3, x1 + x3).
</p>
<p>We want to see whether T is invertible and, if so, find its inverse. T has an
inverse if and only if it is bijective. By the comments after Theorem 2.3.13
this is the case if and only if T is either surjective or injective. The lat-
ter is equivalent to kerT = |0〉. But kerT is the set of all vectors satisfying
T(x1, x2, x3)= (0,0,0), or
</p>
<p>x1 + x2 = 0, x2 + x3 = 0, x1 + x3 = 0.
</p>
<p>The reader may check that the unique solution to these equations is x1 =
x2 = x3 = 0. Thus, the only vector belonging to kerT is the zero vector.
Therefore, T has an inverse.
</p>
<p>To find T&minus;1 apply T&minus;1T= 1 to (x1, x2, x3):
</p>
<p>(x1, x2, x3)= T&minus;1T(x1, x2, x3)= T&minus;1(x1 + x2, x2 + x3, x1 + x3).
</p>
<p>S. Hassani, Mathematical Physics, DOI 10.1007/978-3-319-01195-0_4,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>101</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-01195-0_4">http://dx.doi.org/10.1007/978-3-319-01195-0_4</a></div>
</div>
<div class="page"><p/>
<p>102 4 Operator Algebra
</p>
<p>This equation demonstrates how T&minus;1 acts on vectors. To make this more
apparent, we let x1 + x2 = x, x2 + x3 = y, x1 + x3 = z, solve for x1, x2, and
x3 in terms of x, y, and z, and substitute in the preceding equation to obtain
</p>
<p>T&minus;1(x, y, z)= 1
2
(x &minus; y + z, x + y &minus; z,&minus;x + y + z).
</p>
<p>Rewriting this equation in terms of x1, x2, and x3 gives
</p>
<p>T&minus;1(x1, x2, x3)=
1
</p>
<p>2
(x1 &minus; x2 + x3, x1 + x2 &minus; x3,&minus;x1 + x2 + x3).
</p>
<p>We can easily verify that T&minus;1T= 1 and that TT&minus;1 = 1.
</p>
<p>Since End(V) is associative, Theorem 3.1.2 applies to it. Nevertheless,
we restate it in the context of operators as a corollary in which we also
include a generalization of Theorem 2.3.19:
</p>
<p>Corollary 4.1.2 The inverse of a linear operator is unique. If T and S are
two invertible linear operators on V, then TS is also invertible and
</p>
<p>(TS)&minus;1 = S&minus;1T&minus;1.
</p>
<p>The following proposition, whose straightforward proof is left as an ex-
ercise for the reader, turns out to be useful later on:
</p>
<p>Proposition 4.1.3 An endomorphism T &isin; End(V) is invertible iff it sends a
basis of V onto another basis of V.
</p>
<p>Let V1 and V2 be vector spaces and L1(V1) and L2(V2) the set of their
endomorphisms. A natural definition of L(V1 &otimes;V2) is given by
</p>
<p>L(V1 &otimes;V2)&sim;= (L1 &otimes;L2)(V1 &otimes;V2)&sim;=L1(V1)&otimes;L2(V2). (4.1)
</p>
<p>In particular, if V1 =C, V2 = V is a real vector space, then
</p>
<p>L(C&otimes;V)&sim;=L
(
V
C
)
, (4.2)
</p>
<p>where VC is the complexification of V as given in Definition 2.4.8. It is
important to note that
</p>
<p>L(C&otimes;V)≇C&otimes;L(V)
</p>
<p>because L(C)≇C.
</p>
<p>4.1.1 Polynomials of Operators
</p>
<p>From Sect. 3.6, we know that we can construct polynomials of T &isin; End(V)
such as
</p>
<p>p(T)= α01+ α1T+ α2T2 + &middot; &middot; &middot; + αnTn.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.1 Algebra of End(V) 103
</p>
<p>We shall use these polynomials as starting points for constructing functions
of operators.
</p>
<p>Example 4.1.4 Let Tθ :R2 &rarr;R2 be the linear operator that rotates vectors
in the xy-plane through the angle θ , that is,
</p>
<p>Tθ (x, y)= (x cos θ &minus; y sin θ, x sin θ + y cos θ).
</p>
<p>We are interested in powers of Tθ :
</p>
<p>T2θ (x, y)= Tθ
(
</p>
<p>x&prime;︷ ︸︸ ︷
x cos θ &minus; y sin θ,
</p>
<p>y&prime;︷ ︸︸ ︷
x sin θ + y cos θ
</p>
<p>)
</p>
<p>=
(
x&prime; cos θ &minus; y&prime; sin θ, x&prime; sin θ + y&prime; cos θ
</p>
<p>)
</p>
<p>=
(
(x cos θ &minus; y sin θ) cos θ &minus; (x sin θ + y cos θ) sin θ,
(x cos θ &minus; y sin θ) sin θ + (x sin θ + y cos θ) cos θ
</p>
<p>)
</p>
<p>= (x cos 2θ &minus; y sin 2θ, x sin 2θ + y cos 2θ).
</p>
<p>Thus, T2 rotates (x, y) by 2θ . Similarly, one can show that
</p>
<p>T3θ (x, y)= (x cos 3θ &minus; y sin 3θ, x sin 3θ + y cos 3θ),
</p>
<p>and in general, Tnθ (x, y)= (x cosnθ &minus; y sinnθ, x sinnθ + y cosnθ), which
shows that Tnθ is a rotation of (x, y) through the angle nθ , that is, T
</p>
<p>n
θ = Tnθ .
</p>
<p>This result could have been guessed because Tnθ is equivalent to rotating
(x, y) n times, each time by an angle θ .
</p>
<p>Negative powers of an invertible linear operator T are defined by T&minus;m =
(T&minus;1)m. The exponents of T satisfy the usual rules.1 In particular, for any
two integers m and n (positive or negative), TmTn = Tm+n and (Tm)n = Tmn.
The first relation implies that the inverse of Tm is T&minus;m. One can further gen-
eralize the exponent to include fractions and ultimately all real numbers; but
we need to wait until Chap. 6, in which we discuss the spectral decomposi-
tion theorem.
</p>
<p>Example 4.1.5 Let us evaluate T&minus;nθ for the operator of the previous exam-
ple. First, let us find T&minus;1θ (see Fig. 4.1). We are looking for an operator such
that T&minus;1θ Tθ (x, y)= (x, y), or
</p>
<p>T&minus;1θ (x cos θ &minus; y sin θ, x sin θ + y cos θ)= (x, y). (4.3)
</p>
<p>We define x&prime; = x cos θ &minus; y sin θ and y&prime; = x sin θ + y cos θ and solve x and
y in terms of x&prime; and y&prime; to obtain x = x&prime; cos θ + y&prime; sin θ and y =&minus;x&prime; sin θ +
y&prime; cos θ . Substituting for x and y in Eq. (4.3) yields
</p>
<p>T&minus;1θ
(
x&prime;, y&prime;
</p>
<p>)
=
(
x&prime; cos θ + y&prime; sin θ,&minus;x&prime; sin θ + y&prime; cos θ
</p>
<p>)
.
</p>
<p>1These rules apply to any associative algebra, not just to End(V).</p>
<p/>
</div>
<div class="page"><p/>
<p>104 4 Operator Algebra
</p>
<p>Fig. 4.1 The operator Tθ and its inverse as they act on a point in the plane
</p>
<p>Comparing this with the action of Tθ in the previous example, we discover
that the only difference between the two operators is the sign of the sin θ
term. We conclude that T&minus;1θ has the same effect as T&minus;θ . So we have
</p>
<p>T&minus;1θ = T&minus;θ and T&minus;nθ =
(
T&minus;1θ
</p>
<p>)n = (T&minus;θ )n = T&minus;nθ .
</p>
<p>It is instructive to verify that T&minus;nθ T
n
θ = 1:
</p>
<p>T&minus;nθ T
n
θ (x, y)= T&minus;nθ
</p>
<p>(
x&prime;︷ ︸︸ ︷
</p>
<p>x cosnθ &minus; y sinnθ,
y&prime;︷ ︸︸ ︷
</p>
<p>x sinnθ + y cosnθ
)
</p>
<p>=
(
x&prime; cosnθ + y&prime; sinnθ,&minus;x&prime; sinnθ + y&prime; cosnθ
</p>
<p>)
</p>
<p>=
(
(x cosnθ &minus; y sinnθ) cosnθ + (x sinnθ + y cosnθ) sinnθ,
&minus;(x cosnθ &minus; y sinnθ) sinnθ + (x sinnθ + y cosnθ) cosnθ
</p>
<p>)
</p>
<p>=
(
x
(
cos2 nθ + sin2 nθ
</p>
<p>)
, y
</p>
<p>(
sin2 nθ + cos2 nθ
</p>
<p>))
= (x, y).
</p>
<p>Similarly, we can show that TnθT
&minus;n
θ (x, y)= (x, y).
</p>
<p>One has to keep in mind that p(T) is not, in general, invertible, even if T
is. In fact, the sum of two invertible operators is not necessarily invertible.
For example, although T and &minus;T are invertible, their sum, the zero operator,
is not.
</p>
<p>4.1.2 Functions of Operators
</p>
<p>We can go one step beyond polynomials of operators and, via Taylor expan-
sion, define functions of them. Consider an ordinary function f (x), which
has the Taylor expansion
</p>
<p>f (x)=
&infin;&sum;
</p>
<p>k=0
</p>
<p>(x &minus; x0)k
k!
</p>
<p>dkf
</p>
<p>dxk
</p>
<p>∣∣∣∣
x=x0</p>
<p/>
</div>
<div class="page"><p/>
<p>4.1 Algebra of End(V) 105
</p>
<p>in which x0 is a point where f (x) and all its derivatives are defined. To this
function, there corresponds a function of the operator T, defined as
</p>
<p>f (T)=
&infin;&sum;
</p>
<p>k=0
</p>
<p>dkf
</p>
<p>dxk
</p>
<p>∣∣∣∣
x=x0
</p>
<p>(T&minus; x01)k
k! . (4.4)
</p>
<p>Because this series is an infinite sum of operators, difficulties may arise
concerning its convergence. However, as will be shown in Chap. 6, f (T)
is always defined for finite-dimensional vector spaces. In fact, it is always a
polynomial in T (see also Problem 4.1). For the time being, we shall think of
f (T) as a formal infinite series. A simplification results when the function
can be expanded about x = 0. In this case we obtain
</p>
<p>f (T)=
&infin;&sum;
</p>
<p>k=0
</p>
<p>dkf
</p>
<p>dxk
</p>
<p>∣∣∣∣
x=0
</p>
<p>Tk
</p>
<p>k! . (4.5)
</p>
<p>A widely used function is the exponential, whose expansion is easily found
to be
</p>
<p>eT &equiv; exp(T)=
&infin;&sum;
</p>
<p>k=0
</p>
<p>Tk
</p>
<p>k! . (4.6)
</p>
<p>Example 4.1.6 Let us evaluate exp(αT) when T :R2 &rarr;R2 is given by
</p>
<p>T(x, y)= (&minus;y, x).
</p>
<p>We can find a general formula for the action of Tn on (x, y). Start with
n= 2:
</p>
<p>T2(x, y)= T(&minus;y, x)= (&minus;x,&minus;y)=&minus;(x, y)=&minus;1(x, y).
</p>
<p>Thus, T2 =&minus;1. From T and T2 we can easily obtain higher powers of T. For
example: T3 = T(T2)=&minus;T, T4 = T2T2 = 1, and in general,
</p>
<p>T2n = (&minus;1)n1 for n= 0,1,2, . . .
</p>
<p>T2n+1 = (&minus;1)nT for n= 0,1,2, . . .
</p>
<p>Thus,
</p>
<p>exp(αT)=
&sum;
</p>
<p>n odd
</p>
<p>(αT)n
</p>
<p>n! +
&sum;
</p>
<p>n even
</p>
<p>(αT)n
</p>
<p>n! =
&infin;&sum;
</p>
<p>k=0
</p>
<p>(αT)2k+1
</p>
<p>(2k + 1)! +
&infin;&sum;
</p>
<p>k=0
</p>
<p>(αT)2k
</p>
<p>(2k)!
</p>
<p>=
&infin;&sum;
</p>
<p>k=0
</p>
<p>α2k+1T2k+1
</p>
<p>(2k+ 1)! +
&infin;&sum;
</p>
<p>k=0
</p>
<p>α2kT2k
</p>
<p>(2k)!
</p>
<p>=
&infin;&sum;
</p>
<p>k=0
</p>
<p>(&minus;1)kα2k+1
(2k + 1)! T+
</p>
<p>&infin;&sum;
</p>
<p>k=0
</p>
<p>(&minus;1)kα2k
(2k)! 1</p>
<p/>
</div>
<div class="page"><p/>
<p>106 4 Operator Algebra
</p>
<p>= T
&infin;&sum;
</p>
<p>k=0
</p>
<p>(&minus;1)kα2k+1
(2k+ 1)! + 1
</p>
<p>&infin;&sum;
</p>
<p>k=0
</p>
<p>(&minus;1)kα2k
(2k)! .
</p>
<p>The two series are recognized as sinα and cosα, respectively. Therefore, we
get
</p>
<p>eαT = T sinα + 1 cosα,
which shows that eαT is a polynomial (of first degree) in T.
</p>
<p>The action of eαT on (x, y) is given by
</p>
<p>eαT(x, y)= (sinαT+ cosα1)(x, y)= sinαT(x, y)+ cosα1(x, y)
= (sinα)(&minus;y, x)+ (cosα)(x, y)
= (&minus;y sinα,x sinα)+ (x cosα,y cosα)
= (x cosα &minus; y sinα,x sinα + y cosα).
</p>
<p>The reader will recognize the final expression as a rotation in the xy-plane
generator of the rotation
</p>
<p>through an angle α. Thus, we can think of eαT as a rotation operator of angle
α about the z-axis. In this context T is called the generator of the rotation.
</p>
<p>4.1.3 Commutators
</p>
<p>The result of multiplication of two operators depends on the order in which
the operators appear. This means that if T,U &isin; L(V), then TU &isin; L(V) and
UT &isin;L(V); however, in general UT �= TU. When this is the case, we say that
U and T do not commute. The extent to which two operators fail to commute
is given in the following definition.
</p>
<p>Definition 4.1.7 The commutator [U,T] of the two operators U and T in
commutator defined
</p>
<p>L(V) is another operator in L(V), defined as
</p>
<p>[U,T] &equiv; UT&minus; TU.
</p>
<p>An immediate consequence of this definition is the following:
</p>
<p>Proposition 4.1.8 For S,T,U &isin;L(V) and α,β &isin;C (or R), we have
</p>
<p>[U,T] = &minus;[T,U], antisymmetry
[αU, βT] = αβ[U,T], linearity
[S,T+U] = [S,T] + [S,U], linearity in the right entry
[S+ T,U] = [S,U] + [T,U], linearity in the left entry
[ST,U] = S[T,U] + [S,U]T, right derivation property
[S,TU] = [S,T]U+ T[S,U], left derivation property
[
[S,T],U
</p>
<p>]
+
[
[U,S],T
</p>
<p>]
+
[
[T,U],S
</p>
<p>]
= 0, Jacobi identity</p>
<p/>
</div>
<div class="page"><p/>
<p>4.2 Derivatives of Operators 107
</p>
<p>Proof In almost all cases the proof follows immediately from the definition.
The only minor exceptions are the derivation properties. We prove the left
derivation property:
</p>
<p>[S,TU] = S(TU)&minus; (TU)S= STU&minus; TUS+ TSU&minus; TSU︸ ︷︷ ︸
&equiv;0
</p>
<p>= (ST&minus; TS)U+ T(SU&minus;US)= [S,T]U+ T[S,U].
</p>
<p>The right derivation property is proved in exactly the same way. �
</p>
<p>A useful consequence of the definition and Proposition 4.1.8 is
</p>
<p>[
A,Am
</p>
<p>]
= 0 for m= 0,&plusmn;1,&plusmn;2, . . . .
</p>
<p>In particular, [A,1] = 0 and [A,A&minus;1] = 0.
An example of the commutators of operators is that of D and T defined
</p>
<p>in Example 2.3.5. The reader is urged to verify that
</p>
<p>[D,T] = 1 (4.7)
</p>
<p>4.2 Derivatives of Operators
</p>
<p>Up to this point we have been discussing the algebraic properties of op-
erators, static objects that obey certain algebraic rules and fulfill the static
needs of some applications. However, physical quantities are dynamic, and
if we want operators to represent physical quantities, we must allow them to
change with time. This dynamism is best illustrated in quantum mechanics,
where physical observables are represented by operators.
</p>
<p>Let us consider a mapping H :R&rarr; End(V), which2 takes in a real num-
ber and gives out a linear operator on the vector space V. We denote the
</p>
<p>a time-dependent
</p>
<p>operator does not
</p>
<p>commute with itself at
</p>
<p>different times
</p>
<p>image of t &isin; R by H(t), which acts on the underlying vector space V.
The physical meaning of this is that as t (usually time) varies, its image
H(t) also varies. Therefore, for different values of t , we have different op-
erators. In particular, [H(t),H(t &prime;)] �= 0 for t �= t &prime;. A concrete example is
an operator that is a linear combination of the operators D and T intro-
duced in Example 2.3.5, with time-dependent scalars. To be specific, let
H(t) = D cosωt + T sinωt , where ω is a constant. As time passes, H(t)
changes its identity from D to T and back to D. Most of the time it has a
hybrid identity! Since D and T do not commute [see Eq. (4.7)], values of
H(t) for different times do not necessarily commute.
</p>
<p>Of particular interest are operators that can be written as exp(H(t)),
where H(t) is a &ldquo;simple&rdquo; operator; i.e., the dependence of H(t) on t is sim-
</p>
<p>2Strictly speaking, the domain of H must be an interval [a, b] of the real line, because
H may not be defined for all R. However, for our purposes, such a fine distinction is not
necessary.</p>
<p/>
</div>
<div class="page"><p/>
<p>108 4 Operator Algebra
</p>
<p>pler than the corresponding dependence of exp(H(t)). We have already en-
countered such a situation in Example 4.1.6, where it was shown that the
operation of rotation around the z-axis could be written as exp(αT), and the
action of T on (x, y) was a great deal simpler than the corresponding action
of exp(αT).
</p>
<p>Such a state of affairs is very common in physics. In fact, it can be shown
that many operators of physical interest can be written as a product of sim-
pler operators, each being of the form exp(αT). For example, we know from
</p>
<p>Euler&rsquo;s theorem and
</p>
<p>Euler angles
Euler&rsquo;s theorem in mechanics that an arbitrary rotation in three dimensions
can be written as a product of three simpler rotations, each being a rotation
through a so-called Euler angle about an axis.
</p>
<p>Definition 4.2.1 For the mapping H : R&rarr; End(V), we define the deriva-
derivative of an operator
</p>
<p>tive as
</p>
<p>dH
</p>
<p>dt
= lim
</p>
<p>�t&rarr;0
H(t +�t)&minus;H(t)
</p>
<p>�t
.
</p>
<p>This derivative also belongs to End(V).
</p>
<p>As long as we keep track of the order, practically all the rules of differ-
entiation apply to operators. For example,
</p>
<p>d
</p>
<p>dt
(UT)= dU
</p>
<p>dt
T+UdT
</p>
<p>dt
.
</p>
<p>We are not allowed to change the order of multiplication on the RHS, not
even when both operators being multiplied are the same on the LHS. For
instance, if we let U= T= H in the preceding equation, we obtain
</p>
<p>d
</p>
<p>dt
</p>
<p>(
H2
</p>
<p>)
= dH
</p>
<p>dt
H+HdH
</p>
<p>dt
.
</p>
<p>This is not, in general, equal to 2H dH
dt
</p>
<p>.
</p>
<p>Example 4.2.2 Let us find the derivative of exp(tH), where H is indepen-
dent of t . Using Definition 4.2.1, we have
</p>
<p>d
</p>
<p>dt
exp(tH)= lim
</p>
<p>�t&rarr;0
exp[(t +�t)H] &minus; exp(tH)
</p>
<p>�t
.
</p>
<p>However, for infinitesimal �t we have
</p>
<p>exp
[
(t +�t)H
</p>
<p>]
&minus; exp(tH)= etHe�tH &minus; etH
</p>
<p>= etH(1+H�t)&minus; etH = etHH�t.
</p>
<p>Therefore,
</p>
<p>d
</p>
<p>dt
exp(tH)= lim
</p>
<p>�t&rarr;0
etHH�t
</p>
<p>�t
= etHH.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.2 Derivatives of Operators 109
</p>
<p>Since H and etH commute,3 we also have
</p>
<p>d
</p>
<p>dt
exp(tH)= HetH.
</p>
<p>Note that in deriving the equation for the derivative of etH, we have used
the relation etHe�tH = e(t+�t)H. This may seem trivial, but it will be shown
later that in general, eS+T �= eSeT.
</p>
<p>Now let us evaluate the derivative of a more general time-dependent op-
erator, exp[H(t)]:
</p>
<p>d
</p>
<p>dt
exp
</p>
<p>[
H(t)
</p>
<p>]
= lim
</p>
<p>�t&rarr;0
exp[H(t +�t)] &minus; exp[H(t)]
</p>
<p>�t
.
</p>
<p>If H(t) possesses a derivative, we have, to the first order in �t ,
</p>
<p>H(t +�t)= H(t)+�t dH
dt
</p>
<p>,
</p>
<p>and we can write exp[H(t +�t)] = exp[H(t)+�tdH/dt]. It is very tempt-
ing to factor out the exp[H(t)] and expand the remaining part. However, as
we will see presently, this is not possible in general. As preparation, consider
the following example, which concerns the integration of an operator.
</p>
<p>Example 4.2.3 The Schr&ouml;dinger equation
</p>
<p>i
&part;
</p>
<p>&part;t
</p>
<p>∣∣ψ(t)
&rang;
= H
</p>
<p>∣∣ψ(t)
&rang;
</p>
<p>can be turned into an operator differential equation as follows. Define the
so-called evolution operator U(t) by |ψ(t)〉 = U(t)|ψ(0)〉, and substitute
</p>
<p>evolution operator
in the Schr&ouml;dinger equation to obtain
</p>
<p>i
&part;
</p>
<p>&part;t
U(t)
</p>
<p>∣∣ψ(0)
&rang;
= HU(t)
</p>
<p>∣∣ψ(0)
&rang;
.
</p>
<p>Ignoring the arbitrary vector |ψ(0)〉 results in dU/dt =&minus;iHU(t), a differen-
tial equation in U(t), where H is not dependent on t . We can find a solution
to such an equation by repeated differentiation followed by Taylor series
expansion. Thus,
</p>
<p>d2U
</p>
<p>dt2
=&minus;iHdU
</p>
<p>dt
=&minus;iH
</p>
<p>[
&minus;iHU(t)
</p>
<p>]
= (&minus;iH)2U(t),
</p>
<p>d3U
</p>
<p>dt3
= d
</p>
<p>dt
</p>
<p>[
(&minus;iH)2U(t)
</p>
<p>]
= (&minus;iH)2 dU
</p>
<p>dt
= (&minus;iH)3U(t).
</p>
<p>3This is a consequence of a more general result that if two operators commute, any pair
of functions of those operators also commute (see Problem 4.14).</p>
<p/>
</div>
<div class="page"><p/>
<p>110 4 Operator Algebra
</p>
<p>In general dnU/dtn = (&minus;iH)nU(t). Assuming that U(t) is well-defined at
t = 0, the above relations say that all derivatives of U(t) are also well-
defined at t = 0. Therefore, we can expand U(t) around t = 0 to obtain
</p>
<p>U(t)=
&infin;&sum;
</p>
<p>n=0
</p>
<p>tn
</p>
<p>n!
</p>
<p>(
dnU
</p>
<p>dtn
</p>
<p>)
</p>
<p>t=0
=
</p>
<p>&infin;&sum;
</p>
<p>n=0
</p>
<p>tn
</p>
<p>n! (&minus;iH)
nU(0)
</p>
<p>=
( &infin;&sum;
</p>
<p>n=0
</p>
<p>(&minus;itH)n
n!
</p>
<p>)
U(0)= e&minus;itHU(0).
</p>
<p>But U(0)= 1 by the definition of U(t). Hence, U(t)= e&minus;itH, and
∣∣ψ(t)
</p>
<p>&rang;
= e&minus;itH
</p>
<p>∣∣ψ(0)
&rang;
.
</p>
<p>Let us see under what conditions we have exp(S+ T) = exp(S) exp(T).
We consider only the case where the commutator of the two operators com-
mutes with both of them:
</p>
<p>[
T, [S,T]
</p>
<p>]
= 0=
</p>
<p>[
S, [S,T]
</p>
<p>]
.
</p>
<p>Now consider the operator U(t) = etSetTe&minus;t (S+T) and differentiate it using
the result of Example 4.2.2 and the product rule for differentiation:
</p>
<p>dU
</p>
<p>dt
= SetSetTe&minus;t (S+T) + etSTetTe&minus;t (S+T) &minus; etSetT(S+ T)e&minus;t (S+T)
</p>
<p>= SetSetTe&minus;t (S+T) &minus; etSetTSe&minus;t (S+T). (4.8)
</p>
<p>The three factors of U(t) are present in all terms; however, they are not
always next to one another. We can switch the operators if we introduce a
commutator. For instance, etTS= SetT + [etT,S].
</p>
<p>It is left as a problem for the reader to show that if [S,T] commutes with
S and T, then [etT,S] = &minus;t[S,T]etT, and therefore, etTS= SetT&minus; t[S,T]etT.
Substituting this in Eq. (4.8) and noting that etSS = SetS yields dU/dt =
t[S,T]U(t). The solution to this equation is
</p>
<p>U(t)= exp
(
t2
</p>
<p>2
[S,T]
</p>
<p>)
&rArr; etSetTe&minus;t (S+T) = exp
</p>
<p>(
t2
</p>
<p>2
[S,T]
</p>
<p>)
</p>
<p>because U(0)= 1. We thus have the following:
</p>
<p>Proposition 4.2.4 Let S,T &isin; L(V). If [S, [S,T]] = 0= [T, [S,T]], then the
Baker&ndash;Campbell&ndash;Hausdorff formula holds:
</p>
<p>Baker-Campbell-
</p>
<p>Hausdorff
</p>
<p>formula
etSetTe&minus;(t
</p>
<p>2/2)[S,T] = et (S+T). (4.9)
</p>
<p>In particular, etSetT = et (S+T) if and only if [S,T] = 0.
</p>
<p>If t = 1, Eq. (4.9) reduces to
</p>
<p>eSeTe&minus;(1/2)[S,T] = eS+T. (4.10)</p>
<p/>
</div>
<div class="page"><p/>
<p>4.2 Derivatives of Operators 111
</p>
<p>Now assume that both H(t) and its derivative commute with [H, dH/dt].
Letting S= H(t) and T=�tdH/dt in (4.10), we obtain
</p>
<p>eH(t+�t) = eH(t)+�tdH/dt
</p>
<p>= eH(t)e�t(dH/dt)e&minus;[H(t),�tdH/dt]/2.
</p>
<p>For infinitesimal �t , this yields
</p>
<p>eH(t+�t) = eH(t)
(
1+�t dH
</p>
<p>dt
</p>
<p>)(
1&minus; 1
</p>
<p>2
�t
</p>
<p>[
H(t),
</p>
<p>dH
</p>
<p>dt
</p>
<p>])
</p>
<p>= eH(t)
{
1+�t dH
</p>
<p>dt
&minus; 1
</p>
<p>2
�t
</p>
<p>[
H(t),
</p>
<p>dH
</p>
<p>dt
</p>
<p>]}
,
</p>
<p>and we have
</p>
<p>d
</p>
<p>dt
eH(t) = eH dH
</p>
<p>dt
&minus; 1
</p>
<p>2
eH
</p>
<p>[
H,
</p>
<p>dH
</p>
<p>dt
</p>
<p>]
.
</p>
<p>We can also write
</p>
<p>eH(t+�t) = e[H(t)+�tdH/dt] = e[�tdH/dt+H(t)]
</p>
<p>= e[�tdH/dt]eH(t)e&minus;[�tdH/dt,H(t)]/2,
</p>
<p>which yields
</p>
<p>d
</p>
<p>dt
eH(t) = dH
</p>
<p>dt
eH + 1
</p>
<p>2
eH
</p>
<p>[
H,
</p>
<p>dH
</p>
<p>dt
</p>
<p>]
.
</p>
<p>Adding the above two expressions and dividing by 2 yields the following
symmetric expression for the derivative:
</p>
<p>d
</p>
<p>dt
eH(t) = 1
</p>
<p>2
</p>
<p>(
dH
</p>
<p>dt
eH + eH dH
</p>
<p>dt
</p>
<p>)
= 1
</p>
<p>2
</p>
<p>{
dH
</p>
<p>dt
, eH
</p>
<p>}
,
</p>
<p>where {S,T} &equiv; ST + TS is called the anticommutator of the operators S
anticommutator
</p>
<p>and T. We, therefore, have the following proposition.
</p>
<p>Proposition 4.2.5 Let H : R&rarr; L(V) and assume that H and its derivative
commute with [H, dH/dt]. Then
</p>
<p>d
</p>
<p>dt
eH(t) = 1
</p>
<p>2
</p>
<p>{
dH
</p>
<p>dt
, eH
</p>
<p>}
.
</p>
<p>In particular, if [H, dH/dt] = 0, then
d
</p>
<p>dt
eH(t) = dH
</p>
<p>dt
eH = eH dH
</p>
<p>dt
.
</p>
<p>A frequently encountered operator is F(t) = etABe&minus;tA, where A and B
are t-independent. It is straightforward to show that
</p>
<p>dF
</p>
<p>dt
=
[
A,F(t)
</p>
<p>]
and
</p>
<p>d
</p>
<p>dt
</p>
<p>[
A,F(t)
</p>
<p>]
=
[
A,
</p>
<p>dF
</p>
<p>dt
</p>
<p>]
.</p>
<p/>
</div>
<div class="page"><p/>
<p>112 4 Operator Algebra
</p>
<p>Using these results, we can write
</p>
<p>d2F
</p>
<p>dt2
= d
</p>
<p>dt
</p>
<p>[
A,F(t)
</p>
<p>]
=
[
A,
</p>
<p>[
A,F(t)
</p>
<p>]]
&equiv; A2
</p>
<p>[
F(t)
</p>
<p>]
,
</p>
<p>and in general, dnF/dtn = An[F(t)], where An[F(t)] is defined inductively
as An[F(t)] = [A,An&minus;1[F(t)]], with A0[F(t)] &equiv; F(t). For example,
</p>
<p>A3
[
F(t)
</p>
<p>]
=
[
A,A2
</p>
<p>[
F(t)
</p>
<p>]]
=
[
A,
</p>
<p>[
A,A
</p>
<p>[
F(t)
</p>
<p>]]]
=
[
A,
</p>
<p>[
A,
</p>
<p>[
A,F(t)
</p>
<p>]]]
.
</p>
<p>Evaluating F(t) and all its derivatives at t = 0 and substituting in the
Taylor expansion about t = 0, we get
</p>
<p>F(t)=
&infin;&sum;
</p>
<p>n=0
</p>
<p>tn
</p>
<p>n!
dnF
</p>
<p>dtn
</p>
<p>∣∣∣∣
t=0
</p>
<p>=
&infin;&sum;
</p>
<p>n=0
</p>
<p>tn
</p>
<p>n!A
n
[
F(0)
</p>
<p>]
=
</p>
<p>&infin;&sum;
</p>
<p>n=0
</p>
<p>tn
</p>
<p>n!A
n[B].
</p>
<p>That is,
</p>
<p>etABe&minus;tA =
&infin;&sum;
</p>
<p>n=0
</p>
<p>tn
</p>
<p>n!A
n[B] &equiv; B+ t[A,B] + t
</p>
<p>2
</p>
<p>2!
[
A, [A,B]
</p>
<p>]
+ &middot; &middot; &middot; .
</p>
<p>Sometimes this is written symbolically as
</p>
<p>etABe&minus;tA =
( &infin;&sum;
</p>
<p>n=0
</p>
<p>tn
</p>
<p>n!A
n
</p>
<p>)
[B] &equiv; etA[B],
</p>
<p>where the RHS is merely an abbreviation of the infinite sum in the middle.
For t = 1 we obtain a widely used formula:
</p>
<p>eABe&minus;A = eA[B] =
( &infin;&sum;
</p>
<p>n=0
</p>
<p>1
</p>
<p>n!A
n
</p>
<p>)
[B] &equiv; B+ [A,B] + 1
</p>
<p>2!
[
A, [A,B]
</p>
<p>]
+ &middot; &middot; &middot; .
</p>
<p>If A commutes with [A,B], then the infinite series truncates at the second
term, and we have
</p>
<p>etABe&minus;tA = B+ t[A,B].
For instance, if A and B are replaced by D and T of Example 2.3.5 and
</p>
<p>Eq. (4.7), we get
</p>
<p>etDTe&minus;tD = T+ t[D,T] = T+ t1.
</p>
<p>The RHS shows that the operator T has been translated by an amount t
generator of translation
</p>
<p>(more precisely, by t times the unit operator). We therefore call exp(tD) the
translation operator of T by t , and we call D the generator of translation.
With a little modification T and D become, respectively, the position and
momentum operators in quantum mechanics. Thus,
</p>
<p>momentum as generator
</p>
<p>of translation
</p>
<p>Box 4.2.6 Momentum is the generator of translation in quantum me-
chanics.
</p>
<p>But more of this later!</p>
<p/>
</div>
<div class="page"><p/>
<p>4.3 Conjugation of Operators 113
</p>
<p>4.3 Conjugation of Operators
</p>
<p>We have discussed the notion of the dual of a vector in conjunction with
inner products. We now incorporate linear operators into this notion. Let
|b〉, |c〉 &isin; V and assume that |c〉 = T|b〉. We know that there are linear
functionals in the dual space V&lowast; that are associated with (|b〉)&dagger; = 〈b| and
(|c〉)&dagger; = 〈c|. Is there a linear operator belonging to L(V&lowast;) that somehow
corresponds to T? In other words, can we find a linear operator that relates
〈b| and 〈c| just as T relates |b〉 and |c〉? The answer comes in the following
definition.
</p>
<p>Definition 4.3.1 Let T &isin;L(V) and |a〉, |b〉 &isin; V. The adjoint, or hermitian
adjoint of an operator
</p>
<p>conjugate, of T is denoted by T&dagger; and defined by4
</p>
<p>〈a|T|b〉&lowast; = 〈b|T&dagger;|a〉. (4.11)
</p>
<p>The LHS of Eq. (4.11) can be written as 〈a | c〉&lowast; or 〈c | a〉, in which case
we can identify
</p>
<p>〈c| = 〈b|T&dagger; &rArr;
(
T|b〉
</p>
<p>)&dagger; = 〈b|T&dagger;. (4.12)
</p>
<p>This equation is sometimes used as the definition of the hermitian conjugate.
From Eq. (4.11), the reader may easily verify that 1&dagger; = 1. Thus, using the
unit operator for T, (4.12) justifies Eq. (2.26).
</p>
<p>Some of the properties of conjugation are listed in the following theorem,
whose proof is left as an exercise.
</p>
<p>Theorem 4.3.2 Let U,T &isin; End(V) and α &isin;C. Then
</p>
<p>1. (U+ T)&dagger; = U&dagger; + T&dagger;. 2. (UT)&dagger; = T&dagger;U&dagger;.
</p>
<p>3. (αT)&dagger; = α&lowast;T&dagger;. 4.
(
(T)&dagger;
</p>
<p>)&dagger; = T.
</p>
<p>The last identity holds for finite-dimensional vector spaces; it does not
apply to infinite-dimensional vector spaces in general.
</p>
<p>In previous examples dealing with linear operators T :Rn &rarr;Rn, an ele-
ment of Rn was denoted by a row vector, such as (x, y) for R2 and (x, y, z)
for R3. There was no confusion, because we were operating only in V. How-
ever, since elements of both V and V&lowast; are required when discussing T, T&lowast;,
and T&dagger;, it is helpful to make a distinction between them. We therefore resort
to the convention introduced in Example 2.2.3 by which
</p>
<p>Box 4.3.3 Kets are represented as column vectors and bras as row
vectors.
</p>
<p>4With the notion of adjoint already introduced in Definition 2.4.3, we should probably
not use the same name for T&dagger;. However, the adjoint as defined in Definition 2.4.3 is rarely
used in physics. Furthermore, both the notation and the context will make it clear which
adjoint one is talking about. Therefore, there is no risk of confusion.</p>
<p/>
</div>
<div class="page"><p/>
<p>114 4 Operator Algebra
</p>
<p>Example 4.3.4 Let us find the hermitian conjugate of the operator T :
C3 &rarr;C3 given by
</p>
<p>T
</p>
<p>⎛
⎝
α1
α2
α3
</p>
<p>⎞
⎠=
</p>
<p>⎛
⎝
α1 &minus; iα2 + α3
</p>
<p>iα1 &minus; α3
α1 &minus; α2 + iα3
</p>
<p>⎞
⎠ .
</p>
<p>Introduce
</p>
<p>|a〉 =
</p>
<p>⎛
⎝
α1
α2
α3
</p>
<p>⎞
⎠ and |b〉 =
</p>
<p>⎛
⎝
β1
β2
β3
</p>
<p>⎞
⎠
</p>
<p>with dual vectors 〈a| = (α&lowast;1 α&lowast;2 α&lowast;3) and 〈b| = (β&lowast;1 β&lowast;2 β&lowast;3 ), respectively. We
use Eq. (4.11) to find T&dagger;:
</p>
<p>〈b|T&dagger;|a〉
</p>
<p>= 〈a|T|b〉&lowast; =
</p>
<p>⎡
⎣(α&lowast;1 α&lowast;2 α&lowast;3
</p>
<p>)
T
</p>
<p>⎛
⎝
β1
β2
β3
</p>
<p>⎞
⎠
⎤
⎦
&lowast;
</p>
<p>=
</p>
<p>⎡
⎣(α&lowast;1 α&lowast;2 α&lowast;3
</p>
<p>)
⎛
⎝
β1 &minus; iβ2 + β3
</p>
<p>iβ1 &minus; β3
β1 &minus; β2 + iβ3
</p>
<p>⎞
⎠
⎤
⎦
&lowast;
</p>
<p>=
[
α&lowast;1β1 &minus; iα&lowast;1β2 + α&lowast;1β3 + iα&lowast;2β1 &minus; α&lowast;2β3 + α&lowast;3β1 &minus; α&lowast;3β2 + iα&lowast;3β3
</p>
<p>]&lowast;
.
</p>
<p>Taking the complex conjugate of all the numbers inside the square brackets,
we find
</p>
<p>〈b|T&dagger;|a〉 =
(
β&lowast;1 β
</p>
<p>&lowast;
2 β
</p>
<p>&lowast;
3
</p>
<p>)
︸ ︷︷ ︸
</p>
<p>=〈b|
</p>
<p>⎛
⎝
α1 &minus; iα2 + α3
</p>
<p>iα1 &minus; α3
α1 &minus; α2 &minus; iα3
</p>
<p>⎞
⎠ .
</p>
<p>Therefore, we obtain
</p>
<p>T&dagger;
</p>
<p>⎛
⎝
α1
α2
α3
</p>
<p>⎞
⎠=
</p>
<p>⎛
⎝
α1 &minus; iα2 + α3
</p>
<p>iα1 &minus; α3
α1 &minus; α2 &minus; iα3
</p>
<p>⎞
⎠ .
</p>
<p>4.3.1 Hermitian Operators
</p>
<p>The process of conjugation of linear operators looks much like conjugation
of complex numbers. Equation (4.11) alludes to this fact, and Theorem 4.3.2
provides further evidence. It is therefore natural to look for operators that
are counterparts of real numbers. One can define complex conjugation for
operators and thereby construct real operators. However, these real operators
will not be interesting because&mdash;as it turns out&mdash;they completely ignore the
complex character of the vector space. The following alternative definition</p>
<p/>
</div>
<div class="page"><p/>
<p>4.3 Conjugation of Operators 115
</p>
<p>makes use of hermitian conjugation, and the result will have much wider
application than is allowed by a mere complex conjugation.
</p>
<p>Definition 4.3.5 A linear operator H &isin; L(V) is called hermitian, or self-
hermitian and
</p>
<p>anti-hermitian operators
adjoint, if H&dagger; = H. Similarly, A &isin; L(V) is called anti-hermitian if A&dagger; =
&minus;A.
</p>
<p>Historical Notes
</p>
<p>Charles Hermite (1822&ndash;1901), one of the most eminent French mathematicians of the
nineteenth century, was particularly distinguished for the clean elegance and high artistic
quality of his work. As a student, he courted disaster by neglecting his routine assigned
work to study the classic masters of mathematics; and though he nearly failed his exam-
inations, he became a first-rate creative mathematician while still in his early twenties.
In 1870 he was appointed to a professorship at the Sorbonne, where he trained a whole
generation of well-known French mathematicians, including Picard, Borel, and Poincar&eacute;.
</p>
<p>Charles Hermite
</p>
<p>1822&ndash;1901
</p>
<p>The character of his mind is suggested by a remark of Poincar&eacute;: &ldquo;Talk with M. Hermite.
He never evokes a concrete image, yet you soon perceive that the most abstract entities
are to him like living creatures.&rdquo; He disliked geometry, but was strongly attracted to num-
ber theory and analysis, and his favorite subject was elliptic functions, where these two
fields touch in many remarkable ways. Earlier in the century the Norwegian genius Abel
had proved that the general equation of the fifth degree cannot be solved by functions
involving only rational operations and root extractions. One of Hermite&rsquo;s most surprising
achievements (in 1858) was to show that this equation can be solved by elliptic functions.
His 1873 proof of the transcendence of e was another high point of his career.5 If he had
been willing to dig even deeper into this vein, he could probably have disposed of π as
well, but apparently he had enough of a good thing. As he wrote to a friend, &ldquo;I shall risk
nothing on an attempt to prove the transcendence of the number π . If others undertake
this enterprise, no one will be happier than I at their success, but believe me, my dear
friend, it will not fail to cost them some efforts.&rdquo; As it turned out, Lindemann&rsquo;s proof
nine years later rested on extending Hermite&rsquo;s method.
Several of his purely mathematical discoveries had unexpected applications many years
later to mathematical physics. For example, the Hermitian forms and matrices that he
invented in connection with certain problems of number theory turned out to be crucial
for Heisenberg&rsquo;s 1925 formulation of quantum mechanics, and Hermite polynomials (see
Chap. 8) are useful in solving Schr&ouml;dinger&rsquo;s wave equation.
</p>
<p>The following observations strengthen the above conjecture that conjuga-
tion of complex numbers and hermitian conjugation of operators are some-
how related.
</p>
<p>Definition 4.3.6 The expectation value 〈T〉a of an operator T in the &ldquo;state&rdquo;
expectation value|a〉 is a complex number defined by 〈T〉a = 〈a|T|a〉.
</p>
<p>The complex conjugate of the expectation value is6
</p>
<p>〈T〉&lowast; = 〈a|T|a〉&lowast; = 〈a|T&dagger;|a〉.
</p>
<p>5Transcendental numbers are those that are not roots of polynomials with integer coeffi-
cients.
6When no risk of confusion exists, it is common to drop the subscript &ldquo;a&rdquo; and write 〈T〉
for the expectation value of T.</p>
<p/>
</div>
<div class="page"><p/>
<p>116 4 Operator Algebra
</p>
<p>In words, T&dagger;, the hermitian conjugate of T, has an expectation value that
is the complex conjugate of the latter&rsquo;s expectation value. In particular, if T
is hermitian&mdash;is equal to its hermitian conjugate&mdash;its expectation value will
be real.
</p>
<p>What is the analogue of the known fact that a complex number is the sum
of a real number and a pure imaginary one? The decomposition
</p>
<p>T= 1
2
</p>
<p>(
T+ T&dagger;
</p>
<p>)
+ 1
</p>
<p>2
</p>
<p>(
T&minus; T&dagger;
</p>
<p>)
&equiv; X+A
</p>
<p>shows that any operator can be written as a sum of a hermitian operator
X= 12 (T+ T&dagger;) and an anti-hermitian operator A= 12 (T&minus; T&dagger;).
</p>
<p>We can go even further, because any anti-hermitian operator A can be
written as A = i(&minus;iA) in which &minus;iA is hermitian: (&minus;iA)&dagger; = (&minus;i)&lowast;A&dagger; =
i(&minus;A)=&minus;iA. Denoting &minus;iA by Y, we write T= X+ iY, where both X and
Y are hermitian. This is the analogue of the decomposition z = x + iy in
which both x and y are real.
</p>
<p>Clearly, we should expect some departures from a perfect correspon-
dence. This is due to a lack of commutativity among operators. For instance,
although the product of two real numbers is real, the product of two hermi-
tian operators is not, in general, hermitian:
</p>
<p>(TU)&dagger; = U&dagger;T&dagger; = UT �= TU.
</p>
<p>We have seen the relation between expectation values and conjugation prop-
erties of operators. The following theorem completely characterizes hermi-
tian operators in terms of their expectation values:
</p>
<p>Theorem 4.3.7 A linear map H on a complex inner product space is her-
mitian if and only if 〈a|H|a〉 is real for all |a〉.
</p>
<p>Proof We have already pointed out that a hermitian operator has real expec-
tation values. Conversely, assume that 〈a|H|a〉 is real for all |a〉. Then
</p>
<p>〈a|H|a〉 = 〈a|H|a〉&lowast; = 〈a|H&dagger;|a〉 &hArr; 〈a|H&minus;H&dagger;|a〉 = 0 &forall;|a〉.
</p>
<p>By Theorem 2.3.8 we must have H&minus;H&dagger; = 0. �
</p>
<p>Example 4.3.8 In this example, we illustrate the result of the above the-
orem with 2 &times; 2 matrices. The matrix H =
</p>
<p>( 0 &minus;i
i 0
</p>
<p>)
is hermitian7 and acts
</p>
<p>on C2. Let us take an arbitrary vector |a〉 =
( α1
α2
</p>
<p>)
and evaluate 〈a|H|a〉. We
</p>
<p>have
</p>
<p>H|a〉 =
(
</p>
<p>0 &minus;i
i 0
</p>
<p>)(
α1
α2
</p>
<p>)
=
(&minus;iα2
</p>
<p>iα1
</p>
<p>)
.
</p>
<p>7We assume that the reader has a casual familiarity with hermitian matrices. Think of
an n &times; n matrix as a linear operator that acts on column vectors whose elements are
components of vectors defined in the standard basis of Cn or Rn. A hermitian matrix then
becomes a hermitian operator.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.3 Conjugation of Operators 117
</p>
<p>Therefore,
</p>
<p>〈a|H|a〉 =
(
α&lowast;1 α
</p>
<p>&lowast;
2
</p>
<p>)(&minus;iα2
iα1
</p>
<p>)
=&minus;iα&lowast;1α2 + iα&lowast;2α1
</p>
<p>= iα&lowast;2α1 +
(
iα&lowast;2α1
</p>
<p>)&lowast; = 2 Re
(
iα&lowast;2α1
</p>
<p>)
,
</p>
<p>and 〈a|H|a〉 is real.
For the most general 2 &times; 2 hermitian matrix H =
</p>
<p>( α β
β&lowast; γ
</p>
<p>)
, where α and γ
</p>
<p>are real, we have
</p>
<p>H|a〉 =
(
α β
</p>
<p>β&lowast; γ
</p>
<p>)(
α1
α2
</p>
<p>)
=
(
αα1 + βα2
β&lowast;α1 + γ α2
</p>
<p>)
</p>
<p>and
</p>
<p>〈a|H|a〉 =
(
α&lowast;1 α
</p>
<p>&lowast;
2
</p>
<p>)( αα1 + βα2
β&lowast;α1 + γ α2
</p>
<p>)
= α&lowast;1(αα1 + βα2)+ α&lowast;2
</p>
<p>(
β&lowast;α1 + γ α2
</p>
<p>)
</p>
<p>= α|α1|2 + α&lowast;1βα2 + α&lowast;2β&lowast;α1 + γ |α2|2
</p>
<p>= α|α1|2 + γ |α2|2 + 2 Re
(
α&lowast;1βα2
</p>
<p>)
.
</p>
<p>Again 〈a|H|a〉 is real.
</p>
<p>Definition 4.3.9 An operator A on an inner product space is called positive
(written A&ge; 0) if 〈a|A|a〉 &ge; 0 for all |a〉 �= |0〉. By Theorem 4.3.7 A is nec-
</p>
<p>positive, strictly positive,
</p>
<p>and positive definite
</p>
<p>operators
</p>
<p>essarily hermitian. We say A is strictly positive or positive definite (written
A&gt; 0) if 〈a|A|a〉&gt; 0 for all |a〉 �= |0〉.
</p>
<p>Theorem 4.3.10 A strictly positive operator T is invertible.
</p>
<p>Proof By Proposition 2.3.14, it is sufficient to prove that kerT = {|0〉}. If
|0〉 �= |a〉 &isin; kerT, then 〈a|T|a〉 = 0, contradicting the fact that T is strictly
positive. �
</p>
<p>Example 4.3.11 An example of a positive operator is the square of a her-
mitian operator.8 We note that for any hermitian operator H and any vector
|a〉, we have 〈a|H2|a〉 = 〈a|H&dagger;H|a〉 = 〈Ha | Ha〉 &ge; 0 because of the positive
definiteness of the inner product.
</p>
<p>From the discussion of the example above, we conclude that the square
of an invertible hermitian operator is positive definite.
</p>
<p>8This is further evidence that hermitian operators are analogues of real numbers: The
square of any real number is positive.</p>
<p/>
</div>
<div class="page"><p/>
<p>118 4 Operator Algebra
</p>
<p>4.3.2 Unitary Operators
</p>
<p>The reader may be familiar with two- and three-dimensional rigid rotations
and the fact that they preserve distances and the scalar product.9 Can this be
generalized to complex inner product spaces? Let |a〉, |b〉 &isin; V, and let U be
an operator on V that preserves the scalar product; that is, given |b&prime;〉 = U|b〉
and |a&prime;〉 = U|a〉, then 〈a&prime; | b&prime;〉 = 〈a | b〉. This yields
</p>
<p>&lang;
a&prime;
</p>
<p>∣∣ b&prime;
&rang;
=
(
〈a|U&dagger;
</p>
<p>)(
U|b〉
</p>
<p>)
= 〈a|U&dagger;U|b〉 = 〈a | b〉 = 〈a|1|b〉.
</p>
<p>Since this is true for arbitrary |a〉 and |b〉, we obtain U&dagger;U = 1. In the next
chapter, when we introduce the concept of the determinant of operators, we
shall see that this relation implies that U and U&dagger; are both invertible,10 with
each one being the inverse of the other.
</p>
<p>Definition 4.3.12 Let V be a finite-dimensional inner product space. An
operator U is called a unitary operator if U&dagger; = U&minus;1. Unitary operators
</p>
<p>unitary operators
preserve the inner product of V.
</p>
<p>Example 4.3.13 The linear transformation T :C3 &rarr;C3 given by
</p>
<p>T
</p>
<p>⎛
⎝
α1
α2
α3
</p>
<p>⎞
⎠=
</p>
<p>⎛
⎝
</p>
<p>(α1 &minus; iα2)/
&radic;
</p>
<p>2
(α1 + iα2 &minus; 2α3)/
</p>
<p>&radic;
6
</p>
<p>{α1 &minus; α2 + α3 + i(α1 + α2 + α3)}/
&radic;
</p>
<p>6
</p>
<p>⎞
⎠
</p>
<p>is unitary. In fact, let
</p>
<p>|a〉 =
</p>
<p>⎛
⎝
α1
α2
α3
</p>
<p>⎞
⎠ and |b〉 =
</p>
<p>⎛
⎝
β1
β2
β3
</p>
<p>⎞
⎠
</p>
<p>with dual vectors 〈a| = (α&lowast;1 α&lowast;2 α&lowast;3) and 〈b| = (β&lowast;1 β&lowast;2 β&lowast;3 ), respectively. We
use Eq. (4.11) and the procedure of Example 4.3.4 to find T&dagger;. The result is
</p>
<p>T&dagger;
</p>
<p>⎛
⎝
α1
α2
α3
</p>
<p>⎞
⎠=
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>α1&radic;
2
+ α2&radic;
</p>
<p>6
+ α3(1&minus;i)&radic;
</p>
<p>6
</p>
<p>iα1&radic;
2
&minus; iα2&radic;
</p>
<p>6
&minus; α3(1+i)&radic;
</p>
<p>6
</p>
<p>&minus; 2α2&radic;
6
+ α3(1&minus;i)&radic;
</p>
<p>6
</p>
<p>⎞
⎟⎟⎟⎠ ,
</p>
<p>and we can verify that
</p>
<p>TT&dagger;
</p>
<p>⎛
⎝
α1
α2
α3
</p>
<p>⎞
⎠=
</p>
<p>⎛
⎝
α1
α2
α3
</p>
<p>⎞
⎠ .
</p>
<p>9We have also encountered isometries, which are more general than unitary operators.
The word &ldquo;unitary&rdquo; is usually reserved for isometries on sesquilinear (hermitian) inner
product spaces.
10This implication holds only for finite-dimensional vector spaces.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.4 Idempotents 119
</p>
<p>Thus TT&dagger; = 1. Similarly, we can show that T&dagger;T= 1 and therefore that T is
unitary.
</p>
<p>4.4 Idempotents
</p>
<p>We have already considered the decomposition of a vector space into sub-
spaces in Sect. 2.1.3. We have also pointed out the significance of subspaces
resulting from the fact that physics frequently takes place not inside the
whole vector space, but in one of its subspaces. For instance, the study of
projectile motion teaches us that it is very convenient to &ldquo;project&rdquo; the mo-
tion onto the horizontal and vertical axes and to study these projections sep-
arately. It is, therefore, appropriate to ask how we can go from a full space
to one of its subspaces in the context of linear operators.
</p>
<p>Let us first consider a simple example. A point in the plane is designated
by the coordinates (x, y). A subspace of the plane is the x-axis. Is there
a linear operator,11 say Px , that acts on such a point and somehow sends
it into that subspace? Of course, there are many operators from R2 to R.
However, we are looking for a specific one. We want Px to project the point
onto the x-axis. Such an operator has to act on (x, y) and produce (x,0):
Px(x, y)= (x,0). Therefore, if the point already lies on the x-axis, Px does
not change it. In particular, if we apply Px twice, we get the same result as if
we apply it only once. And this is true for any point in the plane. Therefore,
our operator must have the property P2x = Px , i.e., it must be an idempotent
of the algebra End(V).
</p>
<p>Suppose that V is the direct sum of r of its subspaces:
</p>
<p>V=U1 &oplus; &middot; &middot; &middot; &oplus;Ur =
r&oplus;
</p>
<p>i=1
Ui .
</p>
<p>For any |v〉 &isin; V, define Pj by Pj |v〉 = |vj 〉, where |vj 〉 is the component of
|v〉 in Uj . It is easy (and instructive) to prove that Pj is a linear operator;
i.e., that Pj &isin; End(V). Moreover, since Pj |vj 〉 = |vj 〉 for all |vj 〉 &isin; Uj , we
have
</p>
<p>P2j |v〉 = PjPj |v〉 = Pj |vj 〉 = |vj 〉 = Pj |v〉
</p>
<p>for all |v〉 &isin; V. It follows that P2j = Pj , i.e., that Pj is an idempotent.
Next note that, for j �= k,
</p>
<p>PjPk|v〉 = Pj |vk〉 = |0〉,
</p>
<p>because |vk〉 has no component in Uj . Since this is true for all j �= k and all
|v〉 &isin; V, we have
</p>
<p>PjPk = PkPj = 0,
</p>
<p>11We want this operator to preserve the vector-space structure of the plane and the axis.</p>
<p/>
</div>
<div class="page"><p/>
<p>120 4 Operator Algebra
</p>
<p>i.e., that the idempotents are orthogonal. Furthermore, since
</p>
<p>1|v〉 = |v〉 =
r&sum;
</p>
<p>j=1
|vj 〉 =
</p>
<p>r&sum;
</p>
<p>j=1
Pj |v〉 =
</p>
<p>(
r&sum;
</p>
<p>j=1
Pj
</p>
<p>)
|v〉
</p>
<p>for all |v〉 &isin; V, we must have &sum;rj=1 Pj = 1. We summarize the foregoing
observation as
</p>
<p>Proposition 4.4.1 Let V be the direct sum of {Ui}ri=1 and let Pj &isin;L(V) be
defined by Pj |v〉 = |vj 〉 with |vj 〉 the component of |v〉 in Uj . Then {Pi}ri=1
is a complete set of orthogonal idempotents, i.e.,
</p>
<p>PiPj = δijPi (no sum) and
r&sum;
</p>
<p>j=1
Pj = 1.
</p>
<p>If T &isin; End(V) and {Pi}ri=1 is a complete set of orthogonal idempotents
corresponding to {Ui}ri=1, then T can be written as a sum of operators
which restrict to the subspaces. More precisely, multiply the sum in Propo-
sition 4.4.1 by T on the right to get
</p>
<p>T=
r&sum;
</p>
<p>j=1
PjT&equiv;
</p>
<p>r&sum;
</p>
<p>j=1
Tj , Tj = PjT (4.13)
</p>
<p>and note that Tj &isin; End(Uj ).
</p>
<p>4.4.1 Projection Operators
</p>
<p>When the vector space carries an inner product, it is useful to demand her-
miticity for the idempotents:
</p>
<p>Definition 4.4.2 A hermitian idempotent of End(V) is called a projection
projection operators
</p>
<p>operator.
</p>
<p>Consider two projection operators P1 and P2. We want to investigate con-
ditions under which P1 + P2 becomes a projection operator. By definition,
</p>
<p>P1 +P2 = (P1 +P2)2 = P21 +P1P2 +P2P1 +P22 = P1 +P1P2 +P2P1 +P2.
</p>
<p>So P1 + P2 is a projection operator if and only if
</p>
<p>P1P2 + P2P1 = 0. (4.14)
</p>
<p>Multiply this on the left by P1 to get
</p>
<p>P21P2 + P1P2P1 = 0 &rArr; P1P2 + P1P2P1 = 0.
</p>
<p>Now multiply the same equation on the right by P1 to get
</p>
<p>P1P2P1 + P2P21 = 0 &rArr; P1P2P1 + P2P1 = 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.4 Idempotents 121
</p>
<p>These last two equations yield
</p>
<p>P1P2 &minus; P2P1 = 0. (4.15)
</p>
<p>The solution to Eqs. (4.14) and (4.15) is P1P2 = P2P1 = 0. We therefore
have the following result.
</p>
<p>Proposition 4.4.3 Let P1,P2 &isin;L(V) be projection operators. Then P1+P2
is a projection operator if and only if P1 and P2 are orthogonal.
</p>
<p>More generally, if there is a set {Pi}mi=1 of projection operators satisfying
</p>
<p>PiPj =
{
Pi if i = j,
0 if i �= j,
</p>
<p>then P=&sum;mi=1 Pi is also a projection operator.
Given a normal vector |e〉, one can show easily that P= |e〉〈e| is a pro-
</p>
<p>jection operator:
</p>
<p>&bull; P is hermitian: P&dagger; = (|e〉〈e|)&dagger; = (〈e|)&dagger;(|e〉)&dagger; = |e〉〈e|.
&bull; P equals its square: P2 = (|e〉〈e|)(|e〉〈e|)= |e〉 〈e|e〉︸︷︷︸
</p>
<p>=1
</p>
<p>〈e| = |e〉〈e|.
</p>
<p>Let |y〉 be any nonzero vector in an inner product space V. The normal
vector |ey〉 along |y〉 is |ey〉 = |y〉/
</p>
<p>&radic;〈y | y〉. From this, we construct the
projection operator Py along |y〉:
</p>
<p>Py = |ey〉〈ey | =
|y〉&radic;〈y | y〉
</p>
<p>〈y|&radic;〈y | y〉 =
|y〉〈y|
〈y | y〉 .
</p>
<p>Given any vector |x〉, Py |x〉 is the component of |x〉 along |y〉. Hence,
</p>
<p>|x〉 &minus; Py |x〉 or (1&minus; Py)|x〉
</p>
<p>is the component of |x〉 perpendicular to |y〉. The reflection of |x〉 in |y〉 is
therefore (see Fig. 4.2)
</p>
<p>Py |x〉 &minus; (1&minus; Py)|x〉 = 2Py |x〉 &minus; |x〉 = (2Py &minus; 1)|x〉 = 2
〈y | x〉
〈y | y〉 |y〉 &minus; |x〉.
</p>
<p>As shown in Fig. 4.2, from a two- or three-dimenstional geometric point of
view, it is clear that the negative of this last vector is the reflection in the
plane perpendicular to |y〉.12 We generalize this to any vector space:
</p>
<p>Definition 4.4.4 For any nonzero vector |y〉 the projection operator Py
along |y〉 and the reflection operator Ry in the plane perpendicular to |y〉 reflection operator
</p>
<p>12One can note more directly&mdash;also shown in Fig. 4.2&mdash;that in three-dimensional geom-
etry, if one adds to |x〉 twice the negative of its projection on |y〉, one gets the reflection
of |x〉 in the plane perpendicular to |y〉.</p>
<p/>
</div>
<div class="page"><p/>
<p>122 4 Operator Algebra
</p>
<p>Fig. 4.2 The vectors |x〉 and |y〉 and the reflections of |x〉 in |y〉 and in the plane perpen-
dicular to |y〉
</p>
<p>are given by
</p>
<p>Py =
|y〉〈y|
〈y|y〉 and Ry = 1&minus; 2Py = 1&minus; 2
</p>
<p>|y〉〈y|
〈y|y〉 .
</p>
<p>For any other vector |x〉, the component |x〉y of |x〉 along |y〉 and its reflec-
tion |x〉r,y in the plane perpendicular to |y〉 are given by
</p>
<p>|x〉y = Py |x〉 =
〈y|x〉
〈y|y〉 |y〉 and |x〉r,y = Ry |x〉 = |x〉 &minus; 2
</p>
<p>〈y|x〉
〈y|y〉 |y〉.
</p>
<p>The relations |y〉y = |y〉 and |y〉r,y =&minus;|y〉 confirm our intuitive geomet-
rical expectation.
</p>
<p>Example 4.4.5 Let V be a one-dimensional vector space. Let |a〉 be any
nonzero vector in V. Any other vector |x〉 can be written as |x〉 = α|a〉 for
some number α. Then
</p>
<p>Pa|x〉 =
|a〉〈a|
〈a|a〉 |x〉 =
</p>
<p>〈a|x〉
〈a|a〉 |a〉 =
</p>
<p>α〈a|a〉
〈a|a〉 |a〉 = α|a〉 = |x〉.
</p>
<p>Since this is true for all |a〉 and |x〉, we conclude that Py = 1 for any |y〉 in
a one-dimensional vector space. The reflection operator can also be found.
Therefore,
</p>
<p>Py = 1 and Ry =&minus;1
in a one-dimensional vector space such as the complex vector space C and
the real vector space R.
</p>
<p>We can take an orthonormal basis B = {|ei〉}Ni=1 and construct a set of
projection operators {Pi = |ei〉〈ei |}Ni=1. The operators Pi are mutually or-
thogonal. Thus, their sum
</p>
<p>&sum;N
i=1 Pi is also a projection operator.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.4 Idempotents 123
</p>
<p>Proposition 4.4.6 Let B = {|ei〉}Ni=1 be an orthonormal basis for VN . Then
the set {Pi = |ei〉〈ei |}Ni=1 consists of mutually orthogonal projection opera-
tors, and
</p>
<p>completeness relation
N&sum;
</p>
<p>i=1
Pi =
</p>
<p>N&sum;
</p>
<p>i=1
|ei〉〈ei | = 1.
</p>
<p>This relation is called the completeness relation.
</p>
<p>Proof The proof is left as Problem 4.26. �
</p>
<p>If we choose only the first m &lt; N vectors, then the projection operator
P(m) &equiv; &sum;mi=1 |ei〉〈ei | projects arbitrary vectors into the subspace spanned
by the first m basis vectors {|ei〉}mi=1. In other words, when P(m) acts on any
vector |a〉 &isin; V, the result will be a linear combination of only the first m
vectors. The simple proof of this fact is left as an exercise. These points are
illustrated in the following example.
</p>
<p>Example 4.4.7 Consider three orthonormal vectors {|ei〉}3i=1 &isin;R3 given by
</p>
<p>|e1〉 =
1&radic;
2
</p>
<p>⎛
⎝
</p>
<p>1
1
0
</p>
<p>⎞
⎠ , |e2〉 =
</p>
<p>1&radic;
6
</p>
<p>⎛
⎝
</p>
<p>1
&minus;1
2
</p>
<p>⎞
⎠ , |e3〉 =
</p>
<p>1&radic;
3
</p>
<p>⎛
⎝
&minus;1
1
1
</p>
<p>⎞
⎠ .
</p>
<p>The projection operators associated with each of these can be obtained by
noting that 〈ei | is a row vector. Therefore,
</p>
<p>P1 = |e1〉〈e1| =
1
</p>
<p>2
</p>
<p>⎛
⎝
</p>
<p>1
1
0
</p>
<p>⎞
⎠(1 1 0
</p>
<p>)
= 1
</p>
<p>2
</p>
<p>⎛
⎝
</p>
<p>1 1 0
1 1 0
0 0 0
</p>
<p>⎞
⎠ .
</p>
<p>Similarly,
</p>
<p>P2 =
1
</p>
<p>6
</p>
<p>⎛
⎝
</p>
<p>1
&minus;1
2
</p>
<p>⎞
⎠(1 &minus;1 2
</p>
<p>)
= 1
</p>
<p>6
</p>
<p>⎛
⎝
</p>
<p>1 &minus;1 2
&minus;1 1 &minus;2
2 &minus;2 4
</p>
<p>⎞
⎠
</p>
<p>and
</p>
<p>P3 =
1
</p>
<p>3
</p>
<p>⎛
⎝
&minus;1
1
1
</p>
<p>⎞
⎠(&minus;1 1 1
</p>
<p>)
= 1
</p>
<p>3
</p>
<p>⎛
⎝
</p>
<p>1 &minus;1 &minus;1
&minus;1 1 1
&minus;1 1 1
</p>
<p>⎞
⎠ .
</p>
<p>Note that Pi projects onto the line along |ei〉. This can be tested by let-
ting Pi act on an arbitrary vector and showing that the resulting vector is
perpendicular to the other two vectors. For example, let P2 act on an arbi-
trary column vector:
</p>
<p>|a〉 &equiv; P2
</p>
<p>⎛
⎝
x
</p>
<p>y
</p>
<p>z
</p>
<p>⎞
⎠= 1
</p>
<p>6
</p>
<p>⎛
⎝
</p>
<p>1 &minus;1 2
&minus;1 1 &minus;2
2 &minus;2 4
</p>
<p>⎞
⎠
⎛
⎝
x
</p>
<p>y
</p>
<p>z
</p>
<p>⎞
⎠= 1
</p>
<p>6
</p>
<p>⎛
⎝
</p>
<p>x &minus; y + 2z
&minus;x + y &minus; 2z
2x &minus; 2y + 4z
</p>
<p>⎞
⎠ .</p>
<p/>
</div>
<div class="page"><p/>
<p>124 4 Operator Algebra
</p>
<p>We verify that |a〉 is perpendicular to both |e1〉 and |e3〉:
</p>
<p>〈e1|a〉 =
1&radic;
2
</p>
<p>(
1 1 0
</p>
<p>) 1
6
</p>
<p>⎛
⎝
</p>
<p>x &minus; y + 2z
&minus;x + y &minus; 2z
2x &minus; 2y + 4z
</p>
<p>⎞
⎠= 0.
</p>
<p>Similarly, 〈e3|a〉 = 0. So indeed, |a〉 is along |e2〉.
We can find the operator that projects onto the plane formed by |e1〉 and
</p>
<p>|e2〉. This is
</p>
<p>P1 + P2 =
1
</p>
<p>3
</p>
<p>⎛
⎝
</p>
<p>2 1 1
1 2 &minus;1
1 &minus;1 2
</p>
<p>⎞
⎠ .
</p>
<p>When this operator acts on an arbitrary column vector, it produces a vector
lying in the plane of |e1〉 and |e2〉, or perpendicular to |e3〉:
</p>
<p>|b〉 &equiv; (P1 + P2)
</p>
<p>⎛
⎝
x
</p>
<p>y
</p>
<p>z
</p>
<p>⎞
⎠= 1
</p>
<p>3
</p>
<p>⎛
⎝
</p>
<p>2 1 1
1 2 &minus;1
1 &minus;1 2
</p>
<p>⎞
⎠
⎛
⎝
x
</p>
<p>y
</p>
<p>z
</p>
<p>⎞
⎠= 1
</p>
<p>3
</p>
<p>⎛
⎝
</p>
<p>2x + y + z
x + 2y &minus; z
x &minus; y + 2z
</p>
<p>⎞
⎠ .
</p>
<p>It is easy to show that 〈e3|b〉 = 0. The operators that project onto the other
two planes are obtained similarly. Finally, we verify easily that
</p>
<p>P1 + P2 + P3 =
</p>
<p>⎛
⎝
</p>
<p>1 0 0
0 1 0
0 0 1
</p>
<p>⎞
⎠= 1,
</p>
<p>i.e., that completeness relation holds.
</p>
<p>Example 4.4.8 We want to find the most general projection and reflection
operators in a real two-dimensional vector space V. Without loss of gener-
ality, we assume that V=R2, and consider a vector
</p>
<p>|y〉 =
(
η1
η2
</p>
<p>)
.
</p>
<p>Then
</p>
<p>Py =
|y〉〈y|
〈y|y〉 =
</p>
<p>1
</p>
<p>η21 + η22
</p>
<p>(
η1
η2
</p>
<p>)(
η1 η2
</p>
<p>)
= 1
</p>
<p>η21 + η22
</p>
<p>(
η21 η1η2
η1η2 η
</p>
<p>2
2
</p>
<p>)
.
</p>
<p>Let
</p>
<p>η21
</p>
<p>η21 + η22
&equiv; cos2 α &rArr; η1&radic;
</p>
<p>η21 + η22
=&plusmn; cosα,
</p>
<p>and
</p>
<p>η22
</p>
<p>η21 + η22
&equiv; sin2 α &rArr; η2&radic;
</p>
<p>η21 + η22
=&plusmn; sinα.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.5 Representation of Algebras 125
</p>
<p>If the product η1η2 is negative, we can define a new angle which is the
negative of the old angle. This will change the sign of η1η2 and make it
positive without changing the signs of η21 and η
</p>
<p>2
2 . Thus the most general
</p>
<p>projection operator in R2 is
</p>
<p>Py =
(
</p>
<p>cos2 α sinα cosα
sinα cosα sin2 α
</p>
<p>)
. (4.16)
</p>
<p>Now that we have the projection operator along |y〉, we can construct the
reflection operator in a plane perpendicular to |y〉:
</p>
<p>Ry = 1&minus; 2Py =
(
</p>
<p>1 0
0 1
</p>
<p>)
&minus; 2
</p>
<p>(
cosα2 sinα cosα
</p>
<p>sinα cosα sinα2
</p>
<p>)
</p>
<p>=
(
</p>
<p>1 &minus; 2 cosα2 &minus;2 sinα cosα
&minus;2 sinα cosα 1 &minus; 2 sinα2
</p>
<p>)
=
(&minus; cos 2α &minus; sin 2α
&minus; sin 2α cos 2α
</p>
<p>)
.
</p>
<p>Defining φ =&minus;2α, we see that a general reflection is of the form
</p>
<p>Rφ =
(&minus; cosφ sinφ
</p>
<p>sinφ cosφ
</p>
<p>)
.
</p>
<p>It is interesting to note that
The product of two
</p>
<p>reflections in R2 is a
</p>
<p>rotation.Rφ1Rφ2 =
(
</p>
<p>cos(φ2 &minus; φ1) &minus; sin(φ2 &minus; φ1)
sin(φ2 &minus; φ1) cos(φ2 &minus; φ1)
</p>
<p>)
.
</p>
<p>This matrix describes a rotation of angle φ2 &minus; φ1 in R2 (see Problem 5.9 in
Chap. 5), which is clearly an isometry. We have just shown that the product
of two reflections is an isometry. It turns out that this is a general property
of isometries: they can always be expressed as a product of reflections (see
Theorem 26.5.17).
</p>
<p>4.5 Representation of Algebras
</p>
<p>The operator algebra, i.e., the algebra L(V) of the endomorphisms of a vec-
tor space, plays a significant role in physical applications as demonstrated so
far in this chapter. These (abstract) operators take on a concrete (numerical)
look once they are identified as matrices, the topic of Chap. 5. This sug-
gests making an identification of any given algebra with a (sub)algebra of
L(V), which subsequently could be identified as a collection of numbers&mdash;
what physicists are after&mdash;constituting the rows and columns of matrices.
The vague notion of &ldquo;identification&rdquo; is made precise by the concept of ho-
momorphism of algebras.
</p>
<p>For the following definition, it is convenient to introduce some notation.
Let both F and K denote either R or C with the condition that F &sube;K. So,
for instance, when F= R, then K can be either R and C; but when F= C,
then K can be only C. If V is a vector space over K, we denote the algebra</p>
<p/>
</div>
<div class="page"><p/>
<p>126 4 Operator Algebra
</p>
<p>of its endomorphisms by LK(V) or EndK(V). When there is no danger of
confusion, we remove the subscript K.
</p>
<p>Definition 4.5.1 Let A be an associative algebra over F with identity
1A. A K-representation of A in a vector space V over K is a homo-
morphism ρ :A&rarr; EndK(V) such that ρ(1A)= 1, where 1 is the unit
operator in EndK(V). The representation ρ is said to be faithful if it
is injective.
</p>
<p>Proposition 4.5.2 A nontrivial representation of a simple algebra is faith-
ful.
</p>
<p>representation of an
</p>
<p>algebra in a vector space
</p>
<p>Proof Since any representation is a homomorphism, the proof follows im-
mediately from Proposition 3.2.14. �
</p>
<p>Example 4.5.3 Let A = H, the (real) algebra of the quaternions, i.e., ele-
ments of the form
</p>
<p>a representation of the
</p>
<p>algebra of quaternions q = q0e0 + q1e1 + q2e2 + q3e3, qi &isin;R
</p>
<p>as given in Example 3.1.16. Let V= R4. Consider ρ :H&rarr; EndR(R4), the
real representation of quaternions given by
</p>
<p>Tq |x〉 &equiv; Tq
</p>
<p>⎛
⎜⎜⎝
</p>
<p>x1
x2
x3
x4
</p>
<p>⎞
⎟⎟⎠=
</p>
<p>⎛
⎜⎜⎝
</p>
<p>q0x1 &minus; q1x2 &minus; q2x3 &minus; q3x4
q1x1 + q0x2 &minus; q3x3 + q2x4
q2x1 + q3x2 + q0x3 &minus; q1x4
q3x1 &minus; q2x2 + q1x3 + q0x4
</p>
<p>⎞
⎟⎟⎠
</p>
<p>=
</p>
<p>⎛
⎜⎜⎝
</p>
<p>q0 &minus;q1 &minus;q2 &minus;q3
q1 q0 &minus;q3 q2
q2 q3 q0 &minus;q1
q3 &minus;q2 q1 q0
</p>
<p>⎞
⎟⎟⎠
</p>
<p>⎛
⎜⎜⎝
</p>
<p>x1
x2
x3
x4
</p>
<p>⎞
⎟⎟⎠
</p>
<p>where Tq &equiv; ρ(q) &isin; L(R4), and for convenience, we have written the ele-
ment of R4 as a column vector and introduced the matrix of q&rsquo;s. With this
matrix, we can simply write
</p>
<p>ρ(q)=
</p>
<p>⎛
⎜⎜⎝
</p>
<p>q0 &minus;q1 &minus;q2 &minus;q3
q1 q0 &minus;q3 q2
q2 q3 q0 &minus;q1
q3 &minus;q2 q1 q0
</p>
<p>⎞
⎟⎟⎠ . (4.17)
</p>
<p>Using this matrix, it is straightforward, but slightly tedious, to show directly
that ρ(qp)= ρ(q)ρ(p). Hence, ρ is indeed a representation of H. However,
instead, we calculate the matrices corresponding to the basis vectors of H.
Since q0 = 1 and q1 = q2 = q3 = 0 for e0, we get ρ(e0)= 1, as we should,</p>
<p/>
</div>
<div class="page"><p/>
<p>4.5 Representation of Algebras 127
</p>
<p>and as is evident from (4.17). Similarly, we can calculate the matrices of the
other basis vectors. The results are given below
</p>
<p>ρ(e0)=
</p>
<p>⎛
⎜⎜⎝
</p>
<p>1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1
</p>
<p>⎞
⎟⎟⎠ , ρ(e1)=
</p>
<p>⎛
⎜⎜⎝
</p>
<p>0 &minus;1 0 0
1 0 0 0
0 0 0 &minus;1
0 0 1 0
</p>
<p>⎞
⎟⎟⎠ ,
</p>
<p>ρ(e2)=
</p>
<p>⎛
⎜⎜⎝
</p>
<p>0 0 &minus;1 0
0 0 0 1
1 0 0 0
0 &minus;1 0 0
</p>
<p>⎞
⎟⎟⎠ , ρ(e3)=
</p>
<p>⎛
⎜⎜⎝
</p>
<p>0 0 0 &minus;1
0 0 &minus;1 0
0 1 0 0
1 0 0 0
</p>
<p>⎞
⎟⎟⎠ .
</p>
<p>It is now easier to check that ρ(eiej )= ρ(ei)ρ(ej ) for i, j = 0,1,2,3, and
hence, by Proposition 3.1.19, that ρ is indeed a representation.
</p>
<p>Definition 4.5.4 A subspace W of V is called stable or invariant under
stable, invariant, and
</p>
<p>irreducible subspaces
a representation ρ : A &rarr; EndC(V) &equiv; End(V) if ρ(a)|w〉 is in W for all
|w〉 &isin;W and all a &isin;A. A representation ρ is called irreducible if the only
stable subspaces are W= V and W= {|0〉V }.
</p>
<p>Problem 4.34 shows that if ρ is surjective, then it is irreducible.
</p>
<p>Proposition 4.5.5 Let ρ :A&rarr; End(V) be a representation and |v〉 an ar-
bitrary nonzero vector in V. Then
</p>
<p>W&equiv; ρ(A)|v〉 =
{
|w〉 &isin; V | |w〉 = ρ(a)|v〉 for some a &isin;A
</p>
<p>}
</p>
<p>is a stable subspace of V. In particular, ρ is irreducible if and only if
ρ(A)|v〉 = V.
</p>
<p>Proof The straightforward but instructive proof is the content of Prob-
lem 4.37. �
</p>
<p>Isomorphic vector spaces are indistinguishable. So can be their corre-
sponding representations. More precisely,
</p>
<p>equivalent
</p>
<p>representations
</p>
<p>Definition 4.5.6 Suppose T : V1 &sim;= V2 is an isomorphism. Two rep-
resentations ρ1 : A &rarr; End(V1) and ρ2 : A &rarr; End(V2) are called
equivalent if
</p>
<p>T ◦ ρ1(a)= ρ2(a) ◦ T for all a &isin;A.
</p>
<p>We write ρ1 &sim; ρ2 to indicate the equivalence of the two representa-
tions.
</p>
<p>Sometimes the condition of equivalence is written as
</p>
<p>ρ1(a)= T&minus;1 ◦ ρ2(a) ◦ T or ρ2(a)= T ◦ ρ1(a) ◦ T&minus;1. (4.18)</p>
<p/>
</div>
<div class="page"><p/>
<p>128 4 Operator Algebra
</p>
<p>Just as we can combine two vector spaces to get a new vector space, we
can combine two representations to obtain a new representation.
</p>
<p>Definition 4.5.7 Let ρ and η be representations of A in U and V, respec-
tively. We define representations ρ &oplus; η, called the direct sum, and ρ &otimes; η,
called the tensor product, of ρ and η, respectively in U&oplus;V and U&otimes;V by
</p>
<p>(ρ &oplus; η)(a)= ρ(a)&oplus; η(a) a &isin;A
(ρ &otimes; η)(a)= ρ(a)&otimes; η(a) a &isin;A.
</p>
<p>It should be obvious that if ρ1 &sim; ρ2 and η1 &sim; η2, then ρ1 &oplus; η1 &sim; ρ2 &oplus; η2
and ρ1 &otimes; η1 &sim; ρ2 &otimes; η2.
</p>
<p>Since an algebra A is also a vector space, it is possible to come up withusing |a〉 and a in certain
representations of an
</p>
<p>algebra
</p>
<p>representations of the form ρ : A &rarr; End(A). When there is no danger of
confusion, we designate members of A as a ket when they are considered
simply as vectors, but use bold face type when the same member participates
in an algebra multiplication. Thus |a〉 &isin; A when the member is considered
as a vector and a &isin; A when the same member is one of the factors in a
product.
</p>
<p>regular representation of
</p>
<p>an algebra
Definition 4.5.8 The regular representation of A in A is the repre-
sentation ρL :A&rarr; End(A) given by ρL(a)|b〉 = ab.
</p>
<p>It is trivial to show that this is indeed a representation, i.e., that ρL(ab)=
ρL(a)ρL(b).
</p>
<p>If A is unital, then ρL(a)= ρL(a&prime;) implies that ρL(a)|1〉 = ρL(a&prime;)|1〉 or
that a1 = a&prime;1, namely that a = a&prime;, indicating that ρL is injective, and the
representation faithful, or ρL(A)&sim;=A.
</p>
<p>ρL is simply the left-multiplication of A. What about the right-multipli-
cation? If we set ρR(a)|b〉 = ba, then
</p>
<p>ρR(ab)|c〉 = cab= (ca)b=
(
ρR(a)|c〉
</p>
<p>)
b= ρR(b)
</p>
<p>(
ρR(a)|c〉
</p>
<p>)
</p>
<p>=
(
ρR(b)ρR(a)
</p>
<p>)
|c〉.
</p>
<p>Hence, ρR(ab) = ρR(b)ρR(a). Again if A is unital, then ρR is faithful
and ρR(A) &sim;= Aop, where Aop is the algebra opposite to A given in Defi-
nition 3.1.8.
</p>
<p>Theorem 4.5.9 Let L be a minimal left ideal of an algebra A. Then the
representation ρ(L) : A &rarr; End(L), the regular representation of A in L,
given by
</p>
<p>ρ(L)(a)|x〉 &equiv; ρL(a)|x〉 = ax for a &isin;A and |x〉 &equiv; x &isin;L,
</p>
<p>is irreducible.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.5 Representation of Algebras 129
</p>
<p>Proof We note that ρ(L)(A)|x〉 = Ax. Since L is minimal, Ax = L by
Theorem 3.2.6, and ρ(L)(A)|x〉 = L. By Proposition 4.5.5, ρ(L) is irre-
ducible. �
</p>
<p>Theorem 4.5.10 All irreducible representations of a simple algebra
A are faithful and equivalent to ρ(L), the regular representation of A
in the minimal left ideal L.
</p>
<p>Proof The faithfulness is a consequence of Proposition 4.5.2. Let ρ :A&rarr;
End(V) be an irreducible representation. For x &isin;L and a vector |e〉 &isin; V, let
ρ(x)|e〉 = |v〉. Then
</p>
<p>ρ(L)|e〉 = ρ(Ax)|e〉 = ρ(A)ρ(x)|e〉 = ρ(A)|v〉 = V. (4.19)
</p>
<p>The first equality follows from Theorem 3.2.6, and the last equality from
Proposition 4.5.5.
</p>
<p>Now consider a linear map T :L&rarr; V given by T(y)= ρ(y)|e〉, and note
that by Eq. (4.19),
</p>
<p>T(L)= ρ(L)|e〉 = V.
Therefore, T is surjective. Now let z be a nonzero member of L. If z &isin; kerT,
then by Theorem 3.2.6, L=Az and
</p>
<p>T(L)= ρ(L)|e〉 = ρ(Az)|e〉 = ρ(A)ρ(z)|e〉 = ρ(A)T(z)= {0}
</p>
<p>which contradicts the previous equation. Therefore, kerT = {0} and T is
injective, hence, bijective.
</p>
<p>To complete the proof, we have to show that
</p>
<p>T ◦ ρ(L)(a)= ρ(a) ◦ T for all a &isin;A.
</p>
<p>If y &isin;L, then the right-hand side gives
</p>
<p>ρ(a) ◦ T(y)= ρ(a)ρ(y)|e〉 = ρ(ay)|e〉 = T(ay),
</p>
<p>while the left-hand side yields
</p>
<p>(
T ◦ ρ(L)(a)
</p>
<p>)
y= T
</p>
<p>(
ρ(L)(a)y
</p>
<p>)
= T(ay).
</p>
<p>This completes the proof. �
</p>
<p>A consequence of this theorem is
</p>
<p>Corollary 4.5.11 All minimal left ideals of a simple algebra are isomor-
phic.
</p>
<p>Proof If L&prime; is another left ideal of A, then let V = L&prime; in Theorem 4.5.10.
Then T of the theorem establishes an isomorphism between L and L&prime;. �</p>
<p/>
</div>
<div class="page"><p/>
<p>130 4 Operator Algebra
</p>
<p>Theorem 4.5.12 Two irreducible representations of a semi-simple algebra
are equivalent if and only if they have the same kernel.
</p>
<p>Proof Recall from Theorem 3.5.25 that a semi-simple algebra A is the di-
rect sum of simple algebras, each component being an ideal of A. Let
</p>
<p>A= I1 &oplus; I2 &oplus; &middot; &middot; &middot; &oplus; Ir =
r&oplus;
</p>
<p>i=1
Ii
</p>
<p>and ρ :A&rarr; End(V) be an irreducible representation. Assume there is 0 �=
xp &isin; Ip for some p and |e〉 &isin; V such that |v〉 &equiv; ρ(xp)|e〉 �= |0〉. Then since
ρ is irreducible, by Proposition 4.5.5
</p>
<p>V= ρ(A)|v〉 = ρ(A)ρ(xp)|e〉 = ρ(Axp)|e〉 &sube; ρ(Ip)|e〉.
</p>
<p>But obviously, ρ(Ip)|e〉 &sube; V. Hence,
</p>
<p>ρ(Ip)|e〉 = V, (4.20)
</p>
<p>which also indicates that any |x〉 &isin; V can be written as |x〉 = ρ(yp)|e〉 for
some yp &isin; Ip . Now since, IpIk = IkIp = {0} for k �= p, we have
</p>
<p>ρ(zk)|x〉 = ρ(zk)ρ(yp)|e〉 = ρ(zkyp)|e〉 = ρ(0)|e〉 = |0〉
</p>
<p>for all |x〉 &isin; V. It follows that ρ(zk) is the zero operator, i.e., zk &isin; kerρ for
all k �= p, or
</p>
<p>r&oplus;
</p>
<p>i=1
i �=p
</p>
<p>Ii &sube; kerρ.
</p>
<p>Now let ρ|Ip : Ip &rarr; End(V) be the restriction of ρ to Ip , i.e., a represen-
tation of Ip in V. Then T : Ip &rarr; V given by T(zp) = ρ(zp)|e〉 is an iso-
morphism by the proof of Theorem 4.5.10. Hence, ρ(zp) = 0 implies that
zp = 0, i.e., that Ip &cap; kerρ = {0}. This yields
</p>
<p>r&oplus;
</p>
<p>i=1
i �=p
</p>
<p>Ii = kerρ.
</p>
<p>Let ρ1 :A&rarr; End(V1) and ρ2 :A&rarr; End(V2) be two irreducible repre-
sentations of a semi-simple algebra A. Assume further that ρ1 and ρ2 have
the same kernel; i.e. that for some 1 &le; p &le; r ,
</p>
<p>kerρ1 = kerρ2 =
r&oplus;
</p>
<p>i=1
i �=p
</p>
<p>Ii .
</p>
<p>Then as shown above there are isomorphisms T1 : Ip &rarr; V1 and T2 : Ip &rarr;
V2 given by
</p>
<p>T1(zp)= ρ1(zp)|e1〉, and T2(zp)= ρ2(zp)|e2〉</p>
<p/>
</div>
<div class="page"><p/>
<p>4.6 Problems 131
</p>
<p>with
</p>
<p>ρ1(Ip)|e1〉 = V1 and ρ2(Ip)|e2〉 = V2 (4.21)
as in Eq. (4.20). The composite map S&equiv; T2 ◦ T&minus;11 maps V1 isomorphically
onto V2. We now show that
</p>
<p>S ◦ ρ1(a)= ρ2(a) ◦ S for all a &isin;A,
</p>
<p>and hence that ρ1 &sim; ρ2. Applying the right-hand side of this equation on a
|v1〉 &isin; V1, and noting that by (4.21) |v1〉 = ρ1(zp)|e1〉 for some zp &isin; Ip , we
get
</p>
<p>ρ2(a) ◦ S|v1〉 = ρ2(a) ◦
(
T2 ◦ T&minus;11
</p>
<p>)
ρ1(zp)|e1〉 = ρ2(a) ◦ T2
</p>
<p>(
T&minus;11 ρ1(zp)|e1〉
</p>
<p>)
</p>
<p>=
(
ρ2(a)
</p>
<p>)
◦ T2(zp)= ρ2(a)ρ2(zp)|e2〉 = ρ2(azp)|e2〉,
</p>
<p>while the left-hand side gives
</p>
<p>S ◦ ρ1(a)|v1〉 =
(
T2 ◦ T&minus;11
</p>
<p>)
ρ1(a)ρ1(zp)|e1〉 =
</p>
<p>(
T2 ◦ T&minus;11
</p>
<p>)
ρ1(azp)|e1〉
</p>
<p>= T2
(
T&minus;11 ρ1(azp)|e1〉
</p>
<p>)
= T2(azp)= ρ2(azp)|e2〉.
</p>
<p>We have shown that if two irreducible representations of a semi-simple al-
gebra have the same kernel, then they are equivalent. The converse is much
easier to prove (see Problem 4.36). �
</p>
<p>4.6 Problems
</p>
<p>4.1 Consider a linear operator T on a finite-dimensional vector space V.
</p>
<p>(a) Show that there exists a polynomial p such that p(T)= 0. Hint: Take
a basis B = {|ai〉}Ni=1 and consider the vectors {Tk|a1〉}Mk=0 for large
enough M and conclude that there exists a polynomial p1(T) such
that p1(T)|a1〉 = 0. Do the same for |a2〉, etc. Now take the product of
all such polynomials.
</p>
<p>(b) From (a) conclude that for large enough n, Tn can be written as a linear
combination of smaller powers of T.
</p>
<p>(c) Now conclude that any infinite series in T collapses to a polynomial
in T.
</p>
<p>4.2 Use mathematical induction to show that [A,Am] = 0.
</p>
<p>4.3 For D and T defined in Example 2.3.5:
</p>
<p>(a) Show that [D,T] = 1.
(b) Calculate the linear transformations D3T3 and T3D3.
</p>
<p>4.4 Consider three linear operators L1,L2, and L3 satisfying the commuta-
tion relations [L1,L2] = iL3, [L3,L1] = iL2, [L2,L3] = iL1, and define the
new operators L&plusmn; = L1 &plusmn; iL2.</p>
<p/>
</div>
<div class="page"><p/>
<p>132 4 Operator Algebra
</p>
<p>(a) Show that the operator L2 &equiv; L21 + L22 + L23 commutes with Lk , k =
1,2,3.
</p>
<p>(b) Show that the set {L+,L&minus;,L3} is closed under commutation, i.e., the
commutator of any two of them can be written as a linear combination
of the set. Determine these commutators.
</p>
<p>(c) Write L2 in terms of L+,L&minus;, and L3.
</p>
<p>4.5 Prove the rest of Proposition 4.1.8.
</p>
<p>4.6 Show that if [[A,B],A] = 0, then for every positive integer k,
[
Ak,B
</p>
<p>]
= kAk&minus;1[A,B].
</p>
<p>Hint: First prove the relation for low values of k; then use mathematical
induction.
</p>
<p>4.7 Show that for D and T defined in Example 2.3.5,
[
Dk,T
</p>
<p>]
= kDk&minus;1 and
</p>
<p>[
Tk,D
</p>
<p>]
=&minus;kTk&minus;1.
</p>
<p>4.8 Evaluate the derivative of H&minus;1(t) in terms of the derivative of H(t) by
differentiating their product.
</p>
<p>4.9 Show that for any α,β &isin;R and any H &isin; End(V), we have
</p>
<p>eαHeβH = e(α+β)H.
</p>
<p>4.10 Show that (U+ T)(U&minus; T)= U2 &minus; T2 if and only if [U,T] = 0.
</p>
<p>4.11 Prove that if A and B are hermitian, then i[A,B] is also hermitian.
</p>
<p>4.12 Find the solution to the operator differential equation
</p>
<p>dU
</p>
<p>dt
= tHU(t).
</p>
<p>Hint: Make the change of variable y = t2 and use the result of Exam-
ple 4.2.3.
</p>
<p>4.13 Verify that
</p>
<p>d
</p>
<p>dt
H3 =
</p>
<p>(
dH
</p>
<p>dt
</p>
<p>)
H2 +H
</p>
<p>(
dH
</p>
<p>dt
</p>
<p>)
H+H2
</p>
<p>(
dH
</p>
<p>dt
</p>
<p>)
.
</p>
<p>4.14 Show that if A and B commute, and f and g are arbitrary functions,
then f (A) and g(B) also commute.
</p>
<p>4.15 Assuming that [[S,T],T] = 0= [[S,T],S], show that
[
S, exp(tT)
</p>
<p>]
= t[S,T] exp(tT).
</p>
<p>Hint: Expand the exponential and use Problem 4.6.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.6 Problems 133
</p>
<p>4.16 Prove that
</p>
<p>exp(H1 +H2 +H3)= exp(H1) exp(H2) exp(H3)
</p>
<p>&times; exp
{
&minus;1
</p>
<p>2
</p>
<p>(
[H1,H2] + [H1,H3] + [H2,H3]
</p>
<p>)}
</p>
<p>provided that H1,H2, and H3 commute with all the commutators. What is
the generalization to H1 +H2 + &middot; &middot; &middot; +Hn?
</p>
<p>4.17 Denoting the derivative of A(t) by Ȧ, show that
</p>
<p>d
</p>
<p>dt
[A,B] = [Ȧ,B] + [A, Ḃ].
</p>
<p>4.18 Prove Theorem 4.3.2. Hint: Use Eq. (4.11) and Theorem 2.3.7.
</p>
<p>4.19 Let A(t)&equiv; exp(tH)A0 exp(&minus;tH), where H and A0 are constant opera-
tors. Show that dA/dt = [H,A(t)]. What happens when H commutes with
A(t)?
</p>
<p>4.20 Let |f 〉, |g〉 &isin;C(a, b) with the additional property that
</p>
<p>f (a)= g(a)= f (b)= g(b)= 0.
</p>
<p>Show that for such functions, the derivative operator D is anti-hermitian.
The inner product is defined as usual:
</p>
<p>〈f |g〉 &equiv;
&int; b
</p>
<p>a
</p>
<p>f &lowast;(t)g(t) dt.
</p>
<p>4.21 In this problem, you will go through the steps of proving the rigorous
statement of the Heisenberg uncertainty principle. Denote the expectation
(average) value of an operator A in a state |Ψ 〉 by Aavg. Thus, Aavg = 〈A〉 =
〈Ψ |A|Ψ 〉. The uncertainty (deviation from the mean) in the normalized state
|Ψ 〉 of the operator A is given by
</p>
<p>�A=
&radic;&lang;
</p>
<p>(A&minus;Aavg)2
&rang;
=
&radic;
〈Ψ |(A&minus;Aavg1)2|Ψ 〉.
</p>
<p>(a) Show that for any two hermitian operators A and B, we have
</p>
<p>∣∣〈Ψ |AB|Ψ 〉
∣∣2 &le; 〈Ψ |A2|Ψ 〉〈Ψ |B2|Ψ 〉.
</p>
<p>Hint: Apply the Schwarz inequality to an appropriate pair of vectors.
(b) Using the above and the triangle inequality for complex numbers,
</p>
<p>show that
∣∣〈Ψ |[A,B]|Ψ 〉
</p>
<p>∣∣2 &le; 4〈Ψ |A2|Ψ 〉〈Ψ |B2|Ψ 〉.
</p>
<p>(c) Define the operators A&prime; = A&minus; α1, B&prime; = B&minus; β1, where α and β are
real numbers. Show that A&prime; and B&prime; are hermitian and [A&prime;,B&prime;] = [A,B].</p>
<p/>
</div>
<div class="page"><p/>
<p>134 4 Operator Algebra
</p>
<p>(d) Now use all the results above to show the celebrated uncertainty rela-
tion
</p>
<p>(�A)(�B)&ge; 1
2
</p>
<p>∣∣〈Ψ |[A,B]|Ψ 〉
∣∣.
</p>
<p>What does this reduce to for position operator x and momentum oper-
</p>
<p>Heisenberg uncertainty
</p>
<p>principle
</p>
<p>ator p if [x,p] = i�?
</p>
<p>4.22 Show that U= expA is unitary if A is anti-hermitian. Furthermore, if
A commutes with A&dagger;, then expA is unitary. Hint: Use Proposition 4.2.4 on
UU&dagger; = 1 and U&dagger;U= 1
</p>
<p>4.23 Find T&dagger; for each of the following linear operators.
</p>
<p>(a) T :R2 &rarr;R2 given by
</p>
<p>T
</p>
<p>(
x
</p>
<p>y
</p>
<p>)
=
(
x + y
x &minus; y
</p>
<p>)
.
</p>
<p>(b) T :R3 &rarr;R3 given by
</p>
<p>T
</p>
<p>⎛
⎝
x
</p>
<p>y
</p>
<p>z
</p>
<p>⎞
⎠=
</p>
<p>⎛
⎝
</p>
<p>x + 2y &minus; z
3x &minus; y + 2z
&minus;x + 2y + 3z
</p>
<p>⎞
⎠ .
</p>
<p>(c) T :R2 &rarr;R2 given by
</p>
<p>T
</p>
<p>(
x
</p>
<p>y
</p>
<p>)
=
(
x cos θ &minus; y sin θ
x sin θ + y cos θ
</p>
<p>)
,
</p>
<p>where θ is a real number. What is T&dagger;T?
(d) T :C2 &rarr;C2 given by
</p>
<p>T
</p>
<p>(
α1
α2
</p>
<p>)
=
(
α1 &minus; iα2
iα1 + α2
</p>
<p>)
.
</p>
<p>(e) T :C3 &rarr;C3 given by
</p>
<p>T
</p>
<p>⎛
⎝
α1
α2
α3
</p>
<p>⎞
⎠=
</p>
<p>⎛
⎝
</p>
<p>α1 + iα2 &minus; 2iα3
&minus;2iα1 + α2 + iα3
iα1 &minus; 2iα2 + α3
</p>
<p>⎞
⎠ .
</p>
<p>4.24 Show that if P is a (hermitian) projection operator, so are 1&minus; P and
U&dagger;PU for any unitary operator U.
</p>
<p>4.25 For the vector
</p>
<p>|a〉 = 1&radic;
2
</p>
<p>⎛
⎜⎜⎝
</p>
<p>0
1
&minus;1
0
</p>
<p>⎞
⎟⎟⎠ ,</p>
<p/>
</div>
<div class="page"><p/>
<p>4.6 Problems 135
</p>
<p>(a) find the associated projection matrix, Pa .
(b) Verify that Pa does project an arbitrary vector in C4 along |a〉.
(c) Verify directly that the matrix 1&minus; Pa is also a projection operator.
</p>
<p>4.26 Prove Proposition 4.4.6
</p>
<p>4.27 Let |a1〉 &equiv; a1 = (1,1,&minus;1) and |a2〉 &equiv; a2 = (&minus;2,1,&minus;1).
(a) Construct (in the form of a matrix) the projection operators P1 and P2
</p>
<p>that project onto the directions of |a1〉 and |a2〉, respectively. Verify
that they are indeed projection operators.
</p>
<p>(b) Construct (in the form of a matrix) the operator P= P1+P2 and verify
directly that it is a projection operator.
</p>
<p>(c) Let P act on an arbitrary vector (x, y, z). What is the dot product of
the resulting vector with the vector a1 &times; a2? What can you say about
P and your conclusion in (b)?
</p>
<p>4.28 Let P(m) = &sum;mi=1 |ei〉〈ei | be a projection operator constructed out of
the first m orthonormal vectors of the basis B = {|ei〉}Ni=1 of V. Show that
P(m) projects into the subspace spanned by the first m vectors in B .
</p>
<p>4.29 What is the length of the projection of the vector (3,4,&minus;4) onto a line
whose parametric equation is x = 2t + 1, y =&minus;t + 3, z= t &minus; 1? Hint: Find
a unit vector in the direction of the line and construct its projection operator.
</p>
<p>4.30 The parametric equation of a line L in a coordinate system with origin
O is
</p>
<p>x = 2t + 1, y = t + 1, z=&minus;2t + 2.
A point P has coordinates (3,&minus;2,1).
(a) Using the projection operators, find the length of the projection of OP
</p>
<p>on the line L.
(b) Find the vector whose beginning is P and ends perpendicularly on L.
(c) From this vector calculate the distance from P to L.
</p>
<p>4.31 Let the operator U :C2 &rarr;C2 be given by
</p>
<p>U
</p>
<p>(
α1
α2
</p>
<p>)
=
</p>
<p>⎛
⎝
i
α1&radic;
</p>
<p>2
&minus; i α2&radic;
</p>
<p>2
α1&radic;
</p>
<p>2
+ α2&radic;
</p>
<p>2
</p>
<p>⎞
⎠ .
</p>
<p>Find U&dagger; and test if U is unitary.
</p>
<p>4.32 Show that the product of two unitary operators is always unitary, but
the product of two hermitian operators is hermitian if and only if they com-
mute.
</p>
<p>4.33 Let S be an operator that is both unitary and hermitian. Show that
</p>
<p>(a) S is involutive (i.e., S2 = 1), and</p>
<p/>
</div>
<div class="page"><p/>
<p>136 4 Operator Algebra
</p>
<p>(b) S= P+ &minus; P&minus;, where P+ and P&minus; are hermitian projection operators.
</p>
<p>4.34 Show that if a representation ρ : A &rarr; L(V) is surjective, then it is
irreducible. Hint: The operator |a〉〈a| is in L(V) for any |a〉 &isin; V.
</p>
<p>4.35 Show that ρ(eiej )= ρ(ei)ρ(ej ) for i, j = 0,1,2,3 in Example 4.5.3.
</p>
<p>4.36 Show that any two equivalent representations of any algebra have the
same kernel.
</p>
<p>4.37 To prove Proposition 4.5.5, first show that ρ(A)|v〉 is a subspace. Then
prove that ρ(A)W&sub;W. For the &ldquo;only if&rdquo; part of an irreducible representa-
tion, take |v〉 to be in any subspace of V.</p>
<p/>
</div>
<div class="page"><p/>
<p>5Matrices
</p>
<p>So far, our theoretical investigation has been dealing mostly with abstract
vectors and abstract operators. As we have seen in examples and problems,
concrete representations of vectors and operators are necessary in most ap-
plications. Such representations are obtained by choosing a basis and ex-
pressing all operations in terms of components of vectors and matrix repre-
sentations of operators.
</p>
<p>5.1 Representing Vectors and Operators
</p>
<p>Let us choose a basis BV = {|ai〉}Ni=1 of a vector space VN , and express an
arbitrary vector |x〉 in this basis: |x〉 =&sum;Ni=1 ξi |ai〉. We write
</p>
<p>x =
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>ξ1
ξ2
...
</p>
<p>ξN
</p>
<p>⎞
⎟⎟⎟⎠ (5.1)
</p>
<p>and say that the column vector x represents |x〉 in BV . We can also have a
representation of vectors
</p>
<p>linear transformation A &isin;L(VN ,WM) act on the basis vectors in BV to give
vectors in the M-dimensional vector space WM : |wk〉 = A|ak〉. The latter
can be written as a linear combination of basis vectors BW = {|bj 〉}Mj=1 in
WM :
</p>
<p>|w1〉 =
M&sum;
</p>
<p>j=1
αj1|bj 〉, |w2〉 =
</p>
<p>M&sum;
</p>
<p>j=1
αj2|bj 〉, . . . , |wN 〉 =
</p>
<p>M&sum;
</p>
<p>j=1
αjN |bj 〉.
</p>
<p>Note that the components have an extra subscript to denote which of the N
vectors {|wi〉}Ni=1 they are representing. The components can be arranged in
a column as before to give a representation of the corresponding vectors:
</p>
<p>w1 =
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>α11
α21
...
</p>
<p>αM1
</p>
<p>⎞
⎟⎟⎟⎠ , w2 =
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>α12
α22
...
</p>
<p>αM2
</p>
<p>⎞
⎟⎟⎟⎠ , . . . , wN =
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>α1N
α2N
...
</p>
<p>αMN
</p>
<p>⎞
⎟⎟⎟⎠ .
</p>
<p>S. Hassani, Mathematical Physics, DOI 10.1007/978-3-319-01195-0_5,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>137</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-01195-0_5">http://dx.doi.org/10.1007/978-3-319-01195-0_5</a></div>
</div>
<div class="page"><p/>
<p>138 5 Matrices
</p>
<p>The operator itself is determined by the collection of all these vectors, i.e.,
by a matrix. We write this as
</p>
<p>A =
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>α11 α12 . . . α1N
α21 α22 . . . α2N
...
</p>
<p>...
...
</p>
<p>αM1 αM2 . . . αMN
</p>
<p>⎞
⎟⎟⎟⎠ (5.2)
</p>
<p>and call A the matrix representing A in bases BV and BW . This statement
representation of
</p>
<p>operators
is also summarized symbolically as
</p>
<p>A|ai〉 =
M&sum;
</p>
<p>j=1
αji |bj 〉, i = 1,2, . . . ,N. (5.3)
</p>
<p>We thus have the following rule:
</p>
<p>Box 5.1.1 To find the matrix A representing A in bases BV =
{|ai〉}Ni=1 and BW = {|bj 〉}Mj=1, express A|ai〉 as a linear combination
of the vectors in BW . The components form the ith column of A.
</p>
<p>Now consider the vector |y〉 = A|x〉 in WM . This vector can be written
in two ways: On the one hand, |y〉 =&sum;Mj=1 ηj |bj 〉. On the other hand,
</p>
<p>|y〉 = A|x〉 = A
N&sum;
</p>
<p>i=1
ξi |ai〉 =
</p>
<p>N&sum;
</p>
<p>i=1
ξiA|ai〉
</p>
<p>=
N&sum;
</p>
<p>i=1
ξi
</p>
<p>(
M&sum;
</p>
<p>j=1
αji |bj 〉
</p>
<p>)
=
</p>
<p>M&sum;
</p>
<p>j=1
</p>
<p>(
N&sum;
</p>
<p>i=1
ξiαji
</p>
<p>)
|bj 〉.
</p>
<p>Since |y〉 has a unique set of components in the basis BW , we conclude that
</p>
<p>ηj =
N&sum;
</p>
<p>i=1
αjiξi, j = 1,2, . . . ,M. (5.4)
</p>
<p>This is written as
⎛
⎜⎜⎜⎝
</p>
<p>η1
η2
...
</p>
<p>ηM
</p>
<p>⎞
⎟⎟⎟⎠=
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>α11 α12 . . . α1N
α21 α22 . . . α2N
...
</p>
<p>...
...
</p>
<p>αM1 αM2 . . . αMN
</p>
<p>⎞
⎟⎟⎟⎠
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>ξ1
ξ2
...
</p>
<p>ξN
</p>
<p>⎞
⎟⎟⎟⎠ &rArr; y = Ax, (5.5)
</p>
<p>in which the usual matrix multiplication rule is understood. This matrix
equation is the representation of the operator equation |y〉 = A|x〉 in the
bases BV and BW .
</p>
<p>The construction above indicates that&mdash;once the bases are fixed in the
two vector spaces&mdash;to every operator there corresponds a unique matrix.
This uniqueness is the result of the uniqueness of the components of vectors</p>
<p/>
</div>
<div class="page"><p/>
<p>5.1 Representing Vectors and Operators 139
</p>
<p>in a basis. On the other hand, given an M &times;N matrix A with elements αij ,
one can construct a unique linear operator TA defined by its action on the ba-
sis vectors (see Box 2.3.6): TA|ai〉 &equiv;
</p>
<p>&sum;M
j=1 αji |bj 〉. Thus, there is a one-to-
</p>
<p>one correspondence between operators and matrices. This correspondence
is in fact a linear isomorphism:
</p>
<p>The operator TA
associated with a
</p>
<p>matrix A
</p>
<p>Proposition 5.1.2 The two vector spaces L(VN ,WM) andMM&times;N are iso-
morphic. An explicit isomorphism is established only when a basis is chosen
for each vector space, in which case, an operator is identified with its matrix
representation.
</p>
<p>Example 5.1.3 In this example, we construct a matrix representation of the matrix representation of
the complex structure Jcomplex structure J on a real vector space V introduced in Sect. 2.4. There
</p>
<p>are two common representations, each corresponding to a different ordering
of the vectors in the basis {|ei〉, J|ei〉}mi=1 of V. One ordering is to let J|ei〉
come right after |ei〉. The other is to collect all the J|ei〉 after the |ei〉 in the
same order. We consider the first ordering in this example, and leave the
other for the reader to construct.
</p>
<p>In the first ordering, for each |ei〉, we let |ei+1〉 = J|ei〉. Starting with
|e1〉, we have
</p>
<p>J|e1〉 = |e2〉 = 0 &middot; |e1〉 + 1 &middot; |e2〉 + 0 &middot; |e3〉 + &middot; &middot; &middot; + 0 &middot; |e2m〉,
</p>
<p>J|e2〉 = J2|e1〉 = &minus;|e1〉 = &minus;1 &middot; |e1〉 + 0 &middot; |e2〉 + 0 &middot; |e3〉 + &middot; &middot; &middot; + 0 &middot; |e2m〉.
</p>
<p>These two equations give the first two columns as
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>0 &minus;1
1 0
0 0
...
</p>
<p>...
</p>
<p>0 0
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠
.
</p>
<p>For the third and fourth basis vectors, we get
</p>
<p>J|e3〉 = |e4〉 = 0 &middot; |e1〉 + 0 &middot; |e2〉 + 0 &middot; |e3〉 + 1 &middot; |e4〉 + &middot; &middot; &middot; + 0 &middot; |e2m〉
</p>
<p>J|e4〉 = J2|e3〉 = &minus;|e3〉 = 0 &middot; |e1〉 + 0 &middot; |e2〉 &minus; 1 &middot; |e3〉 + &middot; &middot; &middot; + 0 &middot; |e2m〉,
</p>
<p>giving rise to the following third and fourth columns:
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>0 0
0 0
0 &minus;1
1 0
...
</p>
<p>...
</p>
<p>0 0
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎠
.</p>
<p/>
</div>
<div class="page"><p/>
<p>140 5 Matrices
</p>
<p>It should now be clear that the matrix representation of J is of the form
</p>
<p>J =
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>R1 0 . . . 0
0 R2 . . . 0
...
</p>
<p>...
...
</p>
<p>0 0 . . . Rm
</p>
<p>⎞
⎟⎟⎟⎠ ,
</p>
<p>where the zeros are the 2 &times; 2 zero matrices and Rk =
( 0 &minus;1
</p>
<p>1 0
</p>
<p>)
for all k.
</p>
<p>Notation 5.1.4 Let A &isin; L(VN ,WM). Choose a bases BV for V and BW
for W. We denote the matrix representing A in these bases by MBWBV (A),
where
</p>
<p>M
BW
BV
</p>
<p>:L(VN ,WM)&rarr;MM&times;N
</p>
<p>is the basis-dependent linear isomorphism. When V=W, we leave out the
subscripts and superscripts of M, keeping in mind that all matrices are rep-
resentations in a single basis.
</p>
<p>Given the linear transformations A : VN &rarr;WM and B :WM &rarr;UK , we
can form the composite linear transformation B◦A : VN &rarr;UK . We can also
choose bases BV = {|ai〉}Ni=1,BW = {|bi〉}Mi=1,BU = {|ci〉}Ki=1 for V, W, and
U, respectively. Then A,B, and B ◦ A will be represented by an M &times; N , a
K &times;M , and a K &times;N matrix, respectively, and we have
</p>
<p>M
BU
BV
</p>
<p>(B ◦A)=MBUBW (B)M
BW
BV
</p>
<p>(A), (5.6)
</p>
<p>where on the right-hand side the product is defined as the usual product of
matrices. If V=W=U, we write (5.6) as
</p>
<p>M(B ◦A)=M(B)M(A) (5.7)
</p>
<p>Matrices are determined entirely by their elements. For this reason a ma-
trix A whose elements are α11, α12, . . . is sometimes denoted by (αij ). Sim-
ilarly, the elements of this matrix are denoted by (A)ij . So, on the one hand,
we have (αij )= A, and on the other hand (A)ij = αij . In the context of this
notation, therefore, we can write
</p>
<p>(A + B)ij = (A)ij + (B)ij &rArr; (αij + βij )= (αij )+ (βij ),
(γA)ij = γ (A)ij &rArr; γ (αij )= (γ αij ),
</p>
<p>(0)ij = 0,
(1)ij = δij .
</p>
<p>A matrix, as a representation of a linear operator, is well-defined only in
reference to a specific basis. A collection of rows and columns of numbers
by themselves have no operational meaning. When we manipulate matri-
ces and attach meaning to them, we make an unannounced assumption re-
garding the basis: We have the standard basis of Cn (or Rn) in mind. The
following example should clarify this subtlety.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.1 Representing Vectors and Operators 141
</p>
<p>Example 5.1.5 Let us find the matrix representation of the linear operator
A &isin;L(R3), given by
</p>
<p>A
</p>
<p>⎛
⎝
x
</p>
<p>y
</p>
<p>z
</p>
<p>⎞
⎠=
</p>
<p>⎛
⎝
x &minus; y + 2z
</p>
<p>3x &minus; z
2y + z
</p>
<p>⎞
⎠ (5.8)
</p>
<p>in the basis
</p>
<p>B =
</p>
<p>⎧
⎨
⎩|a1〉 =
</p>
<p>⎛
⎝
</p>
<p>1
1
0
</p>
<p>⎞
⎠ , |a2〉 =
</p>
<p>⎛
⎝
</p>
<p>1
0
1
</p>
<p>⎞
⎠ , |a3〉 =
</p>
<p>⎛
⎝
</p>
<p>0
1
1
</p>
<p>⎞
⎠
⎫
⎬
⎭ .
</p>
<p>There is a tendency to associate the matrix
</p>
<p>⎛
⎝
</p>
<p>1 &minus;1 2
3 0 &minus;1
0 2 1
</p>
<p>⎞
⎠
</p>
<p>with the operator A. The following discussion will show that this is false.
To obtain the first column of the matrix representing A, we note that
</p>
<p>A|a1〉 = A
</p>
<p>⎛
⎝
</p>
<p>1
1
0
</p>
<p>⎞
⎠=
</p>
<p>⎛
⎝
</p>
<p>0
3
2
</p>
<p>⎞
⎠= 1
</p>
<p>2
</p>
<p>⎛
⎝
</p>
<p>1
1
0
</p>
<p>⎞
⎠&minus; 1
</p>
<p>2
</p>
<p>⎛
⎝
</p>
<p>1
0
1
</p>
<p>⎞
⎠+ 5
</p>
<p>2
</p>
<p>⎛
⎝
</p>
<p>0
1
1
</p>
<p>⎞
⎠
</p>
<p>= 1
2
|a1〉 &minus;
</p>
<p>1
</p>
<p>2
|a2〉 +
</p>
<p>5
</p>
<p>2
|a3〉.
</p>
<p>So, by Box 5.1.1, the first column of the matrix is
</p>
<p>⎛
⎜⎜⎝
</p>
<p>1
2
</p>
<p>&minus; 12
5
2
</p>
<p>⎞
⎟⎟⎠ .
</p>
<p>The other two columns are obtained from
</p>
<p>A|a2〉 = A
</p>
<p>⎛
⎝
</p>
<p>1
0
1
</p>
<p>⎞
⎠=
</p>
<p>⎛
⎝
</p>
<p>3
2
1
</p>
<p>⎞
⎠= 2
</p>
<p>⎛
⎝
</p>
<p>1
1
0
</p>
<p>⎞
⎠+
</p>
<p>⎛
⎝
</p>
<p>1
0
1
</p>
<p>⎞
⎠+ 0
</p>
<p>⎛
⎝
</p>
<p>0
1
1
</p>
<p>⎞
⎠ ,
</p>
<p>A|a3〉 = A
</p>
<p>⎛
⎝
</p>
<p>0
1
1
</p>
<p>⎞
⎠=
</p>
<p>⎛
⎝
</p>
<p>1
&minus;1
3
</p>
<p>⎞
⎠=&minus;3
</p>
<p>2
</p>
<p>⎛
⎝
</p>
<p>1
1
0
</p>
<p>⎞
⎠+ 5
</p>
<p>2
</p>
<p>⎛
⎝
</p>
<p>1
0
1
</p>
<p>⎞
⎠+ 1
</p>
<p>2
</p>
<p>⎛
⎝
</p>
<p>0
1
1
</p>
<p>⎞
⎠ ,
</p>
<p>giving the second and the third columns, respectively. The whole matrix is
then
</p>
<p>A =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>1
2 2 &minus; 32
</p>
<p>&minus; 12 1 52
5
2 0
</p>
<p>1
2
</p>
<p>⎞
⎟⎟⎠ .</p>
<p/>
</div>
<div class="page"><p/>
<p>142 5 Matrices
</p>
<p>As long as all vectors are represented by columns whose entries are ex-
pansion coefficients of the vectors in B , A and A are indistinguishable. How-
</p>
<p>ever, the action of A on the column vector
( x
y
z
</p>
<p>)
will not yield the RHS of
</p>
<p>Eq. (5.8)! Although this is not usually emphasized, the column vector on
the LHS of Eq. (5.8) is really the vector
</p>
<p>x
</p>
<p>⎛
⎝
</p>
<p>1
0
0
</p>
<p>⎞
⎠+ y
</p>
<p>⎛
⎝
</p>
<p>0
1
0
</p>
<p>⎞
⎠+ z
</p>
<p>⎛
⎝
</p>
<p>0
0
1
</p>
<p>⎞
⎠ ,
</p>
<p>which is an expansion in terms of the standard basis of R3 rather than in
terms of B .
</p>
<p>We can expand A
( x
y
z
</p>
<p>)
in terms of B , yielding
</p>
<p>A
</p>
<p>⎛
⎝
x
</p>
<p>y
</p>
<p>z
</p>
<p>⎞
⎠=
</p>
<p>⎛
⎝
x &minus; y + 2z
</p>
<p>3x &minus; z
2y + z
</p>
<p>⎞
⎠
</p>
<p>=
(
</p>
<p>2x &minus; 3
2
y
</p>
<p>)⎛
⎝
</p>
<p>1
1
0
</p>
<p>⎞
⎠+
</p>
<p>(
&minus;x + 1
</p>
<p>2
y + 2z
</p>
<p>)⎛
⎝
</p>
<p>1
0
1
</p>
<p>⎞
⎠
</p>
<p>+
(
x + 3
</p>
<p>2
y &minus; z
</p>
<p>)⎛
⎝
</p>
<p>0
1
1
</p>
<p>⎞
⎠ .
</p>
<p>This says that in the basis B this vector has the representation
</p>
<p>⎛
⎝A
</p>
<p>⎛
⎝
x
</p>
<p>y
</p>
<p>z
</p>
<p>⎞
⎠
⎞
⎠
</p>
<p>B
</p>
<p>=
</p>
<p>⎛
⎜⎜⎝
</p>
<p>2x &minus; 32y
&minus;x + 12y + 2z
x + 32y &minus; z
</p>
<p>⎞
⎟⎟⎠ . (5.9)
</p>
<p>Similarly,
( x
y
z
</p>
<p>)
is represented by
</p>
<p>⎛
⎝
⎛
⎝
x
</p>
<p>y
</p>
<p>z
</p>
<p>⎞
⎠
⎞
⎠
</p>
<p>B
</p>
<p>=
</p>
<p>⎛
⎜⎜⎝
</p>
<p>1
2x + 12y &minus; 12z
1
2x &minus; 12y + 12z
</p>
<p>&minus; 12x + 12y + 12z
</p>
<p>⎞
⎟⎟⎠ . (5.10)
</p>
<p>Applying A to the RHS of (5.10) yields the RHS of (5.9), as it should.
</p>
<p>5.2 Operations onMatrices
</p>
<p>There are two basic operations that one can perform on a matrix to obtain
a new one; these are transposition and complex conjugation. The transposetranspose of a matrix</p>
<p/>
</div>
<div class="page"><p/>
<p>5.2 Operations on Matrices 143
</p>
<p>of an M&times;N matrix A is an N &times;M matrix At obtained by interchanging the
rows and columns of A:
</p>
<p>(
At
)
ij
= (A)ji, or (αij )t = (αji). (5.11)
</p>
<p>The following theorem, whose proof follows immediately from the defi-
nition of transpose, summarizes the important properties of the operation of
transposition.
</p>
<p>Theorem 5.2.1 Let A and B be two matrices for which the operation of
addition and/or multiplication are defined. Then
</p>
<p>(a) (A + B)t = At + Bt ,
(b) (AB)t = BtAt ,
(c) (At )t = A.
</p>
<p>Let T &isin; L(V,W) and BV = {|ai〉}Ni=1 and BW = {|bi〉}Mi=1 bases in V
and W. Then
</p>
<p>T|ai〉 =
M&sum;
</p>
<p>j=1
(T)ji |bj 〉,
</p>
<p>where T =MBWBV (T). Let T
&lowast; &isin; L(W&lowast;,V&lowast;) be the pull-back of T and B&lowast;V =
</p>
<p>{θθθk}Nk=1 and B&lowast;W = {φφφl}Ml=1 bases dual to BV and BW . Then
</p>
<p>T&lowast;φφφl =
N&sum;
</p>
<p>k=1
</p>
<p>(
T&lowast;
)
kl
θθθk.
</p>
<p>Apply both sides of this equation to |ai〉 to get
</p>
<p>LHS =
(
T&lowast;φφφl
</p>
<p>)
|ai〉 &equiv;φφφl
</p>
<p>(
T|ai〉
</p>
<p>)
</p>
<p>=φφφl
(
</p>
<p>M&sum;
</p>
<p>j=1
(T)ji |bj 〉
</p>
<p>)
=
</p>
<p>M&sum;
</p>
<p>j=1
(T)ji
</p>
<p>=δlj︷ ︸︸ ︷
φφφl
(
|bj 〉
</p>
<p>)
= (T)li
</p>
<p>and
</p>
<p>RHS =
N&sum;
</p>
<p>k=1
</p>
<p>(
T&lowast;
)
kl
θθθk|ai〉 =
</p>
<p>N&sum;
</p>
<p>k=1
</p>
<p>(
T&lowast;
)
kl
δki =
</p>
<p>(
T&lowast;
)
il
.
</p>
<p>Comparing the last two equations, we have Matrix of pullback of T is
transpose of matrix of T.
</p>
<p>Proposition 5.2.2 Let T &isin;L(V,W) and BV and BW be bases in V andW.
Let T&lowast;, B&lowast;V , and B
</p>
<p>&lowast;
W be duals to T, BV , and BW , respectively. Let T =
</p>
<p>M
BW
BV
</p>
<p>(T) and T&lowast; =MB
&lowast;
V
</p>
<p>B&lowast;W
(T&lowast;). Then T&lowast; = Tt .
</p>
<p>Of special interest is a matrix that is equal to either its transpose or the
negative of its transpose. Such matrices occur frequently in physics.</p>
<p/>
</div>
<div class="page"><p/>
<p>144 5 Matrices
</p>
<p>Definition 5.2.3 A matrix S is symmetric if St = S. Similarly, a ma-
trix A is antisymmetric if At =&minus;A.
</p>
<p>Any matrix A can be written as A = 12 (A + At )+ 12 (A &minus; At ), where the
</p>
<p>symmetric and
</p>
<p>antisymmetric matrices
</p>
<p>first term is symmetric and the second is antisymmetric.
The elements of a symmetric matrix A satisfy the relation αji = (At )ij =
</p>
<p>(A)ij = αij ; i.e., the matrix is symmetric under reflection through the main
diagonal. On the other hand, for an antisymmetric matrix we have αji =
&minus;αij . In particular, the diagonal elements of an antisymmetric matrix are all
zero.
</p>
<p>A (real) matrix satisfying AtA = AAt = 1 is called orthogonal.orthogonal matrix
Complex conjugation is an operation under which all elements of a ma-complex conjugation
</p>
<p>trix are complex conjugated. Denoting the complex conjugate of A by A&lowast;,
we have (A&lowast;)ij = (A)&lowast;ij , or (αij )&lowast; = (α&lowast;ij ). A matrix is real if and only if
A&lowast; = A. Clearly, (A&lowast;)&lowast; = A.
</p>
<p>Under the combined operation of complex conjugation and transposition,
the rows and columns of a matrix are interchanged and all of its elements
are complex conjugated. This combined operation is called the adjoint op-
eration, or hermitian conjugation, and is denoted by &dagger;, as with operators.hermitian conjugate
Thus, we have
</p>
<p>A&dagger; =
(
At
)&lowast; =
</p>
<p>(
A&lowast;
</p>
<p>)t
,
</p>
<p>(
A&dagger;
</p>
<p>)
ij
= (A)&lowast;ji or (αij )&dagger; =
</p>
<p>(
α&lowast;ji
</p>
<p>)
.
</p>
<p>(5.12)
</p>
<p>Two types of matrices are important enough to warrant a separate definition.
</p>
<p>Definition 5.2.4 A hermitian matrix H satisfies H&dagger; = H, or, in terms ofhermitian and unitary
matrices elements, η&lowast;ij = ηji . A unitary matrix U satisfies U&dagger;U = UU&dagger; = 1, or, in
</p>
<p>terms of elements,
&sum;N
</p>
<p>k=1 μikμ
&lowast;
jk =
</p>
<p>&sum;N
k=1 μ
</p>
<p>&lowast;
kiμkj = δij .
</p>
<p>Remarks It follows immediately from this definition that
</p>
<p>1. The diagonal elements of a hermitian matrix are real.
2. The kth column of a hermitian matrix is the complex conjugate of its
</p>
<p>kth row, and vice versa.
3. A real hermitian matrix is symmetric.
4. The rows of an N &times;N unitary matrix, when considered as vectors in
</p>
<p>CN , form an orthonormal set, as do the columns.
5. A real unitary matrix is orthogonal.
</p>
<p>It is sometimes possible (and desirable) to transform a matrix into a form
in which all of its off-diagonal elements are zero. Such a matrix is called a
diagonal matrix.diagonal matrices
</p>
<p>Box 5.2.5 A diagonal matrix whose diagonal elements are {λk}Nk=1 is
denoted by diag(λ1, λ2, . . . , λN ).</p>
<p/>
</div>
<div class="page"><p/>
<p>5.2 Operations on Matrices 145
</p>
<p>Example 5.2.6 In this example, we derive a useful identity for functions of
a diagonal matrix. Let D = diag(λ1, λ2, . . . , λn) be a diagonal matrix, and
f (x) a function that has a Taylor series expansion f (x)=&sum;&infin;k=0 akxk . The
same function of D can be written as
</p>
<p>f (D)=
&infin;&sum;
</p>
<p>k=0
akD
</p>
<p>k =
&infin;&sum;
</p>
<p>k=0
ak
[
diag(λ1, λ2, . . . , λn)
</p>
<p>]k
</p>
<p>=
&infin;&sum;
</p>
<p>k=0
ak diag
</p>
<p>(
λk1, λ
</p>
<p>k
2, . . . , λ
</p>
<p>k
n
</p>
<p>)
</p>
<p>= diag
( &infin;&sum;
</p>
<p>k=0
akλ
</p>
<p>k
1,
</p>
<p>&infin;&sum;
</p>
<p>k=0
akλ
</p>
<p>k
2, . . . ,
</p>
<p>&infin;&sum;
</p>
<p>k=0
akλ
</p>
<p>k
n
</p>
<p>)
</p>
<p>= diag
(
f (λ1), f (λ2), . . . , f (λn)
</p>
<p>)
.
</p>
<p>In words, the function of a diagonal matrix is equal to a diagonal matrix
whose entries are the same function of the corresponding entries of the orig-
inal matrix. In the above derivation, we used the following obvious proper-
ties of diagonal matrices:
</p>
<p>a diag(λ1, λ2, . . . , λn)= diag(aλ1, aλ2, . . . , aλn),
diag(λ1, λ2, . . . , λn)+ diag(ω1,ω2, . . . ,ωn)
</p>
<p>= diag(λ1 +ω1, . . . , λn +ωn),
diag(λ1, λ2, . . . , λn) &middot; diag(ω1,ω2, . . . ,ωn)= diag(λ1ω1, . . . , λnωn).
</p>
<p>Example 5.2.7 In this example, we list some familiar matrices in physics.
</p>
<p>(a) A prototypical symmetric matrix is that of the moment of inertia en-
countered in mechanics. The ij th element of this matrix is defined as
Iij &equiv;
</p>
<p>�
ρ(x1, x2, x3)xixj dV , where xi is the ith Cartesian coordi-
</p>
<p>nate of a point in the distribution of mass described by the volume
density ρ(x1, x2, x3). It is clear that Iij = Iji , or I = It . The moment
of inertia matrix can be represented as
</p>
<p>I =
</p>
<p>⎛
⎝
I11 I12 I13
I12 I22 I23
I13 I23 I33
</p>
<p>⎞
⎠ .
</p>
<p>It has six independent elements.
(b) An example of an antisymmetric matrix is the electromagnetic field
</p>
<p>tensor given by
</p>
<p>F =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>0 &minus;B3 B2 E1
B3 0 &minus;B1 E2
&minus;B2 B1 0 E3
&minus;E1 &minus;E2 &minus;E3 0
</p>
<p>⎞
⎟⎟⎠ .</p>
<p/>
</div>
<div class="page"><p/>
<p>146 5 Matrices
</p>
<p>(c) Examples of hermitian matrices are the 2 &times; 2 Pauli spin matrices:
</p>
<p>σ1 =
(
</p>
<p>0 1
1 0
</p>
<p>)
, σ2 =
</p>
<p>(
0 &minus;i
i 0
</p>
<p>)
, σ3 =
</p>
<p>(
1 0
0 &minus;1
</p>
<p>)
.
</p>
<p>(d) The most frequently encountered orthogonal matrices are rotations.
</p>
<p>Pauli spin matrices
</p>
<p>One such matrix represents the rotation of a 3-dimensional rigid body
in terms of Euler angles and is used in mechanics. Attaching a coor-Euler angles
dinate system to the body, a general rotation can be decomposed into
a rotation of angle ϕ about the z-axis, followed by a rotation of angle
θ about the new x-axis, followed by a rotation of angle ψ about the
new z-axis. We simply exhibit this matrix in terms of these angles and
leave it to the reader to show that it is indeed orthogonal.
</p>
<p>(
cosψ cosϕ &minus; sinψ cos θ sinϕ &minus; cosψ sinϕ &minus; sinψ cos θ cosϕ sinψ sin θ
sinψ cosϕ + cosψ cos θ sinϕ &minus; sinψ sinϕ + cosψ cos θ cosϕ &minus; cosψ sin θ
</p>
<p>sin θ sinϕ sin θ cosϕ cos θ
</p>
<p>)
.
</p>
<p>5.3 Orthonormal Bases
</p>
<p>The matrix representation of A &isin; End(V) is facilitated by choosing an or-
thonormal basis B = {|ei〉}Ni=1. The matrix elements of A can be found in
such a basis by &ldquo;multiplying&rdquo; both sides of A|ei〉 =
</p>
<p>&sum;N
k=1 αki |ek〉 on the left
</p>
<p>by 〈ej |:
</p>
<p>〈ej |A|ei〉 = 〈ej |
(
</p>
<p>N&sum;
</p>
<p>k=1
αki |ek〉
</p>
<p>)
=
</p>
<p>N&sum;
</p>
<p>k=1
αki 〈ej |ek〉︸ ︷︷ ︸
</p>
<p>=δjk
</p>
<p>= αji,
</p>
<p>or
</p>
<p>(A)ij = αij = 〈ei |A|ej 〉. (5.13)
We can also show that in an orthonormal basis, the ith component ξi of
</p>
<p>a vector is found by multiplying the vector by 〈ei |. This expression for ξi
allows us to write the expansion of |x〉 as
</p>
<p>|x〉 =
N&sum;
</p>
<p>j=1
〈ej |x〉︸ ︷︷ ︸
</p>
<p>ξj
</p>
<p>|ej 〉 =
N&sum;
</p>
<p>j=1
|ej 〉〈ej |x〉 &rArr; 1=
</p>
<p>N&sum;
</p>
<p>j=1
|ej 〉〈ej |, (5.14)
</p>
<p>which is the same as in Proposition 4.4.6.
Let us now investigate the representation of the special operators dis-
</p>
<p>cussed in Chap. 4 and find the connection between those operators and the
matrices encountered in the last section. We begin by calculating the matrix
representing the hermitian conjugate of an operator T. In an orthonormal
basis, the elements of this matrix are given by Eq. (5.13), τij = 〈ei |T|ej 〉.
Taking the complex conjugate of this equation and using the definition of T&dagger;
</p>
<p>given in Eq. (4.11), we obtain
</p>
<p>τ &lowast;ij = 〈ei |T|ej 〉&lowast; = 〈ej |T&dagger;|ei〉, or
(
T&dagger;
)
ij
= τ &lowast;ji .</p>
<p/>
</div>
<div class="page"><p/>
<p>5.3 Orthonormal Bases 147
</p>
<p>This is precisely how the adjoint of a matrix was defined. Note how cru-
cially this conclusion depends on the orthonormality of the basis vectors. If
the basis were not orthonormal, we could not use Eq. (5.13) on which the
conclusion is based. Therefore,
</p>
<p>Box 5.3.1 Only in an orthonormal basis is the adjoint of an operator
represented by the adjoint of the matrix representing that operator.
</p>
<p>In particular, a hermitian operator is represented by a hermitian matrix
only if an orthonormal basis is used. The following example illustrates this
point.
</p>
<p>Example 5.3.2 Consider the matrix representation of the hermitian opera-
tor H in a general&mdash;not orthonormal&mdash;basis B = {|ai〉}Ni=1. The elements of
the matrix corresponding to H are given by
</p>
<p>H|ak〉 =
N&sum;
</p>
<p>j=1
ηjk|aj 〉, or H|ai〉 =
</p>
<p>N&sum;
</p>
<p>j=1
ηji |aj 〉. (5.15)
</p>
<p>Taking the product of the first equation with 〈ai | and complex-conjugating
the result gives
</p>
<p>〈ai |H|ak〉&lowast; =
(
</p>
<p>N&sum;
</p>
<p>j=1
ηjk〈ai |aj 〉
</p>
<p>)&lowast;
=
</p>
<p>N&sum;
</p>
<p>j=1
η&lowast;jk〈aj |ai〉.
</p>
<p>But by the definition of a hermitian operator,
</p>
<p>〈ai |H|ak〉&lowast; = 〈ak|H&dagger;|ai〉 = 〈ak|H|ai〉.
</p>
<p>So we have 〈ak|H|ai〉 =
&sum;N
</p>
<p>j=1 η
&lowast;
jk〈aj |ai〉.
</p>
<p>On the other hand, multiplying the second equation in (5.15) by 〈ak|
gives
</p>
<p>〈ak|H|ai〉 =
N&sum;
</p>
<p>j=1
ηji〈ak|aj 〉.
</p>
<p>The only conclusion we can draw from this discussion is
</p>
<p>N&sum;
</p>
<p>j=1
η&lowast;jk〈aj |ai〉 =
</p>
<p>N&sum;
</p>
<p>j=1
ηji〈ak|aj 〉.
</p>
<p>Because this equation does not say anything about each individual ηij ,
we cannot conclude, in general, that η&lowast;ij = ηji . However, if the |ai〉&rsquo;s
are orthonormal, then 〈aj |ai〉 = δji and 〈ak|aj 〉 = δkj , and we obtain&sum;N
</p>
<p>j=1 η
&lowast;
jkδji =
</p>
<p>&sum;N
j=1 ηjiδkj , or η
</p>
<p>&lowast;
ik = ηki , as expected of a hermitian ma-
</p>
<p>trix.</p>
<p/>
</div>
<div class="page"><p/>
<p>148 5 Matrices
</p>
<p>Similarly, we expect the matrices representing unitary operators to be
unitary only if the basis is orthonormal. This is an immediate consequence
of Eq. (5.12), but we shall prove it in order to provide yet another example
of how the completeness relation, Eq. (5.14), is used. Since UU&dagger; = 1, we
have
</p>
<p>〈ei |UU&dagger;|ej 〉 = 〈ei |1|ej 〉 = 〈ei |ej 〉 = δij .
</p>
<p>We insert the completeness relation 1 =&sum;Nk=1 |ek〉〈ek| between U and U&dagger;
on the LHS:
</p>
<p>〈ei |U
(
</p>
<p>N&sum;
</p>
<p>k=1
|ek〉〈ek|
</p>
<p>)
U&dagger;|ej 〉 =
</p>
<p>N&sum;
</p>
<p>k=1
〈ei |U|ek〉︸ ︷︷ ︸
</p>
<p>&equiv;μik
</p>
<p>〈ek|U&dagger;|ej 〉︸ ︷︷ ︸
&equiv;μ&lowast;jk
</p>
<p>= δij .
</p>
<p>This equation gives the first half of the requirement for a unitary matrix
given in Definition 5.2.4. By redoing the calculation for U&dagger;U, we could ob-
tain the second half of that requirement.
</p>
<p>5.4 Change of Basis
</p>
<p>It is often advantageous to describe a physical problem in a particular basis
because it takes a simpler form there, but the general form of the result may
still be of importance. In such cases the problem is solved in one basis, and
the result is transformed to other bases. Let us investigate this point in some
detail.
</p>
<p>Given a basis B = {|ai〉}Ni=1, we can write an arbitrary vector |a〉 with
components {αi}Ni=1 in B as |a〉 =
</p>
<p>&sum;N
i=1 αi |ai〉. Now suppose that we
</p>
<p>change the basis to B &prime; = {|a&prime;j 〉}Nj=1. How are the components of |a〉 in B &prime;
related to those in B? To answer this question, we write |ai〉 in terms of B &prime;
vectors,
</p>
<p>|ai〉 =
N&sum;
</p>
<p>j=1
ρji |a&prime;j 〉, i = 1,2, . . . ,N,
</p>
<p>which can also be abbreviated as
⎛
⎜⎜⎜⎝
</p>
<p>|a1〉
|a2〉
...
</p>
<p>|aN 〉
</p>
<p>⎞
⎟⎟⎟⎠=
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>ρ11 ρ21 &middot; &middot; &middot; ρN1
ρ12 ρ22 &middot; &middot; &middot; ρN2
...
</p>
<p>...
...
</p>
<p>ρ1N ρ2N &middot; &middot; &middot; ρNN
</p>
<p>⎞
⎟⎟⎟⎠
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>|a&prime;1〉
|a&prime;2〉
...
</p>
<p>|a&prime;N 〉
</p>
<p>⎞
⎟⎟⎟⎠ . (5.16)
</p>
<p>In this notation, we also have
</p>
<p>|a〉 =
(
α1 α2 . . . αN
</p>
<p>)
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>|a1〉
|a2〉
...
</p>
<p>|aN 〉
</p>
<p>⎞
⎟⎟⎟⎠&equiv; a
</p>
<p>t
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>|a1〉
|a2〉
...
</p>
<p>|aN 〉
</p>
<p>⎞
⎟⎟⎟⎠ ,</p>
<p/>
</div>
<div class="page"><p/>
<p>5.4 Change of Basis 149
</p>
<p>where a is the column representation of |a〉 in B . Now multiply both sides
of (5.16) by at to get
</p>
<p>|a〉 = at
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>|a1〉
|a2〉
...
</p>
<p>|aN 〉
</p>
<p>⎞
⎟⎟⎟⎠&equiv; a
</p>
<p>tRt
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>|a&prime;1〉
|a&prime;2〉
...
</p>
<p>|a&prime;N 〉
</p>
<p>⎞
⎟⎟⎟⎠&equiv;
</p>
<p>(
α&prime;1 α
</p>
<p>&prime;
2 . . . α
</p>
<p>&prime;
N
</p>
<p>)
︸ ︷︷ ︸
</p>
<p>&equiv;a&prime;t
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>|a&prime;1〉
|a&prime;2〉
...
</p>
<p>|a&prime;N 〉
</p>
<p>⎞
⎟⎟⎟⎠
</p>
<p>where R is the transpose of the N &times; N matrix of Eq. (5.16), and the last
equality expresses |a〉 in B &prime;. We therefore conclude that
</p>
<p>a&prime;t &equiv; atRt ,
</p>
<p>where a&prime; designates a column vector with elements α&prime;j , the components of
|a〉 in B &prime;. Taking the transpose of the last equation yields
</p>
<p>a&prime; = Ra or
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>α&prime;1
α&prime;2
...
</p>
<p>α&prime;N
</p>
<p>⎞
⎟⎟⎟⎠=
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>ρ11 ρ12 . . . ρ1N
ρ21 ρ22 . . . ρ2N
...
</p>
<p>...
...
</p>
<p>ρN1 ρN2 . . . ρNN
</p>
<p>⎞
⎟⎟⎟⎠
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>α1
α2
...
</p>
<p>αN
</p>
<p>⎞
⎟⎟⎟⎠ , (5.17)
</p>
<p>which in component form can be written as
</p>
<p>α&prime;j =
N&sum;
</p>
<p>i=1
ρjiαi for j = 1,2, . . . ,N. (5.18)
</p>
<p>The matrix R is called the basis transformation matrix. It is invertible basis transformation
matrixbecause it is a linear transformation that maps one basis onto another (see
</p>
<p>Proposition 4.1.3).
What happens to a matrix representation of an operator when we trans-
</p>
<p>form the basis? Consider the equation |b〉 = A|a〉, where |a〉 and |b〉 have
components {αi}Ni=1 and {βi}Ni=1, respectively, in B . This equation has a
corresponding matrix equation b = Aa. Now, if we change the basis, the
columns of the components of |a〉 and |b〉 will change to those of a&prime; and b&prime;,
respectively. We seek a matrix A&prime; such that b&prime; = A&prime;a&prime;. This matrix will be
the transform of A. Using Eq. (5.17), we write Rb = A&prime;Ra, or b = R&minus;1A&prime;Ra.
Comparing this with b = Aa and applying the fact that both equations hold
for arbitrary a and b, we conclude that
</p>
<p>R&minus;1A&prime;R = A, or A&prime; = RAR&minus;1. (5.19)
</p>
<p>This is called a similarity transformation on A, and A&prime; is said to be similar similarity transformation
to A.
</p>
<p>The transformation matrix R can easily be found for orthonormal bases
B = {|ei〉}Ni=1 and B &prime; = {|e&prime;i〉}Ni=1. We have |ei〉 =
</p>
<p>&sum;N
k=1 ρki |e&prime;k〉. Multiply-
</p>
<p>ing this equation by 〈e&prime;j |, we obtain
</p>
<p>&lang;
e&prime;j
</p>
<p>∣∣ ei
&rang;
=
</p>
<p>N&sum;
</p>
<p>k=1
ρki
</p>
<p>&lang;
e&prime;j
</p>
<p>∣∣ e&prime;k
&rang;
=
</p>
<p>N&sum;
</p>
<p>k=1
ρkiδjk = ρji . (5.20)</p>
<p/>
</div>
<div class="page"><p/>
<p>150 5 Matrices
</p>
<p>That is,
</p>
<p>Box 5.4.1 To find the ij th element of the matrix that changes the
components of a vector in the orthonormal basis B to those in the
orthonormal basis B &prime;, take the j th ket in B and multiply it by the ith
bra in B &prime;.
</p>
<p>To find the ij th element of the matrix that changes B &prime; into B , we take the
j th ket in B &prime; and multiply it by the ith bra in B: ρ&prime;ij = 〈ei |e&prime;j 〉. However, the
matrix R&prime; must be R&minus;1, as can be seen from Eq. (5.17). On the other hand,
(ρ&prime;ij )
</p>
<p>&lowast; = 〈ei |e&prime;j 〉&lowast; = 〈e&prime;j |ei〉 = ρji , or
(
R&minus;1
</p>
<p>)&lowast;
ij
= ρji, or
</p>
<p>(
R&minus;1
</p>
<p>)
ij
= ρ&lowast;ji =
</p>
<p>(
R&dagger;
</p>
<p>)
ij
. (5.21)
</p>
<p>This shows that R is a unitary matrix and yields an important result.
</p>
<p>Theorem 5.4.2 The matrix that transforms one orthonormal basis into an-
other is necessarily unitary.
</p>
<p>From Eqs. (5.20) and (5.21) we have (R&dagger;)ij = 〈ei |e&prime;j 〉. Thus,
</p>
<p>Box 5.4.3 To obtain the j th column of R&dagger;, we take the j th vec-
tor in the new basis and successively &ldquo;multiply&rdquo; it by 〈ei | for i =
1,2, . . . ,N .
</p>
<p>In particular, if the original basis is the standard basis of CN and |e&prime;j 〉 is
represented by a column vector in that basis, then the j th column of R&dagger; is
simply the vector |e&prime;j 〉.
</p>
<p>Example 5.4.4 In this example, we show that the similarity transform of a
function of a matrix is the same function of the similarity transform of the
matrix:
</p>
<p>Rf (A)R&minus;1 = f
(
RAR&minus;1
</p>
<p>)
.
</p>
<p>The proof involves inserting 1 = R&minus;1R between factors of A in the Taylor
series expansion of f (A):
</p>
<p>Rf (A)R&minus;1 = R
( &infin;&sum;
</p>
<p>k=0
akA
</p>
<p>k
</p>
<p>)
R&minus;1 =
</p>
<p>&infin;&sum;
</p>
<p>k=0
akRA
</p>
<p>kR&minus;1 =
&infin;&sum;
</p>
<p>k=0
akR
</p>
<p>k times︷ ︸︸ ︷
AA &middot; &middot; &middot;A R&minus;1
</p>
<p>=
&infin;&sum;
</p>
<p>k=0
ak
</p>
<p>k times︷ ︸︸ ︷
RAR&minus;1RAR&minus;1 &middot; &middot; &middot;RAR&minus;1 =
</p>
<p>&infin;&sum;
</p>
<p>k=0
ak
(
RAR&minus;1
</p>
<p>)k
</p>
<p>= f
(
RAR&minus;1
</p>
<p>)
.
</p>
<p>This completes the proof.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.5 Determinant of a Matrix 151
</p>
<p>5.5 Determinant of a Matrix
</p>
<p>An important concept associated with linear operators is the determinant,
which we have already discussed in Sect. 2.6.1. Determinants are also
defined for matrices. If A is representing A in some basis, then we set
det A = detA. That this relation is basis-independent is, of course, obvious
from Definition 2.6.10 and the discussion preceding it. However, it can also
be shown directly, as we shall do later in this chapter.
</p>
<p>Let A be a linear operator on V. Let {|ek〉}Nk=1 be a basis of V in which A
is represented by A. Then the left-hand side of Eq. (2.32) becomes
</p>
<p>LHS =���
(
A|e1〉, . . . ,A|eN 〉
</p>
<p>)
=���
</p>
<p>(
N&sum;
</p>
<p>i1=1
αi11|ei1〉, . . . ,
</p>
<p>N&sum;
</p>
<p>iN=1
αiNN |eiN 〉
</p>
<p>)
</p>
<p>=
N&sum;
</p>
<p>i1...iN=1
αi11 . . . αiNN���
</p>
<p>(
|ei1〉, . . . , |eiN 〉
</p>
<p>)
</p>
<p>=
&sum;
</p>
<p>π
</p>
<p>απ(1)1 . . . απ(N)N���
(
|eπ(1)〉, . . . , |eπ(N)〉
</p>
<p>)
</p>
<p>=
&sum;
</p>
<p>π
</p>
<p>απ(1)1 . . . απ(N)Nǫπ &middot;���
(
|e1〉, . . . , |eN 〉
</p>
<p>)
,
</p>
<p>where π is the permutation taking k to ik . The right-hand side of Eq. (2.32)
is just the product of detA and ���(|e〉1, . . . , |e〉N ). Hence,
</p>
<p>detA= det A =
&sum;
</p>
<p>π
</p>
<p>ǫπαπ(1)1 . . . απ(N)N &equiv;
&sum;
</p>
<p>π
</p>
<p>ǫπ
</p>
<p>N&prod;
</p>
<p>k=1
(A)π(k)k . (5.22)
</p>
<p>Since π(k)= ik , the product in the sum can be written as
N&prod;
</p>
<p>k=1
(A)ikk =
</p>
<p>N&prod;
</p>
<p>k=1
(A)ikπ&minus;1(ik) =
</p>
<p>N&prod;
</p>
<p>k=1
(A)kπ&minus;1(k) =
</p>
<p>N&prod;
</p>
<p>k=1
</p>
<p>(
At
)
π&minus;1(k)k,
</p>
<p>where the second equality follows because we can commute the numbers
until (A)1π&minus;1(1) becomes the first term of the product, (A)2π&minus;1(2), the second
term, and so on. Substituting this in (5.22) and noting that
</p>
<p>&sum;
π =
</p>
<p>&sum;
π&minus;1 and
</p>
<p>ǫπ&minus;1 = ǫπ , we have
</p>
<p>Theorem 5.5.1 Let A &isin; L(V) and A its representation in any basis of V.
Then
</p>
<p>detA= det A =
&sum;
</p>
<p>π
</p>
<p>ǫπ
</p>
<p>N&prod;
</p>
<p>j=1
(A)π(j)j =
</p>
<p>&sum;
</p>
<p>i1...iN
</p>
<p>εi1i2...iN (A)i11 . . . (A)iNN
</p>
<p>detA= det At =
&sum;
</p>
<p>π
</p>
<p>ǫπ
</p>
<p>N&prod;
</p>
<p>j=1
(A)jπ(j) =
</p>
<p>&sum;
</p>
<p>i1...iN
</p>
<p>εi1i2...iN (A)1i1 . . . (A)NiN
</p>
<p>where εi1i2...iN is the symbol introduced in (2.29). In particular, det A
t =
</p>
<p>det A.</p>
<p/>
</div>
<div class="page"><p/>
<p>152 5 Matrices
</p>
<p>Let A be any N&times;N matrix. Let |vj 〉 &isin;RN be the j th column of A. Define
the linear operator A &isin; End(RN ) by
</p>
<p>A|ej 〉 = |vj 〉, j = 1, . . . ,N, (5.23)
</p>
<p>where {|ej 〉}Nj=1 is the standard basis of RN . Then A is the matrix repre-
senting A in the standard basis. Now let ��� be a determinant function in RN
</p>
<p>whose value is one at the standard basis. Then
</p>
<p>���
(
|v1〉, . . . , |vN 〉
</p>
<p>)
=���
</p>
<p>(
A|e1〉, . . . ,A|eN 〉
</p>
<p>)
</p>
<p>= detA &middot;���
(
|e1〉, . . . , |eN 〉
</p>
<p>)
= detA
</p>
<p>and, therefore,
</p>
<p>det A =���
(
|v1〉, . . . , |vN 〉
</p>
<p>)
. (5.24)
</p>
<p>If instead of columns, we use rows |uj 〉, we obtain det At =���(|u1〉, . . . ,
|uN 〉). Since��� is a multilinear skew-symmetric function, and det At = det A,
we have the following familiar theorem.
</p>
<p>Theorem 5.5.2 Let A be a square matrix. Then
</p>
<p>1. det A is linear with respect to any row or column vector of A.
2. If any two rows or two columns of A are interchanged, det A changes
</p>
<p>sign.
3. Adding a multiple of one row (column) of A to another row (column) of
</p>
<p>A does not change det A.
4. det A = 0 iff the rows (columns) are linearly dependent.
</p>
<p>5.5.1 Matrix of the Classical Adjoint
</p>
<p>Since by Corollary 2.6.13, the classical adjoint of A is essentially the inverse
of A, we expect its matrix representation to be essentially the inverse of the
matrix of A. To find this matrix, choose a basis {|ej 〉}Nj=1 which evaluates
the determinant function of Eq. (2.33) to 1. Then ad(A)|ei〉 = cji |ej 〉, with
cji forming the representation matrix of ad(A). Thus, substituting |ei〉 for
|v〉 on both sides of (2.33) and using the fact that {|ej 〉}Nj=1 are linearly
independent, we get
</p>
<p>(&minus;1)j&minus;1���
(
|ei〉,A|e1〉, . . . , Â|ej 〉, . . . ,A|eN 〉
</p>
<p>)
= cji
</p>
<p>or
</p>
<p>cji = (&minus;1)j&minus;1���
(
|ei〉,
</p>
<p>N&sum;
</p>
<p>k1=1
(A)k11|ek1〉, . . . , Â|ej 〉, . . . ,
</p>
<p>N&sum;
</p>
<p>kN=1
(A)kNN |ekN 〉
</p>
<p>)
</p>
<p>= (&minus;1)j&minus;1
&sum;
</p>
<p>k1...kN
</p>
<p>(A)k11 . . . (A)kNN���
(
|ei〉, |ek1〉, . . . , |ekN 〉
</p>
<p>)
</p>
<p>= (&minus;1)j&minus;1
&sum;
</p>
<p>k1...kN
</p>
<p>(A)k11 . . . (A)kNNǫik1...kN .</p>
<p/>
</div>
<div class="page"><p/>
<p>5.5 Determinant of a Matrix 153
</p>
<p>The product in the sum does not include (A)kj j . This means that the entire
j th column is missing in the product. Furthermore, because of the skew-
symmetry of ǫik1...kN , none of the km&rsquo;s can be i, and since km&rsquo;s label the
rows, the ith row is also absent in the sum. Now move i from the first lo-
cation to the ith location. This will introduce a factor of (&minus;1)i&minus;1 due to
the i &minus; 1 exchanges of indices. Inserting all this information in the previous
equation, we obtain
</p>
<p>cji = (&minus;1)i+j
&sum;
</p>
<p>k1...kN
</p>
<p>(A)k11 . . . (A)kNNǫk1...i...kN . (5.25)
</p>
<p>Now note that the sum is a determinant of an (N &minus; 1)&times; (N &minus; 1) matrix
obtained from A by eliminating its ith row and j th column. This determi-
nant is called a minor of order N &minus; 1 and denoted by Mij . The product minor of orderN &minus; 1
(&minus;1)i+jMij is called the cofactor of (A)ij , and denoted by (cof A)ij . cofactor of an element of
</p>
<p>a matrixWith this and another obvious notation, (5.25) becomes
</p>
<p>(ad A)ji &equiv; cji = (&minus;1)i+jMij = (cof A)ij . (5.26)
</p>
<p>With the matrix of the adjoint at our disposal, we can write Eq. (2.34) in the
matrix form. Doing so, and taking the ikth element of all sides, we get
</p>
<p>N&sum;
</p>
<p>j=1
ad(A)ij (A)jk = detA &middot; δik =
</p>
<p>N&sum;
</p>
<p>j=1
(A)ij ad(A)jk .
</p>
<p>Setting k = i yields
</p>
<p>detA=
N&sum;
</p>
<p>j=1
ad(A)ij (A)ji =
</p>
<p>N&sum;
</p>
<p>j=1
(A)ij ad(A)ji
</p>
<p>or, using (5.26),
</p>
<p>detA=
N&sum;
</p>
<p>j=1
(A)ji(cof A)ji =
</p>
<p>N&sum;
</p>
<p>j=1
(A)ij (cof A)ij . (5.27)
</p>
<p>This is the familiar expansion of a determinant by its ith column or ith row.
</p>
<p>Historical Notes
</p>
<p>Vandermonde, Alexandre-Thi&eacute;ophile, also known as Alexis, Abnit, and Charles-
Auguste Vandermonde (1735&ndash;1796) had a father, a physician who directed his sickly
son toward a musical career. An acquaintanceship with Fontaine, however, so stimulated
Vandermonde that in 1771 he was elected to the Acad&eacute;mie des Sciences, to which he pre-
sented four mathematical papers (his total mathematical production) in 1771&ndash;1772. Later,
Vandermonde wrote several papers on harmony, and it was said at that time that musicians
considered Vandermonde to be a mathematician and that mathematicians viewed him as
a musician.
Vandermonde&rsquo;s membership in the Academy led to a paper on experiments with cold,
made with Bezout and Lavoisier in 1776, and a paper on the manufacture of steel with
Berthollet and Monge in 1786. Vandermonde became an ardent and active revolutionary,
being such a close friend of Monge that he was termed &ldquo;femme de Monge&rdquo;. He was a
member of the Commune of Paris and the club of the Jacobins. In 1782 he was director of</p>
<p/>
</div>
<div class="page"><p/>
<p>154 5 Matrices
</p>
<p>the Conservatoire des Arts et M&eacute;tiers and in 1792, chief of the Bureau de l&rsquo;Habillement
des Armies. He joined in the design of a course in political economy for the &Eacute;cole Nor-
male and in 1795 was named a member of the Institut National.
Vandermonde is best known for the theory of determinants. Lebesgue believed that the
attribution of determinant to Vandermonde was due to a misreading of his notation. Nev-
ertheless, Vandermonde&rsquo;s fourth paper was the first to give a connected exposition of
determinants, because he (1) defined a contemporary symbolism that was more com-
plete, simple, and appropriate than that of Leibniz; (2) defined determinants as functions
apart from the solution of linear equations presented by Cramer but also treated by Van-
dermonde; and (3) gave a number of properties of these functions, such as the number
and signs of the terms and the effect of interchanging two consecutive indices (rows or
columns), which he used to show that a determinant is zero if two rows or columns are
identical.
Vandermonde&rsquo;s real and unrecognized claim to fame was lodged in his first paper, in
which he approached the general problem of the solvability of algebraic equations through
a study of functions invariant under permutations of the roots of the equations. Cauchy
assigned priority in this to Lagrange and Vandermonde. Vandermonde read his paper in
November 1770, but he did not become a member of the Academy until 1771, and the pa-
per was not published until 1774. Although Vandermonde&rsquo;s methods were close to those
later developed by Abel and Galois for testing the solvability of equations, and although
his treatment of the binomial equation xn&minus;1 = 0 could easily have led to the anticipation
of Gauss&rsquo;s results on constructible polygons, Vandermonde himself did not rigorously or
completely establish his results, nor did he see the implications for geometry. Neverthe-
less, Kronecker dates the modern movement in algebra to Vandermonde&rsquo;s 1770 paper.
Unfortunately, Vandermonde&rsquo;s spurt of enthusiasm and creativity, which in two years
produced four insightful mathematical papers at least two of which were of substantial
importance, was quickly diverted by the exciting politics of the time and perhaps by poor
health.
</p>
<p>Example 5.5.3 Let O and U denote, respectively, an orthogonal and a uni-
tary n&times; n matrix; that is, OOt = OtO = 1, and UU&dagger; = U&dagger;U = 1. Taking the
determinant of the first equation and using Theorems 2.6.11 (with λ = 1)
and 5.5.1, we obtain
</p>
<p>(det O)
(
det Ot
</p>
<p>)
= (det O)2 = det 1 = 1.
</p>
<p>Therefore, for an orthogonal matrix, we get det O =&plusmn;1.
Orthogonal transformations preserve a real inner product. Among such
</p>
<p>transformations are the so-called inversions, which, in their simplest form,
multiply a vector by &minus;1. In three dimensions this corresponds to a reflection
through the origin. The matrix associated with this operation is &minus;1:
</p>
<p>⎛
⎝
x
</p>
<p>y
</p>
<p>z
</p>
<p>⎞
⎠&rarr;
</p>
<p>⎛
⎝
&minus;x
&minus;y
&minus;z
</p>
<p>⎞
⎠=
</p>
<p>⎛
⎝
&minus;1 0 0
0 &minus;1 0
0 0 &minus;1
</p>
<p>⎞
⎠
⎛
⎝
x
</p>
<p>y
</p>
<p>z
</p>
<p>⎞
⎠ ,
</p>
<p>which has a determinant of &minus;1. This is a prototype of other, more compli-
cated, orthogonal transformations whose determinants are &minus;1. The set of
orthogonal matrices in n dimensions is denoted by O(n).O(n) and SO(n)
</p>
<p>The other orthogonal transformations, whose determinants are +1, are
of special interest because they correspond to rotations in three dimensions.
The set of orthogonal matrices in n dimensions having determinant +1 is
denoted by SO(n). These matrices are special because they have the math-
ematical structure of a (continuous) group, which finds application in many</p>
<p/>
</div>
<div class="page"><p/>
<p>5.5 Determinant of a Matrix 155
</p>
<p>areas of advanced physics. We shall come back to the topic of group theory
later in the book.
</p>
<p>We can obtain a similar result for unitary transformations. We take the
determinant of both sides of U&dagger;U = 1:
</p>
<p>det
(
U&lowast;
</p>
<p>)t
det U = det U&lowast; det U = (det U)&lowast;(det U)= |det U|2 = 1.
</p>
<p>Thus, we can generally write det U = eiα , with α &isin; R. The set of unitary
matrices in n dimensions is denoted by U(n). The set of those matrices with U(n) and SU(n)
α = 0 forms a group to which 1 belongs and that is denoted by SU(n). This
group has found applications in the description of fundamental forces and
the dynamics of fundamental particles.
</p>
<p>5.5.2 Inverse of a Matrix
</p>
<p>Equation (5.26) shows that the matrix of the classical adjoint is the transpose
of the cofactor matrix. Using this, and writing (2.34) in matrix form yields
</p>
<p>(cof A)tA = detA &middot; 1 = A(cof A)t .
</p>
<p>Therefore, we have
</p>
<p>Theorem 5.5.4 The matrix A has an inverse if and only if det A �= 0. Fur- inverse of a matrix
thermore,
</p>
<p>A&minus;1 = (cof A)
t
</p>
<p>det A
. (5.28)
</p>
<p>This is the matrix form of the operator equation in Corollary 2.6.13.
</p>
<p>Example 5.5.5 The inverse of a 2 &times; 2 matrix is easily found:
(
a b
</p>
<p>c d
</p>
<p>)&minus;1
= 1
</p>
<p>ad &minus; bc
</p>
<p>(
d &minus;b
&minus;c a
</p>
<p>)
(5.29)
</p>
<p>if ad &minus; bc �= 0.
</p>
<p>We defined the determinant of an operator intrinsically, i.e., independent
of a basis. We have also connected this intrinsic property to the determinant
of the matrix representing that operator in some basis. We can now show di-
rectly that the matrices representing an operator in two arbitrary bases have
the same determinant. We leave this as exercise for the reader in Problem
5.23.
</p>
<p>Algorithm for Calculating the Inverse of a Matrix
</p>
<p>There is a more practical way of calculating the inverse of matrices. In the
following discussion of this method, we shall confine ourselves simply to
stating a couple of definitions and the main theorem, with no attempt at
providing any proofs. The practical utility of the method will be illustrated
by a detailed analysis of examples.</p>
<p/>
</div>
<div class="page"><p/>
<p>156 5 Matrices
</p>
<p>Definition 5.5.6 An elementary row operation on a matrix is one of theelementary row
operation following:
</p>
<p>(a) interchange of two rows of the matrix,
(b) multiplication of a row by a nonzero number, and
(c) addition of a multiple of one row to another.
</p>
<p>Elementary column operations are defined analogously.
</p>
<p>Definition 5.5.7 A matrix is in triangular, or row-echelon, form if it satis-triangular, or
row-echelon form of a
</p>
<p>matrix
</p>
<p>fies the following three conditions:
</p>
<p>1. Any row consisting of only zeros is below any row that contains at least
one nonzero element.
</p>
<p>2. Going from left to right, the first nonzero entry of any row is to the left
of the first nonzero entry of any lower row.
</p>
<p>3. The first nonzero entry of each row is 1.
</p>
<p>Theorem 5.5.8 For any invertible n&times; n matrix A, the n&times; 2n matrix (A|1)
can be transformed into the n &times; 2n matrix (1|A&minus;1) by means of a finite
number of elementary row operations.1
</p>
<p>A systematic way of transforming (A|1) into (1|A&minus;1) is first to bring A
into triangular form and then eliminate all nonzero elements of each column
by elementary row operations.
</p>
<p>Example 5.5.9
Let us evaluate the inverse of
</p>
<p>A =
</p>
<p>⎛
⎝
</p>
<p>1 2 &minus;1
0 1 &minus;2
2 1 &minus;1
</p>
<p>⎞
⎠ .
</p>
<p>We start with
⎛
⎝
</p>
<p>1 2 &minus;1
0 1 &minus;2
2 1 &minus;1
</p>
<p>∣∣∣∣∣∣
</p>
<p>1 0 0
0 1 0
0 0 1
</p>
<p>⎞
⎠&equiv; M
</p>
<p>and apply elementary row operations to M to bring the left half of it into
triangular form. If we denote the kth row by (k) and the three operations of
Definition 5.5.6, respectively, by (k)&harr; (j), α(k), and α(k)+ (j), we get
</p>
<p>M &minus;&minus;&minus;&minus;&minus;&minus;&rarr;
&minus;2(1)+(3)
</p>
<p>⎛
⎝
</p>
<p>1 2 &minus;1
0 1 &minus;2
0 &minus;3 1
</p>
<p>∣∣∣∣∣∣
</p>
<p>1 0 0
0 1 0
&minus;2 0 1
</p>
<p>⎞
⎠
</p>
<p>1The matrix (A|1) denotes the n &times; 2n matrix obtained by juxtaposing the n &times; n unit
matrix to the right of A. It can easily be shown that if A, B, and C are n&times;n matrices, then
A(B|C)= (AB|AC).</p>
<p/>
</div>
<div class="page"><p/>
<p>5.5 Determinant of a Matrix 157
</p>
<p>&minus;&minus;&minus;&minus;&minus;&rarr;
3(2)+(3)
</p>
<p>⎛
⎝
</p>
<p>1 2 &minus;1
0 1 &minus;2
0 0 &minus;5
</p>
<p>∣∣∣∣∣∣
</p>
<p>1 0 0
0 1 0
&minus;2 3 1
</p>
<p>⎞
⎠
</p>
<p>&minus;&minus;&minus;&rarr;
&minus; 15 (3)
</p>
<p>⎛
⎝
</p>
<p>1 2 &minus;1
0 1 &minus;2
0 0 1
</p>
<p>∣∣∣∣∣∣
</p>
<p>1 0 0
0 1 0
</p>
<p>2/5 &minus;3/5 &minus;1/5
</p>
<p>⎞
⎠&equiv; M&prime;.
</p>
<p>The left half of M&prime; is in triangular form. However, we want all entries above
any 1 in a column to be zero as well, i.e., we want the left-hand matrix to
be 1. We can do this by appropriate use of type 3 elementary row operations:
</p>
<p>M&prime; &minus;&minus;&minus;&minus;&minus;&minus;&rarr;
&minus;2(2)+(1)
</p>
<p>⎛
⎝
</p>
<p>1 0 3
0 1 &minus;2
0 0 1
</p>
<p>∣∣∣∣∣∣
</p>
<p>1 &minus;2 0
0 1 0
</p>
<p>2/5 &minus;3/5 &minus;1/5
</p>
<p>⎞
⎠
</p>
<p>&minus;&minus;&minus;&minus;&minus;&minus;&rarr;
&minus;3(3)+(1)
</p>
<p>⎛
⎝
</p>
<p>1 0 0
0 1 &minus;2
0 0 1
</p>
<p>∣∣∣∣∣∣
</p>
<p>&minus;1/5 &minus;1/5 3/5
0 1 0
</p>
<p>2/5 &minus;3/5 &minus;1/5
</p>
<p>⎞
⎠
</p>
<p>&minus;&minus;&minus;&minus;&minus;&rarr;
2(3)+(2)
</p>
<p>⎛
⎝
</p>
<p>1 0 0
0 1 0
0 0 1
</p>
<p>∣∣∣∣∣∣
</p>
<p>&minus;1/5 &minus;1/5 3/5
4/5 &minus;1/5 &minus;2/5
2/5 &minus;3/5 &minus;1/5
</p>
<p>⎞
⎠ .
</p>
<p>The right half of the resulting matrix is A&minus;1.
</p>
<p>Example 5.5.10 It is instructive to start with a matrix that is not invertible
and show that it is impossible to turn it into 1 by elementary row operations.
Consider the matrix
</p>
<p>B =
</p>
<p>⎛
⎝
</p>
<p>2 &minus;1 3
1 &minus;2 1
&minus;1 5 0
</p>
<p>⎞
⎠ .
</p>
<p>Let us systematically bring it into triangular form:
</p>
<p>M =
</p>
<p>⎛
⎝
</p>
<p>2 &minus;1 3
1 &minus;2 1
&minus;1 5 0
</p>
<p>∣∣∣∣∣∣
</p>
<p>1 0 0
0 1 0
0 0 1
</p>
<p>⎞
⎠&minus;&minus;&minus;&minus;&rarr;
</p>
<p>(1)&harr;(2)
</p>
<p>⎛
⎝
</p>
<p>1 &minus;2 1
2 &minus;1 3
&minus;1 5 0
</p>
<p>∣∣∣∣∣∣
</p>
<p>0 1 0
1 0 0
0 0 1
</p>
<p>⎞
⎠
</p>
<p>&minus;&minus;&minus;&minus;&minus;&minus;&rarr;
&minus;2(1)+(2)
</p>
<p>⎛
⎝
</p>
<p>1 &minus;2 1
0 3 1
&minus;1 5 0
</p>
<p>∣∣∣∣∣∣
</p>
<p>0 1 0
1 &minus;2 0
0 0 1
</p>
<p>⎞
⎠
</p>
<p>&minus;&minus;&minus;&minus;&rarr;
(1)+(3)
</p>
<p>⎛
⎝
</p>
<p>1 &minus;2 1
0 3 1
0 3 1
</p>
<p>∣∣∣∣∣∣
</p>
<p>0 1 0
1 &minus;2 0
0 1 1
</p>
<p>⎞
⎠</p>
<p/>
</div>
<div class="page"><p/>
<p>158 5 Matrices
</p>
<p>&minus;&minus;&minus;&minus;&minus;&rarr;
&minus;(2)+(3)
</p>
<p>⎛
⎝
</p>
<p>1 &minus;2 1
0 3 1
0 0 0
</p>
<p>∣∣∣∣∣∣
</p>
<p>0 1 0
1 &minus;2 0
&minus;1 3 1
</p>
<p>⎞
⎠
</p>
<p>&minus;&minus;&rarr;
1
3 (2)
</p>
<p>⎛
⎝
</p>
<p>1 &minus;2 1
0 1 1/3
0 0 0
</p>
<p>∣∣∣∣∣∣
</p>
<p>0 1 0
1/3 &minus;2/3 0
&minus;1 3 1
</p>
<p>⎞
⎠ .
</p>
<p>The matrix B is now in triangular form, but its third row contains all zeros.
There is no way we can bring this into the form of a unit matrix. We therefore
conclude that B is not invertible. This is, of course, obvious, since it can
easily be verified that B has a vanishing determinant.
</p>
<p>Rank of a Matrix
</p>
<p>Given any M&times;N matrix A, an operator TA &isin;L(VN ,WM) can be associated
with A, and one can construct the kernel and the range of TA. The rank of TA
is called the rank of A. Since the rank of an operator is basis independent,rank of a matrix
this definition makes sense.
</p>
<p>Now suppose that we choose a basis for the kernel of TA and extend
it to a basis of V. Let V1 denote the span of the remaining basis vectors.
Similarly, we choose a basis for TA(V) and extend it to a basis for W. In
these two bases, the M &times;N matrix representing TA will have all zeros ex-
cept for an r &times; r submatrix, where r is the rank of TA. The reader may
verify that this submatrix has a nonzero determinant. In fact, the submatrix
represents the isomorphism between V1 and TA(V), and, by its very con-
struction, is the largest such matrix. Since the determinant of an operator is
basis-independent, we have the following proposition.
</p>
<p>Proposition 5.5.11 The rank of a matrix is the dimension of the largest
(square) submatrix whose determinant is not zero.
</p>
<p>5.5.3 Dual Determinant Function
</p>
<p>Let V and V&lowast; be N -dimensional dual vector spaces, and let��� : VN&times;V&lowast;N &rarr;
C be a function defined by
</p>
<p>���
(
|v1〉, . . . , |vN 〉,φφφ1, . . . ,φφφN
</p>
<p>)
= det
</p>
<p>(
φφφi
</p>
<p>(
|vj 〉
</p>
<p>))
, φφφi &isin; V&lowast;, |vj 〉 &isin; V.
</p>
<p>(5.30)
By Theorem 5.5.2, ��� is a skew-symmetric linear function in |v1〉, . . . , |vN 〉
as well as in φφφ1, . . . ,φφφN . Considering the first set of arguments and taking
a nonzero determinant function ��� in V, we can write
</p>
<p>���
(
|v1〉, . . . , |vN 〉,φφφ1, . . . ,φφφN
</p>
<p>)
=��� &middot;���(φφφ1, . . . ,φφφN )︸ ︷︷ ︸
</p>
<p>&isin;C
</p>
<p>by Corollary 2.6.8. We note that ��� is a determinant function in V&lowast;. Thus,
again by Corollary 2.6.8,
</p>
<p>���(φφφ1, . . . ,φφφN )= β &middot;���&lowast;(φφφ1, . . . ,φφφN ),</p>
<p/>
</div>
<div class="page"><p/>
<p>5.5 Determinant of a Matrix 159
</p>
<p>for some nonzero determinant function ���&lowast; in V&lowast; and some β &isin;C. Combin-
ing the last two equations, we obtain
</p>
<p>���
(
|v1〉, . . . , |vN 〉,φφφ1, . . . ,φφφN
</p>
<p>)
= β���
</p>
<p>(
|v1〉, . . . , |vN 〉
</p>
<p>)
���&lowast;(φφφ1, . . . ,φφφN ).
</p>
<p>(5.31)
Now let {ǫǫǫ}Ni=1 and {|ej 〉}Nj=1 be dual bases. Then Eq. (5.30) gives
</p>
<p>���
(
|e1〉, . . . , |eN 〉,ǫǫǫ1, . . . ,ǫǫǫN
</p>
<p>)
= det(δij )= 1,
</p>
<p>and Eq. (5.31) yields
</p>
<p>1 = β���
(
|e1〉, . . . , |eN 〉
</p>
<p>)
���&lowast;(ǫǫǫ1, . . . ,ǫǫǫN ).
</p>
<p>This implies that β �= 0. Multiplying both sides of (5.30) by α &equiv; β&minus;1 and
using (5.31), we obtain
</p>
<p>Proposition 5.5.12 For any pair of nonzero determinant functions ��� and
���&lowast; in V and V&lowast;, respectively, there is a nonzero constant α &isin;C such that
</p>
<p>���
(
|v1〉, . . . , |vN 〉
</p>
<p>)
���&lowast;(φφφ1, . . . ,φφφN )= α det
</p>
<p>(
φφφi
</p>
<p>(
|vj 〉
</p>
<p>))
</p>
<p>for |vj 〉 &isin; V and φφφi &isin; V&lowast;.
</p>
<p>Definition 5.5.13 Two nonzero determinant function ��� and ���&lowast; in V and
V&lowast;, respectively, are called dual if dual determinant
</p>
<p>functions
</p>
<p>���
(
|v1〉, . . . , |vN 〉
</p>
<p>)
���&lowast;(φφφ1, . . . ,φφφN )= det
</p>
<p>(
φφφi
</p>
<p>(
|vj 〉
</p>
<p>))
.
</p>
<p>It is clear that if��� and���&lowast; are any two determinant functions, then��� and
α&minus;1���&lowast; are dual. Furthermore, if ���&lowast;1 and ���
</p>
<p>&lowast;
2 are dual to ���, then ���
</p>
<p>&lowast;
1 =���&lowast;2 ,
</p>
<p>because they both satisfy the equation of Definition 5.5.13 and��� is nonzero.
We thus have
</p>
<p>Proposition 5.5.14 Every nonzero determinant function in V has a unique
dual determinant function.
</p>
<p>Here is another way of proving the equality of the determinants of a ma-
trix and its transpose:
</p>
<p>Proposition 5.5.15 Let T&lowast; &isin; End(V&lowast;) be the dual of T &isin; End(V). Then
detT&lowast; = detT. In particular, det Tt = det T.
</p>
<p>Proof Use Definition 5.5.13 to get
</p>
<p>���
(
|v1〉, . . . , |vN 〉
</p>
<p>)
���&lowast;
</p>
<p>(
T&lowast;φφφ1, . . . ,T&lowast;φφφN
</p>
<p>)
= det
</p>
<p>(
T&lowast;φφφi
</p>
<p>(
|vj 〉
</p>
<p>))
</p>
<p>or
</p>
<p>detT&lowast; &middot;���
(
|v1〉, . . . , |vN 〉
</p>
<p>)
���&lowast;(φφφ1, . . . ,φφφN )= det
</p>
<p>(
T&lowast;φφφi
</p>
<p>(
|vj 〉
</p>
<p>))
.</p>
<p/>
</div>
<div class="page"><p/>
<p>160 5 Matrices
</p>
<p>Furthermore,
</p>
<p>���
(
T|v1〉, . . . ,T|vN 〉
</p>
<p>)
���&lowast;(φφφ1, . . . ,φφφN )= det
</p>
<p>(
φφφi
</p>
<p>(
T|vj 〉
</p>
<p>))
</p>
<p>or
</p>
<p>detT &middot;���
(
|v1〉, . . . , |vN 〉
</p>
<p>)
���&lowast;(φφφ1, . . . ,φφφN )= det
</p>
<p>(
φφφi
</p>
<p>(
T|vj 〉
</p>
<p>))
.
</p>
<p>Now noting that T&lowast;φφφi(|vj 〉)&equiv;φφφi(T|vj 〉), we obtain the equality of the deter-
minant of T and T&lowast;, and by Proposition 5.2.2, the equality of the determinant
of T and Tt . �
</p>
<p>5.6 The Trace
</p>
<p>Another intrinsic quantity associated with an operator that is usually defined
in terms of matrices is given in the following definition.
</p>
<p>Definition 5.6.1 Let A be an N &times;N matrix. The mapping tr :MN&times;N &rarr;C
(or R) given by tr A =&sum;Ni=1 αii is called the trace of A.trace of a square matrix
</p>
<p>Theorem 5.6.2 The trace is a linear mapping. Furthermore,
</p>
<p>tr(AB)= tr(BA) and tr At = tr A.
</p>
<p>Proof To prove the first identity, we use the definitions of the trace and the
matrix product:
</p>
<p>tr(AB)=
N&sum;
</p>
<p>i=1
(AB)ii =
</p>
<p>N&sum;
</p>
<p>i=1
</p>
<p>N&sum;
</p>
<p>j=1
(A)ij (B)ji =
</p>
<p>N&sum;
</p>
<p>i=1
</p>
<p>N&sum;
</p>
<p>j=1
(B)ji(A)ij
</p>
<p>=
N&sum;
</p>
<p>j=1
</p>
<p>(
N&sum;
</p>
<p>i=1
(B)ji(A)ij
</p>
<p>)
=
</p>
<p>N&sum;
</p>
<p>j=1
(BA)jj = tr(BA).
</p>
<p>The linearity of the trace and the second identity follow directly from the
definition. �
</p>
<p>Example 5.6.3 In this example, we show a very useful connection betweenconnection between
trace and determinant the trace and the determinant that holds when a matrix is only infinitesimally
</p>
<p>different from the unit matrix. Let us calculate the determinant of 1 + ǫA to
first order in ǫ. Using the definition of determinant, we write
</p>
<p>det(1 + ǫA)=
n&sum;
</p>
<p>i1,...,in=1
ǫi1...in(δ1i1 + ǫα1i1) . . . (δnin + ǫαnin)
</p>
<p>=
n&sum;
</p>
<p>i1,...,in=1
ǫi1...inδ1i1 . . . δnin
</p>
<p>+ ǫ
n&sum;
</p>
<p>k=1
</p>
<p>n&sum;
</p>
<p>i1,...,in=1
ǫi1...inδ1i1 . . . δ̂kik . . . δninαkik .</p>
<p/>
</div>
<div class="page"><p/>
<p>5.6 The Trace 161
</p>
<p>The first sum is just the product of all the Kronecker deltas. In the second
sum, δ̂kik means that in the product of the deltas, δkik is absent. This term is
obtained by multiplying the second term of the kth parentheses by the first
term of all the rest. Since we are interested only in the first power of ǫ, we
stop at this term. Now, the first sum is reduced to ǫ12...n = 1 after all the
Kronecker deltas are summed over. For the second sum, we get
</p>
<p>ǫ
</p>
<p>n&sum;
</p>
<p>k=1
</p>
<p>n&sum;
</p>
<p>i1,...,in=1
ǫi1...inδ1i1 . . . δ̂kik . . . δninαkik
</p>
<p>= ǫ
n&sum;
</p>
<p>k=1
</p>
<p>n&sum;
</p>
<p>ik=1
ǫ12...ik ...nαkik
</p>
<p>= ǫ
n&sum;
</p>
<p>k=1
ǫ12...k...nαkk = ǫ
</p>
<p>n&sum;
</p>
<p>k=1
αkk = ǫ tr A, (5.32)
</p>
<p>where the last line follows from the fact that the only nonzero value for
ǫ12...ik ...n is obtained when ik is equal to the missing index, i.e., k, in which
case it will be 1. Thus det(1 + ǫA)= 1 + ǫ tr A.
</p>
<p>Similar matrices have the same trace: If A&prime; = RAR&minus;1, then
</p>
<p>tr A&prime; = tr
(
RAR&minus;1
</p>
<p>)
= tr
</p>
<p>[
R
(
AR&minus;1
</p>
<p>)]
= tr
</p>
<p>[(
AR&minus;1
</p>
<p>)
R
]
</p>
<p>= tr
[
A
(
R&minus;1R
</p>
<p>)]
= tr(A1)= tr A.
</p>
<p>The preceding discussion is summarized in the following proposition.
</p>
<p>Proposition 5.6.4 To every operator A &isin;L(V) are associated two intrinsic
numbers, detA and trA, which are the determinant and trace of the matrix
representation of the operator in any basis of V.
</p>
<p>It follows from this proposition that the result of Example 5.6.3 can be
written in terms of operators:
</p>
<p>det(1+ ǫA)= 1 + ǫ trA. (5.33)
</p>
<p>A particularly useful formula that can be derived from this equation is the
derivative at t = 0 of an operator A(t) depending on a single variable with
the property that A(0)= 1. To first order in t , we can write A(t)= 1+ tȦ(0)
where a dot represents differentiating with respect to t . Substituting this
in Eq. (5.33) and differentiating with respect to t , we obtain the important
result
</p>
<p>d
</p>
<p>dt
det
</p>
<p>(
A(t)
</p>
<p>)∣∣∣∣
t=0
</p>
<p>= tr Ȧ(0). (5.34)
</p>
<p>Example 5.6.5 We have seen that the determinant of a product of matrices
is the product of the determinants. On the other hand, the trace of a sum relation between
</p>
<p>determinant and traceof matrices is the sum of traces. When dealing with numbers, products and</p>
<p/>
</div>
<div class="page"><p/>
<p>162 5 Matrices
</p>
<p>sums are related via the logarithm and exponential: αβ = exp{lnα + lnβ}.
A generalization of this relation exists for diagonalizable matrices, i.e., ma-
trices which can be transformed into diagonal form by a suitable similarity
transformation. Let A be such a matrix, i.e., let D = RAR&minus;1 for some simi-
larity transformation R and some diagonal matrix D = diag(λ1, λ2, . . . , λn).
The determinant of a diagonal matrix is simply the product of its elements:
</p>
<p>det D = λ1λ2 . . . λn.
</p>
<p>Taking the natural log of both sides and using the result of Example 5.2.6,
we have
</p>
<p>ln(det D)= lnλ1 + lnλ2 + &middot; &middot; &middot; + lnλn = tr(ln D),
</p>
<p>which can also be written as det D = exp{tr(ln D)}.
In terms of A, this reads det(RAR&minus;1)= exp{tr(ln(RAR&minus;1))}. Now invoke
</p>
<p>the invariance of determinant and trace under similarity transformation and
the result of Example 5.4.4 to obtain
</p>
<p>det A = exp
{
tr
(
R(ln A)R&minus;1
</p>
<p>)}
= exp
</p>
<p>{
tr(ln A)
</p>
<p>}
. (5.35)
</p>
<p>This is an important equation, which is sometimes used to define the deter-
minant of operators in infinite-dimensional vector spaces.
</p>
<p>Both the determinant and the trace are mappings from MN&times;N to C. The
determinant is not a linear mapping, but the trace is; and this opens up the
possibility of defining an inner product in the vector space of N &times;N matri-
ces in terms of the trace:
</p>
<p>Proposition 5.6.6 For any two matrices A,B &isin;MN&times;N , the mapping
</p>
<p>g :MN&times;N &times;MN&times;N &rarr;C
</p>
<p>defined by g(A,B)= tr(A&dagger;B) is a sesquilinear inner product.
</p>
<p>Proof The proof follows directly from the linearity of trace and the defini-
tion of hermitian conjugate. �
</p>
<p>Just as determinant of an operator was defined in terms of the operator
itself (see Definition 2.6.10), the trace of an operator can be defined similarly
as follows. Let ��� be a nonzero determinant function in V, and T &isin; L(V).
Define trT by
</p>
<p>N&sum;
</p>
<p>i=1
���
(
|a1〉, . . . ,T|ai〉, . . . , |aN 〉
</p>
<p>)
= (trT) &middot;���
</p>
<p>(
|a1〉, . . . , |aN 〉
</p>
<p>)
. (5.36)
</p>
<p>Then one can show that trT= tr T, for any matrix T representing T in some
basis of V. The details are left as an exercise for the reader.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.7 Problems 163
</p>
<p>5.7 Problems
</p>
<p>5.1 Show that if |c〉 = |a〉 + |b〉, then in any basis the components of |c〉
are equal to the sums of the corresponding components of |a〉 and |b〉. Also
show that the elements of the matrix representing the sum of two operators
are the sums of the elements of the matrices representing those two opera-
tors.
</p>
<p>5.2 Show that the unit operator 1 is represented by the unit matrix in any
basis.
</p>
<p>5.3 The linear operator A :R3 &rarr;R2 is given by
</p>
<p>A
</p>
<p>⎛
⎝
x
</p>
<p>y
</p>
<p>z
</p>
<p>⎞
⎠=
</p>
<p>(
2x + y &minus; 3z
x + y &minus; z
</p>
<p>)
.
</p>
<p>Construct the matrix representing A in the standard bases of R3 and R2.
</p>
<p>5.4 Find the matrix representation of the complex structure J on a real vec-
tor space V introduced in Sect. 2.4 in the basis
</p>
<p>{
|e1〉, |e2〉, . . . , |em〉, J|e1〉, J|e2〉, . . . , J|em〉
</p>
<p>}
.
</p>
<p>5.5 The linear transformation T :R3 &rarr;R3 is defined as
</p>
<p>T(x1, x2, x3)= (x1 + x2 &minus; x3,2x1 &minus; x3, x1 + 2x2).
</p>
<p>Find the matrix representation of T in
</p>
<p>(a) the standard basis of R3,
(b) the basis consisting of |a1〉 = (1,1,0), |a2〉 = (1,0,&minus;1), and |a3〉 =
</p>
<p>(0,2,3).
</p>
<p>5.6 Prove that for Eq. (5.6) to hold, we must have
</p>
<p>(
M
</p>
<p>BU
BV
</p>
<p>(B ◦A)
)
kj
=
</p>
<p>M&sum;
</p>
<p>i=1
</p>
<p>(
M
</p>
<p>BU
BW
</p>
<p>(B)
)
ki
</p>
<p>(
M
</p>
<p>BW
BV
</p>
<p>(A)
)
ij
</p>
<p>5.7 Show that the diagonal elements of an antisymmetric matrix are all zero.
</p>
<p>5.8 Show that the number of independent real parameters for an N &times;N
(a) (real) symmetric matrix is N(N + 1)/2,
(b) (real) antisymmetric matrix is N(N &minus; 1)/2,
(c) (real) orthogonal matrix is N(N &minus; 1)/2,
(d) (complex) unitary matrix is N2,
(e) (complex) hermitian matrix is N2.</p>
<p/>
</div>
<div class="page"><p/>
<p>164 5 Matrices
</p>
<p>5.9 Show that an arbitrary orthogonal 2&times; 2 matrix can be written in one of
the following two forms:
</p>
<p>(
cos θ &minus; sin θ
sin θ cos θ
</p>
<p>)
or
</p>
<p>(
cos θ sin θ
sin θ &minus; cos θ
</p>
<p>)
.
</p>
<p>The first is a pure rotation (its determinant is +1), and the second has deter-
minant &minus;1. The form of the choices is dictated by the assumption that the
first entry of the matrix reduces to 1 when θ = 0.
</p>
<p>5.10 Derive the formulas
</p>
<p>cos(θ1 + θ2)= cos θ1 cos θ2 &minus; sin θ1 sin θ2,
sin(θ1 + θ2)= sin θ1 cos θ2 + cos θ1 sin θ2
</p>
<p>by noting that the rotation of the angle θ1 + θ2 in the xy-plane is the product
of two rotations. (See Problem 5.9.)
</p>
<p>5.11 Prove that if a matrix M satisfies MM&dagger; = 0, then M = 0. Note that in
general, M2 = 0 does not imply that M is zero. Find a nonzero 2 &times; 2 matrix
whose square is zero.
</p>
<p>5.12 Construct the matrix representations of
</p>
<p>D : Pc4[t]&rarr; Pc4[t] and T : Pc3[t]&rarr; Pc4[t],
</p>
<p>the derivative and multiplication-by-t operators. Choose {1, t, t2, t3} as your
basis of Pc3[t] and {1, t, t2, t3, t4} as your basis of Pc4[t]. Use the matrix of
D so obtained to find the first, second, third, fourth, and fifth derivatives of
a general polynomial of degree 4.
</p>
<p>5.13 Find the transformation matrix R that relates the (orthonormal) stan-
dard basis of C3 to the orthonormal basis obtained from the following vec-
tors via the Gram-Schmidt process:
</p>
<p>|a1〉 =
</p>
<p>⎛
⎝
</p>
<p>1
i
</p>
<p>0
</p>
<p>⎞
⎠ , |a2〉 =
</p>
<p>⎛
⎝
</p>
<p>0
1
&minus;i
</p>
<p>⎞
⎠ , |a3〉 =
</p>
<p>⎛
⎝
</p>
<p>i
</p>
<p>0
&minus;1
</p>
<p>⎞
⎠ .
</p>
<p>Verify that R is unitary, as expected from Theorem 5.4.2.
</p>
<p>5.14 If the matrix representation of an endomorphism T of C2 with respect
to the standard basis is
</p>
<p>( 1 1
1 1
</p>
<p>)
, what is its matrix representation with respect
</p>
<p>to the basis
{( 1
</p>
<p>1
</p>
<p>)
,
( 1
&minus;1
</p>
<p>)}
?
</p>
<p>5.15 If the matrix representation of an endomorphism T of C3 with respect
to the standard basis is
</p>
<p>⎛
⎝
</p>
<p>0 1 1
1 0 &minus;1
&minus;1 &minus;1 0
</p>
<p>⎞
⎠</p>
<p/>
</div>
<div class="page"><p/>
<p>5.7 Problems 165
</p>
<p>what is the representation of T with respect to the basis
</p>
<p>⎧
⎨
⎩
</p>
<p>⎛
⎝
</p>
<p>0
1
&minus;1
</p>
<p>⎞
⎠ ,
</p>
<p>⎛
⎝
</p>
<p>1
&minus;1
1
</p>
<p>⎞
⎠ ,
</p>
<p>⎛
⎝
&minus;1
1
0
</p>
<p>⎞
⎠
⎫
⎬
⎭?
</p>
<p>5.16 Using Theorem 5.5.1, calculate the determinant of a general 3 &times; 3
matrix and obtain the familiar expansion of such a determinant in terms of
the first row of the matrix.
</p>
<p>5.17 Using Theorem 5.5.1, show that if two rows (two columns) of a matrix
are equal, then its determinant is zero.
</p>
<p>5.18 Show that det(αA)= αN det A for an N &times;N matrix A and a complex
number α.
</p>
<p>5.19 Show that det 1 = 1 for any unit matrix.
</p>
<p>5.20 Find a specific pair of matrices A and B such that det(A+B) �= det A+
det B. Therefore, the determinant is not a linear mapping. Hint: Any pair of
matrices will most likely work. In fact, the challenge is to find a pair such
that det(A + B)= det A + det B.
</p>
<p>5.21 Let A be any N &times; N matrix. Replace its ith row (column) with any
one of its other rows (columns), leaving the latter unchanged. Now expand
the determinant of the new matrix by its ith row (column) to show that
</p>
<p>N&sum;
</p>
<p>j=1
(A)ji(cof A)jk =
</p>
<p>N&sum;
</p>
<p>j=1
(A)ij (cof A)kj = 0, k �= i.
</p>
<p>5.22 Demonstrate the result of Problem 5.21 using an arbitrary 4&times;4 matrix
and evaluating the sum explicitly.
</p>
<p>5.23 Suppose that A is represented by A in one basis and by A&prime; in an-
other, related to the first by a similarity transformation R. Show directly
that det A&prime; = det A.
</p>
<p>5.24 Show explicitly that det(AB)= det A det B for 2 &times; 2 matrices.
</p>
<p>5.25 Given three N &times; N matrices A, B, and C such that AB = C with C
invertible, show that both A and B must be invertible. Thus, any two oper-
ators A and B on a finite-dimensional vector space satisfying AB = 1 are
invertible and each is the inverse of the other. Note: This is not true for
infinite-dimensional vector spaces.</p>
<p/>
</div>
<div class="page"><p/>
<p>166 5 Matrices
</p>
<p>5.26 Show directly that the similarity transformation induced by R does not
change the determinant or the trace of A where
</p>
<p>R =
</p>
<p>⎛
⎝
</p>
<p>1 2 &minus;1
0 1 &minus;2
2 1 &minus;1
</p>
<p>⎞
⎠ and A =
</p>
<p>⎛
⎝
</p>
<p>3 &minus;1 2
0 1 &minus;2
1 &minus;3 &minus;1
</p>
<p>⎞
⎠ .
</p>
<p>5.27 Find the matrix that transforms the standard basis of C3 to the vectors
</p>
<p>|a1〉 =
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>1&radic;
2
</p>
<p>1&radic;
6
</p>
<p>1+i&radic;
6
</p>
<p>⎞
⎟⎟⎟⎠ , |a2〉 =
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>&minus;i&radic;
2
</p>
<p>i&radic;
6
</p>
<p>&minus;1+i&radic;
6
</p>
<p>⎞
⎟⎟⎟⎠ , |a3〉 =
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>0
</p>
<p>&minus;2&radic;
6
</p>
<p>1+i&radic;
6
</p>
<p>⎞
⎟⎟⎟⎠ .
</p>
<p>Show that this matrix is unitary.
</p>
<p>5.28 Consider the three operators L1,L2, and L3 satisfying
</p>
<p>[L1,L2] = iL3, [L3,L1] = iL2, [L2,L3] = iL1.
</p>
<p>Show that the trace of each of these operators is necessarily zero.
</p>
<p>5.29 Show that in the expansion of the determinant given in Theorem 5.5.1,
no two elements of the same row or the same column can appear in each
term of the sum.
</p>
<p>5.30 Find the inverse of the following matrices if they exist:
</p>
<p>A =
</p>
<p>⎛
⎝
</p>
<p>3 &minus;1 2
1 0 &minus;3
&minus;2 1 &minus;1
</p>
<p>⎞
⎠ , B =
</p>
<p>⎛
⎝
</p>
<p>0 1 &minus;1
1 2 0
&minus;1 &minus;2 1
</p>
<p>⎞
⎠ ,
</p>
<p>C =
</p>
<p>⎛
⎝
</p>
<p>1 0 1
0 1 0
1 0 &minus;1
</p>
<p>⎞
⎠ .
</p>
<p>5.31 Find inverses for the following matrices using both methods discussed
in this chapter.
</p>
<p>A =
</p>
<p>⎛
⎝
</p>
<p>2 1 &minus;1
2 1 2
&minus;1 &minus;2 &minus;2
</p>
<p>⎞
⎠ , B =
</p>
<p>⎛
⎝
</p>
<p>1 2 &minus;1
0 1 &minus;2
2 1 &minus;1
</p>
<p>⎞
⎠ ,
</p>
<p>C =
</p>
<p>⎛
⎝
</p>
<p>1 &minus;1 1
&minus;1 1 1
1 &minus;1 &minus;2
</p>
<p>⎞
⎠ ,
</p>
<p>D =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>1/
&radic;
</p>
<p>2 0 (1 &minus; i)/(2
&radic;
</p>
<p>2) (1 + i)/(2
&radic;
</p>
<p>2)
0 1/
</p>
<p>&radic;
2 (1 &minus; i)/(2
</p>
<p>&radic;
2) &minus;(1 + i)/(2
</p>
<p>&radic;
2)
</p>
<p>1/
&radic;
</p>
<p>2 0 &minus;(1 &minus; i)/(2
&radic;
</p>
<p>2) &minus;(1 + i)/(2
&radic;
</p>
<p>2)
0 1/
</p>
<p>&radic;
2 &minus;(1 &minus; i)/(2
</p>
<p>&radic;
2) (1 + i)/(2
</p>
<p>&radic;
2)
</p>
<p>⎞
⎟⎟⎠ .</p>
<p/>
</div>
<div class="page"><p/>
<p>5.7 Problems 167
</p>
<p>5.32 Let A be an operator on V. Show that if detA= 0, then there exists a
nonzero vector |x〉 &isin; V such that A|x〉 = 0.
</p>
<p>5.33 For which values of α are the following matrices invertible? Find the
inverses whenever they exist.
</p>
<p>A =
</p>
<p>⎛
⎝
</p>
<p>1 α 0
α 1 α
0 α 1
</p>
<p>⎞
⎠ , B =
</p>
<p>⎛
⎝
α 1 0
1 α 1
0 1 α
</p>
<p>⎞
⎠ ,
</p>
<p>C =
</p>
<p>⎛
⎝
</p>
<p>0 1 α
1 α 0
α 0 1
</p>
<p>⎞
⎠ , D =
</p>
<p>⎛
⎝
</p>
<p>1 1 1
1 1 α
1 α 1
</p>
<p>⎞
⎠ .
</p>
<p>5.34 Let {ai}Ni=1, be the set consisting of the N rows of an N &times;N matrix A
and assume that the ai are orthogonal to each other. Show that
</p>
<p>|det A| = ‖a1‖ ‖a2‖ &middot; &middot; &middot; ‖aN‖.
</p>
<p>Hint: Consider AA&dagger;. What would the result be if A were a unitary matrix?
</p>
<p>5.35 Prove that a set of n homogeneous linear equations in n unknowns has
a nontrivial solution if and only if the determinant of the matrix of coeffi-
cients is zero.
</p>
<p>5.36 Use determinants to show that an antisymmetric matrix whose dimen-
sion is odd cannot have an inverse.
</p>
<p>5.37 Let V be a real inner product space. Let ��� : VN &times;VN &rarr;R be a func-
tion defined by
</p>
<p>���
(
|v1〉, . . . , |vN 〉, |u1〉, . . . , |uN 〉
</p>
<p>)
= det
</p>
<p>(
〈ui |vj 〉
</p>
<p>)
.
</p>
<p>Follow the same procedure as in Sect. 5.5.3 to show that for any determinant
function ��� in V there is a nonzero constant α &isin;R such that
</p>
<p>���
(
|v1〉, . . . , |vN 〉
</p>
<p>)
���
(
|u1〉, . . . , |uN 〉
</p>
<p>)
= α det
</p>
<p>(
〈ui |vj 〉
</p>
<p>)
</p>
<p>for |ui〉, |vj 〉 &isin; V.
</p>
<p>5.38 Show that tr(|a〉〈b|) = 〈b|a〉. Hint: Evaluate the trace in an orthonor-
mal basis.
</p>
<p>5.39 Show that if two invertible N&times;N matrices A and B anticommute (that
is, AB + BA = 0), then (a) N must be even, and (b) tr A = tr B = 0.
</p>
<p>5.40 Show that for a spatial rotation Rn̂(θ) of an angle θ about an arbitrary
axis n̂, trRn̂(θ)= 1 + 2 cos θ .
</p>
<p>5.41 Express the sum of the squares of elements of a matrix as a trace. Show
that this sum is invariant under an orthogonal transformation of the matrix.</p>
<p/>
</div>
<div class="page"><p/>
<p>168 5 Matrices
</p>
<p>5.42 Let S and A be a symmetric and an antisymmetric matrix, respectively,
and let M be a general matrix. Show that
</p>
<p>(a) tr M = tr Mt ,
(b) tr(SA)= 0; in particular, tr A = 0,
(c) SA is antisymmetric if and only if [S,A] = 0,
(d) MSMt is symmetric and MAMt is antisymmetric,
(e) MHM&dagger; is hermitian if H is.
</p>
<p>5.43 Find the trace of each of the following linear operators:
</p>
<p>(a) T :R3 &rarr;R3 given by
</p>
<p>T(x, y, z)= (x + y &minus; z,2x + 3y &minus; 2z, x &minus; y).
</p>
<p>(b) T :R3 &rarr;R3 given by
</p>
<p>T(x, y, z)= (y &minus; z, x + 2y + z, z&minus; y).
</p>
<p>(c) T :C4 &rarr;C4 given by
</p>
<p>T(x, y, z,w)= (x+ iy&minus; z+ iw,2ix+3y&minus;2iz&minus;w,x&minus; iy, z+ iw).
</p>
<p>5.44 Use Eq. (5.35) to derive Eq. (5.33).
</p>
<p>5.45 Suppose that there are two operators A and B such that [A,B] = c1,
where c is a constant. Show that the vector space in which such operators
are defined cannot be finite-dimensional. Conclude that the position and mo-
mentum operators of quantum mechanics can be defined only in infinite di-
mensions.
</p>
<p>5.46 Use Eq. (5.36) to show that trT= tr T, for any matrix T representing T
in some basis of V.</p>
<p/>
</div>
<div class="page"><p/>
<p>6Spectral Decomposition
</p>
<p>The last chapter discussed matrix representation of operators. It was pointed
out there that such a representation is basis-dependent. In some bases, the
operator may &ldquo;look&rdquo; quite complicated, while in others it may take a simple
form. In a &ldquo;special&rdquo; basis, the operator may look the simplest: It may be a
diagonal matrix. This chapter investigates conditions under which a basis
exists in which the operator is represented by a diagonal matrix.
</p>
<p>6.1 Invariant Subspaces
</p>
<p>We start by recalling the notion of the direct sum of more than two subspaces
and assume that
</p>
<p>V=U1 &oplus;U2 &oplus; &middot; &middot; &middot; &oplus;Ur &equiv;
r&oplus;
</p>
<p>j=1
Uj . (6.1)
</p>
<p>Then by Proposition 4.4.1, there exist idempotents {Pj }rj=1 such that
</p>
<p>PiPj = δijPi (no sum) and
r&sum;
</p>
<p>j=1
Pj = 1. (6.2)
</p>
<p>Definition 6.1.1 Let V be an inner product space. Let M be any subspace
of V. Denote by M&perp; the set of all vectors in V orthogonal to all the vectors
in M. M&perp; (pronounced &ldquo;em perp&rdquo;) is called the orthogonal complement
of M.
</p>
<p>orthogonal complement
</p>
<p>of a subspace
Proposition 6.1.2 M&perp; is a subspace of V.
</p>
<p>Proof The straightforward proof is left as an exercise for the reader. �
</p>
<p>If V of Eq. (6.1) is an inner product space, and the subspaces are mutually
when projection
</p>
<p>operators become
</p>
<p>hermitian
</p>
<p>orthogonal, then for arbitrary |u〉, |v〉 &isin; V,
</p>
<p>〈u|Pj |v〉 = 〈u|vj 〉 = 〈uj |vj 〉 = 〈vj |uj 〉&lowast; = 〈v|uj 〉&lowast; = 〈v|Pj |u〉&lowast;
</p>
<p>S. Hassani, Mathematical Physics, DOI 10.1007/978-3-319-01195-0_6,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>169</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-01195-0_6">http://dx.doi.org/10.1007/978-3-319-01195-0_6</a></div>
</div>
<div class="page"><p/>
<p>170 6 Spectral Decomposition
</p>
<p>which shows that Pj is hermitian.
Consider an orthonormal basis BM = {|ei〉}mi=1 for M, and extend it to
</p>
<p>a basis B = {|ei〉}Ni=1 for V. Now construct a (hermitian) projection opera-
tor P =&sum;Mi=1 |ei〉〈ei |. This is the operator that projects an arbitrary vector
in V onto the subspace M. It is straightforward to show that 1 &minus; P is the
projection operator that projects onto M&perp; (see Problem 6.1).
</p>
<p>An arbitrary vector |a〉 &isin; V can be written as
</p>
<p>|a〉 = (P+ 1&minus; P)|a〉 = P|a〉︸︷︷︸
in M
</p>
<p>+ (1&minus; P)|a〉︸ ︷︷ ︸
in M&perp;
</p>
<p>.
</p>
<p>Furthermore, the only vector that can be in both M and M&perp; is the zero
vector, because it is the only vector orthogonal to itself. We thus have
</p>
<p>Proposition 6.1.3 If V is an inner product space, then V = M&oplus;M&perp; for
any subspace M. Furthermore, the projection operators corresponding to
M and M&perp; are hermitian.
</p>
<p>This section explores the possibility of obtaining subspaces by means
of the action of a linear operator on vectors of an N -dimensional vector
space V. Let |a〉 be any vector in V, and A a linear operator on V. The
vectors
</p>
<p>|a〉,A|a〉,A2|a〉, . . . ,AN |a〉
are linearly dependent (there are N + 1 of them). Let M&equiv; Span{Ak|a〉}Nk=0.
It follows that, m &equiv; dimM &le; dimV, and M has the property that for any
vector |x〉 &isin; M the vector A|x〉 also belongs to M (show this!). In other
words, no vector in M &ldquo;leaves&rdquo; the subspace when acted on by A.
</p>
<p>Definition 6.1.4 A subspace M is an invariant subspace of the operator
invariant subspace;
</p>
<p>reduction of an operator
A if A transforms vectors of M into vectors of M. This is written succinctly
as A(M) &sub;M. We say that M reduces A if both M and M&perp; are invariant
subspaces of A.
</p>
<p>Starting with a basis of M, we can extend it to a basis B = {|ai〉}Ni=1 of V
whose first m vectors span M. The matrix representation of A in such a ba-
sis is given by the relation A|ai〉 =
</p>
<p>&sum;N
j=1 αji |aj 〉, i = 1,2, . . . ,N . If i &le;m,
</p>
<p>then αji = 0 for j &gt; m, because A|ai〉 belongs to M when i &le;m and there-
fore can be written as a linear combination of only {|a1〉, |a2〉, . . . , |am〉}.
Thus, the matrix representation of A in B will have the form
</p>
<p>A =
(
</p>
<p>A11 A12
</p>
<p>021 A22
</p>
<p>)
,
</p>
<p>where A11 is an m&times;m matrix, A12 an m&times; (N &minus;m) matrix, 021 the (N &minus;
matrix representation of
</p>
<p>an operator in a
</p>
<p>subspace
</p>
<p>m)&times;m zero matrix, and A22 an (N &minus;m)&times; (N &minus;m) matrix. We say that
A11 represents the operator A in the m-dimensional subspace M.
</p>
<p>It may also happen that the subspace spanned by the remaining basis
vectors in B , namely |am+1〉, |am+2〉, . . . , |aN 〉, is also an invariant subspace</p>
<p/>
</div>
<div class="page"><p/>
<p>6.1 Invariant Subspaces 171
</p>
<p>of A. Then A12 will be zero, and A will take a block diagonal form:1
block diagonal matrix
</p>
<p>defined
A =
</p>
<p>(
A11 0
</p>
<p>0 A22
</p>
<p>)
.
</p>
<p>If a matrix representing an operator can be brought into this form by a
suitable choice of basis, it is called reducible; otherwise, it is called ir-
</p>
<p>reducible and irreducible
</p>
<p>matrices
reducible. A reducible matrix A is denoted in two different ways:2
</p>
<p>A =
(
</p>
<p>A1 0
</p>
<p>0 A2
</p>
<p>)
&hArr; A = A1 &oplus; A2. (6.3)
</p>
<p>For example, when M reduces A and one chooses a basis the first m vectors
of which are in M and the remaining ones in M&perp;, then A is reducible.
</p>
<p>We have seen on a number of occasions the significance of the hermitian
conjugate of an operator (e.g., in relation to hermitian and unitary operators).
The importance of this operator will be borne out further when we study the
spectral theorem later in this chapter. Let us now investigate some properties
of the adjoint of an operator in the context of invariant subspaces.
</p>
<p>Lemma 6.1.5 A subspaceM of an inner product space V is invariant under
condition for invariance
</p>
<p>the linear operator A if and only if M&perp; is invariant under A&dagger;.
</p>
<p>Proof The proof is left as a problem. �
</p>
<p>An immediate consequence of the above lemma and the two identities
(A&dagger;)&dagger; = A and (M&perp;)&perp; =M is contained in the following theorem.
</p>
<p>Theorem 6.1.6 A subspace of V reduces A if and only if it is invariant
under both A and A&dagger;.
</p>
<p>Lemma 6.1.7 Let M be a subspace of V and P the hermitian projection
operator onto M. Then M is invariant under the linear operator A if and
only if AP= PAP.
</p>
<p>Proof Suppose M is invariant. Then for any |x〉 in V, we have
</p>
<p>P|x〉 &isin;M &rArr; AP|x〉 &isin;M &rArr; PAP|x〉 = AP|x〉.
</p>
<p>Since the last equality holds for arbitrary |x〉, we have AP= PAP.
Conversely, suppose AP= PAP. For any |y〉 &isin;M, we have
</p>
<p>P|y〉 = |y〉 &rArr; AP︸︷︷︸
=PAP
</p>
<p>|y〉 = A|y〉 = P
(
AP|y〉
</p>
<p>)
&isin;M.
</p>
<p>Therefore, M is invariant under A. �
</p>
<p>1From now on, we shall denote all zero matrices by the same symbol regardless of their
dimensionality.
2It is common to use a single subscript for submatrices of a block diagonal matrix, just as
it is common to use a single subscript for entries of a diagonal matrix.</p>
<p/>
</div>
<div class="page"><p/>
<p>172 6 Spectral Decomposition
</p>
<p>Theorem 6.1.8 Let M be a subspace of V, P the hermitian projection op-
erator of V ontoM, and A a linear operator on V. ThenM reduces A if and
only if A and P commute.
</p>
<p>Proof Suppose M reduces A. Then by Theorem 6.1.6, M is invariant under
both A and A&dagger;. Lemma 6.1.7 then implies
</p>
<p>AP= PAP and A&dagger;P= PA&dagger;P. (6.4)
</p>
<p>Taking the adjoint of the second equation yields (A&dagger;P)&dagger; = (PA&dagger;P)&dagger;, or PA=
PAP. This equation together with the first equation of (6.4) yields PA= AP.
</p>
<p>Conversely, suppose that PA = AP. Then P2A = PAP, whence PA =
PAP. Taking adjoints gives A&dagger;P = PA&dagger;P, because P is hermitian. By
Lemma 6.1.7, M is invariant under A&dagger;. Similarly, from PA = AP, we get
PAP= AP2, whence PAP= AP. Once again by Lemma 6.1.7, M is invari-
ant under A. By Theorem 6.1.6, M reduces A. �
</p>
<p>6.2 Eigenvalues and Eigenvectors
</p>
<p>The main goal of the remaining part of this chapter is to prove that certain
kinds of operators, for example a hermitian operator, is diagonalizable, that
is, that we can always find an (orthonormal) basis in which it is represented
by a diagonal matrix.
</p>
<p>Let us begin by considering eigenvalues and eigenvectors, which are gen-
eralizations of familiar concepts in two and three dimensions. Consider the
operation of rotation about the z-axis by an angle θ denoted by Rz(θ).
Such a rotation takes any vector (x, y) in the xy-plane to a new vector
(x cos θ &minus; y sin θ, x sin θ + y cos θ). Thus, unless (x, y) = (0,0) or θ is an
integer multiple of 2π , the vector will change. Is there a nonzero vector that
is so special (eigen, in German) that it does not change when acted on by
Rz(θ)? As long as we confine ourselves to two dimensions, the answer is no.
But if we lift ourselves up from the two-dimensional xy-plane, we encounter
many such vectors, all of which lie along the z-axis.
</p>
<p>The foregoing example can be generalized to any rotation (normally
specified by Euler angles). In fact, the methods developed in this section
can be used to show that a general rotation, given by Euler angles, always
has an unchanged vector lying along the axis around which the rotation takes
place. This concept is further generalized in the following definition.
</p>
<p>Definition 6.2.1 Let A &isin; End(V) be a linear transformation, and |a〉 a
nonzero vector such that
</p>
<p>eigenvector and
</p>
<p>eigenvalue A|a〉 = λ|a〉, (6.5)
</p>
<p>with λ &isin;C. We then say that |a〉 is an eigenvector of A with eigenvalue λ.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.2 Eigenvalues and Eigenvectors 173
</p>
<p>Proposition 6.2.2 Add the zero vector to the set of all eigenvectors of A
belonging to the same eigenvalue λ, and denote the span of the resulting set
by Mλ. Then Mλ is a subspace of V, and every (nonzero) vector in Mλ is
an eigenvector of A with eigenvalue λ.
</p>
<p>Proof The proof follows immediately from the above definition and the def-
inition of a subspace. �
</p>
<p>Definition 6.2.3 The subspace Mλ is referred to as the eigenspace of A eigenspace
corresponding to the eigenvalue λ. Its dimension is called the geometric
multiplicity of λ. An eigenvalue is called simple if its geometric multiplic-
ity is 1. The set of eigenvalues of A is called the spectrum of A. spectrum
</p>
<p>By their very construction, eigenspaces corresponding to different eigen-
values have no vectors in common except the zero vector. This can be
demonstrated by noting that if |v〉 &isin;Mλ &cap;Mμ for λ �= μ, then
</p>
<p>0 = (A&minus; λ1)|v〉 = A|v〉&minus; λ|v〉 = μ|v〉&minus; λ|v〉 = (μ&minus; λ)︸ ︷︷ ︸
�=0
</p>
<p>|v〉 &rArr; |v〉 = 0.
</p>
<p>An immediate consequence of this fact is
</p>
<p>Mλ +Mμ =Mλ &oplus;Mμ
</p>
<p>if λ �= μ.
More generally,
</p>
<p>Proposition 6.2.4 If {λi}ri=1 are distinct eigenvalues of an operator A and
Mi is the eigenspace corresponding to λi , them
</p>
<p>M1 + &middot; &middot; &middot; +Mr =M1 &oplus; &middot; &middot; &middot; &oplus;Mr &equiv;
r&oplus;
</p>
<p>i=1
Mi . (6.6)
</p>
<p>In particular, by Proposition 2.1.15, the eigenvectors of A corresponding to
distinct eigenvalues are linearly independent.
</p>
<p>Let us rewrite Eq. (6.5) as (A&minus; λ1)|a〉 = 0. This equation says that |a〉
is an eigenvector of A if and only if |a〉 belongs to the kernel of A &minus; λ1.
If the latter is invertible, then its kernel will consist of only the zero vector,
which is not acceptable as a solution of Eq. (6.5). Thus, if we are to obtain
nontrivial solutions, A&minus; λ1 must have no inverse. This is true if and only if
</p>
<p>det(A&minus; λ1)= 0. (6.7)
</p>
<p>The determinant in Eq. (6.7) is a polynomial in λ, called the characteris-
tic polynomial of A. The roots of this polynomial are called characteristic characteristic
</p>
<p>polynomial and
</p>
<p>characteristic roots of an
</p>
<p>operator
</p>
<p>roots and are simply the eigenvalues of A. Now, any polynomial of degree
greater than or equal to 1 has at least one (complex) root. This yields the
following theorem.</p>
<p/>
</div>
<div class="page"><p/>
<p>174 6 Spectral Decomposition
</p>
<p>Theorem 6.2.5 Every operator on a finite-dimensional vector space over
C has at least one eigenvalue and therefore at least one eigenvector.
</p>
<p>Let λ1, λ2, . . . , λp be the distinct roots of the characteristic polynomial of
A, and let λj occur mj times. Then mj is called the algebraic multiplicityalgebraic multiplicity
of λj , and
</p>
<p>det(A&minus; λ1)= (λ1 &minus; λ)m1 &middot; &middot; &middot; (λp &minus; λ)mp =
p&prod;
</p>
<p>j=1
(λj &minus; λ)mj . (6.8)
</p>
<p>For λ= 0, this gives
</p>
<p>detA= λm11 λ
m2
2 &middot; &middot; &middot;λ
</p>
<p>mp
p =
</p>
<p>p&prod;
</p>
<p>j=1
λ
mj
j . (6.9)
</p>
<p>Equation (6.9) states that the determinant of an operator is the product ofdeterminant and
eigenvalues all its eigenvalues. In particular,
</p>
<p>Proposition 6.2.6 An operator is invertible iff none of its eigenvalues is
zero.
</p>
<p>Example 6.2.7 Let us find the eigenvalues of a projection operator P. Ifeigenvalues of a
projection operator |a〉 is an eigenvector, then P|a〉 = λ|a〉. Applying P on both sides again, we
</p>
<p>obtain
</p>
<p>P2|a〉 = λP|a〉 = λ
(
λ|a〉
</p>
<p>)
= λ2|a〉.
</p>
<p>But P2 = P; thus, P|a〉 = λ2|a〉. It follows that λ2|a〉 = λ|a〉, or (λ2 &minus;
λ)|a〉 = 0. Since |a〉 �= 0, we must have λ(λ &minus; 1) = 0, or λ = 0,1. Thus,
the only eigenvalues of a projection operator are 0 and 1. The presence of
zero as an eigenvalue of P is an indication that P is not invertible.
</p>
<p>Example 6.2.8 To be able to see the difference between algebraic and ge-
ometric multiplicities, consider the matrix A =
</p>
<p>( 1 1
0 1
</p>
<p>)
, whose characteristic
</p>
<p>polynomial is (1 &minus; λ)2. Thus, the matrix has only one eigenvalue, λ = 1,
with algebraic multiplicity m1 = 2. However, the most general vector |a〉
satisfying (A&minus; 1)|a〉 = 0 is easily shown to be of the form
</p>
<p>( α
0
</p>
<p>)
. This shows
</p>
<p>that Mλ=1 is one-dimensional, i.e., the geometric multiplicity of λ is 1.
</p>
<p>As mentioned at the beginning of this chapter, it is useful to represent an
operator by a diagonal matrix. This motivates the following definition:
</p>
<p>Definition 6.2.9 A linear operator A on a vector space V is said to be di-diagonalizable operators
agonalizable if there is a basis for V all of whose vectors are eigenvectors
of A.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.3 Upper-Triangular Representations 175
</p>
<p>Theorem 6.2.10 Let A be a diagonalizable operator on a vector space V
with distinct eigenvalues {λj }rj=1. Then there are idempotents Pj on V such
that
</p>
<p>(1) 1=
r&sum;
</p>
<p>j=1
Pj , (2) PiPj = 0 for i �= j, (3) A=
</p>
<p>r&sum;
</p>
<p>j=1
λjPj .
</p>
<p>Proof Let Mj denote the eigenspace corresponding to the eigenvalue λj .
Since the eigenvectors span V, by Proposition 6.2.4 we have
</p>
<p>V=M1 &oplus;M2 &oplus; &middot; &middot; &middot; &oplus;Mr .
</p>
<p>This immediately gives (1) and (2) if we use Eqs. (6.1) and (6.2) where Pj
is the projection operator onto Mj .
</p>
<p>To prove (3), let |v〉 be an arbitrary vector in V. Then |v〉 can be written
uniquely as a sum of vectors each coming from one eigenspace. Therefore,
</p>
<p>A|v〉 =
r&sum;
</p>
<p>j=1
A|vj 〉 =
</p>
<p>r&sum;
</p>
<p>j=1
λj |vj 〉 =
</p>
<p>(
r&sum;
</p>
<p>j=1
λjPj
</p>
<p>)
|v〉.
</p>
<p>Since this equality holds for all vectors |v〉, (3) follows. �
</p>
<p>6.3 Upper-Triangular Representations
</p>
<p>Let T &isin; End(V) and {|ai〉}Ni=1 a basis of V. Suppose that Span{|ai〉}mi=1 is
invariant under T for m= 1, . . . ,N , i.e.,
</p>
<p>T
(
Span
</p>
<p>{
|ai〉
</p>
<p>}m
i=1
</p>
<p>)
&sube; Span
</p>
<p>{
|ai〉
</p>
<p>}m
i=1 for each m= 1,2, . . . ,N. (6.10)
</p>
<p>Consider the N &times; N matrix representing T in this basis. Since T|a1〉 &isin;
Span{|a1〉}, all the elements of the first column of this matrix except pos-
sibly the first are zero. Since T|a2〉 &isin; Span{|a1〉, |a2〉}, all the elements of
the second column except possibly the first two are zero. And in general all
the elements of the ith column except possibly the first i elements are zero. upper-triangular
</p>
<p>representationThus the matrix representing T is upper-triangular.
Expanding the determinant of the upper-triangular matrix above by its
</p>
<p>first column and continuing the same process for the cofactors, we see that
detT is simply the product of the elements on the main diagonal. Further-
more, T &minus; λ1 is also an upper-triangular matrix whose diagonal elements
are of the form λi &minus; λ, where λi are the diagonal elements of T. Hence,
</p>
<p>det(T&minus; λ1)= (λ1 &minus; λ) &middot; &middot; &middot; (λN &minus; λ),
</p>
<p>and we have the following:
</p>
<p>Proposition 6.3.1 The operator T is invertible iff its upper-triangular ma-
trix representation has no zero on its main diagonal. The entries on the main
diagonal are simply the eigenvalues of T.</p>
<p/>
</div>
<div class="page"><p/>
<p>176 6 Spectral Decomposition
</p>
<p>As the foregoing discussion shows, upper-triangular representations of an
operator seem to be convenient. But do they exist? In other words, can we
find a basis of V in which an operator T is represented by an upper-triangular
matrix? For the case of a complex vector space the answer is &lsquo;yes,&rsquo; as the
following theorem demonstrates.
</p>
<p>Theorem 6.3.2 Let V be a complex vector space of dimension N and T &isin;
End(V). Then there exists a basis of V in which T is represented by an upper-
triangular matrix.
</p>
<p>Proof We prove the theorem by induction on the dimension of subspaces of
V. For a one-dimensional subspace U, Theorem 6.2.5 guarantees the exis-
tence of a vector |u〉&mdash;the eigenvector of T&mdash;for which Eq. (6.10) holds. Let
U= Span{|u〉} and write
</p>
<p>V=U&oplus;W,
which is possible by Proposition 2.1.16. Let TU and TW be as in Eq. (4.13).
Since TW &isin; End(W) and dimW=N &minus; 1, we can use the induction hypoth-
esis on TW and assume that there exists a basis BW = {|ai〉}N&minus;1i=1 of W, such
that
</p>
<p>TW |ai〉 &isin; Span
{
|a1〉, |a2〉, . . . , |ai〉
</p>
<p>}
for each i = 1,2, . . . ,N &minus; 1.
</p>
<p>Now consider the basis BV = {|u〉, |a1〉, . . . , |aN&minus;1〉}. Then
</p>
<p>T|u〉 = TU |u〉 + TW |u〉 = PUT|u〉 + PWT|u〉
= PU
</p>
<p>(
λ|u〉
</p>
<p>)
+ PW
</p>
<p>(
λ|u〉
</p>
<p>)
︸ ︷︷ ︸
</p>
<p>=|0〉
</p>
<p>= λ|u〉 &isin; Span
{
|u〉
</p>
<p>}
</p>
<p>and
</p>
<p>T|ai〉 = TU |ai〉 + TW |ai〉 = PU
(
T|ai〉
</p>
<p>)
︸ ︷︷ ︸
</p>
<p>&isin;U
</p>
<p>+TW |ai〉
</p>
<p>= α|u〉 +
i&sum;
</p>
<p>k=1
αk|ak〉 &isin; Span
</p>
<p>{
|u〉, |a1〉, . . . , |ai〉
</p>
<p>}
,
</p>
<p>where we used the fact that TW |ai〉 &isin; Span{|ak〉}ik=1. We thus have found a
basis BV for which Eq. (6.10) holds. This completes the proof. �
</p>
<p>The ideal goal of the representation of an operator is to have it in diag-
onal form with its eigenvalues along the diagonal. Theorem 6.3.2 partially
accomplished this for complex vector spaces: it made the lower half of the
representing matrix all zeros. In doing so, it used the algebraic closure of C,
i.e., the fact that any polynomial with coefficients in C has all its roots in C.
To make the upper half also zero, additional properties will be required for
the operator, as we&rsquo;ll see in Sect. 6.4. Thus, for a general operator on a
complex vector space, upper-triangular representation is the best we can ac-
complish. The case of the real vector spaces is even more restrictive as we
shall see in Sect. 6.6.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.4 Complex Spectral Decomposition 177
</p>
<p>6.4 Complex Spectral Decomposition
</p>
<p>This section derives one of the most powerful theorems in the theory of
linear operators, the spectral decomposition theorem. We shall derive the
theorem for operators that generalize hermitian and unitary operators.
</p>
<p>Definition 6.4.1 A normal operator is an operator on an inner product normal operator defined
space that commutes with its adjoint.
</p>
<p>An important consequence of this definition is
</p>
<p>Proposition 6.4.2 The operator A &isin; End(V) satisfies
</p>
<p>‖Ax‖ =
∥∥A&dagger;x
</p>
<p>∥∥ for all |x〉 &isin; V (6.11)
</p>
<p>if and only if A is normal.
</p>
<p>Theorem 6.4.3 Let A be a normal operator on V and U a subspace of V in-
variant under A. Then U is invariant under A&dagger;. Therefore by Theorem 6.1.6,
any invariant subspace of a normal operator reduces it.
</p>
<p>Proof Let {|ei〉}mi=1 be an orthonormal basis of U, and extend it to get
{|ei〉}Ni=1, an orthonormal basis of V. Since U is invariant under A, we can
write
</p>
<p>A|ei〉 =
m&sum;
</p>
<p>j=1
αji |ej 〉, αji = 〈ej |A|ei〉
</p>
<p>and
</p>
<p>A&dagger;|ei〉 =
m&sum;
</p>
<p>j=1
ηji |ej 〉 +
</p>
<p>N&sum;
</p>
<p>j=m+1
ξji |ej 〉,
</p>
<p>where for j = 1,2, . . . ,m, we have
</p>
<p>ηji = 〈ej |A&dagger;|ei〉 = 〈ei |A|ej 〉&lowast; = α&lowast;ij .
</p>
<p>Now note that
</p>
<p>〈ei |A&dagger;A|ei〉 =
m&sum;
</p>
<p>j=1
|αij |2
</p>
<p>while
</p>
<p>〈ei |AA&dagger;|ei〉 =
m&sum;
</p>
<p>j=1
|ηij |2 +
</p>
<p>N&sum;
</p>
<p>j=m+1
|ξij |2 =
</p>
<p>m&sum;
</p>
<p>j=1
|αij |2 +
</p>
<p>N&sum;
</p>
<p>j=m+1
|ξij |2.
</p>
<p>Since A&dagger;A= AA&dagger;, we must have
m&sum;
</p>
<p>j=1
|αij |2 =
</p>
<p>m&sum;
</p>
<p>j=1
|αij |2 +
</p>
<p>N&sum;
</p>
<p>j=m+1
|ξij |2</p>
<p/>
</div>
<div class="page"><p/>
<p>178 6 Spectral Decomposition
</p>
<p>or
</p>
<p>N&sum;
</p>
<p>j=m+1
|ξij |2 = 0 &rArr; ξij = 0 for all i, j =m+ 1, . . . ,N.
</p>
<p>This implies that A&dagger; sends every basis vector of U back to U, and therefore
it does the same for every vector of U. �
</p>
<p>Proposition 6.4.4 Let A be a normal operator on V. Then |x〉 is an eigen-
vector of A with eigenvalue λ if and only if |x〉 is an eigenvector of A&dagger; with
eigenvalue λ&lowast;.
</p>
<p>Proof By Proposition 6.4.2, the fact that (A &minus; λ1)&dagger; = A&dagger; &minus; λ&lowast;1, and the
fact that A&minus; λ1 is normal (reader, verify), we have ‖(A&minus; λ1)x‖ = 0 if and
only if ‖(A&dagger; &minus; λ&lowast;1)x‖ = 0. Since it is only the zero vector that has the zero
norm, we get
</p>
<p>(A&minus; λ1)|x〉 = 0 if and only if
(
A&dagger; &minus; λ&lowast;1
</p>
<p>)
|x〉 = 0.
</p>
<p>This proves the proposition. �
</p>
<p>We obtain a useful consequence of this proposition by applying it to a
hermitian operator H and a unitary operator3 U. In the first case, we get
</p>
<p>H|x〉 = λ|x〉 = H&dagger;|x〉 = λ&lowast;|x〉 &rArr;
(
λ&minus; λ&lowast;
</p>
<p>)
|x〉 = 0 &rArr; λ= λ&lowast;.
</p>
<p>Therefore, λ is real. In the second case, we write
</p>
<p>|x〉 = 1|x〉 = UU&dagger;|x〉 = U
(
λ&lowast;|x〉
</p>
<p>)
= λ&lowast;U|x〉 = λ&lowast;λ|x〉 &rArr; λ&lowast;λ= 1.
</p>
<p>Therefore, λ is unimodular (has absolute value equal to 1). We summarize
the foregoing discussion:
</p>
<p>Corollary 6.4.5 The eigenvalues of a hermitian operator are real.
A unitary operator has eigenvalues whose absolute values are 1.
</p>
<p>Example 6.4.6 Let us find the eigenvalues and eigenvectors of the hermi-
tian matrix H =
</p>
<p>( 0 &minus;i
i 0
</p>
<p>)
. We have
</p>
<p>det(H&minus; λ1)= det
(&minus;λ &minus;i
</p>
<p>i &minus;λ
</p>
<p>)
= λ2 &minus; 1.
</p>
<p>Thus, the eigenvalues, λ1 = 1 and λ2 =&minus;1, are real, as expected.
</p>
<p>3Obviously, both are normal operators.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.4 Complex Spectral Decomposition 179
</p>
<p>To find the eigenvectors, we write
</p>
<p>0 = (H&minus; λ11)|a1〉 = (H&minus; 1)|a1〉 =
(&minus;1 &minus;i
</p>
<p>i &minus;1
</p>
<p>)(
α1
α2
</p>
<p>)
=
(&minus;α1 &minus; iα2
</p>
<p>iα1 &minus; α2
</p>
<p>)
,
</p>
<p>or α2 = iα1, which gives |a1〉 =
( α1
iα1
</p>
<p>)
= α1
</p>
<p>( 1
i
</p>
<p>)
, where α1 is an arbitrary
</p>
<p>complex number. Also,
</p>
<p>0 = (H&minus; λ21)|a2〉 = (H+ 1)|a2〉 =
(
</p>
<p>1 &minus;i
i 1
</p>
<p>)(
β1
β2
</p>
<p>)
=
(
β1 &minus; iβ2
iβ1 + β2
</p>
<p>)
,
</p>
<p>or β2 =&minus;iβ1, which gives |a2〉 =
( β1
&minus;iβ1
</p>
<p>)
= β1
</p>
<p>( 1
&minus;i
</p>
<p>)
, where β1 is an arbitrary
</p>
<p>complex number.
It is desirable, in most situations, to orthonormalize the eigenvectors. In Always normalize the
</p>
<p>eigenvectors!the present case, they are already orthogonal. This is a property shared by
all eigenvectors of a hermitian (in fact, normal) operator stated in the next
theorem. We therefore need to merely normalize the eigenvectors:
</p>
<p>1 = 〈a1|a1〉 = α&lowast;1
(
1 &minus;i
</p>
<p>)
α1
</p>
<p>(
1
i
</p>
<p>)
= 2|α1|2,
</p>
<p>or |α1| = 1/
&radic;
</p>
<p>2 and α1 = eiϕ/
&radic;
</p>
<p>2 for some ϕ &isin; R. A similar result is ob-
tained for β1. The choice ϕ = 0 yields
</p>
<p>|e1〉 =
1&radic;
2
</p>
<p>(
1
i
</p>
<p>)
and |e2〉 =
</p>
<p>1&radic;
2
</p>
<p>(
1
&minus;i
</p>
<p>)
.
</p>
<p>The following theorem proves for all normal operators the orthogonality
property of their eigenvectors illustrated in the example above for a simple
hermitian operator.
</p>
<p>Theorem 6.4.7 An eigenspace of a normal operator reduces that op-
erator. Moreover, eigenspaces of a normal operator are mutually or-
thogonal.
</p>
<p>Proof The first part of the theorem is a trivial consequence of Theo-
rem 6.4.3. To prove the second part, let |u〉 &isin;Mλ and |v〉 &isin;Mμ with λ �= μ.
Then, using Theorem 6.1.6 once more, we obtain
</p>
<p>λ〈v|u〉 = 〈v|λu〉 = 〈v|Au〉 = 〈A&dagger;v|u〉 = 〈μ&lowast;v|u〉 = μ〈v|u〉.
</p>
<p>It follows that (λ&minus;μ)〈v|u〉 = 0 and since λ �= μ, 〈v|u〉 = 0. �
</p>
<p>Theorem 6.4.8 (Complex Spectral Decomposition) Let A be a nor-
mal operator on a finite-dimensional complex inner product space V.
Let λ1, λ2, . . . , λr be its distinct eigenvalues. Then
</p>
<p>V=M1 &oplus;M2 &oplus; &middot; &middot; &middot; &oplus;Mr ,</p>
<p/>
</div>
<div class="page"><p/>
<p>180 6 Spectral Decomposition
</p>
<p>and the nonzero (hermitian) projection operators P1,P2, . . . ,Pr ,
where Pj projects onto Mj , satisfy
</p>
<p>(1) 1=
r&sum;
</p>
<p>j=1
Pj , (2) PiPj = 0 for i �= j,
</p>
<p>(3) A=
r&sum;
</p>
<p>j=1
λjPj .
</p>
<p>Proof Let Pi be the operator that projects onto the eigenspace Mi corre-
sponding to eigenvalue λi . The fact that at least one such eigenspace exists
is guaranteed by Theorem 6.2.5. By Proposition 6.1.3, these projection op-
erators are hermitian. Because of Theorem 6.4.7 [see also Eq. (6.6)], the
only vector common to any two distinct eigenspaces is the zero vector. So,
it makes sense to talk about the direct sum of these eigenspaces. Let
</p>
<p>M=M1 &oplus;M2 &oplus; &middot; &middot; &middot; &oplus;Mr
and P = &sum;ri=1 Pi , where P is the orthogonal projection operator onto M.
Since A commutes with every Pi (Theorem 6.1.8), it commutes with P.
Hence, by Theorem 6.1.8, M reduces A, i.e., M&perp; is also invariant under A.
Now regard the restriction of A to M&perp; as an operator in its own right on the
finite-dimensional vector space M&perp;. Theorem 6.2.5 now forces A to have at
least one eigenvector in M&perp;. But this is impossible because all eigenvectors
of A have been accounted for in its eigenspaces. The only resolution is for
M&perp; to be zero. This gives
</p>
<p>V=M1 &oplus;M2 &oplus; &middot; &middot; &middot; &oplus;Mr and 1=
r&sum;
</p>
<p>i=1
Pi .
</p>
<p>The second equation follows from the first and Eqs. (6.1) and (6.2). The
remaining part of the theorem follows from arguments similar to those used
in the proof of Theorem 6.2.10. �
</p>
<p>We can now establish the connection between the diagonalizability of a
normal operator and the spectral theorem. In each subspace Mi , we choose
an orthonormal basis. The union of all these bases is clearly a basis for the
whole space V. Let us label these basis vectors |esj 〉, where the subscript
indicates the subspace and the superscript indicates the particular vector in
</p>
<p>that subspace. Clearly, 〈esj |es
&prime;
j &prime;〉 = δss&prime;δjj &prime; and Pj =
</p>
<p>&sum;mj
s=1 |esj 〉〈esj |. Noting
</p>
<p>that Pk|es
&prime;
j &prime;〉 = δkj &prime; |es
</p>
<p>&prime;
j &prime;〉, we can obtain the matrix elements of A in such a
</p>
<p>basis:
</p>
<p>&lang;
esj
</p>
<p>∣∣A
∣∣es&prime;j &prime;
</p>
<p>&rang;
=
</p>
<p>r&sum;
</p>
<p>i=1
λi
&lang;
esj
</p>
<p>∣∣Pi
∣∣es&prime;j &prime;
</p>
<p>&rang;
=
</p>
<p>r&sum;
</p>
<p>i=1
λiδij &prime;
</p>
<p>&lang;
esj
</p>
<p>∣∣es&prime;j &prime;
&rang;
= λj &prime;
</p>
<p>&lang;
esj
</p>
<p>∣∣es&prime;j &prime;
&rang;
.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.4 Complex Spectral Decomposition 181
</p>
<p>Only the diagonal elements are nonzero. We note that for each subscript j
we have mj orthonormal vectors |esj 〉, where mj is the dimension of Mj .
Thus, λj occurs mj times as a diagonal element. Therefore, in such an or-
thonormal basis, A will be represented by
</p>
<p>diag(λ1, . . . , λ1︸ ︷︷ ︸
m1 times
</p>
<p>, λ2, . . . , λ2︸ ︷︷ ︸
m2 times
</p>
<p>, . . . , λr , . . . , λr︸ ︷︷ ︸
mr times
</p>
<p>).
</p>
<p>Let us summarize the preceding discussion:
</p>
<p>Corollary 6.4.9 If A &isin; End(V) is normal, then V has an orthonormal basis
consisting of eigenvectors of A. Therefore, a normal operator on a complex
inner product space is diagonalizable.
</p>
<p>Using this corollary, the reader may show the following:
</p>
<p>Corollary 6.4.10 A hermitian operator is positive if and only if all its eigen-
values are positive.
</p>
<p>In light of Corollary 6.4.9, Theorems 6.2.10 and 6.4.8 are converses of
one another. In fact, it is straightforward to show that diagonalizability im-
plies normality. Hence, we have
</p>
<p>Proposition 6.4.11 An operator on a complex inner product space is
normal iff it is diagonalizable.
</p>
<p>Example 6.4.12 (Computation of largest and smallest eigenvalues) There Computation of the
largest and the smallest
</p>
<p>eigenvalues of a normal
</p>
<p>operator
</p>
<p>is an elegant technique that yields the largest and the smallest (in absolute
value) eigenvalues of a normal operator A in a straightforward way if the
eigenspaces of these eigenvalues are one dimensional. For convenience, as-
sume that the eigenvalues are labeled in order of decreasing absolute values:
</p>
<p>|λ1|&gt; |λ2|&gt; &middot; &middot; &middot;&gt; |λr | �= 0.
</p>
<p>Let {|ak〉}Nk=1 be a basis of V consisting of eigenvectors of A, and |x〉 =&sum;N
k=1 ξk|ak〉 an arbitrary vector in V. Then
</p>
<p>Am|x〉 =
N&sum;
</p>
<p>k=1
ξkA
</p>
<p>m|ak〉 =
N&sum;
</p>
<p>k=1
ξkλ
</p>
<p>m
k |ak〉 = λm1
</p>
<p>[
ξ1|a1〉 +
</p>
<p>N&sum;
</p>
<p>k=2
ξk
</p>
<p>(
λk
</p>
<p>λ1
</p>
<p>)m
|ak〉
</p>
<p>]
.
</p>
<p>In the limit m&rarr;&infin;, the summation in the brackets vanishes. Therefore,
</p>
<p>Am|x〉 &asymp; λm1 ξ1|a1〉 and 〈y|Am|x〉 &asymp; λm1 ξ1〈y|a1〉
</p>
<p>for any |y〉 &isin; V. Taking the ratio of this equation and the corresponding one
for m+ 1, we obtain
</p>
<p>lim
m&rarr;&infin;
</p>
<p>〈y|Am+1|x〉
〈y|Am|x〉 = λ1.</p>
<p/>
</div>
<div class="page"><p/>
<p>182 6 Spectral Decomposition
</p>
<p>Note how crucially this relation depends on the fact that λ1 is nondegenerate,
i.e., that M1 is one-dimensional. By taking larger and larger values for m,
we can obtain a better and better approximation to the largest eigenvalue.
</p>
<p>Assuming that zero is not the smallest eigenvalue λr&mdash;and therefore not
an eigenvalue&mdash;of A, we can find the smallest eigenvalue by replacing A
with A&minus;1 and λ1 with 1/λr . The details are left as an exercise for the reader.
</p>
<p>Any given hermitian matrix H can be thought of as the representation of a
hermitian operator in the standard orthonormal basis. We can find a unitary
matrix U that can transform the standard basis to the orthonormal basis con-
sisting of |esj 〉, the eigenvectors of the hermitian operator. The representation
of the hermitian operator in the new basis is UHU&dagger;, as discussed in Sect. 5.3.
However, the above argument showed that the new matrix is diagonal. We
therefore have the following result.
</p>
<p>A hermitian matrix can
</p>
<p>be diagonalized by a
</p>
<p>unitary matrix.
</p>
<p>Corollary 6.4.13 A hermitian matrix can always be brought to diag-
onal form by means of a unitary transformation matrix.
</p>
<p>Example 6.4.14 Let us consider the diagonalization of the hermitian ma-
trix
</p>
<p>H =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>0 0 &minus;1 + i &minus;1 &minus; i
0 0 &minus;1 + i 1 + i
</p>
<p>&minus;1 &minus; i &minus;1 &minus; i 0 0
&minus;1 + i 1 &minus; i 0 0
</p>
<p>⎞
⎟⎟⎠ .
</p>
<p>The characteristic polynomial is det(H&minus;λ1)= (λ+2)2(λ&minus;2)2. Thus, λ1 =
&minus;2 with multiplicity m1 = 2, and λ2 = 2 with multiplicity m2 = 2. To find
the eigenvectors, we first look at the matrix equation (H + 21)|a〉 = 0, or
</p>
<p>⎛
⎜⎜⎝
</p>
<p>2 0 &minus;1 + i &minus;1 &minus; i
0 2 &minus;1 + i 1 + i
</p>
<p>&minus;1 &minus; i &minus;1 &minus; i 2 0
&minus;1 + i 1 &minus; i 0 2
</p>
<p>⎞
⎟⎟⎠
</p>
<p>⎛
⎜⎜⎝
</p>
<p>α1
α2
α3
α4
</p>
<p>⎞
⎟⎟⎠= 0.
</p>
<p>This is a system of linear equations whose &ldquo;solution&rdquo; is
</p>
<p>α3 =
1
</p>
<p>2
(1 + i)(α1 + α2), α4 =
</p>
<p>1
</p>
<p>2
(1 &minus; i)(α1 &minus; α2).
</p>
<p>We have two arbitrary parameters, so we expect two linearly independent
solutions. For the two choices α1 = 2, α2 = 0 and α1 = 0, α2 = 2, we obtain,
respectively,
</p>
<p>|a1〉 =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>2
0
</p>
<p>1 + i
1 &minus; i
</p>
<p>⎞
⎟⎟⎠ and |a2〉 =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>0
2
</p>
<p>1 + i
&minus;1 + i
</p>
<p>⎞
⎟⎟⎠ ,</p>
<p/>
</div>
<div class="page"><p/>
<p>6.4 Complex Spectral Decomposition 183
</p>
<p>which happen to be orthogonal. We simply normalize them to obtain
</p>
<p>|e1〉 =
1
</p>
<p>2
&radic;
</p>
<p>2
</p>
<p>⎛
⎜⎜⎝
</p>
<p>2
0
</p>
<p>1 + i
1 &minus; i
</p>
<p>⎞
⎟⎟⎠ and |e2〉 =
</p>
<p>1
</p>
<p>2
&radic;
</p>
<p>2
</p>
<p>⎛
⎜⎜⎝
</p>
<p>0
2
</p>
<p>1 + i
&minus;1 + i
</p>
<p>⎞
⎟⎟⎠ .
</p>
<p>Similarly, the second eigenvalue equation, (H &minus; 21)|a〉 = 0, gives rise
to the conditions α3 =&minus; 12 (1 + i)(α1 + α2) and α4 =&minus; 12 (1 &minus; i)(α1 &minus; α2),
which produce the orthonormal vectors
</p>
<p>|e3〉 =
1
</p>
<p>2
&radic;
</p>
<p>2
</p>
<p>⎛
⎜⎜⎝
</p>
<p>2
0
</p>
<p>&minus;1 &minus; i
&minus;1 + i
</p>
<p>⎞
⎟⎟⎠ and |e4〉 =
</p>
<p>1
</p>
<p>2
&radic;
</p>
<p>2
</p>
<p>⎛
⎜⎜⎝
</p>
<p>0
2
</p>
<p>&minus;1 &minus; i
1 &minus; i
</p>
<p>⎞
⎟⎟⎠ .
</p>
<p>The unitary matrix that diagonalizes H can be constructed from these
column vectors using the remarks before Example 5.4.4, which imply that
if we simply put the vectors |ei〉 together as columns, the resulting matrix
is U&dagger;:
</p>
<p>U&dagger; = 1
2
&radic;
</p>
<p>2
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>2 0 2 0
</p>
<p>0 2 0 2
</p>
<p>1 + i 1 + i &minus;1 &minus; i &minus;1 &minus; i
</p>
<p>1 &minus; i &minus;1 + i &minus;1 + i 1 &minus; i
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>,
</p>
<p>and the unitary matrix will be
</p>
<p>U =
(
U&dagger;
</p>
<p>)&dagger; = 1
2
&radic;
</p>
<p>2
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>2 0 1 &minus; i 1 + i
</p>
<p>0 2 1 &minus; i &minus;1 &minus; i
</p>
<p>2 0 &minus;1 + i &minus;1 &minus; i
</p>
<p>0 2 &minus;1 + i 1 + i
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>.
</p>
<p>We can easily check that U diagonalizes H, i.e., that UHU&dagger; is diagonal.
</p>
<p>Example 6.4.15 In some physical applications the ability to diagonalize
matrices can be very useful. As a simple but illustrative example, let us con-
sider the motion of a charged particle in a constant magnetic field pointing
</p>
<p>application of
</p>
<p>diagonalization in
</p>
<p>electromagnetism
in the z direction. The equation of motion for such a particle is
</p>
<p>m
dv
dt
</p>
<p>= qv &times; B = q det
</p>
<p>⎛
⎝
</p>
<p>êx êy êz
vx vy vz
0 0 B
</p>
<p>⎞
⎠ ,</p>
<p/>
</div>
<div class="page"><p/>
<p>184 6 Spectral Decomposition
</p>
<p>which in component form becomes
</p>
<p>dvx
</p>
<p>dt
= qB
</p>
<p>m
vy,
</p>
<p>dvy
</p>
<p>dt
=&minus;qB
</p>
<p>m
vx,
</p>
<p>dvz
</p>
<p>dt
= 0.
</p>
<p>Ignoring the uniform motion in the z direction, we need to solve the first
two coupled equations, which in matrix form becomes
</p>
<p>d
</p>
<p>dt
</p>
<p>(
vx
vy
</p>
<p>)
= qB
</p>
<p>m
</p>
<p>(
0 1
&minus;1 0
</p>
<p>)(
vx
vy
</p>
<p>)
=&minus;iω
</p>
<p>(
0 i
&minus;i 0
</p>
<p>)(
vx
vy
</p>
<p>)
, (6.12)
</p>
<p>where we have introduced a factor of i to render the matrix hermitian, and
defined ω = qB/m. If the 2 &times; 2 matrix were diagonal, we would get two
uncoupled equations, which we could solve easily. Diagonalizing the matrix
involves finding a matrix R such that
</p>
<p>D = R
(
</p>
<p>0 i
&minus;i 0
</p>
<p>)
R&minus;1 =
</p>
<p>(
μ1 0
0 μ2
</p>
<p>)
.
</p>
<p>If we could do such a diagonalization, we would multiply (6.12) by R to
get4
</p>
<p>d
</p>
<p>dt
R
</p>
<p>(
vx
vy
</p>
<p>)
=&minus;iωR
</p>
<p>(
0 i
&minus;i 0
</p>
<p>)
R&minus;1R
</p>
<p>(
vx
vy
</p>
<p>)
,
</p>
<p>which can be written as
</p>
<p>d
</p>
<p>dt
</p>
<p>(
v&prime;x
v&prime;y
</p>
<p>)
=&minus;iω
</p>
<p>(
μ1 0
0 μ2
</p>
<p>)(
v&prime;x
v&prime;y
</p>
<p>)
=
(&minus;iωμ1v&prime;x
&minus;iωμ2v&prime;y
</p>
<p>)
, where
</p>
<p>(
v&prime;x
v&prime;y
</p>
<p>)
&equiv; R
</p>
<p>(
vx
vy
</p>
<p>)
.
</p>
<p>We then would have a pair of uncoupled equations
</p>
<p>dv&prime;x
dt
</p>
<p>=&minus;iωμ1v&prime;x,
dv&prime;y
dt
</p>
<p>=&minus;iωμ2v&prime;y
</p>
<p>that have v&prime;x = v&prime;0xe&minus;iωμ1t and v&prime;y = v&prime;0ye&minus;iωμ2t as a solution set, in which
v&prime;0x and v
</p>
<p>&prime;
0y are integration constants.
</p>
<p>To find R, we need the normalized eigenvectors of ( 0 i&minus;i 0 ). But these are
obtained in precisely the same fashion as in Example 6.4.6. There is, how-
ever, an arbitrariness in the solutions due to the choice in numbering the
eigenvalues. If we choose the normalized eigenvectors
</p>
<p>|e1〉 =
1&radic;
2
</p>
<p>(
i
</p>
<p>1
</p>
<p>)
, |e2〉 =
</p>
<p>1&radic;
2
</p>
<p>(&minus;i
1
</p>
<p>)
,
</p>
<p>4The fact that R is independent of t is crucial in this step. This fact, in turn, is a conse-
quence of the independence from t of the original 2 &times; 2 matrix.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.4 Complex Spectral Decomposition 185
</p>
<p>then from comments at the end of Sect. 5.3, we get
</p>
<p>R&minus;1 = R&dagger; = 1&radic;
2
</p>
<p>(
i &minus;i
1 1
</p>
<p>)
&rArr; R =
</p>
<p>(
R&dagger;
</p>
<p>)&dagger; = 1&radic;
2
</p>
<p>(&minus;i 1
i 1
</p>
<p>)
.
</p>
<p>With this choice of R, we have
</p>
<p>R
</p>
<p>(
0 i
&minus;i 0
</p>
<p>)
R&minus;1 =
</p>
<p>(
1 0
0 &minus;1
</p>
<p>)
,
</p>
<p>so that μ1 = 1 =&minus;μ2. Having found R&dagger;, we can write
(
vx
vy
</p>
<p>)
= R&dagger;
</p>
<p>(
v&prime;x
v&prime;y
</p>
<p>)
= 1&radic;
</p>
<p>2
</p>
<p>(
i &minus;i
1 1
</p>
<p>)(
v&prime;0xe
</p>
<p>&minus;iωt
</p>
<p>v&prime;0ye
iωt
</p>
<p>)
. (6.13)
</p>
<p>If the x and y components of velocity at t = 0 are v0x and v0y , respectively,
then
(
v0x
v0y
</p>
<p>)
= R&dagger;
</p>
<p>(
v&prime;0x
v&prime;0y
</p>
<p>)
, or
</p>
<p>(
v&prime;0x
v&prime;0y
</p>
<p>)
= R
</p>
<p>(
v0x
v0y
</p>
<p>)
= 1&radic;
</p>
<p>2
</p>
<p>(&minus;iv0x + v0y
iv0x + v0y
</p>
<p>)
.
</p>
<p>Substituting in (6.13), we obtain
</p>
<p>(
vx
vy
</p>
<p>)
= 1
</p>
<p>2
</p>
<p>(
i &minus;i
1 1
</p>
<p>)(
(&minus;iv0x + v0y)e&minus;iωt
(iv0x + v0y)eiωt
</p>
<p>)
</p>
<p>=
(
</p>
<p>v0x cosωt + v0y sinωt
&minus;v0x sinωt + v0y cosωt
</p>
<p>)
.
</p>
<p>This gives the velocity as a function of time. Antidifferentiating once with
respect to time yields the position vector.
</p>
<p>6.4.1 Simultaneous Diagonalization
</p>
<p>In many situations of physical interest, it is desirable to know whether two
operators are simultaneously diagonalizable. For instance, if there exists a
basis of a Hilbert space of a quantum-mechanical system consisting of si-
multaneous eigenvectors of two operators, then one can measure those two
operators at the same time. In particular, they are not restricted by an uncer-
tainty relation.
</p>
<p>Definition 6.4.16 Two operators are said to be simultaneously diagonal-
izable if they can be written in terms of the same set of projection operators,
</p>
<p>simultaneous
</p>
<p>diagonalization defined
as in Theorem 6.4.8.
</p>
<p>This definition is consistent with the matrix representation of the two op-
erators, because if we take the orthonormal basis B = {|esj 〉} discussed right
after Theorem 6.4.8, we obtain diagonal matrices for both operators. What
are the conditions under which two operators can be simultaneously diago-
nalized? Clearly, a necessary condition is that the two operators commute.</p>
<p/>
</div>
<div class="page"><p/>
<p>186 6 Spectral Decomposition
</p>
<p>This is an immediate consequence of the orthogonality of the projection op-
erators, which trivially implies PiPj = PjPi for all i and j . It is also appar-
ent in the matrix representation of the operators: Any two diagonal matrices
commute. What about sufficiency? Is the commutativity of the two opera-
tors sufficient for them to be simultaneously diagonalizable? To answer this
question, we need the following lemma:
</p>
<p>Lemma 6.4.17 An operator T commutes with a normal operator A if and
only if T commutes with all the projection operators of A.
</p>
<p>Proof The &ldquo;if&rdquo; part is trivial. To prove the &ldquo;only if&rdquo; part, suppose AT= TA,
and let |x〉 be any vector in one of the eigenspaces of A, say Mj . Then
we have A(T|x〉) = T(A|x〉) = T(λj |x〉) = λj (T|x〉); i.e., T|x〉 is in Mj , or
Mj is invariant under T. Since Mj is arbitrary, T leaves all eigenspaces
invariant. In particular, it leaves M&perp;j , the orthogonal complement of Mj (the
direct sum of all the remaining eigenspaces), invariant. By Theorems 6.1.6
and 6.1.8, TPj = PjT; and this holds for all j . �
</p>
<p>necessary and sufficient
</p>
<p>condition for
</p>
<p>simultaneous
</p>
<p>diagonalizability
</p>
<p>Theorem 6.4.18 Two normal operators A and B are simultaneously
diagonalizable iff [A,B] = 0.
</p>
<p>Proof As claimed above, the &ldquo;necessity&rdquo; is trivial. To prove the &ldquo;suffi-
ciency&rdquo;, let
</p>
<p>A=
r&sum;
</p>
<p>j=1
λjPj and B=
</p>
<p>s&sum;
</p>
<p>α=1
μαQα,
</p>
<p>where {λj } and {Pj } are eigenvalues and projections of A, and {μα} and
{Qα} are those of B. Assume [A,B] = 0. Then by Lemma 6.4.17, AQα =
QαA. Since Qα commutes with A, it must commute with the latter&rsquo;s projec-
tion operators: PjQα =QαPj . Now define Rjα &equiv; PjQα , and note that
</p>
<p>R
&dagger;
jα = (PjQα)&dagger; =Q&dagger;αP
</p>
<p>&dagger;
j =QαPj = PjQα = Rjα,
</p>
<p>(Rjα)
2 = (PjQα)2 = PjQαPjQα = PjPjQαQα = PjQα = Rjα.
</p>
<p>Therefore, Rjα are hermitian projection operators. In fact, they are the pro-
jection operators that project onto the intersection of the eigenspaces of A
and B. Furthermore,
</p>
<p>r&sum;
</p>
<p>j=1
Rjα =
</p>
<p>r&sum;
</p>
<p>j=1
Pj
</p>
<p>︸ ︷︷ ︸
=1
</p>
<p>Qα =Qα,
</p>
<p>and similarly,
&sum;s
</p>
<p>α=1 Rjα = Pj . Since,
&sum;
</p>
<p>j,α
</p>
<p>Rjα =
&sum;
</p>
<p>α
</p>
<p>Qα = 1,</p>
<p/>
</div>
<div class="page"><p/>
<p>6.4 Complex Spectral Decomposition 187
</p>
<p>not all Rjα can be zero. In fact, because of this identity, we must have
</p>
<p>V=
&oplus;
</p>
<p>j,α
</p>
<p>Mj &cap;Nα
</p>
<p>where Mj and Nα are the eigenspaces of A and B, respectively. We can now
write A and B as
</p>
<p>A=
&sum;
</p>
<p>j
</p>
<p>λjPj =
&sum;
</p>
<p>j,α
</p>
<p>λjRjα, B=
&sum;
</p>
<p>α
</p>
<p>μαQα =
&sum;
</p>
<p>j,α
</p>
<p>μαRjα.
</p>
<p>By definition, they are simultaneously diagonalizable. �
</p>
<p>Example 6.4.19 Let us find the spectral decomposition of the Pauli spin
matrix
</p>
<p>spectral decomposition
</p>
<p>of a Pauli spin matrix
</p>
<p>σ2 =
(
</p>
<p>0 &minus;i
i 0
</p>
<p>)
.
</p>
<p>The eigenvalues and eigenvectors have been found in Example 6.4.6. These
are
</p>
<p>λ1 = 1, |e1〉 =
1&radic;
2
</p>
<p>(
1
i
</p>
<p>)
and λ2 =&minus;1, |e2〉 =
</p>
<p>1&radic;
2
</p>
<p>(
1
&minus;i
</p>
<p>)
.
</p>
<p>The subspaces Mλj are one-dimensional; therefore,
</p>
<p>P1 = |e1〉〈e1| =
1&radic;
2
</p>
<p>(
1
i
</p>
<p>)
1&radic;
2
</p>
<p>(
1 &minus;i
</p>
<p>)
= 1
</p>
<p>2
</p>
<p>(
1 &minus;i
i 1
</p>
<p>)
,
</p>
<p>P2 = |e2〉〈e2| =
1
</p>
<p>2
</p>
<p>(
1
&minus;i
</p>
<p>)(
1 i
</p>
<p>)
= 1
</p>
<p>2
</p>
<p>(
1 i
&minus;i 1
</p>
<p>)
.
</p>
<p>We can check that P1 + P2 =
( 1 0
</p>
<p>0 1
</p>
<p>)
and
</p>
<p>λ1P1 + λ2P2 =
1
</p>
<p>2
</p>
<p>(
1 &minus;i
i 1
</p>
<p>)
&minus; 1
</p>
<p>2
</p>
<p>(
1 i
&minus;i 1
</p>
<p>)
=
(
</p>
<p>0 &minus;i
i 0
</p>
<p>)
= σ2.
</p>
<p>Example 6.4.20 In this example, we provide another proof that if T is diag-
onalizable, then it must be normal. We saw in Chap. 4 that T can be written
in terms of its so-called Cartesian components as T= X+ iY where both X
and Y are hermitian and can therefore be decomposed according to Theo-
rem 6.4.8. Can we conclude that T is also decomposable? No. Because the
projection operators used in the decomposition of X may not be the same
as those used for Y. However, if X and Y are simultaneously diagonalizable
such that5
</p>
<p>X=
r&sum;
</p>
<p>k=1
λkPk and Y=
</p>
<p>r&sum;
</p>
<p>k=1
λ&prime;kPk, (6.14)
</p>
<p>5Note that X and Y may not have equal number of projection operators. Therefore one of
the sums may contain zeros as part of their summands.</p>
<p/>
</div>
<div class="page"><p/>
<p>188 6 Spectral Decomposition
</p>
<p>then T=&sum;rk=1(λk + iλ&prime;k)Pk . It follows that T has a spectral decomposition,
and therefore is diagonalizable. Theorem 6.4.18 now implies that X and Y
must commute. Since, X= 12 (T+T&dagger;) and Y= 12i (T&minus;T&dagger;), we have [X,Y] =
0 if and only if [T,T&dagger;] = 0; i.e., T is normal.
</p>
<p>6.5 Functions of Operators
</p>
<p>Functions of transformations were discussed in Chap. 4. With the power of
spectral decomposition at our disposal, we can draw many important con-
clusions about them.
</p>
<p>First, we note that if T = &sum;ri=1 λiPi , then, because of orthogonality of
the Pi &rsquo;s
</p>
<p>T2 =
r&sum;
</p>
<p>i=1
λ2i Pi, T
</p>
<p>3 =
r&sum;
</p>
<p>i=1
λ3i Pi, . . . , T
</p>
<p>n =
r&sum;
</p>
<p>i=1
λni Pi .
</p>
<p>Thus, any polynomial p in T has a spectral decomposition given by p(T)=&sum;r
i=1 p(λi)Pi . Generalizing this to functions expandable in power series
</p>
<p>gives
</p>
<p>f (T)=
&infin;&sum;
</p>
<p>i=1
f (λi)Pi . (6.15)
</p>
<p>Example 6.5.1 Let us investigate the spectral decomposition of the follow-
ing unitary (actually orthogonal) matrix:
</p>
<p>U =
(
</p>
<p>cos θ &minus; sin θ
sin θ cos θ
</p>
<p>)
.
</p>
<p>We find the eigenvalues
</p>
<p>det
</p>
<p>(
cos θ &minus; λ &minus; sin θ
</p>
<p>sin θ cos θ &minus; λ
</p>
<p>)
= λ2 &minus; 2 cos θλ+ 1 = 0,
</p>
<p>yielding λ1 = e&minus;iθ and λ2 = eiθ . For λ1 we have (reader, provide the miss-
ing steps)
</p>
<p>(
cos θ &minus; eiθ &minus; sin θ
</p>
<p>sin θ cos θ &minus; eiθ
)(
</p>
<p>α1
α2
</p>
<p>)
= 0
</p>
<p>&rArr; α2 = iα1 &rArr; |e1〉 =
1&radic;
2
</p>
<p>(
1
i
</p>
<p>)
,
</p>
<p>and for λ2,
(
</p>
<p>cos θ &minus; e&minus;iθ &minus; sin θ
sin θ cos θ &minus; e&minus;iθ
</p>
<p>)(
α1
α2
</p>
<p>)
= 0
</p>
<p>&rArr; α2 =&minus;iα1 &rArr; |e2〉 =
1&radic;
2
</p>
<p>(
1
&minus;i
</p>
<p>)
.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.5 Functions of Operators 189
</p>
<p>We note that the Mλj are one-dimensional and spanned by |ej 〉. Thus,
</p>
<p>P1 = |e1〉〈e1| =
1
</p>
<p>2
</p>
<p>(
1
i
</p>
<p>)(
1 &minus;i
</p>
<p>)
= 1
</p>
<p>2
</p>
<p>(
1 &minus;i
i 1
</p>
<p>)
,
</p>
<p>P2 = |e2〉〈e2| =
1
</p>
<p>2
</p>
<p>(
1
&minus;i
</p>
<p>)(
1 i
</p>
<p>)
= 1
</p>
<p>2
</p>
<p>(
1 i
&minus;i 1
</p>
<p>)
.
</p>
<p>Clearly, P1 + P2 = 1, and
</p>
<p>e&minus;iθP1 + eiθP2 =
1
</p>
<p>2
</p>
<p>(
e&minus;iθ &minus;ie&minus;iθ
ie&minus;iθ e&minus;iθ
</p>
<p>)
+ 1
</p>
<p>2
</p>
<p>(
eiθ ieiθ
</p>
<p>&minus;ieiθ eiθ
)
= U.
</p>
<p>If we take the natural log of this equation and use Eq. (6.15), we obtain
</p>
<p>ln U = ln
(
e&minus;iθ
</p>
<p>)
P1 + ln
</p>
<p>(
eiθ
</p>
<p>)
P2 =&minus;iθP1 + iθP2
</p>
<p>= i(&minus;θP1 + θP2)&equiv; iH, (6.16)
</p>
<p>where H &equiv;&minus;θP1 + θP2 is a hermitian operator because θ is real and P1 and
P2 are hermitian. Inverting Eq. (6.16) gives U = eiH, where
</p>
<p>H = θ(&minus;P1 + P2)= θ
(
</p>
<p>0 i
&minus;i 0
</p>
<p>)
.
</p>
<p>Using this matrix in the power series expansion of the exponential, the
reader is urged to verify directly that U = eiH.
</p>
<p>The example above shows that the unitary 2 &times; 2 matrix U can be written
as an exponential of an anti-hermitian operator. This is a general result. In
fact, we have the following theorem, whose proof is left as an exercise for
the reader (see Problem 6.23).
</p>
<p>Theorem 6.5.2 A unitary operator U on a finite-dimensional com-
plex inner product space can be written as U= eiH where H is hermi-
tian. Furthermore, a unitary matrix can be brought to diagonal form
by a unitary transformation matrix.
</p>
<p>The last statement follows from Corollary 6.4.13 and the fact that
</p>
<p>f
(
RHR&minus;1
</p>
<p>)
= Rf (H)R&minus;1
</p>
<p>for any function f that can be expanded in a Taylor series.
A useful function of an operator is its square root. A natural way to de-
</p>
<p>fine the square root of a normal operator A is
&radic;
A=&sum;ri=1(&plusmn;
</p>
<p>&radic;
λi)Pi . This
</p>
<p>The square root of a
</p>
<p>normal operator is
</p>
<p>plagued by
</p>
<p>multivaluedness. In the
</p>
<p>real numbers, we have
</p>
<p>only two-valuedness!
</p>
<p>clearly gives many candidates (2r , to be exact) for the root.
</p>
<p>Definition 6.5.3 The positive square root of a positive (thus hermitian,
thus normal) operator A=&sum;ri=1 λiPi is
</p>
<p>&radic;
A=&sum;ri=1
</p>
<p>&radic;
λiPi .</p>
<p/>
</div>
<div class="page"><p/>
<p>190 6 Spectral Decomposition
</p>
<p>The uniqueness of the spectral decomposition implies that the positive
square root of a positive operator is unique.
</p>
<p>Example 6.5.4 Let us evaluate
&radic;
</p>
<p>A where
</p>
<p>A =
(
</p>
<p>5 3i
&minus;3i 5
</p>
<p>)
.
</p>
<p>First, we have to spectrally decompose A. Its characteristic equation is
</p>
<p>λ2 &minus; 10λ+ 16 = 0,
</p>
<p>with roots λ1 = 8 and λ2 = 2. Since both eigenvalues are positive and A is
hermitian, we conclude that A is indeed positive (Corollary 6.4.10). We can
also easily find its normalized eigenvectors:
</p>
<p>|e1〉 =
1&radic;
2
</p>
<p>(
i
</p>
<p>1
</p>
<p>)
and |e2〉 =
</p>
<p>1&radic;
2
</p>
<p>(&minus;i
1
</p>
<p>)
.
</p>
<p>Thus,
</p>
<p>P1 = |e1〉〈e1| =
1
</p>
<p>2
</p>
<p>(
1 i
&minus;i 1
</p>
<p>)
, P2 = |e2〉〈e2| =
</p>
<p>1
</p>
<p>2
</p>
<p>(
1 &minus;i
i 1
</p>
<p>)
,
</p>
<p>and
&radic;
</p>
<p>A =
&radic;
λ1P1 +
</p>
<p>&radic;
λ2P2
</p>
<p>=
&radic;
</p>
<p>8
1
</p>
<p>2
</p>
<p>(
1 i
&minus;i 1
</p>
<p>)
+
&radic;
</p>
<p>2
1
</p>
<p>2
</p>
<p>(
1 &minus;i
i 1
</p>
<p>)
= 1&radic;
</p>
<p>2
</p>
<p>(
3 i
&minus;i 3
</p>
<p>)
.
</p>
<p>We can easily check that (
&radic;
</p>
<p>A)2 = A.
</p>
<p>Intuitively, higher and higher powers of T, when acting on a few vectors
of the space, eventually exhaust all vectors, and further increase in power
will be a repetition of lower powers. This intuitive idea can be made more
precise by looking at the projection operators. We have already seen that
</p>
<p>Tn =
r&sum;
</p>
<p>j=1
λnjPj , n= 1,2, . . . .
</p>
<p>For various n&rsquo;s one can &ldquo;solve&rdquo; for Pj in terms of powers of T. Since there
are only a finite number of Pj &rsquo;s, only a finite number of powers of T will
suffice. In fact, we can explicitly construct the polynomial in T for Pj . If
there is such a polynomial, by Eq. (6.15) it must satisfy
</p>
<p>Pj = pj (T)=
r&sum;
</p>
<p>k=1
pj (λk)Pk,
</p>
<p>where pj is some polynomial to be determined. By orthogonality of the
projection operators, pj (λk) must be zero unless k = j , in which case it
must be 1. In other words, pj (λk)= δkj . Such a polynomial can be explicitly</p>
<p/>
</div>
<div class="page"><p/>
<p>6.6 Real Spectral Decomposition 191
</p>
<p>constructed:
</p>
<p>pj (x)=
(
</p>
<p>x &minus; λ1
λj &minus; λ1
</p>
<p>)(
x &minus; λ2
λj &minus; λ2
</p>
<p>)
. . .
</p>
<p>(
x &minus; λr
λj &minus; λr
</p>
<p>)
&equiv;
</p>
<p>r&prod;
</p>
<p>k �=j
</p>
<p>x &minus; λk
λj &minus; λk
</p>
<p>.
</p>
<p>Therefore,
</p>
<p>Pj = pj (T)&equiv;
r&prod;
</p>
<p>k �=j
</p>
<p>T&minus; λk1
λj &minus; λk
</p>
<p>, (6.17)
</p>
<p>and we have the following result.
</p>
<p>Proposition 6.5.5 Let V be a finite-dimensional vector space and T &isin;
End(V) a normal operator. Then
</p>
<p>f (T)=
r&sum;
</p>
<p>j=1
f (λj )Pj =
</p>
<p>r&sum;
</p>
<p>j=1
f (λj )
</p>
<p>r&prod;
</p>
<p>k �=j
</p>
<p>T&minus; λk1
λj &minus; λk
</p>
<p>, (6.18)
</p>
<p>i.e., every function of T is a polynomial.
</p>
<p>Example 6.5.6 Let us write
&radic;
</p>
<p>A of the last example as a polynomial in A.
We have
</p>
<p>p1(A)=
r&prod;
</p>
<p>k �=1
</p>
<p>A &minus; λk1
λ1 &minus; λk
</p>
<p>= A &minus; λ21
λ1 &minus; λ2
</p>
<p>= 1
6
(A &minus; 2),
</p>
<p>p2(A)=
r&prod;
</p>
<p>k �=2
</p>
<p>A &minus; λk1
λ2 &minus; λk
</p>
<p>= A &minus; λ11
λ2 &minus; λ1
</p>
<p>=&minus;1
6
(A &minus; 8).
</p>
<p>Substituting in Eq. (6.18), we obtain
</p>
<p>&radic;
A =
</p>
<p>&radic;
λ1p1(A)+
</p>
<p>&radic;
λ2p2(A)=
</p>
<p>&radic;
8
</p>
<p>6
(A&minus;2)&minus;
</p>
<p>&radic;
2
</p>
<p>6
(A&minus;8)=
</p>
<p>&radic;
2
</p>
<p>6
A+
</p>
<p>&radic;
8
</p>
<p>3
1.
</p>
<p>The RHS is clearly a (first-degree) polynomial in A, and it is easy to verify
that it is the matrix of
</p>
<p>&radic;
A obtained in the previous example.
</p>
<p>6.6 Real Spectral Decomposition
</p>
<p>The treatment so far in this chapter has focused on complex inner product
spaces. The complex number system is &ldquo;more complete&rdquo; than the real num-
bers. For example, in preparation for the proof of the spectral decomposition
theorem, we used the existence of roots of a polynomial over the complex
field (this is the fundamental theorem of algebra). A polynomial over the
reals, on the other hand, does not necessarily have all its roots in the real
number system. Since the existence of roots was necessary for the proof
of Theorem 6.3.2, real operators cannot, in general, even be represented by
upper-triangular matrices. It may therefore seem that vector spaces over the</p>
<p/>
</div>
<div class="page"><p/>
<p>192 6 Spectral Decomposition
</p>
<p>reals will not satisfy the useful theorems and results developed for complex
spaces. However, as we shall see in this section, some of the useful results
carry over to the real case.
</p>
<p>Theorem 6.6.1 An operator on a real vector space has invariant subspaces
of dimension 1 or 2.
</p>
<p>Proof Let V be a real vector space of dimension N and T &isin; L(V). Take a
nonzero vector |v〉 &isin; V and consider the N + 1 vectors {Tk|v〉}Nk=0. These
vectors are linearly dependent. Hence, there exist a set of real numbers
{ηk}Nk=0, not all equal to zero, such that
</p>
<p>η0|v〉 + η1T|v〉 + &middot; &middot; &middot; + ηNTN |v〉 = |0〉 or p(T)|v〉 = |0〉, (6.19)
</p>
<p>where p(T)=&sum;Nk=0 ηkTk is a polynomial in T. By Theorem 3.6.5, we have
</p>
<p>p(T)= γ
r&prod;
</p>
<p>i=1
(T&minus; λi1)ki
</p>
<p>R&prod;
</p>
<p>j=1
</p>
<p>(
T2 + αjT+ βj1
</p>
<p>)Kj , (6.20)
</p>
<p>for some nonzero constant γ .6 If all the factors in the two products are in-
jective, then they are all invertible (why?). It follows that p(T) is invertible,
and Eq. (6.19) yields |v〉 = |0〉, which contradicts our assumption. Hence, at
least one of the factors in the product is not injective, i.e., its kernel contains
a nonzero vector. If this factor is one of the terms in the first product, say
T&minus; λm1, and |u〉 �= |0〉 is in its kernel, then
</p>
<p>(T&minus; λm1)|u〉 = |0〉 or T|u〉 = λm|u〉,
</p>
<p>and Span{|u〉} is a one-dimensional invariant subspace.
Now suppose that the non-injective factor is in the second product of
</p>
<p>Eq. (6.20), say T2 + αnT+ βn1 and |v〉 �= |0〉 is in its kernel, then
(
T2 + αnT+ βn1
</p>
<p>)
|v〉 = |0〉.
</p>
<p>It is straightforward to show that Span{|v〉,T|v〉} is an invariant subspace,
whose dimension is 1 if |v〉 happens to be an eigenvector of T, and 2 if
not. �
</p>
<p>Example 6.6.2 Consider the operator T :R2 &rarr;R2 given by
</p>
<p>T
</p>
<p>(
α1
α2
</p>
<p>)
=
(
</p>
<p>α2
&minus;α1
</p>
<p>)
.
</p>
<p>Suppose |x〉 &isin;R2 is an eigenvector of T. Then
</p>
<p>T|x〉 = λ|x〉 &rArr; T2|x〉 = λT|x〉 = λ2|x〉.
</p>
<p>But T2 = &minus;1, as can be easily verified. Therefore, λ2 = &minus;1, and T has no
real eigenvalue. It follows that T has no eigenvectors in R2.
</p>
<p>6We are not assuming that ηN �= 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.6 Real Spectral Decomposition 193
</p>
<p>The preceding example showed that there exist operators on R2 which
have no eigenvectors. The fact that the dimension of the vector space was
even played an important role in the absence of the eigenvectors. This is
not generally true for odd-dimensional vector spaces. In fact, we have the
following:
</p>
<p>Theorem 6.6.3 Every operator on an odd-dimensional real vector space
has a real eigenvalue and an associated eigenvector.
</p>
<p>Proof Let V be a real vector space of odd dimension N and T &isin; L(V).
We prove the theorem by induction on N . Obviously, the theorem holds
for N = 1. If T has no eigenvalue, then by Theorem 6.6.1, there is a two-
dimensional invariant subspace U. Write
</p>
<p>V=U&oplus;W,
</p>
<p>where W has odd dimension N &minus; 2. With TU and TW as in Eq. (4.13), and
the fact that TW &isin;L(W), we can assume that the induction hypothesis holds
for TW , i.e., that it has a real eigenvalue λ and an eigenvector |w〉 in W.
</p>
<p>Now consider the 3-dimensional subspace V3 of V and an operator Tλ
defined by
</p>
<p>V3 =U&oplus; Span{|w〉}, and Tλ = T&minus; λ1,
respectively, and note that TλU&sube;U because U is invariant under T. Further-
more,
</p>
<p>Tλ|w〉 = T|w〉 &minus; λ|w〉 = TU |w〉 + TW |w〉 &minus; λ|w〉︸ ︷︷ ︸
=|0〉
</p>
<p>= TU |w〉 = PU
(
TU |w〉
</p>
<p>)
&isin;U.
</p>
<p>Thus, Tλ : V3 &rarr;U. Invoking the dimension theorem, we see that kerTλ has
dimension at least one. Thus, there is |v3〉 &isin; V3 such that
</p>
<p>Tλ|v3〉 &equiv; (T&minus; λ1)|v3〉 = |0〉,
</p>
<p>i.e., that T has a real eigenvalue and a corresponding eigenvector. �
</p>
<p>6.6.1 The Case of Symmetric Operators
</p>
<p>The existence of at least one eigenvalue was crucial in proving the complex
spectral theorem. A normal operator on a real vector space does not have a
real eigenvalue in general. However, if the operator is self-adjoint (hermi-
tian, symmetric), then it will have a real eigenvalue. To establish this, we
start with the following
</p>
<p>Lemma 6.6.4 Let T be a self-adjoint (hermitian) operator on a vector
space V. Then
</p>
<p>H&equiv; T2 + αT+ β1, α,β &isin;R, α2 &lt; 4β,
</p>
<p>is invertible.</p>
<p/>
</div>
<div class="page"><p/>
<p>194 6 Spectral Decomposition
</p>
<p>Proof By Theorem 4.3.10, it is sufficient to prove that H is strictly positive.
Factor out the polynomial into its linear factors and note that, since α and β
are real, the two roots are complex conjugate of one another. Furthermore,
since α2 &lt; 4β , the imaginary parts of the roots are not zero. Let λ be one of
the roots and let S= T&minus; λ1. Since T is self-adjoint, H= S&dagger;S. Therefore,
</p>
<p>〈a|H|a〉 = 〈a|S&dagger;S|a〉 = 〈Sa|Sa〉 &ge; 0.
</p>
<p>The case of 0 is excluded because it corresponds to
</p>
<p>S|a〉 = |0〉 or (T&minus; λ1)|a〉 = |0〉,
</p>
<p>implying that |a〉 is an eigenvector of T with a non-real eigenvalue. This
contradicts Theorem 4.3.7. Therefore, 〈a|H|a〉&gt; 0. �
</p>
<p>Note that the lemma holds for complex as well as real vector spaces. Prob-
lem 6.24 shows how to prove the lemma without resort to complex roots.
</p>
<p>Proposition 6.6.5 A self-adjoint (symmetric) real operator has a real
eigenvalue.
</p>
<p>Proof As in the proof of Theorem 6.6.1, we have a nonzero vector |v〉 and
a polynomial p(T) such that p(T)|v〉 = |0〉, i.e.,
</p>
<p>r&prod;
</p>
<p>i=1
(T&minus; λi1)ki
</p>
<p>R&prod;
</p>
<p>j=1
</p>
<p>(
T2 + αjT+ βj1
</p>
<p>)Kj |v〉 = |0〉,
</p>
<p>with λi, αj , βj &isin;R and α2j &lt; 4βj . By Lemma 6.6.4, all the quadratic factors
are invertible. Multiplying by their inverses, we get
</p>
<p>r&prod;
</p>
<p>i=1
(T&minus; λi1)ki |v〉 = |0〉.
</p>
<p>At least one of these factors, say i = m, must be non-injective (why?).
Hence,
</p>
<p>(T&minus; λm1)km |v〉 = |0〉.
If |a〉 &equiv; (T&minus; λm1)km&minus;1|v〉 �= |0〉, then |a〉 is an eigenvector of T with real
eigenvalue λm. Otherwise, we have
</p>
<p>(T&minus; λm1)km&minus;1|v〉 = |0〉.
</p>
<p>If |b〉 &equiv; (T&minus; λm1)km&minus;2|v〉 �= |0〉, then |b〉 is an eigenvector of T with real
eigenvalue λm. It is clear that this process has to stop at some point. It fol-
lows that there exists a nonzero vector |c〉 such that (T&minus; λm1)|c〉 = |0〉. �
</p>
<p>Now that we have established the existence of at least one real eigenvalue
for a self-adjoint real operator, we can follow the same steps taken in the
proof of Theorem 6.4.8 and prove the following:</p>
<p/>
</div>
<div class="page"><p/>
<p>6.6 Real Spectral Decomposition 195
</p>
<p>Theorem 6.6.6 Let V be a real inner product space and T a self-
adjoint operator on V. Then there exists an orthonormal basis in V
with respect to which T is represented by a diagonal matrix.
</p>
<p>This theorem is especially useful in applications of classical physics,
which deal mostly with real vector spaces. A typical situation involves a
vector that is related to another vector by a symmetric matrix. It is then
convenient to find a coordinate system in which the two vectors are related
in a simple manner. This involves diagonalizing the symmetric matrix by a
rotation (a real orthogonal matrix). Theorem 6.6.6 reassures us that such a
diagonalization is possible.
</p>
<p>Example 6.6.7 For a system of N point particles constituting a rigid body,
the total angular momentum L =&sum;Ni=1 mi(ri &times; vi) is related to the angular
frequency via
</p>
<p>L =
N&sum;
</p>
<p>i=1
mi
</p>
<p>[
ri &times; (ωωω&times; ri)
</p>
<p>]
=
</p>
<p>N&sum;
</p>
<p>i=1
mi
</p>
<p>[
ωωωri &middot; ri &minus; ri(ri &middot;ωωω)
</p>
<p>]
,
</p>
<p>or
moment of inertia matrix
</p>
<p>⎛
⎝
Lx
Ly
Lz
</p>
<p>⎞
⎠=
</p>
<p>⎛
⎝
Ixx Ixy Ixz
Iyx Iyy Iyz
Izx Izy Izz
</p>
<p>⎞
⎠
⎛
⎝
ωx
ωy
ωz
</p>
<p>⎞
⎠ ,
</p>
<p>where
</p>
<p>Ixx =
N&sum;
</p>
<p>i=1
mi
</p>
<p>(
r2i &minus; x2i
</p>
<p>)
, Iyy =
</p>
<p>N&sum;
</p>
<p>i=1
mi
</p>
<p>(
r2i &minus; y2i
</p>
<p>)
,
</p>
<p>Izz =
N&sum;
</p>
<p>i=1
mi
</p>
<p>(
r2i &minus; z2i
</p>
<p>)
, Ixy =&minus;
</p>
<p>N&sum;
</p>
<p>i=1
mixiyi,
</p>
<p>Ixz =&minus;
N&sum;
</p>
<p>i=1
mixizi, Iyz =&minus;
</p>
<p>N&sum;
</p>
<p>i=1
miyizi,
</p>
<p>with Ixy = Iyx , Ixz = Izx , and Iyz = Izy .
The 3 &times; 3 matrix is denoted by I and is called the moment of inertia
</p>
<p>matrix. It is symmetric, and Theorem 6.6.6 permits its diagonalization by an
orthogonal transformation (the counterpart of a unitary transformation in a
real vector space). But an orthogonal transformation in three dimensions is
merely a rotation of coordinates.7 Thus, Theorem 6.6.6 says that it is always
possible to choose coordinate systems in which the moment of inertia matrix
is diagonal. In such a coordinate system we have Lx = Ixxωx , Ly = Iyyωy ,
and Lz = Izzωz, simplifying the equations considerably.
</p>
<p>7This is not entirely true! There are orthogonal transformations that are composed of a
rotation followed by a reflection about the origin. See Example 5.5.3.</p>
<p/>
</div>
<div class="page"><p/>
<p>196 6 Spectral Decomposition
</p>
<p>Similarly, the kinetic energy of the rigid rotating body,
</p>
<p>T =
N&sum;
</p>
<p>i=1
</p>
<p>1
</p>
<p>2
miv
</p>
<p>2
i =
</p>
<p>N&sum;
</p>
<p>i=1
</p>
<p>1
</p>
<p>2
mivi &middot; (ωωω&times; ri)
</p>
<p>=
N&sum;
</p>
<p>i=1
</p>
<p>1
</p>
<p>2
miωωω &middot; (ri &times; vi)=
</p>
<p>1
</p>
<p>2
ωωω &middot; L = 1
</p>
<p>2
ωt Iω,
</p>
<p>which in general has off-diagonal terms involving Ixy and so forth, reduces
to a simple form: T = 12Ixxω2x + 12Iyyω2y + 12Izzω2z .
</p>
<p>Example 6.6.8 Another application of Theorem 6.6.6 is in the study of
conic sections. The most general form of the equation of a conic section
is
</p>
<p>a1x
2 + a2y2 + a3xy + a4x + a5y + a6 = 0,
</p>
<p>where a1, . . . , a6 are constants. If the coordinate axes coincide with the prin-
cipal axes of the conic section, the xy term will be absent, and the equation
of the conic section takes the familiar form. On geometrical grounds we
have to be able to rotate xy-coordinates to coincide with the principal axes.
We shall do this using the ideas discussed in this chapter.
</p>
<p>First, we note that the general equation for a conic section can be written
in matrix form as
</p>
<p>(
x y
</p>
<p>)( a1 a3/2
a3/2 a2
</p>
<p>)(
x
</p>
<p>y
</p>
<p>)
+
(
a4 a5
</p>
<p>)(x
y
</p>
<p>)
+ a6 = 0.
</p>
<p>The 2 &times; 2 matrix is symmetric and can therefore be diagonalized by means
of an orthogonal matrix R. Then RtR = 1, and we can write
</p>
<p>(
x y
</p>
<p>)
RtR
</p>
<p>(
a1 a3/2
a3/2 a2
</p>
<p>)
RtR
</p>
<p>(
x
</p>
<p>y
</p>
<p>)
+
(
a4 a5
</p>
<p>)
RtR
</p>
<p>(
x
</p>
<p>y
</p>
<p>)
+ a6 = 0.
</p>
<p>Let
</p>
<p>R
</p>
<p>(
x
</p>
<p>y
</p>
<p>)
=
(
x&prime;
</p>
<p>y&prime;
</p>
<p>)
, R
</p>
<p>(
a1 a3/2
a3/2 a2
</p>
<p>)
Rt =
</p>
<p>(
a&prime;1 0
0 a&prime;2
</p>
<p>)
,
</p>
<p>R
</p>
<p>(
a4
a5
</p>
<p>)
=
(
a&prime;4
a&prime;5
</p>
<p>)
.
</p>
<p>Then we get
</p>
<p>(
x&prime; y&prime;
</p>
<p>)(a&prime;1 0
0 a&prime;2
</p>
<p>)(
x&prime;
</p>
<p>y&prime;
</p>
<p>)
+
(
a&prime;4 a
</p>
<p>&prime;
5
</p>
<p>)(x&prime;
y&prime;
</p>
<p>)
+ a6 = 0;
</p>
<p>or
</p>
<p>a&prime;1x
&prime;2 + a&prime;2y&prime;2 + a&prime;4x&prime; + a&prime;5y&prime; + a6 = 0.
</p>
<p>The cross term has disappeared. The orthogonal matrix R is simply a rota-
tion. In fact, it rotates the original coordinate system to coincide with the
principal axes of the conic section.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.6 Real Spectral Decomposition 197
</p>
<p>Example 6.6.9 In this example we investigate conditions under which a
multivariable function has a maximum or a minimum.
</p>
<p>A point a = (a1, a2, . . . , an) &isin; Rn is a maximum (minimum) of a func-
tion
</p>
<p>f (x1, x2, . . . , xn)&equiv; f (r)
if
</p>
<p>&nabla;f |xi=ai =
(
&part;f
</p>
<p>&part;x1
,
&part;f
</p>
<p>&part;x2
, . . . ,
</p>
<p>&part;f
</p>
<p>&part;xn
</p>
<p>)
</p>
<p>xi=ai
= 0.
</p>
<p>For small xi &minus;ai , the difference f (r)&minus;f (a) is negative (positive). To relate
this difference to the topics of this section, write the Taylor expansion of the
function around a keeping terms up to the second order:
</p>
<p>f (r)= f (a)+
n&sum;
</p>
<p>i=1
(xi &minus; ai)
</p>
<p>(
&part;f
</p>
<p>&part;xi
</p>
<p>)
</p>
<p>r=a
</p>
<p>+ 1
2
</p>
<p>n&sum;
</p>
<p>i,j
</p>
<p>(xi &minus; ai)(xj &minus; aj )
(
</p>
<p>&part;2f
</p>
<p>&part;xi&part;xj
</p>
<p>)
</p>
<p>r=a
+ &middot; &middot; &middot; ,
</p>
<p>or, constructing a column vector out of δi &equiv; xi &minus; ai and a symmetric matrix
Dij out of the second derivatives, we can write
</p>
<p>f (r)= f (a)+ 1
2
</p>
<p>n&sum;
</p>
<p>i,j
</p>
<p>δiδjDij + &middot; &middot; &middot; &rArr; f (r)&minus; f (a)=
1
</p>
<p>2
δtDδ+ &middot; &middot; &middot;
</p>
<p>because the first derivatives vanish. For a to be a minimum point of f , the
RHS of the last equation must be positive for arbitrary δ. This means that D
must be a positive matrix.8 Thus, all its eigenvalues must be positive (Corol-
lary 6.4.10). Similarly, we can show that for a to be a maximum point of f ,
&minus;D must be positive definite. This means that D must have negative eigen-
values.
</p>
<p>extrema of a
</p>
<p>multivariable function
When we specialize the foregoing discussion to two dimensions, we ob-
</p>
<p>tain results that are familiar from calculus. For the function f (x, y) to have
a minimum, the eigenvalues of the matrix
</p>
<p>(
fxx fxy
fyx fyy
</p>
<p>)
</p>
<p>must be positive. The characteristic polynomial
</p>
<p>det
</p>
<p>(
fxx &minus; λ fxy
fyx fyy &minus; λ
</p>
<p>)
= 0 &rArr; λ2 &minus; (fxx +fyy)λ+fxxfyy &minus;f 2xy = 0
</p>
<p>yields two eigenvalues:
</p>
<p>λ1 =
fxx + fyy +
</p>
<p>&radic;
(fxx &minus; fyy)2 + 4f 2xy
</p>
<p>2
,
</p>
<p>8Note that D is already symmetric&mdash;the real analogue of hermitian.</p>
<p/>
</div>
<div class="page"><p/>
<p>198 6 Spectral Decomposition
</p>
<p>λ2 =
fxx + fyy &minus;
</p>
<p>&radic;
(fxx &minus; fyy)2 + 4f 2xy
</p>
<p>2
.
</p>
<p>These eigenvalues will be both positive if
</p>
<p>fxx + fyy &gt;
&radic;
(fxx &minus; fyy)2 + 4f 2xy,
</p>
<p>and both negative if
</p>
<p>fxx + fyy &lt;&minus;
&radic;
(fxx &minus; fyy)2 + 4f 2xy .
</p>
<p>Squaring these inequalities and simplifying yields
</p>
<p>fxxfyy &gt; f
2
xy,
</p>
<p>which shows that fxx and fyy must have the same sign. If they are both
positive (negative), we have a minimum (maximum). This is the familiar
condition for the attainment of extrema by a function of two variables.
</p>
<p>6.6.2 The Case of Real Normal Operators
</p>
<p>The establishment of spectral decomposition for symmetric (self-adjoint)
operators and its diagonalization was fairly straightforward, requiring only
the assurance that the operator had a real eigenvalue, i.e., a one-dimensional
invariant subspace. The general case of a normal operator does not embody
this assurance. Hence, we do not expect a full diagonalization. Nevertheless,
we can explore the minimal invariant subspaces of a normal operator on a
real vector space.
</p>
<p>Let&rsquo;s start with Theorem 6.6.1 and first note that the one dimensional in-
variant subspaces of an operator T consist of vectors belonging to the kernel
of a polynomial of first degree in T; i.e., these subspaces consist of vectors
|u〉 such that
</p>
<p>pλ(T)|u〉 &equiv; (T&minus; λ1)|u〉 = |0〉. (6.21)
</p>
<p>Since TT&dagger; = T&dagger;T, a subspace labeled by λ is invariant under both T and T&dagger;.
Next we note that the same applies to two-dimensional case. The vectors
</p>
<p>|v〉 in the two-dimensional invariant subspaces satisfy
</p>
<p>pα,β(T)|v〉 &equiv;
(
T2 + αT+ β1
</p>
<p>)
|v〉 = |0〉. (6.22)
</p>
<p>Again because of the commutativity of T and T&dagger;, if |v〉 is in a subspace, so
is T&dagger;|v〉, and the subspace is invariant under both T and T&dagger;.
</p>
<p>Denote the subspace consisting of all vectors |u〉 satisfying Eq. (6.21)
by Mλ, and the subspace consisting of all vectors |v〉 satisfying Eq. (6.22)
by Mα,β . We have already seen that Mλ &cap;Mλ&prime; = {|0〉} if λ �= λ&prime;. We further</p>
<p/>
</div>
<div class="page"><p/>
<p>6.6 Real Spectral Decomposition 199
</p>
<p>assume that there is no overlap between Mλs and Mα,βs, i.e., the latter con-
tain no eigenvectors. Now we show the same for two different Mα,βs. Let
|v〉 &isin;Mα,β &cap;Mα&prime;,β &prime; . Then
</p>
<p>(
T2 + αT+ β1
</p>
<p>)
|v〉 = |0〉
</p>
<p>(
T2 + α&prime;T+ β &prime;1
</p>
<p>)
|v〉 = |0〉.
</p>
<p>Subtract the two equations to get
</p>
<p>[(
α&minus; α&prime;
</p>
<p>)
T+
</p>
<p>(
β &minus; β &prime;
</p>
<p>)
1
]
|v〉 = |0〉.
</p>
<p>If α �= α&prime;, then dividing by α &minus; α&prime; leads to an eigenvalue equation implying
that |v〉 must belong to one of the Mλs, which is a contradiction. Therefore,
α = α&prime;, and if β �= β &prime;, then |v〉 = |0〉.
</p>
<p>Now consider the subspace
</p>
<p>M=
(
</p>
<p>r&oplus;
</p>
<p>i=1
Mλi
</p>
<p>)
&oplus;
(
</p>
<p>s&oplus;
</p>
<p>j=1
Mαj ,βj
</p>
<p>)
,
</p>
<p>where {λi}ri=1 exhausts all the distinct eigenvalues and {(αj , βj )}sj=1 ex-
hausts all the distinct pairs corresponding to Eq. (6.22). Both T and T&dagger; leave
M invariant. Therefore, M&perp; is also invariant under T. If M&perp; �= {|0〉}, then
it can be considered as a vector space on its own, and T can find either a
one-dimensional or a two-dimensional invariant subspace. This contradicts
the assumption that both of these are accounted for in the direct sums above.
Hence, we have
</p>
<p>Theorem 6.6.10 Let V be a real vector space and T a normal operator
on V. Let {λi}ri=1 be complete set of the distinct eigenvalues of T and
{(αj , βj )}sj=1 all the distinct pairs labeling the second degree polynomials
of Eq. (6.22). Let Mλi = kerpλi (T) andMαj ,βj = kerpαj ,βj (T) as in (6.21)
and (6.22). Then
</p>
<p>V=
(
</p>
<p>r&oplus;
</p>
<p>i=1
Mλi
</p>
<p>)
&oplus;
(
</p>
<p>s&oplus;
</p>
<p>j=1
Mαj ,βj
</p>
<p>)
,
</p>
<p>where λi, αj , βj &isin;R and α2j &lt; 4βj .
</p>
<p>We now seek bases of V with respect to which T has as simple a repre-
sentation as possible. Let mi denote the dimension of Mλi and {|a(i)k 〉}
</p>
<p>mi
k=1 a
</p>
<p>basis of Mλi . To construct a basis for Mαj ,βj , let |b
(αj ,βj )
</p>
<p>1 〉 be a vector lin-
early independent from |a(i)k 〉 for all i and k. Let |b
</p>
<p>(αj ,βj )
</p>
<p>2 〉 = T|b
(αj ,βj )
</p>
<p>1 〉, and
note that |b(αj ,βj )1 〉 and |b
</p>
<p>(αj ,βj )
</p>
<p>2 〉 are linearly independent from each other
and all the |a(i)k 〉s (why?). Pick |b
</p>
<p>(αj ,βj )
</p>
<p>3 〉 to be linearly independent from all
the previously constructed vectors and let |b(αj ,βj )4 〉 = T|b
</p>
<p>(αj ,βj )
</p>
<p>3 〉. Continue</p>
<p/>
</div>
<div class="page"><p/>
<p>200 6 Spectral Decomposition
</p>
<p>this process until a basis for Mαj ,βj is constructed. Do this for all j . If we
denote the dimension of Mαj ,βj by nj , then
</p>
<p>BV &equiv;
(
</p>
<p>r⋃
</p>
<p>i=1
</p>
<p>{∣∣a(i)k
&rang;}mi
k=1
</p>
<p>)
&cup;
(
</p>
<p>s⋃
</p>
<p>j=1
</p>
<p>{∣∣b(αj ,βj )k
&rang;}nj
k=1
</p>
<p>)
</p>
<p>is a basis for V.
How does the matrix MT of T look like in this basis? We leave it to the
</p>
<p>reader to verify that
</p>
<p>MT = diag(λ11m1, . . . , λr1mr ,Mα1,β1 , . . . ,Mαs ,βs ), (6.23)
</p>
<p>where diag means a block diagonal matrix, 1k is a k&times; k identity matrix, and
</p>
<p>Mαj ,βj = diag(J1, . . . , Jnj ), Jk =
(
</p>
<p>0 &minus;βj
1 &minus;αj
</p>
<p>)
, k = 1, . . . , nj . (6.24)
</p>
<p>In other words, MT has the eigenvalues of T on the main diagonal up to
m1 + &middot; &middot; &middot; +mr and then 2&times; 2 matrices similar to Jk (possibly with different
αj and βj ) for the rest of the diagonal positions.
</p>
<p>Consider any eigenvector |x1〉 of T (if it exists). Obviously, Span{|x1〉} is
a subspace of V invariant under T. By Theorem 6.4.3, Span{|x1〉} reduces T.
Thus, we can write
</p>
<p>V= Span
{
|x1〉
</p>
<p>}
&oplus; Span
</p>
<p>{
|x1〉
</p>
<p>}&perp;
.
</p>
<p>Now pick a new eigenvector |x2〉 in Span{|x1〉}&perp; (if it exists) and write
</p>
<p>V= Span
{
|x1〉
</p>
<p>}
&oplus; Span
</p>
<p>{
|x2〉
</p>
<p>}
&oplus; Span
</p>
<p>{
|x2〉
</p>
<p>}&perp;
.
</p>
<p>Continue this until all the eigenvectors are exhausted (there may be none).
Then, we have
</p>
<p>V=
(
</p>
<p>M&oplus;
</p>
<p>i=1
Span
</p>
<p>{
|xi〉
</p>
<p>}
)
&oplus;
(
</p>
<p>M&oplus;
</p>
<p>i=1
Span
</p>
<p>{
|xi〉
</p>
<p>}
)&perp;
</p>
<p>&equiv;
(
</p>
<p>M&oplus;
</p>
<p>i=1
Span
</p>
<p>{
|xi〉
</p>
<p>}
)
&oplus;W.
</p>
<p>Since a real vector space has minimal invariant subspaces of dimensions
one and two, W contains only two-dimensional subspaces (if any). Let |y1〉
be a nonzero vector in W. Then there is a second degree polynomial of
the type given in Eq. (6.22) whose kernel is the two-dimensional subspace
Span{|y1〉,T|y1〉} of W. This subspace is invariant under T, and by Theo-
rem 6.4.3, it reduces T in W. Thus,
</p>
<p>W= Span
{
|y1〉,T|y1〉
</p>
<p>}
&oplus; Span
</p>
<p>{
|y1〉,T|y1〉
</p>
<p>}&perp;
.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.6 Real Spectral Decomposition 201
</p>
<p>Continuing this process and noting that W does not contain any one-
dimensional invariant subspace, we obtain
</p>
<p>W=
K&oplus;
</p>
<p>j=1
Span
</p>
<p>{
|yj 〉,T|yj 〉
</p>
<p>}
</p>
<p>and hence,
</p>
<p>Real Spectral
</p>
<p>Decomposition TheoremTheorem 6.6.11 (Real Spectral Decomposition) Let V be a real vec-
tor space and T a normal operator on V. Let |xi〉 and |yj 〉 satisfy
Eqs. (6.21) and (6.22), respectively. Then,
</p>
<p>V=
(
</p>
<p>M&oplus;
</p>
<p>i=1
Span
</p>
<p>{
|xi〉
</p>
<p>}
)
&oplus;
(
</p>
<p>K&oplus;
</p>
<p>j=1
Span
</p>
<p>{
|yj 〉,T|yj 〉
</p>
<p>}
)
</p>
<p>(6.25)
</p>
<p>with dimV= 2K +M .
</p>
<p>We have thus written V as a direct sum of one-and two-dimensional sub-
spaces. Either K (e.g., in the case of a real self-adjoint operator) or M (e.g.,
in the case of the operator of Example 6.5.1) could be zero.
</p>
<p>An important application of Theorem 6.6.11 is the spectral decompo-
sition of an orthogonal (isometric) operator. This operator has the property
that OOt = 1. Taking the determinants of both sides, we obtain (detO)2 = 1.
Using Theorem 6.6.11 (or 6.6.10), we see that the representation of O con-
sists of some 1 &times; 1 and some 2 &times; 2 matrices placed along the diagonal.
Furthermore, these matrices are orthogonal (why?). Since the eigenvalues
of an orthogonal operator have absolute value 1 (this is the real version of
the second part of Corollary 6.4.5), a 1 &times; 1 orthogonal matrix can be only
&plusmn;1. An orthogonal 2 &times; 2 matrix is of the forms given in Problem 5.9, i.e.,
</p>
<p>R2(θj )&equiv;
(
</p>
<p>cos θj &minus; sin θj
sin θj cos θj
</p>
<p>)
or
</p>
<p>(
cos θj sin θj
sin θj &minus; cos θj
</p>
<p>)
, (6.26)
</p>
<p>in which the first has a determinant +1 and the second &minus;1. We thus have
the following:
</p>
<p>Theorem 6.6.12 A real orthogonal operator on a real inner product space
V cannot, in general, be completely diagonalized. The closest it can get to a
diagonal form is
</p>
<p>Odiag = diag
(
1,1, . . . ,1︸ ︷︷ ︸
</p>
<p>N+
</p>
<p>,&minus;1,&minus;1, . . . ,&minus;1︸ ︷︷ ︸
N&minus;
</p>
<p>,R2(θ1),R2(θ2), . . . ,R2(θm)
)
,
</p>
<p>where N+ + N&minus; + 2m = dimV and R2(θj ) is as given in (6.26). Further-
more, the matrix that transforms an orthogonal matrix into the form above
is itself an orthogonal matrix.</p>
<p/>
</div>
<div class="page"><p/>
<p>202 6 Spectral Decomposition
</p>
<p>The last statement follows from Theorem 6.5.2 and the fact that an orthog-
onal matrix is the real analogue of a unitary matrix.
</p>
<p>Example 6.6.13 In this example, we illustrate an intuitive (and non-
rigorous) &ldquo;proof&rdquo; of the diagonalization of an orthonormal operator, which
in some sense involves the complexification of a real vector space.
</p>
<p>Think of the orthogonal operator O as a unitary operator.9 Since the abso-
lute value of the eigenvalues of a unitary operator is 1, the only real possibil-
ities are &plusmn;1. To find the other eigenvalues we note that as a unitary operator,
O can be written as eA, where A is anti-hermitian (see Problem 6.23). Since
hermitian conjugation and transposition coincide for real vector spaces, we
conclude that A=&minus;At , and A is antisymmetric. It is also real, because O is.
</p>
<p>Let us now consider the eigenvalues of A. If λ is an eigenvalue of A corre-
sponding to the eigenvector |a〉, then 〈a|A|a〉 = λ〈a|a〉. Taking the complex
conjugate of both sides gives 〈a|A&dagger;|a〉 = λ&lowast;〈a|a〉; but A&dagger; = At = &minus;A, be-
cause A is real and antisymmetric. We therefore have 〈a|A|a〉 = &minus;λ&lowast;〈a|a〉,
which gives λ&lowast; = &minus;λ. It follows that if we restrict λ to be real, then it
can only be zero; otherwise, it must be purely imaginary. Furthermore, the
reader may verify that if λ is an eigenvalue of A, so is &minus;λ. Therefore, the
diagonal form of A looks like this:
</p>
<p>Adiag = diag(0,0, . . . ,0, iθ1,&minus;iθ1, iθ2,&minus;iθ2, . . . , iθk,&minus;iθk),
</p>
<p>which gives O the following diagonal form:
</p>
<p>Odiag = eAdiag = diag
(
e0, e0, . . . , e0, eiθ1, e&minus;iθ1 , eiθ2 , e&minus;iθ2 , . . . , eiθk , e&minus;iθk
</p>
<p>)
</p>
<p>with θ1, θ2, . . . , θk all real. It is clear that if O has &minus;1 as an eigenvalue, then
some of the θ &rsquo;s must equal &plusmn;π . Separating the π &rsquo;s from the rest of θ &rsquo;s and
putting all of the above arguments together, we get
</p>
<p>Odiag = diag
(
</p>
<p>1,1, . . . ,1︸ ︷︷ ︸
N+
</p>
<p>,&minus;1,&minus;1, . . . ,&minus;1︸ ︷︷ ︸
N&minus;
</p>
<p>, eiθ1 , e&minus;iθ1 , eiθ2, e&minus;iθ2 , . . . ,
</p>
<p>eiθm, e&minus;iθm
)
</p>
<p>where N+ +N&minus; + 2m= dim O.
Getting insight from Example 6.5.1, we can argue, admittedly in a non-
</p>
<p>rigorous way, that corresponding to each pair e&plusmn;iθj is a 2 &times; 2 matrix of the
form given in Eq. (6.26).
</p>
<p>We can add more rigor to the preceding example by the process of com-
plexification and the notion of a complex structure. Recall from Eq. (2.22)
that a real 2m-dimensional vector space can be reduced to an m-dimensional
complex space. Now consider the restriction of the orthogonal operator O
on the 2K-dimensional vector subspace W of Eq. (6.25), and let J be a
</p>
<p>9This can always be done by formally identifying transposition with hermitian conjuga-
tion, an identification that holds when the underlying field of numbers is real.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.6 Real Spectral Decomposition 203
</p>
<p>complex structure on that subspace. Let {|fi〉, J|fi〉}Ki=1 be an orthonormal
basis of W and WC1 , the complexification of W1 &equiv; Span{|fi〉}Ki=1. Define
the unitary operator U on W1 by
</p>
<p>U|fj 〉 =O|fj 〉,
</p>
<p>and extend it by linearity and Eq. (2.22), which requires that O and J com-
mute. This replaces the orthogonal operator O on the 2K-dimensional vec-
tor space W with a unitary operator U on the K-dimensional vector space
W1. Thus, we can apply the complex spectral decomposition and replace the
|fi〉 with |ei〉, the eigenvectors of U.
</p>
<p>We now find the matrix representation of O in this new orthonormal basis
from that of U. For j = 1, . . . ,K , we have
</p>
<p>O|ej 〉 = U|ej 〉 = eiθj |ej 〉 = (cos θj + i sin θj )|ej 〉
= (cos θj1+ sin θj J)|ej 〉 = cos θj |ej 〉 + sin θj |ej+1〉
</p>
<p>O|ej+1〉 =OJ|ej 〉 = JO|ej 〉 = iU|ej 〉 = ieiθj |ej 〉
= (i cos θj &minus; sin θj )|ej 〉 = (cos θj J&minus; sin θj1)|ej 〉
= &minus; sin θj |ej 〉 + cos θj |ej+1〉.
</p>
<p>Thus the j th and j + 1st columns will be of the form
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>0 0
...
</p>
<p>...
</p>
<p>0 0
cos θj &minus; sin θj
sin θj cos θj
</p>
<p>0 0
...
</p>
<p>...
</p>
<p>0 0
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>.
</p>
<p>Putting all the columns together reproduces the result of Theorem 6.6.12.
</p>
<p>Example 6.6.14 An interesting application of Theorem 6.6.12 occurs in
classical mechanics, where it is shown that the motion of a rigid body con-
sists of a translation and a rotation. The rotation is represented by a 3 &times; 3
orthogonal matrix. Theorem 6.6.12 states that by an appropriate choice of
coordinate systems (i.e., by applying the same orthogonal transformation
that diagonalizes the rotation matrix of the rigid body), one can &ldquo;diagonal-
ize&rdquo; the 3 &times; 3 orthogonal matrix. The &ldquo;diagonal&rdquo; form is
</p>
<p>⎛
⎝
&plusmn;1 0 0
0 &plusmn;1 0
0 0 &plusmn;1
</p>
<p>⎞
⎠ or
</p>
<p>⎛
⎝
&plusmn;1 0 0
0 cos θ &minus; sin θ
0 sin θ cos θ
</p>
<p>⎞
⎠ .</p>
<p/>
</div>
<div class="page"><p/>
<p>204 6 Spectral Decomposition
</p>
<p>Excluding the reflections (corresponding to &minus;1&rsquo;s) and the trivial identity
rotation, we conclude that any rotation of a rigid body can be written as
</p>
<p>⎛
⎝
</p>
<p>1 0 0
0 cos θ &minus; sin θ
0 sin θ cos θ
</p>
<p>⎞
⎠ ,
</p>
<p>which is a rotation through the angle θ about the (new) x-axis.
</p>
<p>Combining the rotation of the example above with the translations, we
obtain the following theorem.
</p>
<p>Theorem 6.6.15 (Euler) The general motion of a rigid body consists ofEuler Theorem
the translation of one point of that body and a rotation about a single axis
through that point.
</p>
<p>Example 6.6.16 As a final example of the application of the results of this
section, let us evaluate the n-fold integral
</p>
<p>In =
&int; &infin;
</p>
<p>&minus;&infin;
dx1
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
dx2 . . .
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
dxne
</p>
<p>&minus;&sum;ni,j=1 mij xixj , (6.27)
</p>
<p>where the mij are elements of a real, symmetric, positive definite matrix,
say M. Because it is symmetric, M can be diagonalized by an orthogonal
matrix R so that RMRt = D is a diagonal matrix whose diagonal entries are
the eigenvalues, λ1, λ2, . . . , λn, of M, whose positive definiteness ensures
that none of these eigenvalues is zero or negative.
</p>
<p>The exponent in (6.27) can be written as
</p>
<p>n&sum;
</p>
<p>i,j=1
mijxixj = xtMx = xtRtRMRtRx = x&prime;tDx&prime; = λ1x&prime;21 + &middot; &middot; &middot; + λnx&prime;2n ,
</p>
<p>where
</p>
<p>x&prime; =
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>x&prime;1
x&prime;2
...
</p>
<p>x&prime;n
</p>
<p>⎞
⎟⎟⎟⎠= Rx = R
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>x1
x2
...
</p>
<p>xn
</p>
<p>⎞
⎟⎟⎟⎠ ,
</p>
<p>or, in component form, x&prime;i =
&sum;n
</p>
<p>j=1 rijxj for i = 1,2, . . . , n. Similarly, since
x = Rtx&prime;, it follows that xi =
</p>
<p>&sum;n
j=1 rjix
</p>
<p>&prime;
j for i = 1,2, . . . , n.
</p>
<p>The &ldquo;volume element&rdquo; dx1 &middot; &middot; &middot;dxn is related to the primed volume ele-
ment as follows:
</p>
<p>dx1 &middot; &middot; &middot;dxn =
∣∣∣∣
&part;(x1, x2, . . . , xn)
</p>
<p>&part;(x&prime;1, x
&prime;
2, . . . , x
</p>
<p>&prime;
n)
</p>
<p>∣∣∣∣dx&prime;1 &middot; &middot; &middot;dx&prime;n &equiv; |det J|dx&prime;1 &middot; &middot; &middot;dx&prime;n,
</p>
<p>where J is the Jacobian matrix whose ij th element is &part;xi/&part;x&prime;j . But
</p>
<p>&part;xi
</p>
<p>&part;x&prime;j
= rji &rArr; J = Rt &rArr; |det J| = |det Rt | = 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.7 Polar Decomposition 205
</p>
<p>Therefore, in terms of x&prime;, the integral In becomes
</p>
<p>In =
&int; &infin;
</p>
<p>&minus;&infin;
dx&prime;1
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
dx&prime;2 &middot; &middot; &middot;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
dx&prime;ne
</p>
<p>&minus;λ1x&prime;21 &minus;λ2x&prime;22 &minus;&middot;&middot;&middot;&minus;λnx&prime;2n
</p>
<p>=
(&int; &infin;
</p>
<p>&minus;&infin;
dx&prime;1e
</p>
<p>&minus;λ1x&prime;21
)(&int; &infin;
</p>
<p>&minus;&infin;
dx&prime;2e
</p>
<p>&minus;λ2x&prime;22
)
&middot; &middot; &middot;
</p>
<p>(&int; &infin;
</p>
<p>&minus;&infin;
dx&prime;ne
</p>
<p>&minus;λnx&prime;2n
)
</p>
<p>=
&radic;
</p>
<p>π
</p>
<p>λ1
</p>
<p>&radic;
π
</p>
<p>λ2
&middot; &middot; &middot;
</p>
<p>&radic;
π
</p>
<p>λn
= πn/2 1&radic;
</p>
<p>λ1λ2 &middot; &middot; &middot;λn
= πn/2(det M)&minus;1/2,
</p>
<p>because the determinant of a matrix is the product of its eigenvalues. This
result can be written as
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
dnxe&minus;x
</p>
<p>tMx = πn/2(det M)&minus;1/2,
</p>
<p>which gives an analytic definition of the determinant. analytic definition of the
determinant of a matrix
</p>
<p>Proposition 6.6.17 The determinant of a positive definite matrix M is
given by
</p>
<p>det M = π
n
</p>
<p>(
&int;&infin;
&minus;&infin; d
</p>
<p>nxe&minus;xtMx)2
.
</p>
<p>6.7 Polar Decomposition
</p>
<p>We have seen many similarities between operators and complex numbers.
For instance, hermitian operators behave very much like the real numbers:
they have real eigenvalues; their squares are positive; every operator can be
written as X + iY, where both X and Y are hermitian; and so forth. Also,
unitary operators can be written as exp(iH), where H is hermitian. So uni-
tary operators are the analogue of complex numbers of unit magnitude such
as eiθ .
</p>
<p>A general complex number z can be written as reiθ , where r = &radic;z&lowast;z.
Can we write an arbitrary operator T in an analogous way? Perhaps as&radic;
T&dagger;T exp(iH), with H hermitian? The following theorem provides the an-
</p>
<p>swer.
</p>
<p>Theorem 6.7.1 (Polar Decomposition) An operator T on a (real or com- polar decomposition
theoremplex) finite-dimensional inner product space can be written as T= UR where
</p>
<p>R is a positive operator and U an isometry (a unitary or orthogonal opera-
tor).
</p>
<p>Proof With insight from the complex number theory, let R=
&radic;
T&dagger;T, where
</p>
<p>the right-hand side is understood as the positive square root. Now note that
</p>
<p>〈Ta|Ta〉 = 〈a|T&dagger;T|a〉 = 〈a|R2|a〉 = 〈a|R&dagger;R|a〉 = 〈Ra|Ra〉</p>
<p/>
</div>
<div class="page"><p/>
<p>206 6 Spectral Decomposition
</p>
<p>because R is positive, and therefore self-adjoint. This shows that T|a〉 and
R|a〉 are connected by an isometry. Since T|a〉 and R|a〉 belong to the ranges
of the two operators, this isometry can be defined only on those ranges.
</p>
<p>Define the linear (reader, verify!) isometry U : R(V)&rarr; T(V) by UR|x〉 =
T|x〉 for |x〉 &isin; V, and note that by its very definition, U is surjective. First
we have to make sure that U is well defined, i.e., it does not map the same
vector onto two different vectors. This is a legitimate concern, because R
may not be injective, and two different vectors of V may be mapped by R
onto the same vector. So, assume that R|a1〉 = R|a2〉. Then
</p>
<p>UR|a1〉 = UR|a2〉 &rArr; T|a1〉 = T|a2〉.
</p>
<p>Hence, U is well defined.
Next note that any linear isometry is injective (Theorem 2.3.12). There-
</p>
<p>fore, U is invertible and R(V)&perp; &sim;= T(V)&perp;. To complete the proof, let {|ei〉}mi=1
be an orthonormal basis of R(V)&perp; and {|fi〉}mi=1 an orthonormal basis of
T(V)&perp; and extend U by setting U|ei〉 = |fi〉. �
</p>
<p>We note that if T is injective, then R is invertible, and therefore, unique.
However, U is not unique, because for any isometry S : T(V)&rarr; T(V), the
operator S ◦U works just as well in the proof.
</p>
<p>It is interesting to note that the positivity of R and the nonuniqueness of
U are the analogue of the positivity of r and the nonuniqueness of eiθ in the
polar representation of complex numbers:
</p>
<p>z= reiθ = rei(θ+2nπ) &forall;n &isin; Z.
</p>
<p>In practice, R is found by spectrally decomposing T&dagger;T and taking its pos-
itive square root.10 Once R is found, U can be calculated from the definition
T= UR. This last step is especially simple if T is injective.
</p>
<p>Example 6.7.2 Let us find the polar decomposition of
</p>
<p>A =
(
&minus;2i
</p>
<p>&radic;
7
</p>
<p>0 3
</p>
<p>)
.
</p>
<p>We have
</p>
<p>R2 = A&dagger;A =
(
</p>
<p>2i 0&radic;
7 3
</p>
<p>)(
&minus;2i
</p>
<p>&radic;
7
</p>
<p>0 3
</p>
<p>)
=
(
</p>
<p>4 2i
&radic;
</p>
<p>7
&minus;2i
</p>
<p>&radic;
7 16
</p>
<p>)
.
</p>
<p>The eigenvalues and eigenvectors of R2 are routinely found to be
</p>
<p>λ1 = 18, λ2 = 2, |e1〉 =
1
</p>
<p>2
&radic;
</p>
<p>2
</p>
<p>(
i&radic;
7
</p>
<p>)
, |e2〉 =
</p>
<p>1
</p>
<p>2
&radic;
</p>
<p>2
</p>
<p>(&radic;
7
i
</p>
<p>)
.
</p>
<p>10It is important to pay attention to the order of the two operators: One decomposes T&dagger;T,
not TT&dagger;.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.7 Polar Decomposition 207
</p>
<p>The projection matrices are
</p>
<p>P1 = |e1〉〈e1| =
1
</p>
<p>8
</p>
<p>(
1 i
</p>
<p>&radic;
7
</p>
<p>&minus;i
&radic;
</p>
<p>7 7
</p>
<p>)
,
</p>
<p>P2 = |e2〉〈e2| =
1
</p>
<p>8
</p>
<p>(
7 &minus;i
</p>
<p>&radic;
7
</p>
<p>i
&radic;
</p>
<p>7 1
</p>
<p>)
.
</p>
<p>Thus,
</p>
<p>R =
&radic;
λ1P1 +
</p>
<p>&radic;
λ2P2 =
</p>
<p>1
</p>
<p>4
</p>
<p>(
5
&radic;
</p>
<p>2 i
&radic;
</p>
<p>14
&minus;i
</p>
<p>&radic;
14 11
</p>
<p>&radic;
2
</p>
<p>)
.
</p>
<p>To find U, we note that det A is nonzero. Hence, A is invertible, which im-
plies that R is also invertible. The inverse of R is
</p>
<p>R&minus;1 = 1
24
</p>
<p>(
11
</p>
<p>&radic;
2 &minus;i
</p>
<p>&radic;
14
</p>
<p>i
&radic;
</p>
<p>14 5
&radic;
</p>
<p>2
</p>
<p>)
.
</p>
<p>The unitary matrix is simply
</p>
<p>U = AR&minus;1 = 1
24
</p>
<p>(&minus;i15
&radic;
</p>
<p>2 3
&radic;
</p>
<p>14
3i
&radic;
</p>
<p>14 15
&radic;
</p>
<p>2
</p>
<p>)
.
</p>
<p>It is left for the reader to verify that U is indeed unitary.
</p>
<p>Example 6.7.3 Let us decompose the following real matrix into its polar
form:
</p>
<p>A =
(
</p>
<p>2 0
3 &minus;2
</p>
<p>)
.
</p>
<p>The procedure is the same as in the complex case. We have
</p>
<p>R2 = AtA =
(
</p>
<p>2 3
0 &minus;2
</p>
<p>)(
2 0
3 &minus;2
</p>
<p>)
=
(
</p>
<p>13 &minus;6
&minus;6 4
</p>
<p>)
</p>
<p>with eigenvalues λ1 = 1 and λ2 = 16 and normalized eigenvectors
</p>
<p>|e1〉 =
1&radic;
5
</p>
<p>(
1
2
</p>
<p>)
and |e2〉 =
</p>
<p>1&radic;
5
</p>
<p>(
2
&minus;1
</p>
<p>)
.
</p>
<p>The projection operators are
</p>
<p>P1 = |e1〉〈e1| =
1
</p>
<p>5
</p>
<p>(
1 2
2 4
</p>
<p>)
, P2 = |e2〉〈e2| =
</p>
<p>1
</p>
<p>5
</p>
<p>(
4 &minus;2
&minus;2 1
</p>
<p>)
.
</p>
<p>Thus, we have
</p>
<p>R =
&radic;
</p>
<p>R2 =
&radic;
λ1P1 +
</p>
<p>&radic;
λ2P2
</p>
<p>= 1
5
</p>
<p>(
1 2
2 4
</p>
<p>)
+ 4
</p>
<p>5
</p>
<p>(
4 &minus;2
&minus;2 1
</p>
<p>)
= 1
</p>
<p>5
</p>
<p>(
17 &minus;6
&minus;6 8
</p>
<p>)
.</p>
<p/>
</div>
<div class="page"><p/>
<p>208 6 Spectral Decomposition
</p>
<p>We note that A is invertible. Thus, R is also invertible, and
</p>
<p>R&minus;1 = 1
20
</p>
<p>(
8 6
6 17
</p>
<p>)
.
</p>
<p>This gives O = AR&minus;1, or
</p>
<p>O = 1
5
</p>
<p>(
4 3
3 &minus;4
</p>
<p>)
.
</p>
<p>It is readily verified that O is indeed orthogonal.
</p>
<p>6.8 Problems
</p>
<p>6.1 Let P be the (hermitian) projection operator onto a subspace M. Show
that 1&minus; P projects onto M&perp;. Hint: You need to show that 〈m|P|a〉 = 〈m|a〉
for arbitrary |a〉 &isin; V and |m〉 &isin;M; therefore, consider 〈m|P|a〉&lowast;, and use the
hermiticity of P.
</p>
<p>6.2 Show that a subspace M of an inner product space V is invariant under
the linear operator A if and only if M&perp; is invariant under A&dagger;.
</p>
<p>6.3 Show that the intersection of two invariant subspaces of an operator is
also an invariant subspace.
</p>
<p>6.4 Let π be a permutation of the integers {1,2, . . . , n}. Find the spectrum
of Aπ , if for |x〉 = (α1, α2, . . . , αn) &isin;Cn, we define
</p>
<p>Aπ |x〉 = (απ(1), . . . , απ(n)).
</p>
<p>6.5 Let |a1〉 &equiv; a1 = (1,1,&minus;1) and |a2〉 &equiv; a2 = (&minus;2,1,&minus;1).
(a) Construct (in the form of a matrix) the projection operators P1 and P2
</p>
<p>that project onto the directions of |a1〉 and |a2〉, respectively. Verify
that they are indeed projection operators.
</p>
<p>(b) Construct (in the form of a matrix) the operator P= P1+P2 and verify
directly that it is a projection operator.
</p>
<p>(c) Let P act on an arbitrary vector (x, y, z). What is the dot product of
the resulting vector with the vector a1 &times; a2? Is that what you expect?
</p>
<p>6.6 Show that
</p>
<p>(a) the coefficient of λN in the characteristic polynomial of any linear
operator is (&minus;1)N , where N = dimV, and
</p>
<p>(b) the constant in the characteristic polynomial of an operator is its de-
terminant.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.8 Problems 209
</p>
<p>6.7 Operators A and B satisfy the commutation relation [A,B] = 1. Let |b〉
be an eigenvector of B with eigenvalue λ. Show that e&minus;τA|b〉 is also an
eigenvector of B, but with eigenvalue λ+ τ . This is why e&minus;τA is called the
translation operator for B. Hint: First find [B, e&minus;τA]. translation operator
</p>
<p>6.8 Find the eigenvalues of an involutive operator, that is, an operator A
with the property A2 = 1.
</p>
<p>6.9 Assume that A and A&prime; are similar matrices. Show that they have the
same eigenvalues.
</p>
<p>6.10 In each of the following cases, determine the counterclockwise rota-
tion of the xy-axes that brings the conic section into the standard form and
determine the conic section.
</p>
<p>(a) 11x2 + 3y2 + 6xy &minus; 12 = 0,
</p>
<p>(b) 5x2 &minus; 3y2 + 6xy + 6 = 0,
</p>
<p>(c) 2x2 &minus; y2 &minus; 4xy &minus; 3 = 0,
</p>
<p>(d) 6x2 + 3y2 &minus; 4xy &minus; 7 = 0,
</p>
<p>(e) 2x2 + 5y2 &minus; 4xy &minus; 36 = 0.
</p>
<p>6.11 Show that if A is invertible, then the eigenvectors of A&minus;1 are the same
as those of A and the eigenvalues of A&minus;1 are the reciprocals of those of A.
</p>
<p>6.12 Find all eigenvalues and eigenvectors of the following matrices:
</p>
<p>A1 =
(
</p>
<p>1 1
0 i
</p>
<p>)
B1 =
</p>
<p>(
0 1
0 0
</p>
<p>)
C1 =
</p>
<p>⎛
⎝
</p>
<p>2 &minus;2 &minus;1
&minus;1 3 1
2 &minus;4 &minus;1
</p>
<p>⎞
⎠
</p>
<p>A2 =
</p>
<p>⎛
⎝
</p>
<p>1 0 1
0 1 0
1 0 1
</p>
<p>⎞
⎠ B2 =
</p>
<p>⎛
⎝
</p>
<p>1 1 0
1 0 1
0 1 1
</p>
<p>⎞
⎠ C2 =
</p>
<p>⎛
⎝
&minus;1 1 1
1 &minus;1 1
1 1 &minus;1
</p>
<p>⎞
⎠
</p>
<p>A3 =
</p>
<p>⎛
⎝
</p>
<p>1 1 1
0 1 1
0 0 1
</p>
<p>⎞
⎠ B3 =
</p>
<p>⎛
⎝
</p>
<p>1 1 1
1 1 1
1 1 1
</p>
<p>⎞
⎠ C3 =
</p>
<p>⎛
⎝
</p>
<p>0 1 1
1 0 1
1 1 0
</p>
<p>⎞
⎠
</p>
<p>6.13 Show that a 2&times;2 rotation matrix does not have a real eigenvalue (and,
therefore, eigenvector) when the rotation angle is not an integer multiple
of π . What is the physical interpretation of this?
</p>
<p>6.14 Three equal point masses are located at (a, a,0), (a,0, a), and
(0, a, a). Find the moment of inertia matrix as well as its eigenvalues and
the corresponding eigenvectors.
</p>
<p>6.15 Consider (α1, α2, . . . , αn) &isin; Cn and define Eij as the operator that in-
terchanges αi and αj . Find the eigenvalues of this operator.</p>
<p/>
</div>
<div class="page"><p/>
<p>210 6 Spectral Decomposition
</p>
<p>6.16 Find the eigenvalues and eigenvectors of the operator &minus;id/dx acting
in the vector space of differentiable functions C1(&minus;&infin;,&infin;).
</p>
<p>6.17 Show that a hermitian operator is positive iff its eigenvalues are posi-
tive.
</p>
<p>6.18 Show that ‖Ax‖ = ‖A&dagger;x‖ if and only if A is normal.
</p>
<p>6.19 What are the spectral decompositions of A&dagger;, A&minus;1, and AA&dagger; for an in-
vertible normal operator A?
</p>
<p>6.20 Consider the matrix
</p>
<p>A =
(
</p>
<p>2 1 + i
1 &minus; i 3
</p>
<p>)
.
</p>
<p>(a) Find the eigenvalues and the orthonormal eigenvectors of A.
(b) Calculate the projection operators (matrices) P1 and P2 and verify that&sum;
</p>
<p>i Pi = 1 and
&sum;
</p>
<p>i λiPi = A.
(c) Find the matrices
</p>
<p>&radic;
A, sin(θA), and cos(θA) and show directly that
</p>
<p>sin2(θA)+ cos2(θA)= 1.
</p>
<p>(d) Is A invertible? If so, find A&minus;1 using spectral decomposition of A.
</p>
<p>6.21 Consider the matrix
</p>
<p>A =
</p>
<p>⎛
⎝
</p>
<p>4 i 1
&minus;i 4 &minus;i
1 i 4
</p>
<p>⎞
⎠ .
</p>
<p>(a) Find the eigenvalues of A. Hint: Try λ= 3 in the characteristic poly-
nomial of A.
</p>
<p>(b) For each λ, find a basis for Mλ the eigenspace associated with the
eigenvalue λ.
</p>
<p>(c) Use the Gram-Schmidt process to orthonormalize the above basis vec-
tors.
</p>
<p>(d) Calculate the projection operators (matrices) Pi for each subspace and
verify that
</p>
<p>&sum;
i Pi = 1 and
</p>
<p>&sum;
i λiPi = A.
</p>
<p>(e) Find the matrices
&radic;
</p>
<p>A, sin(πA/2), and cos(πA/2).
(f) Is A invertible? If so, find the eigenvalues and eigenvectors of A&minus;1.
</p>
<p>6.22 Show that if two hermitian matrices have the same set of eigenvalues,
then they are unitarily related.
</p>
<p>6.23 Prove that corresponding to every unitary operator U acting on a finite-
dimensional vector space, there is a hermitian operator H such that U =
exp(iH).</p>
<p/>
</div>
<div class="page"><p/>
<p>6.8 Problems 211
</p>
<p>6.24 Prove Lemma 6.6.4 by showing that
</p>
<p>〈a|T2 + αT+ β1|a〉 &ge; ‖Ta‖2 &minus; |α|‖Ta‖‖a‖ + β〈a|a〉,
</p>
<p>which can be obtained from the Schwarz inequality in the form |〈a|b〉| &ge;
&minus;‖a‖‖b‖. Now complete the square on the right-hand side.
</p>
<p>6.25 Show that a normal operator T on a real vector space can be diagonal-
ized as in Eqs. (6.23) and (6.24).
</p>
<p>6.26 Show that an arbitrary matrix A can be &ldquo;diagonalized&rdquo; as D = UAV,
where U is unitary and D is a real diagonal matrix with only nonnegative
eigenvalues. Hint: There exists a unitary matrix that diagonalizes AA&dagger;.
</p>
<p>6.27 Find the polar decomposition of the following matrices:
</p>
<p>A =
(
</p>
<p>2i 0&radic;
7 3
</p>
<p>)
, B =
</p>
<p>(
41 &minus;12i
12i 34
</p>
<p>)
, C =
</p>
<p>⎛
⎝
</p>
<p>1 0 1
0 1 &minus;i
1 i 0
</p>
<p>⎞
⎠ .
</p>
<p>6.28 Show that for an arbitrary matrix A, both AA&dagger; and A&dagger;A have the same
set of eigenvalues. Hint: Use the polar decomposition theorem.
</p>
<p>6.29 Show that
</p>
<p>(a) if λ is an eigenvalue of an antisymmetric operator, then so is &minus;λ, and
(b) antisymmetric operators (matrices) of odd dimension cannot be invert-
</p>
<p>ible.
</p>
<p>6.30 Find the unitary matrices that diagonalize the following hermitian ma-
trices:
</p>
<p>A1 =
(
</p>
<p>2 &minus;1 + i
&minus;1 &minus; i &minus;1
</p>
<p>)
, A2 =
</p>
<p>(
3 i
&minus;i 3
</p>
<p>)
, A3 =
</p>
<p>(
1 &minus;i
i 0
</p>
<p>)
,
</p>
<p>B1 =
</p>
<p>⎛
⎝
</p>
<p>1 &minus;1 &minus;i
&minus;1 0 i
i &minus;i &minus;1
</p>
<p>⎞
⎠ , B2 =
</p>
<p>⎛
⎝
</p>
<p>2 0 i
0 &minus;1 &minus;i
&minus;i i 0
</p>
<p>⎞
⎠ .
</p>
<p>Warning! You may have to resort to numerical approximations for some of
these.
</p>
<p>6.31 Let A =
( 1 α
</p>
<p>0 1
</p>
<p>)
, where α &isin; C and α �= 0. Show that it is impossible to
</p>
<p>find an invertible 2 &times; 2 matrix R such that RAR&minus;1 is diagonal. Now show
that A is not normal as expected from Proposition 6.4.11.</p>
<p/>
</div>
<div class="page"><p/>
<p>Part II
</p>
<p>Infinite-Dimensional Vector Spaces</p>
<p/>
</div>
<div class="page"><p/>
<p>7Hilbert Spaces
</p>
<p>The basic concepts of finite-dimensional vector spaces introduced in Chap. 2
can readily be generalized to infinite dimensions. The definition of a vector
space and concepts of linear combination, linear independence, subspace,
span, and so forth all carry over to infinite dimensions. However, one thing
is crucially different in the new situation, and this difference makes the study
of infinite-dimensional vector spaces both richer and more nontrivial: In a
finite-dimensional vector space we dealt with finite sums; in infinite dimen-
sions we encounter infinite sums. Thus, we have to investigate the conver-
gence of such sums.
</p>
<p>7.1 The Question of Convergence
</p>
<p>The intuitive notion of convergence acquired in calculus makes use of the
idea of closeness. This, in turn, requires the notion of distance.1 We consid-
ered such a notion in Chap. 2 in the context of a norm, and saw that the inner
product had an associated norm. However, it is possible to introduce a norm
on a vector space without an inner product.
</p>
<p>One such norm, applicable to Cn and Rn, was
</p>
<p>‖a‖p &equiv;
(
</p>
<p>n&sum;
</p>
<p>i=1
|αi |p
</p>
<p>)1/p
,
</p>
<p>where p is an integer. The &ldquo;natural&rdquo; norm, i.e., that induced on Cn (or Rn)
by the usual inner product, corresponds to p = 2. The distance between
two points depends on the particular norm used. For example, consider the
&ldquo;point&rdquo; (or vector) |b〉 = (0.1,0.1, . . . ,0.1) in a 1000-dimensional space
(n = 1000). One can easily check that the distance of this vector from the
origin varies considerably with p: ‖b‖1 = 100, ‖b‖2 = 3.16, ‖b‖10 = 0.2.
</p>
<p>Closeness is a relative
</p>
<p>concept!
This variation may give the impression that there is no such thing as &ldquo;close-
ness&rdquo;, and it all depends on how one defines the norm. This is not true,
</p>
<p>1It is possible to introduce the idea of closeness abstractly, without resort to the notion
of distance, as is done in topology. However, distance, as applied in vector spaces, is as
abstract as we want to get.
</p>
<p>S. Hassani, Mathematical Physics, DOI 10.1007/978-3-319-01195-0_7,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>215</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-01195-0_7">http://dx.doi.org/10.1007/978-3-319-01195-0_7</a></div>
</div>
<div class="page"><p/>
<p>216 7 Hilbert Spaces
</p>
<p>because closeness is a relative concept: One always compares distances.
A norm with large p shrinks all distances of a space, and a norm with small
p stretches them. Thus, although it is impossible (and meaningless) to say
that &ldquo;|a〉 is close to |b〉&rdquo; because of the dependence of distance on p, one
can always say &ldquo;|a〉 is closer to |b〉 than |c〉 is to |d〉&rdquo;, regardless of the value
of p.
</p>
<p>Now that we have a way of telling whether vectors are close together
or far apart, we can talk about limits and the convergence of sequences of
vectors. Let us begin by recalling the definition of a Cauchy sequence (see
Definition 1.3.4):
</p>
<p>Definition 7.1.1 An infinite sequence of vectors {|ai〉}&infin;i=1 in a normed lin-Cauchy sequence
defined ear space V is called a Cauchy sequence if lim i &rarr;&infin;
</p>
<p>j &rarr;&infin;
‖ai &minus; aj‖ = 0.
</p>
<p>A convergent sequence is necessarily Cauchy. This can be shown using
the triangle inequality (see Problem 7.2). However, there may be Cauchy
sequences in a given vector space that do not converge to any vector in
that space (see the example below). Such a convergence requires additional
properties of a vector space summarized in the following definition.
</p>
<p>Definition 7.1.2 A complete vector space V is a normed linear space for
complete vector space
</p>
<p>defined
which every Cauchy sequence of vectors in V has a limit vector in V. In
other words, if {|ai〉}&infin;i=1 is a Cauchy sequence, then there exists a vector
|a〉 &isin; V such that limi&rarr;&infin; ‖ai &minus; a‖ = 0.
</p>
<p>Example 7.1.3 (1) R is complete with respect to the absolute-value norm
‖α‖ = |α|. In other words, every Cauchy sequence of real numbers has a
limit in R. This is proved in real analysis.
(2) C is complete with respect to the norm ‖α‖ = |α| =
</p>
<p>&radic;
(Reα)2 + (Imα)2.
</p>
<p>Using |α| &le; |Reα| + | Imα|, one can show that the completeness of C fol-
lows from that of R. Details are left as an exercise for the reader.
(3) The set of rational numbers Q is not complete with respect to the
absolute-value norm. In fact, {(1+1/k)k}&infin;k=1 is a sequence of rational num-
bers that is Cauchy but does not converge to a rational number; it converges
to e, the base of the natural logarithm, which is known to be an irrational
number. (See also the discussion after Definition 1.3.4.)
</p>
<p>Let {|ai〉}&infin;i=1 be a Cauchy sequence of vectors in a finite-dimensional
vector space VN . Choose an orthonormal basis {|ek〉}Nk=1 in VN such that2
|ai〉 =
</p>
<p>&sum;N
k=1 α
</p>
<p>(i)
k |ek〉 and |aj 〉 =
</p>
<p>&sum;N
k=1 α
</p>
<p>(j)
</p>
<p>k |ek〉. Then
</p>
<p>2Recall that one can always define an inner product on a finite-dimensional vector space.
So, the existence of orthonormal bases is guaranteed.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.1 The Question of Convergence 217
</p>
<p>‖ai &minus; aj‖2 = 〈ai &minus; aj |ai &minus; aj 〉 =
∥∥∥∥∥
</p>
<p>N&sum;
</p>
<p>k=1
</p>
<p>(
α
(i)
k &minus; α
</p>
<p>(j)
k
</p>
<p>)
|ek〉
</p>
<p>∥∥∥∥∥
</p>
<p>2
</p>
<p>=
N&sum;
</p>
<p>k,l=1
</p>
<p>(
α
(i)
k &minus; α
</p>
<p>(j)
k
</p>
<p>)&lowast;(
α
(i)
l &minus; α
</p>
<p>(j)
l
</p>
<p>)
〈ek|el〉 =
</p>
<p>N&sum;
</p>
<p>k=1
</p>
<p>∣∣α(i)k &minus; α
(j)
k
</p>
<p>∣∣2.
</p>
<p>The LHS goes to zero, because the sequence is assumed Cauchy. Further-
more, all terms on the RHS are positive. Thus, they too must go to zero
as i, j &rarr;&infin;. By the completeness of C, there must exist αk &isin; C such that
limn&rarr;&infin; α
</p>
<p>(n)
k = αk for k = 1,2, . . . ,N . Now consider |a〉 &isin; VN given by
</p>
<p>|a〉 =&sum;Nk=1 αk|ek〉. We claim that |a〉 is the limit of the above sequence of
vectors in VN . Indeed,
</p>
<p>lim
i&rarr;&infin;
</p>
<p>‖ai &minus; a‖2 = lim
i&rarr;&infin;
</p>
<p>N&sum;
</p>
<p>k=1
</p>
<p>∣∣α(i)k &minus; αk
∣∣2 =
</p>
<p>N&sum;
</p>
<p>k=1
lim
i&rarr;&infin;
</p>
<p>∣∣α(i)k &minus; αk
∣∣2 = 0.
</p>
<p>We have proved the following:
</p>
<p>all finite-dimensional
</p>
<p>vector spaces are
</p>
<p>complete
</p>
<p>Proposition 7.1.4 Every Cauchy sequence in a finite-dimensional in-
ner product space over C (or R) is convergent. In other words, every
finite-dimensional complex (or real) inner product space is complete
with respect to the norm induced by its inner product.
</p>
<p>The next example shows how important the word &ldquo;finite&rdquo; is.
</p>
<p>Example 7.1.5 Consider {fk}&infin;k=1, the infinite sequence of continuous func-
tions defined in the interval [&minus;1,+1] by
</p>
<p>fk(x)=
</p>
<p>⎧
⎪⎨
⎪⎩
</p>
<p>1 if 1/k &le; x &le; 1,
(kx + 1)/2 if &minus;1/k &le; x &le; 1/k,
0 if &minus;1 &le; x &le;&minus;1/k.
</p>
<p>This sequence belongs to C0(&minus;1,1), the inner product space of continu-
ous functions with its usual inner product: 〈f |g〉 =
</p>
<p>&acute; 1
&minus;1 f
</p>
<p>&lowast;(x)g(x) dx. It is
</p>
<p>straightforward to verify that ‖fk&minus;fj‖2 =
&acute; 1
&minus;1 |fk(x)&minus;fj (x)|2dx &minus;&minus;&minus;&minus;&rarr;
</p>
<p>k,j&rarr;&infin;
0. Therefore, the sequence is Cauchy. However, the limit of this sequence is
(see Fig. 7.1)
</p>
<p>f (x)=
{
</p>
<p>1 if 0 &lt; x &le; 1,
0 if &minus;1 &le; x &lt; 0,
</p>
<p>which is discontinuous at x = 0 and therefore does not belong to the space
in which the original sequence lies.
</p>
<p>We see that infinite-dimensional vector spaces are not generally com-
plete. It is a nontrivial task to show whether or not a given infinite-
dimensional vector space is complete.</p>
<p/>
</div>
<div class="page"><p/>
<p>218 7 Hilbert Spaces
</p>
<p>Fig. 7.1 The limit of the sequence of the continuous functions fk is a discontinuous
function that is 1 for x &gt; 0 and 0 for x &lt; 0
</p>
<p>Any vector space (finite- or infinite-dimensional) contains all finite linear
combinations of the form
</p>
<p>&sum;n
i=1 αi |ai〉 when it contains all the |ai〉&rsquo;s. This
</p>
<p>follows from the very definition of a vector space. However, the situation
is different when n goes to infinity. For the vector space to contain the infi-
nite sum, firstly, the meaning of such a sum has to be clarified, i.e., a norm
and an associated convergence criterion needs to be put in place. Secondly,
the vector space has to be complete with respect to that norm. A complete
normed vector space is called a Banach space. We shall not deal with a gen-
</p>
<p>Banach space
eral Banach space, but only with those spaces whose norms arise naturally
from an inner product. This leads to the following definition:
</p>
<p>Definition 7.1.6 A complete inner product space, commonly denoted by
H, is called a Hilbert space.
</p>
<p>Thus, all finite-dimensional real or complex vector spaces are Hilbert
spaces. However, when we speak of a Hilbert space, we shall usually assume
that it is infinite-dimensional.
</p>
<p>Hilbert space defined
</p>
<p>It is convenient to use orthonormal vectors in studying Hilbert spaces.
So, let us consider an infinite sequence {|ei〉}&infin;i=1 of orthonormal vectors all
belonging to a Hilbert space H. Next, take any vector |f 〉 &isin; H, construct
the complex numbers fi = 〈ei |f 〉, and form the sequence of vectors3
</p>
<p>|fn〉 =
n&sum;
</p>
<p>i=1
fi |ei〉 for n= 1,2, . . . (7.1)
</p>
<p>For the pair of vectors |f 〉 and |fn〉, the Schwarz inequality gives
</p>
<p>∣∣〈f |fn〉
∣∣2 &le; 〈f |f 〉〈fn|fn〉 = 〈f |f 〉
</p>
<p>(
n&sum;
</p>
<p>i=1
|fi |2
</p>
<p>)
, (7.2)
</p>
<p>3We can consider |fn〉 as an &ldquo;approximation&rdquo; to |f 〉, because both share the same com-
ponents along the same set of orthonormal vectors. The sequence of orthonormal vectors
acts very much as a basis. However, to be a basis, an extra condition must be met. We
shall discuss this condition shortly.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.1 The Question of Convergence 219
</p>
<p>where Eq. (7.1) has been used to evaluate 〈fn|fn〉. On the other hand, taking
the inner product of (7.1) with 〈f | yields
</p>
<p>〈f |fn〉 =
n&sum;
</p>
<p>i=1
fi〈f |ei〉 =
</p>
<p>n&sum;
</p>
<p>i=1
fif
</p>
<p>&lowast;
i =
</p>
<p>n&sum;
</p>
<p>i=1
|fi |2.
</p>
<p>Substitution of this in Eq. (7.2) yields the Parseval inequality: Parseval inequality
</p>
<p>n&sum;
</p>
<p>i=1
|fi |2 &le; 〈f |f 〉. (7.3)
</p>
<p>This conclusion is true for arbitrarily large n and can be stated as follows:
</p>
<p>Proposition 7.1.7 Let {|ei〉}&infin;i=1 be an infinite set of orthonormal vectors in
a Hilbert space, H. Let |f 〉 &isin;H and define complex numbers fi = 〈ei |f 〉.
Then the Bessel inequality holds:
</p>
<p>&sum;&infin;
i=1 |fi |2 &le; 〈f |f 〉. Bessel inequality
</p>
<p>The Bessel inequality shows that the vector
</p>
<p>&infin;&sum;
</p>
<p>i=1
fi |ei〉 &equiv; lim
</p>
<p>n&rarr;&infin;
</p>
<p>n&sum;
</p>
<p>i=1
fi |ei〉
</p>
<p>converges; that is, it has a finite norm. However, the inequality does not say
whether the vector converges to |f 〉. To make such a statement we need
completeness:
</p>
<p>Definition 7.1.8 A sequence of orthonormal vectors {|ei〉}&infin;i=1 in a Hilbert complete orthonormal
sequence of vectors;
</p>
<p>basis forH
</p>
<p>space H is called complete if the only vector in H that is orthogonal to all
the |ei〉 is the zero vector, in which case {|ei〉}&infin;i=1 is called a basis for H.
</p>
<p>The notion of completeness does not enter the discussion of an N -
dimensional vector space, because any N orthonormal vectors form a basis.
If you take away some of the vectors, you don&rsquo;t have a basis, because you
have less than N vectors. The situation is different in infinite dimensions. If
you start with a basis and take away some of the vectors, you still have an
infinite number of orthonormal vectors. The notion of completeness ensures
that no orthonormal vector is taken out of a basis. This completeness prop-
erty is the extra condition alluded to (in the footnote) above, and is what is
required to make a basis.
</p>
<p>In mathematics literature, one distinguishes between a general and a sep-
arable Hilbert space. The latter is characterized by having a countable basis.
Thus, in the definition above, the Hilbert space is actually a separable one,
and from now on, by Hilbert space we shall mean a separable Hilbert space.
</p>
<p>Proposition 7.1.9 Let {|ei〉}&infin;i=1 be an orthonormal sequence in H. Then
the following statements are equivalent:
</p>
<p>1. {|ei〉}&infin;i=1 is complete.
2. |f 〉 =&sum;&infin;i=1 |ei〉〈ei |f 〉 &forall;|f 〉 &isin;H.</p>
<p/>
</div>
<div class="page"><p/>
<p>220 7 Hilbert Spaces
</p>
<p>3.
&sum;&infin;
</p>
<p>i=1 |ei〉〈ei | = 1.
4. 〈f |g〉 =&sum;&infin;i=1〈f |ei〉〈ei |g〉 &forall;|f 〉, |g〉 &isin;H.
5. ‖f ‖2 =&sum;&infin;i=1 |〈ei |f 〉|2 &forall;|f 〉 &isin;H.
</p>
<p>Proof We shall prove the implications 1 &rArr; 2 &rArr; 3 &rArr; 4 &rArr; 5 &rArr; 1.
1 &rArr; 2: It is sufficient to show that the vector |ψ〉 &equiv; |f 〉&minus;&sum;&infin;i=1 |ei〉〈ei |f 〉
</p>
<p>is orthogonal to all the |ej 〉:
</p>
<p>〈ej |ψ〉 = 〈ej |f 〉 &minus;
&infin;&sum;
</p>
<p>i=1
</p>
<p>δij︷ ︸︸ ︷
〈ej |ei〉〈ei |f 〉 = 0.
</p>
<p>2 &rArr; 3: Since |f 〉 = 1|f 〉 =&sum;&infin;i=1(|ei〉〈ei |)|f 〉 is true for all |f 〉 &isin;H, we
must have 1=&sum;&infin;i=1 |ei〉〈ei |.
</p>
<p>3 &rArr; 4: 〈f |g〉 = 〈f |1|g〉 = 〈f |(&sum;&infin;i=1 |ei〉〈ei |)|g〉 =
&sum;&infin;
</p>
<p>i=1〈f |ei〉〈ei |g〉.
4 &rArr; 5: Let |g〉 = |f 〉 in statement 4 and recall that 〈f |ei〉 = 〈ei |f 〉&lowast;.
5 &rArr; 1: Let |f 〉 be orthogonal to all the |ei〉. Then all the terms in the sum
</p>
<p>are zero implying that ‖f ‖2 = 0, which in turn gives |f 〉 = 0,
because only the zero vector has a zero norm. �
</p>
<p>The equalityParseval equality;
generalized Fourier
</p>
<p>coefficients ‖f ‖2 = 〈f |f 〉 =
&infin;&sum;
</p>
<p>i=1
</p>
<p>∣∣〈ei |f 〉
∣∣2 =
</p>
<p>&infin;&sum;
</p>
<p>i=1
|fi |2, fi = 〈ei |f 〉, (7.4)
</p>
<p>is called the Parseval equality, and the complex numbers fi are called gen-
eralized Fourier coefficients. The relation
</p>
<p>1=
&infin;&sum;
</p>
<p>i=1
|ei〉〈ei | (7.5)
</p>
<p>is called the completeness relation.completeness relation
</p>
<p>Historical Notes
</p>
<p>David Hilbert (1862&ndash;1943), the greatest mathematician of the twentieth century, re-
</p>
<p>David Hilbert 1862&ndash;1943
</p>
<p>ceived his Ph.D. from the University of K&ouml;nigsberg and was a member of the staff there
from 1886 to 1895. In 1895 he was appointed to the chair of mathematics at the University
of G&ouml;ttingen, where he continued to teach for the rest of his life.
Hilbert is one of that rare breed of late 19th-century mathematicians whose spectrum
of expertise covered a wide range, with formal set theory at one end and mathemati-
cal physics at the other. He did superb work in geometry, algebraic geometry, algebraic
number theory, integral equations, and operator theory. The seminal two-volume book
Methoden der mathematische Physik by R. Courant, still one of the best books on the
subject, was greatly influenced by Hilbert.
Hilbert&rsquo;s work in geometry had the greatest influence in that area since Euclid. A system-
atic study of the axioms of Euclidean geometry led Hilbert to propose 21 such axioms, and
he analyzed their significance. He published Grundlagen der Geometrie in 1899, putting
geometry on a formal axiomatic foundation. His famous 23 Paris problems challenged
(and still today challenge) mathematicians to solve fundamental questions.
It was late in his career that Hilbert turned to the subject for which he is most famous
among physicists. A lecture by Erik Holmgren in 1901 on Fredholm&rsquo;s work on integral
equations, which had already been published in Sweden, aroused Hilbert&rsquo;s interest in</p>
<p/>
</div>
<div class="page"><p/>
<p>7.2 The Space of Square-Integrable Functions 221
</p>
<p>the subject. David Hilbert, having established himself as the leading mathematician of
his time by his work on algebraic numbers, algebraic invariants, and the foundations of
geometry, now turned his attention to integral equations. He says that an investigation
of the subject showed him that it was important for the theory of definite integrals, for
the development of arbitrary functions in series (of special functions or trigonometric
functions), for the theory of linear differential equations, for potential theory, and for the
calculus of variations. He wrote a series of six papers from 1904 to 1910 and reproduced
them in his book Grundz&uuml;ge einer allgemeinen Theorie der linearen Integralgleichungen
(1912). During the latter part of this work he applied integral equations to problems of
mathematical physics.
It is said that Hilbert discovered the correct field equation for general relativity in 1915
(one year before Einstein) using the variational principle, but never claimed priority.
Hilbert claimed that he worked best out-of-doors. He accordingly attached an 18-foot
blackboard to his neighbor&rsquo;s wall and built a covered walkway there so that he could work
outside in any weather. He would intermittently interrupt his pacing and his blackboard
computations with a few turns around the rest of the yard on his bicycle, or he would pull
some weeds, or do some garden trimming. Once, when a visitor called, the maid sent him
to the backyard and advised that if the master wasn&rsquo;t readily visible at the blackboard to
look for him up in one of the trees.
Highly gifted and highly versatile, David Hilbert radiated over mathematics a catching &ldquo;Wir m&uuml;ssen wissen. Wir
</p>
<p>werden wissen.&rdquo;optimism and a stimulating vitality that can only be called &ldquo;the spirit of Hilbert.&rdquo; Engraved
on a stone marker set over Hilbert&rsquo;s grave in G&ouml;ttingen are the master&rsquo;s own optimistic
words: &ldquo;Wir m&uuml;ssen wissen. Wir werden wissen.&rdquo; (&ldquo;We must know. We shall know.&rdquo;)
</p>
<p>7.2 The Space of Square-Integrable Functions
</p>
<p>Chapter 2 showed that the collection of all continuous functions defined on
an interval [a, b] forms a linear vector space. Example 7.1.5 showed that
this space is not complete. Can we enlarge this space to make it complete?
Since we are interested in an inner product as well, and since a natural inner
product for functions is defined in terms of integrals, we want to make sure
that our functions are integrable. However, integrability does not require
continuity, it only requires piecewise continuity. In this section we shall dis-
cuss conditions under which the space of functions becomes complete. An
important class of functions has already been mentioned in Chap. 2. These
functions satisfy the inner product given by
</p>
<p>〈g|f 〉 =
ˆ b
</p>
<p>a
</p>
<p>g&lowast;(x)f (x)w(x)dx.
</p>
<p>If g(x)= f (x), we obtain
</p>
<p>〈f |f 〉 =
ˆ b
</p>
<p>a
</p>
<p>∣∣f (x)
∣∣2w(x)dx. (7.6)
</p>
<p>Functions for which such an integral is defined are said to be square-
integrable.
</p>
<p>square-integrable
</p>
<p>functions
</p>
<p>The space of square-integrable functions over the interval [a, b] is de-
noted by L2w(a, b). In this notation L stands for Lebesgue, who generalized
the notion of the ordinary Riemann integral to cases for which the integrand
could be highly discontinuous; 2 stands for the power of f (x) in the integral;
a and b denote the limits of integration; and w refers to the weight function</p>
<p/>
</div>
<div class="page"><p/>
<p>222 7 Hilbert Spaces
</p>
<p>(a strictly positive real-valued function). When w(x) = 1, we use the no-
tation L2(a, b). The significance of L2w(a, b) lies in the following theorem
(for a proof, see [Reed 80, Chap. III]):
</p>
<p>Theorem 7.2.1 (Riesz-Fischer theorem) The space L2w(a, b) is complete.L2w(a, b) is complete
</p>
<p>A complete infinite-dimensional inner product space was earlier defined
to be a Hilbert space. The following theorem shows that the number of
(separable) Hilbert spaces is severely restricted. (For a proof, see [Frie 82,
p. 216].)
</p>
<p>all Hilbert spaces are
</p>
<p>alike
</p>
<p>Theorem 7.2.2 All complete inner product spaces with countable bases are
isomorphic to L2w(a, b).
</p>
<p>L2w(a, b) is defined in terms of functions that satisfy Eq. (7.6). Yet an
</p>
<p>inner product involves integrals of the form
&acute; b
</p>
<p>a
g&lowast;(x)f (x)w(x)dx. Are such
</p>
<p>integrals well-defined and finite? Using the Schwarz inequality, which holds
for any inner product space, finite or infinite, one can show that the integral
is defined.
</p>
<p>The isomorphism of Theorem 7.2.2 makes the Hilbert space more tan-
gible, because it identifies the space with a space of functions, objects that
are more familiar than abstract vectors. Nonetheless, a faceless function is
very little improvement over an abstract vector. What is desirable is a set
of concrete functions with which we can calculate. The following theorem
provides such functions (for a proof, see [Simm 83, pp. 154&ndash;161]).
</p>
<p>Theorem 7.2.3 (Stone-Weierstrass approximation theorem) The se-
quence of monomials {xk}&infin;k=0 forms a basis of L2w(a, b).
</p>
<p>Thus, any square-integrable function f can be written as f (x) =&sum;&infin;
k=0 αkx
</p>
<p>k . This theorem shows that L2w(a, b) is indeed a separable Hilbert
space as expected in Theorem 7.2.2.
</p>
<p>7.2.1 Orthogonal Polynomials
</p>
<p>The monomials {xk}&infin;k=0 are not orthonormal but are linearly independent.
If we wish to obtain an orthonormal&mdash;or simply orthogonal&mdash;linear combi-
nation of these vectors, we can use the Gram-Schmidt process. The result
</p>
<p>the polynomials
</p>
<p>{Cn(x)}&infin;n=0 are
orthogonal to each other will be certain polynomials, denoted by Cn(x), that are orthogonal to one
</p>
<p>another and span L2w(a, b).
Such orthogonal polynomials satisfy very useful recurrence relations,
</p>
<p>which we now derive. In the following discussion p&le;k(x) denotes a generic
polynomial of degree less than or equal to k. For example, 3x5 &minus; 4x2 +
5, 2x + 1, &minus;2.4x4 + 3x3 &minus; x2 + 6, and 2 are all denoted by p&le;5(x) or
p&le;8(x) or p&le;59(x) because they all have degrees less than or equal to 5, 8,</p>
<p/>
</div>
<div class="page"><p/>
<p>7.2 The Space of Square-Integrable Functions 223
</p>
<p>and 59. Since a polynomial of degree less than n can be written as a linear
combination of Ck(x) with k &lt; n, we have the obvious property
</p>
<p>ˆ b
</p>
<p>a
</p>
<p>Cn(x)p&le;n&minus;1(x)w(x)dx = 0. (7.7)
</p>
<p>Let k(m)m and k
(m&minus;1)
m denote, respectively, the coefficients of xm and xm&minus;1
</p>
<p>in Cm(x), and let
</p>
<p>hm =
ˆ b
</p>
<p>a
</p>
<p>[
Cm(x)
</p>
<p>]2
w(x)dx. (7.8)
</p>
<p>The polynomial Cn+1(x) &minus; (k(n+1)n+1 /k
(n)
n )xCn(x) has degree less than or
</p>
<p>equal to n, and therefore can be expanded as a linear combination of the
Cj (x):
</p>
<p>Cn+1(x)&minus;
k
(n+1)
n+1
k
(n)
n
</p>
<p>xCn(x)=
n&sum;
</p>
<p>j=0
ajCj (x). (7.9)
</p>
<p>Take the inner product of both sides of this equation with Cm(x):
</p>
<p>ˆ b
</p>
<p>a
</p>
<p>Cn+1(x)Cm(x)w(x)dx &minus;
k
(n+1)
n+1
k
(n)
n
</p>
<p>ˆ b
</p>
<p>a
</p>
<p>xCn(x)Cm(x)w(x)dx
</p>
<p>=
n&sum;
</p>
<p>j=0
aj
</p>
<p>ˆ b
</p>
<p>a
</p>
<p>Cj (x)Cm(x)w(x)dx.
</p>
<p>The first integral on the LHS vanishes as long as m&le; n; the second integral
vanishes if m&le; n&minus; 2 [if m&le; n&minus; 2, then xCm(x) is a polynomial of degree
n&minus; 1]. Thus, we have
</p>
<p>n&sum;
</p>
<p>j=0
aj
</p>
<p>ˆ b
</p>
<p>a
</p>
<p>Cj (x)Cm(x)w(x)dx = 0 for m&le; n&minus; 2.
</p>
<p>The integral in the sum is zero unless j = m, by orthogonality. Therefore,
the sum reduces to
</p>
<p>am
</p>
<p>ˆ b
</p>
<p>a
</p>
<p>[
Cm(x)
</p>
<p>]2
w(x)dx = 0 for m&le; n&minus; 2.
</p>
<p>Since the integral is nonzero, we conclude that am = 0 for m= 0,1,2, . . . ,
n&minus; 2, and Eq. (7.9) reduces to
</p>
<p>Cn+1(x)&minus;
k
(n+1)
n+1
k
(n)
n
</p>
<p>xCn(x)= an&minus;1Cn&minus;1(x)+ anCn(x). (7.10)
</p>
<p>It can be shown that if we define
</p>
<p>αn =
k
(n+1)
n+1
k
(n)
n
</p>
<p>, βn = αn
(
</p>
<p>k
(n)
n+1
</p>
<p>k
(n+1)
n+1
</p>
<p>&minus; k
(n&minus;1)
n
</p>
<p>k
(n)
n
</p>
<p>)
, γn =&minus;
</p>
<p>hn
</p>
<p>hn&minus;1
</p>
<p>αn
</p>
<p>αn&minus;1
,
</p>
<p>(7.11)</p>
<p/>
</div>
<div class="page"><p/>
<p>224 7 Hilbert Spaces
</p>
<p>then Eq. (7.10) can be expressed as
</p>
<p>Cn+1(x)= (αnx + βn)Cn(x)+ γnCn&minus;1(x), (7.12)
</p>
<p>or
</p>
<p>xCn(x)=
1
</p>
<p>αn
Cn+1(x)&minus;
</p>
<p>βn
</p>
<p>αn
Cn(x)&minus;
</p>
<p>γn
</p>
<p>αn
Cn&minus;1(x). (7.13)
</p>
<p>Other recurrence relations, involving higher powers of x, can be obtained
from the one above. For example, a recurrence relation involving x2 can be
</p>
<p>a recurrence relation for
</p>
<p>orthogonal polynomials
</p>
<p>obtained by multiplying both sides of Eq. (7.13) by x and expanding each
term of the RHS using that same equation. The result will be
</p>
<p>x2Cn(x)=
1
</p>
<p>αnαn+1
Cn+2(x)&minus;
</p>
<p>(
βn+1
</p>
<p>αnαn+1
+ βn
</p>
<p>α2n
</p>
<p>)
Cn+1(x)
</p>
<p>&minus;
(
</p>
<p>γn+1
αnαn+1
</p>
<p>&minus; β
2
n
</p>
<p>α2n
+ γn
</p>
<p>αnαn&minus;1
</p>
<p>)
Cn(x)
</p>
<p>+
(
βnγn
</p>
<p>α2n
+ βn&minus;1γn
</p>
<p>αnαn&minus;1
</p>
<p>)
Cn&minus;1(x)+
</p>
<p>γn&minus;1γn
αnαn&minus;1
</p>
<p>Cn&minus;2(x). (7.14)
</p>
<p>Example 7.2.4 As an application of the recurrence relations above, let us
evaluate
</p>
<p>I1 &equiv;
ˆ b
</p>
<p>a
</p>
<p>xCm(x)Cn(x)w(x)dx.
</p>
<p>Substituting (7.13) in the integral gives
</p>
<p>I1 =
1
</p>
<p>αn
</p>
<p>ˆ b
</p>
<p>a
</p>
<p>Cm(x)Cn+1(x)w(x)dx &minus;
βn
</p>
<p>αn
</p>
<p>ˆ b
</p>
<p>a
</p>
<p>Cm(x)Cn(x)w(x)dx
</p>
<p>&minus; γn
αn
</p>
<p>ˆ b
</p>
<p>a
</p>
<p>Cm(x)Cn&minus;1(x)w(x)dx.
</p>
<p>We now use the orthogonality relations among the Ck(x) to obtain
</p>
<p>I1 =
1
</p>
<p>αn
δm,n+1
</p>
<p>=hm︷ ︸︸ ︷
ˆ b
</p>
<p>a
</p>
<p>C2m(x)w(x)dx&minus;
βn
</p>
<p>αn
δmn
</p>
<p>ˆ b
</p>
<p>a
</p>
<p>C2m(x)w(x)dx
</p>
<p>&minus; γn
αn
</p>
<p>δm,n&minus;1
</p>
<p>ˆ b
</p>
<p>a
</p>
<p>C2m(x)w(x)dx
</p>
<p>=
(
</p>
<p>1
</p>
<p>αm&minus;1
δm,n+1 &minus;
</p>
<p>βm
</p>
<p>αm
δmn &minus;
</p>
<p>γm+1
αm+1
</p>
<p>δm,n&minus;1
</p>
<p>)
hm,
</p>
<p>or
</p>
<p>I1 =
</p>
<p>⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
</p>
<p>hm/αm&minus;1 if m= n+ 1,
&minus;βmhm/αm if m= n,
&minus;γm+1hm/αm+1 if m= n&minus; 1,
0 otherwise.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.2 The Space of Square-Integrable Functions 225
</p>
<p>Example 7.2.5 Let us find the orthogonal polynomials forming a basis of
L2(&minus;1,+1), which we denote by Pn(x), where n is the degree of the poly-
nomial. Let P0(x)= 1. To find P1(x), write P1(x)= ax + b, and determine
a and b in such a way that P1(x) is orthogonal to P0(x):
</p>
<p>0 =
ˆ 1
</p>
<p>&minus;1
P1(x)P0(x) dx =
</p>
<p>ˆ 1
</p>
<p>&minus;1
(ax + b)dx = 1
</p>
<p>2
ax2
</p>
<p>∣∣∣∣
1
</p>
<p>&minus;1
+ 2b= 2b.
</p>
<p>So one of the coefficients, b, is zero. To find the other one, we need
some standardization procedure. We &ldquo;standardize&rdquo; Pn(x) by requiring that
Pn(1)= 1 &forall;n. For n= 1 this yields a &times; 1 = 1, or a = 1, so that P1(x)= x.
</p>
<p>We can calculate P2(x) similarly: Write P2(x)= ax2 + bx + c, impose
the condition that it be orthogonal to both P1(x) and P0(x), and enforce the
standardization procedure. All this will yield
</p>
<p>0 =
ˆ 1
</p>
<p>&minus;1
P2(x)P0(x) dx =
</p>
<p>2
</p>
<p>3
a + 2c, 0 =
</p>
<p>ˆ 1
</p>
<p>&minus;1
P2(x)P1(x) dx =
</p>
<p>2
</p>
<p>3
b,
</p>
<p>and P2(1)= a + b+ c= 1. These three equations have the unique solution
a = 3/2, b = 0, c = &minus;1/2. Thus, P2(x) = 12 (3x2 &minus; 1). These are the first
three Legendre polynomials, which are part of a larger group of polynomials
to be discussed in Chap. 8.
</p>
<p>7.2.2 Orthogonal Polynomials and Least Squares
</p>
<p>The method of least squares is no doubt familiar to the reader. In the simplest
procedure, one tries to find a linear function that most closely fits a set of
data. By definition, &ldquo;most closely&rdquo; means that the sum of the squares of
the differences between the data points and the corresponding values of the
linear function is minimum. More generally, one seeks the best polynomial
fit to the data.
</p>
<p>We shall consider a related topic, namely least-square fitting of a given
function with polynomials. Suppose f (x) is a function defined on (a, b). We
want to find a polynomial that most closely approximates f . Write such a
polynomial as p(x)=&sum;nk=0 akxk , where the ak&rsquo;s are to be determined such
that
</p>
<p>S(a0, a1, . . . , an)&equiv;
ˆ b
</p>
<p>a
</p>
<p>[
f (x)&minus; a0 &minus; a1x &minus; &middot; &middot; &middot; &minus; anxn
</p>
<p>]2
dx
</p>
<p>is a minimum. Differentiating S with respect to the ak&rsquo;s and setting the result
equal to zero gives
</p>
<p>0 = &part;S
&part;aj
</p>
<p>=
ˆ b
</p>
<p>a
</p>
<p>2
(
&minus;xj
</p>
<p>)
[
f (x)&minus;
</p>
<p>n&sum;
</p>
<p>k=0
akx
</p>
<p>k
</p>
<p>]
dx,
</p>
<p>or
n&sum;
</p>
<p>k=0
ak
</p>
<p>ˆ b
</p>
<p>a
</p>
<p>xj+kdx =
ˆ b
</p>
<p>a
</p>
<p>f (x)xjdx.</p>
<p/>
</div>
<div class="page"><p/>
<p>226 7 Hilbert Spaces
</p>
<p>One can rewrite this in matrix form as Ba = c, where a is a column vector
with components ak , and B and c are a matrix and a column vector whose
components are
</p>
<p>Bkj =
bj+k+1 &minus; aj+k+1
</p>
<p>j + k + 1 and cj =
ˆ b
</p>
<p>a
</p>
<p>f (x)xjdx. (7.15)
</p>
<p>By solving this matrix equation, one finds the ak&rsquo;s, which in turn give the
best fit.
</p>
<p>A drawback of the procedure above is that the desire for a higher-degree
polynomial fit entails the implementation of the procedure from scratch and
the solution of a completely new matrix equation. One way to overcome this
difficulty is to use orthogonal polynomials. Then we would have
</p>
<p>S(a0, a1, . . . , an)&equiv;
ˆ b
</p>
<p>a
</p>
<p>[
f (x)&minus;
</p>
<p>n&sum;
</p>
<p>k=0
akCk(x)
</p>
<p>]2
w(x)dx,
</p>
<p>where we have introduced a weight function w(x) for convenience. The
derivative equation becomes
</p>
<p>0 = &part;S
&part;aj
</p>
<p>=
ˆ b
</p>
<p>a
</p>
<p>2
[
&minus;Cj (x)
</p>
<p>]
[
f (x)&minus;
</p>
<p>n&sum;
</p>
<p>k=0
akCk(x)
</p>
<p>]
w(x)dx,
</p>
<p>or
n&sum;
</p>
<p>k=0
ak
</p>
<p>ˆ b
</p>
<p>a
</p>
<p>Cj (x)Ck(x)w(x)dx
</p>
<p>︸ ︷︷ ︸
=0 unless j = k
</p>
<p>=
ˆ b
</p>
<p>a
</p>
<p>Cj (x)f (x)w(x)dx.
</p>
<p>It follows that
</p>
<p>aj =
&acute; b
</p>
<p>a
Cj (x)f (x)w(x)dx
</p>
<p>&acute; b
</p>
<p>a
[Cj (x)]2w(x)dx
</p>
<p>, j = 0,1, . . . , n, (7.16)
</p>
<p>which is true regardless of the number of polynomials in the sum. Hence,
once we find {aj }mj=0, we can add the (m+ 1)st polynomial and determine
am+1 from Eq. (7.16) without altering the previous coefficients.
</p>
<p>Example 7.2.6 Let us find the least-square fit to f (x) = cos( 12πx) in the
interval (&minus;1,+1) using polynomials of second degree. First we use a single
polynomial whose coefficients are determined by Eq. (7.15). We can easily
calculate the column vector c:
</p>
<p>c0 =
ˆ 1
</p>
<p>&minus;1
cos
</p>
<p>(
1
</p>
<p>2
πx
</p>
<p>)
dx = 4
</p>
<p>π
,
</p>
<p>c1 =
ˆ 1
</p>
<p>&minus;1
x cos
</p>
<p>(
1
</p>
<p>2
πx
</p>
<p>)
dx = 0,
</p>
<p>c2 =
ˆ 1
</p>
<p>&minus;1
x2 cos
</p>
<p>(
1
</p>
<p>2
πx
</p>
<p>)
dx =&minus; 32
</p>
<p>π3
+ 4
</p>
<p>π
.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.3 Continuous Index 227
</p>
<p>The elements of the matrix B can also be calculated easily. To find the un-
known ak&rsquo;s, we need to solve
</p>
<p>⎛
⎜⎝
</p>
<p>2 0 23
0 23 0
2
3 0
</p>
<p>2
5
</p>
<p>⎞
⎟⎠
</p>
<p>⎛
⎝
a0
a1
a2
</p>
<p>⎞
⎠=
</p>
<p>⎛
⎜⎝
</p>
<p>4
π
</p>
<p>0
</p>
<p>&minus; 32
π3
</p>
<p>+ 4
π
</p>
<p>⎞
⎟⎠ .
</p>
<p>The solution is
</p>
<p>a0 =
24
</p>
<p>π3
, a1 = 0, a2 =
</p>
<p>6
</p>
<p>π
&minus; 72
</p>
<p>π3
.
</p>
<p>Therefore,
</p>
<p>cos
</p>
<p>(
1
</p>
<p>2
πx
</p>
<p>)
&asymp; 24
</p>
<p>π3
+
(
</p>
<p>6
</p>
<p>π
&minus; 72
</p>
<p>π3
</p>
<p>)
x2.
</p>
<p>If we wish to use orthogonal polynomials with w(x)= 1, we can employ
the polynomials found in Example 7.2.5. Then
</p>
<p>aj =
&acute; 1
&minus;1 Pj (x) cos(
</p>
<p>1
2πx)dx
</p>
<p>&acute; 1
&minus;1[Pj (x)]2 dx
</p>
<p>, j = 0,1,2,
</p>
<p>which yields
</p>
<p>a0 =
2
</p>
<p>π
, a1 = 0, a2 =&minus;
</p>
<p>120
</p>
<p>π3
+ 20
</p>
<p>π
.
</p>
<p>We can therefore write
</p>
<p>cos
</p>
<p>(
1
</p>
<p>2
πx
</p>
<p>)
&asymp; 2
</p>
<p>π
P0 +
</p>
<p>(
&minus;120
</p>
<p>π3
+ 20
</p>
<p>π
</p>
<p>)
P2.
</p>
<p>7.3 Continuous Index
</p>
<p>Once we allow the number of dimensions to be infinite, we open the door
for numerous possibilities that are not present in the finite case. One such
possibility arises because of the variety of infinities. We have encountered
two types of infinity in Chap. 1, the countable infinity and the uncountable
infinity. The paradigm of the former is the &ldquo;number&rdquo; of integers, and that of
the latter is the &ldquo;number&rdquo; of real numbers. The nature of dimensionality of
the vector space is reflected in the components of a general vector, which has
a finite number of components in a finite-dimensional vector space, a count-
ably infinite number of components in an infinite-dimensional vector space
with a countable basis, and an uncountably infinite number of components
in an infinite-dimensional vector space with no countable basis.
</p>
<p>To gain an understanding of the nature of, and differences between, the
three types of vector spaces mentioned above, it is convenient to think of
components as functions of a &ldquo;counting set&rdquo;. Thus, the components fi of</p>
<p/>
</div>
<div class="page"><p/>
<p>228 7 Hilbert Spaces
</p>
<p>a vector |f 〉 in an N -dimensional vector space can be thought of as val-
ues of a function f defined on the finite set {1,2, . . . ,N}, and to empha-
size such functional dependence, we write f (i) instead of fi . Similarly, the
components fi of a vector |f 〉 in a Hilbert space with the countable basis
B = {|ei〉}&infin;i=1 can be thought of as values of a function f :N&rarr;C, where N
is the (infinite) set of natural numbers. The next step is to allow the counting
set to be uncountable, i.e., a continuum such as the real numbers or an in-
terval thereof. This leads to a &ldquo;component&rdquo; of the form f (x) corresponding
to a function f : R&rarr; C. What about the vectors themselves? What sort of
a basis gives rise to such components?
</p>
<p>Because of the isomorphism of Theorem 7.2.2, we shall concentrate on
L2w(a, b). In keeping with our earlier notation, let {|ex〉}x&isin;R be a set of vec-
tors and interpret f (x) as 〈ex |f 〉. The inner product of L2w(a, b) can now
be written as
</p>
<p>〈g|f 〉 =
ˆ b
</p>
<p>a
</p>
<p>g&lowast;(x)f (x)w(x)dx =
ˆ b
</p>
<p>a
</p>
<p>〈g|ex〉〈ex |f 〉w(x)dx
</p>
<p>= 〈g|
(
ˆ b
</p>
<p>a
</p>
<p>|ex〉w(x)〈ex |dx
)
|f 〉.
</p>
<p>The last line suggests writing
</p>
<p>ˆ b
</p>
<p>a
</p>
<p>|ex〉w(x)〈ex |dx = 1.
</p>
<p>In the physics literature the &ldquo;e&rdquo; is ignored, and one writes |x〉 for |ex〉.
Hence, we obtain the completeness relation for a continuous index:
</p>
<p>completeness relation
</p>
<p>for a continuous index
</p>
<p>ˆ b
</p>
<p>a
</p>
<p>|x〉w(x)〈x|dx = 1, or
ˆ b
</p>
<p>a
</p>
<p>|x〉〈x|dx = 1, (7.17)
</p>
<p>where in the second integral, w(x) is set equal to unity. We also have
</p>
<p>|f 〉 =
(
ˆ b
</p>
<p>a
</p>
<p>|x〉w(x)〈x|dx
)
|f 〉 =
</p>
<p>ˆ b
</p>
<p>a
</p>
<p>f (x)w(x)|x〉dx, (7.18)
</p>
<p>which shows how to expand a vector |f 〉 in terms of the |x〉&rsquo;s.
Take the inner product of (7.18) with 〈x&prime;| to obtain
</p>
<p>〈x&prime;|f 〉 = f
(
x&prime;
)
=
ˆ b
</p>
<p>a
</p>
<p>f (x)w(x)〈x&prime;|x〉dx,
</p>
<p>where x&prime; is assumed to lie in the interval (a, b), otherwise f (x&prime;) = 0 by
definition. This equation, which holds for arbitrary f , tells us immediately
that w(x)〈x&prime;|x〉 is no ordinary function of x and x&prime;. For instance, sup-
pose f (x&prime;) = 0. Then, the result of integration is always zero, regardless
of the behavior of f at other points. Clearly, there is an infinitude of func-
tions that vanish at x&prime;, yet all of them give the same integral! Pursuing this
line of argument more quantitatively, one can show that w(x)〈x&prime;|x〉 = 0
if x �= x&prime;, w(x)〈x|x〉 = &infin;, w(x)〈x&prime;|x〉 is an even function of x &minus; x&prime;, and</p>
<p/>
</div>
<div class="page"><p/>
<p>7.3 Continuous Index 229
</p>
<p>Fig. 7.2 The Gaussian bell-shaped curve approaches the Dirac delta function as the
width of the curve approaches zero. The value of ǫ is 1 for the dashed curve, 0.25 for
the heavy curve and 0.05 for the light curve
</p>
<p>&acute; b
</p>
<p>a
w(x)〈x&prime;|x〉dx = 1. The proof is left as a problem. The reader may rec-
</p>
<p>ognize this as the Dirac delta function
Dirac delta function
</p>
<p>δ
(
x &minus; x&prime;
</p>
<p>)
=w(x)〈x&prime;|x〉, (7.19)
</p>
<p>which, for a function f defined on the interval (a, b), has the following
property:4
</p>
<p>ˆ b
</p>
<p>a
</p>
<p>f (x)δ
(
x &minus; x&prime;
</p>
<p>)
dx =
</p>
<p>{
f (x&prime;) if x&prime; &isin; (a, b),
0 if x&prime; /&isin; (a, b).
</p>
<p>(7.20)
</p>
<p>Written in the form 〈x&prime;|x〉 = δ(x&minus;x&prime;)/w(x), Eq. (7.19) is the generalization
of the orthonormality relation of vectors to the case of a continuous index.
</p>
<p>The Dirac delta function is anything but a &ldquo;function&rdquo;. Nevertheless, there
is a well-developed branch of mathematics, called generalized function the-
ory or functional analysis, studying it and many other functions like it in a
highly rigorous fashion. We shall only briefly explore this territory of math-
ematics in the next section. At this point we simply mention the fact that
the Dirac delta function can be represented as the limit of certain sequences
of ordinary functions. The following three examples illustrate some of these
representations.
</p>
<p>Example 7.3.1 Consider a Gaussian curve whose width approaches zero at
the same time that its height approaches infinity in such a way that its area
remains constant. In the infinite limit, we obtain the Dirac delta function. In
fact, we have
</p>
<p>δ
(
x &minus; x&prime;
</p>
<p>)
= lim
</p>
<p>ǫ&rarr;0
1&radic;
ǫπ
</p>
<p>e&minus;(x&minus;x
&prime;)2/ǫ .
</p>
<p>In the limit of ǫ &rarr; 0, the height of this Gaussian goes to infinity while its
width goes to zero (see Fig. 7.2). Furthermore, for any nonzero value of ǫ,
</p>
<p>4For an elementary discussion of the Dirac delta function with many examples of its
application, see [Hass 08].</p>
<p/>
</div>
<div class="page"><p/>
<p>230 7 Hilbert Spaces
</p>
<p>Fig. 7.3 The function sinT x/x also approaches the Dirac delta function as the width of
the curve approaches zero. The value of T is 0.5 for the dashed curve, 2 for the heavy
curve, and 15 for the light curve
</p>
<p>we can easily verify that
ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>1&radic;
ǫπ
</p>
<p>e&minus;(x&minus;x
&prime;)2/ǫ dx = 1.
</p>
<p>This relation is independent of ǫ and therefore still holds in the limit ǫ &rarr; 0.
The limit of the Gaussian behaves like the Dirac delta function.
</p>
<p>Example 7.3.2 Consider the function DT (x &minus; x&prime;) defined as
</p>
<p>DT
(
x &minus; x&prime;
</p>
<p>)
&equiv; 1
</p>
<p>2π
</p>
<p>ˆ T
</p>
<p>&minus;T
ei(x&minus;x
</p>
<p>&prime;)tdt.
</p>
<p>The integral is easily evaluated, with the result
</p>
<p>DT
(
x &minus; x&prime;
</p>
<p>)
= 1
</p>
<p>2π
</p>
<p>ei(x&minus;x
&prime;)t
</p>
<p>i(x &minus; x&prime;)
</p>
<p>∣∣∣∣
T
</p>
<p>&minus;T
= 1
</p>
<p>π
</p>
<p>sinT (x &minus; x&prime;)
x &minus; x&prime; .
</p>
<p>The graph of DT (x &minus; 0) as a function of x for various values of T is shown
in Fig. 7.3. Note that the width of the curve decreases as T increases. The
area under the curve can be calculated:
ˆ &infin;
</p>
<p>&minus;&infin;
DT
</p>
<p>(
x &minus; x&prime;
</p>
<p>)
dx = 1
</p>
<p>π
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>sinT (x &minus; x&prime;)
x &minus; x&prime; dx =
</p>
<p>1
</p>
<p>π
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>siny
</p>
<p>y
dy
</p>
<p>︸ ︷︷ ︸
=π
</p>
<p>= 1.
</p>
<p>Figure 7.3 shows that DT (x &minus; x&prime;) becomes more and more like the Dirac
delta function as T gets larger and larger. In fact, we have
</p>
<p>δ
(
x &minus; x&prime;
</p>
<p>)
= lim
</p>
<p>T&rarr;&infin;
1
</p>
<p>π
</p>
<p>sinT (x &minus; x&prime;)
x &minus; x&prime; . (7.21)
</p>
<p>To see this, we note that for any finite T we can write
</p>
<p>DT
(
x &minus; x&prime;
</p>
<p>)
= T
</p>
<p>π
</p>
<p>sinT (x &minus; x&prime;)
T (x &minus; x&prime;) .</p>
<p/>
</div>
<div class="page"><p/>
<p>7.3 Continuous Index 231
</p>
<p>Fig. 7.4 The step function, or θ -function, shown in the figure has the Dirac delta function
as its derivative
</p>
<p>Furthermore, for values of x that are very close to x&prime;,
</p>
<p>T
(
x &minus; x&prime;
</p>
<p>)
&rarr; 0 and sinT (x &minus; x
</p>
<p>&prime;)
T (x &minus; x&prime;) &rarr; 1.
</p>
<p>Thus, for such values of x and x&prime;, we have DT (x &minus; x&prime;)&asymp; (T /π), which is
large when T is large. This is as expected of a delta function: δ(0)=&infin;. On
the other hand, the width of DT (x &minus; x&prime;) around x&prime; is given, roughly, by the
distance between the points at which DT (x&minus;x&prime;) drops to zero: T (x&minus;x&prime;)=
&plusmn;π , or x &minus; x&prime; =&plusmn;π/T . This width is roughly �x = 2π/T , which goes to
zero as T grows. Again, this is as expected of the delta function.
</p>
<p>The preceding example suggests another representation of the Dirac delta
function:
</p>
<p>δ
(
x &minus; x&prime;
</p>
<p>)
= 1
</p>
<p>2π
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
ei(x&minus;x
</p>
<p>&prime;)t dt. (7.22)
</p>
<p>Example 7.3.3 A third representation of the Dirac delta function involves
the step function θ(x &minus; x&prime;), which is defined as
</p>
<p>step function or θ
</p>
<p>function
</p>
<p>θ
(
x &minus; x&prime;
</p>
<p>)
&equiv;
{
</p>
<p>0 if x &lt; x&prime;,
</p>
<p>1 if x &gt; x&prime;
</p>
<p>and is discontinuous at x = x&prime;. We can approximate this step function by a
variety of continuous functions. One such function is Tǫ(x &minus; x&prime;) defined by
</p>
<p>Tǫ
(
x &minus; x&prime;
</p>
<p>)
&equiv;
</p>
<p>⎧
⎪⎨
⎪⎩
</p>
<p>0 if x &le; x&prime; &minus; ǫ,
1
2ǫ (x &minus; x&prime; + ǫ) if x&prime; &minus; ǫ &le; x &le; x&prime; + ǫ,
1 if x &ge; x&prime; + ǫ,
</p>
<p>where ǫ is a small positive number as shown in Fig. 7.4. It is clear that
</p>
<p>θ
(
x &minus; x&prime;
</p>
<p>)
= lim
</p>
<p>ǫ&rarr;0
Tǫ
(
x &minus; x&prime;
</p>
<p>)
.</p>
<p/>
</div>
<div class="page"><p/>
<p>232 7 Hilbert Spaces
</p>
<p>Now let us consider the derivative of Tǫ(x &minus; x&prime;) with respect to x:
</p>
<p>dTǫ
</p>
<p>dx
</p>
<p>(
x &minus; x&prime;
</p>
<p>)
=
</p>
<p>⎧
⎪⎨
⎪⎩
</p>
<p>0 if x &lt; x&prime; &minus; ǫ,
1
2ǫ if x
</p>
<p>&prime; &minus; ǫ &lt; x &lt; x&prime; + ǫ,
0 if x &gt; x&prime; + ǫ.
</p>
<p>We note that the derivative is not defined at x = x&prime; &minus; ǫ and x = x&prime; + ǫ, and
that dTǫ/dx is zero everywhere except when x lies in the interval (x&prime; &minus;
ǫ, x&prime; + ǫ), where it is equal to 1/(2ǫ) and goes to infinity as ǫ &rarr; 0. Here
again we see signs of the delta function. In fact, we also note that
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>(
dTǫ
</p>
<p>dx
</p>
<p>)
dx =
</p>
<p>ˆ x&prime;+ǫ
</p>
<p>x&prime;&minus;ǫ
</p>
<p>(
dTǫ
</p>
<p>dx
</p>
<p>)
dx =
</p>
<p>ˆ x&prime;+ǫ
</p>
<p>x&prime;&minus;ǫ
</p>
<p>1
</p>
<p>2ǫ
dx = 1.
</p>
<p>It is not surprising, then, to find that limǫ&rarr;0 dTǫdx (x &minus; x&prime;) = δ(x &minus; x&prime;). As-
suming that the interchange of the order of differentiation and the limiting
process is justified, we obtain the important identity
</p>
<p>d
</p>
<p>dx
θ
(
x &minus; x&prime;
</p>
<p>)
= δ
</p>
<p>(
x &minus; x&prime;
</p>
<p>)
. (7.23)
</p>
<p>Now that we have some understanding of one continuous index, we can
</p>
<p>δ function as derivative
</p>
<p>of θ function
</p>
<p>generalize the results to several continuous indices. In the earlier discussion
we looked at f (x) as the xth component of some abstract vector |f 〉. For
functions of n variables, we can think of f (x1, . . . , xn) as the component
of an abstract vector |f 〉 along a basis vector |x1, . . . , xn〉.5 This basis is
a direct generalization of one continuous index to n. Then f (x1, . . . , xn)
is defined as f (x1, . . . , xn)= 〈x1, . . . , xn|f 〉. If the region of integration is
denoted by Ω , and we use the abbreviations
</p>
<p>r &equiv; (x1, x2, . . . , xn), dnx = dx1dx2 . . . dxn,
</p>
<p>|x1, x2, . . . , xn〉 = |r〉, δ
(
x1 &minus; x&prime;1
</p>
<p>)
. . . δ
</p>
<p>(
xn &minus; x&prime;n
</p>
<p>)
= δ
</p>
<p>(
r &minus; r&prime;
</p>
<p>)
,
</p>
<p>then we can write
</p>
<p>|f 〉 =
ˆ
</p>
<p>Ω
</p>
<p>dnxf (r)w(r)|r〉,
ˆ
</p>
<p>Ω
</p>
<p>dnx|r〉w(r)〈r| = 1,
</p>
<p>f
(
r&prime;
)
=
ˆ
</p>
<p>Ω
</p>
<p>dnxf (r)w(r)〈r&prime;|r〉, 〈r&prime;|r〉w(r)= δ
(
r &minus; r&prime;
</p>
<p>)
,
</p>
<p>(7.24)
</p>
<p>where dnx is the &ldquo;volume&rdquo; element and Ω is the region of integration of
interest.
</p>
<p>For instance, if the region of definition of the functions under considera-
tion is the surface of the unit sphere, then [with w(r)= 1], one gets
</p>
<p>ˆ 2π
</p>
<p>0
dφ
</p>
<p>ˆ π
</p>
<p>0
sin θ dθ |θ,φ〉〈θ,φ| = 1. (7.25)
</p>
<p>5Do not confuse this with an n-dimensional vector. In fact, the dimension is n-fold infi-
nite: each xi counts one infinite set of numbers!</p>
<p/>
</div>
<div class="page"><p/>
<p>7.4 Generalized Functions 233
</p>
<p>This will be used in our discussion of spherical harmonics in Chap. 13.
An important identity using the three-dimensional Dirac delta function
</p>
<p>comes from potential theory. This is (see [Hass 08] for a discussion of this
equation)
</p>
<p>&nabla;2
(
</p>
<p>1
</p>
<p>|r &minus; r&prime;|
</p>
<p>)
=&minus;4πδ
</p>
<p>(
r &minus; r&prime;
</p>
<p>)
. (7.26)
</p>
<p>7.4 Generalized Functions
</p>
<p>Paul Adrian Maurice Dirac discovered the delta function in the late 1920s
while investigating scattering problems in quantum mechanics. This &ldquo;func-
tion&rdquo; seemed to violate most properties of other functions known to mathe-
maticians at the time. However, later, when mathematicians found a rigorous
way of studying this and other functions similar to it, a new vista in higher
mathematics opened up.
</p>
<p>The derivative of the delta function, δ&prime;(x &minus; x&prime;) is such that for any ordi-
nary function f (x),
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
f (x)δ&prime;(x &minus; a)dx =&minus;
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
f &prime;(x)δ(x &minus; a)dx =&minus;f &prime;(a).
</p>
<p>We can define δ&prime;(x &minus; a) by this relation. In addition, we can define the
derivative of any function, including discontinuous functions, at any point
(including points of discontinuity, where the usual definition of derivative
fails) by this relation. That is, if ϕ(x) is a &ldquo;bad&rdquo; function whose derivative is
not defined at some point(s), and f (x) is a &ldquo;good&rdquo; function, we can define
the derivative of ϕ(x) by
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
f (x)ϕ&prime;(x) dx &equiv;&minus;
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
f &prime;(x)ϕ(x) dx.
</p>
<p>The integral on the RHS is well-defined.
Functions such as the Dirac delta function and its derivatives of all orders
</p>
<p>are not functions in the traditional sense. What is common among all of them
is that in most applications they appear inside an integral, and we saw in
Chap. 2 that integration can be considered as a linear functional on the space
of continuous functions. It is therefore natural to describe such functions in
terms of linear functionals. This idea was picked up by Laurent Schwartz
in the 1950s who developed it into a new branch of mathematics called
generalized functions, or distributions.
</p>
<p>A distribution is a mathematical entity that appears inside an integral in
conjunction with a well-behaved test function&mdash;which we assume to de-
pend on n variables&mdash;such that the result of integration is a well-defined
number. Depending on the type of test function used, different kinds of dis-
tributions can be defined. If we want to include the Dirac delta function and
</p>
<p>test function
</p>
<p>its derivatives of all orders, then the test functions must be infinitely differ-
entiable, that is, they must be C&infin; functions on Rn (or Cn). Moreover, in</p>
<p/>
</div>
<div class="page"><p/>
<p>234 7 Hilbert Spaces
</p>
<p>order for the theory of distributions to be mathematically feasible, all the
test functions must be of compact support, i.e., they must vanish outside
a finite &ldquo;volume&rdquo; of Rn (or Cn). One common notation for such functions
is C&infin;F (R
</p>
<p>n) or C&infin;F (C
n) (F stands for &ldquo;finite&rdquo;). The definitive property of
</p>
<p>distributions concerns the way they combine with test functions to give a
number. The test functions used clearly form a vector space over R or C. In
this vector-space language, distributions are linear functionals. The linearity
is a simple consequence of the properties of the integral. We therefore have
the following definition of a distribution.
</p>
<p>Definition 7.4.1 A distribution, or generalized function, is a continuous6generalized functions
and distributions
</p>
<p>defined
</p>
<p>linear functional on the space C&infin;F (R
n) or C&infin;F (C
</p>
<p>n). If f &isin; C&infin;F and ϕ is a
distribution, then ϕ[f ] =
</p>
<p>&acute;&infin;
&minus;&infin; ϕ(r)f (r) d
</p>
<p>nx.
</p>
<p>Another notation used in place of ϕ[f ] is 〈ϕ,f 〉. This is more appealing
not only because ϕ is linear, in the sense that ϕ[αf +βg] = αϕ[f ]+βϕ[g],
but also because the set of all such linear functionals forms a vector space;
that is, the linear combination of the ϕ&rsquo;s is also defined. Thus, 〈ϕ,f 〉 sug-
gests a mutual &ldquo;democracy&rdquo; for both f &rsquo;s and ϕ&rsquo;s.
</p>
<p>We now have a shorthand way of writing integrals. For instance, if δa
represents the Dirac delta function δ(x &minus; a), with an integration over x un-
derstood, then 〈δa, f 〉 = f (a). Similarly, 〈δ&prime;a, f 〉 = &minus;f &prime;(a), and for linear
combinations, 〈αδa + βδ&prime;a, f 〉 = αf (a)&minus; βf &prime;(a).
</p>
<p>Example 7.4.2 An ordinary (continuous) function g can be thought of as
a special case of a distribution. The linear functional g : C&infin;F (R) &rarr; R is
simply defined by 〈g,f 〉 &equiv; g[f ] =
</p>
<p>&acute;&infin;
&minus;&infin; g(x)f (x) dx.
</p>
<p>Example 7.4.3 An interesting application of distributions (generalized
functions) occurs when the notion of density is generalized to include not
only (smooth) volume densities, but also point-like, linear, and surface den-
sities.
</p>
<p>A point charge q located at r0 can be thought of as having a charge
density ρ(r)= qδ(r&minus;r0). In the language of linear functionals, we interpret
ρ as a distribution, ρ : C&infin;F (R3)&rarr;R, which for an arbitrary function f gives
</p>
<p>ρ[f ] = 〈ρ,f 〉 = qf (r0). (7.27)
</p>
<p>The delta function character of ρ can be detected from this equation by
recalling that the LHS is
</p>
<p>ˆ
</p>
<p>ρ(r)f (r) d3x = lim
N &rarr;&infin;
�Vi &rarr; 0
</p>
<p>N&sum;
</p>
<p>i=1
ρ(ri)f (ri)�Vi .
</p>
<p>6See [Zeid 95, pp. 27, 156&ndash;160] for a formal definition of continuity for linear function-
als.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.4 Generalized Functions 235
</p>
<p>On the RHS of this equation, the only volume element that contributes is
the one that contains the point r0; all the rest contribute zero. As �Vi &rarr; 0,
the only way that the RHS can give a nonzero number is for ρ(r0)f (r0)
to be infinite. Since f is a well-behaved function, ρ(r0) must be infinite,
implying that ρ(r) acts as a delta function. This shows that the definition of
Eq. (7.27) leads to a delta-function behavior for ρ. Similarly for linear and
surface densities.
</p>
<p>The example above and Problems 7.12 and 7.13 suggest that a distribu-
tion that confines an integral to a lower-dimensional space must have a delta
function in its definition.
</p>
<p>We have seen that the delta function can be thought of as the limit of an
ordinary function. This idea can be generalized.
</p>
<p>Definition 7.4.4 Let {ϕn(x)} be a sequence of functions such that
</p>
<p>lim
n&rarr;&infin;
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
ϕn(x)f (x) dx
</p>
<p>exists for all f &isin; C&infin;F (R). Then the sequence is said to converge to the dis-
tribution ϕ, defined by
</p>
<p>〈ϕ,f 〉 = lim
n&rarr;&infin;
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
ϕn(x)f (x) dx &forall;f.
</p>
<p>This convergence is denoted by ϕn &rarr; ϕ.
</p>
<p>For example, it can be verified that
</p>
<p>n&radic;
π
e&minus;n
</p>
<p>2x2 &rarr; δ(x) and 1 &minus; cosnx
nπx2
</p>
<p>&rarr; δ(x)
</p>
<p>and so on. The proofs are left as exercises.
</p>
<p>Historical Notes
</p>
<p>&ldquo;Physical Laws should have mathematical beauty.&rdquo; This statement was Dirac&rsquo;s response
to the question of his philosophy of physics, posed to him in Moscow in 1955. He wrote
it on a blackboard that is still preserved today.
Paul Adrien Maurice Dirac (1902&ndash;1984), was born in 1902 in Bristol, England, of a
Swiss, French-speaking father and an English mother. His father, a taciturn man who
refused to receive friends at home, enforced young Paul&rsquo;s silence by requiring that only
French be spoken at the dinner table. Perhaps this explains Dirac&rsquo;s later disinclination
toward collaboration and his general tendency to be a loner in most aspects of his life.
The fundamental nature of his work made the involvement of students difficult, so perhaps
Dirac&rsquo;s personality was well-suited to his extraordinary accomplishments.
Dirac went to Merchant Venturer&rsquo;s School, the public school where his father taught
</p>
<p>Paul Adrien Maurice
</p>
<p>Dirac 1902&ndash;1984
</p>
<p>French, and while there displayed great mathematical abilities. Upon graduation, he fol-
lowed in his older brother&rsquo;s footsteps and went to Bristol University to study electrical
engineering. He was 19 when he graduated Bristol University in 1921. Unable to find
a suitable engineering position due to the economic recession that gripped post-World
War I England, Dirac accepted a fellowship to study mathematics at Bristol University.
This fellowship, together with a grant from the Department of Scientific and Industrial
Research, made it possible for Dirac to go to Cambridge as a research student in 1923.
At Cambridge Dirac was exposed to the experimental activities of the Cavendish Labora-
tory, and he became a member of the intellectual circle over which Rutherford and Fowler</p>
<p/>
</div>
<div class="page"><p/>
<p>236 7 Hilbert Spaces
</p>
<p>presided. He took his Ph.D. in 1926 and was elected in 1927 as a fellow. His appointment
as university lecturer came in 1929. He assumed the Lucasian professorship following
Joseph Larmor in 1932 and retired from it in 1969. Two years later he accepted a position
at Florida State University where he lived out his remaining years. The FSU library now
carries his name.
In the late 1920s the relentless march of ideas and discoveries had carried physics to
</p>
<p>&ldquo;The amount of
</p>
<p>theoretical ground one
</p>
<p>has to cover before
</p>
<p>being able to solve
</p>
<p>problems of real
</p>
<p>practical value is rather
</p>
<p>large, but this
</p>
<p>circumstance is an
</p>
<p>inevitable consequence
</p>
<p>of the fundamental part
</p>
<p>played by
</p>
<p>transformation theory
</p>
<p>and is likely to become
</p>
<p>more pronounced in the
</p>
<p>theoretical physics of the
</p>
<p>future.&rdquo; P.A.M. Dirac
</p>
<p>(1930)
</p>
<p>a generally accepted relativistic theory of the electron. Dirac, however, was dissatisfied
with the prevailing ideas and, somewhat in isolation, sought for a better formulation. By
1928 he succeeded in finding an equation, the Dirac equation, that accorded with his own
ideas and also fit most of the established principles of the time. Ultimately, this equation,
and the physical theory behind it, proved to be one of the great intellectual achievements
of the period. It was particularly remarkable for the internal beauty of its mathemati-
cal structure, which not only clarified previously mysterious phenomena such as spin
and the Fermi-Dirac statistics associated with it, but also predicted the existence of an
electron-like particle of negative energy, the antielectron, or positron, and, more recently,
it has come to play a role of great importance in modern mathematics, particularly in the
interrelations between topology, geometry, and analysis. Heisenberg characterized the
discovery of antimatter by Dirac as &ldquo;the most decisive discovery in connection with the
properties or the nature of elementary particles. . . . This discovery of particles and an-
tiparticles by Dirac . . . changed our whole outlook on atomic physics completely.&rdquo; One
of the interesting implications of his work that predicted the positron was the prediction
of a magnetic monopole. Dirac won the Nobel Prize in 1933 for this work.
Dirac is not only one of the chief authors of quantum mechanics, but he is also the cre-
ator of quantum electrodynamics and one of the principal architects of quantum field
theory. While studying the scattering theory of quantum particles, he invented the (Dirac)
delta function; in his attempt at quantizing the general theory of relativity, he founded
constrained Hamiltonian dynamics, which is one of the most active areas of theoretical
physics research today. One of his greatest contributions is the invention of bra 〈 | and
ket | 〉.
While at Cambridge, Dirac did not accept many research students. Those who worked
with him generally thought that he was a good supervisor, but one who did not spend
much time with his students. A student needed to be extremely independent to work
under Dirac. One such student was Dennis Sciama, who later became the supervisor of
Stephen Hawking, the current holder of the Lucasian chair. Salam and Wigner, in their
preface to the Festschrift that honors Dirac on his seventieth birthday and commemorates
his contributions to quantum mechanics succinctly assessed the man:
</p>
<p>Dirac is one of the chief creators of quantum mechanics. . . . Posterity will rate
Dirac as one of the greatest physicists of all time. The present generation values
him as one of its greatest teachers. . . . On those privileged to know him, Dirac has
left his mark . . . by his human greatness. He is modest, affectionate, and sets the
highest possible standards of personal and scientific integrity. He is a legend in his
own lifetime and rightly so.
</p>
<p>(Taken from Schweber, S.S. &ldquo;Some chapters for a history of quantum field theory: 1938-
1952&rdquo;, in Relativity, Groups, and Topology II vol. 2, B.S. DeWitt and R. Stora, eds.,
North-Holland, Amsterdam, 1984.)
</p>
<p>Definition 7.4.5 The derivative of a distribution ϕ is another distributionderivative of a
distribution ϕ&prime; defined by 〈ϕ&prime;, f 〉 = &minus;〈ϕ,f &prime;〉 &forall;f &isin; C&infin;F .
</p>
<p>Example 7.4.6 We can combine the last two definitions to show that if the
functions θn are defined as
</p>
<p>θn(x)&equiv;
</p>
<p>⎧
⎪⎨
⎪⎩
</p>
<p>0 if x &le;&minus; 1
n
,
</p>
<p>(nx + 1)/2 if &minus; 1
n
&le; x &le; 1
</p>
<p>n
,
</p>
<p>1 if x &ge; 1
n
,</p>
<p/>
</div>
<div class="page"><p/>
<p>7.5 Problems 237
</p>
<p>then θ &prime;n(x)&rarr; δ(x).
We write the definition of the derivative, 〈θ &prime;n, f 〉 = &minus;〈θn, f &prime;〉, in terms of
</p>
<p>integrals:
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
θ &prime;n(x)f (x) dx
</p>
<p>=&minus;
ˆ &infin;
</p>
<p>&minus;&infin;
θn(x)
</p>
<p>df
</p>
<p>dx
dx =&minus;
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
θn(x) df
</p>
<p>=&minus;
(
ˆ &minus;1/n
</p>
<p>&minus;&infin;
θn(x) df +
</p>
<p>ˆ 1/n
</p>
<p>&minus;1/n
θn(x) df +
</p>
<p>ˆ &infin;
</p>
<p>1/n
θn(x) df
</p>
<p>)
</p>
<p>=&minus;
(
</p>
<p>0 +
ˆ 1/n
</p>
<p>&minus;1/n
</p>
<p>nx + 1
2
</p>
<p>df +
ˆ &infin;
</p>
<p>1/n
df
</p>
<p>)
</p>
<p>=&minus;n
2
</p>
<p>ˆ 1/n
</p>
<p>&minus;1/n
x df &minus; 1
</p>
<p>2
</p>
<p>ˆ 1/n
</p>
<p>&minus;1/n
df &minus;
</p>
<p>ˆ &infin;
</p>
<p>1/n
df
</p>
<p>=&minus;n
2
</p>
<p>(
xf (x)
</p>
<p>∣∣1/n
&minus;1/n &minus;
</p>
<p>ˆ 1/n
</p>
<p>&minus;1/n
f (x)dx
</p>
<p>)
</p>
<p>&minus; 1
2
</p>
<p>(
f (1/n)&minus; f (&minus;1/n)
</p>
<p>)
&minus; f (&infin;)+ f (1/n).
</p>
<p>For large n, we have 1/n&asymp; 0 and f (&plusmn;1/n)&asymp; f (0). Thus,
ˆ &infin;
</p>
<p>&minus;&infin;
θ &prime;n(x)f (x) dx &asymp;&minus;
</p>
<p>n
</p>
<p>2
</p>
<p>(
1
</p>
<p>n
f
</p>
<p>(
1
</p>
<p>n
</p>
<p>)
+ 1
</p>
<p>n
f
</p>
<p>(
&minus;1
n
</p>
<p>)
&minus; 2
</p>
<p>n
f (0)
</p>
<p>)
+ f (0)
</p>
<p>&asymp; f (0).
</p>
<p>The approximation becomes equality in the limit n&rarr;&infin;. Thus,
</p>
<p>lim
n&rarr;&infin;
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
θ &prime;n(x)f (x) dx = f (0)= 〈δ0, f 〉 &rArr; θ &prime;n &rarr; δ.
</p>
<p>Note that f (&infin;)= 0 because of the assumption that all functions must van-
ish outside a finite volume.
</p>
<p>7.5 Problems
</p>
<p>7.1 Show that |‖a‖ &minus; ‖b‖| &le; ‖a &plusmn; b‖ &le; ‖a‖ + ‖b‖.
</p>
<p>7.2 Show that a convergent sequence is necessarily Cauchy.
</p>
<p>7.3 Verify that the sequence of functions {fk(x)} defined in Example 7.1.5
is a Cauchy sequence.
</p>
<p>7.4 Prove the completeness of C, using the completeness of R.</p>
<p/>
</div>
<div class="page"><p/>
<p>238 7 Hilbert Spaces
</p>
<p>7.5 Let L1(R) be the set of all functions f such that ‖f ‖ &equiv;
&acute;&infin;
&minus;&infin; |f (x)|dx
</p>
<p>is finite. This is clearly a normed vector space. Let f and g be nonzero
functions such that at no x are f (x) and g(x) both nonzero. Verify that
</p>
<p>(a) ‖f &plusmn; g‖ = ‖f ‖ + ‖g‖.
(b) ‖f + g‖2 + ‖f &minus; g‖2 = 2(‖f ‖ + ‖g‖)2.
(c) Using parts (a), (b), and Theorem 2.2.9, show that L1(R) is not an
</p>
<p>inner product space.
</p>
<p>This construction shows that not all norms arise from an inner product.
</p>
<p>7.6 Use Eq. (7.10) to derive Eq. (7.12). Hint: To find an, equate the coeffi-
cients of xn on both sides of Eq. (7.10). To find an&minus;1, multiply both sides of
Eq. (7.10) by Cn&minus;1w(x) and integrate, using the definitions of k
</p>
<p>(n)
n , k
</p>
<p>(n&minus;1)
n ,
</p>
<p>and hn.
</p>
<p>7.7 Evaluate the integral
&acute; b
</p>
<p>a
x2Cm(x)Cn(x)w(x)dx.
</p>
<p>7.8 Write a density function for two point charges q1 and q2 located at
r = r1 and r = r2, respectively.
</p>
<p>7.9 Write a density function for four point charges q1 = q , q2 =&minus;q , q3 = q
and q4 =&minus;q , located at the corners of a square of side 2a, lying in the xy-
plane, whose center is at the origin and whose first corner is at (a, a).
</p>
<p>7.10 Show that δ(f (x))= 1|f &prime;(x0)|δ(x&minus;x0), where x0 is a root of f and x is
confined to values close to x0. Hint: Make a change of variable to y = f (x).
</p>
<p>7.11 Show that
</p>
<p>δ
(
f (x)
</p>
<p>)
=
</p>
<p>m&sum;
</p>
<p>k=1
</p>
<p>1
</p>
<p>|f &prime;(xk)|
δ(x &minus; xk),
</p>
<p>where the xk&rsquo;s are all the roots of f in the interval on which f is defined.
</p>
<p>7.12 Define the distribution ρ : C&infin;(R3)&rarr;R by
</p>
<p>〈ρ,f 〉 =
&uml;
</p>
<p>S
</p>
<p>σ(r)f (r) da(r),
</p>
<p>where σ(r) is a smooth function on a smooth surface S in R3. Show that
ρ(r) is zero if r is not on S and infinite if r is on S.
</p>
<p>7.13 Define the distribution ρ : C&infin;(R3)&rarr;R by
</p>
<p>〈ρ,f 〉 =
ˆ
</p>
<p>C
</p>
<p>λ(r)f (r) dℓ(r),
</p>
<p>where λ(r) is a smooth function on a smooth curve C in R3. Show that ρ(r)
is zero if r is not on C and infinite if r is on C.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.5 Problems 239
</p>
<p>7.14 Express the three-dimensional Dirac delta function as a product of
three one-dimensional delta functions involving the coordinates in
</p>
<p>(a) cylindrical coordinates,
(b) spherical coordinates,
(c) general curvilinear coordinates.
</p>
<p>Hint: The Dirac delta function in R3 satisfies
˝
</p>
<p>δ(r)d3x = 1.
</p>
<p>7.15 Show that
&acute;&infin;
&minus;&infin; δ
</p>
<p>&prime;(x)f (x) dx =&minus;f &prime;(0) where δ&prime;(x)&equiv; d
dx
δ(x).
</p>
<p>7.16 Evaluate the following integrals:
</p>
<p>(a)
ˆ &infin;
</p>
<p>&minus;&infin;
δ
(
x2 &minus; 5x + 6
</p>
<p>)(
3x2 &minus; 7x + 2
</p>
<p>)
dx.
</p>
<p>(b)
ˆ &infin;
</p>
<p>&minus;&infin;
δ
(
x2 &minus; π2
</p>
<p>)
cosx dx.
</p>
<p>(c)
ˆ &infin;
</p>
<p>0.5
δ(sinπx)
</p>
<p>(
2
</p>
<p>3
</p>
<p>)x
dx.
</p>
<p>(d)
ˆ &infin;
</p>
<p>&minus;&infin;
δ
(
e&minus;x
</p>
<p>2)
lnx dx.
</p>
<p>Hint: Use the result of Problem 7.11.
</p>
<p>7.17 Consider |x| as a generalized function and find its derivative.
</p>
<p>7.18 Let η &isin; C&infin;(Rn) be a smooth function on Rn, and let ϕ be a distribu-
tion. Show that ηϕ is also a distribution. What is the natural definition for
ηϕ? What is (ηϕ)&prime;, the derivative of ηϕ?
</p>
<p>7.19 Show that each of the following sequences of functions approaches
δ(x) in the sense of Definition 7.4.4.
</p>
<p>(a)
n&radic;
π
e&minus;n
</p>
<p>2x2 .
</p>
<p>(b)
1 &minus; cosnx
</p>
<p>πnx2
.
</p>
<p>(c)
n
</p>
<p>π
</p>
<p>1
</p>
<p>1 + n2x2 .
</p>
<p>(d)
sinnx
</p>
<p>πx
.
</p>
<p>Hint: Approximate ϕn(x) for large n and x &asymp; 0, and then evaluate the ap-
propriate integral.
</p>
<p>7.20 Show that 12 (1 + tanhnx)&rarr; θ(x) as n&rarr;&infin;.
</p>
<p>7.21 Show that xδ&prime;(x)=&minus;δ(x).</p>
<p/>
</div>
<div class="page"><p/>
<p>8Classical Orthogonal Polynomials
</p>
<p>Example 7.2.5 discussed only one of the many types of the so-called clas-
sical orthogonal polynomials. Historically, these polynomials were discov-
ered as solutions to differential equations arising in various physical prob-
lems. Such polynomials can be produced by starting with 1, x, x2, . . . and
employing the Gram-Schmidt process. However, there is a more elegant, al-
beit less general, approach that simultaneously studies most polynomials of
interest to physicists. We will employ this approach.1
</p>
<p>8.1 General Properties
</p>
<p>Most relevant properties of the polynomials of interest are contained in
</p>
<p>Theorem 8.1.1 Consider the functions
</p>
<p>Fn(x)=
1
</p>
<p>w(x)
</p>
<p>dn
</p>
<p>dxn
</p>
<p>(
wsn
</p>
<p>)
for n= 0,1,2, . . . , (8.1)
</p>
<p>where
</p>
<p>1. F1(x) is a first-degree polynomial in x,
2. s(x) is a polynomial in x of degree less than or equal to 2 with only
</p>
<p>real roots,
3. w(x) is a strictly positive function, integrable in the interval (a, b), that
</p>
<p>satisfies the boundary conditions w(a)s(a)= 0 =w(b)s(b).
Then Fn(x) is a polynomial of degree n in x and is orthogonal&mdash;on the
interval (a, b), with weight w(x)&mdash;to any polynomial pk(x) of degree k &lt; n,
i.e.,
</p>
<p>ˆ b
</p>
<p>a
</p>
<p>pk(x)Fn(x)w(x)dx = 0 for k &lt; n.
</p>
<p>These polynomials are collectively called classical orthogonal polynomials.
</p>
<p>1This approach is due to F.G. Tricomi [Tric 55]. See also [Denn 67].
</p>
<p>S. Hassani, Mathematical Physics, DOI 10.1007/978-3-319-01195-0_8,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>241</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-01195-0_8">http://dx.doi.org/10.1007/978-3-319-01195-0_8</a></div>
</div>
<div class="page"><p/>
<p>242 8 Classical Orthogonal Polynomials
</p>
<p>Before proving the theorem, we need two lemmas:2
</p>
<p>Lemma 8.1.2 The following identity holds:
</p>
<p>dm
</p>
<p>dxm
</p>
<p>(
wsnp&le;k
</p>
<p>)
=wsn&minus;mp&le;k+m, m&le; n.
</p>
<p>Proof See Problem 8.1. �
</p>
<p>Lemma 8.1.3 All the derivatives dm/dxm(wsn) vanish at x = a and x = b,
for all values of m&lt; n.
</p>
<p>Proof Set k = 0 in the identity of the previous lemma and let p&le;0 = 1. Then
we have d
</p>
<p>m
</p>
<p>dxm
(wsn) = wsn&minus;mp&le;m. The RHS vanishes at x = a and x = b
</p>
<p>due to the third condition stated in the theorem. �
</p>
<p>Proof of the theorem We prove the orthogonality first. The proof involves
multiple use of integration by parts:
</p>
<p>ˆ b
</p>
<p>a
</p>
<p>pk(x)Fn(x)w(x)dx =
ˆ b
</p>
<p>a
</p>
<p>pk(x)
1
</p>
<p>w
</p>
<p>[
dn
</p>
<p>dxn
</p>
<p>(
wsn
</p>
<p>)]
wdx
</p>
<p>=
ˆ b
</p>
<p>a
</p>
<p>pk(x)
d
</p>
<p>dx
</p>
<p>[
dn&minus;1
</p>
<p>dxn&minus;1
(
wsn
</p>
<p>)]
dx
</p>
<p>= pk(x)
dn&minus;1
</p>
<p>dxn&minus;1
(
wsn
</p>
<p>)∣∣∣∣
b
</p>
<p>a︸ ︷︷ ︸
=0 by Lemma 8.1.3
</p>
<p>&minus;
ˆ b
</p>
<p>a
</p>
<p>dpk
</p>
<p>dx
</p>
<p>dn&minus;1
</p>
<p>dxn&minus;1
(
wsn
</p>
<p>)
dx.
</p>
<p>This shows that each integration by parts transfers one differentiation from
wsn to pk and introduces a minus sign. Thus, after k integrations by parts,
we get
</p>
<p>ˆ b
</p>
<p>a
</p>
<p>pk(x)Fn(x)w(x)dx = (&minus;1)k
ˆ b
</p>
<p>a
</p>
<p>dkpk
</p>
<p>dxk
</p>
<p>dn&minus;k
</p>
<p>dxn&minus;k
(
wsn
</p>
<p>)
dx
</p>
<p>= C
ˆ b
</p>
<p>a
</p>
<p>d
</p>
<p>dx
</p>
<p>[
dn&minus;k&minus;1
</p>
<p>dxn&minus;k&minus;1
(
wsn
</p>
<p>)]
dx
</p>
<p>= C d
n&minus;k&minus;1
</p>
<p>dxn&minus;k&minus;1
(
wsn
</p>
<p>)∣∣∣∣
b
</p>
<p>a
</p>
<p>= 0,
</p>
<p>where we have used the fact that the kth derivative of a polynomial of de-
gree k is a constant. Note that n &minus; k &minus; 1 &ge; 0 because k &lt; n, so that the
last line of the equation is well-defined. The last equality follows from
Lemma 8.1.3.
</p>
<p>2Recall that p&le;k is a generic polynomial with degree less than or equal to k.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.1 General Properties 243
</p>
<p>To prove the first part of the theorem, we use Lemma 8.1.2 with k = 0,
p&le;0 &equiv; p0 = 1, and m= n to get
</p>
<p>dn
</p>
<p>dxn
</p>
<p>(
wsn
</p>
<p>)
=wp&le;n, or Fn(x)=
</p>
<p>1
</p>
<p>w
</p>
<p>dn
</p>
<p>dxn
</p>
<p>(
wsn
</p>
<p>)
= p&le;n.
</p>
<p>To prove that Fn(x) is a polynomial of degree precisely equal to n, we write
Fn(x) = p&le;n&minus;1(x) + k(n)n xn, multiply both sides by w(x)Fn(x), and inte-
grate over (a, b):
</p>
<p>ˆ b
</p>
<p>a
</p>
<p>[
Fn(x)
</p>
<p>]2
w(x)dx
</p>
<p>=
ˆ b
</p>
<p>a
</p>
<p>p&le;n&minus;1Fn(x)w(x)dx + k(n)n
ˆ b
</p>
<p>a
</p>
<p>xnFn(x)w(x)dx.
</p>
<p>The LHS is a positive quantity because both w(x) and [Fn(x)]2 are pos-
itive, and the first integral on the RHS vanishes by the first part of the
proof. Therefore, the second term on the RHS cannot be zero. In particu-
lar, k(n)n �= 0, and Fn(x) is of degree n. �
</p>
<p>It is customary to introduce a normalization constant in the definition of
Fn(x), and write
</p>
<p>Fn(x)=
1
</p>
<p>Knw
</p>
<p>dn
</p>
<p>dxn
</p>
<p>(
wsn
</p>
<p>)
. (8.2)
</p>
<p>This equation is called the generalized Rodriguez formula. For historical
generalized Rodriguez
</p>
<p>formula
reasons, different polynomial functions are normalized differently, which is
why Kn is introduced here.
</p>
<p>From Theorem 8.1.1 it is clear that the sequence {Fn(x)}&infin;n=0 forms an
orthogonal set of polynomials on [a, b] with weight function w(x).
</p>
<p>All the varieties of classical orthogonal polynomials were discovered as differential equation for
classical orthogonal
</p>
<p>polynomials
</p>
<p>solutions of differential equations. Here, we give a single generic differential
equation satisfied by all the Fn&rsquo;s. The proof is outlined in Problem 8.4.
</p>
<p>Proposition 8.1.4 Let k(1)1 be the coefficient of x in F1(x) and σ2 the coef-
ficient of x2 in s(x). Then the orthogonal polynomials Fn satisfy the differ-
ential equation
</p>
<p>d
</p>
<p>dx
</p>
<p>(
ws
</p>
<p>dFn
</p>
<p>dx
</p>
<p>)
=wλnFn(x) where λn =K1k(1)1 n+ σ2n(n&minus; 1).
</p>
<p>We shall study the differential equation above in the context of the Sturm-
Liouville problem (see Chap. 19), which is an eigenvalue problem involving
differential operators.</p>
<p/>
</div>
<div class="page"><p/>
<p>244 8 Classical Orthogonal Polynomials
</p>
<p>8.2 Classification
</p>
<p>Let us now investigate the consequences of various choices of s(x). We start
with F1(x), and note that it satisfies Eq. (8.2) with n= 1:
</p>
<p>F1(x)=
1
</p>
<p>K1w
</p>
<p>d
</p>
<p>dx
(ws), or
</p>
<p>1
</p>
<p>ws
</p>
<p>d
</p>
<p>dx
(ws)= K1F1(x)
</p>
<p>s
,
</p>
<p>which can be integrated to yield ws = A exp(
&acute;
</p>
<p>K1F1(x) dx/s) where A is
a constant. On the other hand, being a polynomial of degree 1, F1(x) can be
written as F1(x)= k(1)1 x + k
</p>
<p>(0)
1 . It follows that
</p>
<p>w(x)s(x)=A exp
(
ˆ
</p>
<p>K1(k
(1)
1 x + k
</p>
<p>(0)
1 )
</p>
<p>s
dx
</p>
<p>)
,
</p>
<p>w(a)s(a)= 0 =w(b)s(b).
(8.3)
</p>
<p>Next we look at the three choices for s(x): a constant, a polynomial of
degree 1, and a polynomial of degree 2. For a constant s(x), Eq. (8.3) can
be easily integrated:
</p>
<p>w(x)s(x)=A exp
(
ˆ
</p>
<p>K1(k
(1)
1 x + k
</p>
<p>(0)
1 )
</p>
<p>s
dx
</p>
<p>)
&equiv;A exp
</p>
<p>(
ˆ
</p>
<p>(2αx + β)dx
)
</p>
<p>=Aeαx2+βx+C = Beαx2+βx,
</p>
<p>2α &equiv;K1k(1)1 /s, β &equiv;K1k
(0)
1 /s, B &equiv;AeC .
</p>
<p>The interval (a, b) is determined by w(a)s(a)= 0 =w(b)s(b), which yields
</p>
<p>Beαa
2+βa = 0 = Beαb2+βb.
</p>
<p>For nonzero B , the only way that this equality can hold is for α to be negative
and for a and b to be infinite. Since a &lt; b, we must take a = &minus;&infin; and
b = +&infin;. With y =&radic;|α|(x + β/(2α)) and choosing B = s exp(β2/(4α)),
we obtain w(y) = exp(&minus;y2). We also take the constant s to be 1. This is
always possible by a proper choice of constants such as B .
</p>
<p>If the degree of s is 1, then s(x)= σ1x + σ0 and
</p>
<p>w(x)(σ1x + σ0)=A exp
(
ˆ
</p>
<p>K1(k
(1)
1 x + k
</p>
<p>(0)
1 )
</p>
<p>σ1x + σ0
dx
</p>
<p>)
</p>
<p>=A exp
[
ˆ
</p>
<p>(
K1k
</p>
<p>(1)
1
</p>
<p>σ1
+ K1k
</p>
<p>(0)
1 &minus;K1k
</p>
<p>(1)
1 σ0/σ1
</p>
<p>σ1x + σ0
</p>
<p>)
dx
</p>
<p>]
</p>
<p>&equiv; B(σ1x + σ0)ρeγ x,
</p>
<p>where γ = K1k(1)1 /σ1, ρ = K1k
(0)
1 /σ1 &minus; K1k
</p>
<p>(1)
1 σ0/σ
</p>
<p>2
1 , and B is A modi-
</p>
<p>fied by the constant of integration. The last equation above must satisfy the
boundary conditions at a and b:
</p>
<p>B(σ1a + σ0)ρeγ a = 0 = B(σ1b+ σ0)ρeγ b,</p>
<p/>
</div>
<div class="page"><p/>
<p>8.3 Recurrence Relations 245
</p>
<p>Table 8.1 Special cases of Jacobi polynomials
</p>
<p>μ ν w(x) Polynomial
</p>
<p>0 0 1 Legendre, Pn(x)
</p>
<p>λ&minus; 12 λ&minus; 12 (1 &minus; x2)λ&minus;1/2 Gegenbauer, Cλn (x), λ &gt;&minus; 12
&minus; 12 &minus; 12 (1 &minus; x2)&minus;1/2 Chebyshev of the first kind, Tn(x)
1
2
</p>
<p>1
2 (1 &minus; x2)1/2 Chebyshev of the second kind, Un(x)
</p>
<p>which give a =&minus;σ0/σ1, ρ &gt; 0, γ &lt; 0, and b =+&infin;. With appropriate re-
definition of variables and parameters, we can write
</p>
<p>w(y)= yνe&minus;y, ν &gt;&minus;1, s(x)= x, a = 0, b=+&infin;.
</p>
<p>Similarly, we can obtain the weight function and the interval of integra-
tion for the case when s(x) is of degree 2. This result, as well as the results
obtained above, are collected in the following proposition.
</p>
<p>Proposition 8.2.1 If the conditions of Theorem 8.1.1 prevail, then Hermite, Laguerre, and
Jacobi polynomials
</p>
<p>(a) For s(x) of degree zero we get w(x)= e&minus;x2 with s(x)= 1, a =&minus;&infin;,
and b =+&infin;. The resulting polynomials are called Hermite polyno-
mials and are denoted by Hn(x).
</p>
<p>(b) For s(x) of degree 1, we obtain w(x)= xνe&minus;x with ν &gt;&minus;1, s(x)= x,
a = 0, and b = +&infin;. The resulting polynomials are called Laguerre
polynomials and are denoted by Lνn(x).
</p>
<p>(c) For s(x) of degree 2, we get w(x)= (1+x)μ(1&minus;x)ν with μ,ν &gt;&minus;1,
s(x) = 1 &minus; x2, a = &minus;1, and b = +1. The resulting polynomials are
called Jacobi polynomials and are denoted by Pμ,νn (x).
</p>
<p>Jacobi polynomials are themselves divided into other subcategories de-
pending on the values of μ and ν. The most common and widely used of
these are collected in Table 8.1. Note that the definition of each of the pre-
ceding polynomials involves a &ldquo;standardization,&rdquo; which boils down to a par-
ticular choice of Kn in the generalized Rodriguez formula.
</p>
<p>8.3 Recurrence Relations
</p>
<p>Besides the recurrence relations obtained in Sect. 7.2, we can use the differ-
ential equation of Proposition 8.1.4 to construct new recurrence relations in-
volving derivatives. These relations apply only to classical orthogonal poly-
nomials, and not to general ones. We start with Eq. (7.12)
</p>
<p>Fn+1(x)= (αnx + βn)Fn(x)+ γnFn&minus;1(x), (8.4)</p>
<p/>
</div>
<div class="page"><p/>
<p>246 8 Classical Orthogonal Polynomials
</p>
<p>differentiate both sides twice, and substitute for the second derivative from
the differential equation of Proposition 8.1.4. This will yield
</p>
<p>2wsαnF
&prime;
n +
</p>
<p>[
αn
</p>
<p>d
</p>
<p>dx
(ws)+wλn(αnx + βn)
</p>
<p>]
Fn
</p>
<p>&minus;wλn+1Fn+1 +wγnλn&minus;1Fn&minus;1 = 0. (8.5)
</p>
<p>Historical Notes
</p>
<p>Karl Gustav Jacob Jacobi (1804&ndash;1851) was the second son born to a well-to-do Jewish
</p>
<p>Karl Gustav Jacob Jacobi
</p>
<p>1804&ndash;1851
</p>
<p>banking family in Potsdam. An obviously bright young man, Jacobi was soon moved
to the highest class in spite of his youth and remained at the gymnasium for four years
only because he could not enter the university until he was sixteen. He excelled at the
University of Berlin in all the classical subjects as well as mathematical studies, the topic
he soon chose as his career. He passed the examination to become a secondary school
teacher, then later the examination that allowed university teaching, and joined the faculty
at Berlin at the age of twenty. Since promotion there appeared unlikely, he moved in 1826
to the University of K&ouml;nigsberg in search of a more permanent position. He was known as
a lively and creative lecturer who often injected his latest research topics into the lectures.
He began what is now a common practice at most universities&mdash;the research seminar&mdash;for
the most advanced students and his faculty collaborators. The Jacobi &ldquo;school&rdquo;, together
with the influence of Bessel and Neumann (also at K&ouml;nigsberg), sparked a renewal of
mathematical excellence in Germany.
In 1843 Jacobi fell gravely ill with diabetes. After seeing his condition, Dirichlet, with
the help of von Humboldt, secured a donation to enable Jacobi to spend several months
in Italy, a therapy recommended by his doctor. The friendly atmosphere and healthful cli-
mate there soon improved his condition. Jacobi was later given royal permission to move
from K&ouml;nigsberg to Berlin so that his health would not be affected by the harsh winters
in the former location. A salary bonus given to Jacobi to offset the higher cost of living in
the capital was revoked after he made some politically sensitive remarks in an impromptu
speech. A permanent position at Berlin was also refused, and the reduced salary and lack
of security caused considerable hardship for Jacobi and his family. Only after he accepted
a position in Vienna did the Prussian government recognize the desirability of keeping
the distinguished mathematician within its borders, offering him special concessions that
together with his love for his homeland convinced Jacobi to stay. In 1851 Jacobi died after
contracting both influenza and smallpox.
Jacobi&rsquo;s mathematical reputation began largely with his heated competition with Abel in
the study of elliptic functions. Legendre, formerly the star of such studies, wrote Jacobi
of his happiness at having &ldquo;lived long enough to witness these magnanimous contests
between two young athletes equally strong&rdquo;. Although Jacobi and Abel could reasonably
be considered contemporary researchers who arrived at many of the same results inde-
pendently, Jacobi suggested the names &ldquo;Abelian functions&rdquo; and &ldquo;Abelian theorem&rdquo; in a
review he wrote for Crelle&rsquo;s Journal. Jacobi also extended his discoveries in elliptic func-
tions to number theory and the theory of integration. He also worked in other areas of
number theory, such as the theory of quadratic forms and the representation of integers as
sums of squares and cubes. He presented the well-known Jacobian, or functional deter-
minant, in 1841. To physicists, Jacobi is probably best known for his work in dynamics
with the form introduced by Hamilton. Although elegant and quite general, Hamiltonian
dynamics did not lend itself to easy solution of many practical problems in mechanics. In
the spirit of Lagrange, Poisson, and others, Jacobi investigated transformations of Hamil-
ton&rsquo;s equations that preserved their canonical nature (loosely speaking, that preserved the
Poisson brackets in each representation). After much work and a little simplification, the
resulting equations of motion, now known as Hamilton-Jacobi equations, allowed Jacobi
to solve several important problems in ordinary and celestial mechanics. Clebsch and
later Helmholtz amplified their use in other areas of physics.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.3 Recurrence Relations 247
</p>
<p>We can get another recurrence relation involving derivatives by substitut-
ing (8.4) in (8.5) and simplifying:
</p>
<p>2wsαnF
&prime;
n +
</p>
<p>[
αn
</p>
<p>d
</p>
<p>dx
(ws)+w(λn &minus; λn+1)(αnx + βn)
</p>
<p>]
Fn
</p>
<p>+wγn(λn&minus;1 &minus; λn+1)Fn&minus;1 = 0. (8.6)
</p>
<p>Two other recurrence relations can be obtained by differentiating equa-
tions (8.6) and (8.5), respectively, and using the differential equation for Fn.
Now solve the first equation so obtained for γn(d/dx)(wFn&minus;1) and substi-
tute the result in the second equation. After simplification, the result will
be
</p>
<p>2wαnλnFn +
d
</p>
<p>dx
</p>
<p>{[
αn
</p>
<p>d
</p>
<p>dx
(ws)+w(λn &minus; λn&minus;1)(αnx + βn)
</p>
<p>]
Fn
</p>
<p>}
</p>
<p>+ (λn&minus;1 &minus; λn+1)
d
</p>
<p>dx
(wFn+1)= 0. (8.7)
</p>
<p>Finally, we record one more useful recurrence relation:
</p>
<p>An(x)Fn &minus; λn+1(αnx + βn)
dw
</p>
<p>dx
Fn+1 + γnλn&minus;1(αnx + βn)
</p>
<p>dw
</p>
<p>dx
Fn&minus;1
</p>
<p>+Bn(x)F &prime;n+1 + γnDn(x)F &prime;n&minus;1 = 0, (8.8)
</p>
<p>where
</p>
<p>An(x)= (αnx + βn)
[
</p>
<p>2wαnλn + αn
d2
</p>
<p>dx2
(ws)+ λn(αnx + βn)
</p>
<p>dw
</p>
<p>dx
</p>
<p>]
</p>
<p>&minus; α2n
d
</p>
<p>dx
(ws),
</p>
<p>Bn(x)= αn
d
</p>
<p>dx
(ws)&minus;w(αnx + βn)(λn+1 &minus; λn),
</p>
<p>Dn(x)=w(αnx + βn)(λn&minus;1 &minus; λn)&minus; αn
d
</p>
<p>dx
(ws).
</p>
<p>Details of the derivation of this relation are left for the reader. All these
recurrence relations seem to be very complicated. However, complexity is
the price we pay for generality. When we work with specific orthogonal
polynomials, the equations simplify considerably. For instance, for Hermite useful recurrence
</p>
<p>relations for Hermite and
</p>
<p>Legendre polynomials
</p>
<p>and Legendre polynomials Eq. (8.6) yields, respectively,
</p>
<p>H &prime;n = 2nHn&minus;1, and
(
1 &minus; x2
</p>
<p>)
P &prime;n + nxPn &minus; nPn&minus;1 = 0. (8.9)
</p>
<p>Also, applying Eq. (8.7) to Legendre polynomials gives
</p>
<p>P &prime;n+1 &minus; xP &prime;n &minus; (n+ 1)Pn = 0, (8.10)
</p>
<p>and Eq. (8.8) yields
</p>
<p>P &prime;n+1 &minus; P &prime;n&minus;1 &minus; (2n+ 1)Pn = 0. (8.11)</p>
<p/>
</div>
<div class="page"><p/>
<p>248 8 Classical Orthogonal Polynomials
</p>
<p>It is possible to find many more recurrence relations by manipulating the
existing recurrence relations.
</p>
<p>Before studying specific orthogonal polynomials, let us pause for a mo-
ment to appreciate the generality and elegance of the preceding discussion.
With a few assumptions and a single defining equation we have severely
restricted the choice of the weight function and with it the choice of the
interval (a, b). We have nevertheless exhausted the list of the so-called clas-
sical orthogonal polynomials.
</p>
<p>8.4 Details of Specific Examples
</p>
<p>We now construct the specific polynomials used frequently in physics. We
have seen that the four parameters Kn, k
</p>
<p>(n)
n , k
</p>
<p>(n&minus;1)
n , and hn determine all the
</p>
<p>properties of the polynomials. Once Kn is fixed by some standardization,
we can determine all the other parameters: k(n)n and k
</p>
<p>(n&minus;1)
n will be given by
</p>
<p>the generalized Rodriguez formula, and hn can be calculated as follows:
</p>
<p>hn =
ˆ b
</p>
<p>a
</p>
<p>F 2n (x)w(x)dx =
ˆ b
</p>
<p>a
</p>
<p>(
k(n)n x
</p>
<p>n + &middot; &middot; &middot;
)
Fn(x)w(x)dx
</p>
<p>= k(n)n
ˆ b
</p>
<p>a
</p>
<p>wxn
1
</p>
<p>Knw
</p>
<p>dn
</p>
<p>dxn
</p>
<p>(
wsn
</p>
<p>)
dx = k
</p>
<p>(n)
n
</p>
<p>Kn
</p>
<p>ˆ b
</p>
<p>a
</p>
<p>xn
d
</p>
<p>dx
</p>
<p>[
dn&minus;1
</p>
<p>dxn&minus;1
(
wsn
</p>
<p>)]
dx
</p>
<p>= k
(n)
n
</p>
<p>Kn
xn
</p>
<p>dn&minus;1
</p>
<p>dxn&minus;1
(
wsn
</p>
<p>)∣∣∣∣
b
</p>
<p>a
</p>
<p>&minus; k
(n)
n
</p>
<p>Kn
</p>
<p>ˆ b
</p>
<p>a
</p>
<p>d
</p>
<p>dx
</p>
<p>(
xn
</p>
<p>) dn&minus;1
dxn&minus;1
</p>
<p>(
wsn
</p>
<p>)
dx.
</p>
<p>The first term of the last line is zero by Lemma 8.1.3. It is clear that
each integration by parts introduces a minus sign and shifts one differen-
tiation from wsn to xn. Thus, after n integrations by parts and noting that
d0/dx0(wsn)=wsn and dn/dxn(xn)= n!, we obtain
</p>
<p>hn =
(&minus;1)nk(n)n n!
</p>
<p>Kn
</p>
<p>ˆ b
</p>
<p>a
</p>
<p>wsn dx. (8.12)
</p>
<p>8.4.1 Hermite Polynomials
</p>
<p>The Hermite polynomials are standardized such that Kn = (&minus;1)n. Thus, thesummary of properties
of Hermite polynomials generalized Rodriguez formula (8.2) and Proposition 8.2.1 give
</p>
<p>Hn(x)= (&minus;1)nex
2 dn
</p>
<p>dxn
</p>
<p>(
e&minus;x
</p>
<p>2)
. (8.13)
</p>
<p>It is clear that each time e&minus;x
2
</p>
<p>is differentiated, a factor of &minus;2x is intro-
duced. The highest power of x is obtained when we differentiate e&minus;x
</p>
<p>2
n
</p>
<p>times. This yields (&minus;1)nex2(&minus;2x)ne&minus;x2 = 2nxn &rArr; k(n)n = 2n.
To obtain k(n&minus;1)n , we find it helpful to see whether the polynomial is
</p>
<p>even or odd. We substitute &minus;x for x in Eq. (8.13) and get Hn(&minus;x) =
(&minus;1)nHn(x), which shows that if n is even (odd), Hn is an even (odd) poly-
nomial, i.e., it can have only even (odd) powers of x. In either case, the</p>
<p/>
</div>
<div class="page"><p/>
<p>8.4 Details of Specific Examples 249
</p>
<p>next-highest power of x in Hn(x) is not n &minus; 1 but n &minus; 2. Thus, the coef-
ficient of xn&minus;1 is zero for Hn(x), and we have k
</p>
<p>(n&minus;1)
n = 0. For hn, we use
</p>
<p>(8.12) to obtain hn =
&radic;
π 2nn!.
</p>
<p>Next we calculate the recurrence relation of Eq. (7.12). We can readily
calculate the constants needed: αn = 2, βn = 0, γn =&minus;2n. Then substitute
these in Eq. (7.12) to obtain
</p>
<p>Hn+1(x)= 2xHn(x)&minus; 2nHn&minus;1(x). (8.14)
</p>
<p>Other recurrence relations can be obtained similarly.
Finally, the differential equation of Hn(x) is obtained by first noting that
</p>
<p>K1 = &minus;1, σ2 = 0, F1(x) = 2x &rArr; k(1)1 = 2. All of this gives λn = &minus;2n,
which can be used in the equation of Proposition 8.1.4 to get
</p>
<p>d2Hn
</p>
<p>dx2
&minus; 2x dHn
</p>
<p>dx
+ 2nHn = 0. (8.15)
</p>
<p>8.4.2 Laguerre Polynomials
</p>
<p>For Laguerre polynomials, the standardization is Kn = n!. Thus, the gener- summary of properties
of Laguerre polynomialsalized Rodriguez formula (8.2) and Proposition 8.2.1 give
</p>
<p>Lνn(x)=
1
</p>
<p>n!xνe&minus;x
dn
</p>
<p>dxn
</p>
<p>(
xνe&minus;xxn
</p>
<p>)
= 1
</p>
<p>n!x
&minus;νex
</p>
<p>dn
</p>
<p>dxn
</p>
<p>(
xn+νe&minus;x
</p>
<p>)
. (8.16)
</p>
<p>To find k(n)n we note that differentiating e&minus;x does not introduce any new
powers of x but only a factor of &minus;1. Thus, the highest power of x is obtained
by leaving xn+ν alone and differentiating e&minus;x n times. This gives
</p>
<p>1
</p>
<p>n!x
&minus;νexxn+ν(&minus;1)ne&minus;x = (&minus;1)
</p>
<p>n
</p>
<p>n! x
n &rArr; k(n)n =
</p>
<p>(&minus;1)n
n! .
</p>
<p>We may try to check the evenness or oddness of Lνn(x); however, this will
not be helpful because changing x to &minus;x distorts the RHS of Eq. (8.16).
In fact, k(n&minus;1)n �= 0 in this case, and it can be calculated by noticing that
the next-highest power of x is obtained by adding the first derivative of
xn+ν n times and multiplying the result by (&minus;1)n&minus;1, which comes from
differentiating e&minus;x . We obtain
</p>
<p>1
</p>
<p>n!x
&minus;νex
</p>
<p>[
(&minus;1)n&minus;1n(n+ ν)xn+ν&minus;1e&minus;x
</p>
<p>]
= (&minus;1)
</p>
<p>n&minus;1(n+ ν)
(n&minus; 1)! x
</p>
<p>n&minus;1,
</p>
<p>and therefore k(n&minus;1)n = (&minus;1)n&minus;1(n+ ν)/(n&minus; 1)!.
Finally, for hn we get
</p>
<p>hn =
(&minus;1)n[(&minus;1)n/n!]n!
</p>
<p>n!
</p>
<p>ˆ &infin;
</p>
<p>0
xνe&minus;xxn dx = 1
</p>
<p>n!
</p>
<p>ˆ &infin;
</p>
<p>0
xn+νe&minus;x dx.
</p>
<p>If ν is not an integer (and it need not be), the integral on the RHS cannot be
evaluated by elementary methods. In fact, this integral occurs so frequently
in mathematical applications that it is given a special name, the gamma</p>
<p/>
</div>
<div class="page"><p/>
<p>250 8 Classical Orthogonal Polynomials
</p>
<p>function. A detailed discussion of this function can be found in Chap. 12.
At this point, we simply note that
</p>
<p>the gamma function
</p>
<p>Ŵ(z+ 1)&equiv;
ˆ &infin;
</p>
<p>0
xze&minus;x dx, Ŵ(n+ 1)= n! for n &isin;N, (8.17)
</p>
<p>and write hn as
</p>
<p>hn =
Ŵ(n+ ν + 1)
</p>
<p>n! =
Ŵ(n+ ν + 1)
Ŵ(n+ 1) .
</p>
<p>The relevant parameters for the recurrence relation can be easily calculated:
</p>
<p>αn =&minus;
1
</p>
<p>n+ 1 , βn =
2n+ ν + 1
</p>
<p>n+ 1 , γn =&minus;
n+ ν
n+ 1 .
</p>
<p>Substituting these in Eq. (7.12) and simplifying yields
</p>
<p>(n+ 1)Lνn+1 = (2n+ ν + 1 &minus; x)Lνn &minus; (n+ ν)Lνn&minus;1.
</p>
<p>With k(1)1 =&minus;1 and σ2 = 0, we get λn =&minus;n, and the differential equation
of Proposition 8.1.4 becomes
</p>
<p>x
d2Lνn
</p>
<p>dx2
+ (ν + 1 &minus; x)dL
</p>
<p>ν
n
</p>
<p>dx
+ nLνn = 0. (8.18)
</p>
<p>8.4.3 Legendre Polynomials
</p>
<p>Instead of discussing the Jacobi polynomials as a whole, we will discusssummary of properties
of Legendre polynomials a special case of them, the Legendre polynomials Pn(x), which are more
</p>
<p>widely used in physics.
With μ= 0 = ν, corresponding to the Legendre polynomials, the weight
</p>
<p>function for the Jacobi polynomials reduces to w(x)= 1. The standardiza-
tion is Kn = (&minus;1)n2nn!. Thus, the generalized Rodriguez formula reads
</p>
<p>Pn(x)=
(&minus;1)n
2nn!
</p>
<p>dn
</p>
<p>dxn
</p>
<p>[(
1 &minus; x2
</p>
<p>)n]
. (8.19)
</p>
<p>To find k(n)n , we expand the expression in square brackets using the binomial
theorem and take the nth derivative of the highest power of x. This yields
</p>
<p>k(n)n x
n = (&minus;1)
</p>
<p>n
</p>
<p>2nn!
dn
</p>
<p>dxn
</p>
<p>[(
&minus;x2
</p>
<p>)n]= 1
2nn!
</p>
<p>dn
</p>
<p>dxn
</p>
<p>(
x2n
</p>
<p>)
</p>
<p>= 1
2nn!2n(2n&minus; 1)(2n&minus; 2) &middot; &middot; &middot; (n+ 1)x
</p>
<p>n.
</p>
<p>After some algebra (see Problem 8.15), we get k(n)n = 2
nŴ(n+ 12 )
n!Ŵ( 12 )
</p>
<p>.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.4 Details of Specific Examples 251
</p>
<p>Historical Notes
</p>
<p>Adrien-Marie Legendre (1752&ndash;1833) came from a well-to-do Parisian family and re-
</p>
<p>Adrien-Marie Legendre
</p>
<p>1752&ndash;1833
</p>
<p>ceived an excellent education in science and mathematics. His university work was ad-
vanced enough that his mentor used many of Legendre&rsquo;s essays in a treatise on mechan-
ics. A man of modest fortune until the revolution, Legendre was able to devote himself
to study and research without recourse to an academic position. In 1782 he won the prize
of the Berlin Academy for calculating the trajectories of cannonballs taking air resistance
into account. This essay brought him to the attention of Lagrange and helped pave the
way to acceptance in French scientific circles, notably the Academy of Sciences, to which
Legendre submitted numerous papers. In July 1784 he submitted a paper on planetary or-
bits that contained the now-famous Legendre polynomials, mentioning that Lagrange had
been able to &ldquo;present a more complete theory&rdquo; in a recent paper by using Legendre&rsquo;s
results. In the years that followed, Legendre concentrated his efforts in number theory,
celestial mechanics, and the theory of elliptic functions. In addition, he was a prolific cal-
culator, producing large tables of the values of special functions, and he also authored an
elementary textbook that remained in use for many decades. In 1824 Legendre refused
to vote for the government&rsquo;s candidate for Institut National. Because of this, his pension
was stopped and he died in poverty and in pain at the age of 80 after several years of
failing health.
Legendre produced a large number of useful ideas but did not always develop them in the
most rigorous manner, claiming to hold the priority for an idea if he had presented merely
a reasonable argument for it. Gauss, with whom he had several quarrels over priority,
considered rigorous proof the standard of ownership. To Legendre&rsquo;s credit, however, he
was an enthusiastic supporter of his young rivals Abel and Jacobi and gave their work
considerable attention in his writings. Especially in the theory of elliptic functions, the
area of competition with Abel and Jacobi, Legendre is considered more of a trailblazer
than a great builder. Hermite wrote that Legendre &ldquo;is considered the founder of the theory
of elliptic functions&rdquo; and &ldquo;greatly smoothed the way for his successors&rdquo;, but notes that
the recognition of the double periodicity of the inverse function, which allowed the great
progress of others, was missing from Legendre&rsquo;s work.
Legendre also contributed to practical efforts in science and mathematics. He and two of
his contemporaries were assigned in 1787 to a panel conducting geodetic work in cooper-
ation with the observatories at Paris and Greenwich. Four years later the same panel mem-
bers were appointed as the Academy&rsquo;s commissioners to undertake the measurements and
calculations necessary to determine the length of the standard meter. Legendre&rsquo;s seem-
ingly tireless skill at calculating produced large tables of the values of trigonometric and
elliptic functions, logarithms, and solutions to various special equations.
In his famous textbook El&eacute;ments de g&eacute;om&eacute;trie (1794) he gave a simple proof that π is
irrational and conjectured that it is not the root of any algebraic equation of finite degree
with rational coefficients. The textbook was somewhat dogmatic in its presentation of
ordinary Euclidean thought and includes none of the non-Euclidean ideas beginning to be
formed around that time. It was Legendre who first gave a rigorous proof of the theorem
(assuming all of Euclid&rsquo;s postulates, of course) that the sum of the angles of a triangle
is &ldquo;equal to two right angles&rdquo;. Very little of his research in this area was of memorable
quality. The same could possibly be argued for the balance of his writing, but one must
acknowledge the very fruitful ideas he left behind in number theory and elliptic functions
and, of course, the introduction of Legendre polynomials and the important Legendre
transformation used both in thermodynamics and Hamiltonian mechanics.
</p>
<p>To find k(n&minus;1)n , we look at the evenness or oddness of the polynomi-
als. By an investigation of the Rodriguez formula&mdash;as in our study of Her-
mite polynomials&mdash;we note that Pn(&minus;x)= (&minus;1)nPn(x), which tells us that
Pn(x) is either even or odd. In either case, x will not have an (n &minus; 1)st
power. Therefore, k(n&minus;1)n = 0.
</p>
<p>We now calculate hn as given by (8.12):
</p>
<p>hn =
(&minus;1)nk(n)n n!
</p>
<p>Kn
</p>
<p>ˆ 1
</p>
<p>&minus;1
</p>
<p>(
1 &minus; x2
</p>
<p>)n
dx = 2
</p>
<p>nŴ(n+ 12 )/Ŵ( 12 )
2nn!
</p>
<p>ˆ 1
</p>
<p>&minus;1
</p>
<p>(
1 &minus; x2
</p>
<p>)n
dx.</p>
<p/>
</div>
<div class="page"><p/>
<p>252 8 Classical Orthogonal Polynomials
</p>
<p>The integral can be evaluated by repeated integration by parts (see Prob-
lem 8.16). Substituting the result in the expression above yields hn =
2/(2n+ 1).
</p>
<p>We need αn, βn and γn for the recurrence relation:
</p>
<p>αn =
k
(n+1)
n+1
k
(n)
n
</p>
<p>= 2
n+1Ŵ(n+ 1 + 12 )
(n+ 1)!Ŵ( 12 )
</p>
<p>n!Ŵ( 12 )
2nŴ(n+ 12 )
</p>
<p>= 2n+ 1
n+ 1 ,
</p>
<p>where we used the relation Ŵ(z+ 1)= zŴ(z), an important property of the
gamma function. We also have βn = 0 (because k(n&minus;1)n = 0 = k(n)n+1) and
γn =&minus;n/(n+ 1). Therefore, the recurrence relation is
</p>
<p>(n+ 1)Pn+1(x)= (2n+ 1)xPn(x)&minus; nPn&minus;1(x). (8.20)
</p>
<p>Now we use K1 = &minus;2, P1(x) = x &rArr; k(1)1 = 1, and σ2 = &minus;1 to obtain
λn =&minus;n(n+ 1), which yields the following differential equation:
</p>
<p>d
</p>
<p>dx
</p>
<p>[(
1 &minus; x2
</p>
<p>)dPn
dx
</p>
<p>]
=&minus;n(n+ 1)Pn. (8.21)
</p>
<p>This can also be expressed as
</p>
<p>(
1 &minus; x2
</p>
<p>)d2Pn
dx2
</p>
<p>&minus; 2x dPn
dx
</p>
<p>+ n(n+ 1)Pn = 0. (8.22)
</p>
<p>8.4.4 Other Classical Orthogonal Polynomials
</p>
<p>The rest of the classical orthogonal polynomials can be constructed simi-
larly. For the sake of completeness, we merely quote the results.
</p>
<p>Jacobi Polynomials, P
μ,ν
n (x)
</p>
<p>Standardization:
</p>
<p>Kn = (&minus;2)nn!
Constants:
</p>
<p>k(n)n = 2&minus;n
Ŵ(2n+μ+ ν + 1)
n!Ŵ(n+μ+ ν + 1) , k
</p>
<p>(n&minus;1)
n =
</p>
<p>n(ν &minus;μ)
2n+μ+ ν kn,
</p>
<p>hn =
2μ+ν+1Ŵ(n+μ+ 1)Ŵ(n+ ν + 1)
</p>
<p>n!(2n+μ+ ν + 1)Ŵ(n+μ+ ν + 1)
Rodriguez formula:
</p>
<p>Pμ,νn (x)=
(&minus;1)n
2nn! (1 + x)
</p>
<p>&minus;μ(1 &minus; x)&minus;ν d
n
</p>
<p>dxn
</p>
<p>[
(1 + x)μ+n(1 &minus; x)ν+n
</p>
<p>]
</p>
<p>Differential Equation:
</p>
<p>(
1 &minus; x2
</p>
<p>)d2Pμ,νn
dx2
</p>
<p>+
[
μ&minus; ν &minus; (μ+ ν + 2)x
</p>
<p>]dPμ,νn
dx
</p>
<p>+ n(n+μ+ ν + 1)Pμ,νn = 0</p>
<p/>
</div>
<div class="page"><p/>
<p>8.4 Details of Specific Examples 253
</p>
<p>A Recurrence Relation:
</p>
<p>2(n+ 1)(n+μ+ ν + 1)(2n+μ+ ν)Pμ,νn+1
= (2n+μ+ ν + 1)
</p>
<p>[
(2n+μ+ ν)(2n+μ+ ν + 2)x + ν2 &minus;μ2
</p>
<p>]
Pμ,νn
</p>
<p>&minus; 2(n+μ)(n+ ν)(2n+μ+ ν + 2)Pμ,νn&minus;1
</p>
<p>Gegenbauer Polynomials,Cλn(x)
</p>
<p>Standardization:
</p>
<p>Kn = (&minus;2)nn!
Ŵ(n+ λ+ 12 )Ŵ(2λ)
Ŵ(n+ 2λ)Ŵ(λ+ 12 )
</p>
<p>Constants:
</p>
<p>k(n)n =
2n
</p>
<p>n!
Ŵ(n+ λ)
Ŵ(λ)
</p>
<p>, k(n&minus;1)n = 0, hn =
&radic;
πŴ(n+ 2λ)Ŵ(λ+ 12 )
n!(n+ λ)Ŵ(2λ)Ŵ(λ)
</p>
<p>Rodriguez Formula:
</p>
<p>Cλn(x)=
(&minus;1)nŴ(n+ 2λ)Ŵ(λ+ 12 )
2nn!Ŵ(n+ λ+ 12 )Ŵ(2λ)
</p>
<p>(
1 &minus; x2
</p>
<p>)&minus;λ+1/2 dn
dxn
</p>
<p>[(
1 &minus; x2
</p>
<p>)n+λ&minus;1/2]
</p>
<p>Differential Equation:
</p>
<p>(
1 &minus; x2
</p>
<p>)d2Cλn
dx2
</p>
<p>&minus; (2λ+ 1)x dC
λ
n
</p>
<p>dx
+ n(n+ 2λ)Cλn = 0
</p>
<p>A Recurrence Relation:
</p>
<p>(n+ 1)Cλn+1 = 2(n+ λ)xCλn &minus; (n+ 2λ&minus; 1)Cλn&minus;1
</p>
<p>Chebyshev Polynomials of the First Kind, Tn(x)
</p>
<p>Standardization:
</p>
<p>Kn = (&minus;1)n
(2n)!
2nn!
</p>
<p>Constants:
</p>
<p>k(n)n = 2n&minus;1, k(n&minus;1)n = 0, hn =
π
</p>
<p>2
</p>
<p>Rodriguez Formula:
</p>
<p>Tn(x)=
(&minus;1)n2nn!
</p>
<p>(2n)!
(
1 &minus; x2
</p>
<p>)1/2 dn
dxn
</p>
<p>[(
1 &minus; x2
</p>
<p>)n&minus;1/2]
</p>
<p>Differential Equation:
</p>
<p>(
1 &minus; x2
</p>
<p>)d2Tn
dx2
</p>
<p>&minus; x dTn
dx
</p>
<p>+ n2Tn = 0
</p>
<p>A Recurrence Relation:
</p>
<p>Tn+1 = 2xTn &minus; Tn&minus;1</p>
<p/>
</div>
<div class="page"><p/>
<p>254 8 Classical Orthogonal Polynomials
</p>
<p>Chebyshev Polynomials of the Second Kind,Un(x)
</p>
<p>Standardization:
</p>
<p>Kn = (&minus;1)n
(2n+ 1)!
2n(n+ 1)!
</p>
<p>Constants:
</p>
<p>k(n)n = 2n, k(n&minus;1)n = 0, hn =
π
</p>
<p>2
</p>
<p>Rodriguez Formula:
</p>
<p>Un(x)=
(&minus;1)n2n(n+ 1)!
</p>
<p>(2n+ 1)!
(
1 &minus; x2
</p>
<p>)&minus;1/2 dn
dxn
</p>
<p>[(
1 &minus; x2
</p>
<p>)n+1/2]
</p>
<p>Differential Equation:
</p>
<p>(
1 &minus; x2
</p>
<p>)d2Un
dx2
</p>
<p>&minus; 3x dUn
dx
</p>
<p>+ n(n+ 2)Un = 0
</p>
<p>A Recurrence Relation:
</p>
<p>Un+1 = 2xUn &minus;Un&minus;1
</p>
<p>8.5 Expansion in Terms of Orthogonal Polynomials
</p>
<p>Having studied the different classical orthogonal polynomials, we can now
use them to write an arbitrary function f &isin; L2w(a, b) as a series of these
polynomials. If we denote a complete set of orthogonal (not necessarily
classical) polynomials by |Ck〉 and the given function by |f 〉, we may write
</p>
<p>|f 〉 =
&infin;&sum;
</p>
<p>k=0
ak|Ck〉, (8.23)
</p>
<p>where ak is found by multiplying both sides of the equation by 〈Ci | and
using the orthogonality of the |Ck〉&rsquo;s:
</p>
<p>〈Ci |f 〉 =
&infin;&sum;
</p>
<p>k=0
ak〈Ci |Ck〉 = ai〈Ci |Ci〉 &rArr; ai =
</p>
<p>〈Ci |f 〉
〈Ci |Ci〉
</p>
<p>. (8.24)
</p>
<p>This is written in function form as
</p>
<p>ai =
&acute; b
</p>
<p>a
C&lowast;i (x)f (x)w(x)dx
</p>
<p>&acute; b
</p>
<p>a
|Ci(x)|2w(x)dx
</p>
<p>. (8.25)
</p>
<p>We can also &ldquo;derive&rdquo; the functional form of Eq. (8.23) by multiplying both
of its sides by 〈x| and using the fact that 〈x|f 〉 = f (x) and 〈x|Ck〉 = Ck(x).
The result will be
</p>
<p>f (x)=
&infin;&sum;
</p>
<p>k=0
akCk(x). (8.26)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.5 Expansion in Terms of Orthogonal Polynomials 255
</p>
<p>Fig. 8.1 The voltage is +V0 for the upper hemisphere, where 0 &le; θ &lt; π/2, or where
0 &lt; cos θ &le; 1. It is &minus;V0 for the lower hemisphere, where π/2 &lt; θ &le; π , or where
&minus;1 &le; cos θ &lt; 0
</p>
<p>Example 8.5.1 The solution of Laplace&rsquo;s equation in spherically symmet-
ric electrostatic problems that are independent of the azimuthal angle is
given by
</p>
<p>Φ(r, θ)=
&infin;&sum;
</p>
<p>k=0
</p>
<p>(
bk
</p>
<p>rk+1
+ ckrk
</p>
<p>)
Pk(cos θ). (8.27)
</p>
<p>Consider two conducting hemispheres of radius a separated by a small
insulating gap at the equator. The upper hemisphere is held at potential V0
and the lower one at &minus;V0, as shown in Fig. 8.1. We want to find the potential
at points outside the resulting sphere. Since the potential must vanish at
infinity, we expect the second term in Eq. (8.27) to be absent, i.e., ck = 0 &forall;k.
To find bk , substitute a for r in (8.27) and let cos θ &equiv; x. Then,
</p>
<p>Φ(a,x)=
&infin;&sum;
</p>
<p>k=0
</p>
<p>bk
</p>
<p>ak+1
Pk(x),
</p>
<p>where
</p>
<p>Φ(a,x)=
{
&minus;V0 if &minus;1 &lt; x &lt; 0,
+V0 if 0 &lt; x &lt; 1.
</p>
<p>From Eq. (8.25), we have
</p>
<p>bk
</p>
<p>ak+1
=
</p>
<p>&acute; 1
&minus;1 Pk(x)Φ(a, x) dx
&acute; 1
&minus;1 |Pk(x)|2 dx︸ ︷︷ ︸
</p>
<p>=hk
</p>
<p>= 2k+ 1
2
</p>
<p>ˆ 1
</p>
<p>&minus;1
Pk(x)Φ(a, x) dx
</p>
<p>= 2k+ 1
2
</p>
<p>V0
</p>
<p>[
&minus;
ˆ 0
</p>
<p>&minus;1
Pk(x) dx +
</p>
<p>ˆ 1
</p>
<p>0
Pk(x) dx
</p>
<p>]
.
</p>
<p>To proceed, we rewrite the first integral:
</p>
<p>ˆ 0
</p>
<p>&minus;1
Pk(x) dx =&minus;
</p>
<p>ˆ 0
</p>
<p>+1
Pk(&minus;y)dy =
</p>
<p>ˆ 1
</p>
<p>0
Pk(&minus;y)dy = (&minus;1)k
</p>
<p>ˆ 1
</p>
<p>0
Pk(x) dx,</p>
<p/>
</div>
<div class="page"><p/>
<p>256 8 Classical Orthogonal Polynomials
</p>
<p>where we made use of the parity property of Pk(x). Therefore,
</p>
<p>bk
</p>
<p>ak+1
= 2k+ 1
</p>
<p>2
V0
</p>
<p>[
1 &minus; (&minus;1)k
</p>
<p>]ˆ 1
</p>
<p>0
Pk(x) dx.
</p>
<p>It is now clear that only odd polynomials contribute to the expansion. Using
the result of Problem 8.27, we get
</p>
<p>bk
</p>
<p>ak+1
= (&minus;1)(k&minus;1)/2 (2k + 1)(k &minus; 1)!
</p>
<p>2k( k+12 )!( k&minus;12 )!
V0, k odd,
</p>
<p>or
</p>
<p>b2m+1 = (4m+ 3)a2m+2V0(&minus;1)m
(2m)!
</p>
<p>22m+1m!(m+ 1)! .
</p>
<p>Note that Φ(a,x) is an odd function; that is, Φ(a,&minus;x) = &minus;Φ(a,x) as is
evident from its definition. Thus, only odd polynomials appear in the expan-
sion of Φ(a,x) to preserve this property. Having found the coefficients, we
can write the potential:
</p>
<p>Φ(r, θ)= V0
&infin;&sum;
</p>
<p>m=0
(&minus;1)m (4m+ 3)(2m)!
</p>
<p>22m+1m!(m+ 1)!
</p>
<p>(
a
</p>
<p>r
</p>
<p>)2m+2
P2m+1(cos θ).
</p>
<p>The place where Legendre polynomials appear most naturally is, as men-
tioned above, in the solution of Laplace&rsquo;s equation in spherical coordinates.
After the partial differential equation is transformed into three ordinary dif-
ferential equations using the method of the separation of variables, the dif-
ferential equation corresponding to the polar angle θ gives rise to solutions
of which Legendre polynomials are special cases. This differential equa-
tion simplifies to Legendre differential equation if the substitution x = cos θ
is made; in that case, the solutions will be Legendre polynomials in x, or
in cos θ . That is why the argument of Pk(x) is restricted to the interval
[&minus;1,+1].
</p>
<p>Example 8.5.2 We can expand the Dirac delta function in terms of Legen-
dre polynomial. We writeexpanding Dirac delta
</p>
<p>function in terms of
</p>
<p>Legendre polynomials
δ(x)=
</p>
<p>&infin;&sum;
</p>
<p>n=0
anPn(x), (8.28)
</p>
<p>where
</p>
<p>an =
2n+ 1
</p>
<p>2
</p>
<p>ˆ 1
</p>
<p>&minus;1
Pn(x)δ(x) dx =
</p>
<p>2n+ 1
2
</p>
<p>Pn(0). (8.29)
</p>
<p>For odd n this will give zero, because Pn(x) is an odd polynomial. This is
to be expected because δ(x) is an even function of x [δ(x)= δ(&minus;x)= 0 for
x �= 0]. To evaluate Pn(0) for even n, we use the recurrence relation (8.20)
for x = 0:
</p>
<p>(n+ 1)Pn+1(0)=&minus;nPn&minus;1(0),</p>
<p/>
</div>
<div class="page"><p/>
<p>8.6 Generating Functions 257
</p>
<p>or nPn(0) =&minus;(n&minus; 1)Pn&minus;2(0), or Pn(0) = &minus;n&minus;1n Pn&minus;2(0). Iterating this m
times, we obtain
</p>
<p>Pn(0)= (&minus;1)m
(n&minus; 1)(n&minus; 3) &middot; &middot; &middot; (n&minus; 2m+ 1)
n(n&minus; 2)(n&minus; 4) &middot; &middot; &middot; (n&minus; 2m+ 2)Pn&minus;2m(0).
</p>
<p>For n= 2m, this yields
</p>
<p>P2m(0)= (&minus;1)m
(2m&minus; 1)(2m&minus; 3) &middot; &middot; &middot;3 &middot; 1
</p>
<p>2m(2m&minus; 2) &middot; &middot; &middot;4 &middot; 2 P0(0).
</p>
<p>Now we &ldquo;fill the gaps&rdquo; in the numerator by multiplying it&mdash;and the denom-
inator, of course&mdash;by the denominator. This yields
</p>
<p>P2m(0)= (&minus;1)m
2m(2m&minus; 1)(2m&minus; 2) &middot; &middot; &middot;3 &middot; 2 &middot; 1
</p>
<p>[2m(2m&minus; 2) &middot; &middot; &middot;4 &middot; 2]2
</p>
<p>= (&minus;1)m (2m)![2mm!]2 = (&minus;1)
m (2m)!
</p>
<p>22m(m!)2 ,
</p>
<p>because P0(x)= 1. Thus, we can write
</p>
<p>δ(x)=
&infin;&sum;
</p>
<p>m=0
</p>
<p>4m+ 1
2
</p>
<p>(&minus;1)m (2m)!
22m(m!)2 P2m(x).
</p>
<p>We can also derive this expansion as follows. For any complete set of
orthonormal vectors {|fk〉}&infin;k=1, we have
</p>
<p>δ
(
x &minus; x&prime;
</p>
<p>)
=w(x)〈x|x&prime;〉 =w(x)〈x|1|x&prime;〉
</p>
<p>=w(x)〈x|
(&sum;
</p>
<p>k
</p>
<p>|fk〉〈fk|
)
|x&prime;〉 =w(x)
</p>
<p>&sum;
</p>
<p>k
</p>
<p>f &lowast;k
(
x&prime;
)
fk(x).
</p>
<p>Legendre polynomials are not orthonormal; but we can make them so by
dividing Pk(x) by h
</p>
<p>1/2
k =
</p>
<p>&radic;
2/(2k+ 1). Then, noting that w(x) = 1, we
</p>
<p>obtain
</p>
<p>δ
(
x &minus; x&prime;
</p>
<p>)
=
</p>
<p>&infin;&sum;
</p>
<p>k=0
</p>
<p>Pk(x
&prime;)&radic;
</p>
<p>2/(2k + 1)
Pk(x)&radic;
</p>
<p>2/(2k + 1) =
&infin;&sum;
</p>
<p>k=0
</p>
<p>2k+ 1
2
</p>
<p>Pk
(
x&prime;
)
Pk(x).
</p>
<p>For x&prime; = 0 we get
</p>
<p>δ(x)=
&infin;&sum;
</p>
<p>k=0
</p>
<p>2k+ 1
2
</p>
<p>Pk(0)Pk(x),
</p>
<p>which agrees with Eqs. (8.28) and (8.29).
</p>
<p>8.6 Generating Functions
</p>
<p>It is possible to generate all orthogonal polynomials of a certain kind from
a single function of two variables g(x, t) by repeated differentiation of that
function. Such a function is called a generating function. This generating generating function</p>
<p/>
</div>
<div class="page"><p/>
<p>258 8 Classical Orthogonal Polynomials
</p>
<p>Table 8.2 Generating functions for Hermite, Laguerre, Legendre, and both Chebyshev
polynomials
</p>
<p>Polynomial Generating function an
</p>
<p>Hn(x) exp(&minus;t2 + 2xt) 1n!
Lνn(x)
</p>
<p>exp[&minus;xt/(1&minus;t)]
(1&minus;t)ν+1 1
</p>
<p>Pn(x) (t
2 &minus; 2xt + 1)&minus;1/2 1
</p>
<p>Tn(x) (1 &minus; t2)(t2 &minus; 2xt + 1)&minus;1 2 if n �= 0, a0 = 1
Un(x) (t
</p>
<p>2 &minus; 2xt + 1)&minus;1 1
</p>
<p>function is assumed to be expandable in the form
</p>
<p>g(x, t)=
&infin;&sum;
</p>
<p>n=0
ant
</p>
<p>nFn(x), (8.30)
</p>
<p>so that the nth derivative of g(x, t) with respect to t evaluated at t = 0 gives
Fn(x) to within a multiplicative constant. The constant an is introduced for
convenience. Clearly, for g(x, t) to be useful, it must be in closed form. The
derivation of such a function for general Fn(x) is nontrivial, and we shall
not attempt to derive such a general generating function. Instead, we simply
quote these functions in Table 8.2, and leave the derivation of the generating
functions of Hermite and Legendre polynomials as Problems 8.12 and 8.21.
For Laguerre polynomials see [Hass 08, pp. 679&ndash;680].
</p>
<p>8.7 Problems
</p>
<p>8.1 Let n = 1 in Eq. (8.1) and solve for s dw
dx
</p>
<p>. Now substitute this in the
derivative of wsnp&le;k and show that the derivative is equal to wsn&minus;1p&le;k+1.
Repeat this process m times to prove Lemma 8.1.2.
</p>
<p>8.2 Find w(x), a, and b for the case of the classical orthogonal polynomials
in which s(x) is of second degree.
</p>
<p>8.3 Integrate by parts twice and use Lemma 8.1.2 to show that
</p>
<p>ˆ b
</p>
<p>a
</p>
<p>Fm
(
wsF &prime;n
</p>
<p>)&prime;
dx = 0 for m&lt; n.
</p>
<p>8.4 Using Lemma 8.1.2 conclude that
</p>
<p>(a) (wsF &prime;n)
&prime;/w is a polynomial of degree less than or equal to n.
</p>
<p>(b) Write (wsF &prime;n)
&prime;/w as a linear combination of Fi(x), and use their or-
</p>
<p>thogonality and Problem 8.3 to show that the linear combination col-
lapses to a single term.
</p>
<p>(c) Multiply both sides of the differential equation so obtained by Fn and
integrate. The RHS becomes hnλn. For the LHS, carry out the differ-
entiation and note that (ws)&prime;/w =K1F1.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.7 Problems 259
</p>
<p>Now show that K1F1F &prime;n + sF &prime;&prime;n is a polynomial of degree n, and that the
LHS of the differential equation yields {K1k(1)1 n + σ2n(n &minus; 1)}hn. Now
find λn.
</p>
<p>8.5 Derive the recurrence relation of Eq. (8.8). Hint: Differentiate Eq. (8.5)
and substitute for F &prime;&prime;n from the differential equation. Now multiply the re-
sulting equation by αnx + βn and substitute for (αnx + βn)F &prime;n from one of
the earlier recurrence relations.
</p>
<p>8.6 Using only the orthogonality of Hermite polynomials
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
e&minus;x
</p>
<p>2
Hm(x)Hn(x) dx =
</p>
<p>&radic;
π 2nn! δmn
</p>
<p>generate the first three of them.
</p>
<p>8.7 Use the generalized Rodriguez formula for Hermite polynomials and
integration by parts to expand x2k and x2k+1 in terms of Hermite polynomi-
als.
</p>
<p>8.8 Use the recurrence relation for Hermite polynomials to show that
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
xe&minus;x
</p>
<p>2
Hm(x)Hn(x) dx =
</p>
<p>&radic;
π 2n&minus;1n!
</p>
<p>[
δm,n&minus;1 + 2(n+ 1)δm,n+1
</p>
<p>]
.
</p>
<p>What happens when m= n?
</p>
<p>8.9 Apply the general formalism of the recurrence relations given in the
book to Hermite polynomials to find the following:
</p>
<p>Hn +H &prime;n&minus;1 &minus; 2xHn&minus;1 = 0.
</p>
<p>8.10 Show that
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
x2e&minus;x
</p>
<p>2
H 2n (x) dx =
</p>
<p>&radic;
π 2n
</p>
<p>(
n+ 1
</p>
<p>2
</p>
<p>)
n!
</p>
<p>8.11 Use a recurrence relations for Hermite polynomials to show that
</p>
<p>Hn(0)=
{
</p>
<p>0 if n is odd,
</p>
<p>(&minus;1)m (2m)!
m! if n= 2m.
</p>
<p>8.12 Differentiate the expansion of g(x, t) for Hermite polynomials with
respect to x (treating t as a constant) and choose an such that nan = an&minus;1
to obtain a differential equation for g. Solve this differential equation. To
determine the &ldquo;constant&rdquo; of integration use the result of Problem 8.11 to
show that g(0, t)= e&minus;t2 .</p>
<p/>
</div>
<div class="page"><p/>
<p>260 8 Classical Orthogonal Polynomials
</p>
<p>8.13 Use the expansion of the generating function for Hermite polynomials
to obtain
</p>
<p>&infin;&sum;
</p>
<p>m,n=0
e&minus;x
</p>
<p>2
Hm(x)Hn(x)
</p>
<p>smtn
</p>
<p>m!n! = e
&minus;x2+2x(s+t)&minus;(s2+t2).
</p>
<p>Then integrate both sides over x and use the orthogonality of the Hermite
polynomials to get
</p>
<p>&infin;&sum;
</p>
<p>n=0
</p>
<p>(st)n
</p>
<p>(n!)2
ˆ &infin;
</p>
<p>&minus;&infin;
e&minus;x
</p>
<p>2
H 2n (x) dx =
</p>
<p>&radic;
π e2st .
</p>
<p>Deduce from this the normalization constant hn of Hn(x).
</p>
<p>8.14 Using the recurrence relation of Eq. (8.14) repeatedly, show that
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
xke&minus;x
</p>
<p>2
Hm(x)Hm+n(x) dx =
</p>
<p>{
0 if n &gt; k,&radic;
π 2m(m+ k)! if n= k.
</p>
<p>8.15 Show that for Legendre polynomials, k(n)n = 2nŴ(n + 12 )/[n!Ŵ( 12 )].
Hint: Multiply and divide the expression given in the book by n!; take a
factor of 2 out of all terms in the numerator; the even terms yield a factor of
n!, and the odd terms give a gamma function.
</p>
<p>8.16 Using integration by parts several times, show that
</p>
<p>ˆ 1
</p>
<p>&minus;1
</p>
<p>(
1 &minus; x2
</p>
<p>)n
dx = 2
</p>
<p>mn(n&minus; 1) &middot; &middot; &middot; (n&minus;m+ 1)
3 &middot; 5 &middot; 7 &middot; &middot; &middot; (2m&minus; 1)
</p>
<p>ˆ 1
</p>
<p>&minus;1
x2m
</p>
<p>(
1 &minus; x2
</p>
<p>)n&minus;m
dx.
</p>
<p>Now show that
ˆ 1
</p>
<p>&minus;1
</p>
<p>(
1 &minus; x2
</p>
<p>)n
dx = 2Ŵ(
</p>
<p>1
2 )n!
</p>
<p>(2n+ 1)Ŵ(n+ 12 )
.
</p>
<p>8.17 Given that P0(x) = 1 and P1(x) = x, find P2(x), P3(x), and P4(x)
using an appropriate recurrence relation.
</p>
<p>8.18 Use the generalized Rodriguez formula to show that P0(1) = 1 and
P1(1)= 1. Now use a recurrence relation to show that Pn(1)= 1 for all n.
To be rigorous, you need to use mathematical induction.
</p>
<p>8.19 Apply the general formalism of the recurrence relations given in the
book to find the following two relations for Legendre polynomials:
</p>
<p>(a) nPn &minus; xP &prime;n + P &prime;n&minus;1 = 0.
(b) (1 &minus; x2)P &prime;n &minus; nPn&minus;1 + nxPn = 0.
</p>
<p>8.20 Show that
ˆ 1
</p>
<p>&minus;1
xnPn(x) dx =
</p>
<p>2n+1(n!)2
(2n+ 1)! .</p>
<p/>
</div>
<div class="page"><p/>
<p>8.7 Problems 261
</p>
<p>Hint: Use the definition of hn and k
(n)
n and the fact that Pn is orthogonal to
</p>
<p>any polynomial of degree lower than n.
</p>
<p>8.21 Differentiate the expansion of g(x, t) for Legendre polynomials, and
choose an = 1. For P &prime;n, you will substitute two different expressions to get
two equations. First use Eq. (8.11) with n+ 1 replaced by n, to obtain
</p>
<p>(
1 &minus; t2
</p>
<p>)dg
dx
</p>
<p>+ tg = 2
&infin;&sum;
</p>
<p>n=2
ntnPn&minus;1 + 2t.
</p>
<p>As an alternative, use Eq. (8.10) to substitute for P &prime;n and get
</p>
<p>(1 &minus; xt) dg
dx
</p>
<p>=
&infin;&sum;
</p>
<p>n=2
ntnPn&minus;1 + t.
</p>
<p>Combine the last two equations to get (t2 &minus; 2xt + 1)g&prime; = tg. Solve this
differential equation and determine the &ldquo;constant&rdquo; of integration by using
Pn(1)= 1 to show that g(1, t)= 1/(1 &minus; t).
</p>
<p>8.22 Use the generating function for Legendre polynomials to show that
Pn(1)= 1, Pn(&minus;1)= (&minus;1)n, Pn(0)= 0 for odd n, and P &prime;n(1)= n(n+ 1)/2.
</p>
<p>8.23 Both electrostatic and gravitational potential energies depend on the
quantity 1/|r &minus; r&prime;|, where r&prime; is the position vector of a point inside a charge
or mass distribution and r is the position vector of the observation point.
</p>
<p>(a) Let r lie along the z-axis, and use spherical coordinates and the defi-
nition of generating functions to show that
</p>
<p>1
</p>
<p>|r &minus; r&prime;| =
1
</p>
<p>r&gt;
</p>
<p>&infin;&sum;
</p>
<p>n=0
</p>
<p>(
r&lt;
</p>
<p>r&gt;
</p>
<p>)n
Pn(cos θ),
</p>
<p>where r&lt;(r&gt;) is the smaller (larger) of r and r &prime;, and θ is the polar
angle.
</p>
<p>(b) The electrostatic or gravitational potential energy Φ(r) is given by
</p>
<p>Φ(r)= k
˚
</p>
<p>ρ(r&prime;)
|r &minus; r&prime;| d
</p>
<p>3x&prime;,
</p>
<p>where k is a constant and ρ(r&prime;) is the (charge or mass) density func-
tion. Use the result of part (a) to show that if the density depends only
on r &prime;, and not on any angle (i.e., ρ is spherically symmetric), then
Φ(r) reduces to the potential energy of a point charge at the origin for
r &gt; r &prime;.
</p>
<p>(c) What is Φ(r) for a spherically symmetric density which extends from
the origin to a, with a ≫ r for any r of interest?
</p>
<p>(d) Show that the electric field E or gravitational field g (i.e., the negative
gradient of Φ) at any radial distance r from the origin is given by
kQ(r)
</p>
<p>r2
êr , where Q(r) is the charge or mass enclosed in a sphere of
</p>
<p>radius r .</p>
<p/>
</div>
<div class="page"><p/>
<p>262 8 Classical Orthogonal Polynomials
</p>
<p>8.24 Use the generating function for Legendre polynomials and their or-
thogonality to derive the relation
</p>
<p>ˆ 1
</p>
<p>&minus;1
</p>
<p>dx
</p>
<p>1 &minus; 2xt + t2 =
&infin;&sum;
</p>
<p>n=0
t2n
</p>
<p>ˆ 1
</p>
<p>&minus;1
P 2n (x) dx.
</p>
<p>Integrate the LHS, expand the result in powers of t , and compare these pow-
ers on both sides to obtain the normalization constant hn.
</p>
<p>8.25 Evaluate the following integrals using the expansion of the generating
function for Legendre polynomials.
</p>
<p>(a)
ˆ π
</p>
<p>0
</p>
<p>(a cos θ + b) sin θ dθ&radic;
a2 + 2ab cos θ + b2
</p>
<p>.
</p>
<p>(b)
ˆ π
</p>
<p>0
</p>
<p>(a cos2 θ + b sin2 θ) sin θ dθ&radic;
a2 + 2ab cos θ + b2
</p>
<p>.
</p>
<p>8.26 Differentiate the expansion of the Legendre polynomial generating
function with respect to x and manipulate the resulting expression to ob-
tain
</p>
<p>(
1 &minus; 2xt + t2
</p>
<p>) &infin;&sum;
</p>
<p>n=0
tnP &prime;n(x)= t
</p>
<p>&infin;&sum;
</p>
<p>n=0
tnPn(x).
</p>
<p>Equate equal powers of t on both sides to derive the recurrence relation
</p>
<p>P &prime;n+1 + P &prime;n&minus;1 &minus; 2xP &prime;n &minus; Pn = 0.
</p>
<p>8.27 Show that
</p>
<p>ˆ 1
</p>
<p>0
Pk(x) dx =
</p>
<p>⎧
⎨
⎩
δk0 if k is even,
(&minus;1)(k+1)/2(k&minus;1)!
</p>
<p>2k( k&minus;12 )!( k+12 )!
if k is odd.
</p>
<p>Hint: For even k, extend the region of integration to (&minus;1,1) and use the
orthogonality property. For odd k, note that
</p>
<p>dk&minus;1
</p>
<p>dxk&minus;1
(
1 &minus; x2
</p>
<p>)k∣∣1
0
</p>
<p>gives zero for the upper limit (by Lemma 8.1.3). For the lower limit, expand
the expression using the binomial theorem, and carry out the differentiation,
keeping in mind that only one term of the expansion contributes.
</p>
<p>8.28 Show that g(x, t) = g(&minus;x,&minus;t) for both Hermite and Legendre poly-
nomials. Now expand g(x, t) and g(&minus;x,&minus;t) and compare the coefficients
of tn to obtain the parity relations for these polynomials:parity relations
</p>
<p>Hn(&minus;x)= (&minus;1)nHn(x) and Pn(&minus;x)= (&minus;1)nPn(x).
</p>
<p>8.29 Derive the orthogonality of Legendre polynomials directly from the
differential equation they satisfy.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.7 Problems 263
</p>
<p>8.30 Expand |x| in the interval (&minus;1,+1) in terms of Legendre polynomials.
Hint: Use the result of Problem 8.27.
</p>
<p>8.31 Apply the general formalism of the recurrence relations given in the
book to find the following two relations for Laguerre polynomials:
</p>
<p>(a) nLνn &minus; (n+ ν)Lνn&minus;1 &minus; x
dLνn
dx
</p>
<p>= 0.
(b) (n+ 1)Lνn+1 &minus; (2n+ ν + 1 &minus; x)Lνn + (n+ ν)Lνn&minus;1 = 0.
</p>
<p>8.32 From the generating function for Laguerre polynomials given in Ta-
ble 8.2 deduce that Lνn(0)= Ŵ(n+ ν + 1)/[n!Ŵ(ν + 1)].
</p>
<p>8.33 Let Ln &equiv; L0n. Now differentiate both sides of
</p>
<p>g(x, t)= e
&minus;xt/(1&minus;t)
</p>
<p>1 &minus; t =
&infin;&sum;
</p>
<p>0
</p>
<p>tnLn(x)
</p>
<p>with respect to x and compare powers of t to obtain L&prime;n(0) = &minus;n and
L&prime;&prime;n(0) = 12n(n&minus; 1). Hint: Differentiate 1/(1 &minus; t) =
</p>
<p>&sum;&infin;
n=0 t
</p>
<p>n to get an ex-
pression for (1 &minus; t)&minus;2.
</p>
<p>8.34 Expand e&minus;kx as a series of Laguerre polynomials Lνn(x). Find the co-
efficients by using (a) the orthogonality of Lνn(x) and (b) the generating
function.
</p>
<p>8.35 Derive the recurrence relations given in the book for Jacobi, Gegen-
bauer, and Chebyshev polynomials.
</p>
<p>8.36 Show that Tn(&minus;x) = (&minus;1)nTn(x) and Un(&minus;x) = (&minus;1)nUn(x). Hint:
Use g(x, t)= g(&minus;x,&minus;t).
</p>
<p>8.37 Show that Tn(1) = 1, Un(1) = n + 1, Tn(&minus;1) = (&minus;1)n, Un(&minus;1) =
(&minus;1)n(n+ 1), T2m(0)= (&minus;1)m =U2m(0), and T2m+1(0)= 0 =U2m+1(0).</p>
<p/>
</div>
<div class="page"><p/>
<p>9Fourier Analysis
</p>
<p>The single most recurring theme of mathematical physics is Fourier analy-
sis. It shows up, for example, in classical mechanics and the analysis of nor-
mal modes, in electromagnetic theory and the frequency analysis of waves,
in noise considerations and thermal physics, in quantum theory and the
transformation between momentum and coordinate representations, and in
relativistic quantum field theory and creation and annihilation operation for-
malism.
</p>
<p>9.1 Fourier Series
</p>
<p>One way to begin the study of Fourier series and transforms is to invoke
a generalization of the Stone-Weierstrass Approximation Theorem (The-
orem 7.2.3), which established the completeness of monomials, xk . The
generalization of Theorem 7.2.3 permits us to find another set of orthog-
onal functions in terms of which we can expand an arbitrary function. This
generalization involves polynomials in more than one variable ([Simm 83,
pp. 160&ndash;161]):
</p>
<p>generalized
</p>
<p>Stone-Weierstrass
</p>
<p>theoremTheorem 9.1.1 (Generalized Stone-Weierstrass Theorem) If the
function f (x1, x2, . . . , xn) is continuous in the domain {ai &le; xi &le;
bi}ni=1, then it can be expanded in terms of the monomials x
</p>
<p>k1
1 x
</p>
<p>k2
2 &middot; &middot; &middot;x
</p>
<p>kn
n ,
</p>
<p>where the ki are nonnegative integers.
</p>
<p>Now let us consider functions that are periodic and investigate their ex-
pansion in terms of elementary periodic functions. We use the generalized
Stone-Weierstrass theorem with two variables, x and y. A function g(x, y)
can be written as g(x, y)=&sum;&infin;k,m=0 akmxkym. In this equation, x and y can
be considered as coordinates in the xy-plane, which in turn can be written
in terms of polar coordinates r and θ . In that case, we obtain
</p>
<p>f (r, θ)&equiv; g(r cos θ, r sin θ)=
&infin;&sum;
</p>
<p>k,m=0
akmr
</p>
<p>k+m cosk θ sinm θ.
</p>
<p>S. Hassani, Mathematical Physics, DOI 10.1007/978-3-319-01195-0_9,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>265</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-01195-0_9">http://dx.doi.org/10.1007/978-3-319-01195-0_9</a></div>
</div>
<div class="page"><p/>
<p>266 9 Fourier Analysis
</p>
<p>In particular, if we let r = 1, we obtain a function of θ alone, which upon
substitution of complex exponentials for sin θ and cos θ becomes
</p>
<p>f (θ)=
&infin;&sum;
</p>
<p>k,m=0
akm
</p>
<p>1
</p>
<p>2k
(
eiθ + e&minus;iθ
</p>
<p>)k 1
(2i)m
</p>
<p>(
eiθ &minus; e&minus;iθ
</p>
<p>)m =
&infin;&sum;
</p>
<p>n=&minus;&infin;
bne
</p>
<p>inθ ,
</p>
<p>(9.1)
where bn is a constant that depends on akm. The RHS of (9.1) is periodic
with period 2π ; thus, it is especially suitable for periodic functions f (θ)
that satisfy the periodicity condition f (θ &minus; π)= f (θ + π).
</p>
<p>We can also write Eq. (9.1) as
</p>
<p>f (θ)= b0 +
&infin;&sum;
</p>
<p>n=1
</p>
<p>(
bne
</p>
<p>inθ + b&minus;ne&minus;inθ
)
</p>
<p>= b0 +
&infin;&sum;
</p>
<p>n=1
</p>
<p>[
(bn + b&minus;n)︸ ︷︷ ︸
</p>
<p>&equiv;An
</p>
<p>cosnθ + i(bn &minus; b&minus;n)︸ ︷︷ ︸
&equiv;Bn
</p>
<p>sinnθ)
]
</p>
<p>or
</p>
<p>f (θ)= b0 +
&infin;&sum;
</p>
<p>n=1
(An cosnθ +Bn sinnθ). (9.2)
</p>
<p>If f (θ) is real, then b0, An, and Bn are also real. Equation (9.1) or (9.2) is
called the Fourier series expansion of f (θ).
</p>
<p>Let us now concentrate on the elementary periodic functions einθ . We
define the {|en〉}&infin;n=1 such that their &ldquo;θ th components&rdquo; are given by
</p>
<p>〈θ |en〉 =
1&radic;
2π
</p>
<p>einθ , where θ &isin; (&minus;π,π).
</p>
<p>These functions&mdash;or ket vectors&mdash;which belong to L2(&minus;π,π), are or-
thonormal, as can be easily verified. It can also be shown that they are
complete. In fact, for functions that are continuous on (&minus;π,π), this is a
result of the generalized Stone-Weierstrass theorem. It turns out, however,
that {|en〉}&infin;n=1 is also a complete orthonormal sequence for piecewise contin-
uous functions on (&minus;π,π).1 Therefore, any periodic piecewise continuous
</p>
<p>piecewise continuous
function of θ can be expressed as a linear combination of these orthonormal
vectors. Thus if |f 〉 &isin;L2(&minus;π,π), then
</p>
<p>|f 〉 =
&infin;&sum;
</p>
<p>n=&minus;&infin;
fn|en〉, where fn = 〈en|f 〉. (9.3)
</p>
<p>We can write this as a functional relation if we take the θ th component of
Fourier series expansion:
</p>
<p>angular expression
both sides: 〈θ |f 〉 =&sum;&infin;n=&minus;&infin; fn〈θ |en〉, or
</p>
<p>f (θ)= 1&radic;
2π
</p>
<p>&infin;&sum;
</p>
<p>n=&minus;&infin;
fne
</p>
<p>inθ (9.4)
</p>
<p>1A piecewise continuous function on a finite interval is one that has a finite number of
discontinuities in its interval of definition.</p>
<p/>
</div>
<div class="page"><p/>
<p>9.1 Fourier Series 267
</p>
<p>with fn given by
</p>
<p>fn = 〈en|1|f 〉 = 〈en|
(
ˆ π
</p>
<p>&minus;π
|θ〉〈θ |dθ
</p>
<p>)
|f 〉
</p>
<p>=
ˆ π
</p>
<p>&minus;π
〈en|θ〉〈θ |f 〉dθ =
</p>
<p>1&radic;
2π
</p>
<p>ˆ π
</p>
<p>&minus;π
e&minus;inθf (θ) dθ. (9.5)
</p>
<p>It is important to note that even though f (θ) may be defined only for
&minus;π &le; θ &le; π , Eq. (9.4) extends the domain of definition of f (θ) to all the
intervals (2k &minus; 1)π &le; θ &le; (2k + 1)π for all k &isin; Z. Thus, if a function is
to be represented by Eq. (9.4) without any specification of the interval of
definition, it must be periodic in θ . For such functions, the interval of their
definition can be translated by a factor of 2π . Thus, f (θ) with &minus;π &le; θ &le; π
is equivalent to f (θ &minus; 2mπ) with 2mπ &minus; π &le; θ &le; 2mπ + π ; both will
give the same Fourier series expansion. We shall define periodic functions fundamental cell of a
</p>
<p>periodic functionin their fundamental cell such as (&minus;π,π).
</p>
<p>Historical Notes
</p>
<p>Joseph Fourier (1768&ndash;1830) did very well as a young student of mathematics but had &ldquo;The profound study of
</p>
<p>nature is the most
</p>
<p>fruitful source of
</p>
<p>mathematical
</p>
<p>discoveries.&rdquo;
</p>
<p>Joseph Fourier
</p>
<p>set his heart on becoming an army officer. Denied a commission because he was the son
of a tailor, he went to a Benedictine school with the hope that he could continue studying
mathematics at its seminary in Paris. The French Revolution changed those plans and set
the stage for many of the personal circumstances of Fourier&rsquo;s later years, due in part to
his courageous defense of some of its victims, an action that led to his arrest in 1794.
He was released later that year, and he enrolled as a student in the Ecole Normale, which
opened and closed within a year. His performance there, however, was enough to earn him
a position as assistant lecturer (under Lagrange and Monge) in the Ecole Polytechnique.
He was an excellent mathematical physicist, was a friend of Napoleon (so far as such
people have friends), and accompanied him in 1798 to Egypt, where Fourier held various
diplomatic and administrative posts while also conducting research. Napoleon took note
of his accomplishments and, on Fourier&rsquo;s return to France in 1801, appointed him prefect
of the district of Is&egrave;re, in southeastern France, and in this capacity built the first real road
from Grenoble to Turin. He also befriended the boy Champollion, who later deciphered
the Rosetta stone as the first long step toward understanding the hieroglyphic writing of
the ancient Egyptians.
</p>
<p>Joseph Fourier
</p>
<p>1768&ndash;1830
</p>
<p>Like other scientists of his time, Fourier took up the flow of heat. The flow was of interest
as a practical problem in the handling of metals in industry and as a scientific problem
in attempts to determine the temperature in the interior of the earth, the variation of that
temperature with time, and other such questions. He submitted a basic paper on heat con-
duction to the Academy of Sciences of Paris in 1807. The paper was judged by Lagrange,
Laplace, and Legendre, and was not published, mainly due to the objections of Lagrange,
who had earlier rejected the use of trigonometric series. But the Academy did wish to en-
courage Fourier to develop his ideas, and so made the problem of the propagation of heat
the subject of a grand prize to be awarded in 1812. Fourier submitted a revised paper in
1811, which was judged by the men already mentioned and others. It won the prize but
was criticized for its lack of rigor and so was not published at that time in the M&eacute;moires
of the Academy.
Fourier developed a mastery of clear notation, some of which is still in use today. (The
modern integral sign and the placement of the limits of integration near its top and bot-
tom were introduced by Fourier.) It was also his habit to maintain close association be-
tween mathematical relations and physically measurable quantities, especially in limiting
or asymptotic cases, even performing some of the experiments himself. He was one of
the first to begin full incorporation of physical constants into his equations, and made
considerable strides toward the modern ideas of units and dimensional analysis.
Fourier continued to work on the subject of heat and, in 1822, published one of the classics
of mathematics, Th&eacute;orie Analytique de la Chaleur, in which he made extensive use of the
series that now bear his name and incorporated the first part of his 1811 paper practically</p>
<p/>
</div>
<div class="page"><p/>
<p>268 9 Fourier Analysis
</p>
<p>without change. Two years later he became secretary of the Academy and was able to
have his 1811 paper published in its original form in the M&eacute;moires.
Fourier series were of profound significance in connection with the evolution of the
concept of a function, the rigorous theory of definite integrals, and the development of
Hilbert spaces. Fourier claimed that &ldquo;arbitrary&rdquo; graphs can be represented by trigono-
metric series and should therefore be treated as legitimate functions, and it came as a
shock to many that he turned out to be right. The classical definition of the definite inte-
gral due to Riemann was first given in his fundamental paper of 1854 on the subject of
Fourier series. Hilbert thought of a function as represented by an infinite sequence, the
Fourier coefficients of the function.
Fourier himself is one of the fortunate few: his name has become rooted in all civilized
languages as an adjective that is well-known to physical scientists and mathematicians in
every part of the world.
</p>
<p>Functions are not always defined on (&minus;π,π). Let us consider a function
F(x) that is defined on (a, b) and is periodic with period L = b &minus; a. We
define a new variable,
</p>
<p>θ &equiv; 2π
L
</p>
<p>(
x &minus; a &minus; L
</p>
<p>2
</p>
<p>)
&rArr; x = L
</p>
<p>2π
θ + a + L
</p>
<p>2
,
</p>
<p>and note that f (θ)&equiv; F((L/2π)θ + a +L/2) has period (&minus;π,π) because
</p>
<p>f (θ &plusmn; π)= F
(
</p>
<p>L
</p>
<p>2π
(θ &plusmn; π)+ a + L
</p>
<p>2
</p>
<p>)
= F
</p>
<p>(
x &plusmn; L
</p>
<p>2
</p>
<p>)
</p>
<p>and F(x +L/2)= F(x &minus;L/2). If follows that we can expand the latter as
in Eq. (9.4). Using that equation, but writing θ in terms of x, we obtainFourier series expansion:
</p>
<p>general expression
</p>
<p>F(x)= F
(
</p>
<p>L
</p>
<p>2π
θ + a + L
</p>
<p>2
</p>
<p>)
= 1&radic;
</p>
<p>2π
</p>
<p>&infin;&sum;
</p>
<p>n=&minus;&infin;
fn exp
</p>
<p>[
in
</p>
<p>2π
</p>
<p>L
</p>
<p>(
x &minus; a &minus; L
</p>
<p>2
</p>
<p>)]
</p>
<p>= 1&radic;
L
</p>
<p>&infin;&sum;
</p>
<p>n=&minus;&infin;
Fne
</p>
<p>2nπix/L, (9.6)
</p>
<p>where we have introduced2 Fn &equiv;
&radic;
L/2πfne&minus;i(2πn/L)(a+L/2). Using
</p>
<p>Eq. (9.5), we can write
</p>
<p>Fn =
&radic;
</p>
<p>L
</p>
<p>2π
e&minus;i(2πn/L)(a+L/2)
</p>
<p>1&radic;
2π
</p>
<p>ˆ π
</p>
<p>&minus;π
e&minus;inθf (θ) dθ
</p>
<p>=
&radic;
L
</p>
<p>2π
e&minus;i(2πn/L)(a+L/2)
</p>
<p>ˆ a+L
</p>
<p>a
</p>
<p>e&minus;i(2πn/L)(x&minus;a&minus;L/2)F(x)
2π
</p>
<p>L
dx
</p>
<p>= 1&radic;
L
</p>
<p>ˆ b
</p>
<p>a
</p>
<p>e&minus;i(2πn/L)xF(x)dx. (9.7)
</p>
<p>The functions exp(2πinx/L)/
&radic;
L are easily seen to be orthonormal
</p>
<p>as members of L2(a, b). We can introduce {|en〉}&infin;n=1 with the &ldquo;xth com-
ponent&rdquo; given by 〈x|en〉 = (1/
</p>
<p>&radic;
L)e2πinx/L. Then the reader may check
</p>
<p>2The Fn are defined such that what they multiply in the expansion are orthonormal in the
interval (a, b).</p>
<p/>
</div>
<div class="page"><p/>
<p>9.1 Fourier Series 269
</p>
<p>Fig. 9.1 Top: The periodic square wave potential with height taken to be 1. Bottom:
Various approximations to the Fourier series of the square-wave potential. The dashed
plot is that of the first term of the series, the thick grey plot keeps 3 terms, and the solid
plot 15 terms
</p>
<p>that Eqs. (9.6) and (9.7) can be written as |F 〉 = &sum;&infin;n=&minus;&infin;Fn|en〉 with
Fn = 〈n|F 〉.
</p>
<p>Example 9.1.2 In the study of electrical circuits, periodic voltage signals
of different shapes are encountered. An example is a square wave voltage square wave voltage
of height U0, &ldquo;duration&rdquo; T , and &ldquo;rest duration&rdquo; T [see Fig. 9.1(a)]. The
potential as a function of time V (t) can be expanded as a Fourier series. The
interval is (0,2T ) because that is one whole cycle of the potential variation.
We therefore use Eq. (9.6) and write
</p>
<p>V (t)= 1&radic;
2T
</p>
<p>&infin;&sum;
</p>
<p>n=&minus;&infin;
Vne
</p>
<p>2nπit/2T , where
</p>
<p>Vn =
1&radic;
2T
</p>
<p>ˆ 2T
</p>
<p>0
e&minus;2πint/2T V (t) dt.
</p>
<p>The problem is to find Vn. This is easily done by substituting
</p>
<p>V (t)=
{
U0 if 0 &le; t &le; T ,
0 if T &le; t &le; 2T</p>
<p/>
</div>
<div class="page"><p/>
<p>270 9 Fourier Analysis
</p>
<p>in the last integral:
</p>
<p>Vn =
U0&radic;
2T
</p>
<p>ˆ T
</p>
<p>0
e&minus;inπt/T dt = U0&radic;
</p>
<p>2T
</p>
<p>(
&minus; T
inπ
</p>
<p>)[
(&minus;1)n &minus; 1
</p>
<p>]
where n �= 0
</p>
<p>=
{
</p>
<p>0 if n is even and n �= 0,&radic;
2T U0
inπ
</p>
<p>if n is odd.
</p>
<p>For n = 0, we obtain V0 = 1&radic;2T
&acute; 2T
</p>
<p>0 V (t) dt = 1&radic;2T
&acute; T
</p>
<p>0 U0 dt = U0
&radic;
</p>
<p>T
2 .
</p>
<p>Therefore, we can write
</p>
<p>V (t)= 1&radic;
2T
</p>
<p>[
U0
</p>
<p>&radic;
T
</p>
<p>2
+
</p>
<p>&radic;
2T U0
iπ
</p>
<p>( &minus;1&sum;
</p>
<p>n=&minus;&infin;
n odd
</p>
<p>1
</p>
<p>n
einπt/T +
</p>
<p>&infin;&sum;
</p>
<p>n= 1
n odd
</p>
<p>1
</p>
<p>n
einπt/T
</p>
<p>)]
</p>
<p>=U0
{
</p>
<p>1
</p>
<p>2
+ 1
</p>
<p>iπ
</p>
<p>[ &infin;&sum;
</p>
<p>n= 1
n odd
</p>
<p>1
</p>
<p>&minus;ne
&minus;inπt/T +
</p>
<p>&infin;&sum;
</p>
<p>n= 1
n odd
</p>
<p>1
</p>
<p>n
einπt/T
</p>
<p>]}
</p>
<p>=U0
{
</p>
<p>1
</p>
<p>2
+ 2
</p>
<p>π
</p>
<p>&infin;&sum;
</p>
<p>k=0
</p>
<p>1
</p>
<p>2k+ 1 sin
( [2k+ 1]πt
</p>
<p>T
</p>
<p>)}
.
</p>
<p>Figure 9.1(b) shows the graphical representation of the above infinite sum
when only a finite number of terms are present.
</p>
<p>Example 9.1.3 Another frequently used voltage is the sawtooth voltagesawtooth voltage
[see Fig. 9.2(a)]. The equation for V (t) with period T is V (t)= U0t/T for
0 &le; t &le; T , and its Fourier representation is
</p>
<p>V (t)= 1&radic;
T
</p>
<p>&infin;&sum;
</p>
<p>n=&minus;&infin;
Vne
</p>
<p>2nπit/T , where Vn =
1&radic;
T
</p>
<p>ˆ T
</p>
<p>0
e&minus;2πint/T V (t) dt.
</p>
<p>Substituting for V (t) in the integral above yields
</p>
<p>Vn =
1&radic;
T
</p>
<p>ˆ T
</p>
<p>0
e&minus;2πint/TU0
</p>
<p>t
</p>
<p>T
dt =U0T &minus;3/2
</p>
<p>ˆ T
</p>
<p>0
e&minus;2πint/T t dt
</p>
<p>=U0T &minus;3/2
(
</p>
<p>T t
</p>
<p>&minus;i2nπ e
&minus;2πint/T
</p>
<p>∣∣∣∣
T
</p>
<p>0
+ T
</p>
<p>i2nπ
</p>
<p>ˆ T
</p>
<p>0
e&minus;2πint/T dt
</p>
<p>︸ ︷︷ ︸
=0
</p>
<p>)
</p>
<p>=U0T &minus;3/2
(
</p>
<p>T 2
</p>
<p>&minus;i2nπ
</p>
<p>)
=&minus;U0
</p>
<p>&radic;
T
</p>
<p>i2nπ
where n �= 0,
</p>
<p>V0 =
1&radic;
T
</p>
<p>ˆ T
</p>
<p>0
V (t) dt = 1&radic;
</p>
<p>T
</p>
<p>ˆ T
</p>
<p>0
U0
</p>
<p>t
</p>
<p>T
dt = 1
</p>
<p>2
U0
</p>
<p>&radic;
T .</p>
<p/>
</div>
<div class="page"><p/>
<p>9.1 Fourier Series 271
</p>
<p>Fig. 9.2 Top: The periodic saw-tooth potential with height taken to be 1. Bottom: Various
approximations to the Fourier series of the sawtooth potential. The dashed plot is that of
the first term of the series, the thick grey plot keeps 3 terms, and the solid plot 15 terms
</p>
<p>Thus,
</p>
<p>V (t)= 1&radic;
T
</p>
<p>[
1
</p>
<p>2
U0
</p>
<p>&radic;
T &minus; U0
</p>
<p>&radic;
T
</p>
<p>i2π
</p>
<p>( &minus;1&sum;
</p>
<p>n=&minus;&infin;
</p>
<p>1
</p>
<p>n
ei2nπt/T +
</p>
<p>&infin;&sum;
</p>
<p>n=1
</p>
<p>1
</p>
<p>n
ei2nπt/T
</p>
<p>)]
</p>
<p>=U0
{
</p>
<p>1
</p>
<p>2
&minus; 1
</p>
<p>π
</p>
<p>&infin;&sum;
</p>
<p>n=1
</p>
<p>1
</p>
<p>n
sin
</p>
<p>(
2nπt
</p>
<p>T
</p>
<p>)}
.
</p>
<p>Figure 9.2(b) shows the graphical representation of the above series keeping
the first few terms.
</p>
<p>The foregoing examples indicate an important fact about Fourier series.
At points of discontinuity (for example, t = T in the preceding two exam-
ples), the value of the function is not defined, but the Fourier series expan-
sion assigns it a value&mdash;the average of the two values on the right and left
of the discontinuity. For instance, when we substitute t = T in the series of
Example 9.1.3, all the sine terms vanish and we obtain V (T ) = U0/2, the
average of U0 (on the left) and 0 (on the right). We express this as
</p>
<p>V (T )= 1
2
</p>
<p>[
V (T &minus; 0)+ V (T + 0)
</p>
<p>]
&equiv; 1
</p>
<p>2
lim
ǫ&rarr;0
</p>
<p>[
V (T &minus; ǫ)+ V (T + ǫ)
</p>
<p>]
.</p>
<p/>
</div>
<div class="page"><p/>
<p>272 9 Fourier Analysis
</p>
<p>This is a general property of Fourier series. In fact, the main theorem of
Fourier series, which follows, incorporates this property. (For a proof of this
theorem, see [Cour 62].)
</p>
<p>Theorem 9.1.4 The Fourier series of a function f (θ) that is piece-
wise continuous in the interval (&minus;π,π) converges to
</p>
<p>1
</p>
<p>2
</p>
<p>[
f (θ + 0)+ f (θ &minus; 0)
</p>
<p>]
for &minus;π &lt; θ &lt; π,
</p>
<p>1
</p>
<p>2
</p>
<p>[
f (π)+ f (&minus;π)
</p>
<p>]
for θ =&plusmn;π.
</p>
<p>Although we used exponential functions to find the Fourier expansion of
the two examples above, it is more convenient to start with the trigonometric
series when the expansion of a real function is sought. Equation (9.2) already
gives such an expansion. All we need to do now is find expressions for An
and Bn. From the definitions of An and the relation between bn and fn we
get
</p>
<p>An = bn + b&minus;n =
1&radic;
2π
</p>
<p>(fn + f&minus;n)
</p>
<p>= 1&radic;
2π
</p>
<p>(
1&radic;
2π
</p>
<p>ˆ π
</p>
<p>&minus;π
e&minus;inθf (θ) dθ + 1&radic;
</p>
<p>2π
</p>
<p>ˆ π
</p>
<p>&minus;π
einθf (θ) dθ
</p>
<p>)
</p>
<p>= 1
2π
</p>
<p>ˆ π
</p>
<p>&minus;π
</p>
<p>[
e&minus;inθ + einθ
</p>
<p>]
f (θ) dθ = 1
</p>
<p>π
</p>
<p>ˆ π
</p>
<p>&minus;π
cosnθf (θ) dθ. (9.8)
</p>
<p>Similarly,
</p>
<p>Bn =
1
</p>
<p>π
</p>
<p>ˆ π
</p>
<p>&minus;π
sinnθf (θ) dθ,
</p>
<p>b0 =
1&radic;
2π
</p>
<p>f0 =
1
</p>
<p>2π
</p>
<p>ˆ π
</p>
<p>&minus;π
f (θ) dθ &equiv; 1
</p>
<p>2
A0.
</p>
<p>(9.9)
</p>
<p>So, for a function f (θ) defined in (&minus;π,π), the Fourier trigonometric series
is as in Eq. (9.2) with the coefficients given by Eqs. (9.8) and (9.9). For a
function F(x), defined on (a, b), the trigonometric series becomes
</p>
<p>F(x)= 1
2
A0 +
</p>
<p>&infin;&sum;
</p>
<p>n=1
</p>
<p>(
An cos
</p>
<p>2nπx
</p>
<p>L
+Bn sin
</p>
<p>2nπx
</p>
<p>L
</p>
<p>)
, (9.10)
</p>
<p>where
</p>
<p>An =
2
</p>
<p>L
</p>
<p>ˆ b
</p>
<p>a
</p>
<p>cos
</p>
<p>(
2nπx
</p>
<p>L
</p>
<p>)
F(x)dx,
</p>
<p>Bn =
2
</p>
<p>L
</p>
<p>ˆ b
</p>
<p>a
</p>
<p>sin
</p>
<p>(
2nπx
</p>
<p>L
</p>
<p>)
F(x)dx.
</p>
<p>(9.11)</p>
<p/>
</div>
<div class="page"><p/>
<p>9.1 Fourier Series 273
</p>
<p>A convenient rule to remember is that for even (odd) functions&mdash;which
are necessarily defined on a symmetric interval around the origin&mdash;only co-
sine (sine) terms appear in the Fourier expansion.
</p>
<p>It is useful to have a representation of the Dirac delta function in terms
of the present orthonormal basis of Fourier expansion. First we note that we
can represent the delta function in terms of a series in any set of orthonormal
functions (see Problem 9.23):
</p>
<p>δ
(
x &minus; x&prime;
</p>
<p>)
=
&sum;
</p>
<p>n
</p>
<p>fn(x)f
&lowast;
n
</p>
<p>(
x&prime;
)
w(x). (9.12)
</p>
<p>Next we use the basis of the Fourier expansion for which w(x)= 1. We then
obtain
</p>
<p>δ
(
x &minus; x&prime;
</p>
<p>)
=
</p>
<p>&infin;&sum;
</p>
<p>n=&minus;&infin;
</p>
<p>e2πinx/L&radic;
L
</p>
<p>e&minus;2πinx
&prime;/L
</p>
<p>&radic;
L
</p>
<p>= 1
L
</p>
<p>&infin;&sum;
</p>
<p>n=&minus;&infin;
e2πin(x&minus;x
</p>
<p>&prime;)/L.
</p>
<p>9.1.1 The Gibbs Phenomenon
</p>
<p>The plot of the Fourier series expansions in Figs. 9.1(b) and 9.2(b) exhibit
a feature that is common to all such expansions: At the discontinuity of
the periodic function, the truncated Fourier series overestimates the actual
function. This is called the Gibbs phenomenon, and is the subject of this Gibbs phenomenon
subsection.
</p>
<p>Let us approximate the infinite series with a finite sum. Then
</p>
<p>fN (θ)=
1&radic;
2π
</p>
<p>N&sum;
</p>
<p>n=&minus;N
fne
</p>
<p>inθ = 1&radic;
2π
</p>
<p>N&sum;
</p>
<p>n=&minus;N
einθ
</p>
<p>1&radic;
2π
</p>
<p>ˆ 2π
</p>
<p>0
e&minus;inθ
</p>
<p>&prime;
f
(
θ &prime;
)
dθ &prime;
</p>
<p>= 1
2π
</p>
<p>ˆ 2π
</p>
<p>0
dθ &prime;f
</p>
<p>(
θ &prime;
) N&sum;
</p>
<p>n=&minus;N
ein(θ&minus;θ
</p>
<p>&prime;),
</p>
<p>where we substituted Eq. (9.5) in the sum and, without loss of generality,
changed the interval of integration from (&minus;π,π) to (0,2π). Problem 9.2
shows that
</p>
<p>N&sum;
</p>
<p>n=&minus;N
ein(θ&minus;θ
</p>
<p>&prime;) = sin[(N +
1
2 )(θ &minus; θ &prime;)]
</p>
<p>sin[ 12 (θ &minus; θ &prime;)]
.
</p>
<p>It follows that
</p>
<p>fN (θ)=
1
</p>
<p>2π
</p>
<p>ˆ 2π
</p>
<p>0
dθ &prime;f
</p>
<p>(
θ &prime;
) sin[(N + 12 )(θ &minus; θ &prime;)]
</p>
<p>sin[ 12 (θ &minus; θ &prime;)]
</p>
<p>= 1
2π
</p>
<p>ˆ 2π&minus;θ
</p>
<p>&minus;θ
dφf (φ + θ) sin[(N +
</p>
<p>1
2 )φ]
</p>
<p>sin( 12φ)︸ ︷︷ ︸
&equiv;S(φ)
</p>
<p>&equiv; 1
2π
</p>
<p>ˆ 2π&minus;θ
</p>
<p>&minus;θ
dφf (φ + θ)S(φ). (9.13)</p>
<p/>
</div>
<div class="page"><p/>
<p>274 9 Fourier Analysis
</p>
<p>We want to investigate the behavior of fN at a discontinuity of f . By
translating the limits of integration if necessary, we can assume that the
discontinuity of f occurs at a point α such that 0 �= α �= 2π . Let us denote
the jump at this discontinuity for the function itself by �f , and for its finite
Fourier sum by �fN :
</p>
<p>�f &equiv; f (α + ǫ)&minus; f (α &minus; ǫ), �fN &equiv; fN (α + ǫ)&minus; fN (α &minus; ǫ).
</p>
<p>Then, we have
</p>
<p>�fN =
1
</p>
<p>2π
</p>
<p>ˆ 2π&minus;α&minus;ǫ
</p>
<p>&minus;α&minus;ǫ
dφf (φ + α + ǫ)S(φ)
</p>
<p>&minus; 1
2π
</p>
<p>ˆ 2π&minus;α+ǫ
</p>
<p>&minus;α+ǫ
dφf (φ + α &minus; ǫ)S(φ)
</p>
<p>= 1
2π
</p>
<p>{
ˆ &minus;α+ǫ
</p>
<p>&minus;α&minus;ǫ
dφf (φ + α+ ǫ)S(φ)
</p>
<p>+
ˆ 2π&minus;α&minus;ǫ
</p>
<p>&minus;α+ǫ
dφf (φ + α + ǫ)S(φ)
</p>
<p>}
</p>
<p>&minus; 1
2π
</p>
<p>{
ˆ 2π&minus;α&minus;ǫ
</p>
<p>&minus;α+ǫ
dφf (φ + α&minus; ǫ)S(φ)
</p>
<p>+
ˆ 2π&minus;α+ǫ
</p>
<p>2π&minus;α&minus;ǫ
dφf (φ + α &minus; ǫ)S(φ)
</p>
<p>}
</p>
<p>= 1
2π
</p>
<p>{
ˆ &minus;α+ǫ
</p>
<p>&minus;α&minus;ǫ
dφf (φ + α+ ǫ)S(φ)
</p>
<p>&minus;
ˆ 2π&minus;α+ǫ
</p>
<p>2π&minus;α&minus;ǫ
dφf (φ + α &minus; ǫ)S(φ)
</p>
<p>}
</p>
<p>+ 1
2π
</p>
<p>ˆ 2π&minus;α&minus;ǫ
</p>
<p>&minus;α+ǫ
dφ
</p>
<p>[
f (φ + α + ǫ)&minus; f (φ + α &minus; ǫ)
</p>
<p>]
S(φ).
</p>
<p>The first two integrals give zero because of the small ranges of integration
and the continuity of the integrands in those intervals. The integrand of the
third integral is almost zero for all values of the range of integration except
when φ &asymp; 0. Hence, we can confine the integration to the small interval
(&minus;δ,+δ) for which the difference in the square brackets is simply �f . It
now follows that
</p>
<p>�fN (δ)&asymp;
�f
</p>
<p>2π
</p>
<p>ˆ δ
</p>
<p>&minus;δ
</p>
<p>sin[(N + 12 )φ]
sin( 12φ)
</p>
<p>dφ &asymp; �f
π
</p>
<p>ˆ δ
</p>
<p>0
</p>
<p>sin[(N + 12 )φ]
1
2φ
</p>
<p>dφ,
</p>
<p>where we have emphasized the dependence of fN on δ and approximated
the sine in the denominator by its argument, a good approximation due to
the smallness of φ. The reader may find the plot of the integrand in Fig. 7.3,
where it is shown that the major contribution to the integral comes from the
interval [0,π/(N + 12 )], where π/(N + 12 ) is the first zero of the integrand.
Furthermore, it is clear that if the upper limit is larger than π/(N + 12 ), the</p>
<p/>
</div>
<div class="page"><p/>
<p>9.1 Fourier Series 275
</p>
<p>result of the integral will decrease, because in each interval of length 2π , the
area below the horizontal axis is larger than that above. Therefore, if we are
interested in the maximum overshoot of the finite sum, we must set the upper maximum overshoot in
</p>
<p>Gibbs phenomenon
</p>
<p>calculated
</p>
<p>limit equal to π/(N + 12 ). It follows firstly that the maximum overshoot of
the finite sum occurs at π/(N + 12 )&asymp; π/N to the right of the discontinuity.
Secondly, the amount of the maximum overshoot is
</p>
<p>(�fN )max &asymp;
2�f
</p>
<p>π
</p>
<p>ˆ π/(N+ 12 )
</p>
<p>0
</p>
<p>sin[
&equiv;x︷ ︸︸ ︷
</p>
<p>(N + 12 )φ]
φ
</p>
<p>dφ
</p>
<p>= 2
π
�f
</p>
<p>ˆ π
</p>
<p>0
</p>
<p>sinx
</p>
<p>x
dx &asymp; 1.179�f. (9.14)
</p>
<p>Thus
</p>
<p>Box 9.1.5 (The Gibbs Phenomenon) The finite (large-N ) sum ap-
proximation of the discontinuous function overshoots the function it-
self at a discontinuity by about 18 percent.
</p>
<p>9.1.2 Fourier Series in Higher Dimensions
</p>
<p>It is instructive to generalize the Fourier series to more than one dimension.
This generalization is especially useful in crystallography and solid-state
physics, which deal with three-dimensional periodic structures. To gener-
alize to N dimensions, we first consider a special case in which an N -
dimensional periodic function is a product of N one-dimensional periodic
functions. That is, we take the N functions
</p>
<p>f (j)(x)= 1&radic;
Lj
</p>
<p>&infin;&sum;
</p>
<p>k=&minus;&infin;
f
(j)
</p>
<p>k e
2iπkx/Lj , j = 1,2, . . . ,N,
</p>
<p>and multiply them on both sides to obtain
</p>
<p>F(r)= f (1)(x1)f (2)(x2) &middot; &middot; &middot;f (N)(xN )=
1&radic;
V
</p>
<p>&sum;
</p>
<p>k
</p>
<p>Fke
igk&middot;r, (9.15)
</p>
<p>where we have used the following new notations:
</p>
<p>F(r)&equiv; f (1)(x1)f (2)(x2) &middot; &middot; &middot;f (N)(xN ), V = L1L2 &middot; &middot; &middot;LN ,
k &equiv; (k1, k2, . . . , kN ), Fk &equiv; fk1 &middot; &middot; &middot;fkN ,
</p>
<p>gk = 2π(k1/L1, . . . , kN/LN ), r = (x1, x2, . . . , xN ).
</p>
<p>We take Eq. (9.15) as the definition of the Fourier series for any periodic
function of N variables (not just the product of N functions of a single
variable). However, application of (9.15) requires some clarification. In one</p>
<p/>
</div>
<div class="page"><p/>
<p>276 9 Fourier Analysis
</p>
<p>dimension, the shape of the smallest region of periodicity is unique. It is
simply a line segment of length L, for example. In two and more dimen-
sions, however, such regions may have a variety of shapes. For instance, in
two dimensions, they can be rectangles, pentagons, hexagons, and so forth.
Thus, we let V in Eq. (9.15) stand for a primitive cell of the N -dimensional
lattice. This cell is important in solid-state physics, and (in three dimensions)
is called the Wigner-Seitz cell.Wigner-Seitz cell
</p>
<p>It is customary to absorb the factor 1/
&radic;
V into Fk, and write
</p>
<p>F(r)=
&sum;
</p>
<p>k
</p>
<p>Fke
igk&middot;r &hArr; Fk =
</p>
<p>1
</p>
<p>V
</p>
<p>ˆ
</p>
<p>V
</p>
<p>F(r)e&minus;igk&middot;r dNx, (9.16)
</p>
<p>where the sum is a multiple sum over (k1, . . . , kN ) and the integral is a
multiple integral over a single Wigner-Seitz cell.
</p>
<p>Recall that F(r) is a periodic function of r. This means that when r is
changed by R, where R is a vector describing the boundaries of a cell, then
we should get the same function: F(r + R) = F(r). When substituted in
(9.16), this yields
</p>
<p>F(r + R)=
&sum;
</p>
<p>k
</p>
<p>Fke
igk&middot;(r+R) =
</p>
<p>&sum;
</p>
<p>k
</p>
<p>eigk&middot;RFkeigk&middot;r,
</p>
<p>which is equal to F(r) if
</p>
<p>eigk&middot;R = 1, (9.17)
i.e., if gk &middot; R is an integral multiple of 2π .
</p>
<p>In three dimensions R = m1a1 +m2a2 +m3a3, where m1,m2, and m3
are integers and a1,a2, and a3 are crystal axes, which are not generally
orthogonal. On the other hand, gk = n1b1 +n2b2 +n3b3, where n1, n2, and
n3 are integers, and b1,b2, and b3 are the reciprocal lattice vectors definedreciprocal lattice vectors
by
</p>
<p>b1 =
2π(a2 &times; a3)
a1 &middot; (a2 &times; a3)
</p>
<p>, b2 =
2π(a3 &times; a1)
a1 &middot; (a2 &times; a3)
</p>
<p>, b3 =
2π(a1 &times; a2)
a1 &middot; (a2 &times; a3)
</p>
<p>.
</p>
<p>The reader may verify that bi &middot; aj = 2πδij . Thus
</p>
<p>gk &middot; R =
(
</p>
<p>3&sum;
</p>
<p>i=1
nibi
</p>
<p>)
&middot;
(
</p>
<p>3&sum;
</p>
<p>j=1
mjaj
</p>
<p>)
=
&sum;
</p>
<p>i,j
</p>
<p>nimjbi &middot; aj
</p>
<p>= 2π
3&sum;
</p>
<p>j=1
mjnj = 2π(integer),
</p>
<p>and Eq. (9.17) is satisfied.
</p>
<p>9.2 Fourier Transform
</p>
<p>The Fourier series representation of F(x) is valid for the entire real line as
long as F(x) is periodic. However, most functions encountered in physical
applications are defined in some interval (a, b) without repetition beyond</p>
<p/>
</div>
<div class="page"><p/>
<p>9.2 Fourier Transform 277
</p>
<p>Fig. 9.3 (a) The function we want to represent. (b) The Fourier series representation of
the function
</p>
<p>that interval. It would be useful if we could also expand such functions in
some form of Fourier &ldquo;series&rdquo;.
</p>
<p>One way to do this is to start with the periodic series and then let the
period go to infinity while extending the domain of the definition of the
function. As a specific case, suppose we are interested in representing a
function f (x) that is defined only for the interval (a, b) and is assigned the
value zero everywhere else [see Fig. 9.3(a)]. To begin with, we might try
the Fourier series representation, but this will produce a repetition of our
function. This situation is depicted in Fig. 9.3(b).
</p>
<p>Next we may try a function gΛ(x) defined in the interval (a &minus;Λ/2, b+
Λ/2), where Λ is an arbitrary positive number:
</p>
<p>gΛ(x)=
</p>
<p>⎧
⎪⎨
⎪⎩
</p>
<p>0 if a &minus;Λ/2 &lt; x &lt; a,
f (x) if a &lt; x &lt; b,
</p>
<p>0 if b &lt; x &lt; b+Λ/2.
</p>
<p>This function, which is depicted in Fig. 9.4, has the Fourier series represen-
tation
</p>
<p>gΛ(x)=
1&radic;
</p>
<p>L+Λ
</p>
<p>&infin;&sum;
</p>
<p>n=&minus;&infin;
gΛ,ne
</p>
<p>2iπnx/(L+Λ), (9.18)
</p>
<p>where
</p>
<p>gΛ,n =
1&radic;
</p>
<p>L+Λ
</p>
<p>ˆ b+Λ/2
</p>
<p>a&minus;Λ/2
e&minus;2iπnx/(L+Λ)gΛ(x) dx. (9.19)
</p>
<p>We have managed to separate various copies of the original periodic
function by Λ. It should be clear that if Λ &rarr; &infin;, we can completely iso-
late the function and stop the repetition. Let us investigate the behavior of
Eqs. (9.18) and (9.19) as Λ grows without bound. First, we notice that the</p>
<p/>
</div>
<div class="page"><p/>
<p>278 9 Fourier Analysis
</p>
<p>Fig. 9.4 By introducing the parameter Λ, we have managed to separate the copies of the
function
</p>
<p>quantity kn defined by kn &equiv; 2nπ/(L + Λ) and appearing in the exponent
becomes almost continuous. In other words, as n changes by one unit, kn
changes only slightly. This suggests that the terms in the sum in Eq. (9.18)
can be lumped together in j intervals of width �nj , giving
</p>
<p>gΛ(x)&asymp;
&infin;&sum;
</p>
<p>j=&minus;&infin;
</p>
<p>gΛ(kj )&radic;
L+Λ
</p>
<p>eikj x�nj ,
</p>
<p>where kj &equiv; 2njπ/(L+Λ), and gΛ(kj )&equiv; gΛ,nj . Substituting �nj = [(L+
Λ)/2π]�kj in the above sum, we obtain
</p>
<p>gΛ(x)&asymp;
&infin;&sum;
</p>
<p>j=&minus;&infin;
</p>
<p>gΛ(kj )&radic;
L+Λ
</p>
<p>eikj x
L+Λ
</p>
<p>2π
�kj =
</p>
<p>1&radic;
2π
</p>
<p>&infin;&sum;
</p>
<p>j=&minus;&infin;
g̃Λ(kj )e
</p>
<p>ikj x�kj ,
</p>
<p>where we introduced g̃Λ(kj ) defined by g̃Λ(kj )&equiv;
&radic;
(L+Λ)/2π gΛ(kj ). It
</p>
<p>is now clear that the preceding sum approaches an integral in the limit that
Λ&rarr;&infin;. In the same limit, gΛ(x)&rarr; f (x), and we have
</p>
<p>f (x)= 1&radic;
2π
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
f̃ (k)eikxdk, (9.20)
</p>
<p>where
</p>
<p>f̃ (k)&equiv; lim
Λ&rarr;&infin;
</p>
<p>g̃Λ(kj )= lim
Λ&rarr;&infin;
</p>
<p>&radic;
L+Λ
</p>
<p>2π
gΛ(kj )
</p>
<p>= lim
Λ&rarr;&infin;
</p>
<p>&radic;
L+Λ
</p>
<p>2π
</p>
<p>1&radic;
L+Λ
</p>
<p>ˆ b+Λ/2
</p>
<p>a&minus;Λ/2
e&minus;ikj xgΛ(x) dx
</p>
<p>= 1&radic;
2π
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
f (x)e&minus;ikxdx. (9.21)
</p>
<p>Equations (9.20) and (9.21) are called the Fourier integral transforms ofFourier integral
transforms f̃ (k) and f (x), respectively.
</p>
<p>Example 9.2.1 Let us evaluate the Fourier transform of the function de-
fined by
</p>
<p>f (x)=
{
b if |x|&lt; a,
0 if |x|&gt; a</p>
<p/>
</div>
<div class="page"><p/>
<p>9.2 Fourier Transform 279
</p>
<p>Fig. 9.5 The square &ldquo;bump&rdquo; function
</p>
<p>(see Fig. 9.5). From (9.21) we have
</p>
<p>f̃ (k)= 1&radic;
2π
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
f (x)e&minus;ikxdx = b&radic;
</p>
<p>2π
</p>
<p>ˆ a
</p>
<p>&minus;a
e&minus;ikxdx = 2ab&radic;
</p>
<p>2π
</p>
<p>(
sin ka
</p>
<p>ka
</p>
<p>)
,
</p>
<p>which is the function encountered (and depicted) in Example 7.3.2.
Let us discuss this result in detail. First, note that if a &rarr;&infin;, the function
</p>
<p>f (x) becomes a constant function over the entire real line, and we get
</p>
<p>f̃ (k)= 2b&radic;
2π
</p>
<p>lim
a&rarr;&infin;
</p>
<p>sin ka
</p>
<p>k
= 2b&radic;
</p>
<p>2π
πδ(k)
</p>
<p>by the result of Example 7.3.2. This is the Fourier transform of an
everywhere-constant function (see Problem 9.12). Next, let b &rarr; &infin; and
a &rarr; 0 in such a way that 2ab, which is the area under f (x), is 1. Then
f (x) will approach the delta function, and f̃ (k) becomes
</p>
<p>f̃ (k)= lim
b&rarr;&infin;
a &rarr; 0
</p>
<p>2ab&radic;
2π
</p>
<p>sin ka
</p>
<p>ka
= 1&radic;
</p>
<p>2π
lim
a&rarr;0
</p>
<p>sin ka
</p>
<p>ka
= 1&radic;
</p>
<p>2π
.
</p>
<p>So the Fourier transform of the delta function is the constant 1/
&radic;
</p>
<p>2π .
Finally, we note that the width of f (x) is �x = 2a, and the width of f̃ (k)
</p>
<p>is roughly the distance, on the k-axis, between its first two roots, k+ and k&minus;,
on either side of k = 0: �k = k+ &minus; k&minus; = 2π/a. Thus increasing the width
of f (x) results in a decrease in the width of f̃ (k). In other words, when the
function is wide, its Fourier transform is narrow. In the limit of infinite width
(a constant function), we get infinite sharpness (the delta function). The last
two statements are very general. In fact, it can be shown that �x�k &ge; 1
for any function f (x). When both sides of this inequality are multiplied
by the (reduced) Planck constant � &equiv; h/(2π), the result is the celebrated
Heisenberg uncertainty relation:3 Heisenberg uncertainty
</p>
<p>relation
�x�p &ge; �,
</p>
<p>where p = �k is the momentum of the particle.
</p>
<p>3In the context of the uncertainty relation, the width of the function&mdash;the so-called wave
packet&mdash;measures the uncertainty in the position x of a quantum mechanical particle.
Similarly, the width of the Fourier transform measures the uncertainty in k, which is
related to momentum p via p = �k.</p>
<p/>
</div>
<div class="page"><p/>
<p>280 9 Fourier Analysis
</p>
<p>Having obtained the transform of f (x), we can write
</p>
<p>f (x)= 1&radic;
2π
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>2b&radic;
2π
</p>
<p>sin ka
</p>
<p>k
eikxdk = b
</p>
<p>π
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>sin ka
</p>
<p>k
eikxdk.
</p>
<p>Example 9.2.2 Let us evaluate the Fourier transform of a Gaussian g(x)=
ae&minus;bx
</p>
<p>2
with a, b &gt; 0:
</p>
<p>g̃(k)= a&radic;
2π
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
e&minus;b(x
</p>
<p>2+ikx/b)dx = ae
&minus;k2/4b
&radic;
</p>
<p>2π
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
e&minus;b(x+ik/2b)
</p>
<p>2
dx.
</p>
<p>To evaluate this integral rigorously, we would have to use techniques de-
veloped in complex analysis, which are not introduced until Chap. 11 (see
Example 11.3.8). However, we can ignore the fact that the exponent is com-
plex, substitute y = x + ik/(2b), and write
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
e&minus;b[x+ik/(2b)]
</p>
<p>2
dx =
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
e&minus;by
</p>
<p>2
dy =
</p>
<p>&radic;
π
</p>
<p>b
.
</p>
<p>Thus, we have g̃(k)= a&radic;
2b
e&minus;k
</p>
<p>2/(4b), which is also a Gaussian.
</p>
<p>We note again that the width of g(x), which is proportional to 1/
&radic;
b, is
</p>
<p>in inverse relation to the width of g̃(k), which is proportional to
&radic;
b. We
</p>
<p>thus have �x�k &sim; 1.
</p>
<p>Equations (9.20) and (9.21) are reciprocals of one another. However, it is
not obvious that they are consistent. In other words, if we substitute (9.20)
in the RHS of (9.21), do we get an identity? Let&rsquo;s try this:
</p>
<p>f̃ (k)= 1&radic;
2π
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
dx e&minus;ikx
</p>
<p>[
1&radic;
2π
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
f̃
(
k&prime;
)
eik
</p>
<p>&prime;xdk&prime;
]
</p>
<p>= 1
2π
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
dx
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
f̃
(
k&prime;
)
ei(k
</p>
<p>&prime;&minus;k)xdk&prime;.
</p>
<p>We now change the order of the two integrations:
</p>
<p>f̃ (k)=
ˆ &infin;
</p>
<p>&minus;&infin;
dk&prime;f̃
</p>
<p>(
k&prime;
)[ 1
</p>
<p>2π
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
dx ei(k
</p>
<p>&prime;&minus;k)x
]
.
</p>
<p>But the expression in the square brackets is the delta function (see Exam-
ple 7.3.2). Thus, we have f̃ (k)=
</p>
<p>&acute;&infin;
&minus;&infin; dk
</p>
<p>&prime;f̃ (k&prime;)δ(k&prime; &minus; k), which is an iden-
tity.
</p>
<p>As in the case of Fourier series, Eqs. (9.20) and (9.21) are valid even if
f and f̃ are piecewise continuous. In that case the Fourier transforms are
written as
</p>
<p>1
</p>
<p>2
</p>
<p>[
f (x + 0)+ f (x &minus; 0)
</p>
<p>]
= 1&radic;
</p>
<p>2π
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
f̃ (k)eikxdk,
</p>
<p>1
</p>
<p>2
</p>
<p>[
f̃ (k + 0)+ f̃ (k &minus; 0)
</p>
<p>]
= 1&radic;
</p>
<p>2π
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
f (x)e&minus;ikxdx,
</p>
<p>(9.22)</p>
<p/>
</div>
<div class="page"><p/>
<p>9.2 Fourier Transform 281
</p>
<p>where each zero on the LHS is an ǫ that has gone to its limit.
It is useful to generalize Fourier transform equations to more than one
</p>
<p>dimension. The generalization is straightforward:
</p>
<p>f (r)= 1
(2π)n/2
</p>
<p>ˆ
</p>
<p>dnkeik&middot;rf̃ (k),
</p>
<p>f̃ (k)= 1
(2π)n/2
</p>
<p>ˆ
</p>
<p>dnxf (r)e&minus;ik&middot;r.
(9.23)
</p>
<p>Let us now use the abstract notation of Sect. 7.3 to get more insight into
the preceding results. In the language of Sect. 7.3, Eq. (9.20) can be written
as
</p>
<p>〈x|f 〉 =
ˆ &infin;
</p>
<p>&minus;&infin;
〈k|f̃ 〉〈x|k〉dk = 〈x|
</p>
<p>(
ˆ &infin;
</p>
<p>&minus;&infin;
|k〉〈k|dk
</p>
<p>)
|f̃ 〉, (9.24)
</p>
<p>where we have defined
</p>
<p>〈x|k〉 = 1&radic;
2π
</p>
<p>eikx . (9.25)
</p>
<p>Equation (9.24) suggests the identification |f̃ 〉 &equiv; |f 〉 as well as the identity
</p>
<p>1=
ˆ &infin;
</p>
<p>&minus;&infin;
|k〉〈k|dk, (9.26)
</p>
<p>which is the same as (7.17). Equation (7.19) yields
</p>
<p>〈k|k&prime;〉 = δ
(
k &minus; k&prime;
</p>
<p>)
, (9.27)
</p>
<p>which upon the insertion of a unit operator gives an integral representation
of the delta function:
</p>
<p>δ
(
k &minus; k&prime;
</p>
<p>)
= 〈k|1|k&prime;〉 = 〈k|
</p>
<p>(
ˆ &infin;
</p>
<p>&minus;&infin;
|x〉〈x|dx
</p>
<p>)
|k&prime;〉
</p>
<p>=
ˆ &infin;
</p>
<p>&minus;&infin;
〈k|x〉〈x|k&prime;〉dx = 1
</p>
<p>2π
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
dxei(k
</p>
<p>&prime;&minus;k)x .
</p>
<p>Obviously, we can also write
</p>
<p>δ
(
x &minus; x&prime;
</p>
<p>)
= 1
</p>
<p>2π
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
dkei(x&minus;x
</p>
<p>&prime;)k.
</p>
<p>If more than one dimension is involved, we use
</p>
<p>δ
(
k &minus; k&prime;
</p>
<p>)
= 1
</p>
<p>(2π)n
</p>
<p>ˆ
</p>
<p>dnxei(k&minus;k
&prime;)&middot;r,
</p>
<p>δ
(
r &minus; r&prime;
</p>
<p>)
= 1
</p>
<p>(2π)n
</p>
<p>ˆ
</p>
<p>dnkei(r&minus;r
&prime;)&middot;k,
</p>
<p>(9.28)
</p>
<p>with the inner product relations
</p>
<p>〈r|k〉 = 1
(2π)n/2
</p>
<p>eik&middot;r, 〈k|r〉 = 1
(2π)n/2
</p>
<p>e&minus;ik&middot;r. (9.29)</p>
<p/>
</div>
<div class="page"><p/>
<p>282 9 Fourier Analysis
</p>
<p>Equations (9.28) and (9.29) and the identification |f̃ 〉 &equiv; |f 〉 exhibit a
striking resemblance between |r〉 and |k〉. In fact, any given abstract vector
|f 〉 can be expressed either in terms of its r representation, 〈r|f 〉 = f (r), or
in terms of its k representation, 〈k|f 〉 &equiv; f̃ (k). These two representations are
completely equivalent, and there is a one-to-one correspondence between
the two, given by Eq. (9.23). The representation that is used in practice is
dictated by the physical application. In quantum mechanics, for instance,
most of the time the r representation, corresponding to the position, is used,
because then the operator equations turn into differential equations that are
(in many cases) linear and easier to solve than the corresponding equations
in the k representation, which is related to the momentum.
</p>
<p>Example 9.2.3 In this example we evaluate the Fourier transform of theFourier transform of the
Coulomb potential Coulomb potential V (r) of a point charge q: V (r) = q/r . The Fourier
</p>
<p>transform is important in scattering experiments with atoms, molecules, and
solids. As we shall see in the following, the Fourier transform of V (r) is not
defined. However, if we work with the Yukawa potential,Yukawa potential
</p>
<p>Vα(r)=
qe&minus;αr
</p>
<p>r
, α &gt; 0,
</p>
<p>the Fourier transform will be well-defined, and we can take the limit α&rarr; 0
to recover the Coulomb potential. Thus, we seek the Fourier transform of
Vα(r).
</p>
<p>We are working in three dimensions and therefore may write
</p>
<p>Ṽα(k)=
1
</p>
<p>(2π)3/2
</p>
<p>˚
</p>
<p>d3xe&minus;ik&middot;r
qe&minus;αr
</p>
<p>r
.
</p>
<p>It is clear from the presence of r that spherical coordinates are appropriate.
We are free to pick any direction as the z-axis. A simplifying choice in this
case is the direction of k. So, we let k = |k|êz = kêz, or k &middot; r = kr cos θ ,
where θ is the polar angle in spherical coordinates. Now we have
</p>
<p>Ṽα(k)=
q
</p>
<p>(2π)3/2
</p>
<p>ˆ &infin;
</p>
<p>0
r2 dr
</p>
<p>ˆ π
</p>
<p>0
sin θ dθ
</p>
<p>ˆ 2π
</p>
<p>0
dϕe&minus;ikr cos θ
</p>
<p>e&minus;αr
</p>
<p>r
.
</p>
<p>The ϕ integration is trivial and gives 2π . The θ integration is done next:
</p>
<p>ˆ π
</p>
<p>0
sin θe&minus;ikr cos θ dθ =
</p>
<p>ˆ 1
</p>
<p>&minus;1
e&minus;ikrudu= 1
</p>
<p>ikr
</p>
<p>(
eikr &minus; e&minus;ikr
</p>
<p>)
.
</p>
<p>We thus have
</p>
<p>Ṽα(k)=
q(2π)
</p>
<p>(2π)3/2
</p>
<p>ˆ &infin;
</p>
<p>0
dr r2
</p>
<p>e&minus;αr
</p>
<p>r
</p>
<p>1
</p>
<p>ikr
</p>
<p>(
eikr &minus; e&minus;ikr
</p>
<p>)
</p>
<p>= q
(2π)1/2
</p>
<p>1
</p>
<p>ik
</p>
<p>ˆ &infin;
</p>
<p>0
dr
</p>
<p>[
e(&minus;α+ik)r &minus; e&minus;(α+ik)r
</p>
<p>]
</p>
<p>= q
(2π)1/2
</p>
<p>1
</p>
<p>ik
</p>
<p>(
e(&minus;α+ik)r
</p>
<p>&minus;α + ik
</p>
<p>∣∣∣∣
&infin;
</p>
<p>0
+ e
</p>
<p>&minus;(α+ik)r
</p>
<p>α + ik
</p>
<p>∣∣∣∣
&infin;
</p>
<p>0
</p>
<p>)
.</p>
<p/>
</div>
<div class="page"><p/>
<p>9.2 Fourier Transform 283
</p>
<p>Fig. 9.6 The Fourier transform of the potential of a continuous charge distribution at P
is calculated using this geometry
</p>
<p>Note how the factor e&minus;αr has tamed the divergent behavior of the expo-
nential at r &rarr;&infin;. This was the reason for introducing it in the first place.
Simplifying the last expression yields
</p>
<p>Ṽα(k)=
2q&radic;
2π
</p>
<p>1
</p>
<p>k2 + α2 .
</p>
<p>The parameter α is a measure of the range of the potential. It is clear that the
larger α is, the smaller the range. In fact, it was in response to the short range
of nuclear forces that Yukawa introduced α. For electromagnetism, where
the range is infinite, α becomes zero and Vα(r) reduces to V (r). Thus, the
Fourier transform of the Coulomb potential is
</p>
<p>ṼCoul(k)=
2q&radic;
2π
</p>
<p>1
</p>
<p>k2
.
</p>
<p>If a charge distribution is involved, the Fourier transform will be different.
</p>
<p>Example 9.2.4 The example above deals with the electrostatic potential of
a point charge. Let us now consider the case where the charge is distributed
over a finite volume. Then the potential is
</p>
<p>V (r)=
˚
</p>
<p>qρ(r&prime;)
|r&prime; &minus; r|d
</p>
<p>3x&prime; &equiv; q
ˆ
</p>
<p>ρ(r&prime;)
|r&prime; &minus; r|d
</p>
<p>3x&prime;,
</p>
<p>where qρ(r&prime;) is the charge density at r&prime;, and we have used a single integral
because d3x&prime; already indicates the number of integrations to be performed.
Note that we have normalized ρ(r&prime;) so that its integral over the volume is 1,
which is equivalent to assuming that the total charge is q . Figure 9.6 shows
the geometry of the situation.
</p>
<p>Making a change of variables, R &equiv; r&prime;&minus;r, or r&prime; = R+r, and d3x&prime; = d3X,
with R &equiv; (X,Y,Z), we get
</p>
<p>Ṽ (k)= 1
(2π)3/2
</p>
<p>ˆ
</p>
<p>d3xe&minus;ik&middot;rq
ˆ
</p>
<p>ρ(R + r)
R
</p>
<p>d3X. (9.30)</p>
<p/>
</div>
<div class="page"><p/>
<p>284 9 Fourier Analysis
</p>
<p>To evaluate Eq. (9.30), we substitute for ρ(R + r) in terms of its Fourier
transform,
</p>
<p>ρ(R + r)= 1
(2π)3/2
</p>
<p>ˆ
</p>
<p>d3k&prime;ρ̃
(
k&prime;
)
eik
</p>
<p>&prime;&middot;(R+r). (9.31)
</p>
<p>Combining (9.30) and (9.31), we obtain
</p>
<p>Ṽ (k)= q
(2π)3
</p>
<p>ˆ
</p>
<p>d3x d3Xd3k&prime;
eik
</p>
<p>&prime;&middot;R
</p>
<p>R
ρ̃
(
k&prime;
)
eir&middot;(k
</p>
<p>&prime;&minus;k)
</p>
<p>= q
ˆ
</p>
<p>d3Xd3k&prime;
eik
</p>
<p>&prime;&middot;R
</p>
<p>R
ρ̃
(
k&prime;
)( 1
</p>
<p>(2π)3
</p>
<p>ˆ
</p>
<p>d3x eir&middot;(k
&prime;&minus;k)
</p>
<p>)
</p>
<p>︸ ︷︷ ︸
δ(k&prime;&minus;k)
</p>
<p>= qρ̃(k)
ˆ
</p>
<p>d3X
eik&middot;R
</p>
<p>R
. (9.32)
</p>
<p>What is nice about this result is that the contribution of the charge dis-
tribution, ρ̃(k), has been completely factored out. The integral, aside from
a constant and a change in the sign of k, is simply the Fourier transform of
the Coulomb potential of a point charge obtained in the previous example.
We can therefore write Eq. (9.32) as
</p>
<p>Ṽ (k)= (2π)3/2ρ̃(k)ṼCoul(&minus;k)=
4πqρ̃(k)
</p>
<p>|k|2 .
</p>
<p>This equation is important in analyzing the structure of atomic parti-
cles. The Fourier transform Ṽ (k) is directly measurable in scattering exper-
iments. In a typical experiment a (charged) target is probed with a charged
point particle (electron). If the analysis of the scattering data shows a devi-
ation from 1/k2 in the behavior of Ṽ (k), then it can be concluded that the
target particle has a charge distribution. More specifically, a plot of k2Ṽ (k)
versus k gives the variation of ρ̃(k), the form factor, with k. If the resultingform factor
graph is a constant, then ρ̃(k) is a constant, and the target is a point particle
[ρ̃(k) is a constant for point particles, where ρ̃(r&prime;) &prop; δ(r &minus; r&prime;)]. If there is
any deviation from a constant function, ρ̃(k) must have a dependence on k,
and correspondingly, the target particle must have a charge distribution.
</p>
<p>The above discussion, when generalized to four-dimensional relativisticFourier transform and
the discovery of quarks space-time, was the basis for a strong argument in favor of the existence
</p>
<p>of point-like particles&mdash;quarks&mdash;inside a proton in 1968, when the results
of the scattering of high-energy electrons off protons at the Stanford Linear
Accelerator Center revealed deviation from a constant for the proton form
factor.
</p>
<p>9.2.1 Fourier Transforms and Derivatives
</p>
<p>The Fourier transform is very useful for solving differential equations. This
is because the derivative operator in r space turns into ordinary multipli-
cation in k space. For example, if we differentiate f (r) in Eq. (9.23) with</p>
<p/>
</div>
<div class="page"><p/>
<p>9.2 Fourier Transform 285
</p>
<p>respect to xj , we obtain
</p>
<p>&part;
</p>
<p>&part;xj
f (r)= 1
</p>
<p>(2π)n/2
</p>
<p>ˆ
</p>
<p>dnk
&part;
</p>
<p>&part;xj
ei(k1x1+&middot;&middot;&middot;+kj xj+&middot;&middot;&middot;+knxn)f̃ (k)
</p>
<p>= 1
(2π)n/2
</p>
<p>ˆ
</p>
<p>dnk(ikj )e
ik&middot;rf̃ (k).
</p>
<p>That is, every time we differentiate with respect to any component of r,
the corresponding component of k &ldquo;comes down&rdquo;. Thus, the n-dimensional
gradient is
</p>
<p>&nabla;&nabla;&nabla;f (r)= 1
(2π)n/2
</p>
<p>ˆ
</p>
<p>dnk(ik)eik&middot;rf̃ (k),
</p>
<p>and the n-dimensional Laplacian is
</p>
<p>&nabla;2f (r)= 1
(2π)n/2
</p>
<p>ˆ
</p>
<p>dnk
(
&minus;k2
</p>
<p>)
eik&middot;rf̃ (k).
</p>
<p>We shall use Fourier transforms extensively in solving differential equa-
tions later in the book. Here, we can illustrate the above points with a simple
example. Consider the ordinary second-order differential equation
</p>
<p>C2
d2y
</p>
<p>dx2
+C1
</p>
<p>dy
</p>
<p>dx
+C0y = f (x),
</p>
<p>where C0,C1, and C2 are constants. We can &ldquo;solve&rdquo; this equation by simply
substituting the following in it:
</p>
<p>y(x)= 1&radic;
2π
</p>
<p>ˆ
</p>
<p>dkỹ(k)eikx,
dy
</p>
<p>dx
= 1&radic;
</p>
<p>2π
</p>
<p>ˆ
</p>
<p>dkỹ(k)(ik)eikx ,
</p>
<p>d2y
</p>
<p>dx2
=&minus; 1&radic;
</p>
<p>2π
</p>
<p>ˆ
</p>
<p>dkỹ(k)k2eikx, f (x)= 1&radic;
2π
</p>
<p>ˆ
</p>
<p>dkf̃ (k)eikx .
</p>
<p>This gives
</p>
<p>1&radic;
2π
</p>
<p>ˆ
</p>
<p>dkỹ(k)
(
&minus;C2k2 + iC1k+C0
</p>
<p>)
eikx = 1&radic;
</p>
<p>2π
</p>
<p>ˆ
</p>
<p>dkf̃ (k)eikx .
</p>
<p>Equating the coefficients of eikx on both sides, we obtain4
</p>
<p>ỹ(k)= f̃ (k)&minus;C2k2 + iC1k +C0
.
</p>
<p>If we know f̃ (k) [which can be obtained from f (x)], we can calculate
y(x) by Fourier-transforming ỹ(k). The resulting integrals are not generally
easy to evaluate. In some cases the methods of complex analysis may be
helpful; in others numerical integration may be the last resort. However, the
real power of the Fourier transform lies in the formal analysis of differential
equations.
</p>
<p>4Alternatively, we can multiply both sides by e&minus;ik
&prime;x and integrate over x. The result of
</p>
<p>this integration yields δ(k&minus;k&prime;), which collapses the k-integrations and yields the equality
of the integrands.</p>
<p/>
</div>
<div class="page"><p/>
<p>286 9 Fourier Analysis
</p>
<p>9.2.2 The Discrete Fourier Transform
</p>
<p>The preceding remarks alluded to the power of the Fourier transform in
solving certain differential equations. If such a solution is combined with
numerical techniques, the integrals must be replaced by sums. This is par-
ticularly true if our function is given by a table rather than a mathemati-
cal relation, a common feature of numerical analysis. So suppose that we
are given a set of measurements performed in equal time intervals of �t .
Suppose that the overall period in which these measurements are done
is T . We are seeking a Fourier transform of this finite set of data. First we
write
</p>
<p>f̃ (ω)= 1&radic;
2π
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
f (t)e&minus;iωtdt &asymp; 1&radic;
</p>
<p>2π
</p>
<p>N&minus;1&sum;
</p>
<p>n=0
f (tn)e
</p>
<p>&minus;iωtn�t,
</p>
<p>or, discretizing the frequency as well and writing ωm =m�ω, with �ω to
be determined later, we have
</p>
<p>f̃ (m�ω)= 1&radic;
2π
</p>
<p>N&minus;1&sum;
</p>
<p>n=0
f (n�t)e&minus;i(m�ω)n�t
</p>
<p>(
T
</p>
<p>N
</p>
<p>)
. (9.33)
</p>
<p>Since the Fourier transform is given in terms of a finite sum, let us explore
the idea of writing the inverse transform also as a sum. So, multiply both
sides of the above equation by [ei(m�ω)k�t/(
</p>
<p>&radic;
2π)]�ω and sum over m:
</p>
<p>1&radic;
2π
</p>
<p>N&minus;1&sum;
</p>
<p>m=0
f̃ (m�ω)ei(m�ω)k�t�ω
</p>
<p>= T�ω
2πN
</p>
<p>N&minus;1&sum;
</p>
<p>n=0
</p>
<p>N&minus;1&sum;
</p>
<p>m=0
f (n�t)eim�ω�t(k&minus;n)
</p>
<p>= T�ω
2πN
</p>
<p>N&minus;1&sum;
</p>
<p>n=0
f (n�t)
</p>
<p>N&minus;1&sum;
</p>
<p>m=0
eim�ω�t(k&minus;n).
</p>
<p>Problem 9.2 shows that
</p>
<p>N&minus;1&sum;
</p>
<p>m=0
eim�ω�t(k&minus;n) =
</p>
<p>{
N if k = n,
eiN�ω�t(k&minus;n)&minus;1
ei�ω�t(k&minus;n)&minus;1 if k �= n.
</p>
<p>We want the sum to vanish when k �= n. This suggests demanding that
N�ω�t(k &minus; n) be an integer multiple of 2π . Since �ω and �t are to
be independent of this (arbitrary) integer (as well as k and n), we must write
</p>
<p>N�ω�t(k &minus; n)= 2π(k &minus; n) &rArr; N�ω T
N
</p>
<p>= 2π &rArr; �ω= 2π
T
</p>
<p>.
</p>
<p>With this choice, we have the following discrete Fourier transforms:
discrete Fourier
</p>
<p>transforms</p>
<p/>
</div>
<div class="page"><p/>
<p>9.2 Fourier Transform 287
</p>
<p>f̃ (ωj )=
1&radic;
N
</p>
<p>N&minus;1&sum;
</p>
<p>n=0
f (tn)e
</p>
<p>&minus;iωj tn , ωj =
2πj
</p>
<p>T
,
</p>
<p>f (tn)=
1&radic;
N
</p>
<p>N&minus;1&sum;
</p>
<p>j=0
f̃ (ωj )e
</p>
<p>iωj tn , tn = n�t,
(9.34)
</p>
<p>where we have redefined the new f̃ to be
&radic;
</p>
<p>2πN/T times the old f̃ .
Discrete Fourier transforms are used extensively in numerical calculation
</p>
<p>of problems in which ordinary Fourier transforms are used. For instance, if
a differential equation lends itself to a solution via the Fourier transform as
discussed before, then discrete Fourier transforms will give a procedure for
finding the solution numerically. Similarly, the frequency analysis of signals
is nicely handled by discrete Fourier transforms.
</p>
<p>It turns out that discrete Fourier analysis is very intensive computation-
ally. Its status as a popular tool in computational physics is due primarily to
a very efficient method of calculation known as the fast Fourier transform. fast Fourier transform
In a typical Fourier transform, one has to perform a sum of N terms for ev-
ery point. Since there are N points to transform, the total computational time
will be of order N2. In the fast Fourier transform, one takes N to be even
and divides the sum into two other sums, one over the even terms and one
over the odd terms. Then the computation time will be of order 2&times; (N/2)2,
or half the original calculation. Similarly, if N/2 is even, one can further
divide the odd and even sums by two and obtain a computation time of
4 &times; (N/4)2, or a quarter of the original calculation. In general, if N = 2k ,
then by dividing the sums consecutively, we end up with N transforms to be
performed after k steps. So, the computation time will be kN = N log2 N .
For N = 128, the computation time will be 100 log2 128 = 700 as opposed
to 1282 &asymp; 16,400, a reduction by a factor of over 20. The fast Fourier trans-
form is indeed fast!
</p>
<p>9.2.3 Fourier Transform of a Distribution
</p>
<p>Although one can define the Fourier transform of a distribution in exact
analogy to an ordinary function, sometimes it is convenient to define the
Fourier transform of the distribution as a linear functional.
</p>
<p>Let us ignore the distinction between the two variables x and k, and sim-
ply define the Fourier transform of a function f :R&rarr;R as
</p>
<p>f̃ (u)= 1&radic;
2π
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
f (t)eiutdt.
</p>
<p>Now we consider two functions, f and g, and note that
</p>
<p>〈f, g̃〉 &equiv;
ˆ &infin;
</p>
<p>&minus;&infin;
f (u)g̃(u) du=
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
f (u)
</p>
<p>[
1&radic;
2π
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
g(t)e&minus;iutdt
</p>
<p>]
du</p>
<p/>
</div>
<div class="page"><p/>
<p>288 9 Fourier Analysis
</p>
<p>=
ˆ &infin;
</p>
<p>&minus;&infin;
g(t)
</p>
<p>[
1&radic;
2π
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
f (u)e&minus;iutdu
</p>
<p>]
dt
</p>
<p>=
ˆ &infin;
</p>
<p>&minus;&infin;
g(t)f̃ (t) dt = 〈f̃ , g〉.
</p>
<p>The following definition is motivated by the last equation.
</p>
<p>Definition 9.2.5 Let ϕ be a distribution and let f be a C&infin;F function whose
Fourier transform f̃ exists and is also a C&infin;F function. Then we define the
Fourier transform ϕ̃ of ϕ to be the distribution given by
</p>
<p>〈ϕ̃, f 〉 = 〈ϕ, f̃ 〉.
</p>
<p>Example 9.2.6 The Fourier transform of δ(x) is given by
</p>
<p>〈δ̃, f 〉 = 〈δ, f̃ 〉 = f̃ (0)= 1&radic;
2π
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
f (t) dt
</p>
<p>=
ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>(
1&radic;
2π
</p>
<p>)
f (t) dt =
</p>
<p>&lang;
1&radic;
2π
</p>
<p>,f
</p>
<p>&rang;
.
</p>
<p>Thus, δ̃ = 1/
&radic;
</p>
<p>2π , as expected.
The Fourier transform of δ(x &minus; x&prime;)&equiv; δx&prime;(x) is given by
</p>
<p>〈δ̃x&prime; , f 〉 = 〈δx&prime; , f̃ 〉 = f̃
(
x&prime;
)
= 1&radic;
</p>
<p>2π
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
f (t)e&minus;ix
</p>
<p>&prime;t dt
</p>
<p>=
ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>(
1&radic;
2π
</p>
<p>e&minus;ix
&prime;t
)
f (t) dt.
</p>
<p>Thus, if ϕ(x)= δ(x &minus; x&prime;), then ϕ̃(t)= (1/
&radic;
</p>
<p>2π)e&minus;ix
&prime;t .
</p>
<p>9.3 Problems
</p>
<p>9.1 Consider the function f (θ)=&sum;&infin;m=&minus;&infin; δ(θ &minus; 2mπ).
(a) Show that f is periodic of period 2π .
(b) What is the Fourier series expansion for f (θ).
</p>
<p>9.2 Break the sum
&sum;N
</p>
<p>n=&minus;N e
in(θ&minus;θ &prime;) into
</p>
<p>&sum;&minus;1
n=&minus;N +1+
</p>
<p>&sum;N
n=1. Use the ge-
</p>
<p>ometric sum formula
N&sum;
</p>
<p>n=0
arn = a r
</p>
<p>N+1 &minus; 1
r &minus; 1
</p>
<p>to obtain
</p>
<p>N&sum;
</p>
<p>n=1
ein(θ&minus;θ
</p>
<p>&prime;) = ei(θ&minus;θ &prime;) e
iN(θ&minus;θ &prime;) &minus; 1
ei(θ&minus;θ &prime;) &minus; 1 = e
</p>
<p>i 12 (N+1)(θ&minus;θ &prime;)
sin[ 12N(θ &minus; θ &prime;)]
sin[ 12 (θ &minus; θ &prime;)]
</p>
<p>.</p>
<p/>
</div>
<div class="page"><p/>
<p>9.3 Problems 289
</p>
<p>By changing n to &minus;n or equivalently, (θ &minus; θ &prime;) to &minus;(θ &minus; θ &prime;) find a similar
sum from &minus;N to &minus;1. Now put everything together and use the trigonometric
identity
</p>
<p>2 cosα sinβ = sin(α + β)&minus; sin(α &minus; β)
to show that
</p>
<p>N&sum;
</p>
<p>n=&minus;N
ein(θ&minus;θ
</p>
<p>&prime;) = sin[(N +
1
2 )(θ &minus; θ &prime;)]
</p>
<p>sin[ 12 (θ &minus; θ &prime;)]
.
</p>
<p>9.3 Find the Fourier series expansion of the periodic function defined on its
fundamental cell as
</p>
<p>f (θ)=
{
&minus; 12 (π + θ) if &minus;π &le; θ &lt; 0,
1
2 (π &minus; θ) if 0 &lt; θ &le; π.
</p>
<p>9.4 Show that An and Bn in Eq. (9.2) are real when f (θ) is real.
</p>
<p>9.5 Find the Fourier series expansion of the periodic function f (θ) defined
as f (θ)= cosαθ on its fundamental cell, (&minus;π,π)
(a) when α is an integer;
(b) when α is not an integer.
</p>
<p>9.6 Find the Fourier series expansion of the periodic function defined on its
fundamental cell, (&minus;π,π), as f (θ)= θ .
</p>
<p>9.7 Consider the periodic function that is defined on its fundamental cell,
(&minus;a, a), as f (x)= |x|.
(a) Find its Fourier series expansion.
(b) Show that the infinite series gives the same result as the function when
</p>
<p>both are evaluated at x = a.
(c) Evaluate both sides of the expansion at x = 0, and show that
</p>
<p>π2 = 8
&infin;&sum;
</p>
<p>k=0
</p>
<p>1
</p>
<p>(2k + 1)2 .
</p>
<p>9.8 Let f (x) = x be a periodic function defined over the interval (0,2a).
Find the Fourier series expansion of f .
</p>
<p>9.9 Show that the piecewise parabolic &ldquo;approximation&rdquo; to a2 sin(πx/a) in
the interval (&minus;a, a) given by the function
</p>
<p>f (x)=
{
</p>
<p>4x(a + x) if &minus;a &le; x &le; 0
4x(a &minus; x) if 0 &le; x &le; a
</p>
<p>has the Fourier series expansion
</p>
<p>f (x)= 32a
2
</p>
<p>π3
</p>
<p>&infin;&sum;
</p>
<p>n=0
</p>
<p>1
</p>
<p>(2n+ 1)3 sin
(2n+ 1)πx
</p>
<p>a
.</p>
<p/>
</div>
<div class="page"><p/>
<p>290 9 Fourier Analysis
</p>
<p>Plot f (x), a2 sin(πx/a), and the series expansion (up to 20 terms) for a = 1
between &minus;1 and +1 on the same graph.
</p>
<p>9.10 Find the Fourier series expansion of f (θ)= θ2 for |θ |&lt; π . Then show
that
</p>
<p>π2
</p>
<p>6
=
</p>
<p>&infin;&sum;
</p>
<p>n=1
</p>
<p>1
</p>
<p>n2
and
</p>
<p>π2
</p>
<p>12
=&minus;
</p>
<p>&infin;&sum;
</p>
<p>n=1
</p>
<p>(&minus;1)n
n2
</p>
<p>.
</p>
<p>9.11 Find the Fourier series expansion of
</p>
<p>f (t)=
{
</p>
<p>sinωt if 0 &le; t &le; π/ω,
0 if &minus;π/ω &le; t &le; 0.
</p>
<p>9.12 What is the Fourier transform of
</p>
<p>(a) the constant function f (x)= C, and
(b) the Dirac delta function δ(x)?
</p>
<p>9.13 Show that
</p>
<p>(a) if g(x) is real, then g̃&lowast;(k)= g̃(&minus;k), and
(b) if g(x) is even (odd), then g̃(k) is also even (odd).
</p>
<p>9.14 Let gc(x) stand for the single function that is nonzero only on a subin-
terval of the fundamental cell (a, a +L). Define the function g(x) as
</p>
<p>g(x)=
&infin;&sum;
</p>
<p>j=&minus;&infin;
gc(x &minus; jL).
</p>
<p>(a) Show that g(x) is periodic with period L.
(b) Find its Fourier transform g̃(k), and verify that
</p>
<p>g̃(k)= Lg̃c(k)
&infin;&sum;
</p>
<p>m=&minus;&infin;
δ(kL&minus; 2mπ).
</p>
<p>(c) Find the (inverse) transform of g̃(k), and show that it is the Fourier
series of gc(x).
</p>
<p>9.15 Evaluate the Fourier transform of
</p>
<p>g(x)=
{
b&minus; b|x|/a if |x|&lt; a,
0 if |x|&gt; a.
</p>
<p>9.16 Let f (θ) be a periodic function given by f (θ)=&sum;&infin;n=&minus;&infin; aneinθ . Find
its Fourier transform f̃ (t).</p>
<p/>
</div>
<div class="page"><p/>
<p>9.3 Problems 291
</p>
<p>9.17 Let
</p>
<p>f (t)=
{
</p>
<p>sinω0t if |t |&lt; T,
0 if |t |&gt; T.
</p>
<p>Show that
</p>
<p>f̃ (ω)= 1&radic;
2π
</p>
<p>{
sin[(ω&minus;ω0)T ]
</p>
<p>ω&minus;ω0
&minus; sin[(ω+ω0)T ]
</p>
<p>ω+ω0
</p>
<p>}
.
</p>
<p>Verify the uncertainty relation �ω�t &asymp; 4π .
</p>
<p>9.18 If f (x)= g(x + a), show that f̃ (k)= e&minus;iak g̃(k).
</p>
<p>9.19 For a &gt; 0 find the Fourier transform of f (x) = e&minus;a|x|. Is f̃ (k) sym-
metric? Is it real? Verify the uncertainty relations.
</p>
<p>9.20 The displacement of a damped harmonic oscillator is given by
</p>
<p>f (t)=
{
Ae&minus;αteiω0t if t &gt; 0,
</p>
<p>0 if t &lt; 0.
</p>
<p>Find f̃ (ω) and show that the frequency distribution |f̃ (ω)|2 is given by
</p>
<p>∣∣f̃ (ω)
∣∣2 = A
</p>
<p>2
</p>
<p>2π
</p>
<p>1
</p>
<p>(ω&minus;ω0)2 + α2
.
</p>
<p>9.21 Prove the convolution theorem: convolution theorem
ˆ &infin;
</p>
<p>&minus;&infin;
f (x)g(y &minus; x)dx =
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
f̃ (k)g̃(k)eiky dk.
</p>
<p>What will this give when y = 0?
</p>
<p>9.22 Prove Parseval&rsquo;s relation for Fourier transforms: Parseval&rsquo;s relation
ˆ &infin;
</p>
<p>&minus;&infin;
f (x)g&lowast;(x) dx =
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
f̃ (k)g̃&lowast;(k) dk.
</p>
<p>In particular, the norm of a function&mdash;with weight function equal to 1&mdash;is
invariant under Fourier transform.
</p>
<p>9.23 Use the completeness relation 1=&sum;n |n〉〈n| and sandwich it between
|x〉 and 〈x&prime;| to find an expression for the Dirac delta function in terms of an
infinite series of orthonormal functions.
</p>
<p>9.24 Use a Fourier transform in three dimensions to find a solution of the
Poisson equation: &nabla;2Φ(r)=&minus;4πρ(r).
</p>
<p>9.25 For ϕ(x)= δ(x &minus; x&prime;), find ϕ̃(y).
</p>
<p>9.26 Show that ˜̃f (t)= f (&minus;t).</p>
<p/>
</div>
<div class="page"><p/>
<p>292 9 Fourier Analysis
</p>
<p>9.27 The Fourier transform of a distribution ϕ is given by
</p>
<p>ϕ̃(t)=
&infin;&sum;
</p>
<p>n=0
</p>
<p>1
</p>
<p>n!δ
&prime;(t &minus; n).
</p>
<p>What is ϕ(x)? Hint: Use ˜̃ϕ(x)= ϕ(&minus;x)
</p>
<p>9.28 For f (x)=&sum;nk=0 akxk , show that
</p>
<p>f̃ (u)=
&radic;
</p>
<p>2π
n&sum;
</p>
<p>k=0
ikakδ
</p>
<p>(k)(u), where δ(k)(u)&equiv; d
k
</p>
<p>duk
δ(u).</p>
<p/>
</div>
<div class="page"><p/>
<p>Part III
</p>
<p>Complex Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>10Complex Calculus
</p>
<p>Complex analysis, just like real analysis, deals with questions of continuity,
convergence of series, differentiation, integration, and so forth. The reader
is assumed to have been exposed to the algebra of complex numbers.
</p>
<p>10.1 Complex Functions
</p>
<p>A complex function is a map f : C &rarr; C, and we write f (z) = w, where
both z and w are complex numbers.1 The map f can be geometrically
thought of as a correspondence between two complex planes, the z-plane
and the w-plane. The w-plane has a real axis and an imaginary axis, which
we can call u and v, respectively. Both u and v are real functions of the
coordinates of z, i.e., x and y. Therefore, we may write
</p>
<p>f (z)= u(x, y)+ iv(x, y). (10.1)
</p>
<p>This equation gives a unique point (u, v) in the w-plane for each point
(x, y) in the z-plane (see Fig. 10.1). Under f , regions of the z-plane are
mapped onto regions of the w-plane. For instance, a curve in the z-plane
may be mapped into a curve in the w-plane. The following example illus-
trates this point.
</p>
<p>Example 10.1.1 Let us investigate the behavior of a couple of elementary
complex functions. In particular, we shall look at the way a line y =mx in
the z-plane is mapped into curves in the w-plane.
</p>
<p>(a) For w = f (z)= z2, we have
</p>
<p>w = (x + iy)2 = x2 &minus; y2 + 2ixy,
</p>
<p>1Strictly speaking, we should write f : S &rarr;C where S is a subset of the complex plane.
The reason is that most functions are not defined for the entire set of complex numbers,
so that the domain of such functions is not necessarily C. We shall specify the domain
only when it is absolutely necessary. Otherwise, we use the generic notation f :C&rarr; C,
even though f is defined only on a subset of C.
</p>
<p>S. Hassani, Mathematical Physics, DOI 10.1007/978-3-319-01195-0_10,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>295</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-01195-0_10">http://dx.doi.org/10.1007/978-3-319-01195-0_10</a></div>
</div>
<div class="page"><p/>
<p>296 10 Complex Calculus
</p>
<p>Fig. 10.1 A map from the z-plane to the w-plane
</p>
<p>Fig. 10.2 (a) The map z2 takes a line with slope angle α and maps it to a line with twice
the angle in the w-plane. (b) The map ez takes the same line and maps it to a spiral in the
w-plane
</p>
<p>with u(x, y) = x2 &minus; y2 and v(x, y) = 2xy. How does the region of
C consisting of all points of a line get mapped into C? For y = mx,
i.e., for a line in the z-plane with slope m, these equations yield u =
(1 &minus;m2)x2 and v = 2mx2. Eliminating x in these equations, we find
v = [2m/(1 &minus;m2)]u. This is a line passing through the origin of the
w-plane [see Fig. 10.2(a)]. Note that the angle the image line makes
with the real axis of the w-plane is twice the angle the original line
makes with the x-axis. (Show this!).
</p>
<p>(b) The function w = f (z) = ez = ex+iy gives u(x, y) = ex cosy and
v(x, y) = ex siny. What is the image of the line y = mx under
this map? Substituting y = mx, we obtain u = ex cosmx and v =
ex sinmx. Unlike part (a), we cannot eliminate x to find v as an ex-
plicit function of u. Nevertheless, the last pair of equations are para-
metric equations of a curve, which we can plot in a uv-plane as shown
in Fig. 10.2(b).
</p>
<p>Limits of complex functions are defined in terms of absolute values.
Thus, limz&rarr;a f (z) = w0 means that given any real number ǫ &gt; 0, we can
find a corresponding real number δ &gt; 0 such that |f (z)&minus;w0|&lt; ǫ whenever
|z &minus; a| &lt; δ. Similarly, we say that a function f is continuous at z = a if
limz&rarr;a f (z)= f (a).</p>
<p/>
</div>
<div class="page"><p/>
<p>10.2 Analytic Functions 297
</p>
<p>10.2 Analytic Functions
</p>
<p>The derivative of a complex function is defined as usual:
</p>
<p>Definition 10.2.1 Let f :C&rarr;C be a complex function. The derivative of
f at z0 is
</p>
<p>df
</p>
<p>dz
</p>
<p>∣∣∣∣
z0
</p>
<p>= lim
�z&rarr;0
</p>
<p>f (z0 +�z)&minus; f (z0)
�z
</p>
<p>,
</p>
<p>provided that the limit exists and is independent of �z.
</p>
<p>In this definition &ldquo;independent of �z&rdquo; means independent of �x and
�y (the components of �z) and, therefore, independent of the direction of
approach to z0. The restrictions of this definition apply to the real case as
well. For instance, the derivative of f (x) = |x| at x = 0 does not exist2
because it approaches +1 from the right and &minus;1 from the left.
</p>
<p>It can easily be shown that all the formal rules of differentiation that apply
to the real case also apply to the complex case. For example, if f and g are
differentiable, then f &plusmn; g, fg, and&mdash;as long as g is not zero at the point of
interest&mdash;f/g are also differentiable, and their derivatives are given by the
usual rules of differentiation.
</p>
<p>Example 10.2.2 Let us examine the derivative of f (z)= x2 + 2iy2 at z=
1 + i:
</p>
<p>Example illustrating path
</p>
<p>dependence of
</p>
<p>derivative
</p>
<p>df
</p>
<p>dz
</p>
<p>∣∣∣∣
z=1+i
</p>
<p>= lim
�z&rarr;0
</p>
<p>f (1 + i +�z)&minus; f (1 + i)
�z
</p>
<p>= lim
�x &rarr; 0
�y &rarr; 0
</p>
<p>(1 +�x)2 + 2i(1 +�y)2 &minus; 1 &minus; 2i
�x + i�y
</p>
<p>= lim
�x &rarr; 0
�y &rarr; 0
</p>
<p>2�x + 4i�y + (�x)2 + 2i(�y)2
�x + i�y .
</p>
<p>Let us approach z= 1+ i along the line y&minus;1 =m(x&minus;1). Then �y =m�x,
and the limit yields
</p>
<p>df
</p>
<p>dz
</p>
<p>∣∣∣∣
z=1+i
</p>
<p>= lim
�x&rarr;0
</p>
<p>2�x + 4im�x + (�x)2 + 2im2(�x)2
�x + im�x =
</p>
<p>2 + 4im
1 + im .
</p>
<p>It follows that we get infinitely many values for the derivative depending on
the value we assign to m, i.e., depending on the direction along which we
approach 1 + i. Thus, the derivative does not exist at z= 1 + i.
</p>
<p>It is clear from the definition that differentiability puts a severe restric-
tion on f (z) because it requires the limit to be the same for all paths go-
ing through z0. Furthermore, differentiability is a local property: To test
</p>
<p>2One can rephrase this and say that the derivative exists, but not in terms of ordinary func-
tions, rather, in terms of generalized functions&mdash;in this case θ(x)&mdash;discussed in Sect. 7.3.</p>
<p/>
</div>
<div class="page"><p/>
<p>298 10 Complex Calculus
</p>
<p>whether or not a function f (z) is differentiable at z0, we move away from
z0 only by a small amount �z and check the existence of the limit in Defi-
nition 10.2.1.
</p>
<p>What are the conditions under which a complex function is differen-
tiable? For f (z)= u(x, y)+ iv(x, y), Definition 10.2.1 yields
</p>
<p>df
</p>
<p>dz
</p>
<p>∣∣∣∣
z0
</p>
<p>= lim
�x &rarr; 0
�y &rarr; 0
</p>
<p>{
u(x0 +�x,y0 +�y)&minus; u(x0, y0)
</p>
<p>�x + i�y
</p>
<p>+ i v(x0 +�x,y0 +�y)&minus; v(x0, y0)
�x + i�y
</p>
<p>}
.
</p>
<p>If this limit is to exist for all paths, it must exist for the two particular paths
on which �y = 0 (parallel to the x-axis) and �x = 0 (parallel to the y-axis).
For the first path we get
</p>
<p>df
</p>
<p>dz
</p>
<p>∣∣∣∣
z0
</p>
<p>= lim
�x&rarr;0
</p>
<p>u(x0 +�x,y0)&minus; u(x0, y0)
�x
</p>
<p>+ i lim
�x&rarr;0
</p>
<p>v(x0 +�x,y0)&minus; v(x0, y0)
�x
</p>
<p>= &part;u
&part;x
</p>
<p>∣∣∣∣
(x0,y0)
</p>
<p>+ i &part;v
&part;x
</p>
<p>∣∣∣∣
(x0,y0)
</p>
<p>.
</p>
<p>For the second path (�x = 0), we obtain
</p>
<p>df
</p>
<p>dz
</p>
<p>∣∣∣∣
z0
</p>
<p>= lim
�y&rarr;0
</p>
<p>u(x0, y0 +�y)&minus; u(x0, y0)
i�y
</p>
<p>+ i lim
�y&rarr;0
</p>
<p>v(x0, y0 +�y)&minus; v(x0, y0)
i�y
</p>
<p>=&minus;i &part;u
&part;y
</p>
<p>∣∣∣∣
(x0,y0)
</p>
<p>+ &part;v
&part;y
</p>
<p>∣∣∣∣
(x0,y0)
</p>
<p>.
</p>
<p>If f is to be differentiable at z0, the derivatives along the two paths must be
equal. Equating the real and imaginary parts of both sides of this equation
and ignoring the subscript z0 (x0, y0, or z0 is arbitrary), we obtain
</p>
<p>&part;u
</p>
<p>&part;x
= &part;v
</p>
<p>&part;y
and
</p>
<p>&part;u
</p>
<p>&part;y
=&minus; &part;v
</p>
<p>&part;x
. (10.2)
</p>
<p>These two conditions, which are necessary for the differentiability of f , are
called the Cauchy-Riemann conditions.Cauchy-Riemann
</p>
<p>conditions An alternative way of writing the Cauchy-Riemann (C-R) conditions is
obtained by making the substitution3 x = 12 (z + z&lowast;) and y = 12i (z &minus; z&lowast;)
in u(x, y) and v(x, y), using the chain rule to write Eq. (10.2) in terms
of z and z&lowast;, substituting the results in &part;f
</p>
<p>&part;z&lowast; = &part;u&part;z&lowast; + i &part;v&part;z&lowast; and showing that
Eq. (10.2) is equivalent to the single equation &part;f/&part;z&lowast; = 0. This equation
says that
</p>
<p>Box 10.2.3 If f is to be differentiable, it must be independent of z&lowast;.
</p>
<p>3We use z&lowast; to indicate the complex conjugate of z. Occasionally we may use z̄.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.2 Analytic Functions 299
</p>
<p>If the derivative of f exists, the arguments leading to Eq. (10.2) imply
that the derivative can be expressed as Expression for the
</p>
<p>derivative of a
</p>
<p>differentiable complex
</p>
<p>function
</p>
<p>df
</p>
<p>dz
= &part;u
</p>
<p>&part;x
+ i &part;v
</p>
<p>&part;x
= &part;v
</p>
<p>&part;y
&minus; i &part;u
</p>
<p>&part;y
. (10.3)
</p>
<p>The C-R conditions assure us that these two equations are equivalent.
The following example illustrates the differentiability of complex func-
</p>
<p>tions.
</p>
<p>Example 10.2.4 Let us determine whether or not the following functions
are differentiable:
</p>
<p>(a) We have already established that f (z)= x2+2iy2 is not differentiable
at z= 1+ i. We can now show that it has no derivative at any point in
the complex plane (except at the origin). This is easily seen by noting
that u = x2 and v = 2y2, and that &part;u/&part;x = 2x �= &part;v/&part;y = 4y, and
the first Cauchy-Riemann condition is not satisfied. The second C-R
condition is satisfied, but that is not enough.
</p>
<p>We can also write f (z) in terms of z and z&lowast;:
</p>
<p>f (z)=
[
</p>
<p>1
</p>
<p>2
</p>
<p>(
z+ z&lowast;
</p>
<p>)]2
+ 2i
</p>
<p>[
1
</p>
<p>2i
</p>
<p>(
z&minus; z&lowast;
</p>
<p>)]2
</p>
<p>= 1
4
(1 &minus; 2i)
</p>
<p>(
z2 + z&lowast;2
</p>
<p>)
+ 1
</p>
<p>2
(1 + 2i)zz&lowast;.
</p>
<p>f (z) has an explicit dependence on z&lowast;. Therefore, it is not differen-
tiable.
</p>
<p>(b) Now consider f (z)= x2 &minus; y2 + 2ixy, for which u= x2 &minus; y2 and v =
2xy. The C-R conditions become &part;u/&part;x = 2x = &part;v/&part;y and &part;u/&part;y =
&minus;2y =&minus;&part;v/&part;x. Thus, f (z) may be differentiable. Recall that the C-R
conditions are only necessary conditions; we have not shown (but we
will, shortly) that they are also sufficient.
</p>
<p>To check the dependence of f on z&lowast;, substitute x = (z+ z&lowast;)/2 and
y = (z&minus; z&lowast;)/(2i) in u and v to show that f (z)= z2, and thus there is
no z&lowast; dependence.
</p>
<p>(c) Let u(x, y)= ex cosy and v(x, y)= ex siny. Then &part;u/&part;x = ex cosy =
&part;v/&part;y and &part;u/&part;y =&minus;ex siny =&minus;&part;v/&part;x, and the C-R conditions are
satisfied. Also,
</p>
<p>f (z)= ex cosy+ iex siny = ex(cosy+ i siny)= exeiy = ex+iy = ez,
</p>
<p>and there is no z&lowast; dependence.
</p>
<p>The requirement of differentiability is very restrictive: The derivative
must exist along infinitely many paths. On the other hand, the C-R con-
ditions seem deceptively mild: They are derived for only two paths. Never-
theless, the two paths are, in fact, true representatives of all paths; that is,
the C-R conditions are not only necessary, but also sufficient:</p>
<p/>
</div>
<div class="page"><p/>
<p>300 10 Complex Calculus
</p>
<p>Theorem 10.2.5 The function f (z)= u(x, y)+ iv(x, y) is differentiable in
a region of the complex plane if and only if the Cauchy-Riemann conditions,
</p>
<p>&part;u
</p>
<p>&part;x
= &part;v
</p>
<p>&part;y
and
</p>
<p>&part;u
</p>
<p>&part;y
=&minus; &part;v
</p>
<p>&part;x
</p>
<p>(or, equivalently, &part;f/&part;z&lowast; = 0), are satisfied and all first partial derivatives
of u and v are continuous in that region. In that case
</p>
<p>df
</p>
<p>dz
= &part;u
</p>
<p>&part;x
+ i &part;v
</p>
<p>&part;x
= &part;v
</p>
<p>&part;y
&minus; i &part;u
</p>
<p>&part;y
.
</p>
<p>Proof We have already shown the &ldquo;only if&rdquo; part. To show the &ldquo;if&rdquo; part, note
that if the derivative exists at all, it must equal (10.3). Thus, we have to show
that
</p>
<p>lim
�z&rarr;0
</p>
<p>f (z+�z)&minus; f (z)
�z
</p>
<p>= &part;u
&part;x
</p>
<p>+ i &part;v
&part;x
</p>
<p>or, equivalently, that
</p>
<p>∣∣∣∣
f (z+�z)&minus; f (z)
</p>
<p>�z
&minus;
(
&part;u
</p>
<p>&part;x
+ i &part;v
</p>
<p>&part;x
</p>
<p>)∣∣∣∣&lt; ǫ whenever |�z|&lt; δ.
</p>
<p>By definition,
</p>
<p>f (z+�z)&minus; f (z)
= u(x +�x,y +�y)+ iv(x +�x,y +�y)&minus; u(x, y)&minus; iv(x, y).
</p>
<p>Since u and v have continuous first partial derivatives, we can write
</p>
<p>u(x +�x,y +�y)= u(x, y)+ &part;u
&part;x
</p>
<p>�x + &part;u
&part;y
</p>
<p>�y + ǫ1�x + δ1�y,
</p>
<p>v(x +�x,y +�y)= v(x, y)+ &part;v
&part;x
</p>
<p>�x + &part;v
&part;y
</p>
<p>�y + ǫ2�x + δ2�y,
</p>
<p>where ǫ1, ǫ2, δ1, and δ2 are real numbers that approach zero as �x and �y
approach zero. Using these expressions, we can write
</p>
<p>f (z+�z)&minus; f (z)=
(
&part;u
</p>
<p>&part;x
+ i &part;v
</p>
<p>&part;x
</p>
<p>)
�x + i
</p>
<p>(
&minus;i &part;u
</p>
<p>&part;y
+ &part;v
</p>
<p>&part;y
</p>
<p>)
�y
</p>
<p>+ (ǫ1 + iǫ2)�x + (δ1 + iδ2)�y
</p>
<p>=
(
&part;u
</p>
<p>&part;x
+ i &part;v
</p>
<p>&part;x
</p>
<p>)
(�x + i�y)+ ǫ�x + δ�y,
</p>
<p>where ǫ &equiv; ǫ1 + iǫ2, δ &equiv; δ1 + iδ2, and we used the C-R conditions in the last
step. Dividing both sides by �z=�x + i�y, we get
</p>
<p>f (z+�z)&minus; f (z)
�z
</p>
<p>&minus;
(
&part;u
</p>
<p>&part;x
+ i &part;v
</p>
<p>&part;x
</p>
<p>)
= ǫ�x
</p>
<p>�z
+ δ�y
</p>
<p>�z
.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.2 Analytic Functions 301
</p>
<p>By the triangle inequality, |RHS| &le; |ǫ1+ iǫ2|+|δ1+ iδ2|. This follows from
the fact that |�x|/|�z| and |�y|/|�z| are both equal to at most 1. The ǫ and
δ terms can be made as small as desired by making �z small enough. We
have thus established that when the C-R conditions hold, the function f is
differentiable. �
</p>
<p>Historical Notes
</p>
<p>Augustin-Louis Cauchy (1789&ndash;1857) was one of the most influential French mathemati-
</p>
<p>Augustin-Louis Cauchy
</p>
<p>1789&ndash;1857
</p>
<p>cians of the nineteenth century. He began his career as a military engineer, but when his
health broke down in 1813 he followed his natural inclination and devoted himself wholly
to mathematics.
In mathematical productivity Cauchy was surpassed only by Euler, and his collected
works fill 27 fat volumes. He made substantial contributions to number theory and deter-
minants; is considered to be the originator of the theory of finite groups; and did extensive
work in astronomy, mechanics, optics, and the theory of elasticity.
His greatest achievements, however, lay in the field of analysis. Together with his contem-
poraries Gauss and Abel, he was a pioneer in the rigorous treatment of limits, continuous
functions, derivatives, integrals, and infinite series. Several of the basic tests for the con-
vergence of series are associated with his name. He also provided the first existence proof
for solutions of differential equations, gave the first proof of the convergence of a Taylor
series, and was the first to feel the need for a careful study of the convergence behavior
of Fourier series (see Chap. 9). However, his most important work was in the theory of
functions of a complex variable, which in essence he created and which has continued
to be one of the dominant branches of both pure and applied mathematics. In this field,
Cauchy&rsquo;s integral theorem and Cauchy&rsquo;s integral formula are fundamental tools without
which modern analysis could hardly exist (see Chap. 10).
Unfortunately, his personality did not harmonize with the fruitful power of his mind.
He was an arrogant royalist in politics and a self-righteous, preaching, pious believer in
religion&mdash;all this in an age of republican skepticism&mdash;and most of his fellow scientists
disliked him and considered him a smug hypocrite. It might be fairer to put first things
first and describe him as a great mathematician who happened also to be a sincere but
narrow-minded bigot.
</p>
<p>Definition 10.2.6 A function f : C &rarr; C is called analytic at z0 if it is analyticity and
singularity; regular and
</p>
<p>singular points; entire
</p>
<p>functions
</p>
<p>differentiable at z0 and at all other points in some neighborhood of z0. A
point at which f is analytic is called a regular point of f . A point at which
f is not analytic is called a singular point or a singularity of f . A function
for which all points in C are regular is called an entire function.
</p>
<p>Example 10.2.7 (Derivatives of some functions)
</p>
<p>(a) f (z)= z. Here u= x and v = y; the C-R conditions are easily shown
to hold, and for any z, we have df/dz= &part;u/&part;x+ i&part;v/&part;x = 1. There-
fore, the derivative exists at all points of the complex plane.
</p>
<p>(b) f (z) = z2. Here u = x2 &minus; y2 and v = 2xy; the C-R conditions
hold, and for all points z of the complex plane, we have df/dz =
&part;u/&part;x + i&part;v/&part;x = 2x + i2y = 2z. Therefore, f (z) is differentiable
at all points.
</p>
<p>(c) f (z)= zn for n&ge; 1. We can use mathematical induction and the fact
that the product of two entire functions is an entire function to show
that d
</p>
<p>dz
(zn)= nzn&minus;1.
</p>
<p>(d) f (z)= a0 + a1z+ &middot; &middot; &middot;+ an&minus;1zn&minus;1 + anzn, where ai are arbitrary con-
stants. That f (z) is entire follows directly from (c) and the fact that
the sum of two entire functions is entire.</p>
<p/>
</div>
<div class="page"><p/>
<p>302 10 Complex Calculus
</p>
<p>(e) f (z)= 1/z. The derivative can be found to be f &prime;(z)=&minus;1/z2, which
does not exist for z= 0. Thus, z= 0 is a singularity of f (z). However,
any other point of the complex plane is a regular point of f .
</p>
<p>(f) f (z)= |z|2. Using the definition of the derivative, we obtain
</p>
<p>�f
</p>
<p>�z
= |z+�z|
</p>
<p>2 &minus; |z|2
�z
</p>
<p>= (z+�z)(z
&lowast; +�z&lowast;)&minus; zz&lowast;
�z
</p>
<p>= z&lowast; +�z&lowast; + z�z
&lowast;
</p>
<p>�z
.
</p>
<p>For z= 0, �f/�z=�z&lowast;, which goes to zero as �z&rarr; 0. Therefore,
df/dz = 0 at z = 0.4 However, if z �= 0, the limit of �f/�z will de-
pend on how z is approached. Thus, df/dz does not exist if z �= 0.
This shows that |z|2 is differentiable only at z= 0 and nowhere else in
its neighborhood. It also shows that even if the real (here, u= x2+y2)
and imaginary (here, v = 0) parts of a complex function have continu-
ous partial derivatives of all orders at a point, the function may not be
differentiable there.
</p>
<p>(g) f (z) = 1/ sin z: This gives df/dz = &minus; cos z/ sin2 z. Thus, f has in-
finitely many (isolated) singular points at z=&plusmn;nπ for n= 0,1,2, . . . .
</p>
<p>Example 10.2.8 (The complex exponential function) In this example, wecomplex exponential
function find the (unique) function f : C&rarr; C that has the following three proper-
</p>
<p>ties:
</p>
<p>(a) f is single-valued and analytic for all z,
(b) df/dz= f (z), and
(c) f (z1 + z2)= f (z1)f (z2).
Property (b) shows that if f (z) is well behaved, then df/dz is also well
behaved. In particular, if f (z) is defined for all values of z, then f must be
entire.
</p>
<p>For z1 = 0 = z2, property (c) yields f (0) = [f (0)]2 &rArr; f (0) = 1, or
f (0)= 0. On the other hand,
</p>
<p>df
</p>
<p>dz
= lim
</p>
<p>�z&rarr;0
f (z+�z)&minus; f (z)
</p>
<p>�z
</p>
<p>= lim
�z&rarr;0
</p>
<p>f (z)f (�z)&minus; f (z)
�z
</p>
<p>= f (z) lim
�z&rarr;0
</p>
<p>f (�z)&minus; 1
�z
</p>
<p>.
</p>
<p>Property (b) now implies that
</p>
<p>lim
�z&rarr;0
</p>
<p>f (�z)&minus; 1
�z
</p>
<p>= 1 &rArr; f &prime;(0)= 1 and f (0)= 1.
</p>
<p>The first implication follows from the definition of derivative, and the sec-
ond from the fact that the only other choice, namely f (0)= 0, would yield
&minus;&infin; for the limit.
</p>
<p>4Although the derivative of |z|2 exists at z = 0, it is not analytic there (or anywhere
else). To be analytic at a point, a function must have derivatives at all points in some
neighborhood of the given point.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.2 Analytic Functions 303
</p>
<p>Now, we write f (z)= u(x, y)+iv(x, y), for which property (b) becomes
&part;u
</p>
<p>&part;x
+ i &part;v
</p>
<p>&part;x
= u+ iv &rArr; &part;u
</p>
<p>&part;x
= u, &part;v
</p>
<p>&part;x
= v.
</p>
<p>These equations have the most general solution u(x, y) = a(y)ex and
v(x, y) = b(y)ex , where a(y) and b(y) are the &ldquo;constants&rdquo; of integration.
The Cauchy-Riemann conditions now yield a(y) = db/dy and da/dy =
&minus;b(y), whose most general solution is a(y) = A cosy + B siny, b(y) =
A siny &minus; B cosy. On the other hand, f (0) = 1 yields u(0,0) = 1 and
v(0,0)= 0, implying that a(0)= 1, b(0)= 0 or A= 1, B = 0. We therefore
conclude that
</p>
<p>f (z)= a(y)ex + ib(y)ex = ex(cosy + i siny)= exeiy = ez.
</p>
<p>Both ex and eiy are well-defined in the entire complex plane. Hence, ez is
defined and differentiable over all C; therefore, it is entire.
</p>
<p>Example 10.2.7 shows that any polynomial in z is entire. Example 10.2.8
shows that the exponential function ez is also entire. Therefore, any product
and/or sum of polynomials and ez will also be entire. We can build other
entire functions. For instance, eiz and e&minus;iz are entire functions; therefore,
the trigonometric functions, defined as trigonometric functions
</p>
<p>sin z= e
iz &minus; e&minus;iz
</p>
<p>2i
and cos z= e
</p>
<p>iz + e&minus;iz
2
</p>
<p>, (10.4)
</p>
<p>are also entire functions. Problem 10.5 shows that sin z and cos z have only
real zeros. The hyperbolic functions can be defined similarly: hyperbolic functions
</p>
<p>sinh z= e
z &minus; e&minus;z
</p>
<p>2
and cosh z= e
</p>
<p>z + e&minus;z
2
</p>
<p>. (10.5)
</p>
<p>Although the sum and the product of entire functions are entire, the ratio,
in general, is not. For instance, if f (z) and g(z) are polynomials of degrees
m and n, respectively, then for n &gt; 0, the ratio f (z)/g(z) is not entire, be-
cause at the zeros of g(z)&mdash;which always exist and we assume that it is not
a zero of f (z)&mdash;the derivative is not defined.
</p>
<p>The functions u(x, y) and v(x, y) of an analytic function have an inter-
esting property that the following example investigates.
</p>
<p>Example 10.2.9 The family of curves u(x, y) = constant is perpendicular
to the family of curves v(x, y)= constant at each point of the complex plane
where f (z)= u+ iv is analytic.
</p>
<p>This can easily be seen by looking at the normal to the curves. The normal
to the curve u(x, y)= constant is simply &nabla;&nabla;&nabla;u= (&part;u/&part;x, &part;u/&part;y). Similarly,
the normal to the curve v(x, y)= constant is &nabla;&nabla;&nabla;v = (&part;v/&part;x, &part;v/&part;y). Taking
the dot product of these two normals, we obtain
</p>
<p>(&nabla;&nabla;&nabla;u) &middot; (&nabla;&nabla;&nabla;v)= &part;u
&part;x
</p>
<p>&part;v
</p>
<p>&part;x
+ &part;u
</p>
<p>&part;y
</p>
<p>&part;v
</p>
<p>&part;y
= &part;u
</p>
<p>&part;x
</p>
<p>(
&minus;&part;u
&part;y
</p>
<p>)
+ &part;u
</p>
<p>&part;y
</p>
<p>(
&part;u
</p>
<p>&part;x
</p>
<p>)
= 0
</p>
<p>by the C-R conditions.</p>
<p/>
</div>
<div class="page"><p/>
<p>304 10 Complex Calculus
</p>
<p>10.3 Conformal Maps
</p>
<p>The real and imaginary parts of an analytic function separately satisfy the
two-dimensional Laplace&rsquo;s equation:
</p>
<p>&part;2u
</p>
<p>&part;x2
+ &part;
</p>
<p>2u
</p>
<p>&part;y2
= 0, &part;
</p>
<p>2v
</p>
<p>&part;x2
+ &part;
</p>
<p>2v
</p>
<p>&part;y2
= 0. (10.6)
</p>
<p>This can easily be verified from the C-R conditions.
Laplace&rsquo;s equation in three dimensions,
</p>
<p>&part;2Φ
</p>
<p>&part;x2
+ &part;
</p>
<p>2Φ
</p>
<p>&part;y2
+ &part;
</p>
<p>2Φ
</p>
<p>&part;z2
= 0,
</p>
<p>describes the electrostatic potential Φ in a charge-free region of space. In a
typical electrostatic problem the potential Φ is given at certain boundaries
(usually conducting surfaces), and its value at every point in space is sought.
There are numerous techniques for solving such problems, and some of
them will be discussed later in the book. However, some of these problems
have a certain degree of symmetry that reduces them to two-dimensional
problems. In such cases, the theory of analytic functions can be extremely
helpful.
</p>
<p>The symmetry mentioned above is cylindrical symmetry, where the po-
tential is known a priori to be independent of the z-coordinate (the axis
of symmetry). This situation occurs when conductors are cylinders and&mdash;
if there are charge distributions in certain regions of space&mdash;the densities
are z-independent. In such cases, &part;Φ/&part;z= 0, and the problem reduces to a
two-dimensional one.
</p>
<p>Functions satisfying Laplace&rsquo;s equation are called harmonic functions.harmonic functions
Thus, the electrostatic potential is a three-dimensional harmonic function,
and the potential for a cylindrically symmetric charge distribution and
boundary condition is a two-dimensional harmonic function. Since the real
and the imaginary parts of a complex analytic function are also harmonic,
techniques of complex analysis are sometimes useful in solving electrostatic
problems with cylindrical symmetry.5
</p>
<p>To illustrate the connection between electrostatics and complex analysis,
consider a long straight filament with a constant linear charge density λ. It is
shown in introductory electromagnetism that the potential Φ (disregarding
the arbitrary constant that determines the reference potential) is given, in
cylindrical coordinates, by
</p>
<p>Φ = 2λ lnρ = 2λ ln
[(
x2 + y2
</p>
<p>)1/2]= 2λ ln |z|.
</p>
<p>Since Φ satisfies Laplace&rsquo;s equation, we conclude that Φ could be the real
part of an analytic function w(z), which we call the complex potential.complex potential
</p>
<p>5We use electrostatics because it is more familiar to physics students. Engineering stu-
dents are familiar with steady state heat transfer as well, which also involves Laplace&rsquo;s
equation, and therefore is amenable to this technique.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.3 Conformal Maps 305
</p>
<p>Example 10.2.9, plus the fact that the curves u= Φ = constant are circles,
imply that the constant-v curves are rays, i.e., v &prop; ϕ. Choosing the constant
of proportionality as 2λ, we obtain
</p>
<p>w(z)= 2λ lnρ + i2λϕ = 2λ ln
(
ρeiϕ
</p>
<p>)
= 2λ ln z.
</p>
<p>It is useful to know the complex potential of more than one filament of
charge. To find such a potential we must first find w(z) for a line charge
when it is displaced from the origin. If the line is located at z0 = x0 + iy0,
then it is easy to show that w(z)= 2λ ln(z&minus; z0). If there are n line charges
located at z1, z2, . . . , zn, then
</p>
<p>w(z)= 2
n&sum;
</p>
<p>k=1
λk ln(z&minus; zk). (10.7)
</p>
<p>The function w(z) can be used directly to solve a number of electro-
static problems involving simple charge distributions and conductor ar-
rangements. Some of these are illustrated in problems at the end of this
chapter. Instead of treating w(z) as a complex potential, let us look at it as a
map from the z-plane (or xy-plane) to the w-plane (or uv-plane). In partic-
ular, the equipotential curves (circles for a single line of charge) are mapped
onto lines parallel to the v-axis in the w-plane. This is so because equipo-
tential curves are defined by u= constant. Similarly, the constant-v curves
are mapped onto horizontal lines in the w-plane.
</p>
<p>This is an enormous simplification of the geometry. Straight lines, espe-
cially when they are parallel to axes, are by far simpler geometrical objects
than circles,6 especially if the circles are not centered at the origin. So let
us consider two complex &ldquo;worlds&rdquo;. One is represented by the xy-plane and
denoted by z. The other, the &ldquo;prime world&rdquo;, is represented7 by z&prime;, and its
real and imaginary parts by x&prime; and y&prime;. We start in z, where we need to find a
physical quantity such as the electrostatic potential Φ(x,y). If the problem
is too complicated in the z-world, we transfer it to the z&prime;-world, in which it
may be easily solvable; we solve the problem there (in terms of x&prime; and y&prime;)
and then transfer back to the z-world (x and y). The mapping that relates
z and z&prime; must be cleverly chosen. Otherwise, there is no guarantee that the
problem will simplify.
</p>
<p>Two conditions are necessary for the above strategy to work. First, the
differential equation describing the physics must not get more complicated
with the transfer to z&prime;. Since Laplace&rsquo;s equation is already of the simplest
type, the z&prime;-world must also respect Laplace&rsquo;s equation. Second, and more
importantly, the mapping must preserve the angles between curves. This is
necessary because we want the equipotential curves and the field lines to be
perpendicular in both worlds. A mapping that preserves the angle between
two curves at a given point is called a conformal mapping. We already have conformal mapping
such mappings at our disposal, as the following proposition shows.
</p>
<p>6This statement is valid only in Cartesian coordinates. But these are precisely the coordi-
nates we are using in this discussion.
7We are using z&prime; instead of w, and (x&prime;, y&prime;) instead of (u, v).</p>
<p/>
</div>
<div class="page"><p/>
<p>306 10 Complex Calculus
</p>
<p>Proposition 10.3.1 Let γ1 and γ2 be curves in the complex z-plane that
intersect at a point z0 at an angle α. Let f :C&rarr;C be a mapping given by
f (z)= z&prime; = x&prime; + iy&prime; that is analytic at z0. Let γ &prime;1 and γ &prime;2 be the images of γ1
and γ2 under this mapping, which intersect at an angle α&prime;. Then,
</p>
<p>(a) α&prime; = α, that is, the mapping f is conformal, if (dz&prime;/dz)z0 �= 0.
(b) If f is harmonic in (x, y), it is also harmonic in (x&prime;, y&prime;).
</p>
<p>Proof See Problem 10.21. �
</p>
<p>The following are some examples of conformal mappings.
</p>
<p>(a) z&prime; = z+ a, where a is an arbitrary complex constant. This is simply a
translation of the z-plane.translation
</p>
<p>(b) z&prime; = bz, where b is an arbitrary complex constant. This is a dilationdilation
whereby distances are dilated by a factor |b|. A graph in the z-plane
is mapped onto a similar (congruent) graph in the z&prime;-plane that will be
reduced (|b|&lt; 1) or enlarged (|b|&gt; 1) by a factor of |b|.
</p>
<p>(c) z&prime; = 1/z. This is called an inversion. Example 10.3.2 will show thatinversion
under such a mapping, circles are mapped onto circles or straight lines.
</p>
<p>(d) Combining the preceding three transformations yields the general
mapping
</p>
<p>z&prime; = az+ b
cz+ d , (10.8)
</p>
<p>which is conformal if cz + d �= 0 �= dz&prime;/dz. These conditions are
equivalent to ad &minus; bc �= 0.
</p>
<p>Example 10.3.2 A circle of radius r whose center is at a in the z-plane is
described by the equation |z &minus; a| = r . When transforming to the z&prime;-plane
under inversion, this equation becomes |1/z&prime; &minus; a| = r , or |1 &minus; az&prime;| =
r|z&prime;|. Squaring both sides and simplifying yields (r2 &minus; |a|2)|z&prime;|2 +
2 Re(az&prime;)&minus; 1 = 0. In terms of Cartesian coordinates, this becomes
</p>
<p>(
r2 &minus; |a|2
</p>
<p>)(
x&prime;2 + y&prime;2
</p>
<p>)
+ 2
</p>
<p>(
arx
</p>
<p>&prime; &minus; aiy&prime;
)
&minus; 1 = 0, (10.9)
</p>
<p>where a &equiv; ar + iai . We now consider two cases:
1. r �= |a|: Divide by r2 &minus; |a|2 and complete the squares to get
</p>
<p>(
x&prime;+ ar
</p>
<p>r2 &minus; |a|2
)2
</p>
<p>+
(
y&prime;&minus; ai
</p>
<p>r2 &minus; |a|2
)2
</p>
<p>&minus; a
2
r + a2i
</p>
<p>(r2 &minus; |a|2)2 &minus;
1
</p>
<p>r2 &minus; |a|2 = 0
</p>
<p>or defining
</p>
<p>a&prime;r &equiv;&minus;ar/
(
r2 &minus; |a|2
</p>
<p>)
,
</p>
<p>a&prime;i &equiv; ai/
(
r2 &minus; |a|2
</p>
<p>)
and r &prime; &equiv; r/
</p>
<p>∣∣r2 &minus; |a|2
∣∣,
</p>
<p>we have (x&prime; &minus; a&prime;r)2 + (y&prime; &minus; a&prime;i)2 = r &prime;2, which can also be written as
∣∣z&prime; &minus; a&prime;
</p>
<p>∣∣= r &prime;, a&prime; = a&prime;r + ia&prime;i =
a&lowast;
</p>
<p>|a|2 &minus; r2 .</p>
<p/>
</div>
<div class="page"><p/>
<p>10.3 Conformal Maps 307
</p>
<p>Fig. 10.3 In the z-plane, we see two equal cylinders whose centers are separated
</p>
<p>This is a circle in the z&prime;-plane with center at a&prime; and radius of r &prime;.
2. r = a: Then Eq. (10.9) reduces to arx&prime; &minus; aiy&prime; = 12 , which is the equa-
</p>
<p>tion of a line.
</p>
<p>If we use the transformation z&prime; = 1/(z &minus; c) instead of z&prime; = 1/z, then
|z&minus; a| = r becomes |1/z&prime; &minus; (a&minus; c)| = r , and all the above analysis will go
through exactly as before, except that a is replaced by a &minus; c.
</p>
<p>Mappings of the form given in Eq. (10.8) are called homographic trans- homographic
transformationsformations. A useful property of such transformations is that they can map
</p>
<p>an infinite region of the z-plane onto a finite region of the z&prime;-plane. In fact,
points with very large values of z are mapped onto a neighborhood of the
point z&prime; = a/c. Of course, this argument goes both ways: Eq. (10.8) also
maps a neighborhood of &minus;d/c in the z-plane onto large regions of the z&prime;-
plane. The usefulness of homographic transformations is illustrated in the
following example.
</p>
<p>Example 10.3.3 Consider two cylindrical conductors of equal radius r , electrostatic potential of
two charged cylinderheld at potentials u1 and u2, respectively, whose centers are D units of
</p>
<p>length apart. Choose the x-and the y-axes such that the centers of the cylin-
ders are located on the x-axis at distances a1 and a2 from the origin, as
shown in Fig. 10.3. Let us find the electrostatic potential produced by such
a configuration in the xy-plane.
</p>
<p>We know from elementary electrostatics that the problem becomes very
simple if the two cylinders are concentric (and, of course, of different radii).
Thus, we try to map the two circles onto two concentric circles in the z&prime;-
plane such that the infinite region outside the two circles in the z-plane gets
mapped onto the finite annular region between the two concentric circles in
the z&prime;-plane. We then (easily) find the potential in the z&prime;-plane, and transfer
it back to the z-plane.
</p>
<p>The most general mapping that may be able to do the job is that given
by Eq. (10.8). However, it turns out that we do not have to be this general.
In fact, the special case z&prime; = 1/(z &minus; c) in which c is a real constant will
be sufficient. So, z = (1/z&prime;) + c, and the circles |z &minus; ak| = r for k = 1,2
will be mapped onto the circles |z&prime; &minus; a&prime;k| = r &prime;k , where (by Example 10.3.2)
a&prime;k = (ak &minus; c)/[(ak &minus; c)2 &minus; r2] and r &prime;k = r/|(ak &minus; c)2 &minus; r2|.</p>
<p/>
</div>
<div class="page"><p/>
<p>308 10 Complex Calculus
</p>
<p>Fig. 10.4 In the z&prime;-plane, we see two concentric unequal cylinders
</p>
<p>Can we arrange the parameters so that the circles in the z&prime;-plane are con-
centric, i.e., that a&prime;1 = a&prime;2? The answer is yes. We set a&prime;1 = a&prime;2 and solve
for a2 in terms of a1. The result is either the trivial solution a2 = a1, or
a2 = c &minus; r2/(a1 &minus; c). If we place the origin of the z-plane at the center of
the first cylinder, then a1 = 0 and a2 =D = c + r2/c. We can also find a&prime;1
and a&prime;2: a
</p>
<p>&prime;
1 = a&prime;2 &equiv; a&prime; =&minus;c/(c2 &minus; r2), and the geometry of the problem is as
</p>
<p>shown in Fig. 10.4.
For such a geometry the potential at a point in the annular region is given
</p>
<p>by
</p>
<p>Φ &prime; =A lnρ +B =A ln
∣∣z&prime; &minus; a&prime;
</p>
<p>∣∣+B,
where A and B are real constants determined by the conditions Φ &prime;(r &prime;1)= u1
and Φ &prime;(r &prime;2)= u2, which yields
</p>
<p>A= u1 &minus; u2
ln(r &prime;1/r
</p>
<p>&prime;
2)
</p>
<p>and B = u2 ln r
&prime;
1 &minus; u1 ln r &prime;2
</p>
<p>ln(r &prime;1/r
&prime;
2)
</p>
<p>.
</p>
<p>The potential Φ &prime; is the real part of the complex function8
</p>
<p>F
(
z&prime;
)
=A ln
</p>
<p>(
z&prime; &minus; a&prime;
</p>
<p>)
+B,
</p>
<p>which is analytic except at z&prime; = a&prime;, a point lying outside the region of in-
terest. We can now go back to the z-plane by substituting z&prime; = 1/(z&minus; c) to
obtain
</p>
<p>G(z)=A ln
(
</p>
<p>1
</p>
<p>z&minus; c &minus; a
&prime;
)
+B,
</p>
<p>whose real part is the potential in the z-plane:
</p>
<p>Φ(x,y)= Re
[
G(z)
</p>
<p>]
=A ln
</p>
<p>∣∣∣∣
1 &minus; a&prime;z+ a&prime;c
</p>
<p>z&minus; c
</p>
<p>∣∣∣∣+B
</p>
<p>8Writing z= |z|eiθ , we note that ln z= ln |z| + iθ , so that the real part of a complex log
function is the log of the absolute value.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.4 Integration of Complex Functions 309
</p>
<p>=A ln
∣∣∣∣
(1 + a&prime;c&minus; a&prime;x)&minus; ia&prime;y
</p>
<p>(x &minus; c)+ iy
</p>
<p>∣∣∣∣+B
</p>
<p>= A
2
</p>
<p>ln
</p>
<p>[
(1 + a&prime;c&minus; a&prime;x)2 + a&prime;2y2
</p>
<p>(x &minus; c)2 + y2
]
+B.
</p>
<p>This is the potential we want.
</p>
<p>10.4 Integration of Complex Functions
</p>
<p>The derivative of a complex function is an important concept and, as the
previous section demonstrated, provides a powerful tool in physical appli-
cations. The concept of integration is even more important. In fact, we will
see in the next section that derivatives can be written in terms of integrals.
We will study integrals of complex functions in detail in this section.
</p>
<p>The definite integral of a complex function is defined in analogy to that
of a real function:
</p>
<p>&int; α2
α1
</p>
<p>f (z) dz= lim
N &rarr;&infin;
�zi &rarr; 0
</p>
<p>N&sum;
</p>
<p>i=1
f (zi)�zi,
</p>
<p>where �zi is a small segment, situated at zi , of the curve that connects the
complex number α1 to the complex number α2 in the z-plane. Since there
are infinitely many ways of connecting α1 to α2, it is possible to obtain
different values for the integral for different paths. Before discussing the
integral itself, let us first consider the various kinds of path encountered in
complex analysis.
</p>
<p>1. A curve is a map γ : [a, b]&rarr;C from the real interval into the complex curve, simple arc, path,
and smooth arc definedplane given by γ (t) = γr(t)+ iγi(t), where a &le; t &le; b, and γr and γi
</p>
<p>are the real and imaginary parts of γ ; γ (a) is called the initial point of
the curve and γ (b) its final point.
</p>
<p>2. A simple arc, or a Jordan arc, is a curve that does not cross itself, i.e.,
γ is injective (or one to one), so that γ (t1) �= γ (t2) when t1 �= t2.
</p>
<p>3. A path is a finite collection {γ1, γ2, . . . , γn} of simple arcs such that
the initial point of γk+1 coincides with the final point of γk .
</p>
<p>4. A smooth arc is a curve for which dγ /dt = dγr/dt + idγi/dt exists
and is nonzero for t &isin; [a, b].
</p>
<p>5. A contour is a path whose arcs are smooth. When the initial point of γ1 contour defined
coincides with the final point of γn, the contour is said to be a simple
closed contour.
</p>
<p>The path dependence of a complex integral is analogous to the line inte-
gral of a vector field encountered in vector analysis. In fact, we can turn the
integral of a complex function into a line integral as follows. We substitute
f (z)= u+ iv and dz= dx + idy in the integral to obtain
</p>
<p>&int; α2
α1
</p>
<p>f (z) dz=
&int; α2
α1
</p>
<p>(udx &minus; v dy)+ i
&int; α2
α1
</p>
<p>(v dx + udy).</p>
<p/>
</div>
<div class="page"><p/>
<p>310 10 Complex Calculus
</p>
<p>Fig. 10.5 The three different paths of integration corresponding to the integrals I1, I &prime;1,
I2, and I &prime;2
</p>
<p>If we define the two-dimensional vectors A1 &equiv; (u,&minus;v) and A2 &equiv; (v,u), we
get
</p>
<p>&int; α2
α1
</p>
<p>f (z) dz=
&int; α2
α1
</p>
<p>A1 &middot; dr + i
&int; α2
α1
</p>
<p>A2 &middot; dr.
</p>
<p>It follows from Stokes&rsquo; theorem (or Green&rsquo;s theorem, since the vectors lie
in a plane) that the integral of f is path-independent only if both A1 and A2
have vanishing curls. This in turn follows if and only if u and v satisfy the
C-R conditions, and this is exactly what is needed for f (z) to be analytic.
</p>
<p>Path-independence of a line integral of a vector A is equivalent to the
vanishing of the integral along a closed path, and the latter is equivalent to
the vanishing of &nabla; &times; A = 0 at every point of the region bordered by the
closed path. In the case of complex integrals, this result is stated as
</p>
<p>Theorem 10.4.1 (Cauchy-Goursat theorem) Let f :C&rarr;C be analytic onCauchy-Goursat
theorem a simple closed contour C and at all points inside C. Then
</p>
<p>∮
</p>
<p>C
</p>
<p>f (z) dz= 0.
</p>
<p>Example 10.4.2 (Examples of definite integrals)
</p>
<p>(a) Let us evaluate the integral I1 =
&int;
γ1
z dz where γ1 is the straight line
</p>
<p>drawn from the origin to the point (1,2) (see Fig. 10.5). Along such a
line y = 2x and, using t for x, γ1(t)= t + 2it where 0 &le; t &le; 1; so
</p>
<p>I1 =
&int;
</p>
<p>γ1
</p>
<p>z dz=
&int; 1
</p>
<p>0
(t + 2it)(dt + 2idt)
</p>
<p>=
&int; 1
</p>
<p>0
(&minus;3tdt + 4itdt)=&minus;3
</p>
<p>2
+ 2i.
</p>
<p>For a different path γ2, along which y = 2x2, we get γ2(t)= t + 2it2
where 0 &le; t &le; 1, and
</p>
<p>I &prime;1 =
&int;
</p>
<p>γ2
</p>
<p>z dz=
&int; 1
</p>
<p>0
</p>
<p>(
t + 2it2
</p>
<p>)
(dt + 4itdt)=&minus;3
</p>
<p>2
+ 2i.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.4 Integration of Complex Functions 311
</p>
<p>Fig. 10.6 The two semicircular paths for calculating I3 and I &prime;3
</p>
<p>Therefore, I1 = I &prime;1. This is what is expected from the Cauchy-Goursat
theorem because the function f (z) = z is analytic on the two paths
and in the region bounded by them.
</p>
<p>(b) To find I2 &equiv;
&int;
γ1
z2dz with γ1 as in part (a), substitute for z in terms
</p>
<p>of t :
</p>
<p>I2 =
&int;
</p>
<p>γ1
</p>
<p>(t + 2it)2(dt + 2idt)= (1 + 2i)3
&int; 1
</p>
<p>0
t2dt =&minus;11
</p>
<p>3
&minus; 2
</p>
<p>3
i.
</p>
<p>Next we compare I2 with I &prime;2 =
&int;
γ3
z2dz where γ3 is as shown in
</p>
<p>Fig. 10.5. This path can be described by
</p>
<p>γ3(t)=
{
t for 0 &le; t &le; 1,
1 + i(t &minus; 1) for 1 &le; t &le; 3.
</p>
<p>Therefore,
</p>
<p>I &prime;2 =
&int; 1
</p>
<p>0
t2dt +
</p>
<p>&int; 3
</p>
<p>1
</p>
<p>[
1 + i(t &minus; 1)
</p>
<p>]2
(idt)= 1
</p>
<p>3
&minus; 4 &minus; 2
</p>
<p>3
i =&minus;11
</p>
<p>3
&minus; 2
</p>
<p>3
i,
</p>
<p>which is identical to I2, once again because the function is analytic on
γ1 and γ3 as well as in the region bounded by them.
</p>
<p>(c) Now consider I3 &equiv;
&int;
γ4
dz/z where γ4 is the upper semicircle of unit
</p>
<p>radius, as shown in Fig. 10.6. A parametric equation for γ4 can be
given in terms of θ :
</p>
<p>γ4(θ)= cos θ + i sin θ = eiθ &rArr; dz= ieiθdθ, 0 &le; θ &le; π.
</p>
<p>Thus, we obtain
</p>
<p>I3 =
&int; π
</p>
<p>0
</p>
<p>1
</p>
<p>eiθ
ieiθdθ = iπ.
</p>
<p>On the other hand, for γ &prime;4, the lower semicircle of unit radius, we get
</p>
<p>I &prime;3 =
&int;
</p>
<p>γ &prime;4
</p>
<p>1
</p>
<p>z
dz=
</p>
<p>&int; π
</p>
<p>2π
</p>
<p>1
</p>
<p>eiθ
ieiθdθ =&minus;iπ.</p>
<p/>
</div>
<div class="page"><p/>
<p>312 10 Complex Calculus
</p>
<p>Fig. 10.7 A complicated contour can be broken up into simpler ones. Note that the
boundaries of the &ldquo;eyes&rdquo; and the &ldquo;mouth&rdquo; are forced to be traversed in the (negative)
clockwise direction
</p>
<p>Here the two integrals are not equal. From γ4 and γ &prime;4 we can construct
a counterclockwise simple closed contour C, along which the integral
of f (z) = 1/z becomes
</p>
<p>∮
C
dz/z = I3 &minus; I &prime;3 = 2iπ . That the integral
</p>
<p>is not zero is a consequence of the fact that 1/z is not analytic at all
points of the region bounded by the closed contour C.
</p>
<p>The Cauchy-Goursat theorem applies to more complicated regions.
When a region contains points at which f (z) is not analytic, those points
can be avoided by redefining the region and the contour. Such a procedure
requires an agreement on the direction we will take.
</p>
<p>Convention When integrating along a closed contour, we agree to moveconvention for positive
sense of integration
</p>
<p>around a closed contour
</p>
<p>along the contour in such a way that the enclosed region lies to our left. An
integration that follows this convention is called integration in the positive
sense. Integration performed in the opposite direction acquires a minus sign.
</p>
<p>For a simple closed contour, movement in the counterclockwise direction
yields integration in the positive sense. However, as the contour becomes
more complicated, this conclusion breaks down. Figure 10.7 shows a com-
plicated path enclosing a region (shaded) in which the integrand is analytic.
Note that it is possible to traverse a portion of the region twice in opposite
directions without affecting the integral, which may be a sum of integrals for
different pieces of the contour. Also note that the &ldquo;eyes&rdquo; and the &ldquo;mouth&rdquo;
are traversed clockwise! This is necessary because of the convention above.
A region such as that shown in Fig. 10.7, in which holes are &ldquo;punched out&rdquo;,
is called multiply connected. In contrast, a simply connected region is onesimply and multiply
</p>
<p>connected regions in which every simple closed contour encloses only points of the region.
One important consequence of the Cauchy-Goursat theorem is the fol-Cauchy Integral
</p>
<p>Formula (CIF) lowing:</p>
<p/>
</div>
<div class="page"><p/>
<p>10.4 Integration of Complex Functions 313
</p>
<p>Fig. 10.8 The integrand is analytic within and on the boundary of the shaded region. It
is always possible to construct contours that exclude all singular points
</p>
<p>Theorem 10.4.3 (Cauchy integral formula) Let f be analytic on and
within a simple closed contour C integrated in the positive sense. Let
z0 be any interior point to C. Then
</p>
<p>f (z0)=
1
</p>
<p>2πi
</p>
<p>∮
</p>
<p>C
</p>
<p>f (z)
</p>
<p>z&minus; z0
dz
</p>
<p>To prove the Cauchy integral formula (CIF), we need the following
lemma.
</p>
<p>Lemma 10.4.4 (Darboux inequality) Suppose f : C &rarr; C is continuous
and bounded on a path γ , i.e., there exists a positive number M such that
|f (z)| &le;M for all values z &isin; γ . Then
</p>
<p>∣∣∣∣
&int;
</p>
<p>γ
</p>
<p>f (z) dz
</p>
<p>∣∣∣∣&le;MLγ ,
</p>
<p>where Lγ is the length of the path of integration.
</p>
<p>Proof See Problem 10.27. �
</p>
<p>Now we are ready to prove the Cauchy integral formula.
</p>
<p>Proof of CIF Consider the shaded region in Fig. 10.8, which is bounded
by C, by γ0 (a circle of arbitrarily small radius δ centered at z0), and by L1
and L2, two straight line segments infinitesimally close to one another (we
can, in fact, assume that L1 and L2 are right on top of one another; however,
they are separated in the figure for clarity). Let C&prime; = C &cup; γ0 &cup;L1 &cup;L2.
</p>
<p>Since f (z)/(z&minus; z0) is analytic everywhere on the contour C&prime; and inside
the shaded region, we can write
</p>
<p>0 =
∮
</p>
<p>C&prime;
</p>
<p>f (z)
</p>
<p>z&minus; z0
dz=
</p>
<p>∮
</p>
<p>C
</p>
<p>f (z)
</p>
<p>z&minus; z0
dz+
</p>
<p>∮
</p>
<p>γ0
</p>
<p>f (z)
</p>
<p>z&minus; z0
dz (10.10)</p>
<p/>
</div>
<div class="page"><p/>
<p>314 10 Complex Calculus
</p>
<p>because the contributions from L1 and L2 cancel. Let us evaluate the con-
tribution from the infinitesimal circle γ0. First we note that because f (z) is
continuous (differentiability implies continuity), we can write
</p>
<p>∣∣∣∣
f (z)&minus; f (z0)
</p>
<p>z&minus; z0
</p>
<p>∣∣∣∣=
|f (z)&minus; f (z0)|
</p>
<p>|z&minus; z0|
= |f (z)&minus; f (z0)|
</p>
<p>δ
&lt;
</p>
<p>ǫ
</p>
<p>δ
</p>
<p>for z &isin; γ0, where ǫ is a small positive number. We now apply the Darboux
inequality and write
</p>
<p>∣∣∣∣
∮
</p>
<p>γ0
</p>
<p>f (z)&minus; f (z0)
z&minus; z0
</p>
<p>dz
</p>
<p>∣∣∣∣&lt;
ǫ
</p>
<p>δ
2πδ = 2πǫ.
</p>
<p>This means that the integral goes to zero as δ&rarr; 0, or
∮
</p>
<p>γ0
</p>
<p>f (z)
</p>
<p>z&minus; z0
dz=
</p>
<p>∮
</p>
<p>γ0
</p>
<p>f (z0)
</p>
<p>z&minus; z0
dz= f (z0)
</p>
<p>∮
</p>
<p>γ0
</p>
<p>dz
</p>
<p>z&minus; z0
.
</p>
<p>We can easily calculate the integral on the RHS by noting that z&minus; z0 = δeiϕ
and that γ0 has a clockwise direction:
</p>
<p>∮
</p>
<p>γ0
</p>
<p>dz
</p>
<p>z&minus; z0
=&minus;
</p>
<p>&int; 2π
</p>
<p>0
</p>
<p>iδeiϕdϕ
</p>
<p>δeiϕ
=&minus;2πi &rArr;
</p>
<p>∮
</p>
<p>γ0
</p>
<p>f (z)
</p>
<p>z&minus; z0
dz=&minus;2πif (z0).
</p>
<p>Substituting this in (10.10) yields the desired result. �
</p>
<p>Example 10.4.5 We can use the CIF to evaluate the integrals
</p>
<p>I1 =
∮
</p>
<p>C1
</p>
<p>z2dz
</p>
<p>(z2 + 3)2(z&minus; i) , I2 =
∮
</p>
<p>C2
</p>
<p>(z2 &minus; 1)dz
(z&minus; 12 )(z2 &minus; 4)3
</p>
<p>,
</p>
<p>I3 =
∮
</p>
<p>C3
</p>
<p>ez/2dz
</p>
<p>(z&minus; iπ)(z2 &minus; 20)4 ,
</p>
<p>where C1, C2, and C3 are circles centered at the origin with radii r1 = 3/2,
r2 = 1, and r3 = 4.
</p>
<p>For I1 we note that f (z)= z2/(z2 +3)2 is analytic within and on C1, and
z0 = i lies in the interior of C1. Thus,
</p>
<p>I1 =
∮
</p>
<p>C1
</p>
<p>f (z)dz
</p>
<p>z&minus; i = 2πif (i)= 2πi
i2
</p>
<p>(i2 + 3)2 =&minus;i
π
</p>
<p>2
.
</p>
<p>Similarly, f (z)= (z2 &minus; 1)/(z2 &minus; 4)3 for the integral I2 is analytic on and
within C2, and z0 = 1/2 is an interior point of C2. Thus, the CIF gives
</p>
<p>I2 =
∮
</p>
<p>C2
</p>
<p>f (z)dz
</p>
<p>z&minus; 12
= 2πif (1/2)= 32π
</p>
<p>1125
i.
</p>
<p>For the last integral, f (z) = ez/2/(z2 &minus; 20)4, and the interior point is
z0 = iπ :
</p>
<p>I3 =
∮
</p>
<p>C3
</p>
<p>f (z)dz
</p>
<p>z&minus; iπ = 2πif (iπ)=&minus;
2π
</p>
<p>(π2 + 20)4 .</p>
<p/>
</div>
<div class="page"><p/>
<p>10.5 Derivatives as Integrals 315
</p>
<p>The Cauchy integral formula gives the value of an analytic function at
every point inside a simple closed contour when it is given the value of
the function only at points on the contour. It seems as though an analytic
function is not free to change inside a region once its value is fixed on the
contour enclosing that region.
</p>
<p>There is an analogous situation in electrostatics: The specification of the
potential at the boundaries, such as the surfaces of conductors, automatically
determines the potential at any other point in the region of space bounded
by the conductors. This is the content of the uniqueness theorem used in
electrostatic boundary value problems. However, the electrostatic potential Explanation of why the
</p>
<p>Cauchy integral formula
</p>
<p>works!
</p>
<p>Φ is bound by another condition, Laplace&rsquo;s equation; and the combination
of Laplace&rsquo;s equation and the boundary conditions furnishes the uniqueness
of Φ . Similarly, the real and imaginary parts of an analytic function sepa-
rately satisfy Laplace&rsquo;s equation in two dimensions! Thus, it should come
as no surprise that the value of an analytic function on a boundary (contour)
determines the function at all points inside the boundary.
</p>
<p>10.5 Derivatives as Integrals
</p>
<p>The Cauchy Integral Formula is a powerful tool for working with analytic
functions. One of the applications of this formula is in evaluating the deriva-
tives of such functions. It is convenient to change the dummy integration
variable to ξ and write the CIF as
</p>
<p>f (z)= 1
2πi
</p>
<p>∮
</p>
<p>C
</p>
<p>f (ξ) dξ
</p>
<p>ξ &minus; z , (10.11)
</p>
<p>where C is a simple closed contour in the ξ -plane and z is a point within C.
As preparation for defining the derivative of an analytic function, we need
the following result.
</p>
<p>Proposition 10.5.1 Let γ be any path&mdash;a contour, for example&mdash;and g a
continuous function on that path. The function f (z) defined by
</p>
<p>f (z)= 1
2πi
</p>
<p>&int;
</p>
<p>γ
</p>
<p>g(ξ) dξ
</p>
<p>ξ &minus; z
</p>
<p>is analytic at every point z /&isin; γ .
</p>
<p>Proof The proof follows immediately from differentiation of the integral:
</p>
<p>df
</p>
<p>dz
= 1
</p>
<p>2πi
</p>
<p>d
</p>
<p>dz
</p>
<p>&int;
</p>
<p>γ
</p>
<p>g(ξ) dξ
</p>
<p>ξ &minus; z
</p>
<p>= 1
2πi
</p>
<p>&int;
</p>
<p>γ
</p>
<p>g(ξ) dξ
d
</p>
<p>dz
</p>
<p>(
1
</p>
<p>ξ &minus; z
</p>
<p>)
= 1
</p>
<p>2πi
</p>
<p>&int;
</p>
<p>γ
</p>
<p>g(ξ) dξ
</p>
<p>(ξ &minus; z)2 .</p>
<p/>
</div>
<div class="page"><p/>
<p>316 10 Complex Calculus
</p>
<p>This is defined for all values of z not on γ .9 Thus, f (z) is analytic there. �
</p>
<p>We can generalize the formula above to the nth derivative, and obtain
</p>
<p>dnf
</p>
<p>dzn
= n!
</p>
<p>2πi
</p>
<p>&int;
</p>
<p>γ
</p>
<p>g(ξ) dξ
</p>
<p>(ξ &minus; z)n+1 .
</p>
<p>Applying this result to an analytic function expressed by Eq. (10.11), we
obtain the following important theorem.
</p>
<p>Theorem 10.5.2 The derivatives of all orders of an analytic function f (z)derivative of an analytic
function given in terms
</p>
<p>of an integral
</p>
<p>exist in the domain of analyticity of the function and are themselves analytic
in that domain. The nth derivative of f (z) is given by
</p>
<p>f (n)(z)= d
nf
</p>
<p>dzn
= n!
</p>
<p>2πi
</p>
<p>∮
</p>
<p>C
</p>
<p>f (ξ) dξ
</p>
<p>(ξ &minus; z)n+1 . (10.12)
</p>
<p>Example 10.5.3 Let us apply Eq. (10.12) directly to some simple func-
tions. In all cases, we will assume that the contour is a circle of radius r
centered at z.
</p>
<p>(a) Let f (z)=K , a constant. Then, for n= 1 we have
</p>
<p>df
</p>
<p>dz
= 1
</p>
<p>2πi
</p>
<p>∮
</p>
<p>C
</p>
<p>K dξ
</p>
<p>(ξ &minus; z)2 .
</p>
<p>Since ξ is on the circle C centered at z, ξ &minus; z= reiθ and dξ = rieiθdθ . So
we have
</p>
<p>df
</p>
<p>dz
= 1
</p>
<p>2πi
</p>
<p>&int; 2π
</p>
<p>0
</p>
<p>Kireiθdθ
</p>
<p>(reiθ )2
= K
</p>
<p>2πr
</p>
<p>&int; 2π
</p>
<p>0
e&minus;iθdθ = 0.
</p>
<p>(b) Given f (z)= z, its first derivative will be
</p>
<p>df
</p>
<p>dz
= 1
</p>
<p>2πi
</p>
<p>∮
</p>
<p>C
</p>
<p>ξ dξ
</p>
<p>(ξ &minus; z)2 =
1
</p>
<p>2πi
</p>
<p>&int; 2π
</p>
<p>0
</p>
<p>(z+ reiθ )ireiθdθ
(reiθ )2
</p>
<p>= 1
2π
</p>
<p>(
z
</p>
<p>r
</p>
<p>&int; 2π
</p>
<p>0
e&minus;iθdθ +
</p>
<p>&int; 2π
</p>
<p>0
dθ
</p>
<p>)
= 1
</p>
<p>2π
(0 + 2π)= 1.
</p>
<p>(c) Given f (z)= z2, for the first derivative, Eq. (10.12) yields
</p>
<p>df
</p>
<p>dz
= 1
</p>
<p>2πi
</p>
<p>∮
</p>
<p>C
</p>
<p>ξ2 dξ
</p>
<p>(ξ &minus; z)2 =
1
</p>
<p>2πi
</p>
<p>&int; 2π
</p>
<p>0
</p>
<p>(z+ reiθ )2ireiθdθ
(reiθ )2
</p>
<p>= 1
2π
</p>
<p>&int; 2π
</p>
<p>0
</p>
<p>[
z2 +
</p>
<p>(
reiθ
</p>
<p>)2 + 2zreiθ
](
reiθ
</p>
<p>)&minus;1
dθ
</p>
<p>9The interchange of differentiation and integration requires justification. Such an inter-
change can be done if the integral has some restrictive properties. We shall not concern
ourselves with such details. In fact, one can achieve the same result by using the definition
of derivatives and the usual properties of integrals.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.5 Derivatives as Integrals 317
</p>
<p>= 1
2π
</p>
<p>(
z2
</p>
<p>r
</p>
<p>&int; 2π
</p>
<p>0
e&minus;iθdθ + r
</p>
<p>&int; 2π
</p>
<p>0
eiθdθ + 2z
</p>
<p>&int; 2π
</p>
<p>0
dθ
</p>
<p>)
= 2z.
</p>
<p>It can be shown that, in general, (d/dz)zm = mzm&minus;1. The proof is left as
Problem 10.30.
</p>
<p>The CIF is a central formula in complex analysis, and we shall see its
significance in much of the later development of complex analysis. For now,
let us demonstrate its usefulness in proving a couple of important properties
of analytic functions.
</p>
<p>Proposition 10.5.4 The absolute value of an analytic function f (z) cannot
have a local maximum within the region of analyticity of the function.
</p>
<p>Proof Let S &sub; C be the region of analyticity of f and z0 a point in S. Let
γ0 be a circle of radius δ in S, centered at z0. Using the CIF, and noting that
z&minus; z0 = δeiθ , we have
</p>
<p>∣∣f (z0)
∣∣=
</p>
<p>∣∣∣∣
1
</p>
<p>2πi
</p>
<p>∮
</p>
<p>γ0
</p>
<p>f (z)
</p>
<p>z&minus; z0
dz
</p>
<p>∣∣∣∣=
1
</p>
<p>2π
</p>
<p>∣∣∣∣
&int; 2π
</p>
<p>0
</p>
<p>f (z)
</p>
<p>δeiθ
iδeiθdθ
</p>
<p>∣∣∣∣
</p>
<p>&le; 1
2π
</p>
<p>&int; 2π
</p>
<p>0
</p>
<p>∣∣f (z)
∣∣dθ &le; 1
</p>
<p>2π
</p>
<p>&int; 2π
</p>
<p>0
</p>
<p>∣∣f (zmax)
∣∣dθ =
</p>
<p>∣∣f (zmax)
∣∣,
</p>
<p>where zmax is where the maximum value of |f (z)| occurs on γ0. This in-
equality says that for any point z0 that one picks, there is always another
point which produces a larger absolute value for f . Therefore, there can be
no local maximum within S. �
</p>
<p>Proposition 10.5.5 A bounded entire function is necessarily a constant.
</p>
<p>Proof We show that the derivative of such a function is zero. Consider
</p>
<p>df
</p>
<p>dz
= 1
</p>
<p>2πi
</p>
<p>∮
</p>
<p>C
</p>
<p>f (ξ) dξ
</p>
<p>(ξ &minus; z)2 .
</p>
<p>Since f is an entire function, the closed contour C can be chosen to be a
very large circle of radius R with center at z. Taking the absolute value of
both sides yields
</p>
<p>∣∣∣∣
df
</p>
<p>dz
</p>
<p>∣∣∣∣=
1
</p>
<p>2π
</p>
<p>∣∣∣∣
&int; 2π
</p>
<p>0
</p>
<p>f (z)
</p>
<p>(Reiθ )2
iReiθdθ
</p>
<p>∣∣∣∣
</p>
<p>&le; 1
2π
</p>
<p>&int; 2π
</p>
<p>0
</p>
<p>|f (z)|
R
</p>
<p>dθ &le; 1
2π
</p>
<p>&int; 2π
</p>
<p>0
</p>
<p>M
</p>
<p>R
dθ = M
</p>
<p>R
,
</p>
<p>where M is the maximum of the function in the complex plane. Now, as
R&rarr;&infin;, the derivative goes to zero, and the function must be a constant. �
</p>
<p>Proposition 10.5.5 is a very powerful statement about analytic functions.
There are many interesting and nontrivial real functions that are bounded
and have derivatives of all orders on the entire real line. For instance, e&minus;x
</p>
<p>2</p>
<p/>
</div>
<div class="page"><p/>
<p>318 10 Complex Calculus
</p>
<p>is such a function. No such freedom exists for complex analytic functions.
Any nontrivial analytic function is either not bounded (goes to infinity some-
where on the complex plane) or not entire (it is not analytic at some point(s)
of the complex plane).
</p>
<p>A consequence of Proposition 10.5.5 is the following
</p>
<p>fundamental theorem of
</p>
<p>algebra
Theorem 10.5.6 (Fundamental theorem of algebra) Any polynomial
</p>
<p>p(x)= a0 + a1x + &middot; &middot; &middot; + anxn, an �= 0
</p>
<p>can be factored completely as
</p>
<p>p(x)= an(x &minus; z1)(x &minus; z2) &middot; &middot; &middot; (x &minus; zn),
</p>
<p>where the zi are complex numbers.
</p>
<p>Proof Let f (z) = 1/p(z) and assume the contrary, i.e., that p(z) is never
zero for any (finite) z &isin;C. Then f (z) is bounded and analytic for all z &isin;C,
and Proposition 10.5.5 says that f (z) is a constant. This is obviously wrong
if n &gt; 0. Thus, there must be at least one z, say z = z1, for which p(z) is
zero. So, we can factor out (z&minus;z1) from p(z) and write p(z)= (z&minus;z1)q(z)
where q(z) is of degree n&minus;1. Applying the above argument to q(z), we have
p(z) = (z &minus; z1)(z &minus; z2)r(z) where r(z) is of degree n &minus; 2. Continuing in
this way, we can factor p(z) into linear factors. The last polynomial will be
a constant (a polynomial of degree zero) which has to be equal to an to make
the coefficient of zn equal to the original polynomial. �
</p>
<p>The primitive (indefinite integral) of an analytic function can be defined
using definite integrals just as in the real case. Let f : C&rarr; C be analytic
in a region S of the complex plane. Let z0 and z be two points in S, and
define10 F(z)&equiv;
</p>
<p>&int; z
z0
f (ξ) dξ . We can show that F(z) is the primitive of f (z)
</p>
<p>by showing that
</p>
<p>lim
�z&rarr;0
</p>
<p>∣∣∣∣
F(z+�z)&minus; F(z)
</p>
<p>�z
&minus; f (z)
</p>
<p>∣∣∣∣= 0.
</p>
<p>We leave the details as a problem for the reader.
</p>
<p>Proposition 10.5.7 Let f : C&rarr; C be analytic in a region S of C. Then at
every point z &isin; S, there exists an analytic function F :C&rarr;C such that
</p>
<p>dF
</p>
<p>dz
= f (z).
</p>
<p>10Note that the integral is path-independent due to the analyticity of f . Thus, F is well-
defined.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.6 Infinite Complex Series 319
</p>
<p>In the sketch of the proof of Proposition 10.5.7, we used only the continu-
ity of f and the fact that the integral was well-defined. These two conditions
are sufficient to establish the analyticity of F and f , since the latter is the
derivative of the former. The following theorem, due to Morera, states this
fact and is the converse of the Cauchy-Goursat theorem.
</p>
<p>Theorem 10.5.8 (Morera&rsquo;s theorem) Let a function f :C&rarr;C be continu- Morera&rsquo;s theorem
ous in a simply connected region S. If for each simple closed contour C in
S we have
</p>
<p>∮
C
f (ξ) dξ = 0, then f is analytic throughout S.
</p>
<p>10.6 Infinite Complex Series
</p>
<p>The expansion of functions in terms of polynomials or monomials is impor-
tant in calculus and was emphasized in Chaps. 7 and 8. We now apply this
concept to analytic functions.
</p>
<p>10.6.1 Properties of Series
</p>
<p>Complex series are very similar to real series with which the reader is as-
sumed to have some familiarity. Therefore, we state (without proof) the most
important properties of complex series before discussing the quintessential
Taylor and Laurent series.
</p>
<p>A complex series is said to converge absolutely if the real series absolute convergence
</p>
<p>&infin;&sum;
</p>
<p>k=0
|zk| =
</p>
<p>&infin;&sum;
</p>
<p>k=0
</p>
<p>&radic;
x2k + y2k
</p>
<p>converges. Clearly, absolute convergence implies convergence.
</p>
<p>Proposition 10.6.1 If the power series
&sum;&infin;
</p>
<p>k=0 ak(z &minus; z0)k converges for power series
z1 �= z0, then it converges absolutely for every value of z such that |z&minus;z0|&lt;
|z1 &minus; z0|. Similarly if the power series
</p>
<p>&sum;&infin;
k=0 bk/(z &minus; z0)k converges for
</p>
<p>z2 �= z0, then it converges absolutely for every value of z such that |z&minus;z0|&gt;
|z2 &minus; z0|.
</p>
<p>A geometric interpretation of this proposition is that if a power series&mdash;
with positive powers&mdash;converges for a point at a distance r1 from z0, then it
converges for all interior points of the circle whose center is z0, and whose
radius is r1. Similarly, if a power series&mdash;with negative powers&mdash;converges
for a point at a distance r2 from z0, then it converges for all exterior points of
the circle whose center is z0 and whose radius is r2 (see Fig. 10.9). Generally
speaking, positive powers are used for points inside a circle and negative
powers for points outside it.
</p>
<p>The largest circle about z0 such that the first power series of Proposi-
tion 10.6.1 converges is called the circle of convergence of the power series. circle of convergence</p>
<p/>
</div>
<div class="page"><p/>
<p>320 10 Complex Calculus
</p>
<p>Fig. 10.9 (a) Power series with positive exponents converge for the interior points of
a circle. (b) Power series with negative exponents converge for the exterior points of a
circle
</p>
<p>The proposition implies that the series cannot converge at any point outside
the circle of convergence.
</p>
<p>In determining the convergence of a power series
</p>
<p>S(z)&equiv;
&infin;&sum;
</p>
<p>n=0
an(z&minus; z0)n, (10.13)
</p>
<p>we look at the behavior of the sequence of partial sums
</p>
<p>SN (z)&equiv;
N&sum;
</p>
<p>n=0
an(z&minus; z0)n.
</p>
<p>Convergence of (10.13) implies that for any ε &gt; 0, there exists an integer
Nε such that
</p>
<p>∣∣S(z)&minus; SN (z)
∣∣&lt; ε whenever N &gt;Nε.
</p>
<p>In general, the integer Nε may be dependent on z; that is, for different values
uniform convergence
</p>
<p>explained
</p>
<p>of z, we may be forced to pick different Nε&rsquo;s. When Nε is independent of z,
we say that the convergence is uniform.
</p>
<p>power series are
</p>
<p>uniformly convergent
</p>
<p>and analytic
</p>
<p>Theorem 10.6.2 The power series S(z)=&sum;&infin;n=0 an(z&minus; z0)n is uni-
formly convergent for all points within its circle of convergence and
represents a function that is analytic there.
</p>
<p>By substituting the reciprocal of (z&minus;z0) in the power series, we can show
that if
</p>
<p>&sum;&infin;
k=0 bk/(z &minus; z0)k is convergent in the annulus r2 &lt; |z &minus; z0| &lt; r1,
</p>
<p>then it is uniformly convergent for all z in that annulus.
</p>
<p>Theorem 10.6.3 A convergent power series can be differentiated and inte-
grated term by term; that is, if S(z)=&sum;&infin;n=0 an(z&minus; z0)n, then
</p>
<p>power series can be
</p>
<p>differentiated and
</p>
<p>integrated term by term</p>
<p/>
</div>
<div class="page"><p/>
<p>10.6 Infinite Complex Series 321
</p>
<p>dS(z)
</p>
<p>dz
=
</p>
<p>&infin;&sum;
</p>
<p>n=1
nan(z&minus; z0)n&minus;1,
</p>
<p>&int;
</p>
<p>γ
</p>
<p>S(z) dz=
&infin;&sum;
</p>
<p>n=0
an
</p>
<p>&int;
</p>
<p>γ
</p>
<p>(z&minus; z0)ndz
</p>
<p>for any path γ lying in the circle of convergence of the power series.
</p>
<p>10.6.2 Taylor and Laurent Series
</p>
<p>We now state and prove the two main theorems of this section. A Taylor
series consists of terms with only positive powers. A Laurent series allows
for negative powers as well.
</p>
<p>Theorem 10.6.4 (Taylor series) Let f be analytic throughout the interior Taylor series
of a circle C0 having radius r0 and centered at z0. Then at each point z
inside C0,
</p>
<p>f (z)= f (z0)+ f &prime;(z0)(z&minus; z0)+ &middot; &middot; &middot; =
&infin;&sum;
</p>
<p>n=0
</p>
<p>f (n)(z0)
</p>
<p>n! (z&minus; z0)
n. (10.14)
</p>
<p>Proof From the CIF and the fact that z is inside C0, we have
</p>
<p>f (z)= 1
2πi
</p>
<p>∮
</p>
<p>C0
</p>
<p>f (ξ)
</p>
<p>ξ &minus; z dξ.
</p>
<p>On the other hand,
</p>
<p>1
</p>
<p>ξ &minus; z =
1
</p>
<p>ξ &minus; z0 + z0 &minus; z
= 1
</p>
<p>(ξ &minus; z0)(1 &minus; z&minus;z0ξ&minus;z0 )
</p>
<p>= 1
ξ &minus; z0
</p>
<p>1
</p>
<p>1 &minus; z&minus;z0
ξ&minus;z0
</p>
<p>= 1
ξ &minus; z0
</p>
<p>&infin;&sum;
</p>
<p>n=0
</p>
<p>(
z&minus; z0
ξ &minus; z0
</p>
<p>)n
.
</p>
<p>The last equality follows from the fact that |(z&minus;z0)/(ξ&minus;z0)|&lt; 1&mdash;because
z is inside the circle C0 and ξ is on it&mdash;and from the sum of a geometric
series. Substituting in the CIF and using Theorem 10.5.2, we obtain the
result. �
</p>
<p>For z0 = 0 we obtain the Maclaurin series: Maclaurin series
</p>
<p>f (z)= f (0)+ f &prime;(0)z+ &middot; &middot; &middot; =
&infin;&sum;
</p>
<p>n=0
</p>
<p>f (n)(0)
</p>
<p>n! z
n.
</p>
<p>The Taylor expansion requires analyticity of the function at all points
interior to the circle C0. On many occasions there may be a point inside C0
at which the function is not analytic. The Laurent series accommodates such
cases.
</p>
<p>Theorem 10.6.5 (Laurent series) Let C1 and C2 be circles of radii r1 Laurent series
and r2, both centered at z0 in the z-plane with r1 &gt; r2. Let f : C &rarr; C</p>
<p/>
</div>
<div class="page"><p/>
<p>322 10 Complex Calculus
</p>
<p>Fig. 10.10 The annular region within and on whose contour the expanded function is
analytic
</p>
<p>be analytic on C1 and C2 and throughout S, the annular region between the
two circles. Then, at each point z &isin; S, f (z) is given by
</p>
<p>f (z)=
&infin;&sum;
</p>
<p>n=&minus;&infin;
an(z&minus; z0)n where an =
</p>
<p>1
</p>
<p>2πi
</p>
<p>∮
</p>
<p>C
</p>
<p>f (ξ)
</p>
<p>(ξ &minus; z0)n+1
dξ
</p>
<p>and C is any contour within S that encircles z0.
</p>
<p>Proof Let γ be a small closed contour in S enclosing z, as shown in
Fig. 10.10. For the composite contour C&prime; = C1 &cup; C2 &cup; γ , the Cauchy-
Goursat theorem gives
</p>
<p>0 =
∮
</p>
<p>C&prime;
</p>
<p>f (ξ)
</p>
<p>ξ &minus; z dξ =
∮
</p>
<p>C1
</p>
<p>f (ξ)
</p>
<p>ξ &minus; z dξ &minus;
∮
</p>
<p>C2
</p>
<p>f (ξ)
</p>
<p>ξ &minus; z dξ &minus;
∮
</p>
<p>γ
</p>
<p>f (ξ)
</p>
<p>ξ &minus; z dξ,
</p>
<p>where the γ and C2 integrations are negative because their interior lies to
our right as we traverse them. The γ integral is simply 2πif (z) by the CIF.
Thus, we obtain
</p>
<p>2πif (z)=
∮
</p>
<p>C1
</p>
<p>f (ξ)
</p>
<p>ξ &minus; z dξ &minus;
∮
</p>
<p>C2
</p>
<p>f (ξ)
</p>
<p>ξ &minus; z dξ. (10.15)
</p>
<p>Now we use the same trick we used in deriving the Taylor expansion. Since
z is located in the annular region, r2 &lt; |z&minus; z0| &lt; r1. We have to keep this
in mind when expanding the fractions. In particular, for ξ &isin; C1 we want
the ξ term to be in the denominator, and for ξ &isin; C2 we want it to be in the
numerator. Substituting such expansions in Eq. (10.15) yields
</p>
<p>2πif (z)=
&infin;&sum;
</p>
<p>n=0
(z&minus; z0)n
</p>
<p>∮
</p>
<p>C1
</p>
<p>f (ξ) dξ
</p>
<p>(ξ &minus; z0)n+1
</p>
<p>+
&infin;&sum;
</p>
<p>n=0
</p>
<p>1
</p>
<p>(z&minus; z0)n+1
∮
</p>
<p>C2
</p>
<p>f (ξ)(ξ &minus; z0)n dξ. (10.16)</p>
<p/>
</div>
<div class="page"><p/>
<p>10.6 Infinite Complex Series 323
</p>
<p>Fig. 10.11 The arbitrary contour in the annular region used in the Laurent expansion.
The break in C and the gap in the shaded region are magnified for visual clarity
</p>
<p>Now we consider an arbitrary contour C in S that encircles z0. Fig-
ure 10.11 shows a region bounded by a contour composed of C1 and C.
In this region f (ξ)/(ξ &minus; z0)n+1 is analytic (because ξ can never equal z0).
Thus, the integral over the composite contour must vanish by the Cauchy-
Goursat theorem. It follows that the integral over C1 is equal to that over C.
A similar argument shows that the C2 integral can also be replaced by an
integral over C. We let n + 1 = &minus;m in the second sum of Eq. (10.16) to
transform it into
</p>
<p>&minus;&infin;&sum;
</p>
<p>m=&minus;1
</p>
<p>1
</p>
<p>(z&minus; z0)&minus;m
∮
</p>
<p>C
</p>
<p>f (ξ)(ξ &minus; z0)&minus;m&minus;1 dξ
</p>
<p>=
&minus;1&sum;
</p>
<p>m=&minus;&infin;
(z&minus; z0)m
</p>
<p>∮
</p>
<p>C
</p>
<p>f (ξ) dξ
</p>
<p>(ξ &minus; z0)m+1
.
</p>
<p>Changing the dummy index back to n and substituting the result in
Eq. (10.16) yields
</p>
<p>2πif (z)=
&infin;&sum;
</p>
<p>n=0
(z&minus; z0)n
</p>
<p>∮
</p>
<p>C
</p>
<p>f (ξ)
</p>
<p>(ξ &minus; z0)n+1
dξ
</p>
<p>+
n=&minus;1&sum;
</p>
<p>&minus;&infin;
(z&minus; z0)n
</p>
<p>∮
</p>
<p>C
</p>
<p>f (ξ)
</p>
<p>(ξ &minus; z0)n+1
dξ.
</p>
<p>We can now combine the sums and divide both sides by 2πi to get the
desired expansion. �
</p>
<p>The Laurent expansion is convergent as long as r2 &lt; |z &minus; z0| &lt; r1. In
particular, if r2 = 0, and if the function is analytic throughout the interior of
the larger circle, then an will be zero for n=&minus;1,&minus;2, . . . because f (ξ)/(ξ&minus;
z0)
</p>
<p>n+1 will be analytic for negative n, and the integral will be zero by the</p>
<p/>
</div>
<div class="page"><p/>
<p>324 10 Complex Calculus
</p>
<p>Cauchy-Goursat theorem. Thus, only positive powers of (z &minus; z0) will be
present in the series, and we recover the Taylor series, as we should.
</p>
<p>It is clear that we can expand C1 and shrink C2 until we encounter a point
at which f is no longer analytic. This is obvious from the construction of the
proof, in which only the analyticity in the annular region is important, not
its size. Thus, we can include all the possible analytic points by expanding
C1 and shrinking C2.
</p>
<p>Example 10.6.6 Let us expand some functions in terms of series. For an
entire function there is no point in the entire complex plane at which it is
not analytic. Thus, only positive powers of (z&minus; z0) will be present, and we
will have a Taylor expansion that is valid for all values of z.
</p>
<p>(a) Let us expand ez around z0 = 0. The nth derivative of ez is ez. Thus,
f (n)(0)= 1, and Taylor (Maclaurin) expansion gives
</p>
<p>ez =
&infin;&sum;
</p>
<p>n=0
</p>
<p>f (n)(0)
</p>
<p>n! z
n =
</p>
<p>&infin;&sum;
</p>
<p>n=0
</p>
<p>zn
</p>
<p>n! .
</p>
<p>(b) The Maclaurin series for sin z is obtained by noting that
</p>
<p>dn
</p>
<p>dzn
sin z
</p>
<p>∣∣∣∣
z=0
</p>
<p>=
{
</p>
<p>0 if n is even,
</p>
<p>(&minus;1)(n&minus;1)/2 if n is odd
</p>
<p>and substituting this in the Maclaurin expansion:
</p>
<p>sin z=
&sum;
</p>
<p>n odd
</p>
<p>(&minus;1)(n&minus;1)/2 z
n
</p>
<p>n! =
&infin;&sum;
</p>
<p>k=0
(&minus;1)k z
</p>
<p>2k+1
</p>
<p>(2k + 1)! .
</p>
<p>Similarly, we can obtain
</p>
<p>cos z=
&infin;&sum;
</p>
<p>k=0
(&minus;1)k z
</p>
<p>2k
</p>
<p>(2k)! , sinh z=
&infin;&sum;
</p>
<p>k=0
</p>
<p>z2k+1
</p>
<p>(2k + 1)! ,
</p>
<p>cosh z=
&infin;&sum;
</p>
<p>k=0
</p>
<p>z2k
</p>
<p>(2k)! .
</p>
<p>(c) The function 1/(1 + z) is not entire, so the region of its convergence
is limited. Let us find the Maclaurin expansion of this function. The
function is analytic within all circles of radii r &lt; 1. At r = 1 we en-
counter a singularity, the point z=&minus;1. Thus, the series converges for
all points11 z for which |z|&lt; 1. For such points we have
</p>
<p>f (n)(0)= d
n
</p>
<p>dzn
</p>
<p>[
(1 + z)&minus;1
</p>
<p>]∣∣∣∣
z=0
</p>
<p>= (&minus;1)nn!.
</p>
<p>11As remarked before, the series diverges for all points outside the circle |z| = 1. This
does not mean that the function cannot be represented by a series for points outside the
circle. On the contrary, we shall see shortly that Laurent series, with negative powers of
z&minus; z0 are designed precisely for such a purpose.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.6 Infinite Complex Series 325
</p>
<p>Thus,
</p>
<p>1
</p>
<p>1 + z =
&infin;&sum;
</p>
<p>n=0
</p>
<p>f (n)(0)
</p>
<p>n! z
n =
</p>
<p>&infin;&sum;
</p>
<p>n=0
(&minus;1)nzn.
</p>
<p>Taylor and Laurent series allow us to express an analytic function as a
power series. For a Taylor series of f (z), the expansion is routine because
the coefficient of its nth term is simply f (n)(z0)/n!, where z0 is the center of
the circle of convergence. When a Laurent series is applicable, however, the
nth coefficient is not, in general, easy to evaluate. Usually it can be found
by inspection and certain manipulations of other known series. But if we
use such an intuitive approach to determine the coefficients, can we be sure
that we have obtained the correct Laurent series? The following theorem Laurent series is unique
answers this question.
</p>
<p>Theorem 10.6.7 If the series
&sum;&infin;
</p>
<p>n=&minus;&infin; an(z&minus; z0)n converges to f (z)
at all points in some annular region about z0, then it is the unique
Laurent series expansion of f (z) in that region.
</p>
<p>Proof Multiply both sides of f (z)=&sum;&infin;n=&minus;&infin; an(z&minus; z0)n by
</p>
<p>1
</p>
<p>2πi(z&minus; z0)k+1
,
</p>
<p>integrate the result along a contour C in the annular region, and use the
easily verifiable fact that
</p>
<p>1
</p>
<p>2πi
</p>
<p>∮
</p>
<p>C
</p>
<p>dz
</p>
<p>(z&minus; z0)k&minus;n+1
= δkn
</p>
<p>to obtain
</p>
<p>1
</p>
<p>2πi
</p>
<p>∮
</p>
<p>C
</p>
<p>f (z)
</p>
<p>(z&minus; z0)k+1
dz= ak.
</p>
<p>Thus, the coefficient in the power series of f is precisely the coefficient in
the Laurent series, and the two must be identical. �
</p>
<p>We will look at some examples that illustrate the abstract ideas developed
in the preceding collection of theorems and propositions. However, we can
consider a much broader range of examples if we know the arithmetic of
power series. The following theorem about arithmetical manipulations with
power series is not difficult to prove (see [Chur 74]).
</p>
<p>Theorem 10.6.8 Let the two power series You can add, subtract,
and multiply convergent
</p>
<p>power series
f (z)=
</p>
<p>&infin;&sum;
</p>
<p>n=&minus;&infin;
an(z&minus; z0)n and g(z)=
</p>
<p>&infin;&sum;
</p>
<p>n=&minus;&infin;
bn(z&minus; z0)n</p>
<p/>
</div>
<div class="page"><p/>
<p>326 10 Complex Calculus
</p>
<p>be convergent within some annular region r2 &lt; |z&minus; z0|&lt; r1. Then
</p>
<p>f (z)+ g(z)=
&infin;&sum;
</p>
<p>n=&minus;&infin;
(an + bn)(z&minus; z0)n
</p>
<p>and
</p>
<p>f (z)g(z)=
&infin;&sum;
</p>
<p>n=&minus;&infin;
</p>
<p>&infin;&sum;
</p>
<p>m=&minus;&infin;
anbm(z&minus; z0)m+n &equiv;
</p>
<p>&infin;&sum;
</p>
<p>k=&minus;&infin;
ck(z&minus; z0)k
</p>
<p>for z interior to the annular region. Furthermore, if g(z) �= 0 for some neigh-
borhood of z0, then the series obtained by long division of the first series by
the second converges to f (z)/g(z) in that neighborhood.
</p>
<p>This theorem, in essence, says that converging power series can be ma-
nipulated as though they were finite sums (polynomials). Such manipula-
tions are extremely useful when dealing with Taylor and Laurent expansions
in which the straightforward calculation of coefficients may be tedious. The
following examples illustrate the power of infinite-series arithmetic.
</p>
<p>Example 10.6.9 To expand the function f (z) = 2+3z
z2+z3 in a Laurent series
</p>
<p>about z= 0, rewrite it as
</p>
<p>f (z)= 1
z2
</p>
<p>(
2 + 3z
1 + z
</p>
<p>)
= 1
</p>
<p>z2
</p>
<p>(
3 &minus; 1
</p>
<p>1 + z
</p>
<p>)
= 1
</p>
<p>z2
</p>
<p>(
3 &minus;
</p>
<p>&infin;&sum;
</p>
<p>n=0
(&minus;1)nzn
</p>
<p>)
</p>
<p>= 1
z2
</p>
<p>(
3 &minus; 1 + z&minus; z2 + z3 &minus; &middot; &middot; &middot;
</p>
<p>)
= 2
</p>
<p>z2
+ 1
</p>
<p>z
&minus; 1 + z&minus; z2 + &middot; &middot; &middot; .
</p>
<p>This series converges for 0 &lt; |z|&lt; 1. We note that negative powers of z are
also present.12 Using the notation of Theorem 10.6.5, we have an = 0 for
n&le;&minus;3, a&minus;2 = 2, a&minus;1 = 1, and an = (&minus;1)n+1 for n&ge; 0.
</p>
<p>Example 10.6.10 The function f (z)= 1/(4z&minus;z2) is the ratio of two entire
functions. Therefore, by Theorem 10.6.8, it is analytic everywhere except at
the zeros of its denominator, z = 0 and z = 4. For the annular region (here
r2 of Theorem 10.6.5 is zero) 0 &lt; |z| &lt; 4, we expand f (z) in the Laurent
series around z= 0. Instead of actually calculating an, we first note that
</p>
<p>f (z)= 1
4z
</p>
<p>(
1
</p>
<p>1 &minus; z/4
</p>
<p>)
.
</p>
<p>The second factor can be expanded in a geometric series because |z/4|&lt; 1:
</p>
<p>1
</p>
<p>1 &minus; z/4 =
&infin;&sum;
</p>
<p>n=0
</p>
<p>(
z
</p>
<p>4
</p>
<p>)n
=
</p>
<p>&infin;&sum;
</p>
<p>n=0
4&minus;nzn.
</p>
<p>12This is a reflection of the fact that the function is not analytic inside the entire circle
|z| = 1; it blows up at z= 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.6 Infinite Complex Series 327
</p>
<p>Dividing this by 4z, and noting that z = 0 is the only zero of 4z and is
excluded from the annular region, we obtain the expansion
</p>
<p>f (z)=
&infin;&sum;
</p>
<p>n=0
4&minus;n
</p>
<p>zn
</p>
<p>4z
=
</p>
<p>&infin;&sum;
</p>
<p>n=0
4&minus;n&minus;1zn&minus;1.
</p>
<p>Although we derived this series using manipulations of other series, the
uniqueness of series representations assures us that this is the Laurent se-
ries for the indicated region.
</p>
<p>How can we represent f (z) in the region for which |z|&gt; 4? This region
is exterior to the circle |z| = 4, so we expect negative powers of z. To find
the Laurent expansion we write
</p>
<p>f (z)=&minus; 1
z2
</p>
<p>(
1
</p>
<p>1 &minus; 4/z
</p>
<p>)
</p>
<p>and note that |4/z| &lt; 1 for points exterior to the larger circle. The second
factor can be written as a geometric series:
</p>
<p>1
</p>
<p>1 &minus; 4/z =
&infin;&sum;
</p>
<p>n=0
</p>
<p>(
4
</p>
<p>z
</p>
<p>)n
=
</p>
<p>&infin;&sum;
</p>
<p>n=0
4nz&minus;n.
</p>
<p>Dividing by &minus;z2, which is nonzero in the region exterior to the larger circle,
yields
</p>
<p>f (z)=&minus;
&infin;&sum;
</p>
<p>n=0
4nz&minus;n&minus;2.
</p>
<p>Example 10.6.11 The function f (z)= z/[(z&minus; 1)(z&minus; 2)] has a Taylor ex-
pansion around the origin for |z|&lt; 1. To find this expansion, we write13
</p>
<p>f (z)=&minus; 1
z&minus; 1 +
</p>
<p>2
</p>
<p>z&minus; 2 =
1
</p>
<p>1 &minus; z &minus;
1
</p>
<p>1 &minus; z/2 .
</p>
<p>Expanding both fractions in geometric series (both |z| and |z/2| are less
than 1), we obtain f (z)=&sum;&infin;n=0 zn&minus;
</p>
<p>&sum;&infin;
n=0(z/2)
</p>
<p>n. Adding the two series&mdash;
using Theorem 10.6.8&mdash;yields
</p>
<p>f (z)=
&infin;&sum;
</p>
<p>n=0
</p>
<p>(
1 &minus; 2&minus;n
</p>
<p>)
zn for |z|&lt; 1.
</p>
<p>This is the unique Taylor expansion of f (z) within the circle |z| = 1.
For 1 &lt; |z|&lt; 2 we have a Laurent series. To obtain this series, write
</p>
<p>f (z)= 1/z
1/z&minus; 1 &minus;
</p>
<p>1
</p>
<p>1 &minus; z/2 =&minus;
1
</p>
<p>z
</p>
<p>(
1
</p>
<p>1 &minus; 1/z
</p>
<p>)
&minus; 1
</p>
<p>1 &minus; z/2 .
</p>
<p>13We could, of course, evaluate the derivatives of all orders of the function at z = 0 and
use Maclaurin&rsquo;s formula. However, the present method gives the same result much more
quickly.</p>
<p/>
</div>
<div class="page"><p/>
<p>328 10 Complex Calculus
</p>
<p>Since both fractions on the RHS converge in the annular region (|1/z|&lt; 1,
|z/2|&lt; 1), we get
</p>
<p>f (z)=&minus;1
z
</p>
<p>&infin;&sum;
</p>
<p>n=0
</p>
<p>(
1
</p>
<p>z
</p>
<p>)n
&minus;
</p>
<p>&infin;&sum;
</p>
<p>n=0
</p>
<p>(
z
</p>
<p>2
</p>
<p>)n
=&minus;
</p>
<p>&infin;&sum;
</p>
<p>n=0
z&minus;n&minus;1 &minus;
</p>
<p>&infin;&sum;
</p>
<p>n=0
2&minus;nzn
</p>
<p>=&minus;
&minus;&infin;&sum;
</p>
<p>n=&minus;1
zn &minus;
</p>
<p>&infin;&sum;
</p>
<p>n=0
2&minus;nzn =
</p>
<p>&infin;&sum;
</p>
<p>n=&minus;&infin;
anz
</p>
<p>n,
</p>
<p>where an = &minus;1 for n &lt; 0 and an = &minus;2&minus;n for n &ge; 0. This is the unique
Laurent expansion of f (z) in the given region.
</p>
<p>Finally, for |z| &gt; 2 we have only negative powers of z. We obtain the
expansion in this region by rewriting f (z) as follows:
</p>
<p>f (z)=&minus; 1/z
1 &minus; 1/z +
</p>
<p>2/z
</p>
<p>1 &minus; 2/z .
</p>
<p>Expanding the fractions yields
</p>
<p>f (z)=&minus;
&infin;&sum;
</p>
<p>n=0
z&minus;n&minus;1 +
</p>
<p>&infin;&sum;
</p>
<p>n=0
2n+1z&minus;n&minus;1 =
</p>
<p>&infin;&sum;
</p>
<p>n=0
</p>
<p>(
2n+1 &minus; 1
</p>
<p>)
z&minus;n&minus;1.
</p>
<p>This is again the unique expansion of f (z) in the region |z|&gt; 2.
</p>
<p>Example 10.6.12 Define f (z) as
</p>
<p>f (z)=
{
(1 &minus; cos z)/z2 for z �= 0,
1
2 for z= 0.
</p>
<p>We can show that f (z) is an entire function.
Since 1 &minus; cos z and z2 are entire functions, their ratio is analytic every-
</p>
<p>where except at the zeros of its denominator. The only such zero is z = 0.
Thus, Theorem 10.6.8 implies that f (z) is analytic everywhere except pos-
sibly at z= 0. To see the behavior of f (z) at z= 0, we look at its Maclaurin
series:
</p>
<p>1 &minus; cos z= 1 &minus;
&infin;&sum;
</p>
<p>n=0
(&minus;1)n z
</p>
<p>2n
</p>
<p>(2n)! ,
</p>
<p>which implies that
</p>
<p>1 &minus; cos z
z2
</p>
<p>=
&infin;&sum;
</p>
<p>n=1
(&minus;1)n+1 z
</p>
<p>2n&minus;2
</p>
<p>(2n)! =
1
</p>
<p>2
&minus; z
</p>
<p>2
</p>
<p>4! +
z4
</p>
<p>6! &minus; &middot; &middot; &middot; .
</p>
<p>The expansion on the RHS shows that the value of the series at z = 0 is
1
2 , which, by definition, is f (0). Thus, the series converges for all z, and
Theorem 10.6.2 says that f (z) is entire.
</p>
<p>A Laurent series can give information about the integral of a function
around a closed contour in whose interior the function may not be analytic.
In fact, the coefficient of the first negative power in a Laurent series is given</p>
<p/>
</div>
<div class="page"><p/>
<p>10.6 Infinite Complex Series 329
</p>
<p>by
</p>
<p>a&minus;1 =
1
</p>
<p>2πi
</p>
<p>∮
</p>
<p>C
</p>
<p>f (ξ) dξ. (10.17)
</p>
<p>Thus, to find the integral of a (nonanalytic) function around a closed contour
surrounding z0, we write the Laurent series for the function and read off the
coefficient of the 1/(z&minus; z0) term.
</p>
<p>Example 10.6.13 As an illustration of this idea, let us evaluate the integral
I =
</p>
<p>∮
C
dz/[z2(z &minus; 2)], where C is the circle of radius 1 centered at the
</p>
<p>origin. The function is analytic in the annular region 0 &lt; |z| &lt; 2. We can
therefore expand it as a Laurent series about z= 0 in that region:
</p>
<p>1
</p>
<p>z2(z&minus; 2) =&minus;
1
</p>
<p>2z2
</p>
<p>(
1
</p>
<p>1 &minus; z/2
</p>
<p>)
=&minus; 1
</p>
<p>2z2
</p>
<p>&infin;&sum;
</p>
<p>n=0
</p>
<p>(
z
</p>
<p>2
</p>
<p>)n
</p>
<p>=&minus;1
2
</p>
<p>(
1
</p>
<p>z2
</p>
<p>)
&minus; 1
</p>
<p>4
</p>
<p>(
1
</p>
<p>z
</p>
<p>)
&minus; 1
</p>
<p>8
&minus; &middot; &middot; &middot; .
</p>
<p>Thus, a&minus;1 =&minus; 14 , and
∮
C
dz/[z2(z&minus;2)] = 2πia&minus;1 =&minus;iπ/2. A direct evalu-
</p>
<p>ation of the integral is nontrivial. In fact, we will see later that to find certain
integrals, it is advantageous to cast them in the form of a contour integral
and use either Eq. (10.17) or a related equation.
</p>
<p>Let f :C&rarr;C be analytic at z0. Then by definition, there exists a neigh-
borhood of z0 in which f is analytic. In particular, we can find a circle
|z&minus; z0| = r &gt; 0 in whose interior f has a Taylor expansion.
</p>
<p>Definition 10.6.14 Let
</p>
<p>f (z)=
&infin;&sum;
</p>
<p>n=0
</p>
<p>f (n)(z0)
</p>
<p>n! (z&minus; z0)
n &equiv;
</p>
<p>&infin;&sum;
</p>
<p>n=0
an(z&minus; z0)n.
</p>
<p>Then f is said to have a zero of order k at z0 if f (n)(z0) = 0 for n =
0,1, . . . , k &minus; 1 but f (k)(z0) �= 0.
</p>
<p>zero of order k
</p>
<p>In that case f (z) = (z &minus; z0)k
&sum;&infin;
</p>
<p>n=0 ak+n(z &minus; z0)n, where ak �= 0 and
|z&minus; z0|&lt; r . We define g(z) as
</p>
<p>g(z)=
&infin;&sum;
</p>
<p>n=0
ak+n(z&minus; z0)n where |z&minus; z0|&lt; r
</p>
<p>and note that g(z0)= ak �= 0. Convergence of the series on the RHS implies
that g(z) is continuous at z0. Consequently, for each ǫ &gt; 0, there exists δ
such that |g(z)&minus; ak| &lt; ǫ whenever |z &minus; z0| &lt; δ. If we choose ǫ = |ak|/2,
then, for some δ0 &gt; 0, |g(z)&minus; ak| &lt; |ak|/2 whenever |z &minus; z0| &lt; δ0. Thus,
as long as z is inside the circle |z&minus; z0|&lt; δ0, g(z) cannot vanish (because if
it did the first inequality would imply that |ak|&lt; |ak|/2). We therefore have
the following result.</p>
<p/>
</div>
<div class="page"><p/>
<p>330 10 Complex Calculus
</p>
<p>Theorem 10.6.15 Let f : C &rarr; C be analytic at z0 and f (z0) = 0. Thenthe zeros of an analytic
function are isolated there exists a neighborhood of z0 throughout which f has no other zeros
</p>
<p>unless f is identically zero there. Thus, the zeros of an analytic function are
isolated.
</p>
<p>When k = 1, we say that z0 is a simple zero of f . To find the order ofsimple zero
the zero of a function at a point, we differentiate the function, evaluate the
derivative at that point, and continue the process until we obtain a nonzero
value for the derivative.
</p>
<p>Example 10.6.16 Here are some functions with their zeros:
</p>
<p>(a) The zeros of cos z, which are z= (2k+ 1)π/2, are all simple, because
</p>
<p>d
</p>
<p>dz
cos z
</p>
<p>∣∣∣∣
z=(2k+1)π/2
</p>
<p>=&minus;sin
[
(2k + 1)π
</p>
<p>2
</p>
<p>]
�= 0.
</p>
<p>(b) To find the order of the zero of f (z)= ez &minus; 1&minus; z&minus; z2/2 at z= 0, we
differentiate f (z) and evaluate f &prime;(0):
</p>
<p>f &prime;(0)=
(
ez &minus; 1 &minus; z
</p>
<p>)
z=0 = 0.
</p>
<p>Differentiating again gives f &prime;&prime;(0) = (ez &minus; 1)z=0 = 0. Differentiating
once more yields f &prime;&prime;&prime;(0)= (ez)z=0 = 1. Thus, the zero is of order 3.
</p>
<p>10.7 Problems
</p>
<p>10.1 Show that the function w = 1/z maps the straight line y = a in
the z-plane onto a circle in the w-plane with radius 1/(2|a|) and center
(0,1/(2a)).
</p>
<p>10.2 Treating x and y as functions of z and z&lowast;,
</p>
<p>(a) use the chain rule to find &part;f/&part;z&lowast; and &part;f/&part;z in terms of partial deriva-
tives with respect to x and y.
</p>
<p>(b) Evaluate &part;f/&part;z&lowast; and &part;f/&part;z assuming that the Cauchy-Riemann con-
ditions hold.
</p>
<p>10.3 Show that when z is represented by polar coordinates, the derivative
of a function f (z) can be written as
</p>
<p>df
</p>
<p>dz
= e&minus;iθ
</p>
<p>(
&part;U
</p>
<p>&part;r
+ i &part;V
</p>
<p>&part;r
</p>
<p>)
,
</p>
<p>where U and V are the real and imaginary parts of f (z) written in polar
coordinates. What are the C-R conditions in polar coordinates? Hint: Start
with the C-R conditions in Cartesian coordinates and apply the chain rule to
them using r =
</p>
<p>&radic;
x2 + y2 and θ = tan&minus;1(y/x)= cos&minus;1(x/
</p>
<p>&radic;
x2 + y2).</p>
<p/>
</div>
<div class="page"><p/>
<p>10.7 Problems 331
</p>
<p>10.4 Show that d
dz
(ln z) = 1
</p>
<p>z
. Hint: Find u(x, y) and v(x, y) for ln z and
</p>
<p>differentiate them.
</p>
<p>10.5 Show that sin z and cos z have only real roots.
</p>
<p>10.6 Show that
</p>
<p>(a) the sum and the product of two entire functions are entire, and
(b) the ratio of two entire functions is analytic everywhere except at the
</p>
<p>zeros of the denominator.
</p>
<p>10.7 Given that u = 2λ ln[(x2 + y2)1/2], show that v = 2λ tan&minus;1(y/x),
where u and v are the real and imaginary parts of an analytic function w(z).
</p>
<p>10.8 If w(z) is any complex potential, show that its (complex) derivative
gives the components of the electric field.
</p>
<p>10.9 Show that
</p>
<p>(a) the flux through an element of area da of the lateral surface of a cylin-
der (with arbitrary cross section) is dφ = dz(|E|ds) where ds is an
arc length along the equipotential surface.
</p>
<p>(b) Prove that |E| = |dw/dz| = &part;v/&part;s where v is the imaginary part of
the complex potential, and s is the parameter describing the length
along the equipotential curves.
</p>
<p>(c) Combine (a) and (b) to get
</p>
<p>flux per unit z-length = φ
z2 &minus; z1
</p>
<p>= v(P2)&minus; v(P1)
</p>
<p>for any two points P1 and P2 on the cross-sectional curve of the lat-
eral surface. Conclude that the total flux per unit z-length through a
cylinder (with arbitrary cross section) is [v], the total change in v as
one goes around the curve.
</p>
<p>(d) Using Gauss&rsquo;s law, show that the capacitance per unit length for the
capacitor consisting of the two conductors with potentials u1 and u2
is
</p>
<p>c&equiv; charge per unit length
potential difference
</p>
<p>= [v]/4π|u2 &minus; u1|
.
</p>
<p>10.10 Using Eq. (10.7)
</p>
<p>(a) find the equipotential curves (curves of constant u) and curves of con-
stant v for two line charges of equal magnitude and opposite signs
located at y = a and y =&minus;a in the xy-plane.
</p>
<p>(b) Show that
</p>
<p>z= a
(
</p>
<p>sin
v
</p>
<p>2λ
+ i sinh u
</p>
<p>2λ
</p>
<p>)/(
cosh
</p>
<p>u
</p>
<p>2λ
&minus; cos v
</p>
<p>2λ
</p>
<p>)
</p>
<p>by solving Eq. (10.7) for z and simplifying.</p>
<p/>
</div>
<div class="page"><p/>
<p>332 10 Complex Calculus
</p>
<p>(c) Show that the equipotential curves are circles in the xy-plane of
radii a/ sinh(u/2λ) with centers at (0, a coth(u/2λ)), and that the
curves of constant v are circles of radii a/ sin(v/2λ) with centers at
(a cot(v/2λ),0).
</p>
<p>10.11 In this problem, you will find the capacitance per unit length of two
cylindrical conductors of radii R1 and R2 the distance between whose cen-
ters is D by looking for two line charge densities +λ and &minus;λ such that the
two cylinders are two of the equipotential surfaces. From Problem 10.10,
we have
</p>
<p>Ri =
a
</p>
<p>sinh(ui/2λ)
, yi = a coth(ui/2λ), i = 1,2,
</p>
<p>where y1 and y2 are the locations of the centers of the two conductors on
the y-axis (which we assume to connect the two centers).
</p>
<p>(a) Show that D = |y1 &minus; y2| = |R1 cosh u12λ &minus;R2 cosh
u2
2λ |.
</p>
<p>(b) Square both sides and use cosh(a &minus; b)= cosha coshb&minus; sinha sinhb
and the expressions for the R&rsquo;s and the y&rsquo;s given above to obtain
</p>
<p>cosh
</p>
<p>(
u1 &minus; u2
</p>
<p>2λ
</p>
<p>)
=
∣∣∣∣
R21 +R22 &minus;D2
</p>
<p>2R1R2
</p>
<p>∣∣∣∣.
</p>
<p>(c) Now find the capacitance per unit length. Consider the special case of
two concentric cylinders.
</p>
<p>(d) Find the capacitance per unit length of a cylinder and a plane, by let-
ting one of the radii, say R1, go to infinity while h&equiv;R1 &minus;D remains
fixed.
</p>
<p>10.12 Use Equations (10.4) and (10.5) to establish the following identities.
</p>
<p>(a) Re(sin z)= sinx coshy, (b) Im(sin z)= cosx sinhy,
(c) Re(cos z)= cosx coshy, (d) Im(cos z)=&minus; sinx sinhy,
(e) Re(sinh z)= sinhx cosy, (f) Im(sinh z)= coshx siny,
(g) Re(cosh z)= coshx cosy, (h) Im(cosh z)= sinhx siny,
(i) | sin z|2 = sin2 x + sinh2 y, (j) | cos z|2 = cos2 x + sinh2 y,
(k) | sinh z|2 = sinh2 x + sin2 y, (l) | cosh z|2 = sinh2 x + cos2 y.
</p>
<p>10.13 Find all the zeros of sinh z and cosh z.
</p>
<p>10.14 Verify the following hyperbolic identities.
</p>
<p>(a) cosh2 z&minus; sinh2 z= 1.
(b) cosh(z1 + z2)= cosh z1 cosh z2 + sinh z1 sinh z2.
(c) sinh(z1 + z2)= sin z1 cosh z2 + cosh z1 sinh z2.
(d) cosh 2z= cosh2 z+ sinh2 z, sinh 2z= 2 sinh z cosh z.
</p>
<p>(e) tanh(z1 + z2)=
tanh z1 + tanh z2
</p>
<p>1 + tanh z1 tanh z2
.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.7 Problems 333
</p>
<p>10.15 Show that
</p>
<p>(a) tanh
</p>
<p>(
z
</p>
<p>2
</p>
<p>)
= sinhx + i siny
</p>
<p>coshx + cosy , (b) coth
(
z
</p>
<p>2
</p>
<p>)
= sinhx &minus; i siny
</p>
<p>coshx &minus; cosy .
</p>
<p>10.16 Find all values of z such that
</p>
<p>(a) ez =&minus;3, (b) ez = 1 + i
&radic;
</p>
<p>3, (c) e2z&minus;1 = 1.
</p>
<p>10.17 Show that |e&minus;z|&lt; 1 if and only if Re(z) &gt; 0.
</p>
<p>10.18 Show that each of the following functions&mdash;call each one u(x, y)&mdash;
is harmonic, and find the function&rsquo;s harmonic partner, v(x, y), such that
u(x, y)+ iv(x, y) is analytic.
</p>
<p>(a) x3 &minus; 3xy2; (b) ex cosy;
</p>
<p>(c)
x
</p>
<p>x2 + y2 , where x
2 + y2 �= 0;
</p>
<p>(d) e&minus;2y cos 2x; (e) ey2&minus;x2 cos 2xy;
</p>
<p>(f) ex(x cosy &minus; y siny)+ 2 sinhy sinx + x3 &minus; 3xy2 + y.
</p>
<p>10.19 Prove the following identities.
</p>
<p>(a) cos&minus;1 z=&minus;i ln
(
z&plusmn;
</p>
<p>&radic;
z2 &minus; 1
</p>
<p>)
,
</p>
<p>(b) sin&minus;1 z=&minus;i ln
[
iz&plusmn;
</p>
<p>&radic;
1 &minus; z2
</p>
<p>]
,
</p>
<p>(c) tan&minus;1 z= 1
2i
</p>
<p>ln
</p>
<p>(
i &minus; z
i + z
</p>
<p>)
,
</p>
<p>(d) cosh&minus;1 z= ln
(
z&plusmn;
</p>
<p>&radic;
z2 &minus; 1
</p>
<p>)
,
</p>
<p>(e) sinh&minus;1 z= ln
(
z&plusmn;
</p>
<p>&radic;
z2 + 1
</p>
<p>)
,
</p>
<p>(f) tanh&minus;1 z= 1
2
</p>
<p>ln
</p>
<p>(
1 + z
1 &minus; z
</p>
<p>)
.
</p>
<p>10.20 Find the curve defined by each of the following equations.
</p>
<p>(a) z= 1 &minus; it, 0 &le; t &le; 2,
(b) z= t + it2, &minus;&infin;&lt; t &lt;&infin;,
</p>
<p>(c) z= a(cos t + i sin t), π
2
&le; t &le; 3π
</p>
<p>2
,
</p>
<p>(d) z= t + i
t
, &minus;&infin;&lt; t &lt; 0.
</p>
<p>10.21 Prove part (a) of Proposition 10.3.1. Hint: A small displacement
along γi can be written as êx�xi + êy�yi for i = 1,2. Find a unit vector
along each displacement and take the dot product of the two. Do the same</p>
<p/>
</div>
<div class="page"><p/>
<p>334 10 Complex Calculus
</p>
<p>for γ &prime;i ; use the C-R condition to show that the two are equal. Prove part (b)
</p>
<p>by showing that if f (z)= z&prime; = x&prime; + iy&prime; is analytic and &part;2Φ
&part;x2
</p>
<p>+ &part;2Φ
&part;y2
</p>
<p>= 0, then
&part;2Φ
</p>
<p>&part;x&prime;2 +
&part;2Φ
</p>
<p>&part;y&prime;2 = 0.
</p>
<p>10.22 Let f (t)= u(t)+ iv(t) be a (piecewise) continuous complex-valued
function of a real variable t defined in the interval a &le; t &le; b. Show that if
F(t)=U(t)+ iV (t) is a function such that dF/dt = f (t), then
</p>
<p>&int; b
</p>
<p>a
</p>
<p>f (t) dt = F(b)&minus; F(a).
</p>
<p>This is the fundamental theorem of calculus for complex variables.
</p>
<p>10.23 Find the value of the integral
&int;
C
[(z+ 2)/z]dz, where C is
</p>
<p>(a) the semicircle z= 2eiθ , for 0 &le; θ &le; π ,
(b) the semicircle z= 2eiθ , for π &le; θ &le; 2π , and
(c) the circle z= 2eiθ , for &minus;π &le; θ &le; π .
</p>
<p>10.24 Evaluate the integral
&int;
γ
dz/(z&minus; 1 &minus; i) where γ is
</p>
<p>(a) the line joining z1 = 2i and z2 = 3, and
(b) the broken path from z1 to the origin and from there to z2.
</p>
<p>10.25 Evaluate the integral
&int;
C
zm(z&lowast;)ndz, where m and n are integers and
</p>
<p>C is the circle |z| = 1 taken counterclockwise.
</p>
<p>10.26 Let C be the boundary of the square with vertices at the points z= 0,
z= 1, z= 1 + i, and z= i with counterclockwise direction. Evaluate
</p>
<p>∮
</p>
<p>C
</p>
<p>(5z+ 2) dz and
∮
</p>
<p>C
</p>
<p>eπz
&lowast;
dz.
</p>
<p>10.27 Use the definition of an integral as the limit of a sum and the fact that
absolute value of a sum is less than or equal to the sum of absolute values to
prove the Darboux inequality.
</p>
<p>10.28 Let C1 be a simple closed contour. Deform C1 into a new contour
C2 in such a way that C1 does not encounter any singularity of an analytic
function f in the process. Show that
</p>
<p>∮
</p>
<p>C1
</p>
<p>f (z) dz=
∮
</p>
<p>C2
</p>
<p>f (z) dz.
</p>
<p>That is, the contour can always be deformed into simpler shapes (such as a
circle) and the integral evaluated.
</p>
<p>10.29 Use the result of the previous problem to show that
∮
</p>
<p>C
</p>
<p>dz
</p>
<p>z&minus; 1 &minus; i = 2πi and
∮
</p>
<p>C
</p>
<p>(z&minus;1&minus; i)m&minus;1dz= 0 for m=&plusmn;1,&plusmn;2, . . .</p>
<p/>
</div>
<div class="page"><p/>
<p>10.7 Problems 335
</p>
<p>when C is the boundary of a square with vertices at z= 0, z= 2, z= 2+2i,
and z= 2i, taken counterclockwise.
</p>
<p>10.30 Use Eq. (10.12) and the binomial expansion to show that d
dz
(zm) =
</p>
<p>mzm&minus;1.
</p>
<p>10.31 Evaluate
∮
C
dz/(z2&minus;1) where C is the circle |z| = 3 integrated in the
</p>
<p>positive sense. Hint: Deform C into a contour C&prime; that avoids the singularities
of the integrand. Then use Cauchy Goursat theorem.
</p>
<p>10.32 Show that when f is analytic within and on a simple closed contour
C and z0 is not on C, then
</p>
<p>∮
</p>
<p>C
</p>
<p>f &prime;(z) dz
z&minus; z0
</p>
<p>=
∮
</p>
<p>C
</p>
<p>f (z) dz
</p>
<p>(z&minus; z0)2
.
</p>
<p>10.33 Let C be the boundary of a square whose sides lie along the lines
x =&plusmn;3 and y =&plusmn;3. For the positive sense of integration, evaluate each of
the following integrals.
</p>
<p>(a)
∮
</p>
<p>C
</p>
<p>e&minus;zdz
z&minus; iπ/2 , (b)
</p>
<p>∮
</p>
<p>C
</p>
<p>ezdz
</p>
<p>z(z2 + 10) ,
</p>
<p>(c)
∮
</p>
<p>C
</p>
<p>cos zdz
</p>
<p>(z&minus; π4 )(z2 &minus; 10)
, (d)
</p>
<p>∮
</p>
<p>C
</p>
<p>sinh zdz
</p>
<p>z4
,
</p>
<p>(e)
∮
</p>
<p>C
</p>
<p>cosh zdz
</p>
<p>z4
, (f)
</p>
<p>∮
</p>
<p>C
</p>
<p>cos zdz
</p>
<p>z3
,
</p>
<p>(g)
∮
</p>
<p>C
</p>
<p>cos zdz
</p>
<p>(z&minus; iπ/2)2 , (h)
∮
</p>
<p>C
</p>
<p>ezdz
</p>
<p>(z&minus; iπ)2 ,
</p>
<p>(i)
∮
</p>
<p>C
</p>
<p>cos zdz
</p>
<p>z+ iπ , (j)
∮
</p>
<p>C
</p>
<p>ezdz
</p>
<p>z2 &minus; 5z+ 4 ,
</p>
<p>(k)
∮
</p>
<p>C
</p>
<p>sinh zdz
</p>
<p>(z&minus; iπ/2)2 , (l)
∮
</p>
<p>C
</p>
<p>cosh zdz
</p>
<p>(z&minus; iπ/2)2 ,
</p>
<p>(m)
∮
</p>
<p>C
</p>
<p>tan zdz
</p>
<p>(z&minus; α)2 , for &minus;3 &lt; α &lt; 3, (n)
∮
</p>
<p>C
</p>
<p>z2dz
</p>
<p>(z&minus; 2)(z2 &minus; 10) .
</p>
<p>10.34 Let C be the circle |z&minus; i| = 3 integrated in the positive sense. Find
the value of each of the following integrals.
</p>
<p>(a)
∮
</p>
<p>C
</p>
<p>ez
</p>
<p>z2 + π2 dz, (b)
∮
</p>
<p>C
</p>
<p>sinh z
</p>
<p>(z2 + π2)2 dz,
</p>
<p>(c)
∮
</p>
<p>C
</p>
<p>dz
</p>
<p>z2 + 9 , (d)
∮
</p>
<p>C
</p>
<p>dz
</p>
<p>(z2 + 9)2 ,
</p>
<p>(e)
∮
</p>
<p>C
</p>
<p>cosh z
</p>
<p>(z2 + π2)3 dz, (f)
∮
</p>
<p>C
</p>
<p>z2 &minus; 3z+ 4
z2 &minus; 4z+ 3 dz.</p>
<p/>
</div>
<div class="page"><p/>
<p>336 10 Complex Calculus
</p>
<p>10.35 Show that Legendre polynomials (for |x|&lt; 1) can be represented as
</p>
<p>Pn(x)=
(&minus;1)n
</p>
<p>2n(2πi)
</p>
<p>∮
</p>
<p>C
</p>
<p>(1 &minus; z2)n
(z&minus; x)n+1 dz,
</p>
<p>where C is the unit circle around the origin.
</p>
<p>10.36 Let f be analytic within and on the circle γ0 given by |z&minus; z0| = r0
and integrated in the positive sense. Show that Cauchy&rsquo;s inequality holds:Cauchy&rsquo;s inequality
</p>
<p>∣∣f (n)(z0)
∣∣&le; n!M
</p>
<p>rn0
,
</p>
<p>where M is the maximum value of |f (z)| on γ0.
</p>
<p>10.37 Expand sinh z in a Taylor series about the point z= iπ .
</p>
<p>10.38 What is the largest circle within which the Maclaurin series for tanh z
converges to tanh z?
</p>
<p>10.39 Find the (unique) Laurent expansion of each of the following func-
tions about the origin for its entire region of analyticity.
</p>
<p>(a)
1
</p>
<p>(z&minus; 2)(z&minus; 3) ; (b) z cos(z
2); (c) 1
</p>
<p>z2(1 &minus; z) ;
</p>
<p>(d)
sinh z&minus; z
</p>
<p>z4
; (e) 1
</p>
<p>(1 &minus; z)3 ; (f)
1
</p>
<p>z2 &minus; 1 ;
</p>
<p>(g)
z2 &minus; 4
z2 &minus; 9 ; (h)
</p>
<p>1
</p>
<p>(z2 &minus; 1)2 ; (i)
z
</p>
<p>z&minus; 1 .
</p>
<p>10.40 Show that the following functions are entire.
</p>
<p>(a) f (z)=
{
</p>
<p>e2z&minus;1
z2
</p>
<p>&minus; 2
z
</p>
<p>for z �= 0,
2 for z= 0.
</p>
<p>(b) f (z)=
{
</p>
<p>sin z
z
</p>
<p>for z �= 0,
1 for z= 0.
</p>
<p>(c) f (z)=
{
</p>
<p>cos z
z2&minus;π2/4 for z �= &plusmn;π/2,
&minus;1/π for z=&plusmn;π/2.
</p>
<p>10.41 Let f be analytic at z0 and f (z0) = f &prime;(z0) = &middot; &middot; &middot; = f (k)(z0) = 0.
Show that the following function is analytic at z0:
</p>
<p>g(z)=
</p>
<p>⎧
⎨
⎩
</p>
<p>f (z)
</p>
<p>(z&minus;z0)k+1 for z �= z0,
f (k+1)(z0)
(k+1)! for z= z0.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.7 Problems 337
</p>
<p>10.42 Obtain the first few nonzero terms of the Laurent series expansion of
each of the following functions about the origin. Also find the integral of the
function along a small simple closed contour encircling the origin.
</p>
<p>(a)
1
</p>
<p>sin z
; (b) 1
</p>
<p>1 &minus; cos z ; (c)
z
</p>
<p>1 &minus; cosh z ;
</p>
<p>(d)
z2
</p>
<p>z&minus; sin z ; (e)
z4
</p>
<p>6z+ z3 &minus; 6 sinh z ; (f)
1
</p>
<p>z2 sin z
;
</p>
<p>(g)
1
</p>
<p>ez &minus; 1 .</p>
<p/>
</div>
<div class="page"><p/>
<p>11Calculus of Residues
</p>
<p>One of the most powerful tools made available by complex analysis is the
theory of residues, which makes possible the routine evaluation of certain
definite integrals that are impossible to calculate otherwise. The derivation,
application, and analysis of this tool constitute the main focus of this chap-
ter. In the preceding chapter we saw examples in which integrals were re-
lated to expansion coefficients of Laurent series. Here we will develop a
systematic way of evaluating both real and complex integrals.
</p>
<p>11.1 Residues
</p>
<p>Recall that a singular point z0 of f : C&rarr; C is a point at which f fails to
be analytic. If in addition, there is some neighborhood of z0 in which f is
analytic at every point (except of course at z0 itself), then z0 is called an
isolated singularity of f . Almost all the singularities we have encountered isolated singularity
so far have been isolated singularities. However, we will see later&mdash;when
discussing multivalued functions&mdash;that singularities that are not isolated do
exist.
</p>
<p>Let z0 be an isolated singularity of f . Then there exists an r &gt; 0 such
that within the &ldquo;annular&rdquo; region 0 &lt; |z &minus; z0| &lt; r , the function f has the
Laurent expansion
</p>
<p>f (z)=
&infin;&sum;
</p>
<p>n=&minus;&infin;
an(z&minus; z0)n &equiv;
</p>
<p>&infin;&sum;
</p>
<p>n=0
an(z&minus; z0)n +
</p>
<p>b1
</p>
<p>z&minus; z0
+ b2
</p>
<p>(z&minus; z0)2
+ &middot; &middot; &middot;
</p>
<p>where
</p>
<p>an =
1
</p>
<p>2πi
</p>
<p>˛
</p>
<p>C
</p>
<p>f (ξ) dξ
</p>
<p>(ξ &minus; z0)n+1
and bn =
</p>
<p>1
</p>
<p>2πi
</p>
<p>˛
</p>
<p>C
</p>
<p>f (ξ)(ξ &minus; z0)n&minus;1 dξ.
</p>
<p>In particular,
</p>
<p>b1 =
1
</p>
<p>2πi
</p>
<p>˛
</p>
<p>C
</p>
<p>f (ξ) dξ, (11.1)
</p>
<p>where C is any simple closed contour around z0, traversed in the positive
sense, on and interior to which f is analytic except at the point z0 itself.
The complex number b1, which is essentially the integral of f (z) along the
</p>
<p>S. Hassani, Mathematical Physics, DOI 10.1007/978-3-319-01195-0_11,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>339</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-01195-0_11">http://dx.doi.org/10.1007/978-3-319-01195-0_11</a></div>
</div>
<div class="page"><p/>
<p>340 11 Calculus of Residues
</p>
<p>contour, is called the residue of f at the isolated singular point z0. It isresidue defined
important to note that the residue is independent of the contour C as long as
z0 is the only isolated singular point within C.
</p>
<p>Historical Notes
</p>
<p>Pierre Alphonse Laurent (1813&ndash;1854) graduated from the Ecole Polytechnique near
the top of his class and became a second lieutenant in the engineering corps. On his
return from the war in Algeria, he took part in the effort to improve the port at Le Havre,
spending six years there directing various parts of the project. Laurent&rsquo;s superior officers
admired the breadth of his practical experience and the good judgment it afforded the
young engineer. During this period he wrote his first scientific paper, on the calculus of
variations, and submitted it to the French Academy of Sciences for the grand prix in
mathematics. Unfortunately the competition had already closed (although the judges had
not yet declared a winner), and Laurent&rsquo;s submission was not successful. However, the
paper so impressed Cauchy that he recommended its publication, also without success.
The paper for which Laurent is most well known suffered a similar fate. In it he described
a more general form of a theorem earlier proven by Cauchy for the power series expan-
sion of a function. Laurent realized that one could generalize this result to hold in any
annular region between two singular or discontinuous points by using both positive and
negative powers in the series, thus allowing treatment of regions beyond the first singular
or discontinuous point. Again, Cauchy argued for the paper&rsquo;s publication without success.
The passage of time provided a more just reward, however, and the use of Laurent series
became a fundamental tool in complex analysis.
Laurent later worked in the theory of light waves and contended with Cauchy over the in-
terpretation of the differential equations the latter had formulated to explain the behavior
of light. Little came of his work in this area, however, and Laurent died at the age of forty-
two, a captain serving on the committee on fortifications in Paris. His widow pressed to
have two more of his papers read to the Academy, only one of which was published.
</p>
<p>We use the notation Res[f (z0)] to denote the residue of f at the isolated
singular point z0. Equation (11.1) can then be written as
</p>
<p>˛
</p>
<p>C
</p>
<p>f (z) dz= 2πi Res
[
f (z0)
</p>
<p>]
.
</p>
<p>What if there are several isolated singular points within the simple closed
contour C? The following theorem provides the answer.
</p>
<p>residue theorem Theorem 11.1.1 (The residue theorem) Let C be a positively oriented
simple closed contour within and on which a function f is analytic
except at a finite number of isolated singular points z1, z2, . . . , zm
interior to C. Then
</p>
<p>˛
</p>
<p>C
</p>
<p>f (z) dz= 2πi
m&sum;
</p>
<p>k=1
Res
</p>
<p>[
f (zk)
</p>
<p>]
. (11.2)
</p>
<p>Proof Let Ck be the positively traversed circle around zk . Then Fig. 11.1
and the Cauchy-Goursat theorem yield
</p>
<p>0 =
˛
</p>
<p>C&prime;
f (z) dz=&minus;
</p>
<p>˛
</p>
<p>circles
f (z) dz+
</p>
<p>˛
</p>
<p>lines
f (z) dz+
</p>
<p>˛
</p>
<p>C
</p>
<p>f (z) dz,</p>
<p/>
</div>
<div class="page"><p/>
<p>11.1 Residues 341
</p>
<p>Fig. 11.1 Singularities are avoided by going around them
</p>
<p>where C&prime; is the union of all the contours, and the minus sign on the first
integral is due to the fact that the interiors of all circles lie to our right as we
traverse their boundaries. The two equal an opposite contributions of each
line cancel out, and we obtain
</p>
<p>˛
</p>
<p>C
</p>
<p>f (z) dz=
m&sum;
</p>
<p>k=1
</p>
<p>˛
</p>
<p>Ck
</p>
<p>f (z) dz=
m&sum;
</p>
<p>k=1
2πi Res
</p>
<p>[
f (zk)
</p>
<p>]
,
</p>
<p>where in the last step the definition of residue at zk has been used. �
</p>
<p>Example 11.1.2 Let us evaluate the integral
&cedil;
</p>
<p>C
(2z&minus;3) dz/[z(z&minus;1)] where
</p>
<p>C is the circle |z| = 2. There are two isolated singularities in C, z1 = 0 and
z2 = 1. To find Res[f (z1)], we expand around the origin:
</p>
<p>2z&minus; 3
z(z&minus; 1) =
</p>
<p>3
</p>
<p>z
&minus; 1
</p>
<p>z&minus; 1 =
3
</p>
<p>z
+ 1
</p>
<p>1 &minus; z =
3
</p>
<p>z
+ 1 + z+ &middot; &middot; &middot; for |z|&lt; 1.
</p>
<p>This gives Res[f (z1)] = 3. Similarly, expanding around z= 1 gives
</p>
<p>2z&minus; 3
z(z&minus; 1) =
</p>
<p>3
</p>
<p>z&minus; 1 + 1 &minus;
1
</p>
<p>z&minus; 1 =&minus;
1
</p>
<p>z&minus; 1 + 3
&infin;&sum;
</p>
<p>k=0
(&minus;1)n(z&minus; 1)n,
</p>
<p>which yields Res[f (z2)] = &minus;1. Thus,
</p>
<p>˛
</p>
<p>C
</p>
<p>2z&minus; 3
z(z&minus; 1) dz= 2πi
</p>
<p>{
Res
</p>
<p>[
f (z1)
</p>
<p>]
+ Res
</p>
<p>[
f (z2)
</p>
<p>]}
= 2πi(3 &minus; 1)= 4πi.</p>
<p/>
</div>
<div class="page"><p/>
<p>342 11 Calculus of Residues
</p>
<p>11.2 Classification of Isolated Singularities
</p>
<p>Let f : C &rarr; C have an isolated singularity at z0. Then there exist a real
number r &gt; 0 and an annular region 0 &lt; |z &minus; z0| &lt; r such that f can be
represented by the Laurent series
</p>
<p>f (z)=
&infin;&sum;
</p>
<p>n=0
an(z&minus; z0)n +
</p>
<p>&infin;&sum;
</p>
<p>n=1
</p>
<p>bn
</p>
<p>(z&minus; z0)n
. (11.3)
</p>
<p>The second sum in Eq. (11.3), involving negative powers of (z &minus; z0), is
called the principal part of f at z0. We can use the principal part to distin-principal part of a
</p>
<p>function guish three types of isolated singularities. The behavior of the function near
the isolated singularity is fundamentally different in each case.
</p>
<p>1. If bn = 0 for all n &ge; 1, z0 is called a removable singular point of f .removable singular point
In this case, the Laurent series contains only nonnegative powers of
(z&minus; z0), and setting f (z0)= a0 makes the function analytic at z0. For
example, the function f (z)= (ez &minus; 1 &minus; z)/z2, which is indeterminate
at z= 0, becomes entire if we set f (0)= 12 , because its Laurent series
f (z)= 12 + z3! + z
</p>
<p>2
</p>
<p>4! + &middot; &middot; &middot; has no negative power.
2. If bn = 0 for all n &gt; m and bm �= 0, z0 is called a pole of order m. Inpoles defined
</p>
<p>this case, the expansion takes the form
</p>
<p>f (z)=
&infin;&sum;
</p>
<p>n=0
an(z&minus; z0)n +
</p>
<p>b1
</p>
<p>z&minus; z0
+ &middot; &middot; &middot; + bm
</p>
<p>(z&minus; z0)m
</p>
<p>for 0 &lt; |z&minus; z0|&lt; r . In particular, if m= 1, z0 is called a simple pole.simple pole
3. If the principal part of f at z0 has an infinite number of nonzero terms,
</p>
<p>the point z0 is called an essential singularity. A prototype of functionsessential singularity
that have essential singularities is
</p>
<p>exp
</p>
<p>(
1
</p>
<p>z
</p>
<p>)
=
</p>
<p>&infin;&sum;
</p>
<p>n=0
</p>
<p>1
</p>
<p>n!
</p>
<p>(
1
</p>
<p>zn
</p>
<p>)
,
</p>
<p>which has an essential singularity at z= 0 and a residue of 1 there. To
see how strange such functions are, we let a be any real number, andstrange behavior of
</p>
<p>functions with essential
</p>
<p>singularity
</p>
<p>consider z = 1/(lna + 2nπi) for n= 0,&plusmn;1,&plusmn;2, . . . . For such a z we
have e1/z = elna+2nπi = ae2nπi = a. In particular, as n&rarr;&infin;, z gets ar-
bitrarily close to the origin. Thus, in an arbitrarily small neighborhood
of the origin, there are infinitely many points at which the function
exp(1/z) takes on an arbitrary value a. In other words, as z &rarr; 0, the
function gets arbitrarily close to any real number! This result holds for
all functions with essential singularities.
</p>
<p>Example 11.2.1 (Order of poles)
</p>
<p>(a) The function (z2 &minus; 3z+ 5)/(z&minus; 1) has a Laurent series around z= 1
containing only three terms:
</p>
<p>z2 &minus; 3z+ 5
z&minus; 1 =&minus;1 + (z&minus; 1)+
</p>
<p>3
</p>
<p>z&minus; 1 .</p>
<p/>
</div>
<div class="page"><p/>
<p>11.2 Classification of Isolated Singularities 343
</p>
<p>Thus, it has a simple pole at z= 1, with a residue of 3.
(b) The function sin z/z6 has a Laurent series
</p>
<p>sin z
</p>
<p>z6
= 1
</p>
<p>z6
</p>
<p>&infin;&sum;
</p>
<p>n=0
(&minus;1)n z
</p>
<p>2n+1
</p>
<p>(2n+ 1)! =
1
</p>
<p>z5
&minus; 1
</p>
<p>6z3
+ 1
</p>
<p>(5!)z &minus;
z
</p>
<p>7! + &middot; &middot; &middot;
</p>
<p>about z= 0. The principal part has three terms. The pole, at z= 0, is
of order 5, and the function has a residue of 1/120 at z= 0.
</p>
<p>(c) The function (z2 &minus; 5z + 6)/(z &minus; 2) has a removable singularity at
z= 2, because
</p>
<p>z2 &minus; 5z+ 6
z&minus; 2 =
</p>
<p>(z&minus; 2)(z&minus; 3)
z&minus; 2 = z&minus; 3 =&minus;1 + (z&minus; 2)
</p>
<p>and bn = 0 for all n.
</p>
<p>Many functions can be written as the ratio of two polynomials. A func-
tion of this form is called a rational function. If the degree of the numerator rational function
is larger than the denominator, the ratio can be written as a polynomial plus
a rational function the degree of whose numerator is not larger than that
of the denominator. When we talk about rational functions, we exclude the
polynomials. So, we assume that the degree of the numerator is less than or
equal to the degree of the denominator. Such rational functions f have the
property that as z goes to infinity, f does not go to infinity. Stated equiva-
lently, f (1/z) does not go to infinity at the origin.
</p>
<p>Let f be a function whose only singularities in the entire complex plane
are finite poles, i.e., the point at infinity is not a pole of the function. This
means that f (1/z) does not have a pole at the origin. Let {zj }nj=1 be the
poles of f such that zj is of order mj . Expand the function about z1 in a
Laurent series
</p>
<p>f (z)= b1
z&minus; z1
</p>
<p>+ &middot; &middot; &middot;+ bm1
(z&minus; z1)m1
</p>
<p>+
&infin;&sum;
</p>
<p>k=0
ak(z&minus; z1)k &equiv;
</p>
<p>P1(z)
</p>
<p>(z&minus; z1)m1
+ g1(z),
</p>
<p>where
</p>
<p>P1(z)&equiv; b1(z&minus; z1)m1&minus;1 + b2(z&minus; z1)m1&minus;2 + &middot; &middot; &middot; + bm1&minus;1(z&minus; z1)+ bm1
</p>
<p>is a polynomial of degree m1 &minus; 1 in z and g1 is analytic at z1. It should
be clear that the remaining poles of f are in g1. So, expand g1 about z2 in
a Laurent series. A similar argument as above yields g1(z) = P2(z)/(z &minus;
z2)
</p>
<p>m2 + g2(z) where P2(z) is a polynomial of degree m2 &minus; 1 in z and g2 is
analytic at z1 and z2. Continuing in this manner, we get
</p>
<p>f (z)= P1(z)
(z&minus; z1)m1
</p>
<p>+ P2(z)
(z&minus; z2)m2
</p>
<p>+ &middot; &middot; &middot; + Pn(z)
(z&minus; zn)mn
</p>
<p>+ g(z),
</p>
<p>where g has no poles. Since all poles of f have been isolated in the sum,
g must be analytic everywhere in C, i.e., an entire function. Now substitute
1/t for z, take the limit t &rarr; 0, and note that, since the degree of Pi is</p>
<p/>
</div>
<div class="page"><p/>
<p>344 11 Calculus of Residues
</p>
<p>mi &minus; 1, all the terms in the preceding equation go to zero except possibly
g(1/t). Moreover,
</p>
<p>lim
t&rarr;0
</p>
<p>g(1/t) �=&infin;,
</p>
<p>because, by assumption, the point at infinity is not a pole of f . Thus, g
is a bounded entire function. By Proposition 10.5.5, g must be a constant.
Taking a common denominator for all the terms yields a ratio of two poly-
nomials. We have proved the following:
</p>
<p>Proposition 11.2.2 A function whose only singularities are poles in
a finite region of the complex plane is a rational function.
</p>
<p>The isolated singularity that is most important in applications is a pole.
For a function that has a pole of order m at z0, the calculation of residues
is routine. Such a calculation, in turn, enables us to evaluate many integrals
effortlessly. How do we calculate the residue of a function f having a pole
of order m at z0?
</p>
<p>It is clear that if f has a pole of order m, then g : C &rarr; C defined by
g(z)&equiv; (z&minus; z0)mf (z) is analytic at z0. Thus, for any simple closed contour
C that contains z0 but no other singular point of f , we have
</p>
<p>Res
[
f (z0)
</p>
<p>]
= 1
</p>
<p>2πi
</p>
<p>˛
</p>
<p>C
</p>
<p>f (z)dz= 1
2πi
</p>
<p>˛
</p>
<p>C
</p>
<p>g(z)dz
</p>
<p>(z&minus; z0)m
= g
</p>
<p>(m&minus;1)(z0)
(m&minus; 1)! .
</p>
<p>In terms of f this yields1
</p>
<p>Theorem 11.2.3 If f (z) has a pole of order m at z0, then
</p>
<p>Res
[
f (z0)
</p>
<p>]
= 1
</p>
<p>(m&minus; 1)! limz&rarr;z0
dm&minus;1
</p>
<p>dzm&minus;1
[
(z&minus; z0)mf (z)
</p>
<p>]
. (11.4)
</p>
<p>For the special, but important, case of a simple pole, we obtain
</p>
<p>Res
[
f (z0)
</p>
<p>]
= lim
</p>
<p>z&rarr;z0
</p>
<p>[
(z&minus; z0)f (z)
</p>
<p>]
. (11.5)
</p>
<p>11.3 Evaluation of Definite Integrals
</p>
<p>The most widespread application of residues occurs in the evaluation of real
definite integrals. It is possible to &ldquo;complexify&rdquo; certain real definite integrals
and relate them to contour integrations in the complex plane. We will discuss
this method shortly; however, we first need a lemma.
</p>
<p>1The limit is taken because in many cases the mere substitution of z0 may result in an
indeterminate form.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.3 Evaluation of Definite Integrals 345
</p>
<p>Lemma 11.3.1 (Jordan&rsquo;s lemma) Let CR be a semicircle of radius R in the Jordan&rsquo;s lemma
upper half of the complex plane (UHP) and centered at the origin. Let f be
a function that tends uniformly to zero faster than 1/|z| for arg(z) &isin; [0,π]
as |z| &rarr;&infin;. Let α be a nonnegative real number. Then
</p>
<p>lim
R&rarr;&infin;
</p>
<p>IR &equiv; lim
R&rarr;&infin;
</p>
<p>ˆ
</p>
<p>CR
</p>
<p>eiαzf (z) dz= 0.
</p>
<p>Proof For z &isin; CR we write z=Reiθ , dz= iReiθdθ , and
</p>
<p>iαz= iα(R cos θ + iR sin θ)= iαR cos θ &minus; αR sin θ
</p>
<p>and substitute in the absolute value of the integral to show that
</p>
<p>|IR| &le;
ˆ π
</p>
<p>0
e&minus;αR sin θR
</p>
<p>∣∣f
(
Reiθ
</p>
<p>)∣∣dθ.
</p>
<p>By assumption, R|f (Reiθ )| &lt; ǫ(R) independent of θ , where ǫ(R) is an
arbitrary positive number that tends to zero as R &rarr;&infin;. By breaking up the
interval of integration into two equal pieces and changing θ to π &minus; θ in the
second integral, one can show that
</p>
<p>|IR|&lt; 2ǫ(R)
ˆ π/2
</p>
<p>0
e&minus;αR sin θdθ.
</p>
<p>Furthermore, by plotting sin θ and 2θ/π on the same graph, one can easily
see that sin θ &ge; 2θ/π for 0 &le; θ &le; π/2. Thus,
</p>
<p>|IR|&lt; 2ǫ(R)
ˆ π/2
</p>
<p>0
e&minus;(2αR/π)θdθ = πǫ(R)
</p>
<p>αR
</p>
<p>(
1 &minus; e&minus;αR
</p>
<p>)
,
</p>
<p>which goes to zero as R gets larger and larger. �
</p>
<p>Note that Jordan&rsquo;s lemma applies for α = 0 as well, because (1 &minus;
e&minus;αR) &rarr; αR as α &rarr; 0. If α &lt; 0, the lemma is still valid if the semicir-
cle CR is taken in the lower half of the complex plane (LHP) and f (z) goes
to zero uniformly for π &le; arg(z)&le; 2π .
</p>
<p>We are now in a position to apply the residue theorem to the evaluation of
definite integrals. The three types of integrals most commonly encountered
are discussed separately below. In all cases we assume that Jordan&rsquo;s lemma
holds.
</p>
<p>11.3.1 Integrals of Rational Functions
</p>
<p>The first type of integral we can evaluate using the residue theorem is of the
form
</p>
<p>I1 =
ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>p(x)
</p>
<p>q(x)
dx,</p>
<p/>
</div>
<div class="page"><p/>
<p>346 11 Calculus of Residues
</p>
<p>where p(x) and q(x) are real polynomials, and q(x) �= 0 for any real x. We
can then write
</p>
<p>I1 = lim
R&rarr;&infin;
</p>
<p>ˆ R
</p>
<p>&minus;R
</p>
<p>p(x)
</p>
<p>q(x)
dx = lim
</p>
<p>R&rarr;&infin;
</p>
<p>ˆ
</p>
<p>Cx
</p>
<p>p(z)
</p>
<p>q(z)
dz,
</p>
<p>where Cx is the (open) contour lying on the real axis from &minus;R to +R. Since
Jordan&rsquo;s lemma holds by assumption, we can close that contour by adding
to it the semicircle of radius R [see Fig. 11.2(a)]. This will not affect the
value of the integral, because in the limit R &rarr;&infin;, the contribution of the
integral of the semicircle tends to zero. We close the contour in the UHP if
q(z) has at least one zero there. We then get
</p>
<p>I1 = lim
R&rarr;&infin;
</p>
<p>˛
</p>
<p>C
</p>
<p>p(z)
</p>
<p>q(z)
dz= 2πi
</p>
<p>k&sum;
</p>
<p>j=1
Res
</p>
<p>[
p(zj )
</p>
<p>q(zj )
</p>
<p>]
,
</p>
<p>where C is the closed contour composed of the interval (&minus;R,R) and the
semicircle CR , and {zj }kj=1 are the zeros of q(z) in the UHP. We may instead
close the contour in the LHP,2 in which case
</p>
<p>I1 =&minus;2πi
k&sum;
</p>
<p>j=1
Res
</p>
<p>[
p(zj )
</p>
<p>q(zj )
</p>
<p>]
,
</p>
<p>where {zj }kj=1 are the zeros of q(z) in the LHP. The minus sign indicates
that in the LHP we (are forced to) integrate clockwise.
</p>
<p>Example 11.3.2 Let us evaluate the integral I =
&acute;&infin;
</p>
<p>0 x
2 dx/[(x2 + 1)(x2 +
</p>
<p>9)]. Since the integrand is even, we can extend the interval of integration to
all real numbers (and divide the result by 2). It is shown below that Jordan&rsquo;s
lemma holds. Therefore, we write the contour integral corresponding to I :
</p>
<p>I = 1
2
</p>
<p>˛
</p>
<p>C
</p>
<p>z2 dz
</p>
<p>(z2 + 1)(z2 + 9) ,
</p>
<p>where C is as shown in Fig. 11.2(a). Note that the contour is traversed in the
positive sense. This is always true for the UHP. The singularities of the func-
tion in the UHP are the simple poles i and 3i corresponding to the simple
zeros of the denominator. The residues at these poles are
</p>
<p>Res
[
f (i)
</p>
<p>]
= lim
</p>
<p>z&rarr;i
(z&minus; i) z
</p>
<p>2
</p>
<p>(z&minus; i)(z+ i)(z2 + 9) =&minus;
1
</p>
<p>16i
,
</p>
<p>Res
[
f (3i)
</p>
<p>]
= lim
</p>
<p>z&rarr;3i
(z&minus; 3i) z
</p>
<p>2
</p>
<p>(z2 + 1)(z&minus; 3i)(z+ 3i) =
3
</p>
<p>16i
.
</p>
<p>2Provided that Jordan&rsquo;s lemma holds there.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.3 Evaluation of Definite Integrals 347
</p>
<p>Fig. 11.2 (a) The large semicircle is chosen in the UHP. (b) Note how the direction of
contour integration is forced to be clockwise when the semicircle is chosen in the LHP
</p>
<p>Thus, we obtain
</p>
<p>I =
ˆ &infin;
</p>
<p>0
</p>
<p>x2 dx
</p>
<p>(x2 + 1)(x2 + 9) =
1
</p>
<p>2
</p>
<p>˛
</p>
<p>C
</p>
<p>z2 dz
</p>
<p>(z2 + 1)(z2 + 9)
</p>
<p>= πi
(
&minus; 1
</p>
<p>16i
+ 3
</p>
<p>16i
</p>
<p>)
= π
</p>
<p>8
.
</p>
<p>It is instructive to obtain the same results using the LHP. In this case,
the contour is as shown in Fig. 11.2(b) and is taken clockwise, so we have
to introduce a minus sign. The singular points are at z = &minus;i and z = &minus;3i.
These are simple poles at which the residues of the function are
</p>
<p>Res
[
f (&minus;i)
</p>
<p>]
= lim
</p>
<p>z&rarr;&minus;i
(z+ i) z
</p>
<p>2
</p>
<p>(z&minus; i)(z+ i)(z2 + 9) =
1
</p>
<p>16i
,
</p>
<p>Res
[
f (&minus;3i)
</p>
<p>]
= lim
</p>
<p>z&rarr;&minus;3i
(z+ 3i) z
</p>
<p>2
</p>
<p>(z2 + 1)(z&minus; 3i)(z+ 3i) =&minus;
3
</p>
<p>16i
.
</p>
<p>Therefore,
</p>
<p>I =
ˆ &infin;
</p>
<p>0
</p>
<p>x2 dx
</p>
<p>(x2 + 1)(x2 + 9) =
1
</p>
<p>2
</p>
<p>˛
</p>
<p>C
</p>
<p>z2 dz
</p>
<p>(z2 + 1)(z2 + 9)
</p>
<p>=&minus;πi
(
</p>
<p>1
</p>
<p>16i
&minus; 3
</p>
<p>16i
</p>
<p>)
= π
</p>
<p>8
.
</p>
<p>To show that Jordan&rsquo;s lemma applies to this integral, we have only to
establish that limR&rarr;&infin;R|f (Reiθ )| = 0. In the case at hand, α = 0 because
there is no exponential function in the integrand. Thus,
</p>
<p>R
∣∣f
</p>
<p>(
Reiθ
</p>
<p>)∣∣=R
∣∣∣∣
</p>
<p>R2e2iθ
</p>
<p>(R2e2iθ + 1)(R2e2iθ + 9)
</p>
<p>∣∣∣∣=
R3
</p>
<p>|R2e2iθ + 1| |R2e2iθ + 9| ,
</p>
<p>which clearly goes to zero as R&rarr;&infin;.
</p>
<p>Example 11.3.3 Let us now consider a slightly more complicated integral:
ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>x2 dx
</p>
<p>(x2 + 1)(x2 + 4)2 ,</p>
<p/>
</div>
<div class="page"><p/>
<p>348 11 Calculus of Residues
</p>
<p>which turns into
&cedil;
</p>
<p>C
z2 dz/[(z2+1)(z2+4)2] as a contour integral. The poles
</p>
<p>in the UHP are at z= i and z= 2i. The former is a simple pole, and the latter
is a pole of order 2. Thus, using Eqs. (11.5) and (11.4), we obtain
</p>
<p>Res
[
f (i)
</p>
<p>]
= lim
</p>
<p>z&rarr;i
(z&minus; i) z
</p>
<p>2
</p>
<p>(z&minus; i)(z+ i)(z2 + 4)2 =&minus;
1
</p>
<p>18i
,
</p>
<p>Res
[
f (2i)
</p>
<p>]
= 1
</p>
<p>(2 &minus; 1)! limz&rarr;2i
d
</p>
<p>dz
</p>
<p>[
(z&minus; 2i)2 z
</p>
<p>2
</p>
<p>(z2 + 1)(z+ 2i)2(z&minus; 2i)2
]
</p>
<p>= lim
z&rarr;2i
</p>
<p>d
</p>
<p>dz
</p>
<p>[
z2
</p>
<p>(z2 + 1)(z+ 2i)2
]
= 5
</p>
<p>72i
,
</p>
<p>and
ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>x2 dx
</p>
<p>(x2 + 1)(x2 + 4)2 = 2πi
(
&minus; 1
</p>
<p>18i
+ 5
</p>
<p>72i
</p>
<p>)
= π
</p>
<p>36
.
</p>
<p>Closing the contour in the LHP would yield the same result.
</p>
<p>11.3.2 Products of Rational and Trigonometric Functions
</p>
<p>The second type of integral we can evaluate using the residue theorem is of
the form
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>p(x)
</p>
<p>q(x)
cosax dx or
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>p(x)
</p>
<p>q(x)
sinax dx,
</p>
<p>where a is a real number, p(x) and q(x) are real polynomials in x, and q(x)
has no real zeros. These integrals are the real and imaginary parts of
</p>
<p>I2 =
ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>p(x)
</p>
<p>q(x)
eiax dx.
</p>
<p>The presence of eiax dictates the choice of the half-plane: If a &ge; 0, we
choose the UHP; otherwise, we choose the LHP. We must, of course, have
enough powers of x in the denominator to render R|p(Reiθ )/q(Reiθ )| uni-
formly convergent to zero.
</p>
<p>Example 11.3.4 Let us evaluate
&acute;&infin;
&minus;&infin;[cosax/(x2 + 1)2]dx where a �= 0.
</p>
<p>This integral is the real part of the integral I2 =
&acute;&infin;
&minus;&infin; e
</p>
<p>iax dx/(x2 + 1)2.
When a &gt; 0, we close in the UHP as advised by Jordan&rsquo;s lemma. Then we
proceed as for integrals of rational functions. Thus, we have
</p>
<p>I2 =
˛
</p>
<p>C
</p>
<p>eiaz
</p>
<p>(z2 + 1)2 dz= 2πi Res
[
f (i)
</p>
<p>]
for a &gt; 0
</p>
<p>because there is only one pole (of order 2) in the UHP at z = i. We next
calculate the residue:
</p>
<p>Res
[
f (i)
</p>
<p>]
= lim
</p>
<p>z&rarr;i
d
</p>
<p>dz
</p>
<p>[
(z&minus; i)2 e
</p>
<p>iaz
</p>
<p>(z&minus; i)2(z+ i)2
]</p>
<p/>
</div>
<div class="page"><p/>
<p>11.3 Evaluation of Definite Integrals 349
</p>
<p>= lim
z&rarr;i
</p>
<p>d
</p>
<p>dz
</p>
<p>[
eiaz
</p>
<p>(z+ i)2
]
= lim
</p>
<p>z&rarr;i
</p>
<p>[
(z+ i)iaeiaz &minus; 2eiaz
</p>
<p>(z+ i)3
]
</p>
<p>= e
&minus;a
</p>
<p>4i
(1 + a).
</p>
<p>Substituting this in the expression for I2, we obtain I2 = π2 e&minus;a(1 + a) for
a &gt; 0.
</p>
<p>When a &lt; 0, we have to close the contour in the LHP, where the pole of
order 2 is at z=&minus;i and the contour is taken clockwise. Thus, we get
</p>
<p>I2 =
˛
</p>
<p>C
</p>
<p>eiaz
</p>
<p>(z2 + 1)2 dz=&minus;2πi Res
[
f (&minus;i)
</p>
<p>]
for a &lt; 0.
</p>
<p>For the residue we obtain
</p>
<p>Res
[
f (&minus;i)
</p>
<p>]
= lim
</p>
<p>z&rarr;&minus;i
d
</p>
<p>dz
</p>
<p>[
(z+ i)2 e
</p>
<p>iaz
</p>
<p>(z&minus; i)2(z+ i)2
]
=&minus;e
</p>
<p>a
</p>
<p>4i
(1 &minus; a),
</p>
<p>and the expression for I2 becomes I2 = π2 ea(1&minus; a) for a &lt; 0. We can com-
bine the two results and write
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>cosax
</p>
<p>(x2 + 1)2 dx = Re(I2)= I2 =
π
</p>
<p>2
</p>
<p>(
1 + |a|
</p>
<p>)
e&minus;|a|.
</p>
<p>Example 11.3.5 As another example, let us evaluate
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>x sinax
</p>
<p>x4 + 4 dx where a �= 0.
</p>
<p>This is the imaginary part of the integral I2 =
&acute;&infin;
&minus;&infin; xe
</p>
<p>iax dx/(x4 + 4),
which, in terms of z and for the closed contour in the UHP (when a &gt; 0),
becomes
</p>
<p>I2 =
˛
</p>
<p>C
</p>
<p>zeiaz
</p>
<p>z4 + 4 dz= 2πi
m&sum;
</p>
<p>j=1
Res
</p>
<p>[
f (zj )
</p>
<p>]
for a &gt; 0. (11.6)
</p>
<p>The singularities are determined by the zeros of the denominator: z4+4 = 0,
or z= 1&plusmn; i,&minus;1&plusmn; i. Of these four simple poles only two, 1+ i and &minus;1+ i,
are in the UHP. We now calculate the residues:
</p>
<p>Res
[
f (1 + i)
</p>
<p>]
</p>
<p>= lim
z&rarr;1+i
</p>
<p>(z&minus; 1 &minus; i) ze
iaz
</p>
<p>(z&minus; 1 &minus; i)(z&minus; 1 + i)(z+ 1 &minus; i)(z+ 1 + i)
</p>
<p>= (1 + i)e
ia(1+i)
</p>
<p>(2i)(2)(2 + 2i) =
eiae&minus;a
</p>
<p>8i
,</p>
<p/>
</div>
<div class="page"><p/>
<p>350 11 Calculus of Residues
</p>
<p>Res
[
f (&minus;1 + i)
</p>
<p>]
</p>
<p>= lim
z&rarr;&minus;1+i
</p>
<p>(z+ 1 &minus; i) ze
iaz
</p>
<p>(z+ 1 &minus; i)(z+ 1 + i)(z&minus; 1 &minus; i)(z&minus; 1 + i)
</p>
<p>= (&minus;1 + i)e
ia(&minus;1+i)
</p>
<p>(2i)(&minus;2)(&minus;2 + 2i) =&minus;
e&minus;iae&minus;a
</p>
<p>8i
.
</p>
<p>Substituting in Eq. (11.6), we obtain
</p>
<p>I2 = 2πi
e&minus;a
</p>
<p>8i
</p>
<p>(
eia &minus; e&minus;ia
</p>
<p>)
= i π
</p>
<p>2
e&minus;a sina.
</p>
<p>Thus,
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>x sinax
</p>
<p>x4 + 4 dx = Im(I2)=
π
</p>
<p>2
e&minus;a sina for a &gt; 0. (11.7)
</p>
<p>For a &lt; 0, we could close the contour in the LHP. But there is an easier
way of getting to the answer. We note that &minus;a &gt; 0, and Eq. (11.7) yields
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>x sinax
</p>
<p>x4 + 4 dx =&minus;
ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>x sin[(&minus;a)x]
x4 + 4 dx
</p>
<p>=&minus;π
2
e&minus;(&minus;a) sin(&minus;a)= π
</p>
<p>2
ea sina.
</p>
<p>We can collect the two cases in
ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>x sinax
</p>
<p>x4 + 4 dx =
π
</p>
<p>2
e&minus;|a| sina.
</p>
<p>11.3.3 Functions of Trigonometric Functions
</p>
<p>The third type of integral we can evaluate using the residue theorem involves
only trigonometric functions and is of the form
</p>
<p>ˆ 2π
</p>
<p>0
F(sin θ, cos θ) dθ,
</p>
<p>where F is some (typically rational) function of its arguments. Since θ
varies from 0 to 2π , we can consider it an argument of a point z on the
unit circle centered at the origin. Then z= eiθ and e&minus;iθ = 1/z, and we can
substitute cos θ = (z+ 1/z)/2, sin θ = (z&minus; 1/z)/(2i), and dθ = dz/(iz) in
the original integral, to obtain
</p>
<p>˛
</p>
<p>C
</p>
<p>F
</p>
<p>(
z&minus; 1/z
</p>
<p>2i
,
z+ 1/z
</p>
<p>2
</p>
<p>)
dz
</p>
<p>iz
.
</p>
<p>This integral can often be evaluated using the method of residues.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.3 Evaluation of Definite Integrals 351
</p>
<p>Example 11.3.6 Let us evaluate the integral
&acute; 2π
</p>
<p>0 dθ/(1 + a cos θ) where
|a|&lt; 1. Substituting for cos θ and dθ in terms of z, we obtain
</p>
<p>˛
</p>
<p>C
</p>
<p>dz/iz
</p>
<p>1 + a[(z2 + 1)/(2z)] =
2
</p>
<p>i
</p>
<p>˛
</p>
<p>C
</p>
<p>dz
</p>
<p>2z+ az2 + a ,
</p>
<p>where C is the unit circle centered at the origin. The singularities of the
integrand are the zeros of its denominator:
</p>
<p>z1 =
&minus;1 +
</p>
<p>&radic;
1 &minus; a2
</p>
<p>a
and z2 =
</p>
<p>&minus;1 &minus;
&radic;
</p>
<p>1 &minus; a2
a
</p>
<p>.
</p>
<p>For |a|&lt; 1 it is clear that z2 will lie outside the unit circle C; therefore, it
does not contribute to the integral. But z1 lies inside, and we obtain
</p>
<p>˛
</p>
<p>C
</p>
<p>dz
</p>
<p>2z+ az2 + a = 2πi Res
[
f (z1)
</p>
<p>]
.
</p>
<p>The residue of the simple pole at z1 can be calculated:
</p>
<p>Res
[
f (z1)
</p>
<p>]
= lim
</p>
<p>z&rarr;z1
(z&minus; z1)
</p>
<p>1
</p>
<p>a(z&minus; z1)(z&minus; z2)
= 1
</p>
<p>a
</p>
<p>(
1
</p>
<p>z1 &minus; z2
</p>
<p>)
</p>
<p>= 1
a
</p>
<p>(
a
</p>
<p>2
&radic;
</p>
<p>1 &minus; a2
</p>
<p>)
= 1
</p>
<p>2
&radic;
</p>
<p>1 &minus; a2
.
</p>
<p>It follows that
</p>
<p>ˆ 2π
</p>
<p>0
</p>
<p>dθ
</p>
<p>1 + a cos θ =
2
</p>
<p>i
</p>
<p>˛
</p>
<p>C
</p>
<p>dz
</p>
<p>2z+ az2 + a =
2
</p>
<p>i
2πi
</p>
<p>(
1
</p>
<p>2
&radic;
</p>
<p>1 &minus; a2
</p>
<p>)
= 2π&radic;
</p>
<p>1 &minus; a2
.
</p>
<p>Example 11.3.7 As another example, let us consider the integral
</p>
<p>I =
ˆ π
</p>
<p>0
</p>
<p>dθ
</p>
<p>(a + cos θ)2 where a &gt; 1.
</p>
<p>Since cos θ is an even function of θ , we may write
</p>
<p>I = 1
2
</p>
<p>ˆ π
</p>
<p>&minus;π
</p>
<p>dθ
</p>
<p>(a + cos θ)2 where a &gt; 1.
</p>
<p>This integration is over a complete cycle around the origin, and we can make
the usual substitution:
</p>
<p>I = 1
2
</p>
<p>˛
</p>
<p>C
</p>
<p>dz/iz
</p>
<p>[a + (z2 + 1)/2z]2 =
2
</p>
<p>i
</p>
<p>˛
</p>
<p>C
</p>
<p>z dz
</p>
<p>(z2 + 2az+ 1)2 .
</p>
<p>The denominator has the roots z1 =&minus;a+
&radic;
a2 &minus; 1 and z2 =&minus;a&minus;
</p>
<p>&radic;
a2 &minus; 1,
</p>
<p>which are both of order 2. The second root is outside the unit circle because
a &gt; 1. Also, it is easily verified that for all a &gt; 1, z1 is inside the unit circle.</p>
<p/>
</div>
<div class="page"><p/>
<p>352 11 Calculus of Residues
</p>
<p>Since z1 is a pole of order 2, we have
</p>
<p>Res
[
f (z1)
</p>
<p>]
= lim
</p>
<p>z&rarr;z1
d
</p>
<p>dz
</p>
<p>[
(z&minus; z1)2
</p>
<p>z
</p>
<p>(z&minus; z1)2(z&minus; z2)2
]
</p>
<p>= lim
z&rarr;z1
</p>
<p>d
</p>
<p>dz
</p>
<p>[
z
</p>
<p>(z&minus; z2)2
]
= 1
</p>
<p>(z1 &minus; z2)2
&minus; 2z1
</p>
<p>(z1 &minus; z2)3
</p>
<p>= a
4(a2 &minus; 1)3/2 .
</p>
<p>We thus obtain I = 2
i
2πi Res[f (z1)] = πa(a2&minus;1)3/2 .
</p>
<p>11.3.4 Some Other Integrals
</p>
<p>The three types of definite integrals discussed above do not exhaust all pos-
sible applications of the residue theorem. There are other integrals that do
not fit into any of the foregoing three categories but are still manageable.
As the next two examples demonstrate, a clever choice of contours allows
evaluation of other types of integrals.
</p>
<p>Example 11.3.8 Let us evaluate the Gaussian integral
</p>
<p>I =
ˆ &infin;
</p>
<p>&minus;&infin;
eiax&minus;bx
</p>
<p>2
dx where a, b &isin;R, b &gt; 0.
</p>
<p>Completing squares in the exponent, we have
</p>
<p>I =
ˆ &infin;
</p>
<p>&minus;&infin;
e&minus;b[x&minus;ia/(2b)]
</p>
<p>2&minus;a2/4bdx = e&minus;a2/4b lim
R&rarr;&infin;
</p>
<p>ˆ R
</p>
<p>&minus;R
e&minus;b[x&minus;ia/(2b)]
</p>
<p>2
dx.
</p>
<p>If we change the variable of integration to z= x &minus; ia/(2b), we obtain
</p>
<p>I = e&minus;a2/(4b) lim
R&rarr;&infin;
</p>
<p>ˆ R&minus;ia/(2b)
</p>
<p>&minus;R&minus;ia/(2b)
e&minus;bz
</p>
<p>2
dz.
</p>
<p>Let us now define IR :
</p>
<p>IR &equiv;
ˆ R&minus;ia/(2b)
</p>
<p>&minus;R&minus;ia/(2b)
e&minus;bz
</p>
<p>2
dz.
</p>
<p>This is an integral along a straight line C1 that is parallel to the x-axis (see
Fig. 11.3). We close the contour as shown and note that e&minus;bz
</p>
<p>2
is analytic
</p>
<p>throughout the interior of the closed contour (it is an entire function!). Thus,
the contour integral must vanish by the Cauchy-Goursat theorem. So we
obtain
</p>
<p>IR +
ˆ
</p>
<p>C3
</p>
<p>e&minus;bz
2
dz+
</p>
<p>ˆ &minus;R
</p>
<p>R
</p>
<p>e&minus;bx
2
dx +
</p>
<p>ˆ
</p>
<p>C4
</p>
<p>e&minus;bz
2
dz= 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.3 Evaluation of Definite Integrals 353
</p>
<p>Fig. 11.3 The contour for the evaluation of the Gaussian integral
</p>
<p>Along C3, z=R+ iy and
ˆ
</p>
<p>C3
</p>
<p>e&minus;bz
2
dz=
</p>
<p>ˆ 0
</p>
<p>&minus;ia/(2b)
e&minus;b(R+iy)
</p>
<p>2
i dy = ie&minus;bR2
</p>
<p>ˆ 0
</p>
<p>&minus;ia/(2b)
eby
</p>
<p>2&minus;2ibRydy
</p>
<p>which clearly tends to zero as R &rarr;&infin;. We get a similar result for the inte-
gral along C4. Therefore, we have
</p>
<p>IR =
ˆ R
</p>
<p>&minus;R
e&minus;bx
</p>
<p>2
dx &rArr; lim
</p>
<p>R&rarr;&infin;
IR =
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
e&minus;bx
</p>
<p>2
dx =
</p>
<p>&radic;
π
</p>
<p>b
.
</p>
<p>Finally, we get
ˆ &infin;
</p>
<p>&minus;&infin;
eiax&minus;bx
</p>
<p>2
dx =
</p>
<p>&radic;
π
</p>
<p>b
e&minus;a
</p>
<p>2/(4b).
</p>
<p>Example 11.3.9 Let us evaluate I =
&acute;&infin;
</p>
<p>0 dx/(x
3 +1). If the integrand were
</p>
<p>even, we could extend the lower limit of integration to &minus;&infin; and close the
contour in the UHP. Since this is not the case, we need to use a different
trick. To get a hint as to how to close the contour, we study the singularities
of the integrand. These are simply the roots of the denominator: z3 =&minus;1 or
zn = ei(2n+1)π/3 with n= 0,1,2. These, as well as a contour that has only
z0 as an interior point, are shown in Fig. 11.4. We thus have
</p>
<p>I +
ˆ
</p>
<p>CR
</p>
<p>dz
</p>
<p>z3 + 1 +
ˆ
</p>
<p>C2
</p>
<p>dz
</p>
<p>z3 + 1 = 2πi Res
[
f (z0)
</p>
<p>]
. (11.8)
</p>
<p>The CR integral vanishes, as usual. Along C2, z= reiα , with constant α, so
that dz= eiαdr and
</p>
<p>ˆ
</p>
<p>C2
</p>
<p>dz
</p>
<p>z3 + 1 =
ˆ 0
</p>
<p>&infin;
</p>
<p>eiαdr
</p>
<p>(reiα)3 + 1 =&minus;e
iα
</p>
<p>ˆ &infin;
</p>
<p>0
</p>
<p>dr
</p>
<p>r3e3iα + 1 .
</p>
<p>In particular, if we choose 3α = 2π , we obtain
ˆ
</p>
<p>C2
</p>
<p>dz
</p>
<p>z3 + 1 =&minus;e
i2π/3
</p>
<p>ˆ &infin;
</p>
<p>0
</p>
<p>dr
</p>
<p>r3 + 1 =&minus;e
i2π/3I.</p>
<p/>
</div>
<div class="page"><p/>
<p>354 11 Calculus of Residues
</p>
<p>Fig. 11.4 The contour is chosen so that only one of the poles lies inside
</p>
<p>Substituting this in Eq. (11.8) gives
</p>
<p>(
1 &minus; ei2π/3
</p>
<p>)
I = 2πi Res
</p>
<p>[
f (z0)
</p>
<p>]
&rArr; I = 2πi
</p>
<p>1 &minus; ei2π/3 Res
[
f (z0)
</p>
<p>]
.
</p>
<p>On the other hand,
</p>
<p>Res
[
f (z0)
</p>
<p>]
= lim
</p>
<p>z&rarr;z0
(z&minus; z0)
</p>
<p>1
</p>
<p>(z&minus; z0)(z&minus; z1)(z&minus; z2)
</p>
<p>= 1
(z0 &minus; z1)(z0 &minus; z2)
</p>
<p>= 1
(eiπ/3 &minus; eiπ )(eiπ/3 &minus; ei5π/3) .
</p>
<p>These last two equations yield
</p>
<p>I = 2πi
1 &minus; ei2π/3
</p>
<p>1
</p>
<p>(eiπ/3 &minus; eiπ )(eiπ/3 &minus; ei5π/3) =
2π
</p>
<p>3
&radic;
</p>
<p>3
.
</p>
<p>11.3.5 Principal Value of an Integral
</p>
<p>So far we have discussed only integrals of functions that have no singulari-
ties on the contour. Let us now investigate the consequences of the presence
of singular points on the contour. Consider the integral
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>f (x)
</p>
<p>x &minus; x0
dx, (11.9)
</p>
<p>where x0 is a real number and f is analytic at x0. To avoid x0&mdash;which causes
the integrand to diverge&mdash;we bypass it by indenting the contour as shown in
Fig. 11.5 and denoting the new contour by Cu. The contour C0 is simply a
semicircle of radius ǫ. For the contour Cu, we have
ˆ
</p>
<p>Cu
</p>
<p>f (z)
</p>
<p>z&minus; x0
dz=
</p>
<p>ˆ x0&minus;ǫ
</p>
<p>&minus;&infin;
</p>
<p>f (x)
</p>
<p>x &minus; x0
dx +
</p>
<p>ˆ &infin;
</p>
<p>x0+ǫ
</p>
<p>f (x)
</p>
<p>x &minus; x0
dx +
</p>
<p>ˆ
</p>
<p>C0
</p>
<p>f (z)
</p>
<p>z&minus; x0
dz.
</p>
<p>In the limit ǫ &rarr; 0, the sum of the first two terms on the RHS&mdash;when it
exists&mdash;defines the principal value of the integral in Eq. (11.9):
</p>
<p>principal value of an
</p>
<p>integral</p>
<p/>
</div>
<div class="page"><p/>
<p>11.3 Evaluation of Definite Integrals 355
</p>
<p>Fig. 11.5 The contour Cu avoids x0
</p>
<p>P
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>f (x)
</p>
<p>x &minus; x0
dx = lim
</p>
<p>ǫ&rarr;0
</p>
<p>[
ˆ x0&minus;ǫ
</p>
<p>&minus;&infin;
</p>
<p>f (x)
</p>
<p>x &minus; x0
dx +
</p>
<p>ˆ &infin;
</p>
<p>x0+ǫ
</p>
<p>f (x)
</p>
<p>x &minus; x0
dx
</p>
<p>]
.
</p>
<p>The integral over the semicircle is calculated by noting that z &minus; x0 = ǫeiθ
and dz= iǫeiθdθ :
</p>
<p>&acute;
</p>
<p>C0
f (z) dz/(z&minus; x0)=&minus;iπf (x0). Therefore,
</p>
<p>ˆ
</p>
<p>Cu
</p>
<p>f (z)
</p>
<p>z&minus; x0
dz= P
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>f (x)
</p>
<p>x &minus; x0
dx &minus; iπf (x0). (11.10)
</p>
<p>On the other hand, if C0 is taken below the singularity on a contour Cd , say,
we obtain
</p>
<p>ˆ
</p>
<p>Cd
</p>
<p>f (z)
</p>
<p>z&minus; x0
dz= P
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>f (x)
</p>
<p>x &minus; x0
dx + iπf (x0).
</p>
<p>We see that the contour integral depends on how the singular point x0 is
avoided. However, the principal value, if it exists, is unique. To calculate
this principal value we close the contour by adding a large semicircle to it
as before, assuming that the contribution from this semicircle goes to zero
by Jordan&rsquo;s lemma. The contours Cu and Cd are replaced by a closed con-
tour, and the value of the integral will be given by the residue theorem. We
therefore have
</p>
<p>P
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>f (x)
</p>
<p>x &minus; x0
dx =&plusmn;iπf (x0)+ 2πi
</p>
<p>m&sum;
</p>
<p>j=1
Res
</p>
<p>[
f (zj )
</p>
<p>zj &minus; x0
</p>
<p>]
, (11.11)
</p>
<p>where {zj }mj=1 are the poles of f (z), the plus sign corresponds to placing
the infinitesimal semicircle in the UHP, as shown in Fig. 11.5, and the minus
sign corresponds to the other choice.
</p>
<p>Example 11.3.10 Let us use the principal-value method to evaluate the in-
tegral
</p>
<p>I =
ˆ &infin;
</p>
<p>0
</p>
<p>sinx
</p>
<p>x
dx = 1
</p>
<p>2
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>sinx
</p>
<p>x
dx.
</p>
<p>It appears that x = 0 is a singular point of the integrand; in reality, however,
it is only a removable singularity, as can be verified by the Taylor expansion
of sinx/x. To make use of the principal-value method, we write
</p>
<p>I = 1
2
</p>
<p>Im
</p>
<p>(
ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>eix
</p>
<p>x
dx
</p>
<p>)
= 1
</p>
<p>2
Im
</p>
<p>(
P
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>eix
</p>
<p>x
dx
</p>
<p>)
.</p>
<p/>
</div>
<div class="page"><p/>
<p>356 11 Calculus of Residues
</p>
<p>Fig. 11.6 The equivalent contour obtained by &ldquo;stretching&rdquo; Cu, the contour of Fig. 11.5
</p>
<p>We now use Eq. (11.11) with the small circle in the UHP, noting that there
are no singularities for eix/x there. This yields
</p>
<p>P
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>eix
</p>
<p>x
dx = iπe(0) = iπ.
</p>
<p>Therefore,
ˆ &infin;
</p>
<p>0
</p>
<p>sinx
</p>
<p>x
dx = 1
</p>
<p>2
Im(iπ)= π
</p>
<p>2
.
</p>
<p>The principal value of an integral can be written more compactly if we
deform the contour Cu by stretching it into that shown in Fig. 11.6. For small
enough ǫ, such a deformation will not change the number of singularities
within the infinite closed contour. Thus, the LHS of Eq. (11.10) will have
limits of integration &minus;&infin;+ iǫ and +&infin;+ iǫ. If we change the variable of
integration to ξ = z&minus; iǫ, this integral becomes
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>f (ξ + iǫ)
ξ + iǫ &minus; x0
</p>
<p>dξ =
ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>f (ξ) dξ
</p>
<p>ξ &minus; x0 + iǫ
=
ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>f (z) dz
</p>
<p>z&minus; x0 + iǫ
, (11.12)
</p>
<p>where in the last step we changed the dummy integration variable back to z.
Note that since f is assumed to be continuous at all points on the contour,
f (ξ + iǫ)&rarr; f (ξ) for small ǫ. The last integral of Eq. (11.12) shows that
there is no singularity on the new x-axis; we have pushed the singularity
down to x0 &minus; iǫ. In other words, we have given the singularity on the x-axis
a small negative imaginary part. We can thus rewrite Eq. (11.10) as
</p>
<p>P
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>f (x)
</p>
<p>x &minus; x0
dx = iπf (x0)+
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>f (x)dx
</p>
<p>x &minus; x0 + iǫ
,
</p>
<p>where x is used instead of z in the last integral because we are indeed in-
tegrating along the new x-axis&mdash;assuming that no other singularities are
present in the UHP. A similar argument, this time for the LHP, introduces a
minus sign for the first term on the RHS and for the ǫ term in the denomi-
nator. Therefore,
</p>
<p>Proposition 11.3.11 The principal value of an integral with one sim-
ple pole on the real axis is
</p>
<p>P
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>f (x)
</p>
<p>x &minus; x0
dx =&plusmn;iπf (x0)+
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>f (x)dx
</p>
<p>x &minus; x0 &plusmn; iǫ
, (11.13)
</p>
<p>where the plus (minus) sign refers to the UHP (LHP).</p>
<p/>
</div>
<div class="page"><p/>
<p>11.3 Evaluation of Definite Integrals 357
</p>
<p>Fig. 11.7 One of the four choices of contours for evaluating the principal value of the
integral when there are two poles on the real axis
</p>
<p>This result is sometimes abbreviated as
</p>
<p>1
</p>
<p>x &minus; x0 &plusmn; iǫ
= P 1
</p>
<p>x &minus; x0
∓ iπδ(x &minus; x0). (11.14)
</p>
<p>Example 11.3.12 Let us use residues to evaluate the function
</p>
<p>f (k)= 1
2πi
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>eikx dx
</p>
<p>x &minus; iǫ , ǫ &gt; 0.
</p>
<p>We have to close the contour by adding a large semicircle. Whether we do
The integral
</p>
<p>representation of the θ
</p>
<p>(step) functionthis in the UHP or the LHP is dictated by the sign of k: If k &gt; 0, we close in
the UHP. Thus,
</p>
<p>f (k)= 1
2πi
</p>
<p>ˆ
</p>
<p>C
</p>
<p>eikz dz
</p>
<p>z&minus; iǫ = Res
[
</p>
<p>eikz
</p>
<p>z&minus; iǫ
</p>
<p>]
</p>
<p>z&rarr;iǫ
</p>
<p>= lim
z&rarr;iǫ
</p>
<p>[
(z&minus; iǫ) e
</p>
<p>ikz
</p>
<p>z&minus; iǫ
</p>
<p>]
= e&minus;kǫ &minus;&minus;&rarr;
</p>
<p>ǫ&rarr;0
1.
</p>
<p>On the other hand, if k &lt; 0, we must close in the LHP, in which the integrand
is analytic. Thus, by the Cauchy-Goursat theorem, the integral vanishes.
Therefore, we have
</p>
<p>f (k)=
{
</p>
<p>1 if k &gt; 0,
</p>
<p>0 if k &lt; 0.
</p>
<p>This is precisely the definition of the theta function (or step function). Thus, theta (or step) function
</p>
<p>we have obtained an integral representation of that function:
</p>
<p>θ(x)= 1
2πi
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>eixt
</p>
<p>t &minus; iǫ dt.
</p>
<p>Now suppose that there are two singular points on the real axis, at x1 and
x2. Let us avoid x1 and x2 by making little semicircles, as before, letting both
semicircles be in the UHP (see Fig. 11.7). Without writing the integrands,</p>
<p/>
</div>
<div class="page"><p/>
<p>358 11 Calculus of Residues
</p>
<p>we can represent the contour integral by
ˆ x1&minus;ǫ
</p>
<p>&minus;&infin;
+
ˆ
</p>
<p>C1
</p>
<p>+
ˆ x2&minus;ǫ
</p>
<p>x1+ǫ
+
ˆ
</p>
<p>C2
</p>
<p>+
ˆ &infin;
</p>
<p>x2+ǫ
+
ˆ
</p>
<p>CR
</p>
<p>= 2πi
&sum;
</p>
<p>Res.
</p>
<p>The principal value of the integral is naturally defined to be the sum of all
integrals having ǫ in their limits. The contribution from the small semicircle
C1 can be calculated by substituting z&minus; x1 = ǫeiθ in the integral:
</p>
<p>ˆ
</p>
<p>C1
</p>
<p>f (z) dz
</p>
<p>(z&minus; x1)(z&minus; x2)
=
ˆ 0
</p>
<p>π
</p>
<p>f (x1 + ǫeiθ )iǫeiθ dθ
ǫeiθ (x1 + ǫeiθ &minus; x2)
</p>
<p>=&minus;iπ f (x1)
x1 &minus; x2
</p>
<p>,
</p>
<p>with a similar result for C2. Putting everything together, we get
</p>
<p>P
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>f (x)
</p>
<p>(x &minus; x1)(x &minus; x2)
dx &minus; iπ f (x2)&minus; f (x1)
</p>
<p>x2 &minus; x1
= 2πi
</p>
<p>&sum;
Res.
</p>
<p>If we include the case where both C1 and C2 are in the LHP, we get
</p>
<p>P
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>f (x)
</p>
<p>(x &minus; x1)(x &minus; x2)
dx =&plusmn;iπ f (x2)&minus; f (x1)
</p>
<p>x2 &minus; x1
+ 2πi
</p>
<p>&sum;
Res,
</p>
<p>(11.15)
where the plus sign is for the case where C1 and C2 are in the UHP and the
minus sign for the case where both are in the LHP. We can also obtain the
result for the case where the two singularities coincide by taking the limit
x1 &rarr; x2. Then the RHS of the last equation becomes a derivative, and we
obtain
</p>
<p>P
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>f (x)
</p>
<p>(x &minus; x0)2
dx =&plusmn;iπf &prime;(x0)+ 2πi
</p>
<p>&sum;
Res.
</p>
<p>Example 11.3.13 An expression encountered in the study of Green&rsquo;s func-
tions or propagators (which we shall discuss later in the book) is
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>eitx dx
</p>
<p>x2 &minus; k2 ,
</p>
<p>where k and t are real constants. We want to calculate the principal value of
this integral. We use Eq. (11.15) and note that for t &gt; 0, we need to close
the contour in the UHP, where there are no poles:
</p>
<p>P
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>eitx dx
</p>
<p>x2 &minus; k2 = P
ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>eitx dx
</p>
<p>(x &minus; k)(x + k) = iπ
eikt &minus; e&minus;ikt
</p>
<p>2k
=&minus;π sin kt
</p>
<p>k
.
</p>
<p>When t &lt; 0, we have to close the contour in the LHP, where again there are
no poles:
</p>
<p>P
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>eitx dx
</p>
<p>x2 &minus; k2 = P
ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>eitx dx
</p>
<p>(x &minus; k)(x + k) =&minus;iπ
eikt &minus; e&minus;ikt
</p>
<p>2k
= π sin kt
</p>
<p>k
.
</p>
<p>The two results above can be combined into a single relation:
</p>
<p>P
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>eitx dx
</p>
<p>x2 &minus; k2 =&minus;π
sin k|t |
</p>
<p>k
.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.4 Problems 359
</p>
<p>11.4 Problems
</p>
<p>11.1 Evaluate each of the following integrals, for all of which C is the circle
|z| = 3.
</p>
<p>(a)
˛
</p>
<p>C
</p>
<p>4z&minus; 3
z(z&minus; 2) dz. (b)
</p>
<p>˛
</p>
<p>C
</p>
<p>ez
</p>
<p>z(z&minus; iπ) dz.
</p>
<p>(c)
˛
</p>
<p>C
</p>
<p>cos z
</p>
<p>z(z&minus; π) dz. (d)
˛
</p>
<p>C
</p>
<p>z2 + 1
z(z&minus; 1) dz.
</p>
<p>(e)
˛
</p>
<p>C
</p>
<p>cosh z
</p>
<p>z2 + π2 dz. (f)
˛
</p>
<p>C
</p>
<p>1 &minus; cos z
z2
</p>
<p>dz.
</p>
<p>(g)
˛
</p>
<p>C
</p>
<p>sinh z
</p>
<p>z4
dz. (h)
</p>
<p>˛
</p>
<p>C
</p>
<p>z cos
</p>
<p>(
1
</p>
<p>z
</p>
<p>)
dz.
</p>
<p>(i)
˛
</p>
<p>C
</p>
<p>dz
</p>
<p>z3(z+ 5) . (j)
˛
</p>
<p>C
</p>
<p>tan z dz.
</p>
<p>(k)
˛
</p>
<p>C
</p>
<p>dz
</p>
<p>sinh 2z
. (l)
</p>
<p>˛
</p>
<p>C
</p>
<p>ez
</p>
<p>z2
dz.
</p>
<p>(m)
˛
</p>
<p>C
</p>
<p>dz
</p>
<p>z2 sin z
. (n)
</p>
<p>˛
</p>
<p>C
</p>
<p>ez dz
</p>
<p>(z&minus; 1)(z&minus; 2) .
</p>
<p>11.2 Let h(z) be analytic and have a simple zero at z = z0, and let g(z) be
analytic there. Let f (z)= g(z)/h(z), and show that
</p>
<p>Res
[
f (z0)
</p>
<p>]
= g(z0)
</p>
<p>h&prime;(z0)
.
</p>
<p>11.3 Find the residue of f (z)= 1/ cos z at each of its poles.
</p>
<p>11.4 Evaluate the integral
&acute;&infin;
</p>
<p>0 dx/[(x2 + 1)(x2 + 4)] by closing the contour
(a) in the UHP and (b) in the LHP.
</p>
<p>11.5 Evaluate the following integrals, in which a and b are nonzero real
constants.
</p>
<p>(a)
ˆ &infin;
</p>
<p>0
</p>
<p>2x2 + 1
x4 + 5x2 + 6 dx. (b)
</p>
<p>ˆ &infin;
</p>
<p>0
</p>
<p>dx
</p>
<p>6x4 + 5x2 + 1 .
</p>
<p>(c)
ˆ &infin;
</p>
<p>0
</p>
<p>dx
</p>
<p>x4 + 1 . (d)
ˆ &infin;
</p>
<p>0
</p>
<p>cosx dx
</p>
<p>(x2 + a2)2(x2 + b2) .
</p>
<p>(e)
ˆ &infin;
</p>
<p>0
</p>
<p>cosax
</p>
<p>(x2 + b2)2 dx. (f)
ˆ &infin;
</p>
<p>0
</p>
<p>dx
</p>
<p>(x2 + 1)2 .
</p>
<p>(g)
ˆ &infin;
</p>
<p>0
</p>
<p>dx
</p>
<p>(x2 + 1)2(x2 + 2) . (h)
ˆ &infin;
</p>
<p>0
</p>
<p>2x2 &minus; 1
x6 + 1 dx.</p>
<p/>
</div>
<div class="page"><p/>
<p>360 11 Calculus of Residues
</p>
<p>(i)
ˆ &infin;
</p>
<p>0
</p>
<p>x2dx
</p>
<p>(x2 + a2)2 . (j)
ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>x dx
</p>
<p>(x2 + 4x + 13)2 .
</p>
<p>(k)
ˆ &infin;
</p>
<p>0
</p>
<p>x3 sinax
</p>
<p>x6 + 1 dx. (l)
ˆ &infin;
</p>
<p>0
</p>
<p>x2 + 1
x2 + 4 dx.
</p>
<p>(m)
ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>x cosx dx
</p>
<p>x2 &minus; 2x + 10 . (n)
ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>x sinx dx
</p>
<p>x2 &minus; 2x + 10 .
</p>
<p>(o)
ˆ &infin;
</p>
<p>0
</p>
<p>dx
</p>
<p>x2 + 1 . (p)
ˆ &infin;
</p>
<p>0
</p>
<p>x2dx
</p>
<p>(x2 + 4)2(x2 + 25) .
</p>
<p>(q)
ˆ &infin;
</p>
<p>0
</p>
<p>cosax
</p>
<p>x2 + b2 dx. (r)
ˆ &infin;
</p>
<p>0
</p>
<p>dx
</p>
<p>(x2 + 4)2 .
</p>
<p>11.6 Evaluate each of the following integrals by turning it into a contour
</p>
<p>integral around a unit circle.
</p>
<p>(a)
ˆ 2π
</p>
<p>0
</p>
<p>dθ
</p>
<p>5 + 4 sin θ .
</p>
<p>(b)
ˆ 2π
</p>
<p>0
</p>
<p>dθ
</p>
<p>a + cos θ where a &gt; 1.
</p>
<p>(c)
ˆ 2π
</p>
<p>0
</p>
<p>dθ
</p>
<p>1 + sin2 θ
.
</p>
<p>(d)
ˆ 2π
</p>
<p>0
</p>
<p>dθ
</p>
<p>(a + b cos2 θ)2 where a, b &gt; 0.
</p>
<p>(e)
ˆ 2π
</p>
<p>0
</p>
<p>cos2 3θ
</p>
<p>5 &minus; 4 cos 2θ dθ.
</p>
<p>(f)
ˆ π
</p>
<p>0
</p>
<p>dφ
</p>
<p>1 &minus; 2a cosφ + a2 where a �= &plusmn;1.
</p>
<p>(g)
ˆ π
</p>
<p>0
</p>
<p>cos2 3φ dφ
</p>
<p>1 &minus; 2a cosφ + a2 where a �= &plusmn;1.
</p>
<p>(h)
ˆ π
</p>
<p>0
</p>
<p>cos 2φ dφ
</p>
<p>1 &minus; 2a cosφ + a2 where a �= &plusmn;1.
</p>
<p>(i)
ˆ π
</p>
<p>0
tan(x + ia) dx where a &isin;R.
</p>
<p>(j)
ˆ π
</p>
<p>0
ecosφ cos(nφ &minus; sinφ)dφ where n &isin; Z.
</p>
<p>11.7 Evaluate the integral I =
&acute;&infin;
&minus;&infin; e
</p>
<p>αx dx/(1 + ex) for 0 &lt; α &lt; 1. Hint:
Choose a closed (long) rectangle that encloses only one of the zeros of the
</p>
<p>denominator. Show that the contributions of the short sides of the rectangle
</p>
<p>are zero.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.4 Problems 361
</p>
<p>Fig. 11.8 The contour used in Problem 11.8
</p>
<p>11.8 Derive the integration formula
&acute;&infin;
</p>
<p>0 e
&minus;x2 cos(2bx)dx =
</p>
<p>&radic;
π
</p>
<p>2 e
&minus;b2 where
</p>
<p>b �= 0 by integrating the function e&minus;z2 around the rectangular path shown in
Fig. 11.8.
</p>
<p>11.9 Use the result of Example 11.3.12 to show that θ &prime;(k)= δ(k).
</p>
<p>11.10 Find the principal values of the following integrals.
</p>
<p>(a)
ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>sinx dx
</p>
<p>(x2 + 4)(x &minus; 1) . (b)
ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>cosax
</p>
<p>1 + x3 dx where a &ge; 0.
</p>
<p>(c)
ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>x cosx
</p>
<p>x2 &minus; 5x + 6 dx. (d)
ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>1 &minus; cosx
x2
</p>
<p>dx.
</p>
<p>11.11 Evaluate the following integrals.
</p>
<p>(a)
ˆ &infin;
</p>
<p>0
</p>
<p>x2 &minus; b2
x2 + b2
</p>
<p>(
sinax
</p>
<p>x
</p>
<p>)
dx. (b)
</p>
<p>ˆ &infin;
</p>
<p>0
</p>
<p>sinax
</p>
<p>x(x2 + b2) dx.
</p>
<p>(c)
ˆ &infin;
</p>
<p>0
</p>
<p>sinax
</p>
<p>x(x2 + b2)2 dx. (d)
ˆ &infin;
</p>
<p>0
</p>
<p>cos 2ax &minus; cos 2bx
x2
</p>
<p>dx.
</p>
<p>(e)
ˆ &infin;
</p>
<p>0
</p>
<p>sin2 x dx
</p>
<p>x2
. (f)
</p>
<p>ˆ &infin;
</p>
<p>0
</p>
<p>sin3 x dx
</p>
<p>x3
.</p>
<p/>
</div>
<div class="page"><p/>
<p>12Advanced Topics
</p>
<p>The subject of complex analysis is an extremely rich and powerful area of
mathematics. We have already seen some of this richness and power in the
previous chapter. This chapter concludes our discussion of complex analysis
by introducing some other topics with varying degrees of importance.
</p>
<p>12.1 Meromorphic Functions
</p>
<p>Complex functions that have only simple poles as their singularities are nu-
merous in applications and are called meromorphic functions. In this sec- meromorphic functions
tion, we derive an important result for such functions.
</p>
<p>Suppose that f (z) has simple poles at {zj }Nj=1, where N could be in-
finity. Then, assuming that z �= zj for all j , and noting that the residue of
f (ξ)/(ξ &minus; z) at ξ = z is simply f (z), the residue theorem yields
</p>
<p>1
</p>
<p>2πi
</p>
<p>&int;
</p>
<p>Cn
</p>
<p>f (ξ)
</p>
<p>ξ &minus; z dξ = f (z)+
n&sum;
</p>
<p>j=1
Res
</p>
<p>(
f (ξ)
</p>
<p>ξ &minus; z
</p>
<p>)
</p>
<p>ξ=zj
,
</p>
<p>where Cn is a circle containing the first n poles, and it is assumed that the
poles are arranged in order of increasing absolute values. Since the poles of
f are assumed to be simple, we have
</p>
<p>Res
</p>
<p>(
f (ξ)
</p>
<p>ξ &minus; z
</p>
<p>)
</p>
<p>ξ=zj
= lim
</p>
<p>ξ&rarr;zj
(ξ &minus; zj )
</p>
<p>f (ξ)
</p>
<p>ξ &minus; z =
1
</p>
<p>zj &minus; z
lim
ξ&rarr;zj
</p>
<p>[
(ξ &minus; zj )f (ξ)
</p>
<p>]
</p>
<p>= 1
zj &minus; z
</p>
<p>Res
[
f (ξ)
</p>
<p>]
ξ=zj &equiv;
</p>
<p>rj
</p>
<p>zj &minus; z
,
</p>
<p>where rj is, by definition, the residue of f (ξ) at ξ = zj . Substituting in the
preceding equation gives
</p>
<p>f (z)= 1
2πi
</p>
<p>&int;
</p>
<p>Cn
</p>
<p>f (ξ)
</p>
<p>ξ &minus; z dξ &minus;
n&sum;
</p>
<p>j=1
</p>
<p>rj
</p>
<p>zj &minus; z
.
</p>
<p>S. Hassani, Mathematical Physics, DOI 10.1007/978-3-319-01195-0_12,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>363</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-01195-0_12">http://dx.doi.org/10.1007/978-3-319-01195-0_12</a></div>
</div>
<div class="page"><p/>
<p>364 12 Advanced Topics
</p>
<p>Taking the difference between this and the same equation evaluated at z= 0
(assumed to be none of the poles),1 we can write
</p>
<p>f (z)&minus; f (0)= z
2πi
</p>
<p>&int;
</p>
<p>Cn
</p>
<p>f (ξ)
</p>
<p>ξ(ξ &minus; z) dξ +
n&sum;
</p>
<p>j=1
rj
</p>
<p>(
1
</p>
<p>z&minus; zj
+ 1
</p>
<p>zj
</p>
<p>)
.
</p>
<p>If |f (ξ)| approaches a finite value as |ξ | &rarr;&infin;, the integral vanishes for an
infinite circle (which includes all poles now), and we obtain what is called
the Mittag-Leffler expansion of the meromorphic function f :Mittag-Leffler expansion
</p>
<p>f (z)= f (0)+
N&sum;
</p>
<p>j=1
rj
</p>
<p>(
1
</p>
<p>z&minus; zj
+ 1
</p>
<p>zj
</p>
<p>)
. (12.1)
</p>
<p>Now we let g be an entire function with simple zeros. We claim that (a)
(dg/dz)/g(z) is a meromorphic function that is bounded for all values of z,
and (b) its residues are all unity. To see this, note that g is of the form2
</p>
<p>g(z)= (z&minus; z1)(z&minus; z2) &middot; &middot; &middot; (z&minus; zN )f (z),
</p>
<p>where z1, . . . , zN are all the zeros of g, and f is an analytic function that
does not vanish anywhere in the complex plane. It is now easy to see that
</p>
<p>g&prime;(z)
g(z)
</p>
<p>=
N&sum;
</p>
<p>j=1
</p>
<p>1
</p>
<p>z&minus; zj
+ f
</p>
<p>&prime;(z)
f (z)
</p>
<p>.
</p>
<p>This expression has both properties (a) and (b) mentioned above. Further-
more, the last term is an entire function that is bounded for all C. Therefore,
it must be a constant by Proposition 10.5.5. This derivation also verifies
Eq. (12.1), which in the case at hand can be written as
</p>
<p>d
</p>
<p>dz
lng(z)= g
</p>
<p>&prime;(z)
g(z)
</p>
<p>= d
dz
</p>
<p>lng(0)+
N&sum;
</p>
<p>j=1
</p>
<p>(
1
</p>
<p>z&minus; zj
+ 1
</p>
<p>zj
</p>
<p>)
,
</p>
<p>whose solution is readily found and is given in the following
</p>
<p>Proposition 12.1.1 If g is an entire function with simple zeros
{zj }Nj=1, then
</p>
<p>g(z)= g(0)ecz
N&prod;
</p>
<p>j=1
</p>
<p>(
1 &minus; z
</p>
<p>zj
</p>
<p>)
ez/zj where c= (dg/dz)|z=0
</p>
<p>g(0)
</p>
<p>(12.2)
and it is assumed that zj �= 0 for all j .
</p>
<p>1This is not a restrictive assumption because we can always move our coordinate system
so that the origin avoids all poles.
2One can &ldquo;prove&rdquo; this by factoring the simple zeros one by one, writing g(z) = (z &minus;
z1)f1(z) and noting that g(z2)= 0, with z2 �= z1, implies that f1(z)= (z&minus; z2)f2(z), etc.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.2 Multivalued Functions 365
</p>
<p>Fig. 12.1 (a) The origin is a branch point of the natural log function. (b) z0 is a branch
point of f (z) if f (z0 + reiθ ) �= f (z0 + rei(θ+2π))
</p>
<p>12.2 Multivalued Functions
</p>
<p>The arbitrariness, up to a multiple of 2π , of the angle θ = arg(z) in z= reiθ
leads to functions that can take different values at the same point. Consider,
for example, the function f (z) = &radic;z. Writing z in polar coordinates, we
obtain f (z)= f (r, θ)= (reiθ )1/2 =&radic;reiθ/2. This shows that for the same
z = (r, θ) = (r, θ + 2π), we get two different values, f (r, θ) and f (r, θ +
2π)=&minus;f (r, θ).
</p>
<p>This may be disturbing at first. After all, the definition of a function (map-
ping) ensures that for any point in the domain a unique image is obtained.
Here two different images are obtained for the same z. Riemann found a cure
for this complex &ldquo;double vision&rdquo; by introducing what is now called Riemann
sheets. We will discuss these briefly below, but first let us take a closer look
at a prototype of multivalued functions. Consider the natural log function,
ln z. For z= reiθ this is defined as ln z= ln r + iθ = ln |z| + i arg(z) where
arg(z) is defined only to within a multiple of 2π ; that is, arg(z)= θ + 2nπ ,
for n= 0,&plusmn;1,&plusmn;2, . . . .
</p>
<p>We can see the peculiar nature of the logarithmic function by consider-
ing a closed curve around the origin, as shown in Fig. 12.1(a). Starting at
an arbitrary point z on the curve, we move counterclockwise, noticing the
constant increase in the angle θ , until we reach the initial point. Now, the
angle is θ + 2π . Thus, the process of moving around the origin has changed
the value of the log function by 2πi, i.e., (ln z)final &minus; (ln z)initial = 2πi. Note
that in this process z does not change, because
</p>
<p>zfinal = rei(θ+2π) = reiθe2πi = reiθ = zinitial.
</p>
<p>Definition 12.2.1 A branch point of a function f : C&rarr; C is a complex branch point
number z0 with the property that for any (small enough) closed curve C
encircling z0 and for any point z &equiv; z0 + reiθ on the curve, f (z0 + reiθ ) �=
f (z0 + rei(θ+2π)).
</p>
<p>Historical Notes
</p>
<p>Victor-Alexandre Puiseux (1820&ndash;1883) was the first to take up the subject of multival-
ued functions. In 1850 Puiseux published a celebrated paper on complex algebraic func-
tions given by f (u, z)= 0, f a polynomial in u and z. He first made clear the distinction</p>
<p/>
</div>
<div class="page"><p/>
<p>366 12 Advanced Topics
</p>
<p>between poles and branch points that Cauchy had barely perceived, and introduced the
notion of an essential singular point, to which Weierstrass independently had called
attention. Though Cauchy, in the 1846 paper, did consider the variation of simple mul-
tivalued functions along paths that enclosed branch points, Puiseux clarified this subject
too.
Puiseux also showed that the development of a function of z about a branch point z= a
must involve fractional powers of z&minus; a. He then improved on Cauchy&rsquo;s theorem on the
expansion of a function in a Maclaurin series. By his significant investigations of many-
valued functions and their branch points in the complex plane, and by his initial work on
integrals of such functions, Puiseux brought Cauchy&rsquo;s pioneering work in function theory
to the end of what might be called the first stage. The difficulties in the theory of multiple-
valued functions and integrals of such functions were still to be overcome. Cauchy did
write other papers on the integrals of multiplevalued functions in which he attempted to
follow up on Puiseux&rsquo;s work; and though he introduced the notion of branch cuts (lignes
d&rsquo;arr&ecirc;t), he was still confused about the distinction between poles and branch points. This
subject of algebraic functions and their integrals was to be pursued by Riemann.
Puiseux was a keen mountaineer and was the first to scale the Alpine peak that is now
named after him.
</p>
<p>Thus, z = 0 is a branch point of the logarithmic function. Studying the
behavior of ln(1/z)=&minus; ln z around z= 0 will reveal that the point &ldquo;at infin-
ity&rdquo; is also a branch point of ln z. If z0 �= 0 is any other point of the complex
plane, then choosing C to be a small loop, we get
</p>
<p>ln
(
z0 + reiφ
</p>
<p>)
= ln
</p>
<p>[
z0
</p>
<p>(
1 + re
</p>
<p>iφ
</p>
<p>z0
</p>
<p>)]
= ln z0 + ln
</p>
<p>(
1 + re
</p>
<p>iφ
</p>
<p>z0
</p>
<p>)
</p>
<p>&asymp; ln z0 +
reiφ
</p>
<p>z0
for r ≪ |z0|.
</p>
<p>It is now clear that ln(z0+reiθ )= ln(z0+rei(θ+2π)). We therefore conclude
that any point of the complex plane other than the origin cannot be a branch
point of the natural log function.
</p>
<p>12.2.1 Riemann Surfaces
</p>
<p>The idea of a Riemann surface begins with the removal of all points that lie
on the line (or any other curve) joining two branch points. For lnz this means
the removal of all points lying on a curve that starts at z= 0 and extends all
the way to infinity. Such a curve is called a branch cut, or simply a cut.branch cut or simply
</p>
<p>&ldquo;cut&rdquo; Let us concentrate on ln z and take the cut to be along the negative half
of the real axis. Let us also define the functions
</p>
<p>fn(z)= fn(r, θ)
= ln r + i(θ + 2nπ) for &minus;π &lt; θ &lt; π; r &gt; 0; n= 0,&plusmn;1, . . . ,
</p>
<p>so fn(z) takes on the same values for &minus;π &lt; θ &lt; π that ln z takes in the range
(2n&minus; 1)π &lt; θ &lt; (2n+ 1)π . We have replaced the multivalued logarithmic
function by a series of different functions that are analytic in the cut z-plane.
</p>
<p>This process of cutting the z-plane and then defining a sequence of
functions eliminates the contradiction caused by the existence of branch
points, since we are no longer allowed to completely encircle a branch point.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.2 Multivalued Functions 367
</p>
<p>Fig. 12.2 A few sheets of the Riemann surface of the logarithmic function. The path C
encircling the origin O ends up on the lower sheet
</p>
<p>A complete circulation involves crossing the cut, which, in turn, violates the
domain of definition of fn(z).
</p>
<p>We have made good progress. We have replaced the (nonanalytic) multi-
valued function ln z with a series of analytic (in their domain of definition)
functions fn(z). However, there is a problem left: fn(z) has a discontinuity
at the cut. In fact, just above the cut fn(r,π &minus; ǫ)= ln r + i(π &minus; ǫ + 2nπ)
with ǫ &gt; 0, and just below it fn(r,&minus;π + ǫ)= ln r + i(&minus;π + ǫ + 2nπ), so
that
</p>
<p>lim
ǫ&rarr;0
</p>
<p>[
fn(r,π &minus; ǫ)&minus; fn(r,&minus;π + ǫ)
</p>
<p>]
= 2πi.
</p>
<p>To cure this we make the observation that the value of fn(z) just above
the cut is the same as the value of fn+1(z) just below the cut. This sug-
gests the following geometrical construction, due to Riemann: Superpose
an infinite series of cut complex planes one on top of the other, each plane
corresponding to a different value of n. The adjacent planes are connected
along the cut such that the upper lip of the cut in the (n&minus; 1)th plane is con-
nected to the lower lip of the cut in the nth plane. All planes contain the two
branch points. That is, the branch points appear as &ldquo;hinges&rdquo; at which all the
planes are joined. With this geometrical construction, if we cross the cut, we
end up on a different plane adjacent to the previous one (Fig. 12.2).
</p>
<p>The geometric surface thus constructed is called a Riemann surface; Riemann surfaces and
sheetseach plane is called a Riemann sheet and is denoted by Rj , for j =
</p>
<p>0,&plusmn;1,&plusmn;2, . . . . A single-valued function defined on a Riemann sheet is
called a branch of the original multivalued function.
</p>
<p>We have achieved the following: From a multivalued function we have
constructed a sequence of single-valued functions, each defined in a sin-
gle complex plane; from this sequence of functions we have constructed a
single complex function defined on a single Riemann surface. Thus, the log-
arithmic function is analytic throughout the Riemann surface except at the
branch points, which are simply the function&rsquo;s singular points.</p>
<p/>
</div>
<div class="page"><p/>
<p>368 12 Advanced Topics
</p>
<p>Fig. 12.3 The Riemann surface for f (z)= z1/2
</p>
<p>It is now easy to see the geometrical significance of branch points.
A complete cycle around a branch point takes us to another Riemann sheet,
where the function takes on a different form. On the other hand, a complete
cycle around an ordinary point either never crosses the cut, or if it does, it
will cross it back to the original sheet.
</p>
<p>Let us now briefly consider two of the more common multivalued func-
tions and their Riemann surfaces.
</p>
<p>Example 12.2.2 (The function f (z)= z1/n) The only branch points for the
function f (z) = z1/n are z = 0 and the point at infinity. Defining fk(z) &equiv;
r1/nei(θ+2kπ/n) for k = 0,1, . . . , n &minus; 1 and 0 &lt; θ &lt; 2π and following the
same procedure as for the logarithmic function, we see that there must be
n Riemann sheets, labeled R0, R1, . . . , Rn&minus;1, in the Riemann surface. The
lower edge of Rn&minus;1 is pasted to the upper edge of R0 along the cut, which
is taken to be along the positive real axis. The Riemann surface for n= 2 is
shown in Fig. 12.3.
</p>
<p>It is clear that for any noninteger value of α the function f (z)= zα has a
branch point at z= 0 and another at the point at infinity. For irrational α the
number of Riemann sheets is infinite.
</p>
<p>Example 12.2.3 (The function f (z) = (z2 &minus; 1)1/2) The branch points for
the function f (z)= (z2 &minus; 1)1/2 are at z1 =+1 and z2 =&minus;1 (see Fig. 12.4).
Writing z&minus; 1 = r1eiθ1 and z+ 1 = r2eiθ2 , we have
</p>
<p>f (z)=
(
r1e
</p>
<p>iθ1
)1/2(
</p>
<p>r2e
iθ2
</p>
<p>)1/2 =&radic;r1r2ei(θ1+θ2)/2.
</p>
<p>The cut is along the real axis from z = &minus;1 to z = +1. There are two Rie-
mann sheets in the Riemann surface. Clearly, only cycles of 2π involving
one branch point will cross the cut and therefore end up on a different sheet.
Any closed curve that has both z1 and z2 as interior points will remain en-
tirely on the original sheet.
</p>
<p>The notion of branch cuts can be used to evaluate certain integrals thatevaluation of integrals
involving cuts do not fit into the three categories discussed in Chap. 11. The basic idea is
</p>
<p>to circumvent the cut by constructing a contour that is infinitesimally close
to the cut and circles around branch points.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.2 Multivalued Functions 369
</p>
<p>Fig. 12.4 The cut for the function f (z)= (z2 &minus; 1)1/2 is from z1 to z2. Paths that circle
only one of the points cross the cut and end up on the other sheet
</p>
<p>Fig. 12.5 The contour for the evaluation of the integrals of Examples 12.2.4 and 12.2.5
</p>
<p>Example 12.2.4 To evaluate the integral I =
&int;&infin;
</p>
<p>0 x
αdx/(x2 + 1) for
</p>
<p>|α| &lt; 1, consider the complex integral I &prime; =
∮
C
zαdz/(z2 + 1) where C is
</p>
<p>as shown in Fig. 12.5 and the cut is taken along the positive real axis. To
evaluate the contribution from CR and Cr , we let ρ stand for either r or R.
Then we have
</p>
<p>Iρ =
&int;
</p>
<p>Cρ
</p>
<p>(ρeiθ )α
</p>
<p>(ρeiθ )2 + 1 iρe
iθ dθ = i
</p>
<p>&int; 2π
</p>
<p>0
</p>
<p>ρα+1ei(α+1)θ
</p>
<p>ρ2e2iθ + 1 dθ.
</p>
<p>It is clear that since |α|&lt; 1, Iρ &rarr; 0 as ρ &rarr; 0 or ρ &rarr;&infin;.
The contributions from L1 and L2 do not cancel one another because the
</p>
<p>value of the function changes above and below the cut. To evaluate these
two integrals we have to choose a branch of the function. Let us choose that
branch on which zα = |z|αeiαθ for 0 &lt; θ &lt; 2π . Along L1, θ &asymp; 0 or zα = xα ,
and along L2, θ &asymp; 2π or zα = (xe2πi)α . Thus,
</p>
<p>∮
</p>
<p>C
</p>
<p>zα
</p>
<p>z2 + 1 dz=
&int; &infin;
</p>
<p>0
</p>
<p>xα
</p>
<p>x2 + 1 dx +
&int; 0
</p>
<p>&infin;
</p>
<p>xαe2πiα
</p>
<p>(xe2πi)2 + 1 dx
</p>
<p>=
(
1 &minus; e2πiα
</p>
<p>)&int; &infin;
</p>
<p>0
</p>
<p>xα
</p>
<p>x2 + 1 dx. (12.3)</p>
<p/>
</div>
<div class="page"><p/>
<p>370 12 Advanced Topics
</p>
<p>The LHS of this equation can be obtained using the residue theorem. There
are two simple poles, at z = +i and z = &minus;i with residues Res[f (i)] =
(eiπ/2)α/2i and Res[f (&minus;i)] = &minus;(ei3π/2)α/2i. Thus,
</p>
<p>∮
</p>
<p>C
</p>
<p>zα
</p>
<p>z2 + 1 dz= 2πi
(
eiαπ/2
</p>
<p>2i
&minus; e
</p>
<p>i3απ/2
</p>
<p>2i
</p>
<p>)
= π
</p>
<p>(
eiαπ/2 &minus; ei3απ/2
</p>
<p>)
.
</p>
<p>Combining this with Eq. (12.3), we obtain
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>xα
</p>
<p>x2 + 1 dx =
π(eiαπ/2 &minus; ei3απ/2)
</p>
<p>1 &minus; e2πiα =
π
</p>
<p>2
sec
</p>
<p>απ
</p>
<p>2
.
</p>
<p>If we had chosen a different branch of the function, both the LHS and the
RHS of Eq. (12.3) would have been different, but the final result would still
have been the same.
</p>
<p>Example 12.2.5 Here is another integral involving a branch cut:
</p>
<p>I =
&int; &infin;
</p>
<p>0
</p>
<p>x&minus;a
</p>
<p>x + 1 dx for 0 &lt; a &lt; 1.
</p>
<p>To evaluate this integral we use the zeroth branch of the function and the
contour of the previous example (Fig. 12.5). Thus, writing z = ρeiθ , we
have
</p>
<p>2πi Res
[
f (&minus;1)
</p>
<p>]
=
∮
</p>
<p>C
</p>
<p>z&minus;a
</p>
<p>z+ 1 dz=
&int; &infin;
</p>
<p>0
</p>
<p>ρ&minus;a
</p>
<p>ρ + 1 dρ +
∮
</p>
<p>CR
</p>
<p>z&minus;a
</p>
<p>z+ 1 dz
</p>
<p>+
&int; 0
</p>
<p>&infin;
</p>
<p>(ρe2iπ )&minus;a
</p>
<p>ρe2iπ + 1 e
2iπdρ +
</p>
<p>∮
</p>
<p>Cr
</p>
<p>z&minus;a
</p>
<p>z+ 1 dz. (12.4)
</p>
<p>The contributions from both circles vanish by the same argument used in
the previous example. On the other hand, Res[f (&minus;1)] = (&minus;1)&minus;a . For the
branch we are using, &minus;1 = eiπ . Thus, Res[f (&minus;1)] = e&minus;iaπ . The RHS of
Eq. (12.4) yields
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>ρ&minus;a
</p>
<p>ρ + 1 dρ &minus; e
&minus;2iπa
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>ρ&minus;a
</p>
<p>ρ + 1 dρ =
(
1 &minus; e&minus;2iπa
</p>
<p>)
I.
</p>
<p>It follows from (12.4) that (1 &minus; e&minus;2iπa)I = 2πie&minus;iπa , or
&int; &infin;
</p>
<p>0
</p>
<p>x&minus;a
</p>
<p>x + 1 dx =
π
</p>
<p>sinaπ
for 0 &lt; a &lt; 1.
</p>
<p>Example 12.2.6 Let us evaluate I =
&int;&infin;
</p>
<p>0 lnx dx/(x
2 + a2) with a &gt; 0. We
</p>
<p>choose the zeroth branch of the logarithmic function, in which &minus;π &lt; θ &lt; π ,
and use the contour of Fig. 12.6.
</p>
<p>For L1, z= ρeiπ (note that ρ &gt; 0), and for L2, z= ρ. Thus, we have</p>
<p/>
</div>
<div class="page"><p/>
<p>12.2 Multivalued Functions 371
</p>
<p>Fig. 12.6 The contour for the evaluation of the integral of Example 12.2.6
</p>
<p>2πi Res
[
f (ia)
</p>
<p>]
=
∮
</p>
<p>C
</p>
<p>ln z
</p>
<p>z2 + a2 dz
</p>
<p>=
&int; ǫ
</p>
<p>&infin;
</p>
<p>ln(ρeiπ )
</p>
<p>(ρeiπ )2 + a2 e
iπdρ +
</p>
<p>&int;
</p>
<p>Cǫ
</p>
<p>ln z
</p>
<p>z2 + a2 dz
</p>
<p>+
&int; &infin;
</p>
<p>ǫ
</p>
<p>lnρ
</p>
<p>ρ2 + a2 dρ +
&int;
</p>
<p>CR
</p>
<p>ln z
</p>
<p>z2 + a2 dz, (12.5)
</p>
<p>where z= ia is the only singularity&mdash;a simple pole&mdash;in the UHP. Now we
note that
</p>
<p>&int; ǫ
</p>
<p>&infin;
</p>
<p>ln(ρeiπ )
</p>
<p>(ρeiπ )2 + a2 e
iπdρ =
</p>
<p>&int; &infin;
</p>
<p>ǫ
</p>
<p>lnρ + iπ
ρ2 + a2 dρ
</p>
<p>=
&int; &infin;
</p>
<p>ǫ
</p>
<p>lnρ
</p>
<p>ρ2 + a2 dρ + iπ
&int; &infin;
</p>
<p>ǫ
</p>
<p>dρ
</p>
<p>ρ2 + a2 .
</p>
<p>The contributions from the circles tend to zero. On the other hand,
</p>
<p>Res
[
f (ia)
</p>
<p>]
= lim
</p>
<p>z&rarr;ia
(z&minus; ia) ln z
</p>
<p>(z&minus; ia)(z+ ia) =
ln(ia)
</p>
<p>2ia
= 1
</p>
<p>2ia
</p>
<p>(
lna + i π
</p>
<p>2
</p>
<p>)
.
</p>
<p>Substituting the last two results in Eq. (12.5), we obtain
</p>
<p>π
</p>
<p>a
</p>
<p>(
lna + i π
</p>
<p>2
</p>
<p>)
= 2
</p>
<p>&int; &infin;
</p>
<p>ǫ
</p>
<p>lnρ
</p>
<p>ρ2 + a2 dρ + iπ
&int; &infin;
</p>
<p>ǫ
</p>
<p>dρ
</p>
<p>ρ2 + a2 .
</p>
<p>It can also easily be shown that
&int;&infin;
</p>
<p>0 dρ/(ρ
2 + a2) = π/(2a). Thus, in the
</p>
<p>limit ǫ &rarr; 0, we get I = π
2a
</p>
<p>lna. The sign of a is irrelevant because it ap-
</p>
<p>pears as a square in the integral. Thus, we can write
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>lnx
</p>
<p>x2 + a2 dx =
π
</p>
<p>2|a| ln |a|, a �= 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>372 12 Advanced Topics
</p>
<p>12.3 Analytic Continuation
</p>
<p>Analytic functions have certain unique properties, some of which we have
already noted. For instance, the Cauchy integral formula gives the value of
an analytic function inside a simple closed contour once its value on the
contour is known. We have also seen that we can deform the contours of
integration as long as we do not encounter any singularities of the function.
</p>
<p>Combining these two properties and assuming that f :C&rarr;C is analytic
within a region S &sub; C, we can ask the following question: Is it possible to
extend f beyond S? We shall see in this section that the answer is yes in
many cases of interest.3 First consider the following:
</p>
<p>Theorem 12.3.1 Let f1, f2 : C&rarr; C be analytic in a region S. If f1 = f2equal on a piece, equal
all over in a neighborhood of a point z &isin; S, or for a segment of a curve in S, then
</p>
<p>f1 = f2 for all z &isin; S.
</p>
<p>Proof Let g = f1 &minus; f2, and U = {z &isin; S | g(z) = 0}. Then U is a subset of
S that includes the neighborhood of z (or the line segment) in which f1 =
f2. If U is the entire region S, we are done. Otherwise, U has a boundary
beyond which g(z) �= 0. Since all points within the boundary satisfy g(z)=
0, and since g is continuous (more than that, it is analytic) on S, g must
vanish also on the boundary. But the boundary points are not isolated: Any
small circle around any one of them includes points of U as well as points
outside U . Thus, g must vanish on a neighborhood of any boundary point,
implying that g vanishes for some points outside U . This contradicts our
assumption. Thus, U must include the entire region S. �
</p>
<p>A consequence of this theorem is the following corollary.
</p>
<p>Corollary 12.3.2 The behavior of a function that is analytic in a region
S &sub; C is completely determined by its behavior in a (small) neighborhood
of an arbitrary point in that region.
</p>
<p>This process of determining the behavior of an analytic function outside
the region in which it was originally defined is called analytic continu-analytic continuation
ation. Although there are infinitely many ways of analytically continuing
beyond regions of definition, the values of all functions obtained as a result
of diverse continuations are the same at any given point. This follows from
Theorem 12.3.1.
</p>
<p>Let f1, f2 : C&rarr; C be analytic in regions S1 and S2, respectively. Sup-
pose that f1 and f2 have different functional forms in their respective re-
gions of analyticity. If there is an overlap between S1 and S2 and if f1 = f2
within that overlap, then the (unique) analytic continuation of f1 into S2
must be f2, and vice versa. In fact, we may regard f1 and f2 as a single
</p>
<p>3Provided that S is not discrete (countable). (See [Lang 85, p. 91].)</p>
<p/>
</div>
<div class="page"><p/>
<p>12.3 Analytic Continuation 373
</p>
<p>Fig. 12.7 The function defined in the smaller circle is continued analytically into the
larger circle
</p>
<p>function f :C&rarr;C such that
</p>
<p>f (z)=
{
f1(z) when z &isin; S1,
f2(z) when z &isin; S2.
</p>
<p>Clearly, f is analytic for the combined region S = S1 &cup;S2. We then say that
f1 and f2 are analytic continuations of one another.
</p>
<p>Example 12.3.3 Consider the function f1(z)=
&sum;&infin;
</p>
<p>n=0 z
n, which is analytic
</p>
<p>for |z|&lt; 1. We have seen that it converges to 1/(1&minus; z) for |z|&lt; 1. Thus, we
have f1(z)= 1/(1 &minus; z) when |z|&lt; 1, and f1 is not defined for |z|&gt; 1.
</p>
<p>Now let us consider a second function,
</p>
<p>f2(z)=
&infin;&sum;
</p>
<p>n=0
</p>
<p>(
3
</p>
<p>5
</p>
<p>)n+1(
z+ 2
</p>
<p>3
</p>
<p>)n
,
</p>
<p>which converges for |z+ 23 |&lt; 53 . To see what it converges to, we note that
</p>
<p>f2(z)=
3
</p>
<p>5
</p>
<p>&infin;&sum;
</p>
<p>n=0
</p>
<p>[
3
</p>
<p>5
</p>
<p>(
z+ 2
</p>
<p>3
</p>
<p>)]n
.
</p>
<p>Thus,
</p>
<p>f2(z)=
3
5
</p>
<p>1 &minus; 35 (z+ 23 )
= 1
</p>
<p>1 &minus; z when
∣∣∣∣z+
</p>
<p>2
</p>
<p>3
</p>
<p>∣∣∣∣&lt;
5
</p>
<p>3
.
</p>
<p>We observe that although f1(z) and f2(z) have different series representa-
tions in the two overlapping regions (see Fig. 12.7), they represent the same
function, f (z)= 1/(1 &minus; z). We can therefore write
</p>
<p>f (z)=
{
f1(z) when |z|&lt; 1,
f2(z) when |z+ 23 |&lt; 53 ,</p>
<p/>
</div>
<div class="page"><p/>
<p>374 12 Advanced Topics
</p>
<p>Fig. 12.8 The functions f1 and f2 are analytic continuations of each other: f1 analyt-
ically continues f2 into the right half-plane, and f2 analytically continues f1 into the
semicircle in the left half-plane
</p>
<p>and f1 and f2 are analytic continuations of one another. In fact, f (z) =
1/(1 &minus; z) is the analytic continuation of both f1 and f2 for all of C except
z= 1. Figure 12.7 shows Si , the region of definition of fi , for i = 1,2.
</p>
<p>Example 12.3.4 The function f1(z)=
&int;&infin;
</p>
<p>0 e
&minus;ztdt exists only if Re(z) &gt; 0,
</p>
<p>in which case f1(z)= 1/z. Its region of definition S1 is shown in Fig. 12.8
and is simply the right half-plane.
</p>
<p>Now we define f2 by a geometric series: f2(z) = i
&sum;&infin;
</p>
<p>n=0[(z + i)/i]n
where |z+ i|&lt; 1. This series converges, within its circle of convergence S2,
to
</p>
<p>i
1
</p>
<p>1 &minus; (z+ i)/i =
1
</p>
<p>z
.
</p>
<p>Thus, we have
</p>
<p>1
</p>
<p>z
=
{
f1(z) when z &isin; S1,
f2(z) when z &isin; S2.
</p>
<p>The two functions are analytic continuations of one another, and f (z)= 1/z
is the analytic continuation of both f1 and f2 for all z &isin;C except z= 0.
</p>
<p>12.3.1 The Schwarz Reflection Principle
</p>
<p>A result that is useful in some physical applications is referred to as a dis-
persion relation. To derive such a relation we need to know the behavior
of analytic functions on either side of the real axis. This is found using the
Schwarz reflection principle, for which we need the following result.
</p>
<p>Proposition 12.3.5 Let fi be analytic throughout Si , where i = 1,2. Let B
be the boundary between S1 and S2 (Fig. 12.9) and assume that f1 and f2
are continuous on B and coincide there. Then the two functions are analytic</p>
<p/>
</div>
<div class="page"><p/>
<p>12.3 Analytic Continuation 375
</p>
<p>Fig. 12.9 (a) Regions S1 and S2 separated by the boundary B and the contour C. (b) The
contour C splits up into C1 and C2
</p>
<p>continuations of one another and together they define a (unique) function
</p>
<p>f (z)=
{
f1(z) when z &isin; S1 &cup;B,
f2(z) when z &isin; S2 &cup;B,
</p>
<p>which is analytic throughout the entire region S1 &cup; S2 &cup;B .
</p>
<p>Proof The proof consists of showing that the function integrates to zero
along any closed curve in S1 &cup; S2 &cup;B . Once this is done, one can use Mor-
era&rsquo;s theorem to conclude analyticity. The case when the closed curve is
entirely in either S1 or S2 is trivial. When the curve is partially in S1 and
partially in S2 the proof becomes only slightly more complicated, because
one has to split up the contour C into C1 and C2 of Fig. 12.9(b). The details
are left as an exercise. �
</p>
<p>Theorem 12.3.6 (Schwarz reflection principle) Let f be a function that is Schwarz reflection
principleanalytic in a region S that has a segment of the real axis as part of its
</p>
<p>boundaryB . If f (z) is real whenever z is real, then the analytic continuation
g of f into S&lowast; (the mirror image of S with respect to the real axis) exists
and is given by
</p>
<p>g(z)=
(
f
(
z&lowast;
))&lowast; &equiv; f &lowast;
</p>
<p>(
z&lowast;
)
, where z &isin; S&lowast;.
</p>
<p>Proof First, we show that g is analytic in S&lowast;. Let
</p>
<p>f (z)&equiv; u(x, y)+ iv(x, y), g(z)&equiv;U(x,y)+ iV (x, y).
</p>
<p>Then f (z&lowast;)= f (x,&minus;y)= u(x,&minus;y)+ iv(x,&minus;y) and g(z)= f &lowast;(z&lowast;) imply
that U(x,y)= u(x,&minus;y) and V (x, y)=&minus;v(x,&minus;y). Therefore,
</p>
<p>&part;U
</p>
<p>&part;x
= &part;u
</p>
<p>&part;x
= &part;v
</p>
<p>&part;y
=&minus; &part;v
</p>
<p>&part;(&minus;y) =
&part;V
</p>
<p>&part;y
,</p>
<p/>
</div>
<div class="page"><p/>
<p>376 12 Advanced Topics
</p>
<p>Fig. 12.10 The contour used for dispersion relations
</p>
<p>&part;U
</p>
<p>&part;y
=&minus;&part;u
</p>
<p>&part;y
= &part;v
</p>
<p>&part;x
=&minus;&part;V
</p>
<p>&part;x
.
</p>
<p>These are the Cauchy-Riemann conditions for g(z). Thus, g is analytic.
Next, we note that f (x,0)= g(x,0), implying that f and g agree on the
</p>
<p>real axis. Proposition 12.3.5 then implies that f and g are analytic continu-
ations of one another. �
</p>
<p>It follows from this theorem that there exists an analytic function h such
that
</p>
<p>h(z)=
{
f (z) when z &isin; S,
g(z) when z &isin; S&lowast;.
</p>
<p>We note that h(z&lowast;)= g(z&lowast;)= f &lowast;(z)= h&lowast;(z).
</p>
<p>12.3.2 Dispersion Relations
</p>
<p>Let f be analytic throughout the complex plane except at a cut along the
real axis extending from x0 to infinity. For a point z not on the x-axis, the
Cauchy integral formula gives f (z)= (2πi)&minus;1
</p>
<p>&int;
C
f (ξ) dξ/(ξ &minus; z) where C
</p>
<p>is the contour shown in Fig. 12.10.
We assume that f drops to zero fast enough that the contribution from
</p>
<p>the large circle tends to zero. The reader may show that the contribution
from the small half-circle around x0 also vanishes. Then
</p>
<p>f (z)= 1
2πi
</p>
<p>[&int; &infin;+iǫ
</p>
<p>x0+iǫ
</p>
<p>f (ξ)
</p>
<p>ξ &minus; z dξ &minus;
&int; &infin;&minus;iǫ
</p>
<p>x0&minus;iǫ
</p>
<p>f (ξ)
</p>
<p>ξ &minus; z dξ
]
</p>
<p>= 1
2πi
</p>
<p>[&int; &infin;
</p>
<p>x0
</p>
<p>f (x + iǫ)
x &minus; z+ iǫ dx &minus;
</p>
<p>&int; &infin;
</p>
<p>x0
</p>
<p>f (x &minus; iǫ)
x &minus; z&minus; iǫ dx
</p>
<p>]
.
</p>
<p>Since z is not on the real axis, we can ignore the iǫ terms in the denomina-
tors, so that f (z) = (2πi)&minus;1
</p>
<p>&int;&infin;
x0
</p>
<p>[f (x + iǫ)&minus; f (x &minus; iǫ)]dx/(x &minus; z). The
Schwarz reflection principle in the form f &lowast;(z)= f (z&lowast;) can now be used to</p>
<p/>
</div>
<div class="page"><p/>
<p>12.3 Analytic Continuation 377
</p>
<p>yield
</p>
<p>f (x + iǫ)&minus; f (x &minus; iǫ)= f (x + iǫ)&minus; f &lowast;(x + iǫ)= 2i Im
[
f (x + iǫ)
</p>
<p>]
.
</p>
<p>The final result is
</p>
<p>f (z)= 1
π
</p>
<p>&int; &infin;
</p>
<p>x0
</p>
<p>Im[f (x + iǫ)]
x &minus; z dx.
</p>
<p>This is one form of a dispersion relation. It expresses the value of a function dispersion relation
at any point of the cut complex plane in terms of an integral of the imaginary
part of the function on the upper edge of the cut.
</p>
<p>When there are no residues in the UHP, we can obtain other forms of
dispersion relations by equating the real and imaginary parts of Eq. (11.11).
The result is
</p>
<p>Re
[
f (x0)
</p>
<p>]
=&plusmn; 1
</p>
<p>π
P
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>Im[f (x)]
x &minus; x0
</p>
<p>dx,
</p>
<p>Im
[
f (x0)
</p>
<p>]
=∓ 1
</p>
<p>π
P
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>Re[f (x)]
x &minus; x0
</p>
<p>dx,
</p>
<p>(12.6)
</p>
<p>where the upper (lower) sign corresponds to placing the small semicircle
around x0 in the UHP (LHP). The real and imaginary parts of f , as related
by Eq. (12.6), are sometimes said to be the Hilbert transform of one an- Hilbert transform
other.
</p>
<p>In some applications, the imaginary part of f is an odd function of its
argument. Then the first equation in (12.6) can be written as
</p>
<p>Re
[
f (x0)
</p>
<p>]
=&plusmn; 2
</p>
<p>π
P
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>x Im[f (x)]
x2 &minus; x20
</p>
<p>dx.
</p>
<p>To arrive at dispersion relations, the following condition must hold:
</p>
<p>lim
R&rarr;&infin;
</p>
<p>R
∣∣f
</p>
<p>(
Reiθ
</p>
<p>)∣∣= 0,
</p>
<p>where R is the radius of the large semicircle in the UHP (or LHP). If f does
not satisfy this prerequisite, it is still possible to obtain a dispersion relation
called a dispersion relation with one subtraction. This can be done by dispersion relation with
</p>
<p>one subtractionintroducing an extra factor of x in the denominator of the integrand. We
start with Eq. (11.15), confining ourselves to the UHP and assuming that
there are no poles there, so that the sum over residues is dropped:
</p>
<p>f (x2)&minus; f (x1)
x2 &minus; x1
</p>
<p>= 1
iπ
</p>
<p>P
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>f (x)
</p>
<p>(x &minus; x1)(x &minus; x2)
dx.
</p>
<p>The reader may check that by equating the real and imaginary parts on both
sides, letting x1 = 0 and x2 = x0, and changing x to &minus;x in the first half of
the interval of integration, we obtain
</p>
<p>Re[f (x0)]
x0
</p>
<p>= Re[f (0)]
x0
</p>
<p>+ 1
π
</p>
<p>[
P
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>Im[f (&minus;x)]
x(x + x0)
</p>
<p>dx + P
&int; &infin;
</p>
<p>0
</p>
<p>Im[f (x)]
x(x &minus; x0)
</p>
<p>dx
</p>
<p>]
.</p>
<p/>
</div>
<div class="page"><p/>
<p>378 12 Advanced Topics
</p>
<p>For the case where Im[f (&minus;x)] = &minus; Im[f (x)], this equation yields
</p>
<p>Re
[
f (x0)
</p>
<p>]
= Re
</p>
<p>[
f (0)
</p>
<p>]
+ 2x
</p>
<p>2
0
</p>
<p>π
P
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>Im[f (x)]
x(x2 &minus; x20)
</p>
<p>dx. (12.7)
</p>
<p>Example 12.3.7 In optics, it has been shown that the imaginary part of the
forward-scattering light amplitude with frequency ω is related, by the so-
called optical theorem, to the total cross section for the absorption of lightoptical theorem
of that frequency:
</p>
<p>Im
[
f (ω)
</p>
<p>]
= ω
</p>
<p>4π
σtot(ω).
</p>
<p>Substituting this in Eq. (12.7) yields
</p>
<p>Re
[
f (ω0)
</p>
<p>]
= Re
</p>
<p>[
f (0)
</p>
<p>]
+ ω
</p>
<p>2
0
</p>
<p>2π2
P
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>σtot(ω)
</p>
<p>ω2 &minus;ω20
dω. (12.8)
</p>
<p>Thus, the real part of the (coherent) forward scattering of light, that is, the
real part of the index of refraction, can be computed from Eq. (12.8) by
either measuring or calculating σtot(ω), the simpler quantity describing the
absorption of light in the medium. Equation (12.8) is the original Kramers-Kramers-Kronig relation
Kronig relation.
</p>
<p>12.4 The Gamma and Beta Functions
</p>
<p>We have already encountered the gamma function. In this section, we de-
rive some useful relations involving the gamma function and the closely
related beta function. The gamma function is a generalization of the facto-
rial function&mdash;which is defined only for positive integers&mdash;to the system of
complex numbers. By differentiating the integral
</p>
<p>I (α)&equiv;
&int; &infin;
</p>
<p>0
e&minus;αtdt = 1/α
</p>
<p>with respect to α repeatedly and setting α = 1 at the end, we get&int;&infin;
0 t
</p>
<p>ne&minus;tdt = n!. This fact motivates the generalization
</p>
<p>Ŵ(z)&equiv;
&int; &infin;
</p>
<p>0
tz&minus;1e&minus;tdt for Re(z) &gt; 0, (12.9)
</p>
<p>where Ŵ is called the gamma (or factorial) function. It is also called Euler&rsquo;sgamma function defined
integral of the second kind. It is clear from its definition that
</p>
<p>Ŵ(n+ 1)= n! (12.10)
</p>
<p>if n is a positive integer. The restriction Re(z) &gt; 0 assures the convergence
of the integral.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.4 The Gamma and Beta Functions 379
</p>
<p>An immediate consequence of Eq. (12.9) is obtained by integrating it by
parts:
</p>
<p>Ŵ(z+ 1)= zŴ(z). (12.11)
This also leads to Eq. (12.10) by iteration and the fact that Ŵ(1)= 1.
</p>
<p>Another consequence is the analyticity of Ŵ(z). Differentiating Eq. (12.11)
with respect to z, we obtain
</p>
<p>dŴ(z+ 1)
dz
</p>
<p>= Ŵ(z)+ zdŴ(z)
dz
</p>
<p>.
</p>
<p>Thus, dŴ(z)/dz exists and is finite if and only if dŴ(z + 1)/dz is finite
(recall that z �= 0). The procedure of showing the latter is outlined in Prob-
lem 12.16. Therefore, Ŵ(z) is analytic whenever Ŵ(z + 1) is. To see the
singularities of Ŵ(z), we note that
</p>
<p>Ŵ(z+ n)= z(z+ 1)(z+ 2) &middot; &middot; &middot; (z+ n&minus; 1)Ŵ(z),
</p>
<p>or
</p>
<p>Ŵ(z)= Ŵ(z+ n)
z(z+ 1)(z+ 2) &middot; &middot; &middot; (z+ n&minus; 1) . (12.12)
</p>
<p>The numerator is analytic as long as Re(z + n) &gt; 0, or Re(z) &gt; &minus;n.
Thus, for Re(z) &gt; &minus;n, the singularities of Ŵ(z) are the poles at z =
0,&minus;1,&minus;2, . . . ,&minus;n+ 1. Since n is arbitrary, we conclude that
</p>
<p>Box 12.4.1 Ŵ(z) is analytic at all z &isin;C except at z= 0,&minus;1,&minus;2, . . . ,
where Ŵ(z) has simple poles.
</p>
<p>A useful result is obtained by setting z= 12 in Eq. (12.9):
</p>
<p>Ŵ
</p>
<p>(
1
</p>
<p>2
</p>
<p>)
=&radic;π. (12.13)
</p>
<p>This can be obtained by making the substitution u=&radic;t in the integral.
We can derive an expression for the logarithmic derivative of the gamma
</p>
<p>function that involves an infinite series. To do so, we use Eq. (12.2) noting
that 1/Ŵ(z + 1) is an entire function with simple zeros at {&minus;k}&infin;k=1. Equa-
tion (12.2) gives
</p>
<p>1
</p>
<p>Ŵ(z+ 1) = e
γ z
</p>
<p>&infin;&prod;
</p>
<p>k=1
</p>
<p>(
1 + z
</p>
<p>k
</p>
<p>)
e&minus;z/k,
</p>
<p>where γ is a constant to be determined. Using Eq. (12.11), we obtain
</p>
<p>1
</p>
<p>Ŵ(z)
= zeγ z
</p>
<p>&infin;&prod;
</p>
<p>k=1
</p>
<p>(
1 + z
</p>
<p>k
</p>
<p>)
e&minus;z/k. (12.14)</p>
<p/>
</div>
<div class="page"><p/>
<p>380 12 Advanced Topics
</p>
<p>To determine γ , let z = 1 in Eq. (12.14) and evaluate the resulting prod-
uct numerically. The result is γ = 0.57721566 . . . , the so-called Euler&ndash;Euler-Mascheroni
</p>
<p>constant Mascheroni constant.
Differentiating the logarithm of both sides of Eq. (12.14), we obtain
</p>
<p>d
</p>
<p>dz
ln
[
Ŵ(z)
</p>
<p>]
=&minus;1
</p>
<p>z
&minus; γ +
</p>
<p>&infin;&sum;
</p>
<p>k=1
</p>
<p>(
1
</p>
<p>k
&minus; 1
</p>
<p>z+ k
</p>
<p>)
. (12.15)
</p>
<p>Other properties of the gamma function are derivable from the results
presented here. Those derivations are left as problems.
</p>
<p>The beta function, or Euler&rsquo;s integral of the first kind, is defined forbeta function defined
complex numbers a and b as follows:
</p>
<p>B(a, b)&equiv;
&int; 1
</p>
<p>0
ta&minus;1(1 &minus; t)b&minus;1dt where Re(a),Re(b) &gt; 0. (12.16)
</p>
<p>By changing t to 1/t , we can also write
</p>
<p>B(a, b)&equiv;
&int; &infin;
</p>
<p>1
t&minus;a&minus;b(t &minus; 1)b&minus;1dt. (12.17)
</p>
<p>Since 0 &le; t &le; 1 in Eq. (12.16), we can define θ by t = sin2 θ . This gives
</p>
<p>B(a, b)= 2
&int; π/2
</p>
<p>0
sin2a&minus;1 θ cos2b&minus;1 θ dθ. (12.18)
</p>
<p>This relation can be used to establish a connection between the gamma and
beta functions. We note that
</p>
<p>Ŵ(a)=
&int; &infin;
</p>
<p>0
ta&minus;1e&minus;tdt = 2
</p>
<p>&int; &infin;
</p>
<p>0
x2a&minus;1e&minus;x
</p>
<p>2
dx,
</p>
<p>where in the last step we changed the variable to x =&radic;t . Multiply Ŵ(a) by
Ŵ(b) and express the resulting double integral in terms of polar coordinates
to obtain Ŵ(a)Ŵ(b)= Ŵ(a + b)B(a, b), orgamma function and
</p>
<p>beta function are related
</p>
<p>B(a, b)= B(b, a)= Ŵ(a)Ŵ(b)
Ŵ(a + b) . (12.19)
</p>
<p>Let us now establish the following useful relation:
</p>
<p>Ŵ(z)Ŵ(1 &minus; z)= π
sinπz
</p>
<p>. (12.20)
</p>
<p>With a = z and b= 1&minus;z, and using u= tan θ , Eqs. (12.18) and (12.19) give
</p>
<p>Ŵ(z)Ŵ(1 &minus; z)= B(z,1 &minus; z)= 2
&int; &infin;
</p>
<p>0
</p>
<p>u2z&minus;1
</p>
<p>u2 + 1 du for 0 &lt; Re(z) &lt; 1.
</p>
<p>Using the result obtained in Example 12.2.4, we immediately get Eq. (12.20),
valid for 0 &lt; Re(z) &lt; 1. By analytic continuation we then generalize
Eq. (12.20) to values of z for which both sides are analytic.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.4 The Gamma and Beta Functions 381
</p>
<p>Fig. 12.11 The contour C used in evaluating the reciprocal gamma function
</p>
<p>Example 12.4.2 As an illustration of the use of Eq. (12.20), let us show
that Ŵ(z) can also be written as
</p>
<p>1
</p>
<p>Ŵ(z)
= 1
</p>
<p>2πi
</p>
<p>&int;
</p>
<p>C
</p>
<p>et
</p>
<p>tz
dt, (12.21)
</p>
<p>where C is the contour shown in Fig. 12.11. From Eqs. (12.9) and (12.20) it
follows that
</p>
<p>1
</p>
<p>Ŵ(z)
= sinπz
</p>
<p>π
Ŵ(1 &minus; z)= sinπz
</p>
<p>π
</p>
<p>&int; &infin;
</p>
<p>0
e&minus;rr&minus;zdr
</p>
<p>= e
iπz &minus; e&minus;iπz
</p>
<p>2πi
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>e&minus;r
</p>
<p>rz
dr.
</p>
<p>The contour integral of Eq. (12.21) can be evaluated by noting that above
the real axis, t = reiπ = &minus;r , below it t = re&minus;iπ = &minus;r , and, as the reader
may check, that the contribution from the small circle at the origin is zero;
so
</p>
<p>&int;
</p>
<p>C
</p>
<p>et
</p>
<p>tz
dt =
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>e&minus;r
</p>
<p>(reiπ )z
(&minus;dr)+
</p>
<p>&int; 0
</p>
<p>&infin;
</p>
<p>e&minus;r
</p>
<p>(re&minus;iπ )z
(&minus;dr)
</p>
<p>=&minus;e&minus;iπz
&int; &infin;
</p>
<p>0
</p>
<p>e&minus;r
</p>
<p>rz
dr + eiπz
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>e&minus;r
</p>
<p>rz
dr.
</p>
<p>Comparison with the last equation above yields the desired result.
</p>
<p>Another useful relation can be obtained by combining Eqs. (12.11) and
(12.20):
</p>
<p>Ŵ(z)Ŵ(1 &minus; z)= Ŵ(z)(&minus;z)Ŵ(&minus;z)= π
sinπz
</p>
<p>.
</p>
<p>Thus,
</p>
<p>Ŵ(z)Ŵ(&minus;z)=&minus; π
z sinπz
</p>
<p>. (12.22)
</p>
<p>Once we know Ŵ(x) for positive values of real x, we can use Eq. (12.22)
to find Ŵ(x) for x &lt; 0. Thus, for instance, Ŵ( 12 ) =
</p>
<p>&radic;
π gives Ŵ(&minus; 12 ) =
</p>
<p>&minus;2&radic;π . Equation (12.22) also shows that the gamma function has simple
poles wherever z is a negative integer.</p>
<p/>
</div>
<div class="page"><p/>
<p>382 12 Advanced Topics
</p>
<p>12.5 Method of Steepest Descent
</p>
<p>It is shown in statistical mechanics ([Hill 87, pp. 150&ndash;152]) that the partition
function, which generates all the thermodynamical quantities, can be written
as a contour integral. Debye found a very elegant technique of approximat-
ing this contour integral, which we investigate in this section. Consider the
integral
</p>
<p>I (α)&equiv;
&int;
</p>
<p>C
</p>
<p>eαf (z)g(z) dz (12.23)
</p>
<p>where |α| is large and f and g are analytic in some region of C containing
the contour C. Since this integral occurs frequently in physical applications,
it would be helpful if we could find a general approximation for it that is
applicable for all f and g. The fact that |α| is large will be of great help. By
redefining f (z), if necessary, we can assume that α = |α|ei arg(α) is real and
positive [absorb ei arg(α) into the function f (z) if need be].
</p>
<p>The exponent of the integrand can be written as
</p>
<p>αf (z)= αu(x, y)+ iαv(x, y).
</p>
<p>Since α is large and positive, we expect the exponential to be the largest at
the maximum of u(x, y). Thus, if we deform the contour so that it passes
through a point z0 at which u(x, y) is maximum, the contribution to the
integral may come mostly from the neighborhood of z0. This opens up the
possibility of expanding the exponent about z0 and keeping the lowest terms
in the expansion, which is what we are after. There is one catch, however.
Because of the largeness of α, the imaginary part of αf in the exponent
will oscillate violently as v(x, y) changes even by a small amount. This
oscillation can make the contribution of the real part of f (z0) negligibly
small and render the whole procedure useless. Thus, we want to tame the
variation of exp[iv(x, y)] by making v(x, y) vary as slowly as possible.
A necessary condition is for the derivative of v to vanish at z0. This and the
fact that the real part is to have a maximum at z0 lead to
</p>
<p>&part;u
</p>
<p>&part;x
+ i &part;v
</p>
<p>&part;x
= df
</p>
<p>dz
</p>
<p>∣∣∣∣
z0
</p>
<p>= 0. (12.24)
</p>
<p>However, we do not stop here but demand that the imaginary part of f be
constant along the deformed contour: Im[f (z)] = Im[f (z0)] or v(x, y) =
v(x0, y0).
</p>
<p>Equation (12.24) and the Cauchy-Riemann conditions imply that &part;u/&part;x =
0 = &part;u/&part;y at z0. Thus, it might appear that z0 is a maximum (or minimum)
of the surface described by the function u(x, y). This is not true: For the
surface to have a maximum (minimum), both second derivatives, &part;2u/&part;x2
</p>
<p>and &part;2u/&part;y2, must be negative (positive). But that is impossible because
u(x, y) is harmonic&mdash;the sum of these two derivatives is zero. Recall that
a point at which the derivatives vanish but that is neither a maximum nor
a minimum is called a saddle point. That is why the procedure described
below is sometimes called the saddle point approximation.
</p>
<p>saddle point
</p>
<p>approximation</p>
<p/>
</div>
<div class="page"><p/>
<p>12.5 Method of Steepest Descent 383
</p>
<p>We are interested in values of z close to z0. So let us expand f (z) in
a Taylor series about z0, use Eq. (12.24), and keep terms only up to the
second, to obtain
</p>
<p>f (z)= f (z0)+
1
</p>
<p>2
(z&minus; z0)2f &prime;&prime;(z0). (12.25)
</p>
<p>Let us assume that f &prime;&prime;(z0) �= 0, and define
</p>
<p>z&minus; z0 = r1eiθ1 and
1
</p>
<p>2
f &prime;&prime;(z0)= r2eiθ2 (12.26)
</p>
<p>and substitute in the above expansion to obtain
</p>
<p>f (z)&minus; f (z0)= r21 r2ei(2θ1+θ2), (12.27)
</p>
<p>or
</p>
<p>Re
[
f (z)&minus; f (z0)
</p>
<p>]
= r21 r2 cos(2θ1 + θ2),
</p>
<p>Im
[
f (z)&minus; f (z0)
</p>
<p>]
= r21 r2 sin(2θ1 + θ2).
</p>
<p>(12.28)
</p>
<p>The constancy of Im[f (z)] implies that sin(2θ1+θ2)= 0, or 2θ1+θ2 = nπ .
Thus, for θ1 =&minus;θ2/2 + nπ/2 where n= 0,1,2,3, the imaginary part of f
is constant. The angle θ2 is determined by the second equation in (12.26).
Once we determine n, the path of saddle point integration will be specified.
</p>
<p>To get insight into this specification, consider z&minus; z0 = r1ei(&minus;θ2/2+nπ/2),
and eliminate r1 from its real and imaginary parts to obtain
</p>
<p>y &minus; y0 =
[
</p>
<p>tan
</p>
<p>(
nπ
</p>
<p>2
&minus; θ2
</p>
<p>2
</p>
<p>)]
(x &minus; x0).
</p>
<p>This is the equation of a line passing through z0 = (x0, y0) and making
an angle of θ1 = (nπ &minus; θ2)/2 with the real axis. For n = 0,2 we get one
line, and for n = 1,3 we get another that is perpendicular to the first (see
Fig. 12.12). It is to be emphasized that along both these lines the imaginary
part of f (z) remains constant. To choose the correct line, we need to look at
the real part of the function. Also note that these &ldquo;lines&rdquo; are small segments
of (or tangents to) the deformed contour at z0.
</p>
<p>We are looking for directions along which Re(f ) goes through a relative
maximum at z0. In fact, we are after a path on which the function decreases
maximally. This occurs when Re[f (z)]&minus;Re[f (z0)] take the largest negative
</p>
<p>method of steepest
</p>
<p>descent
value. Equation (12.28) determines such a path: It is that path on which
cos(2θ1 + θ2) = &minus;1, or when n = 1,3. There is only one such path in the
region of interest, and the procedure is uniquely determined.4 Because the
descent from the maximum value at z0 is maximum along such a path, this
procedure is called the method of steepest descent.
</p>
<p>4The angle θ1 is still ambiguous by π , because n can be 1 or 3. However, by a suitable
sign convention described below, we can remove this ambiguity.</p>
<p/>
</div>
<div class="page"><p/>
<p>384 12 Advanced Topics
</p>
<p>Fig. 12.12 A segment of the contour C0 in the vicinity of z0. The lines mentioned in the
text are small segments of the contour C0 centered at z0
</p>
<p>Now that we have determined the contour, let us approximate the integral.
Substituting 2θ1 + θ2 = π,3π in Eq. (12.27), we get
</p>
<p>f (z)&minus; f (z0)=&minus;r21 r2 &equiv;&minus;t2 =
1
</p>
<p>2
(z&minus; z0)2f &prime;&prime;(z0). (12.29)
</p>
<p>Using this in Eq. (12.23) yields
</p>
<p>I (α)&asymp;
&int;
</p>
<p>C0
</p>
<p>eα[f (z0)&minus;t
2]g(z) dz= eαf (z0)
</p>
<p>&int;
</p>
<p>C0
</p>
<p>e&minus;αt
2
g(z) dz, (12.30)
</p>
<p>where C0 is the deformed contour passing through z0.
To proceed, we need to solve for z in terms of t . From Eq. (12.29) we
</p>
<p>have
</p>
<p>(z&minus; z0)2 =&minus;
2
</p>
<p>f &prime;&prime;(z0)
t2 =&minus; t
</p>
<p>2
</p>
<p>r2
e&minus;iθ2 .
</p>
<p>Therefore, |z&minus;z0| = |t |/
&radic;
r2, or z&minus;z0 = (|t |/
</p>
<p>&radic;
r2)e
</p>
<p>iθ1 , by the first equation
of (12.26). Let us agree that for t &gt; 0, the point z on the contour will move in
the direction that makes an angle of 0 &le; θ1 &lt; π , and that t &lt; 0 corresponds
to the opposite direction. This convention removes the remaining ambiguity
of the angle θ1, and gives
</p>
<p>z= z0 +
t&radic;
r2
eiθ1, 0 &le; θ1 &lt; π. (12.31)
</p>
<p>Using the Taylor expansion of g(z) about z0, we can write
</p>
<p>g(z) dz=
{ &infin;&sum;
</p>
<p>n=0
</p>
<p>tn
</p>
<p>r
n/2
2 n!
</p>
<p>einθ1g(n)(z0)
</p>
<p>}
eiθ1&radic;
r2
</p>
<p>dt
</p>
<p>=
&infin;&sum;
</p>
<p>n=0
</p>
<p>tn
</p>
<p>r
(n+1)/2
2 n!
</p>
<p>ei(n+1)θ1g(n)(z0) dt,</p>
<p/>
</div>
<div class="page"><p/>
<p>12.5 Method of Steepest Descent 385
</p>
<p>and substituting this in Eq. (12.30) yields
</p>
<p>I (α)&asymp; eαf (z0)
&int;
</p>
<p>C0
</p>
<p>e&minus;αt
2
</p>
<p>{ &infin;&sum;
</p>
<p>n=0
</p>
<p>tn
</p>
<p>r
(n+1)/2
2 n!
</p>
<p>ei(n+1)θ1g(n)(z0)
</p>
<p>}
dt
</p>
<p>= eαf (z0)
&infin;&sum;
</p>
<p>n=0
</p>
<p>ei(n+1)θ1
</p>
<p>r
(n+1)/2
2 n!
</p>
<p>g(n)(z0)
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
e&minus;αt
</p>
<p>2
tn dt. (12.32)
</p>
<p>The extension of the integral limits to infinity does not alter the result sig-
nificantly because α is assumed large and positive. The integral in the sum
is zero for odd n. When n is even, we make the substitution u = αt2 and
show that
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
e&minus;αt
</p>
<p>2
tndt = α&minus;(n+1)/2Ŵ
</p>
<p>[
(n+ 1)/2
</p>
<p>]
.
</p>
<p>With n= 2k, and using r2 = |f &prime;&prime;(z0)|/2, the sum becomes
</p>
<p>I (α)&asymp; eαf (z0)
&infin;&sum;
</p>
<p>k=0
</p>
<p>2k+1/2ei(2k+1)θ1
</p>
<p>|f &prime;&prime;(z0)|k+1/2(2k)!
g(2k)(z0)Ŵ
</p>
<p>(
k+ 1
</p>
<p>2
</p>
<p>)
α&minus;k&minus;1/2.
</p>
<p>(12.33)
This is called the asymptotic expansion of I (α). In most applications, only asymptotic expansion
</p>
<p>of I (α)the first term of the above series is retained, giving
</p>
<p>I (α)&asymp; eαf (z0)
&radic;
</p>
<p>2π
</p>
<p>α
</p>
<p>eiθ1g(z0)&radic;
|f &prime;&prime;(z0)|
</p>
<p>. (12.34)
</p>
<p>Example 12.5.1 Let us approximate the integral
</p>
<p>I (α)&equiv; Ŵ(α + 1)=
&int; &infin;
</p>
<p>0
e&minus;zzαdz,
</p>
<p>where α is a positive real number. First, we must rewrite the integral in the
form of Eq. (12.23). We can do this by noting that zα = eα ln z. Thus, we
have
</p>
<p>I (α)=
&int; &infin;
</p>
<p>0
eα ln z&minus;zdz=
</p>
<p>&int; &infin;
</p>
<p>0
eα(ln z&minus;z/α)dz,
</p>
<p>and we identify f (z)= ln z&minus; z/α and g(z)= 1. The saddle point is found
from f &prime;(z)= 0 or z0 = α. Furthermore, from
</p>
<p>1
</p>
<p>2
f &prime;&prime;(z0)=
</p>
<p>1
</p>
<p>2
</p>
<p>(
&minus; 1
α2
</p>
<p>)
= 1
</p>
<p>2α2
eiπ &rArr; θ2 = π
</p>
<p>and 2θ1 + θ2 = π,3π , as well as the condition 0 &le; θ1 &lt; π , we conclude that
θ1 = 0.
</p>
<p>Substitution in Eq. (12.34) yields
</p>
<p>Ŵ(α + 1)&asymp; eαf (z0)
&radic;
</p>
<p>2π
</p>
<p>α
</p>
<p>1&radic;
1/α2
</p>
<p>=
&radic;
</p>
<p>2παeα(lnα&minus;1) =
&radic;
</p>
<p>2πe&minus;ααα+1/2, (12.35)</p>
<p/>
</div>
<div class="page"><p/>
<p>386 12 Advanced Topics
</p>
<p>Fig. 12.13 The contour for the evaluation of the Hankel function of the first kind
</p>
<p>which is called the Stirling approximation. For α = n, a positive integer,Stirling approximation
this yields the useful result
</p>
<p>n! &asymp;
&radic;
</p>
<p>2π e&minus;nnn+1/2
</p>
<p>with the approximation getting better and better for larger and larger n.
</p>
<p>Example 12.5.2 The Hankel function of the first kind is defined asHankel function of the
first kind
</p>
<p>H (1)ν (α)&equiv;
1
</p>
<p>iπ
</p>
<p>&int;
</p>
<p>C
</p>
<p>e(α/2)(z&minus;1/z)
dz
</p>
<p>zν+1
,
</p>
<p>where C is the contour shown in Fig. 12.13.
We want to find the asymptotic expansion of this function, choosing the
</p>
<p>branch of the function in which &minus;π &lt; θ &lt; π .
We identify f (z) = 12 (z &minus; 1/z) and g(z) = z&minus;ν&minus;1. Next, the stationary
</p>
<p>points of f are calculated:
</p>
<p>df
</p>
<p>dz
= 1
</p>
<p>2
+ 1
</p>
<p>2z2
= 0 &rArr; z0 =&plusmn;i.
</p>
<p>The contour of integration suggests the saddle point z0 = +i. The sec-
ond derivative evaluated at the saddle point gives f &prime;&prime;(z0)=&minus;1/z30 =&minus;i =
e&minus;iπ/2, or θ2 = &minus;π/2. This, and the convention 0 &le; θ1 &lt; π , force us to
choose θ1 = 3π/4. Substituting this in Eq. (12.34) and noting that f (i)= i
and |f &prime;&prime;(z0)| = 1, we obtain
</p>
<p>H (1)ν (α)&equiv;
1
</p>
<p>iπ
I (α)&asymp; 1
</p>
<p>iπ
eαi
</p>
<p>&radic;
2π
</p>
<p>α
ei3π/4i&minus;ν&minus;1 =
</p>
<p>&radic;
2
</p>
<p>απ
ei(α&minus;νπ/2&minus;π/4),
</p>
<p>where we have used i&minus;ν&minus;1 = e&minus;i(ν+1)π/2.
</p>
<p>Although Eq. (12.34) is adequate for most applications, we shall have
occasions to demand a better approximation. One may try to keep higher-
order terms of Eq. (12.33), but that infinite sum is in reality inconsistent.
The reason is that in the product g(z) dz, we kept only the first power of
t in the expansion of z. To restore consistency, let us expand z(t) as well.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.5 Method of Steepest Descent 387
</p>
<p>Suppose
</p>
<p>z&minus; z0 =
&infin;&sum;
</p>
<p>m=1
bmt
</p>
<p>m &rArr; dz=
&infin;&sum;
</p>
<p>m=0
(m+ 1)bm+1tmdt,
</p>
<p>so that
</p>
<p>g(z) dz=
&infin;&sum;
</p>
<p>n=0
</p>
<p>tn
</p>
<p>r
n/2
2 n!
</p>
<p>einθ1g(n)(z0)
</p>
<p>&infin;&sum;
</p>
<p>m=0
(m+ 1)bm+1tmdt
</p>
<p>=
&infin;&sum;
</p>
<p>m,n=0
</p>
<p>einθ1
</p>
<p>r
n/2
2 n!
</p>
<p>(m+ 1)bm+1g(n)(z0)tm+ndt.
</p>
<p>Now introduce l =m+ n and note that the summation over n goes up to l.
This gives
</p>
<p>g(z) dz=
&infin;&sum;
</p>
<p>l=0
</p>
<p>l&sum;
</p>
<p>n=0
</p>
<p>einθ1
</p>
<p>r
n/2
2 n!
</p>
<p>(l &minus; n+ 1)bl&minus;n+1g(n)(z0)
︸ ︷︷ ︸
</p>
<p>&equiv;al
</p>
<p>t ldt =
&infin;&sum;
</p>
<p>l=0
al t
</p>
<p>ldt.
</p>
<p>Substituting this in Eq. (12.30) and changing the contour integration into the
integral from &minus;&infin; to &infin; as before yields
</p>
<p>I (α)&asymp; eαf (z0)
&infin;&sum;
</p>
<p>k=0
a2kα
</p>
<p>&minus;k&minus;1/2Ŵ
(
k+ 1
</p>
<p>2
</p>
<p>)
,
</p>
<p>a2k =
2k&sum;
</p>
<p>n=0
</p>
<p>einθ1
</p>
<p>r
n/2
2 n!
</p>
<p>(2k&minus; n+ 1)b2k&minus;n+1g(n)(z0).
(12.36)
</p>
<p>The only thing left to do is to evaluate bm. We shall not give a general
formula for these coefficients. Instead, we shall calculate the first three of
them. This should reveal to the reader the general method of approximating
them to any order. We have already calculated b1 in Eq. (12.31). To calculate
b2, keep the next-highest term in the expansion of both z and t2. Thus write
</p>
<p>z&minus; z0 = b1t + b2t2, t2 =&minus;
1
</p>
<p>2
f &prime;&prime;(z0)(z&minus; z0)2 &minus;
</p>
<p>1
</p>
<p>6
f &prime;&prime;&prime;(z0)(z&minus; z0)3.
</p>
<p>Now substitute the first equation in the second and equate the coefficients of
equal powers of t on both sides. The second power of t gives nothing new:
It merely reaffirms the value of b1. The coefficient of the third power of t is
&minus;b1b2f &prime;&prime;(z0)&minus; 16b31f &prime;&prime;&prime;(z0). Setting this equal to zero gives
</p>
<p>b2 =&minus;
b21f
</p>
<p>&prime;&prime;&prime;(z0)
6f &prime;&prime;(z0)
</p>
<p>= f
&prime;&prime;&prime;(z0)
</p>
<p>3|f &prime;&prime;(z0)|2
e4iθ1 , (12.37)
</p>
<p>where we substituted for b1 from Eq. (12.31) and used 2θ1 + θ2 = π .
To calculate b3, keep one more term in the expansion of both z and t2 to
</p>
<p>obtain
</p>
<p>z&minus; z0 = b1t + b2t2 + b3t3</p>
<p/>
</div>
<div class="page"><p/>
<p>388 12 Advanced Topics
</p>
<p>Fig. 12.14 Contour used for Problem 12.4
</p>
<p>and
</p>
<p>t2 =&minus;1
2
f &prime;&prime;(z0)(z&minus; z0)2 &minus;
</p>
<p>1
</p>
<p>6
f &prime;&prime;&prime;(z0)(z&minus; z0)3 &minus;
</p>
<p>1
</p>
<p>24
f (iv)(z0)(z&minus; z0)4.
</p>
<p>Once again substitute the first equation in the second and equate the coef-
ficients of equal powers of t on both sides. The second and third powers of
t give nothing new. Setting the coefficient of the fourth power of t equal to
zero yields
</p>
<p>b3 = b31
{
</p>
<p>5[f &prime;&prime;&prime;(z0)]2
72[f &prime;&prime;(z0)]2
</p>
<p>&minus; f
(iv)
</p>
<p>24f &prime;&prime;(z0)
</p>
<p>}
</p>
<p>=
&radic;
</p>
<p>2e3iθ1
</p>
<p>12|f &prime;&prime;(z0)|3/2
{
</p>
<p>5[f &prime;&prime;&prime;(z0)]2
3[f &prime;&prime;(z0)]2
</p>
<p>&minus; f
(iv)
</p>
<p>f &prime;&prime;(z0)
</p>
<p>}
. (12.38)
</p>
<p>12.6 Problems
</p>
<p>12.1 Derive Eq. (12.2) from its logarithmic derivative.
</p>
<p>12.2 Show that the point at infinity is not a branch point for f (z) = (z2 &minus;
1)1/2.
</p>
<p>12.3 Find the following integrals, for which 0 �= a &isin;R.
</p>
<p>(a)
&int; &infin;
</p>
<p>0
</p>
<p>lnx
</p>
<p>(x2 + a2)2 dx, (b)
&int; &infin;
</p>
<p>0
</p>
<p>lnx
</p>
<p>(x2 + a2)2&radic;x dx,
</p>
<p>(c)
&int; &infin;
</p>
<p>0
</p>
<p>(lnx)2
</p>
<p>x2 + a2 dx.
</p>
<p>12.4 Use the contour in Fig. 12.14 to evaluate the following integrals.
</p>
<p>(a)
&int; &infin;
</p>
<p>0
</p>
<p>sinax
</p>
<p>sinhx
dx, (b)
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>x cosax
</p>
<p>sinhx
dx.
</p>
<p>12.5 Show that
&int; π
</p>
<p>0 f (sin θ) dθ = 2
&int; π/2
</p>
<p>0 f (sin θ) dθ for an arbitrary func-
tion f defined in the interval [&minus;1,+1].</p>
<p/>
</div>
<div class="page"><p/>
<p>12.6 Problems 389
</p>
<p>12.6 Find the principal value of the integral
&int;&infin;
&minus;&infin; x sinx dx/(x
</p>
<p>2 &minus; x20) and
evaluate
</p>
<p>I =
&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>x sinx
</p>
<p>(x &minus; x0 &plusmn; iǫ)(x + x0 &plusmn; iǫ)
dx
</p>
<p>for the four possible choices of signs.
</p>
<p>12.7 Use analytic continuation, the analyticity of the exponential, hyper-
bolic, and trigonometric functions, and the analogous identities for real z to
prove the following identities.
</p>
<p>(a) ez = cosh z+ sinh z, (b) cosh2 z&minus; sinh2 z= 1,
(c) sin 2z= 2 sin z cos z.
</p>
<p>12.8 Show that the function 1/z2 represents the analytic continuation into
the domain C&minus; {0} (all the complex plane minus the origin) of the function
defined by
</p>
<p>&sum;&infin;
n=0(n+ 1)(z+ 1)n where |z+ 1|&lt; 1.
</p>
<p>12.9 Find the analytic continuation into C&minus; {i,&minus;i} (all the complex plane
except i and &minus;i) of f (z)=
</p>
<p>&int;&infin;
0 e
</p>
<p>&minus;zt sin t dt where Re(z) &gt; 0.
</p>
<p>12.10 Expand f (z) = &sum;&infin;n=0 zn (defined in its circle of convergence) in a
Taylor series about z = a. For what values of a does this expansion permit
the function f (z) to be continued analytically?
</p>
<p>12.11 The two power series
</p>
<p>f1(z)=
&infin;&sum;
</p>
<p>n=1
</p>
<p>zn
</p>
<p>n
and f2(z)= iπ +
</p>
<p>&infin;&sum;
</p>
<p>n=1
(&minus;1)n (z&minus; 2)
</p>
<p>n
</p>
<p>n
</p>
<p>have no common domain of convergence. Show that they are nevertheless
analytic continuations of one another.
</p>
<p>12.12 Prove that the functions defined by the two series
</p>
<p>1 + az+ a2z2 + &middot; &middot; &middot; and 1
1 &minus; z &minus;
</p>
<p>(1 &minus; a)z
(1 &minus; z)2 +
</p>
<p>(1 &minus; a)2z2
(1 &minus; z)3 &minus; &middot; &middot; &middot;
</p>
<p>are analytic continuations of one another.
</p>
<p>12.13 Show that the function f1(z)= 1/(z2 + 1), where z �= &plusmn;i, is the ana-
lytic continuation into C&minus; {i,&minus;i} of the function f2(z)=
</p>
<p>&sum;&infin;
n=0(&minus;1)nz2n,
</p>
<p>where |z|&lt; 1.
</p>
<p>12.14 Find the analytic continuation into C&minus; {0} of the function
</p>
<p>f (z)=
&int; &infin;
</p>
<p>0
te&minus;ztdt where Re(z) &gt; 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>390 12 Advanced Topics
</p>
<p>12.15 Show that the integral in Eq. (12.9) converges. Hint: First show that
|Ŵ(z+ 1)| &le;
</p>
<p>&int;&infin;
0 t
</p>
<p>xe&minus;tdt where x = Re(z). Now show that
&int; &infin;
</p>
<p>0
txe&minus;tdt &le;
</p>
<p>&int; 1
</p>
<p>0
txe&minus;tdt +
</p>
<p>&int; &infin;
</p>
<p>0
tne&minus;tdt for some integer n &gt; 0
</p>
<p>and conclude that Ŵ(z) is finite.
</p>
<p>12.16 Show that dŴ(z + 1)/dz exists and is finite by establishing the fol-
lowing:
</p>
<p>(a) | ln t |&lt; t + 1/t for t &gt; 0. Hint: For t &ge; 1, show that t &minus; ln t is a mono-
tonically increasing function. For t &lt; 1, make the substitution t = 1/s.
</p>
<p>(b) Use the result from part (a) in the integral for dŴ(z+ 1)/dz to show
that |dŴ(z+ 1)/dz| is finite. Hint: Differentiate inside the integral.
</p>
<p>12.17 Derive Eq. (12.11) from Eq. (12.9).
</p>
<p>12.18 Show that Ŵ( 12 )=
&radic;
π , and that
</p>
<p>(2k &minus; 1)!! &equiv; (2k&minus; 1)(2k &minus; 3) &middot; &middot; &middot;5 &middot; 3 &middot; 1 = 2
k
</p>
<p>&radic;
π
Ŵ
</p>
<p>(
2k+ 1
</p>
<p>2
</p>
<p>)
.
</p>
<p>12.19 Show that Ŵ(z)=
&int; 1
</p>
<p>0 [ln(1/t)]z&minus;1dt with Re(z) &gt; 0.
</p>
<p>12.20 Derive the identity
&int;&infin;
</p>
<p>0 e
xαdx = Ŵ[(α + 1)/α].
</p>
<p>12.21 Consider the function f (z)= (1 + z)α .
(a) Show that dnf/dzn|z=0 = Ŵ(α+1)/Ŵ(α&minus;n+1), and use it to derive
</p>
<p>the relation
</p>
<p>(1 + z)α =
&infin;&sum;
</p>
<p>n=0
</p>
<p>(
α
</p>
<p>n
</p>
<p>)
zn, where
</p>
<p>(
α
</p>
<p>n
</p>
<p>)
&equiv; α!
</p>
<p>n!(α &minus; n)! &equiv;
Ŵ(α + 1)
</p>
<p>n!Ŵ(α &minus; n+ 1) .
</p>
<p>(b) Show that for general complex numbers a and b we can formally write
</p>
<p>(a + b)α =
&infin;&sum;
</p>
<p>n=0
</p>
<p>(
α
</p>
<p>n
</p>
<p>)
anbα&minus;n.
</p>
<p>(c) Show that if α is a positive integer m, the series in part (b) truncates at
n=m.
</p>
<p>12.22 Prove that the residue of Ŵ(z) at z=&minus;k is rk = (&minus;1)k/k!. Hint: Use
Eq. (12.12)</p>
<p/>
</div>
<div class="page"><p/>
<p>12.6 Problems 391
</p>
<p>12.23 Derive the following relation for z= x + iy:
</p>
<p>∣∣Ŵ(z)
∣∣= Ŵ(x)
</p>
<p>&infin;&prod;
</p>
<p>k=0
</p>
<p>[
1 + y
</p>
<p>2
</p>
<p>(x + k)2
]&minus;1/2
</p>
<p>.
</p>
<p>12.24 Using the definition of B(a, b), Eq. (12.16), show that B(a, b) =
B(b, a).
</p>
<p>12.25 Integrate Eq. (12.21) by parts and derive Eq. (12.11).
</p>
<p>12.26 For positive integers n, show that Ŵ( 12 &minus; n)Ŵ( 12 + n)= (&minus;1)nπ .
</p>
<p>12.27 Show that
</p>
<p>(a) B(a, b)= B(a + 1, b)+B(a, b+ 1).
(b) B(a, b+ 1)= ( b
</p>
<p>a+b )B(a, b).
(c) B(a, b)B(a + b, c)= B(b, c)B(a, b+ c).
</p>
<p>12.28 Verify that
&int; 1
&minus;1(1 + t)a(1 &minus; t)bdt = 2a+b+1B(a + 1, b+ 1).
</p>
<p>12.29 Show that the volume of the solid formed by the surface z = xayb ,
the xy-, yz-, and xz-planes, and the plane parallel to the z-axis and going
through the points (0, y0) and (x0,0) is
</p>
<p>xa+10 y
b+1
0
</p>
<p>a + b+ 2B(a + 1, b+ 1).
</p>
<p>12.30 Derive this relation:
&int; &infin;
</p>
<p>0
</p>
<p>sinha x
</p>
<p>coshb x
dx = 1
</p>
<p>2
B
</p>
<p>(
a + 1
</p>
<p>2
,
b&minus; a
</p>
<p>2
</p>
<p>)
where &minus;1 &lt; a &lt; b.
</p>
<p>Hint: Let t = tanh2 x in Eq. (12.16).
</p>
<p>12.31 The Hankel function of the second kind is defined as Hankel function of the
second kind
</p>
<p>H (2)ν (α)&equiv;
1
</p>
<p>iπ
</p>
<p>&int;
</p>
<p>C
</p>
<p>e(α/2)(z&minus;1/z)
dz
</p>
<p>zν+1
,
</p>
<p>where C is the contour shown in Fig. 12.15.
Find the asymptotic expansion of this function.
</p>
<p>12.32 Find the asymptotic dependence of the modified Bessel function of modified Bessel function
of the first kindthe first kind, defined as
</p>
<p>Iν(α)&equiv;
1
</p>
<p>2πi
</p>
<p>∮
</p>
<p>C
</p>
<p>e(α/2)(z+1/z)
dz
</p>
<p>zν+1
,
</p>
<p>where C starts at &minus;&infin;, approaches the origin and circles it, and goes back to
&minus;&infin;. Thus the negative real axis is excluded from the domain of analyticity.</p>
<p/>
</div>
<div class="page"><p/>
<p>392 12 Advanced Topics
</p>
<p>Fig. 12.15 The contour for the evaluation of the Hankel function of the second kind
</p>
<p>12.33 Find the asymptotic dependence of the modified Bessel function of
the second kind:modified Bessel function
</p>
<p>of the second kind
</p>
<p>Kν(α)&equiv;
1
</p>
<p>2
</p>
<p>&int;
</p>
<p>C
</p>
<p>e&minus;(α/2)(z+1/z)
dz
</p>
<p>zν+1
,
</p>
<p>where C starts at &infin;, approaches the origin and circles it, and goes back
to &infin;. Thus the positive real axis is excluded from the domain of analyticity.</p>
<p/>
</div>
<div class="page"><p/>
<p>Part IV
</p>
<p>Differential Equations</p>
<p/>
</div>
<div class="page"><p/>
<p>13Separation of Variables in SphericalCoordinates
</p>
<p>The laws of physics are almost exclusively written in the form of differential
equations (DEs). In (point) particle mechanics there is only one independent
variable, leading to ordinary differential equations (ODEs). In other areas of
physics in which extended objects such as fields are studied, variations with
respect to position are also important. Partial derivatives with respect to co-
ordinate variables show up in the differential equations, which are therefore
called partial differential equations (PDEs). We list the most common PDEs
of mathematical physics in the following.
</p>
<p>13.1 PDEs of Mathematical Physics
</p>
<p>In electrostatics, where time-independent scalar fields, such as potentials,
and vector fields such as electrostatic fields, are studied, the law is described
by Poisson&rsquo;s equation, Poisson&rsquo;s equation
</p>
<p>&nabla;2Φ(r)=&minus;4πρ(r). (13.1)
</p>
<p>In vacuum, where ρ(r)= 0, Eq. (13.1) reduces to Laplace&rsquo;s equation, Laplace&rsquo;s equation
</p>
<p>&nabla;2Φ(r)= 0. (13.2)
</p>
<p>Many electrostatic problems involve conductors held at constant potentials
and situated in vacuum. In the space between such conducting surfaces, the
electrostatic potential obeys Eq. (13.2).
</p>
<p>The most simplified version of the heat equation is heat equation
</p>
<p>&part;T
</p>
<p>&part;t
= a2&nabla;2T (r), (13.3)
</p>
<p>where T is the temperature and a is a constant characterizing the medium
in which heat flows.
</p>
<p>One of the most frequently recurring PDEs encountered in mathematical
physics is the wave equation, wave equation
</p>
<p>&nabla;2Ψ &minus; 1
c2
</p>
<p>&part;2Ψ
</p>
<p>&part;t2
= 0. (13.4)
</p>
<p>S. Hassani, Mathematical Physics, DOI 10.1007/978-3-319-01195-0_13,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>395</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-01195-0_13">http://dx.doi.org/10.1007/978-3-319-01195-0_13</a></div>
</div>
<div class="page"><p/>
<p>396 13 Separation of Variables in Spherical Coordinates
</p>
<p>This equation (or its simplification to lower dimensions) is applied to the
vibration of strings and drums; the propagation of sound in gases, solids,
and liquids; the propagation of disturbances in plasmas; and the propagation
of electromagnetic waves.
</p>
<p>The Schr&ouml;dinger equation, describing nonrelativistic quantum phenom-Schr&ouml;dinger equation
ena, is
</p>
<p>&minus; �
2
</p>
<p>2m
&nabla;2Ψ + V (r)Ψ = i�&part;Ψ
</p>
<p>&part;t
, (13.5)
</p>
<p>where m is the mass of a subatomic particle, � is Planck&rsquo;s constant (divided
by 2π ), V is the potential energy of the particle, and |Ψ (r, t)|2 is the prob-
ability density of finding the particle at r at time t .
</p>
<p>A relativistic generalization of the Schr&ouml;dinger equation for a free parti-
cle of mass m is the Klein-Gordon equation, which, in terms of the naturalKlein-Gordon equation
units (�= 1 = c), reduces to
</p>
<p>&nabla;2φ &minus;m2φ = &part;
2φ
</p>
<p>&part;t2
. (13.6)
</p>
<p>Equations (13.3)&ndash;(13.6) have partial derivatives with respect to time. As
a first step toward solving these PDEs and as an introduction to similar tech-
niques used in the solution of PDEs not involving time,1 let us separate
the time variable. We will denote the functions in all four equations by the
generic symbol Ψ (r, t). The basic idea is to separate the r and t dependence
into factors: Ψ (r, t) &equiv; R(r)T (t). This factorization permits us to separate
the two operations of space differentiation and time differentiation. Let Stime is separated from
</p>
<p>space stand for all spatial derivative operators and write all the relevant equa-
tions either as SΨ = &part;Ψ/&part;t or as SΨ = &part;2Ψ/&part;t2. With this notation and
the above separation, we have
</p>
<p>S(RT )= T (SR)=
{
RdT/dt,
</p>
<p>Rd2T/dt2.
</p>
<p>Dividing both sides by RT , we obtain
</p>
<p>1
</p>
<p>R
SR =
</p>
<p>⎧
⎨
⎩
</p>
<p>1
T
</p>
<p>dT
dt
,
</p>
<p>1
T
</p>
<p>d2T
</p>
<p>dt2
.
</p>
<p>(13.7)
</p>
<p>Now comes the crucial step in the process of separation of variables. The
LHS of Eq. (13.7) is a function of position alone, and the RHS is a function
of time alone. Since r and t are independent variables, the only way that
(13.7) can hold is for both sides to be constant, say α:
</p>
<p>1
</p>
<p>R
SR = α &rArr; SR = αR
</p>
<p>1See [Hass 08] for a thorough discussion of separation in Cartesian and cylindrical coor-
dinates. Chapter 19 of this book also contains examples of solutions to some second-order
linear DEs resulting from such separation.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.1 PDEs of Mathematical Physics 397
</p>
<p>and2
</p>
<p>1
</p>
<p>T
</p>
<p>dT
</p>
<p>dt
= α &rArr; dT
</p>
<p>dt
= αT or 1
</p>
<p>T
</p>
<p>d2T
</p>
<p>dt2
= α &rArr; d
</p>
<p>2T
</p>
<p>dt2
= αT .
</p>
<p>We have reduced the original time-dependent PDE to an ODE,
</p>
<p>dT
</p>
<p>dt
= αT or d
</p>
<p>2T
</p>
<p>dt2
= αT , (13.8)
</p>
<p>and a PDE involving only the position variables, (S &minus; α)R = 0. The
most general form of S &minus; α arising from Eqs. (13.3) through (13.6) is
S&minus; α &equiv; &nabla;2 + f (r). Therefore, Eqs. (13.3)&ndash;(13.6) are equivalent to (13.8),
and &nabla;2R + f (r)R = 0, which we rewrite as
</p>
<p>&nabla;2Ψ (r)+ f (r)Ψ (r)= 0. (13.9)
</p>
<p>This is called a homogeneous PDE because of the zero on the right-hand
side. Of all the PDEs outlined above, Poisson&rsquo;s equation is the only inho-
mogeneous equation. We will restrict ourselves to the homogeneous case in
this chapter.
</p>
<p>Depending on the geometry of the problem, Eq. (13.9) is further sepa-
rated into ODEs each involving a single coordinate of a suitable coordinate
system. We shall see examples of all major coordinate systems (Cartesian,
cylindrical, and spherical) in Chap. 19. For the rest of this chapter, we shall
concentrate on some general aspects of the spherical coordinates.
</p>
<p>Historical Notes
</p>
<p>Jean Le Rond d&rsquo;Alembert (1717&ndash;1783) was the illegitimate son of a famous salon
</p>
<p>Jean Le Rond d&rsquo;Alembert
</p>
<p>1717&ndash;1783
</p>
<p>hostess of eighteenth-century Paris and a cavalry officer. Abandoned by his mother,
d&rsquo;Alembert was raised by a foster family and later educated by the arrangement of his
father at a nearby church-sponsored school, in which he received instruction in the clas-
sics and above-average instruction in mathematics. After studying law and medicine, he
finally chose to pursue a career in mathematics. In the 1740s he joined the ranks of the
philosophes, a growing group of deistic and materialistic thinkers and writers who ac-
tively questioned the social and intellectual standards of the day. He traveled little (he
left France only once, to visit the court of Frederick the Great), preferring instead the
company of his friends in the salons, among whom he was well known for his wit and
laughter.
D&rsquo;Alembert turned his mathematical and philosophical talents to many of the outstand-
ing scientific problems of the day, with mixed success. Perhaps his most famous scien-
tific work, entitled Trait&eacute; de dynamique, shows his appreciation that a revolution was
taking place in the science of mechanics&mdash;the formalization of the principles stated by
Newton into a rigorous mathematical framework. The philosophy to which d&rsquo;Alembert
subscribed, however, refused to acknowledge the primacy of a concept as unclear and ar-
bitrary as &ldquo;force,&rdquo; introducing a certain awkwardness to his treatment and perhaps caus-
ing him to overlook the important principle of conservation of energy. Later, d&rsquo;Alembert
produced a treatise on fluid mechanics (the priority of which is still debated by histori-
ans), a paper dealing with vibrating strings (in which the wave equation makes its first
appearance in physics), and a skillful treatment of celestial mechanics. D&rsquo;Alembert is
also credited with use of the first partial differential equation as well as the first solution
</p>
<p>2In most cases, α is chosen to be real. In the case of the Schr&ouml;dinger equation, it is more
convenient to choose α to be purely imaginary so that the i in the definition of S can be
compensated. In all cases, the precise nature of α is determined by boundary conditions.</p>
<p/>
</div>
<div class="page"><p/>
<p>398 13 Separation of Variables in Spherical Coordinates
</p>
<p>to such an equation using separation of variables. (One should be careful interpreting
&ldquo;first&rdquo;: many of d&rsquo;Alembert&rsquo;s predecessors and contemporaries gave similar, though less
satisfactory, treatments of these milestones.) Perhaps his most well-known contribution
to mathematics (at least among students) is the ratio test for the convergence of infinite
series.
Much of the work for which d&rsquo;Alembert is remembered occurred outside mathematical
physics. He was chosen as the science editor of the Encyclop&eacute;die, and his lengthy Dis-
cours Pr&eacute;liminaire in that volume is considered one of the defining documents of the
Enlightenment. Other works included writings on law, religion, and music.
Since d&rsquo;Alembert&rsquo;s final years were not especially happy ones, perhaps this account of
his life should end with a glimpse at the humanity his philosophy often gave his work.
Like many of his contemporaries, he considered the problem of calculating the relative
risk associated with the new practice of smallpox inoculation, which in rare cases caused
the disease it was designed to prevent. Although not very successful in the mathematical
sense, he was careful to point out that the probability of accidental infection, however
slight or elegantly derived, would be small consolation to a father whose child died from
the inoculation. It is greatly to his credit that d&rsquo;Alembert did not believe such considera-
tions irrelevant to the problem.
</p>
<p>13.2 Separation of the Angular Part
</p>
<p>With Cartesian and cylindrical variables, the boundary conditions are impor-
tant in determining the nature of the solutions of the ODE obtained from the
PDE. In almost all applications, however, the angular part of the spherical
variables can be separated and studied very generally. This is because the
angular part of the Laplacian in the spherical coordinate system is closely
related to the operation of rotation and the angular momentum, which are
independent of any particular situation.
</p>
<p>The separation of the angular part in spherical coordinates can be done
in a fashion exactly analogous to the separation of time by writing Ψ as
a product of three functions, each depending on only one of the variables.
However, we will follow an approach that is used in quantum mechanical
treatments of angular momentum. This approach, which is based on the
operator algebra of Chap. 4 and is extremely powerful and elegant, gives
solutions for the angular part in closed form.
</p>
<p>Define the vector operator 0p as 0p=&minus;i&nabla; so that its j th Cartesian compo-
nent is pj =&minus;i&part;/&part;xj , for j = 1,2,3. In quantum mechanics 0p (multiplied
by �) is the momentum operator. It is easy to verify that3
</p>
<p>[xj ,pk] = iδjk and [xj , xk] = 0 = [pj ,pk]. (13.10)
</p>
<p>We can also define the angular momentum operator as 0L = 0r &times; 0p.angular momentum
operator This is expressed in components as Li = (0r&times; 0p)i = ǫijkxjpk for i = 1,2,3,
</p>
<p>where Einstein&rsquo;s summation convention (summing over repeated indices) is
</p>
<p>3These operators act on the space of functions possessing enough &ldquo;nice&rdquo; properties as to
render the space suitable. The operator xj simply multiplies functions, while pj differen-
tiates them.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.2 Separation of the Angular Part 399
</p>
<p>utilized.4 Using the commutation relations above, we obtain
</p>
<p>[Lj ,Lk] = iǫjklLl .
</p>
<p>We will see shortly that 0L can be written solely in terms of the angles
</p>
<p>commutation relations
</p>
<p>between components of
</p>
<p>angular momentum
</p>
<p>operator
</p>
<p>θ and ϕ. Moreover, there is one factor of 0p in the definition of 0L, so if we
square 0L, we will get two factors of 0p, and a Laplacian may emerge in the
expression for 0L &middot; 0L. In this manner, we may be able to write &nabla;2 in terms of
L2, which depends only on angles. Let us try this:
</p>
<p>L2 = 0L &middot; 0L=
3&sum;
</p>
<p>i=1
LiLi = ǫijkxjpkǫimnxmpn = ǫijkǫimnxjpkxmpn
</p>
<p>= (δjmδkn &minus; δjnδkm)xjpkxmpn = xjpkxjpk &minus; xjpkxkpj .
</p>
<p>We need to write this expression in such a way that factors with the same in-
dex are next to each other, to give a dot product. We must also try, when pos-
sible, to keep the 0p factors to the right so that they can operate on functions
without intervention from the x factors. We do this by using Eq. (13.10):
</p>
<p>L2 = xj (xjpk &minus; iδkj )pk &minus; (pkxj + iδkj )xkpj
= xjxjpkpk &minus; ixjpj &minus; pkxkxjpj &minus; ixjpj
= xjxjpkpk &minus; 2ixjpj &minus; (xkpk &minus; iδkk)xjpj .
</p>
<p>Recalling that δkk =
&sum;3
</p>
<p>k=1 δkk = 3 and xjxj =
&sum;3
</p>
<p>j=1 xjxj =0r &middot; 0r= r2 etc.,
we can write L2 = r20p &middot; 0p + i0r &middot; 0p &minus; (0r &middot; 0p)(0r &middot; 0p), which, if we make the
substitution 0p=&minus;i&nabla;&nabla;&nabla; , yields
</p>
<p>&nabla;2 =&minus;r&minus;2L2 + r&minus;2(r &middot; &nabla;&nabla;&nabla;)(r &middot; &nabla;&nabla;&nabla;)+ r&minus;2r &middot; &nabla;&nabla;&nabla;.
</p>
<p>Letting both sides act on the function Ψ (r, θ,ϕ), we get
</p>
<p>&nabla;2Ψ =&minus; 1
r2
</p>
<p>L2Ψ + 1
r2
</p>
<p>(r &middot; &nabla;&nabla;&nabla;)(r &middot; &nabla;&nabla;&nabla;)Ψ + 1
r2
</p>
<p>r &middot; &nabla;&nabla;&nabla;Ψ. (13.11)
</p>
<p>But we note that r &middot;&nabla;&nabla;&nabla; = r êr &middot;&nabla;&nabla;&nabla; = r&part;/&part;r . We thus get the final form of &nabla;2Ψ
in spherical coordinates: Laplacian separated into
</p>
<p>angular and radial parts
</p>
<p>&nabla;2Ψ =&minus; 1
r2
</p>
<p>L2Ψ + 1
r
</p>
<p>&part;
</p>
<p>&part;r
</p>
<p>(
r
&part;Ψ
</p>
<p>&part;r
</p>
<p>)
+ 1
</p>
<p>r
</p>
<p>&part;Ψ
</p>
<p>&part;r
. (13.12)
</p>
<p>It is important to note that Eq. (13.11) is a general relation that holds in
all coordinate systems. Although all the manipulations leading to it were
done in Cartesian coordinates, since it is written in vector notation, there is
no indication in the final form that it was derived using specific coordinates.
</p>
<p>4It is assumed that the reader is familiar with vector algebra using indices and such objects
as δij and ǫijk . For an introductory treatment, sufficient for our present discussion, see
[Hass 08]. A more advanced treatment of these objects (tensors) can be found in Part VIII
of the present book.</p>
<p/>
</div>
<div class="page"><p/>
<p>400 13 Separation of Variables in Spherical Coordinates
</p>
<p>Equation (13.12) is the spherical version of (13.11) and is the version we
shall use. We will first make the simplifying assumption that in Eq. (13.9),
the master equation, f (r) is a function of r only. Equation (13.9) then be-
comes
</p>
<p>&minus; 1
r2
</p>
<p>L2Ψ + 1
r
</p>
<p>&part;
</p>
<p>&part;r
</p>
<p>(
r
&part;Ψ
</p>
<p>&part;r
</p>
<p>)
+ 1
</p>
<p>r
</p>
<p>&part;Ψ
</p>
<p>&part;r
+ f (r)Ψ = 0.
</p>
<p>Assuming, for the time being, that L2 depends only on θ and ϕ, and sepa-
rating Ψ into a product of two functions, Ψ (r, θ,ϕ)=R(r)Y (θ,ϕ), we can
rewrite this equation as
</p>
<p>&minus; 1
r2
</p>
<p>L2(RY )+ 1
r
</p>
<p>&part;
</p>
<p>&part;r
</p>
<p>[
r
&part;
</p>
<p>&part;r
(RY)
</p>
<p>]
+ 1
</p>
<p>r
</p>
<p>&part;
</p>
<p>&part;r
(RY)+ f (r)RY = 0.
</p>
<p>Dividing by RY and multiplying by r2 yields
</p>
<p>&minus; 1
Y
L2(Y )
</p>
<p>︸ ︷︷ ︸
&minus;α
</p>
<p>+ r
R
</p>
<p>d
</p>
<p>dr
</p>
<p>(
r
dR
</p>
<p>dr
</p>
<p>)
+ r
</p>
<p>R
</p>
<p>dR
</p>
<p>dr
+ r2f (r)
</p>
<p>︸ ︷︷ ︸
+α
</p>
<p>= 0,
</p>
<p>or
</p>
<p>L2Y(θ,ϕ)= αY(θ,ϕ) (13.13)
and
</p>
<p>d2R
</p>
<p>dr2
+ 2
</p>
<p>r
</p>
<p>dR
</p>
<p>dr
+
[
f (r)&minus; α
</p>
<p>r2
</p>
<p>]
R = 0. (13.14)
</p>
<p>We will concentrate on the angular part, Eq. (13.13), leaving the radial part
to the general discussion of ODEs. The rest of this section will focus on
showing that L1 &equiv; Lx , L2 &equiv; Ly , and L3 &equiv; Lz are independent of r .
</p>
<p>Since Li is an operator, we can study its action on an arbitrary function f .
Thus,
</p>
<p>Lif =&minus;iǫijkxj&nabla;kf &equiv;&minus;iǫijkxj&part;f/&part;xk.
We can express the Cartesian xj in terms of r , θ , and ϕ, and use the chain
rule to express &part;f/&part;xk in terms of spherical coordinates. This will give us
Lif expressed in terms of r , θ , and ϕ. It will then emerge that r is absent in
the final expression.
</p>
<p>Let us start with
</p>
<p>x = r sin θ cosϕ, y = r sin θ sinϕ, z= r cos θ,
</p>
<p>and their inverse,
</p>
<p>r =
(
x2 + y2 + z2
</p>
<p>)1/2
, cos θ = z
</p>
<p>r
, tanϕ = y
</p>
<p>x
,
</p>
<p>and express the Cartesian derivatives in terms of spherical coordinates using
the chain rule. The first such derivative is
</p>
<p>&part;f
</p>
<p>&part;x
= &part;f
</p>
<p>&part;r
</p>
<p>&part;r
</p>
<p>&part;x
+ &part;f
</p>
<p>&part;θ
</p>
<p>&part;θ
</p>
<p>&part;x
+ &part;f
</p>
<p>&part;ϕ
</p>
<p>&part;ϕ
</p>
<p>&part;x
. (13.15)</p>
<p/>
</div>
<div class="page"><p/>
<p>13.3 Construction of Eigenvalues of L2 401
</p>
<p>The derivative of one coordinate system with respect to the other can be eas-
ily calculated. For example, &part;r/&part;x = x/r = sin θ cosϕ, and differentiating
both sides of the equation cos θ = z/r , we obtain
</p>
<p>&minus;sin θ &part;θ
&part;x
</p>
<p>=&minus;z&part;r/&part;x
r2
</p>
<p>=&minus;zx
r3
</p>
<p>=&minus;cos θ sin θ cosϕ
r
</p>
<p>&rArr; &part;θ
&part;x
</p>
<p>= cos θ cosϕ
r
</p>
<p>.
</p>
<p>Finally, differentiating both sides of tanϕ = y/x with respect to x yields
&part;ϕ/&part;x =&minus; sinϕ/(r sin θ). Using these expressions in Eq. (13.15), we get
</p>
<p>&part;f
</p>
<p>&part;x
= sin θ cosϕ &part;f
</p>
<p>&part;r
+ cos θ cosϕ
</p>
<p>r
</p>
<p>&part;f
</p>
<p>&part;θ
&minus; sinϕ
</p>
<p>r sin θ
</p>
<p>&part;f
</p>
<p>&part;ϕ
.
</p>
<p>In exactly the same way, we obtain
</p>
<p>&part;f
</p>
<p>&part;y
= sin θ sinϕ &part;f
</p>
<p>&part;r
+ cos θ sinϕ
</p>
<p>r
</p>
<p>&part;f
</p>
<p>&part;θ
+ cosϕ
</p>
<p>r sin θ
</p>
<p>&part;f
</p>
<p>&part;ϕ
,
</p>
<p>&part;f
</p>
<p>&part;z
= cos θ &part;f
</p>
<p>&part;r
&minus; sin θ
</p>
<p>r
</p>
<p>&part;f
</p>
<p>&part;θ
.
</p>
<p>We can now calculate Lx by letting it act on an arbitrary function and
expressing all Cartesian coordinates and derivatives in terms of spherical Cartesian components of
</p>
<p>angular momentum
</p>
<p>operator expressed in
</p>
<p>spherical coordinates
</p>
<p>coordinates. The result is
</p>
<p>Lxf =&minus;iy
&part;f
</p>
<p>&part;z
+ iz &part;f
</p>
<p>&part;y
= i
</p>
<p>(
sinϕ
</p>
<p>&part;
</p>
<p>&part;θ
+ cot θ cosϕ &part;
</p>
<p>&part;ϕ
</p>
<p>)
f,
</p>
<p>or
</p>
<p>Lx = i
(
</p>
<p>sinϕ
&part;
</p>
<p>&part;θ
+ cot θ cosϕ &part;
</p>
<p>&part;ϕ
</p>
<p>)
. (13.16)
</p>
<p>Analogous arguments yield
</p>
<p>Ly = i
(
&minus; cosϕ &part;
</p>
<p>&part;θ
+ cot θ sinϕ &part;
</p>
<p>&part;ϕ
</p>
<p>)
, Lz =&minus;i
</p>
<p>&part;
</p>
<p>&part;ϕ
. (13.17)
</p>
<p>It is left as a problem for the reader to show that by adding the squares of
the components of the angular momentum operator, one obtains angular momentum
</p>
<p>squared as differential
</p>
<p>operator in θ and ϕL2 =&minus; 1
sin θ
</p>
<p>&part;
</p>
<p>&part;θ
</p>
<p>(
sin θ
</p>
<p>&part;
</p>
<p>&part;θ
</p>
<p>)
&minus; 1
</p>
<p>sin2 θ
</p>
<p>&part;2
</p>
<p>&part;ϕ2
, (13.18)
</p>
<p>which is independent of r as promised. Substitution in Eq. (13.12) yields
the familiar expression for the Laplacian in spherical coordinates.
</p>
<p>13.3 Construction of Eigenvalues of L2
</p>
<p>Now that we have L2 in terms of θ and ϕ, we could substitute in Eq. (13.13),
separate the θ and ϕ dependence, and solve the corresponding ODEs. How-
ever, there is a much more elegant way of solving this problem algebraically,</p>
<p/>
</div>
<div class="page"><p/>
<p>402 13 Separation of Variables in Spherical Coordinates
</p>
<p>because Eq. (13.13) is simply an eigenvalue equation for L2. In this section,
we will find the eigenvalues of L2. The next section will evaluate the eigen-
vectors of L2.
</p>
<p>Let us consider L2 as an abstract operator and write (13.13) as
</p>
<p>L2|Y 〉 = α|Y 〉,
</p>
<p>where |Y 〉 is an abstract vector whose (θ,ϕ)th component can be calcu-
lated later. Since L2 is a differential operator, it does not have a (finite-
dimensional) matrix representation. Thus, the determinantal procedure for
calculating eigenvalues and eigenfunctions will not work here, and we have
to find another way.
</p>
<p>The equation above specifies an eigenvalue, α, and an eigenvector, |Y 〉.
There may be more than one |Y 〉 corresponding to the same α. To distin-
guish among these so-called degenerate eigenvectors, we choose a second
operator, say L3 &isin; {Li} that commutes with L2. This allows us to select a
basis in which both L2 and L3 are diagonal, or, equivalently, a basis whose
vectors are simultaneous eigenvectors of both L2 and L3. This is possible by
Theorem 6.4.18 and the fact that both L2 and L3 are hermitian operators in
the space of square-integrable functions. (The proof is left as a problem.)
In general, we would want to continue adding operators until we obtained a
maximum set of commuting operators which could label the eigenvectors.
In this case, L2 and L3 exhaust the set.5 Using the more common subscripts
x, y, and z instead of 1, 2, 3 and attaching labels to the eigenvectors, we
have
</p>
<p>L2|Yα,β〉 = α|Yα,β〉, Lz|Yα,β〉 = β|Yα,β〉. (13.19)
The hermiticity of L2 and Lz implies the reality of α and β . Next we need to
determine the possible values for α and β .
</p>
<p>Define two new operators L+ &equiv; Lx + iLy and L&minus; &equiv; Lx &minus; iLy . It is then
easily verified that
</p>
<p>[
L2,L&plusmn;
</p>
<p>]
= 0, [Lz,L&plusmn;] = &plusmn;L&plusmn;, [L+,L&minus;] = 2Lz. (13.20)
</p>
<p>The first equation implies that L&plusmn; are invariant operators when acting in the
subspace corresponding to the eigenvalue α; that is, L&plusmn;|Yα,β〉 are eigenvec-
tors of L2 with the same eigenvalue α:
</p>
<p>L2
(
L&plusmn;|Yα,β〉
</p>
<p>)
= L&plusmn;
</p>
<p>(
L2|Yα,β〉
</p>
<p>)
= L&plusmn;
</p>
<p>(
α|Yα,β〉
</p>
<p>)
= αL&plusmn;|Yα,β〉.
</p>
<p>The second equation in (13.20) yields
</p>
<p>Lz
(
L+|Yα,β〉
</p>
<p>)
= (LzL+)|Yα,β〉 = (L+Lz + L+)|Yα,β〉
= L+Lz|Yα,β〉 + L+|Yα,β〉 = βL+|Yα,β〉 + L+|Yα,β〉
= (β + 1)L+|Yα,β〉.
</p>
<p>5We could just as well have chosen L2 and any other component as our maximal set.
However, L2 and L3 is the universally accepted choice.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.3 Construction of Eigenvalues of L2 403
</p>
<p>This indicates that L+|Yα,β〉 has one more unit of the Lz eigenvalue than
|Yα,β〉 does. In other words, L+ raises the eigenvalue of Lz by one unit. That
is why L+ is called a raising operator. Similarly, L&minus; is called a lowering angular momentum
</p>
<p>raising and lowering
</p>
<p>operators
operator because Lz(L&minus;|Yα,β〉)= (β &minus; 1)L&minus;|Yα,β〉.
</p>
<p>We can summarize the above discussion as
</p>
<p>L&plusmn;|Yα,β〉 = C&plusmn;|Yα,β&plusmn;1〉,
</p>
<p>where C&plusmn; are constants to be determined by a suitable normalization.
There are restrictions on (and relations between) α and β . First note that
</p>
<p>as L2 is a sum of squares of hermitian operators, it must be a positive oper-
ator; that is, 〈a|L2|a〉 &ge; 0 for all |a〉. In particular,
</p>
<p>0 &le; 〈Yα,β |L2|Yα,β〉 = α〈Yα,β |Yα,β〉 = α‖Yα,β‖2.
</p>
<p>Therefore, α &ge; 0. Next, one can readily show that
</p>
<p>L2 = L+L&minus; + L2z &minus; Lz = L&minus;L+ + L2z + Lz. (13.21)
</p>
<p>Sandwiching both sides of the first equality between |Yα,β〉 and 〈Yα,β | yields
</p>
<p>〈Yα,β |L2|Yα,β〉 = 〈Yα,β |L+L&minus;|Yα,β〉 + 〈Yα,β |L2z |Yα,β〉 &minus; 〈Yα,β |Lz|Yα,β〉,
</p>
<p>with an analogous expression involving L&minus;L+. Using the fact that L+ =
(L&minus;)&dagger;, we get
</p>
<p>α‖Yα,β‖2 = 〈Yα,β |L+L&minus;|Yα,β〉 + β2‖Yα,β‖2 &minus; β‖Yα,β‖2
</p>
<p>= 〈Yα,β |L&minus;L+|Yα,β〉 + β2‖Yα,β‖2 + β‖Yα,β‖2
</p>
<p>=
∥∥L∓|Yα,β〉
</p>
<p>∥∥2 + β2‖Yα,β‖2 ∓ β‖Yα,β‖2. (13.22)
</p>
<p>Because of the positivity of norms, this yields α &ge; β2 &minus; β and α &ge; β2 + β .
Adding these two inequalities gives 2α &ge; 2β2 &rArr; &minus;&radic;α &le; β &le; &radic;α. It fol-
lows that the values of β are bounded. That is, there exist a maximum β ,
denoted by β+, and a minimum β , denoted by β&minus;, beyond which there are
no more values of β . This can happen only if
</p>
<p>L+|Yα,β+〉 = 0, L&minus;|Yα,β&minus;〉 = 0,
</p>
<p>because if L&plusmn;|Yα,β&plusmn;〉 are not zero, then they must have values of β corre-
sponding to β&plusmn; &plusmn; 1, which are not allowed.
</p>
<p>Using β+ for β in Eq. (13.22) yields
(
α &minus; β2+ &minus; β+
</p>
<p>)
‖Yα,β+‖2 = 0.
</p>
<p>By definition |Yα,β+〉 �= 0 (otherwise β+&minus;1 would be the maximum). Thus,
we obtain α = β2+ + β+. An analogous procedure using β&minus; for β yields
α = β2&minus; &minus; β&minus;. We solve these two equations for β+ and β&minus;:
</p>
<p>β+ =
1
</p>
<p>2
(&minus;1 &plusmn;
</p>
<p>&radic;
1 + 4α), β&minus; =
</p>
<p>1
</p>
<p>2
(1 &plusmn;
</p>
<p>&radic;
1 + 4α).</p>
<p/>
</div>
<div class="page"><p/>
<p>404 13 Separation of Variables in Spherical Coordinates
</p>
<p>Since β+ &ge; β&minus; and
&radic;
</p>
<p>1 + 4α &ge; 1, we must choose
</p>
<p>β+ =
1
</p>
<p>2
(&minus;1 +
</p>
<p>&radic;
1 + 4α)=&minus;β&minus;.
</p>
<p>Starting with |Yα,β+〉, we can apply L&minus; to it repeatedly. In each step we
decrease the value of β by one unit. There must be a limit to the number
of vectors obtained in this way, because β has a minimum. Therefore, there
must exist a nonnegative integer k such that
</p>
<p>(L&minus;)k+1|Yα,β+〉 = L&minus;
(
Lk&minus;|Yα,β+〉
</p>
<p>)
= 0.
</p>
<p>Thus, Lk&minus;|Yα,β+〉 must be proportional to |Yα,β&minus;〉. In particular, since
Lk&minus;|Yα,β+〉 has a β value equal to β+ &minus; k, we have β&minus; = β+ &minus; k. Now,
using β&minus; =&minus;β+ (derived above) yields the important result
</p>
<p>β+ =
k
</p>
<p>2
&equiv; j for k &isin;N,
</p>
<p>or α = j (j + 1), since α = β2+ + β+. This result is important enough to be
stated as a theorem.
</p>
<p>eigenvalues of L2 and Lz
given
</p>
<p>Theorem 13.3.1 The eigenvectors of L2, denoted by |Yjm〉, satisfy the
eigenvalue relations
</p>
<p>L2|Yjm〉 = j (j + 1)|Yjm〉, Lz|Yjm〉 =m|Yjm〉,
</p>
<p>where j is a positive integer or half-integer, and m can take a value
in the set {&minus;j,&minus;j + 1, . . . , j &minus; 1, j} of 2j + 1 numbers.
</p>
<p>Let us briefly consider the normalization of the eigenvectors. We already
know that the |Yjm〉, being eigenvectors of the hermitian operators L2 and Lz,
are orthogonal. We also demand that they be of unit norm; that is,
</p>
<p>〈Yjm|Yj &prime;m&prime;〉 = δjj &prime;δmm&prime; . (13.23)
</p>
<p>This will determine the constants C&plusmn;, introduced earlier. Let us consider C+
first, which is defined by L+|Yjm〉 = C+|Yj,m+1〉. The hermitian conjugate
of this equation is 〈Yjm|L&minus; = C&lowast;+〈Yj,m+1|. We contract these two equations
to get
</p>
<p>〈Yjm|L&minus;L+|Yjm〉 = |C+|2〈Yj,m+1|Yj,m+1〉.
Then we use the second relation in Eq. (13.21), Theorem 13.3.1, and (13.23)
to obtain
</p>
<p>j (j + 1)&minus;m(m+ 1)= |C+|2 &rArr; |C+| =
&radic;
j (j + 1)&minus;m(m+ 1).
</p>
<p>Adopting the convention that the argument (phase) of the complex number
C+ is zero (and therefore that C+ is real), we get
</p>
<p>C+ =
&radic;
j (j + 1)&minus;m(m+ 1)</p>
<p/>
</div>
<div class="page"><p/>
<p>13.3 Construction of Eigenvalues of L2 405
</p>
<p>Similarly, C&minus; =
&radic;
j (j + 1)&minus;m(m&minus; 1).
</p>
<p>Box 13.3.2 The raising and lowering operators act on |Yjm〉 as fol-
lows:
</p>
<p>L+|Yjm〉 =
&radic;
j (j + 1)&minus;m(m+ 1)|Yj,m+1〉,
</p>
<p>L&minus;|Yjm〉 =
&radic;
j (j + 1)&minus;m(m&minus; 1)|Yj,m&minus;1〉.
</p>
<p>(13.24)
</p>
<p>Example 13.3.3 Assume that j = l, a positive integer. Let us find an ex-
pression for |Ylm〉 by repeatedly applying L&minus; to |Yll〉. The action for L&minus; is
completely described by Eq. (13.24). For the first power of L&minus;, we obtain
</p>
<p>L&minus;|Yll〉 =
&radic;
l(l + 1)&minus; l(l &minus; 1)|Yl,l&minus;1〉 =
</p>
<p>&radic;
2l|Yl,l&minus;1〉.
</p>
<p>We apply L&minus; once more:
</p>
<p>(L&minus;)2|Yll〉 =
&radic;
</p>
<p>2lL&minus;|Yl,l&minus;1〉 =
&radic;
</p>
<p>2l
&radic;
l(l + 1)&minus; (l &minus; 1)(l &minus; 2)|Yl,l&minus;2〉
</p>
<p>=
&radic;
</p>
<p>2l
&radic;
</p>
<p>2(2l &minus; 1)|Yl,l&minus;2〉 =
&radic;
</p>
<p>2(2l)(2l &minus; 1)|Yl,l&minus;2〉.
</p>
<p>Applying L&minus; a third time yields
</p>
<p>(L&minus;)3|Yll〉 =
&radic;
</p>
<p>2(2l)(2l &minus; 1)L&minus;|Yl,l&minus;2〉 =
&radic;
</p>
<p>2(2l)(2l &minus; 1)
&radic;
</p>
<p>6(l &minus; 1)|Yl,l&minus;3〉
</p>
<p>=
&radic;
</p>
<p>3!(2l)(2l &minus; 1)(2l &minus; 2)|Yl,l&minus;3〉.
</p>
<p>The pattern suggests the following formula for a general power k:
</p>
<p>Lk&minus;|Yll〉 =
&radic;
k!(2l)(2l &minus; 1) &middot; &middot; &middot; (2l &minus; k + 1)|Yl,l&minus;k〉,
</p>
<p>or Lk&minus;|Yll〉 =
&radic;
k!(2l)!/(2l &minus; k)!|Yl,l&minus;k〉. If we set l &minus; k = m and solve for
</p>
<p>|Yl,m〉, we get
</p>
<p>|Yl,m〉 =
&radic;
</p>
<p>(l +m)!
(l &minus;m)!(2l)!L
</p>
<p>l&minus;m
&minus; |Yll〉.
</p>
<p>The discussion in this section is the standard treatment of angular mo-
mentum in quantum mechanics. In the context of quantum mechanics, The-
orem 13.3.1 states the far-reaching physical result that particles can have
integer or half-integer spin. Such a conclusion is tied to the rotation group
in three dimensions, which, in turn, is an example of a Lie group, or a con-
tinuous group of transformations. We shall come back to a study of groups
later. It is worth noting that it was the study of differential equations that
led the Norwegian mathematician Sophus Lie to the investigation of their
symmetries and the development of the beautiful branch of mathematics
and theoretical physics that bears his name. Thus, the existence of a connec-
tion between group theory (rotation, angular momentum) and the differential
equation we are trying to solve should not come as a surprise.</p>
<p/>
</div>
<div class="page"><p/>
<p>406 13 Separation of Variables in Spherical Coordinates
</p>
<p>13.4 Eigenvectors of L2: Spherical Harmonics
</p>
<p>The treatment in the preceding section took place in an abstract vector space.
Let us go back to the function space and represent the operators and vectors
in terms of θ and ϕ.
</p>
<p>First, let us consider Lz in the form of a differential operator, as given in
Eq. (13.17). The eigenvalue equation for Lz becomes
</p>
<p>&minus;i &part;
&part;ϕ
</p>
<p>Yjm(θ,ϕ)=mYjm(θ,ϕ).
</p>
<p>We write Yjm(θ,ϕ)= Pjm(θ)Qjm(ϕ) and substitute in the above equation
to obtain the ODE for ϕ, dQjm/dϕ = imQjm, which has a solution of the
form Qjm(ϕ)= Cjmeimϕ , where Cjm is a constant. Absorbing this constant
into Pjm, we can write
</p>
<p>Yjm(θ,ϕ)= Pjm(θ)eimϕ .
</p>
<p>In classical physics the value of functions must be the same at ϕ as at
ϕ + 2π . This condition restricts the values of m to integers. In quantum
mechanics, on the other hand, it is the absolute values of functions that are
physically measurable quantities, and therefore m can also be a half-integer.
</p>
<p>Box 13.4.1 From now on, we shall assume that m is an integer and
denote the eigenvectors of L2 by Ylm(θ,ϕ), in which l is a nonnegative
integer.
</p>
<p>Our task is to find an analytic expression for Ylm(θ,ϕ). We need differ-
ential expressions for L&plusmn;. These can easily be obtained from the expressions
for Lx and Ly given in Eqs. (13.16) and (13.17). (The straightforward ma-
nipulations are left as a problem.) We thus have
</p>
<p>L&plusmn; = e&plusmn;iϕ
(
&plusmn; &part;
&part;θ
</p>
<p>+ i cot θ &part;
&part;ϕ
</p>
<p>)
. (13.25)
</p>
<p>Since l is the highest value of m, when L+ acts on Yll(θ,ϕ)= Pll(θ)eilϕ the
result must be zero. This leads to the differential equation
</p>
<p>(
&part;
</p>
<p>&part;θ
+ i cot θ &part;
</p>
<p>&part;ϕ
</p>
<p>)[
Pll(θ)e
</p>
<p>ilϕ
]
= 0 &rArr;
</p>
<p>(
d
</p>
<p>dθ
&minus; l cot θ
</p>
<p>)
Pll(θ)= 0.
</p>
<p>The solution to this differential equation is readily found to be
</p>
<p>Pll(θ)= Cl(sin θ)l . (13.26)
</p>
<p>The constant is subscripted because each Pll may lead to a different constant
of integration. We can now write
</p>
<p>Yll(θ,ϕ)= Cl(sin θ)leilϕ .</p>
<p/>
</div>
<div class="page"><p/>
<p>13.4 Eigenvectors of L2 : Spherical Harmonics 407
</p>
<p>With Yll(θ,ϕ) at our disposal, we can obtain any Ylm(θ,ϕ) by repeated
application of L&minus;. In principle, the result of Example 13.3.3 gives all the
(abstract) eigenvectors. In practice, however, it is helpful to have a closed
form (in terms of derivatives) for just the θ part of Ylm(θ,ϕ). So, let us
apply L&minus;, as given in Eq. (13.25) to Yll(θ,ϕ):
</p>
<p>L&minus;Yll = e&minus;iϕ
(
&minus; &part;
&part;θ
</p>
<p>+ i cot θ &part;
&part;ϕ
</p>
<p>)[
Pll(θ)e
</p>
<p>ilϕ
]
</p>
<p>= e&minus;iϕ
[
&minus; &part;
&part;θ
</p>
<p>+ i cot θ(il)
][
Pll(θ)e
</p>
<p>ilϕ
]
</p>
<p>= (&minus;1)ei(l&minus;1)ϕ
(
</p>
<p>d
</p>
<p>dθ
+ l cot θ
</p>
<p>)
Pll(θ).
</p>
<p>It can be shown that for a positive integer,
</p>
<p>(
d
</p>
<p>dθ
+ n cot θ
</p>
<p>)
f (θ)= 1
</p>
<p>sinn θ
</p>
<p>d
</p>
<p>dθ
</p>
<p>[
sinn θf (θ)
</p>
<p>]
. (13.27)
</p>
<p>Using this result and (13.26) yields
</p>
<p>L&minus;Yll = (&minus;1)ei(l&minus;1)ϕ
1
</p>
<p>sinl θ
</p>
<p>d
</p>
<p>dθ
</p>
<p>[
sinl θ
</p>
<p>(
Cl sin
</p>
<p>l θ
)]
</p>
<p>= (&minus;1)Cl
ei(l&minus;1)ϕ
</p>
<p>sinl θ
</p>
<p>d
</p>
<p>dθ
</p>
<p>(
sin2l θ
</p>
<p>)
. (13.28)
</p>
<p>We apply L&minus; to (13.28), and use Eq. (13.27) with n= l &minus; 1 to obtain
</p>
<p>L2&minus;Yll = (&minus;1)2Clei(l&minus;2)ϕ
1
</p>
<p>sinl&minus;1 θ
</p>
<p>d
</p>
<p>dθ
</p>
<p>[
sinl&minus;1 θ
</p>
<p>1
</p>
<p>sinl θ
</p>
<p>d
</p>
<p>dθ
</p>
<p>(
sin2l θ
</p>
<p>)]
</p>
<p>= (&minus;1)2Cl
ei(l&minus;2)ϕ
</p>
<p>sinl&minus;1 θ
</p>
<p>d
</p>
<p>dθ
</p>
<p>[
1
</p>
<p>sin θ
</p>
<p>d
</p>
<p>dθ
</p>
<p>(
sin2l θ
</p>
<p>)]
.
</p>
<p>Making the substitution u= cos θ yields
</p>
<p>L2&minus;Yll = Cl
ei(l&minus;2)ϕ
</p>
<p>(1 &minus; u2)l/2&minus;1
d2
</p>
<p>du2
</p>
<p>[(
1 &minus; u2
</p>
<p>)l]
.
</p>
<p>With a little more effort one can detect a pattern and obtain
</p>
<p>Lk&minus;Yll = Cl
ei(l&minus;k)ϕ
</p>
<p>(1 &minus; u2)(l&minus;k)/2
dk
</p>
<p>duk
</p>
<p>[(
1 &minus; u2
</p>
<p>)l]
.
</p>
<p>If we let k = l &minus;m and make use of the result obtained in Example 13.3.3,
we obtain
</p>
<p>Ylm(θ,ϕ)=
&radic;
</p>
<p>(l +m)!
(l &minus;m)!(2l)!Cl
</p>
<p>eimϕ
</p>
<p>(1 &minus; u2)m/2
d l&minus;m
</p>
<p>dul&minus;m
[(
</p>
<p>1 &minus; u2
)l]
</p>
<p>.</p>
<p/>
</div>
<div class="page"><p/>
<p>408 13 Separation of Variables in Spherical Coordinates
</p>
<p>To specify Ylm(θ,ϕ) completely, we need to evaluate Cl . Since Cl does not
depend on m, we set m= 0 in the above expression, obtaining
</p>
<p>Yl0(u,ϕ)=
1&radic;
(2l)!Cl
</p>
<p>d l
</p>
<p>dul
</p>
<p>[(
1 &minus; u2
</p>
<p>)l]
.
</p>
<p>The RHS looks very much like the Legendre polynomial of Chap. 8. In fact,
</p>
<p>Yl0(u,ϕ)=
Cl&radic;
(2l)! (&minus;1)
</p>
<p>l2l l!Pl(u)&equiv;AlPl(u). (13.29)
</p>
<p>Therefore, the normalization of Yl0 and the Legendre polynomials Pl deter-
mines Cl .
</p>
<p>We now use Eq. (7.25) to obtain the integral form of the orthonormality
relation for Ylm:
</p>
<p>δll&prime;δmm&prime; = 〈Yl&prime;m&prime; |Ylm〉 = 〈Yl&prime;m&prime; |
(
ˆ 2π
</p>
<p>0
dϕ
</p>
<p>ˆ π
</p>
<p>0
sin θ dθ |θ,ϕ〉〈θ,ϕ|
</p>
<p>)
|Ylm〉
</p>
<p>=
ˆ 2π
</p>
<p>0
dϕ
</p>
<p>ˆ π
</p>
<p>0
Y &lowast;l&prime;m&prime;(θ,ϕ)Ylm(θ,ϕ) sin θ dθ, (13.30)
</p>
<p>which in terms of u= cos θ becomes
ˆ 2π
</p>
<p>0
dϕ
</p>
<p>ˆ 1
</p>
<p>&minus;1
Y &lowast;l&prime;m&prime;(u,ϕ)Ylm(u,ϕ)du= δll&prime;δmm&prime; . (13.31)
</p>
<p>Problem 13.15 shows that using (13.30) one gets Al =
&radic;
(2l + 1)/(4π).
</p>
<p>Therefore, Eq. (13.29) yields not only the value of Cl , but also the useful
relation
</p>
<p>Yl0(u,ϕ)=
&radic;
</p>
<p>2l + 1
4π
</p>
<p>Pl(u). (13.32)
</p>
<p>Substituting the value of Cl thus obtained, we finally get
</p>
<p>Ylm(θ,ϕ)= (&minus;1)l
&radic;
</p>
<p>2l + 1
4π
</p>
<p>eimϕ
</p>
<p>2l l!
</p>
<p>&radic;
(l +m)!
(l &minus;m)!
</p>
<p>(
1 &minus; u2
</p>
<p>)&minus;m/2
</p>
<p>&times; d
l&minus;m
</p>
<p>dul&minus;m
[(
</p>
<p>1 &minus; u2
)l]
</p>
<p>, (13.33)
</p>
<p>where u= cos θ . These functions, the eigenfunctions of L2 and Lz, are called
spherical harmonics. They occur frequently in those physical applicationsspherical harmonics
for which the Laplacian is expressed in terms of spherical coordinates.
</p>
<p>One can immediately read off the θ part of the spherical harmonics:
</p>
<p>Plm(u)= (&minus;1)l
&radic;
</p>
<p>2l + 1
4π
</p>
<p>1
</p>
<p>2l l!
</p>
<p>&radic;
(l +m)!
(l &minus;m)!
</p>
<p>(
1 &minus; u2
</p>
<p>)&minus;m/2 d l&minus;m
dul&minus;m
</p>
<p>[(
1 &minus; u2
</p>
<p>)l]
.
</p>
<p>However, this is not the version used in the literature. For historical reasons
the associated Legendre functions Pml (u) are used. These are defined by
</p>
<p>associated Legendre
</p>
<p>functions</p>
<p/>
</div>
<div class="page"><p/>
<p>13.4 Eigenvectors of L2 : Spherical Harmonics 409
</p>
<p>Pml (u)= (&minus;1)m
&radic;
(l +m)!
(l &minus;m)!
</p>
<p>&radic;
4π
</p>
<p>2l + 1Plm(u)
</p>
<p>= (&minus;1)l+m (l +m)!
(l &minus;m)!
</p>
<p>(1 &minus; u2)&minus;m/2
2l l!
</p>
<p>d l&minus;m
</p>
<p>dul&minus;m
[(
</p>
<p>1 &minus; u2
)l]
</p>
<p>.
</p>
<p>Thus,
</p>
<p>Box 13.4.2 The solutions of the angular part of the Laplacian are
</p>
<p>Ylm(θ,ϕ)= (&minus;1)m
[
</p>
<p>2l + 1
4π
</p>
<p>(l &minus;m)!
(l +m)!
</p>
<p>]1/2
Pml (cos θ)e
</p>
<p>imϕ, (13.34)
</p>
<p>where, with u= cos θ ,
</p>
<p>Pml (u)= (&minus;1)l+m
(l +m)!
(l &minus;m)!
</p>
<p>(1 &minus; u2)&minus;m/2
2l l!
</p>
<p>d l&minus;m
</p>
<p>dul&minus;m
[(
</p>
<p>1 &minus; u2
)l]
</p>
<p>.
</p>
<p>(13.35)
</p>
<p>We generated the spherical harmonics starting with Yll(θ,ϕ) and apply-
ing the lowering operator L&minus;. We could have started with Yl,&minus;l(θ,ϕ) instead,
and applied the raising operator L+. The latter procedure is identical to the
former; nevertheless, we outline it below because of some important rela-
tions that emerge along the way. We first note that
</p>
<p>|Yl,&minus;m〉 =
&radic;
</p>
<p>(l +m)!
(l &minus;m)!(2l)!L
</p>
<p>l&minus;m
+ |Yl,&minus;l〉. (13.36)
</p>
<p>(This can be obtained following the steps of Example 13.3.3.) Next, we use
L&minus;|Yl,&minus;l〉 = 0 in differential form to obtain
</p>
<p>(
d
</p>
<p>dθ
&minus; l cot θ
</p>
<p>)
Pl,&minus;l(θ)= 0,
</p>
<p>which has the same form as the differential equation for Pll . Thus, the solu-
tion is Pl,&minus;l(θ)= C&prime;l(sin θ)l , and
</p>
<p>Yl,&minus;l(θ,ϕ)= Pl,&minus;l(θ)e&minus;ilϕ = C&prime;l(sin θ)le&minus;ilϕ .
</p>
<p>Applying L+ repeatedly yields
</p>
<p>Lk+Yl,&minus;l(u,ϕ)= C&prime;l
(&minus;1)ke&minus;i(l&minus;k)ϕ
(1 &minus; u2)(l&minus;k)/2
</p>
<p>dk
</p>
<p>duk
</p>
<p>[(
1 &minus; u2
</p>
<p>)l]
,
</p>
<p>where u= cos θ . Substituting k = l &minus;m and using Eq. (13.36) gives
</p>
<p>Yl,&minus;m(u,ϕ)=
&radic;
</p>
<p>(l +m)!
(l &minus;m)!(2l)!C
</p>
<p>&prime;
l
</p>
<p>(&minus;1)l&minus;me&minus;imϕ
(1 &minus; u2)m/2
</p>
<p>d l&minus;m
</p>
<p>dul&minus;m
[(
</p>
<p>1 &minus; u2
)l]
</p>
<p>.</p>
<p/>
</div>
<div class="page"><p/>
<p>410 13 Separation of Variables in Spherical Coordinates
</p>
<p>The constant C&prime;l can be determined as before. In fact, for m = 0 we get
exactly the same result as before, so we expect C&prime;l to be identical to Cl .
Thus,
</p>
<p>Yl,&minus;m(u,ϕ)= (&minus;1)l+m
&radic;
</p>
<p>2l + 1
4π
</p>
<p>e&minus;imϕ
</p>
<p>2l l!
</p>
<p>&radic;
(l +m)!
(l &minus;m)!
</p>
<p>&times;
(
1 &minus; u2
</p>
<p>)&minus;m/2 d l&minus;m
dul&minus;m
</p>
<p>[(
1 &minus; u2
</p>
<p>)l]
.
</p>
<p>Comparison with Eq. (13.33) yields
</p>
<p>Yl,&minus;m(θ,ϕ)= (&minus;1)mY &lowast;l,m(θ,ϕ), (13.37)
</p>
<p>and using the definition Yl,&minus;m(θ,ϕ) = Pl,&minus;m(θ)e&minus;imϕ and the first part of
Eq. (13.35), we obtain
</p>
<p>P&minus;ml (θ)= (&minus;1)m
(l &minus;m)!
(l +m)!P
</p>
<p>m
l (θ). (13.38)
</p>
<p>The first few spherical harmonics with positive m are given below. Those
with negative m can be obtained using Eq. (13.37).
</p>
<p>For l = 0, Y00 =
1&radic;
4π
</p>
<p>.
</p>
<p>For l = 1, Y10 =
&radic;
</p>
<p>3
</p>
<p>4π
cos θ, Y11 =&minus;
</p>
<p>&radic;
3
</p>
<p>8π
eiϕ sin θ.
</p>
<p>For l = 2, Y20 =
&radic;
</p>
<p>5
</p>
<p>16π
</p>
<p>(
3 cos2 θ &minus; 1
</p>
<p>)
,
</p>
<p>Y21 =&minus;
&radic;
</p>
<p>15
</p>
<p>8π
eiϕ sin θ cos θ,
</p>
<p>Y22 =
&radic;
</p>
<p>15
</p>
<p>32π
e2iϕ sin2 θ.
</p>
<p>For l = 3, Y30 =
&radic;
</p>
<p>7
</p>
<p>16π
</p>
<p>(
5 cos3 θ &minus; 3 cos θ
</p>
<p>)
,
</p>
<p>Y31 =&minus;
&radic;
</p>
<p>21
</p>
<p>64π
eiϕ sin θ
</p>
<p>(
5 cos2 θ &minus; 1
</p>
<p>)
,
</p>
<p>Y32 =
&radic;
</p>
<p>105
</p>
<p>32π
e2iϕ sin2 θ cos θ,
</p>
<p>Y33 =&minus;
&radic;
</p>
<p>35
</p>
<p>64π
e3iϕ sin3 θ.
</p>
<p>From Eqs. (13.13), (13.18), and (13.34) and the fact that α = l(l+ 1) for
some nonnegative integer l, we obtain</p>
<p/>
</div>
<div class="page"><p/>
<p>13.4 Eigenvectors of L2 : Spherical Harmonics 411
</p>
<p>1
</p>
<p>sin θ
</p>
<p>&part;
</p>
<p>&part;θ
</p>
<p>(
sin θ
</p>
<p>&part;
</p>
<p>&part;θ
</p>
<p>)[
Pml e
</p>
<p>imϕ
]
+ 1
</p>
<p>sin2 θ
</p>
<p>&part;2
</p>
<p>&part;ϕ2
</p>
<p>[
Pml e
</p>
<p>imϕ
]
+ l(l + 1)Pml eimϕ
</p>
<p>= 0,
</p>
<p>which gives
</p>
<p>1
</p>
<p>sin θ
</p>
<p>d
</p>
<p>dθ
</p>
<p>(
sin θ
</p>
<p>dPml
</p>
<p>dθ
</p>
<p>)
&minus; m
</p>
<p>2
</p>
<p>sin2 θ
Pml + l(l + 1)Pml = 0.
</p>
<p>As before, we let u= cos θ to obtain
</p>
<p>d
</p>
<p>du
</p>
<p>[(
1 &minus; u2
</p>
<p>)dPml
du
</p>
<p>]
+
[
l(l + 1)&minus; m
</p>
<p>2
</p>
<p>1 &minus; u2
]
Pml = 0. (13.39)
</p>
<p>This is called the associated Legendre differential equation. Its solutions, associated Legendre
differential equationthe associated Legendre functions, are given in closed form in Eq. (13.35).
</p>
<p>For m= 0, Eq. (13.39) reduces to the Legendre differential equation whose
solutions, again given by Eq. (13.35) with m = 0, are the Legendre poly-
nomials encountered in Chap. 8. When m= 0, the spherical harmonics be-
come ϕ-independent. This corresponds to a physical situation in which there
is an explicit azimuthal symmetry. In such cases (when it is obvious that the
physical property in question does not depend on ϕ) a Legendre polynomial,
depending only on cos θ , will multiply the radial function.
</p>
<p>13.4.1 Expansion of Angular Functions
</p>
<p>The orthonormality of spherical harmonics can be utilized to expand func-
tions of θ and ϕ in terms of them. The fact that these functions are complete
will be discussed in a general way in the context of Sturm-Liouville systems.
Assuming completeness for now, we write
</p>
<p>f (θ,ϕ)=
</p>
<p>⎧
⎨
⎩
</p>
<p>&sum;&infin;
l=0
</p>
<p>&sum;l
m=&minus;l almYlm(θ,ϕ) if l is not fixed,
</p>
<p>&sum;l
m=&minus;l almYlm(θ,ϕ) if l is fixed,
</p>
<p>(13.40)
</p>
<p>where we have included the case where it is known a priori that f (θ,ϕ) has
a given fixed l value. To find alm, we multiply both sides by Y &lowast;lm(θ,ϕ) and
integrate over the solid angle. The result, obtained by using the orthonor-
mality relation, is
</p>
<p>alm =
&uml;
</p>
<p>dΩf (θ,ϕ)Y &lowast;lm(θ,ϕ), (13.41)
</p>
<p>where dΩ &equiv; sin θ dθ dϕ is the element of solid angle. A useful special case
of this formula is
</p>
<p>a
(f )
</p>
<p>l0 =
&uml;
</p>
<p>dΩf (θ,ϕ)Y &lowast;l0(θ,ϕ)
</p>
<p>=
&radic;
</p>
<p>2l + 1
4π
</p>
<p>&uml;
</p>
<p>dΩf (θ,ϕ)Pl(cos θ), (13.42)</p>
<p/>
</div>
<div class="page"><p/>
<p>412 13 Separation of Variables in Spherical Coordinates
</p>
<p>Fig. 13.1 The unit vectors êr and êr &prime; with their spherical angles and the angle γ between
them
</p>
<p>where we have introduced an extra superscript to emphasize the relation of
the expansion coefficients with the function being expanded. Another useful
relation is obtained when we let θ = 0 in Eq. (13.40):
</p>
<p>f (θ,ϕ)
∣∣
θ=0 =
</p>
<p>⎧
⎨
⎩
</p>
<p>&sum;&infin;
l=0
</p>
<p>&sum;l
m=&minus;l almYlm(θ,ϕ)|θ=0 if l is not fixed,
</p>
<p>&sum;l
m=&minus;l almYlm(θ,ϕ)|θ=0 if l is fixed.
</p>
<p>From Eqs. (13.35) and (13.34) one can show that
</p>
<p>Ylm(θ,ϕ)
∣∣
θ=0 = δm0Yl0(0, ϕ)= δm0
</p>
<p>&radic;
2l + 1
</p>
<p>4π
.
</p>
<p>Therefore,
</p>
<p>f (θ,ϕ)
∣∣
θ=0 =
</p>
<p>⎧
⎪⎨
⎪⎩
</p>
<p>&sum;&infin;
l=0 a
</p>
<p>(f )
</p>
<p>l0
</p>
<p>&radic;
2l+1
4π if l is not fixed,
</p>
<p>a
(f )
</p>
<p>l0
</p>
<p>&radic;
2l+1
4π if l is fixed.
</p>
<p>(13.43)
</p>
<p>13.4.2 Addition Theorem for Spherical Harmonics
</p>
<p>An important consequence of the expansion in terms of Ylm is called the
addition theorem for spherical harmonics. Consider two unit vectors êraddition theorem for
</p>
<p>spherical harmonics and êr &prime; making spherical angles (θ,ϕ) and (θ &prime;, ϕ&prime;), respectively, as shown in
Fig. 13.1. Let γ be the angle between the two vectors. The addition theorem
states that
</p>
<p>Pl(cosγ )=
4π
</p>
<p>2l + 1
</p>
<p>l&sum;
</p>
<p>m=&minus;l
Y &lowast;lm
</p>
<p>(
θ &prime;, ϕ&prime;
</p>
<p>)
Ylm(θ,ϕ). (13.44)</p>
<p/>
</div>
<div class="page"><p/>
<p>13.5 Problems 413
</p>
<p>We shall not give a proof of this theorem here and refer the reader to an el-
egant proof on page 974 which uses the representation theory of groups. The
addition theorem is particularly useful in the expansion of the frequently oc-
curring expression 1/|r&minus;r&prime;|. For definiteness we assume |r&prime;| &equiv; r &prime; &lt; |r| &equiv; r .
Then, introducing t = r &prime;/r , we have
</p>
<p>1
</p>
<p>|r &minus; r&prime;| =
1
</p>
<p>(r2 + r &prime;2 &minus; 2rr &prime; cosγ )1/2 =
1
</p>
<p>r
</p>
<p>(
1 + t2 &minus; 2t cosγ
</p>
<p>)&minus;1/2
.
</p>
<p>Recalling the generating function for Legendre polynomials from Chap. 8
and using the addition theorem, we get
</p>
<p>1
</p>
<p>|r &minus; r&prime;| =
1
</p>
<p>r
</p>
<p>&infin;&sum;
</p>
<p>l=0
t lPl(cosγ )=
</p>
<p>&infin;&sum;
</p>
<p>l=0
</p>
<p>r &prime;l
</p>
<p>r l+1
4π
</p>
<p>2l + 1
</p>
<p>l&sum;
</p>
<p>m=&minus;l
Y &lowast;lm
</p>
<p>(
θ &prime;, ϕ&prime;
</p>
<p>)
Ylm(θ,ϕ)
</p>
<p>= 4π
&infin;&sum;
</p>
<p>l=0
</p>
<p>l&sum;
</p>
<p>m=&minus;l
</p>
<p>1
</p>
<p>2l + 1
r &prime;l
</p>
<p>r l+1
Y &lowast;lm
</p>
<p>(
θ &prime;, ϕ&prime;
</p>
<p>)
Ylm(θ,ϕ).
</p>
<p>It is clear that if r &lt; r &prime;, we should expand in terms of the ratio r/r &prime;. It is
therefore customary to use r&lt; to denote the smaller and r&gt; to denote the
larger of the two radii r and r &prime;. Then the above equation is written as expansion of 1/|r&minus; r&prime;| in
</p>
<p>spherical coordinates
</p>
<p>1
</p>
<p>|r &minus; r&prime;| = 4π
&infin;&sum;
</p>
<p>l=0
</p>
<p>l&sum;
</p>
<p>m=&minus;l
</p>
<p>1
</p>
<p>2l + 1
r l&lt;
</p>
<p>r l+1&gt;
Y &lowast;lm
</p>
<p>(
θ &prime;, ϕ&prime;
</p>
<p>)
Ylm(θ,ϕ). (13.45)
</p>
<p>This equation is used frequently in the study of Coulomb-like potentials.
</p>
<p>13.5 Problems
</p>
<p>13.1 By applying the operator [xj ,pk] to an arbitrary function f (r), show
that [xj ,pk] = iδjk .
</p>
<p>13.2 Use the defining relation Li = ǫijkxjpk to show that xjpk &minus; xkpj =
ǫijkLi . In both of these expressions a sum over the repeated indices is un-
derstood.
</p>
<p>13.3 For the angular momentum operator Li = ǫijkxjpk , show that the com-
mutation relation [Lj ,Lk] = iǫjklLl holds.
</p>
<p>13.4 Evaluate &part;f/&part;y and &part;f/&part;z in spherical coordinates and find Ly and Lz
in terms of spherical coordinates.
</p>
<p>13.5 Obtain an expression for L2 in terms of θ and ϕ, and substitute the
result in Eq. (13.12) to get the Laplacian in spherical coordinates.
</p>
<p>13.6 Show that L2 = L+L&minus; + L2z &minus; Lz and L2 = L&minus;L+ + L2z + Lz.</p>
<p/>
</div>
<div class="page"><p/>
<p>414 13 Separation of Variables in Spherical Coordinates
</p>
<p>13.7 Show that L2, Lx , Ly , and Lz are hermitian operators in the space of
square-integrable functions.
</p>
<p>13.8 Verify the following commutation relations:
</p>
<p>[
L2,L&plusmn;
</p>
<p>]
= 0, [Lz,L&plusmn;] = &plusmn;L&plusmn;, [L+,L&minus;] = 2Lz.
</p>
<p>13.9 Show that L&minus;|Yαβ〉 has β &minus; 1 as its eigenvalue for Lz, and that |Yα,β&plusmn;〉
cannot be zero.
</p>
<p>13.10 Show that if the |Yjm〉 are normalized to unity, then with proper
choice of phase, L&minus;|Yjm〉 =
</p>
<p>&radic;
j (j + 1)&minus;m(m&minus; 1)|Yj,m&minus;1〉.
</p>
<p>13.11 Derive Eq. (13.36).
</p>
<p>13.12 Starting with Lx and Ly , derive the following expression for L&plusmn;:
</p>
<p>L&plusmn; = e&plusmn;iϕ
(
&plusmn; &part;
&part;θ
</p>
<p>+ i cot θ &part;
&part;ϕ
</p>
<p>)
.
</p>
<p>13.13 Integrate dP/dθ &minus; l cot θP = 0 to find P(θ).
</p>
<p>13.14 Verify the following differential identity:
</p>
<p>(
d
</p>
<p>dθ
+ n cot θ
</p>
<p>)
f (θ)= 1
</p>
<p>sinn θ
</p>
<p>d
</p>
<p>dθ
</p>
<p>[
sinn θf (θ)
</p>
<p>]
.
</p>
<p>13.15 Let l = l&prime; and m=m&prime; = 0 in Eq. (13.31), and substitute for Yl0 from
Eq. (13.29) to obtain Al =
</p>
<p>&radic;
(2l + 1)/4π .
</p>
<p>13.16 Show that
</p>
<p>Lk+Yl,&minus;l(u,ϕ)= C&prime;l
(&minus;1)ke&minus;i(l&minus;k)ϕ
(1 &minus; u2)(l&minus;k)/2
</p>
<p>dk
</p>
<p>duk
</p>
<p>[(
1 &minus; u2
</p>
<p>)l]
.
</p>
<p>13.17 Derive the relations Yl,&minus;m(θ,ϕ)= (&minus;1)mY &lowast;l,m(θ,ϕ) and
</p>
<p>P&minus;ml (θ)= (&minus;1)m
(l &minus;m)!
(l +m)!P
</p>
<p>m
l (θ).
</p>
<p>13.18 Show that
</p>
<p>l&sum;
</p>
<p>m=&minus;l
</p>
<p>∣∣Ylm(θ,ϕ)
∣∣2 = 2l + 1
</p>
<p>4π
.
</p>
<p>Verify this explicitly for l = 1 and l = 2.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.5 Problems 415
</p>
<p>13.19 Show that the addition theorem for spherical harmonics can be writ-
ten as
</p>
<p>Pl(cosγ )= Pl(cos θ)Pl
(
cos θ &prime;
</p>
<p>)
</p>
<p>+ 2
l&sum;
</p>
<p>m=1
</p>
<p>(l &minus;m)!
(l +m)!P
</p>
<p>m
l (cos θ)P
</p>
<p>m
l
</p>
<p>(
cos θ &prime;
</p>
<p>)
cos
</p>
<p>[
m
(
ϕ &minus; ϕ&prime;
</p>
<p>)]
.</p>
<p/>
</div>
<div class="page"><p/>
<p>14Second-Order Linear DifferentialEquations
</p>
<p>The discussion of Chap. 13 has clearly singled out ODEs, especially those
of second order, as objects requiring special attention because most com-
mon PDEs of mathematical physics can be separated into ODEs (of second
order). This is really an oversimplification of the situation. Many PDEs of
physics, both at the fundamental theoretical level (as in the general theory
of relativity) and from a practical standpoint (weather forecast) are nonlin-
ear, and the method of the separation of variables does not work. Since no
general analytic solutions for such nonlinear systems have been found, we
shall confine ourselves to the linear systems, especially those that admit a
separated solution.
</p>
<p>With the exception of the infinite power series, no systematic method
of solving DEs existed during the first half of the nineteenth century. The
majority of solutions were completely ad hoc and obtained by trial and error,
causing frustration and anxiety among mathematicians. It was to overcome
this frustration that Sophus Lie, motivated by the newly developed concept
of group, took up the systematic study of DEs in the second half of the
nineteenth century. This study not only gave a handle on the disarrayed area
of DEs, but also gave birth to one of the most beautiful and fundamental
branches of mathematical physics, Lie group theory. We shall come back to
a thorough treatment of this theory in Parts VII and IX.
</p>
<p>Our main task in this chapter is to study the second-order linear differen-
tial equations (SOLDEs). However, to understand SOLDEs, we need some
basic understanding of differential equations in general. The next section
outlines some essential properties of general DEs. Section 2 is a very brief
introduction to first-order DEs, and the remainder of the chapter deals with
SOLDEs.
</p>
<p>14.1 General Properties of ODEs
</p>
<p>The most general ODE can be expressed as
</p>
<p>G
</p>
<p>(
x, y,
</p>
<p>dy
</p>
<p>dx
,
d2y
</p>
<p>dx2
, . . . ,
</p>
<p>dny
</p>
<p>dxn
</p>
<p>)
= 0, (14.1)
</p>
<p>S. Hassani, Mathematical Physics, DOI 10.1007/978-3-319-01195-0_14,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>417</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-01195-0_14">http://dx.doi.org/10.1007/978-3-319-01195-0_14</a></div>
</div>
<div class="page"><p/>
<p>418 14 Second-Order Linear Differential Equations
</p>
<p>in which G : Rn+2 &rarr; R is a real-valued function of n + 2 real variables.
When G depends explicitly and nontrivially on dny/dxn, Eq. (14.1) is
called an nth-order ODE. An ODE is said to be linear if the part of the
function G that includes y and all its derivatives is linear in y. The most
general nth-order linear ODE is
</p>
<p>p0(x)y + p1(x)
dy
</p>
<p>dx
+ &middot; &middot; &middot; + pn(x)
</p>
<p>dny
</p>
<p>dxn
= q(x) for pn(x) �= 0, (14.2)
</p>
<p>where {pi}ni=0 and q are functions of the independent variable x. Equa-
tion (14.2) is said to be homogeneous if q = 0; otherwise, it is said to be
</p>
<p>homogeneous and
</p>
<p>inhomogeneous ODEs
inhomogeneous and q(x) is called the inhomogeneous term. It is customary,
and convenient, to define a linear differential operator L by1
</p>
<p>L&equiv; p0(x)+ p1(x)
d
</p>
<p>dx
+ &middot; &middot; &middot; + pn(x)
</p>
<p>dn
</p>
<p>dxn
, pn(x) �= 0, (14.3)
</p>
<p>and write Eq. (14.2) as
</p>
<p>L[y] = q(x). (14.4)
A solution of Eq. (14.1) or (14.4) is a single-variable function f :R&rarr;R
</p>
<p>such that G(x,f (x), f &prime;(x), . . . , f (n)(x)) = 0, or L[f ] = q(x), for all x in
the domain of definition of f . The solution of a differential equation may
not exist if we put too many restrictions on it. For instance, if we demand
that f :R&rarr;R be differentiable too many times, we may not be able to find
a solution, as the following example shows.
</p>
<p>Example 14.1.1 The most general solution of dy/dx = |x| that vanishes at
x = 0 is
</p>
<p>f (x)=
{
</p>
<p>1
2x
</p>
<p>2 if x &ge; 0,
&minus; 12x2 if x &le; 0.
</p>
<p>This function is continuous and has first derivative f &prime;(x) = |x|, which is
also continuous at x = 0. However, if we demand that its second derivative
also be continuous at x = 0, we cannot find a solution, because
</p>
<p>f &prime;&prime;(x)=
{
+1 if x &gt; 0,
&minus;1 if x &lt; 0.
</p>
<p>If we want f &prime;&prime;&prime;(x) to exist at x = 0, then we have to expand the notion of a
function to include distributions, or generalized functions.
</p>
<p>Overrestricting a solution for a differential equation results in its absence,
but underrestricting it allows multiple solutions. To strike a balance between
these two extremes, we agree to make a solution as many times differen-
tiable as plausible and to satisfy certain initial conditions. For an nth-order
</p>
<p>1Do not confuse this linear differential operator with the angular momentum (vector)
</p>
<p>operator 0L.</p>
<p/>
</div>
<div class="page"><p/>
<p>14.2 Existence/Uniqueness for First-Order DEs 419
</p>
<p>DE such initial conditions are commonly equivalent (but not restricted) to
a specification of the function and of its first n&minus; 1 derivatives. This sort of
specification is made feasible by the following theorem.
</p>
<p>Theorem 14.1.2 (Implicit function theorem) Let G :Rn+1 &rarr;R have con-
implicit function
</p>
<p>theorem
tinuous partial derivatives up to the kth order in some neighborhood of a
point P0 = (r1, r2, . . . , rn+1) in Rn+1. Let (&part;G/&part;xn+1)|P0 �= 0. Then there
exists a unique function F : Rn &rarr; R that is continuously differentiable k
times at (some smaller) neighborhood of P0 such that
</p>
<p>xn+1 = F(x1, x2, . . . , xn)
</p>
<p>for all points P = (x1, x2, . . . , xn+1) in a neighborhood of P0 and
</p>
<p>G
(
x1, x2, . . . , xn,F (x1, x2, . . . , xn)
</p>
<p>)
= 0.
</p>
<p>Theorem 14.1.2 simply asserts that under certain (mild) conditions we
can &ldquo;solve&rdquo; for one of the independent variables in G(x1, x2, . . . , xn+1)= 0
in terms of the others. A proof of this theorem can be found in advanced
calculus books.
</p>
<p>Application of this theorem to Eq. (14.1) leads to
</p>
<p>dny
</p>
<p>dxn
= F
</p>
<p>(
x, y,
</p>
<p>dy
</p>
<p>dx
,
d2y
</p>
<p>dx2
, . . . ,
</p>
<p>dn&minus;1y
dxn&minus;1
</p>
<p>)
,
</p>
<p>provided that G satisfies the conditions of the theorem. If we know the so-
lution y = f (x) and its derivatives up to order n&minus; 1, we can evaluate its nth
derivative using this equation. In addition, we can calculate the derivatives
of all orders (assuming they exist) by differentiating this equation. This al-
lows us to expand the solution in a Taylor series. Thus&mdash;for solutions that
have derivatives of all orders&mdash;knowledge of the value of a solution and its
first n&minus; 1 derivatives at a point x0 determines that solution at a neighboring
point x.
</p>
<p>We shall not study the general ODE of Eq. (14.1) or even its simpler
linear version (14.2). We will only briefly study ODEs of the first order in
the next section, and then concentrate on linear ODEs of the second order
for the rest of this chapter.
</p>
<p>14.2 Existence/Uniqueness for First-Order DEs
</p>
<p>A general first-order DE (FODE) is of the form G(x,y, y&prime;) = 0. We can
find y&prime; (the derivative of y) in terms of a function of x and y if the func-
tion G(x1, x2, x3) is differentiable with respect to its third argument and
&part;G/&part;x3 �= 0. In that case we have
</p>
<p>the most general FODE
</p>
<p>in normal form
y&prime; &equiv; dy
</p>
<p>dx
= F(x, y), (14.5)</p>
<p/>
</div>
<div class="page"><p/>
<p>420 14 Second-Order Linear Differential Equations
</p>
<p>which is said to be a normal FODE. If F(x, y) is a linear function of y, then
Eq. (14.5) becomes a first-order linear DE (FOLDE), which can generally
be written as
</p>
<p>p1(x)
dy
</p>
<p>dx
+ p0(x)y = q(x). (14.6)
</p>
<p>It can be shown that the general FOLDE has an explicit solution: (see
[Hass 08])
</p>
<p>Theorem 14.2.1 Any first order linear DE of the form p1(x)y&prime;+p0(x)y =
explicit solution to a
</p>
<p>general first-order linear
</p>
<p>differential equation
</p>
<p>q(x), in which p0, p1, and q are continuous functions in some interval
(a, b), has a general solution
</p>
<p>y = f (x)= 1
μ(x)p1(x)
</p>
<p>[
C +
</p>
<p>&int; x
</p>
<p>x1
</p>
<p>μ(t)q(t) dt
</p>
<p>]
, (14.7)
</p>
<p>where C is an arbitrary constant and
</p>
<p>μ(x)= 1
p1(x)
</p>
<p>exp
</p>
<p>[&int; x
</p>
<p>x0
</p>
<p>p0(t)
</p>
<p>p1(t)
dt
</p>
<p>]
, (14.8)
</p>
<p>where x0 and x1 are arbitrary points in the interval (a, b).
</p>
<p>No such explicit solution exists for nonlinear first-order DEs. Neverthe-
less, it is reassuring to know that a solution of such a DE always exists and
under some mild conditions, this solution is unique. We summarize some
of the ideas involved in the proof of the existence and uniqueness of the
solutions to FODEs. (For proofs, see the excellent book by Birkhoff and
Rota [Birk 78].) We first state an existence theorem due to Peano:
</p>
<p>Theorem 14.2.2 (Peano existence theorem) If the function F(x, y) is con-Peano existence
theorem tinuous for the points on and within the rectangle defined by |y &minus; c| &le; K
</p>
<p>and |x &minus; a| &le; N , and if |F(x, y)| &le; M there, then the differential equa-
tion y&prime; = F(x, y) has at least one solution, y = f (x), defined for |x &minus; a| &le;
min(N,K/M) and satisfying the initial condition f (a)= c.
</p>
<p>This theorem guarantees only the existence of solutions. To ensure
uniqueness, the function F needs to have some additional properties. An
important property is stated in the following definition.
</p>
<p>Definition 14.2.3 A function F(x, y) satisfies a Lipschitz condition in aLipschitz condition
domain D &sub;R2 if for some finite constant L (Lipschitz constant), it satisfies
the inequality
</p>
<p>∣∣F(x, y1)&minus; F(x, y2)
∣∣&le; L|y1 &minus; y2|
</p>
<p>for all points (x, y1) and (x, y2) in D.
</p>
<p>Theorem 14.2.4 (Uniqueness) Let f (x) and g(x) be any two solutions ofuniqueness theorem</p>
<p/>
</div>
<div class="page"><p/>
<p>14.3 General Properties of SOLDEs 421
</p>
<p>the FODE y&prime; = F(x, y) in a domain D, where F satisfies a Lipschitz con-
dition with Lipschitz constant L. Then
</p>
<p>∣∣f (x)&minus; g(x)
∣∣&le; eL|x&minus;a|
</p>
<p>∣∣f (a)&minus; g(a)
∣∣.
</p>
<p>In particular, the FODE has at most one solution curve passing through the
point (a, c) &isin;D.
</p>
<p>The final conclusion of this theorem is an easy consequence of the as-
sumed differentiability of F and the requirement f (a) = g(a) = c. The
theorem says that if there is a solution y = f (x) to the DE y&prime; = F(x, y)
satisfying f (a)= c, then it is the solution.
</p>
<p>The requirements of the Peano existence theorem are too broad to yield
solutions that have some nice properties. For instance, the interval of def-
inition of the solutions may depend on their initial values. The following
example illustrates this point.
</p>
<p>Example 14.2.5 Consider the DE dy/dx = ey . The general solution of this
DE can be obtained by direct integration:
</p>
<p>e&minus;ydy = dx &rArr; &minus;e&minus;y = x +C.
</p>
<p>If y = b when x = 0, then C =&minus;e&minus;b , and
</p>
<p>e&minus;y =&minus;x + e&minus;b &rArr; y =&minus; ln
(
e&minus;b &minus; x
</p>
<p>)
.
</p>
<p>Thus, the solution is defined for &minus;&infin;&lt; x &lt; e&minus;b , i.e., the interval of defini-
tion of a solution changes with its initial value.
</p>
<p>To avoid situations illustrated in the example above, one demands not
just the continuity of F&mdash;as does the Peano existence theorem&mdash;but a Lips-
chitz condition for it. Then one ensures not only the existence, but also the
uniqueness:
</p>
<p>Theorem 14.2.6 (Local existence and uniqueness) Suppose that the func-
local existence and
</p>
<p>uniqueness theorem
tion F(x, y) is defined and continuous in the rectangle
</p>
<p>|y &minus; c| &le;K, |x &minus; a| &le;N
</p>
<p>and satisfies a Lipschitz condition there. Let M = max |F(x, y)| in this
rectangle. Then the differential equation y&prime; = F(x, y) has a unique solu-
tion y = f (x) satisfying f (a) = c and defined on the interval |x &minus; a| &le;
min(N,K/M).
</p>
<p>14.3 General Properties of SOLDEs
</p>
<p>The most general SOLDE is
</p>
<p>p2(x)
d2y
</p>
<p>dx2
+ p1(x)
</p>
<p>dy
</p>
<p>dx
+ p0(x)y = p3(x). (14.9)</p>
<p/>
</div>
<div class="page"><p/>
<p>422 14 Second-Order Linear Differential Equations
</p>
<p>Dividing by p2(x) and writing p for p1/p2, q for p0/p2, and r for p3/p2
reduces this to the normal form
</p>
<p>normal form of a SOLDE
</p>
<p>d2y
</p>
<p>dx2
+ p(x)dy
</p>
<p>dx
+ q(x)y = r(x). (14.10)
</p>
<p>Equation (14.10) is equivalent to (14.9) if p2(x) �= 0. The points at which
p2(x) vanishes are called the singular points of the differential equation.
</p>
<p>singular points of a
</p>
<p>SOLDE
There is a crucial difference between the singular points of linear differ-
</p>
<p>ential equations and those of nonlinear differential equations. For a nonlin-
ear differential equation such as (x2 &minus; y)y&prime; = x2 + y2, the curve y = x2
is the collection of singular points. This makes it impossible to construct
solutions y = f (x) that are defined on an interval I = [a, b] of the x-axis
because for any x &isin; I , there is a y for which the differential equation is
undefined. Linear differential equations do not have this problem, because
the coefficients of the derivatives are functions of x only. Therefore, all the
singular &ldquo;curves&rdquo; are vertical. Thus, we have the following:
</p>
<p>Definition 14.3.1 The normal form of a SOLDE, Eq. (14.10), is regular
regular SOLDE
</p>
<p>on an interval [a, b] of the x-axis if p(x), q(x), and r(x) are continuous
on [a, b]. A solution of a normal SOLDE is a twice-differentiable function
y = f (x) that satisfies the SOLDE at every point of [a, b].
</p>
<p>Any function that satisfies (14.10) or (14.9) must necessarily be twice
differentiable, and that is all that is demanded of the solutions. Any higher-
order differentiability requirement may be too restrictive, as was pointed out
in Example 14.1.1. Most solutions to a normal SOLDE, however, automati-
cally have derivatives of order higher than two.
</p>
<p>We write Eq. (14.9) in the operator form as
</p>
<p>L[y] = p3, where L&equiv; p2
d2
</p>
<p>dx2
+ p1
</p>
<p>d
</p>
<p>dx
+ p0. (14.11)
</p>
<p>It is clear that L is a linear operator because d/dx is linear, as are all powers
of it. Thus, for constants α and β ,
</p>
<p>L[αy1 + βy2] = αL[y1] + βL[y2].
</p>
<p>In particular, if y1 and y2 are two solutions of Eq. (14.11), then L[y1 &minus;
y2] = 0. That is, the difference between any two solutions of a SOLDE is a
solution of the homogeneous equation obtained by setting p3 = 0.2
</p>
<p>An immediate consequence of the linearity of L is the following:
</p>
<p>Lemma 14.3.2 If L[u] = r(x), L[v] = s(x), α and β are constants, and
w = αu+ βy, then L[w] = αr(x)+ βs(x).
</p>
<p>The proof of this lemma is trivial, but the result describes the fundamen-
tal property of linear operators: When r = s = 0, that is, in dealing with
</p>
<p>2This conclusion is, of course, not limited to the SOLDE; it holds for all linear DEs.</p>
<p/>
</div>
<div class="page"><p/>
<p>14.3 General Properties of SOLDEs 423
</p>
<p>homogeneous equations, the lemma says that any linear combination of so-
lutions of the homogeneous SOLDE (HSOLDE) is also a solution. This is
called the superposition principle. superposition principle
</p>
<p>Based on physical intuition, we expect to be able to predict the behav-
ior of a physical system if we know the differential equation obeyed by that
system, and, equally importantly, the initial data. Physical intuition also tells
us that if the initial conditions are changed by an infinitesimal amount, then
the solutions will be changed infinitesimally. Thus, the solutions of linear
differential equations are said to be continuous functions of the initial con-
ditions.
</p>
<p>Remark 14.3.1 Nonlinear differential equations can have completely dif- The rise and fall of chaos
ferent solutions for two initial conditions that are infinitesimally close.
Since initial conditions cannot be specified with mathematical precision in
practice, nonlinear differential equations lead to unpredictable solutions, or
chaos. Chaos was a hot topic in the late 1980s and early 1990s. Some en-
thusiasts called it the third pillar of modern physics on a par with relativity
and quantum physics. The enthusiasm has waned, however, because chaos,
driven entirely by the availability of computers and their superb graphic ca-
pabilities, has produced absolutely no fundamental results comparable with
relativity and quantum theory.
</p>
<p>A prediction is not a prediction unless it is unique. This expectation for
linear equations is borne out in the language of mathematics in the form of
an existence theorem and a uniqueness theorem. We consider the latter next.
But first, we need a lemma.
</p>
<p>Lemma 14.3.3 The only solution g(x) of the homogeneous differential
equation y&prime;&prime; + py&prime; + qy = 0 defined on the interval [a, b] that satisfies
g(a)= 0 = g&prime;(a) is the trivial solution g = 0.
</p>
<p>Proof Introduce the nonnegative function u(x) &equiv; [g(x)]2 + [g&prime;(x)]2 and
differentiate it to get
</p>
<p>u&prime;(x)= 2g&prime;g + 2g&prime;g&prime;&prime; = 2g&prime;
(
g + g&prime;&prime;
</p>
<p>)
= 2g&prime;
</p>
<p>(
g &minus; pg&prime; &minus; qg
</p>
<p>)
</p>
<p>=&minus;2p
(
g&prime;
)2 + 2(1 &minus; q)gg&prime;.
</p>
<p>Since (g &plusmn; g&prime;)2 &ge; 0, it follows that 2|gg&prime;| &le; g2 + g&prime;2. Thus,
</p>
<p>2(1 &minus; q)gg&prime; &le; 2
∣∣(1 &minus; q)gg&prime;
</p>
<p>∣∣= 2
∣∣(1 &minus; q)
</p>
<p>∣∣ ∣∣gg&prime;
∣∣
</p>
<p>&le;
∣∣(1 &minus; q)
</p>
<p>∣∣(g2 + g&prime;2
)
&le;
(
1 + |q|
</p>
<p>)(
g2 + g&prime;2
</p>
<p>)
,
</p>
<p>and therefore,
</p>
<p>u&prime;(x)&le;
∣∣u&prime;(x)
</p>
<p>∣∣=
∣∣&minus;2pg&prime;2 + 2(1 &minus; q)gg&prime;
</p>
<p>∣∣
</p>
<p>&le; 2|p|g&prime;2 +
(
1 + |q|
</p>
<p>)(
g2 + g&prime;2
</p>
<p>)
</p>
<p>=
[
1 +
</p>
<p>∣∣q(x)
∣∣]g2 +
</p>
<p>[
1 +
</p>
<p>∣∣q(x)
∣∣+ 2
</p>
<p>∣∣p(x)
∣∣]g&prime;2.</p>
<p/>
</div>
<div class="page"><p/>
<p>424 14 Second-Order Linear Differential Equations
</p>
<p>Now let K = 1+max[|q(x)| + 2|p(x)|], where the maximum is taken over
[a, b]. Then we obtain
</p>
<p>u&prime;(x)&le;K
(
g2 + g&prime;2
</p>
<p>)
=Ku(x) &forall;x &isin; [a, b].
</p>
<p>Using the result of Problem 14.1 yields u(x) &le; u(a)eK(x&minus;a) for all x &isin;
[a, b]. This equation, plus u(a) = 0, as well as the fact that u(x) &ge; 0 im-
ply that u(x)= g2(x)+ g&prime;2(x)= 0. It follows that g(x)= 0 = g&prime;(x) for all
x &isin; [a, b]. �
</p>
<p>uniqueness of solutions
</p>
<p>to SOLDE
</p>
<p>Theorem 14.3.4 (Uniqueness) If p and q are continuous on [a, b],
then at most one solution y = f (x) of the DE y&prime;&prime;+p(x)y&prime;+ q(x)y =
0 can satisfy the initial conditions f (a) = c1 and f &prime;(a) = c2, where
c1 and c2 are arbitrary constants.
</p>
<p>Proof Let f1 and f2 be two solutions satisfying the given initial conditions.
Then their difference, g &equiv; f1&minus;f2, satisfies the homogeneous equation [with
r(x) = 0]. The initial condition that g(x) satisfies is clearly g(a) = 0 =
g&prime;(a). By Lemma 14.3.3, g = 0 or f1 = f2. �
</p>
<p>Theorem 14.3.4 can be applied to any homogeneous SOLDE to find the
latter&rsquo;s most general solution. In particular, let f1(x) and f2(x) be any two
solutions of
</p>
<p>y&prime;&prime; + p(x)y&prime; + q(x)y = 0 (14.12)
</p>
<p>defined on the interval [a, b]. Assume that the two vectors v1 = (f1(a),f &prime;1(a))
and v2 = (f2(a), f &prime;2(a)) in R2 are linearly independent.3 Let g(x) be an-
other solution. The vector (g(a), g&prime;(a)) can be written as a linear combina-
tion of v1 and v2, giving the two equations
</p>
<p>g(a)= c1f1(a)+ c2f2(a),
g&prime;(a)= c1f &prime;1(a)+ c2f &prime;2(a).
</p>
<p>Now consider the function u(x)&equiv; g(x)&minus;c1f1(x)&minus;c2f2(x), which satisfies
Eq. (14.12) and the initial conditions u(a)= u&prime;(a)= 0. By Lemma 14.3.3,
we must have u(x) = 0 or g(x) = c1f1(x)+ c2f2(x). We have proved the
following:
</p>
<p>Theorem 14.3.5 Let f1 and f2 be two solutions of the HSOLDE
</p>
<p>y&prime;&prime; + py&prime; + qy = 0,
</p>
<p>3If they are not, then one must choose a different initial point for the interval.</p>
<p/>
</div>
<div class="page"><p/>
<p>14.4 The Wronskian 425
</p>
<p>where p and q are continuous functions defined on the interval [a, b]. If
(
f1(a), f
</p>
<p>&prime;
1(a)
</p>
<p>)
and
</p>
<p>(
f2(a), f
</p>
<p>&prime;
2(a)
</p>
<p>)
</p>
<p>are linearly independent vectors in R2, then every solution g(x) of this
HSOLDE is equal to some linear combination g(x) = c1f1(x) + c2f2(x)
of f1 and f2 with constant coefficients c1 and c2.
</p>
<p>14.4 TheWronskian
</p>
<p>The two solutions f1(x) and f2(x) in Theorem 14.3.5 have the property that
any other solution g(x) can be expressed as a linear combination of them.
We call f1 and f2 a basis of solutions of the HSOLDE. To form a basis of basis of solutions
solutions, f1 and f2 must be linearly independent.4
</p>
<p>Definition 14.4.1 The Wronskian of any two differentiable functions Wronskian defined
f1(x) and f2(x) is
</p>
<p>W(f1, f2;x)= f1(x)f &prime;2(x)&minus; f2(x)f &prime;1(x)= det
(
f1(x) f
</p>
<p>&prime;
1(x)
</p>
<p>f2(x) f
&prime;
2(x)
</p>
<p>)
.
</p>
<p>Proposition 14.4.2 The Wronskian of any two solutions of Eq. (14.12) sat-
isfies
</p>
<p>W(f1, f2;x)=W(f1, f2; c)e&minus;
&int; x
c p(t) dt ,
</p>
<p>where c is any number in the interval [a, b].
</p>
<p>Proof Differentiating both sides of the definition of Wronskian and substi-
tuting from Eq. (14.12) yields a FOLDE for W(f1, f2;x), which can be
easily solved. The details are left as a problem. �
</p>
<p>An important consequence of Proposition 14.4.2 is that the Wronskian of
any two solutions of Eq. (14.12) does not change sign in [a, b]. In particular,
if the Wronskian vanishes at one point in [a, b], it vanishes at all points in
[a, b].
</p>
<p>The real importance of the Wronskian is contained in the following the-
orem, whose straightforward proof is left as an exercise for the reader.
</p>
<p>Theorem 14.4.3 Two differentiable functions f1 and f2, which are nonzero
in the interval [a, b], are linearly dependent if and only if their Wronskian
vanishes.
</p>
<p>Historical Notes
</p>
<p>Josef Ho&euml;n&eacute; de Wronski (1778&ndash;1853) was born Josef Ho&euml;n&eacute;, but he adopted the name
Wronski around 1810 just after he married. He had moved to France and become a French
</p>
<p>4The linear dependence or independence of a number of functions {fi}ni=1 : [a, b]&rarr;R is
a concept that must hold for all x &isin; [a, b].</p>
<p/>
</div>
<div class="page"><p/>
<p>426 14 Second-Order Linear Differential Equations
</p>
<p>citizen in 1800 and moved to Paris in 1810, the same year he published his first memoir on
</p>
<p>Josef Ho&euml;n&eacute; de Wronski
</p>
<p>1778&ndash;1853
</p>
<p>the foundations of mathematics, which received less than favorable reviews from Lacroix
and Lagrange. His other interests included the design of caterpillar vehicles to compete
with the railways. However, they were never manufactured.
Wronski was interested mainly in applying philosophy to mathematics, the philosophy
taking precedence over rigorous mathematical proofs. He criticised Lagrange&rsquo;s use of
infinite series and introduced his own ideas for series expansions of a function. The coef-
ficients in this series are determinants now known as Wronskians [so named by Thomas
Muir (1844&ndash;1934), a Glasgow High School science master who became an authority on
determinants by devoting most of his life to writing a five-volume treatise on the history
of determinants].
For many years Wronski&rsquo;s work was dismissed as rubbish. However, a closer examination
of the work in more recent times shows that although some is wrong and he has an in-
credibly high opinion of himself and his ideas, there are also some mathematical insights
of great depth and brilliance hidden within the papers.
</p>
<p>14.4.1 A Second Solution to the HSOLDE
</p>
<p>If we know one solution to Eq. (14.12), say f1, then by differentiating both
sides of
</p>
<p>f1(x)f
&prime;
2(x)&minus; f2(x)f &prime;1(x)=W(x)=W(c)e&minus;
</p>
<p>&int; x
c p(t) dt ,
</p>
<p>dividing the result by f 21 , and noting that the LHS will be the derivative of
f2/f1, we can solve for f2 in terms of f1. The result is
</p>
<p>f2(x)= f1(x)
{
C +K
</p>
<p>&int; x
</p>
<p>α
</p>
<p>1
</p>
<p>f 21 (s)
exp
</p>
<p>[
&minus;
&int; s
</p>
<p>c
</p>
<p>p(t) dt
</p>
<p>]
ds
</p>
<p>}
,
</p>
<p>where K &equiv; W(c) is another arbitrary (nonzero) constant; we do not have
to know W(x) (this would require knowledge of f2, which we are trying
to calculate!) to obtain W(c). In fact, the reader is urged to check directly
that f2(x) satisfies the DE of (14.12) for arbitrary C and K . Whenever
possible&mdash;and convenient&mdash;it is customary to set C = 0, because its pres-
ence simply gives a term that is proportional to the known solution f1(x).
</p>
<p>Theorem 14.4.4 Let f1 be a solution of y&prime;&prime; + p(x)y&prime; + q(x)y = 0.
Then
</p>
<p>f2(x)= f1(x)
&int; x
</p>
<p>α
</p>
<p>1
</p>
<p>f 21 (s)
exp
</p>
<p>[
&minus;
&int; s
</p>
<p>c
</p>
<p>p(t) dt
</p>
<p>]
ds,
</p>
<p>is another solution and {f1, f2} forms a basis of solutions of the DE.
</p>
<p>Example 14.4.5 Here are some examples of finding the second solution
from the first:</p>
<p/>
</div>
<div class="page"><p/>
<p>14.4 The Wronskian 427
</p>
<p>(a) A solution to the SOLDE y&prime;&prime; &minus; k2y = 0 is ekx . To find a second solu-
tion, we let C = 0 and K = 1 in Theorem 14.4.4. Since p(x)= 0, we
have
</p>
<p>f2(x)= ekx
(
</p>
<p>0 +
&int; x
</p>
<p>α
</p>
<p>ds
</p>
<p>e2ks
</p>
<p>)
=&minus; 1
</p>
<p>2k
e&minus;kx + e
</p>
<p>&minus;2kα
</p>
<p>2k
ekx,
</p>
<p>which, ignoring the second term (which is proportional to the first
solution), leads directly to the choice of e&minus;kx as a second solution.
</p>
<p>(b) The differential equation y&prime;&prime; + k2y = 0 has sin kx as a solution. With
C = 0, α = π/(2k), and K = 1, we get
</p>
<p>f2(x)= sinkx
(
</p>
<p>0+
&int; x
</p>
<p>π/2k
</p>
<p>ds
</p>
<p>sin2 ks
</p>
<p>)
=&minus; sin kx cot ks|xπ/2k =&minus; coskx.
</p>
<p>(c) For the solutions in part (a),
</p>
<p>W(x)= det
(
ekx kekx
</p>
<p>e&minus;kx &minus;ke&minus;kx
)
=&minus;2k,
</p>
<p>and for those in part (b),
</p>
<p>W(x)= det
(
</p>
<p>sin kx k coskx
coskx &minus;k sin kx
</p>
<p>)
=&minus;k.
</p>
<p>Both Wronskians are constant. In general, the Wronskian of any two
linearly independent solutions of y&prime;&prime; + q(x)y = 0 is constant.
</p>
<p>Most special functions used in mathematical physics are solutions of
SOLDEs. The behavior of these functions at certain special points is deter-
mined by the physics of the particular problem. In most situations physical
expectation leads to a preference for one particular solution over the other.
For example, although there are two linearly independent solutions to the
Legendre DE
</p>
<p>d
</p>
<p>dx
</p>
<p>[(
1 &minus; x2
</p>
<p>)dy
dx
</p>
<p>]
+ n(n+ 1)y = 0,
</p>
<p>the solution that is most frequently encountered is the Legendre polynomial
Pn(x) discussed in Chap. 8. The other solution can be obtained by solving
the Legendre equation or by using Theorem 14.4.4, as done in the following
example.
</p>
<p>Example 14.4.6 The Legendre equation can be reexpressed as
</p>
<p>d2y
</p>
<p>dx2
&minus; 2x
</p>
<p>1 &minus; x2
dy
</p>
<p>dx
+ n(n+ 1)
</p>
<p>1 &minus; x2 y = 0.
</p>
<p>This is an HSOLDE with
</p>
<p>p(x)=&minus; 2x
1 &minus; x2 and q(x)=
</p>
<p>n(n+ 1)
1 &minus; x2 .</p>
<p/>
</div>
<div class="page"><p/>
<p>428 14 Second-Order Linear Differential Equations
</p>
<p>One solution of this HSOLDE is the well-known Legendre polynomial
Pn(x). Using this as our input and employing Theorem 14.4.4, we can gen-
erate another set of solutions.
</p>
<p>Let Qn(x) stand for the linearly independent &ldquo;partner&rdquo; of Pn(x). Then
</p>
<p>Qn(x)= Pn(x)
&int; x
</p>
<p>α
</p>
<p>1
</p>
<p>P 2n (s)
exp
</p>
<p>[&int; s
</p>
<p>0
</p>
<p>2t
</p>
<p>1 &minus; t2 dt
]
ds
</p>
<p>= Pn(x)
&int; x
</p>
<p>α
</p>
<p>1
</p>
<p>P 2n (s)
</p>
<p>[
1
</p>
<p>1 &minus; s2
]
ds =AnPn(x)
</p>
<p>&int; x
</p>
<p>α
</p>
<p>ds
</p>
<p>(1 &minus; s2)P 2n (s)
,
</p>
<p>where An is an arbitrary constant determined by standardization, and α is
an arbitrary point in the interval [&minus;1,+1]. For instance, for n= 0, we have
P0 = 1, and we obtain
</p>
<p>Q0(x)=A0
&int; x
</p>
<p>α
</p>
<p>ds
</p>
<p>1 &minus; s2 =A0
[
</p>
<p>1
</p>
<p>2
ln
</p>
<p>∣∣∣∣
1 + x
1 &minus; x
</p>
<p>∣∣∣∣&minus;
1
</p>
<p>2
ln
</p>
<p>∣∣∣∣
1 + α
1 &minus; α
</p>
<p>∣∣∣∣
]
.
</p>
<p>The standard form of Q0(x) is obtained by setting A0 = 1 and α = 0:
</p>
<p>Q0(x)=
1
</p>
<p>2
ln
</p>
<p>∣∣∣∣
1 + x
1 &minus; x
</p>
<p>∣∣∣∣ for |x|&lt; 1.
</p>
<p>Similarly, since P1(x)= x,
</p>
<p>Q1(x)=A1x
&int; x
</p>
<p>α
</p>
<p>ds
</p>
<p>s2(1 &minus; s2) =Ax +Bx ln
∣∣∣∣
1 + x
1 &minus; x
</p>
<p>∣∣∣∣+C for |x|&lt; 1.
</p>
<p>Here standardization is A= 0, B = 12 , and C =&minus;1. Thus,
</p>
<p>Q1(x)=
1
</p>
<p>2
x ln
</p>
<p>∣∣∣∣
1 + x
1 &minus; x
</p>
<p>∣∣∣∣&minus; 1.
</p>
<p>14.4.2 The General Solution to an ISOLDE
</p>
<p>Inhomogeneous SOLDEs (ISOLDEs) can be most elegantly discussed in
terms of Green&rsquo;s functions, the subject of Chap. 20, which automatically
incorporate the boundary conditions. However, the most general solution of
an ISOLDE, with no boundary specification, can be discussed at this point.
</p>
<p>Let g(x) be a particular solution of
</p>
<p>L[y] = y&prime;&prime; + py&prime; + qy = r(x) (14.13)
</p>
<p>and let h(x) be any other solution of this equation. Then h(x)&minus;g(x) satisfies
Eq. (14.12) and can be written as a linear combination of a basis of solutions
f1(x) and f2(x), leading to the following equation:
</p>
<p>h(x)= c1f1(x)+ c2f2(x)+ g(x). (14.14)</p>
<p/>
</div>
<div class="page"><p/>
<p>14.4 The Wronskian 429
</p>
<p>Thus, if we have a particular solution of the ISOLDE of Eq. (14.13) and two
basis solutions of the HSOLDE, then the most general solution of (14.13)
can be expressed as the sum of a linear combination of the two basis solu-
tions and the particular solution.
</p>
<p>We know how to find a second solution to the HSOLDE once we know
one solution. We now show that knowing one such solution will also allow
us to find a particular solution to the ISOLDE. The method we use is called
the variation of constants. This method can also be used to find a second method of variation of
</p>
<p>constantssolution to the HSOLDE.
Let f1 and f2 be the two (known) solutions of the HSOLDE and g(x)
</p>
<p>the sought-after solution to Eq. (14.13). Write g as g(x) = f1(x)v(x) and
substitute it in (14.13) to get a SOLDE for v(x):
</p>
<p>v&prime;&prime; +
(
p+ 2f
</p>
<p>&prime;
1
</p>
<p>f1
</p>
<p>)
v&prime; = r
</p>
<p>f1
.
</p>
<p>This is a first order linear DE in v&prime;, which has a solution of the form
</p>
<p>v&prime; = W(x)
f 21 (x)
</p>
<p>[
C +
</p>
<p>&int; x
</p>
<p>a
</p>
<p>f1(t)r(t)
</p>
<p>W(t)
dt
</p>
<p>]
,
</p>
<p>where W(x) is the (known) Wronskian of Eq. (14.13). Substituting
</p>
<p>W(x)
</p>
<p>f 21 (x)
= f1(x)f
</p>
<p>&prime;
2(x)&minus; f2(x)f &prime;1(x)
</p>
<p>f 21 (x)
= d
</p>
<p>dx
</p>
<p>(
f2
</p>
<p>f1
</p>
<p>)
</p>
<p>in the above expression for v&prime; and setting C = 0 (we are interested in a
particular solution), we get
</p>
<p>dv
</p>
<p>dx
= d
</p>
<p>dx
</p>
<p>(
f2
</p>
<p>f1
</p>
<p>)&int; x
</p>
<p>a
</p>
<p>f1(t)r(t)
</p>
<p>W(t)
dt
</p>
<p>= d
dx
</p>
<p>[
f2(x)
</p>
<p>f1(x)
</p>
<p>&int; x
</p>
<p>a
</p>
<p>f1(t)r(t)
</p>
<p>W(t)
dt
</p>
<p>]
&minus; f2(x)
</p>
<p>f1(x)
</p>
<p>d
</p>
<p>dx
</p>
<p>&int; x
</p>
<p>a
</p>
<p>f1(t)r(t)
</p>
<p>W(t)
dt
</p>
<p>︸ ︷︷ ︸
=f1(x)r(x)/W(x)
</p>
<p>and
</p>
<p>v(x)= f2(x)
f1(x)
</p>
<p>&int; x
</p>
<p>a
</p>
<p>f1(t)r(t)
</p>
<p>W(t)
dt &minus;
</p>
<p>&int; x
</p>
<p>a
</p>
<p>f2(t)r(t)
</p>
<p>W(t)
dt.
</p>
<p>This leads to the particular solution
</p>
<p>g(x)= f1(x)v(x)= f2(x)
&int; x
</p>
<p>a
</p>
<p>f1(t)r(t)
</p>
<p>W(t)
dt &minus; f1(x)
</p>
<p>&int; x
</p>
<p>a
</p>
<p>f2(t)r(t)
</p>
<p>W(t)
dt.
</p>
<p>(14.15)
We just proved the following result:</p>
<p/>
</div>
<div class="page"><p/>
<p>430 14 Second-Order Linear Differential Equations
</p>
<p>Fig. 14.1 If f &prime;2(x1) &gt; 0 &gt; f
&prime;
2(x2), then (assuming that the Wronskian is positive)
</p>
<p>f1(x1) &gt; 0 &gt; f1(x2)
</p>
<p>Proposition 14.4.7 Given a single solution f1(x) of the HSOLDE
corresponding to an ISOLDE, one can use Theorem 14.4.4 to find
a second solution f2(x) of the HSOLDE and Eq. (14.15) to find a
particular solution g(x)of the ISOLDE. The most general solution h
will then be
</p>
<p>h(x)= c1f1(x)+ c2f2(x)+ g(x).
</p>
<p>14.4.3 Separation and Comparison Theorems
</p>
<p>The Wronskian can be used to derive some properties of the graphs of solu-
tions of HSOLDEs. One such property concerns the relative position of the
zeros of two linearly independent solutions of an HSOLDE.
</p>
<p>Theorem 14.4.8 (Separation) The zeros of two linearly independent solu-the separation theorem
</p>
<p>tions of an HSOLDE occur alternately.
</p>
<p>Proof Let f1(x) and f2(x) be two independent solutions of Eq. (14.12). We
have to show that a zero of f1 exists between any two zeros of f2. The linear
independence of f1 and f2 implies that W(f1, f2;x) �= 0 for any x &isin; [a, b].
Let xi &isin; [a, b] be a zero of f2. Then
</p>
<p>0 �=W(f1, f2;xi)= f1(xi)f &prime;2(xi)&minus; f2(xi)f &prime;1(xi)= f1(xi)f &prime;2(xi).
</p>
<p>Thus, f1(xi) �= 0 and f &prime;2(xi) �= 0. Suppose that x1 and x2&mdash;where x2 &gt; x1&mdash;
are two successive zeros of f2. Since f2 is continuous in [a, b] and f &prime;2(x1) �=
0, f2 has to be either increasing [f &prime;2(x1) &gt; 0] or decreasing [f
</p>
<p>&prime;
2(x1) &lt; 0]
</p>
<p>at x1. For f2 to be zero at x2, the next point, f &prime;2(x2) must have the oppo-
site sign from f &prime;2(x1) (see Fig. 14.1). We proved earlier that the sign of the
Wronskian does not change in [a, b] (see Proposition 14.4.2 and comments
after it). The above equation then says that f1(x1) and f1(x2) also have op-
posite signs. The continuity of f1 then implies that f1 must cross the x-axis</p>
<p/>
</div>
<div class="page"><p/>
<p>14.4 The Wronskian 431
</p>
<p>somewhere between x1 and x2. A similar argument shows that there exists
one zero of f2 between any two zeros of f1. �
</p>
<p>Example 14.4.9 Two linearly independent solutions of y&prime;&prime;+y = 0 are sinx
and cosx. The separation theorem suggests that the zeros of sinx and cosx
must alternate, a fact known from elementary trigonometry: The zeros of
cosx occur at odd multiples of π/2, and those of sinx occur at even multi-
ples of π/2.
</p>
<p>A second useful result is known as the comparison theorem (for a proof,
see [Birk 78, p. 38]).
</p>
<p>Theorem 14.4.10 (Comparison) Let f and g be nontrivial solutions of the comparison theorem
u&prime;&prime; + p(x)u = 0 and v&prime;&prime; + q(x)v = 0, respectively, where p(x) &ge; q(x) for
all x &isin; [a, b]. Then f vanishes at least once between any two zeros of g,
unless p = q and f is a constant multiple of g.
</p>
<p>The form of the differential equations used in the comparison theorem is
not restrictive because any HSOLDE can be cast in this form, as the follow-
ing proposition shows.
</p>
<p>Proposition 14.4.11 If y&prime;&prime; + p(x)y&prime; + q(x)y = 0, then
</p>
<p>u= y exp
[
</p>
<p>1
</p>
<p>2
</p>
<p>&int; x
</p>
<p>α
</p>
<p>p(t) dt
</p>
<p>]
</p>
<p>satisfies u&prime;&prime; + S(x)u= 0, where S(x)= q &minus; 14p2 &minus; 12p&prime;.
</p>
<p>Proof Define w(x) by y =wu, and substitute in the HSOLDE to obtain
(
u&prime;w+w&prime;u
</p>
<p>)&prime; + p
(
u&prime;w+w&prime;u
</p>
<p>)
+ quw = 0,
</p>
<p>or
</p>
<p>wu&prime;&prime; +
(
2w&prime; + pw
</p>
<p>)
u&prime; +
</p>
<p>(
qw+ pw&prime; +w&prime;&prime;
</p>
<p>)
u= 0. (14.16)
</p>
<p>If we demand that the coefficient of u&prime; be zero, we obtain the DE 2w&prime; +
pw = 0, whose solution is
</p>
<p>w(x)= C exp
[
&minus;1
</p>
<p>2
</p>
<p>&int; x
</p>
<p>α
</p>
<p>p(t) dt
</p>
<p>]
.
</p>
<p>Dividing (14.16) by this w and substituting for w yields
</p>
<p>u&prime;&prime; + S(x)u= 0, where S(x)= q + pw
&prime;
</p>
<p>w
+ w
</p>
<p>&prime;&prime;
</p>
<p>w
= q &minus; 1
</p>
<p>4
p2 &minus; 1
</p>
<p>2
p&prime;.
</p>
<p>�
</p>
<p>A useful special case of the comparison theorem is given as the following
corollary whose straightforward but instructive proof is left as a problem.</p>
<p/>
</div>
<div class="page"><p/>
<p>432 14 Second-Order Linear Differential Equations
</p>
<p>Corollary 14.4.12 If q(x)&le; 0 for all x &isin; [a, b], then no nontrivial solution
of the differential equation v&prime;&prime; + q(x)v = 0 can have more than one zero.
</p>
<p>Example 14.4.13 It should be clear from the preceding discussion that the
oscillations of the solutions of v&prime;&prime; + q(x)v = 0 are mostly determined by
the sign and magnitude of q(x). For q(x) &le; 0 there is no oscillation; that
is, there is no solution that changes sign more than once. Now suppose that
q(x) &ge; k2 &gt; 0 for some real k. Then, by Theorem 14.4.10, any solution of
v&prime;&prime; + q(x)v = 0 must have at least one zero between any two successive
zeros of the solution sin kx of u&prime;&prime; + k2u = 0. This means that any solution
of v&prime;&prime;+ q(x)v = 0 has a zero in any interval of length π/k if q(x)&ge; k2 &gt; 0.
</p>
<p>Let us apply this to the Bessel DE,
</p>
<p>y&prime;&prime; + 1
x
y&prime; +
</p>
<p>(
1 &minus; n
</p>
<p>2
</p>
<p>x2
</p>
<p>)
y = 0.
</p>
<p>By Proposition 14.4.11, we can eliminate the y&prime; term by substituting v/
&radic;
x
</p>
<p>for y.5 This transforms the Bessel DE into
</p>
<p>v&prime;&prime; +
(
</p>
<p>1 &minus; 4n
2 &minus; 1
4x2
</p>
<p>)
v = 0.
</p>
<p>We compare this, for n= 0, with u&prime;&prime;+u= 0, which has a solution u= sinx,
and conclude that each interval of length π of the positive x-axis contains atoscillation of the Bessel
</p>
<p>function of order zero least one zero of any solution of order zero (n= 0) of the Bessel equation.
Thus, in particular, the zeroth Bessel function, denoted by J0(x), has a zero
in each interval of length π of the x-axis.
</p>
<p>On the other hand, for 4n2 &minus; 1 &gt; 0, or n &gt; 12 , we have 1 &gt; [1 &minus; (4n2 &minus;
1)/4x2]. This implies that sinx has at least one zero between any two suc-
cessive zeros of the Bessel functions of order greater than 12 . It follows that
such a Bessel function can have at most one zero between any two succes-
sive zeros of sinx (or in each interval of length π on the positive x-axis).
</p>
<p>Example 14.4.14 Let us apply Corollary 14.4.12 to v&prime;&prime; &minus; v = 0 in which
q(x) = &minus;1 &lt; 0. According to the corollary, the most general solution,
c1e
</p>
<p>x + c2e&minus;x , can have at most one zero. Indeed,
</p>
<p>c1e
x + c2e&minus;x = 0 &rArr; x =
</p>
<p>1
</p>
<p>2
ln
</p>
<p>∣∣∣∣&minus;
c2
</p>
<p>c1
</p>
<p>∣∣∣∣,
</p>
<p>and this (real) x (if it exists) is the only possible solution, as predicted by
the corollary.
</p>
<p>5Because of the square root in the denominator, the range of x will have to be restricted
to positive values.</p>
<p/>
</div>
<div class="page"><p/>
<p>14.5 Adjoint Differential Operators 433
</p>
<p>14.5 Adjoint Differential Operators
</p>
<p>We discussed adjoint operators in detail in the context of finite-dimensional
vector spaces in Chap. 4. In particular, the importance of self-adjoint, or
hermitian, operators was clearly spelled out by the spectral decomposition
theorem of Chap. 6. A consequence of that theorem is the completeness of
the eigenvectors of a hermitian operator, the fact that an arbitrary vector can
be expressed as a linear combination of the (orthonormal) eigenvectors of a
hermitian operator.
</p>
<p>Self-adjoint differential operators are equally important because their
&ldquo;eigenfunctions&rdquo; also form complete orthogonal sets, as we shall see later.
This section generalizes the concept of the adjoint to the case of a differen-
tial operator (of second degree).
</p>
<p>Definition 14.5.1 The HSOLDE
</p>
<p>L[y] &equiv; p2(x)y&prime;&prime; + p1(x)y&prime; + p0(x)y = 0 (14.17)
</p>
<p>is said to be exact if exact HSOLDE
</p>
<p>L[f ] &equiv; p2(x)f &prime;&prime; + p1(x)f &prime; + p0(x)f =
d
</p>
<p>dx
</p>
<p>[
A(x)f &prime; +B(x)f
</p>
<p>]
(14.18)
</p>
<p>for all f &isin; C2[a, b] and for some A,B &isin; C1[a, b]. An integrating factor integrating factor for
HSOLDEfor L[y] is a function μ(x) such that μ(x)L[y] is exact.
</p>
<p>If the HSOLDE (14.17) is exact, then Eq. (14.18) gives
</p>
<p>d
</p>
<p>dx
</p>
<p>[
A(x)y&prime; +B(x)y
</p>
<p>]
= 0 &rArr; A(x)y&prime; +B(x)y = C,
</p>
<p>a FOLDE with a constant inhomogeneous term.
If (14.17) has an integrating factor, then even the ISOLDE corresponding
</p>
<p>to it can be solved, because
</p>
<p>μ(x)L[y] = μ(x)r(x) &rArr; d
dx
</p>
<p>[
A(x)y&prime; +B(x)y
</p>
<p>]
= μ(x)r(x)
</p>
<p>&rArr; A(x)y&prime; +B(x)y =
&int; x
</p>
<p>α
</p>
<p>μ(t)r(t) dt,
</p>
<p>which is a general FOLDE. Thus, the existence of an integrating factor com-
pletely solves a SOLDE. It is therefore important to know whether or not a
SOLDE admits an integrating factor. First let us give a criterion for the ex-
actness of a SOLDE.
</p>
<p>Proposition 14.5.2 The HSOLDE of Eq. (14.17) is exact if and only if p&prime;&prime;2 &minus;
p&prime;1 + p0 = 0.
</p>
<p>Proof If the HSOLDE is exact, then Eq. (14.18) holds for all f , implying
that p2 = A, p1 = A&prime; + B , and p0 = B &prime;. It follows that p&prime;&prime;2 = A&prime;&prime;, p&prime;1 =
A&prime;&prime; +B &prime;, and p0 = B &prime;, which in turn give p&prime;&prime;2 &minus; p&prime;1 + p0 = 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>434 14 Second-Order Linear Differential Equations
</p>
<p>Conversely if p&prime;&prime;2 &minus;p&prime;1 +p0 = 0, then, substituting p0 =&minus;p&prime;&prime;2 +p&prime;1 in the
LHS of Eq. (14.17), we obtain
</p>
<p>p2y
&prime;&prime; + p1y&prime; + p0y = p2y&prime;&prime; + p1y&prime; +
</p>
<p>(
&minus;p&prime;&prime;2 + p&prime;1
</p>
<p>)
y
</p>
<p>= p2y&prime;&prime; &minus; p&prime;&prime;2y + (p1y)&prime; =
(
p2y
</p>
<p>&prime; &minus; p&prime;2y
)&prime; + (p1y)&prime;
</p>
<p>= d
dx
</p>
<p>(
p2y
</p>
<p>&prime; &minus; p&prime;2y + p1y
)
,
</p>
<p>and the DE is exact. �
</p>
<p>A general HSOLDE is clearly not exact. Can we make it exact by mul-
tiplying it by an integrating factor? The following proposition contains the
answer.
</p>
<p>Proposition 14.5.3 A function μ is an integrating factor of the HSOLDE of
Eq. (14.17) if and only if it is a solution of the HSOLDE
</p>
<p>M[μ] &equiv; (p2μ)&prime;&prime; &minus; (p1μ)&prime; + p0μ= 0. (14.19)
</p>
<p>Proof This is an immediate consequence of Proposition 14.5.2. �
</p>
<p>We can expand Eq. (14.19) to obtain the equivalent equation
</p>
<p>p2μ
&prime;&prime; +
</p>
<p>(
2p&prime;2 &minus; p1
</p>
<p>)
μ&prime; +
</p>
<p>(
p&prime;&prime;2 &minus; p&prime;1 + p0
</p>
<p>)
μ= 0. (14.20)
</p>
<p>The operator M given byadjoint of a
second-order linear
</p>
<p>differential operator
M&equiv; p2
</p>
<p>d2
</p>
<p>dx2
+
(
2p&prime;2 &minus; p1
</p>
<p>) d
dx
</p>
<p>+
(
p&prime;&prime;2 &minus; p&prime;1 + p0
</p>
<p>)
(14.21)
</p>
<p>is called the adjoint of the operator L and denoted by M&equiv; L&dagger;. The reason
for the use of the word &ldquo;adjoint&rdquo; will be made clear below.
</p>
<p>Proposition 14.5.3 confirms the existence of an integrating factor. How-
ever, the latter can be obtained only by solving Eq. (14.20), which is at least
as difficult as solving the original differential equation! In contrast, the in-
tegrating factor for a FOLDE can be obtained by a mere integration [μ(x)
given in Eq. (14.8) is an integrating factor of the FOLDE (14.6), as the reader
can verify].
</p>
<p>Although integrating factors for SOLDEs are not as useful as their coun-
terparts for FOLDEs, they can facilitate the study of SOLDEs. Let us first
note that the adjoint of the adjoint of a differential operator is the original
operator: (L&dagger;)&dagger; = L (see Problem 14.13). This suggests that if v is an inte-
grating factor of L[u], then u will be an integrating factor of M[v] &equiv; L&dagger;[v].
In particular, multiplying the first one by v and the second one by u
and subtracting the results, we obtain [see Equations (14.17) and (14.19)]
vL[u] &minus; uM[v] = (vp2)u&prime;&prime; &minus; u(p2v)&prime;&prime; + (vp1)u&prime; + u(p1v)&prime;, which can be
simplified to
</p>
<p>vL[u] &minus; uM[v] = d
dx
</p>
<p>[
p2vu
</p>
<p>&prime; &minus; (p2v)&prime;u+ p1uv
]
. (14.22)</p>
<p/>
</div>
<div class="page"><p/>
<p>14.5 Adjoint Differential Operators 435
</p>
<p>Integrating this from a to b yields
</p>
<p>&int; b
</p>
<p>a
</p>
<p>(
vL[u] &minus; uM[v]
</p>
<p>)
dx =
</p>
<p>[
p2vu
</p>
<p>&prime; &minus; (p2v)&prime;u+ p1uv
]∣∣b
a
. (14.23)
</p>
<p>Equations (14.22) and (14.23) are called the Lagrange identities. Equation Lagrange identities
(14.23) embodies the reason for calling M the adjoint of L: If we consider
u and v as abstract vectors |u〉 and |v〉, L and M as operators in a Hilbert
space with the inner product 〈u|v〉 =
</p>
<p>&int; b
a
u&lowast;(x)v(x) dx, then Eq. (14.23) can
</p>
<p>be written as
</p>
<p>〈v|L|u〉 &minus; 〈u|M|v〉 = 〈u|L&dagger;|v〉&lowast; &minus; 〈u|M|v〉 =
[
p2vu
</p>
<p>&prime; &minus; (p2v)&prime;u+ p1uv
]∣∣b
a
.
</p>
<p>If the RHS is zero, then 〈u|L&dagger;|v〉&lowast; = 〈u|M|v〉 for all |u〉, |v〉, and since all
these operators and functions are real, L&dagger; =M.
</p>
<p>As in the case of finite-dimensional vector spaces, a self-adjoint differ-
ential operator merits special consideration. For M[v] &equiv; L&dagger;[v] to be equal
to L, we must have [see Eqs. (14.17) and (14.20)] 2p&prime;2 &minus; p1 = p1 and
p&prime;&prime;2 &minus; p&prime;1 + p0 = p0. The first equation gives p&prime;2 = p1, which also solves
the second equation. If this condition holds, then we can write Eq. (14.17)
as L[y] = p2y&prime;&prime; + p&prime;2y&prime; + p0y, or
</p>
<p>L[y] = d
dx
</p>
<p>[
p2(x)
</p>
<p>dy
</p>
<p>dx
</p>
<p>]
+ p0(x)y = 0.
</p>
<p>Can we make all SOLDEs self-adjoint? Let us multiply both sides of
Eq. (14.17) by a function h(x), to be determined later. We get the new DE
</p>
<p>h(x)p2(x)y
&prime;&prime; + h(x)p1(x)y&prime; + h(x)p0(x)y = 0,
</p>
<p>which we desire to be self-adjoint. This will be accomplished if we choose
h(x) such that hp1 = (hp2)&prime;, or p2h&prime;+h(p&prime;2&minus;p1)= 0, which can be readily
integrated to give
</p>
<p>h(x)= 1
p2
</p>
<p>exp
</p>
<p>[&int; x p1(t)
p2(t)
</p>
<p>dt
</p>
<p>]
.
</p>
<p>We have just proved the following:
</p>
<p>Theorem 14.5.4 The SOLDE of Eq. (14.17) is self-adjoint if and only if all SOLDEs can be made
self-adjointp&prime;2 = p1, in which case the DE has the form
</p>
<p>d
</p>
<p>dx
</p>
<p>[
p2(x)
</p>
<p>dy
</p>
<p>dx
</p>
<p>]
+ p0(x)y = 0.
</p>
<p>If it is not self-adjoint, it can be made so by multiplying it through by
</p>
<p>h(x)= 1
p2
</p>
<p>exp
</p>
<p>[&int; x p1(t)
p2(t)
</p>
<p>dt
</p>
<p>]
.</p>
<p/>
</div>
<div class="page"><p/>
<p>436 14 Second-Order Linear Differential Equations
</p>
<p>Example 14.5.5 (a) The Legendre equation in normal form,
</p>
<p>y&prime;&prime; &minus; 2x
1 &minus; x2 y
</p>
<p>&prime; + λ
1 &minus; x2 y = 0,
</p>
<p>is not self-adjoint. However, if we multiply through by h(x) = 1 &minus; x2, we
get
</p>
<p>(
1 &minus; x2
</p>
<p>)
y&prime;&prime; &minus; 2xy&prime; + λy = 0,
</p>
<p>which can be rewritten as [(1 &minus; x2)y&prime;]&prime; + λy = 0, which is self-adjoint.
(b) Similarly, the normal form of the Bessel equation
</p>
<p>y&prime;&prime; + 1
x
y&prime; +
</p>
<p>(
1 &minus; n
</p>
<p>2
</p>
<p>x2
</p>
<p>)
y = 0
</p>
<p>is not self-adjoint, but multiplying through by h(x)= x yields
</p>
<p>d
</p>
<p>dx
</p>
<p>(
x
dy
</p>
<p>dx
</p>
<p>)
+
(
x &minus; n
</p>
<p>2
</p>
<p>x
</p>
<p>)
y = 0,
</p>
<p>which is clearly self-adjoint.
</p>
<p>14.6 Power-Series Solutions of SOLDEs
</p>
<p>Analysis is one of the richest branches of mathematics, focusing on the end-
less variety of objects we call functions. The simplest kind of function is
a polynomial, which is obtained by performing the simple algebraic opera-
tions of addition and multiplication on the independent variable x. The next
in complexity are the trigonometric functions, which are obtained by taking
ratios of geometric objects. If we demand a simplistic, intuitive approach
to functions, the list ends there. It was only with the advent of derivatives,
integrals, and differential equations that a vastly rich variety of functions
exploded into existence in the eighteenth and nineteenth centuries. For in-
stance, ex , nonexistent before the invention of calculus, can be thought of as
the function that solves dy/dx = y.
</p>
<p>Although the definition of a function in terms of DEs and integrals seems
a bit artificial, for most applications it is the only way to define a function.
For instance, the error function, used in statistics, is defined as
</p>
<p>erf(x)&equiv; 1&radic;
π
</p>
<p>&int; x
</p>
<p>&minus;&infin;
e&minus;t
</p>
<p>2
dt.
</p>
<p>Such a function cannot be expressed in terms of elementary functions. Sim-
ilarly, functions (of x) such as
</p>
<p>&int; &infin;
</p>
<p>x
</p>
<p>sin t
</p>
<p>t
dt,
</p>
<p>&int; π/2
</p>
<p>0
</p>
<p>&radic;
1 &minus; x2 sin2 t dt,
</p>
<p>&int; π/2
</p>
<p>0
</p>
<p>dt&radic;
1 &minus; x2 sin2 t
</p>
<p>,
</p>
<p>and so on are encountered frequently in applications. None of these func-
tions can be expressed in terms of other well-known functions.</p>
<p/>
</div>
<div class="page"><p/>
<p>14.6 Power-Series Solutions of SOLDEs 437
</p>
<p>An effective way of studying such functions is to study the differen-
tial equations they satisfy. In fact, the majority of functions encountered
in mathematical physics obey the HSOLDE of Eq. (14.17) in which the
pi(x) are elementary functions, mostly ratios of polynomials (of degree at
most 2). Of course, to specify functions completely, appropriate boundary
conditions are necessary. For instance, the error function mentioned above
satisfies the HSOLDE y&prime;&prime;+2xy&prime; = 0 with the boundary conditions y(0)= 12
and y&prime;(0)= 1/&radic;π .
</p>
<p>The natural tendency to resist the idea of a function as a solution of a
SOLDE is mostly due to the abstract nature of differential equations. After
all, it is easier to imagine constructing functions by simple multiplications
or with simple geometric figures that have been around for centuries. The
following beautiful example (see [Birk 78, pp. 85&ndash;87]) should overcome
this resistance and convince the skeptic that differential equations contain
all the information about a function.
</p>
<p>Example 14.6.1 (Trigonometric functions as solutions of DEs) We can This example illustrates
that all information
</p>
<p>about sine and cosine is
</p>
<p>hidden in their
</p>
<p>differential equation.
</p>
<p>show that the solutions to y&prime;&prime; + y = 0 have all the properties we expect
of sinx and cosx. Let us denote the two linearly independent solutions of
this equation by C(x) and S(x). To specify these functions completely, we
set C(0)= S&prime;(0)= 1, and C&prime;(0)= S(0)= 0. We claim that this information
is enough to identify C(x) and S(x) as cosx and sinx, respectively.
</p>
<p>First, let us show that the solutions exist and are well-behaved func-
tions. With C(0) and C&prime;(0) given, the equation y&prime;&prime; + y = 0 can generate all
derivatives of C(x) at zero: C&prime;&prime;(0) = &minus;C(0) = &minus;1, C&prime;&prime;&prime;(0) = &minus;C&prime;(0) = 0,
C(4)(0)=&minus;C&prime;&prime;(0)=+1, and, in general,
</p>
<p>C(n)(0)=
{
</p>
<p>0 if n is odd,
</p>
<p>(&minus;1)k if n= 2k where k = 0,1,2, . . . .
</p>
<p>Thus, the Taylor expansion of C(x) is
</p>
<p>C(x)=
&infin;&sum;
</p>
<p>k=0
(&minus;1)k x
</p>
<p>2k
</p>
<p>(2k)! . (14.24)
</p>
<p>Similarly,
</p>
<p>S(x)=
&infin;&sum;
</p>
<p>k=0
(&minus;1)k x
</p>
<p>2k+1
</p>
<p>(2k + 1)! . (14.25)
</p>
<p>A simple ratio test on the series representation of C(x) yields
</p>
<p>lim
k&rarr;&infin;
</p>
<p>ak+1
ak
</p>
<p>= lim
k&rarr;&infin;
</p>
<p>(&minus;1)k+1x2(k+1)/(2k + 2)!
(&minus;1)kx2k/(2k)!
</p>
<p>= lim
k&rarr;&infin;
</p>
<p>&minus;x2
(2k + 2)(2k + 1) = 0,
</p>
<p>which shows that the series for C(x) converges for all values of x. Similarly,
the series for S(x) is also convergent. Thus, we are dealing with well-defined
finite-valued functions.</p>
<p/>
</div>
<div class="page"><p/>
<p>438 14 Second-Order Linear Differential Equations
</p>
<p>Let us now enumerate and prove some properties of C(x) and S(x).
</p>
<p>(a) C&prime;(x)=&minus;S(x).
We prove this relation by differentiating C&prime;&prime;(x) + C(x) = 0 and
</p>
<p>writing the result as [C&prime;(x)]&prime;&prime;+C&prime;(x)= 0 to make evident the fact that
C&prime;(x) is also a solution. Since C&prime;(0)= 0 and [C&prime;(0)]&prime; = C&prime;&prime;(0)=&minus;1,
and since &minus;S(x) satisfies the same initial conditions, the uniqueness
theorem implies that C&prime;(x)=&minus;S(x). Similarly, S&prime;(x)= C(x).
</p>
<p>(b) C2(x)+ S2(x)= 1.
Since the p(x) term is absent from the SOLDE, Proposition 14.4.2
</p>
<p>implies that the Wronskian of C(x) and S(x) is constant. On the other
hand,
</p>
<p>W(C,S;x)= C(x)S&prime;(x)&minus;C&prime;(x)S(x)= C2(x)+ S2(x)
</p>
<p>=W(C,S;0)= C2(0)+ S2(0)= 1.
</p>
<p>(c) S(a + x)= S(a)C(x)+C(a)S(x).
The use of the chain rule easily shows that S(a+ x) is a solution of
</p>
<p>the equation y&prime;&prime; + y = 0. Thus, it can be written as a linear combina-
tion of C(x) and S(x) [which are linearly independent because their
Wronskian is nonzero by (b)]:
</p>
<p>S(a + x)=AS(x)+BC(x). (14.26)
</p>
<p>This is a functional identity, which for x = 0 gives S(a)= BC(0)= B .
If we differentiate both sides of Eq. (14.26), we get
</p>
<p>C(a + x)=AS&prime;(x)+BC&prime;(x)=AC(x)&minus;BS(x),
</p>
<p>which for x = 0 gives C(a)= A. Substituting the values of A and B
in Eq. (14.26) yields the desired identity. A similar argument leads to
</p>
<p>C(a + x)= C(a)C(x)&minus; S(a)S(x).
</p>
<p>(d) Periodicity of C(x) and S(x).
Let x0 be the smallest positive real number such that S(x0) =
</p>
<p>C(x0). Then property (b) implies that C(x0)= S(x0)= 1/
&radic;
</p>
<p>2. On the
other hand,
</p>
<p>S(x0 + x)= S(x0)C(x)+C(x0)S(x)= C(x0)C(x)+ S(x0)S(x)
= C(x0)C(x)&minus; S(x0)S(&minus;x)= C(x0 &minus; x).
</p>
<p>The third equality follows because by Eq. (14.25), S(x) is an odd
function of x. This is true for all x; in particular, for x = x0 it yields
S(2x0)= C(0)= 1, and by property (b), C(2x0)= 0. Using property
(c) once more, we get
</p>
<p>S(2x0 + x)= S(2x0)C(x)+C(2x0)S(x)= C(x),
C(2x0 + x)= C(2x0)C(x)&minus; S(2x0)S(x)=&minus;S(x).</p>
<p/>
</div>
<div class="page"><p/>
<p>14.6 Power-Series Solutions of SOLDEs 439
</p>
<p>Substituting x = 2x0 yields S(4x0) = C(2x0) = 0 and C(4x0) =
&minus;S(2x0)=&minus;1. Continuing in this manner, we can easily obtain
</p>
<p>S(8x0 + x)= S(x), C(8x0 + x)= C(x),
</p>
<p>which prove the periodicity of S(x) and C(x) and show that their pe-
riod is 8x0. It is even possible to determine x0. This determination is
left as a problem, but the result is
</p>
<p>x0 =
&int; 1/&radic;2
</p>
<p>0
</p>
<p>dt&radic;
1 &minus; t2
</p>
<p>.
</p>
<p>A numerical calculation will show that this is π/4.
</p>
<p>14.6.1 Frobenius Method of Undetermined Coefficients
</p>
<p>A proper treatment of SOLDEs requires the medium of complex analysis
and will be undertaken in the next chapter. At this point, however, we are
seeking a formal infinite series solution to the SOLDE
</p>
<p>y&prime;&prime; + p(x)y&prime; + q(x)y = 0,
</p>
<p>where p(x) and q(x) are real and analytic. This means that p(x) and q(x)
can be represented by convergent power series in some interval (a, b). [The
interesting case where p(x) and q(x) may have singularities will be treated
in the context of complex solutions.]
</p>
<p>The general procedure is to write the expansions6
</p>
<p>p(x)=
&infin;&sum;
</p>
<p>k=0
akx
</p>
<p>k, q(x)=
&infin;&sum;
</p>
<p>k=0
bkx
</p>
<p>k, y =
&infin;&sum;
</p>
<p>k=0
ckx
</p>
<p>k (14.27)
</p>
<p>for the coefficient functions p and q and the solution y, substitute them in
the SOLDE, and equate the coefficient of each power of x to zero. For this
purpose, we need expansions for derivatives of y:
</p>
<p>y&prime; =
&infin;&sum;
</p>
<p>k=1
kckx
</p>
<p>k&minus;1 =
&infin;&sum;
</p>
<p>k=0
(k + 1)ck+1xk,
</p>
<p>y&prime;&prime; =
&infin;&sum;
</p>
<p>k=1
(k + 1)kck+1xk&minus;1 =
</p>
<p>&infin;&sum;
</p>
<p>k=0
(k + 2)(k + 1)ck+2xk.
</p>
<p>6Here we are expanding about the origin. If such an expansion is impossible or inconve-
nient, one can expand about another point, say x0. One would then replace all powers of
x in all expressions below with powers of x &minus; x0. These expansions assume that p, q ,
and y have no singularity at x = 0. In general, this assumption is not valid, and a different
approach, in which the whole series is multiplied by a (not necessarily positive integer)
power of x, ought to be taken. Details are provided in Chap. 15.</p>
<p/>
</div>
<div class="page"><p/>
<p>440 14 Second-Order Linear Differential Equations
</p>
<p>Thus
</p>
<p>p(x)y&prime; =
&infin;&sum;
</p>
<p>k=0
</p>
<p>&infin;&sum;
</p>
<p>m=0
amx
</p>
<p>m(k + 1)ck+1xk =
&sum;
</p>
<p>k,m
</p>
<p>(k + 1)amck+1xk+m.
</p>
<p>Let k+m&equiv; n and sum over n. Then the other sum, say m, cannot exceed n.
Thus,
</p>
<p>p(x)y&prime; =
&infin;&sum;
</p>
<p>n=0
</p>
<p>n&sum;
</p>
<p>m=0
(n&minus;m+ 1)amcn&minus;m+1xn.
</p>
<p>Similarly, q(x)y = &sum;&infin;n=0
&sum;n
</p>
<p>m=0 bmcn&minus;mx
n. Substituting these sums and
</p>
<p>the series for y&prime;&prime; in the SOLDE, we obtain
</p>
<p>&infin;&sum;
</p>
<p>n=0
</p>
<p>{
(n+ 1)(n+ 2)cn+2 +
</p>
<p>n&sum;
</p>
<p>m=0
</p>
<p>[
(n&minus;m+ 1)amcn&minus;m+1 + bmcn&minus;m
</p>
<p>]
}
xn = 0.
</p>
<p>For this to be true for all x, the coefficient of each power of x must vanish:
</p>
<p>(n+1)(n+2)cn+2 =&minus;
n&sum;
</p>
<p>m=0
</p>
<p>[
(n&minus;m+1)amcn&minus;m+1 +bmcn&minus;m
</p>
<p>]
for n&ge; 0,
</p>
<p>or
</p>
<p>n(n+1)cn+1 =&minus;
n&minus;1&sum;
</p>
<p>m=0
</p>
<p>[
(n&minus;m)amcn&minus;m+bmcn&minus;m&minus;1
</p>
<p>]
for n&ge; 1. (14.28)
</p>
<p>If we know c0 and c1 (for instance from boundary conditions), we can
uniquely determine cn for n &ge; 2 from Eq. (14.28). This, in turn, gives a
unique power-series expansion for y, and we have the following theorem.
</p>
<p>Theorem 14.6.2 (Existence) For any SOLDE of the form y&prime;&prime; + p(x)y&prime; +existence theorem for
SOLDE q(x)y = 0 with analytic coefficient functions given by the first two equations
</p>
<p>of (14.27), there exists a unique power series, given by the third equation of
(14.27) that formally satisfies the SOLDE for each choice of c0 and c1.
</p>
<p>This theorem merely states the existence of a formal power series and
says nothing about its convergence. The following example will demonstrate
that convergence is not necessarily guaranteed.
</p>
<p>Example 14.6.3 The formal power-series solution for x2y&prime;&minus;y+x = 0 can
be obtained by letting y =&sum;&infin;n=0 cnxn. Then y&prime; =
</p>
<p>&sum;&infin;
n=0(n+ 1)cn+1xn, and
</p>
<p>substitution in the DE gives
</p>
<p>&infin;&sum;
</p>
<p>n=0
(n+ 1)cn+1xn+2 &minus;
</p>
<p>&infin;&sum;
</p>
<p>n=0
cnx
</p>
<p>n + x = 0,
</p>
<p>or
&infin;&sum;
</p>
<p>n=0
(n+ 1)cn+1xn+2 &minus; c0 &minus; c1x &minus;
</p>
<p>&infin;&sum;
</p>
<p>n=2
cnx
</p>
<p>n + x = 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>14.6 Power-Series Solutions of SOLDEs 441
</p>
<p>We see that c0 = 0, c1 = 1, and (n + 1)cn+1 = cn+2 for n &ge; 0. Thus, we
have the recursion relation ncn = cn+1 for n &ge; 1 whose unique solution is
cn = (n&minus; 1)!, which generates the following solution for the DE:
</p>
<p>y = x + x2 + (2!)x3 + (3!)x4 + &middot; &middot; &middot; + (n&minus; 1)!xn + &middot; &middot; &middot; .
</p>
<p>This series is not convergent for any nonzero x.
</p>
<p>The SOLDE solved in the preceding example is not normal. However,
for normal SOLDEs, the power series of y in Eq. (14.27) converges to an
analytic function, as the following theorem shows (for a proof, see [Birk 78,
p. 95]):
</p>
<p>Theorem 14.6.4 For any choice of c0 and c1, the radius of convergence of
any power series solution y =&sum;&infin;k=0 ckxk for the normal HSOLDE
</p>
<p>y&prime;&prime; + p(x)y&prime; + q(x)y = 0
</p>
<p>whose coefficients satisfy the recursion relation of (14.28) is at least as large
as the smaller of the two radii of convergence of the two series for p(x) and
q(x).
</p>
<p>In particular, if p(x) and q(x) are analytic in an interval around x = 0,
then the solution of the normal HSOLDE is also analytic in a neighborhood
of x = 0.
</p>
<p>Example 14.6.5 As an application of Theorem 14.6.2, let us consider the
Legendre equation in its normal form
</p>
<p>y&prime;&prime; &minus; 2x
1 &minus; x2 y
</p>
<p>&prime; + λ
1 &minus; x2 y = 0.
</p>
<p>For |x|&lt; 1 both p and q are analytic, and
</p>
<p>p(x)=&minus;2x
&infin;&sum;
</p>
<p>m=0
</p>
<p>(
x2
)m =
</p>
<p>&infin;&sum;
</p>
<p>m=0
(&minus;2)x2m+1,
</p>
<p>q(x)= λ
&infin;&sum;
</p>
<p>m=0
</p>
<p>(
x2
)m =
</p>
<p>&infin;&sum;
</p>
<p>m=0
λx2m.
</p>
<p>Thus, the coefficients of Eq. (14.27) are
</p>
<p>am =
{
</p>
<p>0 if m is even,
</p>
<p>&minus;2 if m is odd
and bm =
</p>
<p>{
λ if m is even,
</p>
<p>0 if m is odd.
</p>
<p>We want to substitute for am and bm in Eq. (14.28) to find cn+1. It is
convenient to consider two cases: when n is odd and when n is even. For
n= 2r + 1, Eq. (14.28)&mdash;after some algebra&mdash;yields
</p>
<p>(2r + 1)(2r + 2)c2r+2 =
r&sum;
</p>
<p>m=0
(4r &minus; 4m&minus; λ)c2(r&minus;m). (14.29)</p>
<p/>
</div>
<div class="page"><p/>
<p>442 14 Second-Order Linear Differential Equations
</p>
<p>With r &rarr; r + 1, this becomes
</p>
<p>(2r + 3)(2r + 4)c2r+4
</p>
<p>=
r+1&sum;
</p>
<p>m=0
(4r + 4 &minus; 4m&minus; λ)c2(r+1&minus;m)
</p>
<p>= (4r + 4 &minus; λ)c2(r+1) +
r+1&sum;
</p>
<p>m=1
(4r + 4 &minus; 4m&minus; λ)c2(r+1&minus;m)
</p>
<p>= (4r + 4 &minus; λ)c2r+2 +
r&sum;
</p>
<p>m=0
(4r &minus; 4m&minus; λ)c2(r&minus;m)
</p>
<p>= (4r + 4 &minus; λ)c2r+2 + (2r + 1)(2r + 2)c2r+2
=
[
&minus;λ+ (2r + 3)(2r + 2)
</p>
<p>]
c2r+2,
</p>
<p>where in going from the second equality to the third we changed the dummy
index, and in going from the third equality to the fourth we used Eq. (14.29).
Now we let 2r + 2 &equiv; k to obtain (k + 1)(k + 2)ck+2 = [k(k + 1)&minus; λ]ck , or
</p>
<p>ck+2 =
k(k + 1)&minus; λ
(k + 1)(k + 2)ck for even k.
</p>
<p>It is not difficult to show that starting with n = 2r , the case of even n, we
obtain this same equation for odd k. Thus, we can write
</p>
<p>cn+2 =
n(n+ 1)&minus; λ
(n+ 1)(n+ 2)cn. (14.30)
</p>
<p>For arbitrary c0 and c1, we obtain two independent solutions, one of
which has only even powers of x and the other only odd powers. The gen-
eralized ratio test (see [Hass 08, Chap. 5]) shows that the series is divergent
for x =&plusmn;1 unless λ= l(l + 1) for some positive integer l. In that case the
infinite series becomes a polynomial, the Legendre polynomial encountered
in Chap. 8.
</p>
<p>Equation (14.30) could have been obtained by substituting Eq. (14.27)
directly into the Legendre equation. The roundabout way to (14.30) taken
here shows the generality of Eq. (14.28). With specific differential equations
it is generally better to substitute (14.27) directly.
</p>
<p>Example 14.6.6 We studied Hermite polynomials in Chap. 8 in the contextquantum harmonic
oscillator: power series
</p>
<p>method
</p>
<p>of classical orthogonal polynomials. Let us see how they arise in physics.
The one-dimensional time-independent Schroedinger equation for a par-
</p>
<p>ticle of mass m in a potential V (x) is
</p>
<p>&minus; �
2
</p>
<p>2m
</p>
<p>d2ψ
</p>
<p>dx2
+ V (x)ψ =Eψ,
</p>
<p>where E is the total energy of the particle.</p>
<p/>
</div>
<div class="page"><p/>
<p>14.6 Power-Series Solutions of SOLDEs 443
</p>
<p>For a harmonic oscillator, V (x)= 12kx2 &equiv; 12mω2x2 and
</p>
<p>ψ &prime;&prime; &minus; m
2ω2
</p>
<p>�2
x2ψ + 2m
</p>
<p>�2
Eψ = 0.
</p>
<p>Substituting ψ(x)=H(x) exp(&minus;mωx2/2�) and then making the change of
variables x = (1/&radic;mω/�)y yields
</p>
<p>H &prime;&prime; &minus; 2yH &prime; + λH = 0 where λ= 2E
�ω
</p>
<p>&minus; 1. (14.31)
</p>
<p>This is the Hermite differential equation in normal form. We assume the
expansion H(y)=&sum;&infin;n=0 cnyn which yields
</p>
<p>H &prime;(y)=
&infin;&sum;
</p>
<p>n=1
ncny
</p>
<p>n&minus;1 =
&infin;&sum;
</p>
<p>n=0
(n+ 1)cn+1yn,
</p>
<p>H &prime;&prime;(y)=
&infin;&sum;
</p>
<p>n=1
n(n+ 1)cn+1yn&minus;1 =
</p>
<p>&infin;&sum;
</p>
<p>n=0
(n+ 1)(n+ 2)cn+2yn.
</p>
<p>Substituting in Eq. (14.31) gives
</p>
<p>&infin;&sum;
</p>
<p>n=0
</p>
<p>[
(n+ 1)(n+ 2)cn+2 + λcn
</p>
<p>]
yn &minus; 2
</p>
<p>&infin;&sum;
</p>
<p>n=0
(n+ 1)cn+1yn+1 = 0,
</p>
<p>or
</p>
<p>2c2 + λc0 +
&infin;&sum;
</p>
<p>n=0
</p>
<p>[
(n+ 2)(n+ 3)cn+3 + λcn+1 &minus; 2(n+ 1)cn+1
</p>
<p>]
yn+1 = 0.
</p>
<p>Setting the coefficients of powers of y equal to zero, we obtain
</p>
<p>c2 =&minus;
λ
</p>
<p>2
c0,
</p>
<p>cn+3 =
2(n+ 1)&minus; λ
(n+ 2)(n+ 3)cn+1 for n&ge; 0,
</p>
<p>or, replacing n with n&minus; 1,
</p>
<p>cn+2 =
2n&minus; λ
</p>
<p>(n+ 1)(n+ 2)cn, n&ge; 1. (14.32)
</p>
<p>The ratio test yields easily that the series is convergent for all values of y.
Thus, the infinite series whose coefficients obey the recursive rela-
</p>
<p>tion in Eq. (14.32) converges for all y. However, if we demand that
limx&rarr;&infin;ψ(x)= 0, which is necessary on physical grounds, the series must
be truncated. This happens only if λ = 2l for some integer l (see Prob-
lem 14.22 and [Hass 08, Chap. 13]), and in that case we obtain a polynomial,
the Hermite polynomial of order l. A consequence of such a truncation is
the quantization of harmonic oscillator energy:
</p>
<p>2l = λ= 2E
�ω
</p>
<p>&minus; 1 &rArr; E =
(
l + 1
</p>
<p>2
</p>
<p>)
�ω.</p>
<p/>
</div>
<div class="page"><p/>
<p>444 14 Second-Order Linear Differential Equations
</p>
<p>Two solutions are generated from Eq. (14.32), one including only even
powers and the other only odd powers. These are clearly linearly indepen-
dent. Thus, knowledge of c0 and c1 determines the general solution of the
HSOLDE of (14.31).
</p>
<p>14.6.2 QuantumHarmonic Oscillator
</p>
<p>The preceding two examples show how certain special functions used in
mathematical physics are obtained in an analytic way, by solving a differ-
ential equation. We saw in Chap. 13 how to obtain spherical harmonics and
Legendre polynomials by algebraic methods. It is instructive to solve the
harmonic oscillator problem using algebraic methods.
</p>
<p>The Hamiltonian of a one-dimensional harmonic oscillator is
</p>
<p>H= p
2
</p>
<p>2m
+ 1
</p>
<p>2
mω2x2,
</p>
<p>where p=&minus;i�d/dx is the momentum operator. Let us find the eigenvectors
and eigenvalues of H.
</p>
<p>We define the operators
</p>
<p>a&equiv;
&radic;
mω
</p>
<p>2�
x + i p&radic;
</p>
<p>2m�ω
and a&dagger; =
</p>
<p>&radic;
mω
</p>
<p>2�
x &minus; i p&radic;
</p>
<p>2m�ω
.
</p>
<p>Using the commutation relation [x,p] = i�1, we can show that
[
a,a&dagger;
</p>
<p>]
= 1 and H= �ωa&dagger;a+ 1
</p>
<p>2
�ω1. (14.33)
</p>
<p>Furthermore, one can readily show thatcreation and annihilation
operators
</p>
<p>[H,a] = &minus;�ωa,
[
H,a&dagger;
</p>
<p>]
= �ωa&dagger;. (14.34)
</p>
<p>Let |ψE〉 be the eigenvector corresponding to the eigenvalue E: H|ψE〉 =
E|ψE〉, and note that Eq. (14.34) gives
</p>
<p>Ha|ψE〉 = (aH&minus; �ωa)|ψE〉 = (E &minus; �ω)a|ψE〉
</p>
<p>and
</p>
<p>Ha&dagger;|ψE〉 = (E + �ω)a&dagger;|ψE〉.
Thus, a|ψE〉 is an eigenvector of H, with eigenvalue E&minus; �ω, and a&dagger;|ψE〉 is
an eigenvector with eigenvalue E+ �ω. That is why a&dagger; and a are called the
raising and lowering (or creation and annihilation) operators, respectively.
We can write
</p>
<p>a|ψE〉 = cE |ψE&minus;�ω〉.
By applying a repeatedly, we obtain states of lower and lower energies.
</p>
<p>But there is a limit to this because H is a positive operator: It cannot have a
negative eigenvalue. Thus, there must exist a ground state, |ψ0〉, such that</p>
<p/>
</div>
<div class="page"><p/>
<p>14.6 Power-Series Solutions of SOLDEs 445
</p>
<p>a|ψ0〉 = 0. The energy of this ground state (or the eigenvalue corresponding
to |ψ0〉) can be obtained:7
</p>
<p>H|ψ0〉 =
(
�ωa&dagger;a+ 1
</p>
<p>2
�ω
</p>
<p>)
|ψ0〉 =
</p>
<p>1
</p>
<p>2
�ω|ψ0〉.
</p>
<p>Repeated application of the raising operator yields both higher-level states
and eigenvalues. We thus define |ψn〉 by
</p>
<p>(
a&dagger;
)n|ψ0〉 = cn|ψn〉, (14.35)
</p>
<p>where cn is a normalizing constant. The energy of |ψn〉 is n units higher
than the ground state&rsquo;s, or
</p>
<p>En =
(
n+ 1
</p>
<p>2
</p>
<p>)
�ω,
</p>
<p>which is what we obtained in the preceding example.
To find cn, we demand orthonormality for the |ψn〉. Taking the inner
</p>
<p>product of (14.35) with itself, we can show (see Problem 14.23) that
</p>
<p>|cn|2 = n|cn&minus;1|2 &rArr; |cn|2 = n!|c0|2,
</p>
<p>which for |c0| = 1 and real cn yields cn =
&radic;
n!. It follows, then, that
</p>
<p>|ψn〉 =
1&radic;
n!
(
a&dagger;
)n|ψ0〉. (14.36)
</p>
<p>In terms of functions and derivative operators, a|ψ0〉 = 0 gives quantum harmonic
oscillator: connection
</p>
<p>between algebraic and
</p>
<p>analytic methods
</p>
<p>(&radic;
mω
</p>
<p>2�
x +
</p>
<p>&radic;
�
</p>
<p>2mω
</p>
<p>d
</p>
<p>dx
</p>
<p>)
ψ0(x)= 0
</p>
<p>with the solution ψ0(x)= c exp(&minus;mωx2/2�). Normalizing ψ0(x) gives
</p>
<p>1 = 〈ψ0|ψ0〉 = c2
&int; &infin;
</p>
<p>&minus;&infin;
exp
</p>
<p>(
&minus;mωx
</p>
<p>2
</p>
<p>�
</p>
<p>)
dx = c2
</p>
<p>(
�π
</p>
<p>mω
</p>
<p>)1/2
.
</p>
<p>Thus,
</p>
<p>ψ0(x)=
(
mω
</p>
<p>�π
</p>
<p>)1/4
e&minus;mωx
</p>
<p>2/(2�).
</p>
<p>We can now write Eq. (14.36) in terms of differential operators:
</p>
<p>ψn(x)=
1&radic;
n!
</p>
<p>(
mω
</p>
<p>�π
</p>
<p>)1/4(&radic;
mω
</p>
<p>2�
x &minus;
</p>
<p>&radic;
�
</p>
<p>2mω
</p>
<p>d
</p>
<p>dx
</p>
<p>)n
e&minus;mωx
</p>
<p>2/(2�).
</p>
<p>Defining a new variable y =&radic;mω/�x transforms this equation into
</p>
<p>ψn =
(
mω
</p>
<p>�π
</p>
<p>)1/4 1&radic;
2nn!
</p>
<p>(
y &minus; d
</p>
<p>dy
</p>
<p>)n
e&minus;y
</p>
<p>2/2.
</p>
<p>7From here on, the unit operator 1 will not be shown explicitly.</p>
<p/>
</div>
<div class="page"><p/>
<p>446 14 Second-Order Linear Differential Equations
</p>
<p>From this, the relation between Hermite polynomials, and the solutions of
the one-dimensional harmonic oscillator as given in the previous example,
we can obtain a general formula for Hn(x). In particular, if we note that (see
Problem 14.23)
</p>
<p>ey
2/2
</p>
<p>(
y &minus; d
</p>
<p>dy
</p>
<p>)
e&minus;y
</p>
<p>2/2 =&minus;ey2 d
dy
</p>
<p>e&minus;y
2
</p>
<p>and, in general,
</p>
<p>ey
2/2
</p>
<p>(
y &minus; d
</p>
<p>dy
</p>
<p>)n
e&minus;y
</p>
<p>2/2 = (&minus;1)ney2 d
n
</p>
<p>dyn
e&minus;y
</p>
<p>2
,
</p>
<p>we recover the generalized Rodriguez formula of Chap. 8 for Hermite poly-
nomials.
</p>
<p>14.7 SOLDEs with Constant Coefficients
</p>
<p>The solution to a SOLDE with constant coefficients can always be found in
closed form. In fact, we can treat an nth-order linear differential equation
(NOLDE) with constant coefficients with no extra effort. This section out-
lines the procedure for solving such an equation. For details, the reader is re-
ferred to any elementary book on differential equations (see also [Hass 08]).
The most general nth-order linear differential equation (NOLDE) with con-
stant coefficients can be written as
</p>
<p>L[y] &equiv; y(n) + an&minus;1y(n&minus;1) + &middot; &middot; &middot; + a1y&prime; + a0y = r(x). (14.37)
</p>
<p>The corresponding homogeneous NOLDE (HNOLDE) has r(x) = 0. The
solution to the HNOLDE
</p>
<p>L[y] &equiv; y(n) + an&minus;1y(n&minus;1) + &middot; &middot; &middot; + a1y&prime; + a0y = 0 (14.38)
</p>
<p>can be found by making the exponential substitution y = eλx , which results
in the equation
</p>
<p>L
[
eλx
</p>
<p>]
=
(
λn + an&minus;1λn&minus;1 + &middot; &middot; &middot; + a1λ+ a0
</p>
<p>)
eλx = 0.
</p>
<p>This equation will hold only if λ is a root of the characteristic polynomialcharacteristic
polynomial of an
</p>
<p>HNOLDE p(λ)&equiv; λn + an&minus;1λn&minus;1 + &middot; &middot; &middot; + a1λ+ a0,
</p>
<p>which, by the fundamental theorem of algebra (Theorem 10.5.6), can be
written as
</p>
<p>p(λ)= (λ&minus; λ1)k1(λ&minus; λ2)k2 &middot; &middot; &middot; (λ&minus; λm)km , (14.39)
</p>
<p>where λj is the distinct (complex) root of p(λ) with multiplicity kj .</p>
<p/>
</div>
<div class="page"><p/>
<p>14.7 SOLDEs with Constant Coefficients 447
</p>
<p>Theorem 14.7.1 Let {λj }mj=1 be the distinct roots of the characteristic poly-
nomial of the real HNOLDE of Eq. (14.38) with multiplicities {kj }mj=1. Then
the functions
</p>
<p>{{
xrj eλj x
</p>
<p>}kj&minus;1
rj=0
</p>
<p>}m
j=1 &equiv;
</p>
<p>{
eλj x, xeλj x, . . . , xkj&minus;1eλj x
</p>
<p>}m
j=1
</p>
<p>are a basis of solutions of Eq. (14.38).
</p>
<p>When λi is complex, one can write its corresponding solution in terms of
trigonometric functions.
</p>
<p>Example 14.7.2 An equation that is used in both mechanics and circuit
theory is
</p>
<p>d2y
</p>
<p>dt2
+ a dy
</p>
<p>dt
+ by = 0 for a, b &gt; 0. (14.40)
</p>
<p>Its characteristic polynomial is p(λ)= λ2 + aλ+ b, which has the roots
</p>
<p>λ1 =
1
</p>
<p>2
</p>
<p>(
&minus;a +
</p>
<p>&radic;
a2 &minus; 4b
</p>
<p>)
and λ2 =
</p>
<p>1
</p>
<p>2
</p>
<p>(
&minus;a &minus;
</p>
<p>&radic;
a2 &minus; 4b
</p>
<p>)
.
</p>
<p>We can distinguish three different possible motions depending on the
relative sizes of a and b.
</p>
<p>(a) a2 &gt; 4b (overdamped): Here we have two distinct simple roots. The overdamped oscillation
multiplicities are both one (k1 = k2 = 1); therefore, the power of x for
both solutions is zero (r1 = r2 = 0). Let γ &equiv; 12
</p>
<p>&radic;
a2 &minus; 4b. Then the
</p>
<p>most general solution is
</p>
<p>y(t)= e&minus;at/2
(
c1e
</p>
<p>γ t + c2e&minus;γ t
)
.
</p>
<p>Since a &gt; 2γ , this solution starts at y = c1 + c2 at t = 0 and continu-
ously decreases; so, as t &rarr;&infin;, y(t)&rarr; 0.
</p>
<p>(b) a2 = 4b (critically damped): In this case we have one multiple root of critically damped
oscillationorder 2 (k1 = 2); therefore, the power of x can be zero or 1 (r1 = 0,1).
</p>
<p>Thus, the general solution is
</p>
<p>y(t)= c1te&minus;at/2 + c0e&minus;at/2.
</p>
<p>This solution starts at y(0)= c0 at t = 0, reaches a maximum (or min-
imum) at t = 2/a&minus; c0/c1, and subsequently decays (grows) exponen-
tially to zero.
</p>
<p>(c) a2 &lt; 4b (underdamped): Once more, we have two distinct sim- underdamped
oscillationple roots. The multiplicities are both one (k1 = k2 = 1); therefore,
</p>
<p>the power of x for both solutions is zero (r1 = r2 = 0). Let ω &equiv;
1
2
</p>
<p>&radic;
4b&minus; a2. Then λ1 =&minus;a/2 + iω and λ2 = λ&lowast;1 . The roots are com-
</p>
<p>plex, and the most general solution is thus of the form
</p>
<p>y(t)= e&minus;at/2(c1 cosωt + c2 sinωt)=Ae&minus;at/2 cos(ωt + α).
</p>
<p>The solution is a harmonic variation with a decaying amplitude
A exp(&minus;at/2). Note that if a = 0, the amplitude does not decay. That
is why a is called the damping factor (or the damping constant).</p>
<p/>
</div>
<div class="page"><p/>
<p>448 14 Second-Order Linear Differential Equations
</p>
<p>These equations describe either a mechanical system oscillating (with
no external force) in a viscous (dissipative) fluid, or an electrical circuit
consisting of a resistance R, an inductance L, and a capacitance C. For
RLC circuits, a =R/L and b= 1/(LC). Thus, the damping factor depends
on the relative magnitudes of R and L. On the other hand, the frequency
</p>
<p>ω&equiv;
&radic;
</p>
<p>b&minus;
(
a
</p>
<p>2
</p>
<p>)2
=
&radic;
</p>
<p>1
</p>
<p>LC
&minus; R
</p>
<p>2
</p>
<p>4L2
</p>
<p>depends on all three elements. In particular, for R &ge; 2&radic;L/C the circuit does
not oscillate.
</p>
<p>A physical system whose behavior in the absence of a driving force is
described by a HNOLDE will obey an inhomogeneous NOLDE in the pres-
ence of the driving force. This driving force is simply the inhomogeneous
term of the NOLDE. The best way to solve such an inhomogeneous NOLDE
in its most general form is by using Fourier transforms and Green&rsquo;s func-
tions, as we will do in Chap. 20. For the particular, but important, case in
which the inhomogeneous term is a product of polynomials and exponen-
tials, the solution can be found in closed form.
</p>
<p>Theorem 14.7.3 The INOLDE L[y] = eλxS(x), where S(x) is a polyno-
mial, has the particular solution eλxq(x), where q(x) is also a polynomial.
The degree of q(x) equals that of S(x) unless λ = λj , a root of the char-
acteristic polynomial of L, in which case the degree of q(x) exceeds that of
S(x) by kj , the multiplicity of λj .
</p>
<p>Once we know the form of the particular solution of the NOLDE, we can
find the coefficients in the polynomial of the solution by substituting in the
NOLDE and matching the powers on both sides.
</p>
<p>Example 14.7.4 Let us find the most general solutions for the following
two differential equations subject to the boundary conditions y(0)= 0 and
y&prime;(0)= 1.
(a) The first DE we want to consider is
</p>
<p>y&prime;&prime; + y = xex . (14.41)
</p>
<p>The characteristic polynomial is λ2 + 1, whose roots are λ1 = i and
λ2 = &minus;i. Thus, a basis of solutions is {cosx, sinx}. To find the par-
ticular solution we note that λ (the coefficient of x in the exponential
part of the inhomogeneous term) is 1, which is neither of the roots
λ1 and λ2. Thus, the particular solution is of the form q(x)ex , where
q(x)= Ax + B is of degree 1 [same degree as that of S(x)= x]. We
now substitute y = (Ax +B)ex in Eq. (14.41) to obtain the relation
</p>
<p>2Axex + (2A+ 2B)ex = xex .</p>
<p/>
</div>
<div class="page"><p/>
<p>14.7 SOLDEs with Constant Coefficients 449
</p>
<p>Matching the coefficients, we have
</p>
<p>2A= 1 and 2A+ 2B = 0 &rArr; A= 1
2
=&minus;B.
</p>
<p>Thus, the most general solution is
</p>
<p>y = c1 cosx + c2 sinx +
1
</p>
<p>2
(x &minus; 1)ex .
</p>
<p>Imposing the given boundary conditions yields 0 = y(0)= c1 &minus; 12 and
1 = y&prime;(0)= c2. Thus,
</p>
<p>y = 1
2
</p>
<p>cosx + sinx + 1
2
(x &minus; 1)ex
</p>
<p>is the unique solution.
(b) The next DE we want to consider is
</p>
<p>y&prime;&prime; &minus; y = xex . (14.42)
</p>
<p>Here p(λ) = λ2 &minus; 1, and the roots are λ1 = 1 and λ2 = &minus;1. A basis
of solutions is {ex, e&minus;x}. To find a particular solution, we note that
S(x)= x and λ= 1 = λ1. Theorem 14.7.3 then implies that q(x) must
be of degree 2, because λ1 is a simple root, i.e., k1 = 1. We therefore
try
</p>
<p>q(x)=Ax2 +Bx +C &rArr; y =
(
Ax2 +Bx +C
</p>
<p>)
ex .
</p>
<p>Taking the derivatives and substituting in Eq. (14.42) yields two equa-
tions,
</p>
<p>4A= 1 and A+B = 0,
</p>
<p>whose solution is A = &minus;B = 14 . Note that C is not determined, be-
cause Cex is a solution of the homogeneous DE corresponding to
Eq. (14.42), so when L is applied to y, it eliminates the term Cex .
Another way of looking at the situation is to note that the most general
solution to (14.42) is of the form
</p>
<p>y = c1ex + c2e&minus;x +
(
</p>
<p>1
</p>
<p>4
x2 &minus; 1
</p>
<p>4
x +C
</p>
<p>)
ex .
</p>
<p>The term Cex could be absorbed in c1ex . We therefore set C = 0,
apply the boundary conditions, and find the unique solution
</p>
<p>y = 5
4
</p>
<p>sinhx + 1
4
</p>
<p>(
x2 &minus; x
</p>
<p>)
ex .</p>
<p/>
</div>
<div class="page"><p/>
<p>450 14 Second-Order Linear Differential Equations
</p>
<p>14.8 TheWKBMethod
</p>
<p>In this section, we treat the somewhat specialized method of obtaining an
approximate solution to a particular type of second-order DE arising from
the Schr&ouml;dinger equation in one dimension. The method&rsquo;s name comes from
Wentzel, Kramers, and Brillouin, who invented it and applied it for the first
time.
</p>
<p>Suppose we are interested in finding approximate solutions of the DE
</p>
<p>d2y
</p>
<p>dx2
+ q(x)y = 0 (14.43)
</p>
<p>in which q varies &ldquo;slowly&rdquo; with respect to x in the sense discussed below. If
q varies infinitely slowly, i.e., if it is a constant, the solution to Eq. (14.43) is
simply an imaginary exponential (or trigonometric). So, let us define φ(x)
by y = eiφ(x) and rewrite the DE as
</p>
<p>(
φ&prime;
)2 + iφ&prime;&prime; &minus; q = 0. (14.44)
</p>
<p>Assuming that φ&prime;&prime; is small (compared to q), so that y does not oscillate too
rapidly, we can find an approximate solution to the DE:
</p>
<p>φ&prime; =&plusmn;&radic;q &rArr; φ =&plusmn;
&int; &radic;
</p>
<p>q(x) dx. (14.45)
</p>
<p>The condition of validity of our assumption is obtained by differentiating
(14.45):
</p>
<p>∣∣φ&prime;&prime;
∣∣&asymp; 1
</p>
<p>2
</p>
<p>∣∣∣∣
q &prime;&radic;
q
</p>
<p>∣∣∣∣≪ |q|.
</p>
<p>It follows from Eq. (14.45) and the definition of φ that 1/
&radic;
q is approx-
</p>
<p>imately 1/(2π) times one &ldquo;wavelength&rdquo; of the solution y. Therefore, the
approximation is valid if the change in q in one wavelength is small com-
pared to |q|.
</p>
<p>The approximation can be improved by inserting the derivative of (14.45)
in the DE and solving for a new φ:
</p>
<p>(
φ&prime;
)2 &asymp; q &plusmn; i
</p>
<p>2
</p>
<p>q &prime;&radic;
q
</p>
<p>&rArr; φ&prime; &asymp;&plusmn;
(
q &plusmn; i
</p>
<p>2
</p>
<p>q &prime;&radic;
q
</p>
<p>)1/2
,
</p>
<p>or
</p>
<p>φ&prime; &asymp;&plusmn;&radic;q
(
</p>
<p>1 &plusmn; i
2
</p>
<p>q &prime;
</p>
<p>q3/2
</p>
<p>)1/2
=&plusmn;&radic;q
</p>
<p>(
1 &plusmn; i
</p>
<p>4
</p>
<p>q &prime;
</p>
<p>q3/2
</p>
<p>)
</p>
<p>=&plusmn;&radic;q + i
4
</p>
<p>q &prime;
</p>
<p>q
&rArr; φ(x)&asymp;&plusmn;
</p>
<p>&int; &radic;
q dx + i
</p>
<p>4
lnq.
</p>
<p>The two choices give rise to two different solutions, a linear combination of
which gives the most general solution. Thus,
</p>
<p>y &asymp; 1
4
&radic;
q(x)
</p>
<p>{
c1 exp
</p>
<p>[
i
</p>
<p>&int; &radic;
q dx
</p>
<p>]
+ c2 exp
</p>
<p>[
&minus;i
</p>
<p>&int; &radic;
q dx
</p>
<p>]}
. (14.46)</p>
<p/>
</div>
<div class="page"><p/>
<p>14.8 The WKB Method 451
</p>
<p>Equation (14.46) gives an approximate solution to (14.43) in any region
in which the condition of validity holds. The method fails if q changes too
rapidly or if it is zero at some point of the region. The latter is a serious diffi-
culty, since we often wish to join a solution in a region in which q(x) &gt; 0 to
one in a region in which q(x) &lt; 0. There is a general procedure for deriving
the so-called connection formulas relating the constants c1 and c2 of the two connection formulas
solutions on either side of the point where q(x)= 0. We shall not go into the
details of such a derivation, as it is not particularly illuminating.8 We simply
quote a particular result that is useful in applications.
</p>
<p>Suppose that q passes through zero at x0, is positive to the right of x0,
and satisfies the condition of validity in regions both to the right and to
the left of x0. Furthermore, assume that the solution of the DE decreases
exponentially to the left of x0. Under such conditions, the solution to the
left will be of the form
</p>
<p>1
4
&radic;&minus;q(x) exp
</p>
<p>[
&minus;
&int; x0
x
</p>
<p>&radic;
&minus;q(x) dx
</p>
<p>]
, (14.47)
</p>
<p>while to the right, we have
</p>
<p>2
1
</p>
<p>4
&radic;
q(x)
</p>
<p>cos
</p>
<p>[&int; x
</p>
<p>x0
</p>
<p>&radic;
q(x) dx &minus; π
</p>
<p>4
</p>
<p>]
. (14.48)
</p>
<p>A similar procedure gives connection formulas for the case where q is pos-
itive on the left and negative on the right of x0.
</p>
<p>Example 14.8.1 Consider the Schr&ouml;dinger equation in one dimension
</p>
<p>d2ψ
</p>
<p>dx2
+ 2m
</p>
<p>�2
[
E &minus; V (x)
</p>
<p>]
ψ = 0
</p>
<p>where V (x) is a potential well meeting the horizontal line of constant E at
x = a and x = b, so that
</p>
<p>q(x)= 2m
�2
</p>
<p>[
E &minus; V (x)
</p>
<p>]
{
&gt; 0 if a &lt; x &lt; b,
</p>
<p>&lt; 0 if x &lt; a or x &gt; b.
</p>
<p>The solution that is bounded to the left of a must be exponentially decay-
ing. Therefore, in the interval (a, b) the approximate solution, as given by
Eq. (14.48), is
</p>
<p>ψ(x)&asymp; A
(E &minus; V )1/4 cos
</p>
<p>(&int; x
</p>
<p>a
</p>
<p>&radic;
2m
</p>
<p>�2
[
E &minus; V (x)
</p>
<p>]
dx &minus; π
</p>
<p>4
</p>
<p>)
,
</p>
<p>where A is some arbitrary constant. The solution that is bounded to the right
of b must also be exponentially decaying. Hence, the solution for a &lt; x &lt; b
is
</p>
<p>ψ(x)&asymp; B
(E &minus; V )1/4 cos
</p>
<p>(&int; b
</p>
<p>x
</p>
<p>&radic;
2m
</p>
<p>�2
[
E &minus; V (x)
</p>
<p>]
dx &minus; π
</p>
<p>4
</p>
<p>)
.
</p>
<p>8The interested reader is referred to the book by Mathews and Walker, pp. 27&ndash;37.</p>
<p/>
</div>
<div class="page"><p/>
<p>452 14 Second-Order Linear Differential Equations
</p>
<p>Since these two expressions give the same function in the same region, they
must be equal. Thus, A= B , and, more importantly,
</p>
<p>cos
</p>
<p>(&int; x
</p>
<p>a
</p>
<p>&radic;
2m
</p>
<p>�2
[
E &minus; V (x)
</p>
<p>]
dx &minus; π
</p>
<p>4
</p>
<p>)
</p>
<p>= cos
(&int; b
</p>
<p>x
</p>
<p>&radic;
2m
</p>
<p>�2
[
E &minus; V (x)
</p>
<p>]
dx &minus; π
</p>
<p>4
</p>
<p>)
,
</p>
<p>or
&int; b
</p>
<p>a
</p>
<p>&radic;
2m
</p>
<p>[
E &minus; V (x)
</p>
<p>]
dx =
</p>
<p>(
n+ 1
</p>
<p>2
</p>
<p>)
π�.
</p>
<p>This is essentially the Bohr-Sommerfeld quantization condition of pre-1925Bohr-Sommerfeld
quantization condition quantum mechanics.
</p>
<p>14.8.1 Classical Limit of the Schr&ouml;dinger Equation
</p>
<p>As long as we are approximating solutions of second-order DEs that arise
naturally from the Schr&ouml;dinger equation, it is instructive to look at another
approximation to the Schr&ouml;dinger equation, its classical limit in which the
Planck constant goes to zero.
</p>
<p>The idea is to note that since ψ(r, t) is a complex function, one can write
it as
</p>
<p>ψ(r, t)=A(r, t) exp
[
i
</p>
<p>�
S(r, t)
</p>
<p>]
, (14.49)
</p>
<p>where A(r, t) and S(r, t) are real-valued functions. Substituting (14.49) in
the Schr&ouml;dinger equation and separating the real and the imaginary parts
yields
</p>
<p>&part;S
</p>
<p>&part;t
+ &nabla;&nabla;&nabla;S &middot; &nabla;&nabla;&nabla;S
</p>
<p>2m
+ V = �
</p>
<p>2
</p>
<p>2m
</p>
<p>&nabla;2A
A
</p>
<p>,
</p>
<p>m
&part;A
</p>
<p>&part;t
+&nabla;&nabla;&nabla;S &middot; &nabla;&nabla;&nabla;A+ A
</p>
<p>2
&nabla;2S = 0.
</p>
<p>(14.50)
</p>
<p>These two equations are completely equivalent to the Schr&ouml;dinger equa-
tion. The second equation has a direct physical interpretation. Define
</p>
<p>ρ(r, t)&equiv;A2(r, t)=
∣∣ψ(r, t)
</p>
<p>∣∣2 and J(r, t)&equiv;A2(r, t) &nabla;S
m︸︷︷︸
&equiv;v
</p>
<p>= ρv,
</p>
<p>(14.51)
multiply the second equation in (14.50) by 2A/m, and note that it then can
be written as
</p>
<p>&part;ρ
</p>
<p>&part;t
+&nabla; &middot; J = 0, (14.52)
</p>
<p>which is the continuity equation for probability. The fact that J is indeed the
probability current density is left for Problem 14.32.</p>
<p/>
</div>
<div class="page"><p/>
<p>14.9 Problems 453
</p>
<p>The first equation of (14.50) gives an interesting result when �&rarr; 0 be-
cause in this limit, the RHS of the equation will be zero, and we get
</p>
<p>&part;S
</p>
<p>&part;t
+ 1
</p>
<p>2
mv2 + V = 0.
</p>
<p>Taking the gradient of this equation, we obtain
</p>
<p>(
&part;
</p>
<p>&part;t
+ v &middot; &nabla;&nabla;&nabla;
</p>
<p>)
mv +&nabla;&nabla;&nabla;V = 0,
</p>
<p>which is the equation of motion of a classical fluid with velocity field v =
&nabla;&nabla;&nabla;S/m. We thus have the following:
</p>
<p>Proposition 14.8.2 In the classical limit, the solution of the Schr&ouml;dinger Schr&ouml;dinger equation
describes a classical
</p>
<p>statistical mixture when
</p>
<p>�&rarr; 0.
</p>
<p>equation describes a fluid (statistical mixture) of noninteracting classical
particles of massm subject to the potential V (r). The density and the current
density of this fluid are, respectively, the probability density ρ = |ψ |2 and
the probability current density J of the quantum particle.
</p>
<p>14.9 Problems
</p>
<p>14.1 Let u(x) be a differentiable function satisfying the differential in-
equality u&prime;(x) &le; Ku(x) for x &isin; [a, b], where K is a constant. Show that
u(x) &le; u(a)eK(x&minus;a). Hint: Multiply both sides of the inequality by e&minus;Kx ,
and show that the result can be written as the derivative of a nonincreasing
function. Then use the fact that a &le; x to get the final result.
</p>
<p>14.2 Prove Proposition 14.4.2.
</p>
<p>14.3 Let f1(x) = x and f2(x) = |x| for x &isin; [&minus;1,1]. Show that these two
functions are linearly independent in the given interval, and that their Wron-
skian vanishes. Is this a violation of Theorem 14.4.3?
</p>
<p>14.4 How would you generalize the Wronskian to n functions which have
derivatives up to nth order? Prove that the Wronskian vanishes if the func-
tions are linearly dependent.
</p>
<p>14.5 Let f and g be two differentiable functions that are linearly dependent.
Show that their Wronskian vanishes.
</p>
<p>14.6 Show that if (f1, f &prime;1) and (f2, f
&prime;
2) are linearly dependent at one point,
</p>
<p>then f1 and f2 are linearly dependent at all x &isin; [a, b]. Here f1 and f2 are
solutions of the DE of (14.12). Hint: Derive the identity
</p>
<p>W(f1, f2;x2)=W(f1, f2;x1) exp
{
&minus;
&int; x2
x1
</p>
<p>p(t) dt
</p>
<p>}
.</p>
<p/>
</div>
<div class="page"><p/>
<p>454 14 Second-Order Linear Differential Equations
</p>
<p>14.7 Show that the solutions to the SOLDE y&prime;&prime;+q(x)y = 0 have a constant
Wronskian.
</p>
<p>14.8 Find (in terms of an integral) Gn(x), the linearly independent &ldquo;part-
ner&rdquo; of the Hermite polynomial Hn(x). Specialize this to n= 0,1. Is it pos-
sible to find G0(x) and G1(x) in terms of elementary functions?
</p>
<p>14.9 Let f1, f2, and f3 be any three solutions of y&prime;&prime; + py&prime; + qy = 0. Show
that the (generalized 3 &times; 3) Wronskian of these solutions is zero. Thus, any
three solutions of the HSOLDE are linearly dependent.
</p>
<p>14.10 For the HSOLDE y&prime;&prime; + py&prime; + qy = 0, show that
</p>
<p>p =&minus;f1f
&prime;&prime;
2 &minus; f2f &prime;&prime;1
</p>
<p>W(f1, f2)
and q = f
</p>
<p>&prime;
1f
</p>
<p>&prime;&prime;
2 &minus; f &prime;2f &prime;&prime;1
</p>
<p>W(f1, f2)
.
</p>
<p>Thus, knowing two solutions of an HSOLDE allows us to reconstruct the
DE.
</p>
<p>14.11 Let f1, f2, and f3 be three solutions of the third-order linear dif-
ferential equation y&prime;&prime;&prime; + p2(x)y&prime;&prime; + p1(x)y&prime; + p0(x)y = 0. Derive a FODE
satisfied by the (generalized 3 &times; 3) Wronskian of these solutions.
</p>
<p>14.12 Prove Corollary 14.4.12. Hint: Consider the solution u= 1 of the DE
u&prime;&prime; = 0 and apply Theorem 14.4.10.
</p>
<p>14.13 Show that the adjoint of M given in Eq. (14.20) is the original L.
</p>
<p>14.14 Show that if u(x) and v(x) are solutions of the self-adjoint DE
(pu&prime;)&prime; + qu= 0, then Abel&rsquo;s identity, p(uv&prime; &minus; vu&prime;)= constant, holds.Abel&rsquo;s identity
</p>
<p>14.15 Reduce each DE to self-adjoint form.
</p>
<p>(a) x2y&prime;&prime; + xy&prime; + y = 0, (b) y&prime;&prime; + y&prime; tanx = 0.
</p>
<p>14.16 Reduce the self-adjoint DE (py&prime;)&prime; + qy = 0 to u&prime;&prime; + S(x)u = 0 by
an appropriate change of the dependent variable. What is S(x)? Apply this
reduction to the Legendre DE for Pn(x), and show that
</p>
<p>S(x)= 1 + n(n+ 1)&minus; n(n+ 1)x
2
</p>
<p>(1 &minus; x2)2 .
</p>
<p>Now use this result to show that every solution of the Legendre equation has
at least (2n+ 1)/π zeros on (&minus;1,+1).
</p>
<p>14.17 Substitute v = y&prime;/y in the homogeneous SOLDE
</p>
<p>y&prime;&prime; + p(x)y&prime; + q(x)y = 0
</p>
<p>and:</p>
<p/>
</div>
<div class="page"><p/>
<p>14.9 Problems 455
</p>
<p>(a) Show that it turns into v&prime; + v2 + p(x)v + q(x) = 0, which is a first-
order nonlinear equation called the Riccati equation. Would the same
substitution work if the DE were inhomogeneous?
</p>
<p>Riccati equation
</p>
<p>(b) Show that by an appropriate transformation, the Riccati equation can
be directly cast in the form u&prime; + u2 + S(x)= 0.
</p>
<p>14.18 For the function S(x) defined in Example 14.6.1, let S&minus;1(x) be the
inverse, i.e., S&minus;1(S(x))= x. Show that
</p>
<p>d
</p>
<p>dx
</p>
<p>[
S&minus;1(x)
</p>
<p>]
= 1&radic;
</p>
<p>1 &minus; x2
,
</p>
<p>and given that S&minus;1(0)= 0, conclude that
</p>
<p>S&minus;1(x)=
&int; x
</p>
<p>0
</p>
<p>dt&radic;
1 &minus; t2
</p>
<p>.
</p>
<p>14.19 Define sinhx and coshx as the solutions of y&prime;&prime; = y satisfying the
boundary conditions y(0) = 0, y&prime;(0) = 1 and y(0) = 1, y&prime;(0) = 0, respec-
tively. Using Example 14.6.1 as a guide, show that
</p>
<p>(a) cosh2 x &minus; sinh2 x = 1,
(b) cosh(&minus;x)= coshx,
(c) sinh(&minus;x)=&minus; sinhx.
(d) sinh(a + x)= sinha coshx + cosha sinhx.
</p>
<p>14.20 For Example 14.6.5, derive
</p>
<p>(a) Equation (14.29), and
(b) Equation (14.30) by direct substitution.
(c) Let λ = l(l + 1) and calculate the Legendre polynomials Pl(x) for
</p>
<p>l = 0,1,2,3, subject to the condition Pl(1)= 1.
</p>
<p>14.21 Use Eq. (14.32) of Example 14.6.6 to generate the first three Hermite
polynomials. Use the normalization
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>[
Hn(x)
</p>
<p>]2
e&minus;x
</p>
<p>2
dx =&radic;π 2nn!
</p>
<p>to determine the arbitrary constant.
</p>
<p>14.22 The function defined by
</p>
<p>f (x)=
&infin;&sum;
</p>
<p>n=0
cnx
</p>
<p>n, where cn+2 =
2n&minus; λ
</p>
<p>(n+ 1)(n+ 2)cn,
</p>
<p>can be written as f (x) = c0g(x) + c1h(x), where g is even and h is odd
in x. Show that f (x) goes to infinity at least as fast as ex
</p>
<p>2
does, i.e.,
</p>
<p>limx&rarr;&infin; f (x)e&minus;x
2 �= 0. Hint: Consider g(x) and h(x) separately and show</p>
<p/>
</div>
<div class="page"><p/>
<p>456 14 Second-Order Linear Differential Equations
</p>
<p>that
</p>
<p>g(x)=
&infin;&sum;
</p>
<p>n=0
bnx
</p>
<p>n, where bn+1 =
4n&minus; λ
</p>
<p>(2n+ 1)(2n+ 2)bn.
</p>
<p>Then concentrate on the ratio g(x)/ex
2
, where g and ex
</p>
<p>2
are approximated
</p>
<p>by polynomials of very high degrees. Take the limit of this ratio as x &rarr;&infin;,
and use recursion relations for g and ex
</p>
<p>2
. The odd case follows similarly.
</p>
<p>14.23 Refer to Sect. 14.6.2 for this problem.
</p>
<p>(a) Derive the commutation relation [a,a&dagger;] = 1.
(b) Show that the Hamiltonian can be written as given in Eq. (14.33).
(c) Derive the commutation relation [a, (a&dagger;)n] = n(a&dagger;)n&minus;1.
(d) Take the inner product of Eq. (14.35) with itself and use (c) to show
</p>
<p>that |cn|2 = n|cn&minus;1|2. From this, conclude that |cn|2 = n!|c0|2.
(e) For any function f (y), show that
</p>
<p>(
y &minus; d
</p>
<p>dy
</p>
<p>)(
ey
</p>
<p>2/2f
)
=&minus;ey2/2 df
</p>
<p>dy
.
</p>
<p>Apply (y &minus; d/dy) repeatedly to both sides of the above equation to
obtain
</p>
<p>(
y &minus; d
</p>
<p>dy
</p>
<p>)n(
ey
</p>
<p>2/2f
)
= (&minus;1)ney2/2 d
</p>
<p>nf
</p>
<p>dyn
.
</p>
<p>(f) Choose an appropriate f (y) in part (e) and show that
</p>
<p>ey
2/2
</p>
<p>(
y &minus; d
</p>
<p>dy
</p>
<p>)n
e&minus;y
</p>
<p>2/2 = (&minus;1)ney2 d
n
</p>
<p>dyn
</p>
<p>(
e&minus;y
</p>
<p>2)
.
</p>
<p>14.24 Solve Airy&rsquo;s DE, y&prime;&prime; + xy = 0, by the power-series method. ShowAiry&rsquo;s DE
that the radius of convergence for both independent solutions is infinite.
Use the comparison theorem to show that for x &gt; 0 these solutions have
infinitely many zeros, but for x &lt; 0 they can have at most one zero.
</p>
<p>14.25 Show that the functions xreλx , where r = 0,1,2, . . . , k, are linearly
independent. Hint: Apply appropriate powers of D&minus; λ to a linear combina-
tion of xreλx for all possible r&rsquo;s.
</p>
<p>14.26 Find a basis of real solutions for each DE.
</p>
<p>(a) y&prime;&prime; + 5y&prime; + 6 = 0, (b) y&prime;&prime;&prime; + 6y&prime;&prime; + 12y&prime; + 8y = 0,
</p>
<p>(c)
d4y
</p>
<p>dx4
= y, (d) d
</p>
<p>4y
</p>
<p>dx4
=&minus;y.
</p>
<p>14.27 Solve the following initial value problems.
</p>
<p>(a)
d4y
</p>
<p>dx4
= y, y(0)= y&prime;(0)= y&prime;&prime;&prime;(0)= 0, y&prime;&prime;(0)= 1,</p>
<p/>
</div>
<div class="page"><p/>
<p>14.9 Problems 457
</p>
<p>(b)
d4y
</p>
<p>dx4
+ d
</p>
<p>2y
</p>
<p>dx2
= 0, y(0)= y&prime;&prime;(0)= y&prime;&prime;&prime;(0)= 0, y&prime;(0)= 1,
</p>
<p>(c)
d4y
</p>
<p>dx4
= 0, y(0)= y&prime;(0)= y&prime;&prime;(0)= 0, y&prime;&prime;&prime;(0)= 2.
</p>
<p>14.28 Solve y&prime;&prime; &minus; 2y&prime; + y = xex subject to the initial conditions y(0)= 0,
y&prime;(0)= 1.
</p>
<p>14.29 Find the general solution of each equation,
</p>
<p>(a) y&prime;&prime; = xex, (b) y&prime;&prime; &minus; 4y&prime; + 4y = x2,
(c) y&prime;&prime; + y = sinx sin 2x, (d) y&prime;&prime; &minus; y =
</p>
<p>(
1 + e&minus;x
</p>
<p>)2
,
</p>
<p>(e) y&prime;&prime; &minus; y = ex sin 2x, (f) y(6) &minus; y(4) = x2,
(g) y&prime;&prime; &minus; 4y&prime; + 4 = ex + xe2x, (h) y&prime;&prime; + y = e2x .
</p>
<p>14.30 Consider the Euler equation, the Euler equation
</p>
<p>xny(n) + an&minus;1xn&minus;1y(n&minus;1) + &middot; &middot; &middot; + a1xy&prime; + a0y = r(x).
</p>
<p>Substitute x = et and show that such a substitution reduces this to a DE with
constant coefficients. In particular, solve x2y&prime;&prime; &minus; 4xy&prime; + 6y = x.
</p>
<p>14.31 Show that
</p>
<p>(a) the substitution (14.49) reduces the Schr&ouml;dinger equation to (14.50),
and
</p>
<p>(b) derive the continuity equation for probability from the second equa-
tion of (14.50).
</p>
<p>14.32 Show that the usual definition of probability current density,
</p>
<p>J = Re
[
ψ&lowast;
</p>
<p>�
</p>
<p>im
&nabla;&nabla;&nabla;ψ
</p>
<p>]
,
</p>
<p>reduces to that in Eq. (14.51) if we use (14.49).</p>
<p/>
</div>
<div class="page"><p/>
<p>15Complex Analysis of SOLDEs
</p>
<p>We have familiarized ourselves with some useful techniques for finding so-
lutions to differential equations. One powerful method that leads to formal
solutions is power series. We also stated Theorem 14.6.4 which guarantees
the convergence of the solution of the power series within a circle whose
size is at least as large as the smallest of the circles of convergence of the
coefficient functions. Thus, the convergence of the solution is related to the
convergence of the coefficient functions. What about the nature of the con-
vergence, or the analyticity of the solution? Is it related to the analyticity of
the coefficient functions? If so, how? Are the singular points of the coeffi-
cients also singular points of the solution? Is the nature of the singularities
the same? This chapter answers some of these questions.
</p>
<p>Analyticity is best handled in the complex plane. An important reason
for this is the property of analytic continuation discussed in Chap. 12. The
differential equation du/dx = u2 has a solution u=&minus;1/x for all x except
x = 0. Thus, we have to &ldquo;puncture&rdquo; the real line by removing x = 0 from it.
Then we have two solutions, because the domain of definition of u=&minus;1/x
is not connected on the real line (technically, the definition of a function
includes its domain as well as the rule for going from the domain to the
range). In addition, if we confine ourselves to the real line, there is no way
that we can connect the x &gt; 0 region to the x &lt; 0 region. However, in the
complex plane the same equation, dw/dz = w2, has the complex solution
w = &minus;1/z, which is analytic everywhere except at z = 0. Puncturing the
complex plane does not destroy the connectivity of the region of definition
of w. Thus, the solution in the x &gt; 0 region can be analytically continued to
the solution in the x &lt; 0 region by going around the origin.
</p>
<p>The aim of this chapter is to investigate the analytic properties of the
solutions of some well known SOLDEs in mathematical physics. We begin
with a result from differential equation theory (for a proof, see [Birk 78,
p. 223]).
</p>
<p>Proposition 15.0.1 (Continuation principle) The function obtained by an-
continuation principle
</p>
<p>alytic continuation of any solution of an analytic differential equation along
any path in the complex plane is a solution of the analytic continuation of
the differential equation along the same path.
</p>
<p>S. Hassani, Mathematical Physics, DOI 10.1007/978-3-319-01195-0_15,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>459</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-01195-0_15">http://dx.doi.org/10.1007/978-3-319-01195-0_15</a></div>
</div>
<div class="page"><p/>
<p>460 15 Complex Analysis of SOLDEs
</p>
<p>An analytic differential equation is one with analytic coefficient func-
tions. This proposition makes it possible to find a solution in one region of
the complex plane and then continue it analytically. The following example
shows how the singularities of the coefficient functions affect the behavior
of the solution.
</p>
<p>Example 15.0.2 Let us consider the FODE w&prime; &minus; (γ /z)w = 0 for γ &isin; R.
The coefficient function p(z)=&minus;γ /z has a simple pole at z= 0. The solu-
tion to the FODE is easily found to be w = zγ . Thus, depending on whether
γ is a nonnegative integer, a negative integer &minus;m, or a noninteger, the so-
lution has a regular point, a pole of order m, or a branch point at z = 0,
respectively.
</p>
<p>This example shows that the singularities of the solution depend on the
parameters of the differential equation.
</p>
<p>15.1 Analytic Properties of Complex DEs
</p>
<p>To prepare for discussing the analytic properties of the solutions of SOL-
DEs, let us consider some general properties of differential equations from
a complex analytical point of view.
</p>
<p>15.1.1 Complex FOLDEs
</p>
<p>In the homogeneous FOLDE
</p>
<p>dw
</p>
<p>dz
+ p(z)w = 0, (15.1)
</p>
<p>p(z) is assumed to have only isolated singular points. It follows that p(z)
can be expanded about a point z0&mdash;which may be a singularity of p(z)&mdash;as
a Laurent series in some annular region r1 &lt; |z&minus; z0|&lt; r2:
</p>
<p>p(z)=
&infin;&sum;
</p>
<p>n=&minus;&infin;
an(z&minus; z0)n where r1 &lt; |z&minus; z0|&lt; r2.
</p>
<p>The solution to Eq. (15.1), as given in Theorem 14.2.1 with q = 0, is
</p>
<p>w(z)= exp
[
&minus;
&int;
</p>
<p>p(z) dz
</p>
<p>]
</p>
<p>= C exp
[
&minus;a&minus;1
</p>
<p>&int;
dz
</p>
<p>z&minus; z0
&minus;
</p>
<p>&infin;&sum;
</p>
<p>n=0
an
</p>
<p>&int;
(z&minus; z0)ndz
</p>
<p>&minus;
&infin;&sum;
</p>
<p>n=2
a&minus;n
</p>
<p>&int;
(z&minus; z0)&minus;ndz
</p>
<p>]</p>
<p/>
</div>
<div class="page"><p/>
<p>15.1 Analytic Properties of Complex DEs 461
</p>
<p>= C exp
[
&minus;a&minus;1 ln(z&minus; z0)&minus;
</p>
<p>&infin;&sum;
</p>
<p>n=0
</p>
<p>an
</p>
<p>n+ 1 (z&minus; z0)
n+1
</p>
<p>+
&infin;&sum;
</p>
<p>n=1
</p>
<p>a&minus;n&minus;1
n
</p>
<p>(z&minus; z0)&minus;n
]
</p>
<p>We can write this solution as
</p>
<p>w(z)= C(z&minus; z0)αg(z), (15.2)
</p>
<p>where α &equiv;&minus;a&minus;1 and g(z) is an analytic single-valued function in the annu-
lar region r1 &lt; |z&minus; z0|&lt; r2 because g(z) is the exponential of an analytic
function.
</p>
<p>For the special case in which p has a simple pole, i.e., when a&minus;n = 0
for all n &ge; 2, the second sum in the exponent will be absent, and g will be
analytic even at z0. In fact, g(z0)= 1, and choosing C = 1, we can write
</p>
<p>w(z)= (z&minus; z0)α
[
</p>
<p>1 +
&infin;&sum;
</p>
<p>k=1
bk(z&minus; z0)k
</p>
<p>]
. (15.3)
</p>
<p>Depending on the nature of the singularity of p(z) at z0, the solutions
given by Eq. (15.2) have different classifications. For instance, if p(z) has a
removable singularity (if a&minus;n = 0 &forall;n &ge; 1), the solution is Cg(z), which is
</p>
<p>The singularity of the
</p>
<p>coefficient functions of
</p>
<p>an FOLDE determines
</p>
<p>the singularity of the
</p>
<p>solution.
</p>
<p>analytic. In this case, we say that the FOLDE [Eq. (15.1)] has a remov-
able singularity at z0. If p(z) has a simple pole at z0 (if a&minus;1 �= 0 and
a&minus;n = 0 &forall;n &ge; 2), then in general, the solution has a branch point at z0.
In this case we say that the FOLDE has a regular singular point. Finally,
if p(z) has a pole of order m &gt; 1, then the solution will have an essential
singularity (see Problem 15.1). In this case the FOLDE is said to have an
irregular singular point.
</p>
<p>To arrive at the solution given by Eq. (15.2), we had to solve the FOLDE.
Since higher-order differential equations are not as easily solved, it is desir-
able to obtain such a solution through other considerations. The following
example sets the stage for this endeavor.
</p>
<p>Example 15.1.1 A FOLDE has a unique solution, to within a multiplica-
tive constant, given by Theorem 14.2.1. Thus, given a solution w(z),
any other solution must be of the form Cw(z). Let z0 be a singularity
of p(z), and let z &minus; z0 = reiθ . Start at a point z and circle z0 so that
θ &rarr; θ + 2π . Even though p(z) may have a simple pole at z0, the solu-
tion may have a branch point there. This is clear from the general solu-
tion, where α may be a noninteger. Thus, w̃(z) &equiv; w(z0 + rei(θ+2π)) may
be different from w(z). To discover this branch point&mdash;without solving
the DE&mdash;invoke Proposition 15.0.1 and conclude that w̃(z) is also a so-
lution to the FOLDE. Thus, w̃(z) can be different from w(z) by at most
a multiplicative constant: w̃(z) = Cw(z). Define the complex number α
by C = e2πiα . Then the function g(z) &equiv; (z &minus; z0)&minus;αw(z) is single-valued</p>
<p/>
</div>
<div class="page"><p/>
<p>462 15 Complex Analysis of SOLDEs
</p>
<p>around z0. In fact,
</p>
<p>g
(
z0 + rei(θ+2π)
</p>
<p>)
=
[
rei(θ+2π)
</p>
<p>]&minus;α
w
(
z0 + rei(θ+2π)
</p>
<p>)
</p>
<p>= (z&minus; z0)&minus;αe&minus;2πiαe2πiαw(z)= (z&minus; z0)&minus;αw(z)= g(z).
</p>
<p>This argument shows that a solution w(z) of the FOLDE of Eq. (15.1)
can be written as w(z)= (z&minus; z0)αg(z), where g(z) is single-valued.
</p>
<p>15.1.2 The Circuit Matrix
</p>
<p>The method used in Example 15.1.1 can be generalized to obtain a similar
result for the NOLDE
</p>
<p>L[w] = d
nw
</p>
<p>dzn
+ pn&minus;1(z)
</p>
<p>dn&minus;1w
dzn&minus;1
</p>
<p>+ &middot; &middot; &middot; + p1(z)
dw
</p>
<p>dz
+ p0(z)w = 0 (15.4)
</p>
<p>where all the pi(z) are analytic in r1 &lt; |z&minus; z0|&lt; r2.
Let {wj (z)}nj=1 be a basis of solutions of Eq. (15.4), and let z&minus;z0 = reiθ .
</p>
<p>Start at z and analytically continue the functions wj (z) one complete turn
to θ + 2π . Let w̃j (z) &equiv; w̃j (z0 + reiθ ) &equiv; wj (z0 + rei(θ+2π)). Then, by a
generalization of Proposition 15.0.1, {w̃j (z)}nj=1 are not only solutions, but
they are linearly independent (because they are wj &rsquo;s evaluated at a different
point). Therefore, they also form a basis of solutions. On the other hand,
w̃j (z) can be expressed as a linear combination of the wj (z). Thus,
</p>
<p>w̃j (z)=wj
(
z0 + rei(θ+2π)
</p>
<p>)
=
</p>
<p>n&sum;
</p>
<p>k=1
ajkwk(z).
</p>
<p>The matrix A = (ajk), called the circuit matrix of the NOLDE, is invertible,circuit matrix
because it transforms one basis into another. Therefore, it has only nonzero
eigenvalues. We let λ be one such eigenvalue, and choose the column vector
C, with entries {ci}ni=1, to be the corresponding eigenvector of the transpose
of A (note that A and At , have the same set of eigenvalues). At least one such
eigenvector always exists, because the characteristic polynomial of At has
at least one root. Now we let w(z)=&sum;nj=1 cjwj (z). Clearly, this w(z) is a
solution of (15.4), and
</p>
<p>w̃(z)&equiv;w
(
z0 + rei(θ+2π)
</p>
<p>)
=
</p>
<p>n&sum;
</p>
<p>j=1
cjwj
</p>
<p>(
z0 + rei(θ+2π)
</p>
<p>)
</p>
<p>=
n&sum;
</p>
<p>j=1
cj
</p>
<p>n&sum;
</p>
<p>k=1
ajkwk(z)=
</p>
<p>&sum;
</p>
<p>j,k
</p>
<p>(
At
)
kj
cjwk(z)=
</p>
<p>n&sum;
</p>
<p>k=1
λckwk(z)= λw(z).
</p>
<p>If we define α by λ = e2πiα , then w(z0 + rei(θ+2π)) = e2πiαw(z). Now
we write f (z) &equiv; (z &minus; z0)&minus;αw(z). Following the argument used in Exam-
ple 15.1.1, we get f (z0 + rei(θ+2π)) = f (z); that is, f (z) is single-valued
around z0. We thus have the following theorem.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.2 Complex SOLDEs 463
</p>
<p>Theorem 15.1.2 Any homogeneous NOLDE with analytic coefficient func-
tions in r1 &lt; |z&minus; z0|&lt; r2 admits a solution of the form
</p>
<p>w(z)= (z&minus; z0)αf (z)
</p>
<p>where f (z) is single-valued around z0 in r1 &lt; |z&minus; z0|&lt; r2.
</p>
<p>An isolated singular point z0 near which an analytic function w(z) can be
written as w(z)= (z&minus; z0)αf (z), where f (z) is single-valued and analytic
in the punctured neighborhood of z0, is called a simple branch point of simple branch point
w(z). The arguments leading to Theorem 15.1.2 imply that a solution with
a simple branch point exists if and only if the vector C whose components
appear in w(z) is an eigenvector of At , the transpose of the circuit matrix.
Thus, there are as many solutions with simple branch points as there are
linearly independent eigenvectors of At .
</p>
<p>15.2 Complex SOLDEs
</p>
<p>Let us now consider the SOLDE w&prime;&prime; + p(z)w&prime; + q(z)w = 0. Given two
linearly independent solutions w1(z) and w2(z), we form the 2 &times; 2 matrix
A and try to diagonalize it. There are three possible outcomes:
</p>
<p>1. The matrix A is diagonalizable, and we can find two eigenvectors, F(z)
and G(z), corresponding, respectively, to two distinct eigenvalues, λ1
and λ2. This means that
</p>
<p>F
(
z0 + rei(θ+2π)
</p>
<p>)
= λ1F(z), and G
</p>
<p>(
z0 + rei(θ+2π)
</p>
<p>)
= λ2G(z).
</p>
<p>Defining λ1 = e2πiα and λ2 = e2πiβ , we get
</p>
<p>F(z)= (z&minus; z0)αf (z) and G(z)= (z&minus; z0)βg(z),
</p>
<p>as Theorem 15.1.2 suggests. The set {F(z),G(z)} is called a canonical canonical basis of the
SOLDEbasis of the SOLDE.
</p>
<p>2. The matrix A is diagonalizable, and the two eigenvalues are the same.
In this case both F(z) and G(z) have the same constant α:
</p>
<p>F(z)= (z&minus; z0)αf (z) and G(z)= (z&minus; z0)αg(z).
</p>
<p>3. We cannot find two eigenvectors. This corresponds to the case where A
is not diagonalizable. However, we can always find one eigenvector, so
A has only one eigenvalue, λ. We let w1(z) be the solution of the form
(z &minus; z0)αf (z), where f (z) is single-valued and λ = e2πiα . The exis-
tence of such a solution is guaranteed by Theorem 15.1.2. Let w2(z) be
any other linearly independent solution (Theorem 14.3.5 ensures the
existence of such a second solution). Then
</p>
<p>w2
(
z0 + rei(θ+2π)
</p>
<p>)
= aw1(z)+ bw2(z),</p>
<p/>
</div>
<div class="page"><p/>
<p>464 15 Complex Analysis of SOLDEs
</p>
<p>and the circuit matrix will be A =
(
λ 0
a b
</p>
<p>)
, which has eigenvalues λ and b.
</p>
<p>Since A is assumed to have only one eigenvalue (otherwise we would
have the first outcome again), we must have b = λ. This reduces A
to A =
</p>
<p>(
λ 0
a λ
</p>
<p>)
, where a �= 0. The condition a �= 0 is necessary to distin-
</p>
<p>guish this case from the second outcome. Now we analytically continue
h(z)&equiv;w2(z)/w1(z) one whole turn around z0, obtaining
</p>
<p>h
(
z0 + rei(θ+2π)
</p>
<p>)
= w2(z0 + re
</p>
<p>i(θ+2π))
w1(z0 + rei(θ+2π))
</p>
<p>= aw1(z)+ λw2(z)
λw1(z)
</p>
<p>= a
λ
+ w2(z)
</p>
<p>w1(z)
= a
</p>
<p>λ
+ h(z).
</p>
<p>It then follows that the function1
</p>
<p>g1(z)&equiv; h(z)&minus;
a
</p>
<p>2πiλ
ln(z&minus; z0)
</p>
<p>is single-valued in r1 &lt; |z&minus; z0|&lt; r2. If we redefine g1(z) and w2(z) as
(2πiλ/a)g1(z) and (2πiλ/a)w2(z), respectively, we have the follow-
ing:
</p>
<p>Theorem 15.2.1 If p(z) and q(z) are analytic for r1 &lt; |z&minus; z0|&lt; r2, then
the SOLDE w&prime;&prime; + p(z)w&prime; + q(z)w = 0 admits a basis of solutions {w1,w2}
in the neighborhood of the singular point z0, where either
</p>
<p>w1(z)= (z&minus; z0)αf (z), w2(z)= (z&minus; z0)βg(z)
</p>
<p>or, in exceptional cases (when the circuit matrix is not diagonalizable),
</p>
<p>w1(z)= (z&minus; z0)αf (z), w2(z)=w1(z)
[
g1(z)+ ln(z&minus; z0)
</p>
<p>]
.
</p>
<p>The functions f (z), g(z), and g1(z) are analytic and single-valued in the
annular region.
</p>
<p>This theorem allows us to factor out the branch point z0 from the rest
of the solutions. However, even though f (z), g(z), and g1(z) are analytic
in the annular region r1 &lt; |z &minus; z0| &lt; r2, they may very well have poles
of arbitrary orders at z0. Can we also factor out the poles? In general, we
cannot; however, under special circumstances, described in the following
definition, we can.
</p>
<p>Definition 15.2.2 A SOLDE w&prime;&prime; + p(z)w&prime; + q(z)w = 0 that is analytic in
0 &lt; |z&minus; z0|&lt; r is said to have a regular singular point at z0 if p(z) has atregular singular point of
</p>
<p>a SOLDE defined worst a simple pole and q(z) has at worst a pole of order 2 there.
</p>
<p>1Recall that ln(z&minus; z0) increases by 2πi for each turn around z0.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.2 Complex SOLDEs 465
</p>
<p>In a neighborhood of a regular singular point z0, the coefficient functions
p(z) and q(z) have the power-series expansions
</p>
<p>p(z)= a&minus;1
z&minus; z0
</p>
<p>+
&infin;&sum;
</p>
<p>k=0
ak(z&minus; z0)k,
</p>
<p>q(z)= b&minus;2
(z&minus; z0)2
</p>
<p>+ b&minus;1
z&minus; z0
</p>
<p>+
&infin;&sum;
</p>
<p>k=0
bk(z&minus; z0)k.
</p>
<p>Multiplying both sides of the first equation by z &minus; z0 and the second by
(z&minus; z0)2 and introducing P(z) &equiv; (z&minus; z0)p(z), Q(z) &equiv; (z&minus; z0)2q(z), we
obtain
</p>
<p>P(z)=
&infin;&sum;
</p>
<p>k=0
ak&minus;1(z&minus; z0)k, Q(z)=
</p>
<p>&infin;&sum;
</p>
<p>k=0
bk&minus;2(z&minus; z0)k .
</p>
<p>It is also convenient to multiply the SOLDE by (z&minus; z0)2 and write it as
</p>
<p>(z&minus; z0)2w&prime;&prime; + (z&minus; z0)P (z)w&prime; +Q(z)w = 0. (15.5)
</p>
<p>Inspired by the discussion leading to Theorem 15.2.1, we write
</p>
<p>w(z)= (z&minus; z0)ν
&infin;&sum;
</p>
<p>k=0
Ck(z&minus; z0)k, C0 = 1, (15.6)
</p>
<p>where we have chosen the arbitrary multiplicative constant in such a way
that C0 = 1. Substitute this in Eq. (15.5), and change the dummy variable&mdash;
so that all sums start at 0&mdash;to obtain
</p>
<p>&infin;&sum;
</p>
<p>n=0
</p>
<p>{
(n+ ν)(n+ ν &minus; 1)Cn +
</p>
<p>n&sum;
</p>
<p>k=0
</p>
<p>[
(k + ν)an&minus;k&minus;1 + bn&minus;k&minus;2
</p>
<p>]
Ck
</p>
<p>}
</p>
<p>&times; (z&minus; z0)n+ν = 0,
</p>
<p>which results in the recursion relation
</p>
<p>(n+ ν)(n+ ν &minus; 1)Cn =&minus;
n&sum;
</p>
<p>k=0
</p>
<p>[
(k + ν)an&minus;k&minus;1 + bn&minus;k&minus;2
</p>
<p>]
Ck. (15.7)
</p>
<p>For n = 0, this leads to what is known as the indicial equation for the
exponent ν:
</p>
<p>I (ν)&equiv; ν(ν &minus; 1)+ a&minus;1ν + b&minus;2 = 0. (15.8)
The roots of this equation are called the characteristic exponents of z0, and
</p>
<p>indicial equation, indicial
</p>
<p>polynomial,
</p>
<p>characteristic exponents
I (ν) is called its indicial polynomial. In terms of this polynomial, (15.7)
can be expressed as
</p>
<p>I (n+ ν)Cn =&minus;
n&minus;1&sum;
</p>
<p>k=0
</p>
<p>[
(k + ν)an&minus;k&minus;1 + bn&minus;k&minus;2
</p>
<p>]
Ck for n= 1,2, . . . .
</p>
<p>(15.9)</p>
<p/>
</div>
<div class="page"><p/>
<p>466 15 Complex Analysis of SOLDEs
</p>
<p>Equation (15.8) determines what values of ν are possible, and Eq. (15.9)
gives C1, C2, C3, . . . , which in turn determine w(z). Special care must be
taken if the indicial polynomial vanishes at n+ ν for some positive integer
n, that is, if n + ν, in addition to ν, is a root of the indicial polynomial:
I (n+ ν)= 0 = I (ν).
</p>
<p>If ν1 and ν2 are characteristic exponents of the indicial equation and
Re(ν1) &gt; Re(ν2), then a solution for ν1 always exists. A solution for ν2
also exists if ν1 &minus; ν2 �= n for any (positive) integer n. In particular, if z0 is
an ordinary point [a point at which both p(z) and q(z) are analytic], then
only one solution is determined by (15.9). (Why?) The foregoing discussion
is summarized in the following:
</p>
<p>Theorem 15.2.3 If the differential equation w&prime;&prime; +p(z)w&prime; + q(z)w = 0 has
a regular singular point at z= z0, then at least one power series of the form
of (15.6) formally solves the equation. If ν1 and ν2 are the characteristic
exponents of z0, then there are two linearly independent formal solutions
unless ν1 &minus; ν2 is an integer.
</p>
<p>Example 15.2.4 Let us consider some familiar differential equations.
</p>
<p>(a) The Bessel equation is
</p>
<p>w&prime;&prime; + 1
z
w&prime; +
</p>
<p>(
1 &minus; α
</p>
<p>2
</p>
<p>z2
</p>
<p>)
w = 0.
</p>
<p>In this case, the origin is a regular singular point, a&minus;1 = 1, and b&minus;2 =
&minus;α2. Thus, the indicial equation is ν(ν &minus; 1) + ν &minus; α2 = 0, and its
solutions are ν1 = α and ν2 = &minus;α. Therefore, there are two linearly
independent solutions to the Bessel equation unless ν1 &minus;ν2 = 2α is an
integer, i.e., unless α is either an integer or a half-integer.
</p>
<p>(b) For the Coulomb potential f (r) = β/r , the radial equation (13.14)
reduces to
</p>
<p>w&prime;&prime; + 2
z
w&prime; +
</p>
<p>(
β
</p>
<p>z
&minus; α
</p>
<p>z2
</p>
<p>)
w = 0.
</p>
<p>The point z= 0 is a regular singular point at which a&minus;1 = 2 and b&minus;2 =
&minus;α. The indicial polynomial is I (ν)= ν2 + ν &minus; α with characteristic
exponents
</p>
<p>ν1 =&minus;
1
</p>
<p>2
+ 1
</p>
<p>2
</p>
<p>&radic;
1 + 4α and ν2 =&minus;
</p>
<p>1
</p>
<p>2
&minus; 1
</p>
<p>2
</p>
<p>&radic;
1 + 4α.
</p>
<p>There are two independent solutions unless ν1 &minus; ν2 =
&radic;
</p>
<p>1 + 4α is an
integer. In practice, α = l(l+1), where l is some integer; so ν1 &minus; ν2 =
2l + 1, and only one solution is obtained.
</p>
<p>(c) The hypergeometric differential equation ishypergeometric
differential equation
</p>
<p>w&prime;&prime; + γ &minus; (α + β + 1)z
z(1 &minus; z) w
</p>
<p>&prime; &minus; αβ
z(1 &minus; z)w = 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.2 Complex SOLDEs 467
</p>
<p>A substantial number of functions in mathematical physics are solu-
tions of this remarkable equation, with appropriate values for α, β ,
and γ . The regular singular points2 are z = 0 and z = 1. At z = 0,
a&minus;1 = γ and b&minus;2 = 0. The indicial polynomial is I (ν)= ν(ν+γ &minus;1),
whose roots are ν1 = 0 and ν2 = 1&minus;γ . Unless γ is an integer, we have
two formal solutions.
</p>
<p>It is shown in differential equation theory [Birk 78, pp. 40&ndash;242] that as
long as ν1 &minus; ν2 is not an integer, the series solution of Theorem 15.2.3 is
convergent for a neighborhood of z0. What happens when ν1 &minus; ν2 is an
integer? First, as a convenience, we translate the coordinate axes so that the
point z0 coincides with the origin. This will save us some writing, because
instead of powers of z &minus; z0, we will have powers of z. Next we let ν1 =
ν2 + n with n a positive integer. Then, since it is impossible to encounter
any new zero of the indicial polynomial beyond ν1, the recursion relation,
Eq. (15.9), will be valid for all values of n, and we obtain a solution:
</p>
<p>w1(z)= zν1f (z)= zν1
(
</p>
<p>1 +
&infin;&sum;
</p>
<p>k=1
Ckz
</p>
<p>k
</p>
<p>)
,
</p>
<p>which is convergent in the region 0 &lt; |z|&lt; r for some r &gt; 0.
To investigate the nature and the possibility of the second solution, write
</p>
<p>the recursion relations of Eq. (15.9) for the smaller characteristic root ν2:
</p>
<p>I (ν2 + 1)C1 =
&equiv;ρ1I (ν2+1)︷ ︸︸ ︷
</p>
<p>&minus;(ν2a0 + b&minus;1)C0 &rArr; C1 = ρ1,
I (ν2 + 2)C2 =&minus;(ν2a1 + b0)C0 &minus;
</p>
<p>[
(ν2 + 1)a0 + b&minus;1
</p>
<p>]
C1 &rArr; C2 &equiv; ρ2,
</p>
<p>...
</p>
<p>I (ν2 + n&minus; 1)Cn&minus;1 &equiv; ρn&minus;1I (ν2 + n&minus; 1)C0 &rArr; Cn&minus;1 = ρn&minus;1,
I (ν2 + n)Cn = I (ν1)Cn = ρnC0 &rArr; 0 = ρn,
</p>
<p>(15.10)
where in each step, we have used the result of the previous step in which
Ck is given as a multiple of C0 = 1. Here, the ρ&rsquo;s are constants depending
(possibly in a very complicated way) on the ak&rsquo;s and bk&rsquo;s.
</p>
<p>Theorem 15.2.3 guarantees two power series solutions only when ν1 &minus;ν2
is not an integer. When ν1 &minus; ν2 is an integer, Eq. (15.10) shows that a nec-
essary condition for a second power series solution to exist is that ρn = 0.
Therefore, when ρn �= 0, we have to resort to other means of obtaining the
second solution.
</p>
<p>Let us define the second solution as
</p>
<p>w2(z)&equiv;w1(z)h(z)=
&equiv;w1(z)︷ ︸︸ ︷
zν1f (z)h(z) (15.11)
</p>
<p>2The coefficient of w need not have a pole of order 2. Its pole can be of order one as well.</p>
<p/>
</div>
<div class="page"><p/>
<p>468 15 Complex Analysis of SOLDEs
</p>
<p>and substitute in the SOLDE to obtain a FOLDE in h&prime;, namely,
</p>
<p>h&prime;&prime; +
(
p+ 2w&prime;1/w1
</p>
<p>)
h&prime; = 0,
</p>
<p>or, by substituting w&prime;1/w1 = ν1/z+ f &prime;/f , the equivalent FOLDE
</p>
<p>h&prime;&prime; +
(
</p>
<p>2ν1
z
</p>
<p>+ 2f
&prime;
</p>
<p>f
+ p
</p>
<p>)
h&prime; = 0. (15.12)
</p>
<p>Lemma 15.2.5 The coefficient of h&prime; in Eq. (15.12) has a residue of n+ 1.
</p>
<p>Proof Recall that the residue of a function is the coefficient of z&minus;1 in the
Laurent expansion of the function (about z= 0). Let us denote this residue
for the coefficient of h&prime; by A&minus;1. Since f (0)= 1, the ratio f &prime;/f is analytic
at z = 0. Thus, the simple pole at z = 0 comes from the other two terms.
Substituting the Laurent expansion of p(z) gives
</p>
<p>2ν1
z
</p>
<p>+ p = 2ν1
z
</p>
<p>+ a&minus;1
z
</p>
<p>+ a0 + a1z+ &middot; &middot; &middot; .
</p>
<p>This shows that A&minus;1 = 2ν1 + a&minus;1. On the other hand, comparing the two
versions of the indicial polynomial
</p>
<p>ν2 + (a&minus;1 &minus; 1)ν + b&minus;2 and (ν &minus; ν1)(ν &minus; ν2)= ν2 &minus; (ν1 + ν2)ν + ν1ν2
</p>
<p>gives
</p>
<p>ν1 + ν2 =&minus;(a&minus;1 &minus; 1), or 2ν1 &minus; n=&minus;(a&minus;1 &minus; 1).
Therefore, A&minus;1 = 2ν1 + a&minus;1 = n+ 1. �
</p>
<p>Theorem 15.2.6 Suppose that the characteristic exponents of a SOLDE
with a regular singular point at z= 0 are ν1 and ν2. Consider three cases:
1. ν1 &minus; ν2 is not an integer.
2. ν2 = ν1 &minus; n where n is a nonnegative integer, and ρn, as defined in
</p>
<p>Eq. (15.10), vanishes.
3. ν2 = ν1 &minus; n where n is a nonnegative integer, and ρn, as defined in
</p>
<p>Eq. (15.10), does not vanish.
</p>
<p>Then, in the first two cases, there exists a basis of solutions {w1,w2} of the
form
</p>
<p>wi(z)= zνi
(
</p>
<p>1 +
&infin;&sum;
</p>
<p>k=1
C
(i)
k z
</p>
<p>k
</p>
<p>)
, i = 1,2,
</p>
<p>and in the third case, the basis of solutions takes the form
</p>
<p>w1(z)= zν1
(
</p>
<p>1+
&infin;&sum;
</p>
<p>k=1
akz
</p>
<p>k
</p>
<p>)
, w2(z)= zν2
</p>
<p>(
1+
</p>
<p>&infin;&sum;
</p>
<p>k=1
bkz
</p>
<p>k
</p>
<p>)
+Cw1(z) ln z,
</p>
<p>where the power series are convergent in a neighborhood of z= 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.3 Fuchsian Differential Equations 469
</p>
<p>Proof The first two cases have been shown before. For the third case, we
use Lemma 15.2.5 and write
</p>
<p>2ν1
z
</p>
<p>+ 2f
&prime;
</p>
<p>f
+ p = n+ 1
</p>
<p>z
+
</p>
<p>&infin;&sum;
</p>
<p>k=0
ckz
</p>
<p>k,
</p>
<p>and the solution for the FOLDE in h&prime; will be [see Eq. (15.3) and the discus-
sion preceding it]
</p>
<p>h&prime;(z)= z&minus;n&minus;1
(
</p>
<p>1 +
&infin;&sum;
</p>
<p>k=1
bkz
</p>
<p>k
</p>
<p>)
.
</p>
<p>For n= 0, i.e., when the indicial polynomial has a double root, this yields
</p>
<p>h&prime;(z)= 1/z+
&infin;&sum;
</p>
<p>k=1
bkz
</p>
<p>k&minus;1 &rArr; h(z)= ln z+ g1(z),
</p>
<p>where g1 is analytic in a neighborhood of z= 0. For n �= 0, we have h&prime;(z)=
bn/z+
</p>
<p>&sum;&infin;
k �=n bkz
</p>
<p>k&minus;n&minus;1 and, by integration,
</p>
<p>h(z)= bn ln z+
&infin;&sum;
</p>
<p>k �=n
</p>
<p>bk
</p>
<p>k &minus; nz
k&minus;n
</p>
<p>= bn ln z+ z&minus;n
&infin;&sum;
</p>
<p>k �=n
</p>
<p>bk
</p>
<p>k &minus; nz
k = bn ln z+ z&minus;ng2(z),
</p>
<p>where g2 is analytic in a neighborhood of z = 0. Substituting h in
Eq. (15.11) and recalling that ν2 = ν1 &minus; n, we obtain the desired results
of the theorem. �
</p>
<p>15.3 Fuchsian Differential Equations
</p>
<p>In many cases of physical interest, the behavior of the solution of a
SOLDE at infinity is important. For instance, bound state solutions of the
Schr&ouml;dinger equation describing the probability amplitudes of particles in
quantum mechanics must tend to zero as the distance from the center of the
binding force increases.
</p>
<p>We have seen that the behavior of a solution is determined by the be-
havior of the coefficient functions. To determine the behavior at infinity, we
substitute z= 1/t in the SOLDE
</p>
<p>d2w
</p>
<p>dz2
+ p(z)dw
</p>
<p>dz
+ q(z)w = 0 (15.13)
</p>
<p>and obtain
</p>
<p>d2v
</p>
<p>dt2
+
[
</p>
<p>2
</p>
<p>t
&minus; 1
</p>
<p>t2
r(t)
</p>
<p>]
dv
</p>
<p>dt
+ 1
</p>
<p>t4
s(t)v = 0, (15.14)</p>
<p/>
</div>
<div class="page"><p/>
<p>470 15 Complex Analysis of SOLDEs
</p>
<p>where v(t)=w(1/t), r(t)= p(1/t), and s(t)= q(1/t).
Clearly, as z &rarr; &infin;, t &rarr; 0. Thus, we are interested in the behavior of
</p>
<p>(15.14) at t = 0. We assume that both r(t) and s(t) are analytic at t = 0.
Equation (15.14) shows, however, that the solution v(t) may still have sin-
gularities at t = 0 because of the extra terms appearing in the coefficient
functions.
</p>
<p>We assume that infinity is a regular singular point of (15.13), by which
we mean that t = 0 is a regular singular point of (15.14). Therefore, in the
Taylor expansions of r(t) and s(t), the first (constant) term of r(t) and the
first two terms of s(t) must be zero. Thus, we write
</p>
<p>r(t)= a1t + a2t2 + &middot; &middot; &middot; =
&infin;&sum;
</p>
<p>k=1
akt
</p>
<p>k,
</p>
<p>s(t)= b2t2 + b3t3 + &middot; &middot; &middot; =
&infin;&sum;
</p>
<p>k=2
bkt
</p>
<p>k.
</p>
<p>By their definitions, these two equations imply that for p(z) and q(z), and
for large values of |z|, we must have expressions of the form
</p>
<p>p(z)= a1
z
</p>
<p>+ a2
z2
</p>
<p>+ &middot; &middot; &middot; =
&infin;&sum;
</p>
<p>k=1
</p>
<p>ak
</p>
<p>zk
,
</p>
<p>q(z)= b2
z2
</p>
<p>+ b3
z3
</p>
<p>+ &middot; &middot; &middot; =
&infin;&sum;
</p>
<p>k=2
</p>
<p>bk
</p>
<p>zk
.
</p>
<p>(15.15)
</p>
<p>When infinity is a regular singular point of Eq. (15.13), or, equiva-
lently, when the origin is a regular singular point of (15.14), it follows
from Theorem 15.2.6 that there exists at least one solution of the form
v1(t)= tα(1 +
</p>
<p>&sum;&infin;
k=1 Ckt
</p>
<p>k) or, in terms of z,
</p>
<p>w1(z)= z&minus;α
(
</p>
<p>1 +
&infin;&sum;
</p>
<p>k=1
</p>
<p>Ck
</p>
<p>zk
</p>
<p>)
. (15.16)
</p>
<p>Here α is a characteristic exponents at t = 0 of (15.14), whose indicial poly-
nomial is easily found to be α(α &minus; 1)+ (2 &minus; a1)α + b2 = 0.
</p>
<p>Definition 15.3.1 A homogeneous differential equation with single-valued
analytic coefficient functions is called a Fuchsian differential equationFuchsian DE (or FDE)
(FDE) if it has only regular singular points in the extended complex plane,
i.e., the complex plane including the point at infinity.
</p>
<p>It turns out that a particular kind of FDE describes a large class of nonele-
mentary functions encountered in mathematical physics. Therefore, it is in-
structive to classify various kinds of FDEs. A fact that is used in such a
classification is that complex functions whose only singularities in the ex-
tended complex plane are poles are rational functions, i.e., ratios of polyno-
mials (see Proposition 11.2.2). We thus expect FDEs to have only rational
functions as coefficients.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.3 Fuchsian Differential Equations 471
</p>
<p>Consider the case where the equation has at most two regular singular A second-order Fuchsian
DE with two regular
</p>
<p>singular points leads to
</p>
<p>uninteresting solutions!
</p>
<p>points at z1 and z2. We introduce a new variable ξ(z) = z&minus;z1z&minus;z2 . The regular
singular points at z1 and z2 are mapped onto the points ξ1 = ξ(z1)= 0 and
ξ2 = ξ(z2) = &infin;, respectively, in the extended ξ -plane. Equation (15.13)
becomes
</p>
<p>d2u
</p>
<p>dξ2
+Φ(ξ)du
</p>
<p>dξ
+Θ(ξ)u= 0, (15.17)
</p>
<p>where u, Φ , and Θ are functions of ξ obtained when z is expressed in terms
of ξ in w(z), p(z), and q(z), respectively. From Eq. (15.15) and the fact that
ξ = 0 is at most a simple pole of Φ(ξ), we obtain Φ(ξ)= a1/ξ . Similarly,
Θ(ξ)= b2/ξ2. Thus, a SOFDE with two regular singular points is equiva-
lent to the DE w&prime;&prime;+ (a1/z)w&prime;+ (b2/z2)w = 0. Multiplying both sides by z2,
we obtain z2w&prime;&prime;+ a1zw&prime;+ b2w = 0, which is the second-order Euler differ-
ential equation. A general nth-order Euler differential equation is equivalent
to a NOLDE with constant coefficients (see Problem 14.30). Thus, a second
order Fuchsian DE (SOFDE) with two regular singular points is equivalent
to a SOLDE with constant coefficients and produces nothing new.
</p>
<p>The simplest SOFDE whose solutions may include nonelementary func- A second-order Fuchsian
DE with three regular
</p>
<p>singular points leads to
</p>
<p>interesting solutions!
</p>
<p>tions is therefore one having three regular singular points, at say z1, z2, and
z3. By the transformation
</p>
<p>ξ(z)= (z&minus; z1)(z3 &minus; z2)
(z&minus; z2)(z3 &minus; z1)
</p>
<p>we can map z1, z2, and z3 onto ξ1 = 0, ξ2 = &infin;, and ξ3 = 1. Thus, we
assume that the three regular singular points are at z= 0, z= 1, and z=&infin;.
It can be shown [see Problem (15.8)] that the most general p(z) and q(z)
are
</p>
<p>p(z)= A1
z
</p>
<p>+ B1
z&minus; 1 and q(z)=
</p>
<p>A2
</p>
<p>z2
+ B2
</p>
<p>(z&minus; 1)2 &minus;
A3
</p>
<p>z(z&minus; 1) .
</p>
<p>We thus have the following:
</p>
<p>Theorem 15.3.2 The most general second order Fuchsian DE with three
regular singular points can be transformed into the form Riemann differential
</p>
<p>equation
</p>
<p>w&prime;&prime; +
(
A1
</p>
<p>z
+ B1
</p>
<p>z&minus; 1
</p>
<p>)
w&prime; +
</p>
<p>[
A2
</p>
<p>z2
+ B2
</p>
<p>(z&minus; 1)2 &minus;
A3
</p>
<p>z(z&minus; 1)
</p>
<p>]
w = 0, (15.18)
</p>
<p>where A1, A2, A3, B1, and B2 are constants. This equation is called the
Riemann differential equation.
</p>
<p>We can write the Riemann DE in terms of pairs of characteristic expo-
nents, (λ1, λ2), (μ1,μ2), and (ν1, ν2), belonging to the singular points 0, 1,
and &infin;, respectively. The indicial equations are easily found to be
</p>
<p>λ2 + (A1 &minus; 1)λ+A2 = 0,
</p>
<p>μ2 + (B1 &minus; 1)μ+B2 = 0,
</p>
<p>ν2 + (1 &minus;A1 &minus;B1)ν +A2 +B2 &minus;A3 = 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>472 15 Complex Analysis of SOLDEs
</p>
<p>By writing the indicial equations as (λ&minus; λ1)(λ&minus; λ2)= 0, and so forth and
comparing coefficients, we can find the following relations:
</p>
<p>A1 = 1 &minus; λ1 &minus; λ2, A2 = λ1λ2,
B1 = 1 &minus;μ1 &minus;μ2, B2 = μ1μ2,
A1 +B1 = ν1 + ν2 + 1, A2 +B2 &minus;A3 = ν1ν2.
</p>
<p>These equations lead easily to the Riemann identityRiemann identity
</p>
<p>λ1 + λ2 +μ1 +μ2 + ν1 + ν2 = 1. (15.19)
</p>
<p>Substituting these results in (15.18) gives the following result.
</p>
<p>Theorem 15.3.3 A second order Fuchsian DE with three regular singular
points in the extended complex plane is equivalent to the Riemann DE,
</p>
<p>w&prime;&prime; +
(
</p>
<p>1 &minus; λ1 &minus; λ2
z
</p>
<p>+ 1 &minus;μ1 &minus;μ2
z&minus; 1
</p>
<p>)
w&prime;
</p>
<p>+
[
λ1λ2
</p>
<p>z2
+ μ1μ2
</p>
<p>(z&minus; 1)2 +
ν1ν2 &minus; λ1λ2 &minus;μ1μ2
</p>
<p>z(z&minus; 1)
</p>
<p>]
w = 0, (15.20)
</p>
<p>which is uniquely determined by the pairs of characteristic exponents at
each singular point. The characteristic exponents satisfy the Riemann iden-
tity, Eq. (15.19).
</p>
<p>The uniqueness of the Riemann DE allows us to derive identities for so-
lutions and reduce the independent parameters of Eq. (15.20) from five to
three. We first note that if w(z) is a solution of the Riemann DE correspond-
ing to (λ1, λ2), (μ1,μ2), and (ν1, ν2), then the function
</p>
<p>v(z)= zλ(z&minus; 1)μw(z)
</p>
<p>has branch points at z= 0,1,&infin; [because w(z) does]; therefore, it is a solu-
tion of the Riemann DE. Its pairs of characteristic exponents are (see Prob-
lem 15.10)
</p>
<p>(λ1 + λ,λ2 + λ), (μ1 +μ,μ2 +μ), (ν1 &minus; λ&minus;μ,ν2 &minus; λ&minus;μ).
</p>
<p>In particular, if we let λ=&minus;λ1 and μ=&minus;μ1, then the pairs reduce to
</p>
<p>(0, λ2 &minus; λ1), (0,μ2 &minus;μ1), (ν1 + λ1 +μ1, ν2 + λ1 +μ1).
</p>
<p>Defining α &equiv; ν1 + λ1 + μ1, β &equiv; ν2 + λ1 + μ1, and γ &equiv; 1 &minus; λ2 + λ1, and
using (15.19), we can write the pairs as
</p>
<p>(0,1 &minus; γ ), (0, γ &minus; α &minus; β), (α,β),
</p>
<p>which yield the third version of the Riemann DE
</p>
<p>w&prime;&prime; +
(
γ
</p>
<p>z
+ 1 &minus; γ + α + β
</p>
<p>z&minus; 1
</p>
<p>)
w&prime; + αβ
</p>
<p>z(z&minus; 1)w = 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.4 The Hypergeometric Function 473
</p>
<p>This important equation is commonly written in the equivalent form
</p>
<p>z(1 &minus; z)w&prime;&prime; +
[
γ &minus; (1 + α + β)z
</p>
<p>]
w&prime; &minus; αβw = 0 (15.21)
</p>
<p>and is called the hypergeometric differential equation (HGDE). We will hypergeometric DE
study this equation next.
</p>
<p>15.4 The Hypergeometric Function
</p>
<p>The two characteristic exponents of Eq. (15.21) at z = 0 are 0 and 1 &minus; γ .
It follows from Theorem 15.2.6 that there exists an analytic solution (cor-
responding to the characteristic exponent 0) at z = 0. Let us denote this
solution, the hypergeometric function, by F(α,β;γ ; z) and write
</p>
<p>F(α,β;γ ; z)=
&infin;&sum;
</p>
<p>k=0
akz
</p>
<p>k where a0 = 1.
</p>
<p>Substituting in the DE, we obtain the recurrence relation
</p>
<p>ak+1 =
(α + k)(β + k)
(k + 1)(γ + k) ak for k &ge; 0.
</p>
<p>These coefficients can be determined successively if γ is neither zero nor a hypergeometric series
negative integer:
</p>
<p>F(α,β;γ ; z)= Ŵ(γ )
Ŵ(α)Ŵ(β)
</p>
<p>&infin;&sum;
</p>
<p>k=0
</p>
<p>Ŵ(α + k)Ŵ(β + k)
Ŵ(k + 1)Ŵ(γ + k) z
</p>
<p>k . (15.22)
</p>
<p>The series in (15.22) is called the hypergeometric series, because it is the
generalization of F(1, β;β; z), which is simply the geometric series.
</p>
<p>We note immediately from (15.22) that
</p>
<p>Box 15.4.1 The hypergeometric series becomes a polynomial if ei-
ther α or β is a negative integer.
</p>
<p>This is because for k &lt; |α| (or k &lt; |β|) both Ŵ(α + k) [or Ŵ(β + k)]
and Ŵ(α) [or Ŵ(β)] have poles that cancel each other. However, Ŵ(α + k)
[or Ŵ(β + k)] becomes finite for k &gt; |α| (or k &gt; |β|), and the pole in Ŵ(α)
[or Ŵ(β)] makes the denominator infinite. Therefore, all terms of the series
(15.22) beyond k = |α| (or k = |β|) will be zero.
</p>
<p>Many of the properties of the hypergeometric function can be obtained
directly from the HGDE (15.21). For instance, differentiating the HGDE
and letting v =w&prime;, we obtain
</p>
<p>z(1 &minus; z)v&prime;&prime; +
[
γ + 1 &minus; (α + β + 3)z
</p>
<p>]
v&prime; &minus; (α + 1)(β + 1)v = 0,</p>
<p/>
</div>
<div class="page"><p/>
<p>474 15 Complex Analysis of SOLDEs
</p>
<p>which shows that F &prime;(α,β;γ ; z)= CF(α+1, β+1;γ +1; z). The constant
C can be determined by differentiating Eq. (15.22), setting z = 0 in the
result, and noting that F(α + 1, β + 1;γ + 1;0)= 1. Then we obtain
</p>
<p>F &prime;(α,β;γ ; z)= αβ
γ
</p>
<p>F(α + 1, β + 1;γ + 1; z). (15.23)
</p>
<p>Now assume that γ �= 1, and make the substitution w = z1&minus;γ u in the
HGDE to obtain3
</p>
<p>z(1 &minus; z)u&prime;&prime; +
[
γ1 &minus; (α1 + β1 + 1)z
</p>
<p>]
u&prime; &minus; α1β1u= 0,
</p>
<p>where α1 = α &minus; γ + 1, β1 = β &minus; γ + 1, and γ1 = 2 &minus; γ . Thus,
</p>
<p>u= F(α &minus; γ + 1, β &minus; γ + 1;2 &minus; γ ; z),
</p>
<p>and u is therefore analytic at z= 0. This leads to an interesting result. Pro-
vided that γ is not an integer, the two functions
</p>
<p>w1(z)&equiv; F(α,β;γ ; z), w2(z)&equiv; z1&minus;γF(α&minus;γ +1, β&minus;γ +1;2&minus;γ ; z)
(15.24)
</p>
<p>form a canonical basis of solutions to the HGDE at z = 0. This follows
from Theorem 15.2.6 and the fact that (0,1 &minus; γ ) are a pair of (different)
characteristic exponents at z= 0.
</p>
<p>Historical Notes
</p>
<p>Johann Carl Friedrich Gauss (1777&ndash;1855) was the greatest of all mathematicians and
</p>
<p>Johann Carl Friedrich
</p>
<p>Gauss 1777&ndash;1855
</p>
<p>perhaps the most richly gifted genius of whom there is any record. He was born in the
city of Brunswick in northern Germany. His exceptional skill with numbers was clear at a
very early age, and in later life he joked that he knew how to count before he could talk. It
is said that Goethe wrote and directed little plays for a puppet theater when he was 6 and
that Mozart composed his first childish minuets when he was 5, but Gauss corrected an
error in his father&rsquo;s payroll accounts at the age of 3. At the age of seven, when he started
elementary school, his teacher was amazed when Gauss summed the integers from 1 to
100 instantly by spotting that the sum was 50 pairs of numbers each pair summing to 101.
His long professional life is so filled with accomplishments that it is impossible to give
a full account of them in the short space available here. All we can do is simply give a
chronology of his almost uncountable discoveries.
</p>
<p>1792&ndash;1794: Gauss reads the works of Newton, Euler, and Lagrange; discovers the
prime number theorem (at the age of 14 or 15); invents the method of
least squares; conceives the Gaussian law of distribution in the theory of
probability.
</p>
<p>1795: (only 18 years old!) Proves that a regular polygon with n sides is con-
structible (by ruler and compass) if and only if n is the product of a power
of 2 and distinct prime numbers of the form pk = 22
</p>
<p>k + 1, and completely
solves the 2000-year old problem of ruler-and-compass construction of
regular polygons. He also discovers the law of quadratic reciprocity.
</p>
<p>1799: Proves the fundamental theorem of algebra in his doctoral dissertation
using the then-mysterious complex numbers with complete confidence.
</p>
<p>1801: Gauss publishes his Disquisitiones Arithmeticae in which he creates the
modern rigorous approach to mathematics; predicts the exact location of
the asteroid Ceres.
</p>
<p>3In the following discussion, α1, β1, and γ1 will represent the parameters of the new DE
satisfied by the new function defined in terms of the old.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.4 The Hypergeometric Function 475
</p>
<p>1807: Becomes professor of astronomy and the director of the new observatory
at G&ouml;ttingen.
</p>
<p>1809: Publishes his second book, Theoria motus corporum coelestium, a major
two-volume treatise on the motion of celestial bodies and the bible of
planetary astronomers for the next 100 years.
</p>
<p>1812: Publishes Disquisitiones generales circa seriem infinitam, a rigorous
treatment of infinite series, and introduces the hypergeometric function
for the first time, for which he uses the notation F(α,β;γ ; z); an essay
on approximate integration.
</p>
<p>1820&ndash;1830: Publishes over 70 papers, including Disquisitiones generales circa super-
ficies curvas, in which he creates the intrinsic differential geometry of
general curved surfaces, the forerunner of Riemannian geometry and the
general theory of relativity. From the 1830s on, Gauss was increasingly
occupied with physics, and he enriched every branch of the subject he
touched. In the theory of surface tension, he developed the fundamen-
tal idea of conservation of energy and solved the earliest problem in the
calculus of variations. In optics, he introduced the concept of the focal
length of a system of lenses. He virtually created the science of geomag-
netism, and in collaboration with his friend and colleague WilhelmWeber
he invented the electromagnetic telegraph. In 1839 Gauss published his
fundamental paper on the general theory of inverse square forces, which
established potential theory as a coherent branch of mathematics and in
which he established the divergence theorem.
</p>
<p>Gauss had many opportunities to leave G&ouml;ttingen, but he refused all offers and remained
there for the rest of his life, living quietly and simply, traveling rarely, and working with
immense energy on a wide variety of problems in mathematics and its applications. Apart
from science and his family&mdash;he married twice and had six children, two of whom em-
igrated to America&mdash;his main interests were history and world literature, international
politics, and public finance. He owned a large library of about 6000 volumes in many lan-
guages, including Greek, Latin, English, French, Russian, Danish, and of course German.
His acuteness in handling his own financial affairs is shown by the fact that although he
started with virtually nothing, he left an estate over a hundred times as great as his average
annual income during the last half of his life.
The foregoing list is the published portion of Gauss&rsquo;s total achievement; the unpublished
and private part is almost equally impressive. His scientific diary, a little booklet of 19
pages, discovered in 1898, extends from 1796 to 1814 and consists of 146 very concise
statements of the results of his investigations, which often occupied him for weeks or
months. These ideas were so abundant and so frequent that he physically did not have
time to publish them. Some of the ideas recorded in this diary:
</p>
<p>1. Cauchy Integral Formula: Gauss discovers it in 1811, 16 years before Cauchy.
2. Non-Euclidean Geometry: After failing to prove Euclid&rsquo;s fifth postulate at the age
</p>
<p>of 15, Gauss came to the conclusion that the Euclidean form of geometry cannot be
the only one possible.
</p>
<p>3. Elliptic Functions: Gauss had found many of the results of Abel and Jacobi (the
two main contributors to the subject) before these men were born. The facts became
known partly through Jacobi himself. His attention was caught by a cryptic passage
in the Disquisitiones, whose meaning can only be understood if one knows some-
thing about elliptic functions. He visited Gauss on several occasions to verify his
suspicions and tell him about his own most recent discoveries, and each time Gauss
pulled 30-year-old manuscripts out of his desk and showed Jacobi what Jacobi had
just shown him. After a week&rsquo;s visit with Gauss in 1840, Jacobi wrote to his brother,
&ldquo;Mathematics would be in a very different position if practical astronomy had not
diverted this colossal genius from his glorious career.&rdquo;
</p>
<p>A possible explanation for not publishing such important ideas is suggested by his com-
ments in a letter to Bolyai: &ldquo;It is not knowledge but the act of learning, not possession but
the act of getting there, which grants the greatest enjoyment. When I have clarified and
exhausted a subject, then I turn away from it in order to go into darkness again.&rdquo; His was
the temperament of an explorer who is reluctant to take the time to write an account of his</p>
<p/>
</div>
<div class="page"><p/>
<p>476 15 Complex Analysis of SOLDEs
</p>
<p>last expedition when he could be starting another. As it was, Gauss wrote a great deal, but
to have published every fundamental discovery he made in a form satisfactory to himself
would have required several long lifetimes.
</p>
<p>A third relation can be obtained by making the substitution w = (1 &minus;
z)γ&minus;α&minus;βu. This leads to a hypergeometric equation for u with α1 = γ &minus; α,
β1 = γ &minus;β , and γ1 = γ . Furthermore, w is analytic at z= 0, and w(0)= 1.
We conclude that w = F(α,β;γ ; z). We therefore have the identity
</p>
<p>F(α,β;γ ; z)= (1 &minus; z)γ&minus;α&minus;βF(γ &minus; α,γ &minus; β;γ ; z). (15.25)
</p>
<p>To obtain the canonical basis at z= 1, we make the substitution t = 1&minus;z,
and note that the result is again the HGDE, with α1 = α, β1 = β , and γ1 =
α + β &minus; γ + 1. It follows from Eq. (15.24) that
</p>
<p>w3(z)&equiv; F(α,β;α + β &minus; γ + 1;1 &minus; z),
</p>
<p>w4(z)&equiv; (1 &minus; z)γ&minus;α&minus;βF(γ &minus; β,γ &minus; α;γ &minus; α &minus; β + 1;1 &minus; z)
(15.26)
</p>
<p>form a canonical basis of solutions to the HGDE at z= 1.
A symmetry of the hypergeometric function that is easily obtained from
</p>
<p>the HGDE is
</p>
<p>F(α,β;γ ; z)= F(β,α;γ ; z). (15.27)
The six functions
</p>
<p>F(α &plusmn; 1, β;γ ; z), F (α,β &plusmn; 1;γ ; z), F (α,β;γ &plusmn; 1; z)
</p>
<p>are called hypergeometric functions contiguous to F(α,β;γ ; z). The dis-
cussion above showed how to obtain the basis of solutions at z = 1 from
the regular solution to the HDE z= 0, F(α,β;γ ; z). We can show that the
basis of solutions at z =&infin; can also be obtained from the hypergeometric
function.
</p>
<p>Equation (15.16) suggests a function of the form
</p>
<p>v(z)= zrF
(
α1, β1;γ1;
</p>
<p>1
</p>
<p>z
</p>
<p>)
&equiv; zrw
</p>
<p>(
1
</p>
<p>z
</p>
<p>)
&rArr; w(z)= zrv
</p>
<p>(
1
</p>
<p>z
</p>
<p>)
, (15.28)
</p>
<p>where r , α1, β1, and γ1 are to be determined. Since w(z) is a solution of the
HGDE, v will satisfy the following DE (see Problem 15.15):
</p>
<p>z(1 &minus; z)v&prime;&prime; +
[
1 &minus; α &minus; β &minus; 2r &minus; (2 &minus; γ &minus; 2r)z
</p>
<p>]
v&prime;
</p>
<p>&minus;
[
r2 &minus; r + rγ &minus; 1
</p>
<p>z
(r + α)(r + β)
</p>
<p>]
v = 0. (15.29)
</p>
<p>This reduces to the HGDE if r =&minus;α or r =&minus;β . For r =&minus;α, the parameters
become α1 = α, β1 = 1 + α &minus; γ , and γ1 = α &minus; β + 1. For r = &minus;β , the
parameters are α1 = β , β1 = 1 + β &minus; γ , and γ1 = β &minus; α+ 1. Thus,
</p>
<p>v1(z)= z&minus;αF
(
α,1 + α &minus; γ ;α&minus; β + 1; 1
</p>
<p>z
</p>
<p>)
,
</p>
<p>v2(z)= z&minus;βF
(
β,1 + β &minus; γ ;β &minus; α + 1; 1
</p>
<p>z
</p>
<p>) (15.30)</p>
<p/>
</div>
<div class="page"><p/>
<p>15.4 The Hypergeometric Function 477
</p>
<p>form a canonical basis of solutions for the HGDE that are valid about
z=&infin;.
</p>
<p>As the preceding discussion suggests, it is possible to obtain many rela-
tions among the hypergeometric functions with different parameters and in-
dependent variables. In fact, the nineteenth-century mathematician Kummer
showed that there are 24 different (but linearly dependent, of course) solu-
tions to the HGDE. These are collectively known as Kummer&rsquo;s solutions, Kummer&rsquo;s solutions
and six of them were derived above. Another important relation (shown in
Problem 15.16) is that
</p>
<p>zα&minus;γ (1 &minus; z)γ&minus;α&minus;βF
(
γ &minus; α,1 &minus; α;1 &minus; α + β; 1
</p>
<p>z
</p>
<p>)
(15.31)
</p>
<p>also solves the HGDE.
Many of the functions that occur in mathematical physics are related to
</p>
<p>the hypergeometric function. Even some of the common elementary func-
tions can be expressed in terms of the hypergeometric function with appro-
priate parameters. For example, when β = γ , we obtain
</p>
<p>F(α,β;β; z)=
&infin;&sum;
</p>
<p>k=0
</p>
<p>Ŵ(α + k)
Ŵ(α)Ŵ(k + 1)z
</p>
<p>k = (1 &minus; z)&minus;α .
</p>
<p>Similarly,
</p>
<p>F
</p>
<p>(
1
</p>
<p>2
,
</p>
<p>1
</p>
<p>2
; 3
</p>
<p>2
; z2
</p>
<p>)
= sin
</p>
<p>&minus;1 z
z
</p>
<p>, and F(1,1;2;&minus;z)= ln(1 + z)
z
</p>
<p>.
</p>
<p>However, the real power of the hypergeometric function is that it encom-
passes almost all of the nonelementary functions encountered in physics.
Let us look briefly at a few of these.
</p>
<p>Jacobi functions are solutions of the DE Jacobi functions
</p>
<p>(
1 &minus; x2
</p>
<p>)d2u
dx2
</p>
<p>+
[
β &minus; α&minus; (α + β + 2)x
</p>
<p>]du
dx
</p>
<p>+ λ(λ+ α + β + 1)u= 0. (15.32)
</p>
<p>Defining x = 1 &minus; 2z changes this DE into the HGDE with parameters α1 =
λ, β1 = λ+ α+ β + 1, and γ1 = 1+ α. The solutions of Eq. (15.32), called
the Jacobi functions of the first kind, are, with appropriate normalization,
</p>
<p>P
(α,β)
λ (z)=
</p>
<p>Ŵ(λ+ α + 1)
Ŵ(λ+ 1)Ŵ(α + 1)F
</p>
<p>(
&minus;λ,λ+ α + β + 1;1 + α; 1 &minus; z
</p>
<p>2
</p>
<p>)
.
</p>
<p>When λ= n, a nonnegative integer, the Jacobi function turns into a poly-
nomial of degree n with the following expansion:
</p>
<p>P (α,β)n (z)=
Ŵ(n+ α+ 1)
</p>
<p>Ŵ(n+ 1)Ŵ(n+ α+ β + 1)
</p>
<p>&times;
n&sum;
</p>
<p>k=0
</p>
<p>Ŵ(n+ α + β + k + 1)
Ŵ(α + k + 1)
</p>
<p>(
z&minus; 1
</p>
<p>2
</p>
<p>)k
.</p>
<p/>
</div>
<div class="page"><p/>
<p>478 15 Complex Analysis of SOLDEs
</p>
<p>These are the Jacobi polynomials discussed in Chap. 8. In fact, the DE satis-
fied by P (α,β)n (x) of Chap. 8 is identical to Eq. (15.32). Note that the trans-
formation x = 1 &minus; 2z translates the points z = 0 and z = 1 to the points
x = 1 and x =&minus;1, respectively. Thus the regular singular points of the Ja-
cobi functions of the first kind are at &plusmn;1 and &infin;.
</p>
<p>A second, linearly independent, solution of Eq. (15.32) is obtained by
using (15.31). These are called the Jacobi functions of the second kind:
</p>
<p>Q
(α,β)
λ (z)=
</p>
<p>2λ+α+βŴ(λ+ α + 1)Ŵ(λ+ β + 1)
Ŵ(2λ+ α + β + 2)(z&minus; 1)λ+α+1(z+ 1)β
</p>
<p>&times; F
(
λ+ α+ 1, λ+ 1;2λ+ α + β + 2; 2
</p>
<p>1 &minus; z
</p>
<p>)
. (15.33)
</p>
<p>Gegenbauer functions, or ultraspherical functions, are special cases ofGegenbauer functions
Jacobi functions for which α = β = μ&minus; 12 . They are defined by
</p>
<p>C
μ
λ (z)=
</p>
<p>Ŵ(λ+ 2μ)
Ŵ(λ+ 1)Ŵ(2μ)F
</p>
<p>(
&minus;λ,λ+ 2μ;μ+ 1
</p>
<p>2
; 1 &minus; z
</p>
<p>2
</p>
<p>)
. (15.34)
</p>
<p>Note the change in the normalization constant. Linearly independent Gegen-
bauer functions &ldquo;of the second kind&rdquo; can be obtained from the Jacobi func-
tions of the second kind by the substitution α = β = μ&minus; 12 .
</p>
<p>Another special case of the Jacobi functions is obtained when α = β =
0. Those obtained from the Jacobi functions of the first kind are called
Legendre functions of the first kind:Legendre functions
</p>
<p>Pλ(z)&equiv; P (0,0)λ (z)= C
1/2
λ = F
</p>
<p>(
&minus;λ,λ+ 1;1; 1 &minus; z
</p>
<p>2
</p>
<p>)
. (15.35)
</p>
<p>Legendre functions of the second kind are obtained from the Jacobi func-
tions of the second kind in a similar way:
</p>
<p>Qλ(z)=
2λŴ2(λ+ 1)
</p>
<p>Ŵ(2λ+ 2)(z&minus; 1)λ+1F
(
λ+ 1, λ+ 1;2λ+ 2; 2
</p>
<p>1 &minus; z
</p>
<p>)
.
</p>
<p>Other functions derived from the Jacobi functions are obtained similarly (see
Chap. 8).
</p>
<p>15.5 Confluent Hypergeometric Functions
</p>
<p>The transformation x = 1 &minus; 2z translates the regular singular points of the
HGDE by a finite amount. Consequently, the new functions still have two
regular singular points, z = &plusmn;1, in the complex plane. In some physical
cases of importance, only the origin, corresponding to r = 0 in spherical
coordinates (typically the location of the source of a central force), is the
singular point. If we want to obtain a differential equation consistent with
such a case, we have to &ldquo;push&rdquo; the singular point z= 1 to infinity. This can</p>
<p/>
</div>
<div class="page"><p/>
<p>15.5 Confluent Hypergeometric Functions 479
</p>
<p>be achieved by making the substitution t = rz in the HGDE and taking the
limit r &rarr;&infin;. The substitution yields
</p>
<p>d2w
</p>
<p>dt2
+
(
γ
</p>
<p>t
+ 1 &minus; γ + α + β
</p>
<p>t &minus; r
</p>
<p>)
dw
</p>
<p>dt
+ αβ
</p>
<p>t(t &minus; r)w = 0. (15.36)
</p>
<p>If we blindly take the limit r &rarr; &infin; with α, β , and γ remaining finite,
Eq. (15.36) reduces to ẅ + (γ /t)ẇ = 0, an elementary FODE in ẇ. To
obtain a nonelementary DE, we need to manipulate the parameters, to let
some of them tend to infinity. We want γ to remain finite, because other-
wise the coefficient of dw/dt will blow up. We therefore let β or α tend
to infinity. The result will be the same either way because α and β appear
symmetrically in the equation. It is customary to let β = r &rarr; &infin;. In that
case, Eq. (15.36) becomes
</p>
<p>d2w
</p>
<p>dt2
+
(
γ
</p>
<p>t
&minus; 1
</p>
<p>)
dw
</p>
<p>dt
&minus; α
</p>
<p>t
w = 0.
</p>
<p>Multiplying by t and changing the independent variable back to z yields confluent
hypergeometric DE
</p>
<p>zw&prime;&prime;(z)+ (γ &minus; z)w&prime;(z)&minus; αw(z)= 0. (15.37)
</p>
<p>This is called the confluent hypergeometric DE (CHGDE).
Since z= 0 is still a regular singular point of the CHGDE, we can obtain
</p>
<p>expansions about that point. The characteristic exponents are 0 and 1 &minus; γ ,
as before. Thus, there is an analytic solution (corresponding to the charac-
teristic exponent 0) to the CHGDE at the origin, which is called the con-
fluent hypergeometric function and denoted by Φ(α;γ ; z). Since z= 0 is
the only possible (finite) singularity of the CHGDE, Φ(α;γ ; z) is an entire
function.
</p>
<p>We can obtain the series expansion of Φ(α;γ ; z) directly from Eq. (15.22)
and the fact that Φ(α;γ ; z)= limβ&rarr;0 F(α,β;γ ; z/β). The result is
</p>
<p>Φ(α;γ ; z)= Ŵ(γ )
Ŵ(α)
</p>
<p>&infin;&sum;
</p>
<p>k=0
</p>
<p>Ŵ(α + k)
Ŵ(k + 1)Ŵ(γ + k)z
</p>
<p>k . (15.38)
</p>
<p>This is called the confluent hypergeometric series. An argument similar to
the one given in the case of the hypergeometric function shows that
</p>
<p>confluent
</p>
<p>hypergeometric function
</p>
<p>and series
</p>
<p>Box 15.5.1 The confluent hypergeometric function Φ(α;γ ; z) re-
duces to a polynomial when α is a negative integer.
</p>
<p>A second solution of the CHGDE can be obtained, as for the HGDE. If
1 &minus; γ is not an integer, then by taking the limit β &rarr;&infin; of Eq. (15.24), we
obtain the second solution z1&minus;γΦ(α &minus; γ + 1,2 &minus; γ ; z). Thus,
</p>
<p>Proposition 15.5.2 Any solution of the CHGDE can be written as a linear
combination of Φ(α;γ ; z) and z1&minus;γΦ(α &minus; γ + 1,2 &minus; γ ; z).</p>
<p/>
</div>
<div class="page"><p/>
<p>480 15 Complex Analysis of SOLDEs
</p>
<p>15.5.1 Hydrogen-Like Atoms
</p>
<p>The time-independent Schr&ouml;dinger equation for a central potential, in units
in which �=m= 1, is &minus; 12&nabla;2Ψ +V (r)Ψ =EΨ . For the case of hydrogen-
like atoms, V (r)=&minus;Ze2/r , where Z is the atomic number, and the equa-
tion reduces to
</p>
<p>&nabla;2Ψ +
(
</p>
<p>2E + 2Ze
2
</p>
<p>r
</p>
<p>)
Ψ = 0.
</p>
<p>The radial part of this equation is given by Eq. (13.14) with f (r) = 2E +
2Ze2/r . Defining u= rR(r), we may write
</p>
<p>d2u
</p>
<p>dr2
+
(
λ+ a
</p>
<p>r
&minus; b
</p>
<p>r2
</p>
<p>)
u= 0, (15.39)
</p>
<p>where λ = 2E, a = 2Ze2, and b = l(l + 1). This equation can be further
simplified by defining r &equiv; kz (k is an arbitrary constant to be determined
later):
</p>
<p>d2u
</p>
<p>dz2
+
(
λk2 + ak
</p>
<p>z
&minus; b
</p>
<p>z2
</p>
<p>)
u= 0.
</p>
<p>Choosing λk2 =&minus; 14 and introducing α &equiv; a/(2
&radic;
&minus;λ) yields
</p>
<p>d2u
</p>
<p>dz2
+
(
&minus;1
</p>
<p>4
+ α
</p>
<p>z
&minus; b
</p>
<p>z2
</p>
<p>)
u= 0.
</p>
<p>Equations of this form can be transformed into the CHGDE by making
the substitution u(z)= zμe&minus;νzf (z). It then follows that
</p>
<p>d2f
</p>
<p>dz2
+
(
</p>
<p>2μ
</p>
<p>z
&minus; 2ν
</p>
<p>)
df
</p>
<p>dz
+
[
&minus;1
</p>
<p>4
+ μ(μ&minus; 1)
</p>
<p>z2
&minus; 2μν
</p>
<p>z
+ α
</p>
<p>z
&minus; b
</p>
<p>z2
+ ν2
</p>
<p>]
f = 0.
</p>
<p>Choosing ν2 = 14 and μ(μ&minus; 1)= b reduces this equation to
</p>
<p>f &prime;&prime; +
(
</p>
<p>2μ
</p>
<p>z
&minus; 2ν
</p>
<p>)
f &prime; &minus; 2μν &minus; α
</p>
<p>z
f = 0,
</p>
<p>which is in the form of (15.37).
On physical grounds, we expect u(z)&rarr; 0 as z&rarr;&infin;.4 Therefore, ν = 12 .quantization of the
</p>
<p>energy of the hydrogen
</p>
<p>atom
</p>
<p>Similarly, with μ(μ &minus; 1) = b = l(l + 1), we obtain the two possibilities
μ = &minus;l and μ = l + 1. Again on physical grounds, we demand that u(0)
be finite (the wave function must not blow up at r = 0). This implies5 that
μ= l + 1. We thus obtain
</p>
<p>f &prime;&prime; +
[
</p>
<p>2(l + 1)
z
</p>
<p>&minus; 1
]
f &prime; &minus; l + 1 &minus; α
</p>
<p>z
f = 0.
</p>
<p>4This is because the volume integral of |Ψ |2 over all space must be finite. The radial part
of this integral is simply the integral of r2R2(r)= u2(r). This latter integral will not be
finite unless u(&infin;)= 0.
5Recall that μ is the exponent of z= r/k.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.5 Confluent Hypergeometric Functions 481
</p>
<p>Multiplying by z gives
</p>
<p>zf &prime;&prime; +
[
2(l + 1)&minus; z
</p>
<p>]
f &prime; &minus; (l + 1 &minus; α)f = 0.
</p>
<p>Comparing this with Eq. (15.37) shows that f is proportional to Φ(l + 1 &minus;
α,2l + 2; z). Thus, the solution of (15.39) can be written as
</p>
<p>u(z)= Czl+1e&minus;z/2Φ(l + 1 &minus; α,2l + 2; z).
</p>
<p>An argument similar to that used in Problem 14.22 will reveal that the
product e&minus;z/2Φ(l + 1 &minus; α,2l + 2; z) will be infinite unless the power se-
ries representing Φ terminates (becomes a polynomial). It follows from
Box 15.5.1 that this will take place if
</p>
<p>l + 1 &minus; α =&minus;N (15.40)
</p>
<p>for some integer N &ge; 0. In that case we obtain the Laguerre polynomials
</p>
<p>L
j
</p>
<p>N &equiv;
Ŵ(N + j + 1)
</p>
<p>Ŵ(N + 1)Ŵ(j + 1)Φ(&minus;N,j + 1; z), where j = 2l + 1.
</p>
<p>Condition (15.40) is the quantization rule for the energy levels of a
hydrogen-like atom. Writing everything in terms of the original parame-
ters and defining n= N + l + 1 yields&mdash;after restoring all the m&rsquo;s and the
�&rsquo;s&mdash;the energy levels of a hydrogen-like atom:
</p>
<p>E =&minus;Z
2me4
</p>
<p>2�2n2
=&minus;Z2
</p>
<p>(
mc2
</p>
<p>2
</p>
<p>)
α2
</p>
<p>1
</p>
<p>n2
,
</p>
<p>where α = e2/(�c)= 1/137 is the fine-structure constant. fine-structure constant
The radial wave functions can now be written as
</p>
<p>Rn,l(r)=
un,l(r)
</p>
<p>r
= Cr le&minus;Zr/(na0)Φ
</p>
<p>(
&minus;n+ l + 1,2l + 2; 2Zr
</p>
<p>na0
</p>
<p>)
,
</p>
<p>where
</p>
<p>a0 = �2/
(
me2
</p>
<p>)
= 0.529 &times; 10&minus;8 cm
</p>
<p>is the Bohr radius. Bohr radius
</p>
<p>Historical Notes
</p>
<p>Friedrich Wilhelm Bessel (1784&ndash;1846) showed no signs of unusual academic ability in
</p>
<p>Friedrich Wilhelm Bessel
</p>
<p>1784&ndash;1846
</p>
<p>school, although he did show a liking for mathematics and physics. He left school intend-
ing to become a merchant&rsquo;s apprentice, a desire that soon materialized with a seven-year
unpaid apprenticeship with a large mercantile firm in Bremen. The young Bessel proved
so adept at accounting and calculation that he was granted a small salary, with raises,
after only the first year. An interest in foreign trade led Bessel to study geography and
languages at night, astonishingly learning to read and write English in only three months.
He also studied navigation in order to qualify as a cargo officer aboard ship, but his in-
nate curiosity soon compelled him to investigate astronomy at a more fundamental level.
Still serving his apprenticeship, Bessel learned to observe the positions of stars with suf-
ficient accuracy to determine the longitude of Bremen, checking his results against pro-
fessional astronomical journals. He then tackled the more formidable problem of deter-
mining the orbit of Halley&rsquo;s comet from published observations. After seeing the close</p>
<p/>
</div>
<div class="page"><p/>
<p>482 15 Complex Analysis of SOLDEs
</p>
<p>agreement between Bessel&rsquo;s calculations and those of Halley, the German astronomer Ol-
bers encouraged Bessel to improve his already impressive work with more observations.
The improved calculations, an achievement tantamount to a modern doctoral dissertation,
were published with Olbers&rsquo;s recommendation. Bessel later received appointments with
increasing authority at observatories near Bremen and in K&ouml;nigsberg, the latter position
being accompanied by a professorship. (The title of doctor, required for the professorship,
was granted by the University of G&ouml;ttingen on the recommendation of Gauss.)
Bessel proved himself an excellent observational astronomer. His careful measurements
coupled with his mathematical aptitude allowed him to produce accurate positions for a
number of previously mapped stars, taking account of instrumental effects, atmospheric
refraction, and the position and motion of the observation site. In 1820 he determined the
position of the vernal equinox accurate to 0.01 second, in agreement with modern values.
His observation of the variation of the proper motion of the stars Sirius and Procyon led
him to posit the existence of nearby, large, low-luminosity stars called dark companions.
Between 1821 and 1833 he catalogued the positions of about 75,000 stars, publishing his
measurements in detail. One of his most important contributions to astronomy was the
determination of the distance to a star using parallax. This method uses triangulation, or
the determination of the apparent positions of a distant object viewed from two points a
known distance apart, in this case two diametrically opposed points of the Earth&rsquo;s orbit.
The angle subtended by the baseline of Earth&rsquo;s orbit, viewed from the star&rsquo;s perspective,
is known as the star&rsquo;s parallax. Before Bessel&rsquo;s measurement, stars were assumed to be
so distant that their parallaxes were too small to measure, and it was further assumed
that bright stars (thought to be nearer) would have the largest parallax. Bessel correctly
reasoned that stars with large proper motions were more likely to be nearby ones and
selected such a star, 61 Cygni, for his historic measurement. His measured parallax for
that star differs by less than 8 % from the currently accepted value.
Given such an impressive record in astronomy, it seems only fitting that the famous func-
tions that bear Bessel&rsquo;s name grew out of his investigations of perturbations in planetary
systems. He showed that such perturbations could be divided into two effects and treated
separately: the obvious direct attraction due to the perturbing planet and an indirect ef-
fect caused by the sun&rsquo;s response to the perturber&rsquo;s force. The so-called Bessel functions
then appear as coefficients in the series treatment of the indirect perturbation. Although
special cases of Bessel functions were discovered by Bernoulli, Euler, and Lagrange, the
systematic treatment by Bessel clearly established his preeminence, a fitting tribute to the
creator of the most famous functions in mathematical physics.
</p>
<p>15.5.2 Bessel Functions
</p>
<p>The Bessel differential equation is usually written asBessel differential
equation
</p>
<p>w&prime;&prime; + 1
z
w&prime; +
</p>
<p>(
1 &minus; ν
</p>
<p>2
</p>
<p>z2
</p>
<p>)
w = 0. (15.41)
</p>
<p>As in the example above, the substitution w = zμe&minus;ηzf (z) transforms
(15.41) into
</p>
<p>d2f
</p>
<p>dz2
+
(
</p>
<p>2μ+ 1
z
</p>
<p>&minus; 2η
)
df
</p>
<p>dz
+
[
μ2 &minus; ν2
</p>
<p>z2
&minus; η(2μ+ 1)
</p>
<p>z
+ η2 + 1
</p>
<p>]
f = 0,
</p>
<p>which, if we set μ= ν and η= i, reduces to
</p>
<p>f &prime;&prime; +
(
</p>
<p>2ν + 1
z
</p>
<p>&minus; 2i
)
f &prime; &minus; (2ν + 1)i
</p>
<p>z
f = 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.5 Confluent Hypergeometric Functions 483
</p>
<p>Making the further substitution 2iz= t , and multiplying out by t , we obtain
</p>
<p>t
d2f
</p>
<p>dt2
+ (2ν + 1 &minus; t)df
</p>
<p>dt
&minus;
(
ν + 1
</p>
<p>2
</p>
<p>)
f = 0,
</p>
<p>which is in the form of (15.37) with α = ν + 12 and γ = 2ν + 1.
Thus, solutions of the Bessel equation, Eq. (15.41), can be written as con-
</p>
<p>stant multiples of zνe&minus;izΦ(ν + 12 ,2ν + 1;2iz). With proper normalization,
we define the Bessel function of the first kind of order ν as Bessel function of the
</p>
<p>first kind
Jν(z)=
</p>
<p>1
</p>
<p>Ŵ(ν + 1)
</p>
<p>(
z
</p>
<p>2
</p>
<p>)ν
e&minus;izΦ
</p>
<p>(
ν + 1
</p>
<p>2
,2ν + 1;2iz
</p>
<p>)
. (15.42)
</p>
<p>Using Eq. (15.38) and the expansion for e&minus;iz, we can show that
</p>
<p>Jν(z)=
(
z
</p>
<p>2
</p>
<p>)ν &infin;&sum;
</p>
<p>k=0
</p>
<p>(&minus;1)k
k!Ŵ(ν + k+ 1)
</p>
<p>(
z
</p>
<p>2
</p>
<p>)2k
. (15.43)
</p>
<p>The second linearly independent solution can be obtained as usual and is
proportional to
</p>
<p>z1&minus;(2ν+1)
(
z
</p>
<p>2
</p>
<p>)ν
e&minus;izΦ
</p>
<p>(
ν + 1
</p>
<p>2
&minus; (2ν + 1)+ 1,2 &minus; (2ν + 1);2iz
</p>
<p>)
</p>
<p>= C
(
z
</p>
<p>2
</p>
<p>)&minus;ν
e&minus;izΦ
</p>
<p>(
&minus;ν + 1
</p>
<p>2
,&minus;2ν + 1;2iz
</p>
<p>)
= CJ&minus;ν(z),
</p>
<p>provided that 1 &minus; γ = 1 &minus; (2ν + 1) = &minus;2ν is not an integer. When ν is
an integer, J&minus;n(z) = (&minus;1)nJn(z) (see Problem 15.25). Thus, when ν is a
noninteger, the most general solution is of the form AJν(z)+BJ&minus;ν(z).
</p>
<p>How do we find a second linearly independent solution when ν is an
integer n? We first define
</p>
<p>Yν(z)=
Jν(z) cos νπ &minus; J&minus;ν(z)
</p>
<p>sinνπ
, (15.44)
</p>
<p>called the Bessel function of the second kind, or the Neumann function. Bessel function of the
second kind, or
</p>
<p>Neumann function
</p>
<p>For noninteger ν this is simply a linear combination of the two linearly in-
dependent solutions. For integer ν the function is indeterminate. Therefore,
we use l&rsquo;H&ocirc;pital&rsquo;s rule and define
</p>
<p>Yn(z)&equiv; lim
ν&rarr;n
</p>
<p>Yν(z)=
1
</p>
<p>π
lim
ν&rarr;n
</p>
<p>[
&part;Jν
</p>
<p>&part;ν
&minus; (&minus;1)n &part;J&minus;ν
</p>
<p>&part;ν
</p>
<p>]
.
</p>
<p>Equation (15.43) yields
</p>
<p>&part;Jν
</p>
<p>&part;ν
= Jν(z) ln
</p>
<p>(
z
</p>
<p>2
</p>
<p>)
&minus;
(
z
</p>
<p>2
</p>
<p>)ν &infin;&sum;
</p>
<p>k=0
(&minus;1)k Ψ (ν + k + 1)
</p>
<p>k!Ŵ(ν + k + 1)
</p>
<p>(
z
</p>
<p>2
</p>
<p>)2k
,
</p>
<p>where Ψ (z)= (d/dz) lnŴ(z). Similarly,
</p>
<p>&part;J&minus;ν
&part;ν
</p>
<p>=&minus;J&minus;ν(z) ln
(
z
</p>
<p>2
</p>
<p>)
+
(
z
</p>
<p>2
</p>
<p>)&minus;ν &infin;&sum;
</p>
<p>k=0
</p>
<p>Ψ (&minus;ν + k + 1)
k!Ŵ(&minus;ν + k + 1)
</p>
<p>(
z
</p>
<p>2
</p>
<p>)2k
.</p>
<p/>
</div>
<div class="page"><p/>
<p>484 15 Complex Analysis of SOLDEs
</p>
<p>Substituting these expressions in the definition of Yn(z) and using J&minus;n(z)=
(&minus;1)nJn(z), we obtain
</p>
<p>Yn(z)=
2
</p>
<p>π
Jn(z) ln
</p>
<p>(
z
</p>
<p>2
</p>
<p>)
&minus; 1
</p>
<p>π
</p>
<p>(
z
</p>
<p>2
</p>
<p>)n &infin;&sum;
</p>
<p>k=0
(&minus;1)k Ψ (n+ k + 1)
</p>
<p>k!Ŵ(n+ k + 1)
</p>
<p>(
z
</p>
<p>2
</p>
<p>)2k
</p>
<p>&minus; 1
π
</p>
<p>(
z
</p>
<p>2
</p>
<p>)&minus;n
(&minus;1)n
</p>
<p>&infin;&sum;
</p>
<p>k=0
(&minus;1)k Ψ (k &minus; n+ 1)
</p>
<p>k!Ŵ(k &minus; n+ 1)
</p>
<p>(
z
</p>
<p>2
</p>
<p>)2k
. (15.45)
</p>
<p>The natural log term is indicative of the solution suggested by Theo-
rem 15.2.6. Since Yν(z) is linearly independent of Jν(z) for any ν, integer or
noninteger, it is convenient to consider {Jν(z), Yν(z)} as a basis of solutions
for the Bessel equation.
</p>
<p>Another basis of solutions is defined asBessel function of the
third kind or Hankel
</p>
<p>function H (1)ν (z)= Jν(z)+ iYν(z), H (2)ν (z)= Jν(z)&minus; iYν(z), (15.46)
</p>
<p>which are called Bessel functions of the third kind, or Hankel functions.
Replacing z by iz in the Bessel equation yields
</p>
<p>d2w
</p>
<p>dz2
+ 1
</p>
<p>z
</p>
<p>dw
</p>
<p>dz
&minus;
(
</p>
<p>1 + ν
2
</p>
<p>z2
</p>
<p>)
w = 0,
</p>
<p>whose basis of solutions consists of multiples of Jν(iz) and J&minus;ν(iz). Thus,
the modified Bessel functions of the first kind are defined asmodified Bessel
</p>
<p>functions
</p>
<p>Iν(z)&equiv; e&minus;iπν/2Jν(iz)=
(
z
</p>
<p>2
</p>
<p>)ν &infin;&sum;
</p>
<p>k=0
</p>
<p>1
</p>
<p>k!Ŵ(ν + k + 1)
</p>
<p>(
z
</p>
<p>2
</p>
<p>)2k
.
</p>
<p>Similarly, the modified Bessel functions of the second kind are defined as
</p>
<p>Kν(z)=
π
</p>
<p>2 sinνπ
</p>
<p>[
I&minus;ν(z)&minus; Iν(z)
</p>
<p>]
.
</p>
<p>When ν is an integer n, In = I&minus;n, and Kn is indeterminate. Thus, we define
Kn(z) as limν&rarr;nKν(z). This gives
</p>
<p>Kn(z)=
(&minus;1)n
</p>
<p>2
lim
ν&rarr;n
</p>
<p>[
&part;I&minus;ν
&part;ν
</p>
<p>&minus; &part;Iν
&part;ν
</p>
<p>]
,
</p>
<p>which has the power-series representation
</p>
<p>Kn(z)= (&minus;1)n+1In(z) ln
(
z
</p>
<p>2
</p>
<p>)
+ 1
</p>
<p>2
(&minus;1)n
</p>
<p>(
z
</p>
<p>2
</p>
<p>)n &infin;&sum;
</p>
<p>k=0
</p>
<p>Ψ (n+ k+ 1)
k!Ŵ(n+ k + 1)
</p>
<p>(
z
</p>
<p>2
</p>
<p>)2k
</p>
<p>+ 1
2
(&minus;1)n
</p>
<p>(
z
</p>
<p>2
</p>
<p>)&minus;n &infin;&sum;
</p>
<p>k=0
</p>
<p>Ψ (k &minus; n+ 1)
k!Ŵ(k &minus; n+ 1)
</p>
<p>(
z
</p>
<p>2
</p>
<p>)2k
.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.6 Problems 485
</p>
<p>We can obtain a recurrence relation for solutions of the Bessel equation recurrence relation for
solutions of the Bessel
</p>
<p>DE
</p>
<p>as follows. If Zν(z) is a solution of order ν, then (see Problem 15.28)
</p>
<p>Zν+1 = C1zν
d
</p>
<p>dz
</p>
<p>[
z&minus;νZν(z)
</p>
<p>]
and Zν&minus;1 = C2z&minus;ν
</p>
<p>d
</p>
<p>dz
</p>
<p>[
zνZν(z)
</p>
<p>]
.
</p>
<p>If the constants are chosen in such a way that Zν , Z&minus;ν , Zν+1, and Zν&minus;1 sat-
isfy their appropriate series expansions, then C1 =&minus;1 and C2 = 1. Carrying
out the differentiation in the equations for Zν+1 and Zν&minus;1, we obtain
</p>
<p>Zν+1 =
ν
</p>
<p>z
Zν &minus;
</p>
<p>dZν
</p>
<p>dz
, Zν&minus;1 =
</p>
<p>ν
</p>
<p>z
Zν +
</p>
<p>dZν
</p>
<p>dz
. (15.47)
</p>
<p>Adding these two equations yields the recursion relation
</p>
<p>Zν&minus;1(z)+Zν+1(z)=
2ν
</p>
<p>z
Zν(z), (15.48)
</p>
<p>where Zν(z) can be any of the three kinds of Bessel functions.
</p>
<p>15.6 Problems
</p>
<p>15.1 Show that the solution of w&prime; + w/z2 = 0 has an essential singularity
at z= 0.
</p>
<p>15.2 Derive the recursion relation of Eq. (15.7) and express it in terms of
the indicial polynomial, as in Eq. (15.9).
</p>
<p>15.3 Find the characteristic exponent associated with the solution of
</p>
<p>w&prime;&prime; + p(z)w&prime; + q(z)w = 0
</p>
<p>at an ordinary point [a point at which p(z) and q(z) have no poles]. How
many solutions can you find?
</p>
<p>15.4 The Laplace equation in electrostatics when separated in spherical co-
ordinates yields a DE in the radial coordinate given by
</p>
<p>d
</p>
<p>dx
</p>
<p>(
x2
</p>
<p>dy
</p>
<p>dx
</p>
<p>)
&minus; n(n+ 1)y = 0 for n&ge; 0.
</p>
<p>Starting with an infinite series of the form (15.6), show that the two inde-
pendent solutions of this ODE are of the form xn and x&minus;n&minus;1.
</p>
<p>15.5 Find the indicial polynomial, characteristic exponents, and recursion
relation at both of the regular singular points of the Legendre equation,
</p>
<p>w&prime;&prime; &minus; 2z
1 &minus; z2w
</p>
<p>&prime; + α
1 &minus; z2w = 0.
</p>
<p>What is ak , the coefficient of the Laurent expansion, for the point z=+1?</p>
<p/>
</div>
<div class="page"><p/>
<p>486 15 Complex Analysis of SOLDEs
</p>
<p>15.6 Show that the substitution z = 1/t transforms Eq. (15.13) into
Eq. (15.14).
</p>
<p>15.7 Obtain the indicial polynomial of Eq. (15.14) for expansion about
t = 0.
</p>
<p>15.8 Show that Riemann DE represents the most general second order
Fuchsian DE.
</p>
<p>15.9 Derive the indicial equation for the Riemann DE.
</p>
<p>15.10 Show that the transformation v(z) = zλ(z &minus; 1)μw(z) changes the
pairs of characteristic exponents (λ1, λ2), (μ1,μ2), and (ν1, ν2) for the Rie-
mann DE to (λ1+λ,λ2+λ), (μ1+μ,μ2+μ), and (ν1&minus;λ&minus;μ,ν2&minus;λ&minus;μ).
</p>
<p>15.11 Go through the steps leading to Eqs. (15.24), (15.25), and (15.26).
</p>
<p>15.12 Show that the elliptic function of the first kind, defined as
</p>
<p>K(z)=
&int; π/2
</p>
<p>0
</p>
<p>dθ&radic;
1 &minus; z2 sin2 θ
</p>
<p>,
</p>
<p>can be expressed as (π/2)F ( 12 ,
1
2 ;1; z2).
</p>
<p>15.13 By differentiating the hypergeometric series, show that
</p>
<p>dn
</p>
<p>dzn
F(α,β;γ ; z)= Ŵ(α + n)Ŵ(β + n)Ŵ(γ )
</p>
<p>Ŵ(α)Ŵ(β)Ŵ(γ + n) F (α + n,β + n;γ + n; z).
</p>
<p>15.14 Use direct substitution in the hypergeometric series to show that
</p>
<p>F(&minus;α,β;β;&minus;z)= (1 + z)α, F
(
</p>
<p>1
</p>
<p>2
,
</p>
<p>1
</p>
<p>2
; 3
</p>
<p>2
; z2
</p>
<p>)
= 1
</p>
<p>z
sin&minus;1 z,
</p>
<p>F (1,1;2;&minus;z)= 1
z
</p>
<p>ln(1 + z).
</p>
<p>15.15 Show that the substitution v(z) = zrw(1/z) [see Eq. (15.28)] trans-
forms the HGDE into Eq. (15.29).
</p>
<p>15.16 Consider the function v(z) &equiv; zr (1 &minus; z)sF(α1, β1;γ1;1/z) and as-
sume that it is a solution of HGDE. Find a relation among r , s, α1, β1, and
γ1 such that v(z) is written in terms of three parameters rather than five. In
particular, show that one possibility is
</p>
<p>v(z)= zα&minus;γ (1 &minus; z)γ&minus;α&minus;βF(γ &minus; α,1 &minus; α;1 + β &minus; α;1/z).
</p>
<p>Find all such possibilities.
</p>
<p>15.17 Show that the Jacobi functions are related to the hypergeometric
functions.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.6 Problems 487
</p>
<p>15.18 Derive the expression for the Jacobi function of the second kind as
given in Eq. (15.33).
</p>
<p>15.19 Show that z=&infin; is not a regular singular point of the CHGDE.
</p>
<p>15.20 Derive the confluent hypergeometric series from hypergeometric se-
ries.
</p>
<p>15.21 Show that the Weber-Hermite equation, u&prime;&prime; + (ν + 12 &minus; 14z2)u= 0 Weber-Hermite equation
can be transformed into the CHGDE. Hint: Make the substitution u(z) =
exp(&minus; 14z2)v(z).
</p>
<p>15.22 The linear combination
</p>
<p>Ψ (α,γ ; z)&equiv; Ŵ(1 &minus; γ )
Ŵ(α &minus; γ + 1)Φ(α,γ ; z)
</p>
<p>+ Ŵ(γ &minus; 1)
Ŵ(α)
</p>
<p>z1&minus;γΦ(α &minus; γ + 1,2 &minus; γ ; z)
</p>
<p>is also a solution of the CHGDE. Show that the Hermite polynomials can be
written as
</p>
<p>Hn
</p>
<p>(
z&radic;
2
</p>
<p>)
= 2nΨ
</p>
<p>(
&minus;n
</p>
<p>2
,
</p>
<p>1
</p>
<p>2
; z
</p>
<p>2
</p>
<p>2
</p>
<p>)
.
</p>
<p>15.23 Verify that the error function erf(z)=
&int; z
</p>
<p>0 e
&minus;t2dt satisfies the relation
</p>
<p>erf(z)= zΦ( 12 , 32 ;&minus;z2).
</p>
<p>15.24 Derive the series expansion of the Bessel function of the first kind
from that of the confluent hypergeometric series and the expansion of the
exponential. Check your answer by obtaining the same result by substituting
the power series directly in the Bessel DE.
</p>
<p>15.25 Show that J&minus;n(z)= (&minus;1)nJn(z). Hint: Let ν =&minus;n in the expansion
of Jν(z) and use Ŵ(m)=&infin; for a nonpositive integer m.
</p>
<p>15.26 In a potential-free region, the radial part of the Schr&ouml;dinger equation
reduces to
</p>
<p>d2R
</p>
<p>dr2
+ 2
</p>
<p>r
</p>
<p>dR
</p>
<p>dr
+
[
λ&minus; α
</p>
<p>r2
</p>
<p>]
R = 0.
</p>
<p>Write the solutions of this DE in terms of Bessel functions. Hint: Substitute
R = u/&radic;r . These solutions are called spherical Bessel functions. spherical Bessel
</p>
<p>functions
</p>
<p>15.27 Theorem 15.2.6 states that under certain conditions, linearly indepen-
dent solutions of SOLDE at regular singular points exist even though the dif-
ference between the characteristic exponents is an integer. An example is the
case of Bessel functions of half-odd-integer orders. Evaluate the Wronskian
of the two linearly independent solutions, Jν and J&minus;ν , of the Bessel equation</p>
<p/>
</div>
<div class="page"><p/>
<p>488 15 Complex Analysis of SOLDEs
</p>
<p>and show that it vanishes only if ν is an integer. This shows, in particular,
that Jn+1/2 and J&minus;n&minus;1/2 are linearly independent. Hint: Consider the value
of the Wronskian at z= 0, and use the formula Ŵ(ν)Ŵ(1 &minus; ν)= π/ sinνπ .
</p>
<p>15.28 Show that z&plusmn;ν(d/dz)[z∓νZν(z)] is a solution of the Bessel equation
of order ν &plusmn; 1 if Zν is a solution of order ν.
</p>
<p>15.29 Use the recursion relation of Eq. (15.47) to prove that
</p>
<p>(
1
</p>
<p>z
</p>
<p>d
</p>
<p>dz
</p>
<p>)m[
zνZν(z)
</p>
<p>]
= zν&minus;mZν&minus;m(z),
</p>
<p>(
1
</p>
<p>z
</p>
<p>d
</p>
<p>dz
</p>
<p>)m[
z&minus;νZν(z)
</p>
<p>]
= (&minus;1)mz&minus;ν&minus;mZν+m(z).
</p>
<p>15.30 Using the series expansion of the Bessel function, write J1/2(z) and
J&minus;1/2(z) in terms of elementary functions. Hint: First show that
</p>
<p>Ŵ
</p>
<p>(
k+ 3
</p>
<p>2
</p>
<p>)
=&radic;π (2k+ 1)!/
</p>
<p>(
k!22k+1
</p>
<p>)
.
</p>
<p>15.31 From the results of the previous two problems, derive the relations
</p>
<p>J&minus;n&minus;1/2(z)=
&radic;
</p>
<p>2
</p>
<p>π
zn+1/2
</p>
<p>(
1
</p>
<p>z
</p>
<p>d
</p>
<p>dz
</p>
<p>)n(cos z
z
</p>
<p>)
,
</p>
<p>Jn+1/2(z)=
&radic;
</p>
<p>2
</p>
<p>π
zn+1/2
</p>
<p>(
&minus;1
z
</p>
<p>d
</p>
<p>dz
</p>
<p>)n( sin z
z
</p>
<p>)
.
</p>
<p>15.32 Obtain the following integral identities:
</p>
<p>(a)
&int;
</p>
<p>zν+1Jν(z) dz= zν+1Jν+1(z).
</p>
<p>(b)
&int;
</p>
<p>z&minus;ν+1Jν(z) dz=&minus;z&minus;ν+1Jν&minus;1(z).
</p>
<p>(c)
&int;
</p>
<p>zμ+1Jν(z) dz= zμ+1Jν+1(z)+ (μ&minus; ν)zμJν(z)
</p>
<p>&minus;
(
μ2 &minus; ν2
</p>
<p>)&int;
zμ&minus;1Jν(z) dz,
</p>
<p>and evaluate
</p>
<p>(d)
&int;
</p>
<p>z3J0(z) dz.
</p>
<p>Hint: For (c) write zμ+1 = zμ&minus;νzν+1 and use integration by parts.
</p>
<p>15.33 Use Theorem 15.2.6 and the fact that Jn(z) is entire to show that for
integer n, a second solution to the Bessel equation exists and can be written
as Yn(z)= Jn(z)[fn(z)+Cn ln z], where fn(z) is analytic about z= 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.6 Problems 489
</p>
<p>15.34 (a) Show that the Wronskian W(Jν,Z; z) of Jν and any other solu-
tion Z of the Bessel equation, satisfies the equation
</p>
<p>d
</p>
<p>dz
</p>
<p>[
zW(Jν,Z; z)
</p>
<p>]
= 0.
</p>
<p>(b) For some constant A, show that
</p>
<p>d
</p>
<p>dz
</p>
<p>[
Z
</p>
<p>Jν
</p>
<p>]
= W(z)
</p>
<p>J 2ν (z)
= A
</p>
<p>zJ 2ν (z)
.
</p>
<p>(c) Show that the general second solution of the Bessel equation can be
written as
</p>
<p>Zν(z)= Jν(z)
[
B +A
</p>
<p>&int;
dz
</p>
<p>zJ 2ν (z)
</p>
<p>]
.
</p>
<p>15.35 Spherical Bessel functions are defined by
</p>
<p>fl(z)&equiv;
&radic;
π
</p>
<p>2
</p>
<p>(
Zl+1/2(z)&radic;
</p>
<p>z
</p>
<p>)
.
</p>
<p>Let fl(z) denote a spherical Bessel function &ldquo;of some kind.&rdquo; By direct dif-
ferentiation and substitution in the Bessel equation, show that
</p>
<p>(a)
d
</p>
<p>dz
</p>
<p>[
zl+1fl(z)
</p>
<p>]
= zl+1fl&minus;1(z),
</p>
<p>(b)
d
</p>
<p>dz
</p>
<p>[
z&minus;lfl(z)
</p>
<p>]
=&minus;z&minus;lfl+1(z).
</p>
<p>(c) Combine the results of parts (a) and (b) to derive the recursion relations
</p>
<p>fl&minus;1(z)+ fl+1(z)=
2l + 1
</p>
<p>z
fl(z),
</p>
<p>lfl&minus;1(z)&minus; (l + 1)fl+1(z)= (2l + 1)
dfl
</p>
<p>dz
.
</p>
<p>15.36 Show that
</p>
<p>W(Jν, Yν; z)=
2
</p>
<p>πz
, W
</p>
<p>(
H (1)ν ,H
</p>
<p>(2)
ν ; z
</p>
<p>)
= 4
</p>
<p>iπz
.
</p>
<p>Hint: Use Problem 15.34.
</p>
<p>15.37 Verify the following relations:
</p>
<p>(a) Yn+1/2(z)= (&minus;1)n+1J&minus;n&minus;1/2(z), Y&minus;n&minus;1/2(z)= (&minus;1)nJn+1/2(z),
</p>
<p>(b) Y&minus;ν(z)= sinνπJν(z)+ cosνπYν(z)=
Jν(z)&minus; cosνπJ&minus;ν(z)
</p>
<p>sinνπ
,
</p>
<p>(c) Y&minus;n(z)= (&minus;1)nYn(z) in the limit ν &rarr; n in part (b).
</p>
<p>15.38 Use the recurrence relation for the Bessel function to show that
J1(z)=&minus;J &prime;0(z).</p>
<p/>
</div>
<div class="page"><p/>
<p>490 15 Complex Analysis of SOLDEs
</p>
<p>15.39 Let u= Jν(λz) and v = Jν(μz). Multiply the Bessel DE for u by v/z
and that of v by u/z. Subtract the two equations to obtain
</p>
<p>(
λ2 &minus;μ2
</p>
<p>)
zuv = d
</p>
<p>dz
</p>
<p>[
z
</p>
<p>(
u
dv
</p>
<p>dz
&minus; v du
</p>
<p>dz
</p>
<p>)]
.
</p>
<p>(a) Write the above equation in terms of Jν(λz) and Jν(μz) and integrate
both sides with respect to z.
</p>
<p>(b) Now divide both sides by λ2 &minus; μ2 and take the limit as μ&rarr; λ. You
will need to use L&rsquo;H&ocirc;pital&rsquo;s rule.
</p>
<p>(c) Substitute for J &prime;&prime;ν (λz) from the Bessel DE and simplify to get
</p>
<p>&int;
z
[
Jν(λz)
</p>
<p>]2
dz= z
</p>
<p>2
</p>
<p>2
</p>
<p>{[
J &prime;ν(λz)
</p>
<p>]2 +
(
</p>
<p>1 &minus; ν
2
</p>
<p>λ2z2
</p>
<p>)[
Jν(λz)
</p>
<p>]2
}
.
</p>
<p>(d) Finally, let λ = xνn/a, where xνn is the nth root of Jν , and use
Eq. (15.47) to arrive at
</p>
<p>&int; a
</p>
<p>0
zJ 2ν
</p>
<p>(
xνn
</p>
<p>a
z
</p>
<p>)
dz= a
</p>
<p>2
</p>
<p>2
J 2ν+1(xνn).
</p>
<p>15.40 The generating function g(z, t) for Bessel functions of integer order
is
</p>
<p>g(z, t)= exp
[
</p>
<p>1
</p>
<p>2
z(t &minus; 1/t)
</p>
<p>]
.
</p>
<p>To see this, rewrite g(z, t) as ezt/2e&minus;z/2t , expand both factors, and write the
product as powers of tn. Now show that the coefficient of tn is simply Jn(z).
Finally, use J&minus;n(z)= (&minus;1)nJn(z) to derive the formula
</p>
<p>exp
</p>
<p>[
1
</p>
<p>2
z(t &minus; 1/t)
</p>
<p>]
=
</p>
<p>&infin;&sum;
</p>
<p>n=&minus;&infin;
Jn(z)t
</p>
<p>n.
</p>
<p>15.41 Make the substitutions z= βtγ and w = tαu to transform the Bessel
DE into
</p>
<p>t2
d2u
</p>
<p>dt2
+ (2α + 1)t du
</p>
<p>dt
+
(
β2γ 2t2γ + α2 &minus; ν2γ 2
</p>
<p>)
u= 0.
</p>
<p>Now show that Airy&rsquo;s DE, ü&minus; tu= 0, has solutions of the form J1/3( 23 it3/2)Airy&rsquo;s differential
equation and J&minus;1/3( 23 it
</p>
<p>3/2).
</p>
<p>15.42 Show that the general solution of d
2w
</p>
<p>dt2
+ e2t&minus;ν2
</p>
<p>t4
w = 0 is w =
</p>
<p>t[AJν(e1/t )+BYν(e1/t )].
</p>
<p>15.43 Transform dw/dz + w2 + zm = 0 by making the substitution w =
(d/dz) lnv. Now make the further substitutions
</p>
<p>v = u&radic;z and t = 2
m+ 2z
</p>
<p>1+(1/2)m</p>
<p/>
</div>
<div class="page"><p/>
<p>15.6 Problems 491
</p>
<p>to show that the new DE can be transformed into a Bessel equation of order
1/(m+ 2).
</p>
<p>15.44 Starting with the relation
</p>
<p>exp
</p>
<p>[
1
</p>
<p>2
x(t &minus; 1/t)
</p>
<p>]
exp
</p>
<p>[
1
</p>
<p>2
y(t &minus; 1/t)
</p>
<p>]
= exp
</p>
<p>[
1
</p>
<p>2
(x + y)(t &minus; 1/t)
</p>
<p>]
</p>
<p>and the fact that the exponential function is the generating function for
Jn(z), prove the &ldquo;addition theorem&rdquo; for Bessel functions:
</p>
<p>Jn(x + y)=
&infin;&sum;
</p>
<p>k=&minus;&infin;
Jk(x)Jn&minus;k(y).</p>
<p/>
</div>
<div class="page"><p/>
<p>16Integral Transforms and DifferentialEquations
</p>
<p>The discussion in Chap. 15 introduced a general method of solving differen-
tial equations by power series&mdash;also called the Frobenius method&mdash;which
gives a solution that converges within a circle of convergence. In general,
this circle of convergence may be small; however, the function represented
by the power series can be analytically continued using methods presented
in Chap. 12.
</p>
<p>This chapter, which is a bridge between differential equations and opera-
tors on Hilbert spaces (to be developed in the next part), introduces another
method of solving DEs, which uses integral transforms and incorporates
the analytic continuation automatically. The integral transform of a function
v is another function u given by
</p>
<p>u(z)=
&int;
</p>
<p>C
</p>
<p>K(z, t)v(t) dt, (16.1)
</p>
<p>where C is a convenient contour, and K(z, t), called the kernel of the inte-
kernel of integral
</p>
<p>transforms
gral transform, is an appropriate function of two complex variables.
</p>
<p>Example 16.0.1 Let us consider some examples of integral transforms.
examples of integral
</p>
<p>transforms(a) The Fourier transform is familiar from the discussion of Chap. 9.
The kernel is
</p>
<p>K(x,y)= eixy .
(b) The Laplace transform is used frequently in electrical engineering.
</p>
<p>Its kernel is
</p>
<p>K(x,y)= e&minus;xy .
(c) The Euler transform has the kernel
</p>
<p>K(x,y)= (x &minus; y)ν .
</p>
<p>(d) The Mellin transform has the kernel
</p>
<p>K(x,y)=G
(
xy
</p>
<p>)
,
</p>
<p>where G is an arbitrary function. Most of the time K(x,y) is taken to
be simply xy .
</p>
<p>S. Hassani, Mathematical Physics, DOI 10.1007/978-3-319-01195-0_16,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>493</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-01195-0_16">http://dx.doi.org/10.1007/978-3-319-01195-0_16</a></div>
</div>
<div class="page"><p/>
<p>494 16 Integral Transforms and Differential Equations
</p>
<p>(e) The Hankel transform has the kernel
</p>
<p>K(x,y)= yJn(xy),
</p>
<p>where Jn is the nth-order Bessel function.
(f) A transform that is useful in connection with the Bessel equation has
</p>
<p>the kernel
</p>
<p>K(x,y)=
(
x
</p>
<p>2
</p>
<p>)ν
ey&minus;x
</p>
<p>2/4y .
</p>
<p>The idea behind using integral transform is to write the solution u(z)
of a DE in z in terms of an integral such as Eq. (16.1) and choose v and
the kernel in such a way as to render the DE more manageable. Let Lz be
</p>
<p>Strategy for solving DEs
</p>
<p>using integral transforms
a differential operator (DO) in the variable z. We want to determine u(z)
such that Lz[u] = 0, or equivalently, such that
</p>
<p>&int;
C
Lz[K(z, t)]v(t) dt = 0.
</p>
<p>Suppose that we can find Mt , a DO in the variable t , such that Lz[K(z, t)] =
Mt [K(z, t)]. Then the DE becomes
</p>
<p>&int;
C
(Mt [K(z, t)])v(t) dt = 0. If C has a
</p>
<p>and b as initial and final points (a and b may be equal), then the Lagrange
identity [see Eq. (14.23)] yields
</p>
<p>0 = Lz[u] =
&int; b
</p>
<p>a
</p>
<p>K(z, t)M
&dagger;
t
</p>
<p>[
v(t)
</p>
<p>]
dt +Q[K,v]|ba,
</p>
<p>where Q[K,v] is the &ldquo;surface term&rdquo;. If v(t) and the contour C (or a and b)
are chosen in such a way that
</p>
<p>Q[K,v]|ba = 0 and M&dagger;t
[
v(t)
</p>
<p>]
= 0, (16.2)
</p>
<p>the problem is solved. The trick is to find an Mt such that Eq. (16.2) is easier
to solve than the original equation, Lz[u] = 0. This in turn demands a clever
choice of the kernel, K(z, t). This chapter discusses how to solve some com-
mon differential equations of mathematical physics using the general idea
presented above.
</p>
<p>16.1 Integral Representation of the Hypergeometric
Function
</p>
<p>Recall that for the hypergeometric function, the differential operator is
</p>
<p>Lz = z(1 &minus; z)
d2
</p>
<p>dz2
+
[
γ &minus; (α + β + 1)z
</p>
<p>] d
dz
</p>
<p>&minus; αβ.
</p>
<p>For such operators&mdash;whose coefficient functions are polynomials&mdash;the
proper choice for K(z, t) is the Euler kernel, (z &minus; t)s . Applying Lz toEuler kernel
this kernel and rearranging terms, we obtain
</p>
<p>Lz
[
K(z, t)
</p>
<p>]
=
{
z2
[
&minus;s(s &minus; 1)&minus; s(α + β + 1)&minus; αβ
</p>
<p>]
+ z
</p>
<p>[
s(s &minus; 1)+ sγ
</p>
<p>+ st (α + β + 1)+ 2αβt
]
&minus; γ st &minus; αβt2
</p>
<p>}
(z&minus; t)s&minus;2. (16.3)</p>
<p/>
</div>
<div class="page"><p/>
<p>16.1 Integral Representation of the Hypergeometric Function 495
</p>
<p>Note that except for a multiplicative constant, K(z, t) is symmetric in z
and t . This suggests that the general form of Mt may be chosen to be the
same as that of Lz except for the interchange of z and t . If we can manipu-
late the parameters in such a way that Mt becomes simple, then we have a
chance of solving the problem. For instance, if Mt has the form of Lz with
the constant term absent, then the hypergeometric DE effectively reduces to
a FODE (in dv/dt). Let us exploit this possibility.
</p>
<p>The general form of the Mt that we are interested in is
</p>
<p>Mt = p2(t)
d2
</p>
<p>dt2
+ p1(t)
</p>
<p>d
</p>
<p>dt
,
</p>
<p>i.e., with no p0 term. By applying Mt to K(z, t)= (z&minus; t)s and setting the
result equal to the RHS of Eq. (16.3), we obtain
</p>
<p>s(s &minus; 1)p2 &minus; p1sz+ p1st
</p>
<p>= z2
[
&minus;s(s &minus; 1)&minus; s(α + β + 1)&minus; αβ
</p>
<p>]
</p>
<p>+ z
[
s(s &minus; 1)+ sγ + st (α + β + 1)+ 2αβt
</p>
<p>]
&minus; γ st &minus; αβt2,
</p>
<p>for which the coefficients of equal powers of z on both sides must be equal:
</p>
<p>&minus;s(s &minus; 1)&minus; s(α + β + 1)&minus; αβ = 0 &rArr; s =&minus;α or s =&minus;β,
&minus;p1s = s(s &minus; 1)+ sγ + st (α + β + 1)+ 2αβt,
</p>
<p>s(s &minus; 1)p2 + p1st =&minus;γ st &minus; αβt2.
</p>
<p>If we choose s =&minus;α (s =&minus;β leads to an equivalent representation), the co-
efficient functions of Mt will be completely determined. In fact, the second
equation gives p1(t), and the third determines p2(t). We finally obtain
</p>
<p>p1(t)= α + 1 &minus; γ + t (β &minus; α &minus; 1), p2(t)= t &minus; t2,
</p>
<p>and
</p>
<p>Mt =
(
t &minus; t2
</p>
<p>) d2
dt2
</p>
<p>+
[
α+ 1 &minus; γ + t (β &minus; α &minus; 1)
</p>
<p>] d
dt
</p>
<p>, (16.4)
</p>
<p>which, according to Eq. (14.19), yields the following DE for the adjoint:
</p>
<p>M
&dagger;
t [v] =
</p>
<p>d2
</p>
<p>dt2
</p>
<p>[(
t &minus; t2
</p>
<p>)
v
]
&minus; d
</p>
<p>dt
</p>
<p>{[
α&minus; γ + 1 + t (β &minus; α&minus; 1)
</p>
<p>]
v
}
= 0. (16.5)
</p>
<p>The solution to this equation is v(t) = Ctα&minus;γ (t &minus; 1)γ&minus;β&minus;1 (see Prob-
lem 16.5). We also need the surface term, Q[K,v], in the Lagrange identity
(see Problem 16.6 for details):
</p>
<p>Q[K,v](t)= Cαtα&minus;γ+1(t &minus; 1)γ&minus;β(z&minus; t)&minus;α&minus;1.
</p>
<p>Finally, we need a specification of the contour. For different contours
we will get different solutions. The contour chosen must, of course, have
the property that Q[K,v] vanishes as a result of the integration. There are</p>
<p/>
</div>
<div class="page"><p/>
<p>496 16 Integral Transforms and Differential Equations
</p>
<p>two possibilities: Either the contour is closed [a = b in (16.2)] or a �= b but
Q[K,v] takes on the same value at a and at b.
</p>
<p>Let us consider the second of these possibilities. Clearly, Q[K,v](t) van-
ishes at t = 1 if Re(γ ) &gt; Re(β). Also, as t &rarr;&infin;,
</p>
<p>Q[K,v](t)&rarr; (&minus;1)&minus;α&minus;1Cαtα&minus;γ+1tγ&minus;β t&minus;α&minus;1 = (&minus;1)&minus;α&minus;1Cαt&minus;β ,
</p>
<p>which vanishes if Re(β) &gt; 0. We thus take a = 1 and b =&infin;, and assume
that Re(γ ) &gt; Re(β) &gt; 0. It then follows that
</p>
<p>u(z)=
&int; b
</p>
<p>a
</p>
<p>K(z, t)v(t) dt = C&prime;
&int; &infin;
</p>
<p>1
(t &minus; z)&minus;αtα&minus;γ (t &minus; 1)γ&minus;β&minus;1dt. (16.6)
</p>
<p>The constant C&prime; can be determined to be Ŵ(γ )/[Ŵ(β)Ŵ(γ &minus; β)] (see Prob-
lem 16.7). Therefore,
</p>
<p>u(z)&equiv; F(α,β;γ ; z)= Ŵ(γ )
Ŵ(β)Ŵ(γ &minus; β)
</p>
<p>&int; &infin;
</p>
<p>1
(t &minus; z)&minus;αtα&minus;γ (t &minus; 1)γ&minus;β&minus;1dt.
</p>
<p>It is customary to change the variable of integration from t to 1/t . The re-Euler formula for the
hypergeometric function sulting expression is called the Euler formula for the hypergeometric func-
</p>
<p>tion:
</p>
<p>F(α,β;γ ; z)= Ŵ(γ )
Ŵ(β)Ŵ(γ &minus; β)
</p>
<p>&int; 1
</p>
<p>0
(1&minus; tz)&minus;αtβ&minus;1(1&minus; t)γ&minus;β&minus;1dt. (16.7)
</p>
<p>Note that the term (1 &minus; tz)&minus;α in the integral has two branch points in
the z-plane, one at z = 1/t and the other at z =&infin;. Therefore, we cut the
z-plane from z1 = 1/t , a point on the positive real axis, to z2 =&infin;. Since
0 &le; t &le; 1, z1 is somewhere in the interval [1,&infin;). To ensure that the cut is
applicable for all values of t , we take z1 = 1 and cut the plane along the
positive real axis. It follows that Eq. (16.7) is well behaved as long as
</p>
<p>0 &lt; arg(1 &minus; z) &lt; 2π. (16.8)
</p>
<p>We could choose a different contour, which, in general, would lead to a
different solution. The following example illustrates one such choice.
</p>
<p>Example 16.1.1 First note that Q[K,v] vanishes at t = 0 and t = 1 as long
as Re(γ ) &gt; Re(β) and Re(α) &gt; Re(γ )&minus; 1. Hence, we can choose the con-
tour to start at t = 0 and end at t = 1. We then have
</p>
<p>w(z)= C&prime;&prime;
&int; 1
</p>
<p>0
(z&minus; t)&minus;αtα&minus;γ (1 &minus; t)γ&minus;β&minus;1dt
</p>
<p>= C&prime;&prime;z&minus;α
&int; 1
</p>
<p>0
</p>
<p>(
1 &minus; t
</p>
<p>z
</p>
<p>)&minus;α
tα&minus;γ (1 &minus; t)γ&minus;β&minus;1dt. (16.9)
</p>
<p>To see the relation between w(z) and the hypergeometric function, expand
(1 &minus; t/z)&minus;α in the integral to get
</p>
<p>w(z)= C&prime;&prime;z&minus;α
&infin;&sum;
</p>
<p>n=0
</p>
<p>Ŵ(α + n)
Ŵ(α)Ŵ(n+ 1)
</p>
<p>(
1
</p>
<p>z
</p>
<p>)n &int; 1
</p>
<p>0
tα+n&minus;γ (1 &minus; t)γ&minus;β&minus;1dt.
</p>
<p>(16.10)</p>
<p/>
</div>
<div class="page"><p/>
<p>16.1 Integral Representation of the Hypergeometric Function 497
</p>
<p>Now evaluate the integral by changing t to 1/t and using Eqs. (12.19) and
(12.17). This changes the integral to
</p>
<p>&int; &infin;
</p>
<p>1
t&minus;α&minus;n&minus;1+β(t &minus; 1)γ&minus;β&minus;1dt = Ŵ(α + n+ 1 &minus; γ )Ŵ(γ &minus; β)
</p>
<p>Ŵ(α + n+ 1 &minus; β) .
</p>
<p>Substituting this in Eq. (16.10), we obtain
</p>
<p>w(z)= C
&prime;&prime;
</p>
<p>Ŵ(α)
Ŵ(γ &minus; β)z&minus;α
</p>
<p>&infin;&sum;
</p>
<p>n=0
</p>
<p>Ŵ(α + n)Ŵ(α + n+ 1 &minus; γ )
Ŵ(α + n+ 1 &minus; β)Ŵ(n+ 1)
</p>
<p>(
1
</p>
<p>z
</p>
<p>)n
</p>
<p>= C
&prime;&prime;
</p>
<p>Ŵ(α)
Ŵ(γ &minus; β)z&minus;α Ŵ(α)Ŵ(α + 1 &minus; γ )
</p>
<p>Ŵ(α + 1 &minus; β)
&times; F(α,α &minus; γ + 1;α&minus; β + 1;1/z),
</p>
<p>where we have used the hypergeometric series of Chap. 15. Choosing
</p>
<p>C&prime;&prime; = Ŵ(α + 1 &minus; β)
Ŵ(γ &minus; β)Ŵ(α + 1 &minus; γ )
</p>
<p>yields w(z) = z&minus;αF(α,α &minus; γ + 1;α &minus; β + 1;1/z), which is one of the
solutions of the hypergeometric DE [Eq. (15.30)].
</p>
<p>16.1.1 Integral Representation of the Confluent
Hypergeometric Function
</p>
<p>Having obtained the integral representation of the hypergeometric function,
we can readily get the integral representation of the confluent hypergeomet-
ric function by taking the proper limit. It was shown in Chap. 15 that
</p>
<p>Φ(α,γ ; z)= lim
β&rarr;&infin;
</p>
<p>F(α,β;γ ; z/β).
</p>
<p>This suggests taking the limit of Eq. (16.7). The presence of the gamma
functions with β as their arguments complicates things, but on the other
hand, the symmetry of the hypergeometric function can be utilized to our
advantage. Thus, we may write
</p>
<p>Φ(α,γ ; z)= lim
β&rarr;&infin;
</p>
<p>F
</p>
<p>(
α,β;γ ; z
</p>
<p>β
</p>
<p>)
= lim
</p>
<p>β&rarr;&infin;
F
</p>
<p>(
β,α;γ ; z
</p>
<p>β
</p>
<p>)
</p>
<p>= lim
β&rarr;&infin;
</p>
<p>Ŵ(γ )
</p>
<p>Ŵ(α)Ŵ(γ &minus; α)
</p>
<p>&int; 1
</p>
<p>0
</p>
<p>(
1 &minus; tz
</p>
<p>β
</p>
<p>)&minus;β
tα&minus;1(1 &minus; t)γ&minus;α&minus;1dt
</p>
<p>= Ŵ(γ )
Ŵ(α)Ŵ(γ &minus; α)
</p>
<p>&int; 1
</p>
<p>0
ezt tα&minus;1(1 &minus; t)γ&minus;α&minus;1dt (16.11)
</p>
<p>because the limit of the first term in the integral is simply etz. Note that the
condition Re(γ ) &gt; Re(α) &gt; 0 must still hold here.</p>
<p/>
</div>
<div class="page"><p/>
<p>498 16 Integral Transforms and Differential Equations
</p>
<p>Integral transforms are particularly useful in determining the asymptotic
behavior of functions. We shall use them in deriving asymptotic formulas for
Bessel functions later on, and Problem 16.10 derives the asymptotic formula
for the confluent hypergeometric function.
</p>
<p>16.2 Integral Representation of Bessel Functions
</p>
<p>Choosing the kernel, the contour, and the function v(t) that lead to an in-
tegral representation of a function is an art, and the nineteenth century pro-
duced many masters of it. A particularly popular theme in such endeavors
was the Bessel equation and Bessel functions. This section considers the
integral representations of Bessel functions.
</p>
<p>The most effective kernel for the Bessel DE is
</p>
<p>K(z, t)=
(
z
</p>
<p>2
</p>
<p>)ν
exp
</p>
<p>(
t &minus; z
</p>
<p>2
</p>
<p>4t
</p>
<p>)
.
</p>
<p>When the Bessel DO
</p>
<p>Lz &equiv;
d2
</p>
<p>dz2
+ 1
</p>
<p>z
</p>
<p>d
</p>
<p>dz
+
(
</p>
<p>1 &minus; ν
2
</p>
<p>z2
</p>
<p>)
</p>
<p>acts on K(z, t), it yields
</p>
<p>LzK(z, t)=
(
&minus;ν + 1
</p>
<p>t
+ 1 + z
</p>
<p>2
</p>
<p>4t2
</p>
<p>)(
z
</p>
<p>2
</p>
<p>)ν
et&minus;z
</p>
<p>2/4t =
(
d
</p>
<p>dt
&minus; ν + 1
</p>
<p>t
</p>
<p>)
K(z, t).
</p>
<p>Thus, Mt = d/dt &minus; (ν + 1)/t , and Eq. (14.19) gives
</p>
<p>M
&dagger;
t
</p>
<p>[
v(t)
</p>
<p>]
=&minus;dv
</p>
<p>dt
&minus; ν + 1
</p>
<p>t
v = 0,
</p>
<p>whose solution, including the arbitrary constant of integration k, is v(t) =
kt&minus;ν&minus;1. When we substitute this solution and the kernel in the surface term
of the Lagrange identity, Eq. (14.23), we obtain
</p>
<p>Q[K,v](t)= p1K(z, t)v(t)= k
(
z
</p>
<p>2
</p>
<p>)ν
t&minus;ν&minus;1et&minus;z
</p>
<p>2/(4t).
</p>
<p>A contour in the t-plane that ensures the vanishing of Q[K,v] for all values
of v starts at t = &minus;&infin;, comes to the origin, orbits it on an arbitrary circle,
and finally goes back to t =&minus;&infin; (see Fig. 16.1). Such a contour is possible
because of the factor et in the expression for Q[K,v]. We thus can write
</p>
<p>Jν(z)= k
(
z
</p>
<p>2
</p>
<p>)ν &int;
</p>
<p>C
</p>
<p>t&minus;ν&minus;1et&minus;z
2/(4t)dt. (16.12)
</p>
<p>Note that the integrand has a cut along the negative real axis due to the factor
t&minus;ν&minus;1. If ν is an integer, the cut shrinks to a pole at t = 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>16.2 Integral Representation of Bessel Functions 499
</p>
<p>Fig. 16.1 The contour C in the t -plane used in evaluating Jν(z)
</p>
<p>The constant k must be determined in such a way that the above expres-
sion for Jν(z) agrees with the series representation obtained in Chap. 15. It
can be shown (see Problem 16.11) that k = 1/(2πi). Thus, we have
</p>
<p>Jν(z)=
1
</p>
<p>2πi
</p>
<p>(
z
</p>
<p>2
</p>
<p>)ν &int;
</p>
<p>C
</p>
<p>t&minus;ν&minus;1et&minus;z
2/(4t)dt.
</p>
<p>It is more convenient to take the factor (z/2)ν into the integral, introduce
a new integration variable u= 2t/z, and rewrite the preceding equation as
</p>
<p>integral representation
</p>
<p>of Bessel functionJν(z)=
1
</p>
<p>2πi
</p>
<p>&int;
</p>
<p>C
</p>
<p>u&minus;ν&minus;1 exp
[
z
</p>
<p>2
</p>
<p>(
u&minus; 1
</p>
<p>u
</p>
<p>)]
du. (16.13)
</p>
<p>This result is valid as long as Re(zu) &lt; 0 when u&rarr;&minus;&infin; on the negative
real axis; that is, Re(z) must be positive for Eq. (16.13) to work.
</p>
<p>An interesting result can be obtained from Eq. (16.13) when ν is an inte-
ger. In that case the only singularity will be at the origin, so the contour can
be taken to be a circle about the origin. This yields
</p>
<p>Jn(z)=
1
</p>
<p>2πi
</p>
<p>&int;
</p>
<p>C
</p>
<p>u&minus;n&minus;1 exp
[
z
</p>
<p>2
</p>
<p>(
u&minus; 1
</p>
<p>u
</p>
<p>)]
du,
</p>
<p>which is the nth coefficient of the Laurent series expansion of exp[(z/2)(u&minus;
1/u)] about the origin. We thus have this important result:
</p>
<p>Bessel generating
</p>
<p>function
exp
</p>
<p>[
z
</p>
<p>2
</p>
<p>(
t &minus; 1
</p>
<p>t
</p>
<p>)]
=
</p>
<p>&infin;&sum;
</p>
<p>n=&minus;&infin;
Jn(z)t
</p>
<p>n. (16.14)
</p>
<p>The function exp[(z/2)(t &minus; 1/t)] is therefore appropriately called the gen-
erating function for Bessel functions of integer order (see also Prob-
lem 15.40). Equation (16.14) can be useful in deriving relations for such
Bessel functions as the following example shows.
</p>
<p>Example 16.2.1 Let us rewrite the LHS of (16.14) as ezt/2e&minus;z/2t , expand
the exponentials, and collect terms to obtain
</p>
<p>ezt/2e&minus;z/2t =
&infin;&sum;
</p>
<p>m=0
</p>
<p>1
</p>
<p>m!
</p>
<p>(
zt
</p>
<p>2
</p>
<p>)m &infin;&sum;
</p>
<p>n=0
</p>
<p>1
</p>
<p>n!
</p>
<p>(
&minus; z
</p>
<p>2t
</p>
<p>)n
</p>
<p>=
&infin;&sum;
</p>
<p>m=0
</p>
<p>&infin;&sum;
</p>
<p>n=0
</p>
<p>(&minus;1)n
m!n!
</p>
<p>(
z
</p>
<p>2
</p>
<p>)m+n
tm&minus;n.</p>
<p/>
</div>
<div class="page"><p/>
<p>500 16 Integral Transforms and Differential Equations
</p>
<p>If we let m &minus; n = k, change the m summation to k, and note that k goes
from &minus;&infin; to &infin;, we get
</p>
<p>exp
</p>
<p>[
z
</p>
<p>2
</p>
<p>(
t &minus; 1
</p>
<p>t
</p>
<p>)]
=
</p>
<p>&infin;&sum;
</p>
<p>k=&minus;&infin;
</p>
<p>&infin;&sum;
</p>
<p>n=0
</p>
<p>(&minus;1)n
(n+ k)!n!
</p>
<p>(
z
</p>
<p>2
</p>
<p>)2n+k
tk
</p>
<p>=
&infin;&sum;
</p>
<p>k=&minus;&infin;
</p>
<p>[(
z
</p>
<p>2
</p>
<p>)k &infin;&sum;
</p>
<p>n=0
</p>
<p>(&minus;1)n
Ŵ(n+ k + 1)Ŵ(n+ 1)
</p>
<p>(
z
</p>
<p>2
</p>
<p>)2n]
tk.
</p>
<p>Comparing this equation with Eq. (16.14) yields the familiar expansion for
the Bessel function:
</p>
<p>Jk(z)=
(
z
</p>
<p>2
</p>
<p>)k &infin;&sum;
</p>
<p>n=0
</p>
<p>(&minus;1)n
Ŵ(n+ k + 1)Ŵ(n+ 1)
</p>
<p>(
z
</p>
<p>2
</p>
<p>)2n
.
</p>
<p>We can also obtain a recurrence relation for Jn(z). Differentiating both
sides of Eq. (16.14) with respect to t yields
</p>
<p>z
</p>
<p>2
</p>
<p>(
1 + 1
</p>
<p>t2
</p>
<p>)
exp
</p>
<p>[
z
</p>
<p>2
</p>
<p>(
t &minus; 1
</p>
<p>t
</p>
<p>)]
=
</p>
<p>&infin;&sum;
</p>
<p>n=&minus;&infin;
nJn(z)t
</p>
<p>n&minus;1. (16.15)
</p>
<p>Using Eq. (16.14) on the LHS gives
</p>
<p>&infin;&sum;
</p>
<p>n=&minus;&infin;
</p>
<p>(
z
</p>
<p>2
+ z
</p>
<p>2t2
</p>
<p>)
Jn(z)t
</p>
<p>n = z
2
</p>
<p>&infin;&sum;
</p>
<p>n=&minus;&infin;
Jn(z)t
</p>
<p>n + z
2
</p>
<p>&infin;&sum;
</p>
<p>n=&minus;&infin;
Jn(z)t
</p>
<p>n&minus;2
</p>
<p>= z
2
</p>
<p>&infin;&sum;
</p>
<p>n=&minus;&infin;
Jn&minus;1(z)tn&minus;1 +
</p>
<p>z
</p>
<p>2
</p>
<p>&infin;&sum;
</p>
<p>n=&minus;&infin;
Jn+1(z)tn&minus;1,
</p>
<p>(16.16)
</p>
<p>where we substituted n &minus; 1 for n in the first sum and n + 1 for n in the
second. Equating the coefficients of equal powers of t on the LHS and the
RHS of Eqs. (16.15) and (16.16), we get
</p>
<p>nJn(z)=
z
</p>
<p>2
</p>
<p>[
Jn&minus;1(z)+ Jn+1(z)
</p>
<p>]
,
</p>
<p>which was obtained by a different method in Chap. 15 [see Eq. (15.48)].
</p>
<p>We can start with Eq. (16.13) and obtain other integral representations
of Bessel functions by making appropriate substitutions. For instance, we
can let u= ew and assume that the circle of the contour C has unit radius.
The contour C&prime; in the w-plane is determined as follows. Write u= reiθ and
w &equiv; x + iy, so1 reiθ = exeiy yielding r = ex and eiθ = eiy . Along the first
part of C, θ =&minus;π and r goes from &infin; to 1. Thus, along the corresponding
part of C&prime;, y = &minus;π and x goes from &infin; to 0. On the circular part of C,
r = 1 and θ goes from &minus;π to +π . Thus, along the corresponding part of C&prime;,
</p>
<p>1Do not confuse x and y with the real and imaginary parts of z.</p>
<p/>
</div>
<div class="page"><p/>
<p>16.2 Integral Representation of Bessel Functions 501
</p>
<p>Fig. 16.2 The contour C&prime; in the w-plane used in evaluating Jν(z)
</p>
<p>x = 0 and y goes from &minus;π to +π . Finally, on the last part of C&prime;, y = π and
x goes from 0 to &infin;. Therefore, the contour C&prime; in the w-plane is as shown
in Fig. 16.2.
</p>
<p>Substituting u= ew in Eq. (16.13) yields
</p>
<p>Jν(z)=
1
</p>
<p>2πi
</p>
<p>&int;
</p>
<p>C&prime;
ez sinhw&minus;νwdw, Re(z) &gt; 0, (16.17)
</p>
<p>which can be transformed into (see Problem 16.12)
</p>
<p>Jν(z)=
1
</p>
<p>π
</p>
<p>&int; π
</p>
<p>0
cos(νθ &minus; z sin θ) dθ &minus; sinνπ
</p>
<p>π
</p>
<p>&int; &infin;
</p>
<p>0
e&minus;νt&minus;z sinh tdt. (16.18)
</p>
<p>For the special case of integer ν, we obtain
</p>
<p>Jn(z)=
1
</p>
<p>π
</p>
<p>&int; π
</p>
<p>0
cos(nθ &minus; z sin θ) dθ.
</p>
<p>In particular,
</p>
<p>J0(z)=
1
</p>
<p>π
</p>
<p>&int; π
</p>
<p>0
cos(z sin θ) dθ.
</p>
<p>We can use the integral representation for Jν(z) to find the integral rep-
</p>
<p>integral representation
</p>
<p>of Bessel functions of
</p>
<p>integer order
</p>
<p>resentation for Bessel functions of other kinds. For instance, to obtain the
integral representation for the Neumann function Yν(z), we use Eq. (15.44):
</p>
<p>Yν(z)= (cotνπ)Jν(z)&minus;
1
</p>
<p>sinνπ
J&minus;ν(z)
</p>
<p>= cotνπ
π
</p>
<p>&int; π
</p>
<p>0
cos(νθ &minus; z sin θ) dθ &minus; cosνπ
</p>
<p>π
</p>
<p>&int; &infin;
</p>
<p>0
e&minus;νt&minus;z sinh tdt
</p>
<p>&minus; 1
π sinνπ
</p>
<p>&int; π
</p>
<p>0
cos(νθ + z sin θ) dθ &minus; 1
</p>
<p>π
</p>
<p>&int; &infin;
</p>
<p>0
eνt&minus;z sinh tdt
</p>
<p>with Re(z) &gt; 0. Substitute π&minus;θ for θ in the third integral on the RHS. Then
insert the resulting integrals plus Eq. (16.18) in H (1)ν (z)= Jν(z)+ iYν(z) to</p>
<p/>
</div>
<div class="page"><p/>
<p>502 16 Integral Transforms and Differential Equations
</p>
<p>Fig. 16.3 The contour C&prime;&prime; in the w-plane used in evaluating H (1)ν (z)
</p>
<p>obtain
</p>
<p>H (1)ν (z)=
1
</p>
<p>π
</p>
<p>&int; π
</p>
<p>0
ei(z sin θ&minus;νθ)dθ + 1
</p>
<p>iπ
</p>
<p>&int; &infin;
</p>
<p>0
eνt&minus;z sinh tdt
</p>
<p>+ e
&minus;iνπ
</p>
<p>iπ
</p>
<p>&int; &infin;
</p>
<p>0
e&minus;νt&minus;z sinh tdt, Re(z) &gt; 0.
</p>
<p>These integrals can easily be shown to result from integrating along the
contour C&prime;&prime; of Fig. 16.3. Thus, we have
</p>
<p>H (1)ν (z)=
1
</p>
<p>iπ
</p>
<p>&int;
</p>
<p>C&prime;&prime;
ez sinhw&minus;νwdw Re(z) &gt; 0.
</p>
<p>By changing i to &minus;i, we can show that
</p>
<p>H (2)ν (z)=&minus;
1
</p>
<p>iπ
</p>
<p>&int;
</p>
<p>C&prime;&prime;&prime;
ez sinhw&minus;νwdw, Re(z) &gt; 0,
</p>
<p>where C&prime;&prime;&prime; is the mirror image of C&prime;&prime; about the real axis.
</p>
<p>16.2.1 Asymptotic Behavior of Bessel Functions
</p>
<p>As mentioned before, integral representations are particularly useful for de-
termining the asymptotic behavior of functions. For Bessel functions we
can consider two kinds of limits. Assuming that both ν and z = x are real,
we can consider ν &rarr;&infin; or x &rarr;&infin;. First, let us consider the behavior of
Jν(x) of large order. The appropriate method for calculating the asymptotic
form is the method of steepest descent discussed in Chap. 12 for which ν
takes the place of the large parameter α. We use Eq. (16.17) because its
integrand is simpler than that of Eq. (16.13). The form of the integrand in
Eq. (16.17) may want to suggest f (w) = &minus;w and g(w) = ex sinhw . How-
ever, this choice does not allow setting f &prime;(w) equal to zero. To proceed,
therefore, we write the exponent as ν( x
</p>
<p>ν
sinhw&minus;w), and conveniently intro-
</p>
<p>duce x/ν &equiv; 1/ coshw0, with w0 a real number, which we take to be positive.
Substituting this in the equation above, we can read off
</p>
<p>f (w)= sinhw
coshw0
</p>
<p>&minus;w, g(w)= 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>16.2 Integral Representation of Bessel Functions 503
</p>
<p>Fig. 16.4 The contour C0 in the w-plane used in evaluating Jν(z) for large values of ν
</p>
<p>The saddle point is obtained from df/dw = 0 or coshw = coshw0. Thus,
w =&plusmn;w0 + 2inπ , for n= 0,1,2 . . . . Since the contour C&prime; lies in the right
half-plane, we choose w0 as the saddle point. The second derivative f &prime;&prime;(w0)
is simply tanhw0, which is real, making θ2 = 0, and θ1 = π/2 or 3π/2. The
convention of Chap. 12 suggests taking θ1 = π/2 (see Fig. 16.4). The rest is
a matter of substitution. We are interested in the approximation to w up to
the third order in t : w&minus;w0 = b1t+b2t2+b3t3. Using Eqs. (12.31), (12.37),
and (12.38), we can easily find the three coefficients:
</p>
<p>b1 =
&radic;
</p>
<p>2
</p>
<p>|f &prime;&prime;(w0)|1/2
eiθ1 = i
</p>
<p>&radic;
2&radic;
</p>
<p>tanhw0
,
</p>
<p>b2 =
f &prime;&prime;&prime;(w0)
</p>
<p>3|f &prime;&prime;(w0)|2
e4iθ1 = cosh
</p>
<p>2 w0
</p>
<p>3 sinh2 w0
,
</p>
<p>b3 =
{
</p>
<p>5[f &prime;&prime;&prime;(w0)]2
3[f &prime;&prime;(w0)]2
</p>
<p>&minus; f
(4)(w0)
</p>
<p>f &prime;&prime;(w0)
</p>
<p>} &radic;
2e3iθ1
</p>
<p>12|f &prime;&prime;(w0)|3/2
</p>
<p>=&minus;i
&radic;
</p>
<p>2
</p>
<p>12(tanhw0)3/2
</p>
<p>(
5
</p>
<p>3
coth2 w0 &minus; 1
</p>
<p>)
.
</p>
<p>If we substitute the above in Eq. (12.36), we obtain the following asymp-
totic formula valid for ν &rarr;&infin;:
</p>
<p>Jν(x)&asymp;
ex(sinhw0&minus;w0 coshw0)
</p>
<p>(2πx sinhw0)1/2
</p>
<p>[
1 + 1
</p>
<p>8x sinhw0
</p>
<p>(
1 &minus; 5
</p>
<p>3
coth2 w0
</p>
<p>)
+ &middot; &middot; &middot;
</p>
<p>]
,
</p>
<p>where ν is related to w0 via ν = x coshw0.
Let us now consider the asymptotic behavior for large x. It is convenient
</p>
<p>to consider the Hankel functions H (1)ν (x) and H
(2)
ν (x). The contours C&prime;&prime;
</p>
<p>and C&prime;&prime;&prime; involve both the positive and the negative real axis; therefore, it is</p>
<p/>
</div>
<div class="page"><p/>
<p>504 16 Integral Transforms and Differential Equations
</p>
<p>Fig. 16.5 The contour in the w-plane used in evaluating H (1)ν (z) in the limit of large
values of x
</p>
<p>convenient, assuming that x &gt; ν, to write ν = x cosβ so that
</p>
<p>H (1)ν (x)=
1
</p>
<p>iπ
</p>
<p>&int;
</p>
<p>C&prime;&prime;
ex(sinhw&minus;w cosβ)dw.
</p>
<p>The saddle points are given by the solutions to coshw = cosβ , which are
w0 =&plusmn;iβ . Choosing w0 =+iβ , we note that the contour along which
</p>
<p>Im(sinhw&minus;w cosβ)= Im(sinhw0 &minus;w0 cosβ)
</p>
<p>is given by coshu = [sinβ + (v &minus; β) cosβ]/ sinv. This contour is shown
in Fig. 16.5. The rest of the procedure is exactly the same as for Jν(x)
described above. In fact, to obtain the expansion for H (1)ν (x), we simply
replace w0 by iβ . The result is
</p>
<p>H (1)ν (x)
</p>
<p>&asymp;
(
</p>
<p>2
</p>
<p>iπx sinβ
</p>
<p>)1/2
ei(x sinβ&minus;νβ)
</p>
<p>[
1 + 1
</p>
<p>8ix sinβ
</p>
<p>(
1 + 5
</p>
<p>3
cot2 β
</p>
<p>)
+ &middot; &middot; &middot;
</p>
<p>]
.
</p>
<p>When x is much larger than ν, β will be close to π/2, and we have
</p>
<p>H (1)ν (x)&asymp;
&radic;
</p>
<p>2
</p>
<p>πx
ei(x&minus;νπ/2&minus;π/4)
</p>
<p>(
1 + 1
</p>
<p>8ix
</p>
<p>)
,
</p>
<p>which, with 1/x &rarr; 0, is what we obtained in Example 12.5.2.
The other saddle point, at &minus;iβ , gives the other Hankel function, with the
</p>
<p>asymptotic limit
</p>
<p>H (2)ν (x)&asymp;
&radic;
</p>
<p>2
</p>
<p>πx
e&minus;i(x&minus;νπ/2&minus;π/4)
</p>
<p>(
1 &minus; 1
</p>
<p>8ix
</p>
<p>)
</p>
<p>We can now use the expressions for the asymptotic forms of the two Han-
kel functions to write the asymptotic forms of Jν(x) and Yν(x) for large x:
</p>
<p>Jν(x)=
1
</p>
<p>2
</p>
<p>[
H (1)ν (x)+H (2)ν (x)
</p>
<p>]
</p>
<p>&asymp;
&radic;
</p>
<p>2
</p>
<p>πx
</p>
<p>[
cos
</p>
<p>(
x &minus; ν π
</p>
<p>2
&minus; π
</p>
<p>4
</p>
<p>)
+ 1
</p>
<p>8x
sin
</p>
<p>(
x &minus; ν π
</p>
<p>2
&minus; π
</p>
<p>4
</p>
<p>)
+ &middot; &middot; &middot;
</p>
<p>]
,</p>
<p/>
</div>
<div class="page"><p/>
<p>16.3 Problems 505
</p>
<p>Yν(x)=
1
</p>
<p>2i
</p>
<p>[
H (1)ν (x)&minus;H (2)ν (x)
</p>
<p>]
</p>
<p>&asymp;
&radic;
</p>
<p>2
</p>
<p>πx
</p>
<p>[
sin
</p>
<p>(
x &minus; ν π
</p>
<p>2
&minus; π
</p>
<p>4
</p>
<p>)
&minus; 1
</p>
<p>8x
cos
</p>
<p>(
x &minus; ν π
</p>
<p>2
&minus; π
</p>
<p>4
</p>
<p>)
+ &middot; &middot; &middot;
</p>
<p>]
.
</p>
<p>16.3 Problems
</p>
<p>16.1 Use the change of variables k = ln t and ix = ω &minus; α (where k and x
are the common variables used in Fourier transform equations) to show that
the Fourier transform changes into a Mellin transform,
</p>
<p>G(t)= 1
2πi
</p>
<p>&int; i&infin;+α
</p>
<p>&minus;i&infin;+α
F(ω)t&minus;ωdω, where F(ω)=
</p>
<p>&int; &infin;
</p>
<p>0
G(t)tω&minus;1dt.
</p>
<p>16.2 The Laplace transform L[f ] of a function f (t) is defined as
</p>
<p>L[f ](s)&equiv;
&int; &infin;
</p>
<p>0
e&minus;stf (t) dt.
</p>
<p>Show that the Laplace transform of
</p>
<p>(a) f (t)= 1 is 1
s
, where s &gt; 0.
</p>
<p>(b) f (t)= coshωt is s
s2 &minus;ω2 , where s
</p>
<p>2 &gt;ω2.
</p>
<p>(c) f (t)= sinhωt is ω
s2 &minus;ω2 , where s
</p>
<p>2 &gt;ω2.
</p>
<p>(d) f (t)= cosωt is s
s2 +ω2 .
</p>
<p>(e) f (t)= sinωt is ω
s2 +ω2 .
</p>
<p>(f) f (t)= eωt for t &gt; 0, is 1
s &minus;ω, where s &gt; ω.
</p>
<p>(g) f (t)= tn is Ŵ(n+ 1)
sn+1
</p>
<p>, where s &gt; 0, n &gt;&minus;1.
</p>
<p>16.3 Evaluate the integral
</p>
<p>f (t)=
&int; &infin;
</p>
<p>0
</p>
<p>sinωt
</p>
<p>ω
dω
</p>
<p>by finding the Laplace transform and changing the order of integration. Ex-
press the result for both t &gt; 0 and t &lt; 0 in terms of the theta function. (You
will need some results from Problem 16.2.)</p>
<p/>
</div>
<div class="page"><p/>
<p>506 16 Integral Transforms and Differential Equations
</p>
<p>16.4 Show that the Laplace transform of the derivative of a function is given
by L[F &prime;](s)= sL[F ](s)&minus;F(0). Similarly, show that for the second deriva-
tive the transform is
</p>
<p>L
[
F &prime;&prime;
</p>
<p>]
(s)= s2L[F ](s)&minus; sF (0)&minus; F &prime;(0).
</p>
<p>Use these results to solve the differential equation u&prime;&prime;(t)+ ω2u(t)= 0 sub-
ject to the boundary conditions u(0)= a, u&prime;(0)= 0.
</p>
<p>16.5 Solve the DE of Eq. (16.5).
</p>
<p>16.6 Calculate the surface term for the hypergeometric DE.
</p>
<p>16.7 Determine the constant C&prime; in Eq. (16.6), the solution to the hypergeo-
metric DE. Hint: Expand (t &minus; z)&minus;α inside the integral, use Eqs. (12.19) and
(12.17), and compare the ensuing series with the hypergeometric series of
Chap. 15.
</p>
<p>16.8 Derive the Euler formula [Eq. (16.7)].
</p>
<p>16.9 Show that
</p>
<p>F(α,β;γ ;1)= Ŵ(γ )Ŵ(γ &minus; α &minus; β)
Ŵ(γ &minus; α)Ŵ(γ &minus; β) . (16.19)
</p>
<p>Hint: Use Eq. (12.19). Equation (16.19) was obtained by Gauss using only
hypergeometric series.
</p>
<p>16.10 We determine the asymptotic behavior of Φ(α,γ ; z) for z &rarr;&infin; in
this problem. Break up the integral in Eq. (16.11) into two parts, one from
0 to &minus;&infin; and the other from &minus;&infin; to 1. Substitute &minus;t/z for t in the first
integral, and 1 &minus; t/z for t in the second. Assuming that z &rarr;&infin; along the
positive real axis, show that the second integral will dominate, and that
</p>
<p>Φ(α,γ ; z)&rarr; Ŵ(γ )
Ŵ(α)
</p>
<p>zα&minus;γ ez as z&rarr;&infin;.
</p>
<p>16.11 In this problem, we determine the constant k of Eq. (16.12).
</p>
<p>(a) Write the contour integral of Eq. (16.12) for each of the three pieces of
the contour. Note that arg(t)=&minus;π as t comes from &minus;&infin; and arg(t)=
π as t goes to &minus;&infin;. Obtain a real integral from 0 to &infin;.
</p>
<p>(b) Use the relation Ŵ(z)Ŵ(1 &minus; z) = π/ sinπz, obtained in Chap. 12, to
show that
</p>
<p>Ŵ(&minus;z)=&minus; π
Ŵ(z+ 1) sinπz .
</p>
<p>(c) Expand the function exp(z2/4t) in the integral of part (a), and show
that the contour integral reduces to
</p>
<p>&minus;2i sinνπ
&infin;&sum;
</p>
<p>n=0
</p>
<p>(
z
</p>
<p>2
</p>
<p>)2n
Ŵ(&minus;n&minus; ν)
Ŵ(n+ 1) .</p>
<p/>
</div>
<div class="page"><p/>
<p>16.3 Problems 507
</p>
<p>(d) Use the result of part (c) in part (b), and compare the result with the
series expansion of Jν(z) in Chap. 15 to arrive finally at k = 1/(2πi).
</p>
<p>16.12 By integrating along C1, C2, C3, and C4 of Fig. 16.2, derive
Eq. (16.18).
</p>
<p>16.13 By substituting t = exp(iθ) in Eq. (16.14), show that
</p>
<p>eiz sin θ = J0(z)+ 2
&infin;&sum;
</p>
<p>n=1
J2n(z) cos(2nθ)+ 2i
</p>
<p>&infin;&sum;
</p>
<p>n=0
J2n+1(z) sin
</p>
<p>[
(2n+ 1)θ
</p>
<p>]
.
</p>
<p>In particular, show that
</p>
<p>J0(z)=
1
</p>
<p>2π
</p>
<p>&int; 2π
</p>
<p>0
eiz sin θdθ.
</p>
<p>16.14 Derive the integral representations of H (1)ν (x) and H
(2)
ν (x) given in
</p>
<p>Sect. 16.2.</p>
<p/>
</div>
<div class="page"><p/>
<p>Part V
</p>
<p>Operators on Hilbert Spaces</p>
<p/>
</div>
<div class="page"><p/>
<p>17Introductory Operator Theory
</p>
<p>The first two parts of the book dealt almost exclusively with algebraic tech-
niques. The third and fourth part were devoted to analytic methods. In this
introductory chapter, we shall try to unite these two branches of mathematics
to gain insight into the nature of some of the important equations in physics
and their solutions. Let us start with a familiar problem.
</p>
<p>17.1 From Abstract to Integral and Differential Operators
</p>
<p>Let&rsquo;s say we want to solve an abstract operator equation A|u〉 = |v〉 in an
N -dimensional vector space V. To this end, we select a basis B = {|ai〉}Ni=1,
write the equation in matrix form, and solve the resulting system of N lin-
ear equations. This produces the components of the solution |u〉 in B . If
components in another basis B &prime; are desired, they can be obtained using the
similarity transformation connecting the two bases (see Chap. 5).
</p>
<p>There is a standard formal procedure for obtaining the matrix equation.
It is convenient to choose an orthonormal basis B = {|ei〉}Ni=1 for V and
refer all components to this basis. The procedure involves contracting both
sides of the equation with 〈ei | and inserting 1=
</p>
<p>&sum;N
j=1 |ej 〉〈ej | between A
</p>
<p>and |u〉:
N&sum;
</p>
<p>j=1
〈ei |A|ej 〉〈ej |u〉 = 〈ei |v〉 for i = 1,2, . . . ,N,
</p>
<p>or
N&sum;
</p>
<p>j=1
Aijuj = vi for i = 1,2, . . . ,N, (17.1)
</p>
<p>where Aij &equiv; 〈ei |A|ej 〉, uj &equiv; 〈ej |u〉, and vi &equiv; 〈ei |v〉. Equation (17.1) is a
system of N linear equations in N unknowns {uj }Nj=1, which can be solved
to obtain the solution(s) of the original equation in B .
</p>
<p>A convenient basis is that in which A is represented by a diagonal ma-
trix diag(λ1, λ2, . . . , λN ). Then the operator equation takes the simple form
λiui = vi , and the solution becomes immediate.
</p>
<p>S. Hassani, Mathematical Physics, DOI 10.1007/978-3-319-01195-0_17,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>511</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-01195-0_17">http://dx.doi.org/10.1007/978-3-319-01195-0_17</a></div>
</div>
<div class="page"><p/>
<p>512 17 Introductory Operator Theory
</p>
<p>Let us now apply the procedure just described to infinite-dimensional
vector spaces, in particular, for the case of a continuous index. We want to
find the solutions of K|u〉 = |f 〉. Following the procedure used above, we
obtain
</p>
<p>〈x|K
(
ˆ b
</p>
<p>a
</p>
<p>|y〉w(y)〈y|dy
)
</p>
<p>︸ ︷︷ ︸
&equiv;1
</p>
<p>|u〉 =
ˆ b
</p>
<p>a
</p>
<p>〈x|K|y〉w(y)〈y|u〉dy = 〈x|f 〉,
</p>
<p>where we have used the results obtained in Sect. 7.3. Writing this in func-
tional notation, we have
</p>
<p>ˆ b
</p>
<p>a
</p>
<p>K(x,y)w(y)u(y) dy = f (x), (17.2)
</p>
<p>which is the continuous analogue of Eq. (17.1). Here (a, b) is the interval
Integral operators and
</p>
<p>kernels
on which the functions are defined. We note that the indices have turned into
continuous arguments, and the sum has turned into an integral. The operator
K that leads to an equation such as (17.2) is called an integral operator
(IO), and the &ldquo;matrix element&rdquo; K(x,y) is said to be its kernel.
</p>
<p>The discussion of the discrete case mentioned the possibility of the oper-
ator A being diagonal in the given basis B . Let us do the same with (17.2);
that is, noting that x and y are indices for K , let us assume that K(x,y)= 0
for x �= y. Such operators are called local operators. For local operators,
</p>
<p>local operators
the contribution to the integral comes only at the point where x = y (hence,
their name). If K(x,y) is finite at this point, and the functions w(y) and
u(y) are well behaved there, the LHS of (17.2) will vanish, and we will get
inconsistencies. To avoid this, we need to have
</p>
<p>K(x,y)=
{
</p>
<p>0 if x �= y,
&infin; if x = y.
</p>
<p>Thus, K(x,y) has the behavior of a delta function. Letting K(x,y) &equiv;
L(x)δ(x &minus; y)/w(x) and substituting in Eq. (17.2) yields L(x)u(x)= f (x).
</p>
<p>In the discrete case, λi was merely an indexed number; its continuous
analogue, L(x), may represent merely a function. However, the fact that x
</p>
<p>difference between
</p>
<p>discrete and continuous
</p>
<p>operators
</p>
<p>is a continuous variable (index) gives rise to other possibilities for L(x) that
do not exist for the discrete case. For instance, L(x) could be a differential
operator. The derivative, although defined by a limiting process involving
neighboring points, is a local operator. Thus, we can speak of the derivative
of a function at a point. For the discrete case, ui can only &ldquo;hop&rdquo; from i
to i + 1 and then back to i. Such a difference (as opposed to differential)
process is not local; it involves not only i but also i + 1. The &ldquo;point&rdquo; i does
not have an (infinitesimally close) neighbor.
</p>
<p>This essential difference between discrete and continuous operators
makes the latter far richer in possibilities for applications. In particular, if
L(x) is considered a differential operator, the equation L(x)u(x) = f (x)
leads directly to the fruitful area of differential equation theory.</p>
<p/>
</div>
<div class="page"><p/>
<p>17.2 Bounded Operators in Hilbert Spaces 513
</p>
<p>17.2 Bounded Operators in Hilbert Spaces
</p>
<p>The concept of an operator on a Hilbert space is extremely subtle. Even the
elementary characteristics of operators, such as the operation of hermitian
conjugation, cannot generally be defined on the whole Hilbert space.
</p>
<p>In finite-dimensional vector spaces there is a one-to-one correspondence
between operators and matrices. So, in some sense, the study of operators
reduces to a study of matrices, which are collections of real or complex
numbers. Although we have already noted an analogy between matrices and
kernels, a whole new realm of questions arises when Aij is replaced by
K(x,y)&mdash;questions about the continuity of K(x,y) in both its arguments,
about the limit of K(x,y) as x and/or y approach the &ldquo;end points&rdquo; of the
interval on which K is defined, about the boundedness and &ldquo;compactness&rdquo; of
K, and so on. Such subtleties are not unexpected. After all, when we tried to
generalize concepts of finite-dimensional vector spaces to infinite dimen-
sions in Chap. 7, we encountered difficulties. There we were concerned
about vectors only; the generalization of operators is even more compli-
cated.
</p>
<p>Example 17.2.1 Recall that C&infin; is the set of sequences |a〉 = {αi}&infin;i=1,
or of &infin;-tuples (α1, α2, . . . ), that satisfy the convergence requirement&sum;&infin;
</p>
<p>j=1 |αj |2 &lt;&infin; (see Example 2.1.2). It is a Hilbert space with inner prod-
uct defined by 〈a|b〉 = &sum;&infin;j=1 α&lowast;jβj . The standard (orthonormal) basis for
C&infin; is {|ei〉}&infin;i=1, where |ei〉 has all components equal to zero except the ith
one, which is 1. Then one has |a〉 =&sum;&infin;j=1 αj |ej 〉.
</p>
<p>One can introduce an operator X, called the right-shift operator, by
right-shift operator
</p>
<p>X|a〉 = X
( &infin;&sum;
</p>
<p>j=1
αj |ej 〉
</p>
<p>)
=
</p>
<p>&infin;&sum;
</p>
<p>j=1
αj |ej+1〉.
</p>
<p>In other words, X transforms (α1, α2, . . . ) to (0, α1, α2, . . . ). It is straight-
forward to show that X is indeed a linear operator.
</p>
<p>The first step in our study of vector spaces of infinite dimensions was
getting a handle on the convergence of infinite sums. This entailed defining
a norm for vectors and a distance between them. In addition, we noted that
the set of linear transformations L(V,W) was a vector space in its own right.
Since operators are &ldquo;vectors&rdquo; in this space, the study of operators requires
constructing a norm in L(V,W) when V and W are infinite-dimensional.
</p>
<p>Definition 17.2.2 Let H1 and H2 be two Hilbert spaces with norms ‖ &middot; ‖1
and ‖ &middot; ‖2. For any T &isin;L(H1,H2), the number
</p>
<p>max
</p>
<p>{‖Tx‖2
‖x‖1
</p>
<p>∣∣∣ |x〉 �= 0
}</p>
<p/>
</div>
<div class="page"><p/>
<p>514 17 Introductory Operator Theory
</p>
<p>(if it exists) is called1 the operator norm of T and is denoted by ‖T‖. A lin-
operator norm
</p>
<p>ear transformation whose norm is finite is called a bounded linear trans-
formation. A bounded linear transformation from a Hilbert space to itself is
called a bounded operator. The collection of all bounded linear transfor-
</p>
<p>bounded operator
mations, which is a subset of L(H1,H2), will be denoted by B(H1,H2),
and if H1 =H2 &equiv;H, it will be denoted by B(H).
</p>
<p>Note that ‖ &middot; ‖1 and ‖ &middot; ‖2 are the norms induced by the inner product of
H1 and H2. Also note that by dividing by ‖x‖1 we eliminate the possibility
of dilating the norm of ‖T‖ by choosing a &ldquo;long&rdquo; vector. By restricting the
length of |x〉, one can eliminate the necessity for dividing by the length. In
fact, the norm can equivalently be defined as
</p>
<p>‖T‖ = max
{
‖Tx‖2 | ‖x‖1 = 1
</p>
<p>}
= max
</p>
<p>{
‖Tx‖2 | ‖x‖1 &le; 1
</p>
<p>}
. (17.3)
</p>
<p>It is straightforward to show that the three definitions are equivalent and they
indeed define a norm.
</p>
<p>Proposition 17.2.3 An operator T is bounded if and only if it maps vectors
of finite norm to vectors of finite norm.
</p>
<p>Proof Clearly, if T is bounded, then ‖Tx‖ has finite norm. Conversely, if
‖Tx‖2 is finite for all |x〉 (of unit length), max{‖Tx‖2 | ‖x‖1 = 1} is also
finite, and T is bounded. �
</p>
<p>An immediate consequence of the definition is
</p>
<p>‖Tx‖2 &le; ‖T‖ ‖x‖1 &forall;|x〉 &isin;H1. (17.4)
</p>
<p>If we choose |x〉 &minus; |y〉 instead of |x〉, it will follow from (17.4) that as |x〉
approaches |y〉, T|x〉 approaches T|y〉. This is the property that characterizes
continuity:
</p>
<p>Proposition 17.2.4 The bounded operator T &isin;B(H1,H2) is a continuous
bounded operators are
</p>
<p>continuous
linear map from H1 to H2.
</p>
<p>Another consequence of the definition is that
</p>
<p>Box 17.2.5 B(H1,H2) is a vector subspace of L(H1,H2), and for
H1 =H2 =H, we have 1 &isin;B(H) and ‖1‖ = 1.
</p>
<p>1The precise definition uses &ldquo;supremum&rdquo; instead of &ldquo;maximum&rdquo;. Rather than spending a
lot of effort explaining the difference between the two concepts, we use the less precise,
but more intuitively familiar, concept of &ldquo;maximum&rdquo;.</p>
<p/>
</div>
<div class="page"><p/>
<p>17.2 Bounded Operators in Hilbert Spaces 515
</p>
<p>Example 17.2.6 We have seen that in an inner product space, one can as-
sociate a linear operator (linear functional) to every vector. Thus, associated
with the vector |x〉 in a Hilbert space H is the linear operator fx :H &rarr; C
defined by fx(|y〉) &equiv; 〈x|y〉. We want to compare the operator norm of fx
with the norm of |x〉. First note that by using the Schwarz inequality, we get
</p>
<p>‖fx‖ = max
{ |fx(|y〉)|
</p>
<p>‖y‖
∣∣∣ |y〉 �= 0
</p>
<p>}
= max
</p>
<p>{ |〈x|y〉|
‖y‖
</p>
<p>∣∣∣ |y〉 �= 0
}
&le; ‖x‖.
</p>
<p>On the other hand, from ‖x‖2 = fx(|x〉), we obtain
</p>
<p>‖x‖ = fx(|x〉)‖x‖ &le; max
{ |fx(|y〉)|
</p>
<p>‖y‖
∣∣∣ |y〉 �= 0
</p>
<p>}
= ‖fx‖.
</p>
<p>These two inequalities imply that ‖fx‖ = ‖x‖.
</p>
<p>Example 17.2.7 The derivative operator D= d/dx is not a bounded oper-
ator on the Hilbert space2 L2(a, b) of square-integrable functions. With a
function like f (x)=&radic;x &minus; a, one gets
</p>
<p>‖f ‖2 =
ˆ b
</p>
<p>a
</p>
<p>(x &minus; a)dx = 1
2
(b&minus; a)2 &rArr; ‖f ‖ = b&minus; a&radic;
</p>
<p>2
,
</p>
<p>while df/dx = 1/(2&radic;x &minus; a) gives ‖Df ‖2 = 14
&acute; b
</p>
<p>a
dx/(x &minus; a) = &infin;. We
</p>
<p>conclude that ‖D‖ =&infin;.
</p>
<p>derivative operator is
</p>
<p>unbounded
</p>
<p>Since L(H) is an algebra as well as a vector space, one may be inter-
norm of a product is less
</p>
<p>than the product of
</p>
<p>norms.
</p>
<p>ested in the relation between the product of operators and their norms. More
specifically, one may want to know how ‖ST‖ is related to ‖S‖ and ‖T‖.
</p>
<p>Proposition 17.2.8 If S and T are bounded operators, then
</p>
<p>‖ST‖ &le; ‖S‖‖T‖. (17.5)
</p>
<p>In Particular, ‖Tn‖ &le; ‖T‖n.
</p>
<p>Proof Use the definition of operator norm for the product ST:
</p>
<p>‖ST‖ = max
{‖STx‖
</p>
<p>‖x‖
∣∣∣ |x〉 �= 0
</p>
<p>}
</p>
<p>= max
{‖STx‖
</p>
<p>‖Tx‖
‖Tx‖
‖x‖
</p>
<p>∣∣∣ |x〉 �= 0 �= T|x〉
}
</p>
<p>2Here the two Hilbert spaces coincide, so that the derivative operator acts on a single
Hilbert space.</p>
<p/>
</div>
<div class="page"><p/>
<p>516 17 Introductory Operator Theory
</p>
<p>&le; max
{‖S(T|x〉)‖
</p>
<p>‖Tx‖
∣∣∣ T|x〉 �= 0
</p>
<p>}
max
</p>
<p>{‖Tx‖
‖x‖
</p>
<p>∣∣∣ |x〉 �= 0
}
</p>
<p>︸ ︷︷ ︸
=‖T‖
</p>
<p>.
</p>
<p>Now note that the first term on the RHS does not scan all the vectors for
maximality: It scans only the vectors in the image of T. If we include all
vectors, we may obtain a larger number. Therefore,
</p>
<p>max
</p>
<p>{‖S(T|x〉)‖
‖Tx‖
</p>
<p>∣∣∣ T|x〉 �= 0
}
&le; max
</p>
<p>{‖Sx‖
‖x‖
</p>
<p>∣∣∣ |x〉 �= 0
}
= ‖S‖,
</p>
<p>and the desired inequality is established. �
</p>
<p>We can put Eq. (17.5) to immediate good use.
</p>
<p>Proposition 17.2.9 Let H be a Hilbert space and T &isin; B(H). If ‖T‖ &lt; 1,
then 1&minus; T is invertible and (1&minus; T)&minus;1 =&sum;&infin;n=0 Tn.
</p>
<p>Proof First note that the series converges, because
</p>
<p>∥∥∥∥∥
&infin;&sum;
</p>
<p>n=0
Tn
</p>
<p>∥∥∥∥∥&le;
&infin;&sum;
</p>
<p>n=0
</p>
<p>∥∥Tn
∥∥&le;
</p>
<p>&infin;&sum;
</p>
<p>n=0
‖T‖n = 1
</p>
<p>1 &minus; ‖T‖
</p>
<p>and the sum has a finite norm. Furthermore,
</p>
<p>(1&minus; T)
&infin;&sum;
</p>
<p>n=0
Tn = (1&minus; T)
</p>
<p>(
lim
k&rarr;&infin;
</p>
<p>k&sum;
</p>
<p>n=0
Tn
</p>
<p>)
= lim
</p>
<p>k&rarr;&infin;
(1&minus; T)
</p>
<p>k&sum;
</p>
<p>n=0
Tn
</p>
<p>= lim
k&rarr;&infin;
</p>
<p>(
k&sum;
</p>
<p>n=0
Tn &minus;
</p>
<p>k&sum;
</p>
<p>n=0
Tn+1
</p>
<p>)
= lim
</p>
<p>k&rarr;&infin;
(
1&minus; Tk+1
</p>
<p>)
= 1,
</p>
<p>because
</p>
<p>0 &le; lim
k&rarr;&infin;
</p>
<p>∥∥Tk+1
∥∥&le; lim
</p>
<p>k&rarr;&infin;
‖T‖k+1 = 0
</p>
<p>for ‖T‖ &lt; 1, and the vanishing of the norm implies the vanishing of the
operator itself. One can similarly show that (
</p>
<p>&sum;&infin;
n=0 T
</p>
<p>n)(1&minus; T)= 1. �
</p>
<p>A corollary of this proposition is that operators that are &ldquo;close enough&rdquo; to
an invertible operator are invertible (see Problem 17.1). Another corollary,
whose proof is left as a straightforward exercise, is the following:
</p>
<p>Corollary 17.2.10 Let T &isin;B(H) and λ a complex number such that ‖T‖&lt;
|λ|. Then T&minus; λ1 is an invertible operator, and
</p>
<p>(T&minus; λ1)&minus;1 =&minus;1
λ
</p>
<p>&infin;&sum;
</p>
<p>n=0
</p>
<p>(
T
</p>
<p>λ
</p>
<p>)n
.</p>
<p/>
</div>
<div class="page"><p/>
<p>17.3 Spectra of Linear Operators 517
</p>
<p>17.2.1 Adjoints of Bounded Operators
</p>
<p>Adjoints play an important role in the study of operators. We recall that the
adjoint of T is defined as
</p>
<p>〈y|T|x〉&lowast; = 〈x|T&dagger;|y〉 or 〈Tx|y〉 = 〈x|T&dagger;y〉.
</p>
<p>In the finite-dimensional case, we could calculate the matrix representation
of the adjoint in a particular basis using this definition and generalize to all
bases by similarity transformations. That is why we never raised the ques-
tion of the existence of the adjoint of an operator. In the infinite-dimensional
case, one must prove such an existence. We state the following theorem
without proof:
</p>
<p>Theorem 17.2.11 Let T &isin;B(H). Then the adjoint of T, defined by
</p>
<p>〈Tx|y〉 = 〈x|T&dagger;y〉,
</p>
<p>exists. Furthermore, ‖T‖ = ‖T&dagger;‖. T and T&dagger; have equal
norms
</p>
<p>17.3 Spectra of Linear Operators
</p>
<p>One of the most important results of the theory of finite-dimensional vec-
tor spaces is the spectral decomposition theorem developed in Chap. 6. The
infinite-dimensional analogue of that theorem is far more encompassing and
difficult to prove. It is beyond the scope of this book to develop all the ma-
chinery needed for a thorough discussion of the infinite-dimensional spectral
theory. Instead, we shall present the central results, and occasionally intro-
duce the reader to the peripheral arguments when they seem to have their
own merits.
</p>
<p>Definition 17.3.1 Let T &isin; L(H). A complex number λ is called a
regular point of T if the operator (T&minus; λ1)&minus;1 exists and is bounded.
The set of all regular points of T is called the resolvent set of T, and
is denoted by ρ(T). The complement of ρ(T) in the complex plane is
called the spectrum of T and is denoted by σ(T).
</p>
<p>Note that if T is bounded, then T&minus; λ1 is automatically bounded.
</p>
<p>regular point, resolvent
</p>
<p>set, and spectrum of an
</p>
<p>operator
</p>
<p>Corollary 17.2.10 implies that if T is bounded, then ρ(T) is not empty,3
</p>
<p>and that the spectrum of a bounded linear operator on a Hilbert space is
a bounded set. In fact, an immediate consequence of the corollary is that
λ&le; ‖T‖ for all λ &isin; σ(T).
</p>
<p>3One can simply choose a λ whose absolute value is greater than ‖T‖.</p>
<p/>
</div>
<div class="page"><p/>
<p>518 17 Introductory Operator Theory
</p>
<p>It is instructive to contrast the finite-dimensional case against the implica-every eigenvalue of an
operator on a vector
</p>
<p>space of finite dimension
</p>
<p>is in its spectrum and
</p>
<p>vice versa
</p>
<p>tions of the above definition. Recall that because of the dimension theorem,
a linear operator on a finite-dimensional vector space V is invertible if and
only if it is either onto or one-to-one. Now, λ &isin; σ(T) if and only if T&minus; λ1
is not invertible. For finite dimensions, this implies that4 ker(T&minus; λ1) �= 0.
Thus, in finite dimensions, λ &isin; σ(T) if and only if there is a vector |a〉 in V
such that (T&minus;λ1)|a〉 = 0. This is the combined definition of eigenvalue and
eigenvector, and is the definition we will have to use to define eigenvalues
in infinite dimensions. It follows that in the finite-dimensional case, σ(T)
coincides with the set of all eigenvalues of T. This is not true for infinite
dimensions, as the following example shows.
</p>
<p>Example 17.3.2 Consider the right-shift operator acting on C&infin;. It is easynot all points of σ(T) are
eigenvalues to see that ‖Tra‖ = ‖a‖ for all |a〉. This yields ‖Tr‖ = 1, so that any λ that
</p>
<p>belongs to σ(Tr) must be such that |λ| &le; 1. We now show that the converse
is also true, i.e., that if |λ| &le; 1, then λ &isin; σ(Tr). It is sufficient to show that if
0 &lt; |λ| &le; 1, then Tr &minus; λ1 is not invertible. To establish this, we shall show
that Tr &minus; λ1 is not onto.
</p>
<p>Suppose that Tr &minus; λ1 is onto. Then there must be a vector |a〉 such that
(Tr &minus; λ1)|a〉 = |e1〉 where |e1〉 is the first standard basis vector of C&infin;.
Equating components on both sides yields the recursion relations α1 =
&minus;1/λ, and αj&minus;1 = λαj for all j &ge; 2. One can readily solve this recursion
relation to obtain αj =&minus;1/λj for all j . This is a contradiction, because
</p>
<p>&infin;&sum;
</p>
<p>j=1
|αj |2 =
</p>
<p>&infin;&sum;
</p>
<p>j=1
</p>
<p>1
</p>
<p>|λ|2j
</p>
<p>will not converge if 0 &lt; |λ| &le; 1, i.e., |a〉 /&isin;C&infin;, and therefore Tr &minus; λ1 is not
onto.
</p>
<p>We conclude that σ(Tr)= {λ &isin; C | 0 &lt; |λ| &le; 1}. If we could generalize
the result of the finite-dimensional case to C&infin;, we would conclude that all
complex numbers whose magnitude is at most 1 are eigenvalues of Tr . Quite
to our surprise, the following argument shows that Tr has no eigenvalues at
all!
</p>
<p>Suppose that λ is an eigenvalue of Tr . Let |a〉 be any eigenvector for λ.
Since Tr preserves the length of a vector, we have
</p>
<p>〈a|a〉 = 〈Tra|Tra〉 = 〈λa|λa〉 = |λ|2〈a|a〉.
</p>
<p>It follows that |λ| = 1. Now write |a〉 = {αj }&infin;j=1 and let αm be the first
nonzero term of this sequence. Then 0 = 〈Tra|em〉 = 〈λa|em〉 = λαm. The
first equality comes about because Tr |a〉 has its first nonzero term in the
(m + 1)st position. Since λ �= 0, we must have αm = 0, which contradicts
the choice of this number.
</p>
<p>4Note how critical finite-dimensionality is for this implication. In infinite dimensions, an
operator can be one-to-one (thus having a zero kernel) without being onto.</p>
<p/>
</div>
<div class="page"><p/>
<p>17.4 Compact Sets 519
</p>
<p>17.4 Compact Sets
</p>
<p>This section deals with some technical concepts, and as such will be rather
formal. The central concept of this section is compactness. Although we
shall be using compactness sparingly in the sequel, the notion has sufficient
application in higher analysis and algebra that it warrants an introductory
exposure.
</p>
<p>Let us start with the familiar case of the real line, and the intuitive notion
of &ldquo;compactness&rdquo;. Clearly, we do not want to call the entire real line &ldquo;com-
pact&rdquo;, because intuitively, it is not. The next candidate seems to be a &ldquo;finite&rdquo;
interval. So, first consider the open interval (a, b). Can we call it compact?
Intuition says &ldquo;yes&rdquo;, but the following argument shows that it would not be
appropriate to call the open interval compact.
</p>
<p>Consider the map ϕ :R&rarr; (a, b) given by
</p>
<p>ϕ(t)= b&minus; a
2
</p>
<p>tanh t + b+ a
2
</p>
<p>.
</p>
<p>The reader may check that this map is continuous and bijective. Thus, we
can continuously map all of R in a one-to-one manner onto (a, b). This
makes (a, b) &ldquo;look&rdquo; very much5 like R. How can we modify the interval
to make it compact? We do not want to alter its finiteness. So, the obvious
thing to do is to add the end points. Thus, the interval [a, b] seems to be a
good candidate; and indeed it is.
</p>
<p>The next step is to generalize the notion of a closed, finite interval and
eventually come up with a definition that can be applied to all spaces. First
we need some terminology.
</p>
<p>Definition 17.4.1 An open ball Br(x) of radius r and center |x〉 in a open ball
normed vector space V is the set of all vectors in V whose distance from
|x〉 is strictly less than r :
</p>
<p>Br (x)&equiv;
{
|y〉 &isin; V | ‖y &minus; x‖&lt; r
</p>
<p>}
.
</p>
<p>We call Br(x) an open round neighborhood of |x〉. open round
neighborhood
</p>
<p>This is a generalization of open interval because
</p>
<p>(a, b)=
{
y &isin;R
</p>
<p>∣∣∣
∣∣∣∣y &minus;
</p>
<p>a + b
2
</p>
<p>∣∣∣∣&lt;
b&minus; a
</p>
<p>2
</p>
<p>}
.
</p>
<p>Example 17.4.2 A prototype of finite-dimensional normed spaces is Rn.
An open ball of radius r centered at x is
</p>
<p>Br(x)=
{
y &isin;R | (y1 &minus; x1)2 + (y2 &minus; x2)2 + &middot; &middot; &middot; + (yn &minus; xn)2 &lt; r2
</p>
<p>}
.
</p>
<p>Thus, all points inside a circle form an open ball in the xy-plane, and all
interior points of a solid sphere form an open ball in space.
</p>
<p>5In topological jargon one says that (a, b) and R are homeomorphic.</p>
<p/>
</div>
<div class="page"><p/>
<p>520 17 Introductory Operator Theory
</p>
<p>Definition 17.4.3 A bounded subset of a normed vector space is a subset
that can be enclosed in an open ball of finite radius.
</p>
<p>bounded subset
</p>
<p>For example, any region drawn on a piece of paper is a bounded subset
of R2, and any &ldquo;visible&rdquo; part of our environment is a bounded subset of R3
</p>
<p>because we can always find a big enough circle or sphere to enclose these
subsets.
</p>
<p>Definition 17.4.4 A subset O of a normed vector space V is called open ifopen subset
each of its points (vectors) has an open round neighborhood lying entirely
in O. A boundary point of O is a point (vector) in V all of whose open roundboundary point
neighborhoods contain points inside and outside O. A closed subset C of Vclosed subset and
</p>
<p>closure is a subset that contains all of its boundary points. The closure of a subset S
is the union of S and all of its boundary points, and is denoted by S̄.
</p>
<p>For example, the boundary of a region drawn on paper consists of all its
boundary points. A curve drawn on paper has nothing but boundary points.
Every point is also its own boundary. A boundary is always a closed set.
In particular, a point is a closed set. In general, an open set cannot contain
any boundary points. A frequently used property of a closed set C is that a
convergent sequence of points of C converges to a point in C.
</p>
<p>Definition 17.4.5 A subset W of a normed vector space V is dense in V ifdense subset
the closure of W is the entire space V. Equivalently, W is dense if, given any
|u〉 &isin; V and any ǫ &gt; 0, there is a |w〉 &isin;W such that ‖u&minus;w‖&lt; ǫ, i.e., any
vector in V can be approximated, with arbitrary accuracy, by a vector in W .
</p>
<p>A paradigm of dense spaces is the set of rational numbers in the normedrational numbers are
dense in the real
</p>
<p>numbers
</p>
<p>vector space of real numbers. It is a well-known fact that any real number
can be approximated by a rational number with arbitrary accuracy: The dec-
imal (or binary) representation of real numbers is precisely such an approx-
imation. An intuitive way of imagining denseness is that the (necessarily)
infinite subset is equal to almost all of the set, and its members are scattered
&ldquo;densely&rdquo; everywhere in the set. The embedding of the rational numbers in
the set of real numbers, and how they densely populate that set, is a good
mental picture of all dense subsets.
</p>
<p>A useful property involving the concept of closure and openness has to
do with continuous maps between normed vector spaces. Let f :H1 &rarr;H2
be a continuous map. Let O2 be an open set in H2. Let f&minus;1(O2) denote the
inverse image of O2, i.e., all points of H1 that are mapped to O2. Let |x1〉
be a vector in f&minus;1(O2), |x2〉 = f (|x1〉), and let Bǫ(x2) be a ball contained
entirely in O2. Then f&minus;1(Bǫ(x2)) contains |x1〉 and lies entirely in f&minus;1(O2).
Because of the continuity of f , one can now construct an open ball centered
at |x1〉 lying entirely in f&minus;1(Bǫ(x2)), and by inclusion, in f&minus;1(O2). This
shows that every point of f&minus;1(O2) has a round open neighborhood lying
entirely in f&minus;1(O2). Thus, f&minus;1(O2) is an open subset. One can similarly
show the corresponding property for closed subsets. We can summarize this
in the following:</p>
<p/>
</div>
<div class="page"><p/>
<p>17.4 Compact Sets 521
</p>
<p>Proposition 17.4.6 Let f :H1 &rarr;H2 be continuous. Then the inverse im-
age of an open (closed) subset of H2 is an open (closed) subset of H1.
</p>
<p>Consider the resolvent set of a bounded operator T. We claim that this set
is open in C. To see this, note that if λ &isin; ρ(T), then T&minus; λ1 is invertible. On
the other hand, Problem 17.1 shows that operators close to an invertible op-
erator are invertible. Thus, if we choose a sufficiently small positive number
ǫ and consider all complex numbers μ within a distance ǫ from λ, then all
operators of the form T&minus; μ1 are invertible, i.e., μ &isin; ρ(T). Therefore, any ρ(T) is open, and σ(T) is
</p>
<p>closed and bounded in
</p>
<p>C.
</p>
<p>λ &isin; ρ(T) has an open round neighborhood in the complex plane all points
of which are in the resolvent. This shows that the resolvent set is open. In
particular, it cannot contain any boundary points. However, ρ(T) and σ(T)
have to be separated by a common boundary.6 Since ρ(T) cannot contain
any boundary point, σ(T) must carry the entire boundary. This shows that
σ(T) is a closed subset of C. Recalling that σ(T) is also bounded, we have
the following result.
</p>
<p>Proposition 17.4.7 For any T &isin;B(H) the set ρ(T) is an open subset
of C and σ(T) is a closed, bounded subset of C.
</p>
<p>17.4.1 Compactness and Infinite Sequences
</p>
<p>Let us go back to the notion of compactness. It turns out that the feature of
the closed interval [a, b] most appropriate for generalization is the behavior
of infinite sequences of numbers lying in the interval. More specifically,
let {αi}&infin;i=1 be a sequence of infinitely many real numbers all lying in the
interval [a, b]. It is intuitively clear that since there is not enough room for
these points to stay away from each other, they will have to crowd around a
number of points in the interval. For example, the sequence
</p>
<p>{
(&minus;1)n 2n+ 1
</p>
<p>4n
</p>
<p>}&infin;
</p>
<p>n=1
=
{
&minus;3
</p>
<p>4
,+5
</p>
<p>8
,&minus; 7
</p>
<p>12
,+ 9
</p>
<p>16
, . . .
</p>
<p>}
</p>
<p>in the interval [&minus;1,+1] crowds around the two points &minus; 12 and + 12 , i.e., the
sequence has two limits, both in the interval. In fact, the points with even
n accumulate around + 12 and those with odd n crowd around &minus; 12 . It turns
out that all closed intervals of R have this property, namely, all sequences
crowd around some (limit) points of the interval. To see that open intervals
do not share this property consider the open interval (0,1). The sequence
{ 12n+1 }&infin;n=1 = { 13 , 15 , . . . } clearly has the limit point zero, which is not a point
of the interval. But we already know that open intervals are not compact.
</p>
<p>6The spectrum of a bounded operator need not occupy any &ldquo;area&rdquo; in the complex plane.
It may consist of isolated points or line segments, etc., in which case the spectrum will
constitute the entire boundary.</p>
<p/>
</div>
<div class="page"><p/>
<p>522 17 Introductory Operator Theory
</p>
<p>Definition 17.4.8 (Bolzano-Weierstrass Property) A subset K of a normed
vector space is called compact if every (infinite) sequence in K has a con-compact subset
vergent subsequence.
</p>
<p>The reason for the introduction of a subsequence in the definition is that
a sequence may have many points to which it converges. But no matter how
many of these points there may exist, one can always obtain a convergent
subsequence by choosing from among the points in the sequence. For in-
stance, in the example above, one can choose the subsequence consisting
of elements for which n is even. This subsequence converges to the single
point + 12 .
</p>
<p>An important theorem in real analysis characterizes all compact sets in
Rn:7
</p>
<p>Theorem 17.4.9 (BWHB) A subset of Rn is compact if and only if it isσ(T) is compact
closed and bounded.
</p>
<p>We showed earlier that the spectrum of a bounded linear operator is
closed and bounded. Identifying C with R2, the BWHB theorem implies
that
</p>
<p>Box 17.4.10 The spectrum of a bounded linear operator is a compact
subset of C.
</p>
<p>An immediate consequence of the BWHB Theorem is that every bounded
subset of Rn has a compact closure. Since Rn is a prototype of all finite-
dimensional (normed) vector spaces, the same statement is true for all such
vector spaces. What is interesting is that the statement indeed characterizes
the normed space:
</p>
<p>Theorem 17.4.11 A normed vector space is finite-dimensional if and onlycriterion for
finite-dimensionality if every bounded subset has a compact closure.
</p>
<p>This result can also be applied to subspaces of a normed vector space:
A subspace W of a normed vector space V is finite-dimensional if and only
if every bounded subset of W has a compact closure in W. A useful version
of this property is stated in terms of sequences of points (vectors):
</p>
<p>7BWHB stands for Bolzano, Weierstrass, Heine, and Borel. Bolzano and Weierstrass
proved that any closed and bounded subset of R has the Bolzano-Weierstrass prop-
erty. Heine and Borel abstracted the notion of compactness in terms of open sets, and
showed that a closed bounded subset of R is compact. The BWHB theorem as ap-
plied to R is usually called the Heine-Borel theorem (although some authors call it the
Bolzano-Weierstrass theorem). Since the Bolzano&ndash;Weierstrass property and compactness
are equivalent, we have decided to choose BWHB as the name of our theorem.</p>
<p/>
</div>
<div class="page"><p/>
<p>17.5 Compact Operators 523
</p>
<p>Theorem 17.4.12 A subspace W of a normed vector space V is fi-
nite dimensional if and only if every bounded sequence in W has a
convergent subsequence in W.
</p>
<p>Historical Notes
</p>
<p>Karl Theodor Wilhelm Weierstrass (1815&ndash;1897) was both the greatest analyst and the
</p>
<p>Karl Theodor Wilhelm
</p>
<p>Weierstrass 1815&ndash;1897
</p>
<p>world&rsquo;s foremost teacher of advanced mathematics of the last third of the nineteenth
century. His career was also remarkable in another way&mdash;and a consolation to all &ldquo;late
starters&rdquo;&mdash;for he began the solid part of his professional life at the age of almost 40, when
most mathematicians are long past their creative years.
His father sent him to the University of Bonn to qualify for the higher ranks of the Prussian
civil service by studying law and commerce. But Karl had no interest in these subjects.
He infuriated his father by rarely attending lectures, getting poor grades, and instead,
becoming a champion beer drinker. He did manage to become a superb fencer, but when
he returned home, he had no degree.
In order to earn his living, he made a fresh start by teaching mathematics, physics, botany,
German, penmanship, and gymnastics to the children of several small Prussian towns dur-
ing the day. During the nights, however, he mingled with the intellectuals of the past, par-
ticularly the great Norwegian mathematician Abel. His remarkable research on Abelian
functions was carried on for years without the knowledge of another living soul; he didn&rsquo;t
discuss it with anyone at all, or submit it for publication in the mathematical journals of
the day.
All this changed in 1854 when Weierstrass at last published an account of his research on
Abelian functions. This paper caught the attention of an alert professor at the University
of K&ouml;nigsberg who persuaded his university to award Weierstrass an honorary doctor&rsquo;s
degree. The Ministry of Education granted Weierstrass a year&rsquo;s leave of absence with pay
to continue his research, and the next year he was appointed to the University of Berlin,
where he remained the rest of his life.
Weierstrass&rsquo;s great creative talents were evenly divided between his thinking and his
teaching. The student notes of his lectures, and copies of these notes, and copies of copies,
were passed from hand to hand throughout Europe and even America. Like Gauss he was
indifferent to fame, but unlike Gauss he endeared himself to generations of students by
the generosity with which he encouraged them to develop and publish, and receive credit
for, ideas and theorems that he essentially originated himself. Among Weierstrass&rsquo;s stu-
dents and followers were Cantor, Schwarz, H&ouml;lder, Mittag-Leffler, Sonja Kovalevskaya
(Weierstrass&rsquo;s favorite student), Hilbert, Max Planck, Willard Gibbs, and many others.
In 1885 he published the famous theorem now called the Weierstrass approximation
theorem (see Theorems 7.2.3 and 9.1.1), which was given a far-reaching generalization,
with many applications, by the modern American mathematician M. H. Stone.
The quality that came to be known as &ldquo;Weierstrassian rigor&rdquo; was particularly visible in
his contributions to the foundations of real analysis. He refused to accept any statement
as &ldquo;intuitively obvious,&rdquo; but instead demanded ironclad proof based on explicit properties
of the real numbers. The careful reasoning required for these proofs was founded on a
crucial property of the real numbers now known as the BWHB theorem.
</p>
<p>17.5 Compact Operators
</p>
<p>It is straightforward to show that if K is a compact set in H1 and f :H1 &rarr; compact operator
H2 is continuous, then f (K) (the image of K) is compact in H2. Since all
bounded operators are continuous, we conclude that all bounded operators
map compact subsets onto compact subsets. There is a special subset of
B(H1,H2) that deserves particular attention.</p>
<p/>
</div>
<div class="page"><p/>
<p>524 17 Introductory Operator Theory
</p>
<p>Definition 17.5.1 An operator K &isin; B(H1,H2) is called a compact
operator if it maps a bounded subset of H1 onto a subset of H2 with
compact closure.
</p>
<p>Since we will be dealing with function spaces, and since it is easier to
deal with sequences of functions than with subsets of the space of functions,
we find it more useful to have a definition of compact operators in terms of
sequences rather than subsets. Thus, instead of a bounded subset, we take
a subset of it consisting of a (necessarily) bounded sequence. The image of
this sequence will be a sequence in a compact set, which, by definition, must
have a convergent subsequence. We therefore have the following:
</p>
<p>Theorem 17.5.2 An operator K &isin;B(H1,H2) is compact if and only if for
any bounded sequence {|xn〉} in H1, the sequence {K|xn〉} has a convergent
subsequence in H2.
</p>
<p>Example 17.5.3 Consider B(H), the set of bounded operators on the
Hilbert space H. If K is a compact operator and T a bounded operator, then
KT and TK are compact. This is because {T|xn〉 &equiv; |yn〉} is a bounded se-
quence if {|xn〉} is, and {K|yn〉 = KT|xn〉} has a convergent subsequence,
because K is compact. For the second part, use the first definition of the
compact operator and note that K maps bounded sets onto compact sets,
which T (being continuous) maps onto a compact set. As a special case of
this property we note that the product of two compact operators is compact.product of two compact
</p>
<p>operators is compact Similarly, one can show that any linear combination of compact operators is
compact. Thus, any polynomial of a compact operator is compact. In partic-
ular,
</p>
<p>(1&minus; K)n =
n&sum;
</p>
<p>j=0
</p>
<p>n!
j !(n&minus; j)! (&minus;K)
</p>
<p>j = 1+
n&sum;
</p>
<p>j=1
</p>
<p>n!
j !(n&minus; j)! (&minus;K)
</p>
<p>j &equiv; 1&minus; Kn,
</p>
<p>where Kn is a compact operator.
</p>
<p>Definition 17.5.4 An operator T &isin;L(H1,H2) is called a finite rank oper-finite rank operators
ator if its range is finite-dimensional.
</p>
<p>The following is clear from Theorem 17.4.12.linear transformations of
finite-dimensional
</p>
<p>vector spaces are
</p>
<p>compact Proposition 17.5.5 A finite rank operator is compact. In particular,
every linear transformation of a finite-dimensional vector space is
compact.
</p>
<p>Theorem 17.5.6 If {Kn} &isin; L(H1,H2) are compact and K &isin; L(H1,H2) is
such that ‖K&minus; Kn‖&rarr; 0, then K is compact.</p>
<p/>
</div>
<div class="page"><p/>
<p>17.5 Compact Operators 525
</p>
<p>Proof See [DeVi 90]. �
</p>
<p>Recall that given an orthonormal basis {|ei〉}&infin;i=1, any operator T on a
Hilbert space H can be written as
</p>
<p>&sum;&infin;
i,j=1 cij |ei〉〈ej |, where cij = 〈ei |T|ej 〉.
</p>
<p>Now let K be a compact operator and consider the finite rank operators
</p>
<p>Kn &equiv;
n&sum;
</p>
<p>i,j=1
cij |ei〉〈ej |, cij = 〈ei |K|ej 〉.
</p>
<p>Clearly, ‖K&minus; Kn‖&rarr; 0. The hermitian adjoints {K&dagger;n} are also of finite rank
(therefore, compact). Barring some convergence technicality, we see that
K&dagger;, which is the limit of the sequence of these compact operators, is also
compact.
</p>
<p>Theorem 17.5.7 K is a compact operator if and only if K&dagger; is. K is compact iff K&dagger; is
</p>
<p>A particular type of operator occurs frequently in integral equation the-
ory. These are called Hilbert-Schmidt operators and defined as follows:
</p>
<p>Definition 17.5.8 Let H be a Hilbert space, and {|ei〉}&infin;i=1 an orthonormal
basis. An operator T &isin;L(H) is called Hilbert-Schmidt if Hilbert-Schmidt
</p>
<p>operators
</p>
<p>tr(T&dagger;T)&equiv;
&infin;&sum;
</p>
<p>i=1
〈ei |T&dagger;T|ei〉 =
</p>
<p>&infin;&sum;
</p>
<p>i=1
〈Tei |Tei〉 =
</p>
<p>&infin;&sum;
</p>
<p>i=1
‖Tei‖2 &lt;&infin;.
</p>
<p>Theorem 17.5.9 Hilbert-Schmidt operators are compact.
</p>
<p>For a proof, see [Rich 78, pp. 242&ndash;246].
</p>
<p>Example 17.5.10 It is time to give a concrete example of a compact
(Hilbert-Schmidt) operator. For this, we return to Eq. (17.2) with w(y)= 1,
and assume that |u〉 &isin; L2(a, b). Suppose further that the function K(x,y)
is continuous on the closed rectangle [a, b] &times; [a, b] in the xy-plane (or
R2). Under such conditions, K(x,y) is called a Hilbert-Schmidt kernel. Hilbert-Schmidt kernel
We now show that K is compact. First note that due to the continuity of
K(x,y),
</p>
<p>&acute; b
</p>
<p>a
</p>
<p>&acute; b
</p>
<p>a
|K(x,y)|2dx dy &lt;&infin;. Next, we calculate the trace of K&dagger;K.
</p>
<p>Let {|ei〉}&infin;i=1 be any orthonormal basis of L2(a, b). Then
</p>
<p>trK&dagger;K=
&infin;&sum;
</p>
<p>i=1
〈ei |K&dagger;K|ei〉
</p>
<p>=
&infin;&sum;
</p>
<p>i=1
</p>
<p>˚
</p>
<p>〈ei |x〉〈x|K&dagger;|y〉〈y|K|z〉〈z|ei〉dx dy dz
</p>
<p>=
˚
</p>
<p>〈y|K|x〉&lowast;〈y|K|z〉
&infin;&sum;
</p>
<p>i=1
〈z|ei〉〈ei |x〉dx dy dz</p>
<p/>
</div>
<div class="page"><p/>
<p>526 17 Introductory Operator Theory
</p>
<p>=
˚
</p>
<p>〈y|K|x〉&lowast;〈y|K|z〉
</p>
<p>=δ(x&minus;z)︷ ︸︸ ︷
</p>
<p>〈z|
( &infin;&sum;
</p>
<p>i=1
|ei〉〈ei |
</p>
<p>)
</p>
<p>︸ ︷︷ ︸
=1
</p>
<p>|x〉 dx dy dz
</p>
<p>=
ˆ b
</p>
<p>a
</p>
<p>ˆ b
</p>
<p>a
</p>
<p>∣∣K(x,y)
∣∣2dx dy &lt;&infin;.
</p>
<p>Historical Notes
</p>
<p>Bernard Bolzano (1781&ndash;1848) was a Czech philosopher, mathematician, and theologian
who made significant contributions to both mathematics and the theory of knowledge. He
entered the Philosophy Faculty of the University of Prague in 1796, studying philosophy
and mathematics. He wrote &ldquo;My special pleasure in mathematics rested therefore particu-
larly on its purely speculative parts, in other words I prized only that part of mathematics
which was at the same time philosophy.&rdquo;
</p>
<p>Bernard Bolzano
</p>
<p>1781&ndash;1848
</p>
<p>In the autumn of 1800 he began three years of theological study while he was preparing
a doctoral thesis on geometry. He received his doctorate in 1804 for a thesis in which he
gave his view of mathematics and what constitutes a correct mathematical proof. In the
preface he wrote:
</p>
<p>I could not be satisfied with a completely strict proof if it were not derived from
concepts which the thesis to be proved contained, but rather made use of some
fortuitous, alien, intermediate concept, which is always an erroneous transition to
another kind.
</p>
<p>Two days after receiving his doctorate Bolzano was ordained a Roman Catholic priest.
However, he came to realize that teaching and not ministering defined his true vocation.
In the same year, Bolzano was appointed to the chair of philosophy and religion at the
University of Prague. Because of his pacifist beliefs and his concern for economic justice,
he was suspended from his position in 1819 after pressure from the Austrian government.
Bolzano had not given up without a fight but once he was suspended on a charge of heresy
he was put under house arrest and forbidden to publish.
Although some of his books had to be published outside Austria because of government
censorship, he continued to write and to play an important role in the intellectual life of his
country. Bolzano intended to write a series of papers on the foundations of mathematics.
He wrote two, the first of which was published. Instead of publishing the second one he
decided to &ldquo;. . . make myself better known to the learned world by publishing some papers
which, by their titles, would be more suited to arouse attention.&rdquo;
Pursuing this strategy he published Der binomische Lehrsatz . . . (1816) and Rein ana-
lytischer Beweis . . . (1817), which contain an attempt to free calculus from the concept
of the infinitesimal. He is clear in his intention stating in the preface of the first that the
work is &ldquo;a sample of a new way of developing analysis.&rdquo; The paper gives a proof of the
intermediate value theorem with Bolzano&rsquo;s new approach and in the work he defined what
is now called a Cauchy sequence. The concept appears in Cauchy&rsquo;s work four years later
but it is unlikely that Cauchy had read Bolzano&rsquo;s work.
After 1817, Bolzano published no further mathematical works for many years. Between
the late 1820s and the 1840s, he worked on a major work Gr&ouml;ssenlehre. This attempt
to put the whole of mathematics on a logical foundation was published in parts, while
Bolzano hoped that his students would finish and publish the complete work.
His work Paradoxien des Unendlichen, a study of paradoxes of the infinite, was published
in 1851, three years after his death, by one of his students. The word &ldquo;set&rdquo; appears here
for the first time. In this work Bolzano gives examples of 1&ndash;1 correspondences between
the elements of an infinite set and the elements of a proper subset.
Bolzano&rsquo;s theories of mathematical infinity anticipated Georg Cantor&rsquo;s theory of infinite
sets. It is also remarkable that he gave a function which is nowhere differentiable yet
everywhere continuous.</p>
<p/>
</div>
<div class="page"><p/>
<p>17.6 Spectral Theorem for Compact Operators 527
</p>
<p>17.5.1 Spectrum of Compact Operators
</p>
<p>Our next task is to investigate the spectrum σ(K) of a compact operator K
on a Hilbert space H. We are particularly interested in the set of eigenvalues
and eigenvectors of compact operators. Recall that every eigenvalue of an
operator on a vector space of finite dimension is in its spectrum, and that
every point of the spectrum is an eigenvalue (see p. 518). In general, the
second statement is not true. In fact, we saw that the right-shift operator had
no eigenvalue at all, yet its spectrum was the entire unit disk of the complex
plane.
</p>
<p>We first observe that 0 &isin; σ(K), because otherwise 0 &isin; ρ(K), which im-
plies that K = K &minus; 01 is invertible with inverse K&minus;1. The product of two
compact operators (in fact, the product of a compact and a bounded opera-
tor) is compact (see Example 17.5.3). This yields a contradiction8 because
the unit operator cannot be compact: It maps a bounded sequence to itself,
not to a sequence with a convergent subsequence.
</p>
<p>The next theorem, whose proof can be found in [DeVi 90], characterizes
the spectrum of a compact operator completely.
</p>
<p>Theorem 17.5.11 Let K be a compact operator on an infinite-dimensional
Hilbert space H. Then
</p>
<p>1. 0 &isin; σ(K).
2. Each nonzero point of σ(K) is an eigenvalue of K whose eigenspace is
</p>
<p>finite-dimensional.
3. σ(K) is either a finite set or it is a sequence that converges to zero.
</p>
<p>17.6 Spectral Theorem for Compact Operators
</p>
<p>The finite-dimensional spectral decomposition theorem of Chap. 6 was
based on the existence of eigenvalues, eigenspaces, and projection opera-
tors. Such existence was guaranteed by the existence of an inner product for
any finite-dimensional vector space. The task of establishing spectral de-
composition for infinite-dimensional vector spaces is complicated not only
by the possibility of the absence of an inner product, but also by the ques-
tions of completeness, closure, and convergence. One can eliminate the first
two hindrances by restricting oneself to a Hilbert space. However, even so,
one has to deal with other complications of infinite dimensions.
</p>
<p>As an example, consider the relation V = W &oplus; W&perp;, which is trivially
true for any subspace W in finite dimensions once an orthonormal basis is
chosen. Recall that the procedure for establishing this relation is to comple-
ment a basis of W to produce a basis for the whole space. In an infinite-
dimensional Hilbert space, we do not know a priori how to complement the
</p>
<p>8Our conclusion is valid only in infinite dimensions. In finite dimensions, all operators,
including 1, are compact.</p>
<p/>
</div>
<div class="page"><p/>
<p>528 17 Introductory Operator Theory
</p>
<p>Fig. 17.1 The shaded area represents a convex subset of the vector space. It consists
of vectors whose tips lie in the shaded region. It is clear that there is a (unique) vector
belonging to the subset whose length is minimum
</p>
<p>Fig. 17.2 The shaded area represents the subspace M of the vector space. The convex
subset E consists of all vectors connecting points of M to the tip of |u〉. It is clear that
there is a (unique) vector belonging to E whose length is minimum. The figure shows
that this vector is orthogonal to M
</p>
<p>basis of a subspace (which may be infinite-dimensional). Thus, one has to
prove the existence of the orthogonal complement of a subspace. Without
going into details, we sketch the proof. First a definition:
</p>
<p>Definition 17.6.1 A convex subset E of a vector space is a collection ofconvex subset
vectors such that if |u〉 and |v〉 are in E, then |u〉 &minus; t (|u〉 &minus; |v〉) is also in E
for all 0 &le; t &le; 1.
</p>
<p>Intuitively, any two points of a convex subset can be connected by a
straight line segment lying entirely in the subset.
</p>
<p>Let E be a closed convex subset (not a subspace) of a Hilbert space H.
One can show that there exists a unique vector in E with minimal norm (see
Fig. 17.1). Now let M be a subspace of H. For an arbitrary vector |u〉 in
H, consider the subset E = |u〉 &minus;M, i.e., all vectors of the form |u〉 &minus; |m〉
with |m〉 &isin;M. It is easily shown that E is a closed convex set. Denote the
unique vector of minimal norm of |u〉 &minus;M by |u〉 &minus; |Pu〉 with |Pu〉 &isin;M.
One can show that |u〉 &minus; |Pu〉 is orthogonal to |m〉 for all |m〉 &isin; M, i.e.,
(|u〉 &minus; |Pu〉) &isin; M&perp; (see Fig. 17.2). Obviously, only the zero vector can
be simultaneously in M and M&perp;. Furthermore, any vector |u〉 in H can be
written as |u〉 = |Pu〉+(|u〉&minus;|Pu〉) with |Pu〉 &isin;M and (|u〉&minus;|Pu〉) &isin;M&perp;.
This shows that H =M&oplus;M&perp;. In words, a Hilbert space is the direct sum</p>
<p/>
</div>
<div class="page"><p/>
<p>17.6 Spectral Theorem for Compact Operators 529
</p>
<p>of any one of its subspaces and the orthogonal complement of that subspace.
The vector |Pu〉 so constructed is the projection of |u〉 in M.
</p>
<p>A projection operator P can be defined as a linear operator with the prop-
erty that P2 = P. One can then show the following.
</p>
<p>Theorem 17.6.2 The kernel kerP of a projection operator is the orthogonal
complement of the range P(H) of P in H iff P is hermitian.
</p>
<p>17.6.1 Compact Hermitian Operator
</p>
<p>We now concentrate on the compact operators, and first look at hermitian
compact operators. We need two lemmas:
</p>
<p>Lemma 17.6.3 Let H &isin; B(H) be a bounded hermitian operator on the
Hilbert space H. Then ‖H‖ = max{|〈Hx|x〉| | ‖x‖ = 1}.
</p>
<p>Proof Let M denote the positive number on the RHS. From the definition
of the norm of an operator, we easily obtain |〈Hx|x〉| &le; ‖H‖‖x‖2 = ‖H‖, or
M &le; ‖H‖. For the reverse inequality, see Problem 17.6. �
</p>
<p>Lemma 17.6.4 Let K &isin;B(H) be a hermitian compact operator. Then there
is an eigenvalue λ of K such that |λ| = ‖K‖.
</p>
<p>Proof Let {|xn〉} be a sequence of unit vectors such that
</p>
<p>‖K‖ = lim
∣∣〈Kxn|xn〉
</p>
<p>∣∣.
</p>
<p>This is always possible, as the following argument shows. Let ǫ be a small
positive number. There must exist a unit vector |x1〉 &isin;H such that
</p>
<p>‖K‖ &minus; ǫ =
∣∣〈Kx1|x1〉
</p>
<p>∣∣,
</p>
<p>because otherwise, ‖K‖ &minus; ǫ would be greater than or equal to the norm of
the operator (see Lemma 17.6.3). Similarly, there must exist another (differ-
ent) unit vector |x2〉 &isin;H such that ‖K‖&minus; ǫ/2 = |〈Kx2|x2〉|. Continuing this
way, we construct an infinite sequence of unit vectors {|xn〉} with the prop-
erty ‖K‖&minus; ǫ/n= |〈Kxn|xn〉|. This construction clearly produces the desired
sequence. Note that the argument holds for any hermitian bounded operator;
compactness is not necessary.
</p>
<p>Now define λn &equiv; 〈Kxn|xn〉 and let λ = limλn, so that |λ| = ‖K‖. Com-
pactness of K implies that {|Kxn〉} converges. Let |y〉 &isin; H be the limit of
{|Kxn〉}. Then ‖y‖ = lim‖Kxn‖ &le; ‖K‖‖xn‖ = ‖K‖. On the other hand,
</p>
<p>0 &le; ‖Kxn &minus; λxn‖2 = ‖Kxn‖2 &minus; 2λ〈Kxn|xn〉 + |λ|2.
</p>
<p>Taking the limit and noting that λn and λ are real, we get
</p>
<p>0 &le; lim‖Kxn‖2 &minus; 2λ lim〈Kxn|xn〉 + |λ|2 = ‖y‖2 &minus; 2λ2 + λ2
</p>
<p>&rArr; ‖y‖2 &ge; ‖K‖2.</p>
<p/>
</div>
<div class="page"><p/>
<p>530 17 Introductory Operator Theory
</p>
<p>It follows from these two inequalities that ‖y‖ = ‖K‖ and that lim |xn〉 =
|y〉/λ. Furthermore,
</p>
<p>(K&minus; λ1)
(
|y〉/λ
</p>
<p>)
= (K&minus; λ1)
</p>
<p>(
lim |xn〉
</p>
<p>)
= lim(K&minus; λ1)|xn〉 = 0
</p>
<p>Therefore, λ is an eigenvalue of K with eigenvector |y〉/λ and |λ| = ‖K‖. �
</p>
<p>Arrange all the eigenvalues of Theorem 17.5.11 in the order of decreas-
ing absolute value. Let Mn denote the (finite-dimensional) eigenspace cor-
responding to eigenvalue λn, and Pn the projection to Mn. The eigenspaces
are pairwise orthogonal and PnPm = 0 for m �= n. This follows in exact
analogy with the finite-dimensional case.
</p>
<p>First assume that K has only finitely many eigenvalues,
</p>
<p>|λ1| &ge; |λ2| &ge; &middot; &middot; &middot; &ge; |λr |&gt; 0.
</p>
<p>Let
</p>
<p>M&equiv;M1 &oplus;M2 &oplus; &middot; &middot; &middot; &oplus;Mr &equiv;
r&sum;
</p>
<p>j=1
&oplus;Mj &equiv;
</p>
<p>r&oplus;
</p>
<p>j=1
Mj
</p>
<p>and let M0 be the orthogonal complement of M. Since each eigenspace is
invariant under K, so is M. Therefore, by Theorem 6.1.6&mdash;which holds for
infinite-dimensional vector spaces as well&mdash;and the fact that K is hermitian,
M0 is also invariant. Let K0 be the restriction of K to M0. By Lemma 17.6.4,
K0 has an eigenvalue λ such that |λ| = ‖K0‖. If λ �= 0, it must be one of the
eigenvalues already accounted for, because any eigenvalue of K0 is also an
eigenvalue of K. This is impossible, because M0 is orthogonal to all the
eigenspaces. So, λ= 0, or |λ| = ‖K0‖ = 0, or K0 = 0, i.e., K acts as the zero
operator on M0.
</p>
<p>Let P0 be the orthogonal projection on M0. Then H=
&sum;r
</p>
<p>j=0 &oplus;Mj , and
we have 1=&sum;rj=0 Pj , and for an arbitrary |x〉 &isin;H, we have
</p>
<p>K|x〉 = K
(
</p>
<p>r&sum;
</p>
<p>j=0
Pj |x〉
</p>
<p>)
=
</p>
<p>r&sum;
</p>
<p>j=0
K
(
Pj |x〉
</p>
<p>)
=
</p>
<p>r&sum;
</p>
<p>j=1
λj
</p>
<p>(
Pj |x〉
</p>
<p>)
.
</p>
<p>It follows that K =&sum;rj=1 λjPj . Notice that the range of K is
&sum;r
</p>
<p>j=1 &oplus;Mj ,
which is finite-dimensional. Thus, K has finite rank. Barring some technical
details, which we shall not reproduce here, the case of a compact hermitian
operator with infinitely many eigenvalues goes through in the same way (see
[DeVi 90, pp. 179&ndash;180]):
</p>
<p>Theorem 17.6.5 (Spectral Theorem: Compact Hermitian Operators) Letspectral theorem for
compact hermitian
</p>
<p>operators
</p>
<p>K be a compact hermitian operator on a Hilbert space H. Let {λj }Nj=1 be
the distinct nonzero eigenvalues of K arranged in decreasing order of ab-
solute values. For each j let Mj be the eigenspace of K corresponding to
eigenvalue λj and Pj its projection operator with the property PiPj = 0 for
i �= j . Then:</p>
<p/>
</div>
<div class="page"><p/>
<p>17.6 Spectral Theorem for Compact Operators 531
</p>
<p>1. If N &lt;&infin;, then K is an operator of finite rank, K =&sum;Nj=1 λjPj , and
H = M0 &oplus;M1 &oplus; &middot; &middot; &middot; &oplus;MN , or 1 =
</p>
<p>&sum;N
j=0 Pj , where M0 is infinite-
</p>
<p>dimensional.
2. If N =&infin;, then λj &rarr; 0 as j &rarr;&infin;, K=
</p>
<p>&sum;&infin;
j=1 λjPj , and H =M0 &oplus;&sum;&infin;
</p>
<p>j=1 &oplus;Mj , or 1 =
&sum;&infin;
</p>
<p>j=0 Pj , where M0 could be finite- or infinite-
dimensional. Furthermore,
</p>
<p>∥∥∥∥∥K&minus;
m&sum;
</p>
<p>j=1
λjPj
</p>
<p>∥∥∥∥∥= |λm+1| &forall;m,
</p>
<p>which shows that the infinite series above converges for an operator
norm.
</p>
<p>The eigenspaces of a compact hermitian operator are orthogonal and, by
(2) of Theorem 17.6.5, span the entire space. By the Gram&ndash;Schmidt process,
one can select an orthonormal basis for each eigenspace. We therefore have
the following corollary.
</p>
<p>Corollary 17.6.6 If K is a compact hermitian operator on a Hilbert space
H, then the eigenvectors of K constitute an orthonormal basis for H.
</p>
<p>Theorem 17.6.7 Let K be a compact hermitian operator on a Hilbert space
H and let K = &sum;Nj=1 λjPj , where N could be infinite. A bounded linear
operator on H commutes with K if and only if it commutes with every Pj .
</p>
<p>Proof The &ldquo;if&rdquo; part is straightforward. So assume that the bounded oper-
ator T commutes with K. For |x〉 &isin; Mj , we have (K &minus; λj )T|x〉 = T(K &minus;
λj )|x〉 = 0. Similarly, (K &minus; λj )T&dagger;|x〉 = T&dagger;(K &minus; λj )|x〉 = 0, because 0 =
[T,K]&dagger; = [T&dagger;,K]. These equations show that both T and T&dagger; leave Mj invari-
ant. This means that Mj reduces T, and by Theorem 6.1.8, TPj = PjT. �
</p>
<p>17.6.2 Compact Normal Operator
</p>
<p>Next we prove the spectral theorem for a normal operator. Recall that
any operator T can be written as T = X + iY where X = 12 (T + T&dagger;) and
Y = 12i (T &minus; T&dagger;) are hermitian, and since both T and T&dagger; are compact, X
and Y are compact as well. For normal operators, we have the extra con-
dition that [X,Y] = [T,T&dagger;] = 0. Let X =&sum;Nj=1 λjPj and Y =
</p>
<p>&sum;N
k=1 μkQk
</p>
<p>be the spectral decompositions of X and Y. Using Theorem 17.6.7, it is
straightforward to show that if [X,Y] = 0 then [Pj ,Qk] = 0. Now, since
H = &sum;Nj=0 &oplus;Mj =
</p>
<p>&sum;N
k=0 &oplus;Nk , where Mj are the eigenspaces of X and
</p>
<p>Nk those of Y, we have, for any |x〉 &isin;H,
</p>
<p>X|x〉 =
(
</p>
<p>N&sum;
</p>
<p>j=1
λjPj
</p>
<p>)(
N&sum;
</p>
<p>k=0
Qk|x〉
</p>
<p>)
=
</p>
<p>N&sum;
</p>
<p>j=1
</p>
<p>N&sum;
</p>
<p>k=0
λjPjQk|x〉.</p>
<p/>
</div>
<div class="page"><p/>
<p>532 17 Introductory Operator Theory
</p>
<p>Similarly,
</p>
<p>Y|x〉 = Y
(
</p>
<p>N&sum;
</p>
<p>j=0
Pj |x〉
</p>
<p>)
=
</p>
<p>N&sum;
</p>
<p>k=1
</p>
<p>N&sum;
</p>
<p>j=0
μkQkPj |x〉.
</p>
<p>Combining these two relations and noting that QkPj = PjQk gives
</p>
<p>T|x〉 = (X+ iY)|x〉 =
N&sum;
</p>
<p>j=0
</p>
<p>N&sum;
</p>
<p>k=0
(λj + iμk)PjQk|x〉.
</p>
<p>The projection operators PjQk project onto the intersection of Mj and Nk .
Therefore, Mj &cap;Nk are the eigenspaces of T. Only those terms in the sum
for which Mj &cap;Nk �= &empty; contribute. As before, we can order the eigenvalues
according to their absolute values.
</p>
<p>Theorem 17.6.8 (Spectral Theorem: Compact Normal Operators) Let Tspectral theorem for
compact normal
</p>
<p>operators
</p>
<p>be a compact normal operator on a Hilbert spaceH. Let {λj }Nj=1 (where N
can be &infin;) be the distinct nonzero eigenvalues of T arranged in decreasing
order of absolute values. For each n let Mn be the eigenspace of T corre-
sponding to eigenvalue λn and Pn its projection operator with the property
PmPn = 0 for m �= n. Then:
1. If N &lt; &infin;, then T is an operator of finite rank T = &sum;Nn=1 λjPj , and
</p>
<p>H = M0 &oplus;M1 &oplus; &middot; &middot; &middot; &oplus;MN , or 1 =
&sum;N
</p>
<p>j=0 Pj , where M0 is infinite-
dimensional.
</p>
<p>2. If N =&infin;, then λn &rarr; 0 as n&rarr;&infin;, T=
&sum;&infin;
</p>
<p>n=1 λnPn, and H =M0 &oplus;&sum;&infin;
n=1 &oplus;Mn, or 1 =
</p>
<p>&sum;&infin;
j=0 Pj , where M0 could be finite- or infinite-
</p>
<p>dimensional.
</p>
<p>As in the case of a compact hermitian operator, by the Gram-Schmidt
process, one can select an orthonormal basis for each eigenspace of a normal
operator, in which case we have the following:
</p>
<p>Corollary 17.6.9 If T is a compact normal operator on a Hilbert space H,
then the eigenvectors of T constitute an orthonormal basis for H.
</p>
<p>One can use Theorem 17.6.8 to write any function of a normal operator T
as an expansion in terms of the projection operators of T. First we note that
Tk has λkn as its expansion coefficients. Next, we add various powers of T in
the form of a polynomial and conclude that the expansion coefficients for a
polynomial p(T) are p(λn). Finally, for any function f (T) we have
</p>
<p>f (T)=
&infin;&sum;
</p>
<p>n=1
f (λn)Pn. (17.6)
</p>
<p>Historical Notes
</p>
<p>Johann (John) von Neumann, (1903&ndash;1957), the eldest of three sons of Max von Neu-
mann, a well-to-do Jewish banker, was privately educated until he entered the gymnasium
in 1914. His unusual mathematical abilities soon came to the attention of his teachers,</p>
<p/>
</div>
<div class="page"><p/>
<p>17.6 Spectral Theorem for Compact Operators 533
</p>
<p>who pointed out to his father that teaching him conventional school mathematics would
</p>
<p>Johann (John) von
</p>
<p>Neumann 1903&ndash;1957
</p>
<p>be a waste of time; he was therefore tutored in mathematics under the guidance of uni-
versity professors, and by the age of nineteen he was already recognized as a professional
mathematician and had published his first paper.
Von Neumann was Privatdozent at Berlin from 1927 to 1929 and at Hamburg in 1929&ndash;
1930, then went to Princeton University for three years; in 1933 he was invited to join
the newly opened Institute for Advanced Study, of which he was the youngest permanent
member at that time. At the outbreak of World War II, von Neumann was called upon to
participate in various scientific projects related to the war effort: In particular, from 1943
he was a consultant on the construction of the atomic bomb at Los Alamos. After the
war he retained his membership on numerous government boards and committees, and in
1954 he became a member of the Atomic Energy Commission. His health began to fail in
1955, and he died of cancer two years later.
It is only in comparison with the greatest mathematical geniuses of history that von Neu-
mann&rsquo;s scope in pure mathematics may appear somewhat restricted; it was far beyond the
range of most of his contemporaries, and his extraordinary work in applied mathematics,
in which he certainly equals Gauss, Cauchy, or Poincar&eacute;, more than compensates for its
limitations. Von Neumann&rsquo;s work in pure mathematics was accomplished between 1925
and 1940, when he seemed to be advancing at a breathless speed on all fronts of logic
and analysis at once, not to speak of mathematical physics. The dominant theme in von
Neumann&rsquo;s work is by far his work on the spectral theory of operators in Hilbert spaces.
For twenty years he was the undisputed master in this area, which contains what is now
considered his most profound and most original creation, the theory of rings of operators.
The first papers (1927) in which Hilbert space theory appears are those on the foundations
of quantum mechanics. These investigations later led von Neumann to a systematic study
of unbounded hermitian operators.
Von Neumann&rsquo;s most famous work in theoretical physics is his axiomatization of quantum
mechanics. When he began work in that field in 1927, the methods used by its founders
were hard to formulate in precise mathematical terms: &ldquo;Operators&rdquo; on &ldquo;functions&rdquo; were
handled without much consideration of their domain of definition or their topological
properties, and it was blithely assumed that such &ldquo;operators&rdquo;, when self-adjoint, could
always be &ldquo;diagonalized&rdquo; (as in the finite dimensional case), at the expense of introducing
Dirac delta functions as &ldquo;eigenvectors&rdquo;. Von Neumann showed that mathematical rigor
could be restored by taking as basic axioms the assumptions that the states of a physical
system were points of a Hilbert space and that the measurable quantities were Hermitian
(generally unbounded) operators densely defined in that space.
After 1927 von Neumann also devoted much effort to more specific problems of quantum
mechanics, such as the problem of measurement and the foundation of quantum statis-
tics and quantum thermodynamics, proving in particular an ergodic theorem for quan-
tum systems. All this work was developed and expanded in Mathematische Grundlagen
der Quantenmechanik (1932), in which he also discussed the much-debated question of
&ldquo;causality&rdquo; versus &ldquo;indeterminacy&rdquo; and concluded that no introduction of &ldquo;hidden param-
eters&rdquo; could keep the basic structure of quantum theory and restore &ldquo;causality&rdquo;.
Von Neumann&rsquo;s uncommon grasp of applied mathematics, treated as a whole without
divorcing theory from experimental realization, was nowhere more apparent than in his
work on computers. He became interested in numerical computations in connection with
the need for quick estimates and approximate results that developed with the technology
used for the war effort&mdash;particularly the complex problems of hydrodynamics&mdash;and the
completely new problems presented by the harnessing of nuclear energy, for which no
ready-made theoretical solutions were available. Von Neumann&rsquo;s extraordinary ability for
rapid mental calculation was legendary. The story is told of a friend who brought him a
simple kinematics problem. Two trains, a certain given distance apart, move toward each
other at a given speed. A fly, initially on the windshield of one of the trains, flies back
and forth between them, again at a known constant speed. When the trains collide, how
far has the fly traveled? One way to solve the problem is to add up all the successively
smaller distances in each individual flight. (The easy way is to multiply the fly&rsquo;s speed by
the time elapsed until the crash.) After a few seconds of thought, von Neumann quickly
gave the correct answer.
&ldquo;That&rsquo;s strange,&rdquo; remarked his friend, &ldquo;Most people try to sum the infinite series.&rdquo;
&ldquo;What&rsquo;s strange about that?&rdquo; von Neumann replied. &ldquo;That&rsquo;s what I did.&rdquo;</p>
<p/>
</div>
<div class="page"><p/>
<p>534 17 Introductory Operator Theory
</p>
<p>In closing this section, let us remark that the paradigm of compact op-
erators, namely the Hilbert-Schmidt operator, is such because it is defined
on the finite rectangle [a, b] &times; [a, b]. If this rectangle grows beyond limit,
or equivalently, if the Hilbert space is L2(R&infin;), where R&infin; is some infinite
region of the real line, then the compactness property breaks down, as the
following example illustrates.
</p>
<p>Example 17.6.10 Consider the two kernels
</p>
<p>K1(x, t)= e&minus;|x&minus;t | and K2(x, t)= sinxt
</p>
<p>where the first one acts on L2(&minus;&infin;,&infin;) and the second one on L2(0,&infin;).
One can show (see Problem 17.7) that these two kernels have, respectively,
the two eigenfunctions
</p>
<p>eiαt , α &isin;R, and
&radic;
π
</p>
<p>2
eat + t
</p>
<p>a2 + t2 , a &gt; 0,
</p>
<p>corresponding to the two eigenvalues
</p>
<p>λ= 2
1 + α2 , α &isin;R, and λ=
</p>
<p>&radic;
π
</p>
<p>2
.
</p>
<p>We see that in the first case, all real numbers between 0 and 2 are eigenval-
ues, rendering this set uncountable. In the second case, there are infinitely
(in fact, uncountably) many eigenvectors (one for each a) corresponding to
the single eigenvalue
</p>
<p>&radic;
π/2. Note, however, that in the first case the eigen-
</p>
<p>functions and in the second case the kernel have infinite norms.
</p>
<p>17.7 Resolvents
</p>
<p>The discussion of the preceding section showed that the spectrum of a nor-
mal compact operator is countable. Removing the compactness property in
general will remove countability, as shown in Example 17.6.10. We have
also seen that the right-shift operator, a bounded operator, has uncountably
many points in its spectrum. We therefore expect that the sums in Theo-
rem 17.6.8 should be replaced by integrals in the spectral decomposition
theorem for (noncompact) bounded operators. We shall not discuss the spec-
tral theorem for general operators. However, one special class of noncom-
pact operators is essential for the treatment of Sturm-Liouville theory (to be
studied in Chap. 19). For these operators, the concept of resolvent will be
used, which we develop in this section. This concept also makes a connec-
tion between the countable (algebraic) and the uncountable (analytic) cases.
</p>
<p>Definition 17.7.1 Let T be an operator and λ &isin; ρ(T). The operator Rλ(T)&equiv;resolvent of an operator
(T&minus; λ1)&minus;1 is called the resolvent of T at λ.</p>
<p/>
</div>
<div class="page"><p/>
<p>17.7 Resolvents 535
</p>
<p>Two important properties of the resolvent are useful in analyzing the
spectrum of operators. Let us assume that λ,μ &isin; ρ(T), λ �= μ, and take
the difference between their resolvents. Problem 17.8 shows how to obtain
the following relation:
</p>
<p>Rλ(T)&minus; Rμ(T)= (λ&minus;μ)Rλ(T)Rμ(T). (17.7)
</p>
<p>To obtain the second property of the resolvent, we formally (and indefi-
nitely) differentiate Rλ(T) with respect to λ and evaluate the result at λ= μ:
</p>
<p>d
</p>
<p>dλ
Rλ(T)=
</p>
<p>d
</p>
<p>dλ
</p>
<p>[
(T&minus; λ1)&minus;1
</p>
<p>]
= (T&minus; λ1)&minus;2 = R2λ(T).
</p>
<p>Differentiating both sides of this equation, we get 2R3λ(T), and in general,
</p>
<p>dn
</p>
<p>dλn
Rλ(T)= n!Rn+1λ (T) &rArr;
</p>
<p>dn
</p>
<p>dλn
Rλ(T)
</p>
<p>∣∣∣
λ=μ
</p>
<p>= n!Rn+1μ (T).
</p>
<p>Assuming that the Taylor series expansion exists, we may write
</p>
<p>Rλ(T)=
&infin;&sum;
</p>
<p>n=0
</p>
<p>(λ&minus;μ)n
n!
</p>
<p>dn
</p>
<p>dλn
Rλ(T)
</p>
<p>∣∣∣
λ=μ
</p>
<p>=
&infin;&sum;
</p>
<p>n=0
(λ&minus;μ)nRn+1μ (T), (17.8)
</p>
<p>which is the second property of the resolvent.
We now look into the spectral decomposition from an analytical view-
</p>
<p>point. For convenience, we concentrate on the finite-dimensional case and
let A be an arbitrary (not necessarily hermitian) N &times;N matrix. Let λ be a
complex number that is larger (in absolute value) than any of the eigenval-
ues of A. Since all operators on finite-dimensional vector spaces are compact
(by Proposition 17.5.5), Lemma 17.6.4 assures us that |λ| &gt; ‖T‖, and it is
then possible to expand Rλ(T) in a convergent power series as follows:
</p>
<p>Rλ(A)= (A &minus; λ1)&minus;1 =&minus;
1
</p>
<p>λ
</p>
<p>&infin;&sum;
</p>
<p>n=0
</p>
<p>(
A
</p>
<p>λ
</p>
<p>)n
. (17.9)
</p>
<p>This is the Laurent expansion of Rλ(A). We can immediately read off the
residue of Rλ(A) (the coefficient of 1/λ):
</p>
<p>Res
[
Rλ(A)
</p>
<p>]
=&minus;1 &rArr; &minus; 1
</p>
<p>2πi
</p>
<p>˛
</p>
<p>Γ
</p>
<p>Rλ(A) dλ= 1,
</p>
<p>where Γ is a circle with its center at the origin and a radius large enough to
encompass all the eigenvalues of A [see Fig. 17.3(a)]. A similar argument
shows that
</p>
<p>&minus; 1
2πi
</p>
<p>˛
</p>
<p>Γ
</p>
<p>λRλ(A) dλ= A,
</p>
<p>and in general,
</p>
<p>&minus; 1
2πi
</p>
<p>˛
</p>
<p>Γ
</p>
<p>λnRλ(A) dλ= An for n= 0,1, . . .</p>
<p/>
</div>
<div class="page"><p/>
<p>536 17 Introductory Operator Theory
</p>
<p>Fig. 17.3 (a) The large circle encompassing all eigenvalues. (b) the deformed contour
consisting of small circles orbiting the eigenvalues
</p>
<p>Using this and assuming that we can expand the function f (A) in a power
series, we get
</p>
<p>&minus; 1
2πi
</p>
<p>˛
</p>
<p>Γ
</p>
<p>f (λ)Rλ(A) dλ= f (A). (17.10)
</p>
<p>Writing this equation in the form
</p>
<p>1
</p>
<p>2πi
</p>
<p>˛
</p>
<p>Γ
</p>
<p>f (λ)
</p>
<p>λ1 &minus; A dλ= f (A)
</p>
<p>makes it recognizable as the generalization of the Cauchy integral formula
to operator-valued functions.
</p>
<p>To use any of the above integral formulas, we must know the analytic
behavior of Rλ(A). From the formula of the inverse of a matrix given in
Chap. 5, we have
</p>
<p>[
Rλ(A)
</p>
<p>]
jk
</p>
<p>=
[
(A &minus; λ1)&minus;1
</p>
<p>]
jk
</p>
<p>= Cjk(λ)
det(A &minus; λ1) =
</p>
<p>Cjk(λ)
</p>
<p>p(λ)
,
</p>
<p>where Cjk(λ) is the cofactor of the ij th element of the matrix A &minus; λ1 and
p(λ) is the characteristic polynomial of A. Clearly, Cjk(λ) is also a polyno-
mial. Thus, [Rλ(A)]jk is a rational function of λ. It follows that Rλ(A) has
only poles as singularities (see Proposition 11.2.2). The poles are simply
the zeros of the denominator, i.e., the eigenvalues of A. We can deform the
contour Γ in such a way that it consists of small circles γj that encircle the
isolated eigenvalues λj [see Fig. 17.3(b)]. Then, with f (A)= 1, Eq. (17.10)
yields
</p>
<p>1 =&minus; 1
2πi
</p>
<p>r&sum;
</p>
<p>j=1
</p>
<p>˛
</p>
<p>γj
</p>
<p>Rλ(A) dλ=
r&sum;
</p>
<p>j=1
Pj , Pj &equiv;&minus;
</p>
<p>1
</p>
<p>2πi
</p>
<p>˛
</p>
<p>γj
</p>
<p>Rλ(A) dλ.
</p>
<p>(17.11)
It can be shown (see Example 17.7.2 below) that {Pj } is a set of orthog-
onal projection operators. Thus, Eq. (17.11) is a resolution of identity, as
specified in the spectral decomposition theorem in Chap. 6.</p>
<p/>
</div>
<div class="page"><p/>
<p>17.7 Resolvents 537
</p>
<p>Example 17.7.2 We want to show that the Pj are projection operators. First
let i = j . Then9
</p>
<p>P2j =
(
&minus; 1
</p>
<p>2πi
</p>
<p>)2 ˛
</p>
<p>γj
</p>
<p>Rλ(A) dλ
</p>
<p>˛
</p>
<p>γj
</p>
<p>Rμ(A) dμ.
</p>
<p>Note that λ need not be equal to μ. In fact, we are free to choose |λ&minus; λj |&gt;
|μ&minus; λj |, i.e., let the circle corresponding to λ integration be outside that of
μ integration.10 We can then rewrite the above double integral as
</p>
<p>P2j =
(
&minus; 1
</p>
<p>2πi
</p>
<p>)2 ˛
</p>
<p>γ
(λ)
j
</p>
<p>˛
</p>
<p>γ
(μ)
j
</p>
<p>Rλ(A)Rμ(A) dλdμ
</p>
<p>=
(
&minus; 1
</p>
<p>2πi
</p>
<p>)2 ˛
</p>
<p>γ
(λ)
j
</p>
<p>˛
</p>
<p>γ
(μ)
j
</p>
<p>[
Rλ(A)
</p>
<p>λ&minus;μ &minus;
Rμ(A)
</p>
<p>λ&minus;μ
</p>
<p>]
dλdμ
</p>
<p>=
(
&minus; 1
</p>
<p>2πi
</p>
<p>)2{˛
</p>
<p>γ
(λ)
j
</p>
<p>Rλ(A) dλ
</p>
<p>˛
</p>
<p>γ
(μ)
j
</p>
<p>dμ
</p>
<p>λ&minus;μ
</p>
<p>&minus;
˛
</p>
<p>γ
(μ)
j
</p>
<p>Rμ(A) dμ
</p>
<p>˛
</p>
<p>γ
(λ)
j
</p>
<p>dλ
</p>
<p>λ&minus;μ
</p>
<p>}
,
</p>
<p>where we used Eq. (17.7) to go to the second line. Now note that
˛
</p>
<p>γ
(μ)
j
</p>
<p>dμ
</p>
<p>λ&minus;μ = 0 and
˛
</p>
<p>γ
(λ)
j
</p>
<p>dλ
</p>
<p>λ&minus;μ = 2πi
</p>
<p>because λ lies outside γ (μ)j and μ lies inside γ
(λ)
j . Hence,
</p>
<p>P2j =
(
&minus; 1
</p>
<p>2πi
</p>
<p>)2{
0 &minus; 2πi
</p>
<p>˛
</p>
<p>γ
(μ)
j
</p>
<p>Rμ(A) dμ
</p>
<p>}
=&minus; 1
</p>
<p>2πi
</p>
<p>˛
</p>
<p>γ
(μ)
j
</p>
<p>Rμ(A) dμ= Pj .
</p>
<p>The remaining part, namely PjPk = 0 for k �= j , can be done similarly (see
Problem 17.9).
</p>
<p>Now we let f (A) = A in Eq. (17.10), deform the contour as above, and
write
</p>
<p>A =&minus; 1
2πi
</p>
<p>r&sum;
</p>
<p>j=1
</p>
<p>˛
</p>
<p>γj
</p>
<p>λRλ(A) dλ
</p>
<p>=&minus; 1
2πi
</p>
<p>r&sum;
</p>
<p>j=1
</p>
<p>[
λj
</p>
<p>˛
</p>
<p>γj
</p>
<p>Rλ(A) dλ+
˛
</p>
<p>γj
</p>
<p>(λ&minus; λj )Rλ(A) dλ
]
</p>
<p>9We have not discussed multiple integrals of complex functions. A rigorous study of such
integrals involves the theory of functions of several complex variables&mdash;a subject we
have to avoid due to lack of space. However, in the simple case at hand, the theory of real
multiple integrals is an honest guide.
10This is possible because the poles are isolated.</p>
<p/>
</div>
<div class="page"><p/>
<p>538 17 Introductory Operator Theory
</p>
<p>&equiv;
r&sum;
</p>
<p>j=1
(λjPj + Dj ), Dj &equiv;
</p>
<p>˛
</p>
<p>γj
</p>
<p>(λ&minus; λj )Rλ(A) dλ. (17.12)
</p>
<p>It can be shown (see Problem 17.10) that
</p>
<p>Dnj =
˛
</p>
<p>γj
</p>
<p>(λ&minus; λj )nRλ(A) dλ.
</p>
<p>In particular, since Rλ(A) has only poles as singularities, there exists a pos-
itive integer m such that Dmj = 0. We have not yet made any assumptions
about A. If we assume that A is hermitian, for example, then Rλ(A) will
have simple poles (see Problem 17.11). It follows that (λ&minus; λj )Rλ(A) will
be analytic at λj for all j = 1,2, . . . , r , and Dj = 0 in Eq. (17.12). We thus
have
</p>
<p>A =
r&sum;
</p>
<p>j=1
λjPj ,
</p>
<p>which is the spectral decomposition discussed in Chap. 6. Problem 17.12
shows that the Pj are hermitian.
</p>
<p>Example 17.7.3 The most general 2 &times; 2 hermitian matrix is of the form
</p>
<p>A =
(
a11 a12
a&lowast;12 a22
</p>
<p>)
,
</p>
<p>where a11 and a22 are real numbers. Thus,
</p>
<p>det(A &minus; λ1)= λ2 &minus; (a11 + a22)λ+ a11a22 &minus; |a12|2
</p>
<p>which has roots
</p>
<p>λ1 =
1
</p>
<p>2
</p>
<p>[
a11 + a22 &minus;
</p>
<p>&radic;
(a11 &minus; a22)2 + 4|a12|2
</p>
<p>]
,
</p>
<p>λ2 =
1
</p>
<p>2
</p>
<p>[
a11 + a22 +
</p>
<p>&radic;
(a11 &minus; a22)2 + 4|a12|2
</p>
<p>]
.
</p>
<p>The inverse of A &minus; λ1 can immediately be written:
</p>
<p>Rλ(A)= (A &minus; λ1)&minus;1 =
1
</p>
<p>det(A &minus; λ1)
</p>
<p>(
a22 &minus; λ &minus;a12
&minus;a&lowast;12 a11 &minus; λ
</p>
<p>)
</p>
<p>= 1
(λ&minus; λ1)(λ&minus; λ2)
</p>
<p>(
a22 &minus; λ &minus;a12
&minus;a&lowast;12 a11 &minus; λ
</p>
<p>)
.
</p>
<p>We want to verify that Rλ(A) has only simple poles. Two cases arise:
</p>
<p>1. If λ1 �= λ2, then it is clear that Rλ(A) has simple poles.
2. If λ1 = λ2, it appears that Rλ(A) may have a pole of order 2. However,
</p>
<p>note that if λ1 = λ2, then the square roots in the above equations must
vanish. This happens iff a11 = a22 &equiv; a and a12 = 0. It then follows that</p>
<p/>
</div>
<div class="page"><p/>
<p>17.8 Problems 539
</p>
<p>λ1 = λ2 &equiv; a, and
</p>
<p>Rλ(A)=
1
</p>
<p>(λ&minus; a)2
(
a &minus; λ 0
</p>
<p>0 a &minus; λ
</p>
<p>)
.
</p>
<p>This clearly shows that Rλ(A) has only simple poles in this case.
</p>
<p>If A is not hermitian, Dj �= 0; however, Dj is nevertheless nilpotent (see
Definition 3.5.1). This property and Eq. (17.12) can be used to show that
A can be cast into a Jordan canonical form via a similarity transformation.
That is, there exists an N &times;N matrix S such that
</p>
<p>SAS&minus;1 = J =
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>J1 0 0 . . . 0
</p>
<p>0 J2 0 . . . 0
...
</p>
<p>...
...
</p>
<p>...
</p>
<p>0 0 0 . . . Jm
</p>
<p>⎞
⎟⎟⎟⎠
</p>
<p>where Jk is a matrix of the form Jordan canonical form
</p>
<p>Jk =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>λ 1 0 0 . . . 0 0
0 λ 1 0 . . . 0 0
0 0 λ 1 . . . 0 0
...
</p>
<p>...
...
</p>
<p>...
...
</p>
<p>...
</p>
<p>0 0 0 0 . . . λ 1
0 0 0 0 . . . 0 λ
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>in which λ is one of the eigenvalues of A. Different Jk may contain the same
eigenvalues of A. For a discussion of the Jordan canonical form of a matrix,
see [Birk 77], [Denn 67], or [Halm 58].
</p>
<p>17.8 Problems
</p>
<p>17.1 Suppose that S is a bounded operator, T an invertible operator, and that
</p>
<p>‖T&minus; S‖&lt; 1
‖T&minus;1‖
</p>
<p>.
</p>
<p>Show that S is invertible. Hint: Show that T&minus;1S is invertible. Thus, an oper-
ator that is &ldquo;sufficiently close&rdquo; to an invertible operator is invertible.
</p>
<p>17.2 Let V and W be finite-dimensional vector spaces. Show that T &isin;
L(V,W) is necessarily bounded.
</p>
<p>17.3 Let H be a Hilbert space, and T &isin; L(H) an isometry, i.e., a linear
operator that does not change the norm of any vector. Show that ‖T‖ = 1.
</p>
<p>17.4 Show that
</p>
<p>(a) the unit operator is not compact, and that</p>
<p/>
</div>
<div class="page"><p/>
<p>540 17 Introductory Operator Theory
</p>
<p>(b) the inverse of a compact operator cannot be bounded.
</p>
<p>Hint: For (b) use the results of Example 17.5.3.
</p>
<p>17.5 Let |u〉 &isin;H and let M be a subspace of H. Show that the subset E =
|u〉 &minus;M is convex. Show that E is not necessarily a subspace of H.
</p>
<p>17.6 Show that for any hermitian operator H, we have
</p>
<p>4〈Hx|y〉 = 〈H(x + y)|x + y〉 &minus; 〈H(x &minus; y)|x &minus; y〉
+ i
</p>
<p>[
〈H(x + iy)|x + iy〉 &minus; 〈H(x &minus; iy)|x &minus; iy〉
</p>
<p>]
.
</p>
<p>Now let |x〉 = λ|z〉 and |y〉 = |Hz〉/λ, where λ= (‖Hz‖/‖z‖)1/2, and show
that
</p>
<p>‖Hz‖2 = 〈Hx|y〉 &le;M‖z‖‖Hz‖,
where M = max{|〈Hz|z〉|/|‖z‖2}. Now conclude that ‖H‖ &le;M .
</p>
<p>17.7 Show that the two kernels K1(x, t) = e&minus;|x&minus;t | and K2(x, t) = sinxt ,
where the first one acts on L2(&minus;&infin;,&infin;) and the second one on L2(0,&infin;),
have the two eigenfunctions
</p>
<p>eiαt , α &isin;R, and
&radic;
π
</p>
<p>2
eat + t
</p>
<p>a2 + t2 , a &gt; 0,
</p>
<p>respectively, corresponding to the two eigenvalues
</p>
<p>λ= 2
1 + α2 , α &isin;R, and λ=
</p>
<p>&radic;
π
</p>
<p>2
.
</p>
<p>17.8 Derive Eq. (17.7). Hint: Multiply Rλ(T) by 1 = Rμ(T)(T &minus; μ1) and
Rμ(T) by 1= Rλ(T)(T&minus; λ1).
</p>
<p>17.9 Finish Example 17.7.2 by showing that PjPk = 0 for k �= j .
</p>
<p>17.10 Show that Dnj =
&cedil;
</p>
<p>γj
(λ&minus; λj )nRλ(A) dλ. Hint: Use mathematical in-
</p>
<p>duction and the technique used in Example 17.7.2.
</p>
<p>17.11 (a) Take the inner product of |u〉 = (A&minus;λ1)|v〉 with |v〉 and show that
for a hermitian A, Im〈v|u〉 = &minus;(Imλ)‖v‖2. Now use the Schwarz inequality
to obtain
</p>
<p>‖v‖ &le; ‖u‖|Imλ| &rArr;
∥∥Rλ(A)|u〉
</p>
<p>∥∥&le; ‖u‖|Imλ| .
</p>
<p>(b) Use this result to show that
</p>
<p>∥∥(λ&minus; λj )Rλ(A)|u〉
∥∥&le;
</p>
<p>(
1 +
</p>
<p>∣∣∣∣
Re(λ&minus; λj )
Im(λ&minus; λj )
</p>
<p>∣∣∣∣
)
‖u‖ =
</p>
<p>(
1 + |cot θ |
</p>
<p>)
‖u‖,
</p>
<p>where θ is the angle that λ&minus; λj makes with the real axis and λ is chosen to
have an imaginary part. From this result conclude that Rλ(A) has a simple
pole when A is hermitian.</p>
<p/>
</div>
<div class="page"><p/>
<p>17.8 Problems 541
</p>
<p>17.12 (a) Show that when A is hermitian, [Rλ(A)]&dagger; = Rλ&lowast;(A).
(b) Write λ&minus; λj = rj eiθ in the definition of Pj in Eq. (17.11). Take the
</p>
<p>hermitian conjugate of both sides and use (a) to show that Pj is hermitian.
Hint: You will have to change the variable of integration a number of times.</p>
<p/>
</div>
<div class="page"><p/>
<p>18Integral Equations
</p>
<p>The beginning of Chap. 17 showed that to solve a vector-operator equation
one transforms it into an equation involving a sum over a discrete index [the
matrix equation of Eq. (17.1)], or an equation involving an integral over
a continuous index [Eq. (17.2)]. The latter is called an integral equation,
which we shall investigate here using the machinery of Chap. 17.
</p>
<p>18.1 Classification
</p>
<p>Integral equations can be divided into two major groups. Those that have
a variable limit of integration are called Volterra equations; those that Volterra and Fredholm
</p>
<p>equations of first and
</p>
<p>second kind
</p>
<p>have constant limits of integration are called Fredholm equations. If the
unknown function appears only inside the integral, the integral equation is
said to be of the first kind. Integral equations having the unknown function
outside the integral as well as inside are said to be of the second kind. The
four kinds of equations can be written as follows.
</p>
<p>&int; x
</p>
<p>a
</p>
<p>K(x, t)u(t) dt = v(x), Volterra equation of the 1st kind,
&int; b
</p>
<p>a
</p>
<p>K(x, t)u(t) dt = v(x), Fredholm equation of the 1st kind,
</p>
<p>u(x)= v(x)+
&int; x
</p>
<p>a
</p>
<p>K(x, t)u(t) dt, Volterra equation of the 2nd kind,
</p>
<p>u(x)= v(x)+
&int; b
</p>
<p>a
</p>
<p>K(x, t)u(t) dt, Fredholm equation of the 2nd kind.
</p>
<p>In all these equations, K(x, t) is called the kernel of the integral equation. kernel of an integral
equationIn the theory of integral equations of the second kind, one usually mul-
</p>
<p>tiplies the integral by a nonzero complex number λ. Thus, the Fredholm
equation of the second kind becomes
</p>
<p>u(x)= v(x)+ λ
&int; b
</p>
<p>a
</p>
<p>K(x, t)u(t) dt, (18.1)
</p>
<p>S. Hassani, Mathematical Physics, DOI 10.1007/978-3-319-01195-0_18,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>543</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-01195-0_18">http://dx.doi.org/10.1007/978-3-319-01195-0_18</a></div>
</div>
<div class="page"><p/>
<p>544 18 Integral Equations
</p>
<p>and for the Volterra equation of the second kind one obtains
</p>
<p>u(x)= v(x)+ λ
&int; x
</p>
<p>a
</p>
<p>K(x, t)u(t) dt. (18.2)
</p>
<p>A λ that satisfies (18.1) or (18.2) with v(x) = 0 is called a characteristiccharacteristic value of an
integral equation value of the integral equation. In the abstract operator language both equa-
</p>
<p>tions are written as
</p>
<p>|u〉 = |v〉 + λK|u〉 &rArr;
(
K&minus; λ&minus;11
</p>
<p>)
|u〉 = &minus;λ&minus;1|v〉. (18.3)
</p>
<p>Thus λ is a characteristic value if and only if λ&minus;1 is an eigenvalue of K.
Recall that when the interval of integration (a, b) is finite, K(x, t) is called
a Hilbert-Schmidt kernel. Example 17.5.10 showed that K is a compact op-
erator, and by Theorem 17.5.11, the eigenvalues of K either form a finite set
or a sequence that converges to zero.
</p>
<p>Theorem 18.1.1 The characteristic values of a Fredholm equation of the
second kind either form a finite set or a sequence of complex numbers in-
creasing beyond limit in absolute value.
</p>
<p>Our main task in this chapter is to study methods of solving integral equa-
tions of the second kind. We treat the Volterra equation first because it is
easier to solve. Let us introduce the notation
</p>
<p>K[u](x)&equiv;
&int; x
</p>
<p>a
</p>
<p>K(x, t)u(t) dt and Kn[u](x)=K
[
Kn&minus;1[u]
</p>
<p>]
(x)
</p>
<p>(18.4)
whereby K[u] denotes a function whose value at x is given by the integral
on the RHS of the first equation in (18.4). One can show with little difficulty
that the associated operator K is compact. Let M = max{|K(x, t)| | a &le; t &le;
x &le; b} and note that
</p>
<p>∣∣λK[u](x)
∣∣=
</p>
<p>∣∣∣∣λ
&int; x
</p>
<p>a
</p>
<p>K(x, t)u(t) dt
</p>
<p>∣∣∣∣&le; |λ||M|‖u‖&infin;(x &minus; a),
</p>
<p>where ‖u‖&infin; &equiv; max{|u(x)| | x &isin; (a, b)}.
Using mathematical induction, one can show that (see Problem 18.1)
</p>
<p>∣∣(λK)n[u](x)
∣∣&le; |λ|n|M|n‖u‖&infin;
</p>
<p>(x &minus; a)n
n! . (18.5)
</p>
<p>Since b &ge; x, we can replace x with b and still satisfy the inequality. Then
the inequality of Eq. (18.5) will hold for all x, and we can write the equa-
tion as an operator norm inequality: ‖(λK)n‖ &le; |λ|n|M|n‖u‖&infin;(b&minus; a)n/n!.
Therefore,
</p>
<p>∥∥∥∥∥
&infin;&sum;
</p>
<p>n=0
(λK)n
</p>
<p>∥∥∥∥∥&le;
&infin;&sum;
</p>
<p>n=0
</p>
<p>∥∥(λK)n
∥∥&le;
</p>
<p>&infin;&sum;
</p>
<p>n=0
</p>
<p>|λ|n|M|n(b&minus; a)n
n! = e
</p>
<p>M|λ|(b&minus;a),</p>
<p/>
</div>
<div class="page"><p/>
<p>18.1 Classification 545
</p>
<p>and the series
&sum;&infin;
</p>
<p>n=0(λK)
n converges for all λ. In fact, a direct calculation
</p>
<p>shows that the series converges to the inverse of 1&minus; λK. Thus, the latter is
invertible and the spectrum of K has no nonzero points. We have just shown
the following.
</p>
<p>Theorem 18.1.2 The Volterra equation of the second kind has no nonzero Volterra equation of the
second kind has a
</p>
<p>unique solution and no
</p>
<p>nonzero characteristic
</p>
<p>value
</p>
<p>characteristic value. In particular, the operator 1 &minus; λK is invertible, the
equation always has a unique solution given by the convergent infinite series
</p>
<p>u(x)=
&infin;&sum;
</p>
<p>j=0
λj
</p>
<p>&int; x
</p>
<p>a
</p>
<p>Kj (x, t)v(t) dt
</p>
<p>where Kj (x, t) is defined inductively in Eq. (18.4).
</p>
<p>Historical Notes
</p>
<p>Vito Volterra (1860&ndash;1940) was only 11 when he became interested in mathematics while
</p>
<p>Vito Volterra 1860&ndash;1940
</p>
<p>reading Legendre&rsquo;s Geometry. At the age of 13 he began to study the three body problem
and made some progress.
His family were extremely poor (his father had died when Vito was two years old) but
after attending lectures at Florence he was able to proceed to Pisa in 1878. At Pisa he stud-
ied under Betti, graduating as a doctor of physics in 1882. His thesis on hydrodynamics
included some results of Stokes, discovered later but independently by Volterra.
He became Professor of Mechanics at Pisa in 1883, and upon Betti&rsquo;s death, he occupied
the chair of mathematical physics. After spending some time at Turin as the chair of
mechanics, he was awarded the chair of mathematical physics at the University of Rome
in 1900.
Volterra conceived the idea of a theory of functions that depend on a continuous set of val-
ues of another function in 1883. Hadamard was later to introduce the word &ldquo;functional&rdquo;,
which replaced Volterra&rsquo;s original terminology. In 1890 Volterra used his functional cal-
culus to show that the theory of Hamilton and Jacobi for the integration of the differential
equations of dynamics could be extended to other problems of mathematical physics.
His most famous work was done on integral equations. He began this study in 1884, and
in 1896 he published several papers on what is now called the Volterra integral equation.
He continued to study functional analysis applications to integral equations producing a
large number of papers on composition and permutable functions.
During the First World War Volterra joined the Air Force. He made many journeys to
France and England to promote scientific collaboration. After the war he returned to the
University of Rome, and his interests moved to mathematical biology. He studied the
Verhulst equation and the logistic curve. He also wrote on predator&ndash;prey equations.
In 1922 Fascism seized Italy, and Volterra fought against it in the Italian Parliament.
However, by 1930 the Parliament was abolished, and when Volterra refused to take an
oath of allegiance to the Fascist government in 1931, he was forced to leave the University
of Rome. From the following year he lived mostly abroad, mainly in Paris, but also in
Spain and other countries.
</p>
<p>Differential equations can be transformed into integral equations. For in-
stance, consider the SOLDE
</p>
<p>d2u
</p>
<p>dx2
+p1(x)
</p>
<p>du
</p>
<p>dx
+p0(x)u= r(x), u(a)= c1, u&prime;(a)= c2. (18.6)
</p>
<p>By integrating the DE once, we obtain
</p>
<p>du
</p>
<p>dx
=&minus;
</p>
<p>&int; x
</p>
<p>a
</p>
<p>p1(t)u
&prime;(t) dt &minus;
</p>
<p>&int; x
</p>
<p>a
</p>
<p>p0(t)u(t) dt +
&int; x
</p>
<p>a
</p>
<p>r(t) dt + c2.</p>
<p/>
</div>
<div class="page"><p/>
<p>546 18 Integral Equations
</p>
<p>Integrating the first integral by parts gives
</p>
<p>u&prime;(x)=&minus;p1(x)u(x)+
&int; x
</p>
<p>a
</p>
<p>[
p&prime;1(t)&minus; p0(t)
</p>
<p>]
u(t) dt
</p>
<p>︸ ︷︷ ︸
&equiv;f (x)
</p>
<p>+
&int; x
</p>
<p>a
</p>
<p>r(t) dt
</p>
<p>︸ ︷︷ ︸
&equiv;g(x)
</p>
<p>+p1(a)c1 + c2.
</p>
<p>Integrating once more yields
</p>
<p>u(x)=&minus;
&int; x
</p>
<p>a
</p>
<p>p1(t)u(t) dt +
&int; x
</p>
<p>a
</p>
<p>f (s) ds +
&int; x
</p>
<p>a
</p>
<p>g(s) ds
</p>
<p>+ (x &minus; a)
[
p1(a)c1 + c2
</p>
<p>]
</p>
<p>=&minus;
&int; x
</p>
<p>a
</p>
<p>p1(t)u(t) dt +
&int; x
</p>
<p>a
</p>
<p>ds
</p>
<p>&int; s
</p>
<p>a
</p>
<p>[
p&prime;1(t)&minus; p0(t)
</p>
<p>]
u(t) dt
</p>
<p>+
&int; x
</p>
<p>a
</p>
<p>ds
</p>
<p>&int; s
</p>
<p>a
</p>
<p>r(t) dt + (x &minus; a)
[
p1(a)c1 + c2
</p>
<p>]
+ c1
</p>
<p>=
&int; x
</p>
<p>a
</p>
<p>{
(x &minus; t)
</p>
<p>[
p&prime;1(t)&minus; p0(t)
</p>
<p>]
&minus; p1(t)
</p>
<p>}
u(t) dt
</p>
<p>+
&int; x
</p>
<p>a
</p>
<p>(x &minus; t)r(t) dt + (x &minus; a)
[
p1(a)c1 + c2
</p>
<p>]
+ c1, (18.7)
</p>
<p>where we have used the formula
&int; x
</p>
<p>a
</p>
<p>ds
</p>
<p>&int; s
</p>
<p>a
</p>
<p>f (t) dt =
&int; x
</p>
<p>a
</p>
<p>(x &minus; t)f (t) dt,
</p>
<p>which the reader may verify by interchanging the order of integration on the
LHS.
</p>
<p>Proposition 18.1.3 A SOLDE of the form (18.6) is equivalent to a Volterra
equation of the second kind with kernel
</p>
<p>K(x, t)&equiv; (x &minus; t)
[
p&prime;1(t)&minus; p0(t)
</p>
<p>]
&minus; p1(t)
</p>
<p>and
</p>
<p>v(x)&equiv;
&int; x
</p>
<p>a
</p>
<p>(x &minus; t)r(t) dt + (x &minus; a)
[
p1(a)c1 + c2
</p>
<p>]
+ c1.
</p>
<p>We now outline a systematic approach to obtaining the infinite series ofNeumann series solution
Theorem 18.1.2, which also works for the Fredholm equation of the sec-
ond kind. In the latter case, the series is guaranteed to converge only if
|λ|‖K‖ &lt; 1. This approach has the advantage that in each successive step,
we obtain a better approximation to the solution. Writing the equation as
</p>
<p>|u〉 = |v〉 + λK|u〉, (18.8)</p>
<p/>
</div>
<div class="page"><p/>
<p>18.1 Classification 547
</p>
<p>we can interpret it as follows. The difference between |u〉 and |v〉 is λK|u〉.
If λK were absent, the two vectors |u〉 and |v〉 would be equal. The effect
of λK is to change |u〉 in such a way that when the result is added to |v〉,
it gives |u〉. As our initial approximation, therefore, we take |u〉 to be equal
to |v〉 and write |u0〉 = |v〉, where the index reminds us of the order (in
this case zeroth, because λK = 0) of the approximation. To find a better
approximation, we always substitute the latest approximation for |u〉 in the
RHS of Eq. (18.8). At this stage, we have |u1〉 = |v〉+λK|u0〉 = |v〉+λK|v〉.
Still a better approximation is achieved if we substitute this expression in
(18.8):
</p>
<p>|u2〉 = |v〉 + λK|u1〉 = |v〉 + λK
(
|v〉 + λK|v〉
</p>
<p>)
= |v〉 + λK|v〉 + λ2K2|v〉.
</p>
<p>The procedure is now clear. Once |un〉, the nth approximation, is obtained,
we can get |un+1〉 by substituting in the RHS of (18.8).
</p>
<p>Before continuing, let us write the above equations in integral form. In
what follows, we shall concentrate on the Fredholm equation. To obtain the
result for the Volterra equation, one simply replaces b, the upper limit of
integration, with x. The first approximation can be obtained by substituting
v(t) for u(t) on the RHS of Eq. (18.1). This yields
</p>
<p>u1(x)= v(x)+ λ
&int; b
</p>
<p>a
</p>
<p>K(x, t)v(t) dt.
</p>
<p>Substituting this back in Eq. (18.1) gives
</p>
<p>u2(x)= v(x)+ λ
&int; b
</p>
<p>a
</p>
<p>dsK(x, s)u1(s)
</p>
<p>= v(x)+ λ
&int; b
</p>
<p>a
</p>
<p>dsK(x, s)v(s)
</p>
<p>+ λ2
&int; b
</p>
<p>a
</p>
<p>dt
</p>
<p>[&int; b
</p>
<p>a
</p>
<p>K(x, s)K(s, t) ds
</p>
<p>]
v(t)
</p>
<p>= v(x)+ λ
&int; b
</p>
<p>a
</p>
<p>dtK(x, t)v(t)+ λ2
&int; b
</p>
<p>a
</p>
<p>dtK2(x, t)v(t),
</p>
<p>where K2(x, t)&equiv;
&int; b
a
K(x, s)K(s, t) ds. Similar expressions can be derived
</p>
<p>for u3(x), u4(x), and so forth. The integrals expressing various &ldquo;powers&rdquo; of
K can be obtained using Dirac notation and vectors with continuous indices,
as discussed in Sect. 7.3. Thus, for instance,
</p>
<p>K3(x, t)&equiv; 〈x|K
(&int; b
</p>
<p>a
</p>
<p>|s1〉〈s1|ds1
)
</p>
<p>︸ ︷︷ ︸
=1
</p>
<p>K
</p>
<p>(&int; b
</p>
<p>a
</p>
<p>|s2〉〈s2|ds2
)
</p>
<p>︸ ︷︷ ︸
=1
</p>
<p>K|t〉
</p>
<p>=
&int; b
</p>
<p>a
</p>
<p>ds1
</p>
<p>&int; b
</p>
<p>a
</p>
<p>ds2〈x|K|s1〉〈s1|K|s2〉〈s2|K|t〉
</p>
<p>=
&int; b
</p>
<p>a
</p>
<p>ds1
</p>
<p>&int; b
</p>
<p>a
</p>
<p>ds2K(x, s1)K(s1, s2)K(s2, t).</p>
<p/>
</div>
<div class="page"><p/>
<p>548 18 Integral Equations
</p>
<p>We can always use this technique to convert an equation in kets into an
equation in functions and integrals. Therefore, we can concentrate on the
abstract operator equation and its various approximations.
</p>
<p>Continuing to the nth-order approximation, we easily obtain
</p>
<p>|un〉 = |v〉 + λK|v〉 + &middot; &middot; &middot; + λnKn|v〉 =
n&sum;
</p>
<p>j=0
(λK)j |v〉, (18.9)
</p>
<p>whose integral form is
</p>
<p>un(x)=
n&sum;
</p>
<p>j=0
λj
</p>
<p>&int; b
</p>
<p>a
</p>
<p>Kj (x, t)v(t) dt. (18.10)
</p>
<p>Here Kj (x, t) is defined inductively by
</p>
<p>K0(x, t)= 〈x|K0|t〉 = 〈x|1|t〉 = 〈x|t〉 = δ(x &minus; t),
</p>
<p>Kj (x, t)= 〈x|KKj&minus;1|t〉 = 〈x|K
(&int; b
</p>
<p>a
</p>
<p>|s〉〈s|ds
)
Kj&minus;1|t〉
</p>
<p>=
&int; b
</p>
<p>a
</p>
<p>K(x, s)Kj&minus;1(s, t) ds.
</p>
<p>The limit of un(x) as n&rarr;&infin; gives
</p>
<p>u(x)=
&infin;&sum;
</p>
<p>j=0
λj
</p>
<p>&int; b
</p>
<p>a
</p>
<p>Kj (x, t)v(t) dt. (18.11)
</p>
<p>The convergence of this series, called the Neumann series, is always guar-Neumann series
anteed for the Volterra equation. For the Fredholm equation, we need to
impose the extra condition |λ|‖K‖&lt; 1.
</p>
<p>Example 18.1.4 As an example, let us find the solution of u(x) = 1 +
λ
&int; x
</p>
<p>0 u(t) dt , a Volterra equation of the second kind. Here, v(x) = 1 and
K(x, t)= 1, and it is straightforward to calculate approximations to u(x):
</p>
<p>u0(x) = v(x)= 1, u1(x)= 1 + λ
&int; x
</p>
<p>0
K(x, t)u0(t) dt = 1 + λx,
</p>
<p>u2(x) = 1 + λ
&int; x
</p>
<p>0
K(x, t)u1(t) dt = 1 + λ
</p>
<p>&int; x
</p>
<p>0
(1 + λt) dt
</p>
<p>= 1 + λx + λ
2x2
</p>
<p>2
.
</p>
<p>It is clear that the nth term will look like
</p>
<p>un(x)= 1 + λx +
λ2x2
</p>
<p>2
+ &middot; &middot; &middot; + λ
</p>
<p>nxn
</p>
<p>n! =
n&sum;
</p>
<p>j=0
</p>
<p>λjxj
</p>
<p>j ! .
</p>
<p>As n &rarr; &infin;, we obtain u(x) = eλx . By direct substitution, it is readily
checked that this is indeed a solution of the original integral equation.</p>
<p/>
</div>
<div class="page"><p/>
<p>18.2 Fredholm Integral Equations 549
</p>
<p>18.2 Fredholm Integral Equations
</p>
<p>We can use our knowledge of compact operators gained in the previous
chapter to study Fredholm equations of the second kind. With λ �= 0 a com-
plex number, we consider the characteristic equation
</p>
<p>(1&minus; λK)|u〉 = |v〉, or u(x)&minus; λK[u](x)= v(x), (18.12)
</p>
<p>where all functions are square-integrable on [a, b], and K(x, t), the Hilbert-
Schmidt kernel, is square-integrable on the rectangle [a, b] &times; [a, b].
</p>
<p>Using Proposition 17.2.9, we immediately see that Eq. (18.12) has a
unique solution if |λ|‖K‖&lt; 1, and the solution is of the form
</p>
<p>|u〉 = (1&minus; λK)&minus;1|v〉 =
&infin;&sum;
</p>
<p>n=0
λnKn|v〉, (18.13)
</p>
<p>or u(x) = &sum;&infin;n=0 λnKn[v](x), where Kn[v](x) is defined as in Eq. (18.4)
except that now b replaces x as the upper limit of integration.
</p>
<p>Example 18.2.1 Consider the integral equation
</p>
<p>u(x)&minus;
&int; 1
</p>
<p>0
K(x, t)u(t) dt = x, where K(x, t)=
</p>
<p>{
x if 0 &le; x &lt; t,
t if t &lt; x &le; 1.
</p>
<p>Here λ = 1; therefore, a Neumann series solution exists if ‖K‖ &lt; 1. It is
convenient to write K in terms of the theta function:1
</p>
<p>K(x, t)= xθ(t &minus; x)+ tθ(x &minus; t). (18.14)
</p>
<p>This gives |K(x, t)|2 = x2θ(t&minus;x)+ t2θ(x&minus; t) because θ2(x&minus; t)= θ(x&minus; t)
and θ(x &minus; t)θ(t &minus; x)= 0. Thus, we have
</p>
<p>‖K‖2 =
&int; 1
</p>
<p>0
dx
</p>
<p>&int; 1
</p>
<p>0
dt
∣∣K(x, t)
</p>
<p>∣∣2
</p>
<p>=
&int; 1
</p>
<p>0
dx
</p>
<p>&int; 1
</p>
<p>0
x2θ(t &minus; x)dt +
</p>
<p>&int; 1
</p>
<p>0
dx
</p>
<p>&int; 1
</p>
<p>0
t2θ(x &minus; t) dt
</p>
<p>=
&int; 1
</p>
<p>0
dt
</p>
<p>&int; t
</p>
<p>0
x2 dx +
</p>
<p>&int; 1
</p>
<p>0
dx
</p>
<p>&int; x
</p>
<p>0
t2 dt
</p>
<p>=
&int; 1
</p>
<p>0
dt
</p>
<p>(
t3
</p>
<p>3
</p>
<p>)
+
&int; 1
</p>
<p>0
dx
</p>
<p>(
x3
</p>
<p>3
</p>
<p>)
= 1
</p>
<p>6
.
</p>
<p>Since this is less than 1, the Neumann series converges, and we have2
</p>
<p>1Recall that the theta function is defined to be 1 if its argument is positive, and 0 if it is
negative.
2Note that in this case (Fredholm equation), we can calculate the j th term in isolation. In
the Volterra case, it was more natural to calculate the solution up to a given order.</p>
<p/>
</div>
<div class="page"><p/>
<p>550 18 Integral Equations
</p>
<p>u(x)=
&infin;&sum;
</p>
<p>j=0
λj
</p>
<p>&int; b
</p>
<p>a
</p>
<p>Kj (x, t)v(t) dt =
&infin;&sum;
</p>
<p>j=0
</p>
<p>&int; 1
</p>
<p>0
Kj (x, t)t dt &equiv;
</p>
<p>&infin;&sum;
</p>
<p>j=0
fj (x).
</p>
<p>The first few terms are evaluated as follows:
</p>
<p>f0(x)=
&int; 1
</p>
<p>0
K0(x, t)t dt =
</p>
<p>&int; 1
</p>
<p>0
δ(x, t)t dt = x
</p>
<p>f1(x)=
&int; 1
</p>
<p>0
K(x, t)t dt =
</p>
<p>&int; 1
</p>
<p>0
</p>
<p>[
xθ(t &minus; x)+ tθ(x &minus; t)
</p>
<p>]
t dt
</p>
<p>= x
&int; 1
</p>
<p>x
</p>
<p>t dt +
&int; x
</p>
<p>0
t2 dt = x
</p>
<p>2
&minus; x
</p>
<p>3
</p>
<p>6
.
</p>
<p>The next term is trickier than the first two because of the product of the
theta functions. We first substitute Eq. (18.14) in the integral for the second-
order term, and simplify
</p>
<p>f2(x)=
&int; 1
</p>
<p>0
K2(x, t)t dt =
</p>
<p>&int; 1
</p>
<p>0
t dt
</p>
<p>&int; 1
</p>
<p>0
K(x, s)K(s, t) ds
</p>
<p>=
&int; 1
</p>
<p>0
t dt
</p>
<p>&int; 1
</p>
<p>0
</p>
<p>[
xθ(s &minus; x)+ sθ(x &minus; s)
</p>
<p>][
sθ(t &minus; s)+ tθ(s &minus; t)
</p>
<p>]
ds
</p>
<p>= x
&int; 1
</p>
<p>0
t dt
</p>
<p>&int; 1
</p>
<p>0
sθ(s &minus; x)θ(t &minus; s) ds
</p>
<p>+ x
&int; 1
</p>
<p>0
t2 dt
</p>
<p>&int; 1
</p>
<p>0
θ(s &minus; x)θ(s &minus; t) ds
</p>
<p>+
&int; 1
</p>
<p>0
t dt
</p>
<p>&int; 1
</p>
<p>0
s2θ(x &minus; s)θ(t &minus; s) ds
</p>
<p>+
&int; 1
</p>
<p>0
t2 dt
</p>
<p>&int; 1
</p>
<p>0
sθ(x &minus; s)θ(s &minus; t) ds.
</p>
<p>It is convenient to switch the order of integration at this point. This is be-
cause of the presence of θ(x &minus; s) and θ(s &minus; x), which do not involve t and
are best integrated last. Thus, we have
</p>
<p>f2(x)= x
&int; 1
</p>
<p>0
sθ(s &minus; x)ds
</p>
<p>&int; 1
</p>
<p>s
</p>
<p>t dt + x
&int; 1
</p>
<p>0
θ(s &minus; x)ds
</p>
<p>&int; s
</p>
<p>0
t2 dt
</p>
<p>+
&int; 1
</p>
<p>0
s2θ(x &minus; s) ds
</p>
<p>&int; 1
</p>
<p>s
</p>
<p>t dt +
&int; 1
</p>
<p>0
sθ(x &minus; s) ds
</p>
<p>&int; s
</p>
<p>0
t2 dt
</p>
<p>= x
&int; 1
</p>
<p>x
</p>
<p>s ds
</p>
<p>(
1
</p>
<p>2
&minus; s
</p>
<p>2
</p>
<p>2
</p>
<p>)
+ x
</p>
<p>&int; 1
</p>
<p>x
</p>
<p>ds
s3
</p>
<p>3
+
&int; x
</p>
<p>0
s2 ds
</p>
<p>(
1
</p>
<p>2
&minus; s
</p>
<p>2
</p>
<p>2
</p>
<p>)
</p>
<p>+
&int; x
</p>
<p>0
s ds
</p>
<p>s3
</p>
<p>3
</p>
<p>= 5
24
</p>
<p>x &minus; 1
12
</p>
<p>x3 + 1
120
</p>
<p>x5.</p>
<p/>
</div>
<div class="page"><p/>
<p>18.2 Fredholm Integral Equations 551
</p>
<p>As a test of his/her knowledge of θ -function manipulation, the reader is
urged to perform the integration in reverse order. Adding all the terms, we
obtain an approximation for u(x) that is valid for 0 &le; x &le; 1:
</p>
<p>u(x)&asymp; f0(x)+ f1(x)+ f2(x)=
41
</p>
<p>24
x &minus; 1
</p>
<p>4
x3 + 1
</p>
<p>120
x5.
</p>
<p>We have seen that the Volterra equation of the second kind has a unique
solution which can be written as an infinite series (see Theorem 18.1.2).
The case of the Fredholm equation of the second kind is more complicated
because of the existence of eigenvalues. The general solution of Eq. (18.12)
is discussed in the following:
</p>
<p>Theorem 18.2.2 (Fredholm Alternative) Let K be a Hilbert-Schmidt oper- Fredholm alternative
ator and λ a complex number. Then either
</p>
<p>1. λ is a regular value of Eq. (18.12)&mdash;or λ&minus;1 is a regular point of the
operator K&mdash;in which case the equation has the unique solution |u〉 =
(1&minus; λK)&minus;1|v〉, or
</p>
<p>2. λ is a characteristic value of Eq. (18.12) (λ&minus;1 is an eigenvalue of the
operator K), in which case the equation has a solution if and only if |v〉
is in the orthogonal complement of the (finite-dimensional) null space
of 1&minus; λ&lowast;K&dagger;.
</p>
<p>Proof The first part is trivial if we recall that by definition, regular points of
K are those complex numbers μ which make the operator K&minus;μ1 invertible.
</p>
<p>For part (2), we first show that the null space of 1 &minus; λ&lowast;K&dagger; is finite-
dimensional. We note that 1 &minus; λK is invertible if and only if its adjoint
1 &minus; λ&lowast;K&dagger; is invertible, and λ &isin; ρ(K) iff λ&lowast; &isin; ρ(K&dagger;). Since the spectrum
of an operator is composed of all points that are not regular, we conclude
that λ is in the spectrum of K if and only if λ&lowast; is in the spectrum of K&dagger;.
For compact operators, all nonzero points of the spectrum are eigenvalues.
Therefore, the nonzero points of the spectrum of K&dagger;, a compact operator by
Theorem 17.5.7, are all eigenvalues of K&dagger;, and the null space of 1&minus; λ&lowast;K&dagger; is
finite-dimensional (Theorem 17.5.11). Next, we note that the equation itself
requires that |v〉 be in the range of the operator 1 &minus; λK, which, by Theo-
rem 17.6.5, is the orthogonal complement of the null space of 1&minus; λ&lowast;K&dagger;. �
</p>
<p>Historical Notes
</p>
<p>Erik Ivar Fredholm (1866&ndash;1927) was born in Stockholm, the son of a well-to-do mer-
</p>
<p>Erik Ivar Fredholm
</p>
<p>1866&ndash;1927
</p>
<p>chant family. He received the best education possible and soon showed great promise in
mathematics, leaning especially toward the applied mathematics of practical mechanics
in a year of study at Stockholm&rsquo;s Polytechnic Institute. Fredholm finished his education
at the University of Uppsala, obtaining his doctorate in 1898. He also studied at the Uni-
versity of Stockholm during this same period and eventually received an appointment to
the faculty there. Fredholm remained there the rest of his professional life.
His first contribution to mathematics was contained in his doctoral thesis, in which he
studied a first-order partial differential equation in three variables, a problem that arises
in the deformation of anisotropic media. Several years later he completed this work by
finding the fundamental solution to a general elliptic partial differential equation with
constant coefficients.</p>
<p/>
</div>
<div class="page"><p/>
<p>552 18 Integral Equations
</p>
<p>Fredholm is perhaps best known for his studies of the integral equation that bears his
name. Such equations occur frequently in physics. Fredholm&rsquo;s genius led him to note the
similarity between his equation and a relatively familiar matrix-vector equation, resulting
in his identification of a quantity that plays the same role in his equation as the deter-
minant plays in the matrix-vector equation. He thus obtained a method for determining
the existence of a solution and later used an analogous expression to derive a solution to
his equation akin to the Cramer&rsquo;s rule solution to the matrix-vector equation. He further
showed that the solution could be expressed as a power series in a complex variable. This
latter result was considered important enough that Poincar&eacute; assumed it without proof (in
fact he was unable to prove it) in a study of related partial differential equations.
Fredholm then considered the homogeneous form of his equation. He showed that under
certain conditions, the vector space of solutions is finite-dimensional. David Hilbert later
extended Fredholm&rsquo;s work to a complete eigenvalue theory of the Fredholm equation,
which ultimately led to the discovery of Hilbert spaces.
</p>
<p>18.2.1 Hermitian Kernel
</p>
<p>Of special interest are integral equations in which the kernel is hermitian,
which occurs when the operator is hermitian. Such a kernel has the property
that3 〈x|K|t〉&lowast; = 〈t |K|x〉 or [K(x, t)]&lowast; = K(t, x). For such kernels we can
use the spectral theorem for compact hermitian operators to find a series
solution for the integral equation. First we recall that
</p>
<p>K=
N&sum;
</p>
<p>j=1
λ&minus;1j Pj =
</p>
<p>N&sum;
</p>
<p>j=1
λ&minus;1j
</p>
<p>mj&sum;
</p>
<p>k=1
</p>
<p>∣∣e(j)k
&rang;&lang;
e
(j)
</p>
<p>k
</p>
<p>∣∣,
</p>
<p>where we have used λ&minus;1j to denote the eigenvalue of the operator
4 and ex-
</p>
<p>panded the projection operator in terms of orthonormal basis vectors of the
corresponding finite-dimensional eigenspace. Recall that N can be infinity.
Instead of the double sum, we can sum once over all the basis vectors and
write K=&sum;&infin;n=1 λ&minus;1n |un〉〈un|. Here n counts all the orthonormal eigenvec-
tors of the Hilbert space, and λ&minus;1n is the eigenvalue corresponding to the
eigenvector |un〉. Therefore, λ&minus;1n may be repeated in the sum. The action of
K on a vector |u〉 is given by
</p>
<p>K|u〉 =
&infin;&sum;
</p>
<p>n=1
λ&minus;1n 〈un|u〉|un〉. (18.15)
</p>
<p>If the Hilbert space is L2[a, b], we may be interested in the functional form
of this equation. We obtain such a form by multiplying both sides by 〈x|:
</p>
<p>K[u](x)&equiv; 〈x|K|u〉 =
&infin;&sum;
</p>
<p>n=1
λ&minus;1n 〈un|u〉〈x|un〉 =
</p>
<p>&infin;&sum;
</p>
<p>n=1
λ&minus;1n 〈un|u〉un(x).
</p>
<p>That this series converges uniformly in the interval [a, b] is known as the
Hilbert-Schmidt theorem.Hilbert-Schmidt
</p>
<p>theorem
</p>
<p>3Since we are dealing mainly with real functions, hermiticity of K implies the symmetry
of K , i.e., K(x, t)=K(t, x).
4λj is the characteristic value of the integral equation, or the inverse of the eigenvalue of
the corresponding operator.</p>
<p/>
</div>
<div class="page"><p/>
<p>18.2 Fredholm Integral Equations 553
</p>
<p>Example 18.2.3 Let us solve u(x) = x + λ
&int; b
a
K(x, t)u(t) dt , where
</p>
<p>K(x, t) &equiv; xt is a symmetric (hermitian) kernel, by the Neumann series
method. We note that
</p>
<p>‖K‖2 =
&int; b
</p>
<p>a
</p>
<p>&int; b
</p>
<p>a
</p>
<p>∣∣K(x, t)
∣∣2dx dt =
</p>
<p>&int; b
</p>
<p>a
</p>
<p>&int; b
</p>
<p>a
</p>
<p>x2t2dx dt
</p>
<p>=
&int; b
</p>
<p>a
</p>
<p>x2dx
</p>
<p>&int; b
</p>
<p>a
</p>
<p>t2dt =
(&int; b
</p>
<p>a
</p>
<p>x2dx
</p>
<p>)2
= 1
</p>
<p>9
</p>
<p>(
b3 &minus; a3
</p>
<p>)2
,
</p>
<p>or
</p>
<p>‖K‖ =
&int; b
</p>
<p>a
</p>
<p>x2dx = 1
3
</p>
<p>(
b3 &minus; a3
</p>
<p>)
,
</p>
<p>and the Neumann series converges if |λ|(b3 &minus; a3) &lt; 3. Assuming that this
condition holds, we have
</p>
<p>u(x)= x +
&infin;&sum;
</p>
<p>j=1
λj
</p>
<p>&int; b
</p>
<p>a
</p>
<p>Kj (x, t)t dt.
</p>
<p>The special form of the kernel allows us to calculate Kj (x, t) directly:
</p>
<p>Kj (x, t)=
&int; b
</p>
<p>a
</p>
<p>&int; b
</p>
<p>a
</p>
<p>. . .
</p>
<p>&int; b
</p>
<p>a
</p>
<p>K(x, s1)K(s1, s2) &middot; &middot; &middot;K(sj&minus;1, t) ds1ds2 &middot; &middot; &middot; dsj&minus;1
</p>
<p>=
&int; b
</p>
<p>a
</p>
<p>&int; b
</p>
<p>a
</p>
<p>. . .
</p>
<p>&int; b
</p>
<p>a
</p>
<p>xs21s
2
2 &middot; &middot; &middot; s2j&minus;1t ds1ds2 &middot; &middot; &middot; dsj&minus;1
</p>
<p>= xt
(&int; b
</p>
<p>a
</p>
<p>s2 ds
</p>
<p>)j&minus;1
= xt‖K‖j&minus;1.
</p>
<p>It follows that
&int; b
a
Kj (x, t)t dt = x‖K‖j&minus;1 13 (b3 &minus;a3)= x‖K‖j . Substituting
</p>
<p>this in the expression for u(x) yields
</p>
<p>u(x)= x +
&infin;&sum;
</p>
<p>j=1
λjx‖K‖j = x + xλ‖K‖
</p>
<p>&infin;&sum;
</p>
<p>j=1
λj&minus;1‖K‖j&minus;1
</p>
<p>= x
(
</p>
<p>1 + λ‖K‖ 1
1 &minus; λ‖K‖
</p>
<p>)
= x
</p>
<p>1 &minus; λ‖K‖ =
3x
</p>
<p>3 &minus; λ(b3 &minus; a3) .
</p>
<p>Because of the simplicity of the kernel, we can solve the integral equation
exactly. First we write
</p>
<p>u(x)= x + λ
&int; b
</p>
<p>a
</p>
<p>xtu(t) dt = x + λx
&int; b
</p>
<p>a
</p>
<p>tu(t) dt &equiv; x(1 + λA), (18.16)
</p>
<p>where A=
&int; b
a
tu(t) dt . Multiplying both sides by x and integrating, we ob-
</p>
<p>tain
</p>
<p>A=
&int; b
</p>
<p>a
</p>
<p>xu(x)dx = (1 + λA)
&int; b
</p>
<p>a
</p>
<p>x2dx = (1 + λA)‖K‖
</p>
<p>&rArr; A= ‖K‖
1 &minus; λ‖K‖ .</p>
<p/>
</div>
<div class="page"><p/>
<p>554 18 Integral Equations
</p>
<p>Substituting A in Eq. (18.16) gives
</p>
<p>u(x)= x
(
</p>
<p>1 + λ ‖K‖
1 &minus; λ‖K‖
</p>
<p>)
= x
</p>
<p>1 &minus; λ‖K‖ .
</p>
<p>This solution is the same as the first one we obtained. However, no series was
involved here, and therefore no assumption is necessary concerning |λ|‖K‖.
</p>
<p>If one can calculate the eigenvectors |un〉 and the eigenvalues λ&minus;1n ,
then one can obtain a solution for the integral equation in terms of these
eigenfunctions as follows: Substitute (18.15) in the Fredholm equation
[Eq. (18.3)] to get
</p>
<p>|u〉 = |v〉 + λ
&infin;&sum;
</p>
<p>n=1
λ&minus;1n 〈un|u〉|un〉. (18.17)
</p>
<p>Multiply both sides by 〈um|:
</p>
<p>〈um|u〉 = 〈um|v〉 + λ
&infin;&sum;
</p>
<p>n=1
λ&minus;1n 〈un|u〉 〈um|un〉︸ ︷︷ ︸
</p>
<p>=δmn
</p>
<p>= 〈um|v〉 + λλ&minus;1m 〈um|u〉, (18.18)
</p>
<p>or, if λ is not one of the eigenvalues,
(
</p>
<p>1 &minus; λ
λm
</p>
<p>)
〈um|u〉 = 〈um|v〉 &rArr; 〈um|u〉 =
</p>
<p>λm〈um|v〉
λm &minus; λ
</p>
<p>.
</p>
<p>Substituting this in Eq. (18.17) gives
</p>
<p>|u〉 = |v〉 + λ
&infin;&sum;
</p>
<p>n=1
</p>
<p>〈un|v〉
λn &minus; λ
</p>
<p>|un〉, (18.19)
</p>
<p>and in the functional form,
</p>
<p>u(x)= v(x)+ λ
&infin;&sum;
</p>
<p>n=1
</p>
<p>〈un|v〉
λn &minus; λ
</p>
<p>un(x), λ �= λn &forall;n. (18.20)
</p>
<p>In case λ = λm for some m, the Fredholm alternative (Theorem 18.2.2)
says that we will have a solution only if |v〉 is in the orthogonal complement
of the null space of 5 1&minus;λmK. Moreover, Eq. (18.18) shows that 〈um|u〉, the
expansion coefficients of the basis vectors of the eigenspace Mm, cannot be
specified. However, Eq. (18.18) does determine the rest of the coefficients
as before. In this case, the solution can be written as
</p>
<p>|u〉 = |v〉 +
r&sum;
</p>
<p>k=1
ck
∣∣u(k)m
</p>
<p>&rang;
+ λ
</p>
<p>&infin;&sum;
</p>
<p>n= 1
n �=m
</p>
<p>〈un|v〉
λn &minus; λ
</p>
<p>|un〉, (18.21)
</p>
<p>5Remember that K is hermitian; therefore, λm is real.</p>
<p/>
</div>
<div class="page"><p/>
<p>18.2 Fredholm Integral Equations 555
</p>
<p>where r is the (finite) dimension of Mm, k labels the orthonormal basis
{|u(k)m 〉} of Mm, and {ck}rk=1 are arbitrary constants. In functional form, this
equation becomes
</p>
<p>u(x)= v(x)+
r&sum;
</p>
<p>k=1
cku
</p>
<p>(k)
m (x)+ λ
</p>
<p>&infin;&sum;
</p>
<p>n= 1
n �=m
</p>
<p>〈un|v〉
λn &minus; λ
</p>
<p>un(x). (18.22)
</p>
<p>Example 18.2.4 We now give an example of the application of Eq. (18.20).
We want to solve u(x)= 3
</p>
<p>&int; 1
&minus;1 K(x, t)u(t) dt + x2 where
</p>
<p>K(x, t)=
&infin;&sum;
</p>
<p>k=0
</p>
<p>uk(x)uk(t)
</p>
<p>2k/2
, uk(x)=
</p>
<p>&radic;
2k + 1
</p>
<p>2
Pk(x),
</p>
<p>and Pk(x) is a Legendre polynomial.
We first note that {uk} is an orthonormal set of functions, that K(x, t) is
</p>
<p>real and symmetric (therefore, hermitian), and that
</p>
<p>&int; 1
</p>
<p>&minus;1
dt
</p>
<p>&int; 1
</p>
<p>&minus;1
dx
</p>
<p>∣∣K(x, t)
∣∣2
</p>
<p>=
&int; 1
</p>
<p>&minus;1
dt
</p>
<p>&int; 1
</p>
<p>&minus;1
dx
</p>
<p>&infin;&sum;
</p>
<p>k,l=0
</p>
<p>uk(x)uk(t)
</p>
<p>2k/2
ul(x)ul(t)
</p>
<p>2l/2
</p>
<p>=
&infin;&sum;
</p>
<p>k,l=0
</p>
<p>1
</p>
<p>2k/2
1
</p>
<p>2l/2
</p>
<p>&int; 1
</p>
<p>&minus;1
uk(x)ul(x) dx
</p>
<p>︸ ︷︷ ︸
=δkl
</p>
<p>&int; 1
</p>
<p>&minus;1
uk(t)ul(t) dt
</p>
<p>︸ ︷︷ ︸
=δkl
</p>
<p>=
&infin;&sum;
</p>
<p>k=0
</p>
<p>1
</p>
<p>2k
δkk =
</p>
<p>&infin;&sum;
</p>
<p>k=0
</p>
<p>1
</p>
<p>2k
= 2 &lt;&infin;.
</p>
<p>Thus, K(x, t) is a Hilbert-Schmidt kernel.
Now note that
</p>
<p>&int; 1
</p>
<p>&minus;1
K(x, t)uk(t) dt =
</p>
<p>&int; 1
</p>
<p>&minus;1
</p>
<p>&infin;&sum;
</p>
<p>l=0
</p>
<p>ul(x)ul(t)
</p>
<p>2l/2
uk(t) dt
</p>
<p>=
&infin;&sum;
</p>
<p>l=0
</p>
<p>ul(x)
</p>
<p>2l/2
</p>
<p>&int; 1
</p>
<p>&minus;1
ul(t)uk(t) dt
</p>
<p>︸ ︷︷ ︸
=δkl
</p>
<p>= 1
2k/2
</p>
<p>uk(x).
</p>
<p>This shows that uk is an eigenfunction of K(x, t) with eigenvalue 1/2k/2.
Since 3 �= 1/2k/2 for any integer k, we can use Eq. (18.20) to write
</p>
<p>u(x)= x2 + 3
&infin;&sum;
</p>
<p>k=0
</p>
<p>&int; 1
&minus;1 uk(s)s
</p>
<p>2ds
</p>
<p>2k/2 &minus; 3 uk(x).</p>
<p/>
</div>
<div class="page"><p/>
<p>556 18 Integral Equations
</p>
<p>But
&int; 1
&minus;1 uk(s)s
</p>
<p>2ds = 0 for k &ge; 3. For k &le; 2, we use the first three Legendre
polynomials to get
</p>
<p>&int; 1
</p>
<p>&minus;1
u0(s)s
</p>
<p>2ds =
&radic;
</p>
<p>2
</p>
<p>3
,
</p>
<p>&int; 1
</p>
<p>&minus;1
u1(s)s
</p>
<p>2ds = 0,
&int; 1
</p>
<p>&minus;1
u2(s)s
</p>
<p>2ds = 2
&radic;
</p>
<p>2
</p>
<p>3
&radic;
</p>
<p>5
.
</p>
<p>This gives u(x)= 12 &minus; 2x2. The reader is urged to substitute this solution in
the original integral equation and verify that it works.
</p>
<p>18.2.2 Degenerate Kernels
</p>
<p>The preceding example involves the simplest kind of degenerate, or separa-
ble, kernels. A kernel is called degenerate, or separable, if it can be writtendegenerate or separable
</p>
<p>kernel as a finite sum of products of functions of one variable:
</p>
<p>K(x, t)=
n&sum;
</p>
<p>j=1
φj (x)ψ
</p>
<p>&lowast;
j (t), (18.23)
</p>
<p>where φj and ψj are assumed to be square-integrable. Substituting (18.23)
in the Fredholm integral equation of the second kind, we obtain
</p>
<p>u(x)&minus; λ
n&sum;
</p>
<p>j=1
φj (x)
</p>
<p>&int; b
</p>
<p>a
</p>
<p>ψ&lowast;j (t)u(t) dt = v(x).
</p>
<p>If we define μj =
&int; b
a
ψ&lowast;j (t)u(t) dt , the preceding equation becomes
</p>
<p>u(x)&minus; λ
n&sum;
</p>
<p>j=1
μjφj (x)= v(x). (18.24)
</p>
<p>Multiply this equation by ψ&lowast;i (x) and integrate over x to get
</p>
<p>μi &minus; λ
n&sum;
</p>
<p>j=1
μjAij = νi for i = 1,2, . . . , n, (18.25)
</p>
<p>where Aij =
&int; b
a
ψ&lowast;i (t)φj (t) dt and νi =
</p>
<p>&int; b
a
ψ&lowast;i (t)v(t) dt . With μi , νi , and
</p>
<p>Aij as components of column vectors u, v, and a matrix A, we can write the
above linear system of equations as
</p>
<p>u &minus; λAu = v, or (1 &minus; λA)u = v. (18.26)
</p>
<p>We can now determine the μi by solving the system of linear equations
given by (18.25). Once the μi are determined, Eq. (18.24) gives u(x). Thus,
for a degenerate kernel the Fredholm problem reduces to a system of linear
equations.</p>
<p/>
</div>
<div class="page"><p/>
<p>18.2 Fredholm Integral Equations 557
</p>
<p>Example 18.2.5 As a concrete example of an integral equation with de-
generate kernel, we solve u(x) &minus; λ
</p>
<p>&int; 1
0 (1 + xt)u(t) dt = x for two differ-
</p>
<p>ent values of λ. The kernel, K(x, t)= 1 + xt , is separable with φ1(x)= 1,
ψ1(t)= 1, φ2(x)= x, and ψ2(t)= t . This gives the matrix
</p>
<p>A =
(
</p>
<p>1 12
1
2
</p>
<p>1
3
</p>
<p>)
.
</p>
<p>For convenience, we define the matrix B &equiv; 1 &minus; λA.
(a) First assume that λ = 1. In that case B has a nonzero determinant.
</p>
<p>Thus, B&minus;1 exists, and can be calculated to be
</p>
<p>B&minus;1 =
(
&minus; 83 &minus;2
&minus;2 0
</p>
<p>)
.
</p>
<p>With
</p>
<p>ν1 =
&int; 1
</p>
<p>0
ψ&lowast;1 (t)v(t) dt =
</p>
<p>&int; 1
</p>
<p>0
t dt = 1
</p>
<p>2
and
</p>
<p>ν2 =
&int; 1
</p>
<p>0
ψ&lowast;2 (t)v(t) dt =
</p>
<p>&int; 1
</p>
<p>0
t2 dt = 1
</p>
<p>3
</p>
<p>we obtain
(
μ1
μ2
</p>
<p>)
= B&minus;1v =
</p>
<p>(
&minus; 83 &minus;2
&minus;2 0
</p>
<p>)(
1
2
1
3
</p>
<p>)
=
(&minus;2
&minus;1
</p>
<p>)
.
</p>
<p>Equation (18.24) then gives u(x)= μ1φ1(x)+μ2φ2(x)+ x =&minus;2.
(b) Now, for the purpose of illustrating the other alternative of Theorem
</p>
<p>18.2.2, let us take λ= 8 + 2
&radic;
</p>
<p>13. Then
</p>
<p>B = 1 &minus; λA =&minus;
(
</p>
<p>7 + 2
&radic;
</p>
<p>13 4 +
&radic;
</p>
<p>13
4 +
</p>
<p>&radic;
13 (5 + 2
</p>
<p>&radic;
13)/3
</p>
<p>)
,
</p>
<p>and det B = 0. This shows that 8 + 2
&radic;
</p>
<p>13 is a characteristic value of
the equation. We thus have a solution only if v(x) &equiv; x is orthogonal
to the null space of 1 &minus; λ&lowast;A&dagger; = B&dagger;. To determine a basis for this null
space, we have to find vectors |z〉 such that B&dagger;|z〉 = 0. Since λ is real,
and B is real and symmetric, B&dagger; = B, and we must solve
</p>
<p>(
7 + 2
</p>
<p>&radic;
13 4 +
</p>
<p>&radic;
13
</p>
<p>4 +
&radic;
</p>
<p>13 (5 + 2
&radic;
</p>
<p>13)/3
</p>
<p>)(
ξ1
ξ2
</p>
<p>)
= 0.
</p>
<p>The solution to this equation is a multiple of |z〉 &equiv;
( 3
&minus;2&minus;
</p>
<p>&radic;
13
</p>
<p>)
. If the
</p>
<p>integral equation is to have a solution, the column vector v (whose
corresponding ket we denote by |v〉) must be orthogonal to |z〉. But
</p>
<p>〈z|v〉 =
(
3 &minus;2 &minus;
</p>
<p>&radic;
13
</p>
<p>)
(
</p>
<p>1
2
1
3
</p>
<p>)
�= 0.
</p>
<p>Therefore, the integral equation has no solution.</p>
<p/>
</div>
<div class="page"><p/>
<p>558 18 Integral Equations
</p>
<p>The reader may feel uneasy that the functions φj (x) and ψj (t) appearing
in a degenerate kernel are arbitrary to within a multiplicative function. After
all, we can multiply φj (x) by a nonzero function, and divide ψj (t) by the
same function, and get the same kernel. Such a change clearly alters the
matrices A and B and therefore seems likely to change the solution, u(x).
That this is not the case is demonstrated in Problem 18.2. In fact, it can
be shown quite generally that the transformations described above do not
change the solution.
</p>
<p>As the alert reader may have noticed, we have been avoiding the prob-
lem of solving the eigenvalue (characteristic) problem for integral operators.
Such a problem is nontrivial, and the analogue of the finite-dimensional
case, where one works with determinants and characteristic polynomi-
als, does not exist. An exception is a degenerate hermitian6 kernel, i.e.,
a kernel of the form K(x, t) = &sum;ni=1 hi(x)h&lowast;i (t). Substituting this in the
characteristic-value equation
</p>
<p>u(x)= λ
&int; b
</p>
<p>a
</p>
<p>K(x, t)u(t) dt, λ �= 0,
</p>
<p>we obtain u(x) = λ&sum;ni=1 hi(x)
&int; b
a
h&lowast;i (t)u(t) dt . Defining μi &equiv;
</p>
<p>&int; b
a
h&lowast;i (t)&times;
</p>
<p>u(t) dt and substituting it back in the equation gives
</p>
<p>u(x)= λ
n&sum;
</p>
<p>i=1
hi(x)μi . (18.27)
</p>
<p>Multiplying this equation by λ&minus;1h&lowast;k(x) and integrating over x yields
</p>
<p>λ&minus;1μk =
n&sum;
</p>
<p>i=1
</p>
<p>[&int; b
</p>
<p>a
</p>
<p>h&lowast;k(x)hi(x) dx
]
μi &equiv;
</p>
<p>n&sum;
</p>
<p>i=1
mkiμi .
</p>
<p>This is an eigenvalue equation for the hermitian n &times; n matrix M with el-
ements mij , which, by spectral theorem for hermitian operators, can be
solved. In fact, the matrix need not be hermitian; as long as it is normal,
the eigenvalue problem can be solved. Once the eigenvectors and the eigen-
values are found, we can substitute them in Eq. (18.27) and obtain u(x). We
expect to find a finite number of eigenfunctions and eigenvalues. Our anal-
ysis of compact operators included such a case. That analysis also showed
that the entire (infinite-dimensional) Hilbert space could be written as the di-
rect sum of eigenspaces that are finite-dimensional for nonzero eigenvalues.
Therefore, we expect the eigenspace corresponding to the zero eigenvalue
(or infinite characteristic value) to be infinite-dimensional. The following
example illustrates these points.
</p>
<p>Example 18.2.6 Let us find the nonzero characteristic values and corre-
sponding eigenfunctions of the kernel K(x, t) = 1 + sin(x + t) for &minus;π &le;
x, t &le; π .
</p>
<p>6Actually, the problem of a degenerate kernel that leads to a normal matrix, as described
below, can also be solved.</p>
<p/>
</div>
<div class="page"><p/>
<p>18.2 Fredholm Integral Equations 559
</p>
<p>We are seeking functions u and scalars λ satisfying u(x)= λK[u](x), or
</p>
<p>u(x)= λ
&int; π
</p>
<p>&minus;π
</p>
<p>[
1 + sin(x + t)
</p>
<p>]
u(t) dt.
</p>
<p>Expanding sin(x + t), we obtain
</p>
<p>u(x)= λ
&int; π
</p>
<p>&minus;π
[1 + sinx cos t + cosx sin t]u(t) dt, (18.28)
</p>
<p>or
</p>
<p>λ&minus;1u(x)= μ1 +μ2 sinx +μ3 cosx, (18.29)
where μ1 =
</p>
<p>&int; π
&minus;π u(t) dt , μ2 =
</p>
<p>&int; π
&minus;π u(t) cos t dt , and μ3 =
</p>
<p>&int; π
&minus;π u(t) sin t dt .
</p>
<p>Integrate both sides of Eq. (18.29) with respect to x from &minus;π to π to obtain
λ&minus;1μ1 = 2πμ1. Similarly, multiplying by sinx and cosx and integrating
yields
</p>
<p>λ&minus;1μ2 = πμ3 and λ&minus;1μ3 = πμ2. (18.30)
If μ1 �= 0, we get λ&minus;1 = 2π , which, when substituted in (18.30), yields μ2 =
μ3 = 0. We thus have, as a first solution, λ&minus;11 = 2π and |u1〉 = α
</p>
<p>( 1
0
0
</p>
<p>)
, where
</p>
<p>α is an arbitrary constant. Equation (18.29) now gives λ&minus;11 u1(x) = μ1, or
u1(x)= c1, where c1 is an arbitrary constant to be determined.
</p>
<p>On the other hand, μ1 = 0 if λ&minus;1 �= 2π . Then Eq. (18.30) yields λ&minus;1 =
&plusmn;π and μ2 =&plusmn;μ3. For λ&minus;1 &equiv; λ&minus;1+ = π , Eq. (18.29) gives
</p>
<p>u(x)&equiv; u+(x)= c+(sinx + cosx),
</p>
<p>and for λ&minus;1 &equiv; λ&minus;1&minus; =&minus;π , it yields u(x)&equiv; u&minus;(x)= c&minus;(sinx&minus;cosx), where
c&plusmn; are arbitrary constants to be determined by normalization of eigenfunc-
tions. The normalized eigenfunctions are
</p>
<p>u1 =
1&radic;
2π
</p>
<p>, u&plusmn;(x)=
1&radic;
2π
</p>
<p>(sinx &plusmn; cosx).
</p>
<p>Direct substitution in the original integral equation easily verifies that u1,
u+, and u&minus; are eigenfunctions of the integral equation with the eigenvalues
calculated above.
</p>
<p>Let us now consider the zero eigenvalue (or infinite characteristic value).
Divide both sides of Eq. (18.28) by λ and take the limit of λ&rarr;&infin;. Then the
integral equation becomes
</p>
<p>&int; π
</p>
<p>&minus;π
[1 + sinx cos t + cosx sin t]u(t) dt = 0.
</p>
<p>The solutions u(t) to this equation would span the eigenspace corresponding
to the zero eigenvalue, or infinite characteristic value. We pointed out above
that this eigenspace is expected to be infinite-dimensional. This expectation
is borne out once we note that all functions of the form sinnt or cosnt
with n&ge; 2 make the above integral zero; and there are infinitely many such
functions.</p>
<p/>
</div>
<div class="page"><p/>
<p>560 18 Integral Equations
</p>
<p>18.3 Problems
</p>
<p>18.1 Use mathematical induction to derive Eq. (18.5).
</p>
<p>18.2 Repeat part (a) of Example 18.2.5 using
</p>
<p>φ1(x)=
1
</p>
<p>2
, ψ1(t)= 2,
</p>
<p>φ2(x)= x, ψ2(t)= t
</p>
<p>so that we still have K(x, t)= φ1(x)ψ1(t)+ φ2(x)ψ2(t).
</p>
<p>18.3 Use the spectral theorem for compact hermitian operators to show that
if the kernel of a Hilbert-Schmidt operator has a finite number of nonzero
eigenvalues, then the kernel is separable. Hint: See the discussion at the
beginning of Sect. 18.2.1.
</p>
<p>18.4 Use the method of successive approximations to solve the Volterra
equation u(x) = λ
</p>
<p>&int; x
0 u(t)dt . Then derive a DE equivalent to the Volterra
</p>
<p>equation (make sure to include the initial condition), and solve it.
</p>
<p>18.5 Regard the Fourier transform,
</p>
<p>F[f ](x)&equiv; 1&radic;
2π
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
eixyf (y)dy
</p>
<p>as an integral operator.
</p>
<p>(a) Show that F2[f ](x)= f (&minus;x).
(b) Deduce, therefore, that the only eigenvalues of this operator are λ =
</p>
<p>&plusmn;1,&plusmn;i.
(c) Let f (x) be any even function of x. Show that an appropriate choice
</p>
<p>of α can make u= f +αF[f ] an eigenfunction of F. (This shows that
the eigenvalues of F have infinite multiplicity.)
</p>
<p>18.6 For what values of λ does the following integral equation have a solu-
tion?
</p>
<p>u(x)= λ
&int; π
</p>
<p>0
sin(x + t)u(t) dt + x.
</p>
<p>What is that solution? Redo the problem using a Neumann series expansion.
Under what condition is the series convergent?
</p>
<p>18.7 It is possible to multiply the functions φj (x) by γj (x) and ψj (t) by
1/γj (t) and still get the same degenerate kernel, K(x, t)=
</p>
<p>&sum;n
j=1φj (x)ψj (t).
</p>
<p>Show that such arbitrariness, although affecting the matrices A and B, does
not change the solution of the Fredholm problem
</p>
<p>u(x)&minus; λ
&int; b
</p>
<p>a
</p>
<p>K(x, t)u(t)dt = f (x).</p>
<p/>
</div>
<div class="page"><p/>
<p>18.3 Problems 561
</p>
<p>18.8 Show, by direct substitution, that the solution found in Example 18.2.4
does satisfy its integral equation.
</p>
<p>18.9 Solve u(x)= 12
&int; 1
&minus;1(x + t)u(t) dt + x.
</p>
<p>18.10 Solve u(x) = λ
&int; 1
</p>
<p>0 xtu(t) dt + x using the Neumann series method.
For what values of λ is the series convergent? Now find the eigenvalues and
eigenfunctions of the kernel and solve the problem using these eigenvalues
and eigenfunctions.
</p>
<p>18.11 Solve u(x)= λ
&int;&infin;
</p>
<p>0 K(x, t)u(t)dt + xα , where α is any real number
except a negative integer, and K(x, t)= e&minus;(x+t). For what values of λ does
the integral equation have a solution?
</p>
<p>18.12 Solve the integral equations
</p>
<p>(a) u(x)= ex + λ
&int; 1
</p>
<p>0
xtu(t) dt, (b) u(x)= λ
</p>
<p>&int; π
</p>
<p>0
sin(x &minus; t)u(t) dt,
</p>
<p>(c) u(x)= x2 +
&int; 1
</p>
<p>0
xtu(t) dt, (d) u(x)= x +
</p>
<p>&int; x
</p>
<p>0
u(t) dt.
</p>
<p>18.13 Solve the integral equation u(x)= x + λ
&int; 1
</p>
<p>0 (x + t)tu(t) dt , keeping
terms up to λ2.
</p>
<p>18.14 Solve the integral equation u(x)= e&minus;|x| + λ
&int;&infin;
&minus;&infin; e
</p>
<p>&minus;|x&minus;t |u(t) dt , as-
suming that f remains finite as x &rarr;&plusmn;&infin;.
</p>
<p>18.15 Solve the integral equation u(x) = e&minus;|x| + λ
&int;&infin;
</p>
<p>0 u(t) cosxt dt , as-
suming that f remains finite as x &rarr;&plusmn;&infin;.</p>
<p/>
</div>
<div class="page"><p/>
<p>19Sturm-Liouville Systems
</p>
<p>The linear operators discussed in the last two chapters were exclusively in-
tegral operators. Most applications of physical interest, however, involve
differential operators (DO). Unfortunately, differential operators are un-
bounded. We noted that complications arise when one abandons the com-
pactness property of the operator, e.g., sums turn into integrals and one loses
one&rsquo;s grip over the eigenvalues of noncompact operators. The transition to
unbounded operators complicates matters even more. Fortunately, the for-
malism of one type of DOs that occur most frequently in physics can be
studied in the context of compact operators. Such a study is our aim for this
chapter.
</p>
<p>19.1 Compact-Resolvent Unbounded Operators
</p>
<p>As was pointed out in Example 17.2.7, the derivative operator cannot be
defined for all functions in L2(a, b). This motivates the following:
</p>
<p>Definition 19.1.1 Let D be a linear manifold1 in the Hilbert space H. A lin-
ear map T :D&rarr;H will be called a linear operator in2 H. D is called the domain of a linear
</p>
<p>operatordomain of T and often denoted by D(T).
</p>
<p>Example 19.1.2 The domain of the derivative operator D, as an operator
on L2(a, b), cannot be the entire space. On the other hand, D is defined on
the linear manifold M in L2(a, b) spanned by {ei2nπx/L} with L = b &minus; a.
As we saw in Chap. 9, M is dense (see Definition 17.4.5 and the discussion
following it) in L2(a, b). This is the essence of Fourier series: That every
function in L2(a, b) can be expanded in (i.e., approximated by) a Fourier
series. It turns out that many unbounded operators on a Hilbert space share
the same property, namely that their domains are dense in the Hilbert space.
</p>
<p>1A linear manifold of an infinite-dimensional normed vector space V is a proper subset
that is a vector space in its own right, but is not necessarily closed.
2As opposed to on H.
</p>
<p>S. Hassani, Mathematical Physics, DOI 10.1007/978-3-319-01195-0_19,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>563</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-01195-0_19">http://dx.doi.org/10.1007/978-3-319-01195-0_19</a></div>
</div>
<div class="page"><p/>
<p>564 19 Sturm-Liouville Systems
</p>
<p>Another important property of Fourier expansion is the fact that if the
function is differentiable, then one can differentiate both sides, i.e., one can
differentiate a Fourier expansion term by term if such an operation makes
sense for the original function. Define the sequence {fm} by
</p>
<p>fm(x)=
m&sum;
</p>
<p>n=&minus;m
ane
</p>
<p>i2πnx/L, an =
1&radic;
L
</p>
<p>ˆ b
</p>
<p>a
</p>
<p>f (x)e&minus;i2πnx/Ldx.
</p>
<p>Then we can state the property above as follows: Suppose {fm} is in M.
If limfm = f and limf &prime;m = g, then f &prime; = g and f &isin;M. Many unbounded
operators share this property.
</p>
<p>Definition 19.1.3 Let D be a linear manifold in the Hilbert space H. Let
T :D&rarr;H be a linear operator in H. Suppose that for any sequence {|un〉}
in D, both {|un〉} and {T|un〉} converge in H, i.e.,
</p>
<p>lim |un〉 = |u〉 and limT|un〉 = |v〉.
</p>
<p>We say that T is closed if |v〉 &isin;D and T|u〉 = |v〉.closed operator
</p>
<p>Notice that we cannot demand that |v〉 be in D for a general operator.
This, as we saw in the preceding example, will not be appropriate for un-
bounded operators.
</p>
<p>The restriction of the domain of an unbounded operator is necessitated
by the fact that the action of the operator on a vector in the Hilbert space
in general takes that vector out of the space. The following theorem (see
[DeVi 90, pp. 251&ndash;252] for a proof) shows why this is necessary:
</p>
<p>Theorem 19.1.4 A closed linear operator inH that is defined at every point
of H (so that D=H) is bounded.
</p>
<p>Thus, if we are interested in unbounded operators (for instance, differen-
tial operators), we have to restrict their domains. In particular, we have to
accept the possibility of an operator whose adjoint has a different domain.3
</p>
<p>Definition 19.1.5 Let T be a linear operator in H. We shall say that T isdifference between
hermitian and
</p>
<p>self-adjoint operators
hermitian if T&dagger; is an extension of T, i.e., D(T)&sub;D(T&dagger;) and T&dagger;|u〉 = T|u〉
for all |u〉 &isin;D(T). T is called self-adjoint if D(T)=D(T&dagger;).
</p>
<p>As we shall see shortly, certain types of Sturm-Liouville operators, al-
though unbounded, lend themselves to a study within the context of compact
operators.
</p>
<p>Definition 19.1.6 A hermitian linear operator T in a Hilbert space H is said
to have a compact resolvent if there is a μ &isin; ρ(T) for which the resolventoperators with compact
</p>
<p>resolvent Rμ(T) is compact.
</p>
<p>3This subtle difference between hermitian and self-adjoint is stated here merely to warn
the reader and will be confined to the present discussion. The two qualifiers will be
(ab)used interchangeably in the rest of the book.</p>
<p/>
</div>
<div class="page"><p/>
<p>19.1 Compact-Resolvent Unbounded Operators 565
</p>
<p>An immediate consequence of this definition is that Rλ(T) is compact for
all λ &isin; ρ(T). To see this, note that Rλ(T) is bounded by Definition 17.3.1.
Now use Eq. (17.7) and write
</p>
<p>Rλ(T)=
[
1+ (λ&minus;μ)Rλ(T)
</p>
<p>]
Rμ(T).
</p>
<p>The RHS is a product of a bounded4 and a compact operator, and there-
fore must be compact. The compactness of the resolvent characterizes its
spectrum by Theorem 17.6.8. As the following theorem shows, this in turn
characterizes the spectrum of the operators with compact resolvent.
</p>
<p>Theorem 19.1.7 Let T be an operator with compact resolvent Rλ(T) where
λ &isin; ρ(T). Then 0 �= μ &isin; ρ(Rλ(T)) if and only if (λ+1/μ) &isin; ρ(T). Similarly,
μ �= 0 is an eigenvalue of Rλ(T) if and only if (λ+ 1/μ) is an eigenvalue
of T. Furthermore, the eigenvectors of Rλ(T) corresponding to μ coincide
with those of T corresponding to (λ+ 1/μ).
</p>
<p>Proof The proof consists of a series of two-sided implications involving
definitions. We give the proof of the first part, the second part being very
similar:
</p>
<p>μ &isin; ρ
(
Rλ(T)
</p>
<p>)
iff Rλ(T)&minus;μ1 is invertible.
</p>
<p>Rλ(T)&minus;μ1 is invertible iff (T&minus; λ1)&minus;1 &minus;μ1 is invertible.
(T&minus; λ1)&minus;1 &minus;μ1 is invertible iff 1&minus;μ(T&minus; λ1) is invertible.
</p>
<p>1&minus;μ(T&minus; λ1) is invertible iff 1
μ
1&minus; T+ λ1 is invertible.
</p>
<p>(
1
</p>
<p>μ
+ λ
</p>
<p>)
1&minus; T is invertible iff
</p>
<p>(
1
</p>
<p>μ
+ λ
</p>
<p>)
&isin; ρ(T).
</p>
<p>Comparing the LHS of the first line with the RHS of the last line, we obtain
the first part of the theorem. �
</p>
<p>A consequence of this theorem and Theorem 17.5.11 is that the eigen-
spaces of an (unbounded) operator with compact resolvent are finite-
dimensional, i.e., such an operator has only finitely many eigenvectors cor-
responding to each of its eigenvalues. Moreover, arranging the eigenvalues
μn of the resolvent in decreasing order (as done in Theorem 17.6.8), we con-
clude that the eigenvalues of T can be arranged in a sequence in increasing
order of their absolute values and the limit of this sequence is infinity.
</p>
<p>Example 19.1.8 Consider the operator T in L2(0,1) defined by5 Tf =
&minus;f &prime;&prime; having the domain
</p>
<p>4The sum of two bounded operators is bounded.
5We shall depart from our convention here and shall not use the Dirac bar-ket notation
although the use of abstract operators encourages their use. The reason is that in this
example, we are dealing with functions, and it is more convenient to undress the functions
from their Dirac clothing.</p>
<p/>
</div>
<div class="page"><p/>
<p>566 19 Sturm-Liouville Systems
</p>
<p>D(T)=
{
f &isin;L2(0,1) | f &prime;&prime; &isin;L2(0,1), f (0)= f (1)= 0
</p>
<p>}
.
</p>
<p>The reader may check that zero is not an eigenvalue of T. Therefore, we may
choose R0(T)= T&minus;1. We shall study a systematic way of finding inverses of
some specific differential operators in the upcoming chapters on Green&rsquo;s
functions. At this point, suffice it to say that T&minus;1 can be written as a Hilbert-
Schmidt integral operator with kernel
</p>
<p>K(x, t)=
{
x(1 &minus; t) if 0 &le; x &le; t &le; 1,
(1 &minus; x)t if 0 &le; t &le; x &le; 1.
</p>
<p>Thus, if Tf = g, i.e., if f &prime;&prime; =&minus;g, then T&minus;1g = f , or f =K[g], i.e.,
</p>
<p>f (x)=K[g](x)=
ˆ 1
</p>
<p>0
K(x, t)g(t) dt
</p>
<p>=
ˆ x
</p>
<p>0
(1 &minus; x)tg(t) dt +
</p>
<p>ˆ 1
</p>
<p>x
</p>
<p>(1 &minus; x)tg(t) dt.
</p>
<p>It is readily verified that K[g](0)=K[g](1)= 0 and f &prime;&prime;(x)=K[g]&prime;&prime;(x)=
&minus;g.
</p>
<p>We can now use Theorem 19.1.7 with λ = 0 to find all the eigenval-
ues of T: μn is an eigenvalue of T if and only if 1/μn is an eigenvalue of
T&minus;1. These eigenvalues should have finite-dimensional eigenspaces, and we
should be able to arrange them in increasing order of magnitude without
bound. To verify this, we solve f &prime;&prime; =&minus;μf , whose solutions are μn = n2π2
and fn(x)= sinnπx. Note that there is only one eigenfunction correspond-
ing to each eigenvalue. Therefore, the eigenspaces are finite- (one-) dimen-
sional.
</p>
<p>The example above is a special case of a large class of DOs occurring in
mathematical physics. Recall from Theorem 14.5.4 that all linear second-
order differential equations can be made self-adjoint. Moreover, Proposi-
tion 14.4.11 showed that any SOLDE can be transformed into a form in
which the first-derivative term is absent. By dividing the DE by the coeffi-
cient of the second-derivative term if necessary, the study of the most general
second-order linear differential operators boils down to that of the so-called
Sturm-Liouville (S-L) operatorsSturm-Liouville
</p>
<p>operators
</p>
<p>Lx &equiv;
d2
</p>
<p>dx2
&minus; q(x), (19.1)
</p>
<p>which are assumed to be self-adjoint. Differential operators are necessar-
ily accompanied by boundary conditions that specify their domains. So, to
be complete, let us assume that the DO in Eq. (19.1) acts on the subset of
L2(a, b) consisting of functions u that satisfy the following so-called sepa-
rated boundary conditions:separated boundary
</p>
<p>conditions
α1u(a)+ β1u&prime;(a)= 0,
α2u(b)+ β2u&prime;(b)= 0,
</p>
<p>(19.2)</p>
<p/>
</div>
<div class="page"><p/>
<p>19.1 Compact-Resolvent Unbounded Operators 567
</p>
<p>where α1, α2, β1, and β2 are real constants with the property that the matrix
of coefficients has no zero rows. The collection of the DO and the boundary regular Sturm-Liouville
</p>
<p>systemsconditions above is called a regular Sturm-Liouville system.
We now show that the DO of a regular Sturm-Liouville system has com-
</p>
<p>pact resolvent. First observe that by adding αu&mdash;with α an arbitrary number
different from all eigenvalues of the DO&mdash;to both sides of the eigenvalue
equation u&prime;&prime; &minus; qu = λu, we can assume6 that zero is not an eigenvalue of
Lx . Next, suppose that u1(x) and u2(x) are the two linearly independent so-
lutions of the homogeneous DE satisfying the first and the second boundary
conditions of Eq. (19.2), respectively. The operator whose kernel is
</p>
<p>K(x, t)=
{
&minus;u1(x)u2(t)/W(a) if a &le; x &le; t &le; b,
&minus;u1(t)u2(x)/W(a) if a &le; t &le; x &le; b,
</p>
<p>in which W is the Wronskian of the solutions, is a Hilbert-Schmidt operator
and therefore compact. We now show that K(x, t) is the resolvent R0(Lx)=
L&minus;1x &equiv; K of our DO. To see this, write Lxu= v, and
</p>
<p>u(x)=K[v](x)=&minus;u2(x)
W(a)
</p>
<p>ˆ x
</p>
<p>a
</p>
<p>u1(t)v(t) dt &minus;
u1(x)
</p>
<p>W(a)
</p>
<p>ˆ b
</p>
<p>x
</p>
<p>u2(t)v(t) dt.
</p>
<p>Differentiating this once gives
</p>
<p>u&prime;(x)=&minus;u
&prime;
2(x)
</p>
<p>W(a)
</p>
<p>ˆ x
</p>
<p>a
</p>
<p>u1(t)v(t) dt &minus;
u&prime;1(x)
W(a)
</p>
<p>ˆ b
</p>
<p>x
</p>
<p>u2(t)v(t) dt,
</p>
<p>and a second differentiation yields
</p>
<p>u&prime;&prime;(x)=&minus;u
&prime;&prime;
2(x)
</p>
<p>W(a)
</p>
<p>ˆ x
</p>
<p>a
</p>
<p>u1(t)v(t) dt &minus;
u&prime;&prime;1(x)
W(a)
</p>
<p>ˆ b
</p>
<p>x
</p>
<p>u2(t)v(t) dt + v(x).
</p>
<p>The last equation follows from the fact that the Wronskian u&prime;1u2 &minus; u&prime;2u1 is
constant for a DE of the form u&prime;&prime; &minus; qu = 0. By substituting u&prime;&prime;1 = qu1 and
u&prime;&prime;2 = qu2 in the last equation, we verify that u=K[v] is indeed a solution
of the Sturm-Liouville system Lxu= v.
</p>
<p>Next, we show that the eigensolutions of the S-L system are nondegen-
erate, i.e., the eigenspaces are one-dimensional. Suppose f1 and f2 are any
two eigenfunctions corresponding to the same eigenvalue. Then both must
satisfy the same DE and the same boundary conditions; in particular, we
must have
</p>
<p>α1f1(a)+ β1f &prime;1(a)= 0
α1f2(a)+ β1f &prime;2(a)= 0
</p>
<p>&rArr;
(
f1(a) f
</p>
<p>&prime;
1(a)
</p>
<p>f2(a) f
&prime;
2(a)
</p>
<p>)(
α1
β1
</p>
<p>)
=
(
</p>
<p>0
0
</p>
<p>)
. (19.3)
</p>
<p>If α1 and β1 are not both zero, the Wronskian&mdash;the determinant of the
matrix above&mdash;must vanish. Therefore, the two functions must be linearly
dependent. Finally, recall that a Hilbert space on which a compact opera-
tor K is defined can be written as a direct sum of the latter&rsquo;s eigenspaces.
</p>
<p>6Although this will change q&mdash;and the original operator&mdash;no information will be lost
because the eigenvectors will be the same and all eigenvalues will be changed by α.</p>
<p/>
</div>
<div class="page"><p/>
<p>568 19 Sturm-Liouville Systems
</p>
<p>More specifically, H =&sum;Nj=0 &oplus;Mj , where each Mj is finite-dimensional
for j = 1,2, . . . , and N can be finite or infinite. If N is finite, then M0,
which can be considered as the eigenspace of zero eigenvalue,7 will be
infinite-dimensional. If M0 is finite-dimensional (or absent), then N must
be infinite, and the eigenvectors of K will span the entire space, i.e., they
will form a complete orthogonal system. We now show that this holds for
the regular Sturm-Liouville operator.
</p>
<p>Historical Notes
</p>
<p>Jacques Charles Francois Sturm (1803&ndash;1855) made the first accurate determination of
</p>
<p>Jacques Charles Francois
</p>
<p>Sturm 1803&ndash;1855
</p>
<p>the velocity of sound in water in 1826, working with the Swiss engineer Daniel Colladon.
He became a French citizen in 1833 and worked in Paris at the &Eacute;cole Polytechnique where
he became a professor in 1838. In 1840 he succeeded Poisson in the chair of mechanics
in the Facult&eacute; des Sciences, Paris.
The problems of determining the eigenvalues and eigenfunctions of an ordinary differen-
tial equation with boundary conditions and of expanding a given function in terms of an
infinite series of the eigenfunctions, which date from about 1750, became more promi-
nent as new coordinate systems were introduced and new classes of functions arose as the
eigenfunctions of ordinary differential equations. Sturm and his friend Joseph Liouville
decided to tackle the general problem for any second-order linear differential equation.
Sturm had been working since 1833 on problems of partial differential equations, pri-
marily on the flow of heat in a bar of variable density, and hence was fully aware of the
eigenvalue and eigenfunction problem. The mathematical ideas he applied to this prob-
lem are closely related to his investigations of the reality and distribution of the roots of
algebraic equations. His ideas on differential equations, he says, came from the study of
difference equations and a passage to the limit. Liouville, informed by Sturm of the prob-
lems he was working on, took up the same subject. The results of their joint work was
published in several papers which are quite detailed.
</p>
<p>Suppose that the above Hilbert-Schmidt operator K has a zero eigenvalue.
Then, there must exists a nonzero function v such that K[v](x)= 0, i.e.,
</p>
<p>&minus;u2(x)
W(a)
</p>
<p>ˆ x
</p>
<p>a
</p>
<p>u1(t)v(t) dt &minus;
u1(x)
</p>
<p>W(a)
</p>
<p>ˆ b
</p>
<p>x
</p>
<p>u2(t)v(t) dt = 0 (19.4)
</p>
<p>for all x. Differentiate this twice to get
</p>
<p>&minus;u
&prime;&prime;
2(x)
</p>
<p>W(a)
</p>
<p>ˆ x
</p>
<p>a
</p>
<p>u1(t)v(t) dt &minus;
u&prime;&prime;1(x)
W(a)
</p>
<p>ˆ b
</p>
<p>x
</p>
<p>u2(t)v(t) dt + v(x)= 0.
</p>
<p>Now substitute u&prime;&prime;1 = qu1 and u&prime;&prime;2 = qu2 in this equation and use Eq. (19.4) to
conclude that v = 0. This is impossible because no eigenvector can be zero.
Hence, zero is not an eigenvalue of K, i.e., M0 = {0}. Since eigenvectors
of K= L&minus;1x coincide with eigenvectors of Lx , and eigenvalues of Lx are the
reciprocals of the eigenvalues of K, we have the following result.
</p>
<p>Theorem for regular
</p>
<p>Sturm-Liouville systems
Theorem 19.1.9 A regular Sturm-Liouville system has a countable
number of eigenvalues that can be arranged in an increasing se-
quence that has infinity as its limit. The eigenvectors of the Sturm-
</p>
<p>7The reader recalls that when K acts on M0, it yields zero.</p>
<p/>
</div>
<div class="page"><p/>
<p>19.2 Sturm-Liouville Systems and SOLDEs 569
</p>
<p>Liouville operator are nondegenerate and constitute a complete or-
thogonal set. Furthermore, the eigenfunction un(x) corresponding to
the eigenvalue λn has exactly n zeros in its interval of definition.
</p>
<p>The last statement is not a result of operator theory, but can be derived
using the theory of differential equations. We shall not present the details
of its derivation. We need to emphasize that the boundary conditions are an
integral part of S-L systems. Changing the boundary conditions so that, for
example, they are no longer separated may destroy the regularity of the S-L
system.
</p>
<p>19.2 Sturm-Liouville Systems and SOLDEs
</p>
<p>We are now ready to combine our discussion of the preceding section with
the knowledge gained from our study of differential equations. We saw in
Chap. 13 that the separation of PDEs normally results in expressions of the
form
</p>
<p>L[u] + λu= 0, or p2(x)
d2u
</p>
<p>dx2
+ p1(x)
</p>
<p>du
</p>
<p>dx
+ p0(x)u+ λu= 0, (19.5)
</p>
<p>where u is a function of a single variable and λ is, a priori, an arbitrary
constant. This is an eigenvalue equation for the operator L, which is not, in
general, self-adjoint. If we use Theorem 14.5.4 and multiply (19.5) by
</p>
<p>w(x)= 1
p2(x)
</p>
<p>exp
</p>
<p>[
ˆ x p1(t)
</p>
<p>p2(t)
dt
</p>
<p>]
,
</p>
<p>it becomes self-adjoint for real λ, and can be written as
</p>
<p>d
</p>
<p>dx
</p>
<p>[
p(x)
</p>
<p>du
</p>
<p>dx
</p>
<p>]
+
[
λw(x)&minus; q(x)
</p>
<p>]
u= 0 (19.6)
</p>
<p>with p(x) = w(x)p2(x) and q(x) = &minus;p0(x)w(x). Equation (19.6) is the
standard form of the S-L equation. However, it is not in the form studied in
the previous section. To turn it into that form one changes both the indepen-
dent and dependent variables via the so-called Liouville substitution: Liouville substitution
</p>
<p>u(x)= v(t)
[
p(x)w(x)
</p>
<p>]&minus;1/4
, t =
</p>
<p>ˆ x
</p>
<p>a
</p>
<p>&radic;
w(s)
</p>
<p>p(s)
ds. (19.7)
</p>
<p>It is then a matter of chain-rule differentiation to show that Eq. (19.6) be-
comes
</p>
<p>d2v
</p>
<p>dt2
+
[
λ&minus;Q(t)
</p>
<p>]
v = 0, (19.8)
</p>
<p>where
</p>
<p>Q(t)= q(x(t))
w(x(t))
</p>
<p>+
[
p
(
x(t)
</p>
<p>)
w
(
x(t)
</p>
<p>)]&minus;1/4 d2
dt2
</p>
<p>[
(pw)1/4
</p>
<p>]
.</p>
<p/>
</div>
<div class="page"><p/>
<p>570 19 Sturm-Liouville Systems
</p>
<p>Therefore, Theorem 19.1.9 still holds.
</p>
<p>Historical Notes
</p>
<p>Joseph Liouville (1809&ndash;1882) was a highly respected professor at the Coll&egrave;ge de France,
</p>
<p>Joseph Liouville
</p>
<p>1809&ndash;1882
</p>
<p>in Paris, and the founder and editor of the Journal des Math&eacute;matiques Pures et Ap-
pliqu&eacute;es, a famous periodical that played an important role in French mathematical life
through the latter part of the nineteenth century. His own remarkable achievements as a
creative mathematician have only recently received the appreciation they deserve.
He was the first to solve a boundary value problem by solving an equivalent integral
equation. His ingenious theory of fractional differentiation answered the long-standing
question of what reasonable meaning can be assigned to the symbol dny/dxn when n
is not a positive integer. He discovered the fundamental result in complex analysis that
a bounded entire function is necessarily a constant and used it as the basis for his own
theory of elliptic functions. There is also a well-known Liouville theorem in Hamilto-
nian mechanics, which states that volume integrals are time-invariant in phase space. In
collaboration with Sturm, he also investigated the eigenvalue problem of second-order
differential equations.
The theory of transcendental numbers is another branch of mathematics that originated
in Liouville&rsquo;s work. The irrationality of π and e (the fact that they are not solutions of
any linear equations) had been proved in the eighteenth century by Lambert and Euler.
In 1844 Liouville showed that e is not a root of any quadratic equation with integral
coefficients as well. This led him to conjecture that e is transcendental, which means that
it does not satisfy any polynomial equation with integral coefficients.
</p>
<p>Example 19.2.1 The Liouville substitution [Eq. (19.7)] transforms the
Bessel DE (xu&prime;)&prime; + (k2x &minus; ν2/x)u= 0 into
</p>
<p>d2v
</p>
<p>dt2
+
[
k2 &minus; ν
</p>
<p>2 &minus; 1/4
t2
</p>
<p>]
v = 0,
</p>
<p>from which we can obtain an interesting result when ν = 12 . In that case we
have v̈+ k2v = 0, whose solutions are of the form coskt and sinkt . Noting
that u(x)= J1/2(x), Eq. (19.7) gives
</p>
<p>J1/2(kt)=A
sin kt&radic;
</p>
<p>t
or J1/2(kt)= B
</p>
<p>coskt&radic;
t
</p>
<p>,
</p>
<p>and since J1/2(x) is analytic at x = 0, we must have J1/2(kt)=A sin kt/
&radic;
t ,
</p>
<p>which is the result obtained in Chap. 15.
</p>
<p>The appearance of w is the result of our desire to render the differential
operator self-adjoint. It also appears in another context. Recall the Lagrange
identity for a self-adjoint differential operator L:
</p>
<p>uL[v] &minus; vL[u] = d
dx
</p>
<p>{
p(x)
</p>
<p>[
u(x)v&prime;(x)&minus; v(x)u&prime;(x)
</p>
<p>]}
. (19.9)
</p>
<p>If we specialize this identity to the S-L equation of (19.6) with u= u1 cor-
responding to the eigenvalue λ1 and v = u2 corresponding to the eigenvalue
λ2, we obtain for the LHS
</p>
<p>u1L[u2] &minus; u2L[u1] = u1(&minus;λ2wu2)+ u2(λ1wu1)= (λ1 &minus; λ2)wu1u2.</p>
<p/>
</div>
<div class="page"><p/>
<p>19.2 Sturm-Liouville Systems and SOLDEs 571
</p>
<p>Integrating both sides of (19.9) then yields
</p>
<p>(λ1 &minus; λ2)
ˆ b
</p>
<p>a
</p>
<p>wu1u2dx =
{
p(x)
</p>
<p>[
u1(x)u
</p>
<p>&prime;
2(x)&minus; u2(x)u&prime;1(x)
</p>
<p>]}b
a
. (19.10)
</p>
<p>A desired property of the solutions of a self-adjoint DE is their orthogonal-
ity when they belong to different eigenvalues. This property will be satisfied
if we assume an inner product integral with weight function w(x), and if
the RHS of Eq. (19.10) vanishes. There are various boundary conditions
that fulfill the latter requirement. For example, u1 and u2 could satisfy the
boundary conditions of Eq. (19.2). Another set of appropriate boundary con-
ditions (BC) is the periodic BC given by periodic boundary
</p>
<p>conditions
u(a)= u(b) and u&prime;(a)= u&prime;(b). (19.11)
</p>
<p>However, as the following example shows, the latter BCs do not lead to a
regular S-L system.
</p>
<p>Example 19.2.2 The following examples show how BC can change the S-L
systems.
</p>
<p>(a) The S-L system consisting of the S-L equation d2u/dt2 + ω2u = 0
in the interval [0, T ] with the separated BCs u(0) = 0 and u(T ) =
0 has the eigenfunctions un(t) = sin nπT t with n = 1,2, . . . and the
eigenvalues λn = ω2n = (nπ/T )2 with n= 1,2, . . . .
</p>
<p>(b) Let the S-L equation be the same as in part (a) but change the interval
to [&minus;T ,+T ] and the BCs to a periodic one such as u(&minus;T ) = u(T )
and u&prime;(&minus;T )= u&prime;(T ). The eigenvalues are the same as before, but the
eigenfunctions are 1, sin(nπt/T ), and cos(nπt/T ), where n is a posi-
tive integer. Note that there is a degeneracy here in the sense that there
are two linearly independent eigenfunctions having the same eigen-
value (nπ/T )2. By Theorem 19.1.9, the S-L system is not regular.
</p>
<p>(c) The Bessel equation for a given fixed ν2 is
</p>
<p>u&prime;&prime; + 1
x
u&prime; +
</p>
<p>(
k2 &minus; ν
</p>
<p>2
</p>
<p>x2
</p>
<p>)
u= 0, where a &le; x &le; b,
</p>
<p>and it can be turned into an S-L system if we multiply it by
</p>
<p>w(x)= 1
p2(x)
</p>
<p>exp
</p>
<p>[
ˆ x p1(t)
</p>
<p>p2(t)
dt
</p>
<p>]
= exp
</p>
<p>[
ˆ x dt
</p>
<p>t
</p>
<p>]
= x.
</p>
<p>Then we can write
</p>
<p>d
</p>
<p>dx
</p>
<p>(
x
du
</p>
<p>dx
</p>
<p>)
+
(
k2x &minus; ν
</p>
<p>2
</p>
<p>x
</p>
<p>)
u= 0,
</p>
<p>which is in the form of Eq. (19.6) with p = w = x, λ = k2, and
q(x)= ν2/x. If a &gt; 0, we can obtain a regular S-L system by applying
appropriate separated BCs.</p>
<p/>
</div>
<div class="page"><p/>
<p>572 19 Sturm-Liouville Systems
</p>
<p>A regular S-L system is too restrictive for applications where either a or
b or both may be infinite or where either a or b may be a singular point of
the S-L equation. A singular S-L system is one for which one or more ofsingular S-L systems
the following conditions hold:
</p>
<p>1. The interval [a, b] stretches to infinity in either or both directions.
2. Either p or w vanishes at one or both end points a and b.
3. The function q(x) is not continuous in [a, b].
4. Any one of the functions p(x), q(x), and w(x) is singular at a or b.
</p>
<p>Even though the conclusions concerning eigenvalues of a regular S-L
system cannot be generalized to the singular S-L system, the orthogonality
of eigenfunctions corresponding to different eigenvalues can, as long as the
eigenfunctions are square-integrable with weight function w(x):
</p>
<p>Box 19.2.3 The eigenfunctions of a singular S-L system are orthogo-
nal if the RHS of (19.10) vanishes.
</p>
<p>Example 19.2.4 Bessel functions Jν(x) are entire functions. Thus, they are
square-integrable in the interval [0, b] for any finite positive b. For fixed ν
the DE
</p>
<p>r2
d2u
</p>
<p>dr2
+ r du
</p>
<p>dr
+
(
k2r2 &minus; ν2
</p>
<p>)
u= 0 (19.12)
</p>
<p>transforms into the Bessel equation x2u&prime;&prime; + xu&prime; + (x2 &minus; ν2)u = 0 if we
make the substitution kr = x. Thus, the solution of the singular S-L equa-
tion (19.12) that is analytic at r = 0 and corresponds to the eigenvalue k2 is
uk(r)= Jν(kr). For two different eigenvalues, k21 and k22 , the eigenfunctions
are orthogonal if the boundary term of (19.10) corresponding to Eq. (19.12)
vanishes, that is, if
</p>
<p>{
r
[
Jν(k1r)J
</p>
<p>&prime;
ν(k2r)&minus; Jν(k2r)J &prime;ν(k1r)
</p>
<p>]}b
0
</p>
<p>vanishes, which will occur if and only if Jν(k1b)J &prime;ν(k2b)&minus;Jν(k2b)J &prime;ν(k1b)=
0. A common choice is to take Jν(k1b) = 0 = Jν(k2b), that is, to take
both k1b and k2b as (different) roots of the Bessel function of order ν.
We thus have
</p>
<p>&acute; b
</p>
<p>0 rJν(kir)Jν(kj r) dr = 0 if ki and kj are different roots of
Jν(kb)= 0.
</p>
<p>The Legendre equation
</p>
<p>d
</p>
<p>dx
</p>
<p>[(
1 &minus; x2
</p>
<p>)du
dx
</p>
<p>]
+ λu= 0, where &minus;1 &lt; x &lt; 1,
</p>
<p>is already self-adjoint. Thus, w(x)= 1, and p(x)= 1 &minus; x2. The eigenfunc-
tions of this singular S-L system [singular because p(1)= p(&minus;1)= 0] are
regular at the end points x = &plusmn;1 and are the Legendre polynomials Pn(x)
corresponding to λ= n(n+ 1). The boundary term of (19.10) clearly van-
ishes at a =&minus;1 and b=+1. Since Pn(x) are square-integrable on [&minus;1,+1],</p>
<p/>
</div>
<div class="page"><p/>
<p>19.3 Asymptotic Behavior 573
</p>
<p>we obtain the familiar orthogonality relation:
&acute; +1
&minus;1 Pn(x)Pm(x) dx = 0 if
</p>
<p>m �= n.
The Hermite DE is
</p>
<p>u&prime;&prime; &minus; 2xu&prime; + λu= 0. (19.13)
It is transformed into an S-L system if we multiply it by w(x)= e&minus;x2 . The
resulting S-L equation is
</p>
<p>d
</p>
<p>dx
</p>
<p>[
e&minus;x
</p>
<p>2 du
</p>
<p>dx
</p>
<p>]
+ λe&minus;x2u= 0. (19.14)
</p>
<p>The boundary term corresponding to the two eigenfunctions u1(x) and
u2(x) having the respective eigenvalues λ1 and λ2 �= λ1 is
</p>
<p>{
e&minus;x
</p>
<p>2[
u1(x)u
</p>
<p>&prime;
2(x)&minus; u2(x)u&prime;1(x)
</p>
<p>]}b
a
.
</p>
<p>This vanishes for arbitrary u1 and u2 (because they are Hermite polynomi-
als) if a =&minus;&infin; and b=+&infin;.
</p>
<p>The function u is an eigenfunction of (19.14) corresponding to the eigen-
value λ if and only if it is a solution of (19.13). Solutions of this DE
corresponding to λ = 2n are the Hermite polynomials Hn(x) discussed in
Chap. 8. We can therefore write
</p>
<p>&acute; +&infin;
&minus;&infin; e
</p>
<p>&minus;x2Hn(x)Hm(x) dx = 0 if m �= n.
This orthogonality relation was also derived in Chap. 8.
</p>
<p>19.3 Asymptotic Behavior
</p>
<p>The S-L problem is central to the solution of many DEs in mathematical
physics. In some cases the S-L equation has a direct bearing on the physics.
For example, the eigenvalue λ may correspond to the orbital angular mo-
mentum of an electron in an atom (see the treatment of spherical harmonics
in Chap. 13) or to the energy levels of a particle in a potential (see Exam-
ple 15.5.1). In many cases, then, it is worthwhile to gain some knowledge
of the behavior of an S-L system in the limit of large λ&mdash;high angular mo-
mentum or high energy. Similarly, it is useful to understand the behavior of
the solutions for large values of their arguments. We therefore devote this
section to a discussion of the behavior of solutions of an S-L system in the
limit of large eigenvalues and large independent variable.
</p>
<p>19.3.1 Large Eigenvalues
</p>
<p>We assume that the S-L operator has the form given in Eq. (19.1). This can
always be done for an arbitrary second-order linear DE by multiplying it by
a proper function (to make it self-adjoint) followed by a Liouville substitu-
tion. So, consider an S-L systems of the following form:
</p>
<p>u&prime;&prime; +
[
λ&minus; q(x)
</p>
<p>]
u&equiv; u&prime;&prime; +Q(x)u= 0 where Q= λ&minus; q (19.15)</p>
<p/>
</div>
<div class="page"><p/>
<p>574 19 Sturm-Liouville Systems
</p>
<p>with separated BCs of (19.2). Let us assume that Q(x) &gt; 0 for all x &isin; [a, b],
that is, λ &gt; q(x). This is reasonable, since we are interested in very large λ.
</p>
<p>The study of the system of (19.15) and (19.2) is simplified if we make
the Pr&uuml;fer substitution:Pr&uuml;fer substitution
</p>
<p>u=RQ&minus;1/4 sinφ, u&prime; =RQ1/4 cosφ, (19.16)
</p>
<p>where R(x,λ) and φ(x,λ) are λ-dependent functions of x. This substitu-
tion transforms the S-L equation of (19.15) into a pair of equations (see
Problem 19.3):
</p>
<p>dφ
</p>
<p>dx
=
&radic;
λ&minus; q(x)&minus; q
</p>
<p>&prime;
</p>
<p>4[λ&minus; q(x)] sin 2φ,
</p>
<p>dR
</p>
<p>dx
= Rq
</p>
<p>&prime;
</p>
<p>4[λ&minus; q(x)] cos 2φ.
(19.17)
</p>
<p>The function R(x,λ) is assumed to be positive because any negativity
of u can be transferred to the phase φ(x,λ). Also, R cannot be zero at any
point of [a, b], because both u and u&prime; would vanish at that point, and, by
Lemma 14.3.3, u(x)= 0. Equation (19.17) is very useful in discussing the
asymptotic behavior of solutions of S-L systems both when λ &rarr; &infin; and
when x &rarr; &infin;. Before we discuss such asymptotics, we need to make a
digression.
</p>
<p>It is often useful to have a notation for the behavior of a function f (x,λ)
for large λ and all values of x. If the function remains bounded for all val-
ues of x as λ&rarr;&infin;, we write f (x,λ) = O(1). Intuitively, this means that
as λ gets larger and larger, the magnitude of the function f (x,λ) remains
of order 1. In other words, for no value of x is limλ&rarr;&infin; f (x,λ) infinite.
If λnf (x,λ) = O(1), then we can write f (x,λ) = O(1)/λn. This means
that as λ tends to infinity, f (x,λ) goes to zero as fast as 1/λn does. Some-
times this is written as f (x,λ)=O(λ&minus;n). Some properties of O(1) are as
follows:
</p>
<p>1. If a is a finite real number, then O(1)+ a =O(1).
2. O(1)+O(1)=O(1), and O(1)O(1)=O(1).
3. For finite a and b,
</p>
<p>&acute; b
</p>
<p>a
O(1) dx =O(1).
</p>
<p>4. If r and s are real numbers with r &le; s, then
</p>
<p>O(1)λr +O(1)λs =O(1)λs .
</p>
<p>5. If g(x) is any bounded function of x, then a Taylor series expansion
yields
</p>
<p>[
λ+ g(x)
</p>
<p>]r = λr
[
</p>
<p>1 + g(x)
λ
</p>
<p>]r
</p>
<p>= λr
{
</p>
<p>1 + r g(x)
λ
</p>
<p>+ r(r &minus; 1)
2
</p>
<p>[
g(x)
</p>
<p>λ
</p>
<p>]2
+ O(1)
</p>
<p>λ3
</p>
<p>}
</p>
<p>= λr + rg(x)λr&minus;1 +O(1)λr&minus;2 = λr +O(1)λr&minus;1
</p>
<p>=O(1)λr .</p>
<p/>
</div>
<div class="page"><p/>
<p>19.3 Asymptotic Behavior 575
</p>
<p>Returning to Eq. (19.17) and expanding its RHSs using property 5, we
obtain
</p>
<p>dφ
</p>
<p>dx
=
&radic;
λ+ O(1)&radic;
</p>
<p>λ
+ O(1)
</p>
<p>λ
=
&radic;
λ+ O(1)&radic;
</p>
<p>λ
,
</p>
<p>dR
</p>
<p>dx
= O(1)
</p>
<p>λ
.
</p>
<p>Taylor series expansion of φ(x,λ) and R(x,λ) about x = a then yields
</p>
<p>φ(x,λ)= φ(a,λ)+ (x &minus; a)
&radic;
λ+ O(1)&radic;
</p>
<p>λ
,
</p>
<p>R(x,λ)=R(a,λ)+ O(1)
λ
</p>
<p>(19.18)
</p>
<p>for λ&rarr;&infin;. These results are useful in determining the behavior of λn for
large n. As an example, we use (19.2) and (19.16) to write
</p>
<p>&minus;α1
β1
</p>
<p>= u
&prime;(a)
u(a)
</p>
<p>= R(a,λ)Q
1/4(a,λ) cos[φ(a,λ)]
</p>
<p>R(a,λ)Q&minus;1/4(a,λ) sin[φ(a,λ)]
=Q1/2(a,λ) cot
</p>
<p>[
φ(a,λ)
</p>
<p>]
,
</p>
<p>where we have assumed that β1 �= 0. If β1 = 0, we can take the ratio β1/α1,
which is finite because at least one of the two constants must be different
from zero. Let A=&minus;α1/β1 and write cot[φ(a,λ)] = A/
</p>
<p>&radic;
λ&minus; q(a). Simi-
</p>
<p>larly, cot[φ(b,λ)] = B/&radic;λ&minus; q(b), where B =&minus;α2/β2. Let us concentrate
on the nth eigenvalue and write
</p>
<p>φ(a,λn)= cot&minus;1
A&radic;
</p>
<p>λn &minus; q(a)
, φ(b,λn)= cot&minus;1
</p>
<p>A&radic;
λn &minus; q(b)
</p>
<p>.
</p>
<p>For large λn the argument of cot&minus;1 is small. Therefore, we can expand the
RHS in a Taylor series about zero:
</p>
<p>cot&minus;1 ǫ = cot&minus;1(0)&minus; ǫ + &middot; &middot; &middot; = π
2
&minus; ǫ + &middot; &middot; &middot; = π
</p>
<p>2
+ O(1)&radic;
</p>
<p>λn
</p>
<p>for ǫ =O(1)/&radic;λn. It follows that
</p>
<p>φ(a,λn)=
π
</p>
<p>2
+ O(1)&radic;
</p>
<p>λn
, φ(b,λn)=
</p>
<p>π
</p>
<p>2
+ nπ + O(1)&radic;
</p>
<p>λn
. (19.19)
</p>
<p>The term nπ appears in (19.19) because, by Theorem 19.1.9, the nth eigen-
function has n zeros between a and b. Since u= RQ&minus;1/4 sinφ, this means
that sinφ must go through n zeros as x goes from a to b. Thus, at x = b the
phase φ must be nπ larger than at x = a.
</p>
<p>Substituting x = b in the first equation of (19.18), with λ &rarr; λn, and
using (19.19), we obtain
</p>
<p>π
</p>
<p>2
+ nπ + O(1)&radic;
</p>
<p>λn
= π
</p>
<p>2
+ O(1)&radic;
</p>
<p>λn
+ (b&minus; a)
</p>
<p>&radic;
λn +
</p>
<p>O(1)&radic;
λn
</p>
<p>,
</p>
<p>or
</p>
<p>(b&minus; a)
&radic;
λn = nπ +
</p>
<p>O(1)&radic;
λn
</p>
<p>. (19.20)</p>
<p/>
</div>
<div class="page"><p/>
<p>576 19 Sturm-Liouville Systems
</p>
<p>One consequence of this result is that, limn&rarr;&infin; nλ
&minus;1/2
n = (b&minus; a)/π . Thus,&radic;
</p>
<p>λn = Cnn, where limn&rarr;&infin;Cn = π/(b&minus;a), and Eq. (19.20) can be rewrit-
ten as
</p>
<p>&radic;
λn =
</p>
<p>nπ
</p>
<p>b&minus; a +
O(1)
</p>
<p>Cnn
= nπ
</p>
<p>b&minus; a +
O(1)
</p>
<p>n
. (19.21)
</p>
<p>This equation describes the asymptotic behavior of eigenvalues. The fol-
lowing theorem, stated without proof, describes the asymptotic behavior of
eigenfunctions.
</p>
<p>Theorem 19.3.1 Let {un(x)}&infin;n=0 be the normalized eigenfunctions of the
regular S-L system given by Eqs. (19.15) and (19.2) with β1β2 �= 0. Then,
for n&rarr;&infin;,asymptotic behavior of
</p>
<p>solutions of large order
</p>
<p>un(x)=
&radic;
</p>
<p>2
</p>
<p>b&minus; a cos
nπ(x &minus; a)
</p>
<p>b&minus; a +
O(1)
</p>
<p>n
.
</p>
<p>Example 19.3.2 Let us derive an asymptotic formula for the Legendre
polynomials Pn(x). We first make the Liouville substitution to transform
the Legendre DE [(1 &minus; x2)P &prime;]&prime; + n(n+ 1)Pn = 0 into
</p>
<p>d2v
</p>
<p>dt2
+
[
λn &minus;Q(t)
</p>
<p>]
v = 0, where λn = n(n+ 1). (19.22)
</p>
<p>Here p(x) = 1 &minus; x2 and w(x) = 1, so t =
&acute; x
</p>
<p>ds/
&radic;
</p>
<p>1 &minus; s2 = cos&minus;1 x, or
x(t)= cos t , and
</p>
<p>Pn
(
x(t)
</p>
<p>)
= v(t)
</p>
<p>[
1 &minus; x2(t)
</p>
<p>]&minus;1/4 = v(t)&radic;
sin t
</p>
<p>. (19.23)
</p>
<p>In Eq. (19.22)
</p>
<p>Q(t)=
(
1 &minus; x2
</p>
<p>)&minus;1/4 d2
dt2
</p>
<p>[(
1 &minus; x2
</p>
<p>)1/4]
</p>
<p>= 1&radic;
sin t
</p>
<p>d2
</p>
<p>dt2
[
&radic;
</p>
<p>sin t] = &minus;1
4
</p>
<p>(
1 + 1
</p>
<p>sin2 t
</p>
<p>)
.
</p>
<p>For large n we can neglect Q(t), make the approximation λn &asymp; (n + 12 )2,
and write v̈ + (n+ 12 )2v = 0, whose general solution is
</p>
<p>v(t)=A cos
[(
</p>
<p>n+ 1
2
</p>
<p>)
t + α
</p>
<p>]
,
</p>
<p>where A and α are arbitrary constants. Substituting this solution in (19.23)
yields Pn(cos t)=A cos[(n+ 12 )t +α]/
</p>
<p>&radic;
sin t . To determine α we note that
</p>
<p>Pn(0) = 0 if n is odd. Thus, if we let t = π/2, the cosine term vanishes
for odd n if α =&minus;π/4. Thus, the general asymptotic formula for Legendre
polynomials is
</p>
<p>Pn(cos t)=
A&radic;
sin t
</p>
<p>cos
</p>
<p>[(
n+ 1
</p>
<p>2
</p>
<p>)
t &minus; π
</p>
<p>4
</p>
<p>]
for n&rarr;&infin;.</p>
<p/>
</div>
<div class="page"><p/>
<p>19.4 Expansions in Terms of Eigenfunctions 577
</p>
<p>19.3.2 Large Argument
</p>
<p>Liouville and Pr&uuml;fer substitutions are useful in investigating the behavior of
the solutions of S-L systems for large x as well. The general procedure is to
transform the DE into the form of Eq. (19.8) by the Liouville substitution;
then make the Pr&uuml;fer substitution of (19.16) to obtain two DEs in the form
of (19.17). Solving Eq. (19.17) when x &rarr; &infin; determines the behavior of
φ and R and, subsequently, of u, the solution. Problem 19.4 illustrates this
procedure for the Bessel functions. We simply quote the results:
</p>
<p>Jν(x)=
&radic;
</p>
<p>2
</p>
<p>πx
cos
</p>
<p>[
x &minus;
</p>
<p>(
ν + 1
</p>
<p>2
</p>
<p>)
π
</p>
<p>2
+ ν
</p>
<p>2 &minus; 1/4
2x
</p>
<p>]
+ O(1)
</p>
<p>x5/2
,
</p>
<p>Yν(x)=
&radic;
</p>
<p>2
</p>
<p>πx
sin
</p>
<p>[
x &minus;
</p>
<p>(
ν + 1
</p>
<p>2
</p>
<p>)
π
</p>
<p>2
+ ν
</p>
<p>2 &minus; 1/4
2x
</p>
<p>]
+ O(1)
</p>
<p>x5/2
.
</p>
<p>These two relations easily yield the asymptotic expressions for the Hankel
functions:
</p>
<p>H (1)ν (x)= Jν(x)+ iYν(x)
</p>
<p>=
&radic;
</p>
<p>2
</p>
<p>πx
exp
</p>
<p>{
i
</p>
<p>[
x &minus;
</p>
<p>(
ν + 1
</p>
<p>2
</p>
<p>)
π
</p>
<p>2
+ ν
</p>
<p>2 &minus; 1/4
2x
</p>
<p>]}
+ O(1)
</p>
<p>x5/2
,
</p>
<p>H (2)ν (x)= Jν(x)&minus; iYν(x)
</p>
<p>=
&radic;
</p>
<p>2
</p>
<p>πx
exp
</p>
<p>{
&minus;i
</p>
<p>[
x &minus;
</p>
<p>(
ν + 1
</p>
<p>2
</p>
<p>)
π
</p>
<p>2
+ ν
</p>
<p>2 &minus; 1/4
2x
</p>
<p>]}
+ O(1)
</p>
<p>x5/2
.
</p>
<p>If the last term in the exponent&mdash;which vanishes as x &rarr;&infin;&mdash;is ignored, the
asymptotic expression for H (1)ν (x) matches what was obtained in Chap. 16
using the method of steepest descent.
</p>
<p>19.4 Expansions in Terms of Eigenfunctions
</p>
<p>Chapter 13 showed how the solution of many PDEs can be written as the
product of the solutions of the separated ODEs. These DEs are usually of
Sturm-Liouville type. We saw this in the construction of spherical harmon-
ics. In the rest of this chapter, consisting mainly of illustrative examples, we
shall consider the use of other coordinate systems and construct solutions to
DEs as infinite series expansions in terms of S-L eigenfunctions.
</p>
<p>Central to the expansion of solutions in terms of S-L eigenfunctions is
the question of their completeness. This completeness was established for a
regular S-L system in Theorem 19.1.9.
</p>
<p>We shall shortly state an analogous theorem (without proof) that estab-
lishes the completeness of the eigenfunctions of more general S-L systems.
This theorem requires the following generalization of the separated and the</p>
<p/>
</div>
<div class="page"><p/>
<p>578 19 Sturm-Liouville Systems
</p>
<p>periodic BCs:
</p>
<p>R1u&equiv; α11u(a)+ α12u&prime;(a)+ α13u(b)+ α14u&prime;(b)= 0,
R2u&equiv; α21u(a)+ α22u&prime;(a)+ α23u(b)+ α24u&prime;(b)= 0,
</p>
<p>(19.24)
</p>
<p>where αij are numbers such that the rank of the following matrix is 2:
</p>
<p>a =
(
α11 α12 α13 α14
α21 α22 α23 α24
</p>
<p>)
.
</p>
<p>The separated BCs correspond to the case for which α11 = α1, α12 = β1,
α23 = α2, and α24 = β2, with all other αij zero. Similarly, the periodic BC
is a special case for which α11 = &minus;α13 = α22 = &minus;α24 = 1, with all other
αij zero. It is easy to verify that the rank of the matrix a is 2 for these two
special cases. Let
</p>
<p>U=
{
u &isin; C2[a, b] | Rju= 0, for j = 1,2
</p>
<p>}
(19.25)
</p>
<p>be a subspace of L2w(a, b), and&mdash;to assure the vanishing of the RHS of the
Lagrange identity&mdash;assume that the following equality holds:
</p>
<p>p(b)det
</p>
<p>(
α11 α12
α21 α22
</p>
<p>)
= p(a)det
</p>
<p>(
α13 α14
α23 α24
</p>
<p>)
. (19.26)
</p>
<p>We are now ready to consider the theorem (for a proof, see [Hell 67,
Chap. 7]).
</p>
<p>Theorem 19.4.1 The eigenfunctions {un(x)}&infin;n=1 of an S-L system
consisting of the S-L equation (pu&prime;)&prime; + (λw &minus; q)u = 0 and the BCs
of (19.24) form a complete basis of the subspace U of L2w(a, b) de-
scribed in (19.25). The eigenvalues are real and countably infinite and
each one has a multiplicity of at most 2. They can be ordered accord-
ing to size λ1 &le; λ2 &le; &middot; &middot; &middot; , and their only limit point is +&infin;.
</p>
<p>First note that Eq. (19.26) contains both separated and periodic BCs as
special cases (Problem 19.5). In the case of periodic BCs, we assume that
p(a) = p(b). Thus, all the eigenfunctions discussed so far are covered by
Theorem 19.4.1. Second, the orthogonality of eigenfunctions corresponding
to different eigenvalues and the fact that there are infinitely many distinct
eigenvalues assure the existence of infinitely many eigenfunctions. Third,
the eigenfunctions form a basis of U and not the whole L2w(a, b). Only
those functions u &isin; L2w(a, b) that satisfy the BC in (19.24) are expandable
in terms of un(x). Finally, the last statement of Theorem 19.4.1 is a repeti-
tion of part of Theorem 19.1.9 but is included because the conditions under
which Theorem 19.4.1 holds are more general than those applying to Theo-
rem 19.1.9.
</p>
<p>Part II discussed orthogonal functions in detail and showed how other
functions can be expanded in terms of them. However, the procedure used
in Part II was ad hoc from a logical standpoint. After all, the orthogonal</p>
<p/>
</div>
<div class="page"><p/>
<p>19.5 Separation in Cartesian Coordinates 579
</p>
<p>Fig. 19.1 A rectangular conducting box of which one face is held at the potential f (x, y)
and the other faces are grounded
</p>
<p>polynomials were invented by nineteenth-century mathematical physicists
who, in their struggle to solve the PDEs of physics using the separation of
variables, came across various ODEs of the second order, all of which were
recognized later as S-L systems. From a logical standpoint, therefore, this
chapter should precede Part II. But the order of the chapters was based on
clarity and ease of presentation and the fact that the machinery of differential
equations was a prerequisite for such a discussion.
</p>
<p>Theorem 19.4.1 is the important link between the algebraic and the an-
alytic machinery of differential equation theory. This theorem puts at our
disposal concrete mathematical functions that are calculable to any desired
accuracy (on a computer, say) and can serve as basis functions for all the
expansions described in Part II. The remainder of this chapter is devoted to
solving some PDEs of mathematical physics using the separation of vari-
ables and Theorem 19.4.1.
</p>
<p>19.5 Separation in Cartesian Coordinates
</p>
<p>Problems most suitable for Cartesian coordinates have boundaries with rect-
angular symmetry such as boxes or planes.
</p>
<p>19.5.1 Rectangular Conducting Box
</p>
<p>Consider a rectangular conducting box with sides a, b, and c (see Fig. 19.1).
All faces are held at zero potential except the top face, whose potential is
given by a function f (x, y). Let us find the potential at all points inside the
box.
</p>
<p>The relevant PDE for this situation is Laplace&rsquo;s equation, &nabla;2Φ = 0. Writ-
ing Φ(x,y, z) as a product of three functions, Φ(x,y, z)=X(x)Y (y)Z(z),
yields three ODEs (see Problem 19.6):
</p>
<p>d2X
</p>
<p>dx2
+ λX = 0, d
</p>
<p>2Y
</p>
<p>dy2
+μY = 0, d
</p>
<p>2Z
</p>
<p>dz2
+ νZ = 0, (19.27)
</p>
<p>where λ+μ+ ν = 0. The vanishing of Φ at x = 0 and x = a means that
</p>
<p>Φ(0, y, z) = X(0)Y (y)Z(z)= 0 &forall;y, z &rArr; X(0)= 0,
Φ(a, y, z) = X(a)Y (y)Z(z)= 0 &forall;y, z &rArr; X(a)= 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>580 19 Sturm-Liouville Systems
</p>
<p>We thus obtain an S-L system, X&prime;&prime; + λX = 0, X(0)= 0 =X(a), whose BC
is neither separated nor periodic, but satisfies (19.24) with α11 = α23 = 1
and all other αij zero. This S-L system has the eigenvalues and eigenfunc-
tions
</p>
<p>λn =
(
nπ
</p>
<p>a
</p>
<p>)2
and Xn(x)= sin
</p>
<p>(
nπ
</p>
<p>a
x
</p>
<p>)
for n= 1,2, . . . .
</p>
<p>Similarly, the second equation in (19.27) leads to
</p>
<p>μm =
(
mπ
</p>
<p>b
</p>
<p>)2
and Ym(y)= sin
</p>
<p>(
mπ
</p>
<p>b
y
</p>
<p>)
for m= 1,2, . . . .
</p>
<p>On the other hand, the third equation in (19.27) does not lead to an S-L
system because the BC for the top of the box does not fit (19.24). This is
as expected because the &ldquo;eigenvalue&rdquo; ν is already determined by λ and μ.
Nevertheless, we can find a solution for that equation. The substitution
</p>
<p>γ 2mn =
(
nπ
</p>
<p>a
</p>
<p>)2
+
(
mπ
</p>
<p>b
</p>
<p>)2
</p>
<p>changes the Z equation to Z&prime;&prime; &minus; γ 2mnZ = 0, whose solution, consistent with
Z(0)= 0, is Z(z)= Cmn sinh(γmnz).
</p>
<p>We note that X(x) and Y(y) are functions satisfying R1X = 0 = R2X.
Thus, by Theorem 19.4.1, they can be written as a linear combination of
Xn(x) and Ym(y):
</p>
<p>X(x)=
&infin;&sum;
</p>
<p>n=1
An sin(nπx/a) and Y(y)=
</p>
<p>&infin;&sum;
</p>
<p>m=1
Bm sin(mπb/y).
</p>
<p>Consequently, the most general solution can be expressed as
</p>
<p>Φ(x,y, z)=X(x)Y (y)Z(z)
</p>
<p>=
&infin;&sum;
</p>
<p>n=1
</p>
<p>&infin;&sum;
</p>
<p>m=1
Amn sin
</p>
<p>(
nπ
</p>
<p>a
x
</p>
<p>)
sin
</p>
<p>(
mπ
</p>
<p>b
y
</p>
<p>)
sinh(γmnz),
</p>
<p>where Amn =AnBmCmn.
To specify Φ completely, we must determine the arbitrary constants Amn.
</p>
<p>This is done by imposing the remaining BC, Φ(x,y, c)= f (x, y), yielding
the identity
</p>
<p>f (x, y)=
&infin;&sum;
</p>
<p>n=1
</p>
<p>&infin;&sum;
</p>
<p>m=1
Amn sin
</p>
<p>(
nπ
</p>
<p>a
x
</p>
<p>)
sin
</p>
<p>(
mπ
</p>
<p>b
y
</p>
<p>)
sinh(γmnc)
</p>
<p>=
&infin;&sum;
</p>
<p>n=1
</p>
<p>&infin;&sum;
</p>
<p>m=1
Bmn sin
</p>
<p>(
nπ
</p>
<p>a
x
</p>
<p>)
sin
</p>
<p>(
mπ
</p>
<p>b
y
</p>
<p>)
,</p>
<p/>
</div>
<div class="page"><p/>
<p>19.5 Separation in Cartesian Coordinates 581
</p>
<p>where Bmn &equiv;Amn sinh(γmnc). This is a two-dimensional Fourier series (see
Chap. 9) whose coefficients are given by
</p>
<p>Bmn =
4
</p>
<p>ab
</p>
<p>ˆ a
</p>
<p>0
dx
</p>
<p>ˆ b
</p>
<p>0
dyf (x, y) sin
</p>
<p>(
nπ
</p>
<p>a
x
</p>
<p>)
sin
</p>
<p>(
mπ
</p>
<p>b
y
</p>
<p>)
.
</p>
<p>Historical Notes
</p>
<p>Pierre Simon de Laplace (1749&ndash;1827) was a French mathematician and theoretical as-
</p>
<p>Pierre Simon de Laplace
</p>
<p>1749&ndash;1827
</p>
<p>tronomer who was so famous in his own time that he was known as the Newton of France.
His main interests throughout his life were celestial mechanics, the theory of probability,
and personal advancement.
At the age of 24 he was already deeply engaged in the detailed application of Newton&rsquo;s
law of gravitation to the solar system as a whole, in which the planets and their satel-
lites are not governed by the sun alone, but interact with one another in a bewildering
variety of ways. Even Newton had been of the opinion that divine intervention would oc-
casionally be needed to prevent this complex mechanism from degenerating into chaos.
Laplace decided to seek reassurance elsewhere, and succeeded in proving that the ideal
solar system of mathematics is a stable dynamical system that will endure unchanged for
all time. This achievement was only one of the long series of triumphs recorded in his
monumental treatise M&eacute;canique C&eacute;leste (published in five volumes from 1799 to 1825),
which summed up the work on gravitation of several generations of illustrious mathemati-
cians. Unfortunately for his later reputation, he omitted all reference to the discoveries of
his predecessors and contemporaries, and left it to be inferred that the ideas were entirely
his own. Many anecdotes are associated with this work. One of the best known describes
the occasion on which Napoleon tried to get a rise out of Laplace by protesting that
he had written a huge book on the system of the world without once mentioning God
as the author of the universe. Laplace is supposed to have replied, &ldquo;Sire, I had no need
of that hypothesis.&rdquo; The principal legacy of the M&eacute;canique C&eacute;leste to later generations
lay in Laplace&rsquo;s wholesale development of potential theory, with its far-reaching impli-
cations for a dozen different branches of physical science ranging from gravitation and
fluid mechanics to electromagnetism and atomic physics. Even though he lifted the idea
of the potential from Lagrange without acknowledgment, he exploited it so extensively
that ever since his time the fundamental equation of potential theory has been known as
Laplace&rsquo;s equation. After the French Revolution, Laplace&rsquo;s political talents and greed for
position came to full flower. His compatriots speak ironically of his &ldquo;suppleness&rdquo; and
&ldquo;versatility&rdquo; as a politician. What this really means is that each time there was a change
of regime (and there were many), Laplace smoothly adapted himself by changing his
principles&mdash;back and forth between fervent republicanism and fawning royalism&mdash;and
each time he emerged with a better job and grander titles. He has been aptly compared
with the apocryphal Vicar of Bray in English literature, who was twice a Catholic and
twice a Protestant. The Vicar is said to have replied as follows to the charge of being
a turncoat: &ldquo;Not so, neither, for if I changed my religion, I am sure I kept true to my
principle, which is to live and die the Vicar of Bray.&rdquo;
To balance his faults, Laplace was always generous in giving assistance and encourage-
ment to younger scientists. From time to time he helped forward in their careers such men
as the chemist Gay-Lussac, the traveler and naturalist Humboldt, the physicist Poisson,
and&mdash;appropriately&mdash;the young Cauchy, who was destined to become one of the chief
architects of nineteenth century mathematics.
</p>
<p>19.5.2 Heat Conduction in a Rectangular Plate
</p>
<p>Consider a rectangular heat-conducting plate with sides of length a and b all
held at T = 0. Assume that at time t = 0 the temperature has a distribution
function f (x, y). Let us find the variation of temperature for all points (x, y)
at all times t &gt; 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>582 19 Sturm-Liouville Systems
</p>
<p>The diffusion equation for this problem is
</p>
<p>&part;T
</p>
<p>&part;t
= k2&nabla;2T = k2
</p>
<p>(
&part;2T
</p>
<p>&part;x2
+ &part;
</p>
<p>2T
</p>
<p>&part;y2
</p>
<p>)
.
</p>
<p>A separation of variables, T (x, y, t)=X(x)Y (y)g(t), leads to three DEs:
</p>
<p>d2X
</p>
<p>dx2
+ λX = 0, d
</p>
<p>2Y
</p>
<p>dy2
+μY = 0, dg
</p>
<p>dt
+ k2(λ+μ)g = 0.
</p>
<p>The BCs T (0, y, t)= T (a, y, t)= T (x,0, t)= T (x, b, t)= 0, together with
the three ODEs, give rise to two S-L systems. The solutions to both of these
are easily found:
</p>
<p>λn =
(
nπ
</p>
<p>a
</p>
<p>)2
and Xn(x)= sin
</p>
<p>(
nπ
</p>
<p>a
x
</p>
<p>)
for n= 1,2, . . . ,
</p>
<p>μm =
(
mπ
</p>
<p>b
</p>
<p>)2
and Ym(y)= sin
</p>
<p>(
mπ
</p>
<p>b
y
</p>
<p>)
for m= 1,2, . . . .
</p>
<p>These give rise to the general solutions
</p>
<p>X(x)=
&infin;&sum;
</p>
<p>n=1
An sin
</p>
<p>(
nπ
</p>
<p>a
x
</p>
<p>)
, Y (y)=
</p>
<p>&infin;&sum;
</p>
<p>m=1
Bm sin
</p>
<p>(
mπ
</p>
<p>b
y
</p>
<p>)
.
</p>
<p>With γmn &equiv; k2π2(n2/a2 + m2/b2), the solution to the g equation can be
expressed as g(t)= Cmne&minus;γmnt . Putting everything together, we obtain
</p>
<p>T (x, y, t)=
&infin;&sum;
</p>
<p>n=1
</p>
<p>&infin;&sum;
</p>
<p>m=1
Amne
</p>
<p>&minus;γmnt sin
(
nπ
</p>
<p>a
x
</p>
<p>)
sin
</p>
<p>(
mπ
</p>
<p>b
y
</p>
<p>)
,
</p>
<p>where Amn =AnBmCmn is an arbitrary constant. To determine it, we impose
the initial condition T (x, y,0)= f (x, y). This yields
</p>
<p>f (x, y)=
&infin;&sum;
</p>
<p>n=1
</p>
<p>&infin;&sum;
</p>
<p>m=1
Amn sin
</p>
<p>(
nπ
</p>
<p>a
x
</p>
<p>)
sin
</p>
<p>(
mπ
</p>
<p>b
y
</p>
<p>)
,
</p>
<p>which determines the coefficients Amn:
</p>
<p>Amn =
4
</p>
<p>ab
</p>
<p>ˆ a
</p>
<p>0
dx
</p>
<p>ˆ b
</p>
<p>0
dyf (x, y) sin
</p>
<p>(
nπ
</p>
<p>a
x
</p>
<p>)
sin
</p>
<p>(
mπ
</p>
<p>b
y
</p>
<p>)
.
</p>
<p>19.5.3 Quantum Particle in a Box
</p>
<p>The behavior of an atomic particle of mass μ confined in a rectangular box
with sides a, b, and c (an infinite three-dimensional potential well) is gov-
erned by the Schr&ouml;dinger equation for a free particle,
</p>
<p>i�
&part;ψ
</p>
<p>&part;t
=&minus; �
</p>
<p>2
</p>
<p>2μ
</p>
<p>(
&part;2ψ
</p>
<p>&part;x2
+ &part;
</p>
<p>2ψ
</p>
<p>&part;y2
+ &part;
</p>
<p>2ψ
</p>
<p>&part;z2
</p>
<p>)
,</p>
<p/>
</div>
<div class="page"><p/>
<p>19.5 Separation in Cartesian Coordinates 583
</p>
<p>and the BC that ψ(x, y, z, t) vanishes at all sides of the box for all time.
A separation of variables ψ(x, y, z, t) = X(x)Y (y)Z(z)T (t) yields the
</p>
<p>ODEs
</p>
<p>d2X
</p>
<p>dx2
+ λX = 0, d
</p>
<p>2Y
</p>
<p>dy2
+ σY = 0, d
</p>
<p>2Z
</p>
<p>dz2
+ νX = 0,
</p>
<p>dT
</p>
<p>dt
+ iωT = 0, where ω&equiv; �
</p>
<p>2μ
(λ+ σ + ν).
</p>
<p>The spatial equations, together with the BCs
</p>
<p>ψ(0, y, z, t)=ψ(a, y, z, t) = 0 &rArr; X(0)= 0 =X(a),
ψ(x,0, z, t)=ψ(x, b, z, t) = 0 &rArr; Y(0)= 0 = Y(b),
ψ(x, y,0, t)=ψ(x, y, c, t) = 0 &rArr; Z(0)= 0 = Z(c),
</p>
<p>lead to three S-L systems, whose solutions are easily found:
</p>
<p>Xn(x)= sin
(
nπ
</p>
<p>a
x
</p>
<p>)
, λn =
</p>
<p>(
nπ
</p>
<p>a
</p>
<p>)2
, for n= 1,2, . . . ,
</p>
<p>Ym(y)= sin
(
mπ
</p>
<p>b
y
</p>
<p>)
, σm =
</p>
<p>(
mπ
</p>
<p>b
</p>
<p>)2
, for m= 1,2, . . . ,
</p>
<p>Zl(z)= sin
(
lπ
</p>
<p>c
z
</p>
<p>)
, νl =
</p>
<p>(
lπ
</p>
<p>c
</p>
<p>)2
, for l = 1,2, . . . .
</p>
<p>The time equation, on the other hand, has a solution of the form
</p>
<p>T (t)= Clmne&minus;iωlmnt where
</p>
<p>ωlmn =
�
</p>
<p>2μ
</p>
<p>[(
nπ
</p>
<p>a
</p>
<p>)2
+
(
mπ
</p>
<p>b
</p>
<p>)2
+
(
lπ
</p>
<p>c
</p>
<p>)2]
.
</p>
<p>The solution of the Schr&ouml;dinger equation that is consistent with the BCs is
therefore
</p>
<p>ψ(x, y, z, t)=
&infin;&sum;
</p>
<p>l,m,n=1
Almne
</p>
<p>&minus;iωlmnt sin
(
nπ
</p>
<p>a
x
</p>
<p>)
sin
</p>
<p>(
mπ
</p>
<p>b
y
</p>
<p>)
sin
</p>
<p>(
lπ
</p>
<p>c
z
</p>
<p>)
.
</p>
<p>The constants Almn are determined by the initial shape, ψ(x, y, z,0) of the
wave function. The energy of the particle is
</p>
<p>E = �ωlmn =
�2π2
</p>
<p>2μ
</p>
<p>(
n2
</p>
<p>a2
+ m
</p>
<p>2
</p>
<p>b2
+ l
</p>
<p>2
</p>
<p>c2
</p>
<p>)
.
</p>
<p>Each set of three positive integers (n,m, l) represents a state of the particle.
For a cube, a = b= c&equiv; L, and the energy of the particle is
</p>
<p>E = �
2π2
</p>
<p>2μL2
(
n2 +m2 + l2
</p>
<p>)
= �
</p>
<p>2π2
</p>
<p>2μV 2/3
(
n2 +m2 + l2
</p>
<p>)
(19.28)</p>
<p/>
</div>
<div class="page"><p/>
<p>584 19 Sturm-Liouville Systems
</p>
<p>where V = L3 is the volume of the box. The ground state is (1,1,1), has en-
ergy E = 3�2π2/2μV 2/3, and is nondegenerate (only one state corresponds
to this energy). However, the higher-level states are degenerate. For instance,
the three distinct states (1,1,2), (1,2,1), and (2,1,1) all correspond to the
same energy, E = 6�2π2/2μV 2/3. The degeneracy increases rapidly with
larger values of n, m, and l.
</p>
<p>Equation (19.28) can be written as
</p>
<p>n2 +m2 + l2 =R2, where R2 = 2μEV
2/3
</p>
<p>�2π2
.
</p>
<p>This looks like the equation of a sphere in the nml-space. If R is large, the
number of states contained within the sphere of radius R (the number of
states with energy less than or equal to E) is simply the volume of the first
octant8 of the sphere. If N is the number of such states, we have
</p>
<p>N = 1
8
</p>
<p>(
4π
</p>
<p>3
</p>
<p>)
R3 = π
</p>
<p>6
</p>
<p>(
2μEV 2/3
</p>
<p>�2π2
</p>
<p>)3/2
= π
</p>
<p>6
</p>
<p>(
2μE
</p>
<p>�2π2
</p>
<p>)3/2
V.
</p>
<p>Thus the density of states (the number of states per unit volume) isdensity of states
</p>
<p>n= N
V
</p>
<p>= π
6
</p>
<p>(
2μ
</p>
<p>�2π2
</p>
<p>)3/2
E3/2. (19.29)
</p>
<p>This is an important formula in solid-state physics, because the energy E is
(with minor modifications required by spin) the Fermi energy. If the FermiFermi energy
energy is denoted by Ef , Eq. (19.29) gives Ef = αn2/3, where α is some
constant.
</p>
<p>19.5.4 Wave Guides
</p>
<p>In the preceding examples the time variation is given by a first deriva-
tive. Thus, as far as time is concerned, we have a FODE. It follows that
the initial specification of the physical quantity of interest (temperature T
or Schr&ouml;dinger wave function ψ ) is sufficient to determine the solution
uniquely.
</p>
<p>A second kind of time-dependent PDE occurring in physics is the wave
equation, which contains time derivatives of the second order. Thus, there
are two arbitrary parameters in the general solution. To determine these, we
expect two initial conditions. For example, if the wave is standing, as in
a rope clamped at both ends, the boundary conditions are not sufficient to
determine the wave function uniquely. One also needs to specify the initial
(transverse) velocity of each point of the rope.
</p>
<p>For traveling waves, specification of the wave shape and velocity shape
is not as important as the mode of propagation. For instance, in the theory of
wave guides, after the time variation is separated, a particular time variation,
</p>
<p>8This is because n, m, and l are all positive.</p>
<p/>
</div>
<div class="page"><p/>
<p>19.5 Separation in Cartesian Coordinates 585
</p>
<p>such as e+iωt , and a particular direction for the propagation of the wave, say
the z-axis, are chosen. Thus, if u denotes a component of the electric or the
magnetic field, we can write
</p>
<p>u(x, y, z, t)=ψ(x, y)ei(ωt&plusmn;kz),
</p>
<p>where k is the wave number. The wave equation then reduces to
</p>
<p>&part;2ψ
</p>
<p>&part;x2
+ &part;
</p>
<p>2ψ
</p>
<p>&part;y2
+
(
ω2
</p>
<p>c2
&minus; k2
</p>
<p>)
ψ = 0.
</p>
<p>Introducing γ 2 = ω2/c2&minus;k2 and the transverse gradient &nabla;t = (&part;/&part;x, &part;/&part;y)
and writing the above equation in terms of the full vectors, we obtain
</p>
<p>(
&nabla;2t + γ 2
</p>
<p>){E
B
</p>
<p>}
= 0, where
</p>
<p>{
E
B
</p>
<p>}
=
{
</p>
<p>E(x, y)
B(x, y)
</p>
<p>}
ei(ωt&plusmn;kz). (19.30)
</p>
<p>These are the basic equations used in the study of electromagnetic wave
guides and resonant cavities.
</p>
<p>Maxwell&rsquo;s equations in conjunction with Eq. (19.30) gives the transverse guided waves
components (components perpendicular to the propagation direction) Et
and Bt in terms of the longitudinal components Ez and Bz (see [Lorr 88,
Chap. 33]):
</p>
<p>γ 2Et =&nabla;&nabla;&nabla; t
(
&part;Ez
</p>
<p>&part;z
</p>
<p>)
&minus; i ω
</p>
<p>c
êz &times; (&nabla;&nabla;&nabla; tBz),
</p>
<p>γ 2Bt =&nabla;&nabla;&nabla; t
(
&part;Bz
</p>
<p>&part;z
</p>
<p>)
+ i ω
</p>
<p>c
êz &times; (&nabla;&nabla;&nabla; tEz).
</p>
<p>(19.31)
</p>
<p>Three types of guided waves are usually studied.
</p>
<p>1. Transverse magnetic (TM) waves have Bz = 0 everywhere. The BC on
E demands that Ez vanish at the conducting walls of the guide.
</p>
<p>2. Transverse electric (TE) waves have Ez = 0 everywhere. The BC on B
requires that the normal directional derivative
</p>
<p>&part;Bz
</p>
<p>&part;n
&equiv; ên &middot; (&nabla;&nabla;&nabla;Bz)
</p>
<p>vanish at the walls.
3. Transverse electromagnetic (TEM) waves have Bz = 0 = Ez. For a
</p>
<p>nontrivial solution, Eq. (19.31) demands that γ 2 = 0. This form re-
sembles a free wave with no boundaries.
</p>
<p>We quote the basic equations for the TM mode (see any book on electro-
magnetic theory for further details):
</p>
<p>(
&nabla;2t + γ 2
</p>
<p>)
Ez = 0, Bz = 0,
</p>
<p>γ 2Et =&nabla;&nabla;&nabla; t
(
&part;Ez
</p>
<p>&part;z
</p>
<p>)
, γ 2Bt = i
</p>
<p>ω
</p>
<p>c
êz &times; (&nabla;&nabla;&nabla; tEz),
</p>
<p>(19.32)
</p>
<p>where &nabla;2t is restricted to the x and y terms of the Laplacian in Cartesian
coordinates, and to the ρ and ϕ terms in cylindrical coordinates.</p>
<p/>
</div>
<div class="page"><p/>
<p>586 19 Sturm-Liouville Systems
</p>
<p>Fig. 19.2 A conducting cylindrical can whose top has a potential given by V (ρ,ϕ), with
the rest of the surface grounded
</p>
<p>19.6 Separation in Cylindrical Coordinates
</p>
<p>When the geometry of the boundaries is cylindrical, the appropriate coordi-
nate system is the cylindrical one. This usually leads to Bessel functions &ldquo;of
some kind.&rdquo;
</p>
<p>Before working specific examples of cylindrical geometry, let us con-
sider a question that has more general implications. We saw in the previous
section that separation of variables leads to ODEs in which certain constants
(eigenvalues) appear. Different choices of signs for these constants can lead
to different functional forms of the general solution. For example, an equa-
tion such as d2x/dt2 &minus; kx = 0 can have exponential solutions if k &gt; 0 or
trigonometric solutions if k &lt; 0. One cannot a priori assign a specific sign
to k. Thus, the general form of the solution is indeterminate. However, once
the boundary conditions are imposed, the unique solutions will emerge re-
gardless of the initial functional form of the solutions (see [Hass 08] for a
thorough discussion of this point).
</p>
<p>19.6.1 Conducting Cylindrical Can
</p>
<p>Consider a cylindrical conducting can of radius a and height h (see
Fig. 19.2). The potential varies at the top face as V (ρ,ϕ), while the lat-
eral surface and the bottom face are grounded. Let us find the electrostatic
potential at all points inside the can.
</p>
<p>A separation of variables transforms Laplace&rsquo;s equation into three ODEs:
</p>
<p>d
</p>
<p>dρ
</p>
<p>(
ρ
dR
</p>
<p>dρ
</p>
<p>)
+
(
k2ρ &minus; m
</p>
<p>2
</p>
<p>ρ
</p>
<p>)
R = 0,
</p>
<p>d2S
</p>
<p>dϕ2
+m2S = 0, d
</p>
<p>2Z
</p>
<p>dz2
&minus; k2Z = 0,</p>
<p/>
</div>
<div class="page"><p/>
<p>19.6 Separation in Cylindrical Coordinates 587
</p>
<p>where in anticipation of the correct BCs, we have written the constants as k2
</p>
<p>and &minus;m2 with m an integer. The first of these is the Bessel equation, whose
general solution can be written as R(ρ)=AJm(kρ)+BYm(kρ). The second
DE, when the extra condition of periodicity in ϕ is imposed on the potential,
has the general solution
</p>
<p>S(ϕ)= C cosmϕ +D sinmϕ.
</p>
<p>Finally the third DE has a general solution of the form
</p>
<p>Z(z)=Eekz + Fe&minus;kz.
</p>
<p>We note that none of the three ODEs lead to an S-L system of Theo-
rem 19.4.1 because the BCs associated with them do not satisfy (19.24).
However, we can still solve the problem by imposing the given BCs.
</p>
<p>The fact that the potential must be finite everywhere inside the can (in-
cluding at ρ = 0) forces B to vanish because the Neumann function Ym(kρ)
is not defined at ρ = 0. On the other hand, we want Φ to vanish at ρ = a.
This gives Jm(ka)= 0, which demands that ka be a root of the Bessel func-
tion of order m. Denoting by xmn the nth zero of the Bessel function of order
m, we have ka = xmn, or k = xmn/a for n= 1,2, . . . .
</p>
<p>Similarly, the vanishing of Φ at z= 0 implies that
</p>
<p>E =&minus;F and Z(z)=E sinh
(
xmnz
</p>
<p>a
</p>
<p>)
.
</p>
<p>We can now multiply R, S, and Z and sum over all possible values of m
and n, keeping in mind that negative values of m give terms that are linearly
dependent on the corresponding positive values. The result is the so-called
Fourier-Bessel series: Fourier-Bessel series
</p>
<p>Φ(ρ,ϕ, z)=
&infin;&sum;
</p>
<p>m=0
</p>
<p>&infin;&sum;
</p>
<p>n=1
Jm
</p>
<p>(
xmn
</p>
<p>a
ρ
</p>
<p>)
sinh
</p>
<p>(
xmn
</p>
<p>a
z
</p>
<p>)
</p>
<p>&times; (Amn cosmϕ +Bmn sinmϕ), (19.33)
</p>
<p>where Amn and Bmn are constants to be determined by the remaining BC.
To find these constants we use the orthogonality of the trigonometric and
Bessel functions. For z= h Eq. (19.33) reduces to
</p>
<p>V (ρ,ϕ)=
&infin;&sum;
</p>
<p>m=0
</p>
<p>&infin;&sum;
</p>
<p>n=1
Jm
</p>
<p>(
xmn
</p>
<p>a
ρ
</p>
<p>)
sinh
</p>
<p>(
xmn
</p>
<p>a
h
</p>
<p>)
(Amn cosmϕ +Bmn sinmϕ),
</p>
<p>from which we obtain
</p>
<p>Amn =
2
</p>
<p>πa2J 2m+1(xmn) sinh(xmnh/a)
</p>
<p>&times;
ˆ 2π
</p>
<p>0
dϕ
</p>
<p>ˆ a
</p>
<p>0
dρρV (ρ,ϕ)Jm
</p>
<p>(
xmn
</p>
<p>a
ρ
</p>
<p>)
cosmϕ,</p>
<p/>
</div>
<div class="page"><p/>
<p>588 19 Sturm-Liouville Systems
</p>
<p>Bmn =
2
</p>
<p>πa2J 2m+1(xmn) sinh(xmnh/a)
</p>
<p>&times;
ˆ 2π
</p>
<p>0
dϕ
</p>
<p>ˆ a
</p>
<p>0
dρρV (ρ,ϕ)Jm
</p>
<p>(
xmn
</p>
<p>a
ρ
</p>
<p>)
sinmϕ,
</p>
<p>where we have used the following result derived in Problem 15.39:
</p>
<p>ˆ a
</p>
<p>0
ρJ 2m
</p>
<p>(
xmn
</p>
<p>a
ρ
</p>
<p>)
dρ = a
</p>
<p>2
</p>
<p>2
J 2m+1(xmn). (19.34)
</p>
<p>For the special but important case of azimuthal symmetry, for which V
is independent of ϕ, we obtain
</p>
<p>Amn =
4δm,0
</p>
<p>a2J 21 (x0n) sinh(x0nh/a)
</p>
<p>ˆ a
</p>
<p>0
dρ ρV (ρ)J0
</p>
<p>(
x0n
</p>
<p>a
ρ
</p>
<p>)
,
</p>
<p>Bmn = 0,
</p>
<p>and
</p>
<p>Φ(ρ, z)= 4
a2
</p>
<p>&infin;&sum;
</p>
<p>n=1
An
</p>
<p>J0(
x0n
a
ρ) sinh( x0n
</p>
<p>a
z)
</p>
<p>J 21 (x0n) sinh(x0nh/a)
,
</p>
<p>where
</p>
<p>An =
ˆ a
</p>
<p>0
dρ ρV (ρ)J0
</p>
<p>(
x0n
</p>
<p>a
ρ
</p>
<p>)
,
</p>
<p>and V (ρ) is the ϕ-independent potential at the top face.
The reason we obtained discrete values for k was the demand that Φ
</p>
<p>vanish at ρ = a. If we let a &rarr; &infin;, then k will be a continuous variable,
and instead of a sum over k, we will obtain an integral. This is completely
analogous to the transition from a Fourier series to a Fourier transform, but
we will not pursue it further.
</p>
<p>19.6.2 Cylindrical Wave Guide
</p>
<p>For a TM wave propagating along the z-axis in a hollow circular conductor,
we have [see Eq. (19.32)]
</p>
<p>1
</p>
<p>ρ
</p>
<p>&part;
</p>
<p>&part;ρ
</p>
<p>(
ρ
&part;Ez
</p>
<p>&part;ρ
</p>
<p>)
+ 1
</p>
<p>ρ2
</p>
<p>&part;2Ez
</p>
<p>&part;ϕ2
+ γ 2Ez = 0.
</p>
<p>The separation Ez =R(ρ)S(ϕ) yields S(ϕ)=A cosmϕ +B sinmϕ and
</p>
<p>d2R
</p>
<p>dρ2
+ 1
</p>
<p>ρ
</p>
<p>dR
</p>
<p>dρ
+
(
γ 2 &minus; m
</p>
<p>2
</p>
<p>ρ2
</p>
<p>)
R = 0.
</p>
<p>The solution to this equation, which is regular at ρ = 0 and vanishes at
ρ = a, is
</p>
<p>R(ρ)= CJm
(
xmn
</p>
<p>a
ρ
</p>
<p>)
and γ = xmn
</p>
<p>a
.</p>
<p/>
</div>
<div class="page"><p/>
<p>19.6 Separation in Cylindrical Coordinates 589
</p>
<p>Recalling the definition of γ , we obtain
</p>
<p>ω2
</p>
<p>c2
&minus; k2 = γ 2 = x
</p>
<p>2
mn
</p>
<p>a2
&rArr; k =
</p>
<p>&radic;
ω2
</p>
<p>c2
&minus; x
</p>
<p>2
mn
</p>
<p>a2
.
</p>
<p>This gives the cut-off frequency ωmn = cxmn/a.
The solution for the azimuthally symmetric case (m= 0) is
</p>
<p>Ez(ρ,ϕ, t)=
&infin;&sum;
</p>
<p>n=1
AnJ0
</p>
<p>(
x0n
</p>
<p>a
ρ
</p>
<p>)
ei(ωt&plusmn;knz) and Bz = 0,
</p>
<p>where kn =
&radic;
ω2/c2 &minus; x20n/a2.
</p>
<p>19.6.3 Current Distribution in a Circular Wire
</p>
<p>There are many variations on the theme of Bessel functions. We have en-
countered three kinds of Bessel functions, as well as modified Bessel func-
tions. Another variation encountered in applications leads to what are known
as Kelvin functions, introduced here.
</p>
<p>Consider the flow of charges in an infinitely long wire with a circular
cross section of radius a. We are interested in calculating the variation of
the current density in the wire as a function of time and location. The rel-
evant equation can be obtained by starting with Maxwell&rsquo;s equations for
negligible charge density (&nabla; &middot; E = 0), Ohm&rsquo;s law (j = σE), the assumption
of high electrical conductivity (|σE| ≫ |&part;E/&part;t |), and the usual procedure
of obtaining the wave equation from Maxwell&rsquo;s equations. The result is
</p>
<p>&nabla;2j &minus; 4πσ
c2
</p>
<p>&part;j
&part;t
</p>
<p>= 0.
</p>
<p>Moreover, we make the simplifying assumptions that the wire is along the
z-axis and that there is no turbulence, so j is also along the z direction. We
further assume that j is independent of ϕ and z, and that its time-dependence
is given by e&minus;iωt . Then we get
</p>
<p>d2j
</p>
<p>dρ2
+ 1
</p>
<p>ρ
</p>
<p>d j
</p>
<p>dρ
+ τ 2j = 0, (19.35)
</p>
<p>where τ 2 = i4πσω/c2 &equiv; i2/δ2 and δ = c/
&radic;
</p>
<p>2πσω is called the skin depth. skin depth
The Kelvin equation is usually given as Kelvin equation
</p>
<p>d2w
</p>
<p>dx2
+ 1
</p>
<p>x
</p>
<p>dw
</p>
<p>dx
&minus; ik2w = 0. (19.36)
</p>
<p>If we substitute x =
&radic;
it/k, it becomes ẅ+ ẇ/t +w = 0, which is a Bessel
</p>
<p>equation of order zero. If the solution is to be regular at x = 0, then the
only choice is w(t) = J0(t) = J0(e&minus;iπ/4kx). This is the Kelvin function Kelvin function</p>
<p/>
</div>
<div class="page"><p/>
<p>590 19 Sturm-Liouville Systems
</p>
<p>for Eq. (19.36). It is usually written as
</p>
<p>J0
(
e&minus;iπ/4kx
</p>
<p>)
&equiv; ber(kx)+ i bei(kx)
</p>
<p>where ber and bei stand for &ldquo;Bessel real&rdquo; and &ldquo;Bessel imaginary&rdquo;, respec-
tively. If we substitute z= e&minus;iπ/4kx in the expansion for J0(z) and separate
the real and the imaginary parts of the expansion, we obtain
</p>
<p>ber(x)= 1 &minus; (x/2)
4
</p>
<p>(2!)2 +
(x/2)8
</p>
<p>(4!)2 &minus; &middot; &middot; &middot;
</p>
<p>bei(x)= (x/2)
2
</p>
<p>(1!)2 &minus;
(x/2)6
</p>
<p>(3!)2 +
(x/2)10
</p>
<p>(5!)2 &minus; &middot; &middot; &middot; .
</p>
<p>Equation (19.35) is the complex conjugate of (19.36) with k2 = 2/δ2.
Thus, its solution is
</p>
<p>j (ρ)=AJ0
(
eiπ/4kρ
</p>
<p>)
=A
</p>
<p>{
ber
</p>
<p>(&radic;
2
</p>
<p>δ
ρ
</p>
<p>)
&minus; i bei
</p>
<p>(&radic;
2
</p>
<p>δ
ρ
</p>
<p>)}
.
</p>
<p>We can compare the value of the current density at ρ with its value at the
surface ρ = a:
</p>
<p>∣∣∣∣
j (ρ)
</p>
<p>j (a)
</p>
<p>∣∣∣∣=
[
</p>
<p>ber2(
&radic;
</p>
<p>2
δ
ρ)+ bei2(
</p>
<p>&radic;
2
δ
ρ)
</p>
<p>ber2(
&radic;
</p>
<p>2
δ
a)+ bei2(
</p>
<p>&radic;
2
δ
a)
</p>
<p>]1/2
.
</p>
<p>For low frequencies, δ is large, which implies that ρ/δ is small; thus,
ber(
</p>
<p>&radic;
2ρ/δ)&asymp; 1 and bei(
</p>
<p>&radic;
2ρ/δ)&asymp; 0, and |j (ρ)/j (a)| &asymp; 1; i.e., the current
</p>
<p>density is almost uniform. For higher frequencies the ratio of the current
densities starts at a value less than 1 at ρ = 0 and increases to 1 at ρ = a.
The starting value depends on the frequency. For very large frequencies the
starting value is almost zero (see [Mari 80, pp 150&ndash;156]).
</p>
<p>19.7 Separation in Spherical Coordinates
</p>
<p>Recall that most PDEs encountered in physical applications can be sepa-
rated, in spherical coordinates, into
</p>
<p>L2Y(θ,ϕ)= l(l + 1)Y (θ,ϕ),
d2R
</p>
<p>dr2
+ 2
</p>
<p>r
</p>
<p>dR
</p>
<p>dr
+
[
f (r)&minus; l(l + 1)
</p>
<p>r2
</p>
<p>]
R = 0.
</p>
<p>(19.37)
</p>
<p>We discussed the first of these two equations in great detail in Chap. 13.
In particular, we constructed Ylm(θ,ϕ) in such a way that they formed
an orthonormal sequence. However, that construction was purely algebraic
and did not say anything about the completeness of Ylm(θ,ϕ). With Theo-
rem 19.4.1 at our disposal, we can separate the first equation of (19.37) into</p>
<p/>
</div>
<div class="page"><p/>
<p>19.7 Separation in Spherical Coordinates 591
</p>
<p>two ODEs by writing Ylm(θ,ϕ)= Plm(θ)Sm(ϕ). We obtain
</p>
<p>d2Sm
</p>
<p>dϕ2
+m2Sm = 0,
</p>
<p>d
</p>
<p>dx
</p>
<p>[(
1 &minus; x2
</p>
<p>)dPlm
dx
</p>
<p>]
+
[
l(l + 1)&minus; m
</p>
<p>2
</p>
<p>1 &minus; x2
]
Plm = 0,
</p>
<p>where x = cos θ . These are both S-L systems satisfying the conditions
of Theorem 19.4.1. Thus, the Sm are orthogonal among themselves and
form a complete set for L2(0,2π). Similarly, for any fixed m, the Plm(x)
form a complete orthogonal set for L2(&minus;1,+1) (actually for the subset of
L2(&minus;1,+1) that satisfies the same BC as the Plm do at x =&plusmn;1). Thus, the
products Ylm(x,ϕ)= Plm(x)Sm(ϕ) form a complete orthogonal sequence in
the (Cartesian product) set [&minus;1,+1] &times; [0,2π], which, in terms of spherical
angles, is the unit sphere, 0 &le; θ &le; π , 0 &le; ϕ &le; 2π .
</p>
<p>19.7.1 Radial Part of Laplace&rsquo;s Equation
</p>
<p>Let us consider some specific examples of expansion in the spherical coor-
dinate system starting with the simplest case, Laplace&rsquo;s equation for which
f (r)= 0. The radial equation is therefore
</p>
<p>d2R
</p>
<p>dr2
+ 2
</p>
<p>r
</p>
<p>dR
</p>
<p>dr
&minus; l(l + 1)
</p>
<p>r2
R = 0.
</p>
<p>Multiplying by r2, substituting r = et , and using the chain rule and the fact
that dt/dr = 1/r leads to the following SOLDE with constant coefficients:
</p>
<p>d2R
</p>
<p>dt2
+ dR
</p>
<p>dt
&minus; l(l + 1)R = 0.
</p>
<p>This has a characteristic polynomial p(λ) = λ2 + λ &minus; l(l + 1) with roots
λ1 = l and λ2 =&minus;(l + 1). Thus, a general solution is of the form
</p>
<p>R(t)=Aeλ1t +Beλ2t =A
(
et
)l +B
</p>
<p>(
et
)&minus;l&minus;1
</p>
<p>,
</p>
<p>or, in terms of r , R(r)=Ar l + Br&minus;l&minus;1. Thus, the most general solution of
Laplace&rsquo;s equation is
</p>
<p>Φ(r, θ,ϕ)=
&infin;&sum;
</p>
<p>l=0
</p>
<p>l&sum;
</p>
<p>m=&minus;l
</p>
<p>(
Almr
</p>
<p>l +Blmr&minus;l&minus;1
)
Ylm(θ,ϕ).
</p>
<p>For regions containing the origin, the finiteness of Φ implies that
Blm = 0. Denoting the potential in such regions by Φin, we obtain
</p>
<p>Φin(r, θ,ϕ)=
&infin;&sum;
</p>
<p>l=0
</p>
<p>l&sum;
</p>
<p>m=&minus;l
Almr
</p>
<p>lYlm(θ,ϕ).</p>
<p/>
</div>
<div class="page"><p/>
<p>592 19 Sturm-Liouville Systems
</p>
<p>Similarly, for regions including r =&infin;, we have
</p>
<p>Φout(r, θ,ϕ)=
&infin;&sum;
</p>
<p>l=0
</p>
<p>l&sum;
</p>
<p>m=&minus;l
Blmr
</p>
<p>&minus;l&minus;1Ylm(θ,ϕ).
</p>
<p>To determine Alm and Blm, we need to invoke appropriate BCs. In par-
ticular, for inside a sphere of radius a on which the potential is given by
V (θ,ϕ), we have
</p>
<p>V (θ,ϕ)=Φin(a, θ,ϕ)=
&infin;&sum;
</p>
<p>l=0
</p>
<p>l&sum;
</p>
<p>m=&minus;l
Alma
</p>
<p>lYlm(θ,ϕ).
</p>
<p>Multiplying by Y &lowast;kj (θ,ϕ) and integrating over dΩ = sin θ dθ dϕ, we obtain
</p>
<p>Akj = a&minus;k
&uml;
</p>
<p>dΩV (θ,ϕ)Y &lowast;kj (θ,ϕ)
</p>
<p>&rArr; Alm = a&minus;l
&uml;
</p>
<p>dΩV (θ,ϕ)Y &lowast;lm(θ,ϕ).
</p>
<p>Similarly, for potential outside the sphere,
</p>
<p>Blm = al+1
&uml;
</p>
<p>dΩV (θ,ϕ)Y &lowast;lm(θ,ϕ).
</p>
<p>In particular, if V is independent of ϕ, only the components for which
m= 0 are nonzero, and we have
</p>
<p>Al0 =
2π
</p>
<p>al
</p>
<p>ˆ π
</p>
<p>0
sin θV (θ)Y &lowast;l0(θ) dθ
</p>
<p>= 2π
al
</p>
<p>&radic;
2l + 1
</p>
<p>4π
</p>
<p>ˆ π
</p>
<p>0
sin θV (θ)Pl(cos θ) dθ,
</p>
<p>which yields
</p>
<p>Φin(r, θ)=
&infin;&sum;
</p>
<p>l=0
Al
</p>
<p>(
r
</p>
<p>a
</p>
<p>)l
Pl(cos θ),
</p>
<p>where
</p>
<p>Al =
2
</p>
<p>2l + 1
</p>
<p>ˆ π
</p>
<p>0
sin θV (θ)Pl(cos θ) dθ.
</p>
<p>Similarly,
</p>
<p>Φout(r, θ)=
&infin;&sum;
</p>
<p>l=0
Al
</p>
<p>(
a
</p>
<p>r
</p>
<p>)l+1
Pl(cos θ).</p>
<p/>
</div>
<div class="page"><p/>
<p>19.7 Separation in Spherical Coordinates 593
</p>
<p>19.7.2 Helmholtz Equation in Spherical Coordinates
</p>
<p>The next simplest case after Laplace&rsquo;s equation is that for which f (r) is a
constant. The diffusion equation, the wave equation, and the Schr&ouml;dinger
equation for a free particle give rise to such a case once time is separated
from the rest of the variables.
</p>
<p>The Helmholtz equation is Helmholtz equation
</p>
<p>&nabla;2ψ + k2ψ = 0, (19.38)
</p>
<p>and its radial part is
</p>
<p>d2R
</p>
<p>dr2
+ 2
</p>
<p>r
</p>
<p>dR
</p>
<p>dr
+
[
k2 &minus; l(l + 1)
</p>
<p>r2
</p>
<p>]
R = 0. (19.39)
</p>
<p>(This equation was discussed in Problems 15.26 and 15.35.) The solutions
are spherical Bessel functions, generically denoted by the corresponding spherical Bessel
</p>
<p>functionslower case letter as zl(x) and given by
</p>
<p>zl(x)&equiv;
&radic;
π
</p>
<p>2
</p>
<p>Zl+1/2(x)&radic;
x
</p>
<p>, (19.40)
</p>
<p>where Zν(x) is a solution of the Bessel equation of order ν.
A general solution of (19.39) can therefore be written as
</p>
<p>Rl(r)=Ajl(kr)+Byl(kr). (19.41)
</p>
<p>If the origin is included in the region of interest, then we must set B = 0.
For such a case, the solution to the Helmholtz equation is
</p>
<p>ψk(r, θ,ϕ)=
&infin;&sum;
</p>
<p>l=0
</p>
<p>l&sum;
</p>
<p>m=&minus;l
Almjl(kr)Ylm(θ,ϕ). (19.42)
</p>
<p>The subscript k indicates that ψ is a solution of the Helmholtz equation with
k2 as its constant.
</p>
<p>19.7.3 Quantum Particle in a Hard Sphere
</p>
<p>The time-independent Schr&ouml;dinger equation for a particle in a sphere of
</p>
<p>radius a is &minus; �22μ&nabla;2ψ = Eψ with the BC ψ(a, θ,ϕ) = 0. Here E is the
energy of the particle and μ is its mass. We rewrite the Schr&ouml;dinger equation
as &nabla;2ψ + k2ψ = 0 with k2 = 2μE/�2. Then Eq. (19.41) and the fact that
Rl(r) must be finite at r = 0 yield
</p>
<p>Rl(r)=Ajl(kr)=Ajl(
&radic;
</p>
<p>2μEr/�).
</p>
<p>The vanishing of ψ at a implies that jl(
&radic;
</p>
<p>2μEa/�)= 0, or
&radic;
</p>
<p>2μEa
</p>
<p>�
=Xln for n= 1,2, . . . ,</p>
<p/>
</div>
<div class="page"><p/>
<p>594 19 Sturm-Liouville Systems
</p>
<p>where Xln is the nth zero of jl(x), which is the same as the zero of
Jl+1/2(x). Thus, the energy is quantized as
</p>
<p>Eln =
�2X2ln
2μa2
</p>
<p>for l = 0,1, . . . , n= 1,2, . . . .
</p>
<p>The general solution to the Schr&ouml;dinger equation is
</p>
<p>ψ(r, θ,ϕ)=
&infin;&sum;
</p>
<p>n=1
</p>
<p>&infin;&sum;
</p>
<p>l=0
</p>
<p>l&sum;
</p>
<p>m=&minus;l
Anlmjl
</p>
<p>(
Xln
</p>
<p>r
</p>
<p>a
</p>
<p>)
Ylm(θ,ϕ).
</p>
<p>19.7.4 PlaneWave Expansion
</p>
<p>A particularly useful consequence of Eq. (19.42) is the expansion of a plane
wave in terms of spherical Bessel functions. It is easily verified that if k is
a vector, with k &middot; k = k2, then eik&middot;r is a solution of the Helmholtz equation.
Thus, eik&middot;r can be expanded as in Eq. (19.42). Assuming that k is along the
z-axis, we get k &middot; r = kr cos θ , which is independent of ϕ. Only the terms of
Eq. (19.42) for which m= 0 will survive in such a case, and we may write
</p>
<p>eikr cos θ =
&infin;&sum;
</p>
<p>l=0
Aljl(kr)Pl(cos θ).
</p>
<p>To find Al , let u= cos θ , multiply both sides by Pn(u), and integrate from
&minus;1 to 1:
ˆ 1
</p>
<p>&minus;1
Pn(u)e
</p>
<p>ikrudu=
&infin;&sum;
</p>
<p>l=0
Aljl(kr)
</p>
<p>ˆ 1
</p>
<p>&minus;1
Pn(u)Pl(u) du=Anjn(kr)
</p>
<p>2
</p>
<p>2n+ 1 .
</p>
<p>Thus
</p>
<p>Anjn(kr)=
2n+ 1
</p>
<p>2
</p>
<p>ˆ 1
</p>
<p>&minus;1
Pn(u)e
</p>
<p>ikrudu
</p>
<p>= 2n+ 1
2
</p>
<p>&infin;&sum;
</p>
<p>m=0
</p>
<p>(ikr)m
</p>
<p>m!
</p>
<p>ˆ 1
</p>
<p>&minus;1
Pn(u)u
</p>
<p>mdu. (19.43)
</p>
<p>This equality holds for all values of kr . In particular, both sides should give
the same result in the limit of small kr . From the definition of jn(kr) and
the expansion of Jn(kr), we obtain
</p>
<p>jn(kr)&minus;&minus;&minus;&rarr;
kr&rarr;0
</p>
<p>&radic;
π
</p>
<p>2
</p>
<p>(
kr
</p>
<p>2
</p>
<p>)n 1
Ŵ(n+ 3/2) .
</p>
<p>On the other hand, the first nonvanishing term of the RHS of Eq. (19.43)expansion of eik&middot;r in
spherical harmonics occurs when m= n. Equating these terms on both sides, we get
</p>
<p>An
</p>
<p>&radic;
π
</p>
<p>2
</p>
<p>(
kr
</p>
<p>2
</p>
<p>)n 22n+1n!
(2n+ 1)!&radic;π =
</p>
<p>2n+ 1
2
</p>
<p>in(kr)n
</p>
<p>n!
2n+1(n!)2
(2n+ 1)! , (19.44)</p>
<p/>
</div>
<div class="page"><p/>
<p>19.8 Problems 595
</p>
<p>where we have used
</p>
<p>Ŵ
</p>
<p>(
n+ 3
</p>
<p>2
</p>
<p>)
= (2n+ 1)!
</p>
<p>&radic;
π
</p>
<p>22n+1n! and
ˆ 1
</p>
<p>&minus;1
Pn(u)u
</p>
<p>ndu= 2
n+1(n!)2
(2n+ 1)! .
</p>
<p>Equation (19.44) yields An = in(2n+ 1).
With An thus calculated, we can now write
</p>
<p>eikr cos θ =
&infin;&sum;
</p>
<p>l=0
(2l + 1)iljl(kr)Pl(cos θ). (19.45)
</p>
<p>For an arbitrary direction of k, k &middot;r = kr cosγ , where γ is the angle between
k and r. Thus, we may write
</p>
<p>eik&middot;r =
&infin;&sum;
</p>
<p>l=0
(2l + 1)iljl(kr)Pl(cosγ ),
</p>
<p>and using the addition theorem for spherical harmonics [Eq. (13.44)], we
finally obtain
</p>
<p>eik&middot;r = 4π
&infin;&sum;
</p>
<p>l=0
</p>
<p>l&sum;
</p>
<p>m=&minus;l
iljl(kr)Y
</p>
<p>&lowast;
lm
</p>
<p>(
θ &prime;, ϕ&prime;
</p>
<p>)
Ylm(θ,ϕ), (19.46)
</p>
<p>where θ &prime; and ϕ&prime; are the spherical angles of k and θ and ϕ are those of r.
Such a decomposition of plane waves into components with definite orbital
angular momenta is extremely useful when working with scattering theory
for waves and particles.
</p>
<p>19.8 Problems
</p>
<p>19.1 Show that the Liouville substitution transforms regular S-L systems
into regular S-L systems and separated and periodic BCs into separated and
periodic BCs, respectively.
</p>
<p>19.2 Let u1(x) and u2(x) be transformed, respectively into v1(t) and v2(t)
by the Liouville substitution. Show that the inner product on [a, b] with
weight function w(x) is transformed into the inner product on [0, c] with
unit weight, where c=
</p>
<p>&acute; b
</p>
<p>a
</p>
<p>&radic;
w/p dx.
</p>
<p>19.3 Derive Eq. (19.17) from (19.15) using Pr&uuml;fer substitution.
</p>
<p>19.4 Consider the Bessel DE.
</p>
<p>(a) Show that the Liouville substitution transforms the Bessel DE into
</p>
<p>d2v
</p>
<p>dt2
+
(
k2 &minus; ν
</p>
<p>2 &minus; 1/4
t2
</p>
<p>)
v = 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>596 19 Sturm-Liouville Systems
</p>
<p>(b) Find the equations obtained from the Pr&uuml;fer substitution, and show
that for large x these equations reduce to
</p>
<p>φ&prime; = k
(
</p>
<p>1 &minus; a
2k2x2
</p>
<p>)
+ O(1)
</p>
<p>x3
,
</p>
<p>R&prime;
</p>
<p>R
= O(1)
</p>
<p>x3
,
</p>
<p>where a = ν2 &minus; 14 .
(c) Integrate these equations from x to b &gt; x and take the limit as b&rarr;&infin;
</p>
<p>to get
</p>
<p>φ(x)= φ&infin; + kx +
a
</p>
<p>2kx
+ O(1)
</p>
<p>x2
, R(x)=R&infin; +
</p>
<p>O(1)
</p>
<p>x2
,
</p>
<p>where φ&infin; = limb&rarr;&infin;(φ(b)&minus; kb) and R&infin; = limb&rarr;&infin;R(b).
(d) Substitute these and the appropriate expression for Q&minus;1/4 in Eq. (19.16)
</p>
<p>and show that
</p>
<p>v(x)= R&infin;&radic;
k
</p>
<p>cos
</p>
<p>(
kx &minus; kx&infin; +
</p>
<p>ν2 &minus; 1/4
2kx
</p>
<p>)
+ O(1)
</p>
<p>x2
,
</p>
<p>where kx&infin; &equiv; π/2 &minus; φ&infin;.
(e) Choose R&infin; =
</p>
<p>&radic;
2/π for all solutions of the Bessel DE, and let
</p>
<p>kx&infin; =
(
ν + 1
</p>
<p>2
</p>
<p>)
π
</p>
<p>2
and kx&infin; =
</p>
<p>(
ν + 3
</p>
<p>2
</p>
<p>)
π
</p>
<p>2
</p>
<p>for the Bessel functions Jν(x) and the Neumann functions Yν(x), re-
spectively, and find the asymptotic behavior of these two functions.
</p>
<p>19.5 Show that separated and periodic BCs are special cases of the equality
in Eq. (19.26).
</p>
<p>19.6 Derive Eq. (19.27).
</p>
<p>19.7 A semi-infinite heat-conducting plate of width b is extended along the
positive x-axis with one corner at (0,0) and the other at (0, b). The side
of width b is held at temperature T = f (y), and the two long sides are
held at T = 0. The two flat faces are insulated and the plate is in thermal
equilibrium.
</p>
<p>(a) Find the temperature variation of the plate for all (x, y).
(b) Specialize to the case where the side of width b is held at constant
</p>
<p>temperature T0 (see Fig. 19.3).
</p>
<p>19.8 Repeat Problem 19.7 with the temperature of the short side held at
each of the following:
</p>
<p>(a) T =
{
</p>
<p>0 if 0 &lt; y &lt; b/2,
</p>
<p>T0 if b/2 &lt; y &lt; b.
(b)
</p>
<p>T0
</p>
<p>b
y, 0 &le; y &le; b.
</p>
<p>(c) T0 cos
</p>
<p>(
π
</p>
<p>b
y
</p>
<p>)
, 0 &le; y &le; b. (d) T0 sin
</p>
<p>(
π
</p>
<p>b
y
</p>
<p>)
, 0 &le; y &le; b.</p>
<p/>
</div>
<div class="page"><p/>
<p>19.8 Problems 597
</p>
<p>Fig. 19.3 A semi-infinite heat-conducting plate
</p>
<p>Fig. 19.4 A heat-conducting rectangular plate
</p>
<p>19.9 Find a general solution for the electromagnetic wave propagation in
a resonant cavity, a rectangular box of sides 0 &le; x &le; a, 0 &le; y &le; b, and resonant cavity
0 &le; z &le; d with perfectly conducting walls. Discuss the modes the cavity
can accommodate.
</p>
<p>19.10 The lateral faces of a cube are grounded, and its top and bottom faces
are held at potentials f1(x, y) and f2(x, y), respectively.
</p>
<p>(a) Find a general expression for the potential inside the cube.
(b) Find the potential if the top is held at V0 volts and the bottom at &minus;V0
</p>
<p>volts.
</p>
<p>19.11 Find the potential inside a semi-infinite cylindrical conductor, closed
at the nearby end, whose cross section is a square with sides of length a.
All sides are grounded except the square side, which is held at the constant
potential V0.
</p>
<p>19.12 Consider a rectangular heat-conducting plate with sides of lengths
a and b. Three of the sides are held at T = 0, and the fourth side has a
temperature variation T = f (x) (see Fig. 19.4). The flat faces are insulated,
so they cannot lose heat to the surroundings. Assume a steady-state heat
transfer. heat-conducting plate:
</p>
<p>steady state(a) Show that the separation of variables yields
</p>
<p>Xn(x)= sin
(
nπ
</p>
<p>a
x
</p>
<p>)
, Y (y)= sinh
</p>
<p>(
nπ
</p>
<p>a
y
</p>
<p>)
for n= 1,2, . . .</p>
<p/>
</div>
<div class="page"><p/>
<p>598 19 Sturm-Liouville Systems
</p>
<p>(b) Show that the most general solution is
</p>
<p>T (x, y)=X(x)Y (y)=
&infin;&sum;
</p>
<p>n=1
Bn sin
</p>
<p>(
nπ
</p>
<p>a
x
</p>
<p>)
sinh
</p>
<p>(
nπ
</p>
<p>a
y
</p>
<p>)
</p>
<p>with
</p>
<p>Bn =
2
</p>
<p>a sinh(nπb/a)
</p>
<p>ˆ a
</p>
<p>0
sin
</p>
<p>(
nπ
</p>
<p>a
x
</p>
<p>)
f (x)dx.
</p>
<p>(c) Show that if the fourth side is held at the constant temperature T0, then
we obtain
</p>
<p>T (x, y)= 4T0
π
</p>
<p>&infin;&sum;
</p>
<p>k=0
</p>
<p>1
</p>
<p>2k+ 1
sin[(2k + 1)πx/a] sinh[(2k + 1)πy/a]
</p>
<p>sinh[(2k + 1)πb/a] .
</p>
<p>(19.47)
(d) If the temperature variation of the fourth side is of the form f (x) =
</p>
<p>T0 sin(πx/a), then
</p>
<p>T (x, y)= T0
sin(πx/a) sinh(πy/a)
</p>
<p>sinh(πb/a)
. (19.48)
</p>
<p>19.13 Find the temperature distribution of a rectangular plate (see Fig. 19.4)
with sides of lengths a and b if three sides are held at T = 0 and the fourth
side has a temperature variation given by
</p>
<p>(a)
T0
</p>
<p>a
x, 0 &le; x &lt; a. (b) T0
</p>
<p>a2
x(x &minus; a), 0 &le; x &le; a.
</p>
<p>(c)
T0
</p>
<p>a
</p>
<p>∣∣∣∣x &minus;
a
</p>
<p>2
</p>
<p>∣∣∣∣, 0 &le; x &lt; a. (d) T = 0, 0 &le; x &le; a.
</p>
<p>19.14 Consider a thin heat-conducting bar of length b along the x-axis with
one end at x = 0 held at temperature T0 and the other end at x = b held at
temperature &minus;T0. The lateral surface of the bar is thermally insulated. Find
the temperature distribution at all times if initially it is given by
</p>
<p>(a) T (0, x)=&minus;2T0
b
</p>
<p>x + T0, where 0 &le; x &le; b.
</p>
<p>(b) T (0, x)=&minus;2T0
b2
</p>
<p>x2 + T0, where 0 &le; x &le; b.
</p>
<p>(c) T (0, x)= T0
b
x + T0, where 0 &le; x &lt; b.
</p>
<p>(d) T (0, x)= T0 cos
(
π
</p>
<p>b
x
</p>
<p>)
, where 0 &le; x &le; b.
</p>
<p>Hint: The solution corresponding to the zero eigenvalue is essential and can-
not be excluded.
</p>
<p>19.15 Determine T (x, y, t) for the rectangular plate of Sect. 19.5.2 if ini-
tially the lower left quarter is held at T0 and the rest of the plate is held at
T = 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>19.8 Problems 599
</p>
<p>19.16 All sides of the plate of Sect. 19.5.2 are held at T = 0. Find the
temperature distribution for all time if the initial temperature distribution
is given by
</p>
<p>(a) T (x, y,0)=
{
T0 if 14a &le; x &le; 34a and 14b &le; y &le; 34b,
0 otherwise.
</p>
<p>(b) T (x, y,0)= T0
ab
</p>
<p>xy, where 0 &le; x &lt; a and 0 &le; y &lt; b.
</p>
<p>(c) T (x, y,0)= T0
a
x, where 0 &le; x &lt; a and 0 &lt; y &lt; b.
</p>
<p>19.17 Repeat the example of Sect. 19.5.2 with the temperatures of the sides
equal to T1, T2, T3, and T4. Hint: You must include solutions corresponding
to the zero eigenvalue.
</p>
<p>19.18 A string of length a is fixed at the left end, and the right end moves
with displacement A sinωt . Find ψ(x, t) and a consistent set of initial con-
ditions for the displacement and the velocity.
</p>
<p>19.19 Find the equation for a vibrating rectangular membrane with sides of
lengths a and b rigidly fastened on all sides. For a = b, show that a given
mode frequency may have more than one solution.
</p>
<p>19.20 Repeat the example of Sect. 19.6.1 if the can has semi-infinite length,
the lateral surface is grounded, and
</p>
<p>(a) the base is held at the potential V (ρ,ϕ).
(b) Specialize to the case where the potential of the base is given&mdash;in
</p>
<p>Cartesian coordinates&mdash;by
</p>
<p>(i) V = V0
a
y. (ii) V = V0
</p>
<p>a
x. (iii) V = V0
</p>
<p>a2
xy.
</p>
<p>Hint: Use the integral identity
&acute;
</p>
<p>zν+1Jν(z) dz= zν+1Jν+1(z).
</p>
<p>19.21 Find the steady-state temperature distribution T (ρ,ϕ, z) in a semi-
infinite solid cylinder of radius a if the temperature distribution of the base
is f (ρ,ϕ) and the lateral surface is held at T = 0.
</p>
<p>19.22 Find the steady-state temperature distribution of a solid cylinder with
a height and radius of a, assuming that the base and the lateral surface are
at T = 0 and the top is at T0.
</p>
<p>19.23 The circumference of a flat circular plate of radius a, lying in the xy-
plane, is held at T = 0. Find the temperature distribution for all time if the
temperature distribution at t = 0 is given&mdash;in Cartesian coordinates&mdash;by
</p>
<p>(a)
T0
</p>
<p>a
y. (b)
</p>
<p>T0
</p>
<p>a
x. (c)
</p>
<p>T0
</p>
<p>a2
xy. (d) T0.</p>
<p/>
</div>
<div class="page"><p/>
<p>600 19 Sturm-Liouville Systems
</p>
<p>19.24 Find the temperature of a circular conducting plate of radius a at all
points of its surface for all time t &gt; 0, assuming that its edge is held at T = 0
and initially its surface from the center to a/2 is in contact with a heat bath
of temperature T0.
</p>
<p>19.25 Find the potential of a cylindrical conducting can of radius a and
height h whose top is held at a constant potential V0 while the rest is
grounded.
</p>
<p>19.26 Consider a wave guide with a rectangular cross section of sides a and
b in the x and the y directions, respectively.
</p>
<p>(a) Show that the separated DEs have the following solutions:
</p>
<p>Xn(x)= sin
(
nπ
</p>
<p>a
x
</p>
<p>)
, λn =
</p>
<p>(
nπ
</p>
<p>a
</p>
<p>)2
for n= 1,2, . . . ,
</p>
<p>Ym(y)= sin
(
mπ
</p>
<p>b
y
</p>
<p>)
, μm =
</p>
<p>(
mπ
</p>
<p>b
</p>
<p>)2
for m= 1,2, . . . ,
</p>
<p>with γ 2mn = λn +μm.
(b) Using the fact that the wave number must be real, show that there is a
</p>
<p>cutoff frequency given by
</p>
<p>ωmn = c
&radic;(
</p>
<p>nπ
</p>
<p>a
</p>
<p>)2
+
(
mπ
</p>
<p>b
</p>
<p>)2
for m,n&ge; 1.
</p>
<p>(c) Show that the most general solution for Ez is therefore
</p>
<p>Ez =
&infin;&sum;
</p>
<p>m,n=1
Amn sin
</p>
<p>(
nπ
</p>
<p>a
x
</p>
<p>)
sin
</p>
<p>(
mπ
</p>
<p>b
y
</p>
<p>)
ei(ωt&plusmn;kmnz).
</p>
<p>19.27 Consider a circular heat-conducting plate of radius a whose temper-circular heat-conducting
plate ature at time t = 0 has a distribution function f (ρ,ϕ). Let us find the vari-
</p>
<p>ation of T for all points (ρ,ϕ) on the plate for time t &gt; 0 when the edge is
kept at T = 0.
(a) Show that the two-dimensional heat equation, after separation yields
</p>
<p>the following ODEs:
</p>
<p>dg
</p>
<p>dt
= k2λg, d
</p>
<p>2S
</p>
<p>dϕ2
+μS = 0,
</p>
<p>d2R
</p>
<p>dρ2
+ 1
</p>
<p>ρ
</p>
<p>dR
</p>
<p>dρ
&minus;
(
μ
</p>
<p>ρ2
+ λ
</p>
<p>)
R = 0.
</p>
<p>(b) Show that these DEs leads to the following solutions:
</p>
<p>g(t)=Ae&minus;k2b2t , S(ϕ)= B cosmϕ +C sinmϕ,
R(ρ)=DJm(bρ).
</p>
<p>where λ&equiv;&minus;b2</p>
<p/>
</div>
<div class="page"><p/>
<p>19.8 Problems 601
</p>
<p>(c) Show that the general solution can be written as
</p>
<p>T (ρ,ϕ, t)=
&infin;&sum;
</p>
<p>m=0
</p>
<p>&infin;&sum;
</p>
<p>n=1
e&minus;k
</p>
<p>2(xmn/a)
2tJm
</p>
<p>(
xmn
</p>
<p>a
ρ
</p>
<p>)
</p>
<p>&times; (Amn cosmϕ +Bmn sinmϕ).
</p>
<p>Amn and Bmn can be determined as in Sect. 19.6.1.
</p>
<p>19.28 Consider a quantum particle in a cylindrical can. For an atomic par- quantum particle in a
cylindrical canticle of mass μ confined in a cylindrical can of length L and radius a, the
</p>
<p>relevant Schr&ouml;dinger equation is
</p>
<p>i
&part;ψ
</p>
<p>&part;t
=&minus; �
</p>
<p>2μ
</p>
<p>[
1
</p>
<p>ρ
</p>
<p>&part;
</p>
<p>&part;ρ
</p>
<p>(
ρ
&part;ψ
</p>
<p>&part;ρ
</p>
<p>)
+ 1
</p>
<p>ρ2
</p>
<p>&part;2ψ
</p>
<p>&part;ϕ2
+ &part;
</p>
<p>2ψ
</p>
<p>&part;z2
</p>
<p>]
,
</p>
<p>subject to the BC that ψ(ρ,ϕ, z, t) vanishes at the sides of the can.
</p>
<p>(a) Show that the separation of variables yields
</p>
<p>dT
</p>
<p>dt
=&minus;iωT , d
</p>
<p>2Z
</p>
<p>dz2
+ λZ = 0, d
</p>
<p>2S
</p>
<p>dϕ2
+m2S = 0,
</p>
<p>d2R
</p>
<p>dρ2
+ 1
</p>
<p>ρ
</p>
<p>dR
</p>
<p>dρ
+
(
</p>
<p>2μ
</p>
<p>�
ω&minus; λ&minus; m
</p>
<p>2
</p>
<p>ρ2
</p>
<p>)
R = 0.
</p>
<p>(19.49)
</p>
<p>(b) Show that the energy eigenvalues are
</p>
<p>Ekmn &equiv; �ωkmn =
�2
</p>
<p>2μ
</p>
<p>[(
kπ
</p>
<p>L
</p>
<p>)2
+ x
</p>
<p>2
mn
</p>
<p>a2
</p>
<p>]
,
</p>
<p>where xmn is the nth zero of Jm(x), the Bessel function of order m,
and k is related to λ by λ= (kπ/L)2.
</p>
<p>(c) Show that the general solution can be written as
</p>
<p>ψ =
&infin;&sum;
</p>
<p>k,n= 1
m= 0
</p>
<p>e&minus;iωkmntJm
</p>
<p>(
xmn
</p>
<p>a
ρ
</p>
<p>)
sin
</p>
<p>(
kπ
</p>
<p>L
z
</p>
<p>)
</p>
<p>&times; (Akmn cosmϕ +Bkmn sinmϕ).
</p>
<p>19.29 Find the modes and the corresponding fields of a cylindrical resonant
cavity of length L and radius a. Discuss the lowest TM mode.
</p>
<p>19.30 Two identical long conducting half-cylindrical shells (cross sections
are half-circles) of radius a are glued together in such a way that they are
insulated from one another. One half-cylinder is held at potential V0 and
the other is grounded. Find the potential at any point inside the resulting
cylinder. Hint: Separate Laplace&rsquo;s equation in two dimensions.</p>
<p/>
</div>
<div class="page"><p/>
<p>602 19 Sturm-Liouville Systems
</p>
<p>19.31 A linear charge distribution of uniform density λ extends along the
z-axis from z = &minus;b to z = b. Show that the electrostatic potential at any
point r &gt; b is given by
</p>
<p>Φ(r, θ,ϕ)= 2λ
&infin;&sum;
</p>
<p>k=0
</p>
<p>(b/r)2k+1
</p>
<p>2k+ 1 P2k(cos θ).
</p>
<p>Hint: Consider a point on the z-axis at a distance r &gt; b from the origin.
Solve the simple problem by integration and compare the result with the
infinite series to obtain the unknown coefficients.
</p>
<p>19.32 The upper half of a heat-conducting sphere of radius a has tem-
perature T0; the lower half is maintained at temperature &minus;T0. The whole
sphere is inside an infinitely large mass of heat-conducting material. Find
the steady-state temperature distribution inside and outside the sphere.
</p>
<p>19.33 Find the steady-state temperature distribution inside a sphere of ra-
dius a when the surface temperature is given by:
</p>
<p>(a) T0 cos2 θ, (b) T0 cos4 θ, (c) T0| cos θ |,
</p>
<p>(d) T0(cos θ &minus; cos3 θ), (e) T0 sin2 θ, (f) T0 sin4 θ.
</p>
<p>19.34 Find the electrostatic potential both inside and outside a conducting
sphere of radius a when the sphere is maintained at a potential given by
</p>
<p>(a) V0(cos θ &minus; 3 sin2 θ),
(b) V0(5 cos
</p>
<p>3 θ &minus; 3 sin2 θ),
</p>
<p>(c)
</p>
<p>{
V0 cos θ for the upper hemisphere,
</p>
<p>0 for the lower hemisphere.
</p>
<p>19.35 Find the steady-state temperature distribution inside a solid hemi-
sphere of radius a if the curved surface is held at T0 and the flat surface
at T = 0. Hint: Imagine completing the sphere and maintaining the lower
hemisphere at a temperature such that the overall surface temperature distri-
bution is an odd function about θ = π/2.
</p>
<p>19.36 Find the steady-state temperature distribution in a spherical shell of
inner radius R1 and outer radius R2 when the inner surface has a temperature
T1 and the outer surface a temperature T2.</p>
<p/>
</div>
<div class="page"><p/>
<p>Part VI
</p>
<p>Green&rsquo;s Functions</p>
<p/>
</div>
<div class="page"><p/>
<p>20Green&rsquo;s Functions in One Dimension
</p>
<p>Our treatment of differential equations, with the exception of SOLDEs with
constant coefficients, did not consider inhomogeneous equations. At this
point, however, we can put into use one of the most elegant pieces of ma-
chinery in higher mathematics, Green&rsquo;s functions, to solve inhomogeneous
differential equations.
</p>
<p>This chapter addresses Green&rsquo;s functions in one dimension, that is,
Green&rsquo;s functions of ordinary differential equations. Consider the ODE
Lx[u] = f (x) where Lx is a linear differential operator. In the abstract Dirac
notation this can be formally written as L|u〉 = |f 〉. If L has an inverse
L&minus;1 &equiv; G, the solution can be formally written as |u〉 = L&minus;1|f 〉 = G|f 〉.
Multiplying this by 〈x| and inserting 1 =
</p>
<p>&acute;
</p>
<p>dy|y〉w(y)〈y| between G and
|f 〉 gives
</p>
<p>u(x)=
ˆ
</p>
<p>dyG(x, y)w(y)f (y), (20.1)
</p>
<p>where the integration is over the range of definition of the functions in-
volved. Once we know G(x,y), Eq. (20.1) gives the solution u(x) in an
integral form. But how do we find G(x,y)?
</p>
<p>Sandwiching both sides of LG= 1 between 〈x| and |y〉 and using
</p>
<p>1=
ˆ
</p>
<p>dx&prime;
∣∣x&prime;
</p>
<p>&rang;
w
(
x&prime;
)&lang;
x&prime;
∣∣
</p>
<p>between L and G yields
ˆ
</p>
<p>dx&prime;L
(
x, x&prime;
</p>
<p>)
w
(
x&prime;
)
G
(
x&prime;, y
</p>
<p>)
= 〈x|y〉 = δ(x &minus; y)
</p>
<p>w(x)
</p>
<p>if we use Eq. (7.19). In particular, if L is a local differential operator (see
Sect. 17.1), then L(x, x&prime;)= [δ(x &minus; x&prime;)/w(x)]Lx , and we obtain
</p>
<p>differential equation for
</p>
<p>Green&rsquo;s function
LxG(x,y)=
</p>
<p>δ(x &minus; y)
w(x)
</p>
<p>or LxG(x,y)= δ(x &minus; y), (20.2)
</p>
<p>where the second equation makes the frequently used assumption that
w(x) = 1. G(x,y) is called the Green&rsquo;s function (GF) for the differential Green&rsquo;s function
operator (DO) Lx .
</p>
<p>S. Hassani, Mathematical Physics, DOI 10.1007/978-3-319-01195-0_20,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>605</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-01195-0_20">http://dx.doi.org/10.1007/978-3-319-01195-0_20</a></div>
</div>
<div class="page"><p/>
<p>606 20 Green&rsquo;s Functions in One Dimension
</p>
<p>As discussed in Chaps. 17 and 19, Lx might not be defined for all func-
tions on R. Moreover, a complete specification of Lx requires some initial
(or boundary) conditions. Therefore, we expect G(x,y) to depend on such
initial conditions as well. We note that when Lx is applied to (20.1), we get
</p>
<p>Lxu(x)=
ˆ
</p>
<p>dy
[
Lx(G(x, y))
</p>
<p>]
w(y)f (y)
</p>
<p>=
ˆ
</p>
<p>dy
δ(x &minus; y)
w(x)
</p>
<p>w(y)f (y)= f (x),
</p>
<p>indicating that u(x) is indeed a solution of the original ODE. Equa-
tion (20.2), involving the generalized function δ(x &minus; y) (or distribution in
the language of Sect. 7.3), is meaningful only in the same context. Thus, we
treat G(x,y) not as an ordinary function but as a distribution. Finally, (20.1)
is assumed to hold for an arbitrary (well-behaved) function f .
</p>
<p>20.1 Calculation of Some Green&rsquo;s Functions
</p>
<p>This section presents some examples of calculating G(x,y) for very sim-
ple DOs. Later we will see how to obtain Green&rsquo;s functions for a general
second-order linear differential operator. Although the complete specifica-
tion of GFs requires boundary conditions, we shall introduce unspecified
constants in some of the examples below, and calculate some indefinite GFs.indefinite Green&rsquo;s
</p>
<p>Functions
Example 20.1.1 Let us find the GF for the simplest DO, Lx = d/dx. We
need to find a distribution such that its derivative is the Dirac delta function:1
</p>
<p>G&prime;(x, y)= δ(x &minus; y). In Sect. 7.3, we encountered such a distribution&mdash;the
step function θ(x &minus; y). Thus,
</p>
<p>G(x,y)= θ(x &minus; y)+ α(y),
</p>
<p>where α(y) is the &ldquo;constant&rdquo; of integration.
</p>
<p>The example above did not include a boundary (or initial) condition. Let
us see how boundary conditions affect the resulting GF.
</p>
<p>Example 20.1.2 Let us solve u&prime;(x) = f (x) where x &isin; [0,&infin;) and u(0) =
0. A general solution of this DE is given by Eq. (20.1) and the preceding
example:
</p>
<p>u(x)=
ˆ &infin;
</p>
<p>0
θ(x &minus; y)f (y)dy +
</p>
<p>ˆ &infin;
</p>
<p>0
α(y)f (y)dy.
</p>
<p>The factor θ(x &minus; y) in the first term on the RHS chops off the integral at x:
</p>
<p>u(x)=
ˆ x
</p>
<p>0
f (y)dy +
</p>
<p>ˆ &infin;
</p>
<p>0
α(y)f (y)dy.
</p>
<p>1Here and elsewhere in this chapter, a prime over a GF indicates differentiation with
respect to its first argument.</p>
<p/>
</div>
<div class="page"><p/>
<p>20.1 Calculation of Some Green&rsquo;s Functions 607
</p>
<p>The BC gives
</p>
<p>0 = u(0)= 0 +
ˆ &infin;
</p>
<p>0
α(y)f (y)dy.
</p>
<p>The only way that this can be satisfied for arbitrary f (y) is for α(y) to be
zero. Thus, G(x,y)= θ(x &minus; y), and
</p>
<p>u(x)=
ˆ &infin;
</p>
<p>0
θ(x &minus; y)f (y)dy =
</p>
<p>ˆ x
</p>
<p>0
f (y)dy.
</p>
<p>This is killing a fly with a sledgehammer! We could have obtained the
result by a simple integration. However, the roundabout way outlined here
illustrates some important features of GFs that will be discussed later. The
BC introduced here is very special. What happens if it is changed to u(0)=
a? Problem 20.1 answers that.
</p>
<p>Example 20.1.3 A more complicated DO is Lx = d2/dx2. Let us find its
indefinite GF. To do so, we integrate G&prime;&prime;(x, y)= δ(x&minus; y) once with respect
to x to obtain
</p>
<p>d
</p>
<p>dx
G(x, y)= θ(x &minus; y)+ α(y).
</p>
<p>A second integration yields
</p>
<p>G(x,y)=
ˆ
</p>
<p>dxθ(x &minus; y)+ xα(y)+ η(y),
</p>
<p>where α and η are arbitrary functions and the integral is an indefinite integral
to be evaluated next.
</p>
<p>Let Ω(x,y) be the primitive of θ(x &minus; y); that is,
</p>
<p>dΩ
</p>
<p>dx
= θ(x &minus; y)=
</p>
<p>{
1 if x &gt; y,
</p>
<p>0 if x &lt; y.
(20.3)
</p>
<p>The solution to this equation is
</p>
<p>Ω(x,y)=
{
x + a(y) if x &gt; y,
b(y) if x &lt; y.
</p>
<p>Note that we have not defined Ω(x,y) at x = y. It will become clear below
that Ω(x,y) is continuous at x = y. It is convenient to write Ω(x,y) as
</p>
<p>Ω(x,y)=
[
x + a(y)
</p>
<p>]
θ(x &minus; y)+ b(y)θ(y &minus; x). (20.4)
</p>
<p>To specify a(y) and b(y) further, we differentiate (20.4) and compare it
with (20.3):
</p>
<p>dΩ
</p>
<p>dx
= θ(x &minus; y)+
</p>
<p>[
x + a(y)
</p>
<p>]
δ(x &minus; y)&minus; b(y)δ(x &minus; y)
</p>
<p>= θ(x &minus; y)+
[
x &minus; b(y)+ a(y)
</p>
<p>]
δ(x &minus; y), (20.5)</p>
<p/>
</div>
<div class="page"><p/>
<p>608 20 Green&rsquo;s Functions in One Dimension
</p>
<p>where we have used
</p>
<p>d
</p>
<p>dx
θ(x &minus; y)=&minus; d
</p>
<p>dx
θ(y &minus; x)= δ(x &minus; y).
</p>
<p>For Eq. (20.5) to agree with (20.3), we must have [x &minus; b(y)+ a(y)]δ(x &minus;
y)= 0, which, upon integration over x, yields a(y)&minus; b(y)=&minus;y. Substitut-
ing this in the expression for Ω(x,y) gives
</p>
<p>Ω(x,y)= (x &minus; y)θ(x &minus; y)+ b(y)
[
θ(x &minus; y)+ θ(y &minus; x)
</p>
<p>]
.
</p>
<p>But θ(x) + θ(&minus;x) = 1; therefore, Ω(x,y) = (x &minus; y)θ(x &minus; y) + b(y). It
follows, among other things, that Ω(x,y) is continuous at x = y. We can
now write
</p>
<p>G(x,y)= (x &minus; y)θ(x &minus; y)+ xα(y)+ β(y),
where β(y)= η(y)+ b(y).
</p>
<p>The GF in the example above has two arbitrary functions, α(y) and β(y),
which are the result of underspecification of Lx : A full specification of Lx
requires BCs, as the following example shows.
</p>
<p>Example 20.1.4 Let us calculate the GF of Lx[u] = u&prime;&prime;(x)= f (x) subject
to the BC u(a) = u(b) = 0 where [a, b] is the interval on which Lx is de-
fined. Example 20.1.3 gives us the (indefinite) GF for Lx . Using that, we can
write
</p>
<p>u(x)=
ˆ b
</p>
<p>a
</p>
<p>(x &minus; y)θ(x &minus; y)f (y) dy + x
ˆ b
</p>
<p>a
</p>
<p>α(y)f (y) dy
</p>
<p>+
ˆ b
</p>
<p>a
</p>
<p>β(y)f (y) dy
</p>
<p>=
ˆ x
</p>
<p>a
</p>
<p>(x &minus; y)f (y) dy + x
ˆ b
</p>
<p>a
</p>
<p>α(y)f (y) dy +
ˆ b
</p>
<p>a
</p>
<p>β(y)f (y) dy.
</p>
<p>Applying the BCs yields
</p>
<p>0 = u(a)= a
ˆ b
</p>
<p>a
</p>
<p>α(y)f (y) dy +
ˆ b
</p>
<p>a
</p>
<p>β(y)f (y) dy,
</p>
<p>0 = u(b)=
ˆ b
</p>
<p>a
</p>
<p>(b&minus; y)f (y) dy + b
ˆ b
</p>
<p>a
</p>
<p>α(y)f (y) dy
</p>
<p>+
ˆ b
</p>
<p>a
</p>
<p>β(y)f (y) dy.
</p>
<p>(20.6)
</p>
<p>From these two relations it is possible to determine α(y) and β(y): Sub-
stitute for the last integral on the RHS of the second equation of (20.6) from
the first equation and get
</p>
<p>0 =
ˆ b
</p>
<p>a
</p>
<p>[
b&minus; y + bα(y)&minus; aα(y)
</p>
<p>]
f (y)dy.</p>
<p/>
</div>
<div class="page"><p/>
<p>20.1 Calculation of Some Green&rsquo;s Functions 609
</p>
<p>Since this must hold for arbitrary f (y), we conclude that
</p>
<p>b&minus; y + (b&minus; a)α(y)= 0 &rArr; α(y)=&minus;b&minus; y
b&minus; a .
</p>
<p>Substituting for α(y) in the first equation of (20.6) and noting that the result
holds for arbitrary f , we obtain β(y)= a(b&minus; y)/(b&minus; a). Insertion of α(y)
and β(y) in the expression for G(x,y) obtained in Example 20.1.3 gives
</p>
<p>G(x,y)= (x &minus; y)θ(x &minus; y)+ (x &minus; a)y &minus; b
b&minus; a where a &le; x and y &le; b.
</p>
<p>It is striking that G(a,y) = (a &minus; y)θ(a &minus; y) = 0 (because a &minus; y &le; 0),
and
</p>
<p>G(b,y)= (b&minus; y)θ(b&minus; y)+ (b&minus; a)y &minus; b
b&minus; a = 0
</p>
<p>because θ(b &minus; y) = 1 for all y &le; b [recall that x and y lie in the interval
(a, b)]. These two equations reveal the important fact that as a function of
x, G(x,y) satisfies the same (homogeneous) BC as the solution of the DE.
This is a general property that will be discussed later.
</p>
<p>In all the preceding examples, the BCs were very simple. Specifically, the
value of the solution and/or its derivative at the boundary points was zero.
What if the BCs are not so simple? In particular, how can we handle a case
where u(a) [or u&prime;(a)] and u(b) [or u&prime;(b)] are nonzero?
</p>
<p>Consider a general (second-order) differential operator Lx and the differ-
ential equation Lx[u] = f (x) subject to the BCs u(a) = a1 and u(b) = b1.
We claim that we can reduce this system to the case where u(a)= u(b)= 0.
Recall from Chap. 14 that the most general solution to such a DE is of the
form u= uh + ui where uh, the solution to the homogeneous equation, sat-
isfies Lx[uh] = 0 and contains the arbitrary parameters inherent in solutions
of differential equations. For instance, if the linearly independent solutions
are v and w, then uh(x)= C1v(x)+ C2w(x) and ui is any solution of the
inhomogeneous DE.
</p>
<p>If we demand that uh(a)= a1 and uh(b)= b1, then ui satisfies the sys-
tem
</p>
<p>Lx[ui] = f (x), ui(a)= ui(b)= 0,
which is of the type discussed in the preceding examples. Since Lx is a
SOLDO, we can put all the machinery of Chap. 14 to work to obtain v(x),
w(x), and therefore uh(x). The problem then reduces to a DE for which the
BCs are homogeneous; that is, the value of the solution and/or its derivative
is zero at the boundary points.
</p>
<p>Example 20.1.5 Let us assume that Lx = d2/dx2. Calculation of uh is triv-
ial:
</p>
<p>Lx[uh] = 0 &rArr;
d2uh
</p>
<p>dx2
= 0 &rArr; uh(x)= C1x +C2.</p>
<p/>
</div>
<div class="page"><p/>
<p>610 20 Green&rsquo;s Functions in One Dimension
</p>
<p>To evaluate C1 and C2, we impose the BCs uh(a)= a1 and uh(b)= b1:
</p>
<p>C1a +C2 = a1,
C1b+C2 = b1.
</p>
<p>This gives C1 = (b1 &minus; a1)/(b&minus; a) and C2 = (a1b&minus; ab1)/(b&minus; a).
The inhomogeneous equation defines a problem identical to that of Ex-
</p>
<p>ample 20.1.4. Thus, we can immediately write ui(x)=
&acute; b
</p>
<p>a
G(x,y)f (y) dy,
</p>
<p>where G(x,y) is as given in that example. Thus, the general solution is
</p>
<p>u(x)= b1 &minus; a1
b&minus; a x +
</p>
<p>a1b&minus; ab1
b&minus; a +
</p>
<p>ˆ x
</p>
<p>a
</p>
<p>(x &minus; y)f (y) dy
</p>
<p>+ x &minus; a
b&minus; a
</p>
<p>ˆ b
</p>
<p>a
</p>
<p>(y &minus; b)f (y) dy.
</p>
<p>Example 20.1.5 shows that an inhomogeneous DE with inhomogeneous
BCs can be separated into two DEs, one homogeneous with inhomogeneous
BCs and the other inhomogeneous with homogeneous BCs, the latter being
appropriate for the GF. Furthermore, all the preceding examples indicate that
solutions of DEs can be succinctly written in terms of GFs that automatically
incorporate the BCs as long as the BCs are homogeneous. Can a GF also
give the solution to a homogeneous DE with inhomogeneous BCs?
</p>
<p>20.2 Formal Considerations
</p>
<p>The discussion and examples of the preceding section hint at the power of
Green&rsquo;s functions. The elegance of such a function becomes apparent from
the realization that it contains all the information about the solutions of a
DE for any type of BCs, as we are about to show. Since GFs are inverses
of DOs, let us briefly reexamine the inverse of an operator, which is closely
tied to its spectrum.
</p>
<p>The question as to whether or not an operator A in a finite-dimensional
vector space is invertible is succinctly answered by the value of its determi-
nant: A is invertible if and only if detA �= 0. In fact, as we saw at the begin-
ning of Chap. 17, one translates the abstract operator equation A|u〉 = |v〉
into a matrix equation Au = v and reduces the question to that of the in-
verse of a matrix. This matrix takes on an especially simple form when A is
diagonal, that is, when Aij = λiδij . For this special situation we have
</p>
<p>λiui = vi for i = 1,2, . . . ,N (no sum over i). (20.7)
</p>
<p>This equation has a unique solution (for arbitrary vi ) if and only if λi �= 0
for all i. In that case ui = vi/λi for i = 1,2, . . . ,N . In particular, if vi = 0
for all i, that is, when Eq. (20.7) is homogeneous, the unique solution is
the trivial solution. On the other hand, when some of the λi are zero, there
may be no solution to (20.7), but the homogeneous equation has a nontrivial
solution (ui need not be zero). Proposition 6.2.6 applies to vector spaces of
finite as well as infinite dimensions. Therefore, we restate it here:</p>
<p/>
</div>
<div class="page"><p/>
<p>20.2 Formal Considerations 611
</p>
<p>Theorem 20.2.1 An operator A on a Hilbert space has an inverse if and
only if λ = 0 is not an eigenvalue of A. Equivalently, A is invertible if and
only if the homogeneous equation A|u〉 = 0 has no nontrivial solutions.
</p>
<p>Green&rsquo;s functions are inverses of differential operators. Therefore, it is
important to have a clear understanding of the DOs. An nth-order linear
differential operator (NOLDO) satisfies the following theorem (for a proof,
see [Birk 78, Chap. 6]).
</p>
<p>Theorem 20.2.2 Let
</p>
<p>Lx = pn(x)
dn
</p>
<p>dxn
+ pn&minus;1(x)
</p>
<p>dn&minus;1
</p>
<p>dxn&minus;1
+ &middot; &middot; &middot; + p1(x)
</p>
<p>d
</p>
<p>dx
+ p0(x) (20.8)
</p>
<p>where pn(x) �&equiv; 0 in [a, b]. Let x0 &isin; [a, b] and let {γk}nk=1 be given numbers
and f (x) a given piecewise continuous function on [a, b]. Then the initial
value problem (IVP)
</p>
<p>Lx[u] = f for x &isin; [a, b],
</p>
<p>u(x0)= γ1, u&prime;(x0)= γ2, . . . , u(n&minus;1)(x0)= γn
(20.9)
</p>
<p>has one and only one solution.
</p>
<p>This is simply the existence and uniqueness theorem for a NOLDE.
Equation (20.9) is referred to as the IVP with data {f (x);γ1, . . . , γn}. This initial value problem
theorem is used to define Lx . Part of that definition are the BCs that the
solutions to Lx must satisfy.
</p>
<p>A particularly important BC is the homogeneous one in which γ1 = γ2 =
&middot; &middot; &middot; = γn = 0. In such a case it can be shown (see Problem 20.3) that the
only nontrivial solution of the homogeneous DE Lx[u] = 0 is u&equiv; 0. Theo-
rem 20.2.1 then tells us that Lx is invertible; that is, there is a unique operator
G such that LG= 1. The &ldquo;components&rdquo; version of this last relation is part of
the content of the next theorem.
</p>
<p>Theorem 20.2.3 The DO Lx of Eq. (20.8) associated with the IVP
with data {f (x);0,0, . . . ,0} is invertible; that is, there exists a func-
tion G(x,y) such that
</p>
<p>LxG(x,y)=
δ(x &minus; y)
w(x)
</p>
<p>.
</p>
<p>The importance of homogeneous BCs can now be appreciated. Theo-
rem 20.2.3 is the reason why we had to impose homogeneous BCs to obtain
the GF in all the examples of the previous section.</p>
<p/>
</div>
<div class="page"><p/>
<p>612 20 Green&rsquo;s Functions in One Dimension
</p>
<p>The BCs in (20.9) clearly are not the only ones that can be used. The
most general linear BCs encountered in differential operator theory are
</p>
<p>Ri[u] &equiv;
n&sum;
</p>
<p>j=1
</p>
<p>(
αiju
</p>
<p>(j&minus;1)(a)+ βiju(j&minus;1)(b)
)
= γi, i = 1, . . . , n. (20.10)
</p>
<p>The n row vectors {(αi1, . . . , αin, βi1, . . . , βin)}ni=1 are assumed to be inde-
pendent (in particular, no row is identical to zero). We refer to Ri as bound-
ary functionals because for each (sufficiently smooth) function u, they give
a number γi . The DO of (20.8) and the BCs of (20.10) together form aboundary functionals
</p>
<p>and boundary value
</p>
<p>problem
</p>
<p>boundary value problem (BVP). The DE Lx[u] = f subject to the BCs of
(20.10) is a BVP with data {f (x);γ1, . . . , γn}.
</p>
<p>We note that the Ri are linear; that is,
</p>
<p>Ri[u1 + u2] = Ri[u1] + Ri[u2] and Ri[αu] = αRi[u].
</p>
<p>Since Lx is also linear, we conclude that the superposition principle ap-
plies to the system consisting of Lx[u] = f and the BCs of (20.10), which
is sometimes denoted by (L;R1, . . . ,Rn). If u satisfies the BVP with data
{f ;γ1, . . . , γn} and v satisfies the BVP with data {g;μ1, . . . ,μn}, then
αu+βv satisfies the BVP with data {αf +βg;αγ1+βμ1, . . . , αγn+βμn}.
It follows that if u and v both satisfy the BVP with data {f ;γ1, . . . , γn}, then
u&minus; v satisfies the BVP with data {0;0,0, . . . ,0}, which is called the com-completely
</p>
<p>homogeneous problem pletely homogeneous problem.
Unlike the IVP, the BVP with data {0;0,0, . . . ,0} may have a nontrivial
</p>
<p>solution. If the completely homogeneous problem has no nontrivial solution,
then the BVP with data {f ;γ1, . . . , γn} has at most one solution (a solution
exists for any set of data). On the other hand, if the completely homogeneous
problem has nontrivial solutions, then the BVP with data {f ;γ1, . . . , γn}
either has no solutions or has more than one solution (see [Stak 79, pp. 203&ndash;
204]).
</p>
<p>Recall that when a differential (unbounded) operator Lx acts in a Hilbert
space, such as L2w(a, b), it acts only on its domain. In the context of the
present discussion, this means that not all functions in L2w(a, b) satisfy the
BCs necessary for defining Lx . Thus, the functions for which the operator
is defined (those that satisfy the BCs) form a subset of L2w(a, b), which we
called the domain of Lx and denoted by D(Lx). From a formal standpoint it
is important to distinguish among maps that have different domains. For in-
stance, the Hilbert-Schmidt integral operators, which are defined on a finite
interval, are compact, while those defined on the entire real line are not.
</p>
<p>Definition 20.2.4 Let Lx be the DO of Eq. (20.8). Suppose there exists a
DO L&dagger;x , with the property thatadjoint of a differential
</p>
<p>operator
</p>
<p>w
{
v&lowast;
(
Lx[u]
</p>
<p>)
&minus; u
</p>
<p>(
L&dagger;x[v]
</p>
<p>)&lowast;}= d
dx
</p>
<p>Q
[
u,v&lowast;
</p>
<p>]
for u,v &isin;D(Lx)&cap;D
</p>
<p>(
L&dagger;x
)
,
</p>
<p>where Q[u,v&lowast;], called the conjunct of the functions u and v, depends onconjunct
u, v, and their derivatives of order up to n &minus; 1. The DO L&dagger;x is then called
the formal adjoint of Lx . If L&dagger;x = Lx (without regard to the BCs imposed</p>
<p/>
</div>
<div class="page"><p/>
<p>20.2 Formal Considerations 613
</p>
<p>on their solutions), then Lx is said to be formally self-adjoint. If D(L&dagger;x)&sup;
D(Lx) and L&dagger;x = Lx on D(Lx), then Lx is said to be hermitian. If D(L&dagger;x)=
D(Lx) and L&dagger;x = Lx , then Lx is said to be self-adjoint.
</p>
<p>The relation given in the definition above involving the conjunct is a gen-
eralization of the Lagrange identity and can also be written in integral form: generalized Green&rsquo;s
</p>
<p>identity
ˆ b
</p>
<p>a
</p>
<p>dxw
{
v&lowast;
(
Lx[u]
</p>
<p>)}
&minus;
ˆ b
</p>
<p>a
</p>
<p>dxw
{
u
(
L&dagger;x[v]
</p>
<p>)&lowast;}=Q
[
u,v&lowast;
</p>
<p>]∣∣b
a
</p>
<p>(20.11)
</p>
<p>This form is sometimes called the generalized Green&rsquo;s identity.
</p>
<p>Historical Notes
</p>
<p>George Green (1793?&ndash;1841) was not appreciated in his lifetime. His date of birth is
</p>
<p>George Green
</p>
<p>1793?&ndash;1841
</p>
<p>unknown (however, it is known that he was baptized on 14 July 1793), and no portrait
of him survives. He left school, after only one year&rsquo;s attendance, to work in his father&rsquo;s
bakery. When the father opened a windmill in Nottingham, the boy used an upper room as
a study in which he taught himself physics and mathematics from library books. In 1828,
when he was thirty-five years old, he published his most important work, An Essay on the
Application of Mathematical Analysis to the Theory of Electricity and Magnetism at his
own expense. In it Green apologized for any shortcomings in the paper due to his minimal
formal education or the limited resources available to him, the latter being apparent in the
few previous works he cited. The introduction explained the importance Green placed on
the &ldquo;potential&rdquo; function. The body of the paper generalizes this idea to electricity and
magnetism.
In addition to the physics of electricity and magnetism, Green&rsquo;s first paper also contained
the monumental mathematical contributions for which he is now famous: The relation-
ship between surface and volume integrals we now call Green&rsquo;s theorem, and the Green&rsquo;s
function, a ubiquitous solution to partial differential equations in almost every area of
physics. With little appreciation for the future impact of this work, one of Green&rsquo;s con-
temporaries declared the publication &ldquo;a complete failure&rdquo;. The &ldquo;Essay&rdquo;, which received
little notice because of poor circulation, was saved by Lord Kelvin, who tracked it down
in a German journal.
When his father died in 1829, some of George&rsquo;s friends urged him to seek a college edu-
cation. After four years of self-study, during which he closed the gaps in his elementary
education, Green was admitted to Caius College of Cambridge University at the age of
40, from which he graduated four years later after a disappointing performance on his fi-
nal examinations. Later, however, he was appointed Perce Fellow of Caius College. Two
years after his appointment he died, and his famous 1828 paper was republished, this
time reaching a much wider audience. This paper has been described as &ldquo;the beginning
of mathematical physics in England&rdquo;.
He published only ten mathematical works. In 1833 he wrote three further papers. Two on
electricity were published by the Cambridge Philosophical Society. One on hydrodynam-
ics was published by the Royal Society of Edinburgh (of which he was a Fellow) in 1836.
He also had two papers on hydrodynamics (in particular wave motion in canals), two pa-
pers on reflection and refraction of light, and two papers on reflection and refraction of
sound published in Cambridge.
In 1923 the Green windmill was partially restored by a local businessman as a gesture of
tribute to Green. Einstein came to pay homage. Then a fire in 1947 destroyed the reno-
vations. Thirty years later the idea of a memorial was once again mooted, and sufficient
money was raised to purchase the mill and present it to the sympathetic Nottingham City
Council. In 1980 the George Green Memorial Appeal was launched to secure &pound;20,000 to
get the sails turning again and the machinery working once more. Today, Green&rsquo;s restored
mill stands as a mathematics museum in Nottingham.</p>
<p/>
</div>
<div class="page"><p/>
<p>614 20 Green&rsquo;s Functions in One Dimension
</p>
<p>20.2.1 Second-Order Linear DOs
</p>
<p>Since second-order linear differential operators (SOLDOs) are sufficiently
general for most physical applications, we will concentrate on them. Be-
cause homogeneous BCs are important in constructing Green&rsquo;s functions,
let us first consider BCs of the form
</p>
<p>R1[u] &equiv; α11u(a)+ α12u&prime;(a)+ β11u(b)+ β12u&prime;(b)= 0,
R2[u] &equiv; α21u(a)+ α22u&prime;(a)+ β21u(b)+ β22u&prime;(b)= 0,
</p>
<p>(20.12)
</p>
<p>where it is assumed, as usual, that (α11, α12, β11, β12) and (α21, α22, β21, β22)
are linearly independent.
</p>
<p>If we define the inner product as an integral with weight w, Eq. (20.11)
can be formally written as
</p>
<p>〈v|L|u〉 = 〈u|L&dagger;|v〉&lowast; +Q
[
u,v&lowast;
</p>
<p>]∣∣b
a
.
</p>
<p>This would coincide with the usual definition of the adjoint if the surface
term vanishes, that is, if
</p>
<p>Q
[
u,v&lowast;
</p>
<p>]∣∣
x=b =Q
</p>
<p>[
u,v&lowast;
</p>
<p>]∣∣
x=a . (20.13)
</p>
<p>For this to happen, we need to impose BCs on v. To find these BCs, let us
rewrite Eq. (20.12) in a more compact form. Linear independence of the two
row vectors of coefficients implies that the 2 &times; 4 matrix of coefficients has
rank two. This means that the 2&times;4 matrix has an invertible 2&times;2 submatrix.
By rearranging the terms in Eq. (20.12) if necessary, we can assume that the
second of the two 2 &times; 2 submatrices is invertible. The homogeneous BCs
can then be conveniently written as
</p>
<p>R[u] =
(
R1[u]
R2[u]
</p>
<p>)
=
(
A B
</p>
<p>)(ua
ub
</p>
<p>)
= Aua + Bub = 0, (20.14)
</p>
<p>where
</p>
<p>A &equiv;
(
α11 α12
α21 α22
</p>
<p>)
, B &equiv;
</p>
<p>(
β11 β12
β21 β22
</p>
<p>)
, ua &equiv;
</p>
<p>(
u(a)
</p>
<p>u&prime;(a)
</p>
<p>)
,
</p>
<p>ub &equiv;
(
u(b)
</p>
<p>u&prime;(b)
</p>
<p>)
,
</p>
<p>and B is invertible.
The most general form of the conjunct for a SOLDO is
</p>
<p>Q
[
u,v&lowast;
</p>
<p>]
(x)= q11(x)u(x)v&lowast;(x)+ q12(x)u(x)v&prime;&lowast;(x)
</p>
<p>+ q21(x)u&prime;(x)v&lowast;(x)+ q22(x)u&prime;(x)v&prime;&lowast;(x),
</p>
<p>which can be written in matrix form as
</p>
<p>Q
[
u,v&lowast;
</p>
<p>]
(x)= utxQxv&lowast;x where Qx =
</p>
<p>(
q11(x) q12(x)
</p>
<p>q21(x) q22(x)
</p>
<p>)
, (20.15)</p>
<p/>
</div>
<div class="page"><p/>
<p>20.2 Formal Considerations 615
</p>
<p>and ux and v&lowast;x have similar definitions as ua and ub above. The vanishing of
the surface term becomes
</p>
<p>utbQbv
&lowast;
b = utaQav&lowast;a . (20.16)
</p>
<p>We need to translate this equation into a condition on v&lowast; alone.2 This is
accomplished by solving for two of the four quantities u(a), u&prime;(a), u(b),
and u&prime;(b) in terms of the other two, substituting the result in Eq. (20.16),
and setting the coefficients of the other two equal to zero. Let us assume, as
before, that the submatrix B is invertible, i.e., u(b) and u&prime;(b) are expressible
in terms of u(a) and u&prime;(a). Then ub =&minus;B&minus;1Aua , or utb =&minus;utaAt (Bt )&minus;1, and
we obtain
</p>
<p>&minus;utaAt
(
Bt
)&minus;1
</p>
<p>Qbv
&lowast;
b = utaQav&lowast;a &rArr; uta
</p>
<p>[
At
(
Bt
)&minus;1
</p>
<p>Qbv
&lowast;
b + Qav&lowast;a
</p>
<p>]
= 0,
</p>
<p>and the condition on v&lowast; becomes
</p>
<p>At
(
Bt
)&minus;1
</p>
<p>Qbv
&lowast;
b + Qav&lowast;a = 0. (20.17)
</p>
<p>We see that all factors of u have disappeared, as they should. The expanded
version of the BCs on v&lowast; are written as
</p>
<p>B1
[
v&lowast;
]
&equiv; σ11v&lowast;(a)+ σ12v&prime;&lowast;(a)+ η11v&lowast;(b)+ η12v&prime;&lowast;(b)= 0,
</p>
<p>B2
[
v&lowast;
]
&equiv; σ21v&lowast;(a)+ σ22v&prime;&lowast;(a)+ η21v&lowast;(b)+ η22v&prime;&lowast;(b)= 0.
</p>
<p>(20.18)
</p>
<p>These homogeneous BCs are said to be adjoint to those of (20.12). Because adjoint boundary
conditionsof the difference between BCs and their adjoints, the domain of a differential
</p>
<p>operator need not be the same as that of its adjoint.
</p>
<p>Example 20.2.5 Let Lx = d2/dx2 with the homogeneous BCs
</p>
<p>R1[u] = αu(a)&minus; u&prime;(a)= 0 and R2[u] = βu(b)&minus; u&prime;(b)= 0. (20.19)
</p>
<p>We want to calculate Q[u,v&lowast;] and the adjoint BCs for v. By repeated inte-
gration by parts [or by using Eq. (14.22)], we obtain Q[u,v&lowast;] = u&prime;v&lowast;&minus;uv&prime;&lowast;.
For the surface term to vanish, we must have
</p>
<p>u&prime;(a)v&lowast;(a)&minus; u(a)v&prime;&lowast;(a)= u&prime;(b)v&lowast;(b)&minus; u(b)v&prime;&lowast;(b).
</p>
<p>Substituting from (20.19) in this equation, we get
</p>
<p>u(a)
[
αv&lowast;(a)&minus; v&prime;&lowast;(a)
</p>
<p>]
= u(b)
</p>
<p>[
βv&lowast;(b)&minus; v&prime;&lowast;(b)
</p>
<p>]
,
</p>
<p>which holds for arbitrary u if and only if
</p>
<p>B1
[
v&lowast;
]
= αv&lowast;(a)&minus; v&prime;&lowast;(a)= 0 and B2
</p>
<p>[
v&lowast;
]
= βv&lowast;(b)&minus; v&prime;&lowast;(b)= 0.
</p>
<p>(20.20)
This is a special case, in which the adjoint boundary conditions are the same
as the original BCs (substitute u for v&lowast; to see this).
</p>
<p>2The boundary conditions on v&lowast; should not depend on the choice of u.</p>
<p/>
</div>
<div class="page"><p/>
<p>616 20 Green&rsquo;s Functions in One Dimension
</p>
<p>To see that the original BCs and their adjoints need not be the same, we
consider
</p>
<p>R1[u] = u&prime;(a)&minus; αu(b)= 0 and R2[u] = βu(a)&minus; u&prime;(b)= 0, (20.21)
</p>
<p>from which we obtain u(a)[βv&lowast;(b) + v&prime;&lowast;(a)] = u(b)[αv&lowast;(a) + v&prime;&lowast;(b)].
Thus,
</p>
<p>B1
[
v&lowast;
]
= αv&lowast;(a)+ v&prime;&lowast;(b)= 0 and B2
</p>
<p>[
v&lowast;
]
= βv&lowast;(b)+ v&prime;&lowast;(a)= 0,
</p>
<p>(20.22)
which is not the same as (20.21). Boundary conditions such as those in
(20.19) and (20.20), in which each equation contains the function and its
derivative evaluated at the same point, are called unmixed BCs. On the
other hand, (20.21) and (20.22) are mixed BCs.
</p>
<p>mixed and unmixed BCs
</p>
<p>20.2.2 Self-adjoint SOLDOs
</p>
<p>In Chap. 14, we showed that a SOLDO satisfies the generalized Green&rsquo;s
identity with w(x)= 1. In fact, since u and v are real, Eq. (14.23) is identical
to (20.11) if we set w = 1 and
</p>
<p>Q[u,v] = p2vu&prime; &minus; (p2v)&prime;u+ p1uv. (20.23)
</p>
<p>Also, we have seen that any SOLDO can be made (formally) self-adjoint.
Thus, let us consider the formally self-adjoint SOLDO
</p>
<p>Lx = L&dagger;x =
d
</p>
<p>dx
</p>
<p>(
p
</p>
<p>d
</p>
<p>dx
</p>
<p>)
+ q
</p>
<p>where both p(x) and q(x) are real functions and the inner product is defined
with weight w = 1. If we are interested in formally self-adjoint operators
with respect to a general weight w &gt; 0, we can construct them as follows.
We first note that if Lx is formally self-adjoint with respect to a weight of
unity, then (1/w)Lx is self-adjoint with respect to weight w. Next, we note
that Lx is formally self-adjoint for all functions q , in particular, for wq . Now
we define
</p>
<p>L(w)x =
d
</p>
<p>dx
</p>
<p>(
p
</p>
<p>d
</p>
<p>dx
</p>
<p>)
+ qw
</p>
<p>and note that L(w)x is formally self-adjoint with respect to a weight of unity,
and therefore
</p>
<p>Lx &equiv;
1
</p>
<p>w
L(w)x =
</p>
<p>1
</p>
<p>w
</p>
<p>d
</p>
<p>dx
</p>
<p>(
p
</p>
<p>d
</p>
<p>dx
</p>
<p>)
+ q (20.24)
</p>
<p>is formally self-adjoint with respect to weight w(x) &gt; 0.
For SOLDOs that are formally self-adjoint with respect to weight w, the
</p>
<p>conjunct given in (20.23) becomes
</p>
<p>Q[u,v] = p(x)w(x)
(
vu&prime; &minus; uv&prime;
</p>
<p>)
. (20.25)</p>
<p/>
</div>
<div class="page"><p/>
<p>20.3 Green&rsquo;s Functions for SOLDOs 617
</p>
<p>Thus, the surface term in the generalized Green&rsquo;s identity vanishes if and
only if
</p>
<p>p(b)w(b)
[
v(b)u&prime;(b)&minus; u(b)v&prime;(b)
</p>
<p>]
</p>
<p>= p(a)w(a)
[
v(a)u&prime;(a)&minus; u(a)v&prime;(a)
</p>
<p>]
. (20.26)
</p>
<p>The DO becomes self-adjoint if u and v satisfy Eq. (20.26) as well as the common types of
boundary conditions for
</p>
<p>a SOLDE
</p>
<p>same BCs. It can easily be shown that the following four types of BCs on
u(x) assure the validity of Eq. (20.26) and therefore define a self-adjoint
operator Lx given by (20.24):
</p>
<p>1. The Dirichlet BCs: u(a)= u(b)= 0
2. The Neumann BCs: u&prime;(a)= u&prime;(b)= 0
3. General unmixed BCs: αu(a)&minus; u&prime;(a)= βu(b)&minus; u&prime;(b)= 0
4. Periodic BCs: u(a)= u(b) and u&prime;(a)= u&prime;(b)
</p>
<p>20.3 Green&rsquo;s Functions for SOLDOs
</p>
<p>We are now in a position to find the Green&rsquo;s function for a SOLDO. First,
note that a complete specification of Lx requires not only knowledge of
p0(x), p1(x), and p2(x)&mdash;its coefficient functions&mdash;but also knowledge of
the BCs imposed on the solutions. The most general BCs for a SOLDO are
of the type given in Eq. (20.10) with n= 2. Thus, to specify Lx uniquely, we
consider the system (L;R1,R2) with data (f ;γ1, γ2). This system defines a
unique BVP:
</p>
<p>Lx[u] = p2(x)
d2u
</p>
<p>dx2
+ p1(x)
</p>
<p>du
</p>
<p>dx
+ p0(x)u= f (x),
</p>
<p>Ri[u] = γi, i = 1,2.
(20.27)
</p>
<p>A necessary condition for Lx to be invertible is that the homogeneous DE
Lx[u] = 0 have only the trivial solution u= 0. For u= 0 to be the only solu-
tion, it must be a solution. This means that it must meet all the conditions in
Eq. (20.27). In particular, since Ri are linear functionals of u, we must have
Ri[0] = 0. This can be stated as follows:
</p>
<p>Lemma 20.3.1 A necessary condition for a second-order linear DO to be
invertible is for its associated BCs to be homogeneous.3
</p>
<p>Thus, to study Green&rsquo;s functions we must restrict ourselves to problems
with homogeneous BCs. This at first may seem restrictive, since not all prob-
lems have homogeneous BCs. Can we solve the others by the Green&rsquo;s func-
tion method? The answer is yes, as will be shown later in this chapter.
</p>
<p>The above discussion clearly indicates that the Green&rsquo;s function of Lx ,
being its &ldquo;inverse&rdquo;, is defined only if we consider the system (L;R1,R2)
with data (f ;0,0). If the Green&rsquo;s function exists, it must satisfy the DE of
</p>
<p>3The lemma applies to all linear DOs, not just second order ones.</p>
<p/>
</div>
<div class="page"><p/>
<p>618 20 Green&rsquo;s Functions in One Dimension
</p>
<p>Theorem 20.2.3, in which Lx acts on G(x,y). But part of the definition of
Lx are the BCs imposed on the solutions. Thus, if the LHS of the DE is to
make any sense, G(x,y) must also satisfy those same BCs. We therefore
make the following definition:formal definition of
</p>
<p>Green&rsquo;s function
</p>
<p>Definition 20.3.2 The Green&rsquo;s function of a DO Lx is a function
G(x,y) that satisfies both the DE
</p>
<p>LxG(x,y)=
δ(x &minus; y)
w(x)
</p>
<p>and, as a function of x, the homogeneous BCs Ri[G] = 0 for i = 1,2
where the Ri are defined as in Eq. (20.12).
</p>
<p>It is convenient to study the Green&rsquo;s function for the adjoint of Lx simul-
taneously. Denoting this by g(x, y), we have
</p>
<p>L&dagger;xg(x, y)=
δ(x &minus; y)
w(x)
</p>
<p>, Bi[g] = 0, for i = 1,2, (20.28)
</p>
<p>where Bi are the boundary functionals adjoint to Ri and given in Eq. (20.18).
The function g(x, y) is known as the adjoint Green&rsquo;s function associatedadjoint Green&rsquo;s function
with the DE of (20.27).
</p>
<p>We can now use (20.27) and (20.28) to find the solutions to
</p>
<p>Lx[u] = f (x), Ri[u] = 0 for i = 1,2,
</p>
<p>L&dagger;x[v] = h(x), Bi
[
v&lowast;
]
= 0 for i = 1,2.
</p>
<p>(20.29)
</p>
<p>With v(x) = g(x, y) in Eq. (20.11)&mdash;whose RHS is assumed to be zero&mdash;
we get
</p>
<p>&acute; b
</p>
<p>a
wg&lowast;(x, y)Lx[u]dx =
</p>
<p>&acute; b
</p>
<p>a
wu(x)(L&dagger;x[g])&lowast;dx. Using (20.28) on the
</p>
<p>RHS and (20.29) on the LHS, we obtain
</p>
<p>u(y)=
ˆ b
</p>
<p>a
</p>
<p>g&lowast;(x, y)w(x)f (x) dx.
</p>
<p>Similarly, with u(x)=G(x,y), Eq. (20.11) gives
</p>
<p>v&lowast;(y)=
ˆ b
</p>
<p>a
</p>
<p>G(x,y)w(x)h&lowast;(x) dx,
</p>
<p>or, since w(x) is a (positive) real function,
</p>
<p>v(y)=
ˆ b
</p>
<p>a
</p>
<p>G&lowast;(x, y)w(x)h(x) dx.
</p>
<p>These equations for u(y) and v(y) are not what we expect [see, for in-
stance, Eq. (20.1)]. However, if we take into account certain properties of
Green&rsquo;s functions that we will discuss next, these equations become plausi-
ble.</p>
<p/>
</div>
<div class="page"><p/>
<p>20.3 Green&rsquo;s Functions for SOLDOs 619
</p>
<p>20.3.1 Properties of Green&rsquo;s Functions
</p>
<p>Let us rewrite the generalized Green&rsquo;s identity [Eq. (20.11)], with the RHS
equal to zero, as
</p>
<p>ˆ b
</p>
<p>a
</p>
<p>dtw(t)
{
v&lowast;(t)
</p>
<p>(
Lt [u]
</p>
<p>)}
=
ˆ b
</p>
<p>a
</p>
<p>dtw(t)
{
u(t)
</p>
<p>(
L
</p>
<p>&dagger;
t [v]
</p>
<p>)&lowast;}
. (20.30)
</p>
<p>This is sometimes called Green&rsquo;s identity. Substituting G(t, y) for u(t) and Green&rsquo;s identity
g(t, x) for v(t) gives
</p>
<p>ˆ b
</p>
<p>a
</p>
<p>dtw(t)g&lowast;(t, x)
δ(t &minus; y)
w(t)
</p>
<p>=
ˆ b
</p>
<p>a
</p>
<p>dtw(t)G(t, y)
δ(t &minus; x)
w(t)
</p>
<p>,
</p>
<p>or g&lowast;(y, x)=G(x,y). A consequence of this identity is
</p>
<p>Proposition 20.3.3 G(x,y) must satisfy the adjoint boundary conditions
with respect to its second argument.
</p>
<p>If for the time being we assume that the Green&rsquo;s function associated
with a system (L;R1,R2) is unique, then, since for a self-adjoint differ-
ential operator, Lx and L&dagger;x are identical and u and v both satisfy the same
BCs, we must have G(x,y)= g(x, y) or, using g&lowast;(y, x)=G(x,y), we get
G(x,y) = G&lowast;(y, x). In particular, if the coefficient functions of Lx are all
real, G(x,y) will be real, and we have G(x,y)=G(y,x). We thus have
</p>
<p>Proposition 20.3.4 The Green&rsquo;s function is a symmetric function of
its two arguments: G(x,y)=G(y,x).
</p>
<p>The last property is related to the continuity of G(x,y) and its derivative
at x = y. For a SOLDO, we have
</p>
<p>LxG(x,y)= p2(x)
&part;2G
</p>
<p>&part;x2
+ p1(x)
</p>
<p>&part;G
</p>
<p>&part;x
+ p0(x)G=
</p>
<p>δ(x &minus; y)
w(x)
</p>
<p>,
</p>
<p>where p0, p1, and p2 are assumed to be real and continuous in the interval
[a, b], and w(x) and p2(x) are assumed to be positive for all x &isin; [a, b]. We
multiply both sides of the DE by
</p>
<p>h(x)= μ(x)
p2(x)
</p>
<p>, where μ(x)&equiv; exp
[
ˆ x
</p>
<p>a
</p>
<p>p1(t)
</p>
<p>p2(t)
dt
</p>
<p>]
,
</p>
<p>noting that dμ/dx = (p1/p2)μ. This transforms the DE into
&part;
</p>
<p>&part;x
</p>
<p>[
μ(x)
</p>
<p>&part;
</p>
<p>&part;x
G(x, y)
</p>
<p>]
+ p0(x)μ(x)
</p>
<p>p2(x)
G(x, y)= μ(y)
</p>
<p>p2(y)w(y)
δ(x &minus; y).
</p>
<p>Integrating this equation gives
</p>
<p>μ(x)
&part;
</p>
<p>&part;x
G(x, y)+
</p>
<p>ˆ x
</p>
<p>a
</p>
<p>p0(t)μ(t)
</p>
<p>p2(t)
G(t, y) dt = μ(y)
</p>
<p>p2(y)w(y)
θ(x &minus; y)+ α(y)
</p>
<p>(20.31)</p>
<p/>
</div>
<div class="page"><p/>
<p>620 20 Green&rsquo;s Functions in One Dimension
</p>
<p>because the primitive of δ(x &minus; y) is θ(x &minus; y). Here α(y) is the &ldquo;constant&rdquo;
of integration. First consider the case where p0 = 0, for which the Green&rsquo;s
function will be denoted by G0(x, y). Then Eq. (20.31) becomes
</p>
<p>μ(x)
&part;
</p>
<p>&part;x
G0(x, y)=
</p>
<p>μ(y)
</p>
<p>p2(y)w(y)
θ(x &minus; y)+ α1(y),
</p>
<p>which (since μ, p2, and w are continuous on [a, b], and θ(x &minus; y) has a dis-
continuity only at x = y) indicates that &part;G0/&part;x is continuous everywhere
on [a, b] except at x = y. Now divide the last equation by μ and integrate
the result to get
</p>
<p>G0(x, y)=
μ(y)
</p>
<p>p2(y)w(y)
</p>
<p>ˆ x
</p>
<p>a
</p>
<p>θ(t &minus; y)
μ(t)
</p>
<p>dt + α1(y)
ˆ x
</p>
<p>a
</p>
<p>dt
</p>
<p>μ(t)
+ α2(y).
</p>
<p>Every term on the RHS is continuous except possibly the integral involving
the θ -function. However, that integral can be written as
</p>
<p>ˆ x
</p>
<p>a
</p>
<p>θ(t &minus; y)
μ(t)
</p>
<p>dt = θ(x &minus; y)
ˆ x
</p>
<p>y
</p>
<p>dt
</p>
<p>μ(t)
. (20.32)
</p>
<p>The θ -function in front of the integral is needed to ensure that a &le; y &le; x as
demanded by the LHS of Eq. (20.32). The RHS of Eq. (20.32) is continuous
at x = y with limit being zero as x &rarr; y.
</p>
<p>Next, we write G(x,y)=G0(x, y)+H(x,y), and apply Lx to both sides.
This gives
</p>
<p>δ(x &minus; y)
w(x)
</p>
<p>=
(
p2
</p>
<p>d2
</p>
<p>dx2
+ p1
</p>
<p>d
</p>
<p>dx
</p>
<p>)
G0 + p0G0 + LxH(x,y)
</p>
<p>= δ(x &minus; y)
w(x)
</p>
<p>+ p0G0 + LxH(x,y),
</p>
<p>or p2H &prime;&prime; + p1H &prime; + p0H =&minus;p0G0. The continuity of G0, p0, p1, and p2
on [a, b] implies the continuity of H , because a discontinuity in H would
entail a delta function discontinuity in dH/dx, which is impossible because
there are no delta functions in the equation for H . Since both G0 and H are
continuous, G must also be continuous on [a, b].
</p>
<p>We can now calculate the jump in &part;G/&part;x at x = y. We denote the jump
as �G&prime;(y) and define it as follows:
</p>
<p>�G&prime;(y)&equiv; lim
ǫ&rarr;0
</p>
<p>[
&part;G
</p>
<p>&part;x
(x, y)
</p>
<p>∣∣∣∣
x=y+ǫ
</p>
<p>&minus; &part;G
&part;x
</p>
<p>(x, y)
</p>
<p>∣∣∣∣
x=y&minus;ǫ
</p>
<p>]
.
</p>
<p>Dividing (20.31) by μ(x) and taking the above limit for all terms, we obtain
</p>
<p>�G&prime;(y)+ lim
ǫ&rarr;0
</p>
<p>[
1
</p>
<p>μ(y + ǫ)
</p>
<p>ˆ y+ǫ
</p>
<p>a
</p>
<p>p0(t)μ(t)
</p>
<p>p2(t)
G(t, y) dt
</p>
<p>&minus; 1
μ(y &minus; ǫ)
</p>
<p>ˆ y&minus;ǫ
</p>
<p>a
</p>
<p>p0(t)μ(t)
</p>
<p>p2(t)
G(t, y) dt
</p>
<p>]</p>
<p/>
</div>
<div class="page"><p/>
<p>20.3 Green&rsquo;s Functions for SOLDOs 621
</p>
<p>= μ(y)
p2(y)w(y)
</p>
<p>lim
ǫ&rarr;0
</p>
<p>[
=1︷ ︸︸ ︷
</p>
<p>θ(+ǫ)
μ(y + ǫ) &minus;
</p>
<p>=0︷ ︸︸ ︷
θ(&minus;ǫ)
</p>
<p>μ(y &minus; ǫ)
</p>
<p>]
.
</p>
<p>The second term on the LHS is zero because all functions are continuous at
y. The limit on the RHS is simply 1/μ(y). We therefore obtain
</p>
<p>�G&prime;(y)= 1
p2(y)w(y)
</p>
<p>. (20.33)
</p>
<p>20.3.2 Construction and Uniqueness of Green&rsquo;s Functions
</p>
<p>We are now in a position to calculate the Green&rsquo;s function for a general
SOLDO and show that it is unique.
</p>
<p>Theorem 20.3.5 Consider the system (L;R1,R2) with data (f ;0,0),
in which Lx is a SOLDO. If the homogeneous DE Lx[u] = 0 has
no nontrivial solution, then the GF associated with the given sys-
tem exists and is unique. The solution of the system is u(x) =
&acute; b
</p>
<p>a
dyw(y)G(x, y)f (y) and is also unique.
</p>
<p>Proof The GF satisfies the DE LxG(x,y) = 0 for all x &isin; [a, b] except existence and
uniqueness of GF for a
</p>
<p>second order linear
</p>
<p>differential operator
</p>
<p>x = y. We thus divide [a, b] into two intervals, I1 = [a, y) and I2 = (y, b],
and note that a general solution to the above homogeneous DE can be writ-
ten as a linear combination of a basis of solutions, u1 and u2. Thus, we can
write the solution of the DE as
</p>
<p>Gl(x, y)= c1u1(x)+ c2u2(x) for x &isin; I1
Gr(x, y)= d1u1(x)+ d2u2(x) for x &isin; I2
</p>
<p>and define the GF as
</p>
<p>G(x,y)=
{
Gl(x, y) if x &isin; I1,
Gr(x, y) if x &isin; I2,
</p>
<p>(20.34)
</p>
<p>where c1, c2, d1, and d2 are, in general, functions of y. To determine G(x,y)
we must determine four unknowns. We also have four relations: the continu-
ity of G, the jump in &part;G/&part;x at x = y, and the two BCs R1[G] = R2[G] = 0.
The continuity of G gives
</p>
<p>c1(y)u1(y)+ c2(y)u2(y)= d1(y)u1(y)+ d2(y)u2(y).
</p>
<p>The jump of &part;G/&part;x at x = y yields
</p>
<p>c1(y)u
&prime;
1(y)+ c2(y)u&prime;2(y)&minus; d1(y)u&prime;1(y)&minus; d2(y)u&prime;2(y)=&minus;
</p>
<p>1
</p>
<p>p2(y)w(y)
.</p>
<p/>
</div>
<div class="page"><p/>
<p>622 20 Green&rsquo;s Functions in One Dimension
</p>
<p>Introducing b1 = c1 &minus; d1 and b2 = c2 &minus; d2 changes the two preceding equa-
tions to
</p>
<p>b1u1 + b2u2 = 0,
</p>
<p>b1u
&prime;
1 + b2u&prime;2 =&minus;
</p>
<p>1
</p>
<p>p2w
.
</p>
<p>These equations have a unique solution iff
</p>
<p>det
</p>
<p>(
u1 u2
u&prime;1 u
</p>
<p>&prime;
2
</p>
<p>)
�= 0.
</p>
<p>But the determinant is simply the Wronskian of the two independent solu-
tions and therefore cannot be zero. Thus, b1(y) and b2(y) are determined in
terms of u1, u&prime;1, u2, u
</p>
<p>&prime;
2, p2, and w.
</p>
<p>We now define
</p>
<p>h(x, y)&equiv;
{
b1(y)u1(x)+ b2(y)u2(x) if x &isin; I1,
0 if x &isin; I2.
</p>
<p>so that G(x,y)= h(x, y)+ d1(y)u1(x)+ d2(y)u2(x). We have reduced the
number of unknowns to two, d1 and d2. Imposing the BCs gives two more
relations:
</p>
<p>R1[G] = R1[h] + d1R1[u1] + d2R1[u2] = 0,
R2[G] = R2[h] + d1R2[u1] + d2R2[u2] = 0.
</p>
<p>Can we solve these equations and determine d1 and d2 uniquely? We can, if
</p>
<p>det
</p>
<p>(
R1[u1] R1[u2]
R2[u1] R2[u2]
</p>
<p>)
�= 0.
</p>
<p>It can be shown that this determinant is nonzero (see Problem 20.5).
Having found the unique {bi, di}2i=1, we can calculate ci uniquely, sub-
</p>
<p>stitute all of them in Eq. (20.34), and obtain the unique G(x,y). That u(x)
is also unique can be shown similarly. �
</p>
<p>Example 20.3.6 Let us calculate the GF for Lx = d2/dx2 with BCs u(a)=
u(b) = 0. We note that Lx[u] = 0 with the given BCs has no nontrivial so-
lution (verify this). Thus, the GF exists. The DE for G(x,y) is G&prime;&prime; = 0 for
x �= y, whose solutions are
</p>
<p>G(x,y)=
{
c1x + c2 if a &le; x &lt; y,
d1x + d2 if y &lt; x &le; b.
</p>
<p>(20.35)
</p>
<p>Continuity at x = y gives c1y + c2 = d1y + d2 or b1y + b2 = 0 with bi =
ci &minus; di . The discontinuity of dG/dx at x = y gives
</p>
<p>d1 &minus; c1 =
1
</p>
<p>p2w
= 1 &rArr; b1 =&minus;1</p>
<p/>
</div>
<div class="page"><p/>
<p>20.3 Green&rsquo;s Functions for SOLDOs 623
</p>
<p>assuming that w = 1. From the equations above we also get b2 = y. G(x,y)
must also satisfy the given BCs. Thus, G(a,y)= 0 =G(b,y). Since a &le; y
and b &ge; y, we obtain c1a + c2 = 0 and d1b + d2 = 0, or, after substituting
ci = bi + di ,
</p>
<p>ad1 + d2 = a &minus; y, bd1 + d2 = 0.
The solution to these equations is d1 = (y &minus; a)/(b &minus; a) and d2 = &minus;b(y &minus;
a)/(b&minus; a). With b1, b2, d1, and d2 as given above, we find
</p>
<p>c1 = b1 + d1 =&minus;
b&minus; y
b&minus; a and c2 = b2 + d2 = a
</p>
<p>b&minus; y
b&minus; a .
</p>
<p>Writing Eq. (20.35) as
</p>
<p>G(x,y)= (c1x + c2)θ(y &minus; x)+ (d1x + d2)θ(x &minus; y)
</p>
<p>and using the identity θ(y &minus; x)= 1 &minus; θ(x &minus; y), we get
</p>
<p>G(x,y)= c1x + c2 &minus; (b1x + b2)θ(x &minus; y).
</p>
<p>Using the values found for the b&rsquo;s and c&rsquo;s, we obtain
</p>
<p>G(x,y)= (a &minus; x)
(
b&minus; y
b&minus; a
</p>
<p>)
+ (x &minus; y)θ(x &minus; y),
</p>
<p>which is the same as the GF obtained in Example 20.1.4.
</p>
<p>Example 20.3.7 Let us find the GF for Lx = d2/dx2 + 1 with the BCs
u(0)= u(π/2)= 0. The general solution of Lx[u] = 0 is
</p>
<p>u(x)=A sinx +B cosx.
</p>
<p>If the BCs are imposed, we get u = 0. Thus, G(x,y) exists. The general
form of G(x,y) is
</p>
<p>G(x,y)=
{
c1 sinx + c2 cosx if 0 &le; x &lt; y,
d1 sinx + d2 cosx if y &lt; x &le; π/2.
</p>
<p>(20.36)
</p>
<p>Continuity of G at x = y gives b1 siny + b2 cosy = 0 with bi = ci &minus; di .
The discontinuity of the derivative of G at x = y gives b1 cosy &minus; b2 siny =
&minus;1, where we have set w(x) = 1. Solving these equations yields b1 =
&minus; cosy and b2 = siny. The BCs give
</p>
<p>G(0, y)= 0 &rArr; c2 = 0 &rArr; d2 =&minus;b2 =&minus;siny,
G(π/2, y)= 0 &rArr; d1 = 0 &rArr; c1 =&minus;b1 =&minus;cosy.
</p>
<p>Substituting in Eq. (20.36) gives
</p>
<p>G(x,y)=
{
&minus;cosy sinx if x &lt; y,
&minus;siny cosx if y &lt; x,</p>
<p/>
</div>
<div class="page"><p/>
<p>624 20 Green&rsquo;s Functions in One Dimension
</p>
<p>or, using the theta function,
</p>
<p>G(x,y)=&minus;θ(y &minus; x) cosy sinx &minus; θ(y &minus; x) siny cosx
=&minus;
</p>
<p>[
1 &minus; θ(x &minus; y)
</p>
<p>]
cosy sinx &minus; θ(x &minus; y) siny cosx
</p>
<p>=&minus;cosy sinx + θ(x &minus; y) sin(x &minus; y).
</p>
<p>It is instructive to verify directly that G(x,y) satisfies Lx[G] = δ(x&minus; y):
</p>
<p>Lx[G] = &minus;cosy
(
</p>
<p>d2
</p>
<p>dx2
+ 1
</p>
<p>)
sinx
</p>
<p>︸ ︷︷ ︸
=0
</p>
<p>+
(
</p>
<p>d2
</p>
<p>dx2
+ 1
</p>
<p>)[
θ(x &minus; y) sin(x &minus; y)
</p>
<p>]
</p>
<p>= d
2
</p>
<p>dx2
</p>
<p>[
θ(x &minus; y) sin(x &minus; y)
</p>
<p>]
+ θ(x &minus; y) sin(x &minus; y)
</p>
<p>= d
dx
</p>
<p>[
δ(x &minus; y) sin(x &minus; y)︸ ︷︷ ︸
</p>
<p>=0
</p>
<p>+θ(x &minus; y) cos(x &minus; y)
]
</p>
<p>+ θ(x &minus; y) sin(x &minus; y).
</p>
<p>The first term vanishes because the sine vanishes at the only point where the
delta function is nonzero. Thus, we have
</p>
<p>Lx[G] =
[
δ(x &minus; y) cos(x &minus; y)&minus; θ(x &minus; y) sin(x &minus; y)
</p>
<p>]
</p>
<p>+ θ(x &minus; y) sin(x &minus; y)
= δ(x &minus; y)
</p>
<p>because the delta function demands that x = y, for which cos(x &minus; y)= 1.
</p>
<p>The existence and uniqueness of the Green&rsquo;s function G(x,y) in con-
junction with its properties and its adjoint, imply the existence and unique-
ness of the adjoint Green&rsquo;s function g(x, y). Using this fact, we can show
that the condition for the absence of a nontrivial solution for Lx[u] = 0 is
also a necessary condition for the existence of G(x,y). That is, if G(x,y)
exists, then Lx[u] = 0 implies that u = 0. Suppose G(x,y) exists; then
g(x, y) also exists. In Green&rsquo;s identity let v = g(x, y). This gives an iden-
tity:
</p>
<p>ˆ b
</p>
<p>a
</p>
<p>w(x)g&lowast;(x, y)
(
Lx[u]
</p>
<p>)
dx =
</p>
<p>ˆ b
</p>
<p>a
</p>
<p>w(x)u(x)
(
L&dagger;x[g]
</p>
<p>)&lowast;
dx
</p>
<p>=
ˆ b
</p>
<p>a
</p>
<p>w(x)u(x)
δ(x &minus; y)
w(x)
</p>
<p>dx = u(y).
</p>
<p>In particular, if Lx[u] = 0, then u(y) = 0 for all y. We have proved the
following result.
</p>
<p>Proposition 20.3.8 The DE Lx[u] = 0 implies that u&equiv; 0 if and only
if the GF corresponding to Lx and the homogeneous BCs exist.</p>
<p/>
</div>
<div class="page"><p/>
<p>20.3 Green&rsquo;s Functions for SOLDOs 625
</p>
<p>It is sometimes stated that the Green&rsquo;s function of a SOLDO with con-
stant coefficients depends on the difference x &minus; y. This statement is moti-
vated by the observation that if u(x) is a solution of
</p>
<p>Lx[u] = a2
d2u
</p>
<p>dx2
+ a1
</p>
<p>du
</p>
<p>dx
+ a0u= f (x),
</p>
<p>then u(x&minus; y) is the solution of a2u&prime;&prime;+ a1u&prime;+ a0u= f (x&minus; y) if a0, a1, and
a2 are constant. Thus, if G(x) is a solution of Lx[G] = δ(x) [again assuming
that w(x)= 1], then it seems that the solution of Lx[G] = δ(x &minus; y) is sim-
ply G(x&minus;y). This is clearly wrong, as Examples 20.3.6 and 20.3.7 showed.
The reason is, of course, the BCs. The fact that G(x &minus; y) satisfies the right
DE does not guarantee that it also satisfies the right BCs. The following ex-
ample, however, shows that the conjecture is true for a homogeneous initial
value problem.
</p>
<p>Example 20.3.9 The most general form for the GF is
</p>
<p>G(x,y)=
{
c1u1(x)+ c2u2(x) if a &le; x &lt; y,
d1u1(x)+ d2u2(x) if y &lt; x &le; b.
</p>
<p>The IVP condition G(a,y)= 0 =G&prime;(a, y) implies
</p>
<p>c1u1(a)+ c2u2(a)= 0 and c1u&prime;1(a)+ c2u&prime;2(a)= 0.
</p>
<p>Linear independence of u1 and u2 implies
</p>
<p>det
</p>
<p>(
u1(a) u2(a)
</p>
<p>u&prime;1(a) u
&prime;
2(a)
</p>
<p>)
=W(a;u1, u2) �= 0.
</p>
<p>Hence, c1 = c2 = 0 is the only solution. This gives
</p>
<p>G(x,y)=
{
</p>
<p>0 if a &le; x &lt; y,
d1u1(x)+ d2u2(x) if y &lt; x &le; b.
</p>
<p>(20.37)
</p>
<p>Continuity of G at x = y yields d1u1(y)+ d2u2(y)= 0, while the disconti-
nuity jump condition in the derivative gives d1u&prime;1(y)+d2u&prime;2(y)= 1. Solving
these two equations, we get
</p>
<p>d1 =
u2(y)
</p>
<p>u&prime;1(y)u2(y)&minus; u&prime;2(y)u1(y)
, d2 =&minus;
</p>
<p>u1(y)
</p>
<p>u&prime;1(y)u2(y)&minus; u&prime;2(y)u1(y)
.
</p>
<p>Substituting this in (20.37) gives
</p>
<p>G(x,y)=
[
u2(y)u1(x)&minus; u1(y)u2(x)
u&prime;1(y)u2(y)&minus; u&prime;2(y)u1(y)
</p>
<p>]
θ(x &minus; y). (20.38)
</p>
<p>Equation (20.38) holds for any SOLDO with the given BCs. We now use
the fact that the SOLDO has constant coefficients. In that case, we know the
exact form of u1 and u2. There are two cases to consider:</p>
<p/>
</div>
<div class="page"><p/>
<p>626 20 Green&rsquo;s Functions in One Dimension
</p>
<p>1. If the characteristic polynomial of Lx has two distinct roots λ1 and λ2,
then u1(x) = eλ1x and u2(x) = eλ2x . Writing λ1 = a + b and λ2 =
a&minus;b and substituting the exponential functions and their derivatives in
Eq. (20.38) yields
</p>
<p>G(x,y)=
[
e(a&minus;b)ye(a+b)x &minus; e(a+b)ye(a&minus;b)x
</p>
<p>2be2ay
</p>
<p>]
θ(x &minus; y)
</p>
<p>= 1
2b
</p>
<p>[
e(a+b)(x&minus;y) &minus; e(a&minus;b)(x&minus;y)
</p>
<p>]
θ(x &minus; y),
</p>
<p>which is a function of x &minus; y alone.
2. If λ1 = λ2 = λ, then u1(x) = eλx , u2(x) = xeλx , and substitution of
</p>
<p>these functions in Eq. (20.38) gives
</p>
<p>G(x,y)= (x &minus; y)eλ(x&minus;y)θ(x &minus; y).
</p>
<p>20.3.3 Inhomogeneous BCs
</p>
<p>So far we have concentrated on problems with homogeneous BCs, Ri[u] = 0,
for i = 1,2. What if the BCs are inhomogeneous? It turns out that the
Green&rsquo;s function method, even though it was derived for homogeneous
BCs, solves this kind of problem as well! The secret of this success isGF solves
</p>
<p>inhomogeneous BCs as
</p>
<p>well
</p>
<p>the generalized Green&rsquo;s identity. Suppose we are interested in solving
the DE
</p>
<p>Lx[u] = f (x) with Ri[u] = γi for i = 1,2,
and we have the GF for Lx (with homogeneous BCs, of course). We can
substitute v = g(x, y) = G&lowast;(y, x) in the generalized Green&rsquo;s identity and
use the DE to obtain
</p>
<p>ˆ b
</p>
<p>a
</p>
<p>w(x)G(y, x)f (x) dx &minus;
ˆ b
</p>
<p>a
</p>
<p>w(x)u(x)
(
L&dagger;x[g]
</p>
<p>)&lowast;
dx
</p>
<p>=Q
[
u,g&lowast;(x, y)
</p>
<p>]∣∣x=b
x=a,
</p>
<p>or, using L&dagger;x[g(x, y)] = δ(x &minus; y)/w(y),
</p>
<p>u(y)=
ˆ b
</p>
<p>a
</p>
<p>w(x)G(y, x)f (x) dx &minus;Q
[
u,g&lowast;(x, y)
</p>
<p>]∣∣x=b
x=a .
</p>
<p>To evaluate the surface term, let us write the BCs in matrix form [see
Eq. (20.17)]:
</p>
<p>Aua + Bub = γ &rArr; ub = B&minus;1γ &minus; B&minus;1Aua,
</p>
<p>AGa + BGb = 0 &rArr; At
(
Bt
)&minus;1
</p>
<p>Qbg
&lowast;
b + Qag&lowast;a = 0,</p>
<p/>
</div>
<div class="page"><p/>
<p>20.3 Green&rsquo;s Functions for SOLDOs 627
</p>
<p>where γ is a column vector composed of γ1 and γ2, and we have assumed
that G(x,y) and g&lowast;(x, y) satisfy, respectively, the homogeneous BCs (with
γ = 0) and their adjoints. We have also assumed that the 2 &times; 4 matrix of
coefficients has rank 2, and without loss of generality, let B be the invertible
2 &times; 2 submatrix. Then, assuming the general form of the surface term as in
Eq. (20.15), we obtain
</p>
<p>Q
[
u,g&lowast;(x, y)
</p>
<p>]∣∣x=b
x=a = u
</p>
<p>t
bQbg
</p>
<p>&lowast;
b &minus; utaQag&lowast;a
</p>
<p>=
(
B&minus;1γ &minus; B&minus;1Aua
</p>
<p>)t
Qbg
</p>
<p>&lowast;
b &minus; utaQag&lowast;a
</p>
<p>= γ t
(
Bt
)&minus;1
</p>
<p>Qbg
&lowast;
b &minus; uta
</p>
<p>[
At
(
Bt
)&minus;1
</p>
<p>Qbg
&lowast;
b + Qag&lowast;a
</p>
<p>]
︸ ︷︷ ︸
</p>
<p>= 0 because g&lowast; satisfies
homogeneous adjoint BC
</p>
<p>= γ t
(
Bt
)&minus;1
</p>
<p>Qbg
&lowast;
b, (20.39)
</p>
<p>where
</p>
<p>g&lowast;b &equiv;
(
</p>
<p>g&lowast;(b, y)
&part;
&part;x
g&lowast;(x, y)|x=b
</p>
<p>)
=
(
</p>
<p>G(y,b)
&part;
&part;x
G(y,x)|x=b
</p>
<p>)
.
</p>
<p>It follows that Q[u,g&lowast;(x, y)]|x=bx=a is given entirely in terms of G, its deriva-
tive, the coefficient functions of the DE (hidden in the matrix Q), the homo-
geneous BCs (hidden in B), and the constants γ1 and γ2. The fact that g&lowast;
</p>
<p>and &part;g&lowast;/&part;x appear to be evaluated at x = b is due to the simplifying (but
harmless) assumption that B is invertible, i.e., that u(b) and u&prime;(b) can be
written in terms of u(a) and u&prime;(a). Of course, this may not be possible; then
we have to find another pair of the four quantities in terms of the other two,
in which case the matrices and the vectors will change but the argument, as
well as the conclusion, will remain valid. We can now write
</p>
<p>u(y)=
ˆ b
</p>
<p>a
</p>
<p>w(x)G(y, x)f (x) dx &minus; γ tMg&lowast;, (20.40)
</p>
<p>where a general matrix M has been introduced, and the subscript b has been
removed to encompass cases where submatrices other than B are invertible.
Equation (20.40) shows that u can be determined completely once we know
G(x,y), even though the BCs are inhomogeneous. In practice, there is no
need to calculate M. We can use the expression for Q[u,g&lowast;] obtained from
the Lagrange identity of Chap. 14 and evaluate it at b and a. This, in gen-
eral, involves evaluating u and G and their derivatives at a and b. We know
how to handle the evaluation of G because we can actually construct it (if it
exists). We next find two of the four quantities corresponding to u in terms
of the other two and insert the result in the expression for Q[u,g&lowast;]. Equa-
tion (20.39) then guarantees that the coefficients of the other two terms will
be zero. Thus, we can simply drop all the terms in Q[u,g&lowast;] containing a
factor of the other two terms.</p>
<p/>
</div>
<div class="page"><p/>
<p>628 20 Green&rsquo;s Functions in One Dimension
</p>
<p>Specifically, we use the conjunct for a formally self-adjoint SOLDO [see
Eq. (20.26)] and g&lowast;(x, y)=G(y,x) to obtain
</p>
<p>u(y)=
ˆ b
</p>
<p>a
</p>
<p>w(x)G(y, x)f (x) dx
</p>
<p>&minus;
{
p(x)w(x)
</p>
<p>[
G(y,x)
</p>
<p>du
</p>
<p>dx
&minus; u(x)&part;G
</p>
<p>&part;x
(y, x)
</p>
<p>]}x=b
</p>
<p>x=a
.
</p>
<p>Interchanging x and y gives
</p>
<p>u(x)=
ˆ b
</p>
<p>a
</p>
<p>w(y)G(x, y)f (y) dy
</p>
<p>+
{
p(y)w(y)
</p>
<p>[
u(y)
</p>
<p>&part;G
</p>
<p>&part;y
(x, y)&minus;G(x,y)du
</p>
<p>dy
</p>
<p>]}y=b
</p>
<p>y=a
. (20.41)
</p>
<p>This equation is valid only for a self-adjoint SOLDO. That is, using it re-
quires casting the SOLDO into a self-adjoint form (a process that is always
possible, in light of Theorem 14.5.4).
</p>
<p>By setting f (x)= 0, we can also obtain the solution to a homogeneous
DE Lx[u] = 0 that satisfies the inhomogeneous BCs.
</p>
<p>Example 20.3.10 Let us find the solution of the simple DE d2u/dx2 =
f (x) subject to the simple inhomogeneous BCs u(a) = γ1 and u(b) = γ2.
The GF for this problem has been calculated in Examples 20.1.5 and 20.3.6.
Let us begin by calculating the surface term in Eq. (20.41). We have p(y)=
1, and we set w(y)= 1, then
</p>
<p>surface term = u(b)&part;G
&part;y
</p>
<p>∣∣∣∣
y=b
</p>
<p>&minus;G(x,b)u&prime;(b)&minus; u(a)&part;G
&part;y
</p>
<p>∣∣∣∣
y=a
</p>
<p>+G(x,a)u&prime;(a)
</p>
<p>= γ2
&part;G
</p>
<p>&part;y
</p>
<p>∣∣∣∣
y=b
</p>
<p>&minus; γ1
&part;G
</p>
<p>&part;y
</p>
<p>∣∣∣∣
y=a
</p>
<p>+G(x,a)u&prime;(a)&minus;G(x,b)u&prime;(b).
</p>
<p>That the unwanted (and unspecified) terms are zero can be seen by observ-
ing that G(x,a)= g&lowast;(a, x)= (g(a, x))&lowast;, and that g(x, y) satisfies the BCs
adjoint to the homogeneous BCs (obtained when γi = 0). In this particular
and simple case, the BCs happen to be self-adjoint (Dirichlet BCs). Thus,
u(a) = u(b) = 0 implies that g(a, x) = g(b, x) = 0 for all x &isin; [a, b]. (In a
more general case the coefficient of u&prime;(a) would be more complicated, but
still zero.) Thus, we finally have
</p>
<p>surface term = γ2
&part;G
</p>
<p>&part;y
</p>
<p>∣∣∣∣
y=b
</p>
<p>&minus; γ1
&part;G
</p>
<p>&part;y
</p>
<p>∣∣∣∣
y=a
</p>
<p>.</p>
<p/>
</div>
<div class="page"><p/>
<p>20.3 Green&rsquo;s Functions for SOLDOs 629
</p>
<p>Now, using the expression for G(x,y) obtained in Examples 20.1.5
and 20.3.6, we get
</p>
<p>&part;G
</p>
<p>&part;y
=&minus;a &minus; x
</p>
<p>b&minus; a &minus; θ(x &minus; y)&minus; (x &minus; y)δ(x &minus; y)︸ ︷︷ ︸
=0
</p>
<p>= x &minus; a
b&minus; a &minus; θ(x &minus; y).
</p>
<p>Thus,
</p>
<p>&part;G
</p>
<p>&part;y
</p>
<p>∣∣∣∣
y=b
</p>
<p>= x &minus; a
b&minus; a ,
</p>
<p>&part;G
</p>
<p>&part;y
</p>
<p>∣∣∣∣
y=a
</p>
<p>= x &minus; a
b&minus; a &minus; 1 =
</p>
<p>x &minus; b
b&minus; a .
</p>
<p>Substituting in Eq. (20.41), we get
</p>
<p>u(x)=
ˆ b
</p>
<p>a
</p>
<p>G(x,y)f (y) dy + γ2 &minus; γ1
b&minus; a x +
</p>
<p>bγ1 &minus; aγ2
b&minus; a .
</p>
<p>(Compare this with the result obtained in Example 20.1.5.)
</p>
<p>Green&rsquo;s functions have a very simple and enlightening physical interpre- relation of GF to sources
tation. An inhomogeneous DE such as Lx[u] = f (x) can be interpreted as a
black box (Lx ) that determines a physical quantity (u) when there is a source
(f ) of that physical quantity. For instance, electrostatic potential is a physi-
cal quantity whose source is charge; a magnetic field has an electric current
as its source; displacements and velocities have forces as their sources; and
so forth. Applying this interpretation and assuming that w(x)= 1, we have
G(x,y) as the physical quantity, evaluated at x when its source δ(x &minus; y) is
located at y. To be more precise, let us say that the strength of the source is
S1 and it is located at y1; then the source becomes S1δ(x&minus;y1). The physical
quantity, the Green&rsquo;s function, is then S1G(x,y1), because of the linearity
of Lx : If G(x,y) is a solution of Lx[u] = δ(x&minus;y), then S1G(x,y1) is a solu-
tion of Lx[u] = S1δ(x&minus;y). If there are many sources located at {yi}Ni=1 with
corresponding strengths {Si}Ni=1, then the overall source f as a function of x
becomes f (x)=&sum;Ni=1 Siδ(x&minus;yi), and the corresponding physical quantity
u(x) becomes u(x)=&sum;Ni=1 SiG(x,yi).
</p>
<p>Since the source Si is located at yi , it is more natural to define a func-
tion S(x) and write Si = S(yi). When the number of point sources goes
to infinity and yi becomes a smooth continuous variable, the sums become
integrals, and we have
</p>
<p>f (x)=
ˆ b
</p>
<p>a
</p>
<p>S(y)δ(x &minus; y)dy, u(x)=
ˆ b
</p>
<p>a
</p>
<p>S(y)G(x, y) dy.
</p>
<p>The first integral shows that S(x) = f (x). Thus, the second integral be-
comes u(x)=
</p>
<p>&acute; b
</p>
<p>a
f (y)G(x, y) dy which is precisely what we obtained for-
</p>
<p>mally.</p>
<p/>
</div>
<div class="page"><p/>
<p>630 20 Green&rsquo;s Functions in One Dimension
</p>
<p>20.4 Eigenfunction Expansion
</p>
<p>Green&rsquo;s functions are inverses of differential operators. Inverses of operators
in a Hilbert space are best studied in terms of resolvents. This is because if
an operator A has an inverse, zero is in its resolvent set, and
</p>
<p>R0(A)= Rλ(A)
∣∣
λ=0 = (A&minus; λ1)
</p>
<p>&minus;1∣∣
λ=0 = A
</p>
<p>&minus;1.
</p>
<p>Thus, it is instructive to discuss Green&rsquo;s functions in the context of the re-
solvent of a differential operator. We will consider only the case where the
eigenvalues are discrete, for example, when Lx is a Sturm-Liouville opera-
tor.
</p>
<p>Formally, we have (L&minus; λ1)Rλ(L)= 1, which leads to the DE
</p>
<p>(Lx &minus; λ)Rλ(x, y)=
δ(x &minus; y)
w(x)
</p>
<p>,
</p>
<p>where Rλ(x, y) = 〈x|Rλ(L)|y〉. The DE simply says that Rλ(x, y) is the
Green&rsquo;s function for the operator Lx &minus; λ. So we can rewrite the equation as
</p>
<p>(Lx &minus; λ)Gλ(x, y)=
δ(x &minus; y)
w(x)
</p>
<p>,
</p>
<p>where Lx &minus; λ is a DO having some homogeneous BCs. The GF Gλ(x, y)
exists if and only if (Lx &minus;λ)[u] = 0 has no nontrivial solution, which is true
only if λ is not an eigenvalue of Lx . We choose the BCs in such a way that
Lx becomes self-adjoint.
</p>
<p>Let {λn}&infin;n=1 be the eigenvalues of the system Lx[u] = λu, {Ri[u] = 0}2i=1,
and let the u(k)n (x) be the corresponding eigenfunctions. The index k dis-
tinguishes among the linearly independent vectors corresponding to the
same eigenvalue λn. Assuming that L has compact resolvent (e.g., a Sturm-
Liouville operator), these eigenfunctions form a complete set for the sub-
space of the Hilbert space that consists of those functions that satisfy the
same BCs as the u(k)n (x). In particular, Gλ(x, y) can be expanded in terms
of u(k)n (x). The expansion coefficients are, of course, functions of y. Thus,
we can write
</p>
<p>Gλ(x, y)=
&sum;
</p>
<p>k
</p>
<p>&infin;&sum;
</p>
<p>n=1
a(k)n (y)u
</p>
<p>(k)
n (x)
</p>
<p>where a(k)n (y) =
&acute; b
</p>
<p>a
w(x)u
</p>
<p>&lowast;(k)
n (x)Gλ(x, y) dx. Using Green&rsquo;s identity,
</p>
<p>Eq. (20.30), and the fact that λn is real, we have
</p>
<p>λna
(k)
n (y)=
</p>
<p>ˆ b
</p>
<p>a
</p>
<p>w(x)
[
λnu
</p>
<p>(k)
n (x)
</p>
<p>]&lowast;
Gλ(x, y) dx
</p>
<p>=
ˆ b
</p>
<p>a
</p>
<p>w(x)Gλ(x, y)
{
Lx
[
u(k)n (x)
</p>
<p>]}&lowast;
dx
</p>
<p>=
ˆ b
</p>
<p>a
</p>
<p>w(x)
[
u(k)n (x)
</p>
<p>]&lowast;
Lx
[
Gλ(x, y)
</p>
<p>]
dx</p>
<p/>
</div>
<div class="page"><p/>
<p>20.4 Eigenfunction Expansion 631
</p>
<p>=
ˆ b
</p>
<p>a
</p>
<p>w(x)u&lowast;(k)n (x)
[
δ(x &minus; y)
w(x)
</p>
<p>+ λGλ(x, y)
]
dx
</p>
<p>= u&lowast;(k)n (y)+ λ
ˆ b
</p>
<p>a
</p>
<p>w(x)u&lowast;(k)n (x)Gλ(x, y) dx
</p>
<p>= u&lowast;(k)n (y)+ λa(k)n (y).
</p>
<p>Thus, a(k)n (y)= u&lowast;(k)n (y)/(λn &minus; λ), and the expansion for the Green&rsquo;s func-
tion is
</p>
<p>Gλ(x, y)=
&sum;
</p>
<p>k
</p>
<p>&infin;&sum;
</p>
<p>n=1
</p>
<p>u
&lowast;(k)
n (y)u
</p>
<p>(k)
n (x)
</p>
<p>λn &minus; λ
. (20.42)
</p>
<p>This expansion is valid as long as λn �= λ for any n= 0,1,2, . . . . But this is
precisely the condition that ensures the existence of an inverse for L&minus; λ1.
</p>
<p>An interesting result is obtained from Eq. (20.42) if λ is considered a
complex variable. In that case, Gλ(x, y) has (infinitely many) simple poles
at {λn}&infin;n=1. The residue at the pole λn is &minus;
</p>
<p>&sum;
k u
</p>
<p>&lowast;(k)
n (y)u
</p>
<p>(k)
n (x). If Cm is a
</p>
<p>contour having the poles {λn}mn=1 in its interior, then, by the residue theorem,
we have
</p>
<p>1
</p>
<p>2πi
</p>
<p>˛
</p>
<p>Cm
</p>
<p>Gλ(x, y) dλ=&minus;
&sum;
</p>
<p>k
</p>
<p>m&sum;
</p>
<p>n=1
u&lowast;(k)n (y)u
</p>
<p>(k)
n (x).
</p>
<p>In particular, if we let m&rarr;&infin;, we obtain
</p>
<p>1
</p>
<p>2πi
</p>
<p>˛
</p>
<p>C&infin;
Gλ(x, y) dλ=&minus;
</p>
<p>&sum;
</p>
<p>k
</p>
<p>&infin;&sum;
</p>
<p>n=1
u&lowast;(k)n (y)u
</p>
<p>(k)
n (x)
</p>
<p>=&minus;δ(x &minus; y)
w(x)
</p>
<p>, (20.43)
</p>
<p>where C&infin; is any contour that encircles all the eigenvalues, and in the last
step we used the completeness of the eigenfunctions. Equation (20.43) is the
infinite-dimensional analogue of Eq. (17.10) with f (A)= 1 when the latter
equation is sandwiched between 〈x| and |y〉.
</p>
<p>Example 20.4.1 Consider the DO Lx = d2/dx2 with BCs u(0)= u(a)= 0.
This is an S-L operator with eigenvalues and normalized eigenfunctions
</p>
<p>λn =
(
nπ
</p>
<p>a
</p>
<p>)2
and un(x)=
</p>
<p>&radic;
2
</p>
<p>a
sin
</p>
<p>(
nπ
</p>
<p>a
x
</p>
<p>)
for n= 1,2, . . . .
</p>
<p>Equation (20.42) becomes
</p>
<p>Gλ(x, y)=&minus;
2
</p>
<p>a
</p>
<p>&infin;&sum;
</p>
<p>n=1
</p>
<p>sin(nπx/a) sin(nπy/a)
</p>
<p>λ&minus; (nπ/a)2,
</p>
<p>which leads to</p>
<p/>
</div>
<div class="page"><p/>
<p>632 20 Green&rsquo;s Functions in One Dimension
</p>
<p>1
</p>
<p>2πi
</p>
<p>˛
</p>
<p>C&infin;
Gλ(x, y) dλ
</p>
<p>= 1
2πi
</p>
<p>˛
</p>
<p>C&infin;
</p>
<p>[
&minus;2
a
</p>
<p>&infin;&sum;
</p>
<p>n=1
</p>
<p>sin(nπx/a) sin(nπy/a)
</p>
<p>λ&minus; (nπ/a)2
</p>
<p>]
dλ
</p>
<p>=&minus; 1
2πi
</p>
<p>(
2
</p>
<p>a
</p>
<p>) &infin;&sum;
</p>
<p>n=1
sin
</p>
<p>(
nπ
</p>
<p>a
x
</p>
<p>)
sin
</p>
<p>(
nπ
</p>
<p>a
y
</p>
<p>)
˛
</p>
<p>C&infin;
</p>
<p>dλ
</p>
<p>λ&minus; (nπ/a)2
</p>
<p>=&minus;
(
</p>
<p>2
</p>
<p>a
</p>
<p>) &infin;&sum;
</p>
<p>n=1
sin
</p>
<p>(
nπ
</p>
<p>a
x
</p>
<p>)
sin
</p>
<p>(
nπ
</p>
<p>a
y
</p>
<p>)
Res
</p>
<p>[
1
</p>
<p>λ&minus; (nπ/a)2
]
</p>
<p>λ=λn
</p>
<p>=&minus;
(
</p>
<p>2
</p>
<p>a
</p>
<p>) &infin;&sum;
</p>
<p>n=1
sin
</p>
<p>(
nπ
</p>
<p>a
x
</p>
<p>)
sin
</p>
<p>(
nπ
</p>
<p>a
y
</p>
<p>)
.
</p>
<p>The RHS is recognized as &minus;δ(x &minus; y).
</p>
<p>If zero is not an eigenvalue of Lx , Eq. (20.42) yieldseigenfunction expansion
of GF
</p>
<p>G(x,y)&equiv;G0(x, y)=
&sum;
</p>
<p>k
</p>
<p>&infin;&sum;
</p>
<p>n=1
</p>
<p>u
&lowast;(k)
n (y)u
</p>
<p>(k)
n (x)
</p>
<p>λn
, (20.44)
</p>
<p>which is an expression for the Green&rsquo;s function of Lx in terms of its eigen-
values and eigenfunctions.
</p>
<p>20.5 Problems
</p>
<p>20.1 Using the GF method, solve the DE Lxu(x)= du/dx = f (x) subject
to the BC u(0)= a. Hint: Consider the function v(x)= u(x)&minus; a.
</p>
<p>20.2 Solve the problem of Example 20.1.4 subject to the BCs u(a) =
u&prime;(a)= 0. Show that the corresponding GF also satisfies these BCs.
</p>
<p>20.3 Show that the IVP with data {0;0,0, . . . ,0} has only u&equiv; 0 as a solu-
tion. Hint: Assume otherwise, add u to the solution of the inhomogeneous
equation, and invoke uniqueness.
</p>
<p>20.4 In this problem, we generalize the concepts of exactness and integrat-exact NOLDE
ing factor to a NOLDE. The DO L(n)x &equiv;
</p>
<p>&sum;n
k=0 pk(x)d
</p>
<p>k/dxk is said to be
</p>
<p>exact if there exists a DO M(n&minus;1)x &equiv;
&sum;n&minus;1
</p>
<p>k=0 ak(x)d
k/dxk such that
</p>
<p>L(n)x [u] =
d
</p>
<p>dx
</p>
<p>(
M(n&minus;1)x [u]
</p>
<p>)
&forall;u &isin; Cn[a, b].
</p>
<p>(a) Show that L(n)x is exact iff
&sum;n
</p>
<p>m=0(&minus;1)mdmpm/dxm = 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>20.5 Problems 633
</p>
<p>(b) Show that there exists an integrating factor for L(n)x &mdash;that is, a function
μ(x) such that μ(x)L(n)x is exact&mdash;if and only if μ(x) satisfies the DE
</p>
<p>N(n)x [μ] &equiv;
n&sum;
</p>
<p>m=0
(&minus;1)m d
</p>
<p>m
</p>
<p>dxm
(μpm)= 0.
</p>
<p>The DO N(n)x is the formal adjoint of L
(n)
x .
</p>
<p>20.5 Let Lx be a SOLDO. Assuming that Lx[u] = 0 has no nontrivial solu-
tion, show that the matrix
</p>
<p>R &equiv;
(
R1[u1] R1[u2]
R2[u1] R2[u2]
</p>
<p>)
,
</p>
<p>where u1 and u2 are independent solutions of Lx[u] = 0 and Ri are the
boundary functionals, has a nonzero determinant. Hint: Assume otherwise
and show that the system of homogeneous linear equations αR1[u1] +
βR1[u2] = 0 and αR2[u1]+βR2[u2] = 0 has a nontrivial solution for (α,β).
Reach a contradiction by considering u = αu1 + βu2 as a solution of
Lx[u] = 0.
</p>
<p>20.6 Determine the formal adjoint of each of the operators in (a) through (d)
below (i) as a differential operator, and (ii) as an operator, that is, including
the BCs. Which operators are formally self-adjoint? Which operators are
self-adjoint?
</p>
<p>(a) Lx = d2/dx2 + 1 in [0,1] with BCs u(0)= u(1)= 0.
(b) Lx = d2/dx2 in [0,1] with BCs u(0)= u&prime;(0)= 0.
(c) Lx = d/dx in [0,&infin;] with BCs u(0)= 0.
(d) Lx = d3/dx3 &minus; sinxd/dx + 3 in [0,π] with BCs u(0) = u&prime;(0) = 0,
</p>
<p>u&prime;&prime;(0)&minus; 4u(π)= 0.
</p>
<p>20.7 Show that the Dirichlet, Neumann, general unmixed, and periodic BCs
make the following formally self-adjoint SOLDO self-adjoint:
</p>
<p>Lx =
1
</p>
<p>w
</p>
<p>d
</p>
<p>dx
</p>
<p>(
p
</p>
<p>d
</p>
<p>dx
</p>
<p>)
+ q.
</p>
<p>20.8 Using a procedure similar to that described in the text for SOLDOs,
show that for the FOLDO Lx = p1d/dx + p0
(a) the indefinite GF is
</p>
<p>G(x,y)&equiv; μ(y)
p1(y)w(y)
</p>
<p>[
θ(x &minus; y)
μ(x)
</p>
<p>]
+C(y),
</p>
<p>where μ(x)= exp
[
ˆ x p0(t)
</p>
<p>p1(t)
dt
</p>
<p>]
,
</p>
<p>(b) and the GF itself is discontinuous at x = y with
</p>
<p>lim
ǫ&rarr;0
</p>
<p>[
G(y + ǫ, y)&minus;G(y &minus; ǫ, y)
</p>
<p>]
= 1
</p>
<p>p1(y)w(y)
.</p>
<p/>
</div>
<div class="page"><p/>
<p>634 20 Green&rsquo;s Functions in One Dimension
</p>
<p>(c) For the homogeneous BC
</p>
<p>R[u] &equiv; α1u(a)+ α2u&prime;(a)+ β1u(b)+ β2u&prime;(b)= 0
</p>
<p>construct G(x,y) and show that
</p>
<p>G(x,y)= 1
p1(y)w(y)v(y)
</p>
<p>v(x)θ(x &minus; y)+C(y)v(x),
</p>
<p>where v(x) is any solution to the homogeneous DE Lx[u] = 0 and
</p>
<p>C(y)= β1v(b)+ β2v
&prime;(b)
</p>
<p>R[v]p1(y)w(y)v(y)
, with R[v] �= 0
</p>
<p>(d) Show directly that Lx[G] = δ(x &minus; y)/w(x).
</p>
<p>20.9 Let Lx be a NOLDO with constant coefficients. Show that if u(x) sat-
isfies Lx[u] = f (x), then u(x&minus; y) satisfies Lx[u] = f (x&minus; y). (Note that no
BCs are specified.)
</p>
<p>20.10 Find the GF for Lx = d2/dx2 + 1 with BCs u(0)= u&prime;(0)= 0. Show
that it can be written as a function of x &minus; y only.
</p>
<p>20.11 Find the GF for Lx = d2/dx2 + k2 with BCs u(0)= u(a)= 0.
</p>
<p>20.12 Find the GF for Lx = d2/dx2 &minus; k2 with BCs u(&infin;)= u(&minus;&infin;)= 0.
</p>
<p>20.13 Find the GF for Lx = (d/dx)(xd/dx) given the condition that
G(x,y) is finite at x = 0 and vanishes at x = 1.
</p>
<p>20.14 Evaluate the GF and the solutions for each of the following DEs in
the interval [0,1].
</p>
<p>(a) u&prime;&prime; &minus; k2u= f ; u(0)&minus; u&prime;(0)= a, u(1)= b.
(b) u&prime;&prime; = f ; u(0)= u&prime;(0)= 0.
(c) u&prime;&prime; + 6u&prime; + 9u= 0; u(0)= 0, u&prime;(0)= 1.
(d) u&prime;&prime; +ω2u= f (x), for x &gt; 0; u(0)= a, u&prime;(0)= b.
(e) u(4) = f ; u(0)= 0, u&prime;(0)= 2u&prime;(1), u(1)= a, u&prime;&prime;(0)= 0.
</p>
<p>20.15 Use eigenfunction expansion of the GF to solve the BVP u&prime;&prime; = x,
u(0)= 0, u(1)&minus; 2u&prime;(1)= 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>21Multidimensional Green&rsquo;s Functions:Formalism
</p>
<p>The extensive study of Green&rsquo;s functions in one dimension in the last chapter
has no doubt exhibited the power and elegance of their use in solving inho-
mogeneous differential equations. If the differential equation has a (unique)
solution, the GF exists and contains all the information necessary to build it
up. The solution results from operating on the inhomogeneous term with an
integral operator whose kernel is the appropriate Green&rsquo;s function.
</p>
<p>The Green&rsquo;s function&rsquo;s very existence depends on the type of BCs im-
posed. We encountered two types of problems in solving ODEs. The first,
called initial value problems (IVPs), involves fixing (for an nth-order DE)
the value of the solution and its first n&minus; 1 derivatives at a fixed point. Then
the ODE, if it is sufficiently well-behaved, will determine the values of the
solution in the neighborhood of the fixed point in a unique way. Because of
this uniqueness, Green&rsquo;s functions always exist for IVPs.
</p>
<p>The second type of problems, called boundary value problems (BVPs),
consists&mdash;when the DE is second order&mdash;of determining a relation between
the solution and its derivative evaluated at the boundaries of some interval
[a, b]. These boundary values are relations that we denoted by Ri[u] = γi ,
where i = 1,2. In this case, the existence and uniqueness of the Green&rsquo;s
function are not guaranteed.
</p>
<p>There is a fundamental (topological) difference between a boundary in
one dimension and a boundary in two and more dimensions. In one dimen-
sion a boundary consists of only two points; in 2 and higher dimensions a
boundary has infinitely many points. The boundary of a region in R2 is a
closed curve, in R3 it is a closed surface, and in Rm it is called a hypersur-
face. This fundamental difference makes the study of Green&rsquo;s functions in
higher dimensions more complicated, but also richer and more interesting.
</p>
<p>21.1 Properties of Partial Differential Equations
</p>
<p>This section presents certain facts and properties of PDEs, in particular, how
BCs affect their solutions. We shall discover the important difference be-
tween ODEs and PDEs: The existence of a solution to a PDE satisfying a
given BC depends on the type of the PDE.
</p>
<p>S. Hassani, Mathematical Physics, DOI 10.1007/978-3-319-01195-0_21,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>635</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-01195-0_21">http://dx.doi.org/10.1007/978-3-319-01195-0_21</a></div>
</div>
<div class="page"><p/>
<p>636 21 Multidimensional Green&rsquo;s Functions: Formalism
</p>
<p>We shall be concerned exclusively with a linear PDE. A linear PDE of
order M in m variables is of the form
</p>
<p>Lx[u] = f (x) where Lx =
M&sum;
</p>
<p>|J |=1
</p>
<p>&sum;
</p>
<p>J
</p>
<p>aJ (x)
&part; |J |
</p>
<p>&part;xJ
, (21.1)
</p>
<p>where the following notation has been used:
</p>
<p>x = (x1, . . . , xm), J = (j1, . . . , jm),
</p>
<p>|J | = j1 + j2 + &middot; &middot; &middot; + jm,
&part; |J |
</p>
<p>&part;xJ
= &part;
</p>
<p>|J |
</p>
<p>&part;x
j1
1 &part;x
</p>
<p>j2
2 &middot; &middot; &middot; &part;x
</p>
<p>jm
m
</p>
<p>;
</p>
<p>the jk are nonnegative integers; M , the order of the highest derivative, is
called the order of the PDE. The outer sum in Eq. (21.1) is over |J |; once |J |
is fixed, the inner summation goes over individual jk&rsquo;s with the restriction
that their sum has to equal the given |J |.
</p>
<p>The principal part of Lx isprincipal part of a PDE
</p>
<p>Lp =
&sum;
</p>
<p>|J |=M
aJ (x1, . . . , xm)
</p>
<p>&part;M
</p>
<p>&part;xJ
. (21.2)
</p>
<p>The coefficients aJ and the inhomogeneous (or source) term f are assumed
to be continuous functions of their arguments.
</p>
<p>We consider Eq. (21.1) as an IVP with appropriate initial data. The most
direct generalization of the IVP of ordinary differential equation theory is
to specify the values of u and all its normal derivatives of order less than or
equal to M &minus; 1 on a hypersurface Ŵ of dimension m&minus; 1. This type of initial
data is called Cauchy data, and the resulting IVP is known as the CauchyCauchy data and Cauchy
</p>
<p>problem problem for Lx. The reason that the tangential derivatives do not come into
play here is that once we know the values of u on Ŵ, we can evaluate u
on two neighboring points on Ŵ, take the limit as the points get closer and
closer, and evaluate the tangential derivatives.
</p>
<p>21.1.1 Characteristic Hypersurfaces
</p>
<p>In contrast to the IVP in one dimension, the Cauchy problem for arbitrary
Cauchy data may not have a solution, or if it does, the solution may not be
unique.
</p>
<p>Box 21.1.1 The existence and uniqueness of the solution of the
Cauchy problem depend crucially on the hypersurface Ŵ and on the
type of PDE.
</p>
<p>We assume that Ŵ can be parametrized by a set of m functions of m&minus; 1
parameters. These parameters can be thought of as generalized coordinates
of points of Ŵ.</p>
<p/>
</div>
<div class="page"><p/>
<p>21.1 Properties of Partial Differential Equations 637
</p>
<p>Consider a point P on Ŵ. Introduce m&minus; 1 coordinates ξ2, . . . , ξm, called
tangential coordinates, to label points on Ŵ. Choose, by translation if nec- tangential coordinates
essary, coordinates in such a way that P is the origin, with coordinates
(0,0, . . . ,0). Now let ν = ξ1 stand for the remaining coordinate normal to
Ŵ. Usually ξi is taken to be the ith coordinate of the projection of the point
on Ŵ onto the hyperplane tangent to Ŵ at P .
</p>
<p>As long as we do not move too far away from P , the Cauchy data on Ŵ
can be written as
</p>
<p>u(0, ξ2, . . . , ξm),
&part;u
</p>
<p>&part;ν
(0, ξ2, . . . , ξm), . . . ,
</p>
<p>&part;M&minus;1u
&part;νM&minus;1
</p>
<p>(0, ξ2, . . . , ξm).
</p>
<p>Using the chain rule, &part;u/&part;xi =
&sum;m
</p>
<p>j=1(&part;u/&part;ξj )(&part;ξj/&part;xi), where ξ1 = ν, we
can also determine the first M &minus; 1 derivatives of u with respect to xi . The
fundamental question is whether we can determine u uniquely using the
above Cauchy data and the DE. To motivate the answer, let&rsquo;s look at the
analogous problem in one dimension.
</p>
<p>Consider the M th-order linear ODE
</p>
<p>Lx[u] = aM(x)
dMu
</p>
<p>dxM
+ &middot; &middot; &middot; + a1(x)
</p>
<p>du
</p>
<p>dx
+ a0(x)u= f (x) (21.3)
</p>
<p>with the following initial data at x0: {u(x0), u&prime;(x0), . . . , u(M&minus;1)(x0)}. If the
coefficients {ak(x)}Mk=0 and the inhomogeneous term f (x) are continuous
and if aM(x0) �= 0, then Theorem 20.2.2 implies that there exists a unique
solution to the IVP in a neighborhood of x0.
</p>
<p>For aM(x0) �= 0, Eq. (21.3), the initial data, and a knowledge of f (x0)
give u(M)(x0) uniquely. Having found u(M)(x0), we can calculate, with ar-
bitrary accuracy (by choosing �x small enough), the following set of new
initial data at x1 = x0 +�x:
</p>
<p>u(x1)= u(x0)+ u&prime;(x0)�x, . . . , u(M&minus;1)(x1)= u(M&minus;1)(x0)+ u(M)(x0)�x.
</p>
<p>Using these new initial data and Theorem 20.2.2, we are assured of a unique
solution at x1. Since aM(x) is assumed to be continuous for x1, for suffi-
ciently small �x, aM(x0) is nonzero, and it is possible to find newer initial
data at x2 = x1 +�x. The process can continue until we reach a singularity
of the DE, a point where aM(x) vanishes. We can thus construct the unique
solution of the IVP in an interval (x0, b) as long as aM(x) does not van-
ish anywhere in [x0, b]. This procedure is analogous to the one used in the
analytic continuation of a complex function.
</p>
<p>For aM(x0)= 0, however, we cannot calculate u(M)(x0) unambiguously.
In such a case the LHS of (21.3) is completely determined from the initial
data. If the LHS happens to be equal to f (x0), then the equation is satisfied
for any u(M)(x0), i.e., there exist infinitely many solutions for u(M)(x0);
if the LHS is not equal to f (x0), there are no solutions. The difficulty
can be stated in another way, which is useful for generalization to the m-
dimensional case: If aM(x0) = 0 in (21.3), then the initial data determine
the function Lx[u].</p>
<p/>
</div>
<div class="page"><p/>
<p>638 21 Multidimensional Green&rsquo;s Functions: Formalism
</p>
<p>Let us now return to the question of constructing u and investigate con-
ditions under which the Cauchy problem may have a solution. We follow
the same steps as for the IVP for ODEs. To construct the solution numeri-
cally for points near P but away from Ŵ (since the function is completely
determined on Ŵ, not only its M th derivative but derivatives of all orders
are known on Ŵ), we must be able to calculate &part;Mu/&part;νM at P . This is not
possible if the coefficient of &part;Mu/&part;νM in Lx[u] is zero when x1, . . . , xm
is written in terms of ν, ξ2, . . . , ξm. When this happens, Lx[u] itself will be
determined by the Cauchy data. This motivates the following definition.
</p>
<p>Definition 21.1.2 If Lx[u] can be evaluated at a point P on Ŵ from the
Cauchy data alone, then Ŵ is said to be characteristic for Lx at P . If Ŵ ischaracteristic
</p>
<p>hypersurface characteristic for all its points, then it is called a characteristic hypersur-
face for Lx. The Cauchy problem does not have a solution at a point on the
characteristic hypersurface.
</p>
<p>The following theorem characterizes Ŵ:
</p>
<p>Theorem 21.1.3 Let Ŵ be a smooth (m&minus;1)-dimensional hypersurface. Let
Lx[u] = f be an M th-order linear PDE in m variables. Then Ŵ is char-
acteristic at P &isin; Ŵ if and only if the coefficient of &part;Mu/&part;νM vanishes
when Lx is expressed in terms of the normal&ndash;tangential coordinate system
(ν, ξ2, . . . , ξm).
</p>
<p>One can rephrase the foregoing theorem as follows:
</p>
<p>Box 21.1.4 The hypersurface Ŵ is not characteristic at P if and only
if all M th-order partial derivatives of u with respect to {xi}mi=1 are
unambiguously determined at P by the DE and the Cauchy data on Ŵ.
</p>
<p>In the one-dimensional case the difficulty arose when aM(x0) = 0. In
the language being used here, we could call x0 a &ldquo;characteristic point.&rdquo; This
makes sense because in this special case (m= 1), the hypersurfaces can only
be of dimension 0. Thus, we can say that in the neighborhood of a charac-Characteristic
</p>
<p>&ldquo;hypersurfaces&rdquo; of ODEs
</p>
<p>are points!
</p>
<p>teristic point, the IVP has no well-defined solution.1 For the general case
(m&gt; 1), we can similarly say that the Cauchy problem has no well-defined
solution in the neighborhood of P if P happens to lie on a characteristic
hypersurface of the differential operator. Thus, it is important to determine
the characteristic hypersurfaces of PDEs.
</p>
<p>Example 21.1.5 Let us consider the first-order PDE in two variables
</p>
<p>Lx[u] = a(x, y)
&part;u
</p>
<p>&part;x
+ b(x, y)&part;u
</p>
<p>&part;y
+ F(x, y,u)= 0 (21.4)
</p>
<p>1Here lies the crucial difference between ODEs and PDEs: All ODEs have a universal
characteristic hypersurface, i.e., a point. PDEs, on the other hand, can have a variety of
hypersurfaces.</p>
<p/>
</div>
<div class="page"><p/>
<p>21.1 Properties of Partial Differential Equations 639
</p>
<p>where F(x, y,u)&equiv; c(x, y)u+ d(x, y). For this discussion the form of F is
irrelevant.
</p>
<p>We wish to find the characteristic hypersurfaces (in this case, curves)
of L. The Cauchy data consist of a simple determination of u on Ŵ. By The-
orem 21.1.3, we need to derive relations that ensure that &part;u/&part;x and &part;u/&part;y
cannot be unambiguously determined at P = (x, y). Using an obvious no-
tation, the PDE of Eq. (21.4) gives
</p>
<p>&minus;F
(
P,u(P )
</p>
<p>)
= a(P )&part;u
</p>
<p>&part;x
(P )+ b(P )&part;u
</p>
<p>&part;y
(P ).
</p>
<p>On the other hand, if Q&equiv; (x + dx, y + dy) lies on the curve Ŵ, then
</p>
<p>u(Q)&minus; u(P )= dx &part;u
&part;x
</p>
<p>(P )+ dy &part;u
&part;y
</p>
<p>(P ).
</p>
<p>The Cauchy data determine the LHS of both of the preceding equations.
Treating these equations as a system of two linear equations in two un-
knowns, &part;u/&part;x(P ) and &part;u/&part;y(P ), we conclude that the system has a
unique solution if and only if the matrix of coefficients is invertible. Thus,
by Box 21.1.4, Ŵ is a characteristic curve if and only if
</p>
<p>det
</p>
<p>(
dx dy
</p>
<p>a(P ) b(P )
</p>
<p>)
= b(P )dx &minus; a(P )dy = 0,
</p>
<p>or dy/dx = b(x, y)/a(x, y), assuming that a(x, y) �= 0. Solving this FODE
yields y as a function of x, thus determining the characteristic curve. Note
that a general solution of this FODE involves an arbitrary constant, resulting
in a family of characteristic curves.
</p>
<p>Historical Notes
</p>
<p>Sofia Vasilyevna Kovalevskaya (1850&ndash;1891) is considered the greatest woman math-
</p>
<p>Sofia Vasilyevna
</p>
<p>Kovalevskaya 1850&ndash;1891
</p>
<p>ematician prior to the twentieth century. She grew up in a well-educated family of the
Russian nobility, her father being an artillery general and reputed to be a descendant of
a Hungarian king, Mathias Korvin. Sonja was educated by a British governess and en-
joyed life at the large country estate of her father&rsquo;s family, although the rather progressive
thinking of the Kovalevsky sisters did not always meet with approval from their father.
Sonja has written of two factors that attracted her to the study of mathematics. The first
was her Uncle Pyotr, who had studied the subject on his own and would speak of squaring
the circle and of the asymptote, as well as of many other things that excited her imagina-
tion. The second was a curious &ldquo;wallpaper&rdquo; that was used to cover one of the children&rsquo;s
rooms at Polibino, which turned out to be lecture notes on differential and integral calcu-
lus that had been purchased by her father in student days. These sheets fascinated her and
she would spend hours trying to decipher separate phrases and to find the proper ordering
of the pages.
In the autumn of 1867 Sonja went to St. Petersburg, where she studied calculus with
Alexander Strannolyubsky, a teacher of mathematics at the naval school. While there,
she consulted the prominent Russian mathematician Chebyshev about her mathematical
studies, but since Russian universities were closed to women, there seemed to be no way
that she could pursue advanced studies in her native land.
In order to escape the oppression of women common in Russia at the time, young ladies
of ambition and ability would often arrange a marriage of convenience in order to al-
low study at a foreign university. At the age of 18, Sonya arranged such a marriage
with Vladimir Kovalevsky, a paleontologist, and in 1869 the couple moved to Heidel-
berg, where Sonja took courses from Kirchhoff, Helmholtz, and others. Two years later</p>
<p/>
</div>
<div class="page"><p/>
<p>640 21 Multidimensional Green&rsquo;s Functions: Formalism
</p>
<p>she went to Berlin, where she worked with Weierstrass, who tutored her privately, since
she, as a woman, was not allowed to attend lectures.
The three papers she published in the next three years earned her a doctorate in absentia
from the University of G&ouml;ttingen. Unfortunately, even that distinction was not sufficient
to gain her a university position anywhere in Europe, despite strong recommendation
from the renowned Weierstrass. Her rejections resulted in a six-year period during which
time she neither undertook research nor replied to Weierstrass&rsquo;s letters. She was bitter to
discover that the best job she was offered was teaching arithmetic to elementary classes
of schoolgirls, and remarked, &ldquo;I was unfortunately weak in the multiplication table.&rdquo;
The existence and uniqueness of solutions to partial differential equations occupied the
attention of many notable mathematicians of the last century, including Cauchy, who
transformed the problem into his method of majorant functions. This method was later
extended and refined by Kovalevskaya to include more general cases. The result was the
now-famous Cauchy&ndash;Kovalevskaya theorem. She also contributed to the advancement of
the study of Abelian integrals and functions and applied her knowledge of these topics to
problems in physics, including her paper &ldquo;On the Rotation of a Solid Body About a Fixed
Point,&rdquo; for which she won a 5000-franc prize. She also performed some investigations
into the dynamics of Saturn&rsquo;s rings, inspiring a sonnet in which she is named &ldquo;Muse of
the Heavens.&rdquo; In 1878, Kovalevskaya gave birth to a daughter, but from 1880 increas-
ingly returned to her study of mathematics. In 1882 she began work on the refraction of
light, and wrote three articles on the topic. In the spring of 1883, Vladimir, from whom
Sonja had been separated for two years, committed suicide. After the initial shock, Ko-
valevskaya immersed herself in mathematical work in an attempt to rid herself of feelings
of guilt. Mittag-Leffler managed to overcome opposition to Kovalevskaya in Stockholm,
and obtained for her a position as privat docent. She began to lecture there in early 1884,
was appointed to a five-year extraordinary professorship in June of that year, and in June
1889 became the third woman ever to hold a chair at a European university.
During Kovalevskaya&rsquo;s years at Stockholm she carried out important research, taught
courses on the latest topics in analysis, and became an editor of the new journal Acta
Mathematica. She was the liaison with the mathematicians of Paris and Berlin, and took
part in the organization of international conferences. Interestingly, Kovalevskaya also nur-
tured a parallel career in literature, penning several novels and a drama, &ldquo;The Struggle for
Happiness&rdquo; that was favorably received at the Korsh Theater in Moscow. She died at the
pinnacle of her scientific career from a combination of influenza and pneumonia less than
two years after her election to both the Swedish and the Russian Academies of Sciences.
The latter membership being initiated by Chebyshev, in spite of the Tsarist government&rsquo;s
repeated refusal to grant her a university position in her own country.
</p>
<p>21.1.2 Second-Order PDEs inm Dimensions
</p>
<p>Because of their importance in mathematical physics, the rest of this chapter
and the next will be devoted to SOPDEs. This subsection classifies SOPDEs
and the BCs associated with them.
</p>
<p>The most general linear SOPDE in m variables can be written as
</p>
<p>m&sum;
</p>
<p>j,k=1
Ajk(x)
</p>
<p>&part;2u
</p>
<p>&part;xj&part;xk
+
</p>
<p>m&sum;
</p>
<p>j=1
Bj (x)
</p>
<p>&part;u
</p>
<p>&part;xj
+C(x)u= 0,
</p>
<p>where Ajk can be assumed to be symmetric in j and k. We restrict ourselves
to the simpler case in which the matrix (Ajk) is diagonal.2 We therefore
</p>
<p>2This is not a restriction because, by a change of variables and Theorem 6.6.6 (especially
the comments after it) Ajk can be brought to a diagonal form.</p>
<p/>
</div>
<div class="page"><p/>
<p>21.1 Properties of Partial Differential Equations 641
</p>
<p>consider the PDE
</p>
<p>m&sum;
</p>
<p>j=1
aj (x)
</p>
<p>&part;2u
</p>
<p>&part;x2j
</p>
<p>+ F
(
</p>
<p>x, u,
&part;u
</p>
<p>&part;x
</p>
<p>)
, (21.5)
</p>
<p>where the last term collects all the terms except the second derivatives. We
classify SOPDEs as follows:
</p>
<p>1. Equation (21.5) is said to be of elliptic type at x0 if all the coefficients second-order PDE of
elliptic typeaj (x0) are nonzero and have the same sign.
</p>
<p>2. Equation (21.5) is said to be of ultrahyperbolic type at x0 if all aj (x0) second-order PDEs of
hyperbolic and
</p>
<p>ultrahyperbolic type
</p>
<p>are nonzero but do not have the same sign. If only one of the coeffi-
cients has a sign different from the rest, the equation is said to be of
hyperbolic type.
</p>
<p>3. Equation (21.5) is said to be of parabolic type at x0 if at least one of second-order PDE of
parabolic typethe coefficients aj (x0) is zero.
</p>
<p>If a SOPDE is of a given type at every point of its domain, it is said to be
of that given type. In particular, if the coefficients aj are constants, the type
of the PDE does not change from point to point.
</p>
<p>Example 21.1.6 In this example, we study the SOPDE in two dimensions.
The most general linear SOPDE is
</p>
<p>L[u] = a &part;
2u
</p>
<p>&part;x2
+ 2b &part;
</p>
<p>2u
</p>
<p>&part;x&part;y
+ c &part;
</p>
<p>2u
</p>
<p>&part;y2
+ F
</p>
<p>(
x, y,u,
</p>
<p>&part;u
</p>
<p>&part;x
,
&part;u
</p>
<p>&part;y
</p>
<p>)
= 0, (21.6)
</p>
<p>where a, b, and c are functions of x and y.
To determine the characteristic curves of L, we seek conditions under
</p>
<p>which all second-order partial derivatives of u can be determined from the
DE and the Cauchy data, which are values of u and all its first derivatives
on Ŵ. Consider a point Q&equiv; (x + dx, y + dy) close to P &equiv; (x, y). We can
write
</p>
<p>&part;u
</p>
<p>&part;x
(Q)&minus; &part;u
</p>
<p>&part;x
(P )= dx &part;
</p>
<p>2u
</p>
<p>&part;x2
(P )+ dy &part;
</p>
<p>2u
</p>
<p>&part;x&part;y
(P ),
</p>
<p>&part;u
</p>
<p>&part;y
(Q)&minus; &part;u
</p>
<p>&part;y
(P )= dx &part;
</p>
<p>2u
</p>
<p>&part;x&part;y
(P )+ dy &part;
</p>
<p>2u
</p>
<p>&part;y2
(P ),
</p>
<p>&minus;F
(
P,u(P ),
</p>
<p>&part;u
</p>
<p>&part;x
(P ),
</p>
<p>&part;u
</p>
<p>&part;y
(P )
</p>
<p>)
= a(P )&part;
</p>
<p>2u
</p>
<p>&part;x2
(P )+ 2b(P ) &part;
</p>
<p>2u
</p>
<p>&part;x&part;y
(P )
</p>
<p>+ c(P )&part;
2u
</p>
<p>&part;y2
(P ).
</p>
<p>This system of three linear equations in the three unknowns&mdash;the three sec-
ond derivatives evaluated at P&mdash;has a unique solution if and only if the
determinant of the coefficients is nonzero. Thus, by Box 21.1.4, Ŵ is a char-</p>
<p/>
</div>
<div class="page"><p/>
<p>642 21 Multidimensional Green&rsquo;s Functions: Formalism
</p>
<p>acteristic curve if and only if
</p>
<p>det
</p>
<p>⎛
⎝
</p>
<p>dx dy 0
0 dx dy
</p>
<p>a(P ) 2b(P ) c(p)
</p>
<p>⎞
⎠= 0,
</p>
<p>or a(x, y)(dy)2 &minus;2b(x, y)dxdy+c(x, y)(dx)2 = 0. It then follows, assum-
ing that a(x, y) �= 0, that
</p>
<p>dy
</p>
<p>dx
= b&plusmn;
</p>
<p>&radic;
b2 &minus; ac
a
</p>
<p>. (21.7)
</p>
<p>There are three cases to consider:
</p>
<p>1. If b2 &minus; ac &lt; 0, Eq. (21.7) has no solution, which implies that no char-
acteristic curves exist at P . Problem 21.1 shows that the SOPDE is of
elliptic type. Thus, the Laplace equation in two dimensions is elliptic
because b2&minus;ac=&minus;1. In fact, it is elliptic in the whole plane, or, stated
differently, it has no characteristic curve in the entire xy-plane. This
may lead us to believe that the Cauchy problem for the Laplace equa-
tion in two dimensions has a unique solution. However, even though
the absence of a characteristic hypersurface at P is a necessary con-
dition for the existence of a solution to the Cauchy problem, it is not
sufficient. Problem 21.4 presents a Cauchy problem that is ill-posed,ill-posed Cauchy
</p>
<p>problem meaning that the solution at any fixed point is not a continuous func-
tion of the initial data. Satisfying this continuity condition is required
of a well-posed problem on both mathematical and physical grounds.
</p>
<p>2. If b2 &minus; ac &gt; 0, Eq. (21.7) has two solutions; that is, there are two
characteristic curves passing through P . Problem 21.1 shows that the
SOPDE is of hyperbolic type. The wave equation is such an equation
in the entire R2.
</p>
<p>3. If b2 &minus; ac = 0, Eq. (21.7) has only one solution. In this case there is
only one characteristic curve at P . The SOPDE is parabolic in this case.
The one-dimensional diffusion equation is an example of an SOPDE
that is parabolic in the entire R2.
</p>
<p>The question of what type of BCs to use to obtain a unique solution for aappropriate BCs are
determined by the type
</p>
<p>of PDE
</p>
<p>PDE is a very intricate mathematical problem. As Problem 21.4 shows, even
though it has no characteristic curves in the entire R2, the two-dimensional
Laplace equation does not lead to a well-posed Cauchy problem. On the
other hand, examples in Chap. 19 that dealt with electrostatic potentials and
temperatures led us to believe that a specification of the solution u on a
closed curve in 2D, and a closed surface in 3D, gives a unique solution. This
has a sound physical basis. After all, specifying the temperature (or electro-
static potential) on a closed surface should be enough to give us information
about the temperature (or electrostatic potential) in the region close to the
curve.
</p>
<p>Definition 21.1.7 A boundary condition in which the value of the solutionDirichlet boundary
condition and boundary
</p>
<p>value problem
</p>
<p>is given on a closed hypersurface is called a Dirichlet boundary condition,
and the associated problem, a Dirichlet BVP.</p>
<p/>
</div>
<div class="page"><p/>
<p>21.2 Multidimensional GFs and Delta Functions 643
</p>
<p>There is another type of BC, which on physical grounds is appropriate for
the Laplace equation. This condition is based on the fact that if the surface
charge on a conductor is specified, then the electrostatic potential in the
vicinity of the conductor can be determined uniquely. The surface charge on
a conductor is proportional to the value of the electric field on the conductor.
The electric field, on the other hand, is the normal derivative of the potential.
</p>
<p>Definition 21.1.8 A boundary condition in which the value of the normal Neumann boundary
condition and boundary
</p>
<p>value problem
</p>
<p>derivative of the solution is specified on a closed hypersurface is called a
Neumann BC, and the associated problem, a Neumann boundary value
problem.
</p>
<p>Thus, at least on physical grounds, either a Dirichlet BVP or a Neumann
BVP is a well-posed problem for the Laplace equation.
</p>
<p>For the heat (or diffusion) equation we are given an initial temperature
distribution f (x) on a bar along, say the x-axis, with end points held at
constant temperatures. For a bar with end points at x = a and x = b, this is
equivalent to the data u(0, x)= f (x), u(t, a)= T1, and u(t, b)= T2. These
are not Cauchy data, so we need not worry about characteristic curves. The
boundary curve consists of three parts: (1) t = 0 for a &le; x &le; b, (2) t &gt; 0 for
x = a, and (3) t &gt; 0, for x = b. In the xt-plane, these form an open rectangle
consisting of ab as one side and vertical lines at a and b as the other two.
The problem is to determine u on the side that closes the rectangle, that is,
on the side a &le; x &le; b at t &gt; 0.
</p>
<p>The wave equation requires specification of both u and &part;u/&part;t at t = 0.
The displacement of the boundaries of the waving medium&mdash;a taut rope
for example&mdash;must also be specified. Again the curve is open, as for the
diffusion case, but the initial data are Cauchy. Thus, for the wave equation Boundary conditions for
</p>
<p>elliptic, hyperbolic, and
</p>
<p>parabolic PDEs
</p>
<p>we do have a Cauchy problem with Cauchy data specified on an open curve.
Since the curve, the open rectangle, is not a characteristic curve of the wave
equation, the Cauchy problem is well-posed. We can generalize these BCs
to m dimensions.
</p>
<p>Box 21.1.9 The following correspondences exist between SOPDEs
with m variables and their appropriate BCs:
</p>
<p>1. Elliptic SOPDE &harr; Dirichlet or Neumann BCs on a closed hy-
persurface.
</p>
<p>2. Hyperbolic SOPDE &harr; Cauchy data on an open hypersurface.
3. Parabolic SOPDE&harr; Dirichlet or Neumann BCs on an open hy-
</p>
<p>persurface.
</p>
<p>21.2 Multidimensional GFs and Delta Functions
</p>
<p>This section will discuss some of the characteristics of Green&rsquo;s functions
in higher dimensions. These characteristics are related to the formal partial</p>
<p/>
</div>
<div class="page"><p/>
<p>644 21 Multidimensional Green&rsquo;s Functions: Formalism
</p>
<p>differential operator associated with the Green&rsquo;s function and also to the
delta functions.
</p>
<p>Using the formal idea of several continuous indices, we can turn the op-
erator equation LG= 1 into the PDE
</p>
<p>LxG(x,y)=
δ(x &minus; y)
w(x)
</p>
<p>, (21.8)
</p>
<p>where x,y &isin; Rm, w(x) is a weight function that is usually set equal to one,
and, only in Cartesian coordinates,
</p>
<p>δ(x &minus; y)= δ(x1 &minus; y1)δ(x2 &minus; y2) &middot; &middot; &middot; δ(xm &minus; ym)=
m&prod;
</p>
<p>i=1
δ(xi &minus; yi). (21.9)
</p>
<p>In most applications Cartesian coordinates are not the most convenient to
use. Therefore, it is helpful to express Eqs. (21.8) and (21.9) in other co-
ordinate systems. In particular, it is helpful to know how the delta function
transforms under a general coordinate transformation.
</p>
<p>Let xi = fi(ξ1, . . . , ξm), i = 1,2, . . . ,m, be a coordinate transforma-
tion. Let P be a point whose coordinates are a = (a1, . . . , am) and ααα =
(α1, . . . , αm) in the x and ξ coordinate systems, respectively. Let J be the
Jacobian of the transformation, that is, the absolute value of the determinant
of a matrix whose elements are &part;xi/&part;ξj . For a function F(x) the definition
of the delta function gives
</p>
<p>ˆ
</p>
<p>dmxF(x)δ(x &minus; a)= F(a).
</p>
<p>Expressing this equation in terms of the ξ coordinate system, recalling
that dmx = Jdmξ and ai = fi(ααα), and introducing the notation H(ξξξ) &equiv;
F(f1(ξξξ), . . . , fm(ξξξ)), we obtain
</p>
<p>ˆ
</p>
<p>dmξJH(ξξξ)
</p>
<p>m&prod;
</p>
<p>i=1
δ
(
fi(ξξξ)&minus; fi(ααα)
</p>
<p>)
=H(ααα). (21.10)
</p>
<p>This suggests that
</p>
<p>J
</p>
<p>m&prod;
</p>
<p>i=1
δ
(
fi(ξξξ)&minus; fi(ααα)
</p>
<p>)
=
</p>
<p>m&prod;
</p>
<p>i=1
δ(ξi &minus; αi),
</p>
<p>or, in more compact notation,
</p>
<p>Jδ(x &minus; a)= δ(ξξξ &minus;ααα).
</p>
<p>It is, of course, understood that J �= 0 at P . What happens when J = 0 at P ?
A point at which the Jacobian vanishes is called a singular point of thesingular point of a
</p>
<p>transformation transformation. Thus, all points on the z-axis, including the origin, are sin-
gular points of Cartesian&ndash;spherical transformation. Since J is a determinant,
its vanishing at a point signals lack of invertibility at that point. Thus, in the
transformation from Cartesian to spherical coordinates, all spherical coor-
dinates (5,π,ϕ), with arbitrary ϕ, are mapped to the Cartesian coordinates</p>
<p/>
</div>
<div class="page"><p/>
<p>21.2 Multidimensional GFs and Delta Functions 645
</p>
<p>(0,0,&minus;5). Similarly, the point (0,0,0) in the Cartesian coordinate system
goes to (0, θ, ϕ) in the spherical system, with θ and ϕ arbitrary. A coordi-
nate whose value is not determined at a singular point is called an ignorable ignorable coordinates
coordinate at that point. Thus, at the origin both θ and ϕ are ignorable.
</p>
<p>Among the ξ coordinates, let {ξi}mi=k+1 be ignorable at P with Cartesian
coordinates a. This means that any function, when expressed in terms of
ξ &rsquo;s, will be independent of the ignorable coordinates. A reexamination of
Eq. (21.10) reveals that (see Problem 21.8)
</p>
<p>δ(x&minus;a)= 1|Jk|
</p>
<p>k&prod;
</p>
<p>i=1
δ(ξi&minus;αi), where Jk =
</p>
<p>ˆ
</p>
<p>Jdξk+1 &middot; &middot; &middot; dξm. (21.11)
</p>
<p>In particular, if the transformation is invertible, k =m and Jm = J , and we
recover Jδ(x &minus; a)= δ(ξξξ &minus;ααα).
</p>
<p>Example 21.2.1 In two dimensions the transformation between Cartesian
and polar coordinates is given by x1 &equiv; x = r cos θ &equiv; ξ1 cos ξ2, x2 &equiv; y =
r sin θ &equiv; ξ1 sin ξ2 with the Jacobian
</p>
<p>J = det
(
&part;x1/&part;ξ1 &part;x1/&part;ξ2
&part;x2/&part;ξ1 &part;x2/&part;ξ2
</p>
<p>)
= det
</p>
<p>(
cos ξ2 &minus;ξ1 sin ξ2
sin ξ2 ξ1 cos ξ2
</p>
<p>)
= ξ1 = r,
</p>
<p>which vanishes at the origin. The angle θ is the only ignorable coordinate at
the origin. Thus, k = 2 &minus; 1 = 1, and
</p>
<p>J1 =
ˆ 2π
</p>
<p>0
J dθ =
</p>
<p>ˆ 2π
</p>
<p>0
r dθ = 2πr &rArr; δ(x)&equiv; δ(x)δ(y)= δ(r)
</p>
<p>2πr
.
</p>
<p>In three dimensions, the transformation between Cartesian and spherical
coordinates yields the Jacobian J = r2 sin θ . This vanishes at the origin re-
gardless of the values of θ and ϕ. We thus have two ignorable coordinates
at the origin (therefore, k = 3 &minus; 2 = 1), over which we integrate to obtain
</p>
<p>J1 =
ˆ 2π
</p>
<p>0
dϕ
</p>
<p>ˆ π
</p>
<p>0
dθr2 sin θ = 4πr2 &rArr; δ(x)= δ(r)
</p>
<p>4πr2
.
</p>
<p>21.2.1 Spherical Coordinates inm Dimensions
</p>
<p>In discussing Green&rsquo;s functions in m dimensions, a particular curvilinear
coordinate system will prove useful. This system is the generalization of
spherical coordinates in three dimensions. The m-dimensional spherical co-
ordinate system is defined as
</p>
<p>xk = r
(
m&minus;k&prod;
</p>
<p>j=1
sin θj
</p>
<p>)
cos θm&minus;k+1, k = 1, . . . ,m, (21.12)
</p>
<p>where, by definition, we set θm = 0 and
&prod;0
</p>
<p>j=1 sin θj = 1. (Note that for
m = 3, the first two Cartesian coordinates are switched compared to their
usual definitions.)</p>
<p/>
</div>
<div class="page"><p/>
<p>646 21 Multidimensional Green&rsquo;s Functions: Formalism
</p>
<p>It is not hard to show (see Example 21.2.2) that the Jacobian of the trans-
formation (21.12) is
</p>
<p>J = rm&minus;1(sin θ1)m&minus;2(sin θ2)m&minus;3 &middot; &middot; &middot; (sin θk)m&minus;k&minus;1 &middot; &middot; &middot; sin θm&minus;2 (21.13)
</p>
<p>and that the volume element in terms of these coordinates is
</p>
<p>dmx = J dr dθ1 &middot; &middot; &middot; dθm&minus;1 = rm&minus;1dr dΩm, (21.14)
</p>
<p>whereelement of the
m-dimensional solid
</p>
<p>angle dΩm = (sin θ1)
m&minus;2(sin θ2)m&minus;3 &middot; &middot; &middot; sin θm&minus;2dθ1dθ2 &middot; &middot; &middot; dθm&minus;1 (21.15)
</p>
<p>is the element of the m-dimensional solid angle.
</p>
<p>Example 21.2.2 For m= 4 we have
</p>
<p>x1 = r sin θ1 sin θ2 sin θ3, x2 = r sin θ1 sin θ2 cos θ3,
x3 = r sin θ1 cos θ2, x4 = r cos θ1,
</p>
<p>and the Jacobian is given by
</p>
<p>J = det
</p>
<p>⎛
⎜⎜⎝
</p>
<p>&part;x1/&part;r &part;x1/&part;θ1 &part;x1/&part;θ2 &part;x1/&part;θ3
&part;x2/&part;r &part;x2/&part;θ1 &part;x2/&part;θ2 &part;x2/&part;θ3
&part;x3/&part;r &part;x3/&part;θ1 &part;x3/&part;θ2 &part;x3/&part;θ3
&part;x4/&part;r &part;x4/&part;θ1 &part;x4/&part;θ2 &part;x4/&part;θ3
</p>
<p>⎞
⎟⎟⎠= r
</p>
<p>3 sin2 θ1 sin θ2.
</p>
<p>It is readily seen (one can use mathematical induction to prove it rig-
orously) that the Jacobians for m = 2 (J = r), m = 3 (J = r2 sin θ1), and
m= 4 (J = r3 sin2 θ1 sin θ2) generalize to Eq. (21.13).
</p>
<p>Using the integral
ˆ π
</p>
<p>0
sinn θ dθ =&radic;π Ŵ[(n+ 1)/2]
</p>
<p>Ŵ[(n+ 2)/2] ,
</p>
<p>the total solid angle in m dimensions can be found to be
</p>
<p>Ωm =
2πm/2
</p>
<p>Ŵ(m/2)
. (21.16)
</p>
<p>An interesting result that is readily obtained is an expression of the
delta function in terms of spherical coordinates at the origin. Since r = 0,
Eq. (21.12) shows that all the angles are ignorable. Thus, we have
</p>
<p>J1 =
ˆ
</p>
<p>J dθ1 &middot; &middot; &middot; dθm&minus;1 = rm&minus;1
ˆ
</p>
<p>dΩm = rm&minus;1Ωm,
</p>
<p>which yields
</p>
<p>δ(x)= δ(x1) &middot; &middot; &middot; δ(xm)=
δ(r)
</p>
<p>Ωmrm&minus;1
= Ŵ(m/2)δ(r)
</p>
<p>2πm/2rm&minus;1
. (21.17)</p>
<p/>
</div>
<div class="page"><p/>
<p>21.2 Multidimensional GFs and Delta Functions 647
</p>
<p>21.2.2 Green&rsquo;s Function for the Laplacian
</p>
<p>With the machinery developed above, we can easily obtain the (indefi-
nite) Green&rsquo;s function for the Laplacian in m dimensions. We will ignore
questions of BCs and simply develop a function that satisfies &nabla;2G(x,y)=
δ(x &minus; y). Without loss of generality we let y = 0; that is, we translate the
axes so that y becomes the new origin. Then we have &nabla;2G(x) = δ(x). In
spherical coordinates this becomes
</p>
<p>&nabla;2G(x)= δ(r)
Ωmrm&minus;1
</p>
<p>(21.18)
</p>
<p>by (21.17). Since the RHS is a function of r only, we expect G to behave in
the same way. We now have to express &nabla;2 in terms of spherical coordinates.
In general, this is difficult; however, for a function of r =
</p>
<p>&radic;
x21 + &middot; &middot; &middot; + x2m
</p>
<p>alone, such as F(r), we have
</p>
<p>&part;F
</p>
<p>&part;xi
= &part;F
</p>
<p>&part;r
</p>
<p>&part;r
</p>
<p>&part;xi
= &part;F
</p>
<p>&part;r
</p>
<p>xi
</p>
<p>r
and
</p>
<p>&part;2F
</p>
<p>&part;xi2
= &part;
</p>
<p>2F
</p>
<p>&part;r2
</p>
<p>x2i
</p>
<p>r2
+ &part;F
</p>
<p>&part;r
</p>
<p>(
1
</p>
<p>r
&minus; x
</p>
<p>2
i
</p>
<p>r3
</p>
<p>)
,
</p>
<p>so that
</p>
<p>&nabla;2F(r)=
m&sum;
</p>
<p>i=1
</p>
<p>&part;2F
</p>
<p>&part;xi2
= &part;
</p>
<p>2F
</p>
<p>&part;r2
+ m&minus; 1
</p>
<p>r
</p>
<p>&part;F
</p>
<p>&part;r
= 1
</p>
<p>rm&minus;1
&part;
</p>
<p>&part;r
</p>
<p>(
rm&minus;1
</p>
<p>&part;F
</p>
<p>&part;r
</p>
<p>)
.
</p>
<p>For the Green&rsquo;s function, therefore, we get
</p>
<p>d
</p>
<p>dr
</p>
<p>(
rm&minus;1
</p>
<p>dG
</p>
<p>dr
</p>
<p>)
= δ(r)
</p>
<p>Ωm
. (21.19)
</p>
<p>The solution, for m&ge; 3, is (see Problem 21.9)
</p>
<p>G(r)=&minus; Ŵ(m/2)
2(m&minus; 2)πm/2
</p>
<p>(
1
</p>
<p>rm&minus;2
</p>
<p>)
for m&ge; 3. (21.20)
</p>
<p>We can restore the vector y, at which we placed the origin, by noting that
r = |r| = |x &minus; y|. Thus, we get
</p>
<p>G(x,y)=&minus; Ŵ(m/2)
2(m&minus; 2)πm/2
</p>
<p>(
1
</p>
<p>|x &minus; y|m&minus;2
)
</p>
<p>=&minus; Ŵ(m/2)
2(m&minus; 2)πm/2
</p>
<p>[
m&sum;
</p>
<p>i=1
(xi &minus; yi)2
</p>
<p>]&minus;(m&minus;2)/2
for m&ge; 3.
</p>
<p>(21.21)
</p>
<p>Similarly, we obtain
</p>
<p>Green&rsquo;s function for the
</p>
<p>Laplacian
</p>
<p>G(x,y)= 1
2π
</p>
<p>ln |x &minus; y| = 1
4π
</p>
<p>ln
[
(x1 &minus; y1)2 + (x2 &minus; y2)2
</p>
<p>]
for m= 2.
</p>
<p>(21.22)</p>
<p/>
</div>
<div class="page"><p/>
<p>648 21 Multidimensional Green&rsquo;s Functions: Formalism
</p>
<p>Having found the Green&rsquo;s function for the Laplacian, we can find a so-solution of Poisson
equation inm
</p>
<p>dimensions
</p>
<p>lution to the inhomogeneous equation, the Poisson equation, &nabla;2u=&minus;ρ(x).
Thus, for m&ge; 3, we get
</p>
<p>u(x)=&minus;
ˆ
</p>
<p>dmyG(x,y)ρ(y)= Ŵ(m/2)
2(m&minus; 2)πm/2
</p>
<p>ˆ
</p>
<p>dmy
ρ(y)
</p>
<p>|x &minus; y|m&minus;2 .
</p>
<p>In particular, for m= 3, we obtain
</p>
<p>u(x)= 1
4π
</p>
<p>ˆ
</p>
<p>d3y
ρ(y)
|x &minus; y| ,
</p>
<p>which is the electrostatic potential due to a charge density ρ(y).
</p>
<p>21.3 Formal Development
</p>
<p>The preceding section was devoted to a discussion of the Green&rsquo;s function
for the Laplacian with no mention of the BCs. This section will develop a
formalism that not only works for more general operators, but also incorpo-
rates the BCs.
</p>
<p>21.3.1 General Properties
</p>
<p>Basic to a study of GFs is Green&rsquo;s identity, whose 1-dimensional version we
encountered in Chap. 20. Here, we generalize it to m dimensions. Suppose
there exist two differential operators, Lx and L
</p>
<p>&dagger;
x, which for any two functions
</p>
<p>u and v, satisfy the following relation:3
</p>
<p>v&lowast;Lx[u] &minus; u
(
L&dagger;x[v]
</p>
<p>)&lowast; =&nabla;&nabla;&nabla; &middot; Q
[
u,v&lowast;
</p>
<p>]
&equiv;
</p>
<p>m&sum;
</p>
<p>i=1
</p>
<p>&part;Qi
</p>
<p>&part;xi
</p>
<p>[
u,v&lowast;
</p>
<p>]
. (21.23)
</p>
<p>The differential operator L&dagger;x is&mdash;as in the one-dimensional case&mdash;called the
formal adjoint of Lx. Integrating (21.23) over a closed domain D in Rm with
boundary &part;D, and using the divergence theorem, we obtain
</p>
<p>ˆ
</p>
<p>D
</p>
<p>dmx
{
v&lowast;Lx[u] &minus; u
</p>
<p>(
L&dagger;x[v]
</p>
<p>)&lowast;}=
ˆ
</p>
<p>&part;D
</p>
<p>Q &middot; ên da, (21.24)
</p>
<p>where ên is an m-dimensional unit vector normal to &part;D, and da is an ele-
ment of &ldquo;area&rdquo; of the m-dimensional hypersurface &part;D. Equation (21.24) is
the generalized Green&rsquo;s identity for m dimensions. Note that the weightgeneralized Green&rsquo;s
</p>
<p>identity function is set equal to one for simplicity.
</p>
<p>3The notions of divergence and divergence theorem require the machinery of differential
geometry to which we shall come back later. Here, we are simply using a direct and most
obvious generalization of the notions from three to m dimensions.</p>
<p/>
</div>
<div class="page"><p/>
<p>21.3 Formal Development 649
</p>
<p>The differential operator Lx is said to be formally self-adjoint if the RHS
of Eq. (21.24), the surface term, vanishes. In such a case, we have Lx = L&dagger;x as
in one dimension. This relation is a necessary condition for the surface term
to vanish because u and v are, by assumption, arbitrary. Lx is called self-
adjoint (or, somewhat imprecisely, hermitian) if Lx = L&dagger;x and the domains of
the two operators, as determined by the vanishing of the surface term, are
identical.
</p>
<p>We can use Eq. (21.24) to study the pair of PDEs
</p>
<p>Lx[u] = f (x) and L&dagger;x[v] = h(x). (21.25)
</p>
<p>As in one dimension, we let G(x,y) and g(x,y) denote the Green&rsquo;s func-
tions for Lx and L
</p>
<p>&dagger;
x, respectively. Let us assume that the BCs are such that
</p>
<p>the surface term in Eq. (21.24) vanishes. Then we get Green&rsquo;s identity
</p>
<p>ˆ
</p>
<p>D
</p>
<p>dmxv&lowast;Lx[u] =
ˆ
</p>
<p>D
</p>
<p>dmxu
(
L&dagger;x[v]
</p>
<p>)&lowast;
. (21.26)
</p>
<p>If in this equation we let u = G(x, t) and v = g(x,y), where t,y &isin; D, we
obtain
</p>
<p>ˆ
</p>
<p>D
</p>
<p>dmxg&lowast;(x,y)δ(x &minus; t)=
ˆ
</p>
<p>D
</p>
<p>dmxG(x, t)δ(x &minus; y),
</p>
<p>or g&lowast;(t,y)=G(y, t). In particular, when Lx is formally self-adjoint, we have
G&lowast;(t,y)=G(y, t), or G(t,y)=G(y, t), if all the coefficient functions of Lx
are real. That is, the Green&rsquo;s function will be symmetric.
</p>
<p>If we let v = g(x,y) and use the first equation of (21.25) in (21.26), we Green&rsquo;s functions are
symmetric functions of
</p>
<p>their arguments
</p>
<p>get u(y)=
&acute;
</p>
<p>D
dmxg&lowast;(x,y)f (x), which, using g&lowast;(t,y)=G(y, t) and inter-
</p>
<p>changing x and y, becomes u(x)=
&acute;
</p>
<p>D
dmyG(x,y)f (y). It can similarly be
</p>
<p>shown that v(x)=
&acute;
</p>
<p>D
dmyg(x,y)h(y).
</p>
<p>21.3.2 Fundamental (Singular) Solutions
</p>
<p>The inhomogeneous term of the differential equation to which G(x,y) is a
solution is the delta function, δ(x&minus; y). It would be surprising if G(x,y) did
not &ldquo;take notice&rdquo; of this catastrophic source term and did not adapt itself
to behave differently at x = y than at any other &ldquo;ordinary&rdquo; point. We noted
the singular behavior of the Green&rsquo;s function at x = y in one dimension
when we proved Theorem 20.3.5. There we introduced h(x, y)&mdash;which was
discontinuous at x = y&mdash;as a part of the Green&rsquo;s function. Similarly, when
we discussed the Green&rsquo;s functions for the Laplacian in two and m dimen-
sions earlier in this chapter, we noted that they behaved singularly at r = 0
or x = y. In this section, we study similar properties of the GFs for other
differential operators.
</p>
<p>Next to the Laplacian in difficulty is the formally self-adjoint elliptic
PDO Lx =&nabla;2 +q(x) discussed in Problem 21.10. Substituting this operator
in the generalized Green&rsquo;s identity and using the expression for Q given in</p>
<p/>
</div>
<div class="page"><p/>
<p>650 21 Multidimensional Green&rsquo;s Functions: Formalism
</p>
<p>Problem 21.10, we obtain
ˆ
</p>
<p>D
</p>
<p>dmx
{
vLx[u] &minus; u
</p>
<p>(
Lx[v]
</p>
<p>)}
=
ˆ
</p>
<p>&part;D
</p>
<p>(vên &middot; &nabla;&nabla;&nabla;u&minus; uên &middot; &nabla;&nabla;&nabla;v)da.
</p>
<p>Letting v =G(x,y) and denoting ên &middot; &nabla;&nabla;&nabla; by &part;/&part;n gives
ˆ
</p>
<p>D
</p>
<p>dmx[GLxu&minus; uLxG] =
ˆ
</p>
<p>&part;D
</p>
<p>[
G
&part;u
</p>
<p>&part;n
&minus; u&part;G
</p>
<p>&part;n
</p>
<p>]
da. (21.27)
</p>
<p>We want to use this equation to find out about the behavior of G(x,y) as
|x &minus; y| &rarr; 0. Therefore, assuming that y &isin; D, we divide the domain D
into two parts: one part is a region Dǫ bounded by an infinitesimal hyper-
sphere Sǫ with radius ǫ and center at y; the other is the rest of D. Instead of
D we use the region D&prime; &equiv;D &minus;Dǫ . The following facts are easily deduced
for D&prime;:
</p>
<p>(1) LxG(x,y)= 0 because x �= y in D&prime;;
(2)
</p>
<p>&acute;
</p>
<p>D
= limǫ&rarr;0
</p>
<p>&acute;
</p>
<p>D&prime; ;
(3) &part;D&prime; = &part;D &cup; Sǫ .
</p>
<p>Suppose that we are interested in finding a solution to
</p>
<p>Lx[u] =
[
&nabla;2 + q(x)
</p>
<p>]
u(x)= f (x)
</p>
<p>subject to certain, as yet unspecified, BCs. Using the three facts listed above,
Eq. (21.27) yields
</p>
<p>ˆ
</p>
<p>D
</p>
<p>dmx[GLxu&minus; uLxG]
</p>
<p>= lim
ǫ&rarr;0
</p>
<p>ˆ
</p>
<p>D&prime;
dmx[G Lxu︸︷︷︸
</p>
<p>=f
</p>
<p>&minus;u LxG︸︷︷︸
=0
</p>
<p>]
</p>
<p>= lim
ǫ&rarr;0
</p>
<p>ˆ
</p>
<p>D&prime;
dmxG(x,y)f (x)=
</p>
<p>ˆ
</p>
<p>D
</p>
<p>dmxG(x,y)f (x)
</p>
<p>=
ˆ
</p>
<p>&part;D
</p>
<p>(
G
&part;u
</p>
<p>&part;n
&minus; u&part;G
</p>
<p>&part;n
</p>
<p>)
da +
</p>
<p>ˆ
</p>
<p>Sǫ
</p>
<p>(
G
&part;u
</p>
<p>&part;n
&minus; u&part;G
</p>
<p>&part;n
</p>
<p>)
da.
</p>
<p>We assume that the BCs are such that the integral over &part;D vanishes. This
is a generalization of the one-dimensional case (recall from Chap. 20 that
this is a necessary condition for the existence of Green&rsquo;s functions). More-
over, for an m-dimensional sphere, da = rm&minus;1dΩm, which for Sǫ reduces
to ǫm&minus;1dΩm. Substituting in the preceding equation yields
</p>
<p>ˆ
</p>
<p>D
</p>
<p>dmxG(x,y)f (x)=
ˆ
</p>
<p>Sǫ
</p>
<p>(
G
&part;u
</p>
<p>&part;n
&minus; u&part;G
</p>
<p>&part;n
</p>
<p>)
ǫm&minus;1dΩm.
</p>
<p>We would like the RHS to be u(y). This will be the case if
</p>
<p>lim
ǫ&rarr;0
</p>
<p>ˆ
</p>
<p>Sǫ
</p>
<p>G(x,y)
&part;u
</p>
<p>&part;n
ǫm&minus;1dΩm = 0 and lim
</p>
<p>ǫ&rarr;0
</p>
<p>ˆ
</p>
<p>Sǫ
</p>
<p>u
&part;G
</p>
<p>&part;n
ǫm&minus;1dΩm = u(y)</p>
<p/>
</div>
<div class="page"><p/>
<p>21.3 Formal Development 651
</p>
<p>for arbitrary u. This will happen only if
</p>
<p>lim
r&rarr;0
</p>
<p>G(y + r,y)rm&minus;1 = 0, lim
r&rarr;0
</p>
<p>&part;G
</p>
<p>&part;r
(y + r,y)rm&minus;1 = const. (21.28)
</p>
<p>A solution to these two equations is
</p>
<p>G(x,y)=
</p>
<p>⎧
⎨
⎩
&minus;F(x,y)2π ln(|x &minus; y|)+H(x,y) if m= 2,
&minus; 1
</p>
<p>(m&minus;2)Ωm
F(x,y)
</p>
<p>|x&minus;y|m&minus;2 +H(x,y) if m&ge; 3,
(21.29)
</p>
<p>where H(x,y) and F(x,y) are well behaved at x = y. The introduction of
these functions is necessary because Eq. (21.28) determines the behavior
of G(x,y) only when x &asymp; y. Such behavior does not uniquely determine
G(x,y). For instance, e|x&minus;y| ln(|x &minus; y|) and ln(|x &minus; y|) behave in the same
way as |x &minus; y|&rarr; 0.
</p>
<p>Equation (21.29) shows that for Lx = &nabla;2 + q(x), the Green&rsquo;s function
consists of two parts. The first part determines the singular behavior of the
Green&rsquo;s function as x &rarr; y. The nature of this singularity (how badly the GF
&ldquo;blows up&rdquo; as x &rarr; y) is extremely important, because it is a prerequisite for
our ability to write the solution in terms of an integral representation with the
Green&rsquo;s function as its kernel. Due to their importance in such representa-
tions, the first terms on the RHS of Eq. (21.29) are called the fundamental fundamental solution is
</p>
<p>the singular part of GFsolution of the differential equation, or the singular part of the Green&rsquo;s
function.
</p>
<p>What about the second part of the Green&rsquo;s function? What role does it
play in obtaining a solution? So far we have been avoiding consideration of
BCs. Here H(x,y) can help. We choose H(x,y) in such a way that G(x,y)
satisfies the appropriate BCs. Let us discuss this in greater detail and gener-
ality.
</p>
<p>If BCs are ignored, the Green&rsquo;s function for a SOPDO Lx cannot be de-
termined uniquely. In particular, if G(x,y) is a Green&rsquo;s function, that is, if homogeneous solution
</p>
<p>is the regular part of GFLxG(x,y)= δ(x &minus; y), then so is G(x,y)+H(x,y) as long as H(x,y) is a
solution of the homogeneous equation LxH(x,y) = 0. Thus, we can break
the Green&rsquo;s function into two parts:
</p>
<p>G=Gs +H, where LxGs(x,y)= δ(x&minus; y), LxH(x,y)= 0 (21.30)
</p>
<p>with Gs the singular part of the Green&rsquo;s function. H is called the regular regular part of the
Green&rsquo;s functionpart of the Green&rsquo;s function. Neither Gs nor H (nor G, therefore) is unique.
</p>
<p>However, the appropriate BCs, which depend on the type of Lx, will deter-
mine G uniquely.
</p>
<p>To be more specific, let us assume that we want to find a Green&rsquo;s function
for Lx that vanishes at the boundary &part;D. That is, we wish to find G(x,y)
such that G(xb,y)= 0, where xb is an arbitrary point of the boundary. All
that is required is to find a Gs and an H satisfying Eq. (21.30) with the
BC H(xb,y) = &minus;Gs(xb,y). The latter problem, involving a homogeneous
differential equation, can be handled by the methods of Chap. 19. Since any
discussion of BCs is tied to the type of PDE, we have reserved the discussion
of such specifics for the next chapter.</p>
<p/>
</div>
<div class="page"><p/>
<p>652 21 Multidimensional Green&rsquo;s Functions: Formalism
</p>
<p>21.4 Integral Equations and GFs
</p>
<p>Integral equations are best applied in combination with Green&rsquo;s functions.
In fact, we can use a Green&rsquo;s function to turn a DE into an integral equation.
If this integral equation is compact or has a compact resolvent, then the
problem lends itself to the methods described in Chaps. 17 and 18.
</p>
<p>Let Lx be a SOPDO in m variables. We are interested in solving the
SOPDE
</p>
<p>Lx[u] + λV (x)u(x)= f (x)
subject to some BCs. Here λ is an arbitrary constant, and V (x) is a well-
behaved function on Rm. Transferring the second term on the LHS to the
RHS and then treating the RHS as an inhomogeneous term, we can write
the &ldquo;solution&rdquo; to the PDE as
</p>
<p>u(x)=H(x)+
ˆ
</p>
<p>D
</p>
<p>dmyG0(x,y)
[
f (y)&minus; λV (y)u(y)
</p>
<p>]
,
</p>
<p>where D is the domain of Lx and G0 is the Green&rsquo;s function for Lx with
some, as yet unspecified, BCs. The function H is a solution to the homoge-
neous equation, and it is present to guarantee the appropriate BCs.
</p>
<p>Combining the first term in the integral with H(x), we have
</p>
<p>u(x)= F(x)&minus; λ
ˆ
</p>
<p>D
</p>
<p>dmyG0(x,y)V (y)u(y). (21.31)
</p>
<p>Equation (21.31) is an m-dimensional Fredholm equation whose solution
can be obtained in the form of a Neumann series.
</p>
<p>Example 21.4.1 Consider the bound-state Schr&ouml;dinger equation in one di-
mension:
</p>
<p>&minus; �
2
</p>
<p>2μ
</p>
<p>d2Ψ
</p>
<p>dx2
+ V (x)Ψ (x)=EΨ (x), E &lt; 0.
</p>
<p>We rewrite this equation as
</p>
<p>Lx[Ψ ] &equiv;
(
</p>
<p>d2
</p>
<p>dx2
&minus; κ2
</p>
<p>)
Ψ (x)= 2μ
</p>
<p>�2
V (x)Ψ (x),
</p>
<p>where κ2 = &minus;2μE/�2 &gt; 0. Equation (21.31) gives the equivalent integral
equation
</p>
<p>Ψ (x)= Ψ0(x)+
2μ
</p>
<p>�2
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
G0(x, y)V (y)Ψ (y)dy
</p>
<p>where Ψ0(x) is the solution of Lx[Ψ0] = 0, which is easily found to be of
the general form Ψ0(x)=Aeκx +Be&minus;κx . If we assume that Ψ0(x) remains
finite as x &rarr;&plusmn;&infin;, Ψ0(x) will be zero. Furthermore, it can be shown that
G0(x, y)=&minus;e&minus;κ|x&minus;y|/2κ (see Problem 20.12). Therefore,
</p>
<p>Ψ (x)=&minus; μ
�2κ
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
e&minus;κ|x&minus;y|V (y)Ψ (y)dy.</p>
<p/>
</div>
<div class="page"><p/>
<p>21.4 Integral Equations and GFs 653
</p>
<p>Now consider an attractive delta-function potential with center at a:
</p>
<p>V (x)=&minus;V0δ(x &minus; a), V0 &gt; 0.
</p>
<p>For such a potential, the integral equation yields
</p>
<p>Ψ (x)= μ
�2κ
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
e&minus;κ|x&minus;y|V0δ(y &minus; a)Ψ (y)dy =
</p>
<p>μV0
</p>
<p>�2κ
e&minus;κ|x&minus;a|Ψ (a).
</p>
<p>For this equation to be consistent, i.e., to get an identity when x = a, we
must have
</p>
<p>μV0
</p>
<p>�2κ
= 1 &rArr; κ = μV0
</p>
<p>�2
&rArr; E =&minus;μV0
</p>
<p>2�2
.
</p>
<p>Therefore, there is only one bound state and one energy level for an attrac-
tive delta-function potential.
</p>
<p>There is only one
</p>
<p>nondegenerate
</p>
<p>quantum state for an
</p>
<p>attractive delta function
</p>
<p>potential.
</p>
<p>To find a Neumann-series solution we can substitute the expression for
u given by the RHS of Eq. (21.31) in the integral of that equation. The
resulting equation will have two integrals, in the second of which u appears.
Substituting the new u in the second integral and continuing the process N
times yields
</p>
<p>u(x)= F(x)+
N&minus;1&sum;
</p>
<p>n=1
(&minus;λ)n
</p>
<p>ˆ
</p>
<p>D
</p>
<p>dmyKn(x,y)F (y)
</p>
<p>+ (&minus;λ)N
ˆ
</p>
<p>D
</p>
<p>dmyKN (x,y)u(y),
</p>
<p>where
</p>
<p>K(x,y)&equiv; V (x)G0(x,y),
</p>
<p>Kn(x,y)&equiv;
ˆ
</p>
<p>D
</p>
<p>dmtKn&minus;1(x, t)K(t,y) for n&ge; 2.
(21.32)
</p>
<p>The Neumann series is obtained by letting N &rarr;&infin;:
</p>
<p>u(x)= F(x)+
&infin;&sum;
</p>
<p>n=1
(&minus;λ)n
</p>
<p>ˆ
</p>
<p>D
</p>
<p>dmyKn(x,y)F (y). (21.33)
</p>
<p>Except for the fact that here the integrations are in m variables, Eq. (21.33)
is the same as the Neumann series derived in Sect. 18.1. In exact analogy,
therefore, we abbreviate (21.33) as
</p>
<p>|u〉 = |F 〉 +
&infin;&sum;
</p>
<p>n=1
(&minus;λ)nKn|F 〉. (21.34)
</p>
<p>Equations (21.33) and (21.34) have meaning only if the Neumann series
converges, i.e., if
</p>
<p>|λ|
[
ˆ
</p>
<p>D
</p>
<p>dmy
</p>
<p>ˆ
</p>
<p>D
</p>
<p>dmx
∣∣K(x,y)
</p>
<p>∣∣2
]1/2
</p>
<p>&lt; 1. (21.35)</p>
<p/>
</div>
<div class="page"><p/>
<p>654 21 Multidimensional Green&rsquo;s Functions: Formalism
</p>
<p>We will briefly discuss an intuitive physical interpretation of the Neumann
series due to Feynman. Although Feynman developed this diagrammaticFeynman&rsquo;s diagrammatic
</p>
<p>representation of GF technique for quantum electrodynamics, it has been useful in other areas,
such as statistical and condensed matter physics. In most cases of interest,
the SOPDE is homogeneous, so f (x) = 0. In that case, Lx and V (x) are
called the free operator and the interacting potential, respectively. The so-
lution to Lx[u] = 0 is called the free solution and denoted by uf (x).
</p>
<p>Let us start with Eq. (21.31) written as
</p>
<p>u(x)= uf (x)&minus; λ
ˆ
</p>
<p>Rm
dmyG0(x,y)V (y)u(y), (21.36)
</p>
<p>where G0 stands for the Green&rsquo;s function for the free operator Lx. The full
Green&rsquo;s function, that is, that for Lx + λV , will be denoted by G. Moreover,
as is usually the case, the region D has been taken to be all of Rm. This
implies that no boundary conditions are imposed on u, which in turn permits
us to use the singular part of the Green&rsquo;s function in the integral. Because
of the importance of the full Green&rsquo;s function, we are interested in finding a
series for G in terms of G0, which is supposed to be known. To obtain such
a series we start with the abstract operator equation and write G= G0 + A,
where A is to be determined. Operating on both sides with L (&ldquo;inverse&rdquo; of
G0), we obtain LG= LG0+LA= 1+LA. On the other hand, (L+λV)G= 1,
or LG= 1&minus; λVG. These two equations give
</p>
<p>LA=&minus;λVG &rArr; A=&minus;λL&minus;1VG=&minus;λG0VG.
</p>
<p>Therefore,
</p>
<p>G= G0 &minus; λG0VG. (21.37)
Sandwiching both sides between 〈x| and |z〉, inserting 1=
</p>
<p>&acute;
</p>
<p>|y〉〈y|dmy be-
tween G0 and V and 1=
</p>
<p>&acute;
</p>
<p>|t〉〈t|dmt between V and G, and assuming that V
is local [i.e., V (y, t)= V (y)δ(y &minus; t)], we obtain
</p>
<p>G(x, z)=G0(x, z)&minus; λ
ˆ
</p>
<p>dmyG0(x,y)V (y)G(y, z). (21.38)
</p>
<p>This equation is the analogue of (21.31) and, just like that equation, is
amenable to a Neumann series expansion. The result is
</p>
<p>G(x,y)=G0(x,y)+
&infin;&sum;
</p>
<p>n=1
(&minus;λ)n
</p>
<p>ˆ
</p>
<p>Rm
dmzG0(x, z)Kn(z,y), (21.39)
</p>
<p>where Kn(x, z) is as given in Eq. (21.32).
Feynman&rsquo;s idea is to consider G(x,y) as an interacting propagator be-GF as propagator
</p>
<p>tween points x and y and G0(x,y) as a free propagator. The first term on
the RHS of (21.39) is simply a free propagation from x to y. Diagrammati-
cally, it is represented by a line joining the points x and y [see Fig. 21.1(a)].
The second term is a free propagation from x to y1 (also called a vertex),
interaction at y1 with a potential &minus;λV (y1), and subsequent free propaga-
tion to y [see Fig. 21.1(b)]. According to the third term, the particle or wave</p>
<p/>
</div>
<div class="page"><p/>
<p>21.5 Perturbation Theory 655
</p>
<p>Fig. 21.1 Contributions to the full propagator in (a) the zeroth order, (b) the first order,
and (c) the second order. At each vertex one introduces a factor of &minus;λV and integrates
over all values of the variable of that vertex
</p>
<p>[represented by uf (x)] propagates freely from x to y1, interacts at y1 with
the potential &minus;λV (y1), propagates freely from y1 to y2, interacts for a sec-
ond time with the potential &minus;λV (y2), and finally propagates freely from y2
to y [Fig. 21.1(c)]. The interpretation of the rest of the series in (21.39) is
now clear: The nth-order term of the series has n vertices between x and y
with a factor &minus;λV (yk) and an integration over yk at vertex k. Between any
two consecutive vertices yk and yk+1 there is a factor of the free propagator
G0(yk,yk+1).
</p>
<p>Feynman diagrams are used extensively in relativistic quantum field
theory, for which m = 4, corresponding to the four-dimensional space&ndash;
time. In this context λ is determined by the strength of the interaction.
For quantum electrodynamics, for instance, λ is the fine-structure constant,
e2/�c= 1/137.
</p>
<p>21.5 Perturbation Theory
</p>
<p>Few operator equations lend themselves to an exact solution, and due to the
urgency of finding a solution to such equations in fundamental physics, var-
ious techniques have been developed to approximate solutions to operator
equations. We have already seen instances of such techniques in, for exam-
ple, the WKB method. This section is devoted to a systematic development
of perturbation theory, which is one of the main tools of calculation in quan-
tum mechanics. For a thorough treatment of perturbation theory along the
lines presented here, see [Mess 66, pp. 712&ndash;720].
</p>
<p>The starting point is the resolvent (Definition 17.7.1) of a Hamiltonian
H, which, using z instead of λ, we write as Rz(H). For simplicity, we as-
sume that the eigenvalues of H are discrete. This is a valid assumption if the
Hamiltonian is compact or if we are interested in approximations close to
one of the discrete eigenvalues. Denoting the eigenvalues of H by {Ei}&infin;i=0,
we have
</p>
<p>HPi =EiPi, (21.40)
where Pi is the projection operator to the ith eigenspace. We can write the
resolvent in terms of the projection operators by using Eq. (17.6):
</p>
<p>Rz(H)=
&infin;&sum;
</p>
<p>i=0
</p>
<p>Pi
</p>
<p>Ei &minus; z
. (21.41)</p>
<p/>
</div>
<div class="page"><p/>
<p>656 21 Multidimensional Green&rsquo;s Functions: Formalism
</p>
<p>The projection operator Pi can be written as a contour integral as in
Eq. (17.11). Any sum of these operators can also be written as a contour
integral. For instance, if Ŵ is a circle enclosing the first n+ 1 eigenvalues,
then
</p>
<p>PŴ &equiv;
n&sum;
</p>
<p>i=0
Pi =&minus;
</p>
<p>1
</p>
<p>2πi
</p>
<p>˛
</p>
<p>Ŵ
</p>
<p>Rz(H) dz. (21.42)
</p>
<p>Multiplying Eq. (21.42) by H and using the definition of the resolvent, one
can show that
</p>
<p>HPŴ =&minus;
1
</p>
<p>2πi
</p>
<p>˛
</p>
<p>Ŵ
</p>
<p>zRz(H) dz. (21.43)
</p>
<p>When Ŵ includes all eigenvalues of H, PŴ = 1, and Eq. (21.43) reduces to
(17.10) with A &rarr; T and f (x)&rarr; x.
</p>
<p>To proceed, let us assume that H= H0 + λV where H0 is a Hamiltonian
with known eigenvalues and eigenvectors, and V is a perturbing potential;perturbing potential
λ is a (small) parameter that keeps track of the order of approximation. Let
us also use the abbreviations
</p>
<p>G(z)&equiv;&minus;Rz(H) and G0(z)&equiv;&minus;Rz(H0). (21.44)
</p>
<p>Then a procedure very similar to that leading to Eq. (21.37) yields
</p>
<p>G(z)= G0(z)+ λG0(z)VG(z), (21.45)
</p>
<p>which can be expanded in a Neumann series by iteration:
</p>
<p>G(z)=
&infin;&sum;
</p>
<p>n=0
λnG0(z)
</p>
<p>[
VG0(z)
</p>
<p>]n
. (21.46)
</p>
<p>Let {E0a}, {M0a}, and ma denote, respectively, the eigenvalues of H0, their
corresponding eigenspaces, and the latter&rsquo;s dimensions.4 In the context of
perturbation theory, ma is called the degeneracy of E0a , and E
</p>
<p>0
a is called ma-Degeneracy is the
</p>
<p>dimension of the
</p>
<p>eigenspace of the
</p>
<p>Hamiltonian.
</p>
<p>fold degenerate, with a similar terminology for the perturbed Hamiltonian.
We assume that all eigenspaces have finite dimensions.
</p>
<p>It is clear that eigenvalues and eigenspaces of H will tend to those of H0
when λ &rarr; 0. So, let us collect all eigenspaces of H that tend to M0a and
denote them by {Mai }
</p>
<p>ra
i=1. Similarly, we use E
</p>
<p>a
i and P
</p>
<p>a
i to denote, respec-
</p>
<p>tively, the energy eigenvalue and the projector to the eigenspace Mai . Since
dimension is a discrete quantity, it cannot depend on λ, and we have
</p>
<p>ra&sum;
</p>
<p>i=1
dimMai = dimM0a =ma . (21.47)
</p>
<p>4We use the beginning letters of the Latin alphabet for the unperturbed Hamiltonian.
Furthermore, we attach a superscript &ldquo;0&rdquo; to emphasize that the object belongs to H0.</p>
<p/>
</div>
<div class="page"><p/>
<p>21.5 Perturbation Theory 657
</p>
<p>We also use the notation P for the projector onto the direct sum of Mai &rsquo;s. We
thus have
</p>
<p>P&equiv;
ra&sum;
</p>
<p>i=1
Pai and lim
</p>
<p>λ&rarr;0
P= P0a, (21.48)
</p>
<p>where we have used an obvious notation for the projection operator onto
M0a .
</p>
<p>The main task of perturbation theory is to find the eigenvalues and eigen-
vectors of the perturbed Hamiltonian in terms of a series in powers of λ
of the corresponding unperturbed quantities. Since the eigenvectors&mdash;or,
more appropriately, the projectors onto eigenspaces&mdash;and their correspond-
ing eigenvalues of the perturbed Hamiltonian are related via Eq. (21.40),
this task reduces to writing P as a series in powers of λ whose coefficients
are operators expressible in terms of unperturbed quantities.
</p>
<p>For sufficiently small λ, there exists a contour in the z-plane enclosing
E0a and all E
</p>
<p>a
i &rsquo;s but excluding all other eigenvalues of H and H0. Denote
</p>
<p>this contour by Ŵa and, using Eq. (21.42), write
</p>
<p>P= 1
2πi
</p>
<p>˛
</p>
<p>Ŵa
</p>
<p>G(z) dz.
</p>
<p>It follows from Eq. (21.46) that
</p>
<p>P= P0a +
&infin;&sum;
</p>
<p>n=1
λnA(n), where
</p>
<p>A(n) &equiv; 1
2πi
</p>
<p>˛
</p>
<p>Ŵa
</p>
<p>G0(z)
[
VG0(z)
</p>
<p>]n
dz. (21.49)
</p>
<p>This equation shows that perturbation expansion is reduced to the calcula-
tion of A(n), which is simply the residue of G0(z)[VG0(z)]n. The only singu-
larity of the integrand in Eq. (21.49) comes from G0(z), which, by (21.44)
and (21.41), has a pole at E0a . So, to calculate this residue, we simply expand
G0(z) in a Laurent series about E0a :
</p>
<p>G0(z)=
&sum;
</p>
<p>b
</p>
<p>P0b
</p>
<p>z&minus;E0b
</p>
<p>= P
0
a
</p>
<p>z&minus;E0a
+
&sum;
</p>
<p>b �=a
</p>
<p>P0b
</p>
<p>z&minus;E0b
</p>
<p>= P
0
a
</p>
<p>z&minus;E0a
+
&sum;
</p>
<p>b �=a
</p>
<p>P0b
</p>
<p>(E0a &minus;E0b)(1 +
z&minus;E0a
E0a&minus;E0b
</p>
<p>)
</p>
<p>= P
0
a
</p>
<p>z&minus;E0a
+
&sum;
</p>
<p>b �=a
</p>
<p>&infin;&sum;
</p>
<p>k=0
(&minus;1)k (z&minus;E
</p>
<p>0
a)
</p>
<p>kP0b
</p>
<p>(E0a &minus;E0b)k+1
.</p>
<p/>
</div>
<div class="page"><p/>
<p>658 21 Multidimensional Green&rsquo;s Functions: Formalism
</p>
<p>Switching the order of the two sums, and noting that our space is the Hilbert
space of H0 whose basis can be chosen to consist of eigenstates of H0, we
can write H0 instead of E0b in the denominator to obtain
</p>
<p>&sum;
</p>
<p>b �=a
</p>
<p>P0b
</p>
<p>(E0a &minus;E0b)k+1
=
&sum;
</p>
<p>b �=a
</p>
<p>P0b
(E0a &minus;H0)k+1
</p>
<p>=
&sum;
</p>
<p>b �=a P
0
b
</p>
<p>(E0a &minus;H0)k+1
=
</p>
<p>&equiv;Q0a︷ ︸︸ ︷
1&minus; P0a
</p>
<p>(E0a &minus;H0)k+1
</p>
<p>= Q
0
a
</p>
<p>(E0a &minus;H0)k+1
&equiv; Gk+10
</p>
<p>(
E0a
</p>
<p>)
Q0a =Q0aGk+10
</p>
<p>(
E0a
</p>
<p>)
Q0a,
</p>
<p>where we have used the completeness relation for the P0b&rsquo;s, the fact that Q
0
a
</p>
<p>commutes with H0 [and, therefore, with G
k+1
0 (E
</p>
<p>0
a)], and, in the last equality,
</p>
<p>the fact that Q0a is a projection operator.
5 It follows that
</p>
<p>G0(z)=
P0a
</p>
<p>z&minus;E0a
+
</p>
<p>&infin;&sum;
</p>
<p>k=0
(&minus;1)k
</p>
<p>(
z&minus;E0a
</p>
<p>)k
Q0aG
</p>
<p>k+1
0
</p>
<p>(
E0a
</p>
<p>)
Q0a
</p>
<p>=
&infin;&sum;
</p>
<p>k=0
(&minus;1)k
</p>
<p>(
z&minus;E0a
</p>
<p>)k&minus;1
Sk, (21.50)
</p>
<p>where we have introduced the notation
</p>
<p>Sk &equiv;
{
P0a if k = 0,
&minus;Q0aGk0(E0a)Q0a if k &ge; 1.
</p>
<p>By substituting Eq. (21.50) in G0(z)[VG0(z)]n we obtain a Laurent ex-
pansion whose coefficient of (z&minus;E0a)&minus;1 is A(n). The reader may check that
such a procedure yields
</p>
<p>A(n) = (&minus;1)n+1
&sum;
</p>
<p>(n)
</p>
<p>Sk1VSk2V &middot; &middot; &middot;VSkn+1 , (21.51)
</p>
<p>where by definition,
&sum;
</p>
<p>(p) extends over all nonnegative integers {ki}n+1i=1
such that
</p>
<p>n+1&sum;
</p>
<p>i=1
ki = p &forall;p &ge; 0.
</p>
<p>5Note that although G0(z) has a pole at E0a , the expressions in the last line of the equation
</p>
<p>above make sense because Q0a annihilates all states with eigenvalue E
0
a . The reason for the
</p>
<p>introduction of Q0a on both sides is to ensure that G
k+1
0 (E
</p>
<p>0
a) will not act on an eigenstate
</p>
<p>of E0a on either side.</p>
<p/>
</div>
<div class="page"><p/>
<p>21.5 Perturbation Theory 659
</p>
<p>It turns out that for perturbation expansion, not only do we need the expan-
sion of P [Eqs. (21.49) and (21.51)], but also an expansion for HP. Using
Eqs. (21.43) and (21.44), with Ŵ replaced by Ŵa , we have
</p>
<p>HP= 1
2πi
</p>
<p>˛
</p>
<p>Ŵa
</p>
<p>zG(z) dz= 1
2πi
</p>
<p>˛
</p>
<p>Ŵa
</p>
<p>(
z&minus;E0a +E0a
</p>
<p>)
G(z) dz
</p>
<p>= 1
2πi
</p>
<p>˛
</p>
<p>Ŵa
</p>
<p>(
z&minus;E0a
</p>
<p>)
G(z) dz+E0aP.
</p>
<p>Substituting for G(z) from Eq. (21.46), we can rewrite this equation as
</p>
<p>(
H&minus;E0a
</p>
<p>)
P=
</p>
<p>&infin;&sum;
</p>
<p>n=1
λnB(n), (21.52)
</p>
<p>where
</p>
<p>B(n) = (&minus;1)n&minus;1
&sum;
</p>
<p>(n&minus;1)
Sk1VSk2V &middot; &middot; &middot;VSkn+1 . (21.53)
</p>
<p>Equations (21.52) and (21.53) can be used to approximate the eigenvec-
tors and eigenvalues of the perturbed Hamiltonian in terms of those of the
unperturbed Hamiltonian. It is convenient to consider two cases: the nonde-
generate case in which ma = 1, and the degenerate case in which ma &ge; 2.
</p>
<p>21.5.1 The Nondegenerate Case
</p>
<p>In the nondegenerate case, we let |0a〉 denote the original unperturbed eigen-
state, and use Eq. (21.47) to conclude that the perturbed eigenstate is also
one-dimensional. In fact, it follows from (21.40) that P|0a〉 is the desired
eigenstate. Denoting the latter by |ψ〉 and using Eq. (21.49), we have
</p>
<p>∣∣ψ
&rang;
= P
</p>
<p>∣∣0
a
</p>
<p>&rang;
= P0a
</p>
<p>∣∣0
a
</p>
<p>&rang;
+
</p>
<p>&infin;&sum;
</p>
<p>n=1
λnA(n)
</p>
<p>∣∣0
a
</p>
<p>&rang;
=
∣∣0
a
</p>
<p>&rang;
+
</p>
<p>&infin;&sum;
</p>
<p>n=1
λnA(n)
</p>
<p>∣∣0
a
</p>
<p>&rang;
(21.54)
</p>
<p>because P0a is the projection operator onto |0a〉.
More desirable is the energy of the perturbed state Ea , which obeys the
</p>
<p>relation HP = EaP. Taking the trace of this relation and noting that trP =
trP0a = 1, we obtain
</p>
<p>Ea = tr(HP)= tr
(
E0aP+
</p>
<p>&infin;&sum;
</p>
<p>n=1
λnB(n)
</p>
<p>)
</p>
<p>=E0a +
&infin;&sum;
</p>
<p>n=1
λn trB(n)︸ ︷︷ ︸
</p>
<p>&equiv;εn
=E0a +
</p>
<p>&infin;&sum;
</p>
<p>n=1
λnεn, (21.55)
</p>
<p>where we used Eq. (21.52). Since λ is simply a parameter to keep track
of the order of perturbation, one usually includes it in the definition of the
perturbing potential V. The nth-order correction to the energy is then written</p>
<p/>
</div>
<div class="page"><p/>
<p>660 21 Multidimensional Green&rsquo;s Functions: Formalism
</p>
<p>as
</p>
<p>εn = trB(n). (21.56)
</p>
<p>Since each term of B(n) contains P0a at least once, and since
</p>
<p>tr
(
UP0aT
</p>
<p>)
= tr
</p>
<p>(
TUP0a
</p>
<p>)
</p>
<p>for any pair of operators U and T (or products thereof), one can cast εn
into the form of an expectation value of some product of operators in the
unperturbed state |0a〉. For example,first-order correction to
</p>
<p>energy
ε1 = trB(1) =
</p>
<p>&sum;
</p>
<p>b
</p>
<p>&lang;0
b
</p>
<p>∣∣P0aVP0a
∣∣0
b
</p>
<p>&rang;
=
&lang;0
a
</p>
<p>∣∣V
∣∣0
a
</p>
<p>&rang;
(21.57)
</p>
<p>because P0a|0b〉 = 0 unless b = a. This is the familiar expression for the first
order correction to the energy in nondegenerate perturbation theory. Simi-
larly,
</p>
<p>ε2 = trB(2) =&minus; tr
(
P0aVP
</p>
<p>0
aV
</p>
<p>[
&minus;Q0aGk0
</p>
<p>(
E0a
</p>
<p>)
Q0a
</p>
<p>]
</p>
<p>+ P0aV
[
&minus;Q0aGk0
</p>
<p>(
E0a
</p>
<p>)
Q0a
</p>
<p>]
VP0a +
</p>
<p>[
&minus;Q0aGk0
</p>
<p>(
E0a
</p>
<p>)
Q0a
</p>
<p>]
VP0aVP
</p>
<p>0
a
</p>
<p>)
</p>
<p>=
&lang;0
a
</p>
<p>∣∣VQ0aGk0
(
E0a
</p>
<p>)
Q0aV
</p>
<p>∣∣0
a
</p>
<p>&rang;
.
</p>
<p>The first and the last terms in parentheses give zero because in the trace sum,
P0a gives a nonzero contribution only if the state is |0a〉, which is precisely
the state annihilated by Q0a . Using the completeness relation
</p>
<p>&sum;
b |0b〉〈0b| =
</p>
<p>1 = &sum;c |0c〉〈0c | for the eigenstates of the unperturbed Hamiltonian, we can
rewrite ε2 as
</p>
<p>second-order correction
</p>
<p>to energy
</p>
<p>ε2 =
&sum;
</p>
<p>b,c
</p>
<p>&lang;0
a
</p>
<p>∣∣V
∣∣0
b
</p>
<p>&rang;
0 if b= a︷ ︸︸ ︷&lang;0
b
</p>
<p>∣∣Q0a Gk0
(
E0a
</p>
<p>)
0 if c= a︷ ︸︸ ︷
Q0a
</p>
<p>∣∣0
c
</p>
<p>&rang;
︸ ︷︷ ︸
</p>
<p>δbc/(E
0
a&minus;E0b )
</p>
<p>&lang;0
c
</p>
<p>∣∣V
∣∣0
a
</p>
<p>&rang;
=
&sum;
</p>
<p>b �=a
</p>
<p>|〈0a|V|0b〉|2
E0a &minus;E0b
</p>
<p>.
</p>
<p>This is the familiar expression for the second-order correction to the en-
ergy in nondegenerate perturbation theory.
</p>
<p>21.5.2 The Degenerate Case
</p>
<p>The degenerate case can also start with Eqs. (21.54) and (21.55). The differ-
ence is that εn cannot be determined as conveniently as the nondegenerate
case. For example, the expression for ε1 will involve a sum over a basis of
M0a because P
</p>
<p>0
a|0b〉 is no longer just |0a〉, but some general vector in M0a . In-
</p>
<p>stead of pursuing this line of approach, we present a more common method,
which concentrates on the way M0a and the corresponding eigenspaces of the
perturbed Hamiltonian, denoted by Ma , enter in the calculation of eigenval-
ues and eigenvectors.</p>
<p/>
</div>
<div class="page"><p/>
<p>21.6 Problems 661
</p>
<p>The projector P0a acts as a unit operator when restricted to M
0
a . In partic-
</p>
<p>ular, it is invertible. In the limit of small λ, the projection operator P is close
to P0a ; therefore, it too must be invertible, i.e., P :M0a &rarr;Ma is an isomor-
phism. Similarly, P0a :Ma &rarr;M0a is also an isomorphism&mdash;not necessarily
the inverse of the first one. It follows that for each vector in M0a there is a
unique vector in Ma and vice versa.
</p>
<p>The eigenvalue equation H|Ea〉 =Ea|Ea〉 can thus be written as
</p>
<p>HPa
∣∣E0a
</p>
<p>&rang;
=EaPa
</p>
<p>∣∣E0a
&rang;
,
</p>
<p>where |E0a〉 is the unique vector mapped onto |Ea〉 by Pa . Multiplying both
sides by P0a , we obtain
</p>
<p>P0aHPa
∣∣E0a
</p>
<p>&rang;
=EaP0aPa
</p>
<p>∣∣E0a
&rang;
,
</p>
<p>which is completely equivalent to the previous equation because P0a is in-
vertible. If we define
</p>
<p>Ha &equiv; P0aHPaP0a :M0a &rarr;M0a, Ka &equiv; P0aPaP0a :M0a &rarr;M0a, (21.58)
</p>
<p>the preceding equation becomes
</p>
<p>Ha
∣∣E0a
</p>
<p>&rang;
=EaKa
</p>
<p>∣∣E0a
&rang;
. (21.59)
</p>
<p>As operators on M0a both Ha and Ka are hermitian. In fact, Ka , which can be
written as the product of P0aPa and its hermitian conjugate, is a positive def-
inite operator. Equation (21.59) is a generalized eigenvalue equation whose
eigenvalues Ea are solutions of the equation
</p>
<p>det(Ha &minus; xKa)= 0. (21.60)
</p>
<p>The eigenvectors of this equation, once projected onto Ma by Pa , give the
desired eigenvectors of H.
</p>
<p>The expansions of Ha and Ka are readily obtained from those of HPa and
Pa as given in Eqs. (21.49) and (21.52). We give the first few terms of each
expansion:
</p>
<p>Ka = P0a &minus; λ2P0aVQ0aG20
(
E0a
</p>
<p>)
Q0aVP
</p>
<p>0
a + &middot; &middot; &middot; ,
</p>
<p>Ha =E0aKa + λP0aVP0a + λ2P0aVQ0aG0
(
E0a
</p>
<p>)
Q0aVP
</p>
<p>0
a + &middot; &middot; &middot; .
</p>
<p>(21.61)
</p>
<p>To any given order of approximation, the eigenvalues Ea are obtained by
terminating the series in (21.61) at that order, plugging the resulting finite
sum in Eq. (21.60), and solving the determinant equation.
</p>
<p>21.6 Problems
</p>
<p>21.1 Show that the definitions of the three types of SOPDEs discussed in
Example 21.1.6 are equivalent to the definitions based on Eq. (21.5). Hint:
Diagonalize the matrix of coefficients of the SOPDE:
</p>
<p>a
&part;2u
</p>
<p>&part;x2
+ 2b &part;
</p>
<p>2u
</p>
<p>&part;x&part;y
+ c &part;
</p>
<p>2u
</p>
<p>&part;y2
+ F
</p>
<p>(
x, y,u,
</p>
<p>&part;u
</p>
<p>&part;x
,
&part;u
</p>
<p>&part;y
</p>
<p>)
= 0,</p>
<p/>
</div>
<div class="page"><p/>
<p>662 21 Multidimensional Green&rsquo;s Functions: Formalism
</p>
<p>where a, b, and c are functions of x and y. Write the eigenvalues as (a +
c &plusmn; �)/2 and consider the three cases |�| &lt; |a + c|, |�| &gt; |a + c|, and
|�| = |a + c|.
</p>
<p>21.2 Find the characteristic curves for Lx[u] = &part;u/&part;x.
</p>
<p>21.3 Find the characteristic curves for the two-dimensional wave equation
and the two-dimensional diffusion equation.
</p>
<p>21.4 Solve the Cauchy problem for the two-dimensional Laplace equation
subject to the Cauchy data u(0, y) = 0, (&part;u/&part;x)(0, y) = ǫ sinky, where ǫ
and k are constants. Show that the solution does not vary continuously as the
Cauchy data vary. In particular, show that for any ǫ �= 0 and any preassigned
x &gt; 0, the solution u(x, y) can be made arbitrarily large by choosing k large
enough.
</p>
<p>21.5 Show that the xi in Eq. (21.12) describe an m-dimensional sphere of
radius r , that is,
</p>
<p>&sum;m
i=1 x
</p>
<p>2
i = r2.
</p>
<p>21.6 Use Jδ(x &minus; a) = δ(ξξξ &minus; ααα) and the coordinate transformation from
the spherical coordinate system to Cartesian coordinates to express the 3D
Cartesian delta function in terms of the corresponding spherical delta func-
tion at a point P = (x0, y0, z0) = (r0, θ0, ϕ0) where the Jacobian J is non-
vanishing.
</p>
<p>21.7 Find the volume of an m-dimensional sphere.
</p>
<p>21.8 Prove Eq. (21.11). First, note that the RHS of Eq. (21.10) is a function
of only k of the α&rsquo;s. This means that
</p>
<p>H(ξξξ)|ξξξ=ααα =H(α1, . . . , αk).
</p>
<p>(a) Rewrite Eq. (21.10) by separating the integral into two parts, one in-
volving {ξi}ki=1 and the other involving {ξi}mi=k+1. Compare the RHS
with the LHS and show that
</p>
<p>ˆ
</p>
<p>Jdξk+1 &middot; &middot; &middot; dξmδ(x &minus; a)=
k&prod;
</p>
<p>i=1
δ(ξi &minus; αi).
</p>
<p>(b) Show that this equation implies that δ(x &minus; a) is independent of
{ξi}mi=k+1. Thus, one can take the delta function out of the integral.
</p>
<p>21.9 Find the m-dimensional Green&rsquo;s function for the Laplacian as fol-
lows.
</p>
<p>(a) Solve Eq. (21.19) assuming that r �= 0 and demanding that G(r)&rarr; 0
as r &rarr;&infin; (this can be done only for m&ge; 3).</p>
<p/>
</div>
<div class="page"><p/>
<p>21.6 Problems 663
</p>
<p>(b) Use the divergence theorem in m dimensions and (21.18) to show that
&uml;
</p>
<p>S
</p>
<p>dG
</p>
<p>dr
da = 1,
</p>
<p>where S is a spherical hypersurface of radius r . Now use this and the
result of part (a) to find the remaining constant of integration.
</p>
<p>21.10 Consider the operator Lx = &nabla;2 + b &middot; &nabla;&nabla;&nabla; + c for which {bi}mi=1 and c
are functions of {xi}mi=1.
</p>
<p>(a) Show that L&dagger;x[v] = &nabla;2v &minus;&nabla;&nabla;&nabla; &middot; (bv)+ cv, and
</p>
<p>Q
[
u,v&lowast;
</p>
<p>]
= Q[u,v] = v&nabla;&nabla;&nabla;u&minus; u&nabla;&nabla;&nabla;v + buv.
</p>
<p>(b) Show that a necessary condition for Lx to be self-adjoint is 2b &middot; &nabla;&nabla;&nabla;u+
u(&nabla;&nabla;&nabla; &middot; b)= 0 for arbitrary u.
</p>
<p>(c) By choosing some u&rsquo;s judiciously, show that (b) implies that bi = 0.
Conclude that Lx =&nabla;2 + c(x) is formally self-adjoint.
</p>
<p>21.11 Solve the integral form of the Schr&ouml;dinger equation for an attractive
double delta-function potential
</p>
<p>V (x)=&minus;V0
[
δ(x &minus; a1)+ δ(x &minus; a2)
</p>
<p>]
, V0 &gt; 0.
</p>
<p>Find the eigenfunctions and obtain a transcendental equation for the eigen-
values (see Example 21.4.1).
</p>
<p>21.12 Show that the integral equation associated with the damped har-
monic oscillator DE ẍ + 2γ ẋ + ω20x = 0, having the BCs x(0) = x0,
(dx/dt)t=0 = 0, can be written in either of the following forms.
</p>
<p>(a) x(t)= x0 &minus;
ω20
</p>
<p>2γ
</p>
<p>ˆ t
</p>
<p>0
</p>
<p>[
1 &minus; e&minus;2γ (t&minus;t &prime;)
</p>
<p>]
x
(
t &prime;
)
dt &prime;.
</p>
<p>(b) x(t)= x0 cosω0t +
2γ x0
ω0
</p>
<p>sinω0t &minus; 2γ
ˆ t
</p>
<p>0
cos
</p>
<p>[
ω0
</p>
<p>(
t &minus; t &prime;
</p>
<p>)]
x
(
t &prime;
)
dt &prime;.
</p>
<p>Hint: Take ω20x or 2γ ẋ, respectively, as the inhomogeneous term.
</p>
<p>21.13 Show that for scattering problems (E &gt; 0)
</p>
<p>(a) the integral form of the Schr&ouml;dinger equation in one dimension is
</p>
<p>Ψ (x)= eikx &minus; iμ
�2k
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
eik|x&minus;y|V (y)Ψ (y)dy.
</p>
<p>(b) Divide (&minus;&infin;,+&infin;) into three regions R1 = (&minus;&infin;,&minus;a), R2 = (&minus;a,+a)
and R3 = (a,&infin;). Let ψi(x) be ψ(x) in region Ri . Assume that the
potential V (x) vanishes in R1 and R3. Show that
</p>
<p>ψ1(x)= eikx &minus;
iμ
</p>
<p>�2k
e&minus;ikx
</p>
<p>ˆ a
</p>
<p>&minus;a
eikyV (y)ψ2(y) dy,</p>
<p/>
</div>
<div class="page"><p/>
<p>664 21 Multidimensional Green&rsquo;s Functions: Formalism
</p>
<p>ψ2(x)= eikx &minus;
iμ
</p>
<p>�2k
</p>
<p>ˆ a
</p>
<p>&minus;a
eik|x&minus;y|V (y)ψ2(y) dy,
</p>
<p>ψ3(x)= eikx &minus;
iμ
</p>
<p>�2k
eikx
</p>
<p>ˆ a
</p>
<p>&minus;a
e&minus;ikyV (y)ψ2(y) dy.
</p>
<p>This shows that determining the wave function in regions where there
is no potential requires the wave function in the region where the po-
tential acts.
</p>
<p>(c) Let
</p>
<p>V (x)=
{
V0 if |x|&lt; a,
0 if |x|&gt; a,
</p>
<p>and find ψ2(x) by the method of successive approximations. Show
that the nth term is less than (2μV0a/�2k)n&minus;1 (so the Neumann series
will converge) if (2V0a/�v) &lt; 1, where v is the velocity and μv =
�k is the momentum of the wave. Therefore, for large velocities, the
Neumann series expansion is valid.
</p>
<p>21.14 (a) Show that HRz(H)= 1+zRz(H). (b) Use (a) to prove Eq. (21.43).</p>
<p/>
</div>
<div class="page"><p/>
<p>22Multidimensional Green&rsquo;s Functions:Applications
</p>
<p>The previous chapter gathered together some general properties of the GFs
and their companion, the Dirac delta function. This chapter considers the
Green&rsquo;s functions for elliptic, parabolic, and hyperbolic equations that sat-
isfy the BCs appropriate for each type of PDE.
</p>
<p>22.1 Elliptic Equations
</p>
<p>The most general linear PDE in m variables of the elliptic type was dis-
cussed in Sect. 21.1.2. We will not discuss this general case, because all
elliptic PDOs encountered in mathematical physics are of a much simpler
nature. In fact, the self-adjoint elliptic PDO of the form Lx = &nabla;2 + q(x) is
sufficiently general for purposes of this discussion. Recall from Sect. 21.1.2
that the BCs associated with an elliptic PDE are of two types, Dirichlet and
Neumann. Let us consider these separately.
</p>
<p>22.1.1 The Dirichlet Boundary Value Problem
</p>
<p>A Dirichlet BVP consists of an elliptic PDE together with a Dirichlet BC,
such as
</p>
<p>Lx[u] = &nabla;2u+ q(x)u= f (x) for x &isin;D,
u(xb)= g(xb) for xb &isin; &part;D,
</p>
<p>(22.1)
</p>
<p>where g(xb) is a given function defined on the closed hypersurface &part;D.
The Green&rsquo;s function for the Dirichlet BVP must satisfy the homoge-
</p>
<p>neous BC, for the same reason as in the one-dimensional Green&rsquo;s function.
Thus, the Dirichlet Green&rsquo;s function, denoted by GD(x,y), must satisfy
</p>
<p>Lx
[
GD(x,y)
</p>
<p>]
= δ(x &minus; y), GD(xb,y)= 0 for xb &isin; S.
</p>
<p>As discussed in Sect. 21.3.2, we can separate GD into a singular part G
(s)
D
</p>
<p>and a regular part H where G(s)D satisfies the same DE as GD and H satisfies
</p>
<p>the corresponding homogeneous DE and the BC H(xb,y)=&minus;G(s)D (xb,y).
</p>
<p>S. Hassani, Mathematical Physics, DOI 10.1007/978-3-319-01195-0_22,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>665</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-01195-0_22">http://dx.doi.org/10.1007/978-3-319-01195-0_22</a></div>
</div>
<div class="page"><p/>
<p>666 22 Multidimensional Green&rsquo;s Functions: Applications
</p>
<p>Using Eq. (22.1) and the properties of GD(x, y) in Eq. (21.27), we obtain
</p>
<p>u(x)=
&int;
</p>
<p>D
</p>
<p>dmyGD(x,y)f (y)+
&int;
</p>
<p>&part;D
</p>
<p>g(yb)
&part;GD
</p>
<p>&part;ny
(x,yb) da, (22.2)
</p>
<p>where &part;/&part;ny indicates normal differentiation with respect to the second ar-
gument.
</p>
<p>Historical Notes
</p>
<p>Gustav Peter Lejeune Dirichlet (1805&ndash;1859), the son of a postmaster, first attended
</p>
<p>Gustav Peter Lejeune
</p>
<p>Dirichlet 1805&ndash;1859
</p>
<p>public school, then a private school that emphasized Latin. He was precociously interested
in mathematics; it is said that before the age of twelve he used his pocket money to buy
mathematical books. In 1817 he entered the gymnasium in Bonn. He is reported to have
been an unusually attentive and well-behaved pupil who was particularly interested in
modern history as well as in mathematics.
After two years in Bonn, Dirichlet was sent to a Jesuit college in Cologne that his parents
preferred. Among his teachers was the physicist Georg Simon Ohm, who gave him a
thorough grounding in theoretical physics. Dirichlet completed his Abitur examination
at the very early age of sixteen. His parents wanted him to study law, but mathematics
was already his chosen field. At the time the level of pure mathematics in the German
universities was at a low ebb: Except for the formidable Carl Gauss, in G&ouml;ttingen, there
were no outstanding mathematicians, while in Paris the firmament was studded by such
luminaries as P.-S. Laplace, Adrien Legendre, Joseph Fourier, and Sim&eacute;on Poisson.
Dirichlet arrived in Paris in May 1822. In the summer of 1823 he was fortunate in be-
ing appointed to a well-paid and pleasant position as tutor to the children of General
Maximilien Fay, a national hero of the Napoleonic wars and then the liberal leader of the
opposition in the Chamber of Deputies. Dirichlet was treated as a member of the family
and met many of the most prominent figures in French intellectual life. Among the math-
ematicians, he was particularly attracted to Fourier, whose ideas had a strong influence
upon his later works on trigonometric series and mathematical physics.
General Fay died in November 1825, and the next year Dirichlet decided to return to
Germany, a plan strongly supported by Alexander von Humboldt, who was working for
the strengthening of the natural sciences in Germany. Dirichlet was permitted to qualify
for habilitation as Privatdozent at the University of Breslau; since he did not have the
required doctorate, this was awarded honoris causa by the University of Cologne. His ha-
bilitation thesis dealt with polynomials whose prime divisors belong to special arithmetic
series. A second paper from this period was inspired by Gauss&rsquo;s announcements on the
biquadratic law of reciprocity.
Dirichlet was appointed extraordinary professor in Breslau, but the conditions for sci-
entific work were not inspiring. In 1828 he moved to Berlin, again with the assistance
of Humboldt, to become a teacher of mathematics at the military academy. Shortly after-
ward, at the age of twenty-three, he was appointed extraordinary (later ordinary) professor
at the University of Berlin. In 1831 he became a member of the Berlin Academy of Sci-
ences, and in the same year he married Rebecca Mendelssohn-Bartholdy, sister of Felix
Mendelssohn, the composer.
Dirichlet spent twenty-seven years as a professor in Berlin and exerted a strong influ-
ence on the development of German mathematics through his lectures, through his many
pupils, and through a series of scientific papers of the highest quality that he published
during this period. He was an excellent teacher, always expressing himself with great
clarity. His manner was modest; in his later years he was shy and at times reserved. He
seldom spoke at meetings and was reluctant to make public appearances. In many ways
he was a direct contrast to his lifelong friend, the mathematician Karl Gustav Jacobi.
One of Dirichlet&rsquo;s most important papers, published in 1850, deals with the boundary
value problem, now known as Dirichlet&rsquo;s boundary value problem, in which one wishes
to determine a potential function satisfying Laplace&rsquo;s equation and having prescribed
values on a given surface, in Dirichlet&rsquo;s case a sphere.
In 1855, when Gauss died, the University of G&ouml;ttingen was anxious to seek a successor
of great distinction, and the choice fell upon Dirichlet. Dirichlet moved to G&ouml;ttingen in
the fall of 1855, bought a house with a garden, and seemed to enjoy the quieter life of</p>
<p/>
</div>
<div class="page"><p/>
<p>22.1 Elliptic Equations 667
</p>
<p>a prominent university in a small city. He had a number of excellent pupils and relished
the increased leisure for research. His work in this period was centered on general prob-
lems of mechanics. This new life, however, was not to last long. In the summer of 1858
Dirichlet traveled to a meeting in Montreux, Switzerland, to deliver a memorial speech in
honor of Gauss. While there, he suffered a heart attack and was barely able to return to
his family in G&ouml;ttingen. During his illness his wife died of a stroke, and Dirichlet himself
died the following spring.
</p>
<p>Some special cases of (22.2) are worthy of mention.
</p>
<p>1. The first is u(xb)= 0, the solution to an inhomogeneous DE satisfying
the homogeneous BC. We obtain this by substituting zero for g(xb) in
(22.2) so that only the integration over D remains.
</p>
<p>2. The second special case is when the DE is homogeneous, that is, when
f (x)= 0 but the BC is inhomogeneous. This yields an integration over
the boundary &part;D alone.
</p>
<p>3. Finally, the solution to the homogeneous DE with the homogeneous BC
is simply u= 0, referred to as the zero solution. This is consistent with
physical intuition: If the function is zero on the boundary and there is
no source f (x) to produce any &ldquo;disturbance,&rdquo; we expect no nontrivial
solution.
</p>
<p>Example 22.1.1 (Method of Images and Dirichlet BVP) Let us find the method of images and
Dirichlet BVPGreen&rsquo;s function for the three-dimensional Laplacian Lx = &nabla;2 satisfying
</p>
<p>the Dirichlet BC GD(ρρρ,y)= 0 for ρρρ, on the xy-plane. Here D is the upper
half-space (z&ge; 0) and &part;D is the xy-plane.
</p>
<p>It is more convenient to use r = (x, y, z) and r&prime; = (x&prime;, y&prime;, z&prime;) instead of x
and y, respectively. Using (21.21) as G(s)D , we can write
</p>
<p>GD
(
r, r&prime;
</p>
<p>)
=&minus; 1
</p>
<p>4π |r &minus; r&prime;| +H
(
r, r&prime;
</p>
<p>)
</p>
<p>=&minus; 1
4π
</p>
<p>1&radic;
(x &minus; x&prime;)2 + (y &minus; y&prime;)2 + (z&minus; z&prime;)2
</p>
<p>+H
(
x, y, z;x&prime;, y&prime;, z&prime;
</p>
<p>)
.
</p>
<p>The requirement that GD vanish on the xy-plane gives
</p>
<p>H
(
x, y,0;x&prime;, y&prime;, z&prime;
</p>
<p>)
= 1
</p>
<p>4π
</p>
<p>1&radic;
(x &minus; x&prime;)2 + (y &minus; y&prime;)2 + z&prime;2
</p>
<p>.
</p>
<p>This fixes the dependence of H on all variables except z. On the other hand,
&nabla;2H = 0 in D implies that the form of H must be the same as that of G(s)D
because except at r = r&prime;, the latter does satisfy Laplace&rsquo;s equation. Thus,
because of the symmetry of G(s)D in r and r
</p>
<p>&prime; [GD(r, r&prime;)=GD(r&prime;, r)] and the
evenness of the Laplacian in z (as well as x and y), we have two choices for
the z-dependence: (z&minus; z&prime;)2 and (z+ z&prime;)2. The first gives GD = 0, which is
a trivial solution. Thus, we must choose
</p>
<p>H
(
x, y, z;x&prime;, y&prime;, z&prime;
</p>
<p>)
= 1
</p>
<p>4π
</p>
<p>1&radic;
(x &minus; x&prime;)2 + (y &minus; y&prime;)2 + (z+ z&prime;)2
</p>
<p>.</p>
<p/>
</div>
<div class="page"><p/>
<p>668 22 Multidimensional Green&rsquo;s Functions: Applications
</p>
<p>Note that with r&prime;&prime; &equiv; (x&prime;, y&prime;,&minus;z&prime;), this equation satisfies &nabla;2H =&minus;δ(r&minus; r&prime;&prime;),
and it may appear that H does not satisfy the homogeneous DE, as it should.
However, r&prime;&prime; is outside D, and r �= r&prime;&prime; as long as r &isin;D. So H does satisfy
the homogeneous DE in D. The Green&rsquo;s function for the given Dirichlet BC
is therefore
</p>
<p>GD
(
r, r&prime;
</p>
<p>)
=&minus; 1
</p>
<p>4π
</p>
<p>(
1
</p>
<p>|r &minus; r&prime;| &minus;
1
</p>
<p>|r &minus; r&prime;&prime;|
</p>
<p>)
,
</p>
<p>where r&prime;&prime; is the reflection of r&prime; in the xy-plane.
This result has a direct physical interpretation. If determining the solu-
</p>
<p>tion of the Laplace equation is considered a problem in electrostatics, then
G
</p>
<p>(s)
D (r, r
</p>
<p>&prime;) is simply the potential at r of a unit point charge located at r&prime;,
and GD(r, r&prime;) is the potential of two point charges of opposite signs, one at
r&prime; and the other at the mirror image of r&prime;. The fact that the two charges are
equidistant from the xy-plane ensures the vanishing of the potential in that
plane. The introduction of image charges to ensure the vanishing of GD at
&part;D is common in electrostatics and is known as the method of images. Thismethod of images
method reduces the Dirichlet problem for the Laplacian to finding appropri-
ate point charges outside D that guarantee the vanishing of the potential on
&part;D. For simple geometries, such as the one discussed in this example, de-
termination of the magnitudes and locations of such image charges is easy,
rendering the method extremely useful.
</p>
<p>Having found the Green&rsquo;s function, we can pose the general Dirichlet
BVP:
</p>
<p>&nabla;2u=&minus;ρ(r) and u(x, y,0)= g(x, y), for z &gt; 0.
</p>
<p>The solution is
</p>
<p>u(r)= 1
4π
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
dx&prime;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
dy&prime;
</p>
<p>&int; &infin;
</p>
<p>0
dz&prime;ρ
</p>
<p>(
r&prime;
)( 1
</p>
<p>|r &minus; r&prime;| &minus;
1
</p>
<p>|r &minus; r&prime;&prime;|
</p>
<p>)
</p>
<p>+
&int; &infin;
</p>
<p>&minus;&infin;
dx&prime;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
dy&prime;g
</p>
<p>(
x&prime;y&prime;
</p>
<p>)&part;GD
&part;z
</p>
<p>∣∣∣∣
z=0
</p>
<p>, (22.3)
</p>
<p>where r = (x, y, z), r&prime; = (x&prime;, y&prime;, z&prime;), and r&prime;&prime; = (x&prime;, y&prime;,&minus;z&prime;).
A typical application consists in introducing a number of charges in the
</p>
<p>vicinity of an infinite conducting sheet, which is held at a constant poten-
tial V0. If there are N charges, {qi}Ni=1, located at {ri}Ni=1, then ρ(r) =&sum;N
</p>
<p>i=1 qiδ(r &minus; ri), g(x, y)= const = V0, and we get
</p>
<p>u(r)=
N&sum;
</p>
<p>i=1
</p>
<p>1
</p>
<p>4π
</p>
<p>(
qi
</p>
<p>|r &minus; ri |
&minus; qi|r &minus; r&prime;i |
</p>
<p>)
+ V0
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
dx&prime;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
dy&prime;
</p>
<p>&part;GD
</p>
<p>&part;z
</p>
<p>∣∣∣∣
z=0
</p>
<p>,
</p>
<p>(22.4)
where ri = (xi, yi, zi) and r&prime;i = (xi, yi,&minus;zi). That the double integral in
Eq. (22.4) is unity can be seen by direct integration or by noting that the
sum vanishes when z = 0. On the other hand, u(x, y,0) = V0. Thus, the</p>
<p/>
</div>
<div class="page"><p/>
<p>22.1 Elliptic Equations 669
</p>
<p>solution becomes
</p>
<p>u(r)=
N&sum;
</p>
<p>i=1
</p>
<p>1
</p>
<p>4π
</p>
<p>(
qi
</p>
<p>|r &minus; ri |
&minus; qi|r &minus; r&prime;i |
</p>
<p>)
+ V0.
</p>
<p>Example 22.1.2 (Dirichlet BVP for a Sphere) The method of images is also
applicable when the boundary is a sphere. Inside a sphere of radius a with
center at the origin, we wish to solve this Dirichlet BVP: Dirichlet BVP for a sphere
</p>
<p>&nabla;2u=&minus;ρ(r, θ,ϕ) for r &lt; a, and u(a, θ,ϕ)= g(θ,ϕ).
</p>
<p>The GF satisfies
</p>
<p>&nabla;2GD
(
r, θ,ϕ; r &prime;, θ &prime;, ϕ&prime;
</p>
<p>)
= δ
</p>
<p>(
r &minus; r&prime;
</p>
<p>)
for r &lt; a,
</p>
<p>GD
(
a, θ,ϕ; r &prime;, θ &prime;, ϕ&prime;
</p>
<p>)
= 0.
</p>
<p>(22.5)
</p>
<p>Thus, GD can again be interpreted as the potential of point charges, of which
one is in the sphere and the others are outside.
</p>
<p>We write GD = G(s)D + H and choose H in such a way that the sec-
ond equation in (22.5) is satisfied. As in the case of the xy-plane, let1
</p>
<p>H(r, r&prime;&prime;)=&minus; k4π |r&minus;r&prime;&prime;| , where k is a constant to be determined. If r&prime;&prime; is out-
side the sphere, &nabla;2H will vanish everywhere inside the sphere. The problem
has been reduced to finding k and r&prime;&prime; (the location of the image charge). We
want to choose r&prime;&prime; such that
</p>
<p>1
</p>
<p>|r &minus; r&prime;|
</p>
<p>∣∣∣∣
r=a
</p>
<p>= k|r &minus; r&prime;&prime;|
</p>
<p>∣∣∣∣
r=a
</p>
<p>&rArr; k
(∣∣r &minus; r&prime;
</p>
<p>∣∣)
r=a =
</p>
<p>(∣∣r &minus; r&prime;&prime;
∣∣)
r=a .
</p>
<p>This shows that k must be positive. Squaring both sides and expanding the
result yields
</p>
<p>k2
(
a2 + r &prime;2 &minus; 2ar &prime; cosγ
</p>
<p>)
= a2 + r &prime;&prime;2 &minus; 2ar &prime;&prime; cosγ,
</p>
<p>where γ is the angle between r and r&prime;, and we have assumed that r&prime; and
r&prime;&prime; are in the same direction. If this equation is to hold for arbitrary γ , we
must have k2r &prime; = r &prime;&prime; and k2(a2 + r &prime;2) = a2 + r &prime;&prime;2. Combining these two
equations yields k4r &prime;2 &minus; k2(a2 + r &prime;2)+ a2 = 0, whose positive solutions are
k = 1 and k = a/r . The first choice implies that r &prime;&prime; = r &prime;, which is impossible
because r &prime;&prime; must be outside the sphere. We thus choose k = a/r &prime;, which
gives r&prime;&prime; = (a2/r &prime;2)r&prime;. We then have
</p>
<p>GD
(
r, r&prime;
</p>
<p>)
=&minus; 1
</p>
<p>4π
</p>
<p>[
1
</p>
<p>|r &minus; r&prime;| &minus;
ar &prime;
</p>
<p>|r &prime;2r &minus; a2r&prime;|
</p>
<p>]
. (22.6)
</p>
<p>1Actually, to be general, we must add an arbitrary function f (r&prime;&prime;) to this. However, as the
reader can easily verify, the following argument will show that f (r&prime;&prime;) = 0. Besides, we
are only interested in a solution, not the most general one. All simplifying assumptions
that follow are made for the same reason.</p>
<p/>
</div>
<div class="page"><p/>
<p>670 22 Multidimensional Green&rsquo;s Functions: Applications
</p>
<p>Substituting this in Eq. (22.2), and noting that &part;G/&part;ny = (&part;G/&part;r &prime;)r &prime;=a ,
yields
</p>
<p>u(r)= 1
4π
</p>
<p>&int; a
</p>
<p>0
r &prime;2dr &prime;
</p>
<p>&int; π
</p>
<p>0
sin θ &prime; dθ &prime;
</p>
<p>&times;
&int; 2π
</p>
<p>0
</p>
<p>(
1
</p>
<p>|r &minus; r&prime;| &minus;
ar &prime;
</p>
<p>|r &prime;2r &minus; a2r&prime;|
</p>
<p>)
ρ
(
r&prime;
)
dϕ&prime;
</p>
<p>+ a(a
2 &minus; r2)
4π
</p>
<p>&int; 2π
</p>
<p>0
dϕ&prime;
</p>
<p>&int; π
</p>
<p>0
sin θ &prime; dθ &prime;
</p>
<p>g(θ &prime;, ϕ&prime;)
|r &minus; a|3 , (22.7)
</p>
<p>where a = (a, θ &prime;, ϕ&prime;) is a vector from the origin to a point on the sphere. For
the Laplace equation ρ(r&prime;) = 0, and only the double integral in Eq. (22.7)
will contribute.
</p>
<p>It can be shown that if g(θ &prime;, ϕ&prime;) = const = V0, then u(r) = V0. This is
the familiar fact shown in electromagnetism: If the potential on a sphere is
kept constant, the potential inside the sphere will be constant and equal to
the potential at the surface.
</p>
<p>Example 22.1.3 (Dirichlet BVP for a Circle) In this example we find the
Dirichlet BVP for a circle
</p>
<p>Dirichlet GF for a circle of radius a centered at the origin. The GF is log-
arithmic [see Eq. (21.22)]. Therefore, H is also logarithmic, and its most
general form is
</p>
<p>H
(
r, r&prime;&prime;
</p>
<p>)
=&minus; 1
</p>
<p>2π
ln
(∣∣r &minus; r&prime;&prime;
</p>
<p>∣∣)&minus; 1
2π
</p>
<p>ln
[
f
(
r&prime;&prime;
)]
</p>
<p>=&minus; 1
2π
</p>
<p>ln
(∣∣r &minus; r&prime;&prime;
</p>
<p>∣∣f
(
r&prime;&prime;
))
,
</p>
<p>so that
</p>
<p>GD
(
r, r&prime;
</p>
<p>)
= 1
</p>
<p>2π
ln
(∣∣r &minus; r&prime;
</p>
<p>∣∣)&minus; 1
2π
</p>
<p>ln
(∣∣r &minus; r&prime;&prime;
</p>
<p>∣∣f
(
r&prime;&prime;
))
</p>
<p>= 1
2π
</p>
<p>ln
</p>
<p>∣∣∣∣
r &minus; r&prime;
</p>
<p>(r &minus; r&prime;&prime;)f (r&prime;&prime;)
</p>
<p>∣∣∣∣.
</p>
<p>For GD to vanish at all points on the circle, we must have
∣∣∣∣
</p>
<p>a &minus; r&prime;
(a &minus; r&prime;&prime;)f (r&prime;&prime;)
</p>
<p>∣∣∣∣= 1 &rArr;
∣∣a &minus; r&prime;
</p>
<p>∣∣=
∣∣(r &minus; r&prime;&prime;
</p>
<p>)
f
(
r&prime;&prime;
)∣∣,
</p>
<p>where a is a vector from origin to a point on the circle. Assuming that r&prime;&prime;
</p>
<p>and r&prime; are in the same direction, squaring both sides of the last equation and
expanding the result, we obtain
</p>
<p>(
a2 + r &prime;&prime;2 &minus; 2ar &prime;&prime; cosγ
</p>
<p>)
f 2
</p>
<p>(
r&prime;&prime;
)
= a2 + r &prime;2 &minus; 2ar &prime; cosγ,
</p>
<p>where γ is the angle between a and r&prime; (or r&prime;&prime;). This equation must hold for
arbitrary γ . Hence, we have f 2(r&prime;&prime;)r &prime;&prime; = r &prime; and f 2(r&prime;&prime;)(a2+ r &prime;&prime;2)= a2+ r &prime;2.
These can be solved for f (r&prime;&prime;) and r&prime;&prime;. The result is
</p>
<p>r&prime;&prime; = a
2
</p>
<p>r &prime;2
r&prime;, f
</p>
<p>(
r&prime;&prime;
)
= a
</p>
<p>r &prime;&prime;
= r
</p>
<p>&prime;
</p>
<p>a
.</p>
<p/>
</div>
<div class="page"><p/>
<p>22.1 Elliptic Equations 671
</p>
<p>Substituting these formulas in the expression for GD , we obtain
</p>
<p>GD
(
r, r&prime;
</p>
<p>)
= 1
</p>
<p>2π
ln
(∣∣r &minus; r&prime;
</p>
<p>∣∣)&minus; 1
2π
</p>
<p>ln
</p>
<p>(∣∣∣∣r &minus;
a2
</p>
<p>r &prime;2
r&prime;
∣∣∣∣
r &prime;
</p>
<p>a
</p>
<p>)
.
</p>
<p>To write the solution to the Dirichlet BVP, we also need &part;GD/&part;n =
&part;GD/&part;r
</p>
<p>&prime;. Using polar coordinates, we express GD as
</p>
<p>GD
(
r, r&prime;
</p>
<p>)
= 1
</p>
<p>4π
ln
</p>
<p>∣∣∣∣
r2 + r &prime;2 &minus; 2rr &prime; cos(θ &minus; θ &prime;)
</p>
<p>r2r &prime;2/a2 + a2 &minus; 2rr &prime; cos(θ &minus; θ &prime;)
</p>
<p>∣∣∣∣.
</p>
<p>Differentiation with respect to r &prime; yields
</p>
<p>&part;GD
</p>
<p>&part;n
</p>
<p>∣∣∣∣
r &prime;=a
</p>
<p>= &part;GD
&part;r &prime;
</p>
<p>∣∣∣∣
r &prime;=a
</p>
<p>= 1
2πa
</p>
<p>a2 &minus; r2
r2 + a2 &minus; 2ra cos(θ &minus; θ &prime;) ,
</p>
<p>from which we can immediately write the solution to the two-dimensional
Dirichlet BVP &nabla;2u= ρ, u(r = a)= g(θ &prime;) as
</p>
<p>u(r)=
&int; 2π
</p>
<p>0
dθ &prime;
</p>
<p>&int; a
</p>
<p>0
r &prime;GD
</p>
<p>(
r, r&prime;
</p>
<p>)
ρ
(
r&prime;
)
dr &prime;
</p>
<p>+ a
2 &minus; r2
2πa
</p>
<p>&int; 2π
</p>
<p>0
dθ &prime;
</p>
<p>g(θ &prime;)
r2 + a2 &minus; 2ra cos(θ &minus; θ &prime;) .
</p>
<p>In particular, for Laplace&rsquo;s equation ρ(r&prime;)= 0, and we get
Poisson integral formula
</p>
<p>u(r, θ)= a
2 &minus; r2
2πa
</p>
<p>&int; 2π
</p>
<p>0
dθ &prime;
</p>
<p>g(θ &prime;)
r2 + a2 &minus; 2ra cos(θ &minus; θ &prime;) . (22.8)
</p>
<p>Equation (22.8) is called the Poisson integral formula.
</p>
<p>22.1.2 The Neumann Boundary Value Problem
</p>
<p>The Neumann BVP is not as simple as the Dirichlet BVP because it requires
the normal derivative of the solution. But the normal derivative is related to
the Laplacian through the divergence theorem. Thus, the BC and the DE are
tied together, and unless we impose some solvability conditions, we may
have no solution at all. These points are illustrated clearly if we consider the
Laplacian operator.
</p>
<p>Historical Notes
</p>
<p>Carl Gottfried Neumann (1832&ndash;1925) was the son of Franz Ernst Neumann, a profes-
sor of physics and mineralogy at K&ouml;nigsberg; his mother, Luise Florentine Hagen, was a
sister-in-law of the astronomer Bessel. Neumann received his primary and secondary ed-
ucation in K&ouml;nigsberg, attended the university, and formed particularly close friendships
with the analyst F.J. Richelot and the geometer L.O. Hesse. After passing the examination
for secondary-school teaching, he obtained his doctorate in 1855; in 1858 he qualified for
lecturing in mathematics at Halle, where he became Privatdozent and, in 1863, assistant
professor. In the latter year he was called to Basel, and in 1865 to T&uuml;bingen. From the</p>
<p/>
</div>
<div class="page"><p/>
<p>672 22 Multidimensional Green&rsquo;s Functions: Applications
</p>
<p>autumn of 1868 until his retirement in 1911 he was at the University of Leipzig. In 1864
</p>
<p>Carl Gottfried Neumann
</p>
<p>1832&ndash;1925
</p>
<p>he married Hermine Mathilde Elise Kloss; she died in 1875.
Neumann, who led a quiet life, was a successful university teacher and a productive re-
searcher. More than two generations of future gymnasium teachers received their basic
mathematical education from him. As a researcher he was especially prominent in the
field of potential theory. His investigations into boundary value problems resulted in pio-
neering achievements; in 1870 he began to develop the method of the arithmetical mean
for their solution. He also coined the term &ldquo;logarithmic potential.&rdquo; The second boundary
value problem of potential theory still bears his name; a generalization of it was later
provided by H. Poincar&eacute;.
Neumann was a member of the Berlin Academy and the Societies of G&ouml;ttingen, Mu-
nich, and Leipzig. He performed a valuable service in founding and editing the important
German mathematics periodical Mathematische Annalen.
</p>
<p>Consider the Neumann BVP
</p>
<p>&nabla;2u= f (x) for x &isin;D, and &part;u
&part;n
</p>
<p>= g(x) for x &isin; &part;D.
</p>
<p>Integrating the first equation over D and using the divergence theorem, we
obtain
</p>
<p>&int;
</p>
<p>D
</p>
<p>f (x) dmx =
&int;
</p>
<p>D
</p>
<p>&nabla;&nabla;&nabla; &middot; (&nabla;&nabla;&nabla;u)dmx =
&int;
</p>
<p>&part;D
</p>
<p>ên &middot; &nabla;&nabla;&nabla;uda =
&int;
</p>
<p>&part;D
</p>
<p>&part;u
</p>
<p>&part;n
da.
</p>
<p>It follows that we cannot arbitrarily assign values of &part;u/&part;n on the boundary.
In particular, if the BC is homogeneous, as in the case of Green&rsquo;s functions,
the RHS is zero, and we must have
</p>
<p>&int;
D
f (x) dmx = 0. This relation is a
</p>
<p>restriction on the DE, and is a solvability condition, as mentioned above. To
satisfy this condition, it is necessary to subtract from the inhomogeneous
term its average value over the region D. Thus, if VD is the volume of the
region D, then
</p>
<p>&nabla;2u= f (x)&minus; f̄ where f̄ = 1
VD
</p>
<p>&int;
</p>
<p>D
</p>
<p>f (x) dmx
</p>
<p>ensures that the Neumann BVP is solvable. In particular, the inhomogeneous
term for the Green&rsquo;s function is not simply δ(x&minus; y) but δ(x&minus; y)&minus; δ̄, where
</p>
<p>δ̄ = 1
VD
</p>
<p>&int;
</p>
<p>D
</p>
<p>δ(x &minus; y) dmx = 1
VD
</p>
<p>if y &isin;D.
</p>
<p>Thus, the Green&rsquo;s function for the Neumann BVP, GN (x,y), satisfies
</p>
<p>&nabla;2GN (x,y)= δ(x &minus; y)&minus;
1
</p>
<p>VD
,
</p>
<p>&part;GN
</p>
<p>&part;n
(x,y)= 0 for x &isin; &part;D.
</p>
<p>Applying Green&rsquo;s identity, Eq. (21.27), we get
</p>
<p>u(x)=
&int;
</p>
<p>D
</p>
<p>dmyGN (x,y)f (y)&minus;
&int;
</p>
<p>&part;D
</p>
<p>GN (x,y)
&part;u
</p>
<p>&part;n
da + ū, (22.9)</p>
<p/>
</div>
<div class="page"><p/>
<p>22.2 Parabolic Equations 673
</p>
<p>where ū = (1/VD)
&int;
D
u(x) dmx is the average value of u in D. Equa-
</p>
<p>tion (22.9) is valid only for the Laplacian operator, although a similar result
can be obtained for a general self-adjoint SOLPDO with constant coeffi-
cients. We will not pursue that result, however, since it is of little practical
use.
</p>
<p>Throughout the discussion so far we have assumed that D is bounded;
that is, we have considered points inside D with BCs on the boundary &part;D
specified. This is called an interior BVP. In many physical situations we are
</p>
<p>interior vs exterior BVP
interested in points outside D. We are then dealing with an exterior BVP.
In dealing with such a problem, we must specify the behavior of the Green&rsquo;s
function at infinity. In most cases, the physics of the problem dictates such
behavior. For instance, for the case of an exterior Dirichlet BVP, where
</p>
<p>u(x)=
&int;
</p>
<p>D
</p>
<p>dmyGD(x,y)f (y)+
&int;
</p>
<p>&part;D
</p>
<p>u(yb)
&part;GD
</p>
<p>&part;ny
(x,yb) da
</p>
<p>and it is desired that u(x)&rarr; 0 as |x| &rarr;&infin;, the vanishing of GD(x,y) at in-
finity guarantees that the second integral vanishes, as long as &part;D is a finite
hypersurface. To guarantee the disappearance of the first integral, we must
demand that GD(x,y) tend to zero faster than f (y)dmy tends to infinity. For
most cases of physical interest, the calculation of the exterior Green&rsquo;s func-
tions is not conceptually different from that of the interior ones. However,
the algebra may be more involved.
</p>
<p>Later we will develop general methods for finding the Green&rsquo;s functions
for certain partial differential operators that satisfy appropriate BCs. At this
point, let us simply mention what are called mixed BCs for elliptic PDEs.
A general mixed BC is of the form
</p>
<p>α(x)u(x)+ β(x)&part;u
&part;n
</p>
<p>(x)= γ (x). (22.10)
</p>
<p>Problem 22.6 examines the conditions that the GF must satisfy in such a
case.
</p>
<p>22.2 Parabolic Equations
</p>
<p>Elliptic partial differential equations arise in static problems, where the solu-
tion is independent of time. Of the two major time-dependent equations, the
wave equation and the heat (or diffusion) equation,2 the latter is a parabolic
PDE and the former a hyperbolic PDE. This section examines the heat equa-
tion, which is of the form &nabla;2u = a2&part;u/&part;t . By changing t to t/a2, we can
write the equation as Lx,t [u] &equiv; (&part;/&part;t &minus; &nabla;2)u(x, t) = 0. We wish to calcu-
late the Green&rsquo;s function associated with Lx,t and the homogeneous BCs.
Because of the time variable, we must also specify the solution at t = 0.
</p>
<p>2The heat equation turns into the Schr&ouml;dinger equation if t is changed to
&radic;
&minus;1 t ; thus, the
</p>
<p>following discussion incorporates the Schr&ouml;dinger equation as well.</p>
<p/>
</div>
<div class="page"><p/>
<p>674 22 Multidimensional Green&rsquo;s Functions: Applications
</p>
<p>Thus, we consider the BVP
</p>
<p>Lx,t [u] &equiv;
(
&part;
</p>
<p>&part;t
&minus;&nabla;2
</p>
<p>)
u(x, t)= 0 for x &isin;D,
</p>
<p>u(xb, t)= 0, u(x,0)= h(x) for xb &isin; &part;D, x &isin;D.
(22.11)
</p>
<p>To find a solution to (22.11), we can use a method that turns out to be use-
ful for evaluating Green&rsquo;s functions in general&mdash;the method of eigenfunc-
tions. Let {un}&infin;n=1 be the eigenfunctions of &nabla;2 with eigenvalues {&minus;λn}&infin;n=1.
Let the BC be un(xb)= 0 for xb &isin; &part;D. Then
</p>
<p>&nabla;2un(x)+ λnun(x)= 0 for n= 1,2, . . . , x &isin;D,
un(xb)= 0 for xb &isin; &part;D.
</p>
<p>(22.12)
</p>
<p>Equation (22.12) constitutes a Sturm-Liouville problem in m dimensions,
which we assume to have a solution with {un}&infin;n=1 as a complete orthonormal
set. We can therefore write
</p>
<p>u(x, t)=
&infin;&sum;
</p>
<p>n=1
Cn(t)un(x). (22.13)
</p>
<p>This is possible because at each specific value of t , u(x, t) is a function
of x and therefore can be written as a linear combination of the same set,
{un}&infin;n=1. The coefficients Cn(t) are given by
</p>
<p>Cn(t)=
&int;
</p>
<p>D
</p>
<p>u(x, t)un(x) dmx. (22.14)
</p>
<p>To calculate Cn(t), we differentiate (22.14) with respect to time and use
(22.11) to obtain
</p>
<p>Ċn(t)&equiv;
dCn
</p>
<p>dt
=
&int;
</p>
<p>D
</p>
<p>&part;u
</p>
<p>&part;t
(x, t)un(x) dmx =
</p>
<p>&int;
</p>
<p>D
</p>
<p>[
&nabla;2u(x, t)
</p>
<p>]
un(x) dmx.
</p>
<p>Using Green&rsquo;s identity for the operator &nabla;2 yields
&int;
</p>
<p>D
</p>
<p>[
un&nabla;2u&minus; u&nabla;2un
</p>
<p>]
dmx =
</p>
<p>&int;
</p>
<p>&part;D
</p>
<p>(
un
</p>
<p>&part;u
</p>
<p>&part;n
&minus; u&part;un
</p>
<p>&part;n
</p>
<p>)
da.
</p>
<p>Since both u and un vanish on &part;D, the RHS is zero, and we get
</p>
<p>Ċn(t)=
&int;
</p>
<p>D
</p>
<p>u&nabla;2un dmx =&minus;λn
&int;
</p>
<p>D
</p>
<p>u(x, t)un(x) dmx =&minus;λnCn.
</p>
<p>This has the solution Cn(t)= Cn(0)e&minus;λnt , where
</p>
<p>Cn(0)=
&int;
</p>
<p>D
</p>
<p>u(y,0)un(y) dmy =
&int;
</p>
<p>D
</p>
<p>h(y)un(y) dmy,
</p>
<p>so that
</p>
<p>Cn(t)= e&minus;λnt
&int;
</p>
<p>D
</p>
<p>h(y)un(y) dmy.</p>
<p/>
</div>
<div class="page"><p/>
<p>22.2 Parabolic Equations 675
</p>
<p>Substituting this in (22.13) and switching the order of integration and sum-
mation, we get
</p>
<p>u(x, t)=
&int;
</p>
<p>D
</p>
<p>[ &infin;&sum;
</p>
<p>n=1
e&minus;λntun(x)un(y)
</p>
<p>]
h(y) dmy
</p>
<p>and read off the GF as
&sum;&infin;
</p>
<p>n=1 e
&minus;λntun(x)un(y)θ(t), where we also intro-
</p>
<p>duced the theta function to ensure that the solution vanishes for t &lt; 0. More
generally, we have
</p>
<p>G(x,y; t &minus; τ)=
&infin;&sum;
</p>
<p>n=1
e&minus;λn(t&minus;τ)un(x)un(y)θ(t &minus; τ). (22.15)
</p>
<p>Note the property
</p>
<p>lim
τ&rarr;t
</p>
<p>G(x,y; t &minus; τ)=
&infin;&sum;
</p>
<p>n=1
un(x)un(y)= δ(x &minus; y),
</p>
<p>which is usually written as
</p>
<p>G
(
x,y;0+
</p>
<p>)
= δ(x &minus; y). (22.16)
</p>
<p>The reader may also check that
</p>
<p>Lx,tG(x,y; t &minus; τ)= δ(x &minus; y)δ(t &minus; τ). (22.17)
</p>
<p>This is precisely what we expect for the Green&rsquo;s function of an operator in
the variables x and t . Another property of G(x,y; t &minus; τ) is that it vanishes
on &part;D, as it should.
</p>
<p>Having found the Green&rsquo;s function and noted its properties, we are in
a position to solve the inhomogeneous analogue of Eq. (22.11), in which
the RHS of the first equation is f (x, t), and the zero on the RHS of the
second equation is replaced by g(xb, t). Experience with similar but simpler
problems indicates that to make any progress toward a solution, we must
come up with a form of Green&rsquo;s identity involving Lx,t and its adjoint. It is
easy to show that
</p>
<p>vLx,t [u] &minus; uL&dagger;x,t [v] =
&part;
</p>
<p>&part;t
(uv)&minus;&nabla;&nabla;&nabla; &middot; (v&nabla;&nabla;&nabla;u&minus; u&nabla;&nabla;&nabla;v), (22.18)
</p>
<p>where L&dagger;x,t =&minus;&part;/&part;t &minus;&nabla;2.
Now consider the (m + 1)-dimensional &ldquo;cylinder&rdquo; one of whose bases
</p>
<p>is at t = ǫ, where ǫ is a small positive number. This base is barely above
the m-dimensional hyperplane Rm. The other base is at t = τ &minus; ǫ and is a
duplicate of D &sub;Rm (see Fig. 22.1). Let aμ, where μ= 0,1, . . . ,m, be the
components of an (m+ 1)-dimensional vector a= (a0, a1, . . . , am). Define
an inner product by
</p>
<p>a &middot; b&equiv;
m&sum;
</p>
<p>μ=0
aμbμ &equiv; a0b0 &minus; a1b1 &minus; &middot; &middot; &middot; &minus; ambm &equiv; a0b0 &minus; a &middot; b</p>
<p/>
</div>
<div class="page"><p/>
<p>676 22 Multidimensional Green&rsquo;s Functions: Applications
</p>
<p>Fig. 22.1 The &ldquo;cylinder&rdquo; used in evaluating the GF for the diffusion and wave equations.
Note that the bases are not planes, but hyperplanes (that is, spaces such as Rm)
</p>
<p>and the (m+ 1)-dimensional vector Q&equiv; (Q0,Q) by Q0 = uv, Q = v&nabla;&nabla;&nabla;u&minus;
u&nabla;&nabla;&nabla;v. Then (22.18) can be expressed as
</p>
<p>vLx,t [u] &minus; uL&dagger;x,t [v] =
m&sum;
</p>
<p>μ=0
</p>
<p>&part;Qμ
</p>
<p>&part;xμ
&equiv; &part;Q
</p>
<p>0
</p>
<p>&part;x0
&minus; &part;Q
</p>
<p>1
</p>
<p>&part;x1
&minus; &middot; &middot; &middot; &minus; &part;Q
</p>
<p>m
</p>
<p>&part;xm
. (22.19)
</p>
<p>We recognize the RHS as a divergence in (m+1)-dimensional space. Denot-
ing the volume of the (m+ 1)-dimensional cylinder by D and its boundary
by &part;D and integrating (22.19) over D, we obtain
</p>
<p>&int;
</p>
<p>D
</p>
<p>(
vLx,t [u] &minus; uL&dagger;x,t [v]
</p>
<p>)
dm+1x =
</p>
<p>&int;
</p>
<p>D
</p>
<p>m&sum;
</p>
<p>μ=0
</p>
<p>&part;Qμ
</p>
<p>&part;xμ
dm+1x
</p>
<p>=
&int;
</p>
<p>&part;D
</p>
<p>m&sum;
</p>
<p>μ=0
QμnμdS, (22.20)
</p>
<p>where dS is an element of &ldquo;area&rdquo; of &part;D. Note that the divergence theorem
was used in the last step. The LHS is an integration over t and x, which can
be written as
&int;
</p>
<p>D
</p>
<p>(
vLx,t [u] &minus; uL&dagger;x,t [v]
</p>
<p>)
dm+1x =
</p>
<p>&int; τ&minus;ǫ
</p>
<p>ǫ
</p>
<p>dt
</p>
<p>&int;
</p>
<p>D
</p>
<p>dmx
(
vLx,t [u] &minus; uL&dagger;x,t [v]
</p>
<p>)
.
</p>
<p>The RHS of (22.20), on the other hand, can be split into three parts: a
base at t = ǫ, a base at t = τ &minus; ǫ, and the lateral surface. The base at t = ǫ
is simply the region D, whose outward-pointing normal is in the negative t
direction. Thus, n0 = &minus;1, and ni = 0 for i = 1,2, . . . ,m. The base at t =
τ &minus; ǫ is also the region D; however, its normal is in the positive t direction.
Thus, n0 = 1, and ni = 0 for i = 1,2, . . . ,m. The element of &ldquo;area&rdquo; for
these two bases is simply dmx. The unit normal to the lateral surface has no
time component and is simply the unit normal to the boundary of D. The
element of &ldquo;area&rdquo; for the lateral surface is dt da, where da is an element of</p>
<p/>
</div>
<div class="page"><p/>
<p>22.2 Parabolic Equations 677
</p>
<p>&ldquo;area&rdquo; for &part;D. Putting everything together, we can write (22.20) as
</p>
<p>&int; τ&minus;ǫ
</p>
<p>ǫ
</p>
<p>dt
</p>
<p>&int;
</p>
<p>D
</p>
<p>dmx
(
vLx,t [u] &minus; uL&dagger;x,t [v]
</p>
<p>)
</p>
<p>=
&int;
</p>
<p>D
</p>
<p>(
&minus;Q0
</p>
<p>)∣∣
t=ǫd
</p>
<p>mx +
&int;
</p>
<p>D
</p>
<p>Q0
∣∣
t=τ&minus;ǫd
</p>
<p>mx &minus;
&int;
</p>
<p>&part;D
</p>
<p>da
</p>
<p>&int; τ&minus;ǫ
</p>
<p>ǫ
</p>
<p>dtQ &middot; ên.
</p>
<p>The minus sign for the last term is due to the definition of the inner product.
Substituting for Q yields
</p>
<p>&int; τ&minus;ǫ
</p>
<p>ǫ
</p>
<p>dt
</p>
<p>&int;
</p>
<p>D
</p>
<p>dmx
(
vLx,t [u] &minus; uL&dagger;x,t [v]
</p>
<p>)
</p>
<p>=&minus;
&int;
</p>
<p>D
</p>
<p>u(x, ǫ)v(x, ǫ)dmx +
&int;
</p>
<p>D
</p>
<p>u(x, τ &minus; ǫ)v(x, τ &minus; ǫ)dmx
</p>
<p>&minus;
&int;
</p>
<p>&part;D
</p>
<p>da
</p>
<p>&int; τ&minus;ǫ
</p>
<p>ǫ
</p>
<p>dt
</p>
<p>(
v
&part;u
</p>
<p>&part;n
&minus; u&part;v
</p>
<p>&part;n
</p>
<p>)
. (22.21)
</p>
<p>Let v be g(x,y; t &minus; τ), the GF associated with the adjoint operator. Then
Eq. (22.21) gives
</p>
<p>&int; τ&minus;ǫ
</p>
<p>ǫ
</p>
<p>dt
</p>
<p>&int;
</p>
<p>D
</p>
<p>dmx
[
g(x,y; t &minus; τ)f (x, t)&minus; u(x, t)δ(x &minus; y)δ(t &minus; τ)
</p>
<p>]
</p>
<p>=&minus;
&int;
</p>
<p>D
</p>
<p>u(x, ǫ)g(x,y; ǫ &minus; τ)dmx +
&int;
</p>
<p>D
</p>
<p>u(x, τ &minus; ǫ)g(x,y;&minus;ǫ)dmx
</p>
<p>&minus;
&int;
</p>
<p>&part;D
</p>
<p>da
</p>
<p>&int; τ&minus;ǫ
</p>
<p>ǫ
</p>
<p>dt
</p>
<p>[
g(xb,y; t &minus; τ)
</p>
<p>&part;u
</p>
<p>&part;n
&minus; u(xb, t)
</p>
<p>&part;g
</p>
<p>&part;n
</p>
<p>]
. (22.22)
</p>
<p>We now use the following facts:
</p>
<p>1. δ(t &minus; τ)= 0 in the second integral on the LHS of Eq. (22.22), because
t can never be equal to τ in the range of integration.
</p>
<p>2. Using the symmetry property of the Green&rsquo;s function and the fact that
Lx,t is real, we have g(x,y; t&minus;τ)=G(y,x; τ &minus; t), where we have used
the fact that t and τ are the time components of x and y, respectively.
In particular, by (22.16), g(x,y;&minus;ǫ)=G(y,x; ǫ)= δ(x &minus; y).
</p>
<p>3. The function g(x,y; t &minus; τ) satisfies the same homogeneous BC as
G(x,y; t &minus; τ). Thus, g(xb,y; t &minus; τ)= 0 for xb &isin; &part;D.
</p>
<p>Substituting all the above in (22.22), taking the limit ǫ &rarr; 0, and switch-
ing x and y and t and τ , we obtain
</p>
<p>u(x, t) =
&int; t
</p>
<p>0
dτ
</p>
<p>&int;
</p>
<p>D
</p>
<p>dmyG(x,y; t &minus; τ)f (y, τ )+
&int;
</p>
<p>D
</p>
<p>u(y,0)G(x,y; t)dmy
</p>
<p>&minus;
&int; t
</p>
<p>0
dτ
</p>
<p>&int;
</p>
<p>&part;D
</p>
<p>u(yb, τ )
&part;G
</p>
<p>&part;ny
(x,yb; t &minus; τ) da, (22.23)
</p>
<p>where &part;/&part;ny in the last integral means normal differentiation with respect
to the second argument of the Green&rsquo;s function.</p>
<p/>
</div>
<div class="page"><p/>
<p>678 22 Multidimensional Green&rsquo;s Functions: Applications
</p>
<p>Equation (22.23) gives the complete solution to the BVP associated with
a parabolic PDE. If f (y, τ ) = 0 and u vanishes on the hypersurface &part;D,
then Eq. (22.23) gives
</p>
<p>u(x, t)=
&int;
</p>
<p>D
</p>
<p>u(y,0)G(x,y; t)dmy, (22.24)
</p>
<p>which is the solution to the BVP of Eq. (22.11), which led to the general
GF as evolution operator
</p>
<p>or propagator
Green&rsquo;s function of (22.15). Equation (22.24) lends itself nicely to a phys-
ical interpretation. The RHS can be thought of as an integral operator with
kernel G(x,y; t). This integral operator acts on u(y,0) and gives u(x, t);
that is, given the shape of the solution at t = 0, the integral operator pro-
duces the shape for all subsequent time. That is why G(x,y; t) is called the
evolution operator, or propagator.
</p>
<p>22.3 Hyperbolic Equations
</p>
<p>The hyperbolic equation we will discuss is the wave equation
</p>
<p>Lx,t [u] &equiv;
(
&part;2
</p>
<p>&part;t2
&minus;&nabla;2
</p>
<p>)
u(x, t)= 0, (22.25)
</p>
<p>where we have set the speed of the wave equal to unity.
We wish to calculate the Green&rsquo;s function for Lx,t subject to appropriate
</p>
<p>BCs. Let us proceed as we did for the parabolic equation and write
</p>
<p>G(x,y; t)=
&infin;&sum;
</p>
<p>n=1
Cn(y; t)un(x)
</p>
<p>Cn(y; t)=
&int;
</p>
<p>D
</p>
<p>G(x,y; t)un(x) dmx,
(22.26)
</p>
<p>where un(x) are orthonormal eigenfunctions of &nabla;2 with eigenvalues &minus;λn,
satisfying certain, as yet unspecified, BCs. As usual, we expect G to satisfy
</p>
<p>Lx,t [G] =
(
&part;2
</p>
<p>&part;t2
&minus;&nabla;2
</p>
<p>)
G(x,y; t &minus; τ)= δ(x &minus; y)δ(t &minus; τ). (22.27)
</p>
<p>Substituting (22.26) in (22.27) with τ = 0 and using &nabla;2un =&minus;λnun, gives
&infin;&sum;
</p>
<p>n=1
</p>
<p>{
&part;2
</p>
<p>&part;t2
Cn(y; t)+ λnCn(y; t)
</p>
<p>}
un(x)=
</p>
<p>&infin;&sum;
</p>
<p>n=1
</p>
<p>[
un(y)δ(t)
</p>
<p>]
un(x),
</p>
<p>where we used δ(x &minus; y) = &sum;&infin;n=1 un(x)un(y) on the RHS. The orthonor-
mality of un now gives C̈n(y; t) + λnCn(y; t) = un(y)δ(t). It follows that
Cn(y; t) is separable. In fact,
</p>
<p>Cn(y; t)= un(y)Tn(t) where
(
d2
</p>
<p>dt2
+ λn
</p>
<p>)
Tn(t)= δ(t).</p>
<p/>
</div>
<div class="page"><p/>
<p>22.3 Hyperbolic Equations 679
</p>
<p>This equation describes a one-dimensional Green&rsquo;s function and can be
solved using the methods of Chap. 20. Assuming that Tn(t) = 0 for t &le; 0,
we obtain Tn(t) = (sinωnt/ωn)θ(t), where ω2n = λn. Substituting all the
above results in (22.26), we obtain
</p>
<p>G(x,y; t)=
&infin;&sum;
</p>
<p>n=1
un(x)un(y)
</p>
<p>sinωnt
</p>
<p>ωn
θ(t),
</p>
<p>or, more generally,
</p>
<p>G(x,y; t &minus; τ)=
&infin;&sum;
</p>
<p>n=1
un(x)un(y)
</p>
<p>sinωn(t &minus; τ)
ωn
</p>
<p>θ(t &minus; τ). (22.28)
</p>
<p>We note that
</p>
<p>G
(
x,y;0+
</p>
<p>)
= 0 and &part;G
</p>
<p>&part;t
(x,y; t)
</p>
<p>∣∣∣∣
t&rarr;0+
</p>
<p>= δ(x &minus; y), (22.29)
</p>
<p>as can easily be verified.
With the Green&rsquo;s function for the operator Lx,t of Eq. (22.25) at our dis-
</p>
<p>posal, we can attack the BVP given by
</p>
<p>(
&part;2
</p>
<p>&part;t2
&minus;&nabla;2
</p>
<p>)
u(x, t)= f (x, t) for x &isin;D,
</p>
<p>u(xb, t)= h(xb, t), u(x,0)= φ(x) for xb &isin; &part;D, x &isin;D,
&part;u
</p>
<p>&part;t
(x, t)
</p>
<p>∣∣∣∣
t=0
</p>
<p>=ψ(x) for x &isin;D.
</p>
<p>(22.30)
</p>
<p>As in the case of the parabolic equation, we first derive an appropriate ex-
pression of Green&rsquo;s identity. This can be done by noting that
</p>
<p>vLx,t [u] &minus; uL&dagger;x,t [v] =
&part;
</p>
<p>&part;t
</p>
<p>(
u
&part;v
</p>
<p>&part;t
&minus; v &part;u
</p>
<p>&part;t
</p>
<p>)
&minus;&nabla;&nabla;&nabla; &middot; (u&nabla;&nabla;&nabla;v &minus; v&nabla;&nabla;&nabla;u).
</p>
<p>Thus, Lx,t is formally self-adjoint. Furthermore, we can identify
</p>
<p>Q0 = u&part;v
&part;t
</p>
<p>&minus; v &part;u
&part;t
</p>
<p>and Q = u&nabla;&nabla;&nabla;v&minus; v&nabla;&nabla;&nabla;u.
</p>
<p>Following the procedure used for the parabolic case step by step, we can
easily derive a Green&rsquo;s identity and show that
</p>
<p>u(x, t)=
&int; t
</p>
<p>0
dτ
</p>
<p>&int;
</p>
<p>D
</p>
<p>dmyG(x,y; t &minus; τ)f (y, τ )
</p>
<p>+
&int;
</p>
<p>D
</p>
<p>[
ψ(y)G(x,y; t)&minus; φ(y)&part;G
</p>
<p>&part;t
(x,y; t)
</p>
<p>]
dmy
</p>
<p>&minus;
&int; t
</p>
<p>0
dτ
</p>
<p>&int;
</p>
<p>&part;D
</p>
<p>h(yb, τ )
&part;G
</p>
<p>&part;ny
(x,yb; t &minus; τ) da. (22.31)
</p>
<p>The details are left as Problem 22.11.</p>
<p/>
</div>
<div class="page"><p/>
<p>680 22 Multidimensional Green&rsquo;s Functions: Applications
</p>
<p>For the homogeneous PDE with the homogeneous BC h= 0 =ψ , we get
</p>
<p>u(x, t)=&minus;
&int;
</p>
<p>D
</p>
<p>φ(y)
&part;G
</p>
<p>&part;t
(x,y; t)dmy.
</p>
<p>Note the difference between this equation and Eq. (22.24). Here the prop-
agator is the time derivative of the Green&rsquo;s function. There is another dif-
ference between hyperbolic and parabolic equations. When the solution to
a parabolic equation vanishes on the boundary and is initially zero, and the
PDE is homogeneous [f (x, t)= 0], the solution must be zero. This is clear
from Eq. (22.23). On the other hand, Eq. (22.31) indicates that under the
same circumstance, there may be a nonzero solution for a hyperbolic equa-
tion if ψ is nonzero. In such a case we obtain
</p>
<p>u(x, t)=
&int;
</p>
<p>D
</p>
<p>ψ(y)G(x,y; t)dmy.
</p>
<p>This difference in the two types of equations is due to the fact that hyperbolic
equations have second-order time derivatives. Thus, the initial shape of a
solution is not enough to uniquely specify it. The initial velocity profile is
also essential. We saw examples of this in Chap. 19.
</p>
<p>The discussion of Green&rsquo;s functions has so far been formal. The main
purpose of the remaining sections is to bridge the gap between formalism
and concrete applications. Several powerful techniques are used in obtaining
Green&rsquo;s functions, but we will focus only on two: the Fourier transform
technique, and the eigenfunction expansion technique.
</p>
<p>22.4 The Fourier Transform Technique
</p>
<p>Recall that any Green&rsquo;s function can be written as a sum of a singular part
and a regular part: G=Gs +H . Since we have already discussed homoge-
neous equations in detail in Chap. 19, we will not evaluate H in this section
but will concentrate on the singular parts of various Green&rsquo;s functions.
</p>
<p>The BCs play no role in evaluating Gs . Therefore, the Fourier transform
technique (FTT), which involves integration over all space, can be utilized.
The FTT has a drawback&mdash;it does not work if the coefficient functions are
not constants. For most physical applications treated in this book, however,
this will not be a shortcoming.
</p>
<p>Let us consider the most general SOLPDO with constant coefficients,
</p>
<p>Lx = a0 +
m&sum;
</p>
<p>j=1
aj
</p>
<p>&part;
</p>
<p>&part;xj
+
</p>
<p>m&sum;
</p>
<p>j,k=1
bjk
</p>
<p>&part;2
</p>
<p>&part;xj&part;xk
, (22.32)
</p>
<p>where a0, aj , and bjk are constants. The corresponding Green&rsquo;s function
has a singular part that satisfies the usual PDE with the delta function on the
RHS. The FTT starts with assuming a Fourier integral representation in the
variable x for the singular part and for the delta function:
</p>
<p>Gs(x,y)=
1
</p>
<p>(2π)m/2
</p>
<p>&int;
dmkG̃s(k,y)eik&middot;x,</p>
<p/>
</div>
<div class="page"><p/>
<p>22.4 The Fourier Transform Technique 681
</p>
<p>δ(x &minus; y)= 1
(2π)m
</p>
<p>&int;
dmkeik&middot;(x&minus;y).
</p>
<p>Substituting these equations in the PDE for the GF, we get
</p>
<p>G̃s(k,y)=
1
</p>
<p>(2π)m/2
</p>
<p>(
e&minus;ik&middot;y
</p>
<p>a0 + i
&sum;m
</p>
<p>j=1 ajkj &minus;
&sum;m
</p>
<p>j,l=1 bjkkjkl
</p>
<p>)
</p>
<p>and
</p>
<p>Gs(x,y)=
1
</p>
<p>(2π)m
</p>
<p>&int;
dmk
</p>
<p>eik&middot;(x&minus;y)
</p>
<p>a0 + i
&sum;m
</p>
<p>j=1 ajkj &minus;
&sum;m
</p>
<p>j,l=1 bjkkjkl
. (22.33)
</p>
<p>If we can evaluate the integral in (22.33), we can find G.
The following examples apply Eq. (22.33) to specific problems. Note that
</p>
<p>(22.33) indicates that Gs depends only on x &minus; y. This point was mentioned
in Chap. 20, where it was noted that such dependence occurs when the BCs
play no part in an evaluation of the singular part of the Green&rsquo;s function of
a DE with constant coefficients; and this is exactly the situation here.
</p>
<p>22.4.1 GF for them-Dimensional Laplacian
</p>
<p>We calculated the GF for the m-dimensional Laplacian in Sect. 21.2.2 using
a different method. With a0 = 0 = aj , bj l = δj l , and r = x &minus; y, Eq. (22.33)
reduces to
</p>
<p>Gs(r)=
1
</p>
<p>(2π)m
</p>
<p>&int;
dmk
</p>
<p>eik&middot;r
</p>
<p>&minus;k2 , (22.34)
</p>
<p>where k2 = k21 + &middot; &middot; &middot; + k2m = k &middot; k. To integrate (22.34), we choose spher-
ical coordinates in the m-dimensional k-space. Furthermore, to simplify
calculations we let the km-axis lie along r so that r = (0,0, . . . , |r|) and
k &middot; r = k|r| cos θ1 [see Eq. (21.12)]. Substituting this in (22.34) and writing
dmk in spherical coordinates yields
</p>
<p>Gs(r)=
&minus;1
</p>
<p>(2π)m
</p>
<p>&int;
eik|r| cos θ1
</p>
<p>k2
</p>
<p>&times; km&minus;1(sin θ1)m&minus;2 &middot; &middot; &middot; sin θm&minus;2 dk dθ1 &middot; &middot; &middot; dθm&minus;1. (22.35)
</p>
<p>From Eq. (21.15) we note that dΩm = (sin θ1)m&minus;2dθ1dΩm&minus;1. Thus, after
integrating over the angles θ2, . . . , θm&minus;1, Eq. (22.35) becomes
</p>
<p>Gs(r)=&minus;
1
</p>
<p>(2π)m
Ωm&minus;1
</p>
<p>&int; &infin;
</p>
<p>0
km&minus;3dk
</p>
<p>&int; π
</p>
<p>0
(sin θ1)
</p>
<p>m&minus;2eik|r| cos θ1dθ1.
</p>
<p>The inner integral can be looked up in an integral table (see [Grad 65,
p. 482]):
</p>
<p>&int; π
</p>
<p>0
(sin θ1)
</p>
<p>m&minus;2eik|r| cos θ1dθ1 =
&radic;
π
</p>
<p>(
2
</p>
<p>kr
</p>
<p>)m/2&minus;1
Ŵ
</p>
<p>(
m&minus; 1
</p>
<p>2
</p>
<p>)
Jm/2&minus;1(kr).</p>
<p/>
</div>
<div class="page"><p/>
<p>682 22 Multidimensional Green&rsquo;s Functions: Applications
</p>
<p>Substituting this and (21.16) in the preceding equation and using the result
(see [Grad 65, p. 684])
</p>
<p>&int; &infin;
</p>
<p>0
xμJν(ax) dx = 2μa&minus;μ&minus;1
</p>
<p>Ŵ(
μ+ν+1
</p>
<p>2 )
</p>
<p>Ŵ(
μ&minus;ν+1
</p>
<p>2 )
,
</p>
<p>we obtain
</p>
<p>Gs(r)=&minus;
Ŵ(m/2 &minus; 1)
</p>
<p>4πm/2
</p>
<p>(
1
</p>
<p>rm&minus;2
</p>
<p>)
for m&gt; 2,
</p>
<p>which agrees with (21.20) since Ŵ(m/2)= (m/2 &minus; 1)Ŵ(m/2 &minus; 1).
</p>
<p>22.4.2 GF for them-Dimensional Helmholtz Operator
</p>
<p>For the Helmholtz operator &nabla;2 &minus;μ2, Eq. (22.33) reduces to
</p>
<p>Gs(r)=&minus;
1
</p>
<p>(2π)m
</p>
<p>&int;
dmk
</p>
<p>eik&middot;r
</p>
<p>μ2 + k2 .
</p>
<p>Following the same procedure as in the previous subsection, we find
</p>
<p>Gs(r)=&minus;
Ωm&minus;1
(2π)m
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>km&minus;1dk
μ2 + k2
</p>
<p>&int; π
</p>
<p>0
(sin θ1)
</p>
<p>m&minus;2eikr cos θ1dθ1
</p>
<p>=&minus;Ωm&minus;1
(2π)m
</p>
<p>&radic;
π
</p>
<p>(
2
</p>
<p>r
</p>
<p>)m/2&minus;1
Ŵ
</p>
<p>(
m&minus; 1
</p>
<p>2
</p>
<p>)&int; &infin;
</p>
<p>0
</p>
<p>km/2
</p>
<p>μ2 + k2 Jm/2&minus;1(kr) dk.
</p>
<p>Here we can use the integral formula (see [Grad 65, pp. 686 and 952])
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>Jν(bx)x
ν+1
</p>
<p>(x2 + a2)η+1 dx =
aν&minus;ηbη
</p>
<p>2ηŴ(η+ 1)Kν&minus;η(ab),
</p>
<p>where
</p>
<p>Kν(z)=
iπ
</p>
<p>2
eiνπ/2H (1)ν (iz),
</p>
<p>to obtain
</p>
<p>Gs(r)=&minus;
Ωm&minus;1
(2π)m
</p>
<p>&radic;
π
</p>
<p>(
2
</p>
<p>r
</p>
<p>)m/2&minus;1
Ŵ
</p>
<p>(
m&minus; 1
</p>
<p>2
</p>
<p>)
μm/2&minus;1
</p>
<p>π
</p>
<p>2
eimπ/4H
</p>
<p>(1)
m/2&minus;1(iμr),
</p>
<p>which simplifies to
</p>
<p>Gs(r)=&minus;
π/2
</p>
<p>(2π)m/2
</p>
<p>(
μ
</p>
<p>r
</p>
<p>)m/2&minus;1
eimπ/4H
</p>
<p>(1)
m/2&minus;1(iμr). (22.36)
</p>
<p>It can be shown (see Problem 22.8) that for m= 3 this reduces to Gs(r)=
&minus; e&minus;μr4πr , which is the Yukawa potential due to a unit charge.</p>
<p/>
</div>
<div class="page"><p/>
<p>22.4 The Fourier Transform Technique 683
</p>
<p>We can easily obtain the GF for &nabla;2 + μ2 by substituting &plusmn;iμ for μ in
Eq. (22.36). The result is
</p>
<p>Gs(r)= im+1
π/2
</p>
<p>(2π)m/2
</p>
<p>(
μ
</p>
<p>r
</p>
<p>)m/2&minus;1
H
</p>
<p>(1)
m/2&minus;1(&plusmn;μr). (22.37)
</p>
<p>For m= 3 this yields Gs(r)=&minus;e&plusmn;iμr/(4πr). The two signs in the exponent
correspond to the so-called incoming and outgoing &ldquo;waves&rdquo;.
</p>
<p>Example 22.4.1 For a non-local potential, the time-independent Schr&ouml;din-
ger equation is
</p>
<p>Non-local potentials
</p>
<p>depend not only on the
</p>
<p>observation point, but
</p>
<p>also on some other
</p>
<p>&ldquo;non-local&rdquo; variables.
</p>
<p>&minus; �
2
</p>
<p>2μ
&nabla;2Ψ +
</p>
<p>&int;
</p>
<p>R3
V
(
r, r&prime;
</p>
<p>)
Ψ
(
r&prime;
)
d3r &prime; =EΨ (r).
</p>
<p>Then, the integral equation associated with this differential equation is (see
Sect. 21.4)
</p>
<p>Ψ (r)=Aeik&middot;r &minus; μ
2π�2
</p>
<p>&int;
</p>
<p>R3
d3r &prime;
</p>
<p>eik|r&minus;r
&prime;|
</p>
<p>|r &minus; r&prime;|
</p>
<p>&int;
</p>
<p>R3
d3r &prime;&prime;V
</p>
<p>(
r&prime;, r&prime;&prime;
</p>
<p>)
Ψ
(
r&prime;&prime;
)
. (22.38)
</p>
<p>For a separable potential, for which V (r&prime;, r&prime;&prime;)=&minus;g2U(r&prime;)U(r&prime;&prime;), we can
solve (22.38) exactly. We substitute for V (r&prime;, r&prime;&prime;) in (22.38) to obtain
</p>
<p>Ψ (r)=Aeik&middot;r + μg
2
</p>
<p>2π�2
</p>
<p>&int;
</p>
<p>R3
d3r &prime;
</p>
<p>eik|r&minus;r
&prime;|
</p>
<p>|r &minus; r&prime;| U
(
r&prime;
)&int;
</p>
<p>R3
d3r &prime;&prime;U
</p>
<p>(
r&prime;&prime;
)
Ψ
(
r&prime;&prime;
)
.
</p>
<p>(22.39)
Defining the quantities
</p>
<p>Q(r)&equiv; μg
2
</p>
<p>2π�2
</p>
<p>&int;
</p>
<p>R3
d3r &prime;
</p>
<p>eik|r&minus;r
&prime;|
</p>
<p>|r &minus; r&prime;| U
(
r&prime;
)
, C &equiv;
</p>
<p>&int;
</p>
<p>R3
d3r &prime;&prime;U
</p>
<p>(
r&prime;&prime;
)
Ψ
(
r&prime;&prime;
)
</p>
<p>(22.40)
and substituting them in (22.39) yields Ψ (r)=Aeik&middot;r+CQ(r). Multiplying
both sides of this equation by U(r) and integrating over R3, we get
</p>
<p>C =A
&int;
</p>
<p>R3
eik&middot;rU(r)d3r +C
</p>
<p>&int;
</p>
<p>R3
U(r)Q(r) d3r
</p>
<p>= (2π)3/2AŨ(&minus;k)+C
&int;
</p>
<p>R3
U(r)Q(r) d3r,
</p>
<p>from which we obtain
</p>
<p>C = (2π)
3/2AŨ(&minus;k)
</p>
<p>1 &minus;
&int;
R3
</p>
<p>U(r)Q(r) d3r
,
</p>
<p>leading to the solution
</p>
<p>Ψ (r)=Aeik&middot;r + (2π)
3/2AŨ(&minus;k)
</p>
<p>1 &minus;
&int;
R3
</p>
<p>U(r&prime;)Q(r&prime;) d3r &prime;
Q(r). (22.41)
</p>
<p>In principle, Ũ (&minus;k) [the Fourier transform of U(r)] and Q(r) can be cal-
culated once the functional form of U(r) is known. Equations (22.40) and
(22.41) give the solution to the Schr&ouml;dinger equation in closed form.</p>
<p/>
</div>
<div class="page"><p/>
<p>684 22 Multidimensional Green&rsquo;s Functions: Applications
</p>
<p>22.4.3 GF for them-Dimensional Diffusion Operator
</p>
<p>When dealing with parabolic and hyperbolic equations, we will find it con-
venient to consider the &ldquo;different&rdquo; variable (usually t) as the zeroth coordi-
nate. In the Fourier transform we then use ω=&minus;k0 and write
</p>
<p>Gs(r, t)=
1
</p>
<p>(2π)(m+1)/2
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
dω
</p>
<p>&int;
dmkG̃s(k,ω)ei(k&middot;r&minus;ωt),
</p>
<p>δ(r)δ(t)= 1
(2π)m+1
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
dω
</p>
<p>&int;
dmkei(k&middot;r&minus;ωt),
</p>
<p>(22.42)
</p>
<p>where r is the m-dimensional position vector.
We substitute (22.42) in (&part;/&part;t &minus;&nabla;2)Gs(r, t)= δ(r)δ(t) to obtain
</p>
<p>Gs(r, t)=
1
</p>
<p>(2π)m+1
</p>
<p>&int;
dmkeik&middot;r
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
dω
</p>
<p>e&minus;iωt
</p>
<p>ω+ ik2 , (22.43)
</p>
<p>where as usual, k2 = &sum;mi=1 k2i . The ω integration can be done using the
calculus of residues. The integrand has a simple pole at ω=&minus;ik2, that is, in
the lower half of the complex ω-plane (LHP). To integrate, we must know
the sign of t . If t &gt; 0, the exponential factor dictates that the contour be
closed in the LHP, where there is a pole and, therefore, a contribution to
the residues. On the other hand, if t &lt; 0, the contour must be closed in
the UHP. The integral is then zero because there are no poles in the UHP.
We must therefore introduce a step function θ(t) in the Green&rsquo;s function.
Evaluating the residue, the ω integration yields &minus;2πie&minus;k2t . (The minus sign
arises because of clockwise contour integration in the LHP.) Substituting
this in Eq. (22.43), using spherical coordinates in which the last k-axis is
along r, and integrating over all angles except θ1, we obtain
</p>
<p>Gs(r, t)= θ(t)
Ωm&minus;1
(2π)m
</p>
<p>&int; &infin;
</p>
<p>0
km&minus;1dke&minus;k
</p>
<p>2t
</p>
<p>&int; π
</p>
<p>0
(sin θ1)
</p>
<p>m&minus;2eikr cos θ1dθ1
</p>
<p>= θ(t) Ωm&minus;1
(2π)m
</p>
<p>&radic;
π
</p>
<p>(
2
</p>
<p>r
</p>
<p>)m/2&minus;1
Ŵ
</p>
<p>(
m&minus; 1
</p>
<p>2
</p>
<p>)
</p>
<p>&times;
&int; &infin;
</p>
<p>0
km/2e&minus;k
</p>
<p>2tJm/2&minus;1(kr) dk.
</p>
<p>For the θ1 integration, we used the result quoted in Sect. 22.4.1.
Using the integral formula (see [Grad 65, pp. 716 and 1058])
</p>
<p>&int; &infin;
</p>
<p>0
xμe&minus;αx
</p>
<p>2
Jν(βx)dx
</p>
<p>= β
νŴ(
</p>
<p>μ+ν+1
2 )
</p>
<p>2ν+1α(μ+ν+1)/2Ŵ(ν + 1)Φ
(
μ+ ν + 1
</p>
<p>2
, ν + 1;&minus;β
</p>
<p>2
</p>
<p>4α
</p>
<p>)
,
</p>
<p>where Φ is the confluent hypergeometric function, we obtain
</p>
<p>Gs(r, t)= θ(t)
2π (m&minus;1)/2
</p>
<p>(2π)m
&radic;
π
</p>
<p>(
2
</p>
<p>r
</p>
<p>)m/2&minus;1
rm/2&minus;1
</p>
<p>2m/2tm/2
Φ
</p>
<p>(
m
</p>
<p>2
,
m
</p>
<p>2
;&minus; r
</p>
<p>2
</p>
<p>4t
</p>
<p>)
.
</p>
<p>(22.44)</p>
<p/>
</div>
<div class="page"><p/>
<p>22.4 The Fourier Transform Technique 685
</p>
<p>The power-series expansion for the confluent hypergeometric function Φ
shows that Φ(α,α; z)= ez. Substituting this result in (22.44) and simplify-
ing, we finally obtain
</p>
<p>Gs(r, t)=
e&minus;r
</p>
<p>2/4t
</p>
<p>(4πt)m/2
θ(t). (22.45)
</p>
<p>22.4.4 GF for them-Dimensional Wave Equation
</p>
<p>The difference between this example and the preceding one is that here the
time derivative is of second order. Thus, instead of Eq. (22.43), we start with
</p>
<p>Gs(r, t)=&minus;
1
</p>
<p>(2π)m+1
</p>
<p>&int;
dmkeik&middot;r
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
dω
</p>
<p>e&minus;iωt
</p>
<p>ω2 &minus; k2 . (22.46)
</p>
<p>The ω integration can be done using the method of residues. Since the sin-
gularities of the integrand, ω=&plusmn;k, are on the real axis, it seems reasonable
to use the principal value as the value of the integral. This, in turn, depends
on the sign of t . If t &gt; 0 (t &lt; 0), we have to close the contour in the LHP
(UHP): to avoid the explosion of the exponential. If one also insists on not
including the poles inside the contour,3 then one can show that
</p>
<p>P
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
dω
</p>
<p>e&minus;iωt
</p>
<p>ω2 &minus; k2 =&minus;π
sin kt
</p>
<p>k
ǫ(t),
</p>
<p>where
</p>
<p>ǫ(t)&equiv; θ(t)&minus; θ(&minus;t)=
{
</p>
<p>1 if t &gt; 0,
</p>
<p>&minus;1 if t &lt; 0.
Substituting this in (22.46) and integrating over all angles as done in the
previous examples yields
</p>
<p>Gs(r, t)=
ǫ(t)
</p>
<p>2(2π)m/2rm/2&minus;1
</p>
<p>&int; &infin;
</p>
<p>0
km/2&minus;1Jm/2&minus;1(kr) sin kt dk. (22.47)
</p>
<p>As Problem 22.25 shows, the Green&rsquo;s function given by Eq. (22.47) sat-
Physics determines the
</p>
<p>contour of integration
isfies only the homogeneous wave equation with no delta function on the
RHS. The reason for this is that the principal value of an integral chooses
a specific contour that may not reflect the physical situation. In fact, the
Green&rsquo;s function in (22.47) contains two pieces corresponding to the two
different contours of integration, and it turns out that the physically interest-
ing Green&rsquo;s functions are obtained, not from the principal value, but from
giving small imaginary parts to the poles. Thus, replacing the ω integral with
a contour integral for which the two poles are pushed in the LHP and using
</p>
<p>3This will determine how to (semi)circle around the poles.</p>
<p/>
</div>
<div class="page"><p/>
<p>686 22 Multidimensional Green&rsquo;s Functions: Applications
</p>
<p>the method of residues, we obtain
</p>
<p>Iup &equiv;
&int; &infin;
</p>
<p>&minus;&infin;
dω
</p>
<p>e&minus;iωt
</p>
<p>ω2 &minus; k2 =
&int;
</p>
<p>C1
</p>
<p>e&minus;izt
</p>
<p>z2 &minus; k2 dz=
2π
</p>
<p>k
θ(t) sin kt.
</p>
<p>The integral is zero for t &lt; 0 because for negative values of t , the contour
must be closed in the UHP, where there are no poles inside C1. Substituting
this in (22.46) and working through as before, we obtain what is called the
retarded Green&rsquo;s function:retarded Green&rsquo;s
</p>
<p>function
G(ret)s (r, t)=
</p>
<p>θ(t)
</p>
<p>(2π)m/2rm/2&minus;1
</p>
<p>&int; &infin;
</p>
<p>0
km/2&minus;1Jm/2&minus;1(kr) sin kt dk. (22.48)
</p>
<p>If the poles are pushed in the UHP we obtain the advanced Green&rsquo;sadvanced Green&rsquo;s
function function:
</p>
<p>G(adv)s (r, t)=&minus;
θ(&minus;t)
</p>
<p>(2π)m/2rm/2&minus;1
</p>
<p>&int; &infin;
</p>
<p>0
km/2&minus;1Jm/2&minus;1(kr) sin kt dk.
</p>
<p>(22.49)
Unlike the elliptic and parabolic equations discussed earlier, the integral
</p>
<p>over k is not a function but a distribution, as will become clear below. To
find the retarded and advanced Green&rsquo;s functions, we write the sine term in
the integral in terms of exponentials and use the following (see [Grad 65,
p. 712]):
</p>
<p>&int; &infin;
</p>
<p>0
xνe&minus;αxJν(βx)dx =
</p>
<p>(2β)νŴ(ν + 1/2)&radic;
π(α2 + β2)ν+1/2 for Re(α) &gt;
</p>
<p>∣∣Im(β)
∣∣.
</p>
<p>To ensure convergence at infinity, we add a small negative number to the
exponential and define the integral
</p>
<p>I&plusmn;ǫ &equiv;
&int; &infin;
</p>
<p>0
kνe&minus;(∓it+ǫ)kJν(kr) dk
</p>
<p>= (2r)
νŴ(ν + 1/2)&radic;
</p>
<p>π
</p>
<p>[
(∓it + ǫ)2 + r2
</p>
<p>]&minus;(ν+1/2)
.
</p>
<p>For the GFs, we need to evaluate the (common) integral in (22.48) and
(22.49). With ν =m/2 &minus; 1, we have
</p>
<p>I (ν) &equiv;
&int; &infin;
</p>
<p>0
kνJν(kr) sin kt dk =
</p>
<p>1
</p>
<p>2i
lim
ǫ&rarr;0
</p>
<p>(
I+ǫ &minus; I&minus;ǫ
</p>
<p>)
</p>
<p>= (2r)
νŴ(ν + 1/2)
2i
&radic;
π
</p>
<p>&times; lim
ǫ&rarr;0
</p>
<p>{
1
</p>
<p>[r2 + (&minus;it + ǫ)2]ν+1/2 &minus;
1
</p>
<p>[r2 + (it + ǫ)2]ν+1/2
}
.
</p>
<p>At this point, it is convenient to discuss separately the two cases of m odd
and m even. Let us derive the expression for odd m (the even case is left for
Problem 22.26). Define the integer n= (m&minus; 1)/2 = ν+ 12 and write I (ν) as
</p>
<p>I (n) = (2r)
n&minus;1/2Ŵ(n)
2i
&radic;
π
</p>
<p>lim
ǫ&rarr;0
</p>
<p>{
1
</p>
<p>[r2 + (&minus;it + ǫ)2]n &minus;
1
</p>
<p>[r2 + (it + ǫ)2]n
}
.
</p>
<p>(22.50)</p>
<p/>
</div>
<div class="page"><p/>
<p>22.4 The Fourier Transform Technique 687
</p>
<p>Define u= r2 + (&minus;it + ǫ)2. Then using the identity
</p>
<p>1
</p>
<p>un
= (&minus;1)
</p>
<p>n&minus;1
</p>
<p>(n&minus; 1)!
dn&minus;1
</p>
<p>dun&minus;1
</p>
<p>(
1
</p>
<p>u
</p>
<p>)
</p>
<p>and the chain rule, df/du = (1/2r)&part;f/&part;r , we obtain d/du = (1/2r)&part;/&part;r
and
</p>
<p>1
</p>
<p>[r2 + (&plusmn;it + ǫ)2]n =
1
</p>
<p>(n&minus; 1)!
</p>
<p>(
&minus; 1
</p>
<p>2r
</p>
<p>&part;
</p>
<p>&part;r
</p>
<p>)n&minus;1[ 1
r2 + (&plusmn;it + ǫ)2
</p>
<p>]
.
</p>
<p>Therefore, Eq. (22.50) can be written as
</p>
<p>I (n) =
&int; &infin;
</p>
<p>0
kn&minus;1/2Jn&minus;1/2(kr) sin kt dk
</p>
<p>= (2r)
n&minus;1/2Ŵ(n)
2i
&radic;
π
</p>
<p>1
</p>
<p>(n&minus; 1)!
</p>
<p>&times;
(
&minus; 1
</p>
<p>2r
</p>
<p>&part;
</p>
<p>&part;r
</p>
<p>)n&minus;1{
lim
ǫ&rarr;0
</p>
<p>[
1
</p>
<p>[r2 + (&minus;it + ǫ)2] &minus;
1
</p>
<p>[r2 + (it + ǫ)2]
</p>
<p>]}
.
</p>
<p>(22.51)
</p>
<p>The limit in (22.51) is found in Problem 22.27. Using the result of that
problem and Ŵ(n)= (n&minus; 1)!, we get
</p>
<p>I (n) =
&int; &infin;
</p>
<p>0
kn&minus;1/2Jn&minus;1/2(kr) sin kt dk
</p>
<p>=&minus;
&radic;
π(2r)n&minus;1/2
</p>
<p>2
</p>
<p>(
&minus; 1
</p>
<p>2r
</p>
<p>&part;
</p>
<p>&part;r
</p>
<p>)n&minus;1{1
r
</p>
<p>[
δ(t + r)&minus; δ(t &minus; r)
</p>
<p>]}
. (22.52)
</p>
<p>Employing this result in (22.48) and (22.49) yields
</p>
<p>G(ret)s (r, t)=
1
</p>
<p>4π
</p>
<p>(
&minus; 1
</p>
<p>2πr
</p>
<p>&part;
</p>
<p>&part;r
</p>
<p>)n&minus;1[
δ(t &minus; r)
</p>
<p>r
</p>
<p>]
for n= m&minus; 1
</p>
<p>2
,
</p>
<p>G(adv)s (r, t)=
1
</p>
<p>4π
</p>
<p>(
&minus; 1
</p>
<p>2πr
</p>
<p>&part;
</p>
<p>&part;r
</p>
<p>)n&minus;1[
δ(t + r)
</p>
<p>r
</p>
<p>]
for n= m&minus; 1
</p>
<p>2
.
</p>
<p>(22.53)
</p>
<p>The theta functions are not needed in (22.53) because the arguments of the
delta functions already meet the restrictions imposed by the theta functions.
</p>
<p>The two functions in (22.53) have an interesting physical interpretation.
Green&rsquo;s functions are propagators (of signals of some sort), and G(ret)s (r, t)
is capable of propagating signals only for positive times. On the other hand,
G
</p>
<p>(adv)
s (r, t) can propagate only in the negative time direction. Thus, if ini-
</p>
<p>tially (t = 0) a signal is produced (by appropriate BCs), both G(ret)s (r, t)
and G(adv)s (r, t) work to propagate it in their respective time directions. It
may seem that G(adv)s (r, t) is useless because every signal propagates for-
ward in time. This is true, however, only for classical events. In relativistic
quantum field theory antiparticles are interpreted mathematically as moving
in the negative time direction! Thus, we cannot simply ignore G(adv)s (r, t).</p>
<p/>
</div>
<div class="page"><p/>
<p>688 22 Multidimensional Green&rsquo;s Functions: Applications
</p>
<p>In fact, the correct propagator to choose in this theory is a linear combina-
tion of G(adv)s (r, t) and G
</p>
<p>(ret)
s (r, t), called the Feynman propagator (seeFeynman propagator
</p>
<p>[Wein 95, pp. 274&ndash;280]).
The preceding example shows a subtle difference between Green&rsquo;s func-
</p>
<p>tions for second-order differential operators in one dimension and in higher
dimensions. We saw in Chap. 20 that the former are continuous functions in
the interval on which they are defined. Here, we see that higher dimensional
Green&rsquo;s functions are not only discontinuous, but that they are not even func-
tions in the ordinary sense; they contain a delta function. Thus, in general,
Green&rsquo;s functions in higher dimensions ought to be treated as distributions
(generalized functions).
</p>
<p>22.5 The Eigenfunction Expansion Technique
</p>
<p>Suppose that the differential operator Lx, defined in a domain D with bound-
ary &part;D, has discrete eigenvalues {λn}&infin;n=1 with corresponding orthonormal
eigenfunctions {um(x)}&infin;m=1. These two sets may not be in one-to-one cor-
respondence. Assume that the um(x)&rsquo;s satisfy the same BCs as the Green&rsquo;s
function to be defined below.
</p>
<p>Now consider the operator Lx &minus; λ1, where λ is different from all λn&rsquo;s.
Then, as in the one-dimensional case, this operator is invertible, and we can
define its Green&rsquo;s function by (Lx &minus;λ)Gλ(x,y)= δ(x&minus;y) where the weight
function is set equal to one. The completeness of {un(x)}&infin;m=1 implies that
</p>
<p>δ(x &minus; y)=
&infin;&sum;
</p>
<p>n=1
un(x)u&lowast;n(y) and Gλ(x,y)=
</p>
<p>&infin;&sum;
</p>
<p>n=1
an(y)un(x).
</p>
<p>Substituting these two expansions in the differential equation for GF yields
</p>
<p>&infin;&sum;
</p>
<p>n=1
(λn &minus; λ)an(y)un(x)=
</p>
<p>&infin;&sum;
</p>
<p>n=1
un(x)u&lowast;n(y).
</p>
<p>The orthonormality of the un&rsquo;s gives an(y)= u&lowast;n(y)/(λn &minus; λ). Therefore,
</p>
<p>Gλ(x,y)=
&infin;&sum;
</p>
<p>n=1
</p>
<p>un(x)u&lowast;n(y)
λn &minus; λ
</p>
<p>. (22.54)
</p>
<p>In particular, if zero is not an eigenvalue of Lx, its Green&rsquo;s function can be
written as
</p>
<p>G(x,y)=
&infin;&sum;
</p>
<p>n=1
</p>
<p>un(x)u&lowast;n(y)
λn
</p>
<p>. (22.55)
</p>
<p>This is an expansion of the Green&rsquo;s function in terms of the eigenfunctions
of Lx.
</p>
<p>It is instructive to consider a formal interpretation of Eq. (22.55). Re-
call that the spectral decomposition theorem permits us to write f (A) =&sum;
</p>
<p>i f (λi)Pi for an operator A with (distinct) eigenvalues λi and projec-
tion operators Pi . Allowing repetition of eigenvalues in the sum, we may</p>
<p/>
</div>
<div class="page"><p/>
<p>22.5 The Eigenfunction Expansion Technique 689
</p>
<p>write f (A)=&sum;n f (λn)|un〉〈un|, where n counts all the eigenfunctions cor-
responding to eigenvalues. Now, let f (A)= A&minus;1. Then
</p>
<p>G= A&minus;1 =
&sum;
</p>
<p>n
</p>
<p>λ&minus;1n |un〉〈un| =
&sum;
</p>
<p>n
</p>
<p>|un〉〈un|
λn
</p>
<p>,
</p>
<p>or, in &ldquo;matrix element&rdquo; form,
</p>
<p>G(x,y)= 〈x|G|y〉 =
&sum;
</p>
<p>n
</p>
<p>〈x|un〉〈un|y〉
λn
</p>
<p>=
&sum;
</p>
<p>n
</p>
<p>un(x)u&lowast;n(y)
λn
</p>
<p>.
</p>
<p>This last expression coincides with the RHS of Eq. (22.55).
Equations (22.54) and (22.55) demand that the un(x) form a complete
</p>
<p>discrete orthonormal set. We encountered many examples of such eigen-
functions in discussing Sturm-Liouville systems in Chap. 19. All the S-
L systems there were, of course, one-dimensional. Here we are general-
izing the S-L system to m dimensions. This is not a limitation, however,
because&mdash;for the PDEs of interest&mdash;the separation of variables reduces an
m-dimensional PDE to m one-dimensional ODEs. If the BCs are appropri-
ate, the m ODEs will all be S-L systems. A review of Chap. 19 will reveal
that homogeneous BCs always lead to S-L systems. In fact, Theorem 19.4.1
guarantees this claim. Since Green&rsquo;s functions must also satisfy homoge-
neous BCs, expansions such as those of (22.54) and (22.55) become possi-
ble.
</p>
<p>Example 22.5.1 As a concrete example, let us obtain an eigenfunction
expansion of the GF of the two-dimensional Laplacian, &nabla;2 = &part;2/&part;x2 +
&part;2/&part;y2, inside the rectangular region 0 &le; x &le; a, 0 &le; y &le; b with Dirich-
let BCs. Since the GF vanishes at the boundary, the eigenvalue problem
becomes &nabla;2u = λu with u(0, y) = u(a, y) = u(x,0) = u(x, b) = 0. The
method of separation of variables gives the orthonormal eigenfunctions4
</p>
<p>umn(x, y)=
2&radic;
ab
</p>
<p>sin
</p>
<p>(
nπ
</p>
<p>a
x
</p>
<p>)
sin
</p>
<p>(
mπ
</p>
<p>b
y
</p>
<p>)
for m,n= 1,2, . . . ,
</p>
<p>whose corresponding eigenvalues are λmn =&minus;[(nπa )2 + (mπb )2].
Inserting the eigenfunctions and the eigenvalues in Eq. (22.55), we obtain
</p>
<p>G
(
r, r&prime;
</p>
<p>)
=G
</p>
<p>(
x, y;x&prime;, y&prime;
</p>
<p>)
=
</p>
<p>&infin;&sum;
</p>
<p>m,n=1
</p>
<p>umn(x, y)umn(x
&prime;, y&prime;)
</p>
<p>λmn
</p>
<p>=&minus; 4
ab
</p>
<p>&infin;&sum;
</p>
<p>m,n=1
</p>
<p>sin(nπ
a
x) sin(mπ
</p>
<p>b
y) sin(nπ
</p>
<p>a
x&prime;) sin(mπ
</p>
<p>b
y&prime;)
</p>
<p>(nπ
a
)2 + (mπ
</p>
<p>b
)2
</p>
<p>,
</p>
<p>where we changed x to r and y to r&prime;. Note that the eigenvalues are never
zero; thus, G(r, r&prime;) is well-defined.
</p>
<p>4The inner product is defined as a double integral over the rectangle.</p>
<p/>
</div>
<div class="page"><p/>
<p>690 22 Multidimensional Green&rsquo;s Functions: Applications
</p>
<p>In the preceding example, zero was not an eigenvalue of Lx. This condi-
tion must hold when a Green&rsquo;s function is expanded in terms of eigenfunc-
tions. In physical applications, certain conditions (which have nothing to do
with the BCs) exclude the zero eigenvalue automatically when they are ap-
plied to the Green&rsquo;s function. For instance, the condition that the Green&rsquo;s
function remain finite at the origin is severe enough to exclude the zero
eigenvalue.
</p>
<p>Example 22.5.2 Let us consider the two-dimensional Dirichlet BVP &nabla;2u=
f , with u= 0 on a circle of radius a. If we consider only the BCs and ask
whether zero is an eigenvalue of &nabla;2, the answer will be yes, as the following
argument shows.
</p>
<p>The most general solution to the zero-eigenvalue equation, &nabla;2u = 0, in
polar coordinates can be obtained by the method of separation of variables:
</p>
<p>u(ρ,ϕ)=A+B lnρ +
&infin;&sum;
</p>
<p>n=1
</p>
<p>(
bnρ
</p>
<p>n + b&prime;nρ&minus;n
)
</p>
<p>cosnϕ
</p>
<p>+
&infin;&sum;
</p>
<p>n=1
</p>
<p>(
cnρ
</p>
<p>n + c&prime;nρ&minus;n
)
</p>
<p>sinnϕ. (22.56)
</p>
<p>Invoking the BC gives
</p>
<p>0 = u(a,ϕ)=A+B lna +
&infin;&sum;
</p>
<p>n=1
</p>
<p>(
bna
</p>
<p>n + b&prime;na&minus;n
)
</p>
<p>cosnϕ
</p>
<p>+
&infin;&sum;
</p>
<p>n=1
</p>
<p>(
cna
</p>
<p>n + c&prime;na&minus;n
)
</p>
<p>sinnϕ,
</p>
<p>which holds for arbitrary ϕ if and only if
</p>
<p>A=&minus;B lna, b&prime;n =&minus;bna2n, c&prime;n =&minus;cna2n.
</p>
<p>Substituting in (22.56) gives
</p>
<p>u(ρ,ϕ)= B ln
(
ρ
</p>
<p>a
</p>
<p>)
+
</p>
<p>&infin;&sum;
</p>
<p>n=1
</p>
<p>(
ρn &minus; a
</p>
<p>2n
</p>
<p>ρn
</p>
<p>)
(bn cosnϕ + cn sinnϕ). (22.57)
</p>
<p>Thus, if we demand nothing beyond the BCs, &nabla;2 will have a nontrivial
eigen-solution corresponding to the zero eigenvalue, given by Eq. (22.57).
</p>
<p>Physical reality, however, demands that u(ρ,ϕ) be well-behaved at the
origin. This condition sets B , b&prime;n, and c
</p>
<p>&prime;
n of Eq. (22.56) equal to zero. The
</p>
<p>BCs then make the remaining coefficients in (22.56) vanish. Thus, the de-
mand that u(ρ,ϕ) be well-behaved at ρ = 0 turns the situation completely
around and ensures the nonexistence of a zero eigenvalue for the Laplacian,
which in turn guarantees the existence of a GF.
</p>
<p>In many cases the operator Lx as a whole is not amenable to a full Sturm-
Liouville treatment, and as such will not yield orthonormal eigenvectors
in terms of which the GF can be expanded. However, it may happen that</p>
<p/>
</div>
<div class="page"><p/>
<p>22.5 The Eigenfunction Expansion Technique 691
</p>
<p>Lx can be broken up into two pieces one of which is an S-L operator. In
such a case, the GF can be found as follows: Suppose that L1 and L2 are
two commuting operators with L2 an S-L operator whose eigenvalues and
eigenfunctions are known. Since L2 commutes with L1, it can be regarded as
a constant as far as operations with (and on) L1 are concerned. In particular,
(L1 + L2)G= 1 can be regarded as an operator equation in L1 alone with L2
treated as a constant. Let x1 denote the subset of the variables on which L1
acts, and let x2 denote the remainder of the coordinates. Then we can write
δ(x &minus; y)= δ(x1 &minus; y1)δ(x2 &minus; y2). Now let G1(x1,y1; k) denote the Green&rsquo;s
function for L1 + k, where k is a constant. Then it is easily verified that
</p>
<p>G(x,y)=G1(x1,y1;L2)δ(x2 &minus; y2). (22.58)
</p>
<p>In fact,
</p>
<p>(L1 + L2)G(x,y)=
[
(L1 + L2)G1(x1,y1;L2)
</p>
<p>]
︸ ︷︷ ︸
=δ(x1&minus;y1) by definition of G1
</p>
<p>δ(x2 &minus; y2).
</p>
<p>Once G1 is found as a function of L2, it can operate on δ(x2&minus;y2) to yield the
desired Green&rsquo;s function. The following example illustrates the technique.
</p>
<p>Example 22.5.3 Let us evaluate the Dirichlet GF for the two-dimensional
Helmholtz operator &nabla;2 &minus; k2 in the infinite strip 0 &le; x &le; a, &minus;&infin;&lt; y &lt;&infin;.
Let L1 = &part;2/&part;y2 &minus; k2 and L2 = &part;2/&part;x2. Then,
</p>
<p>G
(
r, r&prime;
</p>
<p>)
&equiv;G
</p>
<p>(
x, x&prime;, y, y&prime;
</p>
<p>)
=G1
</p>
<p>(
y, y&prime;;L2
</p>
<p>)
δ
(
x &minus; x&prime;
</p>
<p>)
,
</p>
<p>where (d2/dy2 &minus; μ2)G1 = δ(y &minus; y&prime;), μ2 &equiv; k2 &minus; L2, and G1(y = &minus;&infin;) =
G1(y =&infin;)= 0. The GF G1 can be readily found (see Problem 20.12):
</p>
<p>G1
(
y, y&prime;;L2
</p>
<p>)
=&minus;e
</p>
<p>&minus;μ|y&minus;y&prime;|
</p>
<p>2μ
=&minus;e
</p>
<p>&minus;
&radic;
</p>
<p>k2&minus;L2|y&minus;y&prime;|
</p>
<p>2
&radic;
k2 &minus; L2
</p>
<p>.
</p>
<p>The full GF is then
</p>
<p>G
(
r, r&prime;
</p>
<p>)
=
(
&minus;e
</p>
<p>&minus;
&radic;
</p>
<p>k2&minus;L2|y&minus;y&prime;|
</p>
<p>2
&radic;
k2 &minus; L2
</p>
<p>)
δ
(
x &minus; x&prime;
</p>
<p>)
. (22.59)
</p>
<p>The operator L2 constitutes an S-L system with eigenvalues λn=&minus;(nπx/a)2
and eigenfunctions un(x) =
</p>
<p>&radic;
2/a sin(nπx/a) where n = 1,2, . . . . There-
</p>
<p>fore, the delta function δ(x &minus; x&prime;) can be expanded in terms of these eigen-
functions:
</p>
<p>δ
(
x &minus; x&prime;
</p>
<p>)
= 2
</p>
<p>a
</p>
<p>&infin;&sum;
</p>
<p>n=1
sin
</p>
<p>(
nπ
</p>
<p>a
x
</p>
<p>)
sin
</p>
<p>(
nπ
</p>
<p>a
x&prime;
)
.</p>
<p/>
</div>
<div class="page"><p/>
<p>692 22 Multidimensional Green&rsquo;s Functions: Applications
</p>
<p>As μ=
&radic;
k2 &minus; L2 acts on the delta function, L2 operates on the first factor in
</p>
<p>the above expansion and gives λn. Thus, L2 in Eq. (22.59) can be replaced
by &minus;(nπx/a)2, and we have
</p>
<p>G
(
r, r&prime;
</p>
<p>)
=&minus;1
</p>
<p>a
</p>
<p>&infin;&sum;
</p>
<p>n=1
</p>
<p>(
&minus;e
</p>
<p>&minus;
&radic;
</p>
<p>k2+(nπx/a)2|y&minus;y&prime;|
&radic;
k2 + (nπx/a)2
</p>
<p>)
sin
</p>
<p>(
nπ
</p>
<p>a
x
</p>
<p>)
sin
</p>
<p>(
nπ
</p>
<p>a
x&prime;
)
.
</p>
<p>Sometimes it is convenient to break an operator into more than two parts.
In fact, in some cases it may be advantageous to define a set of commut-
ing self-adjoint (differential) operators {Mj } such that the full operator L
can be written as L=&sum;j LjMj where the differential operators {Lj } act on
variables on which the Mj have no action. Since the Mj &rsquo;s commute among
themselves, one can find simultaneous eigenfunctions for all of them. Then
one expands part of the delta function in terms of these eigenfunctions in the
hope that the ensuing problem becomes more manageable. The best way to
appreciate this approach is via an example.
</p>
<p>Example 22.5.4 Let us consider the Laplacian in spherical coordinates,
</p>
<p>&nabla;2u= 1
r2
</p>
<p>&part;
</p>
<p>&part;r
</p>
<p>(
r2
</p>
<p>&part;u
</p>
<p>&part;r
</p>
<p>)
+ 1
</p>
<p>r2 sin θ
</p>
<p>[
&part;
</p>
<p>&part;θ
</p>
<p>(
sin θ
</p>
<p>&part;u
</p>
<p>&part;θ
</p>
<p>)
+ &part;
</p>
<p>2u
</p>
<p>&part;ϕ2
</p>
<p>]
.
</p>
<p>If we introduce
</p>
<p>M1u= u, L1u=
1
</p>
<p>r2
</p>
<p>&part;
</p>
<p>&part;r
</p>
<p>(
r2
</p>
<p>&part;u
</p>
<p>&part;r
</p>
<p>)
,
</p>
<p>M2u=
1
</p>
<p>sin θ
</p>
<p>[
&part;
</p>
<p>&part;θ
</p>
<p>(
sin θ
</p>
<p>&part;u
</p>
<p>&part;θ
</p>
<p>)
+ &part;
</p>
<p>2u
</p>
<p>&part;ϕ2
</p>
<p>]
, L2u=
</p>
<p>1
</p>
<p>r2
u,
</p>
<p>(22.60)
</p>
<p>the Laplacian becomes &nabla;2 = L1M1 + L2M2. The mutual eigenfunctions of
M1 and M2 are simply those of M2, which is (the negative of) the angular
momentum operator discussed in Chap. 13, whose eigenfunctions are the
spherical harmonics. We thus have M2Ylm(θ,ϕ)=&minus;l(l + 1)Ylm(θ,ϕ).
</p>
<p>Let us expand the Green&rsquo;s function in terms of the spherical harmonics:
</p>
<p>G
(
r, r&prime;
</p>
<p>)
=
&sum;
</p>
<p>l,m
</p>
<p>glm
(
r; r &prime;, θ &prime;, ϕ&prime;
</p>
<p>)
Ylm(θ,ϕ).
</p>
<p>We also write the delta function as
</p>
<p>δ
(
r &minus; r&prime;
</p>
<p>)
= δ(r &minus; r
</p>
<p>&prime;)δ(θ &minus; θ &prime;)δ(ϕ &minus; ϕ&prime;)
r &prime;2 sin θ &prime;
</p>
<p>= δ(r &minus; r
&prime;)
</p>
<p>r &prime;2
&sum;
</p>
<p>l,m
</p>
<p>Ylm(θ,ϕ)Y
&lowast;
lm
</p>
<p>(
θ &prime;, ϕ&prime;
</p>
<p>)
,
</p>
<p>where we have used the completeness of the spherical harmonics. Substitut-
ing all of the above in &nabla;2G(r, r&prime;)= δ(r &minus; r&prime;), we obtain</p>
<p/>
</div>
<div class="page"><p/>
<p>22.6 Problems 693
</p>
<p>&nabla;2G
(
r, r&prime;
</p>
<p>)
= (L1M1 + L2M2)
</p>
<p>&sum;
</p>
<p>l,m
</p>
<p>glm
(
r; r &prime;, θ &prime;, ϕ&prime;
</p>
<p>)
Ylm(θ,ϕ)
</p>
<p>=
&sum;
</p>
<p>l,m
</p>
<p>{[
L1 &minus; l(l + 1)L2
</p>
<p>]
glm
</p>
<p>(
r; r &prime;, θ &prime;, ϕ&prime;
</p>
<p>)}
Ylm(θ,ϕ)
</p>
<p>= δ(r &minus; r
&prime;)
</p>
<p>r &prime;2
&sum;
</p>
<p>l,m
</p>
<p>Y &lowast;lm
(
θ &prime;, ϕ&prime;
</p>
<p>)
Ylm(θ,ϕ).
</p>
<p>The orthogonality of the Ylm(θ,ϕ) yields
</p>
<p>[
L1 &minus; l(l + 1)L2
</p>
<p>]
glm
</p>
<p>(
r; r &prime;, θ &prime;, ϕ&prime;
</p>
<p>)
= δ(r &minus; r
</p>
<p>&prime;)
r &prime;2
</p>
<p>Y &lowast;lm
(
θ &prime;, ϕ&prime;
</p>
<p>)
.
</p>
<p>This shows that the angular part of glm is simply Y &lowast;lm(θ
&prime;, ϕ&prime;). Separating this
</p>
<p>from the dependence on r and r&prime; and substituting for L1 and L2, we obtain
</p>
<p>1
</p>
<p>r2
</p>
<p>d
</p>
<p>dr
</p>
<p>(
r2
</p>
<p>dglm
</p>
<p>dr
</p>
<p>)
&minus; l(l + 1)
</p>
<p>r2
glm =
</p>
<p>δ(r &minus; r &prime;)
r2
</p>
<p>, (22.61)
</p>
<p>where this last glm is a function of r and r &prime; only. The techniques of Chap. 20
can be employed to solve Eq. (22.61) (see Problem 22.29).
</p>
<p>The separation of the full operator into two &ldquo;smaller&rdquo; operators can also
be used for cases in which both operators have eigenvalues and eigenvec-
tors. The result of such an approach will, of course, be equivalent to the
eigenfunction-expansion approach. However, there will be an arbitrariness
in the operator approach: Which operator are we to choose as our L1? While
in Example 22.5.3 the choice was clear (the operator that had no eigenfunc-
tions), here either operator can be chosen as L1. The ensuing GFs will be
equivalent, and the series representing them will be convergent, of course.
However, the rate of convergence may be different for the two. It turns
out, for example, that if we are interested in G(x,y;x&prime;, y&prime;) for the two-
dimensional Laplacian at points (x, y) whose y-coordinates are far from y&prime;,
then the appropriate expansion is obtained by letting L1 = &part;2/&part;y2, that is,
an expansion in terms of x eigenfunctions. On the other hand, if the Green&rsquo;s
function is to be calculated for a point (x, y) whose x-coordinate is far away
from the singular point (x&prime;, y&prime;), then the appropriate expansion is obtained
by letting L1 = &part;2/&part;x2.
</p>
<p>22.6 Problems
</p>
<p>22.1 Find the GF for the Dirichlet BVP in two dimensions if D is the UHP
and &part;D is the x-axis.
</p>
<p>22.2 Add f (r&prime;&prime;) to H(r, r&prime;&prime;) in Example 22.1.2 and retrace the argument
given there to show that f (r&prime;&prime;)= 0.
</p>
<p>22.3 Use the method of images to find the GF for the Laplacian in the exte-
rior region of a &ldquo;sphere&rdquo; of radius a in two and three dimensions.</p>
<p/>
</div>
<div class="page"><p/>
<p>694 22 Multidimensional Green&rsquo;s Functions: Applications
</p>
<p>22.4 Derive Eq. (22.7) from Eq. (22.6).
</p>
<p>22.5 Using Eq. (22.7) with ρ = 0, show that if g(θ &prime;, ϕ&prime;)= V0, the potential
at any point inside the sphere is V0.
</p>
<p>22.6 Find the BC that the GF must satisfy in order for the solution u to
be representable in terms of the GF when the BC on u is mixed, as in
Eq. (22.10). Assume a self-adjoint SOLPDO of the elliptic type, and con-
sider the two cases α(x) �= 0 and β(x) �= 0 for x &isin; &part;D. Hint: In each case,
divide the mixed BC equation by the nonzero coefficient, substitute the re-
sult in the Green&rsquo;s identity, and set the coefficient of the u term in the &part;D
integral equal to zero.
</p>
<p>22.7 Show that the diffusion operator satisfies
</p>
<p>Lx,tG(x,y; t &minus; τ)= δ(x &minus; y)δ(t &minus; τ).
</p>
<p>Hint: Use
&part;θ
</p>
<p>&part;t
(t &minus; τ)= δ(t &minus; τ).
</p>
<p>22.8 Show that for m = 3 the expression for Gs(r) given by Eq. (22.36)
reduces to Gs(r)=&minus;e&minus;μr/(4πr).
</p>
<p>22.9 The time-independent Schr&ouml;dinger equation can be rewritten as
</p>
<p>(
&nabla;2 + k2
</p>
<p>)
Ψ &minus; 2μ
</p>
<p>�2
V (r)Ψ = 0,
</p>
<p>where k2 = 2μE/�2 and μ is the mass of the particle.
(a) Use techniques of Sect. 21.4 to write an integral equation for Ψ .
(b) Show that the Neumann series solution of the integral equation con-
</p>
<p>verges only if
&int;
</p>
<p>R3
</p>
<p>∣∣V (r)
∣∣2d3r &lt; 2π�
</p>
<p>4 Imk
</p>
<p>μ2
.
</p>
<p>(c) Assume that the potential is of Yukawa type: V (r)= g2e&minus;κr/r . Find a
condition between the (bound state) energy and the potential strength
g that ensures convergence of the Neumann series.
</p>
<p>22.10 Derive Eq. (22.29).
</p>
<p>22.11 Derive Eq. (22.31) using the procedure outlined for parabolic equa-
tions.
</p>
<p>22.12 Consider GF for the Helmholtz operator &nabla;2+μ2 in two dimensions.
(a) Show that
</p>
<p>G
(
r, r&prime;
</p>
<p>)
=&minus; i
</p>
<p>4
H
</p>
<p>(1)
0
</p>
<p>(
μ
∣∣r &minus; r&prime;
</p>
<p>∣∣)+H
(
r, r&prime;
</p>
<p>)
,</p>
<p/>
</div>
<div class="page"><p/>
<p>22.6 Problems 695
</p>
<p>where H(r, r&prime;) satisfies the homogeneous Helmholtz equation.
(b) Separate the variables and use the fact that H is regular at r = r&prime; to
</p>
<p>show that H can be written as
</p>
<p>H
(
r, r&prime;
</p>
<p>)
=
</p>
<p>&infin;&sum;
</p>
<p>n=0
Jn(μr)
</p>
<p>[
an
(
r&prime;
)
</p>
<p>cosnθ + bn
(
r&prime;
)
</p>
<p>sinnθ
]
.
</p>
<p>(c) Now assume a circular boundary of radius a and the BC G(a, r&prime;)= 0,
in which a is a vector from the origin to the circular boundary. Using
this BC, show that
</p>
<p>a0
(
r&prime;
)
= i
</p>
<p>8πJ0(μa)
</p>
<p>&int; 2π
</p>
<p>0
H
</p>
<p>(1)
0
</p>
<p>(
μ
</p>
<p>&radic;
a2 + r &prime;2 &minus; 2ar &prime; cos
</p>
<p>(
θ &minus; θ &prime;
</p>
<p>))
dθ,
</p>
<p>an
(
r&prime;
)
= i
</p>
<p>4πJn(μa)
</p>
<p>&times;
&int; 2π
</p>
<p>0
H
</p>
<p>(1)
0
</p>
<p>(
μ
</p>
<p>&radic;
a2 + r &prime;2 &minus; 2ar &prime; cos
</p>
<p>(
θ &minus; θ &prime;
</p>
<p>))
cosnθ dθ,
</p>
<p>bn
(
r&prime;
)
= i
</p>
<p>4πJn(μa)
</p>
<p>&times;
&int; 2π
</p>
<p>0
H
</p>
<p>(1)
0
</p>
<p>(
μ
</p>
<p>&radic;
a2 + r &prime;2 &minus; 2ar &prime; cos
</p>
<p>(
θ &minus; θ &prime;
</p>
<p>))
sinnθ dθ.
</p>
<p>These equations completely determine H(r, r&prime;) and therefore G(r, r&prime;).
</p>
<p>22.13 Use the Fourier transform technique to find the singular part of the
GF for the diffusion equation in one and three dimensions. Compare your
results with that obtained in Sect. 22.4.3.
</p>
<p>22.14 Show directly that both G(ret)s and G
(adv)
s satisfy &nabla;2G= δ(r)δ(t) in
</p>
<p>three dimensions.
</p>
<p>22.15 Consider a rectangular box with sides a, b, and c located in the first
octant with one corner at the origin. Let D denote the inside of this box.
</p>
<p>(a) Show that zero cannot be an eigenvalue of the Laplacian operator with
the Dirichlet BCs on &part;D.
</p>
<p>(b) Find the GF for this Dirichlet BVP.
</p>
<p>22.16 Find the GF for the two-dimensional Helmholtz equation (&nabla;2 +
k2)u= 0 on the rectangle 0 &le; x &le; a, 0 &le; y &le; b.
</p>
<p>22.17 For the operator ad2/dx2 + b, where a &gt; 0 and b &lt; 0, find the sin-
gular part of the one-dimensional GF.
</p>
<p>22.18 Calculate the GF of the two-dimensional Laplacian operator appro-
priate for Neumann BCs on the rectangle 0 &le; x &le; a, 0 &le; y &le; b.
</p>
<p>22.19 For the Helmholtz operator &nabla;2 &minus; k2 in the half-space z&ge; 0, find the
three-dimensional Dirichlet GF.</p>
<p/>
</div>
<div class="page"><p/>
<p>696 22 Multidimensional Green&rsquo;s Functions: Applications
</p>
<p>22.20 For the Helmholtz operator &nabla;2 &minus; k2 in the half-space z&le; 0, find the
three-dimensional Neumann GF.
</p>
<p>22.21 Using the integral form of the Schr&ouml;dinger equation in three dimen-
sions, show that an attractive delta-function potential V (r) = &minus;V0δ(r &minus; a)
does not have a bound state (E &lt; 0). Contrast this with the result of Exam-
ple 21.4.1.
</p>
<p>22.22 By taking the Fourier transform of both sides of the integral form of
the Schr&ouml;dinger equation, show that for bound-state problems (E &lt; 0), the
equation in &ldquo;momentum space&rdquo; can be written as
</p>
<p>ψ̃(p)=&minus; 2μ
(2π)3/2�2
</p>
<p>(
1
</p>
<p>κ2 + p2
)&int;
</p>
<p>Ṽ (p &minus; q)ψ̃(q) d3q,
</p>
<p>where κ2 =&minus;2μE/�2.
</p>
<p>22.23 Write the bound-state Schr&ouml;dinger integral equation for a non-local
potential, noting that G(r, r&prime;) = e&minus;κ|r&minus;r&prime;|/|r &minus; r&prime;|, where κ2 = &minus;2μE/�2
and μ is the mass of the bound particle. The homogeneous solution is zero,
as is always the case with bound states.
</p>
<p>(a) Assuming that the potential is of the form V (r, r&prime;)=&minus;g2U(r)U(r&prime;),
show that a solution to the Schr&ouml;dinger equation exists iff
</p>
<p>μg2
</p>
<p>2π�2
</p>
<p>&int;
</p>
<p>R3
d3r
</p>
<p>&int;
</p>
<p>R3
d3r &prime;
</p>
<p>e&minus;κ|r&minus;r
&prime;|
</p>
<p>|r &minus; r&prime;| U(r)U
(
r&prime;
)
= 1. (22.62)
</p>
<p>(b) Taking U(r)= e&minus;αr/r , show that the condition in (22.62) becomes
</p>
<p>4πμg2
</p>
<p>α�2
</p>
<p>[
1
</p>
<p>(α + κ)2
]
= 1.
</p>
<p>(c) Since κ &gt; 0, prove that the equation in (b) has a unique solution only
if g2 &gt; �2α2/(4πμ), in which case the bound-state energy is
</p>
<p>E =&minus; �
2
</p>
<p>2μ
</p>
<p>[(
4πμg2
</p>
<p>α�2
</p>
<p>)1/2
&minus; α
</p>
<p>]2
.
</p>
<p>22.24 Repeat calculations in Sects. 22.4.1 and 22.4.2 for m= 2.
</p>
<p>22.25 In this problem, the dimension m is three.
</p>
<p>(a) Derive the following identities:
</p>
<p>&nabla;2
[
f (r)
</p>
<p>r
</p>
<p>]
= &nabla;
</p>
<p>2f
</p>
<p>r
&minus; 2
</p>
<p>r2
</p>
<p>&part;f
</p>
<p>&part;r
+&nabla;2
</p>
<p>(
1
</p>
<p>r
</p>
<p>)
,
</p>
<p>dǫ(t)
</p>
<p>dt
= 2δ(t), &nabla;2δ(t &plusmn; r)= δ&prime;&prime;(t &plusmn; r)&plusmn; 2
</p>
<p>r
δ&prime;(t &plusmn; r),
</p>
<p>where ǫ(t)= θ(t)&minus; θ(&minus;t).</p>
<p/>
</div>
<div class="page"><p/>
<p>22.6 Problems 697
</p>
<p>(b) Use the results of (a) to show that the GF [Eq. (22.47)] derived
from the principal value of the ω integration for the wave equation
in three dimensions satisfies only the homogeneous PDE. Hint: Use
&nabla;2(1/r)= 4πδ(r).
</p>
<p>22.26 Calculate the retarded GF for the wave operator in two dimensions
and show that it is equal to
</p>
<p>G(ret)s (r, t)=
θ(t)
</p>
<p>2π
&radic;
t2 &minus; r2
</p>
<p>.
</p>
<p>Now use this result to obtain the GF for any even number of dimensions:
</p>
<p>G(ret)s (r, t)=
θ(t)
</p>
<p>2π
</p>
<p>(
&minus; 1
</p>
<p>2πr
</p>
<p>&part;
</p>
<p>&part;r
</p>
<p>)n&minus;1[ 1&radic;
t2 &minus; r2
</p>
<p>]
for n=m/2.
</p>
<p>22.27 (a) Find the singular part of the retarded GF and the advanced GF for
the wave equation in three dimensions using Eqs. (22.48) and (22.49). Hint:
J1/2(kr)=
</p>
<p>&radic;
2/πkr sinkr .
</p>
<p>(b) Use (a) and Eq. (22.51) to show that
</p>
<p>lim
ǫ&rarr;0
</p>
<p>{
1
</p>
<p>[r2 + (&minus;it + ǫ)2] &minus;
1
</p>
<p>[r2 + (it + ǫ)2]
</p>
<p>}
=&minus; iπ
</p>
<p>r
</p>
<p>[
δ(t + r)&minus; δ(t &minus; r)
</p>
<p>]
.
</p>
<p>22.28 Show that the eigenfunction expansion of the GF for the Dirichlet
BVP for the Laplacian operator in two dimensions for which the region of
interest is the interior of a circle of radius a is
</p>
<p>G
(
r, r&prime;
</p>
<p>)
=&minus; 2
</p>
<p>π
</p>
<p>&infin;&sum;
</p>
<p>n=0
</p>
<p>&infin;&sum;
</p>
<p>m=1
</p>
<p>ǫnJn(
ρ
a
xnm)Jn(
</p>
<p>ρ&prime;
a
xnm) cosn(ϕ &minus; ϕ&prime;)
</p>
<p>J 2n+1(xnm)x
2
nm
</p>
<p>,
</p>
<p>where ǫ0 = 12 and ǫn = 1 for n &ge; 1, and use has been made of Prob-
lem 15.39.
</p>
<p>22.29 Go back to Example 22.5.4, and
</p>
<p>(a) complete the calculations therein;
(b) find the GF for the Laplacian with Dirichlet BCs on two concentric
</p>
<p>spheres of radii a and b, with a &lt; b.
(c) Consider the case where a &rarr; 0 and b &rarr; &infin; and compare the result
</p>
<p>with the singular part of the GF for the Laplacian.
</p>
<p>22.30 Solve the Dirichlet BVP for the operator &nabla;2 &minus; k2 in the region 0 &le;
x &le; a, 0 &le; y &le; b, &minus;&infin;&lt; z &lt;&infin;. Hint: Separate the operator into L1 and L2.
</p>
<p>22.31 Solve the problem of Example 22.5.1 using the separation of operator
technique and show that the two results are equivalent.
</p>
<p>22.32 Use the operator separation technique to calculate the Dirichlet GF
for the two-dimensional operator &nabla;2 &minus; k2 on the rectangle 0 &le; x &le; a, 0 &le;
y &le; b. Also obtain an eigenfunction expansion for this GF.</p>
<p/>
</div>
<div class="page"><p/>
<p>698 22 Multidimensional Green&rsquo;s Functions: Applications
</p>
<p>22.33 Use the operator separation technique to find the three-dimensional
Dirichlet GF for the Laplacian in a circular cylinder of radius a and height h.
</p>
<p>22.34 Calculate the singular part of the GF for the three-dimensional free
Schr&ouml;dinger operator
</p>
<p>i�
&part;
</p>
<p>&part;t
&minus; �
</p>
<p>2
</p>
<p>2μ
&nabla;2.
</p>
<p>22.35 Use the operator separation technique to show that
</p>
<p>(a) the GF for the Helmholtz operator &nabla;2 + k2 in three dimensions is
</p>
<p>G
(
r, r&prime;
</p>
<p>)
=&minus;ik
</p>
<p>&infin;&sum;
</p>
<p>l=0
</p>
<p>l&sum;
</p>
<p>m=&minus;l
jl(kr&lt;)hl(kr&gt;)Ylm(θ,ϕ)Y
</p>
<p>&lowast;
lm
</p>
<p>(
θ &prime;, ϕ&prime;
</p>
<p>)
,
</p>
<p>where r&lt; (r&gt;) is the smaller (larger) of r and r &prime; and jl and hl are the
spherical Bessel and Hankel functions, respectively. No explicit BCs
are assumed except that there is regularity at r = 0 and that G(r, r&prime;)&rarr;
0 for |r| &rarr;&infin;.
</p>
<p>(b) Obtain the identity
</p>
<p>eik|r&minus;r
&prime;|
</p>
<p>4π |r &minus; r&prime;| = ik
&infin;&sum;
</p>
<p>l=0
</p>
<p>l&sum;
</p>
<p>m=&minus;l
jl(kr&lt;)hl(kr&gt;)Ylm(θ,ϕ)Y
</p>
<p>&lowast;
lm
</p>
<p>(
θ &prime;, ϕ&prime;
</p>
<p>)
.
</p>
<p>(c) Derive the plane wave expansion [see Eq. (19.46)]
</p>
<p>eik&middot;r = 4π
&infin;&sum;
</p>
<p>l=0
</p>
<p>l&sum;
</p>
<p>m=&minus;l
iljl(kr)Y
</p>
<p>&lowast;
lm
</p>
<p>(
θ &prime;, ϕ&prime;
</p>
<p>)
Ylm(θ,ϕ),
</p>
<p>where θ &prime; and ϕ&prime; are assumed to be the angular coordinates of k. Hint:
Let |r&prime;| &rarr;&infin;, and use
</p>
<p>∣∣r &minus; r&prime;
∣∣=
</p>
<p>(
r &prime;2 + r2 &minus; 2r &middot; r&prime;
</p>
<p>)1/2 &rarr; r &prime; &minus; r
&prime; &middot; r
r &prime;
</p>
<p>and the asymptotic formula h(1)l (z)&rarr; (1/z)ei[z+(l+1)(π/2)], valid for
large z.</p>
<p/>
</div>
<div class="page"><p/>
<p>Part VII
</p>
<p>Groups and Their Representations</p>
<p/>
</div>
<div class="page"><p/>
<p>23Group Theory
</p>
<p>The tale of mathematics and physics has been one of love and hate, of har-
mony and discord, and of friendship and animosity. From their simultane-
ous inception in the shape of calculus in the seventeenth century, through
an intense and interactive development in the eighteenth and most of the
nineteenth century, to an estrangement in the latter part of the nineteenth
and the beginning of the twentieth century, mathematics and physics have
experienced the best of times and the worst of times. Sometimes, as in the
case of calculus, nature dictates a mathematical dialect in which the narra-
tive of physics is to be spoken. Other times, man, building upon that dialect,
develops a sophisticated language in which&mdash;as in the case of Lagrangian
and Hamiltonian interpretation of dynamics&mdash;the narrative of physics is set
in the most beautiful poetry. But the happiest courtship, and the most exhila-
rating relationship, takes place when a discovery in physics leads to a devel-
opment in mathematics that in turn feeds back into a better understanding of
physics, leading to new ideas or a new interpretation of existing ideas. Such
a state of affairs began in the 1930s with the advent of quantum mechanics,
and, after a lull of about 30 years, revived in the late 1960s. We are fortu-
nate to be witnesses to one of the most productive collaborations between
the physics and mathematics communities in the history of both.
</p>
<p>It is not an exaggeration to say that the single most important catalyst
that has facilitated this collaboration is the idea of symmetry the study of
which is the main topic of the theory of groups, the subject of this chapter.
Although group theory, in one form or another, was known to mathemati-
cians as early as the beginning of the nineteenth century, it found its way
into physics only after the invention of quantum theory, and in particular,
Dirac&rsquo;s interpretation of it in the language of transformation theory. Eugene
Wigner, in his seminal paper1 of 1939 in which he applied group theoretical
ideas to Lorentz transformations, paved the way for the marriage of group
theory and quantum mechanics. Today, in every application of quantum the-
ory, be it to atoms, molecules, solids, or elementary particles such as quarks
and leptons, group-theoretical techniques are indispensable.
</p>
<p>1E.P. Wigner, On the Unitary Representations of the Inhomogeneous Lorentz Group, Ann.
of Math. 40 (1939) 149&ndash;204.
</p>
<p>S. Hassani, Mathematical Physics, DOI 10.1007/978-3-319-01195-0_23,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>701</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-01195-0_23">http://dx.doi.org/10.1007/978-3-319-01195-0_23</a></div>
</div>
<div class="page"><p/>
<p>702 23 Group Theory
</p>
<p>23.1 Groups
</p>
<p>The prototype of a group is a transformation group, the set of invertible map-
pings of a set onto itself. Let us elaborate on this. First, we take mappings
because they are the most general operations performed between sets. From
a physical standpoint, mappings are essential in understanding the symme-
tries and other basic properties of a theory. For instance, rotations and trans-
lations are mappings of space. Second, the mappings ought to be on a single
set, because we want to be able to compose any given two mappings. We
cannot compose f : A&rarr; B and g : A&rarr; B , because, by necessity, the do-
main of the second must be a subset of the image of the first. With three sets,
</p>
<p>and A
f&minus;&rarr; B , B g&minus;&rarr; C, even if the composition f ◦g is defined, g ◦f will not
</p>
<p>be. Third, we want to be able to undo the mapping. Physically, this means
that we should be able to retrace our path to our original position in the set.
This can happen only if all mappings of interest have an inverse. Finally, we
note that composing a mapping with its inverse yields identity. Therefore,
the identity map must also be included in the set of mappings.
</p>
<p>We shall come back to transformation groups frequently. In fact, almost
all groups considered in this book are transformation groups. However, as
in our study of vector spaces in Chap. 2, it is convenient to give a general
description of (abstract) groups.
</p>
<p>Definition 23.1.1 A group is a set G together with an associative binary
Group defined
</p>
<p>operation G&times;G&rarr;G called multiplication&mdash;and denoted generically by
⋆&mdash;having the following properties:
</p>
<p>1. There exists a unique element2 e &isin;G called the identity such that e ⋆
g = g ⋆ e= g.
</p>
<p>2. For every element g &isin; G, there exists an element g&minus;1, called the in-
verse of g, such that g ⋆ g&minus;1 = g&minus;1 ⋆ g = e.
</p>
<p>To emphasize the binary operation of a group, we designate it as (G,⋆).
</p>
<p>Historical Notes
</p>
<p>&Eacute;variste Galois (1811&ndash;1832) was definitely not the stereotypically dull mathematician,
</p>
<p>&Eacute;variste Galois
</p>
<p>1811&ndash;1832
</p>
<p>quietly creating theorems and teaching students. He was a political firebrand whose life
ended in a mysterious duel when he was only 21 years old. An ardent republican, he
was in the unfortunate position of having Cauchy, an ardent royalist, as the only French
mathematician capable of understanding the significance of his work. His professional
accomplishments (fewer than 100 pages, much of which was published posthumously)
received the attention they deserved many years later. It is truly sad to realize that for
decades, work from the man credited with the foundation of group theory were lost to
the world of mathematics. Galois&rsquo;s early years were relatively happy. His father, a liberal
thinker known for his wit, was director of a boarding school and later mayor of Bourg-la-
Reine. Galois&rsquo;s mother took charge of his early education. A stubborn, eccentric woman,
she mixed classical culture with a fairly stern religious upbringing.
The young Galois entered the College Louis-le-Grand in 1823, but found the harsh disci-
pline imposed by church and political authorities difficult to bear. His interest in mathe-
matics was sparked in class by Vernier, but Galois quickly tired of the elementary char-
acter of the material, preferring instead to read the more advanced original works on his
</p>
<p>2To distinguish between identities of different groups, we sometimes write eG for the
identity of the group G.</p>
<p/>
</div>
<div class="page"><p/>
<p>23.1 Groups 703
</p>
<p>own. After a flawed attempt to solve the general fifth-order equation, Galois submitted a
paper to the Acad&eacute;mie des Sciences in which he described the definitive solution with the
aid of group theory, of which the young Galois can be considered the creator. However,
this strong initial foray into the frontiers of mathematics was accompanied by tragedy and
setback. A few weeks after the paper&rsquo;s submission, his father committed suicide, which
Galois felt was largely to be blamed on those who politically persecuted his father. A
month later the young mathematician failed the entrance examination to the Ecole Poly-
technique, largely due to his refusal to answer in the form demanded by the examiner.
Galois did gain entrance to a less prestigious school for the preparation of secondary-
school teachers. While there he read some of Abel&rsquo;s results (published after Abel&rsquo;s death)
and found that they contained some of the results he had submitted to the Academy in-
cluding the proof of the impossibility of solving quintics. Cauchy, assigned as the judge
for Galois&rsquo;s paper, suggested that he revise it in light of this new information. Galois in-
stead wrote an entirely new manuscript and submitted it in competition for the grand prix
in mathematics. Tragically, the manuscript was lost on the death of Fourier, who had been
assigned to examine it, leaving Galois out of the competition. These events, fueled by a
later, unfair dismissal of another of his papers by Poisson, seem to have driven Galois to-
ward political radicalism and rebellion during the renewed turmoil then plaguing France.
He was arrested several times for his agitations, although he continued work on mathe-
matics while in custody. On May 30, 1832, he was wounded in a duel with an unknown
adversary, the duel perhaps caused by an unhappy love affair. His funeral three days later
sparked riots that raged through Paris in the days that followed.
The delay in recognition of the true scope of Galois&rsquo;s scant but amazing work stemmed
partly from the originality of his ideas and the lack of competent local reviewers. Cauchy
left France after seeing only the early parts of Galois&rsquo;s work, and much of the rest re-
mained unnoticed until Liouville prepared the later manuscripts for publication a decade
after Galois&rsquo;s death. Their true value wasn&rsquo;t appreciated for another two decades. The
young mathematician himself added to the difficulty by deliberately making his writing
so terse that the &ldquo;established scientists&rdquo; for whom he had so much disdain could not un-
derstand it. Those fortunate enough to appreciate Galois&rsquo;s work found fertile ground in
mathematical research, in such fundamental fields as group theory and modern algebra,
for decades to come.
</p>
<p>If the underlying set G has a finite number of elements, the group is called
order of a group
</p>
<p>finite, and its number of elements, denoted by |G|, is called the order of G.
We can also have an infinite group whose cardinality can be countable or
continuous.
</p>
<p>Given an element a &isin;G, we write
</p>
<p>ak &equiv; a ⋆ a ⋆ &middot; &middot; &middot; ⋆ a︸ ︷︷ ︸
k times
</p>
<p>, a&minus;m &equiv; a&minus;1 ⋆ a&minus;1 ⋆ &middot; &middot; &middot; ⋆ a&minus;1︸ ︷︷ ︸
m times
</p>
<p>and note that
</p>
<p>ai ⋆ aj = ai+j for all i, j &isin; Z.
</p>
<p>Example 23.1.2 The following are examples of familiar sets that have
group properties.
</p>
<p>(a) The set Z of integers under the binary operation of addition forms a
group whose identity element is 0 and the inverse of n is &minus;n. This
group is countably infinite.
</p>
<p>(b) The set {&minus;1,+1}, under the binary operation of multiplication, forms
a group whose identity element is 1 and the inverse of each element is
itself. This group is finite.
</p>
<p>(c) The set {&minus;1,+1,&minus;i,+i}, under the binary operation of multiplica-
tion, forms a finite group whose identity element is 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>704 23 Group Theory
</p>
<p>(d) The set R, under the binary operation of addition, forms a group
whose identity element is 0 and the inverse of r is &minus;r . This group
is uncountably infinite.
</p>
<p>(e) The set R+ (Q+) of positive real (rational) numbers, under the binary
operation of multiplication, forms a group whose identity element is
1 and the inverse of r is 1/r . This group is uncountably (countably)
infinite.
</p>
<p>(f) The set C, under the binary operation of addition, forms a group whose
identity element is 0 and the inverse of z is &minus;z. This group is uncount-
ably infinite.
</p>
<p>(g) The uncountably infinite set C&minus; {0} of all complex numbers except
0, under the binary operation of multiplication, forms a group whose
identity element is 1 and the inverse of z is 1/z.
</p>
<p>(h) The uncountably infinite set V of vectors in a vector space, under the
binary operation of addition, forms a group whose identity element is
the zero vector and the inverse of |a〉 is &minus;|a〉.
</p>
<p>(i) The set of invertible n &times; n matrices, under the binary operation of
multiplication, forms a group whose identity element is the n&times; n unit
matrix and the inverse of A is A&minus;1. This group is uncountably infinite.
</p>
<p>The reader is urged to verify that each set given above is indeed a group.
</p>
<p>In general, the elements of a group do not commute. Those groups whose
elements do commute are so important that we give them a special name:
</p>
<p>Definition 23.1.3 A group (G,⋆) is called abelian or commutative if
abelian groups defined
</p>
<p>a ⋆ b= b ⋆ a for all a, b &isin;G. It is common to denote the binary operation of
an abelian group by +.
</p>
<p>All groups of Example 23.1.2 are abelian except the last.
</p>
<p>Example 23.1.4 Let A be a vector potential that gives rise to a magnetic
field B. The set of transformations of A that give rise to the same B is an
abelian group. In fact, such transformations simply add the gradient of a
function to A. The reader can check the details.
</p>
<p>The reader may also verify that the set of invertible mappings f : S &rarr; S,
i.e., the set of transformations of S, is indeed a (nonabelian) group. If S has
n elements, this group is denoted by Sn and is called the symmetric groupsymmetric or
</p>
<p>permutation group
of S. Sn is a nonabelian (unless n&le; 2) finite group that has n! elements. An
element g of Sn is usually denoted by two rows, the top row being S itself&mdash;
usually taken to be 1,2, . . . , n&mdash;and the bottom row its image under g. For
example, g &equiv;
</p>
<p>( 1 2 3 4
2 3 4 1
</p>
<p>)
is an element of S4 such that g(1) = 2, g(2) = 3,
</p>
<p>g(3)= 4, and g(4)= 1.
Consider two groups, the set of vectors in a plane ((x, y),+) and the set
</p>
<p>of complex numbers (C,+), both under addition. Although these are two
different groups, the difference is superficial. We have seen similar differ-
ences in disguise in the context of vector spaces and the notion of isomor-
phism. The same notion applies to group theory:</p>
<p/>
</div>
<div class="page"><p/>
<p>23.2 Subgroups 705
</p>
<p>Definition 23.1.5 Let (G,⋆) and (H,&bull;) be groups. A map f : G&rarr; H is
called a homomorphism if homomorphism,
</p>
<p>isomorphism, and
</p>
<p>automorphismf (a ⋆ b)= f (a) &bull; f (b) &forall;a, b &isin;G.
</p>
<p>An isomorphism is a homomorphism that is also a bijection. Two groups
are isomorphic, denoted by G&sim;=H , if there is an isomorphism f :G&rarr;H .
An isomorphism of a group onto itself is called an automorphism.
</p>
<p>An immediate consequence of this definition is that f (eG) = eH and
f (g&minus;1)= [f (g)]&minus;1 (see Problem 23.9).
</p>
<p>Example 23.1.6 Let G be any group and {1} the multiplicative group con-
sisting of the single number 1. It is straightforward to show that f : G&rarr;
{1}, given by (the only function available!) f (g)= 1 for all g &isin;G is a ho-
momorphism. This homomorphism is called the trivial (or sometimes, sym- trivial homomorphism
metric) homomorphism.
</p>
<p>The establishment of isomorphism f :R2 &rarr;C between ((x, y),+), and
(C,+) is trivial: Just write f (x, y)= x + iy. A less trivial isomorphism is
the exponential map, exp : (R,+) &rarr; (R+, &middot;). The reader may verify that
this is a homomorphism (in particular, it maps addition to multiplication)
and that it is one-to-one.
</p>
<p>We have noted that the set of invertible maps of a set forms a group.
A very important special case of this is when the set is a vector space V and
</p>
<p>general linear group
the maps are all linear.
</p>
<p>Box 23.1.7 The general linear group of a vector space V, denoted
by GL(V), is the set of all invertible endomorphisms of V. In particu-
lar, when V=Cn, we usually write GL(n,C) instead of GL(Cn) with
similar notation for R.
</p>
<p>It is sometimes convenient to display a finite group G = {gi}|G|i=1 as a
|G| &times; |G| table, called the group multiplication table, in which the inter- group multiplication
</p>
<p>tablesection of the ith row and j th column is occupied by gi ⋆ gj . Because of its
trivial multiplication, the identity is usually omitted from the table.
</p>
<p>23.2 Subgroups
</p>
<p>It is customary to write ab instead of a ⋆ b. We shall adhere to this conven-
tion, but restore the ⋆ as necessary to avoid any possible confusion.
</p>
<p>Definition 23.2.1 A subset S of a group G is a subgroup of G if it is a subgroup defined
group in its own right under the binary operation of G, i.e., if it contains the
inverse of all its elements as well as the product of any pair of its elements.</p>
<p/>
</div>
<div class="page"><p/>
<p>706 23 Group Theory
</p>
<p>It follows from this definition that e &isin; S. It is also easy to show that the
intersection of two subgroups is a subgroup (Problem 23.2).
</p>
<p>Example 23.2.2 (Examples of subgroups)
</p>
<p>1. For any G, the subset {e}, consisting of the identity alone, is a subgroup
of G called the trivial subgroup of G.
</p>
<p>2. (Z,+) is a subgroup of (R,+).
3. The set of even integers (but not odd integers) is a subgroup of (Z,+).
</p>
<p>In fact, the set of all multiples of a positive integer m, denoted by Zm,
is a subgroup of Z. It turns out that all subgroups of Z are of this form.
</p>
<p>trivial subgroup
</p>
<p>4. The subset of GL(n,C) consisting of transformations that have unit
determinant is a subgroup of GL(n,C) because the identity transfor-
mation has unit determinant, the inverse of a transformation with unit
determinant also has unit determinant, and the product of two transfor-
mations with unit determinants has unit determinant.
</p>
<p>special linear group
Box 23.2.3 The subgroup of GL(n,C) consisting of elements having
unit determinant is denoted by SL(n,C) and is called the special lin-
ear group.
</p>
<p>5. The set of unitary transformations of Cn, denoted by U(n), is a sub-unitary, orthogonal,
special unitary, and
</p>
<p>special orthogonal
</p>
<p>groups
</p>
<p>group of GL(n,C) because the identity transformation is unitary, the
inverse of a unitary transformation is also unitary, and the product of
two unitary transformations is unitary.
</p>
<p>Box 23.2.4 The set of unitary transformations U(n) is a subgroup of
GL(n,C) and is called the unitary group. Similarly, the set of orthog-
onal transformations of Rn is a subgroup of GL(n,R). It is denoted
by O(n) and called the orthogonal group.
</p>
<p>Each of these groups has a special subgroup whose elements have unit
determinants. These are denoted by SU(n) and SO(n), and called spe-
cial unitary group and special orthogonal group, respectively. The
latter is also called the group of rigid rotations of Rn.
</p>
<p>6. Let x,y &isin;Rn, and define an inner product on Rn by
</p>
<p>x &middot; y =&minus;x1y1 &minus; &middot; &middot; &middot; &minus; xpyp + xp+1yp+1 + &middot; &middot; &middot; + xnyn.
</p>
<p>Denote the subset of GL(n,R) that leaves this inner product invariant
by3 O(p,n&minus; p). Then O(p,n&minus; p) is a subgroup of GL(n,R). The
set of linear transformations among O(p,n&minus;p) that have determinant
</p>
<p>3The reader is warned that what we have denoted by O(p,n&minus; p) is sometimes denoted
by other authors by O(n&minus; p,p) or O(n,p) or O(p,n).</p>
<p/>
</div>
<div class="page"><p/>
<p>23.2 Subgroups 707
</p>
<p>1 is denoted by SO(p,n&minus; p). The special case of p = 0 gives us the
orthogonal and special orthogonal groups.4 When n= 4 and p = 3, we
get the inner product of the special theory of relativity, and O(3,1),
the set of Lorentz transformations, is called the Lorentz group. If one Lorentz and Poincar&eacute;
</p>
<p>groupsadds translations of R4 to O(3,1), one obtains the Poincar&eacute; group,
P(3,1).
</p>
<p>7. Let x,y &isin; R2n, and J the 2n&times; 2n matrix
( 0 1
&minus;1 0
</p>
<p>)
, where 1 is the n&times; n
</p>
<p>unit matrix. The subset of GL(2n,R) that leaves xtJx, called an anti-
symmetric bilinear form, invariant is a subgroup of GL(2n,R) called symplectic group
the symplectic group and denoted by Sp(2n,R). As we shall see in
Chap. 28, the symplectic group is fundamental in the formal treatment
of Hamiltonian mechanics.
</p>
<p>8. Let S be a subgroup of G and g &isin;G. Then it is readily shown that the conjugate subgroup
set
</p>
<p>g&minus;1Sg &equiv;
{
g&minus;1sg | s &isin; S
</p>
<p>}
</p>
<p>is also a subgroup of G, called the subgroup conjugate to S under g,
or the subgroup g-conjugate to S.
</p>
<p>When discussing vector spaces, we noted that given any subset of a vector
space, one could construct a subspace out of it by taking all possible linear
combinations (natural operations of the vector space) of the vectors in the
subset. We called the subspace thus obtained the span of the subset. The
same procedure is applicable in group theory as well. If S is a subset of a
group G, we can generate a subgroup out of S by collecting all possible
products and inverses (natural operations of the group) of the elements of S.
The reader may verify that the result is indeed a subgroup of G.
</p>
<p>Definition 23.2.5 Let S be a subset of a group G. The subgroup generated subgroup generated by
a subsetby S, denoted by 〈S〉, is the union of S and all inverses and products of the
</p>
<p>elements of S.
</p>
<p>In the special case for which S = {a}, a single element, we use 〈a〉 instead cyclic subgroup
of 〈{a}〉 and call it the cyclic subgroup generated by a. It is simply the
collection of all integer powers of a.
</p>
<p>Definition 23.2.6 Let G be a group and a, b &isin; G. The commutator of a commutator of group
elementsand b, denoted by [a, b], is
</p>
<p>[a, b] &equiv; aba&minus;1b&minus;1.
</p>
<p>The subgroup 〈⋃a,b&isin;G[a, b]〉 generated by all commutators of G is
called the commutator subgroup of G. The reader may verify that a group commutator subgroup
</p>
<p>of a groupis abelian if and only if its commutator subgroup is the trivial subgroup, i.e.,
consists of only the identity element.
</p>
<p>4It is customary to write O(n) and SO(n) for O(0, n) and SO(0, n).</p>
<p/>
</div>
<div class="page"><p/>
<p>708 23 Group Theory
</p>
<p>Definition 23.2.7 Let x &isin;G. The set of elements of g that commute with x,centralizer of an element
inG and the center ofG denoted by CG(x), is called the centralizer of x in G. The set Z(G) of
</p>
<p>elements of a group G that commute with all elements of G is called the
center of G.
</p>
<p>Theorem 23.2.8 CG(x) is a subgroup of G and Z(G) is an abelian sub-
group of G. Furthermore, G is abelian if and only if Z(G)=G.
</p>
<p>Proof Proof is immediate from the definitions. �
</p>
<p>Definition 23.2.9 Let G and H be groups and let f :G&rarr;H be a homo-
morphism. The kernel of f is
</p>
<p>kernel of a
</p>
<p>homomorphism
</p>
<p>kerf &equiv;
{
x &isin;G | f (x)= e &isin;H
</p>
<p>}
.
</p>
<p>The reader may check that kerf is a subgroup of G, and f (G) is a sub-
group of H . These are the analogues of the same concepts encountered in
vector spaces. In fact, if we treat a vector space as an additive group, with
the zero vector as identity, then the above definition coincides with that of
linear mappings and vector spaces.
</p>
<p>Carrying the analogy further, we recall that given two subspaces U and
W of a vector space V, we denote by U +W all vectors of V that can be
written as the sum of a vector in U and a vector in W. There is a similar
concept in group theory that is sometimes very useful.
</p>
<p>Definition 23.2.10 Let S and T be subsets of a group (G,⋆). Then one
defines the product of these subsets as
</p>
<p>S ⋆ T &equiv; {s ⋆ t | s &isin; S and t &isin; T }.
</p>
<p>In particular, if T consists of a single element t , then
</p>
<p>S ⋆ t = {s ⋆ t | s &isin; S}.
</p>
<p>As usual, we shall drop the ⋆ and write ST and St . If S is a subgroup, thenleft and right cosets
St is called a right coset5 of S in G. Similarly, tS is called a left coset of S
in G. In either case, t is said to represent the coset.
</p>
<p>Example 23.2.11 Let G= R3 treated as an additive abelian group, and let
S be a plane through the origin. Then t + S is S if t &isin; S (see Problem 23.5);
otherwise, it is a plane parallel to S. In fact, t + S is simply the translation
of all points of S by t .
</p>
<p>Theorem 23.2.12 Any two right (left) cosets of a subgroup are either dis-
joint or identical.
</p>
<p>5Some authors switch our right and left in their definition.</p>
<p/>
</div>
<div class="page"><p/>
<p>23.2 Subgroups 709
</p>
<p>Proof Let S be a subgroup of G and suppose that x &isin; Sa &cap; Sb. Then
x = s1a = s2b with s1, s2 &isin; S. Hence, ab&minus;1 = s&minus;11 s2 &isin; S. By Problem 23.6,
Sa = Sb. The left cosets can be treated in the same way. �
</p>
<p>A more &ldquo;elegant&rdquo; proof starts by showing that an equivalence relation
can be defined on G by
</p>
<p>a ⊲⊳ b &lArr;&rArr; ab&minus;1 &isin; S
</p>
<p>and then proving that the equivalence classes of this relation are cosets of S.
One interpretation of Theorem 23.2.12 is that a and b belong to the same
</p>
<p>right coset of S if and only if ab&minus;1 &isin; S. A second interpretation is that a
coset can be represented by any one of its elements (why?).
</p>
<p>All cosets (right or left) of a subgroup S have the same cardinality as S
itself. This can readily be established by considering the map φ : S &rarr; Sa
(φ : S &rarr; aS) with φ(s)= sa (φ(s)= as) and showing that φ is bijective.
</p>
<p>There are many instances both in physics and mathematics in which a
collection of points of a given set represent a single quantity. For example,
it is not simply the set of ratios of integers that comprise the set of rational
numbers, but the set of certain collections of such ratios: The rational num-
ber 12 represents
</p>
<p>1
2 ,
</p>
<p>2
4 ,
</p>
<p>3
6 , etc. Similarly, a given magnetic field represents an
</p>
<p>infinitude of vector potentials each differing by a gradient from the others,
and a physical state in quantum mechanics is an infinite number of wave
functions differing from one another by a phase.
</p>
<p>With the set of cosets constructed above, it is natural to ask whether
they could be given an algebraic structure.6 The most natural such struc-
ture would clearly be that of a group: Given aS and bS define their product
as abS. Would this operation turn the set of (left) cosets into a group? The
following argument shows that it will, under an important restriction.
</p>
<p>It is clear that the identity of such a group would be S itself. It is equally
clear that we should have (b&minus;1S)(bS)= S, so that (b&minus;1Sb)S = S. It follows
from Problem 23.5 that we must have b&minus;1Sb&sub; S for all b &isin;G. Now replace
b with b&minus;1 and note that bSb&minus;1 &sub; S as well. Let s be an arbitrary element
of S. Then bsb&minus;1 = s&prime; for some s&prime; &isin; S, and s = b&minus;1s&prime;b &isin; b&minus;1Sb. It follows
that S &sub; b&minus;1Sb for all b &isin;G, and, with the reverse inclusion derived above,
that S = b&minus;1Sb. This motivates the following definition.
</p>
<p>Definition 23.2.13 A subgroup N of a group G is called normal if N = normal subgroup
definedg&minus;1Ng (equivalently if Ng = gN ) for all g &isin;G.
</p>
<p>The preceding argument shows that the set of cosets (no specification
is necessary since the right and left cosets coincide) of a normal subgroup
forms a group:
</p>
<p>6The set of cosets of a subgroup is the analog of factor space of a subspace of a vector
space (Sect. 2.1.2) and factor algebra of a subalgebra of an algebra (Sect. 3.2.1). We have
seen that, while a factor space of any subspace can be turned into a vector space, that is
not the case with an algebra: the subalgebra must be an ideal of the algebra. There is a
corresponding restriction for the subgroup.</p>
<p/>
</div>
<div class="page"><p/>
<p>710 23 Group Theory
</p>
<p>Theorem 23.2.14 If N is a normal subgroup of G, then the collection ofquotient or factor group
all cosets of N , denoted by G/N , is a group, called the quotient group or
factor group of G by N .
</p>
<p>We note that the only subgroup conjugate to a normal subgroup N is N
itself (see Example 23.2.2), and that all subgroups of an abelian group are
automatically normal.
</p>
<p>Example 23.2.15 Let G=R3 and let S be a plane through the origin as in
Example 23.2.11. Since G is abelian, S is automatically normal, and G/S
is the set of planes parallel to S. Let ên be a normal to S. Then it is readily
seen that
</p>
<p>G/S = {r ên + S | r &isin;R}.
</p>
<p>We have picked the perpendicular distance between a plane and S (with
sign included) to represent that plane. The reader may check that the quo-
tient group G/S is isomorphic to R. Identifying S with R2, we can write
R3/R2 &sim;=R. The cancellation of exponents is quite accidental here!
</p>
<p>Let G = Z and S = Zm, the set of multiples of the positive integer m.
Since Z is abelian, Zm is normal, and Z/Zm is indeed a group, a typical
element of which looks like k +mZ. By adding (or subtracting) multiples
of m to k, and using mj +mZ = mZ (see Problem 23.5), we can assume
that 0 &le; k &le;m&minus; 1. It follows that Z/Zm is a finite group. Furthermore,
</p>
<p>(k1 +mZ)+ (k2 +mZ)= k1 + k2 +mZ= k+mZ,
</p>
<p>where k is the remainder after enough multiples of m have been subtracted
from k1 + k2. One writes k1 + k2 &equiv; k (mod m). The coset k+mZ is some-
times denoted by k and the quotient group Z/Zm by Zm:
</p>
<p>Zm = {0,1,2, . . . ,m&minus; 1}.
</p>
<p>Zm is a prototype of the finite cyclic groups. It can be shown that every
cyclic group of order m is isomorphic to Zm a generator of which is 1̄ (recall
that the binary operation is addition for Zm).
</p>
<p>Theorem 23.2.16 (First isomorphism theorem) Let G and H be groupsfirst isomorphism
theorem and f :G&rarr;H a homomorphism. Then kerf is a normal subgroup of G,
</p>
<p>and G/kerf is isomorphic to f (G).
</p>
<p>Proof We have already seen that kerf is a subgroup of G. To show that it
is normal, let g &isin;G and x &isin; kerf . Then
</p>
<p>f
(
gxg&minus;1
</p>
<p>)
= f (g)f (x)f
</p>
<p>(
g&minus;1
</p>
<p>)
= f (g)eHf
</p>
<p>(
g&minus;1
</p>
<p>)
= f (g)f
</p>
<p>(
g&minus;1
</p>
<p>)
</p>
<p>= f
(
gg&minus;1
</p>
<p>)
= f (eG)= eH .</p>
<p/>
</div>
<div class="page"><p/>
<p>23.2 Subgroups 711
</p>
<p>It follows that gxg&minus;1 &isin; kerf . Therefore, kerf is normal. We leave it
to the reader to show that φ : G/kerf &rarr; f (G) given by φ(g[kerf ]) &equiv;
φ([kerf ]g)= f (g) is an isomorphism.7 �
</p>
<p>Example 23.2.17 The special linear group of V is a normal subgroup of
the general linear group of V. To see this, note that det : GL(V)&rarr; R+ is a
homomorphism whose kernel is SL(V).
</p>
<p>Definition 23.2.18 Let x &isin;G. A conjugate of x is an element y of G that conjugate and
conjugacy class definedcan be written as y = gxg&minus;1 with g &isin;G. The set of all elements of G con-
</p>
<p>jugate to one another is called a conjugacy class. The ith conjugacy class is
denoted by Ki .
</p>
<p>One can check that &ldquo;x is conjugate to y&rdquo; is an equivalence relation whose
classes are the conjugacy classes. In particular, two different conjugacy
classes are disjoint. One can also show that each element of the center of
a group constitutes a class by itself. In particular, the identity in any group is
in a class by itself, and each element of an abelian group forms a (different)
class.
</p>
<p>Although a normal subgroup N contains the conjugate of each of its ele-
ments, N is not a class. The class containing any given element of N will be
only a proper subset of N (unless N is trivial). The characteristic feature of a
normal subgroup is that it contains the conjugacy classes of all its elements.
This is not shared by other subgroups, which, in general, contain only the
trivial class of the identity element.
</p>
<p>Example 23.2.19 Consider the group SO(3) of rotations in three dimen-
sions. Let us denote a rotation by Rê(θ), where ê is the direction of the axis
of rotation and θ is the angle of rotation. A typical member of the conjugacy
class of Rê(θ) is RRê(θ)R
</p>
<p>&minus;1, where R is some rotation. Let ê&prime; =Rê be the
vector obtained by applying the rotation R on ê, and note that
</p>
<p>RRê(θ)R
&minus;1ê&prime; =RRê(θ)R&minus;1Rê =RRêê =Rê = ê&prime;,
</p>
<p>where we used the fact that Rêê = ê because a rotation leaves its axis un-
changed. This last statement, applied to the equation above, also shows that all rotations having the
</p>
<p>same angle belong to
</p>
<p>the same conjugacy
</p>
<p>class
</p>
<p>RRê(θ)R
&minus;1 is a rotation about ê&prime;. Problem 23.18 establishes that the an-
</p>
<p>gle of rotation associated with RRê(θ)R
&minus;1 is θ . We can summarize this as
</p>
<p>RRê(θ)R
&minus;1 =Rê&prime;(θ).
</p>
<p>The result of this example is summarized as follows:
</p>
<p>Box 23.2.20 All rotations having the same angle belong to the same
conjugacy class of the group of rotations in three dimensions.
</p>
<p>7Compare this theorem with the set-theoretic result obtained in Chap. 1 where the map
X/ ⊲⊳&rarr; f (X) was shown to be bijective if ⊲⊳ is the equivalence relation induced by f .</p>
<p/>
</div>
<div class="page"><p/>
<p>712 23 Group Theory
</p>
<p>23.2.1 Direct Products
</p>
<p>The resolution of a vector space into a direct sum of subspaces was a useful
tool in revealing its structure. The same idea can also be helpful in studying
groups. Recall that the only vector common to the subspaces of a direct sum
is the zero vector. Moreover, any vector of the whole space can be written as
the sum of vectors taken from the subspaces of the direct sum. Considering
a vector space as a (abelian) group, with zero as the identity and summation
as the group operation, leads to the notion of direct product.
</p>
<p>Definition 23.2.21 A group G is said to be the direct product of two of itsinternal direct product of
groups subgroups H1 and H2, and we write G=H1 &times;H2, if
</p>
<p>1. all elements of H1 commute with all elements of H2;
2. the group identity is the only element common to both H1 and H2;
3. every g &isin;G can be written as g = h1h2 with h1 &isin;H1 and h2 &isin;H2.
</p>
<p>It follows from this definition that h1 and h2 are unique, and H1 and
H2 are normal. This kind of direct product is sometimes called internal,
because the &ldquo;factors&rdquo; H1 and H2 are chosen from inside the group G itself.
The external direct product results when we take two unrelated groups and
make a group out of them:
</p>
<p>Proposition 23.2.22 LetG andH be groups. The Cartesian productG&times;Hexternal direct product
of groups can be given a group structure by
</p>
<p>(g,h) ⋆
(
g&prime;, h&prime;
</p>
<p>)
&equiv;
(
gg&prime;, hh&prime;
</p>
<p>)
.
</p>
<p>With this multiplication, G&times;H is called the external direct product of G
and H . Furthermore, G &sim;= G &times; {eH }, H &sim;= {eG} &times; H , G &times; H &sim;= H &times; G,
and to within these isomorphisms, G&times;H is the internal direct product of
G&times; {eH } and {eG} &times;H .
</p>
<p>The proof is left for the reader.
</p>
<p>Historical Notes
</p>
<p>Niels Henrik Abel (1802&ndash;1829) was the second of seven children, son of a Lutheran
</p>
<p>Niels Henrik Abel
</p>
<p>1802&ndash;1829
</p>
<p>minister with a small parish of Norwegian coastal islands. In school he received only av-
erage marks at first, but then his mathematics teacher was replaced by a man only seven
years older than Abel. Abel&rsquo;s alcoholic father died in 1820, leaving almost no financial
support for his young prodigy, who became responsible for supporting his mother and
family. His teacher, Holmboe, recognizing his talent for mathematics, raised money from
his colleagues to enable Abel to attend Christiania (modern Oslo) University. He entered
the university in 1821, 10 years after the university was founded, and soon proved him-
self worthy of his teacher&rsquo;s accolades. His second paper, for example, contained the first
example of a solution to an integral equation.
Abel then received a two-year government travel grant and journeyed to Berlin, where
he met the prominent mathematician Crelle, who soon launched what was to become
the leading German mathematical journal of the nineteenth century, commonly called
Crelle&rsquo;s Journal. From the start, Abel contributed important papers to Crelle&rsquo;s Journal,
including a classic paper on power series, the scope of which clearly reflects his desire
for stringency. His most important work, also published in that journal, was a lengthy
treatment of elliptic functions in which Abel incorporated their inverse functions to show</p>
<p/>
</div>
<div class="page"><p/>
<p>23.3 Group Action 713
</p>
<p>that they are a natural generalization of the trigonometric functions. In later research in
this area, Abel found himself in stiff competition with another young mathematician,
K.G.J. Jacobi. Abel published some papers on functional equations and integrals in 1823.
In it he gives the first solution of an integral equation. In 1824 he proved the impossibility
of solving algebraically the general equation of the fifth degree and published it at his
own expense hoping to obtain recognition for his work.
Despite his proven intellectual success, Abel never achieved material success, not even
a permanent academic position. In December of 1828, while traveling by sled to visit
his fianc&eacute; for Christmas, Abel became seriously ill and died a couple of months later.
Ironically, his death from tuberculosis occurred two days before Crelle wrote with the
happy news of an appointment for Abel at a scientific institute in Berlin. In Abel&rsquo;s eulogy
in his journal, Crelle wrote:
&ldquo;He distinguished himself equally by the purity and nobility of his character and by a rare
modesty which made his person cherished to the same degree as was his genius.&rdquo;
</p>
<p>23.3 Group Action
</p>
<p>The transformation groups introduced at the beginning of this chapter can
be described in the language of abstract groups.
</p>
<p>Definition 23.3.1 Let G be a group and M a set. The left action of G on left action, right action,
left invariance and right
</p>
<p>invariance
M is a map Φ :G&times;M &rarr;M such that
</p>
<p>1. Φ(e,m)=m for all m &isin;M ;
2. Φ(g1g2,m)=Φ(g1,Φ(g2,m)).
</p>
<p>One usually denotes Φ(g,m) by g &middot; m or more simply by gm. The right
action is defined similarly. A subset N &sub;M is called left (right) invariant
if g &middot;m &isin;N (m &middot; g &isin;N ) for all g &isin;G, whenever m &isin;N .
</p>
<p>Example 23.3.2 If we define fg :M &rarr;M by fg(m) &equiv; Φ(g,m) = g &middot;m, any group is isomorphic
to a subgroup of the
</p>
<p>group of transformations
</p>
<p>of an appropriate set
</p>
<p>then fg is recognized as a transformation of M . The collection of such trans-
formations is a subgroup of the set of all transformations of M . Indeed, the
identity transformation is simply fe, the inverse of fg is fg&minus;1 , and the (asso-
ciative) law of composition is fg1 ◦ fg2 = fg1g2 . There is a general theorem
in group theory stating that any group is isomorphic to a subgroup of the
group of transformations of an appropriate set.
</p>
<p>Definition 23.3.3 Let G act on M and let m &isin;M . The orbit of m, denoted orbit, stabilizer; transitive
action and effective
</p>
<p>action
by Gm, is
</p>
<p>Gm= {x &isin;M | x = gm for some g &isin;G}.
</p>
<p>The action is called transitive if Gm = M . The stabilizer of m is Gm =
{g &isin;G | gm=m}. The group action is called free if Gm = {e} for all m &isin;
M ; it is called effective if gm=m for all m &isin;M implies that g = e.
</p>
<p>The reader may verify that the orbit Gm is the smallest invariant subset Stabilizer is a subgroup.
of M containing m, and that</p>
<p/>
</div>
<div class="page"><p/>
<p>714 23 Group Theory
</p>
<p>Box 23.3.4 The stabilizer of m is a subgroup of G, which is some-
times called the little group of G at m.
</p>
<p>Remark 23.3.1 Think of the action of G on M as passing from one point
of M to another. An element g of G &ldquo;transits&rdquo; a region of M to take m &isin;M
to gm &isin;M . The action is therefore, transitive, if G can transit (pass across)
all of M , i.e., G can connect any two points of M .
</p>
<p>If you think of Gm as those elements of G that are confined to (stuck, orHelp with understanding
the terminology of the
</p>
<p>definition above.
imprisoned at) m, then a &ldquo;free&rdquo; action of G does not allow any point of M
to imprison any subset of G.
</p>
<p>Any g &isin;G such that gm=m for all m &isin;M has no &ldquo;effect&rdquo; on M . There-
fore, this g is &ldquo;ineffective&rdquo; in its action on M . For G to act &ldquo;effectively&rdquo;, it
should not have any &ldquo;ineffective&rdquo; member.
</p>
<p>A transitive action is characterized by the fact that given any two points
m1,m2 &isin; M , one can find a g &isin; G such that m2 = gm1. In general, there
may be several gs connecting m1 to m2. If there are g,g&prime; &isin; G such that
m2 = gm1 = g&prime;m1, then
</p>
<p>gm1 = g&prime;m1 &rArr; m1 = g&minus;1g&prime;m1.
</p>
<p>If we want g to be unique for all m1 and m2, then the group action must be
free.
</p>
<p>By definition, the orbits of a group G in M are disjoint and their union
is the entire M . Another way of stating the same thing is to say that G
partitions M into orbits. It should be obvious that the action of G on any
orbit is transitive.
</p>
<p>From the remarks above, we conclude that
</p>
<p>Box 23.3.5 Two points of an orbit are connected by a unique element
of G iff G acts freely on the orbit.
</p>
<p>Example 23.3.6 Let M = R2 and G = SO(2), the planar rotation group.
The action is rotation of a point in the plane about the origin by an angle θ .
The orbits are circles centered at the origin. The action is effective but not
transitive. The stabilizer of every point in the plane is {e}, except the origin,
for which the whole group is the stabilizer. Since the stabilizer at the origin
is not {e}, the group action is not free.
</p>
<p>Let M = S1, the unit circle, and G = SO(2), the rotation group in two
dimensions. The action is displacement of a point on the circle. There is
only one orbit, the entire circle. The action is effective and transitive. The
stabilizer of every point on the circle is {e}; therefore, the action is free as
well.</p>
<p/>
</div>
<div class="page"><p/>
<p>23.4 The Symmetric Group Sn 715
</p>
<p>Let M = G, a group, and let a (proper) subgroup H act on G by left
multiplication. The orbits are right cosets Hg of the subgroup. The action is
effective but not transitive. The stabilizer of every point in the group is {e};
hence the action is free.
</p>
<p>Let M = R &cup; {&infin;}, the set of real numbers including &ldquo;the point at infin-
ity&rdquo;. Define an action of SL(2,R) on M by
</p>
<p>g &middot; x &equiv;
(
a b
</p>
<p>c d
</p>
<p>)
&middot; x = ax + c
</p>
<p>bx + d .
</p>
<p>This is a group action with a law of multiplication identical to the matrix
multiplication. The action is transitive, but neither effective nor free. Indeed,
the reader is urged to show that
</p>
<p>g &middot; x =
(
a b
</p>
<p>c d
</p>
<p>)
&middot; x = x &forall;x iff g =
</p>
<p>(
1 0
0 1
</p>
<p>)
or g =
</p>
<p>(&minus;1 0
0 &minus;1
</p>
<p>)
</p>
<p>making the group action not effective. Furthermore, for every x &isin;M
</p>
<p>Gx =
(
</p>
<p>a b
</p>
<p>bx2 + (d &minus; a)x d
</p>
<p>)
</p>
<p>making the group action not free.
Let M be a set and H the group of transformations of M . Suppose that realization of a group
</p>
<p>there is a homomorphism f :G&rarr;H from a group G into H . Then there is
a natural action of G on M given by g &middot;m&equiv; [f (g)](m). The homomorphism
f is sometimes called a realization of G.
</p>
<p>23.4 The Symmetric Group Sn
</p>
<p>Because of its primary importance as the prototypical finite group, and be-
cause of its significance in quantum statistics, the symmetric (or permuta-
tion) group is briefly discussed in this section. It is also used extensively in
the theory of representation of the general linear group and its subgroups.
</p>
<p>A generic permutation π of n numbers is shown as
</p>
<p>π =
(
</p>
<p>1 2 &middot; &middot; &middot; i &middot; &middot; &middot; n
π(1) π(2) &middot; &middot; &middot; π(i) &middot; &middot; &middot; π(n)
</p>
<p>)
. (23.1)
</p>
<p>Because the mapping is bijective, no two elements can have the same image,
and π(1),π(2), . . . , π(n) exhaust all the elements in the set {i}ni=1.
</p>
<p>We can display the product π2 ◦ π1 of two permutations using π2 ◦
π1(i)&equiv; π2(π1(i)). For instance, if
</p>
<p>π1 =
(
</p>
<p>1 2 3 4
3 4 1 2
</p>
<p>)
and π2 =
</p>
<p>(
1 2 3 4
2 4 3 1
</p>
<p>)
, (23.2)</p>
<p/>
</div>
<div class="page"><p/>
<p>716 23 Group Theory
</p>
<p>Table 23.1 Group multiplication table for S3
</p>
<p>e π2 π3 π4 π5 π6
</p>
<p>π2 e π5 π6 π3 π4
</p>
<p>π3 π6 e π5 π4 π2
</p>
<p>π4 π5 π6 e π2 π3
</p>
<p>π5 π4 π2 π3 π6 e
</p>
<p>π6 π3 π4 π2 e π5
</p>
<p>then the product π2 ◦π1 takes 1 to 3, etc., because π2 ◦π1(1)&equiv; π2(π1(1))=
π2(3)= 3, etc. We display π2 ◦ π1 as
</p>
<p>π2 ◦ π1 =
(
</p>
<p>1 2 3 4
3 1 2 4
</p>
<p>)
.
</p>
<p>Example 23.4.1 Let us construct the multiplication table for S3. Denote the
elements of S3 as follows:
</p>
<p>e =
(
</p>
<p>1 2 3
1 2 3
</p>
<p>)
, π2 =
</p>
<p>(
1 2 3
2 1 3
</p>
<p>)
, π3 =
</p>
<p>(
1 2 3
3 2 1
</p>
<p>)
,
</p>
<p>π4 =
(
</p>
<p>1 2 3
1 3 2
</p>
<p>)
, π5 =
</p>
<p>(
1 2 3
3 1 2
</p>
<p>)
, π6 =
</p>
<p>(
1 2 3
2 3 1
</p>
<p>)
.
</p>
<p>We give only one sample evaluation of the entries and leave the
straightforward&mdash;but instructive&mdash;calculation of the other entries to the
reader. Consider π4 ◦ π5, and note that π5(1) = 3 and π4(3) = 2; so
π4 ◦ π5(1)= 2. Similarly, π4 ◦ π5(2)= 1 and π4 ◦ π5(3)= 3. Thus
</p>
<p>π4 ◦ π5 =
(
</p>
<p>1 2 3
2 1 3
</p>
<p>)
= π2.
</p>
<p>The entire multiplication table is given in Table 23.1.
</p>
<p>Note that both the rows and columns of the group multiplication table
include all elements of the group, and no element is repeated in a row or a
column. This is because left-multiplication of elements of a group by a sin-
gle fixed element of the group simply permutes the group elements. Stated
differently, the left multiplication map Lg :G&rarr;G, given by Lg(x)= gx,
is bijective, as the reader may verify.
</p>
<p>Because we are dealing with finite numbers, repeated application of a
permutation to an integer in the set {i}ni=1 eventually produces the initial
integer. This leads to the following definition.
</p>
<p>Definition 23.4.2 Let π &isin; Sn, i &isin; {1,2, . . . , n}, and let r be the smallestcycles of symmetric
group positive integer such that π r(i) = i. Then the set of r distinct elements
</p>
<p>{πk(i)}r&minus;1k=0 is called a cycle of π of length r or an r-cycle generated by i.
</p>
<p>Start with 1 and apply π to it repeatedly until you obtain 1 again. The col-
lection of elements so obtained forms a cycle in which 1 is contained. Then
we select a second number that is not in this cycle and apply π to it repeat-</p>
<p/>
</div>
<div class="page"><p/>
<p>23.4 The Symmetric Group Sn 717
</p>
<p>edly until the original number is obtained again. Continuing in this way, we
produce a set of disjoint cycles that exhausts all elements of {1,2, . . . , n}.
</p>
<p>Proposition 23.4.3 Any permutation can be broken up into disjoint cycles.
</p>
<p>It is customary to write elements of each cycle in some specific order
within parentheses starting with the first element, say i, on the left, then
π(i) immediately to its right, followed by π2(i), and so on. For example,
the permutations π1 and π2 of Eq. (23.2) and their product have the cycle
structures π1 = (13)(24), π2 = (124)(3), and π2 ◦ π1 = (132)(4), respec-
tively.
</p>
<p>Example 23.4.4 Let π1,π2 &isin; S8 be given by
</p>
<p>π1 =
(
</p>
<p>1 2 3 4 5 6 7 8
3 5 7 1 2 8 4 6
</p>
<p>)
,
</p>
<p>π2 =
(
</p>
<p>1 2 3 4 5 6 7 8
2 5 6 8 1 7 4 3
</p>
<p>)
.
</p>
<p>The reader may verify that
</p>
<p>π2 ◦ π1 =
(
</p>
<p>1 2 3 4 5 6 7 8
6 1 4 2 5 3 8 7
</p>
<p>)
</p>
<p>and that
</p>
<p>π1 = (1374)(25)(68), π2 = (125)(36748),
π2 ◦ π1 = (16342)(5)(78).
</p>
<p>In general, permutations do not commute. The product in reverse order is
</p>
<p>π1 ◦ π2 =
(
</p>
<p>1 2 3 4 5 6 7 8
5 2 8 6 3 4 1 7
</p>
<p>)
= (15387)(2)(46),
</p>
<p>which differs from π2◦π1. However, note that it has the same cycle structure
as π2 ◦ π1, in that cycles of equal length appear in both. This is a general
property of all permutations.
</p>
<p>Definition 23.4.5 If π &isin; Sn has a cycle of length r and all other cycles of cyclic permutations
definedπ have only one element, then π is called a cyclic permutation of length r .
</p>
<p>It follows that π2 &isin; S4 as defined earlier is a cyclic permutation of length
3. Similarly,
</p>
<p>π =
(
</p>
<p>1 2 3 4 5 6
6 2 1 3 5 4
</p>
<p>)
</p>
<p>is a cyclic permutation of length 4 (verify this).
</p>
<p>Definition 23.4.6 A cyclic permutation of length 2 is called a transposi- transpositions defined
tion.</p>
<p/>
</div>
<div class="page"><p/>
<p>718 23 Group Theory
</p>
<p>A transposition (ij) simply switches i and j .
</p>
<p>Example 23.4.7 Products of (not necessarily disjoint) cycles may be asso-
ciated with a permutation whose action on i is obtained by starting with
the first cycle (at the extreme right), locating the first occurrence of i, and
keeping track of what each cycle does to it or its image under the pre-
ceding cycle. For example, let π1 &isin; S6 be given as a product of cycles by
π1 = (143)(24)(456). To find the permutation, we start with 1 and follow
the action of the cycles on it, starting from the right. The first and second
cycles leave 1 alone, and the last cycle takes it to 4. Thus, π1(1)= 4. For 2
we note that the first cycle leaves it alone, the second cycle takes it to 4, and
the last cycle takes 4 to 3. Thus, π1(2)= 3. Similarly, π1(3)= 1, π1(4)= 5,
π1(5)= 6, and π1(6)= 2. Therefore,
</p>
<p>π1 =
(
</p>
<p>1 2 3 4 5 6
4 3 1 5 6 2
</p>
<p>)
.
</p>
<p>We note that π1 is a cyclic permutation of length 6.
It is left to the reader to show that the permutation π2 &isin; S5 given by the
</p>
<p>product π2 = (13)(15)(12)(14) is cyclic: π2 = (14253).
</p>
<p>The square of any transposition is the identity. Therefore, we can include
it in any product of permutations without changing anything.
</p>
<p>Proposition 23.4.8 An r-cycle (i1, i2, . . . , ir ) can be decomposed into the
product of r &minus; 1 transpositions:
</p>
<p>(i1, i2, . . . , ir)= (i1ir)(i1ir&minus;1) &middot; &middot; &middot; (i1i3)(i1i2).
</p>
<p>Proof The proof involves keeping track of what happens to each symbol
when acted upon by the RHS and the LHS and showing that the two give
the same result. This is left as an exercise for the reader. �
</p>
<p>Although the decomposition of Proposition 23.4.8 is not unique, it can be
shown that the parity of the decomposition (whether the number of factors
is even or odd) is unique. For instance, it is easy to verify that
</p>
<p>(1234)= (14)(13)(12)= (14) (34)(34)︸ ︷︷ ︸
1
</p>
<p>1︷ ︸︸ ︷
(23) (12)(12)︸ ︷︷ ︸
</p>
<p>1
</p>
<p>(23)(13)(12).
</p>
<p>That is, (1234) is written as a product of 3 or 9 transpositions, both of which
are odd.
</p>
<p>parity of a permutation
</p>
<p>defined
</p>
<p>We have already seen that any permutation can be written as a product of
cycles. In addition, Proposition 23.4.8 says that these cycles can be further
broken down into products of transpositions. This implies the following (see
[Rotm 84, p. 38]):
</p>
<p>Proposition 23.4.9 Any permutation can be decomposed as a product ofparity of a permutation
is unique transpositions. The parity of the decomposition is unique.</p>
<p/>
</div>
<div class="page"><p/>
<p>23.4 The Symmetric Group Sn 719
</p>
<p>Definition 23.4.10 A permutation is even (odd) if it can be expressed as a even and odd
permutationsproduct of an even (odd) number of transpositions.
</p>
<p>The parity of a permutation can be determined from its cycle structure
and Proposition 23.4.8.
</p>
<p>The reader may verify that the mapping from Sn to the multiplicative
group of {+1,&minus;1} that assigns +1 to even permutations and &minus;1 to odd per-
mutations is a group homomorphism. It follows from the first isomorphism
theorem (Theorem 23.2.16) that
</p>
<p>Box 23.4.11 The set of even permutations, denoted by An, forms a
normal subgroup of Sn.
</p>
<p>This homomorphism is usually denoted by ǫ. We therefore define
</p>
<p>ǫ(π)&equiv; ǫπ =
{
+1 if π is even,
&minus;1 if π is odd.
</p>
<p>(23.3)
</p>
<p>Sometimes δ(π) or δπ as well as sgn(π) is also used. The symbol, ǫi1i2...in
used in the definition of determinants, is closely related to ǫ(π). In fact,
</p>
<p>ǫπ(1)π(2)...π(n) &equiv; ǫπ .
</p>
<p>Suppose π,σ &isin; Sn, and note that8 σ(i) σ
&minus;1
</p>
<p>�&minus;&rarr; i π�&minus;&rarr; π(i) σ�&minus;&rarr; σ ◦π(i), i.e.,
the composite σ ◦ π ◦ σ&minus;1 of the three permutations takes σ(i) to σ ◦ π(i).
This composite can be thought of as the permutation obtained by applying
σ to the two rows of π =
</p>
<p>( 1 2 &middot;&middot;&middot; n
π(1) π(2) &middot;&middot;&middot; π(n)
</p>
<p>)
:
</p>
<p>σ ◦ π ◦ σ&minus;1 =
(
</p>
<p>σ(1) σ (2) &middot; &middot; &middot; σ(n)
σ ◦ π(1) σ ◦ π(2) &middot; &middot; &middot; σ ◦ π(n)
</p>
<p>)
.
</p>
<p>In particular, the cycles of σ ◦ π ◦ σ&minus;1 are obtained by applying σ to the
symbols in the cycles of π . Since σ is bijective, the cycles so obtained will
remain disjoint. It follows that σ ◦ π ◦ σ&minus;1, a conjugate of π , has the same
cycle structure as π itself. In fact, we have the following:
</p>
<p>Theorem 23.4.12 Two permutations are conjugate if and only if they have
the same cycle structure.
</p>
<p>To find the distinct conjugacy classes of Sn, one has to construct dis-
tinct cycle structures of Sn. This in turn is equivalent to partitioning the
numbers from 1 to n into sets of various lengths. Let νk be the number of
k-cycles in a permutation. The cycle structure of this permutation is denoted
</p>
<p>8Recall from Chap. 1 that x
f�&minus;&rarr; y means y = f (x).</p>
<p/>
</div>
<div class="page"><p/>
<p>720 23 Group Theory
</p>
<p>by (1ν1,2ν2, . . . , nνn). Since the total number of symbols is n, we must have&sum;n
k=1 kνk = n. Defining λj &equiv;
</p>
<p>&sum;n
k=j νk , we have
</p>
<p>λ1 + λ2 + &middot; &middot; &middot; + λn = n, λ1 &ge; λ2 &ge; &middot; &middot; &middot; &ge; λn &ge; 0. (23.4)
</p>
<p>The splitting of n into nonnegative integers (λ1, λ2, . . . , λn) as in Eq. (23.4)
is called a partition of n. There is a 1&ndash;1 correspondence between parti-partition of n
tions of n and the cycle structure of Sn. We saw how νk&rsquo;s gave rise to λ&rsquo;s.
Conversely, given a partition of n, we can construct a cycle structure by
νk = λk &minus; λk+1. For example, the partition (32000) of S5 corresponds toConjugacy classes of Sn
</p>
<p>are related to the
</p>
<p>partition of n.
</p>
<p>ν1 = 3&minus; 2 = 1, ν2 = 2&minus; 0 = 2, i.e., one 1-cycle and two 2-cycles. One usu-
ally omits the zeros and writes (32) instead of (32000). When some of the
λ&rsquo;s are repeated, the number of occurrences is indicated by a power of the
corresponding λ; the partition is then written as (μn11 ,μ
</p>
<p>n2
2 , . . . ,μ
</p>
<p>nr
r ), where
</p>
<p>it is understood that λ1 through λn1 have the common value μ1, etc. For ex-
ample, (321) corresponds to a partition of 7 with λ1 = 3, λ2 = 3, and λ3 = 1.
The corresponding cycle structure is ν1 = 0, ν2 = 2, and ν3 = 1, i.e., two 2-
cycles and one 3-cycle. The partitions of length 0 are usually ignored. Since&sum;
</p>
<p>λi = n, no confusion will arise as to which symmetric group the partition
belongs to. Thus (32000) and (332000) are written as (32) and (322), and it
is clear that (32) belongs to S5 and (322) to S8.
</p>
<p>Example 23.4.13 Let us find the different cycle structures of S4. This cor-
responds to different partitions of 4. We can take λ1 = 4 and the rest of the
λ&rsquo;s zero. This gives the partition (4). Next, we let λ1 = 3; then λ2 must be 1,
giving the partition (31). With λ1 = 2, λ2 can be either 2 or 1. In the latter
case, λ3 must be 1 as well, and we obtain two partitions, (22) and (212).
Finally, if λ1 = 1, all other nonzero λ&rsquo;s must be 1 as well (remember that
λk &ge; λk+1). Therefore, the last partition is of the form (14). We see that there
are 5 different partitions of 4. It follows that there are 5 different conjugacy
classes in S4.
</p>
<p>23.5 Problems
</p>
<p>23.1 Let S be a subset of a group G. Show that S is a subgroup if and only
if ab&minus;1 &isin; S whenever a, b &isin; S.
</p>
<p>23.2 Show that the intersection of two subgroups is a subgroup.
</p>
<p>23.3 Let X be a subset of a group G. A word on X is an element w of Gaword on a subset of a
group of the form
</p>
<p>w = xe11 x
e2
2 &middot; &middot; &middot;xenn ,
</p>
<p>where xi &isin;X and ei =&plusmn;1. Show that the set of all words on X is a subgroup
of G.
</p>
<p>23.4 Let [a, b] denote the commutator of a and b. Show that</p>
<p/>
</div>
<div class="page"><p/>
<p>23.5 Problems 721
</p>
<p>(a) [a, b]&minus;1 = [b, a],
(b) [a, a] = e for all a &isin;G, and
(c) ab = [a, b]ba. It is interesting to compare these relations with the fa-
</p>
<p>miliar commutators of operators.
</p>
<p>23.5 Show that if S is a subgroup, then S2 &equiv; SS = S, and tS = S if and
only if t &isin; S. More generally, T S = S if and only if T &sub; S.
</p>
<p>23.6 Show that if S is a subgroup, then Sa = Sb if and only if ba&minus;1 &isin; S
and ab&minus;1 &isin; S (aS = bS if and only if a&minus;1b &isin; S and b&minus;1a &isin; S).
</p>
<p>23.7 Let S be a subgroup of G. Show that a ⊲ b defined by ab&minus;1 &isin; S is an
equivalence relation.
</p>
<p>23.8 Show that CG(x) is a subgroup of G. Let H be a subgroup of G and
suppose x &isin;H . Show that CH (x) is a subgroup of CG(x).
</p>
<p>23.9 (a) Show that the only element a in a group with the property a2 = a
is the identity. (b) Now use eG ⋆ eG = eG to show that any homomorphism
maps identity to identity. (c) Show that if f :G&rarr;H is a homomorphism,
then f (g&minus;1)= [f (g)]&minus;1.
</p>
<p>23.10 Establish a bijection between the set of right cosets and the set of left
cosets of a subgroup. Hint: Define a map that takes St to t&minus;1S.
</p>
<p>23.11 Let G be a finite group and S one of its subgroups. Convince yourself Lagrange&rsquo;s theorem
that the union of all right cosets of S is G. Now use the fact that distinct right
cosets are disjoint and that they have the same cardinality to prove that the
order of S divides the order of G. In fact, |G| = |S||G/S|, where |G/S| is the
number of cosets of S (also called the index of S in G). This is Lagrange&rsquo;s
theorem.
</p>
<p>23.12 Let f : G &rarr; H be a homomorphism. Show that φ : G/kerf &rarr;
f (G) given by φ(g[kerf ])&equiv; φ([kerf ]g)= f (g) is an isomorphism.
</p>
<p>23.13 Let G&prime; denote the commutator subgroup of a group G. Show that G&prime;
</p>
<p>is a normal subgroup of G and that G/G&prime; is abelian.
</p>
<p>23.14 Let M =R&cup; {&infin;}, and define an action of SL(2,R) on M by
(
a b
</p>
<p>c d
</p>
<p>)
&middot; x = ax + c
</p>
<p>bx + d .
</p>
<p>(a) Show that this is indeed a group action with a law of multiplication
identical to the matrix multiplication.
</p>
<p>(b) Show that the action is transitive.
(c) Show that beside identity, there is precisely one other element g of the
</p>
<p>group such that g &middot; x = x for all x &isin;M .</p>
<p/>
</div>
<div class="page"><p/>
<p>722 23 Group Theory
</p>
<p>(d) Show that for every x &isin;M ,
</p>
<p>Gx =
(
</p>
<p>a b
</p>
<p>bx2 + (d &minus; a)x d
</p>
<p>)
</p>
<p>23.15 Show that two conjugacy classes are either disjoint or identical.
</p>
<p>23.16 Show that if all conjugacy classes of a group have only one element,
the group must be abelian.
</p>
<p>23.17 Consider a map from the conjugacy class of G containing x &isin; G
to the set of (left) cosets G/CG(x) given by φ(axa&minus;1) = aCG(x). Show
that φ is a bijection. In particular, show that |CG(x)| = |G|/|KGx | where
KGx is the class in G containing x and |KGx | its order (see Problem 23.11).
Use this result and Problems 23.8 and 23.11 to show that |H |/|KHx | divides
|G|/|KGx |.
</p>
<p>23.18 Show that RRê(θ)R
&minus;1 corresponds to a rotation of angle θ . Hint:
</p>
<p>Consider the effect of rotation on the vectors in the plane perpendicular to
ê, and note that the rotated plane is perpendicular to ê&prime; &equiv;Rê.
</p>
<p>23.19 Let G act on M and let m0 &isin; M . Show that Gm0 is the smallest
invariant subset of M containing m0.
</p>
<p>23.20 Suppose G is the direct product of H1 and H2 and g = h1h2. Show
that the factors h1 and h2 are unique and that H1 and H2 are normal.
</p>
<p>23.21 Show that (g,h), (g&prime;, h&prime;) &isin; G&times;H are conjugate if and only if g is
conjugate to g&prime; and h is conjugate to h&prime;. Therefore, conjugacy classes of the
direct product are obtained by pairing one conjugacy class from each factor.
</p>
<p>23.22 Find the products π1 ◦ π2 and π2 ◦ π1 of the two permutations
</p>
<p>π1 =
(
</p>
<p>1 2 3 4 5 6
3 4 6 5 1 2
</p>
<p>)
and π2 =
</p>
<p>(
1 2 3 4 5 6
2 1 3 6 5 4
</p>
<p>)
.
</p>
<p>23.23 Find the inverses of the permutations
</p>
<p>π1 =
(
</p>
<p>1 2 3 4 5 6 7 8
3 5 7 1 2 8 4 6
</p>
<p>)
,
</p>
<p>π2 =
(
</p>
<p>1 2 3 4 5 6 7 8
2 5 6 8 1 7 4 3
</p>
<p>)
</p>
<p>and show directly that (π1 ◦ π2)&minus;1 = π&minus;12 ◦ π&minus;11 .
</p>
<p>23.24 Find the inverse of each of the following permutations: π1 =( 1 2 3 4
3 2 4 1
</p>
<p>)
, π2 =
</p>
<p>( 1 2 3 4 5
1 4 2 5 3
</p>
<p>)
, π3 =
</p>
<p>( 1 2 3 4 5 6
6 5 4 3 2 1
</p>
<p>)
, and π4 =
</p>
<p>( 1 2 3 4 5
3 4 5 1 2
</p>
<p>)
.</p>
<p/>
</div>
<div class="page"><p/>
<p>23.5 Problems 723
</p>
<p>23.25 Express each of the following products in terms of disjoint cycles.
Assume that all permutations are in S7.
</p>
<p>(a) (123)(347)(456)(145). (b) (34)(562)(273).
</p>
<p>(c) (1345)(134)(13).
</p>
<p>23.26 Express the following permutations as products of disjoint cycles,
and determine which are cyclic.
</p>
<p>(a)
</p>
<p>(
1 2 3 4 5 6
1 3 4 5 6 2
</p>
<p>)
, (b)
</p>
<p>(
1 2 3 4 5 6
2 1 4 5 6 3
</p>
<p>)
,
</p>
<p>(c)
</p>
<p>(
1 2 3 4 5
1 3 5 4 2
</p>
<p>)
.
</p>
<p>23.27 Express the permutation π =
( 1 2 3 4 5 6 7 8
</p>
<p>2 4 1 3 6 8 7 5
</p>
<p>)
as a product of transpo-
</p>
<p>sitions. Is the permutation even or odd?
</p>
<p>23.28 Express the following permutations as products of transpositions, and
determine whether they are even or odd.
</p>
<p>(a)
</p>
<p>(
1 2 3 4 5
3 4 2 1 5
</p>
<p>)
, (b)
</p>
<p>(
1 2 3 4 5 6 7 8
4 1 7 8 3 6 5 2
</p>
<p>)
,
</p>
<p>(c)
</p>
<p>(
1 2 3 4 5 6
6 4 5 3 2 1
</p>
<p>)
, (d)
</p>
<p>(
1 2 3 4 5 6 7
6 7 2 4 1 5 3
</p>
<p>)
.
</p>
<p>23.29 Show that the product of two even or two odd permutations is always
even, and the product of an even and an odd permutation is always odd.
</p>
<p>23.30 Show that π and π&minus;1 have the same parity (both even or both odd).
</p>
<p>23.31 Find the number of distinct conjugacy classes of S5 and S6.</p>
<p/>
</div>
<div class="page"><p/>
<p>24Representation of Groups
</p>
<p>Group action is extremely important in quantum mechanics. Suppose the
Hamiltonian of a quantum system is invariant under a symmetry transforma-
tion of its independent parameters such as position, momentum, and time.
This invariance will show up as certain properties of the solutions of the
Schr&ouml;dinger equation.
</p>
<p>Moreover, the very act of labeling quantum-mechanical states often in-
volves groups and their actions. For example, labeling atomic states by
eigenvalues of angular momentum assumes invariance of the Hamiltonian
under the action of the rotation group (see Chap. 29) on the Hilbert space of
the quantum-mechanical system under consideration.
</p>
<p>24.1 Definitions and Examples
</p>
<p>In the language of group theory, we have the following situation. Put all
the parameters x1, . . . , xp of the Hamiltonian H together to form a space,
say Rp , and write H= H(x1, . . . , xp)&equiv; H(x). A group of symmetry of H
is a group G whose action on Rp leaves H unchanged,1 i.e., H(x &middot;g)= H(x).
For example, a one-dimensional harmonic oscillator, with H = &minus; �22m d
</p>
<p>2
</p>
<p>dx2
+
</p>
<p>1
2mω
</p>
<p>2x2, has, among other things, parity P (defined by Px = &minus;x) as a
symmetry. Thus, the group G= {e,P } is a group of symmetry of H.
</p>
<p>The Hamiltonian H of a quantum-mechanical system is an operator in a
Hilbert space, such as L2(R3), the space of square-integrable functions. The
important question is: What is the proper way of transporting the action of
G from Rp to L2(R3)? This is a relevant question because the solutions of
the Schr&ouml;dinger equation are, in general, functions of the parameters of the
Hamiltonian, and as such will be affected by the symmetry operation on the
Hamiltonian. The answer is provided in the following definition.2
</p>
<p>1It will become clear shortly that the appropriate direction for the action is from the right.
2We have already encountered the notion of representation in the context of algebras.
Groups are much more widely used in physics than algebras, and group representations
have a wider application in physics than their algebraic counterparts. Since some readers
may have skipped the section on the representation of algebras, we&rsquo;ll reintroduce the ideas
here at the risk of being redundant.
</p>
<p>S. Hassani, Mathematical Physics, DOI 10.1007/978-3-319-01195-0_24,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>725</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-01195-0_24">http://dx.doi.org/10.1007/978-3-319-01195-0_24</a></div>
</div>
<div class="page"><p/>
<p>726 24 Representation of Groups
</p>
<p>Definition 24.1.1 Let G be a group and H a Hilbert space. A representa-
representation; carrier
</p>
<p>space and dimension of
</p>
<p>a representation; faithful
</p>
<p>and identity
</p>
<p>representation
</p>
<p>tion of G on H is a homomorphism T : G&rarr; GL(H). The representation
is faithful if the homomorphism is 1&ndash;1. We often denote T (g) by Tg . H is
called the carrier space of T . The trivial homomorphism T : G&rarr; {1} is
also called the identity representation. The dimension of H is called the
dimension of the representation T .
</p>
<p>We do not want to distinguish between representations that differ only by
isomorphic vector spaces, because otherwise we can generate an infinite set
of representations that are trivially related to one another. A vector space
isomorphism f : H &rarr; H&prime; induces a group isomorphism φ : GL(H) &rarr;
GL(H&prime;) defined by
</p>
<p>φ(T)= f ◦ T ◦ f&minus;1 for T &isin;GL(H).
</p>
<p>This motivates the following definition.
</p>
<p>Definition 24.1.2 Two representations T : G &rarr; GL(H) and T &prime; : G &rarr;
equivalent
</p>
<p>representations
GL(H&prime;) are called equivalent if there exists an isomorphism f :H &rarr;H&prime;
such that T&prime;g = f ◦ Tg ◦ f&minus;1 for all g &isin;G.
</p>
<p>Box 24.1.3 Any representation T :G&rarr;GL(H) defines an action of
the group G on the Hilbert space H by Φ(g, |a〉)&equiv; Tg|a〉.
</p>
<p>As we saw in Chaps. 4 and 5, the transformation of an operator A under
Tg would have to be defined by TgA(Tg)&minus;1. For a Hamiltonian with a group
of symmetry G, this leads to the identity
</p>
<p>Tg
[
H(x)
</p>
<p>]
(Tg)
</p>
<p>&minus;1 = H(x &middot; g).
</p>
<p>Similarly, the action of the group on a vector (function) in L2(R3) is defined
by
</p>
<p>(Tgψ)(x)&equiv;ψ(x &middot; g), (24.1)
where the parentheses around Tgψ designate it as a new function. One can
show that if G acts on the independent variables of a function on the right
as in Eq. (24.1), then the vector space of such functions is the carrier space
of a representation of G. In fact,
</p>
<p>(Tg1g2ψ)(x)&equiv;ψ
(
x &middot; (g1g2)
</p>
<p>)
=ψ
</p>
<p>(
(x &middot;g1) &middot;g2
</p>
<p>)
= (Tg2ψ)(x &middot;g1)&equiv; ϕ(x &middot;g1),
</p>
<p>where we have defined the new function ϕ by the last equality. Now note
that
</p>
<p>ϕ(x &middot; g1)= (Tg1ϕ)(x)=
(
Tg1(Tg2ψ)
</p>
<p>)
(x)= (Tg1Tg2ψ)(x).
</p>
<p>It follows from the last two equations that
</p>
<p>Tg1g2ψ = Tg1Tg2ψ.</p>
<p/>
</div>
<div class="page"><p/>
<p>24.1 Definitions and Examples 727
</p>
<p>Since this holds for arbitrary ψ , we must have Tg1g2 = Tg1Tg2 , i.e., that T
is a representation. When the action of a group is &ldquo;naturally&rdquo; from the left,
such as the action of a matrix on a column vector, we replace x &middot; g with
g&minus;1 &middot; x. The reader can check that T : G &rarr; GL(H), given by Tgψ(x) =
ψ(g&minus;1 &middot; x), is indeed a representation.
</p>
<p>Example 24.1.4 Let the Hamiltonian of the time-independent Schr&ouml;dinger
equation H|ψ〉 = E|ψ〉 be invariant under the action of a group G. This
means that
</p>
<p>TgHT
&minus;1
g = H &rArr; [H,Tg] = 0,
</p>
<p>i.e., that H and Tg are simultaneously diagonalizable (Theorem 6.4.18).
It follows that we can choose the energy eigenstates to be eigenstates of
</p>
<p>Energy eigenstates can
</p>
<p>be labeled by
</p>
<p>eigenvalues of the
</p>
<p>symmetry operators as
</p>
<p>well.
</p>
<p>Tg as well, and we can label the states not only by the energy &ldquo;quantum
numbers&rdquo;&mdash;eigenvalues of H&mdash;but also by the eigenvalues of Tg . For exam-
ple, if the Hamiltonian is invariant under the action of parity P , then we can
choose the states to be even, corresponding to parity eigenvalue of +1, or
odd, corresponding to parity eigenvalue of &minus;1. Similarly, if G is the rota-
tion group, then the states can be labeled by the eigenvalues of the rotation
operators, which are, as we shall see, equivalent to the angular momentum
operators discussed in Chap. 13.
</p>
<p>In crystallography and solid-state physics, the Hamiltonian of an (infi-
nite) lattice is invariant under translation by an integer multiple of each so-
called primitive lattice translation, the three noncoplanar vectors that define
a primitive cell of the crystal. The preceding argument shows that the energy
eigenstates can be taken to be the eigenstates of the translation operator as
well.
</p>
<p>It is common to choose a basis and represent all Tg&rsquo;s in terms of matrices. matrix representations
Then one gets a matrix representation of the group G.
</p>
<p>Example 24.1.5 Consider the action of the 2D rotation group SO(2) (rota-
tion about the z-axis) on R3:
</p>
<p>x&prime; = x cos θ &minus; y sin θ,
r&prime; =Rz(θ)r &rArr; y&prime; = x sin θ + y cos θ,
</p>
<p>z&prime; = z.
</p>
<p>For a Hilbert space, also choose R3. Define the homomorphism T : G &rarr;
GL(H) to be the identity map, so that T (Rz(θ))&equiv; Tθ =Rz(θ). The operator
Tθ transforms the standard basis vectors of H as
</p>
<p>Tθ ê1 = Tθ (1,0,0)= (cos θ, sin θ,0)= cos θ ê1 + sin θ ê2 + 0ê3,
Tθ ê2 = Tθ (0,1,0)= (&minus;sin θ, cos θ,0)=&minus;sin θ ê1 + cos θ ê2 + 0ê3,
Tθ ê3 = Tθ (0,0,1)= (0,0,1)= 0ê1 + 0ê2 + ê3.</p>
<p/>
</div>
<div class="page"><p/>
<p>728 24 Representation of Groups
</p>
<p>It follows that the matrix representation of SO(2) in the standard basis of H
is
</p>
<p>Tθ =
</p>
<p>⎛
⎝
</p>
<p>cos θ &minus;sin θ 0
sin θ cos θ 0
</p>
<p>0 0 1
</p>
<p>⎞
⎠ .
</p>
<p>Note that SO(2) is an infinite group; its cardinality is determined by the
&ldquo;number&rdquo; of θ &rsquo;s.
</p>
<p>Example 24.1.6 Let S3 act on R3 on the right by shuffling components:
</p>
<p>(x1, x2, x3) &middot; π = (xπ(1), xπ(2), xπ(3)), π &isin; S3.
</p>
<p>For the carrier space, choose R3 as well. Let T : S3 &rarr; GL(R3) be given as
follows: T (π) is the matrix that takes the column vector x =
</p>
<p>( x1
x2
x3
</p>
<p>)
to
</p>
<p>( xπ(1)
xπ(2)
xπ(3)
</p>
<p>)
.
</p>
<p>As a specific illustration, consider π =
( 1 2 3
</p>
<p>3 1 2
</p>
<p>)
and write Tπ for T (π). Then
</p>
<p>Tπ (ê1)= Tπ (1,0,0)= (1,0,0) &middot; π = (0,1,0)= ê2,
Tπ (ê2)= Tπ (0,1,0)= (0,1,0) &middot; π = (0,0,1)= ê3,
Tπ (ê3)= Tπ (0,0,1)= (0,0,1) &middot; π = (1,0,0)= ê1,
</p>
<p>which give rise to the matrix
</p>
<p>Tπ =
</p>
<p>⎛
⎝
</p>
<p>0 0 1
1 0 0
0 1 0
</p>
<p>⎞
⎠ .
</p>
<p>The reader may construct the other five matrices of this representation and
verify directly that it is indeed a (faithful) representation: Products and in-
verses of permutations are mapped onto products and inverses of the corre-
sponding matrices.
</p>
<p>24.2 Irreducible Representations
</p>
<p>The utility of a representation lies in our comfort with the structure of vec-
tor spaces. The climax of such comfort is the spectral decomposition theo-
rems of (normal) operators on vector spaces of finite (Chap. 6) and infinite
(Chap. 17) dimensions. The operators Tg , relevant to our present discussion,
are, in general, neither normal nor simultaneously commuting. Therefore,
the complete diagonalizability of all Tg&rsquo;s is out of the question (unless the
group happens to be abelian).
</p>
<p>The best thing next to complete diagonalization is to see whether there
are common invariant subspaces of the vector space H carrying the repre-
sentation. We already know how to construct (minimal) &ldquo;invariant&rdquo; subsets
of H: these are precisely the orbits of the action of the group G on H. The
linearity of Tg&rsquo;s guarantees that the span of each orbit is actually an invari-
ant subspace, and that such subspaces are the smallest invariant subspaces</p>
<p/>
</div>
<div class="page"><p/>
<p>24.2 Irreducible Representations 729
</p>
<p>containing a given vector. Our aim is to find those minimal invariant sub-
spaces whose orthogonal complements are also invariant. We encountered
the same situation in Chap. 6 for a single operator.
</p>
<p>Definition 24.2.1 A representation T :G&rarr; GL(H) is called reducible if
reducible and irreducible
</p>
<p>representations
there exist subspaces U and W of H such that H=U&oplus;W and both U and
W are invariant under all Tg&rsquo;s. If no such subspaces exist, H is said to be
irreducible.
</p>
<p>In most cases of physical interest, where H is a Hilbert space, W=U&perp;.
Then, in the language of Definition 6.1.4, a representation is reducible if a
proper subspace of H reduces all Tg&rsquo;s.
</p>
<p>Example 24.2.2 Let S3 act on R3 as in Example 24.1.6. For the carrier
space H, choose the space of functions on R3, and for T , the homomor-
phism T : G &rarr; GL(H), given by Tgψ(x) = ψ(x &middot; g), for ψ &isin; H. Any ψ
that is symmetric in x, y, z, such as xyz, x + y + z, or x2 + y2 + z2,
defines a one-dimensional invariant subspace of H. To obtain another in-
variant subspace, consider ψ1(x, y, z) &equiv; xy and let {πi}6i=1 be as given
in Example 23.4.1. Then, denoting Tπi by Ti , the reader may check
that
</p>
<p>[T1ψ1](x, y, z)=ψ1
(
(x, y, z) &middot; π1
</p>
<p>)
=ψ1(x, y, z)= xy =ψ1(x, y, z),
</p>
<p>[T2ψ1](x, y, z)=ψ1
(
(x, y, z) &middot; π2
</p>
<p>)
=ψ1(y, x, z)= yx =ψ1(x, y, z),
</p>
<p>[T3ψ1](x, y, z)=ψ1
(
(x, y, z) &middot; π3
</p>
<p>)
=ψ1(z, y, x)= zy &equiv;ψ2(x, y, z),
</p>
<p>[T4ψ1](x, y, z)=ψ1
(
(x, y, z) &middot; π4
</p>
<p>)
=ψ1(x, z, y)= xz&equiv;ψ3(x, y, z),
</p>
<p>[T5ψ1](x, y, z)=ψ1
(
(x, y, z) &middot; π5
</p>
<p>)
=ψ1(z, x, y)= zx =ψ3(x, y, z),
</p>
<p>[T6ψ1](x, y, z)=ψ1
(
(x, y, z) &middot; π6
</p>
<p>)
=ψ1(y, z, x)= yz=ψ2(x, y, z).
</p>
<p>This is clearly a three-dimensional invariant subspace of H with ψ1, ψ2,
and ψ3 as a convenient basis, in which the first three permutations are rep-
resented by
</p>
<p>T1 =
</p>
<p>⎛
⎝
</p>
<p>1 0 0
0 1 0
0 0 1
</p>
<p>⎞
⎠ , T2 =
</p>
<p>⎛
⎝
</p>
<p>1 0 0
0 0 1
0 1 0
</p>
<p>⎞
⎠ , T3 =
</p>
<p>⎛
⎝
</p>
<p>0 1 0
1 0 0
0 0 1
</p>
<p>⎞
⎠ .
</p>
<p>It is instructive for the reader to verify these relations and to find the three
remaining matrices.
</p>
<p>Example 24.2.3 Let S3 act on R3 as in Example 24.1.6. For the carrier
space of representation, choose the subspace V of the H of Example 24.2.2
spanned by the six functions x, y, z, xy, xz, and yz. For T , choose the
same homomorphism as in Example 24.2.2 restricted to V. It is clear that
the subspaces U and W spanned, respectively, by the first three and the last
three functions are invariant under S3, and that V= U&oplus;W. It follows that</p>
<p/>
</div>
<div class="page"><p/>
<p>730 24 Representation of Groups
</p>
<p>the representation is reducible. The matrix form of this representation is
found to be of the general form
</p>
<p>(
A 0
</p>
<p>0 B
</p>
<p>)
, where B is one of the 6 matrices of
</p>
<p>Example 24.2.2. The matrix A, corresponding to the three functions x, y,
and z, can be found similarly.
</p>
<p>Let H be a carrier space, finite- or infinite-dimensional. For any vector
|a〉, the reader may check that the span of {Tg|a〉}g&isin;G is an invariant sub-
space of H. If G is finite, this subspace is clearly finite-dimensional. The
irreducible subspace containing |a〉, a subspace of the span of {Tg|a〉}g&isin;G,
will also be finite-dimensional. Because of the arbitrariness of |a〉, it follows
that every vector of H lies in an irreducible subspace, and that
</p>
<p>Box 24.2.4 All irreducible representations of a finite group are finite-
dimensional.
</p>
<p>Due to the importance and convenience of unitary operators (for exam-
ple, the fact that they leave the inner product invariant), it is desirable to be
able to construct a unitary representation&mdash;or a representation that is equiv-
alent to one&mdash;of groups. The following theorem ensures that this desire can
be realized for finite groups.
</p>
<p>All representations are
</p>
<p>equivalent to unitary
</p>
<p>representations. Theorem 24.2.5 Every representation of a finite group G is equiva-
lent to some unitary representation.
</p>
<p>Proof We present the proof because of its simplicity and elegance. Let
T be a representation of G. Consider the positive hermitian operator T &equiv;&sum;
</p>
<p>x&isin;G T
&dagger;
xTx and note that
</p>
<p>T&dagger;gTTg =
&sum;
</p>
<p>x&isin;G
</p>
<p>[
T (g)
</p>
<p>]&dagger;[
T (x)
</p>
<p>]&dagger;
T (x)T (g)
</p>
<p>=
&sum;
</p>
<p>x&isin;G
</p>
<p>[
T (xg)
</p>
<p>]&dagger;
T (xg)=
</p>
<p>&sum;
</p>
<p>y&isin;G
</p>
<p>[
T (y)
</p>
<p>]&dagger;
T (y)= T, (24.2)
</p>
<p>where we have used the fact that the sum over x and y &equiv; xg sweep through
the entire group. Now let S=
</p>
<p>&radic;
T, and multiply both sides of Eq. (24.2)&mdash;
</p>
<p>with S2 replacing T&mdash;by S&minus;1 on the left and by T&minus;1g S
&minus;1 on the right to
</p>
<p>obtain
</p>
<p>S&minus;1T&dagger;gS= ST&minus;1g S&minus;1 &rArr;
(
STgS
</p>
<p>&minus;1)&dagger; =
(
STgS
</p>
<p>&minus;1)&minus;1 &forall;g &isin;G.
</p>
<p>This shows that the representation T &prime; defined by T&prime;g &equiv; STgS&minus;1 for all g &isin;G
is unitary. �
</p>
<p>There is another convenience afforded by unitary representations:</p>
<p/>
</div>
<div class="page"><p/>
<p>24.2 Irreducible Representations 731
</p>
<p>Theorem 24.2.6 Let T : G&rarr; GL(H) be a unitary representation and W
an invariant subspace of H. Then, W&perp; is also invariant.
</p>
<p>Proof Suppose |a〉 &isin;W&perp;. We need to show that Tg|a〉 &isin;W&perp; for all g &isin;G.
To this end, let |b〉 &isin;W. Then
</p>
<p>〈b|Tg|a〉 =
(
〈a|T&dagger;g|b〉
</p>
<p>)&lowast; =
(
〈a|T&minus;1g |b〉
</p>
<p>)&lowast; =
(
〈a|Tg&minus;1 |b〉
</p>
<p>)&lowast; = 0,
</p>
<p>because Tg&minus;1 |b〉 &isin;W. It follows from this equality that Tg|a〉 &isin;W&perp; for all
g &isin;G. �
</p>
<p>The carrier space H of a unitary representation is either irreducible or has
an invariant subspace W, in which case we have H =W&oplus;W&perp;, where, by
Theorem 24.2.6, W&perp; is also invariant. If W and W&perp; are not irreducible, then
they too can be written as direct sums of invariant subspaces. Continuing
this process, we can decompose H into irreducible invariant subspaces W(k)
</p>
<p>such that
</p>
<p>H=W(1) &oplus;W(2) &oplus;W(3) &oplus; &middot; &middot; &middot; .
If the carrier space is finite-dimensional, which we assume from now on and
for which we use the notation V, then the above direct sum is finite and we
write
</p>
<p>V=W(1) &oplus;W(2) &oplus; &middot; &middot; &middot; &oplus;W(p) &equiv;
p&oplus;
</p>
<p>k=1
W
</p>
<p>(k). (24.3)
</p>
<p>One can think of W(k) as the carrier space of an (irreducible) representa-
tion. The homomorphism T (k) :G&rarr; GL(W(k)) is simply the restriction of
T to the subspace W(k), and we write
</p>
<p>Tg = T(1)g &oplus; T(2)g &oplus; &middot; &middot; &middot; &oplus; T(r)g &equiv;
r&oplus;
</p>
<p>k=1
T(k)g .
</p>
<p>If we identify all equivalent irreducible representations and collect them to-
gether, we may rewrite the last equation as
</p>
<p>Tg =m1T(1)g &oplus;m2T(2)g &oplus; &middot; &middot; &middot; &oplus;mρT(ρ)g &equiv;
ρ&oplus;
</p>
<p>α=1
mαT
</p>
<p>(α)
g , (24.4)
</p>
<p>where ρ is the number of inequivalent irreducible representations and mα
are positive integers giving the number of times an irreducible representation
T
(α)
g and all its equivalents occur in a given representation.
</p>
<p>In terms of matrices, Tg will be represented in a block-diagonal form as
</p>
<p>Tg =
</p>
<p>⎛
⎜⎜⎜⎜⎝
</p>
<p>T
(1)
g 0 . . . 0
</p>
<p>0 T
(2)
g . . . 0
</p>
<p>...
...
</p>
<p>...
</p>
<p>0 0 . . . T
(r)
g
</p>
<p>⎞
⎟⎟⎟⎟⎠
,</p>
<p/>
</div>
<div class="page"><p/>
<p>732 24 Representation of Groups
</p>
<p>or
</p>
<p>Tg =
</p>
<p>⎛
⎜⎜⎜⎜⎝
</p>
<p>[T(1)g ]m1 0 . . . 0
0 [T(2)g ]m2 . . . 0
...
</p>
<p>...
...
</p>
<p>0 0 . . . [T(ρ)g ]mρ
</p>
<p>⎞
⎟⎟⎟⎟⎠
,
</p>
<p>where, in the first matrix some of the T(k)g may be equivalent, and in the
</p>
<p>second matrix, [T(α)g ]mα is a block-diagonal matrix consisting of mα copies
of the matrix T(α)g .
</p>
<p>Example 24.2.7 A one-dimensional (and therefore irreducible) represen-
tation, defined for all groups, is the trivial (symmetric) representation
T : G &rarr; C given by T (g) = 1 for all g &isin; G. For the permutation group
</p>
<p>antisymmetric
</p>
<p>representation of a
</p>
<p>permutation group
</p>
<p>Sn, one can define another one-dimensional (thus irreducible) representation
T : Sn &rarr;C, called the antisymmetric representation, given by T (π)=+1
if π is even, and T (π)=&minus;1 if π is odd.
</p>
<p>Given any (matrix) representation T of G, one can form the transpose
inverse matrices (Ttg)
</p>
<p>&minus;1, and complex conjugate matrices T&lowast;g . The reader
may check that each set of these matrices forms a representation of G.
</p>
<p>Definition 24.2.8 The set of matrices (Ttg)
&minus;1 and T&lowast;g are called, respec-adjoint and complex
</p>
<p>conjugate
</p>
<p>representations
</p>
<p>tively, the adjoint representation, denoted by T , and the complex con-
jugate representation, denoted by T &lowast;.
</p>
<p>24.3 Orthogonality Properties
</p>
<p>Homomorphisms preserve group structures. By studying a group that is
more attuned to concrete manipulations, we gain insight into the structure
of groups that are homomorphic to it. The group of invertible operators on a
vector space, especially in their matrix representation, are particularly suited
for such a study because of our familiarity with matrices and operators. The
last section reduced this study to inequivalent irreducible representations.
This section is devoted to a detailed study of such representations.
</p>
<p>Lemma 24.3.1 (Schur&rsquo;s lemma) Let T : G &rarr; GL(V) and T &prime; : G &rarr;
Schur&rsquo;s lemma
</p>
<p>GL(V&prime;) be irreducible representations of G. If A &isin;L(V,V&prime;) is such that
</p>
<p>ATg = T&prime;gA &forall;g &isin;G, (24.5)
</p>
<p>then either A is an isomorphism (i.e., T is equivalent to T &prime;), or A= 0.
</p>
<p>Proof Let |a〉 &isin; kerA. Then
</p>
<p>ATg|a〉 = T&prime;g A|a〉︸︷︷︸
=0
</p>
<p>= 0 &rArr; Tg|a〉 &isin; kerA &forall;g &isin;G.</p>
<p/>
</div>
<div class="page"><p/>
<p>24.3 Orthogonality Properties 733
</p>
<p>It follows that kerA, a subspace of V, is invariant under T . Irreducibility of
T implies that either kerA= V, or kerA= 0. The first case asserts that A is
the zero linear transformation; the second case implies that A is injective.
</p>
<p>Similarly, let |b〉 &isin; A(V). Then |b〉 = A|x〉 for some |x〉 &isin; V:
</p>
<p>T&prime;g|b〉 = T&prime;gA|x〉 = ATg|x〉︸ ︷︷ ︸
&isin;A(V)
</p>
<p>&rArr; T&prime;g|b〉 &isin; A(V) &forall;g &isin;G.
</p>
<p>It follows that A(V), a subspace of V&prime;, is invariant under T &prime;. Irreducibility
of T &prime; implies that either A(V)= 0, or A(V)= V&prime;. The first case is consistent
with the first conclusion drawn above: kerA = V. The second case asserts
that A is surjective. Combining the two results, we conclude that A is either
the zero operator or an isomorphism. �
</p>
<p>Lemma 24.3.1 becomes extremely useful when we concentrate on a sin-
gle irreducible representation, i.e., when T &prime; = T .
</p>
<p>Lemma 24.3.2 Let T :G&rarr;GL(V) be an irreducible representation
of G. If A &isin;L(V) is such that ATg = TgA for all g &isin;G, then A= λ1.
</p>
<p>Proof Replacing V&prime; with V in Lemma 24.3.1, we conclude that A= 0 or A is
an isomorphism of V. In the first case, λ= 0. In the second case, A must have
a nonzero eigenvalue λ and at least one eigenvector (see Theorem 6.2.5). It
follows that the operator A &minus; λ1 commutes with all Tg&rsquo;s and it is not an
isomorphism (why not?). Therefore, it must be the zero operator. �
</p>
<p>We can immediately put this lemma to good use. If G is abelian, all op-
erators {Tx}x&isin;G commute with one another. Focusing on one of these opera-
tors, say Tg , noting that it commutes with all operators of the representation,
and using Lemma 24.3.2, we conclude that Tg = λ1. It follows that when
Tg acts on a vector, it gives a multiple of that vector. Therefore, it leaves any
one-dimensional subspace of the carrier space invariant. Since this is true
for all g &isin;G, we have the following result.
</p>
<p>Theorem 24.3.3 All irreducible representations of an abelian group are
one-dimensional.
</p>
<p>This theorem is an immediate consequence of Schur&rsquo;s lemma, and is in-
dependent of the order of G. In particular, it holds for infinite groups, if
Schur&rsquo;s lemma holds for those groups. One important class of infinite groups
for which Schur&rsquo;s lemma holds is the Lie groups (to be discussed in Part IX).
Thus, all abelian Lie groups have 1-dimensional irreducible representations.
We shall see later that the converse of Theorem 24.3.3 is also true for finite
groups.</p>
<p/>
</div>
<div class="page"><p/>
<p>734 24 Representation of Groups
</p>
<p>Historical Notes
</p>
<p>Issai Schur (1875&ndash;1941) was one of the most brilliant mathematicians active in Germany
</p>
<p>Issai Schur 1875&ndash;1941
</p>
<p>during the first third of the twentieth century. He attended the Gymnasium in Libau (now
Liepaja, Latvia) and then the University of Berlin, where he spent most of his scientific
career from 1911 until 1916. When he returned to Berlin, he was an assistant professor
at Bonn. He became full professor at Berlin in 1919. Schur was forced to retire by the
Nazi authorities in 1935 but was able to emigrate to Palestine in 1939. He died there of a
heart ailment several years later. Schur had been a member of the Prussian Academy of
Sciences before the Nazi purges. He married and had a son and daughter.
Schur&rsquo;s principal field was the representation theory of groups, founded a little before
1900 by his teacher Frobenius. Schur seems to have completed this field shortly be-
fore World War I, but he returned to the subject after 1925, when it became important
for physics. Further developed by his student Richard Brauer, it is in our time experi-
encing an extraordinary growth through the opening of new questions. Schur&rsquo;s disserta-
tion (1901) became fundamental to the representation theory of the general linear group;
in fact, English mathematicians have named certain of the functions appearing in the
work &ldquo;S-functions&rdquo; in Schur&rsquo;s honor. In 1905 Schur reestablished the theory of group
characters&mdash;the keystone of representation theory. The most important tool involved is
&ldquo;Schur&rsquo;s lemma.&rdquo; Along with the representation of groups by integral linear substitutions,
Schur was also the first to study representation by linear fractional substitutions, treating
this more difficult problem almost completely in two works (1904, 1907). In 1906 Schur
considered the fundamental problems that appear when an algebraic number field is taken
as the domain; a number appearing in this connection is now called the Schur index. His
works written after 1925 include a complete description of the rational and of the contin-
uous representations of the general linear group; the foundations of this work were in his
dissertation.
A lively interchange with many colleagues led Schur to contribute important memoirs to
other areas of mathematics. Some of these were published as collaborations with other
authors, although publications with dual authorship were almost unheard of at that time.
Here we simply indicate the areas: pure group theory, matrices, algebraic equations, num-
ber theory, divergent series, integral equations, and function theory.
</p>
<p>Example 24.3.4 Suppose that the Hamiltonian H of a quantum mechanical
All vectors of each
</p>
<p>irreducible subspace of
</p>
<p>the representation of a
</p>
<p>symmetry of the
</p>
<p>hamiltonian are
</p>
<p>eigenstates of the
</p>
<p>hamiltonian
</p>
<p>corresponding to a
</p>
<p>single eigenvalue.
</p>
<p>system with Hilbert space H has a group of symmetry with a representation
T : G &rarr; GL(H). Then HTg = TgH for all g &isin; G. It follows that H = λ1
if the representation is irreducible. Therefore, all vectors of each invariant
irreducible subspace are eigenstates of the hamiltonian corresponding to the
same eigenvalue, i.e., they all have the same energy. Therefore, the degen-
eracy of that energy state is at least as large as the dimension of the carrier
space.
</p>
<p>It is helpful to arrive at the statement above from a different perspective.
Consider a vector |x〉 in the eigenspace Mi corresponding to the energy
eigenvalue Ei . Since Tg and H commute, Tg|x〉 is also in Mi . Therefore, an
eigenspace of a Hamiltonian with a group of symmetry is invariant under
all Tg for any representation T of that group. If T is one of the irreducible
representations of G, say T (α) with dimension nα , then dimMi &ge; nα .
</p>
<p>Consider two irreducible representations T (α) and T (β) of a group G
with carrier spaces W(α) and W(β), respectively. Let X be any operator in
L(W(α),W(β)), and define
</p>
<p>A&equiv;
&sum;
</p>
<p>x&isin;G
T(α)x XT
</p>
<p>(β)
</p>
<p>x&minus;1 =
&sum;
</p>
<p>x&isin;G
T (α)(x)XT (β)
</p>
<p>(
x&minus;1
</p>
<p>)
.</p>
<p/>
</div>
<div class="page"><p/>
<p>24.3 Orthogonality Properties 735
</p>
<p>Then, we have
</p>
<p>T(α)g A=
&sum;
</p>
<p>x&isin;G
T (α)(g)T (α)(x)XT (β)
</p>
<p>(
x&minus;1
</p>
<p>)
T (β)
</p>
<p>(
g&minus;1
</p>
<p>)
T (β)(g)
</p>
<p>=
&sum;
</p>
<p>x&isin;G
T (α)(gx)XT (β)
</p>
<p>(
(gx)&minus;1
</p>
<p>)
</p>
<p>︸ ︷︷ ︸
=A because this sum also covers all G
</p>
<p>T (β)(g)= AT(β)g .
</p>
<p>We are interested in the two cases where T (α) = T (β), and where T (α) is
not equivalent to T (β). In the first case, Lemma 24.3.2 gives A= λ1; in the
second case, Lemma 24.3.1 gives A = 0. Combining these two results and
labeling the constant multiplying the unit operator by X, we can write
</p>
<p>&sum;
</p>
<p>g&isin;G
T (α)(g)XT (β)
</p>
<p>(
g&minus;1
</p>
<p>)
= λXδαβ1. (24.6)
</p>
<p>The presence of the completely arbitrary operator X indicates that Eq. (24.6)
is a powerful statement about&mdash;and a severe restriction on&mdash;the operators
T (α)(g). This becomes more transparent if we select a basis, represent all
operators by matrices, and for X, the matrix representation of X, choose a
matrix whose only nonzero element is 1 and occurs at the lth row and mth
column. Then Eq. (24.6) becomes
</p>
<p>&sum;
</p>
<p>g&isin;G
T
(α)
il (g)T
</p>
<p>(β)
mj
</p>
<p>(
g&minus;1
</p>
<p>)
= λlmδαβδij ,
</p>
<p>where λlm is a constant that can be evaluated by setting j = i, α = β , and
summing over i. The RHS will give λlm
</p>
<p>&sum;
i δii = λlmnα , where nα is the
</p>
<p>dimension of the carrier space of T (α). For the LHS we get
</p>
<p>LHS =
&sum;
</p>
<p>g&isin;G
</p>
<p>&sum;
</p>
<p>i
</p>
<p>T
(α)
il (g)T
</p>
<p>(α)
mi
</p>
<p>(
g&minus;1
</p>
<p>)
=
</p>
<p>&sum;
</p>
<p>g&isin;G
</p>
<p>(
T (α)
</p>
<p>(
g&minus;1
</p>
<p>)
T (α)(g)
</p>
<p>)
ml
</p>
<p>=
&sum;
</p>
<p>g&isin;G
T
(α)
ml
</p>
<p>(
g&minus;1g
</p>
<p>)
=
</p>
<p>&sum;
</p>
<p>g&isin;G
T
(α)
ml (e)︸ ︷︷ ︸
=(1)ml
</p>
<p>= |G|δml,
</p>
<p>where |G| is the order of the group. Putting everything together, we obtain
&sum;
</p>
<p>g&isin;G
T
(α)
il (g)T
</p>
<p>(β)
mj
</p>
<p>(
g&minus;1
</p>
<p>)
= |G|
</p>
<p>nα
δmlδαβδij . (24.7)
</p>
<p>If the representation is unitary, then
</p>
<p>&sum;
</p>
<p>g&isin;G
T
(α)
il (g)T
</p>
<p>(β)&lowast;
jm (g)=
</p>
<p>|G|
nα
</p>
<p>δmlδαβδij . (24.8)
</p>
<p>Equations (24.7) and (24.8) depend on the basis chosen in which to ex-
press matrices. To eliminate this dependence, we first introduce the impor-
tant concept of character.</p>
<p/>
</div>
<div class="page"><p/>
<p>736 24 Representation of Groups
</p>
<p>Definition 24.3.5 Let T :G&rarr; GL(V) be a representation of the group G.
character of a
</p>
<p>representation; simple
</p>
<p>character, compound
</p>
<p>character
</p>
<p>The character of this representation is the map χ :G&rarr;C given by
</p>
<p>χ(g)&equiv; trTg =
&sum;
</p>
<p>i
</p>
<p>Tii(g),
</p>
<p>where T(g) is the matrix representation of Tg in any basis of V. If T is
irreducible, the character is called simple; otherwise, it is called compound.
</p>
<p>The character of the identity element in any representation can be cal-
culated immediately. Since a homomorphism maps identity onto identity,
Te = 1. Therefore,
</p>
<p>χ(e)= tr(1)= dimV. (24.9)
Recall that two elements x, y &isin; G belong to the same conjugacy class if
there exist g &isin; G such that x = gyg&minus;1. This same relation holds for the
operators representing the elements: Tx = TgTyTg&minus;1 . Taking the trace of
both sides, and noting that Tg&minus;1 = T&minus;1g , one obtains
</p>
<p>Box 24.3.6 All elements of a group belonging to the same conjugacy
class have the same character.
</p>
<p>Setting i = l and j =m in (24.7) and summing over i and j , we obtain
&sum;
</p>
<p>g&isin;G
χ (α)(g)χ (β)
</p>
<p>(
g&minus;1
</p>
<p>)
</p>
<p>= |G|
nα
</p>
<p>δαβ
&sum;
</p>
<p>i,j
</p>
<p>δjiδij =
|G|
nα
</p>
<p>δαβ
&sum;
</p>
<p>j
</p>
<p>δjj
</p>
<p>︸ ︷︷ ︸
=nα
</p>
<p>= |G|δαβ . (24.10)
</p>
<p>If the representation is unitary, then (24.8) gives
&sum;
</p>
<p>g&isin;G
χ (α)(g)χ (β)&lowast;(g)= |G|δαβ . (24.11)
</p>
<p>This equation suggests a useful interpretation: Characters can be thought
of as vectors in a |G|-dimensional inner product space. According to
Eq. (24.11), the characters of inequivalent irreducible representations are or-
thogonal. In particular, since there cannot be more orthogonal vectors than
the dimension of a vector space, we conclude that the number of irreducible
inequivalent representations of a group cannot be more that the cardinality
of that group. Actually, we can do better. Restricting ourselves to unitary
representations and collecting all elements belonging to the same conjugacy
class together, we write
</p>
<p>r&sum;
</p>
<p>i=1
ciχ
</p>
<p>(α)
i χ
</p>
<p>(β)&lowast;
i = |G|δαβ &rArr;
</p>
<p>&lang;
χ (β)
</p>
<p>∣∣χ (α)
&rang;
= |G|δαβ , (24.12)</p>
<p/>
</div>
<div class="page"><p/>
<p>24.4 Analysis of Representations 737
</p>
<p>where i labels conjugacy classes, ci is the number of elements in the ith
class, r is the number of classes in G, and |χ (α)〉 &isin; Cr is an r-dimensional
vector with components {c1/2i χ
</p>
<p>(α)
i }ri=1. Equation (24.12) shows that vectors
</p>
<p>belonging to different irreducible representations are orthogonal. Since there
cannot be more orthogonal vectors than the dimension of a vector space, we
conclude that
</p>
<p>Proposition 24.3.7 The number of inequivalent irreducible representations
of a group cannot be more that the number of conjugacy classes of the group,
i.e., ρ &le; r .
</p>
<p>The characters of the adjoint representation are obtained from
</p>
<p>χ̄ (g)= χ
(
g&minus;1
</p>
<p>)
&rArr; χ̄i = χi&prime; ,
</p>
<p>where Ki&prime; is the class consisting of all elements inverse to those of the class
Ki . The equations involving characters of inverses of group elements can be
written in terms of the characters of the adjoint representation. For example,
Eq. (24.10) becomes
</p>
<p>&sum;
</p>
<p>g&isin;G
χ (α)(g)χ̄ (β)(g)= |G|δαβ &rArr;
</p>
<p>r&sum;
</p>
<p>i=1
ciχ
</p>
<p>(α)
i χ̄
</p>
<p>(β)
i = |G|δαβ . (24.13)
</p>
<p>Other relations can be obtained similarly.
</p>
<p>24.4 Analysis of Representations
</p>
<p>We can use the results obtained in the last section to gain insight into a given
representation. Take the trace of both sides of Eq. (24.4) and write the result
as
</p>
<p>χ(g)=m1χ (1)(g)+ &middot; &middot; &middot; +mρχ (ρ)(g)&equiv;
ρ&sum;
</p>
<p>α=1
mαχ
</p>
<p>(α)(g); (24.14)
</p>
<p>i.e., a compound character is a linear combination of simple characters with
nonnegative integer coefficients. Furthermore, the orthogonality of simple
characters gives
</p>
<p>mα =
1
</p>
<p>|G|
&sum;
</p>
<p>g&isin;G
χ(g)χ (α)&lowast;(g), (24.15)
</p>
<p>yielding the number of times the irreducible representation T (α) occurs in
the representation T .
</p>
<p>Another useful relation is obtained if we multiply Eq. (24.14) by its com-
plex conjugate and sum over g; the result is</p>
<p/>
</div>
<div class="page"><p/>
<p>738 24 Representation of Groups
</p>
<p>&sum;
</p>
<p>g&isin;G
</p>
<p>∣∣χ(g)
∣∣2 =
</p>
<p>&sum;
</p>
<p>g&isin;G
χ(g)χ&lowast;(g)=
</p>
<p>&sum;
</p>
<p>g&isin;G
</p>
<p>&sum;
</p>
<p>α
</p>
<p>mαχ
(α)(g)
</p>
<p>&sum;
</p>
<p>β
</p>
<p>mβχ
(β)&lowast;(g)
</p>
<p>=
&sum;
</p>
<p>α,β
</p>
<p>mαmβ
&sum;
</p>
<p>g&isin;G
χ (α)(g)χ (β)&lowast;(g)
</p>
<p>︸ ︷︷ ︸
=|G|δαβ
</p>
<p>= |G|
&sum;
</p>
<p>α
</p>
<p>m2α. (24.16)
</p>
<p>In particular, if T is irreducible, all mα are zero except for one, which is
unity. We therefore obtain the criterion for irreducibility:criterion for irreducibility
</p>
<p>&sum;
</p>
<p>g&isin;G
</p>
<p>∣∣χ(g)
∣∣2 =
</p>
<p>r&sum;
</p>
<p>i=1
ci |χi |2 = |G| if T is irreducible. (24.17)
</p>
<p>For groups of low order and representations of small dimensions,
Eq. (24.16) becomes a powerful tool for testing the irreducibility of the
representation.
</p>
<p>Example 24.4.1 Let G = S3 and consider the representation of Exam-
ple 24.2.2. The characters of the first three elements of this representation
are easily calculated:
</p>
<p>χ1 = trT1 = 3, χ2 = trT2 = 1, χ3 = trT3 = 1.
</p>
<p>Similarly, one can obtain χ4 = 1, χ5 = 0, and χ6 = 0. Substituting this in
Eq. (24.16) yields
</p>
<p>&sum;
</p>
<p>g&isin;G
</p>
<p>∣∣χ(g)
∣∣2 =
</p>
<p>6&sum;
</p>
<p>j=1
|χj |2 = 32 + 12 + 12 + 12 + 02 + 02 = 12.
</p>
<p>Comparing this with the RHS of (24.16) with |G| = 6 yields &sum;αm2α = 2.
This restricts the nonzero α&rsquo;s to two, say α = 1 and α = 2. Moreover, m1 and
m2 can be only 1. Thus, the representation of Example 24.2.2 is reducible,
and there are precisely two inequivalent irreducible representations in it,
each occurring once.
</p>
<p>We can actually find the invariant subspaces corresponding to the two ir-
reducible representations revealed above. The first is easy to guess. Just tak-
ing the sum of the three functions ψ1, ψ2, and ψ3 gives a one-dimensional
invariant subspace; so, let φ1 &equiv; ψ1 +ψ2 +ψ3, and note that the space W1
spanned by φ1 is invariant. The second is harder to discover. However, if we
assume that ψ1, ψ2, and ψ3 are orthonormal, then using the Gram&ndash;Schmidt
process, we can find the other two functions orthogonal to φ1 (but not or-
thogonal to each other!). These are
</p>
<p>φ2 =&minus;ψ1 + 2ψ2 &minus;ψ3, φ3 =&minus;ψ1 &minus;ψ2 + 2ψ3.
</p>
<p>The reader is urged to convince himself/herself that the subspace W(2)
</p>
<p>spanned by φ2 and φ3 is the complement of W(1) [i.e., V =W(1) &oplus;W(2)]
and that it is invariant under all Tg&rsquo;s.</p>
<p/>
</div>
<div class="page"><p/>
<p>24.4 Analysis of Representations 739
</p>
<p>A very useful representation can be constructed as follows. Let G =
{gj }mj=1, and recall that left multiplication of elements of G by a fixed ele-
ment gi is a permutation of (g1, g2, . . . , gm). Denote this permutation by πi .
Now define a representation R : G &rarr; GL(Rm), called the regular repre- regular representation
sentation, by
</p>
<p>Rgi (x1, x2, . . . , xm)= (xπi (1), xπi (2), . . . , xπi (m)).
</p>
<p>That this is indeed a representation is left as a problem for the reader. One
can obtain a matrix representation of R by choosing the standard basis
{êj }mj=1 of Rm and noting that Rgi êj = êπ&minus;1i (j). From such a matrix repre-
sentation it follows that all characters χR of the regular representations are
zero except for the identity, whose character is χR(e)=m [see Eq. (24.9)].
Now use Eq. (24.14) for g = e and for the regular representation to obtain
m =&sum;ρα=1 mαnα where nα is the dimension of the α-th irreducible repre-
sentation. We can find mα by using Eq. (24.15) and noting that only g = e
contributes to the sum:
</p>
<p>mα =
1
</p>
<p>|G|
&sum;
</p>
<p>g&isin;G
χR(g)χ (α)&lowast;(g)= 1
</p>
<p>m
χR(e)χ (α)&lowast;(e)︸ ︷︷ ︸
</p>
<p>nα
</p>
<p>= nα.
</p>
<p>In words,
</p>
<p>Box 24.4.2 The number of times an irreducible representation oc-
curs in the regular representation is equal to the dimension of that
irreducible representation.
</p>
<p>We therefore obtain the important relations
</p>
<p>χRi = |G|δi1 =
ρ&sum;
</p>
<p>α=1
nαχ
</p>
<p>(α)
i and |G| =
</p>
<p>ρ&sum;
</p>
<p>α=1
n2α, (24.18)
</p>
<p>where we have assumed that the first conjugacy class is that of the identity.
For finite groups of small order, the second equation can be very useful in
obtaining the dimensions of irreducible representations.
</p>
<p>Example 24.4.3 A group of order 2 or 3 has only one-dimensional inequiv-
alent irreducible representations, because the only way that Eq. (24.18) can
be satisfied for |G| = 2 or 3 is for all nα&rsquo;s to be 1. A group of order 4
can have either 4 one-dimensional or one 2-dimensional inequivalent irre-
ducible representations. The symmetric group S3, being of order 6, can have
6 one-dimensional, or 2 one-dimensional and one 2-dimensional inequiva-
lent irreducible representations. We shall see later that if all inequivalent
irreducible representations of a group are one-dimensional, then the group
must be abelian. Thus, the first possibility for S3 must be excluded.</p>
<p/>
</div>
<div class="page"><p/>
<p>740 24 Representation of Groups
</p>
<p>24.5 Group Algebra
</p>
<p>Think of group elements as (linearly independent) vectors. In fact, given
any set, one can generate a vector space by taking linear combinations of
the elements of the set assumed to form a basis. In the case of groups one
gets a bonus: The product already defined on the basis (group elements) can
be extended by linearity to all elements of the vector space to turn it into an
algebra called the group algebra. For G= {gj }mj=1, a typical element of thegroup algebra defined
group algebra is a=&sum;mi=1 aigi . One can add two vectors as usual. But the
product of two vectors is also defined:
</p>
<p>ab=
(
</p>
<p>m&sum;
</p>
<p>i=1
aigi
</p>
<p>)(
m&sum;
</p>
<p>j=1
bjgj
</p>
<p>)
=
</p>
<p>m&sum;
</p>
<p>i=1
</p>
<p>m&sum;
</p>
<p>j=1
aibj gigj︸︷︷︸
</p>
<p>gk
</p>
<p>&equiv;
m&sum;
</p>
<p>k=1
ckgk,
</p>
<p>where ck is a sum involving ai and bj . The best way to learn this is to see
an example.
</p>
<p>Example 24.5.1 Let G = S3 and consider a = 2π1 &minus; 3π3 + π5 and b =
π2 &minus; 2π4 + 3π6. Then, using Table 23.1, we obtain
</p>
<p>ab= (2π1 &minus; 3π3 + π5)(π2 &minus; 2π4 + 3π6)
= 2π1π2 &minus; 4π1π4 + 6π1π6 &minus; 3π3π2 + 6π3π4
&minus; 9π3π6 + π5π2 &minus; 2π5π4 + 3π5π6
</p>
<p>= 2π2 &minus; 4π4 + 6π6 &minus; 3π6 + 6π5 &minus; 9π2 + π4 &minus; 2π3 + 3π1
= 3π1 &minus; 7π2 &minus; 2π3 &minus; 3π4 + 6π5 + 3π6.
</p>
<p>24.5.1 Group Algebra and Representations
</p>
<p>Group algebra is very useful for the construction and analysis of representa-
tions of groups. In fact, we have already used a similar approach in the con-
struction of the regular representation. Instead of Rm used before, use the
m-dimensional vector space A, the group algebra. Then left-multiplication
by a group element g can be identified with T(R)g , the operators of the regular
representation, and the invariant subspaces of A become the left ideals of A,
and we can write
</p>
<p>A=L1 &oplus;L2 &oplus; &middot; &middot; &middot; &oplus;Lr .
</p>
<p>Moreover, since the identity element of the group is the identity element of
the algebra as well, we haveresolution of the identity
</p>
<p>e= e1 + &middot; &middot; &middot; + er , e2i = ei, eiej = 0 for i �= j. (24.19)</p>
<p/>
</div>
<div class="page"><p/>
<p>24.5 Group Algebra 741
</p>
<p>It is clear that if a2 = αa, then a/α will be idempotent. So, we can es- essentially idempotent
elementssentially ignore the constant α, which is why a is called essentially idem-
</p>
<p>potent. Now consider the element of the group algebra
</p>
<p>P =
&sum;
</p>
<p>x&isin;G
x (24.20)
</p>
<p>and note that gP =&sum;x&isin;G gx = P . It follows that
</p>
<p>P 2 =
&sum;
</p>
<p>g&isin;G
g
&sum;
</p>
<p>x&isin;G
x =
</p>
<p>&sum;
</p>
<p>g&isin;G
</p>
<p>&sum;
</p>
<p>x&isin;G
gx =
</p>
<p>&sum;
</p>
<p>g&isin;G
P = |G|P.
</p>
<p>So, P is essentially idempotent. Furthermore, the reader may verify that the
ideal generated by P is one-dimensional.
</p>
<p>Let us now apply the notion of the group algebra to derive further rela-
tions among characters. Denote the elements of the ith class Ki of G by
{x(i)l }
</p>
<p>ci
l=1 and construct the element of the group algebra κi &equiv;
</p>
<p>&sum;ci
l=1 x
</p>
<p>(i)
l . If
</p>
<p>in the product of two such quantities
</p>
<p>κiκj =
ci&sum;
</p>
<p>l=1
</p>
<p>cj&sum;
</p>
<p>m=1
x
(i)
l x
</p>
<p>(j)
m , (24.21)
</p>
<p>x
(i)
l x
</p>
<p>(j)
m &equiv; y &isin;G, is in a certain conjugacy class, then the rest of that class
</p>
<p>can be obtained by taking all conjugates of y, i.e., elements of G that can be
written as
</p>
<p>gyg&minus;1 = gx(i)l x
(j)
m g
</p>
<p>&minus;1 = gx(i)l g&minus;1︸ ︷︷ ︸
&isin;Ki
</p>
<p>gx
(j)
m g
</p>
<p>&minus;1
︸ ︷︷ ︸
</p>
<p>&isin;Kj
</p>
<p>.
</p>
<p>It follows that if one member of a class appears in the double sum of
Eq. (24.21), all members will appear there. The reader may check that if
y occurs k times in the double sum, then all members of the class of y occur
k times as well. Collecting all such members together, we can write
</p>
<p>κiκj =
r&sum;
</p>
<p>l=1
cij lκl, (24.22)
</p>
<p>where cij l are positive integers.
Now consider the αth irreducible representation, and add all operators
</p>
<p>corresponding to a given class:
</p>
<p>T
(α)
i &equiv;
</p>
<p>&sum;
</p>
<p>g&isin;Ki
T(α)g &rArr; T(α)i T
</p>
<p>(α)
j =
</p>
<p>r&sum;
</p>
<p>l=1
cij lT
</p>
<p>(α)
l , (24.23)
</p>
<p>where the second equation follows from the same sort of argument used
above to establish Eq. (24.22). One can show that T(α)i commutes with all
</p>
<p>T
(α)
g . Therefore, by Schur&rsquo;s lemma, T
</p>
<p>(α)
i = λ
</p>
<p>(α)
i 1, and the second equation
</p>
<p>in (24.23) becomes
</p>
<p>λ
(α)
i λ
</p>
<p>(α)
j =
</p>
<p>r&sum;
</p>
<p>l=1
cij lλ
</p>
<p>(α)
l . (24.24)</p>
<p/>
</div>
<div class="page"><p/>
<p>742 24 Representation of Groups
</p>
<p>Taking the characters of both sides of T(α)i = λ
(α)
i 1 and using the first equa-
</p>
<p>tion in (24.23), noting that all elements of a class have the same character,
we get
</p>
<p>ciχ
(α)
i = λ
</p>
<p>(α)
i nα &rArr; λ
</p>
<p>(α)
i =
</p>
<p>ciχ
(α)
i
</p>
<p>nα
.
</p>
<p>Substituting this in Eq. (24.24), we obtain
</p>
<p>cicjχ
(α)
i χ
</p>
<p>(α)
j = nα
</p>
<p>r&sum;
</p>
<p>l=1
cij lclχ
</p>
<p>(α)
l . (24.25)
</p>
<p>This is another equation that is useful for computing characters. Note that
this equation connects the purely group properties (ci &rsquo;s and cij l&rsquo;s) with the
</p>
<p>properties of the representation (χ (α)i &rsquo;s and nα). Summing Eq. (24.25) over
α and using the first equation in (24.18), we get
</p>
<p>cicj
</p>
<p>ρ&sum;
</p>
<p>α=1
χ
(α)
i χ
</p>
<p>(α)
j =
</p>
<p>r&sum;
</p>
<p>l=1
cij lcl
</p>
<p>ρ&sum;
</p>
<p>α=1
nαχ
</p>
<p>(α)
l
</p>
<p>︸ ︷︷ ︸
=|G|δl1 by (24.18)
</p>
<p>= cij1|G|
</p>
<p>because c1 = 1 (there is only one element in the class of the identity). Prob-
lem 24.12 shows that cij1 = ciδi&prime;j where Ki&prime; is the class consisting of in-
verses of elements of Ki . It then follows that
</p>
<p>ρ&sum;
</p>
<p>α=1
χ
(α)
i χ
</p>
<p>(α)
j =
</p>
<p>|G|
cj
</p>
<p>δi&prime;j . (24.26)
</p>
<p>For a unitary representation, χ (α)
i&prime; = χ
</p>
<p>(α)&lowast;
i , so Eq. (24.26) becomes
</p>
<p>ρ&sum;
</p>
<p>α=1
χ
(α)
i χ
</p>
<p>(α)&lowast;
j =
</p>
<p>|G|
cj
</p>
<p>δij &rArr; 〈χj |χi〉 =
|G|
cj
</p>
<p>δij , (24.27)
</p>
<p>where |χi〉 &isin;Cρ is a ρ-dimensional vector with components {χ (α)i }
ρ
α=1. This
</p>
<p>equation can also be written in terms of group elements rather than classes.
Since χ (α)i = χ (α)(x) for any x &isin;Ki , we have
</p>
<p>ρ&sum;
</p>
<p>α=1
χ (α)(x)χ (α)&lowast;(y)= |G||KGx |
</p>
<p>δ
(
KGx ,K
</p>
<p>G
y
</p>
<p>)
, (24.28)
</p>
<p>where KGx is the conjugacy class of G containing x, |KGx | is the number of
its elements, and
</p>
<p>δ
(
KGx ,K
</p>
<p>G
y
</p>
<p>)
=
{
</p>
<p>1 if KGx =KGy ,
0 otherwise.
</p>
<p>Equation (24.27) shows that the r vectors χ (α)i are mutually orthogonal;
therefore, r &le; ρ. Combining this with Proposition 24.3.7, we obtain the fol-
lowing:</p>
<p/>
</div>
<div class="page"><p/>
<p>24.6 Relationship of Characters to Those of a Subgroup 743
</p>
<p>Table 24.1 A typical character table
</p>
<p>c1K1
c2K2 &middot; &middot; &middot; ciKi &middot; &middot; &middot; crKr
</p>
<p>T (1) χ
(1)
1 χ
</p>
<p>(1)
2 &middot; &middot; &middot; χ
</p>
<p>(1)
i &middot; &middot; &middot; χ
</p>
<p>(1)
r
</p>
<p>T (2) χ
(2)
1 χ
</p>
<p>(2)
2 &middot; &middot; &middot; χ
</p>
<p>(2)
i &middot; &middot; &middot; χ
</p>
<p>(2)
r
</p>
<p>.
</p>
<p>.
</p>
<p>.
.
.
.
</p>
<p>.
</p>
<p>.
</p>
<p>.
.
.
.
</p>
<p>.
</p>
<p>.
</p>
<p>.
</p>
<p>T (α) χ
(α)
1 χ
</p>
<p>(α)
2 &middot; &middot; &middot; χ
</p>
<p>(α)
i &middot; &middot; &middot; χ
</p>
<p>(α)
r
</p>
<p>.
</p>
<p>.
</p>
<p>.
.
.
.
</p>
<p>.
</p>
<p>.
</p>
<p>.
.
.
.
</p>
<p>.
</p>
<p>.
</p>
<p>.
</p>
<p>T (r) χ
(r)
1 χ
</p>
<p>(r)
2 &middot; &middot; &middot; χ
</p>
<p>(r)
i &middot; &middot; &middot; χ
</p>
<p>(r)
r
</p>
<p>Theorem 24.5.2 The number of inequivalent irreducible representa-
tions of a finite group is equal to the number of conjugacy classes in
the group.
</p>
<p>It is convenient to summarize our result in a square table with rows la-
beled by the irreducible representation and columns labeled by the conju-
gacy classes of G. Then on the αth row and ith column we list χ (α)i , and character table of a finite
</p>
<p>groupwe get Table 24.1, called the character table of G. Note that ci , the order
of Ki , is written as a left superscript. Character tables have the property that
any two of their rows are orthogonal in the sense of Eq. (24.12), and any
two of their columns are orthogonal in the sense of Eq. (24.27).
</p>
<p>If all inequivalent irreducible representations of a group G have dimen-
sion one, then there will be |G| of them [by Eq. (24.18)]. Hence, there will be
|G| conjugacy classes; i.e., each class consists of a single element. By Prob-
lem 23.16, the group must be abelian. Combining this with Theorem 24.3.3,
we have the following theorem.
</p>
<p>Theorem 24.5.3 A finite group is abelian if and only if all its inequivalent
irreducible representations are one-dimensional.
</p>
<p>24.6 Relationship of Characters to Those of a Subgroup
</p>
<p>Let H be a subgroup of G. Denote by KHh and K
G
g the H -class contain-
</p>
<p>ing h &isin;H and the G-class containing g, respectively. Let dj and ci be the
number of elements in the j th H -class and ith G-class, respectively. Any
representation of G defines a representation of H by restriction. An irre-
ducible representation of G may be reducible as a representation of H . This
is because although the subspace W(α) of the carrier space that is irreducible
under G is the smallest such subspace containing a given vector, it is possi-
ble to generate a smaller subspace by applying a subset of the operators Tg
corresponding to those g&rsquo;s that belong to H . It follows that
</p>
<p>T (α)(h)=
&sum;
</p>
<p>σ
</p>
<p>mασ t
(σ )(h), h &isin;H, (24.29)</p>
<p/>
</div>
<div class="page"><p/>
<p>744 24 Representation of Groups
</p>
<p>where mασ are nonnegative integers as in Eq. (24.14) and t (σ ) are irreducible
representations of H . If χ (α) and ξ (σ ) denote the characters of irreducible
representations of G and H , respectively, then the equivalent equation for
the characters is
</p>
<p>χ (α)(h)=
&sum;
</p>
<p>σ
</p>
<p>mασ ξ
(σ )(h), h &isin;H. (24.30)
</p>
<p>Multiply both sides by ξ (κ)&lowast;(h), sum over h &isin; H , and take the complex
conjugate at the end. Then by the orthogonality relation (24.11), applied to
H , we obtain
</p>
<p>mακ =
1
</p>
<p>|H |
&sum;
</p>
<p>h&isin;H
χ (α)&lowast;(h)ξ (κ)(h). (24.31)
</p>
<p>Now multiply both sides of Eq. (24.31) by χ (α)(g), sum over α, and use
Eq. (24.28) to obtain
</p>
<p>&sum;
</p>
<p>α
</p>
<p>mακχ
(α)(g)= |G||H ||KGg |
</p>
<p>&sum;
</p>
<p>h&isin;H
δ
(
KGh ,K
</p>
<p>G
g
</p>
<p>)
ξ (κ)(h). (24.32)
</p>
<p>The sum on the right can be transformed into a sum over conjugacy classes
of H . Then Eq. (24.32) becomes
</p>
<p>&sum;
</p>
<p>α
</p>
<p>mακχ
(α)
i =
</p>
<p>|G|
|H |ci
</p>
<p>&sum;
</p>
<p>j
</p>
<p>dj ξ
(κ)
j , i = 1,2, . . . , r, (24.33)
</p>
<p>where the sum on the LHS is over irreducible representations of G, and on
the RHS it is over those H -classes j that lie in the ith G-class. Note that the
coefficients |G|dj/(|H |ci) are integers by Problem 23.17.
</p>
<p>Equations (24.32) and (24.33) are useful for obtaining characters of G
when those of a subgroup H are known. The general procedure is to note
that the RHS of these equations are completely determined by the structure
of the group G and the characters of H . Varying i, the RHS of (24.33)
determines the r components of a (compound) character |ψ〉, which, by the
LHS, can be written as a linear combination of characters of G:
</p>
<p>|ψ〉 &equiv;
r&sum;
</p>
<p>α=1
mα
</p>
<p>∣∣χ (α)
&rang;
, (24.34)
</p>
<p>where we have suppressed the irrelevant subscript κ . If we know some of
the |χ (α)〉&rsquo;s, we may be able to determine the rest by taking successive inner
products to find the integers mα , and subtracting each irreducible factor of
the sum from the LHS. We illustrate this procedure for Sn in the following
example.
</p>
<p>Example 24.6.1 Let K1 = (12) and K2 = (2) for S2 (see Sect. 23.4 for no-
tation). Example 24.2.7 showed that we can construct two irreducible repre-
sentations for any Sn, the symmetric and the antisymmetric representations.
The reader may verify that these two representations are inequivalent. Since</p>
<p/>
</div>
<div class="page"><p/>
<p>24.6 Relationship of Characters to Those of a Subgroup 745
</p>
<p>Table 24.2 Character table for S2
</p>
<p>1K1
1K2
</p>
<p>T (1) 1 1
</p>
<p>T (2) 1 &minus;1
</p>
<p>Table 24.3 Partially filled character table for S3
</p>
<p>1K1
3K2
</p>
<p>2K3
</p>
<p>T (1) 1 1 1
</p>
<p>T (2) 1 &minus;1 1
T (3) ? ? ?
</p>
<p>the number of inequivalent irreducible representations is equal to the num-
ber of classes in a group, we have all the information needed to construct
the character table for S2. Table 24.2 shows this character table. We want
to use the S2 character table to construct the character table for S3. With
our knowledge of the symmetric and the antisymmetric representations, we
can partially fill in the S3 character table. Let K1 = (13), K2 = (2,1), and
K3 = (3) and note that c1 = 1, c2 = 3, and c3 = 2. Then we obtain Ta-
ble 24.3. To complete the table, we start with κ = 1, and write the RHS of
Eq. (24.33) as
</p>
<p>ψi =
6
</p>
<p>2ci
</p>
<p>&sum;
</p>
<p>j
</p>
<p>dj ξ
(1)
j =
</p>
<p>3
</p>
<p>ci
</p>
<p>&sum;
</p>
<p>j
</p>
<p>ξ
(1)
j
</p>
<p>because dj = 1 for the two classes of S2. The sum on the RHS is over S2-
classes that are inside the ith S3-class. For i = 1, only the first S2-class
contributes. Noting that ξ (κ)j are the entries of Table 24.2, we get
</p>
<p>ψ1 =
3
</p>
<p>c1
ξ
(1)
1 =
</p>
<p>3
</p>
<p>1
&middot; 1 = 3.
</p>
<p>Similarly,
</p>
<p>ψ2 =
3
</p>
<p>c2
ξ
(1)
2 =
</p>
<p>3
</p>
<p>3
&middot; 1 = 1 and ψ3 =
</p>
<p>3
</p>
<p>c3
&middot; 0 = 0.
</p>
<p>The second equation follows from the fact that there are no classes of S2
inside the third class of S3. Equation (24.34) now gives
</p>
<p>|ψ〉 =
</p>
<p>⎛
⎝
</p>
<p>3
1
0
</p>
<p>⎞
⎠=
</p>
<p>r&sum;
</p>
<p>α=1
mα
</p>
<p>∣∣χ (α)
&rang;
.
</p>
<p>We can find the number of times |χ (1)〉 occurs in this compound character
by taking the inner product:
</p>
<p>&lang;
χ (1)
</p>
<p>∣∣ψ
&rang;
=
</p>
<p>r&sum;
</p>
<p>α=1
mα
</p>
<p>&lang;
χ (1)
</p>
<p>∣∣χ (α)
&rang;
=m1|G| = 6m1.</p>
<p/>
</div>
<div class="page"><p/>
<p>746 24 Representation of Groups
</p>
<p>Table 24.4 Complete character table for S3
</p>
<p>1K1
3K2
</p>
<p>2K3
</p>
<p>T (1) 1 1 1
</p>
<p>T (2) 1 &minus;1 1
T (3) 2 0 &minus;1
</p>
<p>But
</p>
<p>&lang;
χ (1)
</p>
<p>∣∣ψ
&rang;
=
</p>
<p>r&sum;
</p>
<p>i=1
ciχ
</p>
<p>(1)
i ψi = 1 &middot; 1 &middot; 3 + 3 &middot; 1 &middot; 1 + 2 &middot; 1 &middot; 0 = 6.
</p>
<p>These two equations show that m1 = 1. So,
⎛
⎝
</p>
<p>3
1
0
</p>
<p>⎞
⎠=
</p>
<p>⎛
⎝
</p>
<p>1
1
1
</p>
<p>⎞
⎠+m2
</p>
<p>∣∣χ (2)
&rang;
+m3
</p>
<p>∣∣χ (3)
&rang;
.
</p>
<p>Subtracting the column vectors, we get a new character:
</p>
<p>∣∣ψ &prime;
&rang;
&equiv;
</p>
<p>⎛
⎝
</p>
<p>2
0
&minus;1
</p>
<p>⎞
⎠=m2
</p>
<p>∣∣χ (2)
&rang;
+m3
</p>
<p>∣∣χ (3)
&rang;
.
</p>
<p>Taking the inner product with |χ (2)〉 yields m2 = 0. It follows that |ψ &prime;〉 is a
simple character. In fact,
</p>
<p>&sum;
</p>
<p>i
</p>
<p>ci
∣∣ψ &prime;i
</p>
<p>∣∣2 = 1 &middot; 22 + 3 &middot; 02 + 2 &middot; (&minus;1)2 = 6,
</p>
<p>and the criterion of irreducibility, Eq. (24.17), is satisfied.
We can now finish up Table 24.3 to obtain Table 24.4, which is the com-
</p>
<p>plete character table for S3.
</p>
<p>24.7 Irreducible Basis Functions
</p>
<p>We have studied the operators Tg and their characters representing group el-
ements in rather extensive detail. Let us now turn our attention to the carrier
space itself. In particular, we want to concentrate on the basis functions of
the irreducible representations. We choose &ldquo;functions,&rdquo; rather than vectors,
because of their use in quantum mechanics as discussed at the beginning of
this chapter.
</p>
<p>Let {|ψ (α)i 〉}
nα
i=1 be a set of basis functions for W
</p>
<p>(α), the αth invariant
irreducible subspace. Invariance of W(α) implies that
</p>
<p>Tg
∣∣ψ (α)i
</p>
<p>&rang;
=
</p>
<p>nα&sum;
</p>
<p>j=1
T
(α)
ji (g)
</p>
<p>∣∣ψ (α)j
&rang;
,
</p>
<p>where T (α)ji (g) are elements of the matrix T
(α)
g representing g &isin;G.</p>
<p/>
</div>
<div class="page"><p/>
<p>24.7 Irreducible Basis Functions 747
</p>
<p>Definition 24.7.1 A function (or vector) |φ(α)i 〉 is said to belong to the functions belonging to
the ith row of the αth
</p>
<p>irreducible
</p>
<p>representation
</p>
<p>ith row of the αth irreducible representation (or to transform accord-
ing to the ith row of the αth irreducible representation) if there exists a
basis {|ψ (α)i 〉}
</p>
<p>nα
i=1 of the αth irreducible representation of G with matrices
</p>
<p>(T
(α)
ji (g)) and nα &minus; 1 other functions {|φ
</p>
<p>(α)
j 〉} such that
</p>
<p>Tg
∣∣φ(α)i
</p>
<p>&rang;
=
</p>
<p>nα&sum;
</p>
<p>j=1
T
(α)
ji (g)
</p>
<p>∣∣φ(α)j
&rang;
. (24.35)
</p>
<p>Functions that belong to rows of irreducible representations have some
remarkable properties. Let |ψ (α)i 〉 and |φ
</p>
<p>(β)
j 〉 transform according to the ith
</p>
<p>and j th rows of the αth and βth irreducible representations, respectively.
Choose an inner product for the carrier space such that all representations
are unitary. Then we have
</p>
<p>&lang;
ψ
</p>
<p>(α)
i
</p>
<p>∣∣φ(β)j
&rang;
=
&lang;
Tgψ
</p>
<p>(α)
i
</p>
<p>∣∣Tgφ(β)j
&rang;
</p>
<p>=
nα&sum;
</p>
<p>l=1
</p>
<p>nβ&sum;
</p>
<p>m=1
T
(α)&lowast;
li (g)T
</p>
<p>(β)
mj (g)
</p>
<p>&lang;
ψ
</p>
<p>(α)
l
</p>
<p>∣∣φ(β)m
&rang;
.
</p>
<p>Summing this equation over g yields |G|〈ψ (α)i |φ
(β)
j 〉 for the LHS, while
</p>
<p>RHS =
nα&sum;
</p>
<p>l=1
</p>
<p>nβ&sum;
</p>
<p>m=1
</p>
<p>(|G|/nα)δαβδlmδij︷ ︸︸ ︷&sum;
</p>
<p>g&isin;G
T
(α)&lowast;
li (g)T
</p>
<p>(β)
mj (g)
</p>
<p>&lang;
ψ
</p>
<p>(α)
l
</p>
<p>∣∣φ(β)m
&rang;
</p>
<p>= |G|
nα
</p>
<p>δαβδij
</p>
<p>nα&sum;
</p>
<p>l=1
</p>
<p>&lang;
ψ
</p>
<p>(α)
l
</p>
<p>∣∣φ(β)l
&rang;
,
</p>
<p>where we have made use of Eq. (24.8). Therefore,
</p>
<p>&lang;
ψ
</p>
<p>(α)
i
</p>
<p>∣∣φ(β)j
&rang;
= 1
</p>
<p>nα
δαβδij
</p>
<p>nα&sum;
</p>
<p>l=1
</p>
<p>&lang;
ψ
</p>
<p>(α)
l
</p>
<p>∣∣φ(α)l
&rang;
. (24.36)
</p>
<p>This shows that functions belonging to different irreducible representa-
tions are orthogonal. We should expect this, because in our construction
of invariant irreducible subspaces, we kept dividing the whole space into
orthogonal complements. What is surprising is that functions transforming
according to different rows of an irreducible representation are orthogonal.
We had no control over this property! It is a consequence of Eq. (24.35).
Another surprise is the independence of the inner product from i: If we let
i = j and α = β on both sides of (24.36), we obtain
</p>
<p>&lang;
ψ
</p>
<p>(α)
i
</p>
<p>∣∣φ(α)i
&rang;
= 1
</p>
<p>nα
</p>
<p>nα&sum;
</p>
<p>l=1
</p>
<p>&lang;
ψ
</p>
<p>(α)
l
</p>
<p>∣∣φ(α)l
&rang;
, (24.37)
</p>
<p>which indicates that the inner product on the LHS is independent of i.</p>
<p/>
</div>
<div class="page"><p/>
<p>748 24 Representation of Groups
</p>
<p>Example 24.7.2 The quantum-mechanical perturbation theory starts with asymmetry and the
quantummechanical
</p>
<p>perturbation theory;
</p>
<p>lifting of degeneracy
</p>
<p>known Hamiltonian H0 with eigenvalues Ei and the corresponding eigen-
states |Ei〉. Subsequently, a (small) perturbing &ldquo;potential&rdquo; V is added to
the Hamiltonian, and the eigenvalues and eigenstates of the new Hamilto-
nian H = H0 + V are sought. One can draw important conclusions about
the eigenvalues and eigenstates of the total Hamiltonian by symmetry argu-
ments.
</p>
<p>Suppose the symmetry group of H0 is G, and that of H is H , which has
to be a subgroup of G. In most cases, the eigenspaces of H0 are irreducible
carrier spaces of G, i.e., their basis vectors transform according to the rows
of irreducible representations of G. If H is a proper subgroup of G, then
the eigenspaces of H0 will split according to Eq. (24.29). We say that some
of the degeneracy is lifted because of the perturbation V. The nature of the
split, i.e., the number and the dimensionality of the vector spaces into which
a given eigenspace splits, can be obtained by the characters of G and H and
Eq. (24.30). The original eigenspaces are represented on an energy diagram
with a line corresponding to each eigenspace. The split of the eigenspace
into k new subspaces is then indicated by the branching of the old line into
k new lines.
</p>
<p>To the lowest approximation&mdash;first-order perturbation theory&mdash;the mag-
nitude of the split, i.e., the difference between the eigenvalues of H0 and
those of H, is given by [see Eq. (21.57)] the expectation value 〈φ(α)i |V|φ
</p>
<p>(α)
j 〉,
</p>
<p>where |φ(α)i 〉 belongs to the ith row of the αth irreducible representation, and
|φ(α)j 〉 to its j th row (i �= j ). Only if this expectation value is nonzero will a
split occur. This, in turn, depends on the symmetry of V: If V is at least as
symmetric as H0 (corresponding to G=H ), then 〈φ(α)i |V|φ
</p>
<p>(α)
j 〉 = 0, and no
</p>
<p>splitting occurs (Problem 24.17). If, on the other hand, V is less symmetric
than H0 (corresponding to H &sub;G), then V|φ(α)j 〉 will not belong to the j th
row of the αth irreducible representation, and in general, 〈φ(α)i |V|φ
</p>
<p>(α)
j 〉 �= 0.
</p>
<p>We have decomposed the carrier space V of a representation into invari-
ant irreducible subspaces W(α). The argument above shows that each W(α)
</p>
<p>has a basis consisting of the &ldquo;rows&rdquo; of the irreducible representations. Cor-
responding to such a basis, there is a set of projection operators P(α)i with
</p>
<p>the property
&sum;
</p>
<p>α,i P
(α)
i = 1 (Chap. 6). Our aim is to find an expression for
</p>
<p>these operators, which have the defining property P(α)i |ψ
(α)
i 〉 = |ψ
</p>
<p>(α)
i 〉. We
</p>
<p>start with Eq. (24.35), multiply both sides of it by T (β)&lowast;lm (g), sum over g &isin;G,
and use Eq. (24.8) to obtain
</p>
<p>&sum;
</p>
<p>g&isin;G
T
(β)&lowast;
lm (g)Tg
</p>
<p>∣∣ψ (α)i
&rang;
=
</p>
<p>nα&sum;
</p>
<p>j=1
</p>
<p>∣∣ψ (α)j
&rang;&sum;
</p>
<p>g&isin;G
T
(β)&lowast;
lm (g)T
</p>
<p>(α)
ji (g)
</p>
<p>= |G|
nα
</p>
<p>nα&sum;
</p>
<p>j=1
</p>
<p>∣∣ψ (α)j
&rang;
δlj δmiδαβ =
</p>
<p>|G|
nα
</p>
<p>∣∣ψ (α)l
&rang;
δmiδαβ .</p>
<p/>
</div>
<div class="page"><p/>
<p>24.7 Irreducible Basis Functions 749
</p>
<p>Let β = α, m= l = i, and multiply both sides by nα/|G|. Then this equation
becomes
</p>
<p>nα
</p>
<p>|G|
&sum;
</p>
<p>g&isin;G
T
(α)&lowast;
ii (g)Tg
</p>
<p>∣∣ψ (α)i
&rang;
=
∣∣ψ (α)i
</p>
<p>&rang;
,
</p>
<p>which suggests the identification projection operator onto
the ith row of the αth
</p>
<p>irreducible
</p>
<p>representation
</p>
<p>P
(α)
i =
</p>
<p>nα
</p>
<p>|G|
&sum;
</p>
<p>g&isin;G
T
(α)&lowast;
ii (g)Tg (24.38)
</p>
<p>with the properties
</p>
<p>P
(α)
i
</p>
<p>∣∣ψ (β)j
&rang;
=
∣∣ψ (α)i
</p>
<p>&rang;
δij δαβ , P
</p>
<p>(α)
i |φ〉 =
</p>
<p>∣∣φ(α)i
&rang;
, (24.39)
</p>
<p>where |φ(α)i 〉 is the projection of |φ〉 along the ith row of the αth irreducible
representation.
</p>
<p>We are also interested in the projection operator that projects onto the
irreducible subspace W(α). Such an operator is obtained by summing P(α)i
over i. We thus obtain projection operator onto
</p>
<p>the αth irreducible
</p>
<p>representationP(α) = nα|G|
&sum;
</p>
<p>g&isin;G
</p>
<p>nα&sum;
</p>
<p>i=1
T
(α)&lowast;
ii (g)
</p>
<p>︸ ︷︷ ︸
=χ (α)&lowast;(g)
</p>
<p>Tg =
nα
</p>
<p>|G|
&sum;
</p>
<p>g&isin;G
χ (α)&lowast;(g)Tg (24.40)
</p>
<p>and
</p>
<p>P(α)
∣∣ψ (β)
</p>
<p>&rang;
=
∣∣ψ (α)
</p>
<p>&rang;
δαβ , P
</p>
<p>(α)|φ〉 =
∣∣φ(α)
</p>
<p>&rang;
, (24.41)
</p>
<p>where |φ(α)〉 is the projection of |φ〉 onto the αth irreducible invariant sub-
space. These formulas are extremely useful in identifying the irreducible
subspaces of a given carrier space:
</p>
<p>Box 24.7.3 Start with a basis {|ai〉} of the carrier space, apply P(α)
of Eq. (24.40) to all basis vectors, and collect all the linearly inde-
pendent vectors of the form P(α)|ai〉. These vectors form a basis of the
αth irreducible representation.
</p>
<p>The following example illustrates this point.
</p>
<p>Example 24.7.4 Consider the representation of S3 given in Example 24.2.2,
where the carrier space is the span of the three functions |ψ1〉 = xy,
|ψ2〉 = yz, and |ψ3〉 = xz.
</p>
<p>We refer to the character table for S3 (Table 24.4) and use Eq. (24.40) to
obtain
</p>
<p>P(1) = 1
6
(T1 + T2 + T3 + T4 + T5 + T6),
</p>
<p>P(2) = 1
6
(T1 &minus; T2 &minus; T3 &minus; T4 + T5 + T6),</p>
<p/>
</div>
<div class="page"><p/>
<p>750 24 Representation of Groups
</p>
<p>P(3) = 2
6
(2T1 &minus; T5 &minus; T6),
</p>
<p>where, as in Example 24.2.2, we have used the notation Ti for Tπi , and the
result n1 = n2 = 1 and n3 = 2 obtained from Eq. (24.18), Theorem 24.5.3,
and the fact that S3 is nonabelian.
</p>
<p>To get the first irreducible subspace of this representation, we apply P(1)
</p>
<p>to |ψ1〉. Since this subspace is one-dimensional, the procedure will give a
basis for it if the vector so obtained is nonzero:
</p>
<p>P(1)|ψ1〉 =
1
</p>
<p>6
(T1 + T2 + T3 + T4 + T5 + T6)|ψ1〉
</p>
<p>= 1
6
</p>
<p>(
|ψ1〉 + |ψ1〉 + |ψ2〉 + |ψ3〉 + |ψ3〉 + |ψ2〉
</p>
<p>)
</p>
<p>= 1
3
</p>
<p>(
|ψ1〉 + |ψ2〉 + |ψ3〉
</p>
<p>)
.
</p>
<p>This is a basis for the carrier space of the irreducible identity representation.
For the second irreducible representation, we get
</p>
<p>P(2)|ψ1〉 =
1
</p>
<p>6
</p>
<p>(
|ψ1〉 &minus; |ψ1〉 &minus; |ψ2〉 &minus; |ψ3〉 + |ψ3〉 + |ψ2〉
</p>
<p>)
= 0.
</p>
<p>Similarly, P(2)|ψ2〉 = 0 and P(2)|ψ3〉 = 0. This means that T (2) is not in-
cluded in the representation we are working with. We should have ex-
pected this, because if this one-dimensional irreducible representation were
included, it would force the last irreducible representation to be one-
dimensional as well [see Eq. (24.18)], and, by Theorem 24.5.3, the group
S3 to be abelian!
</p>
<p>The last irreducible representation is obtained similarly. We have
</p>
<p>P(3)|ψ1〉 =
1
</p>
<p>3
(2T1 &minus; T5 &minus; T6)|ψ1〉 =
</p>
<p>1
</p>
<p>3
</p>
<p>(
2|ψ1〉 &minus; |ψ3〉 &minus; |ψ2〉
</p>
<p>)
,
</p>
<p>P(3)|ψ2〉 =
1
</p>
<p>3
(2T1 &minus; T5 &minus; T6)|ψ2〉 =
</p>
<p>1
</p>
<p>3
</p>
<p>(
2|ψ2〉 &minus; |ψ1〉 &minus; |ψ3〉
</p>
<p>)
.
</p>
<p>These two vectors are linearly independent. Therefore, they form a basis for
the last irreducible representation. The reader may check that P(3)|ψ3〉 is a
linear combination of P(3)|ψ1〉 and P(3)|ψ2〉.
</p>
<p>24.8 Tensor Product of Representations
</p>
<p>A simple quantum mechanical system possessing a group of symmetry is
described by vectors that transform irreducibly (or according to a row of
an irreducible representation). For example, a rotationally invariant system
can be described by an eigenstate of angular momentum, the generator of
rotation.3 These eigenstates transform as rows of irreducible representations
</p>
<p>3Chapter 29 will make explicit the connection between groups and their generators.</p>
<p/>
</div>
<div class="page"><p/>
<p>24.8 Tensor Product of Representations 751
</p>
<p>of the rotation group. At a more fundamental level, the very concept of a
particle or field is thought of as states that transform irreducibly under the
fundamental group of spacetime, the Poincar&eacute; group.
</p>
<p>Often these irreducible states are &ldquo;combined&rdquo; to form new states. For
example, the state of two (noninteracting) particles is described by a two-
particle state, labeled by the combined eigenvalues of the two sets of oper-
ators that describe each particle separately. In the case of angular momen-
tum, the single-particle states may be labeled as |li,mi〉 for i = 1,2. Then
the combined state will be labeled as |l1,m1; l2,m2〉, and one can define an
action of the rotation group on the vector space spanned by these combined
states to construct a representation. We now describe the way in which this
is done.
</p>
<p>Let T : G &rarr; GL(V) and S : G &rarr; GL(W) be two representations of a
group G. Define an action of the group G on V&otimes;W, the tensor product of
V and W, via the representation T &otimes; S :G&rarr;GL(V&otimes;W) given by
</p>
<p>(T &otimes; S)(g)
(
|v〉, |w〉
</p>
<p>)
=
(
T (g)|v〉, S(g)|w〉
</p>
<p>)
.
</p>
<p>We note that
</p>
<p>(T &otimes; S)(g1g2)
(
|v〉, |w〉
</p>
<p>)
</p>
<p>=
(
T (g1g2)|v〉, S(g1g2)|w〉
</p>
<p>)
=
(
T (g1)T (g2)|v〉, S(g1)S(g2)|w〉
</p>
<p>)
</p>
<p>= (T &otimes; S)(g1)
(
T (g2)|v〉, S(g2)|w〉
</p>
<p>)
</p>
<p>=
[
(T &otimes; S)(g1)(T &otimes; S)(g2)
</p>
<p>](
|v〉, |w〉
</p>
<p>)
.
</p>
<p>It follows that T &otimes;S is indeed a representation, called the tensor product or Kronecker product
representationdirect product or Kronecker product representation. It is common, espe-
</p>
<p>cially in the physics literature, to write |v,w〉, or simply |vw〉 for (|v〉, |w〉),
and T S for T &otimes; S. If we choose the orthonormal bases {|vi〉} for V and
{|wa〉} for W, and define an inner product on V&otimes;W by
</p>
<p>〈v,w|v&prime;,w&prime;〉 &equiv; 〈v|v&prime;〉〈w|w&prime;〉,
</p>
<p>we obtain a matrix representation of the group with matrix elements given
by
</p>
<p>(T &otimes; S)ia,jb(g)&equiv; 〈vi,wa|T &otimes; S(g)|vj ,wb〉
= 〈vi |T (g)|vj 〉〈wa|S(g)|wb〉 &equiv; Tij (g)Sab(g).
</p>
<p>Note that the rows and columns of this matrix are distinguished by double
indices. If the matrix T is m&times;m and S is n&times; n, then the matrix T &otimes; S is
(mn)&times; (mn). The character of the tensor product representation is character of a product
</p>
<p>representation is a
</p>
<p>product of charactersχ
T&otimes;S(g)=
</p>
<p>&sum;
</p>
<p>i,a
</p>
<p>(T &otimes; S)ia,ia(g)=
&sum;
</p>
<p>i,a
</p>
<p>Tii(g)Saa(g)=
&sum;
</p>
<p>i
</p>
<p>Tii(g)
&sum;
</p>
<p>a
</p>
<p>Saa(g)
</p>
<p>= χT (g)χS(g) &rArr; χT&otimes;Si = χTi &middot; χSi . (24.42)
</p>
<p>So the character of the tensor product is the product of the individual char-
acters.</p>
<p/>
</div>
<div class="page"><p/>
<p>752 24 Representation of Groups
</p>
<p>An important special case is the tensor product of a representation with
itself. For such a representation, the matrix elements satisfy the symmetry
relation
</p>
<p>(T &otimes; T )ia,jb(g)= (T &otimes; T )ai,bj (g).
This symmetry can be used to decompose the tensor product space into two
subspaces that are separately invariant under the action of the group. To
do this, take the span of all the symmetric vectors of the form (|viwj 〉 +
|vjwi〉) &isin; V&otimes; V and denote it by (V&otimes; V)s . Similarly, take the span of all
the antisymmetric vectors of the form (|viwj 〉&minus;|vjwi〉) &isin; V&otimes;V and denote
it by (V&otimes;V)a . Next note that
</p>
<p>|viwj 〉 =
1
</p>
<p>2
</p>
<p>(
|viwj 〉 + |vjwi〉
</p>
<p>)
+ 1
</p>
<p>2
</p>
<p>(
|viwj 〉 &minus; |vjwi〉
</p>
<p>)
.
</p>
<p>It follows that every vector of the product space can be written as the sum of
a symmetric and an antisymmetric vector. Furthermore, the only vector that
is both symmetric and antisymmetric is the zero vector. Therefore,
</p>
<p>V&otimes;V= (V&otimes;V)s &oplus; (V&otimes;V)a .
</p>
<p>Now consider the action of the group on each of these subspaces sepa-
rately. From the relation
</p>
<p>(T &otimes; T )(g)|viwj 〉 &equiv; (T &otimes; T )(g)
(
|vi〉, |wj 〉
</p>
<p>)
</p>
<p>=
(&sum;
</p>
<p>k
</p>
<p>Tki(g)|vk〉,
&sum;
</p>
<p>l
</p>
<p>Tlj (g)|wl〉
)
</p>
<p>=
&sum;
</p>
<p>k,l
</p>
<p>TkiTlj (g)(g)
(
|vk〉, |wl〉
</p>
<p>)
</p>
<p>=
&sum;
</p>
<p>k,l
</p>
<p>(T &otimes; T )kl,ij (g)|vkwl〉
</p>
<p>we obtain
</p>
<p>(T &otimes; T )(g)
(
|viwj 〉 &plusmn; |vjwi〉
</p>
<p>)
</p>
<p>=
&sum;
</p>
<p>k,l
</p>
<p>[
(T &otimes; T )kl,ij (g)&plusmn; (T &otimes; T )kl,j i(g)
</p>
<p>]
|vkwl〉. (24.43)
</p>
<p>Problem 24.21 shows that the RHS can be written as a sum over the symmet-Kronecker product
reduces to the
</p>
<p>symmetric and the
</p>
<p>antisymmetric
</p>
<p>representations
</p>
<p>ric (for the plus sign) or antisymmetric (for the minus sign) vectors alone. It
follows that
</p>
<p>Box 24.8.1 The Kronecker product of a representation with itself is
always reducible into two representations, the symmetrized product
and the antisymmetrized product representations.</p>
<p/>
</div>
<div class="page"><p/>
<p>24.8 Tensor Product of Representations 753
</p>
<p>24.8.1 Clebsch-Gordan Decomposition
</p>
<p>A common situation in quantum mechanics is to combine two simple sys-
tems into a composite system and see which properties of the original sim-
ple systems the composite system retains. For example, combining the an-
gular momenta of two particles gives a new total angular momentum op-
erator. The question of what single-particle angular momentum states are
included in the states of the total angular momentum operator is the con-
tent of selection rules and is of great physical interest: A quark and an selection rules
antiquark (two fermions) with spin 12 always combine to form a meson (a
boson), because the resulting composite state has no projection onto the
subspace spanned by half-integer-spin particles. In this section, we study
the mathematical foundation of this situation. The tensor product of two ir-
reducible representations T (α) and T (β) of G is denoted by T (α&times;β), and it is,
in general, a reducible representation. The characters, generally compound,
are denoted by χ (α&times;β). Equation (24.14), combined with Eq. (24.42), tells
us what irreducible representations are present in the tensor product, and
therefore onto which irreducible representations the product representation
has nonzero projection:
</p>
<p>χ
(α&times;β)
i = χ
</p>
<p>(α)
i &middot; χ
</p>
<p>(β)
i =
</p>
<p>r&sum;
</p>
<p>σ=1
mαβσ χ
</p>
<p>(σ )
i ,
</p>
<p>where mαβσ are nonnegative integers. We rewrite this more conveniently in
terms of vectors as
</p>
<p>∣∣χ (α&times;β)
&rang;
=
</p>
<p>r&sum;
</p>
<p>σ=1
mαβσ
</p>
<p>∣∣χ (σ )
&rang;
,
</p>
<p>mαβσ =
1
</p>
<p>|G|
&lang;
χ (σ )
</p>
<p>∣∣χ (α&times;β)
&rang;
= 1|G|
</p>
<p>r&sum;
</p>
<p>i=1
ci χ̄
</p>
<p>(σ )
i χ
</p>
<p>(α)
i χ
</p>
<p>(β)
i . (24.44)
</p>
<p>A group for which mαβσ = 0,1 is called simply reducible. simply reducible group
</p>
<p>Historical Notes
</p>
<p>Rudolph Friedrich Alfred Clebsch (1833&ndash;1872) studied mathematics in the shadow
</p>
<p>Rudolph Friedrich Alfred
</p>
<p>Clebsch 1833&ndash;1872
</p>
<p>of Jacobi at the University of K&ouml;nigsberg, two of his teachers having been students of
Jacobi. After graduation he held a number of positions in Germany, including positions
at the universities of Berlin, Giessen, and finally G&ouml;ttingen, where he remained until his
death. He and Carl Neumann, the son of one of the aforementioned teachers who were
students of Jacobi, founded the Mathematische Annalen.
Clebsch began his career in mathematical physics, producing a doctoral thesis on hydro-
dynamics and a book on elasticity in which he treated the elastic vibrations of rods and
plates. These works were primarily mathematical, however, and he soon turned his atten-
tion more to pure mathematics. His links to Jacobi gave rise to his first work in that vein,
concerning problems in variational calculus and partial differential equations, in which
he surpassed the results of Jacobi&rsquo;s work.
Clebsch first achieved significant recognition for his work in projective invariants and
algebraic geometry. He was intrigued by the interplay between algebra and geometry,
and, since many results in the theory of invariants have geometric interpretations, the two
fields seemed natural choices.</p>
<p/>
</div>
<div class="page"><p/>
<p>754 24 Representation of Groups
</p>
<p>Example 24.8.2 Referring to Table 24.5 of Problem 24.15, and using
Eq. (24.42), we can construct the compound character |χ (4&times;5)〉 with com-
ponents 9, &minus;1, 1, 0, &minus;1. Then, we have
</p>
<p>∣∣χ (4&times;5)
&rang;
=
</p>
<p>⎛
⎜⎜⎜⎜⎝
</p>
<p>9
&minus;1
1
0
&minus;1
</p>
<p>⎞
⎟⎟⎟⎟⎠
</p>
<p>=
5&sum;
</p>
<p>σ=1
m45σ
</p>
<p>∣∣χ (σ )
&rang;
, m45σ =
</p>
<p>1
</p>
<p>24
</p>
<p>&lang;
χ (σ )
</p>
<p>∣∣χ (4&times;5)
&rang;
.
</p>
<p>For the first irreducible representation, we get
</p>
<p>m451 =
1
</p>
<p>24
</p>
<p>&lang;
χ (1)
</p>
<p>∣∣χ (4&times;5)
&rang;
= 1
</p>
<p>24
</p>
<p>5&sum;
</p>
<p>i=1
ciχ
</p>
<p>(1)&lowast;
i χ
</p>
<p>(4&times;5)
i
</p>
<p>= 1
24
</p>
<p>[
1 &middot; 1 &middot; 9 + 6 &middot; 1 &middot; (&minus;1)+ 3 &middot; 1 &middot; 1 + 8 &middot; 1 &middot; 0 + 6 &middot; 1 &middot; (&minus;1)
</p>
<p>]
= 0.
</p>
<p>For the second irreducible representation, we get
</p>
<p>m452 =
1
</p>
<p>24
</p>
<p>&lang;
χ (2)
</p>
<p>∣∣χ (4&times;5)
&rang;
</p>
<p>= 1
24
</p>
<p>[
1 &middot; 1 &middot; 9 + 6 &middot; (&minus;1) &middot; (&minus;1)+ 3 &middot; 1 &middot; 1 + 8 &middot; (&minus;1) &middot; 0
</p>
<p>+ 6 &middot; (&minus;1) &middot; (&minus;1)
]
= 1.
</p>
<p>Similarly, m453 = 1, m454 = 1, and m455 = 1. We thus see that the identity
representation is not included in the direct product of irreducible represen-
tations 4 and 5; all other irreducible representations of S4 occur once in
T (4&times;5).
</p>
<p>In terms of representations themselves, we have the so-called Clebsch-Clebsch-Gordan series
Gordan series
</p>
<p>T (α&times;β)(g)=
r&sum;
</p>
<p>σ=1
mαβσ T
</p>
<p>(σ )(g), mαβσ =
1
</p>
<p>|G|
</p>
<p>r&sum;
</p>
<p>i=1
ci χ̄
</p>
<p>(σ )
i χ
</p>
<p>(α)
i χ
</p>
<p>(β)
i ,
</p>
<p>(24.45)
where we have used Eq. (24.13)
</p>
<p>The one-dimensional identity representation plays a special role in theHow to obtain invariants
from the product of
</p>
<p>representations
</p>
<p>application of group theory to physics because any vector (function) in its
carrier space is invariant under the action of the group, and invariant vectors
often describe special states of the quantum mechanical systems. For exam-
ple, the ground state of an atomic system with rotational invariance has zero
orbital angular momentum, corresponding to a spherically symmetric state.
</p>
<p>Another example comes from particle physics. Quarks are usually placed
in the states of an irreducible representation of a group [SU(n), where n is
the number of &ldquo;flavors&rdquo; such as up, down, charm], and antiquarks in its
adjoint. A question of great importance is what combination of quarks and
antiquarks leads to particles&mdash;called singlets&mdash;that are an invariant of the</p>
<p/>
</div>
<div class="page"><p/>
<p>24.8 Tensor Product of Representations 755
</p>
<p>group. For the case of quark-antiquark combination, the answer comes in
the analysis of the tensor product of one irreducible representation, say T (α),
and one adjoint representation, say T̄ (β). In fact, using Eq. (24.45), we have
</p>
<p>m
αβ
</p>
<p>1 =
1
</p>
<p>|G|
</p>
<p>r&sum;
</p>
<p>i=1
ci χ̄
</p>
<p>(1)
i χ
</p>
<p>(α)
i χ̄
</p>
<p>(β)
i =
</p>
<p>1
</p>
<p>|G|
</p>
<p>r&sum;
</p>
<p>i=1
ciχ
</p>
<p>(α)
i χ̄
</p>
<p>(β)
i = δαβ ,
</p>
<p>where we used Eq. (24.13) and the fact that all characters of the identity
representation are unity. Thus
</p>
<p>Box 24.8.3 To construct an invariant state, we need to combine a
representation with its adjoint, in which case we obtain the identity
representation only once.
</p>
<p>Historical Notes
</p>
<p>Paul Albert Gordan (1837&ndash;1912), the son of David Gordan, a merchant, attended gym-
</p>
<p>Paul Albert Gordan
</p>
<p>1837&ndash;1912
</p>
<p>nasium and business school, then worked for several years in banks. His early interest in
mathematics was encouraged by the private tutoring he received from a professor at the
Friedrich Wilhelm Gymnasium. He attended Ernst Kummer&rsquo;s lectures in number theory
at the University of Berlin in 1855, then studied at the universities of Breslau, K&ouml;nigs-
berg, and Berlin. At K&ouml;nigsberg he came under the influence of Karl Jacobi&rsquo;s school, and
at Berlin his interest in algebraic equations was aroused. His dissertation (1862), which
concerned geodesics on spheroids, received a prize offered by the philosophy faculty of
the University of Breslau. The techniques that Gordan employed in it were those of La-
grange and Jacobi.
Gordan&rsquo;s interest in function theory led him to visit G.F.B. Riemann in G&ouml;ttingen in 1862,
but Riemann was ailing, and their association was brief. The following year, Gordan was
invited to Giessen by Clebsch, thus beginning the fruitful collaboration most physicists
recognize. Together they produced work on the theory of Abelian functions, based on Rie-
mann&rsquo;s fundamental paper on that topic, and several of Clebsch&rsquo;s papers are considered
important steps toward establishing for Riemann&rsquo;s theories a firm foundation in terms
of pure algebraic geometry. Of course, the Clebsch-Gordan collaboration also produced
the famous coefficients that bear their names, so indispensable to the theory of angular
momentum coupling found in almost every area of modern physics.
In 1874 Gordan became a professor at Erlangen, where he remained until his retirement
in 1910. He married Sophie Deuer, the daughter of a Giessen professor of Roman law, in
1869. In 1868 Clebsch introduced Gordan to the theory of invariants, which originated in
an observation of George Boole&rsquo;s in 1841 and was further developed by Arthur Cayley
in 1846. Following the work of these two Englishmen, a German branch of the theory
was developed by S.H. Aronhold and Clebsch, the latter elaborating the former&rsquo;s sym-
bolic methods of characterizing algebraic forms and their invariants. Invariant theory was
Gordan&rsquo;s main interest for the rest of his mathematical career; he became known as the
greatest expert in the field, developing many techniques for representing and generating
forms and their invariants.
Gordan made important contributions to algebra and solutions of algebraic equations,
and gave simplified proofs of the transcendence of e and π . The overall style of Gordan&rsquo;s
mathematical work was algorithmic. He shied away from presenting his ideas in informal
literary forms. He derived his results computationally, working directly toward the desired
goal without offering explanations of the concepts that motivated his work.
Gordan&rsquo;s only doctoral student, Emmy Noether, was one of the first women to receive a
doctorate in Germany. She carried on his work in invariant theory for a while, but under
the stimulus of Hilbert&rsquo;s school at G&ouml;ttingen her interests shifted and she became one of
the primary contributors to modern algebra.</p>
<p/>
</div>
<div class="page"><p/>
<p>756 24 Representation of Groups
</p>
<p>So far, we have concentrated on the reduction of the operators and car-
rier spaces into irreducible components. Let us now direct our attention to
the vectors themselves. Given two irreducible representations T (α) and T (β)
</p>
<p>with carrier spaces spanned by vectors {|φ(α)i 〉}
nα
i=1 and {|ψ
</p>
<p>(β)
j 〉}
</p>
<p>nβ
j=1, we form
</p>
<p>the direct product representation T (α&times;β) with the carrier space spanned by
vectors {|φ(α)i ψ
</p>
<p>(β)
j 〉}. We know that T (α&times;β) is reducible, and Eq. (24.45)
</p>
<p>tells us how many times each irreducible factor occurs in T (α&times;β). This
means that the span of {|φ(α)i ψ
</p>
<p>(β)
j 〉} can be decomposed into invariant irre-
</p>
<p>ducible subspaces; i.e., there must exist a basis of the carrier of the product
space the vectors of which belong to irreducible representations of G. More
specifically, we should be able to form the linear combinationsClebsch-Gordan
</p>
<p>coefficients ∣∣Ψ (σ ),qk
&rang;
=
&sum;
</p>
<p>ij
</p>
<p>C(αβ;σ,q|ij ; k)
∣∣φ(α)i ψ
</p>
<p>(β)
j
</p>
<p>&rang;
, (24.46)
</p>
<p>which transform according to the rows of the σ th irreducible representation.
Here the subscript k refers to the row of the σ th representation, and q dis-
tinguishes among functions that have the same σ and k, corresponding to
the case where mαβσ &ge; 2. For simply reducible groups, the label q is unnec-
essary. The coefficients C(αβ;σ,q|ij ; k) are called the Clebsch-Gordan
coefficients for G. These coefficients are normalized such that
</p>
<p>&sum;
</p>
<p>ij
</p>
<p>C&lowast;(αβ;σ,q|ij ; k)C
(
αβ;σ &prime;, q &prime;|ij ; k&prime;
</p>
<p>)
= δσσ &prime;δqq &prime;δkk&prime; ,
</p>
<p>&sum;
</p>
<p>σqk
</p>
<p>C&lowast;(αβ;σ,q|ij ; k)C
(
αβ;σ,q|i&prime;j &prime;; k
</p>
<p>)
= δii&prime;δjj &prime; .
</p>
<p>This will guarantee that |Ψ (σ ),qk 〉 are orthonormal if the product vectors
form an orthonormal set. Using these relations, we can write the inverse
of Eq. (24.46) as
</p>
<p>∣∣φ(α)i ψ
(β)
j
</p>
<p>&rang;
=
&sum;
</p>
<p>σqk
</p>
<p>C&lowast;(αβ;σ,q|ij ; k)
∣∣Ψ (σ ),qk
</p>
<p>&rang;
. (24.47)
</p>
<p>24.8.2 Irreducible Tensor Operators
</p>
<p>An operator A acting in the carrier space of the representation of a group
G is transformed into another operator, A �&rarr; TgAT&minus;1g , by the action of the
group. Just as in the case of vector spaces, one can thus construct a set of
operators that transform among themselves by such action and lump these
operators in irreducible sets.
</p>
<p>Definition 24.8.4 An operator A(α)i is said to transform according to theirreducible set of
operators ith row of the αth irreducible representation if there exist nα &minus; 1 other
</p>
<p>operators {A(α)j } and a basis {|ψ
(α)
i 〉} such that</p>
<p/>
</div>
<div class="page"><p/>
<p>24.8 Tensor Product of Representations 757
</p>
<p>TgA
(α)
i T
</p>
<p>&minus;1
g =
</p>
<p>nα&sum;
</p>
<p>j=1
T
(α)
ji (g)A
</p>
<p>(α)
j , (24.48)
</p>
<p>where (T (α)ji (g)) is the matrix representation of g. The set of such operators
is called an irreducible set of operators (or irreducible tensorial set).
</p>
<p>In particular, if T (α)ij (g)= δij , i.e., if the representation is the identity rep-
resentation, then A= TgAT&minus;1g , and A is called a scalar operator. The term scalar operator
&ldquo;scalar&rdquo; refers to the fact that A has only one &ldquo;component,&rdquo; in contrast to
the other operators of Eq. (24.48), which may possess several components.
</p>
<p>Consider the set of vectors (functions) defined by |ψ (αβ)ij 〉 &equiv; A
(α)
i |φ
</p>
<p>(β)
j 〉,
</p>
<p>where |φ(β)j 〉 transform according to the βth irreducible representation.
These vectors transform according to
</p>
<p>Tg
∣∣ψ (αβ)ij
</p>
<p>&rang;
= TgA(α)i T&minus;1g Tg
</p>
<p>∣∣φ(β)j
&rang;
=
</p>
<p>nα&sum;
</p>
<p>k=1
T
(α)
ki (g)A
</p>
<p>(α)
k
</p>
<p>nβ&sum;
</p>
<p>l=1
T
(β)
lj (g)
</p>
<p>∣∣φ(β)l
&rang;
</p>
<p>=
&sum;
</p>
<p>k,l
</p>
<p>T
(α)
ki (g)T
</p>
<p>(β)
lj (g)A
</p>
<p>(α)
k
</p>
<p>∣∣φ(β)l
&rang;
=
&sum;
</p>
<p>k,l
</p>
<p>T
(α&times;β)
kl,ij (g)
</p>
<p>∣∣ψ (αβ)kl
&rang;
,
</p>
<p>(24.49)
</p>
<p>i.e., according to the representation T (α&times;β). This means that the vectors
|ψ (αβ)ij 〉 have the same transformation properties as the tensor product vec-
tors |φ(α)i ψ
</p>
<p>(β)
j 〉. Therefore, using Eq. (24.47), we can write
</p>
<p>A
(α)
i
</p>
<p>∣∣φ(β)j
&rang;
=
&sum;
</p>
<p>σqk
</p>
<p>C&lowast;(αβ;σ,q|ij ; k)
∣∣Ψ (σ ),qk
</p>
<p>&rang;
,
</p>
<p>and more importantly,
</p>
<p>&lang;
φ
(γ )
m
</p>
<p>∣∣A(α)i
∣∣φ(β)j
</p>
<p>&rang;
=
&sum;
</p>
<p>σqk
</p>
<p>C&lowast;(αβ;σ,q|ij ; k)
&lang;
φ
(γ )
m
</p>
<p>∣∣Ψ (σ ),qk
&rang;
</p>
<p>︸ ︷︷ ︸
use Eq. (24.36) here
</p>
<p>=
&sum;
</p>
<p>q
</p>
<p>C&lowast;(αβ;γ, q|ij ;m)
&lang;
φ
(γ )
m
</p>
<p>∣∣Ψ (γ ),qm
&rang;
. (24.50)
</p>
<p>It follows that the matrix element of the operator A(α)i will vanish unless the
irreducible representation T (γ ) occurs in the reduction of the tensor prod-
uct T (α) &otimes; T (β), and this can be decided from the character tables and the
Clebsch-Gordan series, Eq. (24.45).
</p>
<p>There is another remarkable property of Eq. (24.50) that has significant
physical consequences. Notice how the dependence on i and j is contained
entirely in the Clebsch-Gordan coefficients. Moreover, Eq. (24.37) implies Wigner&ndash;Eckart theorem
</p>
<p>and reduced matrix
</p>
<p>elements
</p>
<p>that 〈φ(γ )m |Ψ (γ ),qm 〉 is independent of m. Therefore, this dependence must
also be contained entirely in Clebsch-Gordan coefficients. One therefore</p>
<p/>
</div>
<div class="page"><p/>
<p>758 24 Representation of Groups
</p>
<p>writes (24.50) as
&lang;
φ
(γ )
m
</p>
<p>∣∣A(α)i
∣∣φ(β)j
</p>
<p>&rang;
&equiv;
&sum;
</p>
<p>q
</p>
<p>C&lowast;(αβ;γ, q|ij ;m)
&lang;
φ(γ )
</p>
<p>∥∥A(α)
∥∥φ(β)
</p>
<p>&rang;
q︸ ︷︷ ︸
</p>
<p>reduced matrix element
</p>
<p>. (24.51)
</p>
<p>This equation is known as the Wigner-Eckart theorem, and the numbers
multiplying the Clebsch-Gordan coefficients are known as the reduced ma-
trix elements.
</p>
<p>From the point of view of physics, Eq. (24.51) can be very useful in cal-
culating matrix elements (expectation values and transition between states),
once we know the transformation properties of the physical operator. For ex-
ample, for a scalar operator S, which, by definition, transforms according
to the identity representation, (24.51) becomes
</p>
<p>&lang;
φ
(γ )
m
</p>
<p>∣∣A
∣∣φ(β)j
</p>
<p>&rang;
=
&lang;
φ(γ )
</p>
<p>∥∥A(α)
∥∥φ(β)
</p>
<p>&rang;
δγβδmj ;
</p>
<p>i.e., scalar operators have no matrix elements between different irreducible
representations of a group, and within an irreducible representation, they are
multiples of the identity matrix. This result is also a consequence of Schur&rsquo;s
lemma.
</p>
<p>24.9 Problems
</p>
<p>24.1 Show that the action of a group G on the space of functions ψ given
by Tgψ(x)=ψ(g&minus;1 &middot; x) is a representation of G.
</p>
<p>24.2 Complete Example 24.1.6.
</p>
<p>24.3 Let the vector space carrying the representation of S3 be the space of
functions. Choose ψ1(x, y, z)&equiv; xy and find the matrix representation of S3
in the minimal invariant subspace containing ψ1. Hint: See Example 24.2.2.
</p>
<p>24.4 Let the vector space carrying the representation of S3 be the space of
functions. Choose (a) ψ1(x, y, z)&equiv; x and (b) ψ1(x, y, z)&equiv; x2, and in each
case, find the matrix representation of S3 in the minimal invariant subspace
containing ψ1.
</p>
<p>24.5 Show that the representations T , T , and T &lowast; are either all reducible or
all irreducible.
</p>
<p>24.6 Use the hermitian conjugate of Eq. (24.5) to show that S&equiv; A&dagger;A com-
mutes with all Tg&rsquo;s. This result is used to prove Schur&rsquo;s lemmas in infinite
dimensions.
</p>
<p>24.7 Show that elements of a group belonging to the same conjugacy class
have the same characters.
</p>
<p>24.8 Show that the regular representation is indeed a representation, i.e.,
that R :G&rarr;GL(m,R) is a homomorphism.</p>
<p/>
</div>
<div class="page"><p/>
<p>24.9 Problems 759
</p>
<p>Table 24.5 Character table for S4
</p>
<p>1K1
6K2
</p>
<p>3K3
8K4
</p>
<p>6K5
</p>
<p>T (1) 1 1 1 1 1
</p>
<p>T (2) 1 &minus;1 1 1 &minus;1
T (3) 2 0 2 &minus;1 0
T (4) 3 1 &minus;1 0 &minus;1
T (5) 3 &minus;1 &minus;1 0 1
</p>
<p>24.9 Prove Maschke&rsquo;s Theorem: The group algebra is semi-simple.
</p>
<p>24.10 Let G be a finite group. Define the element P =&sum;x&isin;G x of the group
algebra and show that the left ideal generated by P is one-dimensional.
</p>
<p>24.11 Show that T(α)i defined in Eq. (24.23) commutes with all operators
</p>
<p>T
(α)
g . Hint: Consider T
</p>
<p>(α)
g T
</p>
<p>(α)
i (T
</p>
<p>(α)
g )
</p>
<p>&minus;1.
</p>
<p>24.12 Let Ki&prime; denote the set of inverses of a conjugacy class Ki with ci
elements.
</p>
<p>(a) Show that Ki&prime; is also a class with ci elements.
(b) Show that identity occurs exactly ci times in the product κiκi&prime; , and
</p>
<p>none in the product κiκj if j �= i&prime; [see Eq. (24.21)].
(c) Conclude that
</p>
<p>cij1 =
{
</p>
<p>0 if j �= i&prime;,
ci if j = i&prime;.
</p>
<p>24.13 Show that the coefficients |G|dj/|H |ci of Eq. (24.33) are integers.
</p>
<p>24.14 Show that the symmetric and the antisymmetric representations of Sn
are inequivalent.
</p>
<p>24.15 Construct the character table for S4 from that of S3 (given as Ta-
ble 24.4), and verify that it is given by Table 24.5.
</p>
<p>24.16 Show that all functions transforming according to a given row of an
irreducible representation have the same norm.
</p>
<p>24.17 Show that if the group of symmetry of V contains that of H0 and
|φ(α)j 〉 belongs to the j th column of the αth irreducible representation, then
so does V|φ(α)j 〉. Conclude that 〈φ
</p>
<p>(α)
i |V|φ
</p>
<p>(α)
j 〉 = 0 for i �= j .
</p>
<p>24.18 Find the irreducible components of the representation of Exam-
ple 24.1.6.
</p>
<p>24.19 Show that P(3)|ψ3〉 of Example 24.7.4 is a linear combination of
P(3)|ψ1〉 and P(3)|ψ2〉.</p>
<p/>
</div>
<div class="page"><p/>
<p>760 24 Representation of Groups
</p>
<p>24.20 Show that the tensor product of two unitary representations is unitary.
</p>
<p>24.21 Switch the dummy indices of the double sum in (24.43), add (sub-
tract) the two sums, and use (T &otimes;T )ia,jb(g)= (T &otimes;T )ai,bj (g) to show that
the double sum can be written as a sum over the symmetric (antisymmetric)
vectors alone.
</p>
<p>24.22 Show that the characters χS(g) and χA(g) of the symmetrized and
antisymmetrized product representations are given, respectively, by
</p>
<p>χS(g)= 1
2
</p>
<p>[(
χ(g)
</p>
<p>)2 + χ
(
g2
)]
</p>
<p>and χA(g)= 1
2
</p>
<p>[(
χ(g)
</p>
<p>)2 &minus; χ
(
g2
)]
.
</p>
<p>24.23 Suppose that A(α)i transforms according to T
(α), and A(β)j according
</p>
<p>to T (β). Show that A(α)i A
(β)
j transforms according to T
</p>
<p>(α&times;β).
</p>
<p>24.24 Show that
</p>
<p>1
</p>
<p>m
αβ
γ
</p>
<p>&sum;
</p>
<p>ij
</p>
<p>∣∣&lang;φ(γ )m
∣∣A(α)i
</p>
<p>∣∣φ(β)j
&rang;∣∣2 =
</p>
<p>∣∣&lang;φ(γ )
∥∥A(α)
</p>
<p>∥∥φ(β)
&rang;
q
</p>
<p>∣∣2.
</p>
<p>One can interpret this as the statement that the square of the reduced matrix
element is proportional to the average (over i and j ) of the square of the full
matrix elements.</p>
<p/>
</div>
<div class="page"><p/>
<p>25Representations of the SymmetricGroup
</p>
<p>The symmetric (permutation) group is an important prototype of finite
groups. In fact, Cayley&rsquo;s theorem (see [Rotm 84, p. 46] for a proof) states Cayley&rsquo;s theorem
that any finite group of order n is isomorphic to a subgroup of Sn. Moreover,
the representation of Sn leads directly to the representation of many of the
Lie groups encountered in physical applications. It is, therefore worthwhile
to devote some time to the analysis of the representations of Sn.
</p>
<p>25.1 Analytic Construction
</p>
<p>The starting point of the construction of representations of the symmetric
group is Eq. (24.33), which is valid for any finite group. There is one simple
character that every group has, namely, the character of the one-dimensional
symmetric representation in which all elements of the group are mapped to
1 &isin;R. Setting ξ (κ)j = 1 in (24.33), and noting that
</p>
<p>&sum;
j dj = di , we obtain
</p>
<p>ψHi &equiv;
|G|di
|H |ci
</p>
<p>, (25.1)
</p>
<p>where {ψHi } are the components of a compound character of G.
Frobenius has shown that by a clever choice of H , one can completely
</p>
<p>solve the problem of the construction of the irreducible representations
of Sn. The interested reader may refer to [Hame 89, pp. 189&ndash;192] for de-
tails. We are really interested in the simple characters of Sn, and Frobenius
came up with a powerful method of calculating them. Since there is a one-to-
one correspondence between the irreducible representations and conjugacy
classes, and another one between conjugacy classes of Sn and partitions of n,
we shall label the simple characters of Sn by partitions of n. Thus, instead of
our common notation χ (α)i , we use χ
</p>
<p>(λ)
(l) , where (λ) denotes a partition of n,
</p>
<p>and (l) a cycle structure of Sn.
Suppose we want to find the irreducible characters corresponding to the
</p>
<p>cycle structure (l)= (1α,2β ,3γ , . . . ). These form a column under the class
(l) in a character table. To calculate the irreducible characters, form two
polynomials in (x1, x2, . . . , xn) as follows. The first one, which is com-
</p>
<p>S. Hassani, Mathematical Physics, DOI 10.1007/978-3-319-01195-0_25,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>761</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-01195-0_25">http://dx.doi.org/10.1007/978-3-319-01195-0_25</a></div>
</div>
<div class="page"><p/>
<p>762 25 Representations of the Symmetric Group
</p>
<p>pletely symmetric in all variables, is
</p>
<p>s(l) &equiv;
(
</p>
<p>n&sum;
</p>
<p>i=1
xi
</p>
<p>)α( n&sum;
</p>
<p>i=1
x2i
</p>
<p>)β( n&sum;
</p>
<p>i=1
x3i
</p>
<p>)γ
&middot; &middot; &middot; . (25.2)
</p>
<p>The second one is completely antisymmetric, and can be written as
</p>
<p>D(x1, . . . , xn)&equiv;
&prod;
</p>
<p>i&lt;j
</p>
<p>(xi &minus; xj )=
&sum;
</p>
<p>π
</p>
<p>ǫπx
n&minus;1
π(1)x
</p>
<p>n&minus;2
π(2) &middot; &middot; &middot;xπ(n&minus;1)x0π(n). (25.3)
</p>
<p>It can be shown that the simple characters of Sn are coefficients of certain
terms of the product of these polynomials. To be exact, we have
</p>
<p>s(l)D(x1, . . . , xn)
</p>
<p>=
&sum;
</p>
<p>(λ)
</p>
<p>χ
(λ)
(l)
</p>
<p>&sum;
</p>
<p>π
</p>
<p>ǫπx
λ1+n&minus;1
π(1) x
</p>
<p>λ2+n&minus;2
π(2) . . . x
</p>
<p>λn&minus;1+1
π(n&minus;1) x
</p>
<p>λn
π(n). (25.4)
</p>
<p>The outer sum goes over all partitions of n, the inner sum over all permu-
tations of Sn. The procedure for finding the simple characters of Sn should
now be clear from (25.4):
</p>
<p>Proposition 25.1.1 To find the simple character χ (λ1...λn)
(1α2β ... )
</p>
<p>, con-
struct the corresponding symmetric and antisymmetric polynomials
of (25.2) and (25.3), multiply them together, collect all terms of the
form
</p>
<p>x
λ1+n&minus;1
1 x
</p>
<p>λ2+n&minus;2
2 &middot; &middot; &middot;x
</p>
<p>λn&minus;1+1
n&minus;1 x
</p>
<p>λn
n .
</p>
<p>The coefficient of such a term is the desired character.
</p>
<p>Example 25.1.2 The best way to understand the procedure described above
is to go through an example in detail. We calculate the characters of S3 using
the above method. Label the rows of the character table with the partitions of
3. These are (3), (2,1), and (1,1,1). Similarly, label the columns with the
conjugacy classes, or cycle structures: (13), (1,2), and (3). The first cycle
structure has α = 3, β = 0 = γ . Therefore,
</p>
<p>s(13) = (x1 + x2 + x3)3 = x31 + x32 + x33
+ 3
</p>
<p>(
x21x2 + x21x3 + x1x22 + x22x3 + x1x23 + x2x23
</p>
<p>)
+ 6x1x2x3
</p>
<p>(25.5)
</p>
<p>and
</p>
<p>D(x1, x2, x3)= (x1 &minus; x2)(x1 &minus; x3)(x2 &minus; x3)
</p>
<p>= x21x2 &minus; x21x3 &minus; x22x1 + x22x3 &minus; x23x2 + x23x1. (25.6)</p>
<p/>
</div>
<div class="page"><p/>
<p>25.1 Analytic Construction 763
</p>
<p>Now we note that for (λ) = (3), λ1 = 3, λ2 = 0, and λ3 = 0. There-
fore, the coefficient of xλ1+n&minus;11 x
</p>
<p>λ2+n&minus;2
2 &middot; &middot; &middot;x
</p>
<p>λn
n = x51x2 gives χ
</p>
<p>(3)
(13)
</p>
<p>. Simi-
larly, for (λ) = (2,1,0), λ1 = 2, λ2 = 1, and λ3 = 0, and the coefficient
of xλ1+n&minus;11 x
</p>
<p>λ2+n&minus;2
2 &middot; &middot; &middot;x
</p>
<p>λn
n = x41x22 gives χ
</p>
<p>(2,1)
(13)
</p>
<p>. Finally, for (λ)= (1,1,1),
λ1 = λ2 = λ3 = 1, and the coefficient of xλ1+n&minus;11 x
</p>
<p>λ2+n&minus;2
2 &middot; &middot; &middot;x
</p>
<p>λn
n = x31x22x3
</p>
<p>gives χ (1,1,1)
(13)
</p>
<p>. These coefficients can be read off by scanning through
Eq. (25.5) while multiplying its terms by those of Eq. (25.6) and keeping
track of the coefficients of the products of the relevant powers of x1, x2,
and x3. The reader may verify that there is only one term of the form x51x2,
</p>
<p>whose coefficient is 1, giving χ (3)
(13)
</p>
<p>= 1; there are two terms of the form
x41x
</p>
<p>2
2 , whose coefficients are &minus;1 and 3, giving χ
</p>
<p>(2,1)
(13)
</p>
<p>= 2; and there are four
terms of the form x31x
</p>
<p>2
2x3, whose coefficients are +1, &minus;3, &minus;3, and +6,
</p>
<p>giving χ (1,1,1)
(13)
</p>
<p>= 1. Therefore, the first column of the character table of S3
is
( 1
</p>
<p>2
1
</p>
<p>)
.
</p>
<p>To obtain the second column, we consider the second conjugacy class,
(1,2), with α = 1 = β and γ = 0. The corresponding symmetric polynomial
is
</p>
<p>s(1,2) = (x1 + x2 + x3)
(
x21 + x22 + x23
</p>
<p>)
</p>
<p>= x31 + x32 + x33 + x21x2 + x21x3 + x1x22 + x2x23 + x1x23 + x2x23 .
(25.7)
</p>
<p>D(x1, x2, x3) is the same as before. Multiplying and keeping track of the
coefficients of x51x2, x
</p>
<p>4
1x
</p>
<p>2
2 , and x
</p>
<p>3
1x
</p>
<p>2
2x3, we obtain χ
</p>
<p>(3)
(1,2) = 1, χ
</p>
<p>(2,1)
(1,2) = 0,
</p>
<p>and χ (1,1,1)(1,2) =&minus;1. It follows that the second column of the character table
of S3 is
</p>
<p>( 1
0
&minus;1
</p>
<p>)
.
</p>
<p>The last column is obtained similarly. We note that α = 0 = β , and γ = 1.
Therefore, the symmetric polynomial is
</p>
<p>s(3) = x31 + x32 + x33 ,
</p>
<p>and the antisymmetric polynomial is the same as before. Multiplying these
two polynomials and extracting the coefficients as before, we get χ (3)(3) = 1,
χ
(2,1)
(3) =&minus;1, and χ
</p>
<p>(1,1,1)
(3) = 1. It follows that the third column of the charac-
</p>
<p>ter table of S3 is
( 1
&minus;1
1
</p>
<p>)
.
</p>
<p>Collecting all the data obtained above, we can reconstruct the character
table of S3. This is shown in Table 25.1. The irreducible representations are
labeled by the three possible partitions of 3, and the conjugacy classes by
the three cycle structures.</p>
<p/>
</div>
<div class="page"><p/>
<p>764 25 Representations of the Symmetric Group
</p>
<p>Table 25.1 The character table for S3. Each column corresponds to a conjugacy class,
each row to a partition of 3. The last two rows have been switched compared to Table 24.4
</p>
<p>(13) (1,2) (3)
</p>
<p>T (3) 1 1 1
</p>
<p>T (2,1) 2 0 &minus;1
T (1,1,1) 1 &minus;1 1
</p>
<p>25.2 Graphical Construction
</p>
<p>The analytic construction of the previous subsection can be handled using
graphical techniques that are considerably simpler. To begin with, let us find
the character of the identity element of Sn. The cycle structure is (1n), i.e.,
all cycles consist of a single element. Thus, α = n, and β , γ , etc. are all
zero. It follows that the LHS of Eq. (25.4) is (
</p>
<p>&sum;
xi)
</p>
<p>nD(xj ). We calculate
this product one power of
</p>
<p>&sum;
xi at a time. For the same reason as in the
</p>
<p>example above, χ (λ)(1n) will be the coefficient of
</p>
<p>x
λ1+n&minus;1
1 x
</p>
<p>λ2+n&minus;2
2 . . . x
</p>
<p>λn&minus;1+1
n&minus;1 x
</p>
<p>λn
n .
</p>
<p>Historical Notes
</p>
<p>Ferdinand Georg Frobenius (1849&ndash;1917), the son of a parson, was born in Berlin and
</p>
<p>Ferdinand Georg
</p>
<p>Frobenius 1849&ndash;1917
</p>
<p>began his mathematical studies at G&ouml;ttingen in 1867. He received his doctorate in Berlin
three years later. Four years later, on the basis of his mathematical research, he was ap-
pointed assistant professor at the University of Berlin. He achieved the rank of full pro-
fessor at the Eidgen&ouml;ssische Polytechnikum Z&uuml;rich before returning to Berlin as a profes-
sor of mathematics in 1892. During the early years of Frobenius&rsquo;s career, modern group
theory was in its infancy. He combined its three main branches of study&mdash;the theory of
solutions to algebraic equations (permutation groups and the work of Galois), geometry
(transformation and Lie groups), and number theory&mdash;to produce the concept of the ab-
stract group. He collaborated with Issai Schur in representation and character theory of
groups.
His paper &Uuml;ber die Gruppencharactere is of fundamental importance. It was presented to
the Berlin Academy on 16 July 1896 and it contains work that Frobenius had done in the
preceding few months. In a series of letters to Dedekind, the first on 12 April 1896, his
ideas on group characters quickly develop, and Frobenius is able to construct a complete
set of representations by complex numbers. In a letter to Dedekind on 26 April 1896
Frobenius finds the irreducible characters for the alternating group, and the symmetric
groups.
In 1897 Frobenius reformulated the work of Molien&mdash;the Latvian student of Klein, who,
in his thesis, classified the semi-simple algebras using the method of group rings&mdash;in
terms of matrices and then showed that his characters are the traces of the irreducible
representations. Frobenius&rsquo;s character theory found important applications in quantum
mechanics and was used with great effect by Burnside, who wrote it up in the 1911
edition of his Theory of Groups of Finite Order.
Frobenius is also remembered as the originator of a series method for solving ordinary
differential equations. Despite the clearly greater importance of his work in group theory,
this method of Frobenius serves admirably to perpetuate his name.
</p>
<p>If we multiply D(xj ) by
&sum;
</p>
<p>xi one x at a time, we increase the power of
one of the xi &rsquo;s by one. If at any stage, two of the exponents become equal,
the term must vanish, due to the antisymmetry of (
</p>
<p>&sum;
xi)D(xj ). Therefore,
</p>
<p>as we raise the degree of the polynomial by one at each stage, the power of
x1 must be raised at least as fast as x2, and the power of x2 must be raised at</p>
<p/>
</div>
<div class="page"><p/>
<p>25.2 Graphical Construction 765
</p>
<p>least as fast as x3, etc. Our goal is to raise the power of x1 by λ1, that of x2
by λ2, and, in general, the power of xi by λi , making sure that at each stage,
the number of multiplications by x1 is greater than or equal to the number
of multiplications by x2, etc. The total number of ways by which we can
reach this goal will be χ (λ)(1n), which is also the dimension of the irreducible
representation (λ) by Eq. (24.9).
</p>
<p>To see the argument more clearly, suppose that we are interested in the
dimension of the irreducible representation of S4 corresponding to (3,1).
Then we must raise the power of x1 by 3 and the power of x2 by 1; x3 and x4
will remain intact, and therefore will not enter in the following discussion.
It follows that D(xj ) is to be multiplied by x31x2, one x-factor at a time,
the number of x1-factors always exceeding the number of x2-factors. The
possible ways of doing this are
</p>
<p>x31x2, x
2
1x2x1, x1x2x
</p>
<p>2
1 . (25.8)
</p>
<p>Note that as we count the factors from left to right, the number of x1&rsquo;s is
always greater than or equal to the number of x2&rsquo;s. Thus x2x31 is absent
because x2 occurs without x1 occurring first. It follows that the dimension
of the irreducible representation (3,1) is 3.
</p>
<p>A graphical way to arrive at the same result is to draw λ1 = 3 boxes on
top and λ2 = 1 box below it:
</p>
<p>The next step is to fill in the boxes with numbers corresponding to the po-
sition of x1 (filling up the first row) and x2 factors (filling up the second
row) in Eq. (25.8). Since in the first term of (25.8), the x1&rsquo;s occupy the first,
second, and third positions, we enter 1, 2, and 3 in the first row, and 4 in
the second row corresponding to the last position occupied by x2. Similarly,
in the second term of (25.8), the x1&rsquo;s occupy the first, second, and fourth
positions; therefore, we enter 1, 2, and 4 in the first row, and 3 in the second
row corresponding to the position occupied by x2. Finally, in the last term
of (25.8), the x1&rsquo;s occupy the first, third, and fourth positions; therefore, we
enter 1, 3, and 4 in the first row, and 2 in the second row corresponding to
the position occupied by x2. The result is the graph shown below:
</p>
<p>Young frame defined
</p>
<p>Definition 25.2.1 Let (λ)= (λ1, λ2, . . . , λn) be a partition of n. The
Young frame (or the Young pattern) associated with (λ) is a collec-
tion of rows of boxes (squares) aligned at the left such that the first
row has λ1 boxes, the second row λ2 boxes, etc. Since λi &ge; λi+1, the
length of the rows decreases as one goes to the bottom of the frame.</p>
<p/>
</div>
<div class="page"><p/>
<p>766 25 Representations of the Symmetric Group
</p>
<p>The Young frame associated with (λ) represents xλ11 &middot; &middot; &middot;x
λn
n , which mul-
</p>
<p>tiplies the antisymmetric polynomial D(xj ). To find the dimension of the
irreducible representation T (λ), we have to count the number of ways in
which the x-factors can be permuted among themselves such that as we
scan the product, the number of xi &rsquo;s is never less than number of xj &rsquo;s if
j &gt; i. This leads to
</p>
<p>Definition 25.2.2 A standard Young tableau (or diagram, or graph) is astandard Young tableaux
defined Young frame filled with numbers 1 through n such that
</p>
<p>1. the numbers are placed consecutively left to right on the rows starting
with 1 in the far-left box of the first row;
</p>
<p>2. no box of any row is to be filled unless all boxes to its left are already
filled;
</p>
<p>3. at each stage, the number of boxes filled in any row is never less than
the number of boxes filled in the rows below it.
</p>
<p>Tableaux satisfying the last condition are called regular graphs.regular graphs
</p>
<p>It follows that in a Young tableau, the number 1 is always in the upper
left-hand box, and that going down in a column, the numbers must increase.
</p>
<p>Theorem 25.2.3 Let (λ) be a partition of n. Then the dimension of
the irreducible representation T (λ) is equal to the number of standard
Young tableaux associated with (λ).
</p>
<p>Example 25.2.4 We wish to calculate the dimension of each irreducible
representation of S4. The partitions are (4), (3,1), (2,2), (2,1,1), and
(1,1,1,1) whose associated Young frames are shown below:
</p>
<p>(4) (3,1) (2,2) (2,1,1) (1,1,1,1)
</p>
<p>The number of standard Young tableaux associated with (λ)= (4) is 1, be-
cause there is only one way to place the numbers 1 through 4 in the four
boxes. Thus, the dimension of T (4) is 1. For (λ) = (3,1), we can place 2
either to the right of 1 or below it. The first choice gives rise to two possibil-
ities for the placement of 3: Either to the right of 2 or below 1. The second
choice gives rise to only one possibility for 3, namely to the right of 1. With
1, 2, and 3 in place, the position of 4 is predetermined. Thus, we have 3 pos-
sibilities for (λ)= (3,1), and the dimension of T (3,1) is 3. For (λ)= (2,2),
we can place 2 either to the right of 1 or below it. Both choices give rise
to only one possibility for 3: In the first case, 3 can only go under 1; in the</p>
<p/>
</div>
<div class="page"><p/>
<p>25.3 Graphical Construction of Characters 767
</p>
<p>Fig. 25.1 The standard Young tableaux, and the dimensions of irreducible representa-
tions of S4
</p>
<p>second case to its right. With 1, 2, and 3 in place, the position of 4 is again
predetermined. Thus, we have 2 possibilities for (λ) = (2,2), and the di-
mension of T (3,1) is 2. The reader may check that the dimension of T (2,1,1)
</p>
<p>is 3, and that of T (1,1,1,1) is 1. Figure 25.1 summarizes these findings. We
note that the dimensions satisfy 12 + 32 + 22 + 32 + 12 = 24, the second
equation of (24.18).
</p>
<p>25.3 Graphical Construction of Characters
</p>
<p>The product of the symmetric polynomial s(l) and the antisymmetric polyno-
mial D(xj ) contains all the information regarding the representations of Sn.
We can extract the simple characters by looking at the coefficients of appro-
priate products of the x-factors. This can also be done graphically. Without
going into the combinatorics of the derivation of the results, we state the
rules for calculating the simple characters, and examine one particular case
in detail to elucidate the procedure, whose statement can be very confusing.
</p>
<p>As before, we label the irreducible representations with the partitions
of n. However, we separate out the common factors in a cyclic structure,
labeling the cycles by l1, l2, etc. For example, (2,12) has l1 = 2, l2 = 1,
and l3 = 1. So, (2,12) becomes (2,1,1), and in general, we write (l) as
(l1, l2, . . . , lm).
</p>
<p>Definition 25.3.1 A regular application of r identical symbols to a Young regular application
frame is the placement of those symbols in the boxes of the frame as follows.
Add the symbols to any given row, starting with the first (farthest to the
left) unoccupied cell, until the symbols are all used or the number of filled
boxes exceeds that of the preceding line by one. In the latter case, go to the</p>
<p/>
</div>
<div class="page"><p/>
<p>768 25 Representations of the Symmetric Group
</p>
<p>preceding line and repeat the procedure, making sure that the final result of
adding all r symbols will be a regular graph. If the number of rows occupied
by the symbols is odd (even) the application is positive (negative).
</p>
<p>As an illustration, consider the regular application of five 2&rsquo;s to the blank
Young frame shown below.
</p>
<p>We cannot start on the first row because it does not have enough boxes for
the five 2&rsquo;s. We can start on the second row and put one 2 in the first box.
This brings the number of 2&rsquo;s in the second row to one more than in the
first row; therefore, we should now go to the first row and put the rest of
the symbols there. We could start at the third row, put one 2 in the first box,
put a second 2 in the first box of the second row, and the rest in the first
row. Altogether we will have 3 regular applications of the five 2&rsquo;s. These are
shown in the diagram below.
</p>
<p>Of these the first and the last tableaux are negative applications, and the
middle one is positive.
</p>
<p>Theorem 25.3.2 The character of the irreducible representation T (λ)
</p>
<p>of the class (l)= (l1, l2, . . . , lm) is obtained by successive regular ap-
plications of l1 identical symbols (usually taken to be 1&rsquo;s), then l2
identical symbols of a different kind (usually taken to be 2&rsquo;s), etc. The
character χ (λ)(l) is then equal to the number of ways of building positive
applications minus the number of ways of building negative applica-
tions.
</p>
<p>The order in which the li &rsquo;s are applied is irrelevant. However, it is usually
convenient to start with the largest cycle.
</p>
<p>The best way to understand the procedure is to construct a character table.
Let us do this for S4. As usual, the rows are labeled by the various partitions
(λ) of 4. We choose the order (4), (3,1), (2,2) = (22), (2,1,1) = (2,12),
(1,1,1,1) = (14). The columns are labeled by classes (l) in the followingDetailed analysis of the
</p>
<p>construction of the
</p>
<p>character table for S4
</p>
<p>order: (14), (2,12), (22), (3,1), (4), where, for example, (2,12) means that
l1 = 2, l2 = 1, and l3 = 1. Example 25.2.4 gives us the first column of the
character table. Similarly, the first row has 1 in all places, because it is the</p>
<p/>
</div>
<div class="page"><p/>
<p>25.3 Graphical Construction of Characters 769
</p>
<p>trivial representation. Our task is therefore to fill in the rest of the table one
row at a time. The second row, with (λ) = (3,1), has a Young frame that
looks like
</p>
<p>and for each class (column) labeled (l1, . . . , lm), we need to fill this in with
l1 identical symbols (1&rsquo;s), l2 identical symbols of a different kind (2&rsquo;s), etc.
</p>
<p>The second column has l1 = 2, l2 = 1 = l3. So we have two 1&rsquo;s, one 2,
and one 3. If we start with the first row, the two 1&rsquo;s can be placed in its
first two boxes. If we start with the second row, the two 1&rsquo;s must be placed
vertically on top of each other. In the first case, we have two choices for
the 2: Either on the first row next to the two 1&rsquo;s, or on the second line. In
the second case, we have only one choice for the 2: in the first row next
to 1. With 1&rsquo;s and 2 in place, the position of 3 is determined. The three
possibilities are shown below:
</p>
<p>The first two are positive applications, the third is negative because the 1&rsquo;s
occupy an even number of rows. We therefore have
</p>
<p>χ
(3,1)
(2,12)
</p>
<p>=+1 + 1 &minus; 1 =+1.
</p>
<p>The third column has l1 = 2 = l2. So we have two 1&rsquo;s and two 2&rsquo;s. We
place the 1&rsquo;s as before. When the two 1&rsquo;s are placed vertically, we can put
the 2&rsquo;s on the first row and we are done. When the 1&rsquo;s are initially placed
in the first row, we have no way of placing the 2&rsquo;s by regular application.
We cannot start on the first row because there is only one spot available (re-
member, we cannot go down once we start at a row). We cannot start on
the second row because once we place the first 2, we are blocked, and the
number of symbols in the second row does not exceed that of the first row
by one. So, there is only one possibility:
</p>
<p>Not allowed Allowed
</p>
<p>The only allowed diagram is obtained by a negative application of 1&rsquo;s.
Therefore, χ (3,1)
</p>
<p>(22)
=&minus;1.
</p>
<p>The fourth column has l1 = 3 and l2 = 1. So we have three 1&rsquo;s and one 2.
There are two ways to place the 1&rsquo;s: all on the first row, or starting on the
second row and working our way up until all boxes are filled except the last
box of the first row. The placement of 2 will be then predetermined. The
result is the two diagrams shown below:</p>
<p/>
</div>
<div class="page"><p/>
<p>770 25 Representations of the Symmetric Group
</p>
<p>Table 25.2 Character table for S4. The rows and columns are labeled by the partitions
of 4 and cycle structures, respectively
</p>
<p>(14) (2,12) (22) (3,1) (4)
</p>
<p>T (4) 1 1 1 1 1
</p>
<p>T (3,1) 3 1 &minus;1 0 &minus;1
T (2
</p>
<p>2) 2 0 2 &minus;1 0
T (2,1
</p>
<p>2) 3 &minus;1 &minus;1 0 1
T (1
</p>
<p>4) 1 &minus;1 1 1 &minus;1
</p>
<p>The first diagram is obtained by a positive application of 1&rsquo;s, the second by
a negative application. Therefore,
</p>
<p>χ
(3,1)
(3,1) =+1 &minus; 1 = 0.
</p>
<p>Finally, for the last column, l1 = 4. There is only one way to put all the
1&rsquo;s in the frame, and that is a negative application. Thus, χ (3,1)(4) =&minus;1.
</p>
<p>Rather than going through the rest of the table in the same gory detail,
we shall point out some of the trickier calculations, and leave the rest of the
table for the reader to fill in. One confusion may arise in the calculation of
</p>
<p>χ
(22)
(22)
</p>
<p>. The frame looks like this,
</p>
<p>and we need to fill this with two 1&rsquo;s and two 2&rsquo;s. The 1&rsquo;s can go into the first
row or the first column. The 2&rsquo;s then can be placed in the second row or the
second column. The result is
</p>
<p>The first diagram has no negative application. The second has two negative
applications, one for the 1&rsquo;s, and one for the 2&rsquo;s. Therefore, the overall sign
</p>
<p>for the second diagram is positive. It follows that χ (2
2)
</p>
<p>(22)
=+1 + 1 =+2.
</p>
<p>The calculation of χ (2
2)
</p>
<p>(4) may also be confusing. We need to place four
1&rsquo;s in the frame. If we start on the first row, we are stuck, because there is
room for only two 1&rsquo;s. If we start in the second row, then we can only go
up: Putting the first 1 in the second row causes that row to have one extra
1 in comparison with the preceding row. However, once we go up, we have
room for only two 1&rsquo;s (we cannot go back down). So, there is no way we
</p>
<p>can place the four 1&rsquo;s in the (22) frame, and χ (2
2)
</p>
<p>(4) = 0.
The character table for S4 is shown in Table 25.2 (see Problem 24.15
</p>
<p>as well). The reader is urged to verify all entries not calculated above. The</p>
<p/>
</div>
<div class="page"><p/>
<p>25.4 Young Operators 771
</p>
<p>Table 25.3 Character table for S5. The rows and columns are labeled by the partitions
of 5 and cycle structures, respectively
</p>
<p>(15) (2,13) (22,1) (3,2) (3,12) (4,1) (5)
</p>
<p>T (5) 1 1 1 1 1 1 1
</p>
<p>T (4,1) 4 2 0 &minus;1 1 0 &minus;1
T (3,2) 5 1 1 1 &minus;1 &minus;1 0
T (3,1
</p>
<p>2) 6 0 &minus;2 0 0 0 1
T (2
</p>
<p>2,1) 5 &minus;1 1 &minus;1 &minus;1 1 0
T (2,1
</p>
<p>3) 4 &minus;2 0 1 1 0 &minus;1
T (1
</p>
<p>5) 1 &minus;1 1 &minus;1 1 &minus;1 1
</p>
<p>character table for S5 can also be calculated with only minor tedium. We
quote the result here in Table 25.3 and let the reader check the entries of the
table.
</p>
<p>25.4 Young Operators
</p>
<p>The group algebra techniques of Sect. 24.5&mdash;which we used in our discus-
sion of representation theory in a very limited way&mdash;provide a powerful and
elegant tool for unraveling the representations of finite groups. These tech-
niques have been particularly useful in the analysis of the representations of
the symmetric group. Our emphasis on the symmetric group is not merely
due to the importance of Sn as a paradigm of all finite groups. It has also
to do with the unexpected usefulness of the representations of Sn in study-
ing the representations of GL(V), the paradigm of all (classical) continuous
groups. We shall come back to this observation later when we discuss rep-
resentations of Lie groups in Chap. 30.
</p>
<p>To begin with, consider the element of the Sn group algebra as defined
in Eq. (24.20). Since multiplying P (on the left) by a group element does
not change P , the ideal generated by P is not only one-dimensional, but all
elements of Sn are represented by the number 1. Therefore, the ideal AP
corresponds to the (irreducible) identity representation.
</p>
<p>For Sn, there is another group algebra element that has similar properties.
This is
</p>
<p>Q=
n!&sum;
</p>
<p>i=1
ǫπiπi, πi &isin; Sn. (25.9)
</p>
<p>The reader may check that
</p>
<p>πjQ= ǫπjQ and Q2 = n!Q.
</p>
<p>As in the case of P , Q generates a one-dimensional ideal, but a left multipli-
cation may introduce a minus sign (when the permutation is odd). Thus, the
ideal generated by Q must correspond to the antisymmetric (or alternating)
representation.</p>
<p/>
</div>
<div class="page"><p/>
<p>772 25 Representations of the Symmetric Group
</p>
<p>All the irreducible representations, including the special one-dimensional
cases above, can be obtained using this group-algebraic method. We shall
not give the proofs here, and we refer the reader to the classic book [Boer 63,
pp. 102&ndash;125]. The starting point is the Young frame corresponding to the
partition (λ)= (λ1, . . . , λm). One puts the numbers 1 through n in the frame
in any order, consistent with tableau construction, so that the end product is
a Young tableau. Let p be any permutation of a Young tableau that per-
mutes only the elements of each row among themselves. Such a p is called
a horizontal permutation. Similarly, let q be a vertical permutation ofhorizontal and vertical
</p>
<p>permutations the Young tableau.
</p>
<p>Definition 25.4.1 Consider the kth Young tableau corresponding to the par-
tition (λ). Let the Young symmetrizer P (λ)k and Young antisymmetrizer
</p>
<p>Q
(λ)
k be the elements of the group algebra of Sn defined asYoung operators defined
</p>
<p>P
(λ)
k =
</p>
<p>&sum;
</p>
<p>p
</p>
<p>p, Q
(λ)
k =
</p>
<p>&sum;
</p>
<p>q
</p>
<p>ǫqq.
</p>
<p>Then, the Young operator Y (λ)k of this tableau, another element of the group
</p>
<p>algebra, is given by Y (λ)k =Q
(λ)
k P
</p>
<p>(λ)
k .
</p>
<p>It can be shown that the following holds.
</p>
<p>Theorem 25.4.2 The Young operator Y (λ)k is essentially idempotent,
and generates a minimal left ideal, hence an irreducible representa-
tion for Sn. Representations thus obtained from different frames are
inequivalent. Different tableaux with the same frame give equivalent
irreducible representations.
</p>
<p>In practice, one usually chooses the standard Young tableaux and applies
the foregoing procedure to them to obtain the entire collection of inequiv-
alent irreducible representations of Sn. We have already seen how to cal-
culate characters of Sn employing both analytical and graphical methods.
Theorem 25.4.2 gives yet another approach to analyzing representations of
Sn. For low values of n this technique may actually be used to determine the
characters, but as n grows, it becomes unwieldy, and the graphical method
becomes more manageable.
</p>
<p>Example 25.4.3 Let us apply this method to S3. The partitions are (3),
(2,1), and (13). There is only one standard Young tableau associated with
(3) and (13). Thus,
</p>
<p>Y (3) = P (3) = 1
3!
</p>
<p>6&sum;
</p>
<p>j=1
πj =
</p>
<p>1
</p>
<p>6
(e+ π2 + π3 + π4 + π5 + π6),</p>
<p/>
</div>
<div class="page"><p/>
<p>25.4 Young Operators 773
</p>
<p>Y (1
3) =Q(13) = 1
</p>
<p>3!
</p>
<p>6&sum;
</p>
<p>j=1
ǫπjπj =
</p>
<p>1
</p>
<p>6
(e&minus; π2 &minus; π3 &minus; π4 + π5 + π6),
</p>
<p>where we have divided these Young operators by 6 to make them idem-
potent; we have also used the notation of Example 23.4.1. One can show
directly that Y (3)Y (1
</p>
<p>3) = 0. In fact, one can prove this for general Sn (see
Problem 25.6).
</p>
<p>For the partition (2,1), there are two Young tableaux. The first one has
the numbers 1 and 2 in the first row and 3 in the second. In the second tableau
the numbers 2 and 3 are switched. Therefore, using the multiplication table
for S3 as given in Example 23.4.1, we have
</p>
<p>Y
(2,1)
1 =Q
</p>
<p>(2,1)
1 P
</p>
<p>(2,1)
1 = (e&minus; π3)(e+ π2)= e+ π2 &minus; π3 &minus; π6,
</p>
<p>Y
(2,1)
2 =Q
</p>
<p>(2,1)
2 P
</p>
<p>(2,1)
2 = (e&minus; π2)(e+ π3)= e&minus; π2 + π3 &minus; π5.
</p>
<p>The reader may verify that the product of any two Young operators cor-
responding to different Young tableaux is zero and that
</p>
<p>Y
(2,1)
1 Y
</p>
<p>(2,1)
1 = 3Y
</p>
<p>(2,1)
1 , Y
</p>
<p>(2,1)
2 Y
</p>
<p>(2,1)
2 = 3Y
</p>
<p>(2,1)
2 .
</p>
<p>Let us calculate the left ideal generated by these four Young operators.
We already know from our discussion at the beginning of this subsection that
L(3) and L(1
</p>
<p>3), the ideals generated by Y (3) and Y (1
3), are one-dimensional.
</p>
<p>Let us find L(2,1)1 , the ideal generated by Y
(2,1)
1 . This is the span of all vectors
</p>
<p>obtained by multiplying Y (2,1)1 on the left by elements of the group algebra.
</p>
<p>It is sufficient to multiply Y (2,1)1 by the basis of the algebra, namely the
group elements:
</p>
<p>eY
(2,1)
1 = Y
</p>
<p>(2,1)
1 ,
</p>
<p>π2Y
(2,1)
1 = π2 + e&minus; π5 &minus; π4 &equiv;X
</p>
<p>(2,1)
1 ,
</p>
<p>π3Y
(2,1)
1 = π3 + π6 &minus; e&minus; π2 =&minus;Y
</p>
<p>(2,1)
1 ,
</p>
<p>π4Y
(2,1)
1 = π4 + π5 &minus; π6 &minus; π3 =&minus;X
</p>
<p>(2,1)
1 + Y
</p>
<p>(2,1)
1 ,
</p>
<p>π5Y
(2,1)
1 = π5 + π4 &minus; π2 &minus; e=&minus;X
</p>
<p>(2,1)
1 ,
</p>
<p>π6Y
(2,1)
1 = π6 + π3 &minus; π4 &minus; π5 =X
</p>
<p>(2,1)
1 &minus; Y
</p>
<p>(2,1)
1 .
</p>
<p>It follows from the above calculation that L(2,1)1 , as a vector space, is
</p>
<p>spanned by {Y (2,1)1 ,X
(2,1)
1 }, and since these two vectors are linearly inde-
</p>
<p>pendent, L(2,1)1 is a two-dimensional minimal ideal corresponding to a two-
dimensional irreducible representation of S3. One can use this basis to find
representation matrices and the simple characters of S3.
</p>
<p>The other two-dimensional irreducible representation of S3, equivalent
to the one above, is obtained by constructing the ideal L(2,1)2 generated by
</p>
<p>Y
(2,1)
2 . This construction is left for the reader, who is also asked to verify its
</p>
<p>dimensionality.</p>
<p/>
</div>
<div class="page"><p/>
<p>774 25 Representations of the Symmetric Group
</p>
<p>The resolution of the identity is easily verified:
</p>
<p>e= 1
6
Y (3)
</p>
<p>︸ ︷︷ ︸
&equiv;e1
</p>
<p>+ 1
6
Y (1
</p>
<p>3)
</p>
<p>︸ ︷︷ ︸
&equiv;e2
</p>
<p>+ 1
3
Y
(2,1)
1
</p>
<p>︸ ︷︷ ︸
&equiv;e3
</p>
<p>+ 1
3
Y
(2,1)
2
</p>
<p>︸ ︷︷ ︸
&equiv;e4
</p>
<p>.
</p>
<p>The ei &rsquo;s are idempotents that satisfy eiej = 0 for i �= j .
</p>
<p>25.5 Products of Representations of Sn
</p>
<p>In the quantum theory of systems of many identical particles, the wave func-
tion must have a particular symmetry under exchange of the particles: If the
particles are all fermions (bosons), the overall wave function must be com-
pletely antisymmetric (symmetric). Since the space of functions of several
variables can provide a carrier space for the representation of any group,
we can, in the case of Sn, think of the antisymmetric (symmetric) functions
as basis functions for the one-dimensional irreducible identity (alternating)
representation. To obtain these basis functions, we apply the Young opera-
tor Y (1
</p>
<p>n) (or Y (n)) to the arguments of any given function of n variables to
obtain the completely antisymmetric (or symmetric) wave function.1
</p>
<p>Under certain conditions, we may require mixed symmetries. For in-
stance, in the presence of spin, the product of the total spin wave function
and the total space wave function must be completely antisymmetric for
Fermions. Thus, the space part (or the spin part) of the wave functions will,
in general, have mixed symmetry. Such a mixed symmetry corresponds to
some other Young operator, and the wave function is obtained by applying
that Young operator to the arguments of the wave function.
</p>
<p>Now suppose that we have two separate systems consisting of n1 and n2
particles, respectively, which are all assumed to be identical. As long as the
two systems are not interacting, each will consist of states that are classified
according to the irreducible representations of its symmetric group. When
the two systems interact, we should classify the states of the total system
according to the irreducible representations of all n1 +n2 particles. We have
already encountered the mathematical procedure for such classification: It is
the Clebsch-Gordan decomposition of the direct product of the states of the
two systems. Since the initial states correspond to Young tableaux, and since
we are interested in the inequivalent irreducible representations, we need to
examine the decomposition of the direct product of Young frames into a
sum of Young frames. We first state (without proof) the procedure for such
a decomposition, and then give an example to illustrate it.
</p>
<p>Theorem 25.5.1 To find the components of Young frames in the product of
two Young frames, draw one of the frames. In the other frame, assign the
same symbol, say 1, to all boxes in the first row, the same symbol 2 to all
</p>
<p>1We must make the additional assumption that the permuted functions are all indepen-
dent.</p>
<p/>
</div>
<div class="page"><p/>
<p>25.5 Products of Representations of Sn 775
</p>
<p>boxes in the second row, etc. Now attach the first row to the first frame, and
enlarge in all possible ways subject to the restriction that no two 1&rsquo;s appear
in the same column, and that the resultant graph be regular. Repeat with the
2&rsquo;s, etc., making sure in each step that as we read from right to left and top
to bottom, no symbol is counted fewer times than the symbol that came after
it. The product is the sum of all diagrams so obtained.
</p>
<p>To illustrate the procedure, consider the product
</p>
<p>&otimes;
</p>
<p>We have put two 1&rsquo;s in the first row and one 2 in the second row of the frame
to the right. Now apply the first row to the frame on the left. The result is
</p>
<p>Now we apply the 2 to each of these graphs separately. We cannot put a 2
to the right of the 1&rsquo;s, because in that case, as we count from right to left,
we would start with a 2 without having counted any 1&rsquo;s. The allowed graphs
obtained from the first diagram are
</p>
<p>Applying the 2 to the second graph, we obtain
</p>
<p>and to the third graph gives
</p>
<p>Finally the last graph yields</p>
<p/>
</div>
<div class="page"><p/>
<p>776 25 Representations of the Symmetric Group
</p>
<p>Fig. 25.2 Some products of Young frames for small values of n
</p>
<p>The entire process described above is written in terms of frames as
</p>
<p>Some simple products, some of which will be used later, are given in
Fig. 25.2.
</p>
<p>25.6 Problems
</p>
<p>25.1 Construct the character table of S4 using the analytical method and
Eq. (25.4).
</p>
<p>25.2 Find all the standard Young tableaux for S5. Thus, determine the di-
mension of each irreducible representations of S5. Check that the dimen-
sions satisfy the second equation of (24.18).
</p>
<p>25.3 Verify the remaining entries of Table 25.2.
</p>
<p>25.4 Construct the character table of S5.
</p>
<p>25.5 Suppose that Q, an element of the group algebra of Sn, is given by
</p>
<p>Q=
n!&sum;
</p>
<p>i=1
ǫπiπi, πi &isin; Sn.</p>
<p/>
</div>
<div class="page"><p/>
<p>25.6 Problems 777
</p>
<p>Show that
</p>
<p>πjQ= ǫπjQ and Q2 = n!Q.
</p>
<p>25.6 Show that Y (n)Y (1
n) = 0. Hint: There are as many even permutations
</p>
<p>in Sn as there are odd permutations.
</p>
<p>25.7 Show that the product of any two Young operators of S3 corresponding
to different Young tableaux is zero and that
</p>
<p>Y
(2,1)
1 Y
</p>
<p>(2,1)
1 = 3Y
</p>
<p>(2,1)
1 , Y
</p>
<p>(2,1)
2 Y
</p>
<p>(2,1)
2 = 3Y
</p>
<p>(2,1)
2 .
</p>
<p>25.8 Construct the ideal L(2,1)2 generated by Y
(2,1)
2 and verify that it is two
</p>
<p>dimensional.
</p>
<p>25.9 Using the ideal L(2,1)1 generated by Y
(2,1)
1 , find the matrices of the
</p>
<p>irreducible representation T (2,1). From these matrices calculate the simple
characters of S3 and compare your result with Table 24.4. Show that the
ideal L(2,1)2 generated by Y
</p>
<p>(2,1)
2 gives the same set of characters.
</p>
<p>25.10 Find all the Young operators for S4 corresponding to the first en-
</p>
<p>try of each row of Fig. 25.1. Find the ideals L(3,1)1 and L
(22)
1 generated by
</p>
<p>the Young operators Y (3,1)1 and Y
(22)
1 corresponding to the second and third
</p>
<p>rows of the table. Show that L(3,1)1 and L
(22)
1 have 3 and 2 dimensions, re-
</p>
<p>spectively.
</p>
<p>25.11 Verify the products of the Young frames of Fig. 25.2.</p>
<p/>
</div>
<div class="page"><p/>
<p>Part VIII
</p>
<p>Tensors andManifolds</p>
<p/>
</div>
<div class="page"><p/>
<p>26Tensors
</p>
<p>Until around 1970s, tensors were almost completely synonymous with (gen-
eral) relativity except for a minor use in hydrodynamics. Students of physics
did not need to study tensors until they took a course in the general theory of
relativity. Then they would read the introductory chapter on tensor algebra
and analysis, solve a few problems to condition themselves for index &ldquo;gym-
nastics&rdquo;, read through the book, learn some basic facts about relativity, and
finally abandon it (unless they became relativists).
</p>
<p>Today, with the advent of gauge theories of fundamental particles, the
realization that gauge fields are to be thought of as geometrical objects, and
the widespread belief that all fundamental interactions (including gravity)
are different manifestations of the same superforce, the picture has changed
drastically.
</p>
<p>Two important developments have taken place as a consequence: Ten-
sors have crept into other interactions besides gravity (such as the weak and
strong nuclear interactions), and the geometrical (coordinate-independent)
aspects of tensors have become more and more significant in the study of all
interactions. The coordinate-independent study of tensors is the focus of the
fascinating field of differential geometry and Lie groups, the subject of the
remainder of the book.
</p>
<p>As is customary, we will consider only real vector spaces and abandon
the Dirac bra and ket notation, whose implementation is most advantageous
in unitary (complex) spaces. From here on, the basis vectors1 of a vector
space V will be distinguished by a subscript and those of its dual space by
a superscript. If {ei}Ni=1 is a basis in V, then {ǫǫǫj }Nj=1 is a basis in V&lowast;. Also, Einstein&rsquo;s summation
</p>
<p>convention
Einstein&rsquo;s summation convention will be used:
</p>
<p>Box 26.0.1 Repeated indices, of which one is an upper and the
other a lower index, are assumed to be summed over: aki b
</p>
<p>i
j means&sum;N
</p>
<p>i=1 a
k
i b
</p>
<p>i
j .
</p>
<p>1We denote vectors by roman boldface, and tensors of higher rank by sans serif bold
letters.
</p>
<p>S. Hassani, Mathematical Physics, DOI 10.1007/978-3-319-01195-0_26,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>781</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-01195-0_26">http://dx.doi.org/10.1007/978-3-319-01195-0_26</a></div>
</div>
<div class="page"><p/>
<p>782 26 Tensors
</p>
<p>As a result of this convention, it is more natural to label the elements of a
matrix representation of an operator A by αji (rather than αji ), because then
</p>
<p>Aei = αji ej .
</p>
<p>26.1 Tensors as Multilinear Maps
</p>
<p>Since tensors are special kinds of linear operators on vector spaces, let us
reconsider L(V,W), the space of all linear mappings from the real vector
space V to the real vector space W. We noted in Chap. 5 that L(V,W)
is isomorphic to a space with dimension dimV &middot; dimW. The following
proposition&mdash;whose proof we leave to the reader&mdash;shows this directly.
</p>
<p>Proposition 26.1.1 Let {ei}N1i=1 be a basis for V and {e&prime;α}
N2
α=1 a basis forW.
</p>
<p>Then
</p>
<p>1. the linear transformations Tjβ : V&rarr;W in the vector space L(V,W),
defined by (note the new way of writing the Kronecker delta)
</p>
<p>T
j
βei = δ
</p>
<p>j
i e
</p>
<p>&prime;
β , j = 1, . . . ,N1; β = 1, . . . ,N2, (26.1)
</p>
<p>form a basis in L(V,W). In particular, dimL(V,W)=N1N2.
2. If ταj are the matrix elements of a matrix representation of a linear
</p>
<p>transformation T &isin; L(V,W) with respect to the two bases above, then
T= ταj T
</p>
<p>j
α .
</p>
<p>The dual space V&lowast; is simply the space L(V,R). Proposition 26.1.1 (with
N2 = 1) then implies that dimV&lowast; = dimV, which was shown in Chap. 2.
The dual space is important in the discussion of tensors, so we consider
some of its properties below.
</p>
<p>When W=R, the basis {Tjβ} of Proposition 26.1.1 reduces to {T
j
</p>
<p>1} and is
denoted by {ǫǫǫj }Nj=1, with N = dimV&lowast; = dimV. The ǫǫǫj &rsquo;s have the property
that
</p>
<p>ǫǫǫj (ei)= δji , (26.2)
which is (26.1) with β = 1 and e&prime;β = e&prime;1 = 1, a basis of R. Equation (26.2)
was also established in Chap. 2. The basis B&lowast; = {ǫǫǫj }Nj=1 is simply the dual
of the basis B = {ei}Ni=1. Note the &ldquo;natural&rdquo; position of the indices for B
and B&lowast;.
</p>
<p>Now suppose that {fi}Ni=1 = B &prime; is another basis of V and R is the (invert-
ible) matrix carrying B onto B &prime;. Let B &prime;&lowast; = {ϕϕϕj }Nj=1 be the dual of B &prime;. We
want to find the matrix that carries B&lowast; onto B &prime;&lowast;. If we denote this matrix by
A and its elements by aji , we have
</p>
<p>δki =ϕϕϕkfi =
(
akl ǫǫǫ
</p>
<p>l
)(
r
j
</p>
<p>i ej
)
= akl r
</p>
<p>j
</p>
<p>i δ
l
j = akl r li = (AR)ki ,
</p>
<p>where the first equality follows from the duality of B &prime; and B &prime;&lowast;. In matrix
form, this equation can be written as AR = 1, or A = R&minus;1. Thus,</p>
<p/>
</div>
<div class="page"><p/>
<p>26.1 Tensors as Multilinear Maps 783
</p>
<p>Box 26.1.2 The matrix that transforms bases of V&lowast; is the inverse of
the matrix that transforms the corresponding bases of V.
</p>
<p>In the equations above, the upper index in matrix elements labels rows,
and the lower index labels columns. This can be remembered by noting that
the column vectors ei can be thought of as columns of a matrix, and the
lower index i then labels those columns. Similarly, ǫǫǫj can be thought of as
rows of a matrix. We now generalize the concept of linear functionals.
</p>
<p>Definition 26.1.3 A map T : V1 &times;V2 &times; &middot; &middot; &middot; &times;Vr &rarr;W is called r-linear if
it is linear in all its variables:
</p>
<p>T
(
v1, . . . , αvi + α&prime;v&prime;i, . . . ,vr
</p>
<p>)
</p>
<p>= αT(v1, . . . ,vi, . . . ,vr)+ α&prime;T
(
v1, . . . ,v&prime;i, . . . ,vr
</p>
<p>)
</p>
<p>for all i.
</p>
<p>multilinear mappings
</p>
<p>defined
</p>
<p>We can easily construct a bilinear map. Let τττ 1 &isin; V&lowast;1 and τττ 2 &isin; V&lowast;2 . We
define the map τττ 1 &otimes; τττ 2 : V1 &times;V2 &rarr;R by
</p>
<p>τττ 1 &otimes; τττ 2(v1,v2)= τττ 1(v1)τττ 2(v2). (26.3)
</p>
<p>The expression τττ 1 &otimes; τττ 2 is called the tensor product of τττ 1 and τττ 2. Clearly,
since τττ 1 and τττ 2 are separately linear, so is τττ 1 &otimes; τττ 2.
</p>
<p>An r-linear map can be multiplied by a scalar, and two r-linear maps can
be added; in each case the result is an r-linear map. Thus, the set of r-linear
maps from V1 &times; &middot; &middot; &middot; &times; Vr into W forms a vector space that is denoted by
L(V1, . . . ,Vr ;W).
</p>
<p>We can also construct multilinear maps on the dual space. First, we note
that we can define a natural linear functional on V&lowast; as follows. We let τττ &isin; V&lowast;
and v &isin; V; then τττ(v) &isin; R. Now we twist this around and define a mapping
v : V&lowast; &rarr; R given by v(τττ ) &equiv; τττ(v). It is easily shown that this mapping is
linear. Thus, we have naturally constructed a linear functional on V&lowast; by
identifying (V&lowast;)&lowast; with V.
</p>
<p>Construction of multilinear maps on V&lowast; is now trivial. For example, let
v1 &isin; V1 and v2 &isin; V2 and define the tensor product v1 &otimes; v2 : V&lowast;1 &times; V&lowast;2 &rarr; R
by
</p>
<p>v1 &otimes; v2(τττ 1,τττ 2)= v1(τττ 1)v2(τττ 2)= τττ 1(v1)τττ 2(v2). (26.4)
We can also construct mixed multilinear maps such as v &otimes; τττ : V&lowast; &times; V&rarr;R
given by
</p>
<p>v &otimes; τττ(θθθ,u)= v(θθθ)τττ(u)= θθθ(v)τττ (u). (26.5)
There is a bilinear map h : V&lowast; &times; V&rarr;R that naturally pairs V and V&lowast;; it
</p>
<p>natural pairing of vectors
</p>
<p>and their duals
is given by h(θθθ,v)&equiv; θθθ(v). This mapping is called the natural pairing of V
and V&lowast; into R and is denoted by using angle brackets:
</p>
<p>h(θθθ,v)&equiv; 〈θθθ,v〉 &equiv; θθθ(v).</p>
<p/>
</div>
<div class="page"><p/>
<p>784 26 Tensors
</p>
<p>Definition 26.1.4 Let V be a vector space with dual space V&lowast;. Then a ten-
tensors; covariant and
</p>
<p>contravariant degrees
sor of type (r, s) is a multilinear mapping
</p>
<p>Trs : V&lowast; &times;V&lowast; &times; &middot; &middot; &middot; &times;V&lowast;︸ ︷︷ ︸
r times
</p>
<p>&times;V&times;V&times; &middot; &middot; &middot; &times;V︸ ︷︷ ︸
s times
</p>
<p>&rarr;R.
</p>
<p>The set of all such mappings for fixed r and s forms a vector space denoted
by Trs (V). The number r is called the contravariant degree of the tensor,
and s is called the covariant degree of the tensor.
</p>
<p>As an example, let v1, . . . ,vr &isin; V and τττ 1, . . . ,τττ s &isin; V&lowast;, and define the
tensor product Trs &equiv; v1 &otimes; &middot; &middot; &middot; &otimes; vr &otimes; τττ 1 &otimes; &middot; &middot; &middot; &otimes; τττ s by
</p>
<p>v1 &otimes; &middot; &middot; &middot; &otimes; vr &otimes; τττ 1 &otimes; &middot; &middot; &middot; &otimes; τττ s
(
θθθ1, . . . ,θθθ r ,u1, . . . ,us
</p>
<p>)
</p>
<p>= v1
(
θθθ1
</p>
<p>)
. . .vr
</p>
<p>(
θθθ r
)
τττ 1(u1) . . .τττ s(us)=
</p>
<p>r&prod;
</p>
<p>i=1
</p>
<p>s&prod;
</p>
<p>j=1
θθθ i(vi)τττ j (uj ).
</p>
<p>Each v in the tensor product requires an element of V&lowast;; that is why the
number of factors of V&lowast; in the Cartesian product equals the number of v&rsquo;s
in the tensor product. As explained in Chap. 1, the Cartesian product with s
factors of V is denoted by Vs (similarly for V&lowast;).
</p>
<p>A tensor of type (0,0) is defined to be a scalar, so T00(V)= R. A tensorcontravariant and
covariant vectors and
</p>
<p>tensors
</p>
<p>of type (1,0), an ordinary vector, is called a contravariant vector, and one
of type (0,1), a dual vector (or a linear functional), is called a covariant
vector. A tensor of type (r,0) is called a contravariant tensor of rank r ,
and one of type (0, s) is called a covariant tensor of rank s. The union
of Trs (V) for all possible r and s can be made into an (infinite-dimensional)tensors form an algebra
algebra, called the algebra of tensors, by defining the following product on
it:
</p>
<p>Definition 26.1.5 The tensor product of a tensor T of type (r, s) and a
multiplication of the
</p>
<p>algebra of tensors
tensor U of type (k, l) is a tensor T&otimes;U of type (r + k, s + l), defined, as an
operator on (V&lowast;)r+k &times;Vs+l , by
</p>
<p>T&otimes;U
(
θθθ1, . . . ,θθθ r+k,u1, . . . ,us+l
</p>
<p>)
</p>
<p>= T
(
θθθ1, . . . ,θθθ r ,u1, . . . ,us
</p>
<p>)
U
(
θθθ r+1, . . . ,θθθ r+k,us+1, . . . ,us+l
</p>
<p>)
.
</p>
<p>This product turns the (infinite-dimensional) vector space of all tensors into
an associative algebra called a tensor algebra.
</p>
<p>This definition is a generalization of Eqs. (26.3), (26.4), and (26.5). It is
easily verified that the tensor product is associative and distributive (over
tensor addition), but not commutative.
</p>
<p>Making computations with tensors requires choosing a basis for V and
one for V&lowast; and representing the tensors in terms of numbers (components).
This process is not, of course, new. Linear operators are represented by ar-
rays of numbers, i.e., matrices. The case of tensors is merely a generalization
of that of linear operators and can be stated as follows:</p>
<p/>
</div>
<div class="page"><p/>
<p>26.1 Tensors as Multilinear Maps 785
</p>
<p>Theorem 26.1.6 Let {ei}Ni=1 be a basis in V, and {ǫǫǫj }Nj=1 a basis in
V&lowast;, usually taken to be the dual of {ei}Ni=1. Then the set of all tensor
products ei1 &otimes; &middot; &middot; &middot; &otimes; eir &otimes; ǫǫǫj1 &otimes; &middot; &middot; &middot; &otimes; ǫǫǫjs forms a basis for Trs (V).
Furthermore, the components of any tensor A &isin; Trs (V) are
</p>
<p>A
j1...jr
i1...is
</p>
<p>= A
(
ǫǫǫj1 , . . . ,ǫǫǫjr , ei1 , . . . , eis
</p>
<p>)
.
</p>
<p>Proof The proof is a simple exercise in employing the definition of tensor
products and keeping track of the multilinear property of tensors. Details are
left for the reader. �
</p>
<p>A useful result of the theorem is the relation
</p>
<p>A=Ai1...irj1...js ei1 &otimes; &middot; &middot; &middot; &otimes; eir &otimes; ǫǫǫ
j1 &otimes; &middot; &middot; &middot; &otimes; ǫǫǫjs . (26.6)
</p>
<p>Note that for every factor in the basis vectors of Trs (V) there are N
possibilities, each one giving a new linearly independent vector of the ba-
sis. Thus, the number of possible tensor products is N r+s , and we have
dimTrs (V)= (dimV)r+s .
</p>
<p>Example 26.1.7 Let us consider the special case of T11(V) as an illustration.
We can write A &isin; T11(V) as A=Aijei &otimes; ǫǫǫj . Given any v &isin; V, we obtain2
</p>
<p>A(v)=
(
Aij ei &otimes; ǫǫǫj
</p>
<p>)
(v)=Aij ei
</p>
<p>[
ǫǫǫj (v)
</p>
<p>]
︸ ︷︷ ︸
</p>
<p>&isin;R
</p>
<p>.
</p>
<p>This shows that A(v) &isin; V and A can be interpreted as a linear operator on V,
i.e., A &isin;L(V). Similarly, for τττ &isin; V&lowast; we get
</p>
<p>T
1
1(V), L(V), and L(V
</p>
<p>&lowast;)
are all the sameA(τττ )=
</p>
<p>(
Aij ei &otimes; ǫǫǫj
</p>
<p>)
(τττ )=Aij
</p>
<p>[
ei(τττ )
</p>
<p>]
︸ ︷︷ ︸
</p>
<p>&isin;R
</p>
<p>ǫǫǫj .
</p>
<p>Thus, A &isin;L(V&lowast;). We have shown that given A &isin; T11(V), there corresponds a
linear operator belonging to L(V) [or L(V&lowast;)] having a natural relation to A.
Similarly, given any A &isin;L(V) [or L(V&lowast;)] with a matrix representation in the
basis {ei}Ni=1 of V (or {ǫǫǫj }Nj=1 of V&lowast;) given by Aij , then corresponding to it
in a natural way is a tensor in T11(V), namely A
</p>
<p>i
j ei&otimes;ǫǫǫj . Problem 26.5 shows
</p>
<p>that the tensor defined in this way is basis-independent. Therefore, there is
a natural one-to-one correspondence among T11(V), L(V), and L(V
</p>
<p>&lowast;). This
natural correspondence is called a natural isomorphism. Whenever there is
</p>
<p>natural isomorphism
a natural isomorphism between two vector spaces, those vector spaces can
be treated as being the same.
</p>
<p>2Here, we are assuming that A acts on an object (such as v) by &ldquo;pairing it up&rdquo; with an
appropriate factor of which A is composed (such as ǫǫǫj ).</p>
<p/>
</div>
<div class="page"><p/>
<p>786 26 Tensors
</p>
<p>We have defined tensors as multilinear machines that take in a vector
from a specific Cartesian product space of V&rsquo;s and V&lowast;&rsquo;s and manufacture a
real number. Given the representation in Eq. (26.6), however, we can gener-
alize the interpretation of a tensor as a linear machine so that it takes a vector
belonging to a Cartesian product space and manufactures a tensor. This cor-
responds to a situation in which not all factors of (26.6) find &ldquo;partners.&rdquo;
An illustration of this situation was presented in the preceding example. To
clarify this, let us consider A &isin; T12(V), given by
</p>
<p>A=Aijkei &otimes; ǫǫǫj &otimes; ǫǫǫk.
</p>
<p>This machine needs a Cartesian-product vector of the form (τττ ,v1,v2), with
τττ &isin; V&lowast; and v1,v2 &isin; V, to give a real number. However, if it is not fed
enough, it will not complete its job. For instance, if we feed it only a dual
vector τττ &isin; V&lowast;, it will give a tensor belonging to T02(V):
</p>
<p>A(τττ )=
(
Aijkei &otimes; ǫǫǫj &otimes; ǫǫǫk
</p>
<p>)
(τττ )=Aijk
</p>
<p>[
ei(τττ )
</p>
<p>]
ǫǫǫj &otimes; ǫǫǫk.
</p>
<p>If we feed it a double vector (v1,v2), it will manufacture a vector in V:
</p>
<p>A(v1,v2)=
(
Aijkei &otimes; ǫǫǫj &otimes; ǫǫǫk
</p>
<p>)
(v1,v2)=Aijkei
</p>
<p>[
ǫǫǫj (v1)
</p>
<p>][
ǫǫǫk(v2)
</p>
<p>]
&isin; V.
</p>
<p>What if we feed it a single vector v? It will blow its whistles and buzz
positioning of vectors
</p>
<p>and their duals to match
</p>
<p>the tensor
</p>
<p>its buzzers, because it does not know whether to give v to ǫǫǫj or ǫǫǫk (it is
smart enough to know that it cannot give v to ei ). That is why we have
to inform the machine with which factor of ǫǫǫ to match v. This is done by
properly positioning v inside a pair of parentheses: If we write (.,v, .), the
machine will know that v belongs to ǫǫǫj , and (., .,v) tells the machine to pair
v with ǫǫǫk . If we write (v, ., .), the machine will give us an &ldquo;error message&rdquo;
because it cannot pair v with ei !
</p>
<p>The components of a tensor A, as given in Eq. (26.6), depend on the
basis in which they are described. If the basis is changed, the components
change. The relation between components of a tensor in different bases is
called the transformation law for that particular tensor. Let us investigate
this concept.
</p>
<p>We use overbars to distinguish among various bases. For instance, B =
{ei}Ni=1, B = {ej }Nj=1, and B = {ek}Nk=1 are three different bases of V. Simi-
larly, B&lowast; = {ǫǫǫi}Ni=1, B
</p>
<p>&lowast; = {ǫǫǫj }Nj=1, and B
&lowast;
= {ǫǫǫ k}Nk=1 are bases of V&lowast;. The
</p>
<p>components are also distinguished with overbars. Recall that if R is the ma-
trix connecting B and B , then S = R&minus;1 connects B&lowast; and B &lowast;. For a tensor A
of type (1,2), Theorem 26.1.6 gives
</p>
<p>A
i
</p>
<p>jk = A
(
ǫǫǫ i, ej , ek
</p>
<p>)
= A
</p>
<p>(
simǫǫǫ
</p>
<p>m, rnj en, r
p
k ep
</p>
<p>)
</p>
<p>= simrnj r
p
</p>
<p>k A
(
ǫǫǫm, en, ep
</p>
<p>)
= simrnj r
</p>
<p>p
</p>
<p>k A
m
np. (26.7)
</p>
<p>This is the law that transforms the components of A from one basis to an-
other.</p>
<p/>
</div>
<div class="page"><p/>
<p>26.1 Tensors as Multilinear Maps 787
</p>
<p>In the classical coordinate-dependent treatment of tensors, Eq. (26.7) was
the defining relation for a tensor of type (1,2). In other words, a tensor of
type (1,2) was defined to be a collection of numbers, Amnp that transformed
</p>
<p>to another collection of numbers A
i
</p>
<p>jk according to the rule in (26.7) when
the basis was changed. In the modern treatment of tensors it is not necessary
to introduce any basis to define tensors. Only when the components of a ten-
sor are needed must bases be introduced. The advantage of the coordinate-
free treatment is obvious, since a (1,2)-type tensor has 27 components in
three dimensions and 64 components in four dimensions, and all of these
are represented by the single symbol A. However, the role of components
should not be downplayed. After all, when it comes to actual calculations,
we are forced to choose a basis and manipulate components.
</p>
<p>Since Trs (V) are vector spaces, it is desirable to investigate mappings
from one such space to another. We will be particularly interested in linear
mappings. For example, f : T10(V)&rarr; T00(V)=R is what was called a linear
functional before. Similarly, t : T10(V) &rarr; T10(V) is a linear transformation
on V. A special linear transformation is tr : T11(V)&rarr; T00(V)=R, given by
</p>
<p>trA= tr
(
Aij ei &otimes; ǫǫǫj
</p>
<p>)
&equiv;Aii &equiv;
</p>
<p>N&sum;
</p>
<p>i=1
Aii .
</p>
<p>This is the same trace encountered in the study of linear transformations in
Chap. 5.
</p>
<p>Although the above definition of the trace makes explicit use of com-
ponents with respect to a basis, it was shown in Chap. 5 that it is in fact
basis-independent (see also Problem 26.7). Functions of tensors that do not
depend on bases are called invariants. Another example of an invariant is a
linear functional (see Problem 26.6).
</p>
<p>Example 26.1.8 Consider the tensor A &isin; T20(V) given by A = e1 &otimes; e1 +
e2 &otimes; e1. We calculate the analogue of the trace for A:
</p>
<p>&sum;2
i=1 Aii = 1+0 = 1.
</p>
<p>Now we change to a new basis, {ē1, ē2}, given by e1 = ē1 + 2ē2 and e2 =
&minus;ē1 + ē2. In terms of the new basis vectors, A is given by
</p>
<p>A= (ē1 + 2ē2)&otimes; (ē1 + 2ē2)+ (&minus;ē1 + ē2)&otimes; (ē1 + 2ē2)
= 3ē2 &otimes; ē1 + 6ē2 &otimes; ē2
</p>
<p>with
&sum;2
</p>
<p>i=1 Āii = 0 + 6 = 6 �=
&sum;2
</p>
<p>i=1 Aii . This kind of &ldquo;trace&rdquo; is not invari-
ant.
</p>
<p>Besides mappings of the form h : Trs (V)&rarr; Tkl (V) that depend on a single
variable, we can define mappings that depend on several variables, in other
words, that take several elements of Trs (V) and give an element of T
</p>
<p>k
l (V).
</p>
<p>We then write h : (Trs (V))m &rarr; Tkl (V). It is understood that h(t1, . . . , tm),
in which all ti are in Trs (V), is a tensor of type (k, l). If h is linear in all tensor-valued
</p>
<p>multilinear map
of its variables, it is called a tensor-valued multilinear map. Furthermore,
if h(t1, . . . , tm) does not depend on the choice of a basis of Trs (V), it is</p>
<p/>
</div>
<div class="page"><p/>
<p>788 26 Tensors
</p>
<p>called a multilinear invariant. In most cases k = 0 = l, and we speak of
scalar-valued invariants, or simply invariants. An example of a multilinear
invariant is the determinant considered as a function of the rows of a matrix.
</p>
<p>The following defines an important class of multilinear invariants:
contraction of a tensor
</p>
<p>Definition 26.1.9 A contraction of a tensor A &isin; Trs (V) with respect
to a contravariant index at position p and a covariant index at position
q is a linear mapping Cpq : Trs (V)&rarr; Tr&minus;1s&minus;1(V) given in component form
by
</p>
<p>[
C
p
q (A)
</p>
<p>]i1...ir&minus;1
j1...js&minus;1
</p>
<p>=Ai1...ip&minus;1kip+1...irj1...jq&minus;1kjq+1...js &equiv;
&sum;
</p>
<p>k
</p>
<p>A
i1...ip&minus;1kip+1...ir
j1...jq&minus;1kjq+1...js .
</p>
<p>It can be readily shown that contractions are basis-independent. The
proof is exactly the same as that for the basis-independence of the trace.
In fact, the trace is a special case of a contraction, in which r = s = 1.
</p>
<p>By applying the contraction mapping repeatedly, we can keep reducing
the rank of a tensor. For example,
</p>
<p>[
C
p2
q2 C
</p>
<p>p1
q1 (A)
</p>
<p>]i1...ir&minus;2
j1...js&minus;2
</p>
<p>=Ai1...ip1&minus;1kip1+1&middot;&middot;&middot;p2&minus;1lip2+1...irj1...jq1&minus;1kjq1+1...jq2&minus;1ljq2+1...js ,
</p>
<p>where a sum over repeated indices k and l is understood on the right-hand
side. Continuing this process, we get Cpmqm . . .C
</p>
<p>p2
q2 C
</p>
<p>p1
q1 : Trs (V) &rarr; Tr&minus;ms&minus;m(V).
</p>
<p>In particular, if r = s, we have Cprqr . . .C
p2
q2 C
</p>
<p>p1
q1 : Trr (V) &rarr; R. In terms of
</p>
<p>components, we have
</p>
<p>Crr . . .C
2
2C
</p>
<p>1
1(A)=Ai1i2...iri1i2...ir ,
</p>
<p>for A &isin; Trr . Ai1i2...iri1i2...ir are the components of A in any basis. This leads to a
pairing of a tensor of type (r,0) with a tensor of type (0, r). If A &isin; Tr0 and
B &isin; T0r , then A&otimes; B &isin; Trr , and the pairing 〈A,B〉 can be defined as
</p>
<p>〈A,B〉 &equiv; Crr . . .C22C11(A&otimes; B)=Ai1i2...irBi1i2...ir (26.8)
</p>
<p>with Einstein&rsquo;s summation convention in place.
The pairing defined above can also be obtained from evaluation. Let
</p>
<p>{vi}Ni=1 be a basis of V and {ωωωi}Ni=1 its dual basis in V&lowast;. Then
</p>
<p>A=Ai1i2...ir vi1 &otimes; vi2 &otimes; &middot; &middot; &middot; &otimes; vir , B= Bj1j2...jrωωωj1 &otimes;ωωωj2 &otimes; &middot; &middot; &middot; &otimes;ωωωjr
</p>
<p>and
</p>
<p>B(A)= Bj1j2...jrωωωj1 &otimes;ωωωj2 &otimes; &middot; &middot; &middot; &otimes;ωωωjr
(
Ai1i2...ir vi1 ,vi2 , . . . ,vir
</p>
<p>)
</p>
<p>= Bj1j2...jrAi1i2...irωωωj1 &otimes;ωωωj2 &otimes; &middot; &middot; &middot; &otimes;ωωωjr (vi1 ,vi2, . . . ,vir )
</p>
<p>= Bj1j2...jrAi1i2...irωωωj1(vi1)ωωωj2(vi2) . . .ωωωjr (vir )</p>
<p/>
</div>
<div class="page"><p/>
<p>26.2 Symmetries of Tensors 789
</p>
<p>= Bj1j2...jrAi1i2...ir δ
j1
i1
δ
j2
i2
. . . δ
</p>
<p>jr
ir
=Ai1i2...irBi1i2...ir . (26.9)
</p>
<p>The linearity inherent in the construction of tensor algebras carries along
some of the properties and structures of the underlying vector spaces. One
such property is isomorphism. Suppose that F : V &rarr; U is a vector space
isomorphism. Then F&lowast; :U&lowast; &rarr; V&lowast;, the pullback of F, is also an isomorphism
(Proposition 2.5.5). Associated to F is a linear map&mdash;which we denote by
the same symbol&mdash;from Trs (V) to T
</p>
<p>r
s (U) defined by
</p>
<p>[
F(T)
</p>
<p>]
︸ ︷︷ ︸
&isin;Trs (U)
</p>
<p>(
θθθ1, . . . ,θθθ r ,u1, . . . ,us
</p>
<p>)
</p>
<p>&equiv; T
(
F&lowast;θθθ1, . . . ,F&lowast;θθθ r ,F&minus;1u1, . . . ,F&minus;1us
</p>
<p>)
, (26.10)
</p>
<p>where T &isin; Trs (V), θθθ i &isin;U&lowast;, and uj &isin;U. The reader may check that this map
is an algebra isomorphism (see Definition 3.1.17). We shall use this iso-
morhism to define derivatives for tensors in Chap. 28.
</p>
<p>26.2 Symmetries of Tensors
</p>
<p>Many applications demand tensors that have some kind of symmetry prop-
erty. We have already encountered a symmetric tensor&mdash;the metric &ldquo;ten-
sor&rdquo; of an inner product: If V is a vector space and v1,v2 &isin; V, then
g(v1,v2)= g(v2,v1). The following generalizes this property.
</p>
<p>Definition 26.2.1 A tensor A is symmetric in the ith and j th variables if
symmetric tensor
</p>
<p>defined
its value as a multilinear function is unchanged when these variables are
interchanged. Clearly, the two variables must be of the same kind.
</p>
<p>From this definition, it follows that in any basis, the components of a
symmetric tensor do not change when the ith and j th indices are inter-
changed.
</p>
<p>Definition 26.2.2 A tensor is contravariant-symmetric if it is symmetric
contravariant-
</p>
<p>symmetric;
</p>
<p>covariant-symmetric;
</p>
<p>symmetric
</p>
<p>in every pair of its contravariant indices and covariant-symmetric if it is
symmetric in every pair of its covariant indices. A tensor is symmetric if it
is both contravariant-symmetric and covariant-symmetric.
</p>
<p>An immediate consequence of this definition is
</p>
<p>Theorem 26.2.3 A tensor S of type (r,0) is symmetric iff for any permuta-
tion π of 1,2, . . . r , and any τττ 1,τττ 2, . . . ,τττ r in V&lowast;, we have
</p>
<p>S
(
τττπ(1),τττπ(2), . . . ,τττπ(r)
</p>
<p>)
= S
</p>
<p>(
τττ 1,τττ 2, . . . ,τττ r
</p>
<p>)</p>
<p/>
</div>
<div class="page"><p/>
<p>790 26 Tensors
</p>
<p>The set Sr(V) of all symmetric tensors of type (r,0) forms a subspace of
the vector space3 Tr0 . Similarly, the set of symmetric tensors of type (0, s)
forms a subspace Ss of T0s . The (independent) components of a symmetric
tensor A &isin; Sr are Ai1i2...ir , where i1 &le; i2 &le; &middot; &middot; &middot; &le; ir ; the other components
are given by symmetry.
</p>
<p>Although a set of symmetric tensors forms a vector space, it does not
form an algebra under the usual multiplication of tensors. In fact, even if
A=Aijei &otimes; ej and B= Bklek &otimes; el are symmetric tensors of type (2,0), the
tensor product A&otimes; B = AijBklei &otimes; ej &otimes; ek &otimes; el need not be a type (4,0)
symmetric tensor. For instance, AikBj l may not equal AijBkl . However, we
can modify the definition of the tensor product (for symmetric tensors) to
give a symmetric product out of symmetric factors.
</p>
<p>Definition 26.2.4 A symmetrizer is an operator S : Tr0 &rarr; Sr given bysymmetrizer
[
S(A)
</p>
<p>](
τττ 1, . . . ,τττ r
</p>
<p>)
= 1
</p>
<p>r!
&sum;
</p>
<p>π
</p>
<p>A
(
τττπ(1), . . . ,τττπ(r)
</p>
<p>)
, (26.11)
</p>
<p>where the sum is taken over the r! permutations of the integers 1,2, . . . , r ,
and τττ 1, . . . ,τττ r are all in V&lowast;. S(A) is often denoted by As .
</p>
<p>Clearly, As is a symmetric tensor. In fact,
</p>
<p>As
(
τττσ(1), . . . ,τττσ(r)
</p>
<p>)
=
[
S(A)
</p>
<p>](
τττσ(1), . . . ,τττσ(r)
</p>
<p>)
</p>
<p>= 1
r!
</p>
<p>&sum;
</p>
<p>π
</p>
<p>A
(
τττπ(σ(1)), . . . ,τττπ(σ(r))
</p>
<p>)
</p>
<p>= 1
r!
</p>
<p>&sum;
</p>
<p>πσ
</p>
<p>A
(
τττπσ(1), . . . ,τττπσ(r)
</p>
<p>)
</p>
<p>= As
(
τττ 1,τττ 2, . . . ,τττ r
</p>
<p>)
, (26.12)
</p>
<p>where we have used the fact that the sum over π is equal to the sum over the
product (or composition) πσ , because they both include all permutations.
Furthermore, if A is symmetric, then S(A)= A:
</p>
<p>[
S(A)
</p>
<p>](
τττ 1, . . . ,τττ r
</p>
<p>)
= 1
</p>
<p>r!
&sum;
</p>
<p>π
</p>
<p>A
(
τττπ(1), . . . ,τττπ(r)
</p>
<p>)
= 1
</p>
<p>r!
&sum;
</p>
<p>π
</p>
<p>A
(
τττ 1, . . . ,τττ r
</p>
<p>)
</p>
<p>= 1
r!
</p>
<p>(&sum;
</p>
<p>π
</p>
<p>1
</p>
<p>)
</p>
<p>︸ ︷︷ ︸
=r!
</p>
<p>A
(
τττ 1, . . . ,τττ r
</p>
<p>)
= A
</p>
<p>(
τττ 1, . . . ,τττ r
</p>
<p>)
. (26.13)
</p>
<p>A similar definition gives the symmetrizer S : T0s &rarr; Ss . Instead of
τττ 1, . . . ,τττ r in (26.11), we would have v1, . . . ,vs .
</p>
<p>3When there is no risk of confusion, we shall delete V from Trs (V), it being understood
that all tensors are defined on some given underlying vector space.</p>
<p/>
</div>
<div class="page"><p/>
<p>26.2 Symmetries of Tensors 791
</p>
<p>Example 26.2.5 For r = 2, we have only two permutations, and
</p>
<p>As
(
τττ 1,τττ 2
</p>
<p>)
= 1
</p>
<p>2
</p>
<p>[
A
(
τττ 1,τττ 2
</p>
<p>)
+A
</p>
<p>(
τττ 2,τττ 1
</p>
<p>)]
.
</p>
<p>For r = 3, we have six permutations of 1, 2, 3, and (26.11) gives
</p>
<p>As
(
τττ 1,τττ 2,τττ 3
</p>
<p>)
= 1
</p>
<p>6
</p>
<p>[
A
(
τττ 1,τττ 2,τττ 3
</p>
<p>)
+A
</p>
<p>(
τττ 2,τττ 1,τττ 3
</p>
<p>)
+A
</p>
<p>(
τττ 1,τττ 3,τττ 2
</p>
<p>)
</p>
<p>+A
(
τττ 3,τττ 1,τττ 2
</p>
<p>)
+A
</p>
<p>(
τττ 3,τττ 2,τττ 1
</p>
<p>)
+A
</p>
<p>(
τττ 2,τττ 3,τττ 1
</p>
<p>)]
.
</p>
<p>It is clear that interchanging any pair of τ &rsquo;s on the RHS of the above two
equations does not change the sum. Thus, As is indeed a symmetric tensor.
</p>
<p>It can be shown that
</p>
<p>dimSr(V)=
(
N + r &minus; 1
</p>
<p>r
</p>
<p>)
&equiv; (N + r &minus; 1)!
</p>
<p>r!(N &minus; 1)! .
</p>
<p>The proof involves counting the number of different integers i1, . . . , ir for
which 1 &le; im &le; im+1 &le;N for each m.
</p>
<p>We are now ready to define a product on the collection of symmetric
tensors and make it an algebra, called the symmetric algebra.
</p>
<p>Definition 26.2.6 The symmetric product of symmetric tensors A &isin; Sr(V)
symmetric tensors form
</p>
<p>an algebra under this
</p>
<p>product
</p>
<p>and B &isin; Ss(V) is denoted by AB and defined as
</p>
<p>AB
(
τττ 1, . . . ,τττ r+s
</p>
<p>)
&equiv; (r + s)!
</p>
<p>r!s! S(A&otimes; B)
(
τττ 1, . . . ,τττ r+s
</p>
<p>)
</p>
<p>= 1
r!s!
</p>
<p>&sum;
</p>
<p>π
</p>
<p>A
(
τττπ(1), . . . ,τττπ(r)
</p>
<p>)
B
(
τττπ(r+1), . . . ,τττπ(r+s)
</p>
<p>)
,
</p>
<p>where the sum is over all permutations of 1,2, . . . , r + s. The symmetric
product of A &isin; Sr(V) and B &isin; Ss(V) is defined similarly.
</p>
<p>Historical Notes
</p>
<p>Leopold Kronecker (1823&ndash;1891) was the son of Isidor Kronecker, a businessman, and
</p>
<p>Leopold Kronecker
</p>
<p>1823&ndash;1891
</p>
<p>Johanna Prausnitzer. They were wealthy and provided private tutoring at home for their
son until he entered the Liegnitz Gymnasium. At the gymnasium, Kronecker&rsquo;s mathemat-
ics teacher was E.E. Kummer, who early recognized the boy&rsquo;s ability and encouraged him
to do independent research. He also received Evangelical religious instruction, although
he was Jewish; he formally converted to Christianity in the last year of his life.
Kronecker matriculated at the University of Berlin in 1841, where he attended lectures
in mathematics given by Dirichlet. Like Gauss and Jacobi, he was interested in classi-
cal philology. He also attended Schelling&rsquo;s philosophy lectures; he was later to make a
thorough study of the works of Descartes, Spinoza, Leibniz, Kant, and Hegel, as well as
those of Schopenhauer, whose ideas he rejected.
Kronecker spent the summer semester of 1843 at the University of Bonn, and the fall
semester at Breslau (now Wroclaw, Poland) because Kummer had been appointed pro-
fessor there. He remained there for two semesters, returning to Berlin in the winter of
1844&ndash;1845 to take the doctorate. Kronecker took his oral examination consisting of ques-
tions not only in mathematics, but also in Greek history of legal philosophy. He was
awarded the doctorate on 10 September 1845.</p>
<p/>
</div>
<div class="page"><p/>
<p>792 26 Tensors
</p>
<p>Dirichlet, his professor and examiner, was to remain one of Kronecker&rsquo;s closest friends,
as was Kummer, his first mathematics teacher. In the meantime, in Berlin, Kronecker
was also becoming better acquainted with Eisenstein and with Jacobi. During the same
period Dirichlet introduced him to Alexander von Humboldt and to the composer Felix
Mendelssohn, who was both Dirichlet&rsquo;s brother-in-law and the cousin of Kummer&rsquo;s wife.
Family business then called Kronecker from Berlin. In its interest he was required to
spend a few years managing an estate near Liegnitz, as well as to dissolve the bank-
ing business of an uncle. In 1848 he married the latter&rsquo;s daughter, his cousin Fanny
Prausnitzer; they had six children. Having temporarily renounced an academic career,
Kronecker continued to do mathematics as a recreation. He both carried on independent
research and engaged in a lively mathematical correspondence with Kummer; he was
not ambitious for fame, and was able to enjoy mathematics as a true amateur. By 1855,
however, Kronecker&rsquo;s circumstances had changed enough to allow him to return to the
academic life in Berlin as a financially independent private scholar.
In 1860 Kummer, seconded by Borchardt and Weierstrass, nominated Kronecker to the
Berlin Academy, of which he became full member on 23 January 1861. Kronecker was
increasingly active and influential in the affairs of the Academy, particularly in recruiting
the most important German and foreign mathematicians for it. His influence outside Ger-
many also increased. He was a member of many learned societies, among them the Paris
Academy and the Royal Society of London. He established other contacts with foreign
scientists in his numerous travels abroad and in extending to them the hospitality of his
Berlin home. For this reason his advice was often solicited in regard to filling mathemat-
ical professorships both in Germany and elsewhere; his recommendations were probably
as significant as those of his erstwhile friend Weierstrass.
The cause of the growing estrangement between Kronecker and Weierstrass was partly
due to the very different temperaments of the two, and their professional and scientific
differences. Since they had long maintained the same circle of friends, their friends, too,
became involved on both levels. A characteristic incident occurred at the new year of
1884&ndash;1885, when H. A. Schwarz, who was both Weierstrass&rsquo;s student and Kummer&rsquo;s son-
in-law, sent Kronecker a greeting that included the phrase: &ldquo;He who does not honor the
Smaller [Kronecker], is not worthy of the Greater [Weierstrass].&rdquo; Kronecker read this
allusion to physical size&mdash;he was a small man, and increasingly self-conscious with age&mdash;
as a slur on his intellectual powers and broke with Schwarz completely.
Kronecker&rsquo;s mathematics lacked a systematic theoretical basis. Nevertheless, he was pre-
eminent in uniting the separate mathematical disciplines. Moreover, in certain ways&mdash;his
refusal to recognize an actual infinity, his insistence that a mathematical concept must
be defined in a finite number of steps, and his opposition to the work of Cantor and
Dedekind&mdash;his approach may be compared to that of intuitionists in the twentieth cen-
tury. Kronecker&rsquo;s mathematics thus remains influential.
</p>
<p>Example 26.2.7 Let us construct the symmetric tensor products of vectors.
First we find the symmetric product of v1 and v2 both belonging to V =
T
</p>
<p>1
0(V):
</p>
<p>(v1v2)
(
τττ 1,τττ 2
</p>
<p>)
&equiv; v1
</p>
<p>(
τττ 1
</p>
<p>)
v2
(
τττ 2
</p>
<p>)
+ v1
</p>
<p>(
τττ 2
</p>
<p>)
v2
(
τττ 1
</p>
<p>)
</p>
<p>= v1
(
τττ 1
</p>
<p>)
v2
(
τττ 2
</p>
<p>)
+ v2
</p>
<p>(
τττ 1
</p>
<p>)
v1
(
τττ 2
</p>
<p>)
</p>
<p>= (v1 &otimes; v2 + v2 &otimes; v1)
(
τττ 1,τττ 2
</p>
<p>)
.
</p>
<p>Since this is true for any pair τττ 1 and τττ 2, we have
</p>
<p>v1v2 = v1 &otimes; v2 + v2 &otimes; v1.
</p>
<p>In general, v1v2 &middot; &middot; &middot;vr =
&sum;
</p>
<p>π vπ(1) &otimes; vπ(2) &otimes; &middot; &middot; &middot; &otimes; vπ(r).
</p>
<p>It is clear from the definition that symmetric multiplication is commu-
tative, associative, and distributive. If we choose a basis {ei}Ni=1 for V and</p>
<p/>
</div>
<div class="page"><p/>
<p>26.2 Symmetries of Tensors 793
</p>
<p>express all symmetric tensors in terms of symmetric products of ei using
the above properties, then any symmetric tensor can be expressed as a linear
combination of terms of the form (e1)n1 &middot; &middot; &middot; (eN )nN .
</p>
<p>Skew-symmetry or antisymmetry is the same as symmetry except that
in the interchange of variables the tensor changes sign.
</p>
<p>Definition 26.2.8 A covariant (contravariant) skew-symmetric (or anti-
covariant and
</p>
<p>contravariant
</p>
<p>skew-symmetric tensors
</p>
<p>symmetric) tensor is one that is skew-symmetric in all pairs of covariant
(contravariant) variables. A tensor is skew-symmetric if it is both covariant
and contravariant skew-symmetric.
</p>
<p>The analogue of Theorem 26.2.3 is
</p>
<p>Theorem 26.2.9 A tensor A of type (r,0) is skew iff for any permutation π
of 1,2, . . . r , and any τττ 1,τττ 2, . . . ,τττ r in V&lowast;, we have
</p>
<p>A
(
τττπ(1),τττπ(2), . . . ,τττπ(r)
</p>
<p>)
= ǫπA
</p>
<p>(
τττ 1,τττ 2, . . . ,τττ r
</p>
<p>)
.
</p>
<p>Definition 26.2.10 An antisymmetrizer is a linear operator A on Tr0 , given antisymmetrizer
by
</p>
<p>[
A(T)
</p>
<p>](
τττ 1, . . . ,τττ r
</p>
<p>)
= 1
</p>
<p>r!
&sum;
</p>
<p>π
</p>
<p>ǫπT
(
τττπ(1), . . . ,τττπ(r
</p>
<p>)
. (26.14)
</p>
<p>A(T) is often denoted by Ta .
</p>
<p>Clearly, Ta is an antisymmetric tensor. In fact, using (ǫσ )2 = 1, which
holds for any permutation, we have
</p>
<p>Ta
(
τττσ(1), . . . ,τττσ(r)
</p>
<p>)
=
[
A(T)
</p>
<p>](
τττσ(1), . . . ,τττσ(r)
</p>
<p>)
</p>
<p>= (ǫσ )2
1
</p>
<p>r!
&sum;
</p>
<p>π
</p>
<p>ǫπA
(
τττπσ(1), . . . ,τττπσ(r)
</p>
<p>)
</p>
<p>= ǫσ
1
</p>
<p>r!
&sum;
</p>
<p>πσ
</p>
<p>ǫπǫσT
(
τττπσ(1), . . . ,τττπσ(r)
</p>
<p>)
</p>
<p>= ǫσTa
(
τττ 1,τττ 2, . . . ,τττ r
</p>
<p>)
, (26.15)
</p>
<p>where we have used the fact that ǫπǫσ = ǫπσ as can be easily verified. If T
is antisymmetric, then A(T)= T:
[
A(T)
</p>
<p>](
τττ 1, . . . ,τττ r
</p>
<p>)
= 1
</p>
<p>r!
&sum;
</p>
<p>π
</p>
<p>ǫπT
(
τττπ(1), . . . ,τττπ(r)
</p>
<p>)
</p>
<p>= 1
r!
</p>
<p>&sum;
</p>
<p>π
</p>
<p>=1︷ ︸︸ ︷
(ǫπ )
</p>
<p>2 T
(
τττ 1, . . . ,τττ r
</p>
<p>)
</p>
<p>= 1
r!
</p>
<p>(&sum;
</p>
<p>π
</p>
<p>1
</p>
<p>)
</p>
<p>︸ ︷︷ ︸
=r!
</p>
<p>T
(
τττ 1, . . . ,τττ r
</p>
<p>)
= T
</p>
<p>(
τττ 1, . . . ,τττ r
</p>
<p>)
. (26.16)</p>
<p/>
</div>
<div class="page"><p/>
<p>794 26 Tensors
</p>
<p>A similar definition gives the antisymmetrizer A on T0s . Instead of
τττ 1, . . . ,τττ r in (26.14), we would have v1, . . . ,vs .
</p>
<p>Example 26.2.11 Let us write out Eq. (26.14) for r = 3. The procedure is
entirely analogous to Example 26.2.5:
</p>
<p>Ta
(
τττ 1,τττ 2,τττ 3
</p>
<p>)
= 1
</p>
<p>6
</p>
<p>[
ǫ123A
</p>
<p>(
τττ 1,τττ 2,τττ 3
</p>
<p>)
+ ǫ213A
</p>
<p>(
τττ 2,τττ 1,τττ 3
</p>
<p>)
</p>
<p>+ ǫ132A
(
τττ 1,τττ 3,τττ 2
</p>
<p>)
+ ǫ312A
</p>
<p>(
τττ 3,τττ 1,τττ 2
</p>
<p>)
</p>
<p>+ ǫ321A
(
τττ 3,τττ 2,τττ 1
</p>
<p>)
+ ǫ231A
</p>
<p>(
τττ 2,τττ 3,τττ 1
</p>
<p>)]
</p>
<p>= 1
6
</p>
<p>[
A
(
τττ 1,τττ 2,τττ 3
</p>
<p>)
&minus;A
</p>
<p>(
τττ 2,τττ 1,τττ 3
</p>
<p>)
&minus;A
</p>
<p>(
τττ 1,τττ 3,τττ 2
</p>
<p>)
</p>
<p>+A
(
τττ 3,τττ 1,τττ 2
</p>
<p>)
&minus;A
</p>
<p>(
τττ 3,τττ 2,τττ 1
</p>
<p>)
+A
</p>
<p>(
τττ 2,τττ 3,τττ 1
</p>
<p>)]
.
</p>
<p>The reader may easily verify that all terms with a plus sign are obtained
from (123) by an even number of interchanges of symbols, and those with a
minus sign by an odd number.
</p>
<p>26.3 Exterior Algebra
</p>
<p>The following discussion will concentrate on tensors of type (r,0). How-
ever, interchanging the roles of V and V&lowast; makes all definitions, theorems,
propositions, and conclusions valid for tensors of type (0, s) as well.
</p>
<p>The set of all skew-symmetric tensors of type (p,0) forms a subspace of
T
p
</p>
<p>0 (V). This subspace is denoted by Λ
p(V&lowast;) and its members are called p-
</p>
<p>vectors.4 It is not, however, an algebra unless we define a skew-symmetricΛp(V&lowast;) and p-vectors
product analogous to that for the symmetric case. This is done in the follow-
ing definition:
</p>
<p>Definition 26.3.1 The exterior product (also called the wedge, Grass-exterior product defined
mann, alternating, or veck product) of two skew-symmetric tensors A &isin;
Λp(V&lowast;) and B &isin;Λq(V&lowast;) is a skew-symmetric tensor belonging to Λp+q(V&lowast;)
and given by 5
</p>
<p>A&and; B&equiv; (r + s)!
r!s! A(A&otimes; B)=
</p>
<p>(r + s)!
r!s! (A&otimes; B)a .
</p>
<p>4The use of V&lowast; in Λp(V&lowast;) is by convention. Since a member of Λp(V&lowast;) acts on p dual
vectors, it is more natural to use V&lowast;.
5The reader should be warned that different authors may use different numerical coeffi-
cients in the definition of the exterior product.</p>
<p/>
</div>
<div class="page"><p/>
<p>26.3 Exterior Algebra 795
</p>
<p>More explicitly,
</p>
<p>A&and; B
(
τττ 1, . . . ,τττ r+s
</p>
<p>)
</p>
<p>= 1
r!s!
</p>
<p>&sum;
</p>
<p>π
</p>
<p>ǫπA
(
τττπ(1), . . . ,τττπ(r)
</p>
<p>)
B
(
τττπ(r+1), . . . ,τττπ(r+s)
</p>
<p>)
.
</p>
<p>Example 26.3.2 Let us find the exterior product of v1 and v2 both belong-
ing to V= T10(V), so that r = s = 1:
</p>
<p>v1 &and; v2
(
τττ 1,τττ 2
</p>
<p>)
=
&sum;
</p>
<p>π
</p>
<p>ǫπv1
(
τττπ(1)
</p>
<p>)
v2
(
τττπ(2)
</p>
<p>)
</p>
<p>= v1
(
τττ 1
</p>
<p>)
v2
(
τττ 2
</p>
<p>)
&minus; v1
</p>
<p>(
τττ 2
</p>
<p>)
v2
(
τττ 1
</p>
<p>)
</p>
<p>= (v1 &otimes; v2 &minus; v2 &otimes; v1)
(
τττ 1,τττ 2
</p>
<p>)
.
</p>
<p>Since this is true for arbitrary τττ 1 and τττ 2, we have
</p>
<p>v1 &and; v2 = v1 &otimes; v2 &minus; v2 &otimes; v1 = 2!A(v1 &otimes; v2).
</p>
<p>The result of the example above can be generalized to
</p>
<p>v1 &and; &middot; &middot; &middot; &and; vr = r!A(v1 &otimes; &middot; &middot; &middot; &otimes; vr)=
&sum;
</p>
<p>π
</p>
<p>ǫπvπ(1) &otimes; &middot; &middot; &middot; &otimes; vπ(r). (26.17)
</p>
<p>In particular, this shows that the exterior product (of vectors) is associative.
If {ej }Nj=1 is a basis with dual {ǫǫǫi}Ni=1, then Eq. (26.17) gives
</p>
<p>e1 &and; &middot; &middot; &middot; &and; eN
(
ǫǫǫi1, . . . ,ǫǫǫiN
</p>
<p>)
=
&sum;
</p>
<p>π
</p>
<p>ǫπδ
i1
π(1) &middot; &middot; &middot; δ
</p>
<p>iN
π(N) = ǫi1,...,iN . (26.18)
</p>
<p>The last equality follows from the fact that the sum is zero unless i1, . . . , iN
is a permutation of 1, . . . ,N and it is 1 if the permutation is even and &minus;1 if
it is odd. We obtain the same result if we switch the e&rsquo;s and the ǫǫǫ&rsquo;s:
</p>
<p>ǫǫǫ1 &and; &middot; &middot; &middot; &and; ǫǫǫN (ei1 , . . . , eiN )=
&sum;
</p>
<p>π
</p>
<p>ǫπδ
π(1)
i1
</p>
<p>&middot; &middot; &middot; δπ(N)iN = ǫi1,...,iN . (26.19)
</p>
<p>Another useful result is obtained when the indices of the last equation are
switched:
</p>
<p>ǫǫǫi1 &and; &middot; &middot; &middot; &and; ǫǫǫiN (e1, . . . , eN )=
&sum;
</p>
<p>π
</p>
<p>ǫπδ
π(i1)
1 &middot; &middot; &middot; δ
</p>
<p>π(iN )
N .
</p>
<p>Now note that
</p>
<p>δ
π(ik)
k &lArr;&rArr; π(ik)= k &lArr;&rArr; ik = π&minus;1(k) &lArr;&rArr; δ
</p>
<p>π(ik)
k = δ
</p>
<p>ik
π&minus;1(k).
</p>
<p>Furthermore,
&sum;
</p>
<p>π =
&sum;
</p>
<p>π&minus;1 and ǫπ = ǫπ&minus;1 . Denoting π&minus;1 by σ , the equation
above gives
</p>
<p>ǫǫǫi1 &and; &middot; &middot; &middot; &and; ǫǫǫiN (e1, . . . , eN )=
&sum;
</p>
<p>σ
</p>
<p>ǫσ δ
i1
σ(1) &middot; &middot; &middot; δ
</p>
<p>iN
σ(N) = ǫi1,...,iN . (26.20)</p>
<p/>
</div>
<div class="page"><p/>
<p>796 26 Tensors
</p>
<p>The following theorem contains the properties of the exterior product (for
a proof, see [Abra 88, p. 394]):
</p>
<p>Theorem 26.3.3 The exterior product is associative and distributive
with respect to the addition of tensors. Furthermore, it satisfies the
following anticommutativity property:
</p>
<p>A&and; B= (&minus;1)pqB&and;A
</p>
<p>whenever A &isin; Λp(V&lowast;) and B &isin; Λq(V&lowast;). In particular, v1 &and; v2 =
&minus;v2 &and; v1 for v1,v2 &isin; V.
</p>
<p>The wedge product of linear functionals of a vector space is particularly
important in the analysis of tensors, as we shall see in the next chapter.
</p>
<p>Definition 26.3.4 The elements of Λp(V) are called p-forms.p-forms defined
</p>
<p>A linear transformation T : V &rarr; W induces a transformation6 T&lowast; :
Λp(W)&rarr;Λp(V) defined bypull-back of p-forms by
</p>
<p>linear transformations (
T&lowast;ρρρ
</p>
<p>)
(v1, . . . ,vp)&equiv; ρρρ(Tv1, . . . ,Tvp), ρρρ &isin;Λp(W), vi &isin; V. (26.21)
</p>
<p>T&lowast;ρρρ is called the pullback of ρρρ by T. The most important properties of
pullback maps are given in the following:
</p>
<p>Proposition 26.3.5 Let T : V&rarr;W and S :W&rarr;U. Then
1. T&lowast; :Λp(W)&rarr;Λp(V) is linear.
2. (S ◦ T)&lowast; = T&lowast; ◦ S&lowast;.
3. If T is the identity map, so is T&lowast;.
4. If T is an isomorphism, so is T&lowast; and (T&lowast;)&minus;1 = (T&minus;1)&lowast;.
5. If ρρρ &isin;Λp(W) and σσσ &isin;Λq(W), then T&lowast;(ρρρ &and;σσσ)= T&lowast;ρρρ &and; T&lowast;σσσ .
</p>
<p>Proof The proof follows directly from definitions and is left as an exercise
for the reader. �
</p>
<p>If {ei}Ni=1 is a basis of V, we can form a basis for Λp(V&lowast;) by constructing
all products of the form ei1 &and;ei2 &and;&middot; &middot; &middot;&and;eip . The number of linearly indepen-
dent such vectors, which is the dimension of Λp(V&lowast;), is equal to the number
of ways p numbers can be chosen from among N distinct numbers in such
a way that no two of them are equal. This is simply the combination of N
objects taken p at a time. Thus, we have
</p>
<p>dimΛp
(
V
&lowast;)=
</p>
<p>(
N
</p>
<p>p
</p>
<p>)
= N !
</p>
<p>p!(N &minus; p)! . (26.22)
</p>
<p>6Note that T&lowast; is the extension of the pullback operator introduced at the end of Chap. 2.</p>
<p/>
</div>
<div class="page"><p/>
<p>26.3 Exterior Algebra 797
</p>
<p>In particular, dimΛN (V&lowast;)= 1.
Any A &isin;Λp(V&lowast;) can be written as
</p>
<p>A=
N&sum;
</p>
<p>i1&lt;i2&lt;&middot;&middot;&middot;&lt;ip
Ai1...ipei1 &and; ei2 &and; &middot; &middot; &middot; &and; eip
</p>
<p>= 1
p!
</p>
<p>N&sum;
</p>
<p>i1,i2,...,ip
</p>
<p>Ai1...ipei1 &and; ei2 &and; &middot; &middot; &middot; &and; eip (26.23)
</p>
<p>where Ai1...ip are the components of A, which are assumed completely anti-
symmetric in all i1, i2, . . . , ip . In the second sum, all i&rsquo;s run from 1 to N . Exterior algebra defined.
</p>
<p>Theorem 26.3.6 Set Λ0(V&lowast;) = R and let Λ(V&lowast;) denote the direct
sum of all Λp(V&lowast;):
</p>
<p>Λ
(
V
&lowast;)=
</p>
<p>N&oplus;
</p>
<p>p=0
Λp
</p>
<p>(
V
&lowast;)&equiv;R&oplus;V&oplus;Λ2
</p>
<p>(
V
&lowast;)&oplus; &middot; &middot; &middot; &oplus;ΛN
</p>
<p>(
V
&lowast;).
</p>
<p>Then Λ(V&lowast;) is a 2N -dimensional algebra with exterior product defin-
ing its multiplication rule.
</p>
<p>Proof The only thing to prove is the dimensionality of the algebra, which is
an easy consequence of Eq. (26.22) and the binomial expansion of (1+1)N :
</p>
<p>2N = (1 + 1)N =
N&sum;
</p>
<p>p=0
</p>
<p>(
N
</p>
<p>p
</p>
<p>)
1p1N&minus;p =
</p>
<p>N&sum;
</p>
<p>p=0
</p>
<p>(
N
</p>
<p>p
</p>
<p>)
.
</p>
<p>�
</p>
<p>Given two vector spaces V and U, one can construct a tensor product
of Λ(V&lowast;) and Λ(U&lowast;) and define a product ⊙ on it as follows. Let Ai &isin;
Λpi (V&lowast;), i = 1,2 and Bj &isin;Λqj (U&lowast;), j = 1,2. Then
</p>
<p>(A1 &otimes; B1)⊙ (A2 &otimes; B2)&equiv; (&minus;1)p2q1(A1 &and;A2)&otimes; (B1 &and; B2). (26.24)
</p>
<p>Definition 26.3.7 The tensor product of the two vector spaces
Λ(V&lowast;) and Λ(U&lowast;) together with the product given in Eq. (26.24)
is called skew tensor product of Λ(V&lowast;) and Λ(U&lowast;) and denoted by
Λ(V&lowast;)&otimes;̂Λ(U&lowast;).
</p>
<p>An elegant way of determining the linear independence of vectors using
the formalism developed so far is given in the following proposition.
</p>
<p>Proposition 26.3.8 A set of vectors, v1, . . . ,vp , is linearly independent if
and only if v1 &and; &middot; &middot; &middot; &and; vp �= 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>798 26 Tensors
</p>
<p>Proof If {vi}pi=1 are independent, then they span a p-dimensional sub-
space M of V. Considering M as a vector space in its own right, we have
dimΛp(M&lowast;)= 1. A basis for Λp(M&lowast;) is simply v1&and;&middot; &middot; &middot;&and;vp , which cannot
be zero.
</p>
<p>Conversely, suppose that α1v1+&middot; &middot; &middot;+αpvp = 0. Then taking the exterior
product of the LHS with v2 &and; v3 &and; &middot; &middot; &middot; &and; vp makes all terms vanish (because
each will have two factors of a vector in the wedge product) except the first
one. Thus, we have α1v1 &and; &middot; &middot; &middot; &and; vp = 0. The fact that the wedge product is
not zero forces α1 to be zero. Similarly, multiplying by v1 &and; v3 &and; &middot; &middot; &middot; &and; vp
shows that α2 = 0, and so on. �
</p>
<p>Example 26.3.9 Let {ei}Ni=1 be a basis for V. Let v1 = e1 + 2e2 &minus; e3, v2 =
3e1 + e2 + 2e3, v3 =&minus;e1 &minus; 3e2 + 2e3.
</p>
<p>Take the wedge product of the first two v&rsquo;s:
</p>
<p>v1 &and; v2 = (e1 + 2e2 &minus; e3)&and; (3e1 + e2 + 2e3)
=&minus;5e1 &and; e2 + 5e1 &and; e3 + 5e2 &and; e3.
</p>
<p>All the wedge products that have repeated factors vanish. Now we multiply
by v3:
</p>
<p>v1 &and; v2 &and; v3 =&minus;5e1 &and; e2 &and; (&minus;e1 &minus; 3e2 + 2e3)
+ 5e1 &and; e3 &and; (&minus;e1 &minus; 3e2 + 2e3)
+ 5e2 &and; e3 &and; (&minus;e1 &minus; 3e2 + 2e3)
</p>
<p>=&minus;10e1 &and; e2 &and; e3 &minus; 15e1 &and; e3 &and; e2 &minus; 5e2 &and; e3 &and; e1 = 0.
</p>
<p>We conclude that the three vectors are linearly dependent.
</p>
<p>As an application of Proposition 26.3.8, let us prove the following.
</p>
<p>Theorem 26.3.10 (Cartan&rsquo;s lemma) Suppose that {ei}pi=1, p &le; dimV, formCartan&rsquo;s lemma
a linearly independent set of vectors in V and that {vi}pi=1 are also vectors
in V such that
</p>
<p>&sum;p
i=1 ei &and;vi = 0. Then all vi are linear combinations of only
</p>
<p>the set {ei}pi=1. Furthermore, if vi =
&sum;p
</p>
<p>j=1 Aijej , then Aij =Aji .
</p>
<p>Proof Multiplying both sides of
&sum;p
</p>
<p>i=1 ei &and; vi = 0 by e2 &and; &middot; &middot; &middot; &and; ep gives
</p>
<p>&minus;v1 &and; e1 &and; e2 &and; &middot; &middot; &middot; &and; ep = 0.
</p>
<p>By Proposition 26.3.8, v1 and the ei are linearly dependent. Similarly, by
multiplying
</p>
<p>&sum;p
i=1 ei &and; vi = 0 by the wedge product with ek missing, we
</p>
<p>show that vk and the ei are linearly dependent. Thus, vk =
&sum;p
</p>
<p>i=1 Akiei , for
all k. Furthermore, we have
</p>
<p>0 =
p&sum;
</p>
<p>k=1
ek &and; vk =
</p>
<p>p&sum;
</p>
<p>k=1
</p>
<p>p&sum;
</p>
<p>i=1
ek &and; (Akiei)=
</p>
<p>&sum;
</p>
<p>k&lt;i
</p>
<p>(Aki &minus;Aik)ek &and; ei,</p>
<p/>
</div>
<div class="page"><p/>
<p>26.3 Exterior Algebra 799
</p>
<p>where the last sum is over both k and i with k &lt; i. Clearly, {ek&and;ei} with k &lt;
i are linearly independent (show this!). Therefore, their coefficients must
vanish. �
</p>
<p>Historical Notes
</p>
<p>Elie Joseph Cartan (1869&ndash;1951), born in Dolomieu (near Chamb&eacute;ry), Savoie, Rh&ocirc;ne-
</p>
<p>Elie Joseph Cartan
</p>
<p>1869&ndash;1951
</p>
<p>Alpes, France, became a student at the Ecole Normale in 1888 and obtained his doctor-
ate in 1894. He lectured at Montpellier (1894&ndash;1896), Lyon (1896&ndash;1903), Nancy (1903&ndash;
1909), and Paris (1909&ndash;1940). He had four children, one of whom, Henri Cartan, was
to produce brilliant work in mathematics. Two others died tragically. Jean, a composer,
died at the age of 25, while Louis, a physicist, was arrested by the Germans in 1942 and
executed after 15 months in captivity.
Cartan added greatly to the theory of continuous groups, which had been initiated by Lie.
His thesis (1894) contains a major contribution to Lie algebras wherein he completed
the classification of the semi-simple algebras that Killing had essentially found. He then
turned to the theory of associative algebras and investigated the structure for these alge-
bras over the real and complex fields. Wedderburn would complete Cartan&rsquo;s work in this
area.
He then turned to representations of semisimple Lie groups. His work is a striking synthe-
sis of Lie theory, classical geometry, differential geometry, and topology, which was to be
found in all Cartan&rsquo;s work. He also applied Grassmann algebra to the theory of exterior
differential forms.
By 1904 Cartan was turning to papers on differential equations, and from 1916 on he
published mainly on differential geometry. Klein&rsquo;s Erlanger Program was seen to be in-
adequate as a general description of geometry by Weyl and Veblen, and Cartan was to
play a major role. He examined a space acted on by an arbitrary Lie group of transfor-
mations, developing a theory of moving frames that generalizes the kinematical theory of
Darboux.
Cartan further contributed to geometry with his theory of symmetric spaces, which have
their origins in papers he wrote in 1926. It develops ideas first studied by Clifford and Cay-
ley and used topological methods developed by Weyl in 1925. This work was completed
by 1932.
Cartan then went on to examine problems on a topic first studied by Poincar&eacute;. By this
stage his son, Henri Cartan, was making major contributions to mathematics, and Elie
Cartan was able to build on theorems proved by his son.
Cartan also published work on relativity and the theory of spinors. He is certainly one of
the most important mathematicians of the first half of the twentieth century.
</p>
<p>Example 26.3.11 The symbol ǫi1i2...iN , called the Levi-Civita tensor, can Levi-Civita tensor and
determinantsbe defined by
</p>
<p>ǫǫǫi1 &and; &middot; &middot; &middot; &and; ǫǫǫiN = ǫi1...iNǫǫǫ1 &and; &middot; &middot; &middot; &and; ǫǫǫN . (26.25)
</p>
<p>In fact, substituting (e1, . . . , e1) on both sides and using Eq. (26.20), the
uniqueness theorem 2.6.4 proves the equality in (26.25).
</p>
<p>Now consider the linear operator E whose action on a basis {ei}Ni=1 is to
permute the vectors so that Eek = eik . Denote the left-hand side of (26.25)
by ���&prime; (a determinant function as defined in Chap. 2) and the N -form on the
right-hand side by ���. Now note that
</p>
<p>���&prime;E = detE &middot;���&prime; = detE &middot; ǫi1...iN���.
</p>
<p>Evaluate both sides on (e1, . . . , eN ) and convince yourself that both deter-
minant functions give 1. This yields 1 = detE &middot; ǫi1...iN . Since (ǫi1...iN )2 = 1,
multiplying both sides by the Levi-Civita tensor, we get detE= ǫi1i2...iN .</p>
<p/>
</div>
<div class="page"><p/>
<p>800 26 Tensors
</p>
<p>Since the determinant is basis-independent, the result of the previous ex-
ample can be summarized as follows:
</p>
<p>Box 26.3.12 The Levi-Civita tensor ǫi1i2...iN takes the same value in
all coordinate systems.
</p>
<p>There is a generalization of Λp(V) that is useful when we discuss Clif-
ford algebras in Chap. 27:
</p>
<p>Definition 26.3.13 A U-valued p-form, is a linear machine that takes p
vectors from V and produces a vector in U. The space of U-valued p-forms
is denoted by Λp(V,U). In this new context, Λp(V)=Λp(V,R).
</p>
<p>26.3.1 Orientation
</p>
<p>The reader is no doubt familiar with the right-handed and left-handed co-
ordinate systems in R3. In this section, we generalize the idea to arbitrary
vector spaces.
</p>
<p>Definition 26.3.14 An oriented basis of an N -dimensional vector space isoriented basis defined
an ordered collection of N linearly independent vectors.
</p>
<p>If {vi}Ni=1 is one oriented basis and {ui}Ni=1 is a second one, then
</p>
<p>u1 &and; u2 &and; &middot; &middot; &middot; &and; uN = (det R)v1 &and; v2 &and; &middot; &middot; &middot; &and; vN ,
</p>
<p>where R is the transformation matrix and det R is a nonzero number (R is
invertible), which can be positive or negative. Accordingly, we have the fol-
lowing definition.
</p>
<p>Definition 26.3.15 An orientation is the collection of all oriented basesoriented vector spaces
defined related by a transformation matrix having a positive determinant. A vector
</p>
<p>space for which an orientation is specified is called an oriented vector space.
</p>
<p>Clearly, there are only two orientations in any vector space. Each oriented
basis is positively related to any oriented basis belonging to the same ori-
entation and negatively related to any oriented basis belonging to the other
orientation. For example, in R3, the bases {ex, ey, ez} and {ey, ex, ez} belong
to different orientations because
</p>
<p>ex &and; ey &and; ez =&minus;ey &and; ex &and; ez.
</p>
<p>The first basis is (by convention) called a right-handed coordinate system,
and the second is called a left-handed coordinate system. Any other basis
is either right-handed or left-handed. There is no third alternative!</p>
<p/>
</div>
<div class="page"><p/>
<p>26.4 Symplectic Vector Spaces 801
</p>
<p>Definition 26.3.16 Let V be a vector space. Let V&lowast; have the oriented basis
{ǫǫǫi}Ni=1. The oriented volume element μμμ &isin;ΛN (V) of V is defined as volume element of a
</p>
<p>vector space
μμμ&equiv; ǫǫǫ1 &and; ǫǫǫ2 &and; &middot; &middot; &middot; &and; ǫǫǫN .
</p>
<p>Note that if {ei} is ordered as {ǫǫǫj }, then μμμ(e1, e2, . . . , eN )=+1/N !, and
we say that {ei} is positively oriented with respect to μμμ. In general, {vi} is
positively oriented with respect to μμμ if μμμ(v1,v2, . . . ,vN ) &gt; 0.
</p>
<p>The volume element of V is defined in terms of a basis for V&lowast;. The reason
for this will become apparent later, when we see that dx, dy, and dz form a
basis for (R3)&lowast;, and dx dy dz&equiv; dx &and; dy &and; dz.
</p>
<p>positive orientation
</p>
<p>26.4 Symplectic Vector Spaces
</p>
<p>Mechanics was a great contributor to the development of tensor analysis. It
provided examples of manifolds that went beyond mere subspaces of Rn.
The phase space of Hamiltonian mechanics is a paradigm of manifolds that
are not &ldquo;hypersurfaces&rdquo; of some Euclidean space. We shall have more to
say about such manifolds in Chap. 28. Here, we shall be content with the
algebraic structure underlying classical mechanics.
</p>
<p>Definition 26.4.1 A 2-form ωωω &isin;Λ2(V) is nondegenerate if ωωω(v1,v2)= 0 symplectic form,
symplectic vector space,
</p>
<p>and symplectic
</p>
<p>transformation
</p>
<p>for all v1 &isin; V implies v2 = 0. A symplectic form on V is a nondegenerate
2-form ωωω &isin; Λ2(V). The pair (V,ωωω) is called a symplectic vector space.
If (V,ωωω) and (W,ρρρ) are symplectic vector spaces, a linear transformation
T : V&rarr;W is called a symplectic transformation or a symplectic map if
T&lowast;ρρρ =ωωω.
</p>
<p>Any 2-form (degenerate or nondegenerate) leads to other quantities that
are also of interest. For instance, given any basis {vi} in V, one defines the
matrix of the 2-form ωωω &isin; Λ2(V) by ωij &equiv; ωωω(vi,vj ). Similarly, one can
define the useful linear map ωωω♭ : V&rarr; V&lowast; by
</p>
<p>The flat ♭ and sharp ♯
</p>
<p>maps defined
</p>
<p>&isin;R︷ ︸︸ ︷[
ωωω♭(v)
</p>
<p>]
︸ ︷︷ ︸
</p>
<p>&isin;V&lowast;
</p>
<p>v&prime; &equiv;ωωω
(
v,v&prime;
</p>
<p>)
. (26.26)
</p>
<p>The rank of ωωω♭ is called the rank of ωωω. The reader may check that rank of a symplectic form
</p>
<p>Box 26.4.2 A 2-form ωωω is nondegenerate if and only if the determi-
nant of (ωij ) is nonzero, if and only if ωωω♭ is an isomorphism, in which
case the inverse of ωωω♭ is denoted by ωωω♯.
</p>
<p>Proposition 26.4.3 Let V be an N -dimensional vector space and ωωω &isin; canonical basis of a
symplectic vector spaceΛ2(V). If the rank of ωωω is r , then r = 2n for some integer n and there exists</p>
<p/>
</div>
<div class="page"><p/>
<p>802 26 Tensors
</p>
<p>a basis {ei} of V, called a canonical basis of V, and a dual basis {ǫǫǫj }, such
that ωωω =&sum;nj=1 ǫǫǫj &and; ǫǫǫj+n, or, equivalently, the N &times;N matrix of ωωω is given
by
</p>
<p>⎛
⎝
</p>
<p>0 1 0
&minus;1 0 0
0 0 0
</p>
<p>⎞
⎠
</p>
<p>where 1 is the n&times; n identity matrix and 0 is the (N &minus; 2n)&times; (N &minus; 2n) zero
matrix.
</p>
<p>Proof Since ωωω �= 0, there exist a pair of vectors e1, e&prime;1 &isin; V such that
ωωω(e1, e&prime;1) �= 0. Dividing e1 by a constant, we can assume ωωω(e1, e&prime;1)= 1. Be-
cause of its antisymmetry, the matrix of ωωω in the plane P1 spanned by e1
and e&prime;1 is
</p>
<p>( 0 1
&minus;1 0
</p>
<p>)
. Let V1 be the ωωω-orthogonal complement of P1, i.e.,
</p>
<p>V1 =
{
v &isin; V |ωωω(v,v1)= 0 &forall;v1 &isin; P1
</p>
<p>}
.
</p>
<p>Then the reader may check that P1 &cap; V1 = 0. Moreover, V = P1 + V1 be-
cause
</p>
<p>v =ωωω
(
v, e&prime;1
</p>
<p>)
e1 &minus;ωωω(v, e1)e&prime;1︸ ︷︷ ︸
&isin;P1
</p>
<p>+v &minus;ωωω
(
v, e&prime;1
</p>
<p>)
e1 +ωωω(v, e1)e&prime;1︸ ︷︷ ︸
</p>
<p>&isin;V1(Reader, verify!)
</p>
<p>for any v &isin; V. Thus, V= P1 &oplus;V1. If ωωω is zero on all pairs of vectors in V1,
then we are done, and the rank of ωωω is 2; otherwise, let e2, e&prime;2 &isin; V1 be such
that ωωω(e2, e&prime;2) �= 0. Proceeding as above, we obtain
</p>
<p>V1 = P2 &oplus;V2 &rArr; V= P1 &oplus;P2 &oplus;V2,
</p>
<p>where P2 is the plane spanned by e2, and e&prime;2 and V2 its ωωω-orthogonal com-
plement in V1. Continuing this process yields
</p>
<p>V= P1 &oplus;P2 &oplus; &middot; &middot; &middot; &oplus;Pn &oplus;Vn,
</p>
<p>where Vn is the subspace of V on which ωωω is zero. This shows that the rank
ofωωω is 2n. By reordering the basis vectors such that e&prime;k &equiv; en+k , we construct
a new basis {ei}Ni=1 in which ωωω has the desired matrix.
</p>
<p>To conclude the proposition, it is sufficient to show that
&sum;n
</p>
<p>j=1 ǫǫǫ
j &and;ǫǫǫj+n,
</p>
<p>in which {ǫǫǫj }Nj=1 is dual to {ei}Ni=1, has the same matrix as ωωω. This is left as
an exercise for the reader. �
</p>
<p>We note that in the canonical basis,
</p>
<p>ωij =
</p>
<p>⎧
⎪⎨
⎪⎩
</p>
<p>0 if i, j &le; n,
δik if j = n+ k, k &le; n,
0 if i &ge; 2n or j &ge; 2n.</p>
<p/>
</div>
<div class="page"><p/>
<p>26.4 Symplectic Vector Spaces 803
</p>
<p>If we write v &isin; V as v =&sum;ni=1(xiei + yien+i)+
&sum;N&minus;2n
</p>
<p>i=1 zie2n+i in the
canonical basis of V, with a corresponding expansion for v&prime;, then the reader
may verify that
</p>
<p>ωωω
(
v,v&prime;
</p>
<p>)
=
</p>
<p>n&sum;
</p>
<p>i=1
</p>
<p>(
xiy
</p>
<p>&prime;
i &minus; x&prime;iyi
</p>
<p>)
.
</p>
<p>The following proposition gives a useful criterion for nondegeneracy
of ωωω:
</p>
<p>Proposition 26.4.4 Let ωωω be a 2-form in the finite-dimensional vector
space V. Then ωωω is nondegenerate iff V has even dimension, say 2n, and
ωωωn &equiv;ωωω &and; &middot; &middot; &middot; &and;ωωω is a volume element of V.
</p>
<p>Proof Supposeωωω is nondegenerate. Then,ωωω♭ is an isomorphism. Therefore,
the rank of ωωω, an even number by Proposition 26.4.3, must equal dimV&lowast; =
dimV. Moreover, by taking successive powers of ωωω and using mathematical
induction, one can show that ωωωn is proportional to ǫǫǫ1 &and; &middot; &middot; &middot; &and; ǫǫǫ2n.
</p>
<p>Conversely, if ωωωn &prop; ǫǫǫ1 &and; &middot; &middot; &middot; &and; ǫǫǫ2n is a volume element, then by Propo-
sition 26.3.8, the {ǫǫǫj } are linearly independent. Furthermore, dimV&lowast; must
equal the number of linearly independent factors in the wedge product of
a volume element. Thus, dimV&lowast; = 2n. But 2n is also the rank of ωωω. It fol-
lows that ωωω♭ is onto. Since V is finite-dimensional, the dimension theorem
implies that ωωω♭ is an isomorphism. �
</p>
<p>Example 26.4.5 Let V be a vector space and V&lowast; its dual. The direct sum
V&oplus;V&lowast; can be turned into a symplectic vector space if we defineωωω &isin;Λ2(V&oplus;
V&lowast;) by
</p>
<p>ωωω
(
v +ϕϕϕ,v&prime; +ϕϕϕ&prime;
</p>
<p>)
&equiv;ϕϕϕ&prime;(v)&minus;ϕϕϕ
</p>
<p>(
v&prime;
)
,
</p>
<p>where v,v&prime; &isin; V and ϕϕϕ,ϕϕϕ&prime; &isin; V&lowast;. The reader may verify that (V&oplus; V&lowast;,ωωω) is
a symplectic vector space. This construction of symplectic vector spaces
is closely related to Hamiltonian dynamics, to which we shall return in
Chap. 28.
</p>
<p>Suppose (V,ωωω) and (W,ρρρ) are 2n-dimensional symplectic vector spaces.
Then, by Proposition 26.3.5, any symplectic map T : (V,ωωω) &rarr; (W,ρρρ) is
volume-preserving, i.e., (T&lowast;ρ)n is a volume element of W. It follows that the
rank of T&lowast; is 2n, and by Proposition 2.5.5, so is the rank of T. The dimension
theorem now implies that T is an isomorphism. Symplectic transformations
on a single vector space have an interesting property:
</p>
<p>Proposition 26.4.6 Let (V,ωωω) be a symplectic vector space. Then the set of
symplectic mappings T : (V,ωωω)&rarr; (V,ωωω) forms a group under composition,
called the symplectic group and denoted by Sp(V,ωωω). symplectic group
</p>
<p>Proof Clearly, Sp(V,ωωω) is a subset of GL(V). One need only show that the
inverse of a symplectic transformation is also such a transformation and that
the product of two symplectic transformations is a symplectic transforma-
tion. The details are left for the reader. �</p>
<p/>
</div>
<div class="page"><p/>
<p>804 26 Tensors
</p>
<p>A matrix is called symplectic if it is the representation of a symplec-symplectic matrices
tic transformation in a canonical basis of the underlying symplectic vector
space. The reader may check that the condition for a matrix A to be sym-
plectic is AtJA = J, where J is the representation ofωωω in the canonical basis:
</p>
<p>J =
(
</p>
<p>0 1
</p>
<p>&minus;1 0
</p>
<p>)
,
</p>
<p>where 1 and 0 are the n&times; n identity and zero matrices, respectively.
</p>
<p>26.5 Inner Product Revisited
</p>
<p>The inner product was defined in Chap. 2 in terms of a metric function that
took two vectors as input and manufactured a number. We now know what
kind of machine this is in the language of tensors.
</p>
<p>Definition 26.5.1 A symmetric bilinear form b on V is a symmetric ten-symmetric bilinear form
sor of type (0,2).
</p>
<p>If {ej }Nj=1 is a basis of V and {ǫǫǫi}Ni=1 is its dual basis, then b= 12bijǫǫǫiǫǫǫj
(recall Einstein&rsquo;s summation convention), because ǫǫǫiǫǫǫj = ǫǫǫi &otimes;ǫǫǫj +ǫǫǫj &otimes;ǫǫǫi
form a basis of S2(V). If v and u are any two vectors in V, then
</p>
<p>b(v,u)= 1
2
bij
</p>
<p>(
ǫǫǫi &otimes; ǫǫǫj + ǫǫǫj &otimes; ǫǫǫi
</p>
<p>)(
vkek, umem
</p>
<p>)
</p>
<p>= 1
2
bijv
</p>
<p>kum
[
ǫǫǫi(ek)ǫǫǫj (em)+ ǫǫǫj (ek)ǫǫǫi(em)
</p>
<p>]
</p>
<p>= 1
2
bijv
</p>
<p>kum
[
δikδ
</p>
<p>j
m + δjk δim
</p>
<p>]
</p>
<p>= 1
2
bij
</p>
<p>(
viuj + vjui
</p>
<p>)
= bijviuj . (26.27)
</p>
<p>For any vector v &isin; V, we can write
</p>
<p>b(v)= 1
2
bijǫǫǫ
</p>
<p>iǫǫǫj (v)= 1
2
bij
</p>
<p>(
ǫǫǫi &otimes; ǫǫǫj + ǫǫǫj &otimes; ǫǫǫi
</p>
<p>)(
vkek
</p>
<p>)
</p>
<p>= 1
2
bijv
</p>
<p>k
[
ǫǫǫiǫǫǫj (ek)+ ǫǫǫjǫǫǫi(ek)
</p>
<p>]
= 1
</p>
<p>2
bijv
</p>
<p>k
[
ǫǫǫiδ
</p>
<p>j
k + ǫǫǫj δik
</p>
<p>]
</p>
<p>= 1
2
bij
</p>
<p>[
vjǫǫǫi + viǫǫǫj
</p>
<p>]
= bijvjǫǫǫi = bijviǫǫǫj . (26.28)
</p>
<p>Thus, b(v) &isin; V&lowast;. This shows that b can be thought of as a mapping from
V to V&lowast;, which we denote by b&lowast; and write b&lowast; : V&rarr; V&lowast;. For this mapping
to make sense, it should not matter which factor in the symmetric product v
contracts with. But this is a trivial consequence of the symmetries bij = bji
and ǫǫǫiǫǫǫj = ǫǫǫjǫǫǫi .</p>
<p/>
</div>
<div class="page"><p/>
<p>26.5 Inner Product Revisited 805
</p>
<p>Let v and u be any two vectors in V. Let {ej }Nj=1 be a basis of V and
{ǫǫǫi}Ni=1 its dual basis. The natural pairing of v and b&lowast;(u) is given by
</p>
<p>&lang;
b&lowast;(u),v
</p>
<p>&rang;
=
&lang;
biju
</p>
<p>jǫǫǫi, vkek
&rang;
= bijujvk
</p>
<p>&lang;
ǫǫǫi, ek
</p>
<p>&rang;
</p>
<p>= bijujvkδik = bijujvi = b(u,v)= b(v,u), (26.29)
</p>
<p>where we used (26.27) in the last step.
The components bijvj of b&lowast;(v) in the basis {ǫǫǫi}Ni=1 of V&lowast; are denoted by
</p>
<p>vi , so
</p>
<p>b&lowast;(v)= viǫǫǫi, where vi = bijvj . (26.30)
We have thus lowered the index of vj by the use of the symmetric bilinear
form b. In applications vi is uniquely defined; furthermore, there is a one-
to-one correspondence between vi and vi . This can happen if and only if the
mapping b&lowast; : V&rarr; V&lowast; is invertible, in which case b is usually denoted by g.
If g&lowast; is invertible, there must exist a unique (g&lowast;)
</p>
<p>&minus;1 &equiv; (g&minus;1)&lowast; : V&lowast; &rarr; V, or
g&minus;1 &isin; S2(V&lowast;)= S2(V), such that
</p>
<p>vjej = v = (g&lowast;)&minus;1g&lowast;(v)= (g&lowast;)&minus;1
(
viǫǫǫ
</p>
<p>i
)
= vi(g&lowast;)&minus;1
</p>
<p>(
ǫǫǫi
)
</p>
<p>= vi
[(
g&minus;1
</p>
<p>)jkej ek
](
ǫǫǫi
)
= vi
</p>
<p>(
g&minus;1
</p>
<p>)jkej ek
(
ǫǫǫi
)
</p>
<p>︸ ︷︷ ︸
=δik
</p>
<p>= vi
(
g&minus;1
</p>
<p>)jiej .
</p>
<p>Comparison of the LHS and the RHS yields vj = vi(g&minus;1)ji . It is customary
to omit the &minus;1 and simply write
</p>
<p>vj = gjivi, (26.31)
</p>
<p>where it is understood that g with upper indices is the inverse of g (with
lower indices).
</p>
<p>Definition 26.5.2 An invertible bilinear form is called nondegenerate. nondegenerate bilinear
forms and inner
</p>
<p>products
</p>
<p>A symmetric bilinear form g that is nondegenerate is called an inner prod-
uct. When there is no danger of confusion, we write 〈u,v〉 instead of g(u,v).
</p>
<p>We therefore see that the presence of a nondegenerate symmetric bilinear
form (or an inner product) naturally connects the vectors in V and V&lowast; in a
unique way. For any vector v &isin; V there is a unique linear functional φφφv &isin; V&lowast;
given by φφφv = g&lowast;(v). One can therefore identify V and V&lowast;. An inner product
makes a vector space self-dual. In particular, Proposition 5.5.12 shows that
there exists a determinant function ���0 such that7
</p>
<p>���0(v1, . . . ,vN )���0(u1, . . . ,uN )= α det
(
g(vi,uj )
</p>
<p>)
. (26.32)
</p>
<p>This is called the Lagrange identity. Lagrange identity
Going from a vector in V to its unique image in V&lowast; is done by simply
</p>
<p>lowering the index using Eq. (26.30), and going the other way involves
using Eq. (26.31) to raise the index. This process can be generalized to
</p>
<p>raising and lowering
</p>
<p>indices
</p>
<p>7See also Problem 5.37.</p>
<p/>
</div>
<div class="page"><p/>
<p>806 26 Tensors
</p>
<p>all tensors. For instance, although in general, there is no connection among
T
</p>
<p>2
0(V), T
</p>
<p>1
1(V), and T
</p>
<p>0
2(V), the introduction of an inner product connects all
</p>
<p>these spaces in a natural way and establishes a one-to-one correspondence
among them. Thus, to a tensor in T20(V) with components t
</p>
<p>ij there corre-
sponds a unique tensor in T11(V), given, in component form, by t
</p>
<p>i
j = gjkt ik ,
</p>
<p>and another unique tensor in T02(V), given by tij = gil t lj = gilgjkt lk .
Let us apply this technique to gij , which is also a tensor and for which
</p>
<p>the lowering process is defined. We have
</p>
<p>gij = gikgkj =
(
g&minus;1
</p>
<p>)ik
gkj = δij . (26.33)
</p>
<p>This relation holds, of course, in all bases.
The inner product has been defined as a nondegenerate symmetric bilin-
</p>
<p>ear form. The important criterion of nondegeneracy has equivalences:
</p>
<p>Proposition 26.5.3 A symmetric bilinear form g is nondegenerate if
and only if
</p>
<p>1. the matrix of components gij has a nonvanishing determinant, or
2. for every nonzero v &isin; V, there exists w &isin; V such that g(v,w) �= 0.
</p>
<p>Proof The first part is a direct consequence of the definition of nondegen-
eracy. The second part follows from the fact that g&lowast; : V &rarr; V&lowast; is invert-
ible iff the nullity of g&lowast; is zero. It follows that if v &isin; V is nonzero, then
g&lowast;(v) �= 0, i.e., g&lowast;(v) is not the zero functional. Thus, there must exist a
vector w &isin; V such that [g&lowast;(v)](w) �= 0. The proposition is proved once we
note that [g&lowast;(v)](w)&equiv; g(v,w). �
</p>
<p>Let (V,g) and (U,h) be inner product spaces. Recall that an isometry is
a linear transformation T : V&rarr;U which preserves the inner product, i.e.,
</p>
<p>g(v1,v2)= h(Tv1,Tv2).
</p>
<p>It was shown in Theorem 2.3.12 that an isometry is injective. However, the
proof relied on the positive definiteness of the inner product. That is not
necessary. In fact, suppose that Tv = 0. Then, for any x &isin; V, we have
</p>
<p>g(x,v)= h(Tx,Tv)= h(Tx,0)= 0.
</p>
<p>By Proposition 26.5.3, v = 0 since g is nondegenerate. It follows that
kerT= {0}, and we have
</p>
<p>Theorem 26.5.4 A linear isometry is injective for all inner products.
</p>
<p>Definition 26.5.5 The g-transpose of a linear endomorphism T : V&rarr; V isg-transpose
the endomorphism Tt given by
</p>
<p>g
(
Ttu,v
</p>
<p>)
= g(u,Tv).</p>
<p/>
</div>
<div class="page"><p/>
<p>26.5 Inner Product Revisited 807
</p>
<p>If T is an isometry, then
</p>
<p>g(u,v)= g(Tu,Tv)= g
(
TtTu,v
</p>
<p>)
.
</p>
<p>Since this holds for arbitrary u and v, we must have TtT= 1. Thus,
</p>
<p>Proposition 26.5.6 An endomorphism T : V&rarr; V is an isometry if and only
if Tt = T&minus;1.
</p>
<p>Proof It is easy to show that if Tt = T&minus;1, then T is an isometry. We have also
shown that if g(u,v)= g(Tu,Tv), then TtT= 1. This last relation by itself
does not imply that T has an inverse. However, if V is finite dimensional,
then it does. (See Problem 5.25.) �
</p>
<p>Definition 26.5.7 A symmetric bilinear form b can be categorized as fol-
lows:
</p>
<p>1. positive (negative) definite: b(v,v) &gt; 0 [b(v,v) &lt; 0] for every
nonzero vector v;
</p>
<p>2. definite: b is either positive definite or negative definite;
3. positive (negative) semidefinite: b(v,v)&ge; 0 [b(v,v)&le; 0] for every v;
4. semidefinite: b is either positive semidefinite or negative semidefinite;
5. indefinite: b is not definite.
</p>
<p>If b is a symmetric bilinear form on V, then the restriction b|W of b on a
subspace W is also symmetric and bilinear, and if b is definite or semidefi-
nite, then so is b|W .
</p>
<p>Definition 26.5.8 The index ν of a symmetric bilinear form b on V is the
dimension of the largest subspace W of V on which b|W is negative definite.
Sometimes ν is referred to as the index of V.
</p>
<p>Example 26.5.9 Some of the categories of the definition above can be il-
lustrated in R2 with v1 = (x1, y1), v2 = (x2, y2), and v = (x, y).
(a) Positive definite: b(v1,v2) = x1x2 + y1y2 because if v �= 0, then one
</p>
<p>of its components is nonzero, and b(v,v)= x2 + y2 &gt; 0.
(b) Negative definite: b(v1,v2)= 12 (x1y2 + x2y1)&minus; x1x2 &minus; y1y2 because
</p>
<p>b(v,v)= xy &minus; x2 &minus; y2 =&minus;1
2
(x &minus; y)2 &minus; 1
</p>
<p>2
x2 &minus; 1
</p>
<p>2
y2,
</p>
<p>which is negative for nonzero v.
(c) Indefinite: b(v1,v2)= x1x2 &minus;y1y2. For v = (x, x), b(v,v)= 0. How-
</p>
<p>ever, b is nondegenerate, because it has the invertible matrix g =( 1 0
0 &minus;1
</p>
<p>)
in the standard basis of R2.
</p>
<p>(d) Positive semidefinite: b(v1,v2) = x1x2 &rArr; b(v,v) = x2 and b(v,v)
is never negative. However, b is degenerate because its matrix in the
standard basis of R2 is b =
</p>
<p>( 1 0
0 0
</p>
<p>)
, which is not invertible.</p>
<p/>
</div>
<div class="page"><p/>
<p>808 26 Tensors
</p>
<p>Let g be an inner product on V. Two vectors u,v &isin; V are said to be
g-orthogonal if g(u,v)= 0. A null or isotropic vector of g is a vector that
</p>
<p>g-orthogonal and null or
</p>
<p>isotropic vectors
</p>
<p>is g-orthogonal to itself. If g is definite, then the only null vector is the zero
vector. The converse is also true, as the following proposition shows.
</p>
<p>Proposition 26.5.10 If g is not definite, then there exists a nonzero
isotropic vector.
</p>
<p>Proof That g is not positive definite implies that there exists a nonzero vec-
tor v &isin; V such that g(v,v) &le; 0. Similarly, that g is not negative definite
implies that there exists a nonzero vector w &isin; V such that g(w,w) &ge; 0.
Construct the vector u = αv + (1 &minus; α)w and note that g(u,u) is a contin-
uous function of α. For α = 0 this function has the value g(w,w)&ge; 0, and
for α = 1 it has the value g(v,v)&le; 0. Thus, there must be some α for which
g(u,u)= 0. �
</p>
<p>Example 26.5.11 In the special theory of relativity, the inner product of
two &ldquo;position&rdquo; four-vectors, r1 = (x1, y1, z1, ct1) and r2 = (x2, y2, z2, ct2),
where c is the speed of light, is defined as
</p>
<p>g(r1, r2)=&minus;x1x2 &minus; y1y2 &minus; z1z2 + c2t1t2.
</p>
<p>This is clearly an indefinite symmetric bilinear form. Proposition 26.5.10
tells us that there must exist a nonzero null vector. Such a vector r satisfies
</p>
<p>g(r, r)= c2t2 &minus; x2 &minus; y2 &minus; z2 = 0,
</p>
<p>or
</p>
<p>c2 = x
2 + y2 + z2
</p>
<p>t2
&rArr; c=&plusmn;
</p>
<p>&radic;
x2 + y2 + z2
</p>
<p>t
=&plusmn;distance
</p>
<p>time
.
</p>
<p>This corresponds to a particle moving with the speed of light. Thus, light
rays are the null vectors in the special theory of relativity.
</p>
<p>Considering the four-vectors as a generalization of three-vectors, it is
more natural to define the inner product as g(r1, r2)= x1x2 +y1y2 +z1z2 &minus;
c2t1t2, so that the Euclidean part remains positive and only the added 4th
dimension carries the negative sign. Both practices are common in physics,
and we shall use both of them in the book.
</p>
<p>As in Chap. 4, we define the component of a vector along another vector
and the reflection of the former in a plane perpendicular to the latter.
</p>
<p>Definition 26.5.12 Let g be an inner product on V and y a non-null (non-
isotropic) vector in V. The projection xy of x along y and the reflection
xr,y of x in a plane perpendicular to y are given by
</p>
<p>projection and reflection
</p>
<p>for g
</p>
<p>xy =
g(x,y)
g(y,y)
</p>
<p>y and xr,y = x &minus; 2
g(x,y)
g(y,y)
</p>
<p>y.</p>
<p/>
</div>
<div class="page"><p/>
<p>26.5 Inner Product Revisited 809
</p>
<p>We can also introduce operators, as we did in Chap. 4. The bra and ket
notation was suitable for the projection operators. However, we still can
construct projection and reflection operators. In fact, using g&lowast; and recalling
that φφφy = g&lowast;(y) is the linear functional such that φφφy(x) = g(x,y), we can
define
</p>
<p>Py = y
1
</p>
<p>g(y,y)
φφφy (26.34)
</p>
<p>and verify that
</p>
<p>P2y =
(
</p>
<p>y
1
</p>
<p>g(y,y)
φφφy
</p>
<p>)(
y
</p>
<p>1
</p>
<p>g(y,y)
φφφy
</p>
<p>)
= y 1
</p>
<p>g(y,y)
φφφy(y)
</p>
<p>1
</p>
<p>g(y,y)
φφφy
</p>
<p>= y 1
g(y,y)
</p>
<p>g(y,y)
1
</p>
<p>g(y,y)
φφφy = y
</p>
<p>1
</p>
<p>g(y,y)
φφφy = Py .
</p>
<p>From this we obtain the reflection operator
</p>
<p>Ry = 1&minus; 2Py = 1&minus; 2y
1
</p>
<p>g(y,y)
φφφy . (26.35)
</p>
<p>The reflection operator has the property R2y = 1, or R&minus;1y = Ry , as expected.
Furthermore, one can easily show that
</p>
<p>g(x,Pyz)= g(z,Pyx)=
g(x,y)g(z,y)
</p>
<p>g(y,y)
,
</p>
<p>indicating that Py is symmetric (i.e., Pty = Py ). It follows that Ry is also
symmetric and
</p>
<p>1= R2y = RtyRy, (26.36)
</p>
<p>i.e., that Ry is an isometry by Proposition 26.5.6.
</p>
<p>26.5.1 Subspaces
</p>
<p>Let V be a vector space with inner product g. Let W be a subspace of V.
Let W&perp; be all vectors in V which are g-orthogonal to all vectors in W.
Ordinarily, we would call W&perp; the orthogonal complement of W, but if g
is not definite, we can&rsquo;t. Here is why: In Example 26.5.11, eliminate the y
and z coordinates and consider two-dimensional vectors (x, ct). Now let W
be the span of any null vector. Then clearly W = W&perp;, and W&perp; does not
complement W. Nevertheless, we have the following
</p>
<p>Lemma 26.5.13 LetW be a subspace of a finite-dimensional inner product
space V. Then
</p>
<p>(1) dimW+ dimW&perp; = dimV.
(2) (W&perp;)&perp; =W.</p>
<p/>
</div>
<div class="page"><p/>
<p>810 26 Tensors
</p>
<p>Proof (1) Let {ei}mi=1 be a basis of W with the dual basis {ǫǫǫi}mi=1. Consider
the linear operator gW : V&rarr;W&lowast; given by
</p>
<p>gW (v)=
m&sum;
</p>
<p>i=1
g(v, ei)ǫǫǫi .
</p>
<p>It is not hard to show that gW is onto. Using the dimension theorem (Theo-
rem 2.3.13), we can write
</p>
<p>dimW&lowast; + dim kergW = dimV.
</p>
<p>Since dimW&lowast; = dimW, all that is left to show is that dim kergW =
dimW&perp;. We show more than that; we prove that kergW =W&perp;. In fact,
</p>
<p>v &isin; kergW &lArr;&rArr;
m&sum;
</p>
<p>i=1
g(v, ei)ǫǫǫi = 0
</p>
<p>&lArr;&rArr; g(v, ei)= 0, i = 1,2, . . . ,m.
</p>
<p>The last equality follows from the linear independence of {ǫǫǫi}mi=1, and it
holds if and only if v &isin;W&perp;.
</p>
<p>(2) If v &isin;W, then v is orthogonal to all vectors in W&perp;, i.e., v &isin; (W&perp;)&perp;.
Thus, W &sub; (W&perp;)&perp;. Applying (1) to the subspace W&perp;, we get dimW&perp; +
dim(W&perp;)&perp; = dimV. Hence, dimW= dim(W&perp;)&perp;, and W= (W&perp;)&perp;. �
</p>
<p>A subspace W of an inner product space (V,g) is called nondegener-nondegenerate
subspace ate if g|W is nondegenerate. When g is definite, any subspace of V inherits
</p>
<p>a definite inner product. Therefore, in this case every subspace is nonde-
generate. However, when g is not definite, there will always be degenerate
subspaces. For example, if v is null, then the span of v is clearly degenerate.
</p>
<p>Proposition 26.5.14 A subspace W of an inner product space V is nonde-
generate if and only if W&oplus;W&perp; = V.
</p>
<p>Proof Clearly, g|W is nondegenerate if and only if W&cap;W&perp; = 0, because if
there were 0 �= w &isin;W &cap;W&perp;, it would have to be orthogonal to all vectors
in W, making g|W degenerate. From Problem 2.8, we have
</p>
<p>dim
(
W+W&perp;
</p>
<p>)
+ dim
</p>
<p>(
W&cap;W&perp;
</p>
<p>)
= dimW+ dimW&perp; = dimV,
</p>
<p>where in the last step, we used (1) of Lemma 26.5.13. Therefore, dim(W+
W&perp;)= dimV if and only if W is nondegenerate. Since W+W&perp; is a sub-
space of V, we get W+W&perp; = V if and only if W is nondegenerate. The last
sum is actually a direct sum because of the first statement in the proof. �
</p>
<p>An immediate consequence of this proposition and (2) of Lemma 26.5.13
is
</p>
<p>Corollary 26.5.15 A subspace W of an inner product space is nondegen-
erate if and only if W&perp; is nondegenerate.</p>
<p/>
</div>
<div class="page"><p/>
<p>26.5 Inner Product Revisited 811
</p>
<p>Equation (26.36) tells us that a reflection is an isometry. Is an isometry
also a reflection? This question is not without merit. In fact, Example 4.4.8
established a connection between reflections and isometries in R2. Geomet-
ric reasonings also make a connection between reflections and isometries.
Take a vector in three dimensions and apply an isometry to it. The resulting
vector will have the same length. Draw the two vectors from the same point.
Find the difference between the two vectors (connect the tips of the two ar-
rows). Construct the perpendicular bisector plane of this vector. Clearly the
vector and its isometric image are reflections of one another in this plane.
Although we have constructed a reflection from the isometry, its construc-
tion depends on the vector on which the isometry acts (see Problem 26.28).
Is it possible to find a general dependence between an isometry and reflec-
tions not involving any vector? The theorem to follow makes the relation
more explicit, but first we need a lemma:
</p>
<p>Lemma 26.5.16 Let x and y be two vectors in V such that g(x,x) =
g(y,y) �= 0. Then there is a reflection R such that R(x)=&plusmn;y.
</p>
<p>Proof Because of the relation
</p>
<p>g(x + y,x + y)+ g(x &minus; y,x &minus; y)= 4g(x,x) �= 0,
</p>
<p>at least one of the terms on the left is nonzero. Assume that g(x &minus; y,
x &minus; y) �= 0, and let z = x &minus; y. Then the reflection operator
</p>
<p>Rz = 1&minus; 2Pz = 1&minus; 2z
1
</p>
<p>g(z, z)
φφφz
</p>
<p>is such that
</p>
<p>Rz(x)= x &minus; 2z
g(z,x)
g(z, z)
</p>
<p>= x &minus; 2(x &minus; y) g(x &minus; y,x)
g(x &minus; y,x &minus; y)
</p>
<p>= x &minus; 2(x &minus; y) g(x,x)&minus; g(x,y)
2g(x,x)&minus; 2g(x,y) = y.
</p>
<p>If g(x + y,x + y) �= 0, then let z = x + y. The reflection operator Rz,
when acting on x, yields
</p>
<p>Rz(x)= x &minus; 2z
g(z,x)
g(z, z)
</p>
<p>= x &minus; 2(x + y) g(x + y,x)
g(x + y,x + y)
</p>
<p>= x &minus; 2(x + y) g(x,x)+ g(x,y)
2g(x,x)+ 2g(x,y) =&minus;y,
</p>
<p>and the proof of the lemma is complete. �
</p>
<p>Theorem 26.5.17 Let V be an N -dimensional inner product space.
Then any isometry T of V is the product of at most N + 1 reflections.</p>
<p/>
</div>
<div class="page"><p/>
<p>812 26 Tensors
</p>
<p>Proof We prove the theorem by induction on the dimension of V. For N = 1
and x &isin; V, we have Tx = &plusmn;x. Since R = &minus;1 for any reflection operator in
one-dimension, we see that T= R for the negative sign, and T= R2 for the
positive sign. Hence, T is the product of at most two reflections.
</p>
<p>Now let T be an isometry of V. Choose a vector x such that g(x,x) �= 0,
and set y = Tx. Since g(y,y) = g(x,x), by Lemma 26.5.16, there exists a
reflection R such that Rx =&plusmn;y. Set T1 = R ◦ T and note that
</p>
<p>T1x = R ◦ Tx = Ry =&plusmn;R2x =&plusmn;x.
</p>
<p>So T1 leaves Span{x} invariant. By Propositions 2.3.17 and 2.3.21, it leaves
V1, the orthogonal complement of Span{x}, also invariant. Since Span{x} is
non-degenerate, so is V1 by Corollary 26.5.15. Furthermore, since V1 has
dimension N &minus; 1, the induction hypothesis applies to it, and we can write
</p>
<p>R ◦ T&equiv; T1 = R1R2 . . .RN , (26.37)
</p>
<p>where each Ri is a reflection in V1.
Now, each reflection in V1 extends to a reflection in V. In fact, if Rz1
</p>
<p>is defined for z1 &isin; V1, and v = v1 + αx is any vector in V, define Rz1v =
Rz1v1 + αx. Then
</p>
<p>Rz1 v = v1 &minus; 2z1
g(z1,v1)
g(z1, z1)
</p>
<p>+ αx
</p>
<p>= v1 + αx &minus; 2z1
g(z1,v1 + αx)
</p>
<p>g(z1, z1)
= v &minus; 2z1
</p>
<p>g(z1,v)
g(z1, z1)
</p>
<p>,
</p>
<p>because g(z1,x) = 0. This shows that Rz1 is a reflection in V. Multiplying
both sides of (26.37) by R and noting that R2 = 1, we get
</p>
<p>T= R2 ◦ T= R ◦ T1 = RR1R2 . . .RN .
</p>
<p>Thus T is the product of R and N other reflections. �
</p>
<p>26.5.2 Orthonormal Basis
</p>
<p>Whenever there is an inner product on a vector space, there is the possibility
of orthogonal basis vectors. Since, in general, g(v,v) is allowed to be neg-
ative or zero, we have to redefine what we mean by a vector of norm 1. If
g(v,v) �= 0, we define the norm of v as ‖v‖ = |
</p>
<p>&radic;
g(v,v)|. A unit vector, or
</p>
<p>a vector of norm 1 obtained from v is simply v/‖v‖.
</p>
<p>Theorem 26.5.18 An inner product space V has an orthonormal basis.
</p>
<p>Proof Start with the polarization identity,polarization identity
</p>
<p>g
(
v,v&prime;
</p>
<p>)
= 1
</p>
<p>4
</p>
<p>[
g
(
v + v&prime;,v + v&prime;
</p>
<p>)
&minus; g
</p>
<p>(
v &minus; v&prime;,v &minus; v&prime;
</p>
<p>)]
,</p>
<p/>
</div>
<div class="page"><p/>
<p>26.5 Inner Product Revisited 813
</p>
<p>and use it to convince yourself that g is identically zero unless there exists
a vector v such that g(v,v) �= 0. Let e1 = v/‖v‖, and note that g(e1, e1)&equiv;
η1 =&plusmn;1. Now suppose that we have found a set {ei}mi=1 of m orthonormal
vectors in V. We show that as long as m &lt; dimV, we can add one more
unit vector to the set. Let W be the subspace spanned by {ei}mi=1. Then W is
nondegenerate, and by Corollary 26.5.15, W&perp; is also nondegenerate. Hence,
there exist a vector u &isin;W&perp; with g(u,u) �= 0, and em+1 &equiv; u/‖u‖ is a unit
vector orthogonal to all vectors {ei}mi=1. �
</p>
<p>Definition 26.5.19 Let B = {ei}Ni=1 be a basis of V and ηij &equiv; g(ei, ej ).
We say B is g-orthonormal if ηij = 0 for i �= j , and ηii = &plusmn;1. The ηii
are called the diagonal components of g. We use n+ and n&minus; to denote
the number of vectors ei for which ηii is, respectively, +1 and &minus;1. The
collection (η11, η22, . . . , ηNN ) is called the signature of g.
</p>
<p>g-orthonormal vectors,
</p>
<p>diagonal components of
</p>
<p>g, and signature of g
</p>
<p>Example 26.5.20 Let V = R3 and v1 = (x1, y1, z1), v2 = (x2, y2, z2), and
v = (x, y, z). Define the symmetric bilinear form
</p>
<p>g(v1,v2)=
1
</p>
<p>2
(x1y2 + x2y1 + y1z2 + y2z1 + x1z2 + x2z1)
</p>
<p>so that g(v,v)= xy + yz+ xz. We wish to find a set of vectors in R3 that
are orthonormal with respect to this bilinear form. Clearly, e1 = (1,1,0) is
such that g(e1, e1) = 1. So e1 is one of our vectors. Consider v = (1,0,1)
and note that the vector r2 = v &minus; [g(v, e1)/g(e1, e1)]e1, suggested by the
Gram&ndash;Schmidt process, is orthogonal to e1. Furthermore, it is easily verified
that g(r2, r2)=&minus; 54 . Therefore, our second vector is
</p>
<p>e2 =
r2&radic;
</p>
<p>|g(r2, r2)|
=
(
&minus; 1&radic;
</p>
<p>5
,&minus; 3&radic;
</p>
<p>5
,
</p>
<p>2&radic;
5
</p>
<p>)
</p>
<p>with g(e2, e2)=&minus;1. Finally, we take w = (0,1,1). Then
</p>
<p>r3 = w &minus;
g(w, e1)
g(e1, e1)
</p>
<p>e1 &minus;
g(w, e2)
g(e2, e2)
</p>
<p>e2 =
4
</p>
<p>10
(&minus;3,1,1)
</p>
<p>will be orthogonal to both e1 and e2 with g(r3, r3) = &minus; 45 . Thus, the third
vector can be chosen to be
</p>
<p>e3 =
r3&radic;
</p>
<p>|g(r3, r3)|
=
(
&minus; 3&radic;
</p>
<p>5
,
</p>
<p>1&radic;
5
,
</p>
<p>1&radic;
5
</p>
<p>)
,
</p>
<p>and we obtain g(e1, e1)= 1, g(e2, e2)=&minus;1, g(e3, e3)=&minus;1, g(ei, ej )= 0
for i �= j . We also have n+ = 1, n&minus; = 2. Although we have worked in a
particular basis, Theorem 26.5.21 below guarantees that n+ and n&minus; are (or-
thonormal) basis-independent.
</p>
<p>The matrix of g in an orthonormal basis is the diagonal matrix of ηij . The
elements of the inverse of this matrix (which is equal to the matrix itself)
are denoted by ηij . The seemingly unnecessary use of superscripts for the</p>
<p/>
</div>
<div class="page"><p/>
<p>814 26 Tensors
</p>
<p>inverse is not only consistent with the discussion leading to Eq. (26.31), but
also with index manipulations of tensors. For example, when superscripts
are used for the inverse, we have ηijηjk = δki , with indices properly located.
</p>
<p>Let {ei}Ni=1 be an orthonormal basis of V and v = viei an arbitrary vector
in V. Now take the inner product of v with ej to obtain
</p>
<p>g(v, ej )= g
(
viei, ej
</p>
<p>)
= viηij .
</p>
<p>Multiply both sides by ηjk (with sum over repeated indices understood):
</p>
<p>ηjkg(v, ej )= viηijηjk = viδki = vk.
</p>
<p>This leads to the orthogonal expansion of an arbitrary vector v:
</p>
<p>v = ηjkg(v, ej )ek =
N&sum;
</p>
<p>k=1
ηkkg(v, ek)ek =
</p>
<p>N&sum;
</p>
<p>k=1
ηkkg(v, ek)ek. (26.38)
</p>
<p>If W is a nondegenerate subspace of an inner product space V, and if we
enlarge an orthonormal basis {ei}mi=1 of W to an orthonormal basis of V,
then the operator PW projecting onto W is defined as
</p>
<p>PW (v)=
m&sum;
</p>
<p>j,k=1
ηjkg(v, ej )ek =
</p>
<p>m&sum;
</p>
<p>k=1
ηkkg(v, ek)ek =
</p>
<p>m&sum;
</p>
<p>k=1
ηkkg(v, ek)ek.
</p>
<p>(26.39)
Clearly, PW (v)= v if v &isin;W and PW (v)= 0 if v &isin;W&perp;.
</p>
<p>Theorem 26.5.21 The number n&minus; of negative signs in (η11, η22, . . . , ηNN ),
the signature of any orthonormal basis {ei}Ni=1 of an inner product space V,Sylvester&rsquo;s theorem
is equal to ν, the index of V.
</p>
<p>Proof Assume that the first n&minus; signs are negative. If n&minus; = 0 or n&minus; = N ,
the proof is trivial. Let U be the span of {ei}n&minus;i=1. It is obvious that g|U is
negative definite. By Definition 26.5.8, ν &ge; n&minus;.
</p>
<p>Now let W be an arbitrary subspace of V on which g is negative definite,
and define the linear map π :W&rarr;U by
</p>
<p>π(w)=
n&minus;&sum;
</p>
<p>k=1
ηkkg(w, ek)ek =&minus;
</p>
<p>n&minus;&sum;
</p>
<p>k=1
g(w, ek)ek.
</p>
<p>We claim that π is injective. To prove our claim, we show kerπ = 0. If
π(w)= 0, then by Eq. (26.38), w =&sum;Nk=n&minus;+1 g(w, ek)ek , and
</p>
<p>g(w,w)= g
(
</p>
<p>N&sum;
</p>
<p>k=n&minus;+1
g(w, ek)ek,
</p>
<p>N&sum;
</p>
<p>j=n&minus;+1
g(w, ej )ej
</p>
<p>)
</p>
<p>=
N&sum;
</p>
<p>k=n&minus;+1
</p>
<p>N&sum;
</p>
<p>j=n&minus;+1
g(w, ek)g(w, ej )g(ek, ej )︸ ︷︷ ︸
</p>
<p>=ηkj</p>
<p/>
</div>
<div class="page"><p/>
<p>26.5 Inner Product Revisited 815
</p>
<p>=
N&sum;
</p>
<p>k=n&minus;+1
</p>
<p>[
g(w, ek)
</p>
<p>]2
ηkk =
</p>
<p>N&sum;
</p>
<p>k=n&minus;+1
</p>
<p>[
g(w, ek)
</p>
<p>]2
.
</p>
<p>The left-hand side is negative (unless w = 0 in which case it is zero), the
right-hand side is positive (or zero). The only way that the equality can hold
is for w to be the zero vector. Hence, kerπ = 0, and π is injective. This
implies that dimW&le; n&minus;. In particular, if W has maximal dimension ν, we
have ν &le; n&minus;. This, along with the conclusion of the first paragraph of the
proof, yields ν = n&minus;. �
</p>
<p>Corollary 26.5.22 LetW&minus; denote the largest subspace of the inner product
space V on which g is negative definite. Then V=W&minus;&oplus;W+, whereW+ is
the orthogonal complement of W&minus; and g is positive definite on W+.
</p>
<p>Take the Euclidean n-space Rn and for some integer 0 &le; ν &le; n, change
the signs of the first ν terms in the usual inner product of Rn:
</p>
<p>〈u,v〉 &equiv; g(u,v)= ηijuivj =
n&sum;
</p>
<p>i=1
ηiiu
</p>
<p>ivi =&minus;
ν&sum;
</p>
<p>i=1
uivi +
</p>
<p>n&sum;
</p>
<p>i=ν+1
uivi .
</p>
<p>(26.40)
The resulting inner product space, denoted by Rnν , is called the semi- Semi-Euclidean and
</p>
<p>Minkowski spacesEuclidean space. For n &ge; 2, Rn1 is called the Minkowski n-space. R41 is
the space of the special theory of relativity.
</p>
<p>Proposition 26.5.23 Let Rnν and R
m
μ be semi-Euclidean spaces. Then
</p>
<p>Rnν &oplus;Rmμ &sim;=Rn+mν+μ .
</p>
<p>Proof Apply Eq. (2.12). �
</p>
<p>For Rnν , substitute the vectors of an orthonormal basis {ei}Ni=1 for both vi
and uj in Eq. (26.32) to get
</p>
<p>���0(e1, . . . , eN )2 = α det
(
g(ei, ej )
</p>
<p>)
= α det(ηij )= α(&minus;1)ν .
</p>
<p>This shows that α(&minus;1)ν &gt; 0. Hence, we can define a new determinant func-
tion by
</p>
<p>���=&plusmn; ���0&radic;
α(&minus;1)ν , (26.41)
</p>
<p>for which (26.32) takes the form
</p>
<p>���(v1, . . . ,vN )���(u1, . . . ,uN )= (&minus;1)ν det
(
g(vi,uj )
</p>
<p>)
. (26.42)
</p>
<p>A determinant function satisfying this equation is called a normed deter- normed determinant
functionminant function. Equation (26.41) shows that there are exactly two normed
</p>
<p>determinant functions ��� and &minus;��� in V.</p>
<p/>
</div>
<div class="page"><p/>
<p>816 26 Tensors
</p>
<p>Orthonormal bases allow us to speak of the oriented volume element.Orthonormal bases give
the same oriented
</p>
<p>volume element.
</p>
<p>Suppose {ǫǫǫj }Nj=1 is an oriented orthonormal basis of V&lowast;. If {ϕϕϕk}Nk=1 is an-
other orthonormal basis in the same orientation and related to {ǫǫǫj } by a
matrix R, then
</p>
<p>ϕϕϕ1 &and;ϕϕϕ2 &and; &middot; &middot; &middot; &and;ϕϕϕN = (det R)ǫǫǫ1 &and; ǫǫǫ2 &and; &middot; &middot; &middot; &and; ǫǫǫN .
</p>
<p>Since {ϕϕϕk} and {ǫǫǫj } are orthonormal, the determinant of g, which is det(ηij ),
is (&minus;1)ν in both of them. Problem 26.26 then implies that (det R)2 = 1 or
det R = &plusmn;1. However, {ϕϕϕk} and {ǫǫǫj } belong to the same orientation. Thus,
det R =+1, and {ϕϕϕk} and {ǫǫǫj } give the same volume element.
</p>
<p>Definition 26.5.24 The volume element of an inner product space (V,g)volume element relative
to g relative to g is a volume element obtained from any orthonormal basis
</p>
<p>of V&lowast;.
</p>
<p>We should emphasize that the invariance of ν is true for g-orthonormal
bases. As a counterexample, consider g of Example 26.5.20 applied to the
standard basis of R3, which we designate with a prime. It is readily verified
that
</p>
<p>g
(
e&prime;i, e
</p>
<p>&prime;
i
</p>
<p>)
= 0 for i = 1,2,3.
</p>
<p>So it might appear that ν = 0 for this basis. However, the standard basis is
not g-orthonormal. In fact,
</p>
<p>g
(
e&prime;1, e
</p>
<p>&prime;
2
</p>
<p>)
= 1
</p>
<p>2
= g
</p>
<p>(
e&prime;1, e
</p>
<p>&prime;
3
</p>
<p>)
= g
</p>
<p>(
e&prime;2, e
</p>
<p>&prime;
3
</p>
<p>)
.
</p>
<p>That is why the nonstandard vectors e1, v, and w were chosen in Exam-
ple 26.5.20.
</p>
<p>Example 26.5.25 Let {ei}Ni=1 be a basis of V and {ǫǫǫi}Ni=1 its dual. We can
define the permutation tensor:permutation tensor
</p>
<p>δ
i1i2...iN
j1j2...jN
</p>
<p>= ǫǫǫi1 &and; ǫǫǫi2 &and; &middot; &middot; &middot; &and; ǫǫǫiN (ej1 , ej2 , . . . , ejN ). (26.43)
</p>
<p>It is clear from this definition that δi1i2...iNj1j2...jN is completely skew-symmetric in
all upper indices. That it is also skew-symmetric in the lower indices can be
seen as follows. Assume that two of the lower indices are equal. This means
having two ej &rsquo;s equal in (26.43). These two ej &rsquo;s will contract with two ǫǫǫi &rsquo;s,
say ǫǫǫk and ǫǫǫl . Thus, in the expansion there will be a term Cǫǫǫk(ej )ǫǫǫl(ej ),
where C is the product of all the other factors. Since the product is com-
pletely skew-symmetric in the upper indices, there must also exist another
term, with a minus sign and in which the upper indices k and l are inter-
changed: &minus;Cǫǫǫl(ej )ǫǫǫk(ej ). This makes the sum zero, and by Theorem 2.6.3,
(26.43) is antisymmetric in the lower indices as well.
</p>
<p>This suggests that δi1i2...iNj1j2...jN &prop; ǫ
i1i2...iN ǫj1j2...jN . To find the proportional-
</p>
<p>ity constant, we note that (see Problem 26.14)
</p>
<p>δ12...N12...N =
&sum;
</p>
<p>π
</p>
<p>ǫπ(1)π(2)...π(N)δ
1
π(1)δ
</p>
<p>2
π(2) &middot; &middot; &middot; δNπ(N).</p>
<p/>
</div>
<div class="page"><p/>
<p>26.5 Inner Product Revisited 817
</p>
<p>The only contribution to the sum comes from the permutation with the prop-
erty π(i) = i. This is the identity permutation for which ǫπ = 1. Thus, we
have δ12...N12...N = 1. On the other hand, by Problem 26.25,
</p>
<p>ǫ12...Nǫ12...N = ǫ12...N = (&minus;1)n&minus; .
</p>
<p>Therefore, the proportionality constant is (&minus;1)n&minus; . Thus
</p>
<p>ǫi1i2...iN ǫj1j2...jN = (&minus;1)n&minus;δi1i2...iNj1j2...jN . (26.44)
</p>
<p>We can find an explicit expression for the permutation tensor of Exam-
ple 26.5.25. Expanding the RHS of Eq. (26.43) using Eq. (26.18), we obtain
</p>
<p>δ
i1i2...iN
j1j2...jN
</p>
<p>=
&sum;
</p>
<p>π
</p>
<p>ǫπδ
i1
π(j1)
</p>
<p>δ
i2
π(j2)
</p>
<p>&middot; &middot; &middot; δiNπ(jN ),
</p>
<p>ǫi1i2...iN ǫj1j2...jN = (&minus;1)n&minus;
&sum;
</p>
<p>π
</p>
<p>ǫπδ
i1
π(j1)
</p>
<p>δ
i2
π(j2)
</p>
<p>&middot; &middot; &middot; δiNπ(jN ).
(26.45)
</p>
<p>Furthermore, the first equation of (26.45) can be written concisely as a de-
terminant, because
</p>
<p>δ
i1i2...iN
12...N =
</p>
<p>&sum;
</p>
<p>π
</p>
<p>ǫπ(1)π(2)...π(N)δ
i1
π(1)δ
</p>
<p>i2
π(2) &middot; &middot; &middot; δ
</p>
<p>iN
π(N).
</p>
<p>The RHS is clearly the determinant of a matrix (expanded with respect to
the ith row) whose elements are δikk . The same holds true if 1,2, . . . ,N is
replaced by j1, j2, . . . , jN ; thus,
</p>
<p>δ
i1i2...iN
j1j2...jN
</p>
<p>= det
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>δ
i1
j1
</p>
<p>δ
i1
j2
</p>
<p>&middot; &middot; &middot; δi1jN
δ
i2
j1
</p>
<p>δ
i2
j2
</p>
<p>&middot; &middot; &middot; δi2jN
...
</p>
<p>...
...
</p>
<p>δ
iN
j1
</p>
<p>δ
iN
j2
</p>
<p>&middot; &middot; &middot; δiNjN
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠
. (26.46)
</p>
<p>Example 26.5.26 Let us apply the second equation of (26.45) to Eu-
clidean R3:
</p>
<p>ǫijkǫlmn = δil δ
j
mδ
</p>
<p>k
n &minus; δil δ
</p>
<p>j
nδ
</p>
<p>k
m &minus; δimδ
</p>
<p>j
</p>
<p>l δ
k
n + δimδ
</p>
<p>j
nδ
</p>
<p>k
l + δinδ
</p>
<p>j
</p>
<p>l δ
k
m &minus; δinδ
</p>
<p>j
mδ
</p>
<p>k
l .
</p>
<p>From this fundamental relation, we can obtain other useful formulas. For
example, setting n= k and summing over k, we get
</p>
<p>ǫijkǫlmk = 3δil δ
j
m &minus; δil δ
</p>
<p>j
m &minus; 3δimδ
</p>
<p>j
l + δimδ
</p>
<p>j
l + δimδ
</p>
<p>j
l &minus; δil δ
</p>
<p>j
m = δil δ
</p>
<p>j
m &minus; δimδ
</p>
<p>j
l .
</p>
<p>Now set m= j in this equation and sum over j :
</p>
<p>ǫijkǫljk = 3δil &minus; δil = 2δil .
</p>
<p>Finally, let l = i and sum over i:
</p>
<p>ǫijkǫijk = 2δii = 2 &middot; 3 = 3!.</p>
<p/>
</div>
<div class="page"><p/>
<p>818 26 Tensors
</p>
<p>In general,
</p>
<p>δ
i1i2...iN
i1i2...iN
</p>
<p>=N !, or ǫi1i2...iN ǫi1i2...iN = (&minus;1)n&minus;N !,
</p>
<p>δ
i1i2...iN&minus;1iN
i1i2...iN&minus;1jN = (N &minus; 1)!δ
</p>
<p>iN
jN
, or
</p>
<p>ǫi1i2...iN&minus;1iN ǫi1i2...iN&minus;1jN = (&minus;1)n&minus;(N &minus; 1)!δiNjN ,
</p>
<p>and
</p>
<p>δ
i1i2...iN&minus;2iN&minus;1iN
i1i2...iN&minus;2jN&minus;1jN = (N &minus; 2)!
</p>
<p>(
δ
iN&minus;1
jN&minus;1δ
</p>
<p>iN
jN
</p>
<p>&minus; δiN&minus;1jN δ
iN
jN&minus;1
</p>
<p>)
</p>
<p>= (N &minus; 2)!δiN&minus;1iNjN&minus;1jN , or
</p>
<p>ǫi1i2...iN&minus;2iN&minus;1iN ǫi1i2...iN&minus;2jN&minus;1jN
</p>
<p>= (&minus;1)n&minus;(N &minus; 2)!
(
δ
iN&minus;1
jN&minus;1δ
</p>
<p>iN
jN
</p>
<p>&minus; δiN&minus;1jN δ
iN
jN&minus;1
</p>
<p>)
.
</p>
<p>More generally,
</p>
<p>δ
i1i2...ipip+1...iN
i1i2...ipjp+1...jN = p!δ
</p>
<p>ip+1...iN
jp+1...jN . (26.47)
</p>
<p>Equation (26.47) can be generalized even further:
</p>
<p>δ
ik+1...ik+pik+p+1...iN
ik+1...ik+pjk+p+1...jN =
</p>
<p>(p+ k)!
k! δ
</p>
<p>ik+p+1...iN
jk+p+1...jN . (26.48)
</p>
<p>If you set the j &rsquo;s equal to i&rsquo;s in Eq. (26.47), you get N ! on the left-hand
side. Equation (26.48) then yields N !/p! on the right-hand side, making the
two sides equal.
</p>
<p>Another useful property of the permutation tensor is (Problem 26.15)
</p>
<p>δ
i1i2...ir
j1j2...jr
</p>
<p>Ai1i2...ir =
&sum;
</p>
<p>π
</p>
<p>ǫπAπ(j1)π(j2)...π(jr ) (26.49)
</p>
<p>for any tensor (the tensor could have more indices, and some of the r indices
could be mixed). In particular if A is antisymmetric in i1i2 . . . ir , then
</p>
<p>δ
i1i2...ir
j1j2...jr
</p>
<p>Ai1i2...ir = r!Aj1j2...jr . (26.50)
</p>
<p>Example 26.5.27 As an application of the foregoing formalism, we can
express the determinant of a 2&times; 2 matrix in terms of traces. Let A be such a
matrix with elements Aij . Then
</p>
<p>det A = ǫijAi1A
j
</p>
<p>2 =
1
</p>
<p>2
</p>
<p>(
ǫijA
</p>
<p>i
1A
</p>
<p>j
</p>
<p>2 &minus; ǫijAi2A
j
</p>
<p>1
</p>
<p>)
= 1
</p>
<p>2
</p>
<p>(
ǫij ǫ
</p>
<p>klAikA
j
</p>
<p>l
</p>
<p>)
</p>
<p>= 1
2
AikA
</p>
<p>j
l
</p>
<p>(
δki δ
</p>
<p>l
j &minus; δkj δli
</p>
<p>)
= 1
</p>
<p>2
</p>
<p>(
AiiA
</p>
<p>j
j &minus;AijA
</p>
<p>j
i
</p>
<p>)
</p>
<p>= 1
2
</p>
<p>[
(tr A)(tr A)&minus;
</p>
<p>(
A2
</p>
<p>)i
i
</p>
<p>]
= 1
</p>
<p>2
</p>
<p>[
(tr A)2 &minus; tr
</p>
<p>(
A2
</p>
<p>)]
.</p>
<p/>
</div>
<div class="page"><p/>
<p>26.5 Inner Product Revisited 819
</p>
<p>We can generalize the result of the example above and express the deter-
minant of an N &times;N matrix as
</p>
<p>det A = 1
N !ǫ
</p>
<p>i1i2...iN ǫj1j2...jNA
j1
i1
A
j2
i2
&middot; &middot; &middot;AjNiN
</p>
<p>= 1
N !
</p>
<p>&sum;
</p>
<p>π
</p>
<p>ǫπδ
i1
π(j1)
</p>
<p>δ
i2
π(j2)
</p>
<p>&middot; &middot; &middot; δiNπ(jN )A
j1
i1
A
j2
i2
&middot; &middot; &middot;AjNiN . (26.51)
</p>
<p>26.5.3 Inner Product onΛp(V,U)
</p>
<p>If (V,g) is an inner product space, then g induces an inner product g̃ on
Λp(V) as follows. Extend the mapping g&minus;1&lowast; : V&lowast; &rarr; V to the space of p-
forms by applying it to each factor (e.g., in the expansion of the p-form in a
basis of Λp(V)). This extension makes g&minus;1&lowast; a map g
</p>
<p>&minus;1
&lowast; :Λp(V)&rarr;Λp(V&lowast;)
</p>
<p>which takes a p-formβββ and turns it into a p-vector g&minus;1&lowast; (βββ). Then the pairing
〈ααα,g&minus;1&lowast; (βββ)〉, with ααα,βββ &isin;Λp(V), is the desired induced inner product. More
specifically, let {ej }Nj=1 be a basis of V and {ǫǫǫi}Ni=1 its dual basis. Then,
</p>
<p>g&minus;1&lowast; (βββ)&equiv; g&minus;1&lowast;
(
</p>
<p>1
</p>
<p>p!βi1i2...ipǫǫǫ
i1 &and; ǫǫǫi2 &and; &middot; &middot; &middot; &and; ǫǫǫip
</p>
<p>)
</p>
<p>= 1
p!βi1i2...ip
</p>
<p>(
g&minus;1&lowast;
</p>
<p>(
ǫǫǫi1
</p>
<p>)
&and; g&minus;1&lowast;
</p>
<p>(
ǫǫǫi2
</p>
<p>)
&and; &middot; &middot; &middot; &and; g&minus;1&lowast;
</p>
<p>(
ǫǫǫip
</p>
<p>))
</p>
<p>= 1
p!βi1i2...ip
</p>
<p>(
gi1j1ej1 &and; gi2j2ej2 &and; &middot; &middot; &middot; &and; gipjpejp
</p>
<p>)
</p>
<p>= 1
p!g
</p>
<p>i1j1gi2j2 &middot; &middot; &middot;gipjpβi1i2...ipej1 &and; ej2 &and; &middot; &middot; &middot; &and; ejp
</p>
<p>&equiv; 1
p!β
</p>
<p>j1j2...jpej1 &and; ej2 &and; &middot; &middot; &middot; &and; ejp .
</p>
<p>Note how the indices of the components of βββ have been raised by the com-
ponents of the g&minus;1. Pairing this last expression with ααα, we get
</p>
<p>g̃(ααα,βββ)=
&lang;
ααα,g&minus;1&lowast; (βββ)
</p>
<p>&rang;
</p>
<p>= 1
(p!)2αi1i2...ipβ
</p>
<p>j1j2...jp
&lang;
ǫǫǫi1 &and; &middot; &middot; &middot; &and; ǫǫǫip , ej1 &and; &middot; &middot; &middot; &and; ejp
</p>
<p>&rang;
</p>
<p>&equiv; 1
(p!)2αi1i2...ipβ
</p>
<p>j1j2...jpǫǫǫi1 &and; ǫǫǫi2 &and; &middot; &middot; &middot; &and; ǫǫǫip (ej1, ej2 , . . . , ejp )
</p>
<p>= 1
(p!)2
</p>
<p>&sum;
</p>
<p>π
</p>
<p>ǫπδ
i1
π(j1)
</p>
<p>δ
i2
π(j2)
</p>
<p>&middot; &middot; &middot; δip
π(jp)
</p>
<p>αi1i2...ipβ
j1j2...jp ,
</p>
<p>where in the last step we used Eq. (26.18). Therefore,
</p>
<p>g̃(ααα,βββ)= 1
(p!)2
</p>
<p>&sum;
</p>
<p>π
</p>
<p>ǫπαπ(j1)π(j2)...π(jp)β
j1j2...jp = 1
</p>
<p>p!αj1j2...jpβ
j1j2...jp ,
</p>
<p>(26.52)</p>
<p/>
</div>
<div class="page"><p/>
<p>820 26 Tensors
</p>
<p>because απ(j1)π(j2)...π(jp) = ǫπαj1j2...jp due to the antisymmetry of the com-
ponents of p-forms.
</p>
<p>Having found g̃, we can extend it further to Λp(V,U) if U has an inner
product h. Let {fa}dimUa=1 be a basis of U, and note that any ααα &isin; Λp(V,U)
can be written as ααα = &sum;dimUa=1 αααafa , where αααa &isin; Λp(V). Denote the inner
product of Λp(V,U) as g̃h and, for
</p>
<p>ααα =
dimU&sum;
</p>
<p>a=1
αααafa, βββ =
</p>
<p>dimU&sum;
</p>
<p>b=1
βββbfb,
</p>
<p>define it as
</p>
<p>g̃h(ααα,βββ)=
dimU&sum;
</p>
<p>a,b=1
g̃
(
αααa,βββb
</p>
<p>)
h(fa, fb)&equiv;
</p>
<p>dimU&sum;
</p>
<p>a,b=1
habg̃
</p>
<p>(
αααa,βββb
</p>
<p>)
. (26.53)
</p>
<p>It is routine to show that g̃h is basis-independent.
</p>
<p>26.6 The Hodge Star Operator
</p>
<p>It was established in Chap. 4 that all vector spaces of the same dimension are
isomorphic. Therefore, the two vector spaces Λp(V) and ΛN&minus;p(V) having
the same dimension,
</p>
<p>(
N
p
</p>
<p>)
=
</p>
<p>(
N
</p>
<p>N&minus;p
)
, must be isomorphic. In fact, there is a
</p>
<p>natural isomorphism between the two spaces:
</p>
<p>Hodge star operator Definition 26.6.1 Let g be an inner product and {ǫǫǫi}Ni=1 a g-
orthonormal ordered basis of V&lowast;. The Hodge star operator is a lin-
ear mapping, &lowast; :Λp(V)&rarr;ΛN&minus;p(V), given by (remember Einstein&rsquo;s
summation convention!)
</p>
<p>&lowast;
(
ǫǫǫi1 &and; &middot; &middot; &middot; &and; ǫǫǫip
</p>
<p>)
&equiv; 1
</p>
<p>(N &minus; p)!ǫ
i1i2...ip
jp+1...jNǫ
</p>
<p>ǫǫjp+1 &and; &middot; &middot; &middot; &and; ǫǫǫjN . (26.54)
</p>
<p>A similar star operator can be defined on p-vectors. ǫ
i1i2...ip
jp+1...jN is obtained
</p>
<p>from ǫj1...jN by raising its first p subscripts.
Although this definition is based on a choice of basis, it can be shown that
</p>
<p>the operator is basis-independent. In fact, ifωωω= 1
p!ωi1...ipǫǫǫ
</p>
<p>i1 &and;&middot; &middot; &middot;&and;ǫǫǫip , then
Eq. (26.54) gives
</p>
<p>&lowast;ωωω= 1
p!(N &minus; p)!ω
</p>
<p>j1...jpǫj1j2...jNǫǫǫ
jp+1 &and; &middot; &middot; &middot; &and; ǫǫǫjN . (26.55)</p>
<p/>
</div>
<div class="page"><p/>
<p>26.6 The Hodge Star Operator 821
</p>
<p>Now let {vj }Nj=1 be any other basis of V positively oriented relative to
{ǫǫǫi}Ni=1. Let {θθθ j }Nj=1 be dual to {vj }Nj=1. Write vi = R
</p>
<p>j
i ej and (therefore)
</p>
<p>θθθ i = (R&minus;1)ijǫǫǫj . Furthermore, denoting g&minus;1(ωωω) by ω̃ωω, we have
</p>
<p>ωj1...jp &equiv; ω̃ωω
(
ǫǫǫj1 , . . .ǫǫǫjp
</p>
<p>)
= ω̃ωω
</p>
<p>(
R
j1
i1
θθθ i1 , . . .R
</p>
<p>jp
ip
θθθ ip
</p>
<p>)
=Rj1i1 . . .R
</p>
<p>jp
ip
ω̄i1...ip ,
</p>
<p>where ω̄i1...ip are the components of ωωω in the general basis {vj }Nj=1. Substi-
tuting the last expression above and ǫǫǫj &rsquo;s in terms of θθθ i &rsquo;s in Eq. (26.55), we
get
</p>
<p>&lowast;ωωω= 1
p!(N &minus; p)!R
</p>
<p>j1
i1
. . .R
</p>
<p>jp
ip
ω̄i1...ipǫj1j2...jN
</p>
<p>(
R
jp+1
ip+1 θ
</p>
<p>θθ ip+1
)
&and; &middot; &middot; &middot; &and;
</p>
<p>(
R
jN
iN
θθθ iN
</p>
<p>)
</p>
<p>= 1
p!(N &minus; p)! ω̄
</p>
<p>i1...ip
(
ǫj1j2...jNR
</p>
<p>j1
i1
. . .R
</p>
<p>jN
iN
</p>
<p>)
︸ ︷︷ ︸
</p>
<p>=ǫi1i2 ...iN detR
</p>
<p>θθθ ip+1 &and; &middot; &middot; &middot; &and; θθθ iN .
</p>
<p>Using the result detR = |detG|1/2 &equiv; |G|1/2 (Problem 26.26), where G de-
notes the matrix of g in {vj }Nj=1, we finally obtain
</p>
<p>&lowast;ωωω= |G|1/2 1
p!(N &minus; p)! ω̄
</p>
<p>i1...ipǫi1i2...iNθθθ
ip+1 &and; &middot; &middot; &middot; &and; θθθ iN
</p>
<p>= |G|1/2 1
p! ω̄
</p>
<p>i1...ipǫi1i2...iNθθθ
ip+1 &otimes; &middot; &middot; &middot; &otimes; θθθ iN , (26.56)
</p>
<p>where the last equality follows because θθθ ip+1 &otimes; &middot; &middot; &middot; &otimes; θθθ iN does not have a
symmetry. Note that this last expression reduces to (26.55), because |G| = 1
in an orthonormal basis, and &lowast;ωωω as given by Eq. (26.56) is indeed basis-
independent.
</p>
<p>Example 26.6.2 Let us apply Definition 26.6.1 to Λp(R3&lowast;) for p = 0,1,
2,3. Let {e1, e2, e3} be an oriented orthonormal basis of R3.
(a) For Λ0(R3&lowast;)=R a basis is 1, and (26.54) gives
</p>
<p>&lowast;1 = 1
3!ǫ
</p>
<p>ijkei &and; ej &and; ek = e1 &and; e2 &and; e3.
</p>
<p>(b) For Λ1(R3&lowast;) = R3 a basis is {e1, e2, e3}, and (26.54) gives &lowast;ei =
1
2!ǫ
</p>
<p>jk
i ej &and; ek , or &lowast;e1 = e2 &and; e3, &lowast;e2 = e3 &and; e1, &lowast;e3 = e1 &and; e2.
</p>
<p>(c) For Λ2(R3&lowast;) a basis is {e1 &and; e2, e1 &and; e3, e2 &and; e3}, and (26.54) gives
&lowast;ei&and;ej = ǫkijek , or &lowast;(e1&and;e2)= e3, &lowast;(e1&and;e3)=&minus;e2, &lowast;(e2&and;e3)= e1.
</p>
<p>(d) For Λ3(R3&lowast;) a basis is {e1 &and; e2 &and; e3}, and (26.54) yields &lowast;(e1 &and; e2 &and;
e3)= ǫ123 = 1.
</p>
<p>The preceding example may suggest that applying the Hodge star opera-
tor twice (composition of &lowast; with itself, or &lowast;◦&lowast;) is equivalent to applying the
identity operator. This is partially true. The following theorem is a precise
statement of this conjecture. (For a proof, see [Bish 80, p. 111].)</p>
<p/>
</div>
<div class="page"><p/>
<p>822 26 Tensors
</p>
<p>Theorem 26.6.3 Let V be an oriented space with an inner product g.
For A &isin;Λp(V), we have
</p>
<p>&lowast; ◦ &lowast;A&equiv; &lowast; &lowast;A= (&minus;1)ν(&minus;1)p(N&minus;p)A, (26.57)
</p>
<p>where ν is the index of g and N = dimV.
</p>
<p>In particular, for Euclidean spaces with an odd number of dimensions
(such as R3), &lowast; &lowast;A= A.
</p>
<p>One can extend the star operation to any A &isin; Λp(V) by writing A as a
linear combination of basis vectors of Λp(V) constructed out of {ei}Ni=1, and
using the linearity of &lowast;.
</p>
<p>The star operator creates an (N &minus; p)-form out of a p-form. If we take
the exterior product of a p-form and the star of another p-form, we get an
N -form, which is proportional to a volume element. In fact, one can prove
(Problem 26.33)
</p>
<p>Theorem 26.6.4 Let (V,g) be an inner product space and μμμ a vol-
ume element relative to g. Let g̃ be the inner product induced by g on
Λp(V) and given explicitly in Eq. (26.52). Then for ααα,βββ &isin;Λp(V), we
have ααα &and; &lowast;βββ = g̃(ααα,βββ)μμμ.
</p>
<p>In the discussion of exterior algebra one encounters sums of the formantisymmetric tensors
with numerical
</p>
<p>coefficients A
i1...ipvi1 &and; &middot; &middot; &middot; &and; vip .
</p>
<p>It is important to note that Ai1...ip is assumed skew-symmetric. For example,
if A = e1 &and; e2, then in the sum A = Aijei &and; ej , the nonzero components
consist of A12 = 12 and A21 = &minus; 12 . Similarly, when B = e1 &and; e2 &and; e3 is
written in the form B = Bijkei &and; ej &and; ek , it is understood that the nonzero
components of B are not restricted to B123. Other components, such as B132,
B231, and so on, are also nonzero. In fact, we have
</p>
<p>B123 =&minus;B132 =&minus;B213 = B231 = B312 =&minus;B321 = 1
6
.
</p>
<p>This should be kept in mind when sums over exterior products with numer-
ical coefficients are encountered.
</p>
<p>Example 26.6.5 Let a,b &isin; R3 and {e1, e2, e3} an oriented orthonormalCross product is defined
only in three
</p>
<p>dimensions!
</p>
<p>basis of R3. Then a = aiei and b = bj ej . Let us calculate a &and; b and
&lowast;(a &and; b). We assume a Euclidean g on R3. Then a &and; b = (aiei)&and; (bj ej )=
aibjei &and; ej , and
</p>
<p>&lowast;(a&and;b)= &lowast;
(
aiei
</p>
<p>)
&and;
(
bjej
</p>
<p>)
= aibj &lowast; (ei &and;ej )= aibj
</p>
<p>(
ǫkij ek
</p>
<p>)
=
(
ǫkija
</p>
<p>ibj
)
ek.
</p>
<p>We see that &lowast;(a &and; b) is a vector with components [&lowast;(a &and; b)]k = ǫkijaibj ,
which are precisely the components of a &times; b.</p>
<p/>
</div>
<div class="page"><p/>
<p>26.7 Problems 823
</p>
<p>The correspondence between a &and; b and a &times; b holds only in three dimen-
sions, because dimΛ1(V)= dimΛ2(V) only if dimV= 3.
</p>
<p>Example 26.6.6 We can use the results of Examples 26.5.26 and 26.6.5 to
establish a sample of familiar vector identities componentwise.
</p>
<p>(a) For the triple cross product, we have
</p>
<p>[
a &times; (b &times; c)
</p>
<p>]k = ǫkijai(b &times; c)j = ǫkijai
(
ǫ
j
lmb
</p>
<p>lcm
)
= aiblcmǫkij ǫj lm
</p>
<p>= aiblcmǫkij ǫlmj = aiblcm
(
δkl δ
</p>
<p>i
m &minus; δkmδil
</p>
<p>)
</p>
<p>= aibkci &minus; aibick = (a &middot; c)bk &minus; (a &middot; b)ck,
</p>
<p>which is the kth component of b(a &middot; c)&minus; c(a &middot;b). In deriving the above
&ldquo;bac cab&rdquo; rule, we used the fact that one can swap an upper index with
the same lower index: aibi = aibi .
</p>
<p>(b) Next we show the familiar statement that the divergence of curl is zero.
Let &part;i denote differentiation with respect to xi . Then
</p>
<p>&nabla;&nabla;&nabla; &middot; (&nabla;&nabla;&nabla; &times; a)= &part;i(&nabla;&nabla;&nabla; &times; a)i = &part;iǫijk&part;jak = ǫijk&part;i&part;jak
=&minus;ǫjik&part;i&part;jak =&minus;ǫjik&part;j&part;iak =&minus;&part;j
</p>
<p>(
ǫjik&part;iak
</p>
<p>)
</p>
<p>=&minus;&part;j (&nabla;&nabla;&nabla; &times; a)j =&minus;&nabla;&nabla;&nabla; &middot; (&nabla;&nabla;&nabla; &times; a) &rArr; &nabla;&nabla;&nabla; &middot; (&nabla;&nabla;&nabla; &times; a)= 0.
</p>
<p>(c) Finally, we show that curl of gradient is zero:
</p>
<p>[
&nabla;&nabla;&nabla; &times; (&nabla;&nabla;&nabla;f )
</p>
<p>]k = ǫijk&part;j&part;kf = ǫijk&part;j&part;kf = 0,
</p>
<p>because ǫijk is antisymmetric in jk, while &part;j&part;kf is symmetric in jk.
</p>
<p>Example above shows in general that
</p>
<p>Box 26.6.7 When the product of two tensors is summed over a pair
of indices in which one of the tensors is symmetric and the other anti-
symmetric, the result is zero.
</p>
<p>26.7 Problems
</p>
<p>26.1 Show that the mapping v : V&lowast; &rarr;R given by v(τττ )= τττ(v) is linear.
</p>
<p>26.2 Show that the components of a tensor product are the products of the
components of the factors:
</p>
<p>(U&otimes; T)i1...ir+kj1...js+l =U
i1...ir
j1...js
</p>
<p>T
ir+1...ir+k
js+1...js+l .</p>
<p/>
</div>
<div class="page"><p/>
<p>824 26 Tensors
</p>
<p>26.3 Show that ej1 &otimes; &middot; &middot; &middot; &otimes; ejr &otimes; ǫǫǫi1 &otimes; &middot; &middot; &middot; &otimes; ǫǫǫis are linearly independent.
Hint: Consider Aj1...jri1...is ej1 &otimes; &middot; &middot; &middot; &otimes; ejr &otimes; ǫǫǫ
</p>
<p>i1 &otimes; &middot; &middot; &middot; &otimes; ǫǫǫis = 0 and evaluate the
LHS on appropriate tensors to show that all coefficients are zero.
</p>
<p>26.4 What is the tensor product of A = 2ex &minus; ey + 3ez with itself?
</p>
<p>26.5 If A &isin; L(V) is represented by Aij in the basis {ei} and by A&prime;kl in {e&prime;k},
then show that
</p>
<p>A&prime;kl e
&prime;
k &otimes; ǫǫǫ&prime;l =Aijei &otimes; ǫǫǫj ,
</p>
<p>where {ǫǫǫj } and {ǫǫǫ&prime;l} are dual to {ei} and {e&prime;k}, respectively.
</p>
<p>26.6 Prove that the linear functional F : V &rarr; R is a linear invariant, i.e.,
basis-independent, function.
</p>
<p>26.7 Show that tr : T11 &rarr;R is an invariant linear function.
</p>
<p>26.8 If A is skew-symmetric in some pair of variables, show that S(A)= 0.
</p>
<p>26.9 Using the exterior product show whether the following three vectors
are linearly dependent or independent:
</p>
<p>v1 = 2e1 &minus; e2 + 3e3 &minus; e4,
v2 =&minus;e1 + 3e2 &minus; 2e4,
v3 = 3e1 + 2e2 &minus; 4e3 + e4.
</p>
<p>26.10 Show that {ek &and; ei} with k &lt; i are linearly independent.
</p>
<p>26.11 Let v &isin; V be nonzero, and let A &isin; Λp(V&lowast;). Show that v &and; A = 0 if
and only if there exists B &isin;Λp&minus;1(V&lowast;) such that A = v &and; B. Hint: Let v be
the first vector of a basis; separate out v in the expansion of A in terms of
the p-fold wedge products of basis vectors, and multiply the result by v.
</p>
<p>26.12 Let A &isin; Λ2(V) with components Aij . Show that A &and; A = 0 if and
only if AijAkl &minus;AikAj l +AilAjk = 0 for all i, j, k, l in any basis.
</p>
<p>26.13 Let {e1, e2, e3} be any basis in R3. Define an operator E : R3 &rarr; R3
that permutes any set of three vectors {v1,v2,v3} to {vi,vj ,vk}. Find the
matrix representation of this operator and show that detE= ǫijk .
</p>
<p>26.14 Starting with the definition of the permutation tensor δi1i2...iNj1j2...jN , and
writing the wedge product in terms of the antisymmetrized tensor product,
show that
</p>
<p>δ
i1i2...iN
j1j2...jN
</p>
<p>=
&sum;
</p>
<p>π
</p>
<p>ǫπ(j1)π(j2)...π(jN )δ
i1
π(j1)
</p>
<p>δ
i2
π(j2)
</p>
<p>&middot; &middot; &middot; δiNπ(jN ).
</p>
<p>26.15 Derive Eqs. (26.49) and (26.50).</p>
<p/>
</div>
<div class="page"><p/>
<p>26.7 Problems 825
</p>
<p>26.16 Show that a 2-form ωωω is nondegenerate if and only if the determinant
of (ωij ) is nonzero if and only if ωωω♭ is an isomorphism.
</p>
<p>26.17 Let V be a finite-dimensional vector space and ωωω &isin;Λ2(V). Suppose
there exist a pair of vectors e1, e&prime;1 &isin; V such that ωωω(e1, e&prime;1) �= 0. Let P1 be the
plane spanned by e1 and e&prime;1, and V1 the ωωω-orthogonal complement of P1.
Show that V1 &cap;P1 = 0, and that v &minus;ωωω(v, e&prime;1)e1 +ωωω(v, e1)e&prime;1 is in V1.
</p>
<p>26.18 Show that
&sum;n
</p>
<p>j=1 ǫǫǫ
j &and; ǫǫǫj+n, in which {ǫǫǫj }Nj=1 is dual to {ei}Ni=1, the
</p>
<p>canonical basis of a symplectic vector space V, has the same matrix as ωωω.
</p>
<p>26.19 Suppose that V is a symplectic vector space and v,v&prime; &isin; V are ex-
pressed in a canonical basis of V with coefficients {xi, yi, zi} and {x&prime;i, y&prime;i, z&prime;i}.
Show that
</p>
<p>ωωω
(
v,v&prime;
</p>
<p>)
=
</p>
<p>n&sum;
</p>
<p>i=1
</p>
<p>(
xiy
</p>
<p>&prime;
i &minus; x&prime;iyi
</p>
<p>)
.
</p>
<p>26.20 Let V be a vector space and V&lowast; its dual. Define ωωω &isin;Λ2(V&oplus;V&lowast;) by
</p>
<p>ωωω
(
v +ϕϕϕ,v&prime; +ϕϕϕ&prime;
</p>
<p>)
&equiv;ϕϕϕ&prime;(v)&minus;ϕϕϕ
</p>
<p>(
v&prime;
)
</p>
<p>where v,v&prime; &isin; V and ϕϕϕ,ϕϕϕ&prime; &isin; V&lowast;. Show that (V&oplus;V&lowast;,ωωω) is a symplectic vector
space.
</p>
<p>26.21 By taking successive powers of ωωω show that
</p>
<p>ωωωk =
n&sum;
</p>
<p>j1...jk=1
ǫǫǫj1 &and; ǫǫǫj1+n &and; &middot; &middot; &middot; &and; ǫǫǫjk &and; ǫǫǫjk+n.
</p>
<p>Conclude that
</p>
<p>ωωωn = n!(&minus;1)[n/2]ǫǫǫ1 &and; &middot; &middot; &middot; &and; ǫǫǫ2n,
where [n/2] is the largest integer less than or equal to n/2.
</p>
<p>26.22 Show that the condition for a matrix A to be symplectic is AtJA = J
where J =
</p>
<p>( 0 1
&minus;1 0
</p>
<p>)
is the representation of ωωω in the canonical basis.
</p>
<p>26.23 Show that Sp(V,ωωω) is a subgroup of GL(V).
</p>
<p>26.24 Show that the linear operator gW defined in Lemma 26.5.13 is onto.
</p>
<p>26.25 (a) Show that the inverse of the (diagonal) matrix of g in an orthonor-
mal basis is the same as the matrix of g.
</p>
<p>(b) Now show that ǫ12...N = (&minus;1)νǫ12...N = (&minus;1)ν .
</p>
<p>26.26 Let {ei}Ni=1 be a g-orthonormal basis of V. Let ηηη be the matrix with
elements ηij , which is the matrix of g in this orthonormal basis. Let {vj }Nj=1
be another (not necessarily orthonormal) basis of V with a transformation
matrix R, i.e., vi = rji ej .</p>
<p/>
</div>
<div class="page"><p/>
<p>826 26 Tensors
</p>
<p>(a) Using G to denote the matrix of g in {vj }Nj=1, show that
</p>
<p>det G = detηηη(det R)2 = (&minus;1)ν(det R)2.
</p>
<p>In particular, the sign of this determinant is invariant. Why is det G
not equal to detηηη? Is there any conflict with the statement that the
determinant is basis-independent?
</p>
<p>(b) Let μμμ be the volume element related to g, and let |G| = |detG|. Show
that if {vj }Nj=1 is positively oriented relative to μμμ, then
</p>
<p>μμμ= |G|1/2v1 &and; v2 &and; &middot; &middot; &middot; &and; vN .
</p>
<p>26.27 Let b be a symmetric bilinear form. Show that the kernel of b&lowast; : V&rarr;
V&lowast; consists of all vectors u &isin; V such that b(u,v) = 0 for all v &isin; V. Show
also that in the b-orthonormal basis {ej }, the set {ei | b(ei, ei)= 0} is a basis
of kerb, and therefore the set of linearly independent isotropic vectors is the
nullity of b.
</p>
<p>26.28 For this problem, we return to the Dirac bra and ket notation. Let T be
an isometry in the real vector space V. Then |y〉 = (T&minus; 1)|x〉 is the vector,
which, in three-dimensions, connects the tip of |x〉 to its isometric image.
(a) Show that 〈y|y〉 = 2〈x|(1&minus; T)|x〉.
(b) Show that
</p>
<p>Py = (T&minus; 1)
|x〉〈x|
</p>
<p>2〈x|(1&minus; T)|x〉
(
Tt &minus; 1
</p>
<p>)
</p>
<p>and
</p>
<p>Ry = 1&minus; (T&minus; 1)
|x〉〈x|
</p>
<p>〈x|(1&minus; T)|x〉
(
Tt &minus; 1
</p>
<p>)
.
</p>
<p>(c) Verify that Ry |x〉 = T|x〉, as we expect.
</p>
<p>26.29 Use Eq. (26.51) to show that for a 3 &times; 3 matrix A,
</p>
<p>det A = 1
3!
[
(tr A)3 &minus; 3 tr A tr
</p>
<p>(
A2
</p>
<p>)
+ 2 tr
</p>
<p>(
A3
</p>
<p>)]
.
</p>
<p>26.30 Find the index and the signature for the bilinear form g on R3 given
by g(v1,v2)= x1y2 + x2y1 &minus; y1z2 &minus; y2z1.
</p>
<p>26.31 In relativistic electromagnetic theory the current J and the electro-
magnetic field tensor F are, respectively, a four-vector8 and an antisymmet-
ric tensor of rank 2. That is, J = J kek and F = F ij ei &and; ej . Find the com-
ponents of &lowast;J and &lowast;F. Recall that the space of relativity is a 4D Minkowski
space.
</p>
<p>8It turns out to be more natural to consider J as a 3-form. However, such a fine distinction
is not of any consequence for the present discussion.</p>
<p/>
</div>
<div class="page"><p/>
<p>26.7 Problems 827
</p>
<p>26.32 Show that ǫj1j2...jNR
j1
i1
. . .R
</p>
<p>jN
iN
</p>
<p>= ǫi1i2...iN detR.
</p>
<p>26.33 Prove Theorem 26.6.4.
</p>
<p>26.34 Show that where there is a sum over an upper index and a lower
index, swapping the upper index to a lower index, and vice versa, does not
change the sum. In other words, AiBi =AiBi .
</p>
<p>26.35 Show the following vector identities, using the definition of cross
products in terms of ǫijk .
</p>
<p>(a) A &times; A = 0.
(b) &nabla;&nabla;&nabla; &middot; (A &times; B)= (&nabla;&nabla;&nabla; &times; A) &middot; B &minus; (&nabla;&nabla;&nabla; &times; B) &middot; A.
(c) &nabla;&nabla;&nabla; &times; (A &times; B)= (B &middot; &nabla;&nabla;&nabla;)A + A(&nabla;&nabla;&nabla; &middot; B)&minus; (A &middot; &nabla;&nabla;&nabla;)B &minus; B(&nabla;&nabla;&nabla; &middot; A).
(d) &nabla;&nabla;&nabla; &times; (&nabla;&nabla;&nabla; &times; A)=&nabla;&nabla;&nabla;(&nabla;&nabla;&nabla; &middot; A)&minus;&nabla;2A.
</p>
<p>26.36 A vector operator V is defined as a set of three operators, {V1,V2,V3},
satisfying the following commutation relations with angular momentum:
[Vi, Jj ] = iǫijkVk . Show that VkVk commutes with all components of an-
gular momentum.
</p>
<p>26.37 The Pauli spin matrices
</p>
<p>σ 1 =
(
</p>
<p>0 1
1 0
</p>
<p>)
, σ 2 =
</p>
<p>(
0 &minus;i
i 0
</p>
<p>)
, σ 3 =
</p>
<p>(
1 0
0 &minus;1
</p>
<p>)
</p>
<p>describe a particle with spin 12 in nonrelativistic quantum mechanics. Verify
that these matrices satisfy
</p>
<p>[
σ i, σ j
</p>
<p>]
&equiv; σ iσ j&minus;σ jσ i = 2iǫijk σ k,
</p>
<p>{
σ i, σ j
</p>
<p>}
&equiv; σ iσ j+σ jσ i = 2δij12,
</p>
<p>where 12 is the unit 2&times;2 matrix. Show also that σ iσ j = iǫijk σ k + δij12, and
for any two vectors a and b, (σσσ &middot; a)(σσσ &middot; b)= a &middot; b12 + iσσσ &middot; (a &times; b).
</p>
<p>26.38 Show that any contravariant tensor of rank two can be written as the
sum of a symmetric tensor and an antisymmetric tensor. Can this be gener-
alized to tensors of arbitrary rank?</p>
<p/>
</div>
<div class="page"><p/>
<p>27Clifford Algebras
</p>
<p>The last chapter introduced the exterior product, which multiplied a p-
vector and a q-vector to yield a (p + q)-vector. By directly summing the
spaces of all such vectors, we obtained a vector space which was closed un-
der multiplication. This led to a 2n-dimensional algebra, which we called
the exterior algebra (see Theorem 26.3.6).
</p>
<p>In the meantime we revisited inner product and considered non-Euclidean
inner products, which are of physical significance. In this chapter, we shall
combine the exterior product with the inner product to create a new type of
algebra, the Clifford algebra, which happens to have important applications
in physics.
</p>
<p>In our definition of exterior product in the previous chapter, we assumed
that the number of vectors was equal to the number of linear functionals
taken from the dual space [see Eq. (26.14)]. As a result of this complete
pairing, we always ended up with a number. It is useful, however, to define
an &ldquo;incomplete&rdquo; pairing in which the number of vectors and dual vectors
are not the same. In particular, if we have a p-vector and a single 1-form,
then we can pair the 1-form with one of the factors of the p-vector to get a
(p&minus; 1)-vector. This process is important enough to warrant the following:
</p>
<p>Definition 27.0.1 Let A be a p-vector and θθθ a 1-form in a vector space V.
Then define iθθθ :Λp(V&lowast;)&rarr;Λp&minus;1(V&lowast;) by
</p>
<p>interior product of a
</p>
<p>1-form and a p-vector
iθθθA(θθθ1, . . . ,θθθp&minus;1)= A(θθθ,θθθ1, . . . ,θθθp&minus;1).
</p>
<p>iθθθA is called the interior product or contraction of θθθ and A.
</p>
<p>Note that if A is a 1-vector v, then iθθθv = 〈θθθ,v〉, and if it is a real num-
ber α, then (by definition) iθθθα = 0.
</p>
<p>An immediate consequence of Definition 27.0.1 is the following:
</p>
<p>Theorem 27.0.2 Let A be a p-vector and B be a q-vector on a vector
space V. Then, iθθθ is an antiderivation with respect to the wedge product: antiderivation
</p>
<p>iθθθ (A&and; B)= (iθθθA)&and; B+ (&minus;1)pA&and; (iθθθB).
</p>
<p>S. Hassani, Mathematical Physics, DOI 10.1007/978-3-319-01195-0_27,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>829</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-01195-0_27">http://dx.doi.org/10.1007/978-3-319-01195-0_27</a></div>
</div>
<div class="page"><p/>
<p>830 27 Clifford Algebras
</p>
<p>If (V,g) is an inner product space, we can define the interior product
of a 1-vector v and a p-vector A. In fact, if g&lowast; : V &rarr; V&lowast; is as defined in
Eq. (26.30), then let
</p>
<p>ivA&equiv; ig&lowast;(v)A. (27.1)
</p>
<p>In particular, if A is a vector u, then ivu = g(u,v).
</p>
<p>27.1 Construction of Clifford Algebras
</p>
<p>Let V be a real vector space with inner product g. Let v &isin; V and A &isin;
Λp(V&lowast;). Define the product &or; : V&times;Λp(V&lowast;)&rarr;Λp+1(V&lowast;)&oplus;Λp&minus;1(V&lowast;) by
</p>
<p>Clifford product
</p>
<p>v &or;A= v &and;A+ ivA (27.2)
</p>
<p>where ivA is as defined in Eq. (27.1). This product is called the Clifford
product.
</p>
<p>The special case of p = 1 is of importance. For such a case, we obtain
</p>
<p>v &or; u = v &and; u + ivu = v &and; u + g(u,v) (27.3)
</p>
<p>which can also be written as
</p>
<p>v &or; u + u &or; v = 2g(u,v). (27.4)
</p>
<p>This equation is sometimes taken as the definition of the Clifford product
and the starting point of the Clifford algebra (to be discussed below).
</p>
<p>We see that the Clifford product has been defined on the vector space
which underlies the exterior algebra. However, the left factor in the Clifford
product is just a vector, not a general member of the exterior algebra. Is it
possible to define Clifford product of a q-vector and a p-vector? It is indeed
possible if we assume that &or; is associative and distributes over addition. To
show this, pick a basis and write a q-vector in terms of that basis. Thus, let
A be as before and let B &isin;Λq(V&lowast;), and write
</p>
<p>B= 1
q!b
</p>
<p>j1...jq ej1 &and; &middot; &middot; &middot; &and; ejq .
</p>
<p>Then
</p>
<p>q!B&or;A= bj1...jq ej1 &and; &middot; &middot; &middot; &and; ejq &or;A
</p>
<p>= bj1...jq ej1 &and; &middot; &middot; &middot; &and; ejq&minus;2 &and; (ejq&minus;1 &or; ejq + gjqjq&minus;1)&or;A
</p>
<p>= bj1...jq ej1 &and; &middot; &middot; &middot; &and; ejq&minus;2 &and; (ejq&minus;1 &or; ejq &or;A) (27.5)
</p>
<p>because gjqjq&minus;1 is symmetric under the exchange of jq and jq&minus;1 while
bj1...jq is antisymmetric. To continue, we use Eq. (27.2) and rewrite the term
in the parentheses on the last line of Eq. (27.5):</p>
<p/>
</div>
<div class="page"><p/>
<p>27.1 Construction of Clifford Algebras 831
</p>
<p>ejq&minus;1 &or; ejq &or;A= ejq&minus;1 &or; (ejq &and;A+ iejqA)
</p>
<p>= ejq&minus;1 &or; (ejq &and;A)+ ejq&minus;1 &or; (iejqA)
</p>
<p>= ejq&minus;1 &and; (ejq &and;A)+ iejq&minus;1 (ejq &and;A)
</p>
<p>+ ejq&minus;1 &and; (iejqA)+ iejq&minus;1 (iejqA)
</p>
<p>= ejq&minus;1 &and; ejq &and;A+ gjqjq&minus;1A&minus; ejq &and; (iejq&minus;1A)
</p>
<p>+ ejq&minus;1 &and; (iejqA)+ iejq&minus;1 (iejqA),
</p>
<p>where in the last equality, we used the antiderivation property of the interior
product (Theorem 27.0.2). Substituting the last equation in (27.5) yields
</p>
<p>q!B&or;A= q!B&and;A+ bj1...jq ej1 &and; &middot; &middot; &middot; &and; ejq&minus;2 &and; ejq&minus;1 &and; (iejqA)
</p>
<p>&minus; bj1...jq ej1 &and; &middot; &middot; &middot; &and; ejq&minus;2 &and; ejq &and; (iejq&minus;1A)
</p>
<p>+ bj1...jq ej1 &and; &middot; &middot; &middot; &and; ejq&minus;2 &and;
[
iejq&minus;1 (iejqA)
</p>
<p>]
. (27.6)
</p>
<p>The right-hand side is given entirely in terms of wedge products, which
are known operations. Hence, the Clifford product of any p-vector and q-
vector can be defined, and this product is in Λ(V&lowast;) of Theorem 26.3.6. Thus,
Λ(V&lowast;) is an algebra not only under the wedge product but also under the
Clifford product. With the latter as the multiplication rule, Λ(V&lowast;) is called
a Clifford algebra and denoted by CV .
</p>
<p>Clifford algebra
</p>
<p>Historical Notes
</p>
<p>At the age of 15 William Clifford went to King&rsquo;s College, London where he excelled
</p>
<p>William Clifford
</p>
<p>1845&ndash;1879
</p>
<p>in mathematics, classics, English literature, and gymnastics. Three years later, he entered
Trinity College, Cambridge, where he won not only prizes for mathematics but also one
for a speech he delivered on Sir Walter Raleigh. In 1868, he was elected to a Fellowship at
Trinity, and three years later, he was appointed to the chair of Mathematics and Mechanics
at University College London. In 1874 he was elected a Fellow of the Royal Society. He
was also an active member of the London Mathematical Society which held its meetings
at University College.
Clifford read the work of Riemann and Lobachevsky on non-euclidean geometry, and
became interested in the subject. Almost 50 years before the advent of Einstein&rsquo;s general
theory of relativity, he wrote On the space theory of matter in which he argued that energy
and matter are different aspects of the curvature of space.
Clifford generalised the quaternions (introduced by Hamilton two years before Clifford&rsquo;s
birth) to what he called the biquaternions and he used them to study motion in non-
euclidean spaces and on certain surfaces.
As a teacher, Clifford&rsquo;s reputation was outstanding and famous for his clarity of expla-
nation of difficult mathematical problems. Not only was he a highly original teacher and
researcher, he was also a philosopher of science. At the age of 23 he delivered a lecture
to the Royal Institution entitled Some of the conditions of mental development, in which
he tried to explain how scientific discovery comes about.
He was eccentric in appearance, habits and opinions. A fellow undergraduate describes
him as follows: &ldquo;His neatness and dexterity were unusually great, but the most remarkable
thing was his great strength as compared with his weight. At one time he would pull up
on the bar with either hand.&rdquo;
Like another British mathematician, Charles Dodgson, he took pleasure in entertaining
children. Although he never achieved Dodgson&rsquo;s success in writing such books as Alice&rsquo;s
Adventures in Wonderland (which the latter wrote under the pseudonym Lewis Carroll),
Clifford wrote The Little People, a collection of fairy stories written to amuse children.</p>
<p/>
</div>
<div class="page"><p/>
<p>832 27 Clifford Algebras
</p>
<p>In 1876 Clifford suffered a physical collapse, which was made worse by overwork, and
most likely, caused by it. He would spend the entire day teaching and doing adminis-
trative work, and the entire night doing research. Spending six months in Algeria and
Spain allowed him to recover sufficiently to resume his work. But after 18 months he col-
lapsed again, after which he spent some time in Mediterranean countries, but this was not
enough to improve his health. After a couple of months in England in late 1878, he left
for Madeira. The hoped-for recovery never materialized and he died a few months later.
</p>
<p>We have shown that the Clifford product of a p-vector and a q-vector
lies in Λ(V&lowast;). This implies that the underlying vector space of the Clifford
algebra is a subspace of Λ(V&lowast;). However, it can be shown that the set of
Clifford products exhaust the entire Λ(V&lowast;); i.e., that the Clifford algebra is
2N -dimensional. This follows from the fact that a p-vector A, which can be
written as
</p>
<p>A= 1
p!a
</p>
<p>i1i2...ipei1 &and; ei2 &and; &middot; &middot; &middot; &and; eip , (27.7)
</p>
<p>can also be written as
</p>
<p>A&rarr; a= 1
p!a
</p>
<p>i1i2...ipei1 &or; ei2 &or; &middot; &middot; &middot; &or; eip , (27.8)
</p>
<p>where we have introduced a new notation to differentiate between members
of the exterior algebra and the Clifford algebra. The details of the derivation
of (27.8) from (27.7) are given as Problem 27.1.
</p>
<p>27.1.1 The Dirac Equation
</p>
<p>The interest in the Clifford algebra in the physics community came about
after Dirac discovered the relativistic wave equation for an electron. As is
usually the case, when a mathematical topic finds its way into physics, a
healthy collaboration between physicists and mathematicians sets in and the
topic becomes an active area of research in both fields. Dirac&rsquo;s discovery
and its connection with the Clifford algebra has led to some fundamental
results in many branches of mathematics. It is therefore worthwhile to see
how Dirac discovered the equation that now bears his name.
</p>
<p>The transition from classical to quantum mechanics is made by chang-
ing the energy E and momentum p to derivative operators:1 E &rarr; i&part;/&part;t
and p &rarr; &minus;i&nabla; which act on the wave function ψ . Thus a non-relativistic
free particle, whose energy and momentum are related by E = p2/2m, is
described by the Schr&ouml;dinger equation
</p>
<p>i
&part;ψ
</p>
<p>&part;t
= (&minus;i&nabla;)
</p>
<p>2
</p>
<p>2m
ψ or i
</p>
<p>&part;ψ
</p>
<p>&part;t
=&minus; 1
</p>
<p>2m
&nabla;2ψ.
</p>
<p>The relativistic energy-momentum relation, E2 &minus; p2 = m2, leads to
Klein-Gordon equation whose time derivative is of second order. Although
eventually accepted as a legitimate equation for relativistic particles, Klein-
</p>
<p>1We are using the natural units for which the Planck constant (over 2π ) and the speed of
light are set to 1: �= 1 = c.</p>
<p/>
</div>
<div class="page"><p/>
<p>27.1 Construction of Clifford Algebras 833
</p>
<p>Gordon equation was initially abandoned because, due to its second deriva-
tive in time, it gave rise to negative probabilities. Therefore, it was desirable
to find a relativistic equation which was first order in time derivative, and
Dirac found precisely such an equation.
</p>
<p>Dirac&rsquo;s idea was to factor out E2 &minus;p2 into (E&minus;p)(E+p) and to some-
how incorporate the mass term in the factorization. We avoid writing E and
p as derivatives, but consider them as commuting operators. Since it is not
possible to include m in a straightforward factorization, Dirac came up with
the ingenious idea of multiplying E and p operators by quantities to be de-
termined by certain consistency conditions. More precisely, he considered
an equation of the form
</p>
<p>(
βE +
</p>
<p>3&sum;
</p>
<p>i=1
αipi +m
</p>
<p>)
ψ = 0,
</p>
<p>and demanded that β and αi be chosen in such way that
</p>
<p>(
βE +
</p>
<p>3&sum;
</p>
<p>j=1
αjpj &minus;m
</p>
<p>)(
βE +
</p>
<p>3&sum;
</p>
<p>i=1
αipi +m
</p>
<p>)
ψ = 0 (27.9)
</p>
<p>reduce to
(
E2 &minus;
</p>
<p>3&sum;
</p>
<p>i=1
p2i &minus;m2
</p>
<p>)
ψ = 0. (27.10)
</p>
<p>Multiplying the two parentheses above, we obtain
</p>
<p>β2E2 +
3&sum;
</p>
<p>i=1
βαiEpi + βmE +
</p>
<p>3&sum;
</p>
<p>j=1
αjβEpj +
</p>
<p>3&sum;
</p>
<p>i,j=1
αjαipjpi
</p>
<p>+
3&sum;
</p>
<p>j=1
mαjpj &minus; βmE &minus;
</p>
<p>3&sum;
</p>
<p>i=1
mαipi &minus;m2
</p>
<p>= β2E2 +
3&sum;
</p>
<p>i=1
(βαi + αiβ)Epi +
</p>
<p>1
</p>
<p>2
</p>
<p>3&sum;
</p>
<p>i,j=1
(αjαi + αiαj )pipj &minus;m2.
</p>
<p>For this to be equal to the expression in parentheses of Eq. (27.10), we need
to have
</p>
<p>β2 = 1, βαi + αiβ = 0,
1
</p>
<p>2
(αjαi + αiαj )=&minus;δij .
</p>
<p>The last condition is the result of the fact that pjpi is symmetric in ij , and
therefore, its product with the antisymmetric part of αiαj automatically van-
ishes. Letting β &equiv; γ 0 and αi = γ i , the above conditions can be condensed
into the single condition
</p>
<p>γ μγ ν + γ νγ μ = 2ημν . (27.11)</p>
<p/>
</div>
<div class="page"><p/>
<p>834 27 Clifford Algebras
</p>
<p>This equation is identical to (27.4), hence the connection between the Dirac
equation and Clifford algebra.
</p>
<p>It is clear that Eq. (27.11) cannot hold if the γ s are ordinary numbers.
In fact, Dirac showed that they have to be 4 &times; 4 matrices, now called Dirac
γ matrices. If the γ s are 4 &times; 4 matrices, then ψ must be a column vectorDirac γ matrices
with 4 components. It turns out that two of these components correspond
to the two components of the electron spin. It took a while before the other
two components were identified as those of the antiparticle of the electron,
namely positron.
</p>
<p>27.2 General Properties of the Clifford Algebra
</p>
<p>Equation (27.8) implies that a vector in Λ(V&lowast;), being a direct sum of p-
vectors for different p&rsquo;s, can be expressed as a linear combination of the
basis vectors of Λp(V&lowast;), where the basis vectors are given as Clifford prod-
uct (rather than the wedge product) of the basis vectors of V:
</p>
<p>Theorem 27.2.1 Let {ei}Ni=1 be a basis of an inner product space V. Then
the 2N vectors
</p>
<p>1, ei, ei &or; ej (i &lt; j), ei &or; ej &or; ek, (i &lt; j &lt; k), . . . , e1 &or; e2 &or; &middot; &middot; &middot; &or; eN
form a basis of CV .
</p>
<p>Thus, if u is an arbitrary vector of Λ(V&lowast;), then it can be expressed as
follows:
</p>
<p>u= α1 + uiei + u|i1i2|ei1 &or; ei2 + &middot; &middot; &middot; + u|i1...iN |ei1 &or; &middot; &middot; &middot; &or; eiN (27.12)
</p>
<p>where |i1i2 . . . ip| means that the sum over repeated indices is over i1 &lt; i2 &lt;
&middot; &middot; &middot;&lt; ip . Equation (27.12) is sometimes written as
</p>
<p>u= α1 + uiei +
1
</p>
<p>2!u
i1i2ei1 &or; ei2 + &middot; &middot; &middot; +
</p>
<p>1
</p>
<p>N !u
i1...iN ei1 &or; &middot; &middot; &middot; &or; eiN (27.13)
</p>
<p>where the coefficients are assumed completely antisymmetric in all their
indices, but the sum has no ordering.
</p>
<p>Note the appearance of 1 in the sum multiplying the scalar α. This sug-
gests changing (27.4) to
</p>
<p>v &or; u + u &or; v = 2g(u,v)1, (27.14)
</p>
<p>which, when specialized to the basis vectors, becomes
</p>
<p>eiej + ej ei = 2gij1, e2i &equiv; ei &or; ei = gii1, (27.15)
</p>
<p>where we have removed the multiplication sign &or;, a practice which we shall
often adhere to from now on. Equation (27.15) completely frees the Clifford
algebra from the exterior algebra, with which we started our discussion. This
is easily seen in an orthonormal basis:
</p>
<p>e2i =&plusmn;1, eiej =&minus;ejei if i �= j. (27.16)</p>
<p/>
</div>
<div class="page"><p/>
<p>27.2 General Properties of the Clifford Algebra 835
</p>
<p>Multiplying elements u and v, each expressed as in (27.12) or (27.13), re-
duces to the multiplication of various Clifford products of the basis vectors.
In such multiplications, one commutes the basis vectors which appear twice
in the product using (27.16) until all repetitions disappear and one regains
Clifford products of basis vectors appearing in (27.13). The following ex-
ample should clarify this.
</p>
<p>Example 27.2.2 Let V be a 2-dimensional Euclidean vector space (i.e.,
gij = δij ) with the orthonormal basis {e1, e2}. Consider two elements u and
v of the Clifford algebra over V. These can very generally be written as
</p>
<p>u= αu1 + β1ue1 + β2ue2 + γue1e2
v= αv1 + β1ve1 + β2ve2 + γve1e2
</p>
<p>and
</p>
<p>u&or; v=
(
αu1 + β1ue1 + β2ue2 + γue1e2
</p>
<p>)
&or;
(
αv1 + β1ve1 + β2ve2 + γve1e2
</p>
<p>)
</p>
<p>= αuαv1 + αuβ1ve1 + αuβ2ve2 + αuγve1e2
+ αvβ1ue1 + β1uβ1v e1e1︸︷︷︸
</p>
<p>=1
</p>
<p>+β1uβ2ve1e2 + β1uγv e1e1︸︷︷︸
=1
</p>
<p>e2
</p>
<p>+ βuα2ve2 + β2uβ1v e2e1︸︷︷︸
&minus;e1e2
</p>
<p>+β2uβ2v e2e2︸︷︷︸
=1
</p>
<p>+β2uγv
&minus;e1︷ ︸︸ ︷
</p>
<p>e2e1︸︷︷︸
&minus;e1e2
</p>
<p>e2
</p>
<p>+ γuαve1e2 + γuβ1v
&minus;e2︷ ︸︸ ︷
</p>
<p>e1 e2e1︸︷︷︸
&minus;e1e2
</p>
<p>+γuβ2ve1 e2e2︸︷︷︸
=1
</p>
<p>+γuγv
=&minus;1︷ ︸︸ ︷
</p>
<p>e1 e2e1︸︷︷︸
&minus;e1e2
</p>
<p>e2 .
</p>
<p>We see that the right-hand side is a linear combination of 1, e1, e2, and e1e2,
as it should be since the Clifford algebra is closed under multiplication.
Problem 27.2 asks you to find the coefficients of the linear combination.
</p>
<p>Although we will primarily be dealing with real vector spaces, Eqs. (27.15) complex Clifford
algebrasand (27.16) could be applied to complex vector spaces. Therefore, it is pos-
</p>
<p>sible to have complex Clifford algebras, and we shall occasionally deal with
such algebras as well.
</p>
<p>If u in (27.13) or (27.12) contains products of only even numbers of basis
vectors, then it is called an even element of the algebra. The collection of even and odd elements
all even elements of CV is a subalgebra of CV and is denoted by C0V . The
odd elements are denoted by C1V , and although they form a subspace of CV ,
obviously, they do not form a subalgebra. As a vector space, CV is the direct
sum of the even and odd subspaces:
</p>
<p>CV = C0V &oplus; C1V . (27.17)
</p>
<p>The discussion above can be made slightly more formal.</p>
<p/>
</div>
<div class="page"><p/>
<p>836 27 Clifford Algebras
</p>
<p>Definition 27.2.3 Let ω be the linear automorphism of V given by ω(a)=
&minus;a for all a &isin; V. The involution of the Clifford algebra CV induced by ω is
called the degree involution and is denoted by ωV .degree involution
</p>
<p>Note that for any u &isin; CV given by (27.13), ωV (u) is obtained by chang-
ing the sign of all the vectors in that equation. It is obvious that ω2V = ι,
where ι(u) = u for all u &isin; CV . Thus, ωV is indeed an involution of CV .
Now, an involution has only two eigenvalues, &plusmn;1, and the intersection of
the eigenspaces of these eigenvalues is zero. Moreover, we can identify these
eigenspaces as C0V and C
</p>
<p>1
V , where
</p>
<p>C
0
V = ker(ωV &minus; ι), C1V = ker(ωV + ι). (27.18)
</p>
<p>Consider two inner product spaces V and U, and define the inner product
on their direct sum V&oplus;U by
</p>
<p>〈v1 &oplus; u1,v2 &oplus; u2〉 &equiv; 〈v1,v2〉 + 〈u1,u2〉.
</p>
<p>Then we have the following important theorem: (For a proof, see [Greu 78,
p. 234])
</p>
<p>Theorem 27.2.4 Let W= V&oplus;U. Then the Clifford algebra CW is isomor-
phic to the skew symmetric tensor product of CV and CU :
</p>
<p>CW &equiv; CV&oplus;U &sim;= CV &otimes;̂CU .
</p>
<p>The skew symmetric tensor product was defined for exterior algebras
in Definition 26.3.7, but it can also be defined for Clifford algebras. One
merely has to change &and; to &or;. Note that the caret over &otimes; signifies the product
defined on the space CV &otimes; CU . Thus, as vector spaces, CV&oplus;U &sim;= CV &otimes; CU .
Since all Clifford algebras are direct sums of their even and odd subspaces,
we have
</p>
<p>C
0
W &oplus; C1W &sim;=
</p>
<p>(
C
</p>
<p>0
V &oplus; C1V
</p>
<p>)
&otimes;
(
C
</p>
<p>0
U &oplus; C1U
</p>
<p>)
</p>
<p>&sim;=
(
C
</p>
<p>0
V &otimes; C0U
</p>
<p>)
&oplus;
(
C
</p>
<p>0
V &otimes; C1U
</p>
<p>)
&oplus;
(
C
</p>
<p>1
V &otimes; C0U
</p>
<p>)
&oplus;
(
C
</p>
<p>1
V &otimes; C1U
</p>
<p>)
.
</p>
<p>In particular,
</p>
<p>C
0
W
</p>
<p>&sim;=
(
C
</p>
<p>0
V &otimes; C0U
</p>
<p>)
&oplus;
(
C
</p>
<p>1
V &otimes; C1U
</p>
<p>)
</p>
<p>C
1
W
</p>
<p>&sim;=
(
C
</p>
<p>0
V &otimes; C1U
</p>
<p>)
&oplus;
(
C
</p>
<p>1
V &otimes; C0U
</p>
<p>)
.
</p>
<p>(27.19)
</p>
<p>Furthermore, if we invoke the product of Definition 26.3.7 on the first equa-
tion above, we get
</p>
<p>C
0
W
</p>
<p>&sim;=
(
C
</p>
<p>0
V &otimes;̂C0U
</p>
<p>)
&oplus;
(
C
</p>
<p>1
V &otimes;̂C1U
</p>
<p>)
. (27.20)
</p>
<p>The second equation in (27.19), when restricted to vector spaces themselves,
yields
</p>
<p>W&sim;= (1V &otimes;U)&oplus; (V&otimes; 1U ), (27.21)</p>
<p/>
</div>
<div class="page"><p/>
<p>27.2 General Properties of the Clifford Algebra 837
</p>
<p>where 1V and 1U are the identities of CV and CU , respectively.
Consider the linear map σV : CV &rarr; CopV given by
</p>
<p>σV (a&or; b)= σV (b)&or; σV (a), σV (v)= v, a,b &isin; CV , v &isin; V. (27.22)
</p>
<p>It is straightforward to show that σV is an involution and that it commutes
with the degree involution:
</p>
<p>σV ◦ωV = ωV ◦ σV (27.23)
</p>
<p>Definition 27.2.5 The conjugation involution is defined as σV ◦ ωV . The conjugation involution
conjugate of an element a &isin; CV is
</p>
<p>ā= σV ◦ωV (a).
</p>
<p>In particular, v̄ =&minus;v if v &isin; V.
</p>
<p>It is clear from the definition that
</p>
<p>a&or; b= b̄&or; ā, a,b &isin; CV .
</p>
<p>We saw a special case of this relation in our discussion of the quaternions in
Example 3.1.16.
</p>
<p>27.2.1 Homomorphismwith Other Algebras
</p>
<p>Let V be an inner product space and A an algebra with identity. A linear map
ϕ : V&rarr;A can always be extended to a unital homomorphism φ : CV &rarr;A.
Indeed, since CV consists of sums of Clifford products of vectors in V, one
simply has to define the action of φ on a product of vectors in V. The obvious
choice is
</p>
<p>φ(v1 &or; v2 &or; &middot; &middot; &middot; &or; vm)= ϕ(v1)ϕ(v2) . . . ϕ(vm)
</p>
<p>where on the right-hand side the product in A is denoted by juxtaposition.
For ϕ to be extendable to a unital homomorphism, it has to be compatible
with Eq. (27.14); i.e., it has to satisfy
</p>
<p>φ(v &or; u)+ φ(u &or; v)= 2g(u,v)φ(1)
</p>
<p>or, denoting g(u,v) by 〈u,v〉,
</p>
<p>ϕ(v)ϕ(u)+ ϕ(u)ϕ(v)= 2〈u,v〉1A. (27.24)
</p>
<p>By setting u = v, we obtain an equivalent condition
</p>
<p>ϕ(v)2 = 〈v,v〉1A. (27.25)</p>
<p/>
</div>
<div class="page"><p/>
<p>838 27 Clifford Algebras
</p>
<p>Example 27.2.6 Let ϕ : R2 &rarr; L(R2) be a linear map. We want to extend
it to a homomorphism φ : CR2 &rarr;L(R2). It is convenient to identify L(R2)
with the set of 2 &times; 2 matrices and write
</p>
<p>ϕ(v)=
(
α11 α12
α21 α22
</p>
<p>)
. (27.26)
</p>
<p>Let v = (α,β). For the extension to be possible, according to Eq. (27.25),
we must have
</p>
<p>(
α11 α12
α21 α22
</p>
<p>)(
α11 α12
α21 α22
</p>
<p>)
=
(
α2 + β2
</p>
<p>)(1 0
0 1
</p>
<p>)
.
</p>
<p>One convenient solution to this equation is α11 = α = &minus;α22 and α12 =
α21 = β . Hence, we write Eq. (27.26) as
</p>
<p>ϕ(α,β)=
(
α β
</p>
<p>β &minus;α
</p>
<p>)
. (27.27)
</p>
<p>Now let {e1, e2} be the standard basis of R2. Then {1, e1, e2, e1 &or; e2} is a
basis of CR2 . Furthermore, φ(1) is the 2 &times; 2 unit matrix, and by (27.27)
</p>
<p>ϕ(e1)= ϕ(1,0)=
(
</p>
<p>1 0
0 &minus;1
</p>
<p>)
, ϕ(e2)= ϕ(0,1)=
</p>
<p>(
0 1
1 0
</p>
<p>)
,
</p>
<p>φ(e1 &or; e2)= ϕ(e1)ϕ(e2)=
(
</p>
<p>1 0
0 &minus;1
</p>
<p>)(
0 1
1 0
</p>
<p>)
=
(
</p>
<p>0 1
&minus;1 0
</p>
<p>)
.
</p>
<p>It is easy to show (see Problem 27.7) that these 4 matrices form a basis of
L(R2). Since φ maps a basis onto a basis, it is a linear isomorphism and
therefore and algebra isomorphism. Thus, CR2 &sim;=L(R2).
</p>
<p>27.2.2 The Canonical Element
</p>
<p>Strictly speaking, the identification of (27.8) with (27.7) should be consid-
ered as an isomorphism φV of Λ(V&lowast;) and CV :
</p>
<p>φV (ei1 &and; ei2 &and; &middot; &middot; &middot; &and; eip )= ei1 &or; ei2 &or; &middot; &middot; &middot; &or; eip . (27.28)
</p>
<p>Invoking Proposition 2.6.7, we conclude that,
</p>
<p>Definition 27.2.7 Given a determinant function ��� in V, there is a unique
element e� &isin; CV such thatcanonical element
</p>
<p>φV (ei1 &and; &middot; &middot; &middot; &and; eiN )=���(ei1 , . . . , eiN ) &middot; e�. (27.29)
</p>
<p>e� is called the canonical element in CV with respect to the determinant
function ���.</p>
<p/>
</div>
<div class="page"><p/>
<p>27.2 General Properties of the Clifford Algebra 839
</p>
<p>Now choose an orthogonal basis {ei}Ni=1 in V for which���(ei1 , . . . , eiN )=
1. Then, (27.28) and (27.29) yield
</p>
<p>e� = e1 &or; &middot; &middot; &middot; &or; eN . (27.30)
</p>
<p>Next use the Lagrange identity (26.32) and write it in the form
</p>
<p>det
(
〈xi,yj 〉
</p>
<p>)
= λ����(x1, . . . ,xN )���(y1, . . . ,yN ) xi,yj &isin; V. (27.31)
</p>
<p>Setting xi = yi = ei , and evaluating the determinant on the left-hand side of
the equation, we obtain
</p>
<p>λ� = 〈e1, e1〉 . . . 〈eN , eN 〉 = g11 . . . gNN
= (e1 &or; e1) . . . (eN &or; eN )= e21 . . . e2N , (27.32)
</p>
<p>where we used Eq. (27.15). Using (27.32), together with (27.15) and
(27.30), one can easily show that
</p>
<p>e2� &equiv; e� &or; e� = (&minus;1)N(N&minus;1)/2λ� &middot; 1. (27.33)
</p>
<p>Since λ� �= 0, it follows that e� is invertible.
Equation (27.30) can be used to show that
</p>
<p>ei &or; e� = (&minus;1)N&minus;1e� &or; ei,
</p>
<p>and since any vector in V is a linear combination of the basis {ei}Ni=1, the
equation holds for arbitrary vectors. We thus have the following:
</p>
<p>Theorem 27.2.8 The canonical element e� satisfies the relations
</p>
<p>e� &or; v = (&minus;1)N&minus;1v &or; e�, v &isin; V,
</p>
<p>e� &or; u= ωN&minus;1V (u)&or; e�, u &isin; CV ,
</p>
<p>where ωV is the degree involution of Definition 27.2.3. In particular,
e� &or; u= u&or; e� if N is odd, and e� &or; u= ωV (u)&or; e� if N is even.
</p>
<p>27.2.3 Center and Anticenter
</p>
<p>Definition 27.2.9 The center of the Clifford algebra CV , denoted by ZV ,
consists of elements a &isin; CV satisfying
</p>
<p>a&or; u= u&or; a &forall;u &isin; CV .
</p>
<p>The anticenter of the Clifford algebra CV , denoted by ZV , consists of ele-
ments a &isin; CV satisfying
</p>
<p>a&or; u= ωV (u)&or; a &forall;u &isin; CV .</p>
<p/>
</div>
<div class="page"><p/>
<p>840 27 Clifford Algebras
</p>
<p>Since CV is generated by V (i.e., it is sums of products of elements in V),
it follows that
</p>
<p>a &isin; ZV if and only if a&or; x = x &or; a &forall;x &isin; V,
</p>
<p>and
</p>
<p>a &isin; ZV if and only if a&or; x =&minus;x &or; a &forall;x &isin; V.
It is easy to show that ZV is a subalgebra of CV and that both ZV and ZV
</p>
<p>are invariant under the degree involution ωV . Therefore, as in Eq. (27.17)
</p>
<p>ZV = Z0V &oplus;V Z1V
ZV = Z0V &oplus;V Z
</p>
<p>1
V
</p>
<p>(27.34)
</p>
<p>where &oplus;V indicates a direct sum of vector spaces.
</p>
<p>Proposition 27.2.10 Z
1
V = 0. That is, the anticenter of any Clifford algebra
</p>
<p>consists of even elements only.
</p>
<p>Proof If a &isin; Z1V , then a&or; x =&minus;x &or; a for any x &isin; V. Equation (27.30) then
gives
</p>
<p>a&or; e� = (&minus;1)Ne� &or; a.
</p>
<p>On the other hand, Theorem 27.2.8 and ωV (a)=&minus;a for a &isin; Z
1
V = C1V &cap;ZV
</p>
<p>yields
</p>
<p>e� &or; a= ωN&minus;1V (a)&or; e� = (&minus;1)N&minus;1a&or; e�.
The last two equations, therefore, give a&or;e� = 0, and since e� is invertible,
we have a= 0. �
</p>
<p>Proposition 27.2.11 If V is odd-dimensional, then e� &isin; ZV , and if it
is even-dimensional, then e� &isin; ZV .
</p>
<p>Proof The proof follows immediately from Theorem 27.2.8. �
</p>
<p>Consider the linear map φV : CV &rarr; CV given by
</p>
<p>φV (u)= e� &or; u, u &isin; CV
</p>
<p>and note that since e� is invertible, φV is a linear isomorphism. If N is
odd, then Eq. (27.30) shows that φV : C0V &rarr; C1V and φV establishes an iso-
morphism of C0V and C
</p>
<p>1
V . Now restrict the map to ZV . Then, using Theo-
</p>
<p>rem 27.2.8, for x &isin; V, we obtain
</p>
<p>φV (u)&or; x = e� &or; u&or; x = e� &or; x &or; u
</p>
<p>= (&minus;1)N&minus;1x &or; e� &or; u= (&minus;1)N&minus;1x &or; φV (u).</p>
<p/>
</div>
<div class="page"><p/>
<p>27.2 General Properties of the Clifford Algebra 841
</p>
<p>Similarly,
</p>
<p>φV (v)&or; x = (&minus;1)Nx &or; φV (v) v &isin; ZV , x &isin; V.
</p>
<p>We have just proved
</p>
<p>Proposition 27.2.12 IfN is odd, then φV restricts to linear automorphisms
of ZV and ZV and establishes an isomorphism between Z0V and Z
</p>
<p>1
V . If N is
</p>
<p>even, then φV interchanges ZV and ZV .
</p>
<p>Proposition 27.2.13 Z0V = Span{1}.
</p>
<p>Proof We use induction on the dimension of V. For N = 1, the proposition
is trivial. Assume that it holds for N&minus;1, and choose v &isin; V such that 〈v,v〉 �=
0. With U the orthogonal complement of v, we can write
</p>
<p>V= Span{v} &oplus;U
</p>
<p>and note that (27.20) and (27.21) become
</p>
<p>C
0
V
&sim;=
(
1 &otimes;̂C0U
</p>
<p>)
&oplus;
(
v &otimes;̂C1U
</p>
<p>)
</p>
<p>V&sim;= (1&otimes;U)&oplus; (v &otimes; 1).
</p>
<p>Identifying the left and right hand sides of these equations, we write
</p>
<p>u= 1&otimes; b+ v &otimes; c, u &isin; C0V , b &isin; C0U , c &isin; C1U
x = 1&otimes; y + v &otimes; 1, x &isin; V, y &isin;U.
</p>
<p>We now use the multiplication rule of Eq. (26.24), noting that 1 and b have
even degrees while v, y, and c have odd degrees:
</p>
<p>u&or; x = (1 &otimes; b+ v &otimes; c)⊙ (1&otimes; y + v &otimes; 1)
= (1 &otimes; b)⊙ (1&otimes; y)+ (1 &otimes; b)⊙ (v &otimes; 1)
+ (v &otimes; c)⊙ (1&otimes; y)+ (v &otimes; c)⊙ (v &otimes; 1)
</p>
<p>= 1&otimes; (b&or; y)+ v &otimes; b+ v &otimes; (c&or; y)&minus; (v &or; v)&otimes; c.
</p>
<p>Similarly,
</p>
<p>x &or; u= 1&otimes; (y &or; b)+ v &otimes; b&minus; v &otimes; (y &or; c)+ (v &or; v)&otimes; c.
</p>
<p>Now assume that u is in the center of the Clifford algebra. Then the two
equations above are equal, and noting that v &or; v = 〈v,v〉1, we obtain
</p>
<p>1&otimes; (b&or; y &minus; y &or; b&minus; 2〈v,v〉c)+ v &otimes; (c&or; y + y &or; c)= 0,</p>
<p/>
</div>
<div class="page"><p/>
<p>842 27 Clifford Algebras
</p>
<p>or
</p>
<p>b&or; y &minus; y &or; b&minus; 2〈v,v〉c= 0
c&or; y + y &or; c= 0.
</p>
<p>(27.35)
</p>
<p>The second equation implies that c &isin; ZU and hence c &isin; Z1U . Then by Propo-
sition 27.2.10, c= 0. The first relation in (27.35) now implies that b &isin; ZU
and therefore b &isin; Z0U . By induction assumption, b is a multiple of the iden-
tity in CU . Thus,
</p>
<p>u= 1&otimes; (α1U )+ v &otimes; 0= α1,
i.e., u &isin; Span{1}. �
</p>
<p>Theorem 27.2.14 Let V be an N -dimensional space. Then
</p>
<p>(a) If N is odd, ZV = Span{1,e�}, ZV = 0.
(b) If N is even, ZV = Span{1}, ZV = Span{e�}. Thus all Clifford
</p>
<p>algebras over an even-dimensional vector space are central.
</p>
<p>Proof Suppose that N is odd and a &isin; ZV . Then, a &or; x = &minus;x &or; a, for any
x &isin; V. Equation (27.30) then yields a&or; e� =&minus;e� &or; a. On the other hand,
the second equation of Theorem 27.2.8 implies that a&or;e� = e�&or;a. Hence,
a&or;e� = 0, and since e� is invertible, we have a= 0. This proves the second
part of (a).
</p>
<p>Next observe that by Proposition 27.2.13 Z0V = Span{1} and that, since
N is assumed odd, by Proposition 27.2.12, φV is an automorphism of
ZV and an isomorphism between Z0V and Z
</p>
<p>1
V . Since Z
</p>
<p>0
V = Span{1} and
</p>
<p>φV (1)= e�, we must have Z1V = Span{e�}.
Now consider the case when N is even. For a &isin; ZV , we have a &or; e� =
</p>
<p>e� &or; a. On the other hand, by Theorem 27.2.8, a &or; e� = ωV (a) &or; e�.
Since e� is invertible, we have ωV (a) = a or (ωV &minus; ι)a = 0. Therefore,
by Eq. (27.18), a &isin; Z0V and hence ZV = Z0V . Proposition 27.2.13 now gives
ZV = Span{1}.
</p>
<p>Since φV interchanges ZV and ZV , we have
</p>
<p>ZV = φV (ZV )= φV
(
Span{1}
</p>
<p>)
= Span
</p>
<p>{
φV (1)
</p>
<p>}
= Span{e�}.
</p>
<p>This completes the proof. �
</p>
<p>27.2.4 Isomorphisms
</p>
<p>Let (V,g) be an inner product space. If we change the sign of the inner
product, we get another inner product space Ṽ &equiv; (V,&minus;g). Next consider
two vector spaces V and U, and suppose that ��� can be chosen in such a</p>
<p/>
</div>
<div class="page"><p/>
<p>27.3 General Classification of Clifford Algebras 843
</p>
<p>way that the canonical element of CV satisfies e2� =&plusmn;1. Then we have the
following two theorems whose proof can be found in [Greu 78, pp. 244&ndash;
245]:
</p>
<p>Theorem 27.2.15 Let dimV= 2m, and assume that��� can be chosen such
that λ� = (&minus;1)m. Then the Clifford algebras CV and CṼ are isomorphic.
</p>
<p>Theorem 27.2.16 Let V be an even-dimensional inner product space
and U any other inner product space. Then
</p>
<p>CV&oplus;U &sim;= CV &otimes; CU if e2� = 1
</p>
<p>CV&oplus;Ũ &sim;= CV &otimes; CU if e2� =&minus;1.
</p>
<p>Another theorem, which will be useful in the classification of Clifford
algebras and whose proof is given in [Greu 78, p. 248], is the following.
</p>
<p>Theorem 27.2.17 Let V be an even-dimensional inner product space. As-
sume that V has an antisymmetric involution ω (so that ωt =&minus;ω). Then the
Clifford algebra CV is isomorphic to L(Λ(V1)), the set of linear transfor-
mation of Λ(V1) where V1 = ker(ω&minus; ι).
</p>
<p>Recall from Theorem 26.3.6 that dimΛ(V1) = 2dimV1 , and that all real
vector spaces of dimension N are isomorphic to RN . Therefore, we can
identify L(Λ(V1)) with L(R2
</p>
<p>dimV1
), and obtain the algebra isomorphism
</p>
<p>CV
&sim;=L
</p>
<p>(
R2
</p>
<p>dimV1 ) (27.36)
</p>
<p>for V of the theorem above.
</p>
<p>27.3 General Classification of Clifford Algebras
</p>
<p>In almost all our preceding discussion, we have assumed that our scalars
come from R. There is a good reason for that: the complex Clifford algebras
are very limited and applications in physics almost exclusively focus on real
Clifford algebras. In this subsection, we include complex numbers as our
scalars and classify all complex Clifford algebras.
</p>
<p>Definition 27.3.1 Let F denote either C or R and let V be a vector space
over F. Choose a basis {ei}ni=1 for V and let v =
</p>
<p>&sum;n
i=1 ηiei be a vector in V. quadratic form
</p>
<p>A quadratic form of index ν on V is a map Qν : V&rarr; F given by
</p>
<p>Qν(v)=&minus;
ν&sum;
</p>
<p>i=1
η2i +
</p>
<p>n&sum;
</p>
<p>i=ν+1
η2i . (27.37)</p>
<p/>
</div>
<div class="page"><p/>
<p>844 27 Clifford Algebras
</p>
<p>A quadratic form yields an inner product.2 In fact, defining
</p>
<p>2g(u,v)=Qν(u + v,u + v)&minus;Qν(u,u)&minus;Qν(v,v),
</p>
<p>it is easy to verify that g is indeed a symmetric bilinear map. Conversely,
given an inner product g of index ν, one can construct a quadratic form:
Qν(v) = g(v,v). It turns out that the basis vectors chosen to define the
quadratic from are automatically g-orthonormal.
</p>
<p>When F= C, there is only one kind of quadratic form: that with ν = 0.
This is because one can alway change ηk to iηk to turn all the negative
terms in the sum to positive terms. However, when F = R, we obtain dif-
ferent quadratic forms depending on the index ν. Thus, the real quadratic
form leads to the inner product of Rnν introduced in Eq. (26.40) and the cor-
responding Clifford algebra will be discussed in detail in the next section.
</p>
<p>Before we classify the easy case of complex Clifford algebras, let us ob-
serve some general properties of the Clifford algebra over F, which we de-
note by CV (F). First, we note that since e2i &equiv; ei &or; ei = g(ei, ei)1, eki �= 0 for
any positive integer k. Therefore, CV (F) cannot contain a radical for any V
over F. This means that CV (F) is semi-simple. Moreover, Theorem 27.2.14
implies that CV (F) is simple if V is even-dimensional.
</p>
<p>Next, we look at the case of odd-dimensional vector spaces which is
only slightly more complicated. In this case ZV = Span{1,e�} by Theo-
rem 27.2.14, and as we shall see presently, e2� plays a significant role in the
classification of CV (F) when dimV is odd. Equation (27.33) gives e2� in
terms of λ� of Eq. (27.32). If F= C, then we can choose g(ei, ei)= 1. In
fact, for any non-null vector v &isin; V, we have
</p>
<p>g(ev, ev)&equiv; g
(
</p>
<p>v&radic;
g(v,v)
</p>
<p>,
v&radic;
</p>
<p>g(v,v)
</p>
<p>)
= 1.
</p>
<p>Hence, we can always set λ� = 1 when F=C. We can&rsquo;t do this for the real
case because
</p>
<p>&radic;
g(v,v) may be pure imaginary.
</p>
<p>If F = R, then because of the Lagrange identities (26.42) and (27.31),
λ� = (&minus;1)ν and the canonical element satisfies the relation
</p>
<p>e2� = (&minus;1)N(N&minus;1)/2+ν1. (27.38)
</p>
<p>Thus e2� =&plusmn;1 depending on the index ν and dimension N of V.
We discuss the case of e2� = +1 first. The elements P&plusmn; = 12 (1 &plusmn; e�)
</p>
<p>are two orthogonal idempotents belonging to the center of CV (F). Since
P+ + P&minus; = 1, we have the decomposition
</p>
<p>CV (F)= C+V (F)&oplus; C&minus;V (F)&equiv; P+CV (F)&oplus; P&minus;CV (F), (27.39)
</p>
<p>where C+V (F) and C
&minus;
V (F) are subalgebras (actually ideals) of CV (F).
</p>
<p>2Here we define an inner product simply as a symmetric bilinear map as in Defini-
tion 2.4.2.</p>
<p/>
</div>
<div class="page"><p/>
<p>27.3 General Classification of Clifford Algebras 845
</p>
<p>Since e� is the product of an odd number of vectors, ωV (e�) = &minus;e�.
Hence, ωV (P&plusmn;)= P∓. Furthermore, ωV , being an involution, has an inverse.
Thus, it is an isomorphism of C+V (F) and C
</p>
<p>&minus;
V (F).
</p>
<p>We now show that C&plusmn;V (F) are central. We do this for C
+
V (F), with the other
</p>
<p>case following immediately from the proof of C+V (F). Let a+ &isin; Z+V (F), the
center of C+V (F), and b &isin; CV (F). Then
</p>
<p>a+b= a+(b+ + b&minus;)= a+b+ + a+b&minus; = a+b+ = b+a+ = ba+
because 0= a+b&minus; = b&minus;a+. It follows that a+ &isin; ZV (F). Therefore,
</p>
<p>a+ = α1+ βe� = α(P+ + P&minus;)+ β(P+ &minus; P&minus;)= (α+ β)P+ + (α&minus; β)P&minus;.
</p>
<p>Since a+ has no component in P&minus;, we must have α = β and a+ = 2αP+.
Hence, a+ &isin; Span{P+}. But P+ is the identity of C+V (F). It now follows that
C
+
V (F) is central simple. Similarly, C
</p>
<p>&minus;
V (F) is central simple. We summarize
</p>
<p>the foregoing discussion as follows:
</p>
<p>Theorem 27.3.2 Let F be either C or R and V a vector space
over F.
</p>
<p>(a) If dimV is even, then CV (F) is central simple.
(b) If dimV is odd, then CV (C) is the direct sum of two isomorphic
</p>
<p>central simple Clifford algebras. CV (R) is the direct sum of two
isomorphic central simple Clifford algebras if e2� =+1.
</p>
<p>We are now ready to classify all complex Clifford algebras. All we have
to do is to use Theorem 3.5.29:
</p>
<p>Theorem 27.3.3 A complex Clifford algebra CV (C) is isomorphic to either
a total complex matrix algebra or a direct sum of two such algebras:
</p>
<p>(a) CV (C)&sim;=Mr(C) for some positive integer r , if dimV is even.
(b) CV (C)&sim;=Ms(C)&oplus;Ms(C) for some positive integer s, if dimV is odd.
</p>
<p>Although the real Clifford algebras are classified in the next section in
much more detail, it is instructive to give a classification of CV (R) based
on what we know from our study of algebras in general. If V is even-
dimensional, CV (R) is central simple, and by Theorem 3.5.30, it is of the
form D&otimes;Mn where D is R or H.
</p>
<p>If V is odd-dimensional, then we have to consider two cases. If e2� =+1,
then CV (R) is the direct sum of two central algebras and thus isomorphic to
</p>
<p>R&otimes;Mr &sim;=Mr(R) or to H&otimes;Ms &sim;=Ms(H)&sim;=H&otimes;Ms(R),
</p>
<p>for some nonnegative integers r and s. If e2� = &minus;1, then the center of
CV (R), which is Span{1,e�}, is isomorphic to C, and again by Theo-
rem 3.5.30, CV (R) is isomorphic to
</p>
<p>C&otimes;Mp &sim;=Mp(C) or to C&otimes;H&otimes;Mq &sim;=H&otimes;Mq(C),</p>
<p/>
</div>
<div class="page"><p/>
<p>846 27 Clifford Algebras
</p>
<p>for some nonnegative integers p and q .
We summarize this discussion in
</p>
<p>Theorem 27.3.4 A real Clifford algebra CV (R) is classified as follows:
</p>
<p>(a) If V is even-dimensional, then CV (R)&sim;=D&otimes;Mr &sim;=Mr(D), where r
is a positive integer and D = R or H, i.e., CV (R) is a total matrix
algebra over reals or quaternions.
</p>
<p>(b) If V is odd-dimensional, then we have to consider two cases:
1. If e2� =&minus;1, then CV (R)&sim;=Ms(D), where s is a positive integer
</p>
<p>and D is either C or C&otimes;H.
2. If e2� = 1, then CV (R)&sim;=Mp(D)&oplus;Mp(D), where p is a positive
</p>
<p>integer and D is either R or H.
</p>
<p>27.4 The Clifford Algebras Cνμ(R)
</p>
<p>Our discussion of inner products in Sect. 26.5 showed that orthonormal
bases are especially convenient. In such bases, the metric matrix gij &equiv; ηij
is diagonal, with ηii = &plusmn;1. In fact, if ν is the index of V (see Theo-
rem 26.5.21), then, introducing μ&equiv;N &minus; ν, we have
</p>
<p>gij = ηij =
</p>
<p>⎧
⎪⎨
⎪⎩
</p>
<p>0 if i �= j,
+1 if 1 &le; i &le; μ,
&minus;1 if μ+ 1 &le; i &le;N,
</p>
<p>(27.40)
</p>
<p>and Eq. (27.15) becomes
</p>
<p>ei &or; ej =&minus;ej &or; ei if i �= j,
ei &or; ei =+1 if 1 &le; i &le; μ,
ei &or; ei =&minus;1 if μ+ 1 &le; i &le;N.
</p>
<p>(27.41)
</p>
<p>The Clifford algebra determined by (27.41) is denoted by Cνμ(R).
3 It is theThe Clifford algebra
</p>
<p>Cνμ(R) Clifford algebra of the vector space R
n
ν introduced on page 815.
</p>
<p>Example 27.4.1 The simplest Cνμ(R) is when one of the subscripts is 0 and
the other 1. First let μ= 0 and ν = 1. In this case, V is one-dimensional. Let
e be the basis vector of V. Then a basis of the Clifford algebra C10(R) is
{1, e}, and an arbitrary element of C10(R) can be written as α1 + βe. The
multiplication of any two such elements is completely determined by the
multiplication of the basis vectors:
</p>
<p>1&or; 1= 1, 1&or; e = e &or; 1= e, e &or; e =&minus;1.
</p>
<p>3Many other notations are also used to denote this algebra. Among them are Cμ,ν(R),
C(μ, ν), Cℓμ,ν(R), Cℓp,q (R), and C(p,q) where q = ν and p = μ. Occasionally, we&rsquo;ll
use one of these notations as well.</p>
<p/>
</div>
<div class="page"><p/>
<p>27.4 The Clifford Algebras Cνμ(R) 847
</p>
<p>If we identify e with i =
&radic;
&minus;1 and &or; with ordinary multiplication, then
</p>
<p>C10(R) becomes identical with the (algebra of) complex numbers. Thus,
C10(R)
</p>
<p>&sim;=C. C10(R)&sim;=C
Now let μ = 1 and ν = 0. Again, V is one-dimensional with e as its
</p>
<p>basis vector. The basis of the Clifford algebra C01(R) is again {1, e}. The
multiplication of the basis vectors gives
</p>
<p>1&or; 1= 1, 1&or; e = e &or; 1= e, e &or; e = 1.
</p>
<p>This shows that 1 and e have identical properties, and since 1 is a basis of
R, so must be e. We conclude that C01(R)
</p>
<p>&sim;=R&oplus;R. As a direct sum of two C01(R)&sim;=R&oplus;R
algebras, R&oplus;R has the product rule,
</p>
<p>(α1 &oplus; α2)(β1 &oplus; β2)= (α1β1 &oplus; α2β2).
</p>
<p>In analogy with the ordinary complex numbers, R&oplus; R with this multipli- split complex numbers
cation rule is called split complex numbers. Problem 27.6 establishes a
concrete algebra isomorphism between C01 and R&oplus;R.
</p>
<p>Example 27.4.2 In this example, we consider a slightly more complicated
Clifford algebra than Example 27.4.1, namely C20(R). Let e1 and e2 be the
two orthonormal basis vectors of the two-dimensional vector space V on
which the Clifford algebra C20(R) is defined. This algebra is 4-dimensional
with a basis {1, e1, e2, e1 &or; e2}. To make the multiplication of the basis vec-
tors more transparent, let&rsquo;s set e1 = a, e2 = b, e1 &or; e2 = c. Then it is clear
that
</p>
<p>a&or; a=&minus;1, a&or; b= c, a&or; c=&minus;b
b&or; a=&minus;c, b&or; b=&minus;1, b&or; c= a
c&or; a= b, c&or; b=&minus;a, c&or; c=&minus;1.
</p>
<p>(27.42)
</p>
<p>Most of these are self-evident. The less obvious ones can be easily shown
using Eq. (27.41). For example,
</p>
<p>c&or; c= e1 &or; e2 &or; e1︸ ︷︷ ︸
=&minus;e1&or;e2
</p>
<p>&or;e2 =&minus; e1 &or; e1︸ ︷︷ ︸
=&minus;1
</p>
<p>&or; e2 &or; e2︸ ︷︷ ︸
=&minus;1
</p>
<p>=&minus;1.
</p>
<p>Comparison of Eq. (27.42) with Example 3.1.16 reveals that C20(R) is the
algebra of quaternions: C20(R)
</p>
<p>&sim;=H. C20(R)&sim;=H
</p>
<p>The two examples above identified some low-dimensional Clifford al-
gebras of the type Cνμ(R) with certain familiar algebras. It is possible to
identify all Cνμ(R) with more familiar algebras as we proceed to show in the
following. We first need to establish some isomorphisms among the algebras
Cνμ(R) themselves.
</p>
<p>Set N = 2 in Eq. (27.38) to get e2� = (&minus;1)1+ν1. In particular, for ν = 0
and ν = 2, we get e2� = &minus;1 for both C02(R) and C20(R). Now in The-
orem 27.2.16, let U = Rnn&minus;ν and V = R22 (recall that V has to be even-
dimensional) and note that Ũ = Rnν . The second identity of that theorem</p>
<p/>
</div>
<div class="page"><p/>
<p>848 27 Clifford Algebras
</p>
<p>then gives
</p>
<p>C
R
</p>
<p>2
2&oplus;Rnν
</p>
<p>&sim;= CR22 &otimes;CRnn&minus;ν &equiv; C
2
0(R)&otimes; Cμν (R). (27.43)
</p>
<p>Note the position of μ and ν in the last term! Also note that since R22 &oplus;
Rnν =Rnν &oplus;R22, we must have C20(R)&otimes;C
</p>
<p>μ
ν (R)= Cμν (R)&otimes;C20(R). But R22 &oplus;
</p>
<p>Rnν =Rn+2ν+2 by Proposition 26.5.23. Hence, the left-hand side of the equation
above is simply Cν+2μ (R). By choosing U=R20, and going through the same
procedure we obtain a similar result. The following theorem, in which we
have restored μ and ν to their normal position on the right-hand side of
(27.43) (now written on the left in the following theorem), summarizes these
results.
</p>
<p>Theorem 27.4.3 There exist the following Clifford algebra isomor-
phisms:
</p>
<p>Cνμ(R)&otimes; C20(R)&sim;= Cμ+2ν (R)
</p>
<p>Cνμ(R)&otimes; C02(R)&sim;= C
μ
ν+2(R)
</p>
<p>Theorem 27.4.4 Suppose that μ= ν + 4k for some integer k. Then
</p>
<p>Cνμ(R)&sim;= Cμν (R)
</p>
<p>Proof We note that N = μ+ ν = 2ν + 4k = 2(ν + 2k)&equiv; 2m. Therefore, if
��� is the normed determinant function, then
</p>
<p>λ� = (&minus;1)ν = (&minus;1)m&minus;2k = (&minus;1)m.
</p>
<p>Now apply Theorem 27.2.15, noting that R̃nν =Rnμ. �
</p>
<p>For the special case of ν = 0, we obtain
</p>
<p>C0μ(R)&sim;= Cμ0 (R) if N = 4k. (27.44)
</p>
<p>The case of μ= ν is important in the classification of the Clifford alge-
bras. In this case, we can write
</p>
<p>Rnν =R2νν =R2μμ =Rμ0 &oplus;Rμμ, (27.45)
</p>
<p>where the inner product is positive definite in the first subspace and negative
definite in the second. Let {êi}μi=1 and {f̂i}
</p>
<p>μ
i=1 be orthonormal bases of R
</p>
<p>μ
0
</p>
<p>and Rμμ, respectively, so that
</p>
<p>〈êi, êj 〉 = δij , 〈f̂i, f̂j 〉 = &minus;δij , i, j = 1,2, . . . ,μ.
</p>
<p>Now define an involution ω on Rnν as follows:
</p>
<p>ω(êi)= f̂i, and ω(f̂i)= êi, i = 1,2, . . . ,μ. (27.46)</p>
<p/>
</div>
<div class="page"><p/>
<p>27.4 The Clifford Algebras Cνμ(R) 849
</p>
<p>Then it can easily be shown that ωt = &minus;ω (see Problem 27.12). Hence,
by Theorem 27.2.17 and Eq. (27.36), Cμμ(R) = L(R2
</p>
<p>dimV1
), where V1 =
</p>
<p>ker(ω&minus; ι). But
</p>
<p>u =
μ&sum;
</p>
<p>i=1
(αi êi + βi f̂i) &isin; ker(ω&minus; ι) &lArr;&rArr; αi = βi
</p>
<p>as can be readily shown. This yields V1 = Span {êi + f̂i}μi=1. Therefore,
dimV1 = μ, and (27.36) gives the isomorphism
</p>
<p>Cμμ(R)&sim;=L
(
R2
</p>
<p>μ)
. (27.47)
</p>
<p>Example 27.4.5 For the simplest case of μ = 1, we have C11(R) &sim;=
L(R2) &sim;= M2&times;2. The isomorphism can be established directly by a pro-
cedure similar to Example 27.2.6.
</p>
<p>For the case of μ = 2, we have C22(R) &sim;= L(R4) &sim;=M4&times;4. Furthermore,
setting μ= 2 and ν = 0 in the second equation of Theorem 27.4.3, we obtain
</p>
<p>C22(R)&sim;= C02(R)&otimes; C02(R)&sim;=H&otimes;H
</p>
<p>where we used the result of Example 27.4.2. Hence, we have the isomor-
phisms
</p>
<p>C22(R)&sim;=L
(
R4
</p>
<p>)&sim;=M4&times;4 &sim;=H&otimes;H. (27.48)
</p>
<p>Problem 27.14 gives a direct isomorphic map from H&otimes;H to L(R4).
</p>
<p>27.4.1 Classification of C0n(R) and C
n
0(R)
</p>
<p>From the structure of C0n(R) and C
n
0(R) for low values of n, we can con-
</p>
<p>struct all of these algebras by using Theorem 27.4.3. First, let us collect the
results of Examples 27.2.6, 27.4.1, and 27.4.2:
</p>
<p>C10(R)&sim;=C, C01(R)&sim;=R&oplus;R, C02(R)&sim;=L
(
R2
</p>
<p>)
, C20(R)&sim;=H.
</p>
<p>(27.49)
Next let μ= 1 and ν = 0 in the first equation of Theorem 27.4.3 to obtain
</p>
<p>C30(R)&sim;= C01(R)&otimes;C20(R)&sim;= (R&oplus;R)&otimes;H&sim;= (R&otimes;H)&oplus; (R&otimes;H)&sim;=H&oplus;H,
</p>
<p>where we used Eqs. (2.16) and (2.18). Similarly, with μ= 0 and ν = 1, the
second equation of Theorem 27.4.3 yields
</p>
<p>C03(R)&sim;= C10(R)&otimes; C02(R)&sim;=C&otimes;L
(
R2
</p>
<p>)
.
</p>
<p>Setting μ= 2, ν = 0 in the first equation of Theorem 27.4.3, we obtain
</p>
<p>C40(R)&sim;= C04(R)&sim;= C02(R)&otimes; C20(R)&sim;=L
(
R2
</p>
<p>)
&otimes;H&sim;=H&otimes;L
</p>
<p>(
R2
</p>
<p>)</p>
<p/>
</div>
<div class="page"><p/>
<p>850 27 Clifford Algebras
</p>
<p>Table 27.1 Classification of C0n(R) and C
n
0(R) for n&le; 8
</p>
<p>n C0n(R) C
n
0(R)
</p>
<p>1 R&oplus;R C
2 L(R2) H
</p>
<p>3 C&otimes;L(R2) H&oplus;H
4 H&otimes;L(R2) H&otimes;L(R2)
5 (H&otimes;L(R2))&oplus; (H&otimes;L(R2)) C&otimes;L(R2)&otimes;H
6 H&otimes;L(R4) L(R8)
7 C&otimes;H&otimes;L(R4) L(R8)&oplus;L(R8)
8 L(R16) L(R16)
</p>
<p>where the first isomorphism comes from Eq. (27.44) and the last from
Eq. (2.17). We can continue the construction of the rest of C0n(R) and C
</p>
<p>n
0(R)
</p>
<p>for n&le; 8. The results are tabulated in Table 27.1. The reader is urged to ver-
ify that the entries of the table are consistent with Theorem 27.3.4, keeping
in mind that L(Rn) can be identified as the total matrix algebra R&otimes;Mn.
</p>
<p>Let V= R80 and U= Rn0 , noting that e2� = 1 for CV = C08(R). Now use
Theorem 27.2.16 to obtain
</p>
<p>C
R
</p>
<p>8
0&oplus;Rn0
</p>
<p>&sim;= C
R
</p>
<p>8
0
&otimes; CRn0 or CRn+80
</p>
<p>&sim;= C
R
</p>
<p>8
0
&otimes; CRn0
</p>
<p>or
</p>
<p>C0n+8(R)&sim;= C0n(R)&otimes; C08(R)&sim;= C0n(R)&otimes;L
(
R16
</p>
<p>)
. (27.50)
</p>
<p>Similarly, using V=R88 and U=Rnn in Theorem 27.2.16 yields
</p>
<p>Cn+80 (R)&sim;= Cn0(R)&otimes;L
(
R16
</p>
<p>)
. (27.51)
</p>
<p>It is clear that these two equations plus Table 27.1 generate Cn0(R) and
C0n(R) for all n.
</p>
<p>4
</p>
<p>Matrices are much more convenient and intuitive to use than linear trans-
formations. It is therefore instructive to rewrite Table 27.1 in terms of ma-
trices keeping in mind that with F being C or H,
</p>
<p>L
(
Rn
</p>
<p>)&sim;=Mn(R) and F&otimes;L
(
Rn
</p>
<p>)&sim;= F&otimes;Mn(R)&sim;=Mn(F), (27.52)
</p>
<p>where Mn(F) denotes an n &times; n matrix with entries in F. The results are
given in Table 27.2.
</p>
<p>We also write the periodicity relations (27.50) and (27.51) in terms of
matrices:
</p>
<p>C0n+8(R)&sim;= C0n(R)&otimes;M16(R)
</p>
<p>Cn+80 (R)&sim;= Cn0(R)&otimes;M16(R).
(27.53)
</p>
<p>4It is worth noting that, by using V = R40 or V = R44, we could obtain formulas for
C0n+4(R) and C
</p>
<p>n+4
0 (R) analogous to (27.50) and (27.51). However, as entries 5, 6, and 7
</p>
<p>of Table 27.1 can testify, they would not be as appealing as the formulas obtained above.
This is primarily because C80(R)&sim;= C08(R)&sim;=L(R16).</p>
<p/>
</div>
<div class="page"><p/>
<p>27.4 The Clifford Algebras Cνμ(R) 851
</p>
<p>Table 27.2 Classification of C0n(R) and C
n
0(R) for n&le; 8 in terms of matrices
</p>
<p>n C0n(R) C
n
0(R)
</p>
<p>1 R&oplus;R C
2 M2(R) H
</p>
<p>3 C&otimes;M2(R)&sim;=M2(C) H&oplus;H
4 H&otimes;M2(R)&sim;=M2(H) H&otimes;M2(R)&sim;=M2(H)
5 M2(H)&oplus;M2(H) H&otimes;M2(C)&sim;=C&otimes;M2(H)
6 H&otimes;M4(R)&sim;=M4(H) M8(R)
7 H&otimes;M4(C)&sim;=C&otimes;M4(H) M8(R)&oplus;M8(R)
8 M16(R) M16(R)
</p>
<p>27.4.2 Classification of Cνμ(R)
</p>
<p>We can now complete the task of classifying all of the algebras Cνμ(R). The
case of μ= ν is given by Eq. (27.47). For the case of μ&gt; ν, let μ= ν + σ
and note that
</p>
<p>Rnν =Rμ0 &oplus;Rνν =
(
Rσ0 &oplus;Rν0
</p>
<p>)
&oplus;Rνν =Rσ0 &oplus;
</p>
<p>(
Rν0 &oplus;Rνν
</p>
<p>)
=Rσ0 &oplus;R2νν .
</p>
<p>Now let V=R2νν and note that CV = Cνν(R). Furthermore, Eq. (27.38), with
N = 2ν gives e2� = 1. Hence, with U = Rσ0 , the first equation of Theo-
rem 27.2.16 yields
</p>
<p>Cνμ(R)&sim;= Cνν(R)&otimes; C0σ (R)&sim;=L
(
R2
</p>
<p>ν )&otimes; C0σ (R), (27.54)
</p>
<p>where we used (27.47).
If μ&lt; ν, let ν = μ+ρ and note that Rnν =Rρρ &oplus;R2μμ . With V=R2μμ and
</p>
<p>U=Rρρ , the first equation of Theorem 27.2.16 yields
</p>
<p>Cνμ(R)&sim;= Cμμ(R)&otimes; Cρ0 (R)&sim;=L
(
R2
</p>
<p>μ)&otimes; Cρ0 (R). (27.55)
</p>
<p>It is worthwhile to collect these results in a theorem. Noting that C00
&sim;=R
</p>
<p>and that A&otimes;R=A for any real algebra, we can combine the three cases of
μ= ν, μ&gt; ν, and μ&lt; ν into two cases:
</p>
<p>Theorem 27.4.6 The following isomorphisms hold:
</p>
<p>Cνμ(R)&sim;=L
(
R2
</p>
<p>ν )&otimes; C0μ&minus;ν(R)&sim;=M2ν (R)&otimes; C0μ&minus;ν(R), if μ&ge; ν,
</p>
<p>Cνμ(R)&sim;=L
(
R2
</p>
<p>μ)&otimes; Cν&minus;μ0 (R)&sim;=M2μ(R)&otimes; C
ν&minus;μ
0 (R), if μ&le; ν.
</p>
<p>This theorem together with Table (27.2) and the periodicity relations
</p>
<p>Cνμ+8(R)&sim;= Cνμ(R)&otimes;M16(R)
</p>
<p>Cν+8μ (R)&sim;= Cνμ(R)&otimes;M16(R),
(27.56)</p>
<p/>
</div>
<div class="page"><p/>
<p>852 27 Clifford Algebras
</p>
<p>which come from Eq. (27.53), determine all the algebras Cνμ(R).
From Theorem 27.4.6, the periodicity relation (27.53), and Table 27.2,
</p>
<p>we get the following:
</p>
<p>Theorem 27.4.7 All Clifford algebras Cνμ(R) with μ&minus; ν �= 1 mod 4 are
simple. Those with μ&minus;ν = 1 mod 4 are direct sums of two identical simple
algebras.
</p>
<p>27.4.3 The Algebra C13(R)
</p>
<p>For the Minkowski n-space, Rn1 , Theorem 27.4.6 gives
</p>
<p>C1n&minus;1(R)&sim;=M2(R)&otimes; C0n&minus;2(R).
</p>
<p>When n= 4, this reduces to
</p>
<p>C13(R)&sim;=M2(R)&otimes; C02(R)&sim;=M2(R)&otimes;M2(R)&sim;=M4(R).
</p>
<p>In the language of Chap. 3, C13(R) is a total matrix algebra, which, by either
Theorem 3.5.27 (and the remarks after it) or Theorem 3.3.2, is simple. We
now find a basis {eij } of this algebra.
</p>
<p>First we find the diagonals {eii}4i=1, which are obviously primitive, or-
thogonal to each other, and
</p>
<p>1= e11 + e22 + e33 + e44.
</p>
<p>Thus, by Theorem 3.5.32, the identity has rank 4. Next, we construct four
primitive orthogonal idempotents {Pi}4i=1 out of the basis vectors5 {êη}3η=0
of C13(R) and their products and identify them with {eii}4i=1. The easiest
way to construct these idempotents is to find x and y such that
</p>
<p>x2 = 1= y2, xy= yx.
</p>
<p>Then the product of 12 (1&plusmn; x) and 12 (1&plusmn; y) for all sign choices yields four
primitive orthogonal idempotents, as the reader may verify. There are many
choices for x and y. We choose x= ê1 and y= ê02, where we use the com-
mon abbreviation êη1...ηp &equiv; êη1 &or; &middot; &middot; &middot; &or; êηp , and set
</p>
<p>P1 =
1
</p>
<p>4
(1+ ê1)(1+ ê02)&equiv; e11,
</p>
<p>P2 =
1
</p>
<p>4
(1+ ê1)(1&minus; ê02)&equiv; e22,
</p>
<p>P3 =
1
</p>
<p>4
(1&minus; ê1)(1+ ê02)&equiv; e33,
</p>
<p>P4 =
1
</p>
<p>4
(1&minus; ê1)(1&minus; ê02)&equiv; e44.
</p>
<p>(27.57)
</p>
<p>5Here we are using the physicists&rsquo; convention of numbering the basis vectors from 0 to 3
with ê0 = ê4 and using Greek letters for indices.</p>
<p/>
</div>
<div class="page"><p/>
<p>27.4 The Clifford Algebras Cνμ(R) 853
</p>
<p>Since the Pis are all primitive (thus, of rank 1), by Theorem 3.5.32, they are
similar. Indeed, one can easily show that
</p>
<p>ê03P1ê
&minus;1
03 = ê03P1ê03 = P2,
</p>
<p>ê3P1ê
&minus;1
3 = ê3P1ê3 = P3,
</p>
<p>ê0P1ê
&minus;1
0 =&minus;ê0P1ê0 = P4.
</p>
<p>(27.58)
</p>
<p>Equations (27.57) and (27.58) determine all eij s, as we now demonstrate.
Write the first relation of (27.58) as ê03P1 = P2ê03 or ê03e11 = e22ê03. Since
ê03 &isin; C13(R), it can be written as a linear combination of {eij }. With ê03 =&sum;4
</p>
<p>i,j=1 αijeij , we have
</p>
<p>4&sum;
</p>
<p>i,j=1
αijeij e11 = e22
</p>
<p>4&sum;
</p>
<p>i,j=1
αij eij or
</p>
<p>4&sum;
</p>
<p>i=1
αi1ei1 =
</p>
<p>4&sum;
</p>
<p>j=1
α2j e2j .
</p>
<p>Linear independence of {eij } implies that αi1 = 0 for i �= 2 and α2j = 0 for
j �= 1. Therefore, the left-hand side (or the right-hand side) of the equation
reduces to α21e21. Hence, we have
</p>
<p>ê03P1 = α21e21. (27.59)
</p>
<p>We can also write the first relation of (27.58) as P1ê03 = ê03P2 or e11ê03 =
ê03e22, which yields
</p>
<p>e11
4&sum;
</p>
<p>i,j=1
αijeij =
</p>
<p>4&sum;
</p>
<p>i,j=1
αij eije22 or
</p>
<p>4&sum;
</p>
<p>j=1
α1j e1j =
</p>
<p>4&sum;
</p>
<p>i=1
αi2ei2.
</p>
<p>Again, linear independence of {eij } implies that α1j = 0 for j �= 2 and αi2 =
0 for i �= 1. Therefore, the left-hand side (or the right-hand side) of the
equation reduces to α12e12, and we get
</p>
<p>P1ê03 = α12e12. (27.60)
</p>
<p>Multiply Eqs. (27.59) and (27.60) to get
</p>
<p>(ê03P1)(P1ê03)= (α21e21)(α12e12)
</p>
<p>or
</p>
<p>ê03P1ê03 = α21α12e21e12 = α21α12e22 = α21α12P2.
Comparing this with the first equation in (27.58), we conclude that
α21α12 = 1, which is also a consistency condition for ê03 &or; ê03 = 1. There
are several choices for αij , all of which satisfy this as well as other condi-
tions obtained above. Therefore, we are at liberty to set α21 = 1 = α12 and
write
</p>
<p>ê03P1 = e21, P1ê03 = ê03P2 = e12. (27.61)</p>
<p/>
</div>
<div class="page"><p/>
<p>854 27 Clifford Algebras
</p>
<p>Table 27.3 The basis eij for the total matrix algebra C13(R)
</p>
<p>eij j = 1 j = 2 j = 3 j = 4
i = 1 P1 ê03P2 ê3P3 &minus;ê0P4
i = 2 ê03P1 P2 ê0P3 &minus;ê3P4
i = 3 ê3P1 &minus;ê0P2 P3 ê03P4
i = 4 ê0P1 &minus;ê3P2 ê03P3 P4
</p>
<p>Going through the same procedure using the second and third relations of
(27.58), we obtain
</p>
<p>ê3P1 = e31, P1ê3 = ê3P3 = e13,
ê0P1 = e41, P1ê0 = ê0P4 =&minus;e14.
</p>
<p>(27.62)
</p>
<p>Having found ei1 and e1j , we can find all the eij because eij = ei1e1j . The
result is summarized in Table 27.3.
</p>
<p>Now that we have the basis we were after, we can express the basis vec-
tors {êη}3η=0 of the underlying vector space in terms of the new basis. Writ-
ing
</p>
<p>êη =
4&sum;
</p>
<p>i,j=1
γη,ij eij ,
</p>
<p>multiplying it on the right by ekl and on the left by emn, we obtain
</p>
<p>emnêηekl = γη,nkeml,
</p>
<p>which, for m= 1 = l yields
</p>
<p>e1nêηek1 = γη,nke11. (27.63)
</p>
<p>Thus, to find γη,nk , multiply êη on the left by e1n and on the right by ek1
and read the coefficient of e11 in the expression obtained. As an example,
we find γ3,12. We have
</p>
<p>e11ê3e21 = γ3,12e11.
</p>
<p>The left-hand side can be evaluated from the table:
</p>
<p>e11ê3e21 = e11ê3ê03P1 = e11ê3 &or; ê0 &or; ê3P1
=&minus;e11ê3 &or; ê3 &or; ê0P1
=&minus;e11ê0P1 =&minus;e11e41 = 0.
</p>
<p>Thus, γ3,12 = 0. Similarly, we find γ3,13:
</p>
<p>e11ê3e31 = γ3,13e11.
</p>
<p>Using Table 27.3, we get
</p>
<p>e11ê3e31 = e11ê3ê3P1 = e11ê3 &or; ê3P1 = e111e11 = e11.</p>
<p/>
</div>
<div class="page"><p/>
<p>27.4 The Clifford Algebras Cνμ(R) 855
</p>
<p>Thus, γ3,13 = 1.
We can continue this way and obtain all coefficients γη,ij . However, an
</p>
<p>easier way is to solve for êη from Eq. (27.57). Thus
</p>
<p>ê1 = P1 + P2 &minus; P3 &minus; P4 = e11 + e22 &minus; e33 &minus; e44,
</p>
<p>giving the matrix
</p>
<p>γ1 =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>1 0 0 0
0 1 0 0
0 0 &minus;1 0
0 0 0 &minus;1
</p>
<p>⎞
⎟⎟⎠ .
</p>
<p>Similarly,
</p>
<p>ê02 = P1 + P3 &minus; P2 &minus; P4, (27.64)
</p>
<p>from which we can get ê2 by multiplying on the left by ê0 and noting that
</p>
<p>ê0ê02 = ê0ê0ê2 =&minus;ê2.
</p>
<p>Thus,
</p>
<p>ê2 =&minus;ê0P1 &minus; ê0P3 + ê0P2 + ê0P4 =&minus;e41 &minus; e23 &minus; e32 &minus; e14 (27.65)
</p>
<p>where use was made of Table 27.3 in the last step. It follows that
</p>
<p>γ2 =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>0 0 0 &minus;1
0 0 &minus;1 0
0 &minus;1 0 0
&minus;1 0 0 0
</p>
<p>⎞
⎟⎟⎠ .
</p>
<p>The remaining two matrices can be obtained similarly. The details are left
as Problem 27.22. The result is
</p>
<p>γ3 =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>0 0 1 0
0 0 0 &minus;1
1 0 0 0
0 &minus;1 0 0
</p>
<p>⎞
⎟⎟⎠ , γ0 =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>0 0 0 &minus;1
0 0 1 0
0 &minus;1 0 0
1 0 0 0
</p>
<p>⎞
⎟⎟⎠ .
</p>
<p>Since êμ &or; êν + êν &or; êμ = 2ημν by (27.15) and (27.40), we have
</p>
<p>γμγν + γνγμ = 2ημν1, (27.66)
</p>
<p>which can also be verified directly by matrix multiplication. Equation (27.66)
is identical to (27.11) obeyed by the Dirac gamma matrices. The matri-
ces that Dirac used in his equation had complex entries. The matrices con-
structed above are all real. They are called the Majorana representation of Majorana representation
the Dirac matrices.</p>
<p/>
</div>
<div class="page"><p/>
<p>856 27 Clifford Algebras
</p>
<p>27.5 Problems
</p>
<p>27.1 Starting with Eq. (27.8), write eip&minus;1 &or;eip in terms of the wedge product
using Eq. (27.3). Then use the more general Clifford product (27.2) repeat-
edly until you have turned all the &or;&rsquo;s to &and;&rsquo;s.
</p>
<p>27.2 Find the coefficients of 1, e1, e2, and e1 &or; e2 for the Clifford product
u&or; v of Example 27.2.2.
</p>
<p>27.3 Show that Eqs. (27.24) and (27.25) are equivalent.
</p>
<p>27.4 Show that because of (27.24), ϕ can be extended to an algebra homo-
morphism only if it is an injective linear map.
</p>
<p>27.5 Show that the conjugation involution of Definition 27.2.5 coincides
with the usual complex and quaternion conjugation. Show that a&or; b =
b̄&or; ā.
</p>
<p>27.6 Let ϕ :R&rarr;R&oplus;R be a linear map. Assume a completely general form
for ϕ, i.e., assume ϕ(α) = (β &oplus; γ ). Extend this linear map to a homomor-
phism φ : C01 &rarr;R&oplus;R. Imposing the consistency condition (27.25), deduce
that β2 = α2 = γ 2. Now show that a non-trivial homomorphism sends 1 to
1&oplus;1 and e to 1&oplus;&minus;1, and therefore is an isomorphism. Finally for a general
element of C01, show that
</p>
<p>φ(α1 + βe)= (α + β,α &minus; β), α,β &isin;R.
</p>
<p>27.7 Show that the four matrices
(
</p>
<p>1 0
0 1
</p>
<p>)
,
</p>
<p>(
1 0
0 &minus;1
</p>
<p>)
,
</p>
<p>(
0 1
1 0
</p>
<p>)
,
</p>
<p>(
0 1
&minus;1 0
</p>
<p>)
</p>
<p>are linearly independent.
</p>
<p>27.8 Derive Eq. (27.33).
</p>
<p>27.9 Show that the center ZV is a subalgebra of CV .
</p>
<p>27.10 Show that both ZV and ZV are invariant under the degree involution
ωV .
</p>
<p>27.11 Let Qν : V &rarr; F be a quadratic form defined in terms of the basis
{ei}ni=1 and g the inner product derived from Qν . Show that g(ei, ej ) =
&plusmn;δij .
</p>
<p>27.12 Let u,v &isin; R2μμ of Eq. (27.45). Write u and v in terms of the ba-
sis vectors {êi}μi=1 and {f̂i}
</p>
<p>μ
i=1 and show that the ω of Eq. (27.46) satisfies
</p>
<p>〈u,ωv〉 = 〈&minus;ωu,v〉, implying that ωt =&minus;ω. With u =&sum;μi=1(αi êi + βi f̂i),
show that u &isin; ker(ω&minus; ι) iff αi = βi .</p>
<p/>
</div>
<div class="page"><p/>
<p>27.5 Problems 857
</p>
<p>27.13 Following Example 27.2.6, show directly that C11(R)
&sim;= L(R2) &sim;=
</p>
<p>M2&times;2.
</p>
<p>27.14 Let x = (x1, x2, x3, x4) &isin;L(R4). Define φ :H&otimes;H&rarr;L(R4) by
</p>
<p>φ(p&otimes; q)x = p &middot; x &middot; q&lowast;, p,q &isin;H, x &isin;L
(
R4
</p>
<p>)
</p>
<p>where on the right-hand side, x = x1 + x2i + x3j + x4k is a quaternion.
Show that φ is an algebra homomorphism, whose kernel is zero. Now invoke
the dimension theorem and the fact that H&otimes;H and L(R4) have the same
dimension to show that φ is an isomorphism.
</p>
<p>27.15 Let V=R40 or V= R44 and note that e2� = 1 for CV . Now use Theo-
rem 27.2.17 to show that
</p>
<p>C0 or 88 or 0(R)&sim;= C0 or 44 or 0(R)&otimes; C0 or 44 or 0(R).
</p>
<p>27.16 Complete the remainder of Table 27.1.
</p>
<p>27.17 Using V=R40 or V=R44, derive formulas for C0n+4(R) and Cn+40 (R)
analogous to (27.50) and (27.51).
</p>
<p>27.18 Show that
</p>
<p>C90(R)&sim;=C&otimes;L
(
R16
</p>
<p>)
, C09(R)&sim;=L
</p>
<p>(
R16
</p>
<p>)
&oplus;L
</p>
<p>(
R16
</p>
<p>)
,
</p>
<p>C100 (R)&sim;=H&otimes;L
(
R16
</p>
<p>)
, C010(R)&sim;=L
</p>
<p>(
R32
</p>
<p>)
.
</p>
<p>27.19 Show that if x2 = 1= y2 and xy= yx, then the four quantities 14 (1&plusmn;
x)(1&plusmn; y) are orthogonal idempotents.
</p>
<p>27.20 Verify all of the relations in Eq. (27.58).
</p>
<p>27.21 Derive Eq. (27.62).
</p>
<p>27.22 Note that
</p>
<p>ê0 = ê01= ê0P1 + ê0P2 + ê0P3 + ê0P4
ê3 = ê31= ê3P1 + ê3P2 + ê3P3 + ê3P4.
</p>
<p>Now use Table 27.3 to express each term on the right in terms of eij .</p>
<p/>
</div>
<div class="page"><p/>
<p>28Analysis of Tensors
</p>
<p>Tensor algebra deals with lifeless vectors and tensors&mdash;objects that do not
move, do not change, possess no dynamics. Whenever there is a need for
tensors in physics, there is also a need to know the way these tensors change
with position and time. Tensors that depend on position and time are called
tensor fields and are the subject of this chapter.
</p>
<p>In studying the algebra of tensors, we learned that they are generaliza-
tions of vectors. Once we have a vector space V and its dual space V&lowast;, we
can take the tensor products of factors of V and V&lowast; and create tensors of var-
ious kinds. Thus, once we know what a vector is, we can make up tensors
from it.
</p>
<p>In our discussion of tensor algebra, we did not concern ourselves with
what a vector was; we simply assumed that it existed. Because all the vectors
considered there were stationary, their mere existence was enough. How-
ever, in tensor analysis, where things keep changing from point to point
(and over time), the existence of vectors at one point does not guarantee
their existence at all points. Therefore, we now have to demand more from
vectors than their mere existence. Tied to the concept of vectors is the notion
of space, or space-time. Let us consider this first.
</p>
<p>28.1 Differentiable Manifolds
</p>
<p>Space is one of the undefinables in elementary physics. Length and time
intervals are concepts that are &ldquo;God given&rdquo;, and any definitions of these
concepts will be circular. This is true as long as we are confined within a
single space. In classical physics, this space is the three-dimensional Eu-
clidean space in which every motion takes place. In special relativity, space
is changed to Minkowski space-time. In nonrelativistic quantum mechanics,
the underlying space is the (infinite-dimensional) Hilbert space, and time is
the only dynamical parameter. In the general theory of relativity, gravitation
and space-time are intertwined through the concept of curvature.
</p>
<p>Mathematicians have invented a unifying theme that brings the common
features of all spaces together. This unifying theme is the theory of differ-
entiable manifolds. A rigorous understanding of differentiable manifolds is
</p>
<p>S. Hassani, Mathematical Physics, DOI 10.1007/978-3-319-01195-0_28,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>859</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-01195-0_28">http://dx.doi.org/10.1007/978-3-319-01195-0_28</a></div>
</div>
<div class="page"><p/>
<p>860 28 Analysis of Tensors
</p>
<p>beyond the scope of this book. However, a working knowledge of mani-
fold theory is surprisingly simple. Let us begin with a crude definition of a
differentiable manifold.
</p>
<p>Definition 28.1.1 A differentiable manifold is a collection of objects
differentiable manifold
</p>
<p>provisionally defined
called points that are connected to each other in a smooth fashion such
that the neighborhood of each point looks like the neighborhood of an m-
dimensional (Cartesian) space; m is called the dimension of the manifold.
</p>
<p>As is customary in the literature, we use &ldquo;manifold&rdquo; to mean &ldquo;differen-
tiable manifold&rdquo;.
</p>
<p>Example 28.1.2 The following are examples of differentiable manifolds.
</p>
<p>(a) The space Rn is an n-dimensional manifold.
(b) The surface of a sphere is a two-dimensional manifold.
(c) A torus is a two-dimensional manifold.
(d) The collection of all n&times;n real matrices whose elements are real func-
</p>
<p>tions having derivatives of all orders is an n2-dimensional manifold.
Here a point is an n&times; n matrix.
</p>
<p>(e) The collection of all rotations in R3 is a three-dimensional manifold.
(Here a point is a rotation.)
</p>
<p>(f) Any smooth surface in R3 is a two-dimensional manifold.
(g) The unit n-sphere Sn, which is the collection of points in Rn+1 satis-
</p>
<p>fying
</p>
<p>x21 + &middot; &middot; &middot; + x2n+1 = 1,
is a manifold.
</p>
<p>Any surface with sharp kinks, edges, or points cannot be a manifold.
Thus, neither a cone nor a finite cylinder is a two-dimensional manifold.
However, an infinitely long cylinder is a manifold.
</p>
<p>Let UP denote a neighborhood of P . When we say that this neighborhood
looks like an m-dimensional Cartesian space, we mean that there exists a bi-
jective map ϕ :UP &rarr;Rm from a neighborhood UP of P to a neighborhood
ϕ(UP ) of ϕ(P ) in Rm, such that as we move the point P continuously in
UP , its image moves continuously in ϕ(UP ). Since ϕ(P ) &isin;Rm, we can de-
fine functions xi : UP &rarr; R such that ϕ(P ) = (x1(P ), x2(P ), . . . , xm(P )).
These functions are called coordinate functions of ϕ. The numbers xi(P )coordinate functions
</p>
<p>and charts are called coordinates of P . The neighborhood UP together with its map-
ping ϕ form a chart, denoted by (UP , ϕ).
</p>
<p>Now let (VP ,μ) be another chart at P with coordinate functions μ(P )=
(y1(P ), y2(P ), . . . , ym(P )) (see Fig. 28.1). It is assumed that the map μ ◦
ϕ&minus;1 : ϕ(UP &cap; Vp)&rarr; μ(UP &cap; VP ), which maps a subset of Rm to another
subset of Rm, possesses derivatives of all orders. Then, we say that the two
charts μ and ϕ are C&infin;-related. Such a relation underlies the concept ofC&infin;-related charts and
</p>
<p>atlases smoothness in the definition of a manifold. A collection of charts that cover
the manifold and of which each pair is C&infin;-related is called a C&infin; atlas.</p>
<p/>
</div>
<div class="page"><p/>
<p>28.1 Differentiable Manifolds 861
</p>
<p>Fig. 28.1 Two charts (UP , ϕ) and (VP ,μ), containing P are mapped into Rm. The func-
tion μ ◦ ϕ&minus;1 is an ordinary function from Rm to Rm
</p>
<p>Example 28.1.3 For the two-dimensional unit sphere S2 we can construct
an atlas as follows. Let P = (x, y, z) be a point in S2. Then x2+y2+z2 = 1,
or
</p>
<p>z=&plusmn;
&radic;
</p>
<p>1 &minus; x2 &minus; y2.
The plus sign corresponds to the upper hemisphere, and the minus sign to
the lower hemisphere. Let U+3 be the upper hemisphere with the equator
removed. Then a chart (U+3 , ϕ3) with ϕ3 : U+3 &rarr; R2 can be constructed
by projecting on the xy-plane: ϕ3(P ) = (x, y). Similarly, (U&minus;3 ,μ3) with
μ3 :U&minus;3 &rarr;R2 given by μ3(P )= (x, y) is a chart for the lower hemisphere.
</p>
<p>In manifold theory the neighborhoods on which mappings of charts are Construction of an atlas
for the sphere S2 .defined have no boundaries (thus the word &ldquo;open&rdquo;). This is because it is
</p>
<p>more convenient to define limits on boundaryless (open) neighborhoods.
Thus, in the above two charts the equator, which is the boundary for both
hemispheres, must be excluded. With this exclusion U+3 and U
</p>
<p>&minus;
3 cannot
</p>
<p>cover the entire S2; hence, they do not form an atlas. More charts are needed
to cover the unit two-sphere. Two such charts are the right and left hemi-
spheres U+2 and U
</p>
<p>&minus;
2 , for which y &gt; 0 and y &lt; 0, respectively. However,
</p>
<p>these two neighborhoods leave two points uncovered, the points (1,0,0)
and (&minus;1,0,0). Again this is because boundaries of the right and left hemi-
spheres must be excluded. Adding the front and back hemispheres U&plusmn;1 to
the collection covers these two points. Then S2 is completely covered and
we have an atlas. There is, of course, a lot of overlap among charts. We now
show that these overlaps are C&infin;-related.
</p>
<p>As an illustration, we consider the overlap between U+3 and U
+
2 . This is
</p>
<p>the upper-right quarter of the sphere. Let (U+3 , ϕ3) and (U
+
2 , ϕ2) be charts
</p>
<p>with
</p>
<p>ϕ3(x, y, z)= (x, y), ϕ2(x, y, z)= (x, z).
The inverses are therefore given by
</p>
<p>ϕ&minus;13 (x, y)= (x, y, z)=
(
x, y,
</p>
<p>&radic;
1 &minus; x2 &minus; y2
</p>
<p>)
,</p>
<p/>
</div>
<div class="page"><p/>
<p>862 28 Analysis of Tensors
</p>
<p>Fig. 28.2 A chart mapping points of S2 into R2. Note that the map is not defined for
θ = 0,π , and therefore at least one more chart is required to cover the whole sphere
</p>
<p>ϕ&minus;12 (x, z)= (x, y, z)=
(
x,
</p>
<p>&radic;
1 &minus; x2 &minus; z2, z
</p>
<p>)
,
</p>
<p>and
</p>
<p>ϕ2 ◦ ϕ&minus;13 (x, y)= ϕ2
(
x, y,
</p>
<p>&radic;
1 &minus; x2 &minus; y2
</p>
<p>)
=
(
x,
</p>
<p>&radic;
1 &minus; x2 &minus; y2
</p>
<p>)
.
</p>
<p>Let us denote ϕ2 ◦ ϕ&minus;13 by F , so that F : R2 &rarr; R2 is described by two
functions, the components of F :
</p>
<p>F1(x, y)= x and F2(x, y)=
&radic;
</p>
<p>1 &minus; x2 &minus; y2.
</p>
<p>The first component has derivatives of all orders at all points. The second
component has derivatives of all orders at all points except at x2 + y2 = 1,
which is excluded from the region of overlap of U+3 and U
</p>
<p>+
2 , for which z
</p>
<p>can never be zero. Thus, F has derivatives of all orders at all points of its
domain of definition.
</p>
<p>One can similarly show that all regions of overlap for all charts have this
property, i.e., all charts are C&infin;-related.
</p>
<p>Example 28.1.4 For S2 of the preceding example, we can find a new atlas
in terms of new coordinate functions. Since x21 + x22 + x23 = 1, we can use
spherical coordinates θ = cos&minus;1 x3, ϕ = tan&minus;1(x2/x1). A chart is then given
by (S2 &minus; {1} &minus; {&minus;1},μ), where μ(P ) = (θ,ϕ) maps a point of S2 onto a
region in R2. This is schematically shown in Fig. 28.2. The singletons {1}
and {&minus;1} are the north and the south poles, respectively.
</p>
<p>This chart cannot cover all of S2, however, because when θ = 0 (or π ),
the value of the azimuthal angle ϕ is not determined. In other words, θ = 0
(or π ) determines one point of the sphere (the north pole or the south pole),
but its image in R2 is the whole range of ϕ values. Therefore, we must
exclude θ = 0 (or π ) from the chart (S2,μ). To cover these two points, we
need more charts.
</p>
<p>Example 28.1.5 A third atlas for S2 is the so-called stereographic projec-illustration of
stereographic projection tion shown in Fig. 28.3. In such a mapping the image of a point is obtained
</p>
<p>by drawing a line from the north pole to that point and extending it, if nec-
essary, until it intersects the x1x2-plane. It can be verified that the mapping</p>
<p/>
</div>
<div class="page"><p/>
<p>28.1 Differentiable Manifolds 863
</p>
<p>Fig. 28.3 Stereographic projection of S2 into R2. Note that the north pole has no image
under this map; another chart is needed to cover the whole sphere
</p>
<p>ϕ : S2 &minus; {1}&rarr;R2 is given by
</p>
<p>ϕ(x1, x2, x3)=
(
</p>
<p>x1
</p>
<p>1 &minus; x3
,
</p>
<p>x2
</p>
<p>1 &minus; x3
</p>
<p>)
.
</p>
<p>We see that this mapping fails for x3 = 1, that is, the north pole. Therefore,
the north pole must be excluded (thus, the domain S2 &minus; {1}). To cover the
north pole we need another stereographic projection&mdash;this time from the
south pole. Then the two mappings will cover all of S2, and it can be shown
that the two charts are C&infin;-related (see Example 28.1.12).
</p>
<p>The three foregoing examples illustrate the following fact, which can be
shown to hold rigorously:
</p>
<p>Box 28.1.6 It is impossible to cover the whole S2 with just one chart.
</p>
<p>Example 28.1.7 Let V be an m-dimensional real vector space. Fix any vector spaces are
manifoldsbasis {ei} in V with dual basis {ǫǫǫi}. Define φ : V &rarr; Rm by φ(v) =
</p>
<p>(ǫǫǫ1(v), . . . ,ǫǫǫm(v)). Then the reader may verify that (V, φ) is an atlas. Lin-
earity of φ ensures that it has derivatives of all orders. This construction
shows that V is a manifold of dimension m.
</p>
<p>If M and N are manifolds of dimensions m and n, respectively, we can
construct their product manifold M &times;N , a manifold of dimension m+ n. product manifold
</p>
<p>definedA typical chart on M &times;N is obtained from charts on M and N as follows.
Let (U,ϕ) be a chart on M and (V ,μ) one on N . Then a chart on M &times;N is
(U &times; V,ϕ &times;μ) where
</p>
<p>ϕ &times;μ(P,Q)=
(
ϕ(P ),μ(Q)
</p>
<p>)
&isin;Rm &times;Rn =Rm+n for P &isin;U, Q &isin; V.
</p>
<p>Definition 28.1.8 Let M be a manifold. A subset N of M is called a sub- submanifold
manifold of M if N is a manifold in its own right.
</p>
<p>A trivial, but important, example of submanifolds is the so-called open open submanifolds
submanifold. If M is a manifold and U is an open subset1 of M , then U
</p>
<p>1Recall that an open subset U is one each of whose points is the center of an open ball
lying entirely in U .</p>
<p/>
</div>
<div class="page"><p/>
<p>864 28 Analysis of Tensors
</p>
<p>Fig. 28.4 Corresponding to every map f : M &rarr; N there exists a coordinate map
μ ◦ f ◦ ϕ&minus;1 :Rm &rarr;Rn
</p>
<p>inherits a manifold structure from M by taking any chart (Uα, ϕα) and re-
stricting ϕα to U &cap;Uα . It is clear that dimU = dimM .
</p>
<p>Having gained familiarity with manifolds, it is now appropriate to con-
sider maps between them that are compatible with their structure.
</p>
<p>Definition 28.1.9 Let M and N be manifolds of dimensions m and n, re-
spectively. Let f : M &rarr; N be a map. We say that f is C&infin;, or differen-differentiable maps and
</p>
<p>their coordinate
</p>
<p>expressions
</p>
<p>tiable, if for every chart (U,ϕ) in M and every chart (V ,μ) in N , the com-
posite map μ◦f ◦ϕ&minus;1 :Rm &rarr;Rn, called the coordinate expression for f ,
is C&infin; wherever it is defined.2
</p>
<p>The content of this definition is illustrated in Fig. 28.4. A particularly
important special case occurs when N = R; then we call f a (real-valued)function as a special kind
</p>
<p>of map function. The collection of all C&infin; functions at a point P &isin;M is denoted
by F&infin;(P ): If f &isin; F&infin;(P ), then f :UP &rarr;R is C&infin; for some neighborhood
UP of P .
</p>
<p>Let f :M &rarr;N be a differentiable map. Then f is automatically contin-
uous. Now let V be an open subset of N . The set f&minus;1(V ) is an open subset
of M by Proposition 17.4.6.3
</p>
<p>Proposition 28.1.10 Let M be an m-dimensional manifold, f :M &rarr; N a
differentiable map, and V an open subset of N . Then f&minus;1(V ), the set of
points of M mapped onto V , is an open m-dimensional submanifold of M .
</p>
<p>Just as the concept of isomorphism identified all vector spaces, algebras,
and groups that were equivalent to one another, it is desirable to introduce a
notion that brings together those manifolds that &ldquo;look alike&rdquo;.
</p>
<p>2The domain of μ ◦f ◦ϕ&minus;1 is not all of Rm, but only its open subset ϕ(U). However, we
shall continue to abuse the notation and write Rm instead of ϕ(U). This way, we do not
have to constantly change the domain as U changes. The domain is always clear from the
context.
3Although Proposition 17.4.6 was shown for normed linear spaces, it really holds for all
&ldquo;spaces&rdquo; for which the concept of open set is defined.</p>
<p/>
</div>
<div class="page"><p/>
<p>28.1 Differentiable Manifolds 865
</p>
<p>Definition 28.1.11 A bijective differentiable map whose inverse is also dif-
ferentiable is called a diffeomorphism. Two manifolds between which a dif- diffeomorphism and
</p>
<p>local diffeomorphism
</p>
<p>defined
</p>
<p>feomorphism exists are called diffeomorphic. Let M and N be manifolds.
M is said to be diffeomorphic to N at P &isin;M if there is a neighborhood
U of P and a diffeomorphism f : U &rarr; f (U). Then f is called a local
diffeomorphism at P .
</p>
<p>In our discussion of groups, we saw that the set of linear isomorphisms diffeomorphisms of a
manifold form a groupof a vector space V onto itself forms a group GL(V). The set of diffeomor-
</p>
<p>phisms of a manifold M onto itself also forms a group, which is denoted by
Diff(M).
</p>
<p>Example 28.1.12 The generalization of a sphere is the unit n-sphere, n-sphere and its
stereographic projectionwhich is a subset of Rn+1 defined by
</p>
<p>Sn =
{
(x1, . . . , xn+1) &isin;Rn+1 | x21 + &middot; &middot; &middot; + x2n+1 = 1
</p>
<p>}
.
</p>
<p>The stereographic projection defines an atlas for Sn as follows. For all points
of Sn except (0,0, . . . ,1), the north pole, define the chart ϕ+ : Sn &minus; {1} &equiv;
U+ &rarr;Rn by
</p>
<p>ϕ+(x1, . . . , xn+1)
</p>
<p>=
(
</p>
<p>x1
</p>
<p>1 &minus; xn+1
, . . . ,
</p>
<p>xn
</p>
<p>1 &minus; xn+1
</p>
<p>)
for (x1, . . . , xn+1) &isin;U+.
</p>
<p>To include the north pole, consider a second chart ϕ&minus; : Sn &minus; {&minus;1} &equiv;U&minus; &rarr;
Rn defined by
</p>
<p>ϕ&minus;(x1, . . . , xn+1)
</p>
<p>=
(
</p>
<p>x1
</p>
<p>1 + xn+1
, . . . ,
</p>
<p>xn
</p>
<p>1 + xn+1
</p>
<p>)
for (x1, . . . , xn+1) &isin;U&minus;.
</p>
<p>Next, let us find the inverses of these maps. We find the inverse of ϕ+;
that of ϕ&minus; can be found similarly. Let ξk &equiv; xk/(1 &minus; xn+1). Then one can
readily show that
</p>
<p>n&sum;
</p>
<p>k=1
ξ2k =
</p>
<p>1 + xn+1
1 &minus; xn+1
</p>
<p>&rArr; xn+1 =
&sum;n
</p>
<p>k=1 ξ
2
k &minus; 1&sum;n
</p>
<p>k=1 ξ
2
k + 1
</p>
<p>and
</p>
<p>xi =
2ξi
</p>
<p>1 +&sum;nk=1 ξ2k
for i = 1,2, . . . , n.
</p>
<p>From the definition of ϕ+, we have
</p>
<p>ϕ&minus;1+ (ξ1, . . . , ξn)= (x1, . . . , xn, xn+1)
</p>
<p>=
(
</p>
<p>2ξ1
1 +&sum;nk=1 ξ2k
</p>
<p>, . . . ,
2ξn
</p>
<p>1 +&sum;nk=1 ξ2k
,
</p>
<p>&sum;n
k=1 ξ
</p>
<p>2
k &minus; 1&sum;n
</p>
<p>k=1 ξ
2
k + 1
</p>
<p>)
.
</p>
<p>(28.1)</p>
<p/>
</div>
<div class="page"><p/>
<p>866 28 Analysis of Tensors
</p>
<p>On the overlap of U+ and U&minus;, i.e., on all points of Sn except the north
and the south poles, ϕ&minus; ◦ ϕ&minus;1+ : Rn &rarr; Rn can be calculated by noting that
ϕ&minus; has the following effect on a typical entry of Eq. (28.1):
</p>
<p>xj �&rarr;
xj
</p>
<p>1 + xn+1
=
</p>
<p>2ξj
1+&sum;nk=1 ξ2k
</p>
<p>1 +
&sum;n
</p>
<p>k=1 ξ
2
k&minus;1&sum;n
</p>
<p>k=1 ξ
2
k+1
</p>
<p>= ξj&sum;n
k=1 ξ
</p>
<p>2
k
</p>
<p>.
</p>
<p>Therefore,
</p>
<p>ϕ&minus; ◦ ϕ&minus;1+ (ξ1, . . . , ξn)=
(
</p>
<p>ξ1&sum;n
k=1 ξ
</p>
<p>2
k
</p>
<p>, . . . ,
ξn&sum;n
k=1 ξ
</p>
<p>2
k
</p>
<p>)
.
</p>
<p>It is clear that ϕ&minus; ◦ ϕ&minus;1+ has derivatives of all orders except possibly at a
point for which ξi = 0 for all i. But this would correspond to xn+1 = 1,
which is excluded from the region of overlap.
</p>
<p>28.2 Curves and Tangent Vectors
</p>
<p>We noted above that functions are special cases of Definition 28.1.9. An-
other special case occurs when M =R. This is important enough to warrant
a separate definition.
</p>
<p>Definition 28.2.1 A differentiable curve in the manifold M is a C&infin; mapdifferentiable curve
of an interval of R to M .
</p>
<p>This definition should be familiar from calculus, where M = R3 and a
curve is given by its parametric equation (f1(t), f2(t), f3(t)), or simply by
r(t). The point γ (a) &isin;M is called the initial point, and γ (b) &isin;M is calledinitial and final points of
</p>
<p>a curve the final point of the curve γ . A curve is closed if γ (a)= γ (b).
We are now ready to consider what a vector at a point is. All the familiar
</p>
<p>vectors in classical physics, such as displacement, velocity, momentum, and
so forth, are based on the displacement vector. Let us see how we can gen-
eralize such a vector so that it is compatible with the concept of a manifold.
</p>
<p>In R2, we define the displacement vector from P to Q as a directed
straight line that starts at P and ends at Q. Furthermore, the direction of
the vector remains the same if we connect P to any other final point on the
line PQ located beyond Q. This is because R2 is a flat space, a straight line
is well-defined, and there is no ambiguity in the direction of the vector from
P to Q.
</p>
<p>Things change, however, if we move to a two-dimensional spherical sur-
face such as the globe. How do we define the straight line from New York
to Beijing? There is no satisfactory definition of the word &ldquo;straight&rdquo; on a
curved surface. Let us say that &ldquo;straight&rdquo; means shortest distance. Then our
shortest path would lie on a great circle passing through New York and Bei-
jing. Define the &ldquo;direction&rdquo; of the trip as the &ldquo;straight&rdquo; arrow, say 1 km in
length, connecting our present position to the next point 1 km away. As we</p>
<p/>
</div>
<div class="page"><p/>
<p>28.2 Curves and Tangent Vectors 867
</p>
<p>move from New York to Beijing, going westward, the tip of the arrow keeps
changing direction. Its direction in New York is slightly different from its
direction in Chicago. In San Francisco the direction is changed even more,
and by the time we reach Beijing, the tip of the arrow will be almost opposite
to its original direction.
</p>
<p>The reason for such a changing arrow is, of course, the curvature of the
manifold. We can minimize this curvature effect if we do not go too far from
New York. If we stay close to New York, the surface of the earth appears
flat, and we can draw arrows between points. The closer the two points, the
better the approximation to flatness. Clearly, the concept of a vector is a
local concept, and the process of constructing a vector is a limiting process.
</p>
<p>The limiting process in the globe example entailed the notions of &ldquo;close-
ness&rdquo;. Such a notion requires the concept of distance, which is natural for
a globe but not necessary for a general manifold. For most manifolds it is
possible to define a metric that gives the &ldquo;distance&rdquo; between two points of
the manifold. However, the concept of a vector is too general to require such
an elaborate structure as a metric. The abstract usefulness of a metric is a re-
sult of its real-valuedness: given two points P1 and P2, the distance between
them, d(P1,P2), is a nonnegative real number. Thus, distances between dif-
ferent points can be compared.
</p>
<p>We have already defined two concepts for manifolds (more basic than
the concept of a metric) that together can replace the concept of a metric in
defining a vector as a limit. These are the concepts of (real-valued) functions
and curves. Let us see how functions and curves can replace metrics.
</p>
<p>Let γ : [a, b] &rarr;M be a curve in the manifold M . Let P &isin;M be a point
of M that lies on γ such that γ (c)= P for some c &isin; [a, b]. Let f &isin; F&infin;(P ).
Restrict f to the neighboring points of P that lie on γ . Then the composite
function f ◦ γ :R&rarr;R is a real-valued function on R.
</p>
<p>We can compare values of f ◦ γ for various real numbers close to c&mdash;
as in calculus. If u &isin; [a, b] denotes4 the variable, then f ◦ γ (u)= f (γ (u))
gives the value of f ◦ γ at various u&rsquo;s. In particular, the difference �(f ◦
γ )&equiv; f (γ (u))&minus; f (γ (c)) is a measure of how close the point γ (u) &isin;M is
to P . Going one step further, we define
</p>
<p>d(f ◦ γ )
du
</p>
<p>∣∣∣∣
u=c
</p>
<p>= lim
u&rarr;c
</p>
<p>f (γ (u))&minus; f (γ (c))
u&minus; c , (28.2)
</p>
<p>the usual derivative of an ordinary function of one variable. However, this
derivative depends on γ and on the point P . The function f is merely a test
function. We could choose any other function to test how things change with
movement along γ . What is important is not which function we choose, but
how the curve γ causes it to change with movement along γ away from P .
This change is determined by the directional derivative along γ at P , as
given by (28.2). A directional derivative determines a tangent which, in turn,
suggests a tangent vector. That is why the tangent vector at P along γ is
defined to be the directional derivative itself!
</p>
<p>4We usually use u or t to denote the (real) argument of the map γ : [a, b]&rarr;M .</p>
<p/>
</div>
<div class="page"><p/>
<p>868 28 Analysis of Tensors
</p>
<p>The use of derivative as tangent vector may appear strange to the novice,
especially physicists encountering it for the first time, but it has been familiar
to mathematicians for a long time. It is hard for the beginner to imagine vec-
tors being charged with the responsibility of measuring the rate of change of
functions. It takes some mental adjustment to get used to this idea. The fol-
lowing simple illustration may help with establishing the vector-derivative
connection.
</p>
<p>Example 28.2.2 Let us take the familiar case of a plane and consider theillustration of the
equality of vectors and
</p>
<p>directional derivatives
</p>
<p>vector a = ax êx+ay êy . What kind of a directional derivative can correspond
to a? First we need a curve γ : R&rarr; R2 that is somehow associated with a.
It is not hard to convince oneself that the most natural association is that of
vectors to tangents. Thus, we seek a curve whose tangent is (parallel to) a.
The easiest (but not the only) way is simply to take the straight line along
a; that is, let γ (u) = (axu,ayu). The directional derivative at u = 0 for an
arbitrary function f :R2 &rarr;R is given by
</p>
<p>d(f ◦ γ )
du
</p>
<p>∣∣∣∣
u=0
</p>
<p>= lim
u&rarr;0
</p>
<p>f (γ (u))&minus; f (γ (0))
u
</p>
<p>= lim
u&rarr;0
</p>
<p>f (axu,ayu)&minus; f (0,0)
u
</p>
<p>.
</p>
<p>(28.3)
Taylor expansion in two dimensions yields
</p>
<p>f (axu,ayu)= f (0,0)+ axu
&part;f
</p>
<p>&part;x
</p>
<p>∣∣∣∣
u=0
</p>
<p>+ ayu
&part;f
</p>
<p>&part;y
</p>
<p>∣∣∣∣
u=0
</p>
<p>+ &middot; &middot; &middot; .
</p>
<p>Substituting in (28.3), we obtain
</p>
<p>d(f ◦ γ )
du
</p>
<p>∣∣∣∣
u=0
</p>
<p>= lim
u&rarr;0
</p>
<p>axu(&part;f/&part;x)u=0 + ayu(&part;f/&part;y)u=0 + &middot; &middot; &middot;
u
</p>
<p>= ax
&part;f
</p>
<p>&part;x
+ ay
</p>
<p>&part;f
</p>
<p>&part;y
=
(
ax
</p>
<p>&part;
</p>
<p>&part;x
+ ay
</p>
<p>&part;
</p>
<p>&part;y
</p>
<p>)
f.
</p>
<p>This clearly shows the connection between directional derivatives and vec-
tors. In fact, the correspondences &part;/&part;x &harr; êx and &part;/&part;y &harr; êy establish this
connection very naturally.
</p>
<p>Note that the curve γ chosen above is by no means unique. In fact, there
are infinitely many curves that have the same tangent at u= 0 and give the
same directional derivative.
</p>
<p>Since vectors are the same as derivatives, we expect them to have the
properties shared by derivatives:
</p>
<p>Definition 28.2.3 Let M be a differentiable manifold. A tangent vector attangent vector defined
P &isin; M is an operator t : F&infin;(P ) &rarr; R such that for every f,g &isin; F&infin;(P )
and α,β &isin;R
1. t is linear: t(αf + βg)= αt(f )+ βt(g);
2. t satisfies the derivation property:derivation property of
</p>
<p>tangent vectors
t(fg)= g(P )t(f )+ f (P )t(g).</p>
<p/>
</div>
<div class="page"><p/>
<p>28.2 Curves and Tangent Vectors 869
</p>
<p>The operator t is an abstraction of the derivative operator. Note that t(f ),
g(P ), f (P ), and t(g) are all real numbers.
</p>
<p>The reader may easily check that if addition and scalar multiplication of
tangent vectors are defined in an obvious way, the set of all tangent vectors at
P &isin;M becomes a vector space, called the tangent space at P and denoted tangent space defined
by TP (M). If U is an open subset of M (therefore, an open submanifold of
M), then it is clear that
</p>
<p>TP (U)= TP (M) for all P &isin;U. (28.4)
</p>
<p>Definition 28.2.3 was motivated by Eqs. (28.2) and (28.3). Let us go back-
wards and see if (28.2) is indeed a tangent, that is, if it satisfies the two
conditions of Definition 28.2.3.
</p>
<p>Proposition 28.2.4 Let γ be a C&infin; curve in M such that γ (c)= P . Define vectors tangent to a
curve0γγγ (c) : F&infin;(P )&rarr;R by
</p>
<p>(
0γγγ (c)
</p>
<p>)
(f )&equiv; d
</p>
<p>du
f ◦ γ
</p>
<p>∣∣∣∣
u=c
</p>
<p>, f &isin; F&infin;(P ).
</p>
<p>Then 0γγγ (c) is a tangent vector at P called the vector tangent to γ at c.
</p>
<p>Proof We have to show that the two conditions of Definition 28.2.3 are sat-
isfied for f,g &isin; F&infin;(P ) and α,β &isin; R. The first condition is trivial. For the
second condition, we use the product rule for ordinary differentiation as fol-
lows:
</p>
<p>(
0γγγ (c)
</p>
<p>)
(fg)= d
</p>
<p>du
(fg) ◦ γ
</p>
<p>∣∣∣∣
u=c
</p>
<p>&equiv; d
du
</p>
<p>[
(f ◦ γ )(g ◦ γ )
</p>
<p>]∣∣∣∣
u=c
</p>
<p>=
[
d
</p>
<p>du
(f ◦ γ )
</p>
<p>∣∣∣∣
u=c
</p>
<p>]
(g ◦ γ )u=c + (f ◦ γ )u=c
</p>
<p>[
d
</p>
<p>du
(g ◦ γ )
</p>
<p>∣∣∣∣
u=c
</p>
<p>]
</p>
<p>=
[(
0γγγ (c)
</p>
<p>)
(f )
</p>
<p>]
g
(
γ (c)
</p>
<p>)
+ f
</p>
<p>(
γ (c)
</p>
<p>)[(
0γγγ (c)
</p>
<p>)
(g)
</p>
<p>]
</p>
<p>=
[(
0γγγ (c)
</p>
<p>)
(f )
</p>
<p>]
g(P )+ f (P )
</p>
<p>[(
0γγγ (c)
</p>
<p>)
(g)
</p>
<p>]
.
</p>
<p>Note that in going from the first equality to the second, we used the fact that
by definition, the product of two functions evaluated at a point is the product
of the values of the two functions at that point. �
</p>
<p>Let us now consider a special curve and corresponding tangent vector
that is of extreme importance in applications. Let ϕ = (x1, x2, . . . , xm) be a
coordinate system at P , where xi :M &rarr; R is the ith coordinate function.
Then ϕ is a bijective C&infin; mapping from the manifold M into Rm. Its inverse,
ϕ&minus;1 : Rm &rarr;M , is also a C&infin; mapping. Now, the ith coordinate of P is the
real number u &equiv; xi(P ). Suppose that all coordinates of P are held fixed
except the ith one, which is allowed to vary with u describing this variation.
</p>
<p>Definition 28.2.5 Let (UP , ϕ) be a chart at P &isin; M . Then the curve γ i :
R&rarr;M , defined by
</p>
<p>γ i(u)= ϕ&minus;1
(
x1(P ), . . . , xi&minus;1(P ),u, xi+1(P ), . . . , xm(P )
</p>
<p>)</p>
<p/>
</div>
<div class="page"><p/>
<p>870 28 Analysis of Tensors
</p>
<p>is called the ith coordinate curve through P . The tangent vector to thiscoordinate curve,
coordinate vector field,
</p>
<p>and coordinate frames
</p>
<p>curve at P is denoted by &part;i |P and is called the ith coordinate vector field
at P . The collection of all vector fields at P is called a coordinate frame
at P . The variable u is arbitrary in the sense that it can be replaced by any
(good) function of u.
</p>
<p>Let c= xi(P ). Then for f &isin; F&infin;(P ), we have
</p>
<p>(&part;i |P )f =
(
0γγγ i(c)
</p>
<p>)
(f )= d
</p>
<p>du
f ◦ γ i
</p>
<p>∣∣∣∣
u=c
</p>
<p>= d
du
</p>
<p>f
(
ϕ&minus;1
</p>
<p>(
x1(P ), . . . , xi&minus;1(P ),u, xi+1(P ), . . . , xm(P )
</p>
<p>))∣∣∣∣
u=c
</p>
<p>&equiv; &part;f
&part;xi
</p>
<p>∣∣∣∣
P
</p>
<p>&rArr; &part;i |P =
&part;
</p>
<p>&part;xi
</p>
<p>∣∣∣∣
P
</p>
<p>, (28.5)
</p>
<p>where the last equality is a (natural) definition of the partial derivative of
f with respect to the ith coordinate evaluated at the point P . This partial
derivative is again a C&infin; function at P . We therefore have the following:
</p>
<p>Proposition 28.2.6 The coordinate frame {&part;i |P }mi=1 at P is a set of opera-
tors &part;i(P ) : F&infin;(P )&rarr;R given by
</p>
<p>(&part;i |P )f =
&part;f
</p>
<p>&part;xi
</p>
<p>∣∣∣∣
P
</p>
<p>&equiv; d
du
</p>
<p>f
(
ϕ&minus;1
</p>
<p>(
x1(P ), . . . , xi&minus;1(P ),u, xi+1(P ), . . . , xm(P )
</p>
<p>))∣∣∣∣
u=c
</p>
<p>.
</p>
<p>(28.6)
</p>
<p>Another common notation for &part;f/&part;xi is f,i
</p>
<p>Example 28.2.7 Pick a point P = (sin θ cosϕ, sin θ sinϕ, cos θ) on the
sphere S2 in a chart (UP ,μ) given by μ(sin θ cosϕ, sin θ sinϕ, cos θ) =
(θ,ϕ). If θ is kept constant and ϕ is allowed to vary over values given by u,
then the coordinate curve associated with ϕ is given by
</p>
<p>γϕ(u)= μ&minus;1(θ, u)= (sin θ cosu, sin θ sinu, cos θ).
</p>
<p>As u varies, γϕ(u) describes a curve on S2. This curve is simply a circle of
radius sin θ . The tangent to this curve at any point is &part;/&part;ϕ, or simply &part;ϕ , the
derivative with respect to the coordinate ϕ.
</p>
<p>Similarly, the curve γθ (u) describes a great circle on S2 with tangent
&part;θ &equiv; &part;/&part;θ .
</p>
<p>The vector space TP (M) of all tangents at P was mentioned earlier. In
the case of S2 this tangent space is simply a plane tangent to the sphere at a
point. Also, the two vectors, &part;θ and &part;ϕ encountered in Example 28.2.7 are
clearly linearly independent. Thus, they form a basis for the tangent plane.
This argument can be generalized to any manifold. The following theorem
is such a generalization (for a proof, see [Bish 80, pp. 51&ndash;53]):</p>
<p/>
</div>
<div class="page"><p/>
<p>28.2 Curves and Tangent Vectors 871
</p>
<p>Theorem 28.2.8 Let M be an m-dimensional manifold and P &isin;M . Then
the set {&part;i |P }mi=1 forms a basis of TP (M). In particular, TP (M) is m-
dimensional. An arbitrary vector, t &isin; TP (M), can be written as Remember Einstein&rsquo;s
</p>
<p>summation convention!
</p>
<p>t= αi&part;i |P , where αi = t
(
xi
)
.
</p>
<p>The last statement can be derived by letting both sides operate on xj and
using Eq. (28.6). Let M = V, a vector space. Choose a basis {ei} in V with
its dual considered as coordinate functions. Then, at every v &isin; V, there is a
natural isomorphism φ : V &rarr; Tv(V) mapping a vector u = αiei &isin; V onto
αi&part;i |v &isin; Tv(V). The reader may verify that this isomorphism is coordinate
independent; i.e., if one chooses any other basis of V with its corresponding
dual, then φ(v) will be the same vector as before, expressed in the new
coordinate basis. Thus,
</p>
<p>Box 28.2.9 If V is a vector space, then for all v &isin; V, one can identify
Tv(V) with V itself.
</p>
<p>Suppose we have two coordinate systems at P , {xi} with tangents &part;i |P
and {yj } with tangents &nabla;j |P . Any t &isin; TP (M) can be expressed either in
terms of &part;i |P or in terms of &nabla;j |P : t = αi&part;i |P = βj&nabla;j |P . We can use this
relation to obtain αi in terms of βj : From Theorem 28.2.8, we have
</p>
<p>αi = t
(
xi
)
=
(
βj&nabla;j
</p>
<p>∣∣
P
</p>
<p>)(
xi
)
&equiv;
[
βj
</p>
<p>&part;
</p>
<p>&part;yj
</p>
<p>∣∣∣∣
P
</p>
<p>](
xi
)
= βj &part;x
</p>
<p>i
</p>
<p>&part;yj
</p>
<p>∣∣∣∣
P
</p>
<p>. (28.7)
</p>
<p>In particular, if t = &nabla;k|P , then βj = t(yj ) = [&nabla;k|P ](yj ) = δjk , and (28.7)
gives αi = &part;xi/&part;yk . Thus, using Eq. (28.5),
</p>
<p>&part;
</p>
<p>&part;yj
</p>
<p>∣∣∣∣
P
</p>
<p>= &part;x
i
</p>
<p>&part;yj
</p>
<p>&part;
</p>
<p>&part;xi
</p>
<p>∣∣∣∣
P
</p>
<p>. (28.8)
</p>
<p>For any function f &isin; F&infin;(P ), Eq. (28.8) yields
[
</p>
<p>&part;
</p>
<p>&part;yj
</p>
<p>∣∣∣∣
P
</p>
<p>]
f = &part;f
</p>
<p>&part;yj
</p>
<p>∣∣∣∣
P
</p>
<p>= &part;x
i
</p>
<p>&part;yj
</p>
<p>∣∣∣∣
P
</p>
<p>[
&part;
</p>
<p>&part;xi
</p>
<p>∣∣∣∣
P
</p>
<p>]
f = &part;x
</p>
<p>i
</p>
<p>&part;yj
</p>
<p>∣∣∣∣
P
</p>
<p>&part;f
</p>
<p>&part;xi
</p>
<p>∣∣∣∣
P
</p>
<p>.
</p>
<p>This is the chain rule for differentiation.
</p>
<p>Example 28.2.10 Let us find the coordinate curves and the coordinate
frame at P = (x, y, z) on S2. We use the coordinates of Example 28.1.3.
In particular, consider ϕ3, whose inverse is given by
</p>
<p>ϕ&minus;13 (x, y)=
(
x, y,
</p>
<p>&radic;
1 &minus; x2 &minus; y2
</p>
<p>)
.</p>
<p/>
</div>
<div class="page"><p/>
<p>872 28 Analysis of Tensors
</p>
<p>The coordinate curve γ2(u) along y is obtained by letting y be a function5
</p>
<p>of u:
</p>
<p>γ2(u)= ϕ&minus;13
(
x,h(u)
</p>
<p>)
=
(
x,h(u),
</p>
<p>&radic;
1 &minus; x2 &minus; h2(u)
</p>
<p>)
,
</p>
<p>where h(0)= y and h&prime;(0)= α, a constant. To find the coordinate vector field
at P , let f &isin; F&infin;(P ), and note that
</p>
<p>&part;2f =
d
</p>
<p>du
f
(
γ2(u)
</p>
<p>)∣∣∣∣
u=0
</p>
<p>= d
du
</p>
<p>f
(
x,h(u),
</p>
<p>&radic;
1 &minus; x2 &minus; h2(u)
</p>
<p>)∣∣∣∣
u=0
</p>
<p>= &part;f
&part;y
</p>
<p>dh
</p>
<p>du
</p>
<p>∣∣∣∣
u=0
</p>
<p>+ &part;f
&part;z
</p>
<p>[
1
</p>
<p>2
</p>
<p>(
&minus;2h(u)
</p>
<p>)dh
du
</p>
<p>1&radic;
1 &minus; x2 &minus; h2(u)
</p>
<p>]
</p>
<p>u=0
</p>
<p>= α
(
&part;f
</p>
<p>&part;y
&minus; y
</p>
<p>z
</p>
<p>&part;f
</p>
<p>&part;z
</p>
<p>)
= α
</p>
<p>(
&part;
</p>
<p>&part;y
&minus; y
</p>
<p>z
</p>
<p>&part;
</p>
<p>&part;z
</p>
<p>)
f.
</p>
<p>So, choosing the function h in such a way that α = 1,
</p>
<p>&part;2 = &part;y &minus;
y
</p>
<p>z
&part;z,
</p>
<p>where &part;y and &part;z are the coordinate vector fields of R3. The coordinate vector
field &part;1 can be obtained similarly.
</p>
<p>28.3 Differential of a Map
</p>
<p>Now that we have constructed tangent spaces and defined bases for them,
we are ready to consider the notion of the differential (derivative) of a map
between manifolds.
</p>
<p>Definition 28.3.1 Let M and N be manifolds of dimensions m and n,
respectively, and let ψ : M &rarr; N be a C&infin; map. Let P &isin; M , and let
Q = ψ(P ) &isin; N be the image of P . Then there is induced a map ψ&lowast;P :differential of a map at a
</p>
<p>point TP (M)&rarr; TQ(N), called the differential of ψ at P and given as follows.
Let t &isin; TP (M) and f &isin; F&infin;(Q). The action of ψ&lowast;P (t) &isin; TQ(N) on f is
defined as
</p>
<p>(
ψ&lowast;P (t)
</p>
<p>)
(f )&equiv; t(f ◦ψ). (28.9)
</p>
<p>The reader may check that the differential of a composite map is the
composite of the corresponding differentials, i.e.,
</p>
<p>(ψ ◦ φ)&lowast;P =ψ&lowast;φ(P ) ◦ φ&lowast;P . (28.10)
</p>
<p>Furthermore, if ψ is a local diffeomorphism at P , then ψ&lowast;P is a vector space
isomorphism. The inverse of this statement&mdash;which is called the inverse
mapping theorem, and is much harder to prove (see [Abra 88, pp. 116 and
196])&mdash;is also true:
</p>
<p>5See the last statement of Definition 28.2.5.</p>
<p/>
</div>
<div class="page"><p/>
<p>28.3 Differential of a Map 873
</p>
<p>Theorem 28.3.2 (Inverse mapping theorem) If ψ :M &rarr; N is a map and inverse mapping
theoremψ&lowast;P : TP (M)&rarr; Tψ(P )(N) is a vector space isomorphism, then ψ is a local
</p>
<p>diffeomorphism at P .
</p>
<p>Let us see how Eq. (28.9) looks in terms of coordinate functions. Suppose
that {xi}mi=1 are coordinates at P and {ya}na=1 are coordinates at Q=ψ(P ).
We note that ya ◦ψ is a real-valued C&infin; function on M . Thus, we may write
(with the function expressed in terms of coordinates)
</p>
<p>ya ◦ψ &equiv; f a
(
x1, . . . , xm
</p>
<p>)
.
</p>
<p>We also have t = αi&part;i |P . Similarly, ψ&lowast;P (t) = βa(&part;/&part;ya)|Q because
{(&part;/&part;ya)|Q} form a basis. Theorem 28.2.8 and Definition 28.3.1 now give
</p>
<p>βa =ψ&lowast;P (t)
(
ya
</p>
<p>)
= t
</p>
<p>(
ya ◦ψ
</p>
<p>)
= t
</p>
<p>(
f a
</p>
<p>)
</p>
<p>=
[
αi&part;i |P
</p>
<p>](
f a
</p>
<p>)
= αi &part;f
</p>
<p>a
</p>
<p>&part;xi
</p>
<p>∣∣∣∣
P
</p>
<p>&equiv;
m&sum;
</p>
<p>i=1
αi
</p>
<p>&part;f a
</p>
<p>&part;xi
</p>
<p>∣∣∣∣
P
</p>
<p>.
</p>
<p>This can be written in matrix form as
⎛
⎜⎜⎜⎝
</p>
<p>β1
</p>
<p>β2
</p>
<p>...
</p>
<p>βn
</p>
<p>⎞
⎟⎟⎟⎠=
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>&part;f 1/&part;x1 &part;f 1/&part;x2 . . . &part;f 1/&part;xm
</p>
<p>&part;f 2/&part;x1 &part;f 2/&part;x2 . . . &part;f 2/&part;xm
</p>
<p>...
...
</p>
<p>...
</p>
<p>&part;f n/&part;x1 &part;f n/&part;x2 . . . &part;f n/&part;xm
</p>
<p>⎞
⎟⎟⎟⎠
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>α1
</p>
<p>α2
</p>
<p>...
</p>
<p>αm
</p>
<p>⎞
⎟⎟⎟⎠ . (28.11)
</p>
<p>The n&times;m matrix is denoted by J and is called the Jacobian matrix of ψ Jacobian matrix of a
differentiable mapwith respect to the coordinates xi and ya . On numerous occasions the two
</p>
<p>manifolds are simply Cartesian spaces, so that ψ :Rm &rarr;Rn. In such a case,
f α is naturally written as ψα , and the Jacobian matrix will have elements
of the form &part;ψα/&part;xi .
</p>
<p>An important special case of the differential of a map is that of a constant
map. Let ψ :M &rarr; {Q} &isin;N be such a map; it maps all points of M onto a
single point Q of N . For any f &isin; F&infin;(Q), the function f ◦ψ &isin; F&infin;(P ) is
constant for all P &isin;M . Let t &isin; TP (M) be an arbitrary vector. Then
</p>
<p>(
ψ&lowast;P (t)
</p>
<p>)
(f )&equiv; t(f ◦ψ)= 0 &forall;f &rArr;ψ&lowast;P (t)= 0 &forall;t (28.12)
</p>
<p>because t(c)= 0 for any constant c. So, Differential of a constant
map is the zero map.
</p>
<p>Box 28.3.3 If ψ :M &rarr; {Q} &isin; N is a constant map, so that it maps
the entire manifold M onto a point Q of N , then ψ&lowast;P : TP (M) &rarr;
TQ(N) is the zero map.
</p>
<p>Two other special cases merit closer attention: M = R for arbitrary N ,
and N = R for arbitrary M . In either case Tc(R) is one-dimensional with
the basis vector (d/du)|c. When M =R, the mapping becomes a curve, γ :
R&rarr;N . The only vector whose image we are interested in is t= (d/du)|c ,</p>
<p/>
</div>
<div class="page"><p/>
<p>874 28 Analysis of Tensors
</p>
<p>with γ (c) = P . From (28.9) using Proposition 28.2.4 in the last step, we
have [
</p>
<p>γ&lowast;c
d
</p>
<p>du
</p>
<p>∣∣∣∣
c
</p>
<p>]
f = d
</p>
<p>du
f ◦ γ
</p>
<p>∣∣∣∣
u=c
</p>
<p>=
(
0γγγ (c)
</p>
<p>)
(f ).
</p>
<p>This tells us that the differential of a curve at c is simply its tangent vector
at γ (c). It is common to leave out the constant vector (d/du)|c, and write
γ&lowast;c for the LHS.
</p>
<p>Example 28.3.4 It is useful to have an expression for the components ofcomponents of tangents
to curves the tangent to a curve γ at an arbitrary point on it. Since γ maps the real
</p>
<p>line to M , with a coordinate patch established on M , we can write γ as
γ = (γ 1, . . . , γm) where γ i = xi ◦ γ are ordinary functions of one variable.
Proposition 28.2.4 then yields
</p>
<p>γ&lowast;tf =
d
</p>
<p>du
f ◦ γ
</p>
<p>∣∣∣∣
u=t
</p>
<p>= d
du
</p>
<p>f
(
γ (u)
</p>
<p>)∣∣∣∣
u=t
</p>
<p>= d
du
</p>
<p>f
(
γ 1(u), . . . , γm(u)
</p>
<p>)∣∣∣∣
u=t
</p>
<p>= &part;f
&part;xi
</p>
<p>dγ i
</p>
<p>du
</p>
<p>∣∣∣∣
u=t
</p>
<p>&equiv; &part;f
&part;xi
</p>
<p>dγ i
</p>
<p>dt
= γ̇ i&part;if,
</p>
<p>or
</p>
<p>γ&lowast;t &equiv; γ̇ i&part;i, where γ̇ i =
dγ i
</p>
<p>dt
. (28.13)
</p>
<p>For this reason, γ&lowast;t is sometimes denoted by γ̇ .
</p>
<p>When N = R, we are dealing with a real-valued function f : M &rarr;
R. The differential of f at P is f&lowast;P : TP (M) &rarr; Tc(R), where c =
f (P ). Since Tc(R) is one-dimensional, for a tangent t &isin; TP (M), we
have f&lowast;P (t) = a(d/du)|c. Let g : R &rarr; R be an arbitrary function on R.
Then [f&lowast;P (t)](g) = a(dg/du)c, or, by definition of the LHS, t(g ◦ f ) =
a(dg/du)c. To find a we choose the function g(u) = u, i.e., the iden-
tity function; then dg/du = 1 and t(g ◦ f ) = t(f ) = a. We thus obtain
f&lowast;P (t)= t(f )(d/du)|c . Since Tc(R) is a flat one-dimensional vector space,
all vectors are the same and there is no need to write (d/du)|c . Thus, we de-
fine the differential of f , denoted by df &equiv; f&lowast;, as a map df : TP (M)&rarr; R
given bydifferential of a
</p>
<p>real-valued function
df (t)= t(f ). (28.14)
</p>
<p>In particular, if f is the coordinate function xi and t is the tangent to the
j th coordinate curve &part;j |P , we obtain
</p>
<p>dxi
∣∣
P
(&part;j |P )= [&part;j |P ]
</p>
<p>(
xi
)
= &part;x
</p>
<p>i
</p>
<p>&part;xj
</p>
<p>∣∣∣∣
P
</p>
<p>= δij . (28.15)
</p>
<p>This shows that
</p>
<p>Box 28.3.5 {dxi |P }mi=1 is dual to the basis {&part;j |P }mj=1 of TP (M).</p>
<p/>
</div>
<div class="page"><p/>
<p>28.3 Differential of a Map 875
</p>
<p>Example 28.3.6 Let f :M &rarr;R be a real-valued function on M . Let xi be
coordinates at P . We want to express df in terms of coordinate functions.
For t &isin; TP (M) we can write t= αi&part;i |P and Remember Einstein&rsquo;s
</p>
<p>summation convention!
df (t)= t(f )= αi[&part;i |P ](f )= αi&part;i(f ),
</p>
<p>where in the last step, we suppressed the P . Theorem 28.2.8 and Eq. (28.14)
yield αi = t(xi)= (dxi)(t). We thus have
</p>
<p>df (t)= &part;i(f )
[(
dxi
</p>
<p>)
(t)
</p>
<p>]
=
[
&part;i(f )
</p>
<p>(
dxi
</p>
<p>)]
(t).
</p>
<p>Since this is true for all t, we get
</p>
<p>df = &part;i(f )
(
dxi
</p>
<p>)
&equiv;
</p>
<p>m&sum;
</p>
<p>i=1
&part;i(f )
</p>
<p>(
dxi
</p>
<p>)
=
</p>
<p>m&sum;
</p>
<p>i=1
</p>
<p>&part;f
</p>
<p>&part;xi
dxi . (28.16)
</p>
<p>This is the classical formula for the differential of a function f . If we choose
yj , the j th member of a new coordinate system, for f , we obtain
</p>
<p>dyj =
m&sum;
</p>
<p>i=1
</p>
<p>&part;yj
</p>
<p>&part;xi
dxi &equiv; &part;y
</p>
<p>j
</p>
<p>&part;xi
dxi, (28.17)
</p>
<p>which is the transformation dual to Eq. (28.8).
</p>
<p>Consider a map φ from the product manifold M &times; N to another mani-
fold L. Then
</p>
<p>φ&lowast; : TP (M)&times; TQ(N)&rarr; Tφ(P,Q)(L).
We want to find φ&lowast;(t, s) for t &isin; TP (M) and s &isin; TQ(N). First define the
maps φQ :M &rarr; L and φP : N &rarr; L by φQ(P ) = φ(P,Q) and φP (Q) =
φ(P,Q). Then
</p>
<p>φQ&lowast; : TP (M)&rarr; Tφ(P,Q)(L) and φP&lowast; : TQ(N)&rarr; Tφ(P,Q)(L).
</p>
<p>Now let α(t) and β(t) be the tangent curves associated with t and s passing
through P and Q, respectively. Let f &isin; F&infin;(P,Q). Then,
</p>
<p>φ&lowast;(t, s)(f )=
d
</p>
<p>dt
</p>
<p>[
(f ◦ φ)
</p>
<p>(
α(t), β(t)
</p>
<p>)]
t=0
</p>
<p>= d
dt
</p>
<p>[
(f ◦ φ)
</p>
<p>(
α(t), β(0)
</p>
<p>)]
t=0 +
</p>
<p>d
</p>
<p>dt
</p>
<p>[
(f ◦ φ)
</p>
<p>(
α(0), β(t)
</p>
<p>)]
t=0
</p>
<p>= d
dt
</p>
<p>[
(f ◦ φ)
</p>
<p>(
α(t),Q
</p>
<p>)]
t=0 +
</p>
<p>d
</p>
<p>dt
</p>
<p>[
(f ◦ φ)
</p>
<p>(
P,β(t)
</p>
<p>)]
t=0
</p>
<p>where the second line follows from the chain rule (or partial derivatives) and
the third line from the fact that α passes through P and β through Q. From
the definitions of φP and φQ, we can rewrite the last line as
</p>
<p>φ&lowast;(t, s)(f )=
d
</p>
<p>dt
</p>
<p>[
(f ◦ φQ)
</p>
<p>(
α(t)
</p>
<p>)]
t=0 +
</p>
<p>d
</p>
<p>dt
</p>
<p>[
(f ◦ φP )
</p>
<p>(
β(t)
</p>
<p>)]
t=0
</p>
<p>&equiv; φQ&lowast;(t)f + φP&lowast;(s)f.</p>
<p/>
</div>
<div class="page"><p/>
<p>876 28 Analysis of Tensors
</p>
<p>We thus have the following:
</p>
<p>Proposition 28.3.7 The differential of φ :M &times;N &rarr; L at (P,Q) is a map
φ&lowast; : TP (M)&times; TQ(N)&rarr; Tφ(P,Q)(L) given by φ&lowast;(t, s) = φQ&lowast;(t)+ φP&lowast;(s),
where φQ :M &rarr; L and φP : N &rarr; L are defined by φQ(P ) = φ(P,Q) =
φP (Q).
</p>
<p>The following is a powerful theorem that constructs a submanifold out of
a differentiable map (for a proof, see [Warn 83, p. 31]):
</p>
<p>Theorem 28.3.8 Assume that ψ :M &rarr; N is a C&infin; map, that Q is a point
in the range of ψ , and that ψ&lowast; : TP (M)&rarr; TQ(N) is surjective for all P &isin;
ψ&minus;1(Q). Thenψ&minus;1(Q) is a submanifold ofM and dimψ&minus;1(Q)= dimM&minus;
dimN .
</p>
<p>Compare this theorem with Proposition 28.1.10. There, V was an open
subset of N , and since f&minus;1(V ) is open, it is automatically an open subman-
ifold. The difficulty in proving Theorem 28.3.8 lies in the fact that ψ&minus;1(Q)
is closed because {Q}, a single point of N , is closed.
</p>
<p>We can justify the last statement of the theorem as follows. From
Eq. (28.12), we readily conclude that TP (ψ&minus;1(Q))= kerψ&lowast;P . The dimen-
sion theorem, applied to ψ&lowast;P : TP (M)&rarr; TQ(N), now gives
</p>
<p>dimTP (M)= dim kerψ&lowast;P + rankψ&lowast;P
&rArr; dimM = dimψ&minus;1(Q)+ dimN,
</p>
<p>where the last equality follows from the surjectivity of ψ&lowast;P .
</p>
<p>Example 28.3.9 Consider a C&infin; map f : Rn &rarr; R. Let c &isin; R such that the
partial derivatives of f are defined and not all zero for all points of f&minus;1(c).
Then, according to Eq. (28.11), a vector αi&part;i &isin; TP (Rn) is mapped by f&lowast; to
the vector αi(&part;f/&part;xi)f=cd/dt . Since &part;f/&part;xi are not all zero, by properly
choosing αi , we can make αi(&part;f/&part;xi)f=cd/dt sweep over all real numbers.
Therefore, f&lowast; is surjective, and by Theorem 28.3.8, f&minus;1(c) is an (n&minus; 1)-
dimensional submanifold of Rn. A noteworthy special case is the function
defined by
</p>
<p>f
(
x1, x2, . . . , xn
</p>
<p>)
=
(
x1
)2 +
</p>
<p>(
x2
)2 + &middot; &middot; &middot; +
</p>
<p>(
xn
</p>
<p>)2
</p>
<p>and c = r2 &gt; 0. Then, f&minus;1(c), an (n&minus; 1)-sphere of radius r , is a submani-
fold of Rn.
</p>
<p>28.4 Tensor Fields onManifolds
</p>
<p>So far we have studied vector spaces, learned how to construct tensors out
of vectors, touched on manifolds (the abstraction of spaces), seen how to
construct vectors at a single point in a manifold by the use of the tangent-
at-a-curve idea, and even found the dual vectors dxi |P to the coordinate</p>
<p/>
</div>
<div class="page"><p/>
<p>28.4 Tensor Fields on Manifolds 877
</p>
<p>vectors &part;i |P at a point P of a manifold. We have everything we need to
study the analysis of tensors.
</p>
<p>28.4.1 Vector Fields
</p>
<p>We are familiar with the concept of a vector field in 3D: Electric field, mag-
netic field, gravitational field, velocity field, and so forth are all familiar
notions. We now want to generalize the concept so that it is applicable to a
general manifold. To begin with, let us consider the following definition.
</p>
<p>Definition 28.4.1 The union of all tangent spaces at different points of a
manifold M is denoted by T (M) and called the tangent bundle of M : tangent bundle defined
</p>
<p>T (M)=
⋃
</p>
<p>P&isin;M
TP (M)
</p>
<p>It can be shown ([Bish 80, pp. 158&ndash;164]) that T (M) is a manifold of
dimension 2 dimM .
</p>
<p>Definition 28.4.2 A vector field X on a subset U of a manifold M is a vector field defined
mapping X : U &rarr; T (M) such that X(P ) &equiv; X|P &equiv; XP &isin; TP (M). The set
of vector fields on M is denoted by X(M). Let M and N be manifolds
and F : M &rarr; N a differentiable map. We say that the two vector fields
X &isin;X(M) and Y &isin;X(N) are F -related if F&lowast;(XP )= YF(P ) for all P &isin;M . vector fields related by a
</p>
<p>mapThis is sometimes written simply as F&lowast;X = Y.
</p>
<p>It is worthwhile to point out that F&lowast;X is not, in general, a vector field
on N . To be a vector field, F&lowast;X must be defined at all points of N . The
natural way to define F&lowast;X at Q &isin;N is [F&lowast;X(Q)](f )= X(f ◦F)(P ) where
P is the preimage of Q, i.e., F(P )=Q. But there may not exist any such P
(F may not be onto), or there may be more than one P (F may not be one-
to-one) with such property. Therefore, this natural construction does not lead
to a vector field on N . If F&lowast;X happens to be a vector field on N , then it is
clearly F -related to X. In terms of the coordinates xi , at each point P &isin;M ,
</p>
<p>XP &equiv; X|P =XiP &part;i |P ,
</p>
<p>where the real numbers XiP are components of XP in the basis {&part;i |P }. As P
moves around in U , the real numbers XiP keep changing. Thus, we can think
of XiP as a function of P and define the real-valued function X
</p>
<p>i :M &rarr; R
by Xi(P )&equiv;XiP . Therefore, the components of a vector field are real-valued
functions on M .
</p>
<p>Example 28.4.3 Let M = R3. At each point P = (x, y, z) &isin; R3, let
(êx, êy, êz) be a basis for R3. Let VP be the vector space at P . Then T (R3)
is the collection of all vector spaces VP for all P .
</p>
<p>We can determine the value of an electric field at a point in R3 by first
specifying the point, as P0 = (x0, y0, z0), for example. This uniquely de-
termines the tangent space TP0(R
</p>
<p>3). Once we have the vector space, we</p>
<p/>
</div>
<div class="page"><p/>
<p>878 28 Analysis of Tensors
</p>
<p>can ask what the components of the electric field are in that space. These
components are given by three numbers: Ex(x0, y0, z0), Ey(x0, y0, z0), and
Ez(x0, y0, z0). The argument is the same for any other vector field.
</p>
<p>To specify a &ldquo;point&rdquo; in T (R3), we need three numbers to determine the
location in R3 and another three numbers to determine the components of a
vector field at that point. Thus, a &ldquo;point&rdquo; in T (R3) is given by six &ldquo;coordi-
nates&rdquo; (x, y, z,Ex,Ey,Ez), and T (R3) is a six-dimensional manifold.
</p>
<p>We know how a tangent vector t at a point P &isin; M acts on a function
f &isin; F&infin;(P ) to give a real number t(f ). We can extend this, point by point,
for a vector field X and define a function X(f ) by
</p>
<p>[
X(f )
</p>
<p>]
(P )&equiv; XP (f ), P &isin;U, (28.18)
</p>
<p>where U is a subset of M on which both X and f are defined. The RHS is
well-defined because we know how XP , the vector at P , acts on functions
at P to give the real number [XP ](f ). On the LHS, we have X(f ), which
maps the point P onto a real number. Thus, X(f ) is indeed a real-valued
function on M . We can therefore define vector fields directly as operators
on C&infin; functions satisfying
</p>
<p>X(αf + βg)= αX(f )+ βX(g),
</p>
<p>X(fg)=
[
X(f )
</p>
<p>]
g +
</p>
<p>[
X(g)
</p>
<p>]
f.
</p>
<p>A prototypical vector field is the coordinate vector field &part;i . In general,C&infin; vector fields
X(f ) is not a C&infin; function even if f is. A vector field that produces a C&infin;
</p>
<p>function X(f ) for every C&infin; function f is called a C&infin; vector field. Such a
vector field has components that are C&infin; functions on M .
</p>
<p>The set of tangent vectors TP (M) at a point P &isin; M form an m-
dimensional vector space. The set of vector fields X(M)&mdash;which yield a
vector at every point of the manifold&mdash;also constitutes a vector space. How-
ever, this vector space is (uncountably) infinite-dimensional.
</p>
<p>A property of X(M) that is absent in TP (M) is composition.6 This sug-
gests the possibility of defining a &ldquo;product&rdquo; on X(M) to turn it into an al-
gebra. Let X and Y be vector fields. For X ◦ Y to be a vector field, it has to
satisfy the derivation property. But
</p>
<p>X ◦ Y(fg)= X
(
Y(fg)
</p>
<p>)
= X
</p>
<p>(
Y(f )g + fY(g)
</p>
<p>)
</p>
<p>=
(
X
(
Y(f )
</p>
<p>))
g + Y(f )X(g)+ X(f )Y(g)+ f
</p>
<p>(
X
(
Y(g)
</p>
<p>))
</p>
<p>�=
(
X ◦ Y(f )
</p>
<p>)
g+ f
</p>
<p>(
X ◦ Y(g)
</p>
<p>)
.
</p>
<p>However, the reader may verify that X ◦ Y &minus; Y ◦ X does indeed satisfy the
derivation property. Therefore, by defining the binary operation X(M) &times;
</p>
<p>6Recall that a typical element of TP (M) is a map t : F&infin;(P )&rarr;R for which composition
is meaningless.</p>
<p/>
</div>
<div class="page"><p/>
<p>28.4 Tensor Fields on Manifolds 879
</p>
<p>X(M)&rarr;X(M) as The set of vector fields
forms a Lie algebra
</p>
<p>under Lie bracket
</p>
<p>multiplication.
</p>
<p>[X,Y] &equiv; X ◦ Y &minus; Y ◦ X,
X(M) becomes an algebra, called the Lie algebra of vector fields of M . The
binary operation is called the Lie bracket. Although it was not mentioned at
the time, we have encountered another example of a Lie algebra in Chap. 4,
namely L(V) under the binary operation of the commutation relation. Lie
brackets have the following two properties:
</p>
<p>[X,Y] = &minus;[Y,X],
[
[X,Y],Z
</p>
<p>]
+
[
[Z,X],Y
</p>
<p>]
+
[
[Y,Z],X
</p>
<p>]
= 0.
</p>
<p>These two relations are the defining properties of all Lie algebras. The last
relation is called the Jacobi identity. X(M) with Lie brackets is an example Jacobi identity
of an infinite-dimensional Lie algebra; L(V) with commutators is an exam-
ple of a finite-dimensional Lie algebra.
</p>
<p>We shall have occasion to use the following theorem in our treatment of
Lie groups and algebras in Chap. 29:
</p>
<p>Theorem 28.4.4 Let M and N be manifolds and F : M &rarr; N a differen-
tiable map. Assume that Xi &isin;X(M) is F -related to Yi &isin;X(N) for i = 1,2.
Then [X1,X2] is F -related to [Y1,Y2], i.e.,
</p>
<p>F&lowast;[X1,X2] = [F&lowast;X1,F&lowast;X2].
</p>
<p>Proof Let f be an arbitrary function on N . Then
</p>
<p>(
F&lowast;[X1,X2]
</p>
<p>)
f &equiv; [X1,X2](f ◦ F)= X1
</p>
<p>(
X2(f ◦ F)
</p>
<p>)
&minus; X2
</p>
<p>(
X1(f ◦ F)
</p>
<p>)
</p>
<p>= X1
([
F&lowast;X2(f )
</p>
<p>]
◦ F
</p>
<p>)
&minus; X2
</p>
<p>([
F&lowast;X1(f )
</p>
<p>]
◦ F
</p>
<p>)
</p>
<p>= F&lowast;X1
(
F&lowast;X2(f )
</p>
<p>)
&minus; F&lowast;X2
</p>
<p>(
F&lowast;X1(f )
</p>
<p>)
</p>
<p>= [F&lowast;X1,F&lowast;X2]f,
</p>
<p>where we used Eq. (28.9) in the first, second, and third lines, and the result
of Problem 28.8 in the second line. �
</p>
<p>It is convenient to visualize vector fields as streamlines. In fact, most of
the terminology used in three-dimensional vector analysis, such as flux, di-
vergence, and curl, have their origins in the flow of fluids and the associated
velocity vector fields. The streamlines are obtained&mdash;in nonturbulent flow&mdash;
by starting at one point and drawing a curve whose tangent at all points is
the velocity vector field. For a smooth flow this curve is unique. There is an
exact analogy in manifold theory.
</p>
<p>Definition 28.4.5 Let X &isin; X(M) be defined on an open subset U of M . integral curve of a vector
fieldAn integral curve of X in U is a curve γ whose range lies in U and for
</p>
<p>every t in the domain of γ , the vector tangent to γ satisfies γ&lowast;t = X(γ (t)).
If γ (0)= P , we say that γ starts at P .</p>
<p/>
</div>
<div class="page"><p/>
<p>880 28 Analysis of Tensors
</p>
<p>Let us choose a coordinate system on M . Then X &equiv; Xi&part;i , where Xi
are C&infin; functions on M , and, by (28.13), γ&lowast; = γ̇ i&part;i . The equation for the
integral curve of X will therefore become
</p>
<p>γ̇ i&part;i =Xi
(
γ (t)
</p>
<p>)
&part;i, or
</p>
<p>dγ i
</p>
<p>dt
=Xi
</p>
<p>(
γ 1(t), . . . , γm(t)
</p>
<p>)
, i = 1,2, . . . ,m.
</p>
<p>Since γ i are simply coordinates of points on M , we rewrite the equation
above as
</p>
<p>dxi
</p>
<p>dt
=Xi
</p>
<p>(
x1(t), . . . , xm(t)
</p>
<p>)
, i = 1,2, . . . ,m. (28.19)
</p>
<p>This is a system of first-order differential equations that has a unique (lo-
cal) solution once the initial value γ (0) of the curve, i.e., the coordinates
of the starting point P , is given. The precise statement for existence and
uniqueness of integral curves is contained in the following theorem.
</p>
<p>Theorem 28.4.6 Let X be a C&infin; vector field defined on an open subset U
of M . Suppose P &isin; U , and c &isin; R. Then there is a positive number ǫ and a
unique integral curve γ of X defined on |t &minus; c| &le; ǫ such that γ (c)= P .
</p>
<p>Example 28.4.7 (Examples of integral curves)
</p>
<p>(a) Let M = R with coordinate function x. The vector field X = x&part;x has
an integral curve with initial point x0 given by the DE dx/dt = x(t),
which has the solution x(t)= etx0.
</p>
<p>(b) Let M = Rn with coordinate functions xi . The vector field X =&sum;
ai&part;i has an integral curve, with initial point r0, given by the sys-
</p>
<p>tem of DEs dxi/dt = ai , which has the solution xi(t)= ai t + xi0, or
r = at + r0. The curve is therefore a straight line parallel to a going
through r0.
</p>
<p>(c) Let M =Rn with coordinate functions xi . Consider the vector field
</p>
<p>X =
n&sum;
</p>
<p>i,j=1
aijx
</p>
<p>j&part;i .
</p>
<p>The integral curve of this vector field, with initial point r0, is given
by the system of DEs dxi/dt =&sum;nj=1 aijxj , which can be written in
vector form as dr/dt = Ar where A is a constant matrix. By differ-
entiating this equation several times, one can convince oneself that
dkr/dtk = Akr. The Taylor expansion of r(t) then yields
</p>
<p>r(t)=
&infin;&sum;
</p>
<p>k=0
</p>
<p>1
</p>
<p>k!
dkr
dtk
</p>
<p>∣∣∣∣
t=0
</p>
<p>tk =
&infin;&sum;
</p>
<p>k=0
</p>
<p>tk
</p>
<p>k!A
kr0 = etAr0.
</p>
<p>(d) Let M =R2 with coordinate x, y. The reader may verify that the vec-
tor field X =&minus;y&part;x + x&part;y has an integral curve through (x0, y0) given
by
</p>
<p>x = x0 cos t &minus; y0 sin t,</p>
<p/>
</div>
<div class="page"><p/>
<p>28.4 Tensor Fields on Manifolds 881
</p>
<p>y = x0 sin t + y0 cos t,
</p>
<p>i.e., a circle centered at the origin passing through (x0, y0).
</p>
<p>Going back to the velocity vector field analogy, we can think of integral
curves as the path of particles flowing with the fluid. If we think of the
entire fluid as a manifold M , the flow of particles can be thought of as a
transformation of M . To be precise, let M be an arbitrary manifold, and
X &isin; X(M). At each point P of M , there is a unique local integral curve
γP of X starting at P defined on an open subset U of M . The map Ft :
U &rarr; M defined by Ft (P ) = γP (t) is a (local) transformation of M . The
collection of such maps with different t&rsquo;s is called the flow of the vector
field X. The uniqueness of the integral curve γP implies that Ft is a local
diffeomorphism. In fact, the collection of maps {Ft }t&isin;R forms a (local) one- flow of a vector field
parameter group of transformations in the sense that
</p>
<p>Ft ◦ Fs = Ft+s, F0 = id, (Ft )&minus;1 = F&minus;t . (28.20)
</p>
<p>One has to keep in mind that Ft at a point P &isin;M is, in general, defined only Global 1-parameter
group of
</p>
<p>transformations;
</p>
<p>complete vector fields
</p>
<p>locally in t , i.e., only for t in some open interval that depends on P . For
some special, but important, cases this interval can be taken to be the entire
R for all P , in which case we speak of a global one-parameter group of
transformations, and X is called a complete vector field on M .
</p>
<p>The symbol Ft used for the flow of the vector field X does not contain its
connection to X. In order to make this connection, it is common to define
</p>
<p>Ft &equiv; exp(tX). (28.21)
</p>
<p>This definition, with no significance attached to &ldquo;exp&rdquo; at this point, converts
Eq. (28.20) into
</p>
<p>exp(tX) ◦ exp(sX)= exp
[
(t + s)X
</p>
<p>]
,
</p>
<p>exp(0X)= id,
[
exp(tX)
</p>
<p>]&minus;1 = exp(&minus;tX),
</p>
<p>(28.22)
</p>
<p>which notationally justifies the use of &ldquo;exp&rdquo;. We shall see in our discussion
of Lie groups that this choice of notation is not accidental.
</p>
<p>Using this notation, we can write
</p>
<p>XP (f )&equiv;
d
</p>
<p>dt
f ◦ Ft (P )
</p>
<p>∣∣∣∣
t=0
</p>
<p>= d
dt
</p>
<p>f ◦ exp(tX)
∣∣∣∣
t=0
</p>
<p>.
</p>
<p>One usually leaves out the function f and writes
</p>
<p>XP =
d
</p>
<p>dt
exp(tX)
</p>
<p>∣∣∣∣
t=0
</p>
<p>, (28.23)</p>
<p/>
</div>
<div class="page"><p/>
<p>882 28 Analysis of Tensors
</p>
<p>where it is understood that the LHS acts on some f that must compose on
the RHS to the left of the exponential. Similarly, we have
</p>
<p>(F&lowast;X)F(P ) =
d
</p>
<p>dt
F (exp tX)
</p>
<p>∣∣∣∣
t=0
</p>
<p>,
</p>
<p>G&lowast;F(P )
</p>
<p>(
d
</p>
<p>dt
F (exp tX)
</p>
<p>∣∣∣∣
t=0
</p>
<p>)
</p>
<p>︸ ︷︷ ︸
=F&lowast;(X)
</p>
<p>= d
dt
</p>
<p>G ◦ F(exp tX)
∣∣∣∣
t=0
</p>
<p>,
(28.24)
</p>
<p>where F :M &rarr;N and G :N &rarr;K are maps between manifolds.
</p>
<p>Example 28.4.8 In this example, we derive a useful formula that gives the
value of a function at a neighboring point of P &isin;M located on the integral
curve of X &isin;X(M) going through P . We first note that since XP is tangent
to γP at P = γ (0), by Proposition 28.2.4 we have
</p>
<p>XP (f )=
d
</p>
<p>dt
f
(
γP (t)
</p>
<p>)∣∣∣∣
t=0
</p>
<p>= d
dt
</p>
<p>f
(
Ft (P )
</p>
<p>)∣∣∣∣
t=0
</p>
<p>.
</p>
<p>Next we use the definition of derivative and the fact that F0(P )= P to write
</p>
<p>lim
t&rarr;0
</p>
<p>1
</p>
<p>t
</p>
<p>[
f
(
Ft (P )
</p>
<p>)
&minus; f (P )
</p>
<p>]
= XP (f ).
</p>
<p>Now, if we assume that t is very small, we have
</p>
<p>f
(
Ft (P )
</p>
<p>)
= f (P )+ tXP (f )+ &middot; &middot; &middot; , (28.25)
</p>
<p>which is a Taylor series with only the first two terms kept.
</p>
<p>28.4.2 Tensor Fields
</p>
<p>We have defined vector spaces TP (M) at each point of M . We have also
constructed coordinate bases, {&part;i |P }mi=1, for these vector spaces. At the end
of Sect. 28.2, we showed that the differentials {dxi |P }mi=1 form a basis that
is dual to {&part;i |P }mi=1. Let us concentrate on this dual space, which we will
denote by T&lowast;P (M).
</p>
<p>Taking the union of all T&lowast;P (M) at all points of M , we obtain the cotan-cotangent bundle of a
manifold gent bundle of M :
</p>
<p>T &lowast;(M)=
⋃
</p>
<p>P&isin;M
T
&lowast;
P (M). (28.26)
</p>
<p>This is the dual space of T (M) at each point of M . We can now define the
analogue of the vector field for the cotangent bundle.
</p>
<p>Definition 28.4.9 A differential one-form θθθ on a subset U of a manifolddifferential one-form
M is a mapping θθθ : U &rarr; T &lowast;(M) such that θθθ(P ) &equiv; θθθP &isin; T&lowast;P (M). The col-
lection of all one-forms on M is denoted by X&lowast;(M).</p>
<p/>
</div>
<div class="page"><p/>
<p>28.4 Tensor Fields on Manifolds 883
</p>
<p>If θθθ is a one-form and X is a vector field on M , then θθθ(X) is a real-valued
function on M defined naturally by [θθθ(X)](P )&equiv; (θθθP )(XP ). The first factor
on the RHS is a linear functional at P , and the second factor is a vector at
P . So, the pairing of the two factors produces a real number. A prototypical
one-form is the coordinate differential, dxi .
</p>
<p>Associated with a differentiable map ψ :M &rarr;N , we defined a differen-
tial ψ&lowast; that mapped a tangent space of M to a tangent space of N . The dual
of ψ&lowast; (Definition 2.5.4) is denoted by ψ&lowast; and is called the pullback of ψ . pullback of a
</p>
<p>differentiable mapIt takes a one-form on N to a one-form on M . In complete analogy to the
case of vector fields, θθθ can be written in terms of the basis {dxi}: θθθ = θidxi .
Here θi , the components of θθθ , are real-valued functions on M .
</p>
<p>With the vector spaces TP (M) and T&lowast;P (M) at our disposal, we can con-
struct various kinds of tensors at each point P . The union of all these tensors
is a manifold, and a tensor field can be defined as usual. Thus, we have the
following definition.
</p>
<p>Definition 28.4.10 Let TP (M) and T&lowast;P (M) be the tangent and cotangent
spaces at P &isin; M . Then the set of tensors of type (r, s) on TP (M) is de-
noted by Trs,P (M). The bundle of tensors of type (r, s) over M , denoted by bundle of tensors and
</p>
<p>tensor fieldsT rs (M), is
</p>
<p>T rs (M)=
⋃
</p>
<p>P&isin;M
T
r
s,P (M).
</p>
<p>A tensor field T of type (r, s) over a subset U of M is a mapping T : U &rarr;
T rs (M) such that T(P )&equiv; TP &equiv; T|P &isin; Trs,P (M).
</p>
<p>In particular, T 00 (M) is the set of real-valued functions on M , T
1
0 (M)=
</p>
<p>T (M), and T 01 (M) = T &lowast;(M). Furthermore, since T is a multilinear map,
the parentheses are normally reserved for vectors and their duals, and as
indicated in Definition 28.4.10, the value of T at P &isin;M is written as TP or
T|P . The reader may check that the map
</p>
<p>T :X&lowast;(M)&times; &middot; &middot; &middot; &times;X&lowast;(M)︸ ︷︷ ︸
r times
</p>
<p>&times;X(M)&times; &middot; &middot; &middot; &times;X(M)︸ ︷︷ ︸
s times
</p>
<p>&rarr; T 00 (M)
</p>
<p>defined by
[
T
(
ωωω1, . . . ,ωωωr ,v1, . . . ,vs
</p>
<p>)]
(P )= TP
</p>
<p>(
ωωω1|P , . . . ,ωωωr |P ,v1|P , . . . ,vs |P
</p>
<p>)
</p>
<p>has the property that A crucial property of
tensors
</p>
<p>T
(
. . . , fωωωj + gθθθ j , . . .
</p>
<p>)
= f T
</p>
<p>(
. . . ,ωωωj , . . .
</p>
<p>)
+ gT
</p>
<p>(
. . . ,θθθ j , . . .
</p>
<p>)
,
</p>
<p>T(. . . , f vk + guk, . . . )= f T(. . . ,vk, . . . )+ gT(. . . ,uk, . . . )
(28.27)
</p>
<p>for any two functions f and g on M . Thus,7
</p>
<p>7In mathematical jargon, X(M) and X&lowast;(M) are called modules over the (ring of) real-
valued functions on M . Rings are a generalization of the real numbers (field of real num-
bers) whose elements have all the properties of a field except that they may have no
inverse. A module over a field is a vector space.</p>
<p/>
</div>
<div class="page"><p/>
<p>884 28 Analysis of Tensors
</p>
<p>Box 28.4.11 A tensor is linear in vector fields and 1-forms, even
when the coefficients of linear expansion are functions.
</p>
<p>The components of T with respect to coordinates xi are the mr+s real-
valued functions
</p>
<p>T
i1i2...ir
j1j2...js
</p>
<p>= T
(
dxi1, dxi2, . . . , dxir , &part;j1 , &part;j2 , . . . , &part;js
</p>
<p>)
.
</p>
<p>If tensor fields are to be of any use, we must be able to differentiate them.
We shall consider three types of derivatives with different applications. We
study one of them here, another in the next section, and the third in Chap. 36.
</p>
<p>Derivatives can be defined only for objects that can be added. For func-
tions of a single (real or complex) variable, this is done almost subcon-
sciously: We take the difference between the values of the function at two
nearby points and divide by the length of the interval between the two
points. We extended this definition to operators in Chap. 4 with practically
no change. For functions of more than one variable, one chooses a direction
(a vector) and considers change in the function along that direction. This
leads to the concept of directional derivative, or partial derivative whendirectional derivative
the vector happens to be along one of the axes.
</p>
<p>In all the above cases, the objects being differentiated reside in the same
space: f (t) and f (t + �t) are both real (complex) numbers; H(t) and
H(t +�t) both belong to L(V). When we try to define derivatives of ten-
sor fields, however, we run immediately into trouble: TP and TP &prime; cannot
be compared because they belong to two different spaces, one to Trs,P (M)
and the other to Tr
</p>
<p>s,P &prime;(M). To make comparisons, we need first to establishdifficulty associated with
differentiating tensors a &ldquo;connection&rdquo; between the two spaces. This connection has to be a vec-
</p>
<p>tor space isomorphism so that there is one and only one vector in the sec-
ond space that is to be compared with a given vector in the first space. The
problem is that there are infinitely many isomorphisms between any given
two vector spaces. No &ldquo;natural&rdquo; isomorphism exists between Trs,P (M) and
T
r
s,P &prime;(M); thus the diversity of tensor &ldquo;derivatives!&rdquo; We narrow down this
</p>
<p>diversity by choosing a specific vector at Trs,P (M) and seeking a natural
way of defining the derivative along that vector by associating a &ldquo;natural&rdquo;
isomorphism corresponding to the vector. There are a few methods of doing
this. We describe one of them here.
</p>
<p>First, let us see what happens to tensor fields under a diffeomorphism of
M onto itself. Let F :M &rarr;M be such a diffeomorphism. The differential
F&lowast;P of this diffeomorphism is an isomorphism of TP (M) and TF(P )(M).
This isomorphism induces an isomorphism of the vector spaces Trs,P (M)
and Trs,F (P )(M)&mdash;also denoted by F&lowast;P &mdash;by Eq. (26.10). Let us denote by
F&lowast; a map of T (M) onto T (M) whose restriction to TP (M) is F&lowast;P . If T is a
tensor field on M , then F&lowast;(T) is also a tensor field, whose value at F(Q) is
obtained by letting F&lowast;Q act on T(Q):
</p>
<p>[
F&lowast;(T)
</p>
<p>](
F(Q)
</p>
<p>)
= F&lowast;Q
</p>
<p>(
T(Q)
</p>
<p>)
,</p>
<p/>
</div>
<div class="page"><p/>
<p>28.4 Tensor Fields on Manifolds 885
</p>
<p>or, letting P = F(Q) or Q= F&minus;1(P ),
[
F&lowast;(T)
</p>
<p>]
(P )= F&lowast;F&minus;1(P )
</p>
<p>(
T
(
F&minus;1(P )
</p>
<p>))
. (28.28)
</p>
<p>Now, let X be a vector field and P &isin; M . The flow of X at P defines a
local diffeomorphism Ft : U &rarr; Ft (U) with P &isin; U . The differential Ft&lowast; of
this diffeomorphism is an isomorphism of TP (M) and TFt (P )(M). As dis-
cussed above, this isomorphism induces an isomorphism of the vector space
T
r
s,P (M) onto itself. The derivative we are after is defined by comparing a
</p>
<p>tensor field evaluated at P with the image of the same tensor field under
the isomorphism F&minus;1t&lowast; . The following definition makes this procedure more
precise.
</p>
<p>Definition 28.4.12 Let P &isin; M , X &isin; X(M), and Ft the flow of X defined
in a neighborhood of P . The Lie derivative of a tensor field T at P with Lie derivative of tensor
</p>
<p>fields with respect of a
</p>
<p>vector field
</p>
<p>respect to X is denoted by (LXT)P and defined by
</p>
<p>(LXT)P = lim
t&rarr;0
</p>
<p>1
</p>
<p>t
</p>
<p>[
F&minus;1t&lowast; TFt (P ) &minus; TP
</p>
<p>]
&equiv; d
</p>
<p>dt
F&minus;1t&lowast; TFt (P )
</p>
<p>∣∣∣∣
t=0
</p>
<p>. (28.29)
</p>
<p>Let us calculate the derivative in Eq. (28.29) at an arbitrary value of t .
For this purpose, let Q&equiv; Ft (P ). Then
</p>
<p>d
</p>
<p>dt
F&minus;1t&lowast; TFt (P ) &equiv; lim
</p>
<p>�t&rarr;0
1
</p>
<p>�t
</p>
<p>[
F&minus;1(t+�t)&lowast;TFt+�t (P ) &minus; F&minus;1t&lowast; TFt (P )
</p>
<p>]
</p>
<p>= F&minus;1t&lowast; lim
�t&rarr;0
</p>
<p>1
</p>
<p>�t
</p>
<p>[
F&minus;1�t&lowast;TFt+�t (P ) &minus; TFt (P )
</p>
<p>]
</p>
<p>= F&minus;1t&lowast; lim
�t&rarr;0
</p>
<p>1
</p>
<p>�t
</p>
<p>[
F&minus;1�t&lowast;TF�t (Q) &minus; TQ
</p>
<p>]
&equiv; F&minus;1t&lowast; (LXT)Q.
</p>
<p>Since Q is arbitrary, we can remove it from the equation and write, as the
generalization of Eq. (28.29),
</p>
<p>LXT= Ft&lowast;
d
</p>
<p>dt
F&minus;1t&lowast; T. (28.30)
</p>
<p>An important special case of the definition above is the Lie derivative of a
vector field with respect to another. Let X,Y &isin;X(M). To evaluate the RHS
of (28.29), we apply the first term in the brackets to an arbitrary function f ,
</p>
<p>[
F&minus;1t&lowast; YFt (P )
</p>
<p>]
(f )= YFt (P )
</p>
<p>(
f ◦ F&minus;1t
</p>
<p>)
= Y
</p>
<p>(
f ◦ F&minus;1t
</p>
<p>)∣∣
Ft (P )
</p>
<p>= Y(f ◦ F&minus;t )
∣∣
Ft (P )
</p>
<p>= Y
(
f &minus; tX(f )
</p>
<p>)∣∣
Ft (P )
</p>
<p>= (Yf )Ft (P ) &minus; tY
(
X(f )
</p>
<p>)∣∣
Ft (P )
</p>
<p>= (Yf )P + t
[
X(Yf )
</p>
<p>]
P
&minus; t
</p>
<p>{[
Y(Xf )
</p>
<p>]
P
</p>
<p>+ t
[
X
(
Y(Xf )
</p>
<p>)]
P
</p>
<p>}
</p>
<p>= YP (f )+ tXP ◦ YP (f )&minus; tYP ◦ XP (f )
= YP (f )+ t[XP ,YP ](f )= YP (f )+ t[X,Y]P (f ).</p>
<p/>
</div>
<div class="page"><p/>
<p>886 28 Analysis of Tensors
</p>
<p>The first equality on the first line follows from (28.9), the second equality
from the meaning of YFt (P ); the second equality on the second line and the
fourth line follow from (28.25). Finally, the fifth line follows if we ignore
the t2 term. Therefore,
</p>
<p>(LXY)P (f )= lim
t&rarr;0
</p>
<p>1
</p>
<p>t
</p>
<p>[
F&minus;1t&lowast; YFt (P ) &minus; YP
</p>
<p>]
(f )
</p>
<p>= lim
t&rarr;0
</p>
<p>1
</p>
<p>t
</p>
<p>{
t[X,Y]P
</p>
<p>}
(f )= [X,Y]P (f ).
</p>
<p>Since this is true for all P and f , we getLie derivative is
commutator
</p>
<p>LXY = [X,Y]. (28.31)
</p>
<p>This and other properties of the Lie derivative are summarized in the fol-
lowing proposition.
</p>
<p>Proposition 28.4.13 Let T &isin; T rs (M) and T&prime; be arbitrary tensor fields and
X a given vector field. Then
</p>
<p>Properties of Lie
</p>
<p>derivative 1. LX satisfies a derivation property in the algebra of tensor fields, i.e.,
</p>
<p>LX
(
T&otimes; T&prime;
</p>
<p>)
= (LXT)&otimes; T&prime; + T&otimes;
</p>
<p>(
LXT
</p>
<p>&prime;).
</p>
<p>2. LX is type-preserving, i.e., LXT is a tensor field of type (r, s).
3. LX commutes with the operation of contraction of tensor fields; in par-
</p>
<p>ticular, in combination with property 1, we have
</p>
<p>LX〈θθθ,Y〉 = 〈LXθθθ,Y〉 + 〈θθθ,LXY〉.
</p>
<p>4. LXf = Xf for every function f .
5. LXY = [X,Y] for every vector field Y.
</p>
<p>Proof Except for the last property, which we demonstrated above, the rest
follow directly from definitions and simple manipulations. The details are
left as exercises. �
</p>
<p>Although the Lie derivative of a vector field is nicely given in terms of
commutators, no such simple relation exists for the Lie derivative of a 1-
form. However, if we work in a given coordinate frame, then a useful ex-
pression for the Lie derivative of a 1-form can be obtained. Applying LX to
〈θθθ,X〉, we obtain
</p>
<p>LX〈θθθ,Y〉︸ ︷︷ ︸
=X(〈θθθ,Y〉)
</p>
<p>= 〈LXθθθ,Y〉 + 〈θθθ,LXY〉 = 〈LXθθθ,Y〉 +
&lang;
θθθ, [X,Y]
</p>
<p>&rang;
. (28.32)
</p>
<p>In particular, if Y = &part;i and we write X = Xj&part;j , θθθ = θjdxj , then the LHS
becomes X(θi)=Xj&part;j θi , and the RHS can be written as
</p>
<p>(LXθθθ)i +
&lang;
θθθ,
</p>
<p>[
Xj&part;j , &part;i
</p>
<p>]
︸ ︷︷ ︸
&minus;(&part;iXj )&part;j
</p>
<p>&rang;
.</p>
<p/>
</div>
<div class="page"><p/>
<p>28.4 Tensor Fields on Manifolds 887
</p>
<p>It follows that
</p>
<p>LXθθθ &equiv; (LXθθθ)idxi =
(
Xj&part;j θi + θj&part;iXj
</p>
<p>)
dxi . (28.33)
</p>
<p>We give two other useful properties of the Lie derivative applicable to all
tensors. From the Jacobi identity one can readily deduce that
</p>
<p>L[X,Y]Z = LXLYZ &minus;LYLXZ.
</p>
<p>Similarly, Eq. (28.32) yields
</p>
<p>L[X,Y]θθθ = LXLYθθθ &minus;LYLXθθθ.
</p>
<p>Putting these two equations together, recalling that a general tensor is a lin-
ear combination of tensor products of vectors and 1-forms, and that the Lie
derivative obeys the product rule of differentiation, we obtain
</p>
<p>L[X,Y]T= LXLYT&minus;LYLXT (28.34)
</p>
<p>for any tensor field T. Furthermore, Eq. (28.33) and the linearity of the Lie
bracket imply that LαX+βY = αLX + βLY when acting on vectors and 1-
forms. It follows by the same argument as above that
</p>
<p>LαX+βYT= αLXT+ βLYT &forall;T &isin; Trs (M). (28.35)
</p>
<p>Equation (28.32) gives a rule for calculating the Lie derivative of a 1-
form, i.e., it tells us how to evaluate LXθθθ on a vector Y. We can generalize
this for a p-formωωω. Write the evaluation ofωωω on p vectors as p contractions
as in Eq. (26.8):
</p>
<p>ωωω(X1,X2, . . . ,Xp)= Cpp &middot; &middot; &middot;C22C11(ωωω&otimes; X1 &otimes; X2 &otimes; &middot; &middot; &middot; &otimes; Xp).
</p>
<p>Now apply the Lie derivative on both sides and use its derivation property
and the fact the it commutes with contractions to get
</p>
<p>LX
(
ωωω(X1,X2, . . . ,Xp)
</p>
<p>)
= Cpp &middot; &middot; &middot;C22C11LX(ωωω&otimes; X1 &otimes; X2 &otimes; &middot; &middot; &middot; &otimes; Xp).
</p>
<p>The left-hand side is just X(ωωω(X1,X2, . . . ,Xp)). For the right-hand side, we
use
</p>
<p>LX(ωωω&otimes; X1 &otimes; &middot; &middot; &middot; &otimes; Xp)
</p>
<p>= (LXωωω)&otimes; X1 &otimes; &middot; &middot; &middot; &otimes; Xp +
p&sum;
</p>
<p>i=1
ωωω&otimes; X1 &otimes; &middot; &middot; &middot; &otimes;LXXi &otimes; &middot; &middot; &middot; &otimes; Xp.
</p>
<p>Applying the contractions, using LXXi = [X,Xi], and putting everything
together, we obtain
</p>
<p>X
(
ωωω(X1,X2, . . . ,Xp)
</p>
<p>)
</p>
<p>= (LXωωω)(X1, . . . ,Xp)+
p&sum;
</p>
<p>i=1
ωωω
(
X1, . . . , [X,Xi], . . . ,Xp
</p>
<p>)</p>
<p/>
</div>
<div class="page"><p/>
<p>888 28 Analysis of Tensors
</p>
<p>which finally gives the rule by which LXωωω acts on p vectors:
</p>
<p>(LXωωω)(X1, . . . ,Xp)
</p>
<p>= X
(
ωωω(X1,X2, . . . ,Xp)
</p>
<p>)
&minus;
</p>
<p>p&sum;
</p>
<p>i=1
ωωω
(
X1, . . . , [X,Xi], . . . ,Xp
</p>
<p>)
. (28.36)
</p>
<p>28.5 Exterior Calculus
</p>
<p>Skew-symmetric tensors are of special importance to applications. We stud-
ied these tensors in their algebraic format in Chap. 26. Let us now investigate
them as they reside on manifolds.
</p>
<p>Definition 28.5.1 Let M be a manifold and Q a point of M . Let ΛpQ(M) de-differential forms, or
simply forms, defined note the space of all antisymmetric tensors of rank p over the tangent space
</p>
<p>at Q. Let Λp(M) be the union of all ΛpQ(M) for all Q &isin;M . A differential
p-form ωωω is a mapping ωωω : U &rarr;Λp(M) such that ωωω(Q) &isin;ΛpQ(M) where
U is, as usual, a subset of M . To emphasize their domain of definition, we
sometimes use the notation Λp(U).
</p>
<p>Since {dxi}mi=1 is a basis for T &lowast;Q(M) at every Q &isin;M , {dxi1 &and; &middot; &middot; &middot; &and; dxip }
is a basis for the p-forms. All the algebraic properties established in
Chap. 26 apply to these p-forms at every point Q &isin;M .
</p>
<p>The concept of a pullback has been mentioned a number of times in con-
nection with linear maps. The most frequent use of pullbacks takes place in
conjunction with the p-forms.
</p>
<p>Definition 28.5.2 Let M and N be manifolds and ψ : M &rarr; N a differ-
entiable map. The pullback map on p-forms is the map ψ&lowast; : Λp(N) &rarr;
Λp(M) defined bydefining pullback for
</p>
<p>differential forms
ψ&lowast;ρρρ(X1, . . . ,Xp)= ρρρ(ψ&lowast;X1, . . . ,ψ&lowast;Xp) for ρρρ &isin;Λp(N).
</p>
<p>For p = 0, i.e., for functions on M , ψ&lowast;ωωω&equiv;ωωω ◦ψ .
</p>
<p>It can be shown that
</p>
<p>ψ&lowast;(ωωω &and;ηηη)=ψ&lowast;ωωω &and;ψ&lowast;ηηη, (ψ ◦ φ)&lowast; = φ&lowast; ◦ψ&lowast;. (28.37)
</p>
<p>Since ωωω varies from point to point, we can define its derivatives. Recall
that T 00 (M) is the collection of real-valued functions on M . Since the dual
of R is R, we conclude that Λ0(M), the collection of zero-forms, is the
union of all real-valued functions on M . Also recall that if f is a zero-form,
then df , the differential of f , is a one-form. Thus, the differential operator
d creates a one-form from a zero-form. The fact that this can be generalized
to p-forms is the subject of the next theorem (for a proof, see [Abra 88,
pp. 111&ndash;112]).</p>
<p/>
</div>
<div class="page"><p/>
<p>28.5 Exterior Calculus 889
</p>
<p>Theorem 28.5.3 For each point Q of M , there exists a neighborhood U
and a unique operator d :Λp(U)&rarr;Λp+1(U), called the exterior deriva-
tive operator, such that for any ωωω &isin;Λp(U) and ηηη &isin;Λq(U),
</p>
<p>1. d(ωωω+ηηη)= dωωω+ dηηη if q = p; otherwise the sum is not defined.
2. d(ωωω&and;ηηη)= (dωωω)&and;ηηη+ (&minus;1)pωωω&and; (dηηη); this is called the antiderivation
</p>
<p>property of d with respect to the wedge product.
exterior derivative and
</p>
<p>its antiderivation
</p>
<p>property3. d(dωωω)= 0 for any differential form ωωω; stated differently, d ◦ d = 0.
4. df = (&part;if )dxi for any real-valued function f .
5. d is natural with respect to pullback; that is, dM ◦ψ&lowast; =ψ&lowast;◦dN for any
</p>
<p>differentiable map ψ :M &rarr;N . Here dM (dN ) is the exterior derivative
operating on differential forms of M (N ).
</p>
<p>Example 28.5.4 Let M = R3 and ωωω = aidxi a 1-form on M . The exterior
derivative of ωωω is
</p>
<p>dωωω= (dai)&and; dxi =
(
&part;jaidx
</p>
<p>j
)
&and; dxi =
</p>
<p>&sum;
</p>
<p>j&lt;i
</p>
<p>(&part;jai &minus; &part;iaj )dxj &and; dxi .
</p>
<p>We see that the components of dωωω are the components of &nabla;&nabla;&nabla; &times; A where
A = (a1, a2, a3). It follows that the curl of a vector in R3 is the exterior
derivative of the 1-form constructed out of the components of the vector.
</p>
<p>Example 28.5.5 In relativistic electromagnetic theory the electric and mag-
netic fields are combined to form the electromagnetic field tensor. This is a The homogeneous
</p>
<p>Maxwell&rsquo;s equations are
</p>
<p>written in terms of
</p>
<p>differential forms.
</p>
<p>skew-symmetric tensor field of rank 2, which can be written as8
</p>
<p>F=&minus;Exdt &and; dx &minus;Eydt &and; dy &minus;Ezdt &and; dz
+Bzdx &and; dy &minus;Bydx &and; dz+Bxdy &and; dz, (28.38)
</p>
<p>where t is the time coordinate and the units are such that c, the velocity of
light, is equal to 1.
</p>
<p>Let us take the exterior derivative of F. In the process, we use df =
(&part;if )dx
</p>
<p>i , d(dxi &and; dxj )= 0, and in dEi or dBj we include only the terms
that give a nonzero contribution:
</p>
<p>dF=&minus;
(
&part;Ex
</p>
<p>&part;y
dy + &part;Ex
</p>
<p>&part;z
dz
</p>
<p>)
&and; dt &and; dx &minus;
</p>
<p>(
&part;Ey
</p>
<p>&part;x
dx + &part;Ey
</p>
<p>&part;z
dz
</p>
<p>)
&and; dt &and; dy
</p>
<p>&minus;
(
&part;Ez
</p>
<p>&part;x
dx + &part;Ez
</p>
<p>&part;y
dy
</p>
<p>)
&and; dt &and; dz+
</p>
<p>(
&part;Bz
</p>
<p>&part;t
dt + &part;Bz
</p>
<p>&part;z
dz
</p>
<p>)
&and; dx &and; dy
</p>
<p>&minus;
(
&part;By
</p>
<p>&part;t
dt + &part;By
</p>
<p>&part;y
dy
</p>
<p>)
&and; dx &and; dz+
</p>
<p>(
&part;Bx
</p>
<p>&part;t
dt + &part;Bx
</p>
<p>&part;x
dx
</p>
<p>)
&and; dy &and; dz.
</p>
<p>8Note how in the wedge product, the first factor has a lower index (is an &ldquo;earlier&rdquo; coor-
dinate) than the second factor. If this restriction is to be removed, we need to introduce a
factor of 12 for each component (see Example 28.5.12).</p>
<p/>
</div>
<div class="page"><p/>
<p>890 28 Analysis of Tensors
</p>
<p>Collecting all similar terms and taking into account changes of sign due to
the antisymmetry of the exterior products gives
</p>
<p>dF=
(
&part;Ey
</p>
<p>&part;x
&minus; &part;Ex
</p>
<p>&part;y
+ &part;Bz
</p>
<p>&part;t
</p>
<p>)
dt &and; dx &and; dy
</p>
<p>+
(
&part;Ez
</p>
<p>&part;x
&minus; &part;Ex
</p>
<p>&part;z
&minus; &part;By
</p>
<p>&part;t
</p>
<p>)
dt &and; dx &and; dz
</p>
<p>+
(
&part;Ez
</p>
<p>&part;y
&minus; &part;Ey
</p>
<p>&part;z
+ &part;Bx
</p>
<p>&part;t
</p>
<p>)
dt &and; dy &and; dz
</p>
<p>+
(
&part;Bx
</p>
<p>&part;x
+ &part;By
</p>
<p>&part;y
+ &part;Bz
</p>
<p>&part;z
</p>
<p>)
dx &and; dy &and; dz
</p>
<p>=
[(
</p>
<p>&nabla;&nabla;&nabla; &times; E + &part;B
&part;t
</p>
<p>)
</p>
<p>z
</p>
<p>]
dt &and; dx &and; dy
</p>
<p>+
[(
</p>
<p>&nabla;&nabla;&nabla; &times; E + &part;B
&part;t
</p>
<p>)
</p>
<p>y
</p>
<p>]
dt &and; dz&and; dx
</p>
<p>+
[(
</p>
<p>&nabla;&nabla;&nabla; &times; E + &part;B
&part;t
</p>
<p>)
</p>
<p>x
</p>
<p>]
dt &and; dy &and; dz+ (&nabla;&nabla;&nabla; &middot; B)dx &and; dy &and; dz.
</p>
<p>Each component of dF vanishes because of Maxwell&rsquo;s equations.
</p>
<p>The example above shows that
</p>
<p>Box 28.5.6 The two homogeneous Maxwell&rsquo;s equations can be writ-
ten as dF= 0, where F is defined by Eq. (28.38).
</p>
<p>The exterior derivative is a very useful concept in the theory of differ-
ential forms, as illustrated in the preceding example. However, that is not
the only differentiation available to the differential forms. We have already
defined the Lie derivative for arbitrary tensors. Since differential forms are
(antisymmetrized) linear combinations of covariant tensors, Lie differenti-
ation is defined for them as well. In fact, since differential forms have no
contravariant parts, one uses the pullback map F &lowast;t in the definition of the
Lie derivative instead of F&minus;1t&lowast; :
</p>
<p>LXωωω=
(
F &lowast;t
</p>
<p>)&minus;1 d
dt
</p>
<p>F &lowast;t ωωω. (28.39)
</p>
<p>The two derivatives defined so far have the following convenient prop-
erty, whose proof is left as an exercise for the reader:
</p>
<p>Theorem 28.5.7 The exterior derivative d is natural with respect to LX (or
commutes with LX) for X &isin;X(M); that is, d ◦LX = LX ◦ d .
</p>
<p>In the last chapter, we defined the interior product iθ for p-vectors, where
θ is a 1-form. With our shift of emphasis from p-vectors to p-forms in this
chapter, we need to shift the role of vectors and forms.</p>
<p/>
</div>
<div class="page"><p/>
<p>28.5 Exterior Calculus 891
</p>
<p>Definition 28.5.8 Let X be a vector field and ωωω a p-form on a manifold M .
Then the interior product iX :Λp(M)&rarr;Λp&minus;1(M) is defined as follows: interior product of a
</p>
<p>vector field and a
</p>
<p>differential formiXωωω(X1, . . . ,Xp&minus;1)=ωωω(X,X1, . . . ,Xp&minus;1).
</p>
<p>If ωωω &isin;Λ0(M), i.e., if ωωω is just a function, we set iXωωω= 0. Another notation
commonly used for iXωωω is X&rfloor;ωωω.
</p>
<p>The interior product iX has the antiderivation property of Theorem 27.0.2:
</p>
<p>Theorem 28.5.9 Letωωω be a p-form and ηηη a q-form on a manifoldM . Then
</p>
<p>iX(ωωω &and;ηηη)= (iXωωω)&and;ηηη+ (&minus;1)pωωω &and; (iXηηη).
</p>
<p>We have introduced three types of derivation on the algebra of differential
forms: the exterior derivative, the Lie derivative, and the interior product.
The following theorem connects all three derivations in a most useful way
(see A[Abra 88, pp. 115&ndash;116]):
</p>
<p>Theorem 28.5.10 Let ωωω &isin; Λp(M), f &isin; Λ0(M), and X &isin; X(M). Let Relation between d , LX ,
and iXiX : Λp(M) &rarr; Λp&minus;1(M), d : Λp(M) &rarr; Λp+1(M), and LX : Λp(M) &rarr;
</p>
<p>Λp(M) be the interior product, the exterior derivative, and the Lie deriva-
tive, respectively. Then
</p>
<p>1. iXdf = LXf .
2. LX = iX ◦ d + d ◦ iX.
3. LfXωωω= fLXωωω+ df &and; iXωωω.
</p>
<p>If X =Xj&part;j andωωω= ωi1i2...ip+1dxi1 &and;dxi2 &and;&middot; &middot; &middot;&and;dxip+1 , then the reader
may verify that iXωωω=Xiωii1...ipdxi1 &and;&middot; &middot; &middot;&and;dxip . In particular, we have the
useful formula
</p>
<p>iX
(
dxi1 &and; dxi2 &and; &middot; &middot; &middot; &and; dxip+1
</p>
<p>)
</p>
<p>=Xj δi1i2...ip+1j j1...jp dx
j1 &and; dxj2 &and; &middot; &middot; &middot; &and; dxjp
</p>
<p>=Xj
(&sum;
</p>
<p>π
</p>
<p>ǫπδ
i1
π(j)δ
</p>
<p>i2
π(j1)
</p>
<p>. . . δ
ip+1
π(jp)
</p>
<p>)
dxj1 &and; dxj2 &and; &middot; &middot; &middot; &and; dxjp . (28.40)
</p>
<p>Theorem 28.5.11 For a p-form ωωω, we have
</p>
<p>dωωω(X1, . . . ,Xp+1)
</p>
<p>=
p+1&sum;
</p>
<p>i=1
(&minus;1)i+1Xi
</p>
<p>(
ωωω(X1, . . . , X̂i, . . . ,Xp+1)
</p>
<p>)
</p>
<p>+
&sum;
</p>
<p>1&le;i&lt;j&le;p+1
(&minus;1)i+jωωω
</p>
<p>(
[Xi,Xj ],X1, . . . , X̂i, . . . , X̂j , . . . ,Xp+1
</p>
<p>)
</p>
<p>where the circumflex on a symbol means that symbol is to be omitted.</p>
<p/>
</div>
<div class="page"><p/>
<p>892 28 Analysis of Tensors
</p>
<p>Proof We use mathematical induction on p. From item 2 of Theo-
rem 28.5.10, we have
</p>
<p>LXωωω= iX(dωωω)+ d(iXωωω)
</p>
<p>or
</p>
<p>(LXωωω)(X1, . . . ,Xp)=
(
iX(dωωω)
</p>
<p>)
(X1, . . . ,Xp)︸ ︷︷ ︸
</p>
<p>=dωωω(X,X1,...,Xp)
</p>
<p>+
(
d(iXωωω)
</p>
<p>)
(X1, . . . ,Xp)
</p>
<p>or
</p>
<p>dωωω(X,X1, . . . ,Xp)= (LXωωω)(X1, . . . ,Xp)&minus;
(
d(iXωωω)
</p>
<p>)
(X1, . . . ,Xp).
</p>
<p>For the first term on the right-hand side, we use Eq. (28.36). For the sec-
ond term, we use the induction hypothesis because iXωωω is a (p &minus; 1)-form.
A straightforward manipulation then leads to the desired result. �
</p>
<p>Example 28.5.12 Let p = pαdxα be the momentum one-form and write
the electromagnetic field tensor as9 F = 12Fαβdxα &and; dxβ , where α and β
run over the values 0, 1, 2, and 3 with 0 being the time index. Let
</p>
<p>dp
dτ
</p>
<p>&equiv;
(
dpα
</p>
<p>dτ
</p>
<p>)
dxα
</p>
<p>be the derivative of momentum with respect to the proper time, τ . Also, letanalysis of the Lorentz
force law in the
</p>
<p>language of forms
</p>
<p>u= uβ&part;β be the velocity four-vector of a charged particle. Then the Lorentz
force law can be written simply as dp/dτ = qF(u)&equiv;&minus;qiuF, where q is the
electric charge of the particle whose 4-velocity is u. Note that F, a two-form,
contracts with u, a vector, to give a one-form on the RHS. Thus, both sides
are of the same type. Let us write this equation in component form:
</p>
<p>dpα
</p>
<p>dτ
dxα =&minus;q 1
</p>
<p>2
Fαβ iu
</p>
<p>(
dxα &and; dxβ
</p>
<p>)
=&minus;1
</p>
<p>2
qFαβ
</p>
<p>(
uγ δαβγμdx
</p>
<p>μ
)
</p>
<p>= 1
2
qFαβu
</p>
<p>γ
(
δαγ δ
</p>
<p>β
μ &minus; δαμδβγ
</p>
<p>)
dxμ
</p>
<p>= 1
2
qFαβ
</p>
<p>(
uβdxα &minus; uαdxβ
</p>
<p>)
</p>
<p>= 1
2
q(Fαβ &minus; Fβα)uβdxα =
</p>
<p>(
qFαβu
</p>
<p>β
)
dxα. (28.41)
</p>
<p>Equating the components on both sides, we get dpα/dτ = qFαβuβ , which
may be familiar to the reader. To make the equation even more familiar,
consider the component α = 1,
</p>
<p>dp1
</p>
<p>dτ
= qF1βuβ = q
</p>
<p>[
F10u
</p>
<p>0 + F12u2 + F13u3
]
, (28.42)
</p>
<p>9The factor 12 is introduced here to avoid restricting the sum over α and β .</p>
<p/>
</div>
<div class="page"><p/>
<p>28.5 Exterior Calculus 893
</p>
<p>and recall that uα = dxα/dτ , where
</p>
<p>(dτ)2 = (dt)2 &minus;
(
dx1
</p>
<p>)2 &minus;
(
dx2
</p>
<p>)2 &minus;
(
dx3
</p>
<p>)2 = (dt)2
(
1 &minus; v2
</p>
<p>)
</p>
<p>and v = (dx1/dt, dx2/dt, dx3/dt) is the 3-velocity of the particle. Since
x0 = t , we get
</p>
<p>u0 = dt
dτ
</p>
<p>= 1&radic;
1 &minus; v2
</p>
<p>, ui = dx
i
</p>
<p>dτ
= vi&radic;
</p>
<p>1 &minus; v2
for i = 1,2,3.
</p>
<p>Substituting this in (28.42) and remembering that F10 =&minus;F01 =E1, F12 =
B3, and F13 =&minus;F31 =&minus;B2, we obtain
</p>
<p>dp1
</p>
<p>dt
&radic;
</p>
<p>1 &minus; v2
= q
</p>
<p>[
E1
</p>
<p>1&radic;
1 &minus; v2
</p>
<p>+B3
v2&radic;
</p>
<p>1 &minus; v2
&minus;B2
</p>
<p>v3&radic;
1 &minus; v2
</p>
<p>]
,
</p>
<p>or
</p>
<p>dp1
</p>
<p>dt
= q
</p>
<p>[
E1 + (v2B3 &minus; v3B2)
</p>
<p>]
=
[
q(E + v &times; B)
</p>
<p>]
1.
</p>
<p>The other components are obtained similarly. Thus, in vector form we have
</p>
<p>dp
dt
</p>
<p>= q(E + v &times; B),
</p>
<p>where p now represents the 3-momentum of the particle. This is the Lorentz
force law for electromagnetism in its familiar form. Again, note the simpli-
fication offered by the language of forms.
</p>
<p>A combination that is very useful is that of the exterior derivative and the
Hodge star operator. Recall that the latter is defined by
</p>
<p>&lowast;
(
dxi1 &and; &middot; &middot; &middot; &and; dxip
</p>
<p>)
= 1
</p>
<p>(m&minus; p)!ǫ
i1...ip
ip+1...imdx
</p>
<p>ip+1 &and; &middot; &middot; &middot; &and; dxim, (28.43)
</p>
<p>where m is the dimension of the manifold.
</p>
<p>Example 28.5.13 Let us calculate &lowast;F and d(&lowast;F) where F = 12Fαβdxα &and;
dxβ is the electromagnetic field tensor. We have
</p>
<p>&lowast;F= &lowast;
(
</p>
<p>1
</p>
<p>2
Fαβdx
</p>
<p>α&and;dxβ
)
= 1
</p>
<p>2
Fαβ &lowast;
</p>
<p>(
dxα&and;dxβ
</p>
<p>)
= 1
</p>
<p>2
Fαβ
</p>
<p>1
</p>
<p>2!ǫ
αβ
μνdx
</p>
<p>μ&and;dxν
</p>
<p>and
</p>
<p>d(&lowast;F)= d
(
</p>
<p>1
</p>
<p>4
Fαβǫ
</p>
<p>αβ
μνdx
</p>
<p>μ &and; dxν
)
= 1
</p>
<p>4
ǫαβμνFαβ,γ dx
</p>
<p>γ &and; dxμ &and; dxν,
</p>
<p>where Fαβ,γ &equiv; &part;Fαβ/&part;xγ . We can now use the components Fj0 = Ej ,
F12 = B3, F13 = &minus;B2, and F23 = B1 to write d(&lowast;F) in terms of E and B.
After a long but straightforward calculation, we obtain</p>
<p/>
</div>
<div class="page"><p/>
<p>894 28 Analysis of Tensors
</p>
<p>d(&lowast;F)=
[(
</p>
<p>&part;E
&part;t
</p>
<p>&minus;&nabla;&nabla;&nabla; &times; B
)
</p>
<p>z
</p>
<p>]
dt &and; dx &and; dy
</p>
<p>+
[(
</p>
<p>&part;E
&part;t
</p>
<p>&minus;&nabla;&nabla;&nabla; &times; B
)
</p>
<p>y
</p>
<p>]
dt &and; dz&and; dx
</p>
<p>+
[(
</p>
<p>&part;E
&part;t
</p>
<p>&minus;&nabla;&nabla;&nabla; &times; B
)
</p>
<p>x
</p>
<p>]
dt &and; dy &and; dz+ (&nabla;&nabla;&nabla; &middot; E) dx &and; dy &and; dz.
</p>
<p>(28.44)
</p>
<p>The inhomogeneous pair of Maxwell&rsquo;s equations is
</p>
<p>&nabla;&nabla;&nabla; &times; B = &part;E
&part;t
</p>
<p>+ 4πJ, &nabla;&nabla;&nabla; &middot; E = 4πρ, (28.45)
</p>
<p>where ρ and J are charge and current densities, respectively. We can put
these two densities together to form a four-current one-form with ρ as the
zeroth component: J= Jαdxα . Thus,Maxwell&rsquo;s
</p>
<p>inhomogeneous
</p>
<p>equations in the
</p>
<p>language of forms
</p>
<p>&lowast;J= Jα
(
&lowast;dxα
</p>
<p>)
= Jα
</p>
<p>1
</p>
<p>3!ǫ
α
μνρdx
</p>
<p>μ &and; dxν &and; dxρ
</p>
<p>= J0dx &and; dy &and; dz+ Jxdt &and; dy &and; dz+ Jydt &and; dz&and; dx
+ Jzdt &and; dx &and; dy
</p>
<p>= ρdx &and; dy &and; dz&minus; J xdt &and; dy &and; dz&minus; J ydt &and; dz&and; dx
&minus; J zdt &and; dx &and; dy, (28.46)
</p>
<p>where we have used the facts that ρ = J 0 = J0 and J = (J x, J y, J z) =
&minus;(Jx, Jy, Jz).
</p>
<p>Comparing Eqs. (28.44), (28.45), and (28.46), we note that
</p>
<p>Box 28.5.14 In the language of forms, the inhomogeneous pair of
Maxwell&rsquo;s equations has the simple appearance d(&lowast;F)= 4π(&lowast;J).
</p>
<p>Problem 28.15 shows that the relation d2ωωω= 0 is equivalent&mdash;at least in
R3&mdash;to the vanishing of the curl of the gradient and the divergence of the
curl. It is customary in physics to try to go backwards as well, that is, given
that &nabla;&nabla;&nabla; &times; E = 0, to assume that E =&nabla;&nabla;&nabla;f for some function f . Similarly, we
want to believe that &nabla;&nabla;&nabla; &middot; B = 0 implies that B =&nabla;&nabla;&nabla; &times; A.
</p>
<p>What is the analogue of the above statement for a general p-form? Aclosed and exact forms
form ωωω that satisfies dωωω = 0 is called a closed form. An exact form is
one that can be written as the exterior derivative of another form. Thus,
every exact form is automatically closed. This is the Poincar&eacute; lemma. The
converse of this lemma is true only if the region of definition of the form is
topologically simple, as explained in the following.
</p>
<p>Consider a p-formωωω defined on a region U of a manifold M . If all closedregions that are
contractable to a point curves in U can be shrunk to a point in U without encountering any points
</p>
<p>at whichωωω is ill-defined, we say that U is contractable to a point. Ifωωω is not</p>
<p/>
</div>
<div class="page"><p/>
<p>28.5 Exterior Calculus 895
</p>
<p>defined for a point P on M , then any U that contains P is not contractable to
a point. We can now state the converse of the Poincar&eacute; lemma (for a proof,
see [Bish 80, p. 175]):
</p>
<p>Theorem 28.5.15 (Converse of the Poincar&eacute; lemma) Let U be a region in converse of the Poincar&eacute;
lemmaa manifold M such that U is contractable to a point. Let ωωω be a p-form
</p>
<p>on U such that dωωω = 0. Then there exists a (p &minus; 1)-form ηηη on U such that
ωωω= dηηη.
</p>
<p>Example 28.5.16 The electromagnetic field tensor F= 12Fαβdxα &and; dxβ is
a two-form that satisfies dF= 0. The converse of the Poincar&eacute; lemma says
that if F is well behaved in a region U of R4, then there must exist a one-
form ηηη such that F= dηηη.
</p>
<p>Let us write this one-form in terms of coordinates as ηηη = Aαdxα . Then
dηηη=Aα,βdxβ &and; dxα , and we have
</p>
<p>1
</p>
<p>2
Fαβdx
</p>
<p>α &and; dxβ =Aβ,αdxα &and; dxβ
</p>
<p>&rArr; 1
2
(Fαβ &minus;Aβ,α +Aα,β)dxα &and; dxβ = 0.
</p>
<p>Since dxα&and;dxβ are linearly independent and their coefficients are antisym-
metric, each of the latter must vanish. Thus,
</p>
<p>Fαβ =Aβ,α &minus;Aα,β =
&part;Aβ
</p>
<p>&part;xα
&minus; &part;Aα
</p>
<p>&part;xβ
.
</p>
<p>The four-vector Aα is simply the four-potential of relativistic electromag-
netic theory.
</p>
<p>Note that the (p &minus; 1)-form of Theorem 28.5.15 is not unique. In fact, if gauge invariance in the
language of formsααα is any (p&minus; 2)-form, then ωωω can be written as
</p>
<p>ωωω= d(ηηη+ dααα)
</p>
<p>because d(dααα) is identical to zero. This freedom of choice in selecting ηηη is
called gauge invariance, and its generalization plays an important role in
the physics of fundamental interactions.10
</p>
<p>Historical Notes
</p>
<p>Jules Henri Poincar&eacute; (1854&ndash;1912): The development of mathematics in the nineteenth
</p>
<p>Jules Henri Poincar&eacute;
</p>
<p>1854&ndash;1912
</p>
<p>century began under the shadow of a giant, Carl Friedrich Gauss; it ended with the dom-
ination by a genius of similar magnitude, Henri Poincar&eacute;. Both were universal mathe-
maticians in the supreme sense, and both made important contributions to astronomy and
mathematical physics. If Poincar&eacute;&rsquo;s discoveries in number theory do not equal those of
Gauss, his achievements in the theory of functions are at least on the same level&mdash;even
when one takes into account the theory of elliptic and modular functions, which must
be credited to Gauss and which represents in that field his most important discovery, al-
though it was not published during his lifetime. If Gauss was the initiator in the theory
</p>
<p>10Gauge invariance and gauge theories are discussed in detail in Chap. 35.</p>
<p/>
</div>
<div class="page"><p/>
<p>896 28 Analysis of Tensors
</p>
<p>of differentiable manifolds, Poincar&eacute; played the same role in algebraic topology. Finally,
Poincar&eacute; remains the most important figure in the theory of differential equations and the
mathematician who after Newton did the most remarkable work in celestial mechanics.
Both Gauss and Poincar&eacute; had very few students and liked to work alone; but the similarity
ends there. Where Gauss was very reluctant to publish his discoveries, Poincar&eacute;&rsquo;s list of
papers approaches five hundred, which does not include the many books and lecture notes
he published as a result of his teaching at the Sorbonne.
Poincar&eacute;&rsquo;s parents both belonged to the upper middle class, and both their families had
lived in Lorraine for several generations. His paternal grandfather had two sons: L&eacute;on,
Henri&rsquo;s father, was a physician and a professor of medicine at the University of Nancy;
Antoine had studied at the &Eacute;cole Polytechnique and rose to high rank in the engineer-
ing corps. One of Antoine&rsquo;s sons, Raymond, was several times prime minister and was
president of the French Republic during World War I; the other son, Lucien, occupied
high administrative functions in the university. Poincar&eacute;&rsquo;s mathematical ability became
apparent while he was still a student in the lyc&eacute;e. He won first prizes in the concours
g&eacute;n&eacute;al (a competition among students from all French lyc&eacute;es) and in 1873 entered the
&Eacute;cole Polytechnique at the top of his class; his professor at Nancy is said to have re-
ferred to him as a &ldquo;monster of mathematics.&rdquo; After graduation, he followed courses in
engineering at the &Eacute;cole des Mines and worked briefly as an engineer while writing his
thesis for the doctorate in mathematics which he obtained in 1879. Shortly afterward he
started teaching at the University of Caen, and in 1881 he became a professor at the Uni-
versity of Paris, where he taught until his untimely death in 1912. At the early age of
thirty-three he was elected to the Acad&eacute;mie des Sciences and in 1908 to the Acad&eacute;mie
Fran&ccedil;aise. He was also the recipient of innumerable prizes and honors both in France and
abroad.
Before he was thirty years of age, Poincar&eacute; became world famous with his epoch-making
discovery of the &ldquo;automorphic functions&rdquo; of one complex variable (or, as he called them,
the &ldquo;fuchsian&rdquo; and &ldquo;kleinean&rdquo; functions). Much has been written on the &ldquo;competition&rdquo;
between C.F. Klein and Poincar&eacute; in the discovery of automorphic functions. However,
Poincar&eacute;&rsquo;s ignorance of the mathematical literature when he started his researches is al-
most unbelievable. He hardly knew anything on the subject beyond Hermite&rsquo;s work on the
modular functions; he certainly had never read Riemann, and by his own account had not
even heard of the &ldquo;Dirichlet principle,&rdquo; which he was to use in such imaginative fashion
a few years later. Nevertheless, Poincar&eacute;&rsquo;s idea of associating a fundamental domain to
any fuchsian group does not seem to have occurred to Klein, nor did the idea of &ldquo;using&rdquo;
non-Euclidean geometry, which is never mentioned in his papers on modular functions
up to 1880.
Poincar&eacute; was one of the few mathematicians of his time who understood and admired
the work of Lie and his continuators on &ldquo;continuous groups,&rdquo; and in particular the only
mathematician who in the early 1900s realized the depth and scope of E. Cartan&rsquo;s papers.
In 1899 Poincar&eacute; proved what is now called the Poincar&eacute;&ndash;Birkhoff&ndash;Witt theorem which
has become fundamental in the modern theory of Lie algebras. The theory of differential
equations and its applications to dynamics was clearly at the center of Poincar&eacute;&rsquo;s math-
ematical thought; from his first (1878) to his last (1912) paper, he attacked the theory
from all possible angles and very seldom let a year pass without publishing a paper on the
subject. The most extraordinary production of Poincar&eacute;&rsquo;s, also dating from his prodigious
period of creativity (1880&ndash;1883) (reminding us of Gauss&rsquo;s Tagebuch of 1797&ndash;1801), is
the qualitative theory of differential equations. It is one of the few examples of a mathe-
matical theory that sprang apparently out of nowhere and that almost immediately reached
perfection in the hands of its creator. Everything was new in the first two of the four big
papers that Poincar&eacute; published on the subject between 1880 and 1886.
For more than twenty years Poincar&eacute; lectured at the Sorbonne on mathematical physics;
he gave himself to that task with his characteristic thoroughness and energy, with the re-
sult that he became an expert in practically all parts of theoretical physics, and published
more than seventy papers and books on the most varied subjects, with a predilection for
the theories of light and of electromagnetic waves. On two occasions he played an impor-
tant part in the development of the new ideas and discoveries that revolutionized physics
at the end of the nineteenth century. His remark on the possible connection between X-
rays and the phenomenon of phosphorescence was the starting point of H. Becquerel&rsquo;s
experiments that led him to the discovery of radioactivity. On the other hand, Poincar&eacute;</p>
<p/>
</div>
<div class="page"><p/>
<p>28.6 Integration on Manifolds 897
</p>
<p>was active from 1899 on in the discussions concerning Lorentz&rsquo;s theory of the electron;
Poincar&eacute; was the first to observe that the Lorentz transformations form a group; and many
physicists consider that Poincar&eacute; shares with Lorentz and Einstein the credit for the in-
vention of the special theory of relativity. The main leitmotiv of Poincar&eacute;&rsquo;s mathematical
work is clearly the idea of &ldquo;continuity&rdquo;: Whenever he attacks a problem in analysis, we
almost immediately see him investigating what happens when the conditions of the prob-
lem are allowed to vary continuously. He was therefore bound to encounter at every turn
what we now call topological problems. He himself said in 1901, &ldquo;Every problem I had
attacked led me to Analysis situs,&rdquo; particularly the researches on differential equations
and on the periods of multiple integrals. Starting in 1894 he inaugurated in a remark-
able series of six papers&mdash;written during a period of ten years&mdash;the modern methods of
algebraic topology.
Whereas Poincar&eacute; has been accused of being too conservative in physics, he certainly
was very open-minded regarding new mathematical ideas. The quotations in his papers
show that he read extensively, if not systematically, and was aware of all the latest de-
velopments in practically every branch of mathematics. He was probably the first mathe-
matician to use Cantor&rsquo;s theory of sets in analysis. Up to a certain point, he also looked
with favor on the axiomatic trend in mathematics, as it was developing toward the end
of the nineteenth century, and he praised Hilbert&rsquo;s Grundlagen der Geometrie. However,
he obviously had a blind spot regarding the formalization of mathematics, and poked fun
repeatedly at the efforts of the disciples of Peano and Russell in that direction; but, some-
what paradoxically, his criticism of the early attempts of Hilbert was probably the starting
point of some of the most fruitful of the later developments of metamathematics. Poincar&eacute;
stressed that Hilbert&rsquo;s point of view of defining objects by a system of axioms was admis-
sible only if one could prove a priori that such a system did not imply contradiction, and
it is well known that the proof of noncontradiction was the main goal of the theory that
Hilbert founded after 1920. Poincar&eacute; seems to have been convinced that such attempts
were hopeless, and K. G&ouml;del&rsquo;s theorem proved him right.
</p>
<p>28.6 Integration onManifolds
</p>
<p>We mentioned in Chap. 26 that certain exterior products are interpreted as
volume elements. We now exploit this notion and define integration on man- integration of differential
</p>
<p>forms in Rnifolds. Starting with Rn, considered as a manifold, we define the integral of
an n-form ω as follows. Choose a coordinate system {xi}ni=1 in Rn, write
ω= f dx1 &and; &middot; &middot; &middot; &and; dxn, and define the integral of the n-form as
</p>
<p>&int;
</p>
<p>Rn,x
</p>
<p>ω&equiv;
&int;
</p>
<p>Rn
f
(
x1, . . . , xn
</p>
<p>)
dx1 . . . dxn,
</p>
<p>where to avoid dealing with infinities, one assumes that f vanishes outside
a bounded region. The second symbol in the lower part of the integral sign
indicates the variables of integration. Let us now change the coordinates,
say to {yj }nj=1. Using Eq. (28.17), which gives the transformation rule for
1-forms when changing coordinates, and Eq. (2.32), which defines the de-
terminant in terms of n-forms, we obtain
</p>
<p>ω= f dx1 &and; &middot; &middot; &middot; &and; dxn = f det
(
&part;xi
</p>
<p>&part;yj
</p>
<p>)
dy1 &and; &middot; &middot; &middot; &and; dyn,
</p>
<p>where f is now understood to be a function of the y&rsquo;s through the x&rsquo;s. So,
in terms of the new coordinates, the integral becomes</p>
<p/>
</div>
<div class="page"><p/>
<p>898 28 Analysis of Tensors
</p>
<p>&int;
</p>
<p>Rn,y
</p>
<p>ω=
&int;
</p>
<p>Rn
f
(
x1(y), . . . , xn(y)
</p>
<p>)
det
</p>
<p>(
&part;xi
</p>
<p>&part;yj
</p>
<p>)
dy1 &and; &middot; &middot; &middot; &and; dyn.
</p>
<p>If we had the absolute value of the Jacobian in the integral, the two sides
would be equal. So, all we can say at this point is
</p>
<p>&int;
</p>
<p>Rn,y
</p>
<p>ω=&plusmn;
&int;
</p>
<p>Rn,x
</p>
<p>ω.
</p>
<p>We therefore distinguish between two kinds of coordinate transformations:This discussion is
analogous to our
</p>
<p>discussion of orientation
</p>
<p>in vector spaces (see
</p>
<p>Sect. 26.3.1).
</p>
<p>If the Jacobian determinant is positive, we say that the coordinate transfor-
mation is orientation preserving. Otherwise, the transformation is called
orientation reversing.
</p>
<p>Our ability to integrate functions on Rn depends crucially on the fact that
volume elements do not change sign at any point of Rn. If this were not so,
we could find a finite (albeit small) region of space&mdash;in the vicinity of the
point at which the volume element changes sign&mdash;whose volume would be
zero. This property of Rn is the content of the following:
</p>
<p>Definition 28.6.1 A manifold M of dimension n is called orientable if itorientable manifolds
has a nowhere vanishing n-form.
</p>
<p>Any two nonvanishing n-forms ω and ω&prime; on an orientable manifold are
related by a nowhere-vanishing function: ω&prime; = hω. Clearly, h has to be either
positive or negative everywhere. ω and ω&prime; are said to be equivalent if h is
positive. Thus, the nonvanishing n-forms on an orientable manifold fall into
two classes, all members of each class being equivalent to one another, and
a member of one class being related to a member of the other class via a
negative function. Each class is called an orientation on M .
</p>
<p>Given an orientation, an n-form ω, and a chart {Uα, φα} on M , we define
&int;
</p>
<p>M
</p>
<p>ω&equiv;
&sum;
</p>
<p>α
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>(
φ&minus;1α
</p>
<p>)&lowast;
(ω|α), (28.47)
</p>
<p>where (φ&minus;1α )
&lowast; is the pullback of φ&minus;1α :Rn &rarr;M , so that it maps n-forms on
</p>
<p>M to n-forms on Rn; ω|α is the restriction of ω to Uα , and the sum over α
is assumed to exist. This amounts to saying that the region in M on which
ω is defined is finite, or that ω has compact support.Compact support
</p>
<p>We note that the RHS of Eq. (28.47) is an integration on Rn that appears
to depend on the choice of coordinate functions. However, it can be shown
that the integral is independent of such choice. In practice, one chooses a
coordinate patch and transfers the integration to Rn, where the process is
familiar.
</p>
<p>If we choose a coordinate patch {xi}ni=1 and integrate dx1 &and; &middot; &middot; &middot; &and; dxn
according to Eq. (28.47), we obtain the &ldquo;volume&rdquo; of the manifold M . If M
is compact, this volume will be finite.11
</p>
<p>&ldquo;volume&rdquo; of a manifold
</p>
<p>11Recall from Chap. 17 that a subset of Rn is compact iff it is closed and bounded. It is a
good idea to keep this in mind as a paradigm of compact spaces.</p>
<p/>
</div>
<div class="page"><p/>
<p>28.6 Integration on Manifolds 899
</p>
<p>Theorem 28.6.2 (Stokes&rsquo; theorem) Let M be an oriented n-manifold. Let Stokes&rsquo; Theorem
ωωω be an (n&minus; 1)-form with compact support. Then
</p>
<p>&int;
</p>
<p>M
</p>
<p>dωωω= 0
</p>
<p>Proof From Eq. (28.47), we have
&int;
</p>
<p>M
</p>
<p>dωωω=
&sum;
</p>
<p>α
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>(
φ&minus;1α
</p>
<p>)&lowast;
(dωωω|α)=
</p>
<p>&sum;
</p>
<p>α
</p>
<p>&int;
</p>
<p>Rn
d
((
φ&minus;1α
</p>
<p>)&lowast;
ωωω|α
</p>
<p>)
</p>
<p>where in the last equality, we used item 5 of Theorem 28.5.3. Now
(φ&minus;1α )
</p>
<p>&lowast;ωωω|α is an (n &minus; 1)-from on Rn. If βββ is such a form, it can be writ-
ten as
</p>
<p>βββ = βidx1 &and; &middot; &middot; &middot; &and; dx̂i &and; &middot; &middot; &middot; &and; dxn
</p>
<p>and dβββ =&sum;ni=1(&minus;1)i&minus;1&part;iβidx1 &and; &middot; &middot; &middot; &and; dxn. Therefore,
&int;
</p>
<p>Rn
dβββ =
</p>
<p>n&sum;
</p>
<p>i=1
</p>
<p>&int;
</p>
<p>Rn
(&minus;1)i&minus;1&part;iβidx1 &and; &middot; &middot; &middot; &and; dxn
</p>
<p>&equiv;
n&sum;
</p>
<p>i=1
(&minus;1)i&minus;1
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>&part;βi
</p>
<p>&part;xi
dx1 . . . dxn
</p>
<p>=
n&sum;
</p>
<p>i=1
(&minus;1)i&minus;1
</p>
<p>&int;
</p>
<p>Rn&minus;1
</p>
<p>(&int;
</p>
<p>R
</p>
<p>&part;βi
</p>
<p>&part;xi
dxi
</p>
<p>)
dx1 . . . dx̂i . . . dxn
</p>
<p>=
n&sum;
</p>
<p>i=1
(&minus;1)i&minus;1
</p>
<p>&int;
</p>
<p>Rn&minus;1
</p>
<p>(
βi
∣∣xi=&infin;
xi=&minus;&infin;
</p>
<p>)
dx1 . . . dx̂i . . . dxn.
</p>
<p>The term in parentheses is zero because βββ has compact support and all its
components must vanish at infinity. �
</p>
<p>A manifold may have a boundary &part;M , which is an (n&minus; 1)-dimensional
submanifold of M , every point of which has a coordinate neighborhood
in which one of the coordinates is zero. For example, the xy-plane is the
boundary of the lower space on which z= 0. As another example, consider
an open set U of an n-manifold M . Then U is also an n-manifold, and its
boundary &part;U is an (n&minus;1)-dimensional manifold. If M (and therefore, U ) is
oriented, then &part;U inherits an orientation from U . There is another version of
the Stokes&rsquo; Theorem for manifolds with boundary, which we state without
proof.
</p>
<p>Stokes&rsquo; Theorem for
</p>
<p>manifolds with
</p>
<p>boundary
</p>
<p>Theorem 28.6.3 Let U be an oriented n-manifold with boundary
&part;U . Let ωωω be an (n&minus; 1)-form with compact support. Then
</p>
<p>&int;
</p>
<p>U
</p>
<p>dωωω=
&int;
</p>
<p>&part;U
</p>
<p>ωωω</p>
<p/>
</div>
<div class="page"><p/>
<p>900 28 Analysis of Tensors
</p>
<p>Combining the exterior differential with the Hodge star operator, we get
a useful quantity.
</p>
<p>Definition 28.6.4 The codifferential δ is a map δ : Λp(M) &rarr; Λp&minus;1(M)
given by
</p>
<p>δωωω= (&minus;1)ν+1(&minus;1)n(p+1) &lowast; d &lowast;ωωω
</p>
<p>where n is the dimension of M . If ωωω is a 0-form, i.e., a function f , then the
Codifferential defined
</p>
<p>definition leads to δf = 0. Furthermore, since &lowast;&lowast; =&plusmn;1, δ2 &equiv; 0.
</p>
<p>If M has a metric, i.e., a nondegenerate symmetric bilinear form g de-
fined smoothly on each point of M , and g does not vary over M , then we
have the following:
</p>
<p>Proposition 28.6.5 If ωωω = 1
p!ωi1...ipdx
</p>
<p>i1 &and; &middot; &middot; &middot; &and; dxip is a p-form on an n-
manifold M with constant metric g, then
</p>
<p>δωωω= (&minus;1)
p
</p>
<p>(p&minus; 1)!&part;iω
i
</p>
<p>i1...ip&minus;1 dx
i1 &and; &middot; &middot; &middot; &and; dxip&minus;1
</p>
<p>where &part;iω ii1...ip&minus;1 = g
iip&part;iωi1...ip&minus;1ip = &part;i(giipωi1...ip&minus;1ip ).
</p>
<p>Proof Start with the definition of δ and the Hodge star operator:
</p>
<p>δωωω= (&minus;1)ν+1(&minus;1)n(p+1) &lowast; d &lowast;ωωω
</p>
<p>= (&minus;1)
ν+1(&minus;1)n(p+1)
p!(n&minus; p)! &lowast; d
</p>
<p>(
ωi1...ipǫi1...indx
</p>
<p>ip+1 &and; &middot; &middot; &middot; &and; dxin
)
.
</p>
<p>Now note that d differentiates only the function ωi1...ip because d2 = 0.
Differentiating and applying &lowast; afterwards, we get
</p>
<p>δωωω= (&minus;1)
ν+1(&minus;1)n(p+1)
p!(n&minus; p)! ǫi1...in&part;iω
</p>
<p>i1...ip &lowast;
(
dxi &and; dxip+1 &and; &middot; &middot; &middot; &and; dxin
</p>
<p>)
</p>
<p>= (&minus;1)
ν+1(&minus;1)n(p+1)
p!(n&minus; p)! ǫi1...in&part;iω
</p>
<p>i1...ip
</p>
<p>&times; 1
(p&minus; 1)!g
</p>
<p>ijpgip+1jp+1 . . . ginjnǫjp ...jnj1...jp&minus;1dx
j1 &and; &middot; &middot; &middot; &and; dxjp&minus;1 .
</p>
<p>Rearranging the indices of the ǫ,
</p>
<p>ǫjp ...jnj1...jp&minus;1 = (&minus;1)(n&minus;p+1)(p&minus;1)ǫj1...jn ,
</p>
<p>manipulating the powers of &minus;1, noting that AkBk = AkBk , and using the
fact that glm are constant,12 we rewrite the last expression as
</p>
<p>12Reader, see where this fact is used!</p>
<p/>
</div>
<div class="page"><p/>
<p>28.7 Symplectic Geometry 901
</p>
<p>δωωω= (&minus;1)
ν+1(&minus;1)(p&minus;1)2
</p>
<p>p!(n&minus; p)!(p&minus; 1)! ǫ
i1...inǫj1...jn&part;iωi1...ipg
</p>
<p>ijpg
jp+1
ip+1 . . . g
</p>
<p>jn
in
dxj1 &and; &middot; &middot; &middot;
</p>
<p>&and; dxjp&minus;1
</p>
<p>=&minus; (&minus;1)
p&minus;1
</p>
<p>p!(n&minus; p)!(p&minus; 1)!δ
i1...in
j1...jn
</p>
<p>&part;iωi1...ipg
ijpδ
</p>
<p>jp+1
ip+1 . . . δ
</p>
<p>jn
in
dxj1 &and; &middot; &middot; &middot;
</p>
<p>&and; dxjp&minus;1,
</p>
<p>where we used Eq. (26.44), the fact that (&minus;1)m2 = (&minus;1)m for any integer m
and that gij = δij . The last expression is now reduced to
</p>
<p>δωωω=&minus; (&minus;1)
p&minus;1
</p>
<p>p!(n&minus; p)!(p&minus; 1)!δ
i1...ipip+1...in
j1...jpip+1...in&part;iωi1...ipg
</p>
<p>ijpdxj1 &and; &middot; &middot; &middot; &and; dxjp&minus;1
</p>
<p>=&minus; (&minus;1)
p&minus;1
</p>
<p>p!(p&minus; 1)!δ
i1...ip
j1...jp
</p>
<p>&part;iωi1...ipg
ijpdxj1 &and; &middot; &middot; &middot; &and; dxjp&minus;1
</p>
<p>=&minus; (&minus;1)
p&minus;1
</p>
<p>(p&minus; 1)! &part;iωj1...jpg
ijpdxj1 &and; &middot; &middot; &middot; &and; dxjp&minus;1,
</p>
<p>where we used Eq. (26.47) in the first equality and Eq. (26.50) in the last. �
</p>
<p>If M has a metric, then a metric g̃ can be defined on Λp(M) in exact
analogy with Definition 26.3.13, and we have the following:
</p>
<p>Theorem 28.6.6 Let M be an oriented n-manifold with volume element μμμ
and metric g. Let ααα &isin; Λp(M) and βββ &isin; Λp+1(M) such that ααα &and; &lowast;βββ has
compact support. Then
</p>
<p>&int;
</p>
<p>M
</p>
<p>g̃(ααα, δβββ)μμμ=
&int;
</p>
<p>M
</p>
<p>g̃(dααα,βββ)μμμ
</p>
<p>Proof From Theorem 26.6.4, we have
</p>
<p>g̃(ααα, δβββ)μμμ=ααα &and; &lowast;δβββ =ααα &and; &lowast;
(
(&minus;1)ν+1(&minus;1)n(p+2) &lowast; d &lowast;βββ
</p>
<p>)
</p>
<p>= (&minus;1)ν+1(&minus;1)n(p+2)(&minus;1)ν(&minus;1)p(n&minus;p)ααα &and; d &lowast;βββ
=&minus;(&minus;1)pααα &and; d &lowast;βββ
</p>
<p>because (&minus;1)p2 = (&minus;1)&minus;p2 = (&minus;1)p for any integer p. Hence,
</p>
<p>g̃(dααα,βββ)μμμ&minus; g̃(ααα, δβββ)μμμ= dααα &and; &lowast;β + (&minus;1)pααα &and; d &lowast;βββ = d(ααα &and; &lowast;β)
</p>
<p>and the integral over M of the right-hand side is zero by Stokes&rsquo; Theorem. �
</p>
<p>28.7 Symplectic Geometry
</p>
<p>Mechanics stimulated a great deal of dialogue between physics and mathe-
matics in the latter part of the nineteenth century and the beginning of the
twentieth. The branch of mathematics that benefited the most out of this</p>
<p/>
</div>
<div class="page"><p/>
<p>902 28 Analysis of Tensors
</p>
<p>dialog is the theory of differentiable manifolds, whose tribute back to me-
chanics has been the most beautiful language in which the latter can express
itself, the language of symplectic geometry. All the discussion of symplectic
vector spaces of the last chapter can be carried over to the tangent spaces of
a manifold and patched together by the differentiable structure of the mani-
fold.
</p>
<p>Definition 28.7.1 A symplectic form (or a symplectic structure) on asymplectic form,
symplectic structure,
</p>
<p>and symplectic manifold
</p>
<p>defined
</p>
<p>manifold M is a nondegenerate, closed 2-formωωω on M . A symplectic man-
ifold (M,ωωω) is a manifold M together with a symplectic form ωωω on M . We
define the map ♭ :X(M)&rarr;X&lowast;(M) by
</p>
<p>♭(X)&equiv; X♭ = iXωωω=ωωω♭(X)
</p>
<p>and the map ♯ :X&lowast;(M)&rarr;X(M) as the inverse of ♭.
</p>
<p>Chapter 26 identified some special basis, the canonical basis, in which the
symplectic form of a symplectic vector space took on a simple expression.
The analogue of such a basis exists in a symplectic manifold. The reader
should keep in mind that this existence is not automatic, because although
one can find such bases at every point of the manifold, the smooth patching
up of all such bases to cover the entire manifold is not trivial and is the
content of the following important theorem, which we state without proof
(see [Abra 85, p. 175]):
</p>
<p>Theorem 28.7.2 (Darboux) Suppose ωωω is a 2-form on a 2n-dimensionalDarboux theorem
manifold M . Then dωωω = 0 if and only if there is a chart (U,ϕ) at each
P &isin;M such that ϕ(P )= 0 and
</p>
<p>ωωω=
n&sum;
</p>
<p>i=1
dxi &and; dyi,
</p>
<p>where x1, . . . , xn, y1, . . . , yn are coordinates on U . Furthermore, on such a
chart, the volume element μω is
</p>
<p>μω = dx1 &and; &middot; &middot; &middot; &and; dxn &and; dy1 &and; &middot; &middot; &middot; &and; dyn.
</p>
<p>Definition 28.7.3 The charts guaranteed by Darboux&rsquo;s theorem are called
symplectic charts, and the coordinates xi, yi are called canonical coordi-symplectic charts,
</p>
<p>canonical coordinates,
</p>
<p>and canonical
</p>
<p>transformations
</p>
<p>nates. If (M,ωωω) and (N,ρρρ) are symplectic manifolds, then a C&infin; map f :
M &rarr;N is called symplectic, or a canonical transformation, if f &lowast;ρρρ =ωωω.
</p>
<p>Example 28.7.4 In this example, we derive a formula that gives the action
of ωωω♭ and ωωω♯ in terms of components of vectors and 1-forms in canonical
coordinates. LetCoordinate
</p>
<p>representation of sharp
</p>
<p>and flat maps Z &equiv;Xi
&part;
</p>
<p>&part;xi
+ Y i &part;
</p>
<p>&part;yi</p>
<p/>
</div>
<div class="page"><p/>
<p>28.7 Symplectic Geometry 903
</p>
<p>be a vector field. When ωωω♭ acts on Z, it gives a 1-form, which we write as
ωωω♭(Z) &equiv; Ukdxk +Wkdyk . To find the unknowns Uk and Wk , we let both
sides act on coordinate basis vectors. For the RHS, we get
</p>
<p>(
Ukdx
</p>
<p>k +Wkdyk
)( &part;
</p>
<p>&part;xj
</p>
<p>)
=Uk dxk
</p>
<p>(
&part;
</p>
<p>&part;xj
</p>
<p>)
</p>
<p>︸ ︷︷ ︸
=δkj
</p>
<p>+Wk dyk
(
</p>
<p>&part;
</p>
<p>&part;xj
</p>
<p>)
</p>
<p>︸ ︷︷ ︸
=0
</p>
<p>=Uj
</p>
<p>and
</p>
<p>(
Ukdx
</p>
<p>k +Wkdyk
)( &part;
</p>
<p>&part;yj
</p>
<p>)
=Wj .
</p>
<p>For the LHS, we obtain
</p>
<p>[
ωωω♭
</p>
<p>(
Xi
</p>
<p>&part;
</p>
<p>&part;xi
+ Y i &part;
</p>
<p>&part;yi
</p>
<p>)](
&part;
</p>
<p>&part;xj
</p>
<p>)
</p>
<p>=Xi
[
ωωω♭
</p>
<p>(
&part;
</p>
<p>&part;xi
</p>
<p>)](
&part;
</p>
<p>&part;xj
</p>
<p>)
+ Y i
</p>
<p>[
ωωω♭
</p>
<p>(
&part;
</p>
<p>&part;yi
</p>
<p>)](
&part;
</p>
<p>&part;xj
</p>
<p>)
</p>
<p>=Xiωωω
(
</p>
<p>&part;
</p>
<p>&part;xi
,
</p>
<p>&part;
</p>
<p>&part;xj
</p>
<p>)
+ Y iωωω
</p>
<p>(
&part;
</p>
<p>&part;yi
,
</p>
<p>&part;
</p>
<p>&part;xj
</p>
<p>)
.
</p>
<p>But
</p>
<p>ωωω
</p>
<p>(
&part;
</p>
<p>&part;xi
,
</p>
<p>&part;
</p>
<p>&part;xj
</p>
<p>)
=
(
</p>
<p>n&sum;
</p>
<p>k=1
dxk &and; dyk
</p>
<p>)(
&part;
</p>
<p>&part;xi
,
</p>
<p>&part;
</p>
<p>&part;xj
</p>
<p>)
</p>
<p>=
n&sum;
</p>
<p>k=1
dxk
</p>
<p>(
&part;
</p>
<p>&part;xi
</p>
<p>)
dyk
</p>
<p>(
&part;
</p>
<p>&part;xj
</p>
<p>)
</p>
<p>︸ ︷︷ ︸
=0
</p>
<p>&minus;
n&sum;
</p>
<p>k=1
dxk
</p>
<p>(
&part;
</p>
<p>&part;xj
</p>
<p>)
dyk
</p>
<p>(
&part;
</p>
<p>&part;xi
</p>
<p>)
</p>
<p>︸ ︷︷ ︸
=0
</p>
<p>= 0
</p>
<p>and
</p>
<p>ωωω
</p>
<p>(
&part;
</p>
<p>&part;yi
,
</p>
<p>&part;
</p>
<p>&part;xj
</p>
<p>)
=
(
</p>
<p>n&sum;
</p>
<p>k=1
dxk &and; dyk
</p>
<p>)(
&part;
</p>
<p>&part;yi
,
</p>
<p>&part;
</p>
<p>&part;xj
</p>
<p>)
</p>
<p>=
n&sum;
</p>
<p>k=1
dxk
</p>
<p>(
&part;
</p>
<p>&part;yi
</p>
<p>)
</p>
<p>︸ ︷︷ ︸
=0
</p>
<p>dyk
(
</p>
<p>&part;
</p>
<p>&part;xj
</p>
<p>)
</p>
<p>︸ ︷︷ ︸
=0
</p>
<p>&minus;
n&sum;
</p>
<p>k=1
dxk
</p>
<p>(
&part;
</p>
<p>&part;xj
</p>
<p>)
</p>
<p>︸ ︷︷ ︸
=δkj
</p>
<p>dyk
(
</p>
<p>&part;
</p>
<p>&part;yi
</p>
<p>)
</p>
<p>︸ ︷︷ ︸
=δki
</p>
<p>=&minus;δij .</p>
<p/>
</div>
<div class="page"><p/>
<p>904 28 Analysis of Tensors
</p>
<p>It follows that
[
ωωω♭
</p>
<p>(
Xi
</p>
<p>&part;
</p>
<p>&part;xi
+ Y i &part;
</p>
<p>&part;yi
</p>
<p>)](
&part;
</p>
<p>&part;xj
</p>
<p>)
=&minus;Y j .
</p>
<p>Similarly,
[
ωωω♭
</p>
<p>(
Xi
</p>
<p>&part;
</p>
<p>&part;xi
+ Y i &part;
</p>
<p>&part;yi
</p>
<p>)](
&part;
</p>
<p>&part;yj
</p>
<p>)
=Xj .
</p>
<p>Therefore,
</p>
<p>ωωω♭
(
Xi
</p>
<p>&part;
</p>
<p>&part;xi
+ Y i &part;
</p>
<p>&part;yi
</p>
<p>)
=&minus;Y jdxj +Xjdyj , (28.48)
</p>
<p>where a summation over repeated indices is understood.
If we multiply both sides of this equation by ωωω♯ on the left and recall that
</p>
<p>ωωω♯ωωω♭ = 1, we obtain the following equation for the action of ωωω♯:
</p>
<p>ωωω♯
(
Xjdxj + Y jdyj
</p>
<p>)
= Y i &part;
</p>
<p>&part;xi
&minus;Xi &part;
</p>
<p>&part;yi
. (28.49)
</p>
<p>Equations (28.48) and (28.49) are very useful in Hamiltonian mechanics.
</p>
<p>Our discussion of symplectic transformations of symplectic vector spaces
showed that such maps are necessarily isomorphisms. Applied to the present
situation, this means that if f :M &rarr; N is symplectic, then f&lowast; : TP (M)&rarr;
Tf (P )(N) is an isomorphism. Theorem 28.3.2, the inverse mapping theo-
rem, now gives the following theorem.
</p>
<p>Theorem 28.7.5 If f :M &rarr; N is symplectic, then it is a local diffeomor-
phism.
</p>
<p>Example 28.7.6 Hamiltonian mechanics takes place in the phase space of
a system. The phase space is derived from the configuration space as fol-
lows. Let (q1, . . . , qn) be the generalized coordinates of a mechanical sys-
tem. They describe an n-dimensional manifold N . The dynamics of the sys-
tem is described by the (time-independent) Lagrangian L, which is a func-
tion of (q i, q̇ i). But q̇ i are the components of a vector at (q1, . . . , qn) [see
Eq. (28.13) and replace γ i with xi ]. Thus, in the language of manifold the-
ory, a Lagrangian is a function on the tangent bundle, L : T (N)&rarr;R.
</p>
<p>The Hamiltonian is obtained from the Lagrangian by a Legendre trans-from Lagrangian to
Hamiltonian in the
</p>
<p>language of differential
</p>
<p>forms
</p>
<p>formation: H =&sum;ni=1 pi q̇ i&minus;L. The first term can be thought of as a pairing
of an element of the tangent space with its dual. In fact, if P has coordinates
(q1, . . . , qn), then q̇ &equiv; q̇ i&part;i &isin; TP (N) (with the Einstein summation conven-
tion enforced), and if we pair this with the dual vector pjdxj &isin; T&lowast;P (N), we
obtain the first term in the definition of the Hamiltonian. The effect of the
Legendre transformation is to replace q̇ i by pi as the second set of indepen-
dent variables. This has the effect of replacing T (N) with T &lowast;(N). Thus</p>
<p/>
</div>
<div class="page"><p/>
<p>28.7 Symplectic Geometry 905
</p>
<p>Box 28.7.7 The manifold of Hamiltonian dynamics, or the phase
space, is T &lowast;(N), with coordinates (q i,pi) on which the Hamiltonian
H : T &lowast;(N)&rarr;R is defined.
</p>
<p>T &lowast;(N) is 2n-dimensional; so it has the potential of becoming a sym-
plectic manifold. In fact, it can be shown that13 the 2-form suggested by
Darboux&rsquo;s theorem, symplectic 2-form of
</p>
<p>T &lowast;(N)
ωωω&equiv;
</p>
<p>n&sum;
</p>
<p>i=1
dq i &and; dpi, (28.50)
</p>
<p>is nondegenerate, and therefore a symplectic form for T &lowast;(N).
</p>
<p>The phase space, equipped with a symplectic form, turns into a geometric
arena in which Hamiltonian mechanics unfolds. We saw in the above exam-
ple that a Hamiltonian is a function on the phase space. More generally, if
(M,ωωω) is a symplectic manifold, a Hamiltonian H is a real-valued function,
H :M &rarr;R. Given a Hamiltonian, one can define a vector field as follows.
Consider dH &isin; T &lowast;(M). For a symplectic manifold, there is a natural iso-
morphism between T &lowast;(M) and T (M), namely, ωωω♯. The unique vector field
XH associated with dH is the vector field we are after.
</p>
<p>Definition 28.7.8 Let (M,ωωω) be a symplectic manifold and H :M &rarr; R a Hamiltonian vector field
and Hamiltonian
</p>
<p>systems defined
</p>
<p>real-valued function. The vector field
</p>
<p>XH &equiv;ωωω♯(dH)&equiv; (dH)♯
</p>
<p>is called the Hamiltonian vector field with energy function H . The triplet
(M,ωωω,XH ) is called a Hamiltonian system.
</p>
<p>The significance of the Hamiltonian vector field lies in its integral curve
which turns out to be the path of evolution of the system in the phase space.
This is shown in the following proposition.
</p>
<p>Proposition 28.7.9 If (q1, . . . , qn,p1, . . . , pn) are canonical coordinates
for ωωω&mdash;so ωωω=&sum;dq i &and; dpi&mdash;then, in these coordinates
</p>
<p>XH =
&part;H
</p>
<p>&part;pi
</p>
<p>&part;
</p>
<p>&part;xi
&minus; &part;H
</p>
<p>&part;q i
</p>
<p>&part;
</p>
<p>&part;pi
&equiv;
(
&part;H
</p>
<p>&part;pi
,&minus;&part;H
</p>
<p>&part;q i
</p>
<p>)
. (28.51)
</p>
<p>13Here, we are assuming that the mechanical system in question is nonsingular, by which
is meant that there are precisely n independent pi &rsquo;s. There are systems of considerable
importance that happen to be singular. Such systems, among which are included all gauge
theories such as the general theory of relativity, are called constrained systems and are
characterized by the fact that ωωω is degenerate. Although of great interest and currently
under intense study, we shall not discuss constrained systems in this book.</p>
<p/>
</div>
<div class="page"><p/>
<p>906 28 Analysis of Tensors
</p>
<p>Therefore, (q(t),p(t)) is an integral curve of XH iff Hamilton&rsquo;s equations
hold:
</p>
<p>&part;q i
</p>
<p>&part;t
= &part;H
</p>
<p>&part;pi
,
</p>
<p>&part;pi
</p>
<p>&part;t
=&minus;&part;H
</p>
<p>&part;q i
, i = 1, . . . , n. (28.52)
</p>
<p>Proof The first part of the proposition follows from
</p>
<p>dH = &part;H
&part;q i
</p>
<p>dq i + &part;H
&part;pi
</p>
<p>dpi,
</p>
<p>from the definition of XH in terms of dH , and from Eq. (28.49). The second
part follows from the definition of integral curve and Eq. (28.19). �
</p>
<p>We called H the energy function; this is for good reason:
</p>
<p>Theorem 28.7.10 Let (M,ωωω,XH ) be a Hamiltonian system and γ (t) anconservation of energy
in the language of
</p>
<p>symplectic geometry
</p>
<p>integral curve of XH . Then H(γ (t)) is constant in t .
</p>
<p>Proof We show that the time-derivative of H(γ (t)) is zero:
</p>
<p>d
</p>
<p>dt
H
(
γ (t)
</p>
<p>)
= γ&lowast;t (H) by Proposition 28.2.4
</p>
<p>= dH(γ&lowast;t ) by Eq. (28.14)
= dH
</p>
<p>(
XH
</p>
<p>(
γ (t)
</p>
<p>))
by definition of integral curve
</p>
<p>=
[
ωωω♭
</p>
<p>(
XH
</p>
<p>(
γ (t)
</p>
<p>))](
XH
</p>
<p>(
γ (t)
</p>
<p>))
by definition of XH
</p>
<p>(
γ (t)
</p>
<p>)
</p>
<p>=ωωω
(
XH
</p>
<p>(
γ (t)
</p>
<p>)
,XH
</p>
<p>(
γ (t)
</p>
<p>))
by the definition of ωωω♭
</p>
<p>= 0 because ωωω is skew-symmetric
</p>
<p>�
</p>
<p>Theorem 28.7.10 is the statement of the conservation of energy.
</p>
<p>Historical Notes
</p>
<p>Sir William Rowan Hamilton (1805&ndash;1865), the fourth of nine children, was mostly
</p>
<p>Sir William Rowan
</p>
<p>Hamilton 1805&ndash;1865
</p>
<p>raised by an uncle, who quickly realized the extraordinary nature of his young nephew.
By the age of five, Hamilton spoke Latin, Greek, and Hebrew, and by the age of nine had
added more than a half dozen languages to that list. He was also quite famous for his skill
at rapid calculation. Hamilton&rsquo;s introduction to mathematics came at the age of 13, when
he studied Clairaut&rsquo;s Algebra, a task made somewhat easier as Hamilton was fluent in
French by this time. At age 15 he started studying Newton, whose Principia spawned an
interest in astronomy that would provide a great influence in Hamilton&rsquo;s early career.
In 1822, at the age of 18, Hamilton entered Trinity College, Dublin, and in his first year
he obtained the top mark in classics. He divided his studies equally between classics and
mathematics and in his second year he received the top award in mathematical physics.
Hamilton discovered an error in Laplace&rsquo;s M&eacute;chanique c&eacute;leste, and as a result, he came
to the attention of John Brinkley, the Astronomer Royal of Ireland, who said: &ldquo;This young
man, I do not say will be, but is, the first mathematician of his age.&rdquo; While in his final
year as an undergraduate, he presented a memoir entitled Theory of Systems of Rays to
the Royal Irish Academy in which he planted the seeds of symplectic geometry.</p>
<p/>
</div>
<div class="page"><p/>
<p>28.7 Symplectic Geometry 907
</p>
<p>Hamilton&rsquo;s personal life was marked at first by despondency. Rejected by a college
friend&rsquo;s sister, he became ill and nearly suicidal. He was rejected a few years later by
another friend&rsquo;s sister and wound up marrying a very timid woman prone to ill health.
Hamilton&rsquo;s own personality was much more energetic and humorous, and he easily ac-
quired friends among the literati. His own attempts at poetry, which he himself fancied,
were generally considered quite poor. No less an authority than Wordsworth attempted
to convince him that his true calling was mathematics, not poetry. Nevertheless, Hamilton
maintained close connection with the worlds of literature and philosophy, insisting that
the ideas to be gleaned from them were integral parts of his life&rsquo;s work. While Hamilton
is best known in physics for his work in dynamics, more of his time was spent on studies
in optics and the theory of quaternions. In optics, he derived a function of the initial and
final coordinates of a ray and termed it the &ldquo;characteristic function,&rdquo; claiming that it con-
tained &ldquo;the whole of mathematical optics.&rdquo; Interestingly, his approach shed no new light
on the wave/corpuscular debate (being independent of which view was taken), another
appearance of Hamilton&rsquo;s quest for ultimate generality.
In 1833 Hamilton published a study of vectors as ordered pairs. He used algebra to study
dynamics in On a General Method in Dynamics in 1834. The theory of quaternions, on
which he spent most of his time, grew from his dissatisfaction with the current state of the
theoretical foundation of algebra. He was aware of the description of complex numbers as
points in a plane and wondered if any other geometrical representation was possible or if
there existed some hypercomplex number that could be represented by three-dimensional
points in space. If the latter supposition were true, it would entail a natural algebraic
representation of ordinary space. To his surprise, Hamilton found that in order to create
a hypercomplex number algebra for which the modulus of a product equaled the product
of the two moduli, four components were required&mdash;hence, quaternions.
Hamilton felt that this discovery would revolutionize mathematical physics, and he spent
the rest of his life working on quaternions, including publication of a book entitled Ele-
ments of Quaternions, which he estimated would be 400 pages long and take two years to
write. The title suggests that Hamilton modeled his work on Euclid&rsquo;s Elements and indeed
this was the case. The book ended up double its intended length and took seven years to
write. In fact, the final chapter was incomplete when Hamilton died, and the book was
finally published with a preface by his son, William Edwin Hamilton. While quaternions
themselves turned out to be of no such monumental importance, their appearance as the
first noncommutative algebra opened the door for much research in this field, including
much of vector and matrix analysis. (As a side note, the &ldquo;del&rdquo; operator, named later by
Gibbs, was introduced by Hamilton in his papers on quaternions.)
In dynamics, Hamilton extended his characteristic function from optics to the classical
action for a system moving between two points in configuration space. A simple trans-
formation of this function gives the quantity (the time integral of the Lagrangian) whose
variation equals zero in what we now call Hamilton&rsquo;s principle. Jacobi later simplified
the application of Hamilton&rsquo;s idea to mechanics, and it is the Hamilton&ndash;Jacobi equation
that is most often used in such problems. Hamiltonian dynamics was rescued from what
could have become historical obscurity with the advent of quantum mechanics, in which
its close association with ideas in optics found fertile application in the wave mechanics
of de Broglie and Schr&ouml;dinger. Hamilton&rsquo;s later life was unhappy, and he became ad-
dicted to alcohol. He died from a severe attack of gout shortly after receiving the news
that he had been elected the first foreign member of the National Academy of Sciences of
the USA.
</p>
<p>In the theoretical development of mechanics, canonical transformations Flow of Hamiltonian
vector field is canonical
</p>
<p>transformation of
</p>
<p>mechanics.
</p>
<p>play a central role. The following proposition shows that the flows of a
Hamiltonian system are such transformations:
</p>
<p>Proposition 28.7.11 Let (M,ωωω,XH ) be a Hamiltonian system, and Ft the
flow of XH . Then for each t , F &lowast;t ωωω=ωωω, i.e., Ft is symplectic.</p>
<p/>
</div>
<div class="page"><p/>
<p>908 28 Analysis of Tensors
</p>
<p>Proof We have
</p>
<p>d
</p>
<p>dt
F &lowast;t ωωω= F &lowast;t LXHωωω by Eq. (28.39)
</p>
<p>= F &lowast;t (iXH dωωω+ diXHωωω) by Theorem 28.5.10
= F &lowast;t (0 + ddH) because dωωω= 0 and iXωωω=ωωω♭(X)
= 0 because d2 = 0.
</p>
<p>Thus, F &lowast;t ωωω is constant in t . But F
&lowast;
0 = id. Therefore, F &lowast;t ωωω=ωωω. �
</p>
<p>The celebrated Liouville&rsquo;s theorem of mechanics, concerning the preser-
vation of volume of the phase space, is a consequence of the proposition
above:
</p>
<p>Corollary 28.7.12 (Liouville&rsquo;s theorem) Ft preserves the phase volumeLiouville&rsquo;s theorem
μω.
</p>
<p>Definition 28.7.13 Let (M,ωωω) be a symplectic manifold. Let f,g :M &rarr;RPoisson brackets in the
language of symplectic
</p>
<p>geometry
</p>
<p>with Xf = (df )♯ and Xg = (dg)♯ their corresponding Hamiltonian vector
fields. The Poisson bracket of f and g is the function
</p>
<p>{f,g} &equiv;ωωω(Xf ,Xg)= iXg iXfωωω=&minus;iXf iXgωωω.
</p>
<p>We can immediately obtain the familiar expression for the Poisson
bracket of two functions.
</p>
<p>Proposition 28.7.14 In canonical coordinates (q i, . . . , qn,p1, . . . , pn), we
have
</p>
<p>{f,g} =
n&sum;
</p>
<p>i=1
</p>
<p>(
&part;f
</p>
<p>&part;q i
</p>
<p>&part;g
</p>
<p>&part;pi
&minus; &part;f
</p>
<p>&part;pi
</p>
<p>&part;g
</p>
<p>&part;q i
</p>
<p>)
.
</p>
<p>In particular,
</p>
<p>{
q i, qj
</p>
<p>}
= 0, {pi,pj } = 0,
</p>
<p>{
q i,pj
</p>
<p>}
= δij .
</p>
<p>Proof From Eq. (28.51), we have
</p>
<p>ωωω(Xf ,Xg)=ωωω
(
&part;f
</p>
<p>&part;pi
</p>
<p>&part;
</p>
<p>&part;q i
&minus; &part;f
</p>
<p>&part;q i
</p>
<p>&part;
</p>
<p>&part;pi
,
&part;g
</p>
<p>&part;pj
</p>
<p>&part;
</p>
<p>&part;qj
&minus; &part;g
</p>
<p>&part;qj
</p>
<p>&part;
</p>
<p>&part;pj
</p>
<p>)
</p>
<p>=
n&sum;
</p>
<p>i,j=1
</p>
<p>[
&part;f
</p>
<p>&part;pi
</p>
<p>&part;g
</p>
<p>&part;pj
ωωω
</p>
<p>(
&part;
</p>
<p>&part;q i
,
</p>
<p>&part;
</p>
<p>&part;qj
</p>
<p>)
</p>
<p>︸ ︷︷ ︸
=0
</p>
<p>&minus; &part;f
&part;pi
</p>
<p>&part;g
</p>
<p>&part;qj
ωωω
</p>
<p>(
&part;
</p>
<p>&part;q i
,
</p>
<p>&part;
</p>
<p>&part;pj
</p>
<p>)
</p>
<p>︸ ︷︷ ︸
=δji
</p>
<p>&minus; &part;f
&part;q i
</p>
<p>&part;g
</p>
<p>&part;pj
ωωω
</p>
<p>(
&part;
</p>
<p>&part;pi
,
</p>
<p>&part;
</p>
<p>&part;qj
</p>
<p>)
</p>
<p>︸ ︷︷ ︸
=&minus;δij
</p>
<p>+ &part;f
&part;q i
</p>
<p>&part;g
</p>
<p>&part;qj
ωωω
</p>
<p>(
&part;
</p>
<p>&part;pi
,
</p>
<p>&part;
</p>
<p>&part;pj
</p>
<p>)
</p>
<p>︸ ︷︷ ︸
=0
</p>
<p>]</p>
<p/>
</div>
<div class="page"><p/>
<p>28.8 Problems 909
</p>
<p>=
n&sum;
</p>
<p>i=1
</p>
<p>(
&part;f
</p>
<p>&part;q i
</p>
<p>&part;g
</p>
<p>&part;pi
&minus; &part;f
</p>
<p>&part;pi
</p>
<p>&part;g
</p>
<p>&part;q i
</p>
<p>)
,
</p>
<p>where we have assumed that ωωω=&sum;nk=1 dqk &and; dpk . The other formulas fol-
low immediately once we substitute pi or q i for f or g. �
</p>
<p>28.8 Problems
</p>
<p>28.1 Provide the details of the fact that a finite-dimensional vector space V
is a manifold of dimension dimV.
</p>
<p>28.2 Choose a different curve γ : R&rarr; R2 whose tangent at u = 0 is still
(ax, ay) of Example 28.2.2. For instance, you may choose
</p>
<p>γ (u)=
(
ax
</p>
<p>2
(u+ 1)2, ay
</p>
<p>3
(u&minus; 1)3
</p>
<p>)
.
</p>
<p>Show that this curve gives the same relation between partials and unit vec-
tors as obtained in that example. Can you find another curve doing the same
job?
</p>
<p>28.3 For every t &isin; TP (M) and every constant function c &isin; F&infin;(P ), show
that t(c)= 0. Hint: Use both parts of Definition 28.2.3 on the two functions
f = c and g = 1.
</p>
<p>28.4 Find the coordinate vector field &part;1 of Example 28.2.10.
</p>
<p>28.5 Use the procedure of Example 28.2.10 to find a coordinate frame
for S2 corresponding to the stereographic projection charts (see Exam-
ple 28.1.12).
</p>
<p>28.6 Let (xi) and (yj ) be coordinate systems on a subset U of a manifold
M . Let Xi and Y i be the components of a vector field with respect to the
two coordinate systems. Show that Y i =Xj&part;yi/&part;xj .
</p>
<p>28.7 Show that if ψ : M &rarr; N is a local diffeomorphism at P &isin; M , then
ψ&lowast;P : TP (M)&rarr; Tψ(P )(N) is a vector space isomorphism.
</p>
<p>28.8 Let X be a vector field on M and ψ : M &rarr; N a differentiable map.
Then for any function f on N , [ψ&lowast;X](f ) is a function on N . Show that
</p>
<p>X(f ◦ψ)=
{
[ψ&lowast;X](f )
</p>
<p>}
◦ψ.
</p>
<p>28.9 Verify that the vector field X = &minus;y&part;x + x&part;y has an integral curve
through (x0, y0) given by
</p>
<p>x = x0 cos t &minus; y0 sin t,
y = x0 sin t + y0 cos t.</p>
<p/>
</div>
<div class="page"><p/>
<p>910 28 Analysis of Tensors
</p>
<p>28.10 Show that the vector field X = x2&part;x + xy&part;y has an integral curve
through (x0, y0) given by
</p>
<p>x(t)= x0
1 &minus; x0t
</p>
<p>, y(t)= y0
1 &minus; x0t
</p>
<p>.
</p>
<p>28.11 Let X and Y be vector fields. Show that X◦Y&minus;X◦Y is also a vector
field, i.e., it satisfies the derivation property.
</p>
<p>28.12 Prove the remaining parts of Proposition 28.4.13.
</p>
<p>28.13 Suppose that xi are coordinate functions on a subset of M and ωωω and
X are a 1-form and a vector field there. Expressωωω(X) in terms of component
functions of ωωω and X.
</p>
<p>28.14 Show that d ◦LX = LX ◦d . Hint: Use the definition of the Lie deriva-
tive for p-forms and the fact that d commutes with the pullback.
</p>
<p>28.15 Let M =R3 and let f be a real-valued function. Let ωωω = aidxi be a
one-form and ηηη= b1dx2&and;dx3+b2dx3&and;dx1+b3dx1&and;dx2 be a two-form
on R3. Show that
</p>
<p>(a) df gives the gradient of f ,
(b) dηηη gives the divergence of the vector B = (b1, b2, b3), and that
(c) &nabla;&nabla;&nabla; &times; (&nabla;&nabla;&nabla;f )= 0 and &nabla;&nabla;&nabla; &middot; (&nabla;&nabla;&nabla; &times; A)= 0 are consequences of d2 = 0.
</p>
<p>28.16 Show that iX is an antiderivation with respect to the wedge product.
</p>
<p>28.17 Given that F= 12Fαβdxα &and; dxβ , show that F&and; (&lowast;F)= |B|2 &minus; |E|2.
</p>
<p>28.18 Use Eq. (28.41) to show that the zeroth component of the relativistic
Lorentz force law gives the rate of change of energy due to the electric field,
and that the magnetic field does not change the energy.
</p>
<p>28.19 Derive Eq. (28.44).
</p>
<p>28.20 Write the equation
</p>
<p>Fαβ =Aβ,α &minus;Aα,β =
&part;Aβ
</p>
<p>&part;xα
&minus; &part;Aα
</p>
<p>&part;xβ
</p>
<p>in terms of E, B, and vector and scalar potentials.
</p>
<p>28.21 With F= 12Fαβdxα &and; dxβ and J = Jγ dxγ , show that d &lowast;F= 4π(&lowast;J)
takes the following form in components:
</p>
<p>&part;F αβ
</p>
<p>&part;xβ
= 4πJ α,
</p>
<p>where indices are raised and lowered by diag(&minus;1,&minus;1,&minus;1,1).
</p>
<p>28.22 Interpret Theorem 28.5.15 for p = 1 and p = 2 on R3.</p>
<p/>
</div>
<div class="page"><p/>
<p>28.8 Problems 911
</p>
<p>28.23 Let f be a function on R3. Calculate d &lowast; df .
</p>
<p>28.24 Show that current conservation is an automatic consequence of Max-
well&rsquo;s inhomogeneous equation d &lowast; F= 4π(&lowast;J).</p>
<p/>
</div>
<div class="page"><p/>
<p>Part IX
</p>
<p>Lie Groups and Their Applications</p>
<p/>
</div>
<div class="page"><p/>
<p>29Lie Groups and Lie Algebras
</p>
<p>The theory of differential equations had flourished to such a level by the
1860s that a systematic study of their solutions became possible. Sophus
Lie, a Norwegian mathematician, undertook such a study using the same
tool that was developed by Galois and others to study algebraic equations:
group theory. The groups associated with the study of differential equations,
now called Lie groups, unlike their algebraic counterparts, are uncountably
infinite, and, as such, are both intricate and full of far-reaching structures. It
was beyond the wildest dream of any 19th-century mathematician to imag-
ine that a concept as abstract as Lie groups would someday find application
in the study of the heart of matter. Yet, three of the four fundamental inter-
actions are described by Lie groups, and the fourth one, gravity, is described
in a language very akin to the other three.
</p>
<p>29.1 Lie Groups and Their Algebras
</p>
<p>Lie groups are infinite groups that have the extra property that their mul-
tiplication law is differentiable. We have seen that the natural setting for
differentiation is the structure of a manifold. Thus, Lie groups must have
manifold properties as well as group properties.
</p>
<p>Definition 29.1.1 A Lie group G is a differentiable manifold endowed Lie groups defined
with a group structure such that the group operation G&times;G&rarr; G and the
map G&rarr;G given by g �&rarr; g&minus;1 are differentiable. If the dimension of the
underlying manifold is r , we say that G is an r-parameter Lie group.
</p>
<p>Because of the dual nature of Lie groups, most of their mapping proper-
ties combine those of groups and manifolds. For instance, a Lie group ho-
momorphism is a group homomorphism that is also C&infin;, and a Lie group
isomorphism is a group isomorphism that is also a diffeomorphism.
</p>
<p>Example 29.1.2 (GL(V) is a Lie group) As the paradigm of Lie groups, GL(V) is a Lie group
we consider GL(V), the set of invertible operators on an n-dimensional real
vector space V, and show that it is indeed a Lie group. The set L(V) is a
</p>
<p>S. Hassani, Mathematical Physics, DOI 10.1007/978-3-319-01195-0_29,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>915</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-01195-0_29">http://dx.doi.org/10.1007/978-3-319-01195-0_29</a></div>
</div>
<div class="page"><p/>
<p>916 29 Lie Groups and Lie Algebras
</p>
<p>vector space of dimension n2 (Proposition 26.1.1), and therefore, by Exam-
ple 28.1.7, a manifold of the same dimension. The map det :L(V)&rarr;R is a
C&infin; map because the determinant, when expressed in terms of a matrix, is a
polynomial. In particular, it is continuous. Now note that
</p>
<p>GL(V)= det&minus;1
(
R&minus; {0}
</p>
<p>)
</p>
<p>and that R &minus; {0} is open. It follows that GL(V) is an open submanifold
of L(V). Thus, GL(V) is an n2-dimensional manifold. Choosing a basis B
for V and representing operators (points) A of GL(V) as matrices (aij ) in
that basis provides a coordinate patch for GL(V). We denote this coordinate
patch by {xij }, where xij (A)= aij .
</p>
<p>To show that GL(V) is a Lie group, we need to prove that if A,B &isin;GL(V),
then
</p>
<p>AB :GL(V)&times;GL(V)&rarr;GL(V) and A&minus;1 :GL(V)&rarr;GL(V)
</p>
<p>are C&infin; maps of manifolds. This is done by showing that the coordinate
representations of these maps are C&infin;. These representations are simply the
matrix representations of operators. Since AB is a linear function of elements
of the two matrices, it has derivatives of all orders. It follows that AB is C&infin;.
The case of A&minus;1 is only slightly more complicated. We note that
</p>
<p>A&minus;1 = P(aij )
det A
</p>
<p>, P (aij )= a polynomial in aij .
</p>
<p>Thus, since det A is also a polynomial in aij , the kth derivative of A&minus;1 is
of the form Q(aij )/(det A)k , where Q is another polynomial. The fact that
det A �= 0 establishes the C&infin; property of A&minus;1.
</p>
<p>One can similarly show that if V is a complex vector space, then GL(V)
is a manifold of dimension 2n2.
</p>
<p>Example 29.1.3 (SL(V) is a Lie group) Recall that SL(V) is the subgroup
SL(V) is a Lie group
</p>
<p>of GL(V) whose elements have unit determinant. Since det :GL(V)&rarr;R is
C&infin;, Theorem 28.3.8 and the example after it show that SL(V)= det&minus;1(1) is
a submanifold of GL(V) of dimension dimGL(V)&minus; dimR= n2 &minus; 1. Since
it is already a subgroup, we conclude that SL(V) is also a Lie group (Prob-
lem 29.5). Similarly, when V is a complex vector space, one can show that
dimSL(V)= 2n2 &minus; 2.
</p>
<p>Example 29.1.4 (Other examples of Lie groups) The reader may verify the
following:
</p>
<p>(a) Any finite-dimensional vector space is a Lie group under vector addi-
tion.
</p>
<p>(b) The unit circle S1, as a subset of nonzero multiplicative complex num-
bers is a Lie group under multiplication.
</p>
<p>(c) The product G&times;H of two Lie groups is itself a Lie group with the
product manifold structure and the direct product group structure.
</p>
<p>(d) GL(n,R), the set of invertible n &times; n matrices, is a Lie group under
matrix multiplication.</p>
<p/>
</div>
<div class="page"><p/>
<p>29.1 Lie Groups and Their Algebras 917
</p>
<p>(e) Let G = GL(n,R) &times; Rn be the product manifold. Define the group
group of affine motions
</p>
<p>of Rn
operation by (A,u)(B,v)&equiv; (AB,Av + u). The reader may verify that
this operation indeed defines a group structure on G. In fact, G be-
comes a Lie group, called the group of affine motions of Rn, for if
we identify (A,u) with the affine motion1 x �&rarr; Ax+ u of Rn, then the
group operation in G is composition of affine motions. We shall study
in some detail the Poincar&eacute; group, a subgroup of the group of affine
motions, in which the matrices are (pseudo) orthogonal.
</p>
<p>In calculations, one translates all group operations to the corresponding
operations of charts. This is particularly useful when the group multiplica-
tion can be defined only locally. One then speaks of an r-parameter local local Lie groups
Lie group. To be precise, one considers a neighborhood U of the origin
of Rr and defines an associative &ldquo;multiplication&rdquo; m : U &times; U &rarr; Rr and an
inversion i : U0 &rarr; U where U0 is a subset of U . We therefore write the
multiplication as
</p>
<p>m(a,b)= c, a,b, c &isin;Rr ,
where a = (a1, a2, . . . , ar), etc. are coordinates of elements of G. The
coordinates of the identity element of G are taken to be all zero. Thus,
m(a,0)= a and m(a, i(a))= 0. In component forms,
</p>
<p>ck =mk(a,b), mk
(
a, i(a)
</p>
<p>)
= 0, k = 1,2, . . . , r. (29.1)
</p>
<p>The fact that G is a manifold implies that all functions in Eq. (29.1) are
infinitely differentiable.
</p>
<p>Example 29.1.5 As an example of a local 1-parameter Lie group, consider
the multiplication rule m :U &times;U &rarr;R where U = {x &isin;R||x|&lt; 1} and
</p>
<p>m(x,y)= 2xy &minus; x &minus; y
xy &minus; 1 , x, y &isin;U.
</p>
<p>The reader can check that m(x, (y, z)) = m((x, y), z), so that the multipli-
cation is associative. Moreover, m(0, x) = m(x,0) = x for all x &isin; U , and
i(x)= x/(2x &minus; 1), defined for U0 = {x &isin;R||x|&lt; 12 }.
</p>
<p>29.1.1 Group Action
</p>
<p>As mentioned in our discussion of finite groups, the action of a group on a
set is more easily conceived than abstract groups. In the case of Lie groups,
the natural action is not on an arbitrary set, but on a manifold.
</p>
<p>Definition 29.1.6 Let M be a manifold. A local group of transformations
acting on M is a (local) Lie group G, an (open) subset U with the property local group of
</p>
<p>transformations{e} &times;M &sub; U &sub; G &times;M , and a map Ψ : U &rarr; M satisfying the following
conditions:
</p>
<p>1These consist of a linear transformation followed by a translation.</p>
<p/>
</div>
<div class="page"><p/>
<p>918 29 Lie Groups and Lie Algebras
</p>
<p>Fig. 29.1 For small regions of M , we may be able to include a large portion of G.
However, if we want to include all of M , as we should, then only a small neighborhood
of the identity may be available
</p>
<p>1. If (g,P ) &isin;U , (h,Ψ (g,P )) &isin;U , and (hg,P ) &isin;U , then
</p>
<p>Ψ
(
h,Ψ (g,P )
</p>
<p>)
= Ψ (hg,P ).
</p>
<p>2. Ψ (e,P )= P for all P &isin;M .
3. If (g,P ) &isin;U , then (g&minus;1,Ψ (g,P )) &isin;U and Ψ (g&minus;1,Ψ (g,P ))= P .
</p>
<p>Normally, we shall denote Ψ (g,P ) by g &middot;P , or gP . Then the conditions
of the definition above take the simple form
</p>
<p>g &middot; (h &middot; P)= (gh) &middot; P, g,h &isin;G, P &isin;M,
e &middot; P = P for all P &isin;M,
</p>
<p>g&minus;1 &middot; (g &middot; P)= P, g &isin;G, P &isin;M,
(29.2)
</p>
<p>whenever g &middot; P is defined. Note that the word &ldquo;local&rdquo; refers to G and not
M , i.e., we may have to choose a very small neighborhood of the identity
before all the elements of that neighborhood can act on all points of M (see
Fig. 29.1).
</p>
<p>All the properties of a group action described in Chap. 23 can be appliedorbit; stabilizer;
transitive, effective, and
</p>
<p>free action
</p>
<p>here as well. So, one talks about the orbit of G as the collection of points in
M obtained from one another by the action of G; the stabilizer Gx of a point
x &isin; M as the collection of all group elements leaving x fixed; transitive
action of G on M when there is only one orbit; free action of G on M when
Gx = {e} for all x &isin;M ; and effective action of G on M when g &middot; x = x for
all x &isin;M implies that g = e. The only extra condition one has to be aware
of is that the group action is not defined for all elements of G, and that a
sufficiently small neighborhood of the identity needs to be chosen. SinceM/G is the set of orbits
</p>
<p>inM &ldquo;belonging to the same orbit&rdquo; is an equivalence relation on M , the set of
orbits of M is denoted by M/G.
</p>
<p>An important consequence of the free action of a group is the following
</p>
<p>Theorem 29.1.7 If G acts freely on M , then G is diffeomorphic to Gx for
any x &isin;M .</p>
<p/>
</div>
<div class="page"><p/>
<p>29.1 Lie Groups and Their Algebras 919
</p>
<p>Proof We assume a left action. The proof for the right action is identical to
this proof. Consider the map φ : Gx &rarr; G given by φ(y) = g for y = gx.
For this map to make sense, g must be determined uniquely from y. If g2x =
y = g1x, then x = g&minus;12 g1x, and because the action is free, we conclude that
g&minus;12 g1 = e and g1 = g2, so that indeed g is determined uniquely by y. Now,
we have to show that φ is a bijection:
</p>
<p>Surjectivity: If g &isin;G, then clearly gx &isin;Gx and φ(gx)= g.
Injectivity: If φ(y1)= φ(y2), with y1 = g1x and y2 = g2x, then g1 = g2.
</p>
<p>�
</p>
<p>In the old literature, the group action is described in terms of coordinates.
Although for calculations this is desirable, it can be very clumsy for formal
discussions, as we shall see later. Let a = (a1, . . . , ar) be a coordinate sys-
tem on G and x = (x1, . . . , xn) a coordinate system on M . Then the group
action Ψ :G&times;M &rarr;M becomes a set of n functions described by
</p>
<p>x&prime; = Ψ (a,x), x&prime;&prime; = Ψ
(
b,x&prime;
</p>
<p>)
= Ψ
</p>
<p>(
m(b,a),x
</p>
<p>)
, (29.3)
</p>
<p>where m is the multiplication law of the Lie group written in terms of coordi-
nates as given in Eq. (29.1). It is assumed that Ψ is infinitely differentiable.
</p>
<p>Box 29.1.8 Equation (29.3) can be used to unravel the multiplication
law for the Lie group when the latter is given in terms of transforma-
tions.
</p>
<p>Example 29.1.9 (Examples of groups of transformation)
</p>
<p>(a) The two-dimensional rotation group acts on the xy-plane as
</p>
<p>Φ(θ, r)= (x cos θ &minus; y sin θ, x sin θ + y cos θ).
</p>
<p>If we write r&prime; =Φ(θ1, r) and r&prime;&prime; =Φ(θ2, r&prime;), then a simple calculation
shows that
</p>
<p>r&prime;&prime; =
(
x cos(θ1 + θ2)&minus;y sin(θ1 + θ2), x sin(θ1 + θ2)+y cos(θ1 + θ2)
</p>
<p>)
.
</p>
<p>With r&prime;&prime; = (m(θ1, θ2); r), we recognize the &ldquo;multiplication&rdquo; law as
m(θ1, θ2)= θ1 + θ2. The orbits are circles centered at the origin.
</p>
<p>(b) Let M =Rn, a a fixed vector in Rn, and G=R. Define Ψ :R&times;Rn &rarr;
Rn by
</p>
<p>Ψ (t,x)= x + ta, x &isin;Rn, t &isin;R.
</p>
<p>This group action is globally defined. The orbits are straight lines par-
allel to a. The group is the set of translations in the direction a in Rn. translations
The reader may verify that the &ldquo;multiplication&rdquo; law is addition of t&rsquo;s.</p>
<p/>
</div>
<div class="page"><p/>
<p>920 29 Lie Groups and Lie Algebras
</p>
<p>(c) Let G=R+ be the multiplicative group of nonzero positive real num-
bers. Fix real numbers α1, α2, . . . , αn, not all zero. Define the action
of G on Rn by
</p>
<p>Ψ (λ,x)&equiv; λ &middot; x =
(
λα1x1, . . . , λ
</p>
<p>αnxn
)
,
</p>
<p>λ &isin;R+, x = (x1, . . . , xn) &isin;Rn.
</p>
<p>The orbits are obtained by choosing a point in Rn and applying G to
it for different λ&rsquo;s. The result is a curve in Rn. For example, if n= 2,
α1 = 1, and α2 = 2, we get, as the orbit containing x0 the curve
</p>
<p>λ &middot; x0 =
(
λx0, λ
</p>
<p>2y0
)
</p>
<p>&rArr; y = y0
x20
</p>
<p>x2,
</p>
<p>which is a parabola going through the origin and the point (x0, y0).
Note that the orbit containing the origin has only one point. This group
is called the group of scale transformations. The multiplication lawscale transformations
is ordinary multiplication of (positive) real numbers.
</p>
<p>(d) Let G=R4 act on M =R byone-dimensional
projective group
</p>
<p>Φ(a, x)&equiv; a1x + a2
a3x + a4
</p>
<p>, a = (a1, a2, a3, a4), a1a4 &minus; a2a3 �= 0.
</p>
<p>The reader may verify that this is indeed the action of a group (catch
where the condition a1a4 &minus; a2a3 �= 0 is used!), and if x&prime; = Φ(b, x)
and x&prime;&prime; =Φ(a, x&prime;), then
</p>
<p>x&prime;&prime; = (a1b1 + a2b3)x + a1b2 + a2b4
(a3b1 + a4b3)x + a3b2 + a4b4
</p>
<p>,
</p>
<p>so that the multiplication rule is
</p>
<p>m(a,b)= (a1b1 + a2b3, a1b2 + a2b4, a3b1 + a4b3, a3b2 + a4b4).
</p>
<p>This group is called the one-dimensional projective group.
</p>
<p>29.1.2 Lie Algebra of a Lie Group
</p>
<p>The group property of a Lie group G provides a natural diffeomorphism on
G that determines a substantial part of its structure.
</p>
<p>Definition 29.1.10 Let G be a Lie group and g &isin;G. The left translation
by g is a diffeomorphism Lg :G&rarr;G defined byleft translation,
</p>
<p>left-invariant vector
</p>
<p>fields, left-invariant
</p>
<p>forms, and their &ldquo;right&rdquo;
</p>
<p>counterparts
</p>
<p>Lg(h)= gh &forall;h &isin;G.
</p>
<p>A vector field ξ on G is called left-invariant if for each g &isin; G, ξ is Lg-
related to itself; i.e.,2
</p>
<p>Lg&lowast; ◦ ξ = ξ ◦Lg, or Lg&lowast;
(
ξ(h)
</p>
<p>)
= ξ(gh) &forall;g,h &isin;G.
</p>
<p>2When there is no danger of confusion, we shall use ξ(h) for ξ |h.</p>
<p/>
</div>
<div class="page"><p/>
<p>29.1 Lie Groups and Their Algebras 921
</p>
<p>The set of left-invariant vector fields on G is denoted by g. A 1-form whose
pairing with a left-invariant vector field gives a constant function on G is
called a left-invariant 1-form.
</p>
<p>The right translation by g, Rg : G &rarr; G, and right-invariant vector
fields and 1-forms are defined similarly.
</p>
<p>The reader may easily check that right and left translations commute:
</p>
<p>Rg ◦Lh = Lh ◦Rg &forall;g,h &isin;G. (29.4)
</p>
<p>It is convenient to have a coordinate representation of Lg&lowast;. The coordi-
nate representation of Lg is simply the multiplication law Lg(h)=m(g,h),
where we have used the same symbol for coordinates as for group elements.
Equation (28.11) can now be used to write the coordinate representation
of Lg&lowast;:
</p>
<p>Lg&lowast; &rarr;
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>&part;m1/&part;h1 &part;m1/&part;h2 . . . &part;m1/&part;hr
</p>
<p>&part;m2/&part;h1 &part;m2/&part;h2 . . . &part;m2/&part;hr
</p>
<p>...
...
</p>
<p>...
</p>
<p>&part;mr/&part;h1 &part;mr/&part;h2 . . . &part;mr/&part;hr
</p>
<p>⎞
⎟⎟⎟⎠ , (29.5)
</p>
<p>where all the derivatives in the matrix are evaluated at (g,h).
We have already mentioned in Chap. 28 that for a general manifold M ,
</p>
<p>X(M) is an infinite-dimensional Lie algebra under the Lie bracket &ldquo;multipli-
cation&rdquo;. In general, X(M) has no finite-dimensional subalgebra. However,
Lie groups are an exception:
</p>
<p>Proposition 29.1.11 Let G be a Lie group and g the set of its left-
invariant vector fields. Then g is a real vector space, and the map φ :
g &rarr; Te(G), defined by φ(ξ) = ξ(e), is a linear isomorphism. Therefore,
dimg = dimTe(G) = dimG. Furthermore, g is closed under Lie brackets;
i.e., g is a Lie algebra.
</p>
<p>Proof It is clear that g is a real vector space. If φ(ξ) = φ(η) for ξ ,η &isin; g,
then
</p>
<p>ξ(g)= Lg&lowast;
(
ξ(e)
</p>
<p>)
= Lg&lowast;
</p>
<p>(
η(e)
</p>
<p>)
= η(g) &forall;g &isin;G&rArr; ξ = η.
</p>
<p>This shows that φ is injective. To show that φ is surjective, suppose that
v &isin; Te(G) and define the vector field ξ on G by ξ(g)= Lg&lowast;(v) for all g &isin;G.
Then φ(ξ)= v and ξ &isin; g, because
</p>
<p>Lg&lowast; ◦ ξ(h)= Lg&lowast; ◦Lh&lowast;(v)= Lgh&lowast;(v)= ξ(gh)&equiv; ξ(Lgh)= ξ ◦Lg(h).
</p>
<p>This proves the first part of the proposition. The second part follows im-
mediately from the definition of a left-invariant vector field and Theo-
rem 28.4.4. �
</p>
<p>The flow of ξ at g &isin;G can be shown to be
</p>
<p>Ft = g exp(tξ)=Rexp(tξ)g. (29.6)</p>
<p/>
</div>
<div class="page"><p/>
<p>922 29 Lie Groups and Lie Algebras
</p>
<p>Indeed, let Xξ be the vector field associated with this flow. The action of this
vector field on a function f is
</p>
<p>Xξ |g(f )=
d
</p>
<p>dt
</p>
<p>(
f (g exp(tξ))
</p>
<p>)∣∣∣∣
t=0
</p>
<p>.
</p>
<p>Therefore,
</p>
<p>(Lh&lowast;Xξ |g)(f )= Xξ |g(f ◦Lh)=
d
</p>
<p>dt
</p>
<p>(
f ◦Lh(g exp(tξ))
</p>
<p>)∣∣∣∣
t=0
</p>
<p>= d
dt
</p>
<p>(
f (hg exp(tξ))
</p>
<p>)∣∣∣∣
t=0
</p>
<p>= Xξ (hg)(f )=
[
Xξ ◦Lh(g)
</p>
<p>]
(f ).
</p>
<p>Since this is true for all f and g, and Xξ |e = ξ(e), we conclude that Xξ is
the unique left-invariant vector field corresponding to ξ(e).
</p>
<p>The Lie algebra of a Lie
</p>
<p>group
</p>
<p>Definition 29.1.12 The Lie algebra of the Lie group G is the Lie
algebra g of left-invariant vector fields on G. Sometimes we think of
ξ as a vector in Te(G). In that case, we denote by Xξ the left-invariant
vector field whose value at the identity is ξ .
</p>
<p>The isomorphism of g with Te(G) induces a Lie bracket on Te(G) and
turns it into a Lie algebra. In many cases of physical interest, it is this inter-
pretation of the Lie algebra of G that is most useful.
</p>
<p>If two groups stand in some algebraic relation to one another, their Lie
algebras will inherit such relations. More precisely, let G and H be Lie
groups with Lie algebras g and h, respectively. Suppose φ :G&rarr;H is a Lie
group homomorphism. Then identifying g with Te(G) and h with Te(H),
and using Theorem 28.4.4, we conclude that φ&lowast; : g &rarr; h is a Lie algebra
homomorphism, i.e., it preserves the Lie brackets:
</p>
<p>φ&lowast;[ξ ,η] = [φ&lowast;ξ , φ&lowast;η] &forall;ξ ,η &isin; g. (29.7)
</p>
<p>Theorem 29.1.13 If φ : G&rarr; H is a Lie group homomorphism, then φ&lowast; :
g&rarr; h is a Lie algebra homomorphism.
</p>
<p>In particular, if φ is a Lie group isomorphism, then φ&lowast; is a Lie algebra
isomorphism.
</p>
<p>Example 29.1.14 Let V be a complex vector space with its general linear
group GL(V), a 2n2-dimensional Lie group. Recall that GL(V) is an open
submanifold of L(V). By Eq. (28.4), Te(GL(V))Te(L(V)), where e is the
unit operator. Now note that on the one hand, we can identify Te(L(V)) with
L(V) [see the box after Eq. (28.4)]. On the other hand, Te(GL(V)) can be
identified with gl(V), the Lie algebra of GL(V). Therefore, gl(V) &sim;= L(V).
We use the notation A(t) for a curve in GL(V) and Ȧ for the vector tangent
to the curve.</p>
<p/>
</div>
<div class="page"><p/>
<p>29.1 Lie Groups and Their Algebras 923
</p>
<p>It is instructive to construct the coordinate representation of vector fields
on GL(V). Let f :GL(V)&rarr;R be a function and Ȧ a vector field. Then, we
have
</p>
<p>Ȧ(f )= d
dt
</p>
<p>(
f
(
A(t)
</p>
<p>))
= daij
</p>
<p>dt
</p>
<p>&part;f
</p>
<p>&part;xij
,
</p>
<p>or, since f is arbitrary,
</p>
<p>Ȧ= daij
dt
</p>
<p>&part;
</p>
<p>&part;xij
&equiv; ȧij
</p>
<p>&part;
</p>
<p>&part;xij
&equiv; dA
</p>
<p>dt
(t),
</p>
<p>where summation over repeated indices is understood and we introduced
dA/dt as an abbreviation for ȧij (&part;/&part;xij ). However, the one-to-one corre-
spondence between matrices and operators makes this more than just an
abbreviation. Indeed, we can interpret dA/dt as the derivative of A and per-
form such differentiation whenever it is possible. The equation above states
that
</p>
<p>Box 29.1.15 To obtain the matrix elements (coordinates) of the op-
erator Ȧ, one differentiates the t-dependent elements of the (matrix
representation of the) operator A(t).
</p>
<p>Of particular interest are the left invariant vector fields, or equivalently,
the vectors belonging to Te(GL(V)). This amounts to substituting t = 0 in
the formulas above. Thus, if Ȧ &isin; Te(GL(V)),
</p>
<p>Ȧ= ȧij (0)
&part;
</p>
<p>&part;xij
&equiv; dA
</p>
<p>dt
(0). (29.8)
</p>
<p>For the product of two operators, we get
</p>
<p>˙̂AB= d
dt
</p>
<p>f
(
A(t)B(t)
</p>
<p>)∣∣∣∣
t=0
</p>
<p>= d
dt
</p>
<p>(aikbkj )
</p>
<p>∣∣∣∣
t=0
</p>
<p>&part;
</p>
<p>&part;xij
</p>
<p>=
(
ȧik(0) bkj (0)︸ ︷︷ ︸
</p>
<p>δkj
</p>
<p>+aik(0)︸ ︷︷ ︸
δik
</p>
<p>ḃkj (0)
) &part;
&part;xij
</p>
<p>= ȧij (0)
&part;
</p>
<p>&part;xij
+ ḃij (0)
</p>
<p>&part;
</p>
<p>&part;xij
= dA
</p>
<p>dt
(0)+ dB
</p>
<p>dt
(0). (29.9)
</p>
<p>Many of the Lie groups used in physics are subgroups of GL(V). A char-
acterization of the Lie algebras of these subgroups is essential for under-
standing the subgroups themselves and applying them to physical situations.
These subgroups are typically defined in terms of maps φ :GL(V)&rarr;M for
which M is a manifold and φ&lowast; is surjective. To construct the Lie algebra of
subgroups of GL(V), we need to concentrate on the map φ&lowast;e as defined on
Te(GL(V)).
</p>
<p>An important map is det :GL(V)&rarr;C for a complex vector space V. We
are interested in evaluating the map det&lowast; : Te(GL(V))&rarr; T1(C) in which we
consider C&sim;= R2 to be a manifold. For an operator Ȧ &isin; Te(GL(V))&sim;= gl(V)</p>
<p/>
</div>
<div class="page"><p/>
<p>924 29 Lie Groups and Lie Algebras
</p>
<p>and a complex-valued function, we have
Differential of the
</p>
<p>determinant map is the
</p>
<p>trace: det&lowast; = tr det&lowast;(Ȧ)f &equiv;
d
</p>
<p>dt
f
(
detA(t)
</p>
<p>)∣∣∣∣
t=0
</p>
<p>&equiv; dx
dt
</p>
<p>&part;f
</p>
<p>&part;x
+ dy
</p>
<p>dt
</p>
<p>&part;f
</p>
<p>&part;y
</p>
<p>= d
dt
</p>
<p>Re detA(t)
</p>
<p>∣∣∣∣
t=0
</p>
<p>&part;f
</p>
<p>&part;x
+ d
</p>
<p>dt
Im detA(t)
</p>
<p>∣∣∣∣
t=0
</p>
<p>&part;f
</p>
<p>&part;y
</p>
<p>= Re tr Ȧ&part;f
&part;x
</p>
<p>+ Im tr Ȧ&part;f
&part;y
</p>
<p>,
</p>
<p>where we used Eq. (5.34). Since f is arbitrary and {&part;/&part;x, &part;/&part;y} can be
identified with {1, i}, we have
</p>
<p>det&lowast;(Ȧ)= tr Ȧ. (29.10)
</p>
<p>Example 29.1.16 (Lie algebra of SL(V)) The special linear group SL(V) is
characterized by the fact that all its elements have unit determinant.
</p>
<p>Box 29.1.17 The Lie algebra sl(V) of the special linear group is the
set of all traceless operators.
</p>
<p>This is because if we use (29.10) and (28.12) and the fact that SL(V) =
det&minus;1(1), we can conclude that det&lowast;(Ȧ)= tr Ȧ= 0 for all Ȧ &isin; sl(V).
</p>
<p>Example 29.1.18 (Lie algebras of unitary and related groups) Let us first
show that the set of unitary operators on V, denoted by U(V), is a Lie
subgroup of GL(V), called the unitary group of V. Consider the mapunitary group
ψ : GL(V)&rarr;H, where H is the set of hermitian operators considered as a
vector space (therefore, a manifold) over the reals, defined by ψ(A)= AA&dagger;.
Using Eq. (29.9), the reader may verify that ψ&lowast; is surjective and
</p>
<p>ψ&lowast;(Ȧ)= Ȧ+ Ȧ&dagger;. (29.11)
</p>
<p>It follows from Theorem 28.3.8 that U(V) &equiv; ψ&minus;1(1) is a subgroup of
GL(V). Using Eq. (28.12), we conclude that ψ&lowast;(Ȧ) = Ȧ + Ȧ&dagger; = 0 for all
Ȧ &isin; u(V), i.e.,
</p>
<p>Box 29.1.19 The Lie algebra u(V) of the unitary group is the set of
all anti-hermitian operators.
</p>
<p>When the vector space is Cn, we write U(n) instead of U(Cn). By count-
ing the number of independent real parameters of a matrix representing a
hermitian operator, we can conclude that dimH= n2. It follows from The-
orem 28.3.8 that dimU(V)= n2.
</p>
<p>The intersection of SL(V) and U(V), denoted by SU(V), is called the
special unitary group. When the vector space is Cn, we write SU(n) in-special unitary group
stead of SU(Cn). The previous two results yields</p>
<p/>
</div>
<div class="page"><p/>
<p>29.1 Lie Groups and Their Algebras 925
</p>
<p>Box 29.1.20 The Lie algebra su(V) consists of anti-hermitian trace-
less operators. If dimV= n, then dim su(V)= n2 &minus;1. We write su(n)
for su(V) if V=Cn.
</p>
<p>The reader is asked to check that dim su(V)= n2 &minus; 1.
If we restrict ourselves to real vector spaces, then unitary and special orthogonal and special
</p>
<p>orthogonal groupsunitary groups become the orthogonal group O(V) and special orthogo-
nal group SO(V), respectively. Their algebras consist of antisymmetric and
traceless antisymmetric operators, respectively. When V = Rn, we use the
notation O(n) and SO(n).
</p>
<p>Let X be a vector field on G. We know from our discussion of flows
that X has a flow Ft &equiv; exp(tX) at every point g of G with &minus;ǫ &lt; t &lt; ǫ.
Now, since Ft (g) �= g is in G, it follows from the group property of G that
(Ft )
</p>
<p>n(g) = Fnt (g) &isin; G for all n. This shows that the flow of every vector
field on a Lie group is defined for all t &isin; R, i.e., all vector fields on a Lie
group are complete. Now consider g as a vector space and manifold and
define a map exp : g&rarr;G that is simply the flow evaluated at t = 1. It can
be shown that the following result holds ([Warn 83, pp. 103&ndash;104]):
</p>
<p>Theorem 29.1.21 exp : g &rarr; G, called the exponential map, is a diffeo- exponential map of a Lie
algebramorphism of a neighborhood of the origin of g with a neighborhood of the
</p>
<p>identity element of G.
</p>
<p>This theorem states that in a neighborhood of the identity element, a Lie
group, as a manifold, &ldquo;looks like&rdquo; its tangent space there. In particular,
</p>
<p>Box 29.1.22 Two Lie groups that have identical Lie algebras are lo-
cally diffeomorphic.
</p>
<p>Example 29.1.23 (Why exp is called the exponential map) Let V be a
finite-dimensional vector space and A &isin; gl(V). Define, as in Chap. 4,
</p>
<p>etA =
&infin;&sum;
</p>
<p>k=0
</p>
<p>tkAk
</p>
<p>k! = 1+ tA+ &middot; &middot; &middot;
</p>
<p>and note that
</p>
<p>d
</p>
<p>dt
etA = AetA &rArr; d
</p>
<p>dt
etA
</p>
<p>∣∣∣∣
t=0
</p>
<p>= A.
</p>
<p>Furthermore,
</p>
<p>etAesA =
&infin;&sum;
</p>
<p>k=0
</p>
<p>tkAk
</p>
<p>k!
</p>
<p>&infin;&sum;
</p>
<p>n=0
</p>
<p>snAn
</p>
<p>n! =
&infin;&sum;
</p>
<p>k=0
</p>
<p>&infin;&sum;
</p>
<p>n=0
</p>
<p>tksn
</p>
<p>k!n!A
k+n</p>
<p/>
</div>
<div class="page"><p/>
<p>926 29 Lie Groups and Lie Algebras
</p>
<p>=
&infin;&sum;
</p>
<p>m=0
</p>
<p>(
m&sum;
</p>
<p>n=0
</p>
<p>tm&minus;nsn
</p>
<p>(m&minus; n)!n!
</p>
<p>)
</p>
<p>︸ ︷︷ ︸
=(t+s)m/m!
</p>
<p>Am = e(t+s)A.
</p>
<p>It follows that etA has all the properties expected of the flow of the vector
field A.
</p>
<p>The exponential map has some important properties that we shall have
occasion to use later. The first of these properties is the content of the fol-
lowing proposition, whose proof is left as an exercise for the reader.
</p>
<p>Proposition 29.1.24 Let φ :H &rarr;G be a Lie group homomorphism. Then,
for all η &isin; h, we have φ(expH η)= expG(φ&lowast;η).
</p>
<p>For every g &isin; G, let Ig &equiv; R&minus;1g ◦ Lg . The reader may readily verify
that Ig , which takes x &isin; G to gxg&minus;1 &isin; G, is an isomorphism of G, i.e.,
Ig(xy) = Ig(x)Ig(y) and Ig is bijective. It is called the inner automor-inner automorphism of a
</p>
<p>Lie group phism associated with g.
</p>
<p>Definition 29.1.25 The Lie algebra isomorphism Ig&lowast; = R&minus;1g&lowast; ◦Lg&lowast; : g&rarr; gadjoint map of a Lie
algebra is denoted by Adg and is called the adjoint map associated with g.
</p>
<p>Since g is a vector space, the adjoint map can be used to construct a
representation of G.
</p>
<p>Definition 29.1.26 The adjoint representation of a Lie group G is Ad :
G&rarr;GL(g) given by Ad(g)=Adg &equiv; Ig&lowast;.
</p>
<p>Using Proposition 29.1.24, we have the following corollary.
</p>
<p>Corollary 29.1.27 exp(Adgξ) = Ig exp ξ = g exp ξg&minus;1 for all ξ &isin; g and
g &isin;G.
</p>
<p>Let {ξ i} be a basis for the (finite-dimensional) Lie algebra of the Lie
group G. The Lie bracket of two basis vectors, being itself a left-invariant
vector field, can be written as a linear combination of {ξ i}:
</p>
<p>[ξ i, ξ j ] =
n&sum;
</p>
<p>k=1
ckij ξ k.
</p>
<p>On a general manifold, ckij will depend on the point at which the fields are
being evaluated. However, on Lie groups, they are independent of the point,
as the following manipulation shows:
</p>
<p>[
ξ i(g), ξ j (g)
</p>
<p>]
=
[
Lg&lowast;ξ i(e),Lg&lowast;ξ j (e)
</p>
<p>]
= Lg&lowast;
</p>
<p>[
ξ i(e), ξ j (e)
</p>
<p>]
</p>
<p>= Lg&lowast;
n&sum;
</p>
<p>k=1
ckij (e)ξ k(e)=
</p>
<p>n&sum;
</p>
<p>k=1
ckij (e)Lg&lowast;ξ k(e)</p>
<p/>
</div>
<div class="page"><p/>
<p>29.1 Lie Groups and Their Algebras 927
</p>
<p>=
n&sum;
</p>
<p>k=1
ckij (e)ξ k(g).
</p>
<p>Therefore, the value of ckij at any point g &isin; G is the same as its value at Lie&rsquo;s second theorem
the identity, i.e., ckij is a constant. This statement is called Lie&rsquo;s second
theorem.
</p>
<p>Definition 29.1.28 Let {ξ i}ni=1 be a basis for the Lie algebra g of the Lie
group G. Then structure constants of a
</p>
<p>Lie algebra[
ξ i(g), ξ j (g)
</p>
<p>]
=
</p>
<p>n&sum;
</p>
<p>k=1
ckij ξ k(g), (29.12)
</p>
<p>where ckij , which are independent of g, are called the structure constants
of G.
</p>
<p>The structure constants satisfy certain relations that are immediate conse-
quences of the commutation relations. The antisymmetry of the Lie bracket
and the Jacobi identity lead directly to Lie&rsquo;s third theorem
</p>
<p>cκρσ =&minus;cκσρ,
</p>
<p>cκρσ c
ν
κμ + cκσμcνκρ + cκμρcνκσ = 0.
</p>
<p>(29.13)
</p>
<p>The fact that {cκσρ} obey Eq. (29.13) is the content of Lie&rsquo;s third theorem.
</p>
<p>29.1.3 Invariant Forms
</p>
<p>If ω|e is a 1-form on Te(G), then ω &isin;Λ1(G), given by ω|g &equiv; L&lowast;g&minus;1ω|e , is a
left-invariant 1-form:
</p>
<p>ω|g(X|g)= L&lowast;g&minus;1ω|e(X|g)= ω|e(Lg&minus;1&lowast;X|g)
</p>
<p>= ω|e(X|g&minus;1g)= ω|e(X|e) (29.14)
</p>
<p>independent of g. A differential form ωωω on G is called left-invariant if
L&lowast;g(ωωω)=ωωω for every g &isin;G. If ωωω is a left-invariant p-form and {ξξξ i}
</p>
<p>p
</p>
<p>i=1 a set
of left-invariant vector fields, then, as in (29.14), the function ωωω(ξξξ1, . . . ,ξξξp)
is constant on G. The exterior derivative of a left-invariant p-form satisfies
</p>
<p>dωωω(ξξξ1, . . . ,ξξξp+1)
</p>
<p>=
&sum;
</p>
<p>1&le;i&lt;j&le;p+1
(&minus;1)i+jωωω
</p>
<p>(
[ξξξ i,ξξξ j ],ξξξ1, . . . , ξ̂ξξ i, . . . , ξ̂ξξ j , . . . ,ξξξp+1
</p>
<p>)
</p>
<p>where we used Theorem 28.5.11 and the fact that vector fields give zero
when acting on a constant function. For a left-invariant 1-form, this yields
</p>
<p>dωωω(ξξξ,ηηη)=&minus;ωωω
(
[ξξξ,ηηη]
</p>
<p>)
. (29.15)</p>
<p/>
</div>
<div class="page"><p/>
<p>928 29 Lie Groups and Lie Algebras
</p>
<p>Definition 29.1.29 The canonical 1-form θθθ on a Lie group G is the left-
invariant g-valued 1-form uniquely determined by θθθ(ξξξ)= ξξξ for all ξξξ &isin; g.
</p>
<p>Let {ξ i}ni=1 be a basis for g. Then, θθθ =
&sum;n
</p>
<p>i=1 θθθ
iξ i , where θθθ
</p>
<p>i are real-
valued 1-forms. Now the definition of θθθ gives
</p>
<p>ξ j = θθθ(ξ j )=
n&sum;
</p>
<p>i=1
θθθ i(ξ j )ξ i .
</p>
<p>This implies that θθθ i(ξ j )= δij , i.e., that {θθθ i}ni=1 is the dual basis of {ξ i}ni=1.
We now want to express the exterior derivative of θθθ i in the basis {θθθ i}ni=1.
</p>
<p>Since it is a 2-form, dθθθ i = αilmθθθ l &and; θθθm, for some αilm &isin;R to be determined.
Since it is invariant, it must satisfy Eq. (29.15). So, we must have
</p>
<p>αilmθθθ
l &and; θθθm(ξ j , ξ k)=&minus;θθθ i[ξ j , ξ k].
</p>
<p>For the right-hand side, using Eq. (29.12), we have
</p>
<p>θθθ i[ξ j , ξ k] = cljkθθθ i(ξ l)= cljkδil = cijk.
</p>
<p>For the left-hand side, we obtain
</p>
<p>αilmθθθ
l &and; θθθm(ξ j , ξ k)= αilm
</p>
<p>[
θθθ l(ξ j )θθθ
</p>
<p>m(ξ k)&minus; θθθ l(ξ k)θθθm(ξ l)
]
</p>
<p>= αilm
[
δlj δ
</p>
<p>m
k &minus; δlkδmj
</p>
<p>]
= 2αijk
</p>
<p>because αijk is antisymmetric in its lower indices. The last two equations
</p>
<p>imply that αijk = 12cijk . We thus arrive at the Maurer-Cartan equation:Maurer-Cartan equation
</p>
<p>dθθθ i =&minus;1
2
cijkθθθ
</p>
<p>j &and; θθθk. (29.16)
</p>
<p>Multiplying both sides by ξ i and summing over i, the left-hand side be-
comes dθθθ . The right-hand side gives
</p>
<p>&minus;1
2
cijkθθθ
</p>
<p>j &and; θθθkξ i =&minus;
1
</p>
<p>2
θθθ j &and; θθθk[ξ j , ξ k] &equiv; &minus;
</p>
<p>1
</p>
<p>2
[θ , θ ]
</p>
<p>where the last expression is defined by the middle expression. Thus the
Maurer-Cartan equation can also be written as
</p>
<p>dθθθ =&minus;1
2
[θ , θ ]. (29.17)
</p>
<p>29.1.4 Infinitesimal Action
</p>
<p>The action Φ :G&times;M &rarr;M of a Lie group on a manifold M induces a ho-
momorphism of its algebra with X(M). If ξ &isin; g, then exp(tξ) &isin;G can act on
M at a point P to produce a curve γ (t)= exp(tξ) &middot;P going through P . The
tangent to this curve at P is defined to be the image of this homomorphism.</p>
<p/>
</div>
<div class="page"><p/>
<p>29.1 Lie Groups and Their Algebras 929
</p>
<p>Definition 29.1.30 Let Φ : G &times; M &rarr; M be an action. If ξ &isin; g, then infinitesimal generators
of an actionΦ(exp tξ ,P ) is a flow on M . The corresponding vector field on M given
</p>
<p>by
</p>
<p>ξM |P &equiv; ξM(P )&equiv;
d
</p>
<p>dt
Φ(exp tξ ,P )
</p>
<p>∣∣∣∣
t=0
</p>
<p>is called the infinitesimal generator of the action induced by ξ .
</p>
<p>In particular, Infinitesimal generators
of representations ofG
</p>
<p>form a representation
</p>
<p>of g.Box 29.1.31 IfM happens to be a vector space, and the action a rep-
resentation as given in Box 24.1.3, then the infinitesimal generators
constitute a representation of the Lie algebra of the group.
</p>
<p>Example 29.1.32 One can think of left translation on a Lie group G as an
action of G on itself. Let Φ : G &times;G&rarr; G be given by Φ(g,h) = Lg(h).
Then Definition 29.1.30 gives
</p>
<p>ξG(g)=
d
</p>
<p>dt
Φ(exp tξ , g)
</p>
<p>∣∣∣∣
t=0
</p>
<p>= d
dt
</p>
<p>exp tξg
</p>
<p>∣∣∣∣
t=0
</p>
<p>= d
dt
</p>
<p>Rg(exp tξ)
</p>
<p>∣∣∣∣
t=0
</p>
<p>=Rg&lowast;ξ
</p>
<p>by the first equation in (28.24). It follows that ξG is right-invariant. Indeed,
</p>
<p>ξG ◦Rh(g)= ξG(gh)= (Rgh)&lowast;ξ = (Rh ◦Rg)&lowast;ξ =Rh&lowast; ◦Rg&lowast;ξ
=Rh&lowast; ◦ ξG(g).
</p>
<p>Since this holds for all g &isin;G, it follows that ξG ◦ Rh = Rh&lowast; ◦ ξG, demon-
strating that ξG is right-invariant.
</p>
<p>The adjoint map of Definition 29.1.25 induces a natural action on the Lie
algebra g with some important properties that we now explore. Define the adjoint action
adjoint action Φ :G&times; g&rarr; g of G on g&sim;= Te(G) by Φ(g, ξ)=Adg(ξ).
</p>
<p>Theorem 29.1.33 The infinitesimal generator ξg of the adjoint action is
adξ , where adξ (η)&equiv; [ξ ,η].
</p>
<p>Proof In fact,
</p>
<p>ξg(η)=
d
</p>
<p>dt
Φ(exp tξ ,η)
</p>
<p>∣∣∣∣
t=0
</p>
<p>= d
dt
</p>
<p>Adexp tξ (η)
</p>
<p>∣∣∣∣
t=0
</p>
<p>= d
dt
</p>
<p>R&minus;1exp tξ&lowast; ◦Lexp tξ&lowast;(η)
∣∣∣∣
t=0
</p>
<p>= d
dt
</p>
<p>F&minus;1t&lowast; η(exp tξ)
∣∣∣∣
t=0
</p>
<p>= Lξ (η)= [ξ ,η] &equiv; adξ (η), (29.18)</p>
<p/>
</div>
<div class="page"><p/>
<p>930 29 Lie Groups and Lie Algebras
</p>
<p>where we used Eq. (29.6) as well as the definition of Lie derivative,
Eq. (28.31). �
</p>
<p>If Φ :G&times;M &rarr;M is an action, then Φg :M &rarr;M , defined by Φg(P )=
Φ(g,P ), is a diffeomorphism of M . Consequently, Φg&lowast; : TP (M) &rarr;
Tg&middot;P (M) is an isomorphism for every P &isin; M whose inverse is Φ&minus;1g&lowast; =
Φg&minus;1&lowast;.
</p>
<p>Proposition 29.1.34 Let Φ : G &times; M &rarr; M be an action. Then for every
g &isin;G and ξ ,η &isin; g, we have
</p>
<p>(Adgξ)M =Φ&minus;1g&lowast; ξM and [ξM ,ηM ] = &minus;[ξ ,η]M .
</p>
<p>Proof Let P be any point in M . Then,
</p>
<p>(Adgξ)M (P )=
d
</p>
<p>dt
Φ(exp tAdgξ ,P )
</p>
<p>∣∣∣∣
t=0
</p>
<p>(by Definition 29.1.30)
</p>
<p>= d
dt
</p>
<p>Φ
(
g(exp tξ)g&minus;1,P
</p>
<p>)∣∣∣∣
t=0
</p>
<p>(by Corollary 29.1.27)
</p>
<p>= d
dt
</p>
<p>Φ
(
g(exp tξ),Φg&minus;1(P )
</p>
<p>)∣∣∣∣
t=0
</p>
<p>(by definition of action)
</p>
<p>= d
dt
</p>
<p>Φg ◦Φ
(
exp tξ ,Φg&minus;1(P )
</p>
<p>)∣∣∣∣
t=0
</p>
<p>(by definition of Φg)
</p>
<p>=Φg&lowast;|Φ
g&minus;1 (P )
</p>
<p>d
</p>
<p>dt
Φ
(
exp tξ ,Φg&minus;1(P )
</p>
<p>)∣∣∣∣
t=0
</p>
<p>[by (28.24)]
</p>
<p>=Φg&lowast;|Φg&minus;1(P )ξM
(
Φg&minus;1(P )
</p>
<p>)
= (Φg&lowast;ξM)(P ). [by (28.28)]
</p>
<p>The second part of the proposition follows by replacing g with exp tη, so
that
</p>
<p>(Adexp tηξ)M =Φexp tη&lowast;ξM =Φ&minus;1exp t (&minus;η)&lowast;ξM .
</p>
<p>Differentiate both sides with respect to t and note that the LHS gives
[η, ξ ]M . The derivative of the RHS is the Lie derivative of ξM with respect
to &minus;ηM , which is &minus;[ηM , ξM ]. �
</p>
<p>Proposition 29.1.34 calculated the infinitesimal action of Ig for a fixed
g &isin; G. This can be considered as a kind of partial derivative. Proposi-
tion 28.3.7 shows us how to find the total derivative for a general action Φ .
For ξ &isin; Tg(G) and X &isin; TP (M), Proposition 28.3.7 yields
</p>
<p>Φ&lowast;(ξ ,X)=Φg&lowast;(X)+ΦP&lowast;(ξ)&equiv; g &middot; X + ξ &middot; P (29.19)
</p>
<p>where the last identity defines the symbols on its left. Note that if ξ0 = ξ(e),
then ξ = Lg&lowast;ξ0 in the equation above. We will have some occasions to use
Eq. (29.19) later.</p>
<p/>
</div>
<div class="page"><p/>
<p>29.1 Lie Groups and Their Algebras 931
</p>
<p>As mentioned earlier, a Lie group action is usually described in terms of
the parameters of the group, which are simply coordinate functions on the
group G, as well as coordinate functions on the manifold M . The infinitesi-
mal generators, being vector fields on M , will then be expressed as a linear
combination of coordinate frames.
</p>
<p>In the older literature, no mention of the manifold structure is made. comparison of
coordinate
</p>
<p>manipulations with
</p>
<p>geometric
</p>
<p>(coordinate-free)
</p>
<p>analysis
</p>
<p>A Lie group is defined in terms of multiplication functions and other func-
tions that represent the action of the group on the manifold. Thus, an
r-parameter Lie group G is a collection of two sets of functions, mρ :
Rr &times; Rr &rarr; R, ρ = 1,2, . . . , r , representing the group multiplication, and
φi : Rr &times; Rn &rarr; R, i = 1,2, . . . , n, representing the action of G on the
n-dimensional manifold M . We sketch the procedure below, leaving most
of the calculations as exercises for the reader. As we develop the theory, the
reader is urged to compare this &ldquo;coordinate-dependent&rdquo; procedure with the
&ldquo;geometric&rdquo; procedure&mdash;which does not use coordinates&mdash;described so far.
</p>
<p>The action of the group is described by the coordinate transformations3
</p>
<p>x&prime;i = φi(a1, . . . , ar ;x1, . . . , xn), i = 1, . . . , n,
xi = φi(0, . . . ,0;x1, . . . , xn),
</p>
<p>(29.20)
</p>
<p>as well as the group multiplication properties
</p>
<p>cρ =mρ(a1, . . . , ar ;b1, . . . , br), ρ = 1, . . . , r,
aρ =mρ(0, . . . ,0;a1, . . . , ar)=mρ(a1, . . . , ar ;0, . . . ,0),
mρ
</p>
<p>(
a;m(b; c)
</p>
<p>)
=mρ
</p>
<p>(
m(a;b); c
</p>
<p>)
.
</p>
<p>(29.21)
</p>
<p>Equation (29.20) is to be interpreted as a rule that takes the second set of
arguments and transforms them via the first set into the LHS. Now suppose
that we translate from x&prime;i to a neighboring point x
</p>
<p>&prime;
i + dx&prime;i via a set of group
</p>
<p>parameters {δaρ}rρ=1. We can also get to x&prime;i + dx&prime;i from xi via a new set of
parameters,4 which have to be slightly different from {aρ}rρ=1, say {aρ +
daρ}rρ=1. We then have
</p>
<p>x&prime;i + dx&prime;i = φi
(
δa1, . . . , δar ;x&prime;1, . . . , x&prime;n
</p>
<p>)
,
</p>
<p>x&prime;i + dx&prime;i = φi(a1 + da1, . . . , ar + dar ;x1, . . . , xn),
aρ + daρ =mρ(δa1, . . . , δar ;a1, . . . , ar),
</p>
<p>(29.22)
</p>
<p>and, with summation over repeated indices understood,
</p>
<p>dx&prime;i =
&part;φi(a;x&prime;)
</p>
<p>&part;aκ
</p>
<p>∣∣∣∣
a=0
</p>
<p>δaκ &equiv; uiκ
(
x&prime;
)
δaκ ,
</p>
<p>daλ =
&part;mλ(b;a)
</p>
<p>&part;bκ
</p>
<p>∣∣∣∣
b=0
</p>
<p>δaκ &equiv; θλκ(a)δaκ .
(29.23)
</p>
<p>3We use subscripts for coordinate functions here for typographical convenience.
4Here we are assuming that the action of the group is transitive, i.e., that every point of
the manifold can be connected to any other point via a transformation.</p>
<p/>
</div>
<div class="page"><p/>
<p>932 29 Lie Groups and Lie Algebras
</p>
<p>Inverting the second equation and substituting the resulting δa&rsquo;s in the first
equation yields
</p>
<p>dx&prime;i = uiκ
(
x&prime;
)
θ&minus;1κλ (a)daλ, or dxi = uiκ (x)θ&minus;1κλ (a)daλ,
</p>
<p>where in the last equation, we changed the free coordinate variable on both
sides. It then follows that
</p>
<p>&part;xi
</p>
<p>&part;aλ
=
</p>
<p>r&sum;
</p>
<p>κ=1
uiκ(x)θ
</p>
<p>&minus;1
κλ (a). (29.24)
</p>
<p>Equation (29.24) and establishing that uiκ is C&infin; is the content of Lie&rsquo;s firstLie&rsquo;s first theorem
theorem.
</p>
<p>The change of an arbitrary function f (x) due to an infinitesimal transfor-
mation is
</p>
<p>df = &part;f
&part;xi
</p>
<p>dxi =
&part;f
</p>
<p>&part;xi
uiκ(x)δaκ = δaκ
</p>
<p>(
uiκ(x)
</p>
<p>&part;
</p>
<p>&part;xi
</p>
<p>)
f.
</p>
<p>This suggests calling
</p>
<p>Xκ =
n&sum;
</p>
<p>i=1
uiκ(x)
</p>
<p>&part;
</p>
<p>&part;xi
(29.25)
</p>
<p>the infinitesimal generators of the Lie group. The commutator of two ofinfinitesimal generators
as vector fields onM these generators is
</p>
<p>[Xρ,Xσ ] =
[
uiρ
</p>
<p>&part;ujσ
</p>
<p>&part;xi
&minus; uiσ
</p>
<p>&part;ujρ
</p>
<p>&part;xi
</p>
<p>]
&part;
</p>
<p>&part;xi
. (29.26)
</p>
<p>This commutator does not appear to be similar to the one in Defini-
tion 29.1.28, which is necessary if the generators are to form a Lie alge-
bra. However, through a long and tortuous manipulation, outlined in Prob-
lem 29.9, one can show that
</p>
<p>[Xρ,Xσ ] = cκρσXκ (29.27)
</p>
<p>where cκρσ are constants.
One can also obtain this same result by the much simpler method of
</p>
<p>applying Proposition 29.1.34 to both sides of Eq. (29.12):
</p>
<p>[
(ξ i)M , (ξ j )M
</p>
<p>]
=&minus;[ξ ,η]M =&minus;
</p>
<p>(
n&sum;
</p>
<p>k=1
ckij ξ k
</p>
<p>)
</p>
<p>M
</p>
<p>=&minus;
n&sum;
</p>
<p>k=1
ckij (ξ k)M .
</p>
<p>This equation is equivalent to (29.27) if we identify the Xρ &rsquo;s with the
(ξ i)M &rsquo;s and ignore the irrelevant minus sign.
</p>
<p>The reader has hopefully been able to appreciate the power and elegance
of the geometric approach to Lie groups and Lie algebras. The above il-
lustration (Problem 29.9) brings out the tedium and the error-prone proce-
dure of obtaining group-theoretic results through coordinate manipulations,
a procedure used in the old literature including the work of Sophus Lie him-
self. Although such calculations are inevitable in practice, where most Lie</p>
<p/>
</div>
<div class="page"><p/>
<p>29.1 Lie Groups and Their Algebras 933
</p>
<p>groups are given in terms of parameters, they are not suitable for obtaining
formal results.
</p>
<p>Example 29.1.35 The two-dimensional rotation group SO(2) is a 1-
parameter Lie group defined by
</p>
<p>x&prime;1 &equiv; φ1(x1, x2; θ)= x1 cos θ &minus; x2 sin θ,
x&prime;2 &equiv; φ2(x1, x2; θ)= x1 sin θ + x2 cos θ.
</p>
<p>Using Eq. (29.25), we find the (only) generator of this group:
</p>
<p>X = ui
&part;
</p>
<p>&part;xi
where ui =
</p>
<p>&part;φi
</p>
<p>&part;θ
</p>
<p>∣∣∣∣
θ=0
</p>
<p>.
</p>
<p>Explicitly, we have
</p>
<p>u1 =
&part;φ1
</p>
<p>&part;θ
</p>
<p>∣∣∣∣
θ=0
</p>
<p>= (&minus;x1 sin θ &minus; x2 cos θ)|θ=0 =&minus;x2,
</p>
<p>u2 =
&part;φ2
</p>
<p>&part;θ
</p>
<p>∣∣∣∣
θ=0
</p>
<p>= (x1 cos θ &minus; x2 sin θ)|θ=0 = x1,
</p>
<p>and
</p>
<p>X = u1
&part;
</p>
<p>&part;x1
+ u2
</p>
<p>&part;
</p>
<p>&part;x2
=&minus;x2
</p>
<p>&part;
</p>
<p>&part;x1
+ x1
</p>
<p>&part;
</p>
<p>&part;x2
.
</p>
<p>The reader recognizes this, within a factor of i, as the z-component of
the angular momentum operator in quantum mechanics. In fact, with pn =
&minus;i&part;/&part;xn, we have
</p>
<p>X = i(x1p2 &minus; x2p1)= iL3 or L3 =&minus;iX
</p>
<p>where L3 is the third component of angular momentum operator r &times; p.
Therefore,
</p>
<p>Box 29.1.36 Angular momentum operators are the infinitesimal gen-
erators of rotation.
</p>
<p>Inclusion of the other two rotations about the x-axis and the y-axis com-
pletes the set of infinitesimal generators of the rotation group in three dimen-
sions. Let us obtain the commutation relation between these components.
First we note that the x-and y-components can also be calculated as
</p>
<p>Xx = y
&part;
</p>
<p>&part;z
&minus; z &part;
</p>
<p>&part;y
, Xy = z
</p>
<p>&part;
</p>
<p>&part;x
&minus; x &part;
</p>
<p>&part;z
,
</p>
<p>and all the three results can be summarized as Xj = ǫjmnxm&part;/&part;xn, with
summation over the repeated indices understood. In terms of momentum,
this becomes Xj = iǫjmnxmpn and Lj =&minus;iXj = ǫjmnxmpn. The commu-
tation relation among the components of angular momentum can now be</p>
<p/>
</div>
<div class="page"><p/>
<p>934 29 Lie Groups and Lie Algebras
</p>
<p>calculated
</p>
<p>[Lj ,Lk] = ǫjmnǫkrs[xmpn, xrps]
= ǫjmnǫkrs
</p>
<p>(
xm[pn, xr ]ps + xr [xm,ps]pn
</p>
<p>)
</p>
<p>= ǫjmnǫkrs(&minus;iδrnxmps + iδsmxrpn)
=&minus;iǫjmnǫknsxmps + iǫjmnǫkrmxrpn.
</p>
<p>Using
</p>
<p>ǫjmnǫksn = δjkδms &minus; δjsδkm and xjpk &minus; xkpj = ǫjkmLm,
</p>
<p>we obtain
</p>
<p>[Lj ,Lk] = iǫjkmLm
which is the desired result.
</p>
<p>We obtained the generators of rotation by obtaining each component sep-
arately from equations connecting two x&rsquo;s to two xs involving only one an-
gle. We could have used three equations connecting the three x&rsquo;s directly
to the three xs. Such an equation writes each x&rsquo; in terms of the three xs
and trigonometric functions of three angles, the Euler angles. The matrix
connecting the two sets of coordinates is given in Example 5.2.7. Prob-
lem 29.10, which the reader is asked to solve as a very illuminating exercise,
calculates the three components of angular momentum directly.
</p>
<p>The action of a Lie group on M can be reconstructed from its infinitesi-
mal action. The flow of Xκ is the solution of the DE
</p>
<p>dx&prime;i
dt
</p>
<p>= uiκ
(
x&prime;
)
, x&prime;i(0)= xi . (29.28)
</p>
<p>Once the solution is obtained, one can replace t with aκ for each κ . In some
applications, uiκ (x) will be given implicitly in terms of certain parameters of
integration of some DEs [unrelated to (29.28)]. The solution of these DEs
are typically generators of coordinate transformations that can be written
linearly in terms of the parameters. To be more precise, suppose that after
solving some DEs, we obtain
</p>
<p>Xi =
r&sum;
</p>
<p>κ=1
ciκf
</p>
<p>(i)
κ (x1, . . . , xn), (29.29)
</p>
<p>where {ciκ} are the parameters of integration, and Xi are components of the
vector field that generates the coordinate transformation. This means that
for small parameters, one can write
</p>
<p>x&prime;i = xi +
r&sum;
</p>
<p>κ=1
ciκf
</p>
<p>(i)
κ (x1, . . . , xn)</p>
<p/>
</div>
<div class="page"><p/>
<p>29.1 Lie Groups and Their Algebras 935
</p>
<p>and read off uiκ(x)= f (i)κ (x). In that case, we have
</p>
<p>dx&prime;i
dt
</p>
<p>= f (i)κ
(
x&prime;1, . . . , x
</p>
<p>&prime;
n
</p>
<p>)
, x&prime;i(0)= xi . (29.30)
</p>
<p>We shall have occasion to use this formula later.
</p>
<p>29.1.5 Integration on Lie Groups
</p>
<p>As any other manifold, one can define integration on Lie groups; i.e., one
can construct nonvanishing n-forms and use Eq. (28.47) to define integrals
on a Lie group G. Because of the left-invariant property of objects on G, it
would be helpful if the integration process were also left-invariant. For this
to happen, the n-form would have to be left-invariant. It turns out that this
can be accomplished more or less uniquely:
</p>
<p>Proposition 29.1.37 Let G be a Lie group of dimension n. Then there ex-
ists a left-invariant nonvanishing n-form μ that is unique up to a nonzero
multiplicative constant. If G is compact, then μ is also right-invariant and
the multiplicative constant can be chosen to be 1.
</p>
<p>Proof Let μe be any nonzero n-form on Te(G). The desired n-form is the
left translation of this form, i.e., L&lowast;
</p>
<p>g&minus;1μe . Indeed, let {Xi}
n
i=1 be left invari-
</p>
<p>ant. Then
</p>
<p>μg(X1|g, . . . ,Xn|g)= L&lowast;g&minus;1μe(X1|g, . . . ,Xn|g)
</p>
<p>= μe(Lg&minus;1&lowast;X1|g, . . . ,Lg&minus;1&lowast;Xn|g)
</p>
<p>= μe(X1|g&minus;1g, . . . ,Xn|g&minus;1g)= μe(X1|e, . . . ,Xn|e).
</p>
<p>This shows that μ is left-invariant. Now note that any other n-form μ&prime;e on
Te(G) is a constant multiple of μe . Therefore, the corresponding n-form μ
</p>
<p>&prime;
g
</p>
<p>will be a constant multiple of μg .
Let x &isin;G and consider μ&prime; &equiv;R&lowast;xμ. We have
</p>
<p>L&lowast;gμ
&prime; = L&lowast;g ◦R&lowast;xμ=R&lowast;x ◦L&lowast;gμ=R&lowast;xμ= μ&prime;,
</p>
<p>where we used the fact that Lg and Rx commute and that μ is left invariant.
The equation above shows that μ&prime; is also left-invariant. Therefore, μ&prime; = cμ.
If G is compact, we can integrate both sides and note that
</p>
<p>&int;
G
μ =
</p>
<p>&int;
G
μ&prime;
</p>
<p>because μ&prime; is related to μ by a change of variable. Therefore, c = 1 and
R&lowast;xμ= μ. �
</p>
<p>The left-invariant volume element (nonvanishing n-form) guaranteed by
the proposition above is called Haar measure. Since all calculations are Haar measure
done using some coordinate system, we give an explicit expression of the
Haar measure in terms of coordinates (parameters) of a general Lie group.
Let y = (y1, . . . , yr) be the coordinates of the translation of x = (x1, . . . , xr)</p>
<p/>
</div>
<div class="page"><p/>
<p>936 29 Lie Groups and Lie Algebras
</p>
<p>by g &isin; G. Then we can write y = m(g,x), so that dyj = (&part;yj/&part;xi)dxi =
(&part;mj/&part;xi)dxi . Therefore,
</p>
<p>dy1 &and; &middot; &middot; &middot; &and; dyr = det
(
&part;mj (g,x)
</p>
<p>&part;xi
</p>
<p>)
dx1 &and; &middot; &middot; &middot; &and; dxr .
</p>
<p>In particular, if x = 0, the coordinates of the identity, then y will be the
coordinates of g. So, the volume element at g, denoted by dry, will be given
by
</p>
<p>dry = det
(
&part;mj (g,x)
</p>
<p>&part;xi
</p>
<p>)∣∣∣∣
x=0
</p>
<p>drx.
</p>
<p>Note that this is consistent with the geometric definition of the invariant
measure given in Proposition 29.1.37 because Lg&minus;1&lowast; = L&minus;1g&lowast; and the matrix
of L&lowast;g is the inverse of the matrix of Lg&lowast;. The volume element at g, which
is invariant on G&mdash;and therefore has the same value as at the identity&mdash;and
which we denote by dμ(g), will be given by
</p>
<p>density functions
</p>
<p>associated with Haar
</p>
<p>measure
</p>
<p>dμ(g)= dμ(e)&equiv; drx = det&minus;1
(
&part;mj (g,x)
</p>
<p>&part;xi
</p>
<p>)∣∣∣∣
x=0
</p>
<p>drg, (29.31)
</p>
<p>where we have replaced y with the more suggestive g. The volume element
drg is the ordinary Euclidean volume element of Rr evaluated at the param-
eters corresponding to g. The quantity multiplying drg is called the density
function. Note that since we are interested in the derivatives of mj at small
values of x, we can take the components of x to be small, and retain them
only up to the first order. This will sometimes simplify the calculation of the
invariant Haar measure.
</p>
<p>Example 29.1.38 From the multiplication rule for the one-dimensional
projective group given in Example 29.1.9, we easily find
</p>
<p>det
</p>
<p>(
&part;mi
</p>
<p>&part;bj
</p>
<p>)∣∣∣∣
b=0
</p>
<p>= det
</p>
<p>⎛
⎜⎜⎝
</p>
<p>a1 0 a2 0
0 a1 0 a2
a3 0 a4 0
0 a3 0 a4
</p>
<p>⎞
⎟⎟⎠= (a1a4 &minus; a2a3)
</p>
<p>2.
</p>
<p>Thus the density function is (a1a4&minus;a2a3)&minus;2, and the invariant Haar measure
is
</p>
<p>dμ(a)= (a1a4 &minus; a2a3)&minus;2d4a.
</p>
<p>29.2 An Outline of Lie Algebra Theory
</p>
<p>The notion of a Lie algebra has appeared on a number of occasions both in
our study of vector fields on manifolds and, more recently, in the study of
Lie groups in the vicinity of their identity elements. Lie algebras play an</p>
<p/>
</div>
<div class="page"><p/>
<p>29.2 An Outline of Lie Algebra Theory 937
</p>
<p>important role in the representation theory of Lie groups as well. It is there-
fore worth our effort to spend some time getting acquainted with the formal
structure and properties of these algebras. We shall restrict our discussion to
finite-dimensional Lie algebras.
</p>
<p>Definition 29.2.1 A finite-dimensional vector space V over R (or C) is
called a Lie algebra over R (or C) if there is a binary operation, called Lie algebra defined
Lie multiplication, [&middot;, &middot;] : V&times;V&rarr; V on V, satisfying
1. [X,Y] = &minus;[Y,X] for all X,Y &isin; V (antisymmetry).
2. [αX + βY,Z] = α[X,Z] + β[Y,Z] for α,β &isin;R (or C) (linearity).
3. [X, [Y,Z]] + [Z, [X,Y]] + [Y, [Z,X]] = 0 (Jacobi identity).
The concepts of a homomorphism, its kernel, its range, etc. are the same as
before.
</p>
<p>To distinguish Lie algebras from vector spaces, we shall denote the for-
mer by lowercase German letters as we have done for the Lie algebras of
Lie groups.
</p>
<p>Example 29.2.2 Recall from Chap. 3 that an algebra is a vector space with
a product. If this product is associative, then one can construct a Lie algebra
out of the associative algebra by defining [a,b] &equiv; ab &minus; ba. In particular,
the matrix algebra under commutation of matrices becomes a Lie algebra,
which we denote by gl(n,R) [or gl(n,C)].
</p>
<p>Definition 29.2.3 Let v be a Lie algebra. A subspace u of v is called a subalgebra, ideal, and
center of a Lie algebrasubalgebra if [X,Y] &isin; u whenever X,Y &isin; u. The subspace u is called an
</p>
<p>ideal if [X,Y] &isin; u whenever either X &isin; u or Y &isin; u. The center z of v is
the collection of all X &isin; v whose Lie multiplication with all vectors of v
vanishes. A Lie algebra is abelian, or commutative, if z= v.
</p>
<p>If we choose a basis in the Lie algebra v, and express the Lie multipli-
cation of basis vectors as a linear combination of basis vectors, we end up
with basis-dependent structure constants that satisfy Eq. (29.13). The struc-
ture constants completely determine the Lie algebra: Given these constants, Knowing the structure
</p>
<p>constants, one can
</p>
<p>reconstruct the Lie
</p>
<p>algebra!
</p>
<p>one can choose a vector space V of correct dimension, a basis in that space,
and impose the Lie multiplication law among the basis vectors suggested by
the structure constants. Once the Lie multiplication law for basis vectors is
established, the law for arbitrary vectors follows from linearity of Lie mul-
tiplication. This procedure induces a binary operation on V and turns it into
a Lie algebra v. Any other algebra so constructed will be isomorphic to v.
</p>
<p>Example 29.2.4 We can classify all two-dimensional Lie algebras by ana-
lyzing their structure constants. Let X1 and X2 be any two linearly indepen-
dent vectors of the two-dimensional Lie algebra v. Write the only nonzero
Lie bracket as
</p>
<p>[X1,X2] = c1X1 + c2X2.</p>
<p/>
</div>
<div class="page"><p/>
<p>938 29 Lie Groups and Lie Algebras
</p>
<p>There are two cases to consider: Either c1 = 0 = c2 or at least one of the
constants is nonzero. The first case corresponds to a 2-dimensional abelian
Lie algebra:
</p>
<p>[Xi,Xj ] = 0 for i, j = 1,2.
For the second case, suppose c1 �= 0 and define the vectors
</p>
<p>X &equiv; c1X1 + c2X2, Y &equiv; X2/c1.
</p>
<p>Then the nonzero Lie bracket becomes [X,Y] = X.
</p>
<p>The result of Example 29.2.4 is summarized as follows:
</p>
<p>Box 29.2.5 There are only two 2-dimensional Lie algebras given by
either one of the following nonzero Lie bracket relations:
</p>
<p>[X1,X2] = 0 or [X1,X2] = X1.
</p>
<p>Example 29.2.6 The Pauli spin matrices
</p>
<p>σ1 =
(
</p>
<p>0 1
1 0
</p>
<p>)
, σ2 =
</p>
<p>(
0 &minus;i
i 0
</p>
<p>)
, σ3 =
</p>
<p>(
1 0
0 &minus;1
</p>
<p>)
</p>
<p>form a Lie algebra under the commutation relation given by
</p>
<p>[σj , σk] = 2iǫjklσl .
</p>
<p>Thus, cljk = 2iǫjkl . Pauli spin matrices are a basis for su(2).
</p>
<p>Example 29.2.7 The Lie group GL(n,R) has gl(n,R), the set of all real
n&times;n matrices, as its Lie algebra. The standard basis of this Lie algebra, also
called the Weyl basis, consists of matrices eij that have zeros everywhereWeyl basis for gl(n,R)
except at the ij th position. We therefore have
</p>
<p>(eij )kl = δikδj l . (29.32)
</p>
<p>We can readily find the Lie multiplication (commutation relations) for these
matrices. We simply need to look at the elements of the matrix of the com-
mutator:
</p>
<p>(
[eij ,ekl]
</p>
<p>)
mn
</p>
<p>= (eijekl)mn &minus; (ekleij )mn
= (eij )mr(ekl)rn &minus; (ekl)mr(eij )rn
= δimδjrδkrδln &minus; δkmδlrδirδjn = δimδjkδln &minus; δkmδliδjn
= (eil)mnδjk &minus; (ekj )mnδli,</p>
<p/>
</div>
<div class="page"><p/>
<p>29.2 An Outline of Lie Algebra Theory 939
</p>
<p>or
</p>
<p>[eij ,ekl] = δjkeil &minus; δilekj . (29.33)
The structure constants, which are naturally double-indexed, can be read off
from Eq. (29.33):
</p>
<p>cmnij,kl = δjkδmi δnl &minus; δilδmk δnj , (29.34)
where we have used a superscript for some of the Kronecker deltas to con-
form to the position of the corresponding index on the LHS.
</p>
<p>Example 29.2.8 An important datum is the dimension of the Lie group (or
its associated Lie algebra, since they are the same). This datum is not ap-
parent in most cases of interest in which the group is defined in terms of
some geometric property. For example, the symplectic group is defined as
all linear transformations A that leave a certain antisymmetric bilinear form
invariant (Example 23.2.2). In terms of matrices, we have
</p>
<p>x&prime;tJx&prime; = xtJx &rArr; xtAtJAx = xtJx &forall;x &isin;R2n, J =
(
</p>
<p>0 1
</p>
<p>&minus;1 0
</p>
<p>)
.
</p>
<p>It follows that the symplectic group consists of all matrices A such that
</p>
<p>AtJA = J. (29.35)
</p>
<p>If we write A in block form,
</p>
<p>A =
(
</p>
<p>A11 A12
</p>
<p>A21 A22
</p>
<p>)
,
</p>
<p>where Aij are n&times; n matrices, then, Eq. (29.35) becomes
(
</p>
<p>At11 A
t
21
</p>
<p>At12 A
t
22
</p>
<p>)(
0 1
</p>
<p>&minus;1 0
</p>
<p>)(
A11 A12
</p>
<p>A21 A22
</p>
<p>)
=
(
</p>
<p>0 1
</p>
<p>&minus;1 0
</p>
<p>)
,
</p>
<p>or
</p>
<p>At11A21 = At21A11, At22A12 = At12A22, At11A22 &minus; At21A12 = 1.
(29.36)
</p>
<p>For the symplectic algebra sp(2n,R), we are interested in the matrix A
when it is close to the identity. This means that
</p>
<p>A11 = 1 + ǫX11, A22 = 1 + ǫX22, A12 = ǫX12, A21 = ǫX21.
</p>
<p>Substituting these in Eq. (29.36) and keeping terms linear in ǫ, we obtain
the following relations among Xij :
</p>
<p>Xt22 =&minus;X11, Xt12 = X12, Xt21 = X21. (29.37)
</p>
<p>It follows that we need n2 parameters to describe the n &times; n matrices X11
and X22 simultaneously. For the symmetric matrices X12 and X21, we need
n(n + 1)/2 independent parameters each. Therefore, the total number of</p>
<p/>
</div>
<div class="page"><p/>
<p>940 29 Lie Groups and Lie Algebras
</p>
<p>independent parameters needed for (or the dimension of) the symplectic al-
gebra sp(2n,R) is
</p>
<p>n2 + 2n(n+ 1)
2
</p>
<p>= n(2n+ 1).
</p>
<p>Although our attempt is to give a formal discussion of the Lie algebras
and their structure in this section, we shall do this with an eye to the even-
tual utility of this discussion in a better understanding of the Lie algebras
of Lie groups. To make the connection between the present formalism and
the Lie algebras arising from Lie groups, we shall make heavy use of ma-
trix groups, i.e., GL(n,R) [or GL(n,C)] and its subgroups. Equation (29.8)
gives a method of finding the matrices of the algebra if those of the group
are known:
</p>
<p>Box 29.2.9 Differentiate the matrix with respect to a parameter at
the identity (where all parameters are set equal to zero) to find the
matrix &ldquo;in the direction&rdquo; of that parameter.
</p>
<p>29.2.1 The Lie Algebras o(p,n&minus; p) and p(p,n&minus; p)
</p>
<p>Many of the Lie groups encountered in physical applications are special
cases of the (pseudo) orthogonal group O(p,n &minus; p) and its associated
Poincar&eacute; group P(p,n &minus; p). It is therefore worthwhile to study their Lie
algebras in some detail. Introduce the diagonal matrix
</p>
<p>η= diag(&minus;1,&minus;1, . . . ,&minus;1︸ ︷︷ ︸
p times
</p>
<p>,1,1, . . . ,1︸ ︷︷ ︸
n&minus; p times
</p>
<p>)
</p>
<p>and note that the (pseudo) orthogonal group O(p,n&minus; p) consists of n&times; n
matrices that leave the bilinear form x &middot; x &equiv; xtηx invariant for x &isin; Rn. This
means that the matrices A will have to satisfy
</p>
<p>AtηA = η &rArr; (det A)2 = 1. (29.38)
</p>
<p>Such matrices are called η-orthogonal. The fact that O(p,n&minus;p) is a groupη-orthogonal matrices
and that η&minus;1 = η can be used to show that
</p>
<p>AηAt = η. (29.39)
</p>
<p>Example 29.2.10 (The Lorentz group) The group of the special theory of
relativity is the full Lorentz group O(3,1). This is the group of transforma-
tions that leave the invariant length5
</p>
<p>ηijxixj =&minus;x21 &minus; x22 &minus; x23 + x24 &equiv; x20 &minus; x21 &minus; x22 &minus; x23
</p>
<p>5It is common to label the time coordinate with index 0 rather than 4. We shall use this
convention.</p>
<p/>
</div>
<div class="page"><p/>
<p>29.2 An Outline of Lie Algebra Theory 941
</p>
<p>of a 4-vector (x1, x2, x3, x0 = ct) invariant. The (0,0)-components of
Eqs. (29.38) and (29.39) yield
</p>
<p>a200 &minus; a210 &minus; a220 &minus; a230 = 1,
</p>
<p>a200 &minus; a201 &minus; a202 &minus; a203 = 1.
(29.40)
</p>
<p>Either one of these equations implies that a00 &ge; 1 or a00 &le; &minus;1. Lorentz
transformations for which a00 &ge; 1 are called orthochronous. Since det1= orthochronous and
</p>
<p>proper orthochronous
</p>
<p>Lorentz transformations
</p>
<p>+1 and 100 =+1, the identity belongs to the subset consisting of transfor-
mations with det A = +1 and a00 &ge; 1. Such transformations form a sub-
group of O(3,1) called the proper orthochronous Lorentz transforma-
tions, and have the property that they can be reached continuously from
the identity.
</p>
<p>Depending on whether x &middot;x &gt; 0, x &middot;x &lt; 0, or x &middot;x = 0, the vector x is called
timelike, spacelike, or null, respectively. In the special theory of relativity
R4 becomes the set of events. At every event x the set R4 is divided into 5
regions:
</p>
<p>1. All events y = (y1, y2, y3, y0) to which one can go from x by material Timelike, spacelike, and
null vectors; R4 as the
</p>
<p>set of events
</p>
<p>objects, with speed less than c, lie to the future of x, i.e., y0 &minus; x0 &gt; 0,
and are timelike:
</p>
<p>(y0 &minus; x0)2 &gt; (y1 &minus; x1)2 + (y2 &minus; x2)2 + (y3 &minus; x3)2.
</p>
<p>They form a 4-dimensional subset of R4 and are said to lie inside the
future light cone. future light cone
</p>
<p>2. All events y = (y1, y2, y3, y0) to which one can go from x only by a
light signal lie to the future of x, i.e., y0 &minus; x0 &gt; 0, and
</p>
<p>(y0 &minus; x0)2 &minus; (y1 &minus; x1)2 &minus; (y2 &minus; x2)2 &minus; (y3 &minus; x3)2 = 0.
</p>
<p>They form a 3-dimensional subset of R4 and are said to lie on the future
light cone.
</p>
<p>3. All events y = (y1, y2, y3, y0) from which one can come to x by mate-
rial objects, with speed less than c, lie in the past of x, i.e., x0 &minus; y0 &gt; 0,
and are timelike:
</p>
<p>(x0 &minus; y0)2 &gt; (x1 &minus; y1)2 + (x2 &minus; y2)2 + (x3 &minus; y3)2.
</p>
<p>They form a 4-dimensional subset of R4 and are said to lie inside the
past light cone. past light cone
</p>
<p>4. All events y = (y1, y2, y3, y0) from which one can come from x only
by a light signal lie to the past of x, i.e., x0 &minus; y0 &gt; 0, and
</p>
<p>(x0 &minus; y0)2 &minus; (x1 &minus; y1)2 &minus; (x2 &minus; y2)2 &minus; (x3 &minus; y3)2 = 0.
</p>
<p>They form a 3-dimensional subset of R4 and are said to lie on the past
light cone.
</p>
<p>5. All events in the remaining part of R4 form a 4-dimensional subset, are
spacelike, and cannot be connected to x by any means. They are said to
belong to elsewhere. elsewhere</p>
<p/>
</div>
<div class="page"><p/>
<p>942 29 Lie Groups and Lie Algebras
</p>
<p>From a physical standpoint, future and past are observer-independent.
Therefore, if y lies in or on the future light cone of x with respect to one
observer, it should also do so with respect to all observers. Since observers
are connected by Lorentz transformations, we expect the latter to preserve
this relation between x and y. Not all elements of O(3,1) have this property.
However, the proper orthochronous transformations do. The details are left
as a problem for the reader (see Problem 29.15).
</p>
<p>As a prototype of η-orthogonal matrices, consider the matrix obtained
from the unit matrix by removing the iith, ij th, jith, and jj th elements,
and replacing them by an overall 2 &times; 2 matrix. The result, denoted by A(ij),
will look like
</p>
<p>A(ij) =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>1 0 . . . 0 . . . 0 . . . 0
0 1 . . . 0 . . . 0 . . . 0
...
</p>
<p>...
...
</p>
<p>...
...
</p>
<p>0 0 . . . aii . . . aij . . . 0
...
</p>
<p>...
...
</p>
<p>...
...
</p>
<p>0 0 . . . aji . . . ajj . . . 0
...
</p>
<p>...
...
</p>
<p>...
...
</p>
<p>0 0 . . . 0 . . . 0 . . . 1
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>.
</p>
<p>This matrix will transform (x1, . . . , xn) &isin;Rn according to
</p>
<p>x&prime;i = aiixi + aijxj , (no summation!)
x&prime;j = ajixi + ajjxj ,
</p>
<p>x&prime;k = xk for k �= i, j.
</p>
<p>In order for A(ij) to leave the bilinear form xtηx invariant, the 2 &times; 2 sub-
matrix
</p>
<p>( aii aij
aj i ajj
</p>
<p>)
must be either a rotation (corresponding to the case where
</p>
<p>i, j &le; p or i, j &gt; p), or a Lorentz boost6 (corresponding to the case where
i &le; p and j &gt; p). In the first case, we have
</p>
<p>(
aii aij
aji ajj
</p>
<p>)
=
(
</p>
<p>cos θ &minus; sin θ
sin θ cos θ
</p>
<p>)
,
</p>
<p>and in the second case
(
aii aij
aji ajj
</p>
<p>)
=
(
</p>
<p>cosh ξ &minus; sinh ξ
&minus; sinh ξ cosh ξ
</p>
<p>)
,
</p>
<p>where ξ &equiv; tanh&minus;1(v/c) is the &ldquo;rapidity&rdquo;.
The matrices of the algebra are obtained by differentiation at θ = 0 (or
</p>
<p>ξ = 0). Denoting these matrices by Mij , we readily find that for the case of
rotations, Mij has &minus;1 at the ij th position, +1 at the jith position, and 0
</p>
<p>6The elementary Lorentz transformations involving only one space dimension.</p>
<p/>
</div>
<div class="page"><p/>
<p>29.2 An Outline of Lie Algebra Theory 943
</p>
<p>everywhere else. For the case of boosts, Mij has &minus;1 at the ij th and the jith
position, and 0 everywhere else. Both cases can be described by the single
relation
</p>
<p>(Mij )
m
l = ηilδmj &minus; ηj lδmi , Mij =&minus;Mji .
</p>
<p>It is convenient to have all indices in the lower position. So, we multiply
both sides by ηmk to obtain
</p>
<p>(Mij )kl = ηilηjk &minus; ηj lηik, Mij =&minus;Mji . (29.41)
</p>
<p>We can use Eq. (29.41) to find the Lie multiplication (in this case, matrix
commutation relations) for the algebra o(p,n&minus; p): Lie brackets for the
</p>
<p>algebra o(p,n&minus; p)[Mij ,Mkl] = ηikMj l &minus; ηilMjk + ηj lMik &minus; ηjkMil . (29.42)
</p>
<p>The Lie group O(p,n &minus; p) includes rotations and Lorentz transfor-
mation. Another group with considerable significance in physics is the
Poincar&eacute; group P(p,n &minus; p), which includes translations7 in space and
time as well. An element of P(p,n&minus; p) transforms x &isin;Rn to x&prime; = Ax + u,
where u is a column vector representing the translation part of the group. It
is convenient to introduce matrices to represent these group operations. This
is possible if we represent an element of Rn as an (n + 1)-column whose
last element is an insignificant 1. Then, the reader may check that a Poincar&eacute;
transformation can be written as Poincar&eacute; group as
</p>
<p>(n+ 1)&times; (n+ 1)
matrices
</p>
<p>(
x&prime;
</p>
<p>1
</p>
<p>)
=
(
</p>
<p>A u
</p>
<p>0 1
</p>
<p>)(
x
</p>
<p>1
</p>
<p>)
, (29.43)
</p>
<p>where A is the n &times; n matrix of O(p,n &minus; p), and u is an n-dimensional
column vector.
</p>
<p>The Lie algebra of the Poincar&eacute; group is obtained by differentiating the
(n+ 1)&times; (n+ 1) matrix of Eq. (29.43). The differentiation of the matrix A
will give o(p,n&minus;p) of Eq. (29.42). The translation part will lead to matrices
Pi with matrix elements given by
</p>
<p>(Pi)
k
l = δki δn+1l &rArr; (Pi)kl = ηikδn+1l . (29.44)
</p>
<p>These matrices satisfy the following Lie multiplication rules:
</p>
<p>[Pi,Pj ] = 0, [Mij ,Pk] = ηikPj &minus; ηjkPi .
</p>
<p>It then follows that the full Poincar&eacute; algebra p(p,n&minus; p) is described by
the following Lie brackets: Lie brackets for the
</p>
<p>Poincar&eacute; algebra
</p>
<p>p(p,n&minus; p)[Mij ,Mkl] = ηikMj l &minus; ηilMjk + ηj lMik &minus; ηjkMil,
[Mij ,Pk] = ηikPj &minus; ηjkPi,
[Pi,Pj ] = 0.
</p>
<p>(29.45)
</p>
<p>7One can think of the Poincar&eacute; group as a subgroup of the group of affine motions in
which the matrices belong to O(p,n&minus; p) rather than GL(n,R).</p>
<p/>
</div>
<div class="page"><p/>
<p>944 29 Lie Groups and Lie Algebras
</p>
<p>29.2.2 Operations on Lie Algebras
</p>
<p>Definition 29.2.11 Let v be a Lie algebra. A linear operator D : v&rarr; v sat-
isfying
</p>
<p>D[X,Y] = [DX,Y] + [X,DY]
is called a derivation of v.
</p>
<p>Although the product of two derivations is not a derivation, their com-derivation algebra of a
Lie algebra mutator is. Therefore, the set of derivations of a Lie algebra v themselves
</p>
<p>form a Lie algebra Dv under commutations, which is called the derivation
algebra.
</p>
<p>Recall that the infinitesimal generators of the adjoint action of a Lie group
on its algebra were given by adξ [Eq. (29.18)]. We can apply this to a general
Lie algebra v by fixing a vector X &isin; v and defining the map adX : v &rarr; v
given by adX(Y) = [X,Y]. The reader may verify that adX is a derivation
of v and that ad[X,Y] = [adX, adY]. Therefore, the set adv &equiv; {adX | X &isin; v} is
a Lie algebra, a subalgebra of the derivation algebra Dv of v, and is called
the adjoint algebra of v. There is a natural homomorphism ψ : v &rarr; advadjoint algebra of a Lie
</p>
<p>algebra given by ψ(X)= adX whose kernel is the center of v. Furthermore, adv is
an ideal of Dv.
</p>
<p>Example 29.2.12 We construct the matrix representation of the operatorsIllustration of
homomorphism of su(2)
</p>
<p>and its adjoint using
</p>
<p>Pauli spin matrices
</p>
<p>in the adjoint algebra of su(2) with Pauli spin matrices as a basis. From
</p>
<p>adσ1(σ1)= [σ1, σ1] = 0
</p>
<p>we conclude that the first column of the matrix of adσ1 is zero. From
</p>
<p>adσ1(σ2)= [σ1, σ2] = 2iσ3
</p>
<p>we conclude that the second column of the matrix of adσ1 has zeros for the
first two elements and 2i for the last. Similarly, from
</p>
<p>adσ1(σ3)= [σ1, σ3] = &minus;2iσ2
</p>
<p>we conclude that the third column of the matrix of adσ1 has zeros for the first
and third elements and &minus;2i for the second. Thus, the matrix representation
of adσ1 is
</p>
<p>adσ1 =
</p>
<p>⎛
⎝
</p>
<p>0 0 0
0 0 &minus;2i
0 2i 0
</p>
<p>⎞
⎠ .
</p>
<p>Likewise, we can obtain the other two matrix representations; they are
</p>
<p>adσ2 =
</p>
<p>⎛
⎝
</p>
<p>0 0 2i
0 0 0
</p>
<p>&minus;2i 0 0
</p>
<p>⎞
⎠ , adσ3 =
</p>
<p>⎛
⎝
</p>
<p>0 &minus;2i 0
2i 0 0
0 0 0
</p>
<p>⎞
⎠ .
</p>
<p>The reader may readily verify that [adσj , adσk ] = 2iǫjkladσl .</p>
<p/>
</div>
<div class="page"><p/>
<p>29.2 An Outline of Lie Algebra Theory 945
</p>
<p>If ψ is an automorphism of v, i.e., an isomorphism of v onto itself, then
</p>
<p>adψ(X) =ψ ◦ adX ◦ψ&minus;1 &forall;X &isin; v. (29.46)
</p>
<p>Since adX is an operator on the vector space v, one can define the trace of
adX. However, the notion of trace attains a far greater significance when it is
combined with the notion of composition of operators. For X,Y &isin; v, define
</p>
<p>(X | Y)&equiv; tr(adX ◦ adY). (29.47)
</p>
<p>Then one can show that (&middot;|&middot;) is bilinear and symmetric. It becomes an inner
product if the &ldquo;vectors&rdquo; of the Lie algebra are hermitian operators on some
vector space V, or if the underlying vector space is over R (see Proposi-
tion 5.6.6 in Chap. 5). Furthermore, (&middot;|&middot;) satisfies
</p>
<p>(
[X,Y] | Z
</p>
<p>)
+
(
[X,Z] | Y
</p>
<p>)
= 0. (29.48)
</p>
<p>Definition 29.2.13 The symmetric bilinear form (&middot;|&middot;) : v&times; v&rarr; C defined Killing form of a Lie
algebraby (29.47) is called the Killing form of v.
</p>
<p>It is an immediate consequence of this definition and Eq. (29.46) that the
Killing form of v is invariant under all automorphisms of v.
</p>
<p>As noted above, the Killing form is an inner product if the Lie algebra
consists of hermitian operators. This will certainly happen if the Lie alge-
bra is that of a group whose elements are unitary operators on some vector
space V. We shall see shortly that such unitary operators are not only pos-
sible, but have extremely useful properties in the representation of compact
Lie groups. A unitary representation of a Lie group induces a representation
of its Lie algebra whose &ldquo;vectors&rdquo; are hermitian operators. Then the Killing
form becomes an inner product. The natural existence of such Killing forms
for the representation of a compact Lie group motivates the following:
</p>
<p>Definition 29.2.14 A Lie algebra v is compact if it has an inner product compact Lie algebra
(&middot;|&middot;) satisfying
</p>
<p>(
[X,Y] | Z
</p>
<p>)
+
(
[X,Z] | Y
</p>
<p>)
= 0.
</p>
<p>Choose a basis {Xi} for the Lie algebra v and note that (adXi )kj = ckij .
Therefore,
</p>
<p>(Xi | Xj )&equiv; tr(adXi ◦ adXj )= (adXi )kl (adXj )lk = ckilcljk &equiv; gij , (29.49)
</p>
<p>where gij are components of the so-called Cartan metric tensor in the basis Cartan metric tensor of a
Lie algebra{Xi}. If A,B &isin; v have components {ai} and {bi} in the basis {Xi}, then it
</p>
<p>follows from Eq. (29.49) that
</p>
<p>(A | B)= aibjgij , (29.50)
</p>
<p>as expected of a symmetric bilinear form. We can use the Cartan metric to
lower the upper index of the structure constants: cijk &equiv; clijglk . By virtue of</p>
<p/>
</div>
<div class="page"><p/>
<p>946 29 Lie Groups and Lie Algebras
</p>
<p>Eq. (29.49), the new constants may be written in the form
</p>
<p>cijk = clijcrlscskr =
(
&minus;cljscrli &minus; clsicrlj
</p>
<p>)
cskr by (29.13)
</p>
<p>= cljscrilcskr + clsicrlj csrk.
</p>
<p>The reader may now verify that the RHS is completely antisymmetric in i,
j , and k. If the Lie algebra is compact, then one can choose an orthonormal
basis in which glk = δlk (because the inner product is, by definition, positive
definite) and obtain ckij = cijk . We therefore have the following result.
</p>
<p>Proposition 29.2.15 Let v be a compact Lie algebra. Then there exists a
basis of v in which the structure constants are represented by a third-order
completely antisymmetric covariant tensor.
</p>
<p>Historical Notes
</p>
<p>Most mathematicians seem to have little or no interest in history, so that often the name
</p>
<p>Wilhelm Karl Joseph
</p>
<p>Killing 1847&ndash;1923
</p>
<p>attached to a key result is that of the follow-up person who exploited an idea or theorem
rather than its originator (the Jordan form is due to Weierstrass, Wedderburn theory to
Cartan and Molien). No one has suffered from this ahistoricism more than Killing. For
example, the so-called &ldquo;Cartan sub-algebra&rdquo; and &ldquo;Cartan matrix&rdquo; were defined and ex-
ploited by Killing. He exhibited the characteristic equation of an arbitrary element of the
Weyl group when Weyl was 3 years old and listed the orders of the Coxeter transformation
19 years before Coxeter was born!
Wilhelm Karl Joseph Killing (1847&ndash;1923) began university study in M&uuml;nster in 1865
but quickly moved to Berlin and came under the influence of Kummer and Weierstrass.
From 1868 to 1882 much of Killing&rsquo;s energy was devoted to teaching at the gymnasium
level in Berlin and Brilon (south of M&uuml;nster). At one stage, when Weierstrass was urging
him to write up his research on space structures, he was spending as much as 36 hours per
week in the classroom or tutoring. (Now many mathematicians consider 6 hours a week
an intolerable burden!) On the recommendation of Weierstrass, Killing was appointed
professor of mathematics at the Lyzeum Hosianum in Braunsberg, in East Prussia (now
Braniewo in the region of Olsztyn, in Poland). This was a college founded in 1565 by
Bishop Stanislaus Hosius, whose treatise on the Christian faith ran to 39 editions! The
main object of the college was the training of Roman Catholic clergy, so Killing had to
teach a wide range of topics, including the reconciliation of faith and science. Although he
was isolated mathematically during his ten years in Braunsberg, this was the most creative
period in his mathematical life. Killing produced his brilliant work despite worries about
the health of his wife and seven children, demanding administrative duties as rector of
the college and as a member and chairman of the City Council, and his active role in the
Church of St. Catherine.
What we now call Lie algebras were invented by the Norwegian mathematician Sophus
Lie about 1870 and independently by Killing about 1880. Lie was seeking to develop
an approach to the solution of differential equations analogous to the Galois theory of
algebraic equations. Killing&rsquo;s consuming passion was non-Euclidean geometries and their
generalizations, so he was led to the problem of classifying infinitesimal motions of a rigid
body in any type of space (or Raumformen, as he called them).
In 1892 he was called back to his native Westphalia as professor of mathematics at the
University of M&uuml;nster, where he was quickly submerged in teaching, administration, and
charitable activities. He was Rector Magnificus for some period and president of the St.
Vincent de Paul charitable society for ten years. Killing&rsquo;s work was neglected partly be-
cause he was a modest man with high standards; he vastly underrated his own achieve-
ment. His interest was geometry, and for this he needed all real Lie algebras. To obtain
merely the simple Lie algebras over the complex numbers did not appear to him to be
very significant. Another reason was due to Lie, who was quite negative about Killing&rsquo;s
work. At the top of page 770 of a three-volume joint work of Lie and Engel we find the</p>
<p/>
</div>
<div class="page"><p/>
<p>29.2 An Outline of Lie Algebra Theory 947
</p>
<p>following less than generous comment about Killing: &ldquo;With the exception of the preced-
ing unproved theorem . . . all the theorems that are correct are due to Lie and all the false
ones are due to Killing!&rdquo;
Killing was conservative in his political views and vigorously opposed the attempt to re-
form the examination requirements for graduate students at the University of M&uuml;nster by
deleting the compulsory study of philosophy. Engel comments &ldquo;Killing could not see that
for most candidates the test in philosophy was completely worthless.&rdquo; He had a profound
patriotic love of his country, so that his last years (1918&ndash;1923) were deeply pained by the
collapse of social cohesion in Germany after the War of 1914&ndash;18.
(Taken from A.J. Coleman, &ldquo;The Greatest Mathematical Paper of All Times,&rdquo; Mathemat-
ical Intelligencer 11(3) (1989) 29&ndash;38.)
</p>
<p>Example 29.2.16 We can calculate explicitly the Killing form of the Lie
algebras gl(n,R) and sl(n,R). Choose the Weyl basis introduced in Exam-
ple 29.2.7 and expand A,B &isin; gl(n,R) in terms of the Weyl basis vectors:
A = aijeij , B = bijeij . The Cartan metric tensor becomes
</p>
<p>gij,kl = crsij,mncmnkl,rs =
(
δjmδ
</p>
<p>r
i δ
</p>
<p>s
n &minus; δinδrmδsj
</p>
<p>)(
δlrδ
</p>
<p>m
k δ
</p>
<p>n
s &minus; δksδmr δnl
</p>
<p>)
,
</p>
<p>where we have used Eq. (29.34). It follows from these relations, Eq. (29.50),
and a simple index manipulation that
</p>
<p>(A | B)&equiv; aijbklgij,kl = 2n tr(AB)&minus; 2 tr A tr B (29.51)
</p>
<p>for A,B &isin; gl(n,R), and
</p>
<p>(A | B)= 2n tr(AB) (29.52)
</p>
<p>for A,B &isin; sl(n,R), because all matrices in sl(n,R) are traceless.
</p>
<p>A Lie algebra v, as a vector space, may be written as a direct sum of its
subspaces. We express this as
</p>
<p>v= u1 &oplus;V u2 &oplus;V &middot; &middot; &middot; &oplus;V ur =
r&sum;
</p>
<p>k=1
&oplus;V uk.
</p>
<p>If in addition {uk} are Lie subalgebras every one of which commutes with
the rest, we write
</p>
<p>v= u1 &oplus; u2 &oplus; &middot; &middot; &middot; &oplus; ur &equiv;
r&oplus;
</p>
<p>k=1
uk (29.53)
</p>
<p>and say that v has been decomposed into a direct sum of Lie algebras. In
this case, each uk is not only a subalgebra, but also an ideal of v (see Propo-
sition 3.2.11).
</p>
<p>The study of the structure of Lie algebras boils down to the study of the
&ldquo;simplest&rdquo; kind of Lie algebras in terms of which other Lie algebras can
be decomposed. Intuitively, one would want to call a Lie algebra &ldquo;simple&rdquo;
if it has no proper subalgebras. However, in terms of decomposition, such
subalgebras are required to be ideals. So the natural definition of a simple
Lie algebra would be the following (see Definition 3.2.12):</p>
<p/>
</div>
<div class="page"><p/>
<p>948 29 Lie Groups and Lie Algebras
</p>
<p>Definition 29.2.17 A Lie algebra that has no proper ideal is called a simplesemisimple Lie algebras
Lie algebra. A Lie algebra is semisimple if it has no (nonzero) commutative
ideal.
</p>
<p>For example, the pseudo-orthogonal algebra o(p,n&minus; p) is semisimple,
but the Poincar&eacute; algebra p(p,n&minus;p) is not because the translation generators
Pj form a commutative ideal.
</p>
<p>A useful criterion for semisimplicity is given by the following theorem
due to Cartan, which we state without proof (for a proof, see [Baru 86,
pp. 15&ndash;16]):
</p>
<p>Theorem 29.2.18 (Cartan) A Lie algebra v is semisimple iff det(gij ) �= 0.
</p>
<p>The importance of semisimple Lie algebras is embodied in the following
theorem: [Baru 86, pp. 19&ndash;20].
</p>
<p>Theorem 29.2.19 (Cartan) A semisimple complex or real Lie algebra can
be decomposed into a direct sum of pairwise orthogonal simple subalgebras.
This decomposition is unique up to ordering.
</p>
<p>This is the analogue of Theorem 3.5.25.
The orthogonality is with respect to the Killing form. Theorem 29.2.19
</p>
<p>reduces the study of semisimple Lie algebras to that of simple Lie algebras.
What about a general Lie algebra v? If v is compact, then it turns out that it
can be written as v= z&oplus; s where z is the center of v and s is semisimple.
If v is not compact, then the decomposition will not be in terms of a direct
sum, but in terms of what is called a semidirect sum one of whose factors is
semisimple. For details, the reader is referred to the fairly accessible treat-
ment of Barut and Raczka, Chap. 2. From now on we shall restrict our dis-
cussion to semisimple Lie algebras. These algebras are completely known,
because simple algebras have been completely classified. We shall not pur-
sue the classification of Lie algebras. However, we simply state a definition
that is used in such a classification, because we shall have an occasion to
use it in the representation theory of Lie algebras.
</p>
<p>Definition 29.2.20 Let v be a Lie algebra. A subalgebra h of v is calledCartan subalgebra and
the rank of a Lie algebra a Cartan subalgebra if h is the largest commutative subalgebra of v, and
</p>
<p>for all X &isin; h, if adX leaves a subspace of v invariant, then it leaves the
complement of v invariant as well. The dimension of h is called the rank
of v.
</p>
<p>29.3 Problems
</p>
<p>29.1 Show that the set G=GL(n,R)&times;Rn equipped with the &ldquo;product&rdquo;
</p>
<p>(A,u)(B,v)&equiv; (AB,Av + u)
</p>
<p>forms a group. This is called the affine group.affine group</p>
<p/>
</div>
<div class="page"><p/>
<p>29.3 Problems 949
</p>
<p>29.2 Show that m : U &times; U &rarr; R defined in Example 29.1.5 is a local Lie
group.
</p>
<p>29.3 Find the multiplication law for the groups in (b) and (c) of Exam-
ple 29.1.9.
</p>
<p>29.4 Show that the one-dimensional projective group of Example 29.1.9
satisfies all the group properties. In particular, find the identity and the in-
verse of an element in the group.
</p>
<p>29.5 Let G be a Lie group. Let S be a subgroup of G that is also a subman-
ifold of G. Show that S is a Lie group.
</p>
<p>29.6 Show that the differential map of ψ : GL(V) &rarr; H(V), defined by
ψ(A) = AA&dagger;, where H(V) is the set of hermitian operators on V, is sur-
jective. Derive Eq. (29.11).
</p>
<p>29.7 Verify that Ig &equiv;R&minus;1g ◦Lg is an isomorphism.
</p>
<p>29.8 Prove Proposition 29.1.24.
</p>
<p>29.9 Start with Eq. (29.24) and use the fact that second derivative is inde-
pendent of the order of differentiation to obtain
</p>
<p>uiκ
</p>
<p>[
&part;θ&minus;1κμ
&part;aλ
</p>
<p>&minus; &part;θ
&minus;1
κλ
</p>
<p>&part;aμ
</p>
<p>]
+ θ&minus;1κμ
</p>
<p>&part;uiκ
</p>
<p>&part;aλ
&minus; θ&minus;1κλ
</p>
<p>&part;uiκ
</p>
<p>&part;aμ
= 0.
</p>
<p>Now use the chain rule &part;uiκ/&part;aλ = (&part;uiκ/&part;xj )(&part;xj/&part;aλ) and Eq. (29.24)
to get
</p>
<p>uiκ
</p>
<p>[
&part;θ&minus;1κμ
&part;aλ
</p>
<p>&minus; &part;θ
&minus;1
κλ
</p>
<p>&part;aμ
</p>
<p>]
+
[
ujν
</p>
<p>&part;uiκ
</p>
<p>&part;xj
&minus; ujκ
</p>
<p>&part;uiν
</p>
<p>&part;xj
</p>
<p>]
θ&minus;1κμ θ
</p>
<p>&minus;1
νλ = 0,
</p>
<p>or
</p>
<p>ujσ
&part;uiτ
</p>
<p>&part;xj
&minus; ujτ
</p>
<p>&part;uiσ
</p>
<p>&part;xj
= cκστ (a)uiκ (x), (29.54)
</p>
<p>where
</p>
<p>cκστ (a)=
[
&part;θ&minus;1κμ
&part;aλ
</p>
<p>&minus; &part;θ
&minus;1
κλ
</p>
<p>&part;aμ
</p>
<p>]
θμσ θλτ .
</p>
<p>Substituting Eq. (29.54) in Eq. (29.26) leads to (29.27). Now differentiate
both sides of Eq. (29.54) with respect to aρ to get
</p>
<p>&part;cκστ
</p>
<p>&part;aρ
uiκ = 0.
</p>
<p>With the assumption that the uiκ are linearly independent, conclude that the
structure &ldquo;constants&rdquo; are indeed constants.</p>
<p/>
</div>
<div class="page"><p/>
<p>950 29 Lie Groups and Lie Algebras
</p>
<p>29.10 Using
</p>
<p>x&prime;1 &equiv; φ1(x1, x2, x3; θ,ϕ,ψ),
x&prime;2 &equiv; φ2(x1, x2, x3; θ,ϕ,ψ),
x&prime;3 &equiv; φ3(x1, x2, x3; θ,ϕ,ψ),
</p>
<p>obtained from the multiplication of the column vector consisting of x1, x2,
and x3 by the Euler matrix of Example 5.2.7 and employing Eq. (29.25),
find the three components of the angular momentum.
</p>
<p>29.11 Find the invariant Haar measure of the general linear group in two
dimensions.
</p>
<p>29.12 Show that the invariant Haar measure for a compact group satisfies
dμg = dμg&minus;1 . Hint: Define a measure ν by dνg &equiv; dμg&minus;1 and show that ν
is left-invariant. Now use the uniqueness of the left-invariant Haar measure
for compact groups.
</p>
<p>29.13 Show that O(p,n&minus;p) is a group. Use this and the fact that η&minus;1 = η
to show that AηAt = η.
</p>
<p>29.14 Show that the orthogonal group O(p,n &minus; p) has dimension n(n &minus;
1)/2. Hint: Look at its algebra o(p,n&minus; p).
</p>
<p>29.15 Let x = (x1, x2, x3, x0) be a timelike (null, isotropic) 4-vector with
x0 &gt; 0. Let A be a proper orthochronous transformation. Show that x&prime; = Ax
is also timelike (null). Hint: Consider the zeroth component of x&prime; as an in-
ner product of (x1, x2, x3, x0) and another vector and use Schwarz inequal-
ity.
</p>
<p>29.16 Starting with the definition of each matrix, derive Eq. (29.45).
</p>
<p>29.17 Let D1 and D2 be derivations of a Lie algebra v. Show that D1D2 &equiv;
D1 ◦D2 is not a derivation, but [D1,D2] is.
</p>
<p>29.18 Let v be a Lie algebra. Verify that adX is a derivation of v for any
X &isin; v, and that ad[X,Y] = [adX, adY].
</p>
<p>29.19 Show that ψ : v &rarr; adv given by ψ(X) = adX is (a) a homomor-
phism, (b) kerψ is the center of v, and (c) adv is an ideal of Dv.
</p>
<p>29.20 Show that if ψ is an automorphism of v, then
</p>
<p>adψ(X) =ψ ◦ adX ◦ψ&minus;1 &forall;X &isin; v.
</p>
<p>Hint: Apply both sides to an arbitrary element of v.</p>
<p/>
</div>
<div class="page"><p/>
<p>29.3 Problems 951
</p>
<p>29.21 Show that for any Lie algebra,
</p>
<p>cijk = cljscrilcskr + clsicrlj csrk
is completely antisymmetric in all its indices.
</p>
<p>29.22 Show that the Killing form of v is invariant under all automorphisms
of v.
</p>
<p>29.23 Show that the translation generators Pj of the Poincar&eacute; algebra
p(p,n&minus; p) form a commutative ideal.
</p>
<p>29.24 Find the Cartan metrics for o(3,1) and p(3,1), and show directly that
the first is semisimple but the second is not.</p>
<p/>
</div>
<div class="page"><p/>
<p>30Representation of Lie Groups and LieAlgebras
</p>
<p>The representation of Lie groups is closely related to the representation of
their Lie algebras, and we shall discuss them later in this chapter. In the
case of compact groups, however, there is a well developed representation
theory, which we shall consider in the first section. Before discussing com-
pact groups, let us state a definition and a proposition that hold for all Lie
groups.
</p>
<p>Definition 30.0.1 A representation of a Lie group G on a Hilbert space H
is a Lie group homomorphism T :G&rarr;GL(H). Similarly, a representation
of the Lie algebra g is a Lie algebra homomorphism T : g&rarr; gl(H).
</p>
<p>The proposition we have in mind is the important Schur&rsquo;s lemma which
we state without proof (for a proof see [Baru 86, pp. 143&ndash;144]).
</p>
<p>Proposition 30.0.2 (Schur&rsquo;s lemma) A unitary representation T : G &rarr; Schur&rsquo;s lemma
GL(H) of a Lie group G is irreducible if and only if the only operators
commuting with all the Tg are scalar multiples of the unit operator.
</p>
<p>30.1 Representation of Compact Lie Groups
</p>
<p>In this section, we shall consider the representation of compact Lie groups,
because for such groups, many of the ideas developed for finite groups hold.
</p>
<p>Example 30.1.1 (Compactness of U(n), O(n), SU(n), and SO(n)) Identify
GL(n,C) with R2n
</p>
<p>2
via components. The map
</p>
<p>f :GL(n,C)&rarr;GL(n,C) given by f (A)= AA&dagger;
</p>
<p>is continuous because it is simply the products of elements of matrices. It
follows that f&minus;1(1) is closed, because the matrix 1 is a single point in R2n
</p>
<p>2
,
</p>
<p>which is therefore closed. f&minus;1(1) is also bounded, because
</p>
<p>AA&dagger; = 1 &rArr;
n&sum;
</p>
<p>j=1
aija
</p>
<p>&lowast;
kj = δik &rArr;
</p>
<p>n&sum;
</p>
<p>i,j=1
|aij |2 = n.
</p>
<p>S. Hassani, Mathematical Physics, DOI 10.1007/978-3-319-01195-0_30,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>953</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-01195-0_30">http://dx.doi.org/10.1007/978-3-319-01195-0_30</a></div>
</div>
<div class="page"><p/>
<p>954 30 Representation of Lie Groups and Lie Algebras
</p>
<p>Thus, f&minus;1(1) is a (2n2&minus;1)-dimensional sphere of radius &radic;n in R2n2 , which
is clearly bounded. The BWHB theorem (of Chap. 17) now implies that
f&minus;1(1) is compact. Now note that f&minus;1(1) consists of all matrices that have
their hermitian adjoints for an inverse; but these are precisely the set U(n)
of unitary matrices.
</p>
<p>Now consider the map det : U(n) &rarr; C. This map is also continuous,
implying that det&minus;1(1) is a closed subset of U(n). The boundedness of U(n)
implies that det&minus;1(1) is also bounded. Invoking the BWHB theorem again,
we conclude that det&minus;1(1)= SU(n), being closed and bounded, is compact.
</p>
<p>If instead of complex numbers, we restrict ourselves to the reals, O(n)
and SO(n) will replace U(n) and SU(n), respectively.
</p>
<p>The result of the example above can be summarized:
</p>
<p>Box 30.1.2 The unitary U(n), orthogonal O(n), special unitary
SU(n), and special orthogonal SO(n) groups are all compact.
</p>
<p>We now start our study of the representations of compact Lie groups. We
first show that we can always assume that the representation is unitary.
</p>
<p>All representations of
</p>
<p>compact groups can be
</p>
<p>made unitary.
Theorem 30.1.3 Let T : G &rarr; GL(H) be any representation of the com-
pact group G. There exists a new inner product in H relative to which T is
unitary.
</p>
<p>Proof Let 〈|〉 be the initial inner product. Define a new inner product (|) by
</p>
<p>(u|v)&equiv;
&int;
</p>
<p>G
</p>
<p>〈Tgu|Tgv〉dμg
</p>
<p>where dμg is the Haar measure, which is both left- and right-invariant. The
reader may check that this is indeed an inner product. For every h &isin;G, we
have
</p>
<p>(Thu|Thv)=
&int;
</p>
<p>G
</p>
<p>〈TgThu|TgThv〉dμg
</p>
<p>=
&int;
</p>
<p>G
</p>
<p>〈Tghu|Tghv〉dμg (because T is a representation)
</p>
<p>=
&int;
</p>
<p>G
</p>
<p>〈Tghu|Tghv〉dμgh (because μg is right invariant)
</p>
<p>= (u|v).
</p>
<p>This shows that Th is unitary for all h &isin;G. �
</p>
<p>From now on, we shall restrict our discussion to unitary representations
of compact groups.
</p>
<p>The study of representations of compact groups is facilitated by the fol-
lowing construction:</p>
<p/>
</div>
<div class="page"><p/>
<p>30.1 Representation of Compact Lie Groups 955
</p>
<p>Definition 30.1.4 Let T : G &rarr; GL(H) be a unitary representation of the
compact group G and |u〉 &isin;H a fixed vector. The Weyl operator Ku asso- Weyl operator for a
</p>
<p>compact Lie groupciated with |u〉 is defined as
</p>
<p>Ku =
&int;
</p>
<p>G
</p>
<p>|Tgu〉〈Tgu|dμg. (30.1)
</p>
<p>The essential properties of the Weyl operator are summarized in the fol-
lowing:
</p>
<p>Proposition 30.1.5 Let T :G&rarr;GL(H) be a unitary representation of the
compact group G. Then the Weyl operator has the following properties
</p>
<p>1. Ku is hermitian.
2. KuTg = TgKu for all g &isin;G. Therefore, any eigenspace of Ku is an in-
</p>
<p>variant subspace of all Tg&rsquo;s.
3. Ku is a Hilbert-Schmidt operator.
</p>
<p>Proof Statement (1), in the form 〈w|Ku|v〉&lowast; = 〈v|Ku|w〉, follows directly
from the definition.
</p>
<p>(2) From Tg
&int;
G
|Txu〉〈Txu|dμx =
</p>
<p>&int;
G
|TgTxu〉〈Txu|dμx , the fact that T
</p>
<p>is a representation (therefore, TgTx = Tgx ), and redefining the integration
variable to y = gx, we get
</p>
<p>TgKu =
&int;
</p>
<p>G
</p>
<p>|Tyu〉〈Tg&minus;1yu| dμg&minus;1y︸ ︷︷ ︸
=dμy
</p>
<p>=
&int;
</p>
<p>G
</p>
<p>|Tyu〉〈Tg&minus;1Tyu|dμy,
</p>
<p>where we used the left invariance of μ and the fact that T is a representation.
Unitarity of T now gives
</p>
<p>TgKu =
&int;
</p>
<p>G
</p>
<p>|Tyu〉
&lang;
T&dagger;gTyu
</p>
<p>∣∣dμy =
&int;
</p>
<p>G
</p>
<p>|Tyu〉〈Tyu|Tg dμy = KuTg.
</p>
<p>(3) Recall that an operator A &isin; L(H) is Hilbert-Schmidt if &sum;&infin;i=1 ‖A|ei〉‖2
is finite for any orthonormal basis {|ei〉} of H. In the present case, we have
</p>
<p>Ku|ei〉 =
&int;
</p>
<p>G
</p>
<p>|Txu〉〈Txu|ei〉dμx .
</p>
<p>Therefore,
</p>
<p>&infin;&sum;
</p>
<p>i=1
</p>
<p>∥∥Ku|ei〉
∥∥2 =
</p>
<p>&infin;&sum;
</p>
<p>i=1
</p>
<p>(&int;
</p>
<p>G
</p>
<p>〈ei |Tyu〉〈Tyu|dμy
)(&int;
</p>
<p>G
</p>
<p>|Txu〉〈Txu|ei〉dμx
)
</p>
<p>=
&infin;&sum;
</p>
<p>i=1
</p>
<p>&int;
</p>
<p>G
</p>
<p>&int;
</p>
<p>G
</p>
<p>〈ei |Tyu〉〈Tyu|Txu〉〈Txu|ei〉dμx dμy .
</p>
<p>If we switch the order of summation and integration and use
</p>
<p>&infin;&sum;
</p>
<p>i=1
〈Txu|ei〉〈ei |Tyu〉 = 〈Txu|Tyu〉,</p>
<p/>
</div>
<div class="page"><p/>
<p>956 30 Representation of Lie Groups and Lie Algebras
</p>
<p>we obtain
</p>
<p>&infin;&sum;
</p>
<p>i=1
</p>
<p>∥∥Ku|ei〉
∥∥2 =
</p>
<p>&int;
</p>
<p>G
</p>
<p>&int;
</p>
<p>G
</p>
<p>∣∣〈Tyu|Txu〉
∣∣2dμx dμy,
</p>
<p>and using the Schwarz inequality in the integral yields
</p>
<p>&infin;&sum;
</p>
<p>i=1
</p>
<p>∥∥Ku|ei〉
∥∥2 &le;
</p>
<p>&int;
</p>
<p>G
</p>
<p>&int;
</p>
<p>G
</p>
<p>〈Txu|Txu〉〈Tyu|Tyu〉dμx dμy
</p>
<p>=
&int;
</p>
<p>G
</p>
<p>&int;
</p>
<p>G
</p>
<p>〈u|u〉〈u|u〉dμx dμy (because rep. is unitary)
</p>
<p>= ‖u‖4
&int;
</p>
<p>G
</p>
<p>dμx
</p>
<p>&int;
</p>
<p>G
</p>
<p>dμy = ‖u‖4V 2G &lt;&infin;,
</p>
<p>where VG is the finite volume of G. �
</p>
<p>Historical Notes
</p>
<p>Hermann Klaus Hugo Weyl (1885&ndash;1955) attended the gymnasium at Altona and, on
</p>
<p>Hermann Klaus Hugo
</p>
<p>Weyl 1885&ndash;1955
</p>
<p>the recommendation of the headmaster of his gymnasium, who was a cousin of Hilbert,
decided at the age of eighteen to enter the University of G&ouml;ttingen. Except for one year
at Munich he remained at G&ouml;ttingen, as a student and later as Privatdozent, until 1913,
when he became professor at the University of Zurich. After Klein&rsquo;s retirement in 1913,
Weyl declined an offer to be his successor at G&ouml;ttingen but accepted a second offer in
1930, after Hilbert had retired. In 1933 he decided he could no longer remain in Nazi
Germany and accepted a position at the Institute for Advanced Study at Princeton, where
he worked until his retirement in 1951. In the last years of his life he divided his time
between Zurich and Princeton.
Weyl undoubtedly was the most gifted of Hilbert&rsquo;s students. Hilbert&rsquo;s thought dominated
the first part of his mathematical career; and although later he sharply diverged from
his master, particularly on questions related to foundations of mathematics, Weyl always
shared his convictions that the value of abstract theories lies in their success in solving
classical problems and that the proper way to approach a question is through a deep
analysis of the concepts it involves rather than by blind computations.
Weyl arrived at G&ouml;ttingen during the period when Hilbert was creating the spectral the-
ory of self-adjoint operators, and spectral theory and harmonic analysis were central in
Weyl&rsquo;s mathematical research throughout his life. Very soon, however, he considerably
broadened the range of his interests, including areas of mathematics into which Hilbert
had never penetrated, such as the theory of Lie groups and the analytic theory of num-
bers, thereby becoming one of the most universal mathematicians of his generation. He
also had an important role in the development of mathematical physics, the field to which
his most famous books, Raum, Zeit und Materie (1918), on the theory of relativity, and
Gruppentheorie und Quantenmechanik (1928), are devoted.
Weyl&rsquo;s versatility is illustrated in a particularly striking way by the fact that immediately
after some original advances in number theory (which he obtained in 1914), he spent more
than ten years as a geometer&mdash;a geometer in the most modern sense of the word, uniting
in his methods topology, algebra, analysis, and geometry in a display of dazzling virtu-
osity and uncommon depth reminiscent of Riemann. Drawn by war mobilization into the
German army, Weyl did not resume his interrupted work when he was allowed to return to
civilian life in 1916. At Zurich he had worked with Einstein for one year, and he became
keenly interested in the general theory of relativity, which had just been published; with
his characteristic enthusiasm he devoted most of the next five years to exploring the math-
ematical framework of the theory. In these investigations Weyl introduced the concept of
what is now called a linear connection, linked not to the Lorentz group of orthogonal
transformations, but to the enlarged group of conformal transformations; he even thought
for a time that this would give him a unified theory of gravitation and electromagnetism,
the forerunner of what is now called gauge theories.</p>
<p/>
</div>
<div class="page"><p/>
<p>30.1 Representation of Compact Lie Groups 957
</p>
<p>Weyl&rsquo;s use of tensor calculus in his work on relativity led him to reexamine the basic
methods of that calculus and, more generally, of classical invariant theory that had been
its forerunner but had fallen into near oblivion after Hilbert&rsquo;s work of 1890. On the other
hand, his semiphilosophical, semimathematical ideas on the general concept of &ldquo;space&rdquo;
in connection with Einstein&rsquo;s theory had directed his investigations to generalizations of
Helmholtz&rsquo;s problem of characterizing Euclidean geometry by properties of &ldquo;free mobil-
ity.&rdquo; From these two directions Weyl was brought into contact with the theory of linear
representations of Lie groups; his papers on the subject (1925&ndash;1927) certainly repre-
sent his masterpiece and must be counted among the most influential works in twentieth-
century mathematics.
Based on the early 1900s works of Frobenius, I. Schur, and A. Young, Weyl inaugurated
a new approach for the representation of continuous groups by focusing his attention on
Lie groups, rather than Lie algebras.
Very few of Weyl&rsquo;s 150 published books and papers&mdash;even those chiefly of an expository
character&mdash;lack an original idea or a fresh viewpoint. The influence of his works and of
his teaching was considerable: He proved by his example that an &ldquo;abstract&rdquo; approach to
mathematics is perfectly compatible with &ldquo;hard&rdquo; analysis and, in fact, can be one of the
most powerful tools when properly applied.
Weyl was one of that rare breed of modern mathematician whose contribution to physics
was also substantial. In an interview with a reporter in 1929, Dirac is asked the following
question: &ldquo;. . . I want to ask you something more: They tell me that you and Einstein are
the only two real sure-enough high-brows and the only ones who can really understand
each other. I won&rsquo;t ask you if this is straight stuff, for I know you are too modest to
admit it. But I want to know this&mdash;Do you ever run across a fellow that even you can&rsquo;t
understand?&rdquo; To this Dirac replies one word: &ldquo;Weyl.&rdquo;
Weyl had a lifelong interest in philosophy and metaphysics, and his mathematical activity
was seldom free from philosophical undertones or afterthoughts. At the height of the
controversy over the foundations of mathematics, between the formalist school of Hilbert
and the intuitionist school of Brouwer, he actively fought on Brouwer&rsquo;s side. His own
comment, stated somewhat jokingly, sums up his personality: &ldquo;My work always tried to
unite the truth with the beautiful, but when I had to choose one or the other, I usually
chose the beautiful.&rdquo;
</p>
<p>We now come to the most fundamental theorem of representation theory
of compact Lie groups. Before stating and proving this theorem, we need
the following lemma:
</p>
<p>Lemma 30.1.6 Let T :G&rarr; GL(H) be an irreducible unitary representa-
tion of a compact Lie group G. For any nonzero |u〉, |v〉 &isin;H, we have
</p>
<p>1
</p>
<p>‖u‖2‖v‖2
&int;
</p>
<p>G
</p>
<p>∣∣〈v|Tx |u〉
∣∣2 dμx = c, (30.2)
</p>
<p>where c &gt; 0 is a constant independent of |u〉 and |v〉.
</p>
<p>Proof By Schur&rsquo;s lemma and (2) of Proposition 30.1.5, Ku = λ(u)1. There-
fore, on the one hand,
</p>
<p>〈v|Ku|v〉 = λ(u)‖v‖2. (30.3)
</p>
<p>On the other hand,
</p>
<p>〈v|Ku|v〉 =
&int;
</p>
<p>G
</p>
<p>〈v|Txu〉〈Txu|v〉dμx =
&int;
</p>
<p>G
</p>
<p>∣∣〈v|Tx |u〉
∣∣2 dμx . (30.4)</p>
<p/>
</div>
<div class="page"><p/>
<p>958 30 Representation of Lie Groups and Lie Algebras
</p>
<p>Moreover, if we use dμg = dμg&minus;1 (see Problem 29.12), then
</p>
<p>〈v|Ku|v〉 =
&int;
</p>
<p>G
</p>
<p>〈v|Tx |u〉〈u|T&dagger;x |v〉dμx =
&int;
</p>
<p>G
</p>
<p>〈v|Tx |u〉〈u|T&minus;1x |v〉dμx
</p>
<p>=
&int;
</p>
<p>G
</p>
<p>〈u|Tx&minus;1 |v〉〈v|Tx |u〉dμx =
&int;
</p>
<p>G
</p>
<p>〈u|Ty |v〉〈v|Ty&minus;1 |u〉dμy&minus;1
</p>
<p>=
&int;
</p>
<p>G
</p>
<p>〈u|Ty |v〉 〈v|T&dagger;|u〉︸ ︷︷ ︸
=〈Tyv|u〉
</p>
<p>dμy&minus;1︸ ︷︷ ︸
dμy
</p>
<p>= 〈u|Kv|u〉.
</p>
<p>This equality plus Eq. (30.3) gives
</p>
<p>λ(u)‖v‖2 = λ(v)‖u‖2 &rArr; λ(v)‖v‖2 =
λ(u)
</p>
<p>‖u‖2 .
</p>
<p>Since |u〉 and |v〉 are arbitrary, we conclude that λ(u)= c‖u‖2 for all |u〉 &isin;
H, where c is a constant. Equations (30.3) and (30.4) now yield Eq. (30.2).
If we let |u〉 = |v〉 in Eq. (30.4) and use (30.3), we obtain
</p>
<p>&int;
</p>
<p>G
</p>
<p>∣∣〈u|Tx |u〉
∣∣2 dμx = λ(u)‖u‖2 = c‖u‖4.
</p>
<p>That c &gt; 0 follows from the fact that the LHS is a nonnegative continuous
function that has at least one strictly positive value in its integration range,
namely at x = e, the identity. �
</p>
<p>Theorem 30.1.7 Every irreducible unitary representation of a com-
pact Lie group is finite-dimensional.
</p>
<p>Proof Let {|ei〉}ni=1 be any set of orthonormal vectors in H. Then, uni-
tarity of Tg implies that {Tg|ei〉}ni=1 is also an orthonormal set. Applying
Lemma 30.1.6 to |ej 〉 and |e1〉, we obtain
</p>
<p>&int;
</p>
<p>G
</p>
<p>∣∣〈e1|Tx |ej 〉
∣∣2 dμx = c.
</p>
<p>Now sum over j to get
</p>
<p>nc=
n&sum;
</p>
<p>j=1
</p>
<p>&int;
</p>
<p>G
</p>
<p>∣∣〈e1|Tx |ej 〉
∣∣2 dμx =
</p>
<p>&int;
</p>
<p>G
</p>
<p>n&sum;
</p>
<p>j=1
</p>
<p>∣∣〈e1|Txej 〉
∣∣2 dμx
</p>
<p>&le;
&int;
</p>
<p>G
</p>
<p>〈e1|e1〉dμx = VG,
</p>
<p>where we used the Parseval inequality [Eq. (7.3)] as applied to the vector
|e1〉 and the orthonormal set {Tg|ei〉}ni=1. Since both VG and c are finite, n
must be finite as well. Thus, H cannot have an infinite set of orthonormal
vectors. �</p>
<p/>
</div>
<div class="page"><p/>
<p>30.1 Representation of Compact Lie Groups 959
</p>
<p>So far, we have discussed irreducible representations. What can we say
about arbitrary representations? We recall that in the case of finite groups,
every representation can be written as a direct sum of irreducible represen-
tations. Is this also true for compact Lie groups?
</p>
<p>Firstly, we note that the Weyl operator, being Hilbert-Schmidt, is nec-
essarily compact. It is also hermitian. Therefore, by the spectral theo-
rem, its eigenspaces span the carrier space H. Specifically, we can write
H =M0 &oplus;
</p>
<p>&sum;N
j=1 &oplus;Mj , where M0 is the eigenspace corresponding to the
</p>
<p>zero eigenvalue of Ku, and N could be infinity.
Secondly, from the relation 〈v|Ku|v〉 = c‖u‖2‖v‖2 and the fact that c �= 0
</p>
<p>and |u〉 �= 0, we conclude that Ku cannot have any nonzero eigenvector for
its zero eigenvalue. It follows that M0 contains only the zero vector. There-
fore, if H is infinite-dimensional, then N =&infin;.
</p>
<p>Thirdly, consider any representation T of G. Because Ku commutes with
all Tg , each eigenspace of Ku is an invariant subspace under T . If a subspace
U is invariant under T , then U &cap;Mj , a subspace of Mj , is also invariant
(reader, please verify!). Thus, all invariant subspaces of G are reducible
to invariant subspaces of eigenspaces of Ku. In particular, all irreducible
invariant subspaces of T are subspaces of eigenspaces of Ku.
</p>
<p>Lastly, since all Mj are finite-dimensional, we can use the procedure
used in the case of finite groups and decompose Mj into irreducible invari-
ant subspaces of T . We have just shown the following result:
</p>
<p>Theorem 30.1.8 Every unitary representation T of a compact Lie
group G is a direct sum of irreducible finite-dimensional unitary rep-
resentations.
</p>
<p>By choosing a basis for the finite-dimensional invariant subspaces of T ,
we can represent each Tg by a matrix. Therefore,
</p>
<p>Box 30.1.9 Compact Lie groups can be represented by matrices.
</p>
<p>As in the case of finite groups, one can work with matrix elements and
characters of representations. The only difference is that summations are
replaced with integration and order of the group |G| is replaced with VG,
which we take to be unity.1 For example, Eq. (24.6) becomes
</p>
<p>&int;
</p>
<p>G
</p>
<p>T (α)(g)XT (β)
(
g&minus;1
</p>
<p>)
dμg = λXδαβ1, (30.5)
</p>
<p>and the analogue of Eq. (24.8) is
&int;
</p>
<p>G
</p>
<p>T
(α)
il (g)T
</p>
<p>(β)&lowast;
jm (g) dμg =
</p>
<p>1
</p>
<p>nα
δmlδαβδij . (30.6)
</p>
<p>1This can always be done by rescaling the volume element.</p>
<p/>
</div>
<div class="page"><p/>
<p>960 30 Representation of Lie Groups and Lie Algebras
</p>
<p>Characters satisfy similar relations: Eq. (24.11) becomes
&int;
</p>
<p>G
</p>
<p>χ (α)(g)χ (β)&lowast;(g) dμg = δαβ , (30.7)
</p>
<p>and the useful Eq. (24.16) turns into
&int;
</p>
<p>G
</p>
<p>∣∣χ(g)
∣∣2 dμg =
</p>
<p>&sum;
</p>
<p>α
</p>
<p>m2α. (30.8)
</p>
<p>This formula can be used to test for irreducibility of a representation: If the
integral is unity, the representation is irreducible; otherwise, it is reducible.
</p>
<p>Finally, we state the celebrated Peter-Weyl theorem (for a proof, see
[Baru 86, pp. 172&ndash;173])
</p>
<p>Peter-Weyl theorem
</p>
<p>Theorem 30.1.10 (Peter-Weyl theorem) The functions
</p>
<p>&radic;
nαT
</p>
<p>(α)
ij (g), &forall;α and 1 &le; i, j &le; nα,
</p>
<p>form a complete set of functions in L2(G), the Hilbert space of
square-integrable functions on G.
</p>
<p>If u &isin;L2(G), we can write
</p>
<p>u(g)=
&sum;
</p>
<p>α
</p>
<p>nα&sum;
</p>
<p>i,j
</p>
<p>bαijT
(α)
ij (g) where b
</p>
<p>α
ij = nα
</p>
<p>&int;
</p>
<p>G
</p>
<p>u(g)T
(α)&lowast;
ij (g) dμg.
</p>
<p>(30.9)
</p>
<p>Example 30.1.11 Equation (30.9) is the generalization of the Fourier series
expansion of functions. The connection with Fourier series becomes more
transparent if we consider a particular compact group. The unit circle S1 is
a one-dimensional abelian compact 1-parameter Lie group. In fact, fixing an
&ldquo;origin&rdquo; on the circle, any other point can be described by the parameter θ ,
the angular distance from the point to the origin. S1 is obviously abelian; it is
also compact, because it is a bounded closed region of R2 (BWHB theorem).
By Theorem 24.3.3, which holds for all Lie groups, all irreducible repre-
</p>
<p>The Peter-Weyl theorem
</p>
<p>is the generalization of
</p>
<p>the Fourier series
</p>
<p>expansion of periodic
</p>
<p>functions.
</p>
<p>sentations of S1 are 1-dimensional. So T (α)ij (g) &rarr; T (α)(θ). Furthermore,
T (α)(θ)T (α)(θ &prime;) = T (α)(θ + θ &prime;). Differentiating both sides with respect to
θ &prime; at θ &prime; = 0 yields the differential equation
</p>
<p>T (α)(θ)
dT (α)
</p>
<p>dθ &prime;
</p>
<p>∣∣∣∣
θ &prime;=0︸ ︷︷ ︸
</p>
<p>&equiv;a
</p>
<p>= dT
(α)
</p>
<p>dy
</p>
<p>∣∣∣∣
y=θ
</p>
<p>&equiv; dT
(α)
</p>
<p>dθ
, y &equiv; θ + θ &prime;.
</p>
<p>The solution to this DE is Aeaθ . Since T (α) are unitary, and since a 1-
dimensional unitary matrix must look like eiϕ , we must have A = 1. Fur-
thermore, θ and θ + 2π are identified on the unit circle; therefore, we must</p>
<p/>
</div>
<div class="page"><p/>
<p>30.1 Representation of Compact Lie Groups 961
</p>
<p>conclude that a is i times an integer n, which determines the irreducible
representation. We label the irreducible representation by n and write
</p>
<p>T (n)(θ)= einθ , n= 0 &plusmn; 1 &plusmn; 2 . . . .
</p>
<p>The Peter-Weyl theorem now becomes the rule of Fourier series expansion
of periodic functions. This last property follows from the fact that any func-
tion u : S1 &rarr;R is necessarily periodic.
</p>
<p>There are many occasions in physics where the state functions describing
physical quantities transform irreducibly under the action of a Lie group
(which we assume to be compact). Often this Lie group also acts on the
underlying space-time manifold. So we have a situation in which a Lie group
G acts on a Euclidean space Rn as well as on the space of (square-integrable)
functions L(Rn). Therefore, the functions {φ(α)i (x)}, belonging to the αth
irreducible representation transform among themselves not only because of
the index i, but also because of the argument x &isin;Rn.
</p>
<p>To see the connection between physics and representation theory, con-
sider the transformation of the simplest case, a scalar function. As a concrete
example, choose temperature. To observer O at the corner of a room 8 me-
ters long, 6 meters wide, and 3 meters high, the temperature of the center of
the room is given by θ(4,3,1.5) where θ(x, y, z) is a function that gives O
the temperature of various points of the room. Observer O &prime; is sitting in the
middle of the floor, so that the center of the room has coordinates (0,0,1.5).
O &prime; also has a function that gives her the temperature at various points. But
this function must necessarily be different from θ because of the different
coordinates the same points have for O and O &prime;. Calling this function θ &prime;, we
have θ &prime;(0,0,1.5)= θ(4,3,1.5), and in general,
</p>
<p>θ &prime;
(
x&prime;, y&prime;, z&prime;
</p>
<p>)
= θ(x, y, z),
</p>
<p>where (x&prime;, y&prime;, z&prime;) describes the same point for O &prime; that (x, y, z) describes
for O .
</p>
<p>In the context of representation theory, we can think of (x&prime;, y&prime;, z&prime;) as the
transformed coordinates obtained as a result of the action of some group:
(x&prime;, y&prime;, z&prime;)= g &middot; (x, y, z), or x&prime; = g &middot; x. So, the equation above can be written
as
</p>
<p>θ &prime;
(
x&prime;
)
= θ(x)= θ
</p>
<p>(
g&minus;1 &middot; x&prime;
</p>
<p>)
or θ &prime;(x)= θ
</p>
<p>(
g&minus;1 &middot; x
</p>
<p>)
.
</p>
<p>It is natural to call θ &prime; the transform of θ under the action of g and write θ &prime; =
Tgθ . This is one way of constructing a representation [see the comments
after Eq. (24.1)]. Instead of g&minus;1 on the left, one could act with g on the
right.
</p>
<p>When the physical quantity is not a scalar, it is natural to group together
the smallest set of functions that transform into one another. This leads to
the set of functions that transform according to a row of an irreducible rep-
resentation of the group. In some sense, this situation is a combination of</p>
<p/>
</div>
<div class="page"><p/>
<p>962 30 Representation of Lie Groups and Lie Algebras
</p>
<p>(24.1) and (24.35). The reader may verify that
</p>
<p>Tgφ
(α)
i (x)=
</p>
<p>nα&sum;
</p>
<p>j=1
T
(α)
ji (g)φ
</p>
<p>(α)
j
</p>
<p>(
x &middot; g&minus;1
</p>
<p>)
(30.10)
</p>
<p>defines a representation of G.
We now use Box 29.1.31 to construct an irreducible representation of
</p>
<p>the Lie algebra of G from Eq. (30.10). By the definition of the infinitesimal
action, we let g = exp(ξ t) and differentiate both sides with respect to t at
t = 0. This yields
</p>
<p>d
</p>
<p>dt
Texp(ξ t)φ
</p>
<p>(α)
i (x)
</p>
<p>∣∣∣∣
t=0︸ ︷︷ ︸
</p>
<p>&equiv;&sum;j Dj i(ξ)φ
(α)
j (x)
</p>
<p>=
nα&sum;
</p>
<p>j=1
</p>
<p>d
</p>
<p>dt
</p>
<p>{
T
(α)
ji
</p>
<p>(
exp(ξ t)
</p>
<p>)
φ
(α)
j
</p>
<p>(
x &middot; exp(&minus;ξ t)
</p>
<p>)}∣∣∣∣
t=0
</p>
<p>=
nα&sum;
</p>
<p>j=1
</p>
<p>d
</p>
<p>dt
T
(α)
ji
</p>
<p>(
exp(ξ t)
</p>
<p>)∣∣∣∣
t=0
</p>
<p>φ
(α)
j
</p>
<p>(
x &middot; exp(&minus;ξ0)︸ ︷︷ ︸
</p>
<p>=x
</p>
<p>)
</p>
<p>+
nα&sum;
</p>
<p>j=1
T
(α)
ji
</p>
<p>(
exp(ξ0)
</p>
<p>)
︸ ︷︷ ︸
</p>
<p>=δj i
</p>
<p>d
</p>
<p>dt
φ
(α)
j
</p>
<p>(
x &middot; exp(&minus;ξ t)
</p>
<p>)∣∣∣∣
t=0
</p>
<p>,
</p>
<p>where we have defined the matrices Dji(ξ ) for the LHS. The derivative in
</p>
<p>the first sum is simply T(α)ji (ξ) the representation of the generator ξ of the
</p>
<p>1-parameter group of transformations in the space of functions {φ(α)i }. The
derivative in the second sum can be found by writing x&prime;(t) = x &middot; exp(&minus;ξ t)
and differentiating as follows:
</p>
<p>d
</p>
<p>dt
φ
(α)
j
</p>
<p>(
x&prime;(t)
</p>
<p>)∣∣∣∣
t=0
</p>
<p>&equiv; d
dt
</p>
<p>φ
(α)
j
</p>
<p>(
x&prime;1(t), . . . , x&prime;n(t)
</p>
<p>)∣∣∣∣
t=0
</p>
<p>= &part;kφ(α)j
d
</p>
<p>dt
</p>
<p>(
x&prime;k(t)
</p>
<p>)∣∣∣∣
t=0
</p>
<p>= &part;kφ(α)j ukμ(x)
daμ
</p>
<p>dt
&equiv; &part;kφ(α)j Xk(x; ξ)&equiv; &part;νφ
</p>
<p>(α)
j X
</p>
<p>ν(x; ξ),
</p>
<p>where we used Eq. (29.23) and defined Xk(x; ξ) by the last equality. We
also changed the coordinate index to Greek to avoid confusing it with the
index of the functions. Collecting everything together, we obtain
</p>
<p>nα&sum;
</p>
<p>j=1
Dij (ξ )φ
</p>
<p>(α)
j (x)=
</p>
<p>nα&sum;
</p>
<p>j=1
T
(α)
ij (ξ)φ
</p>
<p>(α)
j (x)+
</p>
<p>nα&sum;
</p>
<p>j=1
δijX
</p>
<p>ν(x; ξ)
&part;φ
</p>
<p>(α)
j
</p>
<p>&part;xν
,
</p>
<p>or, since φ(α)j =
&sum;
</p>
<p>k φ
(α)
k (&part;/&part;φ
</p>
<p>(α)
k )φ
</p>
<p>(α)
j ,
</p>
<p>Dij (ξ)= T(α)ij (ξ)φ
(α)
k (x)
</p>
<p>&part;
</p>
<p>&part;φ
(α)
k
</p>
<p>+ δijXν(x; ξ)
&part;
</p>
<p>&part;xν
, (30.11)
</p>
<p>where Xν(x; ξ) is the νth component of the infinitesimal generator of the ac-
tion induced by ξ &isin; g. We shall put Eq. (30.11) to good use when we discuss
symmetries and conservation laws in Chap. 33. The derivative with respect</p>
<p/>
</div>
<div class="page"><p/>
<p>30.2 Representation of the General Linear Group 963
</p>
<p>to the functions, although meaningless at this point, will be necessary when
we discuss conservation laws.
</p>
<p>30.2 Representation of the General Linear Group
</p>
<p>GL(V) is not a compact group, but we can use the experience we gained in
the analysis of the symmetric group to find the irreducible representations
of GL(V). The key is to construct tensor product spaces of V&mdash;which, as the
reader may verify, is a carrier space of GL(V)&mdash;and look for its irreducible
subspaces. In fact, if r is an arbitrary positive integer, T :G&rarr; GL(V) is a
representation, and
</p>
<p>V
&otimes;r &equiv; V&otimes; &middot; &middot; &middot; &otimes;V︸ ︷︷ ︸
</p>
<p>r times
</p>
<p>,
</p>
<p>then T &otimes;r :G&rarr;GL(V&otimes;r), given by
[
T &otimes;r(g)
</p>
<p>]
(v1, . . . ,vr)&equiv; T&otimes;rg (v1, . . . ,vr)= Tg(v1)&otimes; &middot; &middot; &middot; &otimes; Tg(vr),
</p>
<p>is also a representation. In particular, considering V as the (natural) carrier
space for GL(V), we conclude that T &otimes;r :GL(V)&rarr;GL(V&otimes;r) is a represen-
tation.
</p>
<p>This tensor product representation is reducible, because as is evident
from its definition, T&otimes;rg preserves any symmetry of the tensor it acts on. For
example, the subspace of the full nr -dimensional tensor product space&mdash;
with n being the dimension of V&mdash;consisting of the completely symmetric
tensors of the type
</p>
<p>ts &equiv;
&sum;
</p>
<p>π&isin;Sr
vπ(1) &otimes; vπ(2) &otimes; &middot; &middot; &middot; &otimes; vπ(r)
</p>
<p>is invariant. Similarly, the subspace consisting of the completely antisym-
metric tensor products&mdash;the r-fold wedge products&mdash;is invariant.
</p>
<p>To reduce V&otimes;r , we choose a basis {ek}nk=1 for V. Then the collection of
nr tensor products {ek1 &otimes;&middot; &middot; &middot;&otimes;ekr }, where each ki runs from 1 to n, is a basis
for V&otimes;r . An invariant subspace of V&otimes;r is a span of linear combinations of
certain of these basis vectors. Since the only thing that distinguishes among
{ek1 &otimes; &middot; &middot; &middot; &otimes; ekr } is a permutation of the r labels, we start to see the connec-
tion between the reduction of V&otimes;r and Sr . This connection becomes more
evident if we recall that the left multiplication of the group algebra of Sr
by its elements provides the regular representation, which is reducible. The
irreducible representations are the minimal ideals of the algebra generated
by the Young operators.
</p>
<p>The same idea works here as well: Certain linear combination of the basis
vectors of V&otimes;r obtained by permutations can serve as the basis vectors for
irreducible representations of GL(V). Let us elaborate on this. Recall that
a Young operator of Sr is written in the form Y = QP where Q and P
are linear combinations of permutations in Sr . Y has the property that if
</p>
<p>connection between the
</p>
<p>Young tableaux and
</p>
<p>irreducible
</p>
<p>representations of
</p>
<p>GL(V)one operates on it (via left multiplication) with all permutations of Sr , one</p>
<p/>
</div>
<div class="page"><p/>
<p>964 30 Representation of Lie Groups and Lie Algebras
</p>
<p>generates a minimal ideal, i.e., an irreducible representation of Sr . Now let
Y be a Young operator that acts on the indices (k1, . . . , kr), giving linear
combinations of the basis vectors of V&otimes;r . From the minimality of the ideal
generated by Y and the fact that operators in GL(V) permute the factors in
ek1 &otimes; &middot; &middot; &middot; &otimes; ekr in all possible ways, it should now be clear that if we choose
any single basis vector ek1 &otimes; &middot; &middot; &middot; &otimes; ekr , then Y(ek1 &otimes; &middot; &middot; &middot; &otimes; ekr ) generates an
irreducible representation of GL(V). We therefore have the following:
</p>
<p>Theorem 30.2.1 Let {ek}nk=1 be any basis for V. Let Y = QP be the
Young operator of Sr that permutes (and takes linear combinations
of) the basis vectors {ek1 &otimes; &middot; &middot; &middot; &otimes; ekr }. Then for any given such basis
vector, the vectors
</p>
<p>{
T&otimes;rg Y(ek1 &otimes; &middot; &middot; &middot; &otimes; ekr ) | g &isin;GL(V)
</p>
<p>}
</p>
<p>span an irreducible subspace of V&otimes;r .
</p>
<p>A basis of such an irreducible representation can be obtained by taking
into account all the Young tableaux associated with the irreducible represen-
tation. But which of the symmetry types will be realized for given values of
n and r? Clearly, the Young tableau should not contain more than n rows,
because then one of the symbols will be repeated in a column, and the Young
operator will vanish due to the antisymmetry in its column indices. We can
therefore restrict the partition (λ) to
</p>
<p>(λ)= (λ1, λ2, . . . , λn), λ1 + &middot; &middot; &middot; + λn = r, λ1 &ge; λ2 &ge; &middot; &middot; &middot; &ge; λn &ge; 0.
</p>
<p>Let us consider an example for clarification.
</p>
<p>Example 30.2.2 First, let n= r = 2. The tensor product space has 22 = 4
dimensions. To reduce it, we consider the Young operators, which corre-
spond to e + (k1, k2) and e &minus; (k1, k2). Let us denote these operators by
Y1 and Y2, respectively. By applying each one to a generic basis vector
ek1 &otimes; ek2 , we can generate all the irreducible representations. The first oper-
ator gives
</p>
<p>Y1(ek1 &otimes; ek2)= ek1 &otimes; ek2 + ek2 &otimes; ek1,
where k1 and k2 can be 1 or 2. For k1 = k2 = 1, we get 2e1 &otimes; e1. For k1 = 1,
k2 = 2, or k1 = 2, k2 = 1, we get e1 &otimes; e2 + e2 &otimes; e1. Finally, for k1 = k2 = 2,
we get 2e2 &otimes; e2. Altogether, we obtain 3 linearly independent vectors that
are completely symmetric.
</p>
<p>When the second operator acts on a generic basis vector, it gives
</p>
<p>Y2(ek1 &otimes; ek2)= ek1 &otimes; ek2 &minus; ek2 &otimes; ek1 .
</p>
<p>The only time that this is not zero is when k1 and k2 are different. In ei-
ther case, we get &plusmn;(e1 &otimes; e2 &minus; e2 &otimes; e1). This subspace is therefore one-
dimensional.</p>
<p/>
</div>
<div class="page"><p/>
<p>30.2 Representation of the General Linear Group 965
</p>
<p>The reduction of the tensor product space can therefore be written as
</p>
<p>V
&otimes;2 = Span{e1 &otimes; e1, e1 &otimes; e2 + e2 &otimes; e1, e2 &otimes; e2}︸ ︷︷ ︸
</p>
<p>3D symmetric subspace
</p>
<p>&oplus; Span{e1 &otimes; e2 &minus; e2 &otimes; e1}︸ ︷︷ ︸
1D antisymmetric subspace
</p>
<p>.
</p>
<p>Next, let us consider the case of n= 2, r = 3. The tensor product space
has 23 = 8 dimensions. To reduce it, we need to consider all Young opera-
tors of S3. There are four of these, corresponding to the following tableaux:
</p>
<p>Let us denote these operators by Y1, Y2, Y3, and Y4, respectively. By apply-
ing each one to a generic basis vector ek1 &otimes; ek2 &otimes; ek3 , we can generate all
the irreducible representations. The first operator gives
</p>
<p>Y1(ek1 &otimes; ek2 &otimes; ek3)= ek1 &otimes; ek2 &otimes; ek3 + ek1 &otimes; ek3 &otimes; ek2 + ek2 &otimes; ek1 &otimes; ek3
+ ek2 &otimes; ek3 &otimes; ek1 + ek3 &otimes; ek1 &otimes; ek2
+ ek3 &otimes; ek2 &otimes; ek1 ,
</p>
<p>where k1, k2, and k2 can be 1 or 2. For k1 = k2 = k3 = 1, we get 6e1 &otimes; e1 &otimes;
e1. For the case where two of the ki &rsquo;s are 1 and the third is 2, we get
</p>
<p>2(e1 &otimes; e1 &otimes; e2 + e1 &otimes; e2 &otimes; e1 + e2 &otimes; e1 &otimes; e1).
</p>
<p>For the case where two of the ki &rsquo;s are 2 and the third is 1, we get
</p>
<p>2(e1 &otimes; e2 &otimes; e2 + e2 &otimes; e1 &otimes; e2 + e2 &otimes; e2 &otimes; e1).
</p>
<p>Finally, for k1 = k2 = k3 = 2, we get 6e2 &otimes; e2 &otimes; e2. Altogether, we obtain 4
linearly independent vectors that are completely symmetric.
</p>
<p>When the second operator acts on a generic basis vector, it gives2
</p>
<p>Y2(ek1 &otimes; ek2 &otimes; ek3)=
[
e&minus; (k1, k3)
</p>
<p>][
e+ (k1, k2)
</p>
<p>]
(ek1 &otimes; ek2 &otimes; ek3)
</p>
<p>= ek1 &otimes; ek2 &otimes; ek3 + ek2 &otimes; ek1 &otimes; ek3
&minus; ek3 &otimes; ek2 &otimes; ek1 &minus; ek2 &otimes; ek3 &otimes; ek1 .
</p>
<p>If all three indices are the same, we get zero. Suppose k1 = 1. Then k2 can
be 1 or 2. For k2 = 1, we must set k3 = 2 to get e2 &otimes; e1 &otimes; e1 &minus; e1 &otimes; e2 &otimes; e1.
For k2 = 2, we must set k3 = 1 to obtain e1 &otimes; e2 &otimes; e2 &minus; e2 &otimes; e1 &otimes; e2. If we
</p>
<p>2When a symmetric group is considered as an abstract group&mdash;as opposed to a group
of transformations&mdash;we may multiply permutations (keep track of how each number is
repeatedly transformed) from left to right. However, since the permutations here act on
vectors on their right, it is more natural to calculate their products from right to left.</p>
<p/>
</div>
<div class="page"><p/>
<p>966 30 Representation of Lie Groups and Lie Algebras
</p>
<p>start with k1 = 2, we will not produce any new vectors, as the reader is urged
to verify. Therefore, the dimension of the irreducible subspace spanned by
the second Young tableau is 2.
</p>
<p>The action of the third operator on a generic basis vector yields
</p>
<p>Y3(ek1 &otimes; ek2 &otimes; ek3)=
[
e&minus; (k1, k2)
</p>
<p>][
e+ (k1, k3)
</p>
<p>]
(ek1 &otimes; ek2 &otimes; ek3)
</p>
<p>= ek1 &otimes; ek2 &otimes; ek3 + ek2 &otimes; ek1 &otimes; ek3
&minus; ek3 &otimes; ek2 &otimes; ek1 &minus; ek3 &otimes; ek1 &otimes; ek2 .
</p>
<p>The reader may check that we obtain a two-dimensional irreducible repre-
sentation spanned by e1 &otimes; e1 &otimes; e2 &minus; e2 &otimes; e1 &otimes; e1 and e1 &otimes; e2 &otimes; e2 &minus; e2 &otimes;
e2 &otimes; e1.
</p>
<p>The fourth Young operator gives zero because it is completely antisym-
metric in three slots and we have only two indices. The reduction of the
tensor product space can therefore be written as
</p>
<p>V
&otimes;3 = Span
</p>
<p>{
Y1(ek1 &otimes; ek2 &otimes; ek3)
</p>
<p>}
︸ ︷︷ ︸
</p>
<p>dim=4
</p>
<p>&oplus;Span
{
Y2(ek1 &otimes; ek2 &otimes; ek3)
</p>
<p>}
︸ ︷︷ ︸
</p>
<p>dim=2
</p>
<p>&oplus; Span
{
Y3(ek1 &otimes; ek2 &otimes; ek3)
</p>
<p>}
︸ ︷︷ ︸
</p>
<p>dim=2
</p>
<p>.
</p>
<p>We note that the total dimensions on both sides match.
</p>
<p>There is a remarkable formula that gives the dimension of all irreducible
representations of GL(V) (see [Hame 89, pp. 384&ndash;387] for a derivation):
</p>
<p>Theorem 30.2.3 Let V be an n-dimensional vector space, and V(λ), the
irreducible subspace of tensors with symmetry associated with the partition
(λ)= (λ1, . . . , λn). Then
</p>
<p>dimV(λ) = D(l1, . . . , ln)
D(n&minus; 1, n&minus; 2, . . . ,0) ,
</p>
<p>where lj &equiv; λj + n&minus; j and D(x1, . . . , xn) is as given in Eq. (25.3).
</p>
<p>30.3 Representation of Lie Algebras
</p>
<p>The diffeomorphism established by the exponential map (Theorem 29.1.21)
reduces the local study of a Lie group to that of its Lie algebra.3 In this book,
we are exclusively interested in the local properties of Lie groups, and we
</p>
<p>3We use the word &ldquo;local&rdquo; to mean the collection of all points that can be connected to the
identity by a curve in the Lie group G. If this collection exhausts G, then we say that G
is connected. If, furthermore, all closed curves (loops) in G can be shrunk to a point, we
say that G is simply connected. The word &ldquo;local&rdquo; can be replaced by &ldquo;simply connected&rdquo;
in what follows.</p>
<p/>
</div>
<div class="page"><p/>
<p>30.3 Representation of Lie Algebras 967
</p>
<p>shall therefore confine ourselves to Lie algebras to study the structure of Lie
groups. Recall that any Lie group homomorphism leads to a corresponding
Lie algebra homomorphism [Eq. (29.7)]. Conversely, a homomorphism of
Lie algebras can, through the identification of the neighborhoods of their
identities with their Lie algebras, be &ldquo;exponentiated&rdquo; to a (local) homomor-
phism of their Lie groups. This leads to the following theorem (see [Fult 91,
pp. 108 and 119] for a proof).
</p>
<p>Theorem 30.3.1 Let G be a Lie group with algebra g. A represen-
tation T :G&rarr; GL(H) determines a Lie algebra representation T&lowast; :
g&rarr; gl(H). Conversely, a Lie algebra representation T : g&rarr; gl(H)
determines a Lie group representation.
</p>
<p>It follows from this theorem that all (local) Lie group representations
result from corresponding Lie algebra representations. Therefore, we shall
restrict ourselves to the representations of Lie algebras.
</p>
<p>30.3.1 Representation of Subgroups of GL(V)
</p>
<p>Let g be any Lie algebra with basis vectors {Xi}. Let a representation T map
these vectors to {Ti} &isin; gl(H) for some carrier space H. Then, a general ele-
ment X =&sum;i αiXi of g will be mapped to T=
</p>
<p>&sum;
i αiTi . Now suppose that
</p>
<p>h is a subalgebra of g. Then the restriction of T to h provides a representa-
tion of h. This restriction may be reducible. If it is, then there is an invariant
subspace H1 of H. It follows that
</p>
<p>〈b|TX|a〉 = 0 &forall;X &isin; h whenever |a〉 &isin;H1 and |b〉 &isin;H&perp;1 ,
</p>
<p>where TX &equiv; T (X). If we write TX =
&sum;
</p>
<p>i α
(X)
i Ti , then in terms of Ti , the
</p>
<p>equation above can be written as
</p>
<p>dimg&sum;
</p>
<p>i=1
α
(X)
i 〈b|Ti |a〉 &equiv;
</p>
<p>dimg&sum;
</p>
<p>i=1
α
(X)
i τ
</p>
<p>(ba)
i = 0 &forall;X &isin; h, (30.12)
</p>
<p>where τ (ba)i &equiv; 〈b|Ti |a〉 are complex numbers. Equation (30.12) states that
</p>
<p>Box 30.3.2 If T , as a representation of h (a Lie subalgebra of g),
is reducible, then there exist a number of equations that α(X)i must
satisfy whenever X &isin; h. If T , as a representation of g, is irreducible,
then no relation such as given in (30.12) will exist when X runs over
all of g.
</p>
<p>This last statement will be used to analyze certain subgroups of GL(V).</p>
<p/>
</div>
<div class="page"><p/>
<p>968 30 Representation of Lie Groups and Lie Algebras
</p>
<p>Let us first identify GL(V) with GL(n,C). Next, consider GL(n,R),
which is a subgroup of GL(n,C), and transfer the discussion to their re-
spective algebras. If {Xi} is a basis of gl(n,C), then an arbitrary element
can be written as
</p>
<p>&sum;
i αiXi . The difference between gl(n,C) and gl(n,R) is
</p>
<p>that the αi &rsquo;s are real in the latter case; i.e., for all real values of {αi}, the sum
belongs to gl(n,R). Now suppose that T is an irreducible representation of
gl(n,C) that is reducible when restricted to gl(n,R). Equation (30.12) states
that the function
</p>
<p>f (z1, . . . , zn2)&equiv;
n2&sum;
</p>
<p>i=1
ziτ
</p>
<p>(ba)
i
</p>
<p>vanishes for all real values of the zi &rsquo;s. Since this function is obviously en-
tire, it must vanish for all complex values of zi &rsquo;s by analytic continuation
(see Theorem 12.3.1). But this is impossible because T is irreducible for
gl(n,C). We have to conclude that T is irreducible as a representation of
gl(n,R).
</p>
<p>The next subalgebra of gl(n,C) we consider is the Lie algebra sl(n,C)
of the special linear group. The only restriction on the elements of sl(n,C)
is for them to have a vanishing trace. Denoting tr Xi by ti , we conclude
that X = &sum;i αiXi belongs to sl(n,C) if and only if
</p>
<p>&sum;
i αi ti = 0. Let
</p>
<p>(t&lowast;1 , . . . , t
&lowast;
n2
)&equiv; |t〉 &isin;Cn2 . Then sl(n,C) can be characterized as the subspace
</p>
<p>consisting of vectors |a〉 &isin; Cn2 such that 〈a|t〉 = 0. Such a subspace has
n2 &minus; 1 dimensions. If any irreducible representation of gl(n,C) is reducible
for sl(n,C), then the set of complex numbers {αi} must, in addition, satisfy
Eq. (30.12). This amounts to the condition that |a〉 be orthogonal to |τ (ba)〉
as well. But this is impossible, because then the set {|a〉, |t〉, |τ (ba)〉} would
constitute a subspace of Cn
</p>
<p>2
whose dimension is at least n2 + 1: There are
</p>
<p>n2 &minus; 1 of |a〉&rsquo;s, one |t〉, and at least one |τ (ba)〉. Therefore, all irreducible
representations of gl(n,C) are also irreducible representations of sl(n,C).
</p>
<p>The last subalgebra of gl(n,C) we consider is the Lie algebra u(n) of
the unitary group. To study this algebra, we start with the Weyl basis of
Eq. (29.32) for gl(n,C), and construct a new hermitian basis {Xkj } defined
as
</p>
<p>Xjj &equiv; ejj for all j = 1,2, . . . , n,
Xkj &equiv; i
</p>
<p>(
ekj &minus; etkj
</p>
<p>)
if k �= j.
</p>
<p>A typical element of gl(n,C) is of the form
&sum;
</p>
<p>kj αkjXkj , where αkj are
complex numbers. If we restrict ourselves to real values of αkj , then we
obtain the subalgebra of hermitian matrices whose Lie group is the unitary
group U(n). The fact that the irreducible representations of gl(n,C) will
not reduce under u(n) follows immediately from our discussion concerning
gl(n,R). We summarize our findings in the following:
</p>
<p>Theorem 30.3.3 The irreducible representations of GL(n,C) are
also irreducible representations of GL(n,R), SL(n,C), U(n), and
SU(n).</p>
<p/>
</div>
<div class="page"><p/>
<p>30.3 Representation of Lie Algebras 969
</p>
<p>The case of SU(n) follows from the same argument given earlier that
connected GL(n,C) to SL(n,C).
</p>
<p>30.3.2 Casimir Operators
</p>
<p>In the general representation theory of Lie algebras, it is desirable to label
each irreducible representation with a quantity made out of the basis vectors
of the Lie algebra. An example is the labeling of the energy states of a quan-
tum mechanical system with angular momentum. Each value of the total
angular momentum labels an irreducible subspace whose vectors are further
labeled by the third component of angular momentum (see Chap. 13). This
subsection is devoted to the generalization of this concept to an arbitrary Lie
algebra.
</p>
<p>Definition 30.3.4 Let T : g &rarr; gl(H) be a representation of the Lie alge-
bra g. A Casimir operator C for this representation is an operator that
</p>
<p>Casimir operator defined
</p>
<p>commutes with all TX of the representation.
</p>
<p>If the representation is irreducible, then by Schur&rsquo;s lemma, C is a mul-
tiple of the unit operator. Therefore, all vectors of an irreducible invariant
subspace of the carrier space H are eigenvectors of C corresponding to the
same eigenvalue. That Casimir operators actually determine the irreducible
representations of a semisimple Lie algebra is the content of the following
theorem (for a proof, see [Vara 84, pp. 333&ndash;337]).
</p>
<p>Chevalley&rsquo;s theorem
</p>
<p>Theorem 30.3.5 (Chevalley) For every semisimple Lie algebra g of
rank4 r with a basis {Xi}, there exists a set of r Casimir operators
in the form of polynomials in TXi whose eigenvalues characterize the
irreducible representations of g.
</p>
<p>From now on, we shall use the notation Xi for TXi . It follows from Theo-
rem 30.3.5 that all irreducible invariant vector subspaces of the carrier space
can be labeled by the eigenvalues of the r Casimir operators. This means that
each invariant irreducible subspace has a basis all of whose vectors carry a
set of r labels corresponding to the eigenvalues of the r Casimir opera-
tors.
</p>
<p>One Casimir operator&mdash;in the form of a polynomial of degree two&mdash;
which works only for semisimple Lie algebras, is obtained easily:
</p>
<p>C=
&sum;
</p>
<p>i,j
</p>
<p>gijXiXj , (30.13)
</p>
<p>where gij is the inverse of the Cartan metric tensor. In fact, with the sum-
mation convention in place, we have
</p>
<p>4Recall that the rank of g is the dimension of the Cartan subalgebra of g.</p>
<p/>
</div>
<div class="page"><p/>
<p>970 30 Representation of Lie Groups and Lie Algebras
</p>
<p>[C,Xk] = gij [XiXj ,Xk] = gij
{
Xi[Xj ,Xk] + [Xi,Xk]Xj
</p>
<p>}
</p>
<p>= gij
{
crjkXiXr + crikXrXj
</p>
<p>}
</p>
<p>= gij crik(XjXr + XrXj ) (because gij is symmetric)
</p>
<p>= gijgsrciks(XjXr + XrXj )
</p>
<p>= 0 (because gijgsrciks is antisymmetric in j, r).
</p>
<p>The last equality follows from the fact that gij and gsr are symmetric, ciks
is completely antisymmetric [see the discussion following Eq. (29.50)], and
there is a sum over the dummy index s.
</p>
<p>Example 30.3.6 The rotation group SO(3) in R3 is a compact 3-parameter
Lie group. The infinitesimal generators are the three components of the an-
gular momentum operator (see Example 29.1.35). From the commutation
relations of the angular momentum operators developed in Chap. 13, we
conclude that ckij = iǫijk . It follows that the Cartan metric tensor is
</p>
<p>gij = criscsjr = (iǫisr )(iǫjrs)=+ǫisrǫjsr = 2δij .
</p>
<p>Ignoring the factor of 2 and denoting the angular momentum operators by Li ,
we conclude that
</p>
<p>L2 &equiv; L21 + L22 + L23
is a Casimir operator. But this is precisely the operator discussed in detail
in Chap. 13. We found there that the eigenvalues of L2 were labeled by j ,
</p>
<p>irreducible
</p>
<p>representations of the
</p>
<p>rotation group and
</p>
<p>spherical harmonics
</p>
<p>where j was either an integer or a half odd integer. In the context of our
present discussion, we note that the Lie algebra so(3) has rank one, because
there is no higher dimensional subalgebra of so(3) all of whose vectors com-
mute with one another. It follows from Theorem 30.3.5 that L2 is the only
Casimir operator, and that all irreducible representations T (j) of so(3) are
distinguished by their label j . Furthermore, the construction of Chap. 13
showed explicitly that the dimension of T (j) is 2j + 1.
</p>
<p>The connection between the representation of Lie algebras and Lie
groups permits us to conclude that the irreducible representations of the
rotation group are labeled by the (half) integers j , and the j th irreducible
representation has dimension 2j + 1. When j is an integer l and the carrier
space is L2(S2), the square-integrable functions on the unit sphere, then L2
</p>
<p>becomes a differential operator, and the spherical harmonics Ylm(θ,ϕ), with
a fixed value of l, provide a basis for the lth irreducible invariant subspace.
</p>
<p>The last sentence of Example 30.3.6 is at the heart of the connection be-
tween symmetry, Lie groups, and the equations of mathematical physics.
A symmetry operation of mathematical physics is expressed in terms of
the action of a Lie group on an underlying manifold M , i.e., as a group
of transformations of M . The Lie algebra of such a Lie group consists of the
infinitesimal generators of the corresponding transformation. These genera-
</p>
<p>Connection between
</p>
<p>Casimir operators and
</p>
<p>the PDEs of
</p>
<p>mathematical physics
</p>
<p>tors can be expressed as first-order differential operators as in Eq. (29.25).</p>
<p/>
</div>
<div class="page"><p/>
<p>30.3 Representation of Lie Algebras 971
</p>
<p>It is therefore natural to choose as the carrier space of a representation
the Hilbert space L2(M) of the square-integrable functions on M , which,
through the local identification of M with Rm (m= dimM), can be identi-
fied with functions on Rm. Then the infinitesimal generators act directly on
the functions of L2(M) as first-order differential operators.
</p>
<p>The Casimir operators {Cα}rα=1, where r is the rank of the Lie algebra,
are polynomials in the infinitesimal generators, i.e., differential operators of
higher order. On the irreducible invariant subspaces of L2(M), each Cα acts
as a multiple of the identity, so if f (r) belongs to such an invariant subspace,
we have
</p>
<p>Cαf (r)= λ(α)f (r), α = 1,2, . . . , r. (30.14)
</p>
<p>This is a set of differential equations that are invariant under the symmetry
of the physical system, i.e., its solutions transform among themselves under
the action of the group of symmetries.
</p>
<p>It is a stunning reality and a fact of profound significance that many of
the differential equations of mathematical physics are, as in Eq. (30.14), ex-
pressions of the invariance of the Casimir operators of some Lie algebra in a
particular representation. Moreover, all the standard functions of mathemat-
ical physics, such as Bessel, hypergeometric, and confluent hypergeomet-
ric functions, are related to matrix elements in the representations of a few
of the simplest Lie groups (see [Mill 68] for a thorough discussion of this
topic).
</p>
<p>Historical Notes
</p>
<p>Claude Chevalley (1909&ndash;1984) was the only son of Abel and Marguerite Chevalley who
were the authors of the Oxford Concise French Dictionary. He studied under Emile Picard
at the Ecole Normale Sup&eacute;rieur in Paris, graduating in 1929 and becoming the youngest
of the mathematicians of the Bourbaki school.
After graduation, Chevalley went to Germany to continue his studies under Artin at Ham-
</p>
<p>Claude Chevalley
</p>
<p>1909&ndash;1984
</p>
<p>burg during the session 1931&ndash;1932. He then went to the University of Marburg to work
with Hasse, who had been appointed to fill Hensel&rsquo;s chair there in 1930. He was awarded
his doctorate in 1937. A year later Chevalley went to the Institute for Advanced Study at
Princeton, where he also served on the faculty of Princeton University. From July 1949
until June 1957 he served as professor of mathematics at Columbia University, afterwards
returning to the University of Paris.
Chevalley had a major influence on the development of several areas of mathematics. His
papers of 1936 and 1941 led to major advances in class field theory and also in algebraic
geometry. He did pioneering work in the theory of local rings in 1943, developing the
ideas of Krull into a theorem bearing his name. Chevalley&rsquo;s theorem was important in
applications made in 1954 to quasi-algebraically closed fields and the following year to
algebraic groups. Chevalley groups play a central role in the classification of finite simple
groups. His name is also attached to Chevalley decompositions and to a Chevalley type
of semi-simple algebraic group.
Many of his texts have become classics. He wrote Theory of Lie Groups in three vol-
umes which appeared in 1946, 1951, and 1955. He also published Theory of Distributions
(1951), Introduction to the Theory of Algebraic Functions of one Variable (1951), The Al-
gebraic Theory of Spinors (1954), Class Field Theory (1954), Fundamental Concepts of
Algebra (1956), and Foundations of Algebraic Geometry (1958).
Chevalley was awarded many honors for his work. Among these was the Cole Prize of the
American Mathematical Society. He was elected a member of the London Mathematical
Society in 1967.</p>
<p/>
</div>
<div class="page"><p/>
<p>972 30 Representation of Lie Groups and Lie Algebras
</p>
<p>30.3.3 Representation of so(3) and so(3,1)
</p>
<p>Because of their importance in physical applications, we study the repre-
sentations of so(3), the rotation, and so(3,1), the Lorentz, algebras. For
rotations, we define J1 &equiv; &minus;iM23, J2 &equiv; iM13, and J3 &equiv; &minus;iM12,5 and note
that the Ji &rsquo;s satisfy exactly the same commutation relations as the angular
momentum operators of Chap. 13. Therefore, the irreducible representations
of so(3) are labeled by j , which can be an integer or a half-odd integer (see
also Example 30.3.6). These representations are finite-dimensional because
SO(3) is a compact group (Example 30.1.1 and Theorem 30.1.7). The di-
mension of the irreducible representation of so(3) labeled by j is 2j + 1.
</p>
<p>Because of local isomorphism of Lie groups and their Lie algebras, the
same irreducible spaces found for Lie algebras can be used to represent the
Lie groups. In particular, the states {|jm〉}jm=&minus;j , where m is the eigenvalue
of Jz, can also be used as a basis of the j -th irreducible representation.
</p>
<p>The flow of each infinitesimal generator of so(3) is a one-parameter sub-
group of SO(3). For example, exp(M12ϕ) is a rotation of angle ϕ about the
z-axis. Using Euler angles, we can write a general rotation as
</p>
<p>R(ψ, θ,ϕ)= exp(M12ψ) exp(M31θ) exp(M12ϕ).
</p>
<p>The corresponding rotation operator acting on the vectors of the carrier
space is
</p>
<p>R(ψ, θ,ϕ)= exp(M12ψ) exp(M31θ) exp(M12ϕ)= eiJzψeiJyθeiJzϕ .
</p>
<p>The rotation matrix corresponding to the above operator is obtained byrotation matrix
sandwiching R(ψ, θ,ϕ) between basis vectors of a given irreducible repre-
sentation:
</p>
<p>D
(j)
</p>
<p>m&prime;m(ψ, θ,ϕ)&equiv; 〈jm&prime;|R(ψ, θ,ϕ)|jm〉 = 〈jm&prime;|eiJzψeiJyθeiJzϕ |jm〉
</p>
<p>= eim&prime;ψeimϕ〈jm&prime;|eiJyθ |jm〉 = ei(m&prime;ψ+mϕ)d(j)
m&prime;m(θ).
</p>
<p>(30.15)
</p>
<p>Thus, the calculation of rotation matrices is reduced to finding d(j)
m&prime;m(θ).
</p>
<p>These are given by the Wigner formula (see [Hame 89, pp. 348&ndash;357]):Wigner formula for
rotation matrices
</p>
<p>d
(j)
</p>
<p>m&prime;m(θ)=
&sum;
</p>
<p>μ
</p>
<p>φ
(
j,m,m&prime;;μ
</p>
<p>)(
cos
</p>
<p>θ
</p>
<p>2
</p>
<p>)2(j&minus;μ)+m&minus;m&prime;(
sin
</p>
<p>θ
</p>
<p>2
</p>
<p>)2μ+m&prime;&minus;m
</p>
<p>(30.16)
where
</p>
<p>φ
(
j,m,m&prime;;μ
</p>
<p>)
&equiv; (&minus;1)μ [(j +m)!(j &minus;m)!(j +m
</p>
<p>&prime;)!(j &minus;m&prime;)!]1/2
(j +m&minus;μ)!μ!(j &minus;m&prime; &minus;μ)!(m&prime; &minus;m+μ)!
</p>
<p>and the summation extends over all integral values of μ for which the fac-
torials have a meaning. The number of terms in the summation is equal to
1 + τ , where τ is the smallest of the four integers j &plusmn;m, j &plusmn;m&prime;.
</p>
<p>5Sometimes we use Jx , Jy , and Jz instead of J1, J2, and J3.</p>
<p/>
</div>
<div class="page"><p/>
<p>30.3 Representation of Lie Algebras 973
</p>
<p>From the rotation matrices, we can obtain the characters of the rotation
group. However, an easier way is to use Euler&rsquo;s theorem (Theorem 6.6.15),
Example 23.2.19, and Box 24.3.6 to conclude that the character of a rotation
depends only on the angle of rotation, and not on the direction of the rotation
axis. Choosing the z-axis as our only axis of rotation, we obtain
</p>
<p>χ (j)(ϕ)=
j&sum;
</p>
<p>m=&minus;j
〈jm|eiJzϕ |jm〉 =
</p>
<p>j&sum;
</p>
<p>m=&minus;j
eimϕ = e&minus;ijϕ
</p>
<p>j&sum;
</p>
<p>m=&minus;j
ei(j+m)ϕ
</p>
<p>= e&minus;ijϕ
2j&sum;
</p>
<p>k=0
eikϕ = e&minus;ijϕ e
</p>
<p>i(2j+1)ϕ &minus; 1
eiϕ &minus; 1
</p>
<p>= e
i(j+1)ϕ &minus; e&minus;ijϕ
</p>
<p>eiϕ &minus; 1 =
sin(j + 12 )
sin(ϕ/2)
</p>
<p>. (30.17)
</p>
<p>Equation (30.17) can be used to obtain the celebrated addition theo-
rem for angular momenta. Suppose that initially we have two physical sys-
tems corresponding to angular momenta j1 and j2. When these systems
are made to interact with one another, the total system will be described
by the tensor product states. These states are vectors in the tensor product
of the irreducible representations T (j1) and T (j2) of the rotation group, as
discussed in Sect. 24.8. This product is reducible. To find the factors into
which it reduces, we consider its character corresponding to angle ϕ. Using
Eq. (24.42), we have
</p>
<p>χ (j1&times;j2)(ϕ)= χ (j1)(ϕ) &middot; χ (j2)(ϕ)=
j1&sum;
</p>
<p>m1=&minus;j1
eim1ϕ
</p>
<p>j2&sum;
</p>
<p>m2=&minus;j2
eim2ϕ
</p>
<p>=
j1&sum;
</p>
<p>m1=&minus;j1
</p>
<p>j2&sum;
</p>
<p>m2=&minus;j2
ei(m1+m2)ϕ
</p>
<p>=
j1+j2&sum;
</p>
<p>J=|j1&minus;j2|
</p>
<p>J&sum;
</p>
<p>M=&minus;J
eiMϕ =
</p>
<p>j1+j2&sum;
</p>
<p>J=|j1&minus;j2|
χ (J )(ϕ),
</p>
<p>where the double sum on the third line is an equivalent way of writing the
double summation of the second line, as the reader may verify. From this
equation we read off the Clebsch-Gordan decomposition of the tensor prod-
uct: addition theorem for
</p>
<p>angular momenta
</p>
<p>T (j1) &otimes; T (j2) =
j1+j2&sum;
</p>
<p>J=|j1&minus;j2|
T (J ), (30.18)
</p>
<p>which is also known as the addition theorem for angular momenta. Equa-
tion (30.18) shows that (see page 753).
</p>
<p>Box 30.3.7 The rotation group is simply reducible.</p>
<p/>
</div>
<div class="page"><p/>
<p>974 30 Representation of Lie Groups and Lie Algebras
</p>
<p>The RHS of Eq. (30.18) tells us which irreducible representations result
from multiplying T (j1) and T (j2). In particular, if j1 = j2 &equiv; l, the RHS in-
cludes the J = 0 representation, i.e., a scalar. In terms of the states, this says
that we can combine two states with angular momentum l to obtain a scalar
state. Let us find this combination. We use Eq. (24.46) in the form
</p>
<p>|JM〉 =
&sum;
</p>
<p>m1,m2
</p>
<p>C(j1j2;J |m1m2;M)|j1,m1; j2,m2〉, m1 +m2 =M.
</p>
<p>(30.19)
In the case under investigation, J = 0 =M , so (30.19) becomes
</p>
<p>|00〉 =
l&sum;
</p>
<p>m=&minus;l
C(ll;0|m,&minus;m;0)|lm; l,&minus;m〉.
</p>
<p>Problem 30.9 shows that C(ll;0|m,&minus;m;0)= (&minus;1)l&minus;m/
&radic;
</p>
<p>2l + 1, so that
</p>
<p>|00〉 =
l&sum;
</p>
<p>m=&minus;l
</p>
<p>(&minus;1)l&minus;m&radic;
2l + 1
</p>
<p>|lm; l,&minus;m〉.
</p>
<p>Take the &ldquo;inner product&rdquo; of this with 〈θ,ϕ; θ &prime;, ϕ&prime;| to obtain
</p>
<p>〈θ,ϕ; θ &prime;, ϕ&prime;|00〉 =
l&sum;
</p>
<p>m=&minus;l
</p>
<p>(&minus;1)l&minus;m&radic;
2l + 1
</p>
<p>〈θ,ϕ; θ &prime;, ϕ&prime;|lm; l,&minus;m〉
</p>
<p>=
l&sum;
</p>
<p>m=&minus;l
</p>
<p>(&minus;1)l&minus;m&radic;
2l + 1
</p>
<p>〈θ,ϕ|lm〉︸ ︷︷ ︸
Ylm(θ,ϕ)
</p>
<p>〈θ &prime;, ϕ&prime;|l,&minus;m〉︸ ︷︷ ︸
Yl,&minus;m(θ &prime;,ϕ&prime;)
</p>
<p>, (30.20)
</p>
<p>where we have used 〈θ,ϕ; θ &prime;, ϕ&prime;| = 〈θ,ϕ|〈θ &prime;, ϕ&prime;| and contracted each bra
with a ket. We can evaluate the LHS of (30.20) by noting that since it is a
scalar, the choice of orientation of coordinates is immaterial. So, let θ = 0
to get θ &prime; = γ , the angle between the two directions. Then using the facts
</p>
<p>Ylm(0, ϕ)= δm0
&radic;
</p>
<p>2l + 1
4π
</p>
<p>and Yl0(θ,ϕ)=
&radic;
</p>
<p>2l + 1
4π
</p>
<p>Pl(cos θ)
</p>
<p>on the RHS of (30.20), we obtain
</p>
<p>〈θ,ϕ; θ &prime;, ϕ&prime;|00〉 = (&minus;1)
l
</p>
<p>4π
</p>
<p>&radic;
2l + 1Pl(cosγ ).
</p>
<p>Substituting this in the LHS of Eq. (30.20), we get
</p>
<p>Pl(cosγ )=
4π
</p>
<p>2l + 1
</p>
<p>l&sum;
</p>
<p>m=&minus;l
(&minus;1)mYlm(θ,ϕ)Yl,&minus;m
</p>
<p>(
θ &prime;, ϕ&prime;
</p>
<p>)
,
</p>
<p>which is the addition theorem for spherical harmonics discussed in Chap. 13.
Let us now turn to so(3,1). We collect the generators in two categories
</p>
<p>M &equiv; (M1,M2,M3)&equiv; (M23,M31,M12),
N &equiv; (N1,N2,N3)&equiv; (M01,M02,M03),</p>
<p/>
</div>
<div class="page"><p/>
<p>30.3 Representation of Lie Algebras 975
</p>
<p>and verify that
</p>
<p>[Mi,Mj ] = &minus;ǫijkMk, [Ni,Nj ] = ǫijkMk, [Mi,Nj ] = &minus;ǫijkNk,
</p>
<p>and that there are two Casimir operators: M2 &minus;N2 and M &middot;N. It follows that
the irreducible representations of so(3,1) are labeled by two numbers. To
find these numbers, define the generators
</p>
<p>J &equiv; 1
2i
(M + iN), K &equiv; 1
</p>
<p>2i
(M &minus; iN),
</p>
<p>and show that
</p>
<p>[Ji, Jm] = ǫimkJk, [Ki,Kj ] = ǫijmKm, [Ji,Kj ] = 0.
</p>
<p>It follows that the J &rsquo;s and the K&rsquo;s generate two completely independent Lie
algebras isomorphic to the angular momentum algebras and that so(3,1) is a
direct sum of these algebras. Since each one requires a (half-odd) integer to
designate its irreducible representations, we can choose these two numbers
as the eigenvalues of the Casimir operators needed to label the irreducible
representations of so(3,1). Thus, the irreducible representations of so(3,1)
are of the form T (jj
</p>
<p>&prime;), where j and j &prime; can each be an integer or a half-odd
integer.
</p>
<p>30.3.4 Representation of the Poincar&eacute; Algebra
</p>
<p>The Poincar&eacute; algebra p(p,n&minus; p), introduced in Sect. 29.2.1, is the gener-
alization of the Lie algebra of the invariance group of the special theory of
relativity. It contains the Lorentz, the rotation, and the translation groups as
its proper subgroups. Its irreducible representations are of direct physical
significance, and we shall study them here.
</p>
<p>As the first step in the construction of representations of p(p,n&minus;p), we
shall try to find its Casimir operators. Eq. (30.13) suggests one, but it works
only for semisimple Lie algebras, and the Poincar&eacute; algebra is not semisim-
ple. Nevertheless, let us try to find an operator based on that construction.
From the commutation relations for p(p,n &minus; p), as given in Eq. (29.45),
and the double-indexed structure constants defined by,6
</p>
<p>[Mij ,Mkl] = cmnij,klMmn, [Mij ,Pk] = cmij,kPm,
</p>
<p>we obtain
</p>
<p>cmnij,kl = δmj δnl ηik &minus; δmj δnkηil + δmi δnkηj l &minus; δmi δnl ηjk,
</p>
<p>cmij,k = δmj ηik &minus; δmi ηjk.
(30.21)
</p>
<p>6Please make sure to differentiate between the pair (Mij ,Pk) (which acts on p) and the
pair (Mij ,Pk), which acts on the state vectors in the Hilbert space of representation.</p>
<p/>
</div>
<div class="page"><p/>
<p>976 30 Representation of Lie Groups and Lie Algebras
</p>
<p>From these structure constants, we can construct a double indexed &ldquo;metric&rdquo;
</p>
<p>gij,kl = crsij,mncmnkl,rs + crij,mcmkl,r ,
</p>
<p>which the reader may verify to be equal to
</p>
<p>gij,kl = 2(n&minus; 1)(ηjkηil &minus; ηikηj l).
</p>
<p>There is no natural way of constructing a single-indexed metric. Therefore,
we can only contract the M&rsquo;s. In doing so, it is understood that the indices are
raised and lowered by ηij . So, the first candidate for a Casimir operator is
</p>
<p>M2 &equiv; gij,klMijMkl = 2(n&minus;1)(ηjkηil&minus;ηikηj l)MijMkl =&minus;4(n&minus;1)MijMij
</p>
<p>The reader may verify that M2 commutes with all the Mij &rsquo;s but not with
the Pi &rsquo;s. This is to be expected because M2, the total &ldquo;angular momentum&rdquo;
operator7 is a scalar and should commute with all its components. But com-
mutation with the Pi &rsquo;s is not guaranteed.
</p>
<p>The construction above, although a failure, gives us a clue for a success-
ful construction. We can make another scalar out of the P&rsquo;s. The reader may
check that P2 &equiv; ηijPiPj indeed commutes with all elements of the Poincar&eacute;
algebra. We have thus found one Casimir operator. Can we find more? We
have exhausted the polynomials of degree two. The only third-degree poly-
nomials that we can construct are MijPiPj and ηilMijMjkMkl . The first one
is identically zero (why?), and the second one will not commute with the P&rsquo;s.
</p>
<p>To find higher-order polynomials in the infinitesimal generators, we build
new tensors out of them and contract these tensors with one another. For
example, consider the vector
</p>
<p>Ci &equiv;MijPj = ηkjMijPk. (30.22)
</p>
<p>Then CiCi , a fourth-degree polynomial in the generators, is a scalar, and
therefore, it commutes with the Mij &rsquo;s, but unfortunately, not with Pi &rsquo;s.
</p>
<p>Another common way to construct tensors is to contract various numbers
of the generators with the Levi-Civita tensor. For example,
</p>
<p>Wi1...in&minus;3 &equiv; ǫi1...in&minus;3jklMjkPl (30.23)
</p>
<p>is a contravariant tensor of rank n&minus; 3. Let us contract W with itself to find
a scalar (which we expect to commute with all the Mij &rsquo;s):
</p>
<p>W2 &equiv;Wi1...in&minus;3Wi1...in&minus;3
= ǫi1...in&minus;3jklMjkPlǫi1...in&minus;3rstMrsPt
</p>
<p>= (&minus;1)n&minus;
&sum;
</p>
<p>π
</p>
<p>ǫπδ
i1
π(i1)
</p>
<p>δ
i2
π(i2)
</p>
<p>&middot; &middot; &middot; δin&minus;3π(in&minus;3)δ
j
</p>
<p>π(r)δ
k
π(s)δ
</p>
<p>l
π(t)MjkPlM
</p>
<p>rsPt
</p>
<p>= (&minus;1)p(n&minus; 3)!
&sum;
</p>
<p>π
</p>
<p>ǫπδ
j
</p>
<p>π(r)δ
k
π(s)δ
</p>
<p>l
π(t)MjkPlM
</p>
<p>rsPt ,
</p>
<p>7This &ldquo;angular momentum&rdquo; includes ordinary rotations as well as the Lorentz boosts.</p>
<p/>
</div>
<div class="page"><p/>
<p>30.3 Representation of Lie Algebras 977
</p>
<p>where we used Eq. (26.45). The sum above can be carried out, with the final
result
</p>
<p>W2 = 2(&minus;1)p(n&minus; 3)!
(
MijM
</p>
<p>ijP2 &minus; 2CiCi
)
</p>
<p>= 2(&minus;1)p(n&minus; 3)!
(
M2P2 &minus; 2C2
</p>
<p>)
, (30.24)
</p>
<p>where Ci was defined in Eq. (30.22). We have already seen that M2, P2, and
C2 all commute with the Mjk&rsquo;s. The reader may check that W2 commutes
with the Pj &rsquo;s as well. In fact, Wi1...in&minus;3 itself commutes with all the Pj &rsquo;s.
Other tensors and Casimir operators can be constructed in a similar fashion.
</p>
<p>We now want to construct the irreducible vector spaces that are labeled
by the eigenvalues of the Casimir operators. We take advantage of the fact
that the Poincar&eacute; algebra has a commutative subalgebra, the translation gen-
erators. Since the Pk&rsquo;s commute among themselves and with P2 and W2,
we can choose simultaneous eigenvectors of {Pk}nk=1, P2, and W2. In par-
ticular, we can label the vectors of an irreducible invariant subspace by the
eigenvalues of these operators. The P2 and W2 labels will be the same for
all vectors in each irreducible invariant subspace, while the Pk&rsquo;s will label
different vectors of the same invariant subspace.
</p>
<p>Let us concentrate on the momentum labels and let |ψμp 〉 be a vector in
an irreducible representation of p(p,n&minus; p), where p labels momenta and
μ distinguishes among all different vectors that have the same momentum
label. We thus have
</p>
<p>Pk
∣∣ψμp
</p>
<p>&rang;
= pk
</p>
<p>∣∣ψμp
&rang;
</p>
<p>for k = 1,2, . . . , n, (30.25)
</p>
<p>where pk is the eigenvalue of Pk . We also need to know how the &ldquo;rotation&rdquo;
operators act on |ψμp 〉. Instead of the full operator eMij θ
</p>
<p>ij
, we apply its small-
</p>
<p>angle approximation 1+Mij θ ij . Since all states are labeled by momentum,
we expect the rotated state to have a new momentum label, i.e., to be an
eigenstate of Pk . We want to show that (1+Mij θ ij )|ψμp 〉 is an eigenvector
of Pk . Let the eigenvalue be p&prime;, which should be slightly different from p.
Then, the problem reduces to determining δp&prime; &equiv; p&prime; &minus; p. Ignoring the index
μ for a moment, we have
</p>
<p>Pk|ψp&prime;〉 = p&prime;k|ψp&prime;〉 = (pk + δpk)
(
1+Mij θ ij
</p>
<p>)
|ψp〉.
</p>
<p>Using the commutation relations between Pk and Mij , we can write the LHS
as
</p>
<p>LHS = Pk
(
1+Mij θ ij
</p>
<p>)
|ψp〉 =
</p>
<p>[
pk + θ ij (Mijpk + ηjkpi &minus; ηikpj )
</p>
<p>]
|ψp〉.
</p>
<p>The RHS, to first order in infinitesimal quantities, can be expressed as
</p>
<p>RHS =
(
pk + δpk + pkθ ijMij
</p>
<p>)
|ψp〉.
</p>
<p>Comparison of the last two equations shows that
</p>
<p>δpk = θ ij (ηjkpi &minus; ηikpj )= θ ij (ηjkηil &minus; ηikηj l)pl = θ ij (Mij )klpl,</p>
<p/>
</div>
<div class="page"><p/>
<p>978 30 Representation of Lie Groups and Lie Algebras
</p>
<p>where we used Eq. (29.42). It follows that
</p>
<p>p&prime; = p + δp =
(
1 + θ ijMij
</p>
<p>)
p,
</p>
<p>stating that the rotation operator of the carrier Hilbert space rotates the mo-
mentum label of the state. Note that since &ldquo;rotations&rdquo; do not change the
length (induced by η), p&prime; and p have the same length.
</p>
<p>To obtain all the vectors of an irreducible representation of p(p,n&minus; p),
we must apply the rotation operators to vectors such as |ψμp 〉. But not all ro-
tations will change the label p; for example, in three dimensions, the vector
p will not be affected8 by a rotation about p. This motivates the following
definition.
</p>
<p>construction and
</p>
<p>properties of the little
</p>
<p>group
</p>
<p>Definition 30.3.8 Let p0 be any given eigenvalue of the translation genera-
tors. The set Rp0 of all rotations A
</p>
<p>p0 that do not change p0, is a subgroup of
the rotation group O(p,n&minus;p), called the little group corresponding to p0.little group and little
</p>
<p>algebra The little algebra consists of the generators M
p0
ij satisfying
</p>
<p>M
p0
ij p0 = 0.
</p>
<p>The significance of the little group resides in the fact that a representation
of Rp0 induces a representation of the whole Poincar&eacute; group. We shall only
sketch the proof in the following and refer the reader to Mackey [Mack 68]
for a full and rigorous discussion of induced representations.induced representations
</p>
<p>Suppose we have found an irreducible representation of Rp0 with oper-
ators Ap0 . Let App0 be the rotation that carries9 p0 to p, i.e., p = App0 p0.
Consider any rotation A and let p&prime; be the momentum obtained when A acts
on p, i.e., Ap &equiv; p&prime;. Then
</p>
<p>A App0p0︸ ︷︷ ︸
=p
</p>
<p>= Ap&prime;p0p0︸ ︷︷ ︸
&equiv;p&prime;
</p>
<p>&rArr;
(
Ap
</p>
<p>&prime;p0
)&minus;1
</p>
<p>AApp0p0 = p0.
</p>
<p>This shows that (Ap
&prime;p0)&minus;1AApp0 belongs to the little group. So,
</p>
<p>(
Ap
</p>
<p>&prime;p0
)&minus;1
</p>
<p>AApp0 = Ap0
</p>
<p>for some Ap0 &isin;Rp0 . Thus, A = Ap
&prime;p0 Ap0(App0)&minus;1, and
</p>
<p>T (A)
∣∣ψμp
</p>
<p>&rang;
&equiv; A
</p>
<p>∣∣ψμp
&rang;
= Ap&prime;p0Ap0
</p>
<p>(
App0
</p>
<p>)&minus;1∣∣ψμp
&rang;
= Ap&prime;p0Ap0
</p>
<p>∣∣ψμp0
&rang;
</p>
<p>= Ap&prime;p0
&sum;
</p>
<p>ν
</p>
<p>Tνμ
(
Ap0
</p>
<p>)∣∣ψνp0
&rang;
=
&sum;
</p>
<p>ν
</p>
<p>Tνμ
(
Ap0
</p>
<p>)
Ap
</p>
<p>&prime;p0
∣∣ψνp0
</p>
<p>&rang;
</p>
<p>=
&sum;
</p>
<p>ν
</p>
<p>Tνμ
(
Ap0
</p>
<p>)∣∣ψνp&prime;
&rang;
=
&sum;
</p>
<p>ν
</p>
<p>Tνμ
(
Ap0
</p>
<p>)∣∣ψνAp
&rang;
.
</p>
<p>8The reader should be warned that although such a rotation does not change p, the rotation
operator may change the state |ψp〉. However, the resulting state will be an eigenstate of
the Pk &rsquo;s with eigenvalue p.
9We are using the fact that O(p,n&minus; p) is transitive (see Problem 30.15).</p>
<p/>
</div>
<div class="page"><p/>
<p>30.3 Representation of Lie Algebras 979
</p>
<p>Note how the matrix elements of the representation of the little group alone
have entered in the last line. We therefore consider
</p>
<p>T (A)
∣∣ψμp
</p>
<p>&rang;
&equiv;
&sum;
</p>
<p>ν
</p>
<p>Rνμ
(
Ap0
</p>
<p>)∣∣ψνAp
&rang;
, where
</p>
<p>{
A = Ap&prime;p0Ap0(App0)&minus;1,
p&prime; &equiv; Ap.
</p>
<p>(30.26)
To avoid confusion, we have used R for the representation of the little group.
We claim that Eq. (30.26) defines a (matrix) representation of the whole
group. In fact,
</p>
<p>T (A1)T (A2)
∣∣ψμp
</p>
<p>&rang;
= T (A1)
</p>
<p>&sum;
</p>
<p>ν
</p>
<p>Rνμ
(
A
</p>
<p>p0
2
</p>
<p>)∣∣ψνA2p
&rang;
</p>
<p>=
&sum;
</p>
<p>ν
</p>
<p>Rνμ
(
A
</p>
<p>p0
2
</p>
<p>)&sum;
</p>
<p>ρ
</p>
<p>Rρν
(
A
</p>
<p>p0
1
</p>
<p>)∣∣ψρA1A2p
&rang;
</p>
<p>=
&sum;
</p>
<p>ρ
</p>
<p>(&sum;
</p>
<p>ν
</p>
<p>Rρν
(
A
</p>
<p>p0
1
</p>
<p>)
Rνμ
</p>
<p>(
A
</p>
<p>p0
2
</p>
<p>))
</p>
<p>︸ ︷︷ ︸
=Rρμ(Ap01 A
</p>
<p>p0
2 ) since R is a rep.
</p>
<p>∣∣ψρA1A2p
&rang;
.
</p>
<p>The reader may check that A
p0
1 A
</p>
<p>p0
2 &equiv; (A1A2)p0 . Therefore,
</p>
<p>T (A1)T (A2)
∣∣ψμp
</p>
<p>&rang;
=
&sum;
</p>
<p>ρ
</p>
<p>Rρμ
(
(A1A2)
</p>
<p>p0
)∣∣ψρA1A2p
</p>
<p>&rang;
&equiv; T (A1A2)
</p>
<p>∣∣ψμp
&rang;
,
</p>
<p>and T is indeed a representation. It turns out that if R is irreducible, then
so is T . The discussion above shows that the irreducible representations of
the Poincar&eacute; group are entirely determined by those of the little group and
Eq. (30.25). The recipe for the construction of the irreducible representa-
tions of p(p,n&minus; p) is now clear:
</p>
<p>Theorem 30.3.9 Choose any simultaneous eigenvector p0 of the
Pk&rsquo;s. Find the little algebra Rp0 at p0 by finding all Mij &rsquo;s satisfy-
ing Mijp0 = 0. Find all irreducible representations of Rp0 . The same
eigenvalues that label the irreducible representations of Rp0 can be
used, in addition to those of P2 and W2, to label the irreducible rep-
resentations of p(p,n&minus; p).
</p>
<p>We are particularly interested in p(3,1), the symmetry group of the spe-
cial theory of relativity. In applying the formalism developed above, we need
to make contact with the physical world. This always involves interpreta-
tions. Borrowing from the angular momentum theory, in which a physi-
cal system was given the attribute of angular momentum, the label of the
irreducible representation of the rotation group, we attribute the labels of
an irreducible representation of the Poincar&eacute; group, i.e., the eigenvalues of
the four translation generators and the two Casimir operators, to a physi-
cal system. Since the four translation generators are identified as the three</p>
<p/>
</div>
<div class="page"><p/>
<p>980 30 Representation of Lie Groups and Lie Algebras
</p>
<p>components of momentum and energy, and their specification implies their
constancy over time, we have to come to the conclusion that
</p>
<p>Box 30.3.10 An irreducible representation of the Poincar&eacute; group
specifies a free relativistic particle.
</p>
<p>There may be some internal interactions between constituents of a (com-
posite) particle, e.g. between quarks inside a proton, but as a whole, the com-
posite will be interpreted as a single particle. To construct the little group,
we have to specify a 4-momentum p0. We shall consider two cases: In the
first case, p0 &middot; p0 �= 0, whereby the particle is deduced to be massive and we
can choose10 p0 = (0,0,0,m). In the second case, p0 &middot;p0 = 0, in which case
the particle is massless, and we can choose p0 = (p,0,0,p). We consider
these two cases separately.
</p>
<p>The little group (really, the little Lie algebra) for p0 = (0,0,0,m) is ob-
tained by searching for those rotations that leave p0 fixed. This is equivalent
to searching for Mij &rsquo;s that annihilate (0,0,0,m), namely, the solutions to
</p>
<p>(Mijp0)l = (Mij )lr(p0)r = (Mij )l0m= 0 &rArr; (Mij )l0 = 0.
</p>
<p>Since (Mij )l0 = ηi0ηj l &minus; ηj0ηil , we conclude that (Mij )l0 = 0 if and only if
i �= 0 and j �= 0. Thus the little group is generated by (M23,M31,M12) which
are the components of angular momentum. The reader may also verify di-
rectly that when the 4-momentum has only a time component, the Casimir
operator W2 reduces essentially to the total angular momentum operator.
Since we are dealing with a single particle, the total angular momentum can
only be spin. Therefore, we have the following theorem.
</p>
<p>Theorem 30.3.11 In the absence of any interactions, a massive rel-
ativistic particle is specified by its mass m and its spin s, the former
being any positive number, the latter taking on integer or half-odd-
integer values.
</p>
<p>The case of the massless particle can be handled in the same way. We
seek those Mij &rsquo;s that annihilate (p,0,0,p), namely, the solutions to
</p>
<p>(Mijp0)k = (Mij )kr (p0)r = (Mij )k0p+ (Mij )k1p = 0.
</p>
<p>The reader may check that
</p>
<p>(M01p0)k = η1kp&minus; η0kp, (M02p0)k = η2kp, (M03p0)k = η3kp,
(M23p0)k = 0, (M12p0)k = η2kp, (M13p0)k = η3kp.
</p>
<p>10We use units in which c= 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>30.3 Representation of Lie Algebras 981
</p>
<p>Clearly, M23 is one of the generators of the little group. Subtracting the
middle terms and the last terms of each line, we see that M02 &minus; M12 and
M03 &minus;M13 are the other two generators. These happen to be the components
of W. In fact, it is easily verified that
</p>
<p>W 0 =W 1 = M23p, W 2 = 2p(M13 &minus; M03),
W 3 = 2p(M02 &minus; M12).
</p>
<p>(30.27)
</p>
<p>Therefore, the little group is generated by all the components of W. Fur-
thermore, W2 has zero eigenvalue for |ψp0〉 when p0 = (p,0,0,p). Since
both Casimir operators annihilate the state |ψp0〉, we need to come up with
another way of labeling the states.
</p>
<p>Historical Notes
</p>
<p>Eugene Paul Wigner (1902&ndash;1995) was the second of three children born to Hungarian
</p>
<p>Eugene Paul Wigner
</p>
<p>1902&ndash;1995
</p>
<p>Jewish parents in Budapest. His father operated a large leather tannery and hoped that
his son would follow him in that vocation, but the younger Wigner soon discovered both
a taste and an aptitude for mathematics and physics. Although Wigner tried hard to ac-
commodate his father&rsquo;s wishes, he clearly heard his calling, and the world of physics is
fortunate that he did.
Wigner began his education in what he said &ldquo;may have been the finest high school in
the world.&rdquo; He later studied chemical engineering and returned to Budapest to apply that
training in his father&rsquo;s tannery. He kept track of the seminal papers during the early years
of quantum theory and, when the lure of physics became too strong, returned to Berlin to
work in a crystallography lab. He lectured briefly at the University of G&ouml;ttingen before
moving to America to escape the Nazis.
Wigner accepted a visiting professorship to Princeton in 1930. When the appointment
was not made permanent, the disappointed young professor moved to the University of
Wisconsin, where he served happily until his new wife died suddenly of cancer only a few
months after their marriage. As Wigner prepared, quite understandably, to leave Wiscon-
sin, Princeton corrected its earlier mistake and offered him a permanent position. Except
for occasional visiting appointments in America and abroad, he remained at Princeton
until his death.
Wigner&rsquo;s contributions to mathematical physics began during his studies in Berlin, where
his supervisor suggested a problem dealing with the symmetry of atoms in a crystal.
John von Neumann, a fellow Hungarian physicist, pointed out the relevance of papers
by Frobenius and Schur on representation theory. Wigner soon became enamored with
the group theory inherent in the problem and began to apply that approach to quantum
mechanical problems. Largely at the urging of Leo Szilard (another Hungarian physicist
and Wigner&rsquo;s best friend), Wigner collected many of his results into the classic textbook
Group Theory and Its Application to the Quantum Mechanics of Atomic Spectra.
The decades that followed were filled with important contributions to mathematical
physics, with applications of group theory comprising a large share: angular momen-
tum; nuclear physics and SU(4) or &ldquo;supermultiplet&rdquo; theory; parity; and studies of the
Lorentz group and Wigner&rsquo;s classic definition of an elementary particle. Other work in-
cluded early efforts in many-body theory and a paper on level spacings derived from the
properties of Hermitian matrices that later proved useful to workers in quantum chaos.
As with most famous figures, Wigner&rsquo;s personality became as well known as his profes-
sional accomplishments. His insistence on &ldquo;reasonable&rdquo; behavior, for instance, made him
refuse to pay a relative&rsquo;s hospital bill until after the patient was released&mdash;it was obviously
unreasonable to hold a sick person hostage. His gentleness is exemplified in an anecdote
in which on getting into an argument about a tip with a New York City cab driver, Wigner
loses his patience, stamps his foot, and says, &ldquo;Oh, go to hell, . . . please!&rdquo;
He held others&rsquo; feelings in such high regard that it was said to be impossible to follow
Wigner through a door. He was light-hearted and fun-loving, but also devoted to his family
and concerned about the future of the planet. This combination of exceptional skill and
laudable humanity ensures Wigner&rsquo;s place among the most highly regarded of his field.
(Taken from E. Vogt, Phys. Today 48 (12) (1995) 40&ndash;44.)</p>
<p/>
</div>
<div class="page"><p/>
<p>982 30 Representation of Lie Groups and Lie Algebras
</p>
<p>Define the new quantities
</p>
<p>H&plusmn; &equiv;
1
</p>
<p>2
(W1 &plusmn; iW2), H0 &equiv;
</p>
<p>1
</p>
<p>2p
W0
</p>
<p>and the corresponding operators acting on the carrier space. From Eq. (30.27),
it follows that [W1,W2] = 0, W2 =&minus;4H+H&minus;, and that
</p>
<p>[H+,H0] = &minus;H+, [H+,H0] = H&minus;, [H+,H&minus;] = 0.
</p>
<p>Denote the eigenstates of W2 and H0 by |α,β〉:
</p>
<p>W2|α,β〉 = α|α,β〉, H0|α,β〉 = β|α,β〉.
</p>
<p>Then the reader may check that H&plusmn;|α,β〉 has eigenvalues α and β &plusmn; 1. By
applying H&plusmn; repeatedly, we can generate all eigenvalues of H0 and note that
they are of the form
</p>
<p>β = r + n, where n= 0,&plusmn;1,&plusmn;2, . . . and 1 &gt; r &ge; 0.
</p>
<p>Since H0 =M23, H0 is recognized as an angular momentum operator whose
eigenvalues are integer (for bosons) and half-odd integer (for fermions).
Therefore, r = 0 for bosons and r = 12 for fermions.
</p>
<p>Now, within an irreducible representation, only those |α,β〉&rsquo;s can occur
that have the same α. Therefore, if we relabel the β values by integers, then
</p>
<p>〈α,n|H0|α,m〉 = (r + n)δnm.
</p>
<p>Similarly,
</p>
<p>〈α,n|H+|α,m〉 = anδn,m+1,
〈α,n|H&minus;|α,m〉 = bnδn,m&minus;1,
</p>
<p>where an and bn are some constants. It follows that
</p>
<p>α = 〈α,n|W2|α,n〉 = 〈α,n|H+H&minus;|α,n〉
= 〈α,n|H+|α,n&minus; 1〉〈α,n&minus; 1|H&minus;|α,n〉
= anbn.
</p>
<p>If we assume that the representation is unitary, then all Wj &rsquo;s will be hermi-
tian, (H+)&dagger; = H&minus;, so an = b&lowast;n and α = |an|2 &ge; 0.
</p>
<p>If α = 0, then an = 0 and bn = 0 for all n. Consequently, H+ = 0 =
H&minus;, i.e., there are no raising or lowering operators. It follows that there
are only two spin states, corresponding to the maximum and the minimum
eigenvalues of H0. A natural axis for the projection of spin is the direction
of motion of the particle. Then the projection of spin is called helicity. Wehelicity of massless
</p>
<p>particles summarize our discussion in the following theorem.</p>
<p/>
</div>
<div class="page"><p/>
<p>30.4 Problems 983
</p>
<p>Theorem 30.3.12 In the absence of any interactions, a massless rel-
ativistic particle is specified by its spin and its helicity. The former
taking on integer or half-odd-integer values s, the latter having val-
ues +s and &minus;s.
</p>
<p>Theorems 30.3.11 and 30.3.12 are beautiful examples of the fruitfulness
of the interplay between mathematics and physics. Physics has provided
mathematics with a group, the Poincar&eacute; group, and mathematics, through
its theory of group representation, has provided physics with the deep result
that all particles must have a spin that takes on a specific value, and none
other; that massive particles are allowed to have 2s + 1 different values for
the projection of their spin; and that massless particles are allowed to have
only two values for their spin projection. Such far-reaching results that are
both universal and specific makes physics unique among all other sciences.
It also provides impetus for the development of mathematics as the only di-
alect through which nature seems to communicate to us her deepest secrets.
</p>
<p>If α &gt; 0, then the resulting representations will have continuous spin
variables. Such representations do not correspond to particles found in na-
ture; therefore, we shall not pursue them any further.
</p>
<p>30.4 Problems
</p>
<p>30.1 Show that the operation on a compact group defined by
</p>
<p>(u|v)&equiv;
&int;
</p>
<p>G
</p>
<p>〈Tgu|Tgv〉dμg
</p>
<p>is an inner product.
</p>
<p>30.2 Show that the Weyl operator Ku is hermitian.
</p>
<p>30.3 Derive Eqs. (30.5) and (30.6). Hint: Follow the finite-group analogy.
</p>
<p>30.4 Suppose that a Lie group G acts on a Euclidean space Rn as well as
on the space of (square-integrable) functions L(Rn). Let φ(α)i transform as
the ith row of the αth irreducible representation. Verify that the relation
</p>
<p>Tgφ
(α)
i (x)=
</p>
<p>nα&sum;
</p>
<p>j=1
T
(α)
ji (g)φ
</p>
<p>(α)
j
</p>
<p>(
x &middot; g&minus;1
</p>
<p>)
</p>
<p>defines a representation of G.
</p>
<p>30.5 Show that GL(V) is not a compact group. Hint: Find a continuous
function GL(V)&rarr;C whose image is not compact.</p>
<p/>
</div>
<div class="page"><p/>
<p>984 30 Representation of Lie Groups and Lie Algebras
</p>
<p>30.6 Suppose that T :G&rarr;GL(V) is a representation, and let
</p>
<p>V
&otimes;r &equiv; V&otimes; &middot; &middot; &middot; &otimes;V︸ ︷︷ ︸
</p>
<p>r times
</p>
<p>be the r-fold tensor product of V. Show that T &otimes;r :G&rarr;GL(V&otimes;r), given by
</p>
<p>T&otimes;rg (v1, . . . ,vr)= Tg(v1)&otimes; &middot; &middot; &middot; &otimes; Tg(vr),
</p>
<p>is also a representation.
</p>
<p>30.7 Suppose that in Example 30.2.2, we set k1 = 2 for our treatment of
n= 2, r = 3. Show that Y2(ek1 &otimes;ek2 &otimes;ek3) does not produce any new vector
beyond what we obtained for k1 = 1.
</p>
<p>30.8 Show that gijgsrciks is antisymmetric in j and r .
</p>
<p>30.9 Operate L+ on
</p>
<p>|00〉 =
l&sum;
</p>
<p>m=&minus;l
C(ll;0|m,&minus;m;0)|lm; l,&minus;m〉
</p>
<p>and use L+|00〉 = 0 to find a recursive relation among C(ll;0|m,&minus;m;0).
Use normalization and the convention that C(ll;0|m,&minus;m;0) &gt; 0 to show
that
</p>
<p>C(ll;0|m,&minus;m;0)= (&minus;1)l&minus;m/
&radic;
</p>
<p>2l + 1
(see Sect. 13.3).
</p>
<p>30.10 Show that the generators of so(3,1),
</p>
<p>M &equiv; (M1,M2,M3)&equiv; (M23,M31,M12),
N &equiv; (N1,N2,N3)&equiv; (M01,M02,M03),
</p>
<p>satisfy the commutation relations
</p>
<p>[Mi,Mj ] = &minus;ǫijkMk, [Ni,Nj ] = ǫijkMk, [Mi,Nj ] = &minus;ǫijkNk,
</p>
<p>and that M2 &minus; N2 and M &middot; N commute with all the M&rsquo;s and the N &rsquo;s.
</p>
<p>30.11 Let the double-indexed &ldquo;metric&rdquo; of the Poincar&eacute; algebra be defined
as
</p>
<p>gij,kl = crsij,mncmnkl,rs + crij,mcmkl,r ,
where the structure constants are given in Eq. (30.21). Show that
</p>
<p>gij,kl = 2(n&minus; 1)(ηjkηil &minus; ηikηj l).
</p>
<p>30.12 Show that [M2,Mij ] = 0, and
[
M2,Pk
</p>
<p>]
= 4MkjPj + 2(n&minus; 1)Pk.</p>
<p/>
</div>
<div class="page"><p/>
<p>30.4 Problems 985
</p>
<p>30.13 Show that the vector operator
</p>
<p>Ci &equiv;MijPj = ηkjMijPk
</p>
<p>satisfies the following commutation relations:
</p>
<p>[Ci,Pj ] = ηijP2 &minus; PiPj , [Ci,Mjk] = ηikCj &minus; ηijCk,
</p>
<p>[Ci,Cj ] =MijP2.
</p>
<p>Show also that [C2,Mjk] = 0, CiPi = 0, and
</p>
<p>PiCi =&minus;(n&minus; 1)P2,
[
C2,Pi
</p>
<p>]
=
{
2Ci + (n&minus; 1)Pi
</p>
<p>}
P2.
</p>
<p>30.14 Derive Eq. (30.24) and show that Wi1...in&minus;3 commutes with all the
Pj &rsquo;s.
</p>
<p>30.15 Let êx &equiv; (x1, . . . , xn) be any unit vector in Rn.
(a) Show that a matrix is η-orthogonal, i.e., it satisfies Eq. (29.38), if and
</p>
<p>only if its columns are η-orthogonal.
(b) Show that there exists an A &isin;O(p,n&minus; p) such that êx = Aê1 where
</p>
<p>ê1 = (1,0, . . . ,0). Hint: Find the first column of A and use (a).
(c) Conclude that O(p,n&minus; p) is transitive in its action on the collection
</p>
<p>of all vectors of the same length.
</p>
<p>30.16 Verify directly that when the 4-momentum has only a time compo-
nent, the Casimir operator W2 =W &middot;W reduces essentially to the total an-
gular momentum operator.
</p>
<p>30.17 Verify that for the case of a massless particle, when p0 = (p,0,0,p),
</p>
<p>W0 =W1 = M23, W2 = 2p(M13 &minus; M03), W3 = 2p(M02 &minus; M12),
</p>
<p>and that W2 =W &middot;W annihilates |ψp0〉.</p>
<p/>
</div>
<div class="page"><p/>
<p>31Representation of Clifford Algebras
</p>
<p>In Sect. 4.5, we discussed the representation of an algebra and its signifi-
cance in physical applications. This significance is doubled in the case of the
Clifford algebras because of their relation with the Dirac equation, which de-
scribes a relativistic spin- 12 fundamental particle such as a lepton or a quark.
With the representation of Lie groups and Lie algebras behind us, we can
now tackle the important topic of the representation of Clifford algebras.
</p>
<p>31.1 The Clifford Group
</p>
<p>Let CV be a Clifford algebra. Denote by C
&times;
V the set of invertible elements of
</p>
<p>CV , which obviously form a group. Define a map ad : C&times;V &rarr;GL(CV ) by
</p>
<p>ad(a)x= ωV (a)&or; x&or; a&minus;1, (31.1)
</p>
<p>where ωV is the degree involution of CV , and note that
</p>
<p>ad(a1 &or; a2)x= ωV (a1 &or; a2)&or; x&or; (a1 &or; a2)&minus;1
</p>
<p>= ωV (a1)&or;
[
ωV (a2)&or; x&or; a&minus;12
</p>
<p>]
︸ ︷︷ ︸
</p>
<p>=ad(a2)x&equiv;y&isin;CV
</p>
<p>&or;a&minus;11
</p>
<p>= ωV (a1)&or; y&or; a&minus;11 = ad(a1)y= ad(a1) ad(a2)x.
</p>
<p>It follows that ad(a1 &or; a2)= ad(a1) ◦ ad(a2), i.e., that ad is a group homo-
morphism, thus a group representation.
</p>
<p>Definition 31.1.1 The representation defined in Eq. (31.1) is called the
twisted adjoint
</p>
<p>representation
twisted adjoint representation.
</p>
<p>As shown in Problem 31.1, two immediate consequences of Eq. (31.1)
are
</p>
<p>ad
(
ωV (a)
</p>
<p>)
= ωV ◦ ad(a) ◦ωV (31.2)
</p>
<p>and
</p>
<p>ad(y)= Ry = 1&minus; 2y
1
</p>
<p>g(y,y)
φφφy, y &isin; V&cap; C&times;V , (31.3)
</p>
<p>S. Hassani, Mathematical Physics, DOI 10.1007/978-3-319-01195-0_31,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>987</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-01195-0_31">http://dx.doi.org/10.1007/978-3-319-01195-0_31</a></div>
</div>
<div class="page"><p/>
<p>988 31 Representation of Clifford Algebras
</p>
<p>as in Eq. (26.35); i.e., that ad(y) is a reflection operator in V. Equation (31.2)
follows from ωV ◦ωV = ι, and (31.3) from Eq. (27.14).
</p>
<p>Proposition 31.1.2 ker ad = λI, where 0 �= λ &isin;C.
</p>
<p>Proof That λI &isin; ker ad is trivial to show. To prove the converse, assume that
ad(a)= ι. Then ad(a)x= ιx= x, and therefore,
</p>
<p>x= ωV (a)&or; x&or; a&minus;1 or ωV (a)&or; x= x&or; a, for all x &isin; CV . (31.4)
</p>
<p>In particular, with x= 1, we get ωV (a)= a. Substituting this back in (31.4)
yields a &or; x= x &or; a for all x &isin; CV . This, together with ωV (a)= a implies
that a &isin; Z0V . It follows from Proposition 27.2.13 that a &isin; Span{1} or a= λ1.
Since a is invertible, λ �= 0. �
</p>
<p>Clifford group
</p>
<p>Definition 31.1.3 Let ŴV be the set of a &isin; C&times;V such that ad(a)v &isin; V
for all v &isin; V. ŴV is a subgroup of C&times;V and is called the Clifford group
of V.
</p>
<p>Example 31.1.4 It is instructive to find the Clifford group of the simple ex-
Clifford group of C
</p>
<p>ample of complex numbers from first principles to get a handle on the more
general case. We have V= R and {1, e} is a basis of the algebra.1 In order
to apply Eq. (31.1), we need a&minus;1. Of course, we know from our knowledge
of complex numbers what the inverse is. But, since we are starting from first
principles, we find a&minus;1 from scratch.
</p>
<p>Let a= α1+βe. We are looking for a&prime; = α&prime;1+β &prime;e such that a&prime;&or;a= 1.
This translates into
</p>
<p>(α1+ βe)&or;
(
α&prime;1+ β &prime;e
</p>
<p>)
= 1,
</p>
<p>or, since e &or; e =&minus;1, into
(
αα&prime; &minus; ββ &prime;
</p>
<p>)
1+
</p>
<p>(
αβ &prime; + α&prime;β
</p>
<p>)
e = 1.
</p>
<p>It follows that αα&prime;&minus;ββ &prime; = 1 and αβ &prime;+α&prime;β = 0. At least one of the compo-
nents of a, say α is nonzero. Hence, the second equation gives β &prime; =&minus;α&prime;β/α.
Substituting in the first equation, we get
</p>
<p>αα&prime; + α
&prime;β2
</p>
<p>α
= 1 or α&prime; = α
</p>
<p>α2 + β2 ,
</p>
<p>from which we obtain β &prime; =&minus;β/(α2 + β2), and
</p>
<p>a&minus;1 = α1&minus; βe
α2 + β2 ,
</p>
<p>1We avoid using the normal notation for complex numbers to be more in tune with the
notation of the Clifford algebras. So, instead of z, we use a and instead of i we use e.</p>
<p/>
</div>
<div class="page"><p/>
<p>31.1 The Clifford Group 989
</p>
<p>a familiar result from complex number theory. The uniqueness of the inverse
ensures that had we chosen β not to be zero, we would have obtained the
same result.
</p>
<p>Now we try to find the elements of ŴV , which by definition are those
which leave V invariant. Let ηe be an arbitrary element of V and recall that
ωV (1)= 1 and ωV (e)=&minus;e. Equation (31.1) now reads
</p>
<p>(α1&minus; βe)&or; (ηe)&or;
(
α1&minus; βe
α2 + β2
</p>
<p>)
,
</p>
<p>which, after multiplying through, yields
(
</p>
<p>2αβ
</p>
<p>α2 + β2 1+
α2 &minus; β2
α2 + β2 e
</p>
<p>)
η.
</p>
<p>This will be in V (i.e., a will be in ŴV ) for all values of η if either α = 0 or
β = 0, i.e., if a is real or pure imaginary.
</p>
<p>Equation (31.3) shows that any non-null (or non-isotropic) y &isin; V (i.e., a
vector for which g(y,y) �= 0) is contained in the Clifford group of V. We
shall see later that ŴV is indeed generated by such elements.
</p>
<p>Proposition 31.1.5 ŴV is stable under the two involutions ωV and σV , and
thus under the conjugation a �&rarr; ā, where ā= σVωV (a).
</p>
<p>Proof Let a &isin; ŴV , i.e., ad(a)x &isin; V if x &isin; V. Using (31.2), we have
</p>
<p>ad(ωV a)x = ωV ◦ ad(a) ◦ωV x =&minus;ωV ◦ ad(a)x = ad(a)x &isin; V,
</p>
<p>because ωV v =&minus;v for all v &isin; V. Hence, ωV (a) &isin; ŴV .
Next we show that σV a &isin; ŴV . We have
</p>
<p>ad(σV a)x = (ωV σV a)&or; x &or; (σV a)&minus;1 = (σVωV a)&or; x &or;
(
σV a
</p>
<p>&minus;1),
</p>
<p>where we used the fact that ωV and σV commute and that (σV a)&minus;1 =
σV (a
</p>
<p>&minus;1), which is true for all involutions. Since σV x = x, the right-hand
side can be written as
</p>
<p>RHS = (σVωV a)&or; σV x &or; σV
(
a&minus;1
</p>
<p>)
= σV
</p>
<p>[
a&minus;1 &or; x &or;ωV a
</p>
<p>]
,
</p>
<p>using Eq. (27.22). But
</p>
<p>a&minus;1 = ω2V a&minus;1 = ωV
(
ωV a
</p>
<p>&minus;1)= ωV (ωV a)&minus;1,
</p>
<p>and denoting (ωV a)&minus;1 by b &isin; ŴV , we have
</p>
<p>RHS = σV
[
ωV b&or; x &or; b&minus;1
</p>
<p>]
= σV
</p>
<p>[
ad(b)x
</p>
<p>]
= ad(b)x &isin; V.
</p>
<p>It follows that ad(σV a)x &isin; V, i.e., σV a &isin; ŴV . �
</p>
<p>We have defined conjugation for any element of CV . Our experience with
complex and quaternion conjugation leads us to believe that a&or; ā is a non-
negative real number. This does not hold for general a in a general Clifford
algebra. However, we have the following:</p>
<p/>
</div>
<div class="page"><p/>
<p>990 31 Representation of Clifford Algebras
</p>
<p>Proposition 31.1.6 Let θ : CV &rarr; CV be defined by θ(a)= a&or; ā. If a &isin; ŴV ,
then
</p>
<p>θ(a)= a&or; ā= λa1, λa �= 0.
</p>
<p>Proof By Proposition 31.1.5, ā &isin; ŴV . Let x be an arbitrary vector in V, and
set y = ad(ā)x. Then y &isin; V and hence σV y = y. Spelling out this identity
yields
</p>
<p>σV
(
ωV (ā)&or; x&or; ā&minus;1
</p>
<p>)
= ωV (ā)&or; x&or; ā&minus;1
</p>
<p>or
</p>
<p>σV
(
ā&minus;1
</p>
<p>)
&or; x&or; σV (ωV ā)= (σV ā)&minus;1 &or; x&or; σV (ωV ā)= ωV (ā)&or; x&or; ā&minus;1.
</p>
<p>This yields
</p>
<p>x&or; σV (ωV ā)&or; ā= σV (ā)&or;ωV (ā)&or; x.
From ā = σVωV (a), the fact that σV and ωV commute, and that both are
involutions, we get
</p>
<p>σVωV (ā)= a and σV (ā)= ωV (a).
</p>
<p>Using these in the previous equation yields
</p>
<p>x&or; a&or; ā= ωV (a&or; ā)&or; x, x &isin; V.
</p>
<p>Thus, setting b= a&or; ā, we have
</p>
<p>x&or; b= ωV (b)&or; x, x &isin; V.
</p>
<p>Now write b = b0 + b1, with b0 &isin; C0V and b1 &isin; C1V . Then the equation
above becomes
</p>
<p>x&or; b0 + x&or; b1 = (b0 &minus; b1)&or; x,
and setting the odd and even parts of both sides equal yields
</p>
<p>x&or; b0 = b0 &or; x and x&or; b1 =&minus;b1 &or; x, x &isin; V.
</p>
<p>Thus, b0 &isin; Z0V and b1 &isin; Z
1
V . From Propositions 27.2.10 and 27.2.13, we
</p>
<p>now conclude that b0 = λa1 and b1 = 0 whence b = a &or; ā = λa1 and so
θ(a)= λa1. Finally, since a is invertible, λa cannot be zero. �
</p>
<p>Example 31.1.7 In Example 31.1.4, we constructed the Clifford group
Clifford group of the
</p>
<p>quaternions
of C. Although the construction seemed trivial, it featured most of the pro-
cedure used in the general case. As a slightly more complicated example, let
us find the Clifford group of H, the quaternions. In this case, V= R2 with
basis {e1, e2}.
</p>
<p>To simplify the writing, we sometimes remove the Clifford multiplication
sign &or;, and use juxtaposition for product. We also use e12 for e1 &or; e2, in
which case the basis of H is written as {1, e1, e2, e12}, of which the first and
the last are even under the degree involution ωV , and the middle two, odd.</p>
<p/>
</div>
<div class="page"><p/>
<p>31.1 The Clifford Group 991
</p>
<p>For a = α01 + α1e1 + α2e2 + α3e12, and v = β1e1 + β2e2, Eq. (31.1)
becomes
</p>
<p>(α01&minus; α1e1 &minus; α2e2 + α3e12)(β1e1 + β2e2)
α01&minus; α1e1 &minus; α2e2 &minus; α3e12
</p>
<p>α20 + α21 + α22 + α23
,
</p>
<p>where we used Example 3.1.16 to come up with a&minus;1, although we could
have calculated it&mdash;after a straightforward, but tedious labor&mdash;using a tech-
nique similar to the one used in Example 31.1.4. Using the notation |a|2 &equiv;
α20 + α21 + α22 + α23 and multiplying out the equation above (another tedious
calculation), we obtain
</p>
<p>ωV (a)&or; v &or; a&minus;1 =
2
</p>
<p>|a|2
[
(α0α1 + α2α3)β1 + (α0α2 &minus; α1α3)β2
</p>
<p>]
1
</p>
<p>+
[(
α20 &minus; α21 + α22 &minus; α23
</p>
<p>)
β1 &minus; 2(α1α2 + α0α3)β2
</p>
<p>]
e1
</p>
<p>+
[
2(α0α3 &minus; α1α2)β1 +
</p>
<p>(
α20 + α21 &minus; α22 &minus; α23
</p>
<p>)
β2
]
e2.
</p>
<p>For this to be in V for arbitrary v, the coefficient of 1 must vanish for arbi-
trary β1 and β2. This will happen iff
</p>
<p>α0α1 + α2α3 = 0 and α0α2 &minus; α1α3 = 0. (31.5)
</p>
<p>At least one of the α&rsquo;s, say α1 is not zero. Then α0 = &minus;α2α3/α1 from the
first equation, which upon substitution in the second equation of (31.5),
yields α3(α21 + α22) = 0, whose only solution is α3 = 0, and so α0 = 0 as
well. If we assume that α0 is not zero, the solution will be α1 = 0 = α2.
Hence, ŴV consists of algebra members of the form
</p>
<p>a1 = α1e1 + α2e2 or a2 = α01+ α3e12.
</p>
<p>Note that ωV (a1) = &minus;a1, while ωV (a2) = a2. This even-oddness was also
true for the two choices of Example 31.1.4.
</p>
<p>Corollary 31.1.8 The map θ of Proposition 31.1.6 satisfies
</p>
<p>θ(ωa)= θ(a), a &isin; ŴV .
</p>
<p>Proof See Problem 31.2. �
</p>
<p>Proposition 31.1.9 The map λV : ŴV &rarr; C&times; given by λV (a)= λa is a ho-
momorphism from the Clifford group toC&times; (the multiplicative group of com-
plex numbers).
</p>
<p>Proof To prove the homomorphism of λV , note that on the one hand, θ(a&or;
b)= λV (a&or; b)1. On the other hand,
</p>
<p>θ(a&or; b)= a&or; b&or; a&or; b= a&or; (b&or; b̄)&or; ā
= a&or;
</p>
<p>(
λV (b)1
</p>
<p>)
&or; ā= a&or; ā&or;
</p>
<p>(
λV (b)1
</p>
<p>)
</p>
<p>=
(
λV (a)1
</p>
<p>)
&or;
(
λV (b)1
</p>
<p>)
= λV (a)λV (b)1.</p>
<p/>
</div>
<div class="page"><p/>
<p>992 31 Representation of Clifford Algebras
</p>
<p>It follows that
</p>
<p>λV (a&or; b)= λV (a)λV (b),
i.e., that λV is a homomorphism. �
</p>
<p>Corollary 31.1.10 For 0 �= β &isin; C and a &isin; ŴV , the homomorphism λV of
Proposition 31.1.9 satisfies
</p>
<p>1. λV (βa)= β2λV (a).
2. λV (ωa)= λV (a).
3. λV (ad(b)a)= λV (a).
</p>
<p>Proof The second relation follows from Corollary 31.1.8. The proof of the
first and third relations is left as Problem 31.3. �
</p>
<p>Equation (31.3) shows that ad(y) is the reflection operator in the plane
normal to y and, therefore, an isometry. Is ad(a) an isometry for general a?
The answer is yes, if a is restricted to the Clifford group.
</p>
<p>Proposition 31.1.11 Fix a &isin; ŴV and let τa be the restriction of ad(a) to V.
Then τa is an isometry of V.
</p>
<p>Proof Since x̄ =&minus;x for x &isin; V, Eq. (27.14) gives
</p>
<p>θ(x)=&minus;g(x,x)1,
</p>
<p>from which it follows that
</p>
<p>λV (x)=&minus;g(x,x). (31.6)
</p>
<p>Now, the second relation in Corollary 31.1.10 yields
</p>
<p>g(τax, τax)= g
(
ad(a)x, ad(a)x
</p>
<p>)
=&minus;λV
</p>
<p>(
ad(a)x
</p>
<p>)
=&minus;λV (x)= g(x,x),
</p>
<p>showing that τa is an isometry of V. �
</p>
<p>Since ad is a representation, we can establish a group homomorphism
between ŴV and O(V), the group of isometries of V. In fact, we have the
following:
</p>
<p>Proposition 31.1.12 Let ΦV : ŴV &rarr; O(V) be defined by ΦV (a) =
τa . Then ΦV is a surjective homomorphism.
</p>
<p>Proof That ΦV is a homomorphism is immediate. By Theorem 26.5.17,
every isometry is a product of reflections. Therefore, surjection is proved
if we can show that for any reflection in O(V), there is an element in ŴV
which is mapped by ΦV to that reflection. But any reflection Ry in V is</p>
<p/>
</div>
<div class="page"><p/>
<p>31.1 The Clifford Group 993
</p>
<p>just ad(y) with y being non-null, as observed in (31.3). Therefore, ΦV (y)=
τy = ad(y)= Ry , proving that ΦV is surjective. �
</p>
<p>Theorem 31.1.13 The Clifford group ŴV is generated by non-null
vectors y &isin; V.
</p>
<p>Proof Let a &isin; ŴV and set τ =ΦV (a). By Theorem 26.5.17 and the fact that
τy = Ry for y &isin; V,
</p>
<p>τ = Ry1 ◦ &middot; &middot; &middot; ◦ Ryr = τy1 ◦ &middot; &middot; &middot; ◦ τyr =ΦV (y1 &or; &middot; &middot; &middot; &or; yr), yi &isin; V
</p>
<p>where g(yi,yi) �= 0. It follows that
</p>
<p>ΦV
(
a&minus;1 &or; y1 &or; &middot; &middot; &middot; &or; yr
</p>
<p>)
= τ&minus;1τ = ι.
</p>
<p>But ΦV (b)= ad(b). Hence, by Proposition 31.1.2,
</p>
<p>a&minus;1 &or; y1 &or; &middot; &middot; &middot; &or; yr = λI, λ �= 0
</p>
<p>and
</p>
<p>a=
(
λ&minus;1y1
</p>
<p>)
&or; y2 &or; &middot; &middot; &middot; &or; yr , yi &isin; V.
</p>
<p>This completes the proof. �
</p>
<p>Example 31.1.14 Example 31.1.7 showed that the elements of ŴV for
quaternions were of two kinds:
</p>
<p>a1 = α1e1 + α2e2 or a2 = α01+ α3e3.
</p>
<p>The first one is already in R2. To show that the second one is generated by
vectors in R2, take two vectors u = η1e1 + η2e2 and v = ξ1e1 + ξ2e2 and
note that
</p>
<p>u &or; v =&minus;(η1ξ1 + η2ξ2)1+ (η1ξ2 &minus; η2ξ1)e12.
</p>
<p>We want this to be equal to a2. For that to happen, we should have
</p>
<p>η1ξ1 + η2ξ2 =&minus;α0
η1ξ2 &minus; η2ξ1 = α3.
</p>
<p>Clearly, there are infinitely many solutions. One simple solution is η1 = 1,
η2 = 0, ξ1 =&minus;α0, and ξ2 = α3. Then
</p>
<p>u = e1 and v =&minus;α0e1 + α3e2
</p>
<p>are a pair of desired vectors.</p>
<p/>
</div>
<div class="page"><p/>
<p>994 31 Representation of Clifford Algebras
</p>
<p>Corollary 31.1.15 The homomorphism ΦV satisfies
</p>
<p>detΦV (a) &middot; a= ωV (a), a &isin; ŴV .
</p>
<p>Proof Define the map φ : ŴV &rarr; ŴV given by φ(a)= detΦV (a) &middot;a. It is not
difficult to show that φ is a homomorphism and φ(x) = ωV (x) for x &isin; V.
Now apply Theorem 31.1.13. The details are left for Problem 31.4. �
</p>
<p>Example 31.1.16 In this example, we find τa1 and τa2 associated with the
elements of ŴV for H as calculated in Example 31.1.7. For x = ξ1e1 + ξ2e2,
we have
</p>
<p>τa1x = ad(a1)x = ωV (a1)&or; x &or; a&minus;11 =&minus;a1 &or; x &or; a&minus;11
</p>
<p>=&minus;(α1e1 + α2e2)(ξ1e1 + ξ2e2)
(&minus;α1e1 &minus; α2e2
</p>
<p>α21 + α22
</p>
<p>)
</p>
<p>= 1
α21 + α22
</p>
<p>[
&minus;
(
α21ξ1 + 2α1α2ξ2 &minus; α22ξ1
</p>
<p>)
e1
</p>
<p>+
(
α21ξ2 &minus; 2α1α2ξ1 &minus; α22ξ2
</p>
<p>)
e2
]
. (31.7)
</p>
<p>If we represent x as a column vector, i.e., if we represent e1 and e2 by
</p>
<p>e1 =
(
</p>
<p>1
0
</p>
<p>)
and e2 =
</p>
<p>(
0
1
</p>
<p>)
,
</p>
<p>then τa1 can be represented as a 2 &times; 2 matrix whose entries can be read off
from the two last lines of Eq. (31.7):
</p>
<p>τa1 =
</p>
<p>⎛
⎜⎝
</p>
<p>α22&minus;α21
α21+α22
</p>
<p>&minus; 2α1α2
α21+α22
</p>
<p>&minus; 2α1α2
α21+α22
</p>
<p>α21&minus;α22
α21+α22
</p>
<p>⎞
⎟⎠ .
</p>
<p>Similarly,
</p>
<p>τa2x = (α01+ α3e12)(ξ1e1 + ξ2e2)
(
α01&minus; α3e12
α20 + α23
</p>
<p>)
,
</p>
<p>from which we obtain
</p>
<p>τa2 =
</p>
<p>⎛
⎜⎝
</p>
<p>α20&minus;α23
α20+α23
</p>
<p>&minus; 2α0α3
α20+α23
</p>
<p>2α0α3
α20+α23
</p>
<p>α20&minus;α23
α20+α23
</p>
<p>⎞
⎟⎠ .
</p>
<p>It is straightforward to show that det τa1 = &minus;1 and det τa2 = 1. This, to-
gether with ωV (a1)=&minus;a1 and ωV (a2)= a2, verifies the assertion of Corol-
lary 31.1.15.</p>
<p/>
</div>
<div class="page"><p/>
<p>31.2 Spinors 995
</p>
<p>31.2 Spinors
</p>
<p>We now return to the Clifford algebra Cνμ(R). In order not to clutter sub- We are denoting Cνμ(R)
by C(μ, ν) and using
both notations
</p>
<p>interchangeably.
</p>
<p>scripts and superscripts, we shall sometimes use the notation C(μ, ν) in-
stead of Cνμ(R), it being understood that the underlying field is R. Let
V=Rnν and denote byŴŴŴ(μ,ν) the Clifford group of Rnν . Recall from Propo-
sition 31.1.9 the homomorphism λV : ŴŴŴ(μ,ν)&rarr; R&times;. Obviously kerλV =
{a &isin; ŴŴŴ(μ,ν) | λV (a) = 1} is a subgroup of ŴŴŴ(μ,ν). It turns out that, for
applications in physics, a larger subgroup is more appropriate.
</p>
<p>Definition 31.2.1 The group Pin(μ, ν) is the subgroup of ŴŴŴ(μ,ν) consist-
The group Pin(μ, ν)
</p>
<p>ing of elements a satisfying λV (a)=&plusmn;1.
</p>
<p>Clearly, kerλV &sub; Pin(μ, ν), so 1 &isin; Pin(μ, ν), as it should. Now let x be
any non-null vector in V. Then
</p>
<p>λV (&minus;1)= λV
(
&minus; x &or; x
g(x,x)
</p>
<p>)
=
(
&minus; 1
g(x,x)
</p>
<p>)2
λV (x &or; x)
</p>
<p>=
(
&minus; λV (x)
g(x,x)
</p>
<p>)2
= 1,
</p>
<p>where we used the first relation of Corollary 31.1.10 and Eq. (31.6). There-
fore, &minus;1 is contained in the kernel of λV and thus also in Pin(μ, ν).
</p>
<p>From Eq. (31.6) it follows that all vectors x &isin; V for which g(x,x) =
&plusmn;1 are contained in Pin(μ, ν). Theorem 31.1.13 and the fact that λV is a
homomorphism, gives
</p>
<p>Proposition 31.2.2 The group Pin(μ, ν) is generated by x &isin; V for which
g(x,x)=&plusmn;1.
</p>
<p>Let O(μ,ν) be the group of isometries of Rnν and consider the homo-
morphism ΦV :ŴŴŴ(μ,ν)&rarr;O(μ,ν) introduced in Proposition 31.1.12. Let
Φ denote the restriction of ΦV to Pin(μ, ν). Since ΦV is surjective, for any
τ &isin;O(μ,ν), there exists b &isin;ŴŴŴ(μ,ν) such that ΦV (b)= τ . Now let
</p>
<p>a= b&radic;|λV (b)|
and use the first relation of Corollary 31.1.10 to obtain
</p>
<p>λV (a)=
λV (b)
</p>
<p>|λV (b)|
= &plusmn;1.
</p>
<p>This shows that a &isin; Pin(μ, ν). Moreover, since ad(a)= ad(βa) for any β &isin;
R&times; (a fact that follows immediately from the definition of the twisted adjoint
representation), we have Φ(a)=ΦV (a)=ΦV (b)= τ .
</p>
<p>The equality Φ(a) = ad(a) along with Proposition 31.1.2 implies that
a &isin; kerΦ if and only if a= λ1. The first relation of Corollary 31.1.10 gives
λV (a)= λ2. Since a &isin; Pin(μ, ν), we must have |λV (a)| = 1, or λ=&plusmn;1 and
a=&plusmn;1. The discussion above leads to</p>
<p/>
</div>
<div class="page"><p/>
<p>996 31 Representation of Clifford Algebras
</p>
<p>Theorem 31.2.3 The map Φ : Pin(μ, ν)&rarr;O(μ,ν) defined by Φ(a)= τa
is a surjective group homomorphism with kerΦ = {1,&minus;1}.
</p>
<p>The group Spin(μ, ν)
</p>
<p>Definition 31.2.4 The even elements of Pin(μ, ν) form a group de-
noted by Spin(μ, ν). In other words,
</p>
<p>Spin(μ, ν)= Pin(μ, ν)&cap; C0(μ, ν).
</p>
<p>Since ωV (a)= a for a &isin; Spin(μ, ν), Corollary 31.1.15 implies
</p>
<p>Proposition 31.2.5 Any a &isin;ŴŴŴ(μ,ν) is in Spin(μ, ν) iff detΦV (a)= 1.
</p>
<p>The surjective homomorphism Φ : Pin(μ, ν)&rarr;O(μ,ν) restricts to an-
other homomorphism Ψ : Spin(μ, ν)&rarr; SO(μ, ν):
</p>
<p>Theorem 31.2.6 The map Ψ : Spin(μ, ν)&rarr; SO(μ, ν) defined by Ψ (a) =
τa is a surjective group homomorphism. Furthermore, kerΨ = {1,&minus;1}.
</p>
<p>Example 31.2.7 We showed in Example 27.4.2 that H = C(0,2). Thus,
with the new notation, we can write
</p>
<p>ŴŴŴ(0,2)= {a1,a2} = {α1e1 + α2e2, α01+ α3e12}.
</p>
<p>We now want to find λa1 and λa2 . From the definition of θ of Proposi-
tion 31.1.6, we obtain
</p>
<p>λa11= θ(a1)= a1 &or; ā1 = (α1e1 + α2e2)(&minus;α1e1 &minus; α2e2)=
(
α21 + α22
</p>
<p>)
1.
</p>
<p>It follows that λa1 = α21 + α22 . Similarly, λa2 = α20 + α23 .
If we divide ai by its length
</p>
<p>&radic;
|λai |, we obtain the elements of Pin(0,2).
</p>
<p>These are
</p>
<p>b1 =
a1&radic;
|λa1 |
</p>
<p>= α1e1 + α2e2&radic;
α21 + α22
</p>
<p>with λb1 = 1,
</p>
<p>b2 =
a2&radic;
|λa2 |
</p>
<p>= α01+ α3e12&radic;
α20 + α23
</p>
<p>with λb2 = 1.
</p>
<p>The group Spin(0,2) consists of just b2 since that is the only even elementSpin(0,2)=U(1)
of Pin(0,2). Those quaternions that are a linear combination of 1 and only
one of the other three basis elements can be identified with C. We therefore
conclude that Spin(0,2) is the set of complex numbers of unit length, i.e.,
Spin(0,2)=U(1)= {eiϕ | ϕ &isin;R}.</p>
<p/>
</div>
<div class="page"><p/>
<p>31.2 Spinors 997
</p>
<p>31.2.1 Pauli Spin Matrices and Spinors
</p>
<p>We have already seen the connection between the Dirac equation of the rel-
ativistic electron and Clifford algebras. We have also mentioned how the
Dirac equation predicted the existence of the positron through the identifi-
cation of two of the four components of the solution to the equation with
the spin of the positron. The idea of spin came out very naturally from the
Dirac equation. However, the concept was there before. Pauli had already
developed (albeit in an ad hoc way) the theory of a spinning electron in non-
relativistic quantum physics. He had represented the electron by a 2-column
vector with complex entries, a spinor, to account for the two states (up and
down) of an electron spin, and introduced certain 2&times;2 matrices&mdash;now bear-
ing his name&mdash;in the equations that described the behavior of the electron
in magnetic fields. In the following, we use Pauli&rsquo;s matrices and spinors to
motivate generalization to Clifford algebras.
</p>
<p>The fact that we are dealing with a complex 2-column points to the total
matrix algebra M2(C), which is isomorphic to C03(R) as Table 27.2 shows.
The generators of C03(R) are the basis vectors e1, e2, and e3 in R
</p>
<p>3, obeying
the multiplication rule
</p>
<p>ei &or; ej + ej &or; ei &equiv; eiej + ej ei &equiv; eij + eji = 2δij1.
</p>
<p>It is therefore instructive to find three 2&times; 2 matrices which represent e1, e2,
and e3, and therefore, obey the same rule.
</p>
<p>Referring to Eq. (27.25) and Example 27.2.6 with v = (α,β, γ ), we want
to find a relation of the form
</p>
<p>(
α11 α12
α21 α22
</p>
<p>)(
α11 α12
α21 α22
</p>
<p>)
=
(
α2 + β2 + γ 2
</p>
<p>)(1 0
0 1
</p>
<p>)
,
</p>
<p>where αij are now complex numbers. Inspired by the solution in Eq. (27.27),
we try
</p>
<p>ϕ(α,β, γ )=
(
</p>
<p>α β &minus; iγ
β + iγ &minus;α
</p>
<p>)
(31.8)
</p>
<p>and verify that indeed
</p>
<p>(
α β &minus; iγ
</p>
<p>β + iγ &minus;α
</p>
<p>)(
α β &minus; iγ
</p>
<p>β + iγ &minus;α
</p>
<p>)
=
(
α2 + β2 + γ 2
</p>
<p>)(1 0
0 1
</p>
<p>)
.
</p>
<p>For the basis vectors, Eq. (31.8) yields
</p>
<p>ϕ(1,0,0)=
(
</p>
<p>1 0
0 &minus;1
</p>
<p>)
, ϕ(0,1,0)=
</p>
<p>(
0 1
1 0
</p>
<p>)
,
</p>
<p>ϕ(0,0,1)=
(
</p>
<p>0 &minus;i
i 0
</p>
<p>)
,
</p>
<p>(31.9)
</p>
<p>which are the three Pauli spin matrices used in the non-relativistic quantum
physics of electrons.</p>
<p/>
</div>
<div class="page"><p/>
<p>998 31 Representation of Clifford Algebras
</p>
<p>One of the consequences of an isomorphism between two algebras is the
equality of their dimensions. We know that the dimension of C03(R) is 8. In
fact, a basis is given by
</p>
<p>{1, e1, e2, e3, e12, e12, e23, e123}. (31.10)
</p>
<p>Labeling the three matrices of Eq. (31.9) by σ1, σ2, σ3, respectively, we note
that2
</p>
<p>σ1σ2 = iσ3, σ1σ3 =&minus;iσ2, σ2σ3 = iσ1, σ1σ2σ3 = i1.
</p>
<p>Hence, as a real algebra, the matrices
</p>
<p>{1, σ1, σ2, σ3, i1, iσ1, iσ2, iσ3}
</p>
<p>form a basis of M2(C), as the reader can verify directly by taking a linear
combination of them with real coefficients, setting it equal to the zero 2&times; 2
matrix, and showing that all coefficients are zero.
</p>
<p>Pauli spin matrices are matrices! And Chap. 30 showed us that matrices
have a very rich Lie group and Lie algebra structures. It is therefore natural
to see if there is any relation between the Clifford algebra C03(R) and a
Lie algebra, and whether this relation can be extended to the associated Lie
group and Clifford group.
</p>
<p>First note that from the Clifford product σiσj + σjσi = 2δij1, we obtain
the Lie product
</p>
<p>σ1 &bull; σ2 &equiv; [σ1, σ2] = σ1σ2 &minus; σ2σ1 = 2σ1σ2 = 2iσ3,
</p>
<p>plus cyclic permutation of the indices, which can be concisely written as
</p>
<p>[σj , σk] = 2iǫjklσl .
</p>
<p>Now define the matrices sj =&minus;iσj/2 and note that
</p>
<p>[sj , sk] = ǫjklsl . (31.11)
</p>
<p>We thus have two kinds of connection. On the one hand, the three sj be-
ing traceless and anti-hermitian, are connected to the Lie algebra su(2) of
Box 29.1.20. Therefore, the Lie group obtained by exponentiating the sj is
connected to the Lie group SU(2). On the other hand, Problem 31.6 tells
us that Spin(3,0) &sim;= SU(2). It therefore appears that the Clifford algebra
C03(R) is related to the group Spin(3,0) in the same way that the Lie alge-
bra su(2) is related to the Lie group SU(2). The relation is further confirmed
by the commutation relations between the components of angular momen-
tum, which are the generators of the rotation group in three dimensions, i.e.,
SO(3). The commutation relations are given in Example 29.1.35, and they
are identical to those in Eq. (31.11). Hence, there seems to be the following
chain of isomorphisms:
</p>
<p>Spin(3,0)&sim;= SO(3)&sim;= SU(2).
</p>
<p>2In physics literature, the matrices are actually labeled as σ3, σ1, σ2, respectively.</p>
<p/>
</div>
<div class="page"><p/>
<p>31.2 Spinors 999
</p>
<p>The first link3 simply confirms Theorem 31.2.6.
As mentioned earlier, the electron, due to its spin, is represented be a
</p>
<p>2-column with complex entries. These are vectors on which Pauli spin ma-
trices act. How is this translated into the Clifford algebra C03(R)? Recall
from Theorem 3.3.1 that the minimal left ideals of the total matrix algebra
are matrices with only one column nonzero, and that they are generated by
a matrix with only one nonzero entry in that column. Thus, the first step is
to identify the 2-column as the first column of a 2 &times; 2 matrix:4
</p>
<p>(
ψ1
ψ2
</p>
<p>)
&sim;
(
ψ1 0
ψ2 0
</p>
<p>)
.
</p>
<p>Let S stand for the minimal left ideal generated by P, a 2 &times; 2 matrix with a
1 at the first position and zeros everywhere else:
</p>
<p>S=M2(C)
(
</p>
<p>1 0
0 0
</p>
<p>)
&sim;= C03(R)P.
</p>
<p>Using the common convention of labeling the Pauli spin matrices and iden-
tifying C03(R) with M2(C), we write
</p>
<p>e1 &sim;= σ1 =
(
</p>
<p>0 1
1 0
</p>
<p>)
, e2 &sim;= σ2 =
</p>
<p>(
0 &minus;i
i 0
</p>
<p>)
, e3 &sim;= σ3 =
</p>
<p>(
1 0
0 &minus;1
</p>
<p>)
.
</p>
<p>Then
</p>
<p>P= 1
2
(1+ e3) and P2 = P,
</p>
<p>i.e., P is a (primitive) idempotent of C03(R).
A basis of S can be found by left multiplying P by all the basis vectors
</p>
<p>of C03(R). The reader can easily show that we obtain the following four
matrices:
</p>
<p>P= 1
2
(1+ e3)=
</p>
<p>(
1 0
0 0
</p>
<p>)
, e1P=
</p>
<p>1
</p>
<p>2
(e1 + e1e3)=
</p>
<p>(
0 0
1 0
</p>
<p>)
,
</p>
<p>e2P=
1
</p>
<p>2
(e2 + e2e3)=
</p>
<p>(
i 0
0 0
</p>
<p>)
, ie1P=
</p>
<p>1
</p>
<p>2
(ie1 + ie1e3)=
</p>
<p>(
0 0
i 0
</p>
<p>)
.
</p>
<p>(31.12)
By Theorem 27.3.2, all Clifford algebras are either simple or the direct
</p>
<p>sum of two identical simple algebras. By Theorem 3.5.27 and Proposition
3.5.22, a division algebra of a Clifford algebra can be obtained by right and
left multiplying the algebra by a primitive idempotent. Since P is such an
</p>
<p>3It is not exactly an isomorphism, but a homomorphism which is locally an isomorphism,
but not globally. Because of the double-valuedness of the kernel of the homomorphism
of Theorem 31.2.6, one can think of Spin(3,0) as two identical copies of SO(3).
4Here we are ignoring the fact that the entries of the 2-column are functions rather than
numbers. The full treatment of spinors whose entries are functions requires the tensor
analysis of Clifford algebras, the so-called spin bundles, a topic which is beyond the
scope of this book.</p>
<p/>
</div>
<div class="page"><p/>
<p>1000 31 Representation of Clifford Algebras
</p>
<p>idempotent, we can extract the division algebra of C03(R) by right and left
multiplying it by P:
</p>
<p>D
0
3
&sim;= PC03(R)P.
</p>
<p>Since we have already right multiplied C03(R) by P, to get D
0
3 we need to
</p>
<p>multiply the vectors in Eq. (31.12) by P on the left. It then follows that
</p>
<p>D
0
3 = {αP+ βe2P} =
</p>
<p>{(
α + iβ 0
</p>
<p>0 0
</p>
<p>)∣∣∣∣α,β &isin;R
}
&sim;=C,
</p>
<p>which is the obvious statement that the (only) division algebra of M2(C)
is C.
</p>
<p>A general division algebra is only one step away from being a field: it
has to be commutative as well. In the majority of applications in physics,
R and C are the only two division algebras of interest. And since they are
both commutative, the ordering of scalar multiplication of an algebra A is
irrelevant:
</p>
<p>αa= aα for a &isin;A, α &isin; F
where F is either R or C. To be as general as possible, we relax this condition
of commutativity and distinguish between right and left multiplication by
the elements of the division algebra.
</p>
<p>Now note that with S&sim;= C03(R)P and D&equiv;D03 &sim;= PC03(R)P, the natural or-
der of multiplication of S by an element of D is right multiplication because
</p>
<p>SD&sim;=
(
C03(R)P
</p>
<p>)(
PC03(R)P
</p>
<p>)
=
(
C03(RP)
</p>
<p>(
C03(R)P
</p>
<p>)
= SS&sub; S.
</p>
<p>The left multiplication of a left ideal of an algebra by its division algebras,
in general, does not give back the left ideal. In our case here it does because
the division algebra is just C, which is commutative.
</p>
<p>With a multiplication by a division algebra, S becomes a linear structure&mdash;
a vector space on D. We can thus construct the representation ρ(S), the
regular representation of C03(R) in S as given in Definition 4.5.8 and The-
orem 4.5.9. We want to find a matrix representation of C03(R) in S. To do
so, we need to pick a basis of S and represent elements of C03(R) in that
basis. As a complex vector space (recall that D= C) S is two-dimensional.
Take {P1,P2} &equiv; {P, e1P} as the basis of S. To find the representation of an
arbitrary element of C03(R), it is sufficient to find matrices representing the
generators of C03(R). Using the basis in Eq. (31.10) and identifying e123 as
i (because e2123 =&minus;1), we obtain
</p>
<p>e1P1 = e1P= P2 = 0 &middot; P1 + 1 &middot; P2,
e1P2 = e1e1P= P1 = 1 &middot; P1 + 0 &middot; P2,
</p>
<p>from which we deduce that the first basis vector is represented by the matrix( 0 1
1 0
</p>
<p>)
. Similarly,
</p>
<p>e2P1 = e2P= iP2 = 0 &middot; P1 + i &middot; P2,
e2P2 = e2e1P=&minus;iP1 =&minus;i &middot; P1 + 0 &middot; P2,</p>
<p/>
</div>
<div class="page"><p/>
<p>31.2 Spinors 1001
</p>
<p>and
</p>
<p>e3P1 = e3P= P= 1 &middot; P1 + 0 &middot; P2,
e3P2 = e3e1P=&minus;P2 = 0 &middot; P1 &minus; 1 &middot; P2
</p>
<p>give rise to matrices
( 0 &minus;i
i 0
</p>
<p>)
and
</p>
<p>( 1 0
0 &minus;1
</p>
<p>)
, respectively. We thus recover the
</p>
<p>Pauli spin matrices.
</p>
<p>31.2.2 Spinors for Cνμ(R)
</p>
<p>All the foregoing observation regarding C03(R) has its generalization to the
Clifford algebras Cνμ(R). Central to the discussion was the existence of a
primitive idempotent, because it gave rise to a minimal left ideal and the
division algebra (basically the scalars), from which one could find the ma-
trices representing the basis vectors of the algebra, and from those the matrix
representation of the entire algebra.
</p>
<p>The case of C03(R), although illustrative, is not general enough. As will
become apparent, this algebra, to within equivalence of some sort, has only
one primitive idempotent, which we could guess from our knowledge of
2 &times; 2 matrices. In general, the task of finding the primitive idempotent&mdash;by
&ldquo;inspection&rdquo;&mdash;is not easy. Fortunately, there is a procedure which routinely
determines a primitive idempotent for any Clifford algebras Cνμ(R).
</p>
<p>The idea is to start with one of the multi-vectors in the standard basis
</p>
<p>{1, ei1...ir | i1 &lt; &middot; &middot; &middot;&lt; ir}μ+νr=1 (31.13)
</p>
<p>of Cνμ(R), label it eK and note that
</p>
<p>P+K =
1
</p>
<p>2
(1+ eK), and P&minus;K =
</p>
<p>1
</p>
<p>2
(1&minus; eK)
</p>
<p>are two mutually orthogonal idempotents of Cνμ(R) which sum up to 1.
Therefore, the algebra can be reduced to the (vector) direct sum of two sub-
algebras:
</p>
<p>Cνμ(R)= Cνμ(R)P+K &oplus;V Cνμ(R)P&minus;K &equiv; C+μ,ν(R)&oplus;V C&minus;μ,ν(R).
</p>
<p>Note that both the idempotency and mutual orthogonality of P&plusmn;K are impor-
tant in making sure that the above is indeed a vector direct sum. Here is why:
if x &isin; C+μ,ν(R)&cap;C&minus;μ,ν(R). Then it can be written as x= aP+K = bP&minus;K . Thus,
multiplying both sides of the last equality on the right by P&minus;K and using both
properties of P&plusmn;K , we get
</p>
<p>aP+KP
&minus;
K = b
</p>
<p>(
P&minus;K
</p>
<p>)2 &rArr; 0= bP&minus;K = x.</p>
<p/>
</div>
<div class="page"><p/>
<p>1002 31 Representation of Clifford Algebras
</p>
<p>Next we want to break up the C&plusmn;μ,ν(R). To do so, we choose a new multi-
vector eM and construct P
</p>
<p>&plusmn;
M as before. Multiplying these on the left by
</p>
<p>C&plusmn;μ,ν(R), we obtain
</p>
<p>Cνμ(R)= Cνμ(R)P+KP+M &oplus;V Cνμ(R)P&minus;KP+M &oplus;V Cνμ(R)P+KP&minus;M
&oplus;V Cνμ(R)P&minus;KP&minus;M .
</p>
<p>As before, we need to make sure that all the double idempotents are indeed
idempotents and mutually orthogonal. From
</p>
<p>(
P+KP
</p>
<p>+
M
</p>
<p>)2 = P+KP+MP+KP+M = P+KP+M
and
</p>
<p>(
P&minus;KP
</p>
<p>+
M
</p>
<p>)2 = P&minus;KP+MP&minus;KP+M = P&minus;KP+M
we conclude that P+M should commute with both P
</p>
<p>+
K and P
</p>
<p>&minus;
K . This commu-
</p>
<p>tation is achieved by demanding that eKeM = eMeK , which also guarantees
the commutativity of the remaining idempotents as well as the orthogonality
of all four double idempotents.
</p>
<p>Each time we multiply the components&mdash;which are, by the way, left
ideals of Cνμ(R)&mdash;of the direct vector sum by a new idempotent, we de-
crease the dimension by a factor of 2. It is therefore clear that eventually
we will obtain a minimal ideal. The question is when should we stop? To
answer this question, we first need
</p>
<p>Definition 31.2.8 The Radon-Hurwitz number ri for i &isin; Z is given by
Radon-Hurwitz number
</p>
<p>ri = i for i = 0,1,2; r3 = 2;
ri = 3 for i = 4,5,6,7;
</p>
<p>ri+8 = ri + 4.
</p>
<p>Now for the theorem which gives a primitive idempotent for Cνμ(R), and
which we state without proof:
</p>
<p>Theorem 31.2.9 In the standard basis (31.13) of Cνμ(R) there are k
ν
μ =
</p>
<p>ν &minus; rν&minus;μ elements eMi , which commute with one another and square to 1.
They generate 2k
</p>
<p>ν
μ idempotents which add up to 1, and the product,
</p>
<p>f=
kνμ&prod;
</p>
<p>i=1
</p>
<p>1
</p>
<p>2
(1+ eMi ),
</p>
<p>is primitive in Cνμ(R).
</p>
<p>We now see why the construction of the spinors and Pauli spin matrices
was so straightforward:
</p>
<p>k03 = 0 &minus; r&minus;3 =&minus;(r5 &minus; 4)= 1;</p>
<p/>
</div>
<div class="page"><p/>
<p>31.2 Spinors 1003
</p>
<p>and why the four idempotents of Eq. (27.57) worked in the construction of
the Majorana representation of the Dirac matrices:
</p>
<p>k13 = 1 &minus; r1&minus;3 = 1 &minus; r&minus;2 = 1 &minus; (r6 &minus; 4)= 1 &minus; (&minus;1)= 2,
</p>
<p>and the number of idempotents is 22 = 4.
</p>
<p>Definition 31.2.10 The minimal left ideal Sνμ &equiv; Cνμ(R)f, where f is as in
Theorem 31.2.9, together with the right multiplication by the division alge-
</p>
<p>spinor space
bra D&equiv; fCνμ(R)f (multiplication by a scalar), becomes a D-linear structure
called the spinor space of Cνμ(R).
</p>
<p>As indicated before, for all Cνμ(R), the division algebra D is R, C, or H.
Since almost all cases of physical interest deal with R or C, scalar multipli-
cation can be on the left or right. With Sνμ a minimal left ideal of C
</p>
<p>ν
μ(R), by
</p>
<p>Theorem 4.5.9 we have the following:
</p>
<p>Definition 31.2.11 The irreducible representation ρ : Cνμ(R)&rarr; EndD(Sνμ) spin representation
given by
</p>
<p>ρ(a)|s〉 = as, a &isin; Cνμ(R), |s〉 = s &isin; Sνμ
</p>
<p>is called the spin representation of Cνμ(R). If μ&minus; ν �= 1 mod 4, Cνμ(R) is
simple and therefore the spin representation is faithful.
</p>
<p>The procedure for finding the spin representations becomes fairly
straightforward. First find the commuting multi-vectors. The easiest way
is to pick one of the basis vectors of Rνn, say ei with i one of the μ indices.
Once this is picked, the rest of the choices have to be even multi-vectors not
including i in their indices so that ei commutes with them. So, next pick a
bivector, say ejk with j being one of the ν indices (why?) and k one of the
μ ones not equal to i. The next pick has to be even and not include i, j , or k.
Continuing in this fashion, all the kνμ idempotents can be built.
</p>
<p>After finding the idempotents, multiply them to get f. Then multiply f on
the left by all the basis vectors, and choose a set of linearly independent vec-
tors from among the results to form a basis for Sνμ. Left-multiply the basis
vectors of Sνμ by f to find the division algebra D of the representation. This
usually results either in a one-dimensional algebra, which can be identified
as R, or in a two-dimensional algebra one of whose basis vectors squares
to &minus;1, in which case the algebra can be identified as C.
</p>
<p>Now multiply the basis of Sνμ on the left by all the basis vectors of C
ν
μ(R).
</p>
<p>The result will be a D-linear combination of the basis vectors of Sνμ the
coefficients of which form the columns of the matrix representation of the
basis vectors of Cνμ(R). The important case of C
</p>
<p>1
3(R) illustrates all of these
</p>
<p>points.</p>
<p/>
</div>
<div class="page"><p/>
<p>1004 31 Representation of Clifford Algebras
</p>
<p>31.2.3 C13(R) Revisited
</p>
<p>As noted earlier, k13 = 2. So, we pick e1 and e02 &equiv; e0e2 &equiv; e0 &or; e2 as the
two commuting vectors out of which we can construct our idempotents.5 It
follows that
</p>
<p>f= 1
4
(1+ e1)(1+ e02).
</p>
<p>Next, we apply all the basis vectors of C13(R) to f to generate a basis
for S13. We quote the results and ask the reader to verify them:
</p>
<p>1 &middot; f= f, e0 &middot; f&equiv; f0, e1 &middot; f= f, e2 &middot; f&equiv;&minus;f0,
e3 &middot; f&equiv; f3, e01 &middot; f&equiv; f0, e02 &middot; f= f, e03 &middot; f&equiv; f4,
e12 &middot; f= f0, e13 &middot; f=&minus;f3, e23 &middot; f=&minus;f4, e012 &middot; f=&minus;f,
e013 &middot; f=&minus;f4, e023 &middot; f= f3, e123 &middot; f=&minus;f4, e0123 &middot; f= f3.
</p>
<p>There are four different elements, f, f0, f3, and f4. Different, of course
doesn&rsquo;t mean linearly independent, but by writing each out in terms of the
basis vectors of C13(R), the reader can verify that the four elements are in-
deed linearly independent.
</p>
<p>Now we find D by multiplying S13 on the left by f. This means multiplying
the vectors in its basis on the left and collecting all the linearly independent
vectors we obtain. We illustrate one such calculation and let the reader show
that we get zeros except when we multiply f by itself. As an illustration we
show that f &middot; f4 = 0:
</p>
<p>f &middot; f4 = fe0e3f=
1
</p>
<p>16
(1+ e1)(1+ e02)e0e3(1+ e1)(1+ e02)
</p>
<p>= 1
16
</p>
<p>e0(1&minus; e1)(1&minus; e02)(1&minus; e1)(1+ e02)e3
</p>
<p>= 1
16
</p>
<p>e0(1&minus; e1)2 (1&minus; e02)(1+ e02)︸ ︷︷ ︸
=0
</p>
<p>e3 = 0.
</p>
<p>Thus, D is one-dimensional, i.e., it is isomorphic to R. Therefore, S13 pro-
vides a real representation of C13(R).
</p>
<p>Our next task is to find the matrices representing the generators of C13(R),
namely e0, e1, e2, and e3. We show how to construct the zeroth matrix, leav-
ing the rest of the matrices to the reader. To find the matrix corresponding to
e0, multiply the ith basis vector of S13 by e0, write it as a linear combination
of the basis vectors, and note that the coefficients form the ith column of the
desired matrix. Labeling f, f0, f3, and f4 as 1, 2, 3, and 4, we obtain
</p>
<p>e0f= f0 = 0 &middot; f+ 1 &middot; f0 + 0 &middot; f3 + 0 &middot; f4,
</p>
<p>5Here, we are again using the physicists&rsquo; convention of setting e4 = e0, with e20 =&minus;1.</p>
<p/>
</div>
<div class="page"><p/>
<p>31.2 Spinors 1005
</p>
<p>yielding (0,1,0,0, ) as the entries of the first column. Multiplying the other
basis vectors, we get
</p>
<p>e0f0 = e20f=&minus;f=&minus;1 &middot; f+ 0 &middot; f0 + 0 &middot; f3 + 0 &middot; f4,
e0f3 = e0e3f= f4 = 0 &middot; f+ 0 &middot; f0 + 0 &middot; f3 + 1 &middot; f4,
e0f4 = e0e0e3f=&minus;e3f=&minus;f3 = 0 &middot; f+ 0 &middot; f0 &minus; 1 &middot; f3 + 0 &middot; f4.
</p>
<p>Collecting the columns and denoting the matrix by E0, we get
</p>
<p>E0 =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>0 &minus;1 0 0
1 0 0 0
0 0 0 &minus;1
0 0 1 0
</p>
<p>⎞
⎟⎟⎠ .
</p>
<p>The matrices representing the other basis vectors can be found similarly. We
collect all the four matrices in the following equation:
</p>
<p>E0 =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>0 &minus;1 0 0
1 0 0 0
0 0 0 &minus;1
0 0 1 0
</p>
<p>⎞
⎟⎟⎠ , E1 =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>1 0 0 0
0 &minus;1 0 0
0 0 &minus;1 0
0 0 0 1
</p>
<p>⎞
⎟⎟⎠ ,
</p>
<p>E2 =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>0 &minus;1 0 0
&minus;1 0 0 0
0 0 0 &minus;1
0 0 &minus;1 0
</p>
<p>⎞
⎟⎟⎠ , E3 =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>0 0 1 0
0 0 0 &minus;1
1 0 0 0
0 &minus;1 0 0
</p>
<p>⎞
⎟⎟⎠ .
</p>
<p>(31.14)
</p>
<p>The reader may check that these matrices satisfy the Clifford algebra rule,
</p>
<p>EμEν + EνEμ = 2ημν1,
</p>
<p>as they should.
These matrices are one representation of C13(R). Section 27.4.3 gave us
</p>
<p>another. Are these two representations equivalent? If they are, then by Def-
inition 4.5.6, there should be an invertible linear transformation connecting
the two. So, we try to find an invertible matrix A such that AγμA&minus;1 = Eμ,
or Aγμ = EμA. Assume a general A and choose its elements so that these
matrix equalities hold. For example, when μ= 0, we have, on the one had
</p>
<p>Aγ0 =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>α00 α01 α02 α03
α10 α11 α12 α13
α20 α21 α22 α23
α30 α31 α32 α33
</p>
<p>⎞
⎟⎟⎠
</p>
<p>⎛
⎜⎜⎝
</p>
<p>0 0 0 &minus;1
0 0 1 0
0 &minus;1 0 0
1 0 0 0
</p>
<p>⎞
⎟⎟⎠
</p>
<p>=
</p>
<p>⎛
⎜⎜⎝
</p>
<p>α03 &minus;α02 α01 &minus;α00
α13 &minus;α12 α11 &minus;α10
α23 &minus;α22 α21 &minus;α20
α33 &minus;α32 α31 &minus;α30
</p>
<p>⎞
⎟⎟⎠</p>
<p/>
</div>
<div class="page"><p/>
<p>1006 31 Representation of Clifford Algebras
</p>
<p>and on the other hand
</p>
<p>E0A =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>0 &minus;1 0 0
1 0 0 0
0 0 0 &minus;1
0 0 1 0
</p>
<p>⎞
⎟⎟⎠
</p>
<p>⎛
⎜⎜⎝
</p>
<p>α00 α01 α02 α03
α10 α11 α12 α13
α20 α21 α22 α23
α30 α31 α32 α33
</p>
<p>⎞
⎟⎟⎠
</p>
<p>=
</p>
<p>⎛
⎜⎜⎝
</p>
<p>&minus;α10 &minus;α11 &minus;α12 &minus;α13
α00 α01 α02 α03
&minus;α30 &minus;α31 &minus;α32 &minus;α33
α20 α21 α22 α23
</p>
<p>⎞
⎟⎟⎠ .
</p>
<p>For these two matrices to be equal, we must have
</p>
<p>α10 =&minus;α03, α11 = α02, α12 =&minus;α01, α13 = α00,
α30 =&minus;α23, α31 = α22, α32 =&minus;α21, α33 = α20.
</p>
<p>(31.15)
</p>
<p>These conditions eliminate some of the elements of A, so that it now be-
comes
</p>
<p>A =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>α00 α01 α02 α03
&minus;α03 α02 &minus;α01 α00
α20 α21 α22 α23
&minus;α23 α22 &minus;α21 α20
</p>
<p>⎞
⎟⎟⎠ .
</p>
<p>The straightforward calculation of determining the rest of the elements of A
is left to the reader. We simply quote the result:
</p>
<p>A = α00
</p>
<p>⎛
⎜⎜⎝
</p>
<p>1 0 0 0
0 0 0 1
0 0 1 0
0 1 0 0
</p>
<p>⎞
⎟⎟⎠=
</p>
<p>1
</p>
<p>α00
A&minus;1.
</p>
<p>31.3 Problems
</p>
<p>31.1 (a) Use Eq. (31.1) to prove (31.2).
</p>
<p>(b) Use the invertible vector y with g(y,y) �= 0 in Eq. (27.14) to show that
</p>
<p>y = g(y,y)y&minus;1.
</p>
<p>(c) Multiply both sides of Eq. (27.14) written for x and y by y&minus;1 and use
(b) to show that ad(y)x = Ryx.
</p>
<p>31.2 Prove Corollary 31.1.8. Hint: Show that ωV (a)= ωV (ā).
</p>
<p>31.3 Prove the first and third relations of Corollary 31.1.10.
</p>
<p>31.4 Provide the details of the proof of Corollary 31.1.15.</p>
<p/>
</div>
<div class="page"><p/>
<p>31.3 Problems 1007
</p>
<p>31.5 Consider the Clifford algebra C(2,0). Let a = α01+ α1e1 + α2e2 +
α3e12 be an element of this algebra.
</p>
<p>(a) With b= β01+ β1e1 + β2e2 + β3e12, set a&or; b= 1, find βi in terms
of αis to show that
</p>
<p>a&minus;1 = α01&minus; α1e1 &minus; α2e2 &minus; α3e12
α20 &minus; α21 &minus; α22 + α23
</p>
<p>.
</p>
<p>(b) Show that for Eq. (31.1) to hold, we must have
</p>
<p>α0α1 + α2α3 = 0 and α0α2 &minus; α1α3 = 0.
</p>
<p>Therefore, ŴŴŴ(2,0)= {α1e1 + α2e2, α01+ α3e12}.
(c) Now show that
</p>
<p>Spin(2,0)&sim;=
{
z &isin;C | |z| = 1
</p>
<p>}&sim;=U(1)&equiv;
{
eiθ | θ &isin;R
</p>
<p>}
.
</p>
<p>31.6 Using the procedure of Problem 31.5 show that
</p>
<p>Spin(3,0)&sim;=
{
q &isin;H | |q| = 1
</p>
<p>}&sim;= SU(2).
</p>
<p>Warning: You may have to use a computer algebra software!
</p>
<p>31.7 Show that {1, σ1, σ2, σ3, i1, iσ1, iσ2, iσ3}, with σi given by Eq. (31.9),
are eight linearly independent vectors of a real vector space.
</p>
<p>31.8 Find k1μ for μ= 1, . . . ,8. For μ= 6,7,8 find the corresponding max-
imum number of idempotents.
</p>
<p>31.9 Show that the maximal number of idempotents kνμ satisfies the period-
icity condition
</p>
<p>kνμ+8 = kνμ + 4.
</p>
<p>31.10 Go through the same calculation as done in the book for E0 to find
the other matrices of Eq. (31.14).
</p>
<p>31.11 Choose two different multi-vectors for C13(R); find the correspond-
ing f, S13, D, and the matrices representing the basis vectors e0, e1, e2, and e3.</p>
<p/>
</div>
<div class="page"><p/>
<p>32Lie Groups and Differential Equations
</p>
<p>Lie groups and Lie algebras, because of their manifold&mdash;and therefore, dif-
ferentiability&mdash;structure, find very natural applications in areas of physics
and mathematics in which symmetry and differentiability play important
roles. Lie himself started the subject by analyzing the symmetry of differen-
tial equations in the hope that a systematic method of solving them could be
discovered. Later, Emmy Noether applied the same idea to variational prob-
lems involving symmetries and obtained one of the most beautiful pieces
of mathematical physics: the relation between symmetries and conservation
laws. More recently, generalizing the gauge invariance of electromagnetism,
Yang and Mills have considered nonabelian gauge theories in which gauge
invariance is governed by a nonabelian Lie group. Such theories have been
successfully built for three of the four fundamental interactions: electromag-
netism, weak nuclear, and strong nuclear. Furthermore, it has been possible
to cast the fourth interaction, gravity&mdash;as described by Einstein&rsquo;s general
theory of relativity&mdash;in a language very similar to the other three interac-
tions with the promise of unifying all four interactions into a single force.
This chapter is devoted to a treatment of the first topic, application of Lie
groups to DEs. The second topic, the calculus of variations and conserva-
tion laws, will be discussed in the next chapter. The third topic, that of gauge
theories, is treated under the more general setting of fiber bundles in the last
part of the book.
</p>
<p>32.1 Symmetries of Algebraic Equations
</p>
<p>The symmetry group of a system of DEs is a transformation group that acts
on both the independent and dependent variables and transforms solutions
of the system to other solutions. In order to understand this symmetry group,
we shall first tackle the simpler question of the symmetries of a system of
algebraic equations.
</p>
<p>Definition 32.1.1 Let G be a local Lie group of transformations acting on
a manifold M . A subset S &sub; M is called G-invariant and G is called a
</p>
<p>G-invariance and
</p>
<p>symmetry group defined
symmetry group of S if whenever g &middot; P is defined for P &isin; S and g &isin; G,
then g &middot; P &isin; S.
</p>
<p>S. Hassani, Mathematical Physics, DOI 10.1007/978-3-319-01195-0_32,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>1009</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-01195-0_32">http://dx.doi.org/10.1007/978-3-319-01195-0_32</a></div>
</div>
<div class="page"><p/>
<p>1010 32 Lie Groups and Differential Equations
</p>
<p>Example 32.1.2 Let M =R2.
(a) Let G= R+ be the abelian multiplicative group of real numbers. Let
</p>
<p>it act on M componentwise: r &middot; (x, y)= (rx, ry). Then any line going
through the origin is a G-invariant subset of R2.
</p>
<p>(b) If G= SO(2) and it acts on M as usual, then any circle is a G-invariant
subset of R2.
</p>
<p>A system of algebraic equations is a system of equations
system of algebraic
</p>
<p>equations and their
</p>
<p>symmetry group
Fν(x)= 0, ν = 1,2, . . . , n,
</p>
<p>in which Fν : M &rarr; R is smooth. A solution is a point x &isin; M such that
Fν(x)= 0 for ν = 1, . . . , n. The solution set of the system is the collection
of all solutions. A Lie group G is called a symmetry group of the system
if the solution set is G-invariant.
</p>
<p>Definition 32.1.3 Let G be a local Lie group of transformations acting on
invariant map
</p>
<p>a manifold M . A map F :M &rarr;N , where N is another manifold, is called a
G-invariant map if for all P &isin;M and all g &isin;G such that g &middot; P is defined,
F(g &middot; P) = F(P ). A real-valued G-invariant function is called simply an
invariant.
</p>
<p>The crucial property of Lie group theory is that locally the group and its
algebra &ldquo;look alike&rdquo;. This allows the complicated nonlinear conditions of
invariance of subsets and functions to be replaced by the simpler linear con-
ditions of invariance under infinitesimal actions. From Definition 29.1.30,
we obtain the following proposition.
</p>
<p>Proposition 32.1.4 Let G be a local group of transformations acting on a
manifold M . A smooth real-valued function f : M &rarr; R is G-invariant if
and only if
</p>
<p>ξM |P (f )= 0 for all P &isin;M (32.1)
</p>
<p>and for every infinitesimal generator ξ &isin; g.
</p>
<p>Example 32.1.5 The infinitesimal generator for SO(2) is ξM = x&part;y &minus; y&part;x .
Any function of the form f (x2 + y2) is an SO(2)-invariant. To see this, we
apply Proposition 32.1.4:
</p>
<p>(x&part;y &minus; y&part;x)f
(
x2 + y2
</p>
<p>)
= x(2y)f &prime; &minus; y(2x)f &prime; = 0,
</p>
<p>where f &prime; is the derivative of f .
</p>
<p>The criterion for the invariance of the solution set of a system of equa-
tions is a little more complicated, because now we are not dealing with func-
tions themselves, but with their solutions. The following theorem gives such
a criterion (for a proof, see [Olve 86, pp. 82&ndash;83]):</p>
<p/>
</div>
<div class="page"><p/>
<p>32.1 Symmetries of Algebraic Equations 1011
</p>
<p>Theorem 32.1.6 Let G be a local Lie group of transformations acting on
an m-dimensional manifold M . Let F : M &rarr; Rn, n &le; m, define a system
of algebraic equations {Fν(x) = 0}nν=1, and assume that the Jacobian ma-
trix (&part;Fν/&part;xk) is of rank n at every solution x of the system. Then G is a
symmetry group of the system if and only if
</p>
<p>ξM |x(Fν)= 0 &forall;ν whenever Fν(x)= 0 (32.2)
</p>
<p>for every infinitesimal generator ξ &isin; g.
</p>
<p>Note that Eq. (32.2) is required to hold only for solutions x of the system.
</p>
<p>Example 32.1.7 Let M = R2 and G = SO(2). Consider F : M &rarr; R de-
fined by F(x, y)= x2 + y2 &minus; 1. The Jacobian matrix is simply the gradient,
</p>
<p>(&part;F/&part;x, &part;F/&part;y)= (2x,2y),
</p>
<p>and is of rank 1 for all points of the solution set, because it never vanishes
at the points where F(x, y) = 0, i.e., the unit circle. It follows from Theo-
rem 32.1.6 that G is a symmetry group of the equation F(x, y) = 0 if and
only if ξM |r(F )= 0 whenever r &isin; S1. But
</p>
<p>ξM |r(F )= (x&part;y &minus; y&part;x)F |r = 2xy &minus; 2yx = 0.
</p>
<p>This is a proof of the obvious fact that SO(2) takes points of S1 to other
points of S1.
</p>
<p>As a less trivial example, consider the function F :R2 &rarr;R given by
</p>
<p>F(x, y)= x2y2 + y4 + 2x2 + y2 &minus; 2.
</p>
<p>The infinitesimal action of the group yields
</p>
<p>ξM(F )= (x&part;y &minus; y&part;x)F = 2x3y + 2xy3 &minus; 2xy = 2xy
(
x2 + y2 &minus; 1
</p>
<p>)
.
</p>
<p>The reader may check that ξM(F )= 0 whenever F(x, y)= 0. The Jacobian
matrix of the &ldquo;system&rdquo; of equations is the gradient
</p>
<p>&nabla;F =
(
2xy2 + 4x,2x2y + 4y3 + 2y
</p>
<p>)
,
</p>
<p>which vanishes only when x = 0 = y, which does not belong to the solution
set. Therefore, the rank of the Jacobian matrix is 1. We conclude that the
solution set of F(x, y)= 0 is a rotationally invariant subset of R2. Indeed,
we have
</p>
<p>F(x, y)= x2y2 + y4 + 2x2 + y2 &minus; 2 =
(
y2 + 2
</p>
<p>)(
x2 + y2 &minus; 1
</p>
<p>)
,
</p>
<p>and the solution set is just the unit circle. Note that although the solution set
of F(x, y)= 0 is G-invariant, the function itself is not.</p>
<p/>
</div>
<div class="page"><p/>
<p>1012 32 Lie Groups and Differential Equations
</p>
<p>We now discuss how to find invariants of a given group action. Start with
a one-parameter group and write
</p>
<p>v &equiv; ξM =Xi
&part;
</p>
<p>&part;xi
</p>
<p>for the infinitesimal generator of the group in some local coordinates. A lo-
cal invariant F(x) of the group is a solution of the linear, homogeneous
PDE
</p>
<p>v(F )=X1(x) &part;F
&part;x1
</p>
<p>+ &middot; &middot; &middot; +Xn(x) &part;F
&part;xn
</p>
<p>= 0. (32.3)
</p>
<p>It follows that the gradient of F is perpendicular to the vector v. Since the
gradient of F is the normal to the hypersurface of constant F , we may con-
sider the solution of Eq. (32.3) as a surface F(x)= c whose normal is per-
pendicular to v. Each normal determines one hypersurface, and since there
are n&minus;1 linearly independent vectors perpendicular to v, there must be n&minus;1
different hypersurfaces that solve (32.3). Let us write these hypersurfaces as
</p>
<p>F j
(
x1, . . . , xn
</p>
<p>)
= cj , j = 1,2, . . . , n&minus; 1, (32.4)
</p>
<p>and note that
</p>
<p>�F j &asymp;
n&sum;
</p>
<p>i=1
</p>
<p>&part;F j
</p>
<p>&part;xi
�xi = 0, j = 1,2, . . . , n&minus; 1.
</p>
<p>A solution to this equation is suggested by (32.3):
</p>
<p>�xi = αXi &rArr; �x
i
</p>
<p>Xi
= α.
</p>
<p>For �xi &rarr; dxi , we obtain the following set of ODEs, called the character-
istic system of the original PDE,
</p>
<p>characteristic system of a
</p>
<p>PDE dx1
</p>
<p>X1(x)
= dx
</p>
<p>2
</p>
<p>X2(x)
= &middot; &middot; &middot; = dx
</p>
<p>n
</p>
<p>Xn(x)
, (32.5)
</p>
<p>whose solutions determine {F j (x)}n&minus;1j=1. To find these solutions,
</p>
<p>Box 32.1.8 Take the equalities of (32.5) one at a time, solve the first
order DE, write the solution in the form of (32.4), and read off the
functions.
</p>
<p>The reader may check that any function of the F j &rsquo;s is also a solution of
the PDE. In fact, it can be shown that any solution of the PDE is a function
of these F j &rsquo;s (see [Olve 86, pp. 86&ndash;90]).
</p>
<p>Example 32.1.9 Once again, let us consider SO(2), whose infinitesimal
generator is &minus;y&part;x + x&part;y . The characteristic &ldquo;system&rdquo; of equations is
</p>
<p>dx
</p>
<p>&minus;y =
dy
</p>
<p>x
&rArr; x dx + y dy = 0 &rArr; x2 + y2 = c.</p>
<p/>
</div>
<div class="page"><p/>
<p>32.1 Symmetries of Algebraic Equations 1013
</p>
<p>Thus, F(x, y)= x2 + y2, or any function thereof, is an invariant of the ro-
tation group in two dimensions.
</p>
<p>As a less trivial example, consider the vector field
</p>
<p>v =&minus;y &part;
&part;x
</p>
<p>+ x &part;
&part;y
</p>
<p>+
&radic;
a2 &minus; z2 &part;
</p>
<p>&part;z
</p>
<p>where a is a constant. The characteristic system of ODEs is
</p>
<p>dx
</p>
<p>&minus;y =
dy
</p>
<p>x
= dz&radic;
</p>
<p>a2 &minus; z2
.
</p>
<p>The first equation was solved above, giving the invariant F1(x, y, z) =&radic;
x2 + y2 = r . To find the other invariant, solve for x and substitute in the
</p>
<p>second equation to obtain
</p>
<p>dy&radic;
r2 &minus; y2
</p>
<p>= dz&radic;
a2 &minus; z2
</p>
<p>.
</p>
<p>The solution to this DE is
</p>
<p>arcsin
y
</p>
<p>r︸ ︷︷ ︸
α
</p>
<p>= arcsin z
a︸ ︷︷ ︸
</p>
<p>β
</p>
<p>+C &rArr; arcsin y
r
&minus; arcsin z
</p>
<p>a
= C.
</p>
<p>Hence, F2(x, y, z) = arcsin(y/r) &minus; arcsin(z/a) is a second invariant. By
taking the sine of F2, we can come up with an invariant that is algebraic
(rather than trigonometric) in form:
</p>
<p>s = sinF2 = sin(α &minus; β)= sinα cosβ &minus; cosα sinβ
</p>
<p>= y
r
</p>
<p>&radic;
</p>
<p>1 &minus; z
2
</p>
<p>a2
&minus;
&radic;
</p>
<p>1 &minus; y
2
</p>
<p>r2
</p>
<p>z
</p>
<p>a
= y
</p>
<p>&radic;
a2 &minus; z2 &minus; xz
</p>
<p>ra
.
</p>
<p>Any function of r and s is also an invariant.
</p>
<p>When the dimension of the Lie group is larger than one, the computation
of the invariants can be very complicated. If {vk}rk=1 form a basis for the
infinitesimal generators, then the invariants are the joint solutions of the
system of first order PDEs
</p>
<p>vk(F )=
n&sum;
</p>
<p>j=1
X
</p>
<p>j
</p>
<p>k (x)
&part;F
</p>
<p>&part;xj
, k = 1, . . . , r.
</p>
<p>To find such a solution, one solves the first equation and finds all its invari-
ants. Since any function of these invariants is also an invariant, it is natural
to express F as a function of the invariants of v1. One then writes the re-
maining equations in terms of these new variables and proceeds inductively.
</p>
<p>Example 32.1.10 Consider the vector fields
</p>
<p>u =&minus;y&part;x + x&part;y,</p>
<p/>
</div>
<div class="page"><p/>
<p>1014 32 Lie Groups and Differential Equations
</p>
<p>v =
(
x3z+ xy2z+ a3x&radic;
</p>
<p>x2 + y2
</p>
<p>)
&part;x +
</p>
<p>(
x2yz+ y3z+ a3y&radic;
</p>
<p>x2 + y2
</p>
<p>)
&part;y
</p>
<p>&minus;
(&radic;
</p>
<p>x2 + y2z2 + b3
)
&part;z
</p>
<p>where a and b are constants. The invariants of u are functions of r =&radic;
x2 + y2 and z. If we are to have a nontrivial solution, the invariant of
</p>
<p>v as well as its PDE should be expressible in terms of r =
&radic;
x2 + y2 and z.
</p>
<p>The reader may verify that
</p>
<p>v(F )=
(
r2z+ a3
</p>
<p>)&part;F
&part;r
</p>
<p>&minus;
(
rz2 + b3
</p>
<p>)&part;F
&part;z
</p>
<p>= 0
</p>
<p>with the characteristic equation
</p>
<p>dr
</p>
<p>r2z+ a3 =&minus;
dz
</p>
<p>rz2 + b3 .
</p>
<p>This is an exact first-order DE whose solutions are given by
</p>
<p>1
</p>
<p>2
r2z2 + a3z+ b3r = c
</p>
<p>with c an arbitrary constant. Therefore, F = 12 r2z2 + a3z+ b3r , or
</p>
<p>F(x, y, z)= 1
2
</p>
<p>(
x2 + y2
</p>
<p>)
z2 + a3z+ b3
</p>
<p>&radic;
x2 + y2,
</p>
<p>or a function thereof, is the single invariant of this group.
</p>
<p>32.2 Symmetry Groups of Differential Equations
</p>
<p>Let S be a system of partial differential equations involving p independent
variables x = (x1, . . . , xp), and q dependent variables u = (u1, . . . , uq).
The solutions of the system are of the form u = f (x), or, in component
form, uα = f α(x1, . . . , xp), α = 1, . . . , q . Let X = Rp and U = Rq be the
spaces of independent and dependent variables with coordinates {xi} and
{uα}, respectively. Roughly speaking, a symmetry group of the system S
will be a local group of transformations that map solutions of S into solu-
tions of S.
</p>
<p>Historical Notes
</p>
<p>Marius Sophus Lie (1842&ndash;1899) was the youngest son of a Lutheran pastor in Norway.
He studied mathematics and science at Christiania (which became Kristiania, then Oslo
in 1925) University where he attended Sylow&rsquo;s lectures on group theory. There followed
a few years when he could not decide what career to follow. He tutored privately after his
graduation and even dabbled a bit in astronomy and mechanics.
A turning point came in 1868 when he read papers on geometry by Poncelet and Pl&uuml;cker
from which originated the inspiration in the topic of creating geometries by using el-
ements other than points in space, and provided the seed for the rest of Lie&rsquo;s career,
prompting him to call himself a student of Pl&uuml;cker, even though the two had never met.</p>
<p/>
</div>
<div class="page"><p/>
<p>32.2 Symmetry Groups of Differential Equations 1015
</p>
<p>Lie&rsquo;s first publication won him a scholarship to work in Berlin, where he met Klein, who
</p>
<p>Marius Sophus Lie
</p>
<p>1842&ndash;1899
</p>
<p>had also been influenced by Pl&uuml;cker&rsquo;s papers. The two had quite different styles&mdash;Lie
always pursuing the broadest generalization, while Klein could become absorbed in a
charming special case&mdash;but collaborated effectively for many years. However, in 1892
the lifelong friendship between Lie and Klein broke down, and the following year Lie
publicly attacked Klein, saying, &ldquo;I am no pupil of Klein, nor is the opposite the case,
although this might be closer to the truth.&rdquo; Lie and Klein spent a summer in Paris, then
parted for some time before resuming their collaboration in Germany. While in Paris,
Lie discovered the contact transformation, which, for instance, maps lines into spheres.
During the Franco-Prussian War, Lie decided to hike to Italy. On the way, however, he was
arrested as a German spy and his mathematics notes were assumed to be coded messages.
Only after the intervention of Darboux was Lie released, and he decided to return to
Christiania. In 1871 Lie became an assistant at Christiania and obtained his doctorate.
After a short stay in Germany, he again returned to Christiania University, where a chair
of mathematics was created for him. Several years later Lie succeeded Klein at Leipzig,
where he was stricken with a condition, then called neurasthenia, resulting in fatigue and
memory loss and once thought to result from exhaustion of the nervous system. Although
treatment in a mental hospital nominally restored his health, the once robust and happy
Lie became ill-tempered and suspicious, despite the recognition he received for his work.
To lure him back to Norway, his friends at Christiania created another special chair for
him, and Lie returned in the fall of 1898. He died of anemia a few months later. Lie had
started examining partial differential equations, hoping that he could find a theory that
was analogous to Galois&rsquo;s theory of equations. He examined his contact transformations
considering how they affected a process due to Jacobi of generating further solutions
from a given one. This led to combining the transformations in a way that Lie called a
group, but is today called a Lie algebra. At this point he left his original intention of ex-
amining partial differential equations and examined Lie algebras. Killing was to examine
Lie algebras quite independently of Lie, and Cartan was to publish the classification of
semisimple Lie algebras in 1900. Much of the work on transformation groups for which
Lie is best known was collected with the aid of a postdoctoral student sent to Christiania
by Klein in 1884. The student, F. Engel, remained nine months with Lie and was instru-
mental in the production of the three volume work Theorie der Transformationsgruppen,
which appeared between 1888 and 1893. A similar effort to collect Lie&rsquo;s work in con-
tact transformations and partial differential equations was sidetracked as Lie&rsquo;s coworker,
F. Hausdorff, pursued other topics.
The transformation groups now known as Lie groups provided a very fertile area for
research for decades to come, although perhaps not at first. When Killing tried to classify
the simple Lie groups, Lie considered his efforts so poor that he admonished one of his
departing students with these words: &ldquo;Farewell, and if ever you meet that s.o.b., kill him.&rdquo;
Lie&rsquo;s work was continued (somewhat in isolation) by Cartan, but it was the papers of
Weyl in the early 1920s that sparked the renewal of strong interest in Lie groups. Much
of the foundation of the quantum theory of fundamental processes is built on Lie groups.
In 1939, Wigner showed that application of Lie algebras to the Lorentz transformation
required that all particles have the intrinsic properties of mass and spin.
</p>
<p>To make precise the above statement, we have to clarify the meaning
transform the graph of a
</p>
<p>function to find the
</p>
<p>function&rsquo;s transform!
</p>
<p>of the action of G on a function u = f (x). We start with identifying the
function f (i.e., a map) with its graph (see Chap. 1),
</p>
<p>Ŵf &equiv;
{(
x,f (x)
</p>
<p>)
| x &isin;Ω
</p>
<p>}
&sub;X&times;U,
</p>
<p>where Ω &sub;X is the domain of definition of f . If the action of g &isin;G on Ŵf
is defined, then the transform of Ŵf by g is
</p>
<p>g &middot; Ŵf =
{
(x̃, ũ)= g &middot; (x,u) | (x,u) &isin; Ŵf
</p>
<p>}
.
</p>
<p>In general, g &middot;Ŵf may not represent the graph of a function&mdash;in fact, it may transform of a function
by a group elementnot be even a function at all. However, by choosing g close to the identity</p>
<p/>
</div>
<div class="page"><p/>
<p>1016 32 Lie Groups and Differential Equations
</p>
<p>of G and shrinking the size of Ω , we can ensure that g &middot; Ŵf = Ŵf̃ , i.e., that
g &middot; Ŵf is indeed the graph of a function ũ= f̃ (x̃). We write f̃ = g &middot; f and
call f̃ the transform of f by g.
</p>
<p>Example 32.2.1 Let X =R=U , so that we are dealing with an ODE. Let
G= SO(2) be the rotation group acting on X&times;U =R2. The action is given
by
</p>
<p>(x̃, ũ)= θ &middot; (x,u)= (x cos θ &minus; u sin θ, x sin θ + u cos θ). (32.6)
</p>
<p>If u= f (x) is a function, the group SO(2) acts on its graph Ŵf by rotating
it. This process can lead to a rotated graph θ &middot; Ŵf , which may not be the
graph of a single-valued function. However, if we restrict the interval of
definition of f , and make θ small enough, then θ &middot;Ŵf will be the graph of a
well-defined function ũ= f̃ (x̃) with Ŵ
</p>
<p>f̃
= θ &middot; Ŵf . If we substitute f (x) for
</p>
<p>u, we obtain
</p>
<p>(x̃, ũ)= θ &middot;
(
x,f (x)
</p>
<p>)
=
(
x cos θ &minus; f (x) sin θ, x sin θ + f (x) cos θ
</p>
<p>)
,
</p>
<p>or
</p>
<p>x̃ = x cos θ &minus; f (x) sin θ,
ũ= x sin θ + f (x) cos θ.
</p>
<p>(32.7)
</p>
<p>Eliminating x from these two equations yields ũ in terms of x̃, from which
the function f̃ can be deduced.
</p>
<p>As a specific example, consider f (x) = kx2. Then, the first equation of
(32.7) gives
</p>
<p>(k sin θ)x2 &minus; cos θx + x̃ = 0 &rArr; x = cos θ &minus;
&radic;
</p>
<p>cos2 θ &minus; 4kx̃ sin θ
2k sin θ
</p>
<p>,
</p>
<p>where we kept the root of the quadratic equation that gives a finite answer
in the limit θ &rarr; 0. Inserting this in the second equation of (32.7) and sim-
plifying yields
</p>
<p>ũ= f̃ (x̃)= cos θ &minus;
&radic;
</p>
<p>cos2 θ &minus; 4kx̃ sin θ
2k sin2 θ
</p>
<p>&minus; x̃ cot θ.
</p>
<p>We write this as
</p>
<p>f̃ (x)&equiv; (θ &middot; f )(x)= cos θ &minus;
&radic;
</p>
<p>cos2 θ &minus; 4kx sin θ
2k sin2 θ
</p>
<p>&minus; x cot θ.
</p>
<p>This equation defines the function f̃ = θ &middot; f .
</p>
<p>The foregoing example illustrates the general procedure for finding the
transformed function f̃ = g &middot; f :</p>
<p/>
</div>
<div class="page"><p/>
<p>32.2 Symmetry Groups of Differential Equations 1017
</p>
<p>Box 32.2.2 If the rule of transformation of g &isin;G is given by
</p>
<p>(x̃, ũ)= g &middot; (x,u)=
(
Ψg(x,u),Φg(x,u)
</p>
<p>)
,
</p>
<p>then the graph Ŵ
f̃
= g &middot; Ŵf of g &middot; f is given parametrically by
</p>
<p>x̃ = Ψg
(
x,f (x)
</p>
<p>)
, ũ=Φg
</p>
<p>(
x,f (x)
</p>
<p>)
. (32.8)
</p>
<p>In principle, we can solve the first equation for x in terms of x̃ and sub-
stitute in the second equation to find ũ in terms x̃, and consequently f̃ .
</p>
<p>For some special but important cases, the transformed functions can be
obtained explicitly. If G is projectable, i.e., if the action of G on x does projectable group
not depend on u, then Eq. (32.8) takes the special form x̃ = Ψg(x) and ũ=
Φg(x, f (x)) in which Ψg is a diffeomorphism of X with inverse Ψg&minus;1 . If
Ŵf is the graph of a function f , then its transform g &middot; Ŵ is always the graph
of some function. In fact,
</p>
<p>ũ= f̃ (x̃)=Φg
(
x,f (x)
</p>
<p>)
=Φg
</p>
<p>(
Ψg&minus;1(x̃), f
</p>
<p>(
Ψg&minus;1(x̃)
</p>
<p>))
. (32.9)
</p>
<p>In particular, if G transforms only the independent variables, then
</p>
<p>ũ= f̃ (x̃)= f (x)= f
(
Ψg&minus;1(x̃)
</p>
<p>)
&rArr; f̃ = f ◦Ψg&minus;1 . (32.10)
</p>
<p>For example, if G is the group of translations x �&rarr; x+a, then the transform
of f will be defined by f̃ (x)= f (x &minus; a).
</p>
<p>Definition 32.2.3 A symmetry group of a system of DEs S is a local group symmetry group of a
system of DEsof transformations G acting on an open subset M of X&times;U with the property
</p>
<p>that whenever u= f (x) is a solution of S and f̃ &equiv; g &middot;f is defined for g &isin;G,
then u= f̃ (x) is also a solution of S.
</p>
<p>The importance of knowing the symmetry group of a system of DEs lies
in the property that from one solution we may be able to obtain a family
of other solutions by applying the group elements to the given solution. To
find such symmetry groups, we have to be able to &ldquo;prolong&rdquo; the action of
a group to derivatives of the dependent variables as well. This is obvious
because to test a symmetry, we have to substitute not only the transformed
function u= f̃ (x), but also its derivatives in the DE to verify that it satisfies
the DE.
</p>
<p>32.2.1 Prolongation of Functions
</p>
<p>Given a function f :Rp &sup;X&rarr;R, there are
</p>
<p>pk &equiv;
(
p+ k &minus; 1
</p>
<p>k
</p>
<p>)</p>
<p/>
</div>
<div class="page"><p/>
<p>1018 32 Lie Groups and Differential Equations
</p>
<p>different derivatives of order k of f . We use the multi-index notation
</p>
<p>&part;J (k)f (x)&equiv;
&part;kf (x)
</p>
<p>&part;xj1&part;xj2 &middot; &middot; &middot; &part;xjk
</p>
<p>for these derivatives, where J (k) &equiv; (j1, . . . , jk) &isin; Nk is an unordered k-
tuple of nonnegative integers with 1 &le; jk &le; p (see also Sect. 21.1).1 The
order of the multi-index J (k), denoted by |J (k)|, is the sum of its com-
ponents and indicates the order of differentiation. So, in the derivative
above, |J (k)| = j1 + &middot; &middot; &middot; + jk = k. For a smooth map f : X &rarr; U , we have
f (x)= (f 1(x), . . . , f q(x)), so that we need q &middot;pk numbers to represent all
k-th order derivatives &part;J (k)f
</p>
<p>α(x) of all components of f . We include the
case of k = 0, in which case &part;J (0)f α(x)= f α(x).
</p>
<p>To geometrize the treatment of DEs (and thus facilitate the study of their
invariance), we need to construct a space in which derivatives of all orders
up to a certain number n participate. Since derivatives need functions to
act on, we arrive at the space of functions whose derivatives share certain
common properties. To be specific, me make the following definition.
</p>
<p>Definition 32.2.4 Let f and h be functions defined on a neighborhood of
a point a &isin; X with values in U . We say that f and h are n-equivalent atn-equivalence of
</p>
<p>functions a if &part;J (k)f
α(a)= &part;J (k)hα(a) for all α and k = 0,1, . . . , n. The collection of
</p>
<p>all U -valued functions defined on a neighborhood of a will be denoted by
Ŵa(X&times;U), and all functions n-equivalent to f at a by jna f .
</p>
<p>A convenient representative of such equivalent functions is the Taylor
polynomial of order n (the terms in the Taylor series up to nth order) about a.
Now collect all jna f for all a and f , and denote the result by J
</p>
<p>n(X &times; U),
so that
</p>
<p>J n(X&times;U)&equiv;
{
jna f | &forall;a &isin;X and &forall;f &isin; Ŵa(X&times;U)
</p>
<p>}
. (32.11)
</p>
<p>J n(X&times;U) is called the nth prolongation of U , or the nth jet space of U .nth jet space of U
It turns out that J n(X&times;U) is a manifold (see [Saun 89, pp. 98 and 199]).
</p>
<p>Theorem 32.2.5 J n(X &times; U) is a manifold with natural coordinate func-
tions (xi, {uα
</p>
<p>J (k)
}nk=1)&equiv; (xi, uαJ ) defined by
</p>
<p>xi
(
jna f
</p>
<p>)
= ai, uα
</p>
<p>J (k)
</p>
<p>(
jna f
</p>
<p>)
= &part;J (k)f α(a), k = 0,1, . . . , n.
</p>
<p>The natural coordinate functions allow us to identify the space of theNote that the &ldquo;points&rdquo; of
J n(X&times;U) are U -valued
</p>
<p>functions!
</p>
<p>derivatives with various powers of R. Let Uk &equiv; Rqpk denote the set of co-
ordinates uα
</p>
<p>J (k)
, and let U (n) &equiv;U &times;U1 &times; &middot; &middot; &middot; &times;Un be the Cartesian product
</p>
<p>space2 whose coordinates represent all the derivatives uα
J (k)
</p>
<p>of all orders
</p>
<p>1We shall usually omit the superscript (k) in J (k) when it is understood that all orders of
J (k) up to a certain given number are involved.
2Note that U , identified with the space of zeroth derivative, is a factor in U (n).</p>
<p/>
</div>
<div class="page"><p/>
<p>32.2 Symmetry Groups of Differential Equations 1019
</p>
<p>from 0 to n. The dimension of U (n) is
</p>
<p>q + qp1 + &middot; &middot; &middot; + qpn = q
(
p+ n
n
</p>
<p>)
&equiv; qp(n).
</p>
<p>A typical point in U (n) is denoted by u(n), which has qp(n) different com-
ponents {uα
</p>
<p>J (k)
}qα=1, where J (k) runs over all unordered multi-indices J (k) =
</p>
<p>(j1, . . . , jk) with 1 &le; jk &le; p and 0 &le; k &le; n. The nth jet space J n(X &times; U)
can now be identified with X&times;U (n). From now on, we shall use X&times;U (n)
in place of J n(X&times;U).
</p>
<p>Example 32.2.6 Let p = 3 and u = 1, i.e., X = R3 and U = R. The co-
ordinates of X are (x, y, z) and that of U is u. The coordinates of U1 are
(ux, uy, uz), where the subscript denotes the variable of differentiation. Sim-
ilarly, the coordinates of U2 are
</p>
<p>(uxx, uxy, uxz, uyy, uyz, uzz)
</p>
<p>and those of U (2) &equiv;U &times;U1 &times;U2 are
</p>
<p>(u;ux, uy, uz;uxx, uxy, uxz, uyy, uyz, uzz),
</p>
<p>which shows that U (2) is 10-dimensional.
</p>
<p>Definition 32.2.7 Given a smooth map f :X &sup;Ω &rarr; U , we define a map
pr(n)f :Ω &rarr;U (n) whose components (pr(n)f )αJ are given by prolongation of a
</p>
<p>function(
pr(n)f
</p>
<p>)α
J
(x)&equiv; &part;Jf α(x)&equiv;
</p>
<p>({
&part;J (k)f
</p>
<p>α(x)
}n
k=0
</p>
<p>)
.
</p>
<p>This map is called the nth prolongation of f .
</p>
<p>Thus, for each x &isin;X, pr(n)f (x) is a vector in Rqp(n) whose components
are the values of f and all its derivatives up to order n at the point x. For
example, in the case of p = 3, q = 1 discussed above, pr(2)f (x, y, z) has
components
</p>
<p>(
f ; &part;f
</p>
<p>&part;x
,
&part;f
</p>
<p>&part;y
,
&part;f
</p>
<p>&part;z
; &part;
</p>
<p>2f
</p>
<p>&part;x2
,
&part;2f
</p>
<p>&part;x&part;y
,
&part;2f
</p>
<p>&part;x&part;z
,
&part;2f
</p>
<p>&part;y2
,
&part;2f
</p>
<p>&part;y&part;z
,
&part;2f
</p>
<p>&part;z2
</p>
<p>)
.
</p>
<p>When the underlying space is an open subset M of X&times;U , its corresponding
jet space is
</p>
<p>M(n) &equiv;M &times;U1 &times; &middot; &middot; &middot; &times;Un,
which is a subspace of X&times;U (n) &sim;= J n(X&times;U). If the graph of f :X&rarr; U
lies in M , the graph of pr(n)f lies in M(n).
</p>
<p>Prolongation allows us to turn a system of DEs into a system of algebraic
equations: Given a system of l DEs
</p>
<p>�ν
({
xi
}
,
{
uα
</p>
<p>}
,
{
&part;iu
</p>
<p>α
}
,
{
&part;i&part;ju
</p>
<p>α
}
, . . . ,
</p>
<p>{
&part;i1 . . . &part;inu
</p>
<p>α
})
</p>
<p>= 0, ν = 1, . . . , l,</p>
<p/>
</div>
<div class="page"><p/>
<p>1020 32 Lie Groups and Differential Equations
</p>
<p>one can define a map � :M(n) &rarr;Rl and identify the system of DEs with
</p>
<p>S� &equiv;
{(
x,u(n)
</p>
<p>)
&isin;M(n)
</p>
<p>∣∣�
(
x,u(n)
</p>
<p>)
= 0
</p>
<p>}
.
</p>
<p>By identifying the system of DEs with the subset S� of the jet space, we
have translated the abstract relations among the derivatives of u into a geo-
metrical object S�, which is more amenable to symmetry operations.
</p>
<p>Definition 32.2.8 Let Ω be a subset of X and f :Ω &rarr; U a smooth map.
Then f is called a solution of the system of DEs S� if
</p>
<p>�
(
x,pr(n)f (x)
</p>
<p>)
= 0 &forall;x &isin;Ω.
</p>
<p>Just as we identified a function with its graph, we can identify the solution
of a system of DEs with the graph of its prolongation pr(n)f . This graph,
which is denoted by Ŵ(n)f , will clearly be a subset of S�:
</p>
<p>Ŵ
(n)
f &equiv;
</p>
<p>{(
x,pr(n)f (x)
</p>
<p>)}
&sub; S�.
</p>
<p>Box 32.2.9 An nth order system of differential equations is taken to
be a subset S� of the jet space J n(X &times; U), and a solution to be a
smooth map f :Ω &rarr; J n(X &times; U) the graph of whose nth prolonga-
tion pr(n)f is contained in S�.
</p>
<p>Example 32.2.10 Consider Laplace&rsquo;s equation
</p>
<p>&nabla;2u= uxx + uyy + uzz = 0
</p>
<p>with p = 3, q = 1, and n = 2. The total jet space is the 13-dimensional
Euclidean space X&times;U (2), whose coordinates are taken to be
</p>
<p>(x, y, z;u;ux, uy, uz;uxx, uxy, uxz, uyy, uyz, uzz).
</p>
<p>In this 13-dimensional Euclidean space, Laplace&rsquo;s equation defines a 12-
dimensional subspace S� consisting of all points in the jet space whose
eighth, eleventh, and thirteenth coordinates add up to zero. A solution f :
R3 &sup;Ω &rarr;U &sub;R must satisfy
</p>
<p>&part;2f
</p>
<p>&part;x2
+ &part;
</p>
<p>2f
</p>
<p>&part;y2
+ &part;
</p>
<p>2f
</p>
<p>&part;z2
= 0 &forall;(x, y, z) &isin;Ω.
</p>
<p>This is the same as requiring the graph Ŵ(2)f to lie in S�. For example, if
</p>
<p>f (r)&equiv; f (x, y, z)= x3yz&minus; y3xz+ y3 &minus; 3yz2,
</p>
<p>then, collecting each section of pr(2)f with fixed α (see Definition 32.2.7)
separately, we have
</p>
<p>(
pr(2)f
</p>
<p>)1
(r)= x3yz&minus; y3xz+ y3 &minus; 3yz2,</p>
<p/>
</div>
<div class="page"><p/>
<p>32.2 Symmetry Groups of Differential Equations 1021
</p>
<p>(
pr(2)f
</p>
<p>)2
(r)=
</p>
<p>(
3x2yz&minus; y3z, x3z&minus; 3y2xz+ 3y2 &minus; 3z2, x3y &minus; y3x &minus; 6yz
</p>
<p>)
,
</p>
<p>(
pr(2)f
</p>
<p>)3
(r)=
</p>
<p>(
6xyz,3x2z&minus; 3y2z,3x2y &minus; y3,
</p>
<p>&minus; 6xyz+ 6y, x3 &minus; 3xy2 &minus; 6z,&minus;6y
)
</p>
<p>which lies in S� because the sum of the eighth, the eleventh, and the thir-
teenth coordinates of (x, y, z;pr(2)f (x, y, z)) is 6xyz&minus;6xyz+6y&minus;6y = 0.
</p>
<p>32.2.2 Prolongation of Groups
</p>
<p>Suppose G is a group of transformations acting on M &sub;X&times;U . It is possi-
ble to prolong this action to the n-jet space M(n). The resulting group that
acts on M(n) is called the nth prolongation of G and denoted by pr(n)G nth prolongation of a
</p>
<p>group actionwith group elements pr(n)g, for g &isin; G. This prolongation is defined natu-
rally: The derivatives of a function f with respect to x are transformed into
derivatives of f̃ = g &middot; f with respect to x̃. More precisely,
</p>
<p>pr(n)g &middot;
(
jnx f
</p>
<p>)
&equiv;
(
x̃,pr(n)f̃ (x̃)
</p>
<p>)
&equiv;
(
x̃,pr(n)(g &middot; f )(x̃)
</p>
<p>)
. (32.12)
</p>
<p>For n = 0, Eq. (32.12) reduces to the action of G on M as given by
Eq. (32.8). That the outcome of the action of the prolongation of G in the
defining equation (32.12) is independent of the representative function f
follows from the chain rule and the fact that only derivatives up to the nth
order are involved. The following example illustrates this.
</p>
<p>Example 32.2.11 Let X, U , and G be as in Example 32.2.1. In this case,
we have
</p>
<p>pr(1)θ &middot;
(
j1xf
</p>
<p>)
&equiv; pr(1)θ &middot; (x,u,u1)&equiv; (x̃, ũ, ũ1).
</p>
<p>We calculated x̃ and ũ in that example. They are
</p>
<p>x̃ = x cos θ &minus; u sin θ,
ũ= x sin θ + u cos θ.
</p>
<p>(32.13)
</p>
<p>To find ũ1, we need to differentiate the second equation with respect to x̃
and express the result in terms of the original variables. Thus
</p>
<p>ũ1 &equiv;
dũ
</p>
<p>dx̃
= dũ
</p>
<p>dx
</p>
<p>dx
</p>
<p>dx̃
=
(
</p>
<p>sin θ + du
dx
</p>
<p>cos θ
</p>
<p>)
dx
</p>
<p>dx̃
= (sin θ + u1 cos θ)
</p>
<p>dx
</p>
<p>dx̃
;
</p>
<p>dx/dx̃ is obtained by differentiating the first equation of (32.13):
</p>
<p>1 = dx
dx̃
</p>
<p>cos θ &minus; du
dx̃
</p>
<p>sin θ = dx
dx̃
</p>
<p>cos θ &minus; du
dx
</p>
<p>dx
</p>
<p>dx̃
sin θ = (cos θ &minus; u1 sin θ)
</p>
<p>dx
</p>
<p>dx̃
,
</p>
<p>or
</p>
<p>dx
</p>
<p>dx̃
= 1
</p>
<p>cos θ &minus; u1 sin θ
.</p>
<p/>
</div>
<div class="page"><p/>
<p>1022 32 Lie Groups and Differential Equations
</p>
<p>It therefore follows that
</p>
<p>ũ1 =
sin θ + u1 cos θ
cos θ &minus; u1 sin θ
</p>
<p>and
</p>
<p>pr(1)θ &middot; (x,u,u1)=
(
x cos θ &minus; u sin θ, x sin θ + u cos θ, sin θ + u1 cos θ
</p>
<p>cos θ &minus; u1 sin θ
</p>
<p>)
.
</p>
<p>We note that the RHS involves derivatives up to order one. Therefore, the
transformation is independent of the representative function. So, if we had
chosen j1xh where h is 1-equivalent to f , we would have obtained the same
result. This holds for derivatives of all orders. Therefore, the prolongation
of the action of the group G is well-defined.
</p>
<p>In many cases, it is convenient to choose the nth-order Taylor polynomial
as the representative of the class of n-equivalent functions, and, if possible,
write the transformed function f̃ explicitly in terms of x̃, and differentiate
it to obtain the transformed derivatives (see Problem 32.3).
</p>
<p>Example 32.2.11 illustrates an important property of the prolongation
of G. We note that the first prolongation pr(1)G acts on the original coordi-
nates (x,u) in exactly the same way that G does. This holds in general:
</p>
<p>Box 32.2.12 The effect of the nth prolongation pr(n)G to derivatives
up to order m&le; n is exactly the same as the effect of pr(m)G. If we al-
ready know the action of themth-order prolonged group pr(m)G, then
to compute pr(n)G we need only find how the derivatives uαJ of order
higher than m transform, because the lower-order action is already
determined.
</p>
<p>32.2.3 Prolongation of Vector Fields
</p>
<p>The geometrization of a system of DEs makes it possible to use the ma-
chinery of differentiable manifolds, Lie groups, and Lie algebras to un-
ravel the symmetries of the system. At the heart of this machinery are
the infinitesimal transformations, which are directly connected to vector
fields. Therefore, it is necessary to find out how a vector field defined in
M &sub; X &times; U is prolonged. The most natural way to prolong a vector field
is to prolong its integral curve&mdash;which is a one-parameter group of trans-
formations of M&mdash;to a curve in M(n) and then calculate the tangent to the
latter curve.
</p>
<p>Definition 32.2.13 Let M be an open subset of X&times;U and X &isin;X(M). Thenth prolongation of a
vector field nth prolongation of X, denoted by pr(n)X, is a vector field on the nth jet</p>
<p/>
</div>
<div class="page"><p/>
<p>32.2 Symmetry Groups of Differential Equations 1023
</p>
<p>space M(n) defined by
</p>
<p>pr(n)X
∣∣
(x,u(n))
</p>
<p>= d
dt
</p>
<p>pr(n)
[
exp(tX)
</p>
<p>]
&middot;
(
x,u(n)
</p>
<p>)∣∣
t=0
</p>
<p>for any (x,u(n)) &isin;M(n).
</p>
<p>Since (x,u(n)) &isin; M(n) form a coordinate system on M(n), any vector
field in M(n) can be written as a linear combination of &part;/&part;xi and &part;/&part;uαJ
with coefficients being, in general, functions of all coordinates xi and uαJ .
For a prolonged vector, however, we have
</p>
<p>pr(n)X =
p&sum;
</p>
<p>i=1
Xi
</p>
<p>&part;
</p>
<p>&part;xi
+
</p>
<p>q&sum;
</p>
<p>α=1
</p>
<p>&sum;
</p>
<p>J
</p>
<p>XαJ
&part;
</p>
<p>&part;uαJ
, (32.14)
</p>
<p>where Xi and Xα0 are functions only of x
k and u. This is due to the remark
</p>
<p>made in Box 32.2.12. For the same reason, the coefficients Xα
J (m)
</p>
<p>corre-
sponding to derivatives of order m will be independent of coordinates uα
</p>
<p>J (k)
</p>
<p>for k &gt; m. Thus, it is possible to construct various prolongations of a given
vector field recursively.
</p>
<p>Example 32.2.14 Let us consider our recurrent example of X &sim;= U &sim;= R,
G = SO(2). Given the infinitesimal generator v = &minus;u&part;x + x&part;u, one can
solve the DE of its integral curve to obtain3
</p>
<p>exp(tv)(x,u)= (x cos t &minus; u sin t, x sin t + u cos t).
</p>
<p>Example 32.2.11 calculated the first prolongation of SO(2). So
</p>
<p>pr(1) exp(tv) &middot; (x,u,u1)
</p>
<p>=
(
x cos t &minus; u sin t, x sin t + u cos t, sin t + u1 cos t
</p>
<p>cos t &minus; u1 sin t
</p>
<p>)
.
</p>
<p>Differentiating the components with respect to t at t = 0 gives
&part;
</p>
<p>&part;t
(x cos t &minus; u sin t)
</p>
<p>∣∣∣∣
t=0
</p>
<p>=&minus;u,
</p>
<p>&part;
</p>
<p>&part;t
(x sin t + u cos t)
</p>
<p>∣∣∣∣
t=0
</p>
<p>= x,
</p>
<p>&part;
</p>
<p>&part;t
</p>
<p>(
sin t + u1 cos t
cos t &minus; u1 sin t
</p>
<p>)∣∣∣∣
t=0
</p>
<p>= 1 + u21.
</p>
<p>Therefore,
</p>
<p>pr(1)v =&minus;u &part;
&part;x
</p>
<p>+ x &part;
&part;u
</p>
<p>+
(
1 + u21
</p>
<p>) &part;
&part;u1
</p>
<p>.
</p>
<p>Note that the first two terms in pr(1)v are the same as those in v itself, in
agreement with Box 32.2.12.
</p>
<p>3One can, of course, also write the finite group element directly.</p>
<p/>
</div>
<div class="page"><p/>
<p>1024 32 Lie Groups and Differential Equations
</p>
<p>32.3 The Central Theorems
</p>
<p>We are now in a position to state the first central theorem of the application
of Lie groups to the solution of DEs. This theorem is the exact replica of
Theorem 32.1.6 in the language of prolongations:
</p>
<p>Theorem 32.3.1 Let {�ν(x,u(n)) = 0}lν=1 be a system of DEs defined on
M &sub;X&times;U whose Jacobian matrix
</p>
<p>J�
(
x,u(n)
</p>
<p>)
&equiv;
(
&part;�ν
</p>
<p>&part;xi
,
&part;�ν
</p>
<p>&part;uαJ
</p>
<p>)
</p>
<p>has rank l for all (x,u(n)) &isin; S�. IfG is a local Lie group of transformations
acting on M , and
</p>
<p>pr(n)ξM
∣∣
(x,u(n))
</p>
<p>�ν = 0, ν = 1, . . . , l, whenever �
(
x,u(n)
</p>
<p>)
= 0
</p>
<p>for every infinitesimal generator ξ of G, then G is the symmetry group of
the system.
</p>
<p>Example 32.3.2 Consider the first order (so, n= 1) ordinary DE
</p>
<p>�(x,u,u1)=
(
u&minus; x3 &minus; u2x
</p>
<p>)
u1 + x + x2u+ u3 = 0,
</p>
<p>so that X &sim;=R and U &sim;=R. We first note that
</p>
<p>J� =
(
&part;�
</p>
<p>&part;x
,
&part;�
</p>
<p>&part;u
,
&part;�
</p>
<p>&part;u1
</p>
<p>)
</p>
<p>=
((
&minus;3x2 &minus; u2
</p>
<p>)
u1 + 1 + 2xu, (1 &minus; 2ux)u1 + x2 + 3u2, u&minus; x3 &minus; u2x
</p>
<p>)
,
</p>
<p>which is of rank 1 everywhere. Now let us apply the first prolongation of the
generator of SO(2)&mdash;calculated in Example 32.2.14&mdash;to �. We have
</p>
<p>pr(1)v(�)=&minus;u&part;�
&part;x
</p>
<p>+ x &part;�
&part;u
</p>
<p>+
(
1 + u21
</p>
<p>) &part;�
&part;u1
</p>
<p>=&minus;u
[(
&minus;3x2 &minus; u2
</p>
<p>)
u1 + 1 + 2xu
</p>
<p>]
+ x
</p>
<p>[
(1 &minus; 2ux)u1 + x2 + 3u2
</p>
<p>]
</p>
<p>+
(
1 + u21
</p>
<p>)(
u&minus; x3 &minus; u2x
</p>
<p>)
</p>
<p>= u1
[(
u&minus; x3 &minus; u2x
</p>
<p>)
u1 + x + x2u+ u3
</p>
<p>]
= u1�.
</p>
<p>It follows that pr(1)v(�)= 0 whenever �= 0, and that SO(2) is a symmetry
group of the DE. Thus, rotations will change solutions of the DE into other
solutions. In fact, the reader may verify that in polar coordinates, the DE can
be written in the incredibly simple form
</p>
<p>dr
</p>
<p>dθ
= r3,
</p>
<p>and the symmetry of the DE conveys the fact that adding a constant to θ
does not change the polar form of the DE.</p>
<p/>
</div>
<div class="page"><p/>
<p>32.3 The Central Theorems 1025
</p>
<p>Theorem 32.3.1 reduces the invariance of a system of DEs to a criterion
involving the prolongation of the infinitesimal generators of the symmetry
group. The urgent task in front of us is therefore to construct an explicit
formula for the prolongation of a vector field. In order to gain insight into
this construction, we first look at the simpler case of U &sim;=R and a group G
that transforms only the independent variables. Furthermore, we restrict our-
selves to the first prolongation. An infinitesimal generator of such a group
will be of the form
</p>
<p>v =
p&sum;
</p>
<p>i=1
Xi(x)
</p>
<p>&part;
</p>
<p>&part;xi
,
</p>
<p>which is assumed to act on the space M &sub;X&times;U . The integral curve of this
vector field is exp(tv) which acts on points of M as follows:
</p>
<p>(x̃, ũ)= exp(tv) &middot; (x,u)&equiv;
(
Ψt (x), u
</p>
<p>)
&equiv;
(
Ψ it (x), u
</p>
<p>)
.
</p>
<p>By the construction of the integral curves in general, we have
</p>
<p>dΨ it (x)
</p>
<p>dt
</p>
<p>∣∣∣∣
t=0
</p>
<p>=Xi(x). (32.15)
</p>
<p>Denote the coordinates of the first jet space M(1) by (xi, u,uk), where
uk(j
</p>
<p>1
xf )&equiv; &part;f/&part;xk . By the definition of the action of the prolonged group,
</p>
<p>pr(1) exp(tv) &middot;
(
j1xf
</p>
<p>)
=
(
Ψt (x), u, ũj
</p>
<p>)
, (32.16)
</p>
<p>where u = f (x) and ũj = &part;f̃ /&part;x̃j . Once we find ũj , we can differentiate
Eq. (32.16) with respect to t at t = 0 to obtain the prolonged vector field.
Using Eq. (32.10) and commas to indicate differentiation,4 we obtain
</p>
<p>ũj = f̃,j (x̃)= (f ◦Ψ&minus;t ),j (x̃)=
p&sum;
</p>
<p>i=1
f,i
</p>
<p>(
Ψ&minus;t (x̃)︸ ︷︷ ︸
</p>
<p>=x
</p>
<p>)
Ψ i&minus;t,j (x̃)
</p>
<p>=
p&sum;
</p>
<p>i=1
uiΨ
</p>
<p>i
&minus;t,j (x̃).
</p>
<p>4We have found it exceedingly convenient to use commas to indicate differentiation with
respect to the x&rsquo;s. The alternative, i.e., the use of partials, makes it almost impossible to
find one&rsquo;s way in the maze of derivatives involving xi , x̃j , and t , with x̃j depending on t .
The reader will recall that the index after the comma is to be thought of as a &ldquo;position
holder&rdquo;, and the argument as a substitution. Thus, for example,
</p>
<p>f̃,j (x̃)&equiv;
&part;f̃ (r1, . . . , rp)
</p>
<p>&part;rj
</p>
<p>∣∣∣∣
r=x̃
</p>
<p>&equiv; &part;f̃ (s1, . . . , sp)
&part;sj
</p>
<p>∣∣∣∣
s=x̃
</p>
<p>&equiv; &part;f̃ (&hearts;1, . . . ,&hearts;p)
&part;&hearts;j
</p>
<p>∣∣∣∣
&hearts;=x̃
</p>
<p>.</p>
<p/>
</div>
<div class="page"><p/>
<p>1026 32 Lie Groups and Differential Equations
</p>
<p>Since v does not have any component along &part;/&part;u, its prolongation will not
have such components either. The components Uj along &part;/&part;uj are obtained
by differentiating ũj with respect to t :
</p>
<p>Uj (x,u,uj )=
p&sum;
</p>
<p>i=1
ui
</p>
<p>d
</p>
<p>dt
Ψ i&minus;t,j (x̃)
</p>
<p>∣∣∣∣
t=0
</p>
<p>=
p&sum;
</p>
<p>i=1
ui
</p>
<p>&part;Ψ i&minus;t,j
&part;t
</p>
<p>(x̃)
</p>
<p>∣∣∣∣
t=0
</p>
<p>+
p&sum;
</p>
<p>i=1
ui
</p>
<p>p&sum;
</p>
<p>k=1
</p>
<p>(
Ψ i&minus;t,jk(x̃)
</p>
<p>&part;x̃k
</p>
<p>&part;t
</p>
<p>)∣∣∣∣
t=0
</p>
<p>.
</p>
<p>(32.17)
</p>
<p>The derivative of the first term in the sum can be evaluated as follows:
</p>
<p>&part;Ψ i&minus;t,j
&part;t
</p>
<p>(
x̃(t)
</p>
<p>)∣∣∣∣
t=0
</p>
<p>=&minus;
&part;Ψ is,j
</p>
<p>&part;s
</p>
<p>(
x̃(&minus;s)
</p>
<p>)∣∣∣∣
s=0
</p>
<p>&equiv;&minus;
&part;Ψ i,j
</p>
<p>&part;s
</p>
<p>(
s, x̃(&minus;s)
</p>
<p>)∣∣∣∣
s=0
</p>
<p>=&minus;
&part;Ψ i,j
</p>
<p>&part;s
</p>
<p>(
s, x̃(0)
</p>
<p>)∣∣∣∣
s=0
</p>
<p>=&minus;
&part;Ψ i,j
</p>
<p>&part;s
(s, x)
</p>
<p>∣∣∣∣
s=0
</p>
<p>=&minus;
&part;Ψ is,j
</p>
<p>&part;s
(x)
</p>
<p>∣∣∣∣
s=0
</p>
<p>=&minus;
(
&part;Ψ is
</p>
<p>&part;s
</p>
<p>∣∣∣∣
s=0
</p>
<p>)
</p>
<p>,j
</p>
<p>(x)=&minus;Xi,j (x),
</p>
<p>where we have emphasized the dependence of x̃ on t (or s), treated s as
the first independent variable, and in the second line substituted s = 0 in all
x̃&rsquo;s before differentiation. This is possible because we are taking the partial
derivative with respect to the first variable holding all others constant. The
derivative of the second term in Eq. (32.17) can be calculated similarly:
</p>
<p>Ψ i&minus;t,jk
(
x̃(t)
</p>
<p>)∣∣
t=0 = Ψ
</p>
<p>i
0,jk
</p>
<p>(
x̃(0)
</p>
<p>)
= &part;
</p>
<p>2xi
</p>
<p>&part;xj&part;xk
= 0
</p>
<p>because Ψ i0 = xi . We therefore have
</p>
<p>Uj (x,u,uk)=&minus;
p&sum;
</p>
<p>i=1
</p>
<p>&part;Xi
</p>
<p>&part;xj
ui (32.18)
</p>
<p>and
</p>
<p>pr(1)v =
p&sum;
</p>
<p>i=1
</p>
<p>(
Xi
</p>
<p>&part;
</p>
<p>&part;xi
+Ui
</p>
<p>&part;
</p>
<p>&part;ui
</p>
<p>)
= v &minus;
</p>
<p>p&sum;
</p>
<p>i,k=1
</p>
<p>&part;Xk
</p>
<p>&part;xi
uk
</p>
<p>&part;
</p>
<p>&part;ui
. (32.19)
</p>
<p>It is also instructive to consider the case in which U is still R, but G acts
only on the dependent variable. Then v =U(x,u)&part;u, and
</p>
<p>(x̃, ũ)=
(
x,Φt (x,u)
</p>
<p>)
, with U(x,u)= dΦt (x,u)
</p>
<p>dt
</p>
<p>∣∣∣∣
t=0
</p>
<p>.</p>
<p/>
</div>
<div class="page"><p/>
<p>32.3 The Central Theorems 1027
</p>
<p>The reader may check that in this case, the prolongation of v is given by
</p>
<p>pr(1)v = v +
p&sum;
</p>
<p>j=1
Uj
</p>
<p>(
x,u(1)
</p>
<p>) &part;
&part;uj
</p>
<p>, Uj
(
x,u(1)
</p>
<p>)
= &part;U
</p>
<p>&part;xj
+ uj
</p>
<p>&part;U
</p>
<p>&part;u
.
</p>
<p>(32.20)
The second equation in (32.20) can also be written as
</p>
<p>Uj
(
x,pr(1)f (x)
</p>
<p>)
= &part;U
</p>
<p>&part;xj
+ &part;U
</p>
<p>&part;u
</p>
<p>&part;f
</p>
<p>&part;xj
= &part;
</p>
<p>&part;xj
</p>
<p>[
U
(
x,f (x)
</p>
<p>)]
.
</p>
<p>In other words, Uj (x,u(1)) is obtained from U(x,u) by differentiation with
respect to xj , while treating u as a function of x. This leads us to the defini-
tion of the total derivative.
</p>
<p>Definition 32.3.3 Let S :M(n) &rarr; R be a smooth function of x, u, and all
derivatives of u up to nth order. The total derivative of S with respect to total derivative
xi , denoted by DiS, is a smooth function DiS :M(n+1) &rarr;R defined by
</p>
<p>DiS
(
jn+1x f
</p>
<p>)
= &part;
</p>
<p>&part;xi
</p>
<p>[
S
(
x,pr(n)f (x)
</p>
<p>)]
;
</p>
<p>i.e., DiS is obtained from S by differentiating S with respect to xi , treating
u and all the uαJ &rsquo;s as functions of x.
</p>
<p>The following proposition, whose proof is a straightforward application
of the chain rule, gives the explicit formula for calculating the total deriva-
tive:
</p>
<p>Proposition 32.3.4 The ith total derivative of S :M(n) &rarr;R is of the form
</p>
<p>DiS =
&part;S
</p>
<p>&part;xi
+
</p>
<p>q&sum;
</p>
<p>α=1
</p>
<p>&sum;
</p>
<p>J
</p>
<p>uαJ,i
&part;S
</p>
<p>&part;uαJ
</p>
<p>where, for J = (j1, . . . , jk),
</p>
<p>uαJ,i &equiv;
&part;uαJ
</p>
<p>&part;xi
= &part;
</p>
<p>k+1uα
</p>
<p>&part;xi&part;xj1 . . . &part;xjk
</p>
<p>and the sum over J includes derivatives of all orders from 0 to n.
</p>
<p>An immediate consequence of this proposition is
</p>
<p>Diu
α
J = uαJ,i =
</p>
<p>&part;uαJ
</p>
<p>&part;xi
&forall;J,α,
</p>
<p>Di(ST )= TDiS + SDiT .
(32.21)
</p>
<p>Higher-order total derivatives are defined in analogy with partial derivatives:
If I is a multi-index of the form I = (i1, . . . , ik), then the I th total derivative
is
</p>
<p>DIS =Di1Di2 &middot; &middot; &middot;DikS. (32.22)</p>
<p/>
</div>
<div class="page"><p/>
<p>1028 32 Lie Groups and Differential Equations
</p>
<p>As in the case of partial derivatives, the order of differentiation is immaterial.
We are now ready to state the second central theorem of the application of
</p>
<p>Lie groups to the solution of DEs (for a proof, see [Olve 86, pp. 113&ndash;115]).
</p>
<p>Theorem 32.3.5 Let
</p>
<p>v =
p&sum;
</p>
<p>i=1
Xi(x,u)
</p>
<p>&part;
</p>
<p>&part;xi
+
</p>
<p>q&sum;
</p>
<p>α=1
Uα(x,u)
</p>
<p>&part;
</p>
<p>&part;uα
</p>
<p>be a vector field on an open subset M &sub;X&times;U . The nth prolongation of v,
i.e., pr(n)v &isin;X(M(n)), is
</p>
<p>pr(n)v = v +
q&sum;
</p>
<p>α=1
</p>
<p>&sum;
</p>
<p>J
</p>
<p>UαJ
(
x,u(n)
</p>
<p>) &part;
&part;uαJ
</p>
<p>,
</p>
<p>where for J = (j1, . . . , jk), the inner sum extends over 1 &le; |J | &le; n and the
coefficients UαJ are given by
</p>
<p>UαJ
(
x,u(n)
</p>
<p>)
=DJ
</p>
<p>(
Uα &minus;
</p>
<p>p&sum;
</p>
<p>i=1
Xi
</p>
<p>&part;uα
</p>
<p>&part;xi
</p>
<p>)
+
</p>
<p>p&sum;
</p>
<p>i=1
Xi
</p>
<p>&part;uαJ
</p>
<p>&part;xi
</p>
<p>and the higher-order derivative DJ is as given in Eq. (32.22).
</p>
<p>Example 32.3.6 Let p = 2, q = 1, and consider the case in which G acts
only on the independent variables (x, y). The general vector field for this
situation is
</p>
<p>v = ξ(x, y) &part;
&part;x
</p>
<p>+ η(x, y) &part;
&part;y
</p>
<p>.
</p>
<p>We are interested in the first prolongation of this vector field. Thus, n= 1,
X1 = ξ , X2 = η, and J has only one component, which we denote by j
(also written as x or y). Theorem 32.3.5 gives
</p>
<p>UαJ &equiv;Uj =&minus;Dj
2&sum;
</p>
<p>i=1
Xi
</p>
<p>&part;u
</p>
<p>&part;xi
+
</p>
<p>2&sum;
</p>
<p>i=1
Xi
</p>
<p>&part;uj
</p>
<p>&part;xi
=&minus;
</p>
<p>2&sum;
</p>
<p>i=1
</p>
<p>&part;Xi
</p>
<p>&part;xj
</p>
<p>&part;u
</p>
<p>&part;xi
,
</p>
<p>and using the notation ux = &part;u/&part;x and uy = &part;u/&part;y, we obtain
</p>
<p>pr(1)v = v +Ux
&part;
</p>
<p>&part;ux
+Uy
</p>
<p>&part;
</p>
<p>&part;uy
, (32.23)
</p>
<p>where
</p>
<p>Ux =&minus;ux
&part;ξ
</p>
<p>&part;x
&minus; uy
</p>
<p>&part;η
</p>
<p>&part;x
and Uy =&minus;ux
</p>
<p>&part;ξ
</p>
<p>&part;y
&minus; uy
</p>
<p>&part;η
</p>
<p>&part;y
.
</p>
<p>In particular, if G = SO(2), so that v = &minus;y&part;x + x&part;y , then Ux = &minus;uy and
Uy = ux . It then follows that
</p>
<p>pr(1)v =&minus;y &part;
&part;x
</p>
<p>+ x &part;
&part;y
</p>
<p>&minus; uy
&part;
</p>
<p>&part;ux
+ ux
</p>
<p>&part;
</p>
<p>&part;uy
.</p>
<p/>
</div>
<div class="page"><p/>
<p>32.4 Application to Some Known PDEs 1029
</p>
<p>Example 32.3.7 Let p = 1, q = 1, and G= SO(2). The general vector field
for this situation is
</p>
<p>v =&minus;u &part;
&part;x
</p>
<p>+ x &part;
&part;u
</p>
<p>.
</p>
<p>For the first prolongation of this vector field n = 1, X1 = &minus;u, and J has
only one component, which we denote by x. Theorem 32.3.5 gives
</p>
<p>UαJ &equiv;Ux =Dx
(
x &minus;X1ux
</p>
<p>)
+X1uxx = 1 &minus;
</p>
<p>&part;X1
</p>
<p>&part;x
ux = 1 + u2x .
</p>
<p>It follows that
</p>
<p>pr(1)v =&minus;u &part;
&part;x
</p>
<p>+ x &part;
&part;u
</p>
<p>+
(
1 + u2x
</p>
<p>) &part;
&part;ux
</p>
<p>,
</p>
<p>which is the result obtained in Example 32.2.14.
The second prolongation can be obtained as well. Once again we use
</p>
<p>Theorem 32.3.5 with obvious change of notation:
</p>
<p>Uxx =DxDx
(
x &minus;X1ux
</p>
<p>)
+X1uxxx =Dx
</p>
<p>(
1 + u2x + uuxx
</p>
<p>)
&minus; uuxxx
</p>
<p>= 3uxuxx .
</p>
<p>Then
</p>
<p>pr(2)v =&minus;u &part;
&part;x
</p>
<p>+ x &part;
&part;u
</p>
<p>+
(
1 + u2x
</p>
<p>) &part;
&part;ux
</p>
<p>+ 3uxuxx
&part;
</p>
<p>&part;uxx
.
</p>
<p>Using Theorem 32.3.1, we note that the DE uxx = 0 has SO(2) as a sym-
metry group, because with �(x,u,ux, uxx)&equiv; uxx ,
</p>
<p>pr(2)v(�)=&minus;u&part;�
&part;x
</p>
<p>+ x &part;�
&part;u
</p>
<p>+
(
1 + u2x
</p>
<p>) &part;�
&part;ux
</p>
<p>+ 3uxuxx
&part;�
</p>
<p>&part;uxx
= 3uxuxx,
</p>
<p>which vanishes whenever �(x,u,ux, uxx) vanishes. This is the statement
that rotations take straight lines to straight lines.
</p>
<p>32.4 Application to Some Known PDEs
</p>
<p>We have all the tools at our disposal to compute (in principle) the most gen-
eral symmetry group of almost any system of PDEs. The coefficients UαJ of
the prolonged vector field pr(n)v will be functions of the partial derivatives
of the coefficients Xi and Uα of v with respect to both x and u. The infinites-
imal criterion of invariance as given in Theorem 32.3.1 will involve x, u, and
the derivatives of u with respect to x, as well as Xi and Uα and their par-
tial derivatives with respect to x and u. Using the system of PDEs, we can
obtain some of the derivatives of the u&rsquo;s in terms of the others. Substituting
these relations in the equation of infinitesimal criterion, we get an equation
involving u&rsquo;s and powers of its derivatives that are to be treated as indepen-
dent. We then equate the coefficients of these powers of partial derivatives
of u to zero. This will result in a large number of elementary PDEs for the</p>
<p/>
</div>
<div class="page"><p/>
<p>1030 32 Lie Groups and Differential Equations
</p>
<p>coefficient functions Xi and Uα of v, called the defining equations for the
symmetry group of the given system of PDEs. In most applications, these
defining equations can be solved, and the general solution will determine the
most general infinitesimal symmetry of the system. The symmetry group it-
</p>
<p>defining equations for
</p>
<p>the symmetry group of a
</p>
<p>system of PDEs
</p>
<p>self can then be calculated by exponentiation of the vector fields, i.e., by
finding their integral curves. In the remaining part of this section, we con-
struct the symmetry groups of the heat and the wave equations.
</p>
<p>32.4.1 The Heat Equation
</p>
<p>The one-dimensional heat equation ut = uxx corresponds to p = 2, q = 1,
and n= 2. So it is determined by the vanishing of �(x, t, u(2))= ut &minus; uxx .
The most general infinitesimal generator of symmetry appropriate for this
equation can be written as
</p>
<p>v = ξ(x, t, u) &part;
&part;x
</p>
<p>+ τ(x, t, u) &part;
&part;t
</p>
<p>+ φ(x, t, u) &part;
&part;u
</p>
<p>, (32.24)
</p>
<p>which, as the reader may check (see Problem 32.11), has a second prolon-
gation of the form
</p>
<p>pr(2)v = v + φx &part;
&part;ux
</p>
<p>+ φt &part;
&part;ut
</p>
<p>+ φxx &part;
&part;uxx
</p>
<p>+ φxt &part;
&part;uxt
</p>
<p>+ φt t &part;
&part;ut t
</p>
<p>,
</p>
<p>where, for example,
</p>
<p>φt = φt &minus; ξtux + (φu &minus; τt )ut &minus; ξuuxut &minus; τuu2t
φxx = φxx + (2φxu &minus; ξxx)ux &minus; τxxut + (φuu &minus; 2ξxu)u2x
</p>
<p>&minus; 2τxuuxut &minus; ξuuu3x &minus; τuuu2xut + (φu &minus; 2ξx)uxx
&minus; 2τxuxt &minus; 3ξuuxuxx &minus; τuutuxx &minus; 2τuuxuxt , (32.25)
</p>
<p>and subscripts indicate partial derivatives. Theorem 32.3.1 now gives
</p>
<p>pr(2)v(�)= φt &minus; φxx = 0 whenever ut = uxx (32.26)
</p>
<p>as the infinitesimal criterion. Substituting (32.25) in (32.26), replacing ut
with uxx in the resulting equation, and equating to zero the coefficients of
the monomials in derivatives of u, we obtain a number of equations involv-
ing ξ , τ , and φ. These equations as well as the monomials of which they are
coefficients are given in Table 32.1. Complicated as the defining equations
may look, they are fairly easy to solve. From (d) and (f) we conclude that
τ is a function of t only. Then (c) shows that ξ is independent of u, and
(e) gives 2ξx = τt , or ξ(x, t)= 12τtx + η(t), for some arbitrary function η.
From (h) and the fact that ξ is independent of u we get φuu = 0, or
</p>
<p>φ(x, t, u)= α(x, t)u+ β(x, t)</p>
<p/>
</div>
<div class="page"><p/>
<p>32.4 Application to Some Known PDEs 1031
</p>
<p>Table 32.1 The defining equations of the heat equation and the monomials that give rise
to them
</p>
<p>Monomial Coefficient equation
</p>
<p>u2xx 0 = 0 (a)
u2xuxx τuu = 0 (b)
uxuxx 2ξu + 2τxu = 0 (c)
uxuxt 2τu = 0 (d)
uxx 2ξx + τxx &minus; τt = 0 (e)
uxt 2τx = 0 (f)
u3x ξuu = 0 (g)
u2x 2ξxu &minus; φuu = 0 (h)
ux ξxx &minus; 2φxu &minus; ξt = 0 (i)
1 φt &minus; φxx = 0 (j)
</p>
<p>for some as yet undetermined functions α and β . Since ξ is linear in x,
ξxx = 0, and (i) yields ξt =&minus;2φxu =&minus;2αx , or
</p>
<p>&part;α
</p>
<p>&part;x
=&minus;1
</p>
<p>2
ξt =&minus;
</p>
<p>1
</p>
<p>4
τt tx &minus;
</p>
<p>1
</p>
<p>2
ηt &rArr; α(x, t)=&minus;
</p>
<p>1
</p>
<p>8
τt tx
</p>
<p>2 &minus; 1
2
ηtx + ρ(t).
</p>
<p>Finally, with φt = αtu+ βt and φxx = αxxu+ βxx (recall that when taking
partial derivatives, u is considered independent of x and t), the last defining
equation (j) gives αt = αxx and βt = βxx , i.e., that α and β are to satisfy the
heat equation. Substituting α in the heat equation, we obtain
</p>
<p>&minus;1
8
τt t tx
</p>
<p>2 &minus; 1
2
ηt tx + ρt =&minus;
</p>
<p>1
</p>
<p>4
τt t ,
</p>
<p>which must hold for all x and t . Therefore,
</p>
<p>τt t t = 0, ηt t = 0, ρt =&minus;
1
</p>
<p>4
τt t .
</p>
<p>These equations have the solution
</p>
<p>τ = c1t2 + c2t + c3, ρ =&minus;
1
</p>
<p>2
c1t + c4, η= c5t + c6.
</p>
<p>It follows that α(x, t)=&minus; 14c1x2 &minus; 12c5x &minus; 12c1t + c4 and
</p>
<p>ξ(x, t)= 1
2
(2c1t + c2)x + c5t + c6,
</p>
<p>τ (t)= c1t2 + c2t + c3,
</p>
<p>φ(x, t, u)=
(
&minus;1
</p>
<p>4
c1x
</p>
<p>2 &minus; 1
2
c5x &minus;
</p>
<p>1
</p>
<p>2
c1t + c4
</p>
<p>)
u+ β(x, t).
</p>
<p>Inserting in Eq. (32.24) yields</p>
<p/>
</div>
<div class="page"><p/>
<p>1032 32 Lie Groups and Differential Equations
</p>
<p>v =
[
</p>
<p>1
</p>
<p>2
(2c1t + c2)x + c5t + c6
</p>
<p>]
&part;
</p>
<p>&part;x
+
(
c1t
</p>
<p>2 + c2t + c3
) &part;
&part;t
</p>
<p>+
[(
</p>
<p>&minus;1
4
c1x
</p>
<p>2 &minus; 1
2
c5x &minus;
</p>
<p>1
</p>
<p>2
c1t + c4
</p>
<p>)
u+ β(x, t)
</p>
<p>]
&part;
</p>
<p>&part;u
</p>
<p>= c1
[
xt
</p>
<p>&part;
</p>
<p>&part;x
+ t2 &part;
</p>
<p>&part;t
&minus; 1
</p>
<p>4
</p>
<p>(
2t + x2
</p>
<p>)
u
&part;
</p>
<p>&part;u
</p>
<p>]
+ c2
</p>
<p>(
1
</p>
<p>2
x
&part;
</p>
<p>&part;x
+ t &part;
</p>
<p>&part;t
</p>
<p>)
</p>
<p>+ c3
&part;
</p>
<p>&part;t
+ c4u
</p>
<p>&part;
</p>
<p>&part;u
+ c5
</p>
<p>(
t
&part;
</p>
<p>&part;x
&minus; 1
</p>
<p>2
xu
</p>
<p>&part;
</p>
<p>&part;u
</p>
<p>)
+ c6
</p>
<p>&part;
</p>
<p>&part;x
+ β(x, t) &part;
</p>
<p>&part;u
.
</p>
<p>Thus the Lie algebra of the infinitesimal symmetries of the heat equation is
spanned by the six vector fields
</p>
<p>v1 = &part;x, v2 = &part;t , v3 = u&part;u, v4 = x&part;x + 2t&part;t ,
</p>
<p>v5 = 2t&part;x &minus; xu&part;u, v6 = 4tx&part;x + 4t2&part;t &minus;
(
x2 + 2t
</p>
<p>)
u&part;u
</p>
<p>(32.27)
</p>
<p>and the infinite-dimensional subalgebra
</p>
<p>vβ = β(x, t)&part;u,
</p>
<p>where β is an arbitrary solution of the heat equation.
The one-parameter groups Gi generated by the vi can be found by solv-
</p>
<p>ing the appropriate DEs for the integral curves. We show a sample calcula-
tion and leave the rest of the computation to the reader. Consider v5, whose
integral curve is given by the set of DEs
</p>
<p>dx
</p>
<p>ds
= 2t, dt
</p>
<p>ds
= 0, du
</p>
<p>ds
=&minus;xu.
</p>
<p>The second equation shows that t is not affected by the group. So, t = t0,
where t0 is the initial value of t . The first equation now gives
</p>
<p>dx
</p>
<p>ds
= 2t0 &rArr; x = 2t0s + x0,
</p>
<p>and the last equation yields
</p>
<p>du
</p>
<p>ds
=&minus;(2t0s+x0)u &rArr;
</p>
<p>du
</p>
<p>u
=&minus;(2t0s+x0)ds &rArr; u= u0e&minus;t0s
</p>
<p>2&minus;x0s .
</p>
<p>Changing the transformed coordinates to x̃i and removing the subscript
from the initial coordinates, we can write
</p>
<p>exp(v5s) &middot; (x, t, u)= (x̃, t̃ , ũ)=
(
x + 2ts, t, ue&minus;sx&minus;s2t
</p>
<p>)
.
</p>
<p>Table 32.2 gives the result of the action of exp(vis) to (x, t, u).
The symmetry groups G1 and G2 reflect the invariance of the heat equa-
</p>
<p>tion under space and time translations. G3 and Gβ demonstrate the linearity
of the heat equation: We can multiply solutions by constants and add solu-
tions to get new solutions. The scaling symmetry is contained in G4, which
shows that if you scale time by the square of the scaling of x, you obtain</p>
<p/>
</div>
<div class="page"><p/>
<p>32.4 Application to Some Known PDEs 1033
</p>
<p>Table 32.2 The transformations caused by the symmetry group of the heat equation
</p>
<p>Group element Transformed coordinates (x̃, t̃ , ũ)
</p>
<p>G1 = exp(v1s) (x + s, t, u)
G2 = exp(v2s) (x, t + s, u)
G3 = exp(v3s) (x, t, esu)
G4 = exp(v4s) (esx, e2s t, u)
G5 = exp(v5s) (x + 2ts, t, ue&minus;sx&minus;s
</p>
<p>2t )
</p>
<p>G6 = exp(v6s) ( x1&minus;4st , t1&minus;4st , u
&radic;
</p>
<p>1 &minus; 4st exp[ &minus;sx21&minus;4st ])
Gβ = exp(vβs) (x, t, u+ sβ(x, t))
</p>
<p>a new solution. G5 is a Galilean boost to a moving frame. Finally, G6 is a
transformation that cannot be obtained from any physical principle. Since
each group Gi is a one-parameter group of symmetries, if f is a solution of
the heat equation, so are the functions fi &equiv;Gi &middot; f for all i. These functions
can be obtained from Eq. (32.8). As an illustration, we find f6. First note
that for u= f (x, t), we have
</p>
<p>x̃ = x
1 &minus; 4st , t̃ =
</p>
<p>t
</p>
<p>1 &minus; 4st ,
</p>
<p>ũ&equiv; f̃ (x̃, t̃)= f (x, t)
&radic;
</p>
<p>1 &minus; 4st exp
[ &minus;sx2
</p>
<p>1 &minus; 4st
</p>
<p>]
.
</p>
<p>Next solve the first two equations above for x and t in terms of x̃ and t̃ :
</p>
<p>x = x̃
1 + 4st̃ , t =
</p>
<p>t̃
</p>
<p>1 + 4st̃ .
</p>
<p>Finally, substitute in the last equation to get
</p>
<p>f̃ (x̃, t̃ )= f
(
</p>
<p>x̃
</p>
<p>1 + 4st̃ ,
t̃
</p>
<p>1 + 4st̃
</p>
<p>)&radic;
1
</p>
<p>1 + 4st̃ exp
[ &minus;sx̃2
</p>
<p>1 + 4st̃
</p>
<p>]
,
</p>
<p>or, changing x̃ to x and t̃ to t ,
</p>
<p>f6(x, t)=
1&radic;
</p>
<p>1 + 4st
exp
</p>
<p>[ &minus;sx2
1 + 4st
</p>
<p>]
f
</p>
<p>(
x
</p>
<p>1 + 4st ,
t
</p>
<p>1 + 4st
</p>
<p>)
.
</p>
<p>The other transformed functions can be obtained similarly. We simply list
these functions:
</p>
<p>f1(x, t)= f (x &minus; s, t), f2(x, t)= f (x, t &minus; s),
</p>
<p>f3(x, t)= esf (x, t), f4(x, t)= f
(
e&minus;sx, e&minus;2s t
</p>
<p>)
,
</p>
<p>f5(x, t)= e&minus;sx+s
2tf (x &minus; 2st, t), fβ(x, t)= f (x, t)+ sβ(x, t),
</p>
<p>f6(x, t)=
1&radic;
</p>
<p>1 + 4st
exp
</p>
<p>[ &minus;sx2
1 + 4st
</p>
<p>]
f
</p>
<p>(
x
</p>
<p>1 + 4st ,
t
</p>
<p>1 + 4st
</p>
<p>)
.
</p>
<p>(32.28)</p>
<p/>
</div>
<div class="page"><p/>
<p>1034 32 Lie Groups and Differential Equations
</p>
<p>We can find the fundamental solution to the heat equation very simply as
follows. Let f (x, t) be the trivial constant solution c. Then
</p>
<p>u= f6(x, t)=
c&radic;
</p>
<p>1 + 4st
e&minus;sx
</p>
<p>2/(1+4st)
</p>
<p>is also a solution. Now choose c=&radic;s/π and translate t to t &minus; 1/4s (an al-
lowed operation due to the invariance of the heat equation under time trans-
lation G2). The result is
</p>
<p>u= 1&radic;
4πt
</p>
<p>e&minus;x
2/4t ,
</p>
<p>which is the fundamental solution of the heat equation [see (22.45)].
</p>
<p>32.4.2 TheWave Equation
</p>
<p>As the next example of the application of Lie groups to differential equa-
tions, we consider the wave equation in two dimensions. This equation is
written as
</p>
<p>ut t &minus; uxx &minus; uyy = 0, or ηijuij = 0 and �= ηijuij , (32.29)
</p>
<p>where η= diag(1,&minus;1,&minus;1), and subscripts indicate derivatives with respect
to coordinate functions x1 = t , x2 = x, and x3 = y. With p = 3 and q = 1,
a typical generator of symmetry will be of the form
</p>
<p>v =
3&sum;
</p>
<p>i=1
X(i)
</p>
<p>&part;
</p>
<p>&part;xi
+U &part;
</p>
<p>&part;u
, (32.30)
</p>
<p>where {X(i)}3i=1 and U are functions of t , x, y, and u to be determined. The
second prolongation of such a vector field is
</p>
<p>pr(2)v = v +
3&sum;
</p>
<p>i=1
U (i)
</p>
<p>(
x,u(2)
</p>
<p>) &part;
&part;ui
</p>
<p>+
3&sum;
</p>
<p>i,j=1
U (ij)
</p>
<p>(
x,u(2)
</p>
<p>) &part;
&part;uij
</p>
<p>,
</p>
<p>where by Theorem 32.3.5,
</p>
<p>U (i) =Di
(
U &minus;
</p>
<p>3&sum;
</p>
<p>k=1
X(k)uk
</p>
<p>)
+
</p>
<p>3&sum;
</p>
<p>k=1
X(k)uik =DiU &minus;
</p>
<p>3&sum;
</p>
<p>k=1
</p>
<p>(
DiX
</p>
<p>(k)
)
uk,
</p>
<p>U (ij) =DiDj
(
U &minus;
</p>
<p>3&sum;
</p>
<p>k=1
X(k)uk
</p>
<p>)
+
</p>
<p>3&sum;
</p>
<p>k=1
X(k)uijk
</p>
<p>=DiDjU &minus;
3&sum;
</p>
<p>k=1
</p>
<p>[(
DiDjX
</p>
<p>(k)
)
uk + uik
</p>
<p>(
DjX
</p>
<p>(k)
)
+ ujk
</p>
<p>(
DiX
</p>
<p>(k)
)]
,</p>
<p/>
</div>
<div class="page"><p/>
<p>32.4 Application to Some Known PDEs 1035
</p>
<p>and we have used Eq. (32.21). Using (32.21) further, the reader may show
that
</p>
<p>U (ij) =Uij + uk
(
δjkUiu + δikUju &minus;X(k)ij
</p>
<p>)
</p>
<p>+ uluk
(
δilδjkUuu &minus;X(k)iu δj l &minus;X
</p>
<p>(k)
ju δil
</p>
<p>)
</p>
<p>+ ukl
(
δilδjkUu &minus;X(k)i δj l &minus;X
</p>
<p>(k)
j δil
</p>
<p>)
</p>
<p>&minus; ukulm
(
X(k)u δilδjm +X(m)u δilδjk +X(m)u δikδj l
</p>
<p>)
&minus; uiujukX(k)uu ,
</p>
<p>(32.31)
</p>
<p>where a sum over repeated indices is understood.
Applying pr(2)v to �, we obtain the infinitesimal criterion
</p>
<p>U (tt) =U (xx) +U (yy) or ηijU (ij) = 0.
</p>
<p>Multiplying Eq. (32.31) by ηij and setting the result equal to zero yields
</p>
<p>0 = ηijU (ij)
</p>
<p>= ηijUij + uk
(
2ηikUiu &minus; ηijX(k)ij
</p>
<p>)
+ uluk
</p>
<p>(
ηklUuu &minus; 2X(k)iu ηil
</p>
<p>)
</p>
<p>&minus; 2uklX(k)i ηil &minus; 2ukulmX(m)u ηkl &minus; uiujukX(k)uu ηij , (32.32)
</p>
<p>where we have used the wave equation, ηklukl = 0. Equation (32.32) must
hold for all derivatives of u and powers thereof (treated as independent)
modulo the wave equation. Therefore, the coefficients of such &ldquo;monomials&rdquo;
must vanish. For example, since all the terms involving ukulm are indepen-
dent (even after substituting uxx + uyy for ut t ), we have to conclude that
X
</p>
<p>(m)
u = 0 for all m, i.e., that X(i) are independent of u. Setting the coeffi-
</p>
<p>cient of ukul equal to zero and noting that X
(k)
iu = &part;X
</p>
<p>(k)
u /&part;x
</p>
<p>i = 0 yields
</p>
<p>Uuu = 0 &rArr; U(x,y, t, u)= α(x, y, t)u+ β(x, y, t). (32.33)
</p>
<p>Let us concentrate on the functions X(i). These are related via the term
linear in ukl . After inserting the wave equation in this term, we get
</p>
<p>uklX
(k)
i η
</p>
<p>il = u12
(
X
</p>
<p>(2)
1 &minus;X
</p>
<p>(1)
2
</p>
<p>)
+ u13
</p>
<p>(
X
</p>
<p>(3)
1 &minus;X
</p>
<p>(1)
3
</p>
<p>)
&minus; u23
</p>
<p>(
X
</p>
<p>(3)
2 +X
</p>
<p>(2)
3
</p>
<p>)
</p>
<p>+ u22
(
X
</p>
<p>(1)
1 &minus;X
</p>
<p>(2)
2
</p>
<p>)
+ u33
</p>
<p>(
X
</p>
<p>(1)
1 &minus;X
</p>
<p>(3)
3
</p>
<p>)
.
</p>
<p>The uij in this equation are all independent; so, we can set their coefficients
equal to zero:
</p>
<p>X
(2)
1 =X
</p>
<p>(1)
2 , X
</p>
<p>(3)
1 =X
</p>
<p>(1)
3 , X
</p>
<p>(3)
2 +X
</p>
<p>(2)
3 = 0,
</p>
<p>X
(1)
1 =X
</p>
<p>(2)
2 =X
</p>
<p>(3)
3 .
</p>
<p>(32.34)
</p>
<p>The reader may verify that these relations imply that X(i)jkl = 0 for any i, j ,
k, and l. For example,</p>
<p/>
</div>
<div class="page"><p/>
<p>1036 32 Lie Groups and Differential Equations
</p>
<p>Table 32.3 The generators of the conformal group for R3, part of the symmetry group
of the wave equation in two dimensions
</p>
<p>Infinitesimal generator Transformation
</p>
<p>v1 = &part;t , v2 = &part;x , v3 = &part;y Translation
v4 = x&part;t + t&part;x , v6 =&minus;y&part;x + x&part;y , v7 = y&part;t + t&part;y Rotation/Boost
v5 = t&part;t + x&part;x + y&part;y Dilatation
v8 = (t2 + x2 + y2)&part;t + 2xt&part;x + 2yt&part;y &minus; tu&part;u,
v9 = 2xt&part;t + (t2 + x2 &minus; y2)&part;x + 2xy&part;y &minus; xu&part;u,
v10 = 2yt&part;t + 2xy&part;x + (t2 &minus; x2 + y2)&part;y &minus; yu&part;u
</p>
<p>Inversions
</p>
<p>X
(2)
222 =X
</p>
<p>(1)
122 = X
</p>
<p>(3)
322︸︷︷︸
</p>
<p>&equiv;X(3)223
</p>
<p>=&minus;X(2)323︸︷︷︸
&equiv;X(2)233
</p>
<p>=&minus;X(1)133︸︷︷︸
&equiv;X(1)313
</p>
<p>=&minus;X(3)113︸︷︷︸
&equiv;X(3)311
</p>
<p>=&minus;X(2)211︸︷︷︸
&equiv;X(2)121
</p>
<p>=&minus;X(1)221︸︷︷︸
&equiv;X(1)122
</p>
<p>=&minus;X(2)222. (32.35)
</p>
<p>So, the first link of this chain is equal to its negative. Therefore, all the third
derivatives in the chain of Eq. (32.35) vanish. It follows that all X(i)&rsquo;s are
mixed polynomials of at most degree two. Writing the most general such
polynomials for the three functions X(1), X(2), and X(3) and having them
satisfy Eq. (32.34) yields
</p>
<p>X(1) = a1 + a4x + a7y + a5t + a8
(
x2 + y2 + t2
</p>
<p>)
+ 2a9xt + 2a10yt,
</p>
<p>X(2) = a2 + a5x &minus; a6y + a4t + a9
(
x2 &minus; y2 + t2
</p>
<p>)
+ 2a10xy + 2a8xt,
</p>
<p>X(3) = a3 + a6x + a5y + a7t + a10
(
&minus;x2 + y2 + t2
</p>
<p>)
+ 2a9xy + 2a8yt.
</p>
<p>(32.36)
</p>
<p>Setting the coefficient of uk and ηijUij equal to zero and using
Eq. (32.33) gives
</p>
<p>2αx =X(2)xx +X(2)yy &minus;X(2)tt , 2αy =X(3)xx +X(3)yy &minus;X(2)tt ,
</p>
<p>2αt =X(1)tt &minus;X(1)xx &minus;X(1)yy , βt t &minus; βxx &minus; βyy = 0.
</p>
<p>It follows that β is any solution of the wave equation, and
</p>
<p>α(x, y, t)= a11 &minus; a8t &minus; a9x &minus; a10y.
</p>
<p>By inserting the expressions found for X(i) and U in (32.30) and writing
the result in the form
</p>
<p>&sum;
i aivi , we discover that the generators of the sym-
</p>
<p>metry group consist of the ten vector fields given in Table 32.3 as well as
the vector fields
</p>
<p>u&part;u, vβ = β(x, y, t)&part;u
for β an arbitrary solution of the wave equation. The ten vector fields of Ta-
ble 32.3 comprise the generators of the conformal group in three dimensions
whose generalization to m dimensions will be studied in Sect. 37.2.</p>
<p/>
</div>
<div class="page"><p/>
<p>32.5 Application to ODEs 1037
</p>
<p>32.5 Application to ODEs
</p>
<p>The theory of Lie groups finds one of its most rewarding applications in the
integration of ODEs. Lie&rsquo;s fundamental observation was that if one could
come up with a sufficiently large group of symmetries of a system of ODEs,
then one could integrate the system. In this section we outline the general
technique of solving ODEs once we know their symmetries. The following
proposition will be useful (see [Warn 83, p. 40]):
</p>
<p>Proposition 32.5.1 Let M be an n-dimensional manifold and v &isin; X(M).
Assume that v|P �= 0 for some P &isin;M . Then there exists a local chart, i.e.,
local set of coordinate functions, (w1, . . . ,wn) at P such that v = &part;/&part;w1.
</p>
<p>32.5.1 First-Order ODEs
</p>
<p>The most general first-order ODE can be written as
</p>
<p>du
</p>
<p>dx
= F(x,u) &rArr; �(x,u,ux)&equiv; ux &minus; F(x,u)= 0. (32.37)
</p>
<p>A typical infinitesimal generator of the symmetry group of this equation is5
</p>
<p>v =X&part;x +U&part;u, whose prolongation is
</p>
<p>pr(1) = v +U (x) &part;
&part;ux
</p>
<p>, where
</p>
<p>U (x) &equiv;Ux + (Uu &minus;Xx)ux &minus;Xuu2x, (32.38)
</p>
<p>as the reader may verify. The infinitesimal criterion for the one-parameter
group of transformations G to be a symmetry group of Eq. (32.37) is
pr(1)v(�)= 0, or
</p>
<p>&part;U
</p>
<p>&part;x
+
(
&part;U
</p>
<p>&part;u
&minus; &part;X
</p>
<p>&part;x
</p>
<p>)
F &minus; &part;X
</p>
<p>&part;u
F 2 =X&part;F
</p>
<p>&part;x
+U &part;F
</p>
<p>&part;u
. (32.39)
</p>
<p>Any solution (X,U) of this equation generates a 1-parameter group of trans-
formations. The problem is that a systematic procedure for solving (32.39)
is more difficult than solving the original equation. However, in most cases,
one can guess a symmetry transformation (based on physical, or other,
grounds), and that makes Lie&rsquo;s method worthwhile.
</p>
<p>Suppose we have found a symmetry group G with infinitesimal generator
v that does not vanish at P &isin;M &sub;X&times;U . Based on Proposition 32.5.1, we
can introduce new coordinates
</p>
<p>w = ξ(x,u), y = η(x,u) (32.40)
</p>
<p>5The reader is warned against the unfortunate coincidence of notation: X and U represent
both the components of the infinitesimal generator and the spaces of independent and
dependent variables!</p>
<p/>
</div>
<div class="page"><p/>
<p>1038 32 Lie Groups and Differential Equations
</p>
<p>in a neighborhood of P such that v = &part;/&part;w, whose prolongation is also
&part;/&part;w [see (32.38)]. This transforms the DE of (32.37) into6 �̃(y,w,wy)=
0, and the infinitesimal criterion into
</p>
<p>pr(1)v(�̃)= &part;�̃
&part;w
</p>
<p>= 0.
</p>
<p>It follows that �̃ is independent of w. The transformed DE is there-
fore �̃(y,wy) = 0, whose normal form, obtained by implicitly solving for
dw/dy, is
</p>
<p>dw
</p>
<p>dy
=H(y) &rArr; w =
</p>
<p>&int; y
</p>
<p>a
</p>
<p>H(t) dt &minus;w(a)
</p>
<p>for some function H of y alone and some convenient point y = a. Substi-
tuting this expression of w as a function of y in Eq. (32.40) and eliminating
y between the two equations yields u as a function of x.
</p>
<p>Thus our task is to find the change of variables (32.40). For this, we use
v(w)= 1 and v(y)= 0, and express them in terms of x and u:
</p>
<p>v(w)= v(ξ)=X&part;ξ
&part;x
</p>
<p>+U &part;ξ
&part;u
</p>
<p>= 1,
</p>
<p>v(y)= v(η)=X&part;η
&part;x
</p>
<p>+U &part;η
&part;u
</p>
<p>= 0.
(32.41)
</p>
<p>The second equation says that η is an invariant of the group generated by v.
We therefore use the associated characteristic ODE [see (32.3) and (32.5)]
to find y (or η):
</p>
<p>dx
</p>
<p>X(x,u)
= du
</p>
<p>U(x,u)
. (32.42)
</p>
<p>To find w (or ξ ), we introduce χ(x,u, v) = v &minus; ξ(x,u) and note that an
equivalent relation containing the same information as the first equation in
(32.41) is
</p>
<p>X
&part;χ
</p>
<p>&part;x
+U &part;χ
</p>
<p>&part;u
+ &part;χ
</p>
<p>&part;v
= 0,
</p>
<p>which has the characteristic ODE
</p>
<p>dx
</p>
<p>X(x,u)
= du
</p>
<p>U(x,u)
= dv
</p>
<p>1
, (32.43)
</p>
<p>for which we seek a solution of the form v&minus; ξ(x,u)= c to read off ξ(x,u).
The reader may wonder whether it is sane to go through so much trouble
</p>
<p>only to replace the original single ODE with two ODEs such as (32.42) and
(32.43)! The answer is that in practice, the latter two DEs are much easier
to solve than the original ODE.
</p>
<p>6Here we are choosing w to be the dependent variable. This choice is a freedom that is
always available to us.</p>
<p/>
</div>
<div class="page"><p/>
<p>32.5 Application to ODEs 1039
</p>
<p>Example 32.5.2 The homogeneous FODE du/dx = F(u/x) is invariant
under the scaling transformation (x,u) �&rarr; (sx, su) whose infinitesimal gen-
erator is v = x&part;x + u&part;u. The first prolongation of this vector is the same as
the vector itself (reader, verify!).
</p>
<p>To find the new coordinates w and y, first use Eq. (32.42) with X(x,u)=
x and U(x,u)= u:
</p>
<p>dx
</p>
<p>x
= du
</p>
<p>u
&rArr; u
</p>
<p>x
= c1 &rArr; y =
</p>
<p>u
</p>
<p>x
(see Box 32.1.8).
</p>
<p>Next, we note that (32.43) yields
</p>
<p>dx
</p>
<p>x
= du
</p>
<p>u
= dv &rArr; lnu= v+ ln c2 &rArr; v = ln(u/c2).
</p>
<p>Substituting from the previous equation, we obtain
</p>
<p>v = ln(c1x/c2)= lnx + ln(c1/c2)︸ ︷︷ ︸
&equiv;c
</p>
<p>&rArr; v &minus; lnx = c &rArr; w = lnx.
</p>
<p>The chain rule gives du/dx = (1 + ywy)/wy , so that the DE becomes
</p>
<p>1 + ywy
wy
</p>
<p>= F(y) &rArr; dw
dy
</p>
<p>= 1
F(y)&minus; y ,
</p>
<p>which can be integrated to give w =H(y) or lnx =H(y)=H(u/x), which
defines u as an implicit function of x.
</p>
<p>32.5.2 Higher-Order ODEs
</p>
<p>The same argument used in the first order ODEs can be used for higher-order
ODEs to reduce their orders.
</p>
<p>Proposition 32.5.3 Let
</p>
<p>�
(
x,u(n)
</p>
<p>)
=�(x,u,u1, . . . , un)= 0, uk &equiv;
</p>
<p>dku
</p>
<p>dxk
,
</p>
<p>be an nth order ODE. If this ODE has a one-parameter symmetry group,
then there exist variables w = ξ(x,u) and y = η(x,u) such that
</p>
<p>�
(
x,u(n)
</p>
<p>)
= �̃
</p>
<p>(
y,
</p>
<p>dw
</p>
<p>dy
, . . . ,
</p>
<p>dnw
</p>
<p>dyn
</p>
<p>)
= 0,
</p>
<p>i.e., in terms of w and y, the ODE becomes of (n&minus; 1)st order in wy .
</p>
<p>Proof The proof is exactly the same as in the first-order case. The only
difference is that one has to consider pr(n)v, where v = &part;/&part;w. But Prob-
lem 32.7 shows that pr(n)v = v, as in the first-order case. �</p>
<p/>
</div>
<div class="page"><p/>
<p>1040 32 Lie Groups and Differential Equations
</p>
<p>Example 32.5.4 Consider a second-order DE �(u,ux, uxx) = 0, which
does not depend on x explicitly. The fact that &part;�/&part;x = 0 suggests w = x.
So, we switch the dependent and independent variables and write w = x,
and y = u. Then, using the chain rule, we get
</p>
<p>du
</p>
<p>dx
= 1
</p>
<p>wy
,
</p>
<p>d2u
</p>
<p>dx2
=&minus;wyy
</p>
<p>w3y
.
</p>
<p>Substituting in the original DE, we obtain
</p>
<p>�̃(y,wy,wyy)&equiv;�
(
y,
</p>
<p>1
</p>
<p>wy
,&minus;wyy
</p>
<p>w3y
</p>
<p>)
= 0,
</p>
<p>which is of first order in wy .
</p>
<p>Example 32.5.5 The order of the SOLDE uxx + p(x)ux + q(x)u= 0 can
be reduced by noting that the DE is invariant under the scaling transforma-
tion (x,u) �&rarr; (x, su), whose infinitesimal generator is v = u&part;u. With this
vector field, Eqs. (32.42) and (32.43) give
</p>
<p>dx
</p>
<p>0
= du
</p>
<p>u
,
</p>
<p>dx
</p>
<p>0
= du
</p>
<p>u
= dv.
</p>
<p>For the first equation to make sense, we have to have
</p>
<p>dx = 0 &rArr; x = c1 &rArr; y = x (by Box 32.1.8).
</p>
<p>The second equation in u gives
</p>
<p>v = lnu+ c &rArr; v&minus; lnu= c &rArr; w = lnu &rArr; u= ew.
</p>
<p>Using the chain rule, we obtain
</p>
<p>ux =
dw
</p>
<p>dx
ew = dw
</p>
<p>dy
ew and uxx =
</p>
<p>(
wyy +w2y
</p>
<p>)
ew.
</p>
<p>By inserting this in the original DE and writing z=wy , we obtainRiccati equation
</p>
<p>dz
</p>
<p>dy
=&minus;z2 &minus; p(y)z&minus; q(y),
</p>
<p>which is the well-known first-order Riccati equation.
</p>
<p>32.5.3 DEs with Multiparameter Symmetries
</p>
<p>We have seen that 1-parameter symmetries reduce the order of an ODE by 1.
It is natural to suspect that an r-parameter symmetry will reduce the order
by r . Although this suspicion is correct, it turns out that in general, one
cannot reconstruct the solution of the original equation from those of the</p>
<p/>
</div>
<div class="page"><p/>
<p>32.5 Application to ODEs 1041
</p>
<p>reduced (n&minus; r)th-order equation. (See [Olve 86, pp. 148&ndash;158] for a thor-
ough discussion of this problem.) However, the special, but important, case
of second-order DEs is an exception. The deep reason behind this is the ex-
ceptional structure of 2-dimensional Lie algebras given in Box 29.2.5. We
cannot afford to go into details of the reasoning, but simply quote the fol-
lowing important theorem.
</p>
<p>Theorem 32.5.6 Let �(x,u(n)) = 0 be an nth-order ODE invariant
under a 2-parameter group. Then there is an (n &minus; 2)nd-order ODE
�̃(y,w(n&minus;2)) = 0 with the property that the general solution to � can be
found by integrating the general solution to �̃. In particular, a second-order
ODE having a 2-parameter group of symmetries can be solved by integra-
tion.
</p>
<p>Let us analyze the case of a second-order ODE in some detail. By
Box 29.2.5, the infinitesimal generators v1 and v2 satisfy the Lie bracket
relation
</p>
<p>[v1,v2] = cv1, c= 0 or 1.
</p>
<p>We shall treat the abelian case (c= 0) and leave the nonabelian case for the
reader. To begin with, we use s and t for the transformed variables, and at
the end replace them with y and w.
</p>
<p>By Proposition 32.5.1, we can let v1 = &part;/&part;s. Then v2 can be expressed
as the linear combination
</p>
<p>v2 = α(s, t)
&part;
</p>
<p>&part;s
+ β(s, t) &part;
</p>
<p>&part;t
.
</p>
<p>The commutation relation [v1,v2] = 0 gives
</p>
<p>0 = [&part;s, α&part;s + β&part;t ] =
&part;α
</p>
<p>&part;s
&part;t +
</p>
<p>&part;β
</p>
<p>&part;s
&part;s,
</p>
<p>showing that α and β are independent of s. We want to simplify v2 as much
as possible without changing v1. A transformation that accomplishes this is
S = s + h(t) and T = T (t). Then, by Eq. (28.8) we obtain
</p>
<p>v1 = v1(S)
&part;
</p>
<p>&part;S
+ v1(T )
</p>
<p>&part;
</p>
<p>&part;T
= &part;S
</p>
<p>&part;s
</p>
<p>&part;
</p>
<p>&part;S
+ &part;T
</p>
<p>&part;s
</p>
<p>&part;
</p>
<p>&part;T
= &part;
</p>
<p>&part;S
,
</p>
<p>v2 = v2(S)
&part;
</p>
<p>&part;S
+ v2(T )
</p>
<p>&part;
</p>
<p>&part;T
=
(
α
&part;S
</p>
<p>&part;s
+ β &part;S
</p>
<p>&part;t
</p>
<p>)
&part;
</p>
<p>&part;S
+
(
α
&part;T
</p>
<p>&part;s
+ β &part;T
</p>
<p>&part;t
</p>
<p>)
&part;
</p>
<p>&part;T
</p>
<p>=
(
α + βh&prime;
</p>
<p>) &part;
&part;S
</p>
<p>+ βT &prime; &part;
&part;T
</p>
<p>.
</p>
<p>If β �= 0, we choose T &prime; = 1/β and h&prime; =&minus;α/β to obtain
</p>
<p>v1 =
&part;
</p>
<p>&part;s
, v2 =
</p>
<p>&part;
</p>
<p>&part;t
, (32.44)</p>
<p/>
</div>
<div class="page"><p/>
<p>1042 32 Lie Groups and Differential Equations
</p>
<p>where we have substituted s for S and t for T . If β = 0, we choose α = T ,
and change the notation from S to s and T to t to obtain
</p>
<p>v1 =
&part;
</p>
<p>&part;s
, v2 = t
</p>
<p>&part;
</p>
<p>&part;s
. (32.45)
</p>
<p>The next step is to decide which coordinate is the independent variable,
prolong the vector fields, and apply it to the DE to find the infinitesimal
criterion. For β �= 0, the choice is immaterial. So, let w = s and y = t . Then
the prolongation of v1 and v2 will be the same as the vectors themselves,
and with �(y,w,wy,wyy)&equiv; wyy &minus; F(y,w,wy), the infinitesimal criteria
for invariance will be
</p>
<p>0 = pr(2)v1(�)= v1(�)=
&part;�
</p>
<p>&part;w
=&minus;&part;F
</p>
<p>&part;w
,
</p>
<p>0 = pr(2)v2(�)= v2(�)=
&part;�
</p>
<p>&part;y
=&minus;&part;F
</p>
<p>&part;y
.
</p>
<p>It follows that in the (y,w) system, F will be a function of wy alone and
the DE will be of the form
</p>
<p>wyy = F(wy) &rArr;
dwy
</p>
<p>dy
= F(wy) &rArr;
</p>
<p>&int; wy dz
F (z)︸ ︷︷ ︸
</p>
<p>&equiv;H(wy )
</p>
<p>.
</p>
<p>The last equation can be solved for wy in terms of y and the result integrated.
For β = 0, choose w = t and y = s. Then v1 will not prolongate, and as
</p>
<p>the reader may verify,
</p>
<p>pr(2)v2 = v2&minus;w2y
&part;
</p>
<p>&part;wy
&minus;3wywyy
</p>
<p>&part;
</p>
<p>&part;wyy
=w &part;
</p>
<p>&part;y
&minus;w2y
</p>
<p>&part;
</p>
<p>&part;wy
&minus;3wywyy
</p>
<p>&part;
</p>
<p>&part;wyy
,
</p>
<p>and the infinitesimal criteria for invariance will be
</p>
<p>0 = pr(2)v1(�)= v1(�)=
&part;�
</p>
<p>&part;y
=&minus;&part;F
</p>
<p>&part;y
,
</p>
<p>0 = pr(2)v2(�)=&minus;w
&part;F
</p>
<p>&part;y︸︷︷︸
=0
</p>
<p>+w2y
&part;F
</p>
<p>&part;wy
+ 3wy wyy︸︷︷︸
</p>
<p>=F
</p>
<p>.
</p>
<p>It follows that in the (y,w) system, F will be a function of w and wy and
satisfy the DE
</p>
<p>wy
&part;F
</p>
<p>&part;wy
= 3F,
</p>
<p>whose solution is of the form F(w,wy) = w3y F̃ (w). The original DE now
becomes
</p>
<p>wyy =w3y F̃ (w),</p>
<p/>
</div>
<div class="page"><p/>
<p>32.6 Problems 1043
</p>
<p>for which we use the chain rule wyy =wy&part;wy/&part;w to obtain
</p>
<p>dwy
</p>
<p>dw
=w2y F̃ (w) &rArr; &minus;
</p>
<p>1
</p>
<p>wy
=
&int; w
</p>
<p>F̃ (z) dz
</p>
<p>︸ ︷︷ ︸
&equiv;H(w)
</p>
<p>&rArr; dw
dy
</p>
<p>=&minus; 1
H(w)
</p>
<p>,
</p>
<p>which can be integrated. Had we chosen w = s and y = t , F would have
been a function of y and the DE would have reduced to wyy = F(y),
which could be solved by two consecutive integrations. The nonabelian 2-
dimensional Lie algebra can be analyzed similarly. The reader may verify
that if β = 0, the vector fields can be chosen to be
</p>
<p>v1 =
&part;
</p>
<p>&part;s
, v2 = s
</p>
<p>&part;
</p>
<p>&part;s
, (32.46)
</p>
<p>leading to the ODE wyy = wy F̃ (y), and if β �= 0, the vector fields can be
chosen to be
</p>
<p>v1 =
&part;
</p>
<p>&part;s
, v2 = s
</p>
<p>&part;
</p>
<p>&part;s
+ t &part;
</p>
<p>&part;t
, (32.47)
</p>
<p>leading to the ODE wyy = F̃ (wy)/y. Both of these ODEs are integrable as
in the abelian case.
</p>
<p>32.6 Problems
</p>
<p>32.1 Suppose that {Fi}ni=1 are invariants of the PDE (32.3). Show that any
function f (F1,F2, . . . ,Fn) is also an invariant of the PDE.
</p>
<p>32.2 Find the function f̃ = θ &middot; f when f (x)= ax + b and θ is the angle of
rotation of SO(2).
</p>
<p>32.3 Use the result of Problem 32.2 to find ũ1. Hint: Note that a = u1.
</p>
<p>32.4 Transform the DE of Example 32.3.2 from Cartesian to polar coordi-
nates to obtain dr/dθ = r3.
</p>
<p>32.5 Using the definition of total derivative, verify Eq. (32.21).
</p>
<p>32.6 Show that SO(2) is a symmetry group of the first-order DE
</p>
<p>�(x,u,u1)= (u&minus; x)u1 + x + u= 0
</p>
<p>and write the same DE in polar coordinates.
</p>
<p>32.7 Show that the nth prolongation of the generator of the ith translation,
&part;i , is the same as the original vector.
</p>
<p>32.8 Find the first prolongation of the generator of scaling: x&part;x + u&part;u.</p>
<p/>
</div>
<div class="page"><p/>
<p>1044 32 Lie Groups and Differential Equations
</p>
<p>32.9 Show that when the group acts only on the single dependent variable u,
the prolongation of v =U&part;u is given by
</p>
<p>pr(1)v = v +
p&sum;
</p>
<p>j=1
Uj
</p>
<p>&part;
</p>
<p>&part;uj
, where Uj =
</p>
<p>&part;U
</p>
<p>&part;xj
+ uj
</p>
<p>&part;U
</p>
<p>&part;u
.
</p>
<p>32.10 Show that the nth prolongation of v =X(x,u)&part;x +U(x,u)&part;u for an
ordinary DE of nth order is
</p>
<p>pr(n)v = v +
n&sum;
</p>
<p>k=1
U [k]
</p>
<p>&part;
</p>
<p>&part;u(k)
,
</p>
<p>where
</p>
<p>u(k) &equiv; &part;
ku
</p>
<p>&part;xk
and U [k] =Dkx(U &minus;Xux)+Xu(k+1).
</p>
<p>32.11 Compute the second prolongation of the infinitesimal generators of
the symmetry group of the heat equation.
</p>
<p>32.12 Derive Eqs. (32.31) and (32.32).
</p>
<p>32.13 Using Eq. (32.34) show that X(i)jkl = 0 for any i, j , k, and l.
</p>
<p>32.14 The Korteweg-de Vries equation is ut + uxxx + uux = 0. Us-Korteweg-de Vries
equation ing the technique employed in computing the symmetries of the heat and
</p>
<p>wave equations, show that the infinitesimal generators of symmetries of the
Korteweg-de Vries equation are
</p>
<p>v1 = &part;x, v2 = &part;t , translation
v3 = t&part;x + &part;u, Galilean boost
v4 = x&part;x + 3t&part;t &minus; 2u&part;u. scaling
</p>
<p>32.15 Suppose M(x,u)dx +N(x,u)du= 0 has a 1-parameter symmetry
group with generator v = X&part;x + U&part;u. Show that the function q(x,u) =
1/(XM +UN) is an integrating factor.
</p>
<p>32.16 Show that the second prolongation of v = w&part;y (with y treated as
independent variable) is
</p>
<p>pr(2)v = v &minus;w2y
&part;
</p>
<p>&part;wy
&minus; 3wywyy
</p>
<p>&part;
</p>
<p>&part;wyy
.
</p>
<p>32.17 Go through the case of β = 0 in the solution of the second order ODE
and, choosing w = s and y = t , show that F will be a function of y alone
and the original DE will reduce to wyy = F(y).
</p>
<p>32.18 Show that in the case of the nonabelian 2-dimensional Lie algebra,</p>
<p/>
</div>
<div class="page"><p/>
<p>32.6 Problems 1045
</p>
<p>(a) the vector fields can be chosen to be
</p>
<p>v1 =
&part;
</p>
<p>&part;s
, v2 = s
</p>
<p>&part;
</p>
<p>&part;s
</p>
<p>if β = 0.
(b) Show that these vector fields lead to the ODE wyy =wy F̃ (y).
(c) If β �= 0, show that the vector fields can be chosen to be
</p>
<p>v1 =
&part;
</p>
<p>&part;s
, v2 = s
</p>
<p>&part;
</p>
<p>&part;s
+ t &part;
</p>
<p>&part;t
.
</p>
<p>(d) Finally, show that the latter vector fields lead to the ODE wyy =
F̃ (wy)/y.</p>
<p/>
</div>
<div class="page"><p/>
<p>33Calculus of Variations, Symmetries,and Conservation Laws
</p>
<p>In this chapter we shall start with one of the oldest and most useful branches
of mathematical physics, the calculus of variations. After giving the funda-
mentals and some examples, we shall investigate the consequences of sym-
metries associated with variational problems. The chapter then ends with
Noether&rsquo;s theorem, which connects such symmetries with their associated
conservation laws. All vector spaces of relevance in this chapter will be as-
sumed to be real.
</p>
<p>33.1 The Calculus of Variations
</p>
<p>One of the main themes of calculus is the extremal problem: Given a func-
tion f : R &sup; D &rarr; R, find the points in the domain D of f at which f
attains a maximum or minimum. To locate such points, we find the zeros of
the derivative of f . For multivariable functions, f : Rp &sup;Ω &rarr; R, the no-
tion of gradient generalizes that of the derivative. To find the j th component
of the gradient &nabla;f , we calculate the difference �f between the value of f
at (x1, . . . , xj + ε, . . . , xp) and its value at (x1, . . . , xj , . . . , xp), divide this
difference by ε, and take the limit ε&rarr; 0. This is simply partial differentia-
tion, and the j th component of the gradient is just the j th partial derivative
of f .
</p>
<p>33.1.1 Derivative for Hilbert Spaces
</p>
<p>To make contact with the subject of this chapter, let us reinterpret the notion
of differentiation. The most useful interpretation is geometric. In fact, our
first encounter with the derivative is geometrical: We are introduced to the
concept through lines tangent to curves. In this language, the derivative of
a function f : R &sup; Ω &rarr; R at x0 is a line (or function) ψ :Ω &sup;Ω0 &rarr; R
passing through (x0, f (x0)) whose slope is defined to be the derivative of f
at x0 (see Fig. 33.1):
</p>
<p>ψ(x)= f (x0)+ f &prime;(x0)(x &minus; x0).
</p>
<p>S. Hassani, Mathematical Physics, DOI 10.1007/978-3-319-01195-0_33,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>1047</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-01195-0_33">http://dx.doi.org/10.1007/978-3-319-01195-0_33</a></div>
</div>
<div class="page"><p/>
<p>1048 33 Calculus of Variations, Symmetries, and Conservation Laws
</p>
<p>Fig. 33.1 The derivative at (x0, f (x0)) as a linear function passing through the origin
with a slope f &prime;(x0). The function f is assumed to be defined on a subset Ω of the real line.
Ω0 restricts the x&rsquo;s to be close to x0 to prevent the function from misbehaving (blowing
up), and to make sure that the limit in the definition of derivative makes sense
</p>
<p>The function ψ(x) describes a line, but it is not a linear function (in the
vector-space sense of the word). The requirement of linearity is due to our
desire for generalization of differentiation to Hilbert spaces, on which linear
maps are the most natural objects. Therefore, we consider the line parallel
to ψ(x) that passes through the origin. Call this φ(x). Then
</p>
<p>φ(x)= f &prime;(x0)x, (33.1)
</p>
<p>which is indeed a linear function. We identify φ(x) as the derivative of f
at x0. This identification may appear strange at first but, as we shall see
shortly, is the most convenient and useful. Of course, any identification re-
quires a one-to-one correspondence between objects identified. It is clear
that indeed there is a one-to-one correspondence between derivatives at
points and linear functions with appropriate slopes.
</p>
<p>Equation (33.1) can be used to geometrize the definition of derivative.
First consider
</p>
<p>f &prime;(x0)=
φ(x)&minus; φ(x0)
</p>
<p>x &minus; x0
, and f &prime;(x0)= lim
</p>
<p>x&rarr;x0
f (x)&minus; f (x0)
</p>
<p>x &minus; x0
.
</p>
<p>Next note that, contrary to f which is usually defined only for a subset of
the real line, φ is defined for all real numbers R, and that φ(x &minus; x0) =
φ(x)&minus; φ(x0) due to the linearity of φ. Thus, we have
</p>
<p>lim
x&rarr;x0
</p>
<p>f (x)&minus; f (x0)
x &minus; x0
</p>
<p>= φ(x)&minus; φ(x0)
x &minus; x0
</p>
<p>= lim
x&rarr;x0
</p>
<p>φ(x &minus; x0)
x &minus; x0
</p>
<p>,
</p>
<p>or
</p>
<p>lim
x&rarr;x0
</p>
<p>|f (x)&minus; f (x0)&minus; φ(x &minus; x0)|
|x &minus; x0|
</p>
<p>= 0 (33.2)
</p>
<p>where we have introduced absolute values in anticipation of its analogue&mdash;
norm. Equation (33.2) is readily generalized to any complete normed vector
space (Banach space), and in particular to any Hilbert space:</p>
<p/>
</div>
<div class="page"><p/>
<p>33.1 The Calculus of Variations 1049
</p>
<p>Definition 33.1.1 Let H1 and H2 be Hilbert spaces with norms ‖ &middot; ‖1 and
‖ &middot; ‖2, respectively. Let f :H1 &sup;Ω &rarr;H2 be any map and |x0〉 &isin;Ω . Sup-
pose there is a linear map T &isin;L(H1,H2) with the property that
</p>
<p>lim
‖x&minus;x0‖1&rarr;0
</p>
<p>‖f (|x〉)&minus; f (|x0〉)&minus; T(|x〉 &minus; |x0〉)‖2
‖x &minus; x0‖1
</p>
<p>= 0 for |x〉 &isin;Ω.
</p>
<p>Then, we say that f is differentiable at |x0〉, and we define the derivative
differentiability of a
</p>
<p>function on a Hilbert
</p>
<p>space at a point
</p>
<p>of f at |x0〉 to be Df (x0) &equiv; T. If f is differentiable at each |x〉 &isin; Ω , the
map
</p>
<p>Df :Ω &rarr;L(H1,H2) given by Df
(
|x〉
</p>
<p>)
=Df (x)
</p>
<p>is called the derivative of f .
</p>
<p>The reader may verify that if the derivative exists, it is unique.
</p>
<p>Example 33.1.2 Let H1 =Rn and H2 =Rm and f :Rn &sup;Ω &rarr;Rm. Then
for |x〉 &isin;Ω , Df (x) is a linear map, which can be represented by a matrix in
the standard bases of Rn and Rm. To find this matrix, we need to let Df (x)
act on the j th standard basis of Rn, i.e., we need to evaluate Df (x)|ej 〉.
This suggests taking |y〉 = |x〉+h|ej 〉 (with h&rarr; 0) as the vector appearing
in the definition of derivative at |x〉. Then
</p>
<p>‖f (|y〉)&minus; f (|x〉)&minus;Df (x)(|y〉 &minus; |x〉)‖2
‖y &minus; x‖1
</p>
<p>= ‖f (x
1, . . . , xj + h, . . . , xn)&minus; f (x1, . . . , xj , . . . , xn)&minus; hDf (x)|ej 〉‖2
</p>
<p>|h|
</p>
<p>approaches zero as h&rarr; 0, so that the ith component of the ratio also goes to
zero. But the ith component of Df (x)|ej 〉 is simply aij , the ij th component
of the matrix of Df (x). Therefore,
</p>
<p>lim
h&rarr;0
</p>
<p>|f i(x1, . . . , xj + h, . . . , xn)&minus; f i(x1, . . . , xj , . . . , xn)&minus; haij |
|h| = 0,
</p>
<p>which means that aij = &part;f i/&part;xj .
</p>
<p>The result of the example above can be stated as follows:
</p>
<p>Box 33.1.3 For f :Rn &sup;Ω &rarr;Rm, the matrix of Df (x) in the stan-
dard basis of Rn and Rm is the Jacobian matrix of f .
</p>
<p>The case of H2 =R deserves special attention. Let H be a Hilbert space.
Then Df (x) &isin;L(H,R)=H&lowast; is denoted by df (x) and renamed the differ-
ential of f at |x〉. Furthermore, through the inner product, one can identify differential and gradient
</p>
<p>of f at |x〉df :Ω &rarr;H&lowast; with another map defined as follows:</p>
<p/>
</div>
<div class="page"><p/>
<p>1050 33 Calculus of Variations, Symmetries, and Conservation Laws
</p>
<p>Definition 33.1.4 Let H be a Hilbert space and f :H&sup;Ω &rarr;R. The gra-
dient &nabla;f of f is the map &nabla;f :Ω &rarr;H defined by
</p>
<p>〈&nabla;f (x)|a〉 &equiv;
&lang;
df (x), a
</p>
<p>&rang;
&forall;|x〉 &isin;Ω, |a〉 &isin;H
</p>
<p>where 〈 , 〉 is the pairing 〈 , 〉 :H&lowast; &times;H&rarr;R of H and its dual.
</p>
<p>Note that although f is not an element of H&lowast;, df (x) is, for all points
|x〉 &isin;Ω at which the differential is defined.
</p>
<p>Example 33.1.5 Consider the function f : H &rarr; R given by f (|x〉) =
‖x‖2. Since
</p>
<p>‖y &minus; x‖2 = ‖y‖2 &minus; ‖x‖2 &minus; 2〈x|y &minus; x〉
and since the derivative is unique, the reader may check that df (x)|a〉 =
2〈x|a〉, or &nabla;f (|x〉)= 2|x〉.
</p>
<p>Derivatives could be defined in terms of directions as well:
</p>
<p>Definition 33.1.6 Let H1 and H2 be Hilbert spaces. Let f : H1 &sup; Ω &rarr;
H2 be any map and |x〉 &isin;Ω . We say that f has a derivative in the direction
|a〉 &isin;H1 at |x〉 ifdirectional derivative
</p>
<p>d
</p>
<p>dt
f
(
|x〉 + t |a〉
</p>
<p>)∣∣∣∣
t=0
</p>
<p>exists. We call this element of H2 the directional derivative of f in the
direction |a〉 &isin;H1 at |x〉.
</p>
<p>The reader may verify that if f is differentiable at |x〉 (in the context of
Definition 33.1.1), then the directional derivative of f in any direction |a〉
exists at |x〉 and is given by
</p>
<p>d
</p>
<p>dt
f
(
|x〉 + t |a〉
</p>
<p>)∣∣∣∣
t=0
</p>
<p>=Df (x)|a〉. (33.3)
</p>
<p>33.1.2 Functional Derivative
</p>
<p>We now specialize to the Hilbert space of square-integrable functions
L2(Ω) for some open subset Ω of some Rm. We need to change our no-
tation somewhat. Let us agree to denote the elements of L2(Ω) by f , u, etc.
Real-valued functions on L2(Ω) will be denoted by L, H, etc. The m-tuples
will be denoted by boldface lowercase letters. To summarize,
</p>
<p>x,y &isin;Rm, f,u &isin;L2(Ω) &rArr; f,u :Rm &sup;Ω &rarr;R,
</p>
<p>〈f |u〉 =
ˆ
</p>
<p>Ω
</p>
<p>f (x)u(x) dmx, L,H :L2(Ω)&rarr;R.
</p>
<p>Furthermore, the evaluation of L at u is denoted by L[u].</p>
<p/>
</div>
<div class="page"><p/>
<p>33.1 The Calculus of Variations 1051
</p>
<p>When dealing with the space of functions, the gradient of Definition
33.1.4 is called a functional derivative or variational derivative and de- functional derivative or
</p>
<p>variational derivativenoted by δL/δu. So
</p>
<p>&lang;
δL
δu
</p>
<p>∣∣f
&rang;
&equiv;
ˆ
</p>
<p>Ω
</p>
<p>δL
</p>
<p>δu
(x)f (x) dmx = d
</p>
<p>dt
L[u+ tf ]
</p>
<p>∣∣∣∣
t=0
</p>
<p>, (33.4)
</p>
<p>where we have used Eq. (33.3). Note that by definition, δL/δu is an ele-
ment of the Hilbert space L2(Ω); so, the integral of (33.4) makes sense.
Equation (33.4) is frequently used to compute functional derivatives.
</p>
<p>An immediate consequence of Eq. (33.4) is the following important re-
sult.
</p>
<p>Proposition 33.1.7 Let L : L2(Ω)&rarr; R for some Ω &sub; Rm. If L has an ex-
tremum at u, then
</p>
<p>δL
</p>
<p>δu
= 0.
</p>
<p>Proof If L has an extremum at u, then the RHS of (33.4) vanishes for any
function f , in particular, for any orthonormal basis vector |ei〉. Complete-
ness of a basis now implies that the directional derivative must vanish (see
Proposition 7.1.9). �
</p>
<p>Just as in the case of partial derivatives, where some simple relations such
as derivative of powers and products can be used to differentiate more com-
plicated expressions, there are some primitive formulas involving functional
derivatives that are useful in computing other more complicated expressions.
Consider the evaluation function evaluation function
</p>
<p>Ey :L2(Ω)&rarr;R given by Ey[f ] = f (y).
</p>
<p>Using Eq. (33.4), we can easily compute the functional derivative of Ey:
</p>
<p>ˆ
</p>
<p>Ω
</p>
<p>δEy[u]
δu
</p>
<p>(x)f (x) dmx = d
dt
</p>
<p>Ey[u+ tf ]
∣∣∣∣
t=0
</p>
<p>= d
dt
</p>
<p>{
u(y)+ tf (y)
</p>
<p>}∣∣∣∣
t=0
</p>
<p>= f (y) &rArr; δEy[u]
δu
</p>
<p>(x)= δ(x &minus; y). (33.5)
</p>
<p>It is instructive to compare (33.5) with the similar formula in multivari-
able calculus, where real-valued functions f take a vector x and give a real
number. The analogue of the evaluation function is Ei , which takes a vector
x and gives the real number xi , the ith component of x. Using the definition
of partial derivative, one readily shows that &part;Ei/&part;xj = δij , which is (some-
what less precisely) written as &part;xi/&part;xj = δij . The same sort of imprecision
is used to rewrite Eq. (33.5) as
</p>
<p>δu(y)
δu(x)
</p>
<p>&equiv; δuy
δux
</p>
<p>= δ(x &minus; y), (33.6)</p>
<p/>
</div>
<div class="page"><p/>
<p>1052 33 Calculus of Variations, Symmetries, and Conservation Laws
</p>
<p>where we have turned the arguments into indices to make the analogy with
the discrete case even stronger.
</p>
<p>Another useful formula concerns derivatives of square-integrable func-
tions. Let Ey,i denote the evaluation of the derivative of functions with re-
spect to the ith coordinate:
</p>
<p>Ey,i :L2(Ω)&rarr;R given by Ey,i(f )= &part;if (y).
</p>
<p>Then a similar argument as above will show that
</p>
<p>δEy,i
</p>
<p>δu
(x)=&minus;&part;iδ(x &minus; y), or
</p>
<p>δ&part;iu(y)
δu(x)
</p>
<p>=&minus;&part;iδ(x &minus; y),
</p>
<p>and in general,
</p>
<p>δ&part;i1...iku(y)
δu(x)
</p>
<p>= (&minus;1)k&part;i1...ikδ(x &minus; y). (33.7)
</p>
<p>Equation (33.7) holds only if the function f , the so-called test function,
vanishes on &part;Ω , the boundary of the region of integration. If it does not,
then there will be a &ldquo;surface term&rdquo; that will complicate matters considerably.
Fortunately, in most applications this surface term is required to vanish. So,
let us adhere to the convention that
</p>
<p>Box 33.1.8 All test functions f (x) appearing in the integral of
Eq. (33.4) are assumed to vanish at the boundary of Ω .
</p>
<p>For applications, we need to generalize the concept of functions on
Hilbert spaces. First, it is necessary to consider maps from a Hilbert space to
Rn. For simplicity, we confine ourselves to the Hilbert space L2(Ω). Such
a map H : L2(Ω)&rarr; D &sub; Rn, for some subset D of Rn, can be written in
components
</p>
<p>H= (H1,H2, . . . ,Hn), where Hi :L2(Ω)&rarr;R, i = 1, . . . , n.
</p>
<p>Next, we consider an ordinary multivariable function L :Rn &sup;D&rarr;R, and
use it to construct a new function on L2(Ω), the composite of L and H:
</p>
<p>L ◦H :L2(Ω)&rarr;R, L ◦H[u] = L
(
H1[u], . . . ,Hn[u]
</p>
<p>)
.
</p>
<p>Then the functional derivative of L ◦ H can be obtained using the chain
rule and noting that the derivative of L is the common partial derivative. It
follows that
</p>
<p>δL ◦H[u]
δu
</p>
<p>(x)=
{
</p>
<p>δ
</p>
<p>δu
L
(
H1[u], . . . ,Hn[u]
</p>
<p>)}
(x)=
</p>
<p>n&sum;
</p>
<p>i=1
&part;iL
</p>
<p>δHi
</p>
<p>δu
(x), (33.8)
</p>
<p>where &part;iL is the partial derivative of L with respect to its ith argument.</p>
<p/>
</div>
<div class="page"><p/>
<p>33.1 The Calculus of Variations 1053
</p>
<p>Example 33.1.9 Let L : (a, b)&times; R&times; R&rarr; R, be a function of three vari-
ables the first one of which is defined for the real interval (a, b). Let
Hi :L2(a, b)&rarr;R, i = 1,2,3, be defined by
</p>
<p>H1[u] &equiv; x, H2[u] = Ex[u] = u(x), H3[u] = E&prime;x[u] &equiv; u&prime;(x),
</p>
<p>where Ex is the evaluation function and E&prime;x evaluates the derivative. It fol-
lows that L ◦H[u] = L(x,u(x),u&prime;(x)). Then, noting that H1[u] is indepen-
dent of u, we have
</p>
<p>δL ◦H[u]
δu
</p>
<p>(y)= &part;1L
δH1[u]
δu
</p>
<p>(y)+ &part;2L
δEx[u]
δu
</p>
<p>(y)+ &part;3L
δE&prime;x[u]
δu
</p>
<p>(y)
</p>
<p>= 0 + &part;2Lδ(y &minus; x)&minus; &part;3Lδ&prime;(y &minus; x)= &part;2Lδ(x &minus; y)
+ &part;3Lδ&prime;(x &minus; y).
</p>
<p>This is normally written as
</p>
<p>δL(x,u(x),u&prime;(x))
δu
</p>
<p>(y)= &part;L
&part;u
</p>
<p>δ(x &minus; y)+ &part;L
&part;u&prime;
</p>
<p>δ&prime;(x &minus; y), (33.9)
</p>
<p>which is the unintegrated version of the classical Euler-Lagrange equation
for a single particle, to which we shall return shortly.
</p>
<p>A generalization of the example above turns L into a function on Ω &times;
R&times;Rm with Ω &sub;Rm, so that
</p>
<p>L
(
x1, . . . , xm, u(x), &part;1u(x), . . . , &part;mu(x)
</p>
<p>)
&isin;R, with x &isin;Rm.
</p>
<p>The functions {Hi}2m+1i=1 are defined as
</p>
<p>Hi[u] &equiv; xi for i = 1,2, . . . ,m,
Hi[u] &equiv; Ex[u] = u(x) for i =m+ 1,
Hi[u] &equiv; Ex,i[u] = &part;iu(x) for i =m+ 2, . . . ,2m+ 1,
</p>
<p>and lead to the equation
</p>
<p>δL ◦H[u]
δu
</p>
<p>(y)= &part;m+1Lδ(x &minus; y)+
2m+1&sum;
</p>
<p>i=m+2
&part;iL&part;iδ(x &minus; y), (33.10)
</p>
<p>which is the unintegrated version of the classical Euler-Lagrange equation
for a field in m dimensions.
</p>
<p>33.1.3 Variational Problems
</p>
<p>The fundamental theme of the calculus of variations is to find functions
that extremize an integral and are fixed on the boundary of the integration
region. A prime example is the determination of the equation of the curve</p>
<p/>
</div>
<div class="page"><p/>
<p>1054 33 Calculus of Variations, Symmetries, and Conservation Laws
</p>
<p>of minimum length in the xy-plane passing through two points (a1, b1) and
(a2, b2). Such a curve, written as y = u(x), would minimize the integral
</p>
<p>int[u] &equiv;
ˆ a2
</p>
<p>a1
</p>
<p>&radic;
1 +
</p>
<p>[
u&prime;(x)
</p>
<p>]2
dx, u(a1)= b1, u(a2)= b2. (33.11)
</p>
<p>Note that int takes a function and gives a real number, i.e.&mdash;if we restrict
our functions to square-integrable ones&mdash;int belongs to L2(a1, a2). This is
how contact is established between the calculus of variations and what we
have studied so far in this chapter.
</p>
<p>To be as general as possible, we allow the integral to contain derivatives
up to the nth order. Then, using the notation of the previous chapter, we
consider functions L on M(n) &sub;Ω &times;U (n), where we have replaced X with
Ω , so that M =Rp &sup;Ω &times;U &sub;Rq .
</p>
<p>Definition 33.1.10 By an nth-order variational problem we mean findingnth-order variational
problem; Lagrangian;
</p>
<p>functional
the extremum of the real-valued function L :L2(Ω)&rarr;R given by
</p>
<p>L[u] &equiv;
ˆ
</p>
<p>Ω
</p>
<p>L
(
x,u(n)
</p>
<p>)
dpx, (33.12)
</p>
<p>where Ω is a subset of Rp , L is a real-valued function on Ω &times; U (n), and
p(n) = (p + n)!/(n!p!). In this context the function L is called the La-
grangian of the problem, and L is called a functional.1
</p>
<p>The solution to the variational problem is given by Proposition 33.1.7,
moving the functional derivative inside the integral, and a straightforward
(but tedious!) generalization of Eq. (33.10) to include derivatives of order
higher than one. Due to the presence of the integral, the Dirac delta function
and all its derivatives will be integrated out. Before stating the solution of
the variational problem, let us introduce a convenient operator, using the
total derivative operator introduced in Definition 32.3.3.
</p>
<p>Definition 33.1.11 For 1 &le; α &le; q , the αth Euler operator isEuler operator
</p>
<p>Eα &equiv;
&sum;
</p>
<p>J
</p>
<p>(&minus;D)J
&part;
</p>
<p>&part;uαJ
, (33.13)
</p>
<p>where for J = (j1, . . . , jk),
</p>
<p>(&minus;D)J &equiv; (&minus;1)kDJ = (&minus;Dj1)(&minus;Dj2) &middot; &middot; &middot; (&minus;Djk ),
</p>
<p>and the sum extends over all multi-indices J = (j1, . . . , jk), including
J = 0.
</p>
<p>The negative signs are introduced because of the integration by parts in-
volved in the evaluation of the derivatives of the delta function. Although the
sum in Eq. (33.13) extends over all multi-indices, only a finite number of
</p>
<p>1Do not confuse this functional with the linear functional of Chap. 2.</p>
<p/>
</div>
<div class="page"><p/>
<p>33.1 The Calculus of Variations 1055
</p>
<p>terms in the sum will be nonzero, because any function on which the Euler
operator acts depends on a finite number of derivatives.
</p>
<p>Theorem 33.1.12 If u is an extremal of the variational problem (33.12),
then it must be a solution of the Euler-Lagrange equations Euler-Lagrange
</p>
<p>equations
</p>
<p>Eα(L)&equiv;
&sum;
</p>
<p>J
</p>
<p>(&minus;D)J
&part;L
</p>
<p>&part;uαJ
= 0, α = 1, . . . , q.
</p>
<p>For the special case of p = q = 1, the Euler operator becomes
</p>
<p>E= &part;
&part;u
</p>
<p>+
&infin;&sum;
</p>
<p>j=1
(&minus;Dx)j
</p>
<p>&part;
</p>
<p>&part;uj
= &part;
</p>
<p>&part;u
&minus;Dx
</p>
<p>&part;
</p>
<p>&part;ux
+D2x
</p>
<p>&part;
</p>
<p>&part;uxx
&minus; &middot; &middot; &middot; ,
</p>
<p>where Dx is the total derivative with respect to x, and uj is the j th derivative
of u with respect to x; and the Euler-Lagrange equation for the variational
problem
</p>
<p>L[u] &equiv;
ˆ b
</p>
<p>a
</p>
<p>L
(
x,u(n)
</p>
<p>)
dx
</p>
<p>becomes
</p>
<p>E(L)= &part;L
&part;u
</p>
<p>+
n&sum;
</p>
<p>j=1
(&minus;1)jDjx
</p>
<p>&part;L
</p>
<p>&part;uj
= 0. (33.14)
</p>
<p>Since L carries derivatives up to the n-th order and each Dx carries one
derivative, we conclude that Eq. (33.14) is a 2n-th order ODE.
</p>
<p>Example 33.1.13 The variational problem of Eq. (33.11) has a Lagrangian
</p>
<p>L
(
u,u(n)
</p>
<p>)
= L
</p>
<p>(
u,u(1)
</p>
<p>)
=
&radic;
</p>
<p>1 + u2x,
</p>
<p>which is a function of the first derivative only. So, the Euler-Lagrange equa-
tion takes the form
</p>
<p>0 =&minus;Dx
&part;L
</p>
<p>&part;ux
=&minus;Dx
</p>
<p>(
ux&radic;
</p>
<p>1 + u2x
</p>
<p>)
=&minus; d
</p>
<p>dx
</p>
<p>(
ux&radic;
</p>
<p>1 + u2x
</p>
<p>)
=&minus; uxx
</p>
<p>(1 + u2x)3/2
,
</p>
<p>or uxx = 0, so that u = f (x) = c1x + c2. The solution to the variational
problem is a straight line passing through the two points (a1, b1) and
(a2, b2).
</p>
<p>Historical Notes
</p>
<p>Leonhard Euler (1707&ndash;1783) was Switzerland&rsquo;s foremost scientist and one of the three
greatest mathematicians of modern times (Gauss and Riemann being the other two). He
was perhaps the most prolific author of all time in any field. From 1727 to 1783 his
writings poured out in a seemingly endless flood, constantly adding knowledge to every
known branch of pure and applied mathematics, and also to many that were not known
until he created them. He averaged about 800 printed pages a year throughout his long
life, and yet he almost always had something worthwhile to say. The publication of his
complete works was started in 1911, and the end is not in sight. This edition was planned</p>
<p/>
</div>
<div class="page"><p/>
<p>1056 33 Calculus of Variations, Symmetries, and Conservation Laws
</p>
<p>to include 887 titles in 72 volumes, but since that time extensive new deposits of pre-
viously unknown manuscripts have been unearthed, and it is now estimated that more
than 100 large volumes will be required for completion of the project. Euler evidently
wrote mathematics with the ease and fluency of a skilled speaker discoursing on subjects
</p>
<p>Leonhard Euler
</p>
<p>1707&ndash;1783
</p>
<p>with which he is intimately familiar. His writings are models of relaxed clarity. He never
condensed, and he reveled in the rich abundance of his ideas and the vast scope of his
interests. The French physicist Arago, in speaking of Euler&rsquo;s incomparable mathematical
facility, remarked that &ldquo;He calculated without apparent effort, as men breathe, or as eagles
sustain themselves in the wind.&rdquo; He suffered total blindness during the last 17 years of his
life, but with the aid of his powerful memory and fertile imagination, and with assistants
to write his books and scientific papers from dictation, he actually increased his already
prodigious output of work.
Euler was a native of Basel and a student of Johann Bernoulli at the University, but he
soon outstripped his teacher. He was also a man of broad culture, well versed in the clas-
sical languages and literatures (he knew the Aeneid by heart), many modern languages,
physiology, medicine, botany, geography, and the entire body of physical science as it
was known in his time. His personal life was as placid and uneventful as is possible for a
man with 13 children.
Though he was not himself a teacher, Euler has had a deeper influence on the teach-
ing of mathematics than any other person. This came about chiefly through his three
great treatises: Introductio in Analysin Infinitorum (1748); Institutiones Calculi Differ-
entialis (1755); and lnstitutiones Calculi Integralis (1768&ndash;1794). There is considerable
truth in the old saying that all elementary and advanced calculus textbooks since 1748
are essentially copies of Euler or copies of copies of Euler. These works summed up
and codified the discoveries of his predecessors, and are full of Euler&rsquo;s own ideas. He
extended and perfected plane and solid analytic geometry, introduced the analytic ap-
proach to trigonometry, and was responsible for the modern treatment of the functions
lnx and ex . He created a consistent theory of logarithms of negative and imaginary num-
bers, and discovered that lnx has an infinite number of values. It was through his work
that the symbols e, π , and i =
</p>
<p>&radic;
&minus;1 became common currency for all mathematicians,
</p>
<p>and it was he who linked them together in the astonishing relation eiπ =&minus;1. Among his
other contributions to standard mathematical notation were sinx, cosx, the use of f (x)
for an unspecified function, and the use of
</p>
<p>&sum;
for summation.
</p>
<p>His work in all departments of analysis strongly influenced the further development of this
subject through the next two centuries. He contributed many important ideas to differen-
tial equations, including substantial parts of the theory of second-order linear equations
and the method of solution by power series. He gave the first systematic discussion of the
calculus of variations, which he founded on his basic differential equation for a minimiz-
ing curve. He discovered the integral defining the gamma function and developed many
of its applications and special properties. He also worked with Fourier series, encountered
the Bessel functions in his study of the vibrations of a stretched circular membrane, and
applied Laplace transforms to solve differential equations&mdash;all before Fourier, Bessel,
and Laplace were born.
E.T. Bell, the well-known historian of mathematics, observed that &ldquo;One of the most re-
markable features of Euler&rsquo;s universal genius was its equal strength in both of the main
currents of mathematics, the continuous and the discrete.&rdquo; In the realm of the discrete, he
was one of the originators of number theory and made many far-reaching contributions to
this subject throughout his life. In addition, the origins of topology&mdash;one of the dominant
forces in modern mathematics&mdash;lie in his solution of the K&ouml;nigsberg bridge problem and
his formula V &minus; E + F = 2 connecting the numbers of vertices, edges, and faces of a
simple polyhedron.
The distinction between pure and applied mathematics did not exist in Euler&rsquo;s day, and
for him the entire physical universe was a convenient object whose diverse phenomena
offered scope for his methods of analysis. The foundations of classical mechanics had
been laid down by Newton, but Euler was the principal architect. In his treatise of 1736
he was the first to explicitly introduce the concept of a mass-point, or particle, and he was
also the first to study the acceleration of a particle moving along any curve and to use the
notion of a vector in connection with velocity and acceleration. His continued successes
in mathematical physics were so numerous, and his influence was so pervasive, that most
of his discoveries are not credited to him at all and are taken for granted in the physics</p>
<p/>
</div>
<div class="page"><p/>
<p>33.1 The Calculus of Variations 1057
</p>
<p>community as part of the natural order of things. However, we do have Euler&rsquo;s angles for
the rotation of a rigid body, and the all-important Euler-Lagrange equation of variational
dynamics.
Euler was the Shakespeare of mathematics&mdash;universal, richly detailed, and inexhaustible.
</p>
<p>The variational problem is a problem involving only the first functional
derivative, or the first variation. We know from calculus that the first deriva- The first variation is not
</p>
<p>sufficient for a full
</p>
<p>knowledge of the nature
</p>
<p>of the extremum!
</p>
<p>tive by itself cannot determine the nature of the extremum. To test whether
the point in question is maximum or minimum, we need all the second
derivatives (see Example 6.6.9). One uses these derivatives to expand the
functional in a Taylor series up to the second order. The sign of the second
order contribution determines whether the functional is maximum or mini-
mum at the extremal point. In analogy with Example 6.6.9, we expand L[u]
about f up to the second-order derivative:
</p>
<p>L[u] = L[f ] +
ˆ
</p>
<p>Ω
</p>
<p>dpy
δL
</p>
<p>δu(y)
</p>
<p>∣∣∣∣
u=f
</p>
<p>(
u(y)&minus; f (y)
</p>
<p>)
</p>
<p>+ 1
2
</p>
<p>ˆ
</p>
<p>Ω
</p>
<p>dpy
</p>
<p>ˆ
</p>
<p>Ω
</p>
<p>dpy&prime;
δ2L
</p>
<p>δu(y)δu(y&prime;)
</p>
<p>∣∣∣∣
u=f
</p>
<p>(
u(y)&minus; f (y)
</p>
<p>)(
u
(
y&prime;
)
&minus; f
</p>
<p>(
y&prime;
))
.
</p>
<p>The integrals have replaced the sums of the discrete case of Taylor expansion
of the multivariable functions. Since we are interested in comparing u with
the f that extremizes the functional, the second term vanishes and we get
</p>
<p>L[u] = L[f ] + 1
2
</p>
<p>ˆ
</p>
<p>Ω
</p>
<p>dpy
</p>
<p>ˆ
</p>
<p>Ω
</p>
<p>dpy&prime;
δ2L
</p>
<p>δu(y)δu(y&prime;)
</p>
<p>∣∣∣∣
u=f
</p>
<p>&middot;
[(
u(y)&minus; f (y)
</p>
<p>)(
u
(
y&prime;
)
&minus; f
</p>
<p>(
y&prime;
))]
</p>
<p>. (33.15)
</p>
<p>Historical Notes
</p>
<p>Joseph Louis Lagrange (1736&ndash;1813) was born Giuseppe Luigi Lagrangia but adopted
</p>
<p>Joseph Louis Lagrange
</p>
<p>1736&ndash;1813
</p>
<p>the French version of his name. He was the eldest of eleven children, most of whom did
not reach adulthood. His father destined him for the law&mdash;a profession that one of his
brothers later pursued&mdash;and Lagrange offered no objections. But having begun the study
of physics and geometry, he quickly became aware of his talents and henceforth devoted
himself to the exact sciences. Attracted first by geometry, at the age of seventeen he turned
to analysis, then a rapidly developing field.
In 1755, in a letter to the geometer Giulio da Fagnano, Lagrange speaks of one of Euler&rsquo;s
papers published at Lausanne and Geneva in 1744. The same letter shows that as early
as the end of 1754 Lagrange had found interesting results in this area, which was to
become the calculus of variations (a term coined by Euler in 1766). In the same year,
Lagrange sent Euler a summary, written in Latin, of the purely analytical method that
he used for this type of problem. Euler replied to Lagrange that he was very interested
in the technique. Lagrange&rsquo;s merit was likewise recognized in Turin; and he was named,
by a royal decree, professor at the Royal Artillery School with an annual salary of 250
crowns&mdash;a sum never increased in all the years he remained in his native country. Many
years later, in a letter to d&rsquo;Alembert, Lagrange confirmed that this method of maxima and
minima was the first fruit of his studies&mdash;he was only nineteen when he devised it&mdash;and
that he regarded it as his best work in mathematics. In 1756, in a letter to Euler that has
been lost, Lagrange, applying the calculus of variations to mechanics, generalized Euler&rsquo;s
earlier work on the trajectory described by a material point subject to the influence of
central forces to an arbitrary system of bodies, and derived from it a procedure for solving
all the problems of dynamics.</p>
<p/>
</div>
<div class="page"><p/>
<p>1058 33 Calculus of Variations, Symmetries, and Conservation Laws
</p>
<p>In 1757 some young Turin scientists, among them Lagrange, founded a scientific soci-
ety that was the origin of the Royal Academy of Sciences of Turin. One of the main
goals of this society was the publication of a miscellany in French and Latin, Miscellanea
Taurinensia ou M&eacute;langes de Turin, to which Lagrange contributed fundamentally. These
contributions included works on the calculus of variations, probability, vibrating strings,
and the principle of least action.
To enter a competition for a prize, in 1763 Lagrange sent to the Paris Academy of Sci-
ences a memoir in which he provided a satisfactory explanation of the translational motion
of the moon. In the meantime, the Marquis Caraccioli, ambassador from the kingdom of
Naples to the court of Turin, was transferred by his government to London. He took along
the young Lagrange, who until then seems never to have left the immediate vicinity of
Turin. Lagrange was warmly received in Paris, where he had been preceded by his mem-
oir on lunar libration. He may perhaps have been treated too well in the Paris scientific
community, where austerity was not a leading virtue. Being of a delicate constitution,
Lagrange fell ill and had to interrupt his trip. In the spring of 1765 Lagrange returned to
Turin by way of Geneva.
In the autumn of 1765 d&rsquo;Alembert, who was on excellent terms with Frederick II of Prus-
sia, and familiar with Lagrange&rsquo;s work through M&eacute;langes de Turin, suggested to Lagrange
that he accept the vacant position in Berlin created by Euler&rsquo;s departure for St. Peters-
burg. It seems quite likely that Lagrange would gladly have remained in Turin had the
court of Turin been willing to improve his material and scientific situation. On 26 April,
d&rsquo;Alembert transmitted to Lagrange the very precise and advantageous propositions of
the king of Prussia. Lagrange accepted the proposals of the Prussian king and, not with-
out difficulties, obtained his leave through the intercession of Frederick II with the king
of Sardinia. Eleven months after his arrival in Berlin, Lagrange married his cousin Vitto-
ria Conti who died in 1783 after a long illness. With the death of Frederick II in August
1786 he also lost his strongest support in Berlin. Advised of the situation, the princes of
Italy zealously competed in attracting him to their courts. In the meantime the French
government decided to bring Lagrange to Paris through an advantageous offer. Of all the
candidates, Paris was victorious.
Lagrange left Berlin on 18 May 1787 to become pensionnaire v&eacute;t&eacute;ran of the Paris
Academy of Sciences, of which he had been a foreign associate member since 1772.
Warmly welcomed in Paris, he experienced a certain lassitude and did not immediately
resume his research. Yet he astonished those around him by his extensive knowledge of
metaphysics, history, religion, linguistics, medicine, and botany.
In 1792 Lagrange married the daughter of his colleague at the Academy, the astronomer
Pierre Charles Le Monnier. This was a troubled period, about a year after the flight of
the king and his arrest at Varennes. Nevertheless, on 3 June the royal family signed the
marriage contract &ldquo;as a sign of its agreement to the union.&rdquo; Lagrange had no children
from this second marriage, which, like the first, was a happy one.
When the academy was suppressed in 1793, many noted scientists, including Lavoisier,
Laplace, and Coulomb were purged from its membership; but Lagrange remained as its
chairman. For the next ten years, Lagrange survived the turmoil of the aftermath of the
French Revolution, but by March of 1813, he became seriously ill. He died on the morning
of 11 April 1813, and three days later his body was carried to the Panth&eacute;on. The funeral
oration was given by Laplace in the name of the Senate.
</p>
<p>Example 33.1.14 Let us apply Eq. (33.15)to the extremal function of Ex-A straight line segment
is indeed the shortest
</p>
<p>distance between two
</p>
<p>points.
</p>
<p>ample 33.1.13 to see if the line is truly the shortest distance between two
points. The first functional derivative, obtained using Eq. (33.9), is simply
E(L):
</p>
<p>δL
</p>
<p>δu(y)
= E(L)=&minus; uyy
</p>
<p>(1 + u2y)3/2
.</p>
<p/>
</div>
<div class="page"><p/>
<p>33.1 The Calculus of Variations 1059
</p>
<p>To find the second variational derivative, we use the basic relations (33.6),
(33.7), and the chain rule (33.10):
</p>
<p>δ2L
</p>
<p>δu(y&prime;)δu(y)
</p>
<p>∣∣∣∣
u=f
</p>
<p>=&minus; δ
δu(y&prime;)
</p>
<p>[
uyy
</p>
<p>(1 + u2y)3/2
]∣∣∣∣
</p>
<p>u=f
</p>
<p>=&minus;
{(
</p>
<p>1 + u2y
)&minus;3/2 δuyy
</p>
<p>δu(y&prime;)
&minus; uyy
</p>
<p>3
</p>
<p>2
</p>
<p>(
1 + u2y
</p>
<p>)&minus;5/2
2uy
</p>
<p>δuy
</p>
<p>δu(y&prime;)
</p>
<p>}∣∣∣∣
u=f
</p>
<p>=&minus; δ
&prime;&prime;(y &minus; y&prime;)
</p>
<p>(1 + u2y)3/2
∣∣∣∣
u=f
</p>
<p>=&minus; δ
&prime;&prime;(y &minus; y&prime;)
</p>
<p>(1 + c21)3/2
,
</p>
<p>because uyy = 0 and uy = c1 when u= f . Inserting this in Eq. (33.15), we
obtain
</p>
<p>L[u] = L[f ] &minus; 1
2(1 + c21)3/2
</p>
<p>&times;
ˆ a2
</p>
<p>a1
</p>
<p>dy
</p>
<p>ˆ a2
</p>
<p>a1
</p>
<p>dy&prime;δ&prime;&prime;
(
y &minus; y&prime;
</p>
<p>)(
u(y)&minus; f (y)
</p>
<p>)(
u
(
y&prime;
)
&minus; f
</p>
<p>(
y&prime;
))
</p>
<p>= L[f ] &minus; 1
2(1 + c21)3/2
</p>
<p>ˆ a2
</p>
<p>a1
</p>
<p>dy
(
u(y)&minus; f (y)
</p>
<p>) d2
dy2
</p>
<p>(
u(y)&minus; f (y)
</p>
<p>)
.
</p>
<p>The last integral can be integrated by parts, with the result
</p>
<p>(
u(y)&minus; f (y)
</p>
<p>) d
dy
</p>
<p>(
u(y)&minus; f (y)
</p>
<p>)∣∣∣∣
a2
</p>
<p>a1︸ ︷︷ ︸
=0 because u(ai)= f (ai ), i = 1,2
</p>
<p>&minus;
ˆ a2
</p>
<p>a1
</p>
<p>dy
</p>
<p>[
d
</p>
<p>dy
</p>
<p>(
u(y)&minus; f (y)
</p>
<p>)]2
.
</p>
<p>Therefore,
</p>
<p>L[u] = L[f ] + 1
2(1 + c21)3/2
</p>
<p>ˆ a2
</p>
<p>a1
</p>
<p>dy
</p>
<p>[
d
</p>
<p>dy
</p>
<p>(
u(y)&minus; f (y)
</p>
<p>)]2
</p>
<p>︸ ︷︷ ︸
always positive
</p>
<p>.
</p>
<p>It follows that L[f ]&lt; L[u], i.e., that f indeed gives the shortest distance.
</p>
<p>Example 33.1.15 In the special theory of relativity, the element of the in-
variant &ldquo;length&rdquo;, or proper time, is given by
</p>
<p>&radic;
dt2 &minus; dx2. Thus, the total
</p>
<p>proper time between two events (t1, a1) and (t2, a2) is given by
</p>
<p>L[x] =
ˆ t2
</p>
<p>t1
</p>
<p>&radic;
1 &minus; x2t dt, xt &equiv;
</p>
<p>dx
</p>
<p>dt
.
</p>
<p>The extremum of this variational problem is exactly the same as in the previ-
ous example, the only difference being a sign. In fact, the reader may verify
that
</p>
<p>δL[x]
δx(s)
</p>
<p>= E(L)= xss
(1 &minus; x2s )3/2
</p>
<p>,</p>
<p/>
</div>
<div class="page"><p/>
<p>1060 33 Calculus of Variations, Symmetries, and Conservation Laws
</p>
<p>and therefore, x = f (t) = c1t + c2 extremizes the functional. The secondconnection between
variational problem and
</p>
<p>the twin paradox
</p>
<p>variational derivative can be obtained as before. It is left for the reader to
show that in the case at hand, L[f ] &gt; L[x], i.e., that f gives the longest
proper time. Since the function f (t) = c1t + c2 corresponds to an inertial
(unaccelerated) observer, we conclude that
</p>
<p>Box 33.1.16 Accelerated observers measure a shorter proper time
between any two events than inertial observers.
</p>
<p>This is the content of the famous twin paradox, in which the twin who
goes to a distant galaxy and comes back (therefore being accelerated) will
return younger than her (unaccelerated) twin.
</p>
<p>33.1.4 Divergence and Null Lagrangians
</p>
<p>The variational problem integrates a Lagrangian over a region Ω of Rp .
If the Lagrangian happens to be the divergence of a function that vanishes
at the boundary of Ω , the variational problem becomes trivial, because all
functions will extremize the functional. We now study such Lagrangians in
more detail.
</p>
<p>Definition 33.1.17 Let {Fi :M(n) &rarr;R}pi=1 be functions on M(n), and F =
(F1, . . . ,Fp). The total divergence of F is defined to be2total divergence
</p>
<p>D &middot; F &equiv;
p&sum;
</p>
<p>j=1
DjFj ,
</p>
<p>where Dj is the total derivative with respect to xj .
</p>
<p>Now suppose that the Lagrangian L(x,u(n)) can be written as the diver-
gence of some p-tuple F. Then by the divergence theorem,
</p>
<p>L[u] =
ˆ
</p>
<p>Ω
</p>
<p>L
(
x,u(n)
</p>
<p>)
dpx =
</p>
<p>ˆ
</p>
<p>Ω
</p>
<p>D &middot; Fdpx =
ˆ
</p>
<p>&part;Ω
</p>
<p>F &middot; da
</p>
<p>for any u = f (x) and any domain Ω . It follows that L[f ] depends on the
behavior of f only at the boundary. Since in a typical problem no variation
takes place at the boundary, all functions that satisfy the boundary condi-
tions will be solutions of the variational problem, i.e., they satisfy the Euler-
Lagrange equation. Lagrangians that satisfy the Euler-Lagrange equation for
all u and x are called null Lagrangians. It turns out that null Lagrangiansnull Lagrangians
are the only such solutions of the Euler-Lagrange equation (for a proof, see
[Olve 86, pp. 252&ndash;253]).
</p>
<p>2The reader need not be concerned about lack of consistency in the location of indices
(upper vs. lower), because we are dealing with indexed objects, such as Fi , which are not
tensors!</p>
<p/>
</div>
<div class="page"><p/>
<p>33.1 The Calculus of Variations 1061
</p>
<p>Theorem 33.1.18 A function L(x,u(n)) satisfies E(L)&equiv; 0 for all x and u
if and only if L=D &middot;F for some p-tuple of functions F = (F1, . . . ,Fp) of x,
u, and the derivatives of u.
</p>
<p>In preparation for the investigation of symmetries of the variational prob-
lems, we look into the effect of a change of variables on the variational
problem and the Euler operator. This is important, because the variational
problem should be independent of the variables chosen. Let
</p>
<p>x̃ = Ψ (x,u), ũ=Φ(x,u) (33.16)
</p>
<p>be any change of variables. Then by prolongation, we also have ũ(n) =
Φ(n)(x,u(n)) for the derivatives. Substituting u = f (x) and all its prolon-
gations in terms of the new variables, the functional
</p>
<p>L[f ] =
ˆ
</p>
<p>Ω
</p>
<p>L
(
x,pr(n)f (x)
</p>
<p>)
dpx
</p>
<p>will be transformed into
</p>
<p>L̃[f̃ ] =
ˆ
</p>
<p>Ω̃
</p>
<p>L̃
(
x̃,pr(n)f̃ (x̃)
</p>
<p>)
dpx̃,
</p>
<p>where the transformed domain, defined by
</p>
<p>Ω̃ =
{
x̃ = Ψ
</p>
<p>(
x,f (x)
</p>
<p>)
| x &isin;Ω
</p>
<p>}
,
</p>
<p>will depend not only on the original domain Ω , but also on the function f .
The new Lagrangian is then related to the old one by the change of variables
formula for multiple integrals:
</p>
<p>L
(
x,pr(n)f (x)
</p>
<p>)
= L̃
</p>
<p>(
x̃,pr(n)f̃ (x̃)
</p>
<p>)
det J
</p>
<p>(
x,pr(1)f (x)
</p>
<p>)
, (33.17)
</p>
<p>where J is the Jacobian matrix of the change of variables induced by the
function f .
</p>
<p>Starting with Eqs. (33.16) and (33.17), one can obtain the transformation
formula for the Euler operator stated below. The details can be found in
[Olve 86, pp. 254&ndash;255].
</p>
<p>Theorem 33.1.19 Let L(x,u(n)) and L̃(x̃, ũ(n)) be two Lagrangians re-
lated by the change of variable formulas (33.16) and (33.17). Then
</p>
<p>Eα(L)=
q&sum;
</p>
<p>β=1
Fαβ
</p>
<p>(
x,u(1)
</p>
<p>)
Ẽβ(L̃), α = 1, . . . , q
</p>
<p>where Ẽβ is the Euler operator associated with the new variables, and
</p>
<p>Fαβ &equiv; det
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>D1Ψ
1 . . . DpΨ
</p>
<p>1 &part;Ψ 1/&part;uα
</p>
<p>...
...
</p>
<p>...
</p>
<p>D1Ψ
p . . . DpΨ
</p>
<p>p &part;Ψ p/&part;uα
</p>
<p>D1Φ
β . . . DpΦ
</p>
<p>β &part;Φβ/&part;uα
</p>
<p>⎞
⎟⎟⎟⎠ .</p>
<p/>
</div>
<div class="page"><p/>
<p>1062 33 Calculus of Variations, Symmetries, and Conservation Laws
</p>
<p>33.2 Symmetry Groups of Variational Problems
</p>
<p>In the theory of fields, as well as in mechanics, condensed matter theory, and
statistical mechanics, the starting point is usually a Lagrangian. The varia-
tional problem of this Lagrangian gives the classical equations of motion,
and its symmetries lead to the important conservation laws.
</p>
<p>Definition 33.2.1 A local group of transformations G acting on M &sub;Ω0 &times;
U is a variational symmetry group of the functionalvariational symmetry
</p>
<p>group
</p>
<p>L[u] =
ˆ
</p>
<p>Ω0
</p>
<p>L
(
x,u(n)
</p>
<p>)
dpx (33.18)
</p>
<p>if whenever (the closure of) Ω lies in Ω0, f is a function over Ω whose
graph is in M , and g &isin;G is such that f̃ = g &middot; f is a single-valued function
defined over Ω̃ , then
</p>
<p>ˆ
</p>
<p>Ω̃
</p>
<p>L
(
x̃,pr(n)f̃ (x̃)
</p>
<p>)
dpx̃ =
</p>
<p>ˆ
</p>
<p>Ω
</p>
<p>L
(
x,pr(n)f (x)
</p>
<p>)
dpx. (33.19)
</p>
<p>In the physics community, the symmetry group of the variational problem&ldquo;Symmetry of the
Lagrangian&rdquo; is really the
</p>
<p>symmetry group of the
</p>
<p>variational problem!
</p>
<p>is (somewhat erroneously) called the symmetry of the Lagrangian. Note
that if we had used L̃ in the LHS of Eq. (33.19), we would have obtained
an identity valid for all Lagrangians because of Eq. (33.17) and the for-
mula for the change in the volume element of integration. Only symmetric
Lagrangians will satisfy Eq. (33.19).
</p>
<p>As we have experienced so far, the action of a group can be very compli-
cated and very nonlinear. On the other hand, the infinitesimal action simpli-
fies the problem considerably. Fortunately, we have (see [Olve 86, pp. 257&ndash;
258] for a proof).
</p>
<p>Theorem 33.2.2 A local group of transformations G acting on M &sub;Ω0 &times;
U is a variational symmetry group of the functional (33.18) if and only if
</p>
<p>pr(n)v(L)+LD &middot; X = 0 (33.20)
</p>
<p>for all (x,u(n)) &isin;M(n) and every infinitesimal generator
</p>
<p>v =
p&sum;
</p>
<p>i=1
Xi(x,u)
</p>
<p>&part;
</p>
<p>&part;xi
+
</p>
<p>q&sum;
</p>
<p>α=1
Uα(x,u)
</p>
<p>&part;
</p>
<p>&part;uα
</p>
<p>of G, where X &equiv; (X1, . . . ,Xp).
</p>
<p>Example 33.2.3 Consider the case of p = 1 = q , and assume that the La-
grangian is independent of x but depends on u &isin;L2(a, b) and its first deriva-
tive. Then the variational problem takes the form
</p>
<p>L[u] =
ˆ b
</p>
<p>a
</p>
<p>L
(
u(1)
</p>
<p>)
dx &equiv;
</p>
<p>ˆ b
</p>
<p>a
</p>
<p>L(u,ux) dx.</p>
<p/>
</div>
<div class="page"><p/>
<p>33.2 Symmetry Groups of Variational Problems 1063
</p>
<p>Since derivatives are independent of translations, we expect translations to
be part of the symmetry group of this variational problem. Let us verify this.
The infinitesimal generator of translation is &part;x , which is its own prolonga-
tion. Therefore, with X = 1 and U = 0, it follows that
</p>
<p>pr(1)v(L)+LD &middot; X = &part;xL+LDxX = 0 + 0 = 0.
</p>
<p>Example 33.2.4 As a less trivial case, consider the proper time of Example
33.1.15. Lorentz transformations generated by3 v = u&part;x + x&part;u are symme-
tries of that variational problem. We can verify this by noting that the first
prolongation of v is, as the reader is urged to verify,
</p>
<p>pr(1)v = v +
(
1 &minus; u2x
</p>
<p>) &part;
&part;ux
</p>
<p>.
</p>
<p>Therefore,
</p>
<p>pr(1)v(L)= 0 + 0 +
((
</p>
<p>1 &minus; u2x
))1
</p>
<p>2
(&minus;2ux)
</p>
<p>1&radic;
1 &minus; u2x
</p>
<p>=&minus;ux
&radic;
</p>
<p>1 &minus; u2x .
</p>
<p>On the other hand, since X = u and U = x,
</p>
<p>LDx(X)=
&radic;
</p>
<p>1 &minus; u2xDx(u)=
&radic;
</p>
<p>1 &minus; u2xux,
</p>
<p>so that Eq. (33.20) is satisfied.
</p>
<p>In the last chapter, we studied the symmetries of the DEs in some detail.
This chapter introduces us to a particular DE that arises from a variational
problem, namely, the Euler-Lagrange equation. The natural question to ask
now is: How does the variational symmetry manifest itself in the Euler-
Lagrange equation? Barring some technical difficulties, we note that for any
change of variables, if u = f (x) is an extremal of the variational problem
L[u], then ũ= f̃ (x̃) is an extremal of the variational problem L̃[ũ]. In par-
ticular, if the change is achieved by the action of the variational symmetry
group, (x̃, ũ)= g &middot; (x,u) for some g &isin;G, then L̃[ũ] = L[ũ], and g &middot; f is also
an extremal of L. We thus have
</p>
<p>Theorem 33.2.5 If G is the variational symmetry group of a functional,
then G is also the symmetry group of the associated Euler-Lagrange equa-
tions.
</p>
<p>The converse is not true! There are symmetry groups of the Euler-
Lagrange equations that are not the symmetry group of the variational prob-
lem. Problem 33.8 illustrates this for p = 3, q = 1, and the functional
</p>
<p>L[u] = 1
2
</p>
<p>˚ (
u2t &minus; u2x &minus; u2y
</p>
<p>)
dx dy dt, (33.21)
</p>
<p>3In order to avoid confusion in applying formula (33.20), we use x (instead of t ) as the
independent variable and u (instead of x) as the dependent variable.</p>
<p/>
</div>
<div class="page"><p/>
<p>1064 33 Calculus of Variations, Symmetries, and Conservation Laws
</p>
<p>whose Euler-Lagrange equation is the wave equation. The reader is asked
to show that while the rotations and Lorentz boosts of Table 32.3 are vari-
ational symmetries, the dilatations and inversions (special conformal trans-
formations) are not.
</p>
<p>Symmetries of the
</p>
<p>Euler-Lagrange
</p>
<p>equations are not
</p>
<p>necessarily the
</p>
<p>symmetries of the
</p>
<p>corresponding
</p>
<p>variational problem!
</p>
<p>We now treat the case of p = 1 = q , whose Euler-Lagrange equation is
an ODE. Recall that the knowledge of a symmetry group of an ODE led to
a reduction in the order of that ODE. Let us see what happens in the present
case. Suppose v =X&part;x+U&part;u is the infinitesimal generator of a 1-parameter
group of variational symmetries of L. By an appropriate coordinate transfor-
mation from (x,u) to (y,w), as in Sect. 32.5, v will reduce to &part;/&part;w, whose
prolongation is also &part;/&part;w. In terms of the new coordinates, Eq. (33.20) will
reduce to &part;L̃/&part;w = 0; i.e., the new Lagrangian is independent of w, and the
Euler-Lagrange equation (33.14) becomes
</p>
<p>0 = E(L)=
n&sum;
</p>
<p>j=1
(&minus;1)jDjy
</p>
<p>&part;L̃
</p>
<p>&part;wj
= (&minus;Dy)
</p>
<p>[
n&minus;1&sum;
</p>
<p>j=0
(&minus;Dy)j
</p>
<p>&part;L̃
</p>
<p>&part;wj+1
</p>
<p>]
. (33.22)
</p>
<p>Therefore, the expression in the brackets is some constant λ (because Dy is a
total derivative). Furthermore, if we introduce v =wy , the expression in the
brackets becomes the Euler-Lagrange equation of the variational problem
</p>
<p>L̂[v] =
ˆ
</p>
<p>L̂
(
y, v(n&minus;1)
</p>
<p>)
dy, where L̂
</p>
<p>(
y, v(n&minus;1)
</p>
<p>)
= L̃(y,wy, . . . ,wn),
</p>
<p>and every solution w = f (y) of the original (2n)th-order Euler-Lagrange
equation corresponds to the (2n&minus; 2)nd-order equation
</p>
<p>Ê(L̂)= &part;L̂
&part;y
</p>
<p>+
n&minus;1&sum;
</p>
<p>j=1
(&minus;Dy)j
</p>
<p>&part;L̂
</p>
<p>&part;vj
= λ. (33.23)
</p>
<p>Moreover, this equation can be written as the Euler-Lagrange equation for
</p>
<p>L̂λ[v] =
ˆ [
</p>
<p>L̂
(
y, v(n&minus;1)
</p>
<p>)
&minus; λv
</p>
<p>]
dy,
</p>
<p>and λ can be thought of as a Lagrange multiplier, so that in analogy withLagrange multiplier
the multivariable extremal problem,4 the extremization of L̂λ[v] becomes
equivalent to that of L̂[v] subject to the constraint
</p>
<p>&acute;
</p>
<p>v dy = 0. We summarize
the foregoing discussion in the following theorem.
</p>
<p>Theorem 33.2.6 Let p = 1 = q , and L[u] an nth-order variational problem
with a 1-parameter group of variational symmetries G. Then there exists a
one-parameter family of variational problems L̂λ[v] of order n &minus; 1 such
that every solution of the Euler-Lagrange equation for L[u] can be found by
integrating the solutions to the Euler-Lagrange equation for L̂λ[v].
</p>
<p>4See [Math 70, pp. 331&ndash;341] for a discussion of Lagrange multipliers and their use
in variational techniques, especially those used in approximating solutions of the
Schr&ouml;dinger equation.</p>
<p/>
</div>
<div class="page"><p/>
<p>33.3 Conservation Laws and Noether&rsquo;s Theorem 1065
</p>
<p>Thus, we have the following important result:
</p>
<p>Box 33.2.7 A 1-parameter variational symmetry of a functional re-
duces the order of the corresponding Euler-Lagrange equation by
two.
</p>
<p>This conclusion is to be contrasted with the symmetry of ODEs, where
each 1-parameter group of symmetries reduces the order of the ODE by 1.
It follows from Box 33.2.7 that the ODEs of order 2n derived from a varia-
tional problem&mdash;the Euler-Lagrange equation&mdash;are special.
</p>
<p>Example 33.2.8 A first-order variational problem with a 1-parameter
group of symmetries can be integrated out. By transforming to a new coor-
dinate system, we can always assume that the Lagrangian is independent of
the dependent variable (see Proposition 32.5.1). The Euler-Lagrange equa-
tion in this case becomes
</p>
<p>0 = E(L)= &part;L
&part;u︸︷︷︸
=0
</p>
<p>&minus;Dx
&part;L
</p>
<p>&part;ux
&rArr; &part;L
</p>
<p>&part;ux
(x,ux)= λ.
</p>
<p>Solving this implicit relation, we get ux = F(x,λ), which can be integrated
to give u as a function of x (and λ).
</p>
<p>The procedure can be generalized to r-parameter symmetry groups, but
the order cannot be expected to be reduced by 2 unless the group is abelian.
We shall not pursue this matter here, but ask the reader to refer to Prob-
lem 33.9.
</p>
<p>33.3 Conservation Laws and Noether&rsquo;s Theorem
</p>
<p>A conserved physical quantity is generally defined as a quantity whose flux
through any arbitrary closed surface is equal to (the negative of) the rate of
depletion of the quantity in the volume enclosed. This statement, through the
use of the divergence theorem, translates into a relation connecting the time
rate of change of the density and the divergence of the current corresponding
to the physical quantity. Treating time and space coordinates as independent
variables and extending to p independent variables, we have the following:
</p>
<p>Definition 33.3.1 A conservation law for a system of differential equa- current density and
conservation lawtions �(x,u(n))= 0 is a divergence expression D &middot; J = 0 valid for all solu-
</p>
<p>tions u= f (x) of the system. Here,
</p>
<p>J &equiv;
(
J1
(
x,u(n)
</p>
<p>)
, J2
</p>
<p>(
x,u(n)
</p>
<p>)
, . . . , Jp
</p>
<p>(
x,u(n)
</p>
<p>))
</p>
<p>is called current density.</p>
<p/>
</div>
<div class="page"><p/>
<p>1066 33 Calculus of Variations, Symmetries, and Conservation Laws
</p>
<p>For p = 1 = q , i.e., for a system of ODEs, a conservation law takes the
form DxJ (x,u(n)) = 0 for all solutions u = f (x) of the system. This re-
quires J (x,u(n)) to be a constant, i.e., that J (x,u(n)) be a constant of the
motion, or, as it is sometimes called, the first integral of the system.
</p>
<p>constant of the motion,
</p>
<p>or first integral of a
</p>
<p>system of ODEs
In order to understand conservation laws, we need to get a handle on
</p>
<p>those conservation laws that are trivially satisfied.
</p>
<p>Definition 33.3.2 If the current density J itself vanishes for all solutionstrivial conservation law
of the first kind u = f (x) of the system �(x,u(n)) = 0, then D &middot; J = 0 is called a trivial
</p>
<p>conservation law of the first kind.
</p>
<p>To eliminate this kind of triviality, one solves the system and its prolon-
gations �(k)(x,u(n)) = 0 for some of the variables uαJ in terms of the re-
maining variables and substitutes the latter whenever they occur. For exam-
ple, one can differentiate the evolution equation ut = F(x,u(n))&mdash;in which
u(n) have derivatives with respect to x only&mdash;with respect to t and x suffi-
cient number of times (this is what is meant by &ldquo;prolongation&rdquo; of the system
of equations) and solve for all derivatives of u involving time. Then, in the
conservation law, substitute for any such derivatives to obtain a conservation
law involving only x derivatives of u.
</p>
<p>Example 33.3.3 The current density J1 = ( 12u2t + 12u2x,&minus;utux) is easily
seen to be conserved for the system of first-order DEs
</p>
<p>ut = vx, ux = vt .
</p>
<p>By eliminating all the time derivatives in J1, we obtain J2 = ( 12u2x +
1
2v
</p>
<p>2
x,&minus;uxvx), which is also conserved. However, the difference between
</p>
<p>these two currents,
</p>
<p>J = J1 &minus; J2 =
(
</p>
<p>1
</p>
<p>2
u2t &minus;
</p>
<p>1
</p>
<p>2
v2x, uxvx &minus; utux
</p>
<p>)
,
</p>
<p>satisfies a trivial conservation law of the first kind, because the components
of J vanish on the solutions of the system.
</p>
<p>Definition 33.3.4 If the current density J satisfies D &middot;J = 0 for all functionstrivial conservation law
of the second kind; null
</p>
<p>divergence
</p>
<p>u= f (x), even if they are not solutions of the system of DEs, the divergence
identity is called a trivial conservation law of the second kind. In this case
J is called a null divergence.
</p>
<p>If we treat Ji as the components of a (p&minus; 1)-form ωωω, so that the exterior
derivative dωωω is the divergence of J (times a volume element), then the
triviality of the conservation law for J is equivalent to the fact that ωωω is
closed. By the converse of the Poincar&eacute; lemma, there must be a (p&minus;2)-form
ηηη such that ωωω = dηηη. In the context of this chapter, we have the following
theorem.</p>
<p/>
</div>
<div class="page"><p/>
<p>33.3 Conservation Laws and Noether&rsquo;s Theorem 1067
</p>
<p>Theorem 33.3.5 Suppose J = (J1(x,u(n)), . . . , Jp(x,u(n))) is a p-tuple of
functions on X&times;U (n). Then J is a null divergence if and only if there exist
smooth functions Akj (x,u(n)), j, k = 1, . . . , p, antisymmetric in their in-
dices, such that
</p>
<p>Jk =
p&sum;
</p>
<p>j=1
DjAkj , j = 1, . . . , p. (33.24)
</p>
<p>Definition 33.3.6 We say that D &middot; J = 0 is a trivial conservation law trivial conservation law
and the equivalence of
</p>
<p>two conservation laws
</p>
<p>if there exist antisymmetric smooth functions Akj (x,u(n)) satisfying
Eq. (33.24) for all solutions of the system of DEs �(x,u(n)) = 0. Two
conservation laws are equivalent if they differ by a trivial conservation law.
</p>
<p>We shall not distinguish between conservation laws that are equivalent.
It turns out that to within this equivalence, some systems of DEs �ν have
current densities J such that
</p>
<p>D &middot; J =
l&sum;
</p>
<p>ν=1
Qν�ν for some l-tuple Q = (Q1, . . . ,Ql), (33.25)
</p>
<p>where {Qν} are smooth functions of x, u, and all derivatives of u.
</p>
<p>Definition 33.3.7 Equation (33.25) is called the characteristic form of the characteristic form of a
conservation lawconservation law for the current density J, and the l-tuple Q, the character-
</p>
<p>istic of the conservation law.
</p>
<p>We are now in a position to prove the celebrated Noether&rsquo;s theorem.
However, we first need a lemma.
</p>
<p>Lemma 33.3.8 Let v = &sum;pi=1 Xi&part;/&part;xi +
&sum;q
</p>
<p>α=1 U
α&part;/&part;uα where Xi and
</p>
<p>Uα are functions of x and u. Let
</p>
<p>Qα
(
x,u(1)
</p>
<p>)
&equiv;Uα(x,u)&minus;
</p>
<p>p&sum;
</p>
<p>i=1
Xi(x,u)uαi , α = 1, . . . , q.
</p>
<p>Then
</p>
<p>pr(n)v = pr(n)vQ +
p&sum;
</p>
<p>i=1
XiDi, (33.26)
</p>
<p>where
</p>
<p>vQ &equiv;
q&sum;
</p>
<p>α=1
Qα
</p>
<p>(
x,u(1)
</p>
<p>) &part;
&part;uα
</p>
<p>, pr(n)vQ &equiv;
q&sum;
</p>
<p>α=1
</p>
<p>&sum;
</p>
<p>J
</p>
<p>DJQ
α &part;
</p>
<p>&part;uαJ
.
</p>
<p>The sum over J extends over all multi-indices with 0 &le; |J | &le; n, with the
|J | = 0 term being simply vQ.</p>
<p/>
</div>
<div class="page"><p/>
<p>1068 33 Calculus of Variations, Symmetries, and Conservation Laws
</p>
<p>Proof Substitute Qα in the definition of UαJ as given in Theorem 32.3.5 to
obtain
</p>
<p>UαJ =DJQα +
p&sum;
</p>
<p>i=1
XiuαJ,i,
</p>
<p>where Uα0 =Qα +
&sum;p
</p>
<p>i=1 X
iuαi = Uα . It follows that (with J = 0 included
</p>
<p>in the sum)
</p>
<p>pr(n)v =
p&sum;
</p>
<p>i=1
Xi
</p>
<p>&part;
</p>
<p>&part;xi
+
&sum;
</p>
<p>J
</p>
<p>[
q&sum;
</p>
<p>α=1
DJQ
</p>
<p>α +
p&sum;
</p>
<p>i=1
XiuαJ,i
</p>
<p>]
&part;
</p>
<p>&part;uαJ
</p>
<p>=
q&sum;
</p>
<p>α=1
</p>
<p>&sum;
</p>
<p>J
</p>
<p>DJQ
α &part;
</p>
<p>&part;uαJ
+
</p>
<p>p&sum;
</p>
<p>i=1
Xi
</p>
<p>[
&part;
</p>
<p>&part;xi
+
</p>
<p>q&sum;
</p>
<p>α=1
</p>
<p>&sum;
</p>
<p>J
</p>
<p>uαJ,i
&part;
</p>
<p>&part;uαJ
</p>
<p>]
</p>
<p>︸ ︷︷ ︸
=Di by Proposition 32.3.4
</p>
<p>,
</p>
<p>and the lemma is proved. �
</p>
<p>Theorem 33.3.9 (Noether&rsquo;s theorem) LetThe celebrated Noether&rsquo;s
theorem connecting
</p>
<p>symmetries to
</p>
<p>conservation laws
v =
</p>
<p>p&sum;
</p>
<p>i=1
Xi&part;/&part;xi +
</p>
<p>q&sum;
</p>
<p>α=1
Uα&part;/&part;uα
</p>
<p>be the infinitesimal generator of a local 1-parameter group of symmetries
G of the variational problem L[u] =
</p>
<p>&acute;
</p>
<p>L(x,u(n)) dpx. Let
</p>
<p>Qα
(
x,u(1)
</p>
<p>)
&equiv;Uα(x,u)&minus;
</p>
<p>p&sum;
</p>
<p>i=1
Xi(x,u)uαi , u
</p>
<p>α
i =
</p>
<p>&part;uα
</p>
<p>&part;xi
.
</p>
<p>Then there exists a p-tuple J = (J1, . . . , Jp) such that
</p>
<p>D &middot; J =
q&sum;
</p>
<p>α=1
QαEα(L) (33.27)
</p>
<p>is a conservation law in characteristic form for the Euler-Lagrange equation
Eα(L)= 0.
</p>
<p>Proof We use Lemma 33.3.8 in the infinitesimal criterion of the variational
symmetry (33.20) to obtain
</p>
<p>0 = pr(n)v(L)+LD &middot; X
</p>
<p>= pr(n)vQ(L)+
p&sum;
</p>
<p>i=1
XiDiL+L
</p>
<p>p&sum;
</p>
<p>i=1
DiX
</p>
<p>i
</p>
<p>= pr(n)vQ(L)+
p&sum;
</p>
<p>i=1
Di
</p>
<p>(
LXi
</p>
<p>)
= pr(n)vQ(L)+D &middot; (LX). (33.28)</p>
<p/>
</div>
<div class="page"><p/>
<p>33.4 Application to Classical Field Theory 1069
</p>
<p>Using the definition of pr(n)vQ and the identity
</p>
<p>(DjS)T =Dj (ST )&minus; SDjT ,
</p>
<p>we can commute DJ =Dj1 &middot; &middot; &middot;Djk past Qα one factor at a time, each time
introducing a divergence. Therefore,
</p>
<p>pr(n)vQ(L)=
&sum;
</p>
<p>α,J
</p>
<p>DJQ
α &part;L
</p>
<p>&part;uαJ
=
&sum;
</p>
<p>α,J
</p>
<p>Qα(&minus;D)J
&part;L
</p>
<p>&part;uαJ
+D &middot; A
</p>
<p>=
q&sum;
</p>
<p>α=1
QαEα(L)+D &middot; A,
</p>
<p>where A = (A1, . . . ,Ap) is some p-tuple of functions depending on L, the
Qα&rsquo;s, and their derivatives, whose precise form is not needed here. Combin-
ing this with Eq. (33.28), we obtain
</p>
<p>0 =
q&sum;
</p>
<p>α=1
QαEα(L)+D &middot; (A +LX).
</p>
<p>Selecting J =&minus;(A +LX) proves the theorem. �
</p>
<p>33.4 Application to Classical Field Theory
</p>
<p>It is clear from the proof of Noether&rsquo;s theorem that if we are interested in
the conserved current, we need to find A. In general, the expression for A
is very complicated. However, if the variational problem is of first order
(which in most cases of physical interest it is), then we can easily find the
explicit form of A, and, consequently the conserved current J. We leave it
for the reader to prove the following:
</p>
<p>Corollary 33.4.1 Let v = &sum;pi=1 Xi&part;/&part;xi +
&sum;q
</p>
<p>α=1 U
α&part;/&part;uα be the in-
</p>
<p>finitesimal generator of a local 1-parameter group of symmetries G of the
first-order variational problem L[u] =
</p>
<p>&acute;
</p>
<p>L(x,u(1)) dpx. Then5
</p>
<p>Ji =
q&sum;
</p>
<p>α=1
</p>
<p>p&sum;
</p>
<p>j=1
Xjuαj
</p>
<p>&part;L
</p>
<p>&part;uαi
&minus;
</p>
<p>q&sum;
</p>
<p>α=1
Uα
</p>
<p>&part;L
</p>
<p>&part;uαi
&minus;XiL, i = 1, . . . , p
</p>
<p>form the components of a conserved current for the Euler-Lagrange equa-
tion Eα(L)= 0.
</p>
<p>Historical Notes
</p>
<p>Amalie Emmy Noether (1882&ndash;1935), generally considered the greatest of all female
mathematicians up to her time, was the eldest child of Max Noether, research mathe-
matician and professor at the University of Erlangen, and Ida Amalia Kaufmann. Two of
</p>
<p>5We have multiplied Ji by a negative sign to conform to physicists&rsquo; convention.</p>
<p/>
</div>
<div class="page"><p/>
<p>1070 33 Calculus of Variations, Symmetries, and Conservation Laws
</p>
<p>Emmy&rsquo;s three brothers were also scientists. Alfred, her junior by a year, earned a doctor-
ate in chemistry at Erlangen. Fritz, two and a half years younger, became a distinguished
physicist; and his son, Gottfried, became a mathematician.
At first Emmy Noether had planned to be a teacher of English and French. From 1900
</p>
<p>Amalie Emmy Noether
</p>
<p>1882&ndash;1935
</p>
<p>to 1902 she studied mathematics and foreign languages at Erlangen. Then in 1903 she
started her specialization in mathematics at the University of G&ouml;ttingen. At both universi-
ties she was a nonmatriculated auditor at lectures, since at the turn of the century women
could not be admitted as regular students. In 1904 she was permitted to matriculate at
the University of Erlangen, which granted her the Ph.D., summa cum laude, in 1907. Her
sponsor, the algebraist Gordan, strongly influenced her doctoral dissertation on algebraic
invariants. Her divergence from Gordan&rsquo;s viewpoint and her progress in the direction of
the &ldquo;new&rdquo; algebra first began when she was exposed to the ideas of Ernst Fischer, who
came to Erlangen in 1911.
In 1915 Hilbert invited Emmy Noether to G&ouml;ttingen. There she lectured at courses that
were given under his name and applied her profound invariant-theoretic knowledge to
the resolution of problems that he and Felix Klein were considering. Inspired by Hilbert
and Klein&rsquo;s investigation into Einstein&rsquo;s general theory of relativity, Noether wrote her
remarkable 1918 paper in which both the concept of variational symmetry and its con-
nection with conservation laws were set down in complete generality.
Hilbert repeatedly tried to obtain her an appointment as Privatdozent, but the strong prej-
udice against women prevented her habilitation until 1919. In 1922 she was named a
nichtbeamteter ausserordentlicher Professor (&ldquo;unofficial associate professor&rdquo;), a purely
honorary position. Subsequently, a modest salary was provided through a Lehrauftrag
(&ldquo;teaching appointment&rdquo;) in algebra. Thus she taught at G&ouml;ttingen (1922&ndash;1933), inter-
rupted only by visiting professorships at Moscow (1928&ndash;1929) and at Frankfurt (summer
of 1930).
In April 1933 she and other Jewish professors at G&ouml;ttingen were summarily dismissed.
In 1934 Nazi political pressures caused her brother Fritz to resign from his position at
Breslau and to take up duties at the research institute in Tomsk, Siberia. Through the
efforts of Hermann Weyl, Emmy Noether was offered a visiting professorship at Bryn
Mawr College; she departed for the United States in October 1933. Thereafter she lectured
and did research at Bryn Mawr and at the Institute for Advanced Studies, Princeton, but
those activities were cut short by her sudden death from complications following surgery.
Emmy Noether&rsquo;s most important contributions to mathematics were in the area of abstract
algebra. One of the traditional postulates of algebra, namely the commutative law of mul-
tiplication, was relinquished in the earliest example of a generalized algebraic structure,
e.g., in Hamilton&rsquo;s quaternion algebra and also in many of the 1844 Grassmann algebras.
From 1927 to 1929 Emmy Noether contributed notably to the theory of representations,
the object of which is to provide realizations of noncommutative algebras by means of
matrices, or linear transformations. From 1932 to 1934 she was able to probe profoundly
into the structure of noncommutative algebras by means of her concept of the verschr&auml;nk-
tes (&ldquo;cross&rdquo;) product.
Emmy Noether wrote some forty-five research papers and was an inspiration to many
future mathematicians. The so-called Noether school included such algebraists as Hasse
and W. Schmeidler, with whom she exchanged ideas and whom she converted to her
own special point of view. She was particularly influential in the work of B. L. van der
Waerden, who continued to promote her ideas after her death and to indicate the many
concepts for which he was indebted to her.
</p>
<p>Corollary 33.4.1 can be applied to most DEs in physics derivable from a
Lagrangian. We are interested in partial DEs studied in classical field the-
ories. The case of ODEs, studied in point mechanics, is relegated to Prob-
lem (33.11).
</p>
<p>First consider spacetime translation vi = ηij&part;j , where we have intro-
duced the Lorentz metric ηij to include non-Euclidean cases. In order for
vi to be an infinitesimal variational symmetry, it has to satisfy Eq. (33.20),
which in the case at hand, reduces to vi(L)= 0, or &part;iL= 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>33.4 Application to Classical Field Theory 1071
</p>
<p>Box 33.4.2 In order for a variational problem to be invariant under
spacetime translations, its Lagrangian must not depend explicitly on
the coordinates.
</p>
<p>If spacetime translation happens to be a symmetry, then Xi &rarr; ηij ,
and the (double-indexed) conserved current, derived from Corollary 33.4.1,
takes the form
</p>
<p>T ij =
q&sum;
</p>
<p>α=1
</p>
<p>&part;uα
</p>
<p>&part;xj
</p>
<p>&part;L
</p>
<p>&part;uαi
&minus; ηijL.
</p>
<p>Using Greek indices to describe space-time coordinates, and Latin indices energy momentum
current densityto label the components of Rq , we write
</p>
<p>T μν =
q&sum;
</p>
<p>j=1
</p>
<p>&part;φj
</p>
<p>&part;xμ
</p>
<p>&part;L
</p>
<p>&part;φ
j
ν
</p>
<p>&minus; ημνL&equiv;
q&sum;
</p>
<p>j=1
ημσ
</p>
<p>&part;φj
</p>
<p>&part;xσ
</p>
<p>&part;L
</p>
<p>&part;φ
j
ν
</p>
<p>&minus; ημνL, (33.29)
</p>
<p>where we changed the dependent variable u to φ to adhere to the notation
used in the physics literature. Recall that φjν &equiv; &part;φj/&part;xν . T μν is called the
energy momentum current density.
</p>
<p>The quantity T μν , having a vanishing divergence, is really a density,
just as the continuity equation (vanishing of the divergence) for the electric
charge involves the electric charge and current densities. In the electric case,
we find the charge by integrating the charge density, the zeroth component
of the electric 4-current density. Similarly, we find the &ldquo;charge&rdquo; associated
with T μν by integrating its zeroth component. This yields the energy mo-
mentum 4 vector:
</p>
<p>P ν =
ˆ
</p>
<p>V
</p>
<p>T 0νd3x.
</p>
<p>We note that
</p>
<p>dP ν
</p>
<p>dt
= d
</p>
<p>dt
</p>
<p>ˆ
</p>
<p>V
</p>
<p>T 0νd3x =
ˆ
</p>
<p>V
</p>
<p>&part;T 0ν
</p>
<p>&part;t
d3x
</p>
<p>=&minus;
ˆ
</p>
<p>V
</p>
<p>3&sum;
</p>
<p>i=1
</p>
<p>&part;T iν
</p>
<p>&part;xi
d3x =&minus;
</p>
<p>ˆ
</p>
<p>S
</p>
<p>3&sum;
</p>
<p>i=1
T iνdai,
</p>
<p>where we have used the three-dimensional divergence theorem. By taking S
to be infinite, and assuming that T iν &rarr; 0 at infinity (faster than the element
of area dai diverges), we obtain dP ν/dt = 0, the conservation of the 4-
momentum.
</p>
<p>Example 33.4.3 A relativistic scalar field of mass m is a 1-component field
satisfying the Klein&ndash;Gordan equation, which is, as the reader may check, the
Euler-Lagrange equation of
</p>
<p>L[φ] =
ˆ
</p>
<p>L(φ,φμ) d
4x &equiv;
</p>
<p>ˆ
</p>
<p>1
</p>
<p>2
</p>
<p>[
ημν&part;μφ&part;νφ &minus;m2φ2
</p>
<p>]
d4x.</p>
<p/>
</div>
<div class="page"><p/>
<p>1072 33 Calculus of Variations, Symmetries, and Conservation Laws
</p>
<p>The energy momentum current for the scalar field is found to be
</p>
<p>T μν = &part;μφ&part;νφ &minus; ημνL, &part;μφ &equiv; ημν &part;φ
&part;xν
</p>
<p>.
</p>
<p>Note that T μν is symmetric under interchange of its indices. This is a de-
sired feature of the energy momentum current that holds for the scalar field
but is not satisfied in general, as Eq. (33.29) indicates. The reader is urged
to show directly that &part;μT μν = 0 = &part;νT μν , i.e., that energy momentum is
conserved.
</p>
<p>To go beyond translation, we consider classical (nonquantized) fields6
</p>
<p>{φj }nαj=1, which, as is the case in most physical situations, transform among
themselves as the rows of the αth irreducible representation of a Lie group
G that acts on the independent variables. Under these circumstances, the
generators of the symmetry are given by Eq. (30.11):
</p>
<p>Dij (ξ)= T(α)ij (ξ )φk(x)
&part;
</p>
<p>&part;φk
+ δijXν(x; ξ)
</p>
<p>&part;
</p>
<p>&part;xν
, (33.30)
</p>
<p>where ν labels the independent variables. Corollary 33.4.1 now gives the
conserved current as
</p>
<p>J
μ
ij =
</p>
<p>{
Xμ(x; ξ)φkν (x)
</p>
<p>&part;L
</p>
<p>&part;φkν
&minus;Xμ(x; ξ)L
</p>
<p>}
δij &minus; φk(x)
</p>
<p>&part;L
</p>
<p>&part;φkμ
T
(α)
ij (ξ),
</p>
<p>where summation over repeated indices is understood with 1 &le; k &le; nα and
1 &le; ν &le; p. We can rewrite this equation in the form
</p>
<p>Jμ =
{
Xμ(x; ξ)φkν (x)
</p>
<p>&part;L
</p>
<p>&part;φkν
&minus;Xμ(x; ξ)L
</p>
<p>}
1&minus; φk(x) &part;L
</p>
<p>&part;φkμ
T(α)(ξ), (33.31)
</p>
<p>where Jμ and T(α)(ξ) are nα &times; nα matrices whose elements are Jμij and
T
(α)
ij (ξ), respectively, and 1 is the unit matrix of the same dimension.
</p>
<p>We note that the conserved current has a coordinate part (the term that
includes Xμ and multiplies the unit matrix), and an &ldquo;intrinsic&rdquo; part (the term
with no Xμ) represented by the term involving T(α)(ξ). If the field has only
one component (a scalar field), then T(α)(ξ) = 0, and only the coordinate
part contributes to the current.
</p>
<p>The current Jμ acquires an extra index when a component of ξ is chosen.
As a concrete example, consider the case where G is the rotation group in
Rp . Then a typical component of ξ will be ξρσ , corresponding to a rotation
in the ρσ -plane, and the current will be written as Jμ;ρσ . These extra indices
are also reflected in Xμ, as that too is a function of ξ :
</p>
<p>Xμ
(
x; ξρσ
</p>
<p>) &part;
&part;xμ
</p>
<p>= xρ&part;σ &minus; xσ &part;ρ &rArr; Xμ
(
x; ξρσ
</p>
<p>)
= xρδμσ &minus; xσ δμρ .
</p>
<p>6The reader notes that the superscript α, which labeled components of the independent
variable u, is now the label of the irreducible representation. The components of the
dependent variable (now denoted by φ) are labeled by j .</p>
<p/>
</div>
<div class="page"><p/>
<p>33.5 Problems 1073
</p>
<p>The volume integral of J0;ρσ will give the components of angular momen-
tum. When integrated, the term multiplying 1 becomes the orbital angular
momentum, and the remaining term gives the intrinsic spin. The conser-
</p>
<p>orbital angular
</p>
<p>momentum and intrinsic
</p>
<p>spinvation of Jμ;ρσ is the statement of the conservation of total angular mo-
mentum. The label α denotes various representations of the rotation group.
If p = 3, then α is simply the value of the spin. For example, the spin- 12
representation corresponds to α = 12 , and
</p>
<p>T(1/2)(ξ)= 1
2
</p>
<p>(
σ 1, σ 2, σ 3
</p>
<p>)
, or T(1/2)
</p>
<p>(
ξa
</p>
<p>)
= 1
</p>
<p>2
σ a, a = 1,2,3,
</p>
<p>with a labeling the three different &ldquo;directions&rdquo; of rotation.7 If the field is a
scalar, T(α)(ξ)= 0, and the field has only an orbital angular momentum.
</p>
<p>33.5 Problems
</p>
<p>33.1 Show that the derivative of a linear map from one Hilbert space to
another is the map itself.
</p>
<p>33.2 Show that a complex function f : C &sup; Ω &rarr; C considered as a map
f :R2 &sup;Ω &rarr;R2 is differentiable iff it satisfies the Cauchy-Riemann con-
ditions. Hint: Consider the Jacobian matrix of f , and note that a linear com-
plex map T :C&rarr;C is necessarily of the form T(z)= λz for some constant
λ &isin;C.
</p>
<p>33.3 Show that
δEy,i[u]
</p>
<p>δu
(x)=&minus;&part;iδ(x &minus; y).
</p>
<p>33.4 Show that the first functional derivative of L[u] &equiv;
&acute; x2
x1
</p>
<p>&radic;
1 + u2x dx, ob-
</p>
<p>tained using Eq. (33.9), is E(L).
</p>
<p>33.5 Show that for the proper time of special relativity
</p>
<p>δL[x]
δx(s)
</p>
<p>= xss
(1 &minus; x2s )3/2
</p>
<p>.
</p>
<p>Use this to show that the contribution of the second variational derivative to
the Taylor expansion of the functional is always negative.
</p>
<p>33.6 Show that the first prolongation of the Lorentz generator v = u&part;x +
x&part;u is
</p>
<p>pr(1)v = v +
(
1 &minus; u2x
</p>
<p>) &part;
&part;ux
</p>
<p>.
</p>
<p>7Only in three dimensions can one label rotations with a single index. This is because each
coordinate plane has a unique direction (by the use of the right-hand rule) perpendicular
to it that can be identified as the direction of rotation.</p>
<p/>
</div>
<div class="page"><p/>
<p>1074 33 Calculus of Variations, Symmetries, and Conservation Laws
</p>
<p>33.7 Verify that rotation in the xu-plane is a symmetry of the arc-length
variational problem (see Example 33.1.13).
</p>
<p>33.8 Show that v4, v6, and v7 of Table 32.3 are variational symmetries of
Eq. (33.21), but v5, v8, v9, and v10 are not. Find the constant c (if it exists)
such that v5 + cu&part;u is a variational symmetry. Show that no linear combi-
nation of inversions produces a symmetry.
</p>
<p>33.9 The two-dimensional Kepler problem (for a unit point mass) startsKepler problem
with the functional
</p>
<p>L=
ˆ
[
</p>
<p>1
</p>
<p>2
</p>
<p>(
x2t + y2t
</p>
<p>)
&minus; V (r)
</p>
<p>]
dt, r =
</p>
<p>&radic;
x2 + y2.
</p>
<p>(a) Show that L is invariant under t translation and rotation in the xy-
plane.
</p>
<p>(b) Find the generators of t translation and rotation in polar coordinates
and conclude that r is the best choice for the independent variable.
</p>
<p>(c) Rewrite L in polar coordinates and show that it is independent of t
and θ .
</p>
<p>(d) Write the Euler-Lagrange equations and integrate them to get θ as an
integral over r .
</p>
<p>33.10 Prove Corollary 33.4.1.
</p>
<p>33.11 Consider a system of N particles whose total kinetic energy K and
potential energy U are given by
</p>
<p>K(ẋ)= 1
2
</p>
<p>N&sum;
</p>
<p>α=1
mα
</p>
<p>∣∣ẋα
∣∣2, U(t,x)=
</p>
<p>&sum;
</p>
<p>α �=β
kαβ
</p>
<p>∣∣xα &minus; xβ
∣∣&minus;1,
</p>
<p>where xα = (xα, yα, zα) is the position of the αth particle. The variational
problem is of the form
</p>
<p>L[x] =
ˆ &infin;
</p>
<p>&minus;&infin;
L(t,x, ẋ) dt =
</p>
<p>ˆ &infin;
</p>
<p>&minus;&infin;
</p>
<p>[
K(ẋ)&minus;U(t,x)
</p>
<p>]
dt.
</p>
<p>(a) Show that the Euler-Lagrange equations are identical to Newton&rsquo;s sec-
ond law of motion.
</p>
<p>(b) Write the infinitesimal criterion for the vector field
</p>
<p>v = τ(t,x) &part;
&part;t
</p>
<p>+
&sum;
</p>
<p>α
</p>
<p>[
ξα(t,x)
</p>
<p>&part;
</p>
<p>&part;xα
+ ηα(t,x) &part;
</p>
<p>&part;yα
+ ζα(t,x) &part;
</p>
<p>&part;zα
</p>
<p>]
</p>
<p>to be the generator of a 1-parameter group of variational symmetries
of L.</p>
<p/>
</div>
<div class="page"><p/>
<p>33.5 Problems 1075
</p>
<p>(c) Show that the conserved &ldquo;current&rdquo; derived from Corollary 33.4.1 is
</p>
<p>T =
N&sum;
</p>
<p>α=1
mα
</p>
<p>(
ξα ẋα + ηα ẏα + ζα żα
</p>
<p>)
&minus; τE,
</p>
<p>where E =K +U is the total energy of the system.
(d) Find the conditions on U such that (i) time translation, (ii) space trans-
</p>
<p>lations, and (iii) rotations become symmetries of L. In each case, com-
pute the corresponding conserved quantity.
</p>
<p>33.12 Show that the Euler-Lagrange equation of
</p>
<p>L[φ] =
ˆ
</p>
<p>L(φ,φμ) d
4x &equiv;
</p>
<p>ˆ
</p>
<p>1
</p>
<p>2
</p>
<p>[
ημν&part;μφ&part;νφ &minus;m2φ2
</p>
<p>]
d4x
</p>
<p>is the Klein&ndash;Gordan equation. Verify that T μν = &part;μφ&part;νφ &minus; ημνL are the
currents associated with the invariance under translations. Show directly that
T μν is conserved.</p>
<p/>
</div>
<div class="page"><p/>
<p>Part X
</p>
<p>Fiber Bundles</p>
<p/>
</div>
<div class="page"><p/>
<p>34Fiber Bundles and Connections
</p>
<p>The elegance of the geometrical expression of physical ideas has attracted
much attention ever since Einstein proposed his geometrical theory of grav-
ity in 1916. Such an expression was, however, confined to the general theory
of relativity until the 1970s when the language of geometry was found to be
most suitable, not only for gravity, but also for the other three fundamen-
tal forces of nature. Geometry, in the form of gauge field theories of elec-
troweak and strong interactions, has been successful not only in creating a
model&mdash;the so-called standard model&mdash;that explains all experimental re-
sults to remarkable accuracy, but also in providing a common language for
describing all fundamental forces of nature, and with that a hope for unifying
these forces into a single all-embracing force. This hope is encouraged by
the successful unification of electromagnetism with the weak nuclear force
through the medium of geometry and gauge field theory.
</p>
<p>The word &ldquo;geometry&rdquo; is normally used in the mathematics literature for
a manifold on which a &ldquo;machine&rdquo; is defined with the property that it gives
a number when two vectors are fed into it. Symplectic geometry&rsquo;s machine
was a nondegenerate 2-form. Riemannian (or pseudo-Riemannian or semi-
Riemannian) geometry has a symmetric bilinear form (metric, inner prod-
uct). Both of these geometries are important: Symplectic geometry is the
natural setting for Hamiltonian dynamics, and (pseudo- or semi-) Rieman-
nian geometry is the basis of the general theory of relativity.
</p>
<p>The most elegant way of studying geometry, which very naturally en-
compasses the (pseudo-)Riemannian geometry of general relativity and the
gauge theory of the fundamental interactions of physics, is the language of
the fiber bundles, which we set out to do in this chapter.
</p>
<p>34.1 Principal Fiber Bundles
</p>
<p>In Sect. 28.4, we defined the tangent bundle T (M) as the union of the tan-
gent spaces at all points of a manifold M . It can be shown that T (M) is a
manifold, and that there is a differentiable surjective map π : T (M)&rarr;M ,
sending the tangent space Tx(M) at x &isin;M to x.1 The inverse of this map at
</p>
<p>1For ease of notation, we have changed TP (M) of Definition 28.4.1 to Tx(M).
</p>
<p>S. Hassani, Mathematical Physics, DOI 10.1007/978-3-319-01195-0_34,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>1079</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-01195-0_34">http://dx.doi.org/10.1007/978-3-319-01195-0_34</a></div>
</div>
<div class="page"><p/>
<p>1080 34 Fiber Bundles and Connections
</p>
<p>x, π&minus;1(x), is the collection of all vectors at x. The notion of tangent bundle
and the corresponding map π can be generalized to the extremely fruitful
notion of principal fiber bundle.
</p>
<p>Definition 34.1.1 A principal fiber bundle (PFB) over a manifold M withprincipal fiber bundle
Lie group G is a manifold P , called the total space or the bundle space,
and an action of G on P satisfying the following conditions:
</p>
<p>(1) G acts freely on P on the right: Rg(p)&equiv; p &middot; g &equiv; pg &isin; P .
(2) M is the space P/G of the orbits of G in P and the canonical map
</p>
<p>π : P &rarr; P/G is differentiable.
(3) P is locally trivial, i.e., for every point x &isin; M , there is a neighbor-local trivialization
</p>
<p>hood U containing x and a diffeomorphism Tu : π&minus;1(U) &rarr; U &times;G
of the form Tu(p) = (π(p), su(p)) where su : π&minus;1(U)&rarr; G has the
property su(pg)= su(p)g for all g &isin;G and p &isin; π&minus;1(U). The map Tu
is called a local trivialization (LT ).
</p>
<p>A principal fiber bundle will be denoted by P(M,G,π), or P(M,G),
or even just P . M is called the base space, G the structure group, and π
</p>
<p>base space; structure
</p>
<p>group; projection; fiber
the projection of the PFB. For each x &isin;M , π&minus;1(x) is a submanifold of P ,
called the fiber over x. If x = π(p), then π&minus;1(x) is just the orbit of G at p.
By Theorem 29.1.7, every fiber is diffeomorphic to G. There is no natural
group structure on π&minus;1(x). So, although fibers can be thought of as copies
of G, they are so only as manifolds.
</p>
<p>Remark 34.1.1 Just as a fiber sprouts from a single point of the earth
(a spherical 2-manifold), so does a fiber π&minus;1(x) sprout out of a single
point x of the manifold M . And just as you can collect a bunch of fibers
and make a bundle out of them, so can you collect a bunch of π&minus;1(x)&rsquo;s
and make P = ⋃x π&minus;1(x). Furthermore, fibers sprout vertically from the
ground. Similarly, in a sense to be elaborated in our discussion of connec-
tions, π&minus;1(x) are &ldquo;vertical&rdquo; manifolds, while M is &ldquo;horizontal.&rdquo;
</p>
<p>Example 34.1.2 Let M be any manifold and G any Lie group. Let P =
trivial bundle
</p>
<p>M &times;G and let G act on P on the right by the rule: (x, g)g&prime; = (x, gg&prime;). We
note that the action is free because
</p>
<p>(x, g)g&prime; = (x, g) &lArr;&rArr;
(
x,gg&prime;
</p>
<p>)
= (x, g) &lArr;&rArr; gg&prime; = g
</p>
<p>&lArr;&rArr; g&prime; = e.
</p>
<p>Two points (x, g) and (x&prime;, g&prime;) belong to the same orbit iff there is h &isin;G such
that (x, g)h= (x&prime;, g&prime;). This happens iff x&prime; = x and gh= g&prime;. It follows that
for any g, (x, g) belongs to the orbit at (x, e). Therefore, ❏(x, g)❑= ❏(x, e)❑.
This gives a natural identification of P/G with M . For trivialization, let the
neighborhood U of any point be M and let su(x, g)= g. This choice makes
P globally trivial, thus the name trivial for such a bundle.</p>
<p/>
</div>
<div class="page"><p/>
<p>34.1 Principal Fiber Bundles 1081
</p>
<p>Definition 34.1.3 A homomorphism of a principal fiber bundle P &prime;(M &prime;,G&prime;) homomorphism,
isomorphism, and
</p>
<p>automorphism of PFBs
</p>
<p>into another P(M,G) is a pair (f,fG) of maps f : P &prime; &rarr; P and fG :
G&prime; &rarr;G with fG a group homomorphism such that f (p&prime;g&prime;)= f (p&prime;)fG(g&prime;)
for all p&prime; &isin; P &prime; and g&prime; &isin; G&prime;. Every bundle homomorphism induces a map
fM :M &prime; &rarr;M . If f is bijective and fG a group isomorphism, then fM is a
diffeomorphism and (f,fG) is called an isomorphism of P &prime;(M &prime;,G&prime;) onto
P(M,G). An isomorphism of P(M,G) onto itself in which fG = idG is
called an automorphism of P .
</p>
<p>Requirement (3) in Definition 34.1.1 situates x &isin;M in the (sub)bundle
π&minus;1(U) which, through the diffeomorphism Tu, can be identified as the
trivial bundle U &times; G. The natural right action of G on the trivial bundle
U &times;G should therefore be identified with its action on π&minus;1(U). On the one
hand,
</p>
<p>Tu(p)=
(
π(p), su(p)
</p>
<p>)
, Tu(pg)=
</p>
<p>(
π(pg), su(pg)
</p>
<p>)
=
(
π(p), su(pg)
</p>
<p>)
,
</p>
<p>where the last equality follows because p and pg both belong to the orbit
at p. On the other hand,
</p>
<p>(
π(p), su(p)
</p>
<p>)
g =
</p>
<p>(
π(p), su(p)g
</p>
<p>)
for the trivial bundle U &times;G.
</p>
<p>So if the action of G on U &times;G is to be identified with its action on π&minus;1(U),
A local trivialization
</p>
<p>respects the action of
</p>
<p>structure group.
</p>
<p>we must have su(pg)= su(p)g. That is why this equality was demanded in
Definition 34.1.1. We summarize this by saying that Tu respects the action
of G.
</p>
<p>Now let Tu : π&minus;1(U)&rarr;U &times;G and Tv : π&minus;1(V )&rarr; V &times;G be two LTs.
If x &isin; U &cap; V and π(p) = x, then Tu(p) = (π(p), su(p)), and Tv(p) =
(π(p), sv(p)). Since su(p), sv(p) &isin; G, there must exist g &isin; G such that
gsv(p)= su(p). In fact, g = su(p)(su(p))&minus;1. What is interesting about g is
that it can be defined on M .
</p>
<p>transition functions for a
</p>
<p>PFB
</p>
<p>Definition 34.1.4 Let Tu : π&minus;1(U) &rarr; U &times; G and Tv : π&minus;1(V ) &rarr;
V &times;G be two LTs of a PFB P(M,G,π). The transition function
from Tu to Tv is the map guv : U &cap; V &rarr; G, given by guv(x) =
su(p)(su(p))
</p>
<p>&minus;1.
</p>
<p>For this definition to make sense, guv(x) must be independent of p &isin;
π&minus;1(x). Indeed, we have
</p>
<p>Proposition 34.1.5 The transition function guv from the local trivialization
Tu to the local trivialization Tv is independent of the choice of p &isin; π&minus;1(x).
Furthermore,
</p>
<p>(1) guu(x)= e &forall;x &isin;U ;
(2) guv(x)= gvu(x)&minus;1 &forall;x &isin;U &cap; V ;
(3) guv(x)= guw(x)gwv(x) &forall;x &isin;U &cap; V &cap;W .</p>
<p/>
</div>
<div class="page"><p/>
<p>1082 34 Fiber Bundles and Connections
</p>
<p>Proof Let p&prime; &isin; π&minus;1(x) be a different point from p. Since p&prime; is in the same
orbit as p, we must have p&prime; = pg for some g &isin;G. Then
</p>
<p>su
(
p&prime;
)(
su
(
p&prime;
))&minus;1 = su(pg)
</p>
<p>(
su(pg)
</p>
<p>)&minus;1 = su(p)g
(
su(p)g
</p>
<p>)&minus;1
</p>
<p>= su(p)gg&minus;1
(
su(p)
</p>
<p>)&minus;1 = su(p)
(
su(p)
</p>
<p>)&minus;1
</p>
<p>Thus guv is well defined. The other parts of the proposition are trivial. �
</p>
<p>Consider a manifold M . Let {Uα} be an open cover of M , i.e., open sets
such that M = ⋃α Uα . Let G be a Lie group. Construct the set of trivial
PFBs Pα = Uα &times; G. Connect all pairs Pα and Pβ by transition functions
gαβ : Uα &cap; Uβ &rarr; G satisfying (1)&ndash;(3) of Proposition 34.1.5. This process
constructs a PFB with transition functions gαβ . Therefore, a PFB is defined
by its transition functions. In fact, it is only the transition functions that
determine the bundle. Any PFB can be broken down into a collection {Uα &times;
G} of trivial bundles. It is how these trivial bundles are &ldquo;glued together&rdquo; via
the transition functions that distinguishes between different PFBs.
</p>
<p>Given a PFB P(M,G) and a subgroup G&prime; of G, it may be possible to find
some covering {Uα} of M and transition functions gαβ which take values in
G&prime;. The new covering and transition functions define a new PFB P &prime;(M,G&prime;).
Then we say that the PFB P(M,G) is reducible to P &prime;(M,G&prime;). We also say
</p>
<p>reducible bundle
that the structure group G of P(M,G) is reducible to G&prime; if P(M,G) is
reducible to P &prime;(M,G&prime;).
</p>
<p>Example 34.1.6 Let&rsquo;s reconsider the trivial bundle M &times; G. What is the
most general local trivialization of this bundle? Let x &isin; Uα if p = (x, g),
then Tα(p) = (x, g&prime;) for some g&prime; &isin; G, and sα(p) = sα(x, g) = g&prime;. This
means that sα affects only g, and therefore, can be reduced to a function fα :
G&rarr;G having the property that fα(gg&prime;)= fα(g)g&prime;. With g = e, this gives
fα(g
</p>
<p>&prime;) = fα(e)g&prime;. Thus, fα is simply left multiplication by hα &equiv; fα(e),
where hα may depend on Uα . Hence, the most general LT for the trivial
bundle M &times;G is
</p>
<p>Tα(p)= (x,hαg)&equiv;
(
x, sα(p)
</p>
<p>)
or sα
</p>
<p>(
(x, g)
</p>
<p>)
= hαg
</p>
<p>So, the transition functions are of the form gαβ(x)= hαh&minus;1β , and can easily
be shown to satisfy the three conditions of Proposition 34.1.5.
</p>
<p>Can the trivial bundle be reduced? Are there a covering {Uα} and tran-
sition functions gαβ which take values in a subgroup of G? In fact, G can
be drastically reduced! In the above discussion let hα = h for all α. Then,
gαβ(x)= e for all x &isin;Uα &cap;Uβ (and therefore for all x &isin;M).
</p>
<p>The converse of the last statement of the example above is also true:
</p>
<p>Proposition 34.1.7 Any PFB whose structure group can be reduced to the
identity of the group is isomorphic to the trivial PFB.</p>
<p/>
</div>
<div class="page"><p/>
<p>34.1 Principal Fiber Bundles 1083
</p>
<p>Definition 34.1.8 A local section (or local cross section) of a prin-
cipal fiber bundle P(M,G,π) on an open set U &sub; M is a map
σu : U &rarr; P such that π ◦ σu = idU . If U =M , then σu &equiv; σ is called
a global section or simply a section on M .
</p>
<p>Proposition 34.1.9 There is a natural 1-1 correspondence between the
</p>
<p>local section and section
</p>
<p>set of local trivializations and the set of local sections. In particular, if
P(M,G,π) has a (global) section, then P(M,G,π) = M &times; G, the triv-
ial bundle.
</p>
<p>Proof For each local trivialization Tu let σu = T &minus;1u |U&times;{e} :U &sim;=U &times; {e}&rarr;
P . Conversely, for each σu, define Su : U &times; G &rarr; π&minus;1(U) by Su(x, g) =
σu(x)g. Then it can be shown that Su is a bijection and Tu = S&minus;1u is a local
trivialization. �
</p>
<p>Let σu be a local section on U and σv on V . If x &isin; U &cap; V , then σu(x)
and σv(x) both belong to π&minus;1(x). Hence, there must be a g &isin; G such
that σv(x) = σu(x)g. We want to find this g. From the definition of Tu,
we have T &minus;1u (x, e) = p0 for some p0 &isin; P . Thus, Tu(p0) = (x, su(p0)) =
(x, e) implies that su(p0)= e. But T &minus;1u (x, e) = σu(x). Therefore, we have
σu(x) = p0 with su(p0) = e. Similarly, σv(x) = p1 with sv(p1) = e. Let
p1 = p0g. Then e= sv(p1)= sv(p0g)= sv(p0)g, or g = sv(p0)&minus;1. We thus
get σv(x)= p0sv(p0)&minus;1 or σv(x)sv(p0)= p0. Multiplying both sides by an
arbitrary g, we get
</p>
<p>σv(x)sv(p0)g = p0g or σv(x)sv(p0g)= p0g or
σv(x)sv(p)= p &forall;p &isin; P.
</p>
<p>An identical reasoning gives σu(x)su(p) = p. Therefore, σv(x)sv(p) =
σu(x)su(p), or σv(x)= σu(x)su(p)sv(p)&minus;1. Thus,
</p>
<p>σv(x)= σu(x)guv(x) (34.1)
</p>
<p>where guv is the transition function from Tu to Tv .
</p>
<p>Example 34.1.10 (The bundle G(G/H,H)) Let G be a Lie group and H
one of its Lie subgroups. Let H act on G on the right by right multiplication.
Let G/H be the factor group of this action and π :G&rarr;G/H , the natural
projection. It is shown in Lie group theory that such a construction has local
trivializations. Then with G as the total space, M =G/H as the base space,
and π : G &rarr; G/H as the projection, G(G/H,H,π) becomes a principal
fiber bundle.
</p>
<p>Example 34.1.11 (Bundle of linear frames) Let M be an n-manifold. A lin-
ear frame p at x &isin;M is an ordered basis (X1,X2, . . . ,Xn) of the tangent
space Tx(M). Let Lx(M) be the set of all linear frames at x and L(M) the</p>
<p/>
</div>
<div class="page"><p/>
<p>1084 34 Fiber Bundles and Connections
</p>
<p>set of all Lx(M) for all x &isin;M . Let π : L(M)&rarr;M be the map that sends
Lx(M) to x. If A &isin;GL(n,R) is a matrix with components aij , then the action
of GL(n,R) on L(M) on the right is written in matrix form as
</p>
<p>bundle of linear frames
</p>
<p>(X1 X2 . . . Xn)
</p>
<p>⎛
⎜⎜⎜⎜⎝
</p>
<p>a11 a
1
2 . . . a
</p>
<p>1
n
</p>
<p>a21 a
2
2 . . . a
</p>
<p>2
n
</p>
<p>...
...
</p>
<p>...
</p>
<p>an1 a
n
2 . . . a
</p>
<p>n
n
</p>
<p>⎞
⎟⎟⎟⎟⎠
</p>
<p>&equiv; (Y1 Y2 . . . Yn).
</p>
<p>In &ldquo;component&rdquo; form, this can be written as Yi = aji Xj (with summation
convention in place). Since A is invertible, (Y1,Y2, . . . ,Yn) &isin; L(M). So,
indeed GL(n,R) acts on L(M) on the right. It is easy to show that the ac-
tion is free (Problem 34.4). Furthermore, if p,q &isin; π&minus;1(x) &equiv; Lx(M), i.e.,
if p and q are two (ordered) bases of Tx(M), then there must exist an in-
vertible matrix A such that q = pA. Therefore, π&minus;1(x) is indeed the orbit of
GL(n,R) at p. This shows (1) and (2) of Definition 34.1.1. Foregoing the
rather technical details of (3), we find that L(M)(M,GL(n,R)) is indeed a
principal fiber bundle.
</p>
<p>Definition 34.1.12 The PFB described in Example 34.1.11 is called
the bundle of linear frames and denoted by L(M)(M,GL(n,R)), or
simply L(M).
</p>
<p>34.1.1 Associated Bundles
</p>
<p>Let P(M,G) be a PFB and F a manifold on which G acts on the left:
G &times; F &ni; (g, ξ) �&rarr; gξ &isin; F . On the product manifold P &times; F let G act on
the right by the rule Rg(p, ξ)&equiv; (p, ξ)g &equiv; (pg,g&minus;1ξ). Denote the quotient
space of this action by P &times;G F , and let E &equiv; P &times;G F . For ❏p, ξ❑ &isin; E let
πE(❏p, ξ❑)= π(p)= x &isin;M . Then πE is a projection of E onto M . Define
φu : π&minus;1E (U)&rarr;U &times;F by φu(❏p, ξ❑)= (π(p), su(p)ξ), where su : P &rarr;G
is as defined in the local trivialization of P(M,G). One can show that φu is
a diffeomorphism (Problem 34.5) and that E is a fiber bundle.
</p>
<p>associated bundle and
</p>
<p>its standard fiber and
</p>
<p>structure group Definition 34.1.13 The fiber bundle constructed above is called the
fiber bundle over the base M with standard fiber F and struc-
ture group G which is associated with the principal fiber bundle
P(M,G). E is more elaborately denoted by E(M,F,G,P ). The
fiber over x in E, π&minus;1E (x) is denoted by Fx .
</p>
<p>The diffeomorphism φu, when restricted to the fiber over x, is denoted
by φx . It is a diffeomorphism: φx : π&minus;1E (x)&rarr;{x}&times;F &sim;= F by φx(❏p, ξ❑)=
su(p)ξ . Note that this map is determined entirely by p. The inverse of this</p>
<p/>
</div>
<div class="page"><p/>
<p>34.1 Principal Fiber Bundles 1085
</p>
<p>mapping, also determined entirely by p, can be thought of as a map p :
F &rarr; Fx , given by p(ξ)&equiv; pξ = ❏p, ξ❑. It can easily be shown that this map
satisfies
</p>
<p>(pg)ξ = p(gξ) for p &isin; P, g &isin;G, ξ &isin; F. (34.2)
</p>
<p>Theorem 34.1.14 Let P(M,G) be a principal fiber bundle with the
associated bundle E(M,F,G,P ). Then each p &isin; P can be consid-
ered as a diffeomorphic map p : F &rarr; Fx satisfying (34.2).
</p>
<p>Consider two fibers Fx and Fy . They are diffeomorphic, because each
is diffeomorphic to F . In fact if p : E &rarr; Fx and q : E &rarr; Fy , then q ◦
p&minus;1 : Fx &rarr; Fy is called an isomorphism of Fy and Fx . For x = y, q ◦p&minus;1
becomes an automorphism of Fx . Moreover, since π(q) = x = π(p), we
must have q = pg for some g &isin;G. Therefore, any automorphism of Fx is
of the form p ◦ g ◦ p&minus;1.
</p>
<p>Proposition 34.1.15 The group of automorphisms of Fx is isomorphic with
the structure group G.
</p>
<p>Example 34.1.16 The bundle of linear frames consists of fibers which in-
clude all ordered bases of Tx(M) and a right action by GL(n,R), which
is the group of invertible linear transformation of Rn. For every ordered
basis p = (X1,X2, . . . ,Xn) &isin; L(M), let p(êi) = Xi , where {êi}ni=1 is the
standard basis of Rn. This then defines a map p : Rn &rarr; Tx(M). All
this, in conjunction with Theorem 34.1.14, leads to the conclusion that
E(M,Rn,GL(n,R),L(M)) is the bundle associated with the bundle of lin-
ear frames with standard fiber Rn, and that π&minus;1E (x) = Tx(M). But Defini-
tion 28.4.1, and the discussion at the very beginning of this chapter, indicate
that Tx(M) is the fiber over x of the tangent bundle T (M). We also note
that the right action p �&rarr; pA of GL(n,R) on L(M) can be interpreted as the
composite map p ◦ A:
</p>
<p>Rn
A&minus;&rarr;Rn p&minus;&rarr; Tx(M).
</p>
<p>All this discussion is summarized in
</p>
<p>Box 34.1.17 The tangent bundle T (M) is associated with the bun-
dle of linear frames L(M) with standard fiber Rn. The right action
</p>
<p>p �&rarr; pA of GL(n,R) on L(M) can be interpreted as Rn A&minus;&rarr;Rn p&minus;&rarr;
Tx(M), the composite map p ◦ A.
</p>
<p>Example 34.1.18 Let Trs (R
n) be the set of tensors of type (r, s) over the Tensor fields as sections
</p>
<p>of bundles associated to
</p>
<p>L(M)
</p>
<p>vector space Rn. The action of GL(n,R) on Rn can be extended to an action
on Trs (R
</p>
<p>n) by acting on each of the r vectors and s dual vectors separately.</p>
<p/>
</div>
<div class="page"><p/>
<p>1086 34 Fiber Bundles and Connections
</p>
<p>With Trs (R
n) as the standard fiber, we obtain the tensor bundle T rs (M) of
</p>
<p>type (r, s) over M which is associated with L(M). A tensor field of type
(r, s) is a section of this bundle: Trs :M &rarr; T rs (M).
</p>
<p>34.2 Connections in a PFB
</p>
<p>A principal fiber bundle can be thought of (locally, at least) as a continuous
collection of fibers, each fiber located at x &isin; U &sub; M . The points of each
fiber are naturally connected through the action of G. In fact, given a point of
the fiber, we can construct the entire fiber by applying all g &isin;G to that point.
This is by construction and the fact that G acts freely on each fiber. Because
each fiber is an orbit of G, and because G acts freely on the fiber, each
fiber is diffeomorphic to G. However, there is no natural diffeomorphism
connecting one fiber to its neighbor. Such a connection requires an extra
structure on the principal fiber bundle, not surprisingly called connection.
</p>
<p>Given a principal fiber bundle P(M,G), the action of G on P induces
a vector field on P for each A &isin; g (see Definition 29.1.30). In fiber bundle
</p>
<p>fundamental vector field
theory, it is common to denote this vector field by A&lowast; and call it the funda-
mental vector field corresponding to A. With γA(t)&equiv; exp(At) the integral
curve of A, the fundamental vector field at any point p &isin; P is defined as
</p>
<p>A&lowast;p =
d
</p>
<p>dt
</p>
<p>(
pγA(t)
</p>
<p>)∣∣∣∣
t=0
</p>
<p>= d
dt
</p>
<p>(
p exp(At)
</p>
<p>)∣∣∣∣
t=0
</p>
<p>. (34.3)
</p>
<p>Note that γA(0) = e, i.e., the curve passes through the identity of the Lie
group G. This is required because only Te(G) is identified as the Lie al-
gebra of G. Thus, any G-curve that passes through the identity induces a
fundamental vector field in P . Since the action on P is right multiplication,
Proposition 29.1.34 gives (AdgA)&lowast; = R&minus;1g&lowast; A&lowast; &equiv; Rg&minus;1&lowast;A&lowast; or equivalently,
Rg&lowast;A&lowast; = (Adg&minus;1A)&lowast; &equiv; (Ad&minus;1g A)&lowast;.
</p>
<p>The diffeomorphism of each fiber π&minus;1(x) with G leads to the isomor-
phism of the Lie algebra g of G with the tangent space Tp(π&minus;1(x)) at each
point p of the fiber; and since the action of G is confined to a fiber, the fun-
damental field A&lowast; must also be confined to the tangent spaces of the fibers.
To &ldquo;connect&rdquo; one fiber to its neighbor, we use the fundamental vector fields
defined on them. This is a natural thing to do since each A&lowast; originates from
the same A &isin; g.
</p>
<p>Is there anyway that we can make an association of A&lowast; with its origin A
in g? Is there a &ldquo;machine&rdquo; that spits out A when A&lowast; is fed into it? The most
obvious answer is a g-valued one form! So define a g-valued 1-form ωωω by
ωωω(A&lowast;) = A. How would ωωω change on Tp(π&minus;1(x))? The right action of G
on Tp(π&minus;1(x)) induces a right transformation R&lowast;gωωω. What would this give
when acting on A&lowast;?
</p>
<p>R&lowast;gωωω
(
A&lowast;
</p>
<p>)
&equiv;ωωω
</p>
<p>(
Rg&lowast;A&lowast;
</p>
<p>)
=ωωω
</p>
<p>((
Ad&minus;1g A
</p>
<p>)&lowast;)= Ad&minus;1g A= Ad&minus;1g ωωω
(
A&lowast;
</p>
<p>)
.</p>
<p/>
</div>
<div class="page"><p/>
<p>34.2 Connections in a PFB 1087
</p>
<p>Therefore, if a 1-form is to associate A&lowast; with A, it must satisfy R&lowast;gωωω =
Ad&minus;1g ωωω. When extended to the entire P (not just π
</p>
<p>&minus;1(x)), ωωω defines a con-
nection on P .
</p>
<p>Definition 34.2.1 A connection Ŵ is a g-valued 1-form ωωω on P such that
connection 1-form
</p>
<p>for any vector field X &isin; T (P ), ωωω(X) is the unique A &isin; g related to A&lowast; that
passes through that point. We demand that ωωω satisfy the following condi-
tions:
</p>
<p>(a) ωωω(A&lowast;)=A.
(b) R&lowast;gωωω= Ad&minus;1g ωωω on P , i.e., for any vector field X &isin; T (P ),
</p>
<p>ωωω(Rg&lowast;X)= Ad&minus;1g ωωω(X).
</p>
<p>We call ωωω a connection 1-form.
</p>
<p>Any vector field Y in T (P ) can be written as Y = Yh + A&lowast; with A&lowast; in
the tangent space of a fiber. Then, since ωωω(Y)=ωωω(A&lowast;), we get ωωω(Yh)= 0.
A vector field X in P satisfyingωωω(X)= 0 is called a horizontal vector field.
A vector Z in a tangent space of a fiber has the property that π&lowast;(Z)= 0, be-
cause π is the constant map on any fiber: π : π&minus;1(x)&rarr; x (see Box 28.3.3).
</p>
<p>horizontal and vertical
</p>
<p>vector fields
Any vector field Y in P satisfying π&lowast;(Z) = 0 is called a vertical vector
field.2 If we define the horizontal and vertical subspaces as
</p>
<p>H =
{
X &isin; T (P ) |ωωω(X)= 0
</p>
<p>}
, V =
</p>
<p>{
Z &isin; T (P )|π&lowast;(Z)
</p>
<p>}
= 0,
</p>
<p>then T (P ) = H &oplus; V . Furthermore, because of (b) in the definition above,
Hpg = Rg&lowast;Hp . Thus, at every point of P , T (P ) can be written as the
direct sum of a horizontal and a vertical subspace and these subspaces
are smoothly connected to each other. From local trivialization, we con-
</p>
<p>π&lowast; is a linear
isomorphism ofHp and
</p>
<p>TxM .
</p>
<p>clude that dimP = dimM + dimG, and since dimV = dimG, we obtain
dimH = dimM . Hence, π&lowast; :Hp &rarr; Tx(M) is a linear isomorphism.
</p>
<p>34.2.1 Local Expression for a Connection
</p>
<p>The local trivialization of a bundle with its corresponding sections could be
used to define a g-valued 1-form on the base manifold M . The pull-back ofωωω
by a local section σu is indeed a g-valued 1-form on U &sub;M : For Y &isin; T (U),
we have (by definition) σ &lowast;uωωω(Y)=ωωω(σu&lowast;Y). Since local sections depend on
the subsets chosen, and since they are connected via transition functions as
in Eq. (34.1), we have to know how ωωωu &equiv; σ &lowast;uωωω is related to ωωωv &equiv; σ &lowast;vωωω. To
take full advantage of formalism, let us write Eq. (34.1) as a composite map
</p>
<p>σv :U &cap; V α&minus;&rarr; P &times;G Φ&minus;&rarr; P,
</p>
<p>2See the remark after Definition 34.1.1.</p>
<p/>
</div>
<div class="page"><p/>
<p>1088 34 Fiber Bundles and Connections
</p>
<p>where α(x) = (σu(x), guv(x)) and Φ(σu(x), guv(x)) = σu(x)guv(x). Then
σv&lowast; =Φ&lowast; ◦ α&lowast;, and using Proposition 28.3.7, we obtain
</p>
<p>σv&lowast;(Y)=Φ&lowast;(α&lowast;Y)=Φσu(x)&lowast;
(
guv&lowast;(Y)
</p>
<p>)
+Φguv(x)&lowast;
</p>
<p>(
σu&lowast;(Y)
</p>
<p>)
. (34.4)
</p>
<p>In the second term on the right, Φguv(x)&lowast; =Rguv(x)&lowast;, the right multiplication.
For the first term, we note that
</p>
<p>Φσu(x)&lowast;
(
guv&lowast;(Y)
</p>
<p>)
= d
</p>
<p>dt
</p>
<p>(
σu(x)guv
</p>
<p>(
γY (t)
</p>
<p>))∣∣∣∣
t=0
</p>
<p>.
</p>
<p>Now, we note that σu(x) is a point in P and guv(γY (t)) is a curve in G.
Therefore, σu(x)guv(γY (t)) is a curve in P . It is not the curve of a fun-
damental vector field, because guv(γY (0)) = guv(x) �= e. However, if we
rewrite it as
</p>
<p>σu(x)guv(x)guv(x)
&minus;1guv
</p>
<p>(
γY (t)
</p>
<p>)
= σv(x)guv(x)&minus;1guv
</p>
<p>(
γY (t)
</p>
<p>)
︸ ︷︷ ︸
</p>
<p>&equiv;γ
AY
</p>
<p>(t)
</p>
<p>&equiv; σv(x)γAY (t),
</p>
<p>then γAY (0) = e and the vector field associated with γAY (t) is indeed
a fundamental vector field. It is clear that AY = Lguv(x)&minus;1&lowast;guv&lowast;(Y) =
L&minus;1guv(x)&lowast;guv&lowast;(Y). We therefore, write Eq. (34.4) as
</p>
<p>σv&lowast;(Y)=AY&lowast;σv(x) +Rguv(x)&lowast;
(
σu&lowast;(Y)
</p>
<p>)
. (34.5)
</p>
<p>Applying ωωω on both sides, we get
</p>
<p>ωωω
(
σv&lowast;(Y)
</p>
<p>)
&equiv;ωωωv(Y)=ωωω
</p>
<p>(
AY&lowast;σv(x)
</p>
<p>)
+ωωω
</p>
<p>(
Rguv(x)&lowast;
</p>
<p>(
σu&lowast;(Y)
</p>
<p>))
,
</p>
<p>or
local connection forms
</p>
<p>defined onM ωωωv(Y)= L&minus;1guv(x)&lowast;guv&lowast;(Y)+ Ad
&minus;1
guv(x)
</p>
<p>ωωωu(Y), (34.6)
</p>
<p>where for the first term on the right-hand side, we used (a) and for the second
term we used (b) of Definition 34.2.1.
</p>
<p>Let θθθ be the left-invariant canonical 1-form of Definition 29.1.29. For
each U &cap; V define a g-valued 1-form by θθθuv = g&lowast;uvθθθ . Then it can be shown
that Eq. (34.6) can be written succinctly as
</p>
<p>ωωωv = θθθuv + Ad&minus;1guvωωωu. (34.7)
</p>
<p>We defined the connection Ŵ on a principal fiber bundle as a g-valued
1-form having properties (a) and (b) of Definition 34.2.1. Then we showed
two important consequences of the definition: that the 1-form splits T (P )
into horizontal and vertical subspaces at each point of the bundle; and that
it defines a g-valued 1-form on each domain of the local trivializations and
these 1-forms are connected by (34.6). It turns out that the two consequences
are actually equivalent to the definition, i.e., that if Tp(P )=Hp&oplus;Vp at each
p &isin; P such that Hpg =Rg&lowast;Hp , then there exists a 1-form satisfying (a) and
(b) of Definition 34.2.1. Similarly, the existence of a g-valued 1-form ωωωu</p>
<p/>
</div>
<div class="page"><p/>
<p>34.2 Connections in a PFB 1089
</p>
<p>on the domain U of each trivialization Tu leads to the g-valued 1-form of
Definition 34.2.1.
</p>
<p>Example 34.2.2 Suppose that G is a matrix group, i.e., a subgroup of
GL(n,R). Then guv is a matrix-valued function or 0-form. Hence, guv&lowast; =
dguv (see the discussion on page 874), and L
</p>
<p>&minus;1
guv(x)&lowast; is just left multiplica-
</p>
<p>tion by g&minus;1uv (x). Thus the first term on the right-hand side of Eq. (34.6) is
g&minus;1uv (x)dguv(Y). For the second term, we note that for matrices A and B, we
have
</p>
<p>AdAB =
d
</p>
<p>dt
AdA
</p>
<p>(
γB(t)
</p>
<p>)∣∣∣∣
t=0
</p>
<p>= d
dt
</p>
<p>(
AγB(t)A
</p>
<p>&minus;1)
∣∣∣∣
t=0
</p>
<p>= ABA&minus;1.
</p>
<p>Therefore, Ad&minus;1guv(x)ωωωu(Y) = guv(x)
&minus;1ωωωu(Y)guv(x). Consequently, the
</p>
<p>transformation rule (34.6) can be expressed as
</p>
<p>ωωωv = g&minus;1uv dguv + g&minus;1uv ωωωuguv, (34.8)
</p>
<p>where it is understood that the vector field will be evaluated by dguv and ωωωu
on the right-hand side.
</p>
<p>local connection forms
</p>
<p>whenG is a matrix
</p>
<p>group
</p>
<p>34.2.2 Parallelism
</p>
<p>The diffeomorphism of π&minus;1(U) with U &times; G given by a local trivializa-
tion gives rise to an isomorphism of Tp(P ) and Tx(U) &times; Tg(G). A con-
nection splits Tp(P ) into a horizontal subspace and a vertical subspace, of
which the vertical subspace is isomorphic to Tg(G). Therefore, the hor-
izontal subspace must be isomorphic to Tx(U) = Tx(M). In fact since
π&lowast;(Vp) = 0, π&lowast; maps the horizontal subspace isomorphically to Tx(M),
π&lowast; :Hp
</p>
<p>&sim;=&minus;&rarr; Tx(M).
</p>
<p>Definition 34.2.3 The horizontal lift (or simply the lift) of a vector field X horizontal lift of vector
fieldson M is the unique vector field X&lowast; on P , which is horizontal and π&lowast;(X&lowast;p)=
</p>
<p>Xπ(p) for every p &isin; P .
</p>
<p>From the lift of a vector field, we can move on to the lift of a curve
in M . Given a curve γ (t)&equiv; xt in M , we can lift each point of it into P and
get a curve γ &lowast;(t) &equiv; pt in P in such a way that the tangent vector to γ &lowast; is
horizontal and maps to the tangent vector to γ at its corresponding point.
More precisely,
</p>
<p>Definition 34.2.4 Let γ (t) &equiv; xt be a curve in M . A horizontal lift (or horizontal lift of curves
simply the lift) of γ is a horizontal curve γ &lowast;(t) &equiv; pt in P such that
π(γ &lowast;(t)) = γ (t). By a horizontal curve is meant one whose tangent vec-
tors are horizontal.</p>
<p/>
</div>
<div class="page"><p/>
<p>1090 34 Fiber Bundles and Connections
</p>
<p>By local triviality, a curve γ (t) in U &sub; M maps to a curve α(t) in P ,
which may not be horizontal. If there is a horizontal curve γ &lowast;(t), each of
its points can be obtained from α(t) by right multiplication by an element
of G, because α(t) and γ &lowast;(t) both belong to the same fiber for each given t .
So, we have γ &lowast;(t)= α(t)g(t). The question is if this construction actually
works. The answer is yes, and we have the following proposition, whose
proof can be found in [Koba 63, pp. 69&ndash;70]:
</p>
<p>Proposition 34.2.5 Let γ (t), 0 &le; t &le; 1, be a curve in M . For an arbitrary
point p0 of P with π(p0) = γ (0), there exists a unique horizontal lift γ &lowast;
of γ starting at p0 (i.e., with γ &lowast;(0)= p0). Furthermore, the unique lift that
starts at p = p0g is γ &lowast;(t)g.
</p>
<p>Let γ = xt , 0 &le; t &le; 1 be a curve in M . Let p0 be an arbitrary point in
P with π(p0)= γ (0)= x0. The unique lift γ &lowast; of γ through p0 has the end
</p>
<p>parallel displacement of
</p>
<p>fibers
point p1 = γ &lowast;(1) such that π(p1) = γ (1)= x1. By varying p0 in the fiber
π&minus;1(x0), we obtain a bijection for the two fibers π&minus;1(x0) and π&minus;1(x1). De-
note this mapping by the same letter γ and call it the parallel displacement
along the curve γ .
</p>
<p>The notion of parallelism can be extended to the associated bundles as
well. For this, we need to split the tangent spaces of E into horizontal and
vertical at all points w &isin; E. If πE(w) = x &isin;M , then the tangent space to
π&minus;1E (x) at w, denoted by Vw , is by definition the vertical subspace. Let
πG : P &times; F &rarr; E be the natural projection, so that πG(p, ξ)= ❏p, ξ❑&equiv;w.
Choose a pair (p, ξ) &isin; π&minus;1G (w). If you fix p and let ξ vary over the entire F ,
by Theorem 34.1.14, you get a diffeomorphic image of F , namely π&minus;1E (x)
if πE(w)= x. More precisely, the diffeomorphism p : F &rarr; π&minus;1E (x) has the
differential map p&lowast; : T (F )&rarr; T (π&minus;1E (x)) and Vw = p&lowast;(T (F )).
</p>
<p>The procedure described above for obtaining the vertical subspace gives
horizontal and vertical
</p>
<p>subspaces of the
</p>
<p>associated bundle
</p>
<p>us a hint for defining the horizontal subspace Hw as follows. Instead of fix-
ing p, now fix ξ and let p vary. More precisely, define the map fξ : P &rarr;E
by fξ (p) = pξ with differential fξ&lowast; : Tp(P ) &rarr; Tw(E). Define the hori-
zontal subspace of Tw(E) to be the image of the horizontal subspace Hp
of Tp(P ). So, Hw &equiv; fξ&lowast;(Hp). For this assignment to be meaningful (well-
defined), it must be independent of the choice (p, ξ).
</p>
<p>Proposition 34.2.6 If πG(p1, ξ1) = w = πG(p2, ξ2), then fξ1&lowast;(Hp1) =
fξ2&lowast;(Hp2).
</p>
<p>Proof First note that fgξ = fξ ◦ Rg . Next note that if πG(p1, ξ1) =
πG(p2, ξ2), then there must exist a g &isin; G such that p2 = p1g&minus;1 and
ξ2 = gξ1. Now use these two facts plus the invariance of the horizontal
space Hp under right translation to prove the statement. �
</p>
<p>From the diffeomorphism of π&minus;1E (U) and U &times; F , and the fact that U is
an open submanifold of M , we conclude that
</p>
<p>dimTw(E)= dimTx(M)+ dimTξ (F )= dimTx(M)+ dimVw.</p>
<p/>
</div>
<div class="page"><p/>
<p>34.3 Curvature Form 1091
</p>
<p>From the diffeomorphism of π&minus;1(U) and U &times; G, and the split of Tp(P )
into horizontal and vertical subspaces, we conclude that
</p>
<p>dimHp + dimVp = dimTp(P )= dimTx(M)+ dimTg(G)
= dimTx(M)+ dimVp
</p>
<p>so that dimHp = dimTx(M). Furthermore, one can show that fξ is an injec-
tion. Hence, dimHw = dimHp , and therefore, dimHw = dimTx(M). This,
plus the first equation above yields Tw(E)=Hw &oplus; Vw .
</p>
<p>Definition 34.2.7 A vector field Z in E is horizontal if Z = fξ&lowast;(X&lowast;) for
horizontal lift in the
</p>
<p>associated bundle
some horizontal vector field X&lowast; in P . A curve in E is horizontal if its tangent
vector is horizontal at each point of the curve. Given a curve γ in M , a
(horizontal) lift is a horizontal curve γ &lowast;E in E such that πE(γ
</p>
<p>&lowast;)= γ .
</p>
<p>Just as there was a unique horizontal lift for every curve γ in M starting
at a given point of the principal fiber bundle P , so is there a unique hori-
zontal lift of every curve γ in M starting at a given point of the associated
bundle E. In fact, let γ (t) = xt be a curve in M . Let w0 &isin; E be such that
πE(w0) = x0. Then there is a p0 &isin; P such that p0ξ = w0. Let γ &lowast;(t) = pt
be the lift of xt starting at p0. Let wt = ptξ . Then wt is a horizontal curve
starting at w0. The fact that it is unique follows from the uniqueness of the
solution of differential equations with give initial conditions. We thus have
</p>
<p>Theorem 34.2.8 Given a curve γ (t)= xt , 0 &le; t &le; 1 inM and a point w0 &isin;
E such that πE(w0)= x0, there is a unique lift γ &lowast;E(t)=wt starting at w0. In
fact, if w0 = ❏p0, ξ❑, then wt = ptξ , where pt is the lift of xt in P starting
at p0.
</p>
<p>Recall that a (cross) section σ of E is a map σ : U &rarr; E such that
πEσ(x) = x. Let xt , 0 &le; t &le; 1 be a curve in U . Let w0 = σ(x0). Then,
clearly σ(xt ) is a curve in E starting at w0. Let wt be the horizontal lift of
xt starting at w0. In general, of course, wt �= σ(xt ).
</p>
<p>parallel section
</p>
<p>Definition 34.2.9 We say the section σ of E is parallel if σ(xt ), 0 &le;
t &le; 1 is the horizontal lift of xt .
</p>
<p>34.3 Curvature Form
</p>
<p>A connection is a 1-form on P which allows a parallel displacement of
sections of its associated bundles. Infinitesimal displacements carry the no-
tion of differentiation which is important in differential geometry. As a
1-form, the connection accepts another kind of differentiation, namely ex-
terior derivative. But this differentiation ought to be generalized so that it is
compatible with the action of the structure group.</p>
<p/>
</div>
<div class="page"><p/>
<p>1092 34 Fiber Bundles and Connections
</p>
<p>Definition 34.3.1 Let P(M,G) be a principal fiber bundle and ρ : G &rarr;
GL(V) a representation of the structure group on a vector space V. A pseu-
dotensorial form of degree r on P of type (ρ,V) is a V-valued r-form φφφ
</p>
<p>pseudotensorial and
</p>
<p>tensorial forms
on P such that
</p>
<p>R&lowast;gφφφ = ρ
(
g&minus;1
</p>
<p>)
&middot;φφφ.
</p>
<p>φφφ is called a tensorial form if φφφ(X1,X2, . . . ,Xr)= 0 when any of the Xi &isin;
T (P ) is vertical. In this case we say that φφφ is horizontal.
</p>
<p>Example 34.3.2 (a) Let ρ be the adjoint representation Ad : g &rarr; GL(g)
given in Definition 29.1.26. Then (b) of Definition 34.2.1 shows that the
connection form ωωω is a pseudotensorial form of degree 1 of type (Ad,g).
</p>
<p>(b) Let ρ0 be the trivial representation, sending all elements of G to the
identity of GL(V). Then a tensorial form of degree r of type (ρ0,V) is sim-
ply an r-form on P which can be written as φφφ = π&lowast;φφφM where φφφM is a
V-valued r-form on the base manifold M . In particular, if V= R, then φφφ is
the pull-back by π of an ordinary r-form on M .
</p>
<p>Remark 34.3.1 Let E(M,V,G,P ) be the bundle associated with the prin-
cipal fiber bundle P with standard fiber V on which G acts through a rep-
resentation ρ. A tensorial form φφφ of degree r of type (ρ,V) can be consid-
ered as an assignment to each x a multilinear skewsymmetric mapping φ̃φφx
of Tx(M)&times; Tx(M)&times; &middot; &middot; &middot; &times; Tx(M) (r times) into the vector space π&minus;1E (x)
which is the fiber of E over x. Here is how:
</p>
<p>φ̃φφx(X1,X2, . . . ,Xr)&equiv; p
(
φφφ
(
X&lowast;1,X
</p>
<p>&lowast;
2, . . . ,X
</p>
<p>&lowast;
r
</p>
<p>))
, Xi &isin; Tx(M). (34.9)
</p>
<p>On the right-hand side, X&lowast;i is any vector field at p such that π&lowast;(X
&lowast;
i ) =
</p>
<p>Xi , and p is any point of P with π(p) = x. Since φφφ is V-valued,
φφφ(X&lowast;1,X
</p>
<p>&lowast;
2, . . . ,X
</p>
<p>&lowast;
r ) is in V, the standard fiber of E, on which p acts ac-
</p>
<p>cording to Theorem 34.1.14 to give a vector in π&minus;1E (x). As Problem 34.12
shows, the left-hand side of Eq. (34.9) is independent of the choice of p
and X&lowast;i on the right-hand side. Conversely, given a skewsymmetric multi-
linear mapping φ̃φφx of Tx(M)&times; Tx(M)&times; &middot; &middot; &middot; &times; Tx(M) to π&minus;1E (x) for each
x &isin;M , p&minus;1 ◦ φ̃φφx ◦ π&lowast; is a tensorial r-form of type (ρ,V), with p chosen
such that π(p) = x. In particular, a cross section f̃ :M &rarr; E can be iden-
tified with f = p&minus;1 ◦ f̃ ◦ π , which is a V-valued function on P satisfying
f (pg)= ρ(g&minus;1)f (p).
</p>
<p>In the special case where ρ is the identity representation and V=R, φ̃φφ is
just an ordinary r-form, i.e., φ̃φφ &isin;Λr(M).
</p>
<p>Let P(M,G) be a principal fiber bundle with a connection, giving rise
to the split Tp(P )=Hp &oplus;Vp into horizontal and vertical subspaces at each
point p &isin; P . Define h : Tp(P )&rarr;Hp to be the projection onto the horizontal
subspace. For a pseudotensorial r-form φφφ on P , define φφφh by
</p>
<p>(φφφh)(X1,X2, . . . ,Xr)=φφφ(hX1, hX2, . . . , hXr), Xi &isin; Tp(P ). (34.10)</p>
<p/>
</div>
<div class="page"><p/>
<p>34.3 Curvature Form 1093
</p>
<p>Definition 34.3.3 Let P(M,G) be a principal fiber bundle with a connec-
tion 1-formωωω that induces the split Tp(P )=Hp&oplus;Vp . Let h : Tp(P )&rarr;Hp
be the projection onto the horizontal subspace. The exterior covariant
derivative (associated with ωωω) of a (pseudo)tensorial r-form φφφ is defined exterior covariant
</p>
<p>differentiationas Dωφφφ = (dφφφ)h and Dω is called the exterior covariant differentiation.
</p>
<p>The proof of the following proposition is straightforward:
</p>
<p>Proposition 34.3.4 Let φφφ be a pseudotensorial r-form on P of type (ρ,V).
Then
</p>
<p>(a) the form φφφh is a tensorial r-form of type (ρ,V);
(b) dφφφ is a pseudotensorial (r + 1)-form of type (ρ,V);
(c) Dωφφφ is a tensorial (r + 1)-form of type (ρ,V).
</p>
<p>Definition 34.3.5 The tensorial 2-form���ω &equiv;Dωωωω of type (Ad,g) is called
curvature form
</p>
<p>the curvature form of ωωω.
</p>
<p>The proof of the following structure equation can be found in [Koba 63,
pp. 77&ndash;78]
</p>
<p>Theorem 34.3.6 The curvature form ���ω of the connection form ωωω
satisfies the following equation:
</p>
<p>���ω(X,Y)= dωωω(X,Y)+ 1
2
</p>
<p>[
ωωω(X),ωωω(Y)
</p>
<p>]
, X,Y &isin; Tp(P ).
</p>
<p>The equation of this theorem is abbreviated as
structure equation
</p>
<p>���ω = dωωω+ 1
2
[ωωω,ωωω]. (34.11)
</p>
<p>The commutator in (34.11) is a Lie algebra commutator (in particular, it is
not zero). In fact, Eq. (34.13) below captures the meaning of (34.11).
</p>
<p>If X and Y are both horizontal vector fields on P , then Theorem 28.5.11
yields
</p>
<p>���ω(X,Y)=&minus;ωωω
(
[X,Y]
</p>
<p>)
. (34.12)
</p>
<p>Note that the right-hand side is not zero, because the Lie bracket of two
horizontal vector fields is not necessarily horizontal.
</p>
<p>It is convenient to have an expression for the structure equation in terms
of real-valued forms. So, let {Ei}mi=1 be a basis for the Lie algebra g with
structure constants cijk , so that
</p>
<p>[Ej ,Ek] = cijkEi, j, k = 1,2, . . . ,m.</p>
<p/>
</div>
<div class="page"><p/>
<p>1094 34 Fiber Bundles and Connections
</p>
<p>As g-valued forms, ωωω and ���ω can be expressed as ωωω = ωiEi and ���ω =
Ω iEi , where ωi and Ω i are ordinary real-valued forms. It is straightforward
to show that the structure equation can be expressed as
</p>
<p>Ω i = dωi + 1
2
cijkω
</p>
<p>j &and;ωk, i = 1,2, . . . ,m. (34.13)
</p>
<p>Taking the exterior derivative of both sides of this equation, one can show
that
</p>
<p>dΩ i = cijkΩj &and;ωk, or d���ω =
[
���ω,ωωω
</p>
<p>]
. (34.14)
</p>
<p>The proof of the following theorem, which follows easily from the last two
equations, is left as an exercise for the reader (see Problem 34.16):
</p>
<p>Bianchi&rsquo;s identity
</p>
<p>Theorem 34.3.7 (Bianchi&rsquo;s identity) Dω���ω = 0.
</p>
<p>In Sect. 34.2.1, we expressed a connection Ŵ in terms of its 1-form de-
fined on the base manifold M . It is instructive to obtain similar expres-
sions for the curvature form as well. In fact, since the local connection
form was simply the pull-back by the local sections, and since the exterior
derivative and exterior product both commute with the pullback, we define
���ωu &equiv; σ &lowast;u���ω and easily prove
</p>
<p>Theorem 34.3.8 ���ωu = dωωωu + 12 [ωωωu,ωωωu].
</p>
<p>We found how different pieces of the connection 1-form, defined on dif-
ferent subsets of M , were related to each other (see (34.6)). We can do the
same with the curvature 2-form. Using Eq. (34.5) and the definition of pull-
back, we find
</p>
<p>���ωv (X,Y)&equiv; σ v&lowast;���ω(X,Y)=���ω(σv&lowast;X, σv&lowast;Y)
</p>
<p>=���ω
(
AX&lowast;σv(x) +Rguv(x)&lowast;(σu&lowast;X),A
</p>
<p>Y&lowast;
σv(x)
</p>
<p>+Rguv(x)&lowast;(σu&lowast;Y)
)
</p>
<p>=���ω
(
Rguv(x)&lowast;(σu&lowast;X),Rguv(x)&lowast;(σu&lowast;Y)
</p>
<p>)
,
</p>
<p>because ���ω is a tensorial form, and hence, gives zero for its vertical ar-
guments. By Proposition 34.3.4, ���ω is of the same type as ωωω, i.e., of type
(Ad,g). Therefore, we have
</p>
<p>���ωv (X,Y)= Adg&minus;1uv ���
ω(σu&lowast;X, σu&lowast;Y)= Adg&minus;1uv σ
</p>
<p>u&lowast;���ω(X,Y),
</p>
<p>or
</p>
<p>���ωv = Adg&minus;1uv ���
ω
u (34.15)
</p>
<p>Using Eq. (34.14), a similar derivation leads to the local version of the
Bianchi&rsquo;s identity:
</p>
<p>d���ωu =
[
���ωu ,ωωωu
</p>
<p>]
. (34.16)</p>
<p/>
</div>
<div class="page"><p/>
<p>34.3 Curvature Form 1095
</p>
<p>All the foregoing discussion simplifies considerably if the structure group
is abelian. In this case all the structure constants cijk vanish and Equations
</p>
<p>(34.13) and (34.14) become Ω i = dωi and dΩ i = 0, respectively. Further-
more, it can be shown (Problem 34.17) that Adg = idg, the identity of the
Lie algebra of G. We summarize all this in
</p>
<p>The abelian case
</p>
<p>Proposition 34.3.9 If the structure group is abelian, then
</p>
<p>ωωωv = θθθuv +ωωωu, ���ω = dωωω, d���ω = 0,
���ωu = dωωωu, ���ωv =���ωu
</p>
<p>where θθθuv is as in Eq. (34.7) and represents the first term on the right-hand
side of Eq. (34.6).
</p>
<p>34.3.1 Flat Connections
</p>
<p>Let P = M &times;G be a trivial principal fiber bundle. Let π2 : M &times;G &rarr; G
be the projection onto the second factor. The differential of this map, π2&lowast; :
T (M)&times;T (G)&rarr; T (G), has the property that π2&lowast;(X)= 0 if X &isin; T (M) (see
Box 28.3.3). With θθθ the canonical left-invariant 1-form on G, let ωωω = π&lowast;2θθθ .
Then ωωω is a 1-form on P , and one can show that it satisfies the two condi-
tions of Definition 34.2.1. Hence, ωωω is a connection on P . The horizontal
space of this connection is clearly T (M). The connection associated with
</p>
<p>canonical flat
</p>
<p>connection
this ωωω is called the canonical flat connection of P . The Maurer-Cartan
equation (29.17) yields
</p>
<p>dωωω= d
(
π&lowast;2θθθ
</p>
<p>)
= π&lowast;2 (dθθθ)= π&lowast;2
</p>
<p>(
&minus;1
</p>
<p>2
[θ , θ ]
</p>
<p>)
</p>
<p>=&minus;1
2
</p>
<p>[
π&lowast;2 (θ),π
</p>
<p>&lowast;
2 (θ)
</p>
<p>]
=&minus;1
</p>
<p>2
[ωωω,ωωω].
</p>
<p>Comparison with Eq. (34.11) implies that the curvature of the canonical flat
connection is zero.
</p>
<p>Definition 34.3.10 A connection Ŵ in any principal fiber bundle P(M,G)
flat connection
</p>
<p>is called flat if every x &isin;M has a neighborhood U such that the connection
in π&minus;1(U) is isomorphic to the canonical flat connection in U &times;G.
</p>
<p>The vanishing of the curvature is a necessary condition for a connection
to be flat. It turns out that it is also sufficient (see [Koba 63, p. 92] for a
proof):
</p>
<p>Theorem 34.3.11 A connection in a principal fiber bundle P(M,G)
is flat if and only if the curvature form vanishes identically.
</p>
<p>The existence of a flat connection in a principal fiber bundle determines
the nature of the bundle:</p>
<p/>
</div>
<div class="page"><p/>
<p>1096 34 Fiber Bundles and Connections
</p>
<p>Corollary 34.3.12 If P(M,G) has a connection whose curvature form
vanishes identically, then P(M,G) is (isomorphic to) the trivial bundle
M &times;G and the connection is the canonical flat connection.
</p>
<p>34.3.2 Matrix Structure Group
</p>
<p>The structure groups encountered in physics are almost exclusively matrix
groups, or subgroups of GL(n,R). For these groups, the equations derived
above take a simpler form [see, for example, Eq. (34.8)]. Furthermore, it is
a good idea to have these special formulas, so we can use them when need
arises.
</p>
<p>Proposition 34.3.13 Let N be a manifold and G a matrix Lie group with
Lie algebra g. For φφφ &isin;Λk(N,g) and ψψψ &isin;Λj (N,g), we have
</p>
<p>[φφφ,ψψψ] =φφφ &and;ψψψ &minus; (&minus;1)kjψψψ &and;φφφ
</p>
<p>where φφφ and ψψψ are regarded as matrices of R-valued forms and φφφ &and;ψψψ is
matrix multiplication with elements multiplied via wedge product.
</p>
<p>Proof For matrix algebras, the commutator is just the difference in products
of matrices. Therefore,
</p>
<p>[φφφ,ψψψ](X1, . . . ,Xk+j )
</p>
<p>&equiv; 1
k!j !
</p>
<p>&sum;
</p>
<p>π
</p>
<p>ǫπφφφ(Xπ(1), . . . ,Xπ(k))ψψψ(Xπ(k+1), . . . ,Xπ(k+j))
</p>
<p>︸ ︷︷ ︸
=(φφφ&and;ψψψ)(X1,...,Xk+j )
</p>
<p>&minus; 1
k!j !
</p>
<p>&sum;
</p>
<p>π
</p>
<p>ǫπψψψ(Xπ(k+1), . . . ,Xπ(k+j))φφφ(Xπ(1), . . . ,Xπ(k))
</p>
<p>= (φφφ &and;ψψψ)(X1, . . . ,Xk+j )
</p>
<p>&minus; 1
k!j !
</p>
<p>&sum;
</p>
<p>π
</p>
<p>ǫπ (&minus;1)kjψψψ(Xπ(1), . . . ,Xπ(j))φφφ(Xπ(j+1), . . . ,Xπ(j+k)).
</p>
<p>Noting that the last sum is (&minus;1)kj (ψψψ &and; φφφ)(X1, . . . ,Xk+j ), we obtain the
result we are after. �
</p>
<p>Corollary 34.3.14 If G is a matrix group, then
</p>
<p>���ω = dωωω+ωωω &and;ωωω and ���ωu = dωωωu +ωωωu &and;ωωωu.
</p>
<p>Proof The proof follows immediately from Eq. (34.11) and Proposi-
tion 34.3.13 with k = j = 1. �
</p>
<p>The following theorem can also be easily proved:</p>
<p/>
</div>
<div class="page"><p/>
<p>34.4 Problems 1097
</p>
<p>Theorem 34.3.15 Let Tu and Tv be two local trivializations with transition
function guv :U &cap; V &rarr;G, where G is a matrix group. Then
(a) ���ωv = g&minus;1uv ���ωuguv ;
(b) d���ωu =���ωu &and;ωωωu &minus;ωωωu &and;���ωu .
</p>
<p>34.4 Problems
</p>
<p>34.1 Show that
</p>
<p>(a) a fiber bundle homomorphism preserves the fibers, i.e., two points be-
longing to the same fiber of P &prime; get mapped to the same fiber of P ;
</p>
<p>(b) if (f,fG) of Definition 34.1.3 is an isomorphism, then the induced
map fM :M &prime; &rarr;M is a bijection.
</p>
<p>34.2 Finish the proof of Proposition 34.1.5.
</p>
<p>34.3 Complete the proof of Proposition 34.1.9.
</p>
<p>34.4 Using the linear independence of the vectors in a basis, show that the
action of GL(n,R) on L(M) is free.
</p>
<p>34.5 Show that φu : π&minus;1(U) &rarr; U &times; F defined by φu(❏p, ξ❑) = (π(p),
su(p)ξ) for the associated fiber bundle is well defined (i.e., if ❏p&prime;, ξ &prime;❑ =
❏p, ξ❑ then φu(❏p&prime;, ξ &prime;❑)= φu(❏p, ξ❑)) and bijective.
</p>
<p>34.6 Show that the map p :E &rarr; Fx , given by p(ξ)&equiv; pξ = ❏p, ξ❑ satisfies
</p>
<p>(pg)ξ = p(gξ) for p &isin; P, g &isin;G, ξ &isin;E.
</p>
<p>34.7 Show that the map Su : U &times; G &rarr; π&minus;1(U), given by Su(x, g) =
σu(x)g for a local cross section σu is a bijection, and that Tu = S&minus;1u is a
local trivialization satisfying condition (3) of Definition 34.1.1.
</p>
<p>34.8 Provide the details of the proof of Proposition 34.2.6.
</p>
<p>34.9 Show that the map fξ : P &rarr; E defined by fξ (p) = pξ is injective.
Hint: Show that ❏p1, ξ❑= ❏p2, ξ❑ implies p1 = p2.
</p>
<p>34.10 Show that Eq. (34.7) follows from Eq. (34.6).
</p>
<p>34.11 Show that the canonical flat connection on P = M &times; G given by
ωωω= π&lowast;2θθθ , where θθθ is the canonical 1-form on G satisfies both conditions of
Definition 34.2.1.
</p>
<p>34.12 Show that Eq. (34.9) is independent of the choice of p and X&lowast;i on the
right-hand side.</p>
<p/>
</div>
<div class="page"><p/>
<p>1098 34 Fiber Bundles and Connections
</p>
<p>34.13 Prove Proposition 34.3.4.
</p>
<p>34.14 Derive Eq. (34.12).
</p>
<p>34.15 Derive Eq. (34.13).
</p>
<p>34.16 Taking the exterior derivative of both sides of Eq. (34.13), show that
</p>
<p>dΩ i = cijkΩj &and;ωk &minus;
1
</p>
<p>2
cijkc
</p>
<p>j
lmω
</p>
<p>l &and;ωm &and;ωk.
</p>
<p>Using Lie&rsquo;s third theorem (29.13), show that the second term on the right-
hand side vanishes. Now prove Bianchi&rsquo;s identity of Theorem 34.3.7. Hint:
ωωω(X)= 0 if X is horizontal.
</p>
<p>34.17 Let idM :M &rarr;M be the identity map on M . Prove that idM&lowast; is the
identity map on T (M). Let Ig = Rg&minus;1 ◦Lg be the inner automorphism of a
Lie group G. Show that if G is abelian, then Ig = idG for all g &isin;G. Now
show that Adg = idg.
</p>
<p>34.18 Prove Theorem 34.3.8.
</p>
<p>34.19 Provide the details of Corollary 34.3.14.
</p>
<p>34.20 Provide the details of Theorem 34.3.15.</p>
<p/>
</div>
<div class="page"><p/>
<p>35Gauge Theories
</p>
<p>The machinery developed in Chap. 34 has found a natural setting in gauge
theories, which have been successfully used to describe the electromagnetic,
weak nuclear, and strong nuclear interactions. In these physical applications,
one considers a principal fiber bundle P(M,G), where M =R4 with a met-
ric ηηη which is diagonal with η11 =&minus;1 =&minus;ηii for i = 2,3,4, and the struc-
ture group is a matrix group, typically SU(n).
</p>
<p>35.1 Gauge Potentials and Fields
</p>
<p>The application of the theory of the principal fiber bundle to physics concen-
trates on the local quantities. In a typical situation, one picks a local section
σu, which in the new setting is called a choice of gauge, and works with
</p>
<p>choice of gauge; gauge
</p>
<p>potential; gauge field
ωωωu &equiv; σ &lowast;uωωω, now called a gauge potential. The curvature form, being the
derivative of the gauge potential is now called gauge field. Theorem 34.3.11
and Corollary 34.3.12 imply that all principal fiber bundles used in physics
are nontrivial, because otherwise the gauge field would be identically zero.
</p>
<p>Example 35.1.1 Consider the simplest (unitary) matrix group U(1). Any
group member can be written as eiα with α &isin;R. The Lie algebra consists of
just the exponents: u = {iα|α &isin; R}, implying that the basis is i =
</p>
<p>&radic;
&minus;1. If
</p>
<p>σu :U &rarr; P , U &sub;R4 is a local section, then ωωωu is a u-valued 1-form on U .
Since the Lie algebra is one-dimensional, the discussion in Sect. 34.2.1 im-
plies that ωωωu = iAu. Here Au is a real-valued 1-form on U . Pick a coordi-
nate basis in U and write Au =Aukdxk . Note that Au has four components
{Auk}4k=1. That is why it is called a vector potential.
</p>
<p>Another section σv yields ωωωv = iAv . We want to see how ωωωv is related
to ωωωu. Since the structure group is an abelian matrix group, Eq. (34.8) and
Proposition 34.3.9 give
</p>
<p>ωωωv = g&minus;1uv dguv +ωωωu.
</p>
<p>With guv(x)= eiαuv(x), we obtain
</p>
<p>dguv = d
(
eiαuv(x)
</p>
<p>)
= eiαuv(x)i&part;kαuvdxk.
</p>
<p>S. Hassani, Mathematical Physics, DOI 10.1007/978-3-319-01195-0_35,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>1099</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-01195-0_35">http://dx.doi.org/10.1007/978-3-319-01195-0_35</a></div>
</div>
<div class="page"><p/>
<p>1100 35 Gauge Theories
</p>
<p>Hence, the preceding equation yields
</p>
<p>iAvkdx
k = e&minus;iαuv(x)eiαuv(x)i&part;kαuvdxk + iAukdxk,
</p>
<p>or
</p>
<p>Avk =Auk + &part;kαuv, k = 1, . . . ,4. (35.1)
The reader may recognize this as the gauge transformation of the vector
potential of electrodynamics.
</p>
<p>The curvature can be obtained using Proposition 34.3.9, which also
shows that the curvature is independent of the local section (although the
connection itself is not). Thus, ignoring the subscript u, with ωωω = iA =
iAkdx
</p>
<p>k , we obtain
</p>
<p>���= dωωω= d
(
iAkdx
</p>
<p>k
)
= i&part;jAkdxj &and; dxk,
</p>
<p>and writing ���= iF , where F is a real-valued 2-form, yields
</p>
<p>F = &part;jAkdxj &and; dxk =
1
</p>
<p>2
(&part;jAk &minus; &part;kAj )dxj &and; dxk.
</p>
<p>The (antisymmetric) components of F are therefore,
</p>
<p>Fjk = &part;jAk &minus; &part;kAj ,
</p>
<p>which is the familiar electromagnetic field strength, with
</p>
<p>Ej = F1j , B1 = &part;2A3 &minus; &part;3A2, B2 = &part;3A1 &minus; &part;1A3,
B3 = &part;1A2 &minus; &part;2A1.
</p>
<p>The Bianchi&rsquo;s identity, d���= 0 or dF = 0, in terms of components, be-
comes
</p>
<p>0 = dF = &part;lFjkdxl &and; dxj &and; dxk,
which can be shown to lead to the two homogeneous Maxwell&rsquo;s equations:
</p>
<p>&nabla;&nabla;&nabla; &middot; B = 0, &nabla;&nabla;&nabla; &times; E =&minus;&part;B
&part;t
</p>
<p>.
</p>
<p>We summarize the discussion of the preceding example:
</p>
<p>Box 35.1.2 Electromagnetic interaction is a principal fiber bundle
P(M,G) with M a Minkowski space and G=U(1).
</p>
<p>It appears as if the P in the principal fiber bundle had no role in our dis-
cussion above. That is not so! Remember that the structure of P as a bundle
is determined entirely by the transition functions (see the discussion after
Proposition 34.1.5). And we used the transition functions in determining
how the connection 1-forms were glued together on the base manifold.</p>
<p/>
</div>
<div class="page"><p/>
<p>35.1 Gauge Potentials and Fields 1101
</p>
<p>35.1.1 Particle Fields
</p>
<p>Principal fiber bundles give us gauge potentials and gauge fields. A gauge
field is responsible for interaction among matter particles, and an interaction
is typically described by a Lagrangian written in terms of fields and their
derivatives. Therefore, we have to know how to describe matter (or particle)
fields and how to differentiate them.
</p>
<p>The realistic treatment of particle fields requires the introduction of the
so-called Clifford and spinor bundles. This is because all fundamental par- Clifford and spinor
</p>
<p>bundlesticles, i.e., quarks and leptons, are described by (complex) spinors, not vec-
tors. Although we have discussed the algebra of spinors, and it is indeed the
starting point of the discussion of Clifford and spinor bundles, the construc-
tion of a differential structure on these bundles and its pull-back to the base
manifold M = (R4,ηηη) is beyond the scope of this book.
</p>
<p>Therefore, we restrict our discussion to the (admittedly unphysical) case,
where particles are described by vectors rather than spinors. This brief dis-
cussion is not entirely useless, because there are certain similarities between
vector bundles and spinor bundles, and the discussion can pave the way to
the understanding of the realistic case of Clifford and spinor bundles.
</p>
<p>Definition 35.1.3 Let P(M,G) be a principal fiber bundle and V a vec-
tor space on which G acts (on the left) through a representation. Let particle field
E(M,V,G,P ) be the associated bundle with standard fiber V. A section
of E, i.e., a map ψ :M &rarr;E is called a particle field.
</p>
<p>Let Λ̄k(P,V) be the set of tensorial forms of degree k of type (ρ,V).
Let g&rarr; gl(V) be the Lie algebra homomorphism induced by the represen-
tation G&rarr;GL(V). If φφφ &isin;Λk(P,V) and μμμ &isin;Λj (P,g), then we can define
a wedge product &and;̇ :Λj (P,g)&times;Λk(P,V)&rarr;Λj+k(P,V) as follows:
</p>
<p>(μμμ&and;̇φφφ)(X1, . . . ,Xj+k)
</p>
<p>= 1
j !k!
</p>
<p>&sum;
</p>
<p>π
</p>
<p>ǫπμμμ(Xπ(1), . . . ,Xπ(j)) &middot;φφφ(Xπ(j+1), . . . ,Xπ(j+k)). (35.2)
</p>
<p>Note that μμμ(Xπ(1), . . . ,Xπ(j)) &isin; g and φφφ(Xπ(j+1), . . . ,Xπ(j+k)) &isin; V, so the
dot (symbolizing the action of the Lie algebra) between the two makes
sense.
</p>
<p>Theorem 35.1.4 Let Dω be the exterior covariant differentiation of Defini-
tion 34.3.3. If φφφ &isin; Λ̄k(P,V), then
</p>
<p>Dωφφφ = dφφφ +ωωω&and;̇φφφ
DωDωφφφ =���&and;̇φφφ.
</p>
<p>Proof The proof of the first equation involves establishing the equality for
cases in which the vector fields in the argument of Dωφφφ (a) are all horizontal,
(b) all but one are horizontal, and (c) have two or more vertical vectors. The
details can be found in [Ble 81, pp. 44&ndash;45].</p>
<p/>
</div>
<div class="page"><p/>
<p>1102 35 Gauge Theories
</p>
<p>The second equation can be derived by taking the exterior derivative of
the first and using the definition of Dω. We have
</p>
<p>DωDωφφφ = d(dφφφ +ωωω&and;̇φφφ)h= (dωωω)h&and;̇φφφh&minus; (ωωωh)&and;̇(dφφφ)h=Dωωωω&and;̇φφφ
=���&and;̇φφφ,
</p>
<p>because ωωωh&equiv; 0 and φφφh=φφφ. �
</p>
<p>Corollary 35.1.5 Let φφφ &isin; Λ̄k(P,g) and assume that the action on g is via
the adjoint representation. Then Dωφφφ = dφφφ + [ωωω,φφφ].
</p>
<p>Note that there is no conflict with Eq. (34.11) because ωωω is a pseudoten-
sorial form not a tensorial form.
</p>
<p>35.1.2 Gauge Transformation
</p>
<p>The gauge transformation of the electromagnetic vector potential (35.1) was
obtained by starting with local sections glued together by a transition func-
tion and pulling back the connection 1-form using the local sections. So
there is some kind of relation between gauge transformations and local sec-
tions that we want to explore now. We note that all terms of Eq. (35.1) are
evaluated at the same x &isin;M . Therefore, that gauge transformation does not
leave the fiber of the bundle. This is the condition that we want to impose
on any gauge transformation:gauge transformation
</p>
<p>defined
</p>
<p>Definition 35.1.6 A gauge transformation of a principal fiber bun-
dle P(M,G) is an automorphism (f, idG) of the bundle for which
fM = idM , i.e., π(p) = π(f (p)). The set of all gauge transforma-
tions of P(M,G) is denoted by Gau(P ).
</p>
<p>Thus, a gauge transformation does not leave a fiber. We also know that
the right action of the structure group is also confined to a fiber. So the
natural question to ask is &ldquo;Is the right action a gauge transformation?&rdquo; Re-
call (see Definition 34.1.3) that an automorphism satisfies f (pg)= f (p)g.
However, Rh(pg)= pgh �=Rh(p)g = phg.
</p>
<p>As mentioned above, there is a close relationship between cross sectionsThe set of sections of E
can be identified as the
</p>
<p>set of gauge
</p>
<p>transformations.
</p>
<p>and gauge transformations. Let S(E,M,F) be the set of all sections of the
associated bundle E with base manifold M and standard fiber F .1 Consider
S(E,M,G) in which G acts on itself via the adjoint transformation: g &middot; h=
ghg&minus;1.
</p>
<p>Theorem 35.1.7 There is a natural bijection S(E,M,G)&sim;=Gau(P ).
</p>
<p>1Note that particle fields are members of S(E,M,V).</p>
<p/>
</div>
<div class="page"><p/>
<p>35.1 Gauge Potentials and Fields 1103
</p>
<p>Proof For σ &isin; S(E,M,G) define f : P &rarr; P by f (p) = pπ2 ◦ σ ◦ π(p),
where π2 projects onto the second factor of E = P &times;G G. The composite
π2 ◦ σ ◦ π assigns to each p its partner in ❏p,h❑. If the partner of p is h,
then the partner of p&prime; = pg is h&prime; = g&minus;1hg.
</p>
<p>Suppose that f (p)= ph, so that π2 ◦ σ ◦ π(p)= h, then
</p>
<p>f (pg)= pgπ2 ◦ σ ◦ π(pg)= pgh&prime; = pgg&minus;1hg = phg = f (p)g.
</p>
<p>Hence, f &isin; Gau(P ). Conversely, suppose that f &isin; Gau(P ), i.e., that
f (pg) = f (p)g. Define σ by f (p) = pπ2 ◦ σ ◦ π(p), and note that, on
the one hand, f (pg)= pgπ2 ◦ σ ◦ π(pg), and on the other hand f (pg)=
pπ2 ◦ σ ◦ π(p)g. Therefore
</p>
<p>gπ2◦σ ◦π(pg)= π2◦σ ◦π(p)g or π2◦σ ◦π(pg)= g&minus;1π2◦σ ◦π(p)g,
</p>
<p>which is a necessary condition for σ to be well defined: σ(x) could be ❏p,h❑
or ❏p&prime;, h&prime;❑ where p&prime; = pg and h&prime; = g&minus;1hg for some g &isin;G. The preceding
equation restates this relation. �
</p>
<p>Remark 35.1.1 In the proof of Theorem 35.1.7, we introduced the com-
posite map π2 ◦ σ ◦ π , which mapped the first factor of σ(x) = ❏p,h❑ to
its second factor. This is the same construction as the one discussed af-
ter Example 34.3.2, where π2 is denoted by p&minus;1. Sometimes it is more
convenient to work with this map rather than the section σ . So, if F is
a manifold on which G acts on the left, we define a map π12 : P &rarr; F π12 andΠ12(P,F )
</p>
<p>definedwith the property π12(pg) = g&minus;1 &middot; π12(p). This property brings the set
of sections in a bijective correspondence with the set Π12(P,F ) of such
maps: S(E,M,F) &sim;= Π12(P,F ). When F = G, we get the triple isomor-
phism
</p>
<p>S(E,M,G)&sim;=Π12(P,G)&sim;=Gau(P ).
</p>
<p>In particular, any f &isin;Gau(P ) can be written as f (p)= pπ12(p) for some
π12 &isin;Π12(P,G).
</p>
<p>A gauge transformation transforms the connection 1-form as well. How
does the connection 1-form change under a gauge transformation?
</p>
<p>Theorem 35.1.8 If f &isin;Gau(P ) and ωωω is a connection 1-form, then
f &lowast;ωωω is also a connection 1-form.
</p>
<p>Proof We show that f &lowast;ωωω satisfies condition (a) of Definition 34.2.1 and
leave the proof of condition (b) for the reader. Let A &isin; g and let A&lowast; be its
fundamental vector field in P . Then,</p>
<p/>
</div>
<div class="page"><p/>
<p>1104 35 Gauge Theories
</p>
<p>(
f &lowast;ωωω
</p>
<p>)
A&lowast;p =ωωω
</p>
<p>(
f&lowast;A&lowast;p
</p>
<p>)
=ωωω
</p>
<p>(
d
</p>
<p>dt
f
(
pγA(t)
</p>
<p>)∣∣∣∣
t=0
</p>
<p>)
</p>
<p>=ωωω
(
d
</p>
<p>dt
f (p)γA(t)
</p>
<p>∣∣∣∣
t=0
</p>
<p>)
=ωωω
</p>
<p>(
f&lowast;A&lowast;f (p)
</p>
<p>)
=A,
</p>
<p>where to go to the second line, we used f (pg)= f (p)g. �
</p>
<p>Gauge transformations not only map connections to connections, but they
also map tensorial forms to tensorial forms.
</p>
<p>Proposition 35.1.9 If φφφ &isin; Λ̄k(P,V) and f &isin; Gau(P ), then f &lowast;φφφ &isin;
Λ̄k(P,V).
</p>
<p>Proof Using the fact that f ◦ Rg = Rg ◦ f , one can show easily that
R&lowast;g(f
</p>
<p>&lowast;φφφ) = ρ(g&minus;1) &middot; (f &lowast;φφφ). That φφφ is horizontal follows from the equal-
ity f&lowast;A&lowast; =A&lowast; derived in the proof of Theorem 35.1.8. �
</p>
<p>Theorem 35.1.8 and Proposition 35.1.9 tell us that gauge transformations
send connection 1-forms to connection 1-forms and tensorial forms to ten-
sorial forms. With the help of Remark 35.1.1, we can find explicit formulas
for the gauge-transformed forms. Both formulas can be derived once we
know how gauge transformations transform vector fields on P .
</p>
<p>Lemma 35.1.10 Let f &isin; Gau(P ) and π12 &isin; Π12(P,G) be related by
f (p)= pπ12(p). For X &isin; TpP , we have
</p>
<p>f&lowast;(X)=
[
L&minus;1π12(p)&lowast;π12&lowast;(X)
</p>
<p>]&lowast;
f (p)
</p>
<p>+Rπ12(p)&lowast;(X), (35.3)
</p>
<p>where [B]&lowast;q denotes the fundamental vector field of B &isin; g at q &isin; P .
</p>
<p>gauge transformation of
</p>
<p>vector fields
</p>
<p>Proof Let γX(t) be the integral curve of X passing through p (i.e., γX(0)=
p). Then
</p>
<p>f&lowast;(X)=
d
</p>
<p>dt
f
(
γX(t)
</p>
<p>)∣∣∣∣
t=0
</p>
<p>= d
dt
</p>
<p>[
γX(t)π12
</p>
<p>(
γX(t)
</p>
<p>)]∣∣∣∣
t=0
</p>
<p>= d
dt
</p>
<p>[
pπ12
</p>
<p>(
γX(t)
</p>
<p>)]∣∣∣∣
t=0
</p>
<p>+ d
dt
</p>
<p>[
γX(t)π12(p)︸ ︷︷ ︸
=Rπ12(p)γX(t)
</p>
<p>]
∣∣∣∣∣
t=0
</p>
<p>.
</p>
<p>The rest of the proof follows from an identical argument leading to
Eq. (34.5). We leave the details as an exercise for the reader. �
</p>
<p>Using this lemma, we can instantly prove the following theorem:
</p>
<p>Theorem 35.1.11 If f &isin; Gau(P ) and π12 &isin; Π12(P,G) are related by
f (p)= pπ12(p), ωωω is a connection, and φφφ &isin; Λ̄k(P,V), then
</p>
<p>f &lowast;(ωωω)p = L&minus;1π12(p)&lowast;π12&lowast; +Adπ12(p)&minus;1ωωωp and
(
f &lowast;φφφ
</p>
<p>)
p
= π&minus;112 &middot;φφφ.</p>
<p/>
</div>
<div class="page"><p/>
<p>35.2 Gauge-Invariant Lagrangians 1105
</p>
<p>Proof For the first equation, apply ωωω to both sides of the equation of
Lemma 35.1.10. For the second equation, recall that (f &lowast;φφφ)(X1, . . . ,Xk)=
φφφ(f&lowast;X1, . . . , f&lowast;Xk). Now use the Lemma for each term in the argument,
note that φφφ annihilates the first term on the right-hand side of Eq. (35.3), and
use the defining property of a tensorial form as given in Definition 34.3.1. �
</p>
<p>35.2 Gauge-Invariant Lagrangians
</p>
<p>We have already encountered Lagrangians defined on Rn, and have seen
consequences of their symmetry, i.e., conservation laws. In this section, we
want to formulate them on manifolds and impose gauge symmetry on them.
</p>
<p>Definition 35.2.1 A Lagrangian is a map L : P &times;V&times; T (P )&rarr;R satisfy- Lagrangian defined
ing
</p>
<p>L
(
pg,g&minus;1 &middot; v,g&minus;1 &middot; θθθp ◦Rg&minus;1&lowast;
</p>
<p>)
= L(p,v,θθθp),
</p>
<p>for all p &isin; P , v &isin; V, θθθp : Tp(P )&rarr; V, and g &isin;G.
</p>
<p>A Lagrangian whose codomain by definition is R is usually the integral
of a Lagrangian density, which is a function on M . In fact, the condition im-
posed on the Lagrangian in its definition was to ensure that the Lagrangian
density is well-defined. The following proposition connects the two.
</p>
<p>Proposition 35.2.2 Given a LagrangianL, there is a mapL0 :Π12(P,V)&rarr;
C&infin;(M) defined by L0(ψψψ)(x) = L(p,ψψψ(p), dψψψp) for x &isin; M , p &isin; P with
π(p)= x, and ψψψ &isin;Π12(P,V).
</p>
<p>Proof Write ψψψ(pg) = g&minus;1 &middot; ψψψ(p) (the property that ψψψ has to obey
by Remark 35.1.1) as ψψψ ◦ Rg = g&minus;1 &middot; ψψψ with the differential dψψψpg ◦
Rg&lowast; &equiv; ψψψpg&lowast; ◦ Rg&lowast; = g&minus;1 &middot; dψψψp . Now show that L(pg,ψψψ(pg), dψψψpg) =
L(p,ψψψ(p), dψψψp), i.e., all points of the fiber π&minus;1(x) give the same La-
grangian. �
</p>
<p>A gauge transformation f takes p &isin; P to f (p) &isin; P without leaving the
fiber in which p is located. This means that there must exist a g &isin;G such
that f (p)= pg. Now, we are interested in Lagrangians which are invariant
under such a transformation. That is, we want our Lagrangians to obey
</p>
<p>L
(
f (p), v,θθθf (p)
</p>
<p>)
= L(p,v,θθθp) or
</p>
<p>L(pg,v,θθθpg)= L(p,v,θθθp) for g &isin;G.
</p>
<p>Using the defining property of L, we can rewrite the second equation as
</p>
<p>L
(
pgg&minus;1, g &middot; v,g &middot; θθθpg ◦Rg&lowast;
</p>
<p>)
= L(p,v,θθθp).
</p>
<p>Noting that θθθpg = θθθp ◦Rg&minus;1&lowast;, we obtain
</p>
<p>L(p,g &middot; v,g &middot; θθθp)= L(p,v,θθθp). (35.4)</p>
<p/>
</div>
<div class="page"><p/>
<p>1106 35 Gauge Theories
</p>
<p>We say a Lagrangian is G-invariant if it satisfies this equation. We do notG-invariant Lagrangian
use &ldquo;gauge-invariant&rdquo; because Eq. (35.4) does not use f directly. However,
noting that Π12(P,V) = Λ̄0(P,V) and the fact that f acts on Λ̄k(P,V)
via pull-back, we can investigate the gauge-invariance of the Lagrangian
density, i.e., we want to see if L0 of Proposition 35.2.2 is gauge invariant,
i.e., if L0(ψψψ)=L0(f &lowast;ψψψ) or
</p>
<p>L
(
p,ψψψ(p), dψψψp
</p>
<p>) ?= L
(
p,
</p>
<p>(
f &lowast;ψψψ
</p>
<p>)
(p),
</p>
<p>(
f &lowast;dψψψ
</p>
<p>)
p
</p>
<p>)
</p>
<p>= L
(
p,
</p>
<p>(
f &lowast;ψψψ
</p>
<p>)
(p), d
</p>
<p>(
f &lowast;ψψψ
</p>
<p>)
p
</p>
<p>)
.
</p>
<p>If f &isin; Gau(P ) and π12 &isin; Π12(P,G) are related by f (p) = pπ12(p),
then by Theorem 35.1.11, we have f &lowast;ψψψ = π&minus;112 &middot;ψψψ because Π12(P,V) =
Λ̄0(P,V). We now compute d(π&minus;112 &middot;ψψψ). Let γX(t) be the flow of X. Then
</p>
<p>d
(
π&minus;112 &middot;ψψψ
</p>
<p>)
(X)= d
</p>
<p>dt
π&minus;112
</p>
<p>(
γX(t)
</p>
<p>)
&middot;ψψψ
</p>
<p>(
γX(t)
</p>
<p>)∣∣∣∣
t=0
</p>
<p>= d
dt
</p>
<p>π&minus;112 (p) &middot;ψψψ
(
γX(t)
</p>
<p>)∣∣∣∣
t=0
</p>
<p>+ d
dt
</p>
<p>π&minus;112
(
γX(t)
</p>
<p>)
&middot;ψψψ(p)
</p>
<p>∣∣∣∣
t=0
</p>
<p>= π&minus;112 (p) &middot;ψψψ&lowast;p(X)+ π&minus;112&lowast;(X) &middot;ψψψ(p).
</p>
<p>Therefore,
</p>
<p>L0
(
f &lowast;ψψψ
</p>
<p>)
(x)= L
</p>
<p>(
p,
</p>
<p>(
f &lowast;ψψψ
</p>
<p>)
(p), d
</p>
<p>(
f &lowast;ψψψ
</p>
<p>)
p
</p>
<p>)
</p>
<p>= L
(
p,π&minus;112 (p) &middot;ψψψ(p),π&minus;112 (p) &middot; dψψψp + π&minus;112&lowast; &middot;ψψψ(p)
</p>
<p>)
.
</p>
<p>Were it not for the extra term in the third argument of L, we would have
gauge invariance. However, the presence of π&minus;112&lowast; &middot; ψψψ(p) makes the La-
grangian density, as defined in Proposition 35.2.2, not gauge-invariant.
</p>
<p>The very notion of gauge transformation involves a connection 1-form.
Yet nowhere in the definition of the Lagrangian or Lagrangian density did
we make use of a connection. So, it should come as no surprise to see the
failure of invariance of L0 under gauge transformations. Since it is the dif-
ferential term dψψψ of L that causes the violation of the invariance, and since
we do have a differentiation which naturally incorporates the connection
1-form, it is natural to replace dψψψ with Dωψψψ . So, now define a new density,recipe for gauge
</p>
<p>invariant Lagrangians
</p>
<p>L(ψψψ,ωωω)(x)= L
(
p,ψψψ(p),Dωψψψp
</p>
<p>)
, (35.5)
</p>
<p>for x &isin;M , p &isin; P with π(p)= x, and ψψψ &isin;Π12(P,V).
</p>
<p>Theorem 35.2.3 If L in Eq. (35.5) is G-invariant, then L is well-
defined and L(f &lowast;ψψψ,f &lowast;ωωω)=L(ψψψ,ωωω), i.e., L is gauge-invariant.</p>
<p/>
</div>
<div class="page"><p/>
<p>35.3 Construction of Gauge-Invariant Lagrangians 1107
</p>
<p>Proof From Proposition 34.3.4, we have R&lowast;gD
ωψψψpg = g&minus;1 &middot; Dωψψψp . This
</p>
<p>can be rewritten as
</p>
<p>Dωψψψpg ◦Rg&lowast; = g&minus;1 &middot;Dωψψψp or Dωψψψpg = g&minus;1 &middot;Dωψψψp ◦Rg&minus;1&lowast;.
</p>
<p>Then,
</p>
<p>L
(
pg,ψψψ(pg),Dωψψψpg
</p>
<p>)
= L
</p>
<p>(
pg,g&minus;1 &middot;ψψψ(p), g&minus;1 &middot;Dωψψψp ◦Rg&minus;1&lowast;
</p>
<p>)
</p>
<p>= L
(
p,ψψψ(p),Dωψψψp
</p>
<p>)
,
</p>
<p>where in the second equality, we used the defining property of a Lagrangian
in Definition 35.2.1. So, L is well-defined. Now from Theorem 35.1.4 and
Eq. (35.2) with j = 1 and k = 0, we have Dωψψψ = dψψψ +ωωω &middot;ψψψ . Using this
result, we obtain
</p>
<p>L
(
f &lowast;ψψψ,f &lowast;ωωω
</p>
<p>)
(x)= L
</p>
<p>(
p,
</p>
<p>(
f &lowast;ψψψ
</p>
<p>)
(p), d
</p>
<p>(
f &lowast;ψψψ
</p>
<p>)
p
+ f &lowast;ωωωp &middot;
</p>
<p>(
f &lowast;ψψψ
</p>
<p>)
(p)
</p>
<p>)
</p>
<p>= L
(
p,
</p>
<p>(
f &lowast;ψψψ
</p>
<p>)
(p), f &lowast;
</p>
<p>(
dψψψp +ωωωp &middot;ψψψ(p)
</p>
<p>))
</p>
<p>= L
(
p,π12(p)
</p>
<p>&minus;1 &middot;ψψψ(p),f &lowast;
(
Dωψψψp
</p>
<p>))
.
</p>
<p>Then the second equation of Theorem 35.1.11 and the fact that L is
G-invariant yield
</p>
<p>L
(
f &lowast;ψψψ,f &lowast;ωωω
</p>
<p>)
(x)= L
</p>
<p>(
p,π12(p)
</p>
<p>&minus;1 &middot;ψψψ(p),π12(p)&minus;1 &middot;Dωψψψp
)
</p>
<p>= L
(
p,ψψψ(p),Dωψψψp
</p>
<p>)
=L(ψψψ,ωωω)(x),
</p>
<p>showing that L is indeed gauge-invariant. �
</p>
<p>35.3 Construction of Gauge-Invariant Lagrangians
</p>
<p>We have defined Lagrangians in terms of ψψψ &isin; Λ̄0(P,V) and Dωψψψ &isin;
Λ̄1(P,V). Since G acts on V via a representation, G&rarr; GL(V), the most
natural invariants would be in the form of inner products in V and the re-
striction of GL(V) to its unitary subgroup U(V), i.e., we assume that the
representation is G&rarr;U(V), in which case we say that the inner product is
G-orthogonal. We are thus led to defining G-orthogonal inner products on
ψψψ &isin; Λ̄k(P,V).
</p>
<p>Let h be a metric in M . Let π&lowast; be the pull-back of π : P &rarr; M . Then
π&lowast;h is a bilinear form on P . However, it is not an inner product because
</p>
<p>G-orthogonal inner
</p>
<p>product
</p>
<p>π&lowast;h(X,Y)= h(π&lowast;X,π&lowast;Y) vanishes for all Y if X is a nonzero vertical vec-
tor field. Nevertheless, π&lowast;h does become an inner product if the vectors are
confined to the horizontal subspace Hp . Since the forms in Λ̄k(P,V) are
horizontal, they could be thought of as forms in Hp . In particular, Λ̄k(P,R)
could be identified with the regular k-forms Λk(M) as explained in Re-
mark 34.3.1. This means that the inner product h̃ defined on Λk(M) as
in Eq. (26.52) could be lifted up to a similar inner product on Λ̄k(P,R),</p>
<p/>
</div>
<div class="page"><p/>
<p>1108 35 Gauge Theories
</p>
<p>which we denote by h̄. The inner product on Λ̄k(P,V) is then ˜̄hĥ as de-
fined in Eq. 26.53. Specifically, if ψψψ,φφφ &isin; Λ̄k(P,V), {ei}mi=1 is a basis of V,
ψψψ =&sum;mi=1 ψ iei , φφφ =
</p>
<p>&sum;m
j=1 φ
</p>
<p>j ej , and ĥij = ĥ(ei, ej ), then
</p>
<p>˜̄
hĥ(ψψψ,φφφ)=
</p>
<p>m&sum;
</p>
<p>i,j=1
ĥij h̄
</p>
<p>(
ψ i, φj
</p>
<p>)
, (35.6)
</p>
<p>where ψ i, φj &isin; Λ̄k(P,R).
All the discussion above can be summarized as
</p>
<p>Theorem 35.3.1 The functions
</p>
<p>h̃ĥ :Λk(M,V)&times;Λk(M,V)&rarr; C&infin;(M)
˜̄
hĥ : Λ̄k(P,V)&times; Λ̄k(P,V)&rarr; C&infin;(M),
</p>
<p>where the second one is given by Eq. (35.6) and the first by a similar expres-
sion, are well-defined. Furthermore, if ψψψ,φφφ &isin; Λ̄k(P,V) and σ : U &rarr; P is
a local section, then
</p>
<p>h̃ĥ
(
σ &lowast;ψψψ,σ &lowast;φφφ
</p>
<p>)
= ˜̄hĥ(ψψψ,φφφ).
</p>
<p>The last statement is a result of the fact that h̄&asymp; π&lowast;h̃ and π&lowast;σ&lowast; = id.
With an inner product placed on Λ̄k(P,V), we can define the Hodge star
</p>
<p>operator and a codifferential.
</p>
<p>Definition 35.3.2 The covariant codifferential δω : Λ̄k(P,V) &rarr;covariant codifferential
Λ̄k&minus;1(P,V) of φφφ &isin; Λ̄k(P,V) is defined by
</p>
<p>δωφφφ = (&minus;1)ν+1(&minus;1)n(k+1)&lowast;̄Dω(&lowast;̄φφφ),
</p>
<p>where ν is the index of h, n= dimM , and &lowast;̄ = π&lowast;(&lowast;), with &lowast; the star oper-
ator on M .
</p>
<p>Theorem 28.6.6 has an exact analog:
&int;
</p>
<p>M
</p>
<p>˜̄
hĥ
</p>
<p>(
Dωψψψ,φφφ
</p>
<p>)
μμμ=
</p>
<p>&int;
</p>
<p>M
</p>
<p>˜̄
hĥ
</p>
<p>(
ψψψ,δωφφφ
</p>
<p>)
μμμ, (35.7)
</p>
<p>where ψψψ &isin; Λ̄k(P,V) and φφφ &isin; Λ̄k+1(P,V).
Now that we have inner products for various elements of the Lagrangian,
</p>
<p>we can write a G-invariant Lagrangian as an inner product. The simplest
L(p,v,θθθ) which is G-invariant is
</p>
<p>L(p,v,θθθ)= ˜̄hĥ(θθθh,θθθh)&minus;m2ĥ(v, v)
</p>
<p>where the subscript h means the horizontal component, and the constant m2
</p>
<p>a simple Lagrangian
</p>
<p>density for particle fields
</p>
<p>is introduced to match the dimensions of the two terms. This leads to the
gauge-invariant Lagrangian density
</p>
<p>L(ψψψ,ωωω)= ˜̄hĥ
(
Dωψψψ,Dωψψψ
</p>
<p>)
&minus;m2ĥ(ψψψ,ψψψ). (35.8)</p>
<p/>
</div>
<div class="page"><p/>
<p>35.3 Construction of Gauge-Invariant Lagrangians 1109
</p>
<p>We have to add to this a Lagrangian density associated with the connec-
tion itself. So, we might try something similar to the preceding expression.
However, while the first term is allowed, the second term, the mass term,
cannot be present becauseωωω is not a tensorial form. Furthermore, it does not
have a horizontal component in the sense of (a) of Proposition 34.3.4. That
is why
</p>
<p>Box 35.3.3 A gauge-invariant Lagrangian cannot contain a mass
term for the gauge potential.
</p>
<p>If hG is a metric on G, then the gauge Lagrangian density can be ex- gauge Lagrangian
densitypressed as
</p>
<p>LG(ωωω)=&minus;
1
</p>
<p>2
˜̄hhG
</p>
<p>(
Dωωωω,Dωωωω
</p>
<p>)
=&minus;1
</p>
<p>2
˜̄hhG
</p>
<p>(
���ω,���ω
</p>
<p>)
, (35.9)
</p>
<p>where the minus sign and the factor of 12 make the equations of motion
consistent with observation. The total Lagrangian is just the sum of (35.8)
and (35.9):
</p>
<p>Ltot(ψψψ,ωωω)= ˜̄hĥ
(
Dωψψψ,Dωψψψ
</p>
<p>)
&minus;m2ĥ(ψψψ,ψψψ)&minus; 1
</p>
<p>2
˜̄hhG
</p>
<p>(
���ω,���ω
</p>
<p>)
.
</p>
<p>(35.10)
From the Lagrangian density we can obtain the Lagrange&rsquo;s equation by
</p>
<p>variational method. We use Eq. (33.4) to find the variational derivative. The
particle field and connection are stationary if at t = 0
</p>
<p>d
</p>
<p>dt
</p>
<p>&int;
</p>
<p>M
</p>
<p>L(ψψψ + tηηη,ωωω+ tξξξ)μμμ+ d
dt
</p>
<p>&int;
</p>
<p>M
</p>
<p>LG(ωωω+ tξξξ)μμμ= 0, (35.11)
</p>
<p>where ηηη &isin; Λ̄0(P,V) and ξξξ &isin;Λ1(P,g). The first term gives
</p>
<p>d
</p>
<p>dt
</p>
<p>&int;
</p>
<p>M
</p>
<p>L(ψψψ + tηηη,ωωω)μμμ+ d
dt
</p>
<p>&int;
</p>
<p>M
</p>
<p>L(ψψψ,ωωω+ tξξξ)μμμ, (35.12)
</p>
<p>whose first term can be written as
</p>
<p>d
</p>
<p>dt
</p>
<p>&int;
</p>
<p>M
</p>
<p>L
(
p,ψψψ + tηηη,Dω(ψψψ + tηηη)
</p>
<p>)
μμμ
</p>
<p>= d
dt
</p>
<p>&int;
</p>
<p>M
</p>
<p>L
(
p,ψψψ + tηηη,Dω(ψψψ)
</p>
<p>)
μμμ
</p>
<p>+ d
dt
</p>
<p>&int;
</p>
<p>M
</p>
<p>L
(
p,ψψψ,Dω(ψψψ)+ tDω(ηηη)
</p>
<p>)
μμμ
</p>
<p>=
&int;
</p>
<p>M
</p>
<p>ĥ
(
&part;2L
</p>
<p>(
p,ψψψ,Dω(ψψψ)
</p>
<p>)
,ηηη
</p>
<p>)
μμμ
</p>
<p>+
&int;
</p>
<p>M
</p>
<p>˜̄
hĥ
</p>
<p>(
&part;3L
</p>
<p>(
p,ψψψ,Dω(ψψψ)
</p>
<p>)
,Dω(ηηη)
</p>
<p>)
μμμ.</p>
<p/>
</div>
<div class="page"><p/>
<p>1110 35 Gauge Theories
</p>
<p>Denote &part;2L(p,ψψψ,Dω(ψψψ)) by &part;L/&part;ψψψ and &part;3L(p,ψψψ,Dω(ψψψ)) by &part;L/
&part;(Dωψψψ) and use Eq. (35.7) in the second term to obtain
</p>
<p>d
</p>
<p>dt
</p>
<p>&int;
</p>
<p>M
</p>
<p>L
(
p,ψψψ + tηηη,Dω(ψψψ + tηηη)
</p>
<p>)
μμμ
</p>
<p>=
&int;
</p>
<p>M
</p>
<p>ĥ
</p>
<p>(
&part;L
</p>
<p>&part;ψψψ
,ηηη
</p>
<p>)
μμμ+
</p>
<p>&int;
</p>
<p>M
</p>
<p>ĥ
</p>
<p>(
δω
</p>
<p>&part;L
</p>
<p>&part;(Dωψψψ)
,ηηη
</p>
<p>)
μμμ,
</p>
<p>or
</p>
<p>d
</p>
<p>dt
</p>
<p>&int;
</p>
<p>M
</p>
<p>L
(
p,ψψψ + tηηη,Dω(ψψψ + tηηη)
</p>
<p>)
μμμ=
</p>
<p>&int;
</p>
<p>M
</p>
<p>ĥ
</p>
<p>(
δω
</p>
<p>&part;L
</p>
<p>&part;(Dωψψψ)
+ &part;L
</p>
<p>&part;ψψψ
,ηηη
</p>
<p>)
μμμ.
</p>
<p>(35.13)
Note that it can actually be shown that &part;L/&part;ψψψ &isin; Λ̄0(P,V) and &part;L/
&part;(Dωψψψ) &isin; Λ̄1(P,V). However, the variational calculations above paired
these forms with forms of the correct degree. Note also that δω(&part;L/
&part;(Dωψψψ)) &isin; Λ̄0(P,V), and it pairs up in ĥ.
</p>
<p>To calculate the second term of Eq. (35.12), we note from Theo-
rem 35.1.4 that
</p>
<p>Dω+tξ (ψψψ)= dψψψ +ωωω &middot;ψψψ + tξξξ &middot;ψψψ =Dω(ψψψ)+ tξξξ &middot;ψψψ.
</p>
<p>Therefore, (at t = 0), we havecurrent (or current
density) defined
</p>
<p>d
</p>
<p>dt
L(ψψψ,ωωω+ tξξξ)= d
</p>
<p>dt
L
(
p,ψψψ(p),Dω(ψψψ)+ tξξξ &middot;ψψψ
</p>
<p>)
</p>
<p>= ˜̄hĥ
(
&part;3L
</p>
<p>(
p,ψψψ(p),Dωψψψp
</p>
<p>)
,ξξξ &middot;ψψψ
</p>
<p>)
</p>
<p>= ˜̄hĥ
(
</p>
<p>&part;L
</p>
<p>&part;(Dωψψψ)
,ξξξ &middot;ψψψ
</p>
<p>)
&equiv; ˜̄hhG
</p>
<p>(
Jω,ξξξ
</p>
<p>)
, (35.14)
</p>
<p>where the last line defines the current density Jω &isin; Λ̄1(P,g).
For the second term of Eq. (35.11), we assume a Lagrangian as given by
</p>
<p>Eq. (35.9) and write ���t &equiv;���ωωω+tξξξ . Then (at t = 0)
</p>
<p>d
</p>
<p>dt
���t =
</p>
<p>d
</p>
<p>dt
</p>
<p>(
d(ωωω+ tξξξ)+ 1
</p>
<p>2
[ωωω+ tξξξ,ωωω+ tξξξ ]
</p>
<p>)
= dξξξ + [ωωω,ξξξ ] =Dωξξξ .
</p>
<p>Hence, (at t = 0)
</p>
<p>d
</p>
<p>dt
LG(ωωω+ tξξξ)=&minus;
</p>
<p>1
</p>
<p>2
</p>
<p>d
</p>
<p>dt
</p>
<p>˜̄hhG(���t ,���t )
</p>
<p>=&minus;1
2
</p>
<p>d
</p>
<p>dt
</p>
<p>˜̄hhG
(
���ω,���t
</p>
<p>)
&minus; 1
</p>
<p>2
</p>
<p>d
</p>
<p>dt
</p>
<p>˜̄hhG
(
���t ,���
</p>
<p>ω
)
</p>
<p>=&minus;1
2
˜̄hhG
</p>
<p>(
���ω,Dωξξξ
</p>
<p>)
&minus; 1
</p>
<p>2
˜̄hhG
</p>
<p>(
Dωξξξ,���ω
</p>
<p>)
</p>
<p>=&minus;˜̄hhG
(
���ω,Dωξξξ
</p>
<p>)
. (35.15)</p>
<p/>
</div>
<div class="page"><p/>
<p>35.3 Construction of Gauge-Invariant Lagrangians 1111
</p>
<p>We now substitute (35.13), (35.14), and (35.15) in Eq. (35.11) to obtain
&int;
</p>
<p>M
</p>
<p>ĥ
</p>
<p>(
δω
</p>
<p>&part;L
</p>
<p>&part;(Dωψψψ)
+ &part;L
</p>
<p>&part;ψψψ
,ηηη
</p>
<p>)
μμμ+
</p>
<p>&int;
</p>
<p>M
</p>
<p>˜̄hhG(J,ξξξ)μμμ
</p>
<p>&minus;
&int;
</p>
<p>M
</p>
<p>˜̄hhG
(
���ω,Dωξξξ
</p>
<p>)
μμμ= 0.
</p>
<p>Using Eq. (35.7) on the last term, we remove the covariant differentiation
from ξξξ . Then noting that ηηη and ξξξ are arbitrary and independent of each other,
we obtain
</p>
<p>Theorem 35.3.4 The particle fieldψψψ and the connection 1-form ωωω are sta-
tionary relative to the total Lagrangian iff they satisfy the following two
Lagrange&rsquo;s equations:
</p>
<p>(a) δω
&part;L
</p>
<p>&part;(Dωψψψ)
+ &part;L
</p>
<p>&part;ψψψ
= 0,
</p>
<p>(b) δω���ω = Jω(ψψψ).
</p>
<p>Let us examine the current Jω in some more detail. It is common to sup-
</p>
<p>Lagrange&rsquo;s equations
</p>
<p>press the superscript ω, although the current depends on the connection. The
fact that the current pairs up with ξξξ &isin;Λ1(P,g) tells us that J &isin;Λ1(P,g).
Denoting &part;L/&part;(Dωψψψ) by φφφ and letting {eα} and {ei} be bases for g and V,
respectively, the last line of Eq. (35.14) can be expressed as
</p>
<p>˜̄
hĥ
</p>
<p>(
φiei, ξβeβ &middot;ψψψ
</p>
<p>)
= ˜̄hhG
</p>
<p>(
J αeα, ξ
</p>
<p>βeβ
)
,
</p>
<p>where summation over repeated indices is understood and J α , φi , and ξβ
</p>
<p>are real-valued forms. From the definition of the composite inner product,
we get
</p>
<p>h̄
(
φi ĥ(ei, eβ &middot;ψψψ), ξβ
</p>
<p>)
= h̄
</p>
<p>(
J αhG(eα, eβ), ξ
</p>
<p>β
)
&equiv; hGαβ h̄
</p>
<p>(
J α, ξβ
</p>
<p>)
,
</p>
<p>or
</p>
<p>h
αβ
G h̄
</p>
<p>(
φi ĥ(ei, eβ &middot;ψψψ), ξβ
</p>
<p>)
= h̄
</p>
<p>(
J α, ξβ
</p>
<p>)
.
</p>
<p>Since this must hold for any ξβ , we get
</p>
<p>h
αβ
G φ
</p>
<p>i ĥ(ei, eβ &middot;ψψψ)= J α, or J α = hαβG ĥ(φφφ, eβ &middot;ψψψ) and
</p>
<p>J = hαβG ĥ(φφφ, eβ &middot;ψψψ) eα,
</p>
<p>with J = J αeα . Substituting for φφφ, we finally obtain formula for current
density
</p>
<p>J α = hαβG ĥ
(
</p>
<p>&part;L
</p>
<p>&part;(Dωψψψ)
, eβ &middot;ψψψ
</p>
<p>)
and J = hαβG ĥ
</p>
<p>(
&part;L
</p>
<p>&part;(Dωψψψ)
, eβ &middot;ψψψ
</p>
<p>)
eα.
</p>
<p>(35.16)
For the Lagrangian of Eq. (35.10), this simplifies to
</p>
<p>J α = 2hαβG ĥ
(
Dωψψψ, eβ &middot;ψψψ
</p>
<p>)
and J = 2hαβG ĥ
</p>
<p>(
Dωψψψ, eβ &middot;ψψψ
</p>
<p>)
eα. (35.17)</p>
<p/>
</div>
<div class="page"><p/>
<p>1112 35 Gauge Theories
</p>
<p>35.4 Local Equations
</p>
<p>In physical applications, potentials and fields are defined on the Minkowski
manifold M = (R4,ηηη). Therefore, for our formulas to be useful, we need to
pull our equations down to the base manifold M . This is achieved by local
sections. So, let σu : U &rarr; P be such a section, and use Theorem 35.3.1 to
pullback functions and forms from P to M . Then inner products will be
defined on M rather than P . In fact, using the equation of Theorem 35.3.1,
we can write the Lagrangian of Eq. (35.10) as
</p>
<p>Ltot(ψψψ,ωωω)= h̃ĥ
(
σ &lowast;uD
</p>
<p>ωψψψ,σ &lowast;uD
ωψψψ
</p>
<p>)
&minus;m2ĥ
</p>
<p>(
σ &lowast;uψψψ,σ
</p>
<p>&lowast;
uψψψ
</p>
<p>)
</p>
<p>&minus; 1
2
h̃hG
</p>
<p>(
σ &lowast;u���
</p>
<p>ω, σ &lowast;u���
ω
)
.
</p>
<p>As shown in Chap. 34, the result is to substitute local expressions for all
quantities. For example, it can be shown easily that σ &lowast;uD
</p>
<p>ωψψψ = Dωuψψψu
whereωωωu is a g-valued 1-form andψψψu a V-valued function on M . Removing
the subscript u, we write the Lagrangian of Eq. (35.10) as
</p>
<p>Ltot(ψψψ,ωωω)= h̃ĥ
(
Dωψψψ,Dωψψψ
</p>
<p>)
&minus;m2ĥ(ψψψ,ψψψ)&minus; 1
</p>
<p>2
h̃hG
</p>
<p>(
���ω,���ω
</p>
<p>)
, (35.18)
</p>
<p>where all quantities are now defined on M . Let {&part;μ} be a coordinate basis
of M , {er} a basis of V, and {ei} a basis of g. Then the Lagrangian can be
expressed in terms of components:
</p>
<p>Ltot(ψψψ,ωωω)= ĥrs h̃
((
Dωψ
</p>
<p>)r
,
(
Dωψ
</p>
<p>)s)&minus;m2ĥrs
(
ψ r ,ψ s
</p>
<p>)
</p>
<p>&minus; 1
2
hGij h̃
</p>
<p>(
Ω i,Ωj
</p>
<p>)
, (35.19)
</p>
<p>where we have used Eq. (26.53) and the notation h̃ is as in Eq. (26.52).
Finally, with hμν = h(&part;μ, &part;ν) and hμν the inverse matrix of hμν , and using
(26.52), we get
</p>
<p>Ltot(ψψψ,ωωω)= ĥrshμν
(
Dωψ
</p>
<p>)r
μ
</p>
<p>(
Dωψ
</p>
<p>)s
ν
</p>
<p>&minus;m2ĥrsψ rψ s &minus;
1
</p>
<p>4
hGijh
</p>
<p>μαhνβΩ iμνΩ
j
αβ .
</p>
<p>It is more common to use g for the metric of M . So, switch h to g and use
h for the metric of the Lie group G. Then
</p>
<p>Ltot(ψψψ,ωωω)= ĥrsgμν
((
Dωψ
</p>
<p>)r
μ
,
(
Dωψ
</p>
<p>)s
ν
</p>
<p>)
</p>
<p>&minus;m2ĥrs
(
ψ r ,ψ s
</p>
<p>)
&minus; 1
</p>
<p>4
hijg
</p>
<p>μαgνβ
(
Ω iμν,Ω
</p>
<p>j
αβ
</p>
<p>)
,
</p>
<p>(35.20)</p>
<p/>
</div>
<div class="page"><p/>
<p>35.4 Local Equations 1113
</p>
<p>where (see Problem 35.5)
</p>
<p>(
Dωψ
</p>
<p>)r
μ
= &part;μψ r + (Aμ)rsψ s,
</p>
<p>Ω iμν = &part;μAiν &minus; &part;νAiμ +
1
</p>
<p>2
cijk
</p>
<p>(
AjμA
</p>
<p>k
ν &minus;AjνAkμ
</p>
<p>)
.
</p>
<p>(35.21)
</p>
<p>Note that ωωω=Aμdxμ with Aμ generally a matrix whose elements are func-
tions of space and time. It can be written as Aμ =Aiμei in terms of the basis
vectors (matrices) of the Lie algebra.
</p>
<p>Example 35.4.1 Let us now look at a specific example. Let V = R2 with
hrs = δrs and G the group of two-dimensional rotations, so that G acts on
R2 by matrix multiplication. G is an abelian group; so all structure constants
are zero. Since G is a one-parameter group, it is one-dimensional, so is
its Lie algebra. We have already discussed the Lie algebra of this group in
Example 29.1.35, where, if we identify the tangent space of R2 as the space
itself, we see that the generator of the group is
</p>
<p>e =
(
</p>
<p>0 &minus;1
1 0
</p>
<p>)
</p>
<p>which we could also have obtained by differentiating the matrix
</p>
<p>g(θ)=
(
</p>
<p>cos θ &minus; sin θ
sin θ cos θ
</p>
<p>)
</p>
<p>at the identity where θ = 0.
We want to write the Lagrangian (35.20) for this bundle. First we note
</p>
<p>that
</p>
<p>ω= eAμdxμ
</p>
<p>where Aμ is just a function (not a matrix), and it has no i index. The second
formula in Eq. (35.21) becomes
</p>
<p>Ωμν = &part;μAν &minus; &part;νAμ
</p>
<p>because the group is abelian. For the first equation of (35.21), we represent
ψ as a column vector with two components (remember that r and s run
through 1 and 2), and write
</p>
<p>Dωμψ = &part;μψ +Aμeψ =
(
&part;μψ1
&part;μψ2
</p>
<p>)
+Aμ
</p>
<p>(
0 &minus;1
1 0
</p>
<p>)(
ψ1
ψ2
</p>
<p>)
</p>
<p>=
(
&part;μψ1 &minus;Aμψ2
&part;μψ2 +Aμψ1
</p>
<p>)
.
</p>
<p>Then the first term of the Lagrangian becomes</p>
<p/>
</div>
<div class="page"><p/>
<p>1114 35 Gauge Theories
</p>
<p>gμν
(
&part;μψ1 &minus;Aμψ2 &part;μψ2 +Aμψ1
</p>
<p>)(&part;νψ1 &minus;Aνψ2
&part;νψ2 +Aνψ1
</p>
<p>)
</p>
<p>&equiv; gμν
(
D
</p>
<p>ψ
</p>
<p>12μ D
ψ
</p>
<p>21μ
</p>
<p>)(Dψ12ν
D
</p>
<p>ψ
</p>
<p>21ν
</p>
<p>)
&equiv;Dψ12μD
</p>
<p>ψμ
</p>
<p>12 +D
ψ
</p>
<p>21μD
ψμ
</p>
<p>21 (35.22)
</p>
<p>Putting all the terms together, we obtain
</p>
<p>Ltot(ψψψ,ωωω)=Dψ12μD
μ
12+D
</p>
<p>ψ
</p>
<p>21μD
ψμ
</p>
<p>21 &minus;m2
(
ψ21 +ψ22
</p>
<p>)
&minus; 1
</p>
<p>4
ΩμνΩ
</p>
<p>μν (35.23)
</p>
<p>with Dψ12μ and D
ψ
</p>
<p>21μ as defined in (35.22).
It is instructive to find the current of this Lagrangian. Since there is only
</p>
<p>one component, we do not label it. Then, the first equation in (35.17) be-
comes
</p>
<p>Jμ = 2
&lang;
Dωμψψψ, e &middot;ψψψ
</p>
<p>&rang;
</p>
<p>where the angle brackets designate the inner product in R2. With
</p>
<p>Dωμψψψ =
(
D
</p>
<p>ψ
</p>
<p>12μ
</p>
<p>D
ψ
</p>
<p>21μ
</p>
<p>)
and e &middot;ψψψ =
</p>
<p>(
0 &minus;1
1 0
</p>
<p>)(
ψ1
ψ2
</p>
<p>)
=
(&minus;ψ2
</p>
<p>ψ1
</p>
<p>)
,
</p>
<p>we get
</p>
<p>Jμ = 2
(
&minus;ψ2Dψ12μ +ψ1D
</p>
<p>ψ
</p>
<p>21μ
</p>
<p>)
</p>
<p>= 2
[
&minus;ψ2(&part;μψ1 &minus;Aμψ2)+ψ1(&part;μψ2 +Aμψ1)
</p>
<p>]
</p>
<p>= 2ψ1&part;μψ2 &minus; 2ψ2&part;μψ1 + 2Aμ
(
ψ21 +ψ22
</p>
<p>)
.
</p>
<p>In physics literature, the two components of ψ are considered as the
real and imaginary parts of a complex function: ψψψ = ψ1 + iψ2. Then R2
is treated as the complex plane C, and rotation becomes multiplication by
the elements of U(1), with which we started this chapter. The equivalence
e &harr; i between the two methods becomes clear when we note that
(
</p>
<p>0 &minus;1
1 0
</p>
<p>)(
0 &minus;1
1 0
</p>
<p>)
=&minus;
</p>
<p>(
1 0
0 1
</p>
<p>)
and
</p>
<p>(
0 &minus;1
1 0
</p>
<p>)t
=&minus;
</p>
<p>(
0 &minus;1
1 0
</p>
<p>)
,
</p>
<p>with transposition being equivalent to complex conjugation. The reader is
urged to go through the complex treatment as a useful exercise (see Prob-
lem 35.6).
</p>
<p>At the beginning of the chapter, we identified the principal fiber bundle
having U(1) as the structure group with the electromagnetic interaction. In
this example, we have included a matter field, a charged scalar field in the
model. Therefore, our Lagrangian describes a charged scalar field interact-
ing via electromagnetic force.</p>
<p/>
</div>
<div class="page"><p/>
<p>35.5 Problems 1115
</p>
<p>35.5 Problems
</p>
<p>35.1 Prove condition (b) of Definition 34.2.1 for Theorem 35.1.8. Hint:
First show that f ◦Rg =Rg ◦ f .
</p>
<p>35.2 Provide the details of the proof of Lemma 35.1.10.
</p>
<p>35.3 Derive Eq. (35.17) from Eq. (35.16).
</p>
<p>35.4 For σu a local section of M , show that σ &lowast;uD
ωψψψ =Dωuψψψu.
</p>
<p>35.5 Convince yourself why a factor 12 was necessary for the gauge La-
grangian in going from Eq. (35.19) to Eq. (35.20). Using Eqs. (26.23) and
(34.13) show that the components of Ω iμν are as given in (35.21).
</p>
<p>35.6 Let V=C, G=U(1), and write the inner product on C as
</p>
<p>〈z1, z2〉 =
1
</p>
<p>2
(z1z̄2 + z2z̄1),
</p>
<p>with bar indicating complex conjugation.
</p>
<p>(a) Show that the Lagrangian (35.23) can be written as
</p>
<p>L= gμν(&part;μψ + iAμψ)(&part;νψ̄ &minus; iAνψ̄)&minus;m2ψ̄ψ &minus;
1
</p>
<p>4
ΩμνΩμν .
</p>
<p>(b) Show that the current is given by
</p>
<p>Jμ = iψ̄(&part;μψ + iAμψ)&minus; iψ(&part;μψ̄ &minus; iAμψ̄).
</p>
<p>(c) Substitute the real and imaginary parts of ψ and show that (a) and (b)
reduce to the Lagrangian and current given in Example 35.4.1.</p>
<p/>
</div>
<div class="page"><p/>
<p>36Differential Geometry
</p>
<p>In the last two chapters, we introduced the notions of the principal fiber
bundle and its associated bundle. The former made contact with physics by
the introduction of a connection&mdash;identified as the gauge potential&mdash;and its
curvature&mdash;identified as the gauge field. The latter, the associated bundle
with a vector space as its standard fiber, was the convenient setting for par-
ticle fields. The concentration of Chap. 35 was on the objects, the particle
fieldsψψψ , that lived in the vector space and not on the vector space itself. The
importance of the vector space comes from the fact that tangent spaces of
the base manifold M are vector spaces, and their examination leads to the
nature of the base manifold. And that is our aim in this chapter.
</p>
<p>36.1 Connections in a Vector Bundle
</p>
<p>Let P(M,G) be a principal fiber bundle and E(M,Rm,G,P ) its associated
bundle, where G acts on Rm by a representation of G into GL(m,R). In such
a situation, E is called a vector bundle. The set of sections S(E,M,Rm) vector bundle
has a natural vector space structure with obvious addition of vectors and
multiplication by scalars. Furthermore, if λ &isin; C&infin;(M), we have
</p>
<p>(λϕ)(x)= λ(x) &middot; ϕ(x), ϕ &isin; S
(
E,M,Rm
</p>
<p>)
, x &isin;M.
</p>
<p>If ϕ is a section in E, and if ϕ is to have any physical application, we
have to know how to calculate its partial (or directional) derivatives. This
means being able to define a differentiation process for ϕ given a vector
field X on M . For a vector field X, let γX(t) be its integral curve in the
neighborhood of t = 0. Denote this curve by xt , so that X = ẋ0. Lift this
curve up to wt , and see how ϕ changes along this curve. The derivative of ϕ
along wt is what we are after.
</p>
<p>Definition 36.1.1 Let ϕ be a section of E defined on the curve γ = xt in M .
Let ẋt be the vector tangent to γ at xt . The covariant derivative &nabla;ẋtϕ of ϕ covariant derivative
in the direction of (or with respect to) ẋt is given by
</p>
<p>&nabla;ẋtϕ = lim
h&rarr;0
</p>
<p>1
</p>
<p>h
</p>
<p>[
γ t+ht
</p>
<p>(
ϕ(xt+h)
</p>
<p>)
&minus; ϕ(xt )
</p>
<p>]
</p>
<p>S. Hassani, Mathematical Physics, DOI 10.1007/978-3-319-01195-0_36,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>1117</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-01195-0_36">http://dx.doi.org/10.1007/978-3-319-01195-0_36</a></div>
</div>
<div class="page"><p/>
<p>1118 36 Differential Geometry
</p>
<p>Fig. 36.1 The covariant derivative &nabla;ẋ0ϕ. The grey sheets are π&minus;1E (x0) and π&minus;1E (xh), the
fibers of E at x0 and xh. Find ϕ(xh). Construct the horizontal lift wt of xt starting at
ϕ(xh). Go backwards to the fiber at x0. Record w0, and find the difference ϕ(x0)&minus; w0
and divide by h. As h goes to zero, this ratio gives the covariant derivative of ϕ with
respect to ẋ0
</p>
<p>where γ t+ht is the parallel displacement of the fiber π
&minus;1
E (xt+h) along γ to
</p>
<p>the fiber π&minus;1E (xt ).
</p>
<p>Note that &nabla;ẋtϕ &isin; π&minus;1E (xt ), and thus defines a section of E along γ .
</p>
<p>Remark 36.1.1 Definition 36.1.1 has a number of interesting features
which are worth exploring. First, the very notion of derivative involves a
subtraction, an operation that does not exist on all mathematical objects
such as, for example, manifold. Thus, the fact that fibers of this particu-
lar E have a vector-space structure is important. Second, although all fibers
are isomorphic as vector spaces, there is no natural isomorphism connect-
ing them. Parallelism gives an isomorphism, but parallelism depends on the
notion of a horizontal lift of a curve in the base manifold. Horizontal lift,
in turn, depends on the notion of a connection. One interpretation of the
word &ldquo;connection&rdquo; is that it actually does connect fibers through an induced
isomorphism.
</p>
<p>Third, any derivative involves an infinitesimal change. Now that we have
a curve in the base manifold, it can induce a section-dependent curve ϕ(xt )
in E. A natural directional derivative of the section would be to move
along xt and see how ϕ(xt ) changes. When t changes to t + h, the sec-
tion changes from ϕ(xt ) to ϕ(xt+h). But we cannot compute the difference
ϕ(xt+h)&minus;ϕ(xt ), because ϕ(xt+h) &isin; π&minus;1E (xt+h) while ϕ(xt ) &isin; π&minus;1E (xt ), and
we don&rsquo;t know how to subtract two vectors from two different vector spaces.
That is why we need to transfer ϕ(xt+h) to π&minus;1E (xt ) via the parallelism γ
</p>
<p>t+h
t
</p>
<p>(see Fig. 36.1). The word &ldquo;parallelism&rdquo; comes about because the horizontal
lift of xt is as parallel to xt as one can get within the confines of a connec-
tion.</p>
<p/>
</div>
<div class="page"><p/>
<p>36.1 Connections in a Vector Bundle 1119
</p>
<p>Definition 36.1.2 A section ϕ is parallel if ϕ(xt ) is the horizontal lift of xt .
In particular, γ t+ht (ϕ(xt+h))= ϕ(xt ) for all t and h.
</p>
<p>parallel section
</p>
<p>We thus have
</p>
<p>Proposition 36.1.3 A section ϕ is parallel iff &nabla;ẋtϕ = 0 for all t .
</p>
<p>Furthermore, if we rewrite the defining equation of the covariant deriva-
tive as
</p>
<p>γ t+ht
(
ϕ(xt+h)
</p>
<p>)
= ϕ(xt )+ h&nabla;ẋtϕ +O
</p>
<p>(
h2
)
,
</p>
<p>where O(h2) denotes terms of order h2 and higher powers of h, then we
see that two curves that have the same value and tangent vector at t give the
same covariant derivative at t . This means that we can define the covariant
derivative in terms of vectors.
</p>
<p>Definition 36.1.4 Let X &isin; TxM and ϕ a section of E defined on a neigh-
borhood of x &isin;M . Let xt be a curve such that X = ẋ0. Then the covariant covariant derivative in
</p>
<p>the direction of a vectorderivative of ϕ in the direction of X is &nabla;Xϕ =&nabla;ẋ0ϕ. A section ϕ is parallel
on U &sub;M iff &nabla;Xϕ = 0 for all X &isin; TxU , x &isin;U .
</p>
<p>It is convenient to have an alternative definition of the covariant derivative
of a section in the direction of the vector X in terms of its horizontal lift in P .
</p>
<p>Proposition 36.1.5 Let ϕ be defined on U &sub;M . Associate with ϕ an Rm-
valued function f on π&minus;1(U) &sub; P by f (p) = p&minus;1(ϕ(π(p))) for p &isin;
π&minus;1(U) &sub; P as in Remark 34.3.1. Let X&lowast; &isin; TpP be the horizontal lift of
X &isin; TxM . Then X&lowast;f &isin;Rm and p(X&lowast;f ) &isin; π&minus;1(x), and
</p>
<p>&nabla;Xϕ = p
(
X&lowast;f
</p>
<p>)
.
</p>
<p>Proof Let xt be the curve with ẋ0 = X. Let pt be the horizontal lift of xt
such that X&lowast; = ṗ0 and p0 = p. Then we have
</p>
<p>X&lowast;f = lim
h&rarr;0
</p>
<p>1
</p>
<p>h
</p>
<p>[
f (ph)&minus; f (p)
</p>
<p>]
= lim
</p>
<p>h&rarr;0
1
</p>
<p>h
</p>
<p>[
p&minus;1h
</p>
<p>(
ϕ(xh)
</p>
<p>)
&minus; p&minus;1
</p>
<p>(
ϕ(x)
</p>
<p>)]
</p>
<p>and
</p>
<p>pX&lowast;f = lim
h&rarr;0
</p>
<p>1
</p>
<p>h
</p>
<p>[
pp&minus;1h
</p>
<p>(
ϕ(xh)
</p>
<p>)
&minus; ϕ(x)
</p>
<p>]
.
</p>
<p>Set ξ = p&minus;1h (ϕ(xh)), and consider ptξ , which is a horizontal curve in E.
Note that phξ = ϕ(xh) and p0ξ = pp&minus;1h (ϕ(xh)). By the definition of γ h0 , we
have γ h0 (phξ)= p0ξ . Hence, γ h0 ϕ(xh)= pp&minus;1h (ϕ(xh)). Substituting this in
the above equation yields the result we are after. �</p>
<p/>
</div>
<div class="page"><p/>
<p>1120 36 Differential Geometry
</p>
<p>Sometimes it is convenient to write the definition of the covariant deriva-
tive in terms of ordinary derivatives, as follows
</p>
<p>&nabla;ẋtϕ =
d
</p>
<p>ds
γ t+st
</p>
<p>(
ϕ(xt+s)
</p>
<p>)∣∣∣∣
s=0
</p>
<p>. (36.1)
</p>
<p>The covariant derivative satisfies certain important properties which are
sometimes used to define it. We collect these properties in the following
</p>
<p>Proposition 36.1.6 Let X,Y &isin; TxM and ϕ and ψ be sections of E defined
in a neighborhood of x. Then
</p>
<p>(a) &nabla;X(ϕ +ψ)=&nabla;Xϕ +&nabla;Xψ ;
(b) &nabla;αXϕ = α&nabla;Xϕ, where α &isin;R;
(c) &nabla;X+Yϕ =&nabla;Xϕ +&nabla;Yϕ;
(d) &nabla;X(f ϕ)= f (x) &middot; &nabla;Xϕ + (Xf ) &middot; ϕ(x), where f is a real-valued func-
</p>
<p>tion defined in a neighborhood of x.
</p>
<p>Proof (a) follows from the fact that the isomorphism γ t+ht is linear. (b) fol-
low from the fact that if γX(t) is the curve whose tangent is X, then γX(αt)
is the curve whose tangent is αX. (c) follows from Proposition 36.1.5 and
the fact that X&lowast;+Y&lowast; is the lift of X+Y. For (d), let X be tangent to xt with
x0 = x and X = ẋ0. Then use Eq. (36.1):
</p>
<p>&nabla;X(f ϕ)=&nabla;ẋ0(f ϕ)=
d
</p>
<p>dt
γ t0
(
f (xt )ϕ(xt )
</p>
<p>)∣∣∣∣
t=0
</p>
<p>= d
dt
</p>
<p>f (xt )γ
t
0
</p>
<p>(
ϕ(xt )
</p>
<p>)∣∣∣∣
t=0
</p>
<p>= d
dt
</p>
<p>f (x)γ t0
(
ϕ(xt )
</p>
<p>)∣∣∣∣
t=0
</p>
<p>+ d
dt
</p>
<p>f (xt )γ
0
0
</p>
<p>(
ϕ(x)
</p>
<p>)∣∣∣∣
t=0
</p>
<p>= f (x) &middot; &nabla;X(ϕ)+ (Xf ) &middot; ϕ(x)
</p>
<p>where we used γ t0(f (xt )ϕ(xt )) = f (xt )γ t0(ϕ(xt )), which is a property of
the linearity of γ t0 and the fact that f (xt ) is a real number. We also used
γ 00 = id, which should be obvious. �
</p>
<p>Remark 36.1.2 Proposition 36.1.6 applies to vectors at a point of the man-
ifold M . It can be generalized to vector fields by applying it pointwise.
Therefore, the proposition holds for vector fields as well. The minor dif-
ference is that α in (b) of the proposition can also be a function on M .
</p>
<p>36.2 Linear Connections
</p>
<p>From the general vector bundles whose standard fiber was Rm, we now spe-
cialize to the bundle of linear frames L(M) examined in Example 34.1.11,
among whose associated bundles are tangent bundle (Box 34.1.17) and ten-
sor bundle (Example 34.1.18). We use P for L(M) to avoid cluttering the
notations.</p>
<p/>
</div>
<div class="page"><p/>
<p>36.2 Linear Connections 1121
</p>
<p>Definition 36.2.1 A connection Ŵ in L(M)&equiv; P is called a linear connec- linear connection
tion of M .
</p>
<p>Recall that for any principal fiber bundle and its associate bundle E, each
p &isin; P is an isomorphism of F , the standard fiber of E, with π&minus;1E (x). In fact,
p&minus;1 is an F -valued map on π&minus;1E (x). In the present case of L(M), p
</p>
<p>&minus;1 is an
Rn-valued map on TxM . In addition, there is a natural map TpP &rarr; TxM ,
namely π&lowast;. If we combine the two maps, we get a 1-form on L(M):
</p>
<p>Definition 36.2.2 The canonical form θθθ of L(M) &equiv; P is the Rn-valued canonical form
1-form on P defined by
</p>
<p>θθθ(X)= p&minus;1π&lowast;(X) for X &isin; TpP. (36.2)
</p>
<p>Proposition 36.2.3 The canonical form is a tensorial 1-form of type
(id,Rn), where id is the identity representation of GL(n;R).
</p>
<p>Proof Let X be any vector at p &isin; P and g &isin; GL(n;R). Then Rg&lowast;X is a
vector at pg &isin; P . Therefore,
</p>
<p>(
R&lowast;gθθθ
</p>
<p>)
(X)= θθθ(Rg&lowast;X)= (pg)&minus;1
</p>
<p>(
π&lowast;(Rg&lowast;X)
</p>
<p>)
</p>
<p>= g&minus;1p&minus;1
(
π&lowast;(X)
</p>
<p>)
= g&minus;1 &middot; θθθ(X),
</p>
<p>where we used π&lowast;(Rg&lowast;X)= π&lowast;(X) which is implied by π(pg)= π(p). This
shows that θθθ is pseudotensorial. But if X is vertical, then π&lowast;&mdash;and therefore
θθθ&mdash;annihilates it. Hence, θθθ is tensorial. �
</p>
<p>Definition 36.2.4 Let Ŵ be a linear connection of M . For each ξ &isin; Rn and standard horizontal
vector field of a
</p>
<p>connection
p &isin; P define the vector field B(ξ) in such a way that (B(ξ))p is the unique
horizontal lift of pξ &isin; Tπ(p)M . The vector field B(ξ) so defined is called
the standard horizontal vector field of Ŵ corresponding to ξ .
</p>
<p>Proposition 36.2.5 The standard horizontal vector fields have the follow-
ing properties:
</p>
<p>(a) If θθθ is the canonical form of P , then θθθ ◦ B = idRn .
(b) Rg&lowast;(B(ξ))= B(g&minus;1ξ) for g &isin;GL(n;R) and ξ &isin;Rn.
(c) If ξ �= 0, then B(ξ) never vanishes.
</p>
<p>Proof (a) follows directly from the definition of θθθ . In fact
</p>
<p>θθθp
((
</p>
<p>B(ξ)
)
p
</p>
<p>)
= p&minus;1
</p>
<p>(
π&lowast;
</p>
<p>((
B(ξ)
</p>
<p>)
p
</p>
<p>))
= p&minus;1(pξ)= ξ.
</p>
<p>(b) If B(ξ) is a standard horizontal vector field at p, then Rg&lowast;(B(ξ))
is a standard horizontal vector field at pg. Let Rg&lowast;(B(ξ)) &equiv; B(ξ &prime;). Then
π&lowast;((B(ξ &prime;))pg)= pgξ &prime;. We also have π&lowast;((B(ξ))p)= pξ . Since π&lowast;(Rg&lowast;X)=
π&lowast;(X), we must have pgξ &prime; = pξ or ξ &prime; = g&minus;1ξ .
</p>
<p>(c) Assume that (B(ξ))p = 0 for some p. Then pξ = π&lowast;((B(ξ))p) = 0.
Multiplying by p&minus;1, we get ξ = 0. �</p>
<p/>
</div>
<div class="page"><p/>
<p>1122 36 Differential Geometry
</p>
<p>Proposition 36.2.6 Let A&lowast; be the fundamental vector field corresponding
to A &isin; g and B(ξ) the standard horizontal vector field corresponding to
ξ &isin;Rn. Then
</p>
<p>[
A&lowast;,B(ξ)
</p>
<p>]
= B(Aξ).
</p>
<p>Proof Recall that the commutator of two vector fields is the Lie derivative
of one with respect to the other. Hence, using Definition 28.4.12, noting that
the action of G on P is a right action, and using (b) of Proposition 36.2.5,
we have
</p>
<p>[
A&lowast;,B(ξ)
</p>
<p>]
= lim
</p>
<p>t&rarr;0
1
</p>
<p>t
</p>
<p>[
R
g&minus;1t&lowast;
</p>
<p>B(ξ)&minus; B(ξ)
]
</p>
<p>= lim
t&rarr;0
</p>
<p>1
</p>
<p>t
</p>
<p>[
B(gtξ)&minus; B(ξ)
</p>
<p>]
= lim
</p>
<p>t&rarr;0
1
</p>
<p>t
</p>
<p>[
B
(
etAξ
</p>
<p>)
&minus; B(ξ)
</p>
<p>]
</p>
<p>= lim
t&rarr;0
</p>
<p>1
</p>
<p>t
</p>
<p>[
B
(
(1 + tA + &middot; &middot; &middot; )ξ
</p>
<p>)
&minus; B(ξ)
</p>
<p>]
= B(Aξ),
</p>
<p>where we used exp(tA)= etA when the Lie algebra is gl(n;R). �
</p>
<p>Definition 36.2.7 The torsion form of a linear connection ωωω is de-
fined by
</p>
<p>���=Dωθθθ. (36.3)
</p>
<p>Proposition 36.2.3 implies that��� &isin; Λ̄2(P,Rn), i.e., that the torsion form
</p>
<p>torsion form
</p>
<p>is a tensorial 2-form.
</p>
<p>Theorem 36.2.8 Letωωω,���, and��� be the connection form, the torsion form,
and the curvature form of a linear connection Ŵ of L(M). Then we have,
</p>
<p>First structure equation: ���= dθθθ +ωωω&and;̇θθθ , or in detail,
</p>
<p>���(X,Y)= dθθθ(X,Y)+ωωω(X) &middot; θθθ(Y)&minus;ωωω(Y) &middot; θθθ(X)
</p>
<p>Second structure equation: ���= dωωω+ 12 [ωωω,ωωω], or
</p>
<p>���(X,Y)= dωωω(X,Y)+ 1
2
</p>
<p>[
ωωω(X),ωωω(Y)
</p>
<p>]
,
</p>
<p>where X,Y &isin; Tp(L(M)).
</p>
<p>Proof The second structure equation is the result of Theorem 34.3.6. The
first structure equation is derived in [Koba 63, pp. 120&ndash;121]. �
</p>
<p>The equations above which do not act on vector fields are to be inter-
preted as products of matrices whose entries are forms and the multiplica-
tion is through wedge product. We can write the equations above in terms
of components. Let {êi}ni=1 be the standard basis of Rn and {E
</p>
<p>j
i }ni,j=1 be a</p>
<p/>
</div>
<div class="page"><p/>
<p>36.2 Linear Connections 1123
</p>
<p>basis of gl(n,R). Eji is an n&times; n matrix with a 1 at the ith column and j th
row and zero everywhere else. In terms of these basis vectors, we can write
</p>
<p>θθθ = θ i êi, ���=Θ i êi,
</p>
<p>ωωω= ωijE
j
</p>
<p>i , ���=Ω ijE
j
</p>
<p>i ,
(36.4)
</p>
<p>with summation over repeated indices in place. Then the structure equations
become
</p>
<p>Θ i = dθ i +ωij &and; θ j , i = 1,2, . . . , n,
</p>
<p>Ω ij = dωij +ωik &and;ωkj , i, j = 1,2, . . . , n,
(36.5)
</p>
<p>as the reader can verify (see Problem 36.2). Multiplying both sides of the
second equation above by Eji , the left-hand side becomes a matrix with
elements Ω ij . The first terms on the right-hand side becomes the exterior
</p>
<p>derivative of a matrix whose elements are ωij and the second term on the
right-hand side will be simply the matrix product of the latter matrix, where
the elements are wedge-multiplied. We summarize this in the following box,
which we shall use later:
</p>
<p>Box 36.2.9 Let �̂�� be the matrix with elements Ω ij and ω̂ωω the matrix
</p>
<p>with elements ωij . Then �̂�� = dω̂ωω + ω̂ωω &and; ω̂ωω, where d operates on the
elements of ω̂ωω and ω̂ωω&and;ω̂ωω is the multiplication of two matrices in which
ordinary product is replaced with wedge product.
</p>
<p>Theorem 36.2.10 (Bianchi&rsquo;s identities) Letωωω,���, and��� be the connection
form, the torsion form, and the curvature form of a linear connection Ŵ of
L(M). Then
</p>
<p>First Bianchi identity: Dω���=���&and;̇θθθ .
Second Bianchi identity: Dω���ω = 0.
</p>
<p>Proof The first identity is a special case of Theorem 35.1.4. The second
identity was the content of Theorem 34.3.7. �
</p>
<p>36.2.1 Covariant Derivative of Tensor Fields
</p>
<p>Up to this point, we have concentrated on the differentiation of forms, whose
natural differential is Dω. We also need to differentiate general tensors in
the most &ldquo;natural&rdquo; way. As discussed earlier, this natural way is the di-
rectional derivative introduced in Proposition 36.1.6. However, instead of
derivatives with respect to a vector at a point, we generalize to derivatives
in the direction of a vector field as pointed out in Remark 36.1.2. When the
standard fiber is Rn, with n = dimM , the associated bundle is the tangent</p>
<p/>
</div>
<div class="page"><p/>
<p>1124 36 Differential Geometry
</p>
<p>bundle T (M) whose cross sections are vector fields. We thus restate Propo-
sition 36.1.6 for L(M) with the associated bundle T (M) in the direction of
vector fields:
</p>
<p>Proposition 36.2.11 Let X, Y, and Z be vector fields on M . Then
</p>
<p>(a) &nabla;X(Y + Z)=&nabla;XY +&nabla;XZ;
(b) &nabla;X+YZ =&nabla;XZ +&nabla;YZ;
(c) &nabla;fXY = f &middot; &nabla;XY, where f &isin; C&infin;(M);
(d) &nabla;X(fY)= f &middot; &nabla;XY + (Xf ) &middot; Y, where f &isin; C&infin;(M).
</p>
<p>We defined the covariant derivative in terms of parallel displacements
along a path in M and obtained the four equations of Proposition 36.2.11. It
turns out that
</p>
<p>Any derivative satisfying
</p>
<p>(a)&ndash;(d) of
</p>
<p>Proposition 36.2.11 is the
</p>
<p>covariant derivative with
</p>
<p>respect to a linear
</p>
<p>connection.
Theorem 36.2.12 Any derivative which satisfies the four conditions of
Proposition 36.2.11 is the covariant derivative with respect to some linear
connection.
</p>
<p>If instead of Rn, we take Trs (R
n) as the standard basis, the bundle asso-
</p>
<p>ciated with L(M) becomes the tensor bundle T rs (M) of type (r, s) over M .
Being still a vector bundle, we can define a covariant derivative for it. Now,
tensors are products of vector fields and 1-forms, and if we know how the
directional derivative acts on vector fields and one forms, we know how it
acts on all tensors. Since a 1-form pairs up with a vector field to produce a
function, we can also state that if the action of the derivative is known for
vector fields and functions, it is known for all tensors. The action of &nabla;X on
vector fields is given by Proposition 36.2.11. The proposition also includes
its action on functions (see Problem 36.3).
</p>
<p>Theorem 36.2.13 Let T(M) be the algebra of tensor fields (the vector
space of tensor fields together with tensor multiplication as the algebra
multiplication) on M . Then the covariant differentiation has the following
properties:
</p>
<p>(a) &nabla;X : T(M)&rarr; T(M) is a type-preserving derivation;1
(b) &nabla;X commutes with contraction;
(c) &nabla;Xf = Xf for every function f &isin; C&infin;(M) on M ;
(d) &nabla;X+Y =&nabla;X +&nabla;Y
(e) &nabla;fXT= f &middot; &nabla;XT for all f &isin; C&infin;(M) and T &isin; T(M).
</p>
<p>A tensor field T of type (r, s) can be thought of as a multilinear mapping
</p>
<p>T : T (M)&times; T (M)&times; &middot; &middot; &middot; &times; T (M)︸ ︷︷ ︸
s times Cartesian product
</p>
<p>&rarr; Tr0(M).
</p>
<p>The s vector fields from the domain of T fill all the covariant slots, leaving
all the r contravariant slots untouched. With this kind of interpretation, we
have the following:covariant differential
</p>
<p>1See Definition 3.4.1.</p>
<p/>
</div>
<div class="page"><p/>
<p>36.2 Linear Connections 1125
</p>
<p>Definition 36.2.14 Given a tensor field T of type (r, s) the covariant
differential &nabla;T of T is a tensor of type (r, s + 1) given by
</p>
<p>(&nabla;T)(X1, . . . ,Xs;X)= (&nabla;XT)(X1, . . . ,Xs), Xi,X &isin; T (M).
</p>
<p>By a procedure similar to that which led to the Lie derivative (28.36) of
a p-form, we can obtain the following formula:
</p>
<p>(&nabla;T)(X1, . . . ,Xs;X)
</p>
<p>=&nabla;X
(
T(X1, . . . ,Xs)
</p>
<p>)
&minus;
</p>
<p>s&sum;
</p>
<p>i=1
T(X1, . . . ,&nabla;XXi, . . . ,Xs), (36.6)
</p>
<p>where T is a tensor field of type (r, s) and Xi,X &isin; T (M).
A tensor field T, as a section of a bundle associated with L(M), is said to
</p>
<p>be parallel iff &nabla;XT= 0 for all vector fields on M (see Proposition 36.1.3).
This leads to
</p>
<p>Proposition 36.2.15 A tensor field T on M is parallel iff &nabla;T= 0.
</p>
<p>36.2.2 From Forms on P to Tensor Fields onM
</p>
<p>We have defined two kinds of covariant differentiation: Dω, which acts on
differential forms on P , and &nabla; , which acts on the sections of the associ-
ated bundle E. In general, there is no natural relation between the two,
because the standard fiber Rm of E has no relation to the structure of P .
However, when P = L(M) and the standard fiber is Rn, the fibers π&minus;1E (x)
become TxM , the tangent spaces of the base manifold of the bundle, which
are reachable by π&lowast;. Therefore, we expect some kind of a relationship be-
tween the two covariant derivatives. In particular, the two quantities defined
in terms of Dω, namely the torsion and curvature forms, should be related
to quantities defined in terms of &nabla; .
</p>
<p>The torsion form, being an Rn-valued 2-form on P = L(M), takes two
vector fields on P and produces a vector in Rn, the standard fiber of E.
Then, through the action of p &isin; P , this vector can be mapped to a vector
in TxM with x = π(p) as in Theorem 34.1.14. This process, in conjunction
with Remark 34.3.1 allows us to define a map T : T (M)&times; T (M)&rarr; T (M).
This map is called torsion tensor field or just torsion, and is defined as torsion
follows:
</p>
<p>T(X,Y)&equiv; p
(
���
(
X&lowast;,Y&lowast;
</p>
<p>))
for X,Y &isin; TxM, (36.7)
</p>
<p>where p is any point of L(M) such that π(p) = x, and X&lowast; and Y&lowast; are
any two vectors of L(M) such that π&lowast;(X&lowast;) = X and π&lowast;(Y&lowast;) = Y. Re-
mark 34.3.1 ensures that T(X,Y) is independent of p, X&lowast;, and Y&lowast;. Fur-
thermore, T(X,Y)=&minus;T(Y,X), and since it maps T (M)&times; T (M) to T (M),
it is a skew-symmetric tensor field of type (1,2).</p>
<p/>
</div>
<div class="page"><p/>
<p>1126 36 Differential Geometry
</p>
<p>Similarly, The curvature form, being a gl(n,R)-valued 2-form on P =
L(M), takes two vector fields on P and produces a matrix in gl(n,R).
This matrix can act on a vector in Rn, which can be the inverse map of
Theorem 34.1.14 (i.e., the image of a vector in TxM by p&minus;1), to produce
another vector in Rn. Then, through the action of p &isin; P , this vector can
be mapped to a vector in TxM with x = π(p) as in Theorem 34.1.14.
This process, in conjunction with Remark 34.3.1 allows us to define a map
T (M)&times;T (M)&times;T (M)&rarr; T (M). This map is called curvature tensor field
or just curvature, and is defined as follows:curvature
</p>
<p>R(X,Y)Z &equiv; p
(
���
(
X&lowast;,Y&lowast;
</p>
<p>)(
p&minus;1Z
</p>
<p>))
for X,Y,Z &isin; TxM. (36.8)
</p>
<p>It follows that R is a tensor field of type (1,3) with the property that
R(X,Y)=&minus;R(Y,X). Note that R(X,Y) is an endomorphism of TxM , and
is called the curvature transformation of TxM determined by X and Y.curvature
</p>
<p>transformation
</p>
<p>Theorem 36.2.16 The torsion T and curvature R can be expressed in
terms of covariant differentiation as follows:
</p>
<p>T(X,Y)=&nabla;XY &minus;&nabla;YX &minus; [X,Y]
R(X,Y)Z = [&nabla;X,&nabla;Y ]Z &minus;&nabla;[X,Y ]Z,
</p>
<p>where X, Y, and Z are vector fields on M .
</p>
<p>Proof Specialize Proposition 36.1.5 to L(M) and get (&nabla;XY)x = p(X&lowast;pf )
where X&lowast;p is the horizontal lift of X at x. The function f is given by
</p>
<p>f (p)= p&minus;1(Yx)= p&minus;1
(
π&lowast;
</p>
<p>(
Y&lowast;p
</p>
<p>))
&equiv; θθθ
</p>
<p>(
Y&lowast;p
</p>
<p>)
, (36.9)
</p>
<p>where Y&lowast;p is the horizontal lift of Y at x. Thus, we have the identity
</p>
<p>(&nabla;XY)x = p
(
X&lowast;p
</p>
<p>(
θθθ
(
Y&lowast;p
</p>
<p>)))
. (36.10)
</p>
<p>From (36.9), we get p&minus;1(&nabla;XY)x = θθθ((&nabla;XY)&lowast;p). From (36.10), we get
p&minus;1(&nabla;XY)x = X&lowast;p(θθθ(Y&lowast;p)). Therefore, we have another useful identity:
</p>
<p>θθθ
(
(&nabla;XY)&lowast;
</p>
<p>)
p
= X&lowast;p
</p>
<p>(
θθθ
(
Y&lowast;
</p>
<p>))
. (36.11)
</p>
<p>Now use the first structure equation of Theorem 36.2.8 to get���(X&lowast;,Y&lowast;)=
dθθθ(X&lowast;,Y&lowast;) because ωωω(X&lowast;) = 0 = ωωω(Y&lowast;) for horizontal X&lowast; and Y&lowast;. We
therefore have
</p>
<p>T(Xx,Yx)= p
(
���
(
X&lowast;p,Y
</p>
<p>&lowast;
p
</p>
<p>))
= p
</p>
<p>(
dθθθ
</p>
<p>(
X&lowast;p,Y
</p>
<p>&lowast;
p
</p>
<p>))
</p>
<p>= p
(
X&lowast;p
</p>
<p>(
θθθ
(
Y&lowast;
</p>
<p>))
&minus; Y&lowast;p
</p>
<p>(
θθθ
(
X&lowast;
</p>
<p>))
&minus; θθθ
</p>
<p>([
X&lowast;,Y&lowast;
</p>
<p>]
p
</p>
<p>))
</p>
<p>= (&nabla;XY)x &minus; (&nabla;YX)x &minus; [X,Y]x,</p>
<p/>
</div>
<div class="page"><p/>
<p>36.2 Linear Connections 1127
</p>
<p>where we used Theorem 28.5.11 and the fact that π&lowast;([X&lowast;,Y&lowast;])= [X,Y].
To prove the curvature tensor equation, let p&minus;1Z = f (p) = θ(Z&lowast;p) by
</p>
<p>(36.9). Then, we have
</p>
<p>R(Xx,Yx)Zx = p
(
���
(
X&lowast;p,Y
</p>
<p>&lowast;
p
</p>
<p>)(
f (p)
</p>
<p>))
= p
</p>
<p>(
&minus;ωωω
</p>
<p>([
X&lowast;,Y&lowast;
</p>
<p>]
p
</p>
<p>)(
f (p)
</p>
<p>))
</p>
<p>= p
(
&minus;ωωω
</p>
<p>(
v
[
X&lowast;,Y&lowast;
</p>
<p>]
p
</p>
<p>)(
f (p)
</p>
<p>))
= p
</p>
<p>(
&minus;A
</p>
<p>(
f (p)
</p>
<p>))
, (36.12)
</p>
<p>where we used Eq. (34.12) and the fact that ωωω annihilates the horizontal
component of the bracket (v stands for the vertical component). In the last
step, we used (a) of Definition 34.2.1 and denoted by A the element of the
Lie algebra that gives rise to A&lowast;p &equiv; v[X&lowast;,Y&lowast;]p . Now we note that
</p>
<p>&minus;A
(
f (p)
</p>
<p>)
= d
</p>
<p>dt
exp(&minus;At)f (p)
</p>
<p>∣∣∣∣
t=0
</p>
<p>= d
dt
</p>
<p>f
(
p exp(At)
</p>
<p>)∣∣∣∣
t=0
</p>
<p>=A&lowast;pf
</p>
<p>= v
[
X&lowast;,Y&lowast;
</p>
<p>]
p
f = X&lowast;
</p>
<p>(
Y&lowast;pf
</p>
<p>)
&minus; Y&lowast;
</p>
<p>(
X&lowast;pf
</p>
<p>)
&minus; h
</p>
<p>[
X&lowast;,Y&lowast;
</p>
<p>]
p
f
</p>
<p>= X&lowast;
(
Y&lowast;p
</p>
<p>(
θθθ
(
Z&lowast;
</p>
<p>)))
&minus; Y&lowast;
</p>
<p>(
X&lowast;p
</p>
<p>(
θθθ
(
Z&lowast;
</p>
<p>)))
&minus; h
</p>
<p>[
X&lowast;,Y&lowast;
</p>
<p>]
p
</p>
<p>(
θθθ
(
Z&lowast;
</p>
<p>))
.
</p>
<p>It now follows that
</p>
<p>R(Xx,Yx)Zx
</p>
<p>= p
(
X&lowast;
</p>
<p>(
Y&lowast;p
</p>
<p>(
θθθ
(
Z&lowast;
</p>
<p>)))
&minus; Y&lowast;
</p>
<p>(
X&lowast;p
</p>
<p>(
θθθ
(
Z&lowast;
</p>
<p>)))
&minus; h
</p>
<p>[
X&lowast;,Y&lowast;
</p>
<p>]
p
</p>
<p>(
θθθ
(
Z&lowast;
</p>
<p>)))
</p>
<p>= p
(
X&lowast;p
</p>
<p>(
θθθ
(
(&nabla;YZ)&lowast;
</p>
<p>))
&minus; Y&lowast;p
</p>
<p>(
θθθ
(
(&nabla;XZ)&lowast;
</p>
<p>))
&minus; h
</p>
<p>[
X&lowast;,Y&lowast;
</p>
<p>]
p
</p>
<p>(
θθθ
(
Z&lowast;
</p>
<p>)))
</p>
<p>=&nabla;X&nabla;YZ &minus;&nabla;Y&nabla;XZ &minus;&nabla;[X,Y ]Z,
</p>
<p>where we used (36.11) to go from the first line to the second. �
</p>
<p>We also want to express the Bianchi&rsquo;s identities in terms of tensor fields.
We shall confine ourselves to the case where the torsion form is zero. In
most physical applications this is indeed the case. So the first identity of
Theorem 36.2.10 becomes 0 =���&and;̇θθθ . Now let X&lowast;, Y&lowast;, and Z&lowast; be the lifts of
X, Y, and Z. Then, it is easily shown that
</p>
<p>0 = (���&and;̇θθθ)
(
X&lowast;,Y&lowast;,Z&lowast;
</p>
<p>)
= Cyc
</p>
<p>(
���
(
X&lowast;,Y&lowast;
</p>
<p>)
θθθ
(
Z&lowast;
</p>
<p>))
,
</p>
<p>where Cyc means taking the sum of the cyclic permutations of the expres-
sion in parentheses. From Eq. (36.12) and the discussion before it, we also
see that
</p>
<p>R(X,Y)Z = p
(
���
(
X&lowast;,Y&lowast;
</p>
<p>)(
θθθ
(
Z&lowast;
</p>
<p>)))
. (36.13)
</p>
<p>Putting the last two equations together, we obtain
</p>
<p>Cyc
(
R(X,Y)Z
</p>
<p>)
&equiv; R(X,Y)Z + R(Z,X)Y + R(Y,Z)X = 0. (36.14)
</p>
<p>The proof of the second Bianchi&rsquo;s identity is outlined in Problem 36.5.
</p>
<p>Theorem 36.2.17 Let R be the curvature of a linear connection ofM whose
torsion is zero. Then for X,Y and Z in T (M), we have</p>
<p/>
</div>
<div class="page"><p/>
<p>1128 36 Differential Geometry
</p>
<p>Bianchi&rsquo;s 1st identity: Cyc[R(X,Y)Z] = 0;
Bianchi&rsquo;s 2nd identity: Cyc[&nabla;XR(Y,Z)] = 0.
</p>
<p>For the sake of completeness, we give the Bianchi&rsquo;s identities for the case
where torsion is not zero and refer the reader to [Koba 63, pp. 135&ndash;136] for
a proof.
</p>
<p>Theorem 36.2.18 Let R and T be, respectively, the curvature and tor-
sion of a linear connection of M . Then for X,Y and Z in T (M), we
have
</p>
<p>Bianchi&rsquo;s 1st identity: Cyc[R(X,Y)Z] = Cyc[T(T(X,Y),Z) +
(&nabla;XT)(Y,Z)];
Bianchi&rsquo;s 2nd identity: Cyc[&nabla;XR(Y,Z)+ R(T(X,Y),Z)] = 0.
</p>
<p>36.2.3 Component Expressions
</p>
<p>Any application of connections and curvatures requires expressing them in
local coordinates. So, it is useful to have the components and identities in
which they participate in terms of local coordinates. For a linear bundle, the
isomorphism π&minus;1(U) &sim;= U &times; GL(n,R) suggests a local coordinate system
of the form (xi,X
</p>
<p>j
k ), where (x1, . . . , xn) is a coordinate system for U &sub;M
</p>
<p>and Xjk are the elements of a nonsingular n&times; n matrix.
Let&rsquo;s start with the canonical 1-form θθθ =&sum;ni=1 θ i êi , with êi the standard
</p>
<p>basis of Rn. The most general coordinate expression for θ i is
</p>
<p>θ i = aijdxj + b
ij
k dX
</p>
<p>k
j , therefore θθθ =
</p>
<p>(
aijdx
</p>
<p>j + bijk dXkj
)
êi .
</p>
<p>By definition, θθθ = p&minus;1 ◦ π&lowast;, and the presence of π&lowast; annihilates any vertical
component of a vector field. This means that bijk = 0. Now note that as a map
Rn &rarr; TxM , p, whose local coordinates are (xi,Xjk ), sends êk to X
</p>
<p>j
k&part;j , and
</p>
<p>p&minus;1 sends &part;k to Y ik êi , where Y
i
k is the inverse of X
</p>
<p>i
j . Hence, θθθ(&part;k)= Y ik êi .
</p>
<p>So, we have
</p>
<p>Y ik êi = θθθ(&part;k)=
(
aijdx
</p>
<p>j êi
)
(&part;k)= aijdxj (&part;k)êi = aij δ
</p>
<p>j
</p>
<p>k êi = aik êi .
</p>
<p>Thus, we have our first result:
</p>
<p>θ i = Y ik dxk, Y ik =
(
X&minus;1
</p>
<p>)i
k
. (36.15)
</p>
<p>Next, we consider the connection 1-form, ωωω= ωijE
j
i , where ω
</p>
<p>i
j are real-
</p>
<p>valued 1-forms and {Eji } form a basis of gl(n,R). Write
</p>
<p>ωij = aikdXkj + bijkdxk</p>
<p/>
</div>
<div class="page"><p/>
<p>36.2 Linear Connections 1129
</p>
<p>and let it act on a fundamental vector field A&lowast;p = ddt (p expAt)|t=0. If p is
represented by Xαβ , then (with the notation &part;
</p>
<p>β
α &equiv; &part;/&part;Xαβ )
</p>
<p>A&lowast;p =XαβAβγ
&part;
</p>
<p>&part;Xαγ
=XαβAβγ &part;γα
</p>
<p>and when ωij acts on A
&lowast;
p , it should give A
</p>
<p>i
j . So,
</p>
<p>Aij = ωij
(
A&lowast;p
</p>
<p>)
= aikdXkj
</p>
<p>(
XαβA
</p>
<p>β
γ &part;
</p>
<p>γ
α
</p>
<p>)
+ bijkdxk
</p>
<p>(
XαβA
</p>
<p>β
γ &part;
</p>
<p>γ
α
</p>
<p>)
</p>
<p>= aikXαβAβγ dXkj
(
&part;γα
</p>
<p>)
= aikXαβAβγ δkαδ
</p>
<p>γ
</p>
<p>j = aikXkβA
β
j .
</p>
<p>For the equality to hold for all Aij , we must have a
i
kX
</p>
<p>k
β = δiβ , i.e., that aik =
</p>
<p>Y ik , where Y
i
k is the inverse of X
</p>
<p>i
k . So, we write
</p>
<p>ωij = Y ikdXkj + bijkdxk. (36.16)
</p>
<p>To get more insight into the composition of bijk , let us try to find &nabla;&part;j &part;i
using (36.10). To this end, let X&lowast;j be the horizontal lift of &part;j . The general
form of X&lowast;j is
</p>
<p>X&lowast;j = λij&part;i +Λ
β
jγ &part;
</p>
<p>γ
β .
</p>
<p>Since π&lowast;(X&lowast;j ) = &part;j and π&lowast;(&part;
γ
β ) = 0, we get λij = δij . Since ωωω(X&lowast;j ) = 0 =
</p>
<p>ωim(X
&lowast;
j ), we get
</p>
<p>0 = ωim
(
X&lowast;j
</p>
<p>)
= Y ikdXkm
</p>
<p>(
Λ
</p>
<p>β
jγ &part;
</p>
<p>γ
β
</p>
<p>)
+ bimkdxk(&part;j )
</p>
<p>= Y ikΛ
β
jγ δ
</p>
<p>k
βδ
</p>
<p>γ
m + bimkδkj = Y ikΛkjm + bimj .
</p>
<p>Multiply both sides by Xαi (and sum over i, of course) to get
</p>
<p>0 =Xαi Y ikΛkjm + bimjXαi =Λαjm + bimjXαi or Λαjm =&minus;bimjXαi
and
</p>
<p>X&lowast;j = &part;j &minus; bkmjXαk &part;mα . (36.17)
With a similar expression for X&lowast;i . To use (36.10), we need θθθ(X
</p>
<p>&lowast;
i ), which
</p>
<p>we can obtain using Eq. (36.15), noting that θθθ acts only on the horizontal
component of X&lowast;i :
</p>
<p>θθθ
(
X&lowast;i
</p>
<p>)
= Y βk dxk(&part;i)êβ = Y
</p>
<p>β
k δ
</p>
<p>k
i êβ = Y
</p>
<p>β
i êβ .
</p>
<p>As we apply (36.17) to this expression, we must keep in mind that Y βi , being
</p>
<p>the inverse of Xβi , is independent of x
j . Therefore, only the second term of
</p>
<p>(36.17) acts on Y βi . Then
</p>
<p>X&lowast;j
(
θθθ
(
X&lowast;i
</p>
<p>))
=&minus;bkmjXαk &part;mα
</p>
<p>(
Y
β
i êβ
</p>
<p>)
=&minus;bkmjXαk
</p>
<p>(
&part;mα Y
</p>
<p>β
i
</p>
<p>)
êβ
</p>
<p>= bkmjXαk
(
Ymi Y
</p>
<p>β
α
</p>
<p>)
êβ = bkmjYmi êk.</p>
<p/>
</div>
<div class="page"><p/>
<p>1130 36 Differential Geometry
</p>
<p>Finally, we apply p on this and use pêk =Xlk&part;l to obtain
</p>
<p>&nabla;&part;j &part;i = bkmjYmi Xlk&part;l &equiv; Ŵlj i&part;l, (36.18)
</p>
<p>where in the last step, we introduced the Riemann-Christoffel symbols.2
</p>
<p>These symbols make sense, because &nabla;&part;j &part;i is a vector field, which could be
expanded in terms of the basis {&part;l}. The Riemann-Christoffel symbols are
simply the coefficients of the expansion. In terms of these symbols, bijk can
</p>
<p>be written as bijk = ŴlkmY il Xmj and Eq. (36.16) can be expressed as
</p>
<p>ωij = Y ik
(
dXkj + ŴklmXmj dxl
</p>
<p>)
. (36.19)
</p>
<p>Riemann-Christoffel
</p>
<p>symbols
</p>
<p>The Riemann-Christoffel symbols could also be obtained from the con-
nection form. From the 1-form ω on P , we define a local gl(n,R)-valued
1-form ωU on M as follows. At each point x on M let σ be the section
sending x to the linear frame (&part;1, . . . , &part;n). A general section sends x to
(X1i &part;1, . . . ,X
</p>
<p>n
i &part;n) or equivalently, it sends x to the point in P with coor-
</p>
<p>dinates (xi,Xjk ). The particular section we are considering sends x to the
</p>
<p>point in P with coordinates (xi, δjk ). Moreover, we define ωU = σ &lowast;ω. Then
ωU is obtained from Eq. (36.19) by setting Xmj = δmj , Y ik = δik , and dXkj = 0.
Therefore,
</p>
<p>(ωU )
i
j = Ŵiljdxl . (36.20)
</p>
<p>We often omit the subscript U when there is no risk of confusion.
</p>
<p>Historical Notes
</p>
<p>Elwin Bruno Christoffel (1829&ndash;1900) came from a family in the cloth trade. He attended
an elementary school in Montjoie (which was renamed Monschau in 1918) but then spent
a number of years being tutored at home in languages, mathematics, and classics. He
attended secondary schools from 1844 until 1849. At first he studied at the Jesuit gym-
nasium in Cologne but moved to the Friedrich-Wilhelms Gymnasium in the same town
for at least the three final years of his school education. He was awarded the final school
certificate with a distinction in 1849. The next year he went to the University of Berlin
and studied under a number of distinguished mathematicians, including Dirichlet.
</p>
<p>Elwin Bruno Christoffel
</p>
<p>1829&ndash;1900
</p>
<p>After one year of military service in the Guards Artillery Brigade, he returned to Berlin
to study for his doctorate, which was awarded in 1856 with a dissertation on the mo-
tion of electricity in homogeneous bodies. His examiners included mathematicians and
physicists, Kummer being one of the mathematics examiners.
At this point Christoffel spent three years outside the academic world. He returned to
Montjoie, where his mother was in poor health, but read widely from the works of Dirich-
let, Riemann, and Cauchy. It has been suggested that this period of academic isolation
had a major effect on his personality and on his independent approach towards mathemat-
ics. It was during this time that he published his first two papers on numerical integration,
in 1858, in which he generalized Gauss&rsquo;s method of quadrature and expressed the poly-
nomials that are involved as a determinant. This is now called Christoffel&rsquo;s theorem.
In 1859 Christoffel took the qualifying examination to become a university teacher and
was appointed a lecturer at the University of Berlin. Four years later, he was appointed to
a chair at the Polytechnicum in Zurich, filling the post left vacant when Dedekind went
</p>
<p>2The reader is cautioned about the order of the lower indices, as it is different in different
books.</p>
<p/>
</div>
<div class="page"><p/>
<p>36.2 Linear Connections 1131
</p>
<p>to Brunswick. Christoffel was to have a huge influence on mathematics at the Polytech-
nicum, setting up an institute for mathematics and the natural sciences there.
In 1868 Christoffel was offered the chair of mathematics at the Gewerbsakademie in
Berlin, which is now the University of Technology of Berlin. However, after three years
at the Gewerbsakademie in Berlin, Christoffel moved to the University of Strasbourg as
the chair of mathematics, a post he held until he was forced to retire due to ill health in
1892.
Some of Christoffel&rsquo;s early work was on conformal mappings of a simply connected re-
gion bounded by polygons onto a circle. He also wrote important papers that contributed
to the development of the tensor calculus of Gregorio Ricci-Curbastro and Tullio Levi-
Civita. The Christoffel symbols that he introduced are fundamental in the study of tensor
analysis. The Christoffel reduction theorem, so named by Klein, solves the local equiva-
lence problem for two quadratic differential forms. The procedure Christoffel employed
in his solution of the equivalence problem is what Ricci later called covariant differen-
tiation; Christoffel also used the latter concept to define the basic Riemann&ndash;Christoffel
curvature tensor. His approach allowed Ricci and Levi&ndash;Civita to develop a coordinate-
free differential calculus which Einstein, with the help of Grossmann, turned into the
tensor analysis, the mathematical foundation of general relativity.
</p>
<p>The Riemann-Christoffel symbols are functions of local coordinates.
A change of coordinates transforms the symbols according to a rule that
can be easily worked out. In fact, if x̄α is the new coordinates and Ŵ
</p>
<p>α
</p>
<p>βγ is
</p>
<p>the symbols in the new coordinate system, then &nabla;&part;̄β &part;̄γ = Ŵ
α
</p>
<p>βγ &part;̄α . To find
</p>
<p>Ŵ
α
</p>
<p>βγ in terms of the old symbols, substitute (&part;̄αx
i)&part;i for &part;̄α , etc.,
</p>
<p>&nabla; &part;xi
&part;x̄β
</p>
<p>&part;i
</p>
<p>(
&part;xj
</p>
<p>&part;x̄γ
&part;j
</p>
<p>)
= Ŵαβγ
</p>
<p>&part;xk
</p>
<p>&part;x̄α
&part;k,
</p>
<p>then use Proposition 36.2.11 to expand the left-hand side. After some simple
manipulation, which we leave for the reader, we obtain
</p>
<p>Ŵ
α
</p>
<p>βγ = Ŵijk
&part;xj
</p>
<p>&part;x̄β
</p>
<p>&part;xk
</p>
<p>&part;x̄γ
</p>
<p>&part;x̄α
</p>
<p>&part;xi
+ &part;
</p>
<p>2xi
</p>
<p>&part;x̄β&part;x̄γ
</p>
<p>&part;x̄α
</p>
<p>&part;xi
(36.21)
</p>
<p>Because of the presence of the second term, Riemann-Christoffel symbols
are not components of a tensor.
</p>
<p>From the definition of the Riemann-Christoffel symbols, the components
of a connection form, we deduced their transformation properties. It turns
out that
</p>
<p>Theorem 36.2.19 A set of functions Ŵijk , which transform according to
Eq. (36.21) under a change of coordinates, define a unique connection
whose components with respect to the coordinates {xi} are Ŵijk . Further-
more, the connection form ωωω = ωijE
</p>
<p>j
</p>
<p>i is given in terms of the local coordi-
nate system by
</p>
<p>ωij = Y ik
(
dXkj + ŴklmXmj dxl
</p>
<p>)
</p>
<p>Proof See [Koba 63, pp. 142&ndash;143]. �
</p>
<p>Define the components of torsion and curvature tensors by
</p>
<p>T(&part;i, &part;j )= T kij&part;k, R(&part;i, &part;j )&part;k =Rlkij&part;l . (36.22)</p>
<p/>
</div>
<div class="page"><p/>
<p>1132 36 Differential Geometry
</p>
<p>Then, using Theorem 36.2.16 one can easily obtain the following
</p>
<p>Box 36.2.20 The components of torsion and curvature tensors are
given in terms of the Christoffel symbols:
</p>
<p>T kij = Ŵkij &minus; Ŵkji
Rijkl = &part;kŴilj &minus; &part;lŴikj + ŴmljŴikm &minus; ŴmkjŴilm.
</p>
<p>(36.23)
</p>
<p>In particular, Ŵkji = Ŵkij if the torsion tensor vanishes.
</p>
<p>Equation (36.20) pulls down the connection 1-form from P to M . One
can pull down the curvature 2-form as well. Then Eq. (36.5) gives
</p>
<p>(ΩU )
i
j = d(ωU )ij + (ωU )ik &and; (ωU )kj . (36.24)
</p>
<p>As indicated above, we often omit the subscript U . Problem 36.11 shows
that
</p>
<p>Ω ij =
1
</p>
<p>2
Rijkldx
</p>
<p>k &and; dxl (36.25)
</p>
<p>where Rijkl is as given in Eq. (36.23).
</p>
<p>36.2.4 General Basis
</p>
<p>The coordinate expressions derived above express the components of forms
and fields in a coordinate basis. We need not confine ourselves to coordinate
bases. In fact, they are not always the most convenient bases to use. We can
work in a basis {ei} and its dual {ǫǫǫj }. Then the Riemann-Christofel symbols
are defined as
</p>
<p>&nabla;ei ej = ekŴkij . (36.26)
It has to be emphasized that, even in the absence of torsion, in a general
basis, Ŵkij �= Ŵkji . Only in a coordinate basis does this symmetry hold.
</p>
<p>Example 36.2.21 Consider two bases {ei} and {ei&prime;}. Write the primed basis
in terms of the other: ei&prime; =Rji&prime;ej . Then
</p>
<p>ek&prime;Ŵ
k&prime;
i&prime;j &prime; &equiv;&nabla;ei&prime; ej &prime; =&nabla;Rl
</p>
<p>i&prime;el
</p>
<p>(
R
j
</p>
<p>j &prime;ej
)
=Rli&prime;&nabla;el
</p>
<p>(
R
j
</p>
<p>j &prime;ej
)
</p>
<p>=Rli&prime;
{
R
j
</p>
<p>j &prime;&nabla;el (ej )+&nabla;el
(
R
j
</p>
<p>j &prime;
)
ej
}
</p>
<p>=Rli&prime;R
j
</p>
<p>j &prime;emŴ
m
lj +Rli&prime; el
</p>
<p>(
Rmj &prime;
</p>
<p>)
︸ ︷︷ ︸
&equiv;Rm
</p>
<p>j &prime;,l
</p>
<p>em.
</p>
<p>Writing ek&prime; = Rmk&prime;em on the LHS, equating the components on both sides,Connection coefficients
are not tensors! and multiplying both sides by the inverse of the transformation matrix R, we</p>
<p/>
</div>
<div class="page"><p/>
<p>36.2 Linear Connections 1133
</p>
<p>obtain
</p>
<p>Ŵk
&prime;
i&prime;j &prime; =R
</p>
<p>k&prime;
mR
</p>
<p>l
i&prime;R
</p>
<p>j
</p>
<p>j &prime;Ŵ
m
lj︸ ︷︷ ︸
</p>
<p>how a (1,2)-tensor
transforms
</p>
<p>+Rk&prime;mRli&prime;Rmj &prime;,l︸ ︷︷ ︸
nontensorial
</p>
<p>term
</p>
<p>, (36.27)
</p>
<p>where Rk
&prime;
m
&equiv; (R&minus;1)k&prime;m. Equation (36.27) shows once again that the con-
</p>
<p>nection coefficients are not tensors. Equation (36.21) is a special case of
(36.27).
</p>
<p>Applying &nabla;ei to both sides of δ
j
m = 〈em,ǫǫǫj 〉, we obtain
</p>
<p>&nabla;eiǫǫǫj =&minus;Ŵ
j
</p>
<p>ikǫǫǫ
k. (36.28)
</p>
<p>Since an arbitrary tensor of a given kind can be expressed as a linear com-
bination of tensor product of vectors and 1-forms, Eqs. (36.26) and (36.28),
plus the assumed derivation property of &nabla;u, is sufficient to uniquely define
the action of &nabla;u on any tensor and for any vector field u.
</p>
<p>Let T be a tensor of type (r, s). The covariant differential of Defini-
tion 36.2.14, &nabla; : T rs (M) &rarr; T rs+1(M), which is sometimes called the gen-
eralized gradient operator, when acting on T, adds an extra lower index. In gradient operator for
</p>
<p>tensorswhat follows, we want to find the components of &nabla;T in terms of the compo-
nents of T. If
</p>
<p>T= T i1...irj1...js ei1 &otimes; &middot; &middot; &middot; &otimes; eir &otimes; ǫǫǫ
j1 &otimes; &middot; &middot; &middot; &otimes; ǫǫǫjs ,
</p>
<p>then, following the customary practice of putting the extra lower index after
a semicolon, we write significance of
</p>
<p>semicolon in
</p>
<p>components of the
</p>
<p>covariant derivative of a
</p>
<p>tensor
</p>
<p>&nabla;T&equiv; T i1...ir
j1...js ;kei1 &otimes; &middot; &middot; &middot; &otimes; eir &otimes; ǫǫǫ
</p>
<p>j1 &otimes; &middot; &middot; &middot; &otimes; ǫǫǫjs &otimes; ǫǫǫk, (36.29)
</p>
<p>and, with u = ukek ,
</p>
<p>&nabla;uT= T i1...irj1...js ;ku
kei1 &otimes; &middot; &middot; &middot; &otimes; eir &otimes; ǫǫǫj1 &otimes; &middot; &middot; &middot; &otimes; ǫǫǫjs . (36.30)
</p>
<p>Using these relations, we can calculate the components of the covariant
derivative of a general tensor. It is clear that if we use ek instead of u, we
obtain the kth component of the covariant derivative. So, on the one hand,
we have
</p>
<p>&nabla;ekT= T i1...irj1...js ;kei1 &otimes; &middot; &middot; &middot; &otimes; eir &otimes; ǫǫǫ
j1 &otimes; &middot; &middot; &middot; &otimes; ǫǫǫjs , (36.31)
</p>
<p>and on the other hand,
</p>
<p>&nabla;ekT=&nabla;ek
(
T
i1...ir
j1...js
</p>
<p>ei1 &otimes; &middot; &middot; &middot; &otimes; eir &otimes; ǫǫǫj1 &otimes; &middot; &middot; &middot; &otimes; ǫǫǫjs
)
</p>
<p>= T i1...irj1...js ,kei1 &otimes; &middot; &middot; &middot; &otimes; eir &otimes; ǫǫǫ
j1 &otimes; &middot; &middot; &middot; &otimes; ǫǫǫjs
</p>
<p>+ T i1...irj1...js
r&sum;
</p>
<p>m=1
ei1 &otimes; &middot; &middot; &middot; &otimes; &nabla;ekeim︸ ︷︷ ︸
</p>
<p>enŴnkim
</p>
<p>&otimes; . . . eir &otimes; ǫǫǫj1 &otimes; &middot; &middot; &middot; &otimes; ǫǫǫjs</p>
<p/>
</div>
<div class="page"><p/>
<p>1134 36 Differential Geometry
</p>
<p>+ T i1...irj1...js
s&sum;
</p>
<p>m=1
ei1 &otimes; &middot; &middot; &middot; &otimes; eir &otimes; ǫǫǫj1 &otimes; &middot; &middot; &middot; &otimes; &nabla;ekǫǫǫjm︸ ︷︷ ︸
</p>
<p>&minus;Ŵjmknǫǫǫn
by (36.28)
</p>
<p>&middot; &middot; &middot; &otimes; ǫǫǫjs ,
</p>
<p>(36.32)
</p>
<p>where T i1...irj1...js ,k &equiv; ek(T
i1...ir
j1...js
</p>
<p>). Equating the components of Eqs. (36.31) and
(36.32) yieldscomponents of the
</p>
<p>covariant derivative of a
</p>
<p>tensor
T
i1...ir
j1...js ;k = T
</p>
<p>i1...ir
j1...js ,k
</p>
<p>+
r&sum;
</p>
<p>m=1
T
i1...im&minus;1nim+1...ir
j1...jm&minus;1jmjm+1...jsŴ
</p>
<p>im
kn
</p>
<p>&minus;
s&sum;
</p>
<p>m=1
T
i1...im&minus;1imim+1...ir
j1...jm&minus;1njm+1...js Ŵ
</p>
<p>n
kjm
</p>
<p>, (36.33)
</p>
<p>where only the sum over the subindex m has been explicitly displayed;
the (hidden) sum over repeated indices is, as always, understood. Equa-
tion (36.33) is (36.6) written in terms of components.
</p>
<p>If u happens to be tangent to a curve t �&rarr; γ (t), Eq. (36.30) is written as
</p>
<p>&nabla;uT=
DT
</p>
<p>i1...ir
j1...js
</p>
<p>dt
ei1 &otimes; &middot; &middot; &middot; &otimes; eir &otimes; ǫǫǫj1 &otimes; &middot; &middot; &middot; &otimes; ǫǫǫjs , (36.34)
</p>
<p>where DT i1...irj1...js /dt &equiv; T
i1...ir
j1...js ;ku
</p>
<p>k . In a coordinate frame, with ui = ẋi =
dxi/dt , Eqs. (36.30) and (36.33) give
</p>
<p>DT
i1...ir
j1...js
</p>
<p>dt
= T i1...ir
</p>
<p>j1...js ;k
dxk
</p>
<p>dt
=
</p>
<p>dT
i1...ir
j1...js
</p>
<p>dt
+
</p>
<p>r&sum;
</p>
<p>m=1
T
i1...im&minus;1nim+1...ir
j1...jm&minus;1jmjm+1...jsŴ
</p>
<p>im
kn
</p>
<p>dxk
</p>
<p>dt
</p>
<p>&minus;
s&sum;
</p>
<p>m=1
T
i1...im&minus;1imim+1...ir
j1...jm&minus;1njm+1...js Ŵ
</p>
<p>n
kjm
</p>
<p>dxk
</p>
<p>dt
(36.35)
</p>
<p>For the case of a vector (36.35) becomes
</p>
<p>Dvk
</p>
<p>dt
= dv
</p>
<p>k
</p>
<p>dt
+ vjŴkij
</p>
<p>dxi
</p>
<p>dt
. (36.36)
</p>
<p>This is an important equation, to which we shall return shortly.
With the generalized gradient operator defined, we can construct the di-
</p>
<p>vergence of a tensor just as in vector analysis. Given a vector, the divergence
operator &nabla;&middot; acts on it and gives a scalar, or, in the language of tensor anal-
ysis, it lowers the upper indices by 1. This takes place by differentiating
components and contracting the upper index with the newly introduced in-
dex of differentiation. The divergence of an arbitrary tensor is defined indivergence of a tensor
</p>
<p>field precisely the same way:</p>
<p/>
</div>
<div class="page"><p/>
<p>36.2 Linear Connections 1135
</p>
<p>Definition 36.2.22 Given a tensor field T, define its divergence &nabla; &middot;T
to be the tensor obtained from &nabla;T by contracting the last upper index
with the covariant derivative index. In components,
</p>
<p>(&nabla; &middot; T)i1...ir&minus;1j1...js = T
i1...ir&minus;1k
j1...js ;k .
</p>
<p>Example 36.2.23 There is a useful relation between the covariant and the
Lie derivative that we derive now. First, let T be of type (2,0) and write it in
some frame as T= T ij ei &otimes; ej . Apply the covariant derivative with respect
to u to both sides to obtain
</p>
<p>&nabla;uT= u
(
T ij
</p>
<p>)
ei &otimes; ej + T ij (&nabla;uei)&otimes; ej + T ij ei &otimes; (&nabla;uej ).
</p>
<p>Similarly, derivation of the relation
between the Lie and the
</p>
<p>covariant derivativesLuT= u
(
T ij
</p>
<p>)
ei &otimes; ej + T ij (Luei)&otimes; ej + T ij ei &otimes; (Luej ).
</p>
<p>Now use Luej = [u, ej ] = &nabla;uej &minus;&nabla;ej u to get
</p>
<p>LuT=&nabla;uT&minus; T ij
[
(&nabla;eiu)&otimes; ej + ei &otimes; (&nabla;ej u)
</p>
<p>]
. (36.37)
</p>
<p>On the other hand, if we apply &nabla;u and Lu to both sides of δij = 〈ǫǫǫi, ej 〉
and use [u, ei] = &nabla;uei &minus;&nabla;eiu, we obtain
</p>
<p>&nabla;uǫǫǫi = Luǫǫǫi &minus; (&nabla;ej u)iǫǫǫj .
</p>
<p>It follows that for T= Tijǫǫǫi &otimes; ǫǫǫj , we have
</p>
<p>LuT=&nabla;uT+ Tij
[
(&nabla;eku)iǫǫǫk &otimes; ǫǫǫj + (&nabla;eku)jǫǫǫi &otimes; ǫǫǫk
</p>
<p>]
. (36.38)
</p>
<p>One can use Eqs. (36.37) and (36.38) to generalize to a tensor of type (r, s).
</p>
<p>Example 36.2.24 Let f be a function on M . Then &nabla;f is a one form. Call
it φ. In a local coordinate system, it can be written as φ = φidxi , where
</p>
<p>φi = φ(&part;i)=&nabla;f (&part;i)=&nabla;&part;if = &part;if &equiv;
&part;f
</p>
<p>&part;xi
.
</p>
<p>Noting that (&nabla;f )i &equiv; f;i , &nabla;f = f;idxi , and (&nabla;2f )ij = f;ij , let us first find
the covariant derivative of φ. &nabla;φ is a 2-form, whose components can be
found as follows:
</p>
<p>(&nabla;φ)ij =&nabla;φ(&part;i, &part;j )=&nabla;&part;j
(
φ(&part;i)
</p>
<p>)
&minus; φ(&nabla;&part;i&part;j )
</p>
<p>= &part;j (φi)&minus; φ
(
Ŵkij&part;k
</p>
<p>)
= &part;j (&part;if )&minus; Ŵkijφk
</p>
<p>= &part;
2f
</p>
<p>&part;xj&part;xi
&minus; Ŵkij&part;kf =
</p>
<p>&part;2f
</p>
<p>&part;xj&part;xi
&minus; Ŵkij
</p>
<p>&part;f
</p>
<p>&part;xk
,</p>
<p/>
</div>
<div class="page"><p/>
<p>1136 36 Differential Geometry
</p>
<p>where we used Eq. (36.6). We rewrite this as
</p>
<p>f;ij =
&part;2f
</p>
<p>&part;xj&part;xi
&minus; Ŵkij
</p>
<p>&part;f
</p>
<p>&part;xk
. (36.39)
</p>
<p>Reversing the order of indices, we get
</p>
<p>f;ji =
&part;2f
</p>
<p>&part;xi&part;xj
&minus; Ŵkji
</p>
<p>&part;f
</p>
<p>&part;xk
. (36.40)
</p>
<p>Subtracting (36.39) from (36.40) and using (36.23), we obtain
</p>
<p>f;ij &minus; f;ji =
(
Ŵkij &minus; Ŵkji
</p>
<p>) &part;f
&part;xk
</p>
<p>= T kij&part;kf. (36.41)
</p>
<p>Thus, only if the torsion tensor vanishes are the mixed &ldquo;partial&rdquo; covariant
derivatives equal.
</p>
<p>Now we want to find the difference between the mixed second &ldquo;partial&rdquo;
covariant derivatives of a vector field Z = ξ k&part;k . It is more instructive to use
general vectors and then specialize to coordinate vector fields. We are thus
interested in &nabla;2Z(Y,X)&minus; &nabla;2Z(X,Y). Let ψ &equiv; &nabla;Z. From Eq. (36.6), we
have
</p>
<p>&nabla;ψ(X,Y)=&nabla;Y
(
ψ(X)
</p>
<p>)
&minus;ψ(&nabla;YX)
</p>
<p>=&nabla;Y
(
&nabla;Z(X)
</p>
<p>)
&minus;&nabla;Z(&nabla;YX)
</p>
<p>=&nabla;Y&nabla;XZ &minus;&nabla;&nabla;Y XZ.
</p>
<p>Switching X and Y and subtracting, we get
</p>
<p>&nabla;2Z(Y,X)&minus;&nabla;2Z(X,Y)=&nabla;X&nabla;YZ &minus;&nabla;Y&nabla;XZ +&nabla;&nabla;Y XZ &minus;&nabla;&nabla;XYZ
= [&nabla;X,&nabla;Y ]Z &minus;&nabla;&nabla;XY&minus;&nabla;Y XZ
= [&nabla;X,&nabla;Y ]Z &minus;&nabla;T(X,Y)+[X,Y]Z,
</p>
<p>where we used Theorem 36.2.16. We thus have
</p>
<p>&nabla;2Z(Y,X)&minus;&nabla;2Z(X,Y)= [&nabla;X,&nabla;Y ]Z &minus;&nabla;[X,Y]Z &minus;&nabla;T(X,Y)Z,
</p>
<p>or, using Theorem 36.2.16 again,
</p>
<p>&nabla;2Z(Y,X)&minus;&nabla;2Z(X,Y)= R(X,Y)Z +&nabla;T(Y,X)Z. (36.42)
</p>
<p>Substituting &part;i and &part;j for Y and X in the equation above, we can get
</p>
<p>ξ i;lk &minus; ξ i;kl =Rijklξ j + T
j
</p>
<p>lkξ
i
;j . (36.43)
</p>
<p>We leave this as an exercise for the reader.</p>
<p/>
</div>
<div class="page"><p/>
<p>36.3 Geodesics 1137
</p>
<p>36.3 Geodesics
</p>
<p>Let γ be a curve in M . Denote γ (t) by xt , so that γ (0)= x0 &isin;M . Let γ ts be
the parallel displacement along the curve γ in M from Txt (M) to Txs (M).
In particular, consider γ t0 , the parallel displacement from Txt (M) to Tx0(M)
along γ . It is natural to associate the zero vector in Txt (M) with xt .
</p>
<p>3 As t
varies, the zero vector also varies, and by the parallel displacement γ t0 , one
can monitor how the image of xt &ldquo;develops&rdquo; in Tx0(M).
</p>
<p>Definition 36.3.1 The development of the curve γ in M into Tx0(M) is the development of a curve
curve γ t0(xt ) in Tx0(M).
</p>
<p>The following theorem, whose proof can be found in [Koba 63, pp 131&ndash;
132], relates the tangent to the development of a curve and the parallel dis-
placement of its tangent.
</p>
<p>Theorem 36.3.2 Let γ be a curve in M and Yt = γ t0(ẋt ). Let Ct = γ t0(xt )
be the development of γ in M into Tx0(M). Then
</p>
<p>dC
</p>
<p>dt
= Yt .
</p>
<p>This theorem states that the tangent to the development of a curve is the
same as the parallel displacement of the tangent to the curve. In other words,
γ t0 &ldquo;develops&rdquo; not only the curve, but its tangent at every point of the curve.
</p>
<p>An interesting consequence of this theorem is that if ẋt is parallel
along γ , then Yt is independent of t , i.e., Yt is constant, say Yt = a. Then,
Ct = at + b. Hence, we have
</p>
<p>Corollary 36.3.3 The development of γ in M into Tx0(M) is a
straight line iff ẋt is parallel along γ .
</p>
<p>Curves in manifolds with a given linear connection bend for two reasons:
one is because the curve itself goes back and forth in the manifold; the other
is the inherent bending of the manifold itself. The straightest possible lines
in a manifold are those which bend only because of the inherent bending
of the manifold. Given any curve, we can gauge its bending by parallel dis-
placement of vector fields along that curve. If the vector field has a vanishing
covariant derivative, it is said to be parallel along the curve. However, that
says nothing about how &ldquo;curvy&rdquo; the curve itself is.
</p>
<p>To get further insight, let&rsquo;s look at the familiar flat space. In the flat space
of a large sheet of paper, construction of a straight line in a given direction
starting at a given point P0 is done by laying down the end of a vector
(a straight edge) at P0 pointing in the given direction, connecting P0 to a
</p>
<p>3This association becomes plausible if one specializes to two dimensions and notes that
the plane Txt (M) touches M at xt , the natural origin of the plane.</p>
<p/>
</div>
<div class="page"><p/>
<p>1138 36 Differential Geometry
</p>
<p>neighboring point P1 along the vector, moving the vector parallel to itself
to P1, connecting P1 to a neighboring point P2, and continuing the process.
In the language of the machinery of the covariant derivative, we might say
that a straight line is constructed by transporting the tangent vector parallel
to itself.
</p>
<p>Definition 36.3.4 Let M be a manifold and γ a curve in M . Then γ is
called a geodesic of M if the tangent vector ẋt at every point of γ is parallelgeodesics defined
displaced along the curve: &nabla;ẋt ẋt = 0.
</p>
<p>Since the definition is in terms of the parameter t , the parametrization
of the curve becomes important. Such a parameter, if it exists is called an
affine parameter.affine parameter
</p>
<p>It follows from Eq. (36.36)&mdash;with vk = uk = dxk/dt&mdash;that a geodesic
geodesic equation
</p>
<p>curve satisfies the following DE:
</p>
<p>d2xk
</p>
<p>dt2
+ Ŵkij
</p>
<p>dxi
</p>
<p>dt
</p>
<p>dxj
</p>
<p>dt
= 0. (36.44)
</p>
<p>This second-order DE, called the geodesic equation, will have a unique
solution if xi(0) and ẋi(0), i.e., the initial point and the initial direction, are
given. Thus,
</p>
<p>Theorem 36.3.5 Through a given point and in a given direction
passes only one geodesic curve.
</p>
<p>If s(t) is another parametrization of the geodesic curve, then a simple
calculation shows that
</p>
<p>0 = d
2xk
</p>
<p>dt2
+ Ŵkij
</p>
<p>dxi
</p>
<p>dt
</p>
<p>dxj
</p>
<p>dt
=
(
d2xk
</p>
<p>ds2
+ Ŵkij
</p>
<p>dxi
</p>
<p>ds
</p>
<p>dxj
</p>
<p>ds
</p>
<p>)
</p>
<p>︸ ︷︷ ︸
=0
</p>
<p>(
s&prime;(t)
</p>
<p>)2 + dx
k
</p>
<p>ds
s&prime;&prime;(t).
</p>
<p>This requires s&prime;&prime;(t) to be zero, or s = αt + β , with α,β &isin;R.
Corollary 36.3.3 leads immediately to the following:
</p>
<p>Proposition 36.3.6 A curve through x &isin;M is a geodesic iff its devel-
opment into Tx(M) is a straight line.
</p>
<p>36.3.1 Riemann Normal Coordinates
</p>
<p>Starting with a point P of an n-dimensional manifold M on which a co-
variant derivative is defined, we can construct a unique geodesic in every
direction, i.e., for every vector in TP (M). By parallel transportation of the</p>
<p/>
</div>
<div class="page"><p/>
<p>36.3 Geodesics 1139
</p>
<p>tangent vectors at P , we can construct a vector field in a neighborhood of
P : The value of the vector field at Q&mdash;assumed to be close enough to P&mdash;
is the tangent at Q on the geodesic starting at P and passing through Q.4
</p>
<p>The vector field so obtained makes it possible to define an exponential map
from the tangent space to the manifold. In fact, the integral curve exp(tX)
of any tangent vector X in TP (M) is simply the geodesic associated with
the vector.
</p>
<p>The uniqueness of the geodesics establishes a bijection (in fact, a diffeo-
morphism) between a neighborhood of the origin of TP (M) and a neighbor-
hood of P in M . This diffeomorphism can be used to assign coordinates to
all points in the vicinity of P . Recall that a coordinate is a smooth bijection
from M to Rn. Now choose a basis for TP (M) and associate the compo-
nents of tX in this basis to the points on the geodesic exp(tX). Specifically,
if {ai}ni=1 are the components of X in the chosen basis, then Riemann normal
</p>
<p>coordinates
xi(t)= ai t, i = 1,2, . . . , n,
</p>
<p>are the so-called Riemann normal coordinates (RNCs) of points on the
geodesic of X. The geodesic equations in these coordinates become
</p>
<p>Ŵkija
iaj = 0 &rArr; Ŵkji + Ŵkij = 0.
</p>
<p>In particular, if the torsion vanishes, then Ŵkji is symmetric in i and j .
Hence, we have the following:
</p>
<p>Proposition 36.3.7 The connection coefficients at a point P &isin;M vanish in
the Riemann normal coordinates at P if the torsion vanishes.
</p>
<p>Using Eq. (36.33), we immediately obtain the following:
</p>
<p>Corollary 36.3.8 Let T be a tensor field onM with components T i1...irj1...js with
</p>
<p>respect to a Riemann normal coordinate system {xi} at P . Then
</p>
<p>T
i1...ir
j1...js ;k =
</p>
<p>&part;
</p>
<p>&part;xk
T
i1...ir
j1...js
</p>
<p>if the torsion vanishes.
</p>
<p>Riemann normal coordinates are very useful in establishing tensor equa-
tions. This is because two tensors are identical if and only if their com-
ponents are the same in any coordinate frame. Therefore, to show that two
tensors fields are equal, we pick an arbitrary point in M , erect a set of RNCs,
and show that the components of the tensors are equal. Since the connection
coefficients vanish in an RNC system, and covariant derivatives are the same
as ordinary derivatives, tensor manipulations can be simplified considerably.
</p>
<p>4We are assuming that through any two neighboring points one can always draw a
geodesic. For a proof see [Koba 63, pp. 172&ndash;175].</p>
<p/>
</div>
<div class="page"><p/>
<p>1140 36 Differential Geometry
</p>
<p>For example, the components of the curvature tensor in RNCs are
</p>
<p>Rijkl =
&part;Ŵilj
</p>
<p>&part;xk
&minus;
</p>
<p>&part;Ŵikj
</p>
<p>&part;xl
. (36.45)
</p>
<p>This is not a tensor relation&mdash;the RHS is not a tensor in a general coordinate
system. However, if we establish a relation involving the components of the
curvature tensor alone, then that relation will hold in all coordinates, i.e., it
is a tensor relation. For instance, from the equation above one immediately
obtains
</p>
<p>Rijkl +Riljk +Riklj = 0.
Since this involves only a tensor, it must hold in all coordinate frames. This
is the coordinate expression of Bianchi&rsquo;s first identity of Eq. (36.14).5
</p>
<p>Example 36.3.9 Differentiate the second equation in (36.23) with respect
to xm and evaluate the result in RNC to get
</p>
<p>Rijkl;m =Rijkl,m = Ŵilj,km &minus; Ŵikj,lm.
</p>
<p>From this relation and Ŵij l,km = Ŵij l,mk , we obtain the coordinate expres-
sion of Bianchi&rsquo;s second identity of Theorem 36.2.17:
</p>
<p>Rijkl;m +Rijmk;l +Rij lm;k = 0 and Rij [kl;m] = 0. (36.46)
</p>
<p>In Einstein&rsquo;s general relativity, this identity is the analogue of Maxwell&rsquo;s
pair of homogeneous equations: Fαβ,γ + Fγα,β + Fβγ,α = 0.
</p>
<p>Using Proposition 36.3.7, we establish the following tensor identity,
which although derived in Riemann normal coordinates, is true in general.
</p>
<p>Corollary 36.3.10 Let ωωω be a differential form on M . If the torsion van-
ishes, then
</p>
<p>dωωω=A(&nabla;ωωω),
where A is the antisymmetrizer introduced in Eq. (26.14).
</p>
<p>36.4 Problems
</p>
<p>36.1 Use the fact that Rg&lowast;X is a vector at pg &isin; L(M) to show that the
canonical 1-form of L(M) is a tensorial 1-form of type (GL(n,R),Rn).
</p>
<p>36.2 Derive the two structure equations of (36.5). Hint: For the second
equation, use (34.13) with structure constants coming from Example 29.2.7.
</p>
<p>5See also Problem 36.6 for both of Bianchi&rsquo;s identities.</p>
<p/>
</div>
<div class="page"><p/>
<p>36.4 Problems 1141
</p>
<p>36.3 Let Y be a constant vector in (d) of Proposition 36.2.11 to show that
&nabla;Xf = Xf .
</p>
<p>36.4 Derive Eq. (36.6).
</p>
<p>36.5 In this problem, you are going to prove Bianchi&rsquo;s second identity in
terms of curvature tensor.
</p>
<p>(a) Show that
</p>
<p>Dω���
(
X&lowast;,Y&lowast;,Z&lowast;
</p>
<p>)
= Cyc
</p>
<p>(
X&lowast;
</p>
<p>(
���
(
Y&lowast;,Z&lowast;
</p>
<p>))
&minus;���
</p>
<p>([
X&lowast;,Y&lowast;
</p>
<p>]
,Z&lowast;
</p>
<p>))
.
</p>
<p>(b) Using arguments similar to the text, show that
</p>
<p>p
(
X&lowast;
</p>
<p>(
���
(
Y&lowast;,Z&lowast;
</p>
<p>)))
=&nabla;XR(Y,Z)
</p>
<p>(c) Convince yourself that
</p>
<p>p���
([
</p>
<p>X&lowast;,Y&lowast;
]
,Z&lowast;
</p>
<p>)
= R
</p>
<p>(
π&lowast;
</p>
<p>[
X&lowast;,Y&lowast;
</p>
<p>]
,Z
</p>
<p>)
.
</p>
<p>(d) Use π&lowast; = p ◦ θθθ and the fact that
</p>
<p>θθθ
[
X&lowast;,Y&lowast;
</p>
<p>]
= dθθθ
</p>
<p>(
X&lowast;,Y&lowast;
</p>
<p>)
=���
</p>
<p>(
X&lowast;,Y&lowast;
</p>
<p>)
</p>
<p>to show that π&lowast;[X&lowast;,Y&lowast;] = T(X,Y).
(e) Put everything together and show that Cyc[&nabla;XR(Y,Z)] = 0 when tor-
</p>
<p>sion tensor vanishes.
</p>
<p>36.6 Derive the coordinate expression for Bianchi&rsquo;s first and second identi-
ties of Theorem 36.2.18.
</p>
<p>36.7 Use Y ikX
k
j = δij to show that
</p>
<p>&part;mk Y
i
j &equiv;
</p>
<p>&part;
</p>
<p>&part;Xkm
Y ij =&minus;Y ikYmj
</p>
<p>36.8 From Eq. (36.18) show that bijk = ŴlkmY il Xmj . Now use this result to
rewrite Eq. (36.16) as (36.19).
</p>
<p>36.9 Derive Eq. (36.21).
</p>
<p>36.10 Prove the formulas in Eq. (36.23).
</p>
<p>36.11 Substituting (36.20) in (36.24) and noting the antisymmetry of the
wedge product, derive Eq. (36.25).
</p>
<p>36.12 Let Z = ξ k&part;k . Show that
</p>
<p>(&nabla;Z)ij &equiv; ξ i;j =
&part;ξ i
</p>
<p>&part;xj
+ Ŵijkξ k
</p>
<p>and
</p>
<p>ξ i;lk &minus; ξ i;kl =Rijklξ j + T
j
lkξ
</p>
<p>i
;j</p>
<p/>
</div>
<div class="page"><p/>
<p>37Riemannian Geometry
</p>
<p>The differential geometry of the last chapter covered most of what is needed
for many applications. However, it lacked the essential ingredient of a met-
ric. Almost all spaces (manifolds) encountered in physics have a natural
metric which is either known from the beginning, or is derived from some
of its physical properties (general theory of relativity). In this chapter, we
look at spaces whose connections are tied to their metrics.
</p>
<p>37.1 TheMetric Connection
</p>
<p>Let P(M,G) be a principal fiber bundle. Let ρ be a representation of G
into GL(m,R), and E(M,Rm,G,P ) the vector bundle associated with P
with standard fiber Rm on which G acts through ρ. A fiber metric is a map
g :M &rarr; T 02 (E) such that gx &equiv; g(x) is an inner product in the fiber π&minus;1E (x)
which is differentiable in x. For all physical applications, the structure of
the base manifold M is such that a fiber metric always exists for any vector
bundle E associated with P(M,G).
</p>
<p>fiber metric
Given a connection Ŵ in P , we can define a parallelism on the associated
</p>
<p>bundle E based on which we construct an isomorphism of the fibers. If
this isomorphism preserves the inner product, i.e., if the isomorphism is an
isometry, we say that the connection is a metric connection. This property
</p>
<p>metric connection
can be restated by saying that g is parallel. Hence, by Proposition 36.2.15,
we have
</p>
<p>Theorem 37.1.1 A connection Ŵ with covariant differential &nabla; is metric
with respect to the metric g iff &nabla;g= 0, i.e., gij ;k = 0 &forall;i, j, k.
</p>
<p>A statement equivalent to this theorem is that
</p>
<p>Box 37.1.2 The operation of raising and lowering of indices com-
mutes with the operation of covariant differentiation.
</p>
<p>S. Hassani, Mathematical Physics, DOI 10.1007/978-3-319-01195-0_37,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>1143</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-01195-0_37">http://dx.doi.org/10.1007/978-3-319-01195-0_37</a></div>
</div>
<div class="page"><p/>
<p>1144 37 Riemannian Geometry
</p>
<p>Consider two vectors v and w. If the covariant derivative of g vanishes,
then
</p>
<p>&nabla;u
[
g(v,w)
</p>
<p>]
&equiv;&nabla;u〈g,v &otimes; w〉
= 〈&nabla;ug,v &otimes; w〉 +
</p>
<p>&lang;
g, (&nabla;uv)&otimes; w
</p>
<p>&rang;
+
&lang;
g,v &otimes; (&nabla;uw)
</p>
<p>&rang;
</p>
<p>= g(&nabla;uv,w)+ g(v,&nabla;uw). (37.1)
</p>
<p>In particular, if v and w are parallel displaced along u, their inner product
will not change.
</p>
<p>When there is a fiber metric in E, the group representation by which G
acts on the standard fiber of E, can be made to take values in the group
O(m &minus; ν, ν) (see Sect. 29.2.1) so that ρ : G&rarr; O(m &minus; ν, ν) becomes the
new representation. So, the group associated with the vector bundle E is
O(m&minus; ν, ν) when there is a metric connection. In particular, when we deal
with a linear connection, E = T (M) and the structure group becomes O(n&minus;
ν, ν).
</p>
<p>Definition 37.1.3 A Riemannian manifold is a differentiable manifold M
Riemannian and
</p>
<p>pseudo/semi-
</p>
<p>Riemannian manifolds
</p>
<p>defined
</p>
<p>with a metric g &isin; T 02 (M), such that at each point x &isin;M , g|x is a positive
definite inner product. A manifold with an indefinite inner product at each
point is called a pseudo/semi-Riemannian manifold.
</p>
<p>Historical Notes
</p>
<p>No great mind of the past has exerted a deeper influence on the mathematics of the twen-
</p>
<p>Georg Friedrich
</p>
<p>Bernhard Riemann
</p>
<p>1826&ndash;1866
</p>
<p>tieth century than Georg Friedrich Bernhard Riemann (1826&ndash;1866), the son of a poor
country minister in northern Germany. He studied the works of Euler and Legendre while
he was still in secondary school, and it is said that he mastered Legendre&rsquo;s treatise on the
theory of numbers in less than a week. But he was shy and modest, with little awareness
of his own extraordinary abilities, so at the age of 19 he went to the University of G&ouml;t-
tingen with the aim of pleasing his father by studying theology and becoming a minister
himself. Fortunately, this worthy purpose soon stuck in his throat, and with his father&rsquo;s
willing permission he switched to mathematics.
The presence of the legendary Gauss automatically made G&ouml;ttingen the center of the
mathematical world. But Gauss was remote and unapproachable&mdash;particularly to begin-
ning students&mdash;and after only a year Riemann left this unsatisfying environment and went
to the University of Berlin. There he attracted the friendly interest of Dirichlet and Jacobi,
and learned a great deal from both men. Two years later he returned to G&ouml;ttingen, where
he obtained his doctor&rsquo;s degree in 1851. During the next 8 years, despite debilitating
poverty, he created his greatest works. In 1854 he was appointed Privatdozent (unpaid
lecturer), which at that time was the necessary first step on the academic ladder. Gauss
died in 1855, and Dirichlet was called to G&ouml;ttingen as his successor. Dirichlet helped
Riemann in every way he could, first with a small salary (about one-tenth of that paid to
a full professor) and then with a promotion to an assistant professorship. In 1859 he also
died, and Riemann was appointed as a full professor to replace him. Riemann&rsquo;s years of
poverty were over, but his health was broken. At the age of 39 he died of tuberculosis in
Italy, on the last of several trips he undertook in order to escape the cold, wet climate of
northern Germany. Riemann had a short life and published comparatively little, but his
works permanently altered the course of mathematics in analysis, geometry, and number
theory.
It is said that the three greatest mathematicians of modern times are Euler, Gauss, and
Riemann. It is a curiosity of nature that these three names are among the most frequently
mentioned names in the physics literature as well. Aside from the indirect use of his name
in the application of complex analysis in physics, Riemannian geometry has become the
most essential building block of all theories of fundamental interactions, starting with</p>
<p/>
</div>
<div class="page"><p/>
<p>37.1 The Metric Connection 1145
</p>
<p>gravity, which Einstein formulated in this language in 1916. As part of the requirement
to become a Privatdozent, Riemann had to write a probationary essay and to present a
trial lecture to the faculty. It was the custom for the candidate to offer three titles, and the
head of his department usually accepted the first. However, Riemann rashly listed as his
third topic the foundations of geometry. Gauss, who had been turning this subject over in
his mind for 60 years, was naturally curious to see how this particular candidate&rsquo;s &ldquo;glo-
riously fertile originality&rdquo; would cope with such a challenge, and to Riemann&rsquo;s dismay
he designated this as the subject of the lecture. Riemann quickly tore himself away from
his other interests at the time&mdash;&ldquo;my investigations of the connection between electricity,
magnetism, light, and gravitation&rdquo;&mdash;and wrote his lecture in the next two months. The
result was one of the great classical masterpieces of mathematics, and probably the most
important scientific lecture ever given. It is recorded that even Gauss was surprised and
enthusiastic.
</p>
<p>Since a (semi)Riemannian manifold has a metric, at each point x there is
a fiber metric, namely g(x). As before, a connection Ŵ is a metric connec-
tion if g is parallel with respect to Ŵ. There may be many connections on
a (semi)Riemannian manifold. In fact, any set of functions that transform
according to Eq. (36.21) define a unique connection by Theorem 36.2.19.
Many of these connections may be metric. However, one connection stands
out. (See [Koba 63, pp 158&ndash;160] for a proof of the following theorem.)
</p>
<p>Levi-Civita connection
</p>
<p>Theorem 37.1.4 Every (semi-)Riemannian manifold admits a unique
metric connection, called Levi-Civita connection, whose torsion is
zero.
</p>
<p>Because of the uniqueness of the Levi-Civita connection, we can identify
it with the manifold itself. And since an important property of the connection
is whether it is flat or not, we have
</p>
<p>Definition 37.1.5 A (semi-)Riemannian manifold is called flat if its Levi-
Civita connection is flat.
</p>
<p>Applying Theorem 34.3.11 and Eq. (36.8), we also have
</p>
<p>Proposition 37.1.6 A (semi-)Riemannian manifold is flat iff its curvature R
vanishes identically.
</p>
<p>Given the metric g of the (semi)Riemannian manifold, define a covariant
derivative as follows
</p>
<p>2g(&nabla;XY,Z)= X
(
g(Y,Z)
</p>
<p>)
+ Y
</p>
<p>(
g(X,Z)
</p>
<p>)
&minus; Z
</p>
<p>(
g(X,Y)
</p>
<p>)
</p>
<p>+ g
(
[X,Y],Z
</p>
<p>)
+ g
</p>
<p>(
[Z,X],Y
</p>
<p>)
+ g
</p>
<p>(
[Z,Y],X
</p>
<p>)
. (37.2)
</p>
<p>It is straightforward to show that this covariant derivative satisfies the four
conditions of Proposition 36.2.11. Therefore, by Theorem 36.2.12, it is the
covariant derivative with respect to some linear connection. That linear con-
nection turns out to be the Levi-Civita connection. Problem 37.2 shows that
if &nabla;g= 0, then (37.2) holds.</p>
<p/>
</div>
<div class="page"><p/>
<p>1146 37 Riemannian Geometry
</p>
<p>Historical Notes
</p>
<p>Tullio Levi-Civita (1873&ndash;1941), the son of Giacomo Levi-Civita, a lawyer who from
</p>
<p>Tullio Levi-Civita
</p>
<p>1873&ndash;1941
</p>
<p>1908 was a senator, was an outstanding student at the liceo in Padua. In 1890 he enrolled
in the Faculty of Mathematics of the University of Padua. Giuseppe Veronese and Gre-
gorio Ricci-Curbastro were among his teachers. He received his diploma in 1894 and in
1895 became resident professor at the teachers&rsquo; college annexed to the Faculty of Science
at Pavia. From 1897 to 1918 Levi-Civita taught rational mechanics at the University of
Padua. His years in Padua (where in 1914 he married a pupil, Libera Trevisani) were sci-
entifically the most fruitful of his career. In 1918 he became professor of higher analysis
at Rome and, in 1920, of rational mechanics. In 1938, struck by the fascist racial laws
against Jews, he was forced to give up teaching.
The breadth of his scientific interests, his scruples regarding the fulfillment of his aca-
demic responsibilities, and his affection for young people made Levi-Civita the leader of
a flourishing school of mathematicians.
Levi-Civita&rsquo;s approximately 200 memoirs in pure and applied mathematics deal with ana-
lytical mechanics, celestial mechanics, hydrodynamics, elasticity, electromagnetism, and
atomic physics. His most important contribution to science was rooted in the memoir
&ldquo;Sulle trasformazioni delle equazioni dinamiche&rdquo; (1896), which was characterized by the
use of the methods of absolute differential calculus that Ricci had applied only to dif-
ferential geometry. In the &ldquo;M&eacute;thodes de calcul diff&eacute;rentiel absolus et leurs applications&rdquo;,
written with Ricci and published in 1900 in Mathematische Annalen, there is a complete
exposition of the new calculus, which consists of a particular algorithm designed to ex-
press geometric and physical laws in Euclidean and non-Euclidean spaces, particularly
in Riemannian curved spaces. The memoir concerns a very general but laborious type of
calculus that made it possible to deal with many difficult problems, including, according
to Einstein, the formulation of the general theory of relativity.
Although Levi-Civita had expressed certain reservations concerning relativity in the first
years after its formulation (1905), he gradually came to accept the new views. His own
original research culminated in 1917 in the introduction of the concept of parallel trans-
port in curved spaces. With this new concept, absolute differential calculus, having ab-
sorbed other techniques, became tensor calculus, now the essential instrument of the uni-
tary relativistic theories of gravitation and electromagnetism.
In his memoirs of 1903&ndash;1916 Levi-Civita contributed to celestial mechanics in the study
of the three-body problem: the determination of the motion of three bodies, considered
as reduced to their centers of mass and subject to mutual Newtonian attraction. In 1914&ndash;
1916 he succeeded in eliminating the singularities present at the points of possible colli-
sions, past or future. His research in relativity led Levi-Civita to mathematical problems
suggested by atomic physics, which in the 1920s was developing outside the traditional
framework: the general theory of adiabatic invariants, the motion of a body of variable
mass, the extension of the Maxwellian distribution to a system of corpuscles, and the
Schr&ouml;dinger equation.
</p>
<p>The components of the curvature, the Riemann-Christoffel symbols, can
be calculated by substituting the coordinate vector fields {&part;i} in Eq. (37.2).
It is then easy to show that
</p>
<p>Ŵkij =
1
</p>
<p>2
gmk
</p>
<p>(
&part;gjm
</p>
<p>&part;xi
+ &part;gim
</p>
<p>&part;xj
&minus; &part;gij
</p>
<p>&part;xm
</p>
<p>)
. (37.3)
</p>
<p>This equation is sometimes written as
</p>
<p>Ŵkij =
1
</p>
<p>2
</p>
<p>(
&part;gjk
</p>
<p>&part;xi
+ &part;gik
</p>
<p>&part;xj
&minus; &part;gij
</p>
<p>&part;xk
</p>
<p>)
, (37.4)
</p>
<p>where Ŵkij &equiv; gkmŴmij .
Now we consider the connection between an infinitesimal displacement
</p>
<p>and a metric. Let P be a point of M . Let γ be a curve through P such that
γ (c) = P . For an infinitesimal number δu, let P &prime; = γ (c + δu) be a point</p>
<p/>
</div>
<div class="page"><p/>
<p>37.1 The Metric Connection 1147
</p>
<p>on γ close to P . Since the xi are well-behaved functions, xi(P &prime;)&minus; xi(P )
are infinitesimal real numbers. Let ξ i = xi(P &prime;)&minus; xi(P ), and construct the
vector v = ξ i&part;i , where {&part;i} consists of tangent vectors at P . We call v the in-
finitesimal displacement at P . The length (squared) of this vector, g(v,v),
is shown to be gij ξ iξ j . This is called the arc length from P to P &prime;, and
is naturally written as ds2 = gij ξ iξ j . It is customary to write dxi (not a
1-form!) in place of ξ i :
</p>
<p>connection between
</p>
<p>infinitesimal
</p>
<p>displacement, arc length,
</p>
<p>and metric tensor
</p>
<p>ds2 = gijdxidxj , (37.5)
</p>
<p>where the dxi are infinitesimal real numbers.
In applications, it is common to start with the metric tensor g given in
</p>
<p>terms of coordinate differential forms:
</p>
<p>g= gijdxi &otimes; dxj , where gij = gji = g(&part;i, &part;j ). (37.6)
</p>
<p>The equivalence of the arc length (37.5) and the metric (37.6) is the reason
why it is the arc length that is given in most practical problems. Once the arc
length is known, the metric gij can be read off, and all the relevant geometric
quantities can be calculated from it.
</p>
<p>Example 37.1.7 Let us determine the geodesics of the space whose arc
length is given by ds2 = (dx2 + dy2)/y2 (see also Example 37.1.10). With
x = x1 and y = x2, we recognize the metric tensor as
</p>
<p>g11 = g22 =
1
</p>
<p>y2
, g12 = g21 = 0,
</p>
<p>g11 = g22 = y2, g12 = g21 = 0.
</p>
<p>Using Eq. (37.4), we can readily calculate the nonzero connection coeffi-
cients:
</p>
<p>Ŵ112 = Ŵ121 =&minus;Ŵ211 = Ŵ222 =&minus;
1
</p>
<p>y3
.
</p>
<p>The geodesic equation for the first coordinate is
</p>
<p>d2x
</p>
<p>dt2
+ Ŵ1ij
</p>
<p>dxi
</p>
<p>dt
</p>
<p>dxj
</p>
<p>dt
= 0,
</p>
<p>or
</p>
<p>d2x
</p>
<p>dt2
+ Ŵ111
</p>
<p>(
dx
</p>
<p>dt
</p>
<p>)2
+ 2Ŵ112
</p>
<p>dx
</p>
<p>dt
</p>
<p>dy
</p>
<p>dt
+ Ŵ122
</p>
<p>(
dy
</p>
<p>dt
</p>
<p>)2
= 0.
</p>
<p>To find the connection coefficients with raised indices, we multiply those
with all indices lowered by the inverse of the metric tensor. For instance,
</p>
<p>Ŵ112 = g1iŴi12 = g11Ŵ112 + g12︸︷︷︸
=0
</p>
<p>Ŵ212 = y2
(
&minus; 1
y3
</p>
<p>)
=&minus; 1
</p>
<p>y
.</p>
<p/>
</div>
<div class="page"><p/>
<p>1148 37 Riemannian Geometry
</p>
<p>Similarly, Ŵ111 = 0 = Ŵ122, and the geodesic equation for the first coordi-
nate becomes
</p>
<p>d2x
</p>
<p>dt2
&minus; 2 1
</p>
<p>y
</p>
<p>dx
</p>
<p>dt
</p>
<p>dy
</p>
<p>dt
= 0. (37.7)
</p>
<p>For the second coordinate, we need Ŵ211, Ŵ
2
12, and Ŵ
</p>
<p>2
22. These can be
</p>
<p>readily evaluated as above, with the result
</p>
<p>Ŵ211 =&minus;Ŵ222 =
1
</p>
<p>y
, Ŵ212 = 0,
</p>
<p>yielding the geodesic equation for the second coordinate
</p>
<p>d2y
</p>
<p>dt2
+ 1
</p>
<p>y
</p>
<p>(
dx
</p>
<p>dt
</p>
<p>)2
&minus; 1
</p>
<p>y
</p>
<p>(
dy
</p>
<p>dt
</p>
<p>)2
= 0. (37.8)
</p>
<p>With ẋ &equiv; dx/dt , Eq. (37.7) can be written as
</p>
<p>dẋ
</p>
<p>dt
&minus; 2ẋ dy/dt
</p>
<p>y
= 0 &rArr; dẋ/dt
</p>
<p>ẋ
= 2dy/dt
</p>
<p>y
&rArr; ẋ = Cy2.
</p>
<p>Using the chain rule and the notation y&prime; &equiv; dy/dx, we obtain
</p>
<p>dy
</p>
<p>dt
= y&prime;ẋ = Cy2y&prime;,
</p>
<p>d2y
</p>
<p>dt2
= C
</p>
<p>(
2y
</p>
<p>dy
</p>
<p>dt
y&prime; + y2 dy
</p>
<p>&prime;
</p>
<p>dx
ẋ
</p>
<p>)
= C2
</p>
<p>(
2y3y&prime;2 + y4y&prime;&prime;
</p>
<p>)
.
</p>
<p>Substituting in Eq. (37.8) yields
</p>
<p>y3y&prime;2 + y4y&prime;&prime;+ y3 = 0 &rArr;
(
y&prime;
)2 + yy&prime;&prime;+ 1 = 0 &rArr; d
</p>
<p>dx
</p>
<p>(
yy&prime;
</p>
<p>)
+ 1 = 0.
</p>
<p>It follows that yy&prime; =&minus;x +A and x2 + y2 = 2Ax +B . Thus, the geodesics
are circles with arbitrary radii whose centers lie on the x-axis.
</p>
<p>37.1.1 Orthogonal Bases
</p>
<p>The presence of a metric allows for orthonormal bases, which are sometimes
more convenient than the coordinate bases. If the structure group reduces
to O(n), then g(n;R) reduces to o(n), which is the set of antisymmetric
matrices.1 Therefore, the Eji s in Eq. (36.4) will be antisymmetric in i and j ,
making ωij and Ω
</p>
<p>i
j also antisymmetric. This simplifies the calculation of the
</p>
<p>curvature form as given in Box 36.2.9. For this, we need the orthonormal
bases {ei} and {ǫǫǫi}, which can be constructed in terms of {&part;i} and {dxi},
respectively. The following examples illustrate the procedure.
</p>
<p>1Although we are restricting the discussion to O(n), it also applies to the more general
group O(n&minus; ν, ν).</p>
<p/>
</div>
<div class="page"><p/>
<p>37.1 The Metric Connection 1149
</p>
<p>Example 37.1.8 Let us look at a few examples of arc lengths, the corre-
sponding metrics, and the orthogonal bases derived from them.
</p>
<p>(a) For ds2 = dx2 + dy2 + dz2, g is the Euclidean metric of R3, with
gij = δij .
</p>
<p>(b) For ds2 =&minus;dx2 &minus; dy2 &minus; dz2 + dt2, g is the Minkowski (or Lorentz)
metric of R4, with gij = ηij , where ηxx = ηyy = ηzz =&minus;ηt t =&minus;1 and
ηij = 0 for i �= j .
</p>
<p>(c) For ds2 = dr2 + r2(dθ2 + sin2 θdϕ2), the metric is the Euclidean
metric given in spherical coordinates in R3 with grr = 1, gθθ = r2,
gϕϕ = r2 sin2 θ , and all other components zero.
</p>
<p>(d) For ds2 = a2dθ2 + a2 sin2 θdϕ2, the metric is that of a two-dimen-
sional spherical surface, with gθθ = a2, gϕϕ = a2 sin2 θ , and all other
components zero.
</p>
<p>(e) For
</p>
<p>ds2 = dt2 &minus; a2(t)
[
dχ2 + sin2 χ
</p>
<p>(
dθ2 + sin2 θdϕ2
</p>
<p>)]
,
</p>
<p>the metric is the Friedmann metric used in cosmology. Here Friedmann metric
</p>
<p>gt t = 1, gχχ =&minus;
[
a(t)
</p>
<p>]2
,
</p>
<p>gθθ =&minus;
[
a(t)
</p>
<p>]2
sin2 χ, gϕϕ =&minus;
</p>
<p>[
a(t)
</p>
<p>]2
sin2 χ sin2 θ.
</p>
<p>(f) For
</p>
<p>ds2 =
(
</p>
<p>1 &minus; 2M
r
</p>
<p>)
dt2 &minus;
</p>
<p>(
1 &minus; 2M
</p>
<p>r
</p>
<p>)&minus;1
dr2 &minus; r2
</p>
<p>(
dθ2 + sin2 θdϕ2
</p>
<p>)
,
</p>
<p>the metric is the Schwarzschild metric with Schwarzschild metric
</p>
<p>gt t = 1 &minus; 2M/r, grr =&minus;(1 &minus; 2M/r)&minus;1,
</p>
<p>gθθ =&minus;r2, gϕϕ =&minus;r2 sin2 θ.
</p>
<p>For each of the arc lengths above, we have an orthonormal basis of one-
forms:
</p>
<p>(a) g= ǫǫǫ1 &otimes; ǫǫǫ1 + ǫǫǫ2 &otimes; ǫǫǫ2 + ǫǫǫ3 &otimes; ǫǫǫ3, with
</p>
<p>ǫǫǫ1 = dx, ǫǫǫ2 = dy, ǫǫǫ3 = dz;
</p>
<p>(b) g=&minus;ǫǫǫ1 &otimes; ǫǫǫ1 &minus; ǫǫǫ2 &otimes; ǫǫǫ2 &minus; ǫǫǫ3 &otimes; ǫǫǫ3 + ǫǫǫ0 &otimes; ǫǫǫ0, with
</p>
<p>ǫǫǫ1 = dx, ǫǫǫ2 = dy, ǫǫǫ3 = dz, ǫǫǫ0 = dt;
</p>
<p>(c) g= ǫǫǫr &otimes; ǫǫǫr + ǫǫǫθ &otimes; ǫǫǫθ + ǫǫǫϕ &otimes; ǫǫǫϕ , with
</p>
<p>ǫǫǫr = dr, ǫǫǫθ = rdθ, ǫǫǫϕ = r sin θdϕ;
</p>
<p>(d) g= ǫǫǫθ &otimes; ǫǫǫθ + ǫǫǫϕ &otimes; ǫǫǫϕ , with ǫǫǫθ = adθ , ǫǫǫϕ = a sin θdϕ;
(e) g= ǫǫǫt &otimes; ǫǫǫt &minus; ǫǫǫχ &otimes; ǫǫǫχ &minus; ǫǫǫθ &otimes; ǫǫǫθ &minus; ǫǫǫϕ &otimes; ǫǫǫϕ , with
</p>
<p>ǫǫǫt = dt, ǫǫǫχ = a(t)dχ,
ǫǫǫθ = a(t) sinχdθ, ǫǫǫϕ = a(t) sinχ sin θdϕ;</p>
<p/>
</div>
<div class="page"><p/>
<p>1150 37 Riemannian Geometry
</p>
<p>(f) g= ǫǫǫt &otimes; ǫǫǫt &minus; ǫǫǫr &otimes; ǫǫǫr &minus; ǫǫǫθ &otimes; ǫǫǫθ &minus; ǫǫǫϕ &otimes; ǫǫǫϕ , with
</p>
<p>ǫǫǫt = (1 &minus; 2M/r)1/2dt, ǫǫǫr = (1 &minus; 2M/r)&minus;1/2dr,
ǫǫǫθ = rdθ, ǫǫǫϕ = r sin θdϕ.
</p>
<p>Example 37.1.9 In this example, we examine the curvilinear coordinates
used in vector analysis. Recall that in terms of these coordinates the dis-
placement is given by ds2 = h21(dq1)2 + h22(dq2)2 + h23(dq3)2. Therefore,
the orthonormal one-forms are ǫǫǫ1 = h1dq1, ǫǫǫ2 = h2dq2, ǫǫǫ3 = h3dq3. We
also note (see Problem 28.23) that
</p>
<p>d &lowast; df =
(
&part;2f
</p>
<p>&part;x2
+ &part;
</p>
<p>2f
</p>
<p>&part;y2
+ &part;
</p>
<p>2f
</p>
<p>&part;z2
</p>
<p>)
dx &and; dy &and; dz
</p>
<p>=&nabla;2f dx &and; dy &and; dz. (37.9)
</p>
<p>We use this equation to find the Laplacian in terms of q1, q2, and q3:
</p>
<p>df = &part;f
&part;q1
</p>
<p>dq1 +
&part;f
</p>
<p>&part;q2
dq2 +
</p>
<p>&part;f
</p>
<p>&part;q3
dq3
</p>
<p>=
(
</p>
<p>1
</p>
<p>h1
</p>
<p>&part;f
</p>
<p>&part;q1
</p>
<p>)
ǫǫǫ1 +
</p>
<p>(
1
</p>
<p>h2
</p>
<p>&part;f
</p>
<p>&part;q2
</p>
<p>)
ǫǫǫ2 +
</p>
<p>(
1
</p>
<p>h3
</p>
<p>&part;f
</p>
<p>&part;q3
</p>
<p>)
ǫǫǫ3,
</p>
<p>where we substituted orthonormal 1-forms so we could apply the Hodge star
operator. It follows now that
</p>
<p>&lowast;df =
(
</p>
<p>1
</p>
<p>h1
</p>
<p>&part;f
</p>
<p>&part;q1
</p>
<p>)
&lowast; ǫǫǫ1 +
</p>
<p>(
1
</p>
<p>h2
</p>
<p>&part;f
</p>
<p>&part;q2
</p>
<p>)
&lowast; ǫǫǫ2 +
</p>
<p>(
1
</p>
<p>h3
</p>
<p>&part;f
</p>
<p>&part;q3
</p>
<p>)
&lowast; ǫǫǫ3
</p>
<p>=
(
</p>
<p>1
</p>
<p>h1
</p>
<p>&part;f
</p>
<p>&part;q1
</p>
<p>)
ǫǫǫ2 &and; ǫǫǫ3 +
</p>
<p>(
1
</p>
<p>h2
</p>
<p>&part;f
</p>
<p>&part;q2
</p>
<p>)
ǫǫǫ3 &and; ǫǫǫ1 +
</p>
<p>(
1
</p>
<p>h3
</p>
<p>&part;f
</p>
<p>&part;q3
</p>
<p>)
ǫǫǫ1 &and; ǫǫǫ2
</p>
<p>=
(
h2h3
</p>
<p>h1
</p>
<p>&part;f
</p>
<p>&part;q1
</p>
<p>)
dq2 &and; dq3 +
</p>
<p>(
h1h3
</p>
<p>h2
</p>
<p>&part;f
</p>
<p>&part;q2
</p>
<p>)
dq3 &and; dq1
</p>
<p>+
(
h1h2
</p>
<p>h3
</p>
<p>&part;f
</p>
<p>&part;q3
</p>
<p>)
dq1 &and; dq2.
</p>
<p>Differentiating once more, we get
</p>
<p>d &lowast; df = &part;
&part;q1
</p>
<p>(
h2h3
</p>
<p>h1
</p>
<p>&part;f
</p>
<p>&part;q1
</p>
<p>)
dq1 &and; dq2 &and; dq3
</p>
<p>+ &part;
&part;q2
</p>
<p>(
h1h3
</p>
<p>h2
</p>
<p>&part;f
</p>
<p>&part;q2
</p>
<p>)
dq2 &and; dq3 &and; dq1
</p>
<p>+ &part;
&part;q3
</p>
<p>(
h1h2
</p>
<p>h3
</p>
<p>&part;f
</p>
<p>&part;q3
</p>
<p>)
dq3 &and; dq1 &and; dq2
</p>
<p>=
{
</p>
<p>1
</p>
<p>h1h2h3
</p>
<p>[
&part;
</p>
<p>&part;q1
</p>
<p>(
h2h3
</p>
<p>h1
</p>
<p>&part;f
</p>
<p>&part;q1
</p>
<p>)
+ &part;
</p>
<p>&part;q2
</p>
<p>(
h1h3
</p>
<p>h2
</p>
<p>&part;f
</p>
<p>&part;q2
</p>
<p>)
</p>
<p>+ &part;
&part;q3
</p>
<p>(
h1h2
</p>
<p>h3
</p>
<p>&part;f
</p>
<p>&part;q3
</p>
<p>)]}
ǫǫǫ1 &and; ǫǫǫ2 &and; ǫǫǫ3. (37.10)</p>
<p/>
</div>
<div class="page"><p/>
<p>37.1 The Metric Connection 1151
</p>
<p>Since {ǫǫǫ1,ǫǫǫ2,ǫǫǫ3} are orthonormal one-forms (as are {dx, dy, dz}), the vol-
ume elements ǫǫǫ1 &and; ǫǫǫ2 &and; ǫǫǫ3 and dx &and; dy &and; dz are equal. Thus, we substitute
the latter for the former in (37.10), compare with (37.9), and conclude that
</p>
<p>&nabla;2f = 1
h1h2h3
</p>
<p>[
&part;
</p>
<p>&part;q1
</p>
<p>(
h2h3
</p>
<p>h1
</p>
<p>&part;f
</p>
<p>&part;q1
</p>
<p>)
+ &part;
</p>
<p>&part;q2
</p>
<p>(
h1h3
</p>
<p>h2
</p>
<p>&part;f
</p>
<p>&part;q2
</p>
<p>)
</p>
<p>+ &part;
&part;q3
</p>
<p>(
h1h2
</p>
<p>h3
</p>
<p>&part;f
</p>
<p>&part;q3
</p>
<p>)]
,
</p>
<p>which is the result obtained in curvilinear vector analysis.
</p>
<p>In an orthonormal frame, Eq. (36.20) becomes
</p>
<p>ωij = Ŵiljǫǫǫl, (37.11)
</p>
<p>where as usual, we have omitted the subscript U . Furthermore, using
Eq. (36.6) with T= ǫǫǫi , one can easily show that
</p>
<p>&nabla;ǫǫǫi =&minus;Ŵijkǫǫǫk &otimes; ǫǫǫj =&minus;Ŵikjǫǫǫk &otimes; ǫǫǫj =&minus;ωij &otimes; ǫǫǫj
</p>
<p>for a Levi-Civita connection. Now use Corollary 36.3.10 to obtain
</p>
<p>dǫǫǫi =A
(
&nabla;ǫǫǫi
</p>
<p>)
=A
</p>
<p>(
&minus;ωij &otimes; ǫǫǫj
</p>
<p>)
=&minus;ωij &and; ǫǫǫj . (37.12)
</p>
<p>This can also be written in matrix form if we use Box 36.2.9 and define the
column vector εεε with elements ǫǫǫi . We then have
</p>
<p>dεεε =&minus;ω̂ωω &and; εεε.
</p>
<p>Taking the exterior derivative of this equation gives
</p>
<p>0 = d2εεε =&minus;dω̂ωω &and; εεε+ ω̂ωω &and; dεεε =&minus;dω̂ωω &and; εεε&minus; ω̂ωω &and; ω̂ωω &and; εεε =&minus;�̂��&and; εεε.
</p>
<p>Combining the two equations, we get
</p>
<p>dεεε =&minus;ω̂ωω &and; εεε, �̂��&and; εεε = 0. (37.13)
</p>
<p>The antisymmetry of Ω ij in i and j and Eq. (36.25) give
</p>
<p>Rijkl +R
j
ikl = 0, Rijkl +Rij lk = 0. (37.14)
</p>
<p>Similarly, the second relation of Eq. (37.13) can be shown to be equivalent
to Square brackets mean
</p>
<p>antisymmetrization.
Rijkl +Riklj +Riljk = 0 and Ri[jkl] = 0, (37.15)
</p>
<p>where the enclosure of indices in square brackets means complete antisym-
metrization of those indices. The first relation of (37.15) can also be ob-
tained from Bianchi&rsquo;s 1st identity given in Theorem 36.2.17.
</p>
<p>It is also common to lower the upper index by the metric and define
Rijkl = gimRmjkl . Then the new tensor has the additional properties
</p>
<p>Rijkl =Rklij and R[ijkl] = 0. (37.16)</p>
<p/>
</div>
<div class="page"><p/>
<p>1152 37 Riemannian Geometry
</p>
<p>Equation (37.12) gives us a recipe for finding the curvature from the arc
length. Given the arc length, construct the orthogonal 1-forms as in Exam-
ples 37.1.8 and 37.1.9. Then take the exterior derivative of a typical one and
read off ωik from the right-hand side of the equation. Form the matrix ω̂ωω and
use Box 36.2.9 to calculate �̂��. The coefficients of the entries of �̂�� are the
components of the Riemann curvature tensor according to Eq. (36.25).
</p>
<p>Example 37.1.10 Let M =R2, and suppose that the arc length is given by
ds2 = (dx2 + dy2)/y2 (see Example 37.1.7). We can write the metric as
g= ǫǫǫ1 &otimes; ǫǫǫ1 + ǫǫǫ2 &otimes; ǫǫǫ2 if we define
</p>
<p>ǫǫǫ1 = dx
y
</p>
<p>and ǫǫǫ2 = dy
y
.
</p>
<p>To find the curvature tensor, we take the exterior derivative of the ǫǫǫis:
</p>
<p>dǫǫǫ1 = d
(
</p>
<p>1
</p>
<p>y
dx
</p>
<p>)
=&minus; 1
</p>
<p>y2
dy &and; dx = ǫǫǫ1 &and; ǫǫǫ2, dǫǫǫ2 = 0. (37.17)
</p>
<p>From these equations, the antisymmetry of the ω&rsquo;s, and Eq. (37.12), we can
read off ωij . They are ω
</p>
<p>1
1 = ω22 = 0 and ω12 =&minus;ω21 = ǫǫǫ1. Thus, the matrix ω̂ωω
</p>
<p>is
</p>
<p>ω̂ωω= ω21E12 +ω12E21 = ω21
(
</p>
<p>0 1
0 0
</p>
<p>)
+ω12
</p>
<p>(
0 0
1 0
</p>
<p>)
</p>
<p>=
(
</p>
<p>0 ω21
ω12 0
</p>
<p>)
=
(
</p>
<p>0 &minus;ǫǫǫ1
ǫǫǫ1 0
</p>
<p>)
,
</p>
<p>which gives
</p>
<p>dω̂ωω=
(
</p>
<p>0 &minus;dǫǫǫ1
dǫǫǫ1 0
</p>
<p>)
=
(
</p>
<p>0 &minus;ǫǫǫ1 &and; ǫǫǫ2
ǫǫǫ1 &and; ǫǫǫ2 0
</p>
<p>)
,
</p>
<p>and
</p>
<p>ω̂ωω &and; ω̂ωω=
(
</p>
<p>0 &minus;ǫǫǫ1
ǫǫǫ1 0
</p>
<p>)
&and;
(
</p>
<p>0 &minus;ǫǫǫ1
ǫǫǫ1 0
</p>
<p>)
= 0.
</p>
<p>Therefore, the curvature matrix is
</p>
<p>�̂��= dω̂ωω=
(
</p>
<p>0 &minus;ǫǫǫ1 &and; ǫǫǫ2
ǫǫǫ1 &and; ǫǫǫ2 0
</p>
<p>)
&equiv;
(
</p>
<p>0 Ω21
Ω12 0
</p>
<p>)
.
</p>
<p>This shows that the only nonzero independent component of the Riemann
curvature tensor is R1212 =&minus;1.
</p>
<p>Example 37.1.11 For a spherical surface of radius a, the element of length
is
</p>
<p>ds2 = a2dθ2 + a2 sin2 θdϕ2.
The orthonormal forms are ǫǫǫθ = adθ and ǫǫǫϕ = a sin θdϕ, and we have
</p>
<p>dǫǫǫθ = 0, dǫǫǫϕ = a cos θdθ &and; dϕ = 1
a
</p>
<p>cot θǫǫǫθ &and; ǫǫǫϕ .</p>
<p/>
</div>
<div class="page"><p/>
<p>37.1 The Metric Connection 1153
</p>
<p>The matrix ω̂ωω can now be read off:2
</p>
<p>ω̂ωω=
(
</p>
<p>0 &minus; cot θ
a
</p>
<p>ǫǫǫϕ
</p>
<p>cot θ
a
</p>
<p>ǫǫǫϕ 0
</p>
<p>)
.
</p>
<p>A straightforward exterior differentiation yields
</p>
<p>dω̂ωω=
(
</p>
<p>0 1
a2
ǫǫǫθ &and; ǫǫǫϕ
</p>
<p>&minus; 1
a2
ǫǫǫθ &and; ǫǫǫϕ 0
</p>
<p>)
= 1
</p>
<p>a2
</p>
<p>(
0 ǫǫǫθ &and; ǫǫǫϕ
</p>
<p>&minus;ǫǫǫθ &and; ǫǫǫϕ 0
</p>
<p>)
.
</p>
<p>Similarly, ω̂ωω &and; ω̂ωω= 0. Therefore, the curvature matrix is
</p>
<p>�̂��= dω̂ωω= 1
a2
</p>
<p>(
0 ǫǫǫθ &and; ǫǫǫϕ
</p>
<p>&minus;ǫǫǫθ &and; ǫǫǫϕ 0
</p>
<p>)
.
</p>
<p>The only independent component of the Riemann curvature tensor is
Rθϕθϕ = 1/a2, which is constant, as expected for a spherical surface.
</p>
<p>It is clear that when the gij in the expression for the line element are all What is a flat manifold?
constants for all points in the manifold, then ǫǫǫi will be proportional to dxi
</p>
<p>and dǫǫǫi = 0, for all i. This immediately tells us that ω̂ωω = 0, and therefore
�̂�� = 0; that is, the manifold is flat. Thus, for ds2 = dx2 + dy2 + dz2, the
space is flat. However, arc lengths of a flat space come in various guises
with nontrivial coefficients. Does the curvature matrix �̂�� recognize the flat
arc length, or is it possible to fool it into believing that it is privileged with a
curvature when in reality the curvature is still zero? The following example
shows that the curvature matrix can detect flatness no matter how disguised
the line element is!
</p>
<p>Example 37.1.12 In spherical coordinates, the line element (arc length) of
the flat Euclidean space R3 is ds2 = dr2 + r2dθ2 + r2 sin2 θdϕ2. To cal-
culate the curvature matrix, we first need an orthonormal set of one-forms.
These are immediately obtained from the expression above:
</p>
<p>ǫǫǫr = dr, ǫǫǫθ = rdθ, ǫǫǫϕ = r sin θdϕ.
</p>
<p>Taking the exterior derivatives of these one-forms, we obtain
</p>
<p>dǫǫǫr = d2r = 0,
</p>
<p>dǫǫǫθ = d(rdθ)= dr &and; dθ + r d2θ︸︷︷︸
=0
</p>
<p>= ǫǫǫr &and;
(
ǫǫǫθ
</p>
<p>r
</p>
<p>)
= 1
</p>
<p>r
ǫǫǫr &and; ǫǫǫθ ,
</p>
<p>dǫǫǫϕ = d(r sin θ)&and; dϕ = sin θdr &and; dϕ + r cos θdθ &and; dϕ
</p>
<p>= sin θǫǫǫr &and;
(
</p>
<p>ǫǫǫϕ
</p>
<p>r sin θ
</p>
<p>)
+ r cos θ
</p>
<p>(
ǫǫǫθ
</p>
<p>r
</p>
<p>)
&and;
(
</p>
<p>ǫǫǫϕ
</p>
<p>r sin θ
</p>
<p>)
</p>
<p>= 1
r
ǫǫǫr &and; ǫǫǫϕ + cot θ
</p>
<p>r
ǫǫǫθ &and; ǫǫǫϕ .
</p>
<p>2Note that dǫǫǫθ = 0 does not imply ω12 = 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>1154 37 Riemannian Geometry
</p>
<p>We can now find the matrix of one-forms ω̂ωω. In calculating the elements
of ω̂ωω, we remember that it is a skew-symmetric matrix, so all diagonal el-
ements are zero. We also note that dǫǫǫk = 0 does not imply that ωkj = 0.
Keeping these facts in mind, we can easily obtain ω̂ωω (the calculation is left
as a problem for the reader):
</p>
<p>ω̂ωω=
</p>
<p>⎛
⎜⎝
</p>
<p>0 &minus; 1
r
ǫǫǫθ &minus; 1
</p>
<p>r
ǫǫǫϕ
</p>
<p>1
r
ǫǫǫθ 0 &minus; cot θ
</p>
<p>r
ǫǫǫϕ
</p>
<p>1
r
ǫǫǫϕ cot θ
</p>
<p>r
ǫǫǫϕ 0
</p>
<p>⎞
⎟⎠ .
</p>
<p>The exterior derivative of this matrix is found to be
</p>
<p>dω̂ωω=
</p>
<p>⎛
⎜⎜⎝
</p>
<p>0 0 &minus; cot θ
r2
</p>
<p>ǫǫǫθ &and; ǫǫǫϕ
</p>
<p>0 0 1
r2
ǫǫǫθ &and; ǫǫǫϕ
</p>
<p>cot θ
r2
</p>
<p>ǫǫǫθ &and; ǫǫǫϕ &minus; 1
r2
ǫǫǫθ &and; ǫǫǫϕ 0
</p>
<p>⎞
⎟⎟⎠ ,
</p>
<p>which is precisely (the negative of) the exterior product ω̂ωω&and; ω̂ωω, as the reader
may wish to verify. Thus, �̂��= dω̂ωω+ ω̂ωω&and; ω̂ωω= 0, and the space is indeed flat!
</p>
<p>In all the foregoing examples, the curvature was calculated intrinsically.
We never had to leave the space and go to a higher dimension to &ldquo;see&rdquo;
the curvature. For example, in the case of the sphere, the only information
we had was the line element in terms of the coordinates on the sphere. We
never had to resort to any three-dimensional analysis to discover a globe
embedded in the Euclidean R3. As mentioned earlier, if a space has line
elements with constant gij , then the Riemann curvature vanishes trivially.
We have also seen an example in which the components of a metric tensor
were by no means trivial, but �̂�� was smart enough to detect the flatness in
disguise. Under what conditions can we choose coordinate systems in terms
of which the line elements have gij = δij ? To answer this question we need
the following lemma (proved in [Flan 89, pp. 135&ndash;136]):
</p>
<p>Lemma 37.1.13 If ω̂ωω is a matrix of 1-forms such that dω̂ωω+ ω̂ωω&and; ω̂ωω= 0, then
there exists an orthogonal matrix A such that dA = Aω̂ωω.
</p>
<p>The question raised above is intimately related to the connection between
coordinate and orthonormal frames. We have seen the usefulness of both.
Coordinate frames, due to the existence of the related coordinate functions,
are useful for many analytical calculations, for example in Hamiltonian dy-
namics. Orthonormal frames are useful because of the simplicity of expres-
sions inherent in all orthonormal vectors. Furthermore, we saw how cur-
vature was easily calculated once we constructed orthonormal dual frames.
Naturally, we would like to have both. Is it possible to construct frames that
are both coordinate and orthonormal? The following theorem answers this
question:
</p>
<p>Theorem 37.1.14 Let M be a Riemannian manifold. Then M is flat, i.e.,
�̂�� = 0 if and only if there exists a local coordinate system {xi} for which
{&part;i} is an orthonormal basis.</p>
<p/>
</div>
<div class="page"><p/>
<p>37.2 Isometries and Killing Vector Fields 1155
</p>
<p>Proof The existence of orthonormal coordinate frames implies that {dxi}
are orthonormal. Thus, we can use them to find the curvature. But since
d(dxi)= 0 for all i, it follows from Eq. (37.12) that ωij = 0 and ω̂ωω= 0. So
the curvature must vanish.
</p>
<p>Conversely, suppose that �̂�� = 0. Then by Lemma 37.1.13, there exists
an orthogonal matrix A such that dA = Aω̂ωω. Now we define the one-form
column matrix τττ by τττ = Aεεε, where εεε is the column matrix of 1-forms ǫǫǫi .
Then, using Eq. (37.12) in matrix form, we have
</p>
<p>dτττ = d(Aεεε)= dA &and; εεε+ Adεεε = (Aω̂ωω)&and; εεε&minus; A(ω̂ωω &and; εεε)= 0.
</p>
<p>Thus, dτττ i = 0 for all i. By Theorem 28.5.15 there must exist zero-forms
(functions) xi such that τττ i = dxi . These xi are the coordinates we are after.
The basis {&part;i} is obtained using the inverse of A (see the discussion fol-
lowing Proposition 26.1.1). Since A is orthogonal, both {dxi} and {&part;i} are
orthonormal bases. �
</p>
<p>37.2 Isometries and Killing Vector Fields
</p>
<p>We have already defined isometries for inner product spaces. The natural
vector space structure on tangent spaces, plus the introduction of metrics on
manifolds, leads in a natural way to the isometric mappings of manifolds.
</p>
<p>Definition 37.2.1 Let M and N be Riemannian manifolds with metrics gM
and gN , respectively. The smooth map ψ :M &rarr; N is called isometric at isometry defined
P &isin;M if gM(X,Y)= gN (ψ&lowast;X,ψ&lowast;Y) for all X,Y &isin; TP (M). An isometry
of M to N is a bijective smooth map that is isometric at every point of M ,
in which case we have gM = gN ◦ψ&lowast;.
</p>
<p>Of special interest are isometries ψ :M &rarr;M of a single manifold. These Isometries of a manifold
form a subgroup of
</p>
<p>Diff(M).
</p>
<p>happen to be a subgroup of Diff(M), the group of diffeomorphisms of M .
In the subgroup of isometries, we concentrate on the one-parameter groups
of transformations. These define (and are defined by) certain vector fields:
</p>
<p>Definition 37.2.2 Let X &isin; X(M) be a vector field with integral curve Ft .
Then X is called a Killing vector field if Ft is an isometry of M . Killing vector field
</p>
<p>The following proposition follows immediately from the definition of the
Lie derivative [Eq. (28.29)].
</p>
<p>Proposition 37.2.3 A vector field X &isin; X(M) is a Killing vector field
if and only if LXg= 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>1156 37 Riemannian Geometry
</p>
<p>Choosing a coordinate system {xi}, we write g= gijdxi &otimes; dxj and con-
clude that Xj&part;j is a Killing vector field if and only if
</p>
<p>0 = LX
(
gijdx
</p>
<p>i &otimes; dxj
)
</p>
<p>= X(gij )dxi &otimes; dxj + gij
(
LXdx
</p>
<p>i
)
&otimes; dxj + gijdxi &otimes;
</p>
<p>(
LXdx
</p>
<p>j
)
.
</p>
<p>Using Eq. (28.33) for the 1-form dxk , we obtain the Killing equationKilling equation
</p>
<p>Xk&part;kgij + &part;iXkgkj + &part;jXkgki = 0. (37.18)
</p>
<p>If in Eq. (36.38) we replace T with g and u with X, where X is a Killing
vector field, we obtain
</p>
<p>0 = 0 + gij
[
(&nabla;ekX)iǫǫǫk &otimes; ǫǫǫj + (&nabla;ekX)jǫǫǫi &otimes; ǫǫǫk
</p>
<p>]
, (37.19)
</p>
<p>where we have assumed that the covariant derivative is compatible with the
metric tensor, i.e., that it is the Levi-Civita covariant derivative. The reader
may check that Eq. (37.19) leads toKilling equation in terms
</p>
<p>of covariant derivative
Xj ;k +Xk;j = 0. (37.20)
</p>
<p>This is another form of Killing equation.
</p>
<p>Proposition 37.2.4 If X is a Killing vector field, then its inner product with
the tangent to any geodesic is constant along that geodesic, i.e., if u is such
a tangent, then &nabla;u[g(u,X)] = 0.
</p>
<p>Proof We write the desired covariant derivative in component form,
</p>
<p>uk
(
giju
</p>
<p>iXj
)
;k = u
</p>
<p>k gij ;k︸︷︷︸
=0
</p>
<p>uiXj + ukgij ui;k︸︷︷︸
=0
</p>
<p>Xj + ukgijuiXj;k
</p>
<p>= 1
2
(Xi;k +Xk;i)︸ ︷︷ ︸
=0 by (37.20)
</p>
<p>uiuk,
</p>
<p>where the first term vanishes by assumption of the compatibility of the met-
ric and the covariant derivative, and the second term by the geodesic equa-
tion. �
</p>
<p>Example 37.2.5 In a flat m-dimensional manifold we can choose an or-
thonormal coordinate frame (Theorem 37.1.14), so that the Killing equation
becomes
</p>
<p>&part;iXj + &part;jXi = 0. (37.21)
Setting i = j , we see that &part;jXj = 0 (no sum). Differentiating Eq. (37.21)
with respect to xi , we obtain
</p>
<p>&part;2i Xj + &part;j &part;iXi︸︷︷︸
=0
</p>
<p>= 0 &rArr; &part;2i Xj = 0 &forall;i, j.</p>
<p/>
</div>
<div class="page"><p/>
<p>37.2 Isometries and Killing Vector Fields 1157
</p>
<p>Therefore, Xj is linear in xi , i.e., Xj = ajkxk+bj with ajk and bj arbitrary.
Inserting this in (37.21), we get aij + aji = 0. The Killing vector is then
</p>
<p>X =
(
aijx
</p>
<p>j + bi
)
&part;i =
</p>
<p>1
</p>
<p>2
aij
</p>
<p>(
xj&part;i &minus; xi&part;j
</p>
<p>)
+ bi&part;i .
</p>
<p>The first term is clearly the generator of a rotation and the second term that maximally symmetric
spacesof translation. Altogether, there are m(m&minus;1)/2 rotations and m translations.
</p>
<p>So the total number of independent Killing vectors is m(m + 1)/2. Mani-
folds that have this many Killing vectors are called maximally symmetric
spaces.
</p>
<p>Using Eqs. (28.34) and (28.35) one can show that the set of Killing vector
fields on a manifold form a vector subspace of X(M), and that if X and Y
are Killing vector fields, then so is [X,Y]. Thus,
</p>
<p>Box 37.2.6 The set of Killing vector fields forms a Lie algebra.
</p>
<p>Example 37.2.7 From g= dθ&otimes;dθ + sin2 θdϕ&otimes;dϕ, the metric of the unit
sphere S2, one writes the Killing equations
</p>
<p>&part;θXθ + &part;θXθ = 0,
&part;ϕXϕ + &part;ϕXϕ + 2 sin θ cos θXθ = 0, (37.22)
</p>
<p>&part;θXϕ + &part;ϕXθ &minus; 2 cot θXϕ = 0.
</p>
<p>The first equation implies that Xθ = f (ϕ), a function of ϕ only. Substitution
in the second equation yields
</p>
<p>Xϕ =&minus;
1
</p>
<p>2
F(ϕ) sin 2θ + g(θ), where f (ϕ)= dF
</p>
<p>dϕ
.
</p>
<p>Inserting this in the third equation of (37.22), we obtain
</p>
<p>&minus;F(ϕ) cos 2θ + dg
dθ
</p>
<p>+ df
dϕ
</p>
<p>+ 2 cot θ
[
</p>
<p>1
</p>
<p>2
F(ϕ) sin 2θ &minus; g(θ)
</p>
<p>]
= 0,
</p>
<p>or
[
dg
</p>
<p>dθ
&minus; 2 cot θg(θ)
</p>
<p>]
+
[
df
</p>
<p>dϕ
+ F(ϕ)
</p>
<p>]
= 0.
</p>
<p>For this equation to hold for all θ and ϕ, we must have
</p>
<p>dg
</p>
<p>dθ
&minus; 2 cot θg(θ)= C =&minus;df
</p>
<p>dϕ
&minus; F(ϕ),
</p>
<p>where C is a constant. This gives
</p>
<p>g(θ)= (C1 &minus;C cot θ) sin2 θ and f (ϕ)=Xθ =A sinϕ +B cosϕ</p>
<p/>
</div>
<div class="page"><p/>
<p>1158 37 Riemannian Geometry
</p>
<p>with
</p>
<p>Xϕ = (A cosϕ &minus;B sinϕ) sin θ cos θ +C1 sin2 θ.
A general Killing vector field is thus given by
</p>
<p>X =Xθ&part;θ +Xϕ&part;ϕ =ALy &minus;BLx +C1Lz,
</p>
<p>where
</p>
<p>Lx =&minus; cosϕ&part;θ + cot θ sinϕ&part;ϕ,
Ly = sinϕ&part;θ + cot θ cosϕ&part;ϕ,
Lz = &part;ϕ
</p>
<p>are the generators of SO(3).
</p>
<p>Sometimes it is useful to relax the complete invariance of the metric ten-
sor under the diffeomorphism of a manifold induced by a vector field and
allow a change of scale in the metric. More precisely, we consider vector
fields X whose flow Ft changes the metric of M . So Ft&lowast;g= eφ(t)g, where
φ is a real-valued function on M that is also dependent on the parameter t .
Such a transformation keeps angles unchanged but rescales all lengths. In
analogy with those of the complex plane with the same property, we call
such transformations conformal transformations. A vector field that gen-
erates a conformal transformation will satisfydiscussion of conformal
</p>
<p>transformations and
</p>
<p>conformal Killing vector
</p>
<p>fields
Xk&part;kgij + &part;iXkgkj + &part;jXkgki =&minus;ψgij , ψ =
</p>
<p>&part;φ
</p>
<p>&part;t
</p>
<p>∣∣∣∣
t=0
</p>
<p>, (37.23)
</p>
<p>and is called a conformal Killing vector field.
We now specialize to a flat m-dimensional manifold and choose an or-
</p>
<p>thonormal coordinate frame (Theorem 37.1.14). Then Eq. (37.23) becomes
</p>
<p>&part;iXj + &part;jXi =&minus;ψgij . (37.24)
</p>
<p>Multiply both sides by gij and sum over i to obtainRemember Einstein&rsquo;s
summation convention!
</p>
<p>2&part; iXi =&minus;mψ &rArr; &part; iXi =&minus;
m
</p>
<p>2
ψ, m= dimM.
</p>
<p>Apply &part; i to both sides of Eq. (37.24) and sum over i. This yields
</p>
<p>&part; i&part;iXj + &part;j &part; iXi︸︷︷︸
&minus;m2 ψ
</p>
<p>=&minus;&part;jψ &rArr; &part; i&part;iXj =
1
</p>
<p>2
(m&minus; 2)&part;jψ. (37.25)
</p>
<p>Differentiate both sides of the second equation in (37.25) with respect to xk
</p>
<p>and symmetrize the result in j and k to obtain
</p>
<p>(m&minus; 2)&part;j&part;kψ = &part; i&part;i(&part;kXj + &part;jXk)=&minus;gjk&part; i&part;iψ.</p>
<p/>
</div>
<div class="page"><p/>
<p>37.3 Geodesic Deviation and Curvature 1159
</p>
<p>Raising the index j and contracting it with k gives &part; i&part;iψ = 0 if m �= 1. It
follows that
</p>
<p>(m&minus; 2)&part;j&part;kψ = 0. (37.26)
Equations (37.24), (37.25), and (37.26) determine both ψ and Xj if m �= 2.
It follows from Eq. (37.26) that ψ is linear in x, and, consequently [from
(37.24)], that Xi is at most quadratic in x. The most general solution to
Eq. (37.24) satisfying (37.25) and (37.26) is
</p>
<p>Xj (x)= bj + εxj + ajkxk + cjxkxk &minus; 2ckxkxj , (37.27)
</p>
<p>where aij = &minus;aji and indices of the constants are raised and lowered as
usual.
</p>
<p>Equation (37.27) gives the generators of an [(m + 1)(m + 2)/2]-
parameter group of transformations on Rm, m �= 2 called the conformal conformal group
group. The reader should note that translation (represented by the parame-
ters bj ) and rotations (represented by the parameters aij ) are included in this
group. The other finite (as opposed to infinitesimal) transformations of coor-
dinates can be obtained by using Eq. (29.30). For example, the finite trans-
formation generated by the parameter ε is given by the solution to the DE
dx&prime;j/dt = x&prime;j , which is x&prime;j = etxj , or x&prime;j = eεxj , and is called a dilitation dilitation
of coordinates. Similarly, the finite transformation generated by the param-
eter ci is given by the solution to the DE dx&prime;j/dci = δijx&prime;kx&prime;k &minus; 2x&prime;ix&prime;j ,
or
</p>
<p>x&prime;i = x
i &minus; cix2
</p>
<p>1 &minus; 2c &middot; x + c2x2 , where c &middot; x &equiv; c
kxk, c
</p>
<p>2 &equiv; c &middot; c, x2 &equiv; x &middot; x,
</p>
<p>which is called inversion, or the special conformal transformation. Equa-
tions (37.25), and (37.26) place no restriction on ψ (and therefore on Xi )
when m= 2. This means that
</p>
<p>inversion, or special
</p>
<p>conformal
</p>
<p>transformation
</p>
<p>Box 37.2.8 The conformal group is infinite-dimensional for R2.
</p>
<p>In fact, we encountered the conformal transformations of R2 in the con-
text of complex analysis, where we showed that any (therefore, infinitely
many) analytic function is a conformal transformation of C&sim;=R2. The con-
formal group of R2 has important applications in string theory and statistical
mechanics, but we shall not pursue them here.
</p>
<p>37.3 Geodesic Deviation and Curvature
</p>
<p>Geodesics are the straight lines of general manifolds on which, for example,
free particles move. If u represents the tangent to a given geodesic, one can
say that &nabla;uu = 0 is the equation of motion of a free particle. In flat spaces,
the relative velocity of any pair of free particles will not change, so that their</p>
<p/>
</div>
<div class="page"><p/>
<p>1160 37 Riemannian Geometry
</p>
<p>Fig. 37.1 A region of the manifold and the two-dimensional surface defined by s and t
</p>
<p>relative acceleration is always zero. In general, however, due to the effects
of curvature, we expect a nonzero acceleration. Let us elaborate on this.
</p>
<p>Consider some region of the manifold through whose points geodesics
can be drawn in various directions. Concentrate on one geodesic and its
neighboring geodesics. Let t designate the parameter that locates points of
the geodesic. Let s be a continuous parameter that labels different geodesics
(see Fig. 37.1). One can connect the points on some neighboring geodesics
corresponding to the same value of t and obtain a curve parametrized by s.
The collection of all geodesics that pass through all points of this curve
form a two-dimensional submanifold with coordinates t and s. Each such
geodesic is thus described by the geodesic equation &nabla;uu = 0 with u = &part;/&part;t
(because t is a coordinate). Furthermore, as we hop from one geodesic to its
neighbor, the geodesic equation does not change; i.e., the geodesic equation
is independent of s. Translated into the language of calculus, this means
that differentiation of the geodesic equation with respect to s will give zero.
Translated into the higher-level language of tensor analysis, it means that
covariant differentiation of the geodesic equation will yield zero. We write
this final translation as &nabla;n(&nabla;uu) = 0 where n &equiv; &part;/&part;s. This can also be
written as
</p>
<p>0 =&nabla;n&nabla;uu =&nabla;u&nabla;nu + [&nabla;n,&nabla;n]u =&nabla;u
(
&nabla;un + [n,u]
</p>
<p>)
+ [&nabla;n,&nabla;n]u,
</p>
<p>where we have used the first property of Proposition 36.1.6. Using the fact
that n and u are coordinate frames, we conclude that [n,u] = 0, which in
conjunction with the second equation of Theorem 36.2.16, yields
</p>
<p>&nabla;u&nabla;un + R(n,u)u = 0. (37.28)
</p>
<p>The first term can be interpreted as the relative acceleration of two geodesicrelative acceleration
curves (or free particles), because &nabla;u is the generalization of the derivative
with respect to t , and &nabla;un is interpreted as relative velocity. In a flat man-
ifold, the relative acceleration for any pair of free particles is zero. When
curvature is present, it produces a nonzero relative acceleration.</p>
<p/>
</div>
<div class="page"><p/>
<p>37.3 Geodesic Deviation and Curvature 1161
</p>
<p>By writing u = ui&part;i and n = nk&part;k and substituting in Eq. (37.28), we
arrive at the equation of geodesic deviation in coordinate form: equation of geodesic
</p>
<p>deviation
</p>
<p>uiujnkRmijk =
d2nm
</p>
<p>dt2
+ d
</p>
<p>dt
</p>
<p>(
ujnkŴmjk
</p>
<p>)
+ uj dn
</p>
<p>k
</p>
<p>dt
Ŵmjk + uiujnkŴljkŴmil,
</p>
<p>(37.29)
where we have used the fact that ui&part;if = df/dt for any function defined on
the manifold.
</p>
<p>The chain that connects relative acceleration to curvature has another link
that connects the latter two to gravity. From a Newtonian standpoint, grav-
ity is the only force that accelerates all objects at the same rate (equivalence
principle). From a geometric standpoint, this property allows one to include
gravity in the structure of space-time: An object in free fall is considered
&ldquo;free&rdquo;, and its path, a geodesic. Locally, this is in fact a better picture of re-
ality, because inside a laboratory in free fall (such as a space shuttle in orbit
around earth) one actually verifies the first law of motion on all objects float-
ing in midair. One need not include an external phenomenon called gravity.
Gravity becomes part of the fabric of space-time.
</p>
<p>But how does gravity manifest itself? Is there any observable effect that
can indicate the presence of gravity, or by a mere transfer to a freely falling
frame have we been able to completely eliminate gravity? The second alter-
native would be strange indeed, because the source of gravity is matter, and
if we eliminate gravity completely, we have to eliminate matter as well! If
the gravitational field were homogeneous, one could eliminate it&mdash;and the
matter that produces it as well, but no such gravitational field exists. The
inhomogeneity of gravitational fields has indeed an observable effect. Con-
sider two test particles that are falling freely toward the source of gravity
on two different radii. As they get closer and closer to the center, their rel-
ative distance&mdash;in fact, their relative velocity&mdash;changes: They experience a
relative acceleration. Since as we saw in Eq. (37.28), relative acceleration is
related to curvature, we conclude that Einstein&rsquo;s interpretation
</p>
<p>of gravity
</p>
<p>Box 37.3.1 Gravity manifests itself by giving space-time a curvature.
</p>
<p>This is Einstein&rsquo;s interpretation of gravity. From a Newtonian standpoint,
the relative acceleration is caused by the inhomogeneity of the gravitational
field. Such inhomogeneity (in the field of the Moon and the Sun) is respon-
sible for tidal waves. That is why the curvature term in Eq. (37.28) is also
called the tide-producing gravitational force.
</p>
<p>37.3.1 Newtonian Gravity
</p>
<p>The equivalence principle, relating gravity with the curvature of space-time,
is not unique to Einstein. What is unique to him is combining that principle
with the assumption of local Lorentz geometry, i.e., local validity of special</p>
<p/>
</div>
<div class="page"><p/>
<p>1162 37 Riemannian Geometry
</p>
<p>relativity. Cartan also used the equivalence principle to reformulate Newto-
nian gravity in the language of geometry. Rewrite Newton&rsquo;s second law of
motion as
</p>
<p>F =ma &rArr; a =&minus;&nabla;Φ &rArr; &part;
2xj
</p>
<p>&part;t2
+ &part;Φ
</p>
<p>&part;xj
= 0,
</p>
<p>where Φ is the gravitational potential (potential energy per unit mass). The
Newtonian universal time is a parameter that has two degrees of freedom:
Its origin and its unit of measurement are arbitrary. Thus, one can change t
to t = aτ + b without changing the physics of gravity. Taking this freedom
into account, one can write
</p>
<p>d2t
</p>
<p>dτ 2
= 0, d
</p>
<p>2xj
</p>
<p>dτ 2
+ &part;Φ
</p>
<p>&part;xj
</p>
<p>(
dt
</p>
<p>dτ
</p>
<p>)2
= 0. (37.30)
</p>
<p>Comparing this with the geodesic equation, we can read off the nonzero
connection coefficients:
</p>
<p>Ŵ
j
</p>
<p>00 =
&part;Φ
</p>
<p>&part;xj
, j = 1,2,3. (37.31)
</p>
<p>Inserting these in the second formula of Eq. (36.23), we find the nonzero
components of Riemann curvature tensor:
</p>
<p>R
j
</p>
<p>0k0 =&minus;R
j
</p>
<p>00k =
&part;2Φ
</p>
<p>&part;xj&part;xk
. (37.32)
</p>
<p>Contraction of the two nonzero indices leads to the Laplacian of gravita-
tional potential
</p>
<p>R
j
</p>
<p>0j0 =
&part;2Φ
</p>
<p>&part;x2
+ &part;
</p>
<p>2Φ
</p>
<p>&part;y2
+ &part;
</p>
<p>2Φ
</p>
<p>&part;z2
=&nabla;2Φ.
</p>
<p>Therefore, the Poisson equation for gravitational potential can be written in
terms of the curvature tensor:
</p>
<p>R00 &equiv;Rj0j0 = 4πGρ, (37.33)
</p>
<p>where we have introduced the Ricci tensor, defined asRicci tensor defined
</p>
<p>Rik &equiv;Rjijk. (37.34)
</p>
<p>Equations (37.31), (37.32), and (37.33) plus the law of geodesic motion
describe the full content of Newtonian gravitational theory in the geometric
language of tensors.3
</p>
<p>3The classic and comprehensive book Gravitation, by Misner, Thorne, and Wheeler, has
a thorough discussion of Newtonian gravity in the language of geometry in Chap. 13 and
is highly recommended.</p>
<p/>
</div>
<div class="page"><p/>
<p>37.4 General Theory of Relativity 1163
</p>
<p>It is instructive to discover the relation between curvature and gravity
directly from the equation of geodesic deviation as applied to Newtonian
gravity. The geodesic equation is the equation of motion:
</p>
<p>&part;2xj
</p>
<p>&part;t2
+ &part;Φ
</p>
<p>&part;xj
= 0.
</p>
<p>Differentiate this equation with respect to the parameter s, noting that
&part;/&part;s = ni&part;/&part;xi :
</p>
<p>&part;
</p>
<p>&part;s
</p>
<p>(
&part;2xj
</p>
<p>&part;t2
</p>
<p>)
+ &part;
</p>
<p>&part;s
</p>
<p>(
&part;Φ
</p>
<p>&part;xj
</p>
<p>)
= 0 &rArr; &part;
</p>
<p>2
</p>
<p>&part;t2
</p>
<p>(
&part;xj
</p>
<p>&part;s
</p>
<p>)
+ ni &part;
</p>
<p>&part;xi
</p>
<p>(
&part;Φ
</p>
<p>&part;xj
</p>
<p>)
= 0.
</p>
<p>Now note that &part;xj/&part;s = ni&part;xj/&part;xi = nj . So, we obtain
</p>
<p>&part;2nj
</p>
<p>&part;t2
+ ni &part;
</p>
<p>2Φ
</p>
<p>&part;xi&part;xj
= 0.
</p>
<p>This is equivalent to Eq. (37.28), and one recognizes the second term as the
tide-producing (or the curvature) term.
</p>
<p>37.4 General Theory of Relativity
</p>
<p>No treatment of (semi)Riemannian geometry is complete without a discus-
sion of the general theory of relativity. That is why we shall devote this last
section of the current chapter to a brief exposition of this theory.
</p>
<p>We have seen that Newtonian gravity can be translated into the language
of differential geometry by identifying the gravitational tidal effects with
the curvature of space-time. This straightforward interpretation of New-
tonian gravity, in particular the retention of the Euclidean metric and the
universality of time, leads to no new physical effect. Furthermore, it is in-
consistent with the special theory of relativity, which mixes space and time
coordinates via Lorentz transformations. Einstein&rsquo;s general theory of rela-
tivity (GTR) combines the equivalence principle (that freely falling objects
move on geodesics) with the local validity of the special theory of relativity
(that the metric of space-time reduces to the Lorentz-Minkowski metric of
the special theory of relativity).
</p>
<p>37.4.1 Einstein&rsquo;s Equation
</p>
<p>An important tensor that can be constructed out of Riemann curvature tensor
and the metric tensor is the Einstein tensor G, which is related to the Ricci Einstein tensor
tensor defined in Eq. (37.34). To derive the Einstein tensor, first note that the
Ricci tensor is symmetric in its indices (see Problem 37.21):
</p>
<p>Rij =Rji . (37.35)
</p>
<p>Next, define the curvature scalar as curvature scalar defined</p>
<p/>
</div>
<div class="page"><p/>
<p>1164 37 Riemannian Geometry
</p>
<p>R &equiv;Rii &equiv; gijRij . (37.36)
</p>
<p>Now, contract i with m in Eq. (36.46) and use the antisymmetry of the Rie-
mann tensor in its last two indices to obtain
</p>
<p>Rijkl;i +Rjk;l &minus;Rj l;k = 0.
</p>
<p>Finally, contract j and l and use the antisymmetry of the Riemann tensor
in its first as well as its last two indices to get 2Ri
</p>
<p>k;i &minus;R;k = 0, or Rjk;i &minus;
1
2gjkR;i = 0. Summarizing the foregoing discussion, we write
</p>
<p>&nabla; &middot;G= 0, where Gij &equiv;Rij &minus;
1
</p>
<p>2
gijR. (37.37)
</p>
<p>Historical Notes
</p>
<p>Karl Schwarzschild (1873&ndash;1916) was the eldest of five sons and one daughter born to
Moses Martin Schwarzschild and his wife, Henrietta Sabel. His father was a prosperous
member of the business community in Frankfurt, with Jewish forbears in that city traced
back to the sixteenth century.
From his mother, a vivacious, warm person, Karl undoubtedly inherited his happy, out-
going personality, and from his father, a capacity for sustained hard work. His childhood
was spent in comfortable circumstances among a large circle of relatives, whose interests
included art and music; he was the first to become a scientist.
</p>
<p>Karl Schwarzschild
</p>
<p>1873&ndash;1916
</p>
<p>After attending a Jewish primary school, Schwarzschild entered the municipal gymna-
sium in Frankfurt at the age of eleven. His curiosity about the heavens was first mani-
fested then: He saved his allowance and bought lenses to make a telescope. Indulging
this interest, his father introduced him to a friend, J. Epstein, a mathematician who had
a private observatory. With Epstein&rsquo;s son (later professor of mathematics at the Univer-
sity of Strasbourg), Schwarzschild learned to use a telescope and studied mathematics of
a more advanced type than he was getting in school. His precocious mastery of celes-
tial mechanics resulted in two papers on double star orbits, written when he was barely
sixteen.
In 1891 Schwarzschild began two years of study at the University of Strasbourg, where
Ernst Becker, director of the observatory, guided the development of his skills in practical
astronomy&mdash;skills that later were to form a solid underpinning for his masterful mathe-
matical abilities.
At age twenty Schwarzschild went to the University of Munich. Three years later, in
1896, he obtained his Ph.D., summa cum laude. His dissertation was an application of
Poincar&eacute;&rsquo;s theory of stable configurations in rotating bodies to several astronomical prob-
lems, including tidal deformation in satellites and the validity of Laplace&rsquo;s suggestion as
to how the solar system had originated. Before graduating, Schwarzschild also found time
to do some practical work with Michelson&rsquo;s interferometer.
At a meeting of the German Astronomical Society in Heidelberg in 1900 he discussed the
possibility that space was non-Euclidean. In the same year he published a paper giving a
lower limit for the radius of curvature of space as 2500 light years. From 1901 until 1909
he was professor at G&ouml;ttingen, where he collaborated with Klein, Hilbert, and Minkowski,
publishing on electrodynamics and geometrical optics. In 1906, he studied the transport
of energy through a star by radiation.
From G&ouml;ttingen he went to Potsdam, but in 1914 he volunteered for military service.
He served in Belgium, France, and Russia. While in Russia he wrote two papers on Ein-
stein&rsquo;s relativity theory and one on Planck&rsquo;s quantum theory. The quantum theory paper
explained that the Stark effect could be proved from the postulates of quantum theory.
Schwarzschild&rsquo;s relativity papers give the first exact solution of Einstein&rsquo;s general grav-
itational equations, giving an understanding of the geometry of space near a point mass.
He also made the first study of black holes, showing that bodies of sufficiently large mass
would have an escape velocity exceeding the speed of light and so could not be seen.
However, he contracted an illness while in Russia and died soon after returning home.</p>
<p/>
</div>
<div class="page"><p/>
<p>37.4 General Theory of Relativity 1165
</p>
<p>The automatic vanishing of the divergence of the symmetric Einstein ten-
sor has an important consequence in the field equation of GTR. It is remi-
niscent of a similar situation in electromagnetism, in which the vanishing of
the divergence of the fields leads to the conservation of the electric charge,
the source of electromagnetic fields.4
</p>
<p>Just as Maxwell&rsquo;s equations are a generalization of the static electricity
of Coulomb to a dynamical theory, Einstein&rsquo;s GTR is the generalization
of Newtonian static gravity to a dynamical theory. As this generalization
ought to agree with the successes of the Newtonian gravity, Eq. (37.37) must
agree with (37.33). The bold step taken by Einstein was to generalize this
relation involving only a single component of the Ricci tensor to a full tensor
equation. The natural tensor to be used as the source of gravitation is the
stress energy tensor5
</p>
<p>T μν &equiv; (ρ + p)uμuν + pgμν, or T= (ρ + p)u&otimes; u+ pg,
</p>
<p>where the source is treated as a fluid with density ρ, four-velocity u, and
pressure p. So, Einstein suggested the equation G = κT as the generaliza-
tion of Newton&rsquo;s universal law of gravitation. Note that &nabla; &middot; G = 0 auto-
matically guarantees mass-energy conservation as in Maxwell&rsquo;s theory of
electromagnetism. Problem 37.23 calculates κ to be 8π in units in which
the universal gravitational constant and the speed of light are set equal to
unity. We therefore have Einstein&rsquo;s equation of the
</p>
<p>general theory of
</p>
<p>relativityG= 8πT, or R&minus; 1
2
Rg= 8π
</p>
<p>[
(ρ + p)u&otimes; u+ pg
</p>
<p>]
. (37.38)
</p>
<p>This is Einstein&rsquo;s equation of the general theory of relativity.
The Einstein tensor G is nearly the only symmetric second-rank tensor
</p>
<p>made out of the Riemann and metric tensors that is divergence free. The
only other tensor with the same properties is G +Λg, where Λ is the so-
called cosmological constant (see Problem 37.24). When in 1922, Einstein cosmological constant
applied his GTR to the universe itself, he found that the universe ought to
be expanding. Being a firm believer in Nature, he changed his equation to
G+Λg = 8πT to suppress the unobserved prediction of the expansion of
the universe. Later, when the expansion was observed by Hubble, Einstein
referred to this mutilation of his GTR as &ldquo;the biggest blunder of my life&rdquo;.
</p>
<p>With the discovery of an accelerating universe, there has been a revival
of interest in the cosmological constant. However, the fact that it is so small,
and a lack of fundamental explanation of this fact, has become a challenge in
cosmology. Perhaps a unification of GTR with quantum&mdash;a quantum theory
of gravity&mdash;is a solution. But this unification has resisted an indisputable
</p>
<p>4It was Maxwell&rsquo;s discovery of the inconsistency of the pre-Maxwellian equations of
electromagnetism with charge conservation that prompted him to change not only the
fourth equation (to make the entire set of equations consistent with the charge conserva-
tion), but also the course of human history.
5In GTR, it is customary to use the convention that Greek indices run from 0 to 3, i.e., they
include both space and time, while Latin indices encompass only the space components.</p>
<p/>
</div>
<div class="page"><p/>
<p>1166 37 Riemannian Geometry
</p>
<p>solution (of the type that Dirac found in unifying STR with quantum the-
</p>
<p>ory) despite the effort of some of the most brilliant minds in contemporary
</p>
<p>physics.
</p>
<p>Historical Notes
</p>
<p>Aleksandr Aleksandrovich Friedmann (1888&ndash;1925) was born into a musical family&mdash;
his father, Aleksandr Friedmann, being a composer and his mother, Ludmila Voj&aacute;čka, the
daughter of the Czech composer Hynek Voj&aacute;ček.
In 1906 Friedmann graduated from the gymnasium with the gold medal and immediately
enrolled in the mathematics section of the department of physics and mathematics of St.
Petersburg University. While still a student, he wrote a number of unpublished scientific
papers, one of which was awarded a gold medal by the department. After graduation
from the university in 1910, Friedmann was retained in the department to prepare for the
teaching profession.
</p>
<p>Aleksandr
</p>
<p>Aleksandrovich
</p>
<p>Friedmann 1888&ndash;1925
</p>
<p>In the fall of 1914, Friedmann volunteered for service in an aviation detachment, in which
he worked, first on the northern front and later on other fronts, to organize aerologic and
aeronavigational services. While at the front, Friedmann often participated in military
flights as an aircraft observer. In the summer of 1917 he was appointed a section chief
in Russia&rsquo;s first factory for the manufacture of measuring instruments used in aviation;
he later became director of the factory. Friedmann had to relinquish this post because of
the onset of heart disease. From 1918 until 1920, he was professor in the department of
theoretical mechanics of Perm University.
In 1920 he returned to Petrograd and worked at the main physics observatory of the
Academy of Sciences, first as head of the mathematics department and later, shortly be-
fore his death, as director of the observatory. Friedmann&rsquo;s scientific activity was con-
centrated in the areas of theoretical meteorology and hydromechanics, where he demon-
strated his mathematical talent and his unwavering strife for, and ability to attain, the
concrete, practical application of solutions to theoretical problems.
Friedmann made a valuable contribution to Einstein&rsquo;s general theory of relativity. As
always, his interest was not limited simply to familiarizing himself with this new field
of science but led to his own remarkable investigations. Friedmann&rsquo;s work on the theory
of relativity dealt with the cosmological problem. In his paper &ldquo;&Uuml;ber die Kr&uuml;mmung des
Raumes&rdquo; (1922), he outlined the fundamental ideas of his cosmology: the supposition
concerning the homogeneity of the distribution of matter in space and the consequent
homogeneity and isotropy of space-time. This theory is especially important because it
leads to a sufficiently correct explanation of the fundamental phenomenon known as the
&ldquo;red shift&rdquo;. Einstein himself thought that the cosmological solution to the equations of
a field had to be static and had to lead to a closed model of the universe. Friedmann
discarded both conditions and arrived at an independent solution.
Friedmann&rsquo;s interest in the theory of relativity was by no means a passing fancy. In the
last years of his life, together with V.K. Frederiks, he began work on a multivolume text
on modern physics. The first book, The World as Space and Time, is devoted to the theory
of relativity, knowledge of which Friedmann considered one of the cornerstones of an
education in physics.
In addition to his scientific work, Friedmann taught courses in higher mathematics and
theoretical mechanics at various colleges in Petrograd. He found time to create new and
original courses, brilliant in their form and exceedingly varied in their content. Fried-
mann&rsquo;s unique course in theoretical mechanics combined mathematical precision and
logical continuity with original procedural and physical trends.
Friedmann died of typhoid fever at the age of thirty-seven. In 1931, he was posthumously
awarded the Lenin Prize for his outstanding scientific work.</p>
<p/>
</div>
<div class="page"><p/>
<p>37.4 General Theory of Relativity 1167
</p>
<p>37.4.2 Static Spherically Symmetric Solutions
</p>
<p>The general theory of relativity as given in Eq. (37.38) has been strikingly
successful in predicting the spacetime6 structure of our universe. It predicts
the expansion of the universe, and by time-reversed extrapolation, the big
bang cosmology; it predicts the existence of black holes and other final prod-
ucts of stellar collapse; and on a less grandiose scale, it explains the small
precession of Mercury, the bending of light in the gravitational field of the
Sun, and the gravitational redshift. We shall not discuss the solution of Ein-
stein&rsquo;s equation in any detail. However, due to its simplicity and its use of
geometric arguments, we shall consider the solution to Einstein&rsquo;s equation
exterior to a static spherically symmetric distribution of mass.
</p>
<p>Let us first translate the two adjectives used in the last sentence into a
geometric language. Take static first. We call a phenomenon &ldquo;static&rdquo; if at
different instants it &ldquo;looks the same&rdquo;. Thus, a static solution of Einstein&rsquo;s
equation is a spacetime manifold that &ldquo;looks the same&rdquo; for all time. In the
language of geometry &ldquo;looks the same&rdquo; means isometric, because metric is
the essence of the geometry of space-time. In Euclidean physics, time can
be thought of as an axis at each point (moment) of which one can assign a
three-dimensional space corresponding to the &ldquo;spatial universe&rdquo; at that mo-
ment. In the general theory of relativity, space and time can be mixed, but the
character of time as a parameter remains unaltered. Therefore, instead of an
axis&mdash;a straight line&mdash;we pick a curve, a parametric map from the real line
to the manifold of space-time. This curve must be timelike, so that locally,
when curvature is ignored and special relativity becomes a good approxima-
tion, we do not violate causality. The curve must also have the property that
at each point of it, the space-time manifold has the same metric. Moreover,
we need to demand that at each point of this curve, the spatial part of the
space-time is orthogonal to the curve.
</p>
<p>Definition 37.4.1 A spacetime is stationary if there exists a one-parameter stationary and static
spacetimes; time
</p>
<p>translation isometries
</p>
<p>group of isometries Ft , called time translation isometries, whose Killing
vector fields ξξξ are timelike for all t : g(ξξξ,ξξξ) &gt; 0. If in addition, there ex-
ists a spacelike hypersurface +++ that is orthogonal to orbits (curves) of the
isometries, we say that the spacetime is static.
</p>
<p>We can simplify the solution to Einstein&rsquo;s equation by invoking the sym-
metry of spacetime discussed above in our choice of coordinates. Let P be
a point of the spacetime manifold located in a neighborhood of some space-
like hypersurface +++ as shown in Fig. 37.2. Through P passes a single orbit
of the isometry, which starts at a point Q of +++. Let t , the so-called Killing Killing parameter
parameter, stand for the parameter corresponding to the point P with t = 0
being the parameter of Q. On the spacelike hypersurface+++, choose arbitrary
coordinates {xi} for Q. Assign the coordinates (t &equiv; x0, x1, x2, x3) to P .
Since Ft does not change the metric, +++t &equiv; Ft+++, the translation of +++ by Ft ,
</p>
<p>6The reader may be surprised to see the two words &ldquo;space&rdquo; and &ldquo;time&rdquo; juxtaposed with
no hyphen; but this is common practice in relativity.</p>
<p/>
</div>
<div class="page"><p/>
<p>1168 37 Riemannian Geometry
</p>
<p>Fig. 37.2 The coordinates appropriate for a stationary spacetime
</p>
<p>is also orthogonal to the orbit of the isometry. Moreover, the components
of the metric in this coordinate system cannot be dependent on the Killing
parameter t . Thus, in this coordinate system, the spacetime metric takes the
form
</p>
<p>g= g00dt &otimes; dt &minus;
3&sum;
</p>
<p>i,j=1
gijdx
</p>
<p>i &otimes; dxj . (37.39)
</p>
<p>Definition 37.4.2 A spacetime is spherically symmetric if its isometryspherically symmetric
spacetimes group contains a subgroup isomorphic to SO(3) and the orbits of this group
</p>
<p>are two-dimensional spheres.
</p>
<p>In other words, if we think of isometries as the action of some abstract
group, then this group must contain SO(3) as a subgroup. Since SO(3) is
isomorphic to the group of rotations, we conclude that the metric should
be rotationally invariant. The time-translation Killing vector field ξξξ must
be orthogonal to the orbits of SO(3), because otherwise the generators of
SO(3) can change the projection of ξξξ on the spheres and destroy the ro-
tational invariance. Therefore, the 2-dimensional spheres must lie entirely
in the hypersurfaces +++t . Now, we can write down a static spherically sym-
metric metric in terms of appropriate coordinates as follows. Choose the
spherical coordinates (θ,ϕ) for the 2-spheres, and write the metric of this
sphere as
</p>
<p>g2 = r2dθ &otimes; dθ + r2 sin2 θdϕ &otimes; dϕ,
</p>
<p>where r is the &ldquo;radius&rdquo; of the 2-sphere. Choose the third spatial coordinate
to be orthogonal to this sphere, i.e., r . Rotational symmetry now implies
that the components of the metric must be independent of θ and ϕ. The final
form of the metric based entirely on the assumed symmetries is
</p>
<p>g= f (r)dt &otimes; dt &minus; h(r)dr &otimes; dr &minus; r2
(
dθ &otimes; dθ + sin2 θdϕ&otimes; dϕ
</p>
<p>)
. (37.40)</p>
<p/>
</div>
<div class="page"><p/>
<p>37.4 General Theory of Relativity 1169
</p>
<p>We have reduced the problem of finding ten unknown functions {gμν}3μ,ν=0
of four variables {xμ}3μ=0 to that of two functions f and h of one vari-
able. The remaining task is to calculate the Ricci tensor corresponding to
Eq. (37.40), substitute it in Einstein&rsquo;s equation (with the RHS equal to zero),
and solve the resulting differential equation for f and h. We shall not pursue
this further here, and we refer the reader to textbooks on general relativity
(see, for example, [Wald 84, pp. 121&ndash;124]). The final result is the so-called
Schwarzschild solution, which is Schwarzschild solution
</p>
<p>of Einstein&rsquo;s equation
</p>
<p>g=
(
</p>
<p>1 &minus; 2M
r
</p>
<p>)
dt &otimes; dt &minus;
</p>
<p>(
1 &minus; 2M
</p>
<p>r
</p>
<p>)&minus;1
dr &otimes; dr
</p>
<p>&minus; r2
(
dθ &otimes; dθ + sin2 θdϕ &otimes; dϕ
</p>
<p>)
, (37.41)
</p>
<p>where M is the total mass of the gravitating body, and the natural units of
GTR, in which G= 1 = c, have been used.
</p>
<p>A remarkable feature of the Schwarzschild solution is that the metric
components have singularities at r = 2M and at r = 0. It turns out that the
first singularity is due to the choice of coordinates (analogous to the singu-
larity at r = 0, θ = 0,π , and ϕ = 0 in spherical coordinates of R3), while
the second is a true singularity of spacetime. The first singularity occurs at
the so-called Schwarzschild radius whose numerical value is given by Schwarzschild radius
</p>
<p>rS =
2GM
</p>
<p>c2
&asymp; 3 M
</p>
<p>M⊙
km,
</p>
<p>where M⊙ = 2 &times; 1030 kg is the mass of the Sun. Therefore, for an ordinary
body such as the Earth, planets, and a typical star, the Schwarzschild radius
is well inside the body where the Schwarzschild solution is not applicable.
</p>
<p>If we relax the assumption of staticity, we get the following theorem (for
a proof, see [Misn 73, p. 843]):
</p>
<p>Theorem 37.4.3 (Birkhoff&rsquo;s theorem) The Schwarzschild solution is
the only spherically symmetric solution of Einstein&rsquo;s equation in vac-
uum.
</p>
<p>A corollary to this theorem is that all spherically symmetric solutions of
Einstein&rsquo;s equation in vacuum are static. This is analogous to the fact that the
Coulomb solution is the only spherically symmetric solution to Maxwell&rsquo;s
equations in vacuum. It can be interpreted as the statement that in gravity,
as in electromagnetism, there is no monopole &ldquo;radiation&rdquo;.
</p>
<p>All spherically symmetric
</p>
<p>solutions of Einstein&rsquo;s
</p>
<p>equation in vacuum are
</p>
<p>static.
</p>
<p>37.4.3 Schwarzschild Geodesics
</p>
<p>With the metric available to us, we can, in principle, solve the geodesic
equations [Eq. (36.44)] and obtain the trajectories of freely falling particles.
However, a more elegant way is to make further use of the symmetries to</p>
<p/>
</div>
<div class="page"><p/>
<p>1170 37 Riemannian Geometry
</p>
<p>eliminate variables. In particular, Proposition 37.2.4 is extremely useful in
this endeavor. Consider first g(&part;θ ,u) where u is the 4-velocity (tangent to
the geodesic). In the coordinates we are using, this yields
</p>
<p>g(&part;θ ,u)= g
(
&part;θ , ẋ
</p>
<p>μ&part;μ
)
= ẋμ g(&part;θ , &part;μ)︸ ︷︷ ︸
</p>
<p>r2δθμ
</p>
<p>= r2θ̇ .
</p>
<p>This quantity (because of Proposition 37.2.4 and the fact that &part;θ is a Killing
vector field) is a constant of the motion, and its initial value will be an at-
tribute of the particle during its entire motion. We assign zero to this con-
stant, i.e., we assume that initially θ̇ = 0. This is possible, because by rotat-
ing our spacetime&mdash;an allowed operation due to rotational symmetry&mdash;we
take the equatorial plane θ = π/2 to be the initial plane of the motion. Then
the motion will be confined to this plane, because θ̇ = 0 for all time.
</p>
<p>For the parameter of the geodesic equation, choose proper time τ if the
geodesic is timelike (massive particles), and any (affine) parameter if the
geodesic is null (massless particles such as photons). Then gμν ẋμẋν = κ ,
where
</p>
<p>κ =
{
</p>
<p>1 for timelike geodesics,
</p>
<p>0 for null geodesics.
(37.42)
</p>
<p>In terms of our chosen coordinates (with θ = π/2), we have
</p>
<p>κ = gμν ẋμẋν = (1 &minus; 2M/r)ṫ2 &minus; (1 &minus; 2M/r)&minus;1ṙ2 &minus; r2ϕ̇2. (37.43)
</p>
<p>Next, we apply Proposition 37.2.4 to the time translation Killing vector
and write
</p>
<p>E = gμν ẋμξ ν = (1 &minus; 2M/r)ṫ, (37.44)
</p>
<p>where E is a constant of the motion and ξξξ = &part;t . In the case of massive
particles, as r &rarr; &infin;, i.e., as we approach special relativity, E becomes ṫ ,
which is the rest energy of a particle of unit mass.7 Therefore, it is natural to
interpret E for finite r as the total energy (including gravitational potential
energy) per unit mass of a particle on a geodesic.
</p>
<p>Finally, the other rotational Killing vector field &part;ϕ gives another constant
of motion,
</p>
<p>L= g(&part;ϕ,u)= r2ϕ̇, (37.45)
</p>
<p>which can be interpreted as the angular momentum of the particle. This
reduces to Kepler&rsquo;s second law: Equal areas are swept out in equal times, in
the limit of Newtonian (or weak) gravity. However, in strong gravitational
fields, spacetime is not Euclidean, and Eq. (37.45) cannot be interpreted as
&ldquo;areas swept out&rdquo;. Nevertheless, it is interesting that the &ldquo;form&rdquo; of Kepler&rsquo;s
second law does not change even in strong fields.
</p>
<p>7Recall that the 4-momentum of special relativity is pμ =mẋμ.</p>
<p/>
</div>
<div class="page"><p/>
<p>37.4 General Theory of Relativity 1171
</p>
<p>Solving for ṫ and ϕ̇ from (37.44) and (37.45) and inserting the result in
(37.43), we obtain
</p>
<p>1
</p>
<p>2
ṙ2 + 1
</p>
<p>2
</p>
<p>(
1 &minus; 2M
</p>
<p>r
</p>
<p>)(
L2
</p>
<p>r2
+ κ
</p>
<p>)
= 1
</p>
<p>2
E2. (37.46)
</p>
<p>It follows from this equation that the radial motion of a particle on a geodesic
is the same as that of a unit mass particle of energy E2/2 in ordinary one-
dimensional nonrelativistic mechanics moving in an effective potential
</p>
<p>V (r)= 1
2
</p>
<p>(
1 &minus; 2M
</p>
<p>r
</p>
<p>)(
L2
</p>
<p>r2
+ κ
</p>
<p>)
= 1
</p>
<p>2
κ &minus; κM
</p>
<p>r
+ L
</p>
<p>2
</p>
<p>2r2
&minus; ML
</p>
<p>2
</p>
<p>r3
. (37.47)
</p>
<p>Once we solve Eq. (37.46) for the radial motion in this effective potential,
we can find the angular motion and the time coordinate change from (37.44)
and (37.45). The new feature provided by GTR is that in the radial equation
of motion, in addition to the &ldquo;Newtonian term&rdquo; &minus;κM/r and the &ldquo;centrifugal
barrier&rdquo; L2/2r2, we have the new term &minus;ML2/r3, which, although a small
correction for large r , will dominate over the centrifugal barrier term for
small r .
</p>
<p>Massive Particle
</p>
<p>Let us consider first the massive particle case, κ = 1. The extrema of the
effective potential are given by
</p>
<p>Mr2 &minus;L2r + 3ML2 = 0 &rArr; R&plusmn; =
L2 &plusmn;
</p>
<p>&radic;
L4 &minus; 12L2M2
</p>
<p>2M
. (37.48)
</p>
<p>Thus, if L2 &lt; 12M2, no extrema exist, and a particle heading toward the
center of attraction, will fall directly to the Schwarzschild radius r = 2M ,
the zero of the effective potential, and finally into the spacetime singularity
r = 0.
</p>
<p>For L2 &gt; 12M2, the reader may check that R+ is a minimum of V (r),
while R&minus; is a maximum. It follows that stable (unstable) circular orbits exist
at the radius r = R+ (r = R&minus;). In the Newtonian limit of M ≪ L, we get
R+ &asymp; L2/M , which agrees with the calculation in Newtonian gravity (Prob-
lem 37.26). Furthermore, Eq. (37.48) puts a restriction of R+ &gt; 6M on R+
and 3M &lt;R&minus; &lt; 6M on R&minus;. This places the planets of the Sun safely in the
region of stable circular orbits.
</p>
<p>If a massive particle is displaced slightly from its stable equilibrium ra-
dius R+, it will oscillate radially with a frequency ωr given by8
</p>
<p>ω2r =
d2V
</p>
<p>dr2
</p>
<p>∣∣∣∣
r=R+
</p>
<p>= M(R+ &minus; 6M)
R4+ &minus; 3MR3+
</p>
<p>.
</p>
<p>8In the Taylor expansion of any potential V (r) about the equilibrium position r0 of a
particle (of unit mass), it is the second derivative term that resembles Hooke&rsquo;s potential,
1
2 kx
</p>
<p>2 with k = (d2V/dr2)r0 .</p>
<p/>
</div>
<div class="page"><p/>
<p>1172 37 Riemannian Geometry
</p>
<p>On the other hand, the orbital frequency is given by Eq. (37.45),
</p>
<p>ω2ϕ =
L2
</p>
<p>R4+
= M
</p>
<p>R3+ &minus; 3MR2+
,
</p>
<p>where L2 has been calculated from (37.48) and inserted in this equation. In
the Newtonian limit of M ≪ R+, we have ωϕ &asymp; ωr &asymp;M/R3+. If ωϕ = ωr ,
the particle will return to a given value of r in exactly one orbital period and
the orbit will close. The difference between ωϕ and ωr in GTR means that
the orbit will precess at a rate of
</p>
<p>ωp = ωϕ &minus;ωr = (1 &minus;ωr/ωϕ)ωϕ =&minus;
[
(1 &minus; 6M/R+)1/2 &minus; 1
</p>
<p>]
ωϕ,
</p>
<p>which in the limit of M ≪R+ reduces to
</p>
<p>ωp &asymp;
3M3/2
</p>
<p>R
5/2
+
</p>
<p>= 3(GM)
3/2
</p>
<p>c2R
5/2
+
</p>
<p>,
</p>
<p>where in the last equality, we restored the factors of G and c. If we include
the eccentricity e and denote the semimajor axis of the elliptical orbit by a,
then the formula above becomes (see [Misn 73, p. 1110])
</p>
<p>ωp &asymp;
3(GM)3/2
</p>
<p>c2(1 &minus; e2)a5/2 . (37.49)
</p>
<p>Due to its proximity to the Sun, Mercury shows the largest precession
frequency, which, after subtracting all other effects such as perturbations due
to other planets, is 43 seconds of arc per century. This residual precession
rate had been observed prior to the formulation of GTR and had been an
unexplained mystery. Its explanation was one of the most dramatic early
successes of GTR.
</p>
<p>Massless Particle
</p>
<p>We now consider the null geodesics. With κ = 0 in Eq. (37.47), the effective
potential becomes
</p>
<p>V (r)= L
2
</p>
<p>2r2
&minus; ML
</p>
<p>2
</p>
<p>r3
, (37.50)
</p>
<p>which has only a maximum at r = Rmax &equiv; 3M . It follows that in GTR, un-
stable circular orbits of photons exist at r = 3M , and that strong gravity has
significant effect on the propagation of light.
</p>
<p>The minimum energy required to overcome the potential barrier (and
avoid falling into the infinitely deep potential well) is given by
</p>
<p>1
</p>
<p>2
E2 = V (Rmax)=
</p>
<p>L2
</p>
<p>54M2
&rArr; L
</p>
<p>2
</p>
<p>E2
= 27M2.
</p>
<p>In flat spacetime, L/E is precisely the impact parameter b of a photon,
i.e., the distance of closest approach to the origin. Thus the Schwarzschild
geometry will capture any photon sent toward it if the impact parameter is</p>
<p/>
</div>
<div class="page"><p/>
<p>37.4 General Theory of Relativity 1173
</p>
<p>less than bc &equiv;
&radic;
</p>
<p>27M . Hence, the cross section for photon capture is
</p>
<p>σ &equiv; πb2c = 27πM2.
</p>
<p>To analyze the bending of light, we write Eq. (37.46) as
</p>
<p>1
</p>
<p>2
</p>
<p>(
dr
</p>
<p>dϕ
</p>
<p>)2
ϕ̇2 + V (r)= 1
</p>
<p>2
E2.
</p>
<p>Substituting for ϕ̇ from Eq. (37.45) and writing the resulting DE in terms of
a new variable u=M/r , we obtain
</p>
<p>(
du
</p>
<p>dϕ
</p>
<p>)2
+ u2(1 &minus; 2u)= b
</p>
<p>2
</p>
<p>M2
, b&equiv; E
</p>
<p>L
,
</p>
<p>where we used Eq. (37.50) for the effective potential. Differentiating this
with respect to ϕ, we finally get the second-order DE,
</p>
<p>d2u
</p>
<p>dϕ2
+ u= 3u2. (37.51)
</p>
<p>In the large-impact-parameter or small-u approximation, we can ignore the
second-order term on the RHS and solve for u. This will give the equa-
tion of a line in polar coordinates. Substituting this solution on the RHS of
Eq. (37.51) and solving the resulting equation yields the deviation from a
straight line with a deflection angle of
</p>
<p>�ϕ &asymp; 4M
b
</p>
<p>= 4GM
bc2
</p>
<p>, (37.52)
</p>
<p>where we have restored the G&rsquo;s and the c&rsquo;s in the last step.
For a light ray grazing the Sun, b = R⊙ = 7 &times; 108 m and M = M⊙ =
</p>
<p>2 &times; 1030 kg, so that Eq. (37.52) predicts a deflection of 1.747 seconds of
arc. This bending of starlight passing near the Sun has been observed many
times, beginning with the 1919 expedition led by Eddington. Because of the
intrinsic difficulty of such measurements, these observations confirm GTR
only to within 10 % accuracy. However, the bending of radio waves emitted
by quasars has been measured to an accuracy of 1 %, and the result has been
shown to be in agreement with Eq. (37.52) to within this accuracy.
</p>
<p>The last topic we want to discuss, a beautiful illustration of Proposi-
tion 37.2.4 is the gravitational redshift. Let O1 and O2 be two static ob- gravitational redshift
</p>
<p>discussedservers (by which we mean that they each move on an integral curve of
the Killing vector field ξξξ ). It follows that the 4-velocities u1 and u2 of the
observers are proportional to ξξξ . Since u1 and u2 have unit lengths, we have
</p>
<p>ui =
ξξξ i&radic;
</p>
<p>g(ξξξ i,ξξξ i)
, i = 1,2.
</p>
<p>Suppose O1 emits a light beam and O2 receives it. Since light travels on a
geodesic, Proposition 37.2.4 gives
</p>
<p>g(u,ξξξ1)= g(u,ξξξ2), (37.53)</p>
<p/>
</div>
<div class="page"><p/>
<p>1174 37 Riemannian Geometry
</p>
<p>where u is tangent to the light trajectory (or the light signal&rsquo;s 4-velocity).
The frequency of light for any observer is the time component of its 4-
velocity, and because the 4-velocity of an observer has the form (1,0,0,0)
in the frame of that observer, we can write this invariantly as
</p>
<p>ωi = g(u,ui)=
g(u,ξξξ i)&radic;
g(ξξξ i,ξξξ i)
</p>
<p>, i = 1,2.
</p>
<p>In particular, using Eq. (37.53), we obtain
</p>
<p>ω1
</p>
<p>ω2
= g(u,u1)
</p>
<p>g(u,u2)
= g(u,ξξξ1)/
</p>
<p>&radic;
g(ξξξ1,ξξξ1)
</p>
<p>g(u,ξξξ2)/
&radic;
g(ξξξ2,ξξξ2)
</p>
<p>=
&radic;
g(ξξξ2,ξξξ2)&radic;
g(ξξξ1,ξξξ1)
</p>
<p>=
&radic;
</p>
<p>1 &minus; 2M/r2
1 &minus; 2M/r1
</p>
<p>,
</p>
<p>where we used g(ξξξ,ξξξ) = g00 = (1 &minus; 2M/r) for the Schwarzschild space-
time, and r1 and r2 are the radial coordinates of the observers O1 and O2,
respectively. In terms of wavelengths, we have
</p>
<p>λ1
</p>
<p>λ2
=
&radic;
</p>
<p>1 &minus; 2M/r1
1 &minus; 2M/r2
</p>
<p>. (37.54)
</p>
<p>It follows from Eq. (37.54) that as light moves toward regions of weak
gravity (r2 &gt; r1), the wavelength increases (λ2 &gt; λ1), i.e., it will be &ldquo;red-
shifted&rdquo;. This makes sense, because an increase in distance from the center
implies an increase in the gravitational potential energy, and, therefore, a
decrease in a photon&rsquo;s energy �ω. Pound and Rebka used the M&ouml;ssbauer ef-
fect in 1960 to measure the change in the wavelength of a beam of light as it
falls down a tower on the surface of the Earth. They found that, to within the
1 % experimental accuracy, the GTR prediction of the gravitational redshift
was in agreement with their measurement.
</p>
<p>37.5 Problems
</p>
<p>37.1 Show that a derivative &nabla;X defined by Eq. (37.2) satisfies the four con-
ditions of Proposition 36.2.11.
</p>
<p>37.2 Using Eq. (36.6) show that the Levi-Civita connection satisfies
</p>
<p>Z
(
g(X,Y)
</p>
<p>)
= g(&nabla;ZX,Y)+ g(X,&nabla;ZY).
</p>
<p>Write down similar relations for X(g(Y,Z)) and Y(g(X,Z)). Now compute
</p>
<p>X
(
g(Y,Z)
</p>
<p>)
+ Y
</p>
<p>(
g(X,Z)
</p>
<p>)
&minus; Z
</p>
<p>(
g(X,Y)
</p>
<p>)
</p>
<p>and use the fact that Levi-Civita connection is torsion-free to arrive at (37.2).
</p>
<p>37.3 Derive Eq. (37.3) by letting X = &part;i , Y = &part;j , and Z = &part;k in Eq. (37.2).
</p>
<p>37.4 Let A and B be matrices whose elements are one-forms. Show that
</p>
<p>(A &and; B)t =&minus;Bt &and; At .</p>
<p/>
</div>
<div class="page"><p/>
<p>37.5 Problems 1175
</p>
<p>37.5 Write Eq. (37.13) in component form and derive Eq. (37.15).
</p>
<p>37.6 Find dω̂ωω if
</p>
<p>ω̂ωω=
(
</p>
<p>0 &minus; cot θǫǫǫϕ
cot θǫǫǫϕ 0
</p>
<p>)
,
</p>
<p>where (θ,ϕ) are coordinates on the unit sphere S2.
</p>
<p>37.7 Find the curvature of the two-dimensional space whose arc length is
given by ds2 = dx2 + x2dy2.
</p>
<p>37.8 Find the curvature of the three-dimensional space whose arc length is
given by ds2 = dx2 + x2dy2 + dz2.
</p>
<p>37.9 Find the curvature tensors of the Friedmann and Schwarzschild spaces
given in Example 37.1.8.
</p>
<p>37.10 Consider the Euclidean space R3.
</p>
<p>(a) Show that in this space, the composite operator d ◦ &lowast; gives the curl of
a vector when the vector is written as components of a two-form.
</p>
<p>(b) Similarly, show that &lowast; ◦ d is the divergence operator for one-forms.
(c) Use these results and the procedure of Example 37.1.9 to find expres-
</p>
<p>sions for the curl and the divergence of a vector in curvilinear coordi-
nates.
</p>
<p>37.11 Prove the statement in Box 37.1.2.
</p>
<p>37.12 Start with d2xi/dt2 = 0, the geodesic equations in Cartesian coor-
dinates. Transform these equations to spherical coordinates (r, θ,ϕ) using
x = r sin θ cosϕ, y = r sin θ sinϕ, and z= r cos θ , and the chain rule. From
these equations read off the connection coefficients in spherical coordinates
[refer to Eq. (36.44)]. Now use Eq. (36.33) and Definition 36.2.22 to evalu-
ate the divergence of a vector.
</p>
<p>37.13 Find the geodesics of a manifold whose arc element is ds2 = dx2 +
dy2 + dz2.
</p>
<p>37.14 Find the geodesics of the metric ds2 = dx2 + x2dy2.
</p>
<p>37.15 Find the differential equation for the geodesics of the surface of a
sphere of radius a having the line element ds2 = a2dθ2 + a2 sin2 θdϕ2.
Verify that
</p>
<p>A cosϕ +B sinϕ + cot θ = 0
is the intersection of a plane passing through the origin and the sphere. Also,
show that it is a solution of the differential equation of the geodesics. Hence,
the geodesics of a sphere are great circles.</p>
<p/>
</div>
<div class="page"><p/>
<p>1176 37 Riemannian Geometry
</p>
<p>37.16 The Riemann normal coordinates are given by xi = ai t . For each set
of ai , one obtains a different set of geodesics. Thus, we can think of ai as
the parameters that distinguish among the geodesics.
</p>
<p>(a) By keeping all ai (and t) fixed except the j th one and using the defi-
nition of tangent to a curve, show that nj = t&part;j , where nj is (one of)
the n(&rsquo;s) appearing in the equation of geodesic deviation.
</p>
<p>(b) Substitute (a) plus ui = ẋi = ai in Eq. (37.29) to show that
</p>
<p>Rmijk +Rmjik = 3
(
Ŵmik,j + Ŵmjk,i
</p>
<p>)
.
</p>
<p>Substitute for one of the Ŵ&rsquo;s on the RHS using Eq. (36.45).
(c) Now use the cyclic property of the lower indices of the curvature ten-
</p>
<p>sor to show that
</p>
<p>Ŵmij,k =&minus;
1
</p>
<p>3
</p>
<p>(
Rmijk +Rmjik
</p>
<p>)
.
</p>
<p>37.17 Let ω be a 1-form and v a vector.
</p>
<p>(a) Show that the covariant and Lie derivatives, when applied to a 1-form,
are related by
</p>
<p>〈Luω,v〉 = 〈&nabla;uω,v〉 + 〈ω,&nabla;vu〉.
</p>
<p>(b) Use this to derive the identity
</p>
<p>(Luω)i = (&nabla;uω)i +ωj (&nabla;eiu)j .
</p>
<p>37.18 Show that Eq. (37.19) leads to Eq. (37.20).
</p>
<p>37.19 Show that a vector field that generates a conformal transformation
satisfies
</p>
<p>Xk&part;kgij + &part;iXkgkj + &part;jXkgki =&minus;ψgij .
</p>
<p>37.20 Use the symmetries of Rijkl [Eqs. (37.14) and (37.15)] to show that
Rijkl =Rklij and R[ijkl] = 0.
</p>
<p>37.21 Use the symmetry properties of Riemann curvature tensor to show
that
</p>
<p>(a) Riijk =R
j
</p>
<p>mij = 0, and
(b) Rij =Rji .
(c) Show that Ri
</p>
<p>jkl;i +Rjk;l &minus;Rj l;k = 0, and conclude that &nabla; &middot;G= 0, or,
in component form, G ki ;k = 0.
</p>
<p>37.22 Show that in an n-dimensional manifold without metric the number
of independent components of the Riemann curvature tensor is
</p>
<p>n3(n&minus; 1)
2
</p>
<p>&minus; n
2(n&minus; 1)(n&minus; 2)
</p>
<p>6
= n
</p>
<p>2(n2 &minus; 1)
3
</p>
<p>.</p>
<p/>
</div>
<div class="page"><p/>
<p>37.5 Problems 1177
</p>
<p>If the manifold has a metric, the number of components reduces to
</p>
<p>[
n(n&minus; 1)
</p>
<p>2
</p>
<p>]2
&minus; n
</p>
<p>2(n&minus; 1)(n&minus; 2)
6
</p>
<p>= n
2(n2 &minus; 1)
</p>
<p>12
.
</p>
<p>37.23 Consider Einstein&rsquo;s equation R&minus; 12Rg= κT.
(a) Take the trace of both sides of the equation to obtain R = &minus;κT μμ &equiv;
</p>
<p>&minus;κT .
(b) Use (a) to obtain R00 = 12κ(T00 + T
</p>
<p>j
</p>
<p>j ).
(c) Now use the fact that in Newtonian limit Tij ≪ T00 &asymp; ρ to con-
</p>
<p>clude that agreement of Einstein&rsquo;s and Newton&rsquo;s gravity demands that
κ = 8π in units in which the universal gravitational constant is unity.
</p>
<p>37.24 Let Eij be the most general second-rank symmetric tensor con-
structed from the metric and Riemann curvature tensors that is linear in the
curvature tensor.
</p>
<p>(a) Show that
</p>
<p>Eij = aRij + bgijR+Λgij ,
where a, b, and Λ are constants.
</p>
<p>(b) Show that Eij has a vanishing divergence if and only if b=&minus; 12a.
(c) Show that in addition, Eij vanishes in flat space-time if and only if
</p>
<p>Λ= 0.
</p>
<p>37.25 Show that R+ and R&minus;, as given by Eq. (37.48) are, respectively, a
minimum and a maximum of V (r).
</p>
<p>37.26 Use Newtons second law of motion to show that in a circular orbit of
radius R, we have L2 =GMR.
</p>
<p>37.27 Show that R+ &gt; 6M and 3M &lt; R&minus; &lt; 6M , where R&plusmn; are given by
Eq. (37.48).
</p>
<p>37.28 Calculate the energy of a circular orbit using Eq. (37.46), and show
that
</p>
<p>E(R)= R &minus; 2M&radic;
R2 &minus; 3MR
</p>
<p>,
</p>
<p>where R =R&plusmn;.
</p>
<p>37.29 Show that the radial frequency of oscillation of a massive particle in
a stable orbit of radius R+ is given by
</p>
<p>ω2r =
M(R+ &minus; 6M)
R4+ &minus; 3MR+
</p>
<p>.
</p>
<p>37.30 Derive Eq. (37.52) from Eq. (37.51).</p>
<p/>
</div>
<div class="page"><p/>
<p>References
</p>
<p>[Abra 85] Abraham, R., Marsden, J.: Foundations of Mechanics, 2nd edn. Addison-
Wesley, Reading (1985)
</p>
<p>[Abra 88] Abraham, R., Marsden, J., Ratiu, T.: Manifolds, Tensor Analysis, and Ap-
plications, 2nd edn. Springer, Berlin (1988)
</p>
<p>[Axle 96] Axler, S.: Linear Algebra Done Right. Springer, Berlin (1996)
[Baru 86] Barut, A., Raczka, R.: Theory of Group Representations and Applications.
</p>
<p>World Scientific, Singapore (1986)
[Benn 87] Benn, I.M., Tucker, R.W.: An Introduction to Spinors and Geometry with
</p>
<p>Applications in Physics. Adam Hilger, Bristol (1987)
[Birk 77] Birkhoff, G., MacLane, S.: Modern Algebra, 4th edn. Macmillan, London
</p>
<p>(1977)
[Birk 78] Birkhoff, G., Rota, G.-C.: Ordinary Differential Equations, 3rd edn. Wiley,
</p>
<p>New York (1978)
[Bish 80] Bishop, R., Goldberg, S.: Tensor Analysis on Manifolds. Dover, New York
</p>
<p>(1980)
[Ble 81] Bleecker, D.: Gauge Theory and Variational Principles. Addison-Wesley,
</p>
<p>Reading (1981)
[Bly 90] Blyth, T.: Module Theory. Oxford University Press, Oxford (1990)
</p>
<p>[Boer 63] Boerner, H.: Representation of Groups. North-Holland, Amsterdam (1963)
[Chur 74] Churchill, R., Verhey, R.: Complex Variables and Applications, 3rd edn.
</p>
<p>McGraw-Hill, New York (1974)
[Cour 62] Courant, R., Hilbert, D.: Methods of Mathematical Physics, vol. 1. Inter-
</p>
<p>science, New York (1962)
[Denn 67] Dennery, P., Krzywicki, A.: Mathematics for Physicists. Harper and Row,
</p>
<p>New York (1967)
[DeVi 90] DeVito, C.: Functional Analysis and Linear Operator Theory. Addison-
</p>
<p>Wesley, Reading (1990)
[Flan 89] Flanders, H.: Differential Forms with Applications to Physical Sciences.
</p>
<p>Dover, New York (1989)
[Frie 82] Friedman, A.: Foundations of Modern Analysis. Dover, New York (1982)
[Fult 91] Fulton, W., Harris, J.: Representation Theory. Springer, Berlin (1991)
</p>
<p>[Grad 65] Gradshteyn, I., Ryzhik, I.: Table of Integrals, Series, and Products. Aca-
demic Press, New York (1965)
</p>
<p>[Greu 78] Greub, W.: Multilinear Algebra, 2nd edn. Springer, Berlin (1978)
[Halm 58] Halmos, P.: Finite Dimensional Vector Spaces, 2nd edn. Van Nostrand,
</p>
<p>Princeton (1958)
[Hame 89] Hamermesh, M.: Group Theory and Its Application to Physical Problems.
</p>
<p>Dover, New York (1989)
[Hass 08] Hassani, S.: Mathematical Methods for Students of Physics and Related
</p>
<p>Fields, 2nd edn. Springer, Berlin (2008)
[Hell 67] Hellwig, G.: Differential Operators of Mathematical Physics. Addison-
</p>
<p>Wesley, Reading (1967)
[Hill 87] Hildebrand, F.: Statistical Mechanics. Dover, New York (1987)
</p>
<p>[Koba 63] Kobayashi, S., Nomizu, K.: Foundations of Differential Geometry, vol. I.
Wiley, New York (1963)
</p>
<p>[Lang 85] Lang, S.: Complex Analysis, 2nd edn. Springer, Berlin (1985)
</p>
<p>S. Hassani, Mathematical Physics, DOI 10.1007/978-3-319-01195-0,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>1179</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-01195-0">http://dx.doi.org/10.1007/978-3-319-01195-0</a></div>
</div>
<div class="page"><p/>
<p>1180 References
</p>
<p>[Lorr 88] Lorrain, P., Corson, D., Lorrain, F.: Electromagnetic Fields and Waves, 3rd
edn. Freeman, New York (1988)
</p>
<p>[Mack 68] Mackey, G.: Induced Representations. Benjamin, Elmsford (1968)
[Mari 80] Marion, J., Heald, M.: Classical Electromagnetic Radiation, 2nd edn. Aca-
</p>
<p>demic Press, New York (1980)
[Math 70] Mathews, J., Walker, R.: Mathematical Methods of Physics, 2nd edn. Ben-
</p>
<p>jamin, Elmsford (1970)
[Mess 66] Messiah, A.: Quantum Mechanics (2 volumes). Wiley, New York (1966)
[Mill 68] Miller, W.: Lie Theory and Special Functions. Academic Press, New York
</p>
<p>(1968)
[Misn 73] Misner, C., Thorne, K., Wheeler, J.: Gravitation. Freeman, New York
</p>
<p>(1973)
[Olve 86] Olver, P.: Application of Lie Groups to Differential Equations. Springer,
</p>
<p>Berlin (1986)
[Reed 80] Reed, M., Simon, B.: Functional Analysis (4 volumes). Academic Press,
</p>
<p>New York (1980)
[Rich 78] Richtmyer, R.: Principles of Advanced Mathematical Physics. Springer,
</p>
<p>Berlin (1978)
[Rotm 84] Rotman, J.: An Introduction to the Theory of Groups, 3rd edn. Allyn and
</p>
<p>Bacon, Needham Heights (1984)
[Saun 89] Saunders, D.: The Geometry of Jet Bundles. Cambridge University Press,
</p>
<p>Cambridge (1989)
[Simm 83] Simmons, G.: Introduction to Topology and Modern Analysis. Krieger,
</p>
<p>Melbourne (1983)
[Stak 79] Stakgold, I.: Green&rsquo;s Functions and Boundary Value Problems. Wiley, New
</p>
<p>York (1979)
[Tric 55] Tricomi, F.: Vorlesungen &uuml;ber Orthogonalreihen. Springer, Berlin (1955)
[Vara 84] Varadarajan, V.: Lie Groups, Lie Algebras and Their Representations.
</p>
<p>Springer, Berlin (1984)
[Wald 84] Wald, R.: General Relativity. University of Chicago Press, Chicago (1984)
[Warn 83] Warner, F.: Foundations of Differentiable Manifolds and Lie Groups.
</p>
<p>Springer, Berlin (1983)
[Wein 95] Weinberg, S.: The Quantum Theory of Fields (2 volumes). Cambridge Uni-
</p>
<p>versity Press, Cambridge (1995)
[Zeid 95] Zeidler, E.: Applied Functional Analysis. Springer, Berlin (1995)</p>
<p/>
</div>
<div class="page"><p/>
<p>Index
</p>
<p>A
Abel, 115, 246, 251, 301, 475, 523, 703
</p>
<p>biography, 712
Abelian group, 704
Abelian Lie algebra, 937
Abel&rsquo;s identity, 454
Absolute convergence, 319
Addition theorem for spherical harmonics,
</p>
<p>412, 413
Additive identity, 20
Adjoint, 147, 171, 177, 434, 454, 495,
</p>
<p>551, 564, 614, 618, 624, 675,
677, 754, 1092, 1102
</p>
<p>classical, 56, 57, 155
matrix of, 152&ndash;155
</p>
<p>differential operators, 433&ndash;436
formal, 612, 648
operator, 56, 57
</p>
<p>Adjoint action, 929
Adjoint algebra, 944
Adjoint boundary conditions, 615, 619
Adjoint Green&rsquo;s function, 618
Adjoint map, 926
Adjoint of a matrix, 144
Adjoint of an operator, 113
Adjoint representation, 732
</p>
<p>character, 737
Affine group, 948
Affine motion, 917
Affine parameter, 1138
Airy&rsquo;s DE, 456, 490
Algebra, 63&ndash;72, 101, 119, 191, 398, 515,
</p>
<p>740, 784, 797, 829, 891, 921,
942
</p>
<p>antiderivation, 82, 829, 831, 889, 891
associative, 63
automorphism, 70
center, 64
central, 65
central simple, 79, 93, 845
Clifford, 800, 829&ndash;855
</p>
<p>anticenter, 839&ndash;842
canonical element, 838, 839
center, 839&ndash;842
construction, 830&ndash;834
Dirac equation, 832&ndash;834
general classification, 843&ndash;846
</p>
<p>homomorphism with other
algebras, 837, 838
</p>
<p>isomorphisms, 842, 843
properties, 834&ndash;843
</p>
<p>commutative, 63
complex numbers, 295
decomposition, 83&ndash;95
definition, 63
derivation, 81, 82, 99, 106, 868, 887,
</p>
<p>891, 1124
derivation of, 80&ndash;83
derivation of an, 80
derived, 66
dimension of an, 63
epimorphism, 70
exterior, 794&ndash;801
factor, 77, 78
generator, 70
homomorphism, 70, 71
ideal, 73
involution, 72, 82, 836, 837, 839, 843,
</p>
<p>848, 989, 990
isomorphism, 70
Lie, 915&ndash;936
monomorphism, 70
operator, 101&ndash;107
opposite, 66
polynomial, 95&ndash;97
quaternions, 69
representation, 125&ndash;131
semi-simple, 88&ndash;91, 92, 92, 94, 130,
</p>
<p>764, 799, 844
simple, 76
</p>
<p>classification, 92&ndash;95
structure constants, 68
symmetric, 791
tensor, 784
tensor product, 68
total matrix, 78&ndash;80
unital, 63
</p>
<p>Algebra direct sum, 67
Algebra of endomorphisms, 67
Algebra of linear operators, 67
Algebra of polynomials, 67
Algebra tensor product, 68
Algebraic equation, 1010
</p>
<p>symmetry group of, 1010
</p>
<p>S. Hassani, Mathematical Physics, DOI 10.1007/978-3-319-01195-0,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>1181</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-01195-0">http://dx.doi.org/10.1007/978-3-319-01195-0</a></div>
</div>
<div class="page"><p/>
<p>1182 Index
</p>
<p>Algebraic multiplicity, 174
Analytic continuation, 372&ndash;378
</p>
<p>SOLDE, 459
Analytic function, 297&ndash;304
</p>
<p>definition, 301
derivative, 297
derivatives as integrals, 316
entire, 301
poles of, 342
roots (zeros) of, 330
</p>
<p>Angular momentum, 25, 398, 933, 976
addition theorem, 973
commutation relation, 399
eigenvalues, 404
eigenvector, 406&ndash;413
intrinsic spin, 1073
operator, 398
</p>
<p>eigenvalues, 401&ndash;405
in spherical coordinates, 401
</p>
<p>orbital, 1073
quantum mechanics, 405
</p>
<p>Annihilator, 51, 61, 86
left, 73
right, 73
</p>
<p>Anticommutator, 111
Antiderivation, 82, 829, 831, 889, 891
Antisymmetric bilinear form, 707
Antisymmetric representation, 732
Antisymmetrizer, 793
Antisymmetry, 793
Arc length, 1146
Associated bundle, 1084&ndash;1086
</p>
<p>vector bundle, 1117&ndash;1120
vector field
</p>
<p>horizontal, 1091
Associated Legendre functions, 408
Associative algebra, 63
Asymptotic expansion, 385
Atoms, 480&ndash;482
Automorphism, 43, 74, 836, 841, 945,
</p>
<p>1085, 1098, 1102
algebra, 70
group, 705
PFB, 1081
</p>
<p>Azimuthal symmetry, 411
</p>
<p>B
Baker-Campbell-Hausdorff formula, 110
Banach space, 218, 1048
Basis, 23
</p>
<p>dual of, 51
dual of a, 782
oriented, 800
orthonormal, 32
standard, 23
transformation matrix, 149
</p>
<p>Becquerel, 896
Bernoulli, 482, 1056
Bessel, 246, 671, 1056
</p>
<p>biography, 481
</p>
<p>Bessel DE, 432, 482
Bessel equation, 466
</p>
<p>Liouville substitution, 570
Bessel function, 482&ndash;485, 586
</p>
<p>asymptotic behavior, 502&ndash;505
large argument, 504
large order, 503
</p>
<p>confluent hypergeometric, 483
first kind, 483
generating function, 499
integral representation of, 498&ndash;505
modified
</p>
<p>first kind, 391, 484
second kind, 392, 484
</p>
<p>oscillation of, 432
recurrence relation, 485
second kind, 483
spherical, 487, 593
third kind, 484
</p>
<p>Bessel imaginary (bei), 590
Bessel inequality, 219
Bessel real (ber), 590
Beta function, 378&ndash;381
</p>
<p>definition, 380
Bianchi&rsquo;s identities, 1123
Bianchi&rsquo;s identity, 1094
</p>
<p>abelian case, 1095
Bijective map, 6
Bilinear inner product
</p>
<p>complex, 46
Binary operation, 7
Binomial theorem, 13
Birkhoff&rsquo;s theorem, 1169
Block diagonal, 171, 200
Bohr radius, 481
Bohr-Sommerfeld quantization, 452
Bolzano, 11
Bolzano-Weierstrass property, 522
Bolzano-Weierstrass theorem, 522
Boole, 755
Boundary conditions
</p>
<p>adjoint, 615
Dirichlet, 617, 642
general unmixed, 617
homogeneous, 611
mixed, 616
</p>
<p>elliptic PDE, 673
Neumann, 617, 643
periodic, 571, 617
separated, 566
unmixed, 616
</p>
<p>Boundary functionals, 612
Boundary point, 520
Boundary value problem, 612, 635
</p>
<p>Dirichlet, 642, 665&ndash;671
Neumann, 643, 671&ndash;673
</p>
<p>Bounded operator, 513&ndash;517
continuity, 514
</p>
<p>Bra, 20
Branch cut, 366</p>
<p/>
</div>
<div class="page"><p/>
<p>Index 1183
</p>
<p>Brouwer, 957
Bundle
</p>
<p>associated, 1084&ndash;1086
vector bundle, 1117&ndash;1120
</p>
<p>Clifford, 1101
cotangent, 882
spinor, 1101
tangent, 877
tensor, 883
</p>
<p>Bundle of linear frames, 1084, 1120
canonical form, 1121
</p>
<p>Bundle space, 1080
BWHB theorem, 960
</p>
<p>C
Calculus of variations, 1047&ndash;1061
</p>
<p>symmetry groups, 1062&ndash;1065
Canonical 1-form, 928
Canonical basis, 802, 902
</p>
<p>SOLDE, 463
Canonical coordinates, 902
Canonical flat connection, 1095
Canonical form, 1121
Canonical transformation, 902, 907
Cantor, 523, 792, 897
</p>
<p>biography, 10
Cantor set, 12
Cardinality, 10&ndash;12, 23
Cartan, 896, 946, 1015
</p>
<p>biography, 799
Cartan metric tensor, 945, 970
Cartan subalgebra, 948
Cartan&rsquo;s lemma, 798
Cartesian product, 2, 7
Cartesian vectors, 19
Casimir operator, 969, 970, 971, 971,
</p>
<p>975&ndash;977, 979, 980
Cauchy, 154, 340, 366, 475, 533, 581,
</p>
<p>640, 702, 1130
biography, 301
</p>
<p>Cauchy data, 636
Cauchy integral formula, 313
</p>
<p>for operators, 536
Cauchy problem, 636
</p>
<p>ill-posed, 642
Cauchy sequence, 9, 216, 526
Cauchy-Goursat theorem, 310
Cauchy-Riemann conditions, 298, 303
</p>
<p>differentiability, 300
Cauchy&rsquo;s inequality, 336
Cayley, 755, 799
Cayley&rsquo;s theorem, 761
Center, 64
Center of a group, 708
Central algebra, 75, 845
Central force field, 25
Central simple algebra, 93, 845
Centralizer, 708
Chain rule, 96
Champollion, 267
</p>
<p>Character
adjoint representation, 737
compound, 736, 737
conjugacy class, 736
group and its subgroup, 743&ndash;746
of a representation, 736
simple, 736, 737
symmetric group
</p>
<p>graphical construction, 767&ndash;774
Character table, 743
</p>
<p>for S2, 745
for S3, 745
</p>
<p>Characteristic hypersurface, 638
Characteristic polynomial, 173, 182, 197,
</p>
<p>536, 558, 591, 626
HNOLDE, 446
</p>
<p>Characteristic root, 173, 467
Charged scalar field, 1114
Chebyshev, 639
Chebyshev polynomials, 253
Chevalley, biography, 971
Chevalley&rsquo;s theorem, 969
Christoffel, biography, 1130
Circle of convergence, 319
Circuit matrix, 462, 463
Circular heat-conducting plate, 600
Classical adjoint, 56, 57, 155
</p>
<p>matrix of, 152&ndash;155
Classical field theory
</p>
<p>conservation laws, 1069&ndash;1073
Noether&rsquo;s theorem, 1069&ndash;1073
symmetry, 1069&ndash;1073
</p>
<p>Classical orthogonal polynomial, 241&ndash;243
classification, 244, 245
generating functions, 257, 258
recurrence relations, 245&ndash;248
</p>
<p>Classification of simple algebras, 92&ndash;95
Clebsch, 246, 755
</p>
<p>biography, 753
Clebsch-Gordan
</p>
<p>coefficients, 756, 757
decomposition, 753&ndash;756, 774, 973
series, 754, 757
</p>
<p>Clifford, 799
Clifford algebra, 800, 829&ndash;855
</p>
<p>anticenter, 839&ndash;842
canonical element, 838, 839
center, 839&ndash;842
conjugation involution, 837
construction, 830&ndash;834
degree involution, 836
Dirac equation, 832&ndash;834
even element, 835
general classification, 843&ndash;846
homomorphism with other algebras,
</p>
<p>837, 838
isomorphisms, 842, 843
odd element, 835
properties, 834&ndash;843
representation, 987&ndash;1006</p>
<p/>
</div>
<div class="page"><p/>
<p>1184 Index
</p>
<p>Pauli spin matrices, 997&ndash;1001
Clifford algebra C13(R), 852&ndash;855,
</p>
<p>1004&ndash;1006
Clifford algebra Cνμ(R), 846&ndash;855
</p>
<p>classification, 851, 852
Clifford algebra Cνμ(R)
</p>
<p>spin representation, 1003
spinor space of, 1003
</p>
<p>Clifford algebra Cνμ(R)
standard basis, 1001
</p>
<p>Clifford algebra Cn0(R)
classification, 849, 850, 851
</p>
<p>Clifford algebra C0n(R)
classification, 849, 850, 851
</p>
<p>Clifford group, 987&ndash;994
Clifford product, 830
Closed form, 894
Closed subset, 520
Closure, 520
Codifferential, 900
</p>
<p>covariant, 1108
Codomain, 5
Cofactor, 153, 155
Commutative algebra, 63
Commutative group, 704
Commutative Lie algebra, 937
Commutator, 106, 107
</p>
<p>diagonalizability, 186
Commutator subgroup, 707
Compact Lie algebra, 945
Compact Lie group
</p>
<p>representation, 953&ndash;963
Compact operator, 523&ndash;526
</p>
<p>spectral theorem, 527&ndash;534
spectrum, 527
</p>
<p>Compact resolvent, 563&ndash;569
Compact set, 519&ndash;523
Compact subset, 522
Compact support, 234, 898
Comparison theorem, 430&ndash;432
Complement of a set, 2
Complete metric space, 10
Complete o.n. sequence, 219
Completeness relation, 123, 148, 220,
</p>
<p>228, 658
Complex coordinate space, 21
Complex exponential function, 302
Complex FOLDEs, 460&ndash;462
Complex function, 295, 296
</p>
<p>analytic, 301
analytic continuation, 372
branch cut, 366
branch point of, 365
Cauchy-Riemann conditions, 298
continuous, 296
derivatives as integrals, 315&ndash;319
entire, 301
essential singularity, 342
integration, 309&ndash;315
isolated singularity, 339
</p>
<p>isolated zero, 330
meromorphic, 363
multivalued
</p>
<p>branch, 367
Riemann sheet, 367
Riemann surface, 367
</p>
<p>pole of order m, 342
power series, 319
</p>
<p>circle of convergence, 319
principal part of, 342
removable singular point, 342
simple pole, 342
simple zero, 330
zero of order k, 329
</p>
<p>Complex GL(V), 922
Complex hyperbolic function, 303
Complex plane
</p>
<p>contour in, 309
curve in the, 309
multiply connected region, 312
path in the, 309
simply connected region, 312
</p>
<p>Complex potential, 305
Complex series, 319&ndash;321
Complex SOLDE, 463&ndash;469
Complex structure, 45&ndash;48, 139, 163, 202
Complex trigonometric function, 303
Complex vector space, 20
Complexification, 48, 102, 202
Composition of maps, 5
Compound character, 737
Conducting cylindrical can, 586&ndash;588
Confluent hypergeometric function,
</p>
<p>478&ndash;485
definition, 479
integral representation of, 497, 498
</p>
<p>Conformal group, 1159
in 2 dimensions, 1159
</p>
<p>Conformal Killing vector, 1158
Conformal map, 304&ndash;308, 309
</p>
<p>definition, 305
translation, 306
</p>
<p>Conformal transformation, 1158
special, 1159
</p>
<p>Conic sections, 196
Conjugacy class, 711, 736, 737
Conjugate, 711
Conjugate subgroup, 707
Conjugation
</p>
<p>operators, 113, 114
Conjunct, 612, 614, 628
Connection, 1086&ndash;1091
</p>
<p>flat, 1095, 1096
Levi-Civita, 1145
linear, 1120&ndash;1140
</p>
<p>definition, 1121
local expression, 1087&ndash;1089
matrix structure group, 1096, 1097
metric, 1143&ndash;1155
vector bundle, 1117&ndash;1120</p>
<p/>
</div>
<div class="page"><p/>
<p>Index 1185
</p>
<p>Connection 1-form, 1087
Connection coefficients, 1133
Conservation law, 963, 1065&ndash;1069
</p>
<p>characteristic, 1067
classical field theory, 1069&ndash;1073
equivalent, 1067
trivial of the first kind, 1066
trivial of the second kind, 1066
</p>
<p>Conserved current density, 1065
Constant of the motion, 1066
Constrained systems, 905
Continuous index, 227&ndash;233
Contour, 309
</p>
<p>simple closed, 309
Contractable, 894
Contraction, 788
Contravariant degree, 784
Contravariant tensor, 784
Convergence
</p>
<p>infinite vector sum, 215&ndash;220
Convex subset, 528
Convolution theorem, 291
Coordinate curve, 870
Coordinate frame, 870
Coordinate functions, 860
Coordinate representation of Lg&lowast;, 921
Coordinate system
</p>
<p>left-handed, 800
right-handed, 800
</p>
<p>Coordinate transformation
orientation preserving, 898
orientation reversing, 898
</p>
<p>Coset, 708
Cosmological constant, 1165
Cotangent bundle, 882
Coulomb, 1058
Coulomb potential, 466
Countably infinite set, 11
Covariant codifferential, 1108
Covariant degree, 784
Covariant derivative, 1117, 1123&ndash;1125,
</p>
<p>1133
directional, 1119
exterior, 1093
Lie derivative, 1135
</p>
<p>Covariant differential, 1125
Covariant tensor, 784
Crystallography, 275
Current density, 1065, 1110
</p>
<p>energy momentum, 1071
formula for, 1111
</p>
<p>Curvature, 1125&ndash;1132
abelian case, 1095
and gravity, 1161
as relative acceleration, 1160
matrix structure group, 1096, 1097
</p>
<p>Curvature form, 1093
principal fiber bundle, 1091&ndash;1097
structure equation, 1093
</p>
<p>Curvature scalar, 1163
</p>
<p>Curvature tensor field, 1126
Curvature transformation, 1126
Curve
</p>
<p>coordinate, 870
development of, 1137
differentiable, 866
</p>
<p>Curvilinear coordinates, 1150
Cyclic permutation, 717
Cyclic subgroup, 707
</p>
<p>D
</p>
<p>D&rsquo;Alembert
biography, 397
</p>
<p>D&rsquo;Alembert, 1057
Damping factor, 447
Darboux, 799, 1015
Darboux inequality, 313
Darboux theorem, 902
De Broglie, 907
Decomposition
</p>
<p>algebra, 83&ndash;95
Clebsch-Gordan, 753&ndash;756
</p>
<p>Dedekind, 11, 764, 792, 1130
Degeneracy
</p>
<p>energy, 656
lifting of, 748
</p>
<p>Degenerate eigenvectors, 402
Degenerate kernel, 556&ndash;559
Delta function, 229, 512, 624
</p>
<p>derivative of, 233
expansion
</p>
<p>Fourier, 273
general, 257
</p>
<p>Fourier transform, 279
Green&rsquo;s function, 644, 685
integral representation of, 281
Legendre polynomials, 256
limit of sequence, 229, 231
potential, 653
spherical harmonics, 692
step function, 231, 232
Sturm-Liouville eigenfunctions, 691
variational problem, 1054
</p>
<p>Dense subset, 520
Density function, 936
Density of states, 584
Derivation, 81, 82, 99, 106, 887, 891, 944,
</p>
<p>1124
of an algebra, 80&ndash;83
tangent vector, 868
</p>
<p>Derivation algebra, 82, 944
Derivative
</p>
<p>complex function, 315&ndash;319
covariant, 1117
function of operator, 108
functional, 1050&ndash;1053
Hilbert spaces, 1047&ndash;1050
of operators, 107&ndash;112
total, 1027</p>
<p/>
</div>
<div class="page"><p/>
<p>1186 Index
</p>
<p>Derivative operator, 40
unboundedness of, 515
</p>
<p>Derived algebra, 66, 98
Descartes, 791
Determinant, 7, 55, 56, 118, 153&ndash;155,
</p>
<p>158, 160&ndash;162, 173, 175, 201,
205, 557, 558, 567, 610, 641,
644, 661, 706, 719, 788, 800,
801, 806, 816&ndash;818, 839, 897,
898, 916, 924
</p>
<p>analytic definition of, 205
connection with trace, 161
derivative of, 161
exponential of trace, 162
minor, 153
relation to trace, 160
</p>
<p>Determinant function, 54&ndash;56, 152, 158,
159, 162, 167, 799, 805, 815,
838, 848
</p>
<p>dual, 158&ndash;160
normed, 815
</p>
<p>Determinant of a matrix, 151&ndash;160
Development, 1137
Diagonalization
</p>
<p>simultaneous, 185&ndash;188
Diffeomorphism, 865
Differentiable curve, 866
</p>
<p>tangent vector, 868
Differentiable manifold, 859&ndash;866
</p>
<p>dimension of a, 860
Differentiable map, 864
</p>
<p>coordinate expression of, 864
Differential
</p>
<p>of a constant map, 873
of a map, 872
real-valued maps, 874
</p>
<p>Differential equation
analytic, 460
analytic properties, 460&ndash;463
associated Legendre, 411
Bessel, 466
completely homogeneous problem,
</p>
<p>612
Euler, 471
Fuchsian, 469&ndash;473
</p>
<p>definition, 470
homogeneous, 418
hypergeometric, 466
</p>
<p>definition, 473
inhomogeneous, 418
Legendre, 411
linear, 418
</p>
<p>superposition principle, 423
multiparameter symmetry group,
</p>
<p>1040&ndash;1043
Riemann, 471
second order linear
</p>
<p>behavior at infinity, 469
second-order linear
</p>
<p>Frobenius method, 440
</p>
<p>regular, 422
symmetry group, 1014&ndash;1024
</p>
<p>Differential form, 888
closed, 894
exact, 894
Lorentz force law, 892
Maxwell&rsquo;s equations, 890
pullback of, 888
</p>
<p>Differential geometry, 1117&ndash;1140
Differential one-form, 882
Differential operator, 418, 970
</p>
<p>adjoint, 433&ndash;436
linear, 605
</p>
<p>Diffusion equation, 643, 673
one-dimensional
</p>
<p>parabolic, 642
time-dependent, 581, 582
</p>
<p>Dilation, 306
Dilitation, 1159
Dimension theorem, 42, 44, 45, 61, 99,
</p>
<p>193, 518, 803, 810, 857, 876
Dirac, 957
</p>
<p>biography, 235
Dirac delta function, 229, 512, 624
</p>
<p>derivative of, 233
expansion
</p>
<p>Fourier, 273
general, 257
</p>
<p>Fourier transform, 279
Green&rsquo;s function, 644, 685
integral representation of, 281
Legendre polynomials, 256
limit of sequence, 229
spherical harmonics, 692
step function, 231
Sturm-Liouville eigenfunctions, 691
variational problem, 1054
</p>
<p>Dirac equation, 832&ndash;834, 997
Dirac gamma matrices, 834
</p>
<p>Majorana representation, 855, 1003
Direct product
</p>
<p>group, 712, 713
Direct sum, 25&ndash;28, 75, 92, 119, 169, 201,
</p>
<p>528, 558, 567, 712, 731, 797,
803, 810, 834&ndash;836, 840, 852,
947, 959, 999, 1001, 1087
</p>
<p>algebra, 67
definition, 25
inner product, 32
</p>
<p>Directional covariant derivative, 1119
Directional derivative, 884
Dirichlet, 246, 791, 1130, 1144
</p>
<p>biography, 666
Dirichlet boundary condition, 642
Dirichlet BVP, 642, 665&ndash;671
</p>
<p>in two dimensions, 690
Discrete Fourier transform, 286, 287
Dispersion relation, 376&ndash;378
</p>
<p>with one subtraction, 377
Distribution, 234, 418, 686, 688</p>
<p/>
</div>
<div class="page"><p/>
<p>Index 1187
</p>
<p>Distribution (cont.)
density, 234
derivative of, 236
Fourier transform, 287, 288
Fourier transform of a, 288
Green&rsquo;s function as, 606
limit of functions, 235
</p>
<p>Divergence
null Lagrangians, 1060, 1061
of tensors, 1135
total, 1060
</p>
<p>Divergence theorem, 648
Division algebra, 69
</p>
<p>of a Clifford algebra, 999
DOLDE
</p>
<p>hypergeometric
Kummer&rsquo;s solutions, 477
</p>
<p>Domain, 5
Dot product, 7, 29
Dual
</p>
<p>basis, 51, 782
of an operator, 51
space, 48
</p>
<p>Dual determinant function, 158&ndash;160
Dual space, 49
</p>
<p>E
Effective action, 713
Eigenfunction expansion technique
</p>
<p>2D Laplacian, 689
Eigenspace, 173, 180
</p>
<p>compact operator, 527
compact resolvent operator, 565
involution, 836
normal operator, 179
perturbation theory, 655
Weyl operator, 955
</p>
<p>Eigenvalue, 172&ndash;175
angular momentum, 401&ndash;405, 970
Casimir operator, 969
characteristic polynomial, 173
circuit matrix, 462
compact operators, 527
definition, 172
discrete, 630
extrema of functions, 197
Green&rsquo;s functions, 630, 688
harmonic oscillator, 444
hermitian operator, 178
integral equation, 544
invertible operator, 611
involution, 836
largest, 181
orthogonal operator, 201
perturbation theory, 656
positive operator, 181
projection operator, 174
simple, 173
smallest, 181
Sturm-Liouville, 691
</p>
<p>Sturm-Liouville system, 568, 578
unitary operator, 178
upper-triangular matrix, 175
Weyl operator, 959
</p>
<p>Eigenvector, 172&ndash;175, 178, 199
angular momentum, 402, 406&ndash;413
Casimir operator, 969
compact normal operator, 532
compact operators, 527
definition, 172
harmonic oscillator, 444
hermitian operator, 433
infinite dimensions, 518
integral equation, 554
normalized, 190
perturbation theory, 656
simultaneous, 185
SOLDE, 463
Sturm-Liouville system, 567
Weyl operator, 959
</p>
<p>Einstein, 897, 956, 1070, 1131, 1145,
1146, 1164, 1166
</p>
<p>Einstein tensor, 1163
Einstein&rsquo;s equation, 1163&ndash;1166
</p>
<p>Schwarzschild solution, 1169
spherically symmetric solutions,
</p>
<p>1167&ndash;1169
Einstein&rsquo;s summation convention, 781
Electromagnetic field tensor, 145, 826,
</p>
<p>889, 892, 893, 895
Elementary column operation, 156
Elementary row operation, 156
Elliptic PDE, 641, 665&ndash;673
Elsewhere, 941
Empty set, 2
Endomorphism, 39, 40, 42, 80, 81, 101,
</p>
<p>102, 125, 164, 705, 806, 807,
1126
</p>
<p>involution, 72
Energy function, 905
Energy levels, 11
Energy quantum number, 727
Entire function, 301, 343
</p>
<p>Bessel functions, 572
bounded, 317
confluent HGF, 479
inverse of gamma function, 379
with simple zeros, 364
</p>
<p>Epimorphism
algebra, 70
</p>
<p>Equivalence class, 3
representative, 3
</p>
<p>Equivalence relation, 3, 4, 24
Equivalent representations, 726
Error function, 436
</p>
<p>as solution of a DE, 437
Essential singularity, 342
Essentially idempotent, 741, 772
η-orthogonal matrices, 940
Euclid, 220, 907</p>
<p/>
</div>
<div class="page"><p/>
<p>1188 Index
</p>
<p>Euclidean metric, 1149
Euler, 301, 474, 482, 570, 1057, 1144
</p>
<p>biography, 1055
Euler angles, 146, 172, 934, 972
Euler equation, 457
Euler kernel, 494
Euler operator, 1054
Euler theorem, 973
Euler transform, 493
Euler-Lagrange equation, 1055, 1069
</p>
<p>classical, 1053
field, 1053
</p>
<p>Euler-Mascheroni constant, 380
Evaluation function, 1051
Event, 941
Evolution operator, 109, 678
Exact form, 894
Expectation value, 115
Exponential function
</p>
<p>complex, 302
Exponential map, 925
Exterior algebra, 794&ndash;801
Exterior calculus, 888&ndash;897
Exterior covariant derivative, 1093
Exterior derivative, 889
</p>
<p>covariant, 1093
Exterior product, 794
</p>
<p>inner product, 819, 820
</p>
<p>F
F -related vector fields, 877
Factor algebra, 77, 78, 92, 709
Factor group, 710
Factor map, 6
Factor set, 4, 24
Factor space, 24, 25, 77
Factorial function, 378
</p>
<p>Stirling approximation of, 386
Faithful representation, 126, 726
Fast Fourier transform, 287
Fermi energy, 584
Feynman diagram, 654
Feynman propagator, 688
Fiber, 1080
Fiber bundle, 1079&ndash;1097
</p>
<p>abelian case, 1095
principal, 1079&ndash;1086
</p>
<p>Fiber metric, 1143
Field, 20
</p>
<p>gauge, 1099&ndash;1105
magnetic, 3
particle, 1101
tensor
</p>
<p>manifold, 876&ndash;888
vector
</p>
<p>manifold, 877&ndash;882
Fine-structure constant, 481, 655
Finite-rank operator, 524
First integral, 1066
First variation, 1057
</p>
<p>Flat connection, 1095, 1096
Flat manifold, 1153, 1154
Flat map, 801, 902
Flow, 881
FODE
</p>
<p>existence, 419&ndash;421
existence and uniqueness
</p>
<p>local, 421
linear, 420
normal form, 420
Peano existence theorem, 420
uniqueness, 419&ndash;421
uniqueness theorem, 420
</p>
<p>FOLDE, 433
complex, 460&ndash;462
irregular singular point, 461
regular singular point, 461
removable singularity, 461
</p>
<p>Form
invariant
</p>
<p>Lie group, 927, 928
pseudotensorial, 1092
tensorial, 1092
torsion, 1122
</p>
<p>Form factor, 284
Formal adjoint, 612, 633, 648
Four-potential, 895
Four-vector, 808
</p>
<p>energy momentum, 1071
Fourier, 666, 703, 1056
</p>
<p>biography, 267
Fourier integral transforms, 278
Fourier series, 265&ndash;276, 563
</p>
<p>angular variable, 266
fundamental cell, 267
general variable, 268
group theory, 960
higher dimensions, 275, 276
main theorem, 272
Peter-Weyl, 960
sawtooth, 270
square wave, 269
to Fourier transform, 276&ndash;278
two-dimensional, 581
</p>
<p>Fourier transform, 276&ndash;288, 493
Coulomb potential
</p>
<p>charge distribution, 283
point charge, 282
</p>
<p>definition, 278
derivatives, 284, 285
discrete, 286, 287
distribution, 287, 288
Gaussian, 280
Green&rsquo;s functions, 680&ndash;688
higher dimensions, 281
quark model, 284
scattering experiments, 282
</p>
<p>Fourier-Bessel series, 587
Fredholm, 220
Fredholm, biography, 551</p>
<p/>
</div>
<div class="page"><p/>
<p>Index 1189
</p>
<p>Fredholm alternative, 551
Fredholm equation, 543, 652
</p>
<p>second kind
characteristic values, 544
</p>
<p>Fredholm integral equation, 549&ndash;559
Free action, 713
Friedmann, biography, 1166
Friedmann metric, 1149
Frobenius, 734, 957, 981
</p>
<p>biography, 764
Frobenius method, 439&ndash;444
Frobenius Theorem, 93
Fuchsian DE, 469&ndash;473
</p>
<p>definition, 470
Function, 5
</p>
<p>analytic, 297&ndash;304
complex, 295, 296
</p>
<p>derivatives as integrals, 315&ndash;319
integration, 309&ndash;315
</p>
<p>determinant, 54
generalized, 233&ndash;237
inner product, 32
meromorphic, 363&ndash;365
multivalued, 365&ndash;371
of operators, 104&ndash;106
operator, 188&ndash;191
p-linear, 53
piecewise continuous, 266
square-integrable, 221&ndash;227
</p>
<p>Function algebra, 67
Function of operator
</p>
<p>derivative, 108
Functional, 1054
</p>
<p>linear, 48&ndash;53
Functional derivative, 1050&ndash;1053
Fundamental theorem of algebra, 318
Fundamental vector field, 1086
Future light cone, 941
</p>
<p>G
G-invariance, 1009
G-invariant Lagrangian, 1106
g-orthogonal, 808
g-orthonormal, 813
g-transpose, 806
Galois, 154, 764, 946, 1015
</p>
<p>biography, 702
Gamma function, 250, 378&ndash;381
</p>
<p>definition, 378
Gamma matrices, 834
</p>
<p>Majorana representation, 855
Gauge
</p>
<p>choice of, 1099
Gauge field, 1099&ndash;1105
Gauge invariance, 895
Gauge Lagrangian, 1105
Gauge Lagrangian density, 1109
Gauge potential, 1099&ndash;1105
Gauge theories, 1099&ndash;1114
Gauge theory
</p>
<p>local equation, 1112&ndash;1114
Gauge transformation, 1102
Gauss, 154, 251, 301, 482, 523, 533, 666,
</p>
<p>791, 895, 1055, 1130, 1144
biography, 474
</p>
<p>Gay-Lussac, 581
Gegenbauer function, 478
Gegenbauer polynomials, 253
General linear group, 705
</p>
<p>representation, 963&ndash;966
General relativity, 1163&ndash;1174
Generalized Fourier coefficients, 220
Generalized function, 233&ndash;237, 418, 606,
</p>
<p>688
Generalized Green&rsquo;s identity, 613, 626,
</p>
<p>648
Generating function, 257
Generator
</p>
<p>Clifford algebra, 997, 1000
conformal group, 1036, 1159
coordinate transformation, 934
cyclic group, 710
group, 933, 1113
group action, 962
infinitesimal, 929, 932, 970, 976,
</p>
<p>1010&ndash;1013, 1023, 1030, 1037,
1040, 1041, 1062, 1064, 1068,
1069
</p>
<p>Lorentz, 1073
of an algebra, 70
rotation, 106, 750, 933, 998, 1157
translation, 112, 948
</p>
<p>Geodesic, 1137&ndash;1140
relative acceleration, 1160
</p>
<p>Geodesic deviation, 1159&ndash;1163
equation of, 1161
</p>
<p>Geodesic equation, 1138
massive particles, 1170, 1171
massless particles, 1170, 1172
</p>
<p>Geometric multiplicity, 173
Geometry
</p>
<p>Riemannian, 1143&ndash;1174
symplectic, 51, 901&ndash;909
</p>
<p>Gibbs, 523, 907
Gibbs phenomenon, 273&ndash;275
GL(n,R) as a Lie group, 916
GL(V)
</p>
<p>as a Lie group, 915
representation of, 963
</p>
<p>G&ouml;del, 897
Gordan, 1070
</p>
<p>biography, 755
Gradient
</p>
<p>for Hilbert spaces, 1050
Gradient operator, 1133
Gram, 34
Gram-Schmidt process, 33&ndash;35, 164, 210,
</p>
<p>241, 532
Graph, 5
Grassmann, 799, 1070</p>
<p/>
</div>
<div class="page"><p/>
<p>1190 Index
</p>
<p>Grassmann product, 794
Gravitational red-shift, 1173
Gravity
</p>
<p>and curvature, 1161
Newtonian, 1161&ndash;1163
</p>
<p>Green, biography, 613
Green&rsquo;s function, 358
</p>
<p>adjoint, 618
advanced, 686
as a distribution, 606
Dirichlet BC
</p>
<p>circle, 670
eigenfunction expansion, 630&ndash;632
for d/dx, 606
for d2/dx2, 607
formal considerations, 610&ndash;617
Helmholtz operator
</p>
<p>in 2D, 694
in one dimension, 605
indefinite, 606&ndash;610
multidimensional
</p>
<p>delta function, 643&ndash;648
diffusion operator, 684, 685
Dirichlet BVP, 665&ndash;671
eigenfunction expansion, 688&ndash;693
Fourier transform, 680&ndash;688
fundamental solution, 649&ndash;651
general properties, 648, 649
Helmholtz operator, 682&ndash;684
integral equations, 652&ndash;655
Laplacian, 647, 648, 681, 682
Neumann BVP, 671&ndash;673
perturbation theory, 655&ndash;661
wave equation, 685&ndash;688
</p>
<p>Neumann BVP, 673
exterior, 673
interior, 673
</p>
<p>physical interpretation, 629
properties, 619
regular part of, 651
resolvent, 630
retarded, 686
second order DO, 614&ndash;616
self-adjoint SOLDOs, 616, 617
singular part of, 651
SOLDO, 617&ndash;629
</p>
<p>construction, 621&ndash;626
inhomogeneous BCs, 626&ndash;629
properties, 619&ndash;621
uniqueness, 621&ndash;626
</p>
<p>symmetry, 619
Green&rsquo;s identity, 619, 648, 675, 679
</p>
<p>generalized, 648
Group, 8, 702&ndash;705
</p>
<p>1st isomorphism theorem, 710
abelian, 704
affine, 948
algebra
</p>
<p>symmetric group, 771
automorphism, 705
</p>
<p>center of, 708
commutative, 704
commutator of, 707
direct product, 712, 713
</p>
<p>external, 712
internal, 712
</p>
<p>external direct product, 712
finite
</p>
<p>Lagrange&rsquo;s theorem, 721
homomorphism, 705
</p>
<p>kernel of, 708
internal direct product, 712
isomorphism, 705
left action, 713
Lie, 915&ndash;936
multiplication, 702
multiplication table, 705
of affine motions, 917
order of, 703
orthogonal, 706
realization, 715
representation, 725&ndash;732
</p>
<p>character table, 743
criterion for irreducibility, 738
crystallography, 727
irreducible, projection operator,
</p>
<p>749
irreducible basis function, 746&ndash;750
matrix, 727
particles and fields, 751
quantum state parity, 727
tensor product, 750&ndash;758
</p>
<p>right action, 713
rigid rotations, 706
simply reducible, 753
special orthogonal, 706
special unitary, 706
subset
</p>
<p>left invariant, 713
right invariant, 713
word on, 720
</p>
<p>symmetric, 715&ndash;720
symmetry of Hamiltonian, 725
symplectic, 707, 803
unitary, 706
</p>
<p>Group action, 713&ndash;715
effective, 713, 918
free, 713, 918
infinitesimal, 928&ndash;935
infinitesimal generator, 929
Lie groups, 917&ndash;920
orbit, 713
stabilizer, 713
transitive, 713, 918
</p>
<p>Group algebra, 740
representations, 740&ndash;743
</p>
<p>Guided waves
TE, 585
TEM, 585
TM, 585</p>
<p/>
</div>
<div class="page"><p/>
<p>Index 1191
</p>
<p>H
Haar measure, 935
Halley, 481
Hamilton, 246, 545, 1070
</p>
<p>biography, 906
Hamiltonian
</p>
<p>group of symmetry of, 725
Hamiltonian mechanics, 801, 904
Hamiltonian system, 905
Hamiltonian vector field, 905
</p>
<p>energy function, 905
Hankel function, 484
</p>
<p>first kind
asymptotic expansion of, 386
</p>
<p>second kind, 391
Hankel transform, 494
Harmonic functions, 304
Harmonic oscillator, 443, 444&ndash;446
</p>
<p>critically damped, 447
ground state, 444
Hamiltonian, 444
overdamped, 447
underdamped, 447
</p>
<p>Heat equation, 395, 643, 673
symmetry group, 1030&ndash;1034
</p>
<p>Heat transfer
time-dependent, 581, 582
</p>
<p>Heat-conducting plate, 597
Hegel, 791
Heisenberg, 115, 236
Helicity, 982
Helmholtz, 246, 639, 957
Helmholtz equation, 593
Hermite, 251, 896
</p>
<p>biography, 115
Hermite polynomials, 245, 248, 249, 442,
</p>
<p>573
Hermitian, 31, 48, 116, 117, 120, 144,
</p>
<p>147, 172, 177, 178, 181, 186,
189, 205, 402, 525, 533, 555,
558, 564, 613, 924, 945, 955,
968, 982
</p>
<p>Hermitian conjugate, 113&ndash;116, 144, 146,
162, 171, 202, 404, 513, 661
</p>
<p>Hermitian inner product, 31
Hermitian kernel, 552&ndash;556
Hermitian operator, 114&ndash;119
Hilbert, 11, 34, 268, 523, 755, 897, 956,
</p>
<p>1070, 1164
biography, 220
</p>
<p>Hilbert space, 215&ndash;227, 435
basis of, 219
bounded operators in, 513
compact hermitian operator in, 530
compact normal operator in, 532
compact operator in, 524
compact resolvent, 564
convex subset, 528
countable basis, 228
definition, 218
</p>
<p>derivative, 1047&ndash;1050
differential of functions, 1049
directional derivative, 1050
functions on, 1052
</p>
<p>derivative of, 1049
invertible operator in, 611
operator norm, 513
perturbation theory, 658
representation theory, 726, 953
square-integrable functions, 222
</p>
<p>Hilbert transform, 377
Hilbert-Schmidt kernel, 525, 549
Hilbert-Schmidt operator, 525, 955
Hilbert-Schmidt theorem, 552
HNOLDE, 446, 448
</p>
<p>characteristic polynomial, 446
Hodge star operator, 820&ndash;823, 893
H&ouml;lder, 523
Homographic transformations, 307
Homomorphism
</p>
<p>algebra, 70, 71, 77, 82, 98, 125
Clifford algebra, 837
Clifford group, 991
group, 705, 710, 726, 731, 732, 987,
</p>
<p>992
Lie algebra, 922, 944, 953, 1101
Lie group, 915, 922, 928, 953, 967
PFB, 1081
symmetric, 705
trivial, 705
</p>
<p>Horizontal lift, 1089
Horizontal vector field, 1087
HSOLDE
</p>
<p>basis of solutions, 425
comparison theorem, 431
exact, 433
integrating factor, 433
second solution, 426&ndash;428
separation theorem, 430
</p>
<p>Hydrogen, 11
Hydrogen-like atoms, 480&ndash;482
Hyperbolic PDE, 641, 678&ndash;680
Hypergeometric DE, 466
Hypergeometric function, 473&ndash;478
</p>
<p>confluent, 478&ndash;485
integral representation of, 497, 498
</p>
<p>contiguous functions, 476
Euler formula, 496
integral representation of, 494&ndash;498
</p>
<p>Hypergeometric series, 473
Hypersurface, 635
</p>
<p>I
Ideal, 73&ndash;78
Idempotent, 83, 86&ndash;89, 119&ndash;125, 175,
</p>
<p>741, 844, 852, 999, 1002
essentially, 741, 772
primitive, 88, 94, 999, 1001, 1002
principal, 87&ndash;89
rank, 94</p>
<p/>
</div>
<div class="page"><p/>
<p>1192 Index
</p>
<p>Identity
additive, 20
multiplicative, 20
</p>
<p>Identity map, 5
Identity operator, 101
Identity representation, 726
Ignorable coordinate, 645
Image
</p>
<p>map, 5
Image of a subset, 5
Implicit function theorem, 419
Index
</p>
<p>continuous, 227&ndash;233
Indicial equation, 465
</p>
<p>SOLDE, 465
Indicial polynomial, 465
Induced representations, 978
Induction principle, 12
Inductive definition, 14
Inequality
</p>
<p>Bessel, 219
Cauchy, 336
Darboux, 313
Parseval, 219
Schwarz, 35
triangle, 36
</p>
<p>Infinitesimal action
adjoint, 929
</p>
<p>Infinitesimal generator, 929, 932
Initial conditions, 418
Initial value problem, 611, 635
Injective map, 5
Inner automorphism, 926
Inner product, 29&ndash;38, 804&ndash;820
</p>
<p>bra and ket notation, 31
complex bilibear, 46
definition of, 30
direct sum, 32
Euclidean, 31
exterior product, 819, 820
G-orthogonal, 1107
hermitian, 31
indefinite
</p>
<p>orthonormal basis, 812&ndash;819
subspaces, 809&ndash;812
</p>
<p>isotropic vector, 808
norm and, 37
null vector, 808
positive definite, 30
pseudo-Euclidean, 31
sesquilinear, 31
signature, 813
</p>
<p>Inner product space, 31
INOLDE
</p>
<p>particular solution, 448
Integral
</p>
<p>principal value, 354&ndash;358
Integral curve, 879
Integral equation, 543&ndash;548
</p>
<p>characteristic value, 544
</p>
<p>first kind, 543
Fredholm, 549&ndash;559
Green&rsquo;s functions, 652&ndash;655
kernel of, 543
second kind, 543
Volterra, 543
Volterra, of second kind
</p>
<p>solution, 545
Integral operator, 512
Integral transform, 493
</p>
<p>Bessel function, 494
Integration
</p>
<p>complex functions, 309&ndash;315
Lie group, 935, 936
manifolds, 897&ndash;901
</p>
<p>Integration operator, 40
Interior product, 829, 891
Intersection, 2
Intrinsic spin, 1073
Invariant, 1010
</p>
<p>map, 1010
operator
</p>
<p>matrix representation, 171
subspace, 169&ndash;172
</p>
<p>definition, 170
Invariant subspace, 728, 729
Inverse
</p>
<p>image, 5
of a map, 6
of a matrix, 155&ndash;158
</p>
<p>Inverse mapping theorem, 873
Inversion, 154, 306, 1159
Involution, 72, 82, 836, 837, 839, 843,
</p>
<p>848, 989, 990
Irreducible basis function, 746&ndash;750
Irreducible representation, 729
</p>
<p>i-th row
functions, 747
</p>
<p>norm of functions, 747
Irreducible set of operators, 757
Irreducible tensor operators, 756&ndash;758
Irreducible tensorial set, 757
Isolated singularity, 342&ndash;344
Isolated zero, 330
ISOLDE
</p>
<p>general solution, 428&ndash;430
Isometric map, 39, 1155
Isometry, 40, 42, 43, 125, 205, 539, 806,
</p>
<p>807, 811, 826, 992, 1143,
1155&ndash;1159, 1168
</p>
<p>time translation, 1167
Isomorphism, 43&ndash;45, 52, 68, 74, 78, 127,
</p>
<p>139, 140, 158, 222, 228, 661,
704, 719, 721, 726, 789, 796,
801, 838, 845, 847, 851, 871,
872, 884, 905, 921, 922, 926,
930, 945, 972, 998, 1085&ndash;1087,
1089, 1103, 1118, 1128, 1143
</p>
<p>algebra, 70
Clifford algebras, 842, 843</p>
<p/>
</div>
<div class="page"><p/>
<p>Index 1193
</p>
<p>Isomorphism (cont.)
</p>
<p>group, 705
Lie algebra, 922
Lie group, 915
linear, 43&ndash;45
natural, 785
PFB, 1081
</p>
<p>Isotropic vector, 808
</p>
<p>J
</p>
<p>Jacobi, 251, 475, 545, 666, 713, 753, 755,
791, 907, 1144
</p>
<p>biography, 246
Jacobi function
</p>
<p>first kind, 477
second kind, 478
</p>
<p>Jacobi identity, 879, 887, 927
Jacobi polynomials, 245, 250, 252, 478
</p>
<p>special cases, 245
Jacobian matrix, 873
Jordan arc, 309
Jordan canonical form, 539
Jordan&rsquo;s lemma, 345
</p>
<p>K
</p>
<p>Kant, 791
Kelvin, 613
Kelvin equation, 589
Kelvin function, 589
Kepler problem, 1074
Kernel, 41, 42, 51, 130, 158, 173, 192,
</p>
<p>198, 498, 529, 546, 558, 560,
635, 678, 708, 826, 937, 944,
995, 999
</p>
<p>degenerate, 556&ndash;559
hermitian, 552&ndash;556
Hilbert-Schmidt, 525, 544, 555
integral operator, 512
integral transforms, 493
separable, 556
</p>
<p>Ket, 20
Killing, 799, 1015
</p>
<p>biography, 946
Killing equation, 1156
Killing form, 945, 948
</p>
<p>of gl(n,R), 947
Killing parameter, 1167
Killing vector field, 1155&ndash;1159, 1167,
</p>
<p>1170, 1173
conformal, 1158
</p>
<p>Kirchhoff, 639
Klein, 799, 896, 956, 1015, 1070, 1131,
</p>
<p>1164
Klein-Gordon equation, 396
Korteweg-de Vries equation, 1044
Kovalevskaya, 523
</p>
<p>biography, 639
Kramers-Kronig relation, 378
Kronecker, 11, 154
</p>
<p>biography, 791
Kronecker delta, 32, 50, 161, 782, 939
Kronecker product, 751
Kummer, 36, 755, 791, 946, 1130
</p>
<p>L
Lagrange, 154, 246, 251, 267, 474, 482,
</p>
<p>581, 755
biography, 1057
</p>
<p>Lagrange identity, 435, 494, 570, 578,
613, 805
</p>
<p>Lagrange multiplier, 1064
Lagrange&rsquo;s equation, 1111
Lagrangian, 904, 1054
</p>
<p>G-invariant, 1106
gauge, 1109
gauge-invariant, 1105&ndash;1107
</p>
<p>construction, 1107&ndash;1111
null, 1060, 1061
</p>
<p>Lagrangian density, 1105
Laguerre polynomials, 245, 249, 250
Laplace, 34, 267, 666, 906, 1056, 1058,
</p>
<p>1164
biography, 581
</p>
<p>Laplace transform, 493
Laplace&rsquo;s equation, 395
</p>
<p>Cartesian coordinates, 579
cylindrical coordinates, 586
elliptic, 642
</p>
<p>Laplacian
Green&rsquo;s function for, 647
separated
</p>
<p>angle radial, 399
spherical coordinates
</p>
<p>separation of angular part, 398&ndash;401
Laurent, biography, 340
Laurent series, 321&ndash;330, 657
</p>
<p>construction, 322
uniqueness, 325
</p>
<p>Lavoisier, 153, 1058
Least square fit, 225&ndash;227
Lebesgue, 221
Left annihilator, 73
Left coset, 708
Left ideal, 73, 74, 84, 740, 773, 1000,
</p>
<p>1002
minimal, 74, 76, 79, 94, 128, 129, 772,
</p>
<p>999, 1001, 1003
Left translation
</p>
<p>as action, 929
Left-invariant 1-form, 921
Left-invariant vector field, 920
Legendre, 246, 267, 545, 666, 1144
</p>
<p>biography, 251
Legendre equation, 436, 441, 572
Legendre function, 478
Legendre polynomial, 225, 250&ndash;252, 256,
</p>
<p>408, 411, 428, 555
and Laplacian, 256
asymptotic formula, 576</p>
<p/>
</div>
<div class="page"><p/>
<p>1194 Index
</p>
<p>delta function, 256
Legendre transformation, 904
Leibniz, 154, 791
Leibniz formula, 81
Leibniz rule, 16
Length
</p>
<p>vector, 36&ndash;38
Levi-Civita, 1131
</p>
<p>biography, 1146
Levi-Civita connection, 1145
Levi-Civita tensor, 799, 976
Lie, 764, 799, 896, 946
</p>
<p>biography, 1014
Lie algebra, 915&ndash;936
</p>
<p>abelian, 937
adjoint map, 926
Cartan metric tensor, 945
Cartan theorem, 948
center, 937
commutative, 937
compact, 945
decomposition, 947
derivation, 944
ideal, 937
Killing form of, 945
of a Lie group, 920&ndash;927
of SL(V), 924
of unitary group, 924
of vector fields, 879
representation, 966&ndash;983
</p>
<p>definition, 953
semisimple, 948
simple, 948
structure constants, 937
theory, 936&ndash;948
</p>
<p>Lie bracket, 879
Lie derivative, 885
</p>
<p>covariant derivative, 1135
of a 1-form, 886
of p-forms, 890
of vectors, 886
</p>
<p>Lie group, 405, 915&ndash;936
canonical 1-form on, 928
compact
</p>
<p>characters, 960
matrix representation, 959
representation, 953&ndash;963
unitary representation, 954
Weyl operator, 955
</p>
<p>group action, 917&ndash;920
homomorphism, 915
infinitesimal action, 928&ndash;935
integration, 935, 936
</p>
<p>density function, 936
invariant forms, 927, 928
left translation, 920
local, 917
representation, 953
</p>
<p>Lie multiplication, 937
Lie subalgebra, 937
</p>
<p>Lie&rsquo;s first theorem, 932
Lie&rsquo;s second theorem, 927
Lie&rsquo;s third theorem, 927
Light cone, 941
Linear combination, 21
Linear connection, 1120&ndash;1140
</p>
<p>definition, 1121
Linear frame, 1083
Linear functional, 48&ndash;52, 53, 53, 61, 233,
</p>
<p>234, 287, 515, 617, 783, 787,
796, 809, 829, 883
</p>
<p>Linear independence, 21
Linear isomorphism, 43&ndash;45, 49
Linear map, 38&ndash;45, 51, 70, 78, 95, 116,
</p>
<p>563, 789, 801, 814, 837, 838,
840, 856, 1048, 1049, 1073
</p>
<p>invertible, 43
Linear operator, 39&ndash;41, 47, 55, 56, 66,
</p>
<p>113, 115, 116, 119, 139, 140,
151, 170, 171, 174, 422, 513,
515, 517, 522, 529, 531, 564,
785, 793, 799, 810, 944
</p>
<p>determinant, 55, 56
null space of a, 41
</p>
<p>Linear PDE, 636
Linear transformation, 53
</p>
<p>bounded, 514
definition, 39
pullback of a, 51
</p>
<p>Liouville, 568, 703
biography, 570
</p>
<p>Liouville substitution, 569, 573, 576, 577
Liouville&rsquo;s theorem, 908
Lipschitz condition, 420
Little algebra, 978
Little group, 714, 978&ndash;981
Local diffeomorphism, 865
Local group of transformations, 917
Local Lie group, 917
Local operator, 512
Local trivialization, 1080
Logarithmic function, 365
Lorentz, 897
Lorentz algebra, 972
Lorentz force law, 892
Lorentz group, 707, 940
Lorentz metric, 1149
Lorentz transformation, 940
</p>
<p>orthochronous, 941
proper orthochronous, 941
</p>
<p>Lowering indices, 805
Lowering operator, 403
</p>
<p>M
Maclaurin series, 321
Magnetic field, 3
Manifold, 859&ndash;866
</p>
<p>atlas, 860
chart, 860
coordinate functions, 860</p>
<p/>
</div>
<div class="page"><p/>
<p>Index 1195
</p>
<p>Manifold (cont.)
differentiable, 859&ndash;866
differential of a map, 872&ndash;876
flat, 1153
integration, 897&ndash;901
orientable, 898
product, 863
pseudo-Riemannian, 1144
Riemannian, 1144
semi-Riemannian, 1144
subset
</p>
<p>contractable to a point, 894
symplectic, 902
tangent vectors, 866&ndash;872
tensor fields, 876&ndash;888
vector fields, 877&ndash;882
with boundary, 899
</p>
<p>Map, 4&ndash;8
bijective, 6
codomain, 5
conformal, 304&ndash;309
differentiable, 864
differential
</p>
<p>Jacobian matrix of, 873
domain, 5
equality of, 5
functions and, 5
graph of a, 5
identity, 5
image of a subset, 5
injective, 5
inverse of a, 6
isometric, 39
linear, 38&ndash;45
</p>
<p>invertible, 43
manifold, 872&ndash;876
multilinear, 53&ndash;57, 782&ndash;789
</p>
<p>skew-symmetric, 53
one-to-one, 5
onto, 6
p-linear, 53
range of a, 5
surjective, 5
target space, 5
</p>
<p>Maschke&rsquo;s Theorem, 759
Mathematical induction, 12&ndash;14
Matrix, 137&ndash;142
</p>
<p>antisymmetric, 144
basis transformation, 149
block diagonal, 171, 200
circuit, 462, 463
complex conjugate of, 144
determinant of, 151&ndash;160
diagonal, 144
diagonalizable, 162
hermitian, 144
hermitian conjugate of, 144
inverse of, 155&ndash;158
irreducible, 171
</p>
<p>operations on a, 142&ndash;146
orthogonal, 144
rank of, 158
reducible, 171
representation
</p>
<p>orthonormal basis, 146&ndash;148
row-echelon, 156
strictly upper triangular, 66
symmetric, 144
symplectic, 804
transpose of, 142
triangular, 156
unitary, 144
upper triangular, 66
upper-triangular, 175, 176
</p>
<p>Matrix algebra, 66, 78&ndash;80
Matrix of the classical adjoint, 152&ndash;155
Maurer-Cartan equation, 928, 1095
Maximally symmetric spaces, 1157
Maxwell&rsquo;s equations, 894
Mellin transform, 493
Mendelssohn, 666, 792
Meromorphic functions, 363&ndash;365
Method of images, 668
</p>
<p>sphere, 669
Method of steepest descent, 383, 577
Metric, 37
</p>
<p>Friedmann, 1149
Schwarzschild, 1149
</p>
<p>Metric connection, 1143&ndash;1155
Metric space, 8&ndash;10
</p>
<p>complete, 10
convergence, 9
definition, 8
</p>
<p>Minimal ideal, 963
Minimal left ideal, 74, 76, 79, 94, 128,
</p>
<p>129, 772, 999, 1001, 1003
Minkowski, 1164
Minkowski metric, 1149
Mittag-Leffler, 523, 640
Mittag-Leffler expansion, 364
Modified Bessel function, 484
</p>
<p>first kind
asymptotic expansion of, 391
</p>
<p>second kind
asymptotic expansion of, 392
</p>
<p>Moment of inertia, 145, 195
matrix, 145
</p>
<p>Momentum operator, 398
Monge, 153, 267
Monomorphism
</p>
<p>algebra, 70
Morera&rsquo;s theorem, 319
Multidimensional diffusion operator
</p>
<p>Green&rsquo;s function, 684, 685
Multidimensional Helmholtz operator
</p>
<p>Green&rsquo;s function, 682&ndash;684
Multidimensional Laplacian
</p>
<p>Green&rsquo;s function, 681, 682
Multidimensional wave equation</p>
<p/>
</div>
<div class="page"><p/>
<p>1196 Index
</p>
<p>Green&rsquo;s function, 685&ndash;688
Multilinear, 152, 783, 789, 883, 1092,
</p>
<p>1124
Multilinear map, 53&ndash;57, 782&ndash;789
</p>
<p>tensor-valued, 787
Multiplicative identity, 20
Multivalued functions, 365&ndash;371
</p>
<p>N
n-equivalent functions, 1018
n-sphere, 860, 865
n-th jet space, 1018
n-tuple, 3
</p>
<p>complex, 21
real, 21
</p>
<p>Napoleon, 267, 581
Natural isomorphism, 785, 820
Natural numbers, 2, 9
Natural pairing, 783
Neighborhood
</p>
<p>open round, 519
Neumann, 246, 753
</p>
<p>biography, 671
Neumann BC, 643
Neumann BVP, 643, 671&ndash;673
Neumann function, 483
Neumann series, 548, 653, 654
Newton, 397, 474, 581, 896, 906, 1056
Newtonian gravity, 1161&ndash;1163
Nilpotent, 83&ndash;85, 88, 91, 539
Noether, 755
</p>
<p>biography, 1069
Noether&rsquo;s theorem, 1065&ndash;1069
</p>
<p>classical field theory, 1069&ndash;1073
NOLDE
</p>
<p>circuit matrix, 462
constant coefficients, 446&ndash;449
existence and uniqueness, 611
integrating factor, 632
simple branch point, 463
</p>
<p>Non-local potential, 683
Nondegenerate subspace, 810
Norm, 215, 217, 291, 513&ndash;515, 529, 544,
</p>
<p>812
of a vector, 36
operator, 514
product of operators, 516
</p>
<p>Normal coordinates, 1138&ndash;1140
Normal operator, 177
Normal subgroup, 709
Normal vectors, 32
Normed determinant function, 815
Normed linear space, 36
Null divergence, 1066
Null Lagrangian, 1060, 1061
Null space, 41, 551, 554
Null vector, 808, 941
Nullity, 41
Number
</p>
<p>complex, 2
</p>
<p>integer, 2
natural, 2, 9
rational, 4, 9, 10
real, 2
</p>
<p>O
ODE, 417&ndash;419
</p>
<p>first order
symmetry group, 1037&ndash;1039
</p>
<p>higer order
symmetry group, 1039, 1040
</p>
<p>Ohm, 666
Olbers, 482
One-form, 882
One-parameter group, 881
One-to-one correspondence, 6
Open ball, 519
Open subset, 520
Operation
</p>
<p>binary, 7
Operations on matrices, 142&ndash;146
Operator, 39
</p>
<p>adjoint, 113
existence of, 517
</p>
<p>adjoint of, 46
angular momentum, 398
</p>
<p>eigenvalues, 401&ndash;405
annihilation, 444
anti-hermitian, 115
bounded, 513&ndash;517
Casimir, 969&ndash;971
closed, 564
</p>
<p>bounded, 564
compact, 523&ndash;526
</p>
<p>spectral theorem, 527&ndash;534
compact Hermitian
</p>
<p>spectral theorem, 530
compact normal
</p>
<p>spectral theorem, 532
compact resolvent, 564
conjugation, 113, 114
creation, 444
derivative, 40, 107&ndash;112
determinant, 55, 56
diagonalizable, 174
differential, 511, 512
domain of, 563
evolution, 109
expectation value of, 115
extension of, 564
finite rank, 524
formally self-adjoint, 649
functions of, 104&ndash;106, 188&ndash;191
hermitian, 114&ndash;119, 564
</p>
<p>eigenvalue, 178
hermitian conjugate of, 113
Hilbert-Schmidt, 525, 551, 567
Hodge star, 820&ndash;823
idempotent, 119&ndash;125
integral, 511, 512</p>
<p/>
</div>
<div class="page"><p/>
<p>Index 1197
</p>
<p>Operator (cont.)
</p>
<p>integration, 40
inverse, 101
involution, 72
kernel of an, 41
local, 512
negative powers of, 103
norm of, 514
normal, 177
</p>
<p>diagonalizable, 181
eigenspace of, 179
</p>
<p>null space of an, 41
polar decomposition, 205&ndash;208
polarization identity, 41
polynomials, 102&ndash;104
positive, 117
positive definite, 117
projection, 120&ndash;125
</p>
<p>orthogonal, 121
pullback of an, 51
raising and lowering, 403
regular point, 517
representation of, 138
resolvent of, 534
right-shift, 513
scalar, 757
self-adjoint, 46, 115, 564
skew, 46
spectrum, 517, 518
spectrum of, 173
square root, 189
square root of, 189
strictly positive, 117
Sturm-Liouville, 564, 566
symmetric, 193
tensor
</p>
<p>irreducible, 756&ndash;758
trace of, 161
unbounded
</p>
<p>compact resolvent, 563&ndash;569
unitary, 114&ndash;119, 189
</p>
<p>eigenvalue, 178
Operator algebra, 101&ndash;107
Lie algebra o(p,n&minus; p), 940&ndash;943
Opposite algebra, 66
Optical theorem, 378
Orbit, 728, 918
Orbital angular momentum, 1073
Ordered pairs, 2
Orientable manifolds, 898
Orientation, 800, 801, 898
</p>
<p>positive, 801
Oriented basis, 800
Orthogonal, 40
Orthogonal basis
</p>
<p>Riemannian geometry, 1148&ndash;1155
Orthogonal complement, 169, 528&ndash;530,
</p>
<p>551, 729, 747, 802, 812, 841
Orthogonal group, 706, 925
</p>
<p>Lie algebra of, 925
Orthogonal polynomial, 222&ndash;225, 579
</p>
<p>classical, 241, 241&ndash;243
classification, 245
differential equation, 243
generating functions, 257
recurrence relations, 245
</p>
<p>expansion in terms of, 254&ndash;257
least square fit, 225
</p>
<p>Orthogonal transformation, 154
Orthogonal vectors, 32
Orthogonality, 32, 33
</p>
<p>group representation, 732&ndash;737
Orthonormal basis, 32
</p>
<p>indefinite inner product, 812&ndash;819
matrix representation, 146&ndash;148
</p>
<p>P
p-form, 796
</p>
<p>vector-valued, 800
Pairing
</p>
<p>natural, 783
Parabolic PDE, 641, 673&ndash;678
Parallel displacement, 1090
Parallel section, 1091, 1119
Parallelism, 1089&ndash;1091
Parallelogram law, 37
Parameter
</p>
<p>affine, 1138
Parity, 718
</p>
<p>Hermite polynomials, 262
Legendre polynomials, 262
</p>
<p>Parseval equality, 220
Parseval inequality, 219, 958
Parseval&rsquo;s relation, 291
Particle field, 1101
Particle in a box, 582&ndash;584
Particle in a cylindrical can, 601
Particle in a hard sphere, 593
Partition, 4, 720
Past light cone, 941
Pauli spin matrices, 146, 938, 944
</p>
<p>Clifford algebra representations,
997&ndash;1001
</p>
<p>PDE, 635&ndash;643
Cauchy data, 636
Cauchy problem, 636
characteristic hypersurface, 636&ndash;640
characteristic system of, 1012
elliptic, 665&ndash;673
</p>
<p>mixed BCs, 673
homogeneous, 397
hyperbolic, 678&ndash;680
inhomogeneous, 397
order of, 636
parabolic, 673&ndash;678
principal part, 636
second order, 640&ndash;643
second-order
</p>
<p>elliptic, 641</p>
<p/>
</div>
<div class="page"><p/>
<p>1198 Index
</p>
<p>PDE (cont.)
hyperbolic, 641
parabolic, 641
ultrahyperbolic, 641
</p>
<p>PDEs of mathematical physics, 395&ndash;398
Peano, 897
Peirce decomposition, 87, 89, 90, 100
Periodic BC, 571
Permutation, 53
</p>
<p>cyclic, 717
even, 719
odd, 719
parity of, 718
</p>
<p>Permutation group, 715
Permutation tensor, 816
Perturbation theory, 655, 748
</p>
<p>degenerate, 660, 661
first-order, 660
nondegenerate, 659, 660
second-order, 660
</p>
<p>Peter-Weyl theorem, 960
Fourier series, 960
</p>
<p>PFB
local section, 1083
</p>
<p>Phase space, 801
Photon capture
</p>
<p>cross section, 1173
Piecewise continuous, 266
Pin(μ, ν), 995
Planck, 523, 1164
Poincar&eacute;, 115, 533, 552, 672, 799, 1164
</p>
<p>biography, 895
Poincar&eacute; algebra, 943, 948
</p>
<p>representation, 975&ndash;983
Poincar&eacute; group, 707, 917, 943, 979
Poincar&eacute; lemma, 894
</p>
<p>converse of, 895
Poisson, 246, 568, 581, 666, 703
Poisson bracket, 908
Poisson integral formula, 671
Poisson&rsquo;s equation, 395, 648, 1162
Polar decomposition, 205&ndash;208
Polarization identity, 41, 812
Pole, 342
Polynomial, 20
</p>
<p>inner product, 32
operators, 102&ndash;104
orthogonal, 222&ndash;225
</p>
<p>Polynomial algebra, 95&ndash;97
Positive definite operator, 117
Positive operator, 117
Positive orientation, 801
Potential
</p>
<p>gauge, 1099&ndash;1105
non-local, 683
separable, 683
</p>
<p>Power series, 319
differentiation of, 320
integration of, 320
</p>
<p>SOLDE solutions, 436&ndash;446
uniform convergence, 320
</p>
<p>Lie algebra p(p,n&minus; p), 940&ndash;943
Preimage, 5
Primitive idempotent, 88, 94, 999, 1001,
</p>
<p>1002
Principal fiber bubdle
</p>
<p>curvature form, 1091
Principal fiber bundle, 1079&ndash;1086
</p>
<p>associated bundle, 1084&ndash;1086
base space, 1080
connection, 1086&ndash;1091
</p>
<p>matrix structure group, 1096, 1097
curvature
</p>
<p>matrix structure group, 1096, 1097
curvature form, 1097
curve
</p>
<p>horizontal lift, 1089
fundamental vector field, 1086
global section, 1083
lift of curve, 1089
parallelism, 1089&ndash;1091
reducible, 1082
structure group, 1080
</p>
<p>matrix, 1096, 1097
trivial, 1080
vector field
</p>
<p>horizontal lift, 1089
Principal idempotent, 87&ndash;89
Principal part
</p>
<p>PDE, 636
Principal value, 354&ndash;358, 685
Product
</p>
<p>Cartesian, 2, 7
dot, 7
inner, 29&ndash;38
tensor, 28, 29
</p>
<p>Product manifold, 863
Projectable symmetry, 1017
Projection, 6
Projection operator, 120&ndash;125, 169, 174,
</p>
<p>180, 527, 529, 532, 536, 552,
655&ndash;657, 688, 748, 809
</p>
<p>completeness relation, 123
orthogonal, 121
</p>
<p>Projective group
density function, 936
one-dimensional, 920
</p>
<p>Projective space, 4
Prolongation, 1017&ndash;1024
</p>
<p>functions, 1017&ndash;1021
groups, 1021, 1022
of a function, 1019
vector fields, 1022&ndash;1024
</p>
<p>Propagator, 654, 678
Feynman, 688
</p>
<p>Proper subset, 2
Pr&uuml;fer substitution, 574
Pseudo-Riemannian manifold, 1144
Pseudotensorial form, 1092</p>
<p/>
</div>
<div class="page"><p/>
<p>Index 1199
</p>
<p>Puiseux, biography, 365
Pullback, 789, 883, 888, 898, 1094, 1112
</p>
<p>linear transformation, 51
of p-forms, 796
</p>
<p>Q
Quadratic form, 843
Quantization
</p>
<p>harmonic oscillator
algebraic, 445
analytic, 443
</p>
<p>hydrogen atom, 481
Quantum electrodynamics, 654
Quantum harmonic oscillator, 444&ndash;446
Quantum mechanics
</p>
<p>angular momentum, 405
Quantum particle in a box, 582&ndash;584
Quantum state
</p>
<p>even, odd, 727
Quark, 753, 754, 980
Quaternion, 69, 98, 831, 846, 847, 856,
</p>
<p>907, 989, 990, 993, 996, 1070
absolute value, 70
conjugate, 69
pure part, 69
real part, 69
</p>
<p>Quotient group, 710
Quotient map, 6
Quotient set, 4, 24
Quotient space, 24, 25
</p>
<p>R
r-cycle, 716
Radical, 84&ndash;88
Radon-Hurwitz number, 1002
Raising indices, 805
Raising operator, 403
Range of a map, 5
Rank of a matrix, 158
Rational function, 343
</p>
<p>integration of, 345&ndash;348
Rational numbers, 4, 9, 10
</p>
<p>dense subset of reals, 520
Rational trig function
</p>
<p>integration of, 348&ndash;350
Real coordinate space, 21
Real normal operator
</p>
<p>spectral decomposition, 198&ndash;205
Real vector space, 20
Realization, 715
Reciprocal lattice vectors, 276
Recurrence relations, 222
Redshift, 1173
Reduced matrix elements, 758
Reducible bundle, 1082
Reducible representation, 729
Reflection, 808
Reflection operator, 121
Reflection principle, 374&ndash;376
Reflexivity, 3
</p>
<p>Regular point, 301, 460
operator, 517, 551
</p>
<p>Regular representation, 128, 739
Regular singular point
</p>
<p>SOLDE, 464
Relation, 3, 24
</p>
<p>equivalence, 3, 4
Relative acceleration, 1160
Relativistic electromagnetism, 889
Relativity
</p>
<p>general, 1163&ndash;1174
Removable singularity
</p>
<p>FOLDE, 461
Representation
</p>
<p>abelian group, 733
action on Hilbert space, 726
adjoint, 732, 755, 1092, 1102
algebra, 125&ndash;131
angular momentum, 402
carrier space, 726
character of, 736
classical adjoint, 152
Clifford algebras, 987&ndash;1006
compact Lie group, 945, 953&ndash;963
complex conjugate, 732
dimension of, 726
direct sum, 128, 731
equivalent, 127, 726
faithful, 126, 726
general linear group, 715, 963&ndash;966
</p>
<p>Representation of
gl(n,R), 968
</p>
<p>Representation
group, 725&ndash;732
</p>
<p>adjoint, 755
analysis, 737&ndash;739
antisymmetric, 745, 771
identity, 754, 758, 771
irreducible, 734, 737
irreducible basis function, 746&ndash;750
irreducible in regular, 739
orthogonality, 732&ndash;737
tensor product, 750&ndash;758
trivial, 769
</p>
<p>group algebra, 740&ndash;743
hermitian operator, 182
identity, 726, 1092
irreducible, 127, 729
</p>
<p>compact Lie group, 957
finite group, 730
general linear group, 964
Lie group, 1072
semi-simple algebra, 130
</p>
<p>Kronecker product, 751
Lie algebra, 948, 966&ndash;983
</p>
<p>Casimir operator, 969
Lie group, 937, 953
</p>
<p>unitary, 953
matrix
</p>
<p>orthonormal basis, 146&ndash;148</p>
<p/>
</div>
<div class="page"><p/>
<p>1200 Index
</p>
<p>Representation (cont.)
</p>
<p>operator, 161, 169, 199, 923
operators, 138
orthogonal operator, 201
quantum mechanics, 734, 748
quaternions, 126
reducible, 729
regular, 128, 739
semi-simple algebra, 130
simple algebra, 129
</p>
<p>Representation of
sl(n,C), 968
</p>
<p>Representation
so(3), 972
so(3,1), 974
structure group, 1092, 1101, 1117,
</p>
<p>1143, 1144
subgroup, 743
subgroups of GL(V), 967&ndash;969
</p>
<p>Representation of
su(n), 969
</p>
<p>Representation
symmetric group, 761&ndash;776
</p>
<p>analytic construction, 761&ndash;763
graphical construction, 764&ndash;767
products, 774&ndash;776
Young tableaux, 766
</p>
<p>tensor product, 128, 751
antisymmetrized, 752
character, 751
symmetrized, 752
</p>
<p>trivial, 732, 1092
twisted adjoint, 987
</p>
<p>Representation of
u(n), 968
</p>
<p>Representation
unitary, 730
</p>
<p>compact Lie group, 954
upper-triangular, 175
vectors, 137
</p>
<p>Residue, 339&ndash;341
definite integrals, 344&ndash;358
definition, 340
integration
</p>
<p>rational function, 345&ndash;348
rational trig function, 348&ndash;350
trig function, 350&ndash;352
</p>
<p>Residue theorem, 340
Resolution of identity, 536, 740, 774
Resolvent, 534&ndash;539
</p>
<p>compact, 564
unbounded operator, 563&ndash;569
</p>
<p>Green&rsquo;s functions, 630
Laurent expansion, 535
perturbation theory, 655
</p>
<p>Resolvent set, 517
openness of, 521
</p>
<p>Resonant cavity, 585, 597
Riccati equation, 455, 1040
</p>
<p>Ricci, 1131, 1146
Ricci tensor, 1162, 1163, 1165
Riemann, 36, 268, 366, 755, 896, 956,
</p>
<p>1055, 1130
biography, 1144
</p>
<p>Riemann identity, 472
Riemann normal coordinates, 1138&ndash;1140
Riemann sheet, 365, 367
Riemann surface, 366&ndash;371
Riemann-Christoffel symbols, 1130
Riemannian geometry, 1143&ndash;1174
</p>
<p>gravity
Newtonian, 1161&ndash;1163
</p>
<p>isometry, 1155&ndash;1159
Killing vector field, 1155&ndash;1159
Newtonian gravity, 1161&ndash;1163
orthogonal bases, 1148&ndash;1155
</p>
<p>Riemannian manifold, 1144
Riesz-Fischer theorem, 222
Right annihilator, 73
Right coset, 708
Right ideal, 73
Right translation, 921
Right-invariant 1-form, 921
Right-invariant vector field, 921
Right-shift operator, 513
</p>
<p>eigenvalues of, 518
Rigid rotations, 706
Rodriguez formula, 243, 245, 446
Rosetta stone, 267
Rotation algebra, 972
Rotation group, 727, 970
</p>
<p>character, 973
Rotation matrix, 972
</p>
<p>Wigner formula, 972
Russell, 11, 897
</p>
<p>S
Saddle point approximation, 382
Sawtooth voltage, 270
Scalar, 20
Scalar operator, 757, 758
Scalar product, 29
Scale transformations, 920
Scattering theory, 595
Schelling, 791
Schmidt, biography, 34
Schopenhauer, 791
Schr&ouml;dinger, 115, 907
Schr&ouml;dinger equation, 109, 396, 442, 469,
</p>
<p>480, 582, 593, 683, 727
classical limit, 452, 453
one dimensional, 451
</p>
<p>Schur, 764, 957, 981
biography, 734
</p>
<p>Schur&rsquo;s lemma, 732, 733, 758, 953, 969
Schwarz, 523, 792
</p>
<p>biography, 36
Schwarz inequality, 35, 59, 211, 218, 222,
</p>
<p>515, 540, 950, 956</p>
<p/>
</div>
<div class="page"><p/>
<p>Index 1201
</p>
<p>Schwarz reflection principle, 374&ndash;376
Schwarzschild, biography, 1164
Schwarzschild geodesic, 1169&ndash;1174
Schwarzschild metric, 1149
Schwarzschild radius, 1169
Second order PDE, 640&ndash;643
Second-order PDE
</p>
<p>classification, 641
Section
</p>
<p>global, 1083
local, 1083
parallel, 1091, 1119
</p>
<p>Selection rules, 753
Self-adjoint, 115, 193, 194, 198, 201, 206,
</p>
<p>433, 435, 533, 566, 569, 613,
616, 619, 628, 633, 649, 663,
665, 673, 679, 692, 694, 956
</p>
<p>formally, 613
Semi-Riemannian manifold, 1144
Semi-simple algebra, 88&ndash;91, 92, 92, 94,
</p>
<p>130, 764, 799, 844
Semi-simple Lie algebra, 948
Separable kernel, 556
Separable potential, 683
Separated boundary conditions, 566
Separation of variables, 396
</p>
<p>Cartesian, 579&ndash;585
conducting box, 579&ndash;581
conducting plate, 581, 582
quantum particle in a box, 582&ndash;584
wave guides, 584, 585
</p>
<p>cylindrical, 586&ndash;590
conducting cylindrical can,
</p>
<p>586&ndash;588
current distribution, 589, 590
cylindrical wave guide, 588, 589
</p>
<p>spherical, 590&ndash;595
Helmholtz equation, 593
particle in a sphere, 593, 594
plane wave expansion, 594, 595
radial part, 591, 592
</p>
<p>Separation theorem, 430&ndash;432
Sequence, 9
</p>
<p>Cauchy, 9
complete orthonormal, 219
</p>
<p>Series
Clebsch-Gordan, 754
complex, 319&ndash;321
Fourier, 265&ndash;276
Fourier-Bessel, 587
Laurent, 321&ndash;330
Neumann, 653, 654
SOLDE solutions, 436&ndash;446
Taylor, 321&ndash;330
vector, 215&ndash;220
</p>
<p>Sesquilinear inner product, 31
Set, 1&ndash;4
</p>
<p>Cantor, 12
compact, 519&ndash;523
complement of, 2
</p>
<p>countably infinite, 11
element of, 1
empty, 2
intersection, 2
matrices, 7
natural numbers, 2
partition of a, 4
uncountable, 12
union, 2
universal, 2
</p>
<p>Sharp map, 801, 902
Signature of g, 813
Similarity transformation, 148&ndash;151
</p>
<p>orthonormal basis, 149
Simple algebra, 76, 88, 90&ndash;92, 94, 126,
</p>
<p>129, 852, 948, 999
classification, 92&ndash;95
</p>
<p>Simple arc, 309
Simple character, 737
Simple Lie algebra, 948
Simple pole, 342
Simple zero, 330
Simultaneous diagonalizability, 185
Simultaneous diagonalization, 185&ndash;188
Singleton, 2
Singular point, 301, 339, 354, 355
</p>
<p>differential equation, 422
irregular, 461
isolated, 463
regular, 461, 470
removable, 342
Sturm-Liouville equation, 572
transformation, 644
</p>
<p>Singularity, 301, 302, 324, 439, 637
confluent HGDE, 479
essential, 342
Green&rsquo;s function, 651
isolated, 339, 342&ndash;344
</p>
<p>classification, 342
rational function, 343
removable, 343, 355
Schwarzschild solution, 1169
</p>
<p>Skew-symmetry, 53, 793
Skin depth, 589
SL(V) as a Lie group, 916
SL(V)
</p>
<p>Lie algebra of, 924
normal subgroup of GL(V), 711
</p>
<p>Smooth arc, 309
SOLDE, 421&ndash;425
</p>
<p>adjoint, 434
branch point, 464
canonical basis, 463
characteristic exponents, 465
complex, 463&ndash;469
confluent hypergeometric, 479
constant coefficients, 446&ndash;449
existence theorem, 440
Frobenius method, 439&ndash;444
homogeneous, 422</p>
<p/>
</div>
<div class="page"><p/>
<p>1202 Index
</p>
<p>SOLDE (cont.)
hypergeometric
</p>
<p>Jacobi functions, 477
hypergeometric function, 473
indicial equation, 465
integral equation of, 545
Lagrange identity, 435
normal form, 422
power-series solutions, 436&ndash;446
regular singular point, 464
singular point, 422
Sturm-Liouville systems, 569&ndash;573
uniqueness theorem, 424
variation of constants, 429
WKB method, 450&ndash;453
Wronskian, 425
</p>
<p>SOLDO, 614
Solid angle
</p>
<p>m-dimensional, 646
Solid-state physics, 275
Space
</p>
<p>Banach, 218
complex coordinate, 21
dual, 48
factor, 24, 25, 77
inner product, 31
metric, 8&ndash;10
</p>
<p>complete, 10
projective, 4
quotient, 24, 25
real coordinate, 21
square-integrable functions, 221
target, 5
vector, 19&ndash;29
</p>
<p>Spacelike vector, 941
Spacetime
</p>
<p>spherically symmetric, 1168
static, 1167
stationary, 1167
</p>
<p>Spacetime translation, 1070
Span, 22
Special linear group, 706
Special orthogonal group, 706, 925
</p>
<p>Lie algebra of, 925
Special relativity, 808, 940, 975, 979,
</p>
<p>1059
Special unitary group, 706
</p>
<p>Lie algebra of, 924
Spectral decomposition
</p>
<p>complex, 177&ndash;188
orthogonal operator, 201
real, 191&ndash;205
real normal operator, 198&ndash;205
symmetric operator, 193&ndash;198
</p>
<p>Spectral decomposition theorem, 688
Spectral theorem
</p>
<p>compact hermitian, 530
compact normal, 532
compact operators, 527&ndash;534
</p>
<p>Spectrum
bounded operator, 522
closure of, 521
compact operator, 527
Hilbert space operator, 517
integral operator, 545
linear operator, 517, 518
permutation operator, 208
</p>
<p>Spherical Bessel functions, 487, 593
expansion of plane wave, 594
</p>
<p>Spherical coordinates
multidimensional, 645, 646
</p>
<p>Spherical harmonics, 406&ndash;413, 970
addition theorem, 412, 413, 974
definition, 408
expansion in terms of, 411, 412
expansion of plane wave, 595, 698
first few, 410
</p>
<p>Spin representation, 1003
faithful, 1003
</p>
<p>Spin(μ, ν), 996
Spinor, 995&ndash;1006
</p>
<p>algebra Cνμ(R), 1001&ndash;1003
Spinor bundles, 1101
Spinor space, 1003
Spinoza, 791
Split complex numbers, 847
Square wave voltage, 269
Square-integrable functions, 221&ndash;227
Stabilizer, 918
Standard basis, 23
Standard horizontal vector field, 1121
Standard model, 1079
Static spacetime, 1167
Stationary spacetime, 1167
Steepest descent method, 382&ndash;388
Step function, 231, 357, 684
Stereographic projection
</p>
<p>n-sphere, 865
two-sphere, 862
</p>
<p>Stirling approximation, 385
Stokes&rsquo; Theorem, 899
Stone-Weierstrass theorem, 222
</p>
<p>generalized, 265
Stress energy tensor, 1165
Strictly positive operator, 117
Strictly upper triangular matrices, 66
Structure
</p>
<p>complex, 45&ndash;48
Structure constant, 78, 937, 939, 976, 984,
</p>
<p>1093, 1095, 1113
Lie algebra, 927
</p>
<p>Structure equation, 1093
Structure group
</p>
<p>matrix, 1096, 1097
Sturm, biography, 568
Sturm-Liouville
</p>
<p>operator, 566
problem, 243, 674
system, 411, 567, 569&ndash;573, 689</p>
<p/>
</div>
<div class="page"><p/>
<p>Index 1203
</p>
<p>Sturm-Liouville (cont.)
asymptotic behavior, 573&ndash;577
completeness, 577
eigensolutions, 567
eigenvalues, 568
expansion in eigenfunctions,
</p>
<p>577&ndash;579
large argument, 577
large eigenvalues, 573&ndash;576
regular, 567
singular, 572
</p>
<p>Subalgebra, 64, 73&ndash;78
Subgroup, 705&ndash;713
</p>
<p>conjugate, 707
generated by a subset, 707
normal, 709
trivial, 706
</p>
<p>Submanifold, 863
open, 863
</p>
<p>Subset, 2
bounded, 520
closed, 520
convex, 528
dense, 520
open, 520
proper, 2
</p>
<p>Subspace, 22&ndash;24
invariant, 44, 127, 169&ndash;172, 175, 177,
</p>
<p>192, 193, 198, 402, 530, 728,
731, 733, 734, 738, 740, 749,
758, 840, 955, 959, 967, 969,
977, 989
</p>
<p>nondegenerate, 810
stable, 99, 127, 989
</p>
<p>Sum
direct, 25&ndash;28
</p>
<p>Superposition principle
linear DEs, 422
</p>
<p>Surjective map, 5
Symmetric algebra, 791
Symmetric bilinear form, 804
</p>
<p>classification, 807
definite, 807
indefinite, 807
index of, 807
inner product, 805
negative definite, 807
negative semidefinite, 807
nondegenerate, 805
positive definite, 807
positive semidefinite, 807
semidefinite, 807
</p>
<p>Symmetric group, 704, 715&ndash;720
characters
</p>
<p>graphical construction, 767&ndash;771
cycle, 716
identical particles, 774
irreducible representation of, 772
permutation
</p>
<p>parity of, 718
representation, 761&ndash;776
</p>
<p>analytic construction, 761&ndash;763
antisymmetric, 732
graphical construction, 764&ndash;767
products, 774&ndash;776
Young operators, 771&ndash;774
</p>
<p>transposition, 717
Symmetric homomorphism, 705
Symmetric operator
</p>
<p>extremum problem, 197
spectral decomposition, 193&ndash;198
</p>
<p>Symmetric product, 791
Symmetrizer, 790
Symmetry, 3, 8
</p>
<p>algebraic equations, 1009&ndash;1014
calculus of variations, 1062&ndash;1065
conservation laws, 1065&ndash;1069
</p>
<p>classical field theory, 1069&ndash;1073
differential equations, 1014&ndash;1024
first-order ODEs, 1037&ndash;1039
heat equation, 1030&ndash;1034
higher-order ODEs, 1039, 1040
multiparameter, 1040&ndash;1043
tensors, 789&ndash;794
wave equation, 1034&ndash;1036
</p>
<p>Symmetry group
defining equations, 1030
of a subset, 1009
of a system of DEs, 1017
projectable, 1017
transform of a function, 1016
variational, 1062
</p>
<p>Symplectic algebra, 939
Symplectic charts, 902
Symplectic form, 801, 902
</p>
<p>rank of, 801
Symplectic geometry, 51, 901&ndash;909, 1079
</p>
<p>conservation of energy, 906
Symplectic group, 707, 803, 939
Symplectic manifold, 902
Symplectic map, 801, 902
Symplectic matrix, 804
Symplectic structure, 902
Symplectic transformation, 801
Symplectic vector space, 801&ndash;804
</p>
<p>canonical basis of, 802
Hamiltonian dynamics, 803
</p>
<p>T
Tangent bundle, 877
Tangent space, 869
Tangent vector, 868
</p>
<p>manifold, 866&ndash;872
Tangential coordinates, 637
Tangents to a curve
</p>
<p>components, 874
Target space, 5
Taylor expansion, 104
Taylor formula, 96</p>
<p/>
</div>
<div class="page"><p/>
<p>1204 Index
</p>
<p>Taylor series, 321&ndash;330
construction, 321
</p>
<p>Tensor, 784
classical definition, 787
components of, 785
contravariant, 784
contravariant-antisymmetric, 793
contravariant-symmetric, 789
covariant, 784
covariant-antisymmetric, 793
covariant-symmetric, 789
dual space, 782
Levi-Civita, 799
multilinear map, 782&ndash;789
symmetric, 789
symmetric product, 791
symmetries, 789&ndash;794
transformation law, 786
types of, 784
</p>
<p>Tensor algebra, 784
Tensor bundle, 883
Tensor field, 883, 887
</p>
<p>crucial property of, 883
curvature, 1125&ndash;1132
manifold, 876&ndash;888
torsion, 1125&ndash;1132
</p>
<p>Tensor operator
irreducible, 756&ndash;758
</p>
<p>Tensor product, 28, 29, 783, 784
algebra, 68
group representation
</p>
<p>Clebsch-Gordan decomposition,
753&ndash;756
</p>
<p>of vector spaces, 751
Tensorial form, 1092
Test function, 233
Theta function, 357
Timelike vector, 941
Topology, 8
Torsion, 1125&ndash;1132
Torsion form, 1122
Torsion tensor field, 1125
Total derivative, 1027
Total divergence, 1060
Total matrix algebra, 78&ndash;80, 92, 846, 850,
</p>
<p>852, 997, 999
Total space, 1080
Trace, 160&ndash;162
</p>
<p>and determinant, 161
definition, 160
log of determinant, 162
relation to determinant, 160
</p>
<p>Transformation
similarity, 148&ndash;151
</p>
<p>Transformation group, 704
Transition function, 1081
Transivity, 3
Translation, 919
Translation operator, 209
Transpose of a matrix, 142
</p>
<p>Traveling waves, 584
Triangle inequality, 8, 36, 38, 133, 216,
</p>
<p>301
Trigonometric function
</p>
<p>integration of, 350&ndash;352
Trivial bundle, 1080
Trivial homomorphism, 705
Trivial representation, 732
Trivial subgroup, 706
Twin paradox
</p>
<p>as a variational problem, 1060
Twisted adjoint representation, 987
</p>
<p>U
</p>
<p>Unbounded operator, 563&ndash;569
Uncertainty principle, 133
Uncertainty relation, 279
Uncountable set, 12
Union, 2
Unit circle, 7
Unital algebra, 63, 72
Unital homomorphism, 72
Unitary, 40
Unitary group, 706
</p>
<p>Lie algebra of, 924
Unitary operator, 114&ndash;119
Unitary representation, 730
Universal set, 2
Upper-triangular matrix, 66, 83, 175, 176
</p>
<p>V
</p>
<p>Vandermonde, biography, 153
Variational derivative, 1051
Variational problem, 1053&ndash;1060
</p>
<p>twin paradox, 1060
Variational symmetry group, 1062
Vector, 19
</p>
<p>Cartesian, 19
component, 23
dual of, 51
infinite sum, 215&ndash;220
isotropic, 808
length, 36&ndash;38
norm of, 36
normal, 32
null, 808
orthogonal, 32
tangent
</p>
<p>manifold, 866&ndash;872
Vector bundle, 1117
Vector field, 877
</p>
<p>as streamlines, 879
complete, 881
curl of, 889
flow of a, 881
fundamental, 1086
gauge transformation of, 1104
Hamiltonian, 905
horizontal, 1087</p>
<p/>
</div>
<div class="page"><p/>
<p>Index 1205
</p>
<p>Vector field (cont.)
integral curve of, 879
Killing, 1155&ndash;1159
left-invariant, 920
Lie algebra of, 879
manifold, 877&ndash;882
standard horizontal, 1121
vertical, 1087
</p>
<p>Vector potential, 3, 1099
Vector space, 8, 19&ndash;29
</p>
<p>automorphism, 43
basis
</p>
<p>components in a, 23
basis of a, 23
complete, 216
complex, 20
definition, 19
dual, 48
endomorphism of a, 39
finite-dimension
</p>
<p>criterion for, 522
finite-dimensional, 23
indefinite inner product
</p>
<p>orthonormal basis, 812&ndash;819
subspaces, 809&ndash;812
</p>
<p>isomorphism, 43
linear operator on a, 39
Minkowski, 815
normed, 36
</p>
<p>compact subset of, 522
operator on a, 39
orientation, 800, 801
oriented, 800
real, 20
self-dual, 805
semi-Euclidean, 815
symplectic, 801&ndash;804
</p>
<p>Vertical vector field, 1087
Volterra, biography, 545
Volterra equation, 543
Volume element, 801
</p>
<p>relative to an inner product, 816
Von Humboldt, 246, 666, 792
Von Neumann, 981
</p>
<p>biography, 532
</p>
<p>W
</p>
<p>Wave equation, 395, 584
hyperbolic, 642
symmetry group, 1034&ndash;1036
</p>
<p>Wave guide, 584
cylindrical, 588, 589
rectangular, 584, 585, 600
</p>
<p>Weber-Hermite equation, 487
Wedderburn decomposition, 92
Wedge product, 794
Weierstrass, 10, 36, 366, 640, 792, 946
</p>
<p>biography, 523
Weight function, 32
Weyl, 799, 946, 1015, 1070
</p>
<p>biography, 956
Weyl basis, 938, 947
Weyl operator, 955
Wigner, 236, 1015
</p>
<p>biography, 981
Wigner formula, 972
Wigner-Eckart theorem, 758
Wigner-Seitz cell, 276
WKB method, 450&ndash;453
</p>
<p>connection formulas, 451
Wordsworth, 907
Wronski, biography, 425
Wronskian, 425&ndash;432, 567
</p>
<p>Y
Young, 957
Young antisymmetrizer, 772
Young frame, 765, 772
</p>
<p>negative application, 768
positive application, 768
regular application, 767
</p>
<p>Young operator, 771&ndash;774, 963
Young pattern, 765
Young symmetrizer, 772
Young tableaux, 766, 964
</p>
<p>horizontal permutation, 772
regular graphs, 766
vertical permutation, 772
</p>
<p>Yukawa potential, 282
</p>
<p>Z
</p>
<p>Zero of order k, 329</p>
<p/>
</div>
<ul>	<li>Mathematical Physics</li>
<ul>	<li>Preface to Second Edition</li>
	<li>Preface to First Edition</li>
<ul>	<li>Level and Philosophy of Presentation</li>
	<li>Features</li>
	<li>Organization and Topical Coverage</li>
	<li>Acknowledgments</li>
</ul>
	<li>Note to the Reader</li>
	<li>Contents</li>
	<li>List of Symbols</li>
</ul>
	<li>Chapter 1: Mathematical Preliminaries</li>
<ul>	<li>1.1 Sets</li>
<ul>	<li>1.1.1 Equivalence Relations</li>
</ul>
	<li>1.2 Maps</li>
	<li>1.3 Metric Spaces</li>
	<li>1.4 Cardinality</li>
	<li>1.5 Mathematical Induction</li>
	<li>1.6 Problems</li>
</ul>
	<li>Part I: Finite-Dimensional Vector Spaces</li>
<ul>	<li>Chapter 2: Vectors and Linear Maps</li>
<ul>	<li>2.1 Vector Spaces</li>
<ul>	<li>2.1.1 Subspaces</li>
	<li>2.1.2 Factor Space</li>
	<li>2.1.3 Direct Sums</li>
	<li>2.1.4 Tensor Product of Vector Spaces</li>
</ul>
	<li>2.2 Inner Product</li>
<ul>	<li>2.2.1 Orthogonality</li>
	<li>2.2.2 The Gram-Schmidt Process</li>
	<li>2.2.3 The Schwarz Inequality</li>
	<li>2.2.4 Length of a Vector</li>
</ul>
	<li>2.3 Linear Maps</li>
<ul>	<li>2.3.1 Kernel of a Linear Map</li>
	<li>2.3.2 Linear Isomorphism</li>
</ul>
	<li>2.4 Complex Structures</li>
	<li>2.5 Linear Functionals</li>
	<li>2.6 Multilinear Maps</li>
<ul>	<li>2.6.1 Determinant of a Linear Operator</li>
	<li>2.6.2 Classical Adjoint</li>
</ul>
	<li>2.7 Problems</li>
</ul>
	<li>Chapter 3: Algebras</li>
<ul>	<li>3.1 From Vector Space to Algebra</li>
<ul>	<li>3.1.1 General Properties</li>
	<li>3.1.2 Homomorphisms</li>
</ul>
	<li>3.2 Ideals</li>
<ul>	<li>3.2.1 Factor Algebras</li>
</ul>
	<li>3.3 Total Matrix Algebra</li>
	<li>3.4 Derivation of an Algebra</li>
	<li>3.5 Decomposition of Algebras</li>
<ul>	<li>3.5.1 The Radical</li>
	<li>3.5.2 Semi-simple Algebras</li>
	<li>3.5.3 Classiﬁcation of Simple Algebras</li>
</ul>
	<li>3.6 Polynomial Algebra</li>
	<li>3.7 Problems</li>
</ul>
	<li>Chapter 4: Operator Algebra</li>
<ul>	<li>4.1 Algebra of `39`42`"613A``45`47`"603AEnd(V)</li>
<ul>	<li>4.1.1 Polynomials of Operators</li>
	<li>4.1.2 Functions of Operators</li>
	<li>4.1.3 Commutators</li>
</ul>
	<li>4.2 Derivatives of Operators</li>
	<li>4.3 Conjugation of Operators</li>
<ul>	<li>4.3.1 Hermitian Operators</li>
	<li>4.3.2 Unitary Operators</li>
</ul>
	<li>4.4 Idempotents</li>
<ul>	<li>4.4.1 Projection Operators</li>
</ul>
	<li>4.5 Representation of Algebras</li>
	<li>4.6 Problems</li>
</ul>
	<li>Chapter 5: Matrices</li>
<ul>	<li>5.1 Representing Vectors and Operators</li>
	<li>5.2 Operations on Matrices</li>
	<li>5.3 Orthonormal Bases</li>
	<li>5.4 Change of Basis</li>
	<li>5.5 Determinant of a Matrix</li>
<ul>	<li>5.5.1 Matrix of the Classical Adjoint</li>
	<li>5.5.2 Inverse of a Matrix</li>
<ul>	<li>Algorithm for Calculating the Inverse of a Matrix</li>
	<li>Rank of a Matrix</li>
</ul>
	<li>5.5.3 Dual Determinant Function</li>
</ul>
	<li>5.6 The Trace</li>
	<li>5.7 Problems</li>
</ul>
	<li>Chapter 6: Spectral Decomposition</li>
<ul>	<li>6.1 Invariant Subspaces</li>
	<li>6.2 Eigenvalues and Eigenvectors</li>
	<li>6.3 Upper-Triangular Representations</li>
	<li>6.4 Complex Spectral Decomposition</li>
<ul>	<li>6.4.1 Simultaneous Diagonalization</li>
</ul>
	<li>6.5 Functions of Operators</li>
	<li>6.6 Real Spectral Decomposition</li>
<ul>	<li>6.6.1 The Case of Symmetric Operators</li>
	<li>6.6.2 The Case of Real Normal Operators</li>
</ul>
	<li>6.7 Polar Decomposition</li>
	<li>6.8 Problems</li>
</ul>
</ul>
	<li>Part II: Inﬁnite-Dimensional Vector Spaces</li>
<ul>	<li>Chapter 7: Hilbert Spaces</li>
<ul>	<li>7.1 The Question of Convergence</li>
	<li>7.2 The Space of Square-Integrable Functions</li>
<ul>	<li>7.2.1 Orthogonal Polynomials</li>
	<li>7.2.2 Orthogonal Polynomials and Least Squares</li>
</ul>
	<li>7.3 Continuous Index</li>
	<li>7.4 Generalized Functions</li>
	<li>7.5 Problems</li>
</ul>
	<li>Chapter 8: Classical Orthogonal Polynomials</li>
<ul>	<li>8.1 General Properties</li>
	<li>8.2 Classiﬁcation</li>
	<li>8.3 Recurrence Relations</li>
	<li>8.4 Details of Speciﬁc Examples</li>
<ul>	<li>8.4.1 Hermite Polynomials</li>
	<li>8.4.2 Laguerre Polynomials</li>
	<li>8.4.3 Legendre Polynomials</li>
	<li>8.4.4 Other Classical Orthogonal Polynomials</li>
<ul>	<li>Jacobi Polynomials, Pn&micro;,nu(x)</li>
	<li>Gegenbauer Polynomials, Cnlambda(x)</li>
	<li>Chebyshev Polynomials of the First Kind, Tn(x)</li>
	<li>Chebyshev Polynomials of the Second Kind, Un(x)</li>
</ul>
</ul>
	<li>8.5 Expansion in Terms of Orthogonal Polynomials</li>
	<li>8.6 Generating Functions</li>
	<li>8.7 Problems</li>
</ul>
	<li>Chapter 9: Fourier Analysis</li>
<ul>	<li>9.1 Fourier Series</li>
<ul>	<li>9.1.1 The Gibbs Phenomenon</li>
	<li>9.1.2 Fourier Series in Higher Dimensions</li>
</ul>
	<li>9.2 Fourier Transform</li>
<ul>	<li>9.2.1 Fourier Transforms and Derivatives</li>
	<li>9.2.2 The Discrete Fourier Transform</li>
	<li>9.2.3 Fourier Transform of a Distribution</li>
</ul>
	<li>9.3 Problems</li>
</ul>
</ul>
	<li>Part III: Complex Analysis</li>
<ul>	<li>Chapter 10: Complex Calculus</li>
<ul>	<li>10.1 Complex Functions</li>
	<li>10.2 Analytic Functions</li>
	<li>10.3 Conformal Maps</li>
	<li>10.4 Integration of Complex Functions</li>
	<li>10.5 Derivatives as Integrals</li>
	<li>10.6 Inﬁnite Complex Series</li>
<ul>	<li>10.6.1 Properties of Series</li>
	<li>10.6.2 Taylor and Laurent Series</li>
</ul>
	<li>10.7 Problems</li>
</ul>
	<li>Chapter 11: Calculus of Residues</li>
<ul>	<li>11.1 Residues</li>
	<li>11.2 Classiﬁcation of Isolated Singularities</li>
	<li>11.3 Evaluation of Deﬁnite Integrals</li>
<ul>	<li>11.3.1 Integrals of Rational Functions</li>
	<li>11.3.2 Products of Rational and Trigonometric Functions</li>
	<li>11.3.3 Functions of Trigonometric Functions</li>
	<li>11.3.4 Some Other Integrals</li>
	<li>11.3.5 Principal Value of an Integral</li>
</ul>
	<li>11.4 Problems</li>
</ul>
	<li>Chapter 12: Advanced Topics</li>
<ul>	<li>12.1 Meromorphic Functions</li>
	<li>12.2 Multivalued Functions</li>
<ul>	<li>12.2.1 Riemann Surfaces</li>
</ul>
	<li>12.3 Analytic Continuation</li>
<ul>	<li>12.3.1 The Schwarz Reﬂection Principle</li>
	<li>12.3.2 Dispersion Relations</li>
</ul>
	<li>12.4 The Gamma and Beta Functions</li>
	<li>12.5 Method of Steepest Descent</li>
	<li>12.6 Problems</li>
</ul>
</ul>
	<li>Part IV: Differential Equations</li>
<ul>	<li>Chapter 13: Separation of Variables in Spherical Coordinates</li>
<ul>	<li>13.1 PDEs of Mathematical Physics</li>
	<li>13.2 Separation of the Angular Part</li>
	<li>13.3 Construction of Eigenvalues of L2</li>
	<li>13.4 Eigenvectors of L2: Spherical Harmonics</li>
<ul>	<li>13.4.1 Expansion of Angular Functions</li>
	<li>13.4.2 Addition Theorem for Spherical Harmonics</li>
</ul>
	<li>13.5 Problems</li>
</ul>
	<li>Chapter 14: Second-Order Linear Differential Equations</li>
<ul>	<li>14.1 General Properties of ODEs</li>
	<li>14.2 Existence/Uniqueness for First-Order DEs</li>
	<li>14.3 General Properties of SOLDEs</li>
	<li>14.4 The Wronskian</li>
<ul>	<li>14.4.1 A Second Solution to the HSOLDE</li>
	<li>14.4.2 The General Solution to an ISOLDE</li>
	<li>14.4.3 Separation and Comparison Theorems</li>
</ul>
	<li>14.5 Adjoint Differential Operators</li>
	<li>14.6 Power-Series Solutions of SOLDEs</li>
<ul>	<li>14.6.1 Frobenius Method of Undetermined Coefﬁcients</li>
	<li>14.6.2 Quantum Harmonic Oscillator</li>
</ul>
	<li>14.7 SOLDEs with Constant Coefﬁcients</li>
	<li>14.8 The WKB Method</li>
<ul>	<li>14.8.1 Classical Limit of the Schr&ouml;dinger Equation</li>
</ul>
	<li>14.9 Problems</li>
</ul>
	<li>Chapter 15: Complex Analysis of SOLDEs</li>
<ul>	<li>15.1 Analytic Properties of Complex DEs</li>
<ul>	<li>15.1.1 Complex FOLDEs</li>
	<li>15.1.2 The Circuit Matrix</li>
</ul>
	<li>15.2 Complex SOLDEs</li>
	<li>15.3 Fuchsian Differential Equations</li>
	<li>15.4 The Hypergeometric Function</li>
	<li>15.5 Conﬂuent Hypergeometric Functions</li>
<ul>	<li>15.5.1 Hydrogen-Like Atoms</li>
	<li>15.5.2 Bessel Functions</li>
</ul>
	<li>15.6 Problems</li>
</ul>
	<li>Chapter 16: Integral Transforms and Differential Equations</li>
<ul>	<li>16.1 Integral Representation of the Hypergeometric Function</li>
<ul>	<li>16.1.1 Integral Representation of the Conﬂuent Hypergeometric Function</li>
</ul>
	<li>16.2 Integral Representation of Bessel Functions</li>
<ul>	<li>16.2.1 Asymptotic Behavior of Bessel Functions</li>
</ul>
	<li>16.3 Problems</li>
</ul>
</ul>
	<li>Part V: Operators on Hilbert Spaces</li>
<ul>	<li>Chapter 17: Introductory Operator Theory</li>
<ul>	<li>17.1 From Abstract to Integral and Differential Operators</li>
	<li>17.2 Bounded Operators in Hilbert Spaces</li>
<ul>	<li>17.2.1 Adjoints of Bounded Operators</li>
</ul>
	<li>17.3 Spectra of Linear Operators</li>
	<li>17.4 Compact Sets</li>
<ul>	<li>17.4.1 Compactness and Inﬁnite Sequences</li>
</ul>
	<li>17.5 Compact Operators</li>
<ul>	<li>17.5.1 Spectrum of Compact Operators</li>
</ul>
	<li>17.6 Spectral Theorem for Compact Operators</li>
<ul>	<li>17.6.1 Compact Hermitian Operator</li>
	<li>17.6.2 Compact Normal Operator</li>
</ul>
	<li>17.7 Resolvents</li>
	<li>17.8 Problems</li>
</ul>
	<li>Chapter 18: Integral Equations</li>
<ul>	<li>18.1 Classiﬁcation</li>
	<li>18.2 Fredholm Integral Equations</li>
<ul>	<li>18.2.1 Hermitian Kernel</li>
	<li>18.2.2 Degenerate Kernels</li>
</ul>
	<li>18.3 Problems</li>
</ul>
	<li>Chapter 19: Sturm-Liouville Systems</li>
<ul>	<li>19.1 Compact-Resolvent Unbounded Operators</li>
	<li>19.2 Sturm-Liouville Systems and SOLDEs</li>
	<li>19.3 Asymptotic Behavior</li>
<ul>	<li>19.3.1 Large Eigenvalues</li>
	<li>19.3.2 Large Argument</li>
</ul>
	<li>19.4 Expansions in Terms of Eigenfunctions</li>
	<li>19.5 Separation in Cartesian Coordinates</li>
<ul>	<li>19.5.1 Rectangular Conducting Box</li>
	<li>19.5.2 Heat Conduction in a Rectangular Plate</li>
	<li>19.5.3 Quantum Particle in a Box</li>
	<li>19.5.4 Wave Guides</li>
</ul>
	<li>19.6 Separation in Cylindrical Coordinates</li>
<ul>	<li>19.6.1 Conducting Cylindrical Can</li>
	<li>19.6.2 Cylindrical Wave Guide</li>
	<li>19.6.3 Current Distribution in a Circular Wire</li>
</ul>
	<li>19.7 Separation in Spherical Coordinates</li>
<ul>	<li>19.7.1 Radial Part of Laplace's Equation</li>
	<li>19.7.2 Helmholtz Equation in Spherical Coordinates</li>
	<li>19.7.3 Quantum Particle in a Hard Sphere</li>
	<li>19.7.4 Plane Wave Expansion</li>
</ul>
	<li>19.8 Problems</li>
</ul>
</ul>
	<li>Part VI: Green's Functions</li>
<ul>	<li>Chapter 20: Green's Functions in One Dimension</li>
<ul>	<li>20.1 Calculation of Some Green's Functions</li>
	<li>20.2 Formal Considerations</li>
<ul>	<li>20.2.1 Second-Order Linear DOs</li>
	<li>20.2.2 Self-adjoint SOLDOs</li>
</ul>
	<li>20.3 Green's Functions for SOLDOs</li>
<ul>	<li>20.3.1 Properties of Green's Functions</li>
	<li>20.3.2 Construction and Uniqueness of Green's Functions</li>
	<li>20.3.3 Inhomogeneous BCs</li>
</ul>
	<li>20.4 Eigenfunction Expansion</li>
	<li>20.5 Problems</li>
</ul>
	<li>Chapter 21: Multidimensional Green's Functions: Formalism</li>
<ul>	<li>21.1 Properties of Partial Differential Equations</li>
<ul>	<li>21.1.1 Characteristic Hypersurfaces</li>
	<li>21.1.2 Second-Order PDEs in m Dimensions</li>
</ul>
	<li>21.2 Multidimensional GFs and Delta Functions</li>
<ul>	<li>21.2.1 Spherical Coordinates in m Dimensions</li>
	<li>21.2.2 Green's Function for the Laplacian</li>
</ul>
	<li>21.3 Formal Development</li>
<ul>	<li>21.3.1 General Properties</li>
	<li>21.3.2 Fundamental (Singular) Solutions</li>
</ul>
	<li>21.4 Integral Equations and GFs</li>
	<li>21.5 Perturbation Theory</li>
<ul>	<li>21.5.1 The Nondegenerate Case</li>
	<li>21.5.2 The Degenerate Case</li>
</ul>
	<li>21.6 Problems</li>
</ul>
	<li>Chapter 22: Multidimensional Green's Functions: Applications</li>
<ul>	<li>22.1 Elliptic Equations</li>
<ul>	<li>22.1.1 The Dirichlet Boundary Value Problem</li>
	<li>22.1.2 The Neumann Boundary Value Problem</li>
</ul>
	<li>22.2 Parabolic Equations</li>
	<li>22.3 Hyperbolic Equations</li>
	<li>22.4 The Fourier Transform Technique</li>
<ul>	<li>22.4.1 GF for the m-Dimensional Laplacian</li>
	<li>22.4.2 GF for the m-Dimensional Helmholtz Operator</li>
	<li>22.4.3 GF for the m-Dimensional Diffusion Operator</li>
	<li>22.4.4 GF for the m-Dimensional Wave Equation</li>
</ul>
	<li>22.5 The Eigenfunction Expansion Technique</li>
	<li>22.6 Problems</li>
</ul>
</ul>
	<li>Part VII: Groups and Their Representations</li>
<ul>	<li>Chapter 23: Group Theory</li>
<ul>	<li>23.1 Groups</li>
	<li>23.2 Subgroups</li>
<ul>	<li>23.2.1 Direct Products</li>
</ul>
	<li>23.3 Group Action</li>
	<li>23.4 The Symmetric Group Sn</li>
	<li>23.5 Problems</li>
</ul>
	<li>Chapter 24: Representation of Groups</li>
<ul>	<li>24.1 Deﬁnitions and Examples</li>
	<li>24.2 Irreducible Representations</li>
	<li>24.3 Orthogonality Properties</li>
	<li>24.4 Analysis of Representations</li>
	<li>24.5 Group Algebra</li>
<ul>	<li>24.5.1 Group Algebra and Representations</li>
</ul>
	<li>24.6 Relationship of Characters to Those of a Subgroup</li>
	<li>24.7 Irreducible Basis Functions</li>
	<li>24.8 Tensor Product of Representations</li>
<ul>	<li>24.8.1 Clebsch-Gordan Decomposition</li>
	<li>24.8.2 Irreducible Tensor Operators</li>
</ul>
	<li>24.9 Problems</li>
</ul>
	<li>Chapter 25: Representations of the Symmetric Group</li>
<ul>	<li>25.1 Analytic Construction</li>
	<li>25.2 Graphical Construction</li>
	<li>25.3 Graphical Construction of Characters</li>
	<li>25.4 Young Operators</li>
	<li>25.5 Products of Representations of Sn</li>
	<li>25.6 Problems</li>
</ul>
</ul>
	<li>Part VIII: Tensors and Manifolds</li>
<ul>	<li>Chapter 26: Tensors</li>
<ul>	<li>26.1 Tensors as Multilinear Maps</li>
	<li>26.2 Symmetries of Tensors</li>
	<li>26.3 Exterior Algebra</li>
<ul>	<li>26.3.1 Orientation</li>
</ul>
	<li>26.4 Symplectic Vector Spaces</li>
	<li>26.5 Inner Product Revisited</li>
<ul>	<li>26.5.1 Subspaces</li>
	<li>26.5.2 Orthonormal Basis</li>
	<li>26.5.3 Inner Product on Lambdap(V,U)</li>
</ul>
	<li>26.6 The Hodge Star Operator</li>
	<li>26.7 Problems</li>
</ul>
	<li>Chapter 27: Clifford Algebras</li>
<ul>	<li>27.1 Construction of Clifford Algebras</li>
<ul>	<li>27.1.1 The Dirac Equation</li>
</ul>
	<li>27.2 General Properties of the Clifford Algebra</li>
<ul>	<li>27.2.1 Homomorphism with Other Algebras</li>
	<li>27.2.2 The Canonical Element</li>
	<li>27.2.3 Center and Anticenter</li>
	<li>27.2.4 Isomorphisms</li>
</ul>
	<li>27.3 General Classiﬁcation of Clifford Algebras</li>
	<li>27.4 The Clifford Algebras C&micro;nu(R)</li>
<ul>	<li>27.4.1 Classiﬁcation of Cn0(R) and C0n(R)</li>
	<li>27.4.2 Classiﬁcation of C&micro;nu(R)</li>
	<li>27.4.3 The Algebra C31(R)</li>
</ul>
	<li>27.5 Problems</li>
</ul>
	<li>Chapter 28: Analysis of Tensors</li>
<ul>	<li>28.1 Differentiable Manifolds</li>
	<li>28.2 Curves and Tangent Vectors</li>
	<li>28.3 Differential of a Map</li>
	<li>28.4 Tensor Fields on Manifolds</li>
<ul>	<li>28.4.1 Vector Fields</li>
	<li>28.4.2 Tensor Fields</li>
</ul>
	<li>28.5 Exterior Calculus</li>
	<li>28.6 Integration on Manifolds</li>
	<li>28.7 Symplectic Geometry</li>
	<li>28.8 Problems</li>
</ul>
</ul>
	<li>Part IX: Lie Groups and Their Applications</li>
<ul>	<li>Chapter 29: Lie Groups and Lie Algebras</li>
<ul>	<li>29.1 Lie Groups and Their Algebras</li>
<ul>	<li>29.1.1 Group Action</li>
	<li>29.1.2 Lie Algebra of a Lie Group</li>
	<li>29.1.3 Invariant Forms</li>
	<li>29.1.4 Inﬁnitesimal Action</li>
	<li>29.1.5 Integration on Lie Groups</li>
</ul>
	<li>29.2 An Outline of Lie Algebra Theory</li>
<ul>	<li>29.2.1 The Lie Algebras o(p,n-p) and p(p,n-p)</li>
	<li>29.2.2 Operations on Lie Algebras</li>
</ul>
	<li>29.3 Problems</li>
</ul>
	<li>Chapter 30: Representation of Lie Groups and Lie Algebras</li>
<ul>	<li>30.1 Representation of Compact Lie Groups</li>
	<li>30.2 Representation of the General Linear Group</li>
	<li>30.3 Representation of Lie Algebras</li>
<ul>	<li>30.3.1 Representation of Subgroups of GL(V)</li>
	<li>30.3.2 Casimir Operators</li>
	<li>30.3.3 Representation of so(3) and so(3,1)</li>
	<li>30.3.4 Representation of the Poincar&eacute; Algebra</li>
</ul>
	<li>30.4 Problems</li>
</ul>
	<li>Chapter 31: Representation of Clifford Algebras</li>
<ul>	<li>31.1 The Clifford Group</li>
	<li>31.2 Spinors</li>
<ul>	<li>31.2.1 Pauli Spin Matrices and Spinors</li>
	<li>31.2.2 Spinors for C&micro;nu(R)</li>
	<li>31.2.3 C31(R) Revisited</li>
</ul>
	<li>31.3 Problems</li>
</ul>
	<li>Chapter 32: Lie Groups and Differential Equations</li>
<ul>	<li>32.1 Symmetries of Algebraic Equations</li>
	<li>32.2 Symmetry Groups of Differential Equations</li>
<ul>	<li>32.2.1 Prolongation of Functions</li>
	<li>32.2.2 Prolongation of Groups</li>
	<li>32.2.3 Prolongation of Vector Fields</li>
</ul>
	<li>32.3 The Central Theorems</li>
	<li>32.4 Application to Some Known PDEs</li>
<ul>	<li>32.4.1 The Heat Equation</li>
	<li>32.4.2 The Wave Equation</li>
</ul>
	<li>32.5 Application to ODEs</li>
<ul>	<li>32.5.1 First-Order ODEs</li>
	<li>32.5.2 Higher-Order ODEs</li>
	<li>32.5.3 DEs with Multiparameter Symmetries</li>
</ul>
	<li>32.6 Problems</li>
</ul>
	<li>Chapter 33: Calculus of Variations, Symmetries, and Conservation Laws</li>
<ul>	<li>33.1 The Calculus of Variations</li>
<ul>	<li>33.1.1 Derivative for Hilbert Spaces</li>
	<li>33.1.2 Functional Derivative</li>
	<li>33.1.3 Variational Problems</li>
	<li>33.1.4 Divergence and Null Lagrangians</li>
</ul>
	<li>33.2 Symmetry Groups of Variational Problems</li>
	<li>33.3 Conservation Laws and Noether's Theorem</li>
	<li>33.4 Application to Classical Field Theory</li>
	<li>33.5 Problems</li>
</ul>
</ul>
	<li>Part X: Fiber Bundles</li>
<ul>	<li>Chapter 34: Fiber Bundles and Connections</li>
<ul>	<li>34.1 Principal Fiber Bundles</li>
<ul>	<li>34.1.1 Associated Bundles</li>
</ul>
	<li>34.2 Connections in a PFB</li>
<ul>	<li>34.2.1 Local Expression for a Connection</li>
	<li>34.2.2 Parallelism</li>
</ul>
	<li>34.3 Curvature Form</li>
<ul>	<li>34.3.1 Flat Connections</li>
	<li>34.3.2 Matrix Structure Group</li>
</ul>
	<li>34.4 Problems</li>
</ul>
	<li>Chapter 35: Gauge Theories</li>
<ul>	<li>35.1 Gauge Potentials and Fields</li>
<ul>	<li>35.1.1 Particle Fields</li>
	<li>35.1.2 Gauge Transformation</li>
</ul>
	<li>35.2 Gauge-Invariant Lagrangians</li>
	<li>35.3 Construction of Gauge-Invariant Lagrangians</li>
	<li>35.4 Local Equations</li>
	<li>35.5 Problems</li>
</ul>
	<li>Chapter 36: Differential Geometry</li>
<ul>	<li>36.1 Connections in a Vector Bundle</li>
	<li>36.2 Linear Connections</li>
<ul>	<li>36.2.1 Covariant Derivative of Tensor Fields</li>
	<li>36.2.2 From Forms on P to Tensor Fields on M</li>
	<li>36.2.3 Component Expressions</li>
	<li>36.2.4 General Basis</li>
</ul>
	<li>36.3 Geodesics</li>
<ul>	<li>36.3.1 Riemann Normal Coordinates</li>
</ul>
	<li>36.4 Problems</li>
</ul>
	<li>Chapter 37: Riemannian Geometry</li>
<ul>	<li>37.1 The Metric Connection</li>
<ul>	<li>37.1.1 Orthogonal Bases</li>
</ul>
	<li>37.2 Isometries and Killing Vector Fields</li>
	<li>37.3 Geodesic Deviation and Curvature</li>
<ul>	<li>37.3.1 Newtonian Gravity</li>
</ul>
	<li>37.4 General Theory of Relativity</li>
<ul>	<li>37.4.1 Einstein's Equation</li>
	<li>37.4.2 Static Spherically Symmetric Solutions</li>
	<li>37.4.3 Schwarzschild Geodesics</li>
<ul>	<li>Massive Particle</li>
	<li>Massless Particle</li>
</ul>
</ul>
	<li>37.5 Problems</li>
</ul>
</ul>
	<li>References</li>
	<li>Index</li>
</ul>
</body></html>