<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Untitled</title>
</head>
<body><div class="page"><p/>
<p>Data Analysis
</p>
<p>Siegmund Brandt
</p>
<p>Statistical and Computational Methods 
for Scientists and Engineers
</p>
<p>Fourth Edition</p>
<p/>
</div>
<div class="page"><p/>
<p>Data Analysis</p>
<p/>
</div>
<div class="page"><p/>
</div>
<div class="page"><p/>
<p>Siegmund Brandt
</p>
<p>Data Analysis
</p>
<p>Statistical and Computational Methods
for Scientists and Engineers
</p>
<p>Fourth Edition
</p>
<p>123
</p>
<p>Translated by Glen Cowan</p>
<p/>
</div>
<div class="page"><p/>
<p>Siegmund Brandt
Department of Physics
University of Siegen
Siegen, Germany
</p>
<p>Additional material to this book can be downloaded from http://extras.springer.com
</p>
<p>ISBN 978-3-319-03761-5 ISBN 978-3-319-03762-2 (eBook)
DOI 10.1007/978-3-319-03762-2
Springer Cham Heidelberg New York Dordrecht London
</p>
<p>Library of Congress Control Number: 2013957143
</p>
<p>&copy; Springer International Publishing Switzerland 2014
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the ma-
terial is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting,
reproduction on microfilms or in any other physical way, and transmission or information storage and retrieval, elec-
tronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed.
Exempted from this legal reservation are brief excerpts in connection with reviews or scholarly analysis or material
supplied specifically for the purpose of being entered and executed on a computer system, for exclusive use by the
purchaser of the work. Duplication of this publication or parts thereof is permitted only under the provisions of
the Copyright Law of the Publisher&rsquo;s location, in its current version, and permission for use must always be ob-
tained from Springer. Permissions for use may be obtained through RightsLink at the Copyright Clearance Center.
Violations are liable to prosecution under the respective Copyright Law.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication does not
imply, even in the absence of a specific statement, that such names are exempt from the relevant protective laws and
regulations and therefore free for general use.
While the advice and information in this book are believed to be true and accurate at the date of publication, neither
the authors nor the editors nor the publisher can accept any legal responsibility for any errors or omissions that may
be made. The publisher makes no warranty, express or implied, with respect to the material contained herein.
</p>
<p>Printed on acid-free paper
</p>
<p>Springer is part of Springer Science+Business Media (www.springer.com)</p>
<p/>
<div class="annotation"><a href="http://extras.springer.com">http://extras.springer.com</a></div>
<div class="annotation"><a href="www.springer.com">www.springer.com</a></div>
</div>
<div class="page"><p/>
<p>Preface to the Fourth English Edition
</p>
<p>For the present edition, the book has undergone two major changes: Its
appearance was tightened significantly and the programs are now written in
the modern programming language Java.
</p>
<p>Tightening was possible without giving up essential contents by expedi-
ent use of the Internet. Since practically all users can connect to the net, it is
no longer necessary to reproduce program listings in the printed text. In this
way, the physical size of the book was reduced considerably.
</p>
<p>The Java language offers a number of advantages over the older program-
ming languages used in earlier editions. It is object-oriented and hence also
more readable. It includes access to libraries of user-friendly auxiliary rou-
tines, allowing for instance the easy creation of windows for input, output,
or graphics. For most popular computers, Java is either preinstalled or can be
downloaded from the Internet free of charge. (See Sect. 1.3 for details.) Since
by now Java is often taught at school, many students are already somewhat
familiar with the language.
</p>
<p>Our Java programs for data analysis and for the production of graphics,
including many example programs and solutions to programming problems,
</p>
<p>v
</p>
<p>can be downloaded from the page
</p>
<p>I am grateful to Dr. Tilo Stroh for numerous stimulating discussions and
</p>
<p>technical help. The graphics programs are based on previous common work.
</p>
<p>Siegen, Germany Siegmund Brandt
</p>
<p>extras.springer.com.</p>
<p/>
</div>
<div class="page"><p/>
</div>
<div class="page"><p/>
<p>Contents
</p>
<p>Preface to the Fourth English Edition v
</p>
<p>List of Examples xv
</p>
<p>Frequently Used Symbols and Notation xix
</p>
<p>1 Introduction 1
</p>
<p>1.1 Typical Problems of Data Analysis . . . . . . . . . . . . . . . 1
1.2 On the Structure of this Book . . . . . . . . . . . . . . . . . . 2
1.3 About the Computer Programs . . . . . . . . . . . . . . . . . . 5
</p>
<p>2 Probabilities 7
</p>
<p>2.1 Experiments, Events, Sample Space . . . . . . . . . . . . . . . 7
2.2 The Concept of Probability . . . . . . . . . . . . . . . . . . . . 8
2.3 Rules of Probability Calculus: Conditional Probability . . . . 10
2.4 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
</p>
<p>2.4.1 Probability for n Dots in the Throwing of Two
Dice . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
</p>
<p>2.4.2 Lottery 6 Out of 49 . . . . . . . . . . . . . . . . . . . 12
2.4.3 Three-Door Game . . . . . . . . . . . . . . . . . . . . 13
</p>
<p>3 Random Variables: Distributions 15
</p>
<p>3.1 Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . 15
3.2 Distributions of a Single Random Variable . . . . . . . . . . . 15
3.3 Functions of a Single Random Variable, Expectation Value,
</p>
<p>Variance, Moments . . . . . . . . . . . . . . . . . . . . . . . . 17
3.4 Distribution Function and Probability Density of Two
</p>
<p>Variables: Conditional Probability . . . . . . . . . . . . . . . . 25
3.5 Expectation Values, Variance, Covariance, and Correlation . . 27
</p>
<p>vii</p>
<p/>
</div>
<div class="page"><p/>
<p>viii Contents
</p>
<p>3.6 More than Two Variables: Vector and Matrix Notation . . . . 30
3.7 Transformation of Variables . . . . . . . . . . . . . . . . . . . 33
3.8 Linear and Orthogonal Transformations: Error
</p>
<p>Propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
</p>
<p>4 Computer Generated Random Numbers: The Monte
</p>
<p>Carlo Method 41
</p>
<p>4.1 Random Numbers . . . . . . . . . . . . . . . . . . . . . . . . . 41
4.2 Representation of Numbers in a Computer . . . . . . . . . . . 42
4.3 Linear Congruential Generators . . . . . . . . . . . . . . . . . 44
4.4 Multiplicative Linear Congruential Generators . . . . . . . . . 45
4.5 Quality of an MLCG: Spectral Test . . . . . . . . . . . . . . . 47
4.6 Implementation and Portability of an MLCG . . . . . . . . . . 50
4.7 Combination of Several MLCGs . . . . . . . . . . . . . . . . . 52
4.8 Generation of Arbitrarily Distributed Random Numbers . . . 55
</p>
<p>4.8.1 Generation by Transformation of the Uniform
Distribution . . . . . . . . . . . . . . . . . . . . . . . . 55
</p>
<p>4.8.2 Generation with the von Neumann Acceptance&ndash;Re-
jection Technique . . . . . . . . . . . . . . . . . . . . 58
</p>
<p>4.9 Generation of Normally Distributed Random Numbers . . . . 62
4.10 Generation of Random Numbers According
</p>
<p>to a Multivariate Normal Distribution . . . . . . . . . . . . . . 63
4.11 The Monte Carlo Method for Integration . . . . . . . . . . . . 64
4.12 The Monte Carlo Method for Simulation . . . . . . . . . . . . 66
4.13 Java Classes and Example Programs . . . . . . . . . . . . . . 67
</p>
<p>5 Some Important Distributions and Theorems 69
</p>
<p>5.1 The Binomial and Multinomial Distributions . . . . . . . . . 69
5.2 Frequency: The Law of Large Numbers . . . . . . . . . . . . 72
5.3 The Hypergeometric Distribution . . . . . . . . . . . . . . . . 74
5.4 The Poisson Distribution . . . . . . . . . . . . . . . . . . . . . 78
5.5 The Characteristic Function of a Distribution . . . . . . . . . 81
5.6 The Standard Normal Distribution . . . . . . . . . . . . . . . . 84
5.7 The Normal or Gaussian Distribution . . . . . . . . . . . . . . 86
5.8 Quantitative Properties of the Normal Distribution . . . . . . 88
5.9 The Central Limit Theorem . . . . . . . . . . . . . . . . . . . 90
5.10 The Multivariate Normal Distribution . . . . . . . . . . . . . . 94
5.11 Convolutions of Distributions . . . . . . . . . . . . . . . . . . 100
</p>
<p>5.11.1 Folding Integrals . . . . . . . . . . . . . . . . . . . . . 100
5.11.2 Convolutions with the Normal Distribution . . . . . . 103
</p>
<p>5.12 Example Programs . . . . . . . . . . . . . . . . . . . . . . . . 106</p>
<p/>
</div>
<div class="page"><p/>
<p>Contents ix
</p>
<p>6 Samples 109
</p>
<p>6.1 Random Samples. Distribution
of a Sample. Estimators . . . . . . . . . . . . . . . . . . . . . . 109
</p>
<p>6.2 Samples from Continuous Populations: Mean
and Variance of a Sample . . . . . . . . . . . . . . . . . . . . . 111
</p>
<p>6.3 Graphical Representation of Samples: Histograms
and Scatter Plots . . . . . . . . . . . . . . . . . . . . . . . . . . 115
</p>
<p>6.4 Samples from Partitioned Populations . . . . . . . . . . . . . 122
6.5 Samples Without Replacement from Finite Discrete
</p>
<p>Populations. Mean Square Deviation. Degrees of
Freedom . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
</p>
<p>6.6 Samples from Gaussian Distributions: χ2-Distribution . . . . 130
6.7 χ2 and Empirical Variance . . . . . . . . . . . . . . . . . . . . 135
6.8 Sampling by Counting: Small Samples . . . . . . . . . . . . . 136
6.9 Small Samples with Background . . . . . . . . . . . . . . . . 142
6.10 Determining a Ratio of Small Numbers of Events . . . . . . . 144
6.11 Ratio of Small Numbers of Events with Background . . . . . 147
6.12 Java Classes and Example Programs . . . . . . . . . . . . . . 149
</p>
<p>7 The Method of Maximum Likelihood 153
</p>
<p>7.1 Likelihood Ratio: Likelihood Function . . . . . . . . . . . . . 153
7.2 The Method of Maximum Likelihood . . . . . . . . . . . . . . 155
7.3 Information Inequality. Minimum Variance
</p>
<p>Estimators. Sufficient Estimators . . . . . . . . . . . . . . . . 157
7.4 Asymptotic Properties of the Likelihood Function
</p>
<p>and Maximum-Likelihood Estimators . . . . . . . . . . . . . . 164
7.5 Simultaneous Estimation of Several Parameters:
</p>
<p>Confidence Intervals . . . . . . . . . . . . . . . . . . . . . . . 167
7.6 Example Programs . . . . . . . . . . . . . . . . . . . . . . . . 173
</p>
<p>8 Testing Statistical Hypotheses 175
</p>
<p>8.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175
8.2 F -Test on Equality of Variances . . . . . . . . . . . . . . . . . 177
8.3 Student&rsquo;s Test: Comparison of Means . . . . . . . . . . . . . . 180
8.4 Concepts of the General Theory of Tests . . . . . . . . . . . . 185
8.5 The Neyman&ndash;Pearson Lemma and Applications . . . . . . . . 191
8.6 The Likelihood-Ratio Method . . . . . . . . . . . . . . . . . . 194
8.7 The χ2-Test for Goodness-of-Fit . . . . . . . . . . . . . . . . 199
</p>
<p>8.7.1 χ2-Test with Maximal Number of Degrees
of Freedom . . . . . . . . . . . . . . . . . . . . . . . . 199
</p>
<p>8.7.2 χ2-Test with Reduced Number of Degrees
of Freedom . . . . . . . . . . . . . . . . . . . . . . . . 200
</p>
<p>8.7.3 χ2-Test and Empirical Frequency Distribution . . . . 200</p>
<p/>
</div>
<div class="page"><p/>
<p>x Contents
</p>
<p>8.8 Contingency Tables . . . . . . . . . . . . . . . . . . . . . . . . 203
8.9 2&times;2 Table Test . . . . . . . . . . . . . . . . . . . . . . . . . . 204
8.10 Example Programs . . . . . . . . . . . . . . . . . . . . . . . . 205
</p>
<p>9 The Method of Least Squares 209
</p>
<p>9.1 Direct Measurements of Equal or Unequal Accuracy . . . . . 209
9.2 Indirect Measurements: Linear Case . . . . . . . . . . . . . . 214
9.3 Fitting a Straight Line . . . . . . . . . . . . . . . . . . . . . . . 218
9.4 Algorithms for Fitting Linear Functions
</p>
<p>of the Unknowns . . . . . . . . . . . . . . . . . . . . . . . . . 222
9.4.1 Fitting a Polynomial . . . . . . . . . . . . . . . . . . . 222
9.4.2 Fit of an Arbitrary Linear Function . . . . . . . . . . 224
</p>
<p>9.5 Indirect Measurements: Nonlinear Case . . . . . . . . . . . . 226
9.6 Algorithms for Fitting Nonlinear Functions . . . . . . . . . . 228
</p>
<p>9.6.1 Iteration with Step-Size Reduction . . . . . . . . . . . 229
9.6.2 Marquardt Iteration . . . . . . . . . . . . . . . . . . . 234
</p>
<p>9.7 Properties of the Least-Squares Solution: χ2-Test . . . . . . . 236
9.8 Confidence Regions and Asymmetric Errors
</p>
<p>in the Nonlinear Case . . . . . . . . . . . . . . . . . . . . . . . 240
9.9 Constrained Measurements . . . . . . . . . . . . . . . . . . . . 243
</p>
<p>9.9.1 The Method of Elements . . . . . . . . . . . . . . . . 244
9.9.2 The Method of Lagrange Multipliers . . . . . . . . . 247
</p>
<p>9.10 The General Case of Least-Squares Fitting . . . . . . . . . . . 251
9.11 Algorithm for the General Case of Least Squares . . . . . . . 255
9.12 Applying the Algorithm for the General Case
</p>
<p>to Constrained Measurements . . . . . . . . . . . . . . . . . . 258
9.13 Confidence Region and Asymmetric Errors
</p>
<p>in the General Case . . . . . . . . . . . . . . . . . . . . . . . . 260
9.14 Java Classes and Example Programs . . . . . . . . . . . . . . 261
</p>
<p>10 Function Minimization 267
</p>
<p>10.1 Overview: Numerical Accuracy . . . . . . . . . . . . . . . . . 267
10.2 Parabola Through Three Points . . . . . . . . . . . . . . . . . 273
10.3 Function of n Variables on a Line
</p>
<p>in an n-Dimensional Space . . . . . . . . . . . . . . . . . . . . 275
10.4 Bracketing the Minimum . . . . . . . . . . . . . . . . . . . . . 275
10.5 Minimum Search with the Golden Section . . . . . . . . . . . 277
10.6 Minimum Search with Quadratic Interpolation . . . . . . . . . 280
10.7 Minimization Along a Direction in n Dimensions . . . . . . . 280
10.8 Simplex Minimization in n Dimensions . . . . . . . . . . . . 281
10.9 Minimization Along the Coordinate Directions . . . . . . . . 284
10.10 Conjugate Directions . . . . . . . . . . . . . . . . . . . . . . . 285
10.11 Minimization Along Chosen Directions . . . . . . . . . . . . 287</p>
<p/>
</div>
<div class="page"><p/>
<p>Contents xi
</p>
<p>10.12 Minimization in the Direction of Steepest Descent . . . . . . 288
10.13 Minimization Along Conjugate Gradient Directions . . . . . 288
10.14 Minimization with the Quadratic Form . . . . . . . . . . . . . 292
10.15 Marquardt Minimization . . . . . . . . . . . . . . . . . . . . . 292
10.16 On Choosing a Minimization Method . . . . . . . . . . . . . . 295
10.17 Consideration of Errors . . . . . . . . . . . . . . . . . . . . . . 296
10.18 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 298
10.19 Java Classes and Example Programs . . . . . . . . . . . . . . 303
</p>
<p>11 Analysis of Variance 307
</p>
<p>11.1 One-Way Analysis of Variance . . . . . . . . . . . . . . . . . 307
11.2 Two-Way Analysis of Variance . . . . . . . . . . . . . . . . . 311
11.3 Java Class and Example Programs . . . . . . . . . . . . . . . . 319
</p>
<p>12 Linear and Polynomial Regression 321
</p>
<p>12.1 Orthogonal Polynomials . . . . . . . . . . . . . . . . . . . . . 321
12.2 Regression Curve: Confidence Interval . . . . . . . . . . . . . 325
12.3 Regression with Unknown Errors . . . . . . . . . . . . . . . . 326
12.4 Java Class and Example Programs . . . . . . . . . . . . . . . . 329
</p>
<p>13 Time Series Analysis 331
</p>
<p>13.1 Time Series: Trend . . . . . . . . . . . . . . . . . . . . . . . . 331
13.2 Moving Averages . . . . . . . . . . . . . . . . . . . . . . . . . 332
13.3 Edge Effects . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336
13.4 Confidence Intervals . . . . . . . . . . . . . . . . . . . . . . . 336
13.5 Java Class and Example Programs . . . . . . . . . . . . . . . . 340
</p>
<p>Literature 341
</p>
<p>A Matrix Calculations 347
</p>
<p>A.1 Definitions: Simple Operations . . . . . . . . . . . . . . . . . 348
A.2 Vector Space, Subspace, Rank of a Matrix . . . . . . . . . . . 351
A.3 Orthogonal Transformations . . . . . . . . . . . . . . . . . . . 353
</p>
<p>A.3.1 Givens Transformation . . . . . . . . . . . . . . . . . 354
A.3.2 Householder Transformation . . . . . . . . . . . . . . 356
A.3.3 Sign Inversion . . . . . . . . . . . . . . . . . . . . . . 359
A.3.4 Permutation Transformation . . . . . . . . . . . . . . 359
</p>
<p>A.4 Determinants . . . . . . . . . . . . . . . . . . . . . . . . . . . . 360
A.5 Matrix Equations: Least Squares . . . . . . . . . . . . . . . . 362
A.6 Inverse Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . 365
A.7 Gaussian Elimination . . . . . . . . . . . . . . . . . . . . . . . 367
A.8 LR-Decomposition . . . . . . . . . . . . . . . . . . . . . . . . 369
A.9 Cholesky Decomposition . . . . . . . . . . . . . . . . . . . . . 372</p>
<p/>
</div>
<div class="page"><p/>
<p>xii Contents
</p>
<p>A.10 Pseudo-inverse Matrix . . . . . . . . . . . . . . . . . . . . . . 375
</p>
<p>A.11 Eigenvalues and Eigenvectors . . . . . . . . . . . . . . . . . . 376
</p>
<p>A.12 Singular Value Decomposition . . . . . . . . . . . . . . . . . . 379
</p>
<p>A.13 Singular Value Analysis . . . . . . . . . . . . . . . . . . . . . 380
</p>
<p>A.14 Algorithm for Singular Value Decomposition . . . . . . . . . 385
</p>
<p>A.14.1 Strategy . . . . . . . . . . . . . . . . . . . . . . . . . . 385
</p>
<p>A.14.2 Bidiagonalization . . . . . . . . . . . . . . . . . . . . 386
</p>
<p>A.14.3 Diagonalization . . . . . . . . . . . . . . . . . . . . . 388
</p>
<p>A.14.4 Ordering of the Singular Values and Permutation . . 392
</p>
<p>A.14.5 Singular Value Analysis . . . . . . . . . . . . . . . . . 392
</p>
<p>A.15 Least Squares with Weights . . . . . . . . . . . . . . . . . . . 392
</p>
<p>A.16 Least Squares with Change of Scale . . . . . . . . . . . . . . . 393
</p>
<p>A.17 Modification of Least Squares According to Marquardt . . . . 394
</p>
<p>A.18 Least Squares with Constraints . . . . . . . . . . . . . . . . . 396
</p>
<p>A.19 Java Classes and Example Programs . . . . . . . . . . . . . . 399
</p>
<p>B Combinatorics 405
</p>
<p>C Formulas and Methods for the Computation of Statistical
</p>
<p>Functions 409
</p>
<p>C.1 Binomial Distribution . . . . . . . . . . . . . . . . . . . . . . . 409
</p>
<p>C.2 Hypergeometric Distribution . . . . . . . . . . . . . . . . . . . 409
</p>
<p>C.3 Poisson Distribution . . . . . . . . . . . . . . . . . . . . . . . . 410
</p>
<p>C.4 Normal Distribution . . . . . . . . . . . . . . . . . . . . . . . . 410
</p>
<p>C.5 χ2-Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . 412
</p>
<p>C.6 F -Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . 413
</p>
<p>C.7 t-Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . 413
</p>
<p>C.8 Java Class and Example Program . . . . . . . . . . . . . . . . 414
</p>
<p>D The Gamma Function and Related Functions: Methods
</p>
<p>and Programs for Their Computation 415
</p>
<p>D.1 The Euler Gamma Function . . . . . . . . . . . . . . . . . . . 415
</p>
<p>D.2 Factorial and Binomial Coefficients . . . . . . . . . . . . . . . 418
</p>
<p>D.3 Beta Function . . . . . . . . . . . . . . . . . . . . . . . . . . . 418
</p>
<p>D.4 Computing Continued Fractions . . . . . . . . . . . . . . . . . 418
</p>
<p>D.5 Incomplete Gamma Function . . . . . . . . . . . . . . . . . . 420</p>
<p/>
</div>
<div class="page"><p/>
<p>Contents xiii
</p>
<p>D.6 Incomplete Beta Function . . . . . . . . . . . . . . . . . . . . 420
D.7 Java Class and Example Program . . . . . . . . . . . . . . . . 422
</p>
<p>E Utility Programs 425
</p>
<p>E.1 Numerical Differentiation . . . . . . . . . . . . . . . . . . . . 425
E.2 Numerical Determination of Zeros . . . . . . . . . . . . . . . 427
E.3 Interactive Input and Output Under Java . . . . . . . . . . . . 427
E.4 Java Classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 428
</p>
<p>F The Graphics Class DatanGraphics 431
</p>
<p>F.1 Introductory Remarks . . . . . . . . . . . . . . . . . . . . . . . 431
F.2 Graphical Workstations: Control Routines . . . . . . . . . . . 431
F.3 Coordinate Systems, Transformations
</p>
<p>and Transformation Methods . . . . . . . . . . . . . . . . . . . 432
F.3.1 Coordinate Systems . . . . . . . . . . . . . . . . . . . 432
F.3.2 Linear Transformations: Window &ndash; Viewport . . . . . 433
</p>
<p>F.4 Transformation Methods . . . . . . . . . . . . . . . . . . . . . 435
F.5 Drawing Methods . . . . . . . . . . . . . . . . . . . . . . . . . 436
F.6 Utility Methods . . . . . . . . . . . . . . . . . . . . . . . . . . 439
F.7 Text Within the Plot . . . . . . . . . . . . . . . . . . . . . . . . 441
F.8 Java Classes and Example Programs . . . . . . . . . . . . . . 441
</p>
<p>G Problems, Hints and Solutions, and Programming Problems 447
</p>
<p>G.1 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 447
G.2 Hints and Solutions . . . . . . . . . . . . . . . . . . . . . . . . 456
G.3 Programming Problems . . . . . . . . . . . . . . . . . . . . . . 470
</p>
<p>H Collection of Formulas 487
</p>
<p>I Statistical Tables 503
</p>
<p>List of Computer Programs 515
</p>
<p>Index 517</p>
<p/>
</div>
<div class="page"><p/>
</div>
<div class="page"><p/>
<p>List of Examples
</p>
<p>2.1 Sample space for continuous variables . . . . . . . . . . . . . . . . 7
2.2 Sample space for discrete variables . . . . . . . . . . . . . . . . . . 8
3.1 Discrete random variable . . . . . . . . . . . . . . . . . . . . . . . . 15
3.2 Continuous random variable . . . . . . . . . . . . . . . . . . . . . . 15
3.3 Uniform distribution . . . . . . . . . . . . . . . . . . . . . . . . . . 22
3.4 Cauchy distribution . . . . . . . . . . . . . . . . . . . . . . . . . . 23
3.5 Lorentz (Breit&ndash;Wigner) distribution . . . . . . . . . . . . . . . . . 25
3.6 Error propagation and covariance . . . . . . . . . . . . . . . . . . . 38
4.1 Exponentially distributed random numbers . . . . . . . . . . . . . 57
4.2 Generation of random numbers following a Breit&ndash;Wigner
</p>
<p>distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
4.3 Generation of random numbers with a triangular distribution . . . 58
4.4 Semicircle distribution with the simple acceptance&ndash;rejection
</p>
<p>method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
4.5 Semicircle distribution with the general acceptance&ndash;rejection
</p>
<p>method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
4.6 Computation of π . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
4.7 Simulation of measurement errors of points on a line . . . . . . . 66
4.8 Generation of decay times for a mixture of two different
</p>
<p>radioactive substances . . . . . . . . . . . . . . . . . . . . . . . . . 66
5.1 Statistical error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
5.2 Application of the hypergeometric distribution for
</p>
<p>determination of zoological populations . . . . . . . . . . . . . . . 77
5.3 Poisson distribution and independence of radioactive decays . . . 80
5.4 Poisson distribution and the independence of scientific
</p>
<p>discoveries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
5.5 Addition of two Poisson distributed variables with use of the
</p>
<p>characteristic function . . . . . . . . . . . . . . . . . . . . . . . . . 84
</p>
<p>xv</p>
<p/>
</div>
<div class="page"><p/>
<p>xvi List of Examples
</p>
<p>5.6 Normal distribution as the limiting case of the binomial
distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92
</p>
<p>5.7 Error model of Laplace . . . . . . . . . . . . . . . . . . . . . . . . 92
5.8 Convolution of uniform distributions . . . . . . . . . . . . . . . . . 102
5.9 Convolution of uniform and normal distributions . . . . . . . . . . 104
5.10 Convolution of two normal distributions. &ldquo;Quadratic addition
</p>
<p>of errors&rdquo; . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
5.11 Convolution of exponential and normal distributions . . . . . . . . 105
6.1 Computation of the sample mean and variance from data . . . . . 114
6.2 Histograms of the same sample with various choices
</p>
<p>of bin width . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
6.3 Full width at half maximum (FWHM) . . . . . . . . . . . . . . . . 119
6.4 Investigation of characteristic quantities of samples from a
</p>
<p>Gaussian distribution with the Monte Carlo method . . . . . . . . 119
6.5 Two-dimensional scatter plot: Dividend versus price for
</p>
<p>industrial stocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
6.6 Optimal choice of the sample size for subpopulations . . . . . . . 125
6.7 Determination of a lower limit for the lifetime of the proton
</p>
<p>from the observation of no decays . . . . . . . . . . . . . . . . . . 142
7.1 Likelihood ratio . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154
7.2 Repeated measurements of differing accuracy . . . . . . . . . . . 156
7.3 Estimation of the parameter N of the hypergeometric
</p>
<p>distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
7.4 Estimator for the parameter of the Poisson distribution . . . . . . 162
7.5 Estimator for the parameter of the binomial distribution . . . . . . 163
7.6 Law of error combination (&ldquo;Quadratic averaging of individual
</p>
<p>errors&rdquo;) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
7.7 Determination of the mean lifetime from a small number
</p>
<p>of decays . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166
7.8 Estimation of the mean and variance of a normal distribution . . . 171
7.9 Estimators for the parameters of a two-dimensional normal
</p>
<p>distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172
8.1 F -test of the hypothesis of equal variance of two series of
</p>
<p>measurements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180
8.2 Student&rsquo;s test of the hypothesis of equal means of two series of
</p>
<p>measurements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185
8.3 Test of the hypothesis that a normal distribution with given
</p>
<p>variance σ 2 has the mean λ= λ0 . . . . . . . . . . . . . . . . . . . 189
8.4 Most powerful test for the problem of Example 8.3 . . . . . . . . 193
8.5 Power function for the test from Example 8.3 . . . . . . . . . . . . 195
8.6 Test of the hypothesis that a normal distribution of unknown
</p>
<p>variance has the mean value λ= λ0 . . . . . . . . . . . . . . . . . 197</p>
<p/>
</div>
<div class="page"><p/>
<p>List of Examples xvii
</p>
<p>8.7 χ2-test for the fit of a Poisson distribution to an empirical
frequency distribution . . . . . . . . . . . . . . . . . . . . . . . . . 202
</p>
<p>9.1 Weighted mean of measurements of different accuracy . . . . . . 212
9.2 Fitting of various polynomials . . . . . . . . . . . . . . . . . . . . . 223
9.3 Fitting a proportional relation . . . . . . . . . . . . . . . . . . . . . 224
9.4 Fitting a Gaussian curve . . . . . . . . . . . . . . . . . . . . . . . . 231
9.5 Fit of an exponential function . . . . . . . . . . . . . . . . . . . . . 232
9.6 Fitting a sum of exponential functions . . . . . . . . . . . . . . . . 233
9.7 Fitting a sum of two Gaussian functions and a polynomial . . . . . 235
9.8 The influence of large measurement errors on the confidence
</p>
<p>region of the parameters for fitting an exponential function . . . . 241
9.9 Constraint between the angles of a triangle . . . . . . . . . . . . . 245
9.10 Application of the method of Lagrange multipliers to
</p>
<p>Example 9.9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249
9.11 Fitting a line to points with measurement errors in both the
</p>
<p>abscissa and ordinate . . . . . . . . . . . . . . . . . . . . . . . . . . 257
9.12 Fixing parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . 257
9.13 χ2-test of the description of measured points with errors
</p>
<p>in abscissa and ordinate by a given line . . . . . . . . . . . . . . . 259
9.14 Asymmetric errors and confidence region for fitting a straight
</p>
<p>line to measured points with errors in the abscissa and ordinate . . 260
10.1 Determining the parameters of a distribution from the elements
</p>
<p>of a sample with the method of maximum likelihood . . . . . . . . 298
10.2 Determination of the parameters of a distribution from the his-
</p>
<p>togram of a sample by maximizing the likelihood . . . . . . . . . 299
10.3 Determination of the parameters of a distribution from the his-
</p>
<p>togram of a sample by minimization of a sum of squares . . . . . 302
11.1 One-way analysis of variance of the influence of various drugs . . 310
11.2 Two-way analysis of variance in cancer research . . . . . . . . . . 318
12.1 Treatment of Example 9.2 with Orthogonal Polynomials . . . . . 325
12.2 Confidence limits for linear regression . . . . . . . . . . . . . . . . 327
13.1 Moving average with linear trend . . . . . . . . . . . . . . . . . . . 335
13.2 Time series analysis of the same set of measurements using dif-
</p>
<p>ferent averaging intervals and polynomials of different orders . . 338
A.1 Inversion of a 3&times;3 matrix . . . . . . . . . . . . . . . . . . . . . . 369
A.2 Almost vanishing singular values . . . . . . . . . . . . . . . . . . . 381
A.3 Point of intersection of two almost parallel lines . . . . . . . . . . 381
A.4 Numerical superiority of the singular value decomposition
</p>
<p>compared to the solution of normal equations . . . . . . . . . . . . 384
A.5 Least squares with constraints . . . . . . . . . . . . . . . . . . . . . 398</p>
<p/>
</div>
<div class="page"><p/>
</div>
<div class="page"><p/>
<p>Frequently Used Symbols and Notation
</p>
<p>x, y, ξ , η, . . . (ordinary) variable
</p>
<p>x, y, ξ, η, . . . vector variable
</p>
<p>x, y, . . . random variable
</p>
<p>x, y, . . . vector of random
variables
</p>
<p>A, B, C, . . . matrices
</p>
<p>B bias
</p>
<p>cov(x,y) covariance
</p>
<p>F variance ratio
</p>
<p>f (x) probability density
</p>
<p>F(x) distribution function
</p>
<p>E(x)= x̂ mean value, expectation
value
</p>
<p>H hypothesis
</p>
<p>H0 null hypothesis
</p>
<p>L, ℓ likelihood functions
</p>
<p>L(Sc,λ) operating characteristic
function
</p>
<p>M(Sc,λ) power (of a test)
</p>
<p>M minimum function,
target function
</p>
<p>P(A) probability of the event
A
</p>
<p>Q sum of squares
</p>
<p>s2, s2x sample variance
</p>
<p>S estimator
</p>
<p>Sc critical region
</p>
<p>t variable of Student&rsquo;s
distribution
</p>
<p>T Testfunktion
</p>
<p>xm most probable value
(mode)
</p>
<p>x0.5 median
</p>
<p>xq quantile
</p>
<p>x̄ sample mean
</p>
<p>x̃ estimator from
maximum likelihood or
least squares
</p>
<p>α level of significance
</p>
<p>1&minus;α level of confidence
</p>
<p>λ parameter of a
distribution
</p>
<p>ϕ(t) characteristic function
</p>
<p>xix</p>
<p/>
</div>
<div class="page"><p/>
<p>xx Frequently Used Symbols and Notation
</p>
<p>φ(x), ψ(x) probability density and
distribution function of
the normal distribution
</p>
<p>φ0(x), ψ0(x) probability density and
distribution function of
the standard normal
distribution
</p>
<p>σ (x)=Δ(x) standard deviation
</p>
<p>σ 2(x) variance
</p>
<p>χ2 variable of the χ2
</p>
<p>distribution
</p>
<p>Ω(P ) inverse function of the
normal distribution</p>
<p/>
</div>
<div class="page"><p/>
<p>1. Introduction
</p>
<p>1.1 Typical Problems of Data Analysis
</p>
<p>Every branch of experimental science, after passing through an early stage
of qualitative description, concerns itself with quantitative studies of the phe-
nomena of interest, i.e., measurements. In addition to designing and carrying
out the experiment, an important task is the accurate evaluation and complete
exploitation of the data obtained. Let us list a few typical problems.
</p>
<p>1. A study is made of the weight of laboratory animals under the influence
of various drugs. After the application of drug A to 25 animals, an
average increase of 5% is observed. Drug B, used on 10 animals, yields
a 3% increase. Is drug A more effective? The averages 5 and 3% give
practically no answer to this question, since the lower value may have
been caused by a single animal that lost weight for some unrelated
reason. One must therefore study the distribution of individual weights
and their spread around the average value. Moreover, one has to decide
whether the number of test animals used will enable one to differentiate
with a certain accuracy between the effects of the two drugs.
</p>
<p>2. In experiments on crystal growth it is essential to maintain exactly the
ratios of the different components. From a total of 500 crystals, a sam-
ple of 20 is selected and analyzed. What conclusions can be drawn
about the composition of the remaining 480? This problem of sampling
comes up, for example, in quality control, reliability tests of automatic
measuring devices, and opinion polls.
</p>
<p>3. A certain experimental result has been obtained. It must be decided
whether it is in contradiction with some predicted theoretical value
or with previous experiments. The experiment is used for hypothesis
testing.
</p>
<p>S. Brandt, Data Analysis: Statistical and Computational Methods for Scientists and Engineers,
DOI 10.1007/978-3-319-03762-2__1, &copy; Springer International Publishing Switzerland 2014
</p>
<p>1</p>
<p/>
</div>
<div class="page"><p/>
<p>2 1 Introduction
</p>
<p>4. A general law is known to describe the dependence of measured
variables, but parameters of this law must be obtained from experi-
ment. In radioactive decay, for example, the number N of atoms that
decay per second decreases exponentially with time: N(t) = const &middot;
exp(&minus;λt). One wishes to determine the decay constant λ and its mea-
surement error by making maximal use of a series of measured val-
ues N1(t1), N2(t2), . . .. One is concerned here with the problem of
fitting a function containing unknown parameters to the data and the
determination of the numerical values of the parameters and their
errors.
</p>
<p>From these examples some of the aspects of data analysis become appar-
ent. We see in particular that the outcome of an experiment is not uniquely
determined by the experimental procedure but is also subject to chance: it is a
random variable. This stochastic tendency is either rooted in the nature of the
experiment (test animals are necessarily different, radioactivity is a stochastic
phenomenon), or it is a consequence of the inevitable uncertainties of the ex-
perimental equipment, i.e., measurement errors. It is often useful to simulate
with a computer the variable or stochastic characteristics of the experiment in
order to get an idea of the expected uncertainties of the results before carrying
out the experiment itself. This simulation of random quantities on a computer
is called the Monte Carlo method, so named in reference to games of chance.
</p>
<p>1.2 On the Structure of this Book
</p>
<p>The basis for using random quantities is the calculus of probabilities. The
most important concepts and rules for this are collected in Chap. 2. Random
variables are introduced in Chap. 3. Here one considers distributions of ran-
dom variables, and parameters are defined to characterize the distributions,
such as the expectation value and variance. Special attention is given to the
interdependence of several random variables. In addition, transformations be-
tween different sets of variables are considered; this forms the basis of error
propagation.
</p>
<p>Generating random numbers on a computer and the Monte Carlo method
are the topics of Chap. 4. In addition to methods for generating random
numbers, a well-tested program and also examples for generating arbitrarily
distributed random numbers are given. Use of the Monte Carlo method for
problems of integration and simulation is introduced by means of examples.
The method is also used to generate simulated data with measurement errors,
with which the data analysis routines of later chapters can be demonstrated.</p>
<p/>
</div>
<div class="page"><p/>
<p>1.2 On the Structure of this Book 3
</p>
<p>In Chap. 5 we introduce a number of distributions which are of particular
interest in applications. This applies especially to the Gaussian or normal
distribution, whose properties are studied in detail.
</p>
<p>In practice a distribution must be determined from a finite number of
observations, i.e., from a sample. Various cases of sampling are considered in
Chap. 6. Computer programs are presented for a first rough numerical treat-
ment and graphical display of empirical data. Functions of the sample, i.e.,
of the individual observations, can be used to estimate the parameters charac-
terizing the distribution. The requirements that a good estimate should satisfy
are derived. At this stage the quantity χ2 is introduced. This is the sum of
the squares of the deviations between observed and expected values and is
therefore a suitable indicator of the goodness-of-fit.
</p>
<p>The maximum-likelihood method, discussed in Chap. 7, forms the core of
modern statistical analysis. It allows one to construct estimators with optimum
properties. The method is discussed for the single and multiparameter cases
and illustrated in a number of examples. Chapter 8 is devoted to hypothesis
testing. It contains the most commonly used F , t , and χ2 tests and in addition
outlines the general points of test theory.
</p>
<p>The method of least squares, which is perhaps the most widely used
statistical procedure, is the subject of Chap. 9. The special cases of direct,
indirect, and constrained measurements, often encountered in applications,
are developed in detail before the general case is discussed. Programs and
examples are given for all cases. Every least-squares problem, and in general
every problem of maximum likelihood, involves determining the minimum of
a function of several variables. In Chap. 10 various methods are discussed
in detail, by which such a minimization can be carried out. The relative
efficiency of the procedures is shown by means of programs and examples.
</p>
<p>The analysis of variance (Chap. 11) can be considered as an extension
of the F -test. It is widely used in biological and medical research to study
the dependence, or rather to test the independence, of a measured quan-
tity from various experimental conditions expressed by other variables. For
several variables rather complex situations can arise. Some simple numerical
examples are calculated using a computer program.
</p>
<p>Linear and polynomial regression, the subject of Chap. 12, is a special
case of the least-squares method and has therefore already been treated in
Chap. 9. Before the advent of computers, usually only linear least-squares
problems were tractable. A special terminology, still used, was developed for
this case. It seemed therefore justified to devote a special chapter to this sub-
ject. At the same time it extends the treatment of Chap. 9. For example the
determination of confidence intervals for a solution and the relation between
regression and analysis of variance are studied. A general program for poly-
nomial regression is given and its use is shown in examples.</p>
<p/>
</div>
<div class="page"><p/>
<p>4 1 Introduction
</p>
<p>In the last chapter the elements of time series analysis are introduced.
This method is used if data are given as a function of a controlled variable
(usually time) and no theoretical prediction for the behavior of the data as a
function of the controlled variable is known. It is used to try to reduce the sta-
tistical fluctuation of the data without destroying the genuine dependence on
the controlled variable. Since the computational work in time series analysis
is rather involved, a computer program is also given.
</p>
<p>The field of data analysis, which forms the main part of this book, can
be called applied mathematical statistics. In addition, wide use is made of
other branches of mathematics and of specialized computer techniques. This
material is contained in the appendices.
</p>
<p>In Appendix A, titled &ldquo;Matrix Calculations&rdquo;, the most important
concepts and methods from linear algebra are summarized. Of central impor-
tance are procedures for solving systems of linear equations, in particular the
singular value decomposition, which provides the best numerical properties.
</p>
<p>Necessary concepts and relations of combinatorics are compiled in
Appendix B. The numerical value of functions of mathematical statistics must
often be computed. The necessary formulas and algorithms are contained in
Appendix C. Many of these functions are related to the Euler gamma func-
tion and like it can only be computed with approximation techniques. In
Appendix D formulas and methods for gamma and related functions are given.
Appendix E describes further methods for numerical differentiation, for the
determination of zeros, and for interactive input and output under Java.
</p>
<p>The graphical representation of measured data and their errors and in
many cases also of a fitted function is of special importance in data analysis.
In Appendix F a Java class with a comprehensive set of graphical methods is
presented. The most important concepts of computer graphics are introduced
and all of the necessary explanations for using this class are given.
</p>
<p>Appendix G.1 contains problems to most chapters. These problems can
be solved with paper and pencil. They should help the reader to understand
the basic concepts and theorems. In some cases also simple numerical calcu-
lations must be carried out. In Appendix G.2 either the solution of problems
is sketched or the result is simply given. In Appendix G.3 a number of pro-
gramming problems is presented. For each one an example solution is given.
</p>
<p>The set of appendices is concluded with a collection of formulas in
Appendix H, which should facilitate reference to the most important equa-
tions, and with a short collection of statistical tables in Appendix I. Although
all of the tabulated values can be computed (and in fact were computed) with
the programs of Appendix C, it is easier to look up one or two values from
the tables than to use a computer.</p>
<p/>
</div>
<div class="page"><p/>
<p>1.3 About the Computer Programs 5
</p>
<p>1.3 About the Computer Programs
</p>
<p>For the present edition all programs were newly written in the programming
</p>
<p>language Java. Since some time Java is taught in many schools so that young
</p>
<p>readers often are already familiar with that language. Java classes are directly
</p>
<p>executable on all popular computers &ndash; independently of the operating sys-
</p>
<p>tem. The compilation of Java source programs takes place using the Java De-
</p>
<p>velopment Kit, which for many operating systems, in particular Windows,
</p>
<p>Linux, and Mac OSX, can be downloaded free of cost from the Internet,
</p>
<p>There are four groups of computer programs discussed in this book.
</p>
<p>These are
</p>
<p>&bull; The data analysis library in the form of the package
</p>
<p>&bull; The graphics library in the form of the package
</p>
<p>&bull; A collection of example programs in the package
</p>
<p>&bull; Solutions to the programming problems in the package
</p>
<p>The programs of all groups are available both as compiled classes and
</p>
<p>(except for also as source files. In
</p>
<p>addition there is the extensive Java-typical documentation in html format.
</p>
<p>Every class and method of the package deals with a particular,
</p>
<p>well defined problem, which is extensively described in the text. That also
</p>
<p>holds for the graphics library, which allows to produce practically any type of
</p>
<p>line graphics in two dimensions. For many purposes it suffices, however, to
</p>
<p>use one of 5 classes each yielding a complete graphics.
</p>
<p>In order to solve a specific problem the user has to write a short class
</p>
<p>in Java, which essentially consists of calling classes from the data analysis
</p>
<p>library, and which in certain cases organizes the input of the user&rsquo;s data and
</p>
<p>output of the results. The example programs are a collection of such classes.
</p>
<p>The application of each method from the data analysis and graphics libraries
</p>
<p>is demonstrated in at least one example program. Such example programs are
</p>
<p>described in a special section near the end of most chapters.
</p>
<p>Near the end of the book there is a List of Computer Programs in al-
</p>
<p>phabetic order. For each program from the data analysis library and from the
</p>
<p>graphics library page numbers are given, for an explanation of the program
</p>
<p>itself, and for one or several example programs demonstrating its use.
</p>
<p>The programming problems like the example programs are designed to
</p>
<p>help the reader in using computer methods. Working through these problems
</p>
<p>should enable readers to formulate their own specific tasks in data analysis
</p>
<p>http://www.oracle.com/technetwork/java/index.html
</p>
<p>datan,
</p>
<p>datangraphics,
</p>
<p>examples,
</p>
<p>lutions.so
</p>
<p>datangraphics. tan aphics)Da Gr
</p>
<p>datan</p>
<p/>
</div>
<div class="page"><p/>
<p>6 1 Introduction
</p>
<p>to be solved on a computer. For all programming problems, programs exist
</p>
<p>which represent a possible solution.
</p>
<p>In data analysis, of course, data play a special role. The type of data and
</p>
<p>the format in which they are presented to the computer cannot be defined in
</p>
<p>a general textbook since it depends very much on the particular problem at
</p>
<p>hand. In order to have somewhat realistic data for our examples and problems
</p>
<p>we have decided to produce them in most cases within the program using
</p>
<p>the Monte Carlo method. It is particularly instructive to simulate data with
</p>
<p>known properties and a given error distribution and to subsequently analyze
</p>
<p>these data. In the analysis one must in general make an assumption about the
</p>
<p>distribution of the errors. If this assumption is not correct, then the results
</p>
<p>of the analysis are not optimal. Effects that are often decisively important
</p>
<p>in practice can be &ldquo;experienced&rdquo; with exercises combining simulation and
</p>
<p>analysis.
</p>
<p>Here are some short hints concerning the installation of our pro-
</p>
<p>grams. As material accompanying this book, available from the page
</p>
<p>there is a zip file named DatanJ. Down-
</p>
<p>load this file, unzip it while keeping the internal tree structure of subdirecto-
</p>
<p>ries and store it on your computer in a new directory. (It is convenient to also
</p>
<p>give that directory the name
</p>
<p>extras.springer.com,
</p>
<p>DatanJ.) Further action is described in the file
</p>
<p>ReadME in that directory.</p>
<p/>
</div>
<div class="page"><p/>
<p>2. Probabilities
</p>
<p>2.1 Experiments, Events, Sample Space
</p>
<p>Since in this book we are concerned with the analysis of data originating from
experiments, we will have to state first what we mean by an experiment and
its result. Just as in the laboratory, we define an experiment to be a strictly
followed procedure, as a consequence of which a quantity or a set of quan-
tities is obtained that constitutes the result. These quantities are continuous
(temperature, length, current) or discrete (number of particles, birthday of a
person, one of three possible colors). No matter how accurately all conditions
of the procedure are maintained, the results of repetitions of an experiment
will in general differ. This is caused either by the intrinsic statistical nature of
the phenomenon under investigation or by the finite accuracy of the measure-
ment. The possible results will therefore always be spread over a finite region
for each quantity. All of these regions for all quantities that make up the result
of an experiment constitute the sample space of that experiment. Since it is
difficult and often impossible to determine exactly the accessible regions for
the quantities measured in a particular experiment, the sample space actually
used may be larger and may contain the true sample space as a subspace. We
shall use this somewhat looser concept of a sample space.
</p>
<p>Example 2.1: Sample space for continuous variables
</p>
<p>In the manufacture of resistors it is important to maintain the values R (electri-
cal resistance measured in ohms) and N (maximum heat dissipation measured
in watts) at given values. The sample space for R and N is a plane spanned
by axes labeled R and N . Since both quantities are always positive, the first
quadrant of this plane is itself a sample space.
</p>
<p>S. Brandt, Data Analysis: Statistical and Computational Methods for Scientists and Engineers,
DOI 10.1007/978-3-319-03762-2__2, &copy; Springer International Publishing Switzerland 2014
</p>
<p>7</p>
<p/>
</div>
<div class="page"><p/>
<p>8 2 Probabilities
</p>
<p>Example 2.2: Sample space for discrete variables
</p>
<p>In practice the exact values of R and N are unimportant as long as they are
contained within a certain interval about the nominal value (e.g., 99 kΩ &lt;
R &lt; 101kΩ , 0.49 W &lt;N &lt; 0.60 W). If this is the case, we shall say that the
resistor has the properties Rn, Nn. If the value falls below (above) the lower
(upper) limit, then we shall substitute the index n by &minus;(+). The possible val-
ues of resistance and heat dissipation are therefore R&minus;, Rn, R+, N&minus;, Nn, N+.
The sample space now consists of nine points:
</p>
<p>R&minus;N&minus;, R&minus;Nn, R&minus;N+,
RnN&minus;, RnNn, RnN+,
R+N&minus;, R+Nn, R+N+.
</p>
<p>Often one or more particular subspaces of the sample space are of spe-
cial interest. In Example 2.2, for instance, the point Rn, Nn represents the
case where the resistors meet the production specifications. We can give such
subspaces names, e.g., A,B,. . . and say that if the result of an experiment
falls into one such subspace, then the event A (or B,C,. . .) has occurred. If A
has not occurred, we speak of the complementary event Ā (i.e., not A). The
whole sample space corresponds to an event that will occur in every exper-
iment, which we call E. In the rest of this chapter we shall define what we
mean by the probability of the occurrence of an event and present rules for
computations with probabilities.
</p>
<p>2.2 The Concept of Probability
</p>
<p>Let us consider the simplest experiment, namely, the tossing of a coin. Like
the throwing of dice or certain problems with playing cards it is of no practical
interest but is useful for didactic purposes. What is the probability that a &ldquo;fair&rdquo;
coin shows &ldquo;heads&rdquo; when tossed once? Our intuition suggests that this prob-
ability is equal to 1/2. It is based on the assumption that all points in sample
space (there are only two points: &ldquo;heads&rdquo; and &ldquo;tails&rdquo;) are equally probable and
on the convention that we give the event E (here: &ldquo;heads&rdquo; or &ldquo;tails&rdquo;) a prob-
ability of unity. This way of determining probabilities can be applied only to
symmetric experiments and is therefore of little practical use. (It is, however,
of great importance in statistical physics and quantum statistics, where the
equal probabilities of all allowed states is an essential postulate of very suc-
cessful theories.) If no such perfect symmetry exists&mdash;which will even be the
case with normal &ldquo;physical&rdquo; coins&mdash;the following procedure seems reason-</p>
<p/>
</div>
<div class="page"><p/>
<p>2.2 The Concept of Probability 9
</p>
<p>able. In a large number N of experiments the event A is observed to occur n
times. We define
</p>
<p>P (A)= lim
N&rarr;&infin;
</p>
<p>n
</p>
<p>N
(2.2.1)
</p>
<p>as the probability of the occurrence of the event A. This somewhat loose fre-
quency definition of probability is sufficient for practical purposes, although
it is mathematically unsatisfactory. One of the difficulties with this definition
is the need for an infinity of experiments, which are of course impossible
to perform and even difficult to imagine. Although we shall in fact use the
frequency definition in this book, we will indicate the basic concepts of an
axiomatic theory of probability due to KOLMOGOROV [1]. The minimal set
of axioms generally used is the following:
</p>
<p>(a) To each event A there corresponds a non-negative number, its proba-
bility,
</p>
<p>P (A)&ge; 0 . (2.2.2)
</p>
<p>(b) The event E has unit probability,
</p>
<p>P (E)= 1 . (2.2.3)
</p>
<p>(c) If A and B are mutually exclusive events, then the probability of A or
B (written A+B) is
</p>
<p>P (A+B)= P (A)+P (B) . (2.2.4)
</p>
<p>From these axioms&lowast; one obtains immediately the following useful results.
From (b) and (c):
</p>
<p>P (Ā+A)= P (A)+P (Ā)= 1 , (2.2.5)
</p>
<p>and furthermore with (a):
</p>
<p>0 &le; P (A)&le; 1 . (2.2.6)
</p>
<p>From (c) one can easily obtain the more general theorem for mutually exclu-
sive events A, B, C, . . . ,
</p>
<p>P (A+B+C+&middot;&middot; &middot;)= P (A)+P (B)+P (C)+&middot;&middot; &middot; . (2.2.7)
</p>
<p>It should be noted that summing the probabilities of events combined with
&ldquo;or&rdquo; here refers only to mutually exclusive events. If one must deal with events
that are not of this type, then they must first be decomposed into mutually
exclusive ones. In throwing a die, A may signify even, B odd, C less than
4 dots, D 4 or more dots. Suppose one is interested in the probability for the
</p>
<p>&lowast;Sometimes the definition (2.3.1) is included as a fourth axiom.</p>
<p/>
</div>
<div class="page"><p/>
<p>10 2 Probabilities
</p>
<p>event A or C, which are obviously not exclusive. One forms A and C (written
AC) as well as AD, BC, and BD, which are mutually exclusive, and finds for
A or C (sometimes written A +̇C) the expression AC+AD+BC. Note that
the axioms do not prescribe a method for assigning the value of a particular
probability P (A).
</p>
<p>Finally it should be pointed out that the word probability is often used in
common language in a sense that is different or even opposed to that consid-
ered by us. This is subjective probability, where the probability of an event is
given by the measure of our belief in its occurrence. An example of this is:
&ldquo;The probability that the party A will win the next election is 1/3.&rdquo; As another
example consider the case of a certain track in nuclear emulsion which could
have been left by a proton or pion. One often says: &ldquo;The track was caused by
a pion with probability 1/2.&rdquo; But since the event had already taken place and
only one of the two kinds of particle could have caused that particular track,
the probability in question is either 0 or 1, but we do not know which.
</p>
<p>2.3 Rules of Probability Calculus: Conditional Probability
</p>
<p>Suppose the result of an experiment has the property A. We now ask for the
probability that it also has the property B, i.e., the probability of B under the
condition A. We define this conditional probability as
</p>
<p>P (B|A)= P (AB)
P (A)
</p>
<p>. (2.3.1)
</p>
<p>It follows that
P (AB)= P (A)P (B|A) . (2.3.2)
</p>
<p>One can also use (2.3.2) directly for the definition, since here the requirement
P (A) �= 0 is not necessary. From Fig. 2.1 it can be seen that this definition is
reasonable. Consider the event A to occur if a point is in the region labeled
A, and correspondingly for the event (and region) B. For the overlap region
both A and B occur, i.e., the event (AB) occurs. Let the area of the different
regions be proportional to the probabilities of the corresponding events. Then
the probability of B under the condition A is the ratio of the area AB to that
of A. In particular this is equal to unity if A is contained in B and zero if the
overlapping area vanishes.
</p>
<p>Using conditional probability we can now formulate the rule of total
probability. Consider an experiment that can lead to one of n possible mu-
tually exclusive events,
</p>
<p>E =A1 +A2+&middot;&middot; &middot;+An . (2.3.3)</p>
<p/>
</div>
<div class="page"><p/>
<p>2.4 Examples 11
</p>
<p>The probability for the occurrence of any event with the property B is
</p>
<p>P (B)=
n&sum;
</p>
<p>i=1
P (Ai)P (B|Ai) , (2.3.4)
</p>
<p>as can be seen easily from (2.3.2) and (2.2.7).
</p>
<p>AB
A
</p>
<p>B
Fig.2.1: Illustration of conditional probability.
</p>
<p>We can now also define the independence of events. Two events A and
B are said to be independent if the knowledge that A has occurred does not
change the probability for B and vice versa, i.e., if
</p>
<p>P (B|A)= P (B) , (2.3.5)
</p>
<p>or, by use of (2.3.2),
P (AB)= P (A)P (B) . (2.3.6)
</p>
<p>In general several decompositions of the type (2.3.3),
</p>
<p>E = A1 +A2+&middot;&middot; &middot;+An ,
E = B1 +B2+&middot;&middot; &middot;+Bm , (2.3.7)
...
</p>
<p>E = Z1 +Z2 +&middot;&middot; &middot;+Zℓ ,
</p>
<p>are said to be independent, if for all possible combinations α,β, . . . ,ω the
condition
</p>
<p>P (AαBβ &middot; &middot; &middot;Zω)= P (Aα)P (Bβ) &middot; &middot; &middot;P (Zω) (2.3.8)
is fulfilled.
</p>
<p>2.4 Examples
</p>
<p>2.4.1 Probability for n Dots in the Throwing of Two Dice
</p>
<p>If n1 and n2 are the number of dots on the individual dice and if n= n1 +n2,
then one has P (ni) = 1/6; i = 1,2; ni = 1,2, . . . ,6. Because the two dice
are independent of each other one has P (n1,n2) = P (n1)P (n2) = 1/36. By</p>
<p/>
</div>
<div class="page"><p/>
<p>12 2 Probabilities
</p>
<p>considering in how many different ways the sum n = ni +nj can be formed
one obtains
</p>
<p>P2(2) = P (1,1)= 1/36 ,
P2(3) = P (1,2)+P (2,1)= 2/36 ,
P2(4) = P (1,3)+P (2,2)+P (3,1)= 3/36 ,
P2(5) = P (1,4)+P (2,3)+P (3,2)+P (4,1)= 4/36 ,
P2(6) = P (1,5)+P (2,4)+P (3,3)+P (4,2)
</p>
<p>+P (5,1)= 5/36 ,
P2(7) = P (1,6)+P (2,5)+P (3,4)+P (4,3)
</p>
<p>+P (5,2)+P (6,1)= 6/36 ,
P2(8) = P2(6)= 5/36 ,
P2(9) = P2(5)= 4/36 ,
</p>
<p>P2(10) = P2(4)= 3/36 ,
P2(11) = P2(3)= 2/36 ,
P2(12) = P2(2)= 1/36 .
</p>
<p>Of course, the normalization condition
&sum;12
</p>
<p>k=2P2(k)= 1 is fulfilled.
</p>
<p>2.4.2 Lottery 6 Out of 49
</p>
<p>A container holds 49 balls numbered 1 through 49. During the drawing 6
balls are taken out of the container consecutively and none are put back in.
We compute the probabilities P (1), P (2), . . ., P (6) that a player, who before
the drawing has chosen six of the numbers 1, 2, . . ., 49, has predicted exactly
1, 2, . . ., or 6 of the drawn numbers.
</p>
<p>First we compute P (6). The probability to choose as the first number
the one which will also be drawn first is obviously 1/49. If that step was
successful, then the probability to choose as the second number the one which
is also drawn second is 1/48. We conclude that the probability for choosing
six numbers correctly in the order in which they are drawn is
</p>
<p>1
</p>
<p>49 &middot;48 &middot;47 &middot;46 &middot;45 &middot;44 =
43!
49! .
</p>
<p>The order, however, is irrelevant. Since there are 6! possible ways to arrange
six numbers in different orders we have
</p>
<p>P (6)= 6!43!
49! =
</p>
<p>1
(49
</p>
<p>6
</p>
<p>) =
1
</p>
<p>C496
</p>
<p>.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.4 Examples 13
</p>
<p>That is exactly the inverse of the number of combinations C496 of 6 elements
out of 49 (see Appendix B), since all of these combinations are equally prob-
able but only one of them contains only the drawn numbers.
</p>
<p>We may now argue that the container holds two kinds of balls, namely 6
balls in which the player is interested since they carry the numbers which he
selected, and 43 balls whose numbers the player did not select. The result of
the drawing is a sample from a set of 49 elements of which 6 are of one kind
and 43 are of the other. The sample itself contains 6 elements which are drawn
without putting elements back into the container. This method of sampling is
described by the hypergeometric distribution (see Sect. 5.3). The probability
for predicting correctly ℓ out of the 6 drawn numbers is
</p>
<p>P (ℓ)=
(6
ℓ
</p>
<p>)( 43
6&minus;ℓ
</p>
<p>)
(49
</p>
<p>6
</p>
<p>) , ℓ= 0, . . . ,6 .
</p>
<p>2.4.3 Three-Door Game
</p>
<p>In a TV game show a candidate is given the following problem. Three rooms
are closed by three identical doors. One room contains a luxury car, the other
two each contain a goat. The candidate is asked to guess behind which of
the doors the car is. He chooses a door which we will call A. The door A,
however, remains closed for the moment. Of course, behind at least one of the
other doors there is a goat. The quiz master now opens one door which we
will call B to reveal a goat. He now gives the candidate the chance to either
stay with the original choice A or to choose remaining closed door C. Can the
candidate increase his or her chances by choosing C instead of A?
</p>
<p>The answer (astonishing for many) is yes. The probability to find the car
behind the door A obviously is P (A)= 1/3. Then the probability that the car
is behind one of the other doors is P (Ā) = 2/3. The candidate exhausts this
probability fully if he chooses the door C since through the opening of B it is
shown to be a door without the car, so that P (C)= P (Ā).</p>
<p/>
</div>
<div class="page"><p/>
<p>3. Random Variables: Distributions
</p>
<p>3.1 Random Variables
</p>
<p>We will now consider not the probability of observing particular events but
rather the events themselves and try to find a particularly simple way of clas-
sifying them. We can, for instance, associate the event &ldquo;heads&rdquo; with the num-
ber 0 and the event &ldquo;tails&rdquo; with the number 1. Generally we can classify the
events of the decomposition (2.3.3) by associating each event Ai with the real
number i. In this way each event can be characterized by one of the possible
values of a random variable. Random variables can be discrete or continuous.
We denote them by symbols like x, y, . . ..
</p>
<p>Example 3.1: Discrete random variable
</p>
<p>It may be of interest to study the number of coins still in circulation as a
function of their age. It is obviously most convenient to use the year of issue
stamped on each coin directly as the (discrete) random variable, e.g., x = . . .,
1949, 1950, 1951, . . ..
</p>
<p>Example 3.2: Continuous random variable
</p>
<p>All processes of measurement or production are subject to smaller or larger
imperfections or fluctuations that lead to variations in the result, which is
therefore described by one or several random variables. Thus the values of
electrical resistance and maximum heat dissipation characterizing a resistor
in Example 2.1 are continuous random variables.
</p>
<p>3.2 Distributions of a Single Random Variable
</p>
<p>From the classification of events we return to probability considerations. We
consider the random variable x and a real number x, which can assume any
value between &minus;&infin; and +&infin;, and study the probability for the event x &lt; x.
S. Brandt, Data Analysis: Statistical and Computational Methods for Scientists and Engineers,
DOI 10.1007/978-3-319-03762-2__3, &copy; Springer International Publishing Switzerland 2014
</p>
<p>15</p>
<p/>
</div>
<div class="page"><p/>
<p>16 3 Random Variables: Distributions
</p>
<p>This probability is a function of x and is called the (cumulative) distribution
function of x:
</p>
<p>F(x)= P (x &lt; x) . (3.2.1)
If x can assume only a finite number of discrete values, e.g., the number of
dots on the faces of a die, then the distribution function is a step function. It
is shown in Fig. 3.1 for the example mentioned above. Obviously distribution
functions are always monotonic and non-decreasing.
</p>
<p>x
</p>
<p>0.5
</p>
<p>1
</p>
<p>F(x)
</p>
<p>0 1 2 3 4 5 6
</p>
<p>Fig.3.1: Distribution function for throwing
of a symmetric die.
</p>
<p>Because of (2.2.3) one has the limiting case
</p>
<p>lim
x&rarr;&infin;
</p>
<p>F(x)= lim
x&rarr;&infin;
</p>
<p>P (x &lt; x)= P (E)= 1 . (3.2.2)
</p>
<p>Applying Eqs. (2.2.5)&ndash;(3.2.1) we obtain
</p>
<p>P (x &ge; x)= 1&minus;F(x)= 1&minus;P (x &lt; x) (3.2.3)
</p>
<p>and therefore
</p>
<p>lim
x&rarr;&minus;&infin;
</p>
<p>F(x)= lim
x&rarr;&minus;&infin;
</p>
<p>P (x &lt; x)= 1&minus; lim
x&rarr;&minus;&infin;
</p>
<p>P (x &ge; x)= 0 . (3.2.4)
</p>
<p>Of special interest are distribution functions F(x) that are continuous and
differentiable. The first derivative
</p>
<p>f (x)= dF(x)
dx
</p>
<p>= F &prime;(x) (3.2.5)
</p>
<p>is called the probability density (function) of x. It is a measure of the proba-
bility of the event (x &le; x &lt; x+ dx). From (3.2.1) and (3.2.5) it immediately
follows that
</p>
<p>P (x &lt; a)= F(a)=
&int; a
</p>
<p>&minus;&infin;
f (x)dx , (3.2.6)
</p>
<p>P (a &le; x &lt; b)=
&int; b
</p>
<p>a
</p>
<p>f (x)dx = F(b)&minus;F(a) , (3.2.7)</p>
<p/>
</div>
<div class="page"><p/>
<p>3.3 Functions of a Single Random Variable 17
</p>
<p>and in particular &int; &infin;
</p>
<p>&minus;&infin;
f (x)dx = 1 . (3.2.8)
</p>
<p>A trivial example of a continuous distribution is given by the angular
position of the hand of a watch read at random intervals. We obtain a constant
probability density (Fig. 3.2).
</p>
<p>360&deg;
</p>
<p>360&deg;
</p>
<p>x
</p>
<p>x
</p>
<p>0
</p>
<p>1
</p>
<p>0
</p>
<p>F(x)
</p>
<p>f(x)
</p>
<p>0
</p>
<p>360
1
</p>
<p>0
</p>
<p>Fig.3.2: Distribution function and probabil-
ity density for the angular position of a watch
hand.
</p>
<p>3.3 Functions of a Single Random Variable,
</p>
<p>Expectation Value, Variance, Moments
</p>
<p>In addition to the distribution of a random variable x, we are often interested
in the distribution of a function of x. Such a function of a random variable is
also a random variable:
</p>
<p>y =H(x) . (3.3.1)
</p>
<p>The variable y then possesses a distribution function and probability density
in the same way as x.
</p>
<p>In the two simple examples of the last section we were able to give the dis-
tribution function immediately because of the symmetric nature of the prob-
lems. Usually this is not possible. Instead, we have to obtain it from exper-
iment. Often we are limited to determining a few characteristic parameters
instead of the complete distribution.
</p>
<p>The mean or expectation value of a random variable is the sum of all
possible values xi of x multiplied by their corresponding probabilities
</p>
<p>E(x)= x̂ =
n&sum;
</p>
<p>i=1
xiP (x = xi) . (3.3.2)</p>
<p/>
</div>
<div class="page"><p/>
<p>18 3 Random Variables: Distributions
</p>
<p>Note that x̂ is not a random variable but rather has a fixed value. Correspond-
ingly the expectation value of a function (3.3.1) is defined to be
</p>
<p>E{H(x)} =
n&sum;
</p>
<p>i=1
H(xi)P (x = xi) . (3.3.3)
</p>
<p>In the case of a continuous random variable (with a differentiable distribution
function), we define by analogy
</p>
<p>E(x)= x̂ =
&int; &infin;
</p>
<p>&minus;&infin;
xf (x)dx (3.3.4)
</p>
<p>and
</p>
<p>E{H(x)} =
&int; &infin;
</p>
<p>&minus;&infin;
H(x)f (x)dx . (3.3.5)
</p>
<p>If we choose in particular
</p>
<p>H(x)= (x&minus; c)ℓ , (3.3.6)
</p>
<p>we obtain the expectation values
</p>
<p>αℓ = E{(x&minus; c)ℓ} , (3.3.7)
</p>
<p>which are called the ℓ&minus;th moments of the variable about the point c. Of special
interest are the moments about the mean,
</p>
<p>μℓ = E{(x&minus; x̂)ℓ} . (3.3.8)
</p>
<p>The lowest moments are obviously
</p>
<p>μ0 = 1 , μ1 = 0 . (3.3.9)
</p>
<p>The quantity
μ2 = σ 2(x)= var(x)= E{(x&minus; x̂)2} (3.3.10)
</p>
<p>is the lowest moment containing information about the average deviation of
the variable x from its mean. It is called the variance of x.
</p>
<p>We will now try to visualize the practical meaning of the expectation
value and variance of a random variable x. Let us consider the measure-
ment of some quantity, for example, the length x0 of a small crystal using
a microscope. Because of the influence of different factors, such as the im-
perfections of the different components of the microscope and observational
errors, repetitions of the measurement will yield slightly different results for
x. The individual measurements will, however, tend to group themselves in
the neighborhood of the true value of the length to be measured, i.e., it will</p>
<p/>
</div>
<div class="page"><p/>
<p>3.3 Functions of a Single Random Variable 19
</p>
<p>be more probable to find a value of x near to x0 than far from it, providing
no systematic biases exist. The probability density of x will therefore have a
bell-shaped form as sketched in Fig. 3.3, although it need not be symmetric. It
seems reasonable &ndash; especially in the case of a symmetric probability density &ndash;
to interpret the expectation value (3.3.4) as the best estimate of the true value.
It is interesting to note that (3.3.4) has the mathematical form of a center of
gravity, i.e., x̂ can be visualized as the x-coordinate of the center of gravity of
the surface under the curve describing the probability density.
</p>
<p>The variance (3.3.10),
</p>
<p>σ 2(x)=
&int; &infin;
</p>
<p>&minus;&infin;
(x&minus; x̂)2f (x)dx , (3.3.11)
</p>
<p>x
x x^ ^
</p>
<p>(b)
</p>
<p>(a)
</p>
<p>f(x)
</p>
<p>Fig.3.3: Distribution with small variance
(a) and large variance (b).
</p>
<p>which has the form of a moment of inertia, is a measure of the width or dis-
persion of the probability density about the mean. If it is small, the individual
measurements lie close to x̂ (Fig. 3.3a); if it is large, they will in general be
further from the mean (Fig. 3.3b). The positive square root of the variance
</p>
<p>σ =
&radic;
σ 2(x) (3.3.12)
</p>
<p>is called the standard deviation (or sometimes the dispersion) of x. Like the
variance itself it is a measure of the average deviation of the measurements x
from the expectation value.
</p>
<p>Since the standard deviation has the same dimension as x (in our exam-
ple both have the dimension of length), it is identified with the error of the
measurement,</p>
<p/>
</div>
<div class="page"><p/>
<p>20 3 Random Variables: Distributions
</p>
<p>σ(x)=Δx .
</p>
<p>This definition of measurement error is discussed in more detail in Sects. 5.6 &ndash;
5.10. It should be noted that the definitions (3.3.4) and (3.3.10) do not provide
completely a way of calculating the mean or the measurement error, since the
probability density describing a measurement is in general unknown.
</p>
<p>The third moment about the mean is sometimes called skewness. We pre-
fer to define the dimensionless quantity
</p>
<p>γ = μ3/σ 3 (3.3.13)
</p>
<p>to be the skewness of x. It is positive (negative) if the distribution is skew
to the right (left) of the mean. For symmetric distributions the skewness van-
ishes. It contains information about a possible difference between positive and
negative deviation from the mean.
</p>
<p>We will now obtain a few important rules about means and variances. In
the case where
</p>
<p>H(x)= cx , c = const , (3.3.14)
it follows immediately that
</p>
<p>E(cx) = cE(x) ,
σ 2(cx) = c2σ 2(x) , (3.3.15)
</p>
<p>and therefore
</p>
<p>σ 2(x)= E{(x&minus; x̂)2} = E{x2&minus;2xx̂+ x̂2} = E(x2)&minus; x̂2 . (3.3.16)
</p>
<p>We now consider the function
</p>
<p>u = x&minus; x̂
σ (x)
</p>
<p>. (3.3.17)
</p>
<p>It has the expectation value
</p>
<p>E(u)= 1
σ(x)
</p>
<p>E(x&minus; x̂)= 1
σ(x)
</p>
<p>(̂x&minus; x̂)= 0 (3.3.18)
</p>
<p>and variance
</p>
<p>σ 2(u)= 1
σ 2(x)
</p>
<p>E{(x&minus; x̂)2} = σ
2(x)
</p>
<p>σ 2(x)
= 1 . (3.3.19)
</p>
<p>The function u &ndash; which is also a random variable &ndash; has particularly simple
properties, which makes its use in more involved calculations preferable. We
will call such a variable (having zero mean and unit variance) a reduced vari-
able. It is also called a standardized, normalized, or dimensionless variable.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.3 Functions of a Single Random Variable 21
</p>
<p>Although a distribution is mathematically most easily described by its ex-
pectation value, variance, and higher moments (in fact any distribution can be
completely specified by these quantities, cf. Sect. 5.5), it is often convenient to
introduce further definitions so as to better visualize the form of a distribution.
</p>
<p>The mode xm (or most probable value) of a distribution is defined as that
value of the random variable that corresponds to the highest probability:
</p>
<p>P (x = xm)= max . (3.3.20)
</p>
<p>If the distribution has a differentiable probability density, the mode, which
corresponds to its maximum, is easily determined by the conditions
</p>
<p>d
</p>
<p>dx
f (x)= 0 , d
</p>
<p>2
</p>
<p>dx2
f (x) &lt; 0 . (3.3.21)
</p>
<p>In many cases only one maximum exists; the distribution is said to be uni-
modal. The median x0.5 of a distribution is defined as that value of the random
variable for which the distribution function equals 1/2:
</p>
<p>F(x0.5)= P (x &lt; x0.5)= 0.5 . (3.3.22)
</p>
<p>In the case of a continuous probability density Eq. (3.3.22) takes the form
&int; x0.5
&minus;&infin;
</p>
<p>f (x)dx = 0.5 , (3.3.23)
</p>
<p>i.e., the median divides the total range of the random variable into two regions
each containing equal probability.
</p>
<p>It is clear from these definitions that in the case of a unimodal distribution
with continuous probability density that is symmetric about its maximum, the
values of mean, mode, and median coincide. This is not, however, the case for
asymmetric distributions (Fig. 3.4).
</p>
<p>f(x)
</p>
<p>x0.5 xxm x̂
</p>
<p>Fig.3.4: Most probable value
(mode) xm, mean x̂, and me-
dian x0.5 of an asymmetric
distribution.
</p>
<p>The definition (3.3.22) can easily be generalized. The quantities x0.25 and
x0.75 defined by
</p>
<p>F(x0.25)= 0.25 , F (x0.75)= 0.75 (3.3.24)</p>
<p/>
</div>
<div class="page"><p/>
<p>22 3 Random Variables: Distributions
</p>
<p>are called lower and upper quartiles. Similarly we can define deciles x0.1, x0.2,
. . ., x0.9, or in general quantiles xq , by
</p>
<p>F(xq)=
&int; xq
&minus;&infin;
</p>
<p>f (x)dx = q (3.3.25)
</p>
<p>with 0 &le; q &le; 1.
The definition of quantiles is most easily visualized from Fig. 3.5. In a
</p>
<p>plot of the distribution function F(x), the quantile xq can be read off as the
abscissa corresponding to the value q on the ordinate. The quantile xq(q),
regarded as a function of the probability q, is simply the inverse of the distri-
bution function.
</p>
<p>F(x)
</p>
<p>x
0
</p>
<p>0.2
</p>
<p>1.0
</p>
<p>0.8
</p>
<p>0.6
</p>
<p>0.4
</p>
<p>x0.9x0.5x0.2
</p>
<p>Fig.3.5: Median and quantiles
of a continuous distribution.
</p>
<p>Example 3.3: Uniform distribution
</p>
<p>We will now discuss the simplest case of a distribution function of a continu-
ous variable. Suppose that in the interval a &le; x &lt; b the probability density of
x is constant, and it is zero outside of this interval:
</p>
<p>f (x)= c , a &le; x &lt; b ,
f (x)= 0 , x &lt; a , x &ge; b . (3.3.26)
</p>
<p>Because of (3.2.8) one has
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
f (x)dx = c
</p>
<p>&int; b
</p>
<p>a
</p>
<p>dx = c(b&minus;a)= 1
</p>
<p>or
f (x)= 1
</p>
<p>b&minus;a , a &le; x &lt; b ,
</p>
<p>f (x)= 0 , x &lt; a , x &ge; b .
(3.3.27)</p>
<p/>
</div>
<div class="page"><p/>
<p>3.3 Functions of a Single Random Variable 23
</p>
<p>The distribution function is
</p>
<p>F(x)=
&int; x
</p>
<p>a
</p>
<p>dx
b&minus;a =
</p>
<p>x&minus;a
b&minus;a , a &le; x &lt; b ,
</p>
<p>F (x)= 0 , x &lt; a ,
F (x)= 1 , x &ge; b .
</p>
<p>(3.3.28)
</p>
<p>By symmetry arguments the expectation value of x must be the arithmetic
mean of the boundaries a and b. In fact, (3.3.4) immediately gives
</p>
<p>E(x)= x̂ = 1
b&minus;a
</p>
<p>&int; b
</p>
<p>a
</p>
<p>x dx = 1
2
</p>
<p>1
</p>
<p>(b&minus;a)(b
2 &minus;a2)= b+a
</p>
<p>2
. (3.3.29)
</p>
<p>Correspondingly, one obtains from (3.3.10)
</p>
<p>σ 2(x)= 1
12
</p>
<p>(b&minus;a)2 . (3.3.30)
</p>
<p>The uniform distribution is not of great practical interest. It is, how-
ever, particularly easy to handle, being the simplest distribution of a contin-
uous variable. It is often advantageous to transform a distribution function
by means of a transformation of variables into a uniform distribution or the
reverse, to express the given distribution in terms of a uniform distribution.
This method is used particularly in the &ldquo;Monte Carlo method&rdquo; discussed in
Chap. 4.
</p>
<p>Example 3.4: Cauchy distribution
</p>
<p>In the (x,y) plane a gun is mounted at the point (x,y)= (0,&minus;1) such that its
barrel lies in the (x,y) plane and can rotate around an axis parallel to the z
axis (Fig. 3.6).
</p>
<p>The gun is fired such that the angle θ between the barrel and the y axis
is chosen at random from uniform distribution in the range &minus;π/2 &le; θ &lt; π/2,
i.e., the probability density of θ is
</p>
<p>f (θ)= 1
π
</p>
<p>.
</p>
<p>Since
</p>
<p>θ = arctanx , dθ
dx
</p>
<p>= 1
1+x2 ,
</p>
<p>we find by the transformation (cf. Sect. 3.7) θ &rarr; x of the variable for the
probability density in x
</p>
<p>g(x)=
∣∣∣∣
dθ
</p>
<p>dx
</p>
<p>∣∣∣∣f (θ)=
1
</p>
<p>π
</p>
<p>1
</p>
<p>1+x2 . (3.3.31)</p>
<p/>
</div>
<div class="page"><p/>
<p>24 3 Random Variables: Distributions
</p>
<p>Fig.3.6: Model for producing a Cauchy distribution (below) and probability density of the
Cauchy distribution (above).
</p>
<p>A distribution with this probability density (in our example of the position of
hits on the x axis) is called the Cauchy distribution.
</p>
<p>The expectation value of x is (taking the principal value for the integral)
</p>
<p>x̂ = 1
π
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>x dx
</p>
<p>1+x2 = 0 .
</p>
<p>The expression for the variance,
&int; &infin;
</p>
<p>&minus;&infin;
x2g(x)dx = 1
</p>
<p>π
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>x2 dx
</p>
<p>1+x2 =
1
</p>
<p>π
(x&minus; arctanx)
</p>
<p>∣∣∣∣
x=&infin;
</p>
<p>x=&minus;&infin;
</p>
<p>= 2
π
</p>
<p>lim
x&rarr;&infin;
</p>
<p>(x&minus; arctanx) ,
</p>
<p>yields an infinite result. One says that the variance of the Cauchy distribution
does not exist.
</p>
<p>One can, however, construct another measure for the width of the dis-
tribution, the full width at half maximum &lsquo;FWHM&rsquo; (cf. Example 6.3). The
function g(x) has its maximum at x = x̂ = 0 and reaches half its maximum
value at the points xa =&minus;1 and xℓ = 1. Therefore,
</p>
<p>Γ = 2
is the full width at half maximum of the Cauchy distribution.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.4 Two Variables. Conditional Probability 25
</p>
<p>Example 3.5: Lorentz (Breit&ndash;Wigner) distribution
</p>
<p>With x̂ = a = 0 and Γ = 2 we can write the probability density (3.3.31) of
the Cauchy distribution in the form
</p>
<p>g(x)= 2
πΓ
</p>
<p>Γ 2
</p>
<p>4(x&minus;a)2 +Γ 2 . (3.3.32)
</p>
<p>This function is a normalized probability density for all values of a and full
width at half maximum Γ &gt; 0. It is called the probability density of the
Lorentz or also Breit&ndash;Wigner distribution and plays an important role in the
physics of resonance phenomena.
</p>
<p>3.4 Distribution Function and Probability Density
</p>
<p>of Two Variables: Conditional Probability
</p>
<p>We now consider two random variables x and y and ask for the probability
that both x &lt; x and y &lt; y. As in the case of a single variable we expect there
to exist of a distribution function (see Fig. 3.7)
</p>
<p>F(x,y)= P (x &lt; x, y &lt; y) . (3.4.1)
</p>
<p>Fig.3.7: Distribution function of two variables.</p>
<p/>
</div>
<div class="page"><p/>
<p>26 3 Random Variables: Distributions
</p>
<p>We will not enter here into axiomatic details and into the conditions for
the existence of F , since these are always fulfilled in cases of practical in-
terest. If F is a differentiable function of x and y, then the joint probability
density of x and y is
</p>
<p>f (x,y)= &part;
&part;x
</p>
<p>&part;
</p>
<p>&part;y
F (x,y) . (3.4.2)
</p>
<p>One then has
</p>
<p>P (a &le; x &lt; b,c &le; y &lt; d)=
&int; b
</p>
<p>a
</p>
<p>[&int; d
</p>
<p>c
</p>
<p>f (x,y)dy
</p>
<p>]
dx . (3.4.3)
</p>
<p>Often we are faced with the following experimental problem. One deter-
mines approximately with many measurements the joint distribution function
F(x,y). One wishes to find the probability for x without consideration of y.
(For example, the probability density for the appearance of a certain infec-
tious disease might be given as a function of date and geographic location.
For some investigations the dependence on the time of year might be of no
interest.)
</p>
<p>We integrate Eq. (3.4.3) over the whole range of y and obtain
</p>
<p>P (a &le; x &lt; b,&minus;&infin;&lt; y &lt;&infin;)=
&int; b
</p>
<p>a
</p>
<p>[&int; &infin;
</p>
<p>&minus;&infin;
f (x,y)dy
</p>
<p>]
dx =
</p>
<p>&int; b
</p>
<p>a
</p>
<p>g(x)dx ,
</p>
<p>where
</p>
<p>g(x)=
&int; &infin;
</p>
<p>&minus;&infin;
f (x,y)dy (3.4.4)
</p>
<p>is the probability density for x. It is called the marginal probability density
of x. The corresponding distribution for y is
</p>
<p>h(y)=
&int; &infin;
</p>
<p>&minus;&infin;
f (x,y)dx . (3.4.5)
</p>
<p>In analogy to the independence of events [Eq. (2.3.6)] we can now define
the independence of random variables. The variables x and y are said to be
independent if
</p>
<p>f (x,y)= g(x)h(y) . (3.4.6)
Using the marginal distributions we can also define conditional probability
for y under the condition that x is known,
</p>
<p>P (y &le; y &lt; y+dy |x &le; x &le; x+dx) . (3.4.7)
We define the conditional probability density as
</p>
<p>f (y|x)= f (x,y)
g(x)
</p>
<p>, (3.4.8)
</p>
<p>so that the probability of Eq. (3.4.7) is given by
</p>
<p>f (y|x)dy .</p>
<p/>
</div>
<div class="page"><p/>
<p>3.5 Expectation Values, Variance, Covariance, and Correlation 27
</p>
<p>The rule of total probability can now also be expressed for distributions:
</p>
<p>h(y)=
&int; &infin;
</p>
<p>&minus;&infin;
f (x,y)dx =
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
f (y|x)g(x)dx . (3.4.9)
</p>
<p>In the case of independent variables as defined by Eq. (3.4.6) one obtains di-
rectly from Eq. (3.4.8)
</p>
<p>f (y|x)= f (x,y)
g(x)
</p>
<p>= g(x)h(y)
g(x)
</p>
<p>= h(y) . (3.4.10)
</p>
<p>This was expected since, in the case of independent variables, any constraint
on one variable cannot contribute information about the probability distribu-
tion of the other.
</p>
<p>3.5 Expectation Values, Variance, Covariance,
</p>
<p>and Correlation
</p>
<p>In analogy to Eq. (3.3.5) we define the expectation value of a function H(x,y)
to be
</p>
<p>E{H(x,y)} =
&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
H(x,y)f (x,y)dx dy . (3.5.1)
</p>
<p>Similarly, the variance of H(x,y) is defined to be
</p>
<p>σ 2{H(x,y)} = E{[H(x,y)&minus;E(H(x,y))]2} . (3.5.2)
</p>
<p>For the simple case H(x,y)= ax+by, Eq. (3.5.1) clearly gives
</p>
<p>E(ax+by)= aE(x)+bE(y) . (3.5.3)
</p>
<p>We now choose
</p>
<p>H(x,y)= xℓym (ℓ, m non-negative integers) . (3.5.4)
</p>
<p>The expectation values of such functions are the ℓmth moments of x,y about
the origin,
</p>
<p>λℓm = E(xℓym) . (3.5.5)
If we choose more generally
</p>
<p>H(x,y)= (x&minus;a)ℓ(y&minus;b)m , (3.5.6)
</p>
<p>the expectation values
</p>
<p>αℓm = E{(x&minus;a)ℓ(y&minus;b)m} (3.5.7)</p>
<p/>
</div>
<div class="page"><p/>
<p>28 3 Random Variables: Distributions
</p>
<p>are the ℓm-th moments about the point a,b. Of special interest are the mo-
ments about the point λ10,λ01,
</p>
<p>μℓm = E{(x&minus;λ10)ℓ(y&minus;λ01)m} . (3.5.8)
</p>
<p>As in the case of a single variable, the lower moments have a special signifi-
cance, in particular,
</p>
<p>μ00 = λ00 = 1 ,
μ10 = μ01 = 0 ;
</p>
<p>λ10 = E(x)= x̂ ,
λ01 = E(y)= ŷ ; (3.5.9)
</p>
<p>μ11 = E{(x&minus; x̂)(y&minus; ŷ)} = cov(x,y) ,
μ20 = E{(x&minus; x̂)2} = σ 2(x) ,
μ02 = E{(y&minus; ŷ)2} = σ 2(y) .
</p>
<p>We can now express the variance of ax+by in terms of these quantities:
</p>
<p>σ 2(ax+by) = E{[(ax+by)&minus;E(ax+by)]2}
= E{[a(x&minus; x̂)+b(y&minus; ŷ)]2}
= E{a2(x&minus; x̂)2 +b2(y&minus; ŷ)2 +2ab(x&minus; x̂)(y&minus; ŷ)} ,
</p>
<p>(3.5.10)
</p>
<p>σ 2(ax+by) = a2σ 2(x)+b2σ 2(y)+2ab cov(x,y) .
</p>
<p>In deriving (3.5.10) we have made use of (3.3.14). As another example we
consider
</p>
<p>H(x,y)= xy . (3.5.11)
In this case we have to assume the independence of x and y in the sense
of (3.4.6) in order to obtain the expectation value. Then according to (3.5.1)
one has
</p>
<p>E(xy) =
&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
x y g(x)h(y)dx dy
</p>
<p>=
(&int; &infin;
</p>
<p>&minus;&infin;
x g(x)dx
</p>
<p>)(&int; &infin;
</p>
<p>&minus;&infin;
y h(y)dy
</p>
<p>)
(3.5.12)
</p>
<p>or
E(xy)= E(x)E(y) . (3.5.13)
</p>
<p>While the quantities E(x), E(y), σ 2(x), σ 2(y) are very similar to those
obtained in the case of a single variable, we still have to explain the meaning</p>
<p/>
</div>
<div class="page"><p/>
<p>3.5 Expectation Values, Variance, Covariance, and Correlation 29
</p>
<p>of cov(x,y). The concept of covariance is of considerable importance for the
understanding of many of our subsequent problems. From its definition we
see that cov(x,y) is positive if values x &gt; x̂ appear preferentially together
with values y &gt; ŷ. On the other hand, cov(x,y) is negative if in general x &gt;
x̂ implies y &lt; ŷ. If, finally, the knowledge of the value of x does not give
us additional information about the probable position of y, the covariance
vanishes. These cases are illustrated in Fig. 3.8.
</p>
<p>It is often convenient to use the correlation coefficient
</p>
<p>ρ(x,y)= cov(x,y)
σ (x)σ (y)
</p>
<p>(3.5.14)
</p>
<p>rather than the covariance.
Both the covariance and the correlation coefficient offer a (necessar-
</p>
<p>ily crude) measure of the mutual dependence of x and y. To investigate
this further we now consider two reduced variables u and v in the sense of
Eq. (3.3.17) and determine the variance of their sum by using (3.5.9),
</p>
<p>σ 2(u+v)= σ 2(u)+σ 2(v)+2ρ(u,v)σ (u)σ (v) . (3.5.15)
</p>
<p>^
</p>
<p>^
</p>
<p>y
</p>
<p>y
ŷ
</p>
<p>ŷ
</p>
<p>x xx x̂ x̂
</p>
<p>y y
</p>
<p>x
</p>
<p>(c)(b)(a)
</p>
<p>Fig.3.8: Illustration of the covariance between the variables x and y. (a) cov(x,y) &gt; 0;
(b) cov(x,y)&asymp; 0; (c) cov(x,y) &lt; 0.
</p>
<p>From Eq. (3.3.19) we know that σ 2(u)= σ 2(v)= 1. Therefore we have
</p>
<p>σ 2(u+v)= 2(1+ρ(u,v)) (3.5.16)
</p>
<p>and correspondingly
</p>
<p>σ 2(u&minus;v)= 2(1&minus;ρ(u,v)) . (3.5.17)
</p>
<p>Since the variance always fulfills
</p>
<p>σ 2 &ge; 0 , (3.5.18)</p>
<p/>
</div>
<div class="page"><p/>
<p>30 3 Random Variables: Distributions
</p>
<p>it follows that
&minus;1 &le; ρ(u,v) &le; 1 . (3.5.19)
</p>
<p>If one now returns to the original variables x,y, then it is easy to show that
</p>
<p>ρ(u,v)= ρ(x,y) . (3.5.20)
</p>
<p>Thus we have finally shown that
</p>
<p>&minus;1 &le; ρ(x,y)&le; 1 . (3.5.21)
</p>
<p>We now investigate the limiting cases &plusmn;1. For ρ(u,v) = 1 the variance
is σ(u&minus;v) = 0, i.e., the random variable (u&minus;v) is a constant. Expressed in
terms of x,y one has therefore
</p>
<p>u&minus;v = x&minus; x̂
σ (x)
</p>
<p>&minus; y&minus; ŷ
σ (y)
</p>
<p>= const . (3.5.22)
</p>
<p>The equation is always fulfilled if
</p>
<p>y = a+bx , (3.5.23)
</p>
<p>where b is positive. Therefore in the case of a linear dependence (b posi-
tive) between x and y the correlation coefficient takes the value ρ(x,y)=+1.
Correspondingly one finds ρ(x,y) = &minus;1 for a negative linear dependence (b
negative). We would expect the covariance to vanish for two independent vari-
ables x and y, i.e., for which the probability density obeys Eq. (3.4.6). Indeed
with (3.5.9) and (3.5.1) we find
</p>
<p>cov(x,y) =
&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
(x&minus; x̂)(y&minus; ŷ)g(x)h(y)dx dy
</p>
<p>=
(&int; &infin;
</p>
<p>&minus;&infin;
(x&minus; x̂)g(x)dx
</p>
<p>)(&int; &infin;
</p>
<p>&minus;&infin;
(y&minus; ŷ)h(y)dy
</p>
<p>)
</p>
<p>= 0 .
</p>
<p>3.6 More than Two Variables: Vector and Matrix Notation
</p>
<p>In analogy to (3.4.1) we now define a distribution function of n variables
x1,x2, . . . ,xn:
</p>
<p>F(x1,x2, . . . ,xn)= P (x1 &lt; x1, x2 &lt; x2, . . . ,xn &lt; xn) . (3.6.1)</p>
<p/>
</div>
<div class="page"><p/>
<p>3.6 More than Two Variables: Vector and Matrix Notation 31
</p>
<p>If the function F is differentiable with respect to the xi , then the joint proba-
bility density is given by
</p>
<p>f (x1,x2, . . . ,xn)=
&part;n
</p>
<p>&part;x1 &part;x2 &middot; &middot; &middot;&part;xn
F(x1,x2, . . . ,xn) . (3.6.2)
</p>
<p>The probability density of one of the variables xr , the marginal probability
density, is given by
</p>
<p>gr(xr)=
&int; &infin;
</p>
<p>&minus;&infin;
&middot; &middot; &middot;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
f (x1,x2, . . . ,xn)dx1 &middot; &middot; &middot;dxr&minus;1 dxr+1 &middot; &middot; &middot;dxn . (3.6.3)
</p>
<p>If H(x1,x2, . . . ,xn) is a function of n variables, then the expectation value of
H is
</p>
<p>E{H(x1,x2, . . . ,xn)} (3.6.4)
</p>
<p>=
&int; &infin;
</p>
<p>&minus;&infin;
&middot; &middot; &middot;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
H(x1,x2, . . . ,xn)f (x1,x2, . . . ,xn)dx1 &middot; &middot; &middot;dxn .
</p>
<p>With H(x)= xr one obtains
</p>
<p>E(xr) =
&int; &infin;
</p>
<p>&minus;&infin;
. . .
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
xrf (x1,x2, . . . ,xn)dx1 &middot; &middot; &middot;dxn ,
</p>
<p>E(xr) =
&int; &infin;
</p>
<p>&minus;&infin;
xrgr(xr)dxr . (3.6.5)
</p>
<p>The variables are independent if
</p>
<p>f (x1,x2, . . . ,xn)= g1(x1)g2(x2) &middot; &middot; &middot;gn(xn) . (3.6.6)
</p>
<p>In analogy to Eq. (3.6.3) one can define the joint marginal probability den-
sity of ℓ out of the n variables,&lowast; by integrating (3.6.3) over only the n&minus; ℓ
remaining variables,
</p>
<p>g(x1,x2, . . . ,xℓ)=
&int; &infin;
</p>
<p>&minus;&infin;
&middot; &middot; &middot;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
f (x1,x2, . . . ,xn)dxℓ+1 &middot; &middot; &middot;dxn . (3.6.7)
</p>
<p>These ℓ variables are independent if
</p>
<p>g(x1,x2, . . . ,xℓ)= g1(x1)g2(x2) &middot; &middot; &middot;gℓ(xℓ) . (3.6.8)
</p>
<p>The moments of order ℓ1,ℓ2, . . . ,ℓn about the origin are the expectation val-
ues of the functions
</p>
<p>H = xℓ11 x
ℓ2
2 &middot; &middot; &middot;x
</p>
<p>ℓn
n ,
</p>
<p>&lowast;Without loss of generality we can take these to be the variables x1,x2, . . . ,xℓ.</p>
<p/>
</div>
<div class="page"><p/>
<p>32 3 Random Variables: Distributions
</p>
<p>that is,
λℓ1ℓ2...ℓn = E(x
</p>
<p>ℓ1
1 x
</p>
<p>ℓ2
2 &middot; &middot; &middot;xℓnn ) .
</p>
<p>In particular one has
</p>
<p>λ100...0 = E(x1)= x̂1 ,
λ010...0 = E(x2)= x̂2 , (3.6.9)
</p>
<p>...
</p>
<p>λ000...1 = E(xn)= x̂n .
</p>
<p>The moments about (̂x1, x̂2, . . . , x̂n) are correspondingly
</p>
<p>μℓ1ℓ2...ℓn = E{(x1 &minus; x̂1)ℓ1(x2 &minus; x̂2)ℓ2 &middot; &middot; &middot;(xn&minus; x̂n)ℓn} . (3.6.10)
</p>
<p>The variances of the xi are then
</p>
<p>μ200...0 = E{(x1 &minus; x̂1)2} = σ 2(x1) ,
μ020...0 = E{(x2 &minus; x̂2)2} = σ 2(x2) , (3.6.11)
</p>
<p>...
</p>
<p>μ000...2 = E{(xn&minus; x̂n)2} = σ 2(xn) .
</p>
<p>The moment with ℓi = ℓj = 1, ℓk = 0 (i �= k �= j) is called the covariance
between the variables xi and xj ,
</p>
<p>cij = cov(xi,xj )= E{(xi &minus; x̂i)(xj &minus; x̂j )} . (3.6.12)
</p>
<p>It proves useful to represent the n variables x1,x2, . . . ,xn as components of a
vector x in an n-dimensional space. We can then write the distribution func-
tion (3.6.1) as
</p>
<p>F = F(x) . (3.6.13)
Correspondingly, the probability density (3.6.2) is then
</p>
<p>f (x)= &part;
n
</p>
<p>&part;x1&part;x2 &middot; &middot; &middot;&part;xn
F(x) . (3.6.14)
</p>
<p>The expectation value of a function H(x) is then simply
</p>
<p>E{H(x)} =
&int;
</p>
<p>H(x)f (x)dx . (3.6.15)
</p>
<p>We would now like to express the variances and covariances by means of a
matrix.&dagger; This is called the covariance matrix
</p>
<p>&dagger;For details on matrix notation see Appendix A.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.7 Transformation of Variables 33
</p>
<p>C =
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>c11 c12 &middot; &middot; &middot; c1n
c21 c22 &middot; &middot; &middot; c2n
...
</p>
<p>cn1 cn2 &middot; &middot; &middot; cnn
</p>
<p>⎞
⎟⎟⎟⎠ . (3.6.16)
</p>
<p>The elements cij are given by (3.6.12); the diagonal elements are the variances
cii = σ 2(xi). The covariance matrix is clearly symmetric, since
</p>
<p>cij = cji . (3.6.17)
</p>
<p>If we now also write the expectation values of the xi as a vector,
</p>
<p>E(x)= x̂ , (3.6.18)
</p>
<p>we see that each element of the covariance matrix
</p>
<p>cij = E{(xi &minus; x̂i)(xj &minus; x̂j )T}
</p>
<p>is given by the expectation value of the product of the row vector (x&minus; x̂)T and
the column vector (x&minus; x̂), where
</p>
<p>xT = (x1,x2, . . . ,xn) , x=
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>x1
x2
...
</p>
<p>xn
</p>
<p>⎞
⎟⎟⎟⎠ .
</p>
<p>The covariance matrix can therefore be written simply as
</p>
<p>C = E{(x&minus; x̂)(x&minus; x̂)T} . (3.6.19)
</p>
<p>3.7 Transformation of Variables
</p>
<p>As already mentioned in Sect. 3.3, a function of a random variable is itself a
random variable, e.g.,
</p>
<p>y = y(x) .
We now ask for the probability density g(y) for the case where the probability
density f (x) is known.
</p>
<p>Clearly the probability
g(y)dy
</p>
<p>that y falls into a small interval dy must be equal to the probability f (x)dx
that x falls into the &ldquo;corresponding interval&rdquo; dx, f (x)dx = g(y)dy. This is
illustrated in Fig. 3.9. The intervals dx and dy are related by</p>
<p/>
</div>
<div class="page"><p/>
<p>34 3 Random Variables: Distributions
</p>
<p>dx
</p>
<p>dy
</p>
<p>g(y)
</p>
<p>x
</p>
<p>x
</p>
<p>y
</p>
<p>y=y(x)
</p>
<p>f(x)
</p>
<p>Fig.3.9:Transformation of vari-
ables for a probability density of
x to y.
</p>
<p>dy =
∣∣∣∣
dy
</p>
<p>dx
</p>
<p>∣∣∣∣dx , i.e., dx =
∣∣∣∣
dx
</p>
<p>dy
</p>
<p>∣∣∣∣dy .
</p>
<p>The absolute value ensures that we consider the values dx, dy as intervals
without a given direction. Only in this way are the probabilities f (x)dx and
g(x)dy always positive. The probability density is then given by
</p>
<p>g(y)=
∣∣∣∣
dx
</p>
<p>dy
</p>
<p>∣∣∣∣f (x) . (3.7.1)
</p>
<p>We see immediately that g(y) is defined only in the case of a single-valued
function y(x) since only then is the derivative in (3.7.1) uniquely defined.
For functions where this is not the case, e.g., y =&radic;x, one must consider the
individual single-valued parts separately, i.e., y =+&radic;x. Equation (3.7.1) also
guarantees that the probability distribution of y is normalized to unity:
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
g(y)dy =
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
f (x)dx = 1 .
</p>
<p>In the case of two independent variables x, y the transformation to the new
variables
</p>
<p>u = u(x,y) , v = v(x,y) (3.7.2)
can be illustrated in a similar way. One must find the quantity J that relates
the probabilities f (x,y) and g(u,v):
</p>
<p>g(u,v)= f (x,y)
∣∣∣∣J
</p>
<p>(
x,y
</p>
<p>u,v
</p>
<p>)∣∣∣∣ . (3.7.3)
</p>
<p>Figure 3.10 shows in the (x,y) plane two lines each for u = const and v =
const. They bound the surface element dA of the transformed variables u, v
corresponding to the element dx dy of the original variables.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.7 Transformation of Variables 35
</p>
<p>x
</p>
<p>y
</p>
<p>cb
</p>
<p>a
</p>
<p>d
</p>
<p>u(x,y)
</p>
<p>v(x,y)+dv
</p>
<p>v(x,y)
</p>
<p>u(x,y)+du
</p>
<p>dA
</p>
<p>Fig.3.10: Variable transformation
from x, y to u, v.
</p>
<p>These curves of course need not be straight lines. Since, however, dA is
an &ldquo;infinitesimal&rdquo; surface element, it can be treated as a parallelogram, whose
area we will now compute. The coordinates of the corner points a, b, c are
</p>
<p>xa = x(u,v) , ya = y(u,v) ,
xb = x(u,v+dv) , yb = y(u,v+dv) ,
xc = x(u+du,v) , yc = y(u+du,v) .
</p>
<p>We can expand the last two lines in a series and obtain
</p>
<p>xb = x(u,v)+ &part;x&part;v dv , yb = y(u,v)+
&part;y
&part;v
</p>
<p>dv ,
</p>
<p>xc = x(u,v)+ &part;x&part;u du , yc = y(u,v)+
&part;y
&part;u
</p>
<p>du .
</p>
<p>The area of the parallelogram is equal to the absolute value of the determinant
</p>
<p>dA=
</p>
<p>∣∣∣∣∣∣
</p>
<p>1 xa ya
1 xb yb
1 xc yc
</p>
<p>∣∣∣∣∣∣
=
</p>
<p>∣∣∣∣∣∣
</p>
<p>&part;x
&part;u
</p>
<p>&part;y
&part;u
</p>
<p>&part;x
&part;v
</p>
<p>&part;y
&part;v
</p>
<p>∣∣∣∣∣∣
dudv = J
</p>
<p>(
x,y
</p>
<p>u,v
</p>
<p>)
dudv , (3.7.4)
</p>
<p>where the sign is of no consequence because of the absolute value in Eq. (3.7.3).
The expression
</p>
<p>J
</p>
<p>(
x,y
</p>
<p>u,v
</p>
<p>)
=
</p>
<p>∣∣∣∣∣∣
</p>
<p>&part;x
&part;u
</p>
<p>&part;y
&part;u
</p>
<p>&part;x
&part;v
</p>
<p>&part;y
&part;v
</p>
<p>∣∣∣∣∣∣
(3.7.5)
</p>
<p>is called the Jacobian (determinant) of the transformation (3.7.2).
For the general case of n variables x = (x1,x2, . . . ,xn) and the transfor-
</p>
<p>mation
</p>
<p>y1 = y1(x) ,
y2 = y2(x) , (3.7.6)
...
</p>
<p>yn = yn(x) ,</p>
<p/>
</div>
<div class="page"><p/>
<p>36 3 Random Variables: Distributions
</p>
<p>the probability density of the transformed variables is given by
</p>
<p>g(y)=
∣∣∣∣J
</p>
<p>(
x
</p>
<p>y
</p>
<p>)∣∣∣∣f (x) , (3.7.7)
</p>
<p>where the Jacobian is
</p>
<p>J
</p>
<p>(
x
</p>
<p>y
</p>
<p>)
= J
</p>
<p>(
x1,x2, . . . ,xn
</p>
<p>y1,y2, . . . ,yn
</p>
<p>)
=
</p>
<p>∣∣∣∣∣∣∣∣∣∣∣∣∣∣
</p>
<p>&part;x1
&part;y1
</p>
<p>&part;x2
&part;y1
</p>
<p>&middot; &middot; &middot; &part;xn
&part;y1
</p>
<p>&part;x1
&part;y2
</p>
<p>&part;x2
&part;y2
</p>
<p>&middot; &middot; &middot; &part;xn
&part;y2
</p>
<p>...
</p>
<p>&part;x1
&part;yn
</p>
<p>&part;x2
&part;yn
</p>
<p>&middot; &middot; &middot; &part;xn
&part;yn
</p>
<p>∣∣∣∣∣∣∣∣∣∣∣∣∣∣
</p>
<p>. (3.7.8)
</p>
<p>A requirement for the existence of g(y) is of course again the uniqueness of
all derivatives occurring in J .
</p>
<p>3.8 Linear and Orthogonal Transformations:
</p>
<p>Error Propagation
</p>
<p>In practice we deal frequently with linear transformations of variables. The
main reason is that they are particularly easy to handle, and we try therefore
to approximate other transformations by linear ones using Taylor series tech-
niques.
</p>
<p>Consider r linear functions of the n variables x = (x1,x2, . . . ,xn):
y1 = a1 + t11x1 + t12x2 +&middot;&middot; &middot;+ t1nxn ,
y2 = a2 + t21x1 + t22x2 +&middot;&middot; &middot;+ t2nxn , (3.8.1)
...
</p>
<p>yr = ar + tr1x1 + tr2x2 +&middot;&middot; &middot;+ trnxn ,
or in matrix notation,
</p>
<p>y = T x+a . (3.8.2)
The expectation value of y follows from the generalization of (3.5.3)
</p>
<p>E(y)= ŷ= T x̂+a . (3.8.3)
Together with (3.6.19) one obtains the covariance matrix for y,
</p>
<p>Cy = E{(y&minus; ŷ)(y&minus; ŷ)T}
= E{(T x+a&minus;T x̂&minus;a)(T x+a&minus;T x̂&minus;a)T}
= E{T (x&minus; x̂)(x&minus; x̂)TT T}
= TE{(x&minus; x̂)(x&minus; x̂)T}T T ,
</p>
<p>Cy = TCxT T . (3.8.4)</p>
<p/>
</div>
<div class="page"><p/>
<p>3.8 Linear Transformation: Error Propagation 37
</p>
<p>Equation (3.8.4) expresses the well-known law of error propagation. Suppose
the expectation values x̂i have been measured. Suppose as well that the errors
(i.e., the standard deviations or variances) and the covariances of x are known.
One would like to know the errors of an arbitrary function y(x). If the errors
are relatively small, then the probability density is only significantly large in a
small region (on the order of the standard deviation) around the point x̂. One
then performs a Taylor expansion of the functions,
</p>
<p>yi = yi (̂x)+
(
&part;yi
</p>
<p>&part;x1
</p>
<p>)
</p>
<p>x=̂x
(x1 &minus; x̂1)+&middot;&middot; &middot;+
</p>
<p>(
&part;yi
</p>
<p>&part;xn
</p>
<p>)
</p>
<p>x=̂x
(xn&minus; x̂n)
</p>
<p>+ higher-order terms ,
</p>
<p>or in matrix notation,
</p>
<p>y = y(̂x)+T (x&minus; x̂)+ higher-order terms (3.8.5)
</p>
<p>with
</p>
<p>T =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>&part;y1
&part;x1
</p>
<p>&part;y1
&part;x2
</p>
<p>&middot; &middot; &middot; &part;y1
&part;xn
</p>
<p>&part;y2
&part;x1
</p>
<p>&part;y2
&part;x2
</p>
<p>&middot; &middot; &middot; &part;y2
&part;xn
</p>
<p>...
</p>
<p>&part;yr
&part;x1
</p>
<p>&part;yr
&part;x2
</p>
<p>&middot; &middot; &middot; &part;yr
&part;xn
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>x=̂x
</p>
<p>. (3.8.6)
</p>
<p>If one neglects the higher-order terms and substitutes the first partial deriva-
tives of the matrix T into Eq. (3.8.4), then one obtains the law of error prop-
agation. We see in particular that not only the errors (i.e., the variances) of
x but also the covariances make a significant contribution to the errors of y,
that is, to the diagonal elements of Cy . If these are not taken into account in
the error propagation, then the result cannot be trusted.
</p>
<p>The covariances can only be neglected when they vanish anyway, i.e., in
the case of independent original variables x. In this case Cx simplifies to a
diagonal matrix. The diagonal elements of Cy then have the simple form
</p>
<p>σ 2(yi)=
n&sum;
</p>
<p>j=1
</p>
<p>(
&part;yi
</p>
<p>&part;xj
</p>
<p>)2
</p>
<p>x=̂x
σ 2(xj ) . (3.8.7)
</p>
<p>If we now call the standard deviation, i.e., the positive square root of the
variance, the error of the corresponding quantity and use for this the symbol
Δ, Eq. (3.8.7) leads immediately to the formula
</p>
<p>Δyi =
</p>
<p>&radic;&radic;&radic;&radic;
n&sum;
</p>
<p>j=1
</p>
<p>(
&part;yi
</p>
<p>&part;xj
</p>
<p>)2
(Δxj )2 , (3.8.8)</p>
<p/>
</div>
<div class="page"><p/>
<p>38 3 Random Variables: Distributions
</p>
<p>known commonly as the law of the propagation of errors. It must be empha-
sized that this expression is incorrect in cases of non-vanishing covariances.
This is illustrated in the following example.
</p>
<p>Example 3.6: Error propagation and covariance
</p>
<p>In a Cartesian coordinate system a point (x,y) is measured. The measurement
is performed with a coordinate measuring device whose error in y is three
times larger than that in x. The measurements of x and y are independent. We
therefore can take the covariance matrix to be (up to a factor common to all
elements)
</p>
<p>Cx,y =
(
</p>
<p>1 0
0 9
</p>
<p>)
.
</p>
<p>We now evaluate the errors (i.e., the covariance matrix) in polar coordinates
</p>
<p>r =
&radic;
(x2 +y2) , ϕ = arctan y
</p>
<p>x
.
</p>
<p>The transformation matrix (3.8.6) is
</p>
<p>T =
</p>
<p>⎛
⎝
</p>
<p>x
r
</p>
<p>y
r
</p>
<p>&minus; y
r2
</p>
<p>x
</p>
<p>r2
</p>
<p>⎞
⎠ .
</p>
<p>To simplify the numerical calculations we consider only the point (1,1). Then
</p>
<p>T =
( 1&radic;
</p>
<p>2
1&radic;
2
</p>
<p>&minus;12
1
2
</p>
<p>)
</p>
<p>and therefore
</p>
<p>Crϕ =
( 1&radic;
</p>
<p>2
1&radic;
2
</p>
<p>&minus;12
1
2
</p>
<p>)(
1 0
</p>
<p>0 9
</p>
<p>)⎛
⎝
</p>
<p>1&radic;
2
</p>
<p>&minus;12
1&radic;
2
</p>
<p>1
2
</p>
<p>⎞
⎠=
</p>
<p>⎛
⎝
</p>
<p>5 4&radic;
2
</p>
<p>4&radic;
2
</p>
<p>5
2
</p>
<p>⎞
⎠ .
</p>
<p>We can now return to the original Cartesian coordinate system
</p>
<p>x = r cosϕ , y = r sinϕ
</p>
<p>by use of the transformation
</p>
<p>T &prime; =
(
</p>
<p>cosϕ &minus;r sinϕ
sinϕ r cosϕ
</p>
<p>)
=
</p>
<p>⎛
⎝
</p>
<p>1&radic;
2
</p>
<p>&minus;1
1&radic;
2
</p>
<p>1
</p>
<p>⎞
⎠ .</p>
<p/>
</div>
<div class="page"><p/>
<p>3.8 Linear Transformation: Error Propagation 39
</p>
<p>As expected we obtain
</p>
<p>Cxy =
</p>
<p>⎛
⎝
</p>
<p>1&radic;
2
</p>
<p>&minus;1
1&radic;
2
</p>
<p>1
</p>
<p>⎞
⎠
⎛
⎝
</p>
<p>5 4&radic;
2
</p>
<p>4&radic;
2
</p>
<p>5
2
</p>
<p>⎞
⎠
( 1&radic;
</p>
<p>2
1&radic;
2
</p>
<p>&minus;1 1
</p>
<p>)
=
</p>
<p>(
1 0
</p>
<p>0 9
</p>
<p>)
.
</p>
<p>If instead we had used the formula (3.8.8), i.e., if we had neglected the co-
variances in the transformation of r,ϕ to x,y, then we would have obtained
</p>
<p>C &prime;xy =
</p>
<p>⎛
⎝
</p>
<p>1&radic;
2
</p>
<p>&minus;1
1&radic;
2
</p>
<p>1
</p>
<p>⎞
⎠
(
</p>
<p>5 0
</p>
<p>0 52
</p>
<p>)( 1&radic;
2
</p>
<p>1&radic;
2
</p>
<p>&minus;1 1
</p>
<p>)
=
</p>
<p>(
5 0
</p>
<p>0 5
</p>
<p>)
,
</p>
<p>which is different from the original covariance matrix. This example stresses
the importance of covariances, since it is obviously not possible to change
errors of measurements by simply transforming back and forth between coor-
dinate systems.
</p>
<p>Finally we discuss a special type of linear transformation. We consider
the case of exactly n functions y of the n variables x. In particular we take
a= 0 in (3.8.2). One then has
</p>
<p>y = Rx , (3.8.9)
</p>
<p>where R is a square matrix. We now require that the transformation (3.8.9)
leaves the modulus of a vector invariant
</p>
<p>y
2 =
</p>
<p>n&sum;
</p>
<p>i=1
y2i = x2 =
</p>
<p>n&sum;
</p>
<p>i=1
x2i . (3.8.10)
</p>
<p>Using Eq. (A.1.9) we can write
</p>
<p>y
T
y = (Rx)T(Rx)= xTRTRx = xTx .
</p>
<p>This means
RTR = I ,
</p>
<p>or written in terms of components,
</p>
<p>n&sum;
</p>
<p>i=1
rikriℓ = δkℓ =
</p>
<p>{
0 , ℓ �= k ,
1 , ℓ= k . (3.8.11)
</p>
<p>A transformation of the type (3.8.9) that fulfills condition (3.8.11) is said to
be orthogonal. We now consider the determinant of the transformation matrix</p>
<p/>
</div>
<div class="page"><p/>
<p>40 3 Random Variables: Distributions
</p>
<p>D =
</p>
<p>∣∣∣∣∣∣∣∣∣
</p>
<p>r11 r12 &middot; &middot; &middot; r1n
r21 r22 &middot; &middot; &middot; r2n
...
</p>
<p>rn1 rn2 &middot; &middot; &middot; rnn
</p>
<p>∣∣∣∣∣∣∣∣∣
</p>
<p>and form its square. According to rules for computing determinants we obtain
from Eq. (3.8.11)
</p>
<p>D2 =
</p>
<p>∣∣∣∣∣∣∣∣
</p>
<p>1 0 &middot; &middot; &middot; 0
0 1 &middot; &middot; &middot; 0
&middot; &middot; &middot;
0 0 &middot; &middot; &middot; 1
</p>
<p>∣∣∣∣∣∣∣∣
,
</p>
<p>i.e., D =&plusmn;1. The determinant D, however, is the Jacobian determinant of the
transformation (3.8.9),
</p>
<p>J
(y
x
</p>
<p>)
=&plusmn;1 . (3.8.12)
</p>
<p>We multiply the system of equations (3.8.9) on the left with RT and obtain
</p>
<p>RTy = RTRx .
</p>
<p>Because of Eq. (3.8.11) this expression reduces to
</p>
<p>x = RTy . (3.8.13)
</p>
<p>The inverse transformation of an orthogonal transformation is described sim-
ply by the transposed transformation matrix. It is itself orthogonal.
</p>
<p>An important property of any linear transformation of the type
</p>
<p>y1 = r11x1 + r12x2 +&middot;&middot; &middot;+ r1nxn
</p>
<p>is the following. By constructing additional functions y2,y3, . . . ,yn of equiv-
alent form it can be extended to yield an orthogonal transformation as long as
the condition
</p>
<p>n&sum;
</p>
<p>i=1
r21i = 1
</p>
<p>is fulfilled.</p>
<p/>
</div>
<div class="page"><p/>
<p>4. Computer Generated Random Numbers:
</p>
<p>The Monte Carlo Method
</p>
<p>4.1 Random Numbers
</p>
<p>Up to now in this book we have considered the observation of random
variables, but not prescriptions for producing them. It is in many applications
useful, however, to have a sequence of values of a randomly distributed vari-
able x. Since operations must often be carried out with a large number of such
random numbers, it is particularly convenient to have them directly available
on a computer. The correct procedure to create such random numbers would
be to use a statistical process, e.g., the measurement of the time between two
decays from a radioactive source, and to transfer the measured results into the
computer. In practical applications, however, the random numbers are almost
always calculated directly by the computer. Since this works in a strictly
deterministic way, the resulting values are not really random, but rather can
be exactly predicted. They are therefore called pseudorandom.
</p>
<p>Computations with random numbers currently make up a large part of
all computer calculations in the planning and evaluation of experiments. The
statistical behavior which stems either from the nature of the experiment or
from the presence of measurement errors can be simulated on the computer.
The use of random numbers in computer programs is often called the Monte
Carlo method.
</p>
<p>We begin this chapter with a discussion of the representation of numbers
in a computer (Sect. 4.2), which is indispensable for an understanding of what
follows. The best studied method for the creation of uniformly distributed ran-
dom numbers is the subject of Sects. 4.3&ndash;4.7. Sections 4.8 and 4.9 cover the
creation of random numbers that follow an arbitrary distribution and the espe-
cially common case of normally distributed numbers. In the last two sections
one finds discussion and examples of the Monte Carlo method in applications
of numerical integration and simulation.
</p>
<p>S. Brandt, Data Analysis: Statistical and Computational Methods for Scientists and Engineers,
DOI 10.1007/978-3-319-03762-2__4, &copy; Springer International Publishing Switzerland 2014
</p>
<p>41</p>
<p/>
</div>
<div class="page"><p/>
<p>42 4 Random Numbers: The Monte Carlo Method
</p>
<p>In many examples and exercises we will simulate measurements with
the Monte Carlo method and then analyze them. We possess in this way a
computer laboratory, which allows us to study individually the influence of
simulated measurement errors on the results of an analysis.
</p>
<p>4.2 Representation of Numbers in a Computer
</p>
<p>For most applications the representation of numbers used in a computation
is unimportant. It can be of decisive significance, however, for the proper-
ties of computer-generated random numbers. We will restrict ourselves to the
binary representation, which is used today in practically all computers. The
elementary unit of information is the bit,&lowast; which can assume the values of
0 or 1. This is realized physically by two distinguishably different electric or
magnetic states of a component in the computer.
</p>
<p>If one has k bits available for the representation of an integer, then 1 bit is
sufficient to encode the sign. The remaining k&minus;1 bits are used for the binary
representation of the absolute value in the form
</p>
<p>a = a(k&minus;2)2k&minus;2 +a(k&minus;3)2k&minus;3 +&middot;&middot; &middot;+a(1)21 +a(0)20 . (4.2.1)
</p>
<p>Here each of the coefficients a(j) can assume only the values 0 or 1, and thus
can be represented by a single bit.
</p>
<p>The binary representation for non-negative integers is
</p>
<p>00 &middot; &middot; &middot;000 = 0
00 &middot; &middot; &middot;001 = 1
00 &middot; &middot; &middot;010 = 2
00 &middot; &middot; &middot;011 = 3
</p>
<p>...
</p>
<p>One could simply use the first bit to encode the sign and represent the corre-
sponding negative numbers such that in the first bit the 0 is replaced by a 1.
That would give, however, two different representations for the number zero,
or rather +0 and &minus;0. In fact, one uses for negative numbers the &ldquo;complemen-
tary representation&rdquo;
</p>
<p>11 &middot; &middot; &middot;111 = &minus;1
11 &middot; &middot; &middot;110 = &minus;2
11 &middot; &middot; &middot;101 = &minus;3
</p>
<p>...
</p>
<p>&lowast;Abbreviation of binary digit.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.2 Representation of Numbers in a Computer 43
</p>
<p>Then using k bits, integers in the interval
</p>
<p>&minus;2k&minus;1 &le; x &le; 2k&minus;1 &minus;1 (4.2.2)
</p>
<p>can be represented.
In most computers 8 bit are grouped together into one byte. Four bytes
</p>
<p>are generally used for the representation of integers, i.e., k = 32, 2k&minus;1 &minus; 1 =
2147483647. In many small computers only two bytes are available, k = 16,
2k&minus;1 &minus; 1 = 32767. This constraint (4.2.2) must be taken into consideration
when designing a program to generate random numbers.
</p>
<p>Before turning to the representation of fractional numbers in a com-
puter, let us consider a finite decimal fraction, which we can write in various
ways, e.g.,
</p>
<p>x = 17.23 = 0.1723 &middot;102
</p>
<p>or in general
</p>
<p>x =M &middot;10e .
</p>
<p>The quantities M and e are called the mantissa and exponent, respectively.
One chooses the exponent such that the mantissa&rsquo;s nonzero digits are all to
the right of the decimal point, and the first place after the decimal point is not
zero. If one has available n decimal places for the representation of the value
M , then
</p>
<p>m=M &middot;10n
</p>
<p>is an integer. In our example, n = 4 and m = 1723. In this way the decimal
fraction d is represented by the two integers m and e.
</p>
<p>The representation of fractions in the binary system is done in a com-
pletely analogous way. One decomposes a number of the form
</p>
<p>x =M &middot;2e (4.2.3)
</p>
<p>into a mantissa M and exponent e. If nm bits are available for the representa-
tion of the mantissa (including sign), it can be expressed by the integer
</p>
<p>m=M &middot;2nm&minus;1 , &minus;2nm&minus;1 &le;m&le; 2nm&minus;1 &minus;1 . (4.2.4)
</p>
<p>If the exponent with its sign is represented by ne bits, then it can cover the
interval
</p>
<p>&minus;2ne &le; e &le; 2ne &minus;1 . (4.2.5)
</p>
<p>In our Java classes we use floating-point numbers of the type double with
64 bit, nm = 53 for the mantissa and ne = 11 for the exponent.</p>
<p/>
</div>
<div class="page"><p/>
<p>44 4 Random Numbers: The Monte Carlo Method
</p>
<p>For the interval of values in which a floating point number can be
represented in a computer, the constraint (4.2.2) no longer applies but one
has rather the weaker condition
</p>
<p>2emin &lt; |x|&lt; 2emax . (4.2.6)
</p>
<p>Here emin and emax are given by (4.2.5). If 11 bit are available for representing
the exponent (including sign), then one has emax = 210&minus;1= 1023. Therefore,
one has the constraint |x|&lt; 21023 &asymp; 10308.
</p>
<p>When computing with floating point numbers, the concept of the relative
precision of the representation is of considerable significance. There are a
fixed number of binary digits corresponding to a fixed number of decimal
places available for the representation of the mantissa M . If we designate by
α the smallest possible mantissa, then two numbers x1 and x2 can still be
represented as being distinct if
</p>
<p>x1 = x =M &middot;2e , x2 = (M+α) &middot;2e .
</p>
<p>The absolute precision in the representation of x is thus
</p>
<p>Δx = x1 &minus;x2 = α &middot;2e ,
</p>
<p>which depends on the exponent of x. The relative precision
</p>
<p>Δx
</p>
<p>x
= α
</p>
<p>M
</p>
<p>is in contrast independent of x. If n binary digits are available for the represen-
tation of the mantissa, then one has M &asymp; 2n, since the exponent is chosen such
that all n places for the mantissa are completely used. The smallest possible
mantissa is α = 20, so that the relative precision in the representation of x is
</p>
<p>Δx
</p>
<p>x
= 2&minus;n . (4.2.7)
</p>
<p>4.3 Linear Congruential Generators
</p>
<p>Since, as mentioned, computers work in a strictly deterministic way, all
(pseudo)"-random numbers generated in a computer are in the most general
case a function of all of the preceding (pseudo)random numbers&dagger;
</p>
<p>xj+1 = f (xj ,xj&minus;1, . . . ,x1) . (4.3.1)
</p>
<p>Programs for creating random numbers are called random number generators.
</p>
<p>&dagger;Since the numbers are pseudorandom and not strictly random, we use the notation x in
place of x.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.4 Multiplicative Linear Congruential Generators 45
</p>
<p>The best studied algorithm is based on the following rule,
</p>
<p>xj+1 = (a xj + c) mod m . (4.3.2)
</p>
<p>All of the quantities in (4.3.2) are integer valued. Generators using this rule are
called linear congruential generators (LCG). The symbol mod m or modulo
m in (4.3.2) means that the expression before the symbol is divided by m and
only the remainder of the result is taken, e.g., 6 mod 5 = 1. Each random
number made by an LCG according to the rule (4.3.2) depends only on the
number immediately preceding it and on the constant a (the multiplier), on c
(the increment), and on m (the modulus). When these three constants and one
initial value x0 are given, the infinite sequence of random numbers x0, x1, . . .
is determined.
</p>
<p>The sequence is clearly periodic. The maximum period length is m. Only
partial sequences that are short compared to the period length are useful for
computations.
</p>
<p>Theorem on the maximum period of an LCG with c �= 0:
An LCG defined by the values m, a, c, and x0 has the period m
if and only if
</p>
<p>(a) c and m have no common factors;
</p>
<p>(b) b = a&minus; 1 is a multiple of p for every prime number p that
is a factor of m;
</p>
<p>(c) b is a multiple of 4 if m is a multiple of 4.
</p>
<p>The proof of this theorem as well as the theorems of Sect. 4.4
can be found in, e.g., KNUTH [2].
</p>
<p>A simple example is c = 3, a = 5, m = 16. One can easily compute that
x0 = 0 results in the sequence
</p>
<p>0, 3, 2, 13, 4, 7, 6, 1, 8, 11, 10, 5, 12, 15, 14, 9, 0, . . . .
</p>
<p>Since the period m can only be attained when all m possible values are actu-
ally assumed, the choice of the initial value x0 is unimportant.
</p>
<p>4.4 Multiplicative Linear Congruential Generators
</p>
<p>If one chooses c = 0 in (4.3.2), then the algorithm simplifies to
</p>
<p>xj+1 = (axj ) mod m . (4.4.1)
</p>
<p>Generators based on this rule are called multiplicative linear congruential
generators (MLCG). The computation becomes somewhat shorter and thus</p>
<p/>
</div>
<div class="page"><p/>
<p>46 4 Random Numbers: The Monte Carlo Method
</p>
<p>faster. The exact value zero, however, can no longer be produced (except
for the unusable sequence 0, 0, . . .). In addition the period becomes shorter.
Before giving the theorem on the maximum period length for this case, we
introduce the concept of the primitive element modulo m.
</p>
<p>Let a be an integer having no common factors (except unity) with m.
We consider all a for which aλ mod m= 1 for integer λ. The smallest value of
λ for which this relation is valid is called the order of a modulo m. All values
a having the same largest possible order λ(m) are called primitive elements
modulo m.
</p>
<p>Theorem on the order λ(m) of a primitive element modulo m:
</p>
<p>For every integer e and prime number p
</p>
<p>λ(2)= 1 ;
λ(4)= 2 ;
λ(2e)= 2e&minus;2 , e &gt; 2 ;
λ(pe)= pe&minus;1(p&minus;1) , p &gt; 2 .
</p>
<p>(4.4.2)
</p>
<p>Theorem on primitive elements modulo pe: The number a
is a primitive element modulo pe if and only if
</p>
<p>a odd , pe = 2 ;
a mod 4 = 3 , pe = 4 ;
a mod 8 = 3,5,7 , pe = 8 ;
a mod 8 = 3,5 , p = 2 , e &gt; 3 ;
a mod p �= 0 , a(p&minus;1)/q mod p �= 1 , p &gt; 2 , e = 1 ,
</p>
<p>q every prime factor of p&minus;1 ;
a mod p �= 0 , ap&minus;1 mod p2 �= 1 , a(p&minus;1)/q mod p �= 1 ,
</p>
<p>p &gt; 2 , e &gt; 1 , q every prime factor of p&minus;1 .
(4.4.3)
</p>
<p>For large values of p the primitive elements must be determined with com-
puter programs with the aid of this theorem.
</p>
<p>Theorem on the maximum period of anMLCG: The max-
imum period of an MLCG defined by the quantities m, a, c = 0,
x0 is equal to the order λ(m). This is attained if the multiplier a
is a primitive element modulo m and when the initial value x0
and the multiplier m have no common factors (except unity).
</p>
<p>In fact, MLC generators with c= 0 are frequently used in practice. There
are two cases of practical significance in choosing the multiplier m.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.5 Quality of an MLCG: Spectral Test 47
</p>
<p>(i) m = 2e: Here m&minus; 1 can be the largest integer that can be represented
on the computer. According to (4.4.2) the maximum attainable period
length is m/4.
</p>
<p>(ii) m = p: If m is a prime number, the period of m&minus; 1 can be attained
according to (4.4.2).
</p>
<p>4.5 Quality of an MLCG: Spectral Test
</p>
<p>When producing random numbers, the main goal is naturally not just to attain
the longest possible period. This could be achieved very simply with the
sequence 0, 1, 2, . . ., m&minus; 1, 0, 1, . . .. Much more importantly, the individ-
ual elements within a period should follow each other &ldquo;randomly&rdquo;. First the
modulus m is chosen, and then one chooses various multipliers a correspond-
ing to (4.4.3) that guaranty a maximum period. One then constructs gener-
ators with the constants a, m, and c = 0 in the form of computer programs
and checks with statistical tests the randomness of the resulting numbers.
General tests, also applicable to this particular question, will be discussed
in Sect. 8. The spectral test was especially developed for investigating ran-
dom numbers, in particular for detecting non-random dependencies between
neighboring elements in a sequence.
</p>
<p>In a simple example we first consider the case a= 3, m= 7, c= 0, x0 = 1
and obtain the sequence
</p>
<p>1, 3, 2, 6, 4, 5, 1, . . . .
</p>
<p>We now form pairs of neighboring numbers
</p>
<p>(xj , xj+1) , j = 0,1, . . . ,n&minus;1 . (4.5.1)
Here n is the period, which in our example is n = m&minus; 1 = 6. In Fig. 4.1 the
number pairs (4.5.1) are represented as points in a two-dimensional Cartesian
coordinate system. We note &ndash; possibly with surprise &ndash; that they form a reg-
ular lattice. The surprise is somewhat less, however, when we consider two
features of the algorithm (4.3.2):
</p>
<p>(i) All coordinate values xj are integers. In the accessible range of values
1 &le; xj &le; n there are, however, only n2 number pairs (4.5.1) for which
both elements are integer. They lie on a lattice of horizontal and vertical
lines. Two neighboring lines have a separation of one.
</p>
<p>(ii) There are only n different pairs (4.5.1), so that only a fraction of the n2
</p>
<p>points mentioned in (i) are actually occupied.
</p>
<p>We now go from integer numbers xj to transformed numbers
</p>
<p>uj = xj/m (4.5.2)</p>
<p/>
</div>
<div class="page"><p/>
<p>48 4 Random Numbers: The Monte Carlo Method
</p>
<p>with the property
0 &lt; uj &lt; 1 . (4.5.3)
</p>
<p>For simplicity we assume that the sequence x0,x1, . . . has the maximum
possible period m for an MLC generator. The pairs
</p>
<p>Fig.4.1: Diagram of number pairs (4.5.1) for a = 3, m= 7.
</p>
<p>(uj , uj+1) , j = 0,1, . . . ,m&minus;1 , (4.5.4)
</p>
<p>lie in a square whose side has unit length. Because the xj are integers,
the spacing between the horizontal or vertical lattice lines on which the
points (4.5.4) must lie is 1/m. By far not all of these points, however,
are occupied. A finite family of lines can be constructed which pass through
those points that are actually occupied. We consider now the spacing of neigh-
boring lines within a family, look for the family for which this distance is a
maximum, and call this d2.
</p>
<p>If the distances between neighboring lattice lines for all families are
approximately equal, we can then be certain of having a maximally uni-
form distribution of the occupied lattice points on the unit square. Since this
distance is 1/m for a completely occupied lattice (m2 points), we obtain for
a uniformly occupied lattice with m points a distance of d2 &asymp;m&minus;1/2. With a
very nonuniform lattice one obtains the considerably larger value d2 ≫m&minus;1/2.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.5 Quality of an MLCG: Spectral Test 49
</p>
<p>If one now considers not only pairs (4.5.4), but t-tuples of numbers
</p>
<p>(uj ,uj+1 , . . . ,uj+t&minus;1) , (4.5.5)
</p>
<p>one sees that the corresponding points lie on families of (t &minus; 1)-dimen-
sional hyperplanes in a t-dimensional cube whose side has unit length. Let
us investigate as before the distance between neighboring hyperplanes of a
family. We determine the family with the largest spacing and designate this
by dt . One expects for a uniform distribution of points (4.5.5) a distance
</p>
<p>dt &asymp;m&minus;1/t . (4.5.6)
</p>
<p>If the lattice is nonuniform, however, we expect
</p>
<p>dt ≫m&minus;1/t . (4.5.7)
</p>
<p>The situations (4.5.6) and (4.5.7) are shown in Fig. 4.2. Naturally one tries
to achieve as uniform a lattice as possible. One should note that there is at
least a distance (4.5.6) between the lattice points. The lowest decimal places
of random numbers are therefore not random, but rather reflect the structure
of the lattice.
</p>
<p>Theoretical considerations give an upper limit on the smallest possible
lattice spacing,
</p>
<p>Fig.4.2: Diagram of number pairs (4.5.4) for various small values of a and m.</p>
<p/>
</div>
<div class="page"><p/>
<p>50 4 Random Numbers: The Monte Carlo Method
</p>
<p>Table4.1: Suitable moduli m and multipliers a for portable MLC generators for computers
with 32-bit (16-bit) integer arithmetic.
</p>
<p>32 bit 16 bit
m a m a
</p>
<p>2 147 483 647 39 373 32 749 162
2 147 483 563 40 014 32 363 157
2 147 483 399 40 692 32 143 160
2 147 482 811 41 546 32 119 172
2 147 482 801 42 024 31 727 146
2 147 482 739 45 742 31 657 142
</p>
<p>dt &ge; d&lowast;t = ctm&minus;1/t . (4.5.8)
The constants ct are of order unity. They have the numerical values [2]
</p>
<p>c2 = (4/3)&minus;1/4 , c3 = 2&minus;1/6 , c4 = 2&minus;1/4 , c5 = 2&minus;3/10 ,
c6 = (64/3)&minus;1/12 , c7 = 2&minus;3/7 , c8 = 2&minus;1/2 .
</p>
<p>(4.5.9)
The spectral test can now be carried out as follows. For given values
</p>
<p>(m,a) of the modulus and multiplier of an MLCG one determines with a
computer algorithm [2] the values dt (m,a) for small t , e.g., t = 2,3, . . . ,6.
One constructs the test quantities
</p>
<p>St (m,a)=
d&lowast;t (m)
</p>
<p>dt (m,a)
(4.5.10)
</p>
<p>and accepts the generator as usable if the St (m,a) do not exceed a given
limit. Table 4.1 gives the results of extensive investigations by L&rsquo;ECUYER
[3]. The moduli m are prime numbers close to the maximum integer values
representable by 16 or 32 bit. The multipliers are primitive elements modulo
m. They fulfill the requirement a &lt;
</p>
<p>&radic;
m (see Sect. 4.6). The prime numbers
</p>
<p>were chosen such that a does not have to be much smaller than
&radic;
m, but the
</p>
<p>condition (m,a) in Table 4.1 St (m,a) &gt; 0.65, t = 2,3, . . . ,6, still applies.
</p>
<p>4.6 Implementation and Portability of an MLCG
</p>
<p>By implementation of an algorithm one means its realization as a computer
program for a specific type of computer. If the program can be easily trans-
ferred to other computer types and gives there (essentially) the same results,
then the program is said to be portable. In this section we will give a portable
implementation of an MLCG, as realized by WICHMANN and HILL [4] and
L&rsquo;ECUYER [3].</p>
<p/>
</div>
<div class="page"><p/>
<p>4.6 Implementation and Portability of an MLCG 51
</p>
<p>A program that implements the rule (4.4.1) is certain to be portable if the
</p>
<p>computations are carried out exclusively with integers. If the computer has
</p>
<p>k bits for the representation of an integer, then all numbers between &minus;m&minus; 1
and m for m&lt; 2k&minus;1 are available.
</p>
<p>We now choose a multiplier a with
</p>
<p>a2 &lt;m (4.6.1)
</p>
<p>and define
</p>
<p>q =m div a , r =m mod a , (4.6.2)
</p>
<p>so that
</p>
<p>m= aq+ r . (4.6.3)
</p>
<p>The expression m div a defined by (4.6.2) and (4.6.3) is the integer part of
</p>
<p>the quotient m/a. We now compute the right-hand side of (4.4.1), where we
</p>
<p>leave off the index j and note that [(xdiv q)m] mod m= 0, since x div q is
an integer:
</p>
<p>[ax] mod m = [ax&minus; (x div q)m] mod m
= [ax&minus; (x div q)(aq+ r)] mod m
= [a{x&minus; (x div q)q}&minus; (x div q)r] mod m
= [a(x mod q)&minus; (x div q)r] mod m . (4.6.4)
</p>
<p>Since one always has 0 &lt; x &lt; m, it follows that
</p>
<p>a(x mod q) &lt; aq &le;m , (4.6.5)
</p>
<p>(x div q)r &lt; [(aq+ r) div q]r = ar &lt; a2 &lt;m . (4.6.6)
</p>
<p>In this way both terms in square brackets in the last line of (4.6.4) are less
</p>
<p>than m, so that the bracketed expression remains in the interval between
</p>
<p>&minus;m and m.
In the Java class we have implemented the expres-
</p>
<p>sion (4.6.4) in the following three lines, in which all variables are integer:
</p>
<p>k = x / Q;
x = A * (x &minus; k * Q) &minus; k * R;
if(x &lt; 0) x = x + M;
</p>
<p>One should note that division of two integer variables results directly in the
</p>
<p>integer part of the quotient. The first line therefore yields x div q and the last
</p>
<p>line ax mod m, respectively.
</p>
<p>The method yields a partial sequence of ran-
</p>
<p>dom numbers of length N . Each time the subroutine is called, an addi-
</p>
<p>tional partial sequence is produced. The period of the entire sequence is
</p>
<p>DatanRandom
</p>
<p>DatanRandom.mlcg</p>
<p/>
</div>
<div class="page"><p/>
<p>52 4 Random Numbers: The Monte Carlo Method
</p>
<p>m&minus; 1 = 2147483562. The computation is carried out entirely with integer
arithmetic, ensuring portability. The output values are, however, floating point
valued because of the division by m, and therefore correspond to a uniform
distribution between 0 and 1.
</p>
<p>Often one would like to interrupt a computation requiring many random
numbers and continue it later starting from the same place. In this case one can
read out and store the last computed (integer) random number directly before
the interruption, and use it later for producing the next random number. In the
technical terminology one calls such a number the seed of the generator.
</p>
<p>It is sometimes desirable to be able to produce non-overlapping partial
sequences of random numbers not one after the other but rather independently.
In this way one can, for example, carry out parts of larger simulation problems
simultaneously on several computers. As seeds for such partial sequences one
uses elements of the total sequence separated by an amount greater than the
length of each partial sequence. Such seeds can be determined without having
to run through the entire sequence. From (4.4.1) it follows that
</p>
<p>xj+n = (anxj ) mod m= [(an mod m)xj ] mod m . (4.6.7)
</p>
<p>L&rsquo;ECUYER [3] suggests setting n = 2d and choosing some seed x0. The ex-
pression a2
</p>
<p>d
mod m can be computed by beginning with a and squaring it d
</p>
<p>times modulo m. Then one computes xn using (4.6.7) and obtains correspond-
ingly x2n, x3n, . . . .
</p>
<p>4.7 Combination of Several MLCGs
</p>
<p>Since the period of an MLCG is at most m&minus; 1, and since m is restricted to
m&lt; 2k&minus;1 &minus;1 where k is the number of bits available in the computer for the
representation of an integer, only a relatively short period can be attained with
a single MLCG. WICHMANN and HILL [4] and L&rsquo;ECUYER [3] have given a
procedure for combining several MLCGs, which allows for very long periods.
The technique is based on the following two theorems.
</p>
<p>Theorem on the sum of discrete random variables, one of
</p>
<p>which comes from a discrete uniform distribution: If x1, . . . ,xℓ
are independent random variables that can only assume integer
values, and if x1 follows a discrete uniform distribution, so that
</p>
<p>P (x1 = n)=
1
</p>
<p>d
, n= 0,1, . . . ,d&minus;1 ,</p>
<p/>
</div>
<div class="page"><p/>
<p>4.7 Combination of Several MLCGs 53
</p>
<p>then
</p>
<p>x =
</p>
<p>⎛
⎝
</p>
<p>ℓ&sum;
</p>
<p>j=1
xj
</p>
<p>⎞
⎠ mod d (4.7.1)
</p>
<p>also follows this distribution.
</p>
<p>We first demonstrate the proof for ℓ = 2, using the abbreviations min
(x2)= a, max(x2)= b. One has
</p>
<p>P (x = n) =
&infin;&sum;
</p>
<p>k=0
P (x1 +x2 = n+ kd)
</p>
<p>=
b&sum;
</p>
<p>i=a
P (x2 = i)P (x1 = (n&minus; i) mod d)
</p>
<p>= 1
d
</p>
<p>b&sum;
</p>
<p>i=a
P (x2 = i)=
</p>
<p>1
</p>
<p>d
.
</p>
<p>For ℓ= 3 we first construct the variable x&prime;1 = x1+x2, which follows a discrete
uniform distribution between 0 and d&minus;1, and then the sum x&prime;1+x3, which has
only two terms and therefore possesses the same property. The generalization
for ℓ &gt; 3 is obvious.
</p>
<p>Theorem on the period of a family of generators: Con-
sider the random variables xj,i coming from a generator j with
a period pj , so that the generator gives a sequence xj,0,xj,1, . . . ,
xj,pj&minus;1 . We consider now ℓ generators j = 1,2, . . . ,ℓ and the
sequence of ℓ-tuples
</p>
<p>xi = {x1,i,x2,i, . . . ,xℓ,i} , i = 0,1, . . . . (4.7.2)
</p>
<p>Its period p is the smallest common multiple of the periods
p1,p2, . . . ,pℓ of the individual generators. The proof is obtained
directly from the fact that p is clearly a multiple of each pj .
</p>
<p>We now determine the maximum value of the period p. If the ℓ individual
MLCGs have prime numbersmj as moduli, then their periods are pj =mj&minus;1
and are therefore even. Therefore one has
</p>
<p>p &le;
&prod;ℓ
</p>
<p>j=1(mj &minus;1)
2ℓ&minus;1
</p>
<p>. (4.7.3)
</p>
<p>Equality results if the quantities (mj &minus;1)/2 possess no common factors.
The first theorem of this section can now be used to construct a sequence
</p>
<p>with period given by (4.7.3). One forms first the integer quantity</p>
<p/>
</div>
<div class="page"><p/>
<p>54 4 Random Numbers: The Monte Carlo Method
</p>
<p>zi =
</p>
<p>⎛
⎝
</p>
<p>ℓ&sum;
</p>
<p>j=1
(&minus;1)j&minus;1xj,i
</p>
<p>⎞
⎠ mod (m1 &minus;1) . (4.7.4)
</p>
<p>The alternating sign in (4.7.4), which simplifies the construction of the
modulus function, does not contradict the prescription of (4.7.1), since one
could also use in place of x2,x4, . . ., the variables x &prime;2 = &minus;x2, x &prime;4 = &minus;x4, . . ..
The quantity zi can take on the values
</p>
<p>zi &isin; {0,1, . . . ,m1 &minus;2} . (4.7.5)
The transformation to floating point numbers
</p>
<p>ui =
{
</p>
<p>zi/m1 , zi &gt; 0
(m1 &minus;1)/m1 , zi = 0
</p>
<p>(4.7.6)
</p>
<p>gives values in the range 0 &lt; ui &lt; 1.
In the method we use the techniques, assem-
</p>
<p>bled above, to produce uniformly distributed random numbers with a long
period. We combine two MLCGs with m1 = 2147483563, a1 = 40014,
m2 = 2147483399, a2 = 40692. The numbers (m1 &minus; 1)/2 and (m2 &minus; 1)/2
have no common factor. Therefore the period of the combined generator is,
according to (4.7.3),
</p>
<p>p = (m1 &minus;1)(m2 &minus;1)/2 &asymp; 2.3 &middot;1018 .
The absolute values of all integers occurring during the computation remain
in the range &le; 231 &minus;85. The resulting floating point values u are in the range
0 &lt;u&lt; 1. One does not obtain the values 0 or 1, at least if 23 or more bits are
available for the mantissa, which is almost always the case when represent-
ing floating point numbers with 32 bit. The program with the given values of
m1,m2,a1,a2 has been subjected to the spectral test and to many other tests
by L&rsquo;ECUYER [3], who has provided a PASCAL version. He determined that
it satisfied all of the requirements of the tests.
</p>
<p>Figure 4.3 illustrates the difference between the simple MLCG and the
combined generator. For the simple MLCG one can still recognize a struc-
ture in a scatter plot of the number pairs (4.5.4), although with an expansion
of the abscissa by a factor of 1000. The corresponding diagram for the com-
bined generator appears, in contrast, to be completely without structure. For
each diagram one million pairs of random numbers were generated. The plots
correspond only to a narrow strip on the left-hand edge of the unit square.
</p>
<p>In order to initialize non-overlapping partial sequences one can use two
methods:
</p>
<p>(i) One applies the procedure discussed in connection with (4.6.7) to both
MLCGs, naturally with the same value n, in order to construct pairs of
seeds for each partial sequence.
</p>
<p>DatanRandom.ecuy</p>
<p/>
</div>
<div class="page"><p/>
<p>4.8 Generation of Arbitrarily Distributed Random Numbers 55
</p>
<p>(ii) It is considerably easier to use the same seed for the first MLCG for
every partial sequence. For the second MLCG one uses an arbitrary
seed for the first partial sequence, the following random number from
the second MLCG for the second partial sequence, etc. In this way one
obtains partial sequences that can reach a length of (m1 &minus; 1) without
overlapping.
</p>
<p>Fig.4.3: Scatter plots of number pairs (4.5.4) from (a) a MLC generator and (b) a combined
</p>
<p>4.8 Generation of Arbitrarily Distributed Random
</p>
<p>Numbers
</p>
<p>4.8.1 Generation by Transformation of the Uniform Distribution
</p>
<p>If x is a random variable following the uniform distribution,
</p>
<p>f (x)= 1 , 0 &le; x &lt; 1 ; f (x)= 0 , x &lt; 0 , x &ge; 1 , (4.8.1)
and y is a random variable described by the probability density g(y), the trans-
formation (3.7.1) simplifies to
</p>
<p>g(y)dy = dx . (4.8.2)
</p>
<p>generator. The methods
</p>
<p>were used in the generation.
</p>
<p>DatanRandom.mclg and DatanRandom.ecuy, respectively,</p>
<p/>
</div>
<div class="page"><p/>
<p>56 4 Random Numbers: The Monte Carlo Method
</p>
<p>We use the distribution function G(y), which is related to g(y) through
dG(y)/dy = g(y), and write (4.8.2) in the form
</p>
<p>dx = g(y)dy = dG(y) , (4.8.3)
or after integration,
</p>
<p>x =G(y)=
&int; y
</p>
<p>&minus;&infin;
g(t)dt . (4.8.4)
</p>
<p>This equation has the following meaning. If a random number x is taken from
a uniform distribution between 0 and 1 and the function x =G(y) is inverted,
</p>
<p>y =G&minus;1(x) , (4.8.5)
then one obtains a random number y described by the probability density
g(y). The relationship is depicted in Fig. 4.4a. The probability to obtain a
random number x between x and x+ dx is equal to the probability to have a
value y(x) between y and y+dy.
</p>
<p>dx=g(y)dy
</p>
<p>y1 yny2
</p>
<p>yny2y1
</p>
<p>y(x)
y
</p>
<p>P(yj)
</p>
<p>y
</p>
<p>x = G(y)
</p>
<p>P(yk)xj =&sum;
k = 1
</p>
<p>y
</p>
<p>g(y)
</p>
<p>y
</p>
<p>x = G(y) =  g(t)dt
&minus;&infin;
</p>
<p>y
</p>
<p>11
x = G(y) j
</p>
<p>&int;
</p>
<p>(a) (b)
</p>
<p>Fig.4.4: Transformation from a uniformly distributed variable x to a variable y with the dis-
tribution function G(y). The variable y can be continuous (a) or discrete (b).
</p>
<p>The relationship (4.8.4) can be also be used to produce discrete probability
distributions. An example is shown in Fig.4.4b. The random variable y can take
on the valuesy1,y2, . . . ,yn with the probabilitiesP (y1),P (y2), . . . ,P (yn). The
distribution function as given by (3.2.1) is G(y)= P (y &lt; y). The construction
of a step function x =G(y) according to this equation gives the values
</p>
<p>xj =G(yj )=
i&sum;
</p>
<p>k=1
P (yk) , (4.8.6)</p>
<p/>
</div>
<div class="page"><p/>
<p>4.8 Generation of Arbitrarily Distributed Random Numbers 57
</p>
<p>which lie in the range between 0 and 1. From this one can produce random
numbers according to a discrete distribution G(y) by first producing random
numbers x uniformly distributed between 0 and 1. Depending on the interval
j in which x falls, xj&minus;1 &lt; x &lt; xj , the number yj is then produced.
</p>
<p>Example 4.1: Exponentially distributed random numbers
</p>
<p>We would like to generate random numbers according to the probability
density
</p>
<p>g(t)=
</p>
<p>⎧
⎪⎨
⎪⎩
</p>
<p>1
</p>
<p>τ
e&minus;t/τ , t &ge; 0 ,
</p>
<p>0 , t &lt; 0 .
</p>
<p>(4.8.7)
</p>
<p>This is the probability density describing the time t of the decay of a ra-
dioactive nucleus that exists at time t = 0 and has a mean lifetime τ . The
distribution function is
</p>
<p>x =G(t)= 1
τ
</p>
<p>&int; t
</p>
<p>t &prime;=0
g(t &prime;)dt &prime; = 1&minus; e&minus;t/τ . (4.8.8)
</p>
<p>According to (4.8.4) and (4.8.5) we can obtain exponentially distributed ran-
dom numbers t by first generating random numbers uniformly distributed
between 0 and 1 and then finding the inverse function t =G&minus;1(x), i.e.,
</p>
<p>t =&minus;τ ln(1&minus;x) .
</p>
<p>Since 1&minus; x is also uniformly distributed between 0 and 1, it is sufficient to
compute
</p>
<p>t =&minus;τ lnx . (4.8.9)
</p>
<p>Example 4.2: Generation of random numbers following a Breit&ndash;Wigner
distribution
</p>
<p>To generate random numbers y which follow a Breit&ndash;Wigner distribution
(3.3.32),
</p>
<p>g(y)= 2
πΓ
</p>
<p>Γ 2
</p>
<p>4(y&minus;a)2 +Γ 2 ,
</p>
<p>we proceed as discussed in Sect. 4.8.1. We form the distribution function
</p>
<p>x =G(y)=
&int; y
</p>
<p>&minus;&infin;
g(y)dy = 2
</p>
<p>πΓ
</p>
<p>&int; y
</p>
<p>&minus;&infin;
</p>
<p>Γ 2
</p>
<p>4(y&minus;a)2 +Γ 2 dy
</p>
<p>and perform the integration using the substitution
</p>
<p>u= 2(y&minus;a)
Γ
</p>
<p>, du= 2
Γ
</p>
<p>dy .</p>
<p/>
</div>
<div class="page"><p/>
<p>58 4 Random Numbers: The Monte Carlo Method
</p>
<p>Thus we obtain
</p>
<p>x = G(y)= 1
π
</p>
<p>&int; θ=2(y&minus;a)/Γ
</p>
<p>θ=&minus;&infin;
</p>
<p>1
</p>
<p>1+u2 du=
1
</p>
<p>π
[arctanu]2(y&minus;a)/Γ&minus;&infin;
</p>
<p>= arctan2(y&minus;a)/Γ
π
</p>
<p>+ 1
2
</p>
<p>.
</p>
<p>By inversion we obtain
</p>
<p>2(y&minus;a)/Γ = tan
{
π
</p>
<p>(
x&minus; 1
</p>
<p>2
</p>
<p>)}
,
</p>
<p>y = a+ Γ
2
</p>
<p>tan
</p>
<p>{
π
</p>
<p>(
x&minus; 1
</p>
<p>2
</p>
<p>)}
. (4.8.10)
</p>
<p>If x are random numbers uniformly distributed in the interval 0 &lt; x &lt; 1, then
y follows a Breit&ndash;Wigner distribution.
</p>
<p>Example 4.3: Generation of random numbers with a triangular distribution
</p>
<p>In order to generate random numbers y following a triangular distribution as
in Problem 3.2 we form the distribution function
</p>
<p>F(y)=
</p>
<p>⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
</p>
<p>0 , y &lt; a ,
(y&minus;a)2
</p>
<p>(b&minus;a)(c&minus;a) , a &le; y &lt; c ,
</p>
<p>1&minus; (y&minus;b)
2
</p>
<p>(b&minus;a)(b&minus; c) , c &le; y &lt; b ,
1 , b &le; y .
</p>
<p>In particular we have
</p>
<p>F(c)= c&minus;a
b&minus;a .
</p>
<p>Inverting x = F(y) gives
</p>
<p>y = a+
&radic;
(b&minus;a)(c&minus;a)x , x &lt; (c&minus;a)/(b&minus;a) ,
</p>
<p>y = b&minus;
&radic;
(b&minus;a)(b&minus; c)(1&minus;x) , x &ge; (c&minus;a)/(b&minus;a) .
</p>
<p>If x is uniformly distributed with 0 &lt; x &lt; 1, then y follows a triangular distri-
bution.
</p>
<p>4.8.2 Generation with the von Neumann
</p>
<p>Acceptance&ndash;Rejection Technique
</p>
<p>The elegant technique of the previous section requires that the distribution
function x =G(y) be known and that the inverse function y =G&minus;1(x) exists
and be known as well.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.8 Generation of Arbitrarily Distributed Random Numbers 59
</p>
<p>Often one only knows the probability density g(y). One can then use the
VON NEUMANN acceptance&ndash;rejection technique, which we introduce with a
simple example before discussing it in its general form.
</p>
<p>Example 4.4: Semicircle distribution with the simple acceptance&ndash;rejection
method
</p>
<p>As a simple example we generate random numbers following a semicircular
probability density,
</p>
<p>g(y)=
{
</p>
<p>(2/πR2)
&radic;
R2 &minus;y2, |y| &le; R ,
</p>
<p>0, |y|&gt;R . (4.8.11)
</p>
<p>Instead of trying to find and invert the distribution function G(y), we gener-
ate pairs of random numbers (yi,ui). Here yi is uniformly distributed in the
interval available to y, &minus;R &le; y &le; R, and ui is uniformly distributed in the
range of values assumed by the function g(y), 0 &le; u &le; R. For each pair we
test if
</p>
<p>ui &ge; g(yi) . (4.8.12)
If this inequality is fulfilled, we reject the random number yi . The set of ran-
dom numbers yi that are not rejected then follow a probability density g(y),
since each was accepted with a probability proportional to g(yi).
</p>
<p>The technique of Example 4.4 can easily be described geometrically.
To generate random numbers in the interval a &le; y &le; b according to the prob-
ability density g(y), one must consider in the region a &le; y &le; b the curve
</p>
<p>u= g(y) (4.8.13)
</p>
<p>and a constant
</p>
<p>u= d , d &ge; gmax , (4.8.14)
</p>
<p>which is greater than or equal to the maximum value of g(y) in that region.
In the (y,u) plane this constant is described by the line u= d. Pairs of random
numbers (yi,ui) uniformly distributed in the interval a &le; yi &le; b, 0 &le; ui &le; d
correspond to a uniform distribution of points in the corresponding rectangle
of the (y,u)-plane. If all of the points for which (4.8.12) holds are rejected,
then only points under the curve u= g(y) remain. Figure 4.5 shows this situ-
ation for the Example 4.4. [It is clear that the technique also gives meaningful
results if the function is not normalized to one. In Fig. 4.5 we have simply set
g(y)=
</p>
<p>&radic;
R2 &minus;y2 and R = 1.]
</p>
<p>For the transformation technique of Sect. 4.8.1, each random number yi
required only that exactly one random number xi be generated from a uniform</p>
<p/>
</div>
<div class="page"><p/>
<p>60 4 Random Numbers: The Monte Carlo Method
</p>
<p>distribution and that it be transformed according to (4.8.5). In the acceptance&ndash;
rejection technique, pairs yi,ui must always be generated, and a consider-
able fraction of the numbers yi &ndash; depending on the value of ui according
to (4.8.12) &ndash; are rejected. The probability for yi to be accepted is
</p>
<p>E =
&int; b
a
g(y)dy
</p>
<p>(b&minus;a)d . (4.8.15)
</p>
<p>Fig.4.5:All the pairs (yi,ui) produced are marked as points in the (y,u)-plane. Points above
the curve u= g(y) (small points) are rejected.
</p>
<p>We can call E the efficiency of the procedure. If the interval a&le; y &le; b includes
the entire allowed range of y, then the numerator of (4.8.15) is equal to unity,
and one obtains
</p>
<p>E = 1
(b&minus;a)d . (4.8.16)
</p>
<p>The numerator and denominator of (4.8.15) are simply the areas con-
tained in the region a &le; y &le; b under the curves (4.8.13) and (4.8.14), respec-
tively. One distributes points (yi,ui) uniformly under the curve (4.8.14) and
rejects the random numbers yi if the inequality (4.8.12) holds. The efficiency
of the procedure is certainly higher if one uses as the upper curve not the
constant (4.8.14) but rather a curve that is closer to g(y).
</p>
<p>With this in mind the acceptance&ndash;rejection technique can be stated in its
general form:
</p>
<p>(i) One finds a probability density s(y) that is sufficiently simple that
random numbers can be generated from it using the transformation
method, and a constant c such that</p>
<p/>
</div>
<div class="page"><p/>
<p>4.8 Generation of Arbitrarily Distributed Random Numbers 61
</p>
<p>g(y)&le; c &middot; s(y) , a &lt; y &lt; b , (4.8.17)
</p>
<p>holds.
</p>
<p>(ii) One generates one random number y uniformly distributed in the inter-
val a &lt; y &lt; b and a second random number u uniformly distributed in
the interval 0 &lt; u &lt; 1.
</p>
<p>(iii) One rejects y
</p>
<p>u &ge; g(y)
c &middot; s(y) . (4.8.18)
</p>
<p>After the points (ii) and (iii) have been repeated enough times, the resulting
set of accepted random numbers y follows the probability density g(y), since
</p>
<p>P (y &lt; y)=
&int; y
</p>
<p>a
</p>
<p>s(t)
g(t)
</p>
<p>c &middot; s(t) dt =
1
</p>
<p>c
</p>
<p>&int; y
</p>
<p>a
</p>
<p>g(t)dt = 1
c
[G(y)&minus;G(a)] .
</p>
<p>If the interval a &le; y &le; b includes the entire range of y for both g(y) as well
as for s(y), then one obtains an efficiency
</p>
<p>E = 1
c
</p>
<p>. (4.8.19)
</p>
<p>Example 4.5: Semicircle distribution with the general acceptance&ndash;rejection
method
</p>
<p>One chooses for c &middot; s(y) the polygon
</p>
<p>c &middot; s(y)=
</p>
<p>⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
</p>
<p>0 , y &lt;&minus;R ,
3R/2+y , &minus;R &le; y &lt;&minus;R/2 ,
R , &minus;R/2 &le; y &lt; R/2 ,
3R/2&minus;y , R/2 &le; y &lt; R ,
0 , R &le; y .
</p>
<p>The efficiency is clearly
</p>
<p>E = πR
2
</p>
<p>2
&middot; 1
</p>
<p>2R2 &minus;R2/4 =
2π
</p>
<p>7
</p>
<p>in comparison to
</p>
<p>E = πR
2
</p>
<p>2
&middot; 1
</p>
<p>2R2
= π
</p>
<p>4
as in Example 4.4.</p>
<p/>
</div>
<div class="page"><p/>
<p>62 4 Random Numbers: The Monte Carlo Method
</p>
<p>4.9 Generation of Normally Distributed Random Numbers
</p>
<p>By far the most important distribution for data analysis is the normal distribu-
tion, which we will discuss in Sect. 5.7. We present here a program that can
produce random numbers xi following the standard normal distribution with
the probability density
</p>
<p>f (x)= 1&radic;
2π
</p>
<p>e&minus;x
2/2 . (4.9.1)
</p>
<p>The corresponding distribution function F(x) can only be computed and in-
verted numerically (Appendix C). Therefore the simple transformation method
of Sect. 4.8.1 cannot be used. The polar method by BOX and MULLER [5]
described here combines in an elegant way acceptance&ndash;rejection with trans-
formation. The algorithm consists of the following steps:
</p>
<p>(i) Generate two independent random numbers u1,u2 from a uniform dis-
tribution between 0 and 1. Transform v1 = 2u1 &minus;1, v2 = 2u2 &minus;1.
</p>
<p>(ii) Compute s = v21 +v22.
</p>
<p>(iii) If s &ge; 1, return to step (i).
</p>
<p>(iv) x1 = v1
&radic;
&minus;(2/s) lns and x2 = v2
</p>
<p>&radic;
&minus;(2/s) lns are two independent
</p>
<p>random numbers following the standard normal distribution.
</p>
<p>The number pairs (v1,v2) obtained from step (i) are the Cartesian coor-
dinates of a set of points uniformly distributed inside the unit circle. We can
write them as v1 = rcosθ, v2 = rsinθ using the polar coordinates r=
</p>
<p>&radic;
s, θ =
</p>
<p>arctan(v2/v1). The point (x1,x2) then has the Cartesian coordinates
</p>
<p>x1 = cosθ
&radic;
&minus;2lns , x2 = sinθ
</p>
<p>&radic;
&minus;2lns .
</p>
<p>We now ask for the probability
</p>
<p>F(r) = P (
&radic;
&minus;2lns &le; r)= P (&minus;2lns &le; r2)
</p>
<p>= P (s &gt; e&minus;r2/2) .
</p>
<p>Since s = r2 is by construction uniformly distributed between 0 and 1, one
has
</p>
<p>F(r)= P (s &gt; e&minus;r2/2)= 1&minus; e&minus;r2/2 .
The probability density of r is
</p>
<p>f (r)= dF(r)
dr
</p>
<p>= re&minus;r2/2 .</p>
<p/>
</div>
<div class="page"><p/>
<p>4.10 Random Numbers According to a Multivariate Normal Distribution 63
</p>
<p>The joint distribution function of x1 and x2,
</p>
<p>F(x1,x2) = P (x1 &le; x1, x2 &le; x2)= P (rcosθ &le; x1, rsinθ &le; x2)
</p>
<p>= 1
2π
</p>
<p>&int; &int;
</p>
<p>(x1&lt;x1,x2&lt;x2)
</p>
<p>re&minus;r
2/2 dr dϕ
</p>
<p>= 1
2π
</p>
<p>&int; &int;
</p>
<p>(x1&lt;x1,x2&lt;x2)
</p>
<p>e&minus;(x
2
1+x22 )/2 dx dy
</p>
<p>=
(
</p>
<p>1&radic;
2π
</p>
<p>&int; x1
&minus;&infin;
</p>
<p>e&minus;x
2
1/2 dx1
</p>
<p>)(
1&radic;
2π
</p>
<p>&int; x2
&minus;&infin;
</p>
<p>e&minus;x
2
2/2 dx2
</p>
<p>)
,
</p>
<p>and illustrated in Fig. 4.6.
</p>
<p>Fig.4.6: Illustration of the Box&ndash;Muller procedure. (a) Number pairs (v1,v2) are gener-
ated that uniformly populate the square. Those pairs are then rejected that do not lie in-
side the unit circle (marked by small points). (b) This is followed by the transformation
(v1,v2)&rarr; (x1,x2) .
</p>
<p>Many other procedures are described in the literature for the generation
of normally distributed random numbers. They are to a certain extent more
efficient, but are generally more difficult to program than the BOX&ndash;MULLER
procedure.
</p>
<p>4.10 Generation of Random Numbers According
</p>
<p>to a Multivariate Normal Distribution
</p>
<p>The probability density of a multivariate normal distribution of n variables
x= (x1,x2, . . . ,xn) is according to (5.10.1)
</p>
<p>is the product of two distribution functions of the standard normal distribution.
</p>
<p>The procedure is implemented in the method DatanRandom.standard-Normal</p>
<p/>
</div>
<div class="page"><p/>
<p>64 4 Random Numbers: The Monte Carlo Method
</p>
<p>φ(x)= k exp
{
&minus;1
</p>
<p>2
(x&minus;a)TB(x&minus;a)
</p>
<p>}
.
</p>
<p>Here a is the vector of expectation values and B = C&minus;1 is the inverse of the
positive-definite symmetric covariance matrix. With the Cholesky decompo-
sition B =DTD and the substitution u=D(x&minus;a) the exponent takes on the
simple form
</p>
<p>&minus;1
2
uTu=&minus;1
</p>
<p>2
(u21 +u22 +&middot;&middot; &middot;+u2n) .
</p>
<p>Thus the elements ui of the vectors u follow independent standard normal
distributions [cf. (5.10.9)]. One obtains vectors x of random numbers by
first forming a vector u of elements ui which follow the standard normal
distribution and then performing the transformation
</p>
<p>x =D&minus;1u+a .
</p>
<p>4.11 The Monte Carlo Method for Integration
</p>
<p>It follows directly from its construction that the acceptance&ndash;rejection tech-
nique, Sect. 4.8.2, provides a very simple method for numerical integration.
If N pairs of random numbers (y1,ui), i = 1,2, . . . ,N are generated accord-
ing to the prescription of the general acceptance&ndash;rejection technique, and if
N &minus; n of them are rejected because they fulfill condition (4.8.18), then the
numbers N (or n) are proportional to the areas under the curves c &middot; s(y) (or
g(y)), at least in the limit of large N , i.e.,
</p>
<p>&int; b
a
g(y)dy
</p>
<p>c
&int; b
a
s(y)dy
</p>
<p>= lim
N&rarr;&infin;
</p>
<p>n
</p>
<p>N
. (4.11.1)
</p>
<p>Since the function s(y) is chosen to be particularly simple [in the simplest
case one has s(y)= 1/(b&minus;a)], the ratio n/N is a direct measure of the value
of the integral
</p>
<p>I =
&int; b
</p>
<p>a
</p>
<p>g(y)dy =
(
</p>
<p>lim
N&rarr;&infin;
</p>
<p>n
</p>
<p>N
</p>
<p>)
c
</p>
<p>&int; b
</p>
<p>a
</p>
<p>s(y)dy . (4.11.2)
</p>
<p>Here the integrand g(y) does not necessarily have to be normalized, i.e., one
does not need to require &int; &infin;
</p>
<p>&minus;&infin;
g(y)dy = 1
</p>
<p>as long as c is chosen such that (4.8.17) is fulfilled.
</p>
<p>This procedure is implemented in the method DatanRandom.multivariate
</p>
<p>Normal.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.11 The Monte Carlo Method for Integration 65
</p>
<p>Example 4.6: Computation of π
</p>
<p>Referring to Example 4.4 we compute the integral using (4.8.11) with R = 1:
</p>
<p>I =
&int; 1
</p>
<p>0
g(y)dy = π/4 .
</p>
<p>Choosing s(y)= 1 and c = 1 we obtain
</p>
<p>I = lim
N&rarr;&infin;
</p>
<p>n
</p>
<p>N
.
</p>
<p>We expect that when N points are distributed according to a uniform
distribution in the square 0 &le; y &le; 1, 0 &le; u&le; 1, and when n of them lie inside
the unit circle, then the ratio n/N approaches the value I = π/4 in the limit
N &rarr;&infin;. Table 4.2 shows the results for various values of n and for various
sequences of random numbers. The exact value of n/N clearly depends on
the particular sequence. In Sect. 6.8 we will determine that the typical fluctu-
ations of the number n are approximately Δn = &radic;n. Therefore one has for
the relative precision for the determination of the integral (4.11.2)
</p>
<p>ΔI
</p>
<p>I
= Δn
</p>
<p>n
= 1&radic;
</p>
<p>n
. (4.11.3)
</p>
<p>We expect therefore in the columns of Table 4.2 to find the value of π with
precisions of 10, 1, and 0.1%. We find in fact in the three columns fluctuations
in the first, second, and third places after the decimal point.
</p>
<p>Table4.2: Numerical values of 4n/N for various values of n. The entries in the columns
correspond to various sequences of random numbers.
</p>
<p>4n/N
n= 102 n= 104 n= 106
3.419 3.122 3.141
3.150 3.145 3.143
3.279 3.159 3.144
3.419 3.130 3.143
</p>
<p>The Monte Carlo method of integration can now be implemented by a
very simple program. For integration of single variable functions it is usu-
ally better to use other numerical techniques for reasons of computing time.
For integrals with many variables, however, the Monte Carlo method is more
straightforward and often faster as well.</p>
<p/>
</div>
<div class="page"><p/>
<p>66 4 Random Numbers: The Monte Carlo Method
</p>
<p>4.12 The Monte Carlo Method for Simulation
</p>
<p>Many real situations that are determined by statistical processes can be
simulated in a computer with the aid of random numbers. Examples are
automobile traffic in a given system of streets or the behavior of neutrons
in a nuclear reactor. The Monte Carlo method was originally developed for
the latter problem by VON NEUMANN and ULAM. A change of the parame-
ters of the distributions corresponds then to a change in the actual situation.
In this way the effect of additional streets or changes in the reactor can be
investigated without having to undertake costly and time consuming changes
in the real system. Not only processes of interest following statistical laws can
be simulated with the Monte Carlo method, but also the measurement errors
which occur in every measurement.
</p>
<p>Example 4.7: Simulation of measurement errors of points on a line
</p>
<p>We consider a line in the (t,y)-plane. It is described by the equation
</p>
<p>y = at+b . (4.12.1)
</p>
<p>If we choose discrete values of t
</p>
<p>t0 , t1 = t0 +Δt , t2 = t0 +2Δt , . . . , (4.12.2)
</p>
<p>then they correspond to values of y
</p>
<p>yi = ati +b , i = 0,1, . . . ,n&minus;1 . (4.12.3)
</p>
<p>We assume that the values t0, t1, . . . of the &ldquo;controlled variable&rdquo; t can be set
without error. Because of measurement errors, however, instead of the values
yi , one obtains different values
</p>
<p>y &prime;i = yi + εi . (4.12.4)
</p>
<p>Here εi are the measurement errors, which follow a normal distribution with
mean of zero and standard deviation σy (cf. Sect. 5.7).
</p>
<p>(ti,y
&prime;
i). Figure 4.7 as an example
</p>
<p>displays 10 simulated points.
</p>
<p>Example 4.8: Generation of decay times for a mixture of two different
radioactive substances
</p>
<p>At time t = 0 a source consists of N radioactive nuclei of which aN decay
with a lifetime τ1 and (a &minus; 1)N with a mean lifetime τ2, with 0 &le; a &le; 1.
Random numbers for two different problems must be used in the simulation
the decay times occurring: for the choice of the type of nucleus and for the
determination of the decay time of the nucleus chosen, cf. (4.8.9). The method
</p>
<p> The method Datan-
Random.line generates number pairs
</p>
<p>DatanRandom.radio implements this example.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.13 Java Classes and Example Programs 67
</p>
<p>Fig.4.7: Line in the (t,y)-plane and simulated measured values with errors in y.
</p>
<p>of measurement points, scattering about a straight line
Example 4.7 is realized. Parameter input is interactive, output both numerical and
graphical.
</p>
<p>4.13 Java Classes and Example Programs
</p>
<p>Java Class for the Generation of Random Numbers
</p>
<p>contains methods for the generation of random numbers
</p>
<p>following various distributions, in particular
</p>
<p>for the uniform, for the stan-
</p>
<p>dard normal, and for the
</p>
<p>multivariate normal Distribution. Further methods are used to illustrate
</p>
<p>a simple MLC generator or to demonstrate the following examples.
</p>
<p>Example Program 4.1: The class demonstrates the generation
</p>
<p>of random numbers
</p>
<p>One can choose interactively between three generators. After clicking on Go 100
</p>
<p>random numbers are generated and displayed. The seeds before and after generation
</p>
<p>are shown and can be changed interactively.
</p>
<p>Example Program 4.2: The class demonstrates the generation
</p>
<p>DatanRandom.ecuy
</p>
<p>DatanRandom.standard-Normal
</p>
<p>DatanRandom.multivariateNormal
</p>
<p>DatanRandom
</p>
<p>E1Random
</p>
<p>E andom2R</p>
<p/>
</div>
<div class="page"><p/>
<p>68 4 Random Numbers: The Monte Carlo Method
</p>
<p>Example Program 4.3:
</p>
<p>of decay times
Example 4.8 is realized. Parameter input is interactive, output in form of a histogram.
</p>
<p>Example Program 4.4:
</p>
<p>of random numbers from a multivariate normal distribution
The procedure of Sect. 4.10 is realized for the case of two variables. Parameter input
is interactive. The generated number pairs are displayed numerically.
</p>
<p>The class demonstrates the simulationE andom3R
</p>
<p>The class demonstrates the generationE andom4R</p>
<p/>
</div>
<div class="page"><p/>
<p>5. Some Important Distributions and Theorems
</p>
<p>We shall now discuss in detail some specific distributions. This chapter
could therefore be regarded as a collection of examples. These distributions,
however, are of great practical importance and are often encountered in many
applications. Moreover, their study will lead us to a number of important
theorems.
</p>
<p>5.1 The Binomial and Multinomial Distributions
</p>
<p>Consider an experiment having only two possible outcomes. The sample
space can therefore be expressed as
</p>
<p>E = A+ Ā (5.1.1)
</p>
<p>with the probabilities
</p>
<p>P (A)= p , P (Ā)= 1&minus;p = q . (5.1.2)
</p>
<p>One now performs n independent trials of the experiment defined by (5.1.1).
One wishes to find the probability distribution for the quantity x =
</p>
<p>&sum;n
i=1 xi ,
</p>
<p>where one has xi = 1 (or 0) when A (or Ā) occurs as the result of the ith
experiment.
</p>
<p>The probability that the first k trials result in A and all of the rest in Ā is,
using Eq. (2.3.8),
</p>
<p>pk qn&minus;k .
</p>
<p>S. Brandt, Data Analysis: Statistical and Computational Methods for Scientists and Engineers,
DOI 10.1007/978-3-319-03762-2__5, &copy; Springer International Publishing Switzerland 2014
</p>
<p>69</p>
<p/>
</div>
<div class="page"><p/>
<p>70 5 Some Important Distributions and Theorems
</p>
<p>Using the rules of combinatorics, the event &ldquo;outcome A k times in n trials&rdquo;
</p>
<p>occurs in
</p>
<p>(
n
</p>
<p>k
</p>
<p>)
= n!
</p>
<p>k!(n&minus; k)! different ways, according to the order of the
</p>
<p>occurrences of A and Ā (see Appendix B). The probability of this event is
therefore
</p>
<p>P (k)=W nk =
(
</p>
<p>n
</p>
<p>k
</p>
<p>)
pk qn&minus;k . (5.1.3)
</p>
<p>We are interested in the mean value and variance of x. We first find these quan-
tities for the variable xi of an individual event. According to (3.3.2) one has
</p>
<p>E(xi)= 1 &middot;p+0 &middot;q (5.1.4)
</p>
<p>and
</p>
<p>σ 2(xi) = E{(xi &minus;p)2} = (1&minus;p)2p+ (0&minus;p)2q ,
σ 2(xi) = pq . (5.1.5)
</p>
<p>From the generalization of (3.5.3) for x =
&sum;
</p>
<p>xi it follows that
</p>
<p>E(x)=
n&sum;
</p>
<p>i=1
p = np , (5.1.6)
</p>
<p>and from (3.5.10), since all of the covariances vanish because the xi are
independent, one has
</p>
<p>σ 2(x)= npq . (5.1.7)
</p>
<p>Figure 5.1 shows the distributionWnk for various n and for fixed p, and Fig. 5.2
shows it for fixed n and various values of p. Finally in Fig. 5.3 n and p are
both varied but the product np is held constant. The figures will help us to see
relationships between the binomial distribution (5.1.3) and other distributions.
</p>
<p>A logical extension of the binomial distribution deals with experiments
where more than two different outcomes are possible. Equation (5.1.1) is then
replaced by
</p>
<p>E =A1 +A2 +&middot;&middot; &middot;+Aℓ . (5.1.8)
</p>
<p>Let the probability for the outcome Aj be
</p>
<p>P (Aj )= pj ,
ℓ&sum;
</p>
<p>j=1
pj = 1 . (5.1.9)
</p>
<p>We consider again n trials and ask for the probability that the outcome Aj
occurs kj times. This is given by</p>
<p/>
</div>
<div class="page"><p/>
<p>5.1 The Binomial and Multinomial Distributions 71
</p>
<p>Fig.5.1: Binomial distribution for various values of n, with p fixed.
</p>
<p>W n(k1,k2,...,kℓ) =
n!
</p>
<p>&prod;ℓ
j=1 kj !
</p>
<p>ℓ&prod;
</p>
<p>j=1
p
kj
</p>
<p>j ,
</p>
<p>ℓ&sum;
</p>
<p>j=1
kj = n . (5.1.10)
</p>
<p>The proof is left to the reader. The probability distribution (5.1.10) is called
the multinomial distribution.
</p>
<p>We can define a random variable xij that takes on the value 1 when the
ith trial leads to the outcome Aj , and is zero otherwise. In addition define
xj =
</p>
<p>&sum;n
i=1 xij . The expectation value of xj is then
</p>
<p>E(xj )= x̂j = npj . (5.1.11)</p>
<p/>
</div>
<div class="page"><p/>
<p>72 5 Some Important Distributions and Theorems
</p>
<p>Fig.5.2: Binomial distribution for various values of p, with n fixed.
</p>
<p>The elements of the covariance matrix of the xj are
</p>
<p>cij = npi(δij &minus;pj ) . (5.1.12)
</p>
<p>The off-diagonal elements are clearly not zero. This was to be expected, since
from Eq. (5.1.9) the variables xj are not independent.
</p>
<p>5.2 Frequency: The Law of Large Numbers
</p>
<p>Usually the probabilities for the different types of events, e.g., pj in the case
of the multinomial distribution, are not known but have to be obtained from
experiment. One first measures the frequency of the events in n experiments,
</p>
<p>hj =
1
</p>
<p>n
</p>
<p>n&sum;
</p>
<p>i=1
xij =
</p>
<p>1
</p>
<p>n
xj . (5.2.1)
</p>
<p>Unlike the probability, the frequency is a random quantity, since it depends
on the outcomes of the n individual experiments. By use of (5.1.11), (5.1.12),
and (3.3.15) we obtain
</p>
<p>E(hj )= ĥj = E
(xj
n
</p>
<p>)
= pj (5.2.2)</p>
<p/>
</div>
<div class="page"><p/>
<p>5.2 Frequency: The Law of Large Numbers 73
</p>
<p>and
</p>
<p>σ 2(hj )= σ 2
(xj
n
</p>
<p>)
= 1
</p>
<p>n2
σ 2(xj )=
</p>
<p>1
</p>
<p>n
pj (1&minus;pj ) . (5.2.3)
</p>
<p>Fig.5.3:Binomial distribution for various values of n but fixed product np. For higher values
of n the distribution changes very little.
</p>
<p>The product pj (1 &minus; pj ) in Eq. (5.2.3) is at most 1/4. One sees that the
expectation value of the frequency of an event is exactly equal to the prob-
ability that the event will occur, and that the variance of frequency about
this expectation value can be made arbitrarily small as the number of trials
increases. Since pq is at most 1/4, one can always say that the standard de-
viation of hj is at most 1/
</p>
<p>&radic;
n. This property of the frequency is known as the
</p>
<p>law of large numbers. It is clearly the reason for the frequency definition of
probability given by Eq. (2.2.1).
</p>
<p>Frequently the purpose of an experimental investigation is to determine
the probability for the occurrence of a certain type of event. According
to (5.2.2) we can use the frequency as an approximation of the probability.
The square of the error of this approximation is then inversely proportional to
the number of individual experiments. This kind of error, which originates
from the fact that only a finite number of experiments can be performed,
is called the statistical error. It is of prime importance for applications that
are concerned with the counting of individual events, e.g., nuclear particles</p>
<p/>
</div>
<div class="page"><p/>
<p>74 5 Some Important Distributions and Theorems
</p>
<p>passing through a counter, animals with certain traits in heredity experiments,
defective items in quality control, and so forth.
</p>
<p>Example 5.1: Statistical error
Suppose it is known from earlier experiments that a fraction R &asymp; 1/200 of a
sample of fruit flies (Drosophila) develop a certain property A if exposed to
a given dose of X-rays. An experiment is planned to determine the fraction R
with an accuracy of 1%. How large must the original sample be in order to
achieve this accuracy?
</p>
<p>We use Eq. (5.2.3) and find pj = 0.005, (1 &minus; pj ) &asymp; 1. We must now
choose n such that σ(hj )/hj = 200σ(hj )= 0.01. This gives σ(hj )= 0.00005
and σ 2(hj )= 0.25&times;10&minus;8. Equation (5.2.3) gives
</p>
<p>0.25&times;10&minus;8 = 1
n
&times;0.005
</p>
<p>and therefore
n= 2&times;106 .
</p>
<p>A total of two million fruit flies would have to be used. This is practically
impossible. To determine the fraction R with an accuracy of 10% would
require 20 000 flies.
</p>
<p>5.3 The Hypergeometric Distribution
</p>
<p>Although we shall rigorously introduce the concept of random sampling at a
later point, we will now discuss a typical problem of sampling. We consider
a container &ndash; we shall not break with the habit of mathematicians of calling
such a container an urn &ndash; with K white and L=N&minus;K black balls. We want
to determine the probability that in drawing n balls (without replacing them)
we will find exactly k white and l= n&minus;k black ones. The problem is rendered
difficult by the fact that the drawing of a ball of a particular color changes the
ratio of white and black balls and therefore influences the outcome of the next
draw. One clearly has
</p>
<p>(
N
n
</p>
<p>)
equally likely ways to choose n out of N balls. The
</p>
<p>probability that one of these possibilities will occur is therefore 1/
(
N
n
</p>
<p>)
. There
</p>
<p>are
(
K
k
</p>
<p>)
ways to choose k of the K white balls, and
</p>
<p>(
L
ℓ
</p>
<p>)
ways to choose ℓ of
</p>
<p>the L black ones. The required probability is therefore
</p>
<p>Wk =
(
K
k
</p>
<p>)(
L
ℓ
</p>
<p>)
(
N
n
</p>
<p>) . (5.3.1)
</p>
<p>As in Sect. 5.1 we define the random variable x =
&sum;n
</p>
<p>i=1 xi with xi = 1 when
the ith draw results in a black ball, and xi = 0 otherwise. (In other words, we
define k as the random variable x.)</p>
<p/>
</div>
<div class="page"><p/>
<p>5.3 The Hypergeometric Distribution 75
</p>
<p>To compute the expectation values of x we cannot simply add the expec-
tation values of the xi , since these are no longer independent. Instead we must
return to the definition (3.3.2),
</p>
<p>E(x) = 1(
N
n
</p>
<p>)
n&sum;
</p>
<p>i=1
i
</p>
<p>(
K
</p>
<p>i
</p>
<p>)(
N &minus;K
n&minus; i
</p>
<p>)
</p>
<p>= (N &minus;n)!n!
N !
</p>
<p>n&sum;
</p>
<p>i=1
</p>
<p>iK!(N&minus;K)!
i!(K&minus; i)!(n&minus; i)!(N&minus;K&minus;n+ i)!
</p>
<p>= n(n&minus;1)!(N &minus;n)!
N(N &minus;1)!
</p>
<p>n&sum;
</p>
<p>i=1
</p>
<p>K!
(i&minus;1)!(K&minus;1&minus; (i&minus;1))!
</p>
<p>&times; (N &minus;K)!
(n&minus;1&minus; (i&minus;1))!(N&minus;K&minus; (n&minus;1)+ (i&minus;1))! .
</p>
<p>If we substitute i&minus;1 = j , this gives
</p>
<p>E(x) = nK
N
</p>
<p>(n&minus;1)!(N&minus;n)!
(N&minus;1)!
</p>
<p>&times;
n&minus;1&sum;
</p>
<p>j=0
</p>
<p>(K&minus;1)!(N&minus;K)!
j !(K&minus;1&minus; j)!(n&minus;1&minus; j)!(N&minus;K&minus; (n&minus;1)+ j)!
</p>
<p>= nK
N
</p>
<p>1
(
N&minus;1
n&minus;1
</p>
<p>)
n&minus;1&sum;
</p>
<p>j=0
</p>
<p>(
K&minus;1
j
</p>
<p>)(
N &minus;K
n&minus;1&minus; j
</p>
<p>)
.
</p>
<p>With Eq. (B.5) we obtain
</p>
<p>E(x)= nK
N
</p>
<p>. (5.3.2)
</p>
<p>The calculation of the variance follows along the same lines but is rather
lengthy. The result is
</p>
<p>σ 2(x)= nK(N &minus;K)(N&minus;n)
N2(N &minus;1) . (5.3.3)
</p>
<p>Figures 5.4 and 5.5 depict several examples of the distribution. If n ≪ N ,
then drawing a white ball has little influence on the probabilities for the next
draw. We therefore expect that in this case Wk behaves in a manner similar to
a binomial distribution with p = K
</p>
<p>N
and q = N&minus;K
</p>
<p>N
. This is also made clear by
</p>
<p>the similarity of Figs. 5.5 and 5.1. One obtains in fact the same expectation
value,
</p>
<p>E(x)= nK
N
</p>
<p>= np ,</p>
<p/>
</div>
<div class="page"><p/>
<p>76 5 Some Important Distributions and Theorems
</p>
<p>as for the binomial distribution. The variance is then
</p>
<p>σ 2(x)= npq(N &minus;n)
N &minus;1 ,
</p>
<p>which for the case n≪N becomes
</p>
<p>σ 2 = npq .
</p>
<p>Fig.5.4: Hypergeometric distribution for various values of n and small values of K and N .
</p>
<p>There are many applications of the hypergeometric distribution. Opinion
polls, quality controls, and so forth are all based on the experimental scheme
of taking (polling) an object without replacement back into the original
sample or population. The distribution can be generalized in two ways. First
we can of course consider more properties instead of just two (white and
black balls). This leads us to a similar transition as the one from the binomial
to the multinomial distribution. The original sample (population) contains N
elements each of which possesses one of l properties,
</p>
<p>N =N1 +N2 +&middot;&middot; &middot;+Nℓ .
</p>
<p>The probability that n draws (without replacement) will be composed as
</p>
<p>n= n1 +n2 +&middot;&middot; &middot;+nℓ</p>
<p/>
</div>
<div class="page"><p/>
<p>5.3 The Hypergeometric Distribution 77
</p>
<p>Fig.5.5: Hypergeometric distribution for various values of n and for large values of K and
N .
</p>
<p>is, in analogy to Eq. (5.3.1),
</p>
<p>Wn1,n2,...,nℓ =
(
N1
n1
</p>
<p>)(
N2
n2
</p>
<p>)
&middot; &middot; &middot;
</p>
<p>(
Nℓ
nℓ
</p>
<p>)
(
N
n
</p>
<p>) . (5.3.4)
</p>
<p>Another extension of the hypergeometric distribution is obtained in the fol-
lowing way. We saw earlier that consecutive drawings ceased to be indepen-
dent because the balls were not replaced. If now each time we draw a ball of
one type we place more balls of that type back in the urn, this dependence can
be enhanced. One then obtains the Polya distribution. It is of importance in
the study of epidemic diseases, where the appearance of a case of the disease
enhances the probability of future cases.
</p>
<p>Example 5.2: Application of the hypergeometric distribution for
determination of zoological populations
</p>
<p>From a pond K fish are taken and marked. They are then returned to the pond.
After a short while n fish are caught, k of which are found to be marked. Be-
fore the second time that the fish are taken, the pond contains a total of N fish,
of which K are marked. The probability of finding k marked out of n removed
fish is given by Eq. (5.3.1). We will return to this problem in Example 7.3.</p>
<p/>
</div>
<div class="page"><p/>
<p>78 5 Some Important Distributions and Theorems
</p>
<p>5.4 The Poisson Distribution
</p>
<p>Looking at Fig. 5.3 it appears that if n tends to infinity, but at the same time
np = λ is kept constant, the binomial distribution approaches a certain fixed
distribution. We rewrite Eq. (5.1.3) as
</p>
<p>W nk =
(
n
</p>
<p>k
</p>
<p>)
pkqn&minus;k
</p>
<p>= n!
k!(n&minus; k)!
</p>
<p>(
λ
</p>
<p>n
</p>
<p>)k (1&minus; λ
n
</p>
<p>)n
(
1&minus; λ
</p>
<p>n
</p>
<p>)k
</p>
<p>= λ
k
</p>
<p>k!
n(n&minus;1)(n&minus;2) &middot; &middot; &middot;(n&minus; k+1)
</p>
<p>nk
</p>
<p>(
1&minus; λ
</p>
<p>n
</p>
<p>)n
(
1&minus; λ
</p>
<p>n
</p>
<p>)k
</p>
<p>= λ
k
</p>
<p>k!
</p>
<p>(
1&minus; λ
</p>
<p>n
</p>
<p>)n (1&minus; 1
n
</p>
<p>)(
1&minus; 2
</p>
<p>n
</p>
<p>)
&middot; &middot; &middot;
</p>
<p>(
1&minus; k&minus;1
</p>
<p>n
</p>
<p>)
(
1&minus; λ
</p>
<p>n
</p>
<p>)k .
</p>
<p>In the limiting case all of the many individual factors of the term on the right
approach unity. In addition one has
</p>
<p>lim
n&rarr;&infin;
</p>
<p>(
1&minus; λ
</p>
<p>n
</p>
<p>)n
= e&minus;λ ,
</p>
<p>so that in the limit one has
</p>
<p>lim
n&rarr;&infin;
</p>
<p>W nk = f (k)=
λk
</p>
<p>k! e
&minus;λ . (5.4.1)
</p>
<p>The quantity f (k) is the probability of the Poisson distribution. It is plotted
in Fig. 5.6 for various values of λ. As is the case for the other distributions we
have encountered so far, the Poisson distribution is only defined for integer
values of k.
</p>
<p>The distribution satisfies the requirement that the total probability is equal
to unity,
</p>
<p>&infin;&sum;
</p>
<p>k=0
f (k) =
</p>
<p>&infin;&sum;
</p>
<p>k=0
</p>
<p>e&minus;λλk
</p>
<p>k!
</p>
<p>= e&minus;λ
(
</p>
<p>1+λ+ λ
2
</p>
<p>2! +
λ3
</p>
<p>3! + &middot; &middot; &middot;
)
</p>
<p>= e&minus;λeλ ,</p>
<p/>
</div>
<div class="page"><p/>
<p>5.4 The Poisson Distribution 79
</p>
<p>&infin;&sum;
</p>
<p>k=0
f (k) = 1 . (5.4.2)
</p>
<p>The expression in parentheses is in fact the Taylor expansion of eλ.
We now want to determine the mean, variance, and skewness of the
</p>
<p>Poisson distribution. The definition (3.3.2) gives
</p>
<p>E(k) =
&infin;&sum;
</p>
<p>k=0
k
λk
</p>
<p>k! e
&minus;λ =
</p>
<p>&infin;&sum;
</p>
<p>k=1
k
λk
</p>
<p>k! e
&minus;λ
</p>
<p>=
&infin;&sum;
</p>
<p>k=1
</p>
<p>λλk&minus;1
</p>
<p>(k&minus;1)!e
&minus;λ = λ
</p>
<p>&infin;&sum;
</p>
<p>j=0
</p>
<p>λj
</p>
<p>j ! e
&minus;λ
</p>
<p>and using this with (5.4.2),
</p>
<p>E(k)= λ . (5.4.3)
</p>
<p>We would now like to find E(k2). One obtains in a corresponding way
</p>
<p>E(k2) =
&infin;&sum;
</p>
<p>k=1
k2
</p>
<p>λk
</p>
<p>k! e
&minus;λ = λ
</p>
<p>&infin;&sum;
</p>
<p>k=1
k
</p>
<p>λk&minus;1
</p>
<p>(k&minus;1)!e
&minus;λ
</p>
<p>= λ
&infin;&sum;
</p>
<p>j=0
(j +1)λ
</p>
<p>j
</p>
<p>j ! e
&minus;λ = λ
</p>
<p>⎛
⎝
</p>
<p>&infin;&sum;
</p>
<p>j=0
j
λj
</p>
<p>j ! e
&minus;λ+1
</p>
<p>⎞
⎠
</p>
<p>and therefore
</p>
<p>E(k2)= λ(λ+1) . (5.4.4)
</p>
<p>We will use Eqs. (5.4.3) and (5.4.4) to compute the variance. According to
Eq. (3.3.16) one has
</p>
<p>σ 2(k)=E(k2)&minus;{E(k)}2 = λ(λ+1)&minus;λ2 (5.4.5)
</p>
<p>or
</p>
<p>σ 2(k)= λ . (5.4.6)
</p>
<p>We now consider the skewness (3.3.13) of the Poisson distribution. Following
Sect. 3.3 we easily find that
</p>
<p>μ3 = E{(k&minus; k̂)3} = λ .</p>
<p/>
</div>
<div class="page"><p/>
<p>80 5 Some Important Distributions and Theorems
</p>
<p>Fig.5.6: Poisson distribution for various values of λ.
</p>
<p>The skewness (3.3.13) is then
</p>
<p>γ = μ3
σ 3
</p>
<p>= λ
λ
</p>
<p>3
2
</p>
<p>= λ&minus; 12 , (5.4.7)
</p>
<p>that is, the Poisson distribution becomes increasingly symmetric as λ in-
creases. Figure 5.6 shows the distribution for various values of λ. In particular
the distribution with λ= 3 should be compared with Fig. 5.3.
</p>
<p>We have obtained the Poisson distribution from the binomial distribution
with large n but constant λ= np, i.e., small p. We therefore expect it to apply
to processes in which a large number of events occur but of which only very
few have a certain property of interest to us (i.e., a large number of &ldquo;trials&rdquo;
but few &ldquo;successes&rdquo;).
</p>
<p>Example 5.3: Poisson distribution and independence of radioactive decays
</p>
<p>We consider a radioactive nucleus with mean lifetime τ and observe it for a
time T ≪ τ . The probability that it decays within this time interval is W ≪ 1.
We break the observation time T into n smaller time intervals of length t , so
that T = nt . The probability for the nucleus to decay in a particular time in-
terval is p &asymp;W/n. We now observe a radioactive source containing N nuclei
which decay independently from each other for a total time T , and detect a1</p>
<p/>
</div>
<div class="page"><p/>
<p>5.5 The Characteristic Function of a Distribution 81
</p>
<p>decays in time interval 1, a2 decays in interval 2, etc. Let h(k) be the frequency
of decays observed in the interval k, with (k = 0,1, . . .). That is, if nk is the
number of intervals with k decays, then h(k) = nk/n. In the limit N &rarr; &infin;
and for large n the frequency distribution h(k) becomes the probability dis-
tribution (5.4.1). The statistical nature of radioactive decay was established in
this way in a famous experiment by RUTHERFORD and GEIGER.
</p>
<p>Similarly, the frequency of finding k stars per element of the celestial
sphere or k raisins per volume element of a fruit cake is distributed according
to the Poisson law, but not, however, the frequency of finding k animals of a
given species per element of area, at least if these animals live in herds, since
in this case the assumption of independence is not fulfilled.
</p>
<p>As a quantitative example of the Poisson distribution many textbooks dis-
cuss the number of Prussian cavalrymen killed during a period of 20 years by
horse kicks, an example originally due to VON BORTKIEWICZ [6]. We prefer
to turn our attention to a somewhat less macabre example taken from a lecture
of DE SOLLA PRICE [7].
</p>
<p>Example 5.4: Poisson distribution and the independence of scientific
discoveries
</p>
<p>The author first constructs the model of an apple tree with 1000 apples and
1000 pickers with blindfolded eyes who each try at the same time to pick
an apple. Since we are dealing with a model, they do not hinder each other
but it can happen that two or several of them will attempt to pick the same
apple at the same time. The number of apples grabbed simultaneously by k
people (k = 0,1,2, . . .) follows a Poisson distribution. It was determined by
DE SOLLA PRICE that the number of scientific discoveries made indepen-
dently twice, three times, etc. is also distributed according to the Poisson law,
in a way similar to the principle of the blindfolded apple pickers (Table 5.1).
One gets the impression that scientists are not concerned with the activities
of their colleagues. DE SOLLA PRICE believes that this can be explained by
the assumption that scientists have a strong urge write papers, but feel only a
relatively mild need to read them.
</p>
<p>5.5 The Characteristic Function of a Distribution
</p>
<p>So far we have only considered real random variables. In fact, in Sect. 3.1 we
have introduced the concept of a random quantity as a real number associated
with an event. Without changing this concept we can formally construct a
complex random variable from two real ones by writing
</p>
<p>z = x+ i y . (5.5.1)</p>
<p/>
</div>
<div class="page"><p/>
<p>82 5 Some Important Distributions and Theorems
</p>
<p>Table5.1: Simultaneous discovery and the Poisson distribution.
</p>
<p>Number of Cases of Prediction
simultaneous simultaneous of Poisson
discoveries discovery distribution
</p>
<p>0 Not defined 368
1 Not known 368
2 179 184
3 51 61
4 17 15
5 6 3
</p>
<p>&ge;6 8 1
</p>
<p>As its expectation value we define
</p>
<p>E(z)= E(x)+ iE(y) . (5.5.2)
</p>
<p>By analogy with real variables, complex random variables are independent if
the real and imaginary parts are independent among themselves.
</p>
<p>If x is a real random variable with distribution function F(x)= P (x &lt; x)
and probability density f (x), we define its characteristic function to be the
expectation value of the quantity exp(itx):
</p>
<p>ϕ(t)= E{exp(itx)} . (5.5.3)
</p>
<p>That is, in the case of a continuous variable the characteristic function is a
Fourier integral with its known transformation properties:
</p>
<p>ϕ(t)=
&int; &infin;
</p>
<p>&minus;&infin;
exp(itx)f (x)dx . (5.5.4)
</p>
<p>For a discrete variable we obtain instead from (3.3.2)
</p>
<p>ϕ(t)=
&sum;
</p>
<p>i
</p>
<p>exp(itxi)P (x = xi) . (5.5.5)
</p>
<p>We now consider the moments of x about the origin,
</p>
<p>λn = E(xn)=
&int; &infin;
</p>
<p>&minus;&infin;
xnf (x)dx , (5.5.6)</p>
<p/>
</div>
<div class="page"><p/>
<p>5.5 The Characteristic Function of a Distribution 83
</p>
<p>and find that λn can be obtained simply by differentiating the characteristic
function n times at the point t = 0:
</p>
<p>ϕ(n)(t)= d
nϕ(t)
</p>
<p>dtn
= in
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
xn exp(itx)f (x)dx
</p>
<p>and therefore
</p>
<p>ϕ(n)(0)= inλn . (5.5.7)
</p>
<p>If we now introduce the simple coordinate translation
</p>
<p>y = x&minus; x̂ (5.5.8)
</p>
<p>and construct the characteristic function
</p>
<p>ϕy(t)=
&int; &infin;
</p>
<p>&minus;&infin;
exp{i t (x&minus; x̂)}f (x)dx = ϕ(t)exp(&minus;i t x̂) , (5.5.9)
</p>
<p>then its nth derivative is (up to a power of i) equal to the nth moment of x
about the expectation value [cf. (3.3.8)]:
</p>
<p>ϕ(n)y (0)= inμn = inE{(x&minus; x̂)n} , (5.5.10)
</p>
<p>and in particular
</p>
<p>σ 2(x)=&minus;ϕ&prime;&prime;y (0) . (5.5.11)
</p>
<p>Inverting the Fourier transform (5.5.4) we see that it is possible to obtain the
probability density from the characteristic function,
</p>
<p>f (x)= 1
2π
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
exp(&minus;itx)ϕ(t)dt . (5.5.12)
</p>
<p>It is possible to show that a distribution is determined uniquely by its charac-
teristic function. This is the case even for discrete variables where one has
</p>
<p>F(b)&minus;F(a)= i
2π
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>exp(itb)&minus; exp(ita)
t
</p>
<p>ϕ(t)dt , (5.5.13)
</p>
<p>since in this case the probability density is not defined. Often it is more
convenient to use the characteristic function rather than the original distri-
bution. Because of the unique relation between the two it is possible to switch
back and forth at any place in the course of a calculation.
</p>
<p>We now consider the sum of two independent random variables
</p>
<p>w = x+y .</p>
<p/>
</div>
<div class="page"><p/>
<p>84 5 Some Important Distributions and Theorems
</p>
<p>Its characteristic function is
</p>
<p>ϕw(t)=E[exp{it (x+y)}] = E{exp(itx)exp(ity)} .
</p>
<p>Generalizing relation (3.5.13) to complex variables we obtain
</p>
<p>ϕw(t)= E{exp(itx)}E{exp(ity)} = ϕx(t)ϕy(t) , (5.5.14)
</p>
<p>i.e., the characteristic function of a sum of independent random variables is
equal to the product of their respective characteristic functions.
</p>
<p>Example 5.5: Addition of two Poisson distributed variables with use of the
characteristic function
</p>
<p>From Eqs. (5.5.5) and (5.4.1) one obtains for the characteristic function of the
Poisson distribution
</p>
<p>ϕ(t) =
&infin;&sum;
</p>
<p>k=0
exp(itk)
</p>
<p>λk
</p>
<p>k! exp(&minus;λ)= exp(&minus;λ)
&infin;&sum;
</p>
<p>k=0
</p>
<p>(λexp(it))k
</p>
<p>k!
</p>
<p>= exp(&minus;λ)exp(λeit)= exp{λ(eit &minus;1)} . (5.5.15)
</p>
<p>We now form the characteristic function of the sum of two independent
Poisson distributed variables with mean values λ1 and λ2,
</p>
<p>ϕsum(t) = exp{λ1(eit &minus;1)}exp{λ2(eit &minus;1)}
= exp{(λ1 +λ2)(eit &minus;1)} . (5.5.16)
</p>
<p>This is again of the form of Eq. (5.5.15). Therefore the distribution of the sum
of two independent Poisson distributed variables is itself a Poisson variable.
Its mean is the sum of the means of the individual distributions.
</p>
<p>5.6 The Standard Normal Distribution
</p>
<p>The probability density of the standard normal distribution is defined as
</p>
<p>f (x)= φ0(x)=
1&radic;
2π
</p>
<p>e&minus;x
2/2 . (5.6.1)
</p>
<p>This function is depicted in Fig. 5.7a. It has a bell shape with the maxi-
mum at x = 0. From Appendix D.1 we have</p>
<p/>
</div>
<div class="page"><p/>
<p>5.6 The Standard Normal Distribution 85
</p>
<p>Fig.5.7: Probability density (a) and distribution function (b) of the standard normal distribu-
tion.
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
e&minus;x
</p>
<p>2/2 dx =
&radic;
</p>
<p>2π , (5.6.2)
</p>
<p>so that φ0(x) is normalized to one as required. Using the symmetry of
Fig. 5.7a, or alternatively, using the antisymmetry of the integrand we conclude
that the expectation value is
</p>
<p>x̂ = 1&radic;
2π
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
x e&minus;x
</p>
<p>2/2 dx = 0 . (5.6.3)</p>
<p/>
</div>
<div class="page"><p/>
<p>86 5 Some Important Distributions and Theorems
</p>
<p>By integrating by parts we can compute the variance to be
</p>
<p>σ 2 = 1&radic;
2π
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
x2 e&minus;x
</p>
<p>2/2 dx
</p>
<p>= 1&radic;
2π
</p>
<p>{[
&minus;x e&minus;x2/2
</p>
<p>]&infin;
&minus;&infin;
</p>
<p>+
&int; &infin;
</p>
<p>&minus;&infin;
e&minus;x
</p>
<p>2/2 dx
}
= 1 , (5.6.4)
</p>
<p>since the expression in the square brackets vanishes at the integral&rsquo;s boundaries
and the integral in curly brackets is given by Eq. (5.6.2).
</p>
<p>The distribution function of the standard normal distribution
</p>
<p>F(x)= ψ0(x)=
1&radic;
2π
</p>
<p>&int; x
</p>
<p>&minus;&infin;
e&minus;t
</p>
<p>2/2 dt (5.6.5)
</p>
<p>is shown in Fig. 5.7b. It cannot be expressed in analytic form. It is tabulated
numerically in Appendix C.4.
</p>
<p>5.7 The Normal or Gaussian Distribution
</p>
<p>The standardized distribution of the last section had the properties x̂=E(x)=
0, σ 2(x)= 1, i.e., the variable x had the properties of the standardized variable
u in Eq. (3.3.17). If we now replace x by (x&minus; a)/b in (5.6.1), we obtain the
probability density of the normal or Gaussian distribution,
</p>
<p>f (x)= φ(x)= 1&radic;
2πb
</p>
<p>exp
</p>
<p>{
&minus;(x&minus;a)
</p>
<p>2
</p>
<p>2b2
</p>
<p>}
, (5.7.1)
</p>
<p>with
x̂ = a , σ 2(x)= b2 . (5.7.2)
</p>
<p>The characteristic function of the normal distribution (5.7.1) is, using
Eq. (5.5.4),
</p>
<p>ϕ(t)= 1&radic;
2πb
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
exp(itx)exp
</p>
<p>(
&minus;(x&minus;a)
</p>
<p>2
</p>
<p>2b2
</p>
<p>)
dx . (5.7.3)
</p>
<p>With u= (x&minus;a)/b one obtains
</p>
<p>ϕ(t) = 1&radic;
2π
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
exp{&minus;1
</p>
<p>2
u2 + it (bu+a)}du
</p>
<p>= 1&radic;
2π
</p>
<p>exp(ita)
&int; &infin;
</p>
<p>&minus;&infin;
exp{&minus;1
</p>
<p>2
u2 + itbu}du . (5.7.4)</p>
<p/>
</div>
<div class="page"><p/>
<p>5.7 The Normal or Gaussian Distribution 87
</p>
<p>By completing the square the integral can be rewritten as
&int; &infin;
</p>
<p>&minus;&infin;
exp{&minus;1
</p>
<p>2
u2 + itbu}du
</p>
<p>=
&int; &infin;
</p>
<p>&minus;&infin;
exp{&minus;1
</p>
<p>2
(u&minus; itb)2 &minus; 1
</p>
<p>2
t2b2}du
</p>
<p>= exp{&minus;1
2
t2b2}
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
exp{&minus;1
</p>
<p>2
(u&minus; itb)2}du . (5.7.5)
</p>
<p>With r = u&minus; itb the last integral takes on the form
&int; &infin;&minus;itb
</p>
<p>&minus;&infin;&minus;itb
exp{&minus;1
</p>
<p>2
r2}dr .
</p>
<p>The integrand does not have any singularities in the complex r plane. Accord-
ing to the residue theorem, therefore, the contour integral around any closed
path vanishes. Consider a path that runs along the real axis from r = &minus;L to
r = L, and then parallel to the imaginary axis from r = L to r = L&minus; itb and
from there antiparallel to the real axis to r = &minus;L&minus; itb, and finally back to
the starting point r = L. In the limit L&rarr;&infin; the integrand vanishes along the
parts of the path that run parallel to the imaginary axis. One then has
</p>
<p>&int; &infin;&minus;itb
</p>
<p>&minus;&infin;&minus;itb
exp{&minus;1
</p>
<p>2
r2}dr =
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
exp{&minus;1
</p>
<p>2
r2}dr ,
</p>
<p>i.e., we can extend the integral to cover the entire real axis. The integral is
computed in Appendix D.1 and has the value
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
exp{&minus;1
</p>
<p>2
r2}dr =
</p>
<p>&radic;
2π . (5.7.6)
</p>
<p>Substituting this into Eqs. (5.7.5) and (5.7.4) we obtain finally the character-
istic function of the normal distribution
</p>
<p>ϕ(t)= exp(ita)exp(&minus;1
2
b2t2) . (5.7.7)
</p>
<p>For the case a = 0 one obtains from this the following interesting theorem:
A normal distribution with mean value zero has a character-
</p>
<p>istic function that has itself (up to normalization) the form of a
normal distribution. The product of the variances of both func-
tions is one.
</p>
<p>If we now consider the sum of two independent normal distributions, then
by applying Eq. (5.5.14) one immediately sees that the characteristic func-
tion of the sum is again of the form of Eq. (5.7.7). The sum of independent
normally distributed quantities is therefore itself normally distributed. The
Poisson distribution behaves in a similar way (cf. Example 5.5).</p>
<p/>
</div>
<div class="page"><p/>
<p>88 5 Some Important Distributions and Theorems
</p>
<p>5.8 Quantitative Properties of the Normal Distribution
</p>
<p>Figure 5.7a shows the probability density of the standard Gaussian distribution
φ0(x) and the corresponding distribution function. By simple computation
one can determine that the points of inflection of (5.6.1) are at x =&plusmn;1. [In the
case of a general Gaussian distribution (5.7.1) they are at x = a&plusmn; b.] The
distribution function ψ0(x) gives the probability for the random variable to
take on a value smaller than x:
</p>
<p>ψ0(x)= P (x &lt; x) . (5.8.1)
</p>
<p>By symmetry one has
</p>
<p>P (|x|&gt; x)= 2ψ0(&minus;|x|)= 2{1&minus;ψ0(|x|)} (5.8.2)
</p>
<p>or conversely, the probability to obtain a random value within an interval of
width 2x about zero (the expectation value) is
</p>
<p>P (|x| &le; x)= 2ψ0(|x|)&minus;1 . (5.8.3)
</p>
<p>Since the integral (5.6.5) is not easy to evaluate, one typically finds the values
of (5.8.1) and (5.8.3) from statistical tables, e.g., in Tables I.2 and I.3 of the
appendix.
</p>
<p>One can now extend this relation to the general Gaussian distribution
given by Eq. (5.7.1). Its distribution function is
</p>
<p>ψ(x)= ψ0
(
x&minus;a
b
</p>
<p>)
. (5.8.4)
</p>
<p>We are interested in finding the probability to obtain a random value inside
(or outside) of a given multiple of σ = b about the mean value:
</p>
<p>P (|x&minus;a| &le; nσ)= 2ψ0
(
nb
</p>
<p>b
</p>
<p>)
&minus;1 = 2ψ0(n)&minus;1 . (5.8.5)
</p>
<p>From Table I.3 we find
</p>
<p>P (|x&minus;a| &le; σ) = 68.3% , P (|x&minus;a|&gt; σ) = 31.7% ,
P (|x&minus;a| &le; 2σ)= 95.4% , P (|x&minus;a|&gt; 2σ)= 4.6% ,
P (|x&minus;a| &le; 3σ)= 99.8% , P (|x&minus;a|&gt; 3σ)= 0.2% .
</p>
<p>(5.8.6)
</p>
<p>As we will see later in more detail, one can often assume that the
measurement errors of a quantity are distributed according to a Gaussian
distribution about zero. This means that the probability to obtain a value
between x and x+dx is given by</p>
<p/>
</div>
<div class="page"><p/>
<p>5.8 Quantitative Properties of the Normal Distribution 89
</p>
<p>P (x &le; x &lt; x+dx)= φ(x)dx .
</p>
<p>The dispersion σ of the distribution φ(x) is called the standard deviation or
standard error. If the standard error of an instrument is known and one car-
ries out a single measurement, then Eq. (5.8.6) tells us that the probability that
the true value is within an interval given by plus or minus the standard error
about the measured value is 68.3%. It is therefore a common practice to mul-
tiply the standard error with a more or less arbitrary factor in order to improve
this percentage. (One obtains around 99.8% for the factor 3.) This procedure
is, however, misleading and often harmful. If this factor is not explicitly stated,
a comparison of different measurements of the same quantity and especially
the calculation of a weighted average (cf. Example 9.1) is rendered impossible
or is liable to be erroneous.
</p>
<p>The quantiles [see Eq. (3.3.25)] of the standard normal distribution are
of considerable interest. For the distribution function (5.6.5) one obtains by
definition
</p>
<p>P (xp)= P (x &lt; xp)= ψ0(xp) . (5.8.7)
</p>
<p>The quantile xp is therefore given by the inverse function
</p>
<p>xp =Ω(P ) (5.8.8)
</p>
<p>of the distribution function ψ0(xp). This is computed numerically in
Appendix C.4 and is given in Table I.4. Figure 5.8 shows a graphical rep-
resentation.
</p>
<p>We now consider the probability
</p>
<p>P &prime;(x)= P (|x|&lt; x) , x &gt; 0 , (5.8.9)
</p>
<p>for a quantity distributed according to the standard normal distribution to dif-
fer from zero in absolute value by less than x. Since
</p>
<p>P (x) = P (x &lt; x)= ψ0(x)=
&int; x
</p>
<p>&minus;&infin;
φ0(x)dx
</p>
<p>= 1
2
+
&int; x
</p>
<p>0
φ0(x)dx =
</p>
<p>1
</p>
<p>2
+ 1
</p>
<p>2
P &prime;(x)= 1
</p>
<p>2
(P &prime;(x)+1) ,
</p>
<p>it is the inverse function and with it the quantiles of the distribution function
P &prime;(x) are obtained by substituting (P &prime;(x)+ 1)/2 for the argument of the
inverse function of P :
</p>
<p>xp =Ω &prime;(P &prime;)=Ω((P &prime;+1)/2) . (5.8.10)
</p>
<p>This function is tabulated in Table I.5.</p>
<p/>
</div>
<div class="page"><p/>
<p>90 5 Some Important Distributions and Theorems
</p>
<p>Fig.5.8: Quantile of the standard normal distribution.
</p>
<p>5.9 The Central Limit Theorem
</p>
<p>We will now prove the following important theorem. If xi are independent
random variables with mean values a and variances b2, then the variable
</p>
<p>x = lim
n&rarr;&infin;
</p>
<p>n&sum;
</p>
<p>i=1
xi (5.9.1)
</p>
<p>is normally distributed with
</p>
<p>E(x)= na , σ 2(x)= nb2 . (5.9.2)
</p>
<p>From Eq. (3.3.15) one then has that the variable
</p>
<p>ξ = 1
n
</p>
<p>x = lim
n&rarr;&infin;
</p>
<p>1
</p>
<p>n
</p>
<p>n&sum;
</p>
<p>i=1
xi (5.9.3)
</p>
<p>is normally distributed with
</p>
<p>E(ξ)= a , σ 2(ξ)= b2/n . (5.9.4)</p>
<p/>
</div>
<div class="page"><p/>
<p>5.9 The Central Limit Theorem 91
</p>
<p>To prove this we assume for simplicity that all of the xi have the same
distribution. If we denote the characteristic function of the xi by ϕ(t), then
the sum of n variables has the characteristic function {ϕ(t)}n. We now assume
that a = 0. (The general case can be related to this by a simple coordinate
translation x&prime;i = xi &minus; a.) From (5.5.10) we have the first two derivatives of
ϕ(t) at t = 0,
</p>
<p>ϕ&prime;(0)= 0 , ϕ&prime;&prime;(0)=&minus;σ 2 .
We can therefore expand,
</p>
<p>ϕx&prime;(t)= 1&minus;
1
</p>
<p>2
σ 2t2 +&middot;&middot; &middot; .
</p>
<p>Instead of xi let us now choose
</p>
<p>ui =
x&prime;i
</p>
<p>b
&radic;
n
= xi &minus;a
</p>
<p>b
&radic;
n
</p>
<p>as the variable. If we consider n to be fixed for the moment, then this implies
a simple translation and a change of scale. The corresponding characteristic
function is
</p>
<p>ϕui (t)= E{exp(itui)} = E
{
</p>
<p>exp
</p>
<p>(
it
xi &minus;a
b
&radic;
n
</p>
<p>)}
= ϕx&prime;i
</p>
<p>(
t
</p>
<p>b
&radic;
n
</p>
<p>)
</p>
<p>or
</p>
<p>ϕui (t)= 1&minus;
t2
</p>
<p>2n
+&middot;&middot; &middot; .
</p>
<p>The higher-order terms are at most of the order n&minus;2. If we now consider the
limiting case and use
</p>
<p>u = lim
n&rarr;&infin;
</p>
<p>n&sum;
</p>
<p>i=1
ui = lim
</p>
<p>n&rarr;&infin;
</p>
<p>n&sum;
</p>
<p>i=1
</p>
<p>xi &minus;a
b
&radic;
n
</p>
<p>= lim
n&rarr;&infin;
</p>
<p>(x&minus;na)
b
&radic;
n
</p>
<p>(5.9.5)
</p>
<p>we obtain
</p>
<p>ϕu(t)= lim
n&rarr;&infin;
</p>
<p>{ϕui (t)}n = lim
n&rarr;&infin;
</p>
<p>(
1&minus; t
</p>
<p>2
</p>
<p>2n
+&middot;&middot; &middot;
</p>
<p>)n
</p>
<p>or
</p>
<p>ϕu(t)= exp
(
&minus;1
</p>
<p>2
t2
)
</p>
<p>. (5.9.6)
</p>
<p>This, however, is exactly the characteristic function of the standard normal
distribution φ0(u). One therefore has E(u)= 0, σ 2(u)= 1. Using Eqs. (5.9.5)
and (3.3.15) leads directly to the theorem.</p>
<p/>
</div>
<div class="page"><p/>
<p>92 5 Some Important Distributions and Theorems
</p>
<p>Example 5.6: Normal distribution as the limiting case of the binomial
distribution
</p>
<p>Suppose that the individual variables xi in (5.9.1) are described by the simple
distribution given by (5.1.1) and (5.1.2), i.e., they can only take on the value 1
(with probability p) or 0 (with probability 1&minus;p). One then has E(xi) = p,
σ 2(xi)= p(1&minus;p). The variable
</p>
<p>x(n) =
n&sum;
</p>
<p>i=1
xi (5.9.7)
</p>
<p>then follows the binomial distribution, P (x(n) = k) = W nk [see Eqs. (5.1.3),
(5.1.6), (5.1.7)]. As done for (5.9.5) let us consider the distribution of
</p>
<p>u(n) =
n&sum;
</p>
<p>i=1
</p>
<p>xi &minus;p&radic;
np(1&minus;p)
</p>
<p>= 1&radic;
np(1&minus;p)
</p>
<p>(
n&sum;
</p>
<p>i=1
xi &minus;np
</p>
<p>)
. (5.9.8)
</p>
<p>One clearly has P (x = k) = P
(
u(n) = (k&minus;np)/
</p>
<p>&radic;
np(1&minus;p)
</p>
<p>)
= W nk . These
</p>
<p>values, however, lie increasingly closer to each other on the u(n) axis as n in-
creases. Let us denote the distance between two neighboring values of u(n) by
Δu(n). Then the distribution of a discrete variable P (u(n))/Δu(n) finally be-
comes the probability density of a continuous variable. According to the Cen-
tral Limit Theorem this must be a standard normal distribution. This is illus-
trated in Fig. 5.9, where P (u(n))/Δu(n) is shown for various possible values
of u(n).
</p>
<p>Example 5.7: Error model of Laplace
</p>
<p>In 1783 LAPLACE made the following remarks concerning the origin of errors
of an observation. Suppose the true value of a quantity to be measured is m0.
Now let the measurement be disturbed by a large number n of independent
causes, each resulting in a disturbance of magnitude ε. For each disturbance
there exists an equal probability for a variation of the measured value in either
direction, i.e., +ε or &minus;ε. The measurement error is then composed of the sum
of the individual disturbances. It is clear that in this model the probability
distribution of measurement errors will be given by the binomial distribution.
It is interesting nevertheless to follow the model somewhat further, since it
leads directly to the famous Pascal triangle.
</p>
<p>Figure 5.10 shows how the probability distribution is derived from the
model. The starting point is with no disturbance where the probability of
measuring m0 is equal to one. With one disturbance this probability is split
equally between the neighboring possibilities m0 + ε and m0 &minus; ε. The same
happens with every further disturbance. Of course the individual probabilities
leading to the same measured value must be added.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.9 The Central Limit Theorem 93
</p>
<p>Fig.5.9: The quantity P(u(n))/Δu(n) for various values of the discrete variable u(n) for in-
creasing n.
</p>
<p>Numberof
</p>
<p>disturbances
</p>
<p>Deviation from true value
</p>
<p>n &minus; 3ε &minus;2e &minus;e 0 +e +2e +3e
</p>
<p>0 1
</p>
<p>1 1
2
</p>
<p>1
</p>
<p>2
</p>
<p>2
1
</p>
<p>4
</p>
<p>1
</p>
<p>4
, 1
</p>
<p>4
</p>
<p>1
</p>
<p>2
</p>
<p>1
</p>
<p>4
</p>
<p>3
1
</p>
<p>8
</p>
<p>1
</p>
<p>8
, 1
</p>
<p>4
</p>
<p>3
</p>
<p>8
</p>
<p>1
</p>
<p>4
, 1
</p>
<p>8
</p>
<p>3
</p>
<p>8
</p>
<p>1
</p>
<p>8
</p>
<p>Fig.5.10:Connection between the Laplacian error model and the binomial distribution.</p>
<p/>
</div>
<div class="page"><p/>
<p>94 5 Some Important Distributions and Theorems
</p>
<p>Each line of the resulting triangle contains the distribution W nk (k =
0,1, . . . ,n)ofEq. (5.1.3) for thecasep= q = 1/2.Multipliedby1/(pkqn&minus;k)=
2n it becomes a line of binomial coefficients of Pascal&rsquo;s triangle (cf. Ap-
pendix B).
</p>
<p>It is easy to relate this to Example 5.6 by extending Eq. (5.9.8) and
substituting p = 1/2. For n&rarr;&infin; the quantity
</p>
<p>u(n) =
2
(&sum;n
</p>
<p>i=1 εxi &minus;nε/2
)
</p>
<p>&radic;
nε
</p>
<p>follows a normal distribution with expectation value zero and standard de-
viation
</p>
<p>&radic;
nε/2. Thus Gaussian measurement errors can result from a large
</p>
<p>number of small independent disturbances.
</p>
<p>The identification of the measurement error distribution as Gaussian is of
great significance in many computations, particularly for the method of least
squares. The normal distribution for measurement errors is, however, not a
law of nature. The causes of experimental errors can be individually very
complicated. One cannot, therefore, find a distribution function that describes
the behavior of measurement errors in all possible experiments. In particular,
it is not always possible to guaranty symmetry and independence. One must
ask in each individual case whether the measurement errors can be modeled
by a Gaussian distribution. This can be done, for example, by means of a
χ2-test, applied to the distribution of a measured quantity (see Sect. 8.7). It is
always necessary to check the distribution of experimental errors before more
lengthy computations can be used whose results are only meaningful for the
case of a Gaussian error distribution.
</p>
<p>5.10 The Multivariate Normal Distribution
</p>
<p>Consider a vector x of n variables,
</p>
<p>x = (x1,x2, . . . ,xn) .
</p>
<p>We define the probability density of the joint normal distribution of the xi
to be
</p>
<p>φ(x)= k exp{&minus;1
2
(x&minus;a)TB(x&minus;a)} = k exp{&minus;1
</p>
<p>2
g(x)} (5.10.1)
</p>
<p>with
g(x)= (x&minus;a)TB(x&minus;a) . (5.10.2)</p>
<p/>
</div>
<div class="page"><p/>
<p>5.10 The Multivariate Normal Distribution 95
</p>
<p>Here a is an n-component vector and B is an n&times;n matrix, which is symmetric
and positive definite. Since φ(x) is clearly symmetric about the point x = a,
one has
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
&middot; &middot; &middot;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
(x&minus;a)φ(x)dx1 dx2 . . .dxn = 0 , (5.10.3)
</p>
<p>that is,
</p>
<p>E(x&minus;a)= 0
or
</p>
<p>E(x)= a . (5.10.4)
The vector of expectation values is therefore given directly by a.
</p>
<p>We now differentiate Eq. (5.10.3) with respect to a,
&int; &infin;
</p>
<p>&minus;&infin;
&middot; &middot; &middot;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
[I &minus; (x&minus;a)(x&minus;a)TB]φ(x)dx1 dx2 . . .dxn = 0 .
</p>
<p>This means that the expectation value of the quantity in square brackets van-
ishes,
</p>
<p>E{(x&minus;a)(x&minus;a)T}B = I
or
</p>
<p>C = E{(x&minus;a)(x&minus;a)T} = B&minus;1 . (5.10.5)
Comparing with Eq. (3.6.19) one sees that C is the covariance matrix of the
variables x = (x1,x2, . . . ,xn).
</p>
<p>Because of the practical importance of the normal distribution, we would
like to investigate the case of two variables in somewhat more detail. In
particular we are interested in the correlation of the variables. One has
</p>
<p>C = B&minus;1 =
(
</p>
<p>σ 21 cov(x1,x2)
cov(x1,x2) σ 22
</p>
<p>)
. (5.10.6)
</p>
<p>By inversion one obtains for B
</p>
<p>B = 1
σ 21 σ
</p>
<p>2
2 &minus; cov(x1,x2)2
</p>
<p>(
σ 22 &minus;cov(x1,x2)
</p>
<p>&minus;cov(x1,x2) σ 21
</p>
<p>)
. (5.10.7)
</p>
<p>One sees that B is a diagonal matrix if the covariances vanish. One then has
</p>
<p>B0 =
(
</p>
<p>1/σ 21 0
0 1/σ 22
</p>
<p>)
. (5.10.8)
</p>
<p>If we substitute B0 into Eq. (5.10.1), we obtain &ndash; as expected &ndash; the joint
probability density of two independently normally distributed variables as the
product of two normal distributions:</p>
<p/>
</div>
<div class="page"><p/>
<p>96 5 Some Important Distributions and Theorems
</p>
<p>φ = k exp
(
&minus;1
</p>
<p>2
</p>
<p>(x1 &minus;a1)2
</p>
<p>σ 21
</p>
<p>)
exp
</p>
<p>(
&minus;1
</p>
<p>2
</p>
<p>(x2 &minus;a2)2
</p>
<p>σ 22
</p>
<p>)
. (5.10.9)
</p>
<p>In this simple case the constant k takes on the value
</p>
<p>k0 =
1
</p>
<p>2πσ1σ2
,
</p>
<p>as can be determined by integration of (5.10.9) or simply by comparison with
Eq. (5.7.1). In the general case of n variables with non-vanishing covariances,
one has
</p>
<p>k =
(
</p>
<p>detB
</p>
<p>(2π)n
</p>
<p>) 1
2
</p>
<p>. (5.10.10)
</p>
<p>Here det B is the determinant of the matrix B. If the variables are not inde-
pendent, i.e., if the covariance does not vanish, then the expression for the
normal distribution of two variables is somewhat more complicated.
</p>
<p>Let us consider the reduced variables
</p>
<p>ui =
xi &minus;ai
σi
</p>
<p>, i = 1,2 ,
</p>
<p>and make use of the correlation coefficient
</p>
<p>ρ = cov(x1,x2)
σ1σ2
</p>
<p>= cov(u1,u2) .
</p>
<p>Equation (5.10.1) then takes on the simple form
</p>
<p>φ(u1,u2)= k exp(&minus;
1
</p>
<p>2
uTBu)= k exp
</p>
<p>(
&minus;1
</p>
<p>2
g(u)
</p>
<p>)
(5.10.11)
</p>
<p>with
</p>
<p>B = 1
1&minus;ρ2
</p>
<p>(
1 &minus;ρ
</p>
<p>&minus;ρ 1
</p>
<p>)
. (5.10.12)
</p>
<p>Contours of equal probability density are characterized by a constant exponent
in (5.10.11):
</p>
<p>&minus; 1
2
&middot; 1
(1&minus;ρ2)(u
</p>
<p>2
1 +u22 &minus;2u1u2ρ)=&minus;
</p>
<p>1
</p>
<p>2
g(u)= const . (5.10.13)
</p>
<p>Let us take for the moment g(u)= 1.
In the original variables Eq. (5.10.13) becomes
</p>
<p>(x1 &minus;a1)2
</p>
<p>σ 21
&minus;2ρ x1 &minus;a1
</p>
<p>σ1
</p>
<p>x2 &minus;a2
σ2
</p>
<p>+ (x2 &minus;a2)
2
</p>
<p>σ 22
= 1&minus;ρ2 . (5.10.14)</p>
<p/>
</div>
<div class="page"><p/>
<p>5.10 The Multivariate Normal Distribution 97
</p>
<p>This is the equation of an ellipse centered around the point (a1,a2). The
principal axes of the ellipse make an angle α with respect to the axes x1
and x2. This angle and the half-diameters p1 and p2 can be determined from
Eq. (5.10.14) by using the known properties of conic sections:
</p>
<p>tan2α = 2ρσ1σ2
σ 21 &minus;σ 22
</p>
<p>, (5.10.15)
</p>
<p>p21 =
σ 21 σ
</p>
<p>2
2 (1&minus;ρ2)
</p>
<p>σ 22 cos
2α&minus;2ρσ1σ2 sinα cosα+σ 21 sin2α
</p>
<p>, (5.10.16)
</p>
<p>p22 =
σ 21 σ
</p>
<p>2
2 (1&minus;ρ2)
</p>
<p>σ 22 sin
2α+2ρσ1σ2 sinα cosα+σ 21 cos2α
</p>
<p>. (5.10.17)
</p>
<p>The ellipse with these properties is called the covariance ellipse of the
bivariate normal distribution. Several such ellipses are depicted in Fig. 5.11.
The covariance ellipse always lies inside a rectangle determined by the point
(a1,a2) and the standard deviations σ1 and σ2. It touches the rectangle at four
points. For the extreme cases ρ = &plusmn;1 the ellipse becomes one of the two
diagonals of this rectangle.
</p>
<p>From (5.10.14) it is clear that other lines of constant probability (for
g �= 1) are also ellipses, concentric and similar to the covariance ellipse and
situated inside (outside) of it for larger (smaller) probability. The bivariate
</p>
<p>Fig.5.11:Covariance ellipses.</p>
<p/>
</div>
<div class="page"><p/>
<p>98 5 Some Important Distributions and Theorems
</p>
<p>Fig.5.12: Probability density of a bivariate Gaussian distribution (left) and the corresponding
covariance ellipse (right). The three rows of the figure differ only in the numerical value of
the correlation coefficient ρ.
</p>
<p>normal distribution therefore corresponds to a surface in the three-dimensional
space (x1, x2, φ) (Fig. 5.12), whose horizontal sections are concentric ellipses.
For the largest probability this ellipse collapses to the point (a1,a2). The
vertical sections through the center have the form of a Gaussian distribution
whose width is directly proportional to the diameter of the covariance ellipse
along which the section extends. The probability of observing a pair x1, x2 of
random variables inside the covariance ellipse is equal to the integral
</p>
<p>&int;
</p>
<p>A
</p>
<p>φ(x)dx= 1&minus; e&minus; 12 = const , (5.10.18)</p>
<p/>
</div>
<div class="page"><p/>
<p>5.10 The Multivariate Normal Distribution 99
</p>
<p>where the region of integration A is given by the area within the covari-
ance ellipse (5.10.14). The relation (5.10.18) is obtained by application of
the transformation of variables y= T x with T =B&minus;1 to the distribution φ(x).
The resulting distribution has the properties σ(y1) = σ(y2) = 1 and
cov(y1,y2) = 0, i.e., it is of the form of (5.10.9). In this way the region of
integration is transformed to a unit circle centered about (a1,a2).
</p>
<p>In our consideration of the normal distribution of measurement errors of
a variable we found the interval a&minus;σ &le; x &le; a+σ to be the region in which
the probability density f (x) exceeded a given fraction, namely e&minus;1/2 of its
maximum value. The integral over this region was independent of σ . In the
case of two variables, the role of this region is taken over by the covariance
ellipse determined by σ1, σ2, and ρ, and not &ndash; as is sometimes incorrectly
assumed &ndash; by the rectangle that circumscribes the ellipse in Fig. 5.11. The
meaning of the covariance ellipse can also be seen from Fig. 5.13. Points 1
and 2, which lie on the covariance ellipse, correspond to equal probabilities
(P (1)=P (2)=Pe), although the distance of point 1 from the middle is less in
both coordinate directions. In addition, point 3 is more probable, and point 4
less probable (P (4) &lt; Pe, P (3) &gt; Pe), even though point 4 even closer is to
(a1,a2) than point 3.
</p>
<p>2
</p>
<p>x1
</p>
<p>x2
</p>
<p>σ2
</p>
<p>4
</p>
<p>1
</p>
<p>3
</p>
<p>σ1
</p>
<p>Fig.5.13: Relative probability for
various points from a bivariate Gau-
ssian distribution (P1 = P2 = Pe,
P3 &gt; Pe, P4 &lt; Pe).
</p>
<p>For three variables one obtains instead of the covariance ellipse a co-
variance ellipsoid, for n variables a hyperellipsoid in an n-dimensional space
(see also Sect. A.11). According to our construction, the covariance ellipsoid
is the hypersurface in the n-dimensional space on which the function g(x)
in the exponent of the normal distribution (5.10.1) has the constant value
g(x)= 1. For other values g(x) = const one obtains similar ellipsoids which
lie inside (g &lt; 1) or outside (g &gt; 1) of the covariance ellipsoid. In Sect. 6.6 it</p>
<p/>
</div>
<div class="page"><p/>
<p>100 5 Some Important Distributions and Theorems
</p>
<p>will be shown that the function g(x) follows a χ2-distribution with n degrees
of freedom if x follows the normal distribution (5.10.1). The probability to
find x inside the ellipsoid g = const is therefore
</p>
<p>W =
&int; g
</p>
<p>0
f (χ2;n)dχ2 = P
</p>
<p>(n
2
,
g
</p>
<p>2
</p>
<p>)
. (5.10.19)
</p>
<p>Here P is the incomplete gamma function given in Sect. D.5. For g = 1, that
is, for the covariance ellipsoid in n dimensions, this probability is
</p>
<p>Wn = P
(
n
</p>
<p>2
,
</p>
<p>1
</p>
<p>2
</p>
<p>)
. (5.10.20)
</p>
<p>Numerical values for small n are
</p>
<p>W1 = 0.68269 , W2 = 0.39347 , W3 = 0.19875 ,
W4 = 0.09020 , W5 = 0.03734 , W6 = 0.01439 .
</p>
<p>The probability decreases rapidly as n increases. In order to be able to give
regions for various n which correspond to equal probability content, one
specifies a value W on the left-hand side of (5.10.19) and determines the cor-
responding value of g. Then g is the quantile with probability W of the
χ2-distribution with n degrees of freedom (see also Appendix C.5),
</p>
<p>g = χ2W (n) . (5.10.21)
</p>
<p>The ellipsoid that corresponds to the value of g that contains x with the prob-
ability W is called the confidence ellipsoid of probability W . This expres-
sion can be understood to mean that, e.g., for W = 0.9 one should have 90%
confidence that x lies within the confidence ellipsoid.
</p>
<p>The variances σ 2i or the standard deviations Δi = σi also have a certain
meaning for n variables. The probability to observe the variable xi in the
region ai &minus;σi &lt; xi &lt; ai +σi is, as before, 68.3%, independent of the number
n of the variables. This only holds, however, when one places no requirements
on the positions of any of the other variables xj , j �= i.
</p>
<p>5.11 Convolutions of Distributions
</p>
<p>5.11.1 Folding Integrals
</p>
<p>On various occasions we have already discussed sums of random variables,
and in the derivation of the Central Limit Theorem, for example, we found the
characteristic function to be a useful tool in such considerations. We would</p>
<p/>
</div>
<div class="page"><p/>
<p>5.11 Convolutions of Distributions 101
</p>
<p>now like to discuss the distribution of the sum of two quantities, but for greater
clarity we will not make use of the characteristic function.
</p>
<p>A sum of two distributions is often observed in experiments. One could
be interested, for example, in the angular distribution of secondary particles
from the decay of an elementary particle. This can often be used to determine
the spin of the particle. The observed angle is the distribution of a sum of
random quantities, namely the decay angle and its measurement error. One
speaks of the convolution of two distributions.
</p>
<p>A
x
</p>
<p>y
</p>
<p>u = x + y
</p>
<p>Fig.5.14: Integration region for ( 5.11.4).
</p>
<p>Let the original quantities be x and y and the sum
</p>
<p>u = x+y . (5.11.1)
</p>
<p>A requirement for further treatment is that the original variables must be in-
dependent. In this case, the joint probability density is the product of simple
densities,
</p>
<p>f (x,y)= fx(x)fy(y) . (5.11.2)
If we now ask for the distribution function of u, i.e., for
</p>
<p>F(u)= P (u &lt; u)= P (x+y &lt; u) , (5.11.3)
</p>
<p>then this is obtained by integration of (5.11.2) over the hatched region A in
Fig. 5.14;
</p>
<p>F(u) =
&int; &int;
</p>
<p>A
</p>
<p>fx(x)fy(y)dx dy =
&int; &infin;
</p>
<p>&minus;&infin;
fx(x)dx
</p>
<p>&int; u&minus;x
</p>
<p>&minus;&infin;
fy(y)dy
</p>
<p>=
&int; &infin;
</p>
<p>&minus;&infin;
fy(y)dy
</p>
<p>&int; u&minus;y
</p>
<p>&minus;&infin;
fx(x)dx . (5.11.4)
</p>
<p>By differentiation one obtains the probability density for u,
</p>
<p>f (u)= dF(u)
du
</p>
<p>=
&int; &infin;
</p>
<p>&minus;&infin;
fx(x)fy(u&minus;x)dx =
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
fy(y)fx(u&minus;y)dy .
</p>
<p>(5.11.5)</p>
<p/>
</div>
<div class="page"><p/>
<p>102 5 Some Important Distributions and Theorems
</p>
<p>If x or y or both are only defined in a restricted region, then (5.11.5) is still
true. The limits of integration, however, may be limited. We will consider
various cases:
</p>
<p>(a) 0 &le; x &lt;&infin; , &minus;&infin;&lt; y &lt;&infin; :
</p>
<p>f (u)=
&int; u
</p>
<p>&minus;&infin;
fx(u&minus;y)fy(y)dy . (5.11.6)
</p>
<p>(Because y = u&minus;x and since for xmin = 0 one has ymax = u.)
</p>
<p>(b) 0 &le; x &lt;&infin; , 0 &le; y &lt;&infin; :
</p>
<p>f (u)=
&int; u
</p>
<p>0
fx(u&minus;y)fy(y)dy . (5.11.7)
</p>
<p>(c) a &le; x &lt; b , &minus;&infin;&lt; y &lt;&infin; :
</p>
<p>f (u)=
&int; b
</p>
<p>a
</p>
<p>fx(x)fy(u&minus;x)dx . (5.11.8)
</p>
<p>We will demonstrate case (d) in the following example, in which both x and
y are bounded from below and from above.
</p>
<p>Example 5.8: Convolution of uniform distributions
</p>
<p>With
</p>
<p>fx(x)=
{
</p>
<p>1, 0 &le; x &lt; 1
0 otherwise
</p>
<p>and fy(y)=
{
</p>
<p>1, 0 &le; y &lt; 1
0 otherwise
</p>
<p>and Eq. (5.11.8) we obtain
</p>
<p>f (u)=
&int; 1
</p>
<p>0
fy(u&minus;x)dx .
</p>
<p>We substitute v = u&minus;x, dv =&minus;dx and obtain
</p>
<p>f (u)=&minus;
&int; u&minus;1
</p>
<p>u
</p>
<p>fy(v)dv =
&int; u
</p>
<p>u&minus;1
fy(v)dv . (5.11.9)
</p>
<p>Clearly one has 0 &lt; u &lt; 2. We now consider separately the two cases
</p>
<p>(a) 0 &le; u &lt; 1 : f1(u)=
&int; u
</p>
<p>0
fy(v)dv =
</p>
<p>&int; u
</p>
<p>0
dv = u ,
</p>
<p>(b) 1 &le; u &lt; 2 : f2(u)=
&int; 1
</p>
<p>u&minus;1
fy(v)dv =
</p>
<p>&int; 1
</p>
<p>u&minus;1
dv = 2&minus;u .
</p>
<p>(5.11.10)</p>
<p/>
</div>
<div class="page"><p/>
<p>5.11 Convolutions of Distributions 103
</p>
<p>0
3210
</p>
<p>1
</p>
<p>f(u)
</p>
<p>(b
)
</p>
<p>(a)
</p>
<p>(c)
</p>
<p>Fig.5.15: Convolution of uni-
form distributions. Probabil-
ity density of the sum u of
uniformly distributed random
variables x (a) u = x, (b)
u = x+x, (c) u = x+x+x.
</p>
<p>Note that the lower (upper) limit of integration is not lower (higher) than the
value 0 (1). The result is a triangular distribution (Fig. 5.15).
</p>
<p>If this result is folded again with a uniform distribution, i.e., if u is the
sum of three independent uniformly distributed variables, then one obtains
</p>
<p>f (u)=
</p>
<p>⎧
⎪⎪⎨
⎪⎪⎩
</p>
<p>1
2u
</p>
<p>2 , 0 &le; u &lt; 1 ,
1
2(&minus;2u2 +6u&minus;3) , 1 &le; u &lt; 2 ,
1
2(u&minus;3)2 , 2 &le; u &lt; 3 .
</p>
<p>(5.11.11)
</p>
<p>The proof is left to the reader. The distribution consists of three parabolic sec-
tions (Fig. 5.15) and is similar already to the Gaussian distribution predicted
by the Central Limit Theorem.
</p>
<p>5.11.2 Convolutions with the Normal Distribution
</p>
<p>Suppose a quantity of experimental interest x can be considered to be a
random variable with probability density fx(x). It is measured with a mea-
surement error y, which follows a normal distribution with a mean of zero
and a variance of σ 2. The result of the measurement is then the sum
</p>
<p>u = x+y . (5.11.12)
</p>
<p>Its probability density is [see also (5.11.4)]
</p>
<p>f (u)= 1&radic;
2πσ
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
fx(x)exp[&minus;(u&minus;x)2/2σ 2]dx . (5.11.13)
</p>
<p>By carrying out many measurements, f (u) can be experimentally determined.
The experimenter is interested, however, in the function fx(x). Unfortunately,
Eq. (5.11.13) cannot in general be solved for fx(x). This is only possible
for a restricted class of functions f (u). Therefore one usually approaches
the problem in a different way. From earlier measurements or theoretical
considerations one possesses knowledge about the form of fx(x), e.g., one
might assume that fx(x) is described by a uniform distribution, without,</p>
<p/>
</div>
<div class="page"><p/>
<p>104 5 Some Important Distributions and Theorems
</p>
<p>however, knowing its boundaries a and b. One then carries out the convo-
lution (5.11.13), compares the resulting function f (u) with the experiment
and in this way determines the unknown parameters (in our example a and b).
</p>
<p>In many cases it is not even possible to perform the integration (5.11.13)
analytically. Numerical procedures, e.g., the Monte Carlo method, then have
to be used. Sometimes approximations (cf. Example 5.11) give useful results.
Because of the importance in many experiments of convolution with the nor-
mal distribution we will study some examples.
</p>
<p>Example 5.9: Convolution of uniform and normal distributions
</p>
<p>Using Eqs. (3.3.26) and (5.11.8) and substituting v = (x&minus;u)/σ we obtain
</p>
<p>f (u) = 1
b&minus;a
</p>
<p>1&radic;
2πσ
</p>
<p>&int; b
</p>
<p>a
</p>
<p>exp[&minus;(u&minus;x)2/2σ 2]dx
</p>
<p>= 1
b&minus;a
</p>
<p>1&radic;
2π
</p>
<p>&int; (b&minus;u)/σ
</p>
<p>(a&minus;u)/σ
exp(&minus;1
</p>
<p>2
v2)dv ,
</p>
<p>f (u) = 1
b&minus;a
</p>
<p>{
ψ0
</p>
<p>(
b&minus;u
σ
</p>
<p>)
&minus;ψ0
</p>
<p>(
a&minus;u
σ
</p>
<p>)}
. (5.11.14)
</p>
<p>The function ψ has already been defined in (5.6.5). Figure 5.16 shows the re-
sult for a= 0, b= 6, σ = 1. If one has |b&minus;a|≫ σ (as is the case in Fig. 5.16),
one of the terms in parentheses in (5.11.14) is either 0 or 1. The rising edge
of the uniform distribution at u = a is replaced by the distribution function
of the normal distribution with standard deviation σ (see also Fig. 5.7). The
falling edge at u= b is its &ldquo;mirror image&rdquo;.
</p>
<p>0
</p>
<p>f(u)
</p>
<p>2 64 8 10 x,u-2-4-6
</p>
<p>f(x)
</p>
<p>Fig.5.16: Convolution of a uniform and Gaussian distribution.
</p>
<p>Example 5.10: Convolution of two normal distributions. &ldquo;Quadratic
addition of errors&rdquo;
</p>
<p>If one convolutes two normal distributions with mean values 0 and variances
σ 2x and σ
</p>
<p>2
y , one obtains
</p>
<p>f (u)= 1&radic;
2πσ
</p>
<p>exp(&minus;u2/2σ 2) , σ 2 = σ 2x +σ 2y . (5.11.15)</p>
<p/>
</div>
<div class="page"><p/>
<p>5.11 Convolutions of Distributions 105
</p>
<p>The proof has already been shown in Sect. 5.7 with the help of the charac-
teristic function. It can also be obtained by computation of the folding inte-
gral (5.11.5). If the distributions fx(x) and fy(y) describe two independent
sources of measurement errors, the result (5.11.15) is known as the &ldquo;quadratic
addition of errors&rdquo;.
</p>
<p>Example 5.11: Convolution of exponential and normal distributions
</p>
<p>With
</p>
<p>fx(x) =
1
</p>
<p>τ
exp(&minus;x/τ) , x &gt; 0 ,
</p>
<p>fy(y) =
1&radic;
</p>
<p>2πσ
exp(&minus;y2/2σ 2) ,
</p>
<p>Eq. (5.11.6) takes on the following form:
</p>
<p>f (u)= 1&radic;
2πστ
</p>
<p>&int; u
</p>
<p>&minus;&infin;
exp[&minus;(u&minus;y)/τ ]exp(&minus;y2/2σ 2)dy .
</p>
<p>We can rewrite the exponent
</p>
<p>&minus; 1
2σ 2τ
</p>
<p>[2σ 2(u&minus;y)+ τy2]
</p>
<p>= &minus; 1
2σ 2τ
</p>
<p>[
2σ 2u&minus;2σ 2y+ τy2 + σ
</p>
<p>4
</p>
<p>τ
&minus; σ
</p>
<p>4
</p>
<p>τ
</p>
<p>]
</p>
<p>= &minus;u
τ
+ σ
</p>
<p>2
</p>
<p>2τ 2
&minus; 1
</p>
<p>2σ 2
</p>
<p>(
y&minus; σ
</p>
<p>2
</p>
<p>τ
</p>
<p>)2
</p>
<p>and obtain
</p>
<p>f (u)= 1&radic;
2πστ
</p>
<p>exp
</p>
<p>{
σ 2
</p>
<p>2τ 2
&minus; u
</p>
<p>τ
</p>
<p>}&int; u&minus;σ 2/τ
</p>
<p>&minus;&infin;
exp
</p>
<p>(&minus;v2
2σ 2
</p>
<p>)
dv .
</p>
<p>We now require that σ ≪ τ , i.e., that the measurement error is much smaller
than the typical value (width) of the exponential distribution. In addition, we
only consider values of u for which u&minus;σ 2/τ ≫ σ , i.e., u≫ σ . The integral
is then approximately equal to
</p>
<p>&radic;
2πσ or
</p>
<p>f (u)&asymp; 1
τ
</p>
<p>exp
</p>
<p>{
&minus;u
τ
+ σ
</p>
<p>2
</p>
<p>2τ 2
</p>
<p>}
.
</p>
<p>In a semi-logarithmic representation, i.e., in a plot of lnf (u) versus u, the
curve f (u) lies above the curve fx(x), by an amount σ 2/2τ 2, since
</p>
<p>lnf (u)= ln 1
τ
+ σ
</p>
<p>2
</p>
<p>2τ 2
&minus; u
</p>
<p>τ
= lnfx(x)+
</p>
<p>σ 2
</p>
<p>2τ 2
.</p>
<p/>
</div>
<div class="page"><p/>
<p>106 5 Some Important Distributions and Theorems
</p>
<p>This is plotted in Fig. 5.17. The result can be qualitatively understood in the
following way. For each small x interval of the exponential distribution, the
convolution leads with equal probability to a shift to the left or to the right.
Since, however, the exponential distribution for a given u is greater for small
values of x, contributions to the convolution f (u) originate with greater prob-
ability from the left than from the right. This leads to an overall shift to the
right of f (u) with respect to fx(x).
</p>
<p>5
</p>
<p>0,01
</p>
<p>0,05
</p>
<p>0,1
</p>
<p>0,5
</p>
<p>1
f(x)
</p>
<p>f(u)
</p>
<p>x,u
0 1 2 3 4
</p>
<p>Fig.5.17:Convolution of exponential and normal distributions.
</p>
<p>5.12 Example Programs
</p>
<p>frequency and demonstrate statistical fluctuations
The program simulates the problem of Example 5.1. It allows input of values for nexp,
nfly, and P(A), and then consecutively performs nexp simulated experiments. In each
experiment nfly objects are analyzed. Each object has a probability P(A) to have the
property A. For each experiment one line of output is produced containing the current
number iexp of the experiment, the number NA of objects with the property A and the
frequency hA = NA/nfly with which the property A was found. The fluctuation of
hA around the known input value P(A) in the individual experiments gives a good
impression of the statistical error of an experiment.
</p>
<p>Example Program 5.1: Class to simulate empiricalE1Distrib</p>
<p/>
</div>
<div class="page"><p/>
<p>5.12 Example Programs 107
</p>
<p>The principle of the experiment of Rutherford and Geiger is described in
Example 5.3. It is simulated as follows. Input quantities are the number N of de-
cays observed and the number nint of partial intervals ΔT of the total observation
time T . For simplicity the length of each partial interval is set equal to one. A total of
N random events are simulated by simply generating N random numbers uniformly
distributed between 0 and T . They are entered into a histogram with nint intervals.
The histogram is first displayed graphically and then analyzed numerically. For each
number k = 0,1, . . . ,Nint the program determines how many intervals N(k) of the
histogram have k entries. The numbers N(k) themselves are presented in the form of
another histogram.
</p>
<p>Show that for the process simulated in this example program one obtains in the
limit N &rarr;&infin;
</p>
<p>N(k)= nintWNk (p = 1/nint) .
</p>
<p>If N is increased step by step and at the same time λ=Np =N/nint is kept constant,
then for large N one has
</p>
<p>WNk (p = λ/N)&rarr;
λk
</p>
<p>k! e
&minus;λ
</p>
<p>and, in the limit N &rarr;&infin;,
N(k)= nint
</p>
<p>λk
</p>
<p>k! e
&minus;λ .
</p>
<p>Check the above statements by running the program with suitable pairs of numbers,
e.g., (N,nint)= (4,2), (40,20), . . . , (2000,1000), by reading the numbers N(k) from
the graphics display and by comparing them with the statements above.
</p>
<p>Galton&rsquo;s board is a simple implementation of Laplace&rsquo;s model described in
Example 5.7. The vertical board contains rows of horizontally oriented nails as shown
in Fig. 5.18. The rows of nails are labeled j = 1,2, . . . ,n, and row j has j nails. One
by one a total of Nexp balls fall onto the nail in row 1. There each ball is deflected
with probability p to the right and with probability (1&minus;p) to the left. (In a realistic
board one has p = 1/2.) The distance between the nails is chosen in such a way that
in each case the ball hits one of the two nails in row 2 and there again it is deflected
with the probability p to the right. After falling through n rows each ball assumes
one of n+ 1 places, which we denote by k = 0 (on the left), k = 1, . . ., k = n (on
the right). After a total of Nexp experiments (i.e., balls) one finds N(k) balls for each
value k.
</p>
<p>The program allows input of numerical values for Nexp, n, and p. For each
experiment the number k is first set to zero and n random numbers rj are gener-
ated from a uniform distribution and analyzed. For each rj &lt; p (corresponding to a
deflection to the right in row j ) the number k is increased by 1. For each experiment
the value of k is entered into a histogram. After all experiments are simulated the
histogram is displayed.
</p>
<p>Example Program 5.2: Class to simulate the experiment
</p>
<p>of Rutherford and Geiger
</p>
<p>E2Distrib
</p>
<p>Example Program 5.3: Class to simulate Galton&rsquo;s boardE3Distrib</p>
<p/>
</div>
<div class="page"><p/>
<p>108 5 Some Important Distributions and Theorems
</p>
<p>Fig.5.18:Arrangement of nails in Galton&rsquo;s board and one possible trajectory of a ball.
</p>
<p>Show that in the limit N &rarr;&infin; one has
</p>
<p>N(k)=NW nk (p) .
</p>
<p>By choosing and entering suitable pairs of numbers (n,p), e.g., (n,p) = (1,0.5),
(2,0.25), (10,0.05), (100,0.005) approximate the Poisson limit
</p>
<p>N(k)=N λ
k
</p>
<p>k! e
&minus;λ , λ= np ,
</p>
<p>and compare these predictions with the results of your simulations.</p>
<p/>
</div>
<div class="page"><p/>
<p>6. Samples
</p>
<p>In the last chapter we discussed a number of distributions, but we have not
specified how they are realized in a particular case. We have only given the
probability that a random variable x will lie within an interval with boundaries
x and x+ dx. This probability depends on certain parameters describing its
distribution (like λ in the case of the Poisson distribution) which are usually
unknown. We therefore have no direct knowledge of the probability distribu-
tion and have to approximate it by a frequency distribution obtained experi-
mentally. The number of measurements performed for this purpose, called a
sample, is necessarily finite. To discuss the elements of sampling theory we
first have to introduce a number of new definitions.
</p>
<p>6.1 Random Samples. Distribution
</p>
<p>of a Sample. Estimators
</p>
<p>Every sample is taken from a set of elements which correspond to the possible
results of an individual observation. Such a set, which usually has infinitely
many elements, is called a population. If a sample of n elements is taken
from it, then we say that the sample has the size n. Let the distribution of the
random variable x in the population be given by the probability density f (x).
We are interested in the values of x assumed by the individual elements of
the sample. Suppose that we take ℓ samples of size n and find the following
values for x:
</p>
<p>1st sample: x(1)1 ,x
(1)
2 , . . . ,x
</p>
<p>(1)
n ,
</p>
<p>...
</p>
<p>j th sample: x(j)1 ,x
(j)
</p>
<p>2 , . . . ,x
(j)
n ,
</p>
<p>...
</p>
<p>ℓth sample: x(ℓ)1 ,x
(ℓ)
2 , . . . ,x
</p>
<p>(ℓ)
n .
</p>
<p>S. Brandt, Data Analysis: Statistical and Computational Methods for Scientists and Engineers,
DOI 10.1007/978-3-319-03762-2__6, &copy; Springer International Publishing Switzerland 2014
</p>
<p>109</p>
<p/>
</div>
<div class="page"><p/>
<p>110 6 Samples
</p>
<p>We group the result of one sample into an n-dimensional vector
</p>
<p>x
(j) = (x(j)1 ,x
</p>
<p>(j)
</p>
<p>2 , . . . ,x
(j)
n ) , (6.1.1)
</p>
<p>which can be considered the position vector in an n-dimensional sample space
(Sect. 2.1). Its probability density is
</p>
<p>g(x)= g(x1,x2, . . . ,xn) . (6.1.2)
</p>
<p>This function must fulfill two conditions in order for the sample to be random:
</p>
<p>(a) The individual xi must be independent, e.g., one must have
</p>
<p>g(x)= g1(x1)g2(x2) . . . gn(xn) . (6.1.3)
</p>
<p>(b) The individual marginal distributions must be identical and equal to the
probability density f (x) of the population,
</p>
<p>g1(x)= g2(x)= . . . = gn(x)= f (x) . (6.1.4)
</p>
<p>Comparing with (6.1.2) it is clear that there is a simple relation between
a population and a sample only if these conditions are fulfilled. In the
following we will mean by the word sample a random sample unless
otherwise stated.
</p>
<p>It should be emphasized that in the actual process of sampling it is often
quite difficult to ensure randomness. Because of the large variety of applica-
tions, a general prescription cannot be given. In order to obtain reliable results
from sampling, we have to take the utmost precautions to meet the require-
ments (6.1.3) and (6.1.4). Independence (6.1.3) can be checked to a certain
extent by comparing the frequency distributions of the first, second, . . . ele-
ments of a large number of samples. It is very difficult, however, to ensure that
the samples in fact come from a population with the probability density f (x).
If the elements of the population can be numbered, it is often useful to use
random numbers to select the elements for the sample.
</p>
<p>We now suppose that the n elements of a sample are ordered according
to the value of the variable, e.g., marked on the x axis, and we ask for the
number of elements of the sample nx for which x &lt; x, for arbitrary x. The
function
</p>
<p>Wn(x)= nx/n (6.1.5)
takes on the role of an empirical distribution function. It is a step function that
increases by 1/n as soon as x is equal to one of the values x of an element
of the sample. It is called the sample distribution function. It is clearly an
approximation for F(x), the distribution function of the population, which it
approaches in the limit n&rarr;&infin;.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.2 Samples from Continuous Populations 111
</p>
<p>A function of the elements of a sample (6.1.1) is called a statistic. Since x
is a random variable, a statistic is itself a random variable. The most important
example is the sample mean,
</p>
<p>x̄ = 1
n
(x1 +x2 + . . .+xn) . (6.1.6)
</p>
<p>A typical problem of data analysis is the following. The general mathe-
matical form of the probability density of the population is known.
In radioactive decay, for example, the number of nuclei which decay before
the time t = τ is Nτ = N0(1&minus; exp(&minus;λτ)), if N0 nuclei existed at time t = 0.
Here, however, the decay constant λ is, in general, not known. By taking a
finite sample (measuring a finite number of decay times of individual nuclei)
we want to determine the parameter λ as accurately as possible. Since such a
task cannot be exactly solved, because the sample is finite, one speaks of the
estimation of parameters. To estimate a parameter λ of a distribution function
one uses an estimator
</p>
<p>S = S(x1,x2, . . . ,xn) . (6.1.7)
</p>
<p>An estimator is said to be unbiased if for arbitrary sample size the expectation
value of the (random) quantity S is equal to the parameter to be estimated:
</p>
<p>E{S(x1,x2, . . . ,xn)} = λ for all n. (6.1.8)
</p>
<p>An estimator is said to be consistent if its variance vanishes for arbitrarily
large sample size, i.e., if
</p>
<p>lim
n&rarr;&infin;
</p>
<p>σ(S)= 0 . (6.1.9)
</p>
<p>Often one can give a lower limit for the variance of an estimator of a parame-
ter. If one finds an estimator S0 whose variance is equal to this limit, then one
apparently has the &ldquo;best possible&rdquo; estimator. S0 is then said to be an efficient
estimator for λ.
</p>
<p>6.2 Samples from Continuous Populations:
</p>
<p>Mean and Variance of a Sample
</p>
<p>The case of greatest interest in applications concerns a sample from an inf-
initely large continuous population described by the probability density f (x).
The sample mean (6.1.6) is a random variable, as are all statistics. Let us
consider its expectation value
</p>
<p>E(x̄)= 1
n
{E(x1)+E(x2)+ . . .+E(xn)} = x̂ . (6.2.1)</p>
<p/>
</div>
<div class="page"><p/>
<p>112 6 Samples
</p>
<p>This expectation value is equal to the expectation value of x. Since Eq. (6.2.1)
holds for all values of n, the arithmetic mean of a sample is, as one would
expect, an unbiased estimator for the mean of the population. The character-
istic function of the random variable x̄ is
</p>
<p>ϕx̄(t)=
{
ϕ x
</p>
<p>n
(t)
</p>
<p>}n
=
</p>
<p>{
ϕx
</p>
<p>(
t
</p>
<p>n
</p>
<p>)}n
. (6.2.2)
</p>
<p>Next we are interested in the variance of x̄,
</p>
<p>σ 2(x̄) = E{(x̄&minus;E(x̄))2} = E
{(
</p>
<p>x1 +x2+ . . .+xn
n
</p>
<p>&minus; x̂
)2}
</p>
<p>= 1
n2
</p>
<p>E{[(x1 &minus; x̂)+ (x2&minus; x̂)+ . . .+ (xn&minus; x̂)]2} .
</p>
<p>Since all of the xi are independent, all of the cross terms of the type
E{(xi&minus; x̂)(xj &minus; x̂)} , i �= j (i.e., all of the covariances) vanish, and we obtain
</p>
<p>σ 2(x̄)= 1
n
σ 2(x) . (6.2.3)
</p>
<p>One thus shows that x̄ is a consistent estimator for x̂. The variance (6.2.3) is
itself, however, not a random variable, and is therefore not directly obtainable
by experiment. As a definition for the sample variance we could try using the
arithmetic mean of squared differences
</p>
<p>s&prime;2 = 1
n
{(x1 &minus; x̄)2 + (x2 &minus; x̄)2 + . . .+ (xn&minus; x̄)2} . (6.2.4)
</p>
<p>Its expectation value is
</p>
<p>E(s&prime;2) = 1
n
E
</p>
<p>{
n&sum;
</p>
<p>i=1
(xi &minus; x̄)2
</p>
<p>}
</p>
<p>= 1
n
E
</p>
<p>{
n&sum;
</p>
<p>i=1
(xi &minus; x̂+ x̂&minus; x̄)2
</p>
<p>}
</p>
<p>= 1
n
E
</p>
<p>{
n&sum;
</p>
<p>i=1
(xi &minus; x̂)2 +
</p>
<p>n&sum;
</p>
<p>i=1
(x̂&minus; x̄)2 +2
</p>
<p>n&sum;
</p>
<p>i=1
(xi &minus; x̂)(x̂&minus; x̄)
</p>
<p>}
</p>
<p>= 1
n
</p>
<p>n&sum;
</p>
<p>i=1
{E((xi &minus; x̂)2)&minus;E((x̄&minus; x̂)2)}
</p>
<p>= 1
n
</p>
<p>{
nσ 2(x)&minus;n
</p>
<p>(
1
</p>
<p>n
σ 2(x)
</p>
<p>)}
,
</p>
<p>E(s&prime;2) = n&minus;1
n
</p>
<p>σ 2(x) . (6.2.5)</p>
<p/>
</div>
<div class="page"><p/>
<p>6.2 Samples from Continuous Populations 113
</p>
<p>Hence one sees that the sample variance defined in this way is a biased
estimator for the population variance, having an expectation value smaller
than σ 2(x). We can see directly from (6.2.5), however, the size of the bias.
We therefore change our definition (6.2.4) and write for the sample variance
</p>
<p>s2 = 1
n&minus;1{(x1 &minus; x̄)
</p>
<p>2 + (x2 &minus; x̄)2 + . . .+ (xn&minus; x̄)2} . (6.2.6)
</p>
<p>This is now an unbiased estimator for σ 2(x). The value (n&minus;1) in the denom-
inator appears at first to be somewhat strange. One must consider, however,
that for n = 1 the sample mean is equal to the value x of the sole element
of the sample (x = x̄) and that therefore the quantity (6.2.4) would vanish.
That is related to the fact that in (6.2.4) &ndash; and also in (6.2.6) &ndash; the sample
mean x̄ was used instead of the population mean x̂, since the latter was not
known. Part of the information contained in the sample first had to be used
and was not available for the calculation of the variance. The effective number
of elements available for calculating the variance is therefore reduced. This is
taken into consideration by reducing the denominator of the arithmetic mean
(6.2.4). The same line of reasoning is repeated quantitatively in Sect. 6.5.
</p>
<p>If we substitute the estimator for the population variance (6.2.6) into
(6.2.3), we obtain an estimator for the variance of the mean
</p>
<p>s2(x̄)= 1
n
</p>
<p>s2(x)= 1
n(n&minus;1)
</p>
<p>n&sum;
</p>
<p>i=1
(xi &minus; x̄)2 . (6.2.7)
</p>
<p>The corresponding standard deviation can be considered to be the error of the
mean
</p>
<p>Δx̄ =
&radic;
</p>
<p>s2(x̄)= s(x̄)= 1&radic;
n
</p>
<p>s(x) . (6.2.8)
</p>
<p>Of course we are also interested in the error of the sample variance
(6.2.6). In Sect. 6.6 we will show that this quantity can be determined un-
der the assumption that the population follows a normal distribution. We will
use the result here ahead of time. The variance of s2 is
</p>
<p>var(s2)=
(
</p>
<p>σ 2
</p>
<p>n&minus;1
</p>
<p>)2
2(n&minus;1) . (6.2.9)
</p>
<p>If we substitute the estimator (6.2.6) into the right-hand side for σ 2 and take
the square root, we obtain for the error of the sample variance
</p>
<p>Δs2 = s2
&radic;
</p>
<p>2
</p>
<p>(n&minus;1) . (6.2.10)
</p>
<p>Finally we give explicit expressions for estimators of the sample standard dev-
iation and its error. The first is simply the square root of the sample variance</p>
<p/>
</div>
<div class="page"><p/>
<p>114 6 Samples
</p>
<p>s =
&radic;
</p>
<p>s2 = 1&radic;
n&minus;1
</p>
<p>&radic;&sum;
(xi &minus; x̄)2 . (6.2.11)
</p>
<p>The error of the sample standard deviation is obtained from (6.2.10) by error
propagation, which gives
</p>
<p>Δs = s&radic;
2(n&minus;1)
</p>
<p>. (6.2.12)
</p>
<p>Example 6.1: Computation of the sample mean and variance from data
Suppose one has n= 7 measurements of a certain quantity (e.g., the length of
an object). Their values are 10.5, 10.9, 9.2, 9.8, 9.0, 10.4, 10.7. The computa-
tion is made easier if one uses the fact that all of the measured values are near
a = 10, i.e., they are of the form xi = a+ δi . The relation (6.1.6) then gives
</p>
<p>x̄ = 1
n
</p>
<p>n&sum;
</p>
<p>i=1
xi =
</p>
<p>1
</p>
<p>n
</p>
<p>n&sum;
</p>
<p>i=1
(a+ δi)= a+
</p>
<p>1
</p>
<p>n
</p>
<p>n&sum;
</p>
<p>i=1
δi = a+Δ
</p>
<p>with
</p>
<p>Δ = 1
n
</p>
<p>n&sum;
</p>
<p>i=1
δi =
</p>
<p>1
</p>
<p>7
(0.5+0.9&minus;0.8&minus;0.2&minus;1.0+0.4+0.7)
</p>
<p>= 0.5/7 = 0.07 .
We thus have x̄ = 10+Δ= 10.07.
</p>
<p>The sample variance is computed according to (6.2.6) to be
</p>
<p>s2 = 1
n&minus;1
</p>
<p>n&sum;
</p>
<p>i=1
(xi &minus; x̄)2
</p>
<p>= 1
n&minus;1
</p>
<p>n&sum;
</p>
<p>i=1
(x2i &minus;2xix̄+ x̄2)
</p>
<p>= 1
n&minus;1
</p>
<p>{
n&sum;
</p>
<p>i=1
x2i &minus;nx̄2
</p>
<p>}
.
</p>
<p>The result can be obtained either by the first or last line of the relation above.
In the last line only one difference is computed, not n. The numbers to be
squared, however, are usually considerably larger, and one must consider the
problem of rounding errors. We therefore use the original expression
</p>
<p>s2 = 1
6
{0.432 +0.832+0.872 +0.272 +1.072 +0.332+0.632}
</p>
<p>= 1
6
{0.1849+0.6889+0.7569+0.0729+1.1449+0.1089
</p>
<p>+0.3969}
= 3.3543/6 &asymp; 0.56 .</p>
<p/>
</div>
<div class="page"><p/>
<p>6.3 Graphical Representation of Samples 115
</p>
<p>The sample standard deviation is s&asymp; 0.75. From (6.2.8), (6.2.10), and (6.2.12)
we obtain finally Δx̄ = 0.28, Δs2 = 0.32, and Δs = 0.21.
</p>
<p>Naturally one does not usually compute the sample mean and variance by
hand, but rather by the class Sample and its methods.
</p>
<p>6.3 Graphical Representation of Samples:
</p>
<p>Histograms and Scatter Plots
</p>
<p>After the theoretical considerations of the last sections we now turn to some
simple practical aspects of the analysis of sample data. An important tool for
this is the representation of the data in graphical form.
</p>
<p>A sample
x1,x2, . . . ,xn ,
</p>
<p>which depends on a single variable x can be represented simply by means of
tick marks on an x axis. We will call such a representation a one-dimensional
</p>
<p>Table6.1: Values of resistance R of 100 individual resistors of nominal value 200 kΩ . The
data are graphically represented in Fig. 6.1.
</p>
<p>193.199 195.673 195.757 196.051 196.092
196.596 196.679 196.763 196.847 197.267
197.392 197.477 198.189 198.650 198.944
199.070 199.111 199.153 199.237 199.698
199.572 199.614 199.824 199.908 200.118
200.160 200.243 200.285 200.453 200.704
200.746 200.830 200.872 200.914 200.956
200.998 200.998 201.123 201.208 201.333
201.375 201.543 201.543 201.584 201.711
201.878 201.919 202.004 202.004 202.088
202.172 202.172 202.297 202.339 202.381
202.507 202.591 202.633 202.716 202.884
203.051 203.052 203.094 203.094 203.177
203.178 203.219 203.764 203.765 203.848
203.890 203.974 204.184 204.267 204.352
204.352 204.729 205.106 205.148 205.231
205.357 205.400 205.483 206.070 206.112
206.154 206.155 206.615 206.657 206.993
207.243 207.621 208.124 208.375 208.502
208.628 208.670 208.711 210.012 211.394</p>
<p/>
</div>
<div class="page"><p/>
<p>116 6 Samples
</p>
<p>scatter plot. It contains all the information about the sample. Table 6.1 con-
tains the values x1,x2, . . . ,xn of a sample of size 100, obtained from mea-
suring the resistance R of 100 individual resistors of nominal value 200kΩ .
After obtaining the sample the measurements were ordered.
</p>
<p>Fig.6.1: Representation of the data from Table 6.1 as (a) a one-dimensional scatter plot, (b)
a bar diagram, (c) a step diagram, and (d) a diagram of measured points with error bars.
</p>
<p>Figure 6.1a shows the corresponding scatter plot. Qualitatively one can
estimate the mean and variance from the position and width of the clustering
of tick marks.
</p>
<p>Another graphical representation is usually better suited to visualize the
sample by using the second dimension available on the paper. The x axis is
used as abscissa and divided into r intervals
</p>
<p>ξ1,ξ2, . . . ,ξr
</p>
<p>of equal width Δx. These intervals are called bins. The centers of the bins
have the x-values
</p>
<p>x1,x2, . . . ,xr .
</p>
<p>On the vertical axis one plots the corresponding numbers of sample elements
</p>
<p>n1,n2, . . . ,nr</p>
<p/>
</div>
<div class="page"><p/>
<p>6.3 Graphical Representation of Samples 117
</p>
<p>that fall into bins ξ1,ξ2, . . . ,ξr . The diagram obtained in this way is called a
histogram of the sample. This can be interpreted as a frequency distribution,
since hk = nk/n is a frequency, i.e., a measure of the probability pk to observe
a sample element in the interval ξk. For the graphical form of histograms,
various methods are used. In a bar diagram the values nk are represented as
bars perpendicular to the x axis on the xk values (Fig. 6.1b). In a step diagram
the nk are represented as horizontal lines that cover the entire width ξk of the
interval. Neighboring horizontal lines are connected by perpendicular lines
(Fig. 6.1c). The fraction of the area covering each interval ξk of the x axis
is then proportional to the number nk of the sample elements in the interval.
(If one uses the area in the interval for the graphical representation of nk , then
the bins can also have different widths.) In economics bar diagrams are most
commonly used. (Sometimes one also sees diagrams in which, instead of bars,
line segments are used to connect the tips of the bars. In contrast to the step
diagram, the resulting figure does not have an area proportional to the sample
size n.) In the natural sciences, step diagrams are more common.
</p>
<p>In Sect. 6.8 we will determine that as long as the values nk are not too
small, their statistical errors are given by Δnk =
</p>
<p>&radic;
nk. In order to plot them
</p>
<p>on a graph, the observed values nk can be drawn as points with vertical error
bars ending at the points nk&plusmn;
</p>
<p>&radic;
nk (Fig. 6.1d).
</p>
<p>It is clear that the relative errors Δnk/nk = 1/
&radic;
nk decrease for increas-
</p>
<p>ing nk, i.e., for a sample of fixed size n they decrease for increasing bin width
of the histogram. On the other hand, by choosing a larger interval width, one
loses any finer structure of the data with respect to the variable x. The ability
of a histogram to convey information therefore depends crucially on the ap-
propriate choice of the bin width, usually found only after several attempts.
</p>
<p>Example 6.2: Histograms of the same sample with various choices
of bin width
</p>
<p>In Fig. 6.2 four histograms of the same sample are shown. The population
is a Gaussian distribution, which is also shown as a continuous curve. This
was scaled in such a way that the area under the histogram is equal to the
area under the Gaussian curve. Although the information contained in the
plot is greater for a smaller bin width &ndash; for vanishing bin width the histogram
becomes a one dimensional scatter plot &ndash; one notices the similarity between
the histogram and Gaussian curve much more easily for the larger bin width.
This is because for the larger bin width the relative statistical fluctuations of
the contents of individual bins are smaller. The individual steps of the his-
togram differ less from the curve.
</p>
<p>Constructing a histogram from a sample is a simple programming task.
Suppose the histogram has nx bins of width Δx, with the first interval extend-
ing from x = x0 to x = x0 +Δx. The contents of the histogram is put into an</p>
<p/>
</div>
<div class="page"><p/>
<p>118 6 Samples
</p>
<p>Fig.6.2: Histogram of the same sample from a Gaussian distribution represented with four
different bin widths.
</p>
<p>Graphical display of histograms can be accomplished using methods of
the class (Appendix F). With them the user can freely
adjust all of the parameters that determine the appearance of the plot, such as
the page format, scale factors, colors, line thickness, etc. Often it is convenient
to use the class
freedom but by a single call gives rise to the graphical output of a histogram,
stored in the computer.
</p>
<p>A histogram allows a first direct look at the nature of the data. It ans-
wers questions such as &ldquo;Are the data more or less distributed according to a
Gaussian?&rdquo; or &ldquo;Do there exist points exceptionally far away from the average
value?&rdquo;. If the histogram leads one to conclude that the population is dis-
tributed according to a Gaussian, then the mean and standard deviation can
be estimated directly from the plot. The mean is the center of gravity of the
histogram. The standard deviation is obtained as in the following example.
</p>
<p>array with the first bin in the second bin in , etc.
</p>
<p>The histogram is then specified in the computer by the array and the
</p>
<p>three values x0,Δx, and nx . The class permits construction and
</p>
<p>administration of a histogram.
</p>
<p>hist, hist[0] hist[1]
</p>
<p>hist
</p>
<p>stogramHi
</p>
<p>DatanGraphics
</p>
<p>which does not allow for thisGraphicsWithHistogram</p>
<p/>
</div>
<div class="page"><p/>
<p>6.3 Graphical Representation of Samples 119
</p>
<p>Example 6.3: Full width at half maximum (FWHM)
If the form of histogram allows one to assume that the represented sample
originates from a Gaussian distribution, then one can draw by hand a Gaussian
bell curve that follows the histogram as closely as possible. The position of
the maximum is a good estimator for the mean of the sample. One then draws
a horizontal line at half of the height of the maximum. This crosses the bell
curve at the points xa and xb. The quantity
</p>
<p>f = xb&minus;xa
is called the full width at half maximum (FWHM). One can easily determine
for a Gaussian distribution the simple relation
</p>
<p>σ = f&radic;
&minus;8ln 12
</p>
<p>&asymp; 0.4247 f (6.3.1)
</p>
<p>between the standard deviation and FWHM. This expression can be used
to estimate the standard deviation of a sample when f is obtained from a
histogram.
</p>
<p>We now use the Monte Carlo method (Chap. 4) together with histograms
in order to illustrate the concepts of mean, standard deviation, and variance of
a sample, and their errors, as introduced in Sect. 6.2.
</p>
<p>Example 6.4: Investigation of characteristic quantities of samples from a
Gaussian distribution with the Monte Carlo method
</p>
<p>We generate successively 1000 samples of size N = 100 from the standard
normal distribution, e.g., compute the mean x̄, variance s2, and standard
deviation s for each sample as well as the errors Δx̄, Δs2, and Δs, with the
methods of the classSample. We then produce for each of the six quantities
a histogram (Fig. 6.3), containing 1000 entries. Since each of the quantities
is defined as the sum of many random quantities, we expect in all cases that
the histograms should resemble Gaussian distributions. From the histogram
for x̄, we obtain a full width at half maximum of about 0.25, and hence a
standard deviation of approximately 0.1. Indeed the histogram for Δx̄ shows
an approximately Gaussian distribution with mean value Δx̄ = 0.1. (From
the width of this histogram one could determine the error of the error Δx̄
of the mean x̄!) From both histograms one obtains a very clear impression
of the meaning of the error Δx̄ of the mean x̄ for a single sample, as com-
puted according to (6.2.8). It gives (within its error) the standard deviation of
the population, from which the sample mean x̄ comes. If many samples are
successively taken (i.e., if the experiment is repeated many times) then the
frequency distribution of the values x̄ follows a Gaussian distribution about
the population mean with standard deviation Δx̄. The corresponding consid-
erations also hold for the quantities s2, s, and their errors Δs and Δs2.</p>
<p/>
</div>
<div class="page"><p/>
<p>120 6 Samples
</p>
<p>Fig.6.3: Histograms of the quantities x̄, Δx̄, s, Δs, s2, and Δs2 from 1000 samples of size
100 from the standard normal distribution.
</p>
<p>If the elements of the sample depend on two random variables x and
y, then one can construct a scatter plot, where each element is represented
as a point in a Cartesian coordinate system for the variables x and y. Such
a two-dimensional scatter plot provides useful qualitative information about
the relationship between the two variables.
</p>
<p>diagram by a single call. (A plot in the format A5 landscape is generated,
into which the scatter diagram, itself in square format, is fitted. If another plot
format or edge ratio of the diagram is desired, the class has to be adapted
accordingly.)
</p>
<p>The class generates such aGraphicsWith2DScatterDiagram</p>
<p/>
</div>
<div class="page"><p/>
<p>6.3 Graphical Representation of Samples 121
</p>
<p>Example 6.5: Two-dimensional scatter plot: Dividend versus price for
industrial stocks
</p>
<p>Table 6.2 contains a list of the first 10 of 226 data sets, in which the divi-
dend in 1967 (first column) and share price on December 31, 1967 (second
column) are given for a number of industrial stocks. The third column shows
the company name for all German corporations worth more than 10 million
marks. The scatter plot of the number pairs (share price, dividend) is shown
in Fig. 6.4.
</p>
<p>Fig.6.4: Scatter plot of dividend D versus share price P for industrial stocks.
</p>
<p>As expected we see a strong correlation between dividend and share
price. One can see, however, that the dividend does not grow linearly with
the price. It appears that factors other than immediate profit determine the
price of a stock.
</p>
<p>Also shown are histograms for the share price (Fig. 6.5) and dividend
(Fig. 6.6) which can be obtained as projections of the scatter plot onto the
abscissa and ordinate. One clearly observes a non-statistical behavior for the
dividends. It is given as a percent of the nominal value and is therefore almost
always integer. One sees that even numbers are considerably more frequent
than odd numbers.</p>
<p/>
</div>
<div class="page"><p/>
<p>122 6 Samples
</p>
<p>Table6.2: Dividend, share price of a stock, and company name.
</p>
<p>12. 133. ACKERMANN-GOEGGINGEN
08. 417. ADLERWERKE KLEYER
17. 346. AGROB AG FUER GROB U. FEINKERAMIK
25. 765. AG.F.ENERGIEWIRTSCHAFT
16. 355. AG F. LICHT- U. KRAFTVERS.,MCHN.
20. 315. AG.F. IND.U.VERKEHRSW.
08. 138. AG. WESER
16. 295. AEG ALLG.ELEKTR.-GES.
20. 479. ANDREAE-NORIS ZAHN
10. 201. ANKERWERKE
</p>
<p>Fig.6.5: Histogram of the price of industrial stocks.
</p>
<p>6.4 Samples from Partitioned Populations
</p>
<p>It is often advantageous to divide a population G (e.g., all of the students in
Europe) into various subpopulations G1, G2, . . ., Gt (students at university
1,2, . . . , t). Suppose a quantity of interest x follows in the various subpopula-
tions the probability densities f1(x),f2(x), . . . ,ft (x). The distribution func-
tion corresponding to fi(x) is then</p>
<p/>
</div>
<div class="page"><p/>
<p>6.4 Samples from Partitioned Populations 123
</p>
<p>Fig.6.6: Histogram of the dividend of industrial stocks.
</p>
<p>Fi(x)=
&int; x
</p>
<p>&minus;&infin;
fi(x)dx = P (x &lt; x|x &isin;Gi) . (6.4.1)
</p>
<p>This is equal to the conditional probability for x &lt; x given that x is contained
in the subpopulation Gi . The rule of total probability (2.3.4) provides the rela-
tionship between the various Fi(x) and the distribution function F(x) for G,
</p>
<p>F(x)= P (x &lt; x|x &isin;G)=
t&sum;
</p>
<p>i=1
P (x &lt; x|x &isin;Gi)P (x &isin;Gi) ,
</p>
<p>i.e.,
</p>
<p>F(x)=
t&sum;
</p>
<p>i=1
P (x &isin;Gi)Fi(x) . (6.4.2)
</p>
<p>Correspondingly one has for the probability density
</p>
<p>f (x)=
t&sum;
</p>
<p>i=1
P (x &isin;Gi)fi(x) . (6.4.3)</p>
<p/>
</div>
<div class="page"><p/>
<p>124 6 Samples
</p>
<p>If we now abbreviate P (x &isin;Gi) by pi , then one has
</p>
<p>x̂ = E(x)=
&int; &infin;
</p>
<p>&minus;&infin;
xf (x)dx =
</p>
<p>t&sum;
</p>
<p>i=1
pi
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
xfi(x)dx ,
</p>
<p>x̂ =
t&sum;
</p>
<p>i=1
pi x̂i . (6.4.4)
</p>
<p>The population mean is thus the mean of the individual means of the
subpopulations, each weighted by probability of its subpopulation. For the
population variance one obtains
</p>
<p>σ 2(x) =
&int; &infin;
</p>
<p>&minus;&infin;
(x&minus; x̂)2f (x)dx
</p>
<p>=
&int; &infin;
</p>
<p>&minus;&infin;
(x&minus; x̂)2
</p>
<p>t&sum;
</p>
<p>i=1
pifi(x)dx
</p>
<p>=
t&sum;
</p>
<p>i=1
pi
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
{(x&minus; x̂i)+ (x̂i &minus; x̂)}2fi(x)dx .
</p>
<p>All cross terms vanish since the xi are independent, leading to
</p>
<p>σ 2(x)=
t&sum;
</p>
<p>i=1
pi
</p>
<p>{&int; &infin;
</p>
<p>&minus;&infin;
(x&minus; x̂i)2fi(x)dx+ (x̂i &minus; x̂)2
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
fi(x)dx
</p>
<p>}
</p>
<p>or
</p>
<p>σ 2(x)=
t&sum;
</p>
<p>i=1
pi{σ 2i + (x̂i &minus; x̂)2} . (6.4.5)
</p>
<p>One thus obtains the weighted mean of a sum of two terms. The first gives the
dispersion of a subpopulation, the second gives the quadratic deviation of the
mean of this subpopulation from the mean of the whole population.
</p>
<p>Having discussed separating a population into parts, we now take from
each subpopulation Gi a sample of size ni (with
</p>
<p>&sum;t
i=1ni = n) and examine
</p>
<p>the arithmetic mean of the total partitioned sample
</p>
<p>x̄p =
1
</p>
<p>n
</p>
<p>t&sum;
</p>
<p>i=1
</p>
<p>ni&sum;
</p>
<p>j=1
xij =
</p>
<p>1
</p>
<p>n
</p>
<p>t&sum;
</p>
<p>i=1
ni x̄i (6.4.6)</p>
<p/>
</div>
<div class="page"><p/>
<p>6.4 Samples from Partitioned Populations 125
</p>
<p>with the expectation value and variance
</p>
<p>E(x̄p) =
1
</p>
<p>n
</p>
<p>n&sum;
</p>
<p>i=1
ni x̂i , (6.4.7)
</p>
<p>σ 2(x̄p) = E{(x̄p&minus;E(x̄p))2}
</p>
<p>= E
</p>
<p>⎧
⎨
⎩
</p>
<p>(
t&sum;
</p>
<p>i=1
</p>
<p>ni
</p>
<p>n
(x̄i &minus; x̂i)
</p>
<p>)2⎫⎬
⎭
</p>
<p>= 1
n2
</p>
<p>t&sum;
</p>
<p>i=1
n2iE{(x̄i &minus; x̂i)2} ,
</p>
<p>σ 2(x̄p) =
1
</p>
<p>n2
</p>
<p>t&sum;
</p>
<p>i=1
n2i σ
</p>
<p>2(x̄i) . (6.4.8)
</p>
<p>Using (6.2.3) this is finally
</p>
<p>σ 2(x̄p)=
1
</p>
<p>n
</p>
<p>t&sum;
</p>
<p>i=1
</p>
<p>ni
</p>
<p>n
σ 2i . (6.4.9)
</p>
<p>One would obtain the same result by application of the law of error propaga-
tion (3.8.7) to Eq. (6.4.6).
</p>
<p>It is clear that the arithmetic mean x̄p cannot in general be an estimator
for the sample mean x̂, since it depends on the arbitrary choice of the size ni
of the samples from the subpopulations. A comparison of (6.4.7) with (6.4.4)
shows that this is only true for the special case pi = ni/n.
</p>
<p>The population mean x̂ can be estimated in the following way. One
first determines the means x̄i for the subpopulations, and constructs then the
expression
</p>
<p>x̃ =
t&sum;
</p>
<p>i=1
pi x̄i , (6.4.10)
</p>
<p>in analogy to Eq. (6.4.4). By error propagation one obtains for the variance
of x̃
</p>
<p>σ 2(x̃)=
t&sum;
</p>
<p>i=1
p2i σ
</p>
<p>2(x̄i)=
t&sum;
</p>
<p>i=1
</p>
<p>p2i
</p>
<p>ni
σ 2i . (6.4.11)
</p>
<p>Example 6.6: Optimal choice of the sample size for subpopulations
</p>
<p>In order to minimize the variance σ 2(x̃), we cannot simply differentiate the
relation (6.4.11) with respect to all ni , since the ni must satisfy a constraint,
namely</p>
<p/>
</div>
<div class="page"><p/>
<p>126 6 Samples
</p>
<p>t&sum;
</p>
<p>i=1
ni &minus;n= 0 . (6.4.12)
</p>
<p>We must therefore use the method of Lagrange multipliers, by multiplying
Eq. (6.4.12) with a factor μ, adding this to Eq. (6.4.11), and finally setting the
partial derivatives of the ni with respect to μ equal to zero:
</p>
<p>L= σ 2(x̃)+μ(
&sum;
</p>
<p>ni &minus;n)=
&sum;
</p>
<p>(p2i /ni)σ
2
i +μ(
</p>
<p>&sum;
ni &minus;n) ,
</p>
<p>&part;L
</p>
<p>&part;ni
=&minus;
</p>
<p>p2i σ
2
i
</p>
<p>n2i
+μ= 0 , (6.4.13)
</p>
<p>&part;L
</p>
<p>&part;μ
=
</p>
<p>&sum;
ni &minus;n= 0 . (6.4.14)
</p>
<p>From (6.4.13) we obtain
</p>
<p>ni = piσi/
&radic;
μ .
</p>
<p>Together with (6.4.14) this gives
</p>
<p>1/
&radic;
μ= n/
</p>
<p>&sum;
piσi
</p>
<p>and therefore
</p>
<p>ni = npiσi/
&sum;
</p>
<p>piσi . (6.4.15)
</p>
<p>The result (6.4.15) states that the sizes ni of the samples from the subpop-
ulations i should be chosen in such a way that they are proportional to the
probability pi of subpopulation i, weighted with the corresponding standard
deviation.
</p>
<p>As an example assume that a scientific publishing company wants to
estimate the total amount spent for scientific books by two subpopulations:
(1) students and (2) scientific libraries. Further, we will assume that there are
1000 libraries and 106 students in the population and that the standard devia-
tion of the money spent by students is $ 100, and for libraries (which are of
greatly differing sizes) $ 3 &middot;105. We then have
</p>
<p>p1 &asymp; 1 , p2 &asymp; 10&minus;3 , σ1 = 100 , σ2 = 3&times;105
</p>
<p>and from (6.4.15)
</p>
<p>n1 = const &middot;100 , n2 = const &middot;300 , n2 = 3n1 .</p>
<p/>
</div>
<div class="page"><p/>
<p>6.5 Mean Square Deviation 127
</p>
<p>Note that the result does not depend on the means of the partial populations.
The quantities pi , xi , and σi are in general unknown. They must first be esti-
mated from preliminary samples.
</p>
<p>The discussion of subpopulations will be taken up again in Chap. 11.
</p>
<p>6.5 Samples Without Replacement from Finite Discrete
</p>
<p>Populations. Mean Square Deviation. Degrees
</p>
<p>of Freedom
</p>
<p>We first encountered the concept a sample in connection with the hyperge-
ometric distribution (Sect. 5.3). There we determined that the independence
of the individual sample elements was lost by the process of taking elements
without replacing them from a finite (and hence discrete) population. We are
therefore no longer dealing with genuine random sampling, even if no partic-
ular choice among the remaining elements is made.
</p>
<p>To discuss this further let us introduce the following notation. Suppose
the population consists of N elements y1,y2, . . . ,yN . From it we take a sample
of size n with the elements x1,x2, . . . ,xn. (In the hypergeometric distribution,
the yj and hence the xi could only take on the values 0 and 1.)
</p>
<p>Since it is equally probable for each of the remaining elements yj to be
chosen, we obtain for the expectation value
</p>
<p>E(y)= ŷ = ȳ = 1
N
</p>
<p>N&sum;
</p>
<p>j=1
yj . (6.5.1)
</p>
<p>Although ŷ is not a random variable, this expression is the arithmetic mean of
a finite number of elements of the population. A definition of the population
variance encounters the difficulties discussed at the end of Sect. 6.2. We define
it in analogy to (6.2.6) as
</p>
<p>σ 2(y) = 1
N&minus;1
</p>
<p>N&sum;
</p>
<p>j=1
(yj &minus; ȳ)2
</p>
<p>= 1
N&minus;1
</p>
<p>⎧
⎪⎨
⎪⎩
</p>
<p>N&sum;
</p>
<p>j=1
y2j &minus;
</p>
<p>1
</p>
<p>N
</p>
<p>⎛
⎝
</p>
<p>N&sum;
</p>
<p>j=1
yj
</p>
<p>⎞
⎠
</p>
<p>2
⎫
⎪⎬
⎪⎭
</p>
<p>. (6.5.2)
</p>
<p>Let us now consider the sum of squares,
</p>
<p>N&sum;
</p>
<p>j=1
(yj &minus; ȳ)2 . (6.5.3)</p>
<p/>
</div>
<div class="page"><p/>
<p>128 6 Samples
</p>
<p>Since we have not constrained the population in any way, the yj can take on all
possible values. Therefore the first element in the sum in (6.5.2) can also take
on any of the possible values. The same holds for the 2nd, 3rd, . . . , (N &minus;1)th
terms summed. The N th term in the sum is then, however, fixed, since
</p>
<p>N&sum;
</p>
<p>j=1
(yj &minus; ȳ)= 0 . (6.5.4)
</p>
<p>We say that the number of degrees of freedom of the sum of squares (6.5.3)
is N &minus; 1. One can illustrate this connection geometrically. We consider the
case ȳ = 0 and construct an N -dimensional vector space with the yj . The
quadratic sum (6.5.3) is then the square of the absolute value of the position
vector in this space. Because of the equation of constraint (6.5.4) the tip of the
position vector can only move in a space of dimension (N&minus;1). In mechanics
the dimension of such a constrained space is called the number of degrees of
freedom. This is sketched in Fig. 6.7 for the case N = 2. Here the position
vector is constrained to lie on the line y2 =&minus;y1.
</p>
<p>y2
</p>
<p>y1 = &minus;y2
</p>
<p>1
y
</p>
<p>2
y+
</p>
<p>2 2
</p>
<p>y1
</p>
<p>Fig.6.7: A sample of size two
gives a sum of squares with one
degree of freedom.
</p>
<p>A sum of squares divided by the number of degrees of freedom, i.e., an
expression of the form (6.5.2) is called a mean square or to be more complete,
since we are dealing with the differences of the individual values from the
expectation or mean value, mean square deviation. The square root of this
expression, which is then a measure of the dispersion, is called the root mean
square (RMS) deviation.
</p>
<p>We now return to the sample x1,x2, . . . ,xn. For simplicity of notation we
will introduce the Kronecker symbol, which describes the selection procedure
for the sample. It is defined as
</p>
<p>δ
j
i =
</p>
<p>{
1, if xi is the element yj ,
0 otherwise.
</p>
<p>(6.5.5)</p>
<p/>
</div>
<div class="page"><p/>
<p>6.5 Mean Square Deviation 129
</p>
<p>In particular one has
</p>
<p>xi =
N&sum;
</p>
<p>j=1
δ
j
</p>
<p>i yj . (6.5.6)
</p>
<p>Since the selection of any of the yj as the ith element is equally probable,
one has
</p>
<p>P (δ
j
</p>
<p>i = 1)= 1/N . (6.5.7)
</p>
<p>Since δji describes a random procedure, it is clearly a random variable itself.
Its expectation value is found from Eq. (3.3.2) (where n = 2, x1 = 0, x2 = 1)
to be
</p>
<p>E(δ
j
i )= P (δ
</p>
<p>j
i = 1)= 1/N . (6.5.8)
</p>
<p>If now one element xi of the sample is determined, one then has only (N&minus;1)
selection possibilities out of the population for a further element, e.g., xk.
That is,
</p>
<p>P (δ
j
i δ
</p>
<p>ℓ
k = 1)=
</p>
<p>1
</p>
<p>N
</p>
<p>1
</p>
<p>N&minus;1 = E(δ
j
i δ
</p>
<p>ℓ
k) . (6.5.9)
</p>
<p>Since the sample is taken without replacement, one has j �= ℓ, i.e.,
</p>
<p>δ
j
</p>
<p>i δ
j
</p>
<p>k = 0 . (6.5.10)
</p>
<p>Similarly one has
δ
j
i δ
</p>
<p>ℓ
i = 0 , (6.5.11)
</p>
<p>since two different elements of the population cannot simultaneously occur as
the ith element of the sample.
</p>
<p>We consider now the expectation value of x1,
</p>
<p>E(x1)=E
</p>
<p>⎧
⎨
⎩
</p>
<p>N&sum;
</p>
<p>j=1
δ
j
</p>
<p>1yj
</p>
<p>⎫
⎬
⎭=
</p>
<p>N&sum;
</p>
<p>j=1
yjE(δ
</p>
<p>j
</p>
<p>1)=
1
</p>
<p>N
</p>
<p>N&sum;
</p>
<p>j=1
yj = ȳ . (6.5.12)
</p>
<p>Since x1 is in not in any way special, the expectation values of all elements of
the sample, and thus also of their arithmetic mean, have the same value
</p>
<p>E(x̄)= 1
n
</p>
<p>n&sum;
</p>
<p>i=1
E(xi)= ȳ . (6.5.13)
</p>
<p>The arithmetic mean of the sample is thus an unbiased estimator for the pop-
ulation mean.
</p>
<p>Next we consider the sample variance
</p>
<p>s2x =
1
</p>
<p>n&minus;1
</p>
<p>n&sum;
</p>
<p>i=1
(xi &minus; x̄)2 . (6.5.14)</p>
<p/>
</div>
<div class="page"><p/>
<p>130 6 Samples
</p>
<p>By means of a somewhat longer calculation it can be shown that the expecta-
tion value is
</p>
<p>E(s2x)= σ 2(y) . (6.5.15)
The sample variance is thus an unbiased estimator for the population variance.
</p>
<p>The variance of the mean is also of interest:
</p>
<p>σ 2(x̄)= E{(x̄&minus;E(x̄))2} .
</p>
<p>E(x̄) = ȳ is, however, a fixed, not a random quantity, whereas x̄ depends on
the individual sample, and is hence a random variable. One therefore has
</p>
<p>σ 2(x̄) = E(x̄2)&minus; ȳ2 = 1
n
</p>
<p>{(
1&minus; n
</p>
<p>N
</p>
<p>)
σ 2(y)+nȳ2
</p>
<p>}
&minus; ȳ2 ,
</p>
<p>σ 2(x̄) = σ
2(y)
</p>
<p>n
</p>
<p>(
1&minus; n
</p>
<p>N
</p>
<p>)
. (6.5.16)
</p>
<p>Comparing with the case of an infinite continuous sample (6.2.3) one sees the
additional factor (1&minus;n/N). This corresponds to the fact that the variance of x̄
vanishes in the case n=N , where the &ldquo;sample&rdquo; contains the entire population
and where one has exactly x̄ = ȳ.
</p>
<p>6.6 Samples from Gaussian Distributions: χ2-Distribution
</p>
<p>We return now to continuously distributed populations and consider in par-
ticular a Gaussian distribution with mean a and variance σ 2. According to
(5.7.7), the characteristic function of such a Gaussian distribution is
</p>
<p>ϕx(t)= exp(ita)exp(&minus;
1
</p>
<p>2
σ 2t2) . (6.6.1)
</p>
<p>We now take a sample of size n from the population. The characteristic func-
tion of the sample mean was given in (6.2.2) in terms of the characteristic
function of the population. From this we have
</p>
<p>ϕx̄(t)=
{
</p>
<p>exp
</p>
<p>(
i
t
</p>
<p>n
a&minus; σ
</p>
<p>2
</p>
<p>2
</p>
<p>(
t
</p>
<p>n
</p>
<p>)2)}n
. (6.6.2)
</p>
<p>If we consider (x̄&minus;a)= (x̄&minus; x̂) in place of x, then one obtains
</p>
<p>ϕx̄&minus;a(t)= exp
(
&minus;σ
</p>
<p>2t2
</p>
<p>2n
</p>
<p>)
. (6.6.3)</p>
<p/>
</div>
<div class="page"><p/>
<p>6.6 Samples from Gaussian Distributions: χ2-Distribution 131
</p>
<p>This is again the characteristic function of a normal distribution, but with a
different variance,
</p>
<p>σ 2(x̄)= σ 2(x)/n . (6.6.4)
For the simple case of a standard Gaussian distribution (a = 0, σ 2 = 1) we
have
</p>
<p>ϕx̄(t)= exp(&minus;t2/2n) . (6.6.5)
We take a sample from this distribution
</p>
<p>x1,x2, . . . ,xn ,
</p>
<p>but we are interested in particular in the sum of the squares of the sample
elements,
</p>
<p>x2 = x21 +x22+ . . .+x2n . (6.6.6)
We want to show that the quantity x2 follows the distribution function&lowast;
</p>
<p>F(χ2)= 1
Γ (λ)2λ
</p>
<p>&int; χ2
</p>
<p>0
uλ&minus;1e&minus;
</p>
<p>1
2u du , (6.6.7)
</p>
<p>where
</p>
<p>λ= 1
2
n . (6.6.8)
</p>
<p>The quantity n is called the number of degrees of freedom.
We first introduce the abbreviation
</p>
<p>1
</p>
<p>Γ (λ)2λ
= k (6.6.9)
</p>
<p>and determine the probability density to be
</p>
<p>f (χ2)= k(χ2)λ&minus;1e&minus; 12χ2 . (6.6.10)
</p>
<p>For two degrees of freedom, the probability density is clearly an exponential
function. First we want to prove what was claimed by (6.6.7) for one degree
of freedom (λ= 1/2). Thus we ask for the probability that x2 &lt; χ2, or rather,
that &minus;
</p>
<p>&radic;
χ2 &lt; x &lt;+
</p>
<p>&radic;
χ2. This is
</p>
<p>F(χ2) = P (x2 &lt; χ2)= P (&minus;
&radic;
χ2 &lt; x &lt;+
</p>
<p>&radic;
χ2)
</p>
<p>= 1&radic;
2π
</p>
<p>&int; &radic;χ2
</p>
<p>&minus;
&radic;
</p>
<p>χ2
e&minus;
</p>
<p>1
2x
</p>
<p>2
dx = 2&radic;
</p>
<p>2π
</p>
<p>&int; &radic;χ2
</p>
<p>0
e&minus;
</p>
<p>1
2x
</p>
<p>2
dx .
</p>
<p>&lowast;The symbol χ2 (chi squared) was introduced by K. Pearson. Although it is written as
something squared, which reminds one of its origin as a sum of squares, it is treated as a usual
random variable.</p>
<p/>
</div>
<div class="page"><p/>
<p>132 6 Samples
</p>
<p>Setting x2 = u, du= 2x dx, we obtain directly
</p>
<p>F(χ2)= 1&radic;
2π
</p>
<p>&int; χ2
</p>
<p>0
u&minus;
</p>
<p>1
2 e&minus;
</p>
<p>1
2u du . (6.6.11)
</p>
<p>To prove the general case we first find the characteristic function of the
χ2-distribution to be
</p>
<p>ϕχ2(t)=
&int; &infin;
</p>
<p>0
k(χ2)λ&minus;1 exp(&minus;1
</p>
<p>2
χ2 + itχ2)dχ2 (6.6.12)
</p>
<p>or with (1/2&minus; it)χ2 = ν,
</p>
<p>ϕχ2(t)= 2λ(1&minus;2it)&minus;λk
&int; &infin;
</p>
<p>0
νλ&minus;1e&minus;ν dν .
</p>
<p>The integral on the right side is, according to (D.1.1), equal to Γ (λ). One
therefore has
</p>
<p>ϕχ2(t)= (1&minus;2it)&minus;λ . (6.6.13)
</p>
<p>If we now consider the case of a second distribution with λ&prime;, then one has
</p>
<p>ϕ&prime;
χ2
(t)= (1&minus;2it)&minus;λ&prime; .
</p>
<p>Since the characteristic function of a sum is equal to the product of the char-
acteristic functions, one has the following important theorem:
</p>
<p>The sum of two independent χ2 variables with
n1,n2 degrees of freedom follows itself a χ2-distribution
with n= n1 +n2 degrees of freedom.
</p>
<p>This theorem can now be used to easily generalize the claim (6.6.7), proven up
to now only for n= 1. The proof follows from the fact that the individual terms
of the sum of squares are independent and therefore (6.6.6) can be treated as
the sum of n different χ2 variables, each with one degree of freedom.
</p>
<p>In order to obtain the expectation value and variance of the χ2-distribution,
we use the characteristic function, whose derivatives (5.5.7) give the central
moments. We obtain
</p>
<p>E(x2) = &minus;iϕ&prime;(0)= 2λ , (6.6.14)
E(x2) = n
</p>
<p>and
</p>
<p>E{(x2)2} = &minus;ϕ&prime;&prime;(0)= 4λ2 +4λ ,
σ 2(x2) = E{(x2)2}&minus; {E(x2)}2 = 4λ ,
σ 2(x2) = 2n . (6.6.15)</p>
<p/>
</div>
<div class="page"><p/>
<p>6.6 Samples from Gaussian Distributions: χ2-Distribution 133
</p>
<p>The expectation value of the χ2-distribution is thus equal to the number of
degrees of freedom, and the variance is two times larger. Figure 6.8 shows the
probability density of the χ2 distribution for various values of n. One sees [as
can be directly seen also from (6.6.10)] that for χ2 = 0, the function diverges
for n = 1, is equal to 1/2 for n = 2, and vanishes for n &ge; 3. A table of the
χ2-distribution is provided in the appendix (Table I.6).
</p>
<p>The χ2-distribution is of great significance in many applications, where
the quantity χ2 is used as a measure of confidence in a certain result. The
smaller the value of χ2, the greater is the confidence in the result. (After all,
χ2 was defined as the sum of squares of deviations of elements of a sample
from the population mean. See Sect. 8.7.) The distribution function
</p>
<p>F(χ2)= P (x2 &lt; χ2) (6.6.16)
</p>
<p>gives the probability that the random variable x2 is not larger than χ2. In
practice, one frequently uses the quantity
</p>
<p>W(χ2)= 1&minus;F(χ2) (6.6.17)
</p>
<p>as a measure of confidence in a result. W(χ2) is often called the confidence
level. W(χ2) is large for small values of χ2 and falls with increasing χ2. The
</p>
<p>Fig.6.8: Probability density of χ2 for the number of degrees of freedom n= 1,2, . . . ,10. The
expectation value E(χ2)= n moves to the right as n increases.</p>
<p/>
</div>
<div class="page"><p/>
<p>134 6 Samples
</p>
<p>distribution function (6.6.16) is shown in Fig. 6.9 for various numbers of deg-
rees of freedom n. The inverse function, which gives the quantiles of the χ2
</p>
<p>distribution,
χ2F = χ2(F )= χ2 (6.6.18)
</p>
<p>is used especially often in &ldquo;hypothesis testing&rdquo; (see Sect. 8.7). It is tabulated
in the appendix (Table I.7).
</p>
<p>Up to now we have restricted ourselves to the case where the popula-
tion is described by the standard normal distribution. Usually, however, one
has a normal distribution in general form with mean a and variance σ 2. Then
the sum of squares (6.6.6) is clearly no longer distributed according to the
χ2-distribution. One immediately obtains, however, a χ2-distribution by con-
sidering the quantity
</p>
<p>x2 = (x1 &minus;a)
2 + (x2 &minus;a)2 +&middot;&middot; &middot;+ (xn&minus;a)2
</p>
<p>σ 2
. (6.6.19)
</p>
<p>This result follows directly from Eq. (5.8.4).
If the expectation values ai and variances σi of the individual variables
</p>
<p>are different, then one has
</p>
<p>Fig.6.9: Distribution function for χ2 for the number of degrees of freedom n= 1,2, . . . ,20.
The function for n = 1 is the curve at the far left, and the function for n = 20 is at the far
right.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.7 χ2 and Empirical Variance 135
</p>
<p>x2 = (x1 &minus;a1)
2
</p>
<p>σ 21
+ (x2 &minus;a2)
</p>
<p>2
</p>
<p>σ 22
+&middot;&middot; &middot;+ (xn&minus;an)
</p>
<p>2
</p>
<p>σ 2n
. (6.6.20)
</p>
<p>Finally if the n variables are not independent, but are described by a joint
normal distribution (5.10.1) with the expectation values given by the vector a
and the covariance matrix C = B&minus;1, then one has
</p>
<p>x2 = (x&minus;a)TB(x&minus;a) . (6.6.21)
</p>
<p>6.7 χ2 and Empirical Variance
</p>
<p>In Eq. (6.2.6) we found that
</p>
<p>s2 = 1
n&minus;1
</p>
<p>n&sum;
</p>
<p>i=1
(xi &minus; x̄)2 (6.7.1)
</p>
<p>is a consistent, unbiased estimator for the variance σ 2 of a population. Let
the xi be independent and normally distributed with standard deviation σ . We
want to show that the quantity
</p>
<p>n&minus;1
σ 2
</p>
<p>s2 (6.7.2)
</p>
<p>follows the χ2-distribution with f = n&minus;1 degrees of freedom. We first carry
out an orthogonal transformation of the n variables xi (see Sect. 3.8):
</p>
<p>y1 =
1&radic;
1 &middot;2
</p>
<p>(x1 &minus;x2) ,
</p>
<p>y2 =
1&radic;
2 &middot;3
</p>
<p>(x1 +x2&minus;2x3) ,
</p>
<p>y3 =
1&radic;
3 &middot;4
</p>
<p>(x1 +x2+x3 &minus;3x4) ,
</p>
<p>... (6.7.3)
</p>
<p>yn&minus;1 =
1&radic;
</p>
<p>(n&minus;1)n
(x1 +x2+&middot;&middot; &middot;+xn&minus;1 &minus; (n&minus;1)xn) ,
</p>
<p>yn =
1&radic;
n
(x1 +x2 +&middot;&middot; &middot;+xn)=
</p>
<p>&radic;
nx̄ .
</p>
<p>One can verify that this transformation is in fact orthogonal, i.e., that
</p>
<p>n&sum;
</p>
<p>i=1
x2i =
</p>
<p>n&sum;
</p>
<p>i=1
y2i . (6.7.4)</p>
<p/>
</div>
<div class="page"><p/>
<p>136 6 Samples
</p>
<p>Since a sum or difference of independent normally distributed quantities is
again itself normally distributed, all of the yi are normally distributed. The
factors in (6.7.3) ensure that the yi have mean values of zero and standard
deviations σ .
</p>
<p>From (6.7.1) and (6.7.2) one then has
</p>
<p>(n&minus;1)s2 =
n&sum;
</p>
<p>i=1
(xi &minus; x̄)2 =
</p>
<p>n&sum;
</p>
<p>i=1
x2i &minus;2x̄
</p>
<p>n&sum;
</p>
<p>i=1
xi +nx̄2
</p>
<p>=
n&sum;
</p>
<p>i=1
x2i &minus;nx̄2 =
</p>
<p>n&sum;
</p>
<p>i=1
y2i &minus;y2n =
</p>
<p>n&minus;1&sum;
</p>
<p>i=1
y2i .
</p>
<p>This expression is a sum of only (n&minus; 1) independent squared terms. A
comparison with (6.6.19) shows that the quantity (6.7.2) in fact follows a
χ2-distribution with (n&minus;1) degrees of freedom.
</p>
<p>The squared terms (xi &minus; x̄)2 are not linearly independent. One has the
following relation between them:
</p>
<p>n&sum;
</p>
<p>i=1
(xi &minus; x̄)= 0 .
</p>
<p>One can show that every additional relation between the squared terms red-
uces the number of degrees of freedom by one. Later we will make frequent
use of this result, which we only state here without proof.
</p>
<p>6.8 Sampling by Counting: Small Samples
</p>
<p>Samples are often obtained in the following way. One draws n elements from
a population, checks if they possess a given characteristic and accepts only
those k elements into the sample that have the characteristic. The remaining
n&minus; k elements are rejected, i.e., their properties are not recorded. This app-
roach thus becomes the counting of k out of n elements drawn.
</p>
<p>This approach corresponds exactly to selecting a sample according to a
binomial distribution. The parameters p and q of this distribution correspond
then to the occurrence or non-occurrence of the property in question. As will
be shown in Example 7.5,
</p>
<p>S(p)= k
n
</p>
<p>(6.8.1)
</p>
<p>is the maximum likelihood estimator of the parameter p. The variance of S is
</p>
<p>σ 2(S(p))= p(1&minus;p)
n
</p>
<p>. (6.8.2)</p>
<p/>
</div>
<div class="page"><p/>
<p>6.8 Sampling by Counting: Small Samples 137
</p>
<p>By using (6.8.1) it can be estimated from the sample by
</p>
<p>s2(S(p))= 1
n
</p>
<p>k
</p>
<p>n
</p>
<p>(
1&minus; k
</p>
<p>n
</p>
<p>)
. (6.8.3)
</p>
<p>We define the error Δk as
</p>
<p>Δk =
&radic;
[s2(S(np))] . (6.8.4)
</p>
<p>By using (6.8.3) we obtain
</p>
<p>Δk =
&radic;[
</p>
<p>k
</p>
<p>(
1&minus; k
</p>
<p>n
</p>
<p>)]
. (6.8.5)
</p>
<p>The error Δk only depends on the number of elements counted and on the
size of the sample. It is called the statistical error. A particularly important
case is that of small k, or more precisely, the case k ≪ n. In this limit we
can define λ= np and following Sect. 5.4 consider the counted number k as a
single element of a sample taken from a Poisson distributed population with
parameter λ. From (6.8.1) and (6.8.5) we obtain
</p>
<p>S(λ)= S(np) = k , (6.8.6)
</p>
<p>Δλ=
&radic;
</p>
<p>k . (6.8.7)
</p>
<p>(This can be derived by using the result of Example 7.4 with N = 1.) The
result (6.8.7) is often written in an actually incorrect but easy to remember
form,
</p>
<p>Δk =
&radic;
</p>
<p>k ,
</p>
<p>which is read: The statistical error of the counted number k is
&radic;
</p>
<p>k.
In order to interpret the statistical error Δλ =
</p>
<p>&radic;
k we must examine the
</p>
<p>Poisson distribution somewhat more closely. Let us begin with the case where
k is not too small (say, k &gt; 20). For large values of λ the Poisson distribution
becomes a Gaussian distribution with mean λ and variance σ 2 = λ. This can
be seen qualitatively from Fig. 5.6. As long as k is not too small, i.e., k ≫ 1,
we can then treat the Poisson distribution in k with parameter λ as a normal
distribution in x with mean λ and variance σ 2 = λ. The discrete variable k is
then replaced by the continuous variable x. The probability density of x is
</p>
<p>f (x;λ)= 1
σ
&radic;
</p>
<p>2π
exp
</p>
<p>{
&minus;(x&minus;λ)
</p>
<p>2
</p>
<p>2σ 2
</p>
<p>}
= 1&radic;
</p>
<p>2πλ
exp
</p>
<p>{
&minus;(x&minus;λ)
</p>
<p>2
</p>
<p>2λ
</p>
<p>}
. (6.8.8)
</p>
<p>The observation of k events corresponds to observing once the value of the
random variable x = k.</p>
<p/>
</div>
<div class="page"><p/>
<p>138 6 Samples
</p>
<p>With the help of the probability density (6.8.8) we now want to determine
the confidence limits at a given confidence level β = 1&minus;α in such a way that
</p>
<p>P (λ&minus; &le; λ&le; λ+)= 1&minus;α . (6.8.9)
</p>
<p>That is, one requires that the probability that the true value of λ is contained
within the confidence limits λ&minus; and λ+ be equal to the confidence level 1&minus;α.
The limiting cases λ = λ&minus; and λ = λ+ are depicted in Fig. 6.10. They are
determined such that
</p>
<p>P (x &gt; k|λ= λ+)= 1&minus;α/2 , P (x &lt; k|λ= λ&minus;)= 1&minus;α/2 . (6.8.10)
</p>
<p>One clearly has
</p>
<p>α/2 =
&int; x=k
</p>
<p>x=&minus;&infin;
f (x;λ+)dx =
</p>
<p>1
</p>
<p>σ
&radic;
</p>
<p>2π
</p>
<p>&int; x=k
</p>
<p>&minus;&infin;
exp
</p>
<p>{
&minus;(x&minus;λ+)
</p>
<p>2
</p>
<p>2σ 2
</p>
<p>}
</p>
<p>=
&int; u=(k&minus;λ+)/σ
</p>
<p>u=&minus;&infin;
φ0(u)du= ψ0
</p>
<p>(
k&minus;λ+
</p>
<p>σ
</p>
<p>)
, (6.8.11)
</p>
<p>and correspondingly
</p>
<p>1&minus;α/2 =
&int; x=k
</p>
<p>&minus;&infin;
f (x;λ&minus;)dx = ψ0
</p>
<p>(
k&minus;λ&minus;
</p>
<p>σ
</p>
<p>)
. (6.8.12)
</p>
<p>Here φ0 and ψ0 are the probability density and distribution function of the
standard normal distribution introduced in Sect. 5.8. By using the inverse
function Ω of the distribution function ψ0 [see Eq. (5.8.8)], one obtains
</p>
<p>k&minus;λ&minus;
σ
</p>
<p>=Ω(1&minus;α/2) , k&minus;λ+
σ
</p>
<p>=Ω(α/2) . (6.8.13)
</p>
<p>Because of (5.8.10) one has Ω(1 &minus; α/2) = Ω &prime;(1 &minus; α) and because of the
symmetry of the function Ω , Ω(1&minus;α/2)=&minus;Ω(α/2). Further, since α &lt; 1,
one has Ω(1&minus;α/2) &gt; 0, Ω(α/2) &lt; 0. From this we finally obtain
</p>
<p>λ&minus; = k&minus;σΩ &prime;(1&minus;α) , λ+ = k+σΩ &prime;(1&minus;α) . (6.8.14)
</p>
<p>According to (6.8.6), k is the best estimator for λ. Since σ 2 = λ, the
best estimator for σ is given by s =
</p>
<p>&radic;
k. Since we have assumed that k ≫ 1,
</p>
<p>the uncertainty in s is significantly smaller than the uncertainty in k. We can
therefore substitute x = k and s =
</p>
<p>&radic;
k in (6.8.9) and obtain for the confidence
</p>
<p>interval with confidence level 1&minus;α
</p>
<p>λ&minus; = k&minus;
&radic;
</p>
<p>kΩ &prime;(1&minus;α)&le; λ&le; k+
&radic;
</p>
<p>kΩ &prime;(1&minus;α)= λ+ . (6.8.15)</p>
<p/>
</div>
<div class="page"><p/>
<p>6.8 Sampling by Counting: Small Samples 139
</p>
<p>Fig.6.10:Normal distribution with mean λ and standard deviation σ for λ= λ&minus; and λ= λ+.
</p>
<p>For 1&minus;α = 68.3% we find from Sect. 5.8 or Table I.5 that Ω &prime;(α) = 1. What
is usually reported,
</p>
<p>λ= k&plusmn;
&radic;
</p>
<p>k ,
</p>
<p>which was already the result of (6.8.6) and (6.8.7), thus gives the confidence
limits at the confidence level of 68.3%, but only for the case k ≫ 1. For the
confidence level of 90%, i.e., for α = 0.1, we find Ω &prime;(0.1) = 1.65 and for
the confidence level 99% one has Ω &prime;(0.01)= 2.57.
</p>
<p>For very small values of k, one can no longer replace the Poisson distribu-
tion with the normal distribution. We follow therefore reference [25]. We start
again from Eq. (6.8.10), but use, instead of the probability density (6.8.8) for</p>
<p/>
</div>
<div class="page"><p/>
<p>140 6 Samples
</p>
<p>the continuous random variable x with fixed parameter λ, the Poisson proba-
bility for observing the discrete random variable n for a given λ,
</p>
<p>f (n;λ)= λ
n
</p>
<p>n! e
&minus;λ . (6.8.16)
</p>
<p>For the observation k we now determine the confidence limits λ&minus; and λ+,
which fulfill (6.8.10) with x = n (Fig. 6.11) and obtain in analogy to (6.8.11)
and (6.8.12)
</p>
<p>1&minus;α/2 =
&infin;&sum;
</p>
<p>n=k+1
f (n;λ+)= 1&minus;
</p>
<p>k&sum;
</p>
<p>n=0
f (n;λ+)= 1&minus;F(k+1;λ+) ,
</p>
<p>Fig.6.11: Poisson distribution with parameter λ for λ= λ&minus; and λ= λ+.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.8 Sampling by Counting: Small Samples 141
</p>
<p>1&minus;α/2 =
k&minus;1&sum;
</p>
<p>n=0
f (n;λ&minus;)= F(k;λ&minus;)
</p>
<p>or
</p>
<p>α/2 = F(k+1;λ+) , (6.8.17)
1&minus;α/2 = F(k;λ&minus;) . (6.8.18)
</p>
<p>Here,
</p>
<p>F(k;λ)=
k&minus;1&sum;
</p>
<p>n=0
f (n;λ)= P (k &lt; k)
</p>
<p>is the distribution function of the Poisson distribution.
In order to obtain numerical values for the confidence limits λ+ and λ&minus;,
</p>
<p>we solve Eqs. (6.8.17) and (6.8.18). That is, we must construct the inverse
function of the Poisson distribution for fixed k and given probability P (in
our case α/2 and 1&minus;α/2),
</p>
<p>λ= λP (k) . (6.8.19)
</p>
<p>6.8.19) is given in Table I.1 for frequently occurring
values of P .
</p>
<p>For extremely small samples one is often only interested in an upper con-
fidence limit at confidence level β = 1&minus;α. This is obtained by requiring
</p>
<p>P (n &gt; k|λ= λ(up))= β = 1&minus;α (6.8.20)
</p>
<p>instead of (6.8.10). Thus one has
</p>
<p>α =
k&sum;
</p>
<p>n=0
f (n;λ(up))= F(k+1;λ(up)) . (6.8.21)
</p>
<p>For the extreme case k = 0, i.e., for a sample in which no event was
observed, one obtains the upper limit λ(up) by inverting α = F(1;λ(up)).
The upper limit then has the following meaning. If the true value of the
parameter were in fact λ = λ(up) and if one were to repeat the experiment
many times, then the probability of observing at least one event is β. The obs-
ervation k = 0 is then expressed in the following way: One has λ &lt; λ(up) with
a confidence level of 1&minus;α. From Table I.1 one finds that k = 0 corresponds
to λ &lt; 2.996 &asymp; 3 at a confidence level of 95%.
</p>
<p>This is done numerically with the method StatFunct.quantile
</p>
<p>Poisson. The function (</p>
<p/>
</div>
<div class="page"><p/>
<p>142 6 Samples
</p>
<p>Example 6.7: Determination of a lower limit for the lifetime of the proton
from the observation of no decays
</p>
<p>As already mentioned, the probability for the decay of a radioactive nucleus
with the time t is
</p>
<p>P (t)= 1
τ
</p>
<p>&int; t
</p>
<p>0
e&minus;x/τ dx .
</p>
<p>Here τ is the mean lifetime of the nucleus. For t ≪ τ the expression simpli-
fies to
</p>
<p>P (t)= t/τ .
For a total of N nuclei one expects that
</p>
<p>k =NP(t)=N &middot; t/τ
</p>
<p>nuclei will decay within the time t . The mean lifetime τ is obtained by count-
ing the number of such decays. If one observes k decays from a total of N
nuclei in a time t , then one obtains as the measured value of τ
</p>
<p>τ̃ = N
k
t .
</p>
<p>Of particular interest is the mean lifetime of the proton, one of the primary
building blocks of matter. In experiments recently carried out with great effort,
one observes large numbers of protons with detectors capable of detecting
each individual decay. Up to now, not a single decay has been seen. According
to Table I.9, the true expected number of decays λ does not exceed three (at a
confidence level of 95 %). One has therefore
</p>
<p>τ &gt;
N
</p>
<p>3
t
</p>
<p>at this confidence level. Typical experimental values are t = 0.3 years, N =
1033, i.e.,
</p>
<p>τ &gt; 1032 years .
</p>
<p>The proton can therefore be considered as stable even over cosmological time
scales, if one considers that the age of the universe is estimated to be only
around 1010 years.
</p>
<p>6.9 Small Samples with Background
</p>
<p>In many experiments one is faced with the following situation. For the
detected events one cannot determine whether they belong to the type that
is actually of interest (signal events) or to another type (background events).</p>
<p/>
</div>
<div class="page"><p/>
<p>6.9 Small Samples with Background 143
</p>
<p>For the expected number of events in the experiment one then has a Pois-
son distribution with the parameter λ = λS +λB . Here λS is the sought after
parameter of the number of signal events, and λB is the parameter for the
background events, which must of course be known if one wants to obtain
information about λS . (In an experiment as in Example 6.7, one might have,
for example, an admixture of radioactive nuclei whose decays cannot be dis-
tinguished from those of the proton. If the number of such nuclei and their
lifetime is known, then λB can be computed.)
</p>
<p>We are now tempted to simply take the results of the last section, to
determine the confidence limits λ&plusmn; and the upper limit λ(up) and to set
λS&plusmn; = λ&plusmn;&minus;λB , λ(up)S = λ(up)&minus;λB . This procedure can, however, lead to non-
sensical results. (As seen in Example 6.7, one has λ(up) = 3 at a confidence
level of 95%, for k = 0. For λB = 4, k = 0 we would obtain λ(up)S = &minus;1,
although a value λS &lt; 0 has no meaning.)
</p>
<p>The considerations up to now are based on the following. The probability
for observing n events, n= nS +nB , is
</p>
<p>f (n;λS +λB)=
1
</p>
<p>n!e
&minus;(λS+λB )(λS +λB)n , (6.9.1)
</p>
<p>and the probabilities to observe nS signal events, and nB background
events are
</p>
<p>f (nS;λS) =
1
</p>
<p>nS !
e&minus;λSλnSS , (6.9.2)
</p>
<p>f (nB;λB) =
1
</p>
<p>nB !
e&minus;λBλnBB . (6.9.3)
</p>
<p>The validity of (6.9.1) was shown in Example 5.5 with the help of the charac-
teristic function, starting from the independence of the two Poisson distribu-
tions (6.9.2) and (6.9.3). One can also obtain them directly by summation of
all products of the probabilities (6.9.2) and (6.9.3) that lead to n = nS +nB ,
by application of (B.4) and (B.6),
</p>
<p>n&sum;
</p>
<p>nS=0
f (nS;λS)f (n&minus;nS;λB)
</p>
<p>= e&minus;(λS+λB )
n&sum;
</p>
<p>nS=0
</p>
<p>1
</p>
<p>nS !(n&minus;nS)!
λ
nS
S λ
</p>
<p>n&minus;nS
B
</p>
<p>= 1
n!e
</p>
<p>&minus;(λS+λB )
n&sum;
</p>
<p>nS=0
</p>
<p>(
n
</p>
<p>nS
</p>
<p>)
λ
nS
S λ
</p>
<p>n&minus;nS
B
</p>
<p>= 1
n!e
</p>
<p>&minus;(λS+λB )(λS +λB)n
</p>
<p>= f (n;λS +λB) .</p>
<p/>
</div>
<div class="page"><p/>
<p>144 6 Samples
</p>
<p>The difficulties explained above are overcome with a method developed
by ZECH [26]. In an experiment in which k events are recorded, the number
of background events cannot simply be given by (6.9.3), since from the result
of the experiment it is known that nB &le; k. One must therefore replace (6.9.3)
by
</p>
<p>f &prime;(nB;λ)= f (nB;λB)
/
</p>
<p>k&sum;
</p>
<p>nB=0
f (nB;λB) , nB &le; k . (6.9.4)
</p>
<p>This distribution is normalized to unity in the region 0 &le; nB &le; k. In a corre-
sponding way the distribution
</p>
<p>f &prime;(n;λS +λB)= f (n;λS +λB)
/
</p>
<p>k&sum;
</p>
<p>nB=0
f (nB;λB) (6.9.5)
</p>
<p>takes the place of (6.9.1).
In this way one obtains in analogy to (6.8.17) and (6.8.18) for the limits
</p>
<p>of the confidence interval λS&minus; &le; λS &le; λS+ at a confidence level of 1&minus;α
</p>
<p>α/2 = F &prime;(k+1,λS++λB) , (6.9.6)
1&minus;α/2 = F &prime;(k,λS&minus;+λB) . (6.9.7)
</p>
<p>Here,
</p>
<p>F &prime;(k;λS +λB)=
k&minus;1&sum;
</p>
<p>n=0
f &prime;(n;λS +λB)= P (k &lt; k) (6.9.8)
</p>
<p>is the distribution function of the renormalized distribution (6.9.4). If only an
upper limit at confidence level 1&minus;α is desired, then one clearly has in analogy
to (6.8.21)
</p>
<p>α = F &prime;(k+1,λ(up)S +λB) . (6.9.9)
Table 6.3 gives some numerical values computed with the methods of
</p>
<p>the class SmallSample. Note that for k = 0, Eq. (6.9.7) has no meaning,
so that λS&minus; cannot be defined. In this case Eq. (6.9.6) and also λS+ are not
meaningful. (In the table, however, the values for λS&minus; and λS+ are shown as
computed by the program. This sets λS&minus; = 0 and computes λS+ according to
(6.9.6).)
</p>
<p>6.10 Determining a Ratio of Small Numbers of Events
</p>
<p>Often a number of signal events k is measured and compared to a number
of reference events d. One is interested in the true value r of the ratio of the</p>
<p/>
</div>
<div class="page"><p/>
<p>6.10 Determining a Ratio of Small Numbers of Events 145
</p>
<p>Table6.3: Limits λS&minus; and λS+ of the confidence interval and upper confidence limits λ
(up)
S ,
</p>
<p>for various values of λB and various very small sample sizes k for a fixed confidence level of
90%.
</p>
<p>β = 0.90
k λB λS&minus; λS+ λ
</p>
<p>(up)
S
</p>
<p>0 0.0 0.000 2.996 2.303
0 1.0 0.000 2.996 2.303
0 2.0 0.000 2.996 2.303
1 0.0 0.051 4.744 3.890
1 1.0 0.051 4.113 3.272
1 2.0 0.051 3.816 2.995
2 0.0 0.355 6.296 5.322
2 1.0 0.100 5.410 4.443
2 2.0 0.076 4.824 3.877
3 0.0 0.818 7.754 6.681
3 1.0 0.226 6.782 5.711
3 2.0 0.125 5.983 4.926
4 0.0 1.366 9.154 7.994
4 1.0 0.519 8.159 7.000
4 2.0 0.226 7.241 6.087
5 0.0 1.970 10.513 9.275
5 1.0 1.009 9.514 8.276
5 2.0 0.433 8.542 7.306
</p>
<p>number of signal events to the number of reference events, or more precisely,
the ratio of the probability to observe a signal event to that of a reference
event. As an estimator for this ratio one clearly uses
</p>
<p>r̃ = k/d .
</p>
<p>We now ask for the confidence limits of r . If k and d are sufficiently large,
then they may be approximated as Gaussian variables with standard devia-
tions σk =
</p>
<p>&radic;
k, σd =
</p>
<p>&radic;
d. Then according to the law of error propagation
</p>
<p>one has
</p>
<p>Δr =
</p>
<p>&radic;(
&part;r
</p>
<p>&part;k
</p>
<p>)2
k+
</p>
<p>(
&part;r
</p>
<p>&part;d
</p>
<p>)2
d = r
</p>
<p>&radic;
1
</p>
<p>k
+ 1
</p>
<p>d
. (6.10.1)
</p>
<p>If in addition one has d ≫ k, then Δr is simply
</p>
<p>Δr = r&radic;
k
</p>
<p>. (6.10.2)</p>
<p/>
</div>
<div class="page"><p/>
<p>146 6 Samples
</p>
<p>If the requirements for the validity of (6.10.1) or even of (6.10.2) are not
fulfilled, i.e., if k and d are small numbers, then one must use considerations
developed by JAMES and ROOS [23]. Clearly when one observes an individual
event, the probability that it is a signal event is given by
</p>
<p>p = r
1+ r , (6.10.3)
</p>
<p>and the probability that it is a reference event is
</p>
<p>q = 1&minus;p = 1
1+ r . (6.10.4)
</p>
<p>In an experiment in which a total of N = k+d are observed, the probability
that exactly n signal events are present is given by a binomial distribution
(5.1.3). This is
</p>
<p>f (n;r)=
(
</p>
<p>N
</p>
<p>n
</p>
<p>)
pnqN&minus;n =
</p>
<p>(
N
</p>
<p>n
</p>
<p>)(
r
</p>
<p>1+ r
</p>
<p>)n( 1
1+ r
</p>
<p>)N&minus;n
. (6.10.5)
</p>
<p>The probability to have n &lt; k is then
</p>
<p>P (n &lt; k)=
k&minus;1&sum;
</p>
<p>n=0
f (n;r)= F(k;r) (6.10.6)
</p>
<p>with
</p>
<p>F(k;r)=
k&minus;1&sum;
</p>
<p>n=0
</p>
<p>(
N
</p>
<p>n
</p>
<p>)(
r
</p>
<p>1+ r
</p>
<p>)n( 1
1+ r
</p>
<p>)N&minus;n
, (6.10.7)
</p>
<p>i.e., the distribution function of the binomial distribution. To determine the
limits r&minus; and r+ of the confidence interval at the confidence level β = 1&minus;α,
we use in analogy to (6.8.17) and (6.8.18)
</p>
<p>α/2 = F(k+1;r+) , (6.10.8)
1&minus;α/2 = F(k;r&minus;) . (6.10.9)
</p>
<p>If one only seeks an upper limit at the confidence level β = 1&minus;α, it can be
obtained from [see Eq. (6.8.21)]
</p>
<p>α = F(k+1;r (up)) . (6.10.10)
</p>
<p>The quantities r+, r&minus;, and r(up) can be computed for given values of k, d,
and β with the class SmallSample.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.11 Ratio of Small Numbers of Events with Background 147
</p>
<p>6.11 Ratio of Small Numbers of Events with Background
</p>
<p>By combining as done by SWARTZ [24] the ideas of Sects. 6.9 and 6.10, one
can deal with the following situation. In an experiment one has three types
of events: signal, background, and reference. Signal and background events
cannot be distinguished from each other. Suppose in the experiment one has
detected a total of k signal and background events and d reference events. Let
us label with rS and rB the true values (in the sense of the definition at the
beginning of the previous section) of the ratios of the numbers of signal to
reference and background to reference events. Then the probabilities that a
randomly selected event is signal or background, pS and pB , are
</p>
<p>pS =
rS
</p>
<p>1+ rS+ rB
, pB =
</p>
<p>rB
</p>
<p>1+ rS + rB
. (6.11.1)
</p>
<p>The probability that it is a reference event is then
</p>
<p>pR = 1&minus;pS &minus;pB =
1
</p>
<p>1+ rS+ rB
. (6.11.2)
</p>
<p>If one has a total of N= k+d events in the experiment, then the individual
probabilities that one has exactly nS signal events, nB background events, and
nR = N&minus;nS &minus;nB reference events are
</p>
<p>fS(nS;pS) =
(
</p>
<p>N
</p>
<p>nS
</p>
<p>)
p
nS
S (1&minus;pS)N&minus;nS , (6.11.3)
</p>
<p>fB(nB;pB) =
(
</p>
<p>N
</p>
<p>nB
</p>
<p>)
p
nB
B (1&minus;pB)N&minus;nB , (6.11.4)
</p>
<p>fR(nR;pR) =
(
</p>
<p>N
</p>
<p>nR
</p>
<p>)
p
nR
R (1&minus;pR)N&minus;nR . (6.11.5)
</p>
<p>Since there are now three mutually exclusive types of events, one has
instead of a binomial distribution (6.10.5) a trinomial distribution, i.e., a
multinomial distribution (5.1.10) with ℓ = 3. The probability that in an exp-
eriment with a total of N events one has exactly nS signal, nB background,
and N&minus;nS &minus;nB reference events is therefore
</p>
<p>f (nS,nB;rS, rB)
</p>
<p>= N!
nS !nB !(N&minus;nS &minus;nB)!
</p>
<p>p
nS
S p
</p>
<p>nB
B (1&minus;pS &minus;pB)N&minus;nS&minus;nB .
</p>
<p>(6.11.6)
</p>
<p>Here, however, we have not taken into consideration that the number of back-
ground events cannot be greater than k. In a manner similar to (6.9.4) one
uses</p>
<p/>
</div>
<div class="page"><p/>
<p>148 6 Samples
</p>
<p>f &prime;B(nB;pB)= fB(nB;pB)
/
</p>
<p>k&sum;
</p>
<p>nB=0
f (nB;pB) , nB &le; k , (6.11.7)
</p>
<p>in place of fB . This replacement must also be made in (6.11.6), which gives
</p>
<p>f &prime;(nS ,nB;rS, rB)= f (nS,nB;rS, rB)
/
</p>
<p>k&sum;
</p>
<p>nB=0
f (nB;pB) . (6.11.8)
</p>
<p>The probability to have nS signal events regardless of the number of back-
ground events is
</p>
<p>f &prime;(nS;rS, rB)=
k&sum;
</p>
<p>nB=0
f &prime;(nS,nB;rS, rB)
</p>
<p>and finally the probability to have nS &le; k is
</p>
<p>F &prime;(k;rS, rB)=
k&minus;nB&minus;1&sum;
</p>
<p>nS=0
f &prime;(nS;rS, rB)
</p>
<p>=
</p>
<p>k&minus;nB&minus;1&sum;
nS=0
</p>
<p>k&minus;1&sum;
nB=0
</p>
<p>N!
nS !nB !(N&minus;nS &minus;nB)!
</p>
<p>p
nS
S p
</p>
<p>nB
B (1&minus;pS &minus;pB)N&minus;nS&minus;nB
</p>
<p>k&sum;
nB=0
</p>
<p>N!
nB !(N &minus;nB)!
</p>
<p>p
nB
B (1&minus;pB)N&minus;nB
</p>
<p>.
</p>
<p>Since rB was assumed to be known, the quantity F &prime; for a given k depends
only on rS . Similar to (6.9.6) and (6.9.7) one can determine the limits rS+ and
rS&minus; of the confidence region for rS with confidence level β = 1&minus;α from the
following requirement:
</p>
<p>α/2 = F &prime;(k+1;rS+, rB) , (6.11.9)
1&minus;α/2 = F &prime;(k;rS&minus;, rB) . (6.11.10)
</p>
<p>If one only wants an upper limit with confidence level β = 1&minus;α, this can be
found according to (6.9.9) from
</p>
<p>α = F &prime;(k+1;r(up)S , rB) . (6.11.11)
</p>
<p>Table 6.4 contains some numerical values computed with methods of
</p>
<p>rS&minus; have no meaning. Similarly for (6.11.9), rS+ for k = 0 is not meaningful.
(In the table, however, values for rS&minus; and rS+ are given for k= 0 as computed
by the program. For k = 0 this sets rS&minus; = 0 and determines rS+ according to
(6.11.9).)
</p>
<p>the class k = 0, however, (6.11.10) and hence alsoSmallSample. For</p>
<p/>
</div>
<div class="page"><p/>
<p>6.12 Java Classes and Example Programs 149
</p>
<p>Table6.4: Limits rS&minus; and rS+ of the confidence interval and upper confidence limit r
(up)
S for
</p>
<p>various values of rB and various very small values of k for a fixed number of reference events
d and fixed confidence level of 90%.
</p>
<p>β = 0.90,d = 10
k rB rS&minus; rS+ r
</p>
<p>(up)
S
</p>
<p>0 0.0 0.000 0.349 0.259
0 0.1 0.000 0.349 0.259
0 0.2 0.000 0.349 0.259
1 0.0 0.005 0.573 0.450
1 0.1 0.005 0.502 0.382
1 0.2 0.005 0.464 0.348
2 0.0 0.034 0.780 0.627
2 0.1 0.010 0.686 0.535
2 0.2 0.007 0.613 0.467
3 0.0 0.077 0.979 0.799
3 0.1 0.020 0.880 0.701
3 0.2 0.012 0.788 0.612
4 0.0 0.127 1.174 0.968
4 0.1 0.044 1.074 0.869
4 0.2 0.019 0.976 0.771
5 0.0 0.180 1.367 1.135
5 0.1 0.085 1.267 1.035
5 0.2 0.034 1.167 0.936
</p>
<p>6.12 Java Classes and Example Programs
</p>
<p>Java Classes Referring to Samples
</p>
<p>mean, variance and standard deviation as well as the errors of these
quantities.
</p>
<p>samples.
</p>
<p>Sample contains methods computing characteristic parameters of a sample:
</p>
<p>SmallSamle contains methods computing the confidence limits for small
</p>
<p>Histograms allows the construction and administration of a histogram.</p>
<p/>
</div>
<div class="page"><p/>
<p>150 6 Samples
</p>
<p>A scatter plot is created and later displayed graphically. The coordinates of the points
making up the scatter plot are given as pairs of random numbers from a bivariate
normal distribution (cf. Sect. 4.10). The program asks for the parameters of the nor-
mal distribution (means a1, a2, standard deviations σ1, σ2, correlation coefficient ρ)
and for the number of random number pairs to be generated. It generates the pairs
and prepares the caption and the labeling of axes and scales and displays the plot
(Fig. 6.12).
</p>
<p>The program computes the limits λS&minus;, λS+, and λ
(up)
S for the Poisson parameter of
</p>
<p>a signal. The user enters interactively the number of observed events k, the confi-
dence level β = 1&minus; α, and the Poisson parameter λB of the background. Sugges-
tions: (a) Verify a few lines from Table 6.3.
(b) Choose β = 0.683, λB = 0 and compare for different values k the values λS&minus; and
λS+ with the naive statement λ= k&plusmn;
</p>
<p>&radic;
k.
</p>
<p>The program computes the limits rS&minus;, rS+, and r
(up)
S for the ratio r of the number of
</p>
<p>signal to reference events in the limit of a large number of events. More precisely,
r is the ratio of the Poisson parameters λS to λR of the signal to reference events.
The program asks interactively for the number k of observed (signal plus reference)
events, for the number d of reference events, for the confidence level β = 1&minus;α, and
for the expected ratio rB = λB/λR for background events.
</p>
<p>Suggestion: Verify a few lines from Table 6.4.
</p>
<p>Example Program 6.1: The class E1Sample demonstrates the use of the
</p>
<p>class
</p>
<p>This short program generates a sample of size N taken from the standard normal
</p>
<p>distribution. It computes the six quantities: mean, error of the mean, variance, error
</p>
<p>of the variance, standard deviation, and error of the standard deviation, and outputs
</p>
<p>each quantity in a single line.
</p>
<p>Example Program 6.2: The class E2Sample demonstrates the use of the
</p>
<p>classes
</p>
<p>Initialization, filling and graphical representation of a histogram are demonstrated for
</p>
<p>a sample of N elements from the standardized normal distribution. Interactive input
</p>
<p>is provided for N as well as for the lower boundary x0, bin width Δx and the number
</p>
<p>of bins nx of the histogram. The histogram is initialized, the sample elements are
</p>
<p>generated and entered into the histogram. Finally the histogram graphics is produced.
</p>
<p>Example Program 6.3: The class E3Sample demonstrates the use of
</p>
<p>class GraphicsWith2DScatterDiagrams
</p>
<p>Sample
</p>
<p>Histogram and GraphicsWithHistogarm
</p>
<p>Example Program 6.4: The class E4Sample demonstrates using the
</p>
<p>methods of the class SmallSample to compute confidence limits
</p>
<p>Example Program 6.5: The class E5Sample demonstrates the use of
</p>
<p>methods of the class
</p>
<p>ratios
</p>
<p>SmallSample to compute confidence limits of</p>
<p/>
</div>
<div class="page"><p/>
<p>6.12 Java Classes and Example Programs 151
</p>
<p>Fig.6.12: Pairs of random numbers taken from a bivariate normal distribution.
</p>
<p>Example Program 6.6:
</p>
<p>with few events and background
A total of nexp experiments are simulated. In each experiment N objects are analyzed.
Each object yields with probability pS = λS/N a signal event and with probability
pB = λB/N a background event. The numbers of events found in the simulated ex-
periment are kS , kB , and k = kS + kB . In the real experiment only k is known. The
limits λS&minus;, λS+, and λ
</p>
<p>(up)
S for k are computed for a given confidence level β = 1&minus;α
</p>
<p>and a given value of λB and are displayed for each experiment.
Suggestion: Choose, e.g., nexp = 20, N = 1000, λS = 5, λB = 2, β = 0.9 and
</p>
<p>find out whether, as expected, this simulation yields for 10% of the experiments an
interval (λS&minus;,λS+), which does not contain the value λS . Keep in mind the meaning
of the statistical error of your observation when using only 20 experiments.
</p>
<p>Example Program 6.7:
</p>
<p>with few signal events and with reference events
The program asks interactively for the quantities nexp, N , λS , λB , λR, and the con-
fidence level β = 1 &minus; α. It computes the probabilities pS = λS/N , pB = λB/N ,
pR = λR/N , as well as the ratios rS = pS/pR and rB = pB/pR of a total of nexp
simulated experiments, in each of which N objects are analyzed. Each object is taken
to be a signal event with probability pS , a background event with probability pB ,
and a reference event with probability pR. (Here pS + pB + pR ≪ 1 is assumed.)
The simulation yields the numbers kS , kB , and d for signal, background, and refer-
</p>
<p>The class E6Sample simulates experiments
</p>
<p>The class E7Sample simulates experiments</p>
<p/>
</div>
<div class="page"><p/>
<p>152 6 Samples
</p>
<p>ence events, respectively. In a real experiment only the numbers k = kS + kB and d
are known, since signal and background events cannot be distinguished. For the given
values of β and rB and for the quantities k and d which were found in the simulated
experiments, the limits rS&minus;, rS+, and r
</p>
<p>(up)
S are computed and displayed.
</p>
<p>Suggestion: Modify the suggestion accompanying Example Program 6.6 by
choosing an additional input parameter λS = 20. Find out in how many cases the
true value rS used in the simulation is not contained in the interval (rS&minus;, rS+).</p>
<p/>
</div>
<div class="page"><p/>
<p>7. The Method of Maximum Likelihood
</p>
<p>7.1 Likelihood Ratio: Likelihood Function
</p>
<p>In the last chapter we introduced the concept of parameter estimation. We
have also described the desirable properties of estimators, though without
specifying how such estimators can be constructed in a particular case. We
have derived estimators only for the important quantities expectation value
and variance. We now take on the general problem.
</p>
<p>In order to specify explicitly the parameters
</p>
<p>λ= (λ1,λ2, . . . ,λp) ,
we now write the probability density of the random variables
</p>
<p>x = (x1,x2, . . . ,xn)
in the form
</p>
<p>f = f (x;λ) . (7.1.1)
If we now carry out a certain number of experiments, say N , or we draw
a sample of size N from a population, then we can give a number to each
experiment j :
</p>
<p>dP (j) = f (x(j);λ)dx . (7.1.2)
The number dP (j) has the character of an a posteriori probability, i.e., given
after the experiment, how probable it was to find the result x(j) (within a small
interval). The total probability to find exactly all of the events
</p>
<p>x
(1),x(2), . . . ,x(j), . . . ,x(N)
</p>
<p>is then the product
</p>
<p>dP =
N&prod;
</p>
<p>j=1
f (x(j);λ) dx . (7.1.3)
</p>
<p>S. Brandt, Data Analysis: Statistical and Computational Methods for Scientists and Engineers,
DOI 10.1007/978-3-319-03762-2__7, &copy; Springer International Publishing Switzerland 2014
</p>
<p>153</p>
<p/>
</div>
<div class="page"><p/>
<p>154 7 The Method of Maximum Likelihood
</p>
<p>This probability still clearly depends on λ. There are cases where the popula-
tion is determined by only two possible sets of parameters, λ1 and λ2. Such
cases occur, for example, in nuclear physics, where the parity of a state is
necessarily &ldquo;even&rdquo; or &ldquo;odd&rdquo;. One can construct the ratio
</p>
<p>Q=
</p>
<p>N&prod;
</p>
<p>j=1
f (x(j);λ1)
</p>
<p>N&prod;
</p>
<p>j=1
f (x(j);λ2)
</p>
<p>(7.1.4)
</p>
<p>and say that the values λ1 are &ldquo;Q times more probable&rdquo; than the values λ2.
This factor is called the likelihood ratio.&lowast;
</p>
<p>A product of the form
</p>
<p>L=
N&prod;
</p>
<p>j=1
f (x(j);λ) (7.1.5)
</p>
<p>is called a likelihood function. One must clearly distinguish between a prob-
ability density and a likelihood function, which is a function of a sample and
is hence a random variable. In particular, the a posteriori nature of the proba-
bility in (7.1.5) is of significance in many discussions.
</p>
<p>Example 7.1: Likelihood ratio
</p>
<p>Suppose one wishes to decide whether a coin belongs to type A or B by
means of a number of tosses. The coins in question are asymmetric in such a
way that A shows heads with a probability of 1/3, and B shows heads with a
probability of 2/3.
</p>
<p>A B
</p>
<p>Heads 1/3 2/3
Tails 2/3 1/3
</p>
<p>If an experiment yields heads once and tails four times, then one has
LA = 13 &middot; (
</p>
<p>2
3)
</p>
<p>4 and LB = 23 &middot; (
1
3)
</p>
<p>4,
</p>
<p>Q= LA
LB
</p>
<p>= 8.
</p>
<p>One would therefore tend towards the position that the coin is of type A.
</p>
<p>&lowast;Although the likelihood ratio Q and the likelihood functions L and ℓ introduced below
are random variables, since they are functions of a sample, we do not write them here with a
special character type.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.2 The Method of Maximum Likelihood 155
</p>
<p>7.2 The Method of Maximum Likelihood
</p>
<p>The generalization of the likelihood ratio is now clear. One gives the great-
est confidence to that choice of the parameters λ for which the likelihood
function (7.1.5) is a maximum. Figure 7.1 illustrates the situation for various
forms of the likelihood function for the case of a single parameter λ.
</p>
<p>The maximum can be located simply by setting the first derivative of
the likelihood function with respect to the parameter λi equal to zero. The
derivative of a product with many factors is, however, unpleasant to deal with.
One first constructs therefore the logarithm of the likelihood function,
</p>
<p>ℓ= lnL=
N&sum;
</p>
<p>j=1
lnf (x(j);λ). (7.2.1)
</p>
<p>The function ℓ is also often called the likelihood function. Sometimes one
says explicitly &ldquo;log-likelihood function&rdquo;. Clearly the maxima of (7.2.1) are
identical with those of (7.1.5). For the case of a single parameter we now
construct
</p>
<p>ℓ&prime; = dℓ/dλ= 0. (7.2.2)
</p>
<p>The problem of estimating a parameter is now reduced to solving this likeli-
hood equation. By application of (7.2.1) we can write
</p>
<p>ℓ&prime; =
N&sum;
</p>
<p>j=1
</p>
<p>d
</p>
<p>dλ
lnf (x(j);λ)=
</p>
<p>N&sum;
</p>
<p>j=1
</p>
<p>f &prime;
</p>
<p>f
=
</p>
<p>N&sum;
</p>
<p>j=1
ϕ(x(j);λ), (7.2.3)
</p>
<p>λ
&sim;
</p>
<p>λ3
&sim;
</p>
<p>λ1
&sim;
</p>
<p>λ2
&sim;
</p>
<p>λ
&sim;
</p>
<p>λ
</p>
<p>λ
</p>
<p>L(λ)
</p>
<p>L(λ)
</p>
<p>L(λ)
</p>
<p>λ
</p>
<p>Fig.7.1: Likelihood functions.</p>
<p/>
</div>
<div class="page"><p/>
<p>156 7 The Method of Maximum Likelihood
</p>
<p>where
</p>
<p>ϕ(x(j);λ)=
(
</p>
<p>d
</p>
<p>dλ
f (x(j);λ)
</p>
<p>)/
f (x(j);λ) (7.2.4)
</p>
<p>is the logarithmic derivative of the density f with respect to λ.
In the general case of p parameters the likelihood equation (7.2.2) is
</p>
<p>replaced by the system of p simultaneous equations,
</p>
<p>&part;ℓ
</p>
<p>&part;λi
= 0, i = 1,2, . . . ,p. (7.2.5)
</p>
<p>Example 7.2: Repeated measurements of differing accuracy
</p>
<p>If a quantity is measured with different instruments, then the measurement er-
rors are in general different. The measurements x(j) are spread about the true
value λ. Suppose the errors are normally distributed, so that a measurement
corresponds to obtaining a sample from a Gaussian distribution with mean λ
and standard deviation σj . The a posteriori probability for a measured value
is then
</p>
<p>f (x(j);λ)dx = 1&radic;
2πσj
</p>
<p>exp
</p>
<p>(
&minus;(x
</p>
<p>(j)&minus;λ)2
</p>
<p>2σ 2j
</p>
<p>)
dx.
</p>
<p>From all N measurements one obtains the likelihood function
</p>
<p>L=
N&prod;
</p>
<p>j=1
</p>
<p>1&radic;
2πσj
</p>
<p>exp
</p>
<p>(
&minus;(x
</p>
<p>(j)&minus;λ)2
</p>
<p>2σ 2j
</p>
<p>)
(7.2.6)
</p>
<p>with the logarithm
</p>
<p>ℓ=&minus;1
2
</p>
<p>N&sum;
</p>
<p>j=1
</p>
<p>(x(j)&minus;λ)2
</p>
<p>σ 2j
+ const. (7.2.7)
</p>
<p>The likelihood equation thus becomes
</p>
<p>dℓ
</p>
<p>dλ
=
</p>
<p>N&sum;
</p>
<p>j=1
</p>
<p>x(j)&minus;λ
σ 2j
</p>
<p>= 0.
</p>
<p>It has the solution
</p>
<p>λ̃=
</p>
<p>N&sum;
</p>
<p>j=1
</p>
<p>x(j)
</p>
<p>σ 2j
</p>
<p>N&sum;
</p>
<p>j=1
</p>
<p>1
</p>
<p>σ 2j
</p>
<p>. (7.2.8)</p>
<p/>
</div>
<div class="page"><p/>
<p>7.3 Information Inequality. Estimators 157
</p>
<p>Since d2ℓ/dλ2 =&minus;
&sum;
</p>
<p>σ&minus;2j &lt; 0, the solution is, in fact, a maximum. Thus we
see that we obtain the maximum likelihood estimator as the mean of the N
measurements weighted inversely by the variances of the individual measure-
ments.
</p>
<p>Example 7.3: Estimation of the parameter N of the hypergeometric
distribution
</p>
<p>As in the example with coins at the beginning of this chapter, sometimes pa-
rameters to be estimated can only take on discrete values. In Example 5.2
we indicated the possibility of estimating zoological population densities by
means of tagging and recapture. According to (5.3.1), the probability to catch
exactly n fish of which k are tagged out of a pond with an unknown total of
N fish, out of which K are tagged, is given by
</p>
<p>L(k;n,K,N)=
</p>
<p>(
K
</p>
<p>k
</p>
<p>)(
N &minus;K
n&minus; k
</p>
<p>)
</p>
<p>(
N
</p>
<p>n
</p>
<p>) .
</p>
<p>We must now find the value of N for which the function L is maximum. For
this we use the ratio
</p>
<p>L(k;n,k,N)
L(k;n,k,N &minus;1) =
</p>
<p>(N &minus;n)(N &minus; k)
(N &minus;n&minus;K+ k)N
</p>
<p>{
&gt; 1 , Nk &lt; nK ,
&lt; 1 , Nk &gt; nK .
</p>
<p>The function L is thus maximum when N is the integer closest to nK/k.
</p>
<p>7.3 Information Inequality. Minimum Variance
</p>
<p>Estimators. Sufficient Estimators
</p>
<p>We now want to discuss once more the quality of an estimator. In Sect. 6.1 we
called an estimator unbiased if for every sample the bias vanished,
</p>
<p>B(λ)=E(S)&minus;λ= 0. (7.3.1)
Lack of bias is, however, not the only characteristic required of a &ldquo;good&rdquo;
estimator. More importantly one should require that the variance
</p>
<p>σ 2(S)
</p>
<p>is small. Here one must often find a compromise, since there is a connection
between B and σ 2, described by the information inequality.&dagger;
</p>
<p>One immediately sees that it is easy to achieve σ 2(S) = 0 simply by
using a constant for S. We consider an estimator S(x(1),x(2), . . . ,x(N)) that is
</p>
<p>&dagger;This inequality was independently found by H. Cramer, M. Fr&eacute;chet, and C. R. Rao as
well as by other authors. It is also called the Cramer&ndash;Rao or Fr&eacute;chet inequality.</p>
<p/>
</div>
<div class="page"><p/>
<p>158 7 The Method of Maximum Likelihood
</p>
<p>a function of the sample x(1),x(2), . . . ,x(N). According to (6.1.3) and (6.1.4)
the joint probability density of the elements of the sample is
</p>
<p>f (x(1),x(2), . . . ,x(N);λ)= f (x(1);λ)f (x(2);λ) &middot; &middot; &middot;f (x(N);λ).
</p>
<p>The expectation value of S is thus
</p>
<p>E(S)=
&int;
</p>
<p>S(x(1), . . . ,x(N))f (x(1);λ) &middot; &middot; &middot;f (x(N);λ)
</p>
<p>&times;dx(1) dx(2) &middot; &middot; &middot;dx(N). (7.3.2)
</p>
<p>According to (7.3.1), however, one also has
</p>
<p>E(S)= B(λ)+λ.
</p>
<p>We now assume that we can differentiate with respect to λ in the integral. We
then obtain
</p>
<p>1+B &prime;(λ)=
&int;
</p>
<p>S
</p>
<p>⎛
⎝
</p>
<p>N&sum;
</p>
<p>j=1
</p>
<p>f &prime;(x(j);λ)
f (x(j);λ)
</p>
<p>⎞
⎠f (x(1);λ) &middot; &middot; &middot;f (x(N);λ)dx(1) &middot; &middot; &middot;dx(N),
</p>
<p>which is equivalent to
</p>
<p>1+B &prime;(λ)= E
</p>
<p>⎧
⎨
⎩S
</p>
<p>N&sum;
</p>
<p>j=1
</p>
<p>f &prime;(x(j);λ)
f (x(j);λ)
</p>
<p>⎫
⎬
⎭=E
</p>
<p>⎧
⎨
⎩S
</p>
<p>N&sum;
</p>
<p>j=1
ϕ(x(j);λ)
</p>
<p>⎫
⎬
⎭ .
</p>
<p>From (7.2.3) we have
</p>
<p>ℓ&prime; =
N&sum;
</p>
<p>j=1
ϕ(x(j);λ)
</p>
<p>and therefore
1+B &prime;(λ)= E{Sℓ&prime;}. (7.3.3)
</p>
<p>One clearly has
&int;
</p>
<p>f (x(1);λ) &middot; &middot; &middot;f (x(N);λ)dx(1) &middot; &middot; &middot;dx(N) = 1.
</p>
<p>If we also compute the derivative with respect to λ, we obtain
</p>
<p>&int; N&sum;
</p>
<p>j=1
</p>
<p>f &prime;(x(j);λ)
f (x(j);λ) f (x
</p>
<p>(1);λ) &middot; &middot; &middot;f (x(N);λ)dx(1) &middot; &middot; &middot;dx(N) = E(ℓ&prime;)= 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.3 Information Inequality. Estimators 159
</p>
<p>By multiplying this equation by E(S) and subtracting the result of (7.3.3) one
obtains
</p>
<p>1+B &prime;(λ)= E{Sℓ&prime;}&minus;E(S)E(ℓ&prime;)= E{[S&minus;E(S)]ℓ&prime;}. (7.3.4)
</p>
<p>In order to see the significance of this expression, we need to use the Cauchy&ndash;
Schwarz inequality in the following form:
</p>
<p>If x and y are random variables and if x2 and y2 have finite
expectation values, then
</p>
<p>{E(xy)}2 &le; E(x2)E(y2). (7.3.5)
To prove this inequality we consider the expression
</p>
<p>E((ax+y)2)= a2E(x2)+2aE(xy)+E(y2)&ge; 0. (7.3.6)
This is a non-negative number for all values of a. If we consider for the mo-
ment the case of equality, then this is a quadratic equation for a with the
solutions
</p>
<p>a1,2 =&minus;
E(xy)
</p>
<p>E(x2)
&plusmn;
</p>
<p>&radic;(
E(xy)
</p>
<p>E(x2)
</p>
<p>)2
&minus; E(y
</p>
<p>2)
</p>
<p>E(x2)
. (7.3.7)
</p>
<p>The inequality (7.3.6) is then valid for all a if the term under the square root
is negative or zero. From this follows the assertion
</p>
<p>{E(xy)}2
{E(x2)}2 &minus;
</p>
<p>E(y2)
</p>
<p>E(x2)
&le; 0.
</p>
<p>If we now apply the inequality (7.3.5) to (7.3.4), one obtains
</p>
<p>{1+B &prime;(λ)}2 &le; E{[S&minus;E(S)]2}E(ℓ&prime;2). (7.3.8)
</p>
<p>We now use (7.2.3) in order to rewrite the expression for E(ℓ&prime;2),
</p>
<p>E(ℓ&prime;2) = E
</p>
<p>⎧
⎪⎨
⎪⎩
</p>
<p>⎛
⎝
</p>
<p>N&sum;
</p>
<p>j=1
ϕ(x(j);λ)
</p>
<p>⎞
⎠
</p>
<p>2
⎫
⎪⎬
⎪⎭
</p>
<p>= E
</p>
<p>⎧
⎨
⎩
</p>
<p>N&sum;
</p>
<p>j=1
(ϕ(x(j);λ))2
</p>
<p>⎫
⎬
⎭+E
</p>
<p>⎧
⎨
⎩
&sum;
</p>
<p>i �=j
ϕ(x(i);λ)ϕ(x(j);λ)
</p>
<p>⎫
⎬
⎭ .
</p>
<p>All terms on the right-hand side vanish, since for i �= j
</p>
<p>E{ϕ(x(i);λ)ϕ(x(j);λ)} = E{ϕ(x(i);λ)}E{ϕ(x(j);λ)},
</p>
<p>E{ϕ(x;λ)} =
&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>f &prime;(x;λ)
f (x;λ) f (x;λ)dx =
</p>
<p>&int;
f &prime;(x;λ)dx,</p>
<p/>
</div>
<div class="page"><p/>
<p>160 7 The Method of Maximum Likelihood
</p>
<p>and &int; &infin;
</p>
<p>&minus;&infin;
f (x;λ)dx = 1.
</p>
<p>By differentiating the last line with respect to λ one obtains
&int; &infin;
</p>
<p>&minus;&infin;
f &prime;(x;λ)dx = 0.
</p>
<p>Thus one has simply
</p>
<p>E(ℓ&prime;2)=E
</p>
<p>⎧
⎨
⎩
</p>
<p>N&sum;
</p>
<p>j=1
(ϕ(x(j);λ))2
</p>
<p>⎫
⎬
⎭=E
</p>
<p>⎧
⎨
⎩
</p>
<p>N&sum;
</p>
<p>j=1
</p>
<p>(
f &prime;(x(j);λ)
f (x(j);λ)
</p>
<p>)2
⎫
⎬
⎭ .
</p>
<p>Since the individual terms of the sum are independent, the expectation value
of the sum is simply the sum of the expectation values. The individual ex-
pectation values do not depend on the elements of the sample. Therefore
one has
</p>
<p>I (λ)= E(ℓ&prime;2)=NE
{(
</p>
<p>f &prime;(x;λ)
f (x;λ)
</p>
<p>)2}
.
</p>
<p>This expression is called the information of the sample with respect to λ. It
is a non-negative number, which vanishes if the likelihood function does not
depend on the parameter λ.
</p>
<p>It is sometimes useful to write the information in a somewhat different
form. To do this we differentiate the expression
</p>
<p>E
</p>
<p>(
f &prime;(x;λ)
f (x;λ)
</p>
<p>)
=
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>f &prime;(x;λ)
f (x;λ) f (x;λ)dx = 0
</p>
<p>once more with respect to λ and obtain
</p>
<p>0 =
&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>{
f &prime;2
</p>
<p>f
+f
</p>
<p>(
f &prime;
</p>
<p>f
</p>
<p>)&prime;}
dx =
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>{(
f &prime;
</p>
<p>f
</p>
<p>)2
+
(
f &prime;
</p>
<p>f
</p>
<p>)&prime;}
f dx
</p>
<p>= E
{(
</p>
<p>f &prime;
</p>
<p>f
</p>
<p>)2}
+E
</p>
<p>{(
f &prime;
</p>
<p>f
</p>
<p>)&prime;}
.
</p>
<p>The information can then be written as
</p>
<p>I (λ)=NE
{(
</p>
<p>f &prime;(x;λ)
f (x;λ)
</p>
<p>)2}
=&minus;NE
</p>
<p>{(
f &prime;(x;λ)
f (x;λ)
</p>
<p>)&prime;}
</p>
<p>or
I (λ)= E(ℓ&prime;2)=&minus;E(ℓ&prime;&prime;). (7.3.9)</p>
<p/>
</div>
<div class="page"><p/>
<p>7.3 Information Inequality. Estimators 161
</p>
<p>The inequality (7.3.8) can now be written in the following way:
</p>
<p>{1+B &prime;(λ)}2 &le; σ 2(S)I (λ)
</p>
<p>or
</p>
<p>σ 2(S) &ge; {1+B
&prime;(λ)}2
</p>
<p>I (λ)
. (7.3.10)
</p>
<p>This is the information inequality. It gives the connection between the bias
and the variance of an estimator and the information of a sample. It should be
noted that in its derivation no assumption about the estimator was made. The
right-hand side of the inequality (7.3.10) is therefore a lower bound for the
variance of an estimator. It is called the minimum variance bound or Cramer&ndash;
Rao bound. In cases where the bias does not depend on λ, i.e., particularly in
cases of vanishing bias, the inequality (7.3.10) simplifies to
</p>
<p>σ 2(S) &ge; 1/I (λ). (7.3.11)
</p>
<p>This relation justifies using the name information. As the information of a
sample increases, the variance of an estimator can be made smaller.
</p>
<p>We now ask under which circumstances the minimum variance bound is
attained, or explicitly, when the equals sign in the relation (7.3.10) holds. In
the inequality (7.3.6), one has equality if (ax+ y) vanishes, since only then
does one haveE{(ax+y)2}= 0 for all values of a, x, and y. Applied to (7.3.8),
this means that
</p>
<p>ℓ&prime;+a(S&minus;E(S))= 0
or
</p>
<p>ℓ&prime; = A(λ)(S&minus;E(S)). (7.3.12)
Here A means an arbitrary quantity that does not depend on the sample
x(1),x(2), . . . ,x(N), but may be, however, a function of λ. By integration we
obtain
</p>
<p>ℓ=
&int;
</p>
<p>ℓ&prime;dλ= B(λ)S+C(λ)+D (7.3.13)
</p>
<p>and finally
L= d exp{B(λ)S+C(λ)}. (7.3.14)
</p>
<p>The quantities d and D do not depend on λ.
We thus see that estimators attain the minimum variance bound when the
</p>
<p>likelihood function is of the special form (7.3.14). They are therefore called
minimum variance estimators.
</p>
<p>For the case of an unbiased minimum variance estimator we obtain
from (7.3.11)
</p>
<p>σ 2(S)= 1
I (λ)
</p>
<p>= 1
E(ℓ&prime;2)
</p>
<p>. (7.3.15)</p>
<p/>
</div>
<div class="page"><p/>
<p>162 7 The Method of Maximum Likelihood
</p>
<p>By substituting (7.3.12) one obtains
</p>
<p>σ 2(S)= 1
(A(λ))2E{(S&minus;E(S))2} =
</p>
<p>1
</p>
<p>(A(λ))2σ 2(S)
</p>
<p>or
</p>
<p>σ 2(S)= 1|A(λ)| . (7.3.16)
</p>
<p>If instead of (7.3.14) only the weaker requirement
</p>
<p>L= g(S,λ)c(x(1),x(2), . . . ,x(N)) (7.3.17)
</p>
<p>holds, then the estimator S is said to be sufficient for λ. One can show [see,
e.g., Kendall and Stuart, Vol. 2 (1967)], that no other estimator can con-
tribute information about λ that is not already contained in S, if the require-
ment (7.3.17) is fulfilled. Hence the name &ldquo;sufficient estimator&rdquo; (or statistic).
</p>
<p>Example 7.4: Estimator for the parameter of the Poisson distribution
</p>
<p>Consider the Poisson distribution (5.4.1)
</p>
<p>f (k)= λ
k
</p>
<p>k! e
&minus;λ.
</p>
<p>The likelihood function of a sample k(1),k(2), . . . ,k(N) is
</p>
<p>ℓ=
N&sum;
</p>
<p>j=1
{k(j) lnλ&minus; ln(k(j)!)&minus;λ}
</p>
<p>and its derivative with respect to λ is
</p>
<p>dℓ
</p>
<p>dλ
= ℓ&prime; =
</p>
<p>N&sum;
</p>
<p>j=1
</p>
<p>{
k(j)
</p>
<p>λ
&minus;1
</p>
<p>}
= 1
</p>
<p>λ
</p>
<p>N&sum;
</p>
<p>j=1
{k(j)&minus;λ},
</p>
<p>ℓ&prime; = N
λ
(k̄&minus;λ). (7.3.18)
</p>
<p>Comparing with (7.3.12) and (7.3.16) shows that the arithmetic mean k̄ is an
unbiased minimum variance estimator with variance λ/N .</p>
<p/>
</div>
<div class="page"><p/>
<p>7.3 Information Inequality. Estimators 163
</p>
<p>Example 7.5: Estimator for the parameter of the binomial distribution
</p>
<p>The likelihood function of a sample from a binomial distribution with the
parameters p = λ, q = 1&minus;λ is given directly by (5.1.3),
</p>
<p>L(k,λ)=
(
</p>
<p>n
</p>
<p>k
</p>
<p>)
λk(1&minus;λ)n&minus;k.
</p>
<p>(The result of the sample can be summarized by the statement that in n exper-
iments, the event A occurred k times; see Sect. 5.1.) One then has
</p>
<p>ℓ = lnL= k lnλ+ (n&minus;k) ln(1&minus;λ)+ ln
(
</p>
<p>n
</p>
<p>k
</p>
<p>)
,
</p>
<p>ℓ&prime; = k
λ
&minus; n&minus;k
</p>
<p>1&minus;λ =
n
</p>
<p>λ(1&minus;λ)
</p>
<p>(
k
</p>
<p>n
&minus;λ
</p>
<p>)
.
</p>
<p>By comparing with (7.3.12) and (7.3.16) one finds k/n to be a minimum vari-
ance estimator with variance λ(1&minus;λ)/n.
</p>
<p>Example 7.6: Law of error combination (&ldquo;Quadratic averaging of individual
errors&rdquo;)
</p>
<p>We now return to the problem of Example 7.2 of repeated measurements of
the same quantity with varying uncertainties, or expressed in another way,
to the problem of drawing a sample from normal distributions with the same
mean λ and different but known variances σj . From (7.2.7) we obtain
</p>
<p>dℓ
</p>
<p>dλ
= ℓ&prime; =
</p>
<p>N&sum;
</p>
<p>j=1
</p>
<p>x(j)&minus;λ
σ 2j
</p>
<p>.
</p>
<p>We can rewrite this expression as
</p>
<p>ℓ&prime; =
&sum; x(j)
</p>
<p>σ 2j
&minus;
&sum; λ
</p>
<p>σ 2j
</p>
<p>=
N&sum;
</p>
<p>j=1
</p>
<p>1
</p>
<p>σ 2j
</p>
<p>⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
</p>
<p>&sum; x(j)
σ 2j
</p>
<p>&sum; 1
σ 2j
</p>
<p>&minus;λ
</p>
<p>⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
.
</p>
<p>As in Example 7.2 we recognize
</p>
<p>S = λ̃=
</p>
<p>&sum; x(j)
σ 2j
</p>
<p>&sum; 1
σ 2j
</p>
<p>(7.3.19)</p>
<p/>
</div>
<div class="page"><p/>
<p>164 7 The Method of Maximum Likelihood
</p>
<p>as an unbiased estimator for λ. Comparing with (7.3.12) shows that it is
also a minimum variance estimator. From (7.3.16) one determines that its
variance is
</p>
<p>σ 2(λ̃)=
</p>
<p>⎛
⎝
</p>
<p>N&sum;
</p>
<p>j=1
</p>
<p>1
</p>
<p>σ 2j
</p>
<p>⎞
⎠
</p>
<p>&minus;1
</p>
<p>. (7.3.20)
</p>
<p>The relation (7.3.20) often goes by the name of the law of error combination
or quadratic averaging of individual errors. It could have been obtained by
application of the rule of error propagation (3.8.7) to (7.3.19). If we identify
σ(λ̃) as the error of the estimator λ̃ and σj as the error of the j th measurement,
then we can write it in its usual form
</p>
<p>Δλ̃=
(
</p>
<p>1
</p>
<p>(Δx1)2
+ 1
</p>
<p>(Δx2)2
+&middot;&middot; &middot;+ 1
</p>
<p>(Δxn)2
</p>
<p>)&minus; 12
. (7.3.21)
</p>
<p>If all of the measurements have the same error σ = σj , Eqs. (7.3.19), (7.3.20)
simplify to
</p>
<p>λ̃= x̄, σ 2(λ̃)= σ 2/n,
which we have already found in Sect. 6.2.
</p>
<p>7.4 Asymptotic Properties of the Likelihood Function
</p>
<p>and Maximum-Likelihood Estimators
</p>
<p>We can now show heuristically several important properties of the likelihood
function and maximum-likelihood estimators for very large data samples, that
is, for the limit N &rarr;&infin;. The estimator S = λ̃ was defined as the solution to
the likelihood equation
</p>
<p>ℓ&prime;(λ)=
N&sum;
</p>
<p>j=1
</p>
<p>(
f &prime;(x(j);λ)
f (x(j);λ)
</p>
<p>)
</p>
<p>λ̃
</p>
<p>= 0. (7.4.1)
</p>
<p>Let us assume that ℓ&prime;(λ) can be differentiated with respect to λ one more time,
so that we can expand it in a series around the point λ= λ̃,
</p>
<p>ℓ&prime;(λ)= ℓ&prime;(λ̃)+ (λ&minus; λ̃)ℓ&prime;&prime;(λ̃)+&middot;&middot; &middot; . (7.4.2)
</p>
<p>The first term on the right side vanishes because of Eq. (7.4.1). In the second
term one can write explicitly
</p>
<p>ℓ&prime;&prime;(λ̃)=
N&sum;
</p>
<p>j=1
</p>
<p>(
f &prime;(x(j);λ)
f (x(j);λ)
</p>
<p>)&prime;
</p>
<p>λ̃
</p>
<p>.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.4 Asymptotic Properties 165
</p>
<p>This expression has the form of the mean value of a sample. For very large N
it can be replaced by the expectation value of the population (Sect. 6.2),
</p>
<p>ℓ&prime;&prime;(λ̃)=NE
{(
</p>
<p>f &prime;(x;λ)
f (x;λ)
</p>
<p>)&prime;
</p>
<p>λ̃
</p>
<p>}
. (7.4.3)
</p>
<p>Using Eq. (7.3.9) we can now write
</p>
<p>ℓ&prime;&prime;(λ̃)= E(ℓ&prime;&prime;(λ̃))=&minus;E(ℓ&prime;2(λ̃))=&minus;I (λ̃)=&minus;1/b2. (7.4.4)
</p>
<p>In this way we can replace the expression for ℓ&prime;&prime;(λ̃), which is a function the
sample x(1),x(2), . . . ,x(N), by the quantity &minus;1/b2, which only depends on the
probability density f and the estimator λ̃. If one neglects higher-order terms,
Eq. (7.4.2) can be expressed as
</p>
<p>ℓ&prime;(λ)=&minus; 1
b2
</p>
<p>(λ&minus; λ̃). (7.4.5)
</p>
<p>By integration one obtains
</p>
<p>ℓ(λ)=&minus; 1
2b2
</p>
<p>(λ&minus; λ̃)2 + c.
</p>
<p>Inserting λ= λ̃ gives c = ℓ(λ̃), or
</p>
<p>ℓ(λ)&minus; ℓ(λ̃)=&minus;1
2
</p>
<p>(λ&minus; λ̃)2
b2
</p>
<p>. (7.4.6)
</p>
<p>By exponentiation one obtains
</p>
<p>L(λ)= k exp{&minus;(λ&minus; λ̃)2/2b2}, (7.4.7)
</p>
<p>where k is a constant. The likelihood function L(λ) has the form of a normal
distribution with mean λ̃ and variance b2. At the values λ= λ̃&plusmn;b, where λ is
one standard deviation from λ̃, one has
</p>
<p>&minus; (ℓ(λ)&minus; ℓ(λ̃))= 1
2
. (7.4.8)
</p>
<p>We can now compare (7.4.7) with Eqs. (7.3.12) and (7.3.16). Since we
are estimating the parameter λ, we must write S = λ̃ and thus E(S) = λ.
The estimator λ̃ is therefore an unbiased minimum variance estimator with
variance
</p>
<p>σ 2(λ̃)= b2 = 1
I (λ̃)
</p>
<p>= 1
E(ℓ&prime;2(λ̃))
</p>
<p>=&minus; 1
E(ℓ&prime;&prime;(λ̃))
</p>
<p>. (7.4.9)
</p>
<p>Since the estimator λ̃ only possesses this property for the limiting case N &rarr;
&infin;, we call it asymptotically unbiased. This is equivalent to the statement</p>
<p/>
</div>
<div class="page"><p/>
<p>166 7 The Method of Maximum Likelihood
</p>
<p>that the maximum likelihood estimator is consistent (Sect. 6.1). For the same
reason the likelihood function is called asymptotically normal.
</p>
<p>In Sect. 7.2 we interpreted the likelihood function L(λ) as a measure of
the probability that the true value λ0 of a parameter is equal to λ. The result
of an estimator is often represented in abbreviated form,
</p>
<p>λ= λ̃&plusmn;σ(λ̃)= λ̃&plusmn;Δλ̃.
</p>
<p>Since the likelihood function is asymptotically normal, at least in the case of
large samples, i.e., many measurements, this can be interpreted by saying that
the probability that the true value λ0 lies in the interval
</p>
<p>λ̃&minus;Δλ̃ &lt; λ0 &lt; λ̃+Δλ̃
</p>
<p>is 68.3% (Sect. 5.8). In practice the relation above is used for large but finite
samples. Unfortunately one cannot construct any general rule for determining
when a sample is large enough for this procedure to be reliable. Clearly if N
is finite, (7.4.3) is only an approximation, whose accuracy depends not only
on N , but also on the particular probability density f (x;λ).
</p>
<p>Example 7.7: Determination of the mean lifetime from a small number
of decays
</p>
<p>The probability that a radioactive nucleus, which exists at time t = 0, decays
in the time interval between t and t +dt is
</p>
<p>f (t)dt = 1
τ
</p>
<p>exp(&minus;t/τ )dt .
</p>
<p>For observed decay times t1, t2, . . ., tN the likelihood function is
</p>
<p>L= 1
τN
</p>
<p>exp
</p>
<p>{
&minus;1
τ
</p>
<p>N&sum;
</p>
<p>i=1
ti
</p>
<p>}
= 1
</p>
<p>τN
exp
</p>
<p>{
&minus;N
</p>
<p>τ
t̄
</p>
<p>}
.
</p>
<p>Its logarithm is
</p>
<p>ℓ= lnL=&minus;N
τ
t̄&minus;N lnτ
</p>
<p>with the derivative
</p>
<p>ℓ&prime; = N
τ
</p>
<p>(
t̄
</p>
<p>τ
&minus;1
</p>
<p>)
= N
</p>
<p>τ 2
(t̄ &minus; τ).
</p>
<p>Comparing with (7.3.12) we see that τ̃ = t̄ is the maximum likelihood so-
lution, which has a variance of σ 2(τ ) = τ 2/N . For τ = τ̃ = t̄ one obtains
Δτ̃ = t̄ /
</p>
<p>&radic;
N .</p>
<p/>
</div>
<div class="page"><p/>
<p>7.5 Simultaneous Estimation of Several Parameters: Confidence Intervals 167
</p>
<p>For τ̃ = t̄ one has
</p>
<p>ℓ(τ̃ )= ℓmax =&minus;N(1+ ln t̄ ).
</p>
<p>We can write
</p>
<p>&minus; (ℓ(τ )&minus; ℓ(τ̃ ))=N
(
t̄
</p>
<p>τ
+ ln τ
</p>
<p>t̄
&minus;1
</p>
<p>)
.
</p>
<p>From this expression for the log-likelihood function one cannot so easily rec-
ognize the asymptotic form (7.4.6) for N &rarr; &infin;. For small values of N it
clearly does not have this form. Corresponding to (7.4.8), we want to use the
values τ+ = τ̃ +Δ+ and τ&minus; = τ̃ &minus;Δ&minus;, where one has
</p>
<p>&minus; (ℓ(τ&plusmn;)&minus; ℓ(τ̃ ))=
1
</p>
<p>2
</p>
<p>for the asymmetric errors Δ+, Δ&minus;. Clearly we expect for N &rarr;&infin; that Δ+,
Δ&minus; &rarr;Δτ̃ = σ(τ̃ ).
</p>
<p>In Fig. 7.2 the N observed decay times ti are marked as vertical tick marks
on the abscissa for various small values of N . In addition the function &minus;(ℓ&minus;
ℓmax) = &minus;(ℓ(τ )&minus; ℓ(τ̃ )) is plotted. The points τ+ and τ&minus; are found where
a horizontal line intersects &minus;(ℓ&minus; ℓmax) = 1/2. The point τ̃ is indicated by
an additional mark on the horizontal line. One sees that with increasing N
the function &minus;(ℓ&minus;ℓmax) approaches more and more the symmetric parabolic
form and that the errors Δ+, Δ&minus;, and Δτ̃ become closer to each other.
</p>
<p>7.5 Simultaneous Estimation of Several Parameters:
</p>
<p>Confidence Intervals
</p>
<p>We have already given a system of equations (7.2.5) allowing the simulta-
neous determination of p parameters λ = (λ1,λ2, . . . ,λp). It turns out that
it is not the parameter determination but rather the estimation of their errors
that becomes significantly more complicated in the case of several parame-
ters. In particular we will need to consider correlations as well as errors of the
parameters.
</p>
<p>We extend our considerations from Sect. 7.4 on the properties of the like-
lihood function to the case of several parameters. The log-likelihood function
</p>
<p>ℓ(x(1),x(2), . . . ,x(N);λ)=
N&sum;
</p>
<p>j=1
lnf (x(j);λ) (7.5.1)
</p>
<p>can be expanded in a series about the point
</p>
<p>λ̃= (λ̃1, λ̃2, . . . , λ̃p) (7.5.2)</p>
<p/>
</div>
<div class="page"><p/>
<p>168 7 The Method of Maximum Likelihood
</p>
<p>Fig.7.2: Data and log-likelihood function for Example 7.7.
</p>
<p>to give
</p>
<p>ℓ(λ) = ℓ(λ̃)+
p&sum;
</p>
<p>k=1
</p>
<p>(
&part;ℓ
</p>
<p>&part;λk
</p>
<p>)
</p>
<p>λ̃
</p>
<p>(λk&minus; λ̃k)
</p>
<p>+ 1
2
</p>
<p>p&sum;
</p>
<p>ℓ=1
</p>
<p>p&sum;
</p>
<p>m=1
</p>
<p>(
&part;2ℓ
</p>
<p>&part;λℓ&part;λm
</p>
<p>)
</p>
<p>λ̃
</p>
<p>(λℓ&minus; λ̃ℓ)(λm&minus; λ̃m)+&middot;&middot; &middot; . (7.5.3)
</p>
<p>Since by the definition of λ̃ one has
(
</p>
<p>&part;ℓ
</p>
<p>&part;λk
</p>
<p>)
</p>
<p>λ̃
</p>
<p>= 0, k = 1,2, . . . ,p, (7.5.4)</p>
<p/>
</div>
<div class="page"><p/>
<p>7.5 Simultaneous Estimation of Several Parameters: Confidence Intervals 169
</p>
<p>which holds for all k, the series simplifies to
</p>
<p>&minus;
(
ℓ(λ)&minus; ℓ(λ̃)
</p>
<p>)
= 1
</p>
<p>2
(λ&minus; λ̃)TA(λ&minus; λ̃)+&middot;&middot; &middot; (7.5.5)
</p>
<p>with
</p>
<p>&minus;A=
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>&part;2ℓ
</p>
<p>&part;λ21
</p>
<p>&part;2ℓ
</p>
<p>&part;λ1&part;λ2
. . .
</p>
<p>&part;2ℓ
</p>
<p>&part;λ1&part;λp
</p>
<p>&part;2ℓ
</p>
<p>&part;λ1&part;λ2
</p>
<p>&part;2ℓ
</p>
<p>&part;λ22
. . .
</p>
<p>&part;2ℓ
</p>
<p>&part;λ2&part;λp
...
</p>
<p>&part;2ℓ
</p>
<p>&part;λ1&part;λp
</p>
<p>&part;2ℓ
</p>
<p>&part;λ2&part;λp
. . .
</p>
<p>&part;2ℓ
</p>
<p>&part;λ2p
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>λ=λ̃
</p>
<p>. (7.5.6)
</p>
<p>In the limit N &rarr;&infin; we can replace the elements of A, which still depend on
the specific sample, by the corresponding expectation values,
</p>
<p>B = E(A)=
</p>
<p>&minus;
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>E
</p>
<p>(
&part;2ℓ
</p>
<p>&part;λ21
</p>
<p>)
E
</p>
<p>(
&part;2ℓ
</p>
<p>&part;λ1&part;λ2
</p>
<p>)
. . . E
</p>
<p>(
&part;2ℓ
</p>
<p>&part;λ1&part;λp
</p>
<p>)
</p>
<p>E
</p>
<p>(
&part;2ℓ
</p>
<p>&part;λ1&part;λ2
</p>
<p>)
E
</p>
<p>(
&part;2ℓ
</p>
<p>&part;λ22
</p>
<p>)
. . . E
</p>
<p>(
&part;2ℓ
</p>
<p>&part;λ2&part;λp
</p>
<p>)
</p>
<p>...
</p>
<p>E
</p>
<p>(
&part;2ℓ
</p>
<p>&part;λ1&part;λp
</p>
<p>)
E
</p>
<p>(
&part;2ℓ
</p>
<p>&part;λ2&part;λp
</p>
<p>)
. . . E
</p>
<p>(
&part;2ℓ
</p>
<p>&part;λ2p
</p>
<p>)
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>λ=λ̃
</p>
<p>. (7.5.7)
</p>
<p>If we neglect higher-order terms, we can give the likelihood function as
</p>
<p>L= k exp{&minus;1
2
(λ&minus; λ̃)TB(λ&minus; λ̃)}. (7.5.8)
</p>
<p>Comparing with (5.10.1) shows that this is a p-dimensional normal distribu-
tion with mean λ̃ and covariance matrix
</p>
<p>C = B&minus;1. (7.5.9)
</p>
<p>The variances of the maximum likelihood estimators λ̃1, λ̃2, . . . , λ̃p are given
by the diagonal elements of the matrix (7.5.9). The off-diagonal elements are
the covariances between all possible pairs of estimators,</p>
<p/>
</div>
<div class="page"><p/>
<p>170 7 The Method of Maximum Likelihood
</p>
<p>σ 2(λ̃i)= cii, (7.5.10)
cov(λ̃j , λ̃k)= cjk . (7.5.11)
</p>
<p>For the correlation coefficient between the estimators λ̃j , λ̃k we can define
</p>
<p>̺(λ̃j , λ̃k)=
cov(λ̃j , λ̃k)
</p>
<p>σ (λ̃j )σ (λ̃k)
. (7.5.12)
</p>
<p>As in the case of a single parameter, the square roots of the variances are given
as the error or standard deviations of the estimators,
</p>
<p>Δλ̃i = σ(λ̃i)=
&radic;
cii . (7.5.13)
</p>
<p>In Sect. 7.4 we determined that by giving the maximum-likelihood estimator
and its error one defines a region that contains the true value of the parame-
ter with a probability of 68.3%. Since the likelihood function in the several
parameter case is asymptotically a Gaussian distribution of several variables,
this region is not determined only by the errors, but rather by the entire co-
variance matrix. In the special case of two parameters this is the covariance
ellipse, which we introduced in Sect. 5.10.
</p>
<p>The expression (7.5.8) has (with the replacement x= λ) exactly the form
of (5.10.1). We can therefore use it for all of the results of Sect. 5.10. For the
exponent one has
</p>
<p>&minus; 1
2
(λ&minus; λ̃)TB(λ&minus; λ̃)=&minus;1
</p>
<p>2
g(λ)=&minus;
</p>
<p>{
ℓ(λ)&minus; ℓ(λ̃)
</p>
<p>}
. (7.5.14)
</p>
<p>In the parameter space spanned by λ1, . . ., λp, the covariance ellipsoid of the
distribution (7.5.8) is then determined by the condition
</p>
<p>g(λ)= 1 = 2
{
ℓ(λ)&minus; ℓ(λ̃)
</p>
<p>}
. (7.5.15)
</p>
<p>For other values of g(λ) one obtains the confidence ellipsoids introduced in
Sect. 5.10. For smaller values of N , the series (7.5.3) cannot be truncated and
the approximation (7.5.7) is not valid. Nevertheless, the solution (7.5.4) can
clearly still be computed. For a given probability W one obtains instead of a
confidence ellipsoid a confidence region, contained within the hypersurface
</p>
<p>g(λ)= 2
{
ℓ(λ)&minus; ℓ(λ̃)
</p>
<p>}
= const. (7.5.16)
</p>
<p>The value of g is determined in the same way as for the confidence ellipsoid
as in Sect. 5.10.
</p>
<p>In Example 7.7 we computed the region λ̃&minus;Δ&minus; &lt; λ &lt; λ̃+Δ+ for the
case of a single variable. This corresponds to a confidence region with the
probability content 68.3%.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.5 Simultaneous Estimation of Several Parameters: Confidence Intervals 171
</p>
<p>Example 7.8: Estimation of the mean and variance of a normal distribution
</p>
<p>We want to determine the mean λ1 and standard deviation λ2 of a normal
distribution using a sample of size N . This problem occurs, for example, in the
measurement of the range of α-particles in matter. Because of the statistical
nature of the energy loss through a large number of independent individual
collisions, the range of the individual particles is Gaussian distributed about
some mean value. By measuring the range x(j) of N different particles, the
mean λ1 and &ldquo;straggling constant&rdquo; λ2 = σ can be estimated. We obtain the
likelihood function
</p>
<p>L=
N&prod;
</p>
<p>j=1
</p>
<p>1
</p>
<p>λ2
&radic;
</p>
<p>2π
exp
</p>
<p>(
&minus;(x
</p>
<p>(j)&minus;λ1)2
</p>
<p>2λ22
</p>
<p>)
</p>
<p>and
</p>
<p>ℓ=&minus;1
2
</p>
<p>N&sum;
</p>
<p>j=1
</p>
<p>(x(j)&minus;λ1)2
</p>
<p>λ22
&minus;N lnλ2 &minus; const.
</p>
<p>The system of likelihood equations is
</p>
<p>&part;ℓ
</p>
<p>&part;λ1
=
</p>
<p>N&sum;
</p>
<p>j=1
</p>
<p>x(j)&minus;λ1
λ22
</p>
<p>= 0,
</p>
<p>&part;ℓ
</p>
<p>&part;λ2
= 1
</p>
<p>λ32
</p>
<p>N&sum;
</p>
<p>j=1
(x(j)&minus;λ1)2 &minus;
</p>
<p>N
</p>
<p>λ2
= 0.
</p>
<p>Its solution is
</p>
<p>λ̃1 =
1
</p>
<p>N
</p>
<p>N&sum;
</p>
<p>j=1
x(j),
</p>
<p>λ̃2 =
</p>
<p>&radic;&radic;&radic;&radic;&radic;&radic;
</p>
<p>N&sum;
</p>
<p>j=1
(x(j)&minus; λ̃1)2
</p>
<p>N
.
</p>
<p>For the estimator of the mean, the maximum-likelihood method leads to the
arithmetic mean of the individual measurements. For the variance it gives the
quantity s&prime;2 (6.2.4), which has a small bias, and not s2, the unbiased estima-
tor (6.2.6).
</p>
<p>Let us now determine the matrix B. The second derivatives are
</p>
<p>&part;2ℓ
</p>
<p>&part;λ21
= &minus;N
</p>
<p>λ22
,</p>
<p/>
</div>
<div class="page"><p/>
<p>172 7 The Method of Maximum Likelihood
</p>
<p>&part;2ℓ
</p>
<p>&part;λ1&part;λ2
= &minus;2
</p>
<p>&sum;
(x(j)&minus;λ1)
λ32
</p>
<p>,
</p>
<p>&part;2ℓ
</p>
<p>&part;λ22
= &minus;3
</p>
<p>&sum;
(x(j)&minus;λ1)2
</p>
<p>λ42
+ N
</p>
<p>λ22
.
</p>
<p>We use the procedure of (7.5.7), substitute λ1,λ2 by λ̃1, λ̃2 and find
</p>
<p>B =
(
</p>
<p>N/λ̃22 0
0 2N/λ̃22
</p>
<p>)
</p>
<p>or for the covariance matrix
</p>
<p>C = B&minus;1 =
(
</p>
<p>λ̃22/N 0
0 λ̃22/2N
</p>
<p>)
.
</p>
<p>We interpret the diagonal elements as the errors of the corresponding param-
eters, i.e.,
</p>
<p>Δλ̃1 = λ̃2/
&radic;
N, Δλ̃2 = λ̃2/
</p>
<p>&radic;
2N.
</p>
<p>The estimators for λ1 and λ2 are not correlated.
</p>
<p>Example 7.9: Estimators for the parameters of a two-dimensional normal
distribution
</p>
<p>To conclude we consider a population described by a two-dimensional normal
distribution (Sect. 5.10)
</p>
<p>f (x1,x2)=
1
</p>
<p>2πσ1σ2
&radic;
</p>
<p>1&minus;̺2
exp
</p>
<p>[
&minus; 1
</p>
<p>2(1&minus;̺2)
</p>
<p>&times;
{
(x1 &minus;a1)2
</p>
<p>σ 21
+ (x2 &minus;a2)
</p>
<p>2
</p>
<p>σ 22
&minus;2̺(x1 &minus;a1)(x2 &minus;a2)
</p>
<p>σ1σ2
</p>
<p>}]
.
</p>
<p>By constructing and solving a system of five simultaneous likelihood equa-
tions for the five parameters a1,a2,σ 21 ,σ
</p>
<p>2
2 ,̺ we obtain for the maximum-
</p>
<p>likelihood estimators
</p>
<p>x̄1 =
1
</p>
<p>N
</p>
<p>N&sum;
</p>
<p>j=1
x
(j)
</p>
<p>1 , x̄2 =
1
</p>
<p>N
</p>
<p>N&sum;
</p>
<p>j=1
x
(j)
</p>
<p>2 ,
</p>
<p>s&prime;21 =
1
</p>
<p>N
</p>
<p>N&sum;
</p>
<p>j=1
(x
</p>
<p>(j)
</p>
<p>1 &minus; x̄1)
2, s&prime;22 =
</p>
<p>1
</p>
<p>N
</p>
<p>N&sum;
</p>
<p>j=1
(x
</p>
<p>(j)
</p>
<p>2 &minus; x̄2)
2,</p>
<p/>
</div>
<div class="page"><p/>
<p>7.6 Example Programs 173
</p>
<p>r =
</p>
<p>N&sum;
</p>
<p>j=1
(x
</p>
<p>(j)
</p>
<p>1 &minus; x̄1)(x
(j)
</p>
<p>2 &minus; x̄2)
</p>
<p>Ns&prime;1s
&prime;
2
</p>
<p>. (7.5.17)
</p>
<p>Exactly as in Example 7.8, the estimators for the variances s&prime;21 and s
&prime;2
2 are
</p>
<p>biased. This also holds for the expression (7.5.17), the sample correlation
coefficient r. Like all maximum likelihood estimators, r is consistent, i.e., it
provides a good estimation of ̺ for very large samples. For N &rarr;&infin; the prob-
ability density of the random variable r becomes a normal distribution with
mean ̺ and variance
</p>
<p>σ 2(r)= (1&minus;̺2)2/N. (7.5.18)
For finite samples the distribution is asymmetric. It is therefore important to
have a sufficiently large sample before applying Eq. (7.5.17). As a rule of
thumb, N &ge; 500 is usually recommended.
</p>
<p>7.6 Example Programs
</p>
<p>The program performs the computations and the graphical display for the problem
described in Example 7.7. First by Monte Carlo method a total of N decay times ti
of radioactive nuclei with a mean lifetime of τ = 1 are simulated. The number N of
decays and also the seeds for the random number generator are entered interactively.
</p>
<p>normal distribution from a simulated sample
The program asks interactively for the number nexp of experiments to simulate (i.e.,
of the samples to be treated consecutively), for the size npt of each sample and for
the means a1, a2, the standard deviations σ1, σ2, and the correlation coefficient ρ of a
bivariate Gaussian distribution.
</p>
<p>The covariance matrix C of the normal distribution is calculated and the genera-
tor of random numbers from a multivariate Gaussian distribution is initialized. Each
sample is generated and then analyzed, i.e., the quantities x̄1, x̄2, s&prime;1, s
</p>
<p>&prime;
2, and r are
</p>
<p>computed, which are estimates of a1, a2, σ1, σ2, and ρ [cf. (7.5.17)]. The quantities
are displayed for each sample.
</p>
<p>Suggestions: Choose nexp = 20, keep all other parameters fixed, and study the
statistical fluctuations of r for npt = 5, 50, 500. Use the values ρ = 0, 0.5, 0.95.
</p>
<p>Example Program 7.1: The class E1MaxLife computes the mean lifetime
</p>
<p>and its asymmetric errors from a small number of radioactive decays
</p>
<p>Example Program 7.2: The class E2MaxLife computes
</p>
<p>the maximum-likelihood estimates of the parameters of a bivariate</p>
<p/>
</div>
<div class="page"><p/>
<p>8. Testing Statistical Hypotheses
</p>
<p>8.1 Introduction
</p>
<p>Often the problem of a statistical analysis does not involve determining an
originally unknown parameter, but rather, one already has a preconceived
opinion about the value of the parameter, i.e., a hypothesis. In a sample taken
for quality control, for example, one might initially assume that certain critical
values are normally distributed within tolerance levels around their nominal
values. One would now like to test this hypothesis. To elucidate such test
procedures, called statistical tests, we will consider such an example and for
simplicity make the hypothesis that a sample of size 10 originates from a
standard normal distribution.
</p>
<p>Suppose the analysis of the sample resulted in the arithmetic mean
x̄ = 0.154. Under the assumption that our hypothesis is correct, the random
variable x̄ is normally distributed with mean 0 and standard deviation 1&radic;
</p>
<p>10
.
</p>
<p>We now ask for the probability to observe a value |x̄| &ge; 0.154 from such a
distribution. From (5.8.5) and Table I.3 this is
</p>
<p>P (|x̄| &ge; 0.154)= 2{1&minus;ψ0(0.154
&radic;
</p>
<p>10)} = 0.62 .
</p>
<p>Thus we see that even if our hypothesis is correct, there is a probability of
62% that a sample of size 10 will lead to a sample mean that differs from the
population mean by 0.154 or more.
</p>
<p>We now find ourselves in the difficult situation of having to answer the
simple question: &ldquo;Is our hypothesis true or false?&rdquo; A solution to this problem
is provided by the concept of the significance level: One specifies before the
test a (small) test probability α. Staying with our previous example, if P (|x̄| &ge;
0.154)&lt; α, then one would regard the occurrence of x̄= 0.154 as improbable.
That is, one would say that x̄ differs significantly from the hypothesized value
</p>
<p>S. Brandt, Data Analysis: Statistical and Computational Methods for Scientists and Engineers,
DOI 10.1007/978-3-319-03762-2__8, &copy; Springer International Publishing Switzerland 2014
</p>
<p>175</p>
<p/>
</div>
<div class="page"><p/>
<p>176 8 Testing Statistical Hypotheses
</p>
<p>and the hypothesis would be rejected. The converse is, however, not true. If P
does not fall below α, we cannot say that &ldquo;the hypothesis is true&rdquo;, but rather &ldquo;it
is not contradicted by the result of the sample&rdquo;. The choice of the significance
level depends on the problem being considered. For quality control of pencils
one might be satisfied with 1%. If, however, one wishes to determine insur-
ance premiums such that the probability for the company to go bankrupt is
less than α, then one would probably still regard 0.01% as too high. In the
analysis of scientific data α values of 5, 1, or 0.1% are typically used. From
Table I.3 we can obtain limiting values for |x̄| such that a deviation in excess
of these values corresponds to the given probabilities. These are
</p>
<p>0.05 = 2{1&minus;ψ0(1.96)} = 2{1&minus;ψ0(0.62
&radic;
</p>
<p>10)} ,
0.01 = 2{1&minus;ψ0(2.58)} = 2{1&minus;ψ0(0.82
</p>
<p>&radic;
10)} ,
</p>
<p>0.001 = 2{1&minus;ψ0(3.29)} = 2{1&minus;ψ0(1.04
&radic;
</p>
<p>10)} .
</p>
<p>At these significance levels the value |x̄| would have to exceed the values 0.62,
0.82, 1.04 before we could reject the hypothesis.
</p>
<p>In some cases the sign of x̄ is important. In many production processes,
deviations in a positive and negative direction are of different importance. (If a
baker&rsquo;s rolls are too heavy, this reduces profits; if they are too light they cost
the baker his license.) If one sets, e.g.,
</p>
<p>P (x̄ &ge; x&prime;α) &lt; α ,
</p>
<p>then this is referred to as a one-sided test in contrast to the two-sided test,
which we have already considered (Fig. 8.1).
</p>
<p>α
22
</p>
<p>α
α
</p>
<p>f(x)
</p>
<p>x
</p>
<p>f(x)
</p>
<p>xxα xα&minus;x&rsquo;α
Fig.8.1: One-sided
and two-sided tests
</p>
<p>For many tests one does not construct the sample mean but rather a certain
function of the sample called a test statistic, which is particularly suited for
tests of certain hypotheses. As above one specifies a certain significance level
α and determines a region U in the space of possible values of the test statistic
T in which
</p>
<p>PH (T &isin; U)= α .
The index H means that the probability was computed under the assumption
that the hypothesis H is valid. One then obtains a sample, which results in a</p>
<p/>
</div>
<div class="page"><p/>
<p>8.2 F -Test on Equality of Variances 177
</p>
<p>particular value T &prime; for the test statistic. If T &prime; is in the region U , the critical
region of the test, then the hypothesis is rejected.
</p>
<p>In the next sections we will discuss some important tests in detail and
then turn to a more rigorous treatment of test theory.
</p>
<p>8.2 F -Test on Equality of Variances
</p>
<p>The problem of comparing two variances occurs frequently in the development
of measurement techniques or production procedures. Consider two popula-
tions with the same expectation value; e.g., one measures the same quantity
with two different devices without systematic error. One may then ask if they
also have the same variance.
</p>
<p>To test this hypothesis we take samples of size N1 and N2 from both
populations, which we assume to be normally distributed. We construct the
sample variance (6.2.6) and consider the ratio
</p>
<p>F = s21/s22 . (8.2.1)
</p>
<p>If the hypothesis is true, then F will be near unity. It is known from Sect. 6.6
that for every sample we can construct a quantity that follows a χ2-distribution:
</p>
<p>X21 =
(N1 &minus;1)s21
</p>
<p>σ 21
= f1s
</p>
<p>2
1
</p>
<p>σ 21
,
</p>
<p>X22 =
(N2 &minus;1)s22
</p>
<p>σ 22
= f2s
</p>
<p>2
2
</p>
<p>σ 22
.
</p>
<p>The two distributions have f1 = (N1 &minus; 1) and f2 = (N2 &minus; 1) degrees of
freedom.
</p>
<p>Under the assumption that the hypothesis (σ 21 = σ 22 ) is true, one has
</p>
<p>F = f2
f1
</p>
<p>X21
</p>
<p>X22
.
</p>
<p>The probability density of a χ2-distribution with f degrees of freedom is
[see (6.6.10)]
</p>
<p>f (χ2)= 1
Γ (12f )2
</p>
<p>1
2 f
</p>
<p>(χ2)
1
2 (f&minus;2)e&minus;
</p>
<p>1
2 χ
</p>
<p>2
.
</p>
<p>We now compute the probability&lowast;
</p>
<p>&lowast;We use here the symbol W for a distribution function in order to avoid confusion with
the ratio F .</p>
<p/>
</div>
<div class="page"><p/>
<p>178 8 Testing Statistical Hypotheses
</p>
<p>W(Q)= P
(
X21
</p>
<p>X22
&lt;Q
</p>
<p>)
</p>
<p>that the ratio X21/X
2
2 is smaller than Q:
</p>
<p>W(Q)= 1
Γ (12f1)Γ (
</p>
<p>1
2f2)2
</p>
<p>1
2 (f1+f2)
</p>
<p>&int; &int;
x &gt; 0
y &gt; 0
</p>
<p>x/y &gt; Q
</p>
<p>x
1
2f1&minus;1e&minus;
</p>
<p>1
2xy
</p>
<p>1
2f2&minus;1e&minus;
</p>
<p>1
2y dx dy .
</p>
<p>Calculating the integral gives
</p>
<p>W(Q)=
Γ (12f )
</p>
<p>Γ (12f1)Γ (
1
2f2)
</p>
<p>&int; Q
</p>
<p>0
t
</p>
<p>1
2f1&minus;1(t+1)&minus; 12f dt , (8.2.2)
</p>
<p>where
f = f1 +f2 .
</p>
<p>Finally if one sets
F =Qf2/f1 ,
</p>
<p>then the distribution function of the ratio F can be obtained from (8.2.2),
</p>
<p>W(F)= P
(
</p>
<p>s21
</p>
<p>s22
&lt; F
</p>
<p>)
.
</p>
<p>This is called the Fisher F -distribution.&dagger; It depends on the parameters f1
and f2. The probability density for the F -distribution is
</p>
<p>f (F )=
(
f1
</p>
<p>f2
</p>
<p>) 1
2f1 Γ (12(f1 +f2))
</p>
<p>Γ (12f1)Γ (
1
2f2)
</p>
<p>F
1
2 f1&minus;1
</p>
<p>(
1+ f1
</p>
<p>f2
F
</p>
<p>)&minus; 12 (f1+f2)
. (8.2.3)
</p>
<p>This is shown in Fig. 8.2 for fixed values of f1 and f2. The distribution is
reminiscent of the χ2-distribution; it is only non-vanishing for F &ge; 0, and
has a long tail for F &rarr;&infin;. Therefore it cannot be symmetric. One can easily
show that for f2 &gt; 2 the expectation value is simply
</p>
<p>E(F)= f2/(f2 &minus;2) .
</p>
<p>We can now determine a limit F &prime;α with the requirement
</p>
<p>P
</p>
<p>(
s21
</p>
<p>s22
&gt; F &prime;α
</p>
<p>)
= α . (8.2.4)
</p>
<p>&dagger;This is also often called the ν2-distribution, ω2-distribution, or Snedecor distribution.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.2 F -Test on Equality of Variances 179
</p>
<p>This expression means that the limit F &prime;α is equal to the quantile F1&minus;α of the
F -distribution (see Sect. 3.3) since
</p>
<p>P
</p>
<p>(
s21
</p>
<p>s22
&lt; F &prime;α
</p>
<p>)
= P
</p>
<p>(
s21
</p>
<p>s22
&lt; F1&minus;α
</p>
<p>)
= 1&minus;α . (8.2.5)
</p>
<p>If this limit is exceeded, then we say that σ 21 &gt;σ
2
2 with the significance level α.
</p>
<p>The quantiles F1&minus;α for various pairs of values (f1,f2) are given in Table I.8.
In general one applies a two-sided test, i.e., one tests whether the ratio F is
between two limits F &prime;&prime;α and F
</p>
<p>&prime;&prime;&prime;
α , which are determined by
</p>
<p>Fig.8.2: Probability density of the F -distribution for fixed values of f1 = 2,4, . . . ,20. For
f1 = 2 one has f (F ) = e&minus;F . For f1 &gt; 2 the function has a maximum which increases for
increasing f1.
</p>
<p>P
</p>
<p>(
s21
</p>
<p>s22
&gt; F &prime;&prime;α
</p>
<p>)
= 1
</p>
<p>2
α, P
</p>
<p>(
s21
</p>
<p>s22
&lt; F &prime;&prime;&prime;α
</p>
<p>)
= 1
</p>
<p>2
α . (8.2.6)
</p>
<p>Because of the definition of F as a ratio, the inequality
</p>
<p>s21/s
2
2 &lt; F
</p>
<p>&prime;&prime;&prime;
α (f1,f2)</p>
<p/>
</div>
<div class="page"><p/>
<p>180 8 Testing Statistical Hypotheses
</p>
<p>clearly has the same meaning as
</p>
<p>s22/s
2
1 &gt; F
</p>
<p>&prime;&prime;&prime;
α (f2,f1) .
</p>
<p>Here the first argument gives the number of degrees of freedom in the
numerator, and the second in the denominator. The requirement (8.2.6) can
also be written:
</p>
<p>P
</p>
<p>(
s21
</p>
<p>s22
&gt; F &prime;&prime;α (f1,f2)
</p>
<p>)
= 1
</p>
<p>2
α, P
</p>
<p>(
s22
</p>
<p>s21
&gt; F &prime;&prime;α (f2,f1)
</p>
<p>)
= 1
</p>
<p>2
α . (8.2.7)
</p>
<p>Table I.8 can therefore be used for the one-sided as well as the two-sided
F -test.
</p>
<p>A glance at Table I.8 also shows that F1&minus;α/2 &gt; 1 for all reasonable values
of α. Therefore one needs only to find the limit for the ratio
</p>
<p>s2g/s
2
k &gt; F1&minus; 12α
</p>
<p>(fg,fk) . (8.2.8)
</p>
<p>Here the indices g and k give the larger and smaller values of the two
variances, i.e., s2g &gt; s
</p>
<p>2
k . If the inequality (8.2.8) is satisfied, then the hypothesis
</p>
<p>of equal variances must be rejected.
</p>
<p>Example 8.1: F -test of the hypothesis of equal variance of two series of
measurements
</p>
<p>A standard length (100 &micro;m) is measured using two traveling microscopes. The
measurements and computations are summarized in Table 8.1. From Table I.8
we find for the two-sided F -test with a significance level of 10%,
</p>
<p>F &prime;&prime;0.1(6,9)= F0.95(6,9)= 3.37 .
</p>
<p>The hypothesis of equal variances cannot be rejected.
</p>
<p>8.3 Student&rsquo;s Test: Comparison of Means
</p>
<p>We now consider a population that follows a standard Gaussian distribution.
Let x̄ be the arithmetic mean of a sample of size N . According to (6.2.3) the
variance of x̄ is related to the population variance by
</p>
<p>σ 2(x̄)= σ 2(x)/N . (8.3.1)
</p>
<p>If N is sufficiently large, then from the Central Limit Theorem, x̄ will be
normally distributed with mean x̂ and variance σ 2(x̄). That is,</p>
<p/>
</div>
<div class="page"><p/>
<p>8.3 Student&rsquo;s Test: Comparison of Means 181
</p>
<p>y = (x̄&minus; x̂)/σ (x̄) (8.3.2)
</p>
<p>will be described by a standard normal distribution. The quantity σ(x) is,
however, not known. We only know the estimate for σ 2(x),
</p>
<p>s2x =
1
</p>
<p>N &minus;1
</p>
<p>N&sum;
</p>
<p>j=1
(xj &minus; x̂)2 . (8.3.3)
</p>
<p>Then with (8.3.1) we can also estimate σ 2(x̄) to be
</p>
<p>s2x̄ =
1
</p>
<p>N(N &minus;1)
</p>
<p>N&sum;
</p>
<p>j=1
(xj &minus; x̄)2 . (8.3.4)
</p>
<p>We now ask to what extent (8.3.2) differs from the standard Gaussian distribu-
tion if σ(x̄) is replaced by sx̄. By means of a simple translation of coordinates
we can always have x̂ = 0. We therefore only consider the distribution of
</p>
<p>t = x̄/sx̄ = x̄
&radic;
N/sx . (8.3.5)
</p>
<p>Since (N &minus;1)s2x = f s2x follows a χ2-distribution with f =N&minus;1 degrees of
freedom, we can write
</p>
<p>Table8.1: F -test on the equality of variances. Data from Example 8.1.
</p>
<p>Measurement with
Measurement Instrument 1 Instrument 2
number [&micro;m] [&micro;m]
1 100 97
2 101 102
3 103 103
4 98 96
5 97 100
6 98 101
7 102 100
8 101
9 99
10 101
Mean 100 99.8
Degrees of freedom 9 6
s2 34/9 = 3.7 39/6 = 6.5
F = 6.5/3.7 = 1.8</p>
<p/>
</div>
<div class="page"><p/>
<p>182 8 Testing Statistical Hypotheses
</p>
<p>t = x̄
&radic;
N
&radic;
f /χ .
</p>
<p>The distribution function of t is given by
</p>
<p>F(t)= P (t &lt; t)= P
(
</p>
<p>x̄
&radic;
N
&radic;
f
</p>
<p>χ
&lt; t
</p>
<p>)
. (8.3.6)
</p>
<p>After a somewhat lengthy calculation one finds
</p>
<p>F(t)=
Γ (12(f +1))
Γ (12f )
</p>
<p>&radic;
π
&radic;
f
</p>
<p>&int; t
</p>
<p>&minus;&infin;
</p>
<p>(
1+ t
</p>
<p>2
</p>
<p>f
</p>
<p>)&minus; 12 (f+1)
dt .
</p>
<p>The corresponding probability density is
</p>
<p>f (t)=
Γ (12(f +1))
Γ (12f )
</p>
<p>&radic;
π
&radic;
f
</p>
<p>(
1+ t
</p>
<p>2
</p>
<p>f
</p>
<p>)&minus; 12 (f+1)
. (8.3.7)
</p>
<p>Fig.8.3: Student&rsquo;s distribution f (t) for f = 1,2, . . . ,10 degrees of freedom. For f = 1 the
maximum is lowest and the tails are especially prominent.
</p>
<p>Figure 8.3 shows the function f (t) for various degrees of freedom
f = N &minus; 1. A comparison with Fig. 5.7 shows that for f &rarr; &infin;, the distri-
bution (8.3.7) becomes the standard normal distribution φ0(t), as expected.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.3 Student&rsquo;s Test: Comparison of Means 183
</p>
<p>Like φ0(t), f (t) is symmetric about 0 and has a bell shape. Corresponding
to (5.8.3) one has
</p>
<p>P (|t | &le; t)= 2F(|t |)&minus;1 . (8.3.8)
By requiring &int; t &prime;α
</p>
<p>0
f (t)dt = 1
</p>
<p>2
(1&minus;α) (8.3.9)
</p>
<p>we can again determine limits &plusmn;t &prime;α at a given significance level α, where
</p>
<p>t &prime;α = t1&minus; 12α .
</p>
<p>The quantiles t1&minus; 12α
are given in Table I.9 for various values of α and f .
</p>
<p>The application of Student&rsquo;s test&Dagger; can be described in the following way:
One has a hypothesis λ0 for the population mean of a normal distribution. A
sample of size N yields the sample mean x̄ and sample variance s2x. If the
inequality
</p>
<p>|t | = |x̄&minus;λ0|
&radic;
N
</p>
<p>sx
&gt; t &prime;α = t1&minus; 12α (8.3.10)
</p>
<p>is fulfilled for a given significance level α, then the hypothesis must be
rejected.
</p>
<p>This is clearly a two-sided test. If deviations only in one direction are
important, then the corresponding test at the significance level α is
</p>
<p>t = (x̄&plusmn;λ0)
&radic;
N
</p>
<p>sx
&gt; t &prime;2α = t1&minus;α . (8.3.11)
</p>
<p>We can make the test more general and apply it to the problem of com-
paring two mean values. Suppose samples of size N1 and N2 have been taken
from two populations X and Y . We wish to find a measure of correctness for
the hypothesis that the expectation values are equal,
</p>
<p>x̂ = ŷ .
</p>
<p>Because of the Central Limit Theorem, the mean values are almost normally
distributed. Their variances are
</p>
<p>σ 2(x̄)= 1
N1
</p>
<p>σ 2(x), σ 2(ȳ)= 1
N2
</p>
<p>σ 2(y) (8.3.12)
</p>
<p>and the estimates for these quantities are
</p>
<p>&Dagger;The t-distribution was introduced by W. S. Gosset and published under the pseudonym
&ldquo;Student&rdquo;.</p>
<p/>
</div>
<div class="page"><p/>
<p>184 8 Testing Statistical Hypotheses
</p>
<p>s2x̄ =
1
</p>
<p>N1(N1 &minus;1)
</p>
<p>N1&sum;
</p>
<p>j=1
(x&minus; x̄)2 ,
</p>
<p>s2ȳ =
1
</p>
<p>N2(N2 &minus;1)
</p>
<p>N2&sum;
</p>
<p>j=1
(y&minus; ȳ)2 . (8.3.13)
</p>
<p>According to the discussion in Example 5.10, the difference
</p>
<p>Δ= x̄&minus; ȳ (8.3.14)
</p>
<p>also has an approximate normal distribution with
</p>
<p>σ 2(Δ)= σ 2(x̄)+σ 2(ȳ) . (8.3.15)
</p>
<p>If the hypothesis of equal means is true, i.e., Δ̂= 0, then the ratio
</p>
<p>Δ/σ(Δ) (8.3.16)
</p>
<p>follows the standard normal distribution. If σ(Δ) were known one could
immediately give the probability according to (5.8.2) for the hypothesis to
be fulfilled. But only sΔ is known. The corresponding ratio
</p>
<p>Δ/sΔ (8.3.17)
</p>
<p>will in general be somewhat larger.
Usually the hypothesis x̂ = ŷ implies that x̄ and ȳ come from the same
</p>
<p>population. Then σ 2(x) and σ 2(y) are equal, and we can use the weighted
mean of s2x and s
</p>
<p>2
y as the corresponding estimator. The weights are given by
</p>
<p>(N1 &minus;1) and (N2 &minus;1):
</p>
<p>s2 =
(N1 &minus;1)s2x+ (N2 &minus;1)s2y
(N1 &minus;1)+ (N2 &minus;1)
</p>
<p>. (8.3.18)
</p>
<p>From this we construct
</p>
<p>s2x̄ =
s2
</p>
<p>N1
, s2ȳ =
</p>
<p>s2
</p>
<p>N2
,
</p>
<p>and
</p>
<p>s2Δ = s2x̄+s2ȳ =
N1 +N2
N1N2
</p>
<p>s2 . (8.3.19)
</p>
<p>It can be shown (see [8]) that the ratio (8.3.17) follows the Student&rsquo;s t-
distribution with f = N1 +N2 &minus; 2 degrees of freedom. With this one can
now perform Student&rsquo;s difference test:</p>
<p/>
</div>
<div class="page"><p/>
<p>8.4 Concepts of the General Theory of Tests 185
</p>
<p>The quantity (8.3.17) is computed from the results of two samples. This
value is compared to a quantile of Student&rsquo;s distribution with f =N1+N2&minus;2
degrees of freedom with a significance level α. If
</p>
<p>|t | = |Δ|
sΔ
</p>
<p>= |x̄&minus; ȳ|
sΔ
</p>
<p>&ge; t &prime;α = t1&minus; 12α , (8.3.20)
</p>
<p>then the hypothesis of equal means must be rejected. Instead one would
assume x̂ &gt; ŷ or x̂ &lt; ŷ, depending on whether one has x̄ &gt; ȳ or x̄ &lt; ȳ.
</p>
<p>Example 8.2: Student&rsquo;s test of the hypothesis of equal means of two series
of measurements
</p>
<p>Column x of Table 8.2 contains measured values (in arbitrary units) of the
concentration of neuraminic acid in the red blood cells of patients suffering
from a certain blood disease. Column y gives the measured values for a group
of healthy persons. From the mean values and variances of the two samples
one finds
</p>
<p>|Δ| = |x̄&minus; ȳ| = 1.3 ,
</p>
<p>s2 =
15s2x+6s2y
</p>
<p>21
= 9.15 ,
</p>
<p>s2Δ =
23
</p>
<p>112
s2 = 1.88 .
</p>
<p>For α = 5% and f = 21 we find t1&minus;α/2 = 2.08. We must therefore conclude
that the experimental data is not sufficient to determine an influence of the
disease on the concentration.
</p>
<p>8.4 Concepts of the General Theory of Tests
</p>
<p>The test procedures discussed so far have been obtained more or less intuitively
and without rigorous justification. In particular we have not given any specific
reasons for the choice of the critical region. We now want to deal with the
theory of statistical tests in a somewhat more critical way. A complete treat-
ment of this topic would, however, go beyond the scope of this book.
</p>
<p>Each sample of size N can be characterized by N points in the sample
space of Sect. 2.1. For simplicity we will limit ourselves to a continuous ran-
dom variable x, so that the sample can be described byN points (x(1),x(2), . . . ,
x(N)) on the x axis. In the case of r random variables we would have N
points in an r-dimensional space. The result of such a sample, however, could
also be specified by a single point in a space of dimension rN . A sample
of size 2 with a single variable could, for example, be depicted as a point in</p>
<p/>
</div>
<div class="page"><p/>
<p>186 8 Testing Statistical Hypotheses
</p>
<p>Table8.2: Student&rsquo;s difference test on the equality of means. Data from Example 8.2.
</p>
<p>x y
</p>
<p>21 16
24 20
18 22
19 19
25 18
17 19
18 19
22
21
23
18
13
16
23
22
24
N1 = 16 N2 = 7
x̄ = 20.3 ȳ = 19.0
s2x = 171.8/15 s2y = 20/6
</p>
<p>a two-dimensional plane, spanned by the axes x(1),x(2). We will call such a
space the E space. Every hypothesis H consists of an assumption about the
probability density
</p>
<p>f (x;λ1,λ2, . . . ,λp)= f (x;λ) . (8.4.1)
</p>
<p>The hypothesis is said to be simple if the function f is completely specified,
i.e., if the hypothesis gives the values of all of the parameters λi . It is said to be
composite if the general mathematical form of f is known, but the exact value
of at least one parameter remains undetermined. A simple hypothesis could,
for example, specify a standard Gaussian distribution. A Gaussian distribution
with a mean of zero but an undetermined variance, however, is a composite
hypothesis. The hypothesis H0 is called the null hypothesis. Sometimes we
will write explicitly
</p>
<p>H0(λ= λ0)=H0(λ1 = λ10,λ2 = λ20, . . . ,λp = λp0) . (8.4.2)
</p>
<p>Other possible hypotheses are called alternative hypotheses, e.g.,
</p>
<p>H1(λ= λ1)=H1(λ1 = λ11,λ2 = λ21, . . . ,λp = λp1) . (8.4.3)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.4 Concepts of the General Theory of Tests 187
</p>
<p>Often one wants to test a null hypothesis of the type (8.4.2) against a composite
alternative hypothesis
</p>
<p>H1(λ �= λ0)=H1(λ1 �= λ10,λ2 �= λ20, . . . ,λp �= λp0) . (8.4.4)
</p>
<p>Since the null hypothesis makes a statement about the probability density in
the sample space, it also predicts the probability for observing a point X =
(x(1),x(2), . . . ,x(N)) in the E space.&sect; We now define a critical region Sc with
the significance level α by the requirement
</p>
<p>P (X &isin; Sc|H0)= α , (8.4.5)
</p>
<p>i.e., we determine Sc such that the probability to observe a point X within
Sc is α, under the assumption that H0 is true. If the point X from the sample
actually falls into the region Sc, then the hypothesis H0 is rejected. One must
note that the requirement (8.4.5) does not necessarily determine the critical
region Sc uniquely.
</p>
<p>Although using the E space is conceptually elegant, it is usually not very
convenient for carrying out tests. Instead one constructs a test statistic
</p>
<p>T = T (X)= T (x(1),x(2), . . . ,x(N)) (8.4.6)
</p>
<p>and determines a region U of the variable T such that it corresponds to the
critical region Sc, i.e., one performs the mapping
</p>
<p>X &rarr; T (X), Sc(X)&rarr; U(X) . (8.4.7)
</p>
<p>The null hypothesis is rejected if T &isin; U .
Because of the statistical nature of the sample, it is clearly possible that
</p>
<p>the null hypothesis could be true, even though it was rejected since X &isin; Sc.
The probability for such an error, an error of the first kind, is equal to α.
There is in addition another possibility to make a wrong decision, if one does
not reject the hypothesis H0 because X was not in the critical region Sc, even
though the hypothesis was actually false and an alternative hypothesis was
true. This is an error of the second kind. The probability for this,
</p>
<p>P (X �&isin; Sc|H1)= β , (8.4.8)
</p>
<p>depends of course on the particular alternative hypotheses H1. This connec-
tion provides us with a method to specify the critical region Sc. A test is
clearly most reasonable if for a given significance level α the critical region
is chosen such that the probability β for an error of the second kind is a
</p>
<p>&sect;Although X and the function T (x) introduced below are random variables, we do not
use for them a special character type.</p>
<p/>
</div>
<div class="page"><p/>
<p>188 8 Testing Statistical Hypotheses
</p>
<p>minimum. The critical region and therefore the test itself naturally depend
on the alternative hypothesis under consideration.
</p>
<p>Once the critical region has been determined, we can consider the proba-
bility for rejecting the null hypothesis as a function of the &ldquo;true&rdquo; hypothesis,
or rather as a function of the parameters that describe it. In analogy to (8.4.5),
this is
</p>
<p>M(Sc,λ)= P (X &isin; Sc|H) = P (X &isin; Sc|λ) . (8.4.9)
This probability is a function of Sc and of the parameters λ. It is called the
power function of a test. The complementary probability
</p>
<p>L(Sc,λ)= 1&minus;M(Sc,λ) (8.4.10)
</p>
<p>is called the acceptance probability or the operating characteristic function
of the test. It gives the probability to accept&para; the null hypothesis. One clearly
has
</p>
<p>M(Sc,λ0)= α, M(Sc,λ1)= 1&minus;β,
L(Sc,λ0)= 1&minus;α, L(Sc,λ1)= β .
</p>
<p>(8.4.11)
</p>
<p>The most powerful test of a simple hypothesis H0 with respect to the simple
alternative hypothesis is defined by the requirement
</p>
<p>M(Sc,λ1)= 1&minus;β = max . (8.4.12)
</p>
<p>Sometimes there exists a uniformly most powerful test, for which the require-
ment (8.4.12) holds for all possible alternative hypotheses.
</p>
<p>A test is said to be unbiased if its power function is greater than or equal
to α for all alternative hypotheses:
</p>
<p>M(Sc,λ1) &ge; α . (8.4.13)
</p>
<p>This definition is reasonable, since the probability to reject the null hypothesis
is then a minimum if the null hypothesis is true. An unbiased most powerful
test is the most powerful of all the unbiased tests. Correspondingly one can
define a unbiased uniformly most powerful test. In the next sections we will
learn the rules which sometimes allow one to construct tests with such desir-
able properties. Before turning to this task, we will first give an example to
illustrate the definitions just introduced.
</p>
<p>&para;We use here the word &ldquo;acceptance&rdquo; of a hypothesis, although more precisely one should
say, &ldquo;There is no evidence to reject the hypothesis.&rdquo;</p>
<p/>
</div>
<div class="page"><p/>
<p>8.4 Concepts of the General Theory of Tests 189
</p>
<p>Example 8.3: Test of the hypothesis that a normal distribution with given
variance σ 2 has the mean λ= λ0
</p>
<p>We wish to test the hypothesis H0(λ = λ0). As a test statistic we use the
arithmetic mean x̄ = 1
</p>
<p>n
(x1 + x2 + . . .+ xn). (We will see in Example 8.4 that
</p>
<p>this is the most appropriate test statistic for our purposes.) From Sect. 6.2 we
know that x̄ is normally distributed with mean λ and variance σ 2/n, i.e., that
the probability density for x̄ for the case λ= λ0 is given by
</p>
<p>f (x̄;λ0)=
&radic;
n&radic;
</p>
<p>2πσ
exp
</p>
<p>(
&minus; n
</p>
<p>2σ 2
(x̄&minus;λ0)2
</p>
<p>)
. (8.4.14)
</p>
<p>This is shown in Fig. 8.4 together with four different critical regions, all of
which have the same significance level α.
</p>
<p>These are the regions
</p>
<p>U1 : x̄ &lt; λI and x̄ &gt; λII with
&int; λI
&minus;&infin;f (x̄)dx̄ =
</p>
<p>&int;&infin;
λII
</p>
<p>f (x̄)dx̄ = 12α ,
U2 : x̄ &gt; λIII with
</p>
<p>&int;&infin;
λIII
</p>
<p>f (x̄)dx̄ = α ,
U3 : x̄ &lt; λIV with
</p>
<p>&int; λIV
&minus;&infin;f (x̄)dx̄ = α ,
</p>
<p>U4 : λV &le; x̄ &lt; λVI with
&int; λ0
λV
</p>
<p>f (x̄)dx̄ =
&int; λVI
λ0
</p>
<p>f (x̄)dx̄ = 12α .
</p>
<p>In order to obtain the power functions for each of these regions, we must vary
the mean value λ. The probability density of x̄ for an arbitrary value of λ is,
in analogy to (8.4.14), given by
</p>
<p>f (x̄;λ)=
&radic;
n&radic;
</p>
<p>2πσ
exp
</p>
<p>[
&minus; n
</p>
<p>2σ 2
(x̄&minus;λ)2
</p>
<p>]
. (8.4.15)
</p>
<p>The dashed curve in Fig. 8.4b represents the probability density for λ= λ1 =
λ0 +1. The power function (8.4.9) is now simply
</p>
<p>P (x̄ &isin; U |λ)=
&int;
</p>
<p>U
</p>
<p>f (x̄;λ)dx̄ . (8.4.16)
</p>
<p>The power functions obtained in this way for the critical regions U1, U2, U3,
U4 are shown in Fig. 8.4c for n= 2 (solid curve) and n= 10 (dashed curve).
</p>
<p>We can now compare the effects of the four tests corresponding to the
various critical regions. From Fig. 8.4c we immediately see that U1 corre-
sponds to an unbiased test, since the requirement (8.4.13) is clearly fulfilled.
On the other hand, the test with the critical region U2 is more powerful for
the alternative hypothesis H1(λ1 &gt; λ0), but is not good for H1(λ1 &lt; λ0). For
the test with U3, the situation is exactly the opposite. Finally, the region U4
provides a test for which the rejection probability is a maximum if the null hy-
pothesis is true. Clearly this is very undesirable. The test was only constructed
for demonstration purposes. If we compare the first three tests, we see that
none of them are more powerful than the other two for all values of λ1. Thus</p>
<p/>
</div>
<div class="page"><p/>
<p>190 8 Testing Statistical Hypotheses
</p>
<p>x 1
</p>
<p>x 2
</p>
<p>x 1
</p>
<p>x 2
</p>
<p>x 1
</p>
<p>x 2
</p>
<p>x 2
</p>
<p>1x
</p>
<p>n = 2
</p>
<p>= 10n
</p>
<p>&minus;
0x&minus;λ
</p>
<p>&minus;
0x&minus;λ
</p>
<p>&minus;
0x&minus;λ
</p>
<p>&minus;
0x&minus;λ
</p>
<p>-2
</p>
<p>1
</p>
<p>-2
</p>
<p>-3 -2 -1 0 1 2
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>0.8
</p>
<p>0.6
</p>
<p>0.4
</p>
<p>0.2
</p>
<p>-3 -2 0 1 2 3
</p>
<p>-3 -2 -1 0 1 2 3
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>-1
</p>
<p>3
</p>
<p>1
</p>
<p>-3 -2 -1 0 1 2 3
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>-3 -2 -1 0 1 2 3
</p>
<p>-3 -2 -1 0 1 2 3
</p>
<p>-3 -2 -1 0 1 2 3
</p>
<p>-3 -1 0 1 2 3-2
-3
</p>
<p>-1
-1-2-3 1 2 3
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>-3
</p>
<p>-2
</p>
<p>-1
-1-2-3 1 2 3
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>-3
</p>
<p>-2
</p>
<p>-1
-1-3 1 2 3
</p>
<p>2
</p>
<p>3
</p>
<p>-3
</p>
<p>-2
</p>
<p>-1
-1-2-3
</p>
<p>3
</p>
<p>2
</p>
<p>1
</p>
<p>1 2 3
</p>
<p>λ
</p>
<p>λ
</p>
<p>IV
</p>
<p>λ λ
</p>
<p>λ
</p>
<p>λI II
</p>
<p>V VI
</p>
<p>λ
1λ
</p>
<p>0f(x;    )
&minus;
</p>
<p>f(x;    )&minus;
</p>
<p>λ  &minus;λ1 0
</p>
<p>λ  &minus;λ1 0
</p>
<p>λ  &minus;λ1 0
</p>
<p>λ  &minus;λ1 0
</p>
<p>(a 1) (c 1)
</p>
<p>(c 2)
</p>
<p>(c 3)
</p>
<p>(c 4)
</p>
<p>(b 1)
</p>
<p>(b 2)
</p>
<p>(b 3)
</p>
<p>(b 4)
</p>
<p>(a 3)
</p>
<p>(a 4)
</p>
<p>(a 2)
</p>
<p>III
</p>
<p>Fig.8.4: (a) Critical regions in E space, (b) critical region of the test function, and (c) power
function of the test from Example 8.3.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.5 The Neyman&ndash;Pearson Lemma and Applications 191
</p>
<p>we have not succeeded in finding a uniformly most powerful test. In Exam-
ple 8.4, where we will continue the discussion of the present example, we will
determine that for this problem there does not exist a uniformly most powerful
test.
</p>
<p>8.5 The Neyman&ndash;Pearson Lemma and Applications
</p>
<p>In the last section we introduced the E space, in which a sample is represented
by a single point X. The probability to observe a point X within the critical
region Sc &ndash; providing the null hypothesis H0 is true &ndash; was defined in (8.4.5),
</p>
<p>P (X &isin; Sc|H0)= α . (8.5.1)
</p>
<p>We now define a conditional probability in E space,
</p>
<p>f (X|H0).
</p>
<p>One clearly has
</p>
<p>&int;
</p>
<p>Sc
</p>
<p>f (X|H0)dX = P (X &isin; Sc|H0)= α . (8.5.2)
</p>
<p>The NEYMAN&ndash;PEARSON lemma states the following:
</p>
<p>A test of the simple hypothesis H0 with respect to the simple
alternative hypothesis H1 is a most powerful test if the critical
region Sc in E space is chosen such that
</p>
<p>f (X|H0)
f (X|H1)
</p>
<p>{
&le; c for all X &isin; Sc ,
&ge; c for all X �&isin; Sc .
</p>
<p>(8.5.3)
</p>
<p>Here c is a constant which depends on the significance level.
</p>
<p>We will prove this by considering another region S along with Sc. It may
partially overlap with Sc, as sketched in Fig. 8.5. We choose the size of the
region S such that it corresponds to the same significance level, i.e.,
</p>
<p>&int;
</p>
<p>S
</p>
<p>f (X|H0)dX =
&int;
</p>
<p>Sc
</p>
<p>f (X|H0)dX = α .</p>
<p/>
</div>
<div class="page"><p/>
<p>192 8 Testing Statistical Hypotheses
</p>
<p>S
</p>
<p>B
C
</p>
<p>A
</p>
<p>Sc
</p>
<p>Fig.8.5: The regions S and Sc.
</p>
<p>Using the notation of Fig. 8.5, we can write
&int;
</p>
<p>A
</p>
<p>f (X|H0)dX =
&int;
</p>
<p>Sc
</p>
<p>f (X|H0)dX&minus;
&int;
</p>
<p>C
</p>
<p>f (X|H0)dX
</p>
<p>=
&int;
</p>
<p>S
</p>
<p>f (X|H0)dX&minus;
&int;
</p>
<p>C
</p>
<p>f (X|H0)dX
</p>
<p>=
&int;
</p>
<p>B
</p>
<p>f (X|H0)dX .
</p>
<p>Since A is contained in Sc, we can use (8.5.3), i.e.,&int;
</p>
<p>A
</p>
<p>f (X|H0)dX &le; c
&int;
</p>
<p>A
</p>
<p>f (X|H1)dX .
</p>
<p>Correspondingly, since B is outside of Sc, one has
&int;
</p>
<p>B
</p>
<p>f (X|H0)dX &ge; c
&int;
</p>
<p>B
</p>
<p>f (X|H1)dX .
</p>
<p>We can now express the power function (8.4.9) using these integrals:
</p>
<p>M(Sc,λ1) =
&int;
</p>
<p>Sc
</p>
<p>f (X|H1)dX =
&int;
</p>
<p>A
</p>
<p>f (X|H1)dX+
&int;
</p>
<p>C
</p>
<p>f (X|H1)dX
</p>
<p>&ge; 1
c
</p>
<p>&int;
</p>
<p>A
</p>
<p>f (X|H0)dX+
&int;
</p>
<p>C
</p>
<p>f (X|H1)dX
</p>
<p>&ge;
&int;
</p>
<p>B
</p>
<p>f (X|H1)dX+
&int;
</p>
<p>C
</p>
<p>f (X|H1)dX
</p>
<p>&ge;
&int;
</p>
<p>S
</p>
<p>f (X|H1)dX =M(S,λ1)
</p>
<p>or directly,
</p>
<p>M(Sc,λ1) &ge;M(S,λ1) . (8.5.4)
This is exactly the condition (8.4.12) for a uniformly most powerful test.
Since we have not made any assumptions about the alternative hypothesis
H1(λ= λ1) or the region S, we have proven that the requirement (8.5.3)
provides a uniformly most powerful test when it is fulfilled by the alterna-
tive hypothesis.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.5 The Neyman&ndash;Pearson Lemma and Applications 193
</p>
<p>Example 8.4: Most powerful test for the problem of Example 8.3
</p>
<p>We now continue with the ideas from Example 8.3, i.e., we consider tests
with a sample of size N , obtained from a normal distribution with known
variance σ 2 and unknown mean λ. The conditional probability density of a
point X = (x(1),x(2), . . . ,x(N)) in E space is the joint probability density of
the x(j) for given values of λ, i.e.,
</p>
<p>f (X|H0)=
(
</p>
<p>1&radic;
2πσ
</p>
<p>)N
exp
</p>
<p>⎡
⎣&minus; 1
</p>
<p>2σ 2
</p>
<p>N&sum;
</p>
<p>j=1
(x(j)&minus;λ0)2
</p>
<p>⎤
⎦ (8.5.5)
</p>
<p>for the null hypothesis and
</p>
<p>f (X|H1)=
(
</p>
<p>1&radic;
2πσ
</p>
<p>)N
exp
</p>
<p>⎡
⎣&minus; 1
</p>
<p>2σ 2
</p>
<p>N&sum;
</p>
<p>j=1
(x(j)&minus;λ1)2
</p>
<p>⎤
⎦ (8.5.6)
</p>
<p>for the alternative hypothesis. The ratio (8.5.3) takes on the form
</p>
<p>Q= f (X|H0)
f (X|H1)
</p>
<p>= exp
</p>
<p>⎡
⎣&minus; 1
</p>
<p>2σ 2
</p>
<p>⎧
⎨
⎩
</p>
<p>N&sum;
</p>
<p>j=1
(x(j)&minus;λ0)2 &minus;
</p>
<p>N&sum;
</p>
<p>j=1
(x(j)&minus;λ1)2
</p>
<p>⎫
⎬
⎭
</p>
<p>⎤
⎦
</p>
<p>= exp
</p>
<p>⎡
⎣&minus; 1
</p>
<p>2σ 2
</p>
<p>⎧
⎨
⎩N(λ
</p>
<p>2
0 &minus;λ21)&minus;2(λ0 &minus;λ1)
</p>
<p>N&sum;
</p>
<p>j=1
x(j)
</p>
<p>⎫
⎬
⎭
</p>
<p>⎤
⎦ .
</p>
<p>The expression
</p>
<p>exp
</p>
<p>[
&minus; N
</p>
<p>2σ 2
(λ20 &minus;λ21)
</p>
<p>]
= k &ge; 0
</p>
<p>is a non-negative constant. The condition (8.5.3) thus has the form
</p>
<p>k exp
</p>
<p>⎡
⎣λ0 &minus;λ1
</p>
<p>σ 2
</p>
<p>N&sum;
</p>
<p>j=1
x(j)
</p>
<p>⎤
⎦
{
</p>
<p>&le; c, X &isin; Sc ,
&ge; c, X �&isin; Sc .
</p>
<p>This is the same as
</p>
<p>(λ0 &minus;λ1)x̄
{
</p>
<p>&le; c&prime;, X &isin; Sc ,
&ge; c&prime;, X �&isin; Sc .
</p>
<p>(8.5.7)
</p>
<p>Here c&prime; is a constant different from c. Equation (8.5.7) is, however, not only
a condition for Sc, but also specifies directly that x̄ should be used as the
test variable. For each given λ1, i.e., for every simple alternative hypothesis
H1(λ = λ1), (8.5.7) gives a clear prescription for the choice of Sc or U , i.e.,
for the critical region and the test variable x̄.</p>
<p/>
</div>
<div class="page"><p/>
<p>194 8 Testing Statistical Hypotheses
</p>
<p>For the case λ1 &lt; λ0, the relation (8.5.7) becomes
</p>
<p>x̄
</p>
<p>{
&le; c&prime;&prime;, X &isin; Sc ,
&ge; c&prime;&prime;, X �&isin; Sc .
</p>
<p>This corresponds to the situation in Fig. 8.4b3 with c&prime;&prime; = λIV . Similarly, for
every alternative hypothesis with λ1 &gt; λ0, the critical region of the most
powerful test is given by
</p>
<p>x̄ &ge; c&prime;&prime;&prime;
</p>
<p>(see Fig. 8.4b2 with c&prime;&prime;&prime; =λ&prime;&prime;&prime;). There does not exist a uniformly most powerful
test, since the factor (λ0 &minus;λ1) in Eq. (8.5.7) changes sign at λ1 = λ0.
</p>
<p>8.6 The Likelihood-Ratio Method
</p>
<p>The Neyman&ndash;Pearson lemma gave the condition for a uniformly most pow-
erful test. Such a test did not exist if the alternative hypothesis included
parameter values that could be both greater and less than that of the null
hypothesis. We determined this in Example 8.4; it can be shown, however,
that it is true in general. The question thus arises as to what test should be
used when no uniformly most powerful test exists. Clearly this question is
not formulated precisely enough to allow a unique answer. We would like
in the following to give a prescription that allows us to construct tests that
have desirable properties and that have the advantage of being relatively easy
to use.
</p>
<p>We consider from the beginning the general case with p parameters λ=
(λ1,λ2, . . . ,λp). The result of a sample, i.e., the pointX= (x(1),x(2), . . . ,x(N))
in E space is to be used to test a given hypothesis. The (composite) null
hypothesis is characterized by a given region for each parameter. We can use
a p-dimensional space, with the λ1,λ2, . . . ,λp as coordinate axes, and con-
sider the region allowed by the null hypothesis as a region in this parameter
space, called ω. We denote the region in this space representing all possible
values of the parameters by Ω . The most general alternative hypothesis is
then the part of Ω that does not contain ω. We denote this by Ω&minus;ω. Recall
now from Chap. 7 the maximum-likelihood estimator λ̃ for a parameter λ. It
is that value of λ for which the likelihood function is a maximum. In Chap. 7
we tacitly assumed that one searched for the maximum in the entire allowable
parameter space. In the following we will consider maxima in a restricted
region (e.g., in ω). We write in this case λ̃(ω). The likelihood-ratio test defines
a test statistic
</p>
<p>T = f (x
(1),x(2), . . . ,x(N); λ̃(Ω)1 , λ̃
</p>
<p>(Ω)
2 , . . . , λ̃
</p>
<p>(Ω)
p )
</p>
<p>f (x(1),x(2), . . . ,x(N); λ̃(ω)1 , λ̃
(ω)
2 , . . . , λ̃
</p>
<p>(ω)
p )
</p>
<p>. (8.6.1)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.6 The Likelihood-Ratio Method 195
</p>
<p>Here f (x(1),x(2), . . . ,x(N);λ1,λ2, . . . ,λp) is the joint probability density of
the x(j) (j = 1,2, . . . ,N ), i.e., the likelihood function (7.1.5). The procedure
of the likelihood ratio test prescribes that we reject the null hypothesis if
</p>
<p>T &gt; T1&minus;α . (8.6.2)
</p>
<p>Here T1&minus;α is defined by
</p>
<p>P (T &gt; T1&minus;α|H0)=
&int; &infin;
</p>
<p>T1&minus;α
g(T |H0)dT , (8.6.3)
</p>
<p>and g(T |H0) is the conditional probability density for the test statistic T . The
following theorem by WILKS [9] concerns the distribution function of T (or
actually &minus;2lnT ) in the limiting case of very large samples:
</p>
<p>If a population is described by the probability density f (x;λ1,
λ2, . . . ,λp) that satisfies reasonable requirements of continuity,
and if p&minus; r of the p parameters are fixed by the null hypothesis,
while r parameters remain free, then the statistic &minus;2lnT follows
a χ2-distribution with p&minus; r degrees of freedom for very large
samples, i.e., for N &rarr;&infin;.
</p>
<p>We now apply this method to the problem of Examples 8.3 and 8.4, i.e.,
we consider tests with samples from a normally distributed population with
known variance and unknown mean.
</p>
<p>Example 8.5: Power function for the test from Example 8.3
</p>
<p>For the simple hypothesis H0(λ= λ0), the region ω of the parameter space is
reduced to the point λ= λ0. We have thus
</p>
<p>λ̃(ω) = λ0 . (8.6.4)
</p>
<p>If we consider the most general alternative hypothesis H1(λ= λ1 �= λ0), then
we obtain as the maximum-likelihood estimator of λ the sample mean x̄.
The likelihood ratio (8.6.1) thus becomes
</p>
<p>T = f (x
(1),x(2), . . . ,x(N); x̄)
</p>
<p>f (x(1),x(2), . . . ,x(N);λ0)
. (8.6.5)
</p>
<p>The joint probability density is given by (7.2.6),
</p>
<p>f (x(1),x(2), . . . ,x(N))=
(
</p>
<p>1&radic;
2πσ
</p>
<p>)N
exp
</p>
<p>⎡
⎣&minus; 1
</p>
<p>2σ 2
</p>
<p>N&sum;
</p>
<p>j=1
(x(j)&minus;λ)2
</p>
<p>⎤
⎦ . (8.6.6)
</p>
<p>Therefore,</p>
<p/>
</div>
<div class="page"><p/>
<p>196 8 Testing Statistical Hypotheses
</p>
<p>T = exp
</p>
<p>⎡
⎣ 1
</p>
<p>2σ 2
</p>
<p>⎧
⎨
⎩&minus;
</p>
<p>N&sum;
</p>
<p>j=1
(x(j)&minus; x̄)2 +
</p>
<p>N&sum;
</p>
<p>j=1
(x(j)&minus;λ0)2
</p>
<p>⎫
⎬
⎭
</p>
<p>⎤
⎦
</p>
<p>= exp
</p>
<p>⎡
⎣ 1
</p>
<p>2σ 2
</p>
<p>N&sum;
</p>
<p>j=1
(x̄&minus;λ0)2
</p>
<p>⎤
⎦
</p>
<p>= exp
[
</p>
<p>N
</p>
<p>2σ 2
(x̄&minus;λ0)2
</p>
<p>]
.
</p>
<p>We must now calculate T1&minus;α and reject the hypothesis H0 if the inequal-
ity (8.6.2) is fulfilled. Since the logarithm of T is a monotonic function of
T , we can use
</p>
<p>T &prime; = 2lnT = N
σ 2
</p>
<p>(x̄&minus;λ0)2 (8.6.7)
</p>
<p>as the test statistic and reject H0 if
</p>
<p>T &prime; &gt; T &prime;1&minus;α
</p>
<p>with &int; &infin;
</p>
<p>T &prime;1&minus;α
</p>
<p>h(T &prime;|H0)dT &prime; = α .
</p>
<p>In order to calculate the probability density h(T &prime;|H0) of T &prime;, we start with the
density f (x̄) for the sample mean with the condition λ= λ0,
</p>
<p>f (x̄|H0)=
&radic;
</p>
<p>N
</p>
<p>2πσ 2
exp
</p>
<p>(
&minus; N
</p>
<p>2σ 2
(x̄&minus;λ0)2
</p>
<p>)
.
</p>
<p>In order to carry out the transformation of variables (3.7.1), we need in addi-
tion the derivative,
</p>
<p>∣∣∣∣
dx̄
</p>
<p>dT &prime;
</p>
<p>∣∣∣∣=
1
</p>
<p>2
</p>
<p>&radic;
σ 2
</p>
<p>N
T &prime;&minus;1/2 ,
</p>
<p>which can be easily obtained from (8.6.7). One then has
</p>
<p>h(T &prime;|H0)=
∣∣∣∣
</p>
<p>dx̄
</p>
<p>dT &prime;
</p>
<p>∣∣∣∣f (x̄|H0)=
1&radic;
2π
</p>
<p>T &prime;&minus;1/2e&minus;T
&prime;/2 . (8.6.8)
</p>
<p>This is indeed a χ2-distribution for one degree of freedom. Thus in our
example, WILKS&rsquo; theorem holds even for finite N . We see, therefore, that
the likelihood-ratio test yields the unbiased test of Fig. 8.4b1. The test
</p>
<p>T &prime; = N
σ 2
</p>
<p>(x̄&minus;λ0)2 &gt; T &prime;1&minus;α</p>
<p/>
</div>
<div class="page"><p/>
<p>8.6 The Likelihood-Ratio Method 197
</p>
<p>is equivalent to
</p>
<p>(
N
</p>
<p>σ 2
</p>
<p>)1/2
|x̄&minus;λ0|&lt; λ&prime;,
</p>
<p>(
N
</p>
<p>σ 2
</p>
<p>)1/2
|x̄&minus;λ0|&gt; λ&prime;&prime; (8.6.9)
</p>
<p>with
&minus;λ&prime; = λ&prime;&prime; = (T &prime;1&minus;α)1/2 = (χ21&minus;α)1/2 = χ1&minus;α .
</p>
<p>We can use this result to compute explicitly the power function of our test. For
a given value of the population mean λ, the probability density for the sample
mean is
</p>
<p>f (x̄;λ)=
(
</p>
<p>N
</p>
<p>2πσ 2
</p>
<p>)1/2
exp
</p>
<p>[
&minus;N(x̄&minus;λ)
</p>
<p>2
</p>
<p>2σ 2
</p>
<p>]
= φ0
</p>
<p>(
x̄&minus;λ
σ/
</p>
<p>&radic;
N
</p>
<p>)
.
</p>
<p>Using (8.4.9) and (8.6.9) we obtain
</p>
<p>M(Sc;λ) =
&int; A
</p>
<p>&minus;&infin;
f (x̄;λ)dx̄+
</p>
<p>&int; &infin;
</p>
<p>B
</p>
<p>f (x̄;λ)dx̄ (8.6.10)
</p>
<p>= ψ0
(
χ1&minus;α&minus;
</p>
<p>λ&minus;λ0
σ/
</p>
<p>&radic;
N
</p>
<p>)
+ψ0
</p>
<p>(
χ1&minus;α+
</p>
<p>λ&minus;λ0
σ/
</p>
<p>&radic;
N
</p>
<p>)
,
</p>
<p>A = &minus;χ1&minus;ασ/
&radic;
N &minus;λ0, B = χ1&minus;ασ/
</p>
<p>&radic;
N &minus;λ0 .
</p>
<p>Here φ0 and ψ0 are the probability density and distribution function of the
standard normal distribution. The power function (8.6.10) is shown in Fig. 8.6
for α = 0.05 and various values of N/σ 2.
</p>
<p>Example 8.6: Test of the hypothesis that a normal distribution of unknown
variance has the mean value λ= λ0
</p>
<p>In this case the null hypothesis H0(λ = λ0) is composite, i.e., it makes no
statement about σ 2. From Example 7.8 we know the maximum-likelihood
estimator in the full parameter space,
</p>
<p>λ̃(Ω) = x̄ ,
</p>
<p>σ̃ 2(Ω) = 1
N
</p>
<p>N&sum;
</p>
<p>j=1
(x(j)&minus; x̄)2 = s&prime;2 .
</p>
<p>In the parameter space of the null hypothesis we have
</p>
<p>λ̃(ω) = λ0,
</p>
<p>σ̃ 2(ω) = 1
N
</p>
<p>N&sum;
</p>
<p>j=1
(x(j)&minus;λ0)2 .</p>
<p/>
</div>
<div class="page"><p/>
<p>198 8 Testing Statistical Hypotheses
</p>
<p>Fig.8.6: Power function of the test from Example 8.5. The right-most curve corresponds to
N/σ 2 = 1.
</p>
<p>The likelihood ratio (8.6.1) is then
</p>
<p>T =
(&sum;
</p>
<p>(x(j)&minus;λ0)2&sum;
(x(j)&minus; x̄)2
</p>
<p>)N/2
exp
</p>
<p>(
&minus;N
</p>
<p>2
</p>
<p>&sum;
(x(j)&minus; x̄)2&sum;
(x(j)&minus; x̄)2 +
</p>
<p>N
</p>
<p>2
</p>
<p>&sum;
(x(j)&minus;λ0)2&sum;
(x(j)&minus;λ0)2
</p>
<p>)
</p>
<p>=
(&sum;
</p>
<p>(x(j)&minus;λ0)2&sum;
(x(j)&minus; x̄)2
</p>
<p>)N/2
.
</p>
<p>We transform again to a different test statistic T &prime; that is a monotonic function
of T ,
</p>
<p>T &prime; = T 2/N =
&sum;
</p>
<p>(x(j)&minus;λ0)2&sum;
(x(j)&minus; x̄)2 =
</p>
<p>&sum;
(x(j)&minus; x̄)2 +N(x̄&minus;λ0)2&sum;
</p>
<p>(x(j)&minus; x̄)2 , (8.6.11)
</p>
<p>T &prime; = 1+ t
2
</p>
<p>N &minus;1 ,
</p>
<p>where
</p>
<p>t =
&radic;
N
</p>
<p>x̄&minus;λ0(&sum;
(x(j)&minus;x̄)2
N&minus;1
</p>
<p>)1/2 =
&radic;
N
</p>
<p>x̄&minus;λ0
sx
</p>
<p>= x̄&minus;λ0
sx̄
</p>
<p>(8.6.12)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.7 The χ2-Test for Goodness-of-Fit 199
</p>
<p>is Student&rsquo;s test variable introduced in Sect. 8.3. From (8.6.11) we can com-
pute a value of t for a given sample and reject the null hypothesis if
</p>
<p>|t |&gt; t1&minus; 12α .
</p>
<p>The very generally formulated method of the likelihood ratio has led us to
Student&rsquo;s test, which was originally constructed for tests with samples from a
normal distribution with known mean and unknown variance.
</p>
<p>8.7 The χ2-Test for Goodness-of-Fit
</p>
<p>8.7.1 χ2-Test with Maximal Number of Degrees of Freedom
</p>
<p>Suppose one has N measured values gi , i = 1,2, . . . ,N , each with a known
measurement error σi . The meaning of the measurement error is the follow-
ing: gi is a measurement of the (unknown) true quantity hi . One has
</p>
<p>gi = hi + εi, i = 1,2, . . . ,N . (8.7.1)
</p>
<p>Here the deviation εi is a random variable that follows a normal distribution
with mean 0 and standard deviation σi .
</p>
<p>We now want to test the hypothesis specifying the values hi on which the
measurement is based,
</p>
<p>hi = fi, i = 1,2, . . . ,N . (8.7.2)
</p>
<p>If this hypothesis is true, then all of the quantities
</p>
<p>ui =
gi &minus;fi
σi
</p>
<p>, i = 1,2, . . . ,N , (8.7.3)
</p>
<p>follow the standard Gaussian distribution. Therefore,
</p>
<p>T =
N&sum;
</p>
<p>i=1
u2i =
</p>
<p>N&sum;
</p>
<p>i=1
</p>
<p>(
gi &minus;fi
σi
</p>
<p>)2
(8.7.4)
</p>
<p>follows a χ2-distribution for N degrees of freedom. If the hypothesis (8.7.2) is
false, then the individual deviations of the measured values gi from the values
predicted by the hypothesis fi , normalized by the errors σi , (8.7.3), will be
greater. For a given significance level α, the hypothesis (8.7.2) is rejected if
</p>
<p>T &gt; χ21&minus;α , (8.7.5)
</p>
<p>i.e., if the quantity (8.7.4) is greater than the quantile χ21&minus;α of the χ
2-
</p>
<p>distribution for N degrees of freedom.</p>
<p/>
</div>
<div class="page"><p/>
<p>200 8 Testing Statistical Hypotheses
</p>
<p>8.7.2 χ2-Test with Reduced Number of Degrees of Freedom
</p>
<p>The number of degrees of freedom is reduced when the hypothesis to be tested
is less explicit than (8.7.2). For this case we consider the following exam-
ple. Suppose a quantity g can be measured as a function of an independent
controlled variable t , which itself can be set without error,
</p>
<p>g = g(t) .
</p>
<p>The individual measurements gi correspond to given fixed values ti of the
independent variable. The corresponding true quantities hi are given by some
function
</p>
<p>hi = h(ti) .
A particularly simple hypothesis for this function is the linear equation
</p>
<p>f (t)= h(t)= at+b . (8.7.6)
</p>
<p>The hypothesis can in fact include specifying the numerical values for
the parameters a and b. In this case, all values fi in (8.7.2) are exactly known,
and the quantity (8.7.4) follows &ndash; if the hypothesis is true &ndash; a χ2-distribution
for N degrees of freedom.
</p>
<p>The hypothesis may, however, only state: There exists a linear relation-
ship (8.7.6) between the controlled variable t and the variable h. The numer-
ical values of the parameters a and b are, however, unknown. In this case
one constructs estimators ã, b̃ for the parameters, which are functions of the
measurements gi and the errors σi . The hypothesis (8.7.2) is then
</p>
<p>hi = h(ti)= fi = ãti + b̃ .
</p>
<p>Since, however, ã and b̃ are functions of the measurements gi , the normalized
deviations ui in (8.7.3) are no longer all independent. Therefore the number
of degrees of freedom of the χ2-distribution for the sum of squares (8.7.4)
is reduced by 2 to N &minus; 2, since the determination of the two quantities ã, b̃
introduces two equations of constraint between the quantities ui .
</p>
<p>8.7.3 χ2-Test and Empirical Frequency Distribution
</p>
<p>Suppose we have a distribution function F(x) and its probability density
f (x). The full region of the random variable x can be divided into r inter-
vals
</p>
<p>ξ1,ξ2, . . . ,ξi, . . . ,ξr ,
</p>
<p>as shown in Fig. 8.7. By integrating f (x) over the individual intervals we
obtain the probability to observe x in ξi ,</p>
<p/>
</div>
<div class="page"><p/>
<p>8.7 The χ2-Test for Goodness-of-Fit 201
</p>
<p>pi = P (x &isin; ξi)=
&int;
</p>
<p>ξi
</p>
<p>f (x)dx;
r&sum;
</p>
<p>i=1
pi = 1 . (8.7.7)
</p>
<p>We now take a sample of size n and denote by ni the number of elements of the
sample that fall into the interval ξi . An appropriate graphical representation
of the sample is a histogram, as described in Sect. 6.3.
</p>
<p>&hellip;
x
</p>
<p>f(x)
</p>
<p>ξ2 ξk ξrξ1
&hellip; Fig.8.7: Dividing the range of the
</p>
<p>variable x into the intervals ξk .
</p>
<p>One clearly has
r&sum;
</p>
<p>i=1
ni = n . (8.7.8)
</p>
<p>From the (hypothetical) probability density for the population we would have
expected the value
</p>
<p>npi
</p>
<p>for ni . For large values of ni , the variance of ni is equal to ni (Sect. 6.8), and
the distribution of the quantity ui with
</p>
<p>u2i =
(ni &minus;npi)2
</p>
<p>ni
(8.7.9)
</p>
<p>becomes approximately &ndash; if the hypothesis is true &ndash; a standard Gaussian
distribution. This holds also if one uses the expected variances npi instead
of the observed quantities ni in the denominator of (8.7.9),
</p>
<p>u2i =
(ni &minus;npi)2
</p>
<p>npi
. (8.7.10)
</p>
<p>If we now construct the sum of squares of the ui for all intervals,
</p>
<p>X2 =
r&sum;
</p>
<p>i=1
u2i , (8.7.11)
</p>
<p>then we expect (for large n) that this follows a χ2-distribution if the hypoth-
esis is true. The number of degrees of freedom is r&minus; 1, since the ui are not
independent because of (8.7.8). The number of degrees of freedom is reduced
to r&minus;1&minus;p if, in addition,p parameters are determined from the observations.</p>
<p/>
</div>
<div class="page"><p/>
<p>202 8 Testing Statistical Hypotheses
</p>
<p>Example 8.7: χ2-test for the fit of a Poisson distribution to an empirical
frequency distribution
</p>
<p>In an experiment investigating photon-proton interactions, a beam of high
energy photons (γ -quanta) impinge on a hydrogen bubble chamber. The
processes by which a photon materializes in the chamber, electron-positron
pair conversion, are counted in order to obtain a measure of the intensity
of the photon beam. The frequency of cases in which 0,1,2,. . . pairs are ob-
served simultaneously, i.e., in the same bubble-chamber photograph, follows a
Poisson distribution (see Example 5.3). Deviations from the Poisson distribu-
tion provide information about measurement losses, which are important for
uncovering systematic errors. The results of observing n = 355 photographs
are given in column 2 of Table 8.3 and in Fig. 8.8. From Example 7.4, we
know that the maximum-likelihood estimator of the parameter of the Poisson
distribution is given by λ̃=
</p>
<p>&sum;
k knk/
</p>
<p>&sum;
k nk. We find λ̃= 2.33. The values pk
</p>
<p>of the Poisson distribution with this parameter multiplied by n are given in
column 3. By summing the squared terms in column 4 one obtains the value
X2 = 10.44. The problem has six degrees of freedom, since r = 8, p = 1. We
chose α = 1% and find χ20.99 = 16.81 from Table I.7. We therefore have no
reason to reject the hypothesis of a Poisson distribution.
</p>
<p>Table8.3: Data for the χ2-test from Example 8.7.
</p>
<p>Number of
electron pairs
per photograph
</p>
<p>Number of
photographs with
k electron pairs
</p>
<p>Prediction
of Poisson
distribution
</p>
<p>(nk&minus;npk)2
npk
</p>
<p>k nk npk
0 47 34.4 4.61
1 69 80.2 1.56
2 84 93.7 1.00
3 76 72.8 0.14
4 49 42.6 0.96
5 16 19.9 0.76
6 11 7.8 1.31
7 3 2.5 0.10
8 &mdash; (0.7)
</p>
<p>n=
&sum;
</p>
<p>nk = 355 X2 = 10.44</p>
<p/>
</div>
<div class="page"><p/>
<p>8.8 Contingency Tables 203
</p>
<p>k
2 4 6 80
</p>
<p>20
</p>
<p>40
</p>
<p>60
</p>
<p>80
</p>
<p>nk, npk
</p>
<p>Fig.8.8: Comparison of the experimental distri-
bution nk (histogram with solid line) from Exam-
ple 8.7 with the Poisson distribution npk (dashed
line).
</p>
<p>8.8 Contingency Tables
</p>
<p>Suppose n experiments have been carried out whose results are characterized
by the values of two random variables x and y. We consider the two variables
as discrete, being able to take on the values x1, x2, . . ., xk; y1, y2, . . ., yℓ.
Continuous variables can be approximated by discrete ones by dividing their
range into intervals, as shown in Fig. 8.7. Let the number of times the result
x = xi and y = yj is observed be nij . One can arrange the numbers nij in a
matrix, called a contingency table (Table 8.4).
</p>
<p>Table8.4: Contingency table.
</p>
<p>y1 y2 . . . yℓ
</p>
<p>x1 n11 n12 . . . n1ℓ
x2 n21 n22 . . . n2ℓ
...
</p>
<p>...
...
</p>
<p>...
</p>
<p>xk nk1 nk2 . . . nkℓ
</p>
<p>We denote by pi the probability for x = xi to occur, and by qj the prob-
ability for y = yj . If the variables are independent, then the probability to
simultaneously observe x = xi and y = yj is equal to the product pi qj . The
maximum-likelihood estimators for p and q are
</p>
<p>p̃i =
1
</p>
<p>n
</p>
<p>ℓ&sum;
</p>
<p>j=1
nij , q̃j =
</p>
<p>1
</p>
<p>n
</p>
<p>k&sum;
</p>
<p>i=1
nij .</p>
<p/>
</div>
<div class="page"><p/>
<p>204 8 Testing Statistical Hypotheses
</p>
<p>Since
ℓ&sum;
</p>
<p>j=1
q̃j =
</p>
<p>k&sum;
</p>
<p>i=1
p̃i =
</p>
<p>1
</p>
<p>n
</p>
<p>ℓ&sum;
</p>
<p>j=1
</p>
<p>k&sum;
</p>
<p>i=1
nij = 1 ,
</p>
<p>one has k+ ℓ&minus; 2 independent estimators p̃i , q̃j . We can now organize the
elements of the contingency table into a single line,
</p>
<p>n11,n12, . . . ,n1ℓ,n21,n22, . . . ,n2ℓ, . . . ,nkℓ ,
</p>
<p>and carry out a χ2-test. For this we must compute the quantity
</p>
<p>X2 =
k&sum;
</p>
<p>i=1
</p>
<p>ℓ&sum;
</p>
<p>j=1
</p>
<p>(nij &minus;np̃i q̃j )2
np̃i q̃j
</p>
<p>(8.8.1)
</p>
<p>and compare it to the quantile χ21&minus;α of the χ
2-distribution corresponding to a
</p>
<p>given significance level α. The number of degrees of freedom is still obtained
from the number of intervals minus the number of estimated parameters mi-
nus one,
</p>
<p>f = kℓ&minus;1&minus; (k+ ℓ&minus;2)= (k&minus;1)(ℓ&minus;1) .
If the variables are not independent, then nij will not, in general, be near
np̃i q̃j , i.e., one will find
</p>
<p>X2 &gt; χ21&minus;α (8.8.2)
</p>
<p>and the hypothesis will be rejected.
</p>
<p>8.9 2&times;2 Table Test
</p>
<p>The simplest nontrivial contingency table has only two rows and two columns,
and is called a 2&times;2 table, as shown in Table 8.5. It is often used in medical
studies. (The variables x1 and x2 could represent, for example, two differ-
ent treatment methods, and y1 and y2 could represent success and failure of
the treatment. One wishes to determine whether success is independent of
the treatment.)
</p>
<p>Table8.5: 2&times;2 table.
</p>
<p>y1 y2
</p>
<p>x1 n11 = a n12 = b
x2 n21 = c n22 = d</p>
<p/>
</div>
<div class="page"><p/>
<p>8.10 Example Programs 205
</p>
<p>One computes the quantity X2 either according to (8.8.1) or from the
formula
</p>
<p>X2 = n(ad&minus;bc)
2
</p>
<p>(a+b)(c+d)(a+ c)(b+d) ,
</p>
<p>which is obtained by rearranging (8.8.1). If the variables x and y are inde-
pendent, then X2 follows a χ2-distribution with one degree of freedom. One
rejects the hypothesis of independence at the significance level α if
</p>
<p>X2 &gt; χ21&minus;α .
</p>
<p>In order for the quantity X2 to actually follow a χ2-distribution is again
necessary that the individual nij are sufficiently large, (and the hypothesis of
independence must be true).
</p>
<p>8.10 Example Programs
</p>
<p>The program performs a total of nexp simulated experiments. Each experiment consists
of the simulation of two samples of sizes N1 and N2 from normal distributions with
standard deviations σ1 and σ2. The variance of each of the samples is computed using
the class Sample. The sample variances are called s 2gand s
</p>
<p>2
k, so that
</p>
<p>s2g &gt; s
2
k .
</p>
<p>From the corresponding sample sizes the numbers of degrees of freedom fg =Ng&minus;1
and fk =Nk&minus;1 are computed. Finally, the ratio s2g/s2k is compared with the quantile
F1&minus;α/2(fg,fk) at a given confidence level β = 1&minus; α. If the ratio is larger than the
quantile, then the hypothesis of equal variances has to be rejected. The program asks
for the quantities nexp, N1, N2, σ1, σ2, and β. For each simulated experiment one line
of output is displayed.
</p>
<p>Suggestions: Choose nexp = 20 and β = 0.9. (a) For σ1 = σ2 you would expect
the hypothesis to be rejected in 2 out of 20 cases because of an error of the first kind.
Note the large statistical fluctuations, which obviously depend on N1 and N2, and
choose different pairs of values N1, N2 for σ1 = σ2. (b) Check the power of the test
for different variances σ1 �= σ2.
</p>
<p>This short program performs nexp simulation experiments. In each experiment a sam-
ple of size N is drawn from a normal distribution with mean x0 and width σ . Using
the class Sample the sample mean &macr;x and the sample variance s 2xare determined.
If λ0 is the population mean specified by the hypothesis, then the quantity
</p>
<p>Example Program 8.1: The class E1Test generates samples and tests
</p>
<p>the equality of their variances
</p>
<p>Example Program 8.2: The class E2Test generates samples and tests the
</p>
<p>equality of their means with a given value using Student&rsquo;s Test</p>
<p/>
</div>
<div class="page"><p/>
<p>206 8 Testing Statistical Hypotheses
</p>
<p>|t| = |x̄&minus;λ0|
&radic;
N
</p>
<p>sx
</p>
<p>can be used to test the hypothesis. At a given confidence level β = 1&minus;α the hypoth-
esis is rejected if
</p>
<p>|t|&gt; t1&minus;α/2 .
Here t1&minus;α/2 is the quantile of Student&rsquo;s distribution with f = N &minus; 1 degrees of
freedom. The program asks for the quantities nexp, N , x0, σ , λ0, and β. For each
simulated experiment one line of output is displayed.
</p>
<p>Suggestion: Modify the suggestions at the end of Sect. 8.1 to apply to Stu-
dent&rsquo;s test.
</p>
<p>are taken from a normal distribution with known parameters
For samples of size N the hypothesis H0 that they stem from a normal distribution
with mean a0 and standard deviation σ0 is tested. A total of nexp samples are drawn
in simulated experiments from a normally distributed population with mean a and
standard deviation σ . For each sample the quantity
</p>
<p>X2 =
N&sum;
</p>
<p>i=1
</p>
<p>(
xi &minus;a0
σ0
</p>
<p>)2
(8.10.1)
</p>
<p>is computed. Here xi are the elements of the sample. If a = a0 and σ = σ0, then the
quantity X2 follows a χ2-distribution for N degrees of freedom. This quantity can
therefore be used to perform a χ2-test on the hypothesis H0. The program does not,
however, perform the χ2-test, but rather it displays a histogram of the quantity X2 to-
gether with the χ2-distribution. One observes that for a= a0 and σ = σ0 the histogram
and χ2-distribution indeed coincide within statistical fluctuations. If, however, a �= a0
and/or σ �= σ0, then deviations appear. These deviations become particularly clear if
instead of X2 the quantity
</p>
<p>P(X2)= 1&minus;F(X2;N) (8.10.2)
is displayed. Here F(X2,N) is the distribution function (C.5.2) of the χ2-distribution
for N degrees of freedom. F(X2,N) is equal to the probability that a random variable
drawn from a χ2-distribution is smaller than X2. Thus, P is the probability that it is
greater than or equal to X2. If the hypothesis H0 is true, then F and therefore also
P follow uniform distributions between 0 and 1. If, however, H0 is false, then the
distribution of the X2 is not a χ2-distribution, and the distribution of the P is not a
uniform distribution. The test statistic X2 often (not completely correctly) is simply
called &ldquo;χ2&rdquo; and the quantity P is then called the &ldquo;χ2-probability&rdquo;. Large values of X2
</p>
<p>obviously signify that the terms in the sum (8.10.1) are on the average large compared
to unity, i.e., that the xi are significantly different from a0. For large values of X2,
however, P becomes small, cf. (8.10.2). Large values of &ldquo;χ2&rdquo; therefore correspond to
small values of the &ldquo;χ2-probability&rdquo;. The hypothesis H0 is rejected at the confidence
level β = 1&minus;α if X2 &gt; χ21&minus;α(N). That is equivalent to F(X2,N) &gt; β or P &lt; α.
</p>
<p>Example Program 8.3: The class E3Test generates samples
</p>
<p>and computes the test statistic χ2 for the hypothesis that the samples</p>
<p/>
</div>
<div class="page"><p/>
<p>8.10 Example Programs 207
</p>
<p>The program allows for interactive input of the quantities nexp, N , a, a0, σ , σ0
and displays the distributions of both X2 and P(X2) in the form of histograms.
</p>
<p>Suggestions: (a) Choose nexp = 1000; a = a0 = 0, σ = σ0 = 1 and for N = 1,
N = 2, and N = 10 display both X2 and P(X2). (b) Repeat (a), keeping a = 0 and
choosing a0 = 1 and a0 = 5. Explain the shift of the histogram for P(X2). (c) Repeat
(a), keeping σ = 1 fixed and choosing σ0 = 0.5 and σ0 = 2. Discuss the results. (d)
Modify the program so that instead of a0 and σ 20 , the sample mean x̄ sample variance
s2 are used for the computation of X2. The quantity X2 can be used for a χ2-test
of the hypothesis that the samples were drawn from a normal distribution. Display
histograms of X2 and P(X2) and show that X2 follows a χ2-distribution with N &minus; 2
degrees of freedom.</p>
<p/>
</div>
<div class="page"><p/>
<p>9. The Method of Least Squares
</p>
<p>The method of least squares was first developed by LEGENDRE and GAUSS.
In the simplest case it consists of the following prescription:
</p>
<p>The repeated measurements yj can be treated as the sum of
the (unknown) quantity x and the measurement error εj ,
</p>
<p>yj = x+ εj .
</p>
<p>The quantity x should be determined such that the sum of
squares of the errors εj is a minimum,
</p>
<p>&sum;
</p>
<p>j
</p>
<p>ε2j =
&sum;
</p>
<p>j
</p>
<p>(x&minus;yj )2 = min .
</p>
<p>We will see that in many cases this prescription can be derived as a result
of the principle of maximum likelihood, which historically was developed
much later, but that in other cases as well, it provides results with optimal
properties. The method of least squares, which is the most widely used of all
statistical methods, can also be used in the case where the measured quantities
yj are not directly related to the unknown x, but rather indirectly, i.e., as a lin-
ear (or also nonlinear) combination of several unknowns x1, x2, . . .. Because
of the great practical significance of the method we will illustrate the various
cases individually with examples before turning to the most general case.
</p>
<p>9.1 Direct Measurements of Equal or Unequal Accuracy
</p>
<p>The simplest case of direct measurements with equal accuracy has already
been mentioned. Suppose one has carried out n measurements of an unknown
quantity x. The measured quantities yj have measurement errors εj . We now
make the additional assumption that these are normally distributed about zero:
</p>
<p>S. Brandt, Data Analysis: Statistical and Computational Methods for Scientists and Engineers,
DOI 10.1007/978-3-319-03762-2__9, &copy; Springer International Publishing Switzerland 2014
</p>
<p>209</p>
<p/>
</div>
<div class="page"><p/>
<p>210 9 The Method of Least Squares
</p>
<p>yj = x+ εj , E(εj )= 0 , E(ε2j )= σ 2 . (9.1.1)
</p>
<p>This assumption can be justified in many cases by the Central Limit Theorem.
The probability to observe the value yj as the result of a single measurement
is thus proportional to
</p>
<p>fj dy =
1
</p>
<p>σ
&radic;
</p>
<p>2π
exp
</p>
<p>(
&minus;(yj &minus;x)
</p>
<p>2
</p>
<p>2σ 2
</p>
<p>)
dy .
</p>
<p>The log-likelihood function for all n measurements is thus (see Example 7.2)
</p>
<p>ℓ=&minus; 1
2σ 2
</p>
<p>n&sum;
</p>
<p>j=1
(yj &minus;x)2+ const . (9.1.2)
</p>
<p>The maximum likelihood condition,
</p>
<p>ℓ= max ,
</p>
<p>is thus equivalent to
</p>
<p>M =
n&sum;
</p>
<p>j=1
(yj &minus;x)2 =
</p>
<p>n&sum;
</p>
<p>j=1
ε2j = min . (9.1.3)
</p>
<p>This is exactly the least-squares prescription. As we have shown in Exam-
ples 7.2 and 7.6, this leads to the result that the best estimator for x is given
by the arithmetic mean of the yj
</p>
<p>x̃ = ȳ = 1
n
</p>
<p>n&sum;
</p>
<p>j=1
yj . (9.1.4)
</p>
<p>The variance of this estimator is
</p>
<p>σ 2(ȳ)= σ 2/n , (9.1.5)
</p>
<p>or, if we set the measurement errors and standard deviations equal,
</p>
<p>Δx̃ =Δy/
&radic;
n . (9.1.6)
</p>
<p>The more general case of direct measurements of different accuracy has
also already been treated in Example 7.6. Let us assume again a normal dis-
tribution centered about zero for the measurement errors, i.e.,
</p>
<p>yj = x+ εj , E(εj )= 0 , E(ε2j )= σ 2j = 1/gj . (9.1.7)</p>
<p/>
</div>
<div class="page"><p/>
<p>9.1 Direct Measurements of Equal or Unequal Accuracy 211
</p>
<p>Comparing with (7.2.7) gives the requirement of the maximum-likelihood
method
</p>
<p>M =
n&sum;
</p>
<p>j=1
</p>
<p>(yj &minus;x)2
</p>
<p>σ 2j
=
</p>
<p>n&sum;
</p>
<p>j=1
gj (yj &minus;x)2 =
</p>
<p>n&sum;
</p>
<p>j=1
gjε
</p>
<p>2
j = min . (9.1.8)
</p>
<p>The individual terms in the sum are now weighted with the inverse of the
variances. The best estimator for x is then [cf. (7.2.8)]
</p>
<p>x̃ =
&sum;n
</p>
<p>j=1 gjyj&sum;n
j=1 gj
</p>
<p>, (9.1.9)
</p>
<p>i.e., the weighted mean of the individual measurements. One sees that an
individual measurement thus contributes to the final result less when its mea-
surement error is greater. From (7.3.20) we know the variance of x̃; it is
</p>
<p>σ 2(x̃)=
</p>
<p>⎛
⎝
</p>
<p>n&sum;
</p>
<p>j=1
</p>
<p>1
</p>
<p>σ 2j
</p>
<p>⎞
⎠
</p>
<p>&minus;1
</p>
<p>=
</p>
<p>⎛
⎝
</p>
<p>n&sum;
</p>
<p>j=1
gj
</p>
<p>⎞
⎠
</p>
<p>&minus;1
</p>
<p>. (9.1.10)
</p>
<p>We can use the result (9.1.9) in order to compute the best estimates ε̃j of the
original measurement errors εj from (9.1.1) to obtain
</p>
<p>ε̃j = yj &minus; x̃ .
</p>
<p>We expect these quantities to be normally distributed about zero with the vari-
ance σ 2j . That is, the quantities ε̃j/σj should follow a standard Gaussian dis-
tribution. According to Sect. 6.6, the sum of squares
</p>
<p>M =
n&sum;
</p>
<p>j=1
</p>
<p>(
ε̃j
</p>
<p>σj
</p>
<p>)2
=
</p>
<p>n&sum;
</p>
<p>j=1
</p>
<p>(yj &minus; x̃)2
</p>
<p>σ 2j
=
</p>
<p>n&sum;
</p>
<p>j=1
gj (yj &minus; x̃)2 (9.1.11)
</p>
<p>then follows a χ2-distribution with n&minus;1 degrees of freedom.
This property of the quantity M can now be used to carry out a χ2-test
</p>
<p>on the validity of the assumption (9.1.7). If, for a given significance level
α, the quantity M exceeds the value χ21&minus;α, then we would have to recheck
the assumption (9.1.7). Usually one does not doubt that the yj are in fact
measurements of the unknown x. It may be, however, that the errors εj are
not normally distributed. In particular, the measurements may also be biased,
i.e., the expectation value of the errors εj may be different from zero. The
presence of such systematic errors can often be inferred from the failure of
the χ2-test.</p>
<p/>
</div>
<div class="page"><p/>
<p>212 9 The Method of Least Squares
</p>
<p>Example 9.1: Weighted mean of measurements of different accuracy
</p>
<p>The best values for constants of fundamental significance, such as impor-
tant constants of nature, are usually obtained as weighted averages of mea-
surements obtained by different experimental groups. For the properties of
elementary particles, such mean values are compiled at regular intervals.
We will consider as an example somewhat older measurements of the mass of
the neutral K-meson (K0), taken from such a compilation from 1967 [10].
An average was computed from the results of four experiments, all car-
ried out with different techniques. The calculation can be carried out fol-
lowing the scheme of Table 9.1. The resulting value of M is 7.2. If we
choose a significance level of 5%, we find from Table I.7 for three degrees
of freedom χ20.95 = 7.82. At the time of the averaging, one could therefore
assume that the result mK0 = (497.9&plusmn; 0.2)MeV represented the best value
for the mass of the K-meson, as long as no further experiments were carried
out. (More than 40 years later the weighted mean of all measurements was
mK0 = (497.614&plusmn;0.024)MeV [11]).
</p>
<p>Table9.1: Construction of the weighted mean from four measurements of the mass of the
neutral K meson. The yj are the measured values in MeV.
</p>
<p>j yj σj 1/σ 2j = gj yj gj yj &minus; x̃ (yj &minus; x̃)2gj
1 498.1 0.4 6.3 3038.0 0.2 0.3
2 497.44 0.33 10 4974.4 &minus;0.46 2.1
3 498.9 0.5 4 1995.6 1.0 4.0
4 497.44 0.5 4 1989.8 &minus;0.46 0.8&sum;
</p>
<p>24.3 11 997.8 7.2
</p>
<p>x̃ =
&sum;
</p>
<p>yjgj/
&sum;
</p>
<p>gj = 497.9 , Δx̃ = (
&sum;
</p>
<p>gj )
&minus; 12 = 0.20
</p>
<p>Let us now consider the case where the χ2-test fails. As mentioned above,
one usually assumes that at least one of the measurements has a systematic
error. By investigation of the individual measurements, one can sometimes
determine that one or two measurements deviate from the others by a large
amount. Such a case is illustrated in Fig. 9.1a, where several different mea-
surements are shown with their errors. (The measured value is plotted along
the vertical axis; the horizontal axis merely distinguishes between the differ-
ent measurements.) Although the χ2-test would fail if all measurements from
Fig. 9.1a are used, this is not the case if measurements 4 and 6 are excluded.
</p>
<p>Unfortunately the situation is not always so clear. Figure 9.1b shows a
further example where the χ2-test fails. (According to Chap. 8 one would
reject the hypothesis that the measurements are determinations of the same
quantity.) There is no single measurement, however, that is responsible for</p>
<p/>
</div>
<div class="page"><p/>
<p>9.1 Direct Measurements of Equal or Unequal Accuracy 213
</p>
<p>Fig.9.1: Averaging of 10 measurements where the χ2-test fails. (a) Anomalous deviation of
certain measurements: (1) Averaging of all measurements; (2) averaging without y4 and y6.
(b) Errors of individual measurements clearly too small: (1) Error of the mean according to
(9.1.10); (2) error of the mean according to (9.1.13).
</p>
<p>this fact. It would now be mathematically correct to not give any average
value at all, and to make no statement about a best value, as long as no fur-
ther measurements are available. In practice, this is clearly not satisfactory.
ROSENFELD et al. [10] have suggested that the individual measurement er-
rors should be increased by a scale factor
</p>
<p>&radic;
M/(n&minus;1), i.e., one replaces the
</p>
<p>σj by
</p>
<p>σ &prime;j = σj
&radic;
</p>
<p>M
</p>
<p>n&minus;1 . (9.1.12)
</p>
<p>The weighted mean x̃ obtained by using these measurement errors does not
differ from that of expression (9.1.9). The variance, however, is different
from (9.1.10). It becomes
</p>
<p>σ &prime;2(x̃)= M
n&minus;1
</p>
<p>⎛
⎝
</p>
<p>n&sum;
</p>
<p>j=1
</p>
<p>1
</p>
<p>σ 2j
</p>
<p>⎞
⎠
</p>
<p>&minus;1
</p>
<p>. (9.1.13)
</p>
<p>We now compute the analogous expression to (9.1.11),</p>
<p/>
</div>
<div class="page"><p/>
<p>214 9 The Method of Least Squares
</p>
<p>M &prime; = n&minus;1
M
</p>
<p>n&sum;
</p>
<p>j=1
</p>
<p>(yj &minus; x̃)2
</p>
<p>σ 2j
= n&minus;1
</p>
<p>M
M = n&minus;1 . (9.1.14)
</p>
<p>This is the expectation value of χ2 for n&minus; 1 degrees of freedom. Equation
(9.1.14) clearly provided the motivation for relation (9.1.12). We repeat that
this relation has no rigorous mathematical basis. It should be used with cau-
tion, since it hides the influence of systematic errors. On the other hand it
provides reasonable errors for the mean for cases such as those in Fig. 9.1b,
while direct application of Eq. (9.1.10) leads to an error that is far to small to
reflect the actual dispersion of the individual measurements about the mean.
Both solutions for the error of the mean are shown in Fig. 9.1b.
</p>
<p>9.2 Indirect Measurements: Linear Case
</p>
<p>Let us now consider the more general case of several unknown quantities
xi (i = 1,2, . . . , r). The unknowns are often not measured directly. Instead,
only a set of linear functions of the xi are measurable,
</p>
<p>ηj = pj0 +pj1x1 +pj2x2 +&middot;&middot; &middot;+pjrxr . (9.2.1)
</p>
<p>We now write this relation in a somewhat different form,
</p>
<p>fj = ηj +aj0 +aj1x1 +aj2x2 +&middot;&middot; &middot;+ajrxr = 0 . (9.2.2)
</p>
<p>We define a column vector,
</p>
<p>aj =
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>aj1
aj2
...
</p>
<p>ajr
</p>
<p>⎞
⎟⎟⎟⎠ , (9.2.3)
</p>
<p>and write (9.2.2) in the more compact form,
</p>
<p>fj = ηj +aj0 +aTj x= 0 , j = 1,2, . . . ,n . (9.2.4)
</p>
<p>If we define in addition
</p>
<p>η=
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>η1
η2
...
</p>
<p>ηn
</p>
<p>⎞
⎟⎟⎟⎠ , a0 =
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>a10
a20
...
</p>
<p>an0
</p>
<p>⎞
⎟⎟⎟⎠ , A=
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>a11 a12 &middot; &middot; &middot; a1r
a21 a22 &middot; &middot; &middot; a2r
...
</p>
<p>an1 an2 &middot; &middot; &middot; anr
</p>
<p>⎞
⎟⎟⎟⎠ , (9.2.5)</p>
<p/>
</div>
<div class="page"><p/>
<p>9.2 Indirect Measurements: Linear Case 215
</p>
<p>then the system of equations (9.2.4) can be written as a matrix equation,
</p>
<p>f= η+a0 +Ax= 0 . (9.2.6)
</p>
<p>Of course the measured quantities still have measurement errors εj , which we
assume to be normally distributed. We then have&lowast;
</p>
<p>yj = ηj + εj ,
E(εj ) = 0 , (9.2.7)
E(ε2j ) = σ 2j = 1/gj .
</p>
<p>Since the yj are independent measurements, we can arrange the variances
σ 2j in a diagonal covariance matrix for yj or εj ,
</p>
<p>Cy = Cε =
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>σ 21 0
σ 22
</p>
<p>. . .
</p>
<p>0 σ 2n
</p>
<p>⎞
⎟⎟⎟⎠ . (9.2.8)
</p>
<p>In analogy to (9.1.7) we call the inverse of the covariance matrix a weight
matrix,
</p>
<p>Gy =Gε = C&minus;1y = C&minus;1ε =
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>g1 0
g2
</p>
<p>. . .
</p>
<p>0 gn
</p>
<p>⎞
⎟⎟⎟⎠ . (9.2.9)
</p>
<p>If we now put the measurements and errors together into vectors, we obtain
from (9.2.7)
</p>
<p>y= η+ ε . (9.2.10)
</p>
<p>From (9.2.6) one then has
</p>
<p>y&minus; ε+a0 +Ax= 0 . (9.2.11)
</p>
<p>We want to solve this system of equations for the unknowns x with the
maximum-likelihood method. With our assumption (9.2.7), the measurements
yj are normally distributed with the probability density
</p>
<p>f (yj )=
1
</p>
<p>σj
&radic;
</p>
<p>2π
exp
</p>
<p>(
&minus;(yj &minus;ηj )
</p>
<p>2
</p>
<p>2σ 2j
</p>
<p>)
= 1
</p>
<p>σj
&radic;
</p>
<p>2π
exp
</p>
<p>(
&minus;
</p>
<p>ε2j
</p>
<p>2σ 2j
</p>
<p>)
. (9.2.12)
</p>
<p>&lowast;For simplicity of notation we no longer write random variables with a special character
type. From context it will always be evident which variables are random.</p>
<p/>
</div>
<div class="page"><p/>
<p>216 9 The Method of Least Squares
</p>
<p>Thus for all measurements one obtains likelihood functions
</p>
<p>L=
n&prod;
</p>
<p>j=1
f (yj )= (2π)&minus;
</p>
<p>1
2n
</p>
<p>⎛
⎝
</p>
<p>n&prod;
</p>
<p>j=1
σ&minus;1j
</p>
<p>⎞
⎠exp
</p>
<p>⎛
⎝&minus;1
</p>
<p>2
</p>
<p>n&sum;
</p>
<p>j=1
</p>
<p>ε2j
</p>
<p>σ 2j
</p>
<p>⎞
⎠ , (9.2.13)
</p>
<p>ℓ= lnL=&minus;12n ln2π+ ln
</p>
<p>⎛
⎝
</p>
<p>n&prod;
</p>
<p>j=1
σ&minus;1j
</p>
<p>⎞
⎠&minus; 1
</p>
<p>2
</p>
<p>n&sum;
</p>
<p>j=1
</p>
<p>ε2j
</p>
<p>σ 2j
. (9.2.14)
</p>
<p>This expression is clearly a maximum when
</p>
<p>M =
n&sum;
</p>
<p>j=1
</p>
<p>ε2j
</p>
<p>σ 2j
=
</p>
<p>n&sum;
</p>
<p>j=1
</p>
<p>(yj +aTj x+aj0)2
</p>
<p>σ 2j
= min . (9.2.15)
</p>
<p>Using (9.2.9) and (9.2.11), we can rewrite this expression as
</p>
<p>M = εTGyε= min (9.2.16)
or
</p>
<p>M = (y+a0 +Ax)TGy(y+a0 +Ax)= min , (9.2.17)
or in abbreviated form
</p>
<p>c= y+a0 , (9.2.18)
M = (c+Ax)TGy(c+Ax)= min . (9.2.19)
</p>
<p>We will simplify this expression further by using the Cholesky decomposition
(cf. Sect. A.9) of the positive-definite symmetric weight matrix Gy ,
</p>
<p>Gy =HTH . (9.2.20)
In the frequently occurring case of uncorrelated measurements (9.2.9) one has
</p>
<p>H =HT =
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>1/σ1 0
1/σ2
</p>
<p>. . .
</p>
<p>0 1/σn
</p>
<p>⎞
⎟⎟⎟⎠ . (9.2.21)
</p>
<p>Using the notation
</p>
<p>c&prime; =Hc , A&prime; =HA (9.2.22)
Eq. (9.2.19) takes on the simple form
</p>
<p>M = (A&prime;x+ c&prime;)2 = min . (9.2.23)
The method for solving this equation for x is described in detail in Ap-
</p>
<p>pendix A, in particular in Sects. A.5 through A.14. The solution can be written
in the form
</p>
<p>x̃=&minus;A&prime;+c&prime; . (9.2.24)</p>
<p/>
</div>
<div class="page"><p/>
<p>9.2 Indirect Measurements: Linear Case 217
</p>
<p>[See (A.10.3).] Here A&prime;+ is the pseudo-inverse of the matrix A&prime; (see
Sect. A.10). In Sect. A.14, a solution of the form of (9.2.24) is indeed found
with the help of the singular value decomposition of the matrix A&prime;. This pro-
cedure is particularly accurate numerically.
</p>
<p>To compute by hand one uses instead of Eq. (9.2.24) the mathematically
equivalent expression for the solution of the normal equations [see (A.5.17)],
</p>
<p>x̃=&minus;(A&prime;TA&prime;)&minus;1A&prime;Tc&prime; (9.2.25)
</p>
<p>or, with (9.2.22) in terms of the quantities c and A,
</p>
<p>x̃=&minus;(ATGyA)&minus;1ATGy c . (9.2.26)
</p>
<p>The solution includes, of course, the special case of Sect. 9.1. In the case of
direct measurements of different accuracy, x has only one element, a0 van-
ishes, and A is simply an n-component column vector whose elements are all
equal to &minus;1. One then has
</p>
<p>c&prime; =
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>y1/σ1
y2/σ2
</p>
<p>...
</p>
<p>yn/σn
</p>
<p>⎞
⎟⎟⎟⎠ , A
</p>
<p>&prime; =
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>&minus;1/σ1
&minus;1/σ2
</p>
<p>...
</p>
<p>&minus;1/σn
</p>
<p>⎞
⎟⎟⎟⎠ , A
</p>
<p>&prime;TA=
n&sum;
</p>
<p>j=1
</p>
<p>1
</p>
<p>σ 2j
,
</p>
<p>and (9.2.25) becomes
</p>
<p>x̃=
</p>
<p>⎛
⎝
</p>
<p>n&sum;
</p>
<p>j=1
</p>
<p>1
</p>
<p>σ 2j
</p>
<p>⎞
⎠
</p>
<p>&minus;1
n&sum;
</p>
<p>j=1
</p>
<p>yj
</p>
<p>σ 2j
,
</p>
<p>which is identical to (9.1.9).
The solution of (9.2.26) represents a linear relation between the solution
</p>
<p>vector x̃ and the vector of measurements y, since c = y+ a0. We can thus
apply the error propagation techniques of Sect. 3.8. Using (3.8.2) and (3.8.4)
one immediately obtains
</p>
<p>Cx =G&minus;1x = [(ATGyA)&minus;1ATGy]G&minus;1y [(ATGyA)&minus;1ATGy]T .
</p>
<p>The matrices Gy , G&minus;1y , and (A
TGyA) are symmetric, i.e., they are identical to
</p>
<p>their transposed matrices. Using the rule (A.1.8), this expression simplifies to
</p>
<p>G&minus;1
x̃
</p>
<p>= (ATGyA)&minus;1ATGyG&minus;1y GyA(ATGyA)&minus;1
</p>
<p>= (ATGyA)&minus;1(ATGyA)(ATGyA)&minus;1 ,
G&minus;1
</p>
<p>x̃
= (ATGyA)&minus;1 = (A&prime;TA&prime;)&minus;1 . (9.2.27)</p>
<p/>
</div>
<div class="page"><p/>
<p>218 9 The Method of Least Squares
</p>
<p>We have thus obtained a simple expression for the covariance matrix of the
estimators x̃ for the unknowns x. The square roots of the diagonal elements of
this matrix can be viewed as &ldquo;measurement errors&rdquo;, although the quantities x
were not directly measured.
</p>
<p>We can also use the result (9.2.26) to improve the original measure-
ments y. By substituting (9.2.26) into (9.2.11) one obtains a vector of esti-
mators of the measurement errors ε,
</p>
<p>ε̃= Ax̃+ c=&minus;A(ATGyA)&minus;1ATGyc+ c . (9.2.28)
</p>
<p>These measurement errors can now be used to compute improved measured
values,
</p>
<p>η̃= y&minus; ε̃ = y+A(ATGyA)&minus;1ATGyc&minus; c ,
η̃ = A(ATGyA)&minus;1ATGyc&minus;a0 . (9.2.29)
</p>
<p>The η̃ are again linear in y. We can again use error propagation to determine
the covariance matrix of the improved measurements,
</p>
<p>G&minus;1
η̃
</p>
<p>= [A(ATGyA)&minus;1ATGy]G&minus;1y [A(ATGyA)&minus;1ATGy]T ,
G&minus;1
</p>
<p>η̃
= A(ATGyA)&minus;1AT = AG&minus;1x̃ A
</p>
<p>T . (9.2.30)
</p>
<p>The improved measurements η̃ satisfy (9.2.1) if the unknowns are replaced by
their estimators x̃.
</p>
<p>9.3 Fitting a Straight Line
</p>
<p>We will examine in detail the simple but in practice frequently occurring task
of fitting a straight line to a set of measurements yj at various values tj of a
so-called controlled variable t . The values of these variables will be assumed
to be known exactly, i.e., without error. The variable ti could be, for example,
the time at which an observation yi is made, or a temperature or voltage that is
set in an experiment. (If t also has an error, then fitting a line in the (t,y) plane
becomes a nonlinear problem. It will be treated in Sect. 9.10, Example 9.11.)
</p>
<p>In the present case the relation (9.2.1) has the simple form
</p>
<p>ηj = yj &minus; εj = x1 +x2tj
</p>
<p>or using vector notation,
</p>
<p>η&minus;x1 &minus;x2t= 0 .</p>
<p/>
</div>
<div class="page"><p/>
<p>9.3 Fitting a Straight Line 219
</p>
<p>We will attempt to determine the unknown parameters
</p>
<p>x=
(
x1
</p>
<p>x2
</p>
<p>)
</p>
<p>from the measurements in Table 9.2.
A comparison of our problem to Eqs. (9.2.2) through (9.2.6) gives a0 = 0,
</p>
<p>A=&minus;
</p>
<p>⎛
⎜⎜⎝
</p>
<p>1 t1
1 t2
1 t3
1 t4
</p>
<p>⎞
⎟⎟⎠=&minus;
</p>
<p>⎛
⎜⎜⎝
</p>
<p>1 0
1 1
1 2
1 3
</p>
<p>⎞
⎟⎟⎠ , y= c=
</p>
<p>⎛
⎜⎜⎝
</p>
<p>1.4
1.5
3.7
4.1
</p>
<p>⎞
⎟⎟⎠ .
</p>
<p>The matrices Gy and H are found by substitution of the last line of Table 9.2
into (9.2.9) and (9.2.21),
</p>
<p>Gy =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>4
25
</p>
<p>1
4
</p>
<p>⎞
⎟⎟⎠ , H =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>2
5
</p>
<p>1
2
</p>
<p>⎞
⎟⎟⎠ .
</p>
<p>One thus has
</p>
<p>A&prime; =&minus;
</p>
<p>⎛
⎜⎜⎝
</p>
<p>2 0
5 5
1 2
2 6
</p>
<p>⎞
⎟⎟⎠ , c
</p>
<p>&prime; =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>2.8
7.5
3.7
8.2
</p>
<p>⎞
⎟⎟⎠ , A
</p>
<p>&prime;Tc&prime; =&minus;
(
</p>
<p>63.2
</p>
<p>94.1
</p>
<p>)
,
</p>
<p>(A&prime;TA&prime;)&minus;1 =
(
</p>
<p>34 39
39 65
</p>
<p>)&minus;1
</p>
<p>= 1
689
</p>
<p>(
65 &minus;39
</p>
<p>&minus;39 34
</p>
<p>)
=
</p>
<p>(
0.0943 &minus;0.0556
</p>
<p>&minus;0.0566 0.0493
</p>
<p>)
.
</p>
<p>To invert the 2&times;2 matrix we use (A.6.8). The solution (9.2.25) is then
</p>
<p>x̃=
(
</p>
<p>0.0943 &minus;0.0566
&minus;0.0566 0.0493
</p>
<p>)(
63.2
</p>
<p>94.1
</p>
<p>)
=
</p>
<p>(
0.636
</p>
<p>1.066
</p>
<p>)
.
</p>
<p>The covariance matrix of for x̃ is
</p>
<p>Cx̃ =G&minus;1x̃ = (A
&prime;TA&prime;)&minus;1 =
</p>
<p>(
0.0943 &minus;0.0566
</p>
<p>&minus;0.0566 0.0494
</p>
<p>)
.
</p>
<p>Its diagonal elements are the variances of x̃1 and x̃2, and their square roots are
the errors,
</p>
<p>Δx̃1 = 0.307 , Δx̃2 = 0.222 .</p>
<p/>
</div>
<div class="page"><p/>
<p>220 9 The Method of Least Squares
</p>
<p>Table9.2: Data for fitting a straight line.
</p>
<p>j 1 2 3 4
tj 0.0 1.0 2.0 3.0
yj 1.4 1.5 3.7 4.1
σj 0.5 0.2 1.0 0.5
</p>
<p>For the correlation coefficient between x̃1 and x̃2 one finds
</p>
<p>ρ = &minus;0.0566
0.307 &middot;0.222 =&minus;0.830 .
</p>
<p>The improved measurements are
</p>
<p>η̃=&minus;Ax̃=
</p>
<p>⎛
⎜⎜⎝
</p>
<p>1 0
1 1
1 2
1 3
</p>
<p>⎞
⎟⎟⎠
(
</p>
<p>0.636
</p>
<p>1.066
</p>
<p>)
=
</p>
<p>⎛
⎜⎜⎝
</p>
<p>0.636
1.702
2.768
3.834
</p>
<p>⎞
⎟⎟⎠ .
</p>
<p>They lie on a line given by η̃ = &minus;Ax̃, which of course is different in gen-
eral from the &ldquo;true&rdquo; solution. The &ldquo;residual errors&rdquo; of η̃ can be obtained
with (9.2.30),
</p>
<p>G&minus;1
η̃
</p>
<p>= Cη̃ =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>1 0
1 1
1 2
1 3
</p>
<p>⎞
⎟⎟⎠
(
</p>
<p>0.0943 &minus;0.0566
&minus;0.0566 0.0493
</p>
<p>)(
1 1 1 1
0 1 2 3
</p>
<p>)
</p>
<p>=
</p>
<p>⎛
⎜⎜⎝
</p>
<p>0.0943 0.0377 &minus;0.0189 &minus;0.0755
0.0377 0.0305 0.0232 0.0160
</p>
<p>&minus;0.0189 0.0232 0.0653 0.1074
&minus;0.0755 0.0160 0.1074 0.1988
</p>
<p>⎞
⎟⎟⎠ .
</p>
<p>The square roots of the diagonal elements are
</p>
<p>Δη̃1 = 0.31 , Δη̃2 = 0.17 , Δη̃3 = 0.26 , Δη̃4 = 0.45 .
</p>
<p>The fit procedure, in which more measurements (four) were used than were
necessary to determine the two unknowns, has noticeably reduced the indi-
vidual errors of the measurements in comparison to the original values σj .
</p>
<p>Finally we will compute the value of the minimum function (9.2.16),
</p>
<p>M = ε̃TGy ε̃= (y&minus; η̃)TGy(y&minus; η̃)=
</p>
<p>⎛
⎝
</p>
<p>n&sum;
</p>
<p>j=1
</p>
<p>yj &minus; η̃j
σj
</p>
<p>⎞
⎠
</p>
<p>2
</p>
<p>= 4.507 .</p>
<p/>
</div>
<div class="page"><p/>
<p>9.3 Fitting a Straight Line 221
</p>
<p>With this result we can carry out a χ2-test on the goodness-of-fit of a straight
line to our data. Since we started with n= 4 measured values and have deter-
mined r = 2 unknown parameters, we still have n&minus; r = 2 degrees of freedom
available (cf. Sect. 9.7). If we choose a significance level of 5%, we find from
Table I.7 for two degrees of freedom χ20.95 = 5.99. There is thus no reason to
reject the hypothesis of a straight line.
</p>
<p>The results of the fit are shown in Fig. 9.2. The measurements yj are
shown as functions of the variables t . The vertical bars give the measure-
ment errors. They cover the range yj &plusmn;σj . The plotted line corresponds to the
result x̃1, x̃2. The improved measurements lie on this line. They are shown
in Fig. 9.2b together with the residual errors Δη̃j . In order to illustrate the
accuracy of the estimates x̃1, x̃2, we consider the covariance matrix Cx̃ . It
determines a covariance ellipse (Sect. 5.10) in a plane spanned by the vari-
ables x1, x2. This ellipse is shown in Fig. 9.2c. Points on the ellipse cor-
respond to fits of equal probability. Each of these points determines a line
in the (t,y) plane. Some of the points are indicated in Fig. 9.2c and the
corresponding lines are plotted in Fig. 9.2d. The points on the covariance
ellipse thus correspond to a bundle of lines. The line determined by the &ldquo;true&rdquo;
</p>
<p>Fig.9.2: Fit of a straight line to data from Table 9.2. (a) Original measured values and er-
rors; (b) improved measurement values and residual errors; (c) covariance ellipse for the
fitted quantities x1, x2; (d) various lines corresponding to individual points on the covariance
ellipse.</p>
<p/>
</div>
<div class="page"><p/>
<p>222 9 The Method of Least Squares
</p>
<p>values of the unknowns lies in this bundle with the probability 1 &minus; e&minus;1/2;
cf. (5.10.18).
</p>
<p>9.4 Algorithms for Fitting Linear Functions
</p>
<p>of the Unknowns
</p>
<p>The starting point of the ideas in Sect. 9.2 was the assumption (9.2.6) of a
linear relationship between the &ldquo;true&rdquo; values η of the measured quantities y
and the unknowns x. We will write this relation in the form
</p>
<p>η= h(x)=&minus;a0 &minus;Ax , (9.4.1)
</p>
<p>or in terms of components,
</p>
<p>ηj = hj (x)=&minus;a0j &minus;Aj1x1 &minus;Aj2x2 &minus;&middot;&middot; &middot;&minus;Ajrxr . (9.4.2)
</p>
<p>Often it is useful to consider the index j as specifying that the measurement
yj corresponds to a value tj of a controlled variable, which is taken to be
known without error. Then (9.4.2) can be written in the form
</p>
<p>ηj = h(x, tj ) . (9.4.3)
</p>
<p>This relation describes a curve in the (t,η) plane. It is characterized by the
parameters x. The determination of the parameters is thus equivalent to fitting
a curve to the measurements yj = y(tj ). Usually the individual measurements
yj are uncorrelated. The weight matrix Gy is diagonal; it has the Cholesky
decomposition (9.2.20). One then has simply
</p>
<p>A&prime;jk = Ajk/σj , c&prime;j = c/σj ,
</p>
<p>see (9.2.22).
</p>
<p>9.4.1 Fitting a Polynomial
</p>
<p>As a particularly simple but useful example of a function for (9.4.3), let us
consider the relation
</p>
<p>ηj = hj = x1 +x2tj +x3t2j +&middot;&middot; &middot;+xr t r&minus;1j . (9.4.4)
</p>
<p>This is a polynomial in tj , but is linear in the unknowns x. The special case
r = 2 has been treated in detail in Sect. 9.3.</p>
<p/>
</div>
<div class="page"><p/>
<p>9.4 Algorithms for Fitting Linear Functionsof the Unknowns 223
</p>
<p>A comparison of (9.4.4) with (9.4.2) yields directly
</p>
<p>a0j = 0 , Ajℓ =&minus;tℓ&minus;1j
or more completely,
</p>
<p>a0 =
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>0
0
...
</p>
<p>0
</p>
<p>⎞
⎟⎟⎟⎠ , A=&minus;
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>1 t1 t21 . . . t
r&minus;1
1
</p>
<p>1 t2 t22 . . . t
r&minus;1
2
</p>
<p>...
</p>
<p>1 tn t2n . . . t
r&minus;1
n
</p>
<p>⎞
⎟⎟⎟⎠ .
</p>
<p>The class LsqPol performs the fit of a polynomial to data.
</p>
<p>Example 9.2: Fitting of various polynomials
</p>
<p>It is often interesting to fit polynomials of various orders to measured data, as
long as the number of degrees of freedom of the fit is at least one, i.e., n &gt;
ℓ+1. As a numerical example we will use measurements from an elementary
particle physics experiment. Consider an investigation of elastic scattering of
negative K mesons on protons with a fixed K meson energy. The distribution
of the cosine of the scattering angle Θ in the center-of-mass system of the
collision is characteristic of the angular momentum of possible intermediate
states in the collision process. If, in particular, the distribution is considered as
a polynomial in cosΘ , the order of the polynomial can be used to determine
the spin quantum numbers of such intermediate states.
</p>
<p>The measured values yj (j = 1,2, . . . ,10) are simply the numbers of col-
lisions for which cosΘ was observed in a small interval around tj = cosΘj .
As measurement errors the statistical errors were used, i.e., the square roots
of the number of observations. The data are given in Table 9.3. The results
of the fit of polynomials of various orders are summarized in Table 9.4 and
Fig. 9.3. With the χ2-test we can check successively whether a polynomial of
order zero, one, . . . gives a good fit to the data.
</p>
<p>We see that the first two hypotheses (a constant and a straight line) are
not in agreement with the experimental data. This can be seen in Fig. 9.3 and
is also reflected in the values of the minimum function. The hypothesis r = 3,
a second-order polynomial, gives qualitative agreement. Most of the measure-
ments, however, do not fall on the fitted parabola within the error bars. The
χ2-test fails with a significance level of 0.0001. For the hypotheses r = 4, 5,
and 6, however, the agreement is very good. The fitted curves go through the
error bars and are almost identical. The χ2-test does not call for a rejection of
the hypothesis even at α = 0.5. We can therefore conclude that a third order
polynomial is sufficient to describe the data. An even more careful investiga-
tion of the question as to what order a polynomial must be used to describe
the data is possible with orthogonal polynomials; cf. Sect. 12.1.</p>
<p/>
</div>
<div class="page"><p/>
<p>224 9 The Method of Least Squares
</p>
<p>Table9.3: Data for Example 9.2. One has σj =&radic;yj .
</p>
<p>j tj = cosΘj yj
1 &minus;0.9 81
2 &minus;0.7 50
3 &minus;0.5 35
4 &minus;0.3 27
5 &minus;0.1 26
6 0.1 60
7 0.3 106
8 0.5 189
9 0.7 318
</p>
<p>10 0.9 520
</p>
<p>Table9.4: Summary of results from Example 9.2 (n = 10 measured points, r parameters,
f = n&minus; r degrees of freedom).
</p>
<p>r x̃1 x̃2 x̃3 x̃4 x̃5 x̃6 f M
</p>
<p>1 57.85 9 833.55
2 82.66 99.10 8 585.45
3 47.27 185.96 273.61 7 36.41
4 37.94 126.55 312.02 137.59 6 2.85
5 39.62 119.10 276.49 151.91 52.60 5 1.68
6 39.88 121.39 273.19 136.58 56.90 16.72 4 1.66
</p>
<p>9.4.2 Fit of an Arbitrary Linear Function
</p>
<p>The matrix A and the vector c enter into the solution of the problem of
Sect. 9.2. They depend on the form of the function to be fitted and must there-
fore be provided by the user. (In Sect. 9.4.1 the function was known, so the
user did not have to worry about computing A and c.) The class LsqLin
performs the fit of an arbitrary linear function to data.
</p>
<p>Example 9.3: Fitting a proportional relation
</p>
<p>Suppose from the construction of an experiment it is known that the true value
ηj of the measurement yj is directly proportional to the value of the controlled
variable tj :
</p>
<p>ηj = x1tj .</p>
<p/>
</div>
<div class="page"><p/>
<p>9.4 Algorithms for Fitting Linear Functionsof the Unknowns 225
</p>
<p>Fig.9.3: Fit of polynomials of various orders (0,1, . . . ,5) to the data from Example 9.2.
</p>
<p>The constant of proportionality x1 is to be determined from the measurements.
This relation is simpler than a first-order polynomial, which contains two con-
stants. A comparison with (9.4.2) gives
</p>
<p>a0j = 0 , Aj1 =&minus;tj
</p>
<p>and thus
</p>
<p>c=&minus;
</p>
<p>⎛
⎜⎝
</p>
<p>y1
...
</p>
<p>yn
</p>
<p>⎞
⎟⎠ , A=&minus;
</p>
<p>⎛
⎜⎝
</p>
<p>t1
...
</p>
<p>tn
</p>
<p>⎞
⎟⎠ .
</p>
<p>Shown in Fig. 9.4 are the results of the fit of a line through the origin,
i.e., a proportionality, and the fit of a first-order polynomial. The value of the</p>
<p/>
</div>
<div class="page"><p/>
<p>226 9 The Method of Least Squares
</p>
<p>Fig.9.4: Fit of a proportional relation (above) and a first-order polynomial (below) to data.
</p>
<p>minimum function is clearly smaller in the case of the general line, and the fit
is visibly better. The number of degrees of freedom, however, is less, and the
desired constant of proportionality is not determined.
</p>
<p>9.5 Indirect Measurements: Nonlinear Case
</p>
<p>If the relation between the n-vector η of true values of the measured quantities
y and the r-vector of unknowns is given by a function,
</p>
<p>η= h(x) ,</p>
<p/>
</div>
<div class="page"><p/>
<p>9.5 Indirect Measurements: Nonlinear Case 227
</p>
<p>that is not &ndash; as (9.4.1) &ndash; linear in x, then our previous procedure fails in the
determination of the unknowns.
</p>
<p>Instead of (9.2.2) one has
</p>
<p>fj (x,η)= ηj &minus;hj (x)= 0 , (9.5.1)
</p>
<p>or in vector notation,
f(x,η)= 0 . (9.5.2)
</p>
<p>We can, however, relate this situation to the linear case if we expand the
fj in a Taylor series and keep only the first term. We carry out the expansion
about the point x0 = (x10,x20, . . . ,xr0), which is a first approximation for the
unknowns, which has been obtained in some way,
</p>
<p>fj (x,η)= fj (x0,η)+
(
&part;fj
</p>
<p>&part;x1
</p>
<p>)
</p>
<p>x0
</p>
<p>(x1 &minus;x10)+&middot;&middot; &middot;+
(
&part;fj
</p>
<p>&part;xr
</p>
<p>)
</p>
<p>x0
</p>
<p>(xr &minus;xr0) .
</p>
<p>(9.5.3)
If we now define
</p>
<p>ξ = x&minus;x0 =
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>x1 &minus;x10
x2 &minus;x20
</p>
<p>...
</p>
<p>xr &minus;xr0
</p>
<p>⎞
⎟⎟⎟⎠ , (9.5.4)
</p>
<p>ajℓ =
(
&part;fj
</p>
<p>&part;xℓ
</p>
<p>)
</p>
<p>x0
</p>
<p>=&minus;
(
&part;hj
</p>
<p>&part;xℓ
</p>
<p>)
</p>
<p>x0
</p>
<p>, A=
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>a11 a12 &middot; &middot; &middot; a1r
a21 a22 &middot; &middot; &middot; a2r
...
</p>
<p>an1 an2 &middot; &middot; &middot; anr
</p>
<p>⎞
⎟⎟⎟⎠ , (9.5.5)
</p>
<p>cj = fj (x0,y)= yj &minus;hj (x0) , c=
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>c1
c2
...
</p>
<p>cn
</p>
<p>⎞
⎟⎟⎟⎠ , (9.5.6)
</p>
<p>and use the relation (9.2.10), then we obtain
</p>
<p>fj (x0,η)= fj (x0,y&minus; ε)= fj (x0,y)&minus; ε . (9.5.7)
</p>
<p>Thus we can now write the system of equations (9.5.2) in the form
</p>
<p>f= Aξ+ c&minus; ε= 0 , ε= Aξ+ c . (9.5.8)
</p>
<p>The least-squares condition (9.2.16) is then
</p>
<p>M = (c+Aξ)TGy(c+Aξ)= min (9.5.9)</p>
<p/>
</div>
<div class="page"><p/>
<p>228 9 The Method of Least Squares
</p>
<p>in complete analogy to (9.2.19). Using the quantities defined in (9.2.20)
through (9.2.22) we can obtain the solution directly from (9.2.24),
</p>
<p>ξ̃ =&minus;A&prime;+c&prime; . (9.5.10)
</p>
<p>Corresponding to (9.5.4) one can use ξ̃ to find a better approximation
</p>
<p>x1 = x0 + ξ̃ (9.5.11)
</p>
<p>and compute new values of A and c at the point x1. A solution ξ̃ (9.5.10)
for (9.5.9) with these values gives x2 = x1+ ξ̃, etc. This iterative procedure can
be terminated when the minimum function in the last step has not significantly
decreased in comparison to the result of the previous step.
</p>
<p>There is no guaranty, however, for the convergence of this procedure.
Heuristically it is clear, however, that the expectation for convergence is
greater when the Taylor series, truncated after the first term, is a good app-
roximation in the region over which x is varied in the procedure. This is at
least the region between x0 and the solution x̃. (Intermediate steps can also lie
outside of this region.) It is therefore important, particularly in highly nonlin-
ear problems, to start from a good first approximation.
</p>
<p>If the solution x̃ = xn = xn&minus;1 + ξ̃ is reached in n steps, then it can be
expressed as a linear function of ξ̃. Using error propagation one finds that the
covariance matrices of x̃ and ξ̃ are then identical and one finds that
</p>
<p>Cx̃ =G&minus;1x̃ = (A
TGyA)
</p>
<p>&minus;1 = (A&prime;TA&prime;)&minus;1 . (9.5.12)
</p>
<p>The covariance matrix loses its validity, however, if the linear approxima-
tion (9.5.3) is not a good description in the region x̃i &plusmn;Δxi , i = 1, . . . , r .
(Here Δxi =
</p>
<p>&radic;
Cii .)
</p>
<p>9.6 Algorithms for Fitting Nonlinear Functions
</p>
<p>It is sometimes useful to set one or several of the r unknown parameters equal
to given values, i.e., to treat them as constants and not as adjustable param-
eters. This can clearly be done by means of a corresponding definition of f
in (9.5.1). For the user, however, it is more convenient to write only one sub-
program that computes the function f for the r original parameters and when
needed, to communicate to the program that the number of adjustable param-
eters is to be reduced from r to r &prime;. Of course a list with r elements ℓi must also
be given, in which one specifies which parameters xi are to be held constant
(ℓi = 0) and which should remain adjustable (ℓi = 1).
</p>
<p>Two more difficulties come up when implementing the considerations of
the previous section in a program:</p>
<p/>
</div>
<div class="page"><p/>
<p>9.6 Algorithms for Fitting Nonlinear Functions 229
</p>
<p>On the one hand, the elements of the matrix A must be found by
constructing the function to be fitted and differentiating it with respect to the
parameters. Of course it is particularly convenient for the user if he does not
have to program these derivatives himself, but rather can turn this task over to
a routine for numerical differentiation. In Sect. E.1 we provide such a subrou-
tine, which we will call from the program discussed below. Numerical differ-
entiation implies, however, a loss of precision and an increase in computing
time. In addition, the method can fail. Our programs communicate such an
occurrence by means of an output parameter. Thus in some cases the user will
be forced to program the derivatives by hand.
</p>
<p>The second difficulty is related to the fact that the minimum function
</p>
<p>M = (y&minus;h(x))TGy(y&minus;h(x)) (9.6.1)
</p>
<p>is no longer a simple quadratic form of the unknowns like (9.2.17) and (9.2.23).
One consequence of this is that the position of the minimum x̃ cannot be
reached in a single step. In addition, the convergence of the iterative proce-
dure strongly depends on whether the first approximation x0 is in a region
where the minimum function is sufficiently similar to a quadratic form. The
determination of a good first approximation must be handled according to the
given problem. Some examples are given below. If one constructs the iterative
procedure as indicated in the previous section, then it can easily happen that
the minimum function does not decrease with every step. In order to ensure
convergence despite this, two methods can be applied, which are described in
Sects. 9.6.1 and 9.6.2. The first (reduction of step size) is simpler and faster.
The second (the Marquardt procedure) has, however, a larger region of con-
vergence. We give programs for both methods, but recommend applying Mar-
quardt procedure in cases of doubt.
</p>
<p>9.6.1 Iteration with Step-Size Reduction
</p>
<p>As mentioned, the inequality
</p>
<p>M(xi)=M(xi&minus;1 + ξ̃) &lt;M(xi&minus;1) (9.6.2)
</p>
<p>does not hold in every case for the result xi of step i. The following con-
sideration helps us in handling such steps. Let us consider the expression
M(xi&minus;1 + sξ̃) as a function of the quantity s with 0 &le; s &le; 1. If we replace
ξ̃ by sξ̃ in (9.5.9), then we obtain
</p>
<p>M = (c+ sAξ̃)TGy(c+ sAξ̃)= (c&prime;+ sA&prime;ξ̃)2 .</p>
<p/>
</div>
<div class="page"><p/>
<p>230 9 The Method of Least Squares
</p>
<p>Differentiating with respect to s gives
</p>
<p>M &prime; = 2(c&prime;+ sA&prime;ξ̃)TA&prime;ξ̃
</p>
<p>or with c&prime; =&minus;A&prime;ξ̃, cf. (9.5.10),
</p>
<p>M &prime; = 2(s&minus;1)ξ̃TA&prime;TA&prime;ξ̃ ,
</p>
<p>and thus
M &prime;(s = 0) &lt; 0 ,
</p>
<p>if A&prime;TA&prime; =ATGyA is positive definite. (This is always the case near the min-
imum.) The matrix A&prime;TA&prime; gives the curvature of the function M in the space
spanned by the unknowns x1, . . . ,xr . For only one such unknown, the region
of positive curvature is the region between the points of inflection around the
minimum (cf. Fig. 10.1). Since the function M is continuous in s, there exists
a value λ &gt; 0 such that
</p>
<p>M &prime;(s) &lt; 0 , 0 &le; s &le; λ .
</p>
<p>After an iteration i+1, for which (9.6.2) does not hold, one multiplies ξ̃
by a number s, e.g., s = 1/2, and checks whether
</p>
<p>M(xi&minus;1 + sξ̃) &lt; M(xi&minus;1) .
</p>
<p>If this is the case, then one sets xi = xi&minus;1 + sξ̃. If it is not the case, then one
multiplies again with s, and so forth.
</p>
<p>being dependent on a controlled variable t , i.e., yj = yj (tj ). For the true values
ηj corresponding to the measurements one has
</p>
<p>ηj = h(x, tj )
</p>
<p>or [cf. (9.6.1)]
η = h(x, t) .
</p>
<p>in Sect. 9.14. The matrix A is computed by numerical differentiation with the
</p>
<p>This function has to be programmed by the user. That is done within an exten-
</p>
<p>sion of the abstract class DataUserFunction, see the example programs
</p>
<p>The class
</p>
<p>step-size reduction. As in the linear case we consider the measurements as
</p>
<p>LsqNon operates according to this procedure of iteration with
</p>
<p>class
</p>
<p>to provide a method with the same name and the same method declarations,
</p>
<p>computing the matrix A by analytic differentiation.
</p>
<p>AuxDri. Should the accuracy of that method not suffice, the user has</p>
<p/>
</div>
<div class="page"><p/>
<p>9.6 Algorithms for Fitting Nonlinear Functions 231
</p>
<p>Fig.9.5: Measured values and fitted Gaussian curve (above); logarithm of the measured val-
ues (below).
</p>
<p>Example 9.4: Fitting a Gaussian curve
</p>
<p>In many experiments one has signals y(t) that have the form of a Gaussian
curve,
</p>
<p>y(t)= x1 exp(&minus;(t&minus;x2)2/2x23) . (9.6.3)
</p>
<p>One wishes to determine the parameters x1, x2, x3 that give the amplitude,
position of the maximum, and the width of the signal.
</p>
<p>Figure 9.5 shows the result of the fit to data. The values x1 = 0.5, x2 = 1.5,
x3 = 0.2 were used as a first approximation. In fact we could have estimated
significantly better initial values directly from the plot of the measurements,</p>
<p/>
</div>
<div class="page"><p/>
<p>232 9 The Method of Least Squares
</p>
<p>i.e., x1 &asymp; 1 for the amplitude, x2 &asymp; 1.25 for the position of the maximum, and
x3 &asymp; 0.4 for the width.
</p>
<p>Here we have just mentioned a particularly useful aid in determining the
initial approximations: the analysis of a plot of the data by eye. One can also
proceed more formally, however, and consider the logarithm of (9.6.3),
</p>
<p>lny(t) = lnx1 &minus;
(t&minus;x2)2
</p>
<p>2x23
</p>
<p>=
(
</p>
<p>lnx1 &minus;
x22
</p>
<p>2x23
</p>
<p>)
+ t
</p>
<p>(
x2
</p>
<p>x23
</p>
<p>)
&minus; t2
</p>
<p>(
1
</p>
<p>2x23
</p>
<p>)
.
</p>
<p>This is a polynomial in t , which is linear in the three terms in parentheses,
</p>
<p>a1 = lnx1 &minus;
x22
</p>
<p>2x23
, a2 =
</p>
<p>x2
</p>
<p>x23
, a3 =&minus;
</p>
<p>1
</p>
<p>2x23
.
</p>
<p>By taking the logarithm, however, the distribution for the measurement errors,
originally Gaussian, has been changed, so that strictly speaking one cannot fit
a polynomial to determine a1, a2, and a3. For purposes of determining the first
approximation we can disregard this difficulty and set the errors of the lny(ti)
equal to one. We then determine the quantities a1, a2, a3, and from them the
values x1, x2, x3, and use these as initial values.
</p>
<p>The logarithms of the measured values yi are shown in the lower part of
Fig. 9.5. One can see that fit range for the parabola must be restricted to points
in the bell-shaped region of the curve, since the points in the tails are subject
to large fluctuations.
</p>
<p>Example 9.5: Fit of an exponential function
</p>
<p>In studies of radioactivity, for example, a function of the form
</p>
<p>y(t)= x1 exp(&minus;x2t) (9.6.4)
</p>
<p>must be fitted to measured values yi(ti). The program to be provided here by
the user can have the following form.
</p>
<p>In Fig. 9.6 the result of a fit to data is shown. The determination of the first
approximation for the unknowns can again be obtained by fitting a straight
line (graphically or numerically) to the logarithm of the function y(t),
</p>
<p>lny(t)= lnx1 &minus;x2t .
</p>
<p>Here one usually uses only the values at small t , since these points have
smaller fluctuations.</p>
<p/>
</div>
<div class="page"><p/>
<p>9.6 Algorithms for Fitting Nonlinear Functions 233
</p>
<p>Fig.9.6:Measured values and fitted exponential function (above); logarithms of the measured
values (below).
</p>
<p>Example 9.6: Fitting a sum of exponential functions
</p>
<p>A radioactive substance often consists of a mixture of components with differ-
ent decay times. One must therefore fit a sum of several exponential functions.
We will consider the case of two functions
</p>
<p>y(t)= x1 exp(&minus;x2t)+x3 exp(&minus;x4t) . (9.6.5)
</p>
<p>Figure 9.7 shows the result of fitting a sum of two exponential functions
to the data. A first approximation can be determined by fitting two different
lines to lnyi(ti) for the regions of smaller and larger ti .</p>
<p/>
</div>
<div class="page"><p/>
<p>234 9 The Method of Least Squares
</p>
<p>Fig.9.7:Measured values with the fitted sum of two exponential functions (above); logarithm
of the measured values (below).
</p>
<p>9.6.2 Marquardt Iteration
</p>
<p>The procedure with step-size reduction discussed in Sect. 9.6.1 leads to a min-
imum when x is already in a region where A&prime;TA&prime; is positive definite, i.e., in a
region around the minimum where the function M(x) has a positive curvature.
(In the one-dimensional case of Fig. 10.1, this is the region between the two
points of inflection on either side of the minimum.) It is clear, however, that
it must be possible to extend the region of convergence to the region between
the two maxima surrounding the minimum. This possibility is offered by the
Marquardt procedure, which is presented in Sect. 10.15 in a somewhat more</p>
<p/>
</div>
<div class="page"><p/>
<p>9.6 Algorithms for Fitting Nonlinear Functions 235
</p>
<p>want to became familiar with this class should first study Sect. 10.15 and the
introductory sections of Chap. 10 and finally Sect. A.17
</p>
<p>Example 9.7: Fitting a sum of two Gaussian functions and a polynomial
</p>
<p>In practice it is usually not so easy to find the amplitude, position, and width of
signals as was shown in Example 9.4. One usually has more than one signal,
lying on a background that varies slowly with the controlled variable t . Since
in general this background is not well known, it is approximated by a line or a
second-order polynomial. We will consider the sum of such a polynomial and
two Gaussian distributions, i.e., a function of nine unknown parameters,
</p>
<p>h(x, t)= x1 +x2t+x3t2
</p>
<p>+x4 exp{&minus;(x5 &minus; t)2/2x26}+x7 exp{&minus;(x8 &minus; t)2/2x29} . (9.6.6)
</p>
<p>The derivatives (9.5.5) are
</p>
<p>&minus;aj1 =
&part;hj
</p>
<p>&part;x1
= 1 ,
</p>
<p>&minus;aj2 =
&part;hj
</p>
<p>&part;x2
= tj ,
</p>
<p>&minus;aj3 =
&part;hj
</p>
<p>&part;x3
= t2j ,
</p>
<p>&minus;aj4 =
&part;hj
</p>
<p>&part;x4
= exp{&minus;(x5 &minus; tj )2/2x26} ,
</p>
<p>&minus;aj5 =
&part;hj
</p>
<p>&part;x5
= 2x4 exp{&minus;(x5 &minus; tj)2/2x26}
</p>
<p>tj &minus;x5
2x26
</p>
<p>,
</p>
<p>&minus;aj6 =
&part;hj
</p>
<p>&part;x6
= x4 exp{&minus;(x5 &minus; tj)2/2x26}
</p>
<p>(tj &minus;x5)2
</p>
<p>x36
,
</p>
<p>... .
</p>
<p>If the numerical differentiation fails, the derivatives must be computed with a
</p>
<p>shown in Fig. 9.8, however, this is not necessary. The user must supply, of
(9.6.6).
</p>
<p>Figure 9.8 shows the result of fitting to a total of 50 measurements.
It is perhaps interesting to look at some of the intermediate steps in Fig. 9.9.
As a first approximation the parameters of the polynomial were set to zero
(x1 = x2 = x3 = 0). For both of the clearly visible signals, rough estimates of
the amplitude, position, and width were used for the initial values. The func-
tion (9.6.6) with the parameters of this first approximation is shown in the first
</p>
<p>general form. The class LSqMar treats the nonlinear case of least squares;
</p>
<p>the user&rsquo;s task is much the same as in the case of LsqNon. Readers who
</p>
<p>specially written version of the routineAuxDri. For the numerical example
</p>
<p>course, an extension of DataUserFunction to compute</p>
<p/>
</div>
<div class="page"><p/>
<p>236 9 The Method of Least Squares
</p>
<p>Fig.9.8: Measured values and fitted sum of a second-order polynomial and two Gaussian
functions.
</p>
<p>9.7 Properties of the Least-Squares Solution: χ2-Test
</p>
<p>Up to now the method of least squares has merely been an application of
the maximum-likelihood method to a linear problem. The prescription of
least squares (9.2.15) was obtained directly from the maximization of the
likelihood function (9.2.14). In order to be able to specify this likelihood
function in the first place, it was necessary to know the distribution of the
measurement errors. We assumed a normal distribution. But also when there
is no exact knowledge of the error distribution, one can still apply the rela-
tion (9.2.15) and with it the remaining formulas of the last sections. Such a
procedure seems to lack a theoretical justification. The Gauss&ndash;Markov the-
orem, however, states that in this case as well, the method of least squares
provides results with desirable properties. Before entering into this, let us list
once more the properties of the maximum-likelihood solution.
</p>
<p>frame . In the following frames one sees the change of the function
</p>
<p>after 2, 4, 6, and 8 steps, and finally at convergence of the class
</p>
<p>a total of 9 steps.
</p>
<p>(STEP 0)
</p>
<p>LsqMar after</p>
<p/>
</div>
<div class="page"><p/>
<p>9.7 Properties of the Least-Squares Solution: χ2-Test 237
</p>
<p>Fig.9.9: Successive approximations of the fit function to the measurements.
</p>
<p>(a) The solution x̃ is asymptotically unbiased, i.e., for very large samples,
</p>
<p>E(x̃i)= xi , i = 1,2, . . . , r .
</p>
<p>(b) It is a minimum variance estimator, i.e.,
</p>
<p>σ 2(x̃i)= E{(x̃i &minus;xi)2} = min .
</p>
<p>(c) The quantity (9.2.16)
</p>
<p>M = εTGyε
follows a χ2-distribution with n&minus; r degrees of freedom.</p>
<p/>
</div>
<div class="page"><p/>
<p>238 9 The Method of Least Squares
</p>
<p>The properties (a) and (b) are familiar from Chap. 7. We will demonstrate
the validity of (c) for the simple case of direct measurements (r = 1), for
which the matrix Gy is diagonal:
</p>
<p>Gy =
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>1/σ 21 0
1/σ 22
</p>
<p>. . .
</p>
<p>0 1/σ 2n
</p>
<p>⎞
⎟⎟⎟⎠ .
</p>
<p>The quantity M then simply becomes a sum of squares,
</p>
<p>M =
n&sum;
</p>
<p>j=1
ε2j/σ
</p>
<p>2
j . (9.7.1)
</p>
<p>Since each εj comes from a normally distributed population with mean of zero
and variance σ 2j , the quantities ε
</p>
<p>2
j/σ
</p>
<p>2
j are described by a standard Gaussian
</p>
<p>distribution. Thus the sum of squares follows a χ2-distribution with n&minus; 1
degrees of freedom.
</p>
<p>If the distribution of the errors εj is not known, then the least-squares
solution has the following properties:
</p>
<p>(a) The solution is unbiased.
</p>
<p>(b) Of all solutions x&lowast; that are unbiased estimators for x and are linear
combinations of the measurements y, the least-squares solution has the
smallest variance. (This is the GAUSS&ndash;MARKOV theorem.)
</p>
<p>(c) The expectation value of
</p>
<p>M = εTGyε
</p>
<p>is
E(M)= n&minus; r .
</p>
<p>(This is exactly the expectation value of a χ2 variable for n&minus; r degrees
of freedom.)
</p>
<p>The quantity M is often simply called χ2, although it does not necessarily
follow a χ2-distribution. Together with the matrices Cx̃ and Cη̃ it provides
a convenient measure of the quality of a fit with the least-squares method.
If the value of M obtained from the measurements comes out much larger
than n&minus; r , then the assumptions of the calculation must be carefully checked.
The result should not simply be accepted without critical examination.
</p>
<p>The number f = n&minus; r is called the number of degrees of freedom of
a fit or the number of equations of constraint of the fit. It is clear from the</p>
<p/>
</div>
<div class="page"><p/>
<p>9.7 Properties of the Least-Squares Solution: χ2-Test 239
</p>
<p>beginning (see Appendix A), that the problem of least squares can only be
solved for f &ge; 0. Only for f &gt; 0, however, is the quantity M meaningfully
defined and usable for testing the quality of a fit.
</p>
<p>If it is known that the errors are normally distributed, then a χ2-test can
be done in conjunction with the fit. One rejects the result of the fit if
</p>
<p>M = εTGyε&gt; χ21&minus;α(n&minus; r) , (9.7.2)
</p>
<p>i.e., if the quantity M exceeds the quantile of the χ2-distribution with n&minus; r
degrees of freedom at a significance level of α. A larger value of M can be
caused be the following reasons (and also by an error of the first kind):
</p>
<p>(a) The assumed functional dependence f(x,η)= 0 between the measured
values η and the unknown parameters x is false. Either the function
f(x,η) is completely wrong, or some of the parameters taken to be
known are not correct.
</p>
<p>(b) The function f(x,η) is correct, but the series expansion with only one
term is not a sufficiently good approximation in the region of parameter
space covered in the computation.
</p>
<p>(c) The initial approximation x0 is too far from the true value x. Better
values for x0 could lead to more acceptable values of M . This point is
clearly related to (b).
</p>
<p>(d) The covariance matrix of the measurements Cy , which is often only
based on rough estimates or assumptions, is not correct.
</p>
<p>These four points must be carefully taken into consideration if the method
of least squares is to be applied successfully. In many cases the least-squares
computation is repeated many times for different data sets. One can then look
at the empirical frequency distribution of the quantity M and compare it to a
χ2-distribution with the corresponding number of degrees of freedom. Such
a comparison is particularly useful for a good estimate of Cy . At the start of a
new experiment, the apparatus that provides the measured values y is usually
checked by measuring known quantities. In this way the parameters x for sev-
eral data sets are determined. The covariance matrix of the measurements can
then be determined such that the distribution (i.e., the histogram) of M agrees
as well as possible with a χ2-distribution. This investigation is particularly
illustrative when one considers the distribution of F(M) instead of that of
M , where F is the distribution function (6.6.11) of the χ2-distribution. If M
follows a χ2-distribution, then F(M) follows a uniform distribution.</p>
<p/>
</div>
<div class="page"><p/>
<p>240 9 The Method of Least Squares
</p>
<p>9.8 Confidence Regions and Asymmetric Errors
</p>
<p>in the Nonlinear Case
</p>
<p>We begin this section with a brief review of the meaning of the covariance
matrix Cx̃ of the unknown parameters for the linear least-squares case. The
probability that the true value of the unknowns is given by x is given by a
normal distribution of the form (5.10.1),
</p>
<p>φ(x)= k exp{&minus;12(x&minus; x̃)
TB(x&minus; x̃)} , (9.8.1)
</p>
<p>where B = C&minus;1
x̃
</p>
<p>. The exponent (multiplied by &minus;2),
</p>
<p>g(x)= (x&minus; x̃)TB(x&minus; x̃) , (9.8.2)
</p>
<p>for g = 1 = const describes the covariance ellipsoid (cf. Sect. 5.10). For other
values of g(x) = const one obtains confidence ellipsoids for a given proba-
bility W [see (5.10.19)]. In this way confidence ellipsoids can easily be con-
structed that have the point x = x̃ at the center and which contain the true
value x of the unknown with probability W = 0.95.
</p>
<p>We can now find a relation between the confidence ellipsoid and the mini-
mum function. For this we use the expression (9.2.19), which is exactly true in
the linear case, and we compute the difference between the minimum function
M(x) at a point x and the minimum function M(x̃) at the point x̃ where it is a
minimum,
</p>
<p>M(x)&minus;M(x̃)= (x&minus; x̃)TATGyA(x&minus; x̃) . (9.8.3)
According to (9.2.27), one has
</p>
<p>B = C&minus;1
x̃
</p>
<p>=Gx̃ = ATGyA . (9.8.4)
</p>
<p>So the difference (9.8.3) is exactly the function introduced above, g(x).
Thus the covariance ellipsoid is in fact the hypersurface in the space of
</p>
<p>the r variables x1, . . . ,xr , where the function M(x) has the value M(x) =
M(x̃)+ 1. Correspondingly, the confidence ellipsoid with probability W is
the hypersurface for which
</p>
<p>M(x)=M(x̃)+g , (9.8.5)
</p>
<p>where the constant g is the quantile of the χ2-distribution with f = n&minus; r
degrees of freedom according to (5.10.21),
</p>
<p>g = χ2W (f ) . (9.8.6)
</p>
<p>In the nonlinear case of least squares, our considerations remain approx-
imately valid. The approximation becomes better when the nonlinear devi-
ations of the expression (9.5.3) are less in the region where the unknown</p>
<p/>
</div>
<div class="page"><p/>
<p>Confidence Regions and Asymmetric Errorsin the Nonlinear Case 241
</p>
<p>parameters are varied. The deviations are not only small when the deriva-
tives are almost constant, i.e., the function is almost linear, but also when the
variation of the unknown parameters is small. A measure of the variation of
an unknown is, however, its error. Because of error propagation, the errors
of the unknowns are small when the original errors of the measurements are
small. In the nonlinear case, therefore, the covariance matrix retains the same
interpretation as in the linear case as long as the measurement errors are small.
</p>
<p>If the measurement errors are large we retain the interpretation (9.8.5),
i.e., we determine the hypersurface with (9.8.5) and state that the true value
x lies with probability W within the confidence region around the point. This
region is, however, no longer an ellipsoid.
</p>
<p>For only one parameter x, the curve M = M(x) can easily be plotted.
The confidence region is simply a segment of the x axis. For two parame-
ters x1, x2 the boundary of the confidence region is a curve in the (x1,x2)
plane. It is the contour line (9.8.5) of the function M =M(x). The graphical
</p>
<p>a graphical representation is only possible in the form of slices of the con-
fidence region in the (xi,xj ) plane in the (x1,x2, . . . ,xr) space through the
point x̃= (x̃1, x̃2, . . . , x̃r), where xi , xj can be any possible pair of variables.
</p>
<p>Example 9.8: The influence of large measurement errors on the confidence
region of the parameters for fitting an exponential function
</p>
<p>The results of fitting an exponential function as in Example 9.5 to data points
with errors of different sizes are shown in Fig. 9.10. The two fitted parame-
ters x1 and x2 and their errors and correlation coefficient are also shown in
the figure. The quantities Δx1, Δx2, and ρ were computed directly from the
elements of the covariance matrix Cx̃ . As expected, these errors increase for
increasing measurement errors.
</p>
<p>The covariance ellipses for this fit are shown as thin lines in the various
frames of Fig. 9.11. The small circle in the center indicates the solution x̃. In
addition the vertical bars through the point x̃ mark the regions x̃1 &plusmn;Δx1 and
x̃2 &plusmn;Δx2. As a thicker line a contour is indicated which surrounds the con-
fidence region M(x) = M(x̃)+ 1. We see that for small measurement errors
the confidence region and covariance ellipse are practically identical, while
for large measurement errors they clearly differ from each other.
</p>
<p>Computing and plotting the confidence regions requires, to the extent
that they differ from the confidence ellipsoids, considerable effort. It is there-
fore important to be able to decide whether a clear difference between the
two exists. For clarity we will stay with the case of two variables and con-
sider again Example 9.8 and in particular Fig. 9.11. The errors Δxi = σi =
</p>
<p>representation of these contours is performed by the method DatanGra-
</p>
<p>phics. drawContour, cf. Appendix F.5. For more then two parameters,</p>
<p/>
</div>
<div class="page"><p/>
<p>242 9 The Method of Least Squares
</p>
<p>Fig.9.10: Fit of an exponential function η = x1 exp(&minus;x2t) to measurements with errors of
different sizes.
</p>
<p>&radic;
Cx̃(i, i) obtained from the covariance matrix Cx̃ have the following prop-
</p>
<p>erty. The lines xi = x̃i &plusmn;Δxi are tangent to the covariance ellipse. If the co-
variance ellipse must be replaced by a confidence region of less regular form,
then we can nevertheless find the horizontal and vertical tangents, and at those
places in Fig. 9.11 one has
</p>
<p>xi = x̃i +Δxi+ , xi = x̃i &minus;Δxi&minus; . (9.8.7)
</p>
<p>Because of the loss of symmetry, the errors Δx+ and Δx&minus; are in general
different. One speaks of asymmetric errors. For r variables one has tangent
hypersurfaces of dimension r&minus;1 instead of tangent lines.
</p>
<p>We now give a procedure to compute the asymmetric errors Δxi&plusmn;. One is
only interested in asymmetric confidence regions when the asymmetric errors
are significantly different from the symmetric ones. The values xi&plusmn; = x̃i &plusmn;
Δxi&plusmn; have the property that the minimum of the function M for a fixed value
of xi = xi&plusmn; has the value
</p>
<p>min{M(x; xi = xi&plusmn;)} =M(x̃)+g (9.8.8)
</p>
<p>with g = 1. For other values of g one obtains corresponding asymmetric con-
fidence boundaries. If we bring all the terms in (9.8.8) to the left-hand side,</p>
<p/>
</div>
<div class="page"><p/>
<p>9.9 Constrained Measurements 243
</p>
<p>Fig.9.11: Results of the fit from Fig. 9.10 in the parameter space x1, x2. Solution x̃ (small
circle), symmetric errors (error bars), covariance ellipse, asymmetric error limits (horizon-
tal and vertical lines), confidence region corresponding to the probability of the covariance
ellipse (dark contour).
</p>
<p>min{M(x; xi = xi&plusmn;)}&minus;M(x̃)&minus;g = 0 , (9.8.9)
</p>
<p>then we see the problem to be solved is a combination of minimization and
zero-finding. We find the zeros using an iterative procedure as in Sect. E.2.
The zero, i.e., the point xi fulfilling (9.8.9), is first bracketed by the values
xsmall and xbig, corresponding to negative and positive values of (9.8.9). Next,
this interval is reduced by successively dividing it in half until the expres-
sion (9.8.9) differs from zero by no more than g/100. The minimum in (9.8.9)
</p>
<p>9.9 Constrained Measurements
</p>
<p>We now return to the case of Sect. 9.1, where the quantities of interest were
directly measured. The N measurements are, however, no longer completely
</p>
<p>is found with
</p>
<p>errors are determined, are
</p>
<p>the use of
</p>
<p>Fig. 9.11.
</p>
<p>LsqNon or LsqMar. The classes, by which the asymmetric
</p>
<p>LsqAsn, if LsqNon is used, and lsqAsm for
</p>
<p>LsqMar. The asymmetric errors for Example 9.8 are shown in</p>
<p/>
</div>
<div class="page"><p/>
<p>244 9 The Method of Least Squares
</p>
<p>independent, but rather are related to each other by q equations of constraint.
One could measure, for example, the three angles of a triangle. The equation
of constraint says that their sum is equal to 180◦. We again ask for the best
estimates η̃j for the quantities ηj . The measurements give instead the values
</p>
<p>yj = ηj + εj , j = 1,2, . . . ,n . (9.9.1)
</p>
<p>As above let us assume a normal distribution about zero for the measurement
errors εj :
</p>
<p>E(εj )= 0 , E(ε2j )= σ 2j .
The q equations of constraint have the form
</p>
<p>fk(η)= 0 , k = 1,2, . . . ,q . (9.9.2)
</p>
<p>Let us first consider the simple case of linear equations of constraint. The
Eqs. (9.9.2) are then of the form
</p>
<p>b10 +b11η1 +b12η2 +&middot;&middot; &middot;+b1nηn = 0 ,
b20 +b21η1 +b22η2 +&middot;&middot; &middot;+b2nηn = 0 , (9.9.3)
</p>
<p>...
</p>
<p>bq0 +bq1η1 +bq2η2 +&middot;&middot; &middot;+bqnηn = 0 ,
</p>
<p>or in matrix notation,
Bη+b0 = 0 . (9.9.4)
</p>
<p>9.9.1 The Method of Elements
</p>
<p>Although not well suited for automatic processing of data, an illustrative
procedure is the method of elements. We can use the q equations (9.9.3)
to eliminate q of the n quantities η. The remaining n&minus; q quantities αi (i =
1,2, . . . ,n&minus; q) are called elements. They can be chosen arbitrarily from the
original η or they can be a linear combination of them. We can then express
the full vector η as a set of linear combinations of these elements,
</p>
<p>ηj = fj0+fj1α1+fj2α2+&middot;&middot; &middot;+fj,n&minus;qαn&minus;q , j = 1,2, . . . ,n , (9.9.5)
</p>
<p>or
η= Fα+ f0 . (9.9.6)
</p>
<p>Equation (9.9.6) is of the same type as (9.2.2). The solution must thus be of
the form of (9.2.26), i.e.,
</p>
<p>α̃ = (F TGyF)&minus;1F TGy(y&minus; f0) (9.9.7)</p>
<p/>
</div>
<div class="page"><p/>
<p>9.9 Constrained Measurements 245
</p>
<p>describes the estimation of the elements α according to the method of least
squares. The corresponding covariance matrix is
</p>
<p>G&minus;1
α̃
</p>
<p>= (F TGyF)&minus;1 , (9.9.8)
</p>
<p>cf. (9.2.27). The improved measurements are obtained by substituting (9.9.7)
into (9.9.6):
</p>
<p>η̃= F α̃+ f0 = F(F TGyF)&minus;1F TGy(y&minus; f0)+ f0 . (9.9.9)
</p>
<p>By error propagation the covariance matrix is found to be
</p>
<p>G&minus;1
η̃
</p>
<p>= F(F TGyF)&minus;1F T = FG&minus;1α̃ F
T . (9.9.10)
</p>
<p>Example 9.9: Constraint between the angles of a triangle
</p>
<p>Suppose measurements of the angles of a triangle have yielded the values
y1 = 89◦, y2 = 31◦, y3 = 61◦, i.e.,
</p>
<p>y=
</p>
<p>⎛
⎝
</p>
<p>89
31
62
</p>
<p>⎞
⎠ .
</p>
<p>The linear equation of constraint is
</p>
<p>η1 +η2 +η3 = 180 .
</p>
<p>It can be written as
Bη+b0 = 0
</p>
<p>with
B = (1,1,1) , b0 = b0 =&minus;180 .
</p>
<p>As elements we choose η1 and η2. The system (9.9.5) then becomes
</p>
<p>η1 = α1 , η2 = α2 , η3 = 180&minus;α1&minus;α2
</p>
<p>or
</p>
<p>η=
</p>
<p>⎛
⎝
</p>
<p>1 0
0 1
</p>
<p>&minus;1 &minus;1
</p>
<p>⎞
⎠α+
</p>
<p>⎛
⎝
</p>
<p>0
0
</p>
<p>180
</p>
<p>⎞
⎠ ,
</p>
<p>i.e.,
</p>
<p>F =
</p>
<p>⎛
⎝
</p>
<p>1 0
0 1
</p>
<p>&minus;1 &minus;1
</p>
<p>⎞
⎠ , f0 =
</p>
<p>⎛
⎝
</p>
<p>0
0
</p>
<p>180
</p>
<p>⎞
⎠ .</p>
<p/>
</div>
<div class="page"><p/>
<p>246 9 The Method of Least Squares
</p>
<p>We assume a measurement error for the angle of 1◦, i.e.,
</p>
<p>Cy =
</p>
<p>⎛
⎝
</p>
<p>1 0 0
0 1 0
0 0 1
</p>
<p>⎞
⎠= I , Gy = C&minus;1y = I .
</p>
<p>Using these in (9.9.7) gives
</p>
<p>α̃ =
</p>
<p>⎡
⎣
(
</p>
<p>1 0 &minus;1
0 1 &minus;1
</p>
<p>)
I
</p>
<p>⎛
⎝
</p>
<p>1 0
0 1
</p>
<p>&minus;1 &minus;1
</p>
<p>⎞
⎠
⎤
⎦
&minus;1(
</p>
<p>1 0 &minus;1
0 1 &minus;1
</p>
<p>)
I
</p>
<p>⎛
⎝
</p>
<p>89
31
</p>
<p>&minus;118
</p>
<p>⎞
⎠
</p>
<p>=
(
</p>
<p>2 1
1 2
</p>
<p>)&minus;1(207
149
</p>
<p>)
= 1
</p>
<p>3
</p>
<p>(
2 &minus;1
</p>
<p>&minus;1 2
</p>
<p>)(
207
</p>
<p>149
</p>
<p>)
</p>
<p>=
(
</p>
<p>8813
3013
</p>
<p>)
.
</p>
<p>Using (9.9.9) one finally obtains
</p>
<p>η̃= F α̃+ f0 =
</p>
<p>⎛
⎝
</p>
<p>1 0
0 1
</p>
<p>&minus;1 &minus;1
</p>
<p>⎞
⎠
(
</p>
<p>8813
3013
</p>
<p>)
+
</p>
<p>⎛
⎝
</p>
<p>0
0
</p>
<p>180
</p>
<p>⎞
⎠=
</p>
<p>⎛
⎜⎜⎝
</p>
<p>8813
</p>
<p>3013
</p>
<p>6113
</p>
<p>⎞
⎟⎟⎠ .
</p>
<p>This result was clearly to be expected. The &ldquo;excess&rdquo; in the measured sum of
angles of 2◦ is subtracted equally from the three measurements. This would
not have been the case, however, if the individual measurements had errors of
different magnitudes. The reader can easily repeat the computation for such
a case. The residual errors of the improved measurements can be determined
by application of (9.9.10),
</p>
<p>G&minus;1
η̃
</p>
<p>=
</p>
<p>⎛
⎝
</p>
<p>1 0
0 1
</p>
<p>&minus;1 &minus;1
</p>
<p>⎞
⎠
⎡
⎣
(
</p>
<p>1 0 1
0 1 &minus;1
</p>
<p>)
I
</p>
<p>⎛
⎝
</p>
<p>1 0
0 1
</p>
<p>&minus;1 &minus;1
</p>
<p>⎞
⎠
⎤
⎦
&minus;1(
</p>
<p>1 0 1
0 1 &minus;1
</p>
<p>)
</p>
<p>= 1
3
</p>
<p>⎛
⎝
</p>
<p>1 0
0 1
</p>
<p>&minus;1 &minus;1
</p>
<p>⎞
⎠
(
</p>
<p>2 &minus;1
&minus;1 2
</p>
<p>)(
1 0 1
0 1 &minus;1
</p>
<p>)
</p>
<p>= 1
3
</p>
<p>⎛
⎝
</p>
<p>2 &minus;1 &minus;1
&minus;1 2 &minus;1
&minus;1 &minus;1 2
</p>
<p>⎞
⎠ .
</p>
<p>The residual error of each angle is thus equal to
&radic;
</p>
<p>2/3 &asymp; 0.82.</p>
<p/>
</div>
<div class="page"><p/>
<p>9.9 Constrained Measurements 247
</p>
<p>At this point we want to make a general statement on measurements rel-
ated by equations of constraint. Although in the statistical methods used so
far we have found no way of dealing with systematic errors, equations of
constraint offer such a possibility in many cases. If, for example, the sum
of angles in many measurements is observed to be greater than 180◦ more
frequently than less, then one can conclude that the measurement apparatus
has a systematic error.
</p>
<p>9.9.2 The Method of Lagrange Multipliers
</p>
<p>Instead of elements, one more frequently uses the method of Lagrange multi-
pliers. Although both methods clearly give the same results, the latter has the
advantage that all unknowns are treated in the same way and thus the user is
spared having to choose elements. The method of Lagrange multipliers is a
well-known procedure in differential calculus for determination of extrema in
problems with additional constraints.
</p>
<p>We begin again from the linear system of equations of constraint (9.9.4)
</p>
<p>Bη+b0 = 0
</p>
<p>and we recall that the measured quantities are the sum of the true value η and
the measurement error ε,
</p>
<p>y= η+ ε .
Thus one has
</p>
<p>By&minus;Bε+b0 = 0 . (9.9.11)
Since y is known from the measurement, and b0 and B are also constructed
from known quantities, we can construct a column vector with q elements,
</p>
<p>c= By+b0 , (9.9.12)
</p>
<p>which does not contain any unknowns. Thus (9.9.11) can be written in the
form
</p>
<p>c&minus;Bε= 0 . (9.9.13)
We now introduce an additional column vector with q elements, whose ele-
ments, not yet known, are the Lagrange multipliers,
</p>
<p>&micro;=
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>μ1
μ2
...
</p>
<p>μq
</p>
<p>⎞
⎟⎟⎟⎠ . (9.9.14)
</p>
<p>Using this we extend the original minimum function (9.2.16)</p>
<p/>
</div>
<div class="page"><p/>
<p>248 9 The Method of Least Squares
</p>
<p>M = εTGyε
to
</p>
<p>L= εTGyε+2μT(c&minus;Bε) . (9.9.15)
The function L is called the Lagrange function. The requirement
</p>
<p>M = min
with the constraint
</p>
<p>c&minus;Bε= 0
is then fulfilled when the total differential of the Lagrange function vanishes,
i.e., when
</p>
<p>dL= 2εTGy dε&minus;2&micro;TB dε= 0 .
This is equivalent to
</p>
<p>εTGy &minus;&micro;TB = 0 . (9.9.16)
The system (9.9.16) consists of n equations containing a total of n+ q
unknowns, ε1, ε2, . . ., εn and μ1, μ2, . . ., μq . In addition we have the q equa-
tions of constraint (9.9.13). We transpose (9.9.16) and obtain
</p>
<p>Gyε= BT&micro; , ε=G&minus;1y BT&micro; . (9.9.17)
By substitution into (9.9.13) we obtain
</p>
<p>c&minus;BG&minus;1y BT&micro;= 0 ,
which can easily be solved for &micro;:
</p>
<p>&micro;̃= (BG&minus;1y BT)&minus;1c . (9.9.18)
With (9.9.17) we thus have the least-squares estimators of the measurement
errors,
</p>
<p>ε̃=G&minus;1y BT(BG&minus;1y BT)&minus;1c . (9.9.19)
The estimators of the unknowns are then given by (9.9.1),
</p>
<p>η̃= y&minus; ε̃= y&minus;G&minus;1y BT(BG&minus;1y BT)&minus;1c . (9.9.20)
With the abbreviation
</p>
<p>GB = (BG&minus;1y BT)&minus;1
</p>
<p>this becomes
η̃= y&minus;G&minus;1y BTGBc . (9.9.21)
</p>
<p>The covariance matrices of &micro;̃ and η̃ are easily obtained by applying error
propagation to the linear system of equations (9.9.18) and (9.9.19),
</p>
<p>G&minus;1
μ̃
</p>
<p>= (BG&minus;1y BT)&minus;1 =GB , (9.9.22)
G&minus;1
</p>
<p>η̃
= G&minus;1y &minus;G&minus;1y BTGBBG&minus;1y . (9.9.23)</p>
<p/>
</div>
<div class="page"><p/>
<p>9.9 Constrained Measurements 249
</p>
<p>Example 9.10: Application of the method of Lagrange multipliers to
Example 9.9
</p>
<p>We apply the method of Lagrange multipliers to the problem of Example 9.9.
We then have
</p>
<p>c= By+b0 = (1,1,1)
</p>
<p>⎛
⎝
</p>
<p>89
31
62
</p>
<p>⎞
⎠&minus;180 = 182&minus;180 = 2 ,
</p>
<p>and in addition,
</p>
<p>GB = (BGyBT)&minus;1 =
</p>
<p>⎡
⎣(1,1,1)I
</p>
<p>⎛
⎝
</p>
<p>1
1
1
</p>
<p>⎞
⎠
⎤
⎦
&minus;1
</p>
<p>= 3&minus;1 = 1
3
</p>
<p>and
</p>
<p>GyB
T = I
</p>
<p>⎛
⎝
</p>
<p>1
1
1
</p>
<p>⎞
⎠=
</p>
<p>⎛
⎝
</p>
<p>1
1
1
</p>
<p>⎞
⎠ .
</p>
<p>We can now compute (9.9.21),
</p>
<p>η̃=
</p>
<p>⎛
⎝
</p>
<p>89
31
62
</p>
<p>⎞
⎠&minus;
</p>
<p>⎛
⎝
</p>
<p>1
1
1
</p>
<p>⎞
⎠ 2
</p>
<p>3
=
</p>
<p>⎛
⎜⎜⎝
</p>
<p>8813
</p>
<p>3013
</p>
<p>6113
</p>
<p>⎞
⎟⎟⎠ .
</p>
<p>The covariance matrices are then
</p>
<p>G&minus;1
μ̃
</p>
<p>= 1
3
</p>
<p>,
</p>
<p>G&minus;1
η̃
</p>
<p>= I &minus; I
</p>
<p>⎛
⎝
</p>
<p>1
1
1
</p>
<p>⎞
⎠ 1
</p>
<p>3
(1,1,1)I
</p>
<p>= I &minus; 1
3
</p>
<p>⎛
⎝
</p>
<p>1 1 1
1 1 1
1 1 1
</p>
<p>⎞
⎠= 1
</p>
<p>3
</p>
<p>⎛
⎝
</p>
<p>2 &minus;1 &minus;1
&minus;1 2 &minus;1
&minus;1 &minus;1 2
</p>
<p>⎞
⎠ .
</p>
<p>We now generalize the method of Lagrange multipliers for the case of
nonlinear equations of constraint having the general form (9.9.2), i.e.,
</p>
<p>fk(η)= 0 , k = 1,2, . . . ,q .
</p>
<p>These equations can be expanded in a series about η0,
</p>
<p>fk(η)= fk()+
(
&part;fk
</p>
<p>&part;η1
</p>
<p>)
</p>
<p>η0
</p>
<p>(η1 &minus;η10)+&middot;&middot; &middot;+
(
&part;fk
</p>
<p>&part;ηn
</p>
<p>)
</p>
<p>η0
</p>
<p>(ηn&minus;ηn0) . (9.9.24)</p>
<p/>
</div>
<div class="page"><p/>
<p>250 9 The Method of Least Squares
</p>
<p>Here the η0 are the first approximations for the true values η. Using the
definitions
</p>
<p>bkl =
(
&part;fk
</p>
<p>&part;ηl
</p>
<p>)
</p>
<p>η0
</p>
<p>, B =
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>b11 b12 &middot; &middot; &middot; b1n
b21 b22 &middot; &middot; &middot; b2n
...
</p>
<p>bq1 bq2 &middot; &middot; &middot; bqn
</p>
<p>⎞
⎟⎟⎟⎠ ,
</p>
<p>ck = fk(η0) , c=
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>c1
c2
...
</p>
<p>cq
</p>
<p>⎞
⎟⎟⎟⎠ ,
</p>
<p>δk = (ηk&minus;ηk0) , δ=
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>δ1
δ2
...
</p>
<p>δn
</p>
<p>⎞
⎟⎟⎟⎠ ,
</p>
<p>we can write (9.9.24) in the form
</p>
<p>Bδ+ c= 0 . (9.9.25)
</p>
<p>Except for a sign this relation corresponds to (9.9.13). The solution δ̃
therefore can be read off (9.9.19),
</p>
<p>δ̃ =&minus;G&minus;1y BT(BG&minus;1y BT)&minus;1c . (9.9.26)
</p>
<p>As first approximations η0 we use the measured values y,
</p>
<p>η0 = y (9.9.27)
</p>
<p>and obtain
η̃ = η0 + δ̃ . (9.9.28)
</p>
<p>For linear constraint equations this already is the solution.
If the equations are nonlinear an iteration is performed. The prescription
</p>
<p>for step i of the iteration is described for the general case at the end of the
next section. If, in the formulas given there, all terms containing the matrix A
are set to zero, one obtains the iteration procedure for the case of constraint
measurements.
</p>
<p>For each step i one computes
</p>
<p>Mi = εTi Gyεi , εi = y&minus;ηi . (9.9.29)
</p>
<p>The procedure is terminated as convergent, if a further step leads to no appre-
ciable reduction of Mi . We call the result η̃.</p>
<p/>
</div>
<div class="page"><p/>
<p>9.10 The General Case of Least-Squares Fitting 251
</p>
<p>The covariance matrix is still given by (9.9.23), i.e.,
</p>
<p>G&minus;1
η̃
</p>
<p>=G&minus;1y &minus;G&minus;1y BTGBBG&minus;1y , (9.9.30)
</p>
<p>if the elements of B are computed using at η̃. The best estimates of the mea-
surement errors are computed using
</p>
<p>ε̃ = y&minus; η̃ . (9.9.31)
With them the minimum function is found to be
</p>
<p>M̃ = ε̃TGy ε̃ . (9.9.32)
This quantity again can be used for a χ2 test with q degrees of freedom.
</p>
<p>Although the method of Lagrange multipliers is mathematically elegant,
in programs provided here we use the method of orthogonal transformations
(see Sect. 9.12).
</p>
<p>9.10 The General Case of Least-Squares Fitting
</p>
<p>After the preparation of the previous sections we can now take up the general
case of fitting with the method of least squares.
</p>
<p>We first recall the notation. The r unknown parameters are placed in a
vector x. The quantities to be measured form an n-vector η. The values y
actually measured differ from η by the errors ε. We will assume a normal
distribution for the individual errors εj (j = 1,2, . . . ,n), i.e., a normal distri-
bution for the n variables εj with the null vector as the vector of expectation
values, and a covariance matrix Cy =G&minus;1y . The vectors x and η are related by
m functions
</p>
<p>fk(x,η)= fk(x,y&minus; ε)= 0 , k = 1,2, . . . ,m . (9.10.1)
We will further assume that we have already obtained in some way a first
approximation for the unknowns x0. As a first approximation for η we use
η0 = y as in Sect. 9.9. Finally we require that the functions fk can be approxi-
mated by linear functions in the range of variability of our problem, i.e., in the
region around (x0,η0), which is given by the differences x&minus; x0 and η&minus;η0.
We can then write
</p>
<p>fk(x,η)= fk(x0,η0)
</p>
<p>+
(
&part;fk
</p>
<p>&part;x1
</p>
<p>)
</p>
<p>x0,η0
</p>
<p>(x1 &minus;x10)+&middot;&middot; &middot;+
(
&part;fk
</p>
<p>&part;xr
</p>
<p>)
</p>
<p>x0,η0
</p>
<p>(xr &minus;xr0)
</p>
<p>+
(
&part;fk
</p>
<p>&part;η1
</p>
<p>)
</p>
<p>x0,η0
</p>
<p>(η1 &minus;η10)+&middot;&middot; &middot;+
(
&part;fk
</p>
<p>&part;ηn
</p>
<p>)
</p>
<p>x0,η0
</p>
<p>(ηn&minus;ηn0) .
</p>
<p>(9.10.2)</p>
<p/>
</div>
<div class="page"><p/>
<p>252 9 The Method of Least Squares
</p>
<p>With the abbreviations
</p>
<p>akℓ =
(
&part;fk
</p>
<p>&part;xℓ
</p>
<p>)
</p>
<p>x0,η0
</p>
<p>, A =
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>a11 a12 &middot; &middot; &middot; a1r
a21 a22 &middot; &middot; &middot; a2r
...
</p>
<p>am1 am2 &middot; &middot; &middot; amr
</p>
<p>⎞
⎟⎟⎟⎠ , (9.10.3)
</p>
<p>bkℓ =
(
&part;fk
</p>
<p>&part;ηℓ
</p>
<p>)
</p>
<p>x0,η0
</p>
<p>, B =
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>b11 b12 &middot; &middot; &middot; b1n
b21 b22 &middot; &middot; &middot; b2n
...
</p>
<p>bm1 bm2 &middot; &middot; &middot; bmn
</p>
<p>⎞
⎟⎟⎟⎠ , (9.10.4)
</p>
<p>ck = fk(x0,η0) , c =
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>c1
c2
...
</p>
<p>cm
</p>
<p>⎞
⎟⎟⎟⎠ , (9.10.5)
</p>
<p>ξ = x&minus;x0 , δ= η&minus;η0 (9.10.6)
the system of equations (9.10.2) can be written as follows:
</p>
<p>Aξ+Bδ+ c= 0 . (9.10.7)
</p>
<p>The Lagrange function is
</p>
<p>L= δTGyδ+2&micro;T(Aξ+Bδ+ c) . (9.10.8)
</p>
<p>Here &micro; is an m-vector of the Lagrange multipliers. We require that the total
derivative of (9.10.8) with respect to δ vanishes. This is equivalent to requiring
</p>
<p>Gyδ+BT&micro;= 0
</p>
<p>or
δ=&minus;G&minus;1y BT&micro; . (9.10.9)
</p>
<p>Substitution into (9.10.7) gives
</p>
<p>Aξ&minus;BG&minus;1y BT&micro;+ c= 0 (9.10.10)
</p>
<p>or
&micro;=GB(Aξ+ c) , (9.10.11)
</p>
<p>where
GB = (BG&minus;1y BT)&minus;1 . (9.10.12)
</p>
<p>With (9.10.9) we can now write
</p>
<p>δ=&minus;G&minus;1y BTGB(Aξ+ c) . (9.10.13)</p>
<p/>
</div>
<div class="page"><p/>
<p>9.10 The General Case of Least-Squares Fitting 253
</p>
<p>Since the Lagrange function L is a minimum also with respect to ξ, the total
derivative of (9.10.8) with respect to ξ must also vanish, i.e.,
</p>
<p>2&micro;TA= 0 .
</p>
<p>By transposing and substituting (9.10.11) one obtains
</p>
<p>2ATGB(Aξ+ c)= 0
</p>
<p>or
ξ̃ =&minus;(ATGBA)&minus;1ATGBc . (9.10.14)
</p>
<p>Substituting (9.10.14) into (9.10.13) and (9.10.11) immediately gives the es-
timates of the deviations δ and the Lagrange multipliers &micro;,
</p>
<p>δ̃ = &minus;G&minus;1y BTGB(c&minus;A(ATGBA)&minus;1ATGBc) , (9.10.15)
&micro;̃ = GB(c&minus;A(ATGBA)&minus;1ATGBc) . (9.10.16)
</p>
<p>The estimates for the parameters x and for the improved measurements η are
</p>
<p>x̃ = x0 + ξ̃ , (9.10.17)
η̃ = η0 + δ̃ . (9.10.18)
</p>
<p>From (9.10.14), (9.10.4), and (9.10.5) we obtain for the matrix of derivatives
of the elements of ξ̃ with respect to the elements of y
</p>
<p>&part; ξ̃
</p>
<p>&part;y
=&minus;(ATGBA)&minus;1ATGB
</p>
<p>&part;c
</p>
<p>&part;y
=&minus;(ATGBA)&minus;1ATGBB .
</p>
<p>Using error propagation one obtains the covariance matrix
</p>
<p>G&minus;1
x̃
</p>
<p>=G&minus;1
ξ̃
</p>
<p>= (ATGBA)&minus;1 . (9.10.19)
</p>
<p>Correspondingly one finds
</p>
<p>G&minus;1
η̃
</p>
<p>=G&minus;1y &minus;G&minus;1y BTGBBG&minus;1y +G&minus;1y BTGBA(ATGBA)&minus;1ATGBBG&minus;1y .
(9.10.20)
</p>
<p>One can show that under the assumed conditions, i.e., sufficient linearity
of (9.10.2) and normally distributed measurement errors, the minimum func-
tion M , which can also be written in the form
</p>
<p>M = (Bε̃)TGB(Bε̃) , ε̃ = y&minus; η̃ , (9.10.21)
</p>
<p>follows a χ2-distribution with m&minus; r degrees of freedom.</p>
<p/>
</div>
<div class="page"><p/>
<p>254 9 The Method of Least Squares
</p>
<p>If the Eqs. (9.10.1) are linear, then the relations (9.10.17) to (9.10.20)
already are the solutions. In nonlinear cases one can perform an iterative pro-
cedure, which we now discuss in detail for step i with i = 1,2, . . .. For the
functions fk the following holds:
</p>
<p>f
(i)
k (x,η) = fk(xi&minus;1,ηi&minus;1)
</p>
<p>+
(
&part;fk
</p>
<p>&part;x1
</p>
<p>)
</p>
<p>xi&minus;1,ηi&minus;1
</p>
<p>(x1 &minus;x1,i&minus;1)+&middot;&middot; &middot;
</p>
<p>+
(
&part;fk
</p>
<p>&part;xr
</p>
<p>)
</p>
<p>xi&minus;1,ηi&minus;1
</p>
<p>(xr &minus;xr,i&minus;1)
</p>
<p>+
(
&part;fk
</p>
<p>&part;η1
</p>
<p>)
</p>
<p>xi&minus;1,ηi&minus;1
</p>
<p>(η1 &minus;η1,i&minus;1)+&middot;&middot; &middot;
</p>
<p>+
(
&part;fk
</p>
<p>&part;ηn
</p>
<p>)
</p>
<p>xi&minus;1,ηi&minus;1
</p>
<p>(ηn&minus;ηn,i&minus;1) .
</p>
<p>With A(i),B(i),c(i) we denote the quantities A,B,c, evaluated at xi&minus;1,ηi&minus;1.
Furthermore let
</p>
<p>ξ (i) = xi &minus;xi&minus;1 , δ(i) = ηi &minus;ηi&minus;1 .
</p>
<p>Then
A(i)ξ (i)+B(i)δ(i)+ c(i) = 0 .
</p>
<p>We now denote with
</p>
<p>s(i) =
i&minus;1&sum;
</p>
<p>ℓ=1
δ(ℓ)
</p>
<p>the sum of the contributions of all previous steps to improve the measurements
and find for the difference between the measurements y and the approximation
ηi
</p>
<p>y&minus;ηi = y&minus; (η0 + s(i)+ δ(i))=&minus;(s(i)+ δ(i)) ,
since η0 = y. The first term of the Lagrangian function is
</p>
<p>(y&minus;ηi)TGy(y&minus;ηi)= (s(i)+ δ(i))TGy(s(i)+ δ(i))
</p>
<p>and the full Lagrangian is
</p>
<p>L= (s(i)+ δ(i))TGy(s(i)+ δ(i))+2&micro;(i)T(A(i)ξ (i)+B(i)δ(i)+ c(i)) .
</p>
<p>We can now proceed as above and get, with G(i)B = (B(i)G&minus;1y B(i)T)&minus;1,
</p>
<p>ξ (i) =&minus;(A(i)TG(i)B A(i))&minus;1A(i)TG
(i)
B (c
</p>
<p>(i)&minus;B(i)s(i)) .</p>
<p/>
</div>
<div class="page"><p/>
<p>9.11 Algorithm for the General Case of Least Squares 255
</p>
<p>and
</p>
<p>δ(i) =&minus;G&minus;1y B(i)TG
(i)
B (c
</p>
<p>(i)&minus;B(i)s(i)&minus;A(i)(A(i)TG(i)B A(i))&minus;1A(i)T
</p>
<p>&times;G(i)B (c(i)&minus;B(i)s(i))) .
</p>
<p>For every step i we compute
</p>
<p>Mi = εTi Gyεi , εi = y&minus;ηi . (9.10.22)
</p>
<p>The procedure is terminated as convergent, if a new step does not yield an
appreciable further reduction of Mi . The results of the iteration are denoted
by x̃, η̃. The corresponding covariance matrices are given by (9.10.19) and
(9.10.20), if the matrices A and B are evaluated at x̃, η̃. It is, of course, pos-
sible for the iteration process to diverge. In this case points (a)&ndash;(d), raised at
the end of Sect. 9.7, should be considered.
</p>
<p>In the following section we describe a different way to determine the
solutions x̃, η̃. Also for that procedure the formulas (9.10.19), (9.10.20),
and (9.10.21) for the computation of the covariance matrices G&minus;1
</p>
<p>x̃
,G&minus;1
</p>
<p>η̃
and
</p>
<p>the minimum function M remain valid, if the matrices A and B are evaluated
at the position of the solution.
</p>
<p>9.11 Algorithm for the General Case of Least Squares
</p>
<p>use the method of Lagrange multipliers but rather the procedure of Sect. A.18,
which is based on orthogonal transformations.
</p>
<p>At every step of the iteration we must determine the r-vector ξ and the
n-vector δ. We combine both into an (r+n)-vector u,
</p>
<p>u=
(
ξ
</p>
<p>δ
</p>
<p>)
. (9.11.1)
</p>
<p>The m&times; r matrix A and the m&times;n matrix B are also combined into an m&times;
(r+n) matrix E,
</p>
<p>E = (A,B) . (9.11.2)
The vector containing the solutions u must satisfy the constraint (9.10.7), i.e.,
</p>
<p>Eu= d , d=&minus;c . (9.11.3)
</p>
<p>We now consider the minimum function in the ith iterative step. It de-
pends on
</p>
<p>η= y+
i&sum;
</p>
<p>ℓ=1
δℓ = y+
</p>
<p>i&minus;1&sum;
</p>
<p>ℓ=1
δℓ+ δi = y+ s+ δ , δ= δi , (9.11.4)
</p>
<p>In the Java class LSqGen, treating the general case of least squares, we do not</p>
<p/>
</div>
<div class="page"><p/>
<p>256 9 The Method of Least Squares
</p>
<p>where
</p>
<p>s=
i&minus;1&sum;
</p>
<p>ℓ=1
δℓ (9.11.5)
</p>
<p>is the result of all of the previous steps that changed y. One then has
</p>
<p>M = (η&minus;y)TGy(η&minus;y)= (δ+ s)TGy(δ+ s)= min . (9.11.6)
</p>
<p>We now extend the n&times;n matrix Gy to an (r+n)&times; (r+n) matrix
</p>
<p>G=
(
</p>
<p>0 0
0 Gy
</p>
<p>)
}r
}n , (9.11.7)
</p>
<p>for which we find a Cholesky decomposition according to Sect. A.9,
</p>
<p>G= F TF . (9.11.8)
</p>
<p>Then (9.11.6) becomes
</p>
<p>M = (u+ t)TG(u+ t)
= (Fu+F t)2 = min
</p>
<p>or
(Fu&minus;b)2 = min (9.11.9)
</p>
<p>with
</p>
<p>t=
(
0
</p>
<p>s
</p>
<p>)
}r
}n , b=&minus;F t . (9.11.10)
</p>
<p>Now one must merely solve the problem (9.11.9) with the constraint (9.11.3),
e.g., with the procedure of Sect. A.18. With the solution
</p>
<p>ũ=
(
ξ̃
</p>
<p>δ̃
</p>
<p>)
</p>
<p>or rather with the vectors ξ̃, δ̃ one finds improved values for x [cf. (9.10.17)],
η [cf. (9.10.18)], and for s, as well as for t [cf. (9.11.5) and (9.11.10)], with
which an additional iterative step can be carried out.
</p>
<p>The procedure can be regarded as having converged and terminated
when the minimum function (9.11.9) in two successive steps only changes
by an insignificant amount, or it can be terminated without success if after
a given number of steps convergence has not been reached. In the case of
convergence, the covariance of the unknowns can be computed according
to (9.10.19). The calculation of the covariance matrix of the &ldquo;improved&rdquo; mea-
surements η̃ according to (9.10.20) is possible as well. It is, however, rarely</p>
<p/>
</div>
<div class="page"><p/>
<p>9.11 Algorithm for the General Case of Least Squares 257
</p>
<p>of interest. Finally, the value of M obtained in the last step can be used for a
χ2-test of the goodness-of-fit with m&minus; r degrees of freedom.
</p>
<p>has to program the relation (9.10.1), which depends on the problem at hand.
</p>
<p>There exist example programs (Sect. 9.14) for the following examples in this
chapter, including realizations of such classes.
</p>
<p>Example 9.11: Fitting a line to points with measurement errors in both the
abscissa and ordinate
</p>
<p>Suppose a number of measured points (ti, si) in the (t, s) plane are given. Each
point has measurement errors Δti , Δsi , which can, in general, be correlated.
The covariance between the measurement errors Δti and Δsi is ci . We identify
ti and si with elements of the n-vector y of measured quantities
</p>
<p>y1 = t1 , y2 = s1 , . . . , yn&minus;1 = tn/2 , yn = sn/2 .
</p>
<p>The covariance matrix is
</p>
<p>Cy =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>(Δt1)
2 c1 0 0
</p>
<p>c1 (Δs1)
2 0 0
</p>
<p>0 0 (Δt2)2 c2
0 0 c2 (Δs2)2
</p>
<p>. . .
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠
</p>
<p>.
</p>
<p>A straight line in the (t, s) plane is described by the equation s = x1+x2t .
For the assumption of such a line through the measured points, the equations
of constraint (9.10.1) take on the form
</p>
<p>fk(x,η)= η2k&minus;x1 &minus;x2η2k&minus;1 = 0 , k = 1,2, . . . ,n/2 . (9.11.11)
</p>
<p>Because of the term x2η2k&minus;1, the equations are not linear. The derivatives with
respect to η2k&minus;1 depend on x2 and those with respect to x2 depend on η2k&minus;1.
</p>
<p>The results of fitting to four measured points are shown in Fig. 9.12. The
examples in the two individual plots differ only in the correlation coefficient
of the third measurement, ρ3 = 0.5 and ρ3 =&minus;0.5. One can see a noticeable
effect of the sign of ρ3 on the fit result and in particular on the value of the
minimum function.
</p>
<p>Example 9.12: Fixing parameters
</p>
<p>In Fig. 9.13 the results of fitting a line to measured points with errors in the
abscissa and ordinate are shown, where in each plot one parameter of the line
was held fixed. In the upper plot the intercept of the vertical axis x1 was fixed,
and in the lower plot the slope x2 was fixed.
</p>
<p>All these operations are performed by the class
</p>
<p>the numerical computation of the derivatives for the matrix E. The user only
</p>
<p>LsqGen. This includes
</p>
<p>That is done by an extension of the abstract class DatanUserFunction.</p>
<p/>
</div>
<div class="page"><p/>
<p>258 9 The Method of Least Squares
</p>
<p>Fig.9.12: Fitting a line to four measured points in the (t,s) plane. The points are shown with
measurement errors (in t and s) and covariance ellipses. The individual plots show the results
of the fit, the errors, and correlation coefficients.
</p>
<p>9.12 Applying the Algorithm for the General Case
</p>
<p>to Constrained Measurements
</p>
<p>If all of the variables x1, . . . ,xr are fixed, then there are no more unknowns in
the least-squares problem.
</p>
<p>In the equations of constraint (9.10.2) only the components of η are vari-
able. Thus all terms containing the matrix A vanish from the formulas of the
previous sections. As previously, however, the improved measurements η̃ can
be computed. The quantity M can in addition be used for a χ2-test with m</p>
<p/>
</div>
<div class="page"><p/>
<p>9.12 Constrained Measurements with the Algorithm for the General Case 259
</p>
<p>Fig.9.13: Fitting a line to the same points as in the first plot of Fig. 9.12. The intercept with
the vertical axis x1 was held constant in the upper plot, and below the slope x2 was held
constant.
</p>
<p>degrees of freedom for how well the equations of constraint are satisfied by
the measurements. Finally, the covariance matrix of the improved measure-
ments Cη̃ can be determined.
</p>
<p>Mathematically &ndash; as well as when using the program LsqGen &ndash; it does
</p>
<p>not matter whether all variables are fixed (r �= 0, r &prime; = 0) or whether from the
start the equations of constraint do not depend on the variables x (r = 0). In
both cases LsqGen gives the same solution.</p>
<p/>
</div>
<div class="page"><p/>
<p>260 9 The Method of Least Squares
</p>
<p>Example 9.13: χ2-test of the description of measured points with errors
in abscissa and ordinate by a given line
</p>
<p>We use the same measurements as used already in Examples 9.11 and 9.12.
</p>
<p>parameters are shown in Fig. 9.14. For the upper plot, x1 and x2 were fixed
to the values obtained from fitting with two adjustable parameters in Exam-
ple 9.11, Fig. 9.12. Clearly we also obtain the same value of M as previously.
For the lower plot, arbitrarily crude estimates (x1 = 0, x2 = 0.5) were used.
They give a significantly higher value of M . This value would lead to rejec-
tion of the hypothesis with a confidence level of 99% that the data points are
described by a linear relation with these parameter values (χ20.99 = 13.28 for
four degrees of freedom).
</p>
<p>It is also interesting to consider the improved measurements η̃ and their
errors, which are shown in Fig. 9.14. The improved measurements naturally
lie on the line. The measurement errors are the square roots of the diagonal
elements of Cη̃. The correlations between the errors of a measured point in
s and t are obtained from the corresponding off-diagonal elements Cη̃. They
are exactly equal to one. The covariance ellipses of the individual improved
measurements collapse to line segments which lie on the line given by x1, x2.
</p>
<p>9.13 Confidence Region and Asymmetric Errors
</p>
<p>in the General Case
</p>
<p>The results obtained in Sect. 9.8 on confidence regions and asymmetric errors
are also valid for the general case. We therefore limit ourselves to stating that
</p>
<p>Example 9.14: Asymmetric errors and confidence region for fitting a
straight line to measured points with errors in the abscissa and
ordinate
</p>
<p>Figure 9.15 shows the result of fitting to four points with large measurement
errors. From Sect. 9.8 we already know that large measurement errors can lead
to asymmetric errors in the fitted parameters. In fact, we see highly asymmet-
ric errors and large differences between the covariance ellipse and the corre-
sponding confidence region.
</p>
<p>The results of the analysis of these measurements with LsqGen with fixed
</p>
<p>asymmetric errors are computed by the class
</p>
<p>example.
</p>
<p>LsqAsg and to presenting an</p>
<p/>
</div>
<div class="page"><p/>
<p>9.14 Java Classes and Example Programs 261
</p>
<p>Fig.9.14: Constrained measurements. The hypothesis was tested whether the true values of
the measured points, indicated by their covariance ellipses, lie on the line s = x1 +x2t . With
the numerical values of M , a χ2-test with four degrees of freedom can be carried out. Shown
as well are the improved measurements, which lie on the line, and their errors.
</p>
<p>9.14 Java Classes and Example Programs
</p>
<p>Java Classes for Least-Squares Problems
</p>
<p>LsqPol handles the fitting of a polynomial (Sect. 9.4.1).
</p>
<p>LsqLin handles the linear case of indirect measurements (Sect. 9.4.2).
</p>
<p>LsqNon handles the nonlinear case of indirect measurements (Sect. 9.6.1).</p>
<p/>
</div>
<div class="page"><p/>
<p>262 9 The Method of Least Squares
</p>
<p>Fig.9.15: (Above) Measured points with covariance ellipses and fitted line. (Below) Result
of the fit, given in the plane spanned by the parameters x1, x2. Shown are the fitted param-
eter values (circle), symmetric errors (crossed bars), covariance ellipse, asymmetric errors
(horizontal and vertical lines), and the confidence region (dark contour).
</p>
<p>(Sect. 9.8).
</p>
<p>method.
</p>
<p>LsqMar handles the nonlinear case using Marquardt&rsquo;s method (Sect. 9.6.2).
</p>
<p>LsqAsn yields asymmetric errors or confidence limits in the nonlinear case
</p>
<p>LsqAsm yields asymmetric errors or confidence limits using Marquardt&rsquo;s
</p>
<p>LsqGen handles the general case of least squares (Sect. 9.11).</p>
<p/>
</div>
<div class="page"><p/>
<p>9.14 Java Classes and Example Programs 263
</p>
<p>(Sect. 9.13).
</p>
<p>The short program uses the data of Table 9.3 and computes vectors x̃ of coefficients
and their covariance matrix Cx for r = 1,2,3,4. Here r&minus;1 is the degree of the poly-
nomial, i.e., r is the number of elements in x. The results are presented numerically
</p>
<p>Suggestions: (a) Modify the program (by modifying a single statement) so that
the cases r = 1,2, . . . ,10 are treated. Which peculiarity do you expect for r = 10?
(b) Instead of the data of Table 9.3 use different data determined without error by a
polynomial, e.g., y = t2, and let the program determine the parameters of the polyno-
mial from the data. Use different (although obviously incorrect) sets of values for the
errors Δyi , e.g., Δyi =
</p>
<p>&radic;
yi for one run through the program and Δyi = 1 for another
</p>
<p>run. What is the influence of the choice of Δyi on the coefficients x̃, on the minimum
function, and on the covariance matrix?
</p>
<p>The program uses the data of Fig. 9.4 and sets up the matrix A and the vector c; these
are needed for the fit of a proportional relation y = x1t to the data. Next the fit is
</p>
<p>Suggestions: (a) Modify the program so that in addition a first degree polyno-
</p>
<p>result with Fig. 9.4. (b) Display the results graphically as in Fig. 9.4.
</p>
<p>The program solves the following problem. First 20 pairs of values (ti,yi) are gener-
ated. The values ti of the controlled variables are 1/21, 2/21, . . . , 20/21. The values
yi are given by
</p>
<p>yi = x1 exp(&minus;(t&minus;x2)2/2x23 )+ εi .
Here εi is an error taken from a normal distribution with expectation value zero and
width σi . The width σi is different from point to point. It is taken from a uniform
distribution with the limits σ/2 and 3σ/2. Thus, the yi are points scattered within their
errors around a Gaussian curve defined by the parameters x= (x1,x2,x3). The widths
of the error distributions are known, i.e., Δyi = σi . The data points are generated
for the parameter values x1 = 1, x2 = 1.2, x3 = 0.4. (They are identical with those
shown in Fig. 9.5.) The program now successively performs four different fits of a
Gaussian curve to the data. In a first step all three parameters in the fit procedure
are considered variable. Then successively one, two, and finally all parameters are
fixed. Before each fit, first approximations for the variables are set which, of course,
are not modified during the fitting procedure for the fixed parameters. The results are
presented numerically.
</p>
<p>Suggestions: (a) Choose different first approximations for the non-fixed param-
eters and observe the influence of the choice on the results. (b) Obtain first approx-
imations by computation, e.g., by the procedure described in Example 9.4, in which
a parabola is fitted to the logarithms of the data. (c) Modify the program by adding
graphical output corresponding to Fig. 9.5.
</p>
<p>Example Program 9.1: The class E1Lsq demonstrates the use of LsqPol
</p>
<p>LsqAsg yields asymmetric errors or confidence limits in the general case
</p>
<p>Example Program 9.2: The class E2Lsq demonstrates the use of LsqLin
</p>
<p>mial is fitted. Set up the matrix A yourself, i.e., do not use LsqPol. Compare your
</p>
<p>Example Program 9.3: The class E3Lsq demonstrates the use of LsqNon
</p>
<p>performed by calling LsqLin. The results are displayed numerically.</p>
<p/>
</div>
<div class="page"><p/>
<p>264 9 The Method of Least Squares
</p>
<p>The program solves the problem of Example 9.7. First a set of 50 data points (ti,yi) is
generated. They are scattered according to their errors around a curve corresponding
to the sum of a second degree polynomial and two Gaussians, cf. (9.6.6). The nine
parameters of this function are combined to form the vector x. The measurement
errors Δyi are generated by the procedure described in Sect. 9.3. The data points are
generated for predefined values of x. Next, by calling LSQMAR (with significantly
different values x as first approximations) solutions x̃ are obtained by a fit to the data
points. The results are presented numerically and also in graphical form.
</p>
<p>Suggestions: (a) Fix all parameters except for x5 and x8, which determine the
mean values of the two Gaussians used for generating the data. (In the program, as
customary in Java, where indexing begins with 0, they are denoted by x[4] and
x[7].) Allow for interactive input of x5 and x8. For different input values of (x5,x8),
e.g., (10,30), (19,20), (19,15), (10,11), try to determine whether you can still sepa-
rate the two Gaussians by the fit, i.e., whether you obtain significantly different values
for x̃5 and x̃8, considering their errors Δx̃5, Δx̃8. (b) Repeat (a), but for smaller mea-
surement errors. Choose, e.g., σ = 0.1 or 0.01 instead of σ = 0.4.
</p>
<p>The program solves the problem of Example 9.8. First, pairs (ti,yi) of data are pro-
duced. The yi are scattered according to their measurement errors around a curve
given by the function yi(ti)= x1 exp(&minus;x2t), cf. (9.6.4). The measurement errors Δyi
are generated by the procedure already used in Sect. 9.3. Starting from a given first
approximation for the parameters x1, values x̃ of these parameters are fitted to the
</p>
<p>plots are produced. One shows the measurements and the fitted curve. The second
displays, in the (x1,x2)plane, the fitted parameters with symmetric and asymmetric
errors, covariance ellipse, and confidence region.
</p>
<p>The program fits a straight line to points with measurement errors in the abscissa
and ordinate, i.e., it solves the problem of Example 9.11. From the measured values,
their errors and covariances the vector y and the covariance matrix Cy are set up. Det-
ermination of the two parameters x1 (ordinate intercept) and x2 (slope) is done with
</p>
<p>and x2 are obtained by constructing a straight line through the outermost two points.
A loop extends over the two cases of Fig. 9.12. The results are shown numerically
and graphically.
</p>
<p>The program also treats Example 9.11. Again the first approximations of x1 and x2
are obtained by constructing a straight line through the outer two measured points.
</p>
<p>Example Program 9.4: The class E4Lsq demonstrates the use of LsqMar
</p>
<p>Example Program 9.5: The class E5Lsq demonstrates the use of LsqAsn
</p>
<p>Example Program 9.6: The class E6Lsq demonstrates the use of LsqGen
</p>
<p>data using LsqNon. Finally the asymmetric errors are found using LsqAsn. Two
</p>
<p>LsqGen. For this the function (9.11.11) is needed. It is implemented in the method
</p>
<p>getValue of the subclass StraightLine of E6Lsq, which itself is an exten-
</p>
<p>sion of the abstract class x1DatanUserFunction. The first approximations of
</p>
<p>Example Program 9.7: The class E7Lsq demonstrates the use of LsqGen
</p>
<p>with some variables fixed</p>
<p/>
</div>
<div class="page"><p/>
<p>9.14 Java Classes and Example Programs 265
</p>
<p>A loop extends over two cases. In the first one x1 is fixed at x1 = 0.2, in the second
x2 is fixed at x2 = 0.5. The results correspond to Fig. 9.13.
</p>
<p>The problem of Example 9.13 is solved. The results are those of Fig. 9.14.
</p>
<p>and draws the confidence-region contour for the fitted variables
Here, the problem of Example 9.14 is solved. The results are those of Fig. 9.15.
</p>
<p>Example Program 9.8: The class E8Lsq demonstrates the use of LsqGen
</p>
<p>with all variables are fixed and produces a graphical representation of
</p>
<p>improved measurements
</p>
<p>Example Program 9.9: The class E9Lsq demonstrates the use of LsqAsg</p>
<p/>
</div>
<div class="page"><p/>
<p>10. Function Minimization
</p>
<p>Locating extreme values (maxima and minima) is particularly important in
data analysis. This task occurs in solving the least-squares problem in the
form M(x,y)= min and in the maximum likelihood problem as L= max. By
means of a simple change of sign, the latter problem can also be treated as
locating a minimum. We always speak therefore of minimization.
</p>
<p>10.1 Overview: Numerical Accuracy
</p>
<p>We consider first the simple quadratic form
</p>
<p>M(x)= c&minus;bx+ 1
2
Ax2 . (10.1.1)
</p>
<p>It has an extremum where the first derivative vanishes,
</p>
<p>dM
</p>
<p>dx
= 0 =&minus;b+Ax , (10.1.2)
</p>
<p>that is, at the value
</p>
<p>xm =
b
</p>
<p>A
. (10.1.3)
</p>
<p>With M(xm)=Mm, Eq. (10.1.1) can easily be put into the form
</p>
<p>M(x)&minus;Mm =
1
</p>
<p>2
A(x&minus;xm)2 . (10.1.4)
</p>
<p>Although the function whose minimum we want to find does not usually
have the simple form of (10.1.1), it can nevertheless be approximated by
a quadratic form in the region of the minimum, where one has the Taylor
expansion around the point x0,
</p>
<p>S. Brandt, Data Analysis: Statistical and Computational Methods for Scientists and Engineers,
DOI 10.1007/978-3-319-03762-2__10, &copy; Springer International Publishing Switzerland 2014
</p>
<p>267</p>
<p/>
</div>
<div class="page"><p/>
<p>268 10 Function Minimization
</p>
<p>M(x)=M(x0)&minus;b(x&minus;x0)+
1
</p>
<p>2
A(x&minus;x0)2 +&middot;&middot; &middot; , (10.1.5)
</p>
<p>where
b =&minus;M &prime;(x0) , A=M &prime;&prime;(x0) . (10.1.6)
</p>
<p>In this approximation the minimum is given by the point where the
derivative M &prime;(x) is zero, i.e.,
</p>
<p>xmp = x0 +
b
</p>
<p>A
. (10.1.7)
</p>
<p>This holds only if x0 is sufficiently close to the minimum so that terms of order
higher than quadratic in (10.1.5) can be neglected. The situation is depicted
in Fig. 10.1. The function M(x) has a minimum at xm, maxima at xM, and
points of inflection at xs. For the second derivative in the region x &gt; xm one
has M &prime;&prime;(x) &gt; 0 for x &lt; xs and M(x &prime;&prime;) &lt; 0 for x &gt; xs. If we now choose x0 to
be in the region xm &lt; x0 &lt; xM, then the first derivative M &prime;(x) there is always
positive. Therefore xmp lies closer to xm only if x0 &lt; xs. Clearly the point x0
is not in general chosen arbitrarily, but rather as close as possible to where
the minimum is expected to be. We can call this estimated value the zeroth
approximation of xm. Various strategies are available to obtain successively
better approximations:
</p>
<p>M(x)
</p>
<p>xxMxsxmxsxM
</p>
<p>Fig.10.1: The function M(x)
has a minimum at xm, maxima
at the points xM, and points of
inflection at xs.
</p>
<p>(i) Use of the function and its first and second derivatives at x0.
</p>
<p>One computes xmp according to (10.1.7), takes xmp as a first approxi-
mation, i.e., x0 is replaced by xmp, and obtains by repeated application
of (10.1.7) a second approximation. The procedure is repeated until two
successive approximations differ by less than a given value ε. From the
discussion above it follows that this procedure does not converge if the
zeroth approximation lies outside the points of inflection in Fig. 10.1.
</p>
<p>(ii) Use of the function and its first derivative at x0.
</p>
<p>The sign of the derivative M &prime;(x0) = &minus;b at the point x0 determines
the direction in which the function increases. It is assumed that one</p>
<p/>
</div>
<div class="page"><p/>
<p>10.1 Overview: Numerical Accuracy 269
</p>
<p>should search for the minimum in the direction in which the function
decreases. The quantity
</p>
<p>x1 = x0 +b (10.1.8)
is computed, i.e., one replaces the second derivative by the value unity.
</p>
<p>Instead of x0 one now uses x1 as the approximation, and so forth.
Alternatively one can also use instead of (10.1.8) the rule
</p>
<p>x1 = x0 + cb , (10.1.9)
</p>
<p>where c is an arbitrary positive constant. Both rules ensure that the step
from x0 to x1 proceeds in the direction of the minimum. If in addition
one chooses c to be small, then the step is small, so that it does not go
beyond (or not far beyond) the minimum.
</p>
<p>(iii) Use of the function at various points.
</p>
<p>The procedure (i) can be carried out without knowing the derivative of
the function if the function itself is known at three points. One can then
uniquely fit a parabola through these three points and take the extreme
value as an approximation of the minimum of the function. One speaks
of locating the minimum by quadratic interpolation. It is, however, by
no means certain that the extreme value of the parabola is a minimum
and not a maximum. As in procedure (i) it is therefore important that
the three chosen points are already in the region of the minimum of the
function.
</p>
<p>(iv) Successive reduction of an interval containing the minimum.
</p>
<p>In none of the procedures discussed up to this point were we able to
guaranty that the minimum of the function would actually be found.
The minimum can be found with certainty provided one knows an
interval xa &lt; x &lt; xb containing the minimum. If such an interval is
known, one can locate the minimum with arbitrary accuracy by succes-
sively subdividing and checking in which subinterval the minimum is
located.
</p>
<p>In Sects. 10.2&ndash;10.7 we shall examine the minimization of a function of
only one variable. In Sect. 10.2 the formula for a parabola determined by
three points will be given. In Sect. 10.3 it is shown that the minimization
of a function of one variable is equivalent to the minimization of a function
of n variables on a line in an n-dimensional space. Section 10.4 describes a
procedure for locating an interval containing the minimum. In Sect. 10.5 a
minimum search by means of interval division is described. This is combined
in Sect. 10.5 with the procedure of quadratic interpolation in a way that the</p>
<p/>
</div>
<div class="page"><p/>
<p>270 10 Function Minimization
</p>
<p>interpolation is only used when it leads quickly to the minimum of the func-
tion. If this is not the case, one continues with interval division. In this way we
possess a procedure with the certainty of interval division combined with &ndash; as
much as possible &ndash; the speed of quadratic interpolation. The same procedure
can also be used for a function of n variables if the search for the minimum is
restricted to a line in the n-dimensional space. This problem is addressed in
Sect. 10.7.
</p>
<p>Next we turn to the task of searching for the minimum of a function of n
variables. We begin in Sect. 10.8 with the particularly elegant simplex method.
This is followed by the discussion of various procedures of successive mini-
mization along fixed directions in the n-dimensional space. These directions
can simply be the directions of the coordinates (Sect. 10.9), or for a function
that depends only quadratically on the variables they can be chosen such that
the minimum is reached in at most n steps (Sects. 10.10 and 10.11).
</p>
<p>Finally we discuss a procedure of n-dimensional minimization which
employs aspects of methods (i) and (ii) of the one-dimensional case. If x is
the n-dimensional vector of the variables, then the general quadratic form,
i.e., the generalization of (10.1.1) to n variables, is
</p>
<p>M(x) = c&minus;b &middot;x+ 1
2
xTAx (10.1.10)
</p>
<p>= c&minus;
&sum;
</p>
<p>k
</p>
<p>bkxk+
1
</p>
<p>2
</p>
<p>&sum;
</p>
<p>k,ℓ
</p>
<p>xkAkℓxℓ .
</p>
<p>Here A is a symmetric matrix, Akℓ = Aℓk. The partial derivative with respect
to xi is
</p>
<p>&part;M
</p>
<p>&part;xi
= &minus;bi +
</p>
<p>1
</p>
<p>2
</p>
<p>(&sum;
</p>
<p>ℓ
</p>
<p>Aiℓxℓ+
&sum;
</p>
<p>k
</p>
<p>xkAki
</p>
<p>)
</p>
<p>= &minus;bi +
&sum;
</p>
<p>ℓ
</p>
<p>Aiℓxℓ . (10.1.11)
</p>
<p>Expressing all of the partial derivatives as a vector &nabla;M gives
</p>
<p>&nabla;M =&minus;b+Ax . (10.1.12)
</p>
<p>At the minimum point the vector of derivatives vanishes. The minimum is
therefore located at
</p>
<p>xm = A&minus;1b , (10.1.13)
in analogy to (10.1.3).</p>
<p/>
</div>
<div class="page"><p/>
<p>10.1 Overview: Numerical Accuracy 271
</p>
<p>Clearly the function M(x) does not in general have the simple form
of (10.1.10). We can, however, expand it in a series around the point x0,
</p>
<p>M(x)=M(x0)&minus;b(x&minus;x0)+
1
</p>
<p>2
(x&minus;x0)TA(x&minus;x0)+&middot;&middot; &middot; , (10.1.14)
</p>
<p>with the negative gradient
</p>
<p>b=&minus;&nabla;M(x0) , i.e., bi = &minus;
&part;M
</p>
<p>&part;xi
</p>
<p>∣∣∣∣
x=x0
</p>
<p>, (10.1.15)
</p>
<p>and the Hessian matrix of second derivatives
</p>
<p>Aik =
&part;2M
</p>
<p>&part;xi&part;xk
</p>
<p>∣∣∣∣
x=x0
</p>
<p>. (10.1.16)
</p>
<p>The series (10.1.14) is the starting point for various minimization procedures:
</p>
<p>(i) Minimization in the direction of the gradient.
</p>
<p>Starting from the point x0 one searches for the minimum along the
direction given by the gradient &nabla;M(x0) and calls the point where it
is found x1. Starting from x1 one looks for the minimum along the
direction &nabla;M(x1), and so forth. We will discuss this procedure, called
minimization in the direction of steepest descent, in Sect. 10.12.
</p>
<p>(ii) Step of given size in the gradient direction.
</p>
<p>One computes in analogy to (10.1.9)
</p>
<p>x1 = x0 + cb , b=&minus;&nabla;M(x0) , (10.1.17)
</p>
<p>with a given positive c. That is, one takes a step in the direction of
steepest descent of the function, without, however, searching exactly
for the minimum in this direction. Next one computes the gradient at
x1, steps from x1 in the direction of this gradient, etc. In Sect. 10.13 we
will combine this method with the following one.
</p>
<p>(iii) Use of the gradient and the Hessian matrix at x0.
</p>
<p>If we truncate (10.1.14) after the quadratic term, we obtain a function
whose minimum is, according to (10.1.13), given by
</p>
<p>xmp = x0 +A&minus;1b . (10.1.18)
</p>
<p>We take x1 = xmp as the first approximation, compute for this point the
gradient and Hessian matrix, obtain by corresponding use of (10.1.18)
the second approximation, and so forth. This procedure is discussed</p>
<p/>
</div>
<div class="page"><p/>
<p>272 10 Function Minimization
</p>
<p>in Sect. 10.14. It converges quickly if the zeroth approximation x0 is
sufficiently close to the minimum. If that is not the case, however, then
it gives &ndash; as for the corresponding one dimensional procedure &ndash; no
reasonable solution. We will combine it, therefore, in Sect. 10.15 with
method (ii), in order to obtain, when possible, the speed of (iii), but
when necessary, the certainty of (ii).
</p>
<p>In Sects. 10.8 through 10.15 very different methods for solving the same
problem, the minimization of a function of n variables, will be discussed.
In Sect. 10.16 we give information on how to choose one of the methods
appropriate for the problem in question. Section 10.17 is dedicated to con-
siderations of errors. In Sect. 10.18 several examples are discussed in detail.
</p>
<p>Before we find the minimum xm of a function, we would like to inquire
briefly about the numerical accuracy we expect for xm. The minimum is after
all almost always determined by a comparison of values of the function at
points close in x. If we solve (10.1.4) for (x&minus;xm), we obtain
</p>
<p>(x&minus;xm)=
&radic;
</p>
<p>2[M(x)&minus;M(xm)]
A
</p>
<p>.
</p>
<p>We assume that A, i.e., the second derivative of the function M , is of order of
magnitude unity close to the minimum. (This need only be true approximately.
In fact, in numerical calculations one always scales all of the quantities such
that they are of order unity, i.e., not something like 106 or 10&minus;6.) If we com-
pute the function M with the precision δ then the difference M(x)&minus;M(xm)
is also known at best with a precision of δ, i.e., two function values can not
be considered as being significantly different if they differ by only δ. For the
corresponding x values one then has
</p>
<p>(x&minus;xm)&asymp;
&radic;
</p>
<p>2δ
</p>
<p>A
&asymp;
</p>
<p>&radic;
δ . (10.1.19)
</p>
<p>If the computer has n binary places available for representing the man-
tissa, then a value x can be represented with the precision (4.2.7)
</p>
<p>Δx
</p>
<p>x
= 2&minus;n .
</p>
<p>For computing a value x one chooses therefore a relative precision
</p>
<p>ε &ge; 2&minus;n ,
</p>
<p>since it is clearly pointless to try to compute a value with a higher precision
than that with which it can be represented. If x is computed iteratively, i.e., one</p>
<p/>
</div>
<div class="page"><p/>
<p>10.2 Parabola Through Three Points 273
</p>
<p>computes a series x0,x1, . . . of approximations for x, then one can truncate
this series as soon as
</p>
<p>|xk&minus;xk&minus;1|
|xk|
</p>
<p>&lt; ε
</p>
<p>or
|xk&minus;xk&minus;1|&lt; ε|xk| (10.1.20)
</p>
<p>for a given ε. With this prescription we will have difficulties, however,
if xk = 0. We introduce, therefore, in addition to ε a constant t �= 0 and
extend (10.1.20) to
</p>
<p>|xk&minus;xk&minus;1|&lt; ε|xk|+ t . (10.1.21)
The last task remaining is to choose the numerical values for ε and t . If
</p>
<p>x is the position of the minimum, then by (10.1.19) a value for ε must be
chosen that is greater than or equal to the square root of the relative precision
for the representation of a floating point number. With computations using
&ldquo;double precision&rdquo; in Java there are n = 53 binary places available for the
representation of the mantissa. Then only the values
</p>
<p>ε &gt; 2&minus;n/2 &asymp; 2 &middot;10&minus;8
</p>
<p>are reasonable. The quantity t corresponds to an absolute precision. Therefore
it can be chosen to be considerably smaller.
</p>
<p>10.2 Parabola Through Three Points
</p>
<p>If three points (xa,ya), (xb,yb), (xc,yc) of a function are known, then we can
determine the parabola
</p>
<p>y = a0 +a1x+a2x2 (10.2.1)
that passes through these points. Instead of (10.2.1) we can also represent the
parabola by
</p>
<p>y = c0 + c1(x&minus;xb)+ c2(x&minus;xb)2 . (10.2.2)
This relationship is naturally valid for three given points, i.e.,
</p>
<p>yb = c0
</p>
<p>and
(ya&minus;yb)= c1(xa&minus;xb)+ c2(xa&minus;xb)2 ,
</p>
<p>(yc&minus;yb)= c1(xc&minus;xb)+ c2(xc&minus;xb)2 .</p>
<p/>
</div>
<div class="page"><p/>
<p>274 10 Function Minimization
</p>
<p>From this we obtain
</p>
<p>c1 = C[(xc&minus;xb)2(ya&minus;yb)&minus; (xa&minus;xb)2(yc&minus;yb)] , (10.2.3)
c2 = C[&minus;(xc&minus;xb)(ya&minus;yb)+ (xa&minus;xb)(yc&minus;yb)] (10.2.4)
</p>
<p>with
</p>
<p>C = 1
(xa&minus;xb)(xc&minus;xb)2 &minus; (xc&minus;xb)(xa&minus;xb)2
</p>
<p>and for the extremum of the parabola
</p>
<p>xmp = xb&minus;
c1
</p>
<p>2c2
. (10.2.5)
</p>
<p>Fig.10.2: Parabola through three points (small circles) and its extremum (large circle). In the
left figure there is a minimum, and on the right a maximum.
</p>
<p>determine whether the extremum of the parabola is a minimum or a maxi-
mum (cf. Fig. 10.2). One has a minimum if the second derivative of (10.2.2)
with respect to x&minus;xb is positive, i.e., if c2 &gt; 0. We now order the three given
points such that
</p>
<p>xa &lt; xb &lt; xc
</p>
<p>and find that then
</p>
<p>1/C = (xc&minus;xb)(xa&minus;xb)(xc&minus;xa)=&minus;(xc&minus;xb)(xb&minus;xa)(xc&minus;xa) &lt; 0 .
</p>
<p>The class MinParab performs this simple calculation. We must still</p>
<p/>
</div>
<div class="page"><p/>
<p>10.4 Bracketing the Minimum 275
</p>
<p>With this one has for the sign of c2
</p>
<p>signc2 = sign[(xc&minus;xb)(ya&minus;yb)+ (xb&minus;xa)(yc&minus;yb)] .
</p>
<p>Both expressions (xc &minus; xb) and (xb&minus; xa) are positive. Therefore for the ex-
tremum to be a minimum it is sufficient that
</p>
<p>ya &gt; yb , yc &gt; yb . (10.2.6)
</p>
<p>The condition is not necessary, but has the advantage of greater clarity. In the
interval xa &lt; x &lt; xc one clearly has a minimum if there is a point (xb,yb) in
the interval where the function has a value smaller than at the two end points.
Clearly this statement is also valid if the function is not a parabola. We will
make use of this fact in the next section.
</p>
<p>10.3 Function of n Variables on a Line
</p>
<p>in an n-Dimensional Space
</p>
<p>Locating the minimum of the function M(x) of a single variable x in an
interval of the x axis is equivalent to locating the minimum of a function M(x)
of an n-dimensional vector of variables x = (x1,x2, . . . ,xn) with respect to a
given line in the n-dimensional space. If x0 is a fixed point and d is a fixed
vector, then
</p>
<p>x0 +ad , &minus;&infin;&lt; a &lt;&infin; , (10.3.1)
describes a fixed line (see Fig. 10.3), and
</p>
<p>f (a)=M(x0 +ad) (10.3.2)
</p>
<p>is the value of the function at the point a on this line. For n= 1, x0 = 0, d= 1
and with the change of notation a = x, that is, f (x)=M(x), one recovers the
original problem.
</p>
<p>computes the value (10.3.2); it makes use
</p>
<p>by the user, which defines the function M(x). In Sects. 10.4 through 10.6 we
consider the minimum of a function of a single variable. The programs also
treat, however, the case of a minimum of a function of n variables on a line in
the n-dimensional space.
</p>
<p>10.4 Bracketing the Minimum
</p>
<p>For many minimization procedures it is important to know ahead of time that
the minimum xm is located in a specific interval,
</p>
<p>The class FunctionOnline
</p>
<p>of an extension of the abstract class DatanUserFunction, to be provided</p>
<p/>
</div>
<div class="page"><p/>
<p>276 10 Function Minimization
</p>
<p>x0
&rarr;
</p>
<p>ad
</p>
<p>x2
</p>
<p>=
&rarr;
</p>
<p>x
&rarr;
</p>
<p>+
</p>
<p>x1
</p>
<p>d
&rarr;
</p>
<p>x0
&rarr;
</p>
<p>Fig.10.3: The line given by
(10.3.1) in two dimensions.
</p>
<p>xa &lt; xm &lt; xc . (10.4.1)
</p>
<p>By systematically reducing the interval, the position of the minimum can then
be further constrained until finally for a given precision ε one has
</p>
<p>|xa&minus;xc|&lt; ε . (10.4.2)
</p>
<p>There is, in fact, a minimum in the interval (10.4.1) if there is an x value xb
such that
</p>
<p>M(xb) &lt;M(xa) , M(xb) &lt;M(xc) , xa &lt; xb &lt; xc . (10.4.3)
</p>
<p>by giving values xa,xb,xc with the property (10.4.3). The program is based
on a similar subroutine by PRESS et al. [12]. Starting from the input values
xa,xb, which, if necessary, are relabeled so that yb &le; ya , a value xc = xb+
p(xb &minus; xa) is computed that presumably lies in the direction of decreasing
function value, i.e., closer to the minimum. The factor p in our program is
set to p = 1.618034. In this way the original interval (xa,xb) is enlarged by
the ratio of the golden section (cf. Sect. 10.5). The goal is reached if yc &gt;
yb. If this is not the case, a parabola is constructed through the three points
(xa,ya), (xb,yb), (xc,yc), whose minimum is at xm.
</p>
<p>We now examine the point (xm,ym). Here one must distinguish between
various cases:
</p>
<p>(a) xb &lt; xm &lt; xc:
</p>
<p>(a1) ym &lt; yc : (xb,xm,xc) is the desired interval.
(a2) yb &lt; ym : (xa,xb,xm) is the desired interval.
(a3) ym &gt; yc and ym &lt; yb: There is no minimum. The interval will be
</p>
<p>extended further to the right.
</p>
<p>The class MinEnclose attempts to bracket the minimum of a function</p>
<p/>
</div>
<div class="page"><p/>
<p>10.5 Minimum Search with the Golden Section 277
</p>
<p>(b) xc &lt; xm &lt; xend and xend = xb+f (xc&minus;xb) and f = 10 in our program.
</p>
<p>(b1) ym &gt; yc : (xb,xc,xm) is the desired interval.
(b2) ym &lt; yc: There is no minimum. The interval will be extended
</p>
<p>further to the right.
</p>
<p>(c) xend &lt; xm : As a new interval (xb,xc,xend) is used.
</p>
<p>(d) xm &lt; xb : This result is actually impossible. It can, however, be caused by
a rounding error. The interval will be extended further to the right.
</p>
<p>If the goal is not reached in the current step, a further step is taken with
the new interval. Figure 10.4 shows an example of the individual steps carried
out until the bracketing of the minimum is reached.
</p>
<p>Fig.10.4:Bracketing of a minimum with three points a, b, c according to (10.4.3). The initial
values are shown as larger circles, and the results of the individual steps are shown as small
circles.
</p>
<p>10.5 Minimum Search with the Golden Section
</p>
<p>As soon as the minimum has been enclosed by giving three points xa,xb,xc
with the property (10.4.3), the bracketing can easily be tightened further. One
chooses a point x inside the larger of the two subintervals (xa,xb) and (xb,xc).</p>
<p/>
</div>
<div class="page"><p/>
<p>278 10 Function Minimization
</p>
<p>If the value of the function at x is smaller than at xb, then the subinterval
containing x is taken as the new interval. If the value of the function is greater,
then x is taken as the endpoint of the new interval.
</p>
<p>A particularly clever division of the intervals is possible with the golden
section. Let us assume (see Fig. 10.5) that
</p>
<p>g = ℓ
L
</p>
<p>, g &gt;
1
</p>
<p>2
, (10.5.1)
</p>
<p>is the length of the subinterval (xa,xb) (to be determined later) measured
in units of the length of the full interval (xa,xc). We now want to be able
to divide the subinterval (xa,xb) again with a point x corresponding to a
fraction g,
</p>
<p>g = λ
ℓ
</p>
<p>. (10.5.2)
</p>
<p>In addition, the points x and xb should be situated symmetrically with respect
to each other in the interval (xa,xc), i.e.,
</p>
<p>xxbx xc
</p>
<p>l
</p>
<p>L
</p>
<p>xa
</p>
<p>λλ
</p>
<p>Fig.10.5: The golden section.
</p>
<p>λ= L&minus; ℓ . (10.5.3)
It follows that
</p>
<p>ℓ
</p>
<p>λ+ ℓ =
λ
</p>
<p>ℓ
,
</p>
<p>that is,
</p>
<p>λ=
&radic;
</p>
<p>5&minus;1
2
</p>
<p>ℓ
</p>
<p>and
</p>
<p>g =
&radic;
</p>
<p>5&minus;1
2
</p>
<p>&asymp; 0.618034 . (10.5.4)</p>
<p/>
</div>
<div class="page"><p/>
<p>10.5 Minimum Search with the Golden Section 279
</p>
<p>As shown at the beginning of this section (for the case shown in Fig. 10.5
xb&minus; xa &gt; xc &minus; xb), the minimum, which was originally only constrained to
be in the interval (xa,xc), now lies either in the interval (xa,xb) or in the
interval (x,xc). By subdividing by the golden section one obtains intervals of
equal size.
</p>
<p>Fig.10.6: Stepwise bracketing of a minimum by subdividing according to the golden section.
The original interval (larger circles) is reduced with every step (small circles).
</p>
<p>Figure 10.6 shows the first six steps of an example of minimization with
the golden section.</p>
<p/>
</div>
<div class="page"><p/>
<p>280 10 Function Minimization
</p>
<p>10.6 Minimum Search with Quadratic Interpolation
</p>
<p>From the example shown in Fig. 10.6 one sees that the procedure of interval
subdivision is quite certain, but works slowly. We now combine it, therefore,
with quadratic interpolation.
</p>
<p>[13], who first combined the two methods. The meanings of the most impor-
tant variable names in the program are as follows. a and b denote the x values
xa and xb that contain the minimum. xm is their mean value. m is the point at
which the function has its lowest value up to this step, w is the point with the
second lowest, and v is the point with the third lowest value of the function.
u is the point where the function was last computed. One begins with the
two initial values, xa and xb, that contain the minimum and adds a point x,
that divides the interval (xa,xb) according to the golden section. Then in each
subsequent iteration parabolic interpolation is attempted as in Sect. 10.2. The
result is accepted if it lies in the interval defined by the last step and if in this
step the change in the minimum is less than half as much as in the previous
one. By this condition it is ensured that the procedure converges, i.e., that the
steps become smaller on the average, although a temporary increase in step
size is tolerated. If both conditions are not fulfilled, a reduction of the interval
is carried out according to the golden section.
</p>
<p>Numerical questions are handled in a particularly careful way of Brent.
Starting from the two parameters ε and t , which define the relative precision,
and the current value x for the position of the minimum, an absolute precision
Δx = εx + t = tol is computed according to (10.1.21). The iterations are
continued until the half of the interval width falls below the value tol (i.e.,
until the distance from x to xm is not greater than tol) or until the maximum
number of steps is reached. In addition the function is not computed at points
that are separated by a distance less than tol, since such function values
would not differ significantly.
</p>
<p>The first six steps of an example of minimization according to Brent
are shown in Fig. 10.7. Steps according to the golden section are marked by
GS, and those from quadratic interpolation with QI. The comparison with
Fig. 10.6 shows the considerably faster convergence achieved by quadratic
interpolation.
</p>
<p>10.7 Minimization Along a Direction in n Dimensions
</p>
<p>The class RENTMinCombined is based on a program developed by B
</p>
<p>The class MinDir computes the minimum of a function of of n variables
</p>
<p>along the line defined in Sect. 10.3 by x0 and d. It first uses MinEnclose to
</p>
<p>bracket the minimum and then MinCombined to locate it exactly.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.8 Simplex Minimization in n Dimensions 281
</p>
<p>Fig.10.7: Stepwise bracketing of a minimum with the combined method of Brent. The ini-
tial interval (large circles) is reduced with each step (small circles). Steps are carried out
according to the golden section (GS) or by quadratic interpolation (QI).
</p>
<p>ferent strategies to find a minimum in n dimensional space, which we discuss
in Sects. 10.9&ndash;10.15. A different type of strategy is the basis of the simplex
method presented in the next section.
</p>
<p>10.8 Simplex Minimization in n Dimensions
</p>
<p>A simple and very elegant (although relatively slow) procedure for determin-
ing the minimum of a function of several variables is the simplex method by
</p>
<p>The class MinDir is the essential tool used to realize a number of dif-</p>
<p/>
</div>
<div class="page"><p/>
<p>282 10 Function Minimization
</p>
<p>NELDER and MEAD [14]. The variables x1,x2, . . . ,xn define an n-dimensional
space. A simplex is defined in this space by n+1 points xi ,
</p>
<p>xi = (x1i,x2i, . . . ,xni) . (10.8.1)
</p>
<p>A simplex in two dimensions is a triangle with the corner points x1,x2,x3.
We use yi to designate the value of the function at xi and use particular
</p>
<p>indices for labeling special points xi . At the point xH the value of the function
is highest, i.e., yH &gt; yi, i �= H . At the point xh it is higher than at all other
points except xH (yh &gt; yi , i �= H , i �= h) and at the point xℓ it is lowest
(yℓ &lt; yi , i �= ℓ). The simplex is now changed step by step. In each substep one
of four operations takes place, these being reflection, stretching, flattening, or
contraction of the simplex (cf. Fig. 10.8).
</p>
<p>xH
</p>
<p>xH
</p>
<p>xx
</p>
<p>x
</p>
<p>xl
</p>
<p>xr
</p>
<p>xr
</p>
<p>xe
</p>
<p>xH
</p>
<p>(d)
</p>
<p>xa
</p>
<p>(b)
</p>
<p>(a) (c)
</p>
<p>Fig.10.8: Transformation of a simplex (triangle with thick border) into a new form (triangle
with thin border) by means of a reflection (a), stretching (b), flattening (c), and contrac-
tion (d).
</p>
<p>Using x to designate the center-of-gravity of the (hyper)surface of the
simplex opposite to xH ,
</p>
<p>x= 1
N&minus;1
</p>
<p>&sum;
</p>
<p>i �=H
xi , (10.8.2)
</p>
<p>the reflection of xH is
</p>
<p>xr = (1+α)x&minus;αxH (10.8.3)
</p>
<p>with α &gt; 0 as the coefficient of reflection. The reflected simplex differs from
the original one only in that xH is replaced by xr .
</p>
<p>A stretching of the simplex consists of replacing xH by
</p>
<p>xe = γ xr + (1&minus;γ )x (10.8.4)</p>
<p/>
</div>
<div class="page"><p/>
<p>10.8 Simplex Minimization in n Dimensions 283
</p>
<p>with the coefficient of stretching γ . One therefore chooses (for γ &gt; 1) a point
along the line joining xH and xr , which is still further than the point xr .
</p>
<p>In a flattening, xH is replaced by a point lying on the line joining xH
and x. This point is located between the two points,
</p>
<p>xa = βxH + (1&minus;β)x . (10.8.5)
</p>
<p>The coefficient of flattening β is in the range 0 &lt; β &lt; 1.
In the three operations discussed up to now only one point of the simplex
</p>
<p>is changed, that being the point xH , which corresponds to the highest value
of the function. The point is displaced along the line by xH and x. After the
displacement it either lies on the same side of x (flattening) or on the other
side of x (reflection) or even far beyond x (stretching). In contrast to these op-
erations, in a contraction, all points but one are replaced. The point xℓ where
the function has its lowest value is retained. All remaining points are moved
to the midpoints of the line segments joining them with xℓ
</p>
<p>xci = (xi +xℓ)/2 , i �= ℓ . (10.8.6)
</p>
<p>For the original simplex and for each one created by an operation, the
points x and xr and the corresponding function values y and yr are computed.
The next operation is determined as follows:
</p>
<p>(a) If yr &lt; yℓ, a stretching is attempted. If this gives ye &lt; yℓ, then the
stretching is carried out. Otherwise one performs a reflection.
</p>
<p>(b) For yr &gt;yh a reflection is carried out if yr &lt;yH . Otherwise the simplex is
unchanged. In each case this is followed by a flattening. If one obtains
as a result of the flattening a point xa for which the function value
is not less than both yH and y, the flattening is rejected, and instead a
contraction is carried out.
</p>
<p>After every step we examine the quantity
</p>
<p>r = | yH &minus;yℓ || yH | + | yℓ |
. (10.8.7)
</p>
<p>If this falls below a given value, the procedure is terminated and we regard xℓ
as the point where the function has its minimum.
</p>
<p>The class MinSim determines the minimum of a function of n variables
by the simplex method. The program is illustrated in the example of Fig. 10.9.
The triangle with the dark border is the initial simplex. The sequence of
triangles with thin borders starting from it correspond to the individual trans-
formations. One clearly recognizes as the first steps: stretching, stretching,
reflection, reflection, flattening . . .. The simplex first finds its way into the</p>
<p/>
</div>
<div class="page"><p/>
<p>284 10 Function Minimization
</p>
<p>Fig.10.9:Determining the minimum of a function of two variables with the simplex method.
The function is shown by contour lines on which the function is constant. The function is
highest on the outermost contour. The minimum is at the position of the small circle within
the innermost contour. Each simplex is a triangle. The initial simplex is marked by thicker
lines.
</p>
<p>&ldquo;valley&rdquo; of the function and then runs along the bottom of the valley towards
the minimum. It is reshaped in the process so that it is longest in the direction
in which it is progressing, and in this way it can also pass through narrow
valleys. It is worth remarking that this method, which is obtained by consid-
ering the problem in two and three dimensions, also works in n dimensions
and also in this case possesses a certain descriptiveness.
</p>
<p>10.9 Minimization Along the Coordinate Directions
</p>
<p>Some of the methods for searching for a minimum in an n-dimensional space
are based on the following principle. Starting from a point x0 one searches for
the minimum along a given direction in the space. Next one minimizes from
there along another direction and finds a new minimum, etc. Within this gen-
eral framework various strategies can be developed to choose the individual
directions.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.10 Conjugate Directions 285
</p>
<p>The simplest strategy consists in the choice of the coordinate directions in
the space of the n variables xi . We can label the corresponding basis vectors
with e1,e2, . . . ,en, and they are then chosen in order as directions. After en
one begins again with e1,e2, . . .. A partial sequence of minimizations along
all coordinate directions starting from x0 gives the point x1. After a new partial
sequence one obtains x2, etc.
</p>
<p>The procedure is ended successfully when for the values of the function
M(xn) and M(xn&minus;1) for two successive steps one has
</p>
<p>M(xn&minus;1)&minus;M(xn) &lt; ε|M(xn)|+ t , (10.9.1)
</p>
<p>i.e., a condition corresponding to (10.1.21) for given values of ε and t .
We compare here, however, the value of the function M and not the indepen-
dent variables x. Otherwise we would have to compute the distance between
two points in an n-dimensional space.
</p>
<p>Figure 10.10 shows the minimization of the same function as in Fig. 10.9
with the coordinate-direction method. After the first comparatively large step,
which leads into the &ldquo;valley&rdquo; of the function, the following steps are quite
small. The individual directions are naturally perpendicular to each other. The
&ldquo;best&rdquo; point at each step moves along a staircase-like path along the floor of
the valley towards the minimum.
</p>
<p>10.10 Conjugate Directions
</p>
<p>The slow convergence seen in Fig. 10.10 clearly stems from the fact that when
minimizing along one direction, one loses the result of the minimization with
respect to another direction. We will now try to choose the directions in such
a way that this does not happen. For this we suppose for simplicity that the
function is a quadratic form (10.1.10). Its gradient at the point x is then given
by (10.1.12),
</p>
<p>&nabla;M =&minus;b+Ax . (10.10.1)
The change of the gradient in moving by Δx is
</p>
<p>Δ(&nabla;M)=&nabla;M(x+Δx)&minus;&nabla;M(x)=Ax+AΔx&minus;Ax=AΔx . (10.10.2)
</p>
<p>This expression is a vector that gives the direction of change of the gradient
when the argument is moved in the direction Δx.
</p>
<p>If a minimization has been carried out along a direction p, then the
reduction in M with respect to p is retained when one moves in a direction q
such that the gradient only changes perpendicular to p, i.e., when one has
</p>
<p>p &middot; (Aq)= pTAq= 0 . (10.10.3)</p>
<p/>
</div>
<div class="page"><p/>
<p>286 10 Function Minimization
</p>
<p>Fig.10.10: Minimization along the coordinate directions. The starting point is shown by the
larger circle, and the end by the smaller circle. The line shows the results of the individual
steps.
</p>
<p>The vectors p and q are said to be conjugate to each other with respect to the
positive-definite matrix A. If one has n variables, i.e., if A is an n&times;n matrix,
then one can find in general n conjugate linearly independent vectors.
</p>
<p>POWELL [15] has given a method to find a set of conjugate directions
for a function described by a quadratic form. For this one chooses as a first
set of directions n linearly independent unit vectors pi , e.g., the coordinate
directions pi = ei . Starting from a point x0 one finds successively the minima
in the directions pi . The results can be labeled by
</p>
<p>a1 = α1p1 +α2p2 +&middot;&middot; &middot;+αnpn ,
a2 = α2p2 +&middot;&middot; &middot;+αnpn ,
...
</p>
<p>an = αnpn .
</p>
<p>Here a1 is the vector representing all n substeps, a2 contains all of the steps
except the first one, and an is just the last substep. The sum of the n substeps
leads then from the point x0 to
</p>
<p>x1 = x0 +a1 .</p>
<p/>
</div>
<div class="page"><p/>
<p>10.11 Minimization Along Chosen Directions 287
</p>
<p>The direction a1 describes the average direction of the first n substeps. There-
fore we carry out a step in the direction a1, call the result again x0, determine
a new set of directions
</p>
<p>q1 = p2 ,
q2 = p3 ,
...
</p>
<p>qn&minus;1 = pn ,
qn = a1/|a1| ,
</p>
<p>then call these qi again pi and proceed as above. As was shown by POWELL
[15], the directions after n steps, i.e., n(n+ 1) individual minimizations, are
individually conjugate to each other if the function is a quadratic form.
</p>
<p>10.11 Minimization Along Chosen Directions
</p>
<p>The procedure given at the end of the last section contains, however, the
danger that the directions p1, . . . ,pn can become almost linearly dependent,
because at every step p1 is rejected in favor of a1/|a1|, and these directions
need not be very different from step to step. Powell has therefore suggested
not to replace the direction p1 by a1/|a1|, but rather to replace the direc-
tion pmax, along which the greatest reduction of the function took place.
This sounds at first paradoxical, since what is clearly the best direction is
replaced by another. But since out of all the directions pmax gave the largest
contribution towards reducing the function, a1/|a1| has a significant compo-
nent in the direction pmax. By retaining these two similar directions one would
increase the danger of a linear dependence.
</p>
<p>In some cases, however, after completing a step we will retain the old
directions without change. We denote by x0 the point before carrying out the
step in progress, by x1 the point obtained by this step, by
</p>
<p>xe = x1 + (x1 &minus;x0)= 2x1 &minus;x0 (10.11.1)
</p>
<p>an extrapolated point that lies in the new direction x1 &minus;x0 from x0 but is still
further than x1, and by M0,M1, and Me the corresponding function values. If
</p>
<p>Me &ge;M0 , (10.11.2)
</p>
<p>then the function no longer decreases significantly in the direction (x0 &minus; x1).
We then remain with the previous directions.
</p>
<p>Further we denote by ΔM the greatest change in M along a direction in
the step in progress and compute the quantity</p>
<p/>
</div>
<div class="page"><p/>
<p>288 10 Function Minimization
</p>
<p>T = 2(M0 &minus;2M1+Me)(M0 &minus;M1 &minus;ΔM)2&minus; (M0 &minus;Me)2ΔM . (10.11.3)
</p>
<p>If
T &ge; 0 ,
</p>
<p>then we retain the old directions. This requirement is satisfied if either the first
or second factor in the first term of (10.11.3) becomes large. The first factor
(M0 &minus;2M1 +Me) is proportional to a second derivative of the function M . If
it is large (compared to the first derivative in meaningful units), then we are
already close to the minimum. The second factor (M0 &minus;M1 &minus;ΔM)2 is large
when the reduction of the function M0&minus;M1, with the contribution ΔM , does
not stem mainly from a single direction.
</p>
<p>The class MinPow determines the minimum of a function of n variables
by successive minimization along chosen directions according to Powell.
Figure 10.11 shows the advantages of that method. One can see a significantly
faster convergence to the minimum compared to that of Fig. 10.10.
</p>
<p>10.12 Minimization in the Direction of Steepest Descent
</p>
<p>In order to get from a point x0 to the minimum of a function M(x), it is
sufficient to always follow the negative gradient b(x) = &minus;&nabla;M(x). It is thus
obvious that one should look for the minimum along the direction &nabla;M(x0).
One calls this point x1, searches then for the minimum along &nabla;M(x1), and so
forth, until the termination requirement (10.9.1) is satisfied.
</p>
<p>A comparison of the steps taken with this method, shown in Fig. 10.12,
with those from minimization along a coordinate direction (Fig. 10.10) shows,
however, a surprising similarity. In both cases, successive steps are perpendic-
ular to each other. Thus the directions cannot be fitted to the function in the
course of the procedure and convergence is slow.
</p>
<p>The initially surprising fact that successive gradient directions are perpen-
dicular to each other stems from the construction of the procedure. Searching
from x1 for the minimum in the direction b0 means that the derivative in the
direction b0 vanishes at the point x1,
</p>
<p>b0 &middot;&nabla;M(x1)= 0 ,
</p>
<p>and thus the gradient is perpendicular to b0.
</p>
<p>10.13 Minimization Along Conjugate Gradient Directions
</p>
<p>We take up again the idea of conjugate directions from Sect. 10.10. We
construct, starting from an arbitrary vector g1 = h1, two sequences of vectors,</p>
<p/>
</div>
<div class="page"><p/>
<p>10.13 Minimization Along Conjugate Gradient Directions 289
</p>
<p>Fig.10.11:Minimization along a chosen direction with the method of Powell.
</p>
<p>Fig.10.12:Minimization in the direction of steepest descent.</p>
<p/>
</div>
<div class="page"><p/>
<p>290 10 Function Minimization
</p>
<p>gi+1 = gi &minus;λiAhi , i = 1,2, . . . , (10.13.1)
hi+1 = gi+1 +γihi , i = 1,2, . . . , (10.13.2)
</p>
<p>with
</p>
<p>λi =
gTi gi
</p>
<p>gTi Ahi
, γi =
</p>
<p>gTi+1Ahi
</p>
<p>hTi Ahi
. (10.13.3)
</p>
<p>Thus one has
gTi+1gi = 0 , (10.13.4)
</p>
<p>hTi+1Ahi = 0 . (10.13.5)
This means that successive vectors g are orthogonal, and successive vectors h
are conjugate to each other.
</p>
<p>Rewriting the relation (10.13.3) gives
</p>
<p>γi =
gTi+1gi+1
gTi gi
</p>
<p>= (gi+1 &minus;gi)
Tgi+1
</p>
<p>gTi gi
, (10.13.6)
</p>
<p>λi =
gTi hi
</p>
<p>hTi Ahi
. (10.13.7)
</p>
<p>We now try to construct the vectors gi and hi without explicit knowledge of
the Hessian matrix A. To do this we again assume that the function to be
minimized is a quadratic form (10.1.10). At a point xi we define the vector
gi =&minus;&nabla;M(xi). If we now search for the minimum starting from xi along the
direction hi , we find it at xi+1 and construct there gi+1 =&minus;&nabla;M(xi+1). Then
gi+1 and gi are orthogonal, since from (10.1.12) one has
</p>
<p>gi =&minus;&nabla;M(xi)= b&minus;Axi
</p>
<p>and
</p>
<p>gi+1 =&minus;&nabla;M(xi+1)= b&minus;A(xi +λihi)= gi &minus;λiAhi . (10.13.8)
</p>
<p>Here λi has been chosen such that xi+1 is the minimum along the direction
hi . This means that there the gradient is perpendicular to hi ,
</p>
<p>hTi &nabla;M(xi+1)=&minus;hTi gi+1 = 0 . (10.13.9)
</p>
<p>Substituting into (10.13.8) gives indeed
</p>
<p>0 = hTi gi+1 = hTi gi &minus;λihTi Ahi ,
</p>
<p>in agreement with (10.13.7).</p>
<p/>
</div>
<div class="page"><p/>
<p>10.13 Minimization Along Conjugate Gradient Directions 291
</p>
<p>From these results we can now construct the following algorithm. We
begin at x0, construct there the gradient &nabla;M(x0), and set its negative equal to
the two vectors
</p>
<p>g1 =&minus;&nabla;M(x0) , h1 =&minus;&nabla;M(x0) .
</p>
<p>We minimize along h1. At the point of the minimum x1 we construct g2 =
&minus;&nabla;M(x1) and compute from (10.13.6)
</p>
<p>γ1 =
(g2 &minus;g1)Tg1
</p>
<p>gT1g1
</p>
<p>and from (10.13.2)
h2 = g1 +γ1h1 .
</p>
<p>From x1 we then minimize along h2, and so forth.
The class MinCjg determines the minimum of a function of n variables
</p>
<p>by successive minimization along conjugate gradient directions. A comparison
of Fig. 10.13 with Fig. 10.12 shows the superiority of the method of conjugate
gradients over determination of direction according to steepest descent, espe-
cially near the minimum.
</p>
<p>Fig.10.13:Minimization along conjugate gradient directions.</p>
<p/>
</div>
<div class="page"><p/>
<p>292 10 Function Minimization
</p>
<p>10.14 Minimization with the Quadratic Form
</p>
<p>If the function to be minimized M(x) is of the simple form (10.1.10), then
the position of the minimum is given directly by (10.1.13). Otherwise one can
always expand M(x) about a point x0,
</p>
<p>M(x)=M(x0)&minus;b(x&minus;x0)+
1
</p>
<p>2
(x&minus;x0)TA(x&minus;x0)+&middot;&middot; &middot; (10.14.1)
</p>
<p>with
</p>
<p>b=&minus;&nabla;M(x0) , Aik =
&part;2M
</p>
<p>&part;xi&part;xk
(10.14.2)
</p>
<p>and obtain as an approximation for the minimum
</p>
<p>x1 = x0 +A&minus;1b . (10.14.3)
</p>
<p>One can now compute again b and A as derivatives at the point x1 and from
this obtain a further approximation x2 according to (10.14.3), and so forth.
</p>
<p>For the case where the approximation (10.14.2) gives a good description
of the function M(x), the procedure converges quickly, since it tries to jump
directly to the minimum. Otherwise it might not converge at all. We have
already discussed the difficulties for the corresponding one-dimensional case
in Sect. 10.1 with Fig. 10.1.
</p>
<p>The class MinQdr finds the minimum of a function of n variables with
the quadratic form. Figure 10.14 illustrates the operation of the method. One
can observe that the minimum is in fact reached in very few steps.
</p>
<p>10.15 Marquardt Minimization
</p>
<p>MARQUARDT [16] has given a procedure that combines the speed of mini-
mization with the quadratic form near the minimum with the robustness of
the method of steepest descent, where one finds the minimum even starting
from a point far away. It is based on the following simple consideration.
</p>
<p>The prescription (10.14.3), written as a computation of the ith approxi-
mation for the position of the minimum,
</p>
<p>xi = xi&minus;1 +A&minus;1b , (10.15.1)
</p>
<p>means that one obtains xi from the point xi&minus;1 by taking a step along the vector
A&minus;1b. Here b = &minus;&nabla;M(xi&minus;1) is the negative gradient, i.e., a vector in the
direction of steepest descent of the function M at the point xi&minus;1. If in (10.15.1)
one had the unit matrix multiplied by a constant instead of the matrix A, i.e.,
if instead of (10.15.1) one were to use the prescription</p>
<p/>
</div>
<div class="page"><p/>
<p>10.15 Marquardt Minimization 293
</p>
<p>Fig.10.14:Minimization with quadratic form.
</p>
<p>xi = xi&minus;1 + (λI)&minus;1b , (10.15.2)
</p>
<p>then one would step by the vector b/λ from xi&minus;1. This is a step in the direction
of steepest descent of the function, which is smaller for larger values of the
constant λ. A sufficiently small step in the direction of steepest descent is,
however, always a step towards the minimum (at least when one is still in
the &ldquo;approach&rdquo; to the minimum, i.e., in the one-dimensional case of Fig. 10.1,
between the two maxima). The Marquardt procedure consists of interpolat-
ing between the prescriptions (10.15.1) and (10.15.2) in such a way that the
function M is reduced with every step, and such that the fast convergence
of (10.15.1) is exploited as much as possible.
</p>
<p>In place of (10.15.1) or (10.15.2) one computes
</p>
<p>xi = xi&minus;1 + (A+λI)&minus;1b . (10.15.3)
</p>
<p>Here λ is determined in the following way. One first chooses a fixed number
ν &gt; 1 and denotes by λ(i&minus;1) the value of λ from the previous step. As an initial
value one chooses, e.g., λ(0) = 0.01. The value obtained from (10.15.3) of xi
clearly depends on λ. One computes two points xi(λ(i&minus;1)) and xi(λ(i&minus;1)/ν),
where for λ one chooses the values λ(i&minus;1) and λ(i&minus;1)/ν, and the corresponding
function values Mi = M(xi(λ(i&minus;1))) and M(ν)i = M(xi(λ(i&minus;1)/ν)). These</p>
<p/>
</div>
<div class="page"><p/>
<p>294 10 Function Minimization
</p>
<p>are compared with the function value Mi&minus;1 = M(xi&minus;1). The result of the
comparison determines what happens next. The following cases are possible:
</p>
<p>(i) M(ν)i &le;Mi&minus;1:
One sets xi = xi(λ(i&minus;1)/ν) and λ(i) = λ(i&minus;1)/ν.
</p>
<p>(ii) M(ν)i &gt;Mi&minus;1 and Mi &le;Mi&minus;1:
One sets xi = xi(λ(i&minus;1)) and λ(i) = λ(i&minus;1).
</p>
<p>(iii) M(ν)i &gt;Mi&minus;1 and Mi &gt;Mi&minus;1:
One replaces λ(i&minus;1) by λ(i&minus;1)ν and repeats the computation of xi(λ(i&minus;1)
</p>
<p>/ν) and xi(λ(i&minus;1)) and the corresponding function values, and repeats
the comparisons.
</p>
<p>In this way one ensures that the function value in fact decreases with each
step and that the value of λ is always as small as possible when adjusted
to the local situation. Clearly (10.15.3) becomes (10.15.1) for λ &rarr; 0, i.e.,
it describes minimization with the quadratic form. For very large values of
λ, Eq. (10.15.3) becomes the relation (10.15.2), which prescribes a small but
sure step in the direction of steepest descent.
</p>
<p>Fig.10.15:Minimization with the Marquardt procedure.
</p>
<p>The class MinMar finds the minimum of a function of n variables
by Marquardt minimization. In Fig. 10.15 one sees the operation of the</p>
<p/>
</div>
<div class="page"><p/>
<p>10.16 On Choosing a Minimization Method 295
</p>
<p>Marquardt method. It follows a systematic path to the minimum. Compar-
ing with Fig. 10.14 one sees that the method of the quadratic form converged
in fewer steps. The Marquardt procedure, however, leads to the minimum in
many cases where the method of the quadratic form would fail, e.g., in cases
with poorly determined initial values.
</p>
<p>10.16 On Choosing a Minimization Method
</p>
<p>Given the variety of minimization methods, the user is naturally faced with
</p>
<p>the question of choosing a method appropriate to the task. Before we give
</p>
<p>recommendations for this, we will first recall the various methods once again.
</p>
<p>The simplex method (routine MinSim) is particularly robust. Only func-
</p>
<p>tion values M(x) are computed. The method is slow, however. Faster, but still
</p>
<p>quite robust is minimization along a chosen direction (routine MinPow). This
</p>
<p>also requires only function values.
</p>
<p>The method of conjugate gradients (routine MinCjg) requires, as the
</p>
<p>name indicates, the computation of not only the function, but also its gradient.
</p>
<p>The number of iteration steps is, however, approximately equal to that of
</p>
<p>For minimization with the quadratic form (routine MinQdr) and Mar-
</p>
<p>quardt minimization (routine MinMar) one requires in addition the Hessian
</p>
<p>matrix of second derivatives. The derivatives are computed numerically with
</p>
<p>utility routines. The user can replace these utility routines by routines in which
</p>
<p>the analytic formulas for the derivatives are programmed. If the starting val-
</p>
<p>ues are sufficiently accurate,
</p>
<p>convergence is slower for
</p>
<p>often converges when starting from values from which
</p>
<p>From these characteristics of the methods we arrive at the following
</p>
<p>recommendations:
</p>
<p>1. For problems that need only to be solved once or not many times, that
</p>
<p>is, in which the computing time does not play an important role, one
</p>
<p>should choose
</p>
<p>2. For problems occurring repeatedly (with different numerical values)
</p>
<p>one should use
</p>
<p>proximation,
</p>
<p>3. For repeated problems the derivatives needed for
</p>
<p>should be calculated analytically. Although this entails additional pro-
</p>
<p>gramming work, one gains precision compared to numerical derivatives
</p>
<p>and saves in many cases computing time.
</p>
<p>MinPow.
</p>
<p>MinQdr converges after just a few steps. The
</p>
<p>MinMar. The method is, however, more robust; it
</p>
<p>MinQdr would fail.
</p>
<p>MinSim or MinPow.
</p>
<p>MinMar. If one always has an accurate starting ap-
</p>
<p>MinQdr can be used.
</p>
<p>MinMar or MinQdr</p>
<p/>
</div>
<div class="page"><p/>
<p>296 10 Function Minimization
</p>
<p>At this point we should make an additional remark about the comparison
of the minimization methods of this chapter with the method of least squares
from Chap. 9. The method of least squares is a special case of minimization.
The function to be minimized is a sum of squares, e.g., (9.1.8), or the gen-
eralization of a sum of squares, e.g., (9.5.9). In this generalized sum of
squares there appears the matrix of derivatives A. These are not, however,
the derivatives of the function to be minimized, but rather derivatives of a
function f, which characterizes the problem under consideration; cf. (9.5.2).
Second derivatives are never needed. In addition, if one uses the singular value
decomposition, as has been done in our programs in Chap. 9 to solve the least-
squares problem, one works in numerically critical cases with a considerably
higher accuracy than in the computation of sums of squares (cf. Sect. A.13,
particularly Example A.4).
</p>
<p>Least-squares problems should therefore always be carried out with
the routines of Chap. 9. This applies particularly to problems of fitting of
functions like those in the examples of Sect. 9.6, when one has many mea-
sured points. The matrix A then contains many rows, but few columns. In
computing the function to be minimized the product ATA is encountered, and
one is threatened with the above mentioned loss of accuracy in comparison to
the singular value decomposition.
</p>
<p>10.17 Consideration of Errors
</p>
<p>In data analysis minimization procedures are used for determining best esti-
mates x̃ for unknown values x. The function to be minimized M(x) is here
usually a sum of squares (Chap. 9) or a log-likelihood function (multiplied
by &minus;1) as in Chap. 7. In using the equations from Chap. 7 one must note,
however, that there the n-vector that was called λ is now denoted by x. The
variables in Chap. 7 that were called x(i) are the measured values (usually
called y in Chap. 9).
</p>
<p>Information on the error of x̃ is obtained directly from results of Sects. 9.7,
9.8, and 9.13 by defining
</p>
<p>Hik =
(
</p>
<p>&part;2M
</p>
<p>&part;xi&part;xk
</p>
<p>)
</p>
<p>x=̃x
(10.17.1)
</p>
<p>as the elements of the symmetric matrix of second derivatives (the Hessian
matrix) of the minimization function. Here one must note that the factor fQL
takes on the numerical value
</p>
<p>fQL = 1 (10.17.2)
when the function being minimized is a sum of squares. If the function is a
log-likelihood function (times &minus;1) then this must be set equal to</p>
<p/>
</div>
<div class="page"><p/>
<p>10.17 Consideration of Errors 297
</p>
<p>fQL = 1/2 . (10.17.3)
</p>
<p>1. Covariance matrix. Symmetric errors. The covariance matrix of x̃ is
</p>
<p>Cx̃ = 2fQLH&minus;1 . (10.17.4)
</p>
<p>The square roots of the diagonal elements are the (symmetric) errors
</p>
<p>Δx̃i =
&radic;
cii . (10.17.5)
</p>
<p>It is only meaningful to give the covariance matrix if the measurement
errors are small and/or there are many measurements, i.e., the requirements
for (7.5.8) are fulfilled.
</p>
<p>2. Confidence ellipsoid. Symmetric confidence limits. The covariance
matrix defines the covariance ellipsoid; cf. Sects. 5.10 and A.11. Its center
is given by x = x̃. The probability that the true value of x is contained inside
the ellipsoid is given by (5.10.20). The ellipsoid for which this probability has
a given value W , the confidence level, is given by the confidence matrix
</p>
<p>C
(W)
x̃ = χ2W (nf )Cx̃ . (10.17.6)
</p>
<p>Here χ2W (nf ) is the quantile of the χ
2-distribution for nf degrees of freedom
</p>
<p>and probability P =W ; cf. (5.10.19) and (C.5.3). The number of degrees of
freedom nf is equal to the number of measured values minus the number of
parameters determined by the minimization. The square roots of the diagonal
elements of C(W)x̃ are the distances from the symmetric confidence limits,
</p>
<p>x
(W)
i&plusmn; = x̃i &plusmn;
</p>
<p>&radic;
c
(W)
ii . (10.17.7)
</p>
<p>The class MinCov yields the covariance or confidence matrix, respectively,
for parameters determined by minimization.
</p>
<p>3. Confidence region. If giving a covariance or confidence ellipsoid is not
meaningful, it is still possible to give a confidence region with confidence
level W . It is determined by the hypersurface
</p>
<p>M(x)=M(̃x)+χ2W (nf )fQL . (10.17.8)
</p>
<p>With the help of the following routine a contour of a cross section through
this hypersurface is drawn in a plane that contains the point x̃ and is parallel
to (xi,xj ). Here xi and xj are two components of the vector of parameters x.
The boundary of the confidence region in a plane spanned by two parameters,
which were found by minimization, can be graphically shown with the method
DatanGraphics.drawContour, cf. Examples 10.1&ndash;10.3 and Example
Programs 10.2&ndash;10.4.</p>
<p/>
</div>
<div class="page"><p/>
<p>298 10 Function Minimization
</p>
<p>4. Asymmetric errors and confidence limits. If the confidence region is
not an ellipsoid, the asymmetric confidence limits for the variable xi can still
be determined by
</p>
<p>min
{
M(̃x); xi = x(W)i&plusmn;
</p>
<p>}
=M(̃x)+χ2W (nf )fQL . (10.17.9)
</p>
<p>The differences
</p>
<p>Δx
(W)
i+ = x
</p>
<p>(W)
i+ &minus; x̃i ,
</p>
<p>Δx
(W)
i&minus; = x̃i &minus;x
</p>
<p>(W)
i&minus; (10.17.10)
</p>
<p>are the (asymmetric) distances from the confidence limits. If one takes χ2W (nf )
= 1, the asymmetric errors Δxi+ and Δxi&minus; are obtained. The class MinAsy
yields asymmetric errors or the distances from the confidence limits, respec-
tively, for parameters, determined by minimization.
</p>
<p>10.18 Examples
</p>
<p>Example 10.1: Determining the parameters of a distribution from the
elements of a sample with the method of maximum likelihood
</p>
<p>Suppose one has N measurements y1, y2, . . ., yn that can be assumed to
come from a normal distribution with expectation value a = x1 and standard
deviation σ = x2. The likelihood function is
</p>
<p>L=
N&prod;
</p>
<p>i=1
</p>
<p>1
</p>
<p>x2
&radic;
</p>
<p>2π
exp
</p>
<p>{
&minus;(yi &minus;x1)
</p>
<p>2
</p>
<p>2x22
</p>
<p>}
(10.18.1)</p>
<p/>
</div>
<div class="page"><p/>
<p>10.18 Examples 299
</p>
<p>and its logarithm is
</p>
<p>ℓ=&minus;
N&sum;
</p>
<p>i=1
</p>
<p>(yi &minus;x1)2
</p>
<p>2x22
&minus;N ln{x2
</p>
<p>&radic;
2π} . (10.18.2)
</p>
<p>The task of determining the maximum-likelihood estimators x̃1, x̃2 has already
been solved in Example 7.8 by setting the analytically calculated deriva-
tives of the function ℓ(x) to zero. Here we will accomplish this by means
of numerical minimization of ℓ(x). For this we must first provide a function
that computes the function to be minimized,
</p>
<p>M(x)=&minus;ℓ(x) .
</p>
<p>This simple example is implemented in Example Programs 10.2 and 10.3.
The results of the minimization of this user function are shown in Fig. 10.16
for two samples. Confidence regions and covariance ellipses agree reasonably
well with each other. The agreement is better for the larger sample.
</p>
<p>Example 10.2: Determination of the parameters of a distribution from the
histogram of a sample by maximizing the likelihood
</p>
<p>Instead of the original sample y1, y2, . . ., yN as in Example 10.1, one often
uses the corresponding histogram. Denoting by ni the number of observations
that fall into the interval centered about the point ti with width Δt ,
</p>
<p>ti &minus;Δt/2 &le; y &lt; ti +Δt/2 , (10.18.3)
</p>
<p>the histogram is given by the pairs of numbers
</p>
<p>(ti,ni) , i = 1,2, . . . ,n . (10.18.4)
</p>
<p>If the original sample is taken from a normal distribution with expectation
value x1 = a and standard deviation x2 = σ , i.e., from the probability density
</p>
<p>f (t;x1,x2)=
1
</p>
<p>x2
&radic;
</p>
<p>2π
exp
</p>
<p>{
&minus;(t&minus;x1)
</p>
<p>2
</p>
<p>2x2
</p>
<p>}
, (10.18.5)
</p>
<p>then one might expect the ni(ti) (at least in the limit N &rarr;&infin;) to be given by
</p>
<p>gi =NΔtf (ti;x1,x2) . (10.18.6)
</p>
<p>The quantities ni(ti) are integers, which are clearly not equal in general to gi .
We can, however, regard each ni(ti) as a sample of size one from a Poisson
distribution with the expectation value
</p>
<p>λi = gi . (10.18.7)</p>
<p/>
</div>
<div class="page"><p/>
<p>300 10 Function Minimization
</p>
<p>Fig.10.16:Determination of the parameters x1 (mean) and x2 (width) of a Gaussian distribu-
tion by maximization of the log-likelihood function of a sample. The plots on the left show
two different samples, marked as one-dimensional scatter plots (tick marks) on the y axis. The
curves f (y) are Gaussian distributions with the fitted parameters. The right-hand plots show
in the (x1,x2) plane the values of the parameters obtained with symmetric errors and covari-
ance ellipses, as well as the confidence region for χ2W = 1 and the corresponding confidence
boundaries (horizontal and vertical lines).
</p>
<p>The a posteriori probability to observe the value ni(ti) is clearly
</p>
<p>1
</p>
<p>ni !
λ
ni
i e
</p>
<p>&minus;λi . (10.18.8)
</p>
<p>The likelihood function for the observation of the entire histogram is
</p>
<p>L=
n&prod;
</p>
<p>i=1
</p>
<p>1
</p>
<p>ni !
λ
ni
i e
</p>
<p>&minus;λi , (10.18.9)
</p>
<p>and its logarithm is
</p>
<p>ℓ=&minus;
n&sum;
</p>
<p>i=1
lnni !+
</p>
<p>n&sum;
</p>
<p>i=1
ni lnλi &minus;
</p>
<p>n&sum;
</p>
<p>i=1
λi . (10.18.10)
</p>
<p>If we use for λi the notation of (10.18.7) and find the minimum of &minus;ℓ with
respect to x1, x2, then this determines the best estimates of the parame-
ters x1, x2. Of course the same procedure can be applied not only in the</p>
<p/>
</div>
<div class="page"><p/>
<p>10.18 Examples 301
</p>
<p>case of a simple Gaussian distribution, but for any distribution that depends
on parameters. One must simply use the corresponding probability density
in (10.18.5) and (10.18.6). In the user function given below one must change
only one instruction in order to replace the Gaussian by another distribu-
tion. This example is implemented in Example Program 10.4. There the user
</p>
<p>Fig.10.17: Determination of the parameters x1 (mean) and x2 (width) of a Gaussian distri-
bution by maximizing the log-likelihood function of a histogram. The left-hand plots show
two histograms corresponding to the samples from Fig. 10.16. The curves are the Gaussian
distributions normalized to the histograms. The right-hand plots show the symmetric errors,
covariance ellipse, and confidence region represented as in Fig. 10.16.
</p>
<p>The results of the minimization with this user function are shown in
Fig. 10.17 for two histograms, which are based on the samples from Fig. 10.16.
The results are very similar to those from Example 10.1. The errors of
the parameters, however, are somewhat larger. This is to be expected, since
some information is necessarily lost when constructing a histogram from the
sample.
</p>
<p>A histogram can be viewed as a sample in a compressed representation.
The compression becomes greater as the bin width of the histogram increases.
This is made clear in Fig. 10.18. One sees that for the same sample, the errors
of the determined parameters increase for greater bin width. The effect is rel-
atively small, however, for the relatively large sample size in this case.
</p>
<p>function carries the name MinLogLikeHistPoison.</p>
<p/>
</div>
<div class="page"><p/>
<p>302 10 Function Minimization
</p>
<p>Fig.10.18: As in Fig. 10.17, but for histograms with different interval widths of the same
sample.
</p>
<p>Example 10.3: Determination of the parameters of a distribution from the
histogram of a sample by minimization of a sum of squares
</p>
<p>If the bin contents ni of a histogram (10.18.4) are sufficiently large, then
the statistical fluctuations of each ni can be approximated by a Gaussian
distribution with the standard deviation
</p>
<p>Δni =
&radic;
ni . (10.18.11)
</p>
<p>(See Sect. 6.8.) The weighted sum of squares describing the deviation of the
histogram contents ni from the expected values gi from (10.18.6) is then
</p>
<p>Q=
N&sum;
</p>
<p>i=1
</p>
<p>(ni &minus;gi)2
ni
</p>
<p>. (10.18.12)</p>
<p/>
</div>
<div class="page"><p/>
<p>10.19 Java Classes and Example Programs 303
</p>
<p>When carrying out the sum one must take care (in contrast to Example 10.2)
that empty bins (ni = 0) are not included. Even better is to not include bins in
the sum even with low bin contents, e.g., ni &lt; 4.
</p>
<p>Also this example is implemented in Example Program 10.4. Here gi is
given by (10.18.6) as in the previous example. The sum is carried out over all
</p>
<p>The statistical fluctuations of the bin contents are taken as approximately
Gaussian.
</p>
<p>Figure 10.19 shows the results obtained by minimizing the quadratic
sum for the histogram in Fig. 10.18. Since the histograms are based on the
same sample, the values ni decrease for decreasing bin width, whereby the
requirement for using the sum of squares becomes less well fulfilled. Thus we
cannot trust as much the results nor the errors for smaller bin width. One sees,
however, that the errors given by the procedure in fact increase for decreasing
bin width.
</p>
<p>We emphasize that the determination of parameters from a histogram by
quadratic-sum minimization gives less exact results than those obtained by
likelihood maximization. This is because the assumption of a normal distribu-
tion of the ni with the width (10.18.11) is only an approximation, which often
requires large bin widths for the histogram and thus implies a loss of informa-
tion. If, however, enough data, i.e., sufficiently large samples, are available,
then the difference between the two procedures is small. One should compare,
e.g., Fig. 10.18 (upper right-hand plot) with Fig. 10.19 (lower right-hand plot).
</p>
<p>10.19 Java Classes and Example Programs
</p>
<p>bins with n &gt;i 0. The user function is called MinHistSumOfSquares.
</p>
<p>Java Classes for Minimization Problems
</p>
<p>n-dimensional space.
</p>
<p>n-dimensional
</p>
<p>space.
</p>
<p>with a combined method according tho Brent.
</p>
<p>n-dimensional space.
</p>
<p>n-dimensional space using the simplex
</p>
<p>method.
</p>
<p>MinParab finds the extremum of a parabola through three given points.
</p>
<p>FunctionOnline computes the value of a function on a straight line in
</p>
<p>MinEnclose brackets the minimum on a straight line in
</p>
<p>MinCombined finds the minimum in a given interval along a straight line
</p>
<p>MinDir finds the minimum on a straight line in
</p>
<p>MinSim finds the minimum in</p>
<p/>
</div>
<div class="page"><p/>
<p>304 10 Function Minimization
</p>
<p>Fig.10.19:Determination of the parameters x1 (mean) and x2 (width) of a Gaussian distribu-
tion by minimization of a weighted sum of squares. The histograms on the left are the same
as in Fig. 10.18. The fit results and errors, covariance ellipses, and confidence regions are
represented as in Figs. 10.17 and 10.18.
</p>
<p>n-dimensional space with Powell&rsquo;s method
</p>
<p>of chosen directions.
</p>
<p>n-dimensional space with the method of con-
</p>
<p>jugate directions.
</p>
<p>n-dimensional space with quadratic form.
</p>
<p>n-dimensional space with Marquardt&rsquo;s
</p>
<p>method.
</p>
<p>MinPow finds the minimum in
</p>
<p>MinCjg finds the minimum in
</p>
<p>MinQdr finds the minimum in
</p>
<p>MInMar finds the minimum in</p>
<p/>
</div>
<div class="page"><p/>
<p>The program solves the problem in Example 10.1. First a sample is drawn from the
standard normal distribution. Next the sample is used to estimate the parameters
x1 (mean) and x2 (standard deviation) of the population by minimizing the nega-
tive of the likelihood function (10.18.2). This is done withMinSim. The covariance
matrix of the parameters is determined by callingMinCov. The results are presented
numerically. The rest of the program performs the graphical display of the sample
as a one-dimensional scatter plot and of the fitted function as in Fig. 10.16 (left-hand
plots).
</p>
<p>Suggestion: Run the program for samples of different size.
</p>
<p>10.19 Java Classes and Example Programs 305
</p>
<p>Example Program 10.1: The class E1Min demonstrates the use of
</p>
<p>The program calls one of the classes, as requested by the user, in order to solve
</p>
<p>the following problem. One wants to find the minimum of the function f = f (x) =
f (x1,x2,x3). The search is started at the point x
</p>
<p>(in) = (x(in)1 ,x
(in)
2 ,x
</p>
<p>(in)
3 ), which is also
</p>
<p>input by the user. The program treats consecutively four different cases:
</p>
<p>(i) No variables are fixed,
</p>
<p>(ii) x3 is fixed,
</p>
<p>(iii) x2 and x3 are fixed,
</p>
<p>(iv) All variables are fixed.
</p>
<p>The user can choose one of the following functions to be minimized:
</p>
<p>f1(x) = r2 , r =
&radic;
x21 +x22 +x23 ,
</p>
<p>f2(x) = r10 ,
f3(x) = r ,
f4(x) = &minus;e&minus;r
</p>
<p>2
</p>
<p>,
</p>
<p>f5(x) = r6 &minus;2r4 + r2 ,
f6(x) = r2e&minus;r
</p>
<p>2
</p>
<p>,
</p>
<p>f7(x) = &minus;e&minus;r
2 &minus;10e&minus;r2a , r2a = (x1 &minus;3)
</p>
<p>2 + (x2 &minus;3)2 + (x3 &minus;3)2 .
</p>
<p>Suggestions: Discuss the functions f1 through f7. All of them have a minimum
</p>
<p>at x1 = x2 = x3 = 0. Some possess additional minima. Study the convergence behav-
ior of the different minimization methods for these functions using different starting
</p>
<p>points and explain this behavior qualitatively.
</p>
<p>Example Program 10.2: The class E2Min determines the parameters
</p>
<p>of a distribution from the elements of a sample and demonstrates the
</p>
<p>use of MinCov
</p>
<p>MinSim, MinPow, MinCjg, MinQdr, and MinMar
</p>
<p>MinCov finds the covariance matrix of the coordinates of the minimum.
</p>
<p>MinAsy finds the asymmetric errors of the coordinates of the minimum.</p>
<p/>
</div>
<div class="page"><p/>
<p>306 10 Function Minimization
</p>
<p>the symmetric errors, the covariance ellipse, and the asymmetric errors are displayed
</p>
<p>plot corresponds to Fig. 10.16 (right-hand plots).
</p>
<p>The program solves the problem of Examples 10.2 and 10.3. First a sample of
size nev is drawn from a standard normal distribution and a histogram with nt
bins between t0 = &minus;5.25 and tmax = 5.25 is constructed from the sample. The bin
centers are ti (i = 1,2, . . . ,nt ), and the bin contents are ni . As chosen by the user
either the likelihood function ℓ given by (10.18.10) is maximized (i.e., &minus;ℓ is min-
</p>
<p>sum of squares Q given by (10.18.12) is minimized, again with MinSim, but using
</p>
<p>The results are presented in graphical form. One plot contains the histogram and
the fitted Gaussian (i.e., a plot corresponding to the plots on the left-hand side of
Figs. 10.17 or 10.18), a second one presents the solution in the (x1,x2) plane with
symmetric and asymmetric errors, covariance ellipse, and confidence region (corre-
sponding to the plots on the right-hand side of those figures).
</p>
<p>Suggestions: (a) Choose nev = 100. Show that for likelihood maximization the
errors Δx1, Δx2 increase if you decrease the number of bins beginning with nt = 100,
but that for the minimization of the sum of squares the number of bins has to be small
to get meaningful errors. (b) Show that for nev = 1000 and nt = 50 or nt = 20 there
is practically no difference between the results of the methods.
</p>
<p>Example Program 10.3: The class E3Min demonstrates the use of
</p>
<p>The class solves the same problem as the previous example. In addition it com-
</p>
<p>putes the asymmetric errors of the parameters using
</p>
<p>MinAsy and draws the boundary of a confidence region
</p>
<p>MinAsy. Then the solution,
</p>
<p>graphically in the (x ,x )1 2 plane, and with the help of the method DatanGra-
</p>
<p>phics.drawContour, the contour of the confidence region is shown as well. The
</p>
<p>Example Program 10.4: The class E4Min determines the parameters of a
</p>
<p>distribution from the histogram of a sample
</p>
<p>imized) by MinSim and the user function MinLogLikeHistPoison, or the
</p>
<p>MinHistSumOfSquares.</p>
<p/>
</div>
<div class="page"><p/>
<p>11. Analysis of Variance
</p>
<p>The analysis of variance (or ANOVA), originally developed by R. A. FISHER,
concerns testing the hypothesis of equal means of a number of samples.
Such problems occur, for example, in the comparison of a series of measure-
ments carried out under different conditions, or in quality control of sam-
ples produced by different machines. One tries to discover what influence the
changing of external variables (e.g., experimental conditions, the number of
a machine) has on a sample. For the simple case of only two samples, this
problem can also be solved with Student&rsquo;s difference test (Sect. 8.3).
</p>
<p>We speak of one-way analysis of variance, or also one-way classification,
when only one external variable is changed. The evaluation of a series of mea-
surements of an object micrometer performed with different microscopes can
serve as an example. One has a two- (or more) way analysis of variance (two-
way classification) when several variables are changed simultaneously. In the
example above, if different observers carry out the series of measurements
with each microscope, then a two-way analysis of variance can investigate
influences of both the observer and the instrument on the result.
</p>
<p>11.1 One-Way Analysis of Variance
</p>
<p>Let us consider a sample of size n, which can be divided into t groups
according to a certain criterion A. Clearly the criterion must be related to
the sampling or measuring process. We say that the groups are constructed
according to the classification A. We assume that the populations from which
the t subsamples are taken are normally distributed with the same variance
σ 2. We now want to test the hypothesis that the mean values of these popu-
lations are also equal. If this hypothesis is true, then all of the samples come
from the same population. We can then apply the results of Sect. 6.4 (samples
from subpopulations). Using the same notation as there, we have t groups of
size ni with
</p>
<p>S. Brandt, Data Analysis: Statistical and Computational Methods for Scientists and Engineers,
DOI 10.1007/978-3-319-03762-2__11, &copy; Springer International Publishing Switzerland 2014
</p>
<p>307</p>
<p/>
</div>
<div class="page"><p/>
<p>308 11 Analysis of Variance
</p>
<p>n=
t&sum;
</p>
<p>i=1
ni
</p>
<p>and we write the j th element of the ith group as xij . The sample mean of the
ith group is
</p>
<p>x̄i =
1
</p>
<p>ni
</p>
<p>ni&sum;
</p>
<p>j=1
xij (11.1.1)
</p>
<p>and the mean of the entire sample is
</p>
<p>x̄ = 1
n
</p>
<p>t&sum;
</p>
<p>i=1
</p>
<p>ni&sum;
</p>
<p>j=1
xij =
</p>
<p>1
</p>
<p>n
</p>
<p>t&sum;
</p>
<p>i=1
ni x̄i . (11.1.2)
</p>
<p>We now construct the sum of squares
</p>
<p>Q =
t&sum;
</p>
<p>i=1
</p>
<p>ni&sum;
</p>
<p>j=1
(xij &minus; x̄)2 =
</p>
<p>t&sum;
</p>
<p>i=1
</p>
<p>ni&sum;
</p>
<p>j=1
(xij &minus; x̄i + x̄i &minus; x̄)2
</p>
<p>=
t&sum;
</p>
<p>i=1
</p>
<p>ni&sum;
</p>
<p>j=1
(xij &minus; x̄i)2 +
</p>
<p>t&sum;
</p>
<p>i=1
</p>
<p>ni&sum;
</p>
<p>j=1
(x̄i &minus; x̄)2 +2
</p>
<p>t&sum;
</p>
<p>i=1
</p>
<p>ni&sum;
</p>
<p>j=1
(xij &minus; x̄i)(x̄i &minus; x̄) .
</p>
<p>The last term vanishes because of (11.1.1) and (11.1.2). One therefore has
</p>
<p>Q =
t&sum;
</p>
<p>i=1
</p>
<p>ni&sum;
</p>
<p>j=1
(xij &minus; x̄)2 =
</p>
<p>t&sum;
</p>
<p>i=1
ni(x̄i &minus; x̄)2 +
</p>
<p>t&sum;
</p>
<p>i=1
</p>
<p>ni&sum;
</p>
<p>j=1
(xij &minus; x̄i)2 ,
</p>
<p>Q = QA+QW . (11.1.3)
</p>
<p>The first term is the sum of squares between the groups obtained with the
classification A. The second term is a sum over the sums of squares within
a group. The sum of squares Q is decomposed into a sum of two sums of
squares corresponding to different &ldquo;sources&rdquo; &ndash; the variation of means within
the classification A and the variation of measurements within the groups. If
our hypothesis is correct, then Q is a sum of squares from a normal distri-
bution, i.e., Q/σ 2 follows a χ2-distribution with n&minus; 1 degrees of freedom.
Correspondingly, for each group the quantity
</p>
<p>Qi
</p>
<p>σ 2
= 1
</p>
<p>σ 2
</p>
<p>ni&sum;
</p>
<p>j=1
(xij &minus; x̄i)2
</p>
<p>follows a χ2-distribution with ni &minus;1 degrees of freedom. The sum
</p>
<p>QW
</p>
<p>σ 2
=
</p>
<p>t&sum;
</p>
<p>i=1
</p>
<p>Qi
</p>
<p>σ 2</p>
<p/>
</div>
<div class="page"><p/>
<p>11.1 One-Way Analysis of Variance 309
</p>
<p>is then described by a χ2-distribution with
&sum;
</p>
<p>i(ni &minus; 1) = n&minus; t degrees of
freedom (see Sect. 6.6). Finally, QA/σ 2 follows a χ2-distribution with t &minus; 1
degrees of freedom.
</p>
<p>The expressions
</p>
<p>s2 = Q
n&minus;1 =
</p>
<p>1
</p>
<p>n&minus;1
&sum;
</p>
<p>i
</p>
<p>&sum;
</p>
<p>j
</p>
<p>(xij &minus; x̄)2 ,
</p>
<p>s2A =
QA
</p>
<p>t&minus;1 =
1
</p>
<p>t &minus;1
&sum;
</p>
<p>i
</p>
<p>ni(x̄i &minus; x̄)2 , (11.1.4)
</p>
<p>s2W =
QW
</p>
<p>n&minus; t =
1
</p>
<p>n&minus; t
&sum;
</p>
<p>i
</p>
<p>&sum;
</p>
<p>j
</p>
<p>(xij &minus; x̄i)2
</p>
<p>are unbiased estimators of the population variances. (In Sect. 6.5 we called
such expressions mean squares.) The ratio
</p>
<p>F = s2A/s2W (11.1.5)
</p>
<p>can thus be used to carry out an F -test.
If the hypothesis of equal means is false, then the values x̄i of the individ-
</p>
<p>ual groups will be quite different. Thus s2A will be relatively large, while s
2
W ,
</p>
<p>which is the mean of the variances of the individual groups, will not change
much. This means that the ratio (11.1.5) will be large. Therefore one uses a
one-sided F -test. The hypothesis of equal means is rejected at the significance
level α if
</p>
<p>F = s2A/s2W &gt; F1&minus;α(t&minus;1, n&minus; t) . (11.1.6)
The sums of squares can be computed according to two equivalent formulas,
</p>
<p>Q =
&sum;
</p>
<p>i
</p>
<p>&sum;
</p>
<p>j
</p>
<p>(xij &minus; x̄)2 =
&sum;
</p>
<p>i
</p>
<p>&sum;
</p>
<p>j
</p>
<p>x2ij &minus;nx̄2 ,
</p>
<p>QA =
&sum;
</p>
<p>i
</p>
<p>ni(x̄i &minus; x̄)2 =
&sum;
</p>
<p>i
</p>
<p>ni x̄
2
i &minus;nx̄2 , (11.1.7)
</p>
<p>QW =
&sum;
</p>
<p>i
</p>
<p>&sum;
</p>
<p>j
</p>
<p>(xij &minus; x̄i)2 =
&sum;
</p>
<p>i
</p>
<p>&sum;
</p>
<p>j
</p>
<p>x2ij &minus;
&sum;
</p>
<p>i
</p>
<p>ni x̄
2
i .
</p>
<p>The expression on the right of each line is usually easier to compute. Since
each sum of squares is obtained by computing the difference of two relatively
large numbers, one must pay attention to possible problems with errors in
rounding. Although one only needs the sums QA and QW in order to com-
pute the ratio F , it is recommended to compute Q as well, since one can then
perform a check using (11.1.3), i.e., Q=QA+QW . The check is only mean-
ingful when Q is computed with the left-hand form of (11.1.3). Usually the</p>
<p/>
</div>
<div class="page"><p/>
<p>310 11 Analysis of Variance
</p>
<p>results of an analysis of variance are summarized in a so-called analysis of
variance table, (or ANOVA table) as shown in Table 11.1.
</p>
<p>Before carrying out an analysis of variance one must consider whether
the requirements are met under which the procedure has been derived. In par-
ticular, one must check the assumption of a normal distribution for the mea-
surements within each group. This is by no means certain in every case. If, for
example, the measured valuesare always positive (e.g., the length or weight
</p>
<p>Table11.1: ANOVA table for one-way classification.
</p>
<p>Source SS DF MS F
(sum of (degrees of (mean
squares) freedom) square)
</p>
<p>Between the
groups QA t&minus;1 s2A =
</p>
<p>QA
</p>
<p>t&minus;1
Within the
groups QW n&minus; t s2W =
</p>
<p>QW
</p>
<p>n&minus; t F =
s2A
</p>
<p>s2W
</p>
<p>Sum Q n&minus;1 s2 = Q
n&minus;1
</p>
<p>of an object) and if the standard deviation is of a magnitude comparable to the
measured values, then the probability density can be asymmetric and thus not
Gaussian. If, however, the original measurements (let us denote them for the
moment by x &prime;) are transformed using a monotonic transformation such as
</p>
<p>x = a log(x &prime;+b) , (11.1.8)
</p>
<p>where a and b are appropriately chosen constants, then a normal distribution
can often be sufficiently well approximated. Other transformations sometimes
used are x =
</p>
<p>&radic;
x &prime; or x = 1/x &prime;.
</p>
<p>Example 11.1: One-way analysis of variance of the influence of
various drugs
</p>
<p>The spleens of mice with cancer are often attacked particularly strongly. The
weight of the spleen can thus serve as a measure of the reaction to various
drugs. The drugs (I&ndash;III) were used to treat ten mice each. Table 11.2 contains
the measured spleen weights, which have already been transformed accord-
ing to x = logx &prime;, where x &prime; is the weight in grams. Most of the calculation
is presented in Table 11.2. Table 11.3 contains the resulting ANOVA table.
Since even at a significance level of 50% the F -test gives F0.5(2,24) = 3.4,
one cannot reject the hypothesis of equal mean values. The experiment thus
showed no significant difference in the effectiveness of the three drugs.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.2 Two-Way Analysis of Variance 311
</p>
<p>Table11.2: Data for Example 11.1.
</p>
<p>Experiment Group
number I II III
</p>
<p>1 19 40 32
2 45 28 26
3 26 26 30
4 23 15 17
5 36 24 23
6 23 26 24
7 26 36 29
8 33 27 20
9 22 28 &mdash;
</p>
<p>10 &mdash; 19 &mdash;
&sum;
</p>
<p>i
</p>
<p>&sum;
j x
</p>
<p>2
ij = 20607&sum;
</p>
<p>j xj 253 269 201
&sum;
</p>
<p>i
</p>
<p>&sum;
j xij = 723
</p>
<p>ni 9 10 8 n= 27
nx̄2 = 19360
</p>
<p>x̄i 28.11 26.90 25.13 x̄ = 26.78
x̄2i 790.23 723.61 631.52
</p>
<p>&sum;
i ni x̄
</p>
<p>2
i = 19398
</p>
<p>Table11.3: ANOVA table for Example 11.1.
</p>
<p>Source SS DF MS F
Between the groups 38 2 19.0 0.377
Within the groups 1209 24 50.4
Sum 1247 26 47.8
</p>
<p>11.2 Two-Way Analysis of Variance
</p>
<p>Before we turn to analysis of variance with two external variables, we would
like to examine more carefully the results obtained for one-way classification.
We denoted the j th measurement of the quantity x in group i by xij . We now
assume for simplicity that each group contains the same number of measure-
ments, i.e., ni = J . In addition we denote the total number of groups by I .
The classification into individual groups was done according to the criterion
A, e.g., the production number of a microscope, by which the groups can be
distinguished. The labeling according to measurement and group is illustrated
in Table 11.4.</p>
<p/>
</div>
<div class="page"><p/>
<p>312 11 Analysis of Variance
</p>
<p>We can write the individual means of the groups in the form
</p>
<p>x̄.. = x̄ =
1
</p>
<p>IJ
</p>
<p>&sum;
</p>
<p>i
</p>
<p>&sum;
</p>
<p>j
</p>
<p>xij ,
</p>
<p>x̄i. =
1
</p>
<p>J
</p>
<p>&sum;
</p>
<p>j
</p>
<p>xij , (11.2.1)
</p>
<p>x̄.j =
1
</p>
<p>I
</p>
<p>&sum;
</p>
<p>i
</p>
<p>xij .
</p>
<p>Table11.4: One-way classification.
</p>
<p>Measurement Classification A
number A1 A2 . . . Ai . . . AI
1 x11 x22 xi1 xI1
2 x12 x22 xi2 xI2
...
</p>
<p>j x1j x2j xij xIj
...
</p>
<p>J x1J x2J xiJ xIJ
</p>
<p>Here a point denotes summation over the index that it replaces. This notation
allows a simple generalization to a larger number of indices. The analysis
of variance with one-way classification is based on the assumption that the
measurements within a group only differ by the measurement errors, which
follow a normal distribution with a mean of zero and variance σ 2. That is, we
consider the model
</p>
<p>xij = μi + εij . (11.2.2)
</p>
<p>The goal of an analysis of variance was to test the hypothesis
</p>
<p>H0(μ1 = μ2 = . . . = μI = μ). (11.2.3)
</p>
<p>By choosing measurements out of a certain group i and by applying the
maximum-likelihood method to (11.2.2) one obtains the estimator
</p>
<p>μ̃i = x̄i. =
1
</p>
<p>J
</p>
<p>&sum;
</p>
<p>j
</p>
<p>xij . (11.2.4)</p>
<p/>
</div>
<div class="page"><p/>
<p>11.2 Two-Way Analysis of Variance 313
</p>
<p>If H0 is true, then one has
</p>
<p>μ̃= x̄ = 1
IJ
</p>
<p>&sum;
</p>
<p>i
</p>
<p>&sum;
</p>
<p>j
</p>
<p>xij =
1
</p>
<p>I
</p>
<p>&sum;
</p>
<p>i
</p>
<p>μ̃i . (11.2.5)
</p>
<p>The (composite) alternative hypothesis is that not all of the μi are equal.
We want, however, to retain the concept of the overall mean and we write
</p>
<p>μi = μ+ai .
</p>
<p>The model (11.2.2) then has the form
</p>
<p>xij = μ+ai + εij . (11.2.6)
</p>
<p>Between the quantities ai , which represent a measure of the deviation of the
mean for the ith group from the overall mean, one has the relation
</p>
<p>&sum;
</p>
<p>i
</p>
<p>ai = 0 . (11.2.7)
</p>
<p>The maximum-likelihood estimators for the ai are
</p>
<p>ãi = x̄i.&minus; x̄ . (11.2.8)
</p>
<p>The one-way analysis of variance of Sect. 11.1 was derived from the identity
</p>
<p>xij &minus; x̄ = (x̄i.&minus; x̄)+ (xij &minus; x̄i.) , (11.2.9)
</p>
<p>which describes the deviation of the individual measurements from the overall
mean. The sum of squares Q of these deviations could then be decomposed
into the terms QA and QW ; cf. (11.1.3).
</p>
<p>After this preparation we now consider a two-way classification, where
the measurements are divided into groups according to two criteria, A and B.
The measurements xijk belong to class Ai , which is given by the classification
according to A, and also to class Bj . The index k denotes the measurement
number within the group that belongs to both class Ai and class Bj .
</p>
<p>A two-way classification is said to be crossed, when a certain classifica-
tion Bj has the same meaning for all classes A. If, for example, microscopes
are classified by A and observers by B, and if each observer carries out a mea-
surement with each microscope, then the classifications are crossed. If, how-
ever, one compares the microscopes in different laboratories, and if therefore
in each laboratory a different group of J observers makes measurements with
a certain microscope i, then the classification B is said to be nested in A. The
index j then merely counts the classes B within a certain class A.</p>
<p/>
</div>
<div class="page"><p/>
<p>314 11 Analysis of Variance
</p>
<p>The simplest case is a crossed classification with only one observation.
Since then k = 1 for all observations xijk, we can drop the index k. One uses
the model
</p>
<p>xij = μ+ai +bj + εij ,
&sum;
</p>
<p>i
</p>
<p>ai = 0 ,
&sum;
</p>
<p>j
</p>
<p>bj = 0 , (11.2.10)
</p>
<p>where ε is normally distributed with mean zero and variance σ 2.
The null hypothesis says that by classification according to A or B, no devia-
tion from the overall mean occurs. We write this in the form of two individual
hypotheses,
</p>
<p>H
(A)
0 (a1 = a2 = &middot;&middot; &middot; = aI = 0) , H
</p>
<p>(B)
0 (b1 = b2 = &middot;&middot; &middot; = bJ = 0) . (11.2.11)
</p>
<p>The least-squares estimators for ai and bj are
</p>
<p>ãi = x̄i.&minus; x̄ , b̃j = x̄.j = x̄ .
</p>
<p>In analogy to Eq. (11.2.9) we can write
</p>
<p>xij &minus; x̄ = (x̄i.&minus; x̄)+ (x̄.j &minus; x̄)+ (xij &minus; x̄i.&minus; x̄.j + x̄) . (11.2.12)
</p>
<p>In a similar way the sum of squares can be written
&sum;
</p>
<p>i
</p>
<p>&sum;
</p>
<p>j
</p>
<p>(xij &minus; x̄)2 =Q=QA+QB +QW , (11.2.13)
</p>
<p>where
</p>
<p>QA = J
&sum;
</p>
<p>i
</p>
<p>(x̄i.&minus; x̄)2 = J
&sum;
</p>
<p>i
</p>
<p>x̄2i.&minus; IJ x̄2 ,
</p>
<p>QB = I
&sum;
</p>
<p>j
</p>
<p>(x̄.j &minus; x̄)2 = I
&sum;
</p>
<p>j
</p>
<p>x̄2.j &minus; IJ x̄2 , (11.2.14)
</p>
<p>QW =
&sum;
</p>
<p>i
</p>
<p>&sum;
</p>
<p>j
</p>
<p>(xij &minus; x̄i.&minus; x̄.j + x̄)2
</p>
<p>=
&sum;
</p>
<p>i
</p>
<p>&sum;
</p>
<p>j
</p>
<p>x2ij &minus;J
&sum;
</p>
<p>i
</p>
<p>x̄2i.&minus; I
&sum;
</p>
<p>j
</p>
<p>x̄2.j + IJ x̄2 .
</p>
<p>When divided by the corresponding number of degrees of freedom, these
sums are estimators of σ 2, providing that the hypotheses (11.2.11) are cor-
rect. The hypotheses H (A)0 and H
</p>
<p>(B)
0 can be tested individually by using the
</p>
<p>ratio
</p>
<p>F (A) = s2A/s2W , F (B) = s2B/s2W . (11.2.15)</p>
<p/>
</div>
<div class="page"><p/>
<p>11.2 Two-Way Analysis of Variance 315
</p>
<p>Here one uses one-sided F -tests as in Sect. 11.1. The overall situation can be
summarized in an ANOVA table (Table 11.5).
</p>
<p>If more than one observation is made in each group, then the crossed
classification can be generalized in various ways. The most important gener-
alization involves interaction between the classes. One then has the model
</p>
<p>xijk = μ+ai +bj + (ab)ij + εijk . (11.2.16)
</p>
<p>The quantity (ab)ij is called the interaction between the classes Ai and Bj .
It describes the deviation from the group mean that occurs because of the
specific interaction of Ai and Bj . The parameters ai , bj , (ab)ij are related by
</p>
<p>&sum;
</p>
<p>i
</p>
<p>ai =
&sum;
</p>
<p>j
</p>
<p>bj =
&sum;
</p>
<p>i
</p>
<p>&sum;
</p>
<p>j
</p>
<p>(ab)ij = 0 . (11.2.17)
</p>
<p>Their maximum-likelihood estimators are
</p>
<p>ãi = x̄i..&minus; x̄ , b̃j = x̄.j.&minus; x̄ ,
(ãb)ij = x̄ij.+ x̄&minus; x̄i..&minus; x̄.j. .
</p>
<p>(11.2.18)
</p>
<p>Table11.5: Analysis of variance table for crossed two-way classification with only one ob-
servation.
</p>
<p>Source SS DF MS F
</p>
<p>Class. A QA I &minus;1 s2A =
QA
</p>
<p>I &minus;1 F
(A) = s
</p>
<p>2
A
</p>
<p>s2W
</p>
<p>Class. B QB J &minus;1 s2B =
QB
</p>
<p>J &minus;1 F
(B) = s
</p>
<p>2
B
</p>
<p>s2W
Within
groups QW (I &minus;1)(J &minus;1) s2W =
</p>
<p>QW
</p>
<p>(I &minus;1)(J &minus;1)
</p>
<p>Sum Q IJ &minus;1 s2 = Q
IJ &minus;1
</p>
<p>The null hypothesis can be divided into three individual hypotheses,
</p>
<p>H
(A)
0 (ai = 0; i = 1,2, . . . , I ) , H
</p>
<p>(B)
0 (bj = 0; j = 1,2, . . . ,J ) ,
</p>
<p>H
(AB)
0 ((ab)ij = 0; i = 1,2, . . . , I ; j = 1,2, . . . ,J ) ,
</p>
<p>(11.2.19)
which can then be tested individually. The analysis of variance is based on the
identity
</p>
<p>xijk&minus; x̄ = (x̄i..&minus; x̄)+ (x̄.j.&minus; x̄) (11.2.20)
+ (x̄ij.+ x̄&minus; x̄i..&minus; x̄.j.)+ (xijk&minus; x̄ij.) ,</p>
<p/>
</div>
<div class="page"><p/>
<p>316 11 Analysis of Variance
</p>
<p>which allows the decomposition of the sum of squares of deviations into four
terms,
</p>
<p>Q =
&sum;
</p>
<p>i
</p>
<p>&sum;
</p>
<p>j
</p>
<p>&sum;
</p>
<p>k
</p>
<p>(xijk&minus; x̄)2
</p>
<p>= QA+QB +QAB +QW , (11.2.21)
</p>
<p>QA = JK
&sum;
</p>
<p>i
</p>
<p>(x̄i..&minus; x̄)2 ,
</p>
<p>QB = IK
&sum;
</p>
<p>j
</p>
<p>(x̄.j.&minus; x̄)2 ,
</p>
<p>QAB = K
&sum;
</p>
<p>i
</p>
<p>&sum;
</p>
<p>j
</p>
<p>(x̄ij.+ x̄&minus; x̄i..&minus; x̄.j.)2 ,
</p>
<p>QW =
&sum;
</p>
<p>i
</p>
<p>&sum;
</p>
<p>j
</p>
<p>&sum;
</p>
<p>k
</p>
<p>(xijk&minus; x̄ij.)2 .
</p>
<p>The degrees of freedom and mean squares as well as the F -ratio, which can
be used for testing the hypotheses, are given in Table 11.6.
</p>
<p>Table11.6: Analysis of variance table for crossed two-way classification.
</p>
<p>Source SS DF MS F
</p>
<p>Class. A QA I &minus;1 s2A =
QA
</p>
<p>I &minus;1 F
(A) = s
</p>
<p>2
A
</p>
<p>s2W
</p>
<p>Class. B QB J &minus;1 s2B =
QB
</p>
<p>J &minus;1 F
(B) = s
</p>
<p>2
B
</p>
<p>s2W
</p>
<p>Interaction QAB (I &minus;1)(J &minus;1) s2AB =
QAB
</p>
<p>(I &minus;1)(J &minus;1) F
(AB) = s
</p>
<p>2
AB
</p>
<p>s2W
</p>
<p>Within
groups QW IJ (K&minus;1) s2W =
</p>
<p>QW
</p>
<p>IJ (K&minus;1)
</p>
<p>Sum Q IJK&minus;1 s2 = Q
IJK&minus;1
</p>
<p>Finally, we will give the simplest case of a nested two-way classification.
Because the classification B is only defined within the individual classes of A,
the terms bj and (ab)ij from Eq. (11.2.10) are not defined, since they imply a
sum over i for fixed j . Therefore one uses the model
</p>
<p>xijk = μ+ai +bij + εijk (11.2.22)
with &sum;
</p>
<p>i ai = 0 ,
&sum;
</p>
<p>i
</p>
<p>&sum;
j bij = 0 ,
</p>
<p>ãi = x̄i..&minus; x̄ , b̃ij = x̄ij.&minus; x̄i.. .</p>
<p/>
</div>
<div class="page"><p/>
<p>11.2 Two-Way Analysis of Variance 317
</p>
<p>The term bij is a measure of the deviation of the measurements of class
Bj within class Ai from the overall mean of class Ai . The null hypothesis
consists of
</p>
<p>H
(A)
0 (ai = 0; i = 1,2, . . . , I ) ,
</p>
<p>H
(B(A))
0 (bij = 0; i = 1,2, . . . , I ; j = 1,2, . . . ,J ) .
</p>
<p>(11.2.23)
</p>
<p>An analysis of variance for testing these hypotheses can be carried out with
the help of Table 11.7. Here one has
</p>
<p>QA = JK
&sum;
</p>
<p>i
</p>
<p>(x̄i..&minus; x̄)2 ,
</p>
<p>QB(A) = K
&sum;
</p>
<p>i
</p>
<p>&sum;
</p>
<p>j
</p>
<p>(x̄ij.&minus; x̄i..)2 ,
</p>
<p>QW =
&sum;
</p>
<p>i
</p>
<p>&sum;
</p>
<p>j
</p>
<p>&sum;
</p>
<p>k
</p>
<p>(xijk&minus; x̄ij.)2 ,
</p>
<p>Q = QA+QB(A)+QW =
&sum;
</p>
<p>i
</p>
<p>&sum;
</p>
<p>j
</p>
<p>&sum;
</p>
<p>k
</p>
<p>(xijk&minus; x̄)2 .
</p>
<p>In a similar way one can construct various models for two-way or multiple
classification. For each model the total sum of squares is decomposed into a
certain sum of individual sums of squares, which, when divided by the corre-
sponding number of degrees of freedom, can be used to carry out an F -test.
With this, the hypotheses implied by the model can be tested.
</p>
<p>Some models are, at least formally, contained within others. For example
one finds by comparing Tables 11.6 and 11.7 the relation
</p>
<p>QB(A) =QB +QAB . (11.2.24)
A similar relation holds for the corresponding number of degrees of freedom,
</p>
<p>fB(A) = fB +fAB . (11.2.25)
</p>
<p>Table11.7: Analysis of variance table for nested two-way classification.
</p>
<p>Source SS DF MS F
</p>
<p>Class. A QA I &minus;1 s2A =
QA
</p>
<p>I &minus;1 F
(A) = s
</p>
<p>2
A
</p>
<p>s2W
</p>
<p>Within A QB(A) I (J &minus;1) s2B(A) =
AB(A)
</p>
<p>I (J &minus;1) F
(B(A)) =
</p>
<p>s2B(A)
</p>
<p>s2W
Within
groups QW IJ (K&minus;1) s2W =
</p>
<p>QW
</p>
<p>IJ (K&minus;1)
</p>
<p>Sum Q IJK&minus;1 s2 = Q
IJK&minus;1</p>
<p/>
</div>
<div class="page"><p/>
<p>318 11 Analysis of Variance
</p>
<p>Example 11.2: Two-way analysis of variance in cancer research
</p>
<p>Two groups of rats are injected with the amino acid thymidine containing
traces of tritium, a radioactive isotope of hydrogen. In addition, one of the
groups receives a certain carcinogen. The incorporation of thymidine into the
skin of the rats is investigated as a function of time by measuring the number
of tritium decays per unit area of skin. The classifications are crossed since the
time dependence is controlled in the same way for both series of test animals.
The measurements are compiled in Table 11.8. The numbers are already trans-
formed from the original counting rates x &prime; according to x = 50logx &prime;&minus; 100.
</p>
<p>Table 11.9. There is no doubt that the presence or absence of the carcinogen
(classification A)has an influence on the result, since the ratio F (A) is very
large. We now want to test the existence of a time dependence (classification
B) and of an interaction between A and B at a significance level of α = 0.01.
From Table I.8 we find F0.99 = 2.72. The hypotheses of time independence
and vanishing interaction must therefore be rejected. Table 11.9 also contains
the values of α for which the hypothesis would not need to be rejected. They
are very small.
</p>
<p>Table11.8: Data for Example 11.2.
</p>
<p>Obs. Injection Time after injection (h)
no. 4 8 12 16 20 24 28 32 36 48
1 34 54 44 51 62 61 59 66 52 52
2 Thymidine 40 57 52 46 61 70 67 59 63 50
3 38 40 53 51 54 64 58 67 60 44
4 36 43 51 49 60 68 66 58 59 52
1 28 23 42 43 31 32 25 24 26 26
2 Thymidine 32 23 41 48 45 38 27 26 31 27
3 and 34 29 34 36 41 32 27 32 25 27
4 Carcinogen 27 30 39 43 37 34 28 30 26 30
</p>
<p>The results, obtained with the class AnalysisOfVariance, are shown in</p>
<p/>
</div>
<div class="page"><p/>
<p>11.3 Java Class and Example Programs 319
</p>
<p>Table11.9: Printout from Example 11.2.
</p>
<p>Analysis of variance table
Source Sum of Degrees of Mean F Ratio Alpha
</p>
<p>squares freedom square
</p>
<p>A 9945.80 1 9945.80 590.547 25 0.00E&minus;10
B 1917.50 9 213.06 12.650 50 0.54E&minus;10
INT. 2234.95 9 248.33 14.744 85 0.03E&minus;10
W 1010.50 60 16.84
TTL. 15 108.75 79 191.25
</p>
<p>11.3 Java Class and Example Programs
</p>
<p>Java Class
</p>
<p>The short program analyses the data of Example 11.2. Data and output are presented
as in Table 11.9.
</p>
<p>Example Program 11.2:
</p>
<p>an analysis of variance on them
The program allows interactive input of numerical values for σ , the quantities I,J,K
and three further parameters: Δi,Δj ,Δk. It generates data of the simple form
</p>
<p>xijk = iΔi + jΔj + kΔk+ εijk .
</p>
<p>Here the quantities εijk are taken from a normal distribution with zero mean standard
deviation σ . An analysis of variance is performed on the data and the results are
presented for crossed and for nested two-way classification.
</p>
<p>Suggestion: Take a value �= 0 for only one of the parameters Δi,Δj ,Δk and
interpret the resulting analysis of variance table.
</p>
<p>analysis of variance.
</p>
<p>Example Program 11.1: The class E1Anova demonstrates the use of
</p>
<p>AnalysisOfVariance performs a crossed as well as a nested two-way
</p>
<p>AnalysisOfVariance
</p>
<p>The class E2Anova simulates data and performs</p>
<p/>
</div>
<div class="page"><p/>
<p>12. Linear and Polynomial Regression
</p>
<p>The fitting of a linear function (or, more generally, of a polynomial) to
measured data that depend on a controlled variable is probably the most com-
monly occurring task in data analysis. This procedure is also referred to as
linear (or polynomial) regression. Although we have already treated this prob-
lem in Sect. 9.4.1, we take it up again here in greater detail. Here we will
use different numerical methods, emphasize the most appropriate choice for
the order of the polynomial, treat in detail the question of confidence limits,
and also give a procedure for the case where the measurement errors are not
known.
</p>
<p>12.1 Orthogonal Polynomials
</p>
<p>In Sect. 9.4.1 we have already fitted a polynomial of order r&minus;1,
</p>
<p>η(t)= x1 +x2t +&middot;&middot; &middot;+xr t r&minus;1 , (12.1.1)
</p>
<p>to measured values yi(ti), which corresponded to a given value ti of the con-
trolled variable t . Here the quantities η(t) were the true values of the mea-
sured quantities y(t). In Example 9.2 we saw that there can be a maximum
reasonable order for the polynomial, beyond which a further increase in the
order gives no significant improvement in the fit. We want here to pursue the
question of how to find the optimal order of the polynomial. Example 9.2
shows that when increasing the order, all of the coefficients x1, x2, . . . change
and that all of the coefficients are in general correlated. Because of this the
situation becomes difficult to judge. These difficulties are avoided by using
orthogonal polynomials.
</p>
<p>Instead of (12.1.1) one describes the data by the expression
</p>
<p>η(t)= x1f1(t)+x2f2(t)+&middot;&middot; &middot;+xrfr(t) . (12.1.2)
</p>
<p>S. Brandt, Data Analysis: Statistical and Computational Methods for Scientists and Engineers,
DOI 10.1007/978-3-319-03762-2__12, &copy; Springer International Publishing Switzerland 2014
</p>
<p>321</p>
<p/>
</div>
<div class="page"><p/>
<p>322 12 Linear and Polynomial Regression
</p>
<p>Here the quantities fj are polynomials of order j &minus;1,
</p>
<p>fj (t)=
j&sum;
</p>
<p>k=1
bjkt
</p>
<p>k&minus;1 . (12.1.3)
</p>
<p>These are chosen to satisfy the condition of orthogonality (or more precisely
condition of orthonormality) with respect to the values ti and the measurement
weights gi = 1/σ 2i ,
</p>
<p>N&sum;
</p>
<p>i=1
gifj (ti)fk(ti)= δjk . (12.1.4)
</p>
<p>Defining
</p>
<p>Aij = fj (ti), A=
</p>
<p>⎛
⎜⎝
</p>
<p>A11 A12 &middot; &middot; &middot; A1r
...
</p>
<p>AN1 AN2 &middot; &middot; &middot; ANr
</p>
<p>⎞
⎟⎠ (12.1.5)
</p>
<p>and using the matrix notation (9.2.9)
</p>
<p>Gy =
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>g1 0
g2
</p>
<p>. . .
</p>
<p>0 gN
</p>
<p>⎞
⎟⎟⎟⎠ , (12.1.6)
</p>
<p>Eq. (12.1.4) has the simple form
</p>
<p>ATGyA= I . (12.1.7)
</p>
<p>The least-squares requirement,
</p>
<p>N&sum;
</p>
<p>i=1
gi
</p>
<p>⎧
⎨
⎩yi(ti)&minus;
</p>
<p>r&sum;
</p>
<p>j=1
xjfj (ti)
</p>
<p>⎫
⎬
⎭
</p>
<p>2
</p>
<p>= (y&minus;Ax)TGy(y&minus;Ax)= min , (12.1.8)
</p>
<p>is of the form (9.2.19) and thus has the solution (9.2.26)
</p>
<p>x̃=&minus;ATGyy , (12.1.9)
</p>
<p>where we have used (9.2.18) and (12.1.7). Because of (9.2.27) and (12.1.7)
one has
</p>
<p>Cx̃ = I , (12.1.10)</p>
<p/>
</div>
<div class="page"><p/>
<p>12.1 Orthogonal Polynomials 323
</p>
<p>i.e., the covariance matrix of the coefficients x̃1, . . ., x̃r is simply the unit
matrix. In particular, the coefficients are uncorrelated.
</p>
<p>We now discuss the procedure for determining the matrix elements
</p>
<p>Aij = fj (ti)=
j&sum;
</p>
<p>k=1
bjkt
</p>
<p>k&minus;1
i . (12.1.11)
</p>
<p>For j = 1, the orthogonality condition gives
</p>
<p>N&sum;
</p>
<p>i=1
gib
</p>
<p>2
11 = 1, b11 = 1/
</p>
<p>&radic;&sum;
</p>
<p>i
</p>
<p>gi . (12.1.12)
</p>
<p>For j = 2 there are two orthogonality conditions. First we obtain
&sum;
</p>
<p>i
</p>
<p>gif2(ti)f1(ti)=
&sum;
</p>
<p>i
</p>
<p>gi(b21 +b22ti)b11 = 0
</p>
<p>and from this
</p>
<p>b21 =&minus;b22
&sum;
</p>
<p>giti&sum;
gi
</p>
<p>=&minus;b22 t̄ , (12.1.13)
</p>
<p>where
</p>
<p>t̄ =
&sum;
</p>
<p>gi ti/
&sum;
</p>
<p>gi (12.1.14)
</p>
<p>is the weighted mean of the values of the controlled variable ti at which the
measurements were made. The second orthogonality condition for j = 2 gives
</p>
<p>&sum;
gi[f2(ti)]2 =
</p>
<p>&sum;
gi(b21 +b22ti)2 =
</p>
<p>&sum;
gib
</p>
<p>2
22(ti &minus; t̄ )2 = 1
</p>
<p>or
</p>
<p>b22 = 1/
&radic;&sum;
</p>
<p>gi(ti &minus; t̄ )2 . (12.1.15)
Substitution into (12.1.13) gives b21.
</p>
<p>For j &gt; 2 one can obtain the values of Aij = fj (ti) recursively from the
quantities for j &minus;1 and j &minus;2. We make the ansatz
</p>
<p>γfj (ti)= (ti &minus;α)fj&minus;1(ti)&minus;βfj&minus;2(ti) , (12.1.16)
</p>
<p>multiply by gifj&minus;1(ti), sum over i, and obtain
</p>
<p>γ
&sum;
</p>
<p>gifj (ti)fj&minus;1(ti)= 0
=
</p>
<p>&sum;
gi ti[fj&minus;1(ti)]2 &minus;α
</p>
<p>&sum;
gi[fj&minus;1(ti)]2 &minus;β
</p>
<p>&sum;
gifj&minus;2(ti)fj&minus;1(ti) .</p>
<p/>
</div>
<div class="page"><p/>
<p>324 12 Linear and Polynomial Regression
</p>
<p>Because of the orthogonality condition, the second term on the right-hand side
is equal to unity, and the third term vanishes. Thus one has
</p>
<p>α =
&sum;
</p>
<p>gi ti[fj&minus;1(ti)]2 . (12.1.17)
By multiplication of (12.1.16) by gifj&minus;2(ti) and summing one obtains in the
same way
</p>
<p>β =
&sum;
</p>
<p>gi tifj&minus;1(ti)fj&minus;2(t) . (12.1.18)
</p>
<p>Finally by computing the expression
&sum;
</p>
<p>gif
2
j (ti)= 1
</p>
<p>and substituting fj (ti) from (12.1.16) one obtains
</p>
<p>γ 2 =
&sum;
</p>
<p>gi[(ti &minus;α)fj&minus;1(ti)&minus;βfj&minus;2(ti)]2 . (12.1.19)
Once the quantities α, β, γ have been computed for a given j , then the coef-
ficients bjk are determined to be
</p>
<p>bj1 = (&minus;αbj&minus;1,1 &minus;βbj&minus;2,1)/γ ,
bjk = (bj&minus;1,k&minus;1 &minus;αbj&minus;1,k&minus;βbj&minus;2,k)/γ , k = 2, . . . ,j &minus;2 ,
</p>
<p>bj,j&minus;1 = (bj&minus;1,j&minus;2 &minus;αbj&minus;1,j&minus;1)/γ ,
bjj = bj&minus;1,j&minus;1/γ . (12.1.20)
With (12.1.3) and (12.1.5) one thus obtains the quantities Aij = fj (ti).
</p>
<p>Since the procedure is recursive, the column j of the matrix A is obtained
from the elements of the columns to its left. Be extending the matrix to the
right, the original matrix elements are not changed. This has the consequence
that by increasing the number of terms in the polynomial (12.1.1) from r to
r &prime;, the coefficients x̃1, . . ., x̃r are retained, and one merely obtains additional
coefficients. All of the x̃j are uncorrelated and have the standard deviation
σx̃j = 1. The order of the polynomial sufficient to describe the data can now
be determined by the requirement
</p>
<p>|̃xj |&lt; c, j &gt; r .
With this the contribution of all higher coefficients x̃j is smaller than cσx̃j .
The simplest choice of c is clearly c = 1.
</p>
<p>For a given r one can now substitute the values bjk into (12.1.3) and fj (ti)
into (12.1.2) in order to obtain the best estimates η̃i(ti) of the true values ηi(ti)
corresponding to the measurements yi(ti). One can also compute the quantity
</p>
<p>M =
N&sum;
</p>
<p>i=1
</p>
<p>(̃ηi(ti)&minus;yi(ti))2
</p>
<p>σ 2i
.
</p>
<p>If the data are in fact described by the polynomial of the chosen order, then
the quantity M follows a χ2-distribution with f =N&minus; r degrees of freedom.
It can be used for a χ2-test for the goodness-of-fit of the polynomial.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.2 Regression Curve: Confidence Interval 325
</p>
<p>Example 12.1: Treatment of Example 9.2 with Orthogonal Polynomials
</p>
<p>Application of polynomial regression to the data of Example 9.2 yields the
results shown in Table 12.1. The numerical values of the minimization func-
tion are, of course, exactly the same as in Example 9.2. The fitted poly-
nomials are also the same. The numerical values x̃1, x̃2, . . . are different
from those in Example 9.2 since these quantities are now defined differently.
We have emphasized that the covariance matrix for x̃ is the r&times; r unit matrix.
We see that the quantities x6, x7, . . ., x10 have magnitudes less than unity and
thus are no longer significantly different from zero. It is more difficult to judge
the significance of x5 = 1.08. In many cases one would not consider this value
as being clearly different from zero. This means that a third order polynomial
(i.e., r = 4) is sufficient to describe the data.
</p>
<p>Table12.1: Results of the application of polynomial regression using the data from
Example 9.2.
</p>
<p>r x̃r M Degrees of freedom
1 24.05 833.55 9
2 15.75 585.45 8
3 23.43 36.41 7
4 5.79 2.85 6
5 1.08 1.69 5
6 0.15 1.66 4
7 0.85 0.94 3
8 &minus;0.41 0.77 2
9 &minus;0.45 0.57 1
</p>
<p>10 0.75 0.00 0
</p>
<p>12.2 Regression Curve: Confidence Interval
</p>
<p>Once the order of the polynomial (12.1.2) in one way or another has been
fixed, then every point of the regression polynomial or &ndash; in reference to the
graphical representation &ndash; the regression curve can be computed. This is done
by first substituting the recursively computed values bjk into (12.1.3) for a
given t , and using the thus computed fj (t) and the parameters x̃ in (12.1.2):
</p>
<p>η̃(t)=
r&sum;
</p>
<p>j=1
x̃j
</p>
<p>⎛
⎝
</p>
<p>j&sum;
</p>
<p>k=1
bjkt
</p>
<p>k&minus;1
</p>
<p>⎞
⎠= dT(t )̃x . (12.2.1)</p>
<p/>
</div>
<div class="page"><p/>
<p>326 12 Linear and Polynomial Regression
</p>
<p>Here d is an r-vector with the elements
</p>
<p>dj (t)=
j&sum;
</p>
<p>k=1
bjkt
</p>
<p>k&minus;1 . (12.2.2)
</p>
<p>After error propagation, (3.8.4), the variance of η̃(t) is
</p>
<p>σ 2η̃(t) = dT(t)d(t) , (12.2.3)
</p>
<p>since Cx̃ = I . The reduced variable
</p>
<p>u= η̃(t)&minus;η(t)
ση̃(t)
</p>
<p>(12.2.4)
</p>
<p>thus follows a standard Gaussian distribution. This also means that the prob-
ability for the position of the true value η(t) is distributed according to a
Gaussian of width ση̃(t) centered about the value η̃(t).
</p>
<p>We can now easily give a confidence interval for η(t). According to
Sect. 5.8 one has with probability P that
</p>
<p>|u| &ge;Ω &prime;(P )=Ω
(
</p>
<p>1
</p>
<p>2
(P +1)
</p>
<p>)
, (12.2.5)
</p>
<p>e.g., Ω &prime;(0.95) = 1.96. Thus the confidence limits at the confidence level P
(e.g., P = 0.95) are
</p>
<p>η(t)= η̃(t)&plusmn;Ω &prime;(P )ση̃(t) = η̃(t)&plusmn; δη̃(t) . (12.2.6)
</p>
<p>12.3 Regression with Unknown Errors
</p>
<p>If the measurement errors σi = 1/
&radic;
gi are not known, but it can be assumed
</p>
<p>that they are equal,
σi = σ, i = 1, . . . ,N , (12.3.1)
</p>
<p>then one can easily obtain an estimate s for σ directly from the regression
itself. The quantity
</p>
<p>N&sum;
</p>
<p>i=1
</p>
<p>(̃η(ti)&minus;yi(ti))2
σ 2
</p>
<p>=M (12.3.2)
</p>
<p>follows a χ2-distribution with f =N&minus;r degrees of freedom. More precisely,
if many similar experiments with N measurements each were to be carried
out, then the values of M computed for each experiment would be distributed
like a χ2-variable with f = N &minus; r degrees of freedom. Its expectation value</p>
<p/>
</div>
<div class="page"><p/>
<p>12.3 Regression with Unknown Errors 327
</p>
<p>is simply f = N &minus; r . If the value of σ in (12.3.2) is not known, then it can
be replaced by an estimate s such that on the right-hand side one has the
expectation value of M ,
</p>
<p>s2 =
&sum;
</p>
<p>(̃η(ti)&minus;yi(ti))2/(N &minus; r) . (12.3.3)
</p>
<p>Here all of the steps necessary to compute η̃(ti) must be carried out using
the formulas of the last section and with the value σi = 1. (In fact, for the
case (12.3.1) the quantities σi and gi could be removed completely from the
formulas for computing x̃ and η̃(t).)
</p>
<p>If we define
s̄2η̃(t) = dT(t)d(t)
</p>
<p>as the value of the variance of η̃(t) obtained for σi = 1, then
</p>
<p>s2η̃(t) = s2s̄2η̃(t) (12.3.4)
</p>
<p>is clearly the estimate of the variance of η̃(t), which is based on the esti-
mate (12.3.3) for the variance of the measured quantities. Replacing ση̃(t) by
sη̃(t) in (12.2.4), we obtain the variable
</p>
<p>v = η̃(t)&minus;η(t)
sη̃(t)
</p>
<p>, (12.3.5)
</p>
<p>which no longer follows a standard normal distribution but rather Student&rsquo;s
t-distribution for f =N &minus; r degrees of freedom (cf. Sect. 8.3). For the confi-
dence limits at the confidence level P = 1&minus;α one now has
</p>
<p>η(t)= η̃(t)&plusmn; t1&minus;α/2sη̃(t) = η̃(t)&plusmn; δη̃(t) , (12.3.6)
</p>
<p>where t1&minus;α/2 is the quantile of the t-distribution for f = N &minus; r degrees of
freedom.
</p>
<p>It must be emphasized at this point that in the case where the errors are
not known, one loses the possibility of applying the χ2-test. The goodness-
of-fit for such a polynomial cannot be tested. Thus the procedure described at
the end of Sect. 12.1 for determining the order of the polynomial is no longer
valid. One must rely on a priori knowledge about the order of the polynomial.
Therefore one almost always refrains from treating anything beyond linear
dependences between η and t , i.e., the case r = 2.
</p>
<p>Example 12.2: Confidence limits for linear regression
</p>
<p>In the upper plot of Fig. 12.1, four measured points with their errors, the cor-
responding regression line, and the limits at a confidence level of 95% are
shown. The measured points are taken from the example of Sect. 9.3. The con-
fidence limits clearly become wider the more one leaves the region of the</p>
<p/>
</div>
<div class="page"><p/>
<p>328 12 Linear and Polynomial Regression
</p>
<p>Fig.12.1: (Above) Measurements with errors and regression line with 95% confidence lim-
its. (Below) Measured points with errors assumed to be of unknown but equal magnitude,
regression line, and 95% confidence limits.
</p>
<p>measured points. The narrowest region is close to the measured point with the
smallest error. The plot is similar to Fig. 9.2d. It is easy to convince oneself
that the envelope of the lines of Fig. 9.2d corresponds to the 68.3% confidence
limit. In the lower plot of Fig. 12.1 the same measured points were used, but
the errors were treated as equal but of unknown magnitude. The regression
line and 95% confidence limits are shown for this assumption.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.4 Java Class and Example Programs 329
</p>
<p>12.4 Java Class and Example Programs
</p>
<p>Java Class
</p>
<p>The short program contains the data of Examples 9.2 and 12.1. As measurement
errors the statistical errors Δyi =
</p>
<p>&radic;
yi are taken. A polynomial regression with r = 10
</p>
<p>parameters is performed. The results are presented numerically (see also Table 12.1).
</p>
<p>The program uses the data of Example 12.2. The user can declare the errors to be
either known or unknown and chooses a probability P which determines the con-
fidence limits. The graphics &ndash; corresponding to Fig. 12.1 &ndash; contains data points,
regression line, and confidence limits.
</p>
<p>Example Program 12.1: The class E1Reg demonstrates the use of
</p>
<p>Regression performs a polynomial regression.
</p>
<p>Regression
</p>
<p>Example Program 12.2: The class E2Reg demonstrates the use of
</p>
<p>The same problem as in E1Reg is treated. An additional input variable is r max, the
</p>
<p>maximum number of terms in the polynomials to be presented graphically. Graphs of
</p>
<p>these polynomials are shown together with the data points.
</p>
<p>Suggestion: Choose consecutively rmax = 2,3,4,6,10. Try to explain why the
curves for rmax = 3 or 4 seem to be particularly convincing although for rmax = 10 all
data points lie exactly on the graph of the polynomial.
</p>
<p>Example Program 12.3: The class E3Reg demonstrates the use of
</p>
<p>confidence limits
</p>
<p>This program again treats the same problem as
</p>
<p>are r, the order of the polynomial to be fitted, and the probability P , which determines
</p>
<p>the confidence limits. Presented in a plot are the data points with their errors, the
</p>
<p>regression line, and &ndash; in a different color &ndash; its confidence limits.
</p>
<p>Example Program 12.4: The class E4Reg demonstrates the linear
</p>
<p>regression with known and with unknown errors
</p>
<p>Regression and presents the results in graphical form
</p>
<p>Regression and graphically represents the regression line and its
</p>
<p>E1Reg. Additional input parameters</p>
<p/>
</div>
<div class="page"><p/>
<p>13. Time Series Analysis
</p>
<p>13.1 Time Series: Trend
</p>
<p>In the previous chapter we considered the dependence of a random variable y
on a controlled variable t . As in that case we will assume here that y consists
of two parts, the true value of the measured quantity η and a measurement
error ε,
</p>
<p>yi = ηi + εi , i = 1,2, . . . ,n . (13.1.1)
In Chap. 12 we assumed that ηi was a polynomial in t . The measurement error
εi was considered to be normally distributed about zero.
</p>
<p>We now want to make less restrictive assumptions about η. In this chap-
ter we call the controlled variable t &ldquo;time&rdquo;, although in many applications it
could be something different. The method we want to discuss is called time
series analysis and is often applied in economic problems. It can always be
used where one has little or no knowledge about the functional relationship
between η and t . In considering time series problems it is common to observe
the yi at equally spaced points in time,
</p>
<p>ti &minus; ti&minus;1 =Δt = const , (13.1.2)
</p>
<p>since this leads to a significant simplification of the formulas.
An example of a time series is shown in Fig. 13.1. If we look first only at
</p>
<p>the measured points, we notice strong fluctuations from point to point. Nev-
ertheless they clearly follow a certain trend. In the left half of the plot they
are mostly positive, and in the right half, mostly negative. One could qualita-
tively obtain the average time dependence by drawing a smooth curve by hand
through the points. Since, however, such curves are not free from personal in-
fluences, and are thus not reproducible, we must try to develop an objective
method.
</p>
<p>S. Brandt, Data Analysis: Statistical and Computational Methods for Scientists and Engineers,
DOI 10.1007/978-3-319-03762-2__13, &copy; Springer International Publishing Switzerland 2014
</p>
<p>331</p>
<p/>
</div>
<div class="page"><p/>
<p>332 13 Time Series Analysis
</p>
<p>Fig.13.1:Data points (circles) and moving average (joined by line segments).
</p>
<p>We use the notation from (13.1.1) and call ηi the trend and εi the random
component of the measurement yi . In order to obtain a smoother function of
t , one can, for example, construct for every value of yi the expression
</p>
<p>ui =
1
</p>
<p>2k+1
</p>
<p>i+k&sum;
</p>
<p>j=i&minus;k
yj , (13.1.3)
</p>
<p>i.e., the unweighted mean of the measurements for the times
</p>
<p>ti&minus;k, ti&minus;k+1, . . . , ti&minus;1, ti, ti+1, . . . , ti+k .
</p>
<p>The expression (13.1.3) is called a moving average of y.
</p>
<p>13.2 Moving Averages
</p>
<p>Of course the moving average (13.1.3) is a very simple construction. We will
show later (in Example 13.1) that the use of a moving average of this form is
equivalent to the assumption that η is a linear function of time in the interval
considered,
</p>
<p>ηj = α+βtj , j =&minus;k,&minus;k+1, . . . ,k . (13.2.1)</p>
<p/>
</div>
<div class="page"><p/>
<p>13.2 Moving Averages 333
</p>
<p>Here α and β are constants. They can be estimated from the data by linear
regression.
</p>
<p>Instead of restricting ourselves to the linear case, we will assume more
generally that η can be a polynomial of order ℓ.
</p>
<p>In the averaging interval, t takes on the values
</p>
<p>tj = ti + jΔt , j =&minus;k,&minus;k+1, . . . ,k . (13.2.2)
</p>
<p>Because η is a polynomial in t ,
</p>
<p>ηj = a1 +a2tj +a3t2j +&middot;&middot; &middot;+aℓ+1tℓj , (13.2.3)
</p>
<p>it is also a polynomial in j ,
</p>
<p>ηj = x1 +x2j +x3j2 +&middot;&middot; &middot;+xℓ+1j ℓ , (13.2.4)
</p>
<p>since (13.2.2) describes a linear transformation between tj and j , i.e., it is
merely a change of scale. We now want to obtain the coefficients x1, x2, . . .,
xℓ+1 from the data by fitting with least squares. This task has already been
treated in Sect. 9.4.1. We assume (in the absence of any better knowledge) that
all of the measurements are of the same accuracy. Thus the matrix Gy = aI
is simply a multiple of the unit matrix I . According to (9.2.26), the vector of
coefficients is thus given by
</p>
<p>x̃=&minus;(ATA)&minus;1ATy , (13.2.5)
</p>
<p>where A is a (2k+1)&times; (ℓ+1) matrix,
</p>
<p>A=&minus;
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>1 &minus;k (&minus;k)2 . . . (&minus;k)ℓ
1 &minus;k+1 (&minus;k+1)2 . . . (&minus;k+1)ℓ
...
</p>
<p>1 k k2 . . . kℓ
</p>
<p>⎞
⎟⎟⎟⎠ . (13.2.6)
</p>
<p>For the trend η̃0 at the center of the averaging interval (j = 0) we obtain
from (13.2.4) the estimate
</p>
<p>η̃0 = x̃1 . (13.2.7)
It is equal to the first coefficient of the polynomial. According to (13.2.5), x̃1
is obtained by multiplication of the column vector of measurements y on the
left with the row vector
</p>
<p>a= (&minus;(ATA)&minus;1AT)1 , (13.2.8)
</p>
<p>i.e., with the first row of the matrix &minus;(ATA)&minus;1AT. We obtain
</p>
<p>η̃0 = ay= a&minus;ky&minus;k+a&minus;k+1y&minus;k+1 +&middot;&middot; &middot;+a0y0 +&middot;&middot; &middot;+akyk . (13.2.9)</p>
<p/>
</div>
<div class="page"><p/>
<p>334 13 Time Series Analysis
</p>
<p>Table13.1: Components of the vector a for computing moving averages.
</p>
<p>a= (a&minus;k, a&minus;k+1, . . . , ak)=
1
</p>
<p>A
(α&minus;k, α&minus;k+1, . . . , αk)
</p>
<p>α&minus;j = αj
</p>
<p>ℓ= 2 and ℓ= 3
</p>
<p>k A α&minus;7 α&minus;6 α&minus;5 α&minus;4 α&minus;3 α&minus;2 α&minus;1 α0
</p>
<p>2 35 &minus;3 12 17
3 21 &minus;2 3 6 7
4 231 &minus;21 14 39 54 59
5 429 &minus;36 9 44 69 84 89
6 143 &minus;11 0 9 16 21 24 25
7 1105 &minus;78 &minus;13 42 87 122 147 162 167
</p>
<p>ℓ= 4 and ℓ= 5
</p>
<p>k A α&minus;7 α&minus;6 α&minus;5 α&minus;4 α&minus;3 α&minus;2 α&minus;1 α0
</p>
<p>3 231 5 &minus;30 75 131
4 429 15 &minus;55 30 135 179
5 429 18 &minus;45 &minus;10 60 120 143
6 2431 110 &minus;198 &minus;135 110 390 600 677
7 46189 2145 &minus;2860 &minus;2937 &minus;165 3755 7500 10125 11063
</p>
<p>This is a linear function of the measurements within the averaging interval.
Here the vector a does not depend on the measurements, but rather only on ℓ
and k, i.e., on the order of the polynomial and on the length of the interval.
Clearly one must choose
</p>
<p>ℓ &lt; 2k+1 ,
</p>
<p>since otherwise there would not remain any degrees of freedom for the least-
squares fit. The components of a for small values of ℓ and k can be obtained
from Table 13.1.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.2 Moving Averages 335
</p>
<p>Equation (13.2.9) describes the moving average corresponding to the
assumed polynomial (13.2.4). Once the vector a is determined, the moving
averages
</p>
<p>ui = η̃0(i)= ay(i)= a1yi&minus;k+a2yi&minus;k+1 +&middot;&middot; &middot;+a2k+1yi+k (13.2.10)
can easily be computed for each value of i.
</p>
<p>Example 13.1: Moving average with linear trend
</p>
<p>In the case of a linear trend function,
</p>
<p>ηj = x1 +x2j ,
</p>
<p>the matrix A becomes simply
</p>
<p>A=&minus;
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>1 &minus;k
1 &minus;k+1
...
</p>
<p>1 k
</p>
<p>⎞
⎟⎟⎟⎠ .
</p>
<p>One then has
</p>
<p>ATA =
(
</p>
<p>1 1 . . . 1
&minus;k &minus;k+1 . . . k
</p>
<p>)
⎛
⎜⎜⎜⎝
</p>
<p>1 &minus;k
1 &minus;k+1
...
</p>
<p>...
</p>
<p>1 k
</p>
<p>⎞
⎟⎟⎟⎠
</p>
<p>=
(
</p>
<p>2k+1 0
0 k(k+1)(2k+1)/3
</p>
<p>)
,
</p>
<p>(ATA)&minus;1 =
</p>
<p>⎛
⎜⎝
</p>
<p>1
</p>
<p>2k+1 0
</p>
<p>0
3
</p>
<p>k(k+1)(2k+1)
</p>
<p>⎞
⎟⎠ ,
</p>
<p>a = (&minus;(ATA)&minus;1AT)1 =
1
</p>
<p>2k+1(1,1, . . . ,1) .
</p>
<p>In this case the moving average is simply the unweighted mean (13.1.3).
</p>
<p>For more complicated models one can obtain the vectors a either by
solving (13.2.8) or simply from Table 13.1. Because of the symmetry of A,
one can show that polynomials of odd order (i.e., ℓ = 2n with n an integer)
have the same values of a as those of polynomials of the next lower order
ℓ= 2n&minus;1. One can also easily show that a has the symmetry
</p>
<p>aj = a&minus;j , j = 1,2, . . . ,k . (13.2.11)</p>
<p/>
</div>
<div class="page"><p/>
<p>336 13 Time Series Analysis
</p>
<p>13.3 Edge Effects
</p>
<p>Of course the moving average (13.2.10) can be used to estimate the trend
only for points i that have at least k neighboring points both to the right and
left, since the averaging interval covers 2k+1 points. This means that for the
first and last k points of a time series one must use a different estimator. One
obtains the most obvious generalization of the estimator by extrapolating the
polynomial (13.2.4) rather than using it only at the center of an interval. One
then obtains the estimators
</p>
<p>η̃i = ui = x̃(k+1)1 + x̃
(k+1)
2 (i&minus; k&minus;1)+ x̃
</p>
<p>(k+1)
3 (i&minus; k&minus;1)2 +&middot;&middot; &middot;
</p>
<p>+ x̃(k+1)ℓ+1 (i&minus; k&minus;1)ℓ , i &le; k ,
η̃i = ui = x̃(n&minus;k)1 + x̃
</p>
<p>(n&minus;k)
2 (i+ k&minus;n)+ x̃
</p>
<p>(n&minus;k)
3 (i+ k&minus;n)2 +&middot;&middot; &middot;
</p>
<p>+ x̃(n&minus;k)ℓ+1 (i+ k&minus;n) , i &gt; n&minus; k . (13.3.1)
</p>
<p>Here the notation x̃(k+1) and x̃(n&minus;k) indicates that the coefficients x̃ were de-
termined for the first and last intervals of the time series for which the centers
are at (k+1) and (n&minus; k).
</p>
<p>The estimators are now defined even for i &lt; 1 and i &gt; n. They thus offer
the possibility to continue the time series (e.g., into the future). Such extrapo-
lations must be treated with great care for two reasons:
</p>
<p>(i) Usually there is no theoretical justification for the assumption that the
trend is described by a polynomial. It merely simplifies the compu-
tation of the moving average. Without a theoretical understanding for a
trend model, the meaning of extrapolations is quite unclear.
</p>
<p>(ii) Even in cases where the trend can rightly be described by a polynomial,
the confidence limits quickly diverge from the estimated polynomial in
the extrapolated region. The extrapolation becomes very inaccurate.
</p>
<p>Whether point (i) is correct must be carefully checked in each individual case.
The more general point (ii) is already familiar from the linear regression (cf.
Fig. 12.1). We will investigate this in detail in the next section.
</p>
<p>13.4 Confidence Intervals
</p>
<p>We first consider the confidence interval for the moving average ui from
Eq. (13.2.10). The errors of the measurements yi are unknown and must there-
fore first be estimated. From (12.3.2) one obtains for the sample variance of
the yj in the interval of length 2k+1,</p>
<p/>
</div>
<div class="page"><p/>
<p>13.4 Confidence Intervals 337
</p>
<p>s2y =
1
</p>
<p>2k&minus; ℓ
</p>
<p>k&sum;
</p>
<p>j=&minus;k
(yj &minus; η̃j )2 , (13.4.1)
</p>
<p>where η̃j is given by
</p>
<p>η̃j = x̃1 + x̃2j + x̃3j2 +&middot;&middot; &middot;+ x̃ℓ+1j ℓ . (13.4.2)
</p>
<p>The covariance matrix for the measurements can then be estimated by
</p>
<p>G&minus;1y &asymp; s2yI . (13.4.3)
</p>
<p>The covariance matrix of the coefficients x is then given by (9.2.27),
</p>
<p>G&minus;1x̃ &asymp; (ATGyA)&minus;1 = s2y(ATA)&minus;1 . (13.4.4)
</p>
<p>Since ui = η̃0 = x̃1, we thus have for an estimator of the variance of ui
</p>
<p>s2x̃1 = (G
&minus;1
x̃ )11 = s2y((ATA)&minus;1)11 = s2ya0 . (13.4.5)
</p>
<p>From (13.2.6), (13.2.7), and (13.2.8) one easily obtains that (ATA)&minus;111 = a0,
since the middle row of A is &minus;(1,0,0, . . . ,0).
</p>
<p>Using the same reasoning as in Sect. 12.3 we obtain at a confidence level
of 1&minus;α
</p>
<p>|̃η0(i)&minus;η0(i)|
sya0
</p>
<p>&le; t1&minus; 12α . (13.4.6)
</p>
<p>For a given α we can give the confidence limits as
</p>
<p>η&plusmn;0 (i)= η̃0(i)&plusmn;a0syt1&minus; 12α . (13.4.7)
</p>
<p>Here t1&minus; 12α
is a quantile of Student&rsquo;s distribution for 2k&minus; ℓ degrees of free-
</p>
<p>dom. The true value of the trend lies within these limits with a confidence
level of 1&minus;α.
</p>
<p>Completely analogous, although more difficult computationally, is the
determination of confidence limits at the ends of the time series. The mov-
ing average is now given by (13.3.1). Labeling the arguments in the expres-
sions (13.3.1) j = i&minus; k&minus;1 and j = i+ k&minus;n, we obtain
</p>
<p>η̃ = Tx . (13.4.8)
</p>
<p>Here T is a row vector of length ℓ+1,
</p>
<p>T= (1,j,j 2, . . . ,j ℓ) . (13.4.9)</p>
<p/>
</div>
<div class="page"><p/>
<p>338 13 Time Series Analysis
</p>
<p>According to the law of error propagation (3.8.4) we obtain
</p>
<p>G&minus;1η̃ = TG&minus;1x T T . (13.4.10)
</p>
<p>With (13.4.4) we finally have
</p>
<p>G&minus;1η̃ &asymp; s2η̃ = s2yT (ATA)&minus;1T T , (13.4.11)
</p>
<p>where s2y is again given by (13.4.1).
The quantity s2η̃ can now be computed for every value of j , even for values
</p>
<p>lying outside of the time series itself. Thus we obtain the confidence limits
</p>
<p>η&plusmn;(i)= η̃(i)&plusmn; sη̃t1&minus; 12α . (13.4.12)
</p>
<p>Caution is always recommended in interpreting the results of a time series
analysis. This is particularly true for two reasons:
</p>
<p>1. There is usually no a priori justification for the mathematical model
on which the time series analysis is based. One has simply chosen a
convenient procedure in order to &ldquo;separate out&rdquo; statistical fluctuations.
</p>
<p>2. The user has considerable freedom in choosing the parameters k and ℓ,
which, however, can have a significant influence on the results. The fol-
lowing example gives an impression of the magnitude of such
influences.
</p>
<p>Example 13.2: Time series analysis of the same set of measurements using
different averaging intervals and polynomials of different orders
</p>
<p>Figures 13.2 and 13.3 contain time series analyses of the average number of
sun spots observed in the 36 months from January 1962 through December
1964. Various values of k and ℓ were used. The individual plots in Fig. 13.2
show ℓ = 1 (linear averaging) but different interval lengths (2k+1 = 5, 7, 9,
and 11). One can see that the curve of moving averages becomes smoother
and the confidence interval becomes narrower when k increases, but that then
the mean deviation of the individual observations from the curve also in-
creases. The extrapolation outside the range of measured points is, of course,
a straight line. (For ℓ= 0 we would have obtained the same moving averages
for the inner points. The outer and extrapolated points would lie, however, on
a horizontal line, since a polynomial of order zero is a constant.) The plots
in Fig. 13.3 correspond to the interval lengths 2k+ 1 = 7 and ℓ = 1,2,3,4.
The moving averages lie closer to the data points and the confidence interval
becomes larger when the value of ℓ increases.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.4 Confidence Intervals 339
</p>
<p>Fig.13.2: Time series analyses of the same data with fixed ℓ and various values of k.
</p>
<p>Fig.13.3: Time series analyses of the same data with fixed k and various values of ℓ.</p>
<p/>
</div>
<div class="page"><p/>
<p>340 13 Time Series Analysis
</p>
<p>From these observations we can derive the following qualitative rules:
</p>
<p>1. The averaging interval should not be chosen larger than the region
where one expects that the data can be well described by a polyno-
mial of the given order. That is, for ℓ = 1 the interval 2k+ 1 should
be chosen such that the expected nonlinear effects within the interval
remain small.
</p>
<p>2. On the other hand, the smoothing effect becomes stronger as the length
of the averaging interval increases. As a rule of thumb, the smoothing
becomes more effective for increasing 2k+1&minus; ℓ.
</p>
<p>3. Caution is required in extrapolation of a time series, especially in non-
linear cases.
</p>
<p>The art of time series analysis is, of course, much more highly developed
than what we have been able to describe in this short chapter. The interested
reader is referred to the specialized literature, where, for example, smoothing
functions other than polynomials or multidimensional analyses are treated.
</p>
<p>13.5 Java Class and Example Programs
</p>
<p>Java Class for Time Series Analysis
</p>
<p>TimeSeries performs a time series analysis.
</p>
<p>The uses the data of Example 13.2. After setting some parameters it performs a time
with k = 2, ℓ= 2. The data, moving aver-
</p>
<p>ages, and distances to the confidence limits (at a confidence level of 90%) are output
numerically.
</p>
<p>for the parameters k and ℓ and for the confidence level P and then performs a time
series analysis. Subsequently a plot is produced in which the data are displayed as
small circles. The floating averages are shown as a polyline. Polylines in a different
color indicate the confidence limits.
</p>
<p>Suggestion: Produce the individual plots of Figs. 13.2 and 13.3.
</p>
<p>Example Program 13.1: The class E1TimSer demonstrates the use of
</p>
<p>TimesSeries
</p>
<p>Example Program 13.2: The class E2TimSer performs a time series
</p>
<p>analysis and yields graphical output
</p>
<p>The program starts works on the same data as E1TimSer. It allows interactive input
</p>
<p>series analysis by a call of TimesSeries</p>
<p/>
</div>
<div class="page"><p/>
<p>Literature
</p>
<p>Literature Cited in the Text
</p>
<p>[1] A. KOLMOGOROV, Ergebn. Math. 2 (1933) 3
</p>
<p>[2] D.E. KNUTH, The Art of Computer Programming, vol. 2, Addison-
Wesley, Reading MA 1981
</p>
<p>[3] P. L&rsquo;ECUYER, Comm. ACM 31 (1988) 742
</p>
<p>[4] B.A. WICHMANN and I.D. HILL, Appl. Stat. 31 (1982) 188
</p>
<p>[5] G.E.P. BOX and M.E. MULLER, Ann. Math. Stat. 29 (1958) 611
</p>
<p>[6] L. VON BORTKIEWICZ, Das Gesetz der kleinen Zahlen, Teubner,
Leipzig 1898
</p>
<p>[7] D.J. DE SOLLA PRICE, Little Science, Big Science, Columbia University
Press, New York 1965
</p>
<p>[8] M.G. KENDALL and A. STUART, The Advanced Theory of Statistics,
vol. 2, Charles Griffin, London 1968
</p>
<p>[9] S.S. WILKS, Ann. Math. Stat., 9 (1938) 60
</p>
<p>[10] A.H. ROSENFELD, H.A. BARBERO-GALTIERI, W.J. PODOLSKI, L.R.
PRICE, P. S&Ouml;DING, CH.G. WOHL, M. ROOS, and W.J. WILLIS, Rev.
Mod. Phys. 33 (1967) 1
</p>
<p>[11] PARTICLE DATA GROUP, Physics Letters B204 (1988) 1
</p>
<p>[12] W.H. PRESS, B.P. FLANNERY, S.A. TEUKOLSKY and W.T. VETTER-
LING, Numerical Recipes, Cambridge University Press, Cambridge
1986
</p>
<p>S. Brandt, Data Analysis: Statistical and Computational Methods for Scientists and Engineers,
DOI 10.1007/978-3-319-03762-2, &copy; Springer International Publishing Switzerland 2014
</p>
<p>341</p>
<p/>
</div>
<div class="page"><p/>
<p>342 Literature
</p>
<p>[13] R.P. BRENT, Algorithms for Minimization without Derivatives, Prentice-
Hall, Englewood Cliffs NJ 1973
</p>
<p>[14] J.A. NELDER and R. MEAD, Computer Journal 7 (1965) 308
</p>
<p>[15] M.J.D. POWELL, Computer Journal 7 (1965) 155
</p>
<p>[16] D.W. MARQUARDT, J. Soc. Ind. Appl. Math. 11 (1963) 431
</p>
<p>[17] C. LANCZOS, SIAM J. Numerical Analysis 1 (1964) 86
</p>
<p>[18] C.L. LAWSON and R.J. HANSON, Solving Least Squares Problems,
Prentice-Hall, Englewood Cliffs NJ 1974
</p>
<p>[19] G.H. GOLUB and W. KAHN, SIAM J. Numerical Analysis 2 (1965) 205
</p>
<p>[20] P.A. BUSINGER and G.H. GOLUB, Comm. ACM 12 (1969) 564
</p>
<p>[21] G.H. GOLUB and C. REINSCH, in Linear Algebra (J.H. WILKINSON and
C. REINSCH, eds.), p. 134, Springer, Berlin 1971
</p>
<p>[22] J.G.F. FRANCIS, Computer Journal 4 (1960) 265, 332
</p>
<p>[23] F. JAMES and M. ROOS, Nuclear Physics B172 (1980) 475
</p>
<p>[24] M.L. SWARTZ, Nuclear Instruments and Methods A294 (1966) 278
</p>
<p>[25] V.H. REGENER, Physical Review 84 (1951) 161
</p>
<p>[26] G. ZECH, Nuclear Instruments and Methods A277 (1989) 608
</p>
<p>[27] H. RUTISHAUSER, Numerische Mathematik 5 (1963) 48
</p>
<p>[28] W. ROMBERG, Det. Kong. Norske Videnskapers Selskap Forhandlinger
28 (1955) Nr. 7
</p>
<p>[29] K.S. KOELBIG, in CERN Computer Centre Program Library, Program
D401, CERN, Geneva 1990
</p>
<p>Bibliography
</p>
<p>The following short list contains several books which can serve to extend the
breadth and depth of the present material. It is of course in no way complete,
but instead merely indicates a number of books of varied levels of difficulty.</p>
<p/>
</div>
<div class="page"><p/>
<p>Literature 343
</p>
<p>Probability
</p>
<p>L. BREIMAN, Probability, Addison-Wesley, Reading MA 1968
</p>
<p>H. CRAM&Egrave;R, The Elements of Probability Theory, Wiley, New York 1955
</p>
<p>B.V. GNEDENKO, The Theory of Probability, 4th ed., Chelsea, New York
1967
</p>
<p>KAI LAI CHUNG, A Course in Probability Theory, Harcourt, Brace and World,
New York 1968
</p>
<p>K. KRICKEBERG, Wahrscheinlichkeitsrechnung, Teubner, Stuttgart 1963
</p>
<p>Mathematical Statistics
</p>
<p>P.R. BEVINGTON, Data Reduction and Error Analysis for the Physical Sci-
ences, McGraw-Hill, New York 1969
</p>
<p>B.E. COOPER, Statistics for Experimentalists, Pergamon, Oxford 1969
</p>
<p>G. COWAN, Statistical Data Analysis, Clarendon Press, Oxford 1998
</p>
<p>H. CRAM&Egrave;R, Mathematical Methods of Statistics, University Press, Princeton
1946
</p>
<p>W.J. DIXON and F.J. MASSEY, Introduction to Statistical Analysis, McGraw-
Hill, New York 1969
</p>
<p>D. DUMAS DE RAULY, L&rsquo;Estimation Statistique, Gauthier-Villars, Paris 1968
</p>
<p>W.T. EADIE, D. DRIJARD, F.E. JAMES, M. ROOS and B.SADOULET, Sta-
tistical Methods in Experimental Physics, North-Holland, Amsterdam
1971
</p>
<p>W. FELLER, An Introduction to Probability Theory and its Allpications, 2
Vols., Wiley, New York 1968
</p>
<p>M. FISZ, Probability Theory and Mathematical Statistics, Wiley, New York
1963
</p>
<p>D.A.S. FRASER, Statistics; An Introduction, Wiley, New York 1958
</p>
<p>H. FREEMAN, Introduction to Statistical Inference, Addison-Wesley, Read-
ing, MA 1963
</p>
<p>A.G. FRODENSEN and O. SKJEGGESTAD, Probability and Statistics in Parti-
cle Physics, Universitetsforlaget, Bergen 1979
</p>
<p>P.G. HOEL, Introduction to Mathematical Statistics, 4th ed., Wiley, New York
1971</p>
<p/>
</div>
<div class="page"><p/>
<p>344 Literature
</p>
<p>M.G. KENDALL and A. STUART The Advanced Theory of Statistics, 4th ed.,
3 vols., Charles Griffin, London 1977
</p>
<p>L. LYONS, Statistics for Nuclear and Particle Physicists, Cambridge Univer-
sity Press, Cambridge 1986
</p>
<p>J. MANDEL, The Statistical Analysis of Experimental Data, Interscience, New
York 1964
</p>
<p>S.L. MEYER, Data Analysis for Scientists and Engineers, Wiley, New York
1975
</p>
<p>L. SACHS, Statistische Auswertungsmethoden, 5. Auf l., Springer, Berlin 1978
</p>
<p>E. SVERDRUP, Laws and Chance Variations, 2 Bde., North-Holland, Amster-
dam 1967
</p>
<p>B.L. VAN DER WAERDEN, Mathematische Statistik, 3. Auf l., Springer, Berlin
1971
</p>
<p>S.S. WILKS, Mathematical Statistics, Wiley, New York 1962
</p>
<p>T. YAMANE, Elementary Sampling Theory, Prentice-Hall, Englewood
Cliffs, NJ 1967
</p>
<p>T. YAMANE, Statistics, An Introductory Analysis, Harper and Row, New York
1967
</p>
<p>Numerical Methods: Matrix Programs
</p>
<p>A. BJ&Ouml;RK and G. DAHLQUIST, Numerische Methoden, R. Oldenbourg
Verlag, M&uuml;nchen 1972
</p>
<p>R.P. BRENT, Algorithms for Minimization without Derivatives, Prentice-Hall,
Englewood Cliffs, NJ 1973
</p>
<p>C.T. FIKE, Computer Evaluation of Mathematical Functions, Prentice-Hall,
Englewood Cliffs, NJ 1968
</p>
<p>G.H. GOLUB and C.F. VAN LOAN, Matrix Computations, Johns Hopkins Uni-
versity Press, Baltimore 1983
</p>
<p>R.W. HAMMING, Numerical Methods for Engineers and Scientists, 2nd ed.,
McGraw-Hill, New York 1973
</p>
<p>D.E. KNUTH, The Art of Computer Programming, 3 vols., Addison-Wesley,
Reading MA 1968
</p>
<p>C.L. LAWSON and R.J. HANSON, Solving Least Squares Problems, Prentice-
Hall, Englewood Cliffs NJ 1974</p>
<p/>
</div>
<div class="page"><p/>
<p>Literature 345
</p>
<p>W.H. PRESS, B.P. FLANNERY, S.A. TEUKOLSKY, and W.T. VETTERLING,
Numerical Recipes, Cambridge University Press, Cambridge 1986
</p>
<p>J.R. RICE, Numerical Methods, Software and Analysis, McGraw-Hill, New
York 1983
</p>
<p>J. STOER and R. BURLISCH, Einf&uuml;hrung in die Numerische Mathematik, 2
Bde., Springer, Berlin 1983
</p>
<p>J.H. WILKINSON and C. REINSCH, Linear Algebra, Springer, Berlin 1971
</p>
<p>Collections of Formulas: Statistical Tables
</p>
<p>M. ABRAMOWITZ and A. STEGUN, Handbook of Mathematical Functions,
Dover, New York, 1965
</p>
<p>R.A. FISHER and F. YATES, Statistical Tables for Biological, Agricultural and
Medical Research, Oliver and Boyd, London 1957
</p>
<p>U. GRAF and H.J. HENNING, Formeln und Tabellen zur Mathematischen
Statistik, Springer, Berlin 1953
</p>
<p>A. HALD, Statistical Tables and Formulas, Wiley, New York 1960
</p>
<p>G.A. KORN and T.M. KORN, Mathematical Handbook for Scientists and En-
gineers, 2nd ed., McGraw-Hill, New York 1968
</p>
<p>D.V. LINDLEY and J.C.P. MILLER, Cambridge Elementary Statistical Tables,
University Press, Cambridge 1961
</p>
<p>D.B. OWEN, Handbook of Statistical Tables, Addison-Wesley, Reading MA
1962
</p>
<p>E.S. PEARSON and H.O. HARTLEY, Biometrica Tables for Statisticians, Uni-
versity Press, Cambridge 1958
</p>
<p>W. WETZEL, M.D. J&Ouml;HNK, and P. NAEVE, Statistische Tabellen, Walter de
Gruyter, Berlin 1967</p>
<p/>
</div>
<div class="page"><p/>
<p>A. Matrix Calculations
</p>
<p>The solution of the over-determined linear system of equations Ax &asymp; b is of
central significance in data analysis. This can be solved in an optimal way
with the singular value decomposition, first developed in the late 1960s. In
this appendix the elementary definitions and calculation rules for matrices and
vectors are summarized in Sects. A.1 and A.2. In Sect. A.3 orthogonal trans-
formations are introduced, in particular the Givens and Householder transfor-
mations, which provide the key to the singular value decomposition.
</p>
<p>After a few remarks on determinants (Sect. A.4) there follows in Sect. A.5
a discussion of various cases of matrix equations and a theorem on the orthog-
onal decomposition of an arbitrary matrix, which is of central importance in
this regard. The classical procedure of normal equations, which, however, is
inferior to the singular value decomposition, is described here.
</p>
<p>Sections A.6&ndash;A.8 concern the particularly simple case of exactly deter-
mined, non-singular matrix equations. In this case the inverse matrix A&minus;1
</p>
<p>exists, and the solution to the problem Ax = b is x = A&minus;1b. Methods and
programs for finding the solution are given. The important special case of a
positive-definite symmetric matrix is treated in Sect. A.9.
</p>
<p>In Sect. A.10 we define the pseudo-inverse matrix A+ of an arbitrary ma-
trix A. After introducing eigenvectors and eigenvalues in Sect. A.11, the sin-
gular value decomposition is presented in Sects. A.12 and A.13. Computer
routines are given in Sect. A.14. Modifications of the procedure and the con-
sideration of constraints are the subject of Sects. A.15 through A.18.
</p>
<p>It has been attempted to make the presentation illustrative rather than
mathematically rigorous. Proofs in the text are only indicated in a general
way, or are omitted entirely. As mentioned, the singular value decomposition
is not yet widespread. An important goal of this appendix is to make its
use possible. For readers with a basic knowledge of matrix calculations, the
material covered in Sects. A.3, A.12, A.13, A.14.1, and A.18 is sufficient for
this task. Sections A.14.2 through A.14.5 contain technical details on carrying
out the singular value decomposition and can be omitted by hurried users.
</p>
<p>S. Brandt, Data Analysis: Statistical and Computational Methods for Scientists and Engineers,
DOI 10.1007/978-3-319-03762-2, &copy; Springer International Publishing Switzerland 2014
</p>
<p>347</p>
<p/>
</div>
<div class="page"><p/>
<p>348 A Matrix Calculations
</p>
<p>A.1 Definitions: Simple Operations
</p>
<p>By a vector in m dimensions (an m-vector) a we mean an m-tuple of real
numbers, which are the components of a. The arrangement of the components
in the form
</p>
<p>a=
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>a1
a2
...
</p>
<p>am
</p>
<p>⎞
⎟⎟⎟⎠ (A.1.1)
</p>
<p>is called a column vector.
A m&times;n matrix is a rectangular arrangement of m&times;n numbers in m rows
</p>
<p>and n columns,
</p>
<p>A=
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>A11 A12 &middot; &middot; &middot; A1n
A21 A22 &middot; &middot; &middot; A2n
...
</p>
<p>...
...
</p>
<p>Am1 Am2 &middot; &middot; &middot; Amn
</p>
<p>⎞
⎟⎟⎟⎠ . (A.1.2)
</p>
<p>It can be viewed to be composed of n column vectors. By transposition of the
m&times;n matrix A one obtains an n&times;m matrix AT with elements
</p>
<p>ATik = Aki . (A.1.3)
</p>
<p>Under transposition a column vector becomes a row vector,
</p>
<p>aT = (a1,a2, . . . ,am) . (A.1.4)
</p>
<p>A column vector is an m&times;1 matrix; a row vector is a 1&times;m matrix.
For matrices one has the following elementary rules for addition, sub-
</p>
<p>traction and multiplication by a constant
</p>
<p>A&plusmn;B = C , Cik = Aik&plusmn;Bik , (A.1.5)
</p>
<p>αA= B , Bik = αAik . (A.1.6)
The product AB of two matrices is only defined if the number of columns of
the first matrix is equal to the number of rows of the second, e.g., A= Am&times;ℓ
and B = Bℓ&times;m. One has then
</p>
<p>All procedures described in this Appendix are implemented as methods
</p>
<p>of the classes
</p>
<p>few cases will we refer to these methods explicitly, in oder to establish the
</p>
<p>connection with some of the more complicated algorithms in the text.
</p>
<p>DatanVector orDatanMatrix, respectively. Only in a</p>
<p/>
</div>
<div class="page"><p/>
<p>A.1 Definitions: Simple Operations 349
</p>
<p>AB = C , Cik =
ℓ&sum;
</p>
<p>j=1
AijBjk . (A.1.7)
</p>
<p>Since
</p>
<p>CTik = Cki =
ℓ&sum;
</p>
<p>j=1
AkjBji =
</p>
<p>ℓ&sum;
</p>
<p>j=1
ATjkB
</p>
<p>T
ij =
</p>
<p>ℓ&sum;
</p>
<p>j=1
BTijA
</p>
<p>T
jk ,
</p>
<p>one has
CT = (AB)T = BTAT . (A.1.8)
</p>
<p>With (A.1.7) one can also define the product of a row vector aT with a column
vector b, if both have the same number of elements m,
</p>
<p>a &middot;b= aTb= c , c =
m&sum;
</p>
<p>j=1
ajbj . (A.1.9)
</p>
<p>The result is then a number, i.e., a scalar. The product (A.1.9) is called the
scalar product. It is usually written without indicating the transposition sim-
ply as a &middot;b. The vectors a and b are orthogonal to each other if their scalar
product vanishes. Starting from (A.1.9) one obtains the following useful prop-
erty of the matrix product (A.1.7). The element Cik, which is located at the
intersection of the ith row and the kth column of the product matrix C, is
equal to the scalar product of the ith row of the first matrix A with the kth
column of the second matrix B.
</p>
<p>The diagonal elements of a matrix (A.1.2) are the elements Aii . They
form the main diagonal of the matrix A. If all of the non-diagonal elements
vanish, Aij = 0, i �= j , then A is a diagonal matrix. An n&times;n diagonal matrix
all of whose diagonal elements are unity is the n-dimensional unit matrix
In = I ,
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>1 0 . . . 0
0 1 . . . 0
...
</p>
<p>0 0 . . . 1
</p>
<p>⎞
⎟⎟⎟⎠=
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>1 0
1
</p>
<p>. . .
</p>
<p>0 1
</p>
<p>⎞
⎟⎟⎟⎠= I . (A.1.10)
</p>
<p>The null matrix has only zeros as elements:
</p>
<p>0 =
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>0 0 . . . 0
0 0 . . . 0
...
</p>
<p>0 0 . . . 0
</p>
<p>⎞
⎟⎟⎟⎠ . (A.1.11)
</p>
<p>A null matrix with only one column is the null vector 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>350 A Matrix Calculations
</p>
<p>We will now mention several more special types of square matrices.
A square matrix is symmetric if
</p>
<p>Aik = Aki . (A.1.12)
</p>
<p>If
</p>
<p>Aik =&minus;Aki , (A.1.13)
</p>
<p>then the matrix is antisymmetric.
A bidiagonal matrix B possesses non-vanishing elements only on the
</p>
<p>main diagonal (bii) and on the parallel diagonal directly above it (bi,i+1).
A tridiagonal matrix possesses in a addition non-vanishing elements di-
</p>
<p>rectly below the main diagonal. A lower triangular matrix has non-vanishing
elements only on and below the main diagonal, an upper triangular matrix
only on and above the main diagonal.
</p>
<p>The Euclidian norm or the absolute value of a vector is
</p>
<p>|a| = ‖a‖2 = a =
&radic;
aTa=
</p>
<p>&radic;&sum;
</p>
<p>j
</p>
<p>a2j . (A.1.14)
</p>
<p>A vector with unit norm is called a unit vector. We write this in the form
</p>
<p>â= a/a .
</p>
<p>More general vector norms are
</p>
<p>‖a‖p = (
&sum;
</p>
<p>j
</p>
<p>|aj |p)1/p , 1 &le; p &lt;&infin; , (A.1.15)
</p>
<p>‖a‖&infin; = max
j
</p>
<p>|aj | .
</p>
<p>For every vector norm ‖x‖ one defines a matrix norm ‖A‖ as
</p>
<p>‖A‖ = max
x�=0
</p>
<p>‖Ax‖/‖x‖ . (A.1.16)
</p>
<p>Matrix norms have the following properties:
</p>
<p>‖A‖ &gt; 0 , A �= 0 ; ‖A‖ = 0 , A= 0 , (A.1.17)
‖αA‖ = |α|‖A‖ , α real , (A.1.18)
</p>
<p>‖A+B‖ &le; ‖A‖+‖B‖ , (A.1.19)
‖AB‖ &le; ‖A‖‖B‖ . (A.1.20)</p>
<p/>
</div>
<div class="page"><p/>
<p>A.2 Vector Space, Subspace, Rank of a Matrix 351
</p>
<p>A.2 Vector Space, Subspace, Rank of a Matrix
</p>
<p>An n-dimensional vector space is the set of all n-dimensional vectors. If u
and v are vectors in this space, then αu and u+ v are also in the space, i.e.,
the vector space is closed under vector addition and under multiplication with
a scalar α. The vectors a1,a2, . . . ,ak are linearly independent if
</p>
<p>k&sum;
</p>
<p>j=1
αjaj �= 0 (A.2.1)
</p>
<p>for all αj except for α1 = α2 = &middot;&middot; &middot; = αk = 0. Otherwise they are linearly
dependent. The maximum number kmax of vectors that can be linearly inde-
pendent is equal to the dimension of the vector space n. An arbitrary set of n
linearly independent vectors a1,a2, . . . ,an forms a basis of the vector space.
Any vector a can be expressed as a linear combination of the basis vectors,
</p>
<p>a=
n&sum;
</p>
<p>j=1
αjaj . (A.2.2)
</p>
<p>A special basis is
</p>
<p>e1 =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>1
0
0
...
</p>
<p>0
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠
</p>
<p>, e2 =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>0
1
0
...
</p>
<p>0
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠
</p>
<p>, . . . , en =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>0
0
0
...
</p>
<p>1
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠
</p>
<p>. (A.2.3)
</p>
<p>These basis vectors are orthonormal, i.e.,
</p>
<p>ei &middot; ej = δij =
{
</p>
<p>1 , i = j ,
0 , i �= j . (A.2.4)
</p>
<p>The component aj of the vector a is the scalar product of a with the basis
vector ej ,
</p>
<p>a &middot; ej = aj , (A.2.5)
</p>
<p>cf. (A.1.1) and (A.1.9).
For n&le; 3 vectors can be visualized geometrically. A vector a can be rep-
</p>
<p>resented as an arrow of length a. The basis vectors (A.2.3) are perpendicular
to each other and are of unit length. The perpendicular projections of a onto
the directions of the basis vectors are the components (A.2.5), as shown in
Fig. A.1.</p>
<p/>
</div>
<div class="page"><p/>
<p>352 A Matrix Calculations
</p>
<p>e 1
</p>
<p>e2
</p>
<p>a1
</p>
<p>a2
</p>
<p>a
</p>
<p>0
</p>
<p>&AElig;
</p>
<p>&AElig;
</p>
<p>&AElig;
</p>
<p>Fig.A.1: The vector a in the sys-
tem of orthonormal basis vectors
e1, e2.
</p>
<p>A subset T of a vector space S is called a subspace if it is itself closed
under vector addition and multiplication with a scalar. The greatest possi-
ble number of linearly independent vectors in T is the dimension of T . The
product of an m&times;n matrix A with an n-vector a is an m-vector b,
</p>
<p>b= Aa . (A.2.6)
</p>
<p>The relation (A.2.6) can be regarded as a mapping or transformation of the
vector a onto the vector b.
</p>
<p>The span of a set of vectors a1, . . . ,ak is the vector space defined by the
set of all linear combinations u of these vectors,
</p>
<p>u=
k&sum;
</p>
<p>j=1
αjaj . (A.2.7)
</p>
<p>It has a dimension m &le; k. The column space of an m&times; n matrix A is the
span of the n column vectors of A; in this case the m-vectors u have the form
u=Ax with arbitrary n-vectors x. Clearly the dimension of the column space
is &le;min(m,n). Similarly, the row space of A is the span of the m row vectors.
</p>
<p>The null space or kernel of A consists of the set of vectors x for which
</p>
<p>Ax= 0 . (A.2.8)
</p>
<p>Column and row spaces of an m&times;n matrix have the same dimension. This is
called the rank of the matrix. An m&times;n has full rank if
</p>
<p>Rang(A)= min(m,n) . (A.2.9)
</p>
<p>Otherwise it has reduced rank. An n&times;n matrix with Rang(A) &lt; n is said to
be singular.</p>
<p/>
</div>
<div class="page"><p/>
<p>A.3 Orthogonal Transformations 353
</p>
<p>A vector a is orthogonal to a subspace T if it is orthogonal to every vector
t &isin; T . (A trivial example is t= t1e1 + t2e2, a= ae3, a &middot; t= 0.) A subspace U
is orthogonal to the subspace T if for every pair of vectors u &isin; U , t &isin; T one
has u &middot; t = 0. The set of all vectors u+ t forms a vector space V , called the
direct sum of T and U ,
</p>
<p>V = T &oplus;U . (A.2.10)
Its dimension is
</p>
<p>dim(V )= dim(T )+dim(U) . (A.2.11)
If (A.2.10) holds, then T and U are subspaces of V . They are called orthog-
onal complements, T = U&perp;, U = T &perp;. If T is a subspace of S, then there
always exists an orthogonal complement T &perp;, such that S = T &oplus; T &perp;. Every
vector a &isin; S can then be uniquely decomposed into the form a = t+u with
t&isin; T and u&isin; T &perp;. For the norms of the vectors the relation a2 = t2+u2 holds.
</p>
<p>If T is an (n&minus;1)-dimensional subspace of an n-dimensional vector space
S and if s is a fixed vector in S, then the set of all vectors h= s+ t with t &isin; T
forms an (n&minus; 1)-dimensional hyperplane H in S. If H is given and h0 is an
arbitrary fixed vector in H , then T is the set of all vectors t= h&minus;h0, h &isin; H ,
as shown in Fig. A.2. If û is a unit vector in the one-dimensional subspace
T =H&perp; orthogonal to H , then the scalar product
</p>
<p>û &middot;h= d (A.2.12)
</p>
<p>has the same value for all h &isin; H , where d is the distance of the hyperplane
from the origin (see Fig. A.3).
</p>
<p>e 1
</p>
<p>h
</p>
<p>h 0
</p>
<p>t
</p>
<p>e2
</p>
<p>0
H
</p>
<p>&AElig;
</p>
<p>&AElig;
</p>
<p>&AElig;
</p>
<p>&AElig;
</p>
<p>&AElig;
</p>
<p>Fig.A.2: Hyperplane H in a
two-dimensional vector space.
</p>
<p>For a given û and d, Eq. (A.2.12) defines a hyperplane H . It divides the
n-dimensional vector space into two half spaces, which consist of the set of
vectors x for which û &middot;x&lt; 0 and û &middot;x&gt; 0.
</p>
<p>A.3 Orthogonal Transformations
</p>
<p>According to (A.2.6), the mapping of an n-vector a onto an n-vector b is per-
formed by multiplication by a square n&times;n matrix, b =Qa. If the length of</p>
<p/>
</div>
<div class="page"><p/>
<p>354 A Matrix Calculations
</p>
<p>e2
</p>
<p>e 1
</p>
<p>h
</p>
<p>u
</p>
<p>H
0
</p>
<p>T = H
&perp;
</p>
<p>d
</p>
<p>&AElig;
</p>
<p>&AElig;
</p>
<p>&AElig;
</p>
<p>&AElig;
</p>
<p>Fig.A.3: Hyperplane H and
complementary one-dimen-
sional vector space T .
</p>
<p>the vector (A.1.14) remains unchanged, one speaks of an orthogonal trans-
formation. For such a case one has b = a or b2 = a2, i.e.,
</p>
<p>bTb= aTQTQa= aTa ,
</p>
<p>and thus
QTQ= I . (A.3.1)
</p>
<p>A square matrix Q that fulfills (A.3.1) is said to be orthogonal.
It is clear that transformations are orthogonal when the transformed vec-
</p>
<p>tor b is obtained from a by means of a spatial rotation and/or reflection. We
will examine some orthogonal transformations that are important for the ap-
plications of this appendix.
</p>
<p>A.3.1 Givens Transformation
</p>
<p>The Givens rotation is a transformation that affects only components in a
plane spanned by two orthogonal basis vectors. For simplicity we will first
consider only two-dimensional vectors.
</p>
<p>v
</p>
<p>e2e2
</p>
<p>e 1 e 1
</p>
<p>u
</p>
<p>J
</p>
<p>J
</p>
<p>&AElig;
</p>
<p>&AElig;
</p>
<p>&AElig;
</p>
<p>&AElig;
</p>
<p>&AElig;
</p>
<p>&AElig;
</p>
<p>&AElig;
</p>
<p>&AElig;
</p>
<p>u
</p>
<p>v&rsquo;
</p>
<p>&rsquo;
</p>
<p>(b)(a)
</p>
<p>Fig.A.4: Application
of the Givens trans-
formation (a) to the
vector v that defines
the transformation
and (b) to an arbitrary
vector u.</p>
<p/>
</div>
<div class="page"><p/>
<p>A.3 Orthogonal Transformations 355
</p>
<p>A vector v can be represented as
</p>
<p>v=
(
v1
</p>
<p>v2
</p>
<p>)
= v
</p>
<p>(
c
</p>
<p>s
</p>
<p>)
, c= cosϑ = v1/v , s = sinϑ = v2/v . (A.3.2)
</p>
<p>A rotation by an angle &minus;ϑ transforms the vector v into the vector v&prime; = Gv,
whose second component vanishes, as in Fig. A.4a. Clearly one has
</p>
<p>G=
(
</p>
<p>c s
</p>
<p>&minus;s c
</p>
<p>)
, v&prime; =Gv=
</p>
<p>(
v
</p>
<p>0
</p>
<p>)
. (A.3.3)
</p>
<p>Of course the transformation thus defined with the vector v can also be applied
to any other vector u, as shown in Fig. A.4b. In n dimensions the Givens
rotation leads to the transformation
</p>
<p>v&rarr; v&prime; =Gv ,
</p>
<p>such that v&prime;k = 0. The components v&prime;ℓ = vℓ, ℓ �= k, ℓ �= i, remain unchanged
and v&prime;i is determined such that the norm of the vector remains unchanged,
v&prime; = v. This is clearly given by
</p>
<p>G=
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>1
. . .
</p>
<p>1
c s
</p>
<p>1
. . .
</p>
<p>1
&minus;s c
</p>
<p>1
. . .
</p>
<p>1
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>&larr; 1
</p>
<p>&larr; i
</p>
<p>&larr; k
</p>
<p>&larr; n
</p>
<p>. (A.3.4)</p>
<p/>
</div>
<div class="page"><p/>
<p>356 A Matrix Calculations
</p>
<p>In practical applications, however, the full matrix is not needed. The method
</p>
<p>defines a Givens
</p>
<p>transformation by the input of the two components v ,v1 2,
</p>
<p>ponents of another vector. The method
</p>
<p>plies it to the defining vector.
</p>
<p>defineANDAp-
</p>
<p>A.3.2 Householder Transformation
</p>
<p>The Givens rotation is used to transform a vector in such a way that a given
</p>
<p>vector component vanishes. A more general transformation is the House-
</p>
<p>holder transformation. If the original vector is
</p>
<p>v =
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>v1
v2
...
</p>
<p>vn
</p>
<p>⎞
⎟⎟⎟⎠ , (A.3.5)
</p>
<p>then for the transformed vector we want to have
</p>
<p>v&prime; =Hv =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>v1
...
</p>
<p>vp&minus;1
v&prime;p
vp+1
...
</p>
<p>vℓ&minus;1
0
...
</p>
<p>0
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>. (A.3.6)
</p>
<p>That is, the components v&prime;ℓ, v
&prime;
ℓ+1, . . ., v
</p>
<p>&prime;
n should vanish. The remaining com-
</p>
<p>ponents should (with the exception of v&prime;p) remain unchanged. The component
v&prime;p must be changed in such a way that one has v = v&prime;. From the Pythagorean
theorem in n&minus; ℓ+1 dimensions one has
</p>
<p>v&prime;2p = v
2
H = v
</p>
<p>2
p+
</p>
<p>n&sum;
</p>
<p>i=ℓ
v2i
</p>
<p>DatanMatrix.defineGivensTransformation
</p>
<p>DatanMatrix._
</p>
<p>ApplyGivensTransformation applies that transformation to two com-
</p>
<p>DatanMatrix.
</p>
<p>plyGivensTransformation defines a transformation and directly ap-</p>
<p/>
</div>
<div class="page"><p/>
<p>A.3 Orthogonal Transformations 357
</p>
<p>or
</p>
<p>v&prime;p =&minus;σvH =&minus;σ
</p>
<p>&radic;&radic;&radic;&radic;v2p+
n&sum;
</p>
<p>i=ℓ
v2i (A.3.7)
</p>
<p>with σ =&plusmn;1. We choose
σ = sign(vp) . (A.3.8)
</p>
<p>We now construct the matrix H of (A.3.6). To do this we decompose the
vector v into a sum,
</p>
<p>v= vH +vH&perp; , vH =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>0
...
</p>
<p>0
vp
0
...
</p>
<p>0
vℓ
...
</p>
<p>vn
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>, vH&perp; =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>v1
...
</p>
<p>vp&minus;1
0
</p>
<p>vp+1
...
</p>
<p>vℓ&minus;1
0
...
</p>
<p>0
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>. (A.3.9)
</p>
<p>Here the vector vH is in the subspace spanned by the basis vectors ep, eℓ,
eℓ+1, . . ., en, and vH&perp; is in the subspace orthogonal to it. We now construct
</p>
<p>u= vH +σvH ep (A.3.10)
</p>
<p>and
</p>
<p>H = In&minus;
2uuT
</p>
<p>u2
. (A.3.11)
</p>
<p>If we now decompose an arbitrary vector a into a sum of vectors parallel and
perpendicular to u,
</p>
<p>a= a‖+a&perp; ,
with
</p>
<p>a‖ =
uuT
</p>
<p>u2
a= u
</p>
<p>u2
(u &middot;a)= û(̂u &middot;a) , a&perp; = a&minus;a‖ ,
</p>
<p>then one has
</p>
<p>a&prime;‖ =Ha‖ =&minus;a‖ , a&prime;&perp; =Ha&perp; = a&perp; .
</p>
<p>Thus we see that the transformation is a reflection in the subspace that is
orthogonal to the vector u, as in Fig. A.5. One can easily verify that H in fact
yields the transformation (A.3.6).
</p>
<p>The transformation is uniquely determined by the vector u. According to
(A.3.10) one has for the components of this vector up = vp+σvH , uℓ = vℓ,</p>
<p/>
</div>
<div class="page"><p/>
<p>358 A Matrix Calculations
</p>
<p>v||
</p>
<p>U
</p>
<p>v
</p>
<p>u
</p>
<p>e
</p>
<p>e
v
</p>
<p>v
v e
</p>
<p>v
</p>
<p>v&minus;
δ
</p>
<p>1
</p>
<p>1
</p>
<p>2
</p>
<p>&perp;
</p>
<p>&perp;
</p>
<p>&rsquo;
</p>
<p>||
</p>
<p>&AElig;
</p>
<p>&AElig;
</p>
<p>&AElig;
</p>
<p>&AElig;
&AElig;
</p>
<p>&AElig;
</p>
<p>&AElig; &AElig;
</p>
<p>&AElig;
</p>
<p>&AElig;
</p>
<p>Fig.A.5: The vector v is
mapped onto v&prime; according
to a Householder transfor-
mation such that v&prime;2 = 0.
The mapping corresponds
to a reflection in the sub-
space U that is orthogonal
to the auxiliary vector u.
</p>
<p>uℓ+1 = vℓ+1, . . ., un = vn, and ui = 0 for all other i. If the vector v and the
indices p and ℓ are given, then only up must be computed. The quantity u2
</p>
<p>appearing in (A.3.11) is then
</p>
<p>u2 = u2p+
n&sum;
</p>
<p>i=ℓ
u2i = (vp+σvH )2 +
</p>
<p>n&sum;
</p>
<p>i=ℓ
v2i
</p>
<p>= v2p+
n&sum;
</p>
<p>i=ℓ
v2i +v2H +2σvHvp
</p>
<p>= 2v2H +2σvHvp = 2vH (vH +σvp)= 2vHup .
</p>
<p>We can thus write (A.3.11) in the form
</p>
<p>H = In&minus;buuT , b= (vHup)&minus;1 . (A.3.12)
</p>
<p>The matrix H , however, is not needed explicitly to compute a trans-
formed vector,
</p>
<p>c&prime; =Hc .
</p>
<p>It is sufficient to know the vector u and the constant b. Since, however, u
</p>
<p>only differs from v in the element up (and in the vanishing elements), it is
</p>
<p>sufficient, starting from v, to first compute the quantities up and b and when
</p>
<p>applying the transformation to use in addition the elements vℓ, vℓ+1, . . ., vn.
These are at the same time the corresponding elements of u.
</p>
<p>By the method DatanMatrix.defineHouseholderTranfor-
</p>
<p>mation a transformation is defined; with DatanMatrix.applyHouse-
</p>
<p>HolderTransformation it is applied to a vector.</p>
<p/>
</div>
<div class="page"><p/>
<p>A.3 Orthogonal Transformations 359
</p>
<p>A.3.3 Sign Inversion
</p>
<p>If the diagonal element Iii of the unit matrix is replaced by &minus;1, then one
obtains a symmetric orthogonal matrix,
</p>
<p>R(i) =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>1
. . .
</p>
<p>&minus;1
. . .
</p>
<p>1
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠
</p>
<p>.
</p>
<p>Applying this to the vector a,
</p>
<p>a&prime; = R(i)a ,
</p>
<p>changes the sign of the element ai and leaves all the other elements un-
changed. Clearly R(i) is a Householder matrix which produces a reflection
in the subspace orthogonal to the basis vector ei . This can be seen immedi-
ately by substituting u= ei in (A.3.11).
</p>
<p>A.3.4 Permutation Transformation
</p>
<p>The n&times;n unit matrix (A.1.10) can easily be written as an arrangement of the
basis vectors (A.2.3) in a square matrix,
</p>
<p>In = (e1,e2, . . . ,en)=
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>1 0 &middot; &middot; &middot; 0
0 1 &middot; &middot; &middot; 0
...
</p>
<p>0 0 &middot; &middot; &middot; 1
</p>
<p>⎞
⎟⎟⎟⎠ .
</p>
<p>It is clearly an orthogonal matrix. The orthogonal transformation Ia = a
leaves the vector a unchanged. If we now exchange two of the basis vec-
tors ei and ek, then we obtain the symmetric orthogonal matrix P ik. As an
example we show this for n= 4, i = 2, k = 4,
</p>
<p>P (ik) =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>1 0 0 0
0 0 0 1
0 0 1 0
0 1 0 0
</p>
<p>⎞
⎟⎟⎠ .
</p>
<p>The transformation a&prime; = P (ik)a leads to an exchange of the elements ai and
ak. All other elements remain unchanged:</p>
<p/>
</div>
<div class="page"><p/>
<p>360 A Matrix Calculations
</p>
<p>a=
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>a1
...
</p>
<p>ai
...
</p>
<p>ak
...
</p>
<p>an
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>, a&prime; = P (ik)a=
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>a1
...
</p>
<p>ak
...
</p>
<p>ai
...
</p>
<p>an
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>.
</p>
<p>Multiplication of an n&times;m matrix A on the left with P (ik) permutes the lines
i and k of A. Multiplication of an m&times;n matrix A from the right exchanges
the columns i and k. If D is an n&times;n diagonal matrix, then the elements Dii
and Dkk are exchanged by the operation
</p>
<p>D&prime; = P (ik)DP (ik) .
</p>
<p>A.4 Determinants
</p>
<p>To every n&times;n matrix A one can associate a number, its determinant, detA.
A determinant, like the corresponding matrix, is written as a square arrange-
ment of the matrix elements, but is enclosed, however, by vertical lines. The
determinants of orders two and three are defined by
</p>
<p>detA=
∣∣∣∣
A11 A12
A21 A22
</p>
<p>∣∣∣∣= A11A22 &minus;A12A21 (A.4.1)
</p>
<p>and
</p>
<p>detA =
</p>
<p>∣∣∣∣∣∣
</p>
<p>A11 A12 A13
A21 A22 A23
A31 A32 A33
</p>
<p>∣∣∣∣∣∣
</p>
<p>=
A11A22A33 &minus; A11A23A32
</p>
<p>+ A12A23A31 &minus; A12A21A33
+ A13A21A32 &minus; A13A22A31
</p>
<p>(A.4.2)
</p>
<p>or, written in another way,
</p>
<p>detA =
A11(A22A33 &minus; A23A32)
</p>
<p>&minus; A12(A21A33 &minus; A23A31)
+ A13(A21A32 &minus; A22A31) .
</p>
<p>(A.4.3)
</p>
<p>A general determinant of order n is written in the form</p>
<p/>
</div>
<div class="page"><p/>
<p>A.4 Determinants 361
</p>
<p>detA=
</p>
<p>∣∣∣∣∣∣∣∣∣
</p>
<p>A11 A12 . . . A1n
A21 A22 . . . A2n
...
</p>
<p>An1 An2 . . . Ann
</p>
<p>∣∣∣∣∣∣∣∣∣
. (A.4.4)
</p>
<p>The cofactor A&dagger;ij of the element Aij of a matrix is a determinant of order
(n&minus; 1), calculated from the matrix obtained by deleting the ith row and j th
column of the original matrix, and multiplying by (&minus;1)i+j ,
</p>
<p>A
&dagger;
ij = (&minus;1)i+j
</p>
<p>∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣
</p>
<p>A11 A12 . . . A1,j&minus;1 A1,j+1 . . . A1n
A21 A22 . . . A2,j&minus;1 A2,j+1 . . . A2n
...
</p>
<p>Ai&minus;1,1 Ai&minus;1,2 . . . Ai&minus;1,j&minus;1 Ai&minus;1,j+1 . . . Ai&minus;1,n
Ai+1,1 Ai+1,2 . . . Ai+1,j&minus;1 Ai+1,j+1 . . . Ai+1,n
...
</p>
<p>An1 An2 . . . An,j&minus;1 An,j+1 . . . Ann
</p>
<p>∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣
</p>
<p>.
</p>
<p>(A.4.5)
Determinants of higher order can be written as the sum of all elements of
</p>
<p>any row or column multiplied with the corresponding cofactors,
</p>
<p>detA=
n&sum;
</p>
<p>k=1
AikA
</p>
<p>&dagger;
ik =
</p>
<p>n&sum;
</p>
<p>k=1
AkjA
</p>
<p>&dagger;
kj . (A.4.6)
</p>
<p>One can easily show that the result is independent of the choice of row i or
column j . Equation (A.4.3) already shows that (A.4.6) is correct for n= 3.
Determinants of arbitrary order can be computed by decomposing them ac-
cording to their cofactors until one reaches, for example, the order two. A
singular matrix, i.e., a square matrix whose rows or columns are not linearly
independent, has determinant zero.
</p>
<p>From A we can construct a further matrix by replacing each element ij
by the cofactor of the element j i. In this way we obtain the adjoint matrix
of A,
</p>
<p>A&dagger; =
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>A
&dagger;
11 A
</p>
<p>&dagger;
21 . . . A
</p>
<p>&dagger;
n1
</p>
<p>A
&dagger;
12 A
</p>
<p>&dagger;
22 . . . A
</p>
<p>&dagger;
n2
</p>
<p>...
</p>
<p>A
&dagger;
1n A
</p>
<p>&dagger;
2n . . . A
</p>
<p>&dagger;
nn
</p>
<p>⎞
⎟⎟⎟⎠ . (A.4.7)
</p>
<p>For determinants the following rules hold:
</p>
<p>detA = detAT , (A.4.8)
detAB = detAdetB . (A.4.9)
</p>
<p>For an orthogonal matrix Q one has QQT = I , i.e.,</p>
<p/>
</div>
<div class="page"><p/>
<p>362 A Matrix Calculations
</p>
<p>detI = 1 = detQdetQ
</p>
<p>and thus
</p>
<p>detQ=&plusmn;1 . (A.4.10)
</p>
<p>A.5 Matrix Equations: Least Squares
</p>
<p>A system of m linear equations with n unknowns x1, x2, . . ., xn has the general
form
</p>
<p>a11x1 +a12x2 +&middot;&middot; &middot;+a1nxn&minus;b1 = 0 ,
a21x1 +a22x2 +&middot;&middot; &middot;+a2nxn&minus;b2 = 0 ,
...
</p>
<p>am1x1 +am2x2 +&middot;&middot; &middot;+amnxn&minus;bm = 0
</p>
<p>(A.5.1)
</p>
<p>or in matrix notation
</p>
<p>Ax&minus;b= 0 . (A.5.2)
</p>
<p>In finding the solution to this equation x we must distinguish between various
cases, which can be characterized by the values of m, n, and
</p>
<p>k = Rang(A) .
</p>
<p>The vector Ax is in the column space of A, which is of dimension k.
Since b is an m-vector, the equation can in general only be fulfilled if k =m,
i.e., for k= n=m and for k=m&lt;n, since k&le;min(m,n). For k= n=m one
has n independent equations (A.5.1) with n unknowns, which have a unique
solution. If k =m&lt; n, then there are arbitrarily many n-vectors x that can be
mapped on to the m-vector Ax= b such that (A.5.2) is fulfilled. The system
of equations is underdetermined. The solution is not unique.
</p>
<p>For k = Rang(A) �=m there is, in general, no solution of (A.5.2). In this
case we look for a vector x̃, for which the left-hand side of (A.5.2) is a vector
of minimum Euclidian norm. That is, we replace Eq. (A.5.2) by
</p>
<p>r2 = (Ax&minus;b)2 = min , (A.5.3)
</p>
<p>i.e., we look for a vector x̃ for which the mapping Ãx differs as little as possi-
ble from b.
</p>
<p>Given a m-vector c, only for k = Rang(A)= n does there exist only one
n-vector x, such that Ax = c. Therefore, only for the case k = n is there a
unique solution x̃. Thus there exists for Rang(A) = n and m &ge; n a unique
solution x̃ of (A.5.3). For n=m one has r = 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>A.5 Matrix Equations: Least Squares 363
</p>
<p>The relation (A.5.3) is also often written simply in the form
</p>
<p>Ax&minus;b&asymp; 0 (A.5.4)
</p>
<p>or even, not entirely correctly, in the form (A.5.2). One calls the solution
vector x̃ the least-squares solution of (A.5.4). In Table A.1 we list again the
various cases that result from different relationships between m, n, and k.
</p>
<p>TableA.1: Behavior of the solutions of (A.5.3) for various cases. m is the row number, n is
the column number and k is the rank of the matrix A.
</p>
<p>Case Rang(A) Residual Solution unique
1a m= n k = n r = 0 Yes
1b k &lt; n r &ge; 0 No
2a m&gt; n k = n r &ge; 0 Yes
2b k &lt; n r &ge; 0 No
3a m&lt; n k =m r = 0 No
3b k &lt; m r &ge; 0 No
</p>
<p>We now want to state more formally what we have determined in this
section with respect to the solution of (A.5.4).
</p>
<p>Theorem on the orthogonal decomposition of a matrix:
</p>
<p>Every m&times;n matrix A of rank k can be written in the form
</p>
<p>A=H RKT , (A.5.5)
</p>
<p>where H is an m&times;m orthogonal matrix, K is an n&times;n orthogo-
nal matrix, and R is an m&times;n matrix of the form
</p>
<p>R =
(
</p>
<p>R11 0
0 0
</p>
<p>)
(A.5.6)
</p>
<p>and where R11 is a k&times;k matrix of rank k.
</p>
<p>Substituting (A.5.5) into (A.5.4) and multiplying from the left by HT
</p>
<p>leads to
RKT x&asymp;HTb . (A.5.7)
</p>
<p>We define
</p>
<p>HTb= g=
(
</p>
<p>g1
g2
</p>
<p>)
}k
}m&minus; k (A.5.8)
</p>
<p>and
</p>
<p>KTx= p=
(
</p>
<p>p1
p2
</p>
<p>)
}k
}n&minus; k , (A.5.9)</p>
<p/>
</div>
<div class="page"><p/>
<p>364 A Matrix Calculations
</p>
<p>so that (A.5.7) takes on the form
</p>
<p>Rp&asymp; g . (A.5.10)
</p>
<p>Because of (A.5.6), this breaks into two independent relations
</p>
<p>R11p1 = g1 (A.5.11)
</p>
<p>and
0 &middot;p2 &asymp; g2 . (A.5.12)
</p>
<p>If m= k and/or n= k, then the corresponding lower partial vectors are absent
in (A.5.8) and/or (A.5.9) and the corresponding lower matrices are absent in
(A.5.6). Since in (A.5.11) R11 is a k&times; k matrix of rank k, and p1 and g1 are
k-vectors, there exists a solution vector p̃1 for which equality holds. Because
of the null matrix on the left-hand side of (A.5.12), we cannot derive any
information about p2 from this relation.
</p>
<p>Theorem on the solutions of Ax &asymp; b: If p̃1 is the unique
solution vector of (A.5.11), then the following statements hold:
</p>
<p>(i) All solutions of (A.5.3) have the form
</p>
<p>x̂=K
(
</p>
<p>p̃1
p̃2
</p>
<p>)
. (A.5.13)
</p>
<p>Here p̃2 is an arbitrary (n&minus; k)-vector, i.e., the solution is
unique for k = n. There is always, however, a unique solu-
tion of minimum absolute value:
</p>
<p>x̃=K
(
</p>
<p>p̃1
0
</p>
<p>)
. (A.5.14)
</p>
<p>(ii) All solutions x̂ have the same residual vector
</p>
<p>r= b&minus; Âx=H
(
</p>
<p>0
g2
</p>
<p>)
(A.5.15)
</p>
<p>with the absolute value r = g2 = |g2|. The residual vanishes
for k =m.
</p>
<p>The problem Ax&asymp; b should always be handled with orthogonal decom-
positions, preferably with the singular value decomposition and singular value
analysis described in Sects. A.12 and A.13. Numerically the results are at least
as accurate as with other methods, and are often more accurate (cf. Sect. A.13,
Example A.4).</p>
<p/>
</div>
<div class="page"><p/>
<p>A.6 Inverse Matrix 365
</p>
<p>Nevertheless we will briefly present as well the method of normal equa-
tions. The method is very transparent compared to the orthogonal decompo-
sition and is therefore always described in textbooks.
</p>
<p>We consider the square (A.5.3) of the residual vector
</p>
<p>r2 = (Ax&minus;b)T(Ax&minus;b)= xTATAx&minus;2bTAx+bTb
</p>
<p>=
m&sum;
</p>
<p>i=1
</p>
<p>n&sum;
</p>
<p>j=1
</p>
<p>n&sum;
</p>
<p>ℓ=1
AijAiℓxjxℓ&minus;2
</p>
<p>m&sum;
</p>
<p>i=1
</p>
<p>n&sum;
</p>
<p>j=1
biAijxj +
</p>
<p>m&sum;
</p>
<p>i=1
b2i .
</p>
<p>The requirement r2 = min leads to
</p>
<p>&part;r2
</p>
<p>&part;xk
= 2
</p>
<p>m&sum;
</p>
<p>i=1
</p>
<p>n&sum;
</p>
<p>ℓ=1
AikAiℓxℓ&minus;2
</p>
<p>m&sum;
</p>
<p>i=1
biAik = 0 , k = 1, . . . ,n .
</p>
<p>These n linear equations are called normal equations. They can be arranged
in a matrix,
</p>
<p>ATAx=ATb . (A.5.16)
This is a system of n equations with n unknowns. If the equations are
</p>
<p>linearly independent, then the n&times;n matrix (ATA) is of full rank n; it is not
singular. According to Sect. A.6 there then exists an inverse (ATA)&minus;1, such
that (ATA)&minus;1(ATA)= I . Thus the desired solution to (A.5.3) is
</p>
<p>x̃= (ATA)&minus;1ATb . (A.5.17)
</p>
<p>This simple prescription is, however, useless if ATA is singular or nearly
singular (cf. Example A.4).
</p>
<p>A.6 Inverse Matrix
</p>
<p>For every non-singular n&times;n matrix A, the inverse matrix A&minus;1 is defined by
</p>
<p>AA&minus;1 = In = A&minus;1A . (A.6.1)
</p>
<p>It is also an n&times;n matrix.
If A&minus;1 is known, then the solution of the matrix equation
</p>
<p>Ax= b (A.6.2)
</p>
<p>is given simply by
x= A&minus;1b . (A.6.3)
</p>
<p>As we will show, A&minus;1 only exists for non-singular square matrices, so that
(A.6.3) only gives the solution for the case 1a of Table A.1.</p>
<p/>
</div>
<div class="page"><p/>
<p>366 A Matrix Calculations
</p>
<p>In order to determine A&minus;1, we set A&minus;1 =X and express the column vec-
tors of X as x1, x2, . . ., xn. Equation (A.6.1) is then decomposed into n
equations:
</p>
<p>Axi = ei , i = 1,2, . . . ,n . (A.6.4)
The right-hand sides are the basis vectors (A.2.3). For the case n= 2 we can
write the system (A.6.4) in various equivalent ways, e.g.,
(
</p>
<p>A11 A12
A21 A22
</p>
<p>)(
X11
X21
</p>
<p>)
=
(
</p>
<p>1
0
</p>
<p>)
,
</p>
<p>(
A11 A12
A21 A22
</p>
<p>)(
X12
X22
</p>
<p>)
=
(
</p>
<p>0
1
</p>
<p>)
,
</p>
<p>(A.6.5)
or alternatively
</p>
<p>(
A11 A12
A21 A22
</p>
<p>)(
X11 X12
X21 X22
</p>
<p>)
=
</p>
<p>(
1 0
0 1
</p>
<p>)
,
</p>
<p>or as a system of four equations with four unknowns,
</p>
<p>A11X11 +A12X21 = 1 ,
A21X11 +A22X21 = 0 ,
A11X12 +A12X22 = 0 ,
A21X12 +A22X22 = 1 .
</p>
<p>(A.6.6)
</p>
<p>By elimination and substitution one easily finds
</p>
<p>X11 =
A22
</p>
<p>A11A22 &minus;A12A21
,
</p>
<p>X12 =
&minus;A12
</p>
<p>A11A22 &minus;A12A21
, (A.6.7)
</p>
<p>X21 =
&minus;A21
</p>
<p>A11A22 &minus;A12A21
,
</p>
<p>X22 =
A11
</p>
<p>A11A22 &minus;A12A21
or in matrix notation
</p>
<p>X = A&minus;1 = 1
detA
</p>
<p>(
A22 &minus;A12
</p>
<p>&minus;A21 A11
</p>
<p>)
. (A.6.8)
</p>
<p>The matrix on the right-hand side is the adjoint of the original matrix, i.e.,
</p>
<p>A&minus;1 = A
&dagger;
</p>
<p>detA
. (A.6.9)
</p>
<p>One can show that this relation holds for square matrices of arbitrary order.
From (A.6.9) it is clear that the inverse of a singular matrix, i.e., of a matrix
with vanishing determinant, is not determined.</p>
<p/>
</div>
<div class="page"><p/>
<p>A.7 Gaussian Elimination 367
</p>
<p>In practice the inverse matrix is not computed using (A.6.9) but rather as
in our example of the 2&times; 2 matrix by elimination and substitution from the
system (A.6.4). This system consists of n sets of equations of the form
</p>
<p>Ax= b , (A.6.10)
</p>
<p>each consisting of n equations with n unknowns. Here A is a non-singular
n&times;n matrix. We will first give the solution algorithm for square non-singular
matrices A, but we will later remove these restrictions.
</p>
<p>A.7 Gaussian Elimination
</p>
<p>We will write Eq. (A.6.10) for an n&times;n matrix A in components,
</p>
<p>A11x1 +A12x2 +&middot;&middot; &middot;+A1nxn = b1 ,
A21x1 +A22x2 +&middot;&middot; &middot;+A2nxn = b2 ,
...
</p>
<p>An1x1 +An2x2 +&middot;&middot; &middot;+Annxn = bn ,
</p>
<p>(A.7.1)
</p>
<p>and we will solve the system by Gaussian elimination. For this we define
n&minus;1 multipliers
</p>
<p>mi1 =
Ai1
</p>
<p>A11
, i = 2,3, . . . ,n , (A.7.2)
</p>
<p>multiply the first equation by m21, and subtract it from the second. We then
multiply the first equation by m31 and subtract it from the third, and so forth.
We obtain the system
</p>
<p>A
(1)
11 x1 + A
</p>
<p>(1)
12 x2 +&middot;&middot; &middot;+A
</p>
<p>(1)
1n xn = b
</p>
<p>(1)
1 ,
</p>
<p>A
(2)
22 x2 +&middot;&middot; &middot;+A
</p>
<p>(2)
2n xn = b
</p>
<p>(2)
2 , (A.7.3)
</p>
<p>...
</p>
<p>A
(2)
n2 x2 +&middot;&middot; &middot;+A(2)nn xn = b(2)n ,
</p>
<p>where the unknown x1 has disappeared from all the equations except the first.
The coefficients A(2)ij , b
</p>
<p>(2)
i are given by the equations
</p>
<p>A
(2)
ij = A
</p>
<p>(1)
ij &minus;mi1A
</p>
<p>(1)
1j ,
</p>
<p>b
(2)
i = b
</p>
<p>(1)
i &minus;mi1b
</p>
<p>(1)
1 .
</p>
<p>The procedure is then repeated with the last n&minus;1 equations by defining</p>
<p/>
</div>
<div class="page"><p/>
<p>368 A Matrix Calculations
</p>
<p>mi2 =
A
(2)
i2
</p>
<p>A
(2)
22
</p>
<p>, i = 3,4, . . . ,n ,
</p>
<p>multiplying the second equation of the system (A.7.3) with the corresponding
mi2, and then subtracting it from the third, fourth, . . ., nth equation. In the kth
step of the procedure, the multipliers
</p>
<p>mik =
A
(k)
ik
</p>
<p>A
(k)
kk
</p>
<p>, i = k+1, k+2, . . . ,n , (A.7.4)
</p>
<p>are used and the new coefficients
</p>
<p>A
(k+1)
ij = A
</p>
<p>(k)
ij &minus;mikA
</p>
<p>(k)
kj ,
</p>
<p>b
(k+1)
i = b
</p>
<p>(k)
i &minus;mikb
</p>
<p>(k)
k (A.7.5)
</p>
<p>are computed. After n&minus; 1 steps we have produced the following triangular
system of equations:
</p>
<p>A
(1)
11 x1+ A
</p>
<p>(1)
12 x2 +&middot;&middot; &middot;+ A
</p>
<p>(1)
1n xn = b
</p>
<p>(1)
1 ,
</p>
<p>A
(2)
22 x2 +&middot;&middot; &middot;+ A
</p>
<p>(2)
2n xn = b
</p>
<p>(2)
2 ,
</p>
<p>...
...
</p>
<p>A
(n)
nn xn = b(n)n .
</p>
<p>(A.7.6)
</p>
<p>The last equation contains only xn. By substituting into the next higher
equation one obtains xn&minus;1, i.e., in general
</p>
<p>xi =
1
</p>
<p>A
(i)
ii
</p>
<p>{
b
(i)
i &minus;
</p>
<p>n&sum;
</p>
<p>ℓ=i+1
A
(i)
iℓ xℓ
</p>
<p>}
, i = n,n&minus;1, . . . ,1 . (A.7.7)
</p>
<p>One should note that the change of the elements of the matrix A does not
depend on the right-hand side b. Therefore one can reduce several systems of
equations with different right-hand sides simultaneously. That is, instead of
Ax= b, one can solve the more general system
</p>
<p>AX = B (A.7.8)
</p>
<p>at the same time, where B and X are n&times;m matrices, the matrix X being
unknown.</p>
<p/>
</div>
<div class="page"><p/>
<p>A.8 LR-Decomposition 369
</p>
<p>Example A.1: Inversion of a 3&times;3 matrix
As a numerical example let us consider the inversion of a 3&times; 3 matrix, i.e.,
B = I . The individual computations for the example
</p>
<p>⎛
⎝
</p>
<p>1 2 3
2 1 &minus;2
1 1 2
</p>
<p>⎞
⎠X = I
</p>
<p>are shown in Table A.2. The result is
</p>
<p>X = 1
5
</p>
<p>⎛
⎝
</p>
<p>&minus;4 1 7
6 1 &minus;8
</p>
<p>&minus;1 &minus;1 3
</p>
<p>⎞
⎠ .
</p>
<p>Following the individual steps of the calculation one sees that division is
carried out in two places, namely in Eqs. (A.7.4) and (A.7.7). The denomina-
tor is in both cases a coefficient
</p>
<p>A
(i)
ii , i = 1,2, . . . ,n&minus;1 ,
</p>
<p>i.e., the upper left-hand coefficient of the system, the so-called pivot for the
step i&minus; 1 of the reduction process. Our procedure fails if this coefficient is
equal to zero. In such a case one can simply exchange the ith line of the
system with some other lower line whose first coefficient is not zero. The
system of equations itself is clearly not changed by exchanging two equations.
The procedure still fails if all of the coefficients of a column vanish. In this
case the matrix A is singular, and there is no solution. In practice (at least
when using computers where the extra work is negligible) it is advantageous
to always carry out a reshuffling so that the pivot is the coefficient with the
largest absolute value among those in the first column of the reduced system.
One then always has the largest possible denominator. In this way rounding
errors are kept as small as possible. The procedure just described is called
Gaussian elimination with pivoting.
</p>
<p>Eq. (A.7.8). In the same way the method
mines the inverse of a square nonsingular matrix.
</p>
<p>A.8 LR-Decomposition
</p>
<p>For the elements A(k)ij transformed by Gaussian elimination [cf. (A.7.6)], one
has on and above the main diagonal
</p>
<p>A
(n)
ij = A
</p>
<p>(n&minus;1)
ij = &middot;&middot; &middot; = A
</p>
<p>(i)
ij , i &le; j , (A.8.1)
</p>
<p>Following it, the method DatanMatrix.matrixEquation solves
</p>
<p>DatanMatrix.inverse deter-</p>
<p/>
</div>
<div class="page"><p/>
<p>370 A Matrix Calculations
</p>
<p>TableA.2: Application of Gaussian elimination to Example A.1.
Reduction
</p>
<p>Matrix A Matrix B Multiplier
</p>
<p>1 2 3 1 0 0 &ndash;
Step 0 2 1 &minus;2 0 1 0 2
</p>
<p>1 1 2 0 0 1 1
</p>
<p>&minus;3 &minus;8 &minus;2 1 0 &ndash;
Step 1 &minus;1 &minus;1 &minus;1 0 1 13
</p>
<p>Step 2 53 &minus;
1
3 &minus;
</p>
<p>1
3 1 &ndash;
</p>
<p>Substitution
</p>
<p>j = 1
x3j &minus;15
</p>
<p>x2j &minus;13(&minus;2&minus;8&times;
1
5)=
</p>
<p>6
5
</p>
<p>x1j 1&minus;2&times; 65 +3&times;
1
5 =&minus;
</p>
<p>4
5
</p>
<p>j = 2
x3j &minus;15
</p>
<p>x2j &minus;13(1&minus;
8
5)=
</p>
<p>1
5
</p>
<p>x1j &minus;2&times; 15 +3&times;
1
5 =
</p>
<p>1
5
</p>
<p>j = 3
x3j
</p>
<p>3
5
</p>
<p>x2j &minus;13(0+8&times;
3
5)=&minus;
</p>
<p>8
5
</p>
<p>x1j 2&times; 85 &minus;3&times;
3
5 =
</p>
<p>7
5</p>
<p/>
</div>
<div class="page"><p/>
<p>A.8 LR-Decomposition 371
</p>
<p>and below the main diagonal
</p>
<p>A
(n)
ij = A
</p>
<p>(n&minus;1)
ij = &middot;&middot; &middot; = A
</p>
<p>(j+1)
ij = 0 , i &gt; j . (A.8.2)
</p>
<p>The transformation (A.7.5) is thus only carried out for k = 1,2, . . . , r with
r = min(i&minus;1, j),
</p>
<p>A
(k+1)
ij = A
</p>
<p>(k)
ij &minus;mikA
</p>
<p>(k)
kj . (A.8.3)
</p>
<p>Summation over k gives
</p>
<p>r&sum;
</p>
<p>k=1
A
(k+1)
ij &minus;
</p>
<p>r&sum;
</p>
<p>k=1
A
(k)
ij = A
</p>
<p>(r+1)
ij &minus;Aij =&minus;
</p>
<p>r&sum;
</p>
<p>k=1
mikA
</p>
<p>(k)
kj .
</p>
<p>Thus the elements A(1)ij = Aij of the original matrix A can be written in the
form
</p>
<p>Aij = A(i)ij +
&sum;i&minus;1
</p>
<p>k=1mikA
(k)
kj , i &le; j ,
</p>
<p>Aij = 0+
&sum;j
</p>
<p>k=1mikA
(k)
kj , i &gt; j .
</p>
<p>(A.8.4)
</p>
<p>Noting that the multipliers mik have only been defined up to now for i &gt; k,
we can in addition set
</p>
<p>mii = 1 , i = 1,2, . . . ,n ,
</p>
<p>and reduce the two relations (A.8.4) to one,
</p>
<p>Aij =
p&sum;
</p>
<p>k=1
mikA
</p>
<p>(k)
kj , 1 &le; i, j &le; n , p = min(i,j) .
</p>
<p>The equation shows directly that the matrix A can be represented as the prod-
uct of two matrices L and R. Here L is a lower and R an upper triangular
matrix,
</p>
<p>A= LR , (A.8.5)
</p>
<p>L =
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>m11
m21 m22
...
</p>
<p>mn1 mn2 . . . mnn
</p>
<p>⎞
⎟⎟⎟⎠ , mii = 1 ,
</p>
<p>R =
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>r11 r12 . . . r1n
r22 . . . r2n
</p>
<p>...
</p>
<p>rnn
</p>
<p>⎞
⎟⎟⎟⎠=
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>A
(1)
11 A
</p>
<p>(1)
12 . . . A
</p>
<p>(1)
1n
</p>
<p>A
(2)
22 . . . A
</p>
<p>(2)
2n
</p>
<p>...
</p>
<p>A
(n)
nn
</p>
<p>⎞
⎟⎟⎟⎠ .
</p>
<p>The original system of equations (A.6.10)</p>
<p/>
</div>
<div class="page"><p/>
<p>372 A Matrix Calculations
</p>
<p>Ax= LR x= b
</p>
<p>is thus equivalent to two triangular systems of equations,
</p>
<p>Ly= b , R x= y .
</p>
<p>Instead of resorting to the formulas from Sect. A.7, we can compute the
elements of L and R directly from (A.8.5). We obtain
</p>
<p>rkj =Akj &minus;
&sum;k&minus;1
</p>
<p>ℓ=1 mkℓrℓj , j = k, k+1, . . . , n
mik =
</p>
<p>(
Aik&minus;
</p>
<p>&sum;k&minus;1
ℓ=1 miℓrℓk
</p>
<p>)
/rkk , i = k+1, . . . , n
</p>
<p>}
k = 1, . . . ,n ,
</p>
<p>(A.8.6)
</p>
<p>i.e., for k = 1 one computes the first row of R (which is equal to the first row
of A) and the first column of L; for k = 2 one computes the second row of
R and the second column of L. In a computer program the elements of R
and L can overwrite the original matrix A with the same indices, since when
computing rkj one only needs the element Akj and other elements of R and
L that have already been computed according to (A.8.6). The corresponding
consideration holds for the computation of mik.
</p>
<p>The algorithm (A.8.6) is called the Doolittle LR-decomposition. By in-
cluding pivoting this is clearly equivalent to Gaussian elimination.
</p>
<p>A.9 Cholesky Decomposition
</p>
<p>If A is a real symmetric positive-definite matrix (cf. Sect. A.11), then it can
be uniquely expressed as
</p>
<p>A= UTU . (A.9.1)
Here U is a real upper triangular matrix with positive diagonal elements. The
n&times;n matrix A has the required property for the validity of (A.9.1), in partic-
ular in those cases where it is equal to the product of an arbitrary real n&times;m
matrix B (with m&ge; n and full rank) with its transpose BT, A= BBT.
</p>
<p>To determine U we first carry out the Doolittle LR-decomposition, define
a diagonal matrix D whose diagonal elements are equal to those of R,
</p>
<p>D = diag(r11, r22, . . . , rnn) , (A.9.2)
</p>
<p>and introduce the matrices
</p>
<p>D&minus;1 = diag(r&minus;111 , r
&minus;1
22 , . . . , r
</p>
<p>&minus;1
nn ) (A.9.3)
</p>
<p>and
D&minus;1/2 = diag(r&minus;1/211 , r
</p>
<p>&minus;1/2
22 , . . . , r
</p>
<p>&minus;1/2
nn ) . (A.9.4)</p>
<p/>
</div>
<div class="page"><p/>
<p>A.9 Cholesky Decomposition 373
</p>
<p>We can then write
</p>
<p>A= LR = LDD&minus;1R = LDR&prime; , R&prime; =D&minus;1R . (A.9.5)
</p>
<p>Because of the assumed symmetry one has
</p>
<p>A= AT = (R&prime;)TDLT . (A.9.6)
</p>
<p>Comparing with (A.9.5) gives LT = R&prime;, i.e.,
</p>
<p>L= RTD&minus;1 .
</p>
<p>With
U =D&minus;1/2R =D1/2LT (A.9.7)
</p>
<p>Eq. (A.9.1) is indeed fulfilled. The relation (A.9.7) means for the elements uij
of U
</p>
<p>uij = r&minus;1/2ii rij = r
1/2
ii mji .
</p>
<p>Thus the values r and m can be eliminated in favor of u from (A.8.6), and we
obtain with
</p>
<p>ukk =
(
Akk&minus;
</p>
<p>&sum;k&minus;1
ℓ=1 u
</p>
<p>2
ℓk
</p>
<p>)1/2
</p>
<p>ukj =
(
Akj &minus;
</p>
<p>&sum;k&minus;1
ℓ=1 uℓkuℓj
</p>
<p>)
/ukk , j = k+1, . . . ,n
</p>
<p>⎫
⎬
⎭k = 1, . . . ,n
</p>
<p>n matrix A. For this we will solve the n matrix equations (A.6.4) with the
previously described Cholesky decomposition of A,
</p>
<p>Axi = UTUxi = UTyi = ei , i = 1, . . . ,n . (A.9.8)
</p>
<p>We denote the ℓth component of the three vectors xi , yi , and ei by xiℓ, yiℓ,
and eiℓ. Clearly one has xiℓ = (A&minus;1)iℓ and eiℓ = δiℓ, since eiℓ is the element
(i,ℓ) of the unit matrix.
</p>
<p>the algorithm for the Cholesky decomposition of a real positive-definite sym-
</p>
<p>metric matrix A. Since for such matrices all of the diagonal elements are
</p>
<p>different from zero, one does not need in principle any pivoting. One can also
</p>
<p>show that it would bring no advantage in numerical accuracy. The method
</p>
<p>composition of a symmetric positive definite square matrix.
</p>
<p>Positive-definite symmetric matrices play an important role as weight and
</p>
<p>covariance matrices. In such cases one often requires the Cholesky decom-
</p>
<p>position A = UTU as well as multiplication of U by a matrix. It is eas-
ier computationally if this multiplication is done by the method DatanMa-
</p>
<p>U into
</p>
<p>account.
</p>
<p>Of particular interest is the inversion of a symmetric positive-definite n&times;
</p>
<p>DatanMatrix.choleskyDecomposition performs the Cholesky de-
</p>
<p>trix.choleskyMultiply, which takes the triangular form of</p>
<p/>
</div>
<div class="page"><p/>
<p>374 A Matrix Calculations
</p>
<p>We now determine yiℓ by means of forward substitution. From (A.9.8) it
follows directly that
</p>
<p>ℓ&sum;
</p>
<p>k=1
Ukℓyik = eiℓ = δiℓ ,
</p>
<p>i.e.,
</p>
<p>U11yi1 = δi1 ,
U12yi1 +U22yi2 = δi2 ,
</p>
<p>...
</p>
<p>and thus
</p>
<p>yi1 = δi1/U11 ,
...
</p>
<p>yiℓ =
1
</p>
<p>Uℓℓ
</p>
<p>(
δiℓ&minus;
</p>
<p>ℓ&minus;1&sum;
</p>
<p>k=1
Ukℓyik
</p>
<p>)
.
</p>
<p>Since, however, δiℓ = 0 for i �= ℓ, this expression simplifies to
</p>
<p>yiℓ = 0 , ℓ &lt; i ,
</p>
<p>yiℓ =
1
</p>
<p>Uℓℓ
</p>
<p>(
δiℓ&minus;
</p>
<p>ℓ&minus;1&sum;
</p>
<p>k=i
Ukℓyik
</p>
<p>)
, ℓ&ge; i .
</p>
<p>We can now obtain xiℓ by means of backward substitution of yiℓ into
</p>
<p>Uxi = yi
</p>
<p>or, written in components,
</p>
<p>n&sum;
</p>
<p>k=ℓ
Uℓkxik = yiℓ
</p>
<p>or
</p>
<p>U11xi1 +U12xi2 +&middot;&middot; &middot;+U1nxin = yi1 ,
U22xi2 +&middot;&middot; &middot;+U2nxin = yi2 ,
</p>
<p>...
</p>
<p>Unnxin = yin .
</p>
<p>We obtain</p>
<p/>
</div>
<div class="page"><p/>
<p>A.10 Pseudo-inverse Matrix 375
</p>
<p>xin = yin/Unn ,
...
</p>
<p>xiℓ =
1
</p>
<p>Uℓℓ
</p>
<p>(
yiℓ&minus;
</p>
<p>n&sum;
</p>
<p>k=ℓ+1
Uℓkxik
</p>
<p>)
.
</p>
<p>If we compute xiℓ only for ℓ &ge; i, then by backward substitution one only en-
counters the elements yiℓ for ℓ&ge; i, i.e., only non-vanishing yiℓ. The vanishing
elements yiℓ thus do not need to be stored. The elements xiℓ for ℓ &lt; i follow
simply from the symmetry of the original matrix, xiℓ = xℓi .
</p>
<p>the Cholesky decomposition of A. Then a loop is carried out over the var-
ious right-hand sides ei , i = 1, . . . ,n, of (A.9.8). As the result of running
through the loop once one obtains the elements xin, xi,n&minus;1, . . ., xii of row i.
They are stored as the corresponding elements of the output matrix A. The
elements of the vector y, which is only used for intermediate results, can be
stored in the last row of A. Finally the elements below the main diagonal are
filled by copies of their mirror images.
</p>
<p>A.10 Pseudo-inverse Matrix
</p>
<p>We now return to the problem of Sect. A.5, that of solving the Eq. (A.5.4)
</p>
<p>Ax&asymp; b (A.10.1)
</p>
<p>for an arbitrary m&times;n matrix A of rank k. According to (A.5.14), the unique
solution of minimum norm is
</p>
<p>x̃=K
(
p1
</p>
<p>0
</p>
<p>)
.
</p>
<p>The vector p1 is the solution of Eq. (A.5.11) and therefore
</p>
<p>p̃1 = R&minus;111 g1 ,
</p>
<p>since R11 is non-singular. Because of (A.5.8) one has finally
</p>
<p>x̃=K
(
</p>
<p>R&minus;111 0
0 0
</p>
<p>)
HTb . (A.10.2)
</p>
<p>In analogy to (A.6.3) we write
</p>
<p>x̃= A+b (A.10.3)
</p>
<p>and call the n&times;m matrix
</p>
<p>The method DatanMatrix.choleskyInversion first performs</p>
<p/>
</div>
<div class="page"><p/>
<p>376 A Matrix Calculations
</p>
<p>A+ =K
(
</p>
<p>R&minus;111 0
0 0
</p>
<p>)
HT (A.10.4)
</p>
<p>the pseudo-inverse of the m&times;n matrix A.
The matrix A+ is uniquely determined by A and does not depend on the
</p>
<p>particular orthogonal decomposition (A.5.5). This can easily be seen if one
denotes the j th column vector of A+ by a+j = A+ej , with ej the j th column
vector of the m-dimensional unit matrix. According to (A.10.3) the vector a+j
is the minimum-length solution of the equation Aa+j = ej , and is therefore
unique.
</p>
<p>A.11 Eigenvalues and Eigenvectors
</p>
<p>We now consider the eigenvalue equation
</p>
<p>Gx= λx (A.11.1)
</p>
<p>for the n&times; n matrix G. If this is fulfilled, then the scalar λ is called the
eigenvalue and the n-vector x the eigenvector of G. Clearly the eigenvector x
is only determined up to an arbitrary factor. One can choose this factor such
that |x| = 1.
</p>
<p>We consider first the particularly simple case where G is a diagonal ma-
trix with non-negative diagonal elements,
</p>
<p>G= STS = S2 =
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>s21
s22
</p>
<p>. . .
</p>
<p>s2n
</p>
<p>⎞
⎟⎟⎟⎠ , (A.11.2)
</p>
<p>which can be expressed as the product of an arbitrary diagonal matrix S with
itself,
</p>
<p>S =
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>s1
s2
</p>
<p>. . .
</p>
<p>sn
</p>
<p>⎞
⎟⎟⎟⎠ . (A.11.3)
</p>
<p>The eigenvalue equation S2x = λx then has the eigenvalues s2i = λi and the
normalized eigenvectors are the basis vectors xi = ei .
</p>
<p>In place of S we now set
</p>
<p>A= U SV T (A.11.4)
</p>
<p>with orthogonal matrices U and V and</p>
<p/>
</div>
<div class="page"><p/>
<p>A.11 Eigenvalues and Eigenvectors 377
</p>
<p>G= ATA= V ST SV T . (A.11.5)
</p>
<p>We can write the eigenvalue equation of G in the form
</p>
<p>Gx= λx (A.11.6)
</p>
<p>or
V STSV T x= λx .
</p>
<p>Multiplying on the left with V T,
</p>
<p>STSV Tx= λV Tx ,
</p>
<p>and comparing with (A.11.1) and (A.11.2) shows that G has the same eigen-
values λi = s2i as S2, but has the orthogonally transformed eigenvectors
</p>
<p>xi = e&prime;i = V ei . (A.11.7)
</p>
<p>One can clearly find the eigenvalues and eigenvectors of G if one knows the
orthogonal matrix V that transforms G into a diagonal matrix,
</p>
<p>V TGV = STS = S2 . (A.11.8)
</p>
<p>The transformation (A.11.8) is called a principal axis transformation.
The name becomes clear by considering the equation
</p>
<p>rTGr= 1 . (A.11.9)
</p>
<p>We are interested in the geometrical position of all points r that fulfill
(A.11.9).
</p>
<p>For the vector r the following two representations are completely equiv-
alent:
</p>
<p>r=
&sum;
</p>
<p>i
</p>
<p>riei (A.11.10)
</p>
<p>and
r=
</p>
<p>&sum;
</p>
<p>i
</p>
<p>r &prime;ie
&prime;
i , (A.11.11)
</p>
<p>with the components ri and r &prime;i taken with respect to the basis vectors ei and
e&prime;i , respectively. With the representation (A.11.11) and by using (A.11.5) and
(A.11.7) we obtain
</p>
<p>1 = rTGr= rTVS2V Tr
=
</p>
<p>&sum;
</p>
<p>i
</p>
<p>(r &prime;ie
&prime;
i)
</p>
<p>TV S2V T
&sum;
</p>
<p>j
</p>
<p>(r &prime;je
&prime;
j )
</p>
<p>=
&sum;
</p>
<p>i
</p>
<p>(r &prime;ie
T
i V
</p>
<p>T)V S2V T
&sum;
</p>
<p>j
</p>
<p>(r &prime;jV ej )
</p>
<p>=
&sum;
</p>
<p>i
</p>
<p>(r &prime;ie
T
i )S
</p>
<p>2
&sum;
</p>
<p>j
</p>
<p>(r&prime;jej )</p>
<p/>
</div>
<div class="page"><p/>
<p>378 A Matrix Calculations
</p>
<p>and finally
n&sum;
</p>
<p>i=1
r &prime;2i s
</p>
<p>2
i =
</p>
<p>n&sum;
</p>
<p>i=1
r &prime;2i /a
</p>
<p>2
i = 1 . (A.11.12)
</p>
<p>This is clearly the equation of an ellipsoid in n dimensions with half-diameters
in the directions e&prime;i and having the lengths
</p>
<p>ai =
1
</p>
<p>si
. (A.11.13)
</p>
<p>The vectors
ai = e&prime;i/si = V ei/si (A.11.14)
</p>
<p>are the principal axes of the ellipsoid. They have the directions of the eigen-
</p>
<p>vectors of G. Their lengths ai = 1/
&radic;
s2i are determined by the eigenvalues s
</p>
<p>2
i .
</p>
<p>The matrix
</p>
<p>C =G&minus;1 = (ATA)&minus;1 = V (S2)&minus;1V T
</p>
<p>clearly has the same eigenvectors as G, but has the eigenvalues 1/s2i . The
lengths of the half-diameters of the ellipsoid described above are then directly
equal to the square roots of the eigenvalues of C.
</p>
<p>The matrix C is called the unweighted covariance matrix of A. The el-
lipsoid is called the unweighted covariance ellipsoid.
</p>
<p>Please note that all considerations were done for matrices of the type
(A.11.5) which, by construction, are symmetric and non-negative definite, i.e.,
they have real, non-negative eigenvalues. Ellipses with finite semiaxes are
obtained only with positive-definite matrices. If the same eigenvalue occurs
several times then the ellipsoid has several principal axes of the same length.
In this case there is a certain ambiguity in the determination of the principal
axes which, however, can always be chosen orthogonal to each other.
</p>
<p>Up to now we have not given any prescription for finding the eigenvalues.
The eigenvalue equation is Gx= λx or
</p>
<p>(G&minus;λI)x= 0 . (A.11.15)
</p>
<p>Written in this way it can be considered as a linear system of equations for
determining x, for which the right-hand side is the null vector. Corresponding
to (A.6.5) and (A.6.9) there is a non-trivial solution only for
</p>
<p>det(G&minus;λI)= 0 . (A.11.16)
</p>
<p>This is the characteristic equation for determining the eigenvalues of A. For
n= 2 this is</p>
<p/>
</div>
<div class="page"><p/>
<p>A.12 Singular Value Decomposition 379
</p>
<p>∣∣∣∣
g11 &minus;λ g12
g21 g22 &minus;λ
</p>
<p>∣∣∣∣= (g11 &minus;λ)(g22 &minus;λ)&minus;g12g21 = 0
</p>
<p>and has the solutions
</p>
<p>λ1,2 =
g11 +g22
</p>
<p>2
&plusmn;
</p>
<p>&radic;
</p>
<p>g12g21 +
(g11 &minus;g22)2
</p>
<p>4
.
</p>
<p>If G is symmetric, as previously assumed, i.e., g12 = g21, then the eigenvalues
are real. If G is positive definite, they are positive.
</p>
<p>The characteristic equation of an n&times;n matrix has n solutions. In practice,
however, for n &gt; 2 one does not find the eigenvalues by using the characteris-
tic equation, but rather by means of an iterative procedure such as the singular
value decomposition (see Sect. A.12).
</p>
<p>A.12 Singular Value Decomposition
</p>
<p>We now consider a particular orthogonal decomposition of the m&times; n ma-
trix A,
</p>
<p>A= U SV T . (A.12.1)
Here and in Sects. A.13 and A.14 we assume that m&ge; n. If this is not the case,
then one can simply extend the matrix A with further rows whose elements
are all zero until it has n rows, so that m= n. The decomposition (A.12.1) is
a special case of the decomposition (A.5.5). The m&times;n matrix S, which takes
the place of R, has the special form
</p>
<p>S =
(
</p>
<p>D 0
0 0
</p>
<p>)
, (A.12.2)
</p>
<p>and D is a k&times; k diagonal matrix with k = Rang(A). The diagonal elements
of S are called the singular values of A. If A is of full rank, then k = n and
all si �= 0. For reduced rank k &lt; n one has si = 0 for i &gt; k. We will see below
that U and V can be determined such that all si are positive and ordered to be
non-increasing,
</p>
<p>s1 &ge; s2 &ge; . . . &ge; sk . (A.12.3)
The singular values of A have a very simple meaning. From Sect. A.11
</p>
<p>one has directly that the si are the square roots of the eigenvalues of G=ATA.
Thus the half-diameters of the covariance ellipsoid of G are ai = 1/si . If G
is singular, then A has the reduced rank k &lt; n, and the n&minus; k singular values
sk+1, . . ., sn vanish. In this case the determinant
</p>
<p>detG= detU detS2 detV = detS2 = s21s22 &middot; &middot; &middot;s2n (A.12.4)
</p>
<p>also vanishes.</p>
<p/>
</div>
<div class="page"><p/>
<p>380 A Matrix Calculations
</p>
<p>With the substitutions H &rarr; U , K &rarr; V , R &rarr; S, R11 &rarr; D we obtain
from Sect. A.5
</p>
<p>Ax= U SV Tx&asymp; b , (A.12.5)
SV Tx&asymp; UTb . (A.12.6)
</p>
<p>With
</p>
<p>V Tx= p=
(
</p>
<p>p1
p2
</p>
<p>)
} k
} n&minus; k , U
</p>
<p>Tb= g=
(
</p>
<p>g1
g2
</p>
<p>)
} k
} m&minus; k (A.12.7)
</p>
<p>one has
Sp= g , (A.12.8)
</p>
<p>i.e.,
Dp1 = g1 (A.12.9)
</p>
<p>and
0 &middot;p2 = g2 . (A.12.10)
</p>
<p>These have the solutions
p̃1 =D&minus;1g1 , (A.12.11)
</p>
<p>i.e.,
pℓ = gℓ/sℓ , ℓ= 1,2, . . . ,k ,
</p>
<p>for arbitrary p2. The solution with minimum absolute value is
</p>
<p>x̃= V
(
</p>
<p>p̃1
0
</p>
<p>)
. (A.12.12)
</p>
<p>The residual vector has the form
</p>
<p>r= b&minus; Ãx= U
(
</p>
<p>0
g2
</p>
<p>)
(A.12.13)
</p>
<p>and the absolute value
</p>
<p>r = |g2| =
(
</p>
<p>m&sum;
</p>
<p>i=k+1
g2i
</p>
<p>)1/2
. (A.12.14)
</p>
<p>A.13 Singular Value Analysis
</p>
<p>The rank k of the matrix A plays a decisive role in finding the solution of
Ax&asymp; b. Here k characterizes the transition from non-zero to vanishing sin-
gular values, sk &gt; 0, sk+1 = 0. How should one judge the case of very small
values of sk, i.e., sk &lt; ε for a given small ε?</p>
<p/>
</div>
<div class="page"><p/>
<p>A.13 Singular Value Analysis 381
</p>
<p>Example A.2: Almost vanishing singular values
</p>
<p>As a simple example we consider the case m= n= 2, U = V = I . One then
has
</p>
<p>Ax= U SV Tx= Sx=
(
</p>
<p>s1 0
0 s2
</p>
<p>)(
x1
x2
</p>
<p>)
=
</p>
<p>(
b1
b2
</p>
<p>)
= b , (A.13.1)
</p>
<p>that is,
x1 = b1/s1 , x2 = b2/s2 . (A.13.2)
</p>
<p>If we now take s2 &rarr; 0 in (A.13.2), then |x2|&rarr;&infin;. At first glance one obtains
a completely different picture if one sets s2 = 0 in (A.13.1) directly. This
gives
</p>
<p>s1x1 = b1 , 0 &middot;x2 = b2 . (A.13.3)
Thus x1 = b1/s1 as in (A.13.2), but x2 is completely undetermined. The solu-
tion x̃ of minimum absolute value is obtained by setting x2 = 0. The question
is now, &ldquo;What is right? x2 =&infin; or x2 = 0?&rdquo; The answer is that one should set
</p>
<p>x2 =
{
</p>
<p>b2/s2 , s2 &ge; ε
0 , s2 &lt; ε
</p>
<p>and choose the parameter ε such that the expression b2/ε is still numerically
well-defined. This means that one must have ε/|b2| ≫ 2&minus;m if m binary digits
are available for the representation of a floating point number (cf. Sect. 4.2).
Thus a finite value of b2 is computed as long as the numerical determination
of this value is reasonable. If this is not the case then one approaches the
situation s2 = 0, where b2 is completely undetermined, and one sets b2 = 0.
</p>
<p>Example A.3: Point of intersection of two almost parallel lines
</p>
<p>We consider the two lines shown in Fig. A.6, which are described by
</p>
<p>&minus;αx1 +x2 = 1&minus;α ,
αx1 +x2 = 1+α .
</p>
<p>For the vector x of the intersection point one has Ax= b with
</p>
<p>A=
(
</p>
<p>&minus;α 1
α 1
</p>
<p>)
, b=
</p>
<p>(
1&minus;α
1+α
</p>
<p>)
.
</p>
<p>One can easily check that A= U SV T holds with
</p>
<p>U = 1&radic;
2
</p>
<p>(
&minus;1 &minus;1
&minus;1 1
</p>
<p>)
, S =
</p>
<p>&radic;
2
</p>
<p>(
1 0
0 α
</p>
<p>)
, V =
</p>
<p>(
0 1
</p>
<p>&minus;1 0
</p>
<p>)
,
</p>
<p>i.e., s1 =
&radic;
</p>
<p>2, s2 = α
&radic;
</p>
<p>2. Using</p>
<p/>
</div>
<div class="page"><p/>
<p>382 A Matrix Calculations
</p>
<p>2(x 
 -1
</p>
<p>) =
    
</p>
<p>(x 
 -1
</p>
<p>)
</p>
<p>α 1
</p>
<p>(x  -1) = -    (x  -1)
</p>
<p>2
</p>
<p>α
1
</p>
<p>2
</p>
<p>1x
</p>
<p>x
</p>
<p>1
</p>
<p>γ
</p>
<p>γ
1
</p>
<p>Fig.A.6: Two lines intersect at the point (1,1) with an angle 2γ = 2arctanα.
</p>
<p>g= UTb=
&radic;
</p>
<p>2
</p>
<p>(
&minus;1
α
</p>
<p>)
= Sp=
</p>
<p>&radic;
2
</p>
<p>(
p1
</p>
<p>αp2
</p>
<p>)
, p=
</p>
<p>(
&minus;1
</p>
<p>1
</p>
<p>)
</p>
<p>one obtains
</p>
<p>x= V p=
(
</p>
<p>1
</p>
<p>1
</p>
<p>)
</p>
<p>independent of α.
If, however, one has s2 = α
</p>
<p>&radic;
2 &lt; ε and then sets s2 = 0, then one obtains
</p>
<p>p=
(&minus;1
</p>
<p>0
</p>
<p>)
, x=
</p>
<p>(
0
</p>
<p>1
</p>
<p>)
.
</p>
<p>From Fig. A.6 one can see that for α&rarr; 0 the two lines come together to a
single line described by x2 = 1. The x1-coordinate of the &ldquo;intersection point&rdquo;
is completely undetermined. It is set equal to zero, since the solution vector x
has minimum length [cf. (A.5.14)].
</p>
<p>As in the case of &ldquo;indirect measurements&rdquo; in Chap. 9 we now assume
that the vector b is equal to the vector of measurements y, which characterize
the two lines, and that their measurement errors are given by the covariance
matrix Cy =G&minus;1y . In the simplest case of equal uncorrelated errors one has
</p>
<p>Cy =G&minus;1y = σ 2I .
</p>
<p>The covariance matrix for the unknowns x̃ is according to (9.2.27)
</p>
<p>Cx = (ATGyA)&minus;1 ,
</p>
<p>and thus in the case of uncorrelated measurement errors,</p>
<p/>
</div>
<div class="page"><p/>
<p>A.13 Singular Value Analysis 383
</p>
<p>Cx = σ 2(ATA)&minus;1 = σ 2C .
</p>
<p>Up to the factor σ 2 it is thus equal to the unweighted covariance matrix C =
(ATA)&minus;1 for the matrix A. For our matrix A one has
</p>
<p>C = (ATA)&minus;1 =
(
</p>
<p>1/2α2 0
0 1/2
</p>
<p>)
.
</p>
<p>The corresponding ellipse has the half-diameters e1/α
&radic;
</p>
<p>2 and e2/
&radic;
</p>
<p>2. The
covariance ellipse of x̃ then has for the case of equal uncorrelated measure-
ments equal half-diameters, multiplied, however, by the factor σ . They have
the lengths σx1 = σ/α
</p>
<p>&radic;
2, σx2 = σ/
</p>
<p>&radic;
2. Clearly one then sets x1 = 0 if the
</p>
<p>inequality x1 ≪ σx1 would hold for a finite fixed x1, i.e., α
&radic;
</p>
<p>2 ≪ σ .
</p>
<p>The decision as to whether a small singular value should be set equal to
zero thus depends on numerical considerations and on a consideration of the
measurement errors. The following fairly general procedure of singular value
analysis has proven to be useful in practice.
</p>
<p>1. With a computer program one carries out the singular value decompo-
sition, which yields, among other things, the ordered singular values
</p>
<p>s1 &ge; s2 &ge; &middot;&middot; &middot; &ge; sk .
</p>
<p>2. Depending on the problem at hand one chooses a positive factor f ≪ 1.
3. All singular values for which si &lt; f s1 are set equal to zero. In place of
</p>
<p>k one has thus ℓ&le; k such that si = 0 for i &gt; ℓ.
4. With the replacements described above (k &rarr; ℓ, sℓ+1 = &middot;&middot; &middot; = sk = 0)
</p>
<p>the formulas of Sect. A.12 retain their validity.
</p>
<p>In place of the residual (A.12.14) one obtains a somewhat larger value,
since in the sum in the expression
</p>
<p>r =
(
</p>
<p>m&sum;
</p>
<p>i=ℓ+1
g2i
</p>
<p>)1/2
(A.13.4)
</p>
<p>one has more terms than in (A.12.14).
The procedure implies that in some cases one has an effective reduction
</p>
<p>of the rank k of the matrix A to a value ℓ &lt; k. If A has its full rank, then this
can be reduced. This has the great advantage that numerical difficulties with
small singular values are avoided. In contrast to, for example, the Gaussian
or Cholesky procedures, the user does not have to worry about whether G =
ATA is singular or nearly singular.</p>
<p/>
</div>
<div class="page"><p/>
<p>384 A Matrix Calculations
</p>
<p>Although the singular value analysis always gives a solution of minimum
absolute value x̃ for the problem Ax&asymp; b, caution is recommended for the case
ℓ &lt; n (regardless of whether k &lt; n or ℓ &lt; k = n). In such a case one has an
(almost) linear dependence of the unknowns x1, . . ., xn. The solution x̃ is not
the only solution of the problem, but rather is simply the solution with the
smallest absolute value out of the many possible solutions.
</p>
<p>We have already remarked in Sect. A.5 that the singular value decom-
position is advantageous also with respect to numerical precision compared
to other methods, especially to the method of normal equations. A detailed
discussion (see, e.g., [18]) is beyond the scope of this book. Here we limit
ourselves to giving an example.
</p>
<p>Example A.4: Numerical superiority of the singular value decomposition
compared to the solution of normal equations
</p>
<p>Consider the problem Ax&asymp; b for
</p>
<p>A=
</p>
<p>⎛
⎝
</p>
<p>1 1
β 0
0 β
</p>
<p>⎞
⎠ .
</p>
<p>The singular values of A are the square roots of the eigenvalues of
</p>
<p>G= ATA=
(
</p>
<p>1+β2 1
1 1+β2
</p>
<p>)
</p>
<p>and are determined by (A.11.16) to be
</p>
<p>s1 =
&radic;
</p>
<p>2+β2 , s2 = |β| .
</p>
<p>This was done with singular value decomposition without using the matrix G.
If the computing precision is ε and if β2 &lt; ε but β &gt; ε, then one obtains
</p>
<p>s1 =
&radic;
</p>
<p>2 , s2 = |β| ,
</p>
<p>i.e., both singular values remain non-zero. If instead of the singular value
decomposition one uses the normal equations
</p>
<p>ATAx=Gx= ATb ,
</p>
<p>then the matrix G appears explicitly. With β2 &lt; ε it is numerically repre-
sented as
</p>
<p>G=
(
</p>
<p>1 1
1 1
</p>
<p>)
.
</p>
<p>This matrix is singular, detG = 0, and cannot be inverted, as foreseen in
(A.5.17). This is also reflected in its singular values
</p>
<p>s1 =
&radic;
</p>
<p>2 , s2 = 0 .</p>
<p/>
</div>
<div class="page"><p/>
<p>A.14 Algorithm for Singular Value Decomposition 385
</p>
<p>A.14 Algorithm for Singular Value Decomposition
</p>
<p>A.14.1 Strategy
</p>
<p>We will now give an algorithm for the singular value decomposition and fol-
low for the most part the presentation of LAWSON and HANSON [18], which
is based on the work of GOLUB and KAHN [19], BUSINGER and GOLUB [20],
and GOLUB and REINSCH [21]. The strategy of the algorithm is based on the
successive application of orthogonal transformations.
</p>
<p>In the first step, the matrix A is transformed into a bidiagonal matrix C,
</p>
<p>A=QCHT . (A.14.1)
</p>
<p>The orthogonal matrices Q and H are themselves products of Househol-
der"=transformation matrices.
</p>
<p>In step 2 the matrix C is brought into diagonal form by means of an
iterative procedure:
</p>
<p>C = U &prime;S&prime;V &prime;T . (A.14.2)
Here the matrices U &prime; and V &prime; are given by products of Givens-transformation
matrices and if necessary reflection matrices. The latter ensure that all diago-
nal elements are non-negative.
</p>
<p>In step 3, further orthogonal matrices U &prime;&prime; and V &prime;&prime; are determined. They
are products of permutation matrices and ensure that the diagonal elements of
</p>
<p>S = U &prime;&prime;TS&prime;V &prime;&prime; (A.14.3)
</p>
<p>are ordered so as to be non-increasing as in (A.12.3). As the result of the first
three steps one obtains the matrices
</p>
<p>U =QU &prime;U &prime;&prime; , V =H V &prime;V &prime;&prime; , (A.14.4)
</p>
<p>which produce the singular value decomposition (A.12.1) as well as the diag-
onal elements s1, s2, . . ., sk.
</p>
<p>In step 4 the singular value analysis is finally carried out. Stated simply,
all singular values are set to zero for which
</p>
<p>si &lt; smin . (A.14.5)
</p>
<p>Thus one can finally give the solution vector x to the equation
</p>
<p>Ax&asymp; b . (A.14.6)
</p>
<p>In practice, Eq. (A.14.6) is often solved simultaneously for various right-
hand sides, which can then be arranged in an m&times;ℓ matrix,</p>
<p/>
</div>
<div class="page"><p/>
<p>386 A Matrix Calculations
</p>
<p>B = (b1,b2, . . . ,bℓ) .
</p>
<p>Instead of the solution vector x̃ we thus obtain the n&times;ℓ solution matrix
</p>
<p>X̃ = (̃x1, x̃2, . . . , x̃ℓ)
</p>
<p>of the equation
AX &asymp; B . (A.14.7)
</p>
<p>methods referred to in Sect. A.14; of course their source code can be studied.)
The goal is to find the m&times;n matrix C in (A.14.1). Here C is of the form
</p>
<p>C =
(
C &prime;
</p>
<p>0
</p>
<p>)
, (A.14.8)
</p>
<p>and C &prime; is an n&times;n bidiagonal matrix
</p>
<p>C &prime; =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎝
</p>
<p>d1 e2
d2 e3
</p>
<p>. .
</p>
<p>. .
</p>
<p>. . en
dn
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎠
</p>
<p>. (A.14.9)
</p>
<p>The goal is achieved by multiplication of the matrix A with appropriate
Householder matrices (alternating on the right and left):
</p>
<p>C =Qn(&middot; &middot; &middot;((Q1A)H2) &middot; &middot; &middot;Hn)=QTAH . (A.14.10)
</p>
<p>The matrix Q1 is computed from the elements of the first column of A
and is applied to all columns of A (and B). It results in only the element
</p>
<p>The method DatanMatrix s.ingularValuedecomposition
</p>
<p>computes this solution. It merely consists of four calls to other methods that
</p>
<p>carry out the four steps described above. These are explained in more detail
</p>
<p>in the following sections.
</p>
<p>Usually one is only interested in the solution matrix X̃ of the problem
</p>
<p>AX&asymp; B and possibly in the number of singular values not set to zero. Some-
times, however, one would like explicit access to the matrices U and V and
</p>
<p>to the singular values. This is made possible by the method DatanMa-
</p>
<p>A.14.2 Bidiagonalization
</p>
<p>The method
</p>
<p>(It is declared as
</p>
<p>trix.pseudoInverse.
</p>
<p>DatanMatrix.sv1 performs the procedure described below.
</p>
<p>privatewithin the class DatanMatrix as are the further</p>
<p/>
</div>
<div class="page"><p/>
<p>A.14 Algorithm for Singular Value Decomposition 387
</p>
<p>(1,1) of A remaining non-zero. The matrix H2 is computed from the first row
of the matrix Q1A. It results in the element (1,1) remaining unchanged, the
element (1,2) being recomputed and the elements (1,3), . . ., (1,n) being set
equal to zero. It is applied to all rows of the matrix (Q1A). One thus obtains
</p>
<p>Q1A=
</p>
<p>⎛
⎜⎜⎜⎜⎝
</p>
<p>&bull; . . .
0 . . .
0 . . .
0 . . .
0 . . .
</p>
<p>⎞
⎟⎟⎟⎟⎠
</p>
<p>, Q1AH2 =
</p>
<p>⎛
⎜⎜⎜⎜⎝
</p>
<p>&bull; &bull; 0 0
0 . . .
0 . . .
0 . . .
0 . . .
</p>
<p>⎞
⎟⎟⎟⎟⎠
</p>
<p>.
</p>
<p>Here &ldquo;&bull;&rdquo; denotes an element of the final bidiagonal matrix C &prime;, and &ldquo;.&rdquo; an
element that will be changed further. Now Q2 is determined from the second
column of Q1AH2 such that upon application to this column the element 1
remains unchanged and the element 2 and all others are changed such that
only element 2 remains non-zero, and the elements 3 through m become zero.
</p>
<p>The procedure can be summarized in the following way. The matrix Qi
is applied to the column vectors, it leaves the elements 1 through i&minus; 1 un-
changed and changes the elements i through m. It produces an orthogonal
transformation in the subspace of the components i through m. At the time of
applying Qi , however, these elements in the columns 1 through i&minus; 1 are al-
ready zero, so that Qi must only be applied explicitly to columns i through n.
The corresponding considerations hold for the matrix Hi . It acts on the row
vectors, leaves elements 1 through i&minus;1 unchanged, and produces an orthog-
onal transformation in the subspace of the components i through n. When
it is applied, these components in the rows 1 through i&minus; 2 are all zero. The
matrix Hi must only be applied explicitly to the rows i&minus;1, . . ., m&minus;1. Since
only n&minus;1 rows of A must be processed, the matrix Hn in (A.14.10) is the unit
matrix; Qn is the unit matrix only if m= n.
</p>
<p>In addition to the transformed matrix QTAH , the matrix H = H2H3 &middot; &middot; &middot;
Hn&minus;1 is also stored. For this the information about each matrix Hi is saved.
As we determined at the end of Sect. A.3, it is sufficient to store the quantities
defined there, up and b, and the elements i+1 through n of the (i&minus;1)th row
vectors defining the matrix Hi . This can be done, however, in the array ele-
ments of these row vectors themselves, since it is these elements that will be
transformed to zero and which therefore do not enter further into the calcula-
tion. One needs to declare additional variables only for the quantities up and b
of each of the matrices Hi . If all of the transformations have been carried out,
then the diagonal elements di and the next-to-diagonal elements ei are trans-
ferred to the arrays e and d. Finally, the product matrix H =H2H3 &middot; &middot; &middot;Hn&minus;1I
is constructed in the first n rows of the array of the original matrix A, in the
order Hn&minus;1I , Hn&minus;2(Hn&minus;1I), . . .. Here as well, the procedure is as economical
as possible, i.e., the Householder matrix is only applied to those columns of</p>
<p/>
</div>
<div class="page"><p/>
<p>388 A Matrix Calculations
</p>
<p>the matrix to the right for which it would actually produce a change. The unit
</p>
<p>matrix I is constructed to the extent that it is needed, row-by-row in the array
</p>
<p>of the original matrix A starting from below. For this there is exactly the right
</p>
<p>amount of space available, which up to this point was necessary for storing
</p>
<p>information about the Householder transformations that were just applied.
</p>
<p>A.14.3 Diagonalization
</p>
<p>This step is implemented in
</p>
<p>C, whose non-vanishing elements are stored in the arrays d and e, is now
brought into diagonal form by appropriately chosen Givens transformations.
</p>
<p>The strategy is chosen such that the lowest non-diagonal element vanishes
</p>
<p>first and the non-diagonal elements always move up and to the left, until C
</p>
<p>is finally diagonal. All of the transformations applied to C from the left are
</p>
<p>also applied to the matrix stored in the array b, and all transformations that
act from the right are also applied to the matrix in a. (We denote the matrix
to be diagonalized during each step by C.)
</p>
<p>Only the upper-left submatrix Ck with k rows and columns is not yet
</p>
<p>diagonal and must still be considered. The index k is determined such that
</p>
<p>ek �= 0 and ej = 0, j &gt; k. This means that the program runs through the loop
k = n, n&minus; 1, . . ., 2. Before the lower non-diagonal element ek is systemati-
cally made zero by means of an iterative procedure, one checks for two spe-
</p>
<p>cial cases, which allow a shortening of the computation by means of special
</p>
<p>treatment.
</p>
<p>Special case 1, dk = 0 (handled in DatanMatrix.s21): A Givens ma-
trix W is applied from the right, which also causes ek to vanish. The matrix
</p>
<p>w is the product Wk&minus;1Wk&minus;2 &middot; &middot; &middot;W1. Here Wi acts on the columns i and k of
Ck, but of course only on those rows where at least one of these columns has
</p>
<p>a non-vanishing element. Wk&minus;1 acts on the row k&minus; 1, annihilates the ele-
ment ek = Ck&minus;1,k, and changes Ck&minus;1,k&minus;1. In addition, Wk&minus;1 acts on the row
k&minus; 2, changes the element Ck&minus;2,k&minus;1, and produces a non-vanishing element
H = Ck&minus;2,k in column k. Now the matrix Wk&minus;2 is applied, which annihilates
exactly this element, but produces a new element in row k&minus;3 and column k.
When the additional element finally makes it to row 1, it can then be annihi-
</p>
<p>lated by the transformation W1. As a result of this treatment of special case
</p>
<p>1, Ck decomposes into a (k&minus; 1)&times; (k&minus; 1) submatrix Ck&minus;1 and a 1&times; 1 null
matrix.
</p>
<p>Special case 2, Ck decomposes into submatrices (handled in DatanMa-
</p>
<p>): If e ℓ= 0 for any value ℓ, 2 &le; ℓ&le; k, then the matrix
</p>
<p>DatanMatrix.sv2. The bidiagonal matrix
</p>
<p>trix.s22</p>
<p/>
</div>
<div class="page"><p/>
<p>A.14 Algorithm for Singular Value Decomposition 389
</p>
<p>Ck =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>d1 e2
. . .
</p>
<p>dℓ&minus;1 0
dℓ eℓ+1
</p>
<p>. . .
</p>
<p>dk&minus;1 ek
dk
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>=
(
</p>
<p>Cℓ&minus;1 0
0 C̄
</p>
<p>)
</p>
<p>can be decomposed into two matrices Cℓ&minus;1 and C̄. One can first diagonalize
C̄ and then Cℓ&minus;1. In particular, if ℓ= k, then one obtains
</p>
<p>Ck =
(
</p>
<p>Ck&minus;1 0
0 dk
</p>
<p>)
.
</p>
<p>Here dk is the singular value, and the loop index can only be decreased by
</p>
<p>one, k &rarr; k&minus;1. First, however, the case dk &lt; 0 must be treated; see &ldquo;change
of sign&rdquo; below. If dℓ&minus;1 = 0, but eℓ �= 0, then C can still be decomposed. For
this one uses the transformation matrix T = TkTk&minus;1, &middot; &middot; &middot; ,Tℓ+1 acting on the
left, where Ti acts on the rows ℓ and i. In particular, Tℓ+1 annihilates the
element eℓ = Cℓ,ℓ+1 and creates an element H = Cℓ,ℓ+2. Tℓ+2 annihilates
this element and creates in its place H = Cℓ,ℓ+3. Finally Tk annihilates the
last created element H = Cℓk by a transformation to Ckk = dk.
</p>
<p>After it has been checked whether one or both of the special cases were
</p>
<p>present and such an occurrence has been treated accordingly, it remains only
</p>
<p>to diagonalize the matrix C̄. This consists of the rows and columns ℓ through
</p>
<p>k of C. If special case 2 was not present, then one has ℓ = 1. The problem is
solved iteratively with the QR-algorithm.
</p>
<p>QR-algorithm (carried out in DatanMatrix.s23): First we denote the
</p>
<p>square output matrix C̄ by C1. We then determine orthogonal matrices Ui , Vi
and carry out transformations
</p>
<p>Ci+1 = UTi CiVi , i = 1,2, . . . ,
</p>
<p>which lead to a diagonal matrix S,
</p>
<p>lim
i&rarr;&infin;
</p>
<p>Ci = S .
</p>
<p>The following prescription is used for determining Ui and Vi:
</p>
<p>(A) One determines the eigenvalues λ1, λ2 of the lower-right 2&times; 2 sub-
matrix of CTi Ci . Here σi is the eigenvalue closest to the lower-right
</p>
<p>element ofCTi Ci .</p>
<p/>
</div>
<div class="page"><p/>
<p>390 A Matrix Calculations
</p>
<p>(B) The matrix Vi is determined such that V Ti (C
T
i Ci &minus; σiI) has upper tri-
</p>
<p>angular form.
</p>
<p>(C) The matrix Ui is determined such that Ci+1 = UTi CiVi is again bidiag-
onal.
</p>
<p>The matrix Vi from step (B) exists, according to a theorem by FRANCIS [22],
if
</p>
<p>(a) CTi Ci is tridiagonal with non-vanishing subdiagonal elements,
</p>
<p>(b) Vi is orthogonal,
</p>
<p>(c) σi is an arbitrary scalar,
</p>
<p>(d) V Ti (C
T
i Ci)Vi is tridiagonal, and
</p>
<p>(e) The first column of V Ti (C
T
i Ci &minus;σiI) except the first element vanishes.
</p>
<p>The requirement (a) is fulfilled, since Ci is bidiagonal and the special case
2 has been treated if necessary; (b) is also fulfilled by constructing Vi as the
product of Givens matrices. This is done in such a way that simultaneously
(d) and (e) are fulfilled. In particular one has
</p>
<p>Vi = R1R2 &middot; &middot; &middot;Rn&minus;1 , UTi = Ln&minus;1Ln &middot; &middot; &middot;L1 ,
</p>
<p>where
</p>
<p>Rj acts on the columns j and j +1 of C,
Lj acts on the rows j and j +1 of C,
R1 is determined such that requirement (e) is fulfilled,
L1, R2, L2, . . . are determined such that (e) is fulfilled without
violating (d).
</p>
<p>For σi one obtains
</p>
<p>σi = d2n + en
(
en&minus;
</p>
<p>dn&minus;1
t
</p>
<p>)
</p>
<p>with
t = f +
</p>
<p>&radic;
1+f 2 , f &ge; 0 ,
</p>
<p>t = f &minus;
&radic;
</p>
<p>1+f 2 , f &lt; 0 ,
and
</p>
<p>f =
d2n&minus;d2n&minus;1 + e2n&minus; e2n&minus;1
</p>
<p>2endn&minus;1
.
</p>
<p>The first column of the matrix (CTi Ci &minus;σiI) is</p>
<p/>
</div>
<div class="page"><p/>
<p>A.14 Algorithm for Singular Value Decomposition 391
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>d21 &minus;σi
d1e2
</p>
<p>0
...
</p>
<p>0
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠
</p>
<p>.
</p>
<p>One determines the matrix R1, which defines a Givens transformation, such
that all elements of the first column of RT1 (C
</p>
<p>T
i Ci&minus;σiI) except the first vanish.
</p>
<p>Application of R1 on Ci produces, however, an additional element H = C21,
so that Ci is no longer bidiagonal,
</p>
<p>Ci =
</p>
<p>⎛
⎜⎜⎜⎜⎝
</p>
<p>. .
</p>
<p>. .
</p>
<p>. .
</p>
<p>. .
</p>
<p>.
</p>
<p>⎞
⎟⎟⎟⎟⎠
</p>
<p>, CiR1 =
</p>
<p>⎛
⎜⎜⎜⎜⎝
</p>
<p>. .
</p>
<p>H . .
</p>
<p>. .
</p>
<p>. .
</p>
<p>.
</p>
<p>⎞
⎟⎟⎟⎟⎠
</p>
<p>.
</p>
<p>By application of L1, this element is projected onto the diagonal. In its place
a new element H = C13 is created,
</p>
<p>L1CiR1 =
</p>
<p>⎛
⎜⎜⎜⎜⎝
</p>
<p>. . H
</p>
<p>. .
</p>
<p>. .
</p>
<p>. .
</p>
<p>.
</p>
<p>⎞
⎟⎟⎟⎟⎠
</p>
<p>.
</p>
<p>By continuing the procedure, the additional element is moved further down
and to the right, and can be completely eliminated in the last step:
</p>
<p>Tn&minus;1
</p>
<p>⎛
⎜⎜⎜⎜⎝
</p>
<p>. .
</p>
<p>. .
</p>
<p>. .
</p>
<p>. .
</p>
<p>H .
</p>
<p>⎞
⎟⎟⎟⎟⎠
</p>
<p>=
</p>
<p>⎛
⎜⎜⎜⎜⎝
</p>
<p>. .
</p>
<p>. .
</p>
<p>. .
</p>
<p>. .
</p>
<p>.
</p>
<p>⎞
⎟⎟⎟⎟⎠
</p>
<p>= Ci+1 .
</p>
<p>If the lower non-diagonal element of Ci+1 is already zero, then the lower
diagonal element is already a singular value. Otherwise the procedure is re-
peated, whereby it is first checked whether now one of the two special cases
is present. The procedure typically converges in about 2k steps; (k is the rank
of the original matrix A). If convergence has still not been reached after 10k
steps, then the algorithm is terminated without success.
</p>
<p>Change of sign. If a singular value dk has been found, i.e., if ek = 0,
then it is checked whether it is negative. If this is the case, then a simple
orthogonal transformation is applied that multiplies the element dk = Ckk of</p>
<p/>
</div>
<div class="page"><p/>
<p>392 A Matrix Calculations
</p>
<p>C and all elements in the kth column of the matrix contained in a by &minus;1. The
index k can then be reduced by one. Corresponding to (A.12.6) the matrix B
is multiplied on the left by UT. This is done successively with the individual
factors making up UT.
</p>
<p>A.14.4 Ordering of the Singular Values and Permutation
</p>
<p>increasing order. This is done by a sequence of permutations of neighboring
singular values, carried out if the singular value that follows is larger then the
preceding. The matrices stored in a and b are multiplied by a corresponding
sequence of permutation matrices; cf. (A.14.4).
</p>
<p>A.14.5 Singular Value Analysis
</p>
<p>In the last step the singular value analysis is carried out as described in
Sect. A.13 by the method
a value ℓ &le; k is determined such that si &lt; f s1 for i &gt; ℓ. The columns of
the array B, which now contains the vectors g, are transformed in their first ℓ
elements into p̃1 according to (A.12.11), and the elements ℓ+1, . . ., n are set
equal to zero. Then the solution vectors x̃, which make up the columns of the
solution matrix X̃, are computed according to (A.12.12).
</p>
<p>A.15 Least Squares with Weights
</p>
<p>Instead of the problem (A.5.3),
</p>
<p>r2 = (Ax&minus;b)T(Ax&minus;b)= min , (A.15.1)
</p>
<p>one often encounters a similar problem that in addition contains a positive-
definite symmetric weight-matrix Gm&times;m,
</p>
<p>r2 = (Ax&minus;b)TG(Ax&minus;b)= min . (A.15.2)
</p>
<p>In (A.15.1) one simply has G= I . Using the Cholesky decomposition (A.9.1)
of G, i.e., G= UTU , one has
</p>
<p>r2 = (Ax&minus;b)TUTU(Ax&minus;b)= min . (A.15.3)
</p>
<p>With the definitions
A&prime; = U A , b&prime; = U b (A.15.4)
</p>
<p>By the method DatanMatrix.sv3 the singular values are put into non-
</p>
<p>f ≪ 1DatanMatrix.sv4. For a given factor</p>
<p/>
</div>
<div class="page"><p/>
<p>A.16 Least Squares with Change of Scale 393
</p>
<p>Eq. (A.15.3) takes on the form
</p>
<p>r2 = (A&prime;x&minus;b&prime;)T(A&prime;x&minus;b&prime;)= min . (A.15.5)
</p>
<p>After the replacement (A.15.4), the problem (A.15.2) is thus equivalent to the
original one (A.15.1).
</p>
<p>In Sect. A.11 we called the n&times;n matrix
</p>
<p>C = (ATA)&minus;1 (A.15.6)
</p>
<p>the unweighted covariance matrix of the unknowns x in the Problem (A.15.1).
In Problem (A.15.2), the weighted covariance matrix
</p>
<p>Cx = (A&prime;TA&prime;)&minus;1 = (ATGA)&minus;1 (A.15.7)
</p>
<p>appears in its place.
</p>
<p>A.16 Least Squares with Change of Scale
</p>
<p>The goal in solving a problem of the type
</p>
<p>(Ax&minus;b)T(Ax&minus;b)= min (A.16.1)
</p>
<p>is the most accurate numerical determination of the solution vector x̃ and the
covariance matrix C. A change of scale in the elements of A, b, and x can
lead to an improvement in the numerical precision.
</p>
<p>Let us assume that the person performing the problem already has an
approximate idea of x̃ and C, which we call z and K . The matrix K has the
Cholesky decomposition K = LTL. By defining
</p>
<p>A&prime; = AL , b&prime; = b&minus;Az , x&prime; = L&minus;1(x&minus; z) , (A.16.2)
</p>
<p>Eq. (A.16.1) becomes
</p>
<p>(A&prime;x&prime;&minus;b&prime;)T(A&prime;x&prime;&minus;b)= min .
</p>
<p>The meaning of the new vector of unknowns x&prime; is easily recognizable for
the case where K is a diagonal matrix. We write
</p>
<p>K =
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>σ 21
σ 22
</p>
<p>. . .
</p>
<p>σ 2n
</p>
<p>⎞
⎟⎟⎟⎠ , L=
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>σ1
σ2
</p>
<p>. . .
</p>
<p>σn
</p>
<p>⎞
⎟⎟⎟⎠ ,</p>
<p/>
</div>
<div class="page"><p/>
<p>394 A Matrix Calculations
</p>
<p>where the quantities σ 2i are the estimated values of the variances of the un-
knowns xi . Thus the ith component of the vector x&prime; becomes
</p>
<p>x &prime;i =
xi &minus; zi
σi
</p>
<p>.
</p>
<p>If in fact one has x̃i = zi and the corresponding variance σ 2i , then x &prime;i = 0 and
has a variance of one. If the estimates are at least of the correct order of
magnitude, the x &prime;i are close to zero and their variances are of order unity. In
addition, in case the full matrix is, in fact, estimated with sufficient accuracy,
the components of x&prime; are not strongly correlated with each other.
</p>
<p>In practice, one carries out the transformation (A.16.2) only in excep-
tional cases. One must take care, however, that &ldquo;reasonable&rdquo; variables are
chosen for (A.16.2). This technique is applied in the graphical representation
of data. If it is known, for example, that a voltage U varies in the region
σ = 10 mV about the value U0 = 1 V, then instead of U one would plot the
quantity U &prime; = (U &minus;U0)/σ , or some similar quantity.
</p>
<p>A.17 Modification of Least Squares According
</p>
<p>to Marquardt
</p>
<p>Instead of the problem
</p>
<p>(Ax&minus;b)T(Ax&minus;b)= min , (A.17.1)
</p>
<p>which we have also written in the shorter form
</p>
<p>Ax&minus;b&asymp; 0 , (A.17.2)
</p>
<p>let us now consider the modified problem
</p>
<p>m {
n {
</p>
<p>(
A
</p>
<p>λI
</p>
<p>)
</p>
<p>︸ ︷︷ ︸
n
</p>
<p>x&asymp;
(
b
</p>
<p>0
</p>
<p>)
} m
} n . (A.17.3)
</p>
<p>Here I is the n&times;n unit matrix and λ is a non-negative number. The modi-
fied problem is of considerable importance for fitting nonlinear functions with
the method of least squares (Sect. 9.5) or for minimization (Sect. 10.15). For
λ= 0 Eq. (A.17.3) clearly becomes (A.17.2). If, on the other hand, λ is very
large, or more precisely, if it is large compared to the absolute values of the
elements of A and b, then the last &ldquo;row&rdquo; of (A.17.3) determines the solution
x̃, which is the null vector for λ&rarr;&infin;.
</p>
<p>We first ask which direction the vector x̃ has for large values of λ. The
normal equations corresponding to (A.17.3), cf. (A.5.16), are</p>
<p/>
</div>
<div class="page"><p/>
<p>A.17 Modification of Least Squares According to Marquardt 395
</p>
<p>(AT, λI)
</p>
<p>(
A
</p>
<p>λI
</p>
<p>)
x= (ATA+λ2I)x= (AT, λI)
</p>
<p>(
b
</p>
<p>0
</p>
<p>)
= ATb
</p>
<p>with the solution
x̃= (ATA+λ2I)&minus;1ATb .
</p>
<p>For large λ the second term in parentheses dominates, and one obtains simply
</p>
<p>x̃= λ&minus;2ATb .
</p>
<p>That is, for large values of λ, the solution vector tends toward the direction of
the vector ATb.
</p>
<p>We will now show that for a given λ the solution x(λ) to (A.17.3) can
easily be found with the singular value decomposition simultaneously with the
determination of the solution x̃ of (A.17.2). The singular value decomposition
(A.12.1) of A is
</p>
<p>A= U
(
S
</p>
<p>0
</p>
<p>)
V T .
</p>
<p>By substituting into (A.17.3) and multiplying on the left we obtain
(
UT 0
</p>
<p>0 V T
</p>
<p>)(
U
(
S
0
</p>
<p>)
V T
</p>
<p>λI
</p>
<p>)
x=
</p>
<p>(
UT 0
</p>
<p>0 V T
</p>
<p>)(
b
</p>
<p>0
</p>
<p>)
</p>
<p>or ⎛
⎝
</p>
<p>S
</p>
<p>0
λI
</p>
<p>⎞
⎠p=
</p>
<p>(
g
</p>
<p>0
</p>
<p>)
, (A.17.4)
</p>
<p>where, using the notation as in Sect. A.12,
</p>
<p>p= V Tx , g= UTb .
</p>
<p>By means of Givens transformations, the matrix on the left-hand side of
(A.17.4) can be brought into diagonal form. One obtains
</p>
<p>⎛
⎝
</p>
<p>S(λ)
</p>
<p>0
0
</p>
<p>⎞
⎠p=
</p>
<p>(
g(λ)
</p>
<p>h(λ)
</p>
<p>)
</p>
<p>with
</p>
<p>g
(λ)
i =
</p>
<p>gisi
</p>
<p>s
(λ)
i
</p>
<p>, i = 1, . . . ,n ,
</p>
<p>g
(λ)
i = gi , i = n+1, . . . ,m ,
</p>
<p>h
(λ)
i = &minus;
</p>
<p>gλi
</p>
<p>s
(λ)
i
</p>
<p>, i = 1, . . . ,n ,
</p>
<p>s
(λ)
i =
</p>
<p>&radic;
s2i +λ2</p>
<p/>
</div>
<div class="page"><p/>
<p>396 A Matrix Calculations
</p>
<p>and thus
</p>
<p>p
(λ)
i =
</p>
<p>gisi
</p>
<p>s2i +λ2
= p(0)i
</p>
<p>s2i
</p>
<p>s2i +λ2
, i = 1, . . . ,k ,
</p>
<p>p
(λ)
i = 0 , i = k+1, . . . ,n .
</p>
<p>The solution x̃(λ) of (A.17.3) is then
</p>
<p>x̃(λ) = V p(λ) .
</p>
<p>A.18 Least Squares with Constraints
</p>
<p>One often encounters the problem (A.5.3)
</p>
<p>r2 = (Ax&minus;b)2 = min (A.18.1)
</p>
<p>with the constraint
Ex= d . (A.18.2)
</p>
<p>Here A is as before an m&times;n matrix and E is an ℓ&times;n matrix. We will restrict
ourselves to the only case that occurs in practice,
</p>
<p>RangE = ℓ &lt; n . (A.18.3)
</p>
<p>The determination of an extreme value with constraints is usually treated
in analysis textbooks with the method of Lagrange multipliers. Here as well
we rely on orthogonal transformations. The following method is due to LAW-
SON and HANSON [18]. It uses a basis of the null space of E. First we carry
out an orthogonal decomposition of E as in (A.5.5),
</p>
<p>E =H RKT . (A.18.4)
</p>
<p>Here we regard the orthogonal n&times;n matrix K as being constructed out of an
n&times;ℓ matrix K1 and an n&times; (n&minus; ℓ) matrix K2,
</p>
<p>K = (K1, K2) . (A.18.5)
</p>
<p>According to (A.5.6) and (A.5.7), all solutions of (A.18.2) have the form
</p>
<p>The method
</p>
<p>tors for two values of λ. It proceeds mostly as DatanMatrix.sigular-
</p>
<p>the method
</p>
<p>problem.
</p>
<p>DatanMatrix.marquardt computes these solution vec-
</p>
<p>ValueDecomposition; only in step 4 instead of Datanmatrix.sv4
</p>
<p>DatanMatrix.svm is used which is adapted to the Marquardt</p>
<p/>
</div>
<div class="page"><p/>
<p>A.18 Least Squares with Constraints 397
</p>
<p>x̂=K1p̃1 +K2p2 = x̃+K2p2 . (A.18.6)
</p>
<p>Here x̃ is the unique solution of minimum absolute value of (A.18.2). For
brevity we will write this in the form x̃=E+d; cf. (A.10.3). p2 is an arbitrary
(n&minus; ℓ)-vector, since the vectors K2p2 form the null space of E,
</p>
<p>EK2p2 = 0 . (A.18.7)
</p>
<p>The constraint (A.18.2) thus says that the vector x for which (A.18.1) is a
minimum must come from the set of all vectors x̂, i.e.,
</p>
<p>(Âx&minus;b)2 = (A(̃x+K2p2)&minus;b)2
</p>
<p>= (AK2p2 &minus; (b&minus;A x̃))2 = min . (A.18.8)
</p>
<p>This relation is a least-squares problem without constraints, from which the
(n&minus; ℓ)-vector p2 can be determined. We write its solution using (A.10.3) in
the form
</p>
<p>p̃2 = (AK2)+(b&minus;A x̃) . (A.18.9)
By substitution into (A.18.6) we finally obtain
</p>
<p>x= x̃+A+(b&minus;A x̃)=E+d+K2(AK2)+(b&minus;AE+d) (A.18.10)
</p>
<p>as the solution of (A.18.1) with the constraint (A.18.2).
The following prescription leads to the solution (A.18.10). Its starting
</p>
<p>point is the fact that one can set H = I because of (A.18.3).
Step 1: One determines an orthogonal matrix K = (K1, K2) as in
</p>
<p>(A.18.5) such that
</p>
<p>EK = (EK1, EK2)= (Ẽ1, 0)
</p>
<p>and such that Ẽ1 is a lower triangular matrix. In addition one computes
</p>
<p>AK = (AK1, AK2)= (Ã1, Ã2) .
</p>
<p>Step 2: One determines the solution p̃1 of
</p>
<p>Ẽ1p1 = d .
</p>
<p>This is easy, since Ẽ1 is a lower triangular matrix of rank ℓ. Clearly one has
x̃=K1p̃1.
</p>
<p>Step 3: One determines the vector
</p>
<p>b̄= b&minus; Ã1̃p1 = b&minus;AK1KT1 x̃= b&minus;A x̃ .
</p>
<p>Step 4: One determines the solution p̃2 to the least-squares problem
(A.18.8) (without constraints)</p>
<p/>
</div>
<div class="page"><p/>
<p>398 A Matrix Calculations
</p>
<p>(Ã2p2 &minus; b̄)2 = min .
</p>
<p>Step 5: From the results of steps 2 and 4 one finds the solution to (A.18.1)
with the constraint (A.18.2)
</p>
<p>x=K
(
p̃1
</p>
<p>p̃2
</p>
<p>)
=
</p>
<p>(
K1p̃1
</p>
<p>K2p̃2
</p>
<p>)
.
</p>
<p>We will now consider a simple example that illustrates both the least-
squares problem with constraints as well as the method of solution given
above.
</p>
<p>Example A.5: Least squares with constraints
</p>
<p>Suppose the relation (A.18.1) has the simple form
</p>
<p>r2 = x2 = min
</p>
<p>for n= 2. One then has m=n= 2, A= I , and b= 0. Suppose the constraint is
</p>
<p>x1 +x2 = 1 ,
</p>
<p>i.e., ℓ= 1, E = (1,1), d= 1.
The problem has been chosen such that it can be solved by inspec-
</p>
<p>tion without mathematical complications. The function z = x2 = x21 + x22
corresponds to a paraboloid in the (x1,x2,z) space, whose minimum is at
x1 = x2 = 0. We want, however, to find not the minimum in the entire (x1,x2)
plane, but rather only on the line x1 +x2 = 1, as shown in Fig. A.7. It clearly
lies at the point where the line has its smallest distance from the origin, i.e.,
at x1 = x2 = 1/2.
</p>
<p>10.5
</p>
<p>0.5
</p>
<p>1
</p>
<p>x1
</p>
<p>2x
</p>
<p>x
</p>
<p>1x  + x   = 12
&AElig;
~
</p>
<p>Fig.A.7: The solution x̃ to Example A.5 lies on the line given by the constraint x1 +x2 = 1.
</p>
<p>Of course one obtains the same result with the algorithm. With</p>
<p/>
</div>
<div class="page"><p/>
<p>A.19 Java Classes and Example Programs 399
</p>
<p>K = 1&radic;
2
</p>
<p>(
1 &minus;1
1 1
</p>
<p>)
</p>
<p>we obtain Ẽ1 =
&radic;
</p>
<p>2, p̃1 = 1/
&radic;
</p>
<p>2,
</p>
<p>Ã1 =
1&radic;
2
</p>
<p>(
1
</p>
<p>1
</p>
<p>)
, (Ã2)=
</p>
<p>1&radic;
2
</p>
<p>(&minus;1
1
</p>
<p>)
, b̄=&minus;1
</p>
<p>2
</p>
<p>(
1
</p>
<p>1
</p>
<p>)
.
</p>
<p>We solve the problem (Ã2p2 &minus; b̄)2 = min with the normal equations
</p>
<p>p̃2 = (Ã
T
2 Ã 2)
</p>
<p>&minus;1Ã
T
2 b̄= (
</p>
<p>&radic;
2)&minus;1 &middot;0 = 0 .
</p>
<p>The full solution is then
</p>
<p>x=K
(
p̃1
</p>
<p>p̃2
</p>
<p>)
= 1&radic;
</p>
<p>2
</p>
<p>(
1 &minus;1
1 1
</p>
<p>)(
1/
&radic;
</p>
<p>2
</p>
<p>0
</p>
<p>)
=
</p>
<p>(
1/2
</p>
<p>1/2
</p>
<p>)
.
</p>
<p>.
</p>
<p>A.19 Java Classes and Example Programs
</p>
<p>Java Classes for Vector and Matrix Operations
</p>
<p>DatanVector contains methods for vector operations.
</p>
<p>DatanMatrix contains methods for matrix operations.
</p>
<p>At first the matrices
</p>
<p>A=
(
</p>
<p>1 2 3
2 1 3
</p>
<p>)
, B =
</p>
<p>(
2 3 1
1 5 4
</p>
<p>)
, C =
</p>
<p>⎛
⎝
</p>
<p>1 5
3 4
2 3
</p>
<p>⎞
⎠
</p>
<p>and the vectors
</p>
<p>u=
</p>
<p>⎛
⎝
</p>
<p>0
3
4
</p>
<p>⎞
⎠ , v=
</p>
<p>⎛
⎝
</p>
<p>3
1
2
</p>
<p>⎞
⎠ , w=
</p>
<p>(
5
</p>
<p>2
</p>
<p>)
</p>
<p>are defined. Then, with the appropriate methods, simple operations are performed
with these quantities. Finally, the resulting matrices and vectors are displayed. The
operations are
</p>
<p>R=A ,R=A+B ,R=A&minus;B ,R=AC ,S=ABT ,T =ATB ,R= I ,R= 0.5A ,
</p>
<p>R=AT ,z=w ,x= u+v ,x= u&minus;v ,d = u &middot;v , s = |u| ,x= 0.5u ,x= 0 .
</p>
<p>The method DatanMatrix.leastSquaresWithConstraints
</p>
<p>solves the problem of least squares (Ax&minus;b)2 = min with the linear constraint
Ex = d.
</p>
<p>Example Program A.1: The class E1Mtx demonstrates simple operations
</p>
<p>of matrix and vector algebra</p>
<p/>
</div>
<div class="page"><p/>
<p>400 A Matrix Calculations
</p>
<p>Example Program A.2: The class E2Mtx demonstrates the handling of
</p>
<p>submatrices and subvectors.
</p>
<p>The matrices
</p>
<p>A=
(
</p>
<p>1 2 3
</p>
<p>2 1 3
</p>
<p>)
, D =
</p>
<p>(
0 2
</p>
<p>1 3
</p>
<p>)
</p>
<p>and the vectors
</p>
<p>u =
</p>
<p>⎛
⎝
</p>
<p>0
</p>
<p>3
</p>
<p>4
</p>
<p>⎞
⎠ , w =
</p>
<p>(
5
</p>
<p>2
</p>
<p>)
</p>
<p>are defined.
</p>
<p>Next a submatrix S is taken from A, and a submatrix of A is overwritten by D.
</p>
<p>A column vector and a row vector are taken from A and inserted into A. Finally some
</p>
<p>elements are taken according to a list from the vector u and assembled in a vector z,
</p>
<p>and elements of w are put into positions (defined by list) of the vector u.
</p>
<p>Example Program A.3: The class E3Mtx demonstrates the performance of
</p>
<p>Givens transformations
</p>
<p>Fist the two vectors
</p>
<p>u =
(
</p>
<p>3
</p>
<p>4
</p>
<p>)
, w =
</p>
<p>(
1
</p>
<p>1
</p>
<p>)
</p>
<p>are defined. Next, by the use of DatanMatrix.defineGivensTransfor-
</p>
<p>c and s for the vector u are computed and dis-
</p>
<p>played. The Givens transformation of u with these parameters is performed with
</p>
<p>u&prime; =
(
</p>
<p>5
</p>
<p>0
</p>
<p>)
.
</p>
<p>Finally, by calling
</p>
<p>c and s are computed for the vector w and the transformation is
</p>
<p>applied to this vector.
</p>
<p>Example Program A.4: The class E4Mtx demonstrates the performance of
</p>
<p>Householder transformations
</p>
<p>First the two vectors
</p>
<p>v =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎝
</p>
<p>1
</p>
<p>2
</p>
<p>0
</p>
<p>4
</p>
<p>3
</p>
<p>4
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎠
</p>
<p>, c =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎝
</p>
<p>1
</p>
<p>2
</p>
<p>0
</p>
<p>4
</p>
<p>3
</p>
<p>4
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎠
</p>
<p>are defined. Moreover, the indices n, p are set to ℓ n = 6, p = 3, ℓ = 5. By
calling
</p>
<p>holder transformation defined by these indices and the vector v is initialized. The
</p>
<p>application of this transformation to the vectors v and c is performed by two calls of
</p>
<p>. The results are dis-
</p>
<p>played alphanumerically.
</p>
<p>mation transformation parameters
</p>
<p>DatanMatrix.applyGivensTransformation yielding
</p>
<p>DatanMatrix.applyGivensTransforma-
</p>
<p>tion parameters
</p>
<p>       
</p>
<p>DatanMatrix.defineHouseholderTransformation the House-
</p>
<p>DatanMatrix.defineHouseholderTransformation</p>
<p/>
</div>
<div class="page"><p/>
<p>A.19 Java Classes and Example Programs 401
</p>
<p>Example Program A.5: The class E5Mtx demonstrates the Gaussian
</p>
<p>algorithm for the solution of matrix equations
</p>
<p>The matrix
</p>
<p>A=
</p>
<p>⎛
⎝
</p>
<p>1 2 3
</p>
<p>2 1 &minus;2
1 1 2
</p>
<p>⎞
⎠
</p>
<p>and the 3 &times; 3 unit matrix B are defined. A call of Datanmatrix.matrix
AX = B, i.e., X is the inverse of A. The
</p>
<p>matrix A is identical to the one chosen in Example A.1. In that example the algo-
</p>
<p>rithm is shown step by step.
</p>
<p>Example Program A.6: The class E6Mtx demonstrates Cholesky
</p>
<p>decomposition and Cholesky inversion
</p>
<p>First, the matrices
</p>
<p>B =
</p>
<p>⎛
⎝
</p>
<p>1 7 13
</p>
<p>3 9 17
</p>
<p>5 11 19
</p>
<p>⎞
⎠ , C =
</p>
<p>⎛
⎝
</p>
<p>1 1
</p>
<p>1 1
</p>
<p>1 1
</p>
<p>⎞
⎠
</p>
<p>are defined and the symmetric, positive-definite matrix A = BTB is constructed. By
calling
</p>
<p>tion A= UTU is performed and the triangular matrix U is displayed. Multiplication
of UT by U yields in fact U UT = S. The method
</p>
<p>R = UC. Finally, by Cholesky inversion with
, the inverse S &minus;1 of S is computed. Mul-
</p>
<p>tiplication with the original matrix yields SS&minus;1 = I .
</p>
<p>Example Program A.7: The class E7Mtx demonstrates the singular value
</p>
<p>decomposition
</p>
<p>The program first operates on the same matrix as
</p>
<p>version is now performed with
</p>
<p>Next, the matrix is replaced by
</p>
<p>A=
</p>
<p>⎛
⎝
</p>
<p>1 2 2
</p>
<p>2 1 1
</p>
<p>1 1 1
</p>
<p>⎞
⎠ .
</p>
<p>This matrix, having two identical columns, is singular. With another call of Datan
</p>
<p>the diagonal matrix D, and the two orthogonal matrices U and V are determined.
</p>
<p>DatanMatrix.choleskyDecompostion the Cholesky decomposi-
</p>
<p>DatanMatrix.cholesky-
</p>
<p>Multiply is then used to compute
</p>
<p>DatanMatrix.choleskyInversion
</p>
<p>E5Mtx. However, the matrix in-
</p>
<p>DatanMatrix.pseudoInverse
</p>
<p>Equation solves the matrix equation
</p>
<p>Matrix.pseudo Inversenot only the psudoinverse matrix but also the residuals,</p>
<p/>
</div>
<div class="page"><p/>
<p>402 A Matrix Calculations
</p>
<p>In the framework of our programs, in particular that of least-squares and minimization
problems, matrix equations are solved nearly exclusively by singular value decompo-
sition. In Sect. A.5 we have listed the different cases of the matrix equation Ax &asymp; b.
If A is an m&times;n matrix then first we must distinguish between the cases m= n (case
1), m &gt; n (case 2), and m &lt; n (case 3). A further subdivision is brought about by
the rank of A. If the rank k of A is k = min(m,n) then we have the case 1a (or 2a or
3a). If, however, k &lt; min(m,n) then we are dealing with case 1b (or 2b or 3b). The
rank of a matrix is equal to its number of non-vanishing singular values. In numerical
calculations, which are always performed with finite accuracy, one obviously has to
define more precisely the meaning of &ldquo;non-vanishing&rdquo;. For this definition we use the
method of singular value analysis (Sect. A.13) and set a singular value equal to zero
if it is smaller than a fraction f of the largest singular value. The number of finite
singular values remaining in this analysis is called the pseudorank. In addition to the
cases mentioned above we consider as well the cases 1c, 2c, and 3c, in which the
matrix A has full rank but not full pseudorank.
</p>
<p>The program consists of two nested loops. The outer loop runs through the
cases 1, 2, 3, the inner loop through the subcases a, b, c. For each case the matrix A
is composed of individual vectors. In the subcase b two of these vectors are chosen
to be identical. In the subcase c they are identical except for one element, which
in one vector differs by ε = 10&minus;12 compared to the value in the other vector. In
case 3 the system of linear equations symbolized by the matrix equation Ax = b has
less equations (m) than unknowns (n). This case does not appear in practice and,
therefore, is not included in the programs. It is simulated here in the following way.
In case 3 with m= 2 and n= 3, the matrix A is extended to become a 3&times;3 matrix by
addition of another row the elements, of which are all set to zero, and correspondingly
m is set to 3.
</p>
<p>If the singular value analysis shows that one or several singular values are
smaller than the fraction f of the largest singular value, then they are set to zero.
In our example program for each of the 9 cases the analysis is performed twice, first
for f = 10&minus;15 and then for f = 10&minus;10. For f = 10&minus;15 in our example cases 1c, 2c,
3c the matrix A has full rank, in spite of the small value of ε = 10&minus;12. the singular
value analysis with f = 10&minus;10 reduces the number of singular values. Note that the
elements of the solution matrix differ as the choice of f changes for cases 1c, 2c, 3c.
The unwieldly numerical values in the case of f = 10&minus;15 show that we are near the
limits of numerical stability.
</p>
<p>with a short class. It solves the problem Ax &asymp; b &ndash; modified according to (A.17.3) &ndash;
for given A, b, and λ and displays the results x1 and x2.
</p>
<p>Example Program A.8: The class E8Mtx demonstrates the solution of
</p>
<p>matrix equations by singular value decomposition for 9 different cases
</p>
<p>Example Program A.9: The class E9Mtx demonstrates the use of the
</p>
<p>method DatanMatrix.marquardt
</p>
<p>The will be rarely called directly. It was
</p>
<p>written to be used in
</p>
<p>method DatanMatrix.marquardt
</p>
<p>LsqMar and MinMar. For completeness we demonstrate it</p>
<p/>
</div>
<div class="page"><p/>
<p>A.19 Java Classes and Example Programs 403
</p>
<p>The problem of Examples 9.9 and 9.10 is solved, i.e., the measurement x1 = 89, x2 =
31, x3 = 62 of the three angles of a triangle and the evaluation of these measurements
using the constraint x1+x2+x3 = 180. The evaluation requires the solution of (Ax&minus;
b)2 = min with the constraint Ex= d with
</p>
<p>A=
</p>
<p>⎛
⎝
</p>
<p>1 0 0
0 1 0
0 0 1
</p>
<p>⎞
⎠ , b=
</p>
<p>⎛
⎝
</p>
<p>89
31
62
</p>
<p>⎞
⎠ , E = (1,1,1) , d= 180 .
</p>
<p>.
</p>
<p>Example Program A.10: The class E10Mtx demonstrates solution of the
</p>
<p>least squares problem with constraints by the method
</p>
<p>DatanGraphics leastSquaresWithConstraints
</p>
<p>In the program the matrices and vectors A, b, E, d are provided. The solution is
</p>
<p>computed by calling .
</p>
<p>It is, of course, identical to the results found in the previously mentioned examples.
</p>
<p>DatanGraphics leastSquaresWithConstraints</p>
<p/>
</div>
<div class="page"><p/>
<p>B. Combinatorics
</p>
<p>Consider n distinguishable objects a1, a2, . . ., an. We ask for the number
of possible ways P kn , in which one can place k of them in a given order.
Such orderings are called permutations. For the example n = 4, k = 2 these
permutations are
</p>
<p>a1a2 , a1a3 , a1a4 ,
</p>
<p>a2a1 , a2a3 , a2a4 ,
</p>
<p>a3a1 , a3a2 , a3a4 ,
</p>
<p>a4a1 , a4a2 , a4a3 ,
</p>
<p>i.e., P nk = 12. The answer for the general problem can be derived from the
following scheme. There are n different possible ways to occupy the first
place in a sequence. When one of these ways has been chosen, however, there
are only n&minus;1 objects left, i.e., there remain n&minus;1 ways to occupy the second
place, and so forth. One therefore has
</p>
<p>P nk = n(n&minus;1)(n&minus;2) &middot; &middot; &middot;(n&minus; k+1) . (B.1)
The result can also be written in the form
</p>
<p>P nk =
n!
</p>
<p>(n&minus; k)! , (B.2)
</p>
<p>where
n! = 1 &middot;2 &middot; &middot; &middot;n ; 0! = 1 , 1! = 1 . (B.3)
</p>
<p>Often one is not interested in the order of the k objects within a permutation
(the same k objects can be arranged in k! different ways within the sequence),
but rather one only considers the number of different ways of choosing of k
objects out of a total of n. Such a choice is called a combination. The number
of possible combinations of k elements out of n is then
</p>
<p>Cnk =
P nk
</p>
<p>k! =
n!
</p>
<p>k!(n&minus; k)! =
(
n
</p>
<p>k
</p>
<p>)
. (B.4)
</p>
<p>S. Brandt, Data Analysis: Statistical and Computational Methods for Scientists and Engineers,
DOI 10.1007/978-3-319-03762-2, &copy; Springer International Publishing Switzerland 2014
</p>
<p>405</p>
<p/>
</div>
<div class="page"><p/>
<p>406 B Combinatorics
</p>
<p>n
n
</p>
<p>k
</p>
<p>10
</p>
<p>111
</p>
<p>1212
</p>
<p>13313
</p>
<p>4 1 4 6 4 1
</p>
<p>5 1 5 10 10 5 1
</p>
<p>6 1 6 15 20 15 6 1
</p>
<p>7 1 7 21 35 35 21 7 1
</p>
<p>8 1 8 28 56 70 56 28 8 1
...
</p>
<p>...
</p>
<p>0 1 2 3 4 5 6 7 8 k
</p>
<p>Fig.B.1: Pascal&rsquo;s triangle.
</p>
<p>For the binomial coefficients
(
n
k
</p>
<p>)
one has the simple recursion relation
</p>
<p>(
n&minus;1
k
</p>
<p>)
+
(
n&minus;1
k&minus;1
</p>
<p>)
=
</p>
<p>(
n
</p>
<p>k
</p>
<p>)
, (B.5)
</p>
<p>which can easily be proven by computation:
</p>
<p>(n&minus;1)!
k!(n&minus; k&minus;1)! +
</p>
<p>(n&minus;1)!
(k&minus;1)!(n&minus; k)!
</p>
<p>= (n&minus; k)(n&minus;1)!+ k(n&minus;1)!
k!(n&minus; k)!
</p>
<p>= n!
k!(n&minus; k)! .
</p>
<p>The recursion formula is the basis for the famous Pascal&rsquo;s triangle,
which, because of its beauty, is shown in Fig. B.1. The name &ldquo;binomial coef-
ficient&rdquo; comes from the well-known binomial theorem,
</p>
<p>(a+b)n =
n&sum;
</p>
<p>k=0
</p>
<p>(
n
</p>
<p>k
</p>
<p>)
akbn&minus;k , (B.6)
</p>
<p>the proof of which (by induction) is left to the reader. We use the theorem in
order to derive a very important property of the coefficient
</p>
<p>(
n
k
</p>
<p>)
. For this we
</p>
<p>write it in the simple form for b = 1, i.e.,
</p>
<p>(a+1)n =
n&sum;
</p>
<p>k=0
</p>
<p>(
n
</p>
<p>k
</p>
<p>)
ak</p>
<p/>
</div>
<div class="page"><p/>
<p>B. Combinatorics 407
</p>
<p>and we then apply it a second time,
</p>
<p>(a+1)n+m = (a+1)n(a+1)m ,
n+m&sum;
</p>
<p>ℓ=0
</p>
<p>(
n+m
ℓ
</p>
<p>)
aℓ =
</p>
<p>n&sum;
</p>
<p>j=0
</p>
<p>(
n
</p>
<p>j
</p>
<p>)
aj
</p>
<p>m&sum;
</p>
<p>k=0
</p>
<p>(
m
</p>
<p>k
</p>
<p>)
ak .
</p>
<p>If we consider only the term with aℓ, then by comparing coefficients we find
</p>
<p>(
n+m
ℓ
</p>
<p>)
=
</p>
<p>ℓ&sum;
</p>
<p>j=0
</p>
<p>(
n
</p>
<p>j
</p>
<p>)(
m
</p>
<p>ℓ&minus; j
</p>
<p>)
. (B.7)</p>
<p/>
</div>
<div class="page"><p/>
<p>C. Formulas and Methods for the Computation
</p>
<p>of Statistical Functions
</p>
<p>C.1 Binomial Distribution
</p>
<p>We present here two function subprograms for computing the binomial distri-
bution (5.1.3)
</p>
<p>Wnk =
(
n
</p>
<p>k
</p>
<p>)
pk(1&minus;p)n&minus;k (C.1.1)
</p>
<p>and the distribution function
</p>
<p>P (k &lt; K)=
K&minus;1&sum;
</p>
<p>k=0
W nk (C.1.2)
</p>
<p>C.2 Hypergeometric Distribution
</p>
<p>The hypergeometric distribution (5.3.1)
</p>
<p>Wk =
(
K
</p>
<p>k
</p>
<p>)(
N&minus;K
n&minus; k
</p>
<p>)/(
N
</p>
<p>n
</p>
<p>)
, n&le;N,k &le;K , (C.2.1)
</p>
<p>and the corresponding distribution function
</p>
<p>P (k &lt; k&prime;)=
k&prime;&minus;1&sum;
</p>
<p>k=0
Wk (C.2.2)
</p>
<p>S. Brandt, Data Analysis: Statistical and Computational Methods for Scientists and Engineers,
DOI 10.1007/978-3-319-03762-2, &copy; Springer International Publishing Switzerland 2014
</p>
<p>409
</p>
<p>are computed by the methods StatFunct.binomial and StatFunct.-
</p>
<p>the logarithm of Euler&rsquo;s gamma function is used in the computation.
</p>
<p>cumulativeBinomial, respectively. For reasons of numerical stability
</p>
<p>are computed by StatFunct.hypergeometric and StatFunct.-
</p>
<p>cumulativeHypergeometric, respectively.</p>
<p/>
</div>
<div class="page"><p/>
<p>410 C Formulas and Methods for the Computation of StatisticalFunctions
</p>
<p>C.3 Poisson Distribution
</p>
<p>The Poisson distribution (5.4.1)
</p>
<p>f (k;λ)= λ
k
</p>
<p>k! e
&minus;λ (C.3.1)
</p>
<p>and the corresponding distribution function
</p>
<p>P (k &lt; K)=
K&minus;1&sum;
</p>
<p>k=0
f (k;λ) (C.3.2)
</p>
<p>The quantities f (k;λ) and F(K;λ) depend not only on the values of the
discrete variables k and K , but also on the continuous parameter λ. For a
given P there is a certain parameter value λP that fulfills Eq. (C.3.2). We can
denote this as the quantile
</p>
<p>λ= λP (K) (C.3.3)
</p>
<p>C.4 Normal Distribution
</p>
<p>The probability density of the standard normal distribution is
</p>
<p>φ0(x)=
1&radic;
2π
</p>
<p>exp(&minus;x2/2) . (C.4.1)
</p>
<p>φ(x)= 1&radic;
2πσ
</p>
<p>exp
</p>
<p>(
&minus;(x&minus;x0)
</p>
<p>2
</p>
<p>2σ 2
</p>
<p>)
, (C.4.2)
</p>
<p>can easily be expressed in terms of the standardized variable
</p>
<p>u= x&minus;x0
σ
</p>
<p>(C.4.3)
</p>
<p>using (C.4.1) to be
</p>
<p>φ(x)= 1
σ
φ0(u) . (C.4.4)
</p>
<p>are computed with the methods and StatFunct.-StatFunct.poisson
</p>
<p>cumulative.Poisson respectively.
</p>
<p>of the Poisson distribution. It is computed by the method StatFunct.-
</p>
<p>quantilePoisson.
</p>
<p>It is computed by the method
</p>
<p>The normal distribution with mean x0 and variance σ
2,
</p>
<p>StatFunc.standardNormal.</p>
<p/>
</div>
<div class="page"><p/>
<p>C.4 Normal Distribution 411
</p>
<p>ψ0(x)=
&int; x
</p>
<p>&minus;&infin;
φ0(x)dx =
</p>
<p>1&radic;
2π
</p>
<p>&int; x
</p>
<p>&minus;&infin;
exp
</p>
<p>(
&minus;x
</p>
<p>2
</p>
<p>2
</p>
<p>)
dx (C.4.5)
</p>
<p>is an integral that cannot be computed in closed form. We can relate it, how-
ever, to the incomplete gamma function described in Sect. D.5.
</p>
<p>The distribution function of a normal distribution with mean x0 and vari-
ance σ 2 is found from (C.4.5) to be
</p>
<p>ψ(x)= ψ0(u) , u=
x&minus;x0
σ
</p>
<p>. (C.4.6)
</p>
<p>We now introduce the error function
</p>
<p>erf(x)= 2&radic;
π
</p>
<p>&int; x
</p>
<p>0
e&minus;t
</p>
<p>2
dt , x &ge; 0 . (C.4.7)
</p>
<p>Comparing with the definition of the incomplete gamma function (D.5.1)
gives
</p>
<p>erf(x)= 2
Γ (12)
</p>
<p>&int; t=x
</p>
<p>t=0
e&minus;t
</p>
<p>2
dt = 1
</p>
<p>Γ (12)
</p>
<p>&int; u=x2
</p>
<p>u=0
e&minus;uu&minus;1/2 du ,
</p>
<p>erf(x)= P
(
</p>
<p>1
</p>
<p>2
,x2
</p>
<p>)
. (C.4.8)
</p>
<p>On the other hand there is a more direct connection between (C.4.6) and
(C.4.7),
</p>
<p>ψ
x0=0,σ=1/
</p>
<p>&radic;
2(x)= ψ0(u=
</p>
<p>&radic;
2x)= 1
</p>
<p>2
[1+ sign(x)erf(|x|)]
</p>
<p>or
</p>
<p>ψ0(u)=
1
</p>
<p>2
</p>
<p>[
1+ sign(u)erf
</p>
<p>( |u|&radic;
2
</p>
<p>)]
,
</p>
<p>i.e.,
</p>
<p>ψ0(u)=
1
</p>
<p>2
</p>
<p>[
1+ sign(u)P
</p>
<p>(
1
</p>
<p>2
,
u2
</p>
<p>2
</p>
<p>)]
. (C.4.9)
</p>
<p>) and
(C.4.5), respectively.
</p>
<p>Finally we compute the quantiles of the standard normal distribution (us-
</p>
<p>It is computed by the method
</p>
<p>The distribution function of the standard normal distribution
</p>
<p>StatFunct.normal.
</p>
<p>The methods StatFunct.cumulativeStandardNormal and Stat-
</p>
<p>Funct.cumulativeNormal yield the distribution functions (C.4.4
</p>
<p>ing the method
</p>
<p>probability P , the quantile xp is defined by the relation
</p>
<p>StatFunct.quantileStandardnormal). For a given</p>
<p/>
</div>
<div class="page"><p/>
<p>412 C Formulas and Methods for the Computation of StatisticalFunctions
</p>
<p>P = ψ0(xp)=
1&radic;
2π
</p>
<p>&int; xp
&minus;&infin;
</p>
<p>e&minus;x
2/2 dx . (C.4.10)
</p>
<p>We determine this by finding the zero of the function
</p>
<p>k(x,P )= P &minus;ψ0(x) (C.4.11)
</p>
<p>using the procedure of Sect. E.2.
For the quantile xP for a probability P of the normal distribution with
</p>
<p>P = ψ0(uP ) , xP = x0 +σuP . (C.4.12)
</p>
<p>C.5 χ2-Distribution
</p>
<p>The probability density (6.6.10) of the χ2-distribution for n degrees of free-
dom,
</p>
<p>f (χ2)= 1
2λΓ (λ)
</p>
<p>(χ2)λ&minus;1e&minus;
1
2χ
</p>
<p>2
, λ= n
</p>
<p>2
, (C.5.1)
</p>
<p>F(χ2) = 1
Γ (λ)
</p>
<p>&int; u=χ2
</p>
<p>u=0
</p>
<p>1
</p>
<p>2
</p>
<p>(u
2
</p>
<p>)λ&minus;1
e&minus;
</p>
<p>1
2udu
</p>
<p>= 1
Γ (λ)
</p>
<p>&int; t=χ2/2
</p>
<p>t=0
e&minus;t tλ&minus;1 dt (C.5.2)
</p>
<p>is seen from (D.5.1) to be an incomplete gamma function
</p>
<p>F(χ2)= P
(
λ,
</p>
<p>χ2
</p>
<p>2
</p>
<p>)
= P
</p>
<p>(
n
</p>
<p>2
,
χ2
</p>
<p>2
</p>
<p>)
(C.5.3)
</p>
<p>h(χ2P )= P &minus;F(χ2P )= 0 , (C.5.4)
</p>
<p>mean x0 and standard deviation σ (computed with StatFunct.quan-
</p>
<p>tileNormal) one has
</p>
<p>and computed by
</p>
<p>The quantile χ2P of the χ
2-distribution for a given probability P , which
</p>
<p>is given by
</p>
<p>StatFunct.cumulativeChiSquared.
</p>
<p>is computed as the zero of the function h(χ ) with StatFunct.quan-
</p>
<p>tileChiSquared
</p>
<p>is computed by the method
</p>
<p>The distribution function
</p>
<p>StatFunct.chiSquared.</p>
<p/>
</div>
<div class="page"><p/>
<p>C.7 t-Distribution 413
</p>
<p>C.6 F -Distribution
</p>
<p>The probability density (8.2.3) of the F -distribution with f1 and f2 degrees
of freedom,
</p>
<p>f (F )=
(
f1
</p>
<p>f2
</p>
<p>) 1
2f1 Γ (12(f1 +f2))
</p>
<p>Γ (12f1)Γ (
1
2f2)
</p>
<p>F
1
2f1&minus;1
</p>
<p>(
1+ f1
</p>
<p>f2
F
</p>
<p>)&minus; 12 (f1+f2)
, (C.6.1)
</p>
<p>.
</p>
<p>F(F)=
Γ (12(f1 +f2))
Γ (12f1)Γ (
</p>
<p>1
2f2)
</p>
<p>(
f1
</p>
<p>f2
</p>
<p>) 1
2f1
</p>
<p>&int; F
</p>
<p>0
F
</p>
<p>1
2f1&minus;1
</p>
<p>(
1+ f1
</p>
<p>f2
F
</p>
<p>)&minus; 12 (f1+f2)
dF
</p>
<p>(C.6.2)
can be rearranged using
</p>
<p>t = f2
f2 +f1F
</p>
<p>, |dt | = f1f2
(f2 +f1F)2
</p>
<p>|dF |
</p>
<p>to be
</p>
<p>F(F) = 1
B(12f1,
</p>
<p>1
2f2)
</p>
<p>&int; t=1
</p>
<p>t= f2
f2+f1F
</p>
<p>(1&minus; t) 12f1&minus;1t 12f2&minus;1 dt (C.6.3)
</p>
<p>= 1&minus; If2/(f2+f1F)
(1
</p>
<p>2f2,
1
2f1
</p>
<p>)
,
</p>
<p>i.e., it is related to the incomplete beta function; cf. (D.6.1). We compute it
</p>
<p>h(F )= P &minus;F(F) . (C.6.4)
</p>
<p>C.7 t-Distribution
</p>
<p>The probability density (8.3.7) of Student&rsquo;s t-distribution with f degrees of
freedom,
</p>
<p>f (t) =
Γ (12(f +1))
</p>
<p>Γ (12f )Γ (
1
2)
&radic;
f
</p>
<p>(
1+ t
</p>
<p>2
</p>
<p>f
</p>
<p>)&minus; 12 (f+1)
(C.7.1)
</p>
<p>= 1
B(12 ,
</p>
<p>f
2 )
&radic;
f
</p>
<p>(
1+ t
</p>
<p>2
</p>
<p>f
</p>
<p>)&minus; 12 (f+1)
,
</p>
<p>is computed with
</p>
<p>The distribution function
</p>
<p>StatFunct.fDistribution.
</p>
<p>with the method
</p>
<p>The quantile FP of the F -distribution for a given probability P is given
</p>
<p>by the zero of the function
</p>
<p>StatFunct.cumulativeFDistribution.
</p>
<p>It is computed by StatFunct.quantileFDistribution.</p>
<p/>
</div>
<div class="page"><p/>
<p>414 C Formulas and Methods for the Computation of StatisticalFunctions
</p>
<p>By using the substitution
</p>
<p>u= f
f + t2 ,
</p>
<p>the distribution function of the t-distribution can be expressed in terms of the
incomplete beta function (D.6.1),
</p>
<p>F(t) = 1
B(12 ,
</p>
<p>f
2 )
&radic;
f
</p>
<p>&int; t
</p>
<p>&minus;&infin;
</p>
<p>(
1+ t
</p>
<p>2
</p>
<p>f
</p>
<p>)&minus; 12 (f+1)
dt (C.7.2)
</p>
<p>= 1
2
+ sign(t)
</p>
<p>B(12 ,
f
2 )
&radic;
f
</p>
<p>&int; |t |
</p>
<p>0
</p>
<p>(
1+ t
</p>
<p>2
</p>
<p>f
</p>
<p>)&minus; 12 (f+1)
dt
</p>
<p>= 1
2
+ sign(t)
</p>
<p>B(12 ,
f
2 )
&radic;
f
</p>
<p>1
</p>
<p>2
</p>
<p>&int; u=1
</p>
<p>u=f/(f+t2)
u
</p>
<p>f
2 &minus;1(1&minus;u) 12 du ,
</p>
<p>F (t) = 1
2
</p>
<p>{
1+ sign(t)
</p>
<p>[
1&minus; If/(f+t2)
</p>
<p>(
f
</p>
<p>2
,
</p>
<p>1
</p>
<p>2
</p>
<p>)]}
. (C.7.3)
</p>
<p>h(t)= P &minus;F(t) (C.7.4)
</p>
<p>is computed with by StatFunct.Student.
</p>
<p>It is computed by the method
</p>
<p>The quantile tP of the t-distribution for a given probability P is computed
</p>
<p>by finding the zero of the function
</p>
<p>StatFunct.cumulativeStudent.
</p>
<p>with the method StatFunct.quantileStudent.
</p>
<p>C.8 Java Class and Example Program
</p>
<p>Java Class for the Computation of Statistical Functions
</p>
<p>Example Program C.1: The class FunctionsDemo demonstrates all
</p>
<p>methods mentioned in this Appendix
</p>
<p>The user first selects a family of functions and then a function from that family.
</p>
<p>Next the parameters, needed in the chosen case, are entered. After the Go button is
</p>
<p>clicked, the function value is computed and displayed.
</p>
<p>StatFunct contains all methods mentioned in this Appendix.</p>
<p/>
</div>
<div class="page"><p/>
<p>D. The Gamma Function and Related
</p>
<p>Functions: Methods and Programs
</p>
<p>for Their Computation
</p>
<p>D.1 The Euler Gamma Function
</p>
<p>Consider a real number x with x+1&gt; 0. We define the Euler gamma function
by
</p>
<p>Γ (x+1)=
&int; &infin;
</p>
<p>0
txe&minus;t dt . (D.1.1)
</p>
<p>Integrating by parts gives
&int; &infin;
</p>
<p>0
txe&minus;t dt = [&minus;txe&minus;t ]&infin;0 +x
</p>
<p>&int; &infin;
</p>
<p>0
tx&minus;1e&minus;t dt = x
</p>
<p>&int; &infin;
</p>
<p>0
tx&minus;1e&minus;t dt .
</p>
<p>Thus one has the relation
</p>
<p>Γ (x+1)= xΓ (x) . (D.1.2)
This is the so-called recurrence relation of the gamma function. From (D.1.1)
it follows immediately that
</p>
<p>Γ (1)= 1 .
With (D.1.2) one then has generally that
</p>
<p>Γ (n+1)= n! , n= 1,2, . . . . (D.1.3)
We now substitute t by 12u
</p>
<p>2 (and dt by udu) and get
</p>
<p>Γ (x+1)=
(1
</p>
<p>2
</p>
<p>)x &int; &infin;
</p>
<p>0
u2x+1e&minus;
</p>
<p>1
2u
</p>
<p>2
du .
</p>
<p>S. Brandt, Data Analysis: Statistical and Computational Methods for Scientists and Engineers,
DOI 10.1007/978-3-319-03762-2, &copy; Springer International Publishing Switzerland 2014
</p>
<p>415</p>
<p/>
</div>
<div class="page"><p/>
<p>416 D The Gamma Function and Related Functions
</p>
<p>If we now choose in particular x =&minus;12 , we obtain
</p>
<p>Γ (12)=
&radic;
</p>
<p>2
&int; &infin;
</p>
<p>0
e&minus;
</p>
<p>1
2u
</p>
<p>2
du= 1&radic;
</p>
<p>2
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
e&minus;
</p>
<p>1
2u
</p>
<p>2
du . (D.1.4)
</p>
<p>The integral can be evaluated in the following way. We consider
</p>
<p>A=
&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
e&minus;
</p>
<p>1
2 (x
</p>
<p>2+y2) dx dy =
&int; &infin;
</p>
<p>&minus;&infin;
e&minus;
</p>
<p>1
2x
</p>
<p>2
dx
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
e&minus;
</p>
<p>1
2y
</p>
<p>2
dy = 2{Γ (12)}
</p>
<p>2 .
</p>
<p>The integral A can transformed into polar coordinates:
</p>
<p>A=
&int; 2π
</p>
<p>0
</p>
<p>&int; &infin;
</p>
<p>0
e&minus;
</p>
<p>1
2 r
</p>
<p>2
r dr dφ =
</p>
<p>&int; 2π
</p>
<p>0
dφ
</p>
<p>&int; &infin;
</p>
<p>0
e&minus;
</p>
<p>1
2 r
</p>
<p>2
r dr = 2πΓ (1)= 2π .
</p>
<p>Setting the two expressions for A equal gives
</p>
<p>Γ (12)=
&radic;
π . (D.1.5)
</p>
<p>Using (D.1.2) we can thus determine the value of the gamma function for
half-integral arguments.
</p>
<p>For arguments that are not positive integers or half-integers, the integral
(D.1.1) cannot be evaluated in closed form. In such cases one must rely on
approximations. We discuss here the approximation of LANCZOS [17], which
is based on the analytic properties of the gamma function. We first extend
the definition of the gamma function to negative arguments by means of the
reflection formula
</p>
<p>Γ (1&minus;x)= π
Γ (x)sin(πx)
</p>
<p>= πx
Γ (1+x)sin(πx) . (D.1.6)
</p>
<p>(By relations (D.1.1) and (D.1.6) the gamma function is also defined for an
arbitrary complex argument if x is complex.) One sees immediately that the
gamma function has poles at zero and at all negative integer values. The
approximation of LANCZOS [17],
</p>
<p>Γ (x+1)=
&radic;
</p>
<p>2π
</p>
<p>(
x+γ + 1
</p>
<p>2
</p>
<p>)x+ 12
exp
</p>
<p>(
&minus;x&minus;γ &minus; 1
</p>
<p>2
</p>
<p>)
(Aγ (x)+ ε) ,
</p>
<p>(D.1.7)
takes into account the first few of these poles by the form of the function Aγ ,
</p>
<p>Aγ (x)= c0 +
c1
</p>
<p>x+1 +
c2
</p>
<p>x+2 +&middot;&middot; &middot;+
cγ+1
</p>
<p>x+γ +1 . (D.1.8)</p>
<p/>
</div>
<div class="page"><p/>
<p>D.1 The Euler Gamma Function 417
</p>
<p>For γ = 5 one has for the error ε the approximation |ε| &lt; 2 &middot; 10&minus;10 for all
</p>
<p>Fig.D.1: The functions Γ (x) and 1/Γ (x).
</p>
<p>The gamma function is plotted in Fig. D.1. For large positive arguments
the gamma function grows so quickly that it is difficult to represent its value
in a computer. In many expressions, however, there appear ratios of gamma
functions which have values in a an unproblematic region. In such cases it is
better to use the logarithm of the gamma function which is computed by the
</p>
<p>points x in the right half of the complex plane. The method Gamma.gamma
</p>
<p>yields Euler&rsquo;s gamma function.
</p>
<p>method Gamma.logGamma.</p>
<p/>
</div>
<div class="page"><p/>
<p>418 D The Gamma Function and Related Functions
</p>
<p>D.2 Factorial and Binomial Coefficients
</p>
<p>The expression
n! = 1 &middot;2 &middot; &middot; &middot;n (D.2.1)
</p>
<p>can either be directly computed as a product or as a gamma function using
(D.1.3).
</p>
<p>When computing binomial coefficients,
(
n
</p>
<p>k
</p>
<p>)
= n!
</p>
<p>k!(n&minus; k)! =
n
</p>
<p>k
&middot; n&minus;1
k&minus;1 &middot; &middot; &middot;
</p>
<p>n&minus; k+1
1
</p>
<p>, (D.2.2)
</p>
<p>the expression on the right-hand side is preferable for numerical reasons to
.
</p>
<p>D.3 Beta Function
</p>
<p>The beta function has two arguments and is defined by
</p>
<p>B(z,w)=
&int; 1
</p>
<p>0
tz&minus;1(1&minus; t)w&minus;1 dt . (D.3.1)
</p>
<p>The integral can be written in a simple way in terms of gamma functions,
</p>
<p>B(z,w)= B(w,z)= Γ (z)Γ (w)
Γ (z+w) . (D.3.2)
</p>
<p>D.2
shows it as a function of w for several fixed values of z.
</p>
<p>D.4 Computing Continued Fractions
</p>
<p>In the next two sections there appear continued fractions, i.e., expressions of
the type
</p>
<p>f = b0 +
a1
</p>
<p>b1 +
a2
</p>
<p>b2 + a3b3+&middot;&middot;&middot;
</p>
<p>, (D.4.1)
</p>
<p>that can also be written in the typographically simpler form
</p>
<p>f = b0 +
a1
</p>
<p>b1+
a2
</p>
<p>b2+
a3
</p>
<p>b3+
&middot;&middot; &middot; . (D.4.2)
</p>
<p>If we denote by fn the value of the fraction (D.4.1) truncated after a finite
number of terms up to the coefficients an and bn, then one has
</p>
<p>the expression in the middle and used in the method Gamma.Binomial.
</p>
<p>In this way the method Gamma.beta computes the beta function. Figure</p>
<p/>
</div>
<div class="page"><p/>
<p>D.4 Computing Continued Fractions 419
</p>
<p>Fig.D.2: The beta function. For increasing z the curves B(z,w) are shifted to the left.
</p>
<p>fn =
An
</p>
<p>Bn
. (D.4.3)
</p>
<p>The quantities An and Bn can be obtained from the following recursion rela-
tion,
</p>
<p>A&minus;1 = 1 , B&minus;1 = 0 , A0 = b0 , B0 = 1 , (D.4.4)
</p>
<p>Aj = bj Aj&minus;1 +aj Aj&minus;2 , (D.4.5)
</p>
<p>Bj = bj Bj&minus;1 +aj Bj&minus;2 . (D.4.6)
</p>
<p>Since the relations (D.4.5) and (D.4.6) are linear in Aj&minus;1,Aj&minus;2 and Bj&minus;1,
Bj&minus;2, respectively, and since in (D.4.3) only the ratio An/Bn appears, one
can always multiply the coefficients Aj ,Aj&minus;1,Aj&minus;2 and Bj ,Bj&minus;1,Bj&minus;2 by
an arbitrary normalization factor. One usually chooses for this factor 1/Bj
and avoids in this way numerical difficulties from very large or very small
numbers, which would otherwise occur in the course of the recursion. For
steps in which Bj = 0, the normalization is not done.
</p>
<p>Continued fractions, in a way similar to series expansions, appear in ap-
proximations of certain functions. In a region where the approximation for
the continued fraction converges, the values of fn&minus;1 and fn for sufficiently</p>
<p/>
</div>
<div class="page"><p/>
<p>420 D The Gamma Function and Related Functions
</p>
<p>large n do not differ much. One can therefore use the following truncation
criterion. If for a given ε ≪ 1 the inequality
</p>
<p>∣∣∣∣
fn&minus;fn&minus;1
</p>
<p>fn
</p>
<p>∣∣∣∣&lt; ε
</p>
<p>holds, then fn is a sufficiently good approximation of f .
</p>
<p>D.5 Incomplete Gamma Function
</p>
<p>The incomplete gamma function is defined for a &gt; 0 by the expression
</p>
<p>P (a,x)= 1
Γ (a)
</p>
<p>&int; x
</p>
<p>0
e&minus;t ta&minus;1 dt . (D.5.1)
</p>
<p>It can be expressed as a series expansion
</p>
<p>P (a,x)= xae&minus;x
&infin;&sum;
</p>
<p>n=0
</p>
<p>xn
</p>
<p>Γ (a+n+1) =
1
</p>
<p>Γ (a)
xae&minus;x
</p>
<p>&infin;&sum;
</p>
<p>n=0
</p>
<p>Γ (a)
</p>
<p>Γ (a+n+1)x
n .
</p>
<p>(D.5.2)
The sum converges quickly for x &lt; a+ 1. One uses the right-hand, not the
middle form of (D.5.2), since the ratio of the two gamma functions reduces to
</p>
<p>Γ (a)
</p>
<p>Γ (a+n+1) =
1
</p>
<p>a
</p>
<p>1
</p>
<p>a+1 &middot; &middot; &middot;
1
</p>
<p>a+n+1 .
</p>
<p>In the region x &gt; a+1 we use the continued fraction
</p>
<p>1&minus;P (a,x)= 1
Γ (a)
</p>
<p>e&minus;xxa
(
</p>
<p>1
</p>
<p>x+
1&minus;a
1+
</p>
<p>1
</p>
<p>x+
2&minus;a
1+
</p>
<p>2
</p>
<p>x+ &middot;&middot; &middot;
)
</p>
<p>. (D.5.3)
</p>
<p>function. It is shown in Fig. D.3 for several values of a. From the figure one
sees immediately that
</p>
<p>P (a,0) = 0 , (D.5.4)
lim
x&rarr;&infin;
</p>
<p>P (a,x) = 1 . (D.5.5)
</p>
<p>D.6 Incomplete Beta Function
</p>
<p>The incomplete beta function is defined for a &gt; 0, b &gt; 0 by the relation
</p>
<p>The method Gamma Incompletegamma yields the incomplete gamma</p>
<p/>
</div>
<div class="page"><p/>
<p>D.6 Incomplete Beta Function 421
</p>
<p>Fig.D.3: The incomplete gamma function. With increasing a the graphs P(a,x) move to the
right.
</p>
<p>Ix(a,b)=
1
</p>
<p>B(a,b)
</p>
<p>&int; x
</p>
<p>0
ta&minus;1(1&minus; t)b&minus;1 dt , x &le; 0 &le; 1 . (D.6.1)
</p>
<p>The function obeys the symmetry relation
</p>
<p>Ix(a,b)= 1&minus; I1&minus;x(b,a) . (D.6.2)
</p>
<p>The expression (D.6.1) can be approximated by the following continued frac-
tion:
</p>
<p>Ix(a,b)=
xa(1&minus;x)b
aB(a,b)
</p>
<p>{
1
</p>
<p>1+
d1
</p>
<p>1+
d2
</p>
<p>1+ &middot;&middot; &middot;
}
</p>
<p>(D.6.3)
</p>
<p>with
</p>
<p>d2m+1 = &minus;
(a+m)(a+b+m)
(a+2m)(a+2m&minus;1) x ,
</p>
<p>d2m =
m(b&minus;m)
</p>
<p>(a+2m&minus;1)(a+2m) x .
</p>
<p>The approximation converges quickly for
</p>
<p>x &gt;
a+1
</p>
<p>a+b+1 . (D.6.4)</p>
<p/>
</div>
<div class="page"><p/>
<p>422 D The Gamma Function and Related Functions
</p>
<p>If this requirement is not fulfilled, then 1&minus; x is greater than the right-hand
side of (D.6.4). In this case one computes I1&minus;x as a continued fraction and
then uses (D.6.2).
</p>
<p>function. In Fig. D.4 it is s displayed for various values of the parameters a
and b. Regardless of these parameters one has
</p>
<p>I0(a,b)= 0 , I1(a,b)= 1 . (D.6.5)
</p>
<p>D.7 Java Class and Example Program
</p>
<p>Java Class for for the Computation of the Gamma Function and Related
</p>
<p>Functions
</p>
<p>Fig.D.4: The incomplete beta function. With increasing a the curves Ix(a,b) move further
to the right.
</p>
<p>only the methods of Appendix C but also all methods mentioned in the
present Appendix
</p>
<p>The method Gamma.incompleteBezta computes the incomplete beta
</p>
<p>Example Program D.1: The class FunctionsDemo demonstrates not
</p>
<p>Gamma contains all methods mentioned in this Appendix.</p>
<p/>
</div>
<div class="page"><p/>
<p>D.7 Java Class and Example Program 423
</p>
<p>The user first selects a family of functions and the a function from that family.
Next the parameters, needed in the chosen case, are entered. After the Go button
is clicked, the function value is computed and displayed.</p>
<p/>
</div>
<div class="page"><p/>
<p>E. Utility Programs
</p>
<p>E.1 Numerical Differentiation
</p>
<p>The derivative df (x)/dx of a function f (x) at the point x is given by the limit
</p>
<p>f &prime;(x)= lim
h&rarr;0
</p>
<p>f (x+h)&minus;f (x)
h
</p>
<p>.
</p>
<p>Obviously one can approximate f &prime;(x) by
</p>
<p>f (x+h)&minus;f (x)
h
</p>
<p>for a small finite value of h. In fact, for this the symmetrical difference ratio
</p>
<p>δ(h) = f (x+h)&minus;f (x&minus;h)
2h
</p>
<p>(E.1.1)
</p>
<p>is more appropriate. This can be seen from the Taylor expansions at the point
x for f (x+h) and f (x&minus;h), which give
</p>
<p>δ(h)= f &prime;(x)+ h
2
</p>
<p>3! f
&prime;&prime;&prime;(x)+ h
</p>
<p>4
</p>
<p>5! f
(5)(x)+&middot;&middot; &middot; ,
</p>
<p>in which the leading additional term is already quadratic in h. Nevertheless,
the choice of h is still critical, since for very small values of h there occur
large rounding errors, and for larger values the approximation may not be
valid.
</p>
<p>One can compute δ(h) for a monotonic sequence of values h= h0,h1,h2,
. . .. If the sequence δ(h0), δ(h1), . . . is monotonic (rising or falling) then this
is a sign of convergence of the series to f &prime;(x). According to RUTISHAUSER
[27], from the series δ(h0), δ(h1) one can also obtain others that converge
more quickly. The method was modeled after the ROMBERG procedure [28]
for numerical integration. Starting from h0 = a one first chooses the sequence
</p>
<p>S. Brandt, Data Analysis: Statistical and Computational Methods for Scientists and Engineers,
DOI 10.1007/978-3-319-03762-2, &copy; Springer International Publishing Switzerland 2014
</p>
<p>425</p>
<p/>
</div>
<div class="page"><p/>
<p>426 E Utility Programs
</p>
<p>h0, h1, . . .= a, 3a/4, a/2, 3a/8, a/4, . . . ,
sets
</p>
<p>T
(k)
</p>
<p>0 = δ(hk) ,
and computes the additional quantities T (k)m
</p>
<p>T
(k)
m =
</p>
<p>2mT (k+1)m&minus;1 &minus;1.125T
(k)
m&minus;1
</p>
<p>2m&minus;1.125 ; m odd, k even ,
</p>
<p>T
(k)
m =
</p>
<p>2m &middot;1.125T (k+1)m&minus;1 &minus;T
(k)
m&minus;1
</p>
<p>2m &middot;1.125&minus;1 ; m odd, k odd ,
</p>
<p>T
(k)
m =
</p>
<p>2mT (k+1)m&minus;1 &minus;Tm&minus;1(k)
2m&minus;1 ; m even .
</p>
<p>Arranging the quantities T (k)m in the form of a triangle,
</p>
<p>T
(0)
</p>
<p>0
</p>
<p>T
(0)
</p>
<p>1
</p>
<p>T
(1)
</p>
<p>0 T
(0)
</p>
<p>2
</p>
<p>T
(1)
</p>
<p>1 T
(0)
</p>
<p>3
</p>
<p>T
(2)
</p>
<p>0 T
(1)
</p>
<p>2
</p>
<p>T
(2)
</p>
<p>1
</p>
<p>T
(3)
</p>
<p>0
...
</p>
<p>the first column contains the sequence of our original difference ratios. Not
only does T (k)0 converge to f
</p>
<p>&prime;(x), but one has in general
</p>
<p>lim
k&rarr;&infin;
</p>
<p>T (k)m = f &prime;(x) , limm&rarr;&infin;T
(k)
m = f &prime;(x) .
</p>
<p>The practical significance of the procedure is based on the fact that the
columns on the right converge particularly quickly.
</p>
<p>T
(0)
</p>
<p>0 , . . ., T
(9)
0 is computed starting
</p>
<p>from a = 1. If it is not monotonic, then a is replaced by a/10 and a new
sequence is computed. After 10 tries without success the procedure is ter-
minated. If, however, a monotonic sequence is found, the triangle scheme is
computed and T (0)9 is given as the best approximation for f
</p>
<p>&prime;(x). The class
AuxDer is similar to the program of KOELBIG [29] with the exception of
minor changes in the termination criteria.
</p>
<p>This program requires considerable computing time. For well behaved
functions it is often sufficient to replace the differential ratio by the difference
ratio (E.1.1). To compute second derivatives the procedure of difference ra-
</p>
<p>In the class AuxDer, the sequence
</p>
<p>tios is extended correspondingly. The classes AuxDri, AuxGrad and Aux-
</p>
<p>Hesse therefore operate on the basis of difference ratios.</p>
<p/>
</div>
<div class="page"><p/>
<p>E.3 Interactive Input and Output Under Java 427
</p>
<p>E.2 Numerical Determination of Zeros
</p>
<p>Computing the quantile xP of a distribution function F(x) for a given proba-
bility P is equivalent to determining the zero of the function
</p>
<p>k(x)= P &minus;F(x) . (E.2.1)
</p>
<p>We treat the problem in two steps. In the first step we determine an interval
(x0,x1) that contains the zero. In the second step we systematically reduce
the interval such that its size becomes smaller than a given value ε.
</p>
<p>In the first step we make use of the fact that k(x) is monotonic, since f (x)
is monotonic. We begin with initial values for x0 and x1. If f (x0) &middot;f (x1) &lt; 0,
i.e., if the function values have different signs, then the zero is contained
within the interval. If this is not the case, then we enlarge the interval in the
direction where the function has the smallest absolute value, and repeat the
procedure with the new values of (x0,x1).
</p>
<p>For localizing the zero within the initial interval (x0,x1) we use a compar-
atively slow but absolutely reliable procedure. The original interval is divided
in half and replaced by the half for which the end points are of the opposite
sign. The procedure is repeated until the interval width decreases below a
given value.
</p>
<p>, see also Example Program 7.1.
</p>
<p>E.3 Interactive Input and Output Under Java
</p>
<p>Java programs usually are event driven, i.e., while running they react to ac-
tions by the user. Thus an interaction between user and program is enabled.
Its detailed design depends on the problem at hand and also on the user&rsquo;s taste.
For our Example Programs four utility programs may suffice to establish sim-
ple interactions. They are explained in Fig. E.1.
</p>
<p>the left. In Fig. E.1 these are an input group, a radio-button group and a go
</p>
<p>This technique is implemented in the class
</p>
<p>in several methods for the computation of quantiles in
</p>
<p>way, i.e., without a call to
</p>
<p>AuxZero. It is also employed
</p>
<p>StatFunct in a direct
</p>
<p>AuxZero. An example for the application of
</p>
<p>AuxZero is the class E1MaxLike
</p>
<p>It shows a screen window produced by the class
</p>
<p>simplest form it consists only of a frame, a title line (here &lsquo;Example for the
</p>
<p>creation of random numbers&rsquo;) and an output region, into which the user&rsquo;s out-
</p>
<p>put can be written. The method
</p>
<p>elements below the title line which will be arranged horizontally starting from
</p>
<p>DatanFrame. In its
</p>
<p>DatanFra.add allows to add additional
</p>
<p>button. The input group is created by the class AuxJInputGroup and the
</p>
<p>radio-button group by AuxJRButtonGroup. Both (as well as Datan-
</p>
<p>Fra) make use of the standard Java Swing classes. The go button is</p>
<p/>
</div>
<div class="page"><p/>
<p>428 E Utility Programs
</p>
<p>Fig.E.1:A window of the type DatanFrame with elements for interactive input, for starting
the program, and for alphanumeric output of results.
</p>
<p>E.4 Java Classes
</p>
<p>directly created by a standard class. The input group itself is composed of
</p>
<p>an arbitrary number of number-input regions, arranged vertically one below
</p>
<p>the other, which are created by the class
</p>
<p>usage of these classes is summarized in the online documentation. we also
</p>
<p>recommend to study the source code of some of our Example Programs.
</p>
<p>AuxJNumberInput. The detailed
</p>
<p>A of derivatives required by LsqNon and
</p>
<p>put and output.
</p>
<p>AuxDer computes the derivative of a function using the Rutishauser method.
</p>
<p>AuxDri computes the matrix
</p>
<p>LsqMar.
</p>
<p>AuxGrad computes the gradient of a function at a given point.
</p>
<p>AuxHesse computes the Hessian matrix of a function at a given point.
</p>
<p>AuxZero finds the zero of a monotonic function.
</p>
<p>DatanFrame creates a screen window with possibilities for interactive in-</p>
<p/>
</div>
<div class="page"><p/>
<p>E.4 Java Classes 429
</p>
<p>window.
</p>
<p>AuxJInputGroup creates an input group within a screen window.
</p>
<p>AuxJInputNumber creates a number-input region within an input group.
</p>
<p>AuxJRButtonGroup creates a radio-button group within a screen</p>
<p/>
</div>
<div class="page"><p/>
<p>F. The Graphics Class DatanGraphics
</p>
<p>F.1 Introductory Remarks
</p>
<p>The graphical display of data and of curves of fitted functions has always
</p>
<p>plete graphical structures; they are listed at the beginning of Sect. F.8.
</p>
<p>F.2 Graphical Workstations: Control Routines
</p>
<p>As mentioned, a plot can be output either as in the form of a screen window
or as a file in postscript format. The latter is easily embedded in digital doc-
uments or directly printed on paper, if necessary after conversion to another
format such as pdf, by the use of a freely available program. For histori-
cal reasons we call both the screen window and the postscript file a graphics
workstation.
</p>
<p>S. Brandt, Data Analysis: Statistical and Computational Methods for Scientists and Engineers,
DOI 10.1007/978-3-319-03762-2, &copy; Springer International Publishing Switzerland 2014
</p>
<p>431
</p>
<p>been an important aid in data analysis. Here we present the class Datan-
</p>
<p>and/or postscript files. We distinguish control, transformation, drawing, and
</p>
<p>auxilliary methods. All will be described in detail in this Appendix and there
</p>
<p>usage will be explained in a number of Example Programs. For many pur-
</p>
<p>poses, however, it is sufficient to use one of only five classes which, in turn,
</p>
<p>resort to
</p>
<p>Graphics comprising methods which produce graphics in screen windows
</p>
<p>DatanGraphics. These classes, by a single call, produce com-
</p>
<p>The method
</p>
<p>window or a file or both, i.e., it initializes buffers into which information is
</p>
<p>written by methods mentioned later. Only after the method DatanGra-
</p>
<p>the screen and/or is the postscript file made available. In this way several
</p>
<p>graphics can be produced one after another. There windows can coexist on
</p>
<p>DatanGraphics.openWorkstation&ldquo;opens&rdquo; a screen
</p>
<p>phics.closeWorstation has been called, is the window presented on</p>
<p/>
</div>
<div class="page"><p/>
<p>432 F The Graphics Class DatanGraphics
</p>
<p>the screen. They be changed in size using the computer mouse; but their
contents is not alterable.
</p>
<p>F.3 Coordinate Systems, Transformations
</p>
<p>and Transformation Methods
</p>
<p>F.3.1 Coordinate Systems
</p>
<p>World Coordinates (WC)
</p>
<p>Figure F.1 shows a plot made by
</p>
<p>coordinates of a point in the WC are denoted by (X,Y ).
</p>
<p>moment that all of the graphical structures, including text, physically exist,
</p>
<p>e.g., that they are made out of bent wire. The coordinate system in which this
</p>
<p>wire structure is described is called the world coordinate system (WC). The
</p>
<p>DatanGraphics. Let us imagine for a
</p>
<p>Fig.F.1: Simple example of a plot produced with DatanGraphics.</p>
<p/>
</div>
<div class="page"><p/>
<p>F.3 Coordinate Systems, Transformations and Transformation Methods 433
</p>
<p>Computing Coordinates (CC)
</p>
<p>If we consider the axes in Fig. F.1, we note that the axes designated with x
and y have about the same length in world coordinates, but have very different
lengths in terms of the numbers shown. The figure shows a plot of a function
</p>
<p>y = f (x) .
</p>
<p>Each point (x,y) appears at the point (X,Y ). We call the coordinate system of
the (x,y) points the computing coordinate system (CC). The transformation
between WC and CC is given by Eq. (F.3.1).
</p>
<p>Device Coordinates (DC)
</p>
<p>From the (fictitious) world coordinate system, the plot must be brought onto
the working surface of a graphics device (terminal screen or paper). We call
the coordinates (u,v) on this surface the device coordinates (DC).
</p>
<p>F.3.2 Linear Transformations: Window &ndash; Viewport
</p>
<p>The concepts defined in this section and the individual transformations are
illustrated in Fig. F.2.
</p>
<p>Let us assume that the computing coordinates in x cover the range
</p>
<p>xa &le; x &le; xb .
</p>
<p>The corresponding range in world coordinates is
</p>
<p>Xa &le;X &le; Xb .
</p>
<p>A linear transformation x &rarr;X is therefore defined by
</p>
<p>X =Xa+ (x&minus;xa)
Xb&minus;Xa
xb&minus;xa
</p>
<p>. (F.3.1)
</p>
<p>The transformation for y &rarr; Y is defined in a corresponding way. One speaks
of the mapping of the window in computing coordinates CC,
</p>
<p>xa &le; x &le; xb , ya &le; y &le; yb , (F.3.2)
</p>
<p>onto the viewport in world coordinates WC,
</p>
<p>Xa &le;X &le; Xb , Ya &le; Y &le; Yb . (F.3.3)</p>
<p/>
</div>
<div class="page"><p/>
<p>434 F The Graphics Class DatanGraphics
</p>
<p>Fig.F.2: The various coordinate systems. Above: window in computing coordinates. Middle:
viewport (small rectangle) and window (large rectangle) in world coordinates. Below: pre-
liminary viewport (dashed rectangle), final adjusted viewport (small rectangle), and border
of the display surface (large rectangle) in device coordinates. The mappings from comput-
ing coordinates to world coordinates and from world coordinates to device coordinates are
indicated by dotted lines.
</p>
<p>The mapping is in general distorted, i.e., a square in CC becomes a rectangle
in WC. It is undistorted only if the aspect ratios of the window and viewport
are equal,
</p>
<p>xb&minus;xa
yb&minus;ya
</p>
<p>= Xb&minus;Xa
Yb&minus;Ya
</p>
<p>. (F.3.4)</p>
<p/>
</div>
<div class="page"><p/>
<p>F.4 Transformation Methods 435
</p>
<p>Next a mapping onto the physically implemented device coordinates DC
must be done. It can of course be defined by again providing a window (in
WC) and a viewport (in DC). One wants, however, to avoid an additional
distortion. We define, therefore, a viewport
</p>
<p>ua &le; u&le; ub , va &le; v &le; vb (F.3.5)
</p>
<p>and a window
X&prime;a &le;X &le; X&prime;b , Y &prime;a &le; Y &le; Y &prime;b . (F.3.6)
</p>
<p>The mapping from the window (F.3.6) onto the viewport (F.3.5) is only car-
ried out if both have the same aspect ratio. Otherwise the viewport (F.3.6) is
reduced symmetrically in width to the right and left or in height symmetrically
above and below such that the viewport
</p>
<p>u&prime;a &le; u&le; u&prime;b , v&prime;a &le; v &le; v&prime;b (F.3.7)
</p>
<p>has the same aspect ratio as the window (F.3.6). In this way a distortion free
mapping is defined between the two.
</p>
<p>F.4 Transformation Methods
</p>
<p>The user of DatanGraphics computes values first in CC that are to
</p>
<p>be plotted. By specifying the window and viewport he or she defines the
</p>
<p>mapping into WC. A possible linear distortion is perfectly acceptable in this
</p>
<p>step, since it provides a simple means of changing the scale.
</p>
<p>The user of
</p>
<p>various coordinate systems by calling the appropriate routines. The trans-
</p>
<p>formations are then applied when drawing a graphical structure without any
</p>
<p>further intervention.
</p>
<p>Transformation CC &rarr; WC
</p>
<p>This transformation is defined by calling the following two methods. Datan
</p>
<p>in computing coodinates, Datan
</p>
<p>Transformation WC &rarr; DC
</p>
<p>This transformation is defined by calling the following two methods. Datan-
</p>
<p>DatanGraphics must define the transformations between the
</p>
<p>Graphics.setWindowInComputingCoordinates sets the window
</p>
<p>Coordinates sets the viewport in world coordinates.
</p>
<p>Graphics.setWindowInWorldCoordinates sets the window</p>
<p/>
</div>
<div class="page"><p/>
<p>436 F The Graphics Class DatanGraphics
</p>
<p>data points, and contour lines (Sect. F.5) the clipping region is the window
</p>
<p>in world coordinates. DatanGraphics.setFormat defines the tempo-
</p>
<p>rary viewport in device coordinates. Into that the final view port is fitted so
</p>
<p>that the width-to-height ratio is the same as for the window in world coordi-
</p>
<p>nates. If
</p>
<p>taken to be A5 landscape. If the workstation is a screen window, then only
</p>
<p>the width-to-height ratio is taken into account. In the case of a postscript file
</p>
<p>the absolute size in centimeters is valid only if a plot of that size will fit on
</p>
<p>the paper in the printer. Otherwise the plot is demagnified until it just fits.
</p>
<p>In both cases the plot is centered on the paper. A call to the method Datan-
</p>
<p>gram about the paper size. If it is not called, then A4 is assumed with a
</p>
<p>margin of 5 mm on all 4 sides.
</p>
<p>In most cases, having defined the transformations in this way, the user
</p>
<p>will be interested only in computing coordinates.
</p>
<p>Clipping
</p>
<p>The graphical structures are not completely drawn under certain circum-
</p>
<p>stances. They are truncated if they extend past the boundary of the so-called
</p>
<p>clipping region. The structures are said to be clipped. For polylines, markers,
</p>
<p>DatanGraphics.setFormat is not called at all, the format is
</p>
<p>Graphics.setStandardPaperSizeAndBorders informs the pro-
</p>
<p>in computing coordinates; for text and graphics utility structures the clipping
</p>
<p>region is the window in world coordinates. These regions can can be set
</p>
<p>explicitely using
</p>
<p>F.5 Drawing Methods
</p>
<p>Colors and Line Widths
</p>
<p>The methods mentioned up to this point carry out organizational tasks, but
</p>
<p>do not, however, produce any graphical structures on the workstation. All
</p>
<p>of the graphical structures created by
</p>
<p>The lines possess a given color and width. The selection of these two at-
</p>
<p>tributes is done as follows. A pair of properties (color, linewidth) is assigned
</p>
<p>to each of set of 8 color indices. The set of 8 is different for screen window
</p>
<p>and postscript file. With the method
</p>
<p>user selects one particular color index. That then is valid until another color
</p>
<p>index is chosen. The user may assign his own choice of color and line width
</p>
<p>to a color index by the methods DatanGraphics.setScreenColor
</p>
<p>and/or
</p>
<p>DatanGraphics.setSmallClippingWindow and
</p>
<p>DatanGraphics.setBigClippingWindow, respectively.
</p>
<p>DatanGraphics consist of lines.
</p>
<p>DatanGraphics.chooseColor the
</p>
<p>DatanGraphics.setPSColor For the background of the screen</p>
<p/>
</div>
<div class="page"><p/>
<p>F.5 Drawing Methods 437
</p>
<p>Fig.F.3: Polymarkers.
</p>
<p>window (the standard is blue) another color can be chosen with Datan-
</p>
<p>ground is always transparent.
</p>
<p>Polylines
</p>
<p>The concepts of a polyline and polymarker have been introduced for par-
</p>
<p>ticularly simple graphics structures. A polyline defines a sequence of line
</p>
<p>segments from the point (x1,y1) through the points (x2,y2), (x3,y3), . . . to
</p>
<p>the point (x ,y )n n . A polyline is drwan with the method DatanGraph-
</p>
<p>A polymarker marks a plotting point with a graphical symbol. The poly-
</p>
<p>markers available in
</p>
<p>Graphics.setScreenBackground. For the postscript file the back-
</p>
<p>ics.Polyline.
</p>
<p>DatanGraphics are shown in Fig. F.3.
</p>
<p>One can clearly achieve an arbitrarily good approximation of any graph-
</p>
<p>ical structure by means of polylines. For example, graphs of functions can be
</p>
<p>displayed with polylines as long as the individual points are sufficiently close
</p>
<p>to each other.
</p>
<p>Sometimes one wants to draw a polyline not as a solid line but rather
</p>
<p>dashed, dotted, or dot-dashed. That is achieved by the method Datan-
</p>
<p>Polymarkers are especially suitable for marking data points. If a data
</p>
<p>point has error bars, then one would like to indicate these errors in one or
</p>
<p>both coordinates by means of error bars. In certain circumstances one would
</p>
<p>even like to show the complete covariance ellipse. This task is performed by
</p>
<p>the method
</p>
<p>Fig. F.4.
</p>
<p>An error bar in the x direction is only drawn if σx &gt; 0, and in the y
</p>
<p>direction only if σy &gt; 0. The covariance ellipse is only draw if σx &gt; 0, σy &gt; 0,
</p>
<p>and cov(x,y) �= 0. Error bars are not drawn if they would lie completely
</p>
<p>Graphics.drawBrokenPolyline.
</p>
<p>DatanGraphics.drawDatapoint. Examples are shown in</p>
<p/>
</div>
<div class="page"><p/>
<p>438 F The Graphics Class DatanGraphics
</p>
<p>Fig.F.4: Example for plotting data points.
</p>
<p>within the polymarker itself. If part of the structure falls outside of the CC
window (F.3.2), then this part is not drawn.
</p>
<p>Histogram
</p>
<p>Contour Lines
</p>
<p>A function f = f (x,y) defines a surface in a three dimensional (x,y,f )
space. One can also get an idea of the function in the (x,y) plane by marking
points for which f (x,y) is equal to a given constant c. The set of all such
points forms the contour line f (x,y) = c. By drawing a set of such contour
lines f (x,y) = c1,c2, . . . one obtains (as with a good topographical map) a
rather good impression of the function.
</p>
<p>Naturally it is impossible to compute the function for all points in the
(x,y) plane. We restrict ourselves to a rectangular region in the (x,y) plane,
usually the window in computing coordinates, and we break it into a total of
</p>
<p>The method
</p>
<p>of a histogram.
</p>
<p>DatanGraphics.drawHistogram displays data in the form</p>
<p/>
</div>
<div class="page"><p/>
<p>F.6 Utility Methods 439
</p>
<p>N = nxny smaller rectangles. The corner points of these smaller rectangles
have x coordinates that are neighboring values in the sequence
</p>
<p>x0,x0 +Δx,x0+2Δx,. . . ,x0 +nxΔx .
The y coordinates of the corner points are adjacent points of the sequence
</p>
<p>y0,y0 +Δy,y0+2Δy,. . . ,y0 +nyΔy .
In each rectangle the contour line is approximated linearly. To do this one
considers the function f (x,y)&minus; c at the four corner points. If the function is
of a different sign at two corner points that are the end points of an edge of
the rectangle, then it is assumed that the contour lines intersect the edge. The
intersection point is computed with linear interpolation. If the intersection
points are on two edges of the rectangle, then they are joined by a line seg-
ment. If there are intersection points on more than two edges, then all pairs of
such points are joined by line segments.
</p>
<p>Clearly the approximation of the contour lines by line segments becomes
better for a finer division into small rectangles. With a finer division, of
course, the required computing time also becomes longer.
</p>
<p>in Fig. F.5.
</p>
<p>F.6 Utility Methods
</p>
<p>With the few methods described up to this point, a great variety of complicated
plots can be produced. By using the methods of this section and the next, the
tasks are made easier for the user, since they help to create graphical structures
typically used in conjunction with the plots of data analysis, such as axes,
coordinate crosses, and explanatory text.
</p>
<p>The method
</p>
<p>contour line. An example of a function represented by contour lines is shown
</p>
<p>DatanGraphics.drawContour computes and draws a
</p>
<p>Frames
</p>
<p>The methode
</p>
<p>plotted part of world coordinate system, i.e., the outer frame of the plots repro-
</p>
<p>duced here. The metod
</p>
<p>hand, draws a frame around the window of the cumputing coordinate system.
</p>
<p>Scales
</p>
<p>The method x direction.
</p>
<p>Ticks appear at the upper and lower edge of the CC window pointing to the
</p>
<p>DataGrpahics.drawFrame draws a frame around the
</p>
<p>DatanGraphics.drawBoundary, on the other
</p>
<p>DatanGraphics.drawScaleX draws a scale in</p>
<p/>
</div>
<div class="page"><p/>
<p>440 F The Graphics Class DatanGraphics
</p>
<p>Fig.F.5: Contour lines f (x,y) =&minus;0.9,&minus;0.8, . . . ,0.8,0.9 of the function f (x)= sin(x+y)
cos([x&minus;y]/2) in the (x,y) plane.
</p>
<p>inside of the window. Below the lower edge numbers appear, marking some
</p>
<p>of these ticks. It is recommended to first call the method DatanGraph-
</p>
<p>an arrow with text can be drawn, showing in the direction of increasing x
</p>
<p>values. The method
</p>
<p>tasks for an axis in y direction.
</p>
<p>The creation of axis divisions and labels with these methods is usually
</p>
<p>done automatically without intervention of the user. Sometimes, however, the
</p>
<p>user will want to influence these operations. This can be done by using the
</p>
<p>method
</p>
<p>method influences only that scale which is generated by the very next call of
</p>
<p>respectively.
</p>
<p>Coordinate Cross
</p>
<p>The method draws a coor-
</p>
<p>dinate cross in the computing coordinate system. The axes of that system
</p>
<p>appear as broken lines inside the CC window.
</p>
<p>ics.drawBoundary, to mark the edges themselves by lines. In addition
</p>
<p>DatanGraphics.drawScaleY performs analogous
</p>
<p>DatanGraphics.setParametersForScale. A call to this
</p>
<p>DatanGraphics.drawScaleX or DatanGraphics.drawScaleY,
</p>
<p>DatanGraphics.drawCoordinateCross</p>
<p/>
</div>
<div class="page"><p/>
<p>F.8 Java Classes and Example Programs 441
</p>
<p>F.7 Text Within the Plot
</p>
<p>Explanatory text makes plots easier to understand. The methods in this section
</p>
<p>create text superimposed on a plot which can be placed at any location.
</p>
<p>The text must be supplied by the user as a character string. Before this
</p>
<p>text is translated into graphics characters, however, it must first be encoded.
</p>
<p>The simple encoding system used here allows the user to display simple math-
</p>
<p>ematical formulas. For this there are three character sets: Roman, Greek, and
</p>
<p>mathematics, as shown in Table F.1. The character set is selected by control
</p>
<p>characters. These are the special characters
</p>
<p>@ for Roman,
&amp; for Greek,
% for mathematics.
</p>
<p>A control character in the text string causes all of the following characters
</p>
<p>to be produced with the corresponding character set, until another control
</p>
<p>symbol appears. The default character set is Roman.
</p>
<p>In addition there exist the following positioning symbols:
</p>
<p>^ for superscript (exponent),
_ for subscript (index),
# for normal height,
" for backspace.
</p>
<p>All characters appear at normal height as long as no positioning symbol has
</p>
<p>appeared. One can move a maximum of two steps from normal height, e.g.,
</p>
<p>Aαβ ,Aαβ . The positioning symbols ˆ and _ remain in effect until the appear-
ance of a #. The symbol " acts only on the character following it. This then
appears over the previous character instead of after it. In this way one obtains,
</p>
<p>e.g., A
β
α instead of A
</p>
<p>β
α .
</p>
<p>The method draws a caption, cen-
</p>
<p>tered slightly below the upper edge of the plotted section of the world coordi-
</p>
<p>nate system.
</p>
<p>Sometimes the user wants to write text at a certain place in the plot, e.g.,
</p>
<p>next to an individual curve or data point, and also to choose the text size. This
</p>
<p>is made possible by the method .
</p>
<p>F.8 Java Classes and Example Programs
</p>
<p>Java Classes Poducing Graphics
</p>
<p>DatanGraphics.drawCaption
</p>
<p>DatanGraphics.drawText
</p>
<p>DatanGraphics contains the methods mentioned in this Appendix.</p>
<p/>
</div>
<div class="page"><p/>
<p>442 F The Graphics Class DatanGraphics
</p>
<p>TableF.1: The various character sets for producing text.
Control characters Control characters
</p>
<p>Roman Greek Math Roman Greek Math
Input @ &amp; % Input @ &amp; %
A A A(ALPHA) &Auml; a a α(alpha) &auml;
B B B(BETA) B b b β(beta) b
C C X(CHI) c c χ(chi) c
D D Δ(DELTA) Δ d d δ(delta) d
E E E(EPSILON) E e e ǫ(epsilon) e
F F Φ(PHI) F f f ϕ(phi) f
G G Γ (GAMMA) �= g g γ (gamma) g
H H H(ETA) H h h η(eta) h
I I I(IOTA)
</p>
<p>&int;
i i ι(iota) i
</p>
<p>J J I(IOTA) J j j ι(iota) j
K K K(KAPPA) K k k κ(kappa) k
L L Λ(LAMBDA) | l l λ(lambda) l
M M M(MU) &plusmn; m m μ(mu) m
N N N(NU) N n n ν(nu) n
O O Ω(OMEGA) &Ouml; o o ω(omega) &ouml;
P P Π(PI) &Ouml; p p π(pi) p
Q Q Θ(THETA) Q q q ϑ(theta) q
R R R(RHO) ◦ r r ρ(rho) r
S S Σ(SIGMA) &szlig; s s σ (sigma) s
T T T(TAU) t t τ (tau) t
U U O(OMICRON) &Uuml; u u o(omicron) &uuml;
V V &Uuml; v v v
W W Ψ (PSI)
</p>
<p>&radic;
w w ψ(psi) w
</p>
<p>X X Ξ (XI) X x x ξ (xi) x
Y Y Υ (UPSILON) &Aring; y y υ(upsilon) y
Z Z Z(ZETA) Z z z ζ (zeta) z
~ &sim; &sim; &sim; &minus; &minus; &minus; &minus;
! ! ! ! = = = &equiv;
$ $ $ $ { { { {
* * # &times; } } } }
( ( &uarr; &larr; | | | |
) ) &darr; &rarr; [ [ &amp; [
+ + + + ] ] @ ]
&lsquo; &lsquo; &lsquo; &lsquo; \
1 1 1 1 : : : :
2 2 2 2 ; ; ; ;
3 3 3 3 &rsquo; &rsquo; &lsquo; &rsquo;
4 4 4 4 &lt; &lt; &sub; &le;
5 5 5 5 &gt; &gt; &sup; &ge;
6 6 6 6 ? ? &sect; &sim;
7 7 7 7 , , , ,
8 8 8 8 . . . .
9 9 9 9 / / \ %
0 0 0 0</p>
<p/>
</div>
<div class="page"><p/>
<p>The program generates the simple plot of Fig. F.1. It opens the workstation and
defines the different coordinate systems. The outer frame is drawn (enclosing the
section of the world coordinate system to be displayed) and the inner frame (the
boudary of the computing coordinate system). Next, the lettered scales for abswcissa
and ordinate are produced as is a caption for the plot. Now, the color index is
changed. In a short loop a total of 201 coordinate pairs (xi,yi) are computed with
xi =&minus;10, &minus;9.9,&minus;9.8, . . . , 10 and yi = f (xi). The function f (x) is the probability
density of the standardized normal distribution. A polyline, defined by these pairs
is drawn. In a second loop the points for a polyline are computed which correspond
to a normal distribution with with mean a = 2 and standard deviation σ = 3. That
polyline is represented as a broken line. Finally, two short straight lines are displayed
in the upper left corner of the plot (one as a solid line and one as a dashed line). To
the right each of these polylines a short text is displayed, indicating the parameters of
the Gaussians displayed as solid and dashed curves, respectively. Before termination
of the program the workstation is closed.
</p>
<p>The short program generates the plot of Fig. F.3, showing the different polymarkers,
</p>
<p>F.8 Java Classes and Example Programs 443
</p>
<p>(an Example Program is
</p>
<p>two-dimensional scatter diagram (an Example Program is
</p>
<p>ics with a histogram and a polyline (an Example Program is
</p>
<p>with data points and one polyline (an Example Program is
</p>
<p>complete plot with data points and several polylines (an Example Pro-
</p>
<p>gram is
</p>
<p>Example Program F.1: The class E1Gr demonstrates the use of the
</p>
<p>following methods of the class
</p>
<p>GraphicsWithHistogram produces a complete plot with a histogram
</p>
<p>E2Sample).
</p>
<p>GraphicsWith2DScatterDiagram produces a complete plot with a
</p>
<p>E3Sample).
</p>
<p>GraphicsWithHistogramAndPolylin eproduces a complete graph-
</p>
<p>E6Gr).
</p>
<p>E7Gr).
</p>
<p>GraphicsWithDataPointsAndPolyline produces a complete plot
</p>
<p>GraphicsWithDataPointAndMultiplePolylines produces a
</p>
<p>E8Gr).
</p>
<p>DatanGraphics:
</p>
<p>openWorkstation, closeWorkstation,
</p>
<p>setWindowInComputingCoordinates,
</p>
<p>setViewportInWorldCoordinates,
</p>
<p>setWindowInWorldCoordinates, setFormat,
</p>
<p>drawFrame, drawBoundary, chooseColor,
</p>
<p>drawPolyline, drawBrokenPolyline, drawScaleX,
</p>
<p>drawScaleY, drawCaption, drawText
</p>
<p>Example Program F.2: The class E2Gr demonstrates the use of the
</p>
<p>method DatanGraphics.drawMark.
</p>
<p>which can be drawn with DatanGraphics.drawMark.</p>
<p/>
</div>
<div class="page"><p/>
<p>444 F The Graphics Class DatanGraphics
</p>
<p>Fig.F.6: Four versions of the same plot with different types of scales.
</p>
<p>The program produces the plot of Fig. F.4, which contains examples for the different
ways to present data points with errors.
</p>
<p>successive calls of of this method in a loop, contours of the function f (x,y) =
sin(x+y)cos((x&minus;y)/2) are drawn. The result is a plot corresponding to Fig. F.5.
</p>
<p>defining the number of contours and the number of intervals in x and y can be set
interactively by the user. Study the changes in the plot resulting from very small
</p>
<p>The program generates the plots shown in Fig. F.6. It contains four plots which differ
only by the design of their scales. The plots are generated in a loop where different
</p>
<p>Example Program F.3: The class E3Gr demonstrates the use of the
</p>
<p>method DatanGraphics.drawDataPoint.
</p>
<p>Example Program F.4: The class E4Gr demonstrates the use of the
</p>
<p>method DatanGrpahics.drawContour
</p>
<p>A window of computing coordinates &minus;π &le; x &le; π , &minus;π &le; y &le; π and a square
viewport in world coordinates are selected. After creating scales and the caption,
</p>
<p>input parameters for DatanGraphics.drawContour are prepared. Next by
</p>
<p>Example Program F.5: The class E5Gr demonstrates the methods
</p>
<p>and
</p>
<p>DatanGraphics.setParametersForSale
</p>
<p>DatanGraphics.drawCoordinateCross
</p>
<p>Suggestions: Extend the program such that the parameters ncontand nstep
</p>
<p>values of nstep.</p>
<p/>
</div>
<div class="page"><p/>
<p>F.8 Java Classes and Example Programs 445
</p>
<p>viewports in world coordinates are chosen in each step, so that the plots correspond
</p>
<p>to the upper-left, upper-right, lower-left, and lower-right quadrant of the window in
</p>
<p>world coordinates. For the upper-left plot the default values for the scale design are
</p>
<p>used. In the upper-right plot the number of ticks and the lettering of the scale is prede-
</p>
<p>fined. In the lower-left plot the size of the symbols used in the lettering of the scales
</p>
<p>is changed. In the lower-right plot the numbers are written in exponential notation.
</p>
<p>All plots contain a coordinate cross, which is generated by calling DatanGraph-
</p>
<p>and a curve corresponding to a Gaussian.
</p>
<p>Example Program F.6: The class E6Gr demonstrates the use of the class
</p>
<p>The program first sets up a histogram which for each bin k contains the Poisson
</p>
<p>probability f (k;λ) for the parameter λ= 10. Next, points on a polyline are computed
corresponding to the probability density of a normal distribution with mean λ and
</p>
<p>variance λ. Finally the text strings for the plot are defined and the complete plot is
</p>
<p>ics.drawCoordinateCross
</p>
<p>GraphicsWithHistogramAndPolyline
</p>
<p>displayed by a call of (Fig. F.7).
</p>
<p>Example Program F.7: The class E7Gr demonstrates the use of the class
</p>
<p>First, by calling
</p>
<p>straight line y = at +b within the simulated errors. Next, the errors to be presented
</p>
<p>GraphicsWithHistogramAndPolyline
</p>
<p>GraphicsWithDataPointsAndPolyline
</p>
<p>DatanRandom.line, data points are generated which lie on a
</p>
<p>Fig.F.7: A plot generated with containing a
</p>
<p>histogram and a polyline.
</p>
<p>GraphicsWithHistogramAndPolyline</p>
<p/>
</div>
<div class="page"><p/>
<p>446 F The Graphics Class DatanGraphics
</p>
<p>Fig.F.8: A plot with data points and a polyline generated by calling GraphicsWith
</p>
<p>.
</p>
<p>in the directions of the horizontal and vertical axes and their covariance are defined.
</p>
<p>The latter two quantities in our example are equal to zero. The polyline defining
</p>
<p>the straight line consists of only two points. Their computation is trivial. After the
</p>
<p>definition of the axis labels and the caption, the plot is displayed by calling Graph-
</p>
<p>(Fig. F.8).
</p>
<p>Example Program F.8: The class E8Gr demonstrates the use of the class
</p>
<p>The program generates 21 data points which lie within the simulated errors on
</p>
<p>a Gaussian curve with zero mean and standard deviation σ = 1, and which span the
abscissa region &minus;3 &le; x &le; 3. Next, points on three polylines are computed corre-
sponding to Gaussian curves with means of zero and standard deviations σ = 0.5,
σ = 1, and σ = 1.5. The polylines span the abscissa region &minus;10 &le; x &le; 10. They are
displayed in different colors. One polyline is shown as a continuous line, the other
</p>
<p>two as dashed lines. Three plots are produced: The first displays only the data points,
</p>
<p>the second only the polylines, and the third shows the data points together with the
</p>
<p>polylines. In this way the automatic choice of the scales in the different cases is
</p>
<p>demonstrated.
</p>
<p>DataPointsAndPolyline
</p>
<p>icsWithDataPointsAndPolyline
</p>
<p>GraphicsWithDataPointsAndMultiplePolylines </p>
<p/>
</div>
<div class="page"><p/>
<p>G. Problems, Hints and Solutions,
</p>
<p>and Programming Problems
</p>
<p>G.1 Problems
</p>
<p>Problem 2.1: Determination of Probabilities
through Symmetry Considerations
</p>
<p>There are n students in a classroom. What is the probability for the fact that at
least two of them have their birthday on the same day? Solve the problem by working
through the following questions:
</p>
<p>(a) What is the number N of possibilities to distribute the n birthdays over the
year (365 days)?
</p>
<p>(b) How large is the number N &prime; of possibilities for which all n birthdays are dif-
ferent?
</p>
<p>(c) How large then is the probability Pdiff that the birthdays are different?
</p>
<p>(d) How large finally is the probability P that at least two birthdays are not dif-
ferent?
</p>
<p>Problem 2.2: Probability for Non-exclusive Events
The probabilities P(A), P(B), and P(AB) �= 0 for non-exclusive events A and B are
given. How large is the probability P(A+B) for the observation of A or B? As an
example compute the probability that a playing card which was drawn at random out
of a deck of 52 cards is either an ace or a diamond.
</p>
<p>Problem 2.3: Dependent and Independent Events
Are the events A and B that a playing card out of a deck is an ace or a diamond
independent
</p>
<p>(a) If an ordinary deck of 52 cards is used,
</p>
<p>(b) If a joker is added to the deck?
</p>
<p>S. Brandt, Data Analysis: Statistical and Computational Methods for Scientists and Engineers,
DOI 10.1007/978-3-319-03762-2, &copy; Springer International Publishing Switzerland 2014
</p>
<p>447</p>
<p/>
</div>
<div class="page"><p/>
<p>448 G Exercises, Hints and Solutions, Programming Problems
</p>
<p>Problem 2.4: Complementary Events
Show that Ā and B̄ are independent if A and B are independent. Use the result of
Problem 2.2 to express P(AB) by P(A), P(B), and P(AB).
</p>
<p>Problem 2.5: Probabilities Drawn from Large and Small Populations
A container holds a large number (&gt;1000) of coins. They are divided into three types
A, B, and C, which make up 20, 30, and 50% of the total.
</p>
<p>(a) What are the probabilities P(A), P(B), P(C) of picking a coin of type A,
B, or C if one coin is taken at random? What are the probabilities P(AB),
P(AC), P(BC), P(AA), P(BB), P(CC), P (2 identical coins), P (2 different
coins) for picking 2 coins?
</p>
<p>(b) What are the probabilities if 10 coins (2 of type A, 3 of type B, and 5 of type
C) are in the container?
</p>
<p>Problem 3.1: Mean, Variance, and Skewness of a Discrete Distribution
The throwing of a die yields as possible results xi = 1,2, . . . ,6. For an ideally sym-
metric die one has pi =P(xi)= 1/6, i = 1,2, . . . ,6. Determine the expectation value
x̂, the variance σ 2(x)= μ2, and the skewness γ of the distribution,
</p>
<p>(a) For an ideally symmetric die,
</p>
<p>(b) For a die with
</p>
<p>p1 =
1
</p>
<p>6
, p2 =
</p>
<p>1
</p>
<p>12
, p3 =
</p>
<p>1
</p>
<p>12
,
</p>
<p>p4 =
1
</p>
<p>6
, p5 =
</p>
<p>3
</p>
<p>12
, p6 =
</p>
<p>3
</p>
<p>12
.
</p>
<p>Problem 3.2: Mean, Mode, Median, and Variance
of a Continuous Distribution
</p>
<p>Consider the probability density f (x) of a triangular distribution of the form shown
in Fig. G.1, given by
</p>
<p>f (x) = 0 , x &lt; a , x &ge; b ,
</p>
<p>f (x) = 2
(b&minus;a)(c&minus;a) (x&minus;a) , a &le; x &lt; c ,
</p>
<p>f (x) = 2
(b&minus;a)(b&minus; c) (b&minus;x) , c &le; x &lt; b .
</p>
<p>Determine the mean x̂, the mode xm, the median x0.5, and the variance σ 2 of the
distribution. For simplicity choose c = 0 (which corresponds to the substitution of x
by x&prime; = x&minus; c). Give explicit results for the symmetric case a =&minus;b and for the case
a =&minus;2b.</p>
<p/>
</div>
<div class="page"><p/>
<p>G.1 Problems 449
</p>
<p>Fig.G.1: Triangular probability density.
</p>
<p>Problem 3.3: Transformation of a Single Variable
</p>
<p>In Appendix D it is shown that
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
exp(&minus;x2/2)dx =
</p>
<p>&radic;
2π .
</p>
<p>Use the transformation y = x/σ to show that
&int; &infin;
</p>
<p>&minus;&infin;
exp(&minus;x2/2σ 2)dx = σ
</p>
<p>&radic;
2π .
</p>
<p>Problem 3.4: Transformation of Several Variables
</p>
<p>A &ldquo;normal distribution of two variables&rdquo; (see Sect. 5.10) can take the form
</p>
<p>f (x,y) = 1
2πσxσy
</p>
<p>exp
</p>
<p>(
&minus;1
</p>
<p>2
</p>
<p>x2
</p>
<p>σ 2x
&minus; 1
</p>
<p>2
</p>
<p>y2
</p>
<p>σ 2y
</p>
<p>)
.
</p>
<p>(a) Determine the marginal probability densities f (x), f (y) by using the results
of Problem 3.3.
</p>
<p>(b) Are x and y independent?</p>
<p/>
</div>
<div class="page"><p/>
<p>450 G Exercises, Hints and Solutions, Programming Problems
</p>
<p>(c) Transform the distribution f (x,y) to the variables
</p>
<p>u= x cosφ+y sinφ , v = y cosφ&minus;x sinφ .
</p>
<p>(The u, v coordinate system has the same origin as the x, y coordinate system,
but it is rotated with respect to the latter by an angle φ.)
Hint: Show that the transformation is orthogonal and use (3.8.12).
</p>
<p>(d) Show that u and v are independent variables only if φ = 0◦, 90◦, 180◦, 270◦
or if σx = σy = σ .
</p>
<p>(e) Consider the case σx = σy = σ , i.e.,
</p>
<p>f (x)= 1
2πσ 2
</p>
<p>exp
</p>
<p>[
&minus; 1
</p>
<p>2σ 2
(x2 +y2)
</p>
<p>]
.
</p>
<p>Transform the distribution to polar coordinates r, φ, determine the marginal
probability densities g(r) and g(φ) and show that r and φ are independent.
</p>
<p>Problem 3.5: Error Propagation
</p>
<p>The period T of a pendulum is given by T = 2π
&radic;
ℓ/g. Here ℓ is the length of
</p>
<p>the pendulum and g is the gravitational acceleration. Compute g and Δg using the
measured values ℓ = 99.8 cm, Δℓ = 0.3 cm, T = 2.03 s, ΔT = 0.05 s and assuming
that the measurements of ℓ and T are uncorrelated.
</p>
<p>Problem 3.6: Covariance and Correlation
</p>
<p>We denote the mass and the velocity of an object by m and v and their measurement
errors by Δm =
</p>
<p>&radic;
σ 2(m) and Δv =
</p>
<p>&radic;
σ 2(v). The measurements are assumed to be
</p>
<p>independent, i.e., cov(m,v)= 0. Furthermore, the relative errors of measurement are
known, i.e.,
</p>
<p>Δm/m= a , Δv/v = b .
</p>
<p>(a) Consider the momentum p = mv and the kinetic energy E = 12mv2 of the
object and compute σ 2(p), σ 2(E), cov(p,E), and the correlation ρ(p,E).
Discuss ρ(p,E) for the special cases a = 0 and b= 0. Hint: Form the vectors
x = (m,v) and y = (p,E). Then approximate y = y(x) by a linear transfor-
mation and finally compute the covariance matrix.
</p>
<p>(b) For the case where the measured values of E, p and the covariance matrix are
known, compute the mass m and its error by error propagation. Use the results
from (a) to verify your result. Note that you will obtain the correct result only
if cov(p,E) is taken into account in the error propagation.</p>
<p/>
</div>
<div class="page"><p/>
<p>G.1 Problems 451
</p>
<p>Problem 5.1: Binomial Distribution
</p>
<p>(a) Prove the recursion formula
</p>
<p>W nk+1 =
n&minus; k
k+1
</p>
<p>p
</p>
<p>q
W nk .
</p>
<p>(b) It may be known for a certain production process that a fraction q = 0.2 of
all pieces produced are defective. This means that in 5 pieces produced the
expected number of non-defective pieces is np= n(1&minus;q)= 5 &middot;0.8 = 4. What
is the probability P2 and the probability P3 that at most 2 or at most 3 pieces
are free from defects? Use relation (a) to simplify the calculation.
</p>
<p>(c) Determine the value km for which the binomial distribution is maximum, i.e.,
km is the most probable value of the distribution. Hint: Since W nk is not a
function of a continuous variable k, the maximum cannot be found by looking
for a zero in the derivative. Therefore, one has to study finite differences
W nk &minus;W nk&minus;1.
</p>
<p>(d) In Sect. 5.1 the binomial distribution was constructed by considering the ran-
dom variable x =
</p>
<p>&sum;n
i=1 xi . Here xi was a random variable that took only the
</p>
<p>values 0 and 1 with the probabilities P(xi = 1)= p and P(xi = 0)= q.
The binomial distribution
</p>
<p>f (x)= f (k)=W nk =
(
n
</p>
<p>k
</p>
<p>)
pkqn&minus;k
</p>
<p>was then obtained by considering in detail the probability to have k cases of
xi = 1 in a total of n observations of the variable xi . Obtain the binomial
distribution in a more formal way by constructing the characteristic function
ϕxi of the variable xi . From the nth power of ϕxi you obtain the characteristic
function of x. Hint: Use the binomial theorem (B.6).
</p>
<p>Problem 5.2: Poisson Distribution
In a certain hospital the doctor on duty is called on the average three times per night.
The number of calls may be considered to be Poisson distributed. What is the proba-
bility for the doctor to have a completely quiet night?
</p>
<p>Problem 5.3: Normal Distribution
The resistance R of electrical resistors produced by a particular machine may be
described by a normal distribution with mean Rm and standard deviation σ .
</p>
<p>The production cost for one resistor is C, and the price is 5C if R = R0 &plusmn;Δ1,
and 2C if R0 &minus;Δ2 &lt; R &lt; R0 &minus;Δ1 or R0 +Δ1 &lt; R &lt; R0 +Δ2. Resistors outside
these limits cannot be sold.
</p>
<p>(a) Determine the profit P per resistor produced for Rm = R0, Δ1 = a1R0, Δ2 =
a2R0, σ = bR0. Use the distribution function ψ0.</p>
<p/>
</div>
<div class="page"><p/>
<p>452 G Exercises, Hints and Solutions, Programming Problems
</p>
<p>(b) Use Table I.2 to compute the numerical values of P for a1 = 0.01, a2 = 0.05,
b = 0.05.
</p>
<p>(c) Show that the probability density (5.7.1) has points of inflection (i.e., second
derivative equal to zero) at x = a&plusmn;b.
</p>
<p>Problem 5.4: Multivariate Normal Distribution
</p>
<p>A planar xy coordinate system is used as target. The probability to observe a hit in
the plane may be given by the normal distribution of Problem 3.4 (e). Use the result
of that problem to determine
</p>
<p>(a) The probability P(R), to observe a hit within a given radius R around the
origin,
</p>
<p>(b) The radius R, within which a hit is observed with a given probability. Com-
pute as a numerical example the value of R for P = 90% and σ = 1.
</p>
<p>Problem 5.5: Convolution
</p>
<p>(a) Prove the relation (5.11.11). Begin with (5.11.9) and use the expression
(5.11.10) for fy(y). In the intervals 0 &le; u &lt; 1 and 2 &le; u &lt; 3 relations
(5.11.10a) and (5.11.10b) hold, since in these intervals one always has y &lt; 1
and y &gt; 1, respectively. In the interval 1 &le; u &lt; 2 the resulting distribution
f (u) must be constructed as sum of two integrals of the type (5.11.9) of which
each contains one of the possible expression for fy(y). In this case particular
care is necessary in the determination of the limits of integration. They are
given by the limits of the intervals in which u and fy(y) are defined.
</p>
<p>(b) Prove the relation (5.11.15) by performing the integration (5.11.5) for the case
that fx and fy are normal distributions with means 0 and standard deviations
σx and σy .
</p>
<p>Problem 6.1: Efficiency of Estimators
</p>
<p>Let x1,x2,x3 be the elements of a sample from a continuous population with unknown
mean x̂, but known variance σ 2.
</p>
<p>(a) Show that the following quantities are unbiased estimators of x̂,
</p>
<p>S1 = 14 x1 +
1
4 x2 +
</p>
<p>1
2x3 ,
</p>
<p>S2 = 15 x1 +
2
5 x2 +
</p>
<p>2
5x3 ,
</p>
<p>S3 = 16 x1 +
1
3 x2 +
</p>
<p>1
2x3 .
</p>
<p>Hint: It is simple to show that S=
&sum;n
</p>
<p>i=1 aixi is unbiased if
&sum;n
</p>
<p>i=1 ai = 1 holds.</p>
<p/>
</div>
<div class="page"><p/>
<p>G.1 Problems 453
</p>
<p>(b) Determine the variances σ 2(S1), σ 2(S2), σ 2(S3) using (3.8.7) and the as-
sumption that the elements x1, x2, x3 are independent.
</p>
<p>(c) Show that the arithmetic mean x̄ = 13x1 +
1
3 x2 +
</p>
<p>1
3 x3 has the smallest vari-
</p>
<p>ance of all estimators of the type S =
&sum;3
</p>
<p>i=1 aixi fulfilling the requirement&sum;3
i=1 ai = 1.
</p>
<p>Hint: Minimize the variance of S= a1x1+a2x2+(1&minus;a1&minus;a2)x3 with respect
to a1 and a2. Compute this variance and compare it with the variances which
you found (b).
</p>
<p>Problem 6.2: Sample Mean and Sample Variance
</p>
<p>Compute the sample mean x̄, the sample variance s2, and an estimate for the variance
of the sample mean s2x̄ = (1/n)s2 for the following sample:
</p>
<p>18, 21, 23, 19, 20, 21, 20, 19, 20, 17.
</p>
<p>Use the method of Example 6.1.
</p>
<p>Problem 6.3: Samples from a Partitioned Population
</p>
<p>An opinion poll is performed on an upcoming election. In our (artificially con-
structed) example the population is partitioned into three subpopulations, and from
each subpopulation a preliminary sample of size 10 is drawn. Each element of the
sample can have the values 0 (vote for party A) or 1 (vote for party B). The samples
are
</p>
<p>i = 1 (pi = 0.1) : xij = 0,0,0,0,1,0,1,0,0,0,
i = 2 (pi = 0.7) : xij = 0,0,1,1,0,1,0,1,1,0,
i = 3 (pi = 0.2) : xij = 0,1,1,1,1,0,1,1,1,0,
</p>
<p>(a) Use these samples to form an estimator for the result of the election and its
variance s2x̄. Does this result show a clear advantage for one party?
</p>
<p>(b) Use these samples to determine for a much larger sample of size n the sizes
ni of the subsamples in such a way that x̃ has the smallest variance (cf. Exam-
ple 6.6).
</p>
<p>Problem 6.4: χ2-distribution
</p>
<p>(a) Determine the skewness γ =μ3/σ 3 of the χ2-distribution using (5.5.7). Begin
by expressing μ3 by λ3, x̂, and E(x2).
</p>
<p>(b) Show that γ &rarr; 0 for n&rarr;&infin;.</p>
<p/>
</div>
<div class="page"><p/>
<p>454 G Exercises, Hints and Solutions, Programming Problems
</p>
<p>Problem 6.5: Histogram
Construct a histogram from the following measured values:
</p>
<p>26.02, 27.13, 24.78, 26.19, 22.57, 25.16, 24.39, 22.73, 25.35, 26.13,
23.15, 26.29, 24.89, 23.76, 26.04, 22.03, 27.48, 25.42, 24.27, 26.58,
27.17, 24.22, 21.86, 26.53, 27.09, 24.73, 26.12, 28.04, 22.78, 25.36.
</p>
<p>Use a bin size of Δx = 1.
Hint: For each value draw a cross of width Δx and height 1. In this way you do not
need to order the measured values, since each cross is drawn within a bin on top of a
preceding cross. In this way the bars of the histogram grow while drawing.
</p>
<p>Problem 7.1: Maximum-Likelihood Estimates
</p>
<p>(a) Suppose it is known that a random variable x follows the uniform distribution
f (x)= 1/b for 0 &lt; x &lt; b. The parameter b is to be estimated from a sample.
Show that S= b̃= xmax is the maximum-likelihood estimator of b. (Hint: This
result cannot be obtained by differentiation, but from a simple consideration
about the likelihood function).
</p>
<p>(b) Write down the likelihood equations for the two parameters a and Γ of the
Lorentz distribution (see Example 3.5). Show that these do not necessarily
have unique solutions. You can, however, easily convince yourself that for
|x(j)&minus;a| ≪ Γ the arithmetic mean x̄ is an estimator of a.
</p>
<p>Problem 7.2: Information
</p>
<p>(a) Determine the information I (λ) of a sample of size N that was obtained from
a normal distribution of known variance σ 2 but unknown mean λ= a.
</p>
<p>(b) Determine the information I (λ) of a sample of size N which was drawn from
a normal distribution of known mean a but unknown variance λ= σ 2. Show
that the maximum-likelihood estimator of σ 2 is given by
</p>
<p>S = 1
N
</p>
<p>N&sum;
</p>
<p>j=1
(x(j)&minus;a)2
</p>
<p>and that the estimator is unbiased, i.e., B(S)= E(S)&minus;λ= 0.
</p>
<p>Problem 7.3: Variance of an Estimator
</p>
<p>(a) Use the information inequality to obtain a lower limit on the variance of the
estimator of the mean in Problem 7.2 (a). Show that this limit is equal to the
minimum variance that was determined in Problem 6.1 (c).
</p>
<p>(b) Use the information inequality to obtain a lower limit on the variance of S in
Problem 7.2 (b).
</p>
<p>(c) Show using Eq. (7.3.12) that S is a minimum variance estimator with the same
variance the lower limit found in (b).</p>
<p/>
</div>
<div class="page"><p/>
<p>G.1 Problems 455
</p>
<p>Problem 8.1: F -Test
</p>
<p>Two samples are given:
</p>
<p>(1) 21, 19, 14, 27, 25, 23, 22, 18, 21, (N1 = 9) ,
(2) 16, 24, 22, 21, 25, 21, 18, (N2 = 7) .
</p>
<p>Does sample (2) have a smaller variance than sample (1) at a significance level of
α = 5%?
</p>
<p>Problem 8.2: Student&rsquo;s Test
</p>
<p>Test the hypothesis that the 30 measurements of Problem 6.5 were drawn from a
population with mean 25.5. Use a level of significance of α = 10%. Assume that the
population is normally distributed.
</p>
<p>Problem 8.3: χ2-Test for Variance
</p>
<p>Use the likelihood-ratio method to construct a test of the hypothesis H0(σ 2 = σ 20 )
that a sample stems from a normal distribution with unknown mean a and unknown
</p>
<p>variance σ 20 . The parameters are λ = (a,σ ). In ω one has λ̃
(ω) = (x̄,σ0), in Ω:
</p>
<p>λ̄
(Ω) = (x̄,s).
</p>
<p>(a) Form the likelihood ratio T .
</p>
<p>(b) Show that instead of T , the test statistic T &prime; =Ns&prime;2/σ 20 can be used as well.
</p>
<p>(c) Show that T &prime; follows a χ2-distribution with N &minus;1 degrees of freedom so that
the test can be performed with the help of Table I.7.
</p>
<p>Problem 8.4: χ2-Test of Goodness-of-Fit
</p>
<p>(a) Determine the mean ã and the variance σ̃ 2 for the histogram of Fig. 6.1b, i.e.,
</p>
<p>xk 193 195 197 199 201 203 205 207 209 211
nk 1 2 9 12 23 25 11 9 6 2
</p>
<p>Use the result of Example 7.8 to construct the estimates. Give the estimators
explicitly as functions of nk and xk .
</p>
<p>(b) Perform (at a significance level of α = 10%) a χ2-test on the goodness-of-fit
of a normal distribution with mean ã and variance σ̃ 2 to the histogram. Use
only those bins of the histogram for which npk &ge; 4. Determine pk from the
difference of two entries in Table I.2. Give a formula for pk as function of
xk , Δx, ã, σ̃ 2, and ψ0. Construct a table for the computation of χ2 containing
columns for xk, nk, npk, and (nk&minus;npk)2/npk.</p>
<p/>
</div>
<div class="page"><p/>
<p>456 G Exercises, Hints and Solutions, Programming Problems
</p>
<p>Problem 8.5: Contingency Table
</p>
<p>(a) In an immunology experiment [taken from SOKAL and ROHLF, Biometry
(Freeman, San Francisco, 1969)] the effect of an antiserum on a particular
type of bacteria is studied. 57 mice received a certain dose of bacteria and
antiserum, whereas 54 mice received bacteria only. After some time the mice
of both groups were counted and the following contingency table constructed.
</p>
<p>Dead Alive Sum
Bacteria and
antiserum 13 44 57
</p>
<p>Only bacteria 25 29 54
Sum 38 73 Total 111
</p>
<p>Test (at a significance level of α= 10%) the hypothesis that the antiserum has
no influence on the survival probability.
</p>
<p>(b) In the computation of χ2 in (a) you will have noticed that the numerators in
(8.8.1) all have the same value. Show that generally for 2&times; 2 contingency
tables the following relation holds,
</p>
<p>nij &minus;np̃i q̃j = (&minus;1)i+j
1
</p>
<p>n
(n11n22 &minus;n12n21) .
</p>
<p>G.2 Hints and Solutions
</p>
<p>Problem 2.1
</p>
<p>(a) N = 365n .
</p>
<p>(b) N &prime; = 365 &middot;364 &middot; &middot; &middot; &middot; &middot; (365&minus;n+1) .
</p>
<p>(c) Pdiff =N &prime;/N =
364
</p>
<p>365
&middot; &middot; &middot; &middot; &middot; 365&minus;n+1
</p>
<p>365
</p>
<p>=
(
</p>
<p>1&minus; 1
365
</p>
<p>)(
1&minus; 2
</p>
<p>365
</p>
<p>)
&middot; &middot; &middot;
</p>
<p>(
1&minus; n&minus;1
</p>
<p>365
</p>
<p>)
.
</p>
<p>(d) P = 1&minus;Pdiff .
</p>
<p>Putting in numbers one obtains P &asymp; 0.5 for n= 23 and P &asymp; 0.99 for n= 57.
</p>
<p>Problem 2.2
</p>
<p>P(A+B)= P(A)+P(B)&minus;P(AB) ,
</p>
<p>P (ace or diamond) = P(ace)+P(diamond)&minus;P(ace+diamond)
</p>
<p>= 4
52
</p>
<p>+ 13
52
</p>
<p>&minus; 1
52
</p>
<p>= 4
13
</p>
<p>.</p>
<p/>
</div>
<div class="page"><p/>
<p>G.2 Hints and Solutions 457
</p>
<p>Problem 2.3
</p>
<p>(a) P(A)= 4
52
</p>
<p>, P (B)= 13
52
</p>
<p>, P (AB)= 1
52
</p>
<p>, i.e., P(AB)= P(A)P (B) .
</p>
<p>(b) P(A)= 4
53
</p>
<p>, P (B)= 13
53
</p>
<p>, P (AB)= 1
53
</p>
<p>, i.e., P(AB) �= P(A)P (B) .
</p>
<p>Problem 2.4
</p>
<p>P(AB)= 1&minus;P(A+B)= 1&minus;P(A)&minus;P(B)+P(AB) .
</p>
<p>For A and B independent one has P(AB)= P(A)P (B). Therefore,
</p>
<p>P(AB) = 1&minus;P(A)&minus;P(B)+P(A)P (B)= (1&minus;P(A))(1&minus;P(B))
= P(Ā)P (B̄) .
</p>
<p>Problem 2.5
</p>
<p>(a) P(A)= 0.2 , P (B)= 0.3 , P (C)= 0.5 ,
P (AB)= 2 &middot;0.2 &middot;0.3 , P (AC)= 2 &middot;0.2 &middot;0.5 ,
P (BC)= 2 &middot;0.3 &middot;0.5 , P (AA)= 0.22 , P (BB)= 0.32 ,
P (CC)= 0.52 .
</p>
<p>(b) P(A)= 2/10 = 0.2 , P (B)= 3/10 = 0.3 , P (C)= 5/10 = 0.5 ,
</p>
<p>P (AB)= 2
10
</p>
<p>&middot; 3
9
+ 3
</p>
<p>10
&middot; 2
</p>
<p>9
, P (AC)= 2
</p>
<p>10
&middot; 5
</p>
<p>9
+ 5
</p>
<p>10
&middot; 2
</p>
<p>9
,
</p>
<p>P (BC)= 3
10
</p>
<p>&middot; 5
9
+ 5
</p>
<p>10
&middot; 3
</p>
<p>9
,
</p>
<p>P (AA)= 2
10
</p>
<p>&middot; 1
9
, P (BB)= 3
</p>
<p>10
&middot; 2
</p>
<p>9
, P (CC)= 5
</p>
<p>10
&middot; 4
</p>
<p>9
.
</p>
<p>For (a) and (b) it holds that
</p>
<p>P(2 identical coins) = P(AA)+P(BB)+P(CC) ,
P (2 different coins) = P(AB)+P(AC)+P(BC)
</p>
<p>= 1&minus;P(2 identical coins) .</p>
<p/>
</div>
<div class="page"><p/>
<p>458 G Exercises, Hints and Solutions, Programming Problems
</p>
<p>Problem 3.1
</p>
<p>(a)
x̂ =
</p>
<p>6&sum;
</p>
<p>i=1
xipi =
</p>
<p>1
</p>
<p>6
</p>
<p>6&sum;
</p>
<p>i=1
i = 21
</p>
<p>6
= 3.5 ,
</p>
<p>σ 2(x) =
6&sum;
</p>
<p>i=1
(xi &minus; x̂)2pi
</p>
<p>= 1
6
(2.52 +1.52 +0.52 +0.52 +1.52 +2.52)
</p>
<p>= 2
6
(6.25+2.25+0.25) = 2.92 ,
</p>
<p>μ3 =
6&sum;
</p>
<p>i=1
(xi &minus; x̂)3pi =
</p>
<p>1
</p>
<p>6
</p>
<p>6&sum;
</p>
<p>i=1
(i&minus;3.5)3 = 0 ,
</p>
<p>γ = μ3/σ 3 = 0 .
</p>
<p>(b) x̂ = 1
12
</p>
<p>(2+2+3+8+15+18)= 4 ,
</p>
<p>σ 2(x) = 1
12
</p>
<p>(2 &middot;32 +1 &middot;22 +1 &middot;12 +3 &middot;12 +3 &middot;22)= 38
12
</p>
<p>= 3.167 ,
</p>
<p>μ3 =
1
</p>
<p>12
(&minus;2 &middot;33 &minus;1 &middot;23 &minus;1 &middot;13 +3 &middot;13 +3 &middot;23)=&minus;3
</p>
<p>γ = μ3/σ 3 =&minus;3/3.1673/2 =&minus;0.533 .
</p>
<p>Problem 3.2
</p>
<p>For a =&minus;b: x̂ = 0 , x0.5 = 0 , σ 2(x)=
b2
</p>
<p>6
.
</p>
<p>For a=&minus;2b: x̂=&minus;b
3
=&minus;0.33b , x0.5=b(
</p>
<p>&radic;
3&minus;2)=&minus;0.27b , σ 2(x)= 7
</p>
<p>18
b2 .
</p>
<p>Problem 3.3
</p>
<p>g(y)=exp
(
&minus;y
</p>
<p>2
</p>
<p>2
</p>
<p>)
, y(x)= x
</p>
<p>σ
; f (x)=dy(x)
</p>
<p>dx
g(y(x))= 1
</p>
<p>σ
exp
</p>
<p>(
&minus; x
</p>
<p>2
</p>
<p>2σ 2
</p>
<p>)
.
</p>
<p>Problem 3.4
</p>
<p>(a) fx(x)=
1&radic;
</p>
<p>2πσx
exp
</p>
<p>(
&minus;1
</p>
<p>2
</p>
<p>x2
</p>
<p>σ 2x
</p>
<p>)
, fy(y)=
</p>
<p>1&radic;
2πσy
</p>
<p>exp
</p>
<p>(
&minus;1
</p>
<p>2
</p>
<p>y2
</p>
<p>σ 2y
</p>
<p>)
.
</p>
<p>(b) Yes, since (3.4.6) is fulfilled.
</p>
<p>(c) The transformation can be written in the form
</p>
<p>(
u
</p>
<p>v
</p>
<p>)
= R
</p>
<p>(
x
</p>
<p>y
</p>
<p>)
, R =
</p>
<p>(
cosφ sinφ
</p>
<p>&minus;sinφ cosφ
</p>
<p>)
.</p>
<p/>
</div>
<div class="page"><p/>
<p>G.2 Hints and Solutions 459
</p>
<p>It is orthogonal, since RTR = I . Therefore,
</p>
<p>g(u,v) = f (x,y) = 1
2πσxσy
</p>
<p>exp
</p>
<p>(
&minus; x
</p>
<p>2
</p>
<p>2σ 2x
&minus; y
</p>
<p>2
</p>
<p>2σ 2y
</p>
<p>)
</p>
<p>= 1
2πσxσy
</p>
<p>exp
</p>
<p>(
&minus;(ucosφ&minus;v sinφ)
</p>
<p>2
</p>
<p>2σ 2x
&minus; (usinφ+v cosφ)
</p>
<p>2
</p>
<p>2σ 2y
</p>
<p>)
</p>
<p>= 1
2πσxσy
</p>
<p>exp
</p>
<p>(
&minus;u
</p>
<p>2 cos2φ+v2 sin2φ&minus;2uv cosφ sinφ
2σ 2x
</p>
<p>&minus;u
2 sin2φ+v2 cos2φ+2uv cosφ sinφ
</p>
<p>2σ 2y
</p>
<p>)
.
</p>
<p>(d) For φ = 90◦ : cosφ = 0, sinφ = 1 etc. the expression g(u,v) factorizes, i.e.,
g(u,v)= gu(u)gv(v).
</p>
<p>For σx = σy = σ :
</p>
<p>g(u,v)= 1
2πσ 2
</p>
<p>exp
</p>
<p>(
&minus;u
</p>
<p>2 +v2
2σ 2
</p>
<p>)
= g(u)g(v) .
</p>
<p>(e)
</p>
<p>J =
</p>
<p>∣∣∣∣∣∣∣∣∣
</p>
<p>&part;x
</p>
<p>&part;r
</p>
<p>&part;y
</p>
<p>&part;r
</p>
<p>&part;x
</p>
<p>&part;φ
</p>
<p>&part;y
</p>
<p>&part;ϕ
</p>
<p>∣∣∣∣∣∣∣∣∣
=
</p>
<p>∣∣∣∣
cosφ sinφ
</p>
<p>&minus;r sinφ r cosφ
</p>
<p>∣∣∣∣= r ,
</p>
<p>g(r,φ) = rf (x,y) = r
2πσ 2
</p>
<p>exp
</p>
<p>(
&minus;x
</p>
<p>2 +y2
2σ 2
</p>
<p>)
</p>
<p>= r
2πσ 2
</p>
<p>exp
</p>
<p>(
&minus; r
</p>
<p>2
</p>
<p>2σ 2
</p>
<p>)
;
</p>
<p>gr(r) =
&int; 2x
</p>
<p>0
g(r,φ)dφ = r
</p>
<p>σ 2
exp
</p>
<p>(
&minus; r
</p>
<p>2
</p>
<p>2σ 2
</p>
<p>)
,
</p>
<p>gφ(φ) =
1
</p>
<p>2πσ 2
</p>
<p>&int; &infin;
</p>
<p>0
r exp
</p>
<p>(
&minus; r
</p>
<p>2
</p>
<p>2σ 2
</p>
<p>)
dr
</p>
<p>= 1
4πσ 2
</p>
<p>&int; &infin;
</p>
<p>0
exp
</p>
<p>(
&minus; u
</p>
<p>2σ 2
</p>
<p>)
du
</p>
<p>= &minus; 1
2π
</p>
<p>[
exp
</p>
<p>(
&minus; u
</p>
<p>2σ 2
</p>
<p>)]&infin;
0
= 1
</p>
<p>2π
;
</p>
<p>g(r,φ) = gr(r)gφ(φ) ,
</p>
<p>therefore, r and φ are independent.</p>
<p/>
</div>
<div class="page"><p/>
<p>460 G Exercises, Hints and Solutions, Programming Problems
</p>
<p>Problem 3.5
</p>
<p>g = 4π2 ℓ
T 2
</p>
<p>= 4π2 99.8
2.032
</p>
<p>cms&minus;2 = 956.09 cms&minus;2 ,
</p>
<p>&part;g
</p>
<p>&part;ℓ
= 4π
</p>
<p>2
</p>
<p>T 2
= 9.58 s&minus;2 , &part;g
</p>
<p>&part;T
=&minus;8π
</p>
<p>2ℓ
</p>
<p>T 3
=&minus;942 cms&minus;3 ,
</p>
<p>(Δg)2 =
(
&part;g
</p>
<p>&part;ℓ
</p>
<p>)2
(Δℓ)2 +
</p>
<p>(
&part;g
</p>
<p>&part;T
</p>
<p>)2
ΔT 2 = 2226 cm2 s&minus;4 ,
</p>
<p>Δg = 47.19 cms&minus;2 .
Problem 3.6
</p>
<p>(a) y = T x ,
</p>
<p>T =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>&part;y1
</p>
<p>&part;x1
</p>
<p>&part;y1
</p>
<p>&part;x2
</p>
<p>&part;y2
</p>
<p>&part;x1
</p>
<p>&part;y2
</p>
<p>&part;x2
</p>
<p>⎞
⎟⎟⎠=
</p>
<p>⎛
⎜⎜⎝
</p>
<p>&part;p
</p>
<p>&part;m
</p>
<p>&part;p
</p>
<p>&part;v
</p>
<p>&part;E
</p>
<p>&part;m
</p>
<p>&part;E
</p>
<p>&part;v
</p>
<p>⎞
⎟⎟⎠=
</p>
<p>(
v m
</p>
<p>1
2v
</p>
<p>2 mv
</p>
<p>)
,
</p>
<p>Cy = T CxT T =
</p>
<p>=
(
</p>
<p>v m
1
2v
</p>
<p>2 mv
</p>
<p>)(
a2m2 0
</p>
<p>0 b2v2
</p>
<p>)(
v 12v
</p>
<p>2
</p>
<p>m mv
</p>
<p>)
</p>
<p>=
(
</p>
<p>(a2 +b2)m2v2 ( 12a2 +b2)m2v3
</p>
<p>( 12a
2 +b2)m2v3 ( 14a2 +b2)m2v4
</p>
<p>)
,
</p>
<p>ρ(p,E) = cov(p,E)
σ (p)σ (E)
</p>
<p>=
( 12a
</p>
<p>2 +b2)m2v3
&radic;
(a2 +b2)m2v2
</p>
<p>&radic;
( 14a
</p>
<p>2 +b2)m2v4
</p>
<p>=
1
2a
</p>
<p>2 +b2&radic;
(a2 +b2)( 14a2 +b2)
</p>
<p>.
</p>
<p>For a = 0 or b = 0: ρ = 1. (In this case either m or v are completely deter-
mined. Therefore, there is a strict relation between E and p.) If, however, a,
b �= 0, one has ρ �= 1, e.g., for a = b one obtains ρ = 3/
</p>
<p>&radic;
10.
</p>
<p>(b) m= 12p2/E , v = E/2p ,
</p>
<p>Cy =
(
</p>
<p>(a2 +b2)p2 (a2 +2b2)Ep
(a2 +2b2)Ep (a2 +4b2)E2
</p>
<p>)
,
</p>
<p>m = Ty ,
</p>
<p>T =
(
&part;m
</p>
<p>&part;y1
,
&part;m
</p>
<p>&part;y2
</p>
<p>)
=
</p>
<p>(
&part;m
</p>
<p>&part;p
,
&part;m
</p>
<p>&part;E
</p>
<p>)
=
</p>
<p>(
p
</p>
<p>E
,&minus; p
</p>
<p>2
</p>
<p>2E2
</p>
<p>)
,
</p>
<p>Cm = σ 2(m)= T CyT T
</p>
<p>=
(
p
</p>
<p>E
,&minus; p
</p>
<p>2
</p>
<p>2E2
</p>
<p>)(
(a2 +b2)p2 (a2 +2b2)Ep
(a2 +2b2)Ep (a2 +4b2)E2
</p>
<p>)
⎛
⎜⎝
</p>
<p>p
</p>
<p>E
</p>
<p>&minus; p
2
</p>
<p>2E2
</p>
<p>⎞
⎟⎠
</p>
<p>= a2 p
4
</p>
<p>4E2
= a2m2 .</p>
<p/>
</div>
<div class="page"><p/>
<p>G.2 Hints and Solutions 461
</p>
<p>Problem 5.1
</p>
<p>(a) W nk+1 =
(
</p>
<p>n
</p>
<p>k+1
</p>
<p>)
pk+1qn&minus;k&minus;1 = n!
</p>
<p>(k+1)!(n&minus; k&minus;1)!p
kqn&minus;k
</p>
<p>p
</p>
<p>q
</p>
<p>= n!
k!(n&minus; k)!
</p>
<p>n&minus; k
k+1p
</p>
<p>kqn&minus;k
p
</p>
<p>q
=W nk
</p>
<p>n&minus; k
k+1
</p>
<p>p
</p>
<p>q
.
</p>
<p>(b) W 53 =
(
</p>
<p>5
</p>
<p>3
</p>
<p>)
0.83 &middot;0.22 = 10 &middot;0.512 &middot;0.04 = 0.2048 ,
</p>
<p>W 54 = W 33 &middot;
2
</p>
<p>4
&middot; 0.8
</p>
<p>0.2
= 0.2048 &middot;2 = 0.4096 ,
</p>
<p>W 55 = W 54 &middot;
1
</p>
<p>5
&middot; 0.8
</p>
<p>0.2
= 0.4096 &middot;0.8 = 0.32768 ,
</p>
<p>P3 = 1&minus;W 54 &minus;W 55 = 0.26272 ,
</p>
<p>P2 = P3 &minus;W 53 = 0.05792 .
</p>
<p>(c) Using the result of (a) we obtain
</p>
<p>W nk &minus;W nk&minus;1 =W nk&minus;1
(
n&minus; k+1
</p>
<p>k
</p>
<p>p
</p>
<p>q
&minus;1
</p>
<p>)
.
</p>
<p>Thus the probability W nk increases as long as the expression in brackets is
positive, i.e.,
</p>
<p>(n&minus; k+1)p
kq
</p>
<p>&minus;1 &gt; 0 .
</p>
<p>Since k and q are positive we have
</p>
<p>(n&minus; k+1)p &gt; kq = k(1&minus;p) , k &lt; (n+1)p .
</p>
<p>The most probable value km is the largest value of k for which this inequality
holds.
</p>
<p>(d) ϕxi (t)= E{eitxi} = qeit &middot;0 +peit = q+peit ;
</p>
<p>ϕx = (q+peit )n =
m&sum;
</p>
<p>k=0
</p>
<p>(
n
</p>
<p>k
</p>
<p>)
qn&minus;kpkeitk =
</p>
<p>m&sum;
</p>
<p>k=0
f (k)eitk = E{eitk} ,
</p>
<p>f (k)=W nk .
</p>
<p>Problem 5.2
</p>
<p>For λ= 3 one has f (0)= λ
0
</p>
<p>0! e
&minus;λ = e&minus;λ = 0.0498 &asymp; 0.05 = 5%.</p>
<p/>
</div>
<div class="page"><p/>
<p>462 G Exercises, Hints and Solutions, Programming Problems
</p>
<p>Problem 5.3
</p>
<p>(a) The fraction
</p>
<p>fr = 2ψ0
(
(R0 &minus;a2R0)&minus;R0
</p>
<p>bR0
</p>
<p>)
= 2ψ0
</p>
<p>(
&minus;a2
</p>
<p>b
</p>
<p>)
</p>
<p>is rejected, since R &lt;R0&minus;Δ2 or R &gt;R0+Δ2. Correspondingly the fractions
</p>
<p>f2 = 2ψ0
(
&minus;a1
</p>
<p>b
</p>
<p>)
&minus;fr and f5 = 1&minus;f2 &minus;fr
</p>
<p>give prices 2C and 5C, respectively. Therefore,
</p>
<p>P = 2f2C+5f5C&minus;C = C{2f2 +5&minus;5f2 &minus;5fr &minus;1}
= C{4&minus;3f2 &minus;5fr}
= C
</p>
<p>{
4&minus;6ψ0
</p>
<p>(
&minus;a1
</p>
<p>b
</p>
<p>)
+3fr &minus;5fr
</p>
<p>}
</p>
<p>= C
{
</p>
<p>4&minus;6ψ0
(
&minus;a1
</p>
<p>b
</p>
<p>)
&minus;4ψ0
</p>
<p>(
&minus;a2
</p>
<p>b
</p>
<p>)}
.
</p>
<p>(b) ψ0(&minus;0.2)= 0.421 ; ψ0(&minus;1)= 0.159 ,
i.e., P = C{4&minus;2.526&minus;0.636} = 0.838C .
</p>
<p>(c)
d2f
</p>
<p>dx2
= 1&radic;
</p>
<p>2πb3
exp(&minus;(x&minus;a)2/2b2){(x&minus;a)2/b2 &minus;1} = 0
</p>
<p>is fulfilled if the expression in the last set of brackets vanishes.
</p>
<p>Problem 5.4
</p>
<p>(a) P(R) =
&int; R
</p>
<p>0
g(r)dr = 1
</p>
<p>σ 2
</p>
<p>&int; R
</p>
<p>0
re&minus;r
</p>
<p>2/2σ 2 dr
</p>
<p>= 1
2σ 2
</p>
<p>&int; R2
</p>
<p>0
e&minus;u/2σ
</p>
<p>2
du
</p>
<p>=
[
e&minus;u/2σ
</p>
<p>2
]0
R2
</p>
<p>= 1&minus; e&minus;R2/2σ 2 .
</p>
<p>(b) 1&minus;P = exp(&minus;R2/2σ 2) , R2/2σ 2 =&minus; ln(1&minus;P) .
For σ = 1, P = 0.9 one obtains
</p>
<p>R =
&radic;
&minus;2ln 0.1 =
</p>
<p>&radic;
4.61 = 2.15 .
</p>
<p>Problem 5.5
</p>
<p>(a) 0 &le; u &lt; 1 :
f (u)=
</p>
<p>&int; u
</p>
<p>u&minus;1
f1(y)dy =
</p>
<p>&int; u
</p>
<p>0
y dy = 1
</p>
<p>2
u2 .
</p>
<p>1 &le; u &lt; 2 :</p>
<p/>
</div>
<div class="page"><p/>
<p>G.2 Hints and Solutions 463
</p>
<p>f (u) =
&int; u
</p>
<p>u&minus;1
f1(y)dy+
</p>
<p>&int; u
</p>
<p>u&minus;1
f2(y)dy
</p>
<p>=
&int; 1
</p>
<p>u&minus;1
y dy+
</p>
<p>&int; u
</p>
<p>1
(2&minus;y)dy
</p>
<p>= 1
2
(1&minus; (u&minus;1)2)+ 1
</p>
<p>2
(1&minus; (2&minus;u)2)
</p>
<p>= 1
2
(&minus;3+6u&minus;2u2) .
</p>
<p>2 &le; u &lt; 3 :
</p>
<p>f (u)=
&int; u
</p>
<p>u&minus;1
f2(y)dy =
</p>
<p>&int; 2
</p>
<p>u&minus;1
(2&minus;y)dy =&minus;
</p>
<p>&int; 0
</p>
<p>3&minus;u
zdz= 1
</p>
<p>2
(3&minus;u)2 .
</p>
<p>(b) f (u)= 1
2πσxσy
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
exp
</p>
<p>(
&minus; x
</p>
<p>2
</p>
<p>2σ 2x
&minus; (u&minus;x)
</p>
<p>2
</p>
<p>2σ 2y
</p>
<p>)
dx .
</p>
<p>Completing the square yields for the exponent
</p>
<p>&minus; σ
2
</p>
<p>2σ 2x σ
2
y
</p>
<p>{(
x&minus; σ
</p>
<p>2
x
</p>
<p>σ 2
u
</p>
<p>)2
&minus; σ
</p>
<p>4
x
</p>
<p>σ 4
u2 + σ
</p>
<p>2
x
</p>
<p>σ 2
u2
</p>
<p>}
.
</p>
<p>With the change of variables v = (σ/σxσy)(x&minus;σ 2x u/σ 2) we obtain
</p>
<p>f (u) = 1
2πσxσy
</p>
<p>exp
</p>
<p>(
σ 4x &minus;σ 2σ 2x
2σ 2σ 2x σ 2y
</p>
<p>u2
</p>
<p>)
σxσy
</p>
<p>σ
</p>
<p>&times;
&int; &infin;
</p>
<p>&minus;&infin;
exp
</p>
<p>(
&minus;1
</p>
<p>2
v2
)
</p>
<p>dv
</p>
<p>= 1&radic;
2πσ
</p>
<p>exp
</p>
<p>(
&minus; u
</p>
<p>2
</p>
<p>2σ 2
</p>
<p>)
,
</p>
<p>since σ 4x = σ 2x (σ 2 &minus;σ 2y ) .
</p>
<p>Problem 6.1
</p>
<p>(a) E(S)= E{
&sum;
</p>
<p>aixi} =
&sum;
</p>
<p>aiE(xi)= x̂
&sum;
</p>
<p>ai = x̂
if and only if&sum;
</p>
<p>ai = 1 .
</p>
<p>(b) σ 2 =
&sum;( &part;S
</p>
<p>&part;xi
</p>
<p>)2
σ 2 = σ 2
</p>
<p>&sum;
a2i ,
</p>
<p>σ 2(S1) = σ 2
(
</p>
<p>1
</p>
<p>16
+ 1
</p>
<p>16
+ 1
</p>
<p>4
</p>
<p>)
= σ 2 &middot; 3
</p>
<p>8
= 0.375σ 2 ,
</p>
<p>σ 2(S2) = σ 2
(
</p>
<p>1
</p>
<p>25
+ 4
</p>
<p>25
+ 4
</p>
<p>25
</p>
<p>)
= σ 2 &middot; 9
</p>
<p>25
= 0.360σ 2 ,
</p>
<p>σ 2(S3) = σ 2
(
</p>
<p>1
</p>
<p>36
+ 1
</p>
<p>9
+ 1
</p>
<p>4
</p>
<p>)
= σ 2 &middot; 7
</p>
<p>18
= 0.389σ 2 .</p>
<p/>
</div>
<div class="page"><p/>
<p>464 G Exercises, Hints and Solutions, Programming Problems
</p>
<p>(c) σ 2(S) = [a21 +a22 + (1&minus; (a1 +a2))2]σ 2 ,
&part;σ 2(S)
</p>
<p>&part;a1
= [2a1 &minus;2(1&minus; (a1 +a2))]σ 2 = 0 ,
</p>
<p>&part;σ 2(S)
</p>
<p>&part;a2
= [2a2 &minus;2(1&minus; (a1 +a2))]σ 2 = 0 ,
</p>
<p>a1 = 1&minus; (a1 +a2) ,
</p>
<p>a2 = 1&minus; (a1 +a2) , a1 = a2 = 13 ;
</p>
<p>σ 2(S) = 13σ 2 = 0.333σ 2 .
</p>
<p>Problem 6.2
</p>
<p>a = 20 ,
</p>
<p>Δ = 1
n
</p>
<p>&sum;
δi
</p>
<p>= 1
10
</p>
<p>(&minus;2+1+3&minus;1+0+1+0&minus;1+0&minus;3)=&minus;0.2 ,
x̄ = 19.8 ,
</p>
<p>s = 1
9
(1.82 +1.22 +3.22 +0.82 +0.22 +
</p>
<p>+1.22 +0.22 +0.82 +0.22 +2.82)
</p>
<p>= 1
9
&middot;25.60 = 2.84 ,
</p>
<p>s2x̄ = 0.284 . Therefore x̄ = 19.8&plusmn;0.53 .
</p>
<p>Problem 6.3
</p>
<p>(a) x̄1 = 0.2, x̄2 = 0.5, x̄3 = 0.7 ,
</p>
<p>x̃ = 0.02+0.35+0.14 = 0.51 ,
</p>
<p>s21 =
1
</p>
<p>9
(2&minus;10 &middot;0.22)= 0.178 ,
</p>
<p>s22 =
1
</p>
<p>9
(5&minus;10 &middot;0.52)= 0.278 ,
</p>
<p>s23 =
1
</p>
<p>9
(7&minus;10 &middot;0.72)= 0.233 ,
</p>
<p>s2
x̃
</p>
<p>= 0.1
2
</p>
<p>10
0.178+ 0.7
</p>
<p>2
</p>
<p>10
0.278+ 0.2
</p>
<p>2
</p>
<p>10
0.233
</p>
<p>= 0.001 &middot;0.178+0.049 &middot;0.278+0.004 &middot;0.233
</p>
<p>= 0.0147 ,</p>
<p/>
</div>
<div class="page"><p/>
<p>G.2 Hints and Solutions 465
</p>
<p>sx̃ =
&radic;
</p>
<p>s2
x̃
= 0.12 .
</p>
<p>The result x̃ = 0.51&plusmn;0.12 does not favor any one party significantly.
</p>
<p>(b) s1 = 0.422, s2 = 0.527, s3 = 0.483 ,
</p>
<p>p1s1 = 0.0422, p2s2 = 0.369, p3s3 = 0.0966 ,
&sum;
</p>
<p>pisi = 0.508 ,
n1
</p>
<p>n
= 0.083, n2
</p>
<p>n
= 0.726, n3
</p>
<p>n
= 0.190 .
</p>
<p>Problem 6.4
</p>
<p>(a) For simplicity we write χ2 = u.
</p>
<p>μ3 = E{(u&minus; û)3} = E{u3 &minus;3u2û+3uû2 &minus; û3}
= E(u3)&minus;3ûE(u2)+3û2E(u)&minus; û3
</p>
<p>= λ3 &minus;3ûE(u2)+2û3 ,
</p>
<p>E(u3) = λ3 =
1
</p>
<p>i3
ϕ&prime;&prime;&prime;(0)
</p>
<p>= iϕ&prime;&prime;&prime;(0)= i(&minus;λ)(&minus;λ&minus;1)(&minus;λ&minus;2)(&minus;2i)3
</p>
<p>= 8λ(λ+1)(λ+2)= 8λ3 +24λ2 +16λ ,
3ûE(u2) = 6λ(4λ2 +4λ)= 24λ3 +24λ2 ,
</p>
<p>2û3 = 2 &middot; (2λ)3 = 16λ3 ,
μ3 = 16λ ,
γ = μ3/σ 3 = 16λ/8λ
</p>
<p>3
2 = 2λ&minus; 12 .
</p>
<p>(b) Obviously γ = 2/
&radic;
λ&rarr; 0 holds for λ= 12n&rarr;&infin;.
</p>
<p>Problem 6.5
</p>
<p>See Fig. G.2.
</p>
<p>Problem 7.1
</p>
<p>(a) L=
N&prod;
</p>
<p>j=1
</p>
<p>1
</p>
<p>b
= 1
</p>
<p>bN
; obviously one has L= max for b = xmax .
</p>
<p>(b) λ1 = a, λ2 = Γ ,
</p>
<p>L =
N&prod;
</p>
<p>j=1
</p>
<p>2
</p>
<p>πλ2
</p>
<p>λ22
</p>
<p>4(x(j)&minus;λ1)2 +λ22
</p>
<p>=
(
</p>
<p>2λ2
π
</p>
<p>)N N&prod;
</p>
<p>j=1
</p>
<p>1
</p>
<p>4(x(j)&minus;λ1)2 +λ22
,</p>
<p/>
</div>
<div class="page"><p/>
<p>466 G Exercises, Hints and Solutions, Programming Problems
</p>
<p>Fig.G.2: Histogram of the data of Problem 6.5.
</p>
<p>ℓ = N(ln2&minus; lnπ + lnλ2)&minus;
N&sum;
</p>
<p>j=1
ln[4(x(j)&minus;λ1)2 +λ22] ,
</p>
<p>&part;ℓ
</p>
<p>&part;λ1
=
</p>
<p>N&sum;
</p>
<p>j=1
</p>
<p>8(x(i)&minus;λ1)
4(x(j)&minus;λ1)2 +λ22
</p>
<p>= 0 ,
</p>
<p>&part;ℓ
</p>
<p>&part;λ2
= N
</p>
<p>λ2
&minus;2λ2
</p>
<p>N&sum;
</p>
<p>j=1
</p>
<p>1
</p>
<p>4(x(j)&minus;λ1)2 +λ22
= 0 .
</p>
<p>There is not necessarily a unique solution since the equations are not linear in
λ1 and λ2. For |x(j)&minus;λ1| ≪ λ2, however, we may write
</p>
<p>8
</p>
<p>λ22
</p>
<p>N&sum;
</p>
<p>j=1
(x(j)&minus;λ1)= 0 , Nλ1 =
</p>
<p>N&sum;
</p>
<p>j=1
x(j) , λ1 = a = x̄ .
</p>
<p>Problem 7.2
</p>
<p>(a)
L =
</p>
<p>N&prod;
</p>
<p>j=1
</p>
<p>1&radic;
2πσ
</p>
<p>exp[&minus;(x(j)&minus;λ)2/2σ 2]
</p>
<p>= (
&radic;
</p>
<p>2πσ)&minus;N
N&prod;
</p>
<p>j=1
exp[&minus;(x(j)&minus;λ)2/2σ 2] ,</p>
<p/>
</div>
<div class="page"><p/>
<p>G.2 Hints and Solutions 467
</p>
<p>ℓ = &minus;N ln(
&radic;
</p>
<p>2πσ)&minus; 1
2σ 2
</p>
<p>N&sum;
</p>
<p>j=1
(x(j)&minus;λ)2 ,
</p>
<p>ℓ&prime; = 1
σ 2
</p>
<p>N&sum;
</p>
<p>j=1
(x(j)&minus;λ) ,
</p>
<p>ℓ&prime;&prime; = &minus;N/σ 2 ,
I (λ) = &minus;E(ℓ&prime;&prime;)=N/σ 2 .
</p>
<p>(b)
L =
</p>
<p>N&prod;
</p>
<p>j=1
</p>
<p>1&radic;
2π
</p>
<p>&radic;
λ
</p>
<p>exp
</p>
<p>(
&minus;(x
</p>
<p>(j)&minus;a)2
2λ
</p>
<p>)
</p>
<p>= (2π)&minus;N/2λ&minus;N/2
N&prod;
</p>
<p>j=1
exp
</p>
<p>(
&minus;(x
</p>
<p>(j)&minus;a)2
2λ
</p>
<p>)
,
</p>
<p>ℓ = &minus;N
2
</p>
<p>ln(2π)&minus; N
2
</p>
<p>lnλ&minus; 1
2λ
</p>
<p>N&sum;
</p>
<p>j=1
(x(j)&minus;a)2 ,
</p>
<p>ℓ&prime; = &minus;N
2λ
</p>
<p>+ 1
2λ2
</p>
<p>N&sum;
</p>
<p>j=1
(x(j)&minus;a)2 ,
</p>
<p>ℓ&prime;&prime; = N
2λ2
</p>
<p>&minus; 1
λ3
</p>
<p>N&sum;
</p>
<p>j=1
(x(j)&minus;a)2 ,
</p>
<p>I (λ) = &minus;E(ℓ&prime;&prime;)=&minus; N
2λ2
</p>
<p>+ 1
λ3
</p>
<p>E
</p>
<p>⎧
⎨
⎩
</p>
<p>N&sum;
</p>
<p>j=1
(x(j)&minus;a)2
</p>
<p>⎫
⎬
⎭
</p>
<p>= &minus; N
2λ2
</p>
<p>+ 1
λ3
</p>
<p>Nλ ,
</p>
<p>I (λ) = 1
2λ2
</p>
<p>(&minus;N +2N)= N
2λ2
</p>
<p>,
</p>
<p>E(S) = 1
N
E
</p>
<p>⎧
⎨
⎩
</p>
<p>N&sum;
</p>
<p>j=1
(x(j)&minus;a)2
</p>
<p>⎫
⎬
⎭= σ
</p>
<p>2 = λ .
</p>
<p>Problem 7.3
</p>
<p>(a) σ 2(S)&ge; 1
I (λ)
</p>
<p>= σ
2
</p>
<p>N
,
</p>
<p>In Problem 6.1 (c) we had σ 2(x̄)= 1/3σ 2 for N = 3.
</p>
<p>(b) σ 2(S)&ge; 1
I (λ)
</p>
<p>= 2λ
2
</p>
<p>N
.
</p>
<p>(c) From Problem 7.2 we take
</p>
<p>ℓ&prime; =&minus;N
2λ
</p>
<p>+ 1
2λ2
</p>
<p>NS = N
2λ2
</p>
<p>(S&minus;λ)= N
2λ2
</p>
<p>(S&minus;E(S)) .</p>
<p/>
</div>
<div class="page"><p/>
<p>468 G Exercises, Hints and Solutions, Programming Problems
</p>
<p>Thus, σ 2(S)= 2λ
2
</p>
<p>N
.
</p>
<p>Problem 8.1
</p>
<p>x̄1 = 21.11 , x̄2 = 21.00 ,
s21 = 14.86 , s22 = 10.00 ,
T = s21/s22 = 1.486 , F0.95(8,6)= 4.15 .
</p>
<p>The variance of sample (2) is not significantly smaller.
</p>
<p>Problem 8.2
</p>
<p>x̄ = 25.142 , s2 = 82.69/29 = 2.85 , s = 1.69 ,
</p>
<p>T = x̄&minus;25.5
s/
&radic;
</p>
<p>30
= 25.142&minus;25.5
</p>
<p>1.69/5.48
=&minus;0.358
</p>
<p>0.309
=&minus;1.16 ,
</p>
<p>|T | &lt; t0.95(f = 29)= 1.70 .
</p>
<p>Therefore the hypothesis cannot be rejected.
</p>
<p>Problem 8.3
</p>
<p>(a)
f (x(1),x(2), . . . ,x(N), λ̃
</p>
<p>(Ω)
)=
</p>
<p>N&prod;
</p>
<p>j=1
</p>
<p>1&radic;
2πs&prime;
</p>
<p>exp
</p>
<p>(
&minus;(x
</p>
<p>(j)&minus; x̄)2
2s&prime;2
</p>
<p>)
</p>
<p>= (
&radic;
</p>
<p>2πs&prime;)&minus;N exp
</p>
<p>⎛
⎝&minus; 1
</p>
<p>2s&prime;2
</p>
<p>N&sum;
</p>
<p>j=1
(x(j)&minus; x̄)2
</p>
<p>⎞
⎠ ,
</p>
<p>f (x(1),x(2), . . . ,x(N), λ̃
(ω)
</p>
<p>)
</p>
<p>= (
&radic;
</p>
<p>2πσ0)
&minus;N exp
</p>
<p>⎛
⎝&minus; 1
</p>
<p>2σ 20
</p>
<p>N&sum;
</p>
<p>j=1
(x(j)&minus; x̄)2
</p>
<p>⎞
⎠ ,
</p>
<p>T =
(σ0
</p>
<p>s&prime;
</p>
<p>)N
exp
</p>
<p>[
1
2Ns
</p>
<p>&prime;2
(
</p>
<p>1
</p>
<p>σ 20
&minus; 1
</p>
<p>s&prime;2
</p>
<p>)]
,
</p>
<p>lnT = N(lnσ0 &minus; lns&prime;)+ 12Ns
&prime;2
(
</p>
<p>1
</p>
<p>σ 20
&minus; 1
</p>
<p>s&prime;2
</p>
<p>)
</p>
<p>= N(lnσ0 &minus; lns&prime;)+ 12N
s&prime;2
</p>
<p>σ 20
&minus; 12N .
</p>
<p>(b) T &prime;=Ns&prime;2/σ 20 is a monotonically increasing function of s&prime; if s&prime; &gt;σ0, otherwise
it is monotonically decreasing. Therefore the test takes on the following form:</p>
<p/>
</div>
<div class="page"><p/>
<p>G.2 Hints and Solutions 469
</p>
<p>T &prime; &gt; T &prime;
1&minus; 12α
</p>
<p>, T &prime; &lt; T &prime;1
2α
</p>
<p>for H0(s
&prime; = σ0) ,
</p>
<p>T &prime; &gt; T &prime;1&minus;α for H0(s
&prime; &lt; σ0) ,
</p>
<p>T &prime; &lt; T &prime;α for H0(s
&prime; &gt; σ0) .
</p>
<p>(c) The answer follows from (6.7.2) and s&prime;2 = (N &minus;1)s2/N .
</p>
<p>Problem 8.4
</p>
<p>(a) ã = 1
n
</p>
<p>&sum;
</p>
<p>k
</p>
<p>nkxk = 202.36 .
</p>
<p>σ̃ 2 = 1
n&minus;1
</p>
<p>&sum;
</p>
<p>k
</p>
<p>nk(xk &minus; ã)2 = 13.40 , σ̃ = 3.66 .
</p>
<p>(b) pk(xk) = ψ0
</p>
<p>(
xk + 12Δx&minus; ã
</p>
<p>σ̃
</p>
<p>)
&minus;ψ0
</p>
<p>(
xk &minus; 12Δx&minus; ã
</p>
<p>σ̃
</p>
<p>)
= ψ0+&minus;ψ0&minus; .
</p>
<p>xk nk ψ0+ ψ0&minus; npk
(nk&minus;npk)2
</p>
<p>npk
193 1 0.011 0.002 (0.9) &ndash;
195 2 0.041 0.011 (3.0) &ndash;
197 9 0.117 0.041 7.6 0.271
199 12 0.260 0.117 14.3 0.362
201 23 0.461 0.260 20.1 0.411
203 25 0.673 0.463 21.2 0.679
205 11 0.840 0.673 16.7 1.948
207 9 0.938 0.840 9.8 0.071
209 6 0.982 0.938 4.3 0.647
211 2 0.996 0.982 (1.4) &ndash;
</p>
<p>X2 = 4.389
The number of degrees of freedom is 7&minus;2 = 5. Since χ20.90(5)= 9.24, the test
does not reject the hypothesis.
</p>
<p>Problem 8.5
</p>
<p>(a) p̃1 =
1
</p>
<p>111
(13+44)= 57
</p>
<p>111
, p̃2 =
</p>
<p>1
</p>
<p>111
(25+29)= 54
</p>
<p>111
,
</p>
<p>q̃1 =
1
</p>
<p>111
(13+25)= 38
</p>
<p>111
, q̃2 =
</p>
<p>1
</p>
<p>111
(44+29)= 73
</p>
<p>111
,
</p>
<p>X2 =
</p>
<p>(
13&minus; 57 &middot;38
</p>
<p>111
</p>
<p>)2
</p>
<p>57 &middot;38
111
</p>
<p>+
</p>
<p>(
44&minus; 57 &middot;73
</p>
<p>111
</p>
<p>)2
</p>
<p>57 &middot;73
111</p>
<p/>
</div>
<div class="page"><p/>
<p>470 G Exercises, Hints and Solutions, Programming Problems
</p>
<p>+
</p>
<p>(
25&minus;
</p>
<p>54 &middot;38
111
</p>
<p>)2
</p>
<p>54 &middot;38
111
</p>
<p>+
</p>
<p>(
29&minus;
</p>
<p>54 &middot;73
111
</p>
<p>)2
</p>
<p>54 &middot;73
111
</p>
<p>=
42.43
</p>
<p>19.51
+
</p>
<p>42.43
</p>
<p>37.49
+
</p>
<p>42.43
</p>
<p>18.49
+
</p>
<p>42.43
</p>
<p>35.51
= 6.78 .
</p>
<p>Since χ20.90 = 2.71 for f = 1, the hypothesis of independence is rejected.
</p>
<p>(b) nij &minus;
1
</p>
<p>n
(ni1 +ni2)(n1j +n2j)
</p>
<p>=
1
</p>
<p>n
[nij (n11 +n12 +n21 +n22)&minus; (ni1 +ni2)(n1j +n2j)] .
</p>
<p>One can easily show that the expression in square brackets takes the form
</p>
<p>(&minus;1)i+j (n11n22 &minus;n12n21) for all i, j .
</p>
<p>G.3 Programming Problems
</p>
<p>Programming Problem 4.1: Program to Generate
</p>
<p>Breit&ndash;Wigner-Distributed Random Numbers
</p>
<p>Write a method with the following declaration
</p>
<p>It is to yield n random numbers, which follow a Breit&ndash;Wigner distribution having a
</p>
<p>mean of a and a FWHM of Γ . Make the method part of a class which allows for
</p>
<p>interactive input of n,a,Γ and numerical as well as graphical output of the random
</p>
<p>numbers in the form a histogram, Fig. G.3. (Example solution:
</p>
<p>Programming Problem 4.2: Program to Generate Random Numbers
</p>
<p>from a Triangular Distribution
</p>
<p>Write a method with the declaration
</p>
<p>It is to yield n random numbers, following a triangular distribution with the parame-
</p>
<p>ters a, b, c generated by the transformation procedure of Example 4.3.
</p>
<p>Write a second method with the declaration
</p>
<p>which solves the same problem, but uses von Neumann&rsquo;s acceptance&ndash;rejection
</p>
<p>method. Which of the two programs is faster?
</p>
<p>Write a class which allows to interactively choose either method. It should also
</p>
<p>allow for numerical and graphical output (as histogram) of the generated numbers,
</p>
<p>Fig. G.4. (Example solution:
</p>
<p>double [] breitWignerNumbers(double a, double gamma, int n).
</p>
<p>S1Random)
</p>
<p>double [] triangularNumbersTrans (double a, double b, double c,
</p>
<p>int n).
</p>
<p>S2Random)
</p>
<p>double [] triangularNumbersRej (double a, double b, double c,
</p>
<p>int n),</p>
<p/>
</div>
<div class="page"><p/>
<p>G.3 Programming Problems 471
</p>
<p>Fig.G.3: Histogram of 1000 random numbers following a Breit&ndash;Wigner distribution with
</p>
<p>a = 10 and Γ = 3.
</p>
<p>Programming Problem 4.3: Program to Generate Data Points with Errors
</p>
<p>of Different Size
</p>
<p>Write a method similar to y =
at +b+Δy. The errors Δy, however, are not to be taken for all data points yi from
the same uniform distribution with the width σ , but Δyi is to be sampled from a
</p>
<p>normal distribution with the width σi . The widths σi are to be taken from a uniform
</p>
<p>distribution within the region σmin &lt; σi &lt; σmax.
</p>
<p>Write a class which calls this method and which displays graphically the straight
</p>
<p>line y = at+b as well as the simulated data points with error bars yi &plusmn;Δyi , Fig. G.5.
(Example solution:
</p>
<p>Programming Problem 5.1: Convolution of Uniform Distributions
</p>
<p>Because of the Central Limit Theorem the quantity x =
&sum;N
</p>
<p>i=1 xi follows in the limit
</p>
<p>N &rarr; &infin; the standard normal distribution if the xi come from an arbitrary distribu-
tion with mean zero and standard deviation 1/
</p>
<p>&radic;
N . Choose for the xi the uniform
</p>
<p>distribution with the limits
</p>
<p>a =&minus;
&radic;
</p>
<p>3/N , b =&minus;a .
</p>
<p>Perform a large number nexp of Monte Carlo experiments, each giving a random value
</p>
<p>x. Produce a histogram of the quantity x and show in addition the distribution of x
</p>
<p>as a continuous curve which you would expect from the standard normal distribution
</p>
<p>S3Random)
</p>
<p>DatanRandom.line which generates data points</p>
<p/>
</div>
<div class="page"><p/>
<p>472 G Exercises, Hints and Solutions, Programming Problems
</p>
<p>Fig.G.4: Histogram of 1000 random numbers following a triangular distribution with a = 1,
b = 4, c = 3.
</p>
<p>Programming Problem 5.2: Convolution of Uniform Distribution
and Normal Distribution
</p>
<p>If x is taken from a uniform distribution between a and b and if y is taken from a
normal distribution with mean zero and width σ , then the quantity u = x+y follows
the distribution (5.11.14). Perform a large number nexp of Monte Carlo experiments,
each resulting in a random number u. Display a histogram of the quantity u and show
in addition a curve of the distribution you would expect from (5.11.14), Fig. G.7.
Allow for the interactive input of the quantities nexp, a, b, and σ .
</p>
<p>Programming Problem 7.1: Distribution of Lifetimes Determined
from a Small Number of Radioactive Decays
</p>
<p>In Example Program 7.1, an estimate t̄ of the mean lifetime τ and its asymmetric
errors Δ&minus; and Δ+ are found from a single small sample. In all cases the program
yields Δ&minus; &lt;Δ+. Write a program that simulates a large number nexp of experiments,
in each of which N radioactive decays of mean lifetime τ = 1 are measured. Compute
for each experiment the estimate t̄ and construct a histogram of the quantity t̄ for all
experiments. Present this histogram Ni(t̄i &le; t̄ &lt; t̄i +Δt̄) and also the cumulative
frequency distribution hi = (1/nexp)
</p>
<p>&sum;
t̄&lt;t̄i
</p>
<p>Ni . Allow for interactive input of nexp
</p>
<p>(Fig. G.6). (Use the class GraphicsWithHistogramAndPolyline for the si-
</p>
<p>multaneous representation of histograms and curves.) Allow for interactive input of
</p>
<p>the quantities nexp and N . (Example solution: S1Distrib)</p>
<p/>
</div>
<div class="page"><p/>
<p>G.3 Programming Problems 473
</p>
<p>Fig.G.5: Data points with errors of different size.
</p>
<p>and N . Demonstrate that the distributions are asymmetric for small N and that they
</p>
<p>become symmetric for large N . Show that for small N the value t̄ = 1 is not the
most probable value, but that it is the expectation value of t̄ . Determine for a fixed
</p>
<p>value of N , e.g., N = 4, limits Δ&minus; and Δ+ in such a way that t̄ &lt; 1&minus;Δ&minus; holds
with the probability 0.683/2 and that with the same probability one has t̄ &gt; 1+Δ+.
Compare the results found with a series of simulated experiments from the program
</p>
<p>Programming Problem 7.2: Distribution of the Sample Correlation
</p>
<p>Coefficient
</p>
<p>Modify the class
</p>
<p>the correlation coefficient r is presented (Fig. G.8). Produce histograms for ρ = 0
and ρ = 0.95, each for npt = 5, 50, 500. Under what circumstances is the distribu-
tion asymmetric and why? Is this asymmetry in contradiction to the Central Limit
</p>
<p>theorem? (Example solution:
</p>
<p>Programming Problem 9.1: Fit of a First-Degree Polynomial to Data
</p>
<p>that Correspond to a Second-Degree Polynomial
</p>
<p>In experimental or empirical studies one is often confronted with a large number of
</p>
<p>measurements or objects of the same kind (animals, elementary particle collisions,
</p>
<p>industrial products from a given production process, . . .). The outcomes of the mea-
</p>
<p>surements performed on each object are described by some law. Certain assumptions
</p>
<p>are made about that law, which are to be checked by experiment.
</p>
<p>E2MaxLike so that instead of numerical output, a histogram of
</p>
<p>S2MaxLike)
</p>
<p>  
</p>
<p>E1MaxLike, Example Program 7.1. (Example solution: S1MaxLike)</p>
<p/>
</div>
<div class="page"><p/>
<p>474 G Exercises, Hints and Solutions, Programming Problems
</p>
<p>Consider the following example. A series of measurements may contain nexp
experiments. Each experiment yields the measurements yi = x1 + x2ti + x3t2i + εi
for 10 values ti = 1,2 . . . ,10 of the controlled variable t . The εi are taken from a
normal distribution with mean zero and width σ . In the analysis of the experiments
it is assumed, however, that the true values ηi underlying the measurements yi can be
described by a first-degree polynomial ηi = x1 + x2t . As result of the fit we obtain
a minimum function M from which we can compute the &ldquo;χ2-probability&rdquo; P = 1&minus;
F(M,n&minus; r). Here F(M,f ) is the distribution function of a χ2 distribution with f
degrees of freedom, n is the number of data points, and r the number of parameters
determined in the fit. If P &lt; α, then the fit of a first-degree polynomial to the data is
rejected at a confidence level of β = 1&minus;α.
</p>
<p>Write a class performing the following steps:
</p>
<p>(i) Interactive input of nexp, x1, x2, x3, σ , Δy.
</p>
<p>(ii) Generation of nexp sets of data (ti,yi,Δy), fit of a first-degree polynomial to
each set of data and computation of P . Entry of P into a histogram.
</p>
<p>(iii) Graphical representation of the histogram.
</p>
<p>Fig.G.6: Histogram of 90 000 random numbers x, each of which is a sum of three uniformly
distributed random numbers. The curve corresponds to the standard normal distribution.
Significant differences between curve and histogram are visible only because of the very
large number of random numbers used.</p>
<p/>
</div>
<div class="page"><p/>
<p>G.3 Programming Problems 475
</p>
<p>Suggestions: (a) Choose nexp = 1000, x1 = x2 = 1, x3 = 0, σ =Δy = 1. As expected
you will obtain a flat distribution for P . (b) Choose (keeping the other input quantities
as above) different values x3 �= 0. You will observe a shift of the distribution towards
small P values, cf. Fig. G.9. Determine approximately the smallest positive value of
x3 such that the hypothesis of a first-degree polynomial is rejected at 90% confidence
level in 95% of all experiments. (c) Choose x3 = 0, but σ �= Δy. You will again
observe a shift in the distribution, e.g., towards larger P values for Δy &gt; σ . (d) From
the experience gained in (a), (b), and (c), one might conclude that if erroneously
too large measurement errors are assumed (Δy &gt; σ ) then a flat P distribution would
result. In this way one would get the impression that a first-degree polynomial could
describe the data. Begin with nexp = 1000, x1 = x2 = 1, x3 = 0.2, σ = 1, Δy = 1 and
</p>
<p>Programming Problem 9.2: Fit of a Power Law (Linear Case)
A power law
</p>
<p>η = xtw
</p>
<p>is linear in the parameter x if w is a constant. This function is to be fitted to measure-
ments (ti,yi) given by
</p>
<p>ti = t0 + (i&minus;1)Δt , i = 1, . . . ,n ,
</p>
<p>Fig.G.7: A histogram of 10 000 random numbers, each of which is the sum of a uniformly
distributed random number and a normally distributed random number. The curve corre-
sponds to the convolution of a uniform and a normal distribution.
</p>
<p>increase Δy in steps of 0.1 up to Δy = 2. (Example solution: SILsq)</p>
<p/>
</div>
<div class="page"><p/>
<p>476 G Exercises, Hints and Solutions, Programming Problems
</p>
<p>Fig.G.8: Histogram of the sample correlation coefficient computed for 1000 samples of size
10 from a bivariate Gaussian distribution with the correlation coefficient ρ =&minus;0.8.
</p>
<p>yi = xtwi + εi .
</p>
<p>Here the εi follow a normal distribution centered about zero with width σ .
</p>
<p>Write a class performing the following steps:
</p>
<p>(i) Interactive input of n, t0, Δt , x, w, σ .
</p>
<p>(ii) Generation of measured points.
</p>
<p>(iii) Fit of the power law.
</p>
<p>(iv) Graphical display of the data and the fitted function, cf. Fig. G.10.
</p>
<p>Programming Problem 9.3: Fit of a Power Law (Nonlinear Case)
If the power law has the form
</p>
<p>η = x1tx2 ,
i.e., if the power itself is an unknown parameter, the problem becomes nonlinear.
For the fit of a nonlinear function we have to start from a first approximation of the
parameters. We limit ourselves to the case ti &gt; 0 for all i which occurs frequently
in practice. Then one has lnη = lnx1 + x2 ln t . If instead of (ti,yi) we now use
</p>
<p>(Example solution: S2Lsq)</p>
<p/>
</div>
<div class="page"><p/>
<p>G.3 Programming Problems 477
</p>
<p>Fig.G.9: Histogram of the χ2-probability for fits of a first-degree polynomial to 1000 data
sets generated according to a second-degree polynomial.
</p>
<p>(ln ti, lnyi) as measured variables, we obtain a linear function in the parameters lnx1
and x2. However, in this transformation the errors are distorted so that they are no
longer Gaussian. We simply choose all errors to be of equal size and use the result of
the linear fit as the first approximation of a nonlinear fit to the (ti,yi). We still have
to keep in mind (in any case for x1 &gt; 0) that one always has η &gt; 0 for t &gt; 0. Because
of measurement errors, however, measured values yi &lt; 0 can occur. Such points of
course must not be used for the computation of the first approximation.
</p>
<p>Write a class with the following steps:
</p>
<p>(i) Interactive input of n, t0, Δt , x1, x2, σ .
</p>
<p>(ii) Generation of the n measured points
</p>
<p>ti = t0 + (i&minus;1)Δt , i = 1, . . . ,n ,
yi = x1tx2i + εi ,
</p>
<p>where εi comes from a normal distribution centered about zero with width σ .
</p>
<p>(iii) Computation of first approximations x1, x2 by fitting a linear function to
</p>
<p>(ln t ,i lny )i with LsqLin.</p>
<p/>
</div>
<div class="page"><p/>
<p>478 G Exercises, Hints and Solutions, Programming Problems
</p>
<p>Fig.G.10:Result of the fit of a parabola y = xt2 to 20 measured points.
</p>
<p>(v) Graphical display of the results, cf. Fig. G.11.
</p>
<p>Programming Problem 9.4: Fit of a Breit&ndash;Wigner Function to Data Points
with Errors
</p>
<p>For the N = 21 values ti =&minus;3,&minus;2.7, . . . , 3 of the controlled variable the measured
values
</p>
<p>yi = f (ti)+ εi (G.3.1)
are to be simulated. Here,
</p>
<p>f (t)= 2
πx2
</p>
<p>x22
</p>
<p>4(t&minus;x1)2 +x22
(G.3.2)
</p>
<p>is the Breit&ndash;Wigner function (3.3.32) with a = x1 and Γ = x2. The measurement
errors εi are to be taken from a normal distribution around zero with width σ . Choose
a = 0 and Γ = 1. The points (ti,yi) scatter within the measurement errors around a
bell-shaped curve with a maximum at t = a. A bell-shaped curve with a maximum at
the same position, however, could be given by the Gaussian function,
</p>
<p>f (t)= 1
x2
&radic;
</p>
<p>2π
exp
</p>
<p>{
&minus;(t&minus;x1)
</p>
<p>2
</p>
<p>2x22
</p>
<p>}
. (G.3.3)
</p>
<p>(iv) Fit of a power law to (t ,y )i i with LsqNon.
</p>
<p>(Example solution: S3Lsq)</p>
<p/>
</div>
<div class="page"><p/>
<p>G.3 Programming Problems 479
</p>
<p>Fig.G.11: Fit of a function y = x1tx2 to 20 data points. The data points are identical to those
in Fig. G.10.
</p>
<p>Write a class with the following properties:
</p>
<p>(i) Interactive input of σ and possibility to choose whether a Breit&ndash;Wigner func-
tion or a Gaussianis to be fitted.
</p>
<p>(ii) Generation of the data, i.e., of the triplets of numbers (ti,yi,Δyi = εi).
</p>
<p>(iii) Fit of the Breit&ndash;Wigner function (G.3.2) or of the Gaussian (G.3.3) to the data
and computation of the minimum function M.
</p>
<p>(iv) Graphical representation of the measured points with measurement errors and
of the fitted function, cf. Fig. G.12.
</p>
<p>Run the program using different values of σ and find out for which range of σ the
data allow a clear discrimination between the Breit&ndash;Wigner and Gaussian functions.
(Example solution: S4Lsq)
</p>
<p>Programming Problem 9.5: Asymmetric Errors and Confidence Region
for the Fit of a Breit&ndash;Wigner Function
</p>
<p>Supplement the solution of Programming Problem 9.4 such that it yields a graphical
representation for the parameters, their errors. covariance ellipse, asymmetric errors,
and confidence region similar to Fig. 9.11. Discuss the differences obtained when
fitting a Breit&ndash;Wigner or a Gaussian, respectively. For each case try σ = 0.1 and
σ = 1. (Example solution: S5Lsq)</p>
<p/>
</div>
<div class="page"><p/>
<p>480 G Exercises, Hints and Solutions, Programming Problems
</p>
<p>Fig.G.12: Fit of a Gaussian to data points that correspond to a Breit&ndash;Wigner function. The
goodness-of-fit is poor.
</p>
<p>Programming Problem 9.6: Fit of a Breit&ndash;Wigner Function to a Histogram
In the Programming Problem 9.4 we started from measurements yi = f (ti)+εi . Here
f (t) was a Breit&ndash;Wigner function (3.3.32), and the measurement errors εi corre-
sponded to a Gaussian distribution centered about zero with width σ .
</p>
<p>We now generate a sample of size nev from a Breit&ndash;Wigner distribution with
mean a = 0 and full width at half maximum Γ = 1. We represent the sample by a
histogram that is again characterized by triplets of numbers (ti,yi,Δyi). Now ti is
the center of the ith bin ti &minus;Δt/2 &le; t &lt; ti +Δt/2, and yi is the number of sample
elements falling into this bin. For not too small yi the corresponding statistical error is
Δyi =
</p>
<p>&radic;
yi . For small values yi this simple statement is problematic. It is completely
</p>
<p>wrong for yi = 0. In the fit of a function to a histogram, care is therefore to be taken
that empty bins (possibly also bins with few entries) are not to be considered as data
points. The function to be fitted is
</p>
<p>f (t)= x3
2
</p>
<p>πx2
</p>
<p>x22
</p>
<p>4(t &minus;x1)2 +x22
. (G.3.4)
</p>
<p>The function is similar to (G.3.2) but there is an additional parameter x3.
</p>
<p>Write a class with the following steps:
</p>
<p>(i) Interactive input of nev (sample size) and nt (number of histogram bins). The
lower limit of the histogram is to be fixed at t =&minus;3, the upper limit at t = 3.</p>
<p/>
</div>
<div class="page"><p/>
<p>G.3 Programming Problems 481
</p>
<p>Fig.G.13: Fit of a Breit&ndash;Wigner function to a histogram.
</p>
<p>(ii) Generation of the sample, cf. Programming Problem 4.1.
</p>
<p>(iii) Construction of the histogram.
</p>
<p>(iv) Construction of the triplets (ti,yi,Δyi) to be used for the fit.
</p>
<p>(v) Fit of the function (G.3.4).
</p>
<p>(vi) Output of the results in numerical and graphical form, cf. Fig. G.13.
</p>
<p>Suggestions: Perform consecutive fits for the same sample but different numbers of
bins. Try to find an optimum for the number of bins. (Example solution: S6Lsq)
</p>
<p>Programming Problem 9.7: Fit of a Circle to Points with Measurement
Errors in Abscissa and Ordinate
</p>
<p>A total of m data points (si, ti) are given in the (s, t) plane. The measurement errors
are defined by 2&times;2 covariance matrices of the form
</p>
<p>(
Δs2i c
</p>
<p>2
i
</p>
<p>c2i Δt
2
i
</p>
<p>)
(G.3.5)
</p>
<p>as in Example 9.11. Here ci =ΔsiΔtiρi and ρi is the correlation coefficient between
the measurement errors Δsi , Δti . As in Example 9.11, construct the vector y of
measurements from the si and ti and construct the covariance matrix Cy . Set up</p>
<p/>
</div>
<div class="page"><p/>
<p>482 G Exercises, Hints and Solutions, Programming Problems
</p>
<p>the equations fk(x,η)= 0 assuming that the true positions underlying the measured
points lie on a circle with center (x1,x2) and radius x3. Write a program with the
following steps:
</p>
<p>(i) Input of m, Δt , Δs, ρ.
</p>
<p>(ii) Generation of m measured points (si, ti) using bivariate normal distributions,
the means of which are positioned at regular intervals on the unit circle (x1 =
x2 = 0, x3 = 1) and the covariance matrix of which is given by (G.3.5) with
Δsi =Δs, Δti =Δt , c =ΔsΔtρ.
</p>
<p>(iii) Determination of a first approximation for x1, x2, x3 by computation of the
parameters of a circle through the first three measured points.
</p>
<p>(v) Graphical representation of the measured points and the fitted circle as in
Fig. G.14.
</p>
<p>(Example solution: S6Lsq)
</p>
<p>Fig.G.14: Measured points with covariance ellipses, circle of the first approximation which
is given by 3 measured points (broken line), and circle fitted to all measured points.
</p>
<p>(iv) Fit to all measured points using
</p>
<p>for this problem.
</p>
<p>LsqGen and a user function specially written</p>
<p/>
</div>
<div class="page"><p/>
<p>G.3 Programming Problems 483
</p>
<p>Programming Problem 10.1: Monte Carlo Minimization to Choose
a Good First Approximation
</p>
<p>For some functions in Example Program 10.1 the choice of the point x0 defining
the first approximation was decisive for success or failure of the minimization. If
a function has several minima and if its value is smallest at one of them, that is if
an &ldquo;absolute minimum&rdquo; exists, the following procedure will work. One uses the
Monte Carlo method to determine a first approximation x0 of the absolute minimum
in a larger region of the parameter space by generating points x = (x1,x2, . . . ,xn)
according to a uniform distribution in that region and by choosing that point at which
the function has the smallest value.
</p>
<p>function f7(x) described in Sect. 10.1. A first approximation x0 within the region
</p>
<p>&minus;10 &lt; x0i &lt; 10 , i = 1,2,3 ,
</p>
<p>is to be determined by the Monte Carlo method. Perform the search for the first
approximation with N points generated at random and allow for an interactive input
</p>
<p>Programming Problem 10.2: Determination of the Parameters
of a Breit&ndash;Wigner Distribution from the Elements of a Sample
</p>
<p>Breit&ndash;Wigner distribution with mean a and full width at half maximum Γ , and that
subsequently determines the numerical values of these parameters by minimization of
the negative log-likelihood function of the sample. Allow for interactive input of the
sample size and the parameters a and Γ used in the simulation. (Example solution:
S2Min)
</p>
<p>Programming Problem 11.1: Two-Way Analysis of Variance with Crossed
Classification
</p>
<p>The model (11.2.16) for the data in an analysis of variance with crossed classification is
</p>
<p>xijk = μ+ai +bj + (ab)ij + εijk .
</p>
<p>The analysis of variance tests the null hypothesis
</p>
<p>ai = bj = (ab)ij = 0 .
</p>
<p>Sect. 11.2, and are subsequently analyzed.
</p>
<p>above formula with
</p>
<p>ai =
(
i&minus; I +1
</p>
<p>2
</p>
<p>)
a ,
</p>
<p>bj =
(
j &minus; J +1
</p>
<p>2
</p>
<p>)
b ,
</p>
<p>(ab)ij = signum(ai) signum(bj ) ab .
</p>
<p>On the basis of E1Min write class that determines the absolute minimum of the
</p>
<p>of N . (Example solution: S1Min)
</p>
<p>By modifying a copy of E2Min produce a class that simulates a sample from a
</p>
<p>Data corresponding to this null hypothesis are generated by the program E2Anova,
</p>
<p>Write a program similar to x ijk according to theE2Anova which generates data</p>
<p/>
</div>
<div class="page"><p/>
<p>484 G Exercises, Hints and Solutions, Programming Problems
</p>
<p>Fig.G.15: Data points with errors and regression polynomials of different degrees.
</p>
<p>These relations fulfill the requirements (11.2.7). The εijk
drawn from a normal distribution with mean zero and standard deviation σ . Allow
for interactive input of the quantities a, b, ab, σ , μ. Perform an analysis of variance
on the simulated data. Study different cases, e.g., a = 0, b �= 0, ab = 0; a �= 0, b= 0,
</p>
<p>Programming Problem 11.2: Two-Way Analysis of Variance
with Nested Classification
</p>
<p>Modify Programming Problem 11.1 for the treatment of a nested classification with
data of the form (11.2.22), e.g.,
</p>
<p>xijk = μ+ai +bij + εijk ,
</p>
<p>and use the relations
</p>
<p>ai =
(
i&minus; I +1
</p>
<p>2
</p>
<p>)
a , bij =
</p>
<p>(
j &minus; J +1
</p>
<p>2
</p>
<p>)
b .
</p>
<p>(Example solution: S2Anova)
</p>
<p>Programming Problem 12.1: Simulation of Data and Plotting Regression
Polynomials of Different Degrees
</p>
<p>Write a class which generates n data points (ti,yi). The ti are to be spread equidis-
tantly over the interval &minus;1 &le; t &le; 1; the yi are to correspond to a polynomial with r
</p>
<p>as in E2Anova are to be
</p>
<p>ab = 0; a = 0, b = 0, ab �= 0; etc. (Example solution: S1Anova)</p>
<p/>
</div>
<div class="page"><p/>
<p>G.3 Programming Problems 485
</p>
<p>terms and to have errors with standard deviation σ . A regression analysis is to be
performed and a plot as in Fig. G.15 of the data and the regression polynomials to be
</p>
<p>Programming Problem 12.2: Simulation of Data and Plotting the
Regression Line with Confidence Limits
</p>
<p>Extend the solution of Programming Problem 12.1 so that a regression polynomial
of the desired degree together with confidence limits, corresponding to Sect. 12.2 is
</p>
<p>Programming Problem 13.1: Extrapolation in a Time Series Analysis
</p>
<p>In Sect. 13.3 we have stressed that one must be very cautious with the interpretation
of the results of a time series analysis at the edge of a time series, and of the extrapo-
lation in regions outside the time series. In particular, we found that the extrapolation
yields meaningless results if the data are not at least approximately described by a
polynomial. The degree of this polynomial must be smaller than or equal to the de-
gree of the polynomial used for the time series analysis.
</p>
<p>Programming Problem 13.2: Discontinuities in Time Series
</p>
<p>In the development of time series analysis it was assumed that the measurements,
apart from their statistical fluctuations, are continuous functions of time. We there-
fore expect unreliable results in regions where the measurements are discontinuous.
Write a program that generates the following three types of time series, analyzes
them, and displays the results graphically. One of them is continuous; the other two
contain discontinuities.
</p>
<p>Sine function:
</p>
<p>yi = sin(πti/180)+ εi , ti = i , i = 1,2, . . . ,n .
</p>
<p>produced. Your program may be largely based on the classes
</p>
<p>(Example solution:
</p>
<p>E4Reg and E2Reg.
</p>
<p>S1Reg)
</p>
<p>shown. (Example solution: S2Reg)
</p>
<p>Study these statements by simulating a number of time series and analyzing
</p>
<p>them. Write a program &ndash; starting from n= 200, i = 1,2, . . . ,n,
generates data of the form
</p>
<p>yi = tmi + εi , ti =
i&minus;100
</p>
<p>50
.
</p>
<p>Here the εi are to be generated according to a normal distribution with mean zero
</p>
<p>and standard deviation σ . Allow for the interactive input of m, σ , k, ℓ, and P .
</p>
<p>After generating the data perform a time series analysis and produce a plot as in
</p>
<p>m, k, and ℓ, and for each combination
</p>
<p>use small values of σ (e.g., σ = 0.001) and large values of σ (e.g., σ = 0.1). (Example
solution:
</p>
<p>E2TimSer&ndash; that for
</p>
<p>E2TimSer. Study different combinations of
</p>
<p>S1TimSer)</p>
<p/>
</div>
<div class="page"><p/>
<p>486 G Exercises, Hints and Solutions, Programming Problems
</p>
<p>Step function:
</p>
<p>yi =
{
</p>
<p>&minus;1+ εi , ti &le; 100
1+ εi , ti &gt; 100
</p>
<p>, ti = i mod 200 , i = 1,2, . . . ,n .
</p>
<p>Sawtooth function:
</p>
<p>yi = (ti &minus;50)/100+ εi , ti = i mod 100 , i = 1,2, . . . ,n .
</p>
<p>The εi are again to be generated according to normal distribution with mean zero
and standard deviation σ . Allow for the choice of one of the functions and for the
interactive input of n, σ , k, ℓ, and P . Study the time series using different values
for the parameters and discuss the results. Figure G.16 shows an example. (Example
solution: S2TimSer)
</p>
<p>Fig.G.16:Time series corresponding to a step function with moving averages and confidence
limits.</p>
<p/>
</div>
<div class="page"><p/>
<p>H. Collection of Formulas
</p>
<p>Probability
</p>
<p>A, B, . . . are events; Ā is the event &ldquo;not A&rdquo;.
</p>
<p>(A+B) and (AB) combine events with logical &ldquo;or&rdquo; or logical &ldquo;and&rdquo;.
P (A) is the probability of the event A.
</p>
<p>P (B|A)= P (AB)/P (A) is the probability for B given the condition A (con-
ditional probability).
</p>
<p>The following rules hold:
For every event A
</p>
<p>P(Ā)= 1&minus;P (A) ,
for mutually exclusive events A, B, . . ., Z
</p>
<p>P(A+B+&middot;&middot; &middot;+Z)= P (A)+P (B)+&middot;&middot; &middot;+P (Z) ,
for independent events A, B, . . ., Z
</p>
<p>P(AB &middot; &middot; &middot;Z)= P (A)P (B) &middot; &middot; &middot;P (Z) .
</p>
<p>Single Random Variable
</p>
<p>Distribution function: F(x)= P (x &lt; x)
Probability density (for F(x) differentiable):
f (x)= F &prime;(x)= df (x)/dx
Moments of order ℓ:
</p>
<p>(a) About the point c: αℓ = E{(x&minus; c)ℓ}
(b) About the origin (central moments): λℓ = E{xℓ}
(c) About the mean: μℓ = E{(x&minus; x̂)ℓ}
</p>
<p>S. Brandt, Data Analysis: Statistical and Computational Methods for Scientists and Engineers,
DOI 10.1007/978-3-319-03762-2, &copy; Springer International Publishing Switzerland 2014
</p>
<p>487</p>
<p/>
</div>
<div class="page"><p/>
<p>488 H Collection of Formulas
</p>
<p>TableH.1: Expectation values for discrete and continuous distributions.
</p>
<p>x discrete x continuous;
F(x) differentiable
</p>
<p>Probability
density &ndash;
</p>
<p>f (x)= F &prime;(x)= dF(x)dx ,&int;&infin;
&minus;&infin;f (x)dx = 1
</p>
<p>Mean of x
(expectation value)
</p>
<p>x̂ = E(x)
=
</p>
<p>&sum;
i xiP (x = xi)
</p>
<p>x̂ = E(x)
=
</p>
<p>&int;&infin;
&minus;&infin;xf (x)dx
</p>
<p>Mean of the
function H(x)
</p>
<p>E{H(x)}
=
</p>
<p>&sum;
iH(xi)P (x = xi)
</p>
<p>E{H(x)}
=
</p>
<p>&int;&infin;
&minus;&infin;H(x)f (x)dx
</p>
<p>Variance: σ 2(x)= var(x)= μ2 = E{(x&minus; x̂)2}
Standard deviation or error of x: Δx = σ(x)=+
</p>
<p>&radic;
σ 2(x)
</p>
<p>Skewness: γ = μ3/σ 3
</p>
<p>Reduced variable: u = (x&minus; x̂)/σ (x) ; E(u)= 0 , σ 2(u)= 1
Mode (most probable value) xm defined by: P (x = xm)= max
Median x0.5 defined by: F(x0.5)= P (x &lt; x0.5)= 0.5
Quantile xq defined by: F(xq)= P (x &lt; xq)= q ; 0 &le; q &le; 1
</p>
<p>Several Random Variables
</p>
<p>Distribution function:
</p>
<p>F(x)= F(x1,x2, . . . ,xn)= P (x1 &lt; x1, x2 &lt; x2, . . . , xn &lt; xn)
</p>
<p>Joint probability density (only for F(x) differentiable with respect to all vari-
ables):
</p>
<p>f (x)= f (x1,x2, . . . ,xn)= &part;nF(x1,x2, . . . ,xn)/&part;x1&part;x2 . . . &part;xn
</p>
<p>Marginal probability density of the variable xi:
</p>
<p>gi(xi)=
&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
&middot; &middot; &middot;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
f (x1,x2, . . . ,xn)dx1 dx2 . . .dxi&minus;1 dxi+1 . . .dxn</p>
<p/>
</div>
<div class="page"><p/>
<p>H. Collection of Formulas 489
</p>
<p>Expectation value of a function H(x):
</p>
<p>E{H(x)} =
&int;
</p>
<p>H(x)f (x)dx
</p>
<p>Expectation value of the variables xi :
</p>
<p>x̂i = E(xi)=
&int;
</p>
<p>xif (x)dx=
&int; &infin;
</p>
<p>&minus;&infin;
xigi(xi)dxi
</p>
<p>The variables x1,x2, . . . ,xn are independent, if
</p>
<p>f (x1,x2, . . . ,xn)= g1(x1)g2(x2) . . .gn(xn)
</p>
<p>Moments of order ℓ1,ℓ2, . . . ,ℓn:
</p>
<p>(a) About c= (c1,c2, . . . ,cn):
αℓ1ℓ2...ℓn = E{(x1 &minus; c1)ℓ1(x2 &minus; c2)ℓ2 &middot; &middot; &middot;(xn&minus; cn)ℓn}
</p>
<p>(b) About the origin: λℓ1ℓ2...ℓn = E{x
ℓ1
1 x
</p>
<p>ℓ2
2 &middot; &middot; &middot;x
</p>
<p>ℓn
n }
</p>
<p>(c) About x̂: μℓ1ℓ2...ℓn = E{(x1 &minus; x̂1)ℓ1(x2 &minus; x̂2)ℓ2 &middot; &middot; &middot;(xn&minus; x̂n)ℓn}
Variance of xi: σ 2(xi)=E{(xi &minus; x̂i)2} = cii
Covariance between xi and xj : cov(xi,xj )= E{(xi &minus; x̂i)(xj &minus; x̂j)} = cij
</p>
<p>For xi , xj independent: cov(xi,xj )= 0
Covariance matrix: C = E{(x&minus; x̂)(x&minus; x̂)T}
Correlation coefficient:
</p>
<p>ρ(x1,x2)= cov(x1,x2)/σ (x1)σ (x2) ; &minus;1 &le; ρ &le; 1
</p>
<p>Rules of computation:
</p>
<p>σ 2(cxi)= c2σ 2(xi) ,
σ 2(axi +bxj )= a2σ 2(xi)+b2σ 2(xj )+2abcov(xi,xj ) ;
a,b,c are constants
</p>
<p>Transformation of Variables
</p>
<p>Original variables: x = (x1,x2, . . . ,xn)
Probability density: f (x)
</p>
<p>Transformed variables: y = (y1,y2, . . . ,yn)
Mapping: y1 = y1(x), y2 = y2(x), . . . , yn = yn(x)
Probability density: g(y)= |J |f (x)</p>
<p/>
</div>
<div class="page"><p/>
<p>490 H Collection of Formulas
</p>
<p>with the Jacobian determinant
</p>
<p>J = J
(
x1,x2, . . . ,xn
</p>
<p>y1,y2, . . . ,yn
</p>
<p>)
=
</p>
<p>∣∣∣∣∣∣∣∣∣∣
</p>
<p>&part;x1
</p>
<p>&part;y1
</p>
<p>&part;x2
</p>
<p>&part;y1
&middot; &middot; &middot; &part;xn
</p>
<p>&part;y1
...
</p>
<p>&part;x1
</p>
<p>&part;yn
</p>
<p>&part;x2
</p>
<p>&part;yn
&middot; &middot; &middot; &part;xn
</p>
<p>&part;yn
</p>
<p>∣∣∣∣∣∣∣∣∣∣
</p>
<p>Error Propagation
</p>
<p>The original variables x have the covariance matrix Cx . The covariance ma-
trix of the transformed variables y is
</p>
<p>Cy = T CxT T with T =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>&part;y1
</p>
<p>&part;x1
</p>
<p>&part;y1
</p>
<p>&part;x2
&middot; &middot; &middot; &part;y1
</p>
<p>&part;xn
</p>
<p>&part;y2
</p>
<p>&part;x1
</p>
<p>&part;y2
</p>
<p>&part;x2
&middot; &middot; &middot; &part;y2
</p>
<p>&part;xn
...
</p>
<p>&part;ym
</p>
<p>&part;x1
</p>
<p>&part;ym
</p>
<p>&part;x2
&middot; &middot; &middot; &part;ym
</p>
<p>&part;xn
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>.
</p>
<p>The formula is only exact for a linear relation between y and x, but is a good
approximation for small deviations from linearity in a region around x̂ of the
magnitude of the standard deviation. Only for vanishing covariances in Cx
does one have
</p>
<p>σ(yi)=Δyi =
</p>
<p>&radic;&radic;&radic;&radic;
m&sum;
</p>
<p>j=1
</p>
<p>(
&part;yi
</p>
<p>&part;xj
</p>
<p>)2
(Δxj )2 .
</p>
<p>The Law of Large Numbers
</p>
<p>A total of n observations are carried out, which are characterized by the ran-
dom variable xi (= 1, if on the ith observation the event A occurs, other-
wise = 0). The frequency of A is
</p>
<p>h = 1
n
</p>
<p>n&sum;
</p>
<p>i=1
xi .
</p>
<p>For n &rarr; &infin; this frequency is equal to the probability p for the occurrence
of A,
</p>
<p>E(h)= h= p , σ 2(h)= 1
n
p(1&minus;p) .</p>
<p/>
</div>
<div class="page"><p/>
<p>H. Collection of Formulas 491
</p>
<p>TableH.2: Distributions of discrete variables.
</p>
<p>Distribution
</p>
<p>Probability for
observing x = k
(x1 = k1, . . . ,
xl = kl)
</p>
<p>Mean
Variance
(elements of
covariance matrix)
</p>
<p>Binomial
Wnk
=
</p>
<p>(
n
k
</p>
<p>)
pk(1&minus;p)n&minus;k x̂ = np σ
</p>
<p>2(x)= np(1&minus;p)
</p>
<p>Multinomial&sum;l
j=1pj = 1
</p>
<p>Wnk1,k2,...,kl
</p>
<p>= n!&prod;l
j=1 kj !
</p>
<p>&prod;l
j=1p
</p>
<p>kj
j
</p>
<p>x̂j = npj cij = npi(δij &minus;pj )
</p>
<p>Hypergeometric
L=N &minus;K,
l = n&minus; k
</p>
<p>Wk =
(
K
k
</p>
<p>)(
L
ℓ
</p>
<p>)
:
(
N
n
</p>
<p>)
x̂ = nK
</p>
<p>N
σ 2(x)= nKL(N&minus;n)
</p>
<p>N2(N&minus;1)
</p>
<p>Poisson f (k)= λk
k! e
</p>
<p>&minus;λ x̂ = λ σ 2(x)= λ
</p>
<p>Central Limit Theorem
</p>
<p>If xi are independent variables with mean a and variance σ 2, then (1/n)&sum;n
i=1 xi for n&rarr;&infin; follows a normal distribution with mean a and variance
</p>
<p>σ 2/n.
</p>
<p>Convolutions of Distributions
</p>
<p>The probability density of the sum u = x+ y of two independent random
variables x and y is
</p>
<p>fu(u)=
&int; &infin;
</p>
<p>&minus;&infin;
fx(x)fy(u&minus;x)dx =
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
fy(y)fx(u&minus;y)dy .
</p>
<p>&ndash; The convolution of two Poisson distributions with the parameters λ1
and λ2 is a Poisson distribution with the parameter λ1 +λ2.
</p>
<p>&ndash; The convolution of two Gaussian distributions with means a1, a2 and
variances σ 21 , σ
</p>
<p>2
2 is a Gaussian distribution with mean a = a1 +a2 and
</p>
<p>variance σ 2 = σ 21 +σ 22 .</p>
<p/>
</div>
<div class="page"><p/>
<p>492 H Collection of Formulas
</p>
<p>TableH.3: Distributions of continuous variables.
</p>
<p>Distribution Probability density Mean
Variance
(covariance
matrix)
</p>
<p>Uniform
{
</p>
<p>0; x &gt; a, x &ge; b
1
</p>
<p>b&minus;a ; a &le; x &lt; b
1
2(b+a) (b&minus;a)2/12
</p>
<p>Gaussian 1
b
&radic;
</p>
<p>2π
exp
</p>
<p>(
&minus; (x&minus;a)2
</p>
<p>2b2
</p>
<p>)
a b2
</p>
<p>Stand.
Gaussian
</p>
<p>1&radic;
2π
</p>
<p>exp(&minus;12x2) 0 1
</p>
<p>Gaussian
of several
variables
</p>
<p>k exp
{
&minus; (x&minus;a)TB(x&minus;a)2
</p>
<p>}
a C = B&minus;1
</p>
<p>χ2 1
</p>
<p>Γ ( 12f )2
1
2 f
(χ2)
</p>
<p>1
2f&minus;1 exp(&minus;12χ2) f 2f
</p>
<p>Fisher&rsquo;s F
</p>
<p>(
f1
f2
</p>
<p>) 1
2f1 Γ ( 12 (f1+f2))
</p>
<p>Γ ( 12f1)Γ (
1
2f2)
</p>
<p>&times;F 12f1&minus;1
(
</p>
<p>1+ f1
f2
F
)&minus; 12 (f1+f2)
</p>
<p>f2
f2&minus;2 ,
f2 &gt; 2
</p>
<p>2f 22 (f1+f2&minus;2)
f1(f2&minus;2)2(f2&minus;4)
</p>
<p>,
</p>
<p>f2 &gt; 4
</p>
<p>Student&rsquo;s t
Γ ( 12 (f+1))
</p>
<p>Γ ( 12f )
&radic;
π
&radic;
f
</p>
<p>(
1+ t2
</p>
<p>f
</p>
<p>)&minus; 12 (f+1)
0
</p>
<p>f
f&minus;2 ,
f &gt; 2
</p>
<p>&ndash; The convolution of two χ2-distributions with f1 and f2 degrees of free-
dom is a χ2-distribution with f = f1 +f2 degrees of freedom.
</p>
<p>Samples
</p>
<p>Population: An infinite (in some cases finite) set of elements described by a
discrete or continuously distributed random variable x.
</p>
<p>(Random) sample of size N : A selection of N elements (x(1), x(2), . . ., x(N))
from the population. (For the requirements for a sample to be random see
Sect. 6.1.)</p>
<p/>
</div>
<div class="page"><p/>
<p>H. Collection of Formulas 493
</p>
<p>TableH.4: Samples from different populations.
</p>
<p>Sample of size n
from a continuously
distributed population
</p>
<p>Sample of size n from a
discrete population of size N .
Variable of the population is y,
variable of the sample is x
</p>
<p>Population
mean E(x)= x̂ ȳ =
</p>
<p>1
N
</p>
<p>N&sum;
j=1
</p>
<p>yj
</p>
<p>Population
variance σ
</p>
<p>2(x) σ 2(y)= 1
N&minus;1
</p>
<p>N&sum;
j=1
</p>
<p>(yj &minus; ȳ)2
</p>
<p>Sample
mean x̄ =
</p>
<p>1
n
</p>
<p>n&sum;
i=1
</p>
<p>xi x̄ = 1n
n&sum;
</p>
<p>i=1
xi
</p>
<p>Sample
variance
mean
</p>
<p>σ 2(x̄)= 1
n
σ 2(x) σ 2(x̄)= σ
</p>
<p>2(y)
n
</p>
<p>(
1&minus; n
</p>
<p>N
</p>
<p>)
</p>
<p>Variance of
the sample s
</p>
<p>2 = 1
n&minus;1
</p>
<p>n&sum;
i=1
</p>
<p>(xi &minus; x̄)2 s2 = 1n&minus;1
n&sum;
</p>
<p>i=1
(xi &minus; x̄)2
</p>
<p>Distribution function of the sample: Wn(x)= nx/N ,
where nx is the number of elements in the sample for which x &lt; x.
</p>
<p>Statistic: An arbitrary function of the elements of a sample
</p>
<p>S = S(x(1), x(2), . . . , x(N)) .
</p>
<p>Estimator: A statistic used to estimate a parameter λ of the population. An
estimator is unbiased, if E(S)= λ and consistent, if
</p>
<p>lim
N&rarr;&infin;
</p>
<p>σ(S)= 0 .
</p>
<p>Maximum-Likelihood Method
</p>
<p>Consider a population described by the probability density f (x,λ), where
λ = (λ1,λ2, . . . ,λp) is a set of parameters. If a sample x(1), x(2), . . ., x(N)
is obtained, then the likelihood function is L=
</p>
<p>&prod;N
j=1 f (x
</p>
<p>(j),λ) and the log-
likelihood function is ℓ= lnL. In order to determine the unknown parameters</p>
<p/>
</div>
<div class="page"><p/>
<p>494 H Collection of Formulas
</p>
<p>λ from the sample the maximum-likelihood method prescribes the value of λ
for which L (or ℓ) is a maximum. That is, one must solve the likelihood equa-
tion &part;ℓ/&part;λi = 0; i = 1,2, . . . ,p or (for only one parameter) dℓ/dλ= ℓ&prime; = 0.
Information of a sample: I (λ)= E(ℓ&prime;2)=&minus;E(ℓ&prime;&prime;)
Information inequality: σ 2(S) &ge; {1&minus;B &prime;(λ)}2/I (λ)
Here S is an estimator for λ, and B(λ)= E(S)&minus;λ is its bias.
An estimator has minimum variance if ℓ&prime; = A(λ)(S &minus;E(S)), where A(λ)
does not depend on the sample.
</p>
<p>The maximum-likelihood estimator λ̃, i.e., the solution of the likelihood equa-
tion, is unique, asymptotically unbiased (i.e., for N &rarr;&infin;), and has minimum
variance.
</p>
<p>The asymptotic form of the likelihood function for one parameter λ is
</p>
<p>L= const &middot; exp
(
&minus;(λ&minus; λ̃)
</p>
<p>2
</p>
<p>2b2
</p>
<p>)
,
</p>
<p>b2 = σ 2(̃λ)= 1
E(ℓ&prime;2(̃λ))
</p>
<p>=&minus; 1
E(ℓ&prime;&prime;(̃λ))
</p>
<p>and for several parameters
</p>
<p>L= const &middot;
{
&minus;1
</p>
<p>2
(λ&minus; λ̃)TB(λ&minus; λ̃)
</p>
<p>}
</p>
<p>with the covariance matrix C = B&minus;1 and
</p>
<p>Bij =&minus;E
(
</p>
<p>&part;2ℓ
</p>
<p>&part;λi&part;λj
</p>
<p>)
</p>
<p>λ=λ̃
.
</p>
<p>Testing Hypotheses
</p>
<p>Null hypothesis H0(λ= λ0): Assumption of values for the parameters λ that
determine the probability distribution f (x,λ) of a population.
</p>
<p>Alternative hypotheses H1(λ = λ1), H2(λ = λ2), . . .: Other possibilities for
λ, against which the null hypothesis is to be tested by consideration of a sam-
ple X = (x(1), x(2), . . . , x(N)) from the population.
A hypothesis is simple if the parameters are completely determined, e.g.,
H0(λ1 = 1, λ2 = 5), otherwise it is composite, e.g., H1(λ1 = 2, λ2 &lt; 7).
Test of a hypothesis H0 with a significance level α or confidence level 1&minus;α:
H0 is rejected if X &isin; Sc, where Sc is the critical region in the sample space
and
</p>
<p>P (X &isin; Sc|H0)= α .</p>
<p/>
</div>
<div class="page"><p/>
<p>H. Collection of Formulas 495
</p>
<p>Error of the first kind: Rejection of H0, although H0 is true. The probability
of this error is α.
</p>
<p>Error of the second kind: H0 is not rejected although H1 is true. The proba-
bility of this error is P (X �&isin; Sc|H1)= β.
Power function:
</p>
<p>M(Sc, λ)= P (X &isin; Sc|H)= P (X &isin; Sc|λ) .
</p>
<p>Operating characteristic function:
</p>
<p>L(Sc, λ)= 1&minus;M(Sc, λ) .
</p>
<p>Most powerful test of H0 with respect to H1 has M(Sc, λ1) = 1&minus;β = max.
A uniformly most powerful test is a most powerful test with respect to all pos-
sible H1.
</p>
<p>An unbiased test has M(Sc, λ1) &ge; α for all possible H1.
NEYMAN&ndash;PEARSON LEMMA: A test of H0 with respect to H1 (both simple
hypotheses) with the critical region Sc is a most powerful test if f (X|H0)/
f (X|H1) &le; c for X &isin; Sc and &ge; c for X �&isin; Sc, where c is a constant only de-
pending on α.
</p>
<p>Test statistic T (X): Scalar function of the sample X. By means of a mapping
X &rarr; T (X), Sc(X) &rarr; U the question as to whether X &isin; Sc can be reformu-
lated as T &isin; U .
Likelihood-ratio test: If ω denotes the region in the parameter space corre-
sponding to the null hypothesis and Ω denotes the entire possible parameter
region, then the test statistic
</p>
<p>T = f (x; λ̃(Ω))/f (x; λ̃(ω))
</p>
<p>is used. Here λ̃
(Ω)
</p>
<p>and λ̃
(ω)
</p>
<p>are the maximum-likelihood estimators in the
regions Ω and ω. H0 is rejected if T &gt; T1&minus;α with P (T &gt; T1&minus;α|H0) =&int;&infin;
T1&minus;α
</p>
<p>g(T )dT = α; g(T ) is the conditional probability density of T for a
given H0.
</p>
<p>WILKS theorem (holds for weak requirements concerning the probability den-
sity of the population): If H0 specifies p&minus; r out of the p parameters, then
&minus;2lnT (where T is the likelihood-ratio test statistic) follows a χ2-distribution
with f = p&minus; r degrees of freedom in the limit n&rarr;&infin;.
</p>
<p>χ2-Test for Goodness-of-Fit
</p>
<p>Hypothesis: The N measured values yi with normally distributed errors σi are
described by given quantities fi .</p>
<p/>
</div>
<div class="page"><p/>
<p>496 H Collection of Formulas
</p>
<p>TableH.5: Frequently used statistical tests for a sample from a normal distribution with mean
λ and variance σ 2. (Case 1: σ known; case 2: σ unknown (Student&rsquo;s test); case 3: χ2-test
of the variance; case 4: Student&rsquo;s difference test of two samples of sizes N1 and N2 on the
significance of s2Δ, cf. (8.3.19); case 5: F -test of two samples).
</p>
<p>Case Test statistic Null hy-pothesis
Critical region
for test statistic
</p>
<p>Number of
degrees of
freedom
</p>
<p>1
</p>
<p>T = x̄&minus;λ0
σ/
</p>
<p>&radic;
N
,
</p>
<p>x̄ = 1
N
</p>
<p>N&sum;
j=1
</p>
<p>x(j)
</p>
<p>λ= λ0
λ&le; λ0
λ&ge; λ0
</p>
<p>|T |&gt;Ω(1&minus;α/2)
T &gt; Ω(1&minus;α)
T &lt; Ω(α)
</p>
<p>&ndash;
</p>
<p>2
</p>
<p>T = x̄&minus;λ0
s/
&radic;
N
,
</p>
<p>s2 = 1
N&minus;1
</p>
<p>&times;
N&sum;
j=1
</p>
<p>(x(j)&minus; x̄)2
</p>
<p>λ= λ0
λ&le; λ0
λ&ge; λ0
</p>
<p>|T |&gt; t1&minus;α/2
T &gt; t1&minus;α
T &lt;&minus;t1&minus;α = tα
</p>
<p>N&minus;1
</p>
<p>3 T = (N &minus;1) s2
σ 20
</p>
<p>σ 2 = σ 20
σ 2 &le; σ 20
σ 2 &ge; σ 20
</p>
<p>χ21&minus;α/2 &lt; T &lt; χ
2
α/2
</p>
<p>T &gt; χ21&minus;α
T &lt; χ2α
</p>
<p>N&minus;1
</p>
<p>4 T = x̄1&minus;x̄2
sΔ
</p>
<p>λ1 = λ2 |T |&gt; t1&minus; 12α N1 +N2 &minus;2
</p>
<p>5
</p>
<p>T = s21/s22 ,
s2i = 1Ni&minus;1
</p>
<p>&times;
Ni&sum;
j=1
</p>
<p>(x
(j)
i &minus; x̄i)2
</p>
<p>σ 21 = σ 22
σ 21 &le; σ 22
σ 21 &ge; σ 22
</p>
<p>F1&minus;α/2 &lt; T &lt; Fα/2
T &gt; F1&minus;α
T &lt; Fα
</p>
<p>f1 =N1 &minus;1
f2 =N2 &minus;1</p>
<p/>
</div>
<div class="page"><p/>
<p>H. Collection of Formulas 497
</p>
<p>Test function: T =
&sum;N
</p>
<p>i=1(yi &minus;fi)2/σ 2i .
Critical region: T &gt; χ21&minus;α.
</p>
<p>Number of degrees of freedom: N (or N &minus;p, if p parameters are determined
from the measurements).
</p>
<p>The Method of Least Squares
</p>
<p>One considers a set of m equations fk(x,η) = 0; k = 1, . . . , m relating the
r-vector of the unknowns x = (x1, x2, . . . , xr) to the n-vector of measurable
quantities η = (η1, η2, . . . , ηn). Instead of η, the quantities y are measured,
which deviate from them by the measurement errors ε, i.e., y = η+ ε. The
quantities ε are assumed to be normally distributed about zero. This is ex-
pressed by the covariance matrix Cy =G&minus;1y . In order to obtain the solution x̃,
η̃, one expands the fk for the first approximations x0, η0 = y. Only the linear
terms of the expansion are kept and the second approximation x1 = x0 + ξ,
η1 = η+ δ is computed. The procedure is repeated iteratively until certain
convergence criteria are met, for example, until a scalar function M no longer
decreases. If the fk are linear in x and η, then only one step is necessary.
The method can be interpreted as a procedure to minimize M . The function
M corresponding to the solution depends on the measurement errors. It is
a random variable and follows a χ2-distribution with f = m&minus; r degrees of
freedom. It can therefore be used for a χ2-test of the goodness-of-fit or of
other assumptions, in particular the assumption Cy =G&minus;1y . If the errors ε are
not normally distribution, but still distributed symmetrically about zero, then
the least-squares solution x̃ still has the smallest possible variance, and one
has E(M)=m&minus; r (Gauss&ndash;Markov theorem).</p>
<p/>
</div>
<div class="page"><p/>
<p>498 H Collection of Formulas
</p>
<p>TableH.6: Least squares in the general case and in the case of constrained measurements.
General case Constrained measurements
</p>
<p>Equations fk(x,η)= 0, k = 1, . . . ,m fk(η)= 0
First approx-
imations
</p>
<p>x0, η0 = y η0 = y
f= Aξ+Bδ+ c+&middot;&middot; &middot; f= Bδ+ c+&middot;&middot; &middot;
</p>
<p>Equations
expanded
</p>
<p>{A}kl = (&part;fk/&part;xl)x0,η0
{B}kl = (&part;fk/&part;ηl)x0,η0
c= f(x0, η0)
</p>
<p>&ndash;
{B}kl = (&part;fk/&part;ηl)η0
c= f(η0)
</p>
<p>Covariance
matrix of the
measurements
</p>
<p>Cy =G&minus;1y Cy =G&minus;1y
</p>
<p>Corrections
ξ̃ =&minus;(ATGBA)&minus;1ATGBc
δ̃=&minus;G&minus;1y BTGB(Ãξ+ c)
GB = (BG&minus;1y BT)&minus;1
</p>
<p>&ndash;
δ̃=&minus;G&minus;1y BTGBc
GB = (BG&minus;1y BT)&minus;1
</p>
<p>Next
step
</p>
<p>x1 = x0 + ξ̃, η1 = η0 + δ̃,
new values for A, B, c, ξ̃, δ̃
</p>
<p>η1 = η0 + δ̃,
new values for B, c, δ̃
</p>
<p>Solution
(after s
steps)
</p>
<p>x̃= xs&minus;1 + ξ̃,
η̃= ηs&minus;1 + δ̃,
ε̃= y&minus; η̃
</p>
<p>η̃= ηs&minus;1 + δ̃,
ε̃= y&minus; η̃
</p>
<p>Minimum
function M = (B ε̃)
</p>
<p>TGB(B ε̃) M = (B ε̃)TGB(B ε̃)
</p>
<p>Covariance
matrices
</p>
<p>G&minus;1x̃ = (ATGBA)&minus;1
G&minus;1η̃ =G&minus;1y
&minus;G&minus;1y BTGBBG&minus;1y
+G&minus;1y BTGBA(ATGBA)&minus;1
&times;ATGBBG&minus;1y
</p>
<p>G&minus;1η̃ =G&minus;1y
&minus;G&minus;1y BTGBBG&minus;1y</p>
<p/>
</div>
<div class="page"><p/>
<p>H. Collection of Formulas 499
</p>
<p>TableH.7: Least squares for indirect and direct measurements.
</p>
<p>Indirect measurements
Direct measurements
of different
accuracy
</p>
<p>Equations fk = ηk&minus;gk(x)= 0 fk = ηk&minus;x
First approx-
imations
</p>
<p>x0, η0 = y x0 = 0, η0 = y
</p>
<p>Equations
expanded
</p>
<p>f= Ãξ&minus; ε+ c+&middot;&middot; &middot;
{A}kl =
</p>
<p>(
&part;fk
&part;xl
</p>
<p>)
x0
</p>
<p>c= y&minus;g(x0)
</p>
<p>f= Ãξ+ ε+ c
</p>
<p>A=&minus;
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>1
1
...
</p>
<p>1
</p>
<p>⎞
⎟⎟⎟⎠
</p>
<p>c= y
</p>
<p>Covariance
matrix of the
measurements
</p>
<p>Cy =G&minus;1y
</p>
<p>Cy =G&minus;1y
</p>
<p>=
</p>
<p>⎛
⎜⎝
</p>
<p>σ 21 0
. . .
</p>
<p>0 σ 2n
</p>
<p>⎞
⎟⎠
</p>
<p>Corrections ξ̃ =&minus;(ATGyA)&minus;1ATGyc &ndash;
</p>
<p>Next
step
</p>
<p>x1 = x0 + ξ̃,
new values for A, c, ξ̃
</p>
<p>&ndash;
</p>
<p>Solution
(after s
steps)
</p>
<p>x̃= xs&minus;1 + ξ̃,
ε̃= Ãξ+ c
</p>
<p>ξ̃ = x̃ =
&sum;
k
</p>
<p>yk/σ
2
k
</p>
<p>&sum;
k
</p>
<p>1/σ 2k
,
</p>
<p>ε̃k = yk&minus; x̃
Minimum
function M = ε̃
</p>
<p>TGy ε̃ M = ε̃TGy ε̃
</p>
<p>Covariance
matrices G
</p>
<p>&minus;1
x̃ = (ATGyA)&minus;1
</p>
<p>G&minus;1x̃ = σ 2(̃x)
</p>
<p>=
(&sum;
</p>
<p>k
</p>
<p>1
σ 2k
</p>
<p>)&minus;1
</p>
<p>G&minus;1η̃ = A(ATGyA)AT &ndash;</p>
<p/>
</div>
<div class="page"><p/>
<p>500 H Collection of Formulas
</p>
<p>Analysis of Variance
</p>
<p>The influence of external variables on a measured random variable x is in-
vestigated. One tries to decide by means of appropriately constructed F -tests
whether x is independent of the external variables. Various models are con-
structed depending on the number of external variables and the assumptions
concerning their influence.
</p>
<p>A simple model is the crossed two-way classification with multiple observa-
tions. Two external variables are used to classify the observations into the
classes Ai , Bj (i = 1,2, . . . , I ; j = 1,2, . . . ,J ). Each class Ai , Bj contains K
observations xijk (k = 1,2, . . . ,K). One assumes the model
</p>
<p>xijk = μ+ai +bj + (ab)ij + εijk ,
</p>
<p>where the error of the observation εijk is assumed to be normally distributed
about zero and ai , bj , and (ab)ij are the influences of the classifications in A,
B and their interactions. Three null hypotheses
</p>
<p>H
(A)
0 (ai = 0, i = 1, . . . , I ) , H
</p>
<p>(B)
0 (bj = 0, j = 1, . . . ,J ) ,
</p>
<p>H
(AB)
0 ((ab)ij = 0, i = 1, . . . , I, j = 1, . . . ,J )
</p>
<p>can be tested with the ratios F (A), F (B), F (AB). They are summarized in an
analysis of variance (ANOVA) table. For other models see Chap. 11.
</p>
<p>Polynomial Regression
</p>
<p>Problem: The true values η(t), for which one has N measurements yi(ti)
with normally distributed measurement errors σi , are to be described by a
polynomial of order r &minus; 1 in the controlled variable t . Instead of η(t) =
x1 +x2t+&middot;&middot; &middot;+xr t r&minus;1 one writes
</p>
<p>η(t)= x1f1(t)+x2f2(t)+&middot;&middot; &middot;+xrfr(t) .
</p>
<p>Here the fj are orthogonal polynomials of order j &minus;1,
</p>
<p>fj (t)=
j&sum;
</p>
<p>k=1
bjkt
</p>
<p>k&minus;1 ,
</p>
<p>whose coefficients bjk are determined by the orthogonality conditions
</p>
<p>N&sum;
</p>
<p>i=1
gifj (ti)fk(ti)= δjk , gi = 1/σ 2i .</p>
<p/>
</div>
<div class="page"><p/>
<p>H. Collection of Formulas 501
</p>
<p>The unknowns xj are obtained by least squares from
</p>
<p>N&sum;
</p>
<p>i=1
gi
</p>
<p>⎧
⎨
⎩yi(ti)&minus;
</p>
<p>r&sum;
</p>
<p>j=1
xjfj (ti)
</p>
<p>⎫
⎬
⎭
</p>
<p>2
</p>
<p>= min .
</p>
<p>The covariance matrix of the xj is the r-dimensional unit matrix.
</p>
<p>Time Series Analysis
</p>
<p>One is given a series of measured values yi(ti), i = 1, . . . ,n, which (in an
unknown way) depend on a controlled variable t (usually time). One treats
the yi as the sum of a trend ηi and an error εi , yi = ηi+εi . The measurements
are carried out at regular time intervals, i.e., ti &minus; ti&minus;1 = const. In order to
minimize the errors εi , a moving average is constructed for every ti (i &gt; k,
i &le; n&minus; k), by fitting a polynomial of order ℓ to the 2k+ 1 measurements
situated symmetrically about measurement i. The result of the fit at the point
ti is the moving average
</p>
<p>η̃0(i)= a&minus;kyi&minus;k +a&minus;k+1yi&minus;k+1 +&middot;&middot; &middot;+akyi+k .
</p>
<p>The coefficients a&minus;k, . . ., ak are given in Table 13.1 for low values of k and ℓ.
For the beginning and end points ti (i &lt; k, i &gt; n&minus;k), the results of the fit can
be used with caution also for points other than at the center of the interval of
the 2k+1 measurements.</p>
<p/>
</div>
<div class="page"><p/>
<p>I. Statistical Tables
</p>
<p>TableI.1: Quantiles λP (k) of the Poisson distribution.
</p>
<p>P =
k&minus;1&sum;
</p>
<p>n=0
e&minus;λP λnP /n!
</p>
<p>P
</p>
<p>k 0.0005 0.0010 0.0050 0.0100 0.0250 0.0500 0.1000
1 7.601 6.908 5.298 4.605 3.689 2.996 2.303
2 9.999 9.233 7.430 6.638 5.572 4.744 3.890
3 12.051 11.229 9.274 8.406 7.225 6.296 5.322
4 13.934 13.062 10.977 10.045 8.767 7.754 6.681
5 15.710 14.794 12.594 11.605 10.242 9.154 7.994
6 17.411 16.455 14.150 13.108 11.668 10.513 9.275
7 19.055 18.062 15.660 14.571 13.059 11.842 10.532
8 20.654 19.626 17.134 16.000 14.423 13.148 11.771
9 22.217 21.156 18.578 17.403 15.763 14.435 12.995
</p>
<p>10 23.749 22.657 19.998 18.783 17.085 15.705 14.206
11 25.256 24.134 21.398 20.145 18.390 16.962 15.407
12 26.739 25.589 22.779 21.490 19.682 18.208 16.598
13 28.203 27.026 24.145 22.821 20.962 19.443 17.782
14 29.650 28.446 25.497 24.139 22.230 20.669 18.958
15 31.081 29.852 26.836 25.446 23.490 21.886 20.128
16 32.498 31.244 28.164 26.743 24.740 23.097 21.292
17 33.902 32.624 29.482 28.030 25.983 24.301 22.452
18 35.294 33.993 30.791 29.310 27.219 25.499 23.606
19 36.676 35.351 32.091 30.581 28.448 26.692 24.756
20 38.047 36.701 33.383 31.845 29.671 27.879 25.903
21 39.410 38.042 34.668 33.103 30.888 29.062 27.045
22 40.764 39.375 35.946 34.355 32.101 30.240 28.184
23 42.110 40.700 37.218 35.601 33.308 31.415 29.320
24 43.449 42.019 38.484 36.841 34.511 32.585 30.453
25 44.780 43.330 39.745 38.077 35.710 33.752 31.584
</p>
<p>S. Brandt, Data Analysis: Statistical and Computational Methods for Scientists and Engineers,
DOI 10.1007/978-3-319-03762-2, &copy; Springer International Publishing Switzerland 2014
</p>
<p>503</p>
<p/>
</div>
<div class="page"><p/>
<p>504 I Statistical Tables
</p>
<p>TableI.1: (continued)
</p>
<p>P =
k&minus;1&sum;
</p>
<p>n=0
e&minus;λP λnP /n!
</p>
<p>P
</p>
<p>k 0.9000 0.9500 0.9750 0.9900 0.9950 0.9990 0.9995
1 0.105 0.051 0.025 0.010 0.005 0.001 0.001
2 0.532 0.355 0.242 0.149 0.103 0.045 0.032
3 1.102 0.818 0.619 0.436 0.338 0.191 0.150
4 1.745 1.366 1.090 0.823 0.672 0.429 0.355
5 2.433 1.970 1.623 1.279 1.078 0.739 0.632
6 3.152 2.613 2.202 1.785 1.537 1.107 0.967
7 3.895 3.285 2.814 2.330 2.037 1.520 1.348
8 4.656 3.981 3.454 2.906 2.571 1.971 1.768
9 5.432 4.695 4.115 3.507 3.132 2.452 2.220
</p>
<p>10 6.221 5.425 4.795 4.130 3.717 2.961 2.699
11 7.021 6.169 5.491 4.771 4.321 3.491 3.202
12 7.829 6.924 6.201 5.428 4.943 4.042 3.726
13 8.646 7.690 6.922 6.099 5.580 4.611 4.269
14 9.470 8.464 7.654 6.782 6.231 5.195 4.828
15 10.300 9.246 8.395 7.477 6.893 5.794 5.402
16 11.135 10.036 9.145 8.181 7.567 6.405 5.990
17 11.976 10.832 9.903 8.895 8.251 7.028 6.590
18 12.822 11.634 10.668 9.616 8.943 7.662 7.201
19 13.671 12.442 11.439 10.346 9.644 8.306 7.822
20 14.525 13.255 12.217 11.082 10.353 8.958 8.453
21 15.383 14.072 12.999 11.825 11.069 9.619 9.093
22 16.244 14.894 13.787 12.574 11.792 10.288 9.741
23 17.108 15.719 14.580 13.329 12.521 10.964 10.397
24 17.975 16.549 15.377 14.089 13.255 11.647 11.060
25 18.844 17.382 16.179 14.853 13.995 12.337 11.730</p>
<p/>
</div>
<div class="page"><p/>
<p>I. Statistical Tables 505
</p>
<p>TableI.2: Normal distribution ψ0(x).
</p>
<p>P (x &lt; x)= ψ0(x)=
1&radic;
2π
</p>
<p>&int; x
</p>
<p>&minus;&infin;
exp(&minus;x2/2)dx
</p>
<p>x 0 1 2 3 4 5 6 7 8 9
&minus;3.0 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001
&minus;2.9 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.001 0.001 0.001
&minus;2.8 0.003 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
&minus;2.7 0.003 0.003 0.003 0.003 0.003 0.003 0.003 0.003 0.003 0.003
&minus;2.6 0.005 0.005 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004
&minus;2.5 0.006 0.006 0.006 0.006 0.006 0.005 0.005 0.005 0.005 0.005
&minus;2.4 0.008 0.008 0.008 0.008 0.007 0.007 0.007 0.007 0.007 0.006
&minus;2.3 0.011 0.010 0.010 0.010 0.010 0.009 0.009 0.009 0.009 0.008
&minus;2.2 0.014 0.014 0.013 0.013 0.013 0.012 0.012 0.012 0.011 0.011
&minus;2.1 0.018 0.017 0.017 0.017 0.016 0.016 0.015 0.015 0.015 0.014
&minus;2.0 0.023 0.022 0.022 0.021 0.021 0.020 0.020 0.019 0.019 0.018
&minus;1.9 0.029 0.028 0.027 0.027 0.026 0.026 0.025 0.024 0.024 0.023
&minus;1.8 0.036 0.035 0.034 0.034 0.033 0.032 0.031 0.031 0.030 0.029
&minus;1.7 0.045 0.044 0.043 0.042 0.041 0.040 0.039 0.038 0.038 0.037
&minus;1.6 0.055 0.054 0.053 0.052 0.051 0.049 0.048 0.047 0.046 0.046
&minus;1.5 0.067 0.066 0.064 0.063 0.062 0.061 0.059 0.058 0.057 0.056
&minus;1.4 0.081 0.079 0.078 0.076 0.075 0.074 0.072 0.071 0.069 0.068
&minus;1.3 0.097 0.095 0.093 0.092 0.090 0.089 0.087 0.085 0.084 0.082
&minus;1.2 0.115 0.113 0.111 0.109 0.107 0.106 0.104 0.102 0.100 0.099
&minus;1.1 0.136 0.133 0.131 0.129 0.127 0.125 0.123 0.121 0.119 0.117
&minus;1.0 0.159 0.156 0.154 0.152 0.149 0.147 0.145 0.142 0.140 0.138
&minus;0.9 0.184 0.181 0.179 0.176 0.174 0.171 0.169 0.166 0.164 0.161
&minus;0.8 0.212 0.209 0.206 0.203 0.200 0.198 0.195 0.192 0.189 0.187
&minus;0.7 0.242 0.239 0.236 0.233 0.230 0.227 0.224 0.221 0.218 0.215
&minus;0.6 0.274 0.271 0.268 0.264 0.261 0.258 0.255 0.251 0.248 0.245
&minus;0.5 0.309 0.305 0.302 0.298 0.295 0.291 0.288 0.284 0.281 0.278
&minus;0.4 0.345 0.341 0.337 0.334 0.330 0.326 0.323 0.319 0.316 0.312
&minus;0.3 0.382 0.378 0.374 0.371 0.367 0.363 0.359 0.356 0.352 0.348
&minus;0.2 0.421 0.417 0.413 0.409 0.405 0.401 0.397 0.394 0.390 0.386
&minus;0.1 0.460 0.456 0.452 0.448 0.444 0.440 0.436 0.433 0.429 0.425
</p>
<p>0.0 0.500 0.496 0.492 0.488 0.484 0.480 0.476 0.472 0.468 0.464</p>
<p/>
</div>
<div class="page"><p/>
<p>506 I Statistical Tables
</p>
<p>TableI.2: (continued)
</p>
<p>P (x &lt; x)= ψ0(x)=
1&radic;
2π
</p>
<p>&int; x
</p>
<p>&minus;&infin;
exp(&minus;x2/2)dx
</p>
<p>x 0 1 2 3 4 5 6 7 8 9
0.0 0.500 0.504 0.508 0.512 0.516 0.520 0.524 0.528 0.532 0.536
0.1 0.540 0.544 0.548 0.552 0.556 0.560 0.564 0.567 0.571 0.575
0.2 0.579 0.583 0.587 0.591 0.595 0.599 0.603 0.606 0.610 0.614
0.3 0.618 0.622 0.626 0.629 0.633 0.637 0.641 0.644 0.648 0.652
0.4 0.655 0.659 0.663 0.666 0.670 0.674 0.677 0.681 0.684 0.688
0.5 0.691 0.695 0.698 0.702 0.705 0.709 0.712 0.716 0.719 0.722
0.6 0.726 0.729 0.732 0.736 0.739 0.742 0.745 0.749 0.752 0.755
0.7 0.758 0.761 0.764 0.767 0.770 0.773 0.776 0.779 0.782 0.785
0.8 0.788 0.791 0.794 0.797 0.800 0.802 0.805 0.808 0.811 0.813
0.9 0.816 0.819 0.821 0.824 0.826 0.829 0.831 0.834 0.836 0.839
1.0 0.841 0.844 0.846 0.848 0.851 0.853 0.855 0.858 0.860 0.862
1.1 0.864 0.867 0.869 0.871 0.873 0.875 0.877 0.879 0.881 0.883
1.2 0.885 0.887 0.889 0.891 0.893 0.894 0.896 0.898 0.900 0.901
1.3 0.903 0.905 0.907 0.908 0.910 0.911 0.913 0.915 0.916 0.918
1.4 0.919 0.921 0.922 0.924 0.925 0.926 0.928 0.929 0.931 0.932
1.5 0.933 0.934 0.936 0.937 0.938 0.939 0.941 0.942 0.943 0.944
1.6 0.945 0.946 0.947 0.948 0.949 0.951 0.952 0.953 0.954 0.954
1.7 0.955 0.956 0.957 0.958 0.959 0.960 0.961 0.962 0.962 0.963
1.8 0.964 0.965 0.966 0.966 0.967 0.968 0.969 0.969 0.970 0.971
1.9 0.971 0.972 0.973 0.973 0.974 0.974 0.975 0.976 0.976 0.977
2.0 0.977 0.978 0.978 0.979 0.979 0.980 0.980 0.981 0.981 0.982
2.1 0.982 0.983 0.983 0.983 0.984 0.984 0.985 0.985 0.985 0.986
2.2 0.986 0.986 0.987 0.987 0.987 0.988 0.988 0.988 0.989 0.989
2.3 0.989 0.990 0.990 0.990 0.990 0.991 0.991 0.991 0.991 0.992
2.4 0.992 0.992 0.992 0.992 0.993 0.993 0.993 0.993 0.993 0.994
2.5 0.994 0.994 0.994 0.994 0.994 0.995 0.995 0.995 0.995 0.995
2.6 0.995 0.995 0.996 0.996 0.996 0.996 0.996 0.996 0.996 0.996
2.7 0.997 0.997 0.997 0.997 0.997 0.997 0.997 0.997 0.997 0.997
2.8 0.997 0.998 0.998 0.998 0.998 0.998 0.998 0.998 0.998 0.998
2.9 0.998 0.998 0.998 0.998 0.998 0.998 0.998 0.999 0.999 0.999
3.0 0.999 0.999 0.999 0.999 0.999 0.999 0.999 0.999 0.999 0.999</p>
<p/>
</div>
<div class="page"><p/>
<p>I. Statistical Tables 507
</p>
<p>TableI.3: Normal distribution 2ψ0(x)&minus;1.
</p>
<p>P (|x|&lt; x)= 2ψ0(x)&minus;1 =
1&radic;
2π
</p>
<p>&int; x
</p>
<p>&minus;x
exp(&minus;x2/2)dx
</p>
<p>x 0 1 2 3 4 5 6 7 8 9
0.0 0.000 0.008 0.016 0.024 0.032 0.040 0.048 0.056 0.064 0.072
0.1 0.080 0.088 0.096 0.103 0.111 0.119 0.127 0.135 0.143 0.151
0.2 0.159 0.166 0.174 0.182 0.190 0.197 0.205 0.213 0.221 0.228
0.3 0.236 0.243 0.251 0.259 0.266 0.274 0.281 0.289 0.296 0.303
0.4 0.311 0.318 0.326 0.333 0.340 0.347 0.354 0.362 0.369 0.376
0.5 0.383 0.390 0.397 0.404 0.411 0.418 0.425 0.431 0.438 0.445
0.6 0.451 0.458 0.465 0.471 0.478 0.484 0.491 0.497 0.503 0.510
0.7 0.516 0.522 0.528 0.535 0.541 0.547 0.553 0.559 0.565 0.570
0.8 0.576 0.582 0.588 0.593 0.599 0.605 0.610 0.616 0.621 0.627
0.9 0.632 0.637 0.642 0.648 0.653 0.658 0.663 0.668 0.673 0.678
1.0 0.683 0.688 0.692 0.697 0.702 0.706 0.711 0.715 0.720 0.724
1.1 0.729 0.733 0.737 0.742 0.746 0.750 0.754 0.758 0.762 0.766
1.2 0.770 0.774 0.778 0.781 0.785 0.789 0.792 0.796 0.799 0.803
1.3 0.806 0.810 0.813 0.816 0.820 0.823 0.826 0.829 0.832 0.835
1.4 0.838 0.841 0.844 0.847 0.850 0.853 0.856 0.858 0.861 0.864
1.5 0.866 0.869 0.871 0.874 0.876 0.879 0.881 0.884 0.886 0.888
1.6 0.890 0.893 0.895 0.897 0.899 0.901 0.903 0.905 0.907 0.909
1.7 0.911 0.913 0.915 0.916 0.918 0.920 0.922 0.923 0.925 0.927
1.8 0.928 0.930 0.931 0.933 0.934 0.936 0.937 0.939 0.940 0.941
1.9 0.943 0.944 0.945 0.946 0.948 0.949 0.950 0.951 0.952 0.953
2.0 0.954 0.956 0.957 0.958 0.959 0.960 0.961 0.962 0.962 0.963
2.1 0.964 0.965 0.966 0.967 0.968 0.968 0.969 0.970 0.971 0.971
2.2 0.972 0.973 0.974 0.974 0.975 0.976 0.976 0.977 0.977 0.978
2.3 0.979 0.979 0.980 0.980 0.981 0.981 0.982 0.982 0.983 0.983
2.4 0.984 0.984 0.984 0.985 0.985 0.986 0.986 0.986 0.987 0.987
2.5 0.988 0.988 0.988 0.989 0.989 0.989 0.990 0.990 0.990 0.990
2.6 0.991 0.991 0.991 0.991 0.992 0.992 0.992 0.992 0.993 0.993
2.7 0.993 0.993 0.993 0.994 0.994 0.994 0.994 0.994 0.995 0.995
2.8 0.995 0.995 0.995 0.995 0.995 0.996 0.996 0.996 0.996 0.996
2.9 0.996 0.996 0.996 0.997 0.997 0.997 0.997 0.997 0.997 0.997
3.0 0.997 0.997 0.997 0.998 0.998 0.998 0.998 0.998 0.998 0.998</p>
<p/>
</div>
<div class="page"><p/>
<p>508 I Statistical Tables
</p>
<p>TableI.4: Quantiles xP =Ω(P) of the normal distribution.
</p>
<p>P = 1&radic;
2π
</p>
<p>&int; xP
&minus;&infin;
</p>
<p>exp(&minus;x2/2)dx
</p>
<p>P 0 1 2 3 4 5 6 7 8 9
0.0 &minus;&infin; &minus;2.33 &minus;2.05 &minus;1.88 &minus;1.75 &minus;1.64 &minus;1.55 &minus;1.48 &minus;1.41 &minus;1.34
0.1 &minus;1.28 &minus;1.23 &minus;1.17 &minus;1.13 &minus;1.08 &minus;1.04 &minus;0.99 &minus;0.95 &minus;0.92 &minus;0.88
0.2 &minus;0.84 &minus;0.81 &minus;0.77 &minus;0.74 &minus;0.71 &minus;0.67 &minus;0.64 &minus;0.61 &minus;0.58 &minus;0.55
0.3 &minus;0.52 &minus;0.50 &minus;0.47 &minus;0.44 &minus;0.41 &minus;0.39 &minus;0.36 &minus;0.33 &minus;0.31 &minus;0.28
0.4 &minus;0.25 &minus;0.23 &minus;0.20 &minus;0.18 &minus;0.15 &minus;0.13 &minus;0.10 &minus;0.08 &minus;0.05 &minus;0.03
0.5 0.00 0.03 0.05 0.08 0.10 0.13 0.15 0.18 0.20 0.23
0.6 0.25 0.28 0.31 0.33 0.36 0.39 0.41 0.44 0.47 0.50
0.7 0.52 0.55 0.58 0.61 0.64 0.67 0.71 0.74 0.77 0.81
0.8 0.84 0.88 0.92 0.95 0.99 1.04 1.08 1.13 1.17 1.23
0.9 1.28 1.34 1.41 1.48 1.55 1.64 1.75 1.88 2.05 2.33</p>
<p/>
</div>
<div class="page"><p/>
<p>I. Statistical Tables 509
</p>
<p>TableI.5: Quantiles x &prime;P =Ω &prime;(P ) of the normal distribution.
</p>
<p>P = 1&radic;
2π
</p>
<p>&int; x&prime;P
&minus;x&prime;P
</p>
<p>exp(&minus;x2/2)dx
</p>
<p>P 0 1 2 3 4 5 6 7 8 9
0.0 0.000 0.013 0.025 0.038 0.050 0.063 0.075 0.088 0.100 0.113
0.1 0.126 0.138 0.151 0.164 0.176 0.189 0.202 0.215 0.228 0.240
0.2 0.253 0.266 0.279 0.292 0.305 0.319 0.332 0.345 0.358 0.372
0.3 0.385 0.399 0.412 0.426 0.440 0.454 0.468 0.482 0.496 0.510
0.4 0.524 0.539 0.553 0.568 0.583 0.598 0.613 0.628 0.643 0.659
0.5 0.674 0.690 0.706 0.722 0.739 0.755 0.772 0.789 0.806 0.824
0.6 0.842 0.860 0.878 0.896 0.915 0.935 0.954 0.974 0.994 1.015
0.7 1.036 1.058 1.080 1.103 1.126 1.150 1.175 1.200 1.227 1.254
0.8 1.282 1.311 1.341 1.372 1.405 1.440 1.476 1.514 1.555 1.598
0.9 1.645 1.695 1.751 1.812 1.881 1.960 2.054 2.170 2.326 2.576
</p>
<p>P 0 1 2 3 4 5 6 7 8 9
0.90 1.645 1.650 1.655 1.660 1.665 1.670 1.675 1.680 1.685 1.690
0.91 1.695 1.701 1.706 1.711 1.717 1.722 1.728 1.734 1.739 1.745
0.92 1.751 1.757 1.762 1.768 1.774 1.780 1.787 1.793 1.799 1.805
0.93 1.812 1.818 1.825 1.832 1.838 1.845 1.852 1.859 1.866 1.873
0.94 1.881 1.888 1.896 1.903 1.911 1.919 1.927 1.935 1.943 1.951
0.95 1.960 1.969 1.977 1.986 1.995 2.005 2.014 2.024 2.034 2.044
0.96 2.054 2.064 2.075 2.086 2.097 2.108 2.120 2.132 2.144 2.157
0.97 2.170 2.183 2.197 2.212 2.226 2.241 2.257 2.273 2.290 2.308
0.98 2.326 2.346 2.366 2.387 2.409 2.432 2.457 2.484 2.512 2.543
0.99 2.576 2.612 2.652 2.697 2.748 2.807 2.878 2.968 3.090 3.291
</p>
<p>P 0 1 2 3 4 5 6 7 8 9
0.990 2.576 2.579 2.583 2.586 2.590 2.594 2.597 2.601 2.605 2.608
0.991 2.612 2.616 2.620 2.624 2.628 2.632 2.636 2.640 2.644 2.648
0.992 2.652 2.656 2.661 2.665 2.669 2.674 2.678 2.683 2.687 2.692
0.993 2.697 2.702 2.706 2.711 2.716 2.721 2.727 2.732 2.737 2.742
0.994 2.748 2.753 2.759 2.765 2.770 2.776 2.782 2.788 2.794 2.801
0.995 2.807 2.814 2.820 2.827 2.834 2.841 2.848 2.855 2.863 2.870
0.996 2.878 2.886 2.894 2.903 2.911 2.920 2.929 2.938 2.948 2.958
0.997 2.968 2.978 2.989 3.000 3.011 3.023 3.036 3.048 3.062 3.076
0.998 3.090 3.105 3.121 3.138 3.156 3.175 3.195 3.216 3.239 3.264
0.999 3.291 3.320 3.353 3.390 3.432 3.481 3.540 3.615 3.719 3.891</p>
<p/>
</div>
<div class="page"><p/>
<p>510 I Statistical Tables
</p>
<p>TableI.6: χ2-distribution F(χ2).
</p>
<p>F(χ
2
)=
</p>
<p>&int; χ2
</p>
<p>0
f (χ
</p>
<p>2;f )dχ2
</p>
<p>f
</p>
<p>χ
2 1 2 3 4 5 6 7 8 9 10
</p>
<p>0.1 0.248 0.049 0.008 0.001 0.000 0.000 0.000 0.000 0.000 0.000
0.2 0.345 0.095 0.022 0.005 0.001 0.000 0.000 0.000 0.000 0.000
0.3 0.416 0.139 0.040 0.010 0.002 0.001 0.000 0.000 0.000 0.000
0.4 0.473 0.181 0.060 0.018 0.005 0.001 0.000 0.000 0.000 0.000
0.5 0.520 0.221 0.081 0.026 0.008 0.002 0.001 0.000 0.000 0.000
0.6 0.561 0.259 0.104 0.037 0.012 0.004 0.001 0.000 0.000 0.000
0.7 0.597 0.295 0.127 0.049 0.017 0.006 0.002 0.000 0.000 0.000
0.8 0.629 0.330 0.151 0.062 0.023 0.008 0.003 0.001 0.000 0.000
0.9 0.657 0.362 0.175 0.075 0.030 0.011 0.004 0.001 0.000 0.000
1.0 0.683 0.393 0.199 0.090 0.037 0.014 0.005 0.002 0.001 0.000
2.0 0.843 0.632 0.428 0.264 0.151 0.080 0.040 0.019 0.009 0.004
3.0 0.917 0.777 0.608 0.442 0.300 0.191 0.115 0.066 0.036 0.019
4.0 0.954 0.865 0.739 0.594 0.451 0.323 0.220 0.143 0.089 0.053
5.0 0.975 0.918 0.828 0.713 0.584 0.456 0.340 0.242 0.166 0.109
6.0 0.986 0.950 0.888 0.801 0.694 0.577 0.460 0.353 0.260 0.185
7.0 0.992 0.970 0.928 0.864 0.779 0.679 0.571 0.463 0.363 0.275
8.0 0.995 0.982 0.954 0.908 0.844 0.762 0.667 0.567 0.466 0.371
9.0 0.997 0.989 0.971 0.939 0.891 0.826 0.747 0.658 0.563 0.468
</p>
<p>10.0 0.998 0.993 0.981 0.960 0.925 0.875 0.811 0.735 0.650 0.560
11.0 0.999 0.996 0.988 0.973 0.949 0.912 0.861 0.798 0.724 0.642
12.0 0.999 0.998 0.993 0.983 0.965 0.938 0.899 0.849 0.787 0.715
13.0 1.000 0.998 0.995 0.989 0.977 0.957 0.928 0.888 0.837 0.776
14.0 1.000 0.999 0.997 0.993 0.984 0.970 0.949 0.918 0.878 0.827
15.0 1.000 0.999 0.998 0.995 0.990 0.980 0.964 0.941 0.909 0.868
16.0 1.000 1.000 0.999 0.997 0.993 0.986 0.975 0.958 0.933 0.900
17.0 1.000 1.000 0.999 0.998 0.996 0.991 0.983 0.970 0.951 0.926
18.0 1.000 1.000 1.000 0.999 0.997 0.994 0.988 0.979 0.965 0.945
19.0 1.000 1.000 1.000 0.999 0.998 0.996 0.992 0.985 0.975 0.960
20.0 1.000 1.000 1.000 1.000 0.999 0.997 0.994 0.990 0.982 0.971</p>
<p/>
</div>
<div class="page"><p/>
<p>I. Statistical Tables 511
</p>
<p>TableI.7: Quantiles χ2P of the χ
2-distribution.
</p>
<p>P =
&int; χ2P
</p>
<p>0
f (χ2;f )dχ2
</p>
<p>P
</p>
<p>f 0.900 0.950 0.990 0.995 0.999
1 2.706 3.841 6.635 7.879 10.828
2 4.605 5.991 9.210 10.597 13.816
3 6.251 7.815 11.345 12.838 16.266
4 7.779 9.488 13.277 14.860 18.467
5 9.236 11.070 15.086 16.750 20.515
6 10.645 12.592 16.812 18.548 22.458
7 12.017 14.067 18.475 20.278 24.322
8 13.362 15.507 20.090 21.955 26.124
9 14.684 16.919 21.666 23.589 27.877
</p>
<p>10 15.987 18.307 23.209 25.188 29.588
11 17.275 19.675 24.725 26.757 31.264
12 18.549 21.026 26.217 28.300 32.909
13 19.812 22.362 27.688 29.819 34.528
14 21.064 23.685 29.141 31.319 36.123
15 22.307 24.996 30.578 32.801 37.697
16 23.542 26.296 32.000 34.267 39.252
17 24.769 27.587 33.409 35.718 40.790
18 25.989 28.869 34.805 37.156 42.312
19 27.204 30.144 36.191 38.582 43.820
20 28.412 31.410 37.566 39.997 45.315
30 40.256 43.773 50.892 53.672 59.703
40 51.805 55.758 63.691 66.766 73.402
50 63.167 67.505 76.154 79.490 86.661
60 74.397 79.082 88.379 91.952 99.607
70 85.527 90.531 100.425 104.215 112.317
80 80.000 101.879 112.329 116.321 124.839
90 107.565 113.145 124.116 128.299 137.208
</p>
<p>100 118.498 124.342 135.807 140.169 149.449</p>
<p/>
</div>
<div class="page"><p/>
<p>512 I Statistical Tables
</p>
<p>TableI.8: Quantiles FP of the F -distribution.
</p>
<p>0.900 = P =
&int; FP
</p>
<p>0
f (F ;f1,f2)dF
</p>
<p>f1
f2 1 2 3 4 5 6 7 8 9 10
1 39.86 49.50 53.59 55.83 57.24 58.20 58.91 59.44 59.86 60.19
2 8.526 9.000 9.162 9.243 9.293 9.326 9.349 9.367 9.381 9.392
3 5.538 5.462 5.391 5.343 5.309 5.285 5.266 5.252 5.240 5.230
4 4.545 4.325 4.191 4.107 4.051 4.010 3.979 3.955 3.936 3.920
5 4.060 3.780 3.619 3.520 3.453 3.405 3.368 3.339 3.316 3.297
6 3.776 3.463 3.289 3.181 3.108 3.055 3.014 2.983 2.958 2.937
7 3.589 3.257 3.074 2.961 2.883 2.827 2.785 2.752 2.725 2.703
8 3.458 3.113 2.924 2.806 2.726 2.668 2.624 2.589 2.561 2.538
9 3.360 3.006 2.813 2.693 2.611 2.551 2.505 2.469 2.440 2.416
</p>
<p>10 3.285 2.924 2.728 2.605 2.522 2.461 2.414 2.377 2.347 2.323
</p>
<p>0.950 = P =
&int; FP
</p>
<p>0
f (F ;f1,f2)dF
</p>
<p>f1
f2 1 2 3 4 5 6 7 8 9 10
1 161.4 199.5 215.7 224.6 230.2 234.0 236.8 238.9 240.5 241.9
2 18.51 19.00 19.16 19.25 19.30 19.33 19.35 19.37 19.38 19.40
3 10.13 9.552 9.277 9.117 9.013 8.941 8.887 8.845 8.812 8.786
4 7.709 6.944 6.591 6.388 6.256 6.163 6.094 6.041 5.999 5.964
5 6.608 5.786 5.409 5.192 5.050 4.950 4.876 4.818 4.772 4.735
6 5.987 5.143 4.757 4.534 4.387 4.284 4.207 4.147 4.099 4.060
7 5.591 4.737 4.347 4.120 3.972 3.866 3.787 3.726 3.677 3.637
8 5.318 4.459 4.066 3.838 3.687 3.581 3.500 3.438 3.388 3.347
9 5.117 4.256 3.863 3.633 3.482 3.374 3.293 3.230 3.179 3.137
</p>
<p>10 4.965 4.103 3.708 3.478 3.326 3.217 3.135 3.072 3.020 2.978</p>
<p/>
</div>
<div class="page"><p/>
<p>I. Statistical Tables 513
</p>
<p>TableI.8: (continued)
</p>
<p>0.975 = P =
&int; FP
</p>
<p>0
f (F ;f1,f2)dF
</p>
<p>f1
f2 1 2 3 4 5 6 7 8 9 10
1 647.8 799.5 864.2 899.6 921.8 937.1 948.2 956.7 963.3 968.6
2 38.51 39.00 39.17 39.25 39.30 39.33 39.36 39.37 39.39 39.40
3 17.44 16.04 15.44 15.10 14.88 14.73 14.62 14.54 14.47 14.42
4 12.22 10.65 9.979 9.605 9.364 9.197 9.074 8.980 8.905 8.844
5 10.01 8.434 7.764 7.388 7.146 6.978 6.853 6.757 6.681 6.619
6 8.813 7.260 6.599 6.227 5.988 5.820 5.695 5.600 5.523 5.461
7 8.073 6.542 5.890 5.523 5.285 5.119 4.995 4.899 4.823 4.761
8 7.571 6.059 5.416 5.053 4.817 4.652 4.529 4.433 4.357 4.295
9 7.209 5.715 5.078 4.718 4.484 4.320 4.197 4.102 4.026 3.964
</p>
<p>10 6.937 5.456 4.826 4.468 4.236 4.072 3.950 3.855 3.779 3.717
</p>
<p>0.990 = P =
&int; FP
</p>
<p>0
f (F ;f1,f2)dF
</p>
<p>f1
f2 1 2 3 4 5 6 7 8 9 10
1 4052 5000 5403 5625 5764 5859 5928 5981 6022 6056
2 98.50 99.00 99.17 99.25 99.30 99.33 99.36 99.37 99.39 99.40
3 34.12 30.82 29.46 28.71 28.24 27.91 27.67 27.49 27.35 27.23
4 21.20 18.00 16.69 15.98 15.52 15.21 14.98 14.80 14.66 14.55
5 16.26 13.27 12.06 11.39 10.97 10.67 10.46 10.29 10.16 10.05
6 13.75 10.92 9.780 9.148 8.746 8.466 8.260 8.102 7.976 7.874
7 12.25 9.547 8.451 7.847 7.460 7.191 6.993 6.840 6.719 6.620
8 11.26 8.649 7.591 7.006 6.632 6.371 6.178 6.029 5.911 5.814
9 10.56 8.022 6.992 6.422 6.057 5.802 5.613 5.467 5.351 5.257
</p>
<p>10 10.04 7.559 6.552 5.994 5.636 5.386 5.200 5.057 4.942 4.849</p>
<p/>
</div>
<div class="page"><p/>
<p>514 I Statistical Tables
</p>
<p>TableI.9: Quantiles tP of Student&rsquo;s distribution.
</p>
<p>P =
&int; tP
&minus;&infin;
</p>
<p>f (t;f )dt
</p>
<p>P
</p>
<p>f 0.9000 0.9500 0.9750 0.9900 0.9950 0.9990 0.9995
1 3.078 6.314 12.706 31.821 63.657 318.309 636.619
2 1.886 2.920 4.303 6.965 9.925 22.327 31.599
3 1.638 2.353 3.182 4.541 5.841 10.215 12.924
4 1.533 2.132 2.776 3.747 4.604 7.173 8.610
5 1.476 2.015 2.571 3.365 4.032 5.893 6.869
6 1.440 1.943 2.447 3.143 3.707 5.208 5.959
7 1.415 1.895 2.365 2.998 3.499 4.785 5.408
8 1.397 1.860 2.306 2.896 3.355 4.501 5.041
9 1.383 1.833 2.262 2.821 3.250 4.297 4.781
</p>
<p>10 1.372 1.812 2.228 2.764 3.169 4.144 4.587
11 1.363 1.796 2.201 2.718 3.106 4.025 4.437
12 1.356 1.782 2.179 2.681 3.055 3.930 4.318
13 1.350 1.771 2.160 2.650 3.012 3.852 4.221
14 1.345 1.761 2.145 2.624 2.977 3.787 4.140
15 1.341 1.753 2.131 2.602 2.947 3.733 4.073
16 1.337 1.746 2.120 2.583 2.921 3.686 4.015
17 1.333 1.740 2.110 2.567 2.898 3.646 3.965
18 1.330 1.734 2.101 2.552 2.878 3.610 3.922
19 1.328 1.729 2.093 2.539 2.861 3.579 3.883
20 1.325 1.725 2.086 2.528 2.845 3.552 3.850
30 1.310 1.697 2.042 2.457 2.750 3.385 3.646
40 1.303 1.684 2.021 2.423 2.704 3.307 3.551
50 1.299 1.676 2.009 2.403 2.678 3.261 3.496
60 1.296 1.671 2.000 2.390 2.660 3.232 3.460
70 1.294 1.667 1.994 2.381 2.648 3.211 3.435
80 1.292 1.664 1.990 2.374 2.639 3.195 3.416
90 1.291 1.662 1.987 2.368 2.632 3.183 3.402
</p>
<p>100 1.290 1.660 1.984 2.364 2.626 3.174 3.390
200 1.286 1.653 1.972 2.345 2.601 3.131 3.340
500 1.283 1.648 1.965 2.334 2.586 3.107 3.310
</p>
<p>1000 1.282 1.646 1.962 2.330 2.581 3.098 3.300</p>
<p/>
</div>
<div class="page"><p/>
<p>List of Computer Programs&lowast;
</p>
<p>AnalysisOfVariance, 318, 319
AuxDer, 426, 428
AuxDri, 230, 426, 428
AuxGrad, 426, 428
AuxHesse, 426, 428
AuxJInputGroup, 427, 429
AuxJNumberInput, 428, 429
AuxJRButtonGroup, 427, 429
AuxZero, 427, 428
DatanFrame, 427, 428
DatanGraphics, 118, 431, 432, 435,
</p>
<p>436
</p>
<p>DatanMatrix, 348, 399
DatanRandom, 67
DatanUserFunction, 230, 257, 275
DatanVector, 348, 399
E10Mtx, 403
E1Anova, 319
E1Distrib, 106
E1Gr, 443
E1Lsq, 263
E1MaxLike, 173, 427
E1Min, 305
E1Mtx, 399
E1Random, 67
E1Reg, 329
E1Sample, 150
E1Test, 205
E1TimSer, 340
E2Anova, 319
</p>
<p>E2Distrib, 107
E2Gr, 443
E2Lsq, 263
E2MaxLike, 173
E2Min, 305
E2Mtx, 400
E2Random, 67
E2Reg, 329
E2Sample, 150, 443
E2Test, 205
E2TimSer, 340
E3Distrib, 107
E3Gr, 444
E3Lsq, 263
E3Min, 306
E3Mtx, 400
E3Random, 68
E3Reg, 329
E3Sample, 150, 443
E3Test, 206
E4Gr, 444
E4Lsq, 264
E4Min, 306
E4Mtx, 400
E4Random, 68
E4Reg, 329
E4Sample, 150
E5Gr, 444
E5Lsq, 264
E5Mtx, 401
</p>
<p>&lowast;The slanted numbers refer to the Appendix.
</p>
<p>S. Brandt, Data Analysis: Statistical and Computational Methods for Scientists and Engineers,
DOI 10.1007/978-3-319-03762-2, &copy; Springer International Publishing Switzerland 2014
</p>
<p>515</p>
<p/>
</div>
<div class="page"><p/>
<p>516 List of Computer Programs
</p>
<p>E5Sample, 150
E6Gr, 443, 445
E6Lsq, 264
E6Mtx, 401
E6Sample, 151
E7Gr, 443, 445
E7Lsq, 264
E7Mtx, 401
E7Sample, 151
E8Gr, 443, 446
E8Lsq, 265
E8Mtx, 402
E9Lsq, 265
E9Mtx, 402
FunctionOnLine, 275, 303
FunctionsDemo, 414, 422
Gamma, 422, 441
GraphicsWith2DScatterDiagram,
</p>
<p>120, 150, 443
GraphicsWithDataPointsAndPolyline,
</p>
<p>443
</p>
<p>GraphicsWithDataPointsAnd&minus;
</p>
<p>MultiplePolylines,
443
</p>
<p>GraphicsWithHistogramAndPolyline,
443
</p>
<p>GraphicsWithHistogram, 118, 150,
443
</p>
<p>Histogram, 149, 150
LsqAsg, 263, 265
LsqAsm, 243, 262
LsqAsn, 243, 262, 264
LsqGEn, 264
LsqGen, 255, 257, 262, 264, 265
LsqLin, 224, 261, 263
LsqMar, 235, 243, 262, 264
LsqNon, 230, 235, 243, 261, 263
</p>
<p>LsqPol, 223, 261, 263
MinAsy, 298, 305, 306
MinCjg, 291, 304, 305
MinCombined, 280, 303
MinCov, 297, 305
MinDir, 280, 281, 303
MinEnclose, 280, 303
MinMar, 294, 304, 305
MinParab, 303
MinPow, 288, 304, 305
MinQdr, 292, 304, 305
MinSim, 283, 303, 305
Regression, 329
S1Anova, 484
S1Distrib, 472
S1Lsq, 475
S1MaxLike, 473
S1Min, 483
S1Random, 470
S1Reg, 485
S1TimSer, 485
S2Anova, 484
S2Lsq, 476
S2MaxLike, 473
S2Min, 483
S2Random, 470
S2Reg, 485
S2TimSer, 486
S3Lsq, 478
S3Random, 471
S4Lsq, 479
S5Lsq, 479
S6Lsq, 481, 482
Sample, 115, 149, 150
SmallSample, 149, 150
StatFunct, 414, 427
TimeSeries, 340</p>
<p/>
</div>
<div class="page"><p/>
<p>Index&lowast;
</p>
<p>a posteriori probability, 153
acceptance probability, 188
addition of errors
</p>
<p>quadratic, 105
alternative hypothesis, 186, 494
analysis of variance, 307, 500
</p>
<p>model, 312
one-way, 307
table, 310
two-way, 311
</p>
<p>crossed classification, 483
nested classification, 484
</p>
<p>and, 10
ANOVA, 307
</p>
<p>table, 310
asymmetric error, 167, 298
asymptotically unbiased estimator,
</p>
<p>165
average
</p>
<p>moving, 332, 501
</p>
<p>background, 142
backward substitution, 374
basis, 351
beta function, 418
</p>
<p>incomplete, 420
bias, 157, 494
bidiagonal matrix, 350
binomial coefficient, 406, 418
binomial distribution, 70, 409, 451
</p>
<p>parameter estimation, 163
binomial theorem, 406
bit, 42
bracketing the minimum, 275
</p>
<p>Breit&ndash;Wigner distribution, 25, 57, 470
byte, 43
</p>
<p>Cauchy distribution, 23
central limit theorem, 90, 491
characteristic equation, 378
χ2-distribution, 130, 412, 453, 510
</p>
<p>quantiles, 511
χ2-test, 199, 206, 455
Cholesky decomposition, 372, 401
Cholesky inversion, 401
classification, 307, 500
</p>
<p>crossed, 313
nested, 313, 316
one-way, 311
two-way, 313
</p>
<p>clipping region, 436
cofactor, 361
color index, 436
column space, 352
column vector, 348
combination, 405
combinatorics, 405
computing coordinates, 433
confidence
</p>
<p>ellipsoid, 100, 240, 297
interval, 326, 336
level, 133, 494
limits, 297, 298
region, 170, 241, 260, 297, 479
</p>
<p>conjugate directions, 285
consistent estimator, 111
constrained measurements, 243, 258
constraint, 396, 403
</p>
<p>equations, 244
</p>
<p>&lowast;The slanted numbers refer to the Appendix.
</p>
<p>S. Brandt, Data Analysis: Statistical and Computational Methods for Scientists and Engineers,
DOI 10.1007/978-3-319-03762-2, &copy; Springer International Publishing Switzerland 2014
</p>
<p>517</p>
<p/>
</div>
<div class="page"><p/>
<p>518 Index
</p>
<p>contingency table, 203, 456
continued fraction, 418
contour line, 438
controlled variable, 218
convolution, 101, 452, 491
</p>
<p>of uniform distribution and normal
distribution, 472
</p>
<p>of uniform distributions, 102, 471
with the normal distribution, 103
</p>
<p>coordinate cross
in graphics, 440
</p>
<p>correlation coefficient, 29, 489
correlation coefficient of a sample, 473
counted number, 137
covariance, 29, 32, 489
</p>
<p>ellipse, 97, 306
ellipsoid, 99, 240
matrix, 297, 489
</p>
<p>weighted, 393
critical region, 177, 187, 494
</p>
<p>decile, 22
degrees of freedom, 128, 238
derivative
</p>
<p>logarithmic, 156
determinant, 360
device coordinates, 433
diagonal matrix, 349
dice, 11
direct measurements
</p>
<p>of different accuracy, 210
of equal accuracy, 209
</p>
<p>direct sum, 353
dispersion, 19
distribution
</p>
<p>binomial, 70, 409, 451
Breit&ndash;Wigner, 57, 470
Breit&ndash;Wigner , 25
Cauchy, 23
χ2, 412, 453, 510
</p>
<p>quantiles, 511
F , 413
</p>
<p>quantiles, 512
frequency, 109
function, 16, 487
</p>
<p>of sample, 110
of several variables, 488
of several variables, 30
of two variables, 25
</p>
<p>Gaussian, 86
hypergeometric, 74, 409
</p>
<p>Lorentz , 25
multivariate normal, 452
multivariate normal, 94
normal, 86, 410, 451, 505, 507
</p>
<p>quantiles, 508, 509
of a continuous variable, 492
of a discrete variable, 491
Poisson, 78, 410, 451
Polya, 77
standard normal, 84, 410
</p>
<p>quantiles, 508, 509
Student&rsquo;s, 182, 413
</p>
<p>quantiles, 514
t , 413
</p>
<p>quantiles, 514
triangular, 58, 448, 470
uniform, 22
unimodal, 21
</p>
<p>efficient estimator, 111
eigenvalue, 376
</p>
<p>equation, 376
eigenvector, 376
elements, 244
equations of constraint, 238
error
</p>
<p>asymmetric, 167, 173, 242, 260,
298, 306, 479
</p>
<p>combination, 164
of mean, 113
of sample variance, 113
of the first kind, 187, 495
of the second kind, 187, 495
standard, 89
statistical, 73, 106, 137
symmetric, 297, 306
</p>
<p>error bars, 117
error function, 411
error model of Laplace, 92
error propagation, 37, 450, 490
E space, 186
estimation, 111
estimator, 111, 493, 494
</p>
<p>asymptotically unbiased, 165, 494
consistent, 111, 493
efficiency of, 452
efficient, 111
minimum variance, 161
unbiased, 111, 493
unique, 494
</p>
<p>event, 8, 487</p>
<p/>
</div>
<div class="page"><p/>
<p>Index 519
</p>
<p>expectation value, 17, 31, 488, 489
experiment, 7
</p>
<p>factorial, 418
F -distribution, 413
</p>
<p>quantiles, 512
fit
</p>
<p>of a Breit&ndash;Wigner-function, 478
of a polynomial, 473
of a power law, 475
of an arbitrary linear function, 224
of an exponential function, 232
of a Breit&ndash;Wigner function, 480
of a circle to points with
</p>
<p>measurement errors in abscissa
and ordinate, 481
</p>
<p>of a Gaussian, 231, 263
of a nonlinear function, 228
of a polynomial, 222, 263
of a proportionality, 224, 263
of a straight line, 218
of a straight line to points with
</p>
<p>measurement errors in abscissa
and ordinate, 264
</p>
<p>of a sum of exponential functions,
233
</p>
<p>of a sum of two Gaussians and a
polynomial, 235, 264
</p>
<p>fluctuation
statistical, 106
</p>
<p>forward substitution, 374
frequency, 106, 490
frequency distribution, 109
F -test, 177, 205, 455
full width at half maximum (FWHM),
</p>
<p>119
functional equation, 415
FWHM, 119
</p>
<p>Galton&rsquo;s board, 107
gamma function, 415
</p>
<p>incomplete, 420
Gauss&ndash;Markov theorem, 238
Gaussian distribution, 86
</p>
<p>multivariate, 94
Gaussian elimination, 367
</p>
<p>with pivoting, 369
Givens transformation, 354
go button, 427
golden section, 278
</p>
<p>graphics, 431
class, 431
workstation, 431
</p>
<p>Hessian matrix, 271
histogram, 117, 454
</p>
<p>bin width, 117
determination of parameters from,
</p>
<p>306
Householder transformation, 356, 400
hypergeometric distribution, 74, 409
hyperplane, 353
hypothesis, 175, 186
</p>
<p>alternative, 186, 494
composite, 186, 494
null, 186, 494
simple, 186, 494
test of, 494
</p>
<p>implementation, 50
incomplete beta function, 420
incomplete gamma function, 420
independence
</p>
<p>of events, 11
of random variables, 26, 31
</p>
<p>indirect measurements
linear case, 214
nonlinear case, 226
</p>
<p>information, 160, 454
inequality, 157, 161, 494
of a sample, 494
</p>
<p>input group, 427
interaction, 315
inverse matrix, 365
</p>
<p>Jacobian determinant, 35, 490
of an orthogonal transformation,
</p>
<p>40
</p>
<p>kernel, 352
Kronecker symbol, 128
</p>
<p>Lagrange function, 248
Lagrange multipliers, 126, 247
Laplace error model, 92
law of large numbers, 73, 490
LCG, 45</p>
<p/>
</div>
<div class="page"><p/>
<p>520 Index
</p>
<p>least squares, 209, 362, 497
according to Marquardt, 394
constrained measurements, 243, 498
direct measurements, 209, 499
general case, 251, 498
indirect measurements, 214, 226,
</p>
<p>499
</p>
<p>properties of the solution, 236
with constraints, 396, 403
with change of scale, 393
with weights, 392
</p>
<p>likelihood
equation, 155, 494
function, 154, 493
</p>
<p>logarithmic, 155, 493
ratio, 154
</p>
<p>likelihood-ratio test, 194, 495
linear combination, 351
linear system of equations, 362
Lorentz distribution, 25
lotto, 12
LR-decomposition, 369
</p>
<p>main diagonal, 349
mantissa, 43
mapping, 352
marginal probability density, 26, 31,
</p>
<p>488
</p>
<p>Marquardt minimization, 292
matrix, 348
</p>
<p>addition, 348
adjoint, 361, 366
antisymmetric, 350
bidiagonal, 350
diagonal, 349
equations, 362
inverse, 365
main diagonal, 349
multiplication
</p>
<p>by a constant, 348
by a matrix, 348
</p>
<p>norm, 350
null, 349
orthogonal, 354
product, 348
pseudo-inverse, 375
rank, 352
singular, 352, 361
subtraction, 348
symmetric, 350
transposition, 348
</p>
<p>triangular, 350
tridiagonal, 350
unit, 349
</p>
<p>maximum likelihood, 493
maximum-likelihood estimates, 454
mean, 17
</p>
<p>error, 113
of sample, 111
</p>
<p>mean square, 128
mean square deviation, 128
median, 21, 488
minimization, 267
</p>
<p>combined method by BRENT, 280
along a direction, 280
along chosen directions, 287
along the coordinate directions, 284
bracketing the minimum, 275
by the method of POWELL, 287
choice of method, 295
errors in, 296
examples, 298
golden section, 277
in the direction of steepest descent,
</p>
<p>271, 288
Marquardt procedure, 292
quadratic interpolation, 280
simplex method, 281
with the quadratic form, 292
</p>
<p>minimum variance
bound, 161
estimator, 161
</p>
<p>MLCG, 45
mode, 21, 488
moments, 18, 27, 31, 487, 489
</p>
<p>about the mean, 18
Monte Carlo method, 41
</p>
<p>for simulation, 66
minimization, 483
of integration, 64
</p>
<p>moving average, 332, 501
</p>
<p>Neyman&ndash;Pearson lemma, 191, 495
norm
</p>
<p>Euclidian, 350
normal distribution, 86, 410, 451, 505,
</p>
<p>507
</p>
<p>multivariate, 63, 94, 452
quantiles, 508, 509
standard, 84, 410
</p>
<p>normal equations, 365
null hypothesis, 186, 494</p>
<p/>
</div>
<div class="page"><p/>
<p>Index 521
</p>
<p>null matrix, 349
null space, 352
null vector, 349
number-input region, 428
</p>
<p>one-sided test, 176
operating characteristic function, 188,
</p>
<p>495
</p>
<p>orthogonal, 349
orthogonal complements, 353
orthogonal matrix, 354
orthogonal polynomials, 500
orthogonal transformation, 354
</p>
<p>parabola through three points, 273
Pascal&rsquo;s triangle, 92, 406
permutation, 405
permutation transformation, 359
pivot, 369
Poisson distribution, 78, 410, 451
</p>
<p>parameter estimation, 162
quantiles, 503
</p>
<p>Polya distribution, 77
polyline, 437
polymarker, 437
polynomial
</p>
<p>orthogonal, 500
polynomial regression, 500
population, 109, 492
portability, 50
power function, 188, 495
precision
</p>
<p>absolute, 44
relative, 44
</p>
<p>primitive element, 46
principal axes, 378
principal axis transformation, 377
probability, 487
</p>
<p>a posteriori, 153
conditional, 10
density, 16, 487
</p>
<p>conditional, 26
joint, 488
marginal, 26, 31, 488
of several variables, 31
of two variables, 26
</p>
<p>frequency definition, 9
total, 10
</p>
<p>pseudo-inverse matrix, 375
</p>
<p>quadratic average of individual errors,
164
</p>
<p>quantile, 22, 488
quartile, 22
</p>
<p>radio-button group, 427
random component, 332
random number generator, 44
</p>
<p>linear congruential (LCG), 45
multiplicative linear congruential
</p>
<p>(MLCG), 45
random numbers, 41
</p>
<p>arbitrarily distributed, 55
acceptance-rejection technique,
</p>
<p>59
transformation procedures, 56
</p>
<p>normally distributed, 62
random variable, 15, 487
</p>
<p>continuous, 15
discrete, 15
</p>
<p>rank, 352
ratio of small numbers of events, 144
</p>
<p>with background, 147
reduced variable, 20, 488
regression, 321
</p>
<p>curve, 325
polynomial, 325, 500
</p>
<p>representation of numbers in a computer,
42
</p>
<p>row space, 352
row vector, 348
</p>
<p>sample, 74, 109, 492
correlation coefficient, 473
distribution function, 110
error
</p>
<p>of variance, 113
of mean, 113
</p>
<p>from a bivariate normal distribution,
173
</p>
<p>from a continuous population, 111
from finite population, 127
from Gaussian distribution, 130
from partitioned population, 453
from subpopulation, 122
graphical representation, 115
information, 160
mean, 111, 150, 453
random, 110</p>
<p/>
</div>
<div class="page"><p/>
<p>522 Index
</p>
<p>size, 109
small, 136
</p>
<p>with background, 142
space, 7
variance, 112, 150, 453
</p>
<p>scalar, 349
scalar product, 349
scale factor, 213
scale in graphics, 439
scatter diagram, 150
</p>
<p>one-dimensional, 116
two-dimensional, 120
</p>
<p>seed, 52
sign inversion, 359
signal, 142
significance level, 175, 494
simplex, 282
singular matrix, 352, 361
singular value, 379
singular value analysis, 380, 383
singular value decomposition, 379, 385,
</p>
<p>401, 402
</p>
<p>skewness, 20, 488
small numbers of events, 136
</p>
<p>ratio, 144
with background, 147
</p>
<p>small sample, 136
span, 352
standard deviation, 19, 89, 488
standard normal distribution, 410
statistic, 111, 493
</p>
<p>test, 187
statistical error, 73, 137
statistical test, 175
steepest descent, 288
step diagram, 117
Student&rsquo;s difference test, 184
Student&rsquo;s distribution, 182, 413
</p>
<p>quantiles, 514
Student&rsquo;s test, 180, 205, 455
subpopulation, 122
subspace, 352
sum of squares, 127, 308
symmetric errors, 297
system of equations
</p>
<p>linear, 362
triangular, 368
underdetermined, 362
</p>
<p>t-distribution, 182, 413
quantiles, 514
</p>
<p>test, 175, 494, 496
χ2, 199, 206, 455, 495
likelihood-ratio, 194, 495
most powerful, 188, 495
one-sided, 176
statistic, 176, 187, 495
Student&rsquo;s , 205
two-sided, 176
unbiased, 188, 495
uniformly most powerful, 188
</p>
<p>text in plot, 441
three-door game, 13
time series, 331
</p>
<p>analysis, 331, 501
extrapolation, 485
</p>
<p>discontinuities, 485
transformation
</p>
<p>Givens, 354, 400
Householder, 356, 400
linear, 36
of a vector, 352
of variables, 33, 449, 489
orthogonal, 39, 354
permutation, 359
principal axis, 377
sign inversion, 359
</p>
<p>transposition, 348
trend, 332, 501
triangular distribution, 58, 448, 470
triangular matrix, 350
triangular system of equations, 368
tridiagonal matrix, 350
two-sided test, 176
2&times;2 table, 204
2&times;2 table test, 204
</p>
<p>underdetermined system of equations,
362
</p>
<p>uniform distribution, 22
unit matrix, 349
unit vector, 350
</p>
<p>variance, 27, 32, 488, 489
of sample, 112
of an estimator, 454
of a random variable, 18
</p>
<p>vector, 348
absolute value, 350
components, 348
norm, 350</p>
<p/>
</div>
<div class="page"><p/>
<p>Index 523
</p>
<p>null, 349
row, 348
space, 351
</p>
<p>basis, 351
closed, 351
dimension, 351
</p>
<p>transformation, 352
unit, 350
</p>
<p>vectors
linearly dependent, 351
</p>
<p>linearly independent, 351
orthonormal, 351
</p>
<p>viewport, 433
</p>
<p>weight matrix, 215
weighted covariance matrix, 393
width
</p>
<p>full at half maximum, 119
Wilks theorem, 195, 495
window, 433
world coordinates, 432</p>
<p/>
</div>
<ul>	<li> Preface to the Fourth English Edition</li>
	<li>Contents</li>
	<li> List of Examples</li>
	<li> Frequently Used Symbols and Notation</li>
	<li>1 Introduction</li>
<ul>	<li>1.1 Typical Problems of Data Analysis</li>
	<li>1.2 On the Structure of this Book</li>
	<li>1.3 About the Computer Programs</li>
</ul>
	<li>2 Probabilities</li>
<ul>	<li>2.1 Experiments, Events, Sample Space</li>
	<li>2.2 The Concept of Probability</li>
	<li>2.3 Rules of Probability Calculus: Conditional Probability</li>
	<li>2.4 Examples</li>
<ul>	<li>2.4.1 Probability for n Dots in the Throwing of TwoDice</li>
	<li>2.4.2 Lottery 6 Out of 49</li>
	<li>2.4.3 Three-Door Game</li>
</ul>
</ul>
	<li>3 Random Variables: Distributions</li>
<ul>	<li>3.1 Random Variables</li>
	<li>3.2 Distributions of a Single Random Variable</li>
	<li>3.3 Functions of a Single Random Variable, Expectation Value, Variance, Moments</li>
	<li>3.4 Distribution Function and Probability Density of TwoVariables: Conditional Probability</li>
	<li>3.5 Expectation Values, Variance, Covariance, and Correlation</li>
	<li>3.6 More than Two Variables: Vector and Matrix Notation</li>
	<li>3.7 Transformation of Variables</li>
	<li>3.8 Linear and Orthogonal Transformations: ErrorPropagation</li>
</ul>
	<li>4 Computer Generated Random Numbers: The MonteCarlo Method</li>
<ul>	<li>4.1 Random Numbers</li>
	<li>4.2 Representation of Numbers in a Computer</li>
	<li>4.3 Linear Congruential Generators</li>
	<li>4.4 Multiplicative Linear Congruential Generators</li>
	<li>4.5 Quality of an MLCG: Spectral Test</li>
	<li>4.6 Implementation and Portability of an MLCG</li>
	<li>4.7 Combination of Several MLCGs</li>
	<li>4.8 Generation of Arbitrarily Distributed Random Numbers </li>
<ul>	<li>4.8.1 Generation by Transformation of the UniformDistribution</li>
	<li>4.8.2 Generation with the von Neumann Acceptance&ndash;Rejection Technique</li>
</ul>
	<li>4.9 Generation of Normally Distributed Random Numbers</li>
	<li>4.10 Generation of Random Numbers Accordingto a Multivariate Normal Distribution</li>
	<li>4.11 The Monte Carlo Method for Integration</li>
	<li>4.12 The Monte Carlo Method for Simulation</li>
	<li>4.13 Java Classes and Example Programs</li>
</ul>
	<li>5 Some Important Distributions and Theorems</li>
<ul>	<li>5.1 The Binomial and Multinomial Distributions</li>
	<li>5.2 Frequency: The Law of Large Numbers</li>
	<li>5.3 The Hypergeometric Distribution</li>
	<li>5.4 The Poisson Distribution</li>
	<li>5.5 The Characteristic Function of a Distribution</li>
	<li>5.6 The Standard Normal Distribution</li>
	<li>5.7 The Normal or Gaussian Distribution</li>
	<li>5.8 Quantitative Properties of the Normal Distribution</li>
	<li>5.9 The Central Limit Theorem</li>
	<li>5.10 The Multivariate Normal Distribution</li>
	<li>5.11 Convolutions of Distributions</li>
<ul>	<li>5.11.1 Folding Integrals</li>
	<li>5.11.2 Convolutions with the Normal Distribution</li>
</ul>
	<li>5.12 Example Programs</li>
</ul>
	<li>6 Samples</li>
<ul>	<li>6.1 Random Samples. Distributionof a Sample. Estimators</li>
	<li>6.2 Samples from Continuous Populations: Meanand Variance of a Sample</li>
	<li>6.3 Graphical Representation of Samples: Histogramsand Scatter Plots</li>
	<li>6.4 Samples from Partitioned Populations</li>
	<li>6.5 Samples Without Replacement from Finite DiscretePopulations. Mean Square Deviation. Degrees ofFreedom</li>
	<li>6.6 Samples from Gaussian Distributions: χ2-Distribution</li>
	<li>6.7 χ2 and Empirical Variance</li>
	<li>6.8 Sampling by Counting: Small Samples</li>
	<li>6.9 Small Samples with Background</li>
	<li>6.10 Determining a Ratio of Small Numbers of Events</li>
	<li>6.11 Ratio of Small Numbers of Events with Background</li>
	<li>6.12 Java Classes and Example Programs</li>
</ul>
	<li>7 The Method of Maximum Likelihood</li>
<ul>	<li>7.1 Likelihood Ratio: Likelihood Function</li>
	<li>7.2 The Method of Maximum Likelihood</li>
	<li>7.3 Information Inequality. Minimum VarianceEstimators. Sufficient Estimators</li>
	<li>7.4 Asymptotic Properties of the Likelihood Functionand Maximum-Likelihood Estimators</li>
	<li>7.5 Simultaneous Estimation of Several Parameters:Confidence Intervals</li>
	<li>7.6 Example Programs</li>
</ul>
	<li>8 Testing Statistical Hypotheses</li>
<ul>	<li>8.1 Introduction</li>
	<li>8.2 F-Test on Equality of Variances</li>
	<li>8.3 Student's Test: Comparison of Means</li>
	<li>8.4 Concepts of the General Theory of Tests</li>
	<li>8.5 The Neyman&ndash;Pearson Lemma and Applications</li>
	<li>8.6 The Likelihood-Ratio Method</li>
	<li>8.7 The χ2-Test for Goodness-of-Fit</li>
<ul>	<li>8.7.1 χ2-Test with Maximal Number of Degreesof Freedom</li>
	<li>8.7.2 χ2-Test with Reduced Number of Degreesof Freedom</li>
	<li>8.7.3 χ2-Test and Empirical Frequency Distribution</li>
</ul>
	<li>8.8 Contingency Tables</li>
	<li>8.9 2 2 Table Test</li>
	<li>8.10 Example Programs</li>
</ul>
	<li>9 The Method of Least Squares</li>
<ul>	<li>9.1 Direct Measurements of Equal or Unequal Accuracy</li>
	<li>9.2 Indirect Measurements: Linear Case</li>
	<li>9.3 Fitting a Straight Line</li>
	<li>9.4 Algorithms for Fitting Linear Functionsof the Unknowns</li>
<ul>	<li>9.4.1 Fitting a Polynomial</li>
	<li>9.4.2 Fit of an Arbitrary Linear Function</li>
</ul>
	<li>9.5 Indirect Measurements: Nonlinear Case</li>
	<li>9.6 Algorithms for Fitting Nonlinear Functions</li>
<ul>	<li>9.6.1 Iteration with Step-Size Reduction</li>
	<li>9.6.2 Marquardt Iteration</li>
</ul>
	<li>9.7 Properties of the Least-Squares Solution: χ2-Test</li>
	<li>9.8 Confidence Regions and Asymmetric Errorsin the Nonlinear Case</li>
	<li>9.9 Constrained Measurements</li>
<ul>	<li>9.9.1 The Method of Elements</li>
	<li>9.9.2 The Method of Lagrange Multipliers</li>
</ul>
	<li>9.10 The General Case of Least-Squares Fitting</li>
	<li>9.11 Algorithm for the General Case of Least Squares</li>
	<li>9.12 Applying the Algorithm for the General Caseto Constrained Measurements</li>
	<li>9.13 Confidence Region and Asymmetric Errorsin the General Case</li>
	<li>9.14 Java Classes and Example Programs</li>
</ul>
	<li>10 Function Minimization</li>
<ul>	<li>10.1 Overview: Numerical Accuracy</li>
	<li>10.2 Parabola Through Three Points</li>
	<li>10.3 Function of n Variables on a Linein an n-Dimensional Space</li>
	<li>10.4 Bracketing the Minimum</li>
	<li>10.5 Minimum Search with the Golden Section</li>
	<li>10.6 Minimum Search with Quadratic Interpolation</li>
	<li>10.7 Minimization Along a Direction in n Dimensions</li>
	<li>10.8 Simplex Minimization in n Dimensions</li>
	<li>10.9 Minimization Along the Coordinate Directions</li>
	<li>10.10 Conjugate Directions</li>
	<li>10.11 Minimization Along Chosen Directions</li>
	<li>10.12 Minimization in the Direction of Steepest Descent</li>
	<li>10.13 Minimization Along Conjugate Gradient Directions</li>
	<li>10.14 Minimization with the Quadratic Form</li>
	<li>10.15 Marquardt Minimization</li>
	<li>10.16 On Choosing a Minimization Method</li>
	<li>10.17 Consideration of Errors</li>
	<li>10.18 Examples</li>
	<li>10.19 Java Classes and Example Programs</li>
</ul>
	<li>11 Analysis of Variance</li>
<ul>	<li>11.1 One-Way Analysis of Variance</li>
	<li>11.2 Two-Way Analysis of Variance</li>
	<li>11.3 Java Class and Example Programs</li>
</ul>
	<li>12 Linear and Polynomial Regression</li>
<ul>	<li>12.1 Orthogonal Polynomials</li>
	<li>12.2 Regression Curve: Confidence Interval</li>
	<li>12.3 Regression with Unknown Errors</li>
	<li>12.4 Java Class and Example Programs</li>
</ul>
	<li>13 Time Series Analysis</li>
<ul>	<li>13.1 Time Series: Trend</li>
	<li>13.2 Moving Averages</li>
	<li>13.3 Edge Effects</li>
	<li>13.4 Confidence Intervals</li>
	<li>13.5 Java Class and Example Programs</li>
</ul>
	<li>Literature</li>
	<li>A Matrix Calculations</li>
<ul>	<li>A.1 Definitions: Simple Operations</li>
	<li>A.2 Vector Space, Subspace, Rank of a Matrix</li>
	<li>A.3 Orthogonal Transformations</li>
<ul>	<li>A.3.1 Givens Transformation</li>
	<li>A.3.2 Householder Transformation</li>
	<li>A.3.3 Sign Inversion</li>
	<li>A.3.4 Permutation Transformation</li>
</ul>
	<li>A.4 Determinants</li>
	<li>A.5 Matrix Equations: Least Squares</li>
	<li>A.6 Inverse Matrix</li>
	<li>A.7 Gaussian Elimination</li>
	<li>A.8 LR-Decomposition</li>
	<li>A.9 Cholesky Decomposition</li>
	<li>A.10 Pseudo-inverse Matrix</li>
	<li>A.11 Eigenvalues and Eigenvectors</li>
	<li>A.12 Singular Value Decomposition</li>
	<li>A.13 Singular Value Analysis</li>
	<li>A.14 Algorithm for Singular Value Decomposition</li>
<ul>	<li>A.14.1 Strategy</li>
	<li>A.14.2 Bidiagonalization</li>
	<li>A.14.3 Diagonalization</li>
	<li>A.14.4 Ordering of the Singular Values and Permutation</li>
	<li>A.14.5 Singular Value Analysis</li>
</ul>
	<li>A.15 Least Squares with Weights</li>
	<li>A.16 Least Squares with Change of Scale</li>
	<li>A.17 Modification of Least Squares According to Marquardt</li>
	<li>A.18 Least Squares with Constraints</li>
	<li>A.19 Java Classes and Example Programs</li>
</ul>
	<li>B Combinatorics</li>
	<li>C Formulas and Methods for the Computation of StatisticalFunctions</li>
<ul>	<li>C.1 Binomial Distribution</li>
	<li>C.2 Hypergeometric Distribution</li>
	<li>C.3 Poisson Distribution</li>
	<li>C.4 Normal Distribution</li>
	<li>C.5 χ2-Distribution</li>
	<li>C.6 F-Distribution</li>
	<li>C.7 t-Distribution</li>
	<li>C.8 Java Class and Example Program</li>
</ul>
	<li>D The Gamma Function and Related Functions: Methodsand Programs for Their Computation</li>
<ul>	<li>D.1 The Euler Gamma Function</li>
	<li>D.2 Factorial and Binomial Coefficients</li>
	<li>D.3 Beta Function</li>
	<li>D.4 Computing Continued Fractions</li>
	<li>D.5 Incomplete Gamma Function</li>
	<li>D.6 Incomplete Beta Function</li>
	<li>D.7 Java Class and Example Program</li>
</ul>
	<li>E Utility Programs</li>
<ul>	<li>E.1 Numerical Differentiation</li>
	<li>E.2 Numerical Determination of Zeros</li>
	<li>E.3 Interactive Input and Output Under Java</li>
	<li>E.4 Java Classes</li>
</ul>
	<li>F The Graphics Class DatanGraphics</li>
<ul>	<li>F.1 Introductory Remarks</li>
	<li>F.2 Graphical Workstations: Control Routines</li>
	<li>F.3 Coordinate Systems, Transformationsand Transformation Methods</li>
<ul>	<li>F.3.1 Coordinate Systems</li>
	<li>F.3.2 Linear Transformations: Window &ndash; Viewport</li>
</ul>
	<li>F.4 Transformation Methods</li>
	<li>F.5 Drawing Methods</li>
	<li>F.6 Utility Methods</li>
	<li>F.7 Text Within the Plot</li>
	<li>F.8 Java Classes and Example Programs</li>
</ul>
	<li>G Problems, Hints and Solutions, and Programming Problems</li>
<ul>	<li>G.1 Problems</li>
	<li>G.2 Hints and Solutions</li>
	<li>G.3 Programming Problems</li>
</ul>
	<li>H Collection of Formulas</li>
	<li>I Statistical Tables</li>
	<li>List of Computer Programs</li>
	<li>Index</li>
</ul>
</body></html>