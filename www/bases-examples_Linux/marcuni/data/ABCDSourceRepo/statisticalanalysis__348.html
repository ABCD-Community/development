<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Untitled</title>
</head>
<body><div class="page"><p/>
<p>Ton J. Cleophas &middot; Aeilko H. Zwinderman 
</p>
<p>Statistical Analysis 
of Clinical Data on 
a Pocket Calculator
Statistics on a Pocket Calculator
</p>
<p> </p>
<p/>
</div>
<div class="page"><p/>
<p>Statistical Analysis of Clinical Data  
on a Pocket Calculator</p>
<p/>
</div>
<div class="page"><p/>
<p>   </p>
<p/>
</div>
<div class="page"><p/>
<p>Ton฀J.฀Cleophas฀ &bull;฀ Aeilko฀H.฀Zwinderman
</p>
<p>Statistical Analysis  
of Clinical Data on  
a Pocket Calculator
</p>
<p>Statistics on a Pocket Calculator</p>
<p/>
</div>
<div class="page"><p/>
<p>Prof. Ton J. Cleophas
Department฀of฀Medicine
Albert฀Schweitzer฀Hospital
Dordrecht,฀The฀Netherlands
and
European฀College฀of฀Pharmaceutical฀
Medicine,฀Lyon,฀France
ajm.cleophas@wxs.nl
</p>
<p>Prof.฀Aeilko฀H.฀Zwinderman
Department฀of฀Epidemiology฀ 
and฀Biostatistics
Academic฀Medical฀Center
Amsterdam,฀The฀Netherlands
and
European฀College฀of฀Pharmaceutical฀
Medicine,฀Lyon,฀France
a.h.zwinderman@amc.uva.nl
</p>
<p>ISBN฀978-94-007-1210-2 e-ISBN฀978-94-007-1211-9
DOI฀10.1007/978-94-007-1211-9
Springer฀Dordrecht฀Heidelberg฀London฀New฀York
</p>
<p>&copy;฀Springer฀Science+Business฀Media฀B.V.฀2011
No฀part฀of฀this฀work฀may฀be฀reproduced,฀stored฀in฀a฀retrieval฀system,฀or฀transmitted฀in฀any฀form฀or฀by฀any฀
means,฀ electronic,฀ mechanical,฀ photocopying,฀ microfilming,฀ recording฀ or฀ otherwise,฀ without฀ written฀฀
permission฀from฀the฀Publisher,฀with฀the฀exception฀of฀any฀material฀supplied฀specifically฀for฀the฀purpose฀
of฀being฀entered฀and฀executed฀on฀a฀computer฀system,฀for฀exclusive฀use฀by฀the฀purchaser฀of฀the฀work.฀
</p>
<p>Printed฀on฀acid-free฀paper฀
</p>
<p>Springer฀is฀part฀of฀Springer฀Science+Business฀Media฀(www.springer.com)</p>
<p/>
</div>
<div class="page"><p/>
<p>v
</p>
<p>The฀ time฀ that฀ statistical฀ analyses,฀ including฀ analysis฀ of฀ variance฀ and฀ regression฀
analyses,฀were฀analyzed฀by฀statistical฀laboratory฀workers,฀has฀gone฀for฀good,฀thanks฀
to฀the฀availability฀of฀user-friendly฀statistical฀software.฀The฀teaching฀department,฀the฀
educations฀ committee,฀ and฀ the฀ scientific฀ committee฀ of฀ the฀ Albert฀ Schweitzer฀
Hospital,฀Dordrecht,฀Netherlands,฀ are฀ pleased฀ to฀ announce฀ that฀ since฀November฀
2009฀the฀entire฀staff฀and฀personal฀is฀able฀to฀perform฀statistical฀analyses฀with฀help฀
of฀SPSS฀Statistical฀Software฀in฀their฀offices฀through฀the฀institution&rsquo;s฀intranet.
</p>
<p>It฀ is฀ our฀ experience฀ as฀masters&rsquo;฀ and฀ doctorate฀ class฀ teachers฀ of฀ the฀European฀
College฀of฀Pharmaceutical฀Medicine฀(EC฀Socrates฀Project)฀that฀students฀are฀eager฀
to฀ master฀ adequate฀ command฀ of฀ statistical฀ software฀ for฀ carrying฀ out฀ their฀ own฀
฀statistical฀ analyses.฀ However,฀ students฀ often฀ lack฀ adequate฀ knowledge฀ of฀ basic฀
principles,฀and฀this฀carries฀ the฀risk฀of฀fallacies.฀Computers฀cannot฀ think,฀and฀can฀
only฀ execute฀ commands฀ as฀ given.฀ As฀ an฀ example,฀ regression฀ analysis฀ usually฀
applies฀independent฀and฀dependent฀variables,฀often฀interprets฀as฀causal฀factors฀and฀
outcome฀factors.฀E.g.,฀gender฀and฀age฀may฀determine฀the฀type฀of฀operation฀or฀the฀
type฀of฀surgeon.฀The฀type฀of฀surgeon฀does฀not฀determine฀the฀age฀and฀gender.฀Yet,฀
software฀programs฀have฀no฀difficulty฀to฀use฀nonsense฀determinants,฀and฀the฀inves-
tigator฀in฀charge฀of฀the฀analysis฀has฀to฀decide฀what฀is฀caused฀by฀what,฀because฀a฀
computer฀can฀not฀do฀a฀thing฀like฀that,฀although฀it฀is฀essential฀to฀the฀analysis.
</p>
<p>It฀ is฀our฀experience฀ that฀a฀pocket฀calculator฀ is฀very฀helpful฀ for฀ the฀purpose฀of฀
studying฀ the฀ basic฀ principles.฀ Also,฀ a฀ number฀ of฀ statistical฀ methods฀ can฀ be฀
฀performed฀more฀easily฀on฀a฀pocket฀calculator,฀than฀using฀a฀software฀program.
</p>
<p>Advantages฀of฀the฀pocket฀calculator฀method฀include฀the฀following.
</p>
<p>฀1.฀ You฀better฀understand฀what฀you฀are฀doing.฀The฀statistical฀software฀program฀is฀
kind฀of฀black฀box฀program.
</p>
<p>฀2.฀ The฀pocket฀calculator฀works฀faster,฀because฀far฀less฀steps฀have฀to฀be฀taken.
฀3.฀ The฀pocket฀calculator฀works฀faster,฀because฀averages฀can฀be฀used.
฀4.฀ With฀ statistical฀ software฀ all฀ individual฀ data฀ have฀ to฀ be฀ included฀ separately,฀ a฀
</p>
<p>time-consuming฀activity฀in฀case฀of฀large฀data฀files.
</p>
<p>Also,฀ some฀ analytical฀methods,฀ for฀ example,฀ power฀ calculations฀ and฀ required฀
sample฀size฀calculations฀are฀difficult฀on฀a฀statistical฀software฀program,฀and฀easy฀on฀
</p>
<p>Preface</p>
<p/>
</div>
<div class="page"><p/>
<p>vi Preface
</p>
<p>a฀ pocket฀ calculator.฀ The฀ current฀ book฀ reviews฀ the฀ pocket฀ calculator฀ methods฀
together฀with฀practical฀examples.฀This฀book฀was฀produced฀together฀with฀the฀simi-
larly฀ sized฀book฀&ldquo;SPSS฀ for฀Starters&rdquo;฀ from฀ the฀ same฀authors฀ (edited฀by฀Springer,฀
Dordrecht฀2010).฀The฀two฀books฀complement฀one฀another.฀However,฀they฀can฀be฀
studied฀separately฀as฀well.
</p>
<p>Lyon฀ Ton฀J.฀Cleophas
December฀2010฀ Aeilko฀H.฀Zwinderman</p>
<p/>
</div>
<div class="page"><p/>
<p>vii
</p>
<p> 1 Introduction .............................................................................................฀ 1
</p>
<p> 2 Standard Deviations ................................................................................ 3
</p>
<p> 3 t-Tests ....................................................................................................... 5
</p>
<p>1฀Sample฀t-Test ......................................................................................... 5
Paired฀t-Test .............................................................................................. 6
Unpaired฀t-Test ..........................................................................................฀ 7
</p>
<p> 4 Non-Parametric Tests .............................................................................฀ 9
Wilcoxon฀Test ...........................................................................................฀ 9
Mann-Whitney฀Test ...................................................................................฀ 10
</p>
<p> 5 Confidence Intervals ...............................................................................฀ 15
</p>
<p> 6 Equivalence Tests ....................................................................................฀ 17
</p>
<p> 7 Power Equations .....................................................................................฀ 19
</p>
<p> 8 Sample Size ..............................................................................................฀ 23
Continuous฀Data,฀Power฀50% ...................................................................฀ 23
Continuous฀Data,฀Power฀80% ...................................................................฀ 24
Continuous฀Data,฀Power฀80%,฀2฀Groups ..................................................฀ 24
Binary฀Data,฀Power฀80% ...........................................................................฀ 25
Binary฀Data,฀Power฀80%,฀2฀Groups ..........................................................฀ 25
</p>
<p> 9 Noninferiority Testing .............................................................................฀ 27
Step฀1:฀Determination฀of฀the฀Margin฀of฀Noninferiority,฀ 
the฀Required฀Sample,฀and฀the฀Expected฀p-Value฀ 
and฀Power฀of฀the฀Study฀Result ..................................................................฀ 27
Step฀2:฀Testing฀the฀Significance฀of฀Difference฀Between฀ 
the฀New฀and฀the฀Standard฀Treatment ........................................................฀ 28
</p>
<p>Contents</p>
<p/>
</div>
<div class="page"><p/>
<p>viii Contents
</p>
<p>Step฀3:฀Testing฀the฀Significance฀of฀Difference฀Between฀ 
the฀New฀Treatment฀and฀a฀Placebo .............................................................฀ 28
Conclusion ................................................................................................฀ 28
</p>
<p>10 Z-Test for Cross-Tabs .............................................................................฀ 29
</p>
<p>11 Chi-Square Tests for Cross-Tabs ...........................................................฀ 31
First฀Example฀Cross-Tab ...........................................................................฀ 31
Chi-Square฀Table฀(c2-Table) .....................................................................฀ 31
Second฀Example฀Cross-Tab ...................................................................... 33
Example฀for฀Practicing฀1 .......................................................................... 33
Example฀for฀Practicing฀2 ..........................................................................฀ 34
</p>
<p>12 Odds Ratios.............................................................................................. 35
</p>
<p>13 Log Likelihood Ratio Tests .....................................................................฀ 37
</p>
<p>14 McNemar&rsquo;s Tests  ....................................................................................฀ 39
Example฀McNemar&rsquo;s฀Test .........................................................................฀ 39
McNemar฀Odds฀Ratios,฀Example .............................................................฀ 40
</p>
<p>15 Bonferroni t-Test .....................................................................................฀ 41
Bonferroni฀t-Test .......................................................................................฀ 41
</p>
<p>16 Variability Analysis .................................................................................฀ 43
One฀Sample฀Variability฀Analysis ..............................................................฀ 43
Two฀Sample฀Variability฀Test .....................................................................฀ 44
</p>
<p>17 Confounding ............................................................................................฀ 47
</p>
<p>18 Interaction ...............................................................................................฀ 49
Example฀of฀Interaction ..............................................................................฀ 50
</p>
<p>19 Duplicate Standard Deviation for Reliability Assessment  
</p>
<p>of Continuous Data .................................................................................฀ 51
</p>
<p>20 Kappas for Reliability Assessment of Binary Data .............................. 53
</p>
<p>Final Remarks ................................................................................................. 55
</p>
<p>Index .................................................................................................................฀ 57</p>
<p/>
</div>
<div class="page"><p/>
<p>1T.J. Cleophas and A.H. Zwinderman, Statistical Analysis of Clinical Data on a Pocket
</p>
<p>Calculator: Statistics on a Pocket Calculator, DOI 10.1007/978-94-007-1211-9_1,
</p>
<p>&copy; Springer Science+Business Media B.V. 2011
</p>
<p>This book contains all statistical tests that are relevant to starting clinical inves-
</p>
<p>tigators. It begins with standard deviations and t-tests, the basic tests for the analysis 
</p>
<p>of continuous data. Next, non-parametric tests are reviewed. They are, particularly, 
</p>
<p>important to investigators whose affection towards medical statistics is little, 
</p>
<p>because they are universally applicable, i.e., irrespective of the spread of the data. 
</p>
<p>Then, confidence intervals and equivalence testing as methods based on confidence 
</p>
<p>intervals are explained.
</p>
<p>In the next chapters power-equations that estimate the statistical power of data 
</p>
<p>samples are reviewed. Methods for calculating the required sample size for a mean-
</p>
<p>ingful study, are the next subject. Non-inferiority testing including comparisons 
</p>
<p>against historical data and sample size assessments are, subsequently, explained. 
</p>
<p>The methods for assessing binary data include: z-tests, chi-square for cross-tabs, 
</p>
<p>log likelihood ratio tests and odds ratio tests. Mc Nemar&rsquo;s tests for the assessment 
</p>
<p>of paired binary data is the subject of Chap. 14. Then, the Bonferroni test for adjust-
</p>
<p>ment of multiple testing is reviewed, as well as chi-square en F-tests for variability 
</p>
<p>analysis of respectively one and two groups of patients.
</p>
<p>In the final chapters the assessment of possible confounding and possible inter-
</p>
<p>action is assessed. Also reliability assessments for continuous and binary data are 
</p>
<p>reviewed.
</p>
<p>Each test method is reported together with (1) a data example from practice, 
</p>
<p>(2) all steps to be taken using a scientific pocket calculator, and (3) the main results 
</p>
<p>and their interpretation. All of the methods described are fast, and can be correctly 
</p>
<p>carried out on a scientific pocket calculator, such as the Casio fx-825, the Texas 
</p>
<p>TI-30, the Sigma AK222, the Commodoor and many other makes. Although several 
</p>
<p>of the described methods can also be carried out with the help of statistical software, 
</p>
<p>the latter procedure will be considerably slower.
</p>
<p>In order to obtain a better overview of the different test methods each chapter 
</p>
<p>will start on an uneven page. The pocket calculator book will be applied as a major 
</p>
<p>help to the workshops &ldquo;Designing and performing clinical research&rdquo; organized by 
</p>
<p>the teaching department of Albert Schweitzer STZ (collaborative top clinical) 
</p>
<p>Chapter 1
</p>
<p>Introduction</p>
<p/>
</div>
<div class="page"><p/>
<p>2 1 Introduction
</p>
<p>Hospital Dordrecht, and the statistics modules at the European College of 
</p>
<p>Pharmaceutical Medicine, Claude Bernard University, Lyon, and Academic Medical 
</p>
<p>Center, Amsterdam.
</p>
<p>The authors of this book are aware that it consists of a minimum of text and do 
</p>
<p>hope that this will enhance the process of mastering the methods. Yet we recom-
</p>
<p>mend that for a better understanding of the test procedures the book be used 
</p>
<p>together with the same authors&rsquo; textbook &ldquo;Statistics Applied to Clinical Trials&rdquo; 4th 
</p>
<p>edition edited 2009, by Springer Dordrecht Netherlands. More complex data files 
</p>
<p>like data files with multiple treatment modalities or multiple predictor variables can 
</p>
<p>not be analyzed with a pocket calculator. We recommend that the in 2010 by the 
</p>
<p>same editor published book &ldquo;SPSS for Starters&rdquo; (Springer, Dordrecht, 2010) from 
</p>
<p>the same authors be used as a complementary help for the readers&rsquo; benefit.
</p>
<p>The human brain excels in making hypotheses, but 
</p>
<p>hypotheses have to be tested with hard data.</p>
<p/>
</div>
<div class="page"><p/>
<p>3
</p>
<p>Standard deviations (SDs) are often being used for summarizing the spread of the 
</p>
<p>data from a sample. If the spread in the data is small, then the same will be true for 
</p>
<p>the standard deviation. Underneath the calculation is illustrated with the help of a 
</p>
<p>data example.
</p>
<p>55
</p>
<p>54
</p>
<p>51
</p>
<p>55
</p>
<p>53
</p>
<p>53
</p>
<p>54
</p>
<p>52+
</p>
<p>Mean =&gt; &hellip;/8 = 53.375
</p>
<p>SD=
</p>
<p>55 (55&ndash;53.375)2
</p>
<p>54 (54&ndash;53.375)2
</p>
<p>51 (51&ndash;53.375)2
</p>
<p>55 (55&ndash;53.375)2
</p>
<p>53 (53&ndash;53.375)2
</p>
<p>53 (53&ndash;53.375)2
</p>
<p>54 (54&ndash;53.375)2
</p>
<p>52 (52&ndash;53.375)2+ 
</p>
<p>SD= &hellip;&hellip;&hellip;&hellip;       =&gt;&hellip;./ n&minus;1=&gt; &radic;&hellip;.=&gt; 1.407885953
</p>
<p>Each scientific pocket calculator has a modus for data-analysis. It is helpful to 
</p>
<p>calculate in a few minutes the mean and standard deviation of a sample.
</p>
<p>Chapter 2
</p>
<p>Standard Deviations
</p>
<p>T.J. Cleophas and A.H. Zwinderman, Statistical Analysis of Clinical Data on a Pocket
</p>
<p>Calculator: Statistics on a Pocket Calculator, DOI 10.1007/978-94-007-1211-9_2,
</p>
<p>&copy; Springer Science+Business Media B.V. 2011</p>
<p/>
</div>
<div class="page"><p/>
<p>4 2 Standard Deviations
</p>
<p>Calculate standard deviation: mean = 53.375 SD = 1.407885953
</p>
<p>The next steps are required:
</p>
<p>Casio fx-825 scientific
</p>
<p>On &hellip; mode &hellip; shift &hellip; AC &hellip; 55 &hellip; M+ &hellip; 54 &hellip; M+ &hellip; 51 &hellip; M+ &hellip; 55 &hellip; M+ 
</p>
<p>&hellip; 53 &hellip; M+ &hellip; 53 &hellip; M+ &hellip; 54 &hellip; M+ &hellip; 52 &hellip; M+ &hellip; shift &hellip; [x] &hellip; shift 
</p>
<p>&hellip; sxn&ndash;1
</p>
<p>Texas TI-30 scientific
</p>
<p>On &hellip; 55 &hellip; S+ &hellip; 54 &hellip; S+ &hellip; 51 &hellip; S+ &hellip; 55 &hellip; S+ &hellip; 53 &hellip; S+ &hellip; 53 &hellip; S+ 
&hellip; 54 &hellip; S+ &hellip; 52 &hellip; S+ &hellip; 2nd &hellip; x &hellip; 2nd &hellip; sxn&ndash;1
</p>
<p>Sigma AK 222 and Commodoor
</p>
<p>On &hellip; 2ndf &hellip; on &hellip; 55 &hellip; M+ &hellip; 54 &hellip; M+ &hellip; 51 &hellip; M+ &hellip; 55 &hellip; M+ &hellip; 53 
</p>
<p>&hellip; M+ &hellip; 53 &hellip; M+ &hellip; 54 &hellip; M+ &hellip; 52 &hellip; M+ &hellip; x=&gt;M &hellip; MR
</p>
<p>Calculator: Electronic Calculator
</p>
<p>On &hellip; mode &hellip; 2 &hellip; 55 &hellip; M+ &hellip; 54 &hellip; M+ &hellip; 51 &hellip; M+ &hellip; 55 &hellip; M+ &hellip; 53  
</p>
<p>&hellip; M+ &hellip; 53 ... M+ &hellip; 54 &hellip; M+ &hellip; 52 &hellip; M+ &hellip; Shift &hellip; S-var &hellip; 1 &hellip;  
</p>
<p>= &hellip; (mean) &hellip; Shift &hellip; S-var &hellip; 3 &hellip; (sd)
</p>
<p>Example:
What is the mean value, what is de SD?
</p>
<p>5
</p>
<p>4
</p>
<p>5
</p>
<p>4
</p>
<p>5
</p>
<p>4
</p>
<p>5
</p>
<p>4</p>
<p/>
</div>
<div class="page"><p/>
<p>5T.J. Cleophas and A.H. Zwinderman, Statistical Analysis of Clinical Data on a Pocket
</p>
<p>Calculator: Statistics on a Pocket Calculator, DOI 10.1007/978-94-007-1211-9_3,
</p>
<p>&copy; Springer Science+Business Media B.V. 2011
</p>
<p>1 Sample t-Test
</p>
<p>As an example, the mean decrease in blood pressure after treatment is calculated 
</p>
<p>with the accompanying p-value. A p-value &lt;0.05 indicates that there is less than 5% 
</p>
<p>probability that such a decrease will be observed purely by the play of chance. 
</p>
<p>There is, thus, &gt;95% chance that the decrease is the result of a real blood pressure 
</p>
<p>lowering effect of the treatment. We call such a decrease statistically significant.
</p>
<p>Patient mm Hg decrease
</p>
<p>1 3
</p>
<p>2 4
</p>
<p>3 &minus;2
</p>
<p>4 3
</p>
<p>5 1
</p>
<p>6 &minus;2
</p>
<p>7 4
</p>
<p>8 3
</p>
<p>Is this decrease statistically significant?
</p>
<p>Mean decrease 1.75 mmHg
</p>
<p>SD 2.49 mmHg
</p>
<p>=
=
</p>
<p>From the standard deviation the standard error (SE) can be calculated using the 
</p>
<p>equation
</p>
<p>( )SE SD / n n sample s
 
</p>
<p>ize
</p>
<p>SE 2.49 / 8 0.88
</p>
<p>= &radic; ฀฀฀ =
</p>
<p>= &radic; =
</p>
<p>De t-value is the test-statistic of the t-test and is calculated as follows:
</p>
<p>t 1.75 / 0.88 1.9886= =
</p>
<p>Chapter 3
</p>
<p>t-Tests</p>
<p/>
</div>
<div class="page"><p/>
<p>6 3 t-Tests
</p>
<p>Because the sample size is 8, the test has here 8&minus;1 = 7 degrees of freedom.
</p>
<p>The t-table on the pages 7&ndash;8 shows that with 7 degrees of freedom the p-value 
</p>
<p>should equal: 0.05 &lt; p &lt; 0.10. This result is close to statistically significant, and is 
</p>
<p>called a trend to significance.
</p>
<p>Paired t-Test
</p>
<p>Two rows of observations in ten persons are given underneath:
</p>
<p>Observation 1:
</p>
<p>6.0, 7.1, 8.1, 7.5, 6.4, 7.9, 6.8, 6.6, 7.3, 5.6
</p>
<p>Observation 2:
</p>
<p>5.1, 8.0, 3.8, 4.4, 5.2, 5.4, 4.3, 6.0, 3.7, 6.2
</p>
<p>Individual differences
</p>
<p>0.9, &minus;0.9, 4.3, 3.1, 1.2, 2.5, 2.5, 0.6, 3.8, &minus;0.6
</p>
<p> A. not significant
</p>
<p> B. 0.05 &lt; p &lt; 0.10
</p>
<p> C. P &lt; 0.05
</p>
<p> D. P &lt; 0.01
</p>
<p>Is there a significant difference between the observation 1 and 2, and which level 
</p>
<p>of significance is correct?
</p>
<p>Mean difference 1.59
</p>
<p>SD of mean difference 1.789
</p>
<p>SE SD / 10 0.566
</p>
<p>t 1.59 / 0.566 2.809
</p>
<p>=
=
</p>
<p>= &radic; =
= =
</p>
<p>10&minus;1 = 9 degrees of freedom, because we have 10 patients and 1 group of patients.
</p>
<p>According to the t-table of page XXX the p-value equals &lt;0.05, and we can 
</p>
<p>conclude that a significant difference between the two observations is in the data: 
</p>
<p>the values of row 1 are significantly higher than those of row 2. The answer C is 
</p>
<p>correct.</p>
<p/>
</div>
<div class="page"><p/>
<p>7Unpaired t-Test
</p>
<p>Unpaired t-Test
</p>
<p>Two matched groups of patients are compared with one another.
</p>
<p>Group 1:
</p>
<p>6.0, 7.1, 8.1, 7.5, 6.4, 7.9, 6.8, 6.6, 7.3, 5.6
</p>
<p>Group 2:
</p>
<p>5.1, 8.0, 3.8, 4.4, 5.2, 5.4, 4.3, 6.0, 3.7, 6.2
</p>
<p>Mean Group 1 = 6.93 SD = 0.806 SE = SD/&radic;10 = 0.255
Mean Group 2 = 5.21 SD = 1.299 SE = SD/&radic;10 = 0.411
</p>
<p> A. not significant
</p>
<p> B. 0.05 &lt; p &lt; 0.10
</p>
<p> C. p &lt; 0.05
</p>
<p> D. P &lt; 0.01
</p>
<p>Is there a significant difference between the two groups, which level of significance 
</p>
<p>is correct?
</p>
<p>Mean Standard deviation (SD)
</p>
<p>6.93 0.806
</p>
<p>5.21&ndash; 1.299
</p>
<p>1.72 pooled SE 
2 20.806 1.299
</p>
<p>0.483
10 10
</p>
<p> 
= + = 
</p>
<p> 
</p>
<p>The t-value = (6.93&minus;5.21)/0.483 = 3.56.
</p>
<p>20&minus;2 = 18 degrees of freedom, because we have 20 patients and 2 groups.
</p>
<p>According to the t-table of page the p-value is &lt;0.01, and we can conclude that 
</p>
<p>that a very significant difference exists between the two groups. The values of 
</p>
<p>group 1 are higher than those of group 2. The answer D is correct.
</p>
<p>t-Table
</p>
<p>df 0.1 0.05 0.01 0.002
</p>
<p>1 6.314 12.706 63.657 318.31
</p>
<p>2 2.920 4.303 9.925 22.326
</p>
<p>3 2.353 3.182 5.841 10.213
</p>
<p>4 2.132 2.776 4.604 7.173
</p>
<p>5 2.015 2.571 4.032 5.893
</p>
<p>6 1.943 2.447 3.707 5.208
</p>
<p>7 1.895 2.365 3.499 4.785
</p>
<p>8 1.860 2.306 3.355 4.501
</p>
<p>9 1.833 2.262 3.250 4.297
</p>
<p>(continued)</p>
<p/>
</div>
<div class="page"><p/>
<p>8 3 t-Tests
</p>
<p>df 0.1 0.05 0.01 0.002
</p>
<p>10 1.812 2.228 3.169 4.144
</p>
<p>11 1.796 2.201 3.106 4.025
</p>
<p>12 1.782 2.179 3.055 3.930
</p>
<p>13 1.771 2.160 3.012 3.852
</p>
<p>14 1.761 2.145 2.977 3.787
</p>
<p>15 1.753 2.131 2.947 3.733
</p>
<p>16 1.746 2.120 2.921 3.686
</p>
<p>17 1.740 2.110 2.898 3.646
</p>
<p>18 1.734 2.101 2.878 3.610
</p>
<p>19 1.729 2.093 2.861 3.579
</p>
<p>20 1.725 2.086 2.845 3.552
</p>
<p>21 1.721 2.080 2.831 3.527
</p>
<p>22 1.717 2.074 2.819 3.505
</p>
<p>23 1.714 2.069 2.807 3.485
</p>
<p>24 1.711 2.064 2.797 3.467
</p>
<p>25 1.708 2.060 2.787 3.450
</p>
<p>26 1.706 2.056 2.779 3.435
</p>
<p>27 1.701 2.052 2.771 3.421
</p>
<p>28 1.701 2.048 2.763 3.408
</p>
<p>29 1.699 2.045 2.756 3.396
</p>
<p>30 1.697 2.042 2.750 3.385
</p>
<p>40 1.684 2.021 2.704 3.307
</p>
<p>60 1.671 2.000 2.660 3.232
</p>
<p>120 1.658 1.950 2.617 3.160
</p>
<p>&infin; 1.645 1.960 2.576 3.090
The rows give t-values adjusted for degrees of freedom. The 
</p>
<p>numbers of degrees of freedom largely correlate with the 
</p>
<p>sample size of a study. With large samples the frequency 
</p>
<p>distribution of the data will be a little bit narrower, and that 
</p>
<p>is corrected in the table. The t-values are to be looked upon 
</p>
<p>as mean results of studies, but not expressed in mmol/l, 
</p>
<p>kilograms, but in so-called SE-units (Standard error units), 
</p>
<p>that are obtained by dividing your mean result by its own 
</p>
<p>standard error. A t-value of 3.56 with 18 degrees of freedom 
</p>
<p>indicates that we will need the row no. 18 of the table. The 
</p>
<p>upper row gives the area under the curve of the Gaussian-like 
</p>
<p>t-distribution. The t-value 3.56 is left from 3.610. Now look 
</p>
<p>right up to the upper row: we are right from 0.01. The 
</p>
<p>p-value equals &lt;0.01
</p>
<p>t-Table (continued)</p>
<p/>
</div>
<div class="page"><p/>
<p>9
</p>
<p>Wilcoxon Test
</p>
<p>The t-tests reviewed in the previous chapter are suitable for studies with normally 
</p>
<p>distributed results. However, if there are outliers, then the t-tests are not sensitive 
</p>
<p>and non-parametric tests have to be applied. We should add that non-parametric are 
</p>
<p>also adequate for testing normally distributed data. And, so, these tests are, actually, 
</p>
<p>universal, and are, therefore, absolutely to be recommended.
</p>
<p>Calculate the p-value with the paired Wilcoxon test.
</p>
<p>Observation 1:
</p>
<p>6.0, 7.1, 8.1, 7.5, 6.4, 7.9, 6.8, 6.6, 7.3, 5.6
</p>
<p>Observation 2:
</p>
<p>5.1, 8.0, 3.8, 4.4, 5.2, 5.4, 4.3, 6.0, 7.3, 6.2
</p>
<p>Individual differences:
</p>
<p>0.9, &minus;0.9, 4.3, 3.1, 1.2, 2.5, 2.5, 0.6, 3.6, &minus;0.6
</p>
<p>Rank number:
</p>
<p>3.5, 3.5, 10, 7, 5, 8, 6, 2, 9, 1
</p>
<p> A. not significant
</p>
<p> B. 0.05 &lt; p &lt; 0.10
</p>
<p> C. p &lt; 0.05
</p>
<p> D. P &lt; 0.01
</p>
<p>Is there a significant difference between observation 1 and 2? Which significance 
</p>
<p>level is correct?
</p>
<p>The individual differences are given a rank number dependent on their magnitude of 
</p>
<p>difference. If two differences are identical, and if they have for example the rank 
</p>
<p>numbers 3 and 4, then an average rank number is given to both of them, which 
</p>
<p>Chapter 4
</p>
<p>Non-Parametric Tests
</p>
<p>T.J. Cleophas and A.H. Zwinderman, Statistical Analysis of Clinical Data on a Pocket
</p>
<p>Calculator: Statistics on a Pocket Calculator, DOI 10.1007/978-94-007-1211-9_4,
</p>
<p>&copy; Springer Science+Business Media B.V. 2011</p>
<p/>
</div>
<div class="page"><p/>
<p>10 4 Non-Parametric Tests
</p>
<p>means 3.5 and 3.5. Next, all positive and all negative rank numbers have to be 
</p>
<p>added up separately. We will find 4.5 and 50.5. According to the Wilcoxon table 
</p>
<p>underneath the smaller one of the two add-up numbers must be smaller than 8 in 
</p>
<p>order to be able to speak of a p-value &lt;0.05. This is true in our example.
</p>
<p>Wilcoxon test table
</p>
<p>Number of pairs P &lt; 0.05 P &lt; 0.01
</p>
<p> 7  2  0
</p>
<p> 8  2  0
</p>
<p> 9  6  2
</p>
<p>10  8  3
</p>
<p>11 11  5
</p>
<p>12 14  7
</p>
<p>13 17 10
</p>
<p>14 21 13
</p>
<p>15 25 16
</p>
<p>16 30 19
</p>
<p>Mann-Whitney Test
</p>
<p>Like the Wilcoxon test, being the non-parametric alternative for the paired 
</p>
<p>t-test, the Mann-Whitney test is the non-parametric alternative for the unpaired 
</p>
<p>t-test. Also this test is applicable for all kinds of data, and, therefore, particu-
</p>
<p>larly, to be recommended for investigators with little affection for medical 
</p>
<p>statistics.
</p>
<p>Calculate the p-value of the difference between two groups of ten patients with 
</p>
<p>the help of this test.</p>
<p/>
</div>
<div class="page"><p/>
<p>11Mann-Whitney Test
</p>
<p>Group 1:
</p>
<p>6.0 7.1, 8.1, 7.5, 6.4, 7.9, 6.8, 6.6, 7.3, 5.6
</p>
<p>Group 2:
</p>
<p>5.1, 8.0, 3.8, 4.4, 5.2, 5.4, 4.3, 6.0, 3.7, 6.2
</p>
<p> A. not significant
</p>
<p> B. 0.05 &lt; p &lt; 0.10
</p>
<p> C. p &lt; 0.05
</p>
<p> D. p &lt; 0.01
</p>
<p>Is there a significant difference between the two groups? What significance level 
</p>
<p>is correct?
</p>
<p>All values are ranked together in ascending order of magnitude. The values from 
</p>
<p>group 1 are printed thin, those from group 2 are printed fat. Add a rank number to 
</p>
<p>each value. If there are identical values, for example, the rank numbers 9 and 10, 
</p>
<p>then replace those rank numbers with average rank numbers, 9.5 and 9.5.
</p>
<p>Subsequently, all fat printed rank numbers are added up, and so are the thin 
</p>
<p>printed rank numbers. We will find the values 142.5 for fat print, and 67.5 for 
</p>
<p>thin print.
</p>
<p>According to the Mann-Whitney table of page 13, the difference should be larger 
</p>
<p>than 71 in order for the significance level of difference to be &lt;0.05. We find a dif-
</p>
<p>ference of 75, which means that there is a p-value &lt;0.05 and that the difference 
</p>
<p>between the two groups is, thus, significant.
</p>
<p>3.7  1
</p>
<p>3.8  2
</p>
<p>4.3  3
</p>
<p>4.4  4
</p>
<p>5.1  5
</p>
<p>5.2  6
</p>
<p>5.4  7
</p>
<p>5.6  8
</p>
<p>6.0  9.5
</p>
<p>6.0  9.5
</p>
<p>6.2 11
</p>
<p>6.4 12
</p>
<p>6.6 13
</p>
<p>6.8 14
</p>
<p>7.1 15
</p>
<p>7.3 16
</p>
<p>7.5 17
</p>
<p>7.9 18
</p>
<p>8.0 19
</p>
<p>8.1 20</p>
<p/>
</div>
<div class="page"><p/>
<p>12 4 Non-Parametric Tests
</p>
<p>Mann-Whitney test
</p>
<p>P &lt; 0.01 levels
</p>
<p>n
1
&rarr;
</p>
<p>n
2
&darr; 2 3 4 5 6 7 8 9 10 11 12 13 14 15
</p>
<p> 4 10
</p>
<p> 5  6 11 17
</p>
<p> 6  7 12 18 26
</p>
<p> 7  7 13 20 27 36
</p>
<p> 8 3  8 14 21 29 38 49
</p>
<p> 9 3  8 15 22 31 40 51 63
</p>
<p>10 3  9 15 23 32 42 53 65  78
</p>
<p>11 4  9 16 24 34 44 55 68  81  96
</p>
<p>12 4 10 17 26 35 46 58 71  85  99 115
</p>
<p>13 4 10 18 27 37 48 60 73  88 103 119 137
</p>
<p>14 4 11 19 28 38 50 63 76  91 106 123 141 160
</p>
<p>15 4 11 20 29 40 52 65 79  94 110 127 145 164 185
</p>
<p>16 4 12 21 31 42 54 67 82  97 114 131 150 169
</p>
<p>17 5 12 21 32 43 56 70 84 100 117 135 154
</p>
<p>18 5 13 22 33 45 58 72 87 103 121 139
</p>
<p>19 5 13 23 34 46 60 74 90 107 124
</p>
<p>20 5 14 24 35 48 62 77 93 110
</p>
<p>21 6 14 25 37 50 64 79 95
</p>
<p>22 6 15 26 38 51 66 82
</p>
<p>23 6 15 27 39 53 68
</p>
<p>24 6 16 28 40 55
</p>
<p>25 6 16 28 42
</p>
<p>26 7 17 29
</p>
<p>27 7 17
</p>
<p>28 7
</p>
<p>The values are the minimal differences that are statistically significant with a p-value &lt;0.01. The 
</p>
<p>upper row gives the size of Group 1, the left column the size of Group 2</p>
<p/>
</div>
<div class="page"><p/>
<p>13Mann-Whitney Test
</p>
<p>Mann-Whitney test
</p>
<p>P &lt; 0.05 levels
</p>
<p>n
1
&rarr;
</p>
<p>n
2
&darr; 2 3 4 5 6 7 8 9 10 11 12 13 14 15
</p>
<p> 5 15
</p>
<p> 6 10 16 23
</p>
<p> 7 10 17 24 32
</p>
<p> 8 11 17 25 34 43
</p>
<p> 9 6 11 18 26 35 45 56
</p>
<p>10 6 12 19 27 37 47 58 71
</p>
<p>11 6 12 20 28 38 49 61 74  87
</p>
<p>12 7 13 21 30 40 51 63 76  90 106
</p>
<p>13 7 14 22 31 41 53 65 79  93 109 125
</p>
<p>14 7 14 22 32 43 54 67 81  96 112 129 147
</p>
<p>15 8 15 23 33 44 56 70 84  99 115 133 151 171
</p>
<p>16 8 15 24 34 46 58 72 86 102 119 137 155
</p>
<p>17 8 16 25 36 47 60 74 89 105 122 140
</p>
<p>18 8 16 26 37 49 62 76 92 108 125
</p>
<p>19 3 9 17 27 38 50 64 78 94 111
</p>
<p>20 3 9 18 28 39 52 66 81 97
</p>
<p>21 3 9 18 29 40 53 68 83
</p>
<p>22 3 10 19 29 42 55 70
</p>
<p>23 3 10 19 30 43 57
</p>
<p>24 3 10 20 31 44
</p>
<p>25 3 11 20 32
</p>
<p>26 3 11 21
</p>
<p>27 4 11
</p>
<p>28 4
</p>
<p>The values are the minimal differences that are statistically significant with a p-value &lt;0.01. The 
</p>
<p>upper row gives the size of Group 1, the left column the size of Group 2</p>
<p/>
</div>
<div class="page"><p/>
<p>15T.J. Cleophas and A.H. Zwinderman, Statistical Analysis of Clinical Data on a Pocket
</p>
<p>Calculator: Statistics on a Pocket Calculator, DOI 10.1007/978-94-007-1211-9_5,
</p>
<p>&copy; Springer Science+Business Media B.V. 2011
</p>
<p>The 95% confidence interval of a study represents an interval covering 95% of 
</p>
<p>many studies similar to our study. It tells you something about what you can expect 
</p>
<p>from future data: if you repeat the study, you will be 95% sure that the outcome will 
</p>
<p>be within the 95% confidence interval. The 95% confidence of a study is found by 
</p>
<p>the equation
</p>
<p>95% confidence interval mean 2 standard error (SE)= &plusmn; &acute;
</p>
<p>The SE is equal to the standard deviation (SD)/&radic;n, where n = the sample size of 
your study. The SD can be calculated from the procedure reviewed in the Chap. 2.
</p>
<p>With an SD of 1.407885953 and a sample size of n = 8,
</p>
<p>your SE 1.407885953 / 8
</p>
<p>0.4977
</p>
<p>= &Ouml;
=
</p>
<p>With a mean value of your study of 53.375
</p>
<p>your 95% confidence interval 53.375 2 0.4977
</p>
<p>between 52.3796 and 54.3704.
</p>
<p>= &plusmn; &acute;
=
</p>
<p>The mean study results are often reported together with 95% confidence intervals. 
</p>
<p>They are also the basis for equivalence studies, which will be reviewed in the next 
</p>
<p>chapter. Also for study results expressed in the form of numbers of events, propor-
</p>
<p>tion of deaths, odds ratios of events, etc., 95% confidence intervals can be readily 
</p>
<p>calculated. Plenty software on the Internet is available to help you calculate the 
</p>
<p>correct confidence intervals.
</p>
<p>Chapter 5
</p>
<p>Confidence Intervals</p>
<p/>
</div>
<div class="page"><p/>
<p>17T.J. Cleophas and A.H. Zwinderman, Statistical Analysis of Clinical Data on a Pocket
</p>
<p>Calculator: Statistics on a Pocket Calculator, DOI 10.1007/978-94-007-1211-9_6,
</p>
<p>&copy; Springer Science+Business Media B.V. 2011
</p>
<p>Equivalence testing is important, if you expect a new treatment to be equally 
</p>
<p>efficaceous as the standard treatment. This new treatment may still be better suit-
</p>
<p>able for practice, if it has fewer adverse effects or other ancillary advantages.
</p>
<p>For the purpose of equivalence testing we need to set boundaries of equivalence 
</p>
<p>prior to the study. After the study we check whether the 95% confidence interval of 
</p>
<p>the study is entirely within the boundaries.
</p>
<p>As an example, in a blood pressure study a difference between the new and 
</p>
<p>standard treatment between &minus;10 and +10 mm Hg is assumed to smaller than clini-
</p>
<p>cally relevant. The boundary of equivalence is, thus, between &minus;10 and +10 mm Hg. 
</p>
<p>This boundary is a priori defined in the protocol.
</p>
<p>Then, the study is carried out, and both the new and the standard treatment pro-
</p>
<p>duce a mean reduction in blood pressure of 10 mm Hg (parallel-group study of 
</p>
<p>20 patients) with standard errors 10 mm Hg.
</p>
<p>The mean difference 10 10 mm Hg
</p>
<p>0 mm Hg
</p>
<p>= -
=
</p>
<p>The standard errors of the mean differences = 10 mm Hg
</p>
<p>The pooled standard error (n 10) (100 /10 100 /10) mm Hg
</p>
<p>20 mm Hg
</p>
<p>4.47 mm Hg
</p>
<p>= = &Ouml; +
</p>
<p>= &Ouml;
=
</p>
<p>The 95% confidence interval of this study 0 2 4.47 mm Hg
</p>
<p>between 8.94 and 8.97 mm Hg
</p>
<p>= &plusmn; &acute;
= - +
</p>
<p>This result is entirely within the a priori defined boundary of equivalence, which 
</p>
<p>means that equivalence is demonstrated in this study.
</p>
<p>Chapter 6
</p>
<p>Equivalence Tests</p>
<p/>
</div>
<div class="page"><p/>
<p>19T.J. Cleophas and A.H. Zwinderman, Statistical Analysis of Clinical Data on a Pocket
</p>
<p>Calculator: Statistics on a Pocket Calculator, DOI 10.1007/978-94-007-1211-9_7,
</p>
<p>&copy; Springer Science+Business Media B.V. 2011
</p>
<p>Power can be defined as statistical conclusive force. A study result is often 
</p>
<p>expressed in the form of the mean result and its standard deviation (SD) or standard 
</p>
<p>error (SE). With the mean result getting larger and the standard error getting 
</p>
<p>smaller, the study obtains increasing power.
</p>
<p>What is the power of the underneath study?
</p>
<p>A blood pressure study shows a mean decrease in blood pressure of 10.8 mm Hg 
</p>
<p>with a standard error of 3.0 mm Hg. Results from study samples are often given in 
</p>
<p>grams, liters, Euros, mm Hg etc. For the calculation of power we have to standardize 
</p>
<p>our study result, which means that the mean result has to be divided by its own 
</p>
<p>standard error:
</p>
<p>Mean SE
</p>
<p> mean / SE SE / SE
</p>
<p> t-value 1.
</p>
<p>&plusmn;
= &plusmn;
= &plusmn;
</p>
<p>The t-values are found in the t-table, can be looked upon as standardized results 
</p>
<p>of all kinds of studies.
</p>
<p>Chapter 7
</p>
<p>Power Equations</p>
<p/>
</div>
<div class="page"><p/>
<p>20 7 Power Equations
</p>
<p>In our blood pressure study the t-value = 10.8/3.0 = 3.6. The unit of the t-value is 
</p>
<p>not mm Hg, but rather SE-units. The question is: what power does the study have, 
</p>
<p>if we assume a type I error (alpha) = 5% and a sample size of n = 20.
</p>
<p>The question is: what is the power of this study if we assume a type I error (alpha) 
</p>
<p>of 5%, and will have a sample size of n = 20.
</p>
<p> A. 90% &lt; power &lt; 95%,
</p>
<p> B. power &gt; 80%,
</p>
<p> C. power &lt; 75%,
</p>
<p> D. power &gt; 75%.
</p>
<p>n = 20 indicates 20&minus;2 = 18 degrees of freedom in the case of two groups of ten 
</p>
<p>patients each.
</p>
<p>We will use the following power equation (prob = probability, z = value on the 
</p>
<p>z-line (the x-axis of the t-distribution)
</p>
<p>1Power 1 prob (z t t )= - &lt; -
</p>
<p>1
</p>
<p>1 1
</p>
<p>1
</p>
<p>t the t-value of your results,
</p>
<p>t the t-value,  that matches a p- value of 0.05 2.1;
</p>
<p>t 3.6; t 2.1;  t t 1.5;
</p>
<p>prob (z t t ) beta type II error 0.05 0.1
</p>
<p>1-beta power 0.9 0.95 between 90% and 95%.
</p>
<p>=
</p>
<p>= =
</p>
<p>= = - =
</p>
<p>&lt; - = = = -
= = - =
</p>
<p>So, there is a very good power here. See below for explanation of the 
</p>
<p>calculation.
</p>
<p>Explanation of the above calculation.
</p>
<p>The t-table on the next page is a more detailed version of the t-table of page 21, 
</p>
<p>and is adequate for power calculations. The degrees of freedom are in the left 
</p>
<p> column and correlate with the sample size of a study. With large samples the fre-
</p>
<p>quency distribution of the data will be a little bit narrower, and that is corrected in 
</p>
<p>the table. The t-values are to be looked upon as mean results of studies, but not 
</p>
<p>expressed in mmol/l, kilograms, but in so-called SE-units (Standard error units), 
</p>
<p>that are obtained by dividing your mean result by its own standard error. With a 
</p>
<p>t-value of 3.6 and 18 degrees of freedom t&minus;t1 equals 1.5. This value is between 
</p>
<p>1.330 and 1.734. Look right up at the upper row for finding beta (type II error = the 
</p>
<p>chance of finding no difference where there is one). We are between 0.1 and 0.05 
</p>
<p>(10% and 5%). This is an adequate estimate of the type II error. The power equals 
</p>
<p>100% &minus; beta = between 90% and 95% in our example.
</p>
<p>t-Table
</p>
<p>Q = 0.4 0.25 0.1 0.05 0.025 0.01 0.005 0.001
</p>
<p>v 2Q = 0.8 0.5 0.2 0.1 0.05 0.02 0.01 0.002
</p>
<p>1 0.325 1.000 3.078 6.314 12.706 31.821 63.657 318.31
</p>
<p>2 0.289 0.816 1.886 2.920 4.303 6.965 9.925 22.326
(continued)</p>
<p/>
</div>
<div class="page"><p/>
<p>217 Power Equations
</p>
<p>3 0.277 0.765 1.638 2.353 3.182 4.547 5.841 10.213
</p>
<p>4 0.171 0.741 1.533 2.132 2.776 3.747 4.604 7.173
</p>
<p>5 0.267 0.727 1.476 2.015 2.571 3.365 4.032 5.893
</p>
<p>6 0.265 0.718 1.440 1.943 2.447 3.143 3.707 5.208
</p>
<p>7 0.263 0.711 1.415 1.895 2.365 2.998 3.499 4.785
</p>
<p>8 0.262 0.706 1.397 1.860 2.306 2.896 3.355 4.501
</p>
<p>9 0.261 0.703 1.383 1.833 2.262 2.821 3.250 4.297
</p>
<p>10 0.261 0.700 1.372 1.812 2.228 2.764 3.169 4.144
</p>
<p>11 0.269 0.697 1.363 1.796 2.201 2.718 3.106 4.025
</p>
<p>12 0.269 0.695 1.356 1.782 2.179 2.681 3.055 3.930
</p>
<p>13 0.259 0.694 1.350 1.771 2.160 2.650 3.012 3.852
</p>
<p>14 0.258 0.692 1.345 1.761 2.145 2.624 2.977 3.787
</p>
<p>15 0.258 0.691 1.341 1.753 2.131 2.602 2.947 3.733
</p>
<p>16 0.258 0.690 1.337 1.746 2.120 2.583 2.921 3.686
</p>
<p>17 0.257 0.689 1.333 1.740 2.110 2.567 2.898 3.646
</p>
<p>18 0.257 0.688 1.330 1.734 2.101 2.552 2.878 3.610
</p>
<p>19 0.257 0.688 1.328 1.729 2.093 2.539 2.861 3.579
</p>
<p>20 0.257 0.687 1.325 1.725 2.086 2.528 2.845 3.552
</p>
<p>21 0.257 0.686 1.323 1.721 2.080 2.518 2.831 3.527
</p>
<p>22 0.256 0.686 1.321 1.717 2.074 2.508 2.819 3.505
</p>
<p>23 0.256 0.685 1.319 1.714 2.069 2.600 2.807 3.485
</p>
<p>24 0.256 0.685 1.318 1.711 2.064 2.492 2.797 3.467
</p>
<p>25 0.256 0.684 1,316 1.708 2.060 2.485 2.787 3.450
</p>
<p>26 0.256 0.654 1,315 1.706 2.056 2.479 2.779 3.435
</p>
<p>27 0.256 0.684 1,314 1.701 2.052 2.473 2.771 3.421
</p>
<p>28 0.256 0.683 1,313 1.701 2.048 2.467 2.763 3.408
</p>
<p>29 0.256 0.683 1.311 1.699 2.045 2.462 2.756 3.396
</p>
<p>30 0.256 0.683 1.310 1.697 2.042 2.457 2.750 3.385
</p>
<p>40 0.255 0.681 1.303 1.684 2.021 2.423 2.704 3.307
</p>
<p>60 0.254 0.679 1.296 1.671 2.000 2.390 2.660 3.232
</p>
<p>120 0.254 0.677 1.289 1.658 1.950 2.358 2.617 3.160
</p>
<p>&infin; 0.253 0.674 1.282 1.645 1.960 2.326 2.576 3.090
The upper row shows p-values = Areas under the curve (AUCs) of t-distributions. The second row 
</p>
<p>gives two-sided p-values, it means that left and right end of the AUCs of the Gaussian-like curves 
</p>
<p>are added up. The left column gives the adjustment for the sample size. If it gets larger, then the 
</p>
<p>corresponding Gaussian-like curves will get a bit narrower. In this manner the estimates become 
</p>
<p>more precise and more in agreement with reality. The t-table is empirical, and has been constructed 
</p>
<p>in the 1930s of the past century with the help of simulation models and practical examples. It is 
</p>
<p>till now the basis of modern statistics, and all modern software makes extensively use of it
</p>
<p>t-Table (continued)</p>
<p/>
</div>
<div class="page"><p/>
<p>23T.J. Cleophas and A.H. Zwinderman, Statistical Analysis of Clinical Data on a Pocket
</p>
<p>Calculator: Statistics on a Pocket Calculator, DOI 10.1007/978-94-007-1211-9_8,
</p>
<p>&copy; Springer Science+Business Media B.V. 2011
</p>
<p>Continuous Data, Power 50%
</p>
<p>An essential part of clinical studies is the question, how many subjects need to be 
</p>
<p>studied in order to answer the studies&rsquo; objectives. As an example, we will use an 
</p>
<p>intended study that has an expected mean effect of 5, and a standard deviation (SD) 
</p>
<p>of 15.
</p>
<p>What required sample size do we need to obtain a significant result, or, in other 
</p>
<p>words, a p-value of at least 0.05.
</p>
<p> A. 16,
</p>
<p> B. 36,
</p>
<p> C. 64,
</p>
<p> D. 100.
</p>
<p>A suitable equation to assess this question can be constructed as follows.
</p>
<p>With a study&rsquo;s t-value of 2.0 SEM-units, a significant p-value of 0.05 will be 
</p>
<p>obtained. This should not be difficult for you to understand when you think of the 
</p>
<p>95% confidence interval of study being between &ndash; and + 2 SEM-units (Chap. 5).
</p>
<p>We assume
</p>
<p>t-value 2 SEMs
</p>
<p>(mean study result) / (standard error)
</p>
<p>(mean study result) / (standard deviation / n)
</p>
<p>(n study s sample size)&rsquo;
</p>
<p>=
=
</p>
<p>= &Ouml;
</p>
<p>=
</p>
<p>Chapter 8
</p>
<p>Sample Size</p>
<p/>
</div>
<div class="page"><p/>
<p>24 8 Sample Size
</p>
<p>From the above equation it can be derived that
</p>
<p>2
</p>
<p>2
</p>
<p>n 2 standard deviation (SD) / (mean study result)
</p>
<p>n required sample size
</p>
<p>4 ( ))SD / (mean study result
</p>
<p>4 (15 / 5) 36
</p>
<p>&Ouml; = &acute;
=
</p>
<p>= &acute;
</p>
<p>= &acute; =
</p>
<p>Answer B is correct.
</p>
<p>You are testing here whether a result of 5 is significantly different from a result of 
</p>
<p>0. Often two groups of data are compared and the standard deviations of the two 
</p>
<p>groups have to be pooled (see page 25). As stated above, with a t-value of 2.0 SEMs 
</p>
<p>a significant result of p = 0.05 is obtained. However, the power of this study is only 
</p>
<p>50%, indicating that you will have 50% chance of an insignificant result the next 
</p>
<p>time you perform a similar study.
</p>
<p>Continuous Data, Power 80%
</p>
<p>What is the required sample size of a study with an expected mean result of 5, and 
</p>
<p>SD of 15, and that should have a p-value of at least 0.05 and a power of at least 80% 
</p>
<p>(power index = (za + zb)
2 = 7.8).
</p>
<p> A. 140,
</p>
<p> B. 70,
</p>
<p> C. 280,
</p>
<p> D. 420.
</p>
<p>An adequate equation is the following.
</p>
<p>2
</p>
<p>2
</p>
<p>Required sample size power index (SD / mean)
</p>
<p>7.8 (15 / 5) 70
</p>
<p>= &acute;
</p>
<p>= &acute; =
</p>
<p>If you wish to have a power in your study of 80% instead of 50%, you will need 
</p>
<p>a larger sample size. With a power of only 50% your required sample size was 
</p>
<p>only 36.
</p>
<p>Continuous Data, Power 80%, 2 Groups
</p>
<p>What is the required sample size of a study with two groups and a mean difference 
</p>
<p>of 5 and SDs of 15 per Group, and that will have a p-value of at least 0.05 and a 
</p>
<p>power of at least 80%. (Power index = (za + zb)
2 = 7.8).</p>
<p/>
</div>
<div class="page"><p/>
<p>25Binary Data, Power 80%, 2 Groups
</p>
<p> A. 140,
</p>
<p> B. 70,
</p>
<p> C. 280,
</p>
<p> D. 420.
</p>
<p>The suitable equation is given underneath.
</p>
<p>( )
</p>
<p>2 2
</p>
<p>2 2 2
</p>
<p>1 2
</p>
<p>2 2 2
</p>
<p>Required sample size power index (pooled SD) / (mean difference)
</p>
<p>pooled SD SD SD
</p>
<p>Required sample size 7.8 (15 15 ) / 5 140.
</p>
<p>= &acute;
</p>
<p>= +
</p>
<p>= &acute; + =
</p>
<p>The required sample size is 140 patients per group. And so, with two groups you 
</p>
<p>will need considerably larger samples than you do with 1 group.
</p>
<p>Binary Data, Power 80%
</p>
<p>What is the required sample size of a study in which you expect an event in 10% 
</p>
<p>of the patients and wish to have a power of 80%.
</p>
<p>10% events means a proportion of events of 0.1.
</p>
<p>The standard deviation (SD) of this proportion is defined by the equation
</p>
<p>[proportion (1  proportion)] (0.1 0.9).&Ouml; &acute; - = &Ouml; &acute;
</p>
<p>The suitable formula is given.
</p>
<p>2 2
</p>
<p>2
</p>
<p>Required sample size power index SD / proportie
</p>
<p>7.8 (0.1 0.9) / 0.1
</p>
<p>7.8 9 71.
</p>
<p>= &acute;
</p>
<p>= &acute; &acute;
= &acute; =
</p>
<p>We conclude that with 10% events you will need about 71 patients in order to 
</p>
<p>obtain a significant number of events for a power of 80% in your study.
</p>
<p>Binary Data, Power 80%, 2 Groups
</p>
<p>What is the required sample size of a study of two groups in which you expect.
</p>
<p>A difference in events between the two groups of 10%, and in which you wish 
</p>
<p>to have a power of 80%.
</p>
<p>10% difference in events means a difference in proportions of events of 0.10.
</p>
<p>Let us assume that in Group one 10% will have an event and in Group two 20%. 
</p>
<p>The standard deviations per group can be calculated.</p>
<p/>
</div>
<div class="page"><p/>
<p>26 8 Sample Size
</p>
<p>2 2
</p>
<p>1 2
</p>
<p>2 2
</p>
<p>For group 1:  SD [proportion (1  proportion)] (0.1 0.9) 0.3.
</p>
<p>For group 2 :  SD [proportion (1  proportion)] (0.2 0.8) 0.4
</p>
<p>The pooled standard deviation of both groups (SD SD )
</p>
<p>(0.3 0.4 )
</p>
<p>0.25 0.
</p>
<p>= &Ouml; &acute; - = &Ouml; &acute; =
</p>
<p>= &Ouml; &acute; - = &Ouml; &acute; =
</p>
<p>= &Ouml; +
</p>
<p>= &Ouml; +
</p>
<p>= &Ouml; = 5
</p>
<p>The adequate equation is underneath.
</p>
<p>2 2
</p>
<p>2 2
</p>
<p>Required sample size power index (pooled SD) / (difference in proportions)
</p>
<p>7.8 0.5 / 0.1
</p>
<p>7.8 25 195.
</p>
<p>= &acute;
</p>
<p>= &acute;
= &acute; =
</p>
<p>Obviously, with a difference of 10% events between two groups we will need 
</p>
<p>about 195 patients per group in order to demonstrate a significant difference with a 
</p>
<p>power of 80%.</p>
<p/>
</div>
<div class="page"><p/>
<p>27T.J. Cleophas and A.H. Zwinderman, Statistical Analysis of Clinical Data on a Pocket 
</p>
<p>Calculator: Statistics on a Pocket Calculator, DOI 10.1007/978-94-007-1211-9_9,  
</p>
<p>&copy; Springer Science+Business Media B.V. 2011
</p>
<p>Just like equivalence studies noninferiority studies are very popular in modern 
</p>
<p>clinical research with many treatments at hand and new compounds being mostly 
</p>
<p>only slightly different from the old ones. Unlike equivalence studies (Chap. 6), non-
</p>
<p>inferiority studies have a single boundary, instead of two boundaries, with an interval 
</p>
<p>of equivalence in between. Noninferiority studies have been criticized for their wide 
</p>
<p>margin of inferiority making it virtually impossible to reject noninferiority.
</p>
<p>As an example, two parallel-groups of patients with rheumatoid arthritis are 
</p>
<p>treated with either a standard or a new nonsteroidal anti-inflammatory drug 
</p>
<p>(NSAID). The reduction of gamma globuline levels (g/l) after treatment is used as 
</p>
<p>the primary estimate of treatment success. The underneath three steps constitute an 
</p>
<p>adequate procedure for noninferiority analysis.
</p>
<p>Step 1: Determination of the Margin of Noninferiority,  
</p>
<p>the Required Sample, and the Expected p-Value  
</p>
<p>and Power of the Study Result
</p>
<p> 1. The left boundaries of the 95% confidence intervals of previously published 
</p>
<p>studies of the standard NSAID versus various alternative NSAIDS were never 
</p>
<p>lower than &minus; 8 g/l. And, so, the margin was set at &minus; 8 g/l.
</p>
<p> 2. Based on a pilot-study with the novel compound the expected mean difference 
</p>
<p>was 0 g/l with an expected standard deviation of 32 g/l. This would mean a 
</p>
<p>required sample size of
</p>
<p> 
2n power index (SD / (margin mean))= &acute; -
</p>
<p> 
2n 7.8 (32 / ( 8 0)) 125 patients per group.= &acute; - - =
</p>
<p>A power index of 7.8 takes care that noninferiority is demonstrated with a power 
</p>
<p>of about 80% in this study (see also Chap. 8).
</p>
<p>Chapter 9
</p>
<p>Noninferiority Testing</p>
<p/>
</div>
<div class="page"><p/>
<p>28 9 Noninferiority Testing
</p>
<p> 3. The mean difference between the new and standard NSAID was calculated to be 
</p>
<p>3.0 g/l with a standard error (SE) of 4.6 g/l. This means that the t-value of the study 
</p>
<p>equaled t = (margin &minus; mean)/SE = (&minus;8 &minus; 3)/4.6 = &minus;2.39 SE-units or SEM-units. This 
</p>
<p>t-value corresponds with a p-value of &lt; 0.05 (page 21 bottom row, why the bottom 
</p>
<p>row can be applied is explained in the next Chapter). Non-inferiority is, thus, 
</p>
<p> demonstrated at p &lt; 0.05.
</p>
<p>Step 2: Testing the Significance of Difference Between  
</p>
<p>the New and the Standard Treatment
</p>
<p>The mean difference between the new and standard treatment equaled 3.0 g/l with 
</p>
<p>an SE of 4.6 g/l. The 95% confidence of this result is 3.0 &plusmn; 2*4.6, and is between &minus; 6.2 
</p>
<p>and 12.2 g/l (* = sign of multiplication). This interval does cross the zero value on 
</p>
<p>the z-axis, which means no significant difference from zero (p &gt; 0.05).
</p>
<p>Step 3: Testing the Significance of Difference Between  
</p>
<p>the New Treatment and a Placebo
</p>
<p>A similarly sized published trial of the standard treatment versus placebo produced 
</p>
<p>a t-value of 2.83, and thus a p-value of 0.0047. The t-value of the current trial 
</p>
<p>equals 3.0/4.6 = 0.65 SE-units. The add-up sum 2.83 + 0.65 = 3.48 is an adequate 
</p>
<p>estimate of the comparison of the new treatment versus placebo. A t-value of 3.48 
</p>
<p>corresponds with a p-value of &lt;0.002 (see page 21, bottom row, the use of bottom 
</p>
<p>row will be explained in the next Chapter). This would mean that the new treatment 
</p>
<p>is significantly better than placebo at p &lt; 0.002.
</p>
<p>Conclusion
</p>
<p>We can now conclude that
</p>
<p> (1) noninferiority is demonstrated at p &lt; 0.05, that
</p>
<p> (2) a significant difference between the new and standard treatment is rejected at 
</p>
<p>p &gt; 0.05, and that
</p>
<p> (3) the new treatment is significantly better than placebo at p &lt; 0.002. Non-inferiority 
</p>
<p>has, thus, been unequivocally demonstrated in this study.</p>
<p/>
</div>
<div class="page"><p/>
<p>29T.J. Cleophas and A.H. Zwinderman, Statistical Analysis of Clinical Data on a Pocket 
</p>
<p>Calculator: Statistics on a Pocket Calculator, DOI 10.1007/978-94-007-1211-9_10,  
</p>
<p>&copy; Springer Science+Business Media B.V. 2011
</p>
<p>Two groups of patients are assessed for being sleepy through the day. We wish to 
</p>
<p>estimate whether group 1 is more sleepy than group 2. The underneath cross-tab 
</p>
<p>gives the data.
</p>
<p>Sleepiness No sleepiness
</p>
<p>Treatment 1 (group 1) 5 (a) 10 (b)
</p>
<p>Treatment 2 (group 2) 9 (c)  6 (d)
</p>
<p> 
</p>
<p>difference between proportions of sleepers per group (d)
z 
</p>
<p>pooled standard error difference
=
</p>
<p> 
2 2
</p>
<p>1 2
</p>
<p>d (9 /15 5 /15)
z 
</p>
<p>pooled SE (SE SE )
</p>
<p>-
= =
</p>
<p>+
</p>
<p> 
</p>
<p>1 1
</p>
<p>1 1 1
</p>
<p>1
</p>
<p>p (1 p )
SE (or SEM )  where p  5/15 etc.... . .. .,
</p>
<p>n
</p>
<p>-
= &Ouml; =
</p>
<p>z = 1.45, not statistically significant from zero, because for a p &lt; 0.05 a z-value of at 
</p>
<p>least 1.96 is required. This means that no significant difference between the two 
</p>
<p>groups is observed. The p-value of the z-test can be obtained by using the bottom 
</p>
<p>row of the t-table from page 21.
</p>
<p>Note:
</p>
<p>For the z-test a normal distribution approach can be used. The t-distributions are 
</p>
<p>usually a bit wider than the normal distributions, and therefore, adjustment for 
</p>
<p>study size using degrees of freedom (left column of the t-table) is required. With 
</p>
<p>a large study size the t-distribution is equal to the normal distribution, and 
</p>
<p>the t-values are equal to the z-values. They are given in the bottom row of the 
</p>
<p>t-table.
</p>
<p>Chapter 10
</p>
<p>Z-Test for Cross-Tabs</p>
<p/>
</div>
<div class="page"><p/>
<p>30 10 Z-Test for Cross-Tabs
</p>
<p>Note:
</p>
<p>A single group z-test is also possible. For example in ten patients we have four 
</p>
<p>responders. We question whether four responders is significantly more than zero 
</p>
<p>responders.
</p>
<p> z proportion / its SE= ( )
</p>
<p> SE [(4 /10 (1 4 /10)) / n]= &Ouml; &acute; -
</p>
<p> (0.24 /10)= &Ouml;
</p>
<p> z 0.4 / 0.24 /10= &Ouml;( )
</p>
<p> z 0.4 / 0.1549=
</p>
<p> z 2.582=
</p>
<p>According to the bottom row of the t-table from page 21 the p-value is &lt; 0.01. The 
</p>
<p>proportion of 0.4 is, thus, significantly larger than a proportion of 0.0.</p>
<p/>
</div>
<div class="page"><p/>
<p>31T.J. Cleophas and A.H. Zwinderman, Statistical Analysis of Clinical Data on a Pocket 
</p>
<p>Calculator: Statistics on a Pocket Calculator, DOI 10.1007/978-94-007-1211-9_11,  
</p>
<p>&copy; Springer Science+Business Media B.V. 2011
</p>
<p>First Example Cross-Tab
</p>
<p>The underneath table shows two separate groups with patients assessed for suffering 
</p>
<p>from sleepiness through the day. We wish to know whether there is a significant 
</p>
<p>difference between the proportions of subjects being sleepy.
</p>
<p>Sleepiness No sleepiness
</p>
<p>Group 1  5 (a) 10 (b) 15 (a + b)
</p>
<p>Group 2  9 (c)  6 (d) 15 (c + d)
</p>
<p>14 (a + c) 16 (b + d) 30 (a + b + c + d)
</p>
<p>The chi-square pocket calculator method is used for testing these data.
</p>
<p>2 2
2 (ad bc) (a b c d) 30 90 30 3,600 30
</p>
<p>(a b)(c d)(b d)(a c) 15 15 16 14 15 15 16 14
</p>
<p>108.000
2.143
</p>
<p>50.400
</p>
<p>χ - + + + ( - ) ( ) &acute;= = =
+ + + + &acute; &acute; &acute; &acute; &acute; &acute;
</p>
<p>= =
</p>
<p>The chi-square value equals 2.143. The chi-square table can tell us whether or not 
</p>
<p>the difference between the groups is significant. See next page for the procedure to 
</p>
<p>be followed.
</p>
<p>Chi-Square Table (c2-Table)
</p>
<p>The underneath chi-square table gives columns and rows: the upper row gives the 
</p>
<p>p-values. The first column gives the degrees of freedom which is here largely in 
</p>
<p>agreement with the numbers of cells in a cross-tab. The simplest cross-tab has 4 
</p>
<p>cells, which means 2 &times; 2 = 4 cells. The table has been constructed such that we have 
</p>
<p>here (2&ndash;1) &times; (2&ndash;1) = 1 degree of freedom. Look at the row with 1 degree of freedom: 
</p>
<p>a chi-square value of 2.143 is left from 2.706. Now look from here right up at the 
</p>
<p>Chapter 11
</p>
<p>Chi-Square Tests for Cross-Tabs</p>
<p/>
</div>
<div class="page"><p/>
<p>32 11 Chi-Square Tests for Cross-Tabs
</p>
<p>Chi-squared distribution
</p>
<p>Two-tailed P-value
</p>
<p>df 0.10 0.05 0.01 0.001
</p>
<p>1 2.706 3.841 6.635 10.827
</p>
<p>2 4.605 5.991 9.210 13.815
</p>
<p>3 6.251 7.815 11.345 16.266
</p>
<p>4 7.779 9.488 13.277 18.466
</p>
<p>5 9.236 11.070 15.086 20.515
</p>
<p>6 10.645 12.592 16.812 22.457
</p>
<p>7 12.017 14.067 18.475 24.321
</p>
<p>8 13.362 15.507 20.090 26.124
</p>
<p>9 14.684 16.919 21.666 27.877
</p>
<p>10 15.987 18.307 23.209 29.588
</p>
<p>11 17.275 19.675 24.725 31.264
</p>
<p>12 18.549 21.026 26.217 32.909
</p>
<p>13 19.812 22.362 27.688 34.527
</p>
<p>14 21.064 23.685 29.141 36.124
</p>
<p>15 22.307 24.996 30.578 37.698
</p>
<p>16 23.542 26.296 32.000 39.252
</p>
<p>17 24.769 27.587 33.409 40.791
</p>
<p>18 25.989 28.869 34.805 42.312
</p>
<p>19 27.204 30.144 36.191 43.819
</p>
<p>20 28.412 31.410 37.566 45.314
</p>
<p>21 29.615 32.671 38.932 46.796
</p>
<p>22 30.813 33.924 40.289 48.268
</p>
<p>23 32.007 35.172 41.638 49.728
</p>
<p>24 33.196 36.415 42.980 51.179
</p>
<p>25 34.382 37.652 44.314 52.619
</p>
<p>26 35.563 38.885 45.642 54.051
</p>
<p>27 36.741 40.113 46.963 55.475
</p>
<p>28 37.916 41.337 48.278 56.892
</p>
<p>29 39.087 42.557 49.588 58.301
</p>
<p>30 40.256 43.773 50.892 59.702
</p>
<p>40 51.805 55.758 63.691 73.403
</p>
<p>50 63.167 67.505 76.154 86.660
</p>
<p>60 74.397 79.082 88.379 99.608
</p>
<p>70 85.527 90.531 100.43 112.32
</p>
<p>80 96.578 101.88 112.33 124.84
</p>
<p>90 107.57 113.15 124.12 137.21
</p>
<p>100 118.50 124.34 135.81 149.45</p>
<p/>
</div>
<div class="page"><p/>
<p>33Example for Practicing 1
</p>
<p>upper row. The corresponding p-value is larger than 0.1 (10%). There is, thus, no 
</p>
<p>significant difference in sleepiness between the two groups. The small difference 
</p>
<p>observed is due to the play of chance.
</p>
<p>Second Example Cross-Tab
</p>
<p>Two partnerships of internists have the intention to associate. However, in one of 
</p>
<p>the two a considerable number of internists has suffered from a burn-out.
</p>
<p>Burn out No burn out
</p>
<p>Partnership 1 3 (a)  7 (b) 10 (a + b)
</p>
<p>Partnership 2 0 (c) 10 (d) 10 (c + d)
</p>
<p>3 (a + c) 17 (b + d) 20 (a + b+ c+ d)
</p>
<p>2 2
2 (ad bc) (a b c d) 30 0 20 900 20 3.529
</p>
<p>(a b)(c d)(b d)(a c) 10 10 17 3 ........
χ - + + + ( - ) ( ) &acute;= = = =
</p>
<p>+ + + + &acute; &acute; &acute;
</p>
<p>According to the chi-square table of the previous page a p-value is found of &lt;0.10.
</p>
<p>This means that no significant difference is found, but a p-value between 0.05 
</p>
<p>and 0.10 is looked upon as a trend to significance. The difference may be due to 
</p>
<p>some avoidable or unavoidable cause. We should add here that values in a cell 
</p>
<p>lower than 5 is considered slightly inappropriate according to some, and another 
</p>
<p>test like the log likelihood ratio test (Chap. 13) is more safe.
</p>
<p>Example for Practicing 1
</p>
<p>Example 2 &times; 2 table Events No events
</p>
<p>Group 1 15 (a) 20 (b) 35 (a + b)
</p>
<p>Group 2 15 (c)  5 (d) 20 (c + d)
</p>
<p>30 (a + c) 25 (b + d) 55 (a + b + c + d)</p>
<p/>
</div>
<div class="page"><p/>
<p>34 11 Chi-Square Tests for Cross-Tabs
</p>
<p>Pocket calculator
</p>
<p>2ad bc a b c d
p ...
</p>
<p>a b c d b d a c
</p>
<p>( - ) ( + + + )
= =
</p>
<p>( + )( + )( + )( + )
</p>
<p>Example for Practicing 2
</p>
<p>Another example 2 &times; 2 table Events No events
</p>
<p>Group 1 16 (a ) 26 (b) 42 (a + b)
</p>
<p>Group 2  5 (c) 30 (d) 35 (c + d)
</p>
<p>21 (a + c) 56 (b + d) 77 (a + b +c + d)
</p>
<p>Pocket calculator
</p>
<p>2ad bc a b c d
p ...
</p>
<p>a b c d b d a c
</p>
<p>( - ) ( + + + )
= =
</p>
<p>( + )( + )( + )( + )</p>
<p/>
</div>
<div class="page"><p/>
<p>35T.J. Cleophas and A.H. Zwinderman, Statistical Analysis of Clinical Data on a Pocket 
</p>
<p>Calculator: Statistics on a Pocket Calculator, DOI 10.1007/978-94-007-1211-9_12,  
</p>
<p>&copy; Springer Science+Business Media B.V. 2011
</p>
<p>The odds ratio test is just like the chi-square test applicable for testing cross-tabs. 
</p>
<p>The advantage of the odds ratio test is that a odds ratio value can be calculated. The 
</p>
<p>odds ratio value is just like the relative risk an estimate of the chance of having an 
</p>
<p>event in group 1 compared to that of group 2. An odds ratio value of 1 indicates no 
</p>
<p>difference between the two groups.
</p>
<p>Example 1
</p>
<p>Events No events
</p>
<p>Numbers of patients
</p>
<p>Group 1 15 (a) 20 (b) 35 (a + b)
</p>
<p>Group 2 15 (c )  5 (d) 20 (c + d)
</p>
<p>30 (a + c) 25 (b + d) 55 (a + b+ c+ d)
</p>
<p>The odds of an event = the number of patients in a group with an event divided 
</p>
<p>by the number without. In group 1 the odds of an event equals = a/b.
</p>
<p>The odds ratio (OR) of group 1 compared to group 2
</p>
<p> a / b / c / d= ( ) ( )
</p>
<p> 
15 / 20 / 15 / 5= ( ) ( )
</p>
<p> 0.25=
</p>
<p>lnOR ln 0.25 1.386 ln natural logarithm= = - ( = )
</p>
<p>The standard error (SE) of the above term
</p>
<p> 
1 / a 1 / b 1 / c 1 / d= &Ouml;( + + + )
</p>
<p> 
1 /15 1 / 20 1 /15 1 / 5= &Ouml;( + + + )
</p>
<p> 0.38333= &Ouml;
</p>
<p> 0.619=
</p>
<p>Chapter 12
</p>
<p>Odds Ratios</p>
<p/>
</div>
<div class="page"><p/>
<p>36 12 Odds Ratios
</p>
<p>The odds ratio can be tested using the z-test (Chap. 10).
</p>
<p> The test-statistic z-value=
</p>
<p> odds ratio / SE= ( )
</p>
<p> 1.386 / 0.619= -
</p>
<p> 2.239= -
</p>
<p>If this value is smaller than &minus;2 or larger than +2, then the odds ratio is significantly 
</p>
<p>different from 1 with p &lt; 0.05. An odds ratio of 1 means that there is no difference 
</p>
<p>in events between group 1 and group 2. The bottom row of the t-table (page 21) 
</p>
<p>gives the z-values matching Gaussian distributions. Look at a z-value of 1.96 right 
</p>
<p>up at the upper row. We will find a p-value here of 0.05. And, so, a z-value larger 
</p>
<p>than 1.96 indicates a p-value of &lt;0.05. There is a significant difference in event 
</p>
<p>between the two groups.
</p>
<p>Example 2
</p>
<p>Events No events
</p>
<p>Number of patients
</p>
<p>Group 1 16 (a ) 26 (b) 42 (a + b)
</p>
<p>Group 2  5 (c ) 30 (d) 35 (c + d)
</p>
<p>21 (a + c) 56 (b + d) 77 (a + b + c + d)
</p>
<p>Test with OR whether there is a significant difference between group 1 and 2.
</p>
<p>See for procedure also example 1.
</p>
<p> 
OR 16 / 26 / 5 / 30= ( ) ( )
</p>
<p> 3.69=
</p>
<p> 
lnOR 1.3056 (ln natural logarithm see the above example)= =
</p>
<p> 
SE 1 /16 1 / 26 1 / 5 1 / 30= &Ouml;( + + + )
</p>
<p> 0.334333= &Ouml;
</p>
<p> 0.578=
</p>
<p> z-value 1.3056 / 0.578=
</p>
<p> 2.259=
</p>
<p>Because this value is larger than 2, a p-value of &lt;0.05 is observed, 0.024 to be pre-
</p>
<p>cise (numerous &ldquo;p-calculator for z-values&rdquo; sites in Google will help you calculate 
</p>
<p>an exact p-value if required.</p>
<p/>
</div>
<div class="page"><p/>
<p>37T.J. Cleophas and A.H. Zwinderman, Statistical Analysis of Clinical Data on a Pocket 
</p>
<p>Calculator: Statistics on a Pocket Calculator, DOI 10.1007/978-94-007-1211-9_13,  
</p>
<p>&copy; Springer Science+Business Media B.V. 2011
</p>
<p>The sensitivity of the chi-square test (Chap. 11) and the odds ratio test (Chap. 12) 
</p>
<p>for testing cross-tabs is limited, and not entirely accurate if the values in one or 
</p>
<p>more cells is smaller than 5. The log likelihood ratio test is an adequate alternative 
</p>
<p>with generally better sensitivity, and, so, it must be absolutely recommended.
</p>
<p>Example 1
</p>
<p>A group of citizens is taking a pharmaceutical company to court for misrepresent-
</p>
<p>ing the danger of fatal rhabdomyolysis due to statin treatment.
</p>
<p>Patients with rhabdomyolysis Patients without
</p>
<p>Company 1 (a) 309,999 (b)
</p>
<p>Citizens 4 (c) 300,289 (d)
</p>
<p>p
co
</p>
<p>= proportion given by the pharmaceutical company = a/(a + b) = 1/310,000
</p>
<p>p
ci
</p>
<p>= proportion given by the citizens = c/(c + d) = 4/300,293
</p>
<p>We make use of the z-test (Chap. 10) for testing log likelihood ratios.
</p>
<p>As it can be shown that &minus;2 log likelihood ratio equals z2, we can test the signifi-
</p>
<p>cance of difference between the two proportions.
</p>
<p> 
</p>
<p>1 / 310,000 1 1 / 310,000
Log likelihood ratio 4 log 300289 log
</p>
<p>4 / 300,293 1 4 / 300,293
</p>
<p>-
= +
</p>
<p>-
</p>
<p> 2.641199= -
</p>
<p> 
2 log likelihood ratio 2 2.641199- = - &acute; -
</p>
<p> 
5.2824 (p 0.05,  because z 2).= &lt; &gt;
</p>
<p> 
2z=
</p>
<p>A z-value larger than 2 means a significant difference in your data (Chap. 10). Here 
</p>
<p>the z-value equals &radic;5.2824 = 2.29834. The &ldquo;p-calculator for z-values&rdquo; in Google 
tells you that the exact p-value = 0.0215, much smaller than 0.05.
</p>
<p>We should note here that both the odds ratio test and chi-square test produced a 
</p>
<p>non-significant result here (p &gt; 0.05). Indeed, the log likelihood ratio test is much 
</p>
<p>Chapter 13
</p>
<p>Log Likelihood Ratio Tests</p>
<p/>
</div>
<div class="page"><p/>
<p>38 13 Log Likelihood Ratio Tests
</p>
<p>more sensitive than the other tests for the same kind of data, which might once in 
</p>
<p>a while be a blessing for desperate investigators.
</p>
<p>Example 2
</p>
<p>Two group of 15 patients at risk for arrhythmias were assessed for the development 
</p>
<p>of torsade de points after calcium channel blockers treatment.
</p>
<p>Patients with torsade de points Patients without
</p>
<p>Calcium channel blocker 1 5 10
</p>
<p>Calcium channel blocker 2 9  6
</p>
<p>The proportion of patients with event from calcium channel blocker 1 is 5/15, 
</p>
<p>from blocker 2 it is 9/15.
</p>
<p> 
</p>
<p>5
L
</p>
<p>/
og
</p>
<p>15
 likelihood ratio 9 log 6 log
</p>
<p>9 /
</p>
<p>1 5 /1
</p>
<p>15 1 /1
</p>
<p>5
</p>
<p>9 5
= +
</p>
<p>-
-
</p>
<p> 2.25= -
</p>
<p> 
2 log likelihood ratio 4.50- =
</p>
<p> 
2z=
</p>
<p> 
z-value 4.50 2.1213= &Ouml; =
</p>
<p> 
p-value 0.05,  because z 2.&lt; &gt;
</p>
<p>Both odds ratio test and chi-square test were again non-significant (p &gt; 0.05).
</p>
<p>Example 3
</p>
<p>Two groups of patients with stage IV New York Heart Association heart failure 
</p>
<p>were assessed for clinical admission while on two beta-blockers.
</p>
<p>Patients with clinical admission Patients without
</p>
<p>Beta blocker 1  77 62
</p>
<p>Beta blocker 2 103 46
</p>
<p>The proportion of patients with event while on beta blocker 1 is 77/139, while 
</p>
<p>on beta blocker 2 it is 103/149.
</p>
<p>77 /139 1 77 /139
Log likelihood ratio 103 log 46 log
</p>
<p>103 /149 1 103 /149
</p>
<p>-
= +
</p>
<p>-
</p>
<p> 
5.882= -
</p>
<p> 
2 log likelihood ratio 11.766- =
</p>
<p> 
</p>
<p>2z=
</p>
<p> 
</p>
<p>z-value 11.766 3.43016= &Ouml; =
</p>
<p> 
</p>
<p>p-value 0.002,  because z 3.090
</p>
<p>(see the t-table on page 21).
</p>
<p>&lt; &gt;
</p>
<p>Both the odds ratio test and chi-square test were also significant. However, at 
</p>
<p>lower levels of significance, both p-values 0.01 &lt; p &lt; 0.05.</p>
<p/>
</div>
<div class="page"><p/>
<p>39T.J. Cleophas and A.H. Zwinderman, Statistical Analysis of Clinical Data on a Pocket 
</p>
<p>Calculator: Statistics on a Pocket Calculator, DOI 10.1007/978-94-007-1211-9_14,  
</p>
<p>&copy; Springer Science+Business Media B.V. 2011
</p>
<p>The past four Chapters have reviewed four methods for analyzing cross-tabs of two 
</p>
<p>groups of patients. Sometimes a single group is assessed twice, and, then, we obtain 
</p>
<p>a slightly different cross-tab. McNemar&rsquo;s test must be applied by analyzing these 
</p>
<p>kind of data.
</p>
<p>Example McNemar&rsquo;s Test
</p>
<p>315 subjects are tested for hypertension using both an automated device (test-1) and 
</p>
<p>a sphygmomanometer (test-2).
</p>
<p>Test 1
</p>
<p>+ &minus; Total
</p>
<p>Test 2 + 184  54 238
</p>
<p>&minus;  14  63  77
</p>
<p>Total 198 117 315
2(54 14)
</p>
<p>Chi - square McNemar 23.5
54 14
</p>
<p>-
= =
</p>
<p>+
</p>
<p>184 subjects scored positive with both tests and 63 scored negative with both tests. 
</p>
<p>These 247 subjects, therefore, give us no information about which of the tests is 
</p>
<p>more likely to score positive.
</p>
<p>The information we require is entirely contained in the 68 subjects for whom the 
</p>
<p>tests did not agree (the discordant pairs). The above table also shows how the chi-
</p>
<p>square value is calculated. The chi-square table (page 32) is used for finding the 
</p>
<p>appropriate p-value. Here we have again 1 degree of freedom. The 1 degree of 
</p>
<p>freedom row of the chi-square table shows that our result of 23.5 is a lot larger than 
</p>
<p>10.827. When looking up at the upper row we will find a p-value &lt; 0.001. The two 
</p>
<p>devices produce significantly different results at p &lt; 0.001.
</p>
<p>Chapter 14
</p>
<p>McNemar&rsquo;s Tests </p>
<p/>
</div>
<div class="page"><p/>
<p>40 14 McNemar&rsquo;s Tests
</p>
<p>McNemar Odds Ratios, Example
</p>
<p>Just like with the usual cross-tabs (Chap. 12) odds ratios can be calculated with the 
</p>
<p>single group cross-tabs. So far we assessed two groups, one treatment. two antihy-
</p>
<p>pertensive treatments are assessed in a single group of patients
</p>
<p>Normotension with drug 1
</p>
<p>Yes No
</p>
<p>Normotension
</p>
<p>with drug 2
Yes (a) 65 (b) 28
</p>
<p>No (c) 12 (d) 34
</p>
<p>Here the OR = b/c, and the SE is not 
1 1 1 1
</p>
<p>,
a b c d
</p>
<p>&aelig; &ouml;+ + +&ccedil; &divide;&egrave; &oslash;
 but rather 
</p>
<p>1 1
.
</p>
<p>b c
</p>
<p>&aelig; &ouml;+&ccedil; &divide;&egrave; &oslash;
</p>
<p> OR 28 /12=
</p>
<p> 2.33=
</p>
<p>lnOR ln 2.33 (ln natural logarithm)= =
</p>
<p> 0.847=
</p>
<p> 
</p>
<p>1 1
SE 0.345
</p>
<p>b c
</p>
<p>&aelig; &ouml;= + =&ccedil; &divide;&egrave; &oslash;
</p>
<p> lnOR 2 SE 0.847 0.690&plusmn; = &plusmn;
</p>
<p> between 0.157 and 1.537,=
</p>
<p>Turn the ln numbers into real numbers by the anti-ln button (the invert button, on 
</p>
<p>many calculators called the 2ndF button) of your pocket calculator.
</p>
<p> between 1.16 and 4.65=
</p>
<p> 
significantly different from 1.0.=
</p>
<p>A p-value can be calculated using the z-test (Chap. 10).
</p>
<p> z lnOR / SEM=
</p>
<p> 0.847 : 0.345=
</p>
<p> 2.455.=
</p>
<p>The bottom row of the t-table (page 21) shows that this z-value is smaller than 
</p>
<p>2.326, and this means the corresponding p-value of &lt; 0.02. The two drugs, thus, 
</p>
<p>produce significantly different results at p &lt; 0.02.</p>
<p/>
</div>
<div class="page"><p/>
<p>41T.J. Cleophas and A.H. Zwinderman, Statistical Analysis of Clinical Data on a Pocket 
</p>
<p>Calculator: Statistics on a Pocket Calculator, DOI 10.1007/978-94-007-1211-9_15,  
</p>
<p>&copy; Springer Science+Business Media B.V. 2011
</p>
<p>The t-test can be used to test the hypothesis that two group means are not different 
</p>
<p>(Chap. 3). When the experimental design involves multiple groups, and, thus, mul-
</p>
<p>tiple tests, we increase our chance of finding a difference. This is, simply, due to 
</p>
<p>the play of chance rather than a real effect. Multiple testing without any adjustment 
</p>
<p>for this increased chance is called data dredging, and is the source of multiple type 
</p>
<p>I errors (chances of finding a difference where there is none). The Bonferroni t-test 
</p>
<p>(and many other methods) are appropriate for the purpose of adjusting the increased 
</p>
<p>risk of type I errors.
</p>
<p>Bonferroni t-Test
</p>
<p>The underneath example studies three groups of patients treated with different 
</p>
<p>hemoglobin improving compounds. The mean increases of hemoglobin are given.
</p>
<p>Sample  
</p>
<p>size
</p>
<p>Mean hemoglobin
</p>
<p>(mmol / l)
</p>
<p>Standard deviation
</p>
<p>(mmol / l)
</p>
<p>Group 1 16  8.725 0.8445
</p>
<p>Group 2 10 10.6300 1.2841
</p>
<p>Group 3 15 12.3000 0.9419
</p>
<p>An overall analysis of variance test produced a p-value of &lt; 0.01. The conclusion 
</p>
<p>is that we have a significant difference in the data, but we will need additional testing 
</p>
<p>to find out where exactly the difference is, between group 1 and 2, between group 1 
</p>
<p>and 3, or between group 2 and 3. The easiest approach is to calculate the t&ndash;test for 
</p>
<p>each comparison. It produces a highly significant difference at p &lt; 0.01 between 
</p>
<p>group 1 versus 3 with no significant differences between the other comparisons. This 
</p>
<p>highly significant result is, however, unadjusted for multiple comparisons. If one 
</p>
<p>analyzes a set of data with three t-tests, each using a 5% critical value for concluding 
</p>
<p>that there is a significant difference, then there is about 3 &times; 5 = 15% chance of finding it. 
</p>
<p>This mechanism is called the Bonferroni inequality.
</p>
<p>Chapter 15
</p>
<p>Bonferroni t-Test</p>
<p/>
</div>
<div class="page"><p/>
<p>42 15 Bonferroni t-Test
</p>
<p>Bonferroni recommended a solution for the inequality, and proposed to follow 
</p>
<p>in case of three t-tests to use a smaller critical level for concluding that there is a 
</p>
<p>significant difference:
</p>
<p> With 1 t-test: critical level 5%=
</p>
<p>With 3 t-tests: critical level 5 / 3 1.6%.= =
</p>
<p>The above equations lead rapidly to very small critical values, otherwise called 
</p>
<p>p-values, and is, therefore, considered to be over-conservative. A somewhat less 
</p>
<p>conservative version of the above equation was also developed by Bonferroni., and 
</p>
<p>it is called the Bonferroni t-test.
</p>
<p>In case of three comparisons the rejection p-value will be 
2
</p>
<p>0.05 0.0166.
3(3 1)
</p>
<p>&acute; =
-
</p>
<p>In the given example a p-value of 0.0166 is still larger than 0.01, and, so, the 
</p>
<p>difference observed remained statistically significant, but using a cut-off p-value of 
</p>
<p>0.0166, instead of 0.05, means that the difference is not highly significant 
</p>
<p>anymore.</p>
<p/>
</div>
<div class="page"><p/>
<p>43T.J. Cleophas and A.H. Zwinderman, Statistical Analysis of Clinical Data on a Pocket
</p>
<p>Calculator: Statistics on a Pocket Calculator, DOI 10.1007/978-94-007-1211-9_16,
</p>
<p>&copy; Springer Science+Business Media B.V. 2011
</p>
<p>In some clinical studies, the spread of the data may be more relevant than the average 
</p>
<p>of the data. E.g., when we assess how a drug reaches various organs, variability of 
</p>
<p>drug concentrations is important, as in some cases too little and in other cases dan-
</p>
<p>gerously high levels get through. Also, variabilities in drug response may be impor-
</p>
<p>tant. For example, the spread of glucose levels of a slow-release-insulin is 
</p>
<p>important.
</p>
<p>One Sample Variability Analysis
</p>
<p>For testing whether the standard deviation (or variance) of a sample is significantly 
</p>
<p>different from the standard deviation (or variance) to be expected the chi-square test 
</p>
<p>with multiple degrees of freedom is adequate. The test statistic, the chi-square-
</p>
<p>value (= c2&ndash;value) is calculated according to
</p>
<p>χ
σ
-
</p>
<p>= -
2
</p>
<p>2
</p>
<p>2
</p>
<p>(n 1)s
for n 1 degrees of freedom
</p>
<p>(n = sample size, s = standard deviation, s2 = variance sample, s = expected standard 
deviation, s2 = expected variance).
</p>
<p>For example, the aminoglycoside compound gentamicin has a small therapeutic 
</p>
<p>index. The standard deviation of 50 measurements is used as a criterion for vari-
</p>
<p>ability. Adequate variability is accepted if the standard deviation is less than 7 mg/l. 
In our sample a standard deviation of 9 mg/l is observed.
</p>
<p>The test procedure is given.
</p>
<p>2 2 250 1 9 / 7 81= ( - ) =χ
</p>
<p>The chi-square table (page 32) shows that, for 50 &minus; 1 = 49 degrees of freedom, we 
</p>
<p>will find a p-value &lt; 0.01. This sample&rsquo;s standard deviation is significantly larger 
</p>
<p>than that required. This means that the variability in plasma gentamicin concentra-
</p>
<p>tions is larger than acceptable.
</p>
<p>Chapter 16
</p>
<p>Variability Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>44 16 Variability Analysis
</p>
<p>Two Sample Variability Test
</p>
<p>F-tests can be applied to test if the variabilities of two samples are significantly 
</p>
<p>different from one another. The division sum of the samples&rsquo; variances (larger 
</p>
<p>variance/smaller variance) is used for the analysis. For example, two formulas of 
</p>
<p>gentamicin produce the following standard deviations of plasma concentrations.
</p>
<p>Patients (n) Standard deviation (SD) (mg/l)
Formula-A 10 3.0
</p>
<p>Formula-B 15 2.0
</p>
<p>2 2
</p>
<p>A B
F-value SD / SD=
</p>
<p> =
2 23.0 / 2.0
</p>
<p> = =9 / 4 2.25
</p>
<p>with degrees of freedom (dfs) for
</p>
<p>formula-A of 10 1 9- =
</p>
<p>formula-B of 15 1 14.- =
</p>
<p>The F-table on the next page shows that an F-value of at least 3.01 is required 
</p>
<p>not to reject the null - hypothesis. Our F-value is 2.25 and, so, the p-value is &gt; 0.05. 
</p>
<p>No significant difference between the two formulas can be demonstrated. This 
</p>
<p>F-test is given on the next page.</p>
<p/>
</div>
<div class="page"><p/>
<p>45Two Sample Variability Test
F
</p>
<p>-T
ab
</p>
<p>le
</p>
<p>d
f 
</p>
<p>o
f 
</p>
<p> 
</p>
<p>d
en
</p>
<p>o
m
</p>
<p>in
at
</p>
<p>o
r
</p>
<p>2
-t
</p>
<p>ai
le
</p>
<p>d
  
</p>
<p>P
-v
</p>
<p>al
u
e
</p>
<p>1
-t
</p>
<p>ai
le
</p>
<p>d
  
</p>
<p>P
-v
</p>
<p>al
u
e
</p>
<p>D
eg
</p>
<p>re
es
</p>
<p> o
f 
</p>
<p>fr
ee
</p>
<p>d
o
m
</p>
<p> (
d
f)
</p>
<p> o
f 
</p>
<p>th
e 
</p>
<p>n
u
m
</p>
<p>er
at
</p>
<p>o
r
</p>
<p>1
2
</p>
<p>3
4
</p>
<p>5
6
</p>
<p>7
8
</p>
<p>9
1
0
</p>
<p>1
5
</p>
<p>2
5
</p>
<p>5
0
0
</p>
<p>1
0
</p>
<p>.0
5
</p>
<p>0
.0
</p>
<p>2
5
</p>
<p>6
4
7
.8
</p>
<p>7
9
9
.5
</p>
<p>8
6
4
.2
</p>
<p>8
9
9
.6
</p>
<p>9
2
1
.8
</p>
<p>9
3
7
.1
</p>
<p>9
4
8
.2
</p>
<p>9
5
6
.6
</p>
<p>9
6
3
.3
</p>
<p>9
6
8
.6
</p>
<p>9
8
4
.9
</p>
<p>9
9
8
.1
</p>
<p>1
0
1
7
.0
</p>
<p>1
0
.1
</p>
<p>0
0
.2
</p>
<p>0
5
</p>
<p>1
6
1
.4
</p>
<p>1
9
9
.5
</p>
<p>2
1
5
.7
</p>
<p>2
2
4
.6
</p>
<p>2
3
0
.2
</p>
<p>2
3
4
.0
</p>
<p>2
3
6
.8
</p>
<p>2
3
8
.9
</p>
<p>2
4
0
.5
</p>
<p>2
4
1
.9
</p>
<p>2
4
5
.9
</p>
<p>2
4
9
.3
</p>
<p>2
5
4
.1
</p>
<p>2
0
.0
</p>
<p>5
0
.0
</p>
<p>2
5
</p>
<p>3
8
.5
</p>
<p>1
3
9
.0
</p>
<p>0
3
9
.1
</p>
<p>7
3
9
.2
</p>
<p>5
3
9
.3
</p>
<p>0
3
9
.3
</p>
<p>3
3
9
.3
</p>
<p>6
3
9
.3
</p>
<p>7
3
9
.3
</p>
<p>9
3
9
.4
</p>
<p>0
3
9
.4
</p>
<p>3
3
9
.4
</p>
<p>6
3
9
.5
</p>
<p>0
</p>
<p>2
0
.1
</p>
<p>0
0
.0
</p>
<p>5
1
8
.5
</p>
<p>1
1
9
.0
</p>
<p>0
1
9
.1
</p>
<p>6
1
9
.2
</p>
<p>5
1
9
.1
</p>
<p>3
1
9
.3
</p>
<p>3
1
9
.3
</p>
<p>5
1
9
.3
</p>
<p>7
1
9
.3
</p>
<p>8
1
9
.4
</p>
<p>0
1
9
.4
</p>
<p>3
1
9
.4
</p>
<p>6
1
9
.4
</p>
<p>9
</p>
<p>3
0
.0
</p>
<p>5
0
.0
</p>
<p>2
5
</p>
<p>1
7
.4
</p>
<p>4
1
6
.0
</p>
<p>4
1
5
.4
</p>
<p>4
1
5
.1
</p>
<p>0
1
4
.8
</p>
<p>8
1
4
.7
</p>
<p>3
1
4
.6
</p>
<p>2
1
4
.5
</p>
<p>4
1
4
.4
</p>
<p>7
1
4
.4
</p>
<p>2
1
4
.2
</p>
<p>5
1
4
.1
</p>
<p>2
1
3
.9
</p>
<p>9
</p>
<p>3
0
.1
</p>
<p>0
0
.0
</p>
<p>5
1
0
.1
</p>
<p>3
9
.5
</p>
<p>5
9
.2
</p>
<p>8
9
.1
</p>
<p>2
9
.0
</p>
<p>1
8
.9
</p>
<p>4
8
.8
</p>
<p>9
8
.8
</p>
<p>5
8
.8
</p>
<p>1
8
.7
</p>
<p>9
8
.7
</p>
<p>0
8
.6
</p>
<p>3
8
.5
</p>
<p>3
</p>
<p>4
0
.0
</p>
<p>5
0
.0
</p>
<p>2
5
</p>
<p>1
2
.2
</p>
<p>2
1
0
.6
</p>
<p>5
9
.9
</p>
<p>8
9
.6
</p>
<p>0
9
.3
</p>
<p>6
9
.2
</p>
<p>0
9
.0
</p>
<p>7
8
.9
</p>
<p>8
8
.9
</p>
<p>0
8
.8
</p>
<p>4
8
.6
</p>
<p>6
8
.5
</p>
<p>0
8
.2
</p>
<p>7
</p>
<p>4
0
.1
</p>
<p>0
0
.0
</p>
<p>5
7
.7
</p>
<p>1
6
.9
</p>
<p>4
6
.5
</p>
<p>9
6
.3
</p>
<p>9
6
.2
</p>
<p>6
6
.1
</p>
<p>6
6
.0
</p>
<p>9
6
.0
</p>
<p>4
6
.0
</p>
<p>0
5
.9
</p>
<p>6
5
.8
</p>
<p>6
5
.7
</p>
<p>7
5
.6
</p>
<p>4
</p>
<p>5
0
</p>
<p>.0
5
</p>
<p>0
.0
</p>
<p>2
5
</p>
<p>1
0
.0
</p>
<p>1
8
.4
</p>
<p>3
7
.7
</p>
<p>6
7
.3
</p>
<p>9
7
.1
</p>
<p>5
6
.9
</p>
<p>8
6
.8
</p>
<p>5
6
.7
</p>
<p>6
6
.6
</p>
<p>8
6
.6
</p>
<p>2
6
.4
</p>
<p>3
6
.2
</p>
<p>7
6
.0
</p>
<p>3
</p>
<p>5
0
.1
</p>
<p>0
0
.0
</p>
<p>5
6
.6
</p>
<p>1
5
.7
</p>
<p>9
5
.4
</p>
<p>1
5
.1
</p>
<p>9
5
.0
</p>
<p>5
4
.9
</p>
<p>5
6
.8
</p>
<p>8
4
.8
</p>
<p>2
4
.7
</p>
<p>7
4
.7
</p>
<p>4
4
.6
</p>
<p>2
4
.5
</p>
<p>2
4
.3
</p>
<p>7
</p>
<p>6
0
.0
</p>
<p>5
0
.0
</p>
<p>2
5
</p>
<p>8
.8
</p>
<p>1
7
.2
</p>
<p>6
6
.6
</p>
<p>0
6
.2
</p>
<p>3
5
.9
</p>
<p>9
5
.8
</p>
<p>2
5
.7
</p>
<p>0
5
.6
</p>
<p>0
5
.5
</p>
<p>2
5
.4
</p>
<p>6
5
.2
</p>
<p>7
5
.1
</p>
<p>1
4
.8
</p>
<p>6
</p>
<p>6
0
.1
</p>
<p>0
0
.0
</p>
<p>5
5
.9
</p>
<p>9
5
.1
</p>
<p>4
4
.7
</p>
<p>6
4
.5
</p>
<p>3
4
.3
</p>
<p>9
4
.2
</p>
<p>8
4
.2
</p>
<p>1
4
.1
</p>
<p>5
4
.1
</p>
<p>0
4
.0
</p>
<p>6
3
.9
</p>
<p>4
3
.8
</p>
<p>3
3
.6
</p>
<p>8
</p>
<p>7
0
.0
</p>
<p>5
0
.0
</p>
<p>2
5
</p>
<p>8
.0
</p>
<p>7
6
.5
</p>
<p>4
5
.8
</p>
<p>9
5
.5
</p>
<p>2
5
.2
</p>
<p>9
5
.1
</p>
<p>2
4
.9
</p>
<p>9
4
.9
</p>
<p>0
4
.8
</p>
<p>2
4
.7
</p>
<p>6
4
.5
</p>
<p>7
4
.4
</p>
<p>0
4
.1
</p>
<p>6
</p>
<p>7
0
</p>
<p>.1
0
</p>
<p>0
.0
</p>
<p>5
5
.5
</p>
<p>9
4
.7
</p>
<p>4
4
.3
</p>
<p>5
4
.1
</p>
<p>2
3
.9
</p>
<p>7
3
.8
</p>
<p>7
3
.7
</p>
<p>9
3
.7
</p>
<p>3
3
.6
</p>
<p>8
3
.6
</p>
<p>4
3
.5
</p>
<p>1
3
.4
</p>
<p>0
3
.2
</p>
<p>4
</p>
<p>8
0
.0
</p>
<p>5
0
.0
</p>
<p>2
5
</p>
<p>7
.5
</p>
<p>7
6
.0
</p>
<p>6
5
.4
</p>
<p>2
5
.0
</p>
<p>5
4
.8
</p>
<p>2
4
.6
</p>
<p>5
4
.5
</p>
<p>3
4
.4
</p>
<p>3
4
.3
</p>
<p>6
4
.3
</p>
<p>0
4
.1
</p>
<p>0
3
.9
</p>
<p>4
3
.6
</p>
<p>8
</p>
<p>8
0
.1
</p>
<p>0
0
.0
</p>
<p>5
5
.3
</p>
<p>2
4
.4
</p>
<p>6
4
.0
</p>
<p>7
3
.8
</p>
<p>4
3
.6
</p>
<p>9
3
.5
</p>
<p>8
3
.5
</p>
<p>0
3
.4
</p>
<p>4
3
.3
</p>
<p>9
3
.3
</p>
<p>5
3
.2
</p>
<p>2
3
.1
</p>
<p>1
2
.9
</p>
<p>4
</p>
<p>9
0
.0
</p>
<p>5
0
.0
</p>
<p>2
5
</p>
<p>7
.2
</p>
<p>1
5
.7
</p>
<p>1
5
.0
</p>
<p>8
4
.7
</p>
<p>2
4
.4
</p>
<p>8
4
.3
</p>
<p>2
4
.2
</p>
<p>0
4
.1
</p>
<p>0
4
.0
</p>
<p>3
3
.9
</p>
<p>6
3
.7
</p>
<p>7
3
.6
</p>
<p>0
3
.3
</p>
<p>5
</p>
<p>9
0
.1
</p>
<p>0
0
.0
</p>
<p>5
5
.1
</p>
<p>2
4
.2
</p>
<p>6
3
.8
</p>
<p>6
3
.6
</p>
<p>3
3
.4
</p>
<p>8
3
.3
</p>
<p>7
3
.2
</p>
<p>9
3
.2
</p>
<p>3
3
.1
</p>
<p>8
3
.1
</p>
<p>4
3
.0
</p>
<p>1
2
.8
</p>
<p>9
2
.7
</p>
<p>2
</p>
<p>1
0
</p>
<p>0
.0
</p>
<p>5
0
.0
</p>
<p>2
5
</p>
<p>6
.9
</p>
<p>4
5
.4
</p>
<p>6
4
.8
</p>
<p>3
4
.4
</p>
<p>7
4
.2
</p>
<p>4
4
.0
</p>
<p>7
3
.9
</p>
<p>5
3
.8
</p>
<p>5
3
.7
</p>
<p>8
3
.7
</p>
<p>2
3
.5
</p>
<p>2
3
.3
</p>
<p>5
3
.0
</p>
<p>9
</p>
<p>1
0
</p>
<p>0
.1
</p>
<p>0
0
.0
</p>
<p>5
4
.9
</p>
<p>6
4
.1
</p>
<p>0
3
.7
</p>
<p>1
3
.4
</p>
<p>8
3
.3
</p>
<p>3
3
.2
</p>
<p>2
3
.1
</p>
<p>4
3
.0
</p>
<p>7
3
.0
</p>
<p>2
2
.9
</p>
<p>8
2
.8
</p>
<p>5
2
.7
</p>
<p>3
2
.5
</p>
<p>5
</p>
<p>1
5
</p>
<p>0
.0
</p>
<p>5
0
.0
</p>
<p>2
5
</p>
<p>6
.2
</p>
<p>0
4
.7
</p>
<p>7
4
.1
</p>
<p>5
3
.8
</p>
<p>0
3
.5
</p>
<p>8
3
.4
</p>
<p>1
3
.2
</p>
<p>9
3
.2
</p>
<p>0
3
.1
</p>
<p>2
3
.0
</p>
<p>6
2
.8
</p>
<p>6
2
.6
</p>
<p>9
2
.4
</p>
<p>1
</p>
<p>1
5
</p>
<p>0
.1
</p>
<p>0
0
.0
</p>
<p>5
4
.5
</p>
<p>4
3
.6
</p>
<p>8
3
.2
</p>
<p>9
3
.0
</p>
<p>6
2
.9
</p>
<p>0
2
.7
</p>
<p>9
2
.7
</p>
<p>1
2
.6
</p>
<p>4
2
.5
</p>
<p>9
2
.5
</p>
<p>4
2
.4
</p>
<p>0
2
.2
</p>
<p>8
2
.0
</p>
<p>8
</p>
<p>2
0
</p>
<p>0
.0
</p>
<p>5
0
.0
</p>
<p>2
5
</p>
<p>5
.8
</p>
<p>7
4
.4
</p>
<p>6
3
.8
</p>
<p>6
3
.5
</p>
<p>1
3
.2
</p>
<p>9
3
.1
</p>
<p>3
3
.0
</p>
<p>1
2
.9
</p>
<p>1
2
.8
</p>
<p>4
2
.7
</p>
<p>7
2
.5
</p>
<p>7
2
.4
</p>
<p>0
2
.1
</p>
<p>0
</p>
<p>2
0
</p>
<p>0
.1
</p>
<p>0
0
.0
</p>
<p>5
4
.3
</p>
<p>5
3
.4
</p>
<p>9
3
.1
</p>
<p>0
2
.8
</p>
<p>7
2
.7
</p>
<p>1
2
.6
</p>
<p>0
2
.5
</p>
<p>1
2
.4
</p>
<p>5
2
.3
</p>
<p>9
2
.3
</p>
<p>5
2
.2
</p>
<p>0
2
.0
</p>
<p>7
1
.8
</p>
<p>6
</p>
<p>3
0
</p>
<p>0
.0
</p>
<p>5
0
.0
</p>
<p>2
5
</p>
<p>5
.5
</p>
<p>7
4
.1
</p>
<p>8
3
.5
</p>
<p>9
3
.2
</p>
<p>5
3
.0
</p>
<p>3
2
.8
</p>
<p>7
2
.7
</p>
<p>5
2
.6
</p>
<p>5
2
.5
</p>
<p>7
2
.5
</p>
<p>1
2
.3
</p>
<p>1
2
.1
</p>
<p>2
1
.8
</p>
<p>1
</p>
<p>3
0
</p>
<p>0
.1
</p>
<p>0
0
.0
</p>
<p>5
4
.1
</p>
<p>7
3
.3
</p>
<p>2
2
.9
</p>
<p>2
2
.6
</p>
<p>9
2
.5
</p>
<p>3
2
.4
</p>
<p>2
2
.3
</p>
<p>3
2
.2
</p>
<p>7
2
.2
</p>
<p>1
2
.1
</p>
<p>6
2
.0
</p>
<p>1
1
.8
</p>
<p>8
1
.6
</p>
<p>4
</p>
<p>5
0
</p>
<p>0
.0
</p>
<p>5
0
.0
</p>
<p>2
5
</p>
<p>5
.3
</p>
<p>4
3
.9
</p>
<p>7
3
.3
</p>
<p>9
3
.0
</p>
<p>5
2
.8
</p>
<p>3
2
.6
</p>
<p>7
2
.5
</p>
<p>5
2
.4
</p>
<p>6
2
.3
</p>
<p>8
2
.3
</p>
<p>2
2
.1
</p>
<p>1
1
.9
</p>
<p>2
1
.5
</p>
<p>7
</p>
<p>5
0
</p>
<p>0
.1
</p>
<p>0
0
.0
</p>
<p>5
4
.0
</p>
<p>3
3
.1
</p>
<p>8
2
.7
</p>
<p>9
2
.5
</p>
<p>6
2
.4
</p>
<p>0
2
.2
</p>
<p>9
2
.2
</p>
<p>0
2
.1
</p>
<p>3
2
.0
</p>
<p>7
2
.0
</p>
<p>3
1
.8
</p>
<p>7
1
.7
</p>
<p>3
1
.4
</p>
<p>6
</p>
<p>1
0
0
</p>
<p>0
.0
</p>
<p>5
0
.0
</p>
<p>2
5
</p>
<p>5
.1
</p>
<p>8
3
.8
</p>
<p>3
3
.2
</p>
<p>5
2
.9
</p>
<p>2
2
.7
</p>
<p>0
2
.5
</p>
<p>4
2
.4
</p>
<p>2
2
.3
</p>
<p>2
2
.2
</p>
<p>4
2
.1
</p>
<p>8
1
.9
</p>
<p>7
1
.7
</p>
<p>7
1
.3
</p>
<p>8
</p>
<p>1
0
0
</p>
<p>0
.1
</p>
<p>0
0
.0
</p>
<p>5
3
.9
</p>
<p>4
3
.0
</p>
<p>9
2
.7
</p>
<p>0
2
.4
</p>
<p>6
2
.3
</p>
<p>1
2
.1
</p>
<p>9
2
.1
</p>
<p>0
2
.0
</p>
<p>3
1
.9
</p>
<p>7
1
.9
</p>
<p>3
1
.7
</p>
<p>7
1
.6
</p>
<p>2
1
.3
</p>
<p>1
</p>
<p>1
0
0
0
</p>
<p>0
.0
</p>
<p>5
0
.0
</p>
<p>2
5
</p>
<p>5
.0
</p>
<p>4
3
.7
</p>
<p>0
3
.1
</p>
<p>3
2
.8
</p>
<p>0
2
.5
</p>
<p>8
2
.4
</p>
<p>2
2
.3
</p>
<p>0
2
.2
</p>
<p>0
2
.1
</p>
<p>3
2
.0
</p>
<p>6
1
.8
</p>
<p>5
1
.6
</p>
<p>4
1
.1
</p>
<p>6
</p>
<p>1
0
0
0
</p>
<p>0
.1
</p>
<p>0
0
.0
</p>
<p>5
3
.8
</p>
<p>5
3
.0
</p>
<p>0
2
.6
</p>
<p>1
2
.3
</p>
<p>8
2
.2
</p>
<p>2
2
.1
</p>
<p>1
2
.0
</p>
<p>2
1
.9
</p>
<p>5
1
.8
</p>
<p>9
1
.8
</p>
<p>4
1
.6
</p>
<p>8
1
.5
</p>
<p>2
1
.1
</p>
<p>3</p>
<p/>
</div>
<div class="page"><p/>
<p>47T.J. Cleophas and A.H. Zwinderman, Statistical Analysis of Clinical Data on a Pocket 
</p>
<p>Calculator: Statistics on a Pocket Calculator, DOI 10.1007/978-94-007-1211-9_17,  
</p>
<p>&copy; Springer Science+Business Media B.V. 2011
</p>
<p>treatment
</p>
<p>modality
</p>
<p>males
</p>
<p>10
</p>
<p>10
</p>
<p>0 1
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>20
</p>
<p>20 females
</p>
<p>treatment efficacy (units)
</p>
<p>In the above study the treatment effects are better in the males than they are in the 
</p>
<p>females. This difference in efficacy does not influence the overall assessment as 
</p>
<p>long as the numbers of males and females in the treatment comparison are equally 
</p>
<p>distributed. If, however, many females received the new treatment, and many males 
</p>
<p>received the control treatment, a peculiar effect on the overall data analysis is 
</p>
<p>observed as demonstrated by the difference in magnitudes of the circles in the 
</p>
<p>above figure: the overall regression line will become close to horizontal, giving rise 
</p>
<p>to the erroneous conclusion that no difference in efficacy exists between treatment 
</p>
<p>and control. This phenomenon is called confounding, and may have a profound 
</p>
<p>effect on the outcome of the study.
</p>
<p>Confounding can be assessed by the method of subclassification. In the above 
</p>
<p>example an overall mean difference between the two treatment modalities is 
</p>
<p>calculated.
</p>
<p>For treatment zero
</p>
<p>Mean effect standard error SE 1.5 units 0.5 units&plusmn; ( ) = &plusmn;
</p>
<p>Chapter 17
</p>
<p>Confounding</p>
<p/>
</div>
<div class="page"><p/>
<p>48 17 Confounding
</p>
<p>For treatment one
</p>
<p>&plusmn; = &plusmn;Mean effect SE 2.5 units 0.6 units
</p>
<p>The mean difference of the two treatments
</p>
<p> 
1.0 units pooled standard error= &plusmn;
</p>
<p> 
2 21.0 (0.5 0.6 )= &plusmn; &Ouml; +
</p>
<p> 1.0 0.61= &plusmn;
</p>
<p> The t-value as calculated 1.0 / 0.61 1.639= =
</p>
<p>With 100 &minus; 2 (100 patients, 2 groups) = 98 degrees of freedom the p-value of this 
</p>
<p>difference is calculated to be
</p>
<p>  = p &gt; 0.10 (according to t-table page 21).
</p>
<p>In order to assess the possibility of confounding, a weighted mean has to be 
</p>
<p>calculated. The underneath equation is adequate for the purpose.
</p>
<p>2 2
</p>
<p>males females
</p>
<p>2 2
</p>
<p>males females
</p>
<p>Difference /  its SE Difference /  its SE
Weighted mean 
</p>
<p>1 /  SE 1 /  SE
</p>
<p>&plusmn;
=
</p>
<p>+
</p>
<p>For the males we find means of 2.0 and 3.0 units, for the females 1.0 and 2.0 
</p>
<p>units. The mean difference for the males and females separately are 1.0 and 1.0 as 
</p>
<p>expected from the above figure. However, the pooled standard errors are different, 
</p>
<p>for the males 0.4, and for the females 0.3 units.
</p>
<p>According to the above equation a weighted t-value is calculated
</p>
<p> 
</p>
<p>2 2
</p>
<p>2 2
</p>
<p>(1.0 / 0.4 1.0 / 0.3 )
</p>
<p>(1 /
Weigh
</p>
<p>0.4
ted m
</p>
<p>1 / 0
ean
</p>
<p>.3 )
=
</p>
<p>+
+
</p>
<p> 1.0=
</p>
<p> 
2 2 2Weighted SE 1 / (1 / 0.4 1 / 0.3 )+
</p>
<p> 0.576=
</p>
<p> 
Weighted SE 0.24=
</p>
<p> t-value 1.0 / 0.24 4.16= =
</p>
<p> 
p-value 0.001&lt;
</p>
<p>The weighted mean is equal to the unweighted mean. However, its SE is much 
</p>
<p>smaller. It means that after adjustment for confounding a very significant difference 
</p>
<p>is observed.
</p>
<p>Other methods for assessing confounding include multiple regression analysis and 
</p>
<p>propensity score assessments. Particularly, with more than a single confounder these 
</p>
<p>two methods are unavoidable, and they can not be carried out on a pocket calculator.</p>
<p/>
</div>
<div class="page"><p/>
<p>49T.J. Cleophas and A.H. Zwinderman, Statistical Analysis of Clinical Data on a Pocket 
</p>
<p>Calculator: Statistics on a Pocket Calculator, DOI 10.1007/978-94-007-1211-9_18,  
</p>
<p>&copy; Springer Science+Business Media B.V. 2011
</p>
<p>males
</p>
<p>90
</p>
<p>70
</p>
<p>50
</p>
<p>30
</p>
<p>10
</p>
<p>0 1
</p>
<p>females
</p>
<p>treatment modality
0 = control medicine
1 = new medicine
</p>
<p>The medical concept of interaction is synonymous to the terms heterogeneity and 
</p>
<p>synergism. Interaction must be distinguished from confounding. In a trial with 
</p>
<p>interaction effects the parallel groups have similar characteristics. However, there 
</p>
<p>are subsets of patients that have an unusually high or low response. The above 
</p>
<p>figure gives an example of a study in which males seem to respond better to the 
</p>
<p>treatment 1 than females. With confounding things are different. For whatever 
</p>
<p>reason the randomization has failed, the parallel groups have asymmetric charac-
</p>
<p>teristics. E.g., in a placebo-controlled trial of two parallel-groups asymmetry of 
</p>
<p>age may be a confounder. The control group is significantly older than the treat-
</p>
<p>ment group, and this can easily explain the treatment difference as demonstrated 
</p>
<p>in the previous chapter.
</p>
<p>Chapter 18
</p>
<p>Interaction</p>
<p/>
</div>
<div class="page"><p/>
<p>50 18 Interaction
</p>
<p>Example of Interaction
</p>
<p>A parallel-group study of verapamil versus metoprolol for the treatment of 
</p>
<p> paroxysmal atrial tachycardias. The numbers of episodes of paroxysmal atrial tachy-
</p>
<p>cardias per patient are the outcome variable.
</p>
<p>Verapamil Metoprolol
</p>
<p>Males 52 28
</p>
<p>48 35
</p>
<p>43 34
</p>
<p>50 32
</p>
<p>43 34
</p>
<p>44 27
</p>
<p>46 31
</p>
<p>46 27
</p>
<p>43 29
</p>
<p>49 25
</p>
<p>464 302 766
</p>
<p>Females 38 43
</p>
<p>42 34
</p>
<p>42 33
</p>
<p>35 42
</p>
<p>33 41
</p>
<p>38 37
</p>
<p>39 37
</p>
<p>34 40
</p>
<p>33 36
</p>
<p>34 35
</p>
<p>368 378 746
</p>
<p>832 680
</p>
<p>Overall metoprolol seems to perform better. However, this is only true only for 
</p>
<p>one subgroup (males).
</p>
<p>Males Females
</p>
<p>Mean
verapamil
</p>
<p> (SD) 46.4 (3.23866) 36.8 (3.489667)
</p>
<p>Mean
metoprolol
</p>
<p> (SD) 30.2 (3.48966) &minus; 37.8 (3.489667) &minus; 
</p>
<p>Difference means (SE) 16.2 (1.50554) &minus;1.0 (1.5606)
</p>
<p>Difference between males and females 17.2 (2.166)
</p>
<p>t - value 17.2 / 2.166 8...= =
</p>
<p> 
&lt;p 0.0001
</p>
<p>There is a significant difference between the males and females, and, thus, a 
</p>
<p>significant interaction between gender and treat-efficacy. Interaction can also be 
</p>
<p>assessed with analysis of variance and regression modeling. These two methods are 
</p>
<p>the methods of choice in case you expect more than a single interaction in your 
</p>
<p>data. They should be carried out on a computer.</p>
<p/>
</div>
<div class="page"><p/>
<p>51T.J. Cleophas and A.H. Zwinderman, Statistical Analysis of Clinical Data on a Pocket 
</p>
<p>Calculator: Statistics on a Pocket Calculator, DOI 10.1007/978-94-007-1211-9_19,  
</p>
<p>&copy; Springer Science+Business Media B.V. 2011
</p>
<p>The reliability, otherwise called reproducibility of diagnostic tests is an important 
</p>
<p>quality criterion. A diagnostic test is very unreliable, if it is not well reproducible.
</p>
<p>Example 1
</p>
<p>Test 1 Test 2 Difference (Difference)2
</p>
<p>Result
</p>
<p>1 11 &minus;10 100
</p>
<p>10 0 10 100
</p>
<p>2 11 &minus;9  81
</p>
<p>12 2 10 100
</p>
<p>11 1 10 100
</p>
<p>1 12 &minus;11 121
</p>
<p>Mean
</p>
<p>6.17 6.17 0 100.3
</p>
<p> 
Duplicate standard deviation duplicate standard deviation (SD)=
</p>
<p> 
</p>
<p>2(1/ 2 mean (difference) )= &Ouml; &acute;
</p>
<p> (1 / 2 100.3)= &Ouml; &acute;
</p>
<p> 7.08=
</p>
<p>The proportional duplicate standard deviation%
</p>
<p> 
</p>
<p>duplicate standard deviation
100%
</p>
<p>overall mean
= &acute;
</p>
<p> 
</p>
<p>7.08
100%
</p>
<p>6.17
= &acute;
</p>
<p> 115%=
</p>
<p>Chapter 19
</p>
<p>Duplicate Standard Deviation for Reliability 
Assessment of Continuous Data</p>
<p/>
</div>
<div class="page"><p/>
<p>52 19 Duplicate Standard Deviation for Reliability Assessment of Continuous Data
</p>
<p>An adequate reliability is obtained with a proportional duplicate standard deviation 
</p>
<p>of 10&ndash;20%. In the current example, although the mean difference between the two 
</p>
<p>tests equals zero, there is, thus, a very poor reproducibility.
</p>
<p>Example 2
</p>
<p>Question is this test well reproducible?
</p>
<p>Test 1 Test 2
</p>
<p>Result
</p>
<p>6.2 5.1
</p>
<p>7.0 7.8
</p>
<p>8.1 3.9
</p>
<p>7.5 5.5
</p>
<p>6.5 6.6
</p>
<p>Analysis:
</p>
<p>Test 1 Test 2 Difference Difference2
</p>
<p>Result
</p>
<p>6.2 5.1 1.1 1.21
</p>
<p>7.0 7.8 &minus;0.8 0.64
</p>
<p>8.1 3.9 4.2 17.64
</p>
<p>7.5 5.5 2.0 4.0
</p>
<p>6.5 6.6 &minus;0.1 0.01
</p>
<p>Mean
</p>
<p>7.06 5.78 4.7
</p>
<p>Grand mean 6.42
</p>
<p> 
1
</p>
<p>2Duplicate standard deviation 4.7= &Ouml; &acute;
</p>
<p> 1.553=
</p>
<p>Proportional duplicate standard deviation %
</p>
<p> 
</p>
<p>duplicate standard deviation
100%
</p>
<p>overall mean
= &acute;
</p>
<p> 
</p>
<p>1.533
100%
</p>
<p>6.42
= &acute;
</p>
<p> 24%=
</p>
<p>A good reproducibility is between 10% and 20%. In the above example repro-
</p>
<p>ducibility is, thus, almost good.</p>
<p/>
</div>
<div class="page"><p/>
<p>53T.J. Cleophas and A.H. Zwinderman, Statistical Analysis of Clinical Data on a Pocket 
</p>
<p>Calculator: Statistics on a Pocket Calculator, DOI 10.1007/978-94-007-1211-9_20,  
</p>
<p>&copy; Springer Science+Business Media B.V. 2011
</p>
<p>The reproducibility of continuous data can be estimated with duplicate standard 
</p>
<p>deviations (Chap. 19). With binary data Cohen&rsquo;s kappas are used for the purpose. 
</p>
<p>Reliability assessment of diagnostic procedures is an important part of the validity 
</p>
<p>assessment of scientific research.
</p>
<p>Example
</p>
<p>Positive (pos) or negative (neg) laboratory tests of 30 patients are assessed. All 
</p>
<p>pati&euml;nts are tested a second time in order to estimate the level of reproducibility 
</p>
<p>of the test.
</p>
<p>1st time
</p>
<p>pos neg
</p>
<p>2nd time pos 10  5 15
</p>
<p>neg  4 11 15
</p>
<p>14 16 30
</p>
<p>If the test is not reproducible at all, then we will find twice the same result in 
</p>
<p>50% of the patients, and a different result the second time in the other 50% of the 
</p>
<p>patients.
</p>
<p>Overall 30 tests have been carried out twice.
</p>
<p> 
</p>
<p>We observe 10 times 2 positive and
</p>
<p>11 times 2 negative.
</p>
<p>&acute;
&acute;
</p>
<p>And thus, twice the same is found in
</p>
<p>21 patients which is considerable more than in half of the cases,
</p>
<p>which should have been15 times.
</p>
<p>Chapter 20
</p>
<p>Kappas for Reliability Assessment  
of Binary Data</p>
<p/>
</div>
<div class="page"><p/>
<p>54 20 Kappas for Reliability Assessment of Binary Data
</p>
<p>Minimal indicates the number of duplicate observations if reproducibility were 
</p>
<p>zero, maximal indicates the number of duplicate observations if the reproducibility 
</p>
<p>were 100%.
</p>
<p> 
</p>
<p>observed minimal
Kappa
</p>
<p>maximal minimal
</p>
<p>-
=
</p>
<p>-
</p>
<p> 
</p>
<p>21 15
</p>
<p>30 15
</p>
<p>-
=
</p>
<p>-
</p>
<p> 0.4=
</p>
<p>A kappa-value of 0.0 means that reproducibility is very poor.
</p>
<p>A kappa of 1.0 would have meant excellent reproducibility.
</p>
<p>In our example we observed a kappa of 0.4, which means reproducibility is very 
</p>
<p>moderate.</p>
<p/>
</div>
<div class="page"><p/>
<p>55T.J. Cleophas and A.H. Zwinderman, Statistical Analysis of Clinical Data on a Pocket 
</p>
<p>Calculator: Statistics on a Pocket Calculator, DOI 10.1007/978-94-007-1211-9,  
</p>
<p>&copy; Springer Science+Business Media B.V. 2011
</p>
<p>Statistics is no bloodless algebra. It is a discipline at the interface of biology and 
</p>
<p>mathematics. Mathematics is used to answer biological questions. Biological pro-
</p>
<p>cesses are full of variations, and statistics gives no certainties, only chances. What 
</p>
<p>kind of chances: chances that your prior hypotheses are true or untrue. The human 
</p>
<p>brain hypothesizes all the time. And we currently believe that hypotheses must be 
</p>
<p>assessed with hard data.
</p>
<p>When it comes to statistical data analyses, clinicians and clinical investigators 
</p>
<p>soon get very nervous, and tend to leave their data to a statistician who runs the data 
</p>
<p>through SAS of SPSS or any other software program to see if there are significant 
</p>
<p>p-values. This practice is called data dredging and is the source of multiple type I 
</p>
<p>errors of finding a difference where there is none.
</p>
<p>The best defense against this practice is the use of simple tests. These tests, 
</p>
<p>generally, provide the best power for confirmative research, because this research 
</p>
<p>is based on sound arguments. Multiple variable tests are not always in place here, 
</p>
<p>as they tend to enhance the risk of power loss, data dredging, and type I errors 
</p>
<p>producing a host of irrelevant p-values. Also multiple variable tests, although inter-
</p>
<p>esting, are considered exploratory rather than confirmatory, in other words they, 
</p>
<p>generally, prove nothing, and have to be confirmed.
</p>
<p>The current book was written for various reasons:
</p>
<p> 1. To review the basic principles of statistical testing which tends to be increasingly 
</p>
<p>forgotten in the current computer era.
</p>
<p> 2. To serve as a primer for nervous investigators who would like to perform their 
</p>
<p>own data analyses but feel inexpert to do so.
</p>
<p> 3. To make investigators better understand what they are doing, when analyzing 
</p>
<p>clinical data.
</p>
<p> 4. To facilitate data analysis by use of a number of rapid pocket calculator 
</p>
<p>methods.
</p>
<p> 5. As a primer for those who wish to master more advanced statistical methods. 
</p>
<p>More advanced methods are reviewed by the same authors in the books &ldquo;SPSS 
</p>
<p>Final Remarks</p>
<p/>
</div>
<div class="page"><p/>
<p>56 Final Remarks
</p>
<p>for Starters&rdquo; 2010, &ldquo;Statistics Applied to Clinical Trials&rdquo; fourth edition, 2009, 
</p>
<p>&ldquo;Statistics Applied to Clinical Trials: Self-Assessment Book, 2002, all of them 
</p>
<p>edited by Springer, Dordrecht. These books closely fit and complement the 
</p>
<p>format and contents of the current book.
</p>
<p>The current book is very condensed, but this should be threshold lowering to 
</p>
<p>readers. As a consequence, however, the theoretical background of the methods 
</p>
<p>described are not sufficiently explained in the text. Extensive theoretical informa-
</p>
<p>tion is also given in the above mentioned books from the same authors.</p>
<p/>
</div>
<div class="page"><p/>
<p>57
</p>
<p>A
</p>
<p>Alpha, 20
</p>
<p>Analysis of variance, 41, 50
</p>
<p>Areas under the curve, 21
</p>
<p>B
</p>
<p>Beta, 20
</p>
<p>Bloodless algebra, 55
</p>
<p>Bonferroni inequality, 41
</p>
<p>Bonferroni t-test, 41&ndash;42
</p>
<p>Boundaries of equivalence, 17
</p>
<p>C
</p>
<p>Chi-square table, 31&ndash;33
</p>
<p>Chi-square test, 31&ndash;35, 37, 38, 43
</p>
<p>Chi-square test for cross-tabs, 31&ndash;34
</p>
<p>Cohen&rsquo;s kappa, 53
</p>
<p>Confidence intervals, 1, 15
</p>
<p>Confounding, 1, 47&ndash;49
</p>
<p>Cross-tabs, 29&ndash;35, 37, 39, 40
</p>
<p>D
</p>
<p>Data dredging, 41
</p>
<p>Degrees of freedom, 6&ndash;8, 20, 29, 31,  
</p>
<p>43&ndash;45, 48
</p>
<p>Dependent variables, v
</p>
<p>Diagnostic tests, 51
</p>
<p>Duplicate standard deviation, 51&ndash;52
</p>
<p>E
</p>
<p>Equivalence tests, 17
</p>
<p>F
</p>
<p>Frequency distribution, 8
</p>
<p>F-table (Fisher), 44, 45
</p>
<p>F-test (Fisher), 44
</p>
<p>G
</p>
<p>Gaussian distribution, 36
</p>
<p>I
</p>
<p>Independent variables, v
</p>
<p>Interaction, 49&ndash;50
</p>
<p>Irrelevant p-values, 42
</p>
<p>K
</p>
<p>Kappa, 53&ndash;54
</p>
<p>Kappa-values, 54
</p>
<p>L
</p>
<p>LnOR, 35, 36, 40
</p>
<p>Ln values, 40
</p>
<p>Log likelihood ratio, 37&ndash;38
</p>
<p>Log likelihood ratio tests, 37&ndash;38
</p>
<p>M
</p>
<p>Mann Whitney tables, 11
</p>
<p>Mann Whitney test, 11&ndash;13
</p>
<p>Margin of inferiority, 27
</p>
<p>Matched groups, 7
</p>
<p>McNemar odds ratios, 40
</p>
<p>McNemar&rsquo;s test, 39&ndash;40
</p>
<p>Index</p>
<p/>
</div>
<div class="page"><p/>
<p>58 Index
</p>
<p>Means, 10, 41, 48
</p>
<p>Multiple regression analysis, 48
</p>
<p>Multiple testing, 1, 41
</p>
<p>Multiple variable tests, 55
</p>
<p>N
</p>
<p>Noninferiority testing, 27&ndash;28
</p>
<p>Non-parametric tests, 1, 9&ndash;13
</p>
<p>Normal distribution, 29
</p>
<p>O
</p>
<p>Odds ratios, 15, 35&ndash;36
</p>
<p>Odds ratio test for cross-tabs, 35&ndash;36
</p>
<p>One-sample t-test, 5&ndash;6
</p>
<p>P
</p>
<p>Paired t-test, 6
</p>
<p>Parallel groups, 27, 49
</p>
<p>Parallel-group study, 17, 50
</p>
<p>P-calculator for z-values, 36, 37
</p>
<p>Pocket calculator method, 31
</p>
<p>Pocket calculators, 1&ndash;3, 31, 34, 40, 48
</p>
<p>Pooled SE. See Pooled standard error
</p>
<p>Pooled standard deviation, 26
</p>
<p>Pooled standard error, 17, 29, 48
</p>
<p>Power, 19&ndash;21, 23&ndash;27
</p>
<p>Power equations, 19&ndash;21
</p>
<p>Power index, 23&ndash;27
</p>
<p>Prior hypothesis, 55
</p>
<p>Propensity scores, 48
</p>
<p>Proportions, 31&ndash;33
</p>
<p>Proportional duplicate standard  
</p>
<p>deviation, 51, 52
</p>
<p>P-values, 5&ndash;13, 21, 23, 24, 27&ndash;33, 36&ndash;44, 48
</p>
<p>R
</p>
<p>Rank numbers, 9&ndash;11
</p>
<p>Regression modeling, 50
</p>
<p>Reliability assessment, 51&ndash;54
</p>
<p>Reproducibility, 51&ndash;54
</p>
<p>S
</p>
<p>Sample size, 1, 6, 8, 15, 20, 23&ndash;27, 43
</p>
<p>Sample size and binary data, 25&ndash;26
</p>
<p>Sample size and continuous data, 23&ndash;25
</p>
<p>SAS statistical software, 55
</p>
<p>SD. See Standard deviation (SD)
</p>
<p>SE. See Standard error (SE)
</p>
<p>SEM. See Standard error of the mean (SEM)
</p>
<p>SEM-unit, 23, 28
</p>
<p>Sensitivity of tests for cross-tabs, 37
</p>
<p>SE-unit, 8, 20, 28
</p>
<p>SPSS for Starters, 2
</p>
<p>SPSS statistical software, v
</p>
<p>Standard deviation (SD), 3&ndash;7, 15, 19, 23&ndash;27, 
</p>
<p>44, 51
</p>
<p>Standard error (SE), 5&ndash;7, 15, 19, 20, 28&ndash;30, 
</p>
<p>35, 36, 40, 47, 48
</p>
<p>Standard error of the mean (SEM), 17, 23, 24, 
</p>
<p>28, 29, 40
</p>
<p>Subclassification, 47
</p>
<p>T
</p>
<p>T-distribution, 20, 21, 29
</p>
<p>T-table, 6&ndash;8, 19&ndash;21, 29, 30, 36, 40, 48
</p>
<p>T-tests, 5&ndash;8, 41&ndash;42
</p>
<p>Two-sided p-values, 21
</p>
<p>Type I error, 20, 41
</p>
<p>Type II error, 20
</p>
<p>U
</p>
<p>Unpaired t-test, 7&ndash;8
</p>
<p>V
</p>
<p>Variability analysis, 1, 43&ndash;45
</p>
<p>Variability test one sample, 43
</p>
<p>Variability test two samples, 44&ndash;45
</p>
<p>W
</p>
<p>Weighted mean, 48
</p>
<p>Weighted standard error, 48
</p>
<p>Wilcoxon table, 10
</p>
<p>Wilcoxon test, 9&ndash;10
</p>
<p>Z
</p>
<p>Z-distribution, 
</p>
<p>Z-test for cross-tabs, 29&ndash;30
</p>
<p>Z-values, 29, 36, 37</p>
<p/>
</div>
<ul>	<li>Statistical Analysisof Clinical Data on a Pocket Calculator</li>
<ul>	<li>Preface</li>
	<li>Contents</li>
	<li>Chapter 1: Introduction</li>
	<li>Chapter 2: Standard Deviations</li>
	<li>Chapter 3: t-Tests</li>
<ul>	<li>1 Sample t-Test</li>
	<li>Paired t-Test</li>
	<li>Unpaired t-Test</li>
</ul>
	<li>Chapter 4: Non-Parametric Tests</li>
<ul>	<li>Wilcoxon Test</li>
	<li>Mann-Whitney Test</li>
</ul>
	<li>Chapter 5: Confidence Intervals</li>
	<li>Chapter 6: Equivalence Tests</li>
	<li>Chapter 7: Power Equations</li>
	<li>Chapter 8: Sample Size</li>
<ul>	<li>Continuous Data, Power 50%</li>
	<li>Continuous Data, Power 80%</li>
	<li>Continuous Data, Power 80%, 2 Groups</li>
	<li>Binary Data, Power 80%</li>
	<li>Binary Data, Power 80%, 2 Groups</li>
</ul>
	<li>Chapter 9: Noninferiority Testing</li>
<ul>	<li>Step 1: Determination of the Margin of Noninferiority, the Required Sample, and the Expected p-Value and Power of the Study Result</li>
	<li>Step 2: Testing the Significance of Difference Between the New and the Standard Treatment</li>
	<li>Step 3: Testing the Significance of Difference Between the New Treatment and a Placebo</li>
	<li>Conclusion</li>
</ul>
	<li>Chapter 10: Z-Test for Cross-Tabs</li>
	<li>Chapter 11: Chi-Square Tests for Cross-Tabs</li>
<ul>	<li>First Example Cross-Tab</li>
	<li>Chi-Square Table (c2-Table)</li>
	<li>Second Example Cross-Tab</li>
	<li>Example for Practicing 1</li>
	<li>Example for Practicing 2</li>
</ul>
	<li>Chapter 12: Odds Ratios</li>
	<li>Chapter 13: Log Likelihood Ratio Tests</li>
	<li>Chapter 14: McNemar&rsquo;s Tests</li>
	<li>Chapter 15: Bonferroni t-Test</li>
<ul>	<li>Bonferroni t-Test</li>
</ul>
	<li>Chapter 16: Variability Analysis</li>
<ul>	<li>One Sample Variability Analysis</li>
	<li>Two Sample Variability Test</li>
</ul>
	<li>Chapter 17: Confounding</li>
	<li>Chapter 18: Interaction</li>
	<li>Chapter 19: Duplicate Standard Deviation for Reliability Assessment of Continuous Data</li>
	<li>Chapter 20: Kappas for Reliability Assessment of Binary Data</li>
	<li>Final Remarks</li>
	<li>Index</li>
</ul>
</ul>
</body></html>