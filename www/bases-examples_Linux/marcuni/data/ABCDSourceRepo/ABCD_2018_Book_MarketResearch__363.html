<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Untitled</title>
</head>
<body><div class="page"><p/>
<p>Springer Texts in Business and Economics
</p>
<p>Market
Research
The Process, Data,
and Methods Using Stata
</p>
<p>Erik Mooi 
Marko Sarstedt 
Irma Mooi-Reci</p>
<p/>
</div>
<div class="page"><p/>
<p>Springer Texts in Business and Economics</p>
<p/>
</div>
<div class="page"><p/>
<p>More information about this series at http://www.springer.com/series/10099</p>
<p/>
<div class="annotation"><a href="http://www.springer.com/series/10099">http://www.springer.com/series/10099</a></div>
</div>
<div class="page"><p/>
<p>Erik Mooi &bull; Marko Sarstedt &bull; Irma Mooi-Reci
</p>
<p>Market Research
</p>
<p>The Process, Data, and Methods
Using Stata</p>
<p/>
</div>
<div class="page"><p/>
<p>Erik Mooi
Department of Management
and Marketing
University of Melbourne
Parkville, Victoria, Australia
</p>
<p>Marko Sarstedt
Chair of Marketing
Otto-von-Guericke-University
Magdeburg, Sachsen-Anhalt, Germany
</p>
<p>Irma Mooi-Reci
School of Social and Political Sciences
University of Melbourne
Parkville, Victoria, Australia
</p>
<p>ISSN 2192-4333 ISSN 2192-4341 (electronic)
Springer Texts in Business and Economics
ISBN 978-981-10-5217-0 ISBN 978-981-10-5218-7 (eBook)
DOI 10.1007/978-981-10-5218-7
</p>
<p>Library of Congress Control Number: 2017946016
</p>
<p># Springer Nature Singapore Pte Ltd. 2018
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of
the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations,
recitation, broadcasting, reproduction on microfilms or in any other physical way, and transmission
or information storage and retrieval, electronic adaptation, computer software, or by similar or
dissimilar methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a specific statement, that such names are exempt
from the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this
book are believed to be true and accurate at the date of publication. Neither the publisher nor the
authors or the editors give a warranty, express or implied, with respect to the material contained
herein or for any errors or omissions that may have been made. The publisher remains neutral with
regard to jurisdictional claims in published maps and institutional affiliations.
</p>
<p>Printed on acid-free paper
</p>
<p>This Springer imprint is published by Springer Nature
The registered company is Springer Nature Singapore Pte Ltd.
The registered company address is: 152 Beach Road, #21-01/04 Gateway East, Singapore 189721,
Singapore</p>
<p/>
</div>
<div class="page"><p/>
<p>To Irma
&ndash; Erik Mooi
</p>
<p>To Johannes
&ndash; Marko Sarstedt
</p>
<p>To Erik
&ndash; Irma Mooi-Reci</p>
<p/>
</div>
<div class="page"><p/>
<p>Preface
</p>
<p>In the digital economy, data have become a valuable commodity,much in theway that
</p>
<p>oil is in the rest of the economy (Wedel and Kannan 2016). Data enable market
</p>
<p>researchers to obtain valuable and novel insights. There aremany new sources of data,
</p>
<p>such as web traffic, social networks, online surveys, and sensors that track suppliers,
</p>
<p>customers, and shipments. A Forbes (2015a) survey of senior executives reveals that
</p>
<p>96% of the respondents consider data-driven marketing crucial to success. Not
</p>
<p>surprisingly, data are valuable to companies who spend over $44 billion a year on
</p>
<p>obtaining insights (Statista.com 2017). So valuable are these insights that companies
</p>
<p>go to great lengths to conceal the findings. Apple, for example, is known to carefully
</p>
<p>hide that it conducts a great deal of research, as the insights from this enable the
</p>
<p>company to gain a competitive advantage (Heisler 2012).
</p>
<p>This book is about being able to supply such insights. It is a valuable skill for
</p>
<p>which there are abundant jobs. Forbes (2015b) shows that IBM, Cisco, and Oracle
</p>
<p>alone have more than 25,000 unfilled data analysis positions. Davenport and Patil
</p>
<p>(2012) label data scientist as the sexiest job of the twenty-first century.
</p>
<p>This book introduces market research, using commonly used quantitative
</p>
<p>techniques such as regression analysis, factor analysis, and cluster analysis. These
</p>
<p>statistical methods have generated findings that have significantly shaped the way
</p>
<p>we see the world today. Unlike most market research books, which use SPSS
</p>
<p>(we&rsquo;ve been there!), this book uses Stata. Stata is a very popular statistical software
</p>
<p>package and has many advanced options that are otherwise difficult to access. It
</p>
<p>allows users to run statistical analyses by means of menus and directly typed
</p>
<p>commands called syntax. This syntax is very useful if you want to repeat analyses
</p>
<p>or find that you have made a mistake. Stata has matured into a user-friendly
</p>
<p>environment for statistical analysis, offering a wide range of features.
</p>
<p>If you search for market(ing) research books on Google or Amazon, you will find
</p>
<p>that there is no shortage of such books. However, this book differs in many
</p>
<p>important ways:
</p>
<p>&ndash; This book is a bridge between the theory of conducting quantitative research and
</p>
<p>its execution, using the market research process as a framework. We discuss
</p>
<p>market research, starting off by identifying the research question, designing the
</p>
<p>vii</p>
<p/>
</div>
<div class="page"><p/>
<p>data collection process, collecting, and describing data. We also introduce
</p>
<p>essential data analysis techniques and the basics of communicating the results,
</p>
<p>including a discussion on ethics. Each chapter on quantitative methods describes
</p>
<p>key theoretical choices and how these are executed in Stata. Unlike most other
</p>
<p>books, we do not discuss theory or application but link the two.
</p>
<p>&ndash; This is a book for nontechnical readers! All chapters are written in an accessible
</p>
<p>and comprehensive way so that readers without a profound background in
</p>
<p>statistics can also understand the introduced data analysis methods. Each chapter
</p>
<p>on research methods includes examples to help the reader gain a hands-on
</p>
<p>feeling for the technique. Each chapter concludes with an illustrated case that
</p>
<p>demonstrates the application of a quantitative method.
</p>
<p>&ndash; To facilitate learning, we use a single case study throughout the book. This case
</p>
<p>deals with a customer survey of a fictitious company called Oddjob Airways
</p>
<p>(familiar to those who have seen the James Bond movie Goldfinger!). We also
</p>
<p>provide additional end-of-chapter cases, including different datasets, thus
</p>
<p>allowing the readers to practice what they have learned. Other pedagogical
</p>
<p>features, such as keywords, examples, and end-of-chapter questions, support
</p>
<p>the contents.
</p>
<p>&ndash; Stata has become a very popular statistics package in the social sciences and
</p>
<p>beyond, yet there are almost no books that show how to use the program without
</p>
<p>diving into the depths of syntax language.
</p>
<p>&ndash; This book is concise, focusing on the most important aspects that a market
</p>
<p>researcher, or manager interpreting market research, should know.
</p>
<p>&ndash; Many chapters provide links to further readings and other websites. Mobile tags
</p>
<p>in the text allow readers to quickly browse related web content using a mobile
</p>
<p>device (see section &ldquo;How to Use Mobile Tags&rdquo;). This unique merger of offline
</p>
<p>and online content offers readers a broad spectrum of additional and readily
</p>
<p>accessible information. A comprehensive web appendix with information on
</p>
<p>further analysis techniques and datasets is included.
</p>
<p>&ndash; Lastly, we have set up a Facebook page called Market Research: The Process,
</p>
<p>Data, and Methods. This page provides a platform for discussions and the
</p>
<p>exchange of market research ideas.
</p>
<p>viii Preface</p>
<p/>
</div>
<div class="page"><p/>
<p>How to Use Mobile Tags
</p>
<p>In this book, there are several mobile tags that allow you to instantly access
</p>
<p>information by means of your mobile phone&rsquo;s camera if it has a mobile tag reader
</p>
<p>installed. For example, the following mobile tag is a link to this book&rsquo;s website at
</p>
<p>http://www.guide-market-research.com.
</p>
<p>Severalmobile phones comewith amobile tag reader already installed, but you can
</p>
<p>also download tag readers. In this book, we use QR (quick response) codes, which can
</p>
<p>be accessed by means of the readers below. Simply visit one of the following
</p>
<p>webpages or download the App from the iPhone App Store or from Google Play:
</p>
<p>&ndash; Kaywa: http://reader.kaywa.com/
</p>
<p>&ndash; i-Nigma: http://www.i-nigma.com/
</p>
<p>Once you have a reader app installed, just start the app and point your camera at
</p>
<p>the mobile tag. This will open your mobile phone browser and direct you to the
</p>
<p>associated website.
</p>
<p>Step 1
</p>
<p>Point at a mobile tag and
</p>
<p>take a picture
</p>
<p>Step 2
</p>
<p>Decoding and
</p>
<p>loading
</p>
<p>Loading WWW
</p>
<p>Step 3
</p>
<p>Website
</p>
<p>Preface ix</p>
<p/>
<div class="annotation"><a href="http://www.guide-market-research.com">http://www.guide-market-research.com</a></div>
<div class="annotation"><a href="http://reader.kaywa.com/">http://reader.kaywa.com/</a></div>
<div class="annotation"><a href="http://www.i-nigma.com/">http://www.i-nigma.com/</a></div>
</div>
<div class="page"><p/>
<p>How to Use This Book
</p>
<p>The following will help you read this book:
</p>
<p>&bull; Stata commands that the user types or the program issues appear in a different
</p>
<p>font.
</p>
<p>&bull; Variable or file names in the main text appear in italics to distinguish them from
</p>
<p>the descriptions.
</p>
<p>&bull; Items from Stata&rsquo;s interface are shown in bold, with successive menu options
separated while variable names are shown in italics. For example, the text could
</p>
<p>read: &ldquo;Go to► Graphics► Scatterplot matrix and enter the variables s1, s2, and
</p>
<p>s3 into the Variables box.&rdquo; This means that the word Variables appears in the
Stata interface while s1, s2, and s3 are variable names.
</p>
<p>&bull; Keywords also appear in bold when they first appear in the main text. We have
used many keywords to help you find everything quickly. Additional index
</p>
<p>terms appear in italics.
</p>
<p>&bull; If you see Web Appendix! Downloads in the book, please go to https://www.
</p>
<p>guide-market-research.com/stata/ and click on downloads.
</p>
<p>In the chapters, youwill also find boxes for the interested reader in whichwe discuss
</p>
<p>details. The text can be understood without reading these boxes, which are therefore
</p>
<p>optional. We have also included mobile tags to help you access material quickly.
</p>
<p>For Instructors
</p>
<p>Besides the benefits described above, this book is also designed to make teaching as
</p>
<p>easy as possible when using this book. Each chapter comes with a set of detailed
</p>
<p>and professionally designed PowerPoint slides for educators, tailored for this book,
</p>
<p>which can be easily adapted to fit a specific course&rsquo;s needs. These are available on
</p>
<p>the website&rsquo;s instructor resources page at http://www.guide-market-research.com.
</p>
<p>You can gain access to the instructor&rsquo;s page by requesting log-in information under
</p>
<p>Instructor Resources.
</p>
<p>x Preface</p>
<p/>
<div class="annotation"><a href="https://www.guide-market-research.com/stata/">https://www.guide-market-research.com/stata/</a></div>
<div class="annotation"><a href="https://www.guide-market-research.com/stata/">https://www.guide-market-research.com/stata/</a></div>
<div class="annotation"><a href="http://www.guide-market-research.com">http://www.guide-market-research.com</a></div>
</div>
<div class="page"><p/>
<p>The book&rsquo;s web appendices are freely available on the accompanying website
</p>
<p>and provide supplementary information on analysis techniques not covered in the
</p>
<p>book and datasets. Moreover, at the end of each chapter, there is a set of questions
</p>
<p>that can be used for in-class discussions.
</p>
<p>If you have any remarks, suggestions, or ideas about this book, please drop us a
</p>
<p>line at erik.mooi@unimelb.edu.au (Erik Mooi), marko.sarstedt@ovgu.de (Marko
</p>
<p>Sarstedt), or irma.mooi@unimelb.edu.au (Irma Mooi-Reci). We appreciate any
</p>
<p>feedback on the book&rsquo;s concept and contents!
</p>
<p>Parkville, VIC, Australia Erik Mooi
</p>
<p>Magdeburg, Germany Marko Sarstedt
</p>
<p>Parkville, VIC, Australia Irma Mooi-Reci
</p>
<p>Preface xi</p>
<p/>
</div>
<div class="page"><p/>
<p>Acknowledgments
</p>
<p>Thanks to all the students who have inspired us with their feedback and constantly
</p>
<p>reinforce our choice to stay in academia. We have many people to thank for making
</p>
<p>this book possible. First, we would like to thank Springer and particularly Stephen
</p>
<p>Jones for all their help and for their willingness to publish this book. We also want
</p>
<p>to thank Bill Rising of StataCorp for providing immensely useful feedback. Ilse
</p>
<p>Evertse has done a wonderful job (again!) proofreading the chapters. She is a great
</p>
<p>proofreader and we cannot recommend her enough! Drop her a line at
</p>
<p>stpubus@gmail.com if you need proofreading help. In addition, we would like to
</p>
<p>thank the team of current and former doctoral students and research fellows at Otto-
</p>
<p>von-Guericke-University Magdeburg, namely, Kati Barth, Janine Dankert, Frauke
</p>
<p>K&uuml;hn, Sebastian Lehmann, Doreen Neubert, and Victor Schliwa. Finally, we would
</p>
<p>like to acknowledge the many insights and 1 suggestions provided by many of our
</p>
<p>colleagues and students. We would like to thank the following:
</p>
<p>Ralf Aigner of Wishbird, Mexico City, Mexico
Carolin Bock of the Technische Universität Darmstadt, Darmstadt, Germany
Cees J. P. M. de Bont of Hong Kong Polytechnic University, Hung Hom,
</p>
<p>Hong Kong
</p>
<p>Bernd Erichson of Otto-von-Guericke-University Magdeburg, Magdeburg,
Germany
</p>
<p>Andrew M. Farrell of the University of Southampton, Southampton, UK
Sebastian Fuchs of BMW Group, M&uuml;nchen, Germany
David I. Gilliland of Colorado State University, Fort Collins, CO, USA
Joe F. Hair Jr. of the University of South Alabama, Mobile, AL, USA
J&euro;org Henseler of the University of Twente, Enschede, The Netherlands
Emile F. J. Lancée of Vrije Universiteit Amsterdam, Amsterdam, The Netherlands
Tim F. Liao of the University of Illinois Urbana-Champaign, USA
Peter S. H. Leeflang of the University of Groningen, Groningen, The Netherlands
Arjen van Lin of Vrije Universiteit Amsterdam, Amsterdam, The Netherlands
Leonard J. Paas of Massey University, Albany, New Zealand
</p>
<p>xiii</p>
<p/>
</div>
<div class="page"><p/>
<p>Sascha Raithel of FU Berlin, Berlin, Germany
Edward E. Rigdon of Georgia State University, Atlanta, GA, USA
Christian M. Ringle of Technische Universität Hamburg-Harburg, Hamburg,
</p>
<p>Germany
</p>
<p>John Rudd of the University of Warwick, Coventry, UK
Sebastian Scharf of Hochschule Mittweida, Mittweida, Germany
Tobias Sch&euro;utz of the ESB Business School Reutlingen, Reutlingen, Germany
Philip Sugai of the International University of Japan, Minamiuonuma, Niigata,
</p>
<p>Japan
</p>
<p>Charles R. Taylor of Villanova University, Philadelphia, PA, USA
Andrés Trujillo-Barrera of Wageningen University &amp; Research
Stefan Wagner of the European School of Management and Technology, Berlin,
</p>
<p>Germany
</p>
<p>Eelke Wiersma of Vrije Universiteit Amsterdam, Amsterdam, The Netherlands
Caroline Wiertz of Cass Business School, London, UK
Michael Zyphur of the University of Melbourne, Parkville, Australia
</p>
<p>References
</p>
<p>Davenport, T. H., &amp; Patil, D. J. (2012). Data scientist. The sexiest job of the 21st century. Harvard
Business Review, 90(October), 70&ndash;76.
</p>
<p>Forbes. (2015a). Data driven and customer centric: Marketers turning insights into impact. http://
www.forbes.com/forbesinsights/data-driven_and_customer-centric/. Accessed 21 Aug 2017.
</p>
<p>Forbes. (2015b).Where big data jobs will be in 2016. http://www.forbes.com/sites/louiscolumbus/
2015/11/16/where-big-data-jobs-will-be-in-2016/#68fece3ff7f1/. Accessed 21 Aug 2017.
</p>
<p>Heisler, Y. (2012). How Apple conducts market research and keeps iOS source code locked down.
Network world, August 3, 2012, http://www.networkworld.com/article/2222892/wireless/
how-apple-conducts-market-research-and-keeps-iossource-code-locked-down.html. Accessed
21 Aug 2017.
</p>
<p>Statista.com. (2017). Market research industry/market &ndash; Statistics &amp; facts. https://www.statista.
com/topics/1293/market-research/. Accessed 21 Aug 2017.
</p>
<p>Wedel, M., &amp; Kannan, P. K. (2016). Marketing analytics for data-rich environments. Journal of
Marketing, 80(6), 97&ndash;121.
</p>
<p>xiv Acknowledgments</p>
<p/>
<div class="annotation"><a href="http://www.forbes.com/forbesinsights/data-driven_and_customer-centric/">http://www.forbes.com/forbesinsights/data-driven_and_customer-centric/</a></div>
<div class="annotation"><a href="http://www.forbes.com/forbesinsights/data-driven_and_customer-centric/">http://www.forbes.com/forbesinsights/data-driven_and_customer-centric/</a></div>
<div class="annotation"><a href="http://www.forbes.com/sites/louiscolumbus/2015/11/16/where-big-data-jobs-will-be-in-2016/#68fece3ff7f1/">http://www.forbes.com/sites/louiscolumbus/2015/11/16/where-big-data-jobs-will-be-in-2016/#68fece3ff7f1/</a></div>
<div class="annotation"><a href="http://www.forbes.com/sites/louiscolumbus/2015/11/16/where-big-data-jobs-will-be-in-2016/#68fece3ff7f1/">http://www.forbes.com/sites/louiscolumbus/2015/11/16/where-big-data-jobs-will-be-in-2016/#68fece3ff7f1/</a></div>
<div class="annotation"><a href="http://www.networkworld.com/article/2222892/wireless/how-apple-conducts-market-research-and-keeps-iossource-code-locked-down.html">http://www.networkworld.com/article/2222892/wireless/how-apple-conducts-market-research-and-keeps-iossource-code-locked-down.html</a></div>
<div class="annotation"><a href="http://www.networkworld.com/article/2222892/wireless/how-apple-conducts-market-research-and-keeps-iossource-code-locked-down.html">http://www.networkworld.com/article/2222892/wireless/how-apple-conducts-market-research-and-keeps-iossource-code-locked-down.html</a></div>
<div class="annotation"><a href="https://www.statista.com/topics/1293/market-research/">https://www.statista.com/topics/1293/market-research/</a></div>
<div class="annotation"><a href="https://www.statista.com/topics/1293/market-research/">https://www.statista.com/topics/1293/market-research/</a></div>
</div>
<div class="page"><p/>
<p>Contents
</p>
<p>1 Introduction to Market Research . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
</p>
<p>1.2 What Is Market and Marketing Research? . . . . . . . . . . . . . . . 2
</p>
<p>1.3 Market Research by Practitioners and Academics . . . . . . . . . . 3
</p>
<p>1.4 When Should Market Research (Not) Be Conducted? . . . . . . . 4
</p>
<p>1.5 Who Provides Market Research? . . . . . . . . . . . . . . . . . . . . . . 5
</p>
<p>1.6 Review Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
</p>
<p>1.7 Further Readings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
</p>
<p>References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
</p>
<p>2 The Market Research Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
</p>
<p>2.2 Identify and Formulate the Problem . . . . . . . . . . . . . . . . . . . . 12
</p>
<p>2.3 Determine the Research Design . . . . . . . . . . . . . . . . . . . . . . . 13
</p>
<p>2.3.1 Exploratory Research . . . . . . . . . . . . . . . . . . . . . . . . 14
</p>
<p>2.3.2 Uses of Exploratory Research . . . . . . . . . . . . . . . . . . 15
</p>
<p>2.3.3 Descriptive Research . . . . . . . . . . . . . . . . . . . . . . . . 17
</p>
<p>2.3.4 Uses of Descriptive Research . . . . . . . . . . . . . . . . . . 17
</p>
<p>2.3.5 Causal Research . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
</p>
<p>2.3.6 Uses of Causal Research . . . . . . . . . . . . . . . . . . . . . . 21
</p>
<p>2.4 Design the Sample and Method of Data Collection . . . . . . . . . 23
</p>
<p>2.5 Collect the Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
</p>
<p>2.6 Analyze the Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
</p>
<p>2.7 Interpret, Discuss, and Present the Findings . . . . . . . . . . . . . . 23
</p>
<p>2.8 Follow-Up . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
</p>
<p>2.9 Review Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
</p>
<p>2.10 Further Readings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
</p>
<p>References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
</p>
<p>3 Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
</p>
<p>3.2 Types of Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
</p>
<p>3.2.1 Primary and Secondary Data . . . . . . . . . . . . . . . . . . . 31
</p>
<p>3.2.2 Quantitative and Qualitative Data . . . . . . . . . . . . . . . 32
</p>
<p>xv</p>
<p/>
</div>
<div class="page"><p/>
<p>3.3 Unit of Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
</p>
<p>3.4 Dependence of Observations . . . . . . . . . . . . . . . . . . . . . . . . . 34
</p>
<p>3.5 Dependent and Independent Variables . . . . . . . . . . . . . . . . . . 35
</p>
<p>3.6 Measurement Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
</p>
<p>3.7 Validity and Reliability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
</p>
<p>3.7.1 Types of Validity . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
</p>
<p>3.7.2 Types of Reliability . . . . . . . . . . . . . . . . . . . . . . . . . 40
</p>
<p>3.8 Population and Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
</p>
<p>3.8.1 Probability Sampling . . . . . . . . . . . . . . . . . . . . . . . . 43
</p>
<p>3.8.2 Non-probability Sampling . . . . . . . . . . . . . . . . . . . . . 45
</p>
<p>3.8.3 Probability or Non-probability Sampling? . . . . . . . . . 46
</p>
<p>3.9 Sample Sizes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
</p>
<p>3.10 Review Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
</p>
<p>3.11 Further Readings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
</p>
<p>References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
</p>
<p>4 Getting Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
</p>
<p>4.2 Secondary Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
</p>
<p>4.2.1 Internal Secondary Data . . . . . . . . . . . . . . . . . . . . . . 53
</p>
<p>4.2.2 External Secondary Data . . . . . . . . . . . . . . . . . . . . . 54
</p>
<p>4.3 Conducting Secondary Data Research . . . . . . . . . . . . . . . . . . 58
</p>
<p>4.3.1 Assess Availability of Secondary Data . . . . . . . . . . . 58
</p>
<p>4.3.2 Assess Inclusion of Key Variables . . . . . . . . . . . . . . 60
</p>
<p>4.3.3 Assess Construct Validity . . . . . . . . . . . . . . . . . . . . . 60
</p>
<p>4.3.4 Assess Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
</p>
<p>4.4 Conducting Primary Data Research . . . . . . . . . . . . . . . . . . . . 62
</p>
<p>4.4.1 Collecting Primary Data Through Observations . . . . . 62
</p>
<p>4.4.2 Collecting Quantitative Data: Designing Surveys . . . . 64
</p>
<p>4.5 Basic Qualitative Research . . . . . . . . . . . . . . . . . . . . . . . . . . 82
</p>
<p>4.5.1 In-Depth Interviews . . . . . . . . . . . . . . . . . . . . . . . . . 82
</p>
<p>4.5.2 Projective Techniques . . . . . . . . . . . . . . . . . . . . . . . 84
</p>
<p>4.5.3 Focus Groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
</p>
<p>4.6 Collecting Primary Data Through Experimental Research . . . . 86
</p>
<p>4.6.1 Principles of Experimental Research . . . . . . . . . . . . . 86
</p>
<p>4.6.2 Experimental Designs . . . . . . . . . . . . . . . . . . . . . . . . 87
</p>
<p>4.7 Review Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
</p>
<p>4.8 Further Readings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90
</p>
<p>References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
</p>
<p>5 Descriptive Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
5.1 The Workflow of Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
</p>
<p>5.2 Create Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
</p>
<p>5.3 Enter Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
</p>
<p>xvi Contents</p>
<p/>
</div>
<div class="page"><p/>
<p>5.4 Clean Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
</p>
<p>5.4.1 Interviewer Fraud . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
</p>
<p>5.4.2 Suspicious Response Patterns . . . . . . . . . . . . . . . . . . 100
</p>
<p>5.4.3 Data Entry Errors . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
</p>
<p>5.4.4 Outliers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
</p>
<p>5.4.5 Missing Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
</p>
<p>5.5 Describe Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
</p>
<p>5.5.1 Univariate Graphs and Tables . . . . . . . . . . . . . . . . . . 110
</p>
<p>5.5.2 Univariate Statistics . . . . . . . . . . . . . . . . . . . . . . . . . 113
</p>
<p>5.5.3 Bivariate Graphs and Tables . . . . . . . . . . . . . . . . . . . 115
</p>
<p>5.5.4 Bivariate Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . 117
</p>
<p>5.6 Transform Data (Optional) . . . . . . . . . . . . . . . . . . . . . . . . . . 120
</p>
<p>5.6.1 Variable Respecification . . . . . . . . . . . . . . . . . . . . . . 120
</p>
<p>5.6.2 Scale Transformation . . . . . . . . . . . . . . . . . . . . . . . . 121
</p>
<p>5.7 Create a Codebook . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
</p>
<p>5.8 The Oddjob Airways Case Study . . . . . . . . . . . . . . . . . . . . . . 124
</p>
<p>5.8.1 Introduction to Stata . . . . . . . . . . . . . . . . . . . . . . . . . 124
</p>
<p>5.8.2 Finding Your Way in Stata . . . . . . . . . . . . . . . . . . . . 126
</p>
<p>5.9 Data Management in Stata . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
</p>
<p>5.9.1 Restrict Observations . . . . . . . . . . . . . . . . . . . . . . . . 134
</p>
<p>5.9.2 Create a New Variable from Existing Variable(s) . . . . 135
</p>
<p>5.9.3 Recode Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . 136
</p>
<p>5.10 Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
</p>
<p>5.10.1 Clean Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
</p>
<p>5.10.2 Describe Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
</p>
<p>5.11 Cadbury and the UK Chocolate Market (Case Study) . . . . . . . 149
</p>
<p>5.12 Review Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150
</p>
<p>5.13 Further Readings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
</p>
<p>References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
</p>
<p>6 Hypothesis Testing &amp; ANOVA . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153
6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153
</p>
<p>6.2 Understanding Hypothesis Testing . . . . . . . . . . . . . . . . . . . . . 154
</p>
<p>6.3 Testing Hypotheses on One Mean . . . . . . . . . . . . . . . . . . . . . 156
</p>
<p>6.3.1 Step 1: Formulate the Hypothesis . . . . . . . . . . . . . . . 156
</p>
<p>6.3.2 Step 2: Choose the Significance Level . . . . . . . . . . . . 158
</p>
<p>6.3.3 Step 3: Select an Appropriate Test . . . . . . . . . . . . . . 160
</p>
<p>6.3.4 Step 4: Calculate the Test Statistic . . . . . . . . . . . . . . 168
</p>
<p>6.3.5 Step 5: Make the Test Decision . . . . . . . . . . . . . . . . . 171
</p>
<p>6.3.6 Step 6: Interpret the Results . . . . . . . . . . . . . . . . . . . 175
</p>
<p>6.4 Two-Samples t-Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175
</p>
<p>6.4.1 Comparing Two Independent Samples . . . . . . . . . . . . 175
</p>
<p>6.4.2 Comparing Two Paired Samples . . . . . . . . . . . . . . . . 177
</p>
<p>Contents xvii</p>
<p/>
</div>
<div class="page"><p/>
<p>6.5 Comparing More Than Two Means: Analysis of Variance
</p>
<p>(ANOVA) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
</p>
<p>6.6 Understanding One-Way ANOVA . . . . . . . . . . . . . . . . . . . . . 180
</p>
<p>6.6.1 Check the Assumptions . . . . . . . . . . . . . . . . . . . . . . 181
</p>
<p>6.6.2 Calculate the Test Statistic . . . . . . . . . . . . . . . . . . . . 182
</p>
<p>6.6.3 Make the Test Decision . . . . . . . . . . . . . . . . . . . . . . 186
</p>
<p>6.6.4 Carry Out Post Hoc Tests . . . . . . . . . . . . . . . . . . . . . 187
</p>
<p>6.6.5 Measure the Strength of the Effects . . . . . . . . . . . . . . 188
</p>
<p>6.6.6 Interpret the Results and Conclude . . . . . . . . . . . . . . 189
</p>
<p>6.6.7 Plotting the Results (Optional) . . . . . . . . . . . . . . . . . 189
</p>
<p>6.7 Going Beyond One-Way ANOVA: The Two-Way
</p>
<p>ANOVA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190
</p>
<p>6.8 Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198
</p>
<p>6.8.1 Independent Samples t-Test . . . . . . . . . . . . . . . . . . . 198
</p>
<p>6.8.2 One-way ANOVA . . . . . . . . . . . . . . . . . . . . . . . . . . 202
</p>
<p>6.8.3 Two-way ANOVA . . . . . . . . . . . . . . . . . . . . . . . . . . 207
</p>
<p>6.9 Customer Analysis at Crédit Samouel (Case Study) . . . . . . . . 212
</p>
<p>6.10 Review Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213
</p>
<p>6.11 Further Readings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213
</p>
<p>References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214
</p>
<p>7 Regression Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216
</p>
<p>7.2 Understanding Regression Analysis . . . . . . . . . . . . . . . . . . . . 216
</p>
<p>7.3 Conducting a Regression Analysis . . . . . . . . . . . . . . . . . . . . . 219
</p>
<p>7.3.1 Check the Regression Analysis Data
</p>
<p>Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219
</p>
<p>7.3.2 Specify and Estimate the Regression Model . . . . . . . . 222
</p>
<p>7.3.3 Test the Regression Analysis Assumptions . . . . . . . . 226
</p>
<p>7.3.4 Interpret the Regression Results . . . . . . . . . . . . . . . . 231
</p>
<p>7.3.5 Validate the Regression Results . . . . . . . . . . . . . . . . 237
</p>
<p>7.3.6 Use the Regression Model . . . . . . . . . . . . . . . . . . . . 239
</p>
<p>7.4 Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243
</p>
<p>7.4.1 Check the Regression Analysis Data
</p>
<p>Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244
</p>
<p>7.4.2 Specify and Estimate the Regression Model . . . . . . . 248
</p>
<p>7.4.3 Test the Regression Analysis Assumptions . . . . . . . . 249
</p>
<p>7.4.4 Interpret the Regression Results . . . . . . . . . . . . . . . . 254
</p>
<p>7.4.5 Validate the Regression Results . . . . . . . . . . . . . . . . 258
</p>
<p>7.5 Farming with AgriPro (Case Study) . . . . . . . . . . . . . . . . . . . . 260
</p>
<p>7.6 Review Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 262
</p>
<p>7.7 Further Readings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 262
</p>
<p>References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263
</p>
<p>xviii Contents</p>
<p/>
</div>
<div class="page"><p/>
<p>8 Principal Component and Factor Analysis . . . . . . . . . . . . . . . . . . . 265
8.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266
</p>
<p>8.2 Understanding Principal Component and Factor Analysis . . . . 267
</p>
<p>8.2.1 Why Use Principal Component and Factor
</p>
<p>Analysis? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267
</p>
<p>8.2.2 Analysis Steps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269
</p>
<p>8.3 Principal Component Analysis . . . . . . . . . . . . . . . . . . . . . . . . 270
</p>
<p>8.3.1 Check Requirements and Conduct Preliminary
</p>
<p>Analyses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270
</p>
<p>8.3.2 Extract the Factors . . . . . . . . . . . . . . . . . . . . . . . . . . 273
</p>
<p>8.3.3 Determine the Number of Factors . . . . . . . . . . . . . . . 278
</p>
<p>8.3.4 Interpret the Factor Solution . . . . . . . . . . . . . . . . . . . 280
</p>
<p>8.3.5 Evaluate the Goodness-of-Fit of the Factor
</p>
<p>Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282
</p>
<p>8.3.6 Compute the Factor Scores . . . . . . . . . . . . . . . . . . . . 283
</p>
<p>8.4 Confirmatory Factor Analysis and Reliability Analysis . . . . . . 284
</p>
<p>8.5 Structural Equation Modeling . . . . . . . . . . . . . . . . . . . . . . . . 289
</p>
<p>8.6 Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 291
</p>
<p>8.6.1 Principal Component Analysis . . . . . . . . . . . . . . . . . 291
</p>
<p>8.6.2 Reliability Analysis . . . . . . . . . . . . . . . . . . . . . . . . . 304
</p>
<p>8.7 Customer Satisfaction at Haver and Boecker (Case Study) . . . 306
</p>
<p>8.8 Review Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 308
</p>
<p>8.9 Further Readings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 309
</p>
<p>References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 309
</p>
<p>9 Cluster Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313
9.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 314
</p>
<p>9.2 Understanding Cluster Analysis . . . . . . . . . . . . . . . . . . . . . . . 314
</p>
<p>9.3 Conducting a Cluster Analysis . . . . . . . . . . . . . . . . . . . . . . . . 316
</p>
<p>9.3.1 Select the Clustering Variables . . . . . . . . . . . . . . . . . 316
</p>
<p>9.3.2 Select the Clustering Procedure . . . . . . . . . . . . . . . . . 321
</p>
<p>9.3.3 Select a Measure of Similarity or Dissimilarity . . . . . 333
</p>
<p>9.3.4 Decide on the Number of Clusters . . . . . . . . . . . . . . . 340
</p>
<p>9.3.5 Validate and Interpret the Clustering Solution . . . . . . 344
</p>
<p>9.4 Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349
</p>
<p>9.4.1 Select the Clustering Variables . . . . . . . . . . . . . . . . . 350
</p>
<p>9.4.2 Select the Clustering Procedure and Measure
</p>
<p>of Similarity or Dissimilarity . . . . . . . . . . . . . . . . . . 353
</p>
<p>9.4.3 Decide on the Number of Clusters . . . . . . . . . . . . . . . 354
</p>
<p>9.4.4 Validate and Interpret the Clustering Solution . . . . . . 358
</p>
<p>9.5 Oh, James! (Case Study) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 362
</p>
<p>9.6 Review Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 363
</p>
<p>9.7 Further Readings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 364
</p>
<p>References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 365
</p>
<p>Contents xix</p>
<p/>
</div>
<div class="page"><p/>
<p>10 Communicating the Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367
10.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367
</p>
<p>10.2 Identify the Audience . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 368
</p>
<p>10.3 Guidelines for Written Reports . . . . . . . . . . . . . . . . . . . . . . . 369
</p>
<p>10.4 Structure the Written Report . . . . . . . . . . . . . . . . . . . . . . . . . 370
</p>
<p>10.4.1 Title Page . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 371
</p>
<p>10.4.2 Executive Summary . . . . . . . . . . . . . . . . . . . . . . . . . 371
</p>
<p>10.4.3 Table of Contents . . . . . . . . . . . . . . . . . . . . . . . . . . . 371
</p>
<p>10.4.4 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 372
</p>
<p>10.4.5 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 372
</p>
<p>10.4.6 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373
</p>
<p>10.4.7 Conclusion and Recommendations . . . . . . . . . . . . . . 383
</p>
<p>10.4.8 Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384
</p>
<p>10.4.9 Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384
</p>
<p>10.5 Guidelines for Oral Presentations . . . . . . . . . . . . . . . . . . . . . . 384
</p>
<p>10.6 Visual Aids in Oral Presentations . . . . . . . . . . . . . . . . . . . . . . 385
</p>
<p>10.7 Structure the Oral Presentation . . . . . . . . . . . . . . . . . . . . . . . 386
</p>
<p>10.8 Follow-Up . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 387
</p>
<p>10.9 Ethics in Research Reports . . . . . . . . . . . . . . . . . . . . . . . . . . 388
</p>
<p>10.10 Review Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 389
</p>
<p>10.11 Further Readings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 389
</p>
<p>References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 389
</p>
<p>Glossary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 391
</p>
<p>Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 411
</p>
<p>xx Contents</p>
<p/>
</div>
<div class="page"><p/>
<p>Introduction to Market Research 1
</p>
<p>Keywords
</p>
<p>American Marketing Association (AMA) &bull; ESOMAR &bull; Field service firms &bull; Full
</p>
<p>service providers &bull; Limited service providers &bull; Segment specialists &bull; Specialized
</p>
<p>service firms &bull; Syndicated data
</p>
<p>Learning Objectives
</p>
<p>After reading this chapter, you should understand:
</p>
<p>&ndash; What market and marketing research are and how they differ.
</p>
<p>&ndash; How practitioner and academic market(ing) research differ.
</p>
<p>&ndash; When market research should be conducted.
</p>
<p>&ndash; Who provides market research and the importance of the market research
</p>
<p>industry.
</p>
<p>1.1 Introduction
</p>
<p>When Toyota developed the Prius&mdash;a highly fuel-efficient car using a hybrid petrol/
</p>
<p>electric engine&mdash;it took a gamble on a grand scale. Honda and General Motors&rsquo;
</p>
<p>previous attempts to develop frugal (electric) cars had not worked well. Just like
</p>
<p>Honda and General Motors, Toyota had also been working on developing a frugal
</p>
<p>car, but focused on a system integrating a petrol and electric engine. These
</p>
<p>development efforts led Toyota to start a project called Global Twenty-first Century
</p>
<p>aimed at developing a car with a fuel economy that was at least 50% better than
</p>
<p>similar-sized cars. This project nearly came to a halt in 1995 when Toyota encoun-
</p>
<p>tered substantial technological problems. The company solved these problems,
</p>
<p>using nearly a thousand engineers, and launched the car, called the Prius, in
</p>
<p>Japan in 1997. Internal Toyota predictions suggested that the car was either going
</p>
<p># Springer Nature Singapore Pte Ltd. 2018
E. Mooi et al., Market Research, Springer Texts in Business and Economics,
DOI 10.1007/978-981-10-5218-7_1
</p>
<p>1</p>
<p/>
</div>
<div class="page"><p/>
<p>to be an instant hit, or that the product&rsquo;s acceptance would be slow, as it takes time
</p>
<p>to teach dealers and consumers about the technology. In 1999, Toyota decided to
</p>
<p>start working on launching the Prius in the US. Initial market research showed that
</p>
<p>it was going to be a difficult task. Some consumers thought it was too small for the
</p>
<p>US and some thought the positioning of the controls was poor for US drivers. There
</p>
<p>were other issues too, such as the design, which many thought was too strongly
</p>
<p>geared towards Japanese drivers.
</p>
<p>While preparing for the launch, Toyota conducted further market research,
</p>
<p>which could, however, not reveal who the potential car buyers would be. Initially,
</p>
<p>Toyota thought the car might be tempting for people concerned with the environ-
</p>
<p>ment, but market research dispelled this belief. Environmentalists dislike technol-
</p>
<p>ogy in general and money is a big issue for this group. A technologically complex
</p>
<p>and expensive car such as the Prius was therefore unlikely to appeal to them.
</p>
<p>Additional market research did little to identify any other good market segment.
</p>
<p>Despite the lack of conclusive findings, Toyota decided to sell the car anyway and
</p>
<p>to await the public&rsquo;s reaction. Before the launch, Toyota put a market research
</p>
<p>system in place to track the initial sales and identify where customers bought the
</p>
<p>car. After the formal launch in 2000, this system quickly found that celebrities were
</p>
<p>buying the car to demonstrate their concern for the environment. Somewhat later,
</p>
<p>Toyota noticed substantially increased sales figures when ordinary consumers
</p>
<p>became aware of the car&rsquo;s appeal to celebrities. It appeared that consumers were
</p>
<p>willing to purchase cars that celebrities endorse.
</p>
<p>CNW Market Research, a market research company specializing in the automo-
</p>
<p>tive industry, attributed part of the Prius&rsquo;s success to its unique design, which
</p>
<p>clearly demonstrated that Prius owners were driving a different car. After substan-
</p>
<p>tial increases in the petrol price, and changes to the car (based on extensive market
</p>
<p>research) to increase its appeal, Toyota&rsquo;s total Prius sales reached about four
</p>
<p>million and the company is now the market leader in hybrid petrol/electric cars.
</p>
<p>This example shows that while market research occasionally helps, sometimes it
</p>
<p>contributes little, or even fails. There are many reasons for market research&rsquo;s
</p>
<p>success varying. These reasons include the budget available for research, the
</p>
<p>support for market research in the organization, the implementation, and the market
</p>
<p>researchers&rsquo; research skills. In this book, we will guide you step by step through the
</p>
<p>practicalities of the basic market research process. These discussions, explanations,
</p>
<p>facts, and methods will help you carry out successful market research.
</p>
<p>1.2 What Is Market and Marketing Research?
</p>
<p>Market research can mean several things. It can be the process by which we gain
</p>
<p>insight into how markets work. Market research is also a function in an organiza-
</p>
<p>tion, or it can refer to the outcomes of research, such as a database of customer
</p>
<p>purchases, or a report that offers recommendations. In this book, we focus on the
</p>
<p>market research process, starting by identifying and formulating the problem,
</p>
<p>continuing by determining the research design, determining the sample and method
</p>
<p>2 1 Introduction to Market Research</p>
<p/>
</div>
<div class="page"><p/>
<p>of data collection, collecting the data, analyzing the data, interpreting, discussing,
</p>
<p>and presenting the findings, and ending with the follow-up.
</p>
<p>Some people consider marketing research and market research to be synony-
</p>
<p>mous, whereas others regard these as different concepts. TheAmericanMarketing
</p>
<p>Association (AMA), the largest marketing association in North America, defines
</p>
<p>marketing research as follows:
</p>
<p>The function that links the consumer, customer, and public to the marketer through
information &ndash; information used to identify and define marketing opportunities and
problems; generate, refine, and evaluate marketing actions; monitor marketing perfor-
mance; and improve understanding of marketing as a process. Marketing research specifies
the information required to address these issues, designs the method for collecting infor-
mation, manages and implements the data collection process, analyzes the results, and
communicates the findings and their implications (American Marketing Association 2004).
</p>
<p>On the other hand, ESOMAR, the world organization for market, consumer and
</p>
<p>societal research, defines market research as:
</p>
<p>The systematic gathering and interpretation of information about individuals and
organisations. It uses the statistical and analytical methods and techniques of the applied
social, behavioural and data sciences to generate insights and support decision-making by
providers of goods and services, governments, non-profit organisations and the general
public. (ICC/ESOMAR international code on market, opinion, and social research and data
analytics 2016).
</p>
<p>Both definitions overlap substantially, but the AMA definition focuses on mar-
</p>
<p>keting research as a function (e.g., a department in an organization), whereas the
</p>
<p>ESOMAR definition focuses on the process. In this book, we focus on the process
</p>
<p>and, thus, on market research.
</p>
<p>1.3 Market Research by Practitioners and Academics
</p>
<p>Practitioners and academics are both involved in marketing and market research.
</p>
<p>Academic and practitioner views of market(ing) research differ in many ways, but
</p>
<p>also have many communalities.
</p>
<p>There is, however, a key difference is their target groups. Academics almost
</p>
<p>exclusively undertake research with the goal of publishing in academic journals.
</p>
<p>Highly esteemed journals include the Journal of Marketing, Journal of Marketing
</p>
<p>Research, Journal of the Academy of Marketing Science, and the International
</p>
<p>Journal of Research in Marketing. On the other hand, practitioners&rsquo; target group is
</p>
<p>the client, whose needs and standards include relevance, practicality, generalizabil-
</p>
<p>ity, and timeliness of insights. Journals, on the other hand, frequently emphasize
</p>
<p>methodological rigor and consistency. Academic journals are often difficult to read
</p>
<p>and understand, while practitioner reports should be easy to read.
</p>
<p>Academics and practitioners differ greatly in their use of and focus on methods.
</p>
<p>Practitioners have adapted and refined some of the methods, such as cluster analysis
</p>
<p>1.3 Market Research by Practitioners and Academics 3</p>
<p/>
</div>
<div class="page"><p/>
<p>and factor analysis, which academics developed originally.1 Developing methods is
</p>
<p>often a goal in itself for academics. Practitioners are more concerned about the
</p>
<p>value of applying specific methods. Standards also differ. Clear principles and
</p>
<p>professional conduct as advocated by ESOMAR and the Australian Market &amp;
</p>
<p>Social Research Society (AMSRS) (for examples, see https://www.esomar.org/
</p>
<p>uploads/public/knowledge-and-standards/codes-and-guidelines/ICCESOMAR-
</p>
<p>International-Code_English.pdf and http://www.amsrs.com.au/documents/item/
</p>
<p>194) mostly guide practitioners&rsquo; methods. Universities and schools sometimes
</p>
<p>impose data collection and analysis standards on academics, but these tend not to
</p>
<p>have the level of detail advocated by ESOMAR or the AMSRS. Interestingly, many
</p>
<p>practitioners claim that their methods meet academic standards, but academics
</p>
<p>never claim that their methods are based on practitioner standards.
</p>
<p>Besides these differences, there are also many similarities. For example, good
</p>
<p>measurement is paramount for academics and practitioners. Furthermore,
</p>
<p>academics and practitioners should be interested in each other&rsquo;s work; academics
</p>
<p>can learn much from the practical issues that practitioners faced, while practitioners
</p>
<p>can gain much from understanding the tools, techniques, and concepts that
</p>
<p>academics develop. Reibstein et al. (2009), who issued an urgent call for the
</p>
<p>academic marketing community to focus on relevant business problems, underlined
</p>
<p>the need to learn from each other. Several other researchers, such as Lee and
</p>
<p>Greenley (2010), Homburg et al. (2015), and Tellis (2017), have echoed this call.
</p>
<p>1.4 When Should Market Research (Not) Be Conducted?
</p>
<p>Market research serves several useful roles in organizations. Most importantly,
</p>
<p>market research can help organizations by providing answers to questions firms
</p>
<p>may have about their customers and competitors; answers that could help such firms
</p>
<p>improve their performance. Specific questions related to this include identifying
</p>
<p>market opportunities, measuring customer satisfaction, and assessing market
</p>
<p>shares. Some of these questions arise ad hoc, perhaps due to issues that the top
</p>
<p>management, or one of the departments or divisions, has identified. Much market
</p>
<p>research is, however, programmatic; it arises because firms systematically evaluate
</p>
<p>market elements. Subway, the restaurant chain, systematically measures customer
</p>
<p>satisfaction, which is an example of programmatic research. This type of research
</p>
<p>does not usually have a distinct beginning and end (contrary to ad hoc research), but
</p>
<p>is executed continuously over time and leads to daily, weekly, or monthly reports.
</p>
<p>The decision to conduct market research may be taken when managers face an
</p>
<p>uncertain situation and when the costs of undertaking good research are (much)
</p>
<p>lower than good decisions&rsquo; expected benefits. Researching trivial issues or issues
</p>
<p>that cannot be changed is not helpful.
</p>
<p>1Roberts et al. (2014) and Hauser (2017) discuss the impact of marketing science tools on
marketing practice.
</p>
<p>4 1 Introduction to Market Research</p>
<p/>
<div class="annotation"><a href="https://www.esomar.org/uploads/public/knowledge-and-standards/codes-and-guidelines/ICCESOMAR-International-Code_English.pdf">https://www.esomar.org/uploads/public/knowledge-and-standards/codes-and-guidelines/ICCESOMAR-International-Code_English.pdf</a></div>
<div class="annotation"><a href="https://www.esomar.org/uploads/public/knowledge-and-standards/codes-and-guidelines/ICCESOMAR-International-Code_English.pdf">https://www.esomar.org/uploads/public/knowledge-and-standards/codes-and-guidelines/ICCESOMAR-International-Code_English.pdf</a></div>
<div class="annotation"><a href="https://www.esomar.org/uploads/public/knowledge-and-standards/codes-and-guidelines/ICCESOMAR-International-Code_English.pdf">https://www.esomar.org/uploads/public/knowledge-and-standards/codes-and-guidelines/ICCESOMAR-International-Code_English.pdf</a></div>
<div class="annotation"><a href="http://www.amsrs.com.au/documents/item/194">http://www.amsrs.com.au/documents/item/194</a></div>
<div class="annotation"><a href="http://www.amsrs.com.au/documents/item/194">http://www.amsrs.com.au/documents/item/194</a></div>
</div>
<div class="page"><p/>
<p>Other issues to consider are the politics within the organization, because if the
</p>
<p>decision to go ahead has already been made (as in the Prius example in the
</p>
<p>introduction), market research is unnecessary. If market research is conducted
</p>
<p>and supports the decision, it is of little value&mdash;and those undertaking the research
</p>
<p>may have been biased in favor of the decision. On the other hand, market research is
</p>
<p>ignored if it rejects the decision.
</p>
<p>Moreover, organizations often need to make very quick decisions, for example,
</p>
<p>when responding to competitive price changes, unexpected changes in regulation,
</p>
<p>or to the economic climate. In such situations, however, market research may only
</p>
<p>be included after decisions have already been made. Consequently, research should
</p>
<p>mostly not be undertaken when urgent decisions have to be made.
</p>
<p>1.5 Who Provides Market Research?
</p>
<p>Many organizations have people, departments, or other companies working for
</p>
<p>them to provide market research. In Fig. 1.1, we show who these providers of
</p>
<p>market research are.
</p>
<p>Most market research is provided internally by specialized market research
</p>
<p>departments, or people tasked with this function. It appears that about 75% of
</p>
<p>organizations have at least one person tasked with carrying out market research.
</p>
<p>This percentage is similar across most industries, although it is much less in govern-
</p>
<p>ment sectors and, particularly, in health care (Iaccobucci and Churchill 2015).
</p>
<p>In larger organizations, a sub department of the marketing department usually
</p>
<p>undertakes internally provided market research. Sometimes this sub department is
</p>
<p>not connected to a marketing department, but to other organizational functions,
</p>
<p>such as corporate planning or sales (Rouziès and Hulland 2014). Many large
</p>
<p>organizations even have a separate market research department. This system of
</p>
<p>having a separate market research department, or merging it with other
</p>
<p>Providers of
</p>
<p>market research
</p>
<p>Internal External
</p>
<p>Syndicated data
</p>
<p>Full service
</p>
<p>Segment specialists
</p>
<p>Limited service
</p>
<p>SpecializedCustomized services Field service
</p>
<p>Fig. 1.1 The providers of market research
</p>
<p>1.5 Who Provides Market Research? 5</p>
<p/>
</div>
<div class="page"><p/>
<p>departments, seems to become more widespread, with the marketing function
</p>
<p>devolving increasingly into other functions within organizations (Sheth and Sisodia
</p>
<p>2006).
</p>
<p>The external providers of market research are a powerful economic force. In
</p>
<p>2015, the Top 50 external providers had a collective turnover of about $21.78 billion
</p>
<p>(Honomichl 2016). The market research industry has also become a global field
</p>
<p>with companies such as The Nielsen Company (USA), Kantar (UK), GfK
</p>
<p>(Germany), and Ipsos (France), playing major roles outside their home markets.
</p>
<p>External providers of market research are either full service providers or
</p>
<p>limited ones.
</p>
<p>Full service providers are large market research companies such as The Nielsen
</p>
<p>Company (http://www.nielsen.com), Kantar (http://www.kantar.com), and GfK
</p>
<p>(http://www.gfk.com). These large companies provide syndicated data and
</p>
<p>customized services. Syndicated data are data collected in a standard format and
</p>
<p>not specifically collected for a single client. These data, or analyses based on the
</p>
<p>data, are then sold to multiple clients. Large marketing research firms mostly collect
</p>
<p>syndicated data, as they have the resources to collect large amounts of data and can
</p>
<p>spread the costs of doing so over a number of clients. For example, The Nielsen
</p>
<p>Company collects syndicated data in several forms: Nielsen&rsquo;s Netratings, which
</p>
<p>collects information on digital media; Nielsen Ratings, which details the type of
</p>
<p>consumer who listens to the radio, watches TV, or reads print media; and Nielsen
</p>
<p>Homescan, which collects panel information on the purchases consumers make.
</p>
<p>These large firms also offer customized services by conducting studies for a specific
</p>
<p>client. These customized services can be very specific, such as helping a client carry
</p>
<p>out specific analyses.
</p>
<p>Measuring TV audiences is critical for advertisers. But measuring the number
</p>
<p>of viewers per program has become more difficult as households currently
</p>
<p>have multiple TVs and may have different viewing platforms. In addition,
</p>
<p>&ldquo;time shift&rdquo; technologies, such as video-on-demand, have further compli-
</p>
<p>cated the tracking of viewer behavior. Nielsen has measured TV and other
</p>
<p>media use for more than 25 years, using a device called the (Portable) People
</p>
<p>Meter. This device measures usage of each TV viewing platform and
</p>
<p>instantly transmits the results back to Nielsen, allowing for instant measure-
</p>
<p>ment. Altogether, Nielsen captures about 40% of the world&rsquo;s viewing
</p>
<p>behavior.2
</p>
<p>In the following seven videos, experts from The Nielsen Company discuss
</p>
<p>how the People Meter works.
</p>
<p>(continued)
</p>
<p>2See http://www.nielsen.com/eu/en/solutions/measurement/television.html for further detail.
</p>
<p>6 1 Introduction to Market Research</p>
<p/>
<div class="annotation"><a href="http://www.nielsen.com">http://www.nielsen.com</a></div>
<div class="annotation"><a href="http://www.kantar.com">http://www.kantar.com</a></div>
<div class="annotation"><a href="http://www.gfk.com">http://www.gfk.com</a></div>
<div class="annotation"><a href="http://www.nielsen.com/eu/en/solutions/measurement/television.html">http://www.nielsen.com/eu/en/solutions/measurement/television.html</a></div>
</div>
<div class="page"><p/>
<p>Contrary to full service providers, which undertake nearly all market research
</p>
<p>activities, limited service providers specialize in one or more services and tend to
</p>
<p>be smaller companies. In fact, many of the specialized market research companies
</p>
<p>are one-man businesses and the owner&mdash;after (or besides) a practitioner or aca-
</p>
<p>demic career&mdash;offers specialized services. Although there are many different types
</p>
<p>of limited service firms, we only discuss three of them: those focused on segmenta-
</p>
<p>tion, field service, and specialized services.
</p>
<p>Segment specialists concentrate on specific market segments. Skytrax, which
</p>
<p>focuses on market research in the airline and airport sector, is an example of such
</p>
<p>specialists. Other segment specialists do not focus on a particular industry, but on a
</p>
<p>type of customer; for example, Ethnic Focus (http://www.ethnicfocus.com), a
</p>
<p>UK-based market research firm, focuses on understanding ethnic minorities.
</p>
<p>Field service firms, such as Survey Sampling International (http://www.
</p>
<p>surveysampling.com), focus on executing surveys, determining samples, sample
</p>
<p>sizes, and collecting data. Some of these firms also translate surveys, or provide
</p>
<p>addresses and contact details.
</p>
<p>Specialized Service firms are a catch-all term for those firms with specific
</p>
<p>technical skills, thus only focusing on specific products, or aspects of products,
</p>
<p>such as market research on taste and smell. Specialized firms may also concentrate
</p>
<p>on a few highly specific market research techniques, or may focus on one or more
</p>
<p>highly specialized analysis techniques, such as time series analysis, panel data
</p>
<p>analysis, or quantitative text analysis. Envirosell (http://www.envirosell.com), a
</p>
<p>research and consultancy firm that analyzes consumer behavior in commercial
</p>
<p>environments, is a well-known example of a specialized service firm.
</p>
<p>A choice between these full service and limited service market research firms
</p>
<p>boils down to a tradeoff between what they can provide (if this is highly specialized,
</p>
<p>you may not have much choice) and the price of doing so. In addition, if you have to
</p>
<p>combine several studies to gain further insight, full service firms may be better than
</p>
<p>multiple limited service firms. The fit and feel with the provider are obviously also
</p>
<p>highly important!
</p>
<p>1.5 Who Provides Market Research? 7</p>
<p/>
<div class="annotation"><a href="http://www.ethnicfocus.com">http://www.ethnicfocus.com</a></div>
<div class="annotation"><a href="http://www.surveysampling.com">http://www.surveysampling.com</a></div>
<div class="annotation"><a href="http://www.surveysampling.com">http://www.surveysampling.com</a></div>
<div class="annotation"><a href="http://www.envirosell.com">http://www.envirosell.com</a></div>
</div>
<div class="page"><p/>
<p>1.6 Review Questions
</p>
<p>1. What is market research? Try to explain what market research is in your own
</p>
<p>words.
</p>
<p>2. Imagine you are the head of a division of Procter &amp; Gamble. You are just about
</p>
<p>ready to launch a new shampoo, but are uncertain about who might buy it. Is it
</p>
<p>useful to conduct a market research study? Should you delay the launch of the
</p>
<p>product?
</p>
<p>3. Try to find the websites of a few market research firms. Look, for example, at the
</p>
<p>services provided by GfK and the Nielsen Company, and compare the extent of
</p>
<p>their offerings to those of specialized firms such as those listed on, for example,
</p>
<p>http://www.greenbook.org.
</p>
<p>4. If you have a specialized research question, such as what market opportunities
</p>
<p>there are for selling music to ethnic minorities, would you use a full service or
</p>
<p>limited service firm (or both)? Please discuss the benefits and drawbacks.
</p>
<p>1.7 Further Readings
</p>
<p>American Marketing Association at http://www.marketingpower.com
</p>
<p>Website of the American Marketing Association. Provides information on their
</p>
<p>activities and also links to two of the premier marketing journals, the Journal of
</p>
<p>Marketing and the Journal of Marketing Research.
</p>
<p>Insights Association at http://www.insightsassociation.org/ Launched in 2017, the
</p>
<p>Insights Association was formed through the merger of two organizations with
</p>
<p>long, respected histories of servicing the market research and analytics industry:
</p>
<p>CASRO (founded in 1975) and MRA (founded in 1957). The organization
</p>
<p>focuses on providing knowledge, advice, and standards to those working in the
</p>
<p>market research profession.
</p>
<p>The British Market Research Society at http://www.mrs.org.uk
</p>
<p>The website of the British Market Research society contains a searchable directory
</p>
<p>of market research providers and useful information on market research careers
</p>
<p>and jobs.
</p>
<p>Associaç~ao Brasileira de Empresas de Pesquisa (Brazilian Association of Research
</p>
<p>Companies) at http://www.abep.org/novo/default.aspx
</p>
<p>The website of the Brazilian Association of Research Companies. It documents
</p>
<p>research ethics, standards, etc.
</p>
<p>ESOMAR at http://www.esomar.org
</p>
<p>The website of ESOMAR, the world organization for market, consumer and societal
</p>
<p>research. Amongst other activities, ESOMAR sets ethical and technical
</p>
<p>standards for market research and publishes books and reports on market
</p>
<p>research.
</p>
<p>GreenBook: The guide for buyers of marketing research services at http://www.
</p>
<p>greenbook.org
</p>
<p>This website provides an overview of many different types of limited service firms.
</p>
<p>8 1 Introduction to Market Research</p>
<p/>
<div class="annotation"><a href="http://www.greenbook.org">http://www.greenbook.org</a></div>
<div class="annotation"><a href="http://www.marketingpower.com">http://www.marketingpower.com</a></div>
<div class="annotation"><a href="http://www.insightsassociation.org/">http://www.insightsassociation.org/</a></div>
<div class="annotation"><a href="http://www.mrs.org.uk">http://www.mrs.org.uk</a></div>
<div class="annotation"><a href="http://www.abep.org/novo/default.aspx">http://www.abep.org/novo/default.aspx</a></div>
<div class="annotation"><a href="http://www.esomar.org">http://www.esomar.org</a></div>
<div class="annotation"><a href="http://www.greenbook.org">http://www.greenbook.org</a></div>
<div class="annotation"><a href="http://www.greenbook.org">http://www.greenbook.org</a></div>
</div>
<div class="page"><p/>
<p>References
</p>
<p>Hauser, J. R. (2017). Phenomena, theory, application, data, and methods all have impact. Journal
of the Academy of Marketing Science, 45(1), 7&ndash;9.
</p>
<p>Homburg, C., Vomberg, A., Enke, M., &amp; Grimm, P. H. (2015). The loss of the marketing
department&rsquo;s influence: Is it happening? And why worry? Journal of the Academy of Marketing
Science, 43(1), 1&ndash;13.
</p>
<p>Honomichl, J. (2016). 2016 Honomichl Gold Top 50. https://www.ama.org/publications/
MarketingNews/Pages/2016-ama-gold-top-50-report.aspx
</p>
<p>Iaccobucci, D., &amp; Churchill, G. A. (2015). Marketing research: Methodological foundations
(11th ed.). CreateSpace Independent Publishing Platform.
</p>
<p>ICC/ESOMAR international code on market and social research. (2007). http://www.
netcasearbitration.com/uploadedFiles/ICC/policy/marketing/Statements/ICCESOMAR_
Code_English.pdf
</p>
<p>Lee, N., &amp; Greenley, G. (2010). The theory-practice divide: Thoughts from the editors and senior
advisory board of EJM. European Journal of Marketing, 44(1/2), 5&ndash;20.
</p>
<p>Reibstein, D. J., Day, G., &amp;Wind, J. (2009). Guest editorial: Is marketing academia losing its way?
Journal of Marketing, 73(4), 1&ndash;3.
</p>
<p>Roberts, J. H., Kayand, U., &amp; Stremersch, S. (2014). From academic research to marketing
practice: Exploring the marketing science value chain. International Journal of Research in
Marketing, 31(2), 128&ndash;140.
</p>
<p>Rouziès, D., &amp; Hulland, J. (2014). Does marketing and sales integration always pay off? Evidence
from a social capital perspective. Journal of the Academy of Marketing Science, 42(5),
511&ndash;527.
</p>
<p>Sheth, J. N., &amp; Sisodia, R. S. (Eds.). (2006). Does marketing need reform? In does marketing need
reform? Fresh perspective on the future. Armonk: M.E. Sharpe.
</p>
<p>Tellis, G. J. (2017). Interesting and impactful research: On phenomena, theory, and writing.
Journal of the Academy of Marketing Science, 45(1), 1&ndash;6.
</p>
<p>References 9</p>
<p/>
<div class="annotation"><a href="https://www.ama.org/publications/MarketingNews/Pages/2016-ama-gold-top-50-report.aspx">https://www.ama.org/publications/MarketingNews/Pages/2016-ama-gold-top-50-report.aspx</a></div>
<div class="annotation"><a href="https://www.ama.org/publications/MarketingNews/Pages/2016-ama-gold-top-50-report.aspx">https://www.ama.org/publications/MarketingNews/Pages/2016-ama-gold-top-50-report.aspx</a></div>
<div class="annotation"><a href="http://www.netcasearbitration.com/uploadedFiles/ICC/policy/marketing/Statements/ICCESOMAR_Code_English.pdf%20">http://www.netcasearbitration.com/uploadedFiles/ICC/policy/marketing/Statements/ICCESOMAR_Code_English.pdf%20</a></div>
<div class="annotation"><a href="http://www.netcasearbitration.com/uploadedFiles/ICC/policy/marketing/Statements/ICCESOMAR_Code_English.pdf%20">http://www.netcasearbitration.com/uploadedFiles/ICC/policy/marketing/Statements/ICCESOMAR_Code_English.pdf%20</a></div>
<div class="annotation"><a href="http://www.netcasearbitration.com/uploadedFiles/ICC/policy/marketing/Statements/ICCESOMAR_Code_English.pdf%20">http://www.netcasearbitration.com/uploadedFiles/ICC/policy/marketing/Statements/ICCESOMAR_Code_English.pdf%20</a></div>
</div>
<div class="page"><p/>
<p>The Market Research Process 2
</p>
<p>Keywords
</p>
<p>Causal research &bull; Descriptive research &bull; Ethnographies &bull; Exploratory research &bull;
</p>
<p>Field experiments &bull; Focus groups &bull; Hypotheses &bull; In-depth interviews &bull; Lab
</p>
<p>experiments &bull; Market segments &bull; Observational studies &bull; Projective
</p>
<p>techniques &bull; Research design &bull; Scanner data &bull; Test markets
</p>
<p>Learning Objectives
</p>
<p>After reading this chapter, you should understand:
</p>
<p>&ndash; How to determine a research design.
</p>
<p>&ndash; The differences between, and examples of, exploratory research, descriptive
</p>
<p>research, and causal research.
</p>
<p>&ndash; What causality is.
</p>
<p>&ndash; The market research process.
</p>
<p>2.1 Introduction
</p>
<p>How do organizations plan for market research processes? In this chapter, we
</p>
<p>explore the market research process and various types of research. We introduce
</p>
<p>the planning of market research projects, starting with identifying and formulating
</p>
<p>the problem and ending with presenting the findings and the follow-up (see
</p>
<p>Fig. 2.1). This chapter is also an outline of the chapters to come.
</p>
<p># Springer Nature Singapore Pte Ltd. 2018
E. Mooi et al., Market Research, Springer Texts in Business and Economics,
DOI 10.1007/978-981-10-5218-7_2
</p>
<p>11</p>
<p/>
</div>
<div class="page"><p/>
<p>2.2 Identify and Formulate the Problem
</p>
<p>The first step in setting up a market research process involves identifying and
</p>
<p>formulating the research problem. Identifying the research problem is valuable, but
</p>
<p>also difficult. To identify the &ldquo;right&rdquo; research problem, we should first identify the
</p>
<p>marketing symptoms or marketing opportunities. The marketing symptom is a prob-
</p>
<p>lem that an organization faces. Examples of marketing symptoms include declining
</p>
<p>market shares, increasing numbers of complaints, or new products that consumers do
</p>
<p>not adopt. In some cases, there is no real problem, but instead a marketing opportu-
</p>
<p>nity, such as the potential benefits that new channels and products offer, or emerging
</p>
<p>market opportunities that need to be explored. Exploring marketing symptoms and
</p>
<p>marketing opportunities requires asking questions such as:
</p>
<p>&ndash; Why is our market share declining?
</p>
<p>&ndash; Why is the number of complaints increasing?
</p>
<p>&ndash; Why are our new products not successful?
</p>
<p>&ndash; How can we enter the market for 3D printers?
</p>
<p>&ndash; How can we increase our online sales?
</p>
<p>Fig. 2.1 The market research process
</p>
<p>12 2 The Market Research Process</p>
<p/>
</div>
<div class="page"><p/>
<p>The research problems that result from such questions can come in different
</p>
<p>forms. Generally, we distinguish three types of research problems:
</p>
<p>&ndash; ambiguous problems,
</p>
<p>&ndash; somewhat defined problems, and
</p>
<p>&ndash; clearly defined problems.
</p>
<p>Ambiguous problems occur when we know very little about the issues that need
</p>
<p>to be solved. For example, ambiguity typically surrounds the introduction of
</p>
<p>radically new technologies or products. When Toyota planned to launch the Prius
</p>
<p>many years ago, critical, but little understood, issues arose, such as the features that
</p>
<p>were essential and even who the potential buyers of such a car were.
</p>
<p>When we face somewhat defined problems, we know the issues (and variables) that
</p>
<p>are important for solving the problem, but not how they are related. For example, when
</p>
<p>an organization wants to export products, it is relatively easy to obtain all sorts of
</p>
<p>information onmarket sizes, economic development, and the political and legal system.
</p>
<p>However, how these variables impact the exporting success may be very uncertain.
</p>
<p>When we face clearly defined problems, the important issues and variables, as
</p>
<p>well as their relationships, are clear. However, we do not know how to make the
</p>
<p>best possible choice. We therefore face the problem of how the situation should be
</p>
<p>optimized. A clearly defined problem may arise when organizations want to change
</p>
<p>their prices. While organizations know that increasing (or decreasing) prices gen-
</p>
<p>erally leads to decreased (increased) demand, the precise relationship (i.e., how
</p>
<p>many units do we sell less when the price is increased by $1?) is unknown.
</p>
<p>2.3 Determine the Research Design
</p>
<p>The research design is related to the identification and formulation of the problem.
</p>
<p>Research problems and research designs are highly related. If we start working on
</p>
<p>an issue that has never been researched before, we seem to enter a funnel where we
</p>
<p>initially ask exploratory questions, because we as yet know little about the issues we
</p>
<p>face. These exploratory questions are best answered using an exploratory research
</p>
<p>design. Once we have a clearer picture of the research issue after our exploratory
</p>
<p>research, we move further into the funnel. Generally, we want to learn more by
</p>
<p>describing the research problem in terms of descriptive research. Once we have a
</p>
<p>reasonably complete picture of all the issues, it may be time to determine exactly
</p>
<p>how key variables are linked. We then move to the narrowest part of the funnel. We
</p>
<p>do this through causal (not casual!) research (see Fig. 2.2).
</p>
<p>Each research design has different uses and requires the application of different
</p>
<p>analysis techniques. For example, whereas exploratory research can help formulate
</p>
<p>problems exactly or structure them, causal research provides exact insights into
</p>
<p>how variables relate. In Fig. 2.3, we provide several examples of different types of
</p>
<p>research, which we will discuss in the following paragraphs.
</p>
<p>2.3 Determine the Research Design 13</p>
<p/>
</div>
<div class="page"><p/>
<p>2.3.1 Exploratory Research
</p>
<p>As its name suggests, the objective of exploratory research is to explore a problem
</p>
<p>or situation. As such, exploratory research has several key uses regarding the
</p>
<p>solving of ambiguous problems. It can help organizations formulate their problems
</p>
<p>exactly. Through initial research, such as interviewing potential customers, the
</p>
<p>opportunities and pitfalls may be identified that help determine or refine the
</p>
<p>Fig. 2.2 The relationship between the marketing problem and the research design
</p>
<p>Exploratory 
</p>
<p>research
</p>
<p>Descriptive 
</p>
<p>research
</p>
<p>Causal 
</p>
<p>research
</p>
<p>Uses
</p>
<p>&bull; Understand structure
</p>
<p>&bull; Formulate problems 
</p>
<p>precisely
</p>
<p>&bull; Generate   
</p>
<p>hypotheses
</p>
<p>&bull; Develop 
</p>
<p>measurement scales 
</p>
<p>&bull; Describe customers or 
</p>
<p>competitors
</p>
<p>&bull; Understand market 
</p>
<p>size
</p>
<p>&bull; Segment markets
</p>
<p>&bull; Measure performance 
</p>
<p>(e.g., share of wallet, 
</p>
<p>brand awareness)
</p>
<p>&bull; Uncover causality
</p>
<p>&bull; Understand the 
</p>
<p>performance effects of 
</p>
<p>marketing mix 
</p>
<p>elements
</p>
<p>Ambiguous 
</p>
<p>problems
</p>
<p>Somewhat 
</p>
<p>defined problems
</p>
<p>Clearly defined 
</p>
<p>problems
</p>
<p>Fig. 2.3 Uses of exploratory, descriptive, and causal research
</p>
<p>14 2 The Market Research Process</p>
<p/>
</div>
<div class="page"><p/>
<p>research problem. It is crucial to discuss this information with the client to ensure
</p>
<p>that your findings are helpful. Such initial research also helps establish priorities
</p>
<p>(what is nice to know and what is important to know?) and eliminate impractical
</p>
<p>ideas. For example, market research helped Toyota dispel the belief that people
</p>
<p>concerned with the environment would buy the Prius, as this target group has an
</p>
<p>aversion to high technology and lacks spending power.
</p>
<p>2.3.2 Uses of Exploratory Research
</p>
<p>Exploratory research can be used to formulate problems precisely. For example,
</p>
<p>focus groups, in-depth interviews, projective techniques, observational studies, and
</p>
<p>ethnographies are often used to achieve this. In the following, we briefly introduce
</p>
<p>each technique, but provide more detailed descriptions in Chap. 4.
</p>
<p>Focus groups usually have between 4 and 6 participants, who discuss a defined
</p>
<p>topic under the leadership of a moderator. The key difference between a depth
</p>
<p>interview and focus group is that focus group participants can interact with one
</p>
<p>another (e.g., &ldquo;What do you mean by. . .?,&rdquo; &ldquo;How does this differ from. . ..&rdquo;),
</p>
<p>thereby providing insight into group dynamics. In-depth interviews consist of an
</p>
<p>interviewer asking an interviewee several questions. Depth interviews allow prob-
</p>
<p>ing on a one-to-one basis, which fosters interaction between the interviewer and the
</p>
<p>respondent. Depth interviews are required when the topic needs to be adjusted for
</p>
<p>each interviewee, for sensitive topics, and/or when the person interviewed has a
</p>
<p>very high status.
</p>
<p>Projective techniques present people with pictures, words, or other stimuli to
</p>
<p>which they respond. For example, a researcher could ask what people think of
</p>
<p>BMW owners (&ldquo;A BMW owner is someone who. . ..&rdquo;) or could show them a picture
</p>
<p>of a BMW and ask them what they associate the picture with. Moreover, when
</p>
<p>designing new products, market researchers can use different pictures and words to
</p>
<p>create analogies to existing products and product categories, thus making the
</p>
<p>adoption of new products more attractive (Feiereisen et al. 2008).
</p>
<p>Observational studies are frequently used to refine research questions and
</p>
<p>clarify issues. Observational studies require an observer to monitor and interpret
</p>
<p>participants&rsquo; behavior. For example, someone could monitor how consumers spend
</p>
<p>their time in shops or how they walk through the aisles of a supermarket. These
</p>
<p>studies require a person, a camera or other tracking devices, such as radio frequency
</p>
<p>identification (RFID) chips, to monitor behavior. Other observational studies may
</p>
<p>comprise click stream data that track information on the web pages people have
</p>
<p>visited. Observational studies can also be useful to understand how people consume
</p>
<p>and/or use products. New technology is being developed in this area, for example,
</p>
<p>market research company Almax (also see Chap. 4) has developed the EyeSee
</p>
<p>Mannequin which helps observe who is attracted by store windows and reveals
</p>
<p>important details about customers, such as their age range, gender, ethnicity, and
</p>
<p>dwell time.
</p>
<p>2.3 Determine the Research Design 15</p>
<p/>
</div>
<div class="page"><p/>
<p>In the award-winning paper &ldquo;An Exploratory Look at Supermarket
</p>
<p>Shopping Paths,&rdquo; Larson et al. (2005) analyze the paths individual shoppers
</p>
<p>take in a grocery store, which the RFID tags located on their shopping carts
</p>
<p>provide. The results provide new perspectives on many long-standing
</p>
<p>perceptions of shopper travel behavior within a supermarket, including
</p>
<p>ideas related to aisle traffic, special promotional displays, and perimeter
</p>
<p>shopping patterns. Before this study, most retailers believed that customers
</p>
<p>walked through the aisles systematically. Larson et al.&rsquo;s (2005) research
</p>
<p>reveals this rarely happens.
</p>
<p>Ethnography (or ethnographic studies) originate from anthropology. In ethno-
</p>
<p>graphic research, a researcher interacts with consumers over a period to observe and
</p>
<p>ask questions. Such studies can consist of, for example, a researcher living with a
</p>
<p>family to observe how they buy, consume, and use products. For example, the
</p>
<p>market research company BBDO used ethnographies to understand consumers&rsquo;
</p>
<p>rituals. The company found that many consumer rituals are ingrained in consumers
</p>
<p>in certain countries, but not in others. For example, women in Colombia, Brazil, and
</p>
<p>Japan are more than twice as likely to apply make-up when in their cars, than
</p>
<p>women in other countries. Miele, a German whitegoods producer, used
</p>
<p>ethnographies to understand how people with allergies do their washing and
</p>
<p>developed washing machines based on the insights gathered (Burrows 2014).
</p>
<p>Exploratory research can also help establish research priorities. What is
</p>
<p>important to know and what is less important? For example, a literature search
</p>
<p>may reveal that there are useful previous studies and that new market research is
</p>
<p>not necessary. Exploratory research may also lead to the elimination of impracti-
</p>
<p>cal ideas. Literature searches, just like interviews, may again help eliminate
</p>
<p>impractical ideas.
</p>
<p>Another helpful aspect of exploratory research is the generation of hypotheses.
</p>
<p>A hypothesis is a claim made about a population, which can be tested by using
</p>
<p>sample results. For example, one could hypothesize that at least 10% of people in
</p>
<p>16 2 The Market Research Process</p>
<p/>
</div>
<div class="page"><p/>
<p>France are aware of a certain product. Marketers frequently suggest hypotheses,
</p>
<p>because they help them structure and make decisions. In Chap. 6, we discuss
</p>
<p>hypotheses and how they can be tested in greater detail.
</p>
<p>Another use of exploratory research is to develop measurement scales. For
</p>
<p>example, what questions can we use to measure customer satisfaction? What
</p>
<p>questions work best in our context? Do potential respondents understand the
</p>
<p>wording, or do we need to make changes? Exploratory research can help us answer
</p>
<p>such questions. For example, an exploratory literature search may use measurement
</p>
<p>scales that tell us how to measure important variables such as corporate reputation
</p>
<p>and service quality.
</p>
<p>2.3.3 Descriptive Research
</p>
<p>As its name implies, descriptive research is all about describing certain phenom-
</p>
<p>ena, characteristics or functions. It can focus on one variable (e.g., profitability) or
</p>
<p>on two or more variables at the same time (&ldquo;what is the relationship between market
</p>
<p>share and profitability?&rdquo; and &ldquo;how does temperature relate to the sale of ice
</p>
<p>cream?&rdquo;). Descriptive research often builds on previous exploratory research.
</p>
<p>After all, to describe something, we must have a good idea of what we need to
</p>
<p>measure and how we should measure it. Key ways in which descriptive research
</p>
<p>can help us include describing customers, competitors, market segments, and
</p>
<p>measuring performance.
</p>
<p>2.3.4 Uses of Descriptive Research
</p>
<p>Market researchers conduct descriptive research for many purposes. These include,
</p>
<p>for example, describing customers or competitors. For instance, how large is the
</p>
<p>UK market for pre-packed cookies? How large is the worldwide market for cruises
</p>
<p>priced $10,000 and more? How many new products did our competitors launch last
</p>
<p>year? Descriptive research helps us answer such questions. Much data are available
</p>
<p>for descriptive purposes, particularly on durable goods and fast moving consumer
</p>
<p>goods. One source of such data are scanner data, which are collected at the
</p>
<p>checkout of a supermarket where details about each product sold are entered into
</p>
<p>a vast database. By using scanner data, it is, for example, possible to describe the
</p>
<p>market for pre-packed cookies in the UK.
</p>
<p>Descriptive research is frequently used to define market segments, or simply
</p>
<p>segments. Since companies can seldom connect with all their (potential) customers
</p>
<p>individually, they dividemarkets into groups of (potential) customerswith similar needs
</p>
<p>and wants. Firms can then target each of these segments by positioning themselves in a
</p>
<p>unique segment (such as Ferrari in the high-end sports car market). Many market
</p>
<p>research companies specialize in market segmentation; an example is Claritas, which
</p>
<p>developed a segmentation scheme for the US market called PRIZM (Potential Ratings
</p>
<p>Index by Zip Markets). PRIZM segments consumers along a multitude of attitudinal,
</p>
<p>2.3 Determine the Research Design 17</p>
<p/>
</div>
<div class="page"><p/>
<p>behavioral, and demographic characteristics; companies can use these segments to
</p>
<p>better target their customers. Segments have names, such as Up-and-Comers (young
</p>
<p>professionals with a college degree and a mid-level income) and Backcountry Folk
</p>
<p>(older, often retired people with a high school degree and low income).
</p>
<p>Another important function of descriptive market research is to measure perfor-
</p>
<p>mance. Nearly all companies regularly track their sales across specific product
</p>
<p>categories to evaluate the performance of the firm, the managers, or specific
</p>
<p>employees. Such descriptive work overlaps with the finance or accounting
</p>
<p>departments&rsquo; responsibilities. However, market researchers also frequently mea-
</p>
<p>sure performance using measures that are quite specific to marketing, such as share
</p>
<p>of wallet (i.e., how much do people spend on a certain brand or company in a
</p>
<p>product category?) and brand awareness (i.e., do you know brand/company X?), or
</p>
<p>the Net Promotor Score, a customer loyalty metric for brands or firms (see Chap. 3
</p>
<p>for more information).
</p>
<p>2.3.5 Causal Research
</p>
<p>Causal research is used to understand the relationships between two or more
</p>
<p>variables. For example, we may wish to estimate how changes in the wording of
</p>
<p>an advertisement impact recall. Causal research provides exact insights into how
</p>
<p>variables relate and may be useful as a test run to try out changes in the marketing
</p>
<p>mix. Market researchers undertake causal research less frequently than exploratory
</p>
<p>or descriptive research. Nevertheless, it is important to understand the delicate
</p>
<p>relationships between important marketing variables and the outcomes they help
</p>
<p>create. The key usage of causal research is to uncover causality. Causality is the
</p>
<p>relationship between an event (the cause) and a second event (the effect) when the
</p>
<p>second event is a consequence of the first. To claim causality, we need to meet the
</p>
<p>following four requirements:
</p>
<p>&ndash; relationship between cause and effect,
</p>
<p>&ndash; time order,
</p>
<p>&ndash; controlling for other factors, and
</p>
<p>&ndash; an explanatory theory.
</p>
<p>First, the cause needs to be related to the effect. For example, if we want to
</p>
<p>determine whether price increases cause sales to drop, there should be a negative
</p>
<p>relationship or correlation between price increases and sales decreases (see
</p>
<p>Chap. 5). Note that people often confuse correlation and causality. Just because
</p>
<p>there is some type of relationship between two variables does not mean that the one
</p>
<p>caused the other (see Box 2.1).
</p>
<p>18 2 The Market Research Process</p>
<p/>
</div>
<div class="page"><p/>
<p>Second, the cause needs to come before the effect. This is the time order&rsquo;s
</p>
<p>requirement. A price increase can obviously only have a causal effect on the sales if
</p>
<p>it occurred before the sales decrease.
</p>
<p>Third, we need to control for other factors. If we increase the price, sales may go
</p>
<p>up, because competitors increase their prices even more. Controlling for other
</p>
<p>factors is difficult, but not impossible. In experiments, we design studies so that
</p>
<p>external factors&rsquo; effect is nil, or as close to nil as possible. This is achieved by, for
</p>
<p>example, conducting experiments in labs where environmental factors, such as the
</p>
<p>conditions, are constant (controlled for). We can also use statistical tools that
</p>
<p>account for external influences to control for other factors. These statistical tools
</p>
<p>include an analysis of variance (see Chap. 6), regression analysis (see Chap. 7), and
</p>
<p>structural equation modeling (see end of Chap. 8).
</p>
<p>Fourth, the need for a good explanatory theory is an important criterion. Without
</p>
<p>a theory, our effects may be due to chance and no &ldquo;real&rdquo; effect may be present. For
</p>
<p>example, we may observe that when we advertise, sales decrease. Without a good
</p>
<p>explanation of this effect (such as people disliking the advertisement), we cannot
</p>
<p>claim that there is a causal relationship.
</p>
<p>Box 2.1 Correlation Does Not Automatically Imply Causation
</p>
<p>Correlation does not automatically imply causality. For example, Fig. 2.4
</p>
<p>plots US fatal motor vehicle crashes (per 100,000 people) against the
</p>
<p>harvested area of melons (in 1,000 acres) between 2000 and 2015.
</p>
<p>Clearly, the picture shows a trend. If the harvested area of melons
</p>
<p>increases, the number of US fatal motor vehicle crashes increases. The
</p>
<p>resulting correlation of 0.839 is very high (we discuss how to interpret
</p>
<p>correlations in Chap. 5). While this correlation is the first requirement to
</p>
<p>determine causality, the story falls short when it comes to explanatory theory.
</p>
<p>What possible mechanism could explain the findings? This is likely a case of
</p>
<p>a spurious correlation, which is simply due to coincidence.
</p>
<p>In the above situation, most people would be highly skeptical and would
</p>
<p>not interpret the correlation as describing a causal mechanism; in other
</p>
<p>instances, the situation is much less clear-cut. Think of claims that are part
</p>
<p>of everyday market research, such as &ldquo;the new advertisement campaign
</p>
<p>caused a sharp increase in sales&rdquo;, &ldquo;our company&rsquo;s sponsorship activities
</p>
<p>helped improve our company&rsquo;s reputation&rdquo;, or &ldquo;declining sales figures are
</p>
<p>caused by competitors&rsquo; aggressive price policies&rdquo;. Even if there is a correla-
</p>
<p>tion, the other requirements for causality may not be met. Causal research
</p>
<p>may help us determine if causality can be claimed.
</p>
<p>(continued)
</p>
<p>2.3 Determine the Research Design 19</p>
<p/>
</div>
<div class="page"><p/>
<p>Box 2.1 (continued)
</p>
<p>Some of these and other examples can be found in Huff (1993) or on
</p>
<p>Wikipedia. Furthermore, check http://www.tylervigen.com for more enter-
</p>
<p>taining examples of spurious correlations&mdash;see also Vigen (2015).
</p>
<p>http://en.wikipedia.org/wiki/Correlation_does_not_imply_causation
</p>
<p>Fig. 2.4 Correlation and causation (the data were taken from the NHTSA Traffic Safety
Facts, DOT HS 810780, and the United States Department of Agriculture, National
Agricultural Statistics Service)
</p>
<p>20 2 The Market Research Process</p>
<p/>
<div class="annotation"><a href="http://www.tylervigen.com">http://www.tylervigen.com</a></div>
<div class="annotation"><a href="http://en.wikipedia.org/wiki/Correlation_does_not_imply_causation">http://en.wikipedia.org/wiki/Correlation_does_not_imply_causation</a></div>
</div>
<div class="page"><p/>
<p>2.3.6 Uses of Causal Research
</p>
<p>Experiments are a key type of causal research and come in the form of either lab or
</p>
<p>field experiments.
</p>
<p>Lab experiments are performed in controlled environments (usually in a com-
</p>
<p>pany or academic lab) to isolate the effects of one or more variables on a certain
</p>
<p>outcome. To do so, researchers impose a treatment (e.g., a new advertisement) that
</p>
<p>induces changes in one variable (e.g., the type of advertising appeal) and evaluate
</p>
<p>its impact on an outcome variable (e.g., product choice). Field experiments are like
</p>
<p>lab experiments in that they examine the impact of one or more variables on a
</p>
<p>certain outcome. However, field experiments are conducted in real-life settings (not
</p>
<p>set up in controlled environments), thus reducing (or even eliminating) plausible
</p>
<p>claims of causality (Gneezy 2017). On the plus side, their realism makes them
</p>
<p>attractive for market research purposes, as the observed effects can probably be
</p>
<p>generalized to similar settings. For example, isi (https://www.isi-goettingen.de/en),
</p>
<p>a German sensory market research company, regularly runs product acceptance
</p>
<p>tests in which consumers sequentially evaluate different products, interrupted by
</p>
<p>short breaks to neutralize their senses. These tests are traditionally run in sensory
</p>
<p>labs under controlled conditions. However, isi also runs field experiments in actual
</p>
<p>consumption environments. Figure 2.5 shows a photo of a field experiment the
</p>
<p>company ran in a coffeehouse to evaluate consumer ratings of different cappuccino
</p>
<p>products. We discuss experimental set-ups in more detail in Chap. 4.
</p>
<p>Fig. 2.5 Field experiment
</p>
<p>2.3 Determine the Research Design 21</p>
<p/>
<div class="annotation"><a href="https://www.isi-goettingen.de/en">https://www.isi-goettingen.de/en</a></div>
</div>
<div class="page"><p/>
<p>Field experiments are not always a good idea. In the city of Rotterdam, the
</p>
<p>local council tried to reduce bike accidents by turning the traffic lights at bike
</p>
<p>crossings at a very busy intersection green at the same time. While the idea
</p>
<p>was that bicyclists would pay more attention, it took less than a minute for
</p>
<p>two accidents to happen. Needless to say, the experiment was cancelled (see
</p>
<p>https://www.youtube.com/watch?v&frac14;QIsLSmbfaiQ, in Dutch only).
</p>
<p>Test markets are a form of field experiment in which organizations in a
</p>
<p>geographically defined area introduce new products and services, or change the
</p>
<p>marketing mix to gauge consumer reactions. Test markets help marketers learn
</p>
<p>about consumer response, thus reducing the risks of a nationwide rollout of new
</p>
<p>products/services or changes in the marketing mix. For example, gps dataservice
</p>
<p>(http://www.gps-dataservice.de/en) runs several test markets for a series of major
</p>
<p>European retailers to evaluate the effect of treatments, such as new product
</p>
<p>launches, price changes, promotions, or product placements, on purchasing behav-
</p>
<p>ior. The company uses data from scanners, customer cards, and other sources (e.g.,
</p>
<p>surveys, observations) to investigate their effects on sales. For example, shelf tests
</p>
<p>involve placing dummy packages in the usual shelves in selected stores, and
</p>
<p>determining the reactions to these new packages by means of shopper observations
</p>
<p>(e.g., eye or physical contact with the product; Fig. 2.6), surveys, and scanner data.
</p>
<p>In Chap. 4, we discuss test markets in more depth.
</p>
<p>Fig. 2.6 Shelf test
</p>
<p>22 2 The Market Research Process</p>
<p/>
<div class="annotation"><a href="https://www.youtube.com/watch?v=QIsLSmbfaiQ">https://www.youtube.com/watch?v=QIsLSmbfaiQ</a></div>
<div class="annotation"><a href="https://www.youtube.com/watch?v=QIsLSmbfaiQ">https://www.youtube.com/watch?v=QIsLSmbfaiQ</a></div>
<div class="annotation"><a href="http://www.gps-dataservice.de/en">http://www.gps-dataservice.de/en</a></div>
</div>
<div class="page"><p/>
<p>2.4 Design the Sample and Method of Data Collection
</p>
<p>Having determined the research design, we need to design a sampling plan and
</p>
<p>choose a data-collecting method. This involves deciding whether to use existing
</p>
<p>(secondary) data or to conduct primary research. We discuss this in more detail in
</p>
<p>Chap. 3.
</p>
<p>2.5 Collect the Data
</p>
<p>Collecting data is a practical, but sometimes difficult, part of the market research
</p>
<p>process. How do we design a survey? How do we measure attitudes toward a
</p>
<p>product, brand, or company if we cannot observe these attitudes directly? How do
</p>
<p>we get CEOs to respond? Dealing with such issues requires careful planning and
</p>
<p>knowledge of the marketing process. We discuss related key issues in Chap. 4.
</p>
<p>2.6 Analyze the Data
</p>
<p>Analyzing data requires technical skills. We discuss how to enter, clean, and
</p>
<p>describe data in Chap. 5. After this, we introduce key techniques, such as hypothesis
</p>
<p>testing and analysis of variance (ANOVA), regression analysis, principal compo-
</p>
<p>nent, factor analysis, and cluster analysis in Chaps. 6, 7, 8 and 9. In each of these
</p>
<p>chapters, we discuss the key theoretical choices and issues that market researchers
</p>
<p>face when using these techniques. We also illustrate how researchers can practically
</p>
<p>deal with these theoretical choices and issues by means of Stata.
</p>
<p>2.7 Interpret, Discuss, and Present the Findings
</p>
<p>When executing the market research process, researchers&rsquo; immediate goals are
</p>
<p>interpreting, discussing, and presenting the findings. Consequently, researchers
</p>
<p>should provide detailed answers and actionable suggestions based on data and
</p>
<p>analysis techniques. The last step is to clearly communicate the findings and
</p>
<p>recommendations to help decision making and implementation. This is further
</p>
<p>discussed in Chap. 10.
</p>
<p>2.8 Follow-Up
</p>
<p>Market researchers often stop when the results have been interpreted, discussed,
</p>
<p>and presented. However, following up on the research findings is important too.
</p>
<p>Implementing market research findings sometimes requires further research,
</p>
<p>because suggestions or recommendations may not be feasible or practical and
</p>
<p>2.8 Follow-Up 23</p>
<p/>
</div>
<div class="page"><p/>
<p>market conditions may have changed. From a market research firm&rsquo;s perspective,
</p>
<p>follow-up research on previously conducted research can be a good way of entering
</p>
<p>new deals for further research. Some market research never ends, for example,
</p>
<p>many firms track customer satisfaction continuously, but even such research can
</p>
<p>have follow-ups, for example, because the management may wish to know the
</p>
<p>causes of drops in customer satisfaction.
</p>
<p>2.9 Review Questions
</p>
<p>1. What is market research? Try to explain what market research is in your own
</p>
<p>words.
</p>
<p>2. Why do we follow a structured process when conducting market research? Are
</p>
<p>there any shortcuts you can take? Compare, for example, Qualtrics&rsquo; market
</p>
<p>research process (http://www.qualtrics.com/blog/marketing-research-process)
</p>
<p>with the process discussed above. What are the similarities and differences?
</p>
<p>3. Describe what exploratory, descriptive, and causal research are and how they are
</p>
<p>related to one another. Provide an example of each type of research.
</p>
<p>4. What are the four requirements for claiming causality? Do we meet these
</p>
<p>requirements in the following situations?
</p>
<p>&ndash; Good user design led to Google&rsquo;s Android becoming the market leader.
</p>
<p>&ndash; When Rolex charges a higher price, this increases sales.
</p>
<p>&ndash; More advertising causes greater sales.
</p>
<p>2.10 Further Readings
</p>
<p>Levitt, S. D., &amp; Dubner, S. J. (2005). Freakonomics. A rogue economist explores
</p>
<p>the hidden side of everything. New York, NY: HarperCollins.
</p>
<p>An entertaining book that discusses statistical (mis)conceptions and introduces
</p>
<p>cases of people confusing correlation and causation.
</p>
<p>Levitt, S. D., &amp; Dubner, S. J. (2009). Superfreakonomics. New York, NY:
</p>
<p>HarperCollins.
</p>
<p>The follow-up book on Freakonomics. Also worth a read.
</p>
<p>Nielsen Retail Measurement at http://www.nielsen.com/us/en/nielsen-solutions/
</p>
<p>nielsen-measurement/nielsen-retail-measurement.html
</p>
<p>Pearl, J. (2009). Causality, Models, reasoning, and inference. New York, NY:
</p>
<p>Cambridge University Press.
</p>
<p>This book provides a comprehensive exposition of the modern analysis of causation.
</p>
<p>Strongly recommended for readers with a sound background in statistics.
</p>
<p>PRIZM by Claritas at http://www.claritas.com/MyBestSegments/Default.jsp?
</p>
<p>ID&frac14;20
</p>
<p>This website allows looking up US lifestyle segments at the zip level.
</p>
<p>24 2 The Market Research Process</p>
<p/>
<div class="annotation"><a href="http://www.qualtrics.com/blog/marketing-research-process">http://www.qualtrics.com/blog/marketing-research-process</a></div>
</div>
<div class="page"><p/>
<p>References
</p>
<p>Burrows, D. (2014). How to use ethnography for in-depth consumer insight.Marketing Week, May
9, 2014. https://www.marketingweek.com/2014/05/09/how-to-use-ethnography-for-in-depth-
consumer-insight/. Accessed 21 Aug 2017.
</p>
<p>Feiereisen, S., Wong, V., &amp; Broderick, A. J. (2008). Analogies and mental simulations in learning
for really new products: The role of visual attention. Journal of Product Innovation Manage-
ment, 25(6), 593&ndash;607.
</p>
<p>Gneezy, A. (2017). Field experimentation in marketing research. Journal of Marketing Research,
54(1), 140&ndash;143.
</p>
<p>Huff, D. (1993). How to lie with statistics. New York: W. W. Norton &amp; Company.
Larson, J. S., Bradlow, E. T., &amp; Fader, P. S. (2005). An exploratory look at supermarket shopping
</p>
<p>paths. International Journal of Research in Marketing, 22(4), 395&ndash;414. http://papers.ssrn.com/
sol3/papers.cfm?abstract_id=723821.
</p>
<p>Vigen, T. (2015). Spurious correlations. New York: Hachette Books.
</p>
<p>References 25</p>
<p/>
<div class="annotation"><a href="https://www.marketingweek.com/2014/05/09/how-to-use-ethnography-for-in-depth-consumer-insight/">https://www.marketingweek.com/2014/05/09/how-to-use-ethnography-for-in-depth-consumer-insight/</a></div>
<div class="annotation"><a href="https://www.marketingweek.com/2014/05/09/how-to-use-ethnography-for-in-depth-consumer-insight/">https://www.marketingweek.com/2014/05/09/how-to-use-ethnography-for-in-depth-consumer-insight/</a></div>
<div class="annotation"><a href="http://papers.ssrn.com/sol3/papers.cfm?abstract_id=723821">http://papers.ssrn.com/sol3/papers.cfm?abstract_id=723821</a></div>
<div class="annotation"><a href="http://papers.ssrn.com/sol3/papers.cfm?abstract_id=723821">http://papers.ssrn.com/sol3/papers.cfm?abstract_id=723821</a></div>
</div>
<div class="page"><p/>
<p>Data 3
</p>
<p>Keywords
</p>
<p>Armstrong and Overton procedure &bull; Case &bull; Census &bull; Constant &bull; Construct &bull;
</p>
<p>Construct validity &bull; Content validity &bull; Criterion validity &bull; Dependence of
</p>
<p>observations &bull; Discriminant validity &bull; Equidistance &bull; Face validity &bull; Formative
</p>
<p>constructs &bull; Index &bull; Index construction &bull; Internal consistency reliability &bull; Inter-
</p>
<p>rater reliability &bull; Items &bull; Latent concept &bull; Latent variable &bull; Measurement
</p>
<p>scaling &bull; Multi-item constructs &bull; Net Promoter Score (NPS) &bull; Nomological
</p>
<p>validity &bull; Non-probability sampling &bull; Observation &bull; Operationalization &bull;
</p>
<p>Population &bull; Predictive validity &bull; Primary data &bull; Probability sampling &bull;
</p>
<p>Qualitative data &bull; Quantitative data &bull; Reflective constructs &bull; Reliability &bull;
</p>
<p>Sample size &bull; Sampling &bull; Sampling error &bull; Scale development &bull; Secondary
</p>
<p>data &bull; Single-item constructs &bull; Test-retest reliability &bull; Unit of analysis &bull;
</p>
<p>Validity &bull; Variable
</p>
<p>Learning Objectives
</p>
<p>After reading this chapter, you should understand:
</p>
<p>&ndash; How to explain what kind of data you use.
</p>
<p>&ndash; The differences between primary and secondary data.
</p>
<p>&ndash; The differences between quantitative and qualitative data.
</p>
<p>&ndash; What the unit of analysis is.
</p>
<p>&ndash; When observations are independent and when dependent.
</p>
<p>&ndash; The difference between dependent and independent variables.
</p>
<p>&ndash; Different measurement scales and equidistance.
</p>
<p>&ndash; Validity and reliability from a conceptual viewpoint.
</p>
<p>&ndash; How to set up different sampling designs.
</p>
<p>&ndash; How to determine acceptable sample sizes.
</p>
<p># Springer Nature Singapore Pte Ltd. 2018
E. Mooi et al., Market Research, Springer Texts in Business and Economics,
DOI 10.1007/978-981-10-5218-7_3
</p>
<p>27</p>
<p/>
</div>
<div class="page"><p/>
<p>3.1 Introduction
</p>
<p>Data are at the heart of market research. By data we mean a collection of facts that
</p>
<p>can be used as a basis for analysis, reasoning, or discussions. Think, for example, of
</p>
<p>people&rsquo;s answers to surveys, existing company records, or observations of
</p>
<p>shoppers&rsquo; behaviors. &ldquo;Good&rdquo; data are vital, because they form the basis of useful
</p>
<p>market research. In this chapter, we discuss different types of data. This discussion
</p>
<p>will help you explain the data you use and why you do so. Subsequently, we
</p>
<p>introduce strategies for collecting data in Chap. 4.
</p>
<p>3.2 Types of Data
</p>
<p>Before we start discussing data, it is a good idea to introduce the terminology we
</p>
<p>will use. In the next sections, we will discuss the following four concepts:
</p>
<p>&ndash; variables,
</p>
<p>&ndash; constants,
</p>
<p>&ndash; cases, and
</p>
<p>&ndash; constructs.
</p>
<p>A variable is an attribute whose value can change. For example, the price of a
</p>
<p>product is an attribute of that product and generally varies over time. If the price
</p>
<p>does not change, it is a constant. A case (or observation) consists of all the
</p>
<p>variables that belong to an object such as a customer, a company, or a country.
</p>
<p>The relationship between variables and cases is that within one case we usually
</p>
<p>find multiple variables. Table 3.1 includes six variables: type of car bought and the
customer&rsquo;s age, as well as brand_1, brand_2, and brand_3, which capture
statements related to brand trust. In the lower rows, you see four observations.
</p>
<p>Another important and frequently used term in market research is construct,
</p>
<p>which refers to a variable that is not directly observable (i.e., a latent variable).
</p>
<p>More precisely, a construct is used to represent latent concepts in statistical
</p>
<p>analyses. Latent concepts represent broad ideas or thoughts about certain phenom-
</p>
<p>ena that researchers have established and want to measure in their research (e.g.,
</p>
<p>Bollen 2002). However, constructs cannot be measured directly, as respondents
</p>
<p>cannot articulate a single response that will completely and perfectly provide a
</p>
<p>measure of that concept. For example, constructs such as satisfaction, loyalty, and
</p>
<p>brand trust cannot be measured directly. However, we can measure satisfaction,
</p>
<p>loyalty, and brand trust by means of several items. The term items (or indicators) is
normally used to indicate posed survey questions. Measuring constructs requires
</p>
<p>combining items to form a multi-item scale, an example of which appears in
Table 3.1 in the form of three items Brand_1 (&ldquo;This brand&rsquo;s product claims are
believable&rdquo;), Brand_2 (&ldquo;This brand delivers what it promises&rdquo;), and Brand_3
(&ldquo;This brand has a name that you can trust&rdquo;). Bear in mind that not all items are
</p>
<p>28 3 Data</p>
<p/>
</div>
<div class="page"><p/>
<p>constructs, for example, the type of car bought and customer&rsquo;s age in Table 3.1 are
not a construct, as these are observable and a single response can measure it
</p>
<p>accurately and fully.
</p>
<p>Like constructs, an index also consists of sets of variables. The difference is that
</p>
<p>an index is created by the variable&rsquo;s &ldquo;causes.&rdquo; For example, we can create an index
</p>
<p>of information search activities, which is the sum of the information that customers
</p>
<p>require from dealers, the promotional materials, the Internet, and other sources.
</p>
<p>This measure of information search activities is also referred to as a composite
measure, but, unlike a construct, the items in an index define what we want to
measure. For example, the Retail Price Index consists of a &ldquo;shopping&rdquo; bag of
common retail products multiplied by their price. Unlike a construct, each item in
</p>
<p>a scale captures a part of the index perfectly.
</p>
<p>The procedure of combining several items is called scale development,
</p>
<p>operationalization, or, in the case of an index, index construction. These
</p>
<p>procedures involve a combination of theory and statistical analysis, such as factor
</p>
<p>analysis (discussed in Chap. 8) aimed at developing an appropriate construct
</p>
<p>measure. For example, in Table 3.1, Brand_1, Brand_2, and Brand_3 are items
that belong to a construct called brand trust (as defined by Erdem and Swait 2004).
The construct is not an individual item that you see in the list, but it is captured by
</p>
<p>calculating the average of several related items. Thus, in terms of brand trust, the
</p>
<p>score of customer 1 is (6 &thorn; 5 &thorn; 7)/3 &frac14; 6.
</p>
<p>But how do we decide which and how many items to use when measuring
</p>
<p>specific constructs? To answer these questions, market researchers make use of
</p>
<p>scale development procedures. These procedures follow an iterative process with
</p>
<p>several steps and feedback loops. For example, DeVellis (2017) provides a thor-
</p>
<p>ough introduction to scale development. Unfortunately, scale development requires
</p>
<p>much (technical) expertise. Describing each step is therefore beyond this book&rsquo;s
</p>
<p>scope. However, many scales do not require this procedure, as existing scales can
</p>
<p>Table 3.1 Quantitative data
</p>
<p>Variable
name
</p>
<p>Type of car
bought
</p>
<p>Customer&rsquo;s
Age Brand_1 Brand_2 Brand_3
</p>
<p>Description Name of car
bought
</p>
<p>Age in years This brand&rsquo;s
product claims
are believable
</p>
<p>This brand
delivers
what it
promises
</p>
<p>This brand
has a name
that you can
trust
</p>
<p>Customer 1 BMW 328i 29 6 5 7
</p>
<p>Customer 2 Mercedes
C180K
</p>
<p>45 6 6 6
</p>
<p>Customer 3 VW Passat
2.0 TFSI
</p>
<p>35 7 5 5
</p>
<p>Customer 4 BMW
525ix
</p>
<p>61 5 4 5
</p>
<p>Coding for Brand_1, Brand_2, and Brand_3: 1 &frac14; fully disagree, 7 &frac14; fully agree
</p>
<p>3.2 Types of Data 29</p>
<p/>
</div>
<div class="page"><p/>
<p>be found in scale handbooks, such as theHandbook of Marketing Scales by Bearden
et al. (2011). Furthermore, marketing and management journals frequently publish
</p>
<p>research articles that introduce new scales, such as for measuring the reputation of
</p>
<p>non-profit organizations (e.g., Sarstedt and Schloderer 2010) or for refining existing
</p>
<p>scales (e.g., Kuppelwieser and Sarstedt 2014). In Box 3.1, we introduce two
</p>
<p>distinctions that are often used to discuss constructs.
</p>
<p>Box 3.1 Types of Constructs
</p>
<p>In reflective constructs, the items are considered to be manifestations of an
</p>
<p>underlying construct (i.e., the items reflect the construct). Our brand trust
</p>
<p>example suggests a reflective construct, as the items reflect trust. Thus, if a
</p>
<p>respondent changes his assessment of brand trust (e.g., due to a negative
</p>
<p>brand experience), this is reflected in the answers to the three items. Reflec-
</p>
<p>tive constructs typically use multiple items (3 or more) to increase the
</p>
<p>measurement stability and accuracy. If we have multiple items, we can use
</p>
<p>analysis techniques to inform us about the measurement quality, such as
</p>
<p>factor analysis and reliability analysis (discussed in Chap. 8). Formative
</p>
<p>constructs consist of several items that define a construct. A typical example
</p>
<p>is socioeconomic status, which is formed by a combination of education,
</p>
<p>income, occupation, and residence. If any of these measures increases, the
</p>
<p>socioeconomic status would increase (even if the other items did not change).
</p>
<p>Conversely, if a person&rsquo;s socioeconomic status increases, this would not
</p>
<p>necessarily go hand in hand with an increase in all four measures. The
</p>
<p>distinction between reflective and formative constructs is that they require
</p>
<p>different approaches to decide on the type and number of items. For example,
</p>
<p>reliability analyses (discussed in Chap. 8) cannot be run on formative
</p>
<p>measures. For an overview of this distinction, see Bollen and
</p>
<p>Diamantopoulos (2017), Diamantopoulos et al. (2008), or Sarstedt et al.
</p>
<p>(2016c).
</p>
<p>Instead of using multiple items to measure constructs (i.e., multi-item
</p>
<p>constructs), researchers and practitioners frequently use single items (i.e.,
</p>
<p>single-item constructs). For example, we may only use &ldquo;This brand has a
</p>
<p>name that you can trust&rdquo; to measure brand trust, instead of using three items.
</p>
<p>A popular single-item measure is the Net Promoter Score (NPS), which
</p>
<p>aims to measure loyalty by using the single question: &ldquo;How likely are you to
recommend our company/product/service to a friend or colleague?&rdquo;
(Reichheld 2003). While this is a good way of making the questionnaire
</p>
<p>shorter, it also reduces the quality of your measures (e.g., Diamantopoulos
</p>
<p>et al. 2012, Sarstedt et al. 2016a, b). You should therefore avoid using single
</p>
<p>items to measure constructs unless you only need a rough proxy measure of a
</p>
<p>latent concept.
</p>
<p>30 3 Data</p>
<p/>
</div>
<div class="page"><p/>
<p>3.2.1 Primary and Secondary Data
</p>
<p>Generally, we can distinguish between primary data and secondary data. While
</p>
<p>primary data are data that a researcher has collected for a specific purpose, another
</p>
<p>researcher collected the secondary data for another purpose.
</p>
<p>The US Consumer Expenditure Survey (www.bls.gov/cex), which makes data
</p>
<p>available on what people in the US buy, such as insurance, personal care items, and
</p>
<p>food, is an example of secondary data. It also includes the prices people pay for
</p>
<p>these products and services. Since these data have already been collected, they are
</p>
<p>secondary data. If a researcher sends out a survey with various questions to find an
</p>
<p>answer to a specific issue, the collected data are primary data. If primary data are
</p>
<p>re-used to answer another research question, they become secondary data.
</p>
<p>Secondary and primary data have their own specific advantages and
</p>
<p>disadvantages, which we illustrate in Table 3.2. The most important reasons for
</p>
<p>using secondary data are that they tend to be cheaper and quick to obtain access to
</p>
<p>(although lengthy processes may be involved). For example, if you want to have
</p>
<p>access to the US Consumer Expenditure Survey, all you have to do is to use your
</p>
<p>web browser to go to www.bls.gov/cex and to download the required files. How-
</p>
<p>ever, the authority and competence of some research organizations could be a
</p>
<p>factor. For example, the claim that Europeans spend 9% of their annual income
</p>
<p>on health may be more believable if it comes from Eurostat (the statistical office of
</p>
<p>the European Community) rather than from a single, primary research survey.
</p>
<p>However, important secondary data drawbacks are that they may not answer
</p>
<p>your research question. If you are, for example, interested in the sales of a specific
</p>
<p>Table 3.2 The advantages and disadvantages of secondary and primary data
</p>
<p>Secondary data Primary data
</p>
<p>Advantages &ndash; Tends to be cheaper &ndash; Are recent
</p>
<p>&ndash; Sample sizes tend to be greater &ndash; Are specific for
the purpose
</p>
<p>&ndash; Tend to have more authority &ndash; Are proprietary
</p>
<p>&ndash; Are usually quick to access
</p>
<p>&ndash; Are easier to compare to other research using the same
data
</p>
<p>&ndash; Are sometimes more accurate (e.g., data on
competitors)
</p>
<p>Disadvantages &ndash; May be outdated &ndash; Are usually
more expensive
</p>
<p>&ndash; May not fully fit the problem &ndash; Take longer to
collect&ndash; There may be hidden errors in the data &ndash; difficult to
</p>
<p>assess the data quality
</p>
<p>&ndash; Usually contain only factual data
</p>
<p>&ndash; No control over data collection
</p>
<p>&ndash; May not be reported in the required form (e.g.,
different units of measurement, definitions, aggregation
levels of the data)
</p>
<p>3.2 Types of Data 31</p>
<p/>
<div class="annotation"><a href="http://www.bls.gov/cex">http://www.bls.gov/cex</a></div>
<div class="annotation"><a href="http://www.bls.gov/cex">http://www.bls.gov/cex</a></div>
</div>
<div class="page"><p/>
<p>product (and not in a product or service category), the US Expenditure Survey may
</p>
<p>not help much. In addition, if you are interested in the reasons for people buying
</p>
<p>products, this type of data may not help answer your question. Lastly, as you did not
</p>
<p>control the data collection, there may be errors in the data.
</p>
<p>In contrast, primary data tend to be highly specific, because the researcher (you!)
</p>
<p>can influence what the research comprises. In addition, research to gather primary
</p>
<p>data can be carried out when and where required and competitors cannot access
</p>
<p>it. However, gathering primary data often requires much time and effort and is
</p>
<p>therefore usually expensive compared to secondary data.
</p>
<p>As a rule, start looking for secondary data first. If they are available, and of
</p>
<p>acceptable quality, use them! We will discuss ways to gather primary and second-
</p>
<p>ary data in Chap. 4.
</p>
<p>3.2.2 Quantitative and Qualitative Data
</p>
<p>Data can be quantitative or qualitative. Quantitative data are presented in values,
</p>
<p>whereas qualitative data are not. Qualitative data can take many forms, such as
</p>
<p>words, stories, observations, pictures, and audio. The distinction between qualita-
</p>
<p>tive and quantitative data is not as black-and-white as it seems, because quantitative
</p>
<p>data are based on qualitative judgments. For example, the questions on brand trust
</p>
<p>in Table 3.1 take the values of 1&ndash;7. There is no reason why we could not have used
</p>
<p>other values to code these answers, such as 0&ndash;6, but it is common practice to code
</p>
<p>the answers to a construct&rsquo;s items on a range of 1&ndash;7.
</p>
<p>In addition, many of the sources that market researchers use, such as Twitter
</p>
<p>feeds or Facebook posts, produce qualitative data. Researchers can code attributes
</p>
<p>of the data, which describe a particular characteristic, thereby turning it into
</p>
<p>quantitative data. Think, for example, of how people respond to a new product in
</p>
<p>an interview. We can code the data by setting neutral responses to 0, somewhat
</p>
<p>positive responses to 1, positive responses to 2, and very positive responses to 3. We
</p>
<p>have therefore turned qualitative data into quantitative data. Box 3.2 shows an
</p>
<p>example of how to code qualitative data.
</p>
<p>Box 3.2 Coding Qualitative Data
</p>
<p>In 2016 Toyota launched new Prius, the Prius Prime (www.toyota.com/
</p>
<p>priusprime/). Not surprisingly, Facebook reactions were divided. Here are
</p>
<p>some examples of Facebook posts:
</p>
<p>&ndash; &ldquo;Love it! But why only 2 seats at the back? Is there any technical reason
</p>
<p>for that?&rdquo;
</p>
<p>&ndash; &ldquo;Wondering if leather seats are offered? The shape of the seats looks super
</p>
<p>comfy!&rdquo;
</p>
<p>(continued)
</p>
<p>32 3 Data</p>
<p/>
<div class="annotation"><a href="http://www.toyota.com/priusprime">http://www.toyota.com/priusprime</a></div>
<div class="annotation"><a href="http://www.toyota.com/priusprime">http://www.toyota.com/priusprime</a></div>
</div>
<div class="page"><p/>
<p>Box 3.2 (continued)
</p>
<p>&ndash; &ldquo;Here&rsquo;s that big black grill on yet another Toyota. Will be very glad when
</p>
<p>this &lsquo;fashion faze&rsquo; is over.&rdquo;
</p>
<p>One way of structuring these responses is to consider the attributes men-
</p>
<p>tioned in the posts. After reading them, you may find that, for example, the
</p>
<p>seat and styling are attributes. You can then categorize in respect of each post
</p>
<p>whether the response was negative, neutral, or positive. If you add the actual
</p>
<p>response, this can later help identify the aspect the posts liked (or disliked).
</p>
<p>As you can see, we have now turned qualitative data into quantitative data!
</p>
<p>Attribute Negative Neutral Positive
</p>
<p>Seats 1-why only two
seats?
</p>
<p>2-are leather seats
offered?
</p>
<p>2-shape looks super
comfy!
</p>
<p>Styling 3-big black grill 1-love it!
</p>
<p>Qualitative data&rsquo;s biggest strength is their richness, as they have the potential to
</p>
<p>offer detailed insights into respondents&rsquo; perceptions, attitudes, and intentions.
</p>
<p>However, their downside is that qualitative data can be interpreted in many ways.
</p>
<p>Thus, the process of interpreting qualitative data is subjective. To reduce subjectiv-
</p>
<p>ity, (multiple) trained researchers should code qualitative data. The distinction
</p>
<p>between quantitative and qualitative data is closely related to that between quanti-
</p>
<p>tative and qualitative research, which we discuss in Box 3.3. Most people think of
</p>
<p>quantitative data as being more factual and precise than qualitative data, but this is
</p>
<p>not necessarily true. Rather, how well qualitative data have been collected and/or
</p>
<p>coded into quantitative data is important.
</p>
<p>3.3 Unit of Analysis
</p>
<p>The unit of analysis is the level at which a variable is measured. Researchers often
</p>
<p>ignore this aspect, but it is crucial because it determines what we can learn from the
</p>
<p>data. Typical measurement levels include that of the respondents, customers, stores,
</p>
<p>companies, or countries. It is best to use data at the lowest possible level, because
</p>
<p>this provides more detail. If we need these data at another level, we can aggregate
</p>
<p>them. Aggregating datameans that we sum up a variable at a lower level to create a
variable at a higher level. For example, if we know how many cars all car dealers in
</p>
<p>a country sell, we can take the sum of all the dealer sales, to create a variable
</p>
<p>measuring countrywide car sales. Aggregation is not possible if we have incomplete
</p>
<p>or missing data at the lower levels.
</p>
<p>3.3 Unit of Analysis 33</p>
<p/>
</div>
<div class="page"><p/>
<p>Box 3.3 Quantitative Research and Qualitative Research
</p>
<p>Market researchers often label themselves as either quantitative or qualitative
</p>
<p>researchers. The two types of researchers use different methodologies, differ-
</p>
<p>ent types of data, and focus on different research questions. Most people
</p>
<p>regard the difference between qualitative and quantitative as the difference
</p>
<p>between numbers and words, with quantitative researchers focusing on num-
</p>
<p>bers and qualitative researchers on words. This distinction is not accurate, as
</p>
<p>many qualitative researchers use numbers in their analyses. The distinction
</p>
<p>should instead depend on when the information is quantified. If we know the
</p>
<p>values that may occur in the data even before the research starts, we conduct
</p>
<p>quantitative research. If we only know this after the data have been collected,
</p>
<p>we conduct qualitative research. Think of it in this way: If we ask survey
</p>
<p>questions and use a few closed questions, such as &ldquo;Is this product of good
</p>
<p>quality?,&rdquo; and the respondents can either choose &ldquo;Completely disagree,&rdquo;
</p>
<p>&ldquo;Somewhat disagree,&rdquo; &ldquo;Neutral,&rdquo; &ldquo;Somewhat agree,&rdquo; and &ldquo;Completely
</p>
<p>agree,&rdquo; we know that the data we will obtain from this will&mdash;at most&mdash;
</p>
<p>contain five different values. Because we know all possible values before-
</p>
<p>hand, the data are quantified beforehand. If, on the other hand, we ask
</p>
<p>someone &ldquo;Is this product of good quality?,&rdquo; he or she could give many
</p>
<p>different answers, such as &ldquo;Yes,&rdquo; &ldquo;No,&rdquo; &ldquo;Perhaps,&rdquo; &ldquo;Last time yes, but
</p>
<p>lately. . .&rdquo;. This means we have no idea what the possible answer values
</p>
<p>are. Therefore, these data are qualitative. We can, however, recode these
</p>
<p>qualitative data, for example, as described in Box 3.2, and assign values to
</p>
<p>each response. Thus, we quantify the data, allowing further statistical
</p>
<p>analysis.
</p>
<p>Qualitative research accounts for 17% of money spent in the market
</p>
<p>research industry, with quantitative research making up the rest.1 Practically,
</p>
<p>market research is often hard to categorize as qualitative or quantitative, as it
</p>
<p>may include elements of both. Research that includes both elements is
</p>
<p>sometimes called hybrid market research, fused market research, or simply
mixed methodology.
</p>
<p>3.4 Dependence of Observations
</p>
<p>A key issue for any data is the degree to which observations are related, or the
</p>
<p>dependence of observations. If we have exactly one observation from each
</p>
<p>individual, store, company, or country, we label the observations independent.
</p>
<p>1See ESOMAR Global Market Research Report 2013.
</p>
<p>34 3 Data</p>
<p/>
</div>
<div class="page"><p/>
<p>That is, the observations are unrelated. If we have multiple observations of each
</p>
<p>individual, store, company, or country, we label them dependent. For example, we
</p>
<p>could ask respondents to rate a type of Cola, then show them an advertisement, and
</p>
<p>again ask them to rate the same type of Cola. Although the advertisement may
</p>
<p>influence the respondents, it is likely that the first response and second response will
</p>
<p>be related. That is, if the respondents first rated the Cola negatively, the chance is
</p>
<p>higher that they will continue to rate the Cola negative rather than positive after the
</p>
<p>advertisement. If the observations are dependent, this often impacts the type of
</p>
<p>analysis we should use. For example, in Chap. 6, we discuss the difference between
</p>
<p>the independent samples t-test (for independent observations) and the paired
samples t-test (for dependent observations).
</p>
<p>3.5 Dependent and Independent Variables
</p>
<p>Dependent variables represent the outcome that market researchers study, while
independent variables are those used to explain the dependent variable(s). For
example, if we use the amount of advertising to explain sales, then advertising is
</p>
<p>the independent variable and sales the dependent.
</p>
<p>This distinction is artificial, as all variables depend on other variables. For
</p>
<p>example, the amount of advertising depends on how important the product is for
</p>
<p>a company, the company&rsquo;s strategy, and other factors. However, this distinction is
</p>
<p>frequently used in the application of statistical methods. While researching
</p>
<p>relationships between variables, we, on the basis of theory and practical
</p>
<p>considerations, need to distinguish between the dependent and the independent
</p>
<p>variables beforehand.
</p>
<p>3.6 Measurement Scaling
</p>
<p>Not all data are equal! For example, we can calculate the respondents&rsquo; average age
</p>
<p>in Table 3.1. However, if we would have coded the color of the car as black = 1,
</p>
<p>blue = 2, silver = 3 it would not make any sense to calculate the average. Why is
</p>
<p>this? The values that we have assigned 1, 2, and 3 are arbitrary; we could just as
</p>
<p>well have changed these value for any other. Therefore, choosing a different coding
</p>
<p>would lead to different results, which is meaningless.Measurement scaling refers
</p>
<p>to two things: the variables we use for measuring a certain construct (see discussion
</p>
<p>above) and the level at which a variable is measured, which we discuss in this
</p>
<p>section. This can be highly confusing!
</p>
<p>There are four levels of measurement:
</p>
<p>&ndash; nominal scale,
</p>
<p>&ndash; ordinal scale,
</p>
<p>&ndash; interval scale, and
</p>
<p>&ndash; ratio scale.
</p>
<p>3.6 Measurement Scaling 35</p>
<p/>
</div>
<div class="page"><p/>
<p>These scales relate to how we quantify what we measure. It is vital to know the
</p>
<p>scale on which something is measured, because, as the gender example above
</p>
<p>illustrates, the measurement scale determines the analysis techniques we can, or
</p>
<p>cannot, use. For example, as indicated above, it makes no sense to calculate. We
</p>
<p>will return to this issue in Chap. 5 and beyond. However, even when we know the
</p>
<p>scale, be aware that, as Fig. 3.1 shows, meaningful calculations are not always
</p>
<p>possible!
</p>
<p>The nominal scale is the most basic level at which we can measure something.
Essentially, if we use a nominal scale, we substitute a word for a numerical value.
</p>
<p>For example, we could code the color of each Prius sold: black &frac14; 1, blue &frac14; 2,
</p>
<p>silver &frac14; 3. In this example, the numerical values represent nothing more than a
</p>
<p>label.
</p>
<p>The ordinal scale provides more information. If a variable is measured on an
ordinal scale, increases or decreases in values give meaningful information. For
</p>
<p>example, if we code the Prius version people bought as the first generation &frac14; 1,
</p>
<p>second generation &frac14; 2, third generation &frac14; 3, and fourth generation &frac14; 4, we know
</p>
<p>Fig. 3.1 Meaningless!
</p>
<p>Table 3.3 Measurement
Scaling
</p>
<p>Label Order Differences Origin is 0
</p>
<p>Nominal scale ✓
</p>
<p>Ordinal scale ✓ ✓
</p>
<p>Interval scale ✓ ✓ ✓
</p>
<p>Ratio scale ✓ ✓ ✓ ✓
</p>
<p>36 3 Data</p>
<p/>
</div>
<div class="page"><p/>
<p>whether the model is more recent. The ordinal scale provides information about the
</p>
<p>order of our observations. However, we do not know if the differences in the order
</p>
<p>are equally spaced. That is, we do not know if the difference between first genera-
</p>
<p>tion and second generation is the same as between second and third generation,
</p>
<p>even though the difference in values (1&ndash;2 and 2&ndash;3) is equal.
</p>
<p>If something is measured on an interval scale, we have precise information on
the rank order at which something is measured and we can interpret the magnitude
</p>
<p>of the differences in values directly. For example, if the temperature in a car
</p>
<p>showroom is 23�C, we know that if it drops to 20�C, the difference is exactly
</p>
<p>3�C. This difference of 3�C is the same as the increase from 23 to 26�C. This exact
</p>
<p>&ldquo;spacing&rdquo; is called equidistance. Equidistant scales are necessary for some analysis
</p>
<p>techniques, such as factor analysis (discussed in Chap. 8). What the interval scale
</p>
<p>does not give us, is an absolute zero point. If the temperature is 0�C it may feel cold,
</p>
<p>but the temperature can drop further. The value of 0 does not therefore mean that
</p>
<p>there is no temperature at all.
</p>
<p>The ratio scale provides the most information. If something is measured on a
ratio scale, we know that a value of 0 means that that the attribute of that particular
</p>
<p>variable is not present. For example, if a dealer sells zero Prius cars (value&frac14; 0) then
</p>
<p>he or she really sells none. Or, if we spend no money on advertising a Prius
</p>
<p>(value &frac14; 0), we really spend no money. Therefore, the origin of the variable is
</p>
<p>equal to 0.
</p>
<p>While it is relatively easy to distinguish between the nominal and the interval
</p>
<p>scales, it is sometimes hard to see the difference between the interval and the ratio
</p>
<p>scales. The difference between the interval and the ratio scales can be ignored in
</p>
<p>most statistical methods. Table 3.3 shows the differences between these four scales.
</p>
<p>3.7 Validity and Reliability
</p>
<p>In any market research process, it is paramount to use &ldquo;good&rdquo; measures. Good
</p>
<p>measures are those that measure what they are supposed to measure and do so
</p>
<p>consistently. For example, if we are interested in knowing whether customers like a
</p>
<p>new TV commercial, we could show a commercial and ask the following two
</p>
<p>questions afterwards:
</p>
<p>1. &ldquo;Did you enjoy watching the commercial?,&rdquo; and
</p>
<p>2. &ldquo;Did the commercial provide the essential information required for a purchase
</p>
<p>decision?&rdquo;
</p>
<p>How do we know if these questions really measure whether or not the viewers
</p>
<p>liked the commercial? We can think of this as a measurement problem through
</p>
<p>which we relate what we want to measure&mdash;whether existing customers like a new
</p>
<p>TV commercial&mdash;with what we actually measure in terms of the questions we ask.
</p>
<p>If these relate perfectly, our actual measurement is equal to what we intend to
</p>
<p>3.7 Validity and Reliability 37</p>
<p/>
</div>
<div class="page"><p/>
<p>measure and we have no measurement error. If these do not relate perfectly, we
</p>
<p>have measurement error.
This measurement error can be divided into a systematic error and a random
</p>
<p>error. We can express this as follows, where XO stands for the observed score (i.e.,
what the customers indicated), XT for the true score (i.e., what the customers&rsquo; true
liking of the commercial is), ES for the systematic error, and ER for the random
error.
</p>
<p>XO &frac14; XT &thorn; ES &thorn; ER
</p>
<p>Systematic error is a measurement error through which we consistently measure
</p>
<p>higher, or lower, than we want to measure. If we were to ask customers, for
</p>
<p>example, to evaluate a TV commercial and offer them remuneration in return,
</p>
<p>they may provide more favorable information than they would otherwise have. This
</p>
<p>may cause us to think that the TV commercial is systematically more enjoyable than
</p>
<p>it is in reality. There may also be random errors. Some customers may be having a
</p>
<p>good day and indicate that they like a commercial, whereas others, who are having a
</p>
<p>bad day, may do the opposite.
</p>
<p>Systematic errors cause the actual measurement to be consistently higher, or
</p>
<p>lower, than what it should be. On the other hand, random error causes (random)
</p>
<p>variation between what we actually measure and what we want to measure.
</p>
<p>The systematic and random error concepts are important, because they relate to
</p>
<p>a measure&rsquo;s validity and reliability. Validity refers to whether we are measuring
</p>
<p>what we want to measure and, therefore, to a situation where the systematic error
</p>
<p>ES is small. Reliability is the degree to which what we measure is free from
random error and therefore relates to a situation where the ER is zero. In Fig. 3.2,
we illustrate the difference between reliability and validity by means of a target
</p>
<p>comparison. In this analogy, different measurements (e.g., of a customer&rsquo;s satis-
</p>
<p>faction with a specific service) are compared to arrows shot at a target. To
</p>
<p>measure each score, we have five measurements (indicated by the black circles),
</p>
<p>which correspond to, for example, questions asked in a survey. The cross
</p>
<p>indicates their average. Validity describes the cross&rsquo;s proximity to the bull&rsquo;s
</p>
<p>eye at the target center. The closer the average to the true score, the higher the
</p>
<p>validity. If several arrows are fired, reliability is the degree to which the arrows
</p>
<p>are apart. If all the arrows are close together, the measure is reliable, even though
</p>
<p>it is not necessarily near the bull&rsquo;s eye. This corresponds to the upper left box
</p>
<p>where we have a scenario in which the measure is reliable, but not valid. In the
</p>
<p>upper right box, both reliability and validity are given. In the lower left box,
</p>
<p>though, we have a situation in which the measure is neither reliable, nor valid.
</p>
<p>This is obviously because the repeated measurements are scattered around and the
</p>
<p>average does not match the true score. However, even if the latter were the case
</p>
<p>(i.e., if the cross were in the bull&rsquo;s eye), we would still not consider the measure
</p>
<p>valid. An unreliable measure can never be valid. If we repeated the measurement,
</p>
<p>say, five more times, the random error would probably shift the cross to a different
</p>
<p>position. Reliability is therefore a necessary condition for validity. This is also
</p>
<p>38 3 Data</p>
<p/>
</div>
<div class="page"><p/>
<p>why the scenario that is not reliable/valid (lower right box) is not included, as it is
</p>
<p>not possible for a measure to be valid, but not reliable.
</p>
<p>3.7.1 Types of Validity
</p>
<p>For some variables, such as length or income, we can objectively verify what the
</p>
<p>true score is. For constructs, such as satisfaction, loyalty and brand trust, this is
</p>
<p>impossible. From a philosophical point of view, one could even argue that there is
</p>
<p>no &ldquo;true&rdquo; score for a construct. So how do we know if a measure is valid? Because
</p>
<p>there is no objective way of verifying what we are measuring, several forms of
</p>
<p>validity have been developed, including face, content, predictive, criterion, dis-
</p>
<p>criminant, and nomological validity (Netemeyer et al. 2003). Researchers fre-
</p>
<p>quently summarize these validity types under the umbrella term construct
</p>
<p>validity, which relates to the correspondence between a measure at the conceptual
</p>
<p>level and a purported measure. The different types of validity help us understand the
</p>
<p>association between what we should measure and what we actually measure,
</p>
<p>thereby increasing the likelihood of adequately measuring the latent concept
</p>
<p>under consideration.
</p>
<p>Fig. 3.2 Validity and reliability
</p>
<p>3.7 Validity and Reliability 39</p>
<p/>
</div>
<div class="page"><p/>
<p>&ndash; Face validity is an absolute minimum requirement for a variable to be valid and
</p>
<p>refers to whether a variable reflects what you want to measure. Essentially, face
</p>
<p>validity exists if a measure seems to make sense. For example, if you want to
</p>
<p>measure trust, using items such as &ldquo;this company is honest and truthful&rdquo;makes a lot
</p>
<p>of sense,whereas &ldquo;this company is notwell known&rdquo;makes little sense. Researchers
</p>
<p>should agree on the face validity before starting the actual measurement. Face
</p>
<p>validity is usually determined by using a sample of experts who discuss and agree
</p>
<p>on the degree of face validity (this is also referred to as expert validity).
&ndash; Content validity is strongly related to face validity, but is more formalized. To
</p>
<p>assess content validity, researchers need to first define what they want to
</p>
<p>measure and discuss what is included in the definition and what not. For
</p>
<p>example, trust between businesses is often defined as the extent to which a
</p>
<p>firm believes that its exchange partner is honest and/or benevolent (Geyskens
</p>
<p>et al. 1998). This definition clearly indicates what should be mentioned in the
</p>
<p>questions used to measure trust (honesty and benevolence). After researchers
</p>
<p>have defined what they want to measure, questions have to be developed that
</p>
<p>relate closely to the definition. Consequently, content validity is mostly achieved
</p>
<p>prior to the actual measurement.
</p>
<p>&ndash; Predictive validity requires a measure to be highly correlated (see Chap. 5 for
</p>
<p>an introduction to correlations) with an outcome variable, measured at a later
</p>
<p>point in time, to which it is conceptually strongly related. For example, loyalty
</p>
<p>should lead to people purchasing a product in the future. Similarly, a measure of
</p>
<p>satisfaction should be predictive of people not complaining about a product or
</p>
<p>service. Assessing predictive validity requires collecting data at two points in
</p>
<p>time and therefore requires a greater effort. If both measures (i.e., the one to be
</p>
<p>evaluated and the outcome variable) are measured at the same point in time, we
</p>
<p>call this criterion validity.
</p>
<p>&ndash; Discriminant validity ensures that a measure is empirically unique and
</p>
<p>represents phenomena of interest that other measures in a model do not capture.
</p>
<p>For example, customer satisfaction and customer loyalty are two distinct latent
</p>
<p>concepts. Discriminant validity requires the constructs used to measure these two
</p>
<p>concepts to also be empirically distinct (i.e., they should not correlate too highly).
</p>
<p>&ndash; Nomological validity is the degree to which a construct behaves as it should in a
</p>
<p>system of related constructs. For example, customer expectations, perceived
</p>
<p>quality, and value have a significant influence on customer satisfaction. Simi-
</p>
<p>larly, satisfaction generally relates positively to customer loyalty. As such, you
</p>
<p>would expect the measure of satisfaction that you are evaluating to correlate with
</p>
<p>these measure.
</p>
<p>3.7.2 Types of Reliability
</p>
<p>How do we know if a measure is reliable? Three key factors are used to assess
</p>
<p>reliability: test-retest reliability, internal consistency reliability, and inter-rater
</p>
<p>reliability (Mitchell and Jolley 2013).
</p>
<p>40 3 Data</p>
<p/>
</div>
<div class="page"><p/>
<p>Test&ndash;retest reliability means that if we measure something twice (also called
</p>
<p>the stability of the measurement), we expect similar outcomes. The stability of
measurement requires a market researcher to have collected two data samples, is
</p>
<p>therefore costly, and could prolong the research process. Operationally, researchers
</p>
<p>administer the same test to the same sample on two different occasions and evaluate
</p>
<p>how strongly the measurements are correlated. We would expect the two
</p>
<p>measurements to correlate highly if a measure is reliable. This approach is not
</p>
<p>without problems, as it is often hard, if not impossible, to survey the same people
</p>
<p>twice. Furthermore, the respondents may learn from past surveys, leading to
</p>
<p>practice effects. In addition, it may be easier to recall events the second time a
survey is administered. Moreover, test&ndash;retest approaches do not work if a survey
</p>
<p>concentrates on specific time points. If we ask respondents to provide information
</p>
<p>on their last restaurant experience, the second test might relate to a different
</p>
<p>restaurant experience. Thus, test&ndash;retest reliability can only be assessed in terms
</p>
<p>of variables that are stable over time.
</p>
<p>Internal consistency reliability is by far the most common way of assessing
</p>
<p>reliability. Internal consistency reliability requires researchers to simultaneously
</p>
<p>use multiple items to measure the same concept. Think, for example, of the set of
</p>
<p>questions commonly used to measure brand trust (i.e., &ldquo;This brand&rsquo;s product claims
</p>
<p>are believable,&rdquo; &ldquo;This brand delivers what it promises,&rdquo; and &ldquo;This brand has a name
</p>
<p>that you can trust&rdquo;). If these items relate strongly, there is a considerable degree of
</p>
<p>internal consistency. There are several ways to calculate indices of internal consis-
</p>
<p>tency, including split-half reliability and Cronbach&rsquo;s α (pronounced as alpha),
</p>
<p>which we discuss in Chap. 8.
</p>
<p>Inter-rater reliability is used to assess the reliability of secondary data or
</p>
<p>qualitative data. If you want to identify, for example the most ethical organizations
</p>
<p>in an industry, you could ask several experts to provide a rating and then calculate
</p>
<p>the degree to which their answers relate.
</p>
<p>3.8 Population and Sampling
</p>
<p>A population is the group of units about which we want to make judgments. These
</p>
<p>units can be groups of individuals, customers, companies, products, or just about
</p>
<p>any subject in which you are interested. Populations can be defined very broadly,
</p>
<p>such as the people living in Canada, or very narrowly, such as the directors of large
</p>
<p>hospitals in Belgium. The research conducted and the research goal determine who
</p>
<p>or what the population will be.
</p>
<p>Sampling is the process through which we select cases from a population. The
</p>
<p>most important aspect of sampling is that the selected sample is representative of
</p>
<p>the population. Representative means that the characteristics of the sample closely
</p>
<p>match those of the population. In Box 3.4, we discuss how to determine whether a
</p>
<p>sample is representative of the population.
</p>
<p>3.8 Population and Sampling 41</p>
<p/>
</div>
<div class="page"><p/>
<p>Box 3.4 Do I Have a Representative Sample?
</p>
<p>It is important for market researchers that their sample is representative of the
</p>
<p>population. How can we determine whether this is so?
</p>
<p>&ndash; The best way to test whether the sample relates to the population is to use a
</p>
<p>database with information on the population (or to draw the sample from
</p>
<p>such databases). For example, the Amadeus database (www.bvdinfo.com)
</p>
<p>provides information on public and private companies around the world at
</p>
<p>the population level. We can (statistically) compare the information from
</p>
<p>these databases to the selected sample. However, this approach can only
</p>
<p>support the tested variables&rsquo; representativeness; that is, the specific repre-
sentativeness. Conversely, global representativeness&mdash;that is, matching
the distribution of all the characteristics of interest to the research ques-
</p>
<p>tion, but which lie outside their scope&mdash;cannot be achieved without a
</p>
<p>census (Kaplan 1964).
</p>
<p>&ndash; You can use (industry) experts to judge the quality of your sample. They
</p>
<p>may look at issues such as the type and proportion of organizations in your
</p>
<p>sample and population.
</p>
<p>&ndash; To check whether the responses of people included in your research differ
</p>
<p>significantly from those of non-respondents (which would lead to your
</p>
<p>sample not being representative), you can use the Armstrong and
</p>
<p>Overton procedure. This procedure calls for comparing the first 50% of
</p>
<p>respondents to the last 50% in respect of key demographic variables. The
</p>
<p>idea behind this procedure is that later respondents more closely match the
</p>
<p>characteristics of non-respondents. If these differences are not significant
</p>
<p>(e.g., through hypothesis tests, discussed in Chap. 6), we find some support
</p>
<p>for there being little, or no, response bias (see Armstrong and Overton
</p>
<p>1977). When the survey design includes multiple waves (e.g. the first wave
</p>
<p>of the survey is web-based and the second wave is by phone), this
</p>
<p>procedure is generally amended by comparing the last wave of
</p>
<p>respondents in a survey design to the earlier waves. There is some evi-
</p>
<p>dence that this procedure is better than Armstrong and Overton&rsquo;s original
</p>
<p>procedure (Lindner et al. 2001).
</p>
<p>&ndash; Using follow-up procedures, a small sample of randomly chosen
</p>
<p>non-respondents can again be contacted to request their cooperation.
</p>
<p>This small sample can be compared against the responses obtained earlier
</p>
<p>to test for differences.
</p>
<p>When we develop a sampling strategy, we have three key choices:
</p>
<p>&ndash; census,
</p>
<p>&ndash; probability sampling, and
</p>
<p>&ndash; non-probability sampling.
</p>
<p>42 3 Data</p>
<p/>
<div class="annotation"><a href="http://www.bvdinfo.com">http://www.bvdinfo.com</a></div>
</div>
<div class="page"><p/>
<p>Box 3.5 The US Census
</p>
<p>https://www.youtube.com/user/uscensusbureau
</p>
<p>If we are lucky and somehow manage to include every unit of the population in
</p>
<p>our study, we have conducted a census. Thus, strictly speaking, this is not a
</p>
<p>sampling strategy. Census studies are rare, because they are very costly and because
</p>
<p>missing just a small part of the population can have dramatic consequences. For
</p>
<p>example, if we were to conduct a census study of directors of Luxemburg banks, we
</p>
<p>may miss a few because they were too busy to participate. If these busy directors
</p>
<p>happen to be those of the very largest companies, any information we collect would
</p>
<p>underestimate the effects of variables that are more important at large banks.
</p>
<p>Census studies work best if the population is small, well-defined, and accessible.
</p>
<p>Sometimes census studies are also conducted for specific reasons. For example, the
</p>
<p>US Census Bureau is required to hold a census of all persons resident in the US
</p>
<p>every 10 years. Check out the US Census Bureau&rsquo;s YouTube channel using the
</p>
<p>mobile tag or URL in Box 3.5 to find out more about the US Census Bureau.
</p>
<p>If we select part of the population, we can distinguish two types of approaches:
</p>
<p>probability sampling and non-probability sampling. Figure 3.3 provides an over-
</p>
<p>view of the different sampling procedures, which we will discuss in the following
</p>
<p>sections.
</p>
<p>3.8.1 Probability Sampling
</p>
<p>Probability sampling approaches provide every individual in the population with a
</p>
<p>chance (not equal to zero) of being included in the sample (Cochran 1977, Levy and
</p>
<p>Lemeshow 2013). This is often achieved by using an accurate sampling frame,
which is a list of individuals in the population. There are various sampling frames,
</p>
<p>such as Dun &amp; Bradstreet&rsquo;s Selectory database (includes executives and
</p>
<p>3.8 Population and Sampling 43</p>
<p/>
<div class="annotation"><a href="https://www.youtube.com/user/uscensusbureau">https://www.youtube.com/user/uscensusbureau</a></div>
</div>
<div class="page"><p/>
<p>companies), the Mint databases (includes companies in North and South America,
</p>
<p>Italy, Korea, the Netherlands, and the UK), and telephone directories. These
</p>
<p>sampling frames rarely cover the population of interest completely and often
</p>
<p>include outdated information, but are frequently used due to their ease of use and
</p>
<p>availability. If the sampling frame and population are very similar, we have little
</p>
<p>sampling error. Starting with a good-quality sampling frame, we can use several
</p>
<p>methods to select units from it (Sarstedt et al. 2017).
</p>
<p>The easiest way is to use simple random sampling, which is achieved by
randomly selecting the number of cases required. This can be achieved by using
</p>
<p>specialized software, or using Stata.2
</p>
<p>Systematic sampling uses a different procedure. We first randomize the order of
all the observations, number them and, finally, select every nth observation. For
example, if our sampling frame consists of 1000 firms and we wish to select just
</p>
<p>100 firms, we could select the 1st observation, the 11th, the 21st, etc. until we reach
</p>
<p>the end of the sampling frame and have our 100 observations.
</p>
<p>Stratified sampling and cluster sampling are more elaborate techniques of
</p>
<p>probability sampling requiring us to divide the sampling frame into different
</p>
<p>groups. When we use stratified sampling, we divide the population into several
different homogenous groups called strata. These strata are based on key sample
</p>
<p>Probability sampling
</p>
<p>Sampling procedures
</p>
<p>Simple random sampling
</p>
<p>Systematic sampling
</p>
<p>Stratified sampling
</p>
<p>Cluster sampling
</p>
<p>Non-probability sampling
</p>
<p>Judgmental sampling
</p>
<p>Snowball sampling
</p>
<p>Quota sampling
</p>
<p>Other types of convenience
</p>
<p>sampling
</p>
<p>Fig. 3.3 Sampling procedures
</p>
<p>2See www.stata.com/support/faqs/statistics/random-samples for details. Stata will be discussed in
detail in Chap. 5 and beyond.
</p>
<p>44 3 Data</p>
<p/>
<div class="annotation"><a href="http://www.stata.com/support/faqs/statistics/random-samples">http://www.stata.com/support/faqs/statistics/random-samples</a></div>
</div>
<div class="page"><p/>
<p>characteristics, such as different departments in organizations, or the areas in which
</p>
<p>consumers live. Subsequently, we draw a random number of observations from
</p>
<p>each stratum. While stratified sampling is more complex and requires accurate
</p>
<p>knowledge of the sampling frame and population, it also helps ensure that the
</p>
<p>sampling frame&rsquo;s characteristics are similar to those of the sample.
</p>
<p>Cluster sampling requires dividing the population into different heterogeneous
groups, with each group&rsquo;s characteristics similar to those of the population. For
</p>
<p>example, we can divide a country&rsquo;s consumers into different provinces, counties, and
</p>
<p>councils. Several of these groups could have key characteristics (e.g., income, political
</p>
<p>preference, household composition) in common, which are very similar (representative
</p>
<p>of) to those of the population.We can select one or more of these representative groups
</p>
<p>and use random sampling to select observations that represent this group. This tech-
</p>
<p>nique requires knowledge of the sampling frame and population, but is convenient
</p>
<p>because gathering data from one group is cheaper and less time consuming.
</p>
<p>Generally, all probability sampling methods allow for drawing representative
</p>
<p>samples from the target population. However, simple random sampling and
</p>
<p>stratified sampling are considered superior in terms of drawing representative
</p>
<p>samples. For a detailed discussion, see Sarstedt et al. (2017).
</p>
<p>Stata features several advanced methods to deal with sampling. A few are
</p>
<p>discussed on http://www.ats.ucla.edu/stat/stata/library/svy_survey.htm. For
</p>
<p>detail, please see http://www.stata.com/manuals13/svy.pdf
</p>
<p>3.8.2 Non-probability Sampling
</p>
<p>Non-probability sampling procedures do not give every individual in the popula-
</p>
<p>tion an equal chance of being included in the sample (Cochran 1977, Levy and
</p>
<p>Lemeshow 2013). This is a drawback, because the resulting sample is most cer-
</p>
<p>tainly not representative of the population, which may bias the subsequent analyses&rsquo;
</p>
<p>results. Nevertheless, non-probability sampling procedures are frequently used as
</p>
<p>they are easily executed, and are normally less costly than probability sampling
</p>
<p>methods. Popular non-probability procedures include judgmental sampling, snow-
</p>
<p>ball sampling, and quota sampling (Sarstedt et al. 2017).
</p>
<p>Judgmental sampling is based on researchers taking an informed guess regarding
which individuals should be included. For example, research companies often have
</p>
<p>panels of respondents who are continuously used in research. Asking these people
</p>
<p>to participate in a new study may provide useful information if we know, from
</p>
<p>experience, that the panel has little sampling frame error.
</p>
<p>Snowball sampling involves existing study participants to recruit other
individuals from among their acquaintances. Snowball sampling is predominantly
</p>
<p>used if access to individuals is difficult. People such as directors, doctors, and high-
</p>
<p>level managers often have little time and are, consequently, difficult to involve. If
</p>
<p>we can ask just a few of them to provide the names and the details of others in a
</p>
<p>3.8 Population and Sampling 45</p>
<p/>
<div class="annotation"><a href="http://www.ats.ucla.edu/stat/stata/library/svy_survey.htm">http://www.ats.ucla.edu/stat/stata/library/svy_survey.htm</a></div>
<div class="annotation"><a href="http://www.stata.com/manuals13/svy.pdf">http://www.stata.com/manuals13/svy.pdf</a></div>
</div>
<div class="page"><p/>
<p>similar position, we can expand our sample quickly and then access them. Simi-
</p>
<p>larly, if you post a link to an online questionnaire on your LinkedIn or Facebook
</p>
<p>page (or send out a link via email) and ask your friends to share it with others, this is
</p>
<p>snowball sampling.
</p>
<p>Quota sampling occurs when we select observations for the sample that are
based on pre-specified characteristics, resulting in the total sample having the same
</p>
<p>distribution of characteristics assumed to exist in the population being studied. In
</p>
<p>other words, the researcher aims to represent the major characteristics of the
</p>
<p>population by sampling a proportional amount of each (which makes the approach
</p>
<p>similar to stratified sampling). Let&rsquo;s say, for example, that you want to obtain a
</p>
<p>quota sample of 100 people based on gender. First you need to find what proportion
</p>
<p>of the population is men and what women. If you find that the larger population is
</p>
<p>40% women and 60% men, you need a sample of 40 women and 60 men for a total
</p>
<p>of 100 respondents. You then start sampling and continue until you have reached
</p>
<p>exactly the same proportions and then stop. Consequently, if you already have
</p>
<p>40 women for the sample, but not yet 60 men, you continue to sample men and
</p>
<p>discard any female respondents that come along. However, since the selection of
</p>
<p>the observations does not occur randomly, this makes quota sampling a
</p>
<p>non-probability technique. That is, once the quota has been fulfilled for a certain
</p>
<p>characteristic (e.g., females), you no longer allow any observations with this
</p>
<p>specific characteristic in the sample. This systematic component of the sampling
</p>
<p>approach can introduce a sampling error. Nevertheless, quota sampling is very
</p>
<p>effective and inexpensive, making it the most important sampling procedure in
</p>
<p>practitioner market research.
</p>
<p>Convenience sampling is a catch-all term for methods (including the three
non-probability sampling techniques just described) in which the researcher
</p>
<p>draws a sample from that part of the population that is close at hand. For example,
</p>
<p>we can use mall intercepts to ask people in a shopping mall if they want to fill out a
survey. The researcher&rsquo;s control over who ends up in the sample is limited and
</p>
<p>influenced by situational factors.
</p>
<p>3.8.3 Probability or Non-probability Sampling?
</p>
<p>Probability sampling methods are recommended, as they result in representative
</p>
<p>samples. Nevertheless, judgmental and, especially, quota sampling might also lead
</p>
<p>to (specific) representativeness (e.g., Moser and Stuart 1953; Stephenson 1979).
</p>
<p>However, both methods&rsquo; ability to be representative depends strongly on the
</p>
<p>researcher&rsquo;s knowledge (Kukull and Ganguli 2012). Only when the researcher
</p>
<p>considers all the factors that have a significant bearing on the effect under study,
</p>
<p>will these methods lead to a representative sample. However, snowball sampling
</p>
<p>never leads to a representative sample, as the entire process depends on the
</p>
<p>participants&rsquo; referrals. Likewise, convenience sampling will almost never yield a
</p>
<p>representative sample, because observations are only selected if they can be
</p>
<p>accessed easily and conveniently. See Sarstedt et al. (2017) for further details.
</p>
<p>46 3 Data</p>
<p/>
</div>
<div class="page"><p/>
<p>3.9 Sample Sizes
</p>
<p>After determining the sampling procedure, we have to determine the sample size.
</p>
<p>Larger sample sizes increase the precision of the research, but are also much more
</p>
<p>expensive to collect. The gains in precision decrease as the sample size increases (inBox
</p>
<p>6.3 we discuss the question whether a sample size can be too large in the context of
</p>
<p>significance testing). It may seem surprising that relatively small sample sizes are
</p>
<p>precise, but the strength of samples comes from selecting samples accurately, rather
</p>
<p>their size. Furthermore, the required sample size has very little relation to the population
</p>
<p>size. That is, a sample of 100 employees from a company with 100,000 employees can
</p>
<p>be nearly as accurate as selecting 100 employees from a company with 1,000
</p>
<p>employees.
</p>
<p>There are some problems with selecting sample sizes. The first is that market
</p>
<p>research companies often push their clients to accept large sample sizes. Since the
</p>
<p>fee for market research services, such as those offered by Qualtrics or Toluna, is
</p>
<p>often directly dependent on the sample size, increasing the sample size increases the
</p>
<p>market research company&rsquo;s profit. Second, if we want to compare different groups,
</p>
<p>we need to multiply the required sample by the number of groups included. That is,
</p>
<p>if 150 observations are sufficient to measure how much people spend on organic
</p>
<p>food, 2 times 150 observations are necessary to compare singles and couples&rsquo;
</p>
<p>expenditure on organic food.
</p>
<p>The figures mentioned above are net sample sizes; that is, these are the actual
</p>
<p>(usable) number of observations we should have. Owing to non-response (discussed
</p>
<p>in Chaps. 4 and 5), a multiple of the initial sample size is normally necessary to
</p>
<p>obtain the desired sample size. Before collecting data, we should have an idea of the
</p>
<p>percentage of respondents we are likely to reach (often high), a percentage estimate
</p>
<p>of the respondents willing to help (often low), as well as a percentage estimate of
</p>
<p>the respondents likely to fill out the survey correctly (often high). For example, if
</p>
<p>we expect to reach 80% of the identifiable respondents, and if 25% are likely to
</p>
<p>help, and 75% of those who help are likely to fully fill out the questionnaire, only
</p>
<p>15% (0.80�0.25�0.75) of identifiable respondents are likely to provide a usable
</p>
<p>response. Thus, if we wish to obtain a net sample size of 100, we need to send
</p>
<p>out desired sample size
likely usable responses
</p>
<p>� �
</p>
<p>&frac14; 100/0.15 &frac14; 667 surveys. In Chap. 4, we will discuss how
</p>
<p>we can increase response rates (the percentage of people willing to help).
</p>
<p>3.10 Review Questions
</p>
<p>1. Explain the difference between items and constructs.
</p>
<p>2. What is the difference between reflective and formative constructs?
</p>
<p>3. Explain the difference between quantitative and qualitative data and give
</p>
<p>examples of each type.
</p>
<p>4. What is the scale on which the following variables are measured?
</p>
<p>&ndash; The amount of money a customer spends on shoes.
</p>
<p>3.10 Review Questions 47</p>
<p/>
</div>
<div class="page"><p/>
<p>&ndash; A product&rsquo;s country-of-origin.
</p>
<p>&ndash; The number of times an individual makes a complaint.
</p>
<p>&ndash; A test&rsquo;s grades.
</p>
<p>&ndash; The color of a mobile phone.
</p>
<p>5. Try to find two websites offering secondary data and discuss the kind of data
</p>
<p>described. Are these qualitative or quantitative data? What is the unit of
</p>
<p>analysis and how are the data measured?
</p>
<p>6. What are &ldquo;good data&rdquo;?
</p>
<p>7. Discuss concepts reliability and validity. How do they relate to each other?
</p>
<p>8. Please comment on the following statement: &ldquo;Face and content validity are
</p>
<p>essentially the same.&rdquo;
</p>
<p>9. What is the difference between predictive and criterion validity?
</p>
<p>10. Imagine you have just been asked to execute a market research study to
</p>
<p>estimate the market for notebooks priced $300 or less. What sampling approach
</p>
<p>would you propose to the client?
</p>
<p>11. Imagine that a university decides to evaluate their students&rsquo; satisfaction. To do
</p>
<p>so, employees issue every 10th student at the student cafeteria on one weekday
</p>
<p>with a questionnaire. Which type of sampling is conducted in this situation?
</p>
<p>Can the resulting sample be representative of the student population?
</p>
<p>3.11 Further Readings
</p>
<p>Mitchell, M. L., &amp; Jolley, J. M. (2013). Research design explained (8th ed.).
Belmont, CA: Wadsworth.
</p>
<p>The book offers an in-depth discussion of different types of reliability and validity,
including how to assess them.
</p>
<p>Churchill, G. A. (1979). A paradigm for developing better measures for marketing
</p>
<p>constructs. Journal of Marketing Research, 16(1), 64&ndash;73.
A landmark article that marked the start of the rethinking process on how to
</p>
<p>adequately measure constructs.
Cochran, W. G. (1977). Sampling techniques (3rd ed.). New York, NY: John Wiley
</p>
<p>and Sons.
</p>
<p>This is a seminal text on sampling techniques, providing a thorough introduction to
this topic. However, please note that most descriptions are rather technical and
require a sound understanding of statistics.
</p>
<p>Diamantopoulos A, Winklhofer HM (2001) Index construction with formative
</p>
<p>indicators: an alternative to scale development. Journal of Marketing Research,
38(2), 269&ndash;277.
</p>
<p>In this seminal article the authors provide guidelines on how to operationalize
formative constructs.
</p>
<p>DeVellis, R. F. (2017). Scale development: Theory and applications (4th ed.).
Thousand Oaks, CA: Sage.
</p>
<p>48 3 Data</p>
<p/>
</div>
<div class="page"><p/>
<p>This is a very accessible book which guides the reader through the classic way of
developing multi-item scales. The text does not discuss how to operationalize
formative constructs, though.
</p>
<p>Marketing Scales Database at www.marketingscales.com/search/search.php
</p>
<p>This website offers an easy-to-search database of marketing-related scales. A
description is given of every scale; the scale origin, reliability, and validity
are discussed and the items given.
</p>
<p>Netemeyer, R. G., Bearden, W. O., &amp; Sharma, S. (2003). Scaling procedures:
Issues and applications. Thousand Oaks, CA: Sage.
</p>
<p>Like DeVellis (2017), this book presents an excellent introduction to the principles
of the scale development of measurement in general.
</p>
<p>References
</p>
<p>Armstrong, J. S., &amp; Overton, T. S. (1977). Estimating nonresponse bias in mail surveys. Journal of
Marketing Research, 14(3), 396&ndash;403.
</p>
<p>Bearden, W. O., Netemeyer, R. G., &amp; Haws, K. L. (2011). Handbook of marketing scales. Multi-
item measures for marketing and consumer behavior research (3rd ed.). Thousand Oaks: Sage.
</p>
<p>Bollen, K. A. (2002). Latent variables in psychology and the social sciences. Annual Review of
Psychology, 53(1), 605&ndash;634.
</p>
<p>Bollen, K. A., &amp; Diamantopoulos, A. (2017). In defense of causal-formative indicators: A minority
report. Psychological Methods, 22(3), 581&ndash;596.
</p>
<p>Cochran, W. G. (1977). Sampling techniques (3rd ed.). New York: Wiley.
DeVellis, R. F. (2017). Scale development: Theory and applications (4th ed.). Thousand Oaks:
</p>
<p>Sage.
Diamantopoulos, A., Riefler, P., &amp; Roth, K. P. (2008). Advancing formative measurement models.
</p>
<p>Journal of Business Research, 61(12), 1203&ndash;1218.
Diamantopoulos, A., Sarstedt, M., Fuchs, C., Wilczynski, P., &amp; Kaiser, S. (2012). Guidelines for
</p>
<p>choosing between multi-item and single-item scales for construct measurement: A predictive
validity perspective. Journal of the Academy of Marketing Science, 40(3), 434&ndash;449.
</p>
<p>Erdem, T., &amp; Swait, J. (2004). Brand credibility, brand consideration, and choice. Journal of
Consumer Research, 31(1), 191&ndash;198.
</p>
<p>Geyskens, I., Steenkamp, J.-B. E. M., &amp; Kumar, N. (1998). Generalizations about trust in
marketing channel relationships using meta-analysis. International Journal of Research in
Marketing, 15(3), 223&ndash;248.
</p>
<p>Kaplan, A. (1964). The conduct of inquiry. San Francisco: Chandler.
Kukull, W. A., &amp; Ganguli, M. (2012). Generalizability. The trees, the forest, and the low-hanging
</p>
<p>fruit. Neurology, 78(23), 1886&ndash;1891.
Kuppelwieser, V., &amp; Sarstedt, M. (2014). Confusion about the dimensionality and measurement
</p>
<p>specification of the future time perspective scale. International Journal of Advertising, 33(1),
113&ndash;136.
</p>
<p>Levy, P. S., &amp; Lemeshow, S. (2013). Sampling of populations: Methods and applications (5th ed.).
Hoboken: Wiley.
</p>
<p>Lindner, J. R., Murphy, T. H., &amp; Briers, G. E. (2001). Handling nonresponse in social science
research. Journal of Agricultural Education, 42(4), 43&ndash;53.
</p>
<p>Mitchell, M. L., &amp; Jolley, J. M. (2013). Research design explained (8th ed.). Belmont: Wadsworth.
Moser, C. A., &amp; Stuart, A. (1953). An experimental study of quota sampling. Journal of the Royal
</p>
<p>Statistical Society. Series A (General), 116(4), 349&ndash;405.
Netemeyer, R. G., Bearden, W. O., &amp; Sharma, S. (2003). Scaling procedures: Issues and
</p>
<p>applications. Thousand Oaks: Sage.
</p>
<p>References 49</p>
<p/>
<div class="annotation"><a href="http://www.marketingscales.com/search/search.php">http://www.marketingscales.com/search/search.php</a></div>
</div>
<div class="page"><p/>
<p>Reichheld, F. F. (2003). The one number you need to grow. Harvard Business Review, 81(12),
46&ndash;55.
</p>
<p>Sarstedt, M., &amp; Schloderer, M. P. (2010). Developing a measurement approach for reputation of
nonprofit organizations. International Journal of Nonprofit and Voluntary Sector Marketing,
15(3), 276&ndash;299.
</p>
<p>Sarstedt, M., Diamantopoulos, A., Salzberger, T., &amp; Baumgartner, P. (2016a). Selecting single
items to measure doubly-concrete constructs: A cautionary tale. Journal of Business Research,
69(8), 3159&ndash;3167.
</p>
<p>Sarstedt, M., Diamantopoulos, A., &amp; Salzberger, T. (2016b). Should we use single items? Better
not. Journal of Business Research, 69(8), 3199&ndash;3203.
</p>
<p>Sarstedt, M., Hair, J. F., Ringle, C. M., Thiele, K. O., &amp; Gudergan, S. P. (2016c). Estimation issues
with PLS and CBSEM: Where the bias lies! Journal of Business Research, 69(10), 3998&ndash;4010.
</p>
<p>Sarstedt, M., Bengart, P., Shaltoni, A. M., &amp; Lehmann, S. (2017, forthcoming). The use of
sampling methods in advertising research: A gap between theory and practice. International
Journal of Advertising.
</p>
<p>Stephenson, C. B. (1979). Probability sampling with quotas: An experiment. Public Opinion
Quarterly, 43(4), 477&ndash;496.
</p>
<p>50 3 Data</p>
<p/>
</div>
<div class="page"><p/>
<p>Getting Data 4
</p>
<p>Keywords
</p>
<p>Back-translation &bull; Balanced scale &bull; Big data &bull; Closed-ended questions &bull; Constant
</p>
<p>sum scale &bull; Customer relationship management &bull; Double-barreled questions &bull;
</p>
<p>Equidistant scale &bull; Ethnography &bull; Experiments &bull; Experimental design &bull; External
</p>
<p>secondary data &bull; External validity &bull; Face-to-face interviews &bull; Focus groups &bull;
</p>
<p>Forced-choice scale &bull; Free-choice scale &bull; In-depth interviews &bull; Internal secondary
</p>
<p>data &bull; Internal validity &bull; Laddering &bull; Likert scale &bull; Mail surveys &bull; Manipulation
</p>
<p>checks &bull; Means-end approach &bull; Mixed mode &bull; Mystery shopping &bull; Observational
</p>
<p>studies &bull; Open-ended questions &bull; Personal interviews &bull; Primary data &bull; Projective
</p>
<p>techniques &bull; Qualitative research &bull; Rank order scales &bull; Reverse-scaled items &bull;
</p>
<p>Secondary data &bull; Semantic differential scales &bull; Sentence completion &bull; Social
</p>
<p>desirability bias &bull; Social media analytics &bull; Social networking data &bull; Surveys &bull;
</p>
<p>Syndicated data &bull; Telephone interviews &bull; Test markets &bull; Treatments &bull;
</p>
<p>Unbalanced scale &bull; Visual analogue scale &bull; Web surveys &bull; Verbatim items
</p>
<p>Learning Objectives
</p>
<p>After reading this chapter, you should understand:
</p>
<p>&ndash; How to find secondary data and decide on their suitability.
</p>
<p>&ndash; How to collect primary data.
</p>
<p>&ndash; How to design a basic questionnaire.
</p>
<p>&ndash; How to set up basic experiments.
</p>
<p>&ndash; How to set up basic qualitative research.
</p>
<p>4.1 Introduction
</p>
<p>In the previous chapter, we discussed some of the key theoretical concepts and
</p>
<p>choices associated with collecting data. These concepts and choices included
</p>
<p>validity, reliability, sampling, and sample sizes. We also discussed different types
</p>
<p># Springer Nature Singapore Pte Ltd. 2018
E. Mooi et al., Market Research, Springer Texts in Business and Economics,
DOI 10.1007/978-981-10-5218-7_4
</p>
<p>51</p>
<p/>
</div>
<div class="page"><p/>
<p>of data. Building on Chap. 3, this chapter discusses the practicalities of collecting
</p>
<p>data. First, we discuss how to work with secondary data. Before collecting primary
</p>
<p>data, market researchers should always consider secondary data, which are often
</p>
<p>available and do not depend on respondents&rsquo; willingness to participate. Although
</p>
<p>secondary data have already been collected, you usually need to spend considerable
</p>
<p>effort preparing them for analysis, which we discuss first. If you find that the
</p>
<p>required secondary data are unavailable, outdated, or very costly, you may have
</p>
<p>to collect primary data. In the sections that follow, we discuss how to collect
</p>
<p>primary data through observations, surveys, and experiments. In Fig. 4.1, we
</p>
<p>provide an overview of some types of secondary and primary data.
</p>
<p>4.2 Secondary Data
</p>
<p>Secondary data are data that have already been gathered, often for a different
</p>
<p>research purpose and some time ago. Secondary data comprise internal secondary
</p>
<p>data, external secondary data, or a mix of both.
</p>
<p>Data collection procedures
</p>
<p>Primary
</p>
<p>AskObserve
</p>
<p>Secondary
</p>
<p>ExternalInternal
</p>
<p>Company 
</p>
<p>records
</p>
<p>Sales reports
</p>
<p>Existing 
</p>
<p>research studies
</p>
<p>Governments
</p>
<p>Trade 
</p>
<p>associations
</p>
<p>Market 
</p>
<p>research firms
</p>
<p>Consulting firms
</p>
<p>(Literature) 
</p>
<p>databases
</p>
<p>Internet &amp;
</p>
<p>social networks 
</p>
<p>Focus groups
</p>
<p>In-depth
</p>
<p>interviews
Test markets
</p>
<p>Projective
</p>
<p>techniques
</p>
<p>Experiments 
</p>
<p>(mix of observe and ask)
</p>
<p>Observational
</p>
<p>studies
Surveys
</p>
<p>Fig. 4.1 Types of primary and secondary data sources
</p>
<p>52 4 Getting Data</p>
<p/>
</div>
<div class="page"><p/>
<p>4.2.1 Internal Secondary Data
</p>
<p>Internal secondary data are data that companies compiled for various reporting
</p>
<p>and analysis purposes. Much of these data have been collected and stored because
</p>
<p>&ldquo;you can&rsquo;t manage what you don&rsquo;t measure.&rdquo;1 Large companies have systems in
</p>
<p>place, such as Enterprise Resource Planning systems (usually abbreviated as ERP
</p>
<p>systems), in which vast amounts of customer, transaction, and performance data are
</p>
<p>stored. In general, internal secondary data comprise the following:
</p>
<p>&ndash; company records,
</p>
<p>&ndash; sales reports, and
</p>
<p>&ndash; existing research studies.
</p>
<p>Company records are a firm&rsquo;s repository of information. They may contain data
</p>
<p>from different business functions, such as finance, or Customer Relationship
</p>
<p>Management (CRM). The finance function may provide internal reviews of an
</p>
<p>organization&rsquo;s financial well-being and strategic advice, as it has access to the
</p>
<p>organization&rsquo;s financial and operational data. The term CRM refers to a system of
</p>
<p>databases and analysis software that tracks and predicts customer behavior. Firms
</p>
<p>such as IBM, Microsoft, and Oracle market the database systems that the analysis
</p>
<p>software utilizes. These database management systems often include information
</p>
<p>on, for example, purchasing behavior, (geo-)demographic customer data, and the
</p>
<p>after-sales service. This information is compiled to allow marketers to track indi-
</p>
<p>vidual customers over different sales channels and types of products in order to
</p>
<p>tailor their offerings to these customers&rsquo; needs. Several information technology
</p>
<p>companies, such as SAP, Oracle, and Salesforce.com sell the analysis software that
</p>
<p>utilizes these databases. Companies use this software to, for example, identify
</p>
<p>customer trends, calculate their profitability per customer, or identify opportunities
</p>
<p>to sell new or different products. The CRM market is substantial, generating about
</p>
<p>$37 billion in 2017.2
</p>
<p>Sales reports are created when products and services are sold to business-to-
</p>
<p>business clients. Many of these reports detail discussions held with clients, as well
</p>
<p>as the products and services sold. The reports therefore provide insights into
</p>
<p>customers&rsquo; needs. Sales reports are also a means of retaining customers&rsquo;
</p>
<p>suggestions regarding products and services, and can be a productive source of
</p>
<p>information. For example, DeMonaco et al. (2005) found that 59% of existing drugs
</p>
<p>had uses other than those that the producing company described. Because it is
</p>
<p>important to be aware of a drug&rsquo;s uses, sales discussions with doctors, hospitals, and
</p>
<p>research institutes can help this company market these drugs. When sales reports
</p>
<p>are available, they are often part of a CRM system.
</p>
<p>1This quote has been attributed to Peter F. Drucker.
2www.gartner.com/DisplayDocument?id&frac14;2515815&amp;ref&frac14;g_sitelink
</p>
<p>4.2 Secondary Data 53</p>
<p/>
<div class="annotation"><a href="https://doi.org/salesforce.com">https://doi.org/salesforce.com</a></div>
<div class="annotation"><a href="http://www.gartner.com/DisplayDocument?id=2515815&amp;ref=g_sitelink">http://www.gartner.com/DisplayDocument?id=2515815&amp;ref=g_sitelink</a></div>
<div class="annotation"><a href="http://www.gartner.com/DisplayDocument?id=2515815&amp;ref=g_sitelink">http://www.gartner.com/DisplayDocument?id=2515815&amp;ref=g_sitelink</a></div>
<div class="annotation"><a href="http://www.gartner.com/DisplayDocument?id=2515815&amp;ref=g_sitelink">http://www.gartner.com/DisplayDocument?id=2515815&amp;ref=g_sitelink</a></div>
</div>
<div class="page"><p/>
<p>Existing research studies are a good source of secondary data. You should,
</p>
<p>however, carefully consider whether existing research studies are still useful and
</p>
<p>what you can learn from them. Even if you believe their findings are outdated, their
</p>
<p>measures may be very useful. Consequently, if you wish to use existing research
</p>
<p>studies, it is important that you ascertain that enough of their details are available to
</p>
<p>make them useful.
</p>
<p>4.2.2 External Secondary Data
</p>
<p>External secondary data have been compiled outside a company for a variety of
</p>
<p>purposes. Important sources of secondary data, which we discuss next, include:
</p>
<p>&ndash; governments,
</p>
<p>&ndash; trade associations,
</p>
<p>&ndash; market research firms,
</p>
<p>&ndash; consulting firms,
</p>
<p>&ndash; (literature) databases, and
</p>
<p>&ndash; internet &amp; social networks.
</p>
<p>Governments often provide data that can be used for market research purposes.
</p>
<p>For example, The CIA World Fact Book provides information on the economy,
</p>
<p>politics, and other issues of nearly every country in the world. Eurostat (the
</p>
<p>statistics office of the European Union) provides detailed information on the
</p>
<p>economy and different market sectors of the European Union. Much of this
</p>
<p>information is free of charge and is an easy starting point for market research
</p>
<p>studies.
</p>
<p>Trade associations are organizations representing different companies whose
</p>
<p>purpose is to promote their common interests. For example, the Auto Alliance&mdash;
</p>
<p>which consists of US automakers&mdash;provides information on the sector and lists the
</p>
<p>key issues it faces. The European Federation of Pharmaceutical Industries and
</p>
<p>Associations represents 1900 pharmaceutical companies operating in Europe. The
</p>
<p>federation provides a detailed list of key figures and facts, and regularly offers
</p>
<p>statistics on the industry. Most of the other trade associations also provide lists of
</p>
<p>their members&rsquo; names and addresses. These can be used, for example, as a sampling
</p>
<p>frame (see Chap. 3). Most trade associations regard ascertaining their members&rsquo;
</p>
<p>opinions a key task and therefore collect data regularly. These data are often
</p>
<p>included in reports that researchers can download from the organization&rsquo;s website.
</p>
<p>Such reports can be a short-cut to identifying key issues and challenges in specific
</p>
<p>industries. Sometimes, these reports are free of charge, but non-members usually
</p>
<p>need to pay a (mostly substantial) fee.
</p>
<p>Market research firms are another source of secondary data. Especially large
</p>
<p>market research firms provide syndicated data that different clients can use (see
</p>
<p>Chap. 1). Syndicated data are standardized, processed information made available
</p>
<p>to multiple (potential) clients, usually for a substantial fee. Syndicated data often
</p>
<p>54 4 Getting Data</p>
<p/>
</div>
<div class="page"><p/>
<p>allow the client&rsquo;s key measures (such as satisfaction or market share) to be
</p>
<p>compared against the rest of the market. Examples of syndicated data include the
</p>
<p>J.D. Power Initial Quality Study, which provides insights into the initial quality of
</p>
<p>cars in the US, and the J.D. Power Vehicle Ownership Satisfaction Study, which
</p>
<p>contains similar data on other markets, such as New Zealand and Germany. GfK
</p>
<p>Spex Retail, which we introduce in Box 4.1, is another important example of
</p>
<p>syndicated data.
</p>
<p>Consulting firms are a rich source of secondary data. Most firms publish full
</p>
<p>reports or summaries of reports on their website. For example, McKinsey &amp;
</p>
<p>Company publish the McKinsey Quarterly, a business journal that includes articles
</p>
<p>on current business trends, issues, problems, and solutions. Oliver Wyman
</p>
<p>publishes regular reports on trends and issues across many different industries.
</p>
<p>Other consulting firms, such as Gartner and Forrester, provide data on various
</p>
<p>topics. These data can be purchased and used for secondary analysis. For example,
</p>
<p>Forrester maintains databases on market segmentation, the allocation of budgets
</p>
<p>across firms, and the degree to which consumers adopt various innovations. Con-
</p>
<p>sulting firms provide general advice, information, and knowledge, while market
</p>
<p>research firms only focus on marketing-related applications. In practice, there is
</p>
<p>some overlap in the activities that consulting and market research firms undertake.
</p>
<p>(Literature) databases comprise professional and academic journals,
</p>
<p>newspapers, and books. Two important literature databases are ProQuest (www.
</p>
<p>proquest.com) and JSTOR (www.jstor.org). ProQuest contains over 9,000 trade
</p>
<p>journals, business publications, and leading academic journals, including highly
</p>
<p>regarded publications such as the Journal of Marketing and the Journal of Market-
</p>
<p>ing Research. A subscription is needed to gain access, although some papers are
</p>
<p>published as open access. Academic institutions often allow their students and,
</p>
<p>sometimes, their alumni to access these journals. JSTOR is like ProQuest, but is
</p>
<p>mostly aimed at academics. Consequently, it provides access to nearly all leading
</p>
<p>Box 4.1 GfK Spex Retail
</p>
<p>GfK is a large market research company. One of its databases, Spex Retail,
</p>
<p>provides resellers, distributors, manufacturers, and website portals with prod-
</p>
<p>uct data. In 20 languages, it offers aggregated data on more than seven million
</p>
<p>products from 20,000 manufacturers in 30 countries. This database provides
</p>
<p>details on IT, consumer electronics, household appliances, and other
</p>
<p>products. Spex Retail also provides insight into new products being launched
</p>
<p>by providing 70,000 new information sheets that describe new products or
</p>
<p>product changes every month. The data can also be used to map product
</p>
<p>categories. Such information helps its clients understand the market structure,
</p>
<p>or identify cross-selling or up-selling possibilities. See www.etilize.com/
</p>
<p>spex-plus-product-data.htm for more details, including a demo of its
</p>
<p>products.
</p>
<p>4.2 Secondary Data 55</p>
<p/>
<div class="annotation"><a href="https://doi.org/www.proquest.com">https://doi.org/www.proquest.com</a></div>
<div class="annotation"><a href="https://doi.org/www.proquest.com">https://doi.org/www.proquest.com</a></div>
<div class="annotation"><a href="https://doi.org/www.jstor.org">https://doi.org/www.jstor.org</a></div>
<div class="annotation"><a href="http://www.etilize.com/spex-plus-product-data.htm">http://www.etilize.com/spex-plus-product-data.htm</a></div>
<div class="annotation"><a href="http://www.etilize.com/spex-plus-product-data.htm">http://www.etilize.com/spex-plus-product-data.htm</a></div>
</div>
<div class="page"><p/>
<p>academic journals. A helpful feature of JSTOR is that the first page of academic
</p>
<p>articles (which contains the abstract) can be read free of charge. In addition,
</p>
<p>JSTOR&rsquo;s information is searchable via Google Scholar (discussed in Box 4.2).
</p>
<p>Certain database firms provide firm-level data, such as names and addresses. For
</p>
<p>example, Bureau van Dijk (www.bvdep.com), as well as Dun and Bradstreet (www.
</p>
<p>dnb.com), publish extensive lists of firm names, the industry in which they operate,
</p>
<p>their profitability, key activities, and address information. This information is often
</p>
<p>used as a sampling frame for surveys.
</p>
<p>Internet data is a catch-all term that refers to data stored to track peoples&rsquo;
</p>
<p>behavior on the Internet. Such data consist of page requests and sessions. A page
</p>
<p>request refers to people clicking on a link or entering a specific Internet address. A
</p>
<p>session is a series of these requests and is often identified by the IP number, a
</p>
<p>specific address that uniquely identifies the receiver for a period of time, or by
</p>
<p>means of a tracking cookie. With this information, researchers can calculate when
</p>
<p>and why people move from one page to another. The conversion rate is a specific
</p>
<p>type of information, namely the ratio of the number of purchases made on a website
</p>
<p>relative to the number of unique visitors, which often interests researchers.
</p>
<p>Facebook, Instagram, and LinkedIn, provide valuable information in the form of
</p>
<p>social networking profiles, which include personal details and information. These
</p>
<p>social networking data reflect how people would like others to perceive them and,
</p>
<p>thus, indicate consumers&rsquo; intentions. Product or company-related user groups are of
</p>
<p>specific interest to market researchers. Take, for example, comments posted on a
</p>
<p>Facebook group site such as that of BMW or Heineken. An analysis of the postings
</p>
<p>helps provide an understanding of how people perceive these brands.
</p>
<p>Interpretations of such postings usually include analyzing five elements: the agent
</p>
<p>(who is posting?), the act (what happened, i.e., what aspect does the posting refer
</p>
<p>to?), the agency (what media is used to perform the action?), the scene (what is the
</p>
<p>background situation?), and the purpose (why do the agents act?). By analyzing this
</p>
<p>qualitative information, market researchers can gain insight into consumers&rsquo;
</p>
<p>motives and actions. Casteleyn et al. (2009), for example, show that the Heineken
</p>
<p>Facebook posts reveal that the brand has a negative image in Belgium. The task of
</p>
<p>collecting, processing, analyzing, and storing social networking data is very chal-
</p>
<p>lenging, due to the data&rsquo;s complexity and richness. To enable these tasks,
</p>
<p>researchers have combined theories and methods from a variety of disciplines
</p>
<p>(e.g., computer science, linguistics, statistics) in the emerging research field of
</p>
<p>social media analytics to develop new approaches to and method for analyzing
</p>
<p>social networking data. These include (1) text mining to derive high-quality infor-
</p>
<p>mation from text, (2) social network analysis to study the structure of the
</p>
<p>relationships between persons, organizations, or institutions in social networks,
</p>
<p>and (3) trend analysis to predict emerging topics in, for example, Twitter tweets
</p>
<p>or Facebook posts (Stieglitz et al. 2014). Several companies, such as Talkwaker
</p>
<p>(www.talkwalker.com), aggregate data from different websites (including blogs)
</p>
<p>and social media platforms, such as Twitter, Facebook, and Instagram, which they
</p>
<p>analyze. These sites also provide statistics, such as the number of mentions or
</p>
<p>complaints, thereby providing insight into people, brands, and products rated on
</p>
<p>various dimensions. Talkwalker, for example, conducted a sentiment analysis of a
</p>
<p>56 4 Getting Data</p>
<p/>
<div class="annotation"><a href="https://doi.org/www.bvdep.com">https://doi.org/www.bvdep.com</a></div>
<div class="annotation"><a href="https://doi.org/www.dnb.com">https://doi.org/www.dnb.com</a></div>
<div class="annotation"><a href="https://doi.org/www.dnb.com">https://doi.org/www.dnb.com</a></div>
<div class="annotation"><a href="https://doi.org/www.talkwalker.com">https://doi.org/www.talkwalker.com</a></div>
</div>
<div class="page"><p/>
<p>major product recall by Toyota. Toyota found that its social media mentions
</p>
<p>increased sharply and were far more negative in the US and Europe than in
</p>
<p>Indonesia and Japan (see Fig. 4.2; https://www.talkwalker.com/blog/navigate-the-
</p>
<p>automotive-recall-storm-with-social-media-monitoring).
</p>
<p>Social websites also provide quantitative information. For example, Facebook&rsquo;s
</p>
<p>Ad Manager provides information on the effectiveness of advertising on Facebook,
</p>
<p>including on measures, such as the click-through-rate, and on demographics, such
</p>
<p>as gender or location.
</p>
<p>Big data is an important term in the context of Internet and social networking
</p>
<p>data. The term big data describes very large datasets, generally a mix of quantitative
</p>
<p>and qualitative data in very large volumes, which are automatically analyzed, often
</p>
<p>with the aim of making predictions. There is no commonly accepted definition of
</p>
<p>the term, but the use of big data has become very important very quickly. Big data&rsquo;s
</p>
<p>use is not unique to market research, but spans boundaries and often includes IT,
</p>
<p>operations, and other parts of organizations. Netflix, a provider of videos and
</p>
<p>movies, relies on big data. Netflix faces the challenge that it pays upfront for the
</p>
<p>videos and the movies it purchases, and therefore needs to understand which, and
</p>
<p>how many, of its customers will watch them. Netflix analyzes two billion hours of
</p>
<p>video each month in an endeavor to understand its customers&rsquo; viewing behavior and
</p>
<p>to determine which videos and movies will become hits. Walmart, the largest
</p>
<p>retailer in the world, also uses big data. One of Walmart&rsquo;s challenges is to
</p>
<p>proactively suggest products and services to its customers. Using big data, Walmart
</p>
<p>connects information from many sources, including their location, and uses a
</p>
<p>product database to find related products. This helps Walmart make online
</p>
<p>recommendations.
</p>
<p>Fig. 4.2 Snapshot of sentiment analysis via www.talkwalker.com
</p>
<p>4.2 Secondary Data 57</p>
<p/>
<div class="annotation"><a href="https://www.talkwalker.com/blog/navigate-the-automotive-recall-storm-with-social-media-monitoring">https://www.talkwalker.com/blog/navigate-the-automotive-recall-storm-with-social-media-monitoring</a></div>
<div class="annotation"><a href="https://www.talkwalker.com/blog/navigate-the-automotive-recall-storm-with-social-media-monitoring">https://www.talkwalker.com/blog/navigate-the-automotive-recall-storm-with-social-media-monitoring</a></div>
<div class="annotation"><a href="http://www.talkwalker.com">http://www.talkwalker.com</a></div>
</div>
<div class="page"><p/>
<p>4.3 Conducting Secondary Data Research
</p>
<p>In Chap. 2, we discussed the market research process, starting with identifying and
</p>
<p>formulating the research question, followed by determining the research design.
</p>
<p>Once these two have been done, your attention should turn to designing the sample
</p>
<p>and the method of data collection. In respect of secondary data, this task involves
</p>
<p>the steps shown in Fig. 4.3.
</p>
<p>4.3.1 Assess Availability of Secondary Data
</p>
<p>Search engines (such as Google or Bing) provide easy access to many sources of the
</p>
<p>secondary data we have just discussed. Furthermore, many (specialist) databases
</p>
<p>also provide access to secondary data.
</p>
<p>Search engines crawl through the Internet, regularly updating their contents.
</p>
<p>Algorithms, which include how websites are linked, and other peoples&rsquo; searches
</p>
<p>evaluate this content and present a set of results. Undertaking searches by means of
</p>
<p>search engines requires careful thought. For example, the word order is important
</p>
<p>(put keywords first) and operators (such as &thorn;, �, and ~) may have to be added to
</p>
<p>restrict searches. In Box 4.2, we discuss the basics of using Google to search the
</p>
<p>Internet.
</p>
<p>Assess construct validity
</p>
<p>Assess sampling
&bull; Assess representativeness of the sample
</p>
<p>&bull; Assess recency,  time frame, and future data collection
</p>
<p>&bull; What is the consistency of the measurement over time?
</p>
<p>Specify the 
</p>
<p>construct 
</p>
<p>theoretically
</p>
<p>Assess ability of secondary data 
</p>
<p>items to measure the construct
</p>
<p>Assess 
</p>
<p>nomological 
</p>
<p>validity
</p>
<p>Assess inclusion of key variables
</p>
<p>Assess availability of secondary data
</p>
<p>Fig. 4.3 Assessing secondary data
</p>
<p>58 4 Getting Data</p>
<p/>
</div>
<div class="page"><p/>
<p>Box 4.2 Using Google to Searching for Secondary Data
</p>
<p>Most people use Google daily, so why not use it for market research purposes
</p>
<p>too? Google has an easy interface, but if you use the standard search box, you
</p>
<p>may not find the data you are looking for. What must you then do to find
</p>
<p>useful data?
</p>
<p>&ndash; You could use Google Scholar (https://scholar.google.com) if you are
</p>
<p>looking for scholarly information such as that found in academic journals.
</p>
<p>While you can search for any information, specific search items can
</p>
<p>usually only be accessed if you have an organizational, university, or
</p>
<p>library password.
</p>
<p>&ndash; By using Google Books (https://books.google.com/books), you can enter
</p>
<p>several keywords to easily search through a very large catalogue of books.
</p>
<p>Google clearly indicates the books in which the search results are found
</p>
<p>and the pages on which the keywords occur. Ngram Viewer (https://
</p>
<p>books.google.com/ngrams/), which shows the relative frequency with
</p>
<p>which words are used, is a cool Google books tool.
</p>
<p>&ndash; If you cannot find what you are looking for, Google allows you to refine
</p>
<p>your search. After you have searched you can typically select All, Images,
</p>
<p>Videos, News, Maps, and More to select the type of result you are
</p>
<p>interested in. Under Tools, you can select the country and time range
</p>
<p>from which results are to be shown.
</p>
<p>&ndash; Google Public Data Explorer (https://www.google.com/publicdata/
</p>
<p>directory) facilitates exploration of a variety of public-interest datasets.
</p>
<p>These include the US Census Bureau, Eurostat, and the OECD datasets.
</p>
<p>This site is particularly useful if you want to obtain visualized data on
</p>
<p>economic indicators.
</p>
<p>&ndash; Try using operators, which are signs that you use to restrict your research.
</p>
<p>For example, putting a minus symbol (�) (without a space) before a search
</p>
<p>word excludes this word from your findings. Putting a sequence of
</p>
<p>words, or an entire sentence in quotation marks (e.g., &ldquo;a concise guide to
</p>
<p>market research&rdquo;) indicates that Google should only search for exact
</p>
<p>matches.
</p>
<p>Databases contain existing data that can be used for market research purposes,
</p>
<p>which we discussed in the section &ldquo;External Secondary Data.&rdquo; Lightspeed Research
</p>
<p>(www.lightspeedresearch.com), for example, maintains several databases, such as
</p>
<p>the Travel &amp; Leisure Specialty Panel, providing details on selected market
</p>
<p>segments. GfK provides several databases that track retail sales. Nielsen maintains
</p>
<p>a large consumer panel of some 250,000 households in 27 countries. It is clearly not
</p>
<p>possible to provide an exhaustive list of the databases available, but an online
</p>
<p>4.3 Conducting Secondary Data Research 59</p>
<p/>
<div class="annotation"><a href="https://scholar.google.com">https://scholar.google.com</a></div>
<div class="annotation"><a href="https://books.google.com/books">https://books.google.com/books</a></div>
<div class="annotation"><a href="https://books.google.com/ngrams/">https://books.google.com/ngrams/</a></div>
<div class="annotation"><a href="https://books.google.com/ngrams/">https://books.google.com/ngrams/</a></div>
<div class="annotation"><a href="https://www.google.com/publicdata/directory">https://www.google.com/publicdata/directory</a></div>
<div class="annotation"><a href="https://www.google.com/publicdata/directory">https://www.google.com/publicdata/directory</a></div>
<div class="annotation"><a href="https://doi.org/www.lightspeedresearch.com">https://doi.org/www.lightspeedresearch.com</a></div>
</div>
<div class="page"><p/>
<p>search, a market research agency, or an expert should help you identify the options
</p>
<p>quickly.
</p>
<p>Once a potential secondary data source has been located, the next task is to
</p>
<p>evaluate the available data. It is important to critically assess the (potential) data&rsquo;s
</p>
<p>fit with your needs. Figure 4.3 provides a set of criteria to help evaluate this fit.
</p>
<p>4.3.2 Assess Inclusion of Key Variables
</p>
<p>Measurement is the first element to assess. It consists of a set of criteria. You should
</p>
<p>first check whether the desired variables are included in the source. The key
</p>
<p>variables in which you are interested, or could use, should obviously be part of
</p>
<p>the data. Also check if these variables are included at the required level of analysis,
</p>
<p>which is called the aggregation level (see Chap. 3). For example, the American
</p>
<p>Customer Satisfaction Index (ACSI) satisfaction dataset reports on satisfaction at
</p>
<p>the company level;3 therefore, if researchers need the measurement of satisfaction
</p>
<p>at a product, service, or store level, these data are inappropriate.
</p>
<p>4.3.3 Assess Construct Validity
</p>
<p>After checking that the desired variables are included in the source, the construct
</p>
<p>validity should be assessed (see Chap. 3 for a discussion of validity). Validity
</p>
<p>relates to whether variables measure what they should measure. Construct validity
</p>
<p>is a general term relating to how a variable is defined conceptually and its suggested
</p>
<p>(empirical) measure (see Chap. 3). Houston (2002) establishes a three-step method
</p>
<p>to assess the construct validity of secondary data measures.
</p>
<p>&ndash; First, specify the theoretical definition of the construct in which you are inter-
</p>
<p>ested. Satisfaction is, for example, often defined as the degree to which an
</p>
<p>experience conforms to expectations and the ideal.
</p>
<p>&ndash; Second, compare your intended measure against this theoretical definition
</p>
<p>(or another acceptable definition of satisfaction). Conceptually, the items should
</p>
<p>fit closely.
</p>
<p>&ndash; Third, assess if these items have nomological validity (see Chap. 3). For exam-
</p>
<p>ple, customer expectations, perceived quality, and value have a significant
</p>
<p>influence on customer satisfaction. Similarly, satisfaction generally relates posi-
</p>
<p>tively to customer loyalty. As such, you would expect the measure of satisfaction
</p>
<p>that you are evaluating to correlate with these measures (if included in the
</p>
<p>database).
</p>
<p>3See www.theacsi.org for a detailed description of how ACSI data are collected.
</p>
<p>60 4 Getting Data</p>
<p/>
<div class="annotation"><a href="http://www.theacsi.org">http://www.theacsi.org</a></div>
</div>
<div class="page"><p/>
<p>Taking these three steps is important, as the construct validity is often poor when
</p>
<p>secondary data are used.4 See Raithel et al. (2012) for an application of this three-
</p>
<p>step process.
</p>
<p>If there are multiple sources of secondary data, identical measures can be
</p>
<p>correlated to assess the construct validity. For example, the Thomson Reuter SDC
</p>
<p>Platinum and the Bioscan database of the American Health Consultants both
</p>
<p>include key descriptors of firms and financial information; they therefore have a
</p>
<p>considerable overlap regarding the measures included. This may raise questions
</p>
<p>regarding which databases are the most suitable, particularly if the measures do not
</p>
<p>correlate highly. Fortunately, databases have been compared; for example, Schil-
</p>
<p>ling (2009) compares several databases, including SDC Platinum.
</p>
<p>4.3.4 Assess Sampling
</p>
<p>Next, the sampling process of the collected secondary data should be assessed. First
</p>
<p>assess the population and the representativeness of the sample drawn from it. For
</p>
<p>example, the sampling process of Nielsen Homescan&rsquo;s data collection effort is
</p>
<p>based on probability sampling, which can lead to representative samples (see
</p>
<p>Chap. 3). Sellers of secondary data often emphasize the size of the data collected,
</p>
<p>but good sampling is more important than sample size! When sampling issues arise,
</p>
<p>these can be difficult to detect in secondary data, because the documents explaining
</p>
<p>the methodology behind secondary data (bases) rarely discuss the data collection&rsquo;s
</p>
<p>weaknesses. For example, in many commercial mailing lists 25% (or more) of the
</p>
<p>firms included routinely have outdated contact information, are bankrupt, or other-
</p>
<p>wise not accurately recorded. This means that the number of contactable firms is
</p>
<p>much lower than the number of firms listed in the database. In addition, many firms
</p>
<p>may not be listed in the database. Whether these issues are problematic depends on
</p>
<p>the research purpose. For example, descriptive statistics may be inaccurate if data
</p>
<p>are missing.
</p>
<p>The recency of the data, the time period over which the data were collected, and
</p>
<p>future intentions to collect data should be assessed next. The data should be recent
</p>
<p>enough to allow decisions to be based on them. The data collection&rsquo;s timespan and
</p>
<p>the intervals at which the data were collected (in years, months, weeks, or in even
</p>
<p>more detail) should match the research question. For example, when introducing
</p>
<p>new products, market competitors&rsquo; market share is an important variable, therefore
</p>
<p>such data must be recent. Also consider whether the data will be updated in the
</p>
<p>future and the frequency with which such updates will done. Spending considerable
</p>
<p>time on getting to know data that will not be refreshed can be frustrating! Other
</p>
<p>measurement issues include definitions that change over time. For example, many
</p>
<p>firms changed their definitions of loyalty from behavioral (actual purchases) to
</p>
<p>4Issues related to construct validity in business marketing are discussed by, for example,
Rindfleisch and Heide (1997). A more general discussion follows in Houston (2002).
</p>
<p>4.3 Conducting Secondary Data Research 61</p>
<p/>
</div>
<div class="page"><p/>
<p>attitudinal (commitment or intentions). Such changes make comparisons over time
</p>
<p>difficult, as measures can only be compared if the definitions are consistent.
</p>
<p>4.4 Conducting Primary Data Research
</p>
<p>Primary data are gathered for a specific research project or task. There are two
</p>
<p>ways of gathering primary data. You can observe consumers&rsquo; behavior, for exam-
</p>
<p>ple, by means of observational studies, or test markets. Alternatively, you can ask
</p>
<p>consumers directly by means of surveys, in-depth interviews, predictive techniques,
</p>
<p>or focus groups. Experiments are a special type of research, which is normally a
</p>
<p>combination of observing and asking. We provide an overview of the various types
</p>
<p>of primary data collection methods in Fig. 4.1.
</p>
<p>Next, we briefly introduce observing as a method of collecting primary data. We
</p>
<p>proceed by discussing how to conduct surveys. Since surveys are the main means of
</p>
<p>collecting primary data by asking, we discuss the process of undertaking survey
</p>
<p>research in enough detail to allow you to set up your own survey-based research
</p>
<p>project. We then discuss in-depth interviews, including a special type of test used in
</p>
<p>these interviews (projective techniques). Last, we discuss combinations of observ-
</p>
<p>ing and asking &ndash; the basics of conducting experimental research.
</p>
<p>4.4.1 Collecting Primary Data Through Observations
</p>
<p>Observational studies can provide important insights that other market research
</p>
<p>techniques do not. Observational techniques shed light on consumers&rsquo; and
</p>
<p>employees&rsquo; behavior and can help answer questions such as: How do consumers
</p>
<p>walk through supermarkets?; how do they consume and dispose of products?; and
</p>
<p>how do employees spend their working day? Observational techniques are normally
</p>
<p>used to understand what people are doing rather than why they are doing it. They
</p>
<p>work well when people find it difficult to put what they are doing into words, such
</p>
<p>as shoppers from different ethnic backgrounds.
</p>
<p>Most observational studies use video recording equipment, or trained
</p>
<p>researchers, who unobtrusively observe what people do (e.g., through one-way
</p>
<p>mirrors or by using recording equipment). Recently, researchers started using
</p>
<p>computer chips (called RFIDs) as observational equipment to trace consumers&rsquo;
</p>
<p>shopping paths within a supermarket. Almax, an Italian company, has developed a
</p>
<p>special type of observational equipment. Their EyeSee product is an in-store
</p>
<p>mannequin equipped with a camera and audio recording equipment. The product
</p>
<p>also comprises software that analyze the camera recordings and provides statistical
</p>
<p>and contextual information, such as the shoppers&rsquo; demographics. Such information
</p>
<p>is useful for developing targeted marketing strategies. For example, a retail com-
</p>
<p>pany found that Chinese visitors prefer to shop in Spanish stores after 4 p.m.,
</p>
<p>prompting these stores to increase their Chinese-speaking staff at these hours.
</p>
<p>Figure 4.4 shows what the EyeSee Mannequin looks like.
</p>
<p>62 4 Getting Data</p>
<p/>
</div>
<div class="page"><p/>
<p>Mystery shopping, when a trained researcher is asked to visit a store or
</p>
<p>restaurant and consume the products or services, is a specific type of observational
</p>
<p>study. For example, McDonalds and Selfridges, the latter a UK retail chain, both
</p>
<p>use mystery shoppers to ensure the quality of their services and products (see Box
</p>
<p>4.3 for an MSNBC video on mystery shopping).
</p>
<p>Sometimes observational studies are conducted in households, with researchers
</p>
<p>participating in them to see how the inhabitants buy, consume, and dispose of
</p>
<p>products or services. The type of study in which the researcher is a participant is
</p>
<p>called an ethnography. An example of an ethnography is Volkswagen&rsquo;s
</p>
<p>Moonraker project, in which several Volkswagen employees followed American
</p>
<p>drivers to gain an understanding of how their usage of and preferences for
</p>
<p>automobiles differ from those of European drivers (Kurylko 2005).
</p>
<p>Test markets are a useful, but costly, type of market research in which a
</p>
<p>company introduces a new product or service to a specific geographic market.
</p>
<p>Test markets are sometimes also used to understand how consumers react to
</p>
<p>different marketing mix instruments, such as changes in pricing, distribution, or
</p>
<p>advertising and communication. Test marketing is therefore about changing a
</p>
<p>product or service offering in a real market and gauging consumers&rsquo; reactions.
</p>
<p>While the results from such studies provide important insights into consumer
</p>
<p>behavior in a real-world setting, they are expensive and difficult to conduct.
</p>
<p>Some frequently used test markets include Hassloch in Germany, as well as
</p>
<p>Indianapolis and Nashville in the US.
</p>
<p>Fig. 4.4 The EyeSee Mannequin
</p>
<p>4.4 Conducting Primary Data Research 63</p>
<p/>
</div>
<div class="page"><p/>
<p>Box 4.3 Using Mystery Shopping to Improve Customer Service
</p>
<p>www.youtube.com/watch?v&frac14;JK2p6GMhs0I
</p>
<p>4.4.2 Collecting Quantitative Data: Designing Surveys
</p>
<p>There is little doubt that surveys are the mainstay of primary market research.
</p>
<p>While it may seem easy to conduct a survey (just ask what you want to know,
</p>
<p>right?), there are many issues that could turn good intentions into bad results. In this
</p>
<p>section, we discuss the key design choices for good surveys. A good survey requires
</p>
<p>at least seven steps. First, determine the survey goal. Next, determine the type of
</p>
<p>questionnaire required and the administration method. Thereafter, decide on the
</p>
<p>questions and the scale, as well as the design of the questionnaire. Conclude by
</p>
<p>pretesting and administering the questionnaire. We show these steps in Fig. 4.5.
</p>
<p>4.4.2.1 Set the Survey Goal
Before you start designing the questionnaire, it is vital to consider the survey goal.
</p>
<p>Is it to collect quantitative data on customers&rsquo; background, to assess customer
</p>
<p>satisfaction, or do you want to understand why and how customers complain?
</p>
<p>These different goals influence the type of questions asked (such as open-ended
</p>
<p>or closed-ended questions), the method of administration (e.g., by mail or on the
</p>
<p>Web), and other design issues discussed below. Two aspects are particularly
</p>
<p>relevant when designing surveys:
</p>
<p>First, consider the information or advice you want to emerge from the study for
</p>
<p>which the survey is required. Say you are asked to help understand check-in waiting
</p>
<p>times at an airport. If the specific study question is to gain an understanding of how
</p>
<p>many minutes travelers are willing to wait before becoming dissatisfied, you should
</p>
<p>be able to provide an answer to the question: How much does travelers&rsquo; satisfaction
</p>
<p>decrease with increased waiting time? If, on the other hand, the specific question is
</p>
<p>64 4 Getting Data</p>
<p/>
<div class="annotation"><a href="https://doi.org/www.youtube.com/watch?v=JK2p6GMhs0I">https://doi.org/www.youtube.com/watch?v=JK2p6GMhs0I</a></div>
<div class="annotation"><a href="https://doi.org/www.youtube.com/watch?v=JK2p6GMhs0I">https://doi.org/www.youtube.com/watch?v=JK2p6GMhs0I</a></div>
</div>
<div class="page"><p/>
<p>to understand how people perceive waiting time (short or long), your questions
</p>
<p>should focus on how travelers perceive this time and, perhaps, what influences their
</p>
<p>perception. Thus, the information or advice you want to provide influences the
</p>
<p>questions that you should pose in a survey.
</p>
<p>Second, consider the method required for the study early in the design process.
</p>
<p>For example, if a study&rsquo;s goal is to determine market segments, you should
</p>
<p>probably use cluster analysis (see Chap. 9). Similarly, if the study&rsquo;s goal is to
</p>
<p>develop a way to systematically measure customer satisfaction, you are likely to use
</p>
<p>factor analysis (see Chap. 8). This approach is crucial, as each method requires
</p>
<p>different types of data. Cluster analysis, for example, generally requires variables
</p>
<p>that are not too highly correlated, meaning that researchers need to use a type of
</p>
<p>questionnaire that can produce these data. On the other hand, factor analysis
</p>
<p>requires data that include different, but highly correlated, variables. If you use
</p>
<p>factor analysis to distinguish between the different aspects of consumer satisfaction,
</p>
<p>you need to design a survey that will produce data allowing you to conduct factor
</p>
<p>analysis.
</p>
<p>Design the questionnaire
</p>
<p>Set the survey goal
</p>
<p>Determine the type of questionnaire and method of administration
</p>
<p>Design the items
</p>
<p>Pretest the questionnaire
</p>
<p>Execution
</p>
<p>Set the scale
</p>
<p>Fig. 4.5 Steps in designing surveys
</p>
<p>4.4 Conducting Primary Data Research 65</p>
<p/>
</div>
<div class="page"><p/>
<p>4.4.2.2 Determine the Type of Questionnaire and Method
of Administration
</p>
<p>After determining the survey goal, you need to decide on the type of questionnaire
</p>
<p>you should use and how it should be administered. There are four key ways of
</p>
<p>administering a survey:
</p>
<p>&ndash; personal interviews,
</p>
<p>&ndash; telephone interviews,
</p>
<p>&ndash; web surveys, and
</p>
<p>&ndash; mail surveys.
</p>
<p>In some cases, researchers combine different ways of administering surveys. This is
</p>
<p>called a mixed mode.
</p>
<p>Personal interviews (or face-to-face interviews) can obtain high response
</p>
<p>rates, since engagement with the respondents is maximized, allowing rich informa-
</p>
<p>tion (visual expressions, etc.) to be collected. Moreover, since people find it hard to
</p>
<p>walk away from interviews, it is possible to collect answers to a reasonably lengthy
</p>
<p>set of questions. Consequently, personal interviews can support long surveys. It is
</p>
<p>also the best type of data collection for open-ended responses. In situations where
</p>
<p>the respondent is initially unknown, this may be the only feasible data collection
</p>
<p>type. Consequently, in-depth interviews may be highly preferable, but they are also
</p>
<p>the costliest per respondent. This is less of a concern if only small samples are
</p>
<p>required (in which case personal interviewing could be the most efficient). Other
</p>
<p>issues with personal interviews include the possibility of interviewer bias (i.e., a
</p>
<p>bias resulting from the interviewer&rsquo;s behavior e.g., in terms of his/her reactions or
</p>
<p>presentation of the questions), respondent bias to sensitive items, and the data
</p>
<p>collection usually takes more time. Researchers normally use personal interviewing
</p>
<p>when they require an in-depth exploration of opinions. Such interviewing may also
</p>
<p>help if drop out is a key concern. For example, if researchers collect data from
</p>
<p>executives around the globe, using methods other than face-to-face interviewing
</p>
<p>may lead to excessive non-response in countries such as Russia or China where
</p>
<p>face-to-face interviews are a sign of respect for and appreciation of the time taken.
</p>
<p>CAPI, which is the abbreviation of computer-assisted personal interviews, is a
</p>
<p>frequently used term in the context of in-depth interviewing. CAPI involves using
</p>
<p>computers during the interviewing process to, for example, route the interviewer
</p>
<p>through a series of questions, or to enter responses directly. Similarly, in CASI
</p>
<p>(computer-assisted self-interviews), the respondent uses a computer to complete the
</p>
<p>survey questionnaire without an interviewer administering it.
</p>
<p>Telephone interviews allow researchers to collect data quickly. These
</p>
<p>interviews also support open-ended responses, although not as well as personal
</p>
<p>interviews. Moreover, interviewer bias can only be controlled moderately, since the
</p>
<p>interviewers follow predetermined protocols, and the respondent&rsquo;s interactions with
</p>
<p>others during the interview is strongly controlled. Telephone interviewing can be a
</p>
<p>good compromise between mailed interviews&rsquo; low cost and the richness of in-depth
</p>
<p>66 4 Getting Data</p>
<p/>
</div>
<div class="page"><p/>
<p>interviews. CATI refers to computer-assisted telephone interviews, which are an
</p>
<p>important method of administering surveys. Until the 1990s, telephone interviews
</p>
<p>were generally conducted via landlines, but mobile phone usage has soared in the
</p>
<p>meantime. In many countries, mobile phone adoption rates are higher than landline
</p>
<p>adoption. This holds especially for African countries, India, and many European
</p>
<p>countries if younger consumers are the targeted interviewees (Vincente and Reis
</p>
<p>2010). Consequently, mobile phone surveys have become dominant in market
</p>
<p>research.
</p>
<p>A decade ago, the differences between landline and mobile phone surveys
</p>
<p>could be large, with younger and richer individuals being overrepresented in
</p>
<p>mobile phone surveys (Vincente et al. 2008). As the adoption of mobile
</p>
<p>phones increased, the use of landlines decreased, which introduced new
</p>
<p>problems in terms of sampling errors (Stern et al. 2014). For example,
</p>
<p>younger people are now less likely to have landlines. An additional issue is
</p>
<p>that landlines are fixed to a geographical area whereas mobile phones are not.
</p>
<p>As people move or travel, mobile phones are far less likely to give useful
</p>
<p>information about geographic areas. While recent research has shown that
</p>
<p>differences in response accuracy between mobile phone and landline surveys
</p>
<p>are small (e.g., with regard to social desirability bias), especially for questions
</p>
<p>that are not cognitively demanding (e.g., Kennedy and Everett 2011; Lynn
</p>
<p>and Kaminska 2013), the samples from which they are drawn can be very
</p>
<p>different in practice. There are some other differences as well. For example,
</p>
<p>the likelihood of full completion of surveys is higher for mobile calling, even
</p>
<p>though completion takes around 10&ndash;15% longer (Lynn and Kaminska 2013).
</p>
<p>Web surveys (sometimes referred to as CAWI, or computer-assisted web
</p>
<p>interviews) are often the least expensive to administer and can be fast in terms of
</p>
<p>data collection, particularly since they can be set up very quickly. Researchers can
</p>
<p>administer web surveys to very large populations, even internationally, because,
</p>
<p>besides the fixed costs of setting up a survey, the marginal costs of administering
</p>
<p>additional web surveys are relatively low.
</p>
<p>Many firms specializing in web surveys will ask $0.30 (or more) for each
</p>
<p>respondent, which is substantially lower than the costs of telephone interviews,
</p>
<p>in-depth interviews, and mail surveys. It is also easy to obtain precise quotes
</p>
<p>quickly. Qualtrics (http://www.qualtrics.com) is a leading web service provider
</p>
<p>that allows a specific type of respondent and a desired sample size to be chosen. For
</p>
<p>example, using Qualtrics&rsquo;s sample to survey 500 current owners of cars to measure
</p>
<p>their satisfaction costs $2,500 for a 10-min survey. This cost increases sharply if
</p>
<p>samples are hard to access and/or require compensation for their time. For example,
</p>
<p>surveying 500 purchasing managers by means of a 10-min survey costs approxi-
</p>
<p>mately $19,500.
</p>
<p>4.4 Conducting Primary Data Research 67</p>
<p/>
<div class="annotation"><a href="https://doi.org/qualtrics.com">https://doi.org/qualtrics.com</a></div>
</div>
<div class="page"><p/>
<p>Web surveys also support complex survey designs with elaborate branching and
</p>
<p>skip patterns that depend on the response. For example, web surveys allow different
</p>
<p>surveys to be created for different types of products. Further, since web surveys
</p>
<p>reveal the questions progressively to the respondents, there is an option to channel
</p>
<p>them to the next question based on their earlier responses. This procedure is called
</p>
<p>adaptive questioning. In addition, web surveys can be created that allow
</p>
<p>respondents to automatically skip questions if they do not apply. For example, if
</p>
<p>a respondent has no experience with an iPad, researchers can create surveys that do
</p>
<p>not ask questions about this product. However, web surveys impose similar burdens
</p>
<p>on the respondents as mail surveys do (see below), which they may experience as an
</p>
<p>issue. This makes administering long web surveys difficult. Moreover, open-ended
</p>
<p>questions tend to be problematic, because few respondents are likely to provide
</p>
<p>answers, leading to a low item response. There is evidence that properly conducted
</p>
<p>web surveys lead to data as good as those obtained from mail surveys; in addition,
</p>
<p>the lack an interviewer and the resultant interviewer bias means they can provide
</p>
<p>better results than personal interviews (Bronner and Ton 2007; Deutskens et al.
</p>
<p>2006). In web surveys, the respondents are also less exposed to evaluation appre-
</p>
<p>hension and less inclined to respond with socially desirable behavior.5Web surveys
</p>
<p>are also used when a quick &ldquo;straw poll&rdquo; is needed on a subject.
</p>
<p>It is important to distinguish between true web-based surveys used for
</p>
<p>collecting information on which marketing decisions will be based and
</p>
<p>polls, or very short surveys, on websites used to increase interactivity.
</p>
<p>These polls/short surveys are used to attract and keep people interested in
</p>
<p>websites and are not part of market research. For example, USA Today
</p>
<p>(http://www.usatoday.com), an American newspaper, regularly publishes
</p>
<p>short polls on their main website.
</p>
<p>Mail surveys are paper-based surveys sent out to respondents. They are a more
</p>
<p>expensive type of survey research and are best used for sensitive items. Since no
</p>
<p>interviewer is present, there is no interviewer bias. However, mail surveys are a
</p>
<p>poor choice for complex survey designs, such as when respondents need to skip a
</p>
<p>large number of questions depending on previously asked questions, as this means
</p>
<p>that the respondent needs to correctly interpret the survey structure. Open-ended
</p>
<p>questions are also problematic, because few people are likely to provide answers to
</p>
<p>such questions if the survey is administered on paper. Other problems include a lack
</p>
<p>of control over the environment in which the respondent fills out the survey and that
</p>
<p>mail surveys take longer than telephone or web surveys. However, in some
</p>
<p>situations, mail surveys are the only way to gather data. For example, while
</p>
<p>executives rarely respond to web-based surveys, they are more likely to respond
</p>
<p>5For a comparison of response behavior between CASI, CAPI, and CATI, see Bronner and
Ton (2007).
</p>
<p>68 4 Getting Data</p>
<p/>
<div class="annotation"><a href="https://doi.org/www.usatoday.com">https://doi.org/www.usatoday.com</a></div>
</div>
<div class="page"><p/>
<p>to paper-based surveys. Moreover, if the participants cannot easily access the web
</p>
<p>(such as employees working in supermarkets, cashiers, etc.), handing out paper
</p>
<p>surveys is likely to be more successful.
</p>
<p>The method of administration also has a significant bearing on a survey&rsquo;s
</p>
<p>maximum duration (Vesta Research 2016). As a rule of thumb, telephone
</p>
<p>interviews should be no longer than 20 min. When calling a on a mobile
</p>
<p>phone, however, the survey should not exceed 5 min. The maximum survey
</p>
<p>duration of web surveys is 20&ndash;10 min for social media-based surveys. Per-
</p>
<p>sonal interviews and mail surveys can be much longer, depending on the
</p>
<p>context. For example, surveys comprising personal interviews on topics that
</p>
<p>respondents find important, could take up to 2 h. However, when topics are
</p>
<p>less important, mail surveys and personal interviews need to be considerably
</p>
<p>shorter.
</p>
<p>Market researchers are increasingly usingmixed mode approaches. An example
</p>
<p>of a mixed mode survey is when potential respondents are first approached by
</p>
<p>phone, asked to participate and confirm their email addresses, after which they are
</p>
<p>given access to a web survey. Mixed mode approaches are also used when people
</p>
<p>are first sent a paper survey and then called if they fail to respond.
</p>
<p>Mixed mode approaches may help, because they signal that the survey is
</p>
<p>important. They may also help improve response rates, as people who are more
</p>
<p>visually oriented prefer mail and web surveys, whereas those who are aurally
</p>
<p>oriented prefer telephone surveys. However, there is only evidence of increased
</p>
<p>response rates when modes are offered sequentially and of mixed modes reducing
</p>
<p>response rates when offered simultaneously (Stern et al. 2014). By providing
</p>
<p>different modes, people can use the mode they most prefer. A downside of mixed
</p>
<p>mode surveys is that they are expensive and require a detailed address list (includ-
</p>
<p>ing a telephone number and matching email address). However, systematic (non)
</p>
<p>response is the most serious mixed mode survey issue. For example, when filling
</p>
<p>out mail surveys, the respondents have more time than when providing answers by
</p>
<p>telephone. If respondents need this time to think about their answers, the responses
</p>
<p>obtained from mail surveys may differ systematically from those obtained from
</p>
<p>telephone surveys.
</p>
<p>4.4.2.3 Design the Items
Designing the items, whether for a personal interview, web survey, or mail survey,
</p>
<p>requires a great deal of thought. Take, for example, the survey item shown in
</p>
<p>Fig. 4.6.
</p>
<p>It is unlikely that people can give meaningful answers to such an item. First,
</p>
<p>using a negation (&ldquo;not&rdquo;) in sentences makes questions hard to understand. Second,
</p>
<p>the reader may not have an iPhone, or may not have experience using it. Third, the
</p>
<p>answer categories are unequally distributed. That is, there is one category above
</p>
<p>4.4 Conducting Primary Data Research 69</p>
<p/>
</div>
<div class="page"><p/>
<p>neutral while there are two below. These issues are likely to create difficulties with
</p>
<p>understanding and answering questions, which may, in turn, cause validity and
</p>
<p>reliability issues. While the last aspect relates to the properties of the scale (see
</p>
<p>section Set the Scale), the first two issues refer to the item content and wording,
</p>
<p>which we will discuss next.
</p>
<p>Item Content
</p>
<p>When deciding on the item content, there are at least three essential rules you
</p>
<p>should keep in mind.
</p>
<p>As a first rule, ask yourself whether everyone will be able to answer each item. If
</p>
<p>the item is, for example, about the quality of train transport and the respondent
</p>
<p>always travels by car, his or her answers will be meaningless. However, the framing
</p>
<p>of items is important, since, for example, questions about why the specific respon-
</p>
<p>dent does not use the train can yield important insights.
</p>
<p>As a second rule, you should check whether respondents can construct or recall
</p>
<p>an answer. If you require details that possibly occurred a long time ago (e.g., &ldquo;what
</p>
<p>information did the real estate agent provide when you bought/rented your current
</p>
<p>house?&rdquo;), the respondents may have to &ldquo;make up&rdquo; an answer, which also leads to
</p>
<p>validity and reliability issues.
</p>
<p>As a third rule, assess whether the respondents are willing to answer an item. If
</p>
<p>contents are considered sensitive (e.g., referring to sexuality, money, etc.),
</p>
<p>respondents may provide more socially desirable answers (e.g., by reporting higher
</p>
<p>or lower incomes than are actually true). Most notably, respondents might choose to
</p>
<p>select a position that they believe society favors (e.g., not to smoke or drink, or to
</p>
<p>exercise), inducing a social desirability bias. They may also not answer such
</p>
<p>questions at all. You should determine whether these items are necessary to attain
</p>
<p>the research objective. If they are not, omit them from the survey. The content that
</p>
<p>respondents may regard as sensitive is subjective and differs across cultures, age
</p>
<p>categories, and other variables. Use your common sense and, if necessary, use
</p>
<p>experts to decide whether the items are appropriate. In addition, make sure you
</p>
<p>pretest the survey and ask the pretest participants whether they were reluctant to
</p>
<p>provide certain answers. To reduce respondents&rsquo; propensity to give socially desir-
</p>
<p>able answers, use indirect questioning (e.g., &ldquo;What do you believe other people
</p>
<p>think about. . .?&rdquo;), frame questions as neutrally as possible, and suggest that many
</p>
<p>people exhibit behavior different from the norm (e.g., Brace 2004). Adjusting the
</p>
<p>response categories also helps when probing sensitive topics. For example, instead
</p>
<p>Fig. 4.6 Example of a bad survey item
</p>
<p>70 4 Getting Data</p>
<p/>
</div>
<div class="page"><p/>
<p>of directly asking about respondents&rsquo; disposable income, you can provide various
</p>
<p>answering categories, which will probably increase their willingness to answer this
</p>
<p>question.
</p>
<p>Designing the items also requires deciding whether to use open-ended or closed-
</p>
<p>ended questions. Open-ended questions provide little or no structure for
</p>
<p>respondents&rsquo; answers. Generally, the researcher asks a question and the respondent
</p>
<p>writes down his or her answer in a box. Open-ended questions (also called verba-
</p>
<p>tim items) are flexible and allow for explanation, making them particularly suitable
</p>
<p>for exploratory research. However, the drawback is that the respondents may be
</p>
<p>reluctant to provide detailed information. Furthermore, answering open-ended
</p>
<p>questions is more difficult and time consuming. In addition, their interpretation
</p>
<p>requires substantial coding. Coding issues arise when respondents provide many
</p>
<p>different answers (such as &ldquo;sometimes,&rdquo; &ldquo;maybe,&rdquo; &ldquo;occasionally,&rdquo; or &ldquo;once in a
</p>
<p>while&rdquo;) and the researcher has to divide these into categories (such as very
</p>
<p>infrequently, infrequently, frequently, and very frequently) for further statistical
</p>
<p>analysis. This coding is very time-consuming, subjective, and difficult. Closed-
</p>
<p>ended questions provide respondents with only a few categories from which to
</p>
<p>choose, which drastically reduces their burden, thereby inducing much higher
</p>
<p>response rates compared to open-ended questions. Closed-ended questions also
</p>
<p>facilitate immediate statistical analysis of the responses as no ex post coding is
</p>
<p>necessary. However, researchers need to identify answer categories in advance and
</p>
<p>limit this number to a manageable amount. Closed-ended questions are dominant in
</p>
<p>research and practice.
</p>
<p>Item Wording
</p>
<p>The golden rule of item wording is to keep it short and simple. That is, use simple
</p>
<p>words and avoid using jargon or slang as some respondents may misunderstand
</p>
<p>these. Similarly, keep grammatical complexities to a minimum by using active
</p>
<p>rather than passive voice, repeat the nouns instead of using pronouns, and avoiding
</p>
<p>possessive forms (Lietz 2010). Use short sentences (20 words max; Oppenheim
</p>
<p>1992) to minimize the cognitive demands required from the respondents (Holbrook
</p>
<p>et al. 2006) and to avoid the risk of double-barreled questions, causing
</p>
<p>respondents to agree with one part of the question but not the other, or which
</p>
<p>they cannot answer without accepting a particular assumption. An example of such
</p>
<p>a double-barreled question is: &ldquo;In general, are you satisfied with the products and
</p>
<p>services of the company?&rdquo; A more subtle example is: &ldquo;Do you have the time to read
</p>
<p>the newspaper every day?&rdquo; This question also contains two aspects, namely &ldquo;having
</p>
<p>time&rdquo; and &ldquo;reading the paper every day.&rdquo; The question &ldquo;Do you read the newspaper
</p>
<p>every day?&rdquo; followed by another about the reasons for a negative or a positive
</p>
<p>answer would be clearer (Lietz 2010).
</p>
<p>Moreover, avoid using the word not or no where possible. This is particularly
</p>
<p>important when other words in the same sentence are negative, such as &ldquo;unable,&rdquo; or
</p>
<p>&ldquo;unhelpful,&rdquo; because sentences with two negatives (called a double negative) are
</p>
<p>hard to understand. For example, a question such as &ldquo;I do not use the email function
</p>
<p>on my iPhone because it is unintuitive&rdquo; is quite hard to follow. Also, avoid the use
</p>
<p>4.4 Conducting Primary Data Research 71</p>
<p/>
</div>
<div class="page"><p/>
<p>of vague quantifiers, such as &ldquo;frequent&rdquo; or &ldquo;occasionally&rdquo; (Dillman et al. 2014),
</p>
<p>which make it difficult for respondents to answer questions (what exactly is meant
</p>
<p>by &ldquo;occasionally?&rdquo;). They also make comparing responses difficult. After all, what
</p>
<p>one person considers &ldquo;occasionally,&rdquo; may be &ldquo;frequent&rdquo; for another.6 Instead, it is
</p>
<p>better to use frames that are precise (&ldquo;once a week&rdquo;).
</p>
<p>Never suggest an answer. For example, a question like &ldquo;Company X has done
</p>
<p>very well, how do you rate it?&rdquo; is highly suggestive and would shift the mean
</p>
<p>response to the positive end of the scale.
</p>
<p>Many researchers recommend including reverse-scaled items in surveys.
</p>
<p>Reverse-scaled means the question, statement (if a Likert scale is used), or word
</p>
<p>pair (if a semantic differential scale is used) are reversed compared to the other
</p>
<p>items in the set. Reverse-scaled items act as cognitive &ldquo;speed bumps&rdquo; that alert
</p>
<p>inattentive respondents that the content varies (Weijters and Baumgartner 2012)
</p>
<p>and help reduce the effect of acquiescence, which relates to a respondent&rsquo;s ten-
</p>
<p>dency to agree with items regardless of the item content (Baumgartner and
</p>
<p>Steenkamp 2001; see also Chap. 5). Furthermore, reverse-scaled items help identify
</p>
<p>straight-lining, which occurs when a respondent marks the same response in almost
</p>
<p>all the items (Chap. 5). However, numerous researchers have shown that reverse-
</p>
<p>scaled items create problems, for example, by generating artificial factors, as
</p>
<p>respondents have greater difficulty verifying the item. This creates misresponse
</p>
<p>rates of 20% and higher (e.g., Swain et al. 2008; Weijters et al. 2013). Given these
</p>
<p>results, we suggest employing reverse-scaled items sparingly (Weijters and
</p>
<p>Baumgartner 2012), for example, only to identify straight-lining. If used, reverse-
</p>
<p>scaled items should not contain a particle negation such as &ldquo;not&rdquo; or &ldquo;no.&rdquo;
</p>
<p>Finally, when you undertake a survey in different countries (or adapt a scale
</p>
<p>from a different language), use professional translators, because translation is a
</p>
<p>complex process. Functionally, translating one language into another seems quite
</p>
<p>easy, as there are many websites, such as Google translate (translate.google.com),
</p>
<p>that do this. However, translating surveys requires preserving the conceptual
</p>
<p>equivalence of whole sentences and paragraphs; current software applications and
</p>
<p>websites cannot ensure this. In addition, cultural differences normally require
</p>
<p>changes to the instrument format or procedure. Back-translation is a technique to
</p>
<p>establish conceptual equivalence across languages. Back-translation requires
</p>
<p>translating a survey instrument into another language, after which another translator
</p>
<p>takes the translated survey instrument and translates it back into the original
</p>
<p>language (Brislin 1970). After the back-translation, the original and back-translated
</p>
<p>instruments are compared and points of divergence are noted. The translation is
</p>
<p>then corrected to more accurately reflect the intent of the wording in the original
</p>
<p>language.
</p>
<p>6Foddy (1993) reported 445 interpretations of the word &ldquo;usually,&rdquo; with the meaning assigned to the
word varying, depending on, for example, the type of activity or who was asked about the activity.
</p>
<p>72 4 Getting Data</p>
<p/>
<div class="annotation"><a href="https://doi.org/google.com">https://doi.org/google.com</a></div>
</div>
<div class="page"><p/>
<p>4.4.2.4 Set the Scale
When deciding on scales, two separate decisions need to be made. First, you need to
</p>
<p>decide on the type of scale. Second, you need to set the properties of the scale you
</p>
<p>choose.
</p>
<p>Type of Scale
</p>
<p>Marketing research and practice have provided a variety of scale types. In the
</p>
<p>following, we discuss the most important (and useful) ones:
</p>
<p>&ndash; likert scales,
</p>
<p>&ndash; semantic differential scales, and
</p>
<p>&ndash; rank order scales.
</p>
<p>The most popular scale type used in questionnaires is the Likert scale (Liu et al.
</p>
<p>2016). Likert scales are used to establish the degree of agreement with a specific
</p>
<p>statement. Such a statement could be &ldquo;I am very committed to Oddjob Airlines.&rdquo;
</p>
<p>The degree of agreement is usually set by scale endpoints ranging from strongly
</p>
<p>disagree to strongly agree. Likert scales are used very frequently and are relatively
</p>
<p>easy to administer. Bear in mind that if the statement is too positive or negative, it is
</p>
<p>unlikely that the endings of the scale will be used, thereby reducing the number of
</p>
<p>answer categories actually used. We show an example of three Likert-scale-type
</p>
<p>items in Fig. 4.7, in which respondents assess the personality of the Oddjob Airways
</p>
<p>brand.
</p>
<p>The semantic differential scale is another scale type that features prominently in
</p>
<p>market research. Semantic differential scales use an opposing pair of words,
</p>
<p>normally adjectives (e.g., young/old, masculine/feminine) constituting the endpoint
</p>
<p>of the scale. Respondents then indicate how well one of the word in each pair
</p>
<p>describes how he or she feels about the object to be rated (e.g., a company or brand).
</p>
<p>These scales are widely used in market research. As with Likert scales, 5 or 7 answer
</p>
<p>categories are commonly used (see the next section on the number of answer
</p>
<p>categories you should use). We provide an example of the semantic differential
</p>
<p>scale in Fig. 4.8, in which respondents provide their view of Oddjob Airways.
</p>
<p>Rank order scales are a unique type of scale, as they force respondents to
</p>
<p>compare alternatives. In its basic form, a rank order scale allows respondents to
</p>
<p>indicate which alternative they rank highest, which alternative they rank second
</p>
<p>highest, etc. Figure 4.9 shows an example. The respondents therefore need to
</p>
<p>balance their answers instead of merely stating that everything is important. In a
</p>
<p>Fig. 4.7 Example of a 7-point Likert scale
</p>
<p>4.4 Conducting Primary Data Research 73</p>
<p/>
</div>
<div class="page"><p/>
<p>more complicated form, rank order scales ask respondents to allocate a certain total
</p>
<p>number of points (often 100) to a number of alternatives. This is called the constant
</p>
<p>sum scale. Constant sum scales work well when a small number of answer
</p>
<p>categories are used (normally up to five). Generally, respondents find constant
</p>
<p>scales that have 6 or 7 answer categories somewhat challenging, while constant
</p>
<p>scales that have eight or more categories are very difficult to answer. The latter are
</p>
<p>thus best avoided.
</p>
<p>In addition to these types of scaling, there are other types, such as graphic rating
</p>
<p>scales, which use pictures to indicate categories, and the MaxDiff scale, in which
</p>
<p>respondents indicate the most and least applicable items. We introduce the MaxDiff
</p>
<p>scale in the Web Appendix (! Downloads).
</p>
<p>Properties of the Scale
</p>
<p>After selecting the type of scale, we need to set the scale properties, which involves
</p>
<p>making several decisions:
</p>
<p>&ndash; decide on the number of response categories,
</p>
<p>&ndash; choose between forced-choice scale and free-choice scales,
</p>
<p>&ndash; design the response categories,
</p>
<p>&ndash; label the response categories,
</p>
<p>&ndash; decide whether to use a &ldquo;don&rsquo;t know&rdquo; option, and
</p>
<p>&ndash; choose between a balanced and unbalanced scale.
</p>
<p>Decide on the number of response categories: When using closed-ended
</p>
<p>questions, the number of answer categories needs to be determined. In its simplest
</p>
<p>form, a survey could use just two answer categories (yes/no). Multiple categories
</p>
<p>(such as, &ldquo;completely disagree,&rdquo; &ldquo;disagree,&rdquo; &ldquo;neutral,&rdquo; &ldquo;agree,&rdquo; &ldquo;completely
</p>
<p>agree&rdquo;) are used more frequently to allow for more nuances. When determining
</p>
<p>how many scale categories to use, we face a trade-off between having more
</p>
<p>variation and differentiation in the responses versus burdening the respondents
</p>
<p>Fig. 4.8 Example of a 7-point semantic differential scale
</p>
<p>Fig. 4.9 Example of a rank order scale
</p>
<p>74 4 Getting Data</p>
<p/>
</div>
<div class="page"><p/>
<p>too much, which can trigger different types of response biases (e.g., Weijters et al.
</p>
<p>2010). Because 5-point scales and 7-point scales are assumed to achieve this trade-
</p>
<p>off well, their use has become common in marketing research (e.g., Fink 2003;
</p>
<p>Peterson 1997). Research on these two scale types in terms of their reliability and
</p>
<p>validity is inconclusive, but the differences between them are generally not very
</p>
<p>pronounced (e.g., Lietz 2010; Peng and Finn 2015; Weijters et al. 2010). Ten-point
</p>
<p>scales are often used in market research practice. However, scales with many
</p>
<p>answer categories often confuse respondents, because the wording differences
</p>
<p>between the scale points become trivial. For example, the difference between
</p>
<p>&ldquo;tend to agree&rdquo; and &ldquo;somewhat agree&rdquo; are subtle and respondents may not be
</p>
<p>able to differentiate between them. In such a case, respondents tend to choose
</p>
<p>categories in the middle of the scale (Rammstedt and Krebs 2007). Given this
</p>
<p>background, we recommend using 5-point or 7-point scales.
</p>
<p>Finally, many web surveys now use levers that allow scaling on a continuum
</p>
<p>without providing response categories. This scale type is called a visual analogue
</p>
<p>scale and is especially recommended if small differences in response behavior need
</p>
<p>to be detected. Visual analogue scales are well known in paper-and-pencil-based
</p>
<p>research (especially in the medical sector) and have become increasingly popular in
</p>
<p>web surveys. A visual analogue scale consists of a line and two anchors, one at each
</p>
<p>end. The anchors often consist of verbal materials that mark opposite ends of a
</p>
<p>semantic dimension (e.g., good and bad). However, the anchors may also be
</p>
<p>pictures, or even sound files. Visual anchors, such as smileys, are often used with
</p>
<p>participants who may not fully grasp the meaning of verbal materials&mdash;for example,
</p>
<p>preschool children (Funke 2010). Compared to ordinal scales, measurement by
</p>
<p>means of visual analogue scales is more exact, leading to more narrow confidence
</p>
<p>intervals, and higher power statistical tests. This exactness helps detect smaller
</p>
<p>effect that may be unobservable with ordinal scales (Reips and Funke 2008).
</p>
<p>However, although not all web survey platforms offer visual analogue scales, you
</p>
<p>should use them if possible. Figure 4.10 shows an example of a visual analogue
</p>
<p>scale that measures customers&rsquo; expectations of Oddjob Airlines.
</p>
<p>Choose between a forced-choice scale and a free-choice scale: Sometimes,
</p>
<p>researchers and practitioners use 4-point or 6-point scales that omit the neutral
</p>
<p>Fig. 4.10 Example of a visual analogue scale
</p>
<p>4.4 Conducting Primary Data Research 75</p>
<p/>
</div>
<div class="page"><p/>
<p>category, thereby forcing the respondents to be positive or negative. Using such a
</p>
<p>forced-choice scale could bias the answers, leading to validity issues.7 By
</p>
<p>providing a neutral category choice (i.e., a free-choice scale), the respondents are
</p>
<p>not forced to give a positive or negative answer. Many respondents feel more
</p>
<p>comfortable about participating in a survey using a free-choice scales (Nowlis
</p>
<p>et al. 2002). Furthermore, research has shown that including a neutral category
</p>
<p>minimizes response bias (Weijters et al. 2010). Therefore, we strongly suggest
</p>
<p>using free-choice scales and including a neutral category.
</p>
<p>Design the response categories: When designing response categories for cate-
</p>
<p>gorical variables, use response categories that are exclusive, so that answers do not
</p>
<p>overlap (e.g., age categories 0&ndash;4, 5&ndash;10, etc.). The question here is: How do we
</p>
<p>decide on the spacing between the categories? For example, should we divide US
</p>
<p>household income in the categories 0&ndash;$9,999, $10,000&ndash;$19,999, $20,000-higher,
</p>
<p>or use another way to set the categories? One suggestion is to use narrower
</p>
<p>categories if the respondent can recall the variable easily. A second suggestion is
</p>
<p>to space the categories so that we, as researchers, expect an approximately equal
</p>
<p>number of observations per category. In the example above, we may find that most
</p>
<p>households have an income of $20,000 or higher and that categories 0&ndash;$9,999 and
</p>
<p>$10,000&ndash;$19,999 are infrequently used. It is best to choose categories where equal
</p>
<p>percentages are expected such as 0&ndash;$24,999, $25,000&ndash;$44,999, $45,000&ndash;$69,999,
</p>
<p>$70,000&ndash;$109,999, $110,000&ndash;and higher. Although the range in each category
</p>
<p>differs, we can reasonably expect each category to hold about 20% of the responses
</p>
<p>if we sample US households randomly (https://en.wikipedia.org/wiki/Household_
</p>
<p>income_in_the_United_States#Household_income_over_time). Box 4.4 shows a
</p>
<p>real-life example of oddly chosen response categories (and there are a few other
</p>
<p>issues too!).
</p>
<p>Box 4.4 Oddly Chosen Response Categories
</p>
<p>7Forced-choice scales can, of course, also be used for uneven response categories such as 5-point
or 7-point Likert scales.
</p>
<p>76 4 Getting Data</p>
<p/>
<div class="annotation"><a href="https://doi.org/wikipedia.org/wiki/Household_income_in_the_United_States#Household_income_over_time">https://doi.org/wikipedia.org/wiki/Household_income_in_the_United_States#Household_income_over_time</a></div>
<div class="annotation"><a href="https://doi.org/wikipedia.org/wiki/Household_income_in_the_United_States#Household_income_over_time">https://doi.org/wikipedia.org/wiki/Household_income_in_the_United_States#Household_income_over_time</a></div>
</div>
<div class="page"><p/>
<p>Note that the response categories also give the respondents the range of accept-
</p>
<p>able answers. Respondents generally view the middle of the scale as normal, or
</p>
<p>most common, and position themselves in relation to this (e.g., Revilla 2015). For
</p>
<p>example, Tourangeau and Smith (1996) found that the reported number of sexual
</p>
<p>partners in an open-ended question is more than twice as high when the answer
</p>
<p>category labels used in a previously asked closed-ended question on the same topic
</p>
<p>shifted to indicate higher numbers (from 0, 1, 2, 3, 4, 5 or more to 0, 1&ndash;4, 5&ndash;9,
</p>
<p>10&ndash;49, 50&ndash;99, 100 or more).
</p>
<p>Label the response categories:A common way of labeling response categories is
</p>
<p>to use endpoint labels only, omitting intermediary labels. For example, instead of
</p>
<p>labeling all five points of a Likert scale (e.g., &ldquo;completely disagree,&rdquo; &ldquo;disagree,&rdquo;
</p>
<p>&ldquo;neutral,&rdquo; &ldquo;agree,&rdquo; and &ldquo;completely agree&rdquo;), we only label the endpoints (e.g.,
</p>
<p>&ldquo;completely disagree&rdquo; and &ldquo;completely agree&rdquo;). While this approach makes
</p>
<p>response category labeling easy, it also amplifies acquiescence, because the
</p>
<p>endpoints reinforce the respondents&rsquo; agreement (Weijters et al. 2010). Conversely,
</p>
<p>if all the categories are labeled, this helps the respondents interpret and differenti-
</p>
<p>ate, making the midpoint more salient and accessible. Labeling the response
</p>
<p>categories fully also increases the scale reliability (Weng 2004), but reduces
</p>
<p>criterion validity. Drawing on Weijters et al. (2010), we generally recommend
</p>
<p>only labeling the endpoints. However, when using the items for prediction-type
</p>
<p>analyses, such as in correlation and regression analyses, it is beneficial to label all
</p>
<p>the categories.8
</p>
<p>Decide whether or not to use a &ldquo;don&rsquo;t know&rdquo; option: Another important choice to
</p>
<p>make is whether or not to include a &ldquo;don&rsquo;t know&rdquo; option in the scaling (Lietz 2010).
</p>
<p>Using a &ldquo;don&rsquo;t know&rdquo; option allows the researcher to distinguish between those
</p>
<p>respondents who have a clear opinion and those who do not. Moreover, the
</p>
<p>respondents may find that answering the survey is slightly easier. While these are
</p>
<p>good reasons for including this category, the drawback is that there will then be
</p>
<p>missing observations. Ifmany respondents choose not to answer, thiswill substantially
</p>
<p>reduce the number of surveys that can be used for analysis. Generally, when designing
</p>
<p>surveys, you should only include a &ldquo;don&rsquo;t know&rdquo; (or &ldquo;undecided&rdquo;) option as an answer
</p>
<p>to questions that the respondents might genuinely not know, for example, when
</p>
<p>requiring answers to factual questions. If included, the option should appear at the
</p>
<p>end of the scale. The &ldquo;don&rsquo;t know&rdquo; option should not be included in other types of
</p>
<p>questions (such as on attitudes or preferences), as researchers are interested in the
</p>
<p>respondents&rsquo; perceptions regardless of their knowledge of the subject matter.
</p>
<p>Choose between a balanced and unbalanced scale: A balanced scale has an
</p>
<p>equal number of positive and negative scale categories. For example, in a 5-point
</p>
<p>Likert scale, we may have two negative categories (&ldquo;completely disagree&rdquo; and
</p>
<p>&ldquo;disagree&rdquo;), a neutral option, and two positive categories (&ldquo;agree&rdquo; and &ldquo;completely
</p>
<p>8Sometimes, researchers number the response options. For example, when numbering the response
options of a 7-point scale you can use only positive numbers (1&ndash;7), or positive and negative
numbers (�3 to &thorn;3). For recommendations, see Cabooter et al. (2016).
</p>
<p>4.4 Conducting Primary Data Research 77</p>
<p/>
</div>
<div class="page"><p/>
<p>agree&rdquo;). Besides this, the wording in a balanced scale should reflect equal distances
</p>
<p>between the scale items. This is called an equidistant scale, which is a requirement
</p>
<p>for analysis techniques such as factor analysis (Chap. 8). Consequently, we strongly
</p>
<p>recommend using a balanced scale instead of an unbalanced scale. A caveat of
</p>
<p>balanced scales is that many constructs cannot have negative values. For example,
</p>
<p>one can have some trust in a company or very little trust, but negative trust is highly
</p>
<p>unlikely. If a scale item cannot be negative, you will have to resort to an unbalanced
</p>
<p>scale in which the endpoints of the scales are unlikely to be exact opposites.
</p>
<p>Table 4.1 summarizes the key choices we have to make when designing surveys.
</p>
<p>4.4.2.5 Design the Questionnaire
After determining the individual questions, you have to integrate these, together
</p>
<p>with other elements, to create the questionnaire. Questionnaire design involves the
</p>
<p>following elements:
</p>
<p>&ndash; designing the starting pages of the questionnaire,
</p>
<p>&ndash; choosing the order of the questions, and
</p>
<p>&ndash; designing the layout and format.
</p>
<p>Starting pages of the questionnaire: At the beginning of each questionnaire, the
</p>
<p>importance and goal are usually described to stress that the results will be treated
</p>
<p>confidentially, and to mention what they will be used for. This is usually followed
</p>
<p>by an example question (and answer), to demonstrate how the survey should be
</p>
<p>filled out. Keep this page very short when using mobile surveys.
</p>
<p>If questions relate to a specific issue, moment, or transaction, you should indicate
</p>
<p>this clearly at the very beginning. For example, &ldquo;Please provide answers to the
</p>
<p>following questions, keeping the purchase of product X in mind.&rdquo; If applicable, you
</p>
<p>should also point out that your survey is conducted in collaboration with a univer-
</p>
<p>sity, a recognized research institute, or a known charity, as this generally increases
</p>
<p>respondents&rsquo; willingness to participate. Moreover, do not forget to provide a name
</p>
<p>and contact details for those participants who have questions, or if technical
</p>
<p>problems should arise. Consider including a photo of the research team, as this
</p>
<p>increases response rates. Lastly, you should thank the respondents for their time and
</p>
<p>describe how the questionnaire should be returned (for mail surveys).
</p>
<p>Order of the questions: Choosing an appropriate question order is crucial,
</p>
<p>because it determines the questionnaire&rsquo;s logical flow and therefore contributes to
</p>
<p>high response rates. The order of questions is usually as follows:
</p>
<p>1. Screening questions (typically simply referred to as screeners) come first. These
</p>
<p>questions determine what parts of the survey a respondent should fill out.
</p>
<p>2. Next, ask questions relating to the study&rsquo;s key variables. This includes the
</p>
<p>dependent variables, followed by the independent variables.
</p>
<p>3. Use a funnel approach. That is, ask questions that are more general first and then
</p>
<p>move on to details. This makes answering the questions easier as the order helps
</p>
<p>the respondents recall. Make sure that sensitive questions are put at the very end
</p>
<p>of this section.
</p>
<p>78 4 Getting Data</p>
<p/>
</div>
<div class="page"><p/>
<p>Table 4.1 A summary of some of the key choices when designing survey items
</p>
<p>Aspect Recommendation
</p>
<p>Item content
</p>
<p>Can all the respondents answer the
question asked?
</p>
<p>Ensure that all potential respondents can answer all
items. If they cannot, ask screener questions to direct
them. If the respondents cannot answer questions,
they should be able to skip them.
</p>
<p>Can the respondents construct or recall
the answers?
</p>
<p>If the answer is no, you should use other methods to
obtain information (e.g., secondary data or
observations).
Moreover, you should ask the respondents about
major aspects before zooming in on details to help
them recall answers.
</p>
<p>Do the respondents want to answer
each question?
</p>
<p>If the questions concern &ldquo;sensitive&rdquo; subjects, check
whether they can be omitted. If not, stress the
confidentiality of the answers and mention why these
answers are useful for the researcher, the respondent,
or society before introducing them.
</p>
<p>Open-ended or closed-ended
questions?
</p>
<p>Keep the subsequent coding in mind. If easy coding
is possible beforehand, design a set of exhaustive
answer categories. Further, remember that open-
ended scale items have a much lower response rate
than closed-ended items.
</p>
<p>Item wording
</p>
<p>Grammar and sentences Use simple wording and grammar. Keep sentences
short. Avoid negations, vague quantifiers, and
double-barreled questions. Employ reverse-scaled
items sparingly. Use back-translation if necessary.
</p>
<p>Type of scale
</p>
<p>Number of response categories Use visual analogue scales. If not available, use
5-point or 7-point scales.
</p>
<p>Forced-choice or free-choice scale? Use a free-choice scale by including a neutral
category.
</p>
<p>Design of the response categories Ensure that the response categories are exclusive and
that each category has approximately the same
percentage of responses.
</p>
<p>What scaling categories should you use
(closed-ended questions only)?
</p>
<p>Use visual analogue scales if possible, otherwise
Likert scales. If the question requires this, use
semantic differential scales, or rank order scales.
</p>
<p>Labeling of response categories Label endpoints only. When using the items for
prediction-type analyses, label all categories.
</p>
<p>Inclusion of a &ldquo;Don&rsquo;t know&rdquo; option Include a &ldquo;Don&rsquo;t know&rdquo; option only for items that the
respondent might genuinely not know. If included,
place this at the end of the scale.
</p>
<p>Balanced or unbalanced scale? Always use a balanced scale. There should be an
exact number of positive and negative wordings in
the scale items. The words at the ends of the scale
should be exact opposites.
</p>
<p>4.4 Conducting Primary Data Research 79</p>
<p/>
</div>
<div class="page"><p/>
<p>4. Questions related to demographics are placed last if they are not part of the
</p>
<p>screening questions. If you ask demographic questions, always check whether
</p>
<p>they are relevant to the research goal and if they are not already known.9 In
</p>
<p>addition, check if these demographics are likely to lead to non-response. Asking
</p>
<p>about demographics, like income, educational attainment, or health, may result
</p>
<p>in a substantial number of respondents refusing to answer. If such sensitive
</p>
<p>demographics are not necessary, omit them from the survey. Note that in certain
</p>
<p>countries asking about a respondent&rsquo;s demographic characteristics means you
</p>
<p>must adhere to specific laws, such as the Data Protection Act 1998 in the UK.
</p>
<p>If your questionnaire comprises several sections (e.g., in the first section, you ask
</p>
<p>about the respondents&rsquo; buying attitudes and, in the following section, about their
</p>
<p>satisfaction with the company&rsquo;s services), you should make the changing context
</p>
<p>clear to the respondents.
</p>
<p>Layout and format of the survey: The layout of both mail and web-based surveys
</p>
<p>should be concise and should conserve space where possible, particularly in respect
</p>
<p>of mobile surveys. Avoid using small and colored fonts, which reduce readability.
</p>
<p>Booklets work well for mail-based surveys, since postage is cheaper if surveys fit in
</p>
<p>standard envelopes. If this is not possible, single-sided stapled paper can also work.
</p>
<p>When using web-based surveys, it is good to have a counter that tells the
</p>
<p>respondents what percentage of the questions they have already filled out. This
</p>
<p>gives them some indication of how much time they are likely to need to complete
</p>
<p>the survey. Make sure the layout is simple and compatible with mobile devices and
</p>
<p>tablets. Qualtrics and other survey tools offer mobile-friendly display options,
</p>
<p>which should always be ticked.
</p>
<p>4.4.2.6 Pretest the Questionnaire and Execution
We have already mentioned the importance of pretesting the survey several times.
</p>
<p>Before any survey is sent out, you should pretest the questionnaire to enhance its
</p>
<p>clarity and to ensure the client&rsquo;s acceptance of the survey. Once the questionnaire is
</p>
<p>in the field, there is no way back! You can pretest questionnaires in two ways. In its
</p>
<p>simplest form, you can use a few experts (say 3&ndash;6) to read the survey, fill it out, and
</p>
<p>comment on it. Many web-based survey tools allow researchers to create a pretested
</p>
<p>version of their survey, in which there is a text box for comments behind every
</p>
<p>question. Experienced market researchers can spot most issues right away and
</p>
<p>should be employed to pretest surveys. If you aim for a very high quality survey,
</p>
<p>you should also send out a set of preliminary (but proofread) questionnaires to a
</p>
<p>small sample of 50&ndash;100 respondents. The responses (or lack thereof) usually
</p>
<p>indicate possible problems and the preliminary data can be analyzed to determine
</p>
<p>the potential results. Never skip pretesting due to time issues, since you are likely to
</p>
<p>run into problems later!
</p>
<p>9The demographics of panels (see Sect. 4.4.2.2) are usually known.
</p>
<p>80 4 Getting Data</p>
<p/>
</div>
<div class="page"><p/>
<p>Box 4.5 Dillman et al.&rsquo;s (2014) Recommendations on How to Increase Response
</p>
<p>Rates
</p>
<p>It is becoming increasingly difficult to get people to fill out surveys. This may
</p>
<p>be due to over-surveying, dishonest firms that disguise sales as research, and a
</p>
<p>lack of time. Dillman et al. (2014) discuss four steps to increase response
</p>
<p>rates:
</p>
<p>1. Send out a pre-notice letter indicating the importance of the study and
</p>
<p>announcing that a survey will be sent out shortly.
</p>
<p>2. Send out the survey with a sponsor letter, again indicating the importance
</p>
<p>of the study.
</p>
<p>3. Follow up after 3&ndash;4 weeks with a thank you note (for those who
</p>
<p>responded) and the same survey, plus a reminder (for those who did not
</p>
<p>respond).
</p>
<p>4. Call or email those who have still not responded and send out a thank you
</p>
<p>note to those who replied during the second round.
</p>
<p>Motivating potential respondents to participate is an increasingly important
</p>
<p>aspect of survey research. In addition to Dillman et al.&rsquo;s (2014) recommendations
</p>
<p>on how to increase response rates (Box 4.5), incentives are used. A simple example
</p>
<p>of such an incentive is to provide potential respondents with a cash reward. In the
</p>
<p>US, one-dollar bills are often used for this purpose. Respondents who participate in
</p>
<p>(online) research panels often receive points that can be exchanged for products and
</p>
<p>services. For example, Research Now, a market research company, gives its
</p>
<p>Canadian panel members AirMiles that can be exchanged for, amongst others,
</p>
<p>free flights. A special type of incentive is to indicate that some money will be
</p>
<p>donated to a charity for every returned survey. ESOMAR, the world organization
</p>
<p>for market and social research (see Chap. 10), suggests that incentives for
</p>
<p>interviews or surveys should &ldquo;be kept to a minimum level proportionate to the
</p>
<p>amount of their time involved, and should not be more than the normal hourly fee
</p>
<p>charged by that person for their professional consultancy or advice.&rdquo;
</p>
<p>Another incentive is to give the participants a chance to win a product or service.
</p>
<p>For example, you could randomly give a number of participants gifts. The
</p>
<p>participants then need to disclose their name and address in exchange for a chance
</p>
<p>to win a gift so that they can be reached. While this is not part of the research itself,
</p>
<p>some respondents may feel uncomfortable providing their contact details, which
</p>
<p>could potentially reduce the response rate.
</p>
<p>Finally, reporting the findings to the participants is an incentive that may help
</p>
<p>participation (particularly in professional settings). This can be done by providing a
</p>
<p>general report of the study and its findings, or by providing a customized report
</p>
<p>detailing the participant&rsquo;s responses and comparing them with all the other
</p>
<p>responses (Winkler et al. 2015). Obviously, anonymity needs to be assured so
</p>
<p>4.4 Conducting Primary Data Research 81</p>
<p/>
</div>
<div class="page"><p/>
<p>that the participants cannot compare their answers to those of other individual
</p>
<p>responses.
</p>
<p>4.5 Basic Qualitative Research
</p>
<p>Qualitative research is mostly used to gain an understanding of why certain things
</p>
<p>happen. It can be used in an exploratory context by defining problems in more
</p>
<p>detail, or by developing hypotheses to be tested in subsequent research. Qualitative
</p>
<p>research also allows researchers to learn about consumers&rsquo; perspectives and vocab-
</p>
<p>ulary, especially if they are not familiar with the context (e.g., the industry). As
</p>
<p>such, qualitative research offers importance guidance when little is known about
</p>
<p>consumers&rsquo; attitudes and perceptions or the market (Barnham 2015).
</p>
<p>As discussed in Chap. 3, qualitative research leads to the collection of qualitative
</p>
<p>data. One can collect qualitative data by explicitly informing the participants that
</p>
<p>you are doing research (directly observed qualitative data), or you can simple
</p>
<p>observe the participants&rsquo; behavior without them being explicitly aware of the
</p>
<p>research goals (indirectly observed qualitative data). There are ethical issues
</p>
<p>associated with conducting research if the participants are not aware of the research
</p>
<p>purpose. Always check the regulations regarding what is allowed in your context
</p>
<p>and what not. It is, in any case, good practice to brief the participants on their role
</p>
<p>and the goal of the research once the data have been collected.
</p>
<p>The two key forms of directly observed qualitative data are in-depth interviews
</p>
<p>and focus groups. Together, they comprise most of the conducted qualitative
</p>
<p>market research. First, we will discuss in-depth interviews, which are&mdash;as the
</p>
<p>terms suggests&mdash;interviews conducted with one participant at a time, allowing for
</p>
<p>high levels of personal interaction between the interviewer and respondent. Next,
</p>
<p>we will discuss projective techniques, a frequently used type of testing procedure in
</p>
<p>in-depth interviews. Lastly, we will introduce focus group discussions, which are
</p>
<p>conducted with multiple participants.
</p>
<p>4.5.1 In-Depth Interviews
</p>
<p>In-depth interviews are qualitative conversations with participants on a specific
</p>
<p>topic. These participants are often consumers, but, in a market research study, they
</p>
<p>may also be the decision-makers, who are interviewed to gain an understanding of
</p>
<p>their clients&rsquo; needs. They may also be government or company representatives. The
</p>
<p>structure levels of interviews vary. In their simplest form, interviews are unstruc-
</p>
<p>tured and the participants talk about a topic in general. This works well if you want
</p>
<p>to obtain insight into a topic, or as an initial step in a research process. Interviews
</p>
<p>can also be fully structured, meaning all the questions and possible answer
</p>
<p>categories are decided beforehand. This way allows you to collect quantitative
</p>
<p>data. However, most in-depth interviews for gathering qualitative data are semi-
</p>
<p>structured and contain a series of questions that need to be addressed, but have no
</p>
<p>82 4 Getting Data</p>
<p/>
</div>
<div class="page"><p/>
<p>specific format regarding what the answers should look like. The person
</p>
<p>interviewed can make additional remarks, or discuss somewhat related issues, but
</p>
<p>is not allowed to wander off too far. In these types of interviews, the interviewer
</p>
<p>often asks questions like &ldquo;that&rsquo;s interesting, could you explain?,&rdquo; or &ldquo;how
</p>
<p>come. . .?&rdquo; to gain more insight into the issue. In highly structured interviews, the
</p>
<p>interviewer has a fixed set of questions and often a fixed amount of time for each
</p>
<p>person&rsquo;s response. The goal of structured interviews is to maximize the compara-
</p>
<p>bility of the answers. Consequently, the set-up of the questions and the structure of
</p>
<p>the answers need to be similar.
</p>
<p>In-depth interviews are unique in that they allow probing on a one-to-one basis,
</p>
<p>thus fostering an interaction between the interviewer and interviewee. In-depth
</p>
<p>interviews also work well when those being interviewed have very little time and
</p>
<p>when they do not want the information to be shared with the other study
</p>
<p>participants. This is, for example, probably the case when you discuss marketing
</p>
<p>strategy decisions with CEOs. The drawbacks of in-depth interviews include the
</p>
<p>amount of time the researcher needs to spend on the interview itself and on
</p>
<p>traveling (if the interview is conducted face-to-face and not via the telephone), as
</p>
<p>well as on transcribing the interview.
</p>
<p>When conducting in-depth interviews, a set format is usually followed. First, the
</p>
<p>interview details are discussed, such as confidentiality issues, the interview topic,
</p>
<p>the structure, and the duration. Moreover, the interviewer should disclose whether
</p>
<p>the interview is being recorded and inform the interviewee that there is no right or
</p>
<p>wrong answer, just opinions on the subject. The interviewer should also try to be
</p>
<p>open and maintain eye contact with the interviewee. Interviewers can end an
</p>
<p>interview by informing their respondents that they have reached the last question
</p>
<p>and thanking them for their time.
</p>
<p>Interviews are often used to investigate means-end issues, in which
</p>
<p>researchers try to understand what ends consumers aim to satisfy and which
</p>
<p>means (consumption) they use to do so. A means-end approach involves
</p>
<p>first determining a product&rsquo;s attributes. These are the functional product
</p>
<p>features, such as the speed a car can reach or its acceleration. Subsequently,
</p>
<p>researchers look at the functional consequences that follow from the product
</p>
<p>benefits. This could be driving fast. The psychosocial consequences, or
</p>
<p>personal benefits, are derived from the functional benefits and, in this exam-
</p>
<p>ple, could include an enhanced status, or being regarded as successful.
</p>
<p>Finally, the psychosocial benefits are linked to people&rsquo;s personal values or
</p>
<p>life goals, such as a desire for success or acceptance. Analyzing and
</p>
<p>identifying the relationships between these steps is called laddering.
</p>
<p>4.5 Basic Qualitative Research 83</p>
<p/>
</div>
<div class="page"><p/>
<p>4.5.2 Projective Techniques
</p>
<p>Projective techniques describe a special type of testing procedure, usually used as
</p>
<p>part of in-depth interviews. These techniques provide the participants with a
</p>
<p>stimulus and then gauge their responses. Although participants in projective
</p>
<p>techniques know that they are participating in a market research study, they may
</p>
<p>not be aware of the research&rsquo;s specific purpose. The stimuli that the projective
</p>
<p>techniques provide are ambiguous and require a response from the participants.
</p>
<p>Sentence completion is a key form of projective techniques, for example:
</p>
<p>An iPhone user is someone who: ...................................
The Apple brand makes me think of: ...................................
iPhones are most liked by: ...................................
</p>
<p>In this example, the respondents are asked to express their feelings, ideas, and
</p>
<p>opinions in a free format.
</p>
<p>Projective techniques&rsquo; advantage is that they allow for responses when people
</p>
<p>are unlikely to respond if they were to know the study&rsquo;s exact purpose. Thus,
</p>
<p>projective techniques can overcome self-censoring and allow expression and fan-
</p>
<p>tasy. In addition, they can change a participant&rsquo;s perspective. Think of the previous
</p>
<p>example. If the participants are iPhone users, the sentence completion example asks
</p>
<p>them how they think other people regard them, not what they think of the iPhone. A
</p>
<p>drawback is that projective techniques require the responses to be interpreted and
</p>
<p>coded, which can be difficult.
</p>
<p>4.5.3 Focus Groups
</p>
<p>Focus groups are interviews conducted with a number of respondents at the same
</p>
<p>time and led by a moderator. This moderator leads the interview, structures it, and
</p>
<p>often plays a central role in transcribing the interview later. Focus groups are
</p>
<p>usually semi or highly structured. The group usually comprises between 4 and
</p>
<p>6 people to allow for interaction between the participants and to ensure that all the
</p>
<p>participants have a say. The duration of a focus group interview varies, but is often
</p>
<p>between 30 and 90 min for company employee focus groups and between 60 and
</p>
<p>120 min for consumers. When focus groups are held with company employees,
</p>
<p>moderators usually travel to the company and conduct their focus group in a room.
</p>
<p>When consumers are involved, moderators often travel to a market research com-
</p>
<p>pany, or hotel, where the focus group meets in a conference room. Market research
</p>
<p>companies often have specially equipped conference rooms with, for example,
</p>
<p>one-way mirrors, built-in microphones, and video recording devices.
</p>
<p>How are focus groups structured? They usually start with the moderator
</p>
<p>introducing the topic and discussing the background. Everyone in the group is
</p>
<p>introduced to all the others to establish rapport. Subsequently, the moderator
</p>
<p>encourages the members of the focus group to speak to one another, instead of
</p>
<p>asking the moderator for confirmation. Once the focus group members start
</p>
<p>84 4 Getting Data</p>
<p/>
</div>
<div class="page"><p/>
<p>discussing topics with one another, the moderator tries to stay in the background,
</p>
<p>merely ensuring that the discussions stay on-topic. Afterwards, the participants are
</p>
<p>briefed and the discussions are transcribed for further analysis.
</p>
<p>Focus groups have distinct advantages: they are relatively cheap compared to
</p>
<p>in-depth interviews, work well with issues that are socially important, or which
</p>
<p>require spontaneity. They are also useful for developing new ideas. On the down-
</p>
<p>side, focus groups do not offer the same opportunity for probing as interviews do,
</p>
<p>and also run a greater risk of going off-topic. Moreover, a few focus group members
</p>
<p>may dominate the discussion and, especially in larger focus groups, &ldquo;voting&rdquo;
</p>
<p>behavior may occur, hindering real discussions and the development of new
</p>
<p>ideas. Table 4.2 summarizes the key differences between focus groups and
</p>
<p>in-depth interviews.
</p>
<p>Table 4.2 Comparing focus groups and in-depth interviews
</p>
<p>Focus groups In-depth interviews
</p>
<p>Group
interactions
</p>
<p>Group interaction, which may
stimulate the respondents to produce
new thoughts.
</p>
<p>There is no group interaction. The
interviewer is responsible for
stimulating the respondents to
produce new ideas.
</p>
<p>Group/peer
pressure
</p>
<p>Group pressure and stimulation may
clarify and challenge thinking.
</p>
<p>In the absence of group pressure, the
respondents&rsquo; thinking is not
challenged.
</p>
<p>Peer pressure and role playing. With just one respondent, role playing
is minimized and there is no peer
pressure.
</p>
<p>Respondent
competition
</p>
<p>Respondents compete with one
another for time to talk. There is less
time to obtain in-depth details from
each participant.
</p>
<p>Individuals are alone with the
interviewer and can express their
thoughts in a non-competitive
environment. There is more time to
obtain detailed information.
</p>
<p>Peer
influence
</p>
<p>Responses in a group may be biased
by other group members&rsquo; opinions.
</p>
<p>With one respondent, there is no
potential of other respondents
influencing this person.
</p>
<p>Subject
sensitivity
</p>
<p>If the subject is sensitive, respondents
may be hesitant to talk freely in the
presence of other people.
</p>
<p>If the subject is sensitive, respondents
may be more likely to talk.
</p>
<p>Stimuli The volume of stimulus materials that
can be used is somewhat limited.
</p>
<p>A fairly large amount of stimulus
material can be used.
</p>
<p>Interviewer
schedule
</p>
<p>It may be difficult to assemble 8 or
10 respondents if they are a difficult
type to recruit (such as busy
executives).
</p>
<p>Individual interviews are easier to
schedule.
</p>
<p>4.5 Basic Qualitative Research 85</p>
<p/>
</div>
<div class="page"><p/>
<p>4.6 Collecting Primary Data Through Experimental Research
</p>
<p>In Chap. 2, we discussed causal research and briefly introduced experiments as a
</p>
<p>means of conducting research. The goal of experiments is to avoid unintended
</p>
<p>influences through the use of randomization. Experiments are typically conducted
</p>
<p>by manipulating one variable, or a few, at a time. For example, we can change the
</p>
<p>price of a product, the type of product, or the package size to determine whether
</p>
<p>these changes affect important outcomes such as attitudes, satisfaction, or
</p>
<p>intentions. Simple field observations are often unable to establish these
</p>
<p>relationships, as inferring causality is problematic. Imagine a company that wants
</p>
<p>to introduce a new type of soft drink aimed at health-conscious consumers. If the
</p>
<p>product were to fail, the managers would probably conclude that the consumers did
</p>
<p>not like the product. However, many (usually unobserved) variables, such as
</p>
<p>competitors&rsquo; price cuts, changing health concerns, and a lack of availability, can
</p>
<p>also influence new products&rsquo; success.
</p>
<p>4.6.1 Principles of Experimental Research
</p>
<p>Experiments deliberately impose one or more treatments and then observe the
</p>
<p>outcome of a specific treatment (Mitchell and Jolley 2013). This way, experiments
</p>
<p>attempt to isolate how one change affects an outcome. The outcome(s) is (are) the
</p>
<p>dependent variable(s) and the independent variable(s) (also referred to as factors)
</p>
<p>are used to explain the outcomes. To examine the influence of the independent
</p>
<p>variable(s) on the dependent variable(s), the participants are subjected to
</p>
<p>treatments. These are supposed to manipulate the participants by putting them in
</p>
<p>different situations. A simple form of treatment could be an advertisement with and
</p>
<p>without humor. In this case, the humor is the independent variable, which can take
</p>
<p>two levels (i.e., with or without humor). If we manipulate, for example, the price
</p>
<p>between low, medium, and high, we have three levels. When selecting independent
</p>
<p>variables, we normally include those that marketers care about and which are
</p>
<p>related to the marketing and design of the products and services. To assure that
</p>
<p>the participants do indeed perceive differences, manipulation checks are
</p>
<p>conducted. For example, if two different messages with and without humor are
</p>
<p>used as stimuli, we could ask the respondents which of the two is more humorous.
</p>
<p>Such manipulation checks help establish an experiment&rsquo;s validity.
</p>
<p>Care should be taken not to include too many of these variables in order to keep
</p>
<p>the experiment manageable. An experiment that includes four independent
</p>
<p>variables, each of which has three levels and includes every possible combination
</p>
<p>(called a full factorial design) requires 43 &frac14; 64 treatments. Large numbers of levels
</p>
<p>(five or more) will dramatically increase the complexity and cost of the research.
</p>
<p>Finally, extraneous variables, such as the age or income of the experiment partici-
</p>
<p>pant, are not changed as part of the experiment. However, it might be important to
</p>
<p>control for their influence when setting up the experimental design.
</p>
<p>86 4 Getting Data</p>
<p/>
</div>
<div class="page"><p/>
<p>Experiments can run in either a lab or field environment. Lab experiments are
</p>
<p>performed in controlled environments (usually in a company or academic lab),
</p>
<p>thereby allowing for isolating the effects of one or more variables on a certain
</p>
<p>outcome. Extraneous variables&rsquo; influence can also be controlled for. However, field
</p>
<p>experiments&rsquo; often lack internal validity, which is the extent to which we can make
</p>
<p>causal claims based on the study results. Lab experiments often take place in highly
</p>
<p>stylized experimental settings, which typically ignore real-world business
</p>
<p>conditions, they therefore usually lack external validity, which is the extent to
</p>
<p>which the study results can be generalized to similar settings. Field experiments, on
</p>
<p>the other hand, are performed in natural environments (e.g., in a supermarket or
</p>
<p>in-home) and, and such, generally exhibit high degrees of external validity. How-
</p>
<p>ever, since controlling for extraneous variables&rsquo; impact is difficult in natural
</p>
<p>environments, field experiments usually lack internal validity (Gneezy 2017).
</p>
<p>4.6.2 Experimental Designs
</p>
<p>Experimental design refers to an experiment&rsquo;s structure. There are various types
</p>
<p>of experimental designs. To clearly separate the different experimental designs,
</p>
<p>researchers have developed the following notation:
</p>
<p>O: A formal observation or measurement of the dependent variable. Subscripts below an
observation O such as 1 or 2, indicate measurements at different points in time.
</p>
<p>X: The test participants&rsquo; exposure to an experimental treatment.
</p>
<p>R: The random assignment of participants. Randomization ensures control over extraneous
variables and increases the experiment&rsquo;s reliability and validity.
</p>
<p>In the following, we will discuss the most prominent experimental designs:
</p>
<p>&ndash; one-shot case study,
</p>
<p>&ndash; before-after design,
</p>
<p>&ndash; before-after design with a control group, and
</p>
<p>&ndash; solomon four-group design.
</p>
<p>The simplest form of experiment is the one-shot case study. This type of
</p>
<p>experiment is structured as follows:10
</p>
<p>X O1
</p>
<p>This means we have one treatment (indicated by X), such as a new advertising
</p>
<p>campaign. After the treatment, we await reactions to it and then measure the
</p>
<p>outcome of the manipulation (indicated by O1), such as the participants&rsquo; willingness
</p>
<p>10If one symbol precedes another, it means that the first symbol precedes the next one in time.
</p>
<p>4.6 Collecting Primary Data Through Experimental Research 87</p>
<p/>
</div>
<div class="page"><p/>
<p>to purchase the advertised product. This type of experimental set-up is common, but
</p>
<p>does not tell us if the effect is causal. One reason for this is that we did not measure
</p>
<p>anything before the treatment and therefore cannot assess what the relationship
</p>
<p>between the treatment and outcome is. The participants&rsquo; willingness to purchase the
</p>
<p>product was perhaps higher before they were shown the advertisement, but since we
</p>
<p>did not measure their willingness to purchase before the treatment, this cannot be
</p>
<p>ruled out. Consequently, this design does not allow us to establish causality.
</p>
<p>The simplest type of experiment that allows us to make causal inferences&mdash;
</p>
<p>within certain limits&mdash;is the before-after design used for one group. The notation
</p>
<p>for this design is:
</p>
<p>O1 X O2
</p>
<p>We have one measurement before (O1) and one after a treatment (O2). Thus, this
</p>
<p>type of design can be used to determine whether an advertisement has a positive,
</p>
<p>negative, or no effect on the participants&rsquo; willingness to purchase a product.
</p>
<p>A problem with this type of design is that we do not have a standard of
</p>
<p>comparison with which to contrast the change between O1 and O2. While the
</p>
<p>advertisement may have increased the participants&rsquo; willingness to purchase, this
</p>
<p>might have been even higher if they had not seen the advertisement. The reason for
</p>
<p>this is that the before-after-design does not control for influences occurring between
</p>
<p>the two measurements. For example, negative publicity after the initial measure-
</p>
<p>ment could influence the subsequent measurement. These issues make the &ldquo;real&rdquo;
</p>
<p>effect of the advertisement hard to assess.
</p>
<p>If we want to have a much better chance of identifying the &ldquo;real&rdquo; effect of a
</p>
<p>treatment, we need a more complex setup, called the before-after design with a
</p>
<p>control group. In this design, we add a control group who is not subjected to the
</p>
<p>treatment X. The notation of this type of experiment is:
</p>
<p>Experimental group (R) O1 X O2
</p>
<p>Control group (R) O3 O4
</p>
<p>The effect attributed to the experiment is the difference between O1 and O2
minus the difference between O3 and O4. For example, if the participants&rsquo; willing-
</p>
<p>ness to purchase increases much stronger in the experimental group (i.e., O2 is
</p>
<p>much higher than O1) than in the control group (i.e., O4 is slightly higher than or
</p>
<p>equal to O3), the advertisement has had an impact on the participants.
</p>
<p>The random assignment of the participants to the experimental and the control
</p>
<p>groups (indicated by R), is an important element of this experimental design. This
</p>
<p>means that, for any given treatment, every participant has an equal probability of
</p>
<p>being chosen for one of the two groups. This ensures that participants with different
</p>
<p>characteristics are spread randomly (and, it is hoped, equally) between the treat-
</p>
<p>ment(s), which will neutralize self-selection. Self-selection occurs when
</p>
<p>participants can select themselves into either the experimental or the control
</p>
<p>group. For example, if participants who like sweets participate in a cookie tasting
</p>
<p>88 4 Getting Data</p>
<p/>
</div>
<div class="page"><p/>
<p>test, they will certainly try to give the treatment (cookie) a try! See Mooi and
</p>
<p>Gilliland (2013) for an example of self-selection and an analysis method.
</p>
<p>However, the before-after experiment with a control group does have
</p>
<p>limitations. The initial measurement O1 may alert the participants that they are
</p>
<p>being studied, which may bias the post measurement O2. This effect is also referred
</p>
<p>to as the before measurement effect, or the testing effect (Campbell and Stanley
</p>
<p>1966). Likewise, the initial measurement O1 may incite the participants to drop out
</p>
<p>of the experiment, leading to no recorded response for O2. If there is a systematic
</p>
<p>reason for the participants to drop out, this will threaten the experiment&rsquo;s validity.
</p>
<p>The Solomon four-group design is an experimental design accounting for before
</p>
<p>measurement effects and is structured as follows (Campbell and Stanley 1966):
</p>
<p>Experimental group 1 (R) O1 X O2
</p>
<p>Control group 1 (R) O3 O4
</p>
<p>Experimental group 2 (R) X O5
</p>
<p>Control group 2 (R) O6
</p>
<p>The design is much more complex, because we need to measure the effects six
</p>
<p>times and administer two treatments. This method provides an opportunity to
</p>
<p>control for the before measurement effect of O1 on O2. The design also provides
</p>
<p>several measures of the treatment&rsquo;s effect (i.e., (O2&ndash;O4), (O2&ndash;O1)&ndash;(O4&ndash;O3), and
</p>
<p>(O6&ndash;O5)). If these measures agree, the inferences about the treatment&rsquo;s effect are
</p>
<p>much stronger.
</p>
<p>In Chap. 6, we discuss how to analyze experimental data using ANOVA and
</p>
<p>various other tests.
</p>
<p>4.7 Review Questions
</p>
<p>1. What is the difference between primary and secondary data? Can primary data
</p>
<p>become secondary data?
</p>
<p>2. Please search for and download two examples of secondary data sources found
</p>
<p>on the Internet. Discuss two potential market research questions that each dataset
</p>
<p>can answer.
</p>
<p>3. Imagine you are asked to understand what consumer characteristics make these
</p>
<p>consumers likely to buy a BMW i3 (http://www.bmw.com). How would you
</p>
<p>collect the data? Would you start with secondary data, or would you start
</p>
<p>collecting primary data directly? Do you think it is appropriate to collect
</p>
<p>qualitative data? If so, at what stage of the process should you do so?
</p>
<p>4. What are the different reasons for choosing interviews rather than focus groups?
</p>
<p>What choice would you make if you want to understand CEOs&rsquo; perceptions of
</p>
<p>the economy, and what would seem appropriate when you want to understand
</p>
<p>how consumers feel about a newly introduced TV program?
</p>
<p>4.7 Review Questions 89</p>
<p/>
<div class="annotation"><a href="https://doi.org/www.bmw.com">https://doi.org/www.bmw.com</a></div>
</div>
<div class="page"><p/>
<p>5. Consider the following examples of survey items relating to how satisfied
</p>
<p>iPhone users are with their performance, reliability, and after-service. Please
</p>
<p>assess their adequacy and make suggestions on how to revise the items, if
</p>
<p>necessary.
</p>
<p>6. Make recommendations regarding the following aspects of scale design: the
</p>
<p>number of response categories, the design and labeling of response categories,
</p>
<p>and the inclusion of a &ldquo;don&rsquo;t know&rdquo; option.
</p>
<p>7. Describe the Solomon four-group design and explain which of the simpler
</p>
<p>experimental design problems it controls for.
</p>
<p>8. If you were to set up an experiment to ascertain what type of product package
</p>
<p>(new or existing) customers prefer, what type of experiment would you choose?
</p>
<p>Please discuss.
</p>
<p>4.8 Further Readings
</p>
<p>Barnham, C. (2015). Quantitative and qualitative research: Perceptual foundations.
</p>
<p>International Journal of Market Research, 57(6), 837&ndash;854.
</p>
<p>This article reflects on the classic distinction between quantitative and qualitative
</p>
<p>research and the underlying assumptions.
</p>
<p>Campbell, D. T., &amp; Stanley, J. C. (1966). Experimental and quasi-experimental
</p>
<p>designs for research. Chicago: Wadsworth Publishing.
</p>
<p>Strongly
disagree
</p>
<p>Somewhat
disagree
</p>
<p>Somewhat
agree Agree
</p>
<p>Completely
agree
</p>
<p>I am satisfied with the
performance and
reliability of my
iPhone.
</p>
<p>□ □ □ □ □
</p>
<p>Strongly
disagree
</p>
<p>Somewhat
disagree Neutral
</p>
<p>Somewhat
agree
</p>
<p>Completely
agree
</p>
<p>I am satisfied with the
after-sales service of
my iPhone.
</p>
<p>□ □ □ □ □
</p>
<p>Not at all
important
</p>
<p>Not
important Neutral Important
</p>
<p>Very
important
</p>
<p>Which of the following
iPhone features do you
find most important?
</p>
<p>Camera □ □ □ □ □
</p>
<p>Music player □ □ □ □ □
</p>
<p>App store □ □ □ □ □
</p>
<p>Web browser □ □ □ □ □
</p>
<p>Mail □ □ □ □ □
</p>
<p>90 4 Getting Data</p>
<p/>
</div>
<div class="page"><p/>
<p>Cook, T. D., &amp; Campbell, D. T. (1979). Quasi-experimentation: Design and
</p>
<p>analysis issues for field settings. Chicago: Wadsworth Publishing.
</p>
<p>These are the two great books on experimental research.
</p>
<p>Dillman, D. A., Smyth, J. D., &amp; Christian, L. M. (2014). Internet, phone, mail, and
</p>
<p>mixed-mode surveys: The tailored design method (4th ed.). Hoboken: Wiley.
</p>
<p>This book gives an excellent overview of how to create questionnaires and how to
</p>
<p>execute them. Mixed-mode surveys and web surveys are specifically addressed.
</p>
<p>FocusGroupTips.com.
</p>
<p>This website provides a thorough explanation of how to set-up focus groups from
</p>
<p>planning to reporting the results.
</p>
<p>Lietz, P. (2010). Current state of the art in questionnaire design: A review of the
</p>
<p>literature. International Journal of Market Research, 52(2), 249&ndash;272.
</p>
<p>Reviews survey design choices from an academic perspective.
</p>
<p>Mystery Shopping Providers Association (www.mysteryshop.org).
</p>
<p>This website discusses the mystery shopping industry in Asia, Europe, North
</p>
<p>America, and South America.
</p>
<p>Veludo-de-Oliveira, T. M, Ikeda, A. A., &amp; Campomar, M. C. (2006). Laddering in
</p>
<p>the practice of marketing research: Barriers and solutions. Qualitative Market
</p>
<p>Research: An International Journal, 9(3), 297&ndash;306.
</p>
<p>This article provides an overview of laddering, the various forms of laddering, and
</p>
<p>biases that may result and how these should be overcome.
</p>
<p>References
</p>
<p>Barnham, C. (2015). Quantitative and qualitative research: Perceptual foundations. International
Journal of Market Research, 57(6), 837&ndash;854.
</p>
<p>Baumgartner, H., &amp; Steenkamp, J. B. E. M. (2001). Response styles in marketing research: A
cross-national investigation. Journal of Marketing Research, 38(2), 143�156.
</p>
<p>Brace, I. (2004). Questionnaire design. How to plan, structure and write survey material for
effective market research. London: Kogan Page.
</p>
<p>Brislin, R. W. (1970). Back-translation for cross-cultural research. Journal of Cross-Cultural
Psychology, 1(3), 185&ndash;216.
</p>
<p>Bronner, F., &amp; Ton, K. (2007). The live or digital interviewer. A comparison between CASI, CAPI
and CATI with respect to differences in response behaviour. International Journal of Market
Research, 49(2), 167&ndash;190.
</p>
<p>Cabooter, E., Weijters, B., Geuens, M., &amp; Vermeir, E. (2016). Scale format effects on response
option interpretation and use. Journal of Business Research, 69(7), 2574&ndash;2584.
</p>
<p>Casteleyn, J., André, M., &amp; Kris, R. (2009). How to use facebook in your market research.
International Journal of Market Research, 51(4), 439&ndash;447.
</p>
<p>DeMonaco, H. J., Ayfer, A., &amp; Hippel, E. V. (2005). The major role of clinicians in the discovery
of off-label drug therapies. Pharmacotherapy, 26(3), 323&ndash;332.
</p>
<p>Deutskens, E., de Jong, A., de Ruyter, K., &amp; Martin, W. (2006). Comparing the generalizability of
online and mail surveys in cross-national service quality research. Marketing Letters, 17(2),
119&ndash;136.
</p>
<p>Dillman, D. A., Smyth, J. D., &amp; Christian, L. M. (2014). Internet, phone, mail, and mixed-mode
surveys: The tailored design method (4th ed.). Hoboken: Wiley.
</p>
<p>Fink, A. (2003). How to ask survey questions. Thousand Oaks: Sage.
</p>
<p>References 91</p>
<p/>
</div>
<div class="page"><p/>
<p>Foddy, W. (1993). Constructing questions for interviews and questionnaires. Theory and practice
in social science research. Cambridge: Cambridge University Press.
</p>
<p>Funke, F. (2010). Internet-based measurement with visual analogue scales. An experimental
investigation. Dissertation, Eberhard Karls Universität T&uuml;bingen. Available online at: https://
publikationen.uni-tuebingen.de/xmlui/handle/10900/49480
</p>
<p>Holbrook, A., Cho, Y. I. K., &amp; Johnson, T. (2006). The impact of question and respondent
characteristics on comprehension and mapping difficulties. Public Opinion Quarterly, 70(4),
565&ndash;595.
</p>
<p>Houston, M. B. (2002). Assessing the validity of secondary data proxies for marketing constructs.
Journal of Business Research, 57(2), 154&ndash;161.
</p>
<p>Kennedy, C., &amp; Everett, S. E. (2011). Use of cognitive shortcuts in landline and cell phone
surveys. Public Opinion Quarterly, 75(2), 336&ndash;348.
</p>
<p>Kurylko, D. T. (2005). Moonraker project seeks marketing savvy for VW. Automotive News
Europe, 10(17), 22.
</p>
<p>Lietz, P. (2010). Research into questionnaire design. A summary of the literature. International
Journal of Market Research, 52(2), 249&ndash;272.
</p>
<p>Liu, M., Lee, S., &amp; Conrad, F. G. (2016). Comparing extreme response styles between agree-
disagree and item-specific scales. Public Opinion Quarterly, 79(4), 952&ndash;975.
</p>
<p>Lynn, P., &amp; Kaminska, O. (2013). The impact of mobile phones on survey measurement error.
Public Opinion Quarterly, 77(2), 586&ndash;605.
</p>
<p>Mooi, E., &amp; Gilliland, D. I. (2013). How contracts and enforcement explain transaction outcomes.
International Journal of Research in Marketing, 30(4), 395&ndash;405.
</p>
<p>Mitchell, M. L., &amp; Jolley, J. M. (2013). Research design explained (8th ed.). Belmont: Wadsworth.
Nowlis, S. M., Kahn, B. E., &amp; Dhar, R. (2002). Coping with ambivalence: The effect of removing a
</p>
<p>neutral option on consumer attitude and preference judgments. Journal of Consumer Research,
29(3), 319&ndash;334.
</p>
<p>Oppenheim, A. N. (1992). Questionnaire design, interviewing and attitude measurement. London:
Pinter.
</p>
<p>Peng, L., &amp; Finn, A. (2015). Assessing response format effects on the scaling of marketing stimuli.
International Journal of Market Research, 58(4), 595&ndash;619.
</p>
<p>Peterson, R. A. (1997). A quantitative analysis of rating-scale response variability. Marketing
Letters, 8(1), 9&ndash;21.
</p>
<p>Raithel, S., Sarstedt, M., Scharf, S., &amp; Schwaiger, M. (2012). On the value relevance of customer
satisfaction. Multiple drivers in multiple markets. Journal of the Academy of Marketing
Science, 40(4), 509&ndash;525.
</p>
<p>Rammstedt, B., &amp; Krebs, D. (2007). Does response scale format affect the answering of personal-
ity scales? European Journal of Psychological Assessment, 23(1), 32&ndash;38.
</p>
<p>Reips, U.-D., &amp; Funke, F. (2008). Interval-level measurement with visual analogue scales in
Internet-based research: VAS generator. Behavior Research Methods, 40(3), 699&ndash;704.
</p>
<p>Revilla, M. (2015). Effect of using different labels for the scales in a web survey. International
Journal of Market Research, 57(2), 225&ndash;238.
</p>
<p>Rindfleisch, A., &amp; Heide, J. B. (1997). Transaction cost analysis: Past, present, and future
applications. Journal of Marketing, 61(4), 30&ndash;54.
</p>
<p>Schilling, M. A. (2009). Understanding the alliance data. Strategic Management Journal, 30(3),
233&ndash;260.
</p>
<p>Stern, M. J., Bilgen, I., &amp; Dillman, D. A. (2014). The state of survey methodology challenges,
dilemmas, and new frontiers in the era of the tailored design. Field Methods, 26(3), 284&ndash;301.
</p>
<p>Stieglitz, S., Dang-Xuan, L., Bruns, A., &amp; Neuberger, C. (2014). Social media analytics, An
interdisciplinary approach and its implications for information systems. Business &amp; Informa-
tion Systems Engineering, 6(2), 89&ndash;96.
</p>
<p>Swain, S. D., Weathers, D., &amp; Niedrich, R. W. (2008). Assessing three sources of misresponse to
reversed Likert items. Journal of Marketing Research, 45(1), 116�131.
</p>
<p>92 4 Getting Data</p>
<p/>
<div class="annotation"><a href="https://publikationen.uni-tuebingen.de/xmlui/handle/10900/49480">https://publikationen.uni-tuebingen.de/xmlui/handle/10900/49480</a></div>
<div class="annotation"><a href="https://publikationen.uni-tuebingen.de/xmlui/handle/10900/49480">https://publikationen.uni-tuebingen.de/xmlui/handle/10900/49480</a></div>
</div>
<div class="page"><p/>
<p>Tourangeau, R., &amp; Smith, T. W. (1996). Asking sensitive questions: The impact of data collection
mode, question format, and question context. Public Opinion Quarterly, 60(2), 275&ndash;304.
</p>
<p>Vesta Research. (2016). Rules of thumb for survey length. Vesta Research Blog. Available at:
http://www.verstaresearch.com/blog/rules-of-thumb-for-survey-length/
</p>
<p>Vicente, P., &amp; Reis, E. (2010). Marketing research with telephone surveys: Is it time to change?
Journal of Global Marketing, 23(4), 321�332.
</p>
<p>Vincente, P., Reis, E., &amp; Santos, M. (2008). Using mobile phones for survey research. Interna-
tional Journal of Market Research, 51(5), 613&ndash;633.
</p>
<p>Weijters, B., &amp; Baumgartner, H. (2012). Misresponse to reversed and negated items in surveys: A
review. Journal of Marketing Research, 49(5), 737&ndash;747.
</p>
<p>Weijters, B., Cabooter, E., &amp; Schillewaert, N. (2010). The effect of rating scale format on response
styles: The number of response categories and response category labels. International Journal
of Research in Marketing, 27(3), 236&ndash;247.
</p>
<p>Weijters, B., Baumgartner, H., &amp; Schillewaert, N. (2013). Reversed item bias: An integrative
model. Psychological Methods, 18(3), 320&ndash;334.
</p>
<p>Weng, L.-J. (2004). Impact of the number of response categories and anchor labels on coefficient
alpha and test-retest reliability. Educational and Psychological Measurement, 64(6), 956&ndash;972.
</p>
<p>Winkler, T. J., Sarstedt, M., Keil, M., &amp; Rost, P. (2015). Selfsurvey.org: A platform for prediction-
based benchmarking and feedback-enabled survey research. In Proceedings of the European
conference on information systems, Paper 204, M&uuml;nster, Germany.
</p>
<p>References 93</p>
<p/>
<div class="annotation"><a href="https://doi.org/www.verstaresearch.com/blog/rules-of-thumb-for-survey-length/">https://doi.org/www.verstaresearch.com/blog/rules-of-thumb-for-survey-length/</a></div>
</div>
<div class="page"><p/>
<p>Descriptive Statistics 5
</p>
<p>Keywords
</p>
<p>Acquiescence &bull; Aggregation &bull; Bar chart &bull; Bivariate statistics &bull; Box plot &bull;
</p>
<p>Codebook &bull; Construct score &bull; Correlation &bull; Covariance &bull; Crosstabs &bull; Data
</p>
<p>entry errors &bull; Dummy variables &bull; Extreme response styles &bull; Frequency table &bull;
</p>
<p>Histogram &bull; Inconsistent answers &bull; Index &bull; Interquartile range &bull; Interviewer
</p>
<p>fraud &bull; Item non-response &bull; Line chart &bull; Listwise deletion &bull; Little&rsquo;s MCAR test &bull;
</p>
<p>Log transformation &bull; Mean &bull; Measures of centrality &bull; Measures of dispersion &bull;
</p>
<p>Median &bull; Middle response styles &bull; Missing (completely) at random &bull; Missing
</p>
<p>data &bull; Multiple imputation &bull; Non-random missing &bull; Outliers &bull; Pie chart &bull; Range &bull;
</p>
<p>Range standardization &bull; Reverse-scaled items &bull; Scale transformation &bull; Scatter
</p>
<p>plot &bull; Skewed data &bull; Stata &bull; Standard deviation &bull; Standardizing variables &bull;
</p>
<p>Straight-lining &bull; Survey non-response &bull; Suspicious response patterns &bull;
</p>
<p>Transforming data &bull; Univariate statistics &bull; Variable respecification &bull;
</p>
<p>Variance &bull; Workflow &bull; z-standardization
</p>
<p>Learning Objectives
</p>
<p>After reading this chapter, you should understand:
</p>
<p>&ndash; The workflow involved in a market research study.
</p>
<p>&ndash; Univariate and bivariate descriptive graphs and statistics.
</p>
<p>&ndash; How to deal with missing values.
</p>
<p>&ndash; How to transform data (z-transformation, log transformation, creating dummies,
aggregating variables).
</p>
<p>&ndash; How to identify and deal with outliers.
</p>
<p>&ndash; What a codebook is.
</p>
<p>&ndash; The basics of using Stata.
</p>
<p># Springer Nature Singapore Pte Ltd. 2018
E. Mooi et al., Market Research, Springer Texts in Business and Economics,
DOI 10.1007/978-981-10-5218-7_5
</p>
<p>95</p>
<p/>
</div>
<div class="page"><p/>
<p>5.1 The Workflow of Data
</p>
<p>Market research projects involving data become more efficient and effective if
</p>
<p>they have a proper workflow of data, which is a strategy to keep track of the
</p>
<p>entering, cleaning, describing, and transforming of data. These data may have
</p>
<p>been collected through surveys or may be secondary data (Chap. 3). Entering,
</p>
<p>cleaning, and analyzing bits of data haphazardly is not a good strategy, since it
</p>
<p>increases the likelihood of making mistakes and makes it hard to replicate results.
</p>
<p>Moreover, without a good data workflow, it becomes hard to document the
</p>
<p>research process and to cooperate on projects. For example, how can you out-
</p>
<p>source the data analysis if you cannot indicate what the data are about or what
</p>
<p>specific values mean? Finally, a lack of a good workflow increases the risk of
</p>
<p>duplicating work or even losing data. In Fig. 5.1, we show the steps required to
</p>
<p>create and describe a dataset after the data have been collected. We subsequently
</p>
<p>discuss each step in greater detail.
</p>
<p>Fig. 5.1 The workflow of data
</p>
<p>96 5 Descriptive Statistics</p>
<p/>
</div>
<div class="page"><p/>
<p>5.2 Create Structure
</p>
<p>The basic idea of setting up a goodworkflow is that good planning saves the researcher
</p>
<p>time and allows other researchers to do their share of the analysis and/or replicate the
</p>
<p>research. After the data collection phase, the first step is to save the available data. We
</p>
<p>recommend keeping track of the dataset by providing data and data-related files in
</p>
<p>separate directories by means of Windows Explorer or macOS Finder. This directory
</p>
<p>should have subdirectories for at least: (1) the data files, (2) commands, (3) a tempo-
</p>
<p>rary directory, and (4) related files; that is, a directorywith files that are directly related
</p>
<p>to a project, such as the survey used to collect the data.1
</p>
<p>In Table 5.1, we show an example of a directory structure. Within the main
</p>
<p>directory, there are four subdirectories, each with distinct files. Notice that in the
</p>
<p>Data files subdirectory, we have the original dataset, two modified datasets (one
</p>
<p>without missing data and one which includes several transformations of the data),
</p>
<p>as well as a zip file that contains the original dataset. If the data file is contained in a
</p>
<p>zip or other archive file, it is stored and unlikely to be modified, but can be easily
</p>
<p>opened if the working file is accidentally overwritten or deleted. In the Data files
</p>
<p>subdirectories, we distinguish between two files with the suffix rev1 and rev2. We
</p>
<p>use rev (abbreviation of revision), but you however, can choose another file name
</p>
<p>Table 5.1 Example of a directory structure for saving market-research-related files
</p>
<p>Directory name Subdirectory name Example file names
</p>
<p>Oddjob Data files oddjob.dta
</p>
<p>oddjob.zip
</p>
<p>oddjob rev1.dta
</p>
<p>oddjob rev2.dta
</p>
<p>Command files Missing data analysis.do
</p>
<p>Descriptives.do
</p>
<p>Factor analysis.do
</p>
<p>Regression analysis.do
</p>
<p>Temporary files Missing data analysis rev1.smcl
</p>
<p>Descriptives rev1.smcl
</p>
<p>Factor analysis rev1.smcl
</p>
<p>Regression analysis rev1.smcl
</p>
<p>Related files Codebook.docx
</p>
<p>Survey.pdf
</p>
<p>Initial findings&ndash;presentation to client.pptx
</p>
<p>Findings&ndash;presentation to client.pptx
</p>
<p>Recommendations rev1.docx
</p>
<p>Recommendations rev2.docx
</p>
<p>1Alternatively, you could also choose one of the many control system versions, including
Subversion, Git, and Mecurial, which enable simple branching and project management. These
systems work well with version control in centralized and in distributed environments.
</p>
<p>5.2 Create Structure 97</p>
<p/>
</div>
<div class="page"><p/>
<p>as long as it clearly indicates the revision on which you are working. In the
</p>
<p>Command files subdirectory we store all commands that were used to manage
</p>
<p>our data. These commands may relate to different project phases, including a
</p>
<p>missing data analysis, descriptives, factor analysis, and other methods used over
</p>
<p>the course of the project. As the name indicates, Temporary files serve as interme-
</p>
<p>diary files that are kept until the final data or command files are established, after
</p>
<p>which they are removed. Finally, in the Related Files subdirectory, we have a
</p>
<p>codebook (more on this later), the survey, two presentations, and two documents
</p>
<p>containing recommendations.
</p>
<p>Another aspect of creating a structure is setting up the variables for your study
</p>
<p>properly. This involves making decisions on the following elements:
</p>
<p>&ndash; variable names,
</p>
<p>&ndash; variable labels,
</p>
<p>&ndash; data type, and
</p>
<p>&ndash; coding of variables.
</p>
<p>The variable names should be clear and short so that they can be read in the dialog
</p>
<p>boxes. For example, if you have three questions on product satisfaction, three on
</p>
<p>loyalty, and several descriptors (age and gender), you could code these variables as
</p>
<p>satisfaction1, satisfaction2, satisfaction3, loyalty1, loyalty2, loyalty3, age, and gender.
In Stata, and most other statistical software programs, you can include variable
</p>
<p>labels that describe what each variable denotes. The description generally includes
the original question if the data were collected by means of surveys. Another point
</p>
<p>to consider is variable coding. Coding means assigning values to a variable. When
collecting quantitative data, the task is relatively easy; we use values that corre-
</p>
<p>spond with the answers for Likert and semantic differential scales (see Chap. 4).
</p>
<p>For example, when using a 7-point Likert scale, responses can be coded as 1&ndash;7 or
</p>
<p>as 0&ndash;6 (with 0 being the most negative and 6 being the most positive response).
</p>
<p>Open-ended questions (qualitative data) require more effort, usually involving a
</p>
<p>three-step process. First, we collect all the responses. In the second step, we group
</p>
<p>these responses. Determining the number of groups and the group to which a
</p>
<p>response belongs is the major challenge in this step. Two or three market
</p>
<p>researchers usually code the responses independently to prevent the process from
</p>
<p>becoming too subjective and thereafter discuss the differences that may arise. The
</p>
<p>third step is providing a value for each group. Stata can perform such analyses with
</p>
<p>the help of the additional software packageWordstat (https://provalisresearch.com/
products/content-analysis-software/wordstat-for-stata/). Please see Krippendorff
</p>
<p>(2012) for more details about coding qualitative variables.
</p>
<p>Once a system has been set up to keep track of your progress, you need to
</p>
<p>consider safeguarding your files. Large companies usually have systems for creat-
</p>
<p>ing backups (extra copies as a safeguard). If you are working alone or for a small
</p>
<p>company, you are probably responsible for this. You should save your most recent
</p>
<p>and second most recent version of your file on a separate drive and have multiple
</p>
<p>copies of your entire drive! Always keep at least two copies and never keep both
</p>
<p>backups in the same place, because you could still lose all your work through theft,
</p>
<p>98 5 Descriptive Statistics</p>
<p/>
<div class="annotation"><a href="https://provalisresearch.com/products/content-analysis-software/wordstat-for-stata">https://provalisresearch.com/products/content-analysis-software/wordstat-for-stata</a></div>
<div class="annotation"><a href="https://provalisresearch.com/products/content-analysis-software/wordstat-for-stata">https://provalisresearch.com/products/content-analysis-software/wordstat-for-stata</a></div>
</div>
<div class="page"><p/>
<p>fire, or an accident! You can use cloud storage services, such as Dropbox, Google
</p>
<p>Drive, or Microsoft&rsquo;s OneDrive for small projects to prevent loss. Always read the
</p>
<p>terms of the cloud storage services carefully to determine whether your data&rsquo;s
</p>
<p>privacy is guaranteed.
</p>
<p>5.3 Enter Data
</p>
<p>How do we enter survey or experimental data into a dataset? Specialized software is
</p>
<p>often used for large datasets, or datasets created by professional firms. For example,
</p>
<p>Epidata (http://www.epidata.dk, freely downloadable) is frequently used to enter
</p>
<p>data from paper-based surveys, Entryware&rsquo;s mobile survey (http://www.techneos.
</p>
<p>com) to enter data from personal intercepts or face-to-face interviewing, and
</p>
<p>Voxco&rsquo;s Interviewer CATI for telephone interviewing. Stata has no dedicated
</p>
<p>data entry platform. It does, however, facilitate the use of StatTransfer (http://
</p>
<p>www.stattransfer.com), a software program designed to simplify the transfer of
</p>
<p>statistical data between many software packages, including Stata, SPSS, Excel,
</p>
<p>and SAS.
</p>
<p>Such software may not be available for smaller projects, in which case data
</p>
<p>should be entered directly into Stata. A significant drawback of direct data entry is
</p>
<p>the risk of typing errors, for which Stata cannot check. Professional software, such
</p>
<p>as Epidata, can directly check if values are admissible. For example, if a survey
</p>
<p>question has only two answer categories, such as gender (coded 0/1), Epidata (and
</p>
<p>other packages) can directly check if the value entered is 0 or 1, and not any other
</p>
<p>value. The software also allows for using multiple typists when very large amounts
</p>
<p>of data need to be entered simultaneously (note that Epidata can export directly to
</p>
<p>Stata).
</p>
<p>5.4 Clean Data
</p>
<p>Cleaning data is the next step in the workflow. It requires checking for:
</p>
<p>&ndash; interviewer fraud,
</p>
<p>&ndash; suspicious response patterns,
</p>
<p>&ndash; data entry errors,
</p>
<p>&ndash; outliers, and
</p>
<p>&ndash; missing data.
</p>
<p>These issues require researchers to make decisions very carefully. In the following,
</p>
<p>we discuss each issue in greater detail.
</p>
<p>5.4 Clean Data 99</p>
<p/>
<div class="annotation"><a href="http://www.epidata.dk">http://www.epidata.dk</a></div>
<div class="annotation"><a href="http://www.techneos.com">http://www.techneos.com</a></div>
<div class="annotation"><a href="http://www.techneos.com">http://www.techneos.com</a></div>
<div class="annotation"><a href="http://www.stattransfer.com">http://www.stattransfer.com</a></div>
<div class="annotation"><a href="http://www.stattransfer.com">http://www.stattransfer.com</a></div>
</div>
<div class="page"><p/>
<p>5.4.1 Interviewer Fraud
</p>
<p>Interviewer fraud is a difficult and serious issue. It ranges from interviewers
</p>
<p>&ldquo;helping&rdquo; respondents provide answers to entire surveys being falsified. Interviewer
</p>
<p>fraud often leads to incorrect results. Fortunately, we can avoid and detect inter-
</p>
<p>viewer fraud in various ways. First, never base interviewers&rsquo; compensation on the
</p>
<p>number of completed responses they submit. Second, check and control for
</p>
<p>discrepancies in respondent selection and responses. If multiple interviewers were
</p>
<p>used, each of whom collected a reasonably large number of responses (n &gt; 100), a
selection of the respondents should be similar. This means that the average
</p>
<p>responses obtained should also be similar. In Chap. 6 we will discuss techniques
</p>
<p>to test this. Third, if possible, contact a random number of respondents afterwards
</p>
<p>for their feedback on the survey. If a substantial number of people claim they were
</p>
<p>not interviewed, interviewer fraud is likely. Furthermore, if people were previously
</p>
<p>interviewed on a similar subject, the factual variables collected, such as their
</p>
<p>gender, should not change (or no more than a trivial percentage), while variables
</p>
<p>such as a respondent&rsquo;s age and highest education level should only move up. We
</p>
<p>can check this by means of descriptive statistics. If substantial interviewer fraud is
</p>
<p>suspected, the data should be discarded. You should check for interviewer fraud
</p>
<p>during the data collection process to safeguard the quality of data collection and
</p>
<p>minimize the risk of having to discard the data in the end.
</p>
<p>5.4.2 Suspicious Response Patterns
</p>
<p>Before analyzing data, we need to identify suspicious response patterns. There are
</p>
<p>two types of response patterns we need to look for:
</p>
<p>&ndash; straight-lining, and
</p>
<p>&ndash; inconsistent answers.
</p>
<p>Straight-lining occurs when a respondent marks the same response in almost all
</p>
<p>the items. For example, if a 7-point scale is used to obtain answers and the response
</p>
<p>pattern is 4 (the middle response), or if the respondent selects only 1s, or only 7s in
</p>
<p>all the items. A common way of identifying straight-lining is by including one or
</p>
<p>more reverse-scaled items in a survey (see Chap. 4). Reverse-scaled means that the
</p>
<p>way the question, statement (when using a Likert scale), or word pair (when using a
</p>
<p>semantic differential scale) is reversed compared to the other items in the set.
</p>
<p>Box 5.1 shows an example of a four-item scale for measuring consumers&rsquo; attitude
</p>
<p>toward the brand (e.g., Sarstedt et al. 2016) with one reverse-scaled item printed in
</p>
<p>bold. By evaluating the response patterns, we can differentiate between those
</p>
<p>respondents who are not consistent for the sake of consistency and those who are
</p>
<p>merely mindlessly consistent. Note, however, that this only applies if respondents
</p>
<p>do not tick the middle option. Straight-lining is very common, especially in web
</p>
<p>surveys where respondents generally pay less attention to the answers. Likewise,
</p>
<p>100 5 Descriptive Statistics</p>
<p/>
</div>
<div class="page"><p/>
<p>long surveys and those with many similarly worded items trigger straight-lining
</p>
<p>(Drolet and Morrison 2001). An alternative is to note potential straight-lined
</p>
<p>responses and include this as a separate category in the subsequent statistical
</p>
<p>analyses. This step avoids the need to reduce the sample and indicates the size
</p>
<p>and direction of any bias.
</p>
<p>However, straight-lining can also be the result of culture-specific response
styles. For example, respondents from different cultures have different tendencies
regarding selecting the mid points (middle response styles) or the end points of a
</p>
<p>response scale (extreme response styles). Similarly, respondents from different
</p>
<p>cultures have different tendencies regarding agreeing with statements, regardless
</p>
<p>of the item content; this tendency is also referred to as acquiescence
</p>
<p>(Baumgartner and Steenkamp 2001). For example, respondents from Spanish-
</p>
<p>speaking countries tend to show higher extreme response styles and high acqui-
</p>
<p>escence, while East Asian (Japanese and Chinese) respondents show a relatively
</p>
<p>high level of middle response style. Within Europe, the Greeks stand out as
</p>
<p>having the highest level of acquiescence and a tendency towards an extreme
</p>
<p>response style. Harzing (2005) and Johnson et al. (2005) provide reviews of
</p>
<p>culture effects on response behavior.
</p>
<p>Inconsistent answers also need to be addressed before analyzing your
</p>
<p>data. Many surveys start with one or more screening questions. The purpose
</p>
<p>of a screening question is to ensure that only individuals who meet the pre-
</p>
<p>scribed criteria complete the survey. For example, a survey of mobile phone
</p>
<p>users may screen for individuals who own an iPhone. If an individual indicates
</p>
<p>that he/she does not have an iPhone, this respondent should be removed from the
</p>
<p>dataset.
</p>
<p>Surveys often ask the same question with slight variations, especially when
</p>
<p>reflective measures (see Box 3.1 in Chap. 3) are used. If a respondent gives a
</p>
<p>different answer to very similar questions, this may raise a red flag and could
</p>
<p>suggest that the respondent did not read the questions closely, or simply marked
</p>
<p>answers randomly to complete the survey as quickly as possible.
</p>
<p>Box 5.1 An Example of a Scale with Reverse-Scaled Items (in Bold)
</p>
<p>Please rate the brand in the advertisement on the following dimensions:
</p>
<p>Dislike 1 2 3 4 5 6 7 Like
</p>
<p>Unpleasant 1 2 3 4 5 6 7 Pleasant
</p>
<p>Good 1 2 3 4 5 6 7 Bad
</p>
<p>Expensive 1 2 3 4 5 6 7 Inexpensive
</p>
<p>Useless 1 2 3 4 5 6 7 Useful
</p>
<p>5.4 Clean Data 101</p>
<p/>
</div>
<div class="page"><p/>
<p>5.4.3 Data Entry Errors
</p>
<p>When data are entered manually, data entry errors occur routinely. Fortunately,
</p>
<p>such errors are easy to spot if they happen outside the variable&rsquo;s range. That is, if an
</p>
<p>item is measured using a 7-point scale, the lowest value should be 1 (or 0) and the
</p>
<p>highest 7 (or 6). We can check if this is true by using descriptive statistics
</p>
<p>(minimum, maximum, and range; see next section). Data entry errors should always
</p>
<p>be corrected by going back to the original survey. If we cannot go back (e.g.,
</p>
<p>because the data were collected using face-to-face interviews), we need to delete
</p>
<p>this specific observation for this specific variable.
</p>
<p>More subtle errors&mdash;for example, incorrectly entering a score of 4 as 3&mdash;are
</p>
<p>difficult to detect using statistics. One way to check for these data entry errors is to
</p>
<p>randomly select observations and compare the entered responses with the original
</p>
<p>survey. We do, of course, expect a small number of errors (below 1%). If many data
</p>
<p>entry errors occur, the dataset should be entered again.
</p>
<p>Manual double data entry is another method to detect data entry errors. That is,
</p>
<p>once the data has been entered manually, a second data checker enters the same data
</p>
<p>a second time and the two separate entries are compared to ensure they match.
</p>
<p>Entries that deviate from one another or values that fall outside the expected range
</p>
<p>of the scales (e.g., 7-point Likert scales should have values that fall within this
</p>
<p>range) are then indicative of data entry errors (Barchard and Verenikina 2013).
</p>
<p>Various studies reveal that&mdash;although double data entry is more laborious and
</p>
<p>expensive&mdash;it still detects errors better than single data entry (Barchard and Pace
</p>
<p>2011; Paulsen et al. 2012).
</p>
<p>5.4.4 Outliers
</p>
<p>Data often contain outliers, which are values situated far from all the other
</p>
<p>observations that may influence results substantially. For example, if we compare
</p>
<p>the average income of 20 households, we may find that the incomes range between
</p>
<p>$20,000 and $100,000, with the average being $45,000. If we considered an
</p>
<p>additional household with an income of, say, $1 million, this would increase the
</p>
<p>average substantially.
</p>
<p>Malcolm Gladwell&rsquo;s (2008) book &ldquo;Outliers: The Story of Success&rdquo; is an
</p>
<p>entertaining study of how some people became exceptionally successful
</p>
<p>(outliers).
</p>
<p>102 5 Descriptive Statistics</p>
<p/>
</div>
<div class="page"><p/>
<p>5.4.4.1 Types of Outliers
Outliers must be interpreted in the context of the study and this interpretation
</p>
<p>should be based on the types of information they provide. Depending on the source
</p>
<p>of their uniqueness, outliers can be classified into three categories:
</p>
<p>&ndash; The first type of outlier is produced by data collection or entry errors. For
</p>
<p>example, if we ask people to indicate their household income in thousands of
</p>
<p>US dollars, some respondents may just indicate theirs in US dollars (not
</p>
<p>thousands). Obviously, there is a substantial difference between $30 and
</p>
<p>$30,000! Moreover, (as discussed before) data entry errors occur frequently.
</p>
<p>Outliers produced by data collection or entry errors should be deleted, or we
</p>
<p>need to determine the correct values by, for example, returning to the
</p>
<p>respondents.
</p>
<p>&ndash; A second type of outlier occurs because exceptionally high or low values are a
</p>
<p>part of reality. While such observations can influence results significantly, they
</p>
<p>are sometimes highly important for researchers, because the characteristics of
</p>
<p>outliers can be insightful. Think, for example, of extremely successful
</p>
<p>companies, or users with specific needs long before most of the relevant market-
</p>
<p>place also needs them (i.e., lead users). Deleting such outliers is not appropriate,
</p>
<p>but the impact that they have on the results must be discussed.
</p>
<p>&ndash; A third type of outlier occurs when combinations of values are exceptionally
rare. For example, if we look at income and expenditure on holidays, we may
</p>
<p>find someone who earns $1,000,000 and spends $500,000 of his/her income on
</p>
<p>holidays. Such combinations are unique and have a very strong impact on the
</p>
<p>results (particularly the correlations that we discuss later in this chapter). In such
</p>
<p>situations, the outlier should be retained, unless specific evidence suggests that it
</p>
<p>is not a valid member of the population under study. It is very useful to flag such
</p>
<p>outliers and discuss their impact on the results.
</p>
<p>5.4.4.2 Detecting Outliers
In a simple form, outliers can be detected using univariate or bivariate graphs and
</p>
<p>statistics.2 When searching for outliers, we need to use multiple approaches to
</p>
<p>ensure that we detect all the observations that can be classified as outliers. In the
</p>
<p>following, we discuss both routes to outlier detection:
</p>
<p>Univariate Detection
</p>
<p>The univariate detection of outliers examines the distribution of observations of
</p>
<p>each variable with the aim of identifying those cases falling outside the range of the
</p>
<p>&ldquo;usual&rdquo; values. In other words, finding outliers means finding observations with
</p>
<p>very low or very high variable values. This can be achieved by calculating the
</p>
<p>2There are multivariate techniques that consider three, or more, variables simultaneously in order
to detect outliers. See Hair et al. (2010) for an introduction, and Agarwal (2013) for a more detailed
methodological discussion.
</p>
<p>5.4 Clean Data 103</p>
<p/>
</div>
<div class="page"><p/>
<p>minimum and maximum value of each variable, as well as the range. Another useful
</p>
<p>option for detecting outliers is by means of box plots, which are a means of
</p>
<p>visualizing the distribution of a variable and pinpointing those observations that
</p>
<p>fall outside the range of the &ldquo;usual&rdquo; values. We introduce the above statistics and
</p>
<p>box plots in greater detail in the Describe Data section.
It is important to recognize that there will always be observations with excep-
</p>
<p>tional values in one or more variables. However, we should strive to identify
</p>
<p>outliers that impact the presented results.
</p>
<p>Bivariate Detection
</p>
<p>We can also examine pairs of variables to identify observations whose
</p>
<p>combinations of variables are exceptionally rare. This is done by using a scatter
plot, which plots all observations in a graph where the x-axis represents the first
variable and the y-axis the second (usually dependent) variable (see the Describe
Data section). Observations that fall markedly outside the range of the other
observations will show as isolated points in the scatter plot.
</p>
<p>A drawback of this approach is the number of scatter plots that we need to draw.
</p>
<p>For example, with 10 variables, we need to draw 45 scatter plots to map all possible
</p>
<p>combinations of variables! Consequently, we should limit the analysis to only a few
</p>
<p>relationships, such as those between a dependent and independent variable in a
</p>
<p>regression. Scatterplots with large numbers of observations are often problematic
</p>
<p>when we wish to detect outliers, as there is usually not just one dot, or a few isolated
</p>
<p>dots, just a cloud of observations where it is difficult to determine a cutoff point.
</p>
<p>5.4.4.3 Dealing with Outliers
In a final step, we need to decide whether to delete or retain outliers, which should
</p>
<p>be based on whether we have an explanation for their occurrence. If there is an
</p>
<p>explanation (e.g., because some exceptionally wealthy people were included in the
</p>
<p>sample), outliers are typically retained, because they are part of the population.
</p>
<p>However, their impact on the analysis results should be carefully evaluated. That is,
</p>
<p>one should run an analysis with and without the outliers to assess if they influence
</p>
<p>the results. If the outliers are due to a data collection or entry error, they should be
</p>
<p>deleted. If there is no clear explanation, outliers should be retained.
</p>
<p>5.4.5 Missing Data
</p>
<p>Market researchers often have to deal with missing data. There are two levels at
</p>
<p>which missing data occur:
</p>
<p>&ndash; Entire surveys are missing (survey non-response).
</p>
<p>&ndash; Respondents have not answered all the items (item non-response).
</p>
<p>Survey non-response (also referred to as unit non-response) occurs when entire
surveys are missing. Survey non-response is very common and regularly only
</p>
<p>104 5 Descriptive Statistics</p>
<p/>
</div>
<div class="page"><p/>
<p>5&ndash;25% of respondents fill out surveys. Although higher percentages are possible,
</p>
<p>they are not the norm in one-shot surveys. Issues such as inaccurate address lists, a
</p>
<p>lack of interest and time, people confusing market research with selling, privacy
</p>
<p>issues, and respondent fatigue also lead to dropping response rates. The issue of
</p>
<p>survey response is best solved by designing proper surveys and survey procedures
</p>
<p>(see Box 4.5 in Chap. 4 for suggestions).
</p>
<p>Item non-response occurs when respondents do not provide answers to certain
</p>
<p>questions. There are different forms of missingness, including people not filling out
</p>
<p>or refusing to answer questions. Item non-response is common and 2&ndash;10% of
</p>
<p>questions usually remain unanswered. However, this number greatly depends on
</p>
<p>factors, such as the subject matter, the length of the questionnaire, and the method
</p>
<p>of administration. Non-response can be much higher in respect of questions that
</p>
<p>people consider sensitive and varies from country to country. In some countries, for
</p>
<p>instance, reporting incomes is a sensitive issue.
</p>
<p>The key issue with item non-response is the type of pattern that the missing data
</p>
<p>follow. Do the missing values occur randomly, or is there some type of underlying
</p>
<p>system?3 Once we have identified the type of missing data, we need to decide how
</p>
<p>to treat them. Figure 5.2 illustrates the process of missing data treatment, which we
</p>
<p>will discuss next.
</p>
<p>5.4.5.1 The Three Types of Missing Data: Paradise, Purgatory,
and Hell
</p>
<p>We generally distinguish between three types of missing data:
</p>
<p>&ndash; missing completely at random (&ldquo;paradise&rdquo;),
</p>
<p>&ndash; missing at random (&ldquo;purgatory&rdquo;), and
</p>
<p>&ndash; non-random missing (&ldquo;hell&rdquo;).
</p>
<p>Data are missing completely at random (MCAR) when the probability of data
</p>
<p>being missing is unrelated to any other measured variable and is unrelated to the
</p>
<p>variable with missing values. MCAR data thus occurs when there is no systematic
</p>
<p>reason for certain data points being missing. For example, MCAR may happen if
</p>
<p>the Internet server hosting the web survey broke down temporarily for a random
</p>
<p>reason. Why is MCAR paradise? When data are MCAR, observations with missing
</p>
<p>data are indistinguishable from those with complete data. If this is the case and little
</p>
<p>data are missing (typically less than 10% in each variable) listwise deletion can be
</p>
<p>used. Listwise deletion means that we only analyze complete cases; in most
</p>
<p>statistical software, such as Stata, this is a default option. Note that this default
</p>
<p>option in Stata only works when estimating models and only applies to the variables
</p>
<p>included in the model. When more than 10% of the data are missing, we can
</p>
<p>use multiple imputation (Eekhout et al. 2014), a more complex approach to missing
</p>
<p>data treatment that we discuss in the section Dealing with Missing Data.
</p>
<p>3For more information on missing data, see https://www.iriseekhout.com
</p>
<p>5.4 Clean Data 105</p>
<p/>
<div class="annotation"><a href="https://www.iriseekhout.com">https://www.iriseekhout.com</a></div>
</div>
<div class="page"><p/>
<p>Unfortunately, data are rarely MCAR. If a missing data point (e.g., xi) is
unrelated to the observed value of xi, but depends on another observed variable,
we consider the data missing at random (MAR). In this case, the probability that
</p>
<p>the data point is missing varies from respondent to respondent. The term MAR is
</p>
<p>unfortunate, because many people confuse it with MCAR; however, the label has
</p>
<p>stuck. An example of MAR is when women are less likely to reveal their income.
</p>
<p>That is, the probability of missing data depends on the gender and not on the
</p>
<p>income. Why is MAR purgatory? When data are MAR, the missing value pattern is
</p>
<p>not random, but this can be handled by more sophisticated missing data techniques
</p>
<p>such as multiple imputation techniques. In the Web Appendix (! Downloads),
we will illustrate how to impute a dataset with missing observations.
</p>
<p>Use one of the following:
</p>
<p>&bull; listwise deletion if &lt; 10% 
</p>
<p>missing;
</p>
<p>&bull; multiple imputation with
</p>
<p>m = 5 if  &gt;10% missing.
</p>
<p>Carry out
</p>
<p>Little&rsquo;s test
</p>
<p>Carry out mean
</p>
<p>difference tests
</p>
<p>Do Not Reject H0 Reject H0
</p>
<p>Data are missing
</p>
<p>completely at
</p>
<p>random (MCAR)
</p>
<p>Data are missing
</p>
<p>at random (MAR)
</p>
<p>or non-random
</p>
<p>missing (NRM)
</p>
<p>Data are missing
</p>
<p>at random (MAR)
</p>
<p>Data are non-
</p>
<p>random missing
</p>
<p>(NRM)
</p>
<p>Use multiple imputation
</p>
<p>method with m = 5.
</p>
<p>Use listwise deletion
</p>
<p>and acknowledge
</p>
<p>limitations arising from
</p>
<p>the missing data.
</p>
<p>Fig. 5.2 Treating missing data
</p>
<p>106 5 Descriptive Statistics</p>
<p/>
</div>
<div class="page"><p/>
<p>Lastly, data are non-random missing (NRM) when the probability that a data
</p>
<p>point (e.g., xi) is missing depends on the variable x and on other unobserved factors.
For example, very affluent and poor people are less likely to indicate their income.
</p>
<p>Thus, the missing income values depend on the income variable, but also on other
</p>
<p>unobserved factors that inhibit the respondents from reporting their incomes. This is
</p>
<p>the most severe type of missing data (&ldquo;hell&rdquo;), as even sophisticated missing data
</p>
<p>techniques do not provide satisfactory solutions. Thus, any result based on NRM
</p>
<p>data should be considered with caution. NRM data can best be prevented by
</p>
<p>extensive pretesting and consultations with experts to avoid surveys that cause
</p>
<p>problematic response behavior. For example, we could use income categories
</p>
<p>instead of querying the respondents&rsquo; income directly, or we could simply omit
</p>
<p>the income variable.
</p>
<p>A visualization of these three missingness mechanisms can be found under
</p>
<p>https://iriseekhout.shinyapps.io/MissingMechanisms/
</p>
<p>5.4.5.2 Testing for the Type of Missing Data
When dealing with missing data, we must ascertain the missing data&rsquo;s type. If the
</p>
<p>dataset is small, we can browse through the data for obvious nonresponse patterns.
</p>
<p>However, missing data patterns become more difficult to identify with an increasing
</p>
<p>sample size and number of variables. Similarly, when we have few observations,
</p>
<p>patterns should be difficult to spot. In these cases, we should use one (or both) of the
</p>
<p>following diagnostic tests to identify missing data patterns:
</p>
<p>&ndash; Little&rsquo;s MCAR test, and
</p>
<p>&ndash; mean difference tests.
</p>
<p>Little&rsquo;s MCAR test (Little 1998) analyzes the pattern of the missing data by
</p>
<p>comparing the observed data with the pattern expected if the data were randomly
</p>
<p>missing. If the test indicates no significant differences between the two patterns, the
</p>
<p>missing data can be classified as MCAR. Put differently, the null hypothesis is that
</p>
<p>the data are MCAR. Thus,
</p>
<p>&ndash; if we do not reject the null hypothesis, we assume that the data are MCAR, and
</p>
<p>&ndash; if we reject the null hypothesis, the data are either MAR or NRM.
</p>
<p>If the data cannot be assumed to be MCAR, we need to test whether the missing
</p>
<p>pattern is caused by another variable in the dataset by using the procedures
</p>
<p>discussed in Chap. 6.
</p>
<p>Looking at group means and their differences can also reveal missing data
</p>
<p>problems. For example, we can run a two independent samples t-test to explore
whether there is a significant difference in the mean of a continuous variable (e.g.,
</p>
<p>income) between the group with missing values and the group without missing
</p>
<p>5.4 Clean Data 107</p>
<p/>
<div class="annotation"><a href="https://iriseekhout.shinyapps.io/MissingMechanisms">https://iriseekhout.shinyapps.io/MissingMechanisms</a></div>
</div>
<div class="page"><p/>
<p>values. In respect of nominal or ordinal variables, we could tabulate the occurrence
</p>
<p>of non-responses against different groups&rsquo; responses. If we put the (categorical)
</p>
<p>variable about which we have concerns in one column of a table (e.g., income
</p>
<p>category), and the number of (non-)responses in another, we obtain a table similar
</p>
<p>to Table 5.2.
</p>
<p>Using the χ2-test (pronounced as chi-square), which we discuss under nonpara-
metric tests in the Web Appendix (! Downloads), we can test if there is a
significant relationship between the respondents&rsquo; (non-)responses in respect of a
</p>
<p>certain variable and their income. In this example, the test indicates that there is a
</p>
<p>significant relationship between the respondents&rsquo; income and the (non-)response
</p>
<p>behavior in respect of another variable, supporting the assumption that the data are
</p>
<p>MAR. We illustrate Little&rsquo;s MCAR test, together with the missing data analysis and
</p>
<p>imputation procedures in this chapter&rsquo;s appendix Web Appendix (! Downloads).
</p>
<p>5.4.5.3 Dealing with Missing Data
Research has suggested a broad range of approaches for dealing with missing data.
</p>
<p>We discuss the listwise deletion and the multiple imputation method.
</p>
<p>Listwise deletion uses only those cases with complete responses in respect of all
</p>
<p>the variables considered in the analysis. If any of the variables used have missing
</p>
<p>values, the observation is omitted from the computation. If many observations have
</p>
<p>some missing responses, this decreases the usable sample size substantially and
</p>
<p>hypotheses are tested with less power (the power of a statistical test is discussed in
</p>
<p>Chap. 6).
</p>
<p>Multiple imputation is a more complex approach to missing data treatment
</p>
<p>(Rubin 1987; Carpenter and Kenward 2013). It is a simulation-based statistical
</p>
<p>technique that facilitates inference by replacing missing observations with a set of
</p>
<p>possible values (as opposed to a single value) representing the uncertainty about the
</p>
<p>missing data&rsquo;s true value (Schafer 1997). The technique involves three steps. First,
</p>
<p>the missing values are replaced by a set of plausible values not once, but m times
(e.g., five times). This procedure yields m imputed datasets, each of which reflects
the uncertainty about the missing data&rsquo;s correct value (Schafer 1997). Second, each
</p>
<p>of the imputed m datasets are analyzed separately by means of standard data
methods. Third and finally, the imputed results from all m datasets (with imputed
values) are combined into a single multiple-imputation dataset to produce statistical
</p>
<p>inferences with valid confidence intervals. This is necessary to reflect the uncer-
</p>
<p>tainty related to the missing values. According to the literature, deciding on the
</p>
<p>number of imputations, m, can be very challenging, especially when the patterns of
the missing data are unclear. As a rule of thumb, an m of at least 5 should be
</p>
<p>Table 5.2 Example of response issues
</p>
<p>Low income Medium income High income
</p>
<p>Response 65 95 70
</p>
<p>Non-response 35 5 30
</p>
<p>N &frac14; 300
</p>
<p>108 5 Descriptive Statistics</p>
<p/>
</div>
<div class="page"><p/>
<p>sufficient to obtain valid inferences (Rubin 1987; White et al. 2011). Additional
</p>
<p>information about the multiple imputation techniques available when using Stata
</p>
<p>can be found in the Web Appendix (! Downloads).
Now that we have briefly reviewed the most common approaches for handling
</p>
<p>missing data, there is still one unanswered question: Which one should you use? As
</p>
<p>shown in Fig. 5.2, if the data are MCAR, listwise deletion is recommended (Graham
</p>
<p>2012) when the missingness is less than 10% and multiple imputation when this is
</p>
<p>greater than 10%. When the data are not MCAR but MAR, listwise deletion yields
</p>
<p>biased results. You should therefore use the multiple imputation method with an
</p>
<p>m of 5 (White et al. 2011). Finally, when the data are NRM, the multiple imputation
method provides inaccurate results. Consequently, you should choose listwise
</p>
<p>deletion and acknowledge the limitations arising from the missing data. Table 5.3
</p>
<p>summarizes the data cleaning issues discussed in this section.
</p>
<p>Table 5.3 Data cleaning issues and how to deal with them
</p>
<p>Problem Action
</p>
<p>Interviewer fraud &ndash; Check with respondents whether they were interviewed and correlate
with previous data if available.
</p>
<p>Suspicious response
patterns
</p>
<p>&ndash; Check for straight lining.
&ndash; Include reverse-scaled items.
&ndash; Consider removing the cases with straight-lined responses.
&ndash; Consider cultural differences in response behavior (middle and
extreme response styles, acquiescence).
&ndash; Check for inconsistencies in response behavior.
</p>
<p>Data entry errors &ndash; Use descriptive statistics (minimum, maximum, range) to check for
obvious data entry errors.
&ndash; Compare a subset of surveys to the dataset to check for
inconsistencies.
</p>
<p>Outliers &ndash; Identify outliers by means of univariate descriptive statistics
(minimum, maximum, range), box plots, and scatter plots.
&ndash; Outliers are usually retained unless they:
. . . are a result of data entry errors,
. . . do not fit the objective of the research, or
. . . influence the results severely (but report results with and without
</p>
<p>outliers for transparency).
</p>
<p>Missing data &ndash; Check the type of missing data by running Little&rsquo;s MCAR test and, if
necessary, mean differences tests.
&ndash; When the data are MCAR, use either listwise deletion or the multiple
imputation method with an m of 5.
&ndash; When the data are MAR, use the multiple imputation method with an
m of 5.
&ndash; When the data are NRM, use listwise deletion and acknowledge the
limitations arising from the missing data.
</p>
<p>5.4 Clean Data 109</p>
<p/>
</div>
<div class="page"><p/>
<p>5.5 Describe Data
</p>
<p>Once we have performed the previous steps, we can turn to the task of describing
</p>
<p>the data. Data can be described one variable at a time (univariate descriptives) or in
</p>
<p>terms of the relationship between two variables (bivariate descriptives). We further
</p>
<p>divide univariate and bivariate descriptives into graphs and tables, as well as
</p>
<p>statistics.
</p>
<p>The choice between the two depends on the information we want to convey.
</p>
<p>Graphs and tables can often tell a non-technical person a great deal. On the other
</p>
<p>hand, statistics require some background knowledge, but have the advantage that
</p>
<p>they take up little space and are exact. We summarize the different types of
</p>
<p>descriptive statistics in Fig. 5.3.
</p>
<p>5.5.1 Univariate Graphs and Tables
</p>
<p>In this section, we discuss the most common univariate graphs and univariate
tables:
</p>
<p>&ndash; bar chart,
</p>
<p>&ndash; histogram,
</p>
<p>&ndash; box plot,
</p>
<p>&ndash; pie chart, and the
</p>
<p>&ndash; frequency table.
</p>
<p>Univariate Bivariate
</p>
<p>Graphs &amp; tables Statistics
</p>
<p>Measures of centrality:
</p>
<p>&bull; Mode
</p>
<p>&bull; Median
</p>
<p>&bull; Mean
</p>
<p>Scatter plots
</p>
<p>Measures of dispersion
</p>
<p>&bull; Range
</p>
<p>&bull; Interquartile Range
</p>
<p>&bull; Variance
</p>
<p>&bull; Standard deviation
</p>
<p>Covariance
</p>
<p>StatisticsGraphs &amp; tables
</p>
<p>Bar chart
</p>
<p>Histogram
</p>
<p>Box plot
</p>
<p>Pie chart
</p>
<p>Crosstabs
</p>
<p>Frequency table
</p>
<p>Describe Data
</p>
<p>Correlation
</p>
<p>Fig. 5.3 The different types of descriptive statistics
</p>
<p>110 5 Descriptive Statistics</p>
<p/>
</div>
<div class="page"><p/>
<p>Figure 5.4 draws on these different types of charts and tables to provide information
</p>
<p>on the characteristics of travelers taken from the Oddjob Airways dataset that we
</p>
<p>use throughout this book.
</p>
<p>A bar chart (Fig. 5.4 top left) is a graphical representation of a single categori-
</p>
<p>cal variable indicating each category&rsquo;s frequency of occurrence. However, each
</p>
<p>bar&rsquo;s height can also represent other indices, such as centrality measures or the
</p>
<p>dispersion of different data groups (see next section). Bar charts are primarily
</p>
<p>useful for describing nominal or ordinal variables. Histograms should be used for
</p>
<p>interval or ratio-scaled variables.
</p>
<p>A histogram (Fig. 5.4 top middle) is a graph that shows how frequently
</p>
<p>categories made from a continuous variable occur. Differing from the bar chart,
</p>
<p>the variable categories on the x-axis are divided into (non-overlapping) classes of
equal width. For example, if you create a histogram for the variable age, you can
use classes of 21&ndash;30, 31&ndash;40, etc. A histogram is commonly used to examine the
</p>
<p>distribution of a variable. For this purpose, a curve following a specific distribution
</p>
<p>(e.g., normal) is superimposed on the bars to assess the correspondence of the actual
</p>
<p>distribution to the desired (e.g., normal) distribution. Given that overlaying a
</p>
<p>normal curve makes most symmetric distributions look more normal then they
</p>
<p>are, you should be cautious when assessing normality by means of histograms. In
</p>
<p>Chap. 6 we will discuss several options for checking the normality of data.
</p>
<p>Histograms plot continuous variables with ranges of the variables grouped
</p>
<p>into intervals (bins), while bar charts plot nominal and ordinal variables.
</p>
<p>de ch at fr us
</p>
<p>0
2
0
0
</p>
<p>20 2
0
</p>
<p>4
0
</p>
<p>6
0
</p>
<p>8
0
</p>
<p>1
0
0
</p>
<p>0
.0
</p>
<p>1
D
</p>
<p>e
n
s
it
y
</p>
<p>A
g
e
</p>
<p>.0
2
</p>
<p>.0
3
</p>
<p>40 60
Age
</p>
<p>80 100
</p>
<p>Gold
</p>
<p>Blue Silver
</p>
<p>4
0
0
</p>
<p>c
o
u
n
t 
o
f 
n
fl
ig
</p>
<p>h
ts 6
</p>
<p>0
0
</p>
<p>8
0
0
</p>
<p>Fig. 5.4 From top left to bottom right; the bar chart, histogram, box plot, pie chart, and frequency
table
</p>
<p>5.5 Describe Data 111</p>
<p/>
</div>
<div class="page"><p/>
<p>Another way of displaying the distribution of a (continuous) variable is the box
</p>
<p>plot (Fig. 5.4 top right) (also referred to as a box-and-whisker plot). The box plot
</p>
<p>is a graph representing a variable&rsquo;s distribution and consists of elements expressing
</p>
<p>the dispersion of the data. Note that several elements refer to terminologies
</p>
<p>discussed in the Univariate Statistics section. Figure 5.5 shows a box plot for the
variable age based on the Oddjob Airways dataset.
</p>
<p>&ndash; Outside values are observations that fall above the 3rd quartile&thorn;1.5 interquartile
range.
</p>
<p>&ndash; The upper adjacent value represents observations with the highest value that fall
within the 3rd quartile &thorn;1.5 interquartile range.
</p>
<p>&ndash; The upper line extending the box (whisker) represents the distance to
observations with the highest values that fall within the following range: 3rd
</p>
<p>quartile &thorn; interquartile range. If there are no observations within this range, the
line is equal to the maximum value.
</p>
<p>&ndash; The top and bottom of the box describe the 3rd quartile (top) and 1st quartile
</p>
<p>(bottom); that is, the box contains the middle 50% of the data, which is
</p>
<p>equivalent to the interquartile range.
</p>
<p>&ndash; The solid line inside the box represents the median.
&ndash; The lower line extending the box (whisker) represents the distance to the
</p>
<p>smallest observation that is within the following range: 1st quartile �
interquartile range. If there are no observations within this range, the line is
</p>
<p>equal to the minimum value.
</p>
<p>&ndash; The lower adjacent value represents observations with lowest values that fall
inside the 3rd quartile �1.5 interquartile range.
</p>
<p>Fig. 5.5 Elements of the box plot
</p>
<p>112 5 Descriptive Statistics</p>
<p/>
</div>
<div class="page"><p/>
<p>We can make statements about the dispersion of the data with a box plot. The larger
</p>
<p>the box, the greater the observations&rsquo; variability. Furthermore, the box plot helps us
</p>
<p>identify outliers in the data.
</p>
<p>The pie chart (i.e., Fig. 5.4 bottom left) visualizes how a variable&rsquo;s different
</p>
<p>values are distributed. Pie charts are particularly useful for displaying percentages
</p>
<p>of variables, because people interpret the entire pie as being 100%, and can easily
</p>
<p>see how often values occur. The limitation of the pie chart is, however, that it is
</p>
<p>difficult to determine the size of segments that are very similar.
</p>
<p>A frequency table (i.e., Fig. 5.4 bottom right) is a table that includes all possible
</p>
<p>values of a variable in absolute terms (i.e., frequency), how often they occur
</p>
<p>relatively (i.e., percentage), and the percentage of the cumulative frequency,
</p>
<p>which is the sum of all the frequencies from the minimum value to the category&rsquo;s
</p>
<p>upper bound (i.e., cumulative frequency). It is similar to the histogram and pie chart
</p>
<p>in that it shows the distribution of a variable&rsquo;s possible values. However, in a
</p>
<p>frequency table, all values are indicated exactly. Like pie charts, frequency tables
</p>
<p>are primarily useful if variables are measured on a nominal or ordinal scale.
</p>
<p>5.5.2 Univariate Statistics
</p>
<p>Univariate statistics fall into two groups: those describing centrality and those
</p>
<p>describing the dispersion of variables. Box 5.2 at the end of this section shows
</p>
<p>sample calculation of the statistics used on a small set of values.
</p>
<p>5.5.2.1 Measures of Centrality
Measures of centrality (sometimes referred to asmeasures of central tendency) are
statistical indices of a &ldquo;typical&rdquo; or &ldquo;average&rdquo; score. There are two main types of
</p>
<p>measures of centrality, the median and the mean.4
</p>
<p>The median is the value that occurs in the middle of the set of scores if they are
</p>
<p>ranked from the smallest to the largest, and it therefore separates the lowest 50% of
</p>
<p>cases from the highest 50% of cases. For example, if 50% of the products in a
</p>
<p>market cost less than $1,000, then this is the median price. Identifying the median
</p>
<p>requires at least ordinal data (i.e., it cannot be used with nominal data).
</p>
<p>The most commonly used measure of centrality is the mean (also called the
</p>
<p>arithmetic mean or, simply, the average). The mean (abbreviated as �x) is the sum of
each observation&rsquo;s value divided by the number of observations:
</p>
<p>�x &frac14; Sum x&eth; &THORN;
n
</p>
<p>&frac14; 1
n
</p>
<p>Xn
</p>
<p>i&frac14;1 xi
</p>
<p>4The mode is another measure. However, unlike the median and mean, it is ill-defined, because it
can take on multiple values. Consequently, we do not discuss the mode.
</p>
<p>5.5 Describe Data 113</p>
<p/>
</div>
<div class="page"><p/>
<p>In the above formula, xi refers to the value of observation i of variable x and n refers
to the total number of observations. The mean is only useful for interval or ratio-
</p>
<p>scaled variables.
</p>
<p>Each measure of centrality has its own use. The mean is most frequently used,
</p>
<p>but is sensitive to very small or large values. Conversely, the median is not sensitive
</p>
<p>to outliers. Consequently, the relationship between the mean and the median
</p>
<p>provides us with valuable information about a variable&rsquo;s distribution. If the mean
</p>
<p>and the median are about the same, the variable is likely to be symmetrically
</p>
<p>distributed (i.e., the left side of the distribution mirrors the right side). If the
</p>
<p>mean differs from the median, this suggests that the variable is asymmetrically
</p>
<p>distributed and/or contains outliers. This is the case when we examine the prices of
</p>
<p>a set of products valued $500, $530, $530, and $10,000; the median is $530, while
</p>
<p>the mean is $2,890. This example illustrates why a single measure of centrality can
</p>
<p>be misleading. We also need to consider the variable&rsquo;s dispersion to gain a more
</p>
<p>complete picture.
</p>
<p>5.5.2.2 Measures of Dispersion
Measures of dispersion provide researchers with information about the variability
</p>
<p>of the data; that is, how far the values are spread out. We differentiate between four
</p>
<p>types of measures of dispersion:
</p>
<p>&ndash; range,
</p>
<p>&ndash; interquartile range,
</p>
<p>&ndash; variance, and
</p>
<p>&ndash; standard deviation.
</p>
<p>The range is the simplest measure of dispersion. It is the difference between the
</p>
<p>highest and the lowest value in a dataset and can be used on data measured at least
</p>
<p>on an ordinal scale. The range is of limited use as a measure of dispersion, because
</p>
<p>it provides information about extreme values and not necessarily about &ldquo;typical&rdquo;
</p>
<p>values. However, the range is valuable when screening data, as it allows for
</p>
<p>identifying data entry errors. For example, a range of more than 6 on a 7-point
</p>
<p>Likert scale would indicate an incorrect data entry.
</p>
<p>The interquartile range is the difference between the 3rd and 1st quartile. The 1st
</p>
<p>quartile corresponds to the value separating the 25% lowest values from the 75%
largest values if the values are ordered sequentially. Correspondingly, the 3rd quartile
</p>
<p>separates the 75% lowest from the 25% highest values. The interquartile range is
</p>
<p>particularly important for drawing box plots.
</p>
<p>The variance (generally abbreviated as s2) is a common measure of dispersion.
The variance is the sum of the squared differences of each value and a variable&rsquo;s
</p>
<p>mean, divided by the sample size minus 1. The variance is only useful if the data are
</p>
<p>interval or ratio-scaled:
</p>
<p>114 5 Descriptive Statistics</p>
<p/>
</div>
<div class="page"><p/>
<p>s2 &frac14;
Pn
</p>
<p>i&frac14;1 xi � �x&eth; &THORN;
2
</p>
<p>n� 1
The variance tells us how strongly observations vary around the mean. A low
</p>
<p>variance indicates that the observations tend to be very close to the mean; a high
</p>
<p>variance indicates that the observations are spread out. Values far from the mean
</p>
<p>increase the variance more than those close to the mean.
</p>
<p>The most commonly used measure of dispersion is the standard deviation
</p>
<p>(usually abbreviated as s). It is the square root of&mdash;and, therefore, a variant of&mdash;
the variance:
</p>
<p>s &frac14;
ffiffiffiffi
</p>
<p>s2
p
</p>
<p>&frac14;
</p>
<p>ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
</p>
<p>Pn
i&frac14;1 xi � �x&eth; &THORN;
</p>
<p>2
</p>
<p>n� 1
</p>
<p>s
</p>
<p>The variance and standard deviation provide similar information, but while the
</p>
<p>variance is expressed on the same scale as the original variable, the standard
</p>
<p>deviation is standardized. Consequently, the following holds for normally
</p>
<p>distributed variables (this will be discussed in the following chapters in more
</p>
<p>detail):
</p>
<p>&ndash; 66% of all observations are between plus and minus one standard deviation units
</p>
<p>from the mean,
</p>
<p>&ndash; 95% of all observations are between plus and minus two standard deviation units
</p>
<p>from the mean, and
</p>
<p>&ndash; 99% of all observations are between plus and minus three standard deviation
</p>
<p>units from the mean.
</p>
<p>Thus, if the mean price is $1,000 and the standard deviation is $150, then 66% of all
</p>
<p>the prices fall between $850 and $1150, 95% fall between $700 and $1300, and
</p>
<p>99% of all the observations fall between $550 and $1,450.
</p>
<p>5.5.3 Bivariate Graphs and Tables
</p>
<p>There are several bivariate graphs and tables, of which the scatter plot and the
crosstab are the most important. Furthermore, several of the graphs, charts, and
</p>
<p>tables discussed in the context of univariate analysis can be used for bivariate
</p>
<p>analysis. For example, box plots can be used to display the distribution of a variable
</p>
<p>in each group (category) of nominal variables.
</p>
<p>A scatter plot (see Fig. 5.6) uses both the y and x-axis to show how two variables
relate to one another. If the observations almost form a straight diagonal line in a
</p>
<p>5.5 Describe Data 115</p>
<p/>
</div>
<div class="page"><p/>
<p>scatter plot, the two variables are strongly related.5 Sometimes, a third variable,
</p>
<p>corresponding to the color or size (e.g., a bubble plot) of the data points, is included,
adding another dimension to the plot.
</p>
<p>Crosstabs (also referred to as contingency tables) are tables in a matrix format
that show the frequency distribution of nominal or ordinal variables. They are the
</p>
<p>equivalent of a scatter plot used to analyze the relationship between two variables.
</p>
<p>While crosstabs are generally used to show the relationship between two variables,
</p>
<p>they can also be used for three or more variables, which, however, makes them
</p>
<p>difficult to grasp. Crosstabs are also part of the χ2-test (pronounced as chi-square),
which we discuss under nonparametric tests in the Web Appendix (!
Downloads).
</p>
<p>0
</p>
<p>0
0
</p>
<p>0 .25 -.5 -.3 -.1 .1 .3 .5.5 .75 1
</p>
<p>-.
5
</p>
<p>-.
3
</p>
<p>-.
1
</p>
<p>.1
.3
</p>
<p>.5
</p>
<p>.2
5
</p>
<p>.5x x
</p>
<p>.7
5
</p>
<p>1
.2
</p>
<p>.4
.6
</p>
<p>.8
1
</p>
<p>.2 .4 .6
y
</p>
<p>y y
</p>
<p>r = 1.0
</p>
<p>r = 0 r = 0
</p>
<p>r = -1.0
</p>
<p>x
</p>
<p>0
.2
</p>
<p>.4
.6
</p>
<p>.8
1
</p>
<p>x
</p>
<p>.8 1 0 .2 .4 .6
y
</p>
<p>.8 1
</p>
<p>Fig. 5.6 Scatter plots and correlations
</p>
<p>5A similar type of chart is the line chart. In a line chart, measurement points are ordered (typically
by their x-axis value) and joined with straight line segments.
</p>
<p>116 5 Descriptive Statistics</p>
<p/>
</div>
<div class="page"><p/>
<p>5.5.4 Bivariate Statistics
</p>
<p>Bivariate statistics involve the analysis of two variables to determine the empirical
</p>
<p>relationship between them. There are two key measures that indicate (linear)
</p>
<p>associations between two variables; we illustrate their computation in Box 5.2:
</p>
<p>&ndash; covariance, and
</p>
<p>&ndash; correlation.
</p>
<p>The covariance is the degree to which two variables vary together. If the
</p>
<p>covariance is zero, then two variables do not vary together. The covariance is the
</p>
<p>sum of the multiplication of the differences between each value of the xi and yi
variables and their means, divided by the sample size minus 1:
</p>
<p>Cov&eth;xi, yi&THORN; &frac14;
1
</p>
<p>n� 1
Xn
</p>
<p>i&frac14;1 &eth;xi � x
� &THORN; ∙ &eth;yi � y
</p>
<p>� &THORN;
</p>
<p>The correlation (typically abbreviated as r) is a common measure of how
strongly two variables relate to each other. The most common type of correlation,
</p>
<p>the Pearson&rsquo;s correlation coefficient, is calculated as follows:
</p>
<p>r &frac14; Cov xi; yi&eth; &THORN;
sx ∙ sy
</p>
<p>&frac14;
Pn
</p>
<p>i&frac14;1 xi � �x&eth; &THORN; ∙ yi � �y&eth; &THORN;
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
</p>
<p>Pn
i&frac14;1 xi � �x&eth; &THORN;
</p>
<p>2
q
</p>
<p>∙
</p>
<p>ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
</p>
<p>Pn
i&frac14;1 yi � �y&eth; &THORN;
</p>
<p>2
q
</p>
<p>The numerator contains the covariance of xi and yi (Cov(xi, yi)), while the
denominator contains the product of the standard deviations of xi and yi.
</p>
<p>6 Thus,
</p>
<p>the correlation is the covariance divided by the product of the standard deviations.
</p>
<p>As a result, the correlation is standardized and, unlike the covariance, is no longer
</p>
<p>dependent on the variables&rsquo; original measurement. More precisely, the correlation
</p>
<p>coefficient ranges from �1 to 1, where �1 indicates a perfect negative relationship
and 1 indicates the contrary. A correlation coefficient of 0 indicates that there is no
</p>
<p>relationship, also implying that their covariance is zero.
</p>
<p>As a rule of thumb (Cohen 1988), an absolute correlation. . .
</p>
<p>&ndash; . . . below 0.30 indicates a weak relationship,
</p>
<p>&ndash; . . . between 0.30 and 0.49 indicates a moderate relationship, and
</p>
<p>&ndash; . . . above 0.49 indicates a strong relationship.
</p>
<p>6Note that the terms n�1 in the numerator and denominator cancel each other and are therefore not
shown here.
</p>
<p>5.5 Describe Data 117</p>
<p/>
</div>
<div class="page"><p/>
<p>The scatter plots in Fig. 5.6 illustrate several correlations between two variables
</p>
<p>x and y. If the observations almost form a straight diagonal line in the scatter plot
(upper left and right in Fig. 5.6), the two variables have a high (absolute) correla-
</p>
<p>tion. If the observations are uniformly distributed in the scatter plot (lower right in
</p>
<p>Fig. 5.6), or one variable is a constant (lower left in Fig. 5.6), the correlation is zero.
</p>
<p>Pearson&rsquo;s correlation coefficient is the most common coefficient and is generally
</p>
<p>simply referred to as the correlation (Agresti and Finlay 2014). Pearson&rsquo;s correla-
</p>
<p>tion is appropriate for calculating correlations between two variables that are
</p>
<p>both interval or ratio-scaled. However, it can also be used when one variable is
</p>
<p>Box 5.2 Sample Calculation of Univariate and Bivariate Statistics
</p>
<p>Consider the following list of values for variables x and y, which we treat as
ratio-scaled:
</p>
<p>x 6 6 7 8 8 8 12 14 14
</p>
<p>y 7 6 6 9 8 5 10 9 9
</p>
<p>Measures of centrality for x:
</p>
<p>Median &frac14; 8
</p>
<p>Mean �x &frac14; 1
9
</p>
<p>6&thorn; 6&thorn; . . .&thorn; 14&thorn; 14&eth; &THORN; &frac14; 83
9
</p>
<p>� 9:22
</p>
<p>Measures of dispersion for x:
</p>
<p>Minimum &frac14; 6
Maximum &frac14; 14
Range &frac14; 14 &ndash; 6 &frac14; 8
Interquartile range &frac14; 6.5
</p>
<p>Variance s2&eth; &THORN; &frac14;
</p>
<p>h
</p>
<p>6� 9:22&eth; &THORN;2 &thorn; . . .&thorn;
�
</p>
<p>14� 9:22
�2
i
</p>
<p>9� 1 &frac14;
83:56
</p>
<p>8
� 10:44:
</p>
<p>Standard deviation s&eth; &THORN; &frac14;
ffiffiffiffi
</p>
<p>s2
p
</p>
<p>&frac14;
ffiffiffiffiffiffiffiffiffiffiffi
</p>
<p>10:44
p
</p>
<p>� 3:23
</p>
<p>Measures of association between x and y:
</p>
<p>Covariance cov x; y&eth; &THORN;&eth; &THORN; &frac14; 1
9� 1 6� 9:22&eth; &THORN; ∙ 7� 7:67&eth; &THORN; &thorn; . . .&frac12;
</p>
<p>&thorn; 14� 9:22&eth; &THORN; ∙ 9� 7:67&eth; &THORN;� &frac14; 31:67
8
</p>
<p>� 3:96
</p>
<p>Correlation r&eth; &THORN; &frac14; 3:96
3:23 ∙ 1:73
</p>
<p>� 0:71
</p>
<p>118 5 Descriptive Statistics</p>
<p/>
</div>
<div class="page"><p/>
<p>interval or ratio-scale and the other is, for example, binary. There are other
</p>
<p>correlation coefficients for variables measured on lower scale levels. Some
</p>
<p>examples are:
</p>
<p>&ndash; Spearman&rsquo;s correlation coefficient and Kendall&rsquo;s tau when at least one variable
for determining the correlation is measured on an ordinal scale.
</p>
<p>&ndash; Contingency coefficient, Cramer&rsquo;s V, and Phi for variables measured on a
nominal scale. These statistical measures are used with crosstabs; we
</p>
<p>discuss these in the context of nonparametric tests in the Web Appendix
</p>
<p>(! Downloads).
</p>
<p>In Table 5.4, we indicate which descriptive statistics are useful for differently
</p>
<p>scaled variables. The brackets X indicate that the use of a graph, table, or statistic
</p>
<p>is potentially useful while (X) indicates that use is possible, but less likely useful,
</p>
<p>because this typically requires collapsing data into categories, resulting in a loss of
</p>
<p>information.
</p>
<p>Table 5.4 Types of descriptive statistics for differently scaled variables
</p>
<p>Nominal Ordinal Interval &amp; ratio
</p>
<p>Univariate graphs &amp; tables
</p>
<p>Bar chart X X
</p>
<p>Histogram X
</p>
<p>Box plot X
</p>
<p>Pie chart X X (X)
</p>
<p>Frequency table X X (X)
</p>
<p>Univariate statistics: Measures of centrality
</p>
<p>Median X X
</p>
<p>Mean X
</p>
<p>Univariate statistics: Measures of dispersion
</p>
<p>Range (X) X
</p>
<p>Interquartile range (X) X
</p>
<p>Variance X
</p>
<p>Standard deviation X
</p>
<p>Bivariate graphs/tables
</p>
<p>Scatter plot X
</p>
<p>Crosstab X X (X)
</p>
<p>Bivariate statistics
</p>
<p>Contingency coefficient X
</p>
<p>Cramer&rsquo;s V X
</p>
<p>Phi X
</p>
<p>Spearman&rsquo;s correlation X
</p>
<p>Kendall&rsquo;s tau X
</p>
<p>Pearson&rsquo;s correlation X
</p>
<p>5.5 Describe Data 119</p>
<p/>
</div>
<div class="page"><p/>
<p>5.6 Transform Data (Optional)
</p>
<p>Transforming data is an optional step in the workflow. Researchers transform data
</p>
<p>as certain analysis techniques require this: it might help interpretation or might help
</p>
<p>meet the assumptions of techniques that will be discussed in subsequent chapters.
</p>
<p>We distinguish two types of data transformation:
</p>
<p>&ndash; variable respecification, and
</p>
<p>&ndash; scale transformation.
</p>
<p>5.6.1 Variable Respecification
</p>
<p>Variable respecification involves transforming data to create new variables or to
</p>
<p>modify existing ones. The purpose of respecification is to create variables that are
</p>
<p>consistent with the study&rsquo;s objective. Recoding a continuous variable into a cate-
gorical variable is an example of a simple respecification. For example, if we have a
</p>
<p>variable that measures a respondent&rsquo;s number of flights (indicating the number of
flights per year), we could code those flights below 5 as low (&frac14;1), between 5 and
10 flights as medium (&frac14;2), and everything above 11 as high (&frac14;3). Recoding a
variable in such a way always results in a loss of information, since the newly
</p>
<p>created variable contains less detail than the original. While there are situations that
</p>
<p>require recoding (e.g., we might be asked to give advice based on different income
</p>
<p>groups where income is continuous), we should generally avoid recoding.
</p>
<p>Another example of respecification is swapping the polarity of a question. If you
</p>
<p>have a variable measured on a 5-point Likert-scale where: 1 &frac14; &ldquo;strongly agree&rdquo;;
2 &frac14; &ldquo;agree&rdquo;; 3 &frac14; &ldquo;undecided&rdquo;; 4 &frac14; &ldquo;disagree&rdquo;; 5 &frac14; &ldquo;strongly disagree&rdquo; and you
wish to switch the polarity of the values so that value 1 reverses to 5, value
</p>
<p>2 becomes 4, and so on.
</p>
<p>Creating a dummy variable is a special way of recoding data. Dummy variables
</p>
<p>(or simply dummies) are binary variables that indicate if a certain trait is present or
not. For example, we can use a dummy variable to indicate that advertising was
</p>
<p>used during a period (value of the dummy is 1) or not (value of the dummy is 0). We
</p>
<p>can also use multiple dummies to capture categorical variables&rsquo; effects. For exam-
</p>
<p>ple, three levels of flight intensity (low, medium, and high) can be represented by
two dummy variables: The first takes a value of 1 if the intensity is high (0 else), the
</p>
<p>second also takes a value of 1 if the intensity is medium (0 else). If both dummies
</p>
<p>take the value 0, this indicates low flight intensity. We always construct one
</p>
<p>dummy less than the number of categories. We explain dummies in further detail
</p>
<p>in the Web Appendix (! Downloads) and more information can be found
at http://www.stata.com/support/faqs/data-management/creating-dummy-variables/.
</p>
<p>Dummies are often used in regression analysis (discussed in Chap. 7).
</p>
<p>120 5 Descriptive Statistics</p>
<p/>
<div class="annotation"><a href="http://www.stata.com/support/faqs/data-management/creating-dummy-variables">http://www.stata.com/support/faqs/data-management/creating-dummy-variables</a></div>
</div>
<div class="page"><p/>
<p>The creation of constructs is a frequently used type of variable respecification.
As described in Chap. 3, a construct is a concept that cannot be observed, but can be
</p>
<p>measured by using multiple items, none of which relate perfectly to the construct.
</p>
<p>To compute a construct measure, we need to calculate the average (or the sum) of
</p>
<p>several related items. For example, a traveler&rsquo;s commitment to fly with Oddjob
Airways can be measured by using the following three items:
</p>
<p>&ndash; I am very committed to Oddjob Airways.
</p>
<p>&ndash; My relationship with Oddjob Airways means a lot to me.
</p>
<p>&ndash; If Oddjob Airways would no longer exist, it would be a true loss for me.
</p>
<p>By calculating the average of these three items, we can form a composite measure
of commitment. If one respondent indicated 4, 3, and 4 on the three items&rsquo; scale, we
calculate a construct score (also referred to as a composite score) for this person as
</p>
<p>follows: (4 &thorn; 3 &thorn; 4)/3 &frac14; 3.67. Note that we should take the average over the
number of nonmissing responses.7 In Chap. 8 we discuss more advanced methods
</p>
<p>of doing this by, for example, creating factor scores.
</p>
<p>Similar to creating constructs, we can create an index of sets of variables. For
</p>
<p>example, we can create an index of information search activities, which is the sum
</p>
<p>of the information that customers require from promotional materials, the Internet,
</p>
<p>and other sources. This measure of information search activities is also referred to
</p>
<p>as a composite measure, but, unlike a construct, the items in an index define the trait
</p>
<p>to be measured.
</p>
<p>5.6.2 Scale Transformation
</p>
<p>Scale transformation involves changing the variable values to ensure comparabil-
</p>
<p>ity with other variables or to make the data suitable for analysis. Different scales are
</p>
<p>often used to measure different variables. For example, we may use a 5-point Likert
</p>
<p>scale for one set of variables and a 7-point Likert scale for a different set of
</p>
<p>variables in our survey. Owing to the differences in scaling, it would not be
</p>
<p>meaningful to make comparisons across any respondent&rsquo;s measurement scales.
</p>
<p>These differences can be corrected by standardizing variables.
</p>
<p>A popular way of standardizing data is by rescaling these to have a mean of 0 and
</p>
<p>a variance of 1. This type of standardization is called the z-standardization.
</p>
<p>Mathematically, standardized scores zi (also called z-scores) can be obtained by
</p>
<p>7In Stata, this is best done using the rowmean command. For example, egen commit-
ment &frac14; rowmean (com1 com2 com3). This command automatically calculates the mean
over the number of nonmissing responses.
</p>
<p>5.6 Transform Data (Optional) 121</p>
<p/>
</div>
<div class="page"><p/>
<p>subtracting the mean �x of every observation xi and dividing it by the standard
deviation s. That is:
</p>
<p>zi &frac14;
xi � �x&eth; &THORN;
</p>
<p>s
</p>
<p>Range standardization (ri) is another standardization technique which scales the
data in a specific range. For example, standardizing a set of values to a range of 0 to
</p>
<p>1 requires subtracting the minimum value of every observation xi and then dividing
it by the range (the difference between the maximum and minimum value).
</p>
<p>ri &frac14;
xi � xmin&eth; &THORN;
xmax � xmin&eth; &THORN;
</p>
<p>The range standardization is particularly useful if the mean, variance, and ranges of
</p>
<p>different variables vary strongly and are used for some forms of cluster analysis (see
</p>
<p>Chap. 9).
</p>
<p>A log transformation&mdash;another type of transformation&mdash;is commonly used if
</p>
<p>we have skewed data. Skewed data occur if we have a variable that is asymmetri-
</p>
<p>cally distributed and can be positive or negative. A positive skew (also called right-
skewed data or data skewed to the right) occurs when many observations are
concentrated on the left side of the distribution, producing a long right tail. When
</p>
<p>data are right-skewed, the mean will be higher than the median. A negative skew
(also called left-skewed data or data skewed to the left) is the opposite, meaning that
many observations are concentrated on the right of the distribution, producing a
</p>
<p>long left tail. When data are negatively skewed, the mean will be lower than the
</p>
<p>median. A histogram will quickly show whether data are skewed. Skewed data can
</p>
<p>be undesirable in analyses. Log transformations are commonly used to transform
</p>
<p>data closer to a normal distribution when the data are right-skewed (i.e., the data are
</p>
<p>non-negative). Taking a natural logarithm will influence the size of the coefficient
</p>
<p>related to the transformed variable, but will not influence the value of its outcome.8
</p>
<p>Finally, aggregation is a special type of transformation. Aggregation means that
</p>
<p>we take variables measured at a lower level to a higher level. For example, if we
</p>
<p>know the average customer&rsquo;s satisfaction with an airline and the distribution
</p>
<p>channels from which they buy (i.e., the Internet or a travel agent), we can calculate
</p>
<p>the average satisfaction at the channel level. Aggregation only works from lower to
</p>
<p>higher levels and is useful if we want to compare groups at a higher level.
</p>
<p>8The logarithm is calculated as follows: If x&frac14; yb, then y&frac14; logb(x) where x is the original variable,
b the logarithm&rsquo;s base, and y the exponent. For example, log 10 of 100 is 2. Logarithms cannot be
calculated for negative values (such as household debt) and for the value of zero. In Stata, you can
generate a log-transformed variable by typing: gen loginc &frac14; log(income), whereby
loginc refers to the newly created log-transformed variable and income refers to the income
variable.
</p>
<p>122 5 Descriptive Statistics</p>
<p/>
</div>
<div class="page"><p/>
<p>While transforming data is often necessary to ensure comparability between
</p>
<p>variables or to make the data suitable for analysis, there are also drawbacks to
</p>
<p>this procedure. Most notably, we may lose information during most
</p>
<p>transformations. For example, recoding the ticket price (measured at the
ratio scale) as a &ldquo;low,&rdquo; &ldquo;medium,&rdquo; and &ldquo;high&rdquo; ticket price will result in an
</p>
<p>ordinal variable. In the transformation process, we have therefore lost infor-
</p>
<p>mation by going from a ratio to an ordinal scale. Another drawback is that
</p>
<p>transformed data are often more difficult to interpret. For example, the log
</p>
<p>(ticket price) is far more difficult to interpret and less intuitive than simply
using the ticket price.
</p>
<p>5.7 Create a Codebook
</p>
<p>After all the variables have been organized and cleaned, and some initial descriptive
</p>
<p>statistics have been calculated, we can create a codebook, containing essential
</p>
<p>details of the data collection and data files, to facilitate sharing. Codebooks usually
</p>
<p>have the following structure:
</p>
<p>Introduction: The introduction discusses the goal of the data collection, why the
data are useful, who participated, and how the data collection effort was
</p>
<p>conducted (mail, Internet, etc.).
</p>
<p>Questionnaire(s): It is common practice to include copies of all the types of
questionnaires used. Thus, if different questionnaires were used for different
</p>
<p>respondents (e.g., for French and Chinese respondents), a copy of each original
</p>
<p>questionnaire should be included. Differences in wording may afterwards
</p>
<p>explain the results of the study, particularly those of cross-national studies,
</p>
<p>even if a back-translation was used (see Chap. 4). These are not the
</p>
<p>questionnaires received from the respondents themselves, but blank copies of
</p>
<p>each type of questionnaire used. Most codebooks include details of each variable
</p>
<p>as comments close to the actual items used. If a dataset was compiled using
</p>
<p>secondary measures (or a combination of primary and secondary data), the
</p>
<p>secondary datasets are often briefly discussed (the version that was used, when
</p>
<p>it was accessed, etc.).
</p>
<p>Description of the variables: This section includes a verbal description of each
variable used. It is useful to provide the variable name as used in the data file, a
</p>
<p>description of what the variable is supposed to measure, and whether the
</p>
<p>measure has previously been used. You should also describe the measurement
</p>
<p>level (see Chap. 3).
</p>
<p>Summary statistics: This section includes descriptive statistics of each variable. The
average (only for interval and ratio-scaled data), minimum, and maximum are
</p>
<p>often shown. In addition, the number of observations and usable observations
</p>
<p>(excluding observations with missing values) are included, just like a histogram
</p>
<p>(if applicable).
</p>
<p>5.7 Create a Codebook 123</p>
<p/>
</div>
<div class="page"><p/>
<p>Datasets: This last section includes the names of the datasets and sometimes the
names of all the revisions of the used datasets. Codebooks sometimes include the
</p>
<p>file date to ensure that the right files are used.
</p>
<p>5.8 The Oddjob Airways Case Study
</p>
<p>The most effective way of learning statistical methods is to apply them to a set of
</p>
<p>data. Before introducing Stata and how to use it, we present the dataset from a
</p>
<p>fictitous company called Oddjob Airways (but with a real website, http://www.
oddjobairways.com) that will guide the examples throughout this book. The dataset
</p>
<p>oddjob.dta ( Web Appendix ! Downloads) stems from a customer survey of
Oddjob Airways. Founded in 1962 by the Korean businessman Toshiyuki Sakata,
</p>
<p>Oddjob Airways is a small premium airline, mainly operating in Europe, but also
</p>
<p>offering flights to the US. In an effort to improve its customers&rsquo; satisfaction, the
</p>
<p>company&rsquo;s marketing department contacted all the customers who had flown with
</p>
<p>the airline during the last 12 months and were registered on the company website. A
</p>
<p>total of 1,065 customers who had received an email with an invitation letter
</p>
<p>completed the survey online.
</p>
<p>The survey resulted in a rich dataset with information about travelers&rsquo; demo-
</p>
<p>graphic characteristics, flight behavior, as well as their price/product satisfaction
</p>
<p>with and expectations in respect of Oddjob Airways. Table 5.5 describes the
</p>
<p>variables in detail.
</p>
<p>5.8.1 Introduction to Stata
</p>
<p>Stata is a computer package specializing in quantitative data analysis, and widely
</p>
<p>used by market researchers. It is powerful, can deal with large datasets, and
</p>
<p>relatively easy to use. In this book, we use Stata MP4 14.2 (to which we simply
</p>
<p>refer to as Stata). Prior versions (12 or higher) for Microsoft Windows, Mac or
</p>
<p>Linux can be used for (almost) all examples throughout the book.
</p>
<p>Stata offers a range of versions and packages; your choice of these depends on
</p>
<p>the size of your dataset and the data processing speed. Stata/SE and Stata/MP are
</p>
<p>recommended for large datasets. The latter is the fastest and largest version of Stata.
</p>
<p>Stata/MP can, for example, process large datasets with up to 32,767 variables and
</p>
<p>20 billion observations, while Stata/SE can process datasets with the same number
</p>
<p>of variables, but a maximum of 2.14 billion observations. Stata/IC is recommended
</p>
<p>124 5 Descriptive Statistics</p>
<p/>
<div class="annotation"><a href="http://www.oddjobairways.com">http://www.oddjobairways.com</a></div>
<div class="annotation"><a href="http://www.oddjobairways.com">http://www.oddjobairways.com</a></div>
</div>
<div class="page"><p/>
<p>Table 5.5 Variable description and label names of the Oddjob Dataset
</p>
<p>Variables Variable description
Variable name
in the dataset
</p>
<p>Demographic measures
</p>
<p>Age of the customer Numerical variable ranging between the ages of
19 and 101.
</p>
<p>Age
</p>
<p>Customer&rsquo;s gender Dichotomous variable, where 1&frac14; Female; 2&frac14;Male. Gender
Language of
customer
</p>
<p>Categorical variable, where 1 &frac14; German;
2 &frac14; English; 3 &frac14; French.
</p>
<p>Language
</p>
<p>Home country Categorical variable, whereby: 1 &frac14; Germany (de),
2 &frac14; Switzerland (ch); 3 &frac14; Austria (at); 4 &frac14; France
(fr), 5 &frac14; the United States (us).
</p>
<p>Country
</p>
<p>Flight behaviour measures
</p>
<p>Flight class Categorical variable distinguishing between the
following categories: 1 &frac14; First; 2 &frac14; Business;
3 &frac14; Economy.
</p>
<p>flight_class
</p>
<p>Latest flight Categorical variable querying when the customer last
flew with Oddjob Airways. Categories are:
1&frac14;Within the last 2 days; 2&frac14;Within the last week;
3 &frac14; Within the last month; 4 &frac14; Within the last
3 months; 5 &frac14; Within the last 6 months; 6 &frac14; Within
the last 12 months.
</p>
<p>flight_latest
</p>
<p>Flight purpose Dichotomous variable distinguishing between:
1 &frac14; Business; 2 &frac14; Leisure.
</p>
<p>flight_purpose
</p>
<p>Flight type Dichotomous variable, where: 1 &frac14; Domestic;
2 &frac14; International.
</p>
<p>flight_type
</p>
<p>Number of flights Numeric variable ranging between 1 and 457 flights
per year.
</p>
<p>nflights
</p>
<p>Traveler&rsquo;s status Categorical variable, where membership status is
defined in terms of: 1 &frac14; Blue; 2 &frac14; Silver; 3 &frac14; Gold.
</p>
<p>status
</p>
<p>Perception and satisfaction measures
</p>
<p>Traveler&rsquo;s
expectations
</p>
<p>23 items reflecting a customer&rsquo;s expectations with
the airline: &ldquo;How high are your expectations that. . .&rdquo;
All items are measured on a continuous scale ranging
from 1 very low to 100 very high.
</p>
<p>e1 to e23
</p>
<p>Traveler&rsquo;s
satisfaction
</p>
<p>23 items reflecting a customer&rsquo;s satisfaction with
Oddjob Airways regarding the features asked in the
expectation items (e1-e23) on a continuous scale
ranging from 1=very unsatisfied to 100=very
satisfied.
</p>
<p>s1 to s23
</p>
<p>Recommendation Item on whether a customer is likely to recommend
the airline to a friend or colleague. This item is
measured on an 11-point Likert-scale ranging from
1 very unlikely to 11 very likely.
</p>
<p>nps
</p>
<p>Reputation One item stating &ldquo;Oddjob Airways is a reputable
airline.&rdquo; This item is measured on a 7-point Likert-
scale ranging from 1 fully disagree to 7 fully agree.
</p>
<p>reputation
</p>
<p>Overall price/
performance
satisfaction
</p>
<p>One item stating &ldquo;Overall I am satisfied with the
price performance ratio of Oddjob Airways.&rdquo; This
item is measured on a 7-point Likert-scale ranging
from 1 fully disagree to 7 fully agree.
</p>
<p>overall_sat
</p>
<p>(continued)
</p>
<p>5.8 The Oddjob Airways Case Study 125</p>
<p/>
</div>
<div class="page"><p/>
<p>for moderate-sized datasets. This can process datasets with a maximum of 2,047
</p>
<p>variables and up to 2.14 billion observations. Small Stata is only available for
</p>
<p>students and handles small-sized datasets with a maximum of 99 variables and
</p>
<p>1,200 observations. To obtain a copy of Stata, check the stata.com website, or ask
</p>
<p>the IT department, or your local Stata distributor. Special student and faculty prices
</p>
<p>are available.
</p>
<p>In the next sections, we will use the ► sign to indicate that you should click
</p>
<p>on something. Options, menu items or drop-down lists that you should look
</p>
<p>up in dialog boxes are printed in bold. Variable names, data files or data
</p>
<p>formats are printed in italics to differentiate them from the rest of the text.
Finally, Stata commands are indicated in Courier.
</p>
<p>5.8.2 Finding Your Way in Stata
</p>
<p>5.8.2.1 The Stata Main/Start Up Window and the Toolbar
If you start up Stata for the first time, it presents a screen as shown in Fig. 5.7. This
</p>
<p>start up screen is the main Stata window in the Mac version.9
</p>
<p>The main Stata window in Fig. 5.7 consists of five sub-windows. The first
</p>
<p>sub-window on the left of the start-up screen is called the Review window, which
</p>
<p>displays the history of commands since starting a session. Successful commands are
</p>
<p>displayed in black, and those containing errors are displayed in red. If you click on one
</p>
<p>of the past commands displayed in theReviewwindow, itwill be automatically copied
</p>
<p>Table 5.5 (continued)
</p>
<p>Variables Variable description
Variable name
in the dataset
</p>
<p>General satisfaction 3 items reflecting a customer&rsquo;s overall satisfaction
with the airline. All items are measured on a 7-point
Likert scale ranging from 1 fully disagree to 7 fully
agree.
</p>
<p>sat1 to sat3
</p>
<p>Loyalty 5 items reflecting a customer&rsquo;s loyalty to the airline.
All items are measured on a 7-point Likert-scale
ranging from 1 fully disagree to 7 fully agree.
</p>
<p>loy1 to loy5
</p>
<p>Commitment 3 items reflecting a customer&rsquo;s commitment to fly
with the airline. All items are measured on a 7-point
Likert-scale ranging from 1 fully disagree to 7 fully
agree.
</p>
<p>com1 to com3
</p>
<p>9If you open Stata in the Windows or Linux operating systems, the toolbar looks a bit different, but
is structured along the same lines as discussed in this chapter.
</p>
<p>126 5 Descriptive Statistics</p>
<p/>
</div>
<div class="page"><p/>
<p>into theCommand window, which is located at the bottom of the central screen. The
</p>
<p>Review window stores and displays your commands. It allows you to recall all
</p>
<p>previous commands, edit, and re-submit them if youwish. The output of your analyses
</p>
<p>is displayed in the Results window, located in the center. The Variables window
</p>
<p>(upper-right) lists all the variables included in the dataset, whereas the Properties
</p>
<p>window (lower-right) displays the features of the selected variable(s).
</p>
<p>Stata&rsquo;s toolbar is located underneath its menu bar. This toolbar contains a range of
</p>
<p>shortcuts to frequently used commands, including opening, saving, printing, view-
</p>
<p>ing, and editing your data. An overview of the toolbar icons is shown in Table 5.6.
</p>
<p>5.8.2.2 The Menu Bar
Stata&rsquo;s menu bar includes File, Edit, Data,Graphics, Statistics, Users,Windows,
</p>
<p>and Help, which will be discussed briefly in this section. The menu options
</p>
<p>Graphics and Statistics will, later in this chapter be discussed in greater detail
</p>
<p>by means of examples.
</p>
<p>You can open Stata&rsquo;s dialog box by simply typing db, followed by the
</p>
<p>operation (i.e., edit, describe, copy, etc.) or technique (regression) that you
</p>
<p>wish to carry out. For example, if you wish to open the data editor, you could
</p>
<p>type: db edit. Similarly, if you wish to specify your regression model, you
</p>
<p>could type db regress, which will open the dialog window for the regres-
</p>
<p>sion model.
</p>
<p>Fig. 5.7 The Stata interface
</p>
<p>5.8 The Oddjob Airways Case Study 127</p>
<p/>
</div>
<div class="page"><p/>
<p>File
</p>
<p>Format Types
</p>
<p>Stata uses multiple file formats. The .dta file format only contains data. Stata can
also import other file formats such as Excel (.xls and .xlsx), SAS, SPSS and can read
text files (such as .txt and .dat). Once these files are open, they can be saved as
Stata&rsquo;s .dta file format. Under ► File, you find all the commands that deal with the
opening, closing, creating, and saving of the different types of files. In the startup
</p>
<p>screen, Stata shows several options for creating or opening datasets. The options
</p>
<p>that you should use are either Open Recent, under which you can find a list with
</p>
<p>recently opened data files, or New Files.
</p>
<p>Table 5.6 Toolbar icons
</p>
<p>Stata&rsquo;s toolbar in detail:
</p>
<p>Symbol Action
</p>
<p>Opens dataset by selecting a dataset from a menu.
</p>
<p>Saves the active dataset.
</p>
<p>Prints contents of a selected window.
</p>
<p>Log begins/closes/suspends/resumes the current log. It also allows for
viewing the current log if a log file is open.
</p>
<p>Opens the viewer that provides advice on finding help and how to search
through Stata&rsquo;s online resources.
</p>
<p>Brings the graph window to the front.
</p>
<p>Opens a new do-file editor or brings a do-file editor to the front.
</p>
<p>Opens the data editor (edit) or brings the data editor to the front. It allows you
to edit variables.
</p>
<p>Opens the data editor (browse) or brings the data editor to the front. It allows
you to browse through the variables.
</p>
<p>This tells Stata to show more output.
</p>
<p>This tells Stata to interrupt the current task.
</p>
<p>Enables searching for help for Stata commands.
</p>
<p>128 5 Descriptive Statistics</p>
<p/>
</div>
<div class="page"><p/>
<p>You can import different types of files into Stata if you go to ► File ► Import,
</p>
<p>select the file format, and then select the file you wish to open. The Example
</p>
<p>dataset menu item is particularly useful for learning new statistical methods, as it
</p>
<p>provides access to all the example datasets mentioned under the titles of the various
</p>
<p>user manuals for a particular statistical method.
</p>
<p>Stata has an extensive number of user-written programs, which, once
</p>
<p>installed, act as Stata commands. If you know the name of the program,
</p>
<p>you can directly type help, followed by a keyword, which initiates a
</p>
<p>keyword search. Alternatively, if you do not know the name of the command,
</p>
<p>but are looking for a specific method, such as cluster analysis, you can type:
</p>
<p>help cluster to initiate a keyword search for this method. This will open a
</p>
<p>new window containing information about this technique from the help files,
</p>
<p>the Stata reference manual, the Stata Journal, the Frequently Asked
</p>
<p>Questions, references to other articles, and other help files.
</p>
<p>Stata .do Files
</p>
<p>In addition to the menu functionalities, we can run Stata by using its command
</p>
<p>language, called Stata command. Think of this as a programming language that
</p>
<p>can be directly used (if you feel comfortable with its command) or via dialog boxes.
</p>
<p>Stata&rsquo;s commands can be saved (as a .do file) for later use. This is particularly
useful if you undertake the same analyses across different datasets. Think, for
</p>
<p>example, of standardized marketing reports on daily, weekly, or monthly sales.
</p>
<p>Note that discussing Stata .do files in great detail is beyond the scope of this book,
but, as we go through the different chapters, we will show all the Stata&rsquo;s commands
</p>
<p>that we have used in this book.
</p>
<p>Edit
</p>
<p>This menu option allows you to copy Stata output and analysis, and paste the
</p>
<p>content into, for example, a Word document. If you want to copy a table from your
</p>
<p>output, first select the table and then go to► Edit► Copy table. Remember that you
</p>
<p>need to have an output to make use of this option!
</p>
<p>View
</p>
<p>The Data Editor button is listed first in this menu option and brings the data editor
</p>
<p>to the front. The data editor looks like a spreadsheet (Fig. 5.8) listing variables in
</p>
<p>columns and the corresponding observations in rows. String variables are displayed
</p>
<p>in red, value labels and encoded numeric variables with value labels in blue, and
</p>
<p>numeric variables in black. The data editor can be used to enter new or amend old
</p>
<p>data across the rows or the columns of the spreadsheet. Entering the new value on
</p>
<p>the cell and pressing Enter will take you to the next row, while, if you press Tab,
</p>
<p>you are able to work across the rows. Blank columns and rows will be marked as
</p>
<p>missing, so make sure that you do not skip columns and rows when entering new
</p>
<p>data. Note that there are different types of variables in the data editor and their
</p>
<p>5.8 The Oddjob Airways Case Study 129</p>
<p/>
</div>
<div class="page"><p/>
<p>properties can be changed in the Properties sub-window located at the bottom right
</p>
<p>of the data editor screen. Another way to bring Data Editor to the front is by typing
</p>
<p>edit in the command window of the Main/Startup Window. Similarly, typing
</p>
<p>browse will open up the data editor (browse) mode, which allows you to navigate
</p>
<p>through your dataset.
</p>
<p>The View menu (which is only found on the Mac version) includes other editing
</p>
<p>options that work in combination with .do files (Do-file Editor), graphs (Graph
Editor), and structural equation modelling (SEM) (SEM-Builder). These options
</p>
<p>are beyond the scope of this book and will not be discussed in detail here.
</p>
<p>Data
</p>
<p>The Data menu provides subcommands that allow for summarizing, inspecting,
</p>
<p>creating, changing or restructuring your dataset.
</p>
<p>Under► Data► Describe data, you can view or inspect the dataset in a detailed
</p>
<p>way, including the content, type, and values of the variables included in the dataset.
</p>
<p>This offers useful information that complements the compact overview that you can
</p>
<p>obtain from ► Data ► Describe data ► Summary statistics.
</p>
<p>Under ► Data ► Data Editor, you can access the different data editor modes
</p>
<p>(i.e., the edit and browse mode). As discussed above, these options are useful if you
</p>
<p>want to browse through the specific records in your data (i.e., the browse mode) or
</p>
<p>wish to change the content of a specific cell (i.e., the edit mode).
</p>
<p>By going to► Data ► Sort, you can sort the data according to the values of one
</p>
<p>or more specific variable(s). Data are usually sorted according to the respondents&rsquo;
</p>
<p>key identifying variable; this a numerical variable that is often called the id
variable. Depending on the purpose of the analysis, you could, for example, sort
</p>
<p>Fig. 5.8 The Stata data editor
</p>
<p>130 5 Descriptive Statistics</p>
<p/>
</div>
<div class="page"><p/>
<p>the data according to the age of the respondents, which will sort the observations in
an ascending order, listing the youngest respondents first, followed by the older
</p>
<p>respondents in the subsequent rows. Alternatively, Advanced sort in the same
</p>
<p>dialog box allows the observations to be arranged in descending order.
</p>
<p>Under ► Data ► Create or change data, you find several options for creating
</p>
<p>new variables. The options ► Create new variable and ► Create new variable
</p>
<p>(extended) allow you to create a new variable from one (or more) existing variables.
</p>
<p>To change or recode the contents of existing variables, you need to select the option
</p>
<p>► Other variable-transformation commands ► Recode categorical variable. This
</p>
<p>command allows you to recode variable values or to combine sets of values into one
</p>
<p>value. For example, as shown in Fig. 5.12, you could create a new variable called
</p>
<p>age_dummy that splits the sample into young to middle-aged (say, 40 or less) and
old passengers (say, more than 40).
</p>
<p>An important element of the Data menu is the Variables Manager, which is a
</p>
<p>tool that allows you to organize and structure the variables in your dataset (see
</p>
<p>Fig. 5.9). Among others it allows you to:
</p>
<p>1. Sort variables: One click on the first column of Variables Manager will sort the
</p>
<p>variables in an ascending order, while a second click will sort them in a
</p>
<p>descending order. If you want to restore the order of the variable list, right-
</p>
<p>click on the first column and select the option Restore column defaults.
</p>
<p>2. Select by filtering through the variable list and to change the variable properties:
</p>
<p>Enter the first letter or name of a variable in the filter box on the upper-left part of
</p>
<p>the screen to filter through the list of the included variables. This is especially
</p>
<p>useful if you want to zoom into a series of variables with similar names. Entering
</p>
<p>e1 in the filter box, for example, will search everything that contains the value
</p>
<p>Fig. 5.9 The variables manager
</p>
<p>5.8 The Oddjob Airways Case Study 131</p>
<p/>
</div>
<div class="page"><p/>
<p>e1. It will bring all variables to the front that have e1 in their root, including e1,
e10, e11 to e19. Once you click on a specific variable (e.g., e1), the name, label,
type, format, value labels, and notes under Variable properties will be
</p>
<p>populated; their content can then be edited if necessary. The properties of each
</p>
<p>of these boxes is briefly discussed below:
</p>
<p>&bull; Name: lists the name of the specified variable. Stata is case sensitive,
</p>
<p>meaning that the variable e1 differs from E1. Variable names must begin
with letters (a to z) or an underscore (_). Subsequent characters can include
</p>
<p>letters (a to z), numbers (0&ndash;9) or an underscore (_). Note that neither spaces
</p>
<p>nor special characters (e.g., %, &amp;, /) are allowed.
</p>
<p>&bull; Label: allows for a longer description of the specified variable. This can, for
</p>
<p>example, be the definition of the variable or the original survey question.
</p>
<p>Click on the tick box to select the option to attach a label to a new variable
</p>
<p>and type in the preferred label for this variable.
</p>
<p>&bull; Type: specifies the output format type of the variable. String refers to words
</p>
<p>and is stored as str#, indicating the string&rsquo;s maximum length. String
</p>
<p>variables are useful if you want to include open-ended answers, email
</p>
<p>addresses or any other type of information that is not a number. Numbers
</p>
<p>are stored in Stata as byte, int, long, float, or double.10
</p>
<p>&bull; Format: describes the display format associated with a specified variable. As
</p>
<p>described in the Stata manual,11 formats are denoted by a % sign, followed by
</p>
<p>the number of characters (i.e., width) and the number of digits following the
</p>
<p>decimal points. For example, %10.2g means that our display format
</p>
<p>(indicated by %) should be 10 characters wide (indicated by the number 10)
</p>
<p>with two digits (indicated by the number 2 following the decimal point). The
</p>
<p>g format indicates that Stata fills the 10 display characters with as much as it
</p>
<p>can fit. In addition, Stata has a range of formats for dates and time stamps to
</p>
<p>control how data are displayed.12
</p>
<p>&bull; Value label: presents a description of the values that a specified variable
</p>
<p>takes. It allows the researcher to create, edit or drop the label of a specified
</p>
<p>variable. The value labels for gender, for example, can be specified as female
(for values coded as 1) and male (for values coded as 0).
</p>
<p>3. Keep variables: if you wish to work with a subset of your variables, select all the
</p>
<p>relevant variables (by dragging the selected variables) and right-click. Then
</p>
<p>select the option Keep only selected variables. Note that this should only be
done after careful consideration, as dropping relevant variables can ruin the
</p>
<p>dataset!
</p>
<p>4. Drop variables: this is similar to the previous action, but now you only select the
</p>
<p>variables that you want to drop from the variables list. You can do so by first
</p>
<p>selecting the variables that you want to drop, then right-click and select the
</p>
<p>10http://www.stata.com/manuals14/ddatatypes.pdf
11http://www.stata.com/manuals14/u.pdf
12http://www.stata.com/manuals14/dformat.pdf
</p>
<p>132 5 Descriptive Statistics</p>
<p/>
<div class="annotation"><a href="http://www.stata.com/manuals14/ddatatypes.pdf">http://www.stata.com/manuals14/ddatatypes.pdf</a></div>
<div class="annotation"><a href="http://www.stata.com/manuals14/u.pdf">http://www.stata.com/manuals14/u.pdf</a></div>
<div class="annotation"><a href="http://www.stata.com/manuals14/dformat.pdf">http://www.stata.com/manuals14/dformat.pdf</a></div>
</div>
<div class="page"><p/>
<p>option Drop selected variables. Here, you also think very carefully before
dropping variables from the list!
</p>
<p>The default missing value in Stata is called system missing, which is indicated
</p>
<p>by a period (.) in the dataset. In addition, there are 26 missing values, also
</p>
<p>called extended missing values, ranging from .a to .z, depending on how the
data are stored. These are necessary to understand the type of the missing
</p>
<p>data. As discussed in the Missing Data section, missing values arise for
various reasons. In some occasions, such as in panel studies where the same
</p>
<p>respondent is approached repeatedly, some respondents may refuse to answer
</p>
<p>a question at each interview leading to missing observations in some
</p>
<p>interviews. It could also be that the researcher decides to skip a question
</p>
<p>from the questionnaire. While both situations lead to missing values, their
</p>
<p>nature differs. The extended missing values in Stata allow the researcher to
</p>
<p>distinguish between these different reasons by assigning an .a to the first
</p>
<p>situation and .b to the second situation, etc.
</p>
<p>Graphics
</p>
<p>Under► Graphics, Stata offers a range of graphical options and tools with which to
</p>
<p>depict data. These vary from graphical options that support distributional graphs,
</p>
<p>including two-way graphs, charts, histograms, box and contour plots to graphs that
</p>
<p>support more advanced statistical methods such as time series, panel, regression,
</p>
<p>survival techniques, etc. We will discuss the application and interpretation of the
</p>
<p>different plots as we move through the different chapters and statistical techniques
</p>
<p>in this book.
</p>
<p>Statistics
</p>
<p>Under ► Statistics, you find numerous analysis procedures, several of which we
</p>
<p>will discuss in the remainder of the book. For example, under Summaries, tables,
</p>
<p>and tests, you can request univariate and bivariate statistics. The rest are numerous
</p>
<p>types of regression techniques, as well as a range of other multivariate analysis
</p>
<p>techniques. We will discuss descriptive statistics in the next section. In Chap. 6, we
</p>
<p>will describe models that fall within the group ► Linear models and related, while
</p>
<p>Chap. 7 will discuss techniques in the ► Multivariate analysis group.
</p>
<p>User
</p>
<p>Under►User, you find three empty data, graphs, and statistics subdirectories. Stata
</p>
<p>programmers can use these to add menu options.
</p>
<p>Window
</p>
<p>The►Window option enables you to bring the different types of windows to front.
</p>
<p>You can also zoom in or minimize the screen.
</p>
<p>5.8 The Oddjob Airways Case Study 133</p>
<p/>
</div>
<div class="page"><p/>
<p>Help
</p>
<p>The ► Help function may come in handy if you need further guidance. Under ►
</p>
<p>Help, you find documentations and references that show you how to use most of the
</p>
<p>commands included in Stata.
</p>
<p>5.9 Data Management in Stata
</p>
<p>In this section, we will illustrate the application of some of the most commonly used
</p>
<p>commands for managing data in Stata. These include the following:
</p>
<p>&ndash; restrict observations,
</p>
<p>&ndash; create a new variable from existing variable(s), and
</p>
<p>&ndash; recode variables.
</p>
<p>5.9.1 Restrict Observations
</p>
<p>You can also use the Summary Statistics command to restrict certain observations
</p>
<p>from the analyses by means of a pre-set condition (e.g., display the summary
</p>
<p>statistics only for those between 25 and 54 years old). To restrict observations, go
</p>
<p>to ► Data ► Describe data ► Summary statistics, which opens a screen similar
</p>
<p>to Fig. 5.10. Under Variables: (leave empty for all variables), enter the condition
</p>
<p>if age &gt; 24 &amp; age &lt; 55. to specify your restriction and then click on OK. Stata
</p>
<p>will now only display the summary statistics for the observations that satisfy this
</p>
<p>condition. In the Stata command (see below) this restriction appears as an if
statement.
</p>
<p>summarize if age&gt;24 &amp; age&lt;55
</p>
<p>To summarize cases with only valid (non-missing) observations, it is com-
</p>
<p>mon to add the following rule after the pre-set condition: &amp; age!&frac14; missing
(). In Stata language, !&frac14; means &ldquo;not equal&rdquo; and missing() means &ldquo;all
numerical and string variables included in the dataset.&rdquo; All together, &amp;
</p>
<p>age! &frac14; missing()means &ldquo;if age is not missing from all the included
variables in the dataset&rdquo;.
</p>
<p>summarize if age&gt;24 &amp; age&lt;55 &amp; age !=missing()
</p>
<p>134 5 Descriptive Statistics</p>
<p/>
</div>
<div class="page"><p/>
<p>5.9.2 Create a New Variable from Existing Variable(s)
</p>
<p>The Create new variable (extended) command enables you to create new
</p>
<p>variables containing the interquartile range, median, row means, standardized
</p>
<p>values, and many more different options. We will not discuss all these options in
</p>
<p>this section, but will demonstrate the power of this tool by creating and index
</p>
<p>variable from the mean of the following three items related to travelers&rsquo; satisfac-
</p>
<p>tion: sat1, sat2, and sat3. Go to ► Data ► Create or change data ► Create new
variable (extended), which will open a dialog box similar to Fig. 5.11.
</p>
<p>Next, enter the name of the new variable (e.g., rating_index) in the Generate
variable box on the upper left part of the screen and select Row Mean from the
</p>
<p>Egen Function drop-down menu. Under Generate variable as type, the variable
</p>
<p>type Float should be automatically selected (i.e., this is Stata&rsquo;s default for numeric
</p>
<p>variable types). Finally, enter sat1, sat2, and sat3 in the Variables box and click on
OK. You have now created a new variable called rating_index that appears at the
bottom of the variable list. Alternatively, you can type the following command:
</p>
<p>egen float rating_index = rowmean(sat1 sat2 sat3)
</p>
<p>Fig. 5.10 Restrict observations
</p>
<p>5.9 Data Management in Stata 135</p>
<p/>
</div>
<div class="page"><p/>
<p>5.9.3 Recode Variables
</p>
<p>Recoding (i.e., changing or transforming) the values of an existing variable according
</p>
<p>to a number of rules is a key data management activity. Numeric variables can be
</p>
<p>changed by means of the recode command. Go to► Data► Create or change data
</p>
<p>► Other variable-transformation commands ► Recode categorical variables. This
</p>
<p>will open a dialog box similar to the Main Tab left in Fig. 5.12.
</p>
<p>Specify the name of the variable that you want to recode (i.e., age) under
Variables in the Main tab. Next, specify the values of the new variable under
</p>
<p>Fig. 5.11 Create new variable(s) extended
</p>
<p>Fig. 5.12 Recode into different variables (Main and Options tabs)
</p>
<p>136 5 Descriptive Statistics</p>
<p/>
</div>
<div class="page"><p/>
<p>Required. These are based on the values of the original variable (age). The
option (min/40 &frac14; 1) indicates that all values ranging from the smallest age
observations to the age of 40 should be coded as 1. Under Optional all other age
</p>
<p>observations are coded as 0 (else &frac14; 0).
Next, click on theOptions tab (right in Fig. 5.12). In the dialog box that follows,
</p>
<p>you need to indicate whether you want to: (1) Replace existing variables, (2)Gen-
</p>
<p>erate new variables, or (3) Generate new variables with this prefix. We always
</p>
<p>recommend using either the second option or the third. If you were to use the first
</p>
<p>option, any changes you make to the variable will result in the overwriting of the
</p>
<p>original variable. Consequently, if you thereafter wish to return to the original data,
</p>
<p>you will either need to revert to a saved previous version, or need to enter all the
</p>
<p>data again, because Stata cannot undo these actions! Select the second option (i.e.,
</p>
<p>Generate new Variables), enter the name of the new variable (i.e., age_dummy),
and then click on OK. Alternatively, the recoding of this variable can be obtained
</p>
<p>through the following command:
</p>
<p>recode age(min/40=1)(else=0),generate(age_dummy)
</p>
<p>You have now created a new dichotomous variable (i.e., age_dummy) located at
the bottom of the variables list.
</p>
<p>Everyone makes mistakes!
</p>
<p>If you are worried that commands may not change the data as desired, type
</p>
<p>preserve in the Command window. This keeps a snapshot of the data in
</p>
<p>the computer&rsquo;s memory. Should you wish to revert, simply type restore to
</p>
<p>go back to where you were. You could also save a copy of the dataset under a
</p>
<p>new name as a milestone and then later delete it manually. This allows you to
</p>
<p>back up multiple steps and across work sessions.
</p>
<p>5.10 Example
</p>
<p>We will now examine the dataset Oddjob.dta in closer detail by following all the
steps in Fig. 5.1. Cleaning the data generally requires checking for interviewer
</p>
<p>fraud, suspicious response patterns, data entry errors, outliers, and missing data.
</p>
<p>Several of these steps rely on statistics and graphs, which we discussed in the
</p>
<p>context of descriptive statistics (e.g., box plots and scatter plots). Note that missing
</p>
<p>values strategies and the multiple imputation technique will be described and
</p>
<p>illustrated by means of Oddjob.dta in the Web Appendix (! Downloads).
</p>
<p>5.10 Example 137</p>
<p/>
</div>
<div class="page"><p/>
<p>5.10.1 Clean Data
</p>
<p>Since the data were cleaned earlier, we need not check for interviewer fraud or
</p>
<p>suspicious response patterns. Beside double data entries to detect and minimize
</p>
<p>errors in the process of data entry, exploratory data analysis is required to spot data
</p>
<p>entry errors that have been overlooked.
</p>
<p>A first step in this procedure is to look at the minimum and maximum values of
</p>
<p>the relevant variables to detect values that are not plausible (i.e., fall outside the
</p>
<p>expected range of scale categories). To do so, go to ► Data ► Describe Data ►
</p>
<p>Summary statistics, which opens a dialog box like the one in Fig. 5.13. The
</p>
<p>Variables box should be left empty to obtain the statistics for all the variables in
</p>
<p>the dataset. Next, select the Standard display option that requires the number of
</p>
<p>observations, the mean, standard deviation, minimum, and maximum values. Pro-
</p>
<p>ceed by clicking onOK. Alternatively, the dialog box for summary statistics can be
</p>
<p>brought to front by typing db summarize in the Command window and clicking
</p>
<p>on enter.
</p>
<p>Table 5.7 shows a partial display of the summary statistics, including the number
</p>
<p>of observations, means, standard deviation, as well as minimum and maximum
</p>
<p>values. Under Obs, we can see that all the listed variables are observed across all
</p>
<p>1,065 respondents, meaning that none of the selected variables suffer from missing
</p>
<p>observations. Among others, it appears that the age of the travelers varies between
</p>
<p>Fig. 5.13 Summary statistics dialog box
</p>
<p>138 5 Descriptive Statistics</p>
<p/>
</div>
<div class="page"><p/>
<p>19 and 101, while the number of flights varies between 1 and 457 flights over the
past 12 months. Particularly the maximum value in number of flights appears to be
</p>
<p>implausible. While this observation could represent a flight attendant, it appears
</p>
<p>more reasonable to consider this observation an outlier, which may need to be
</p>
<p>eliminated, depending on the type of analysis.
</p>
<p>5.10.2 Describe Data
</p>
<p>In the next step, we describe the data in more detail, focusing on those statistics and
</p>
<p>graphs that were not part of the previous step. To do so, we make use of graphs,
</p>
<p>tables, and descriptive statistics. In Fig. 5.14, we show how you can ask for each
</p>
<p>previously discussed graph, table, and statistic in Stata.
</p>
<p>5.10.2.1 Univariate Graphs and Tables
</p>
<p>Bar Charts
</p>
<p>To produce a bar chart that plots the age of respondents against their country of
residence, go to ► Graphics ► Bar chart. This will take you to a dialog box where
</p>
<p>theMain tab is displayed by default (left of Fig. 5.15). Under Type of data specify
</p>
<p>the type of bar chart that you want displayed (Graph of summary statistics). Next,
</p>
<p>under Orientation, you should select the option Vertical, given that it is Stata&rsquo;s
</p>
<p>default and indicates the direction in which the bar chart is displayed.
</p>
<p>Under Statistics to plot, select the variable age under Variables and indicate
that you want to display the mean of this variable for all valid observations by
</p>
<p>selecting the optionmean. Next, click on the Categories tab (displayed to the right
</p>
<p>of Fig. 5.15) to indicate how you want to categorize the data in the bar chart. Tick
</p>
<p>the first box, Group 1 and select the variable country from the drop-down menu
under Grouping Variable.
</p>
<p>Table 5.7 A (partial) output of summary statistics
</p>
<p>Variable |        Obs        Mean    Std. Dev.       Min        Max
-------------+---------------------------------------------------------
</p>
<p>age |      1,065    50.41972    12.27464         19        101
country |      1,065           2    1.551739          1          5
</p>
<p>flight_class |      1,065    2.798122    .4352817          1          3
flight_lat~t |      1,065    3.788732    1.368779          1          6
flight_pur~e |      1,065    1.507042    .5001853          1          2
-------------+---------------------------------------------------------
flight_type |      1,065    1.476056   .499661          1          2
</p>
<p>gender |      1,065    1.737089    .4404212          1          2
</p>
<p>language |      1,065    1.237559    .4473162          1          3
nflights |      1,065    13.41878    20.22647          1        457
</p>
<p>status |      1,065    1.498592    .7204373          1          3
-------------+---------------------------------------------------------
</p>
<p>5.10 Example 139</p>
<p/>
</div>
<div class="page"><p/>
<p>By default, Stata displays the value labels of the grouping variable horizontally,
</p>
<p>but you can change this. By clicking on the Properties button (see Fig. 5.16), which
</p>
<p>is next to the Grouping variable box, set Labels to Angle: 45� and then click
Accept. Stata will then show a graph as in Fig. 5.17.
</p>
<p>Describe Data
</p>
<p>Univariate Bivariate
</p>
<p>Measures of centrality:
</p>
<p>&bull; Mode
</p>
<p>&bull; Median
</p>
<p>&bull; Mean
</p>
<p>Measures of dispersion:
</p>
<p>&bull; Range
</p>
<p>&bull; Interquartile Range
</p>
<p>&bull; Variance
</p>
<p>&bull; Standard deviation
</p>
<p>► Statistics ►
</p>
<p>Summaries, tables, and
</p>
<p>tests ► Other tables ►
</p>
<p>Compact table of
</p>
<p>summary statistics, select
</p>
<p>Scatter plots
</p>
<p>► Graphics ► Twoway
</p>
<p>graph, select Scatter.
</p>
<p>► Graphics ►
</p>
<p>Scatterplot matrix
</p>
<p>Correlation
</p>
<p>► Statistics ►
</p>
<p>Summaries, tables, and
</p>
<p>tests ►Summary and
</p>
<p>descriptive statistics ►
</p>
<p>Correlations and
</p>
<p>covariances.
</p>
<p>Statistics Statistics
</p>
<p>Crosstabs
</p>
<p>► Statistics
</p>
<p>Summaries, tables,
</p>
<p>tables, and tests ►
</p>
<p>Frequency tables ►
</p>
<p>Two-way table
</p>
<p>Graphs &amp; tables Graphs &amp; tables
</p>
<p>Bar chart
</p>
<p>► Graphics ► Bar
</p>
<p>chart
</p>
<p>Histogram
</p>
<p>► Graphics ►
</p>
<p>Histogram
</p>
<p>Box plot
</p>
<p>► Graphics ►Box plot
</p>
<p>Pie chart
</p>
<p>► Graphics ►Pie chart
</p>
<p>Frequency table
</p>
<p>► Statistics ► Summaries,
</p>
<p>tables, tables, and tests ►
</p>
<p>Frequency tables ► One-
</p>
<p>way table
</p>
<p>Covariance
</p>
<p>► Statistics ►
</p>
<p>Summaries, tables, and
</p>
<p>tests ►Summary and
</p>
<p>descriptive statistics ►
</p>
<p>Correlations and
</p>
<p>covariances, go to
</p>
<p>50
th
</p>
<p> percentile, Mean,
</p>
<p>Range, Interquartile
</p>
<p>range, Variance,
</p>
<p>Standard deviation.
Options select Display
</p>
<p>covariances.
</p>
<p>Fig. 5.14 How to ask for graphs, tables, and statistics in Stata
</p>
<p>Fig. 5.15 Main dialog box, Bar chart
</p>
<p>140 5 Descriptive Statistics</p>
<p/>
</div>
<div class="page"><p/>
<p>Fig. 5.16 Properties dialog box
</p>
<p>Fig. 5.17 A bar chart
</p>
<p>5.10 Example 141</p>
<p/>
</div>
<div class="page"><p/>
<p>Tip: You can also edit your graph by right-clicking on the graph. Select the
</p>
<p>option Start Graph Editor, which will start the Graph Editing menu. Double
</p>
<p>click on the x-axis and select the option Label properties to adjust the angle
of the labels in the same way as described above. You can additionally also
</p>
<p>adjust the range of the values you want to display on the x-axis by specifying
the desired range. The same applies to adjusting the label properties of the
</p>
<p>y-axis.
</p>
<p>Histograms
</p>
<p>Histograms are useful for summarizing numerical variables. If you go to ►
</p>
<p>Graphics ► Histogram, Stata will open a dialog box as shown in Fig. 5.18 on the
</p>
<p>left. To plot a histogram that summarizes the respondents&rsquo; age, select the relevant
variable age in the Variable drop-down menu and select the Data are continuous
option. Next, specify Frequency under Y axis and click on OK. Stata will produce
</p>
<p>a histogram as shown in Fig. 5.18 on the right.
</p>
<p>Box Plot
</p>
<p>To ask for a box plot, go to ► Graphics ► Box plot, which will open a dialog box
</p>
<p>similar to the one in Fig. 5.19.
</p>
<p>Specify the option Vertical underOrientation to display the box plot vertically.
</p>
<p>Next, select the relevant variable age in the Variables box and then click onOK. A
box plot like the one in Fig. 5.20 appears.
</p>
<p>Pie Charts
</p>
<p>Pie charts are useful for displaying categorical or binary variables. Create a pie
</p>
<p>chart by going to ► Graphics ► Pie chart, which will open a dialog box similar to
</p>
<p>Fig. 5.18 Dialog box, histogram and a histogram
</p>
<p>142 5 Descriptive Statistics</p>
<p/>
</div>
<div class="page"><p/>
<p>that displayed in Fig. 5.21 on the left. In Stata, the default (standard) option is
</p>
<p>Graph by Categories. Plot respondents&rsquo; membership status (status) by selecting
status under Category variable and then clicking on OK. Stata will show a pie
chart similar to the one on the right in Fig. 5.21.
</p>
<p>Fig. 5.19 Box plots graph dialog box
</p>
<p>Fig. 5.20 Box plot
</p>
<p>5.10 Example 143</p>
<p/>
</div>
<div class="page"><p/>
<p>Frequency Tables
</p>
<p>We can produce a frequency table by clicking on► Statistics► Summaries, tables,
</p>
<p>and tests ► Frequency tables ► One-way table. Select the variable country under
Categorical variable and then click onOK. This operation will produce Table 5.8,
</p>
<p>which displays the value of each country with the corresponding absolute number
</p>
<p>of observations (i.e., Freq.), the relative values (i.e., Percent), as well as the
</p>
<p>cumulative relative values (i.e., Cum.). It shows that 65.25 percent of our sample
</p>
<p>consists of travelers who reside in Germany, followed by travelers from the United
</p>
<p>States (18.31 percent), Austria (10.14 percent), Switzerland (6.20 percent), and,
</p>
<p>finally, France (0.09 percent).
</p>
<p>5.10.2.2 Univariate Statistics
Another useful way of summarizing your data is through the Tabstat option, which
</p>
<p>you can find under ► Statistics ► Summaries, tables, and tests ► Other tables ►
</p>
<p>Compact table of summary statistics. Selecting this menu option opens a dialog box
</p>
<p>similar to that in Fig. 5.22. In the Variables box, select the relevant variables for
</p>
<p>which you would like to display summary statistics. These variables range from age
to gender. Next, under Statistics to display tick the blank boxes and specify the
</p>
<p>Fig. 5.21 Pie chart
</p>
<p>Table 5.8 Example of a frequency table in Stata
</p>
<p>tabulate country
</p>
<p>Home |
</p>
<p>country |      Freq.     Percent        Cum.
------------+-----------------------------------
</p>
<p>de |        695       65.26       65.26
ch |         66        6.20       71.46
</p>
<p>at |        108       10.14       81.60
fr |          1        0.09       81.69
us |        195       18.31      100.00
</p>
<p>------------+-----------------------------------
</p>
<p>Total |      1,065      100.00
</p>
<p>144 5 Descriptive Statistics</p>
<p/>
</div>
<div class="page"><p/>
<p>descriptive statistics to be displayed from the drop-down menu. In this example, we
</p>
<p>specify the number of nonmissing observations (Count in Stata), the median (50th
</p>
<p>percentile in Stata), theMean, the Range, the Interquartile range, the Variance,
</p>
<p>and the Standard deviation.
</p>
<p>Clicking on OK will display an output similar to that in Table 5.9, which
</p>
<p>includes the number of nonmissing observations (N), median (p50), mean (mean),
</p>
<p>range (range), interquartile range (iqr), variance (var), and standard deviation (sd).
</p>
<p>Note that tabstat arranges the table like a dataset, with the variables as
</p>
<p>columns. For a more a typical table, go to the Options tab (see Fig. 5.22) and
</p>
<p>change the value of the Use as columns from Variables to Statistics.
</p>
<p>Table 5.9 Example of a summary table using the tabstat option
</p>
<p>tabstat age-gender, statistics (count p50 mean range iqr var sd)
</p>
<p>stats |       age   country  flight~s  flight~t  fligh~se  fligh~pe    gender
---------+----------------------------------------------------------------------
</p>
<p>N |      1065      1065      1065      1065      1065      1065      1065
p50 |        50         1         3         4         2         1         2
</p>
<p>mean |  50.41972         2  2.798122  3.788732  1.507042 1.476056  1.737089
range |        82         4         2         5         1         1         1
</p>
<p>iqr |        16         2         0         2         1         1         1
variance |  150.6667  2.407895  .1894702  1.873557  .2501853  .2496611  .1939708
</p>
<p>sd |  12.27464  1.551739  .4352817  1.368779  .5001853   .499661  .4404212
--------------------------------------------------------------------------------
</p>
<p>Fig. 5.22 Dialog box for univariate statistics
</p>
<p>5.10 Example 145</p>
<p/>
</div>
<div class="page"><p/>
<p>5.10.2.3 Bivariate Graphs and Tables
</p>
<p>Scatter Plots and Matrix Scatter Plots
</p>
<p>Matrix scatter plots can be easily displayed in Stata by going to ► Graphics ►
</p>
<p>Twoway graph (scatter, line, etc.) or Graphics ► Scatterplot matrix. These two
</p>
<p>separate graphs differ in the following way:
</p>
<p>1. Twoway graph (scatter, line, etc.): plots two variables against each other, but
</p>
<p>you can display multiple scatter plots at a time on the same graph. Produce a
</p>
<p>twoway scatter plot by going to► Graphics► Twoway graph (scatter, line, etc.)
</p>
<p>and clicking on Create. In the dialog box that opens (on the left-hand side of
</p>
<p>Fig. 5.23), select Scatter, enter the outcome variable overall satisfaction
</p>
<p>(overall_sat) in the Y variable box, and age in the X variable box, click on
Accept, and then on OK. The right-hand side of Fig. 5.23 shows the resulting
</p>
<p>scatter plot.
</p>
<p>2. Scatterplot matrix: creates scatter plots for multiple variables simultaneously.
</p>
<p>Going to Graphics ► Scatterplot matrix opens a dialog box similar to the one
</p>
<p>shown on the left of Fig. 5.24. Select nflights, overall_sat, and age underVariables.
To change the density of the scatter matrix, click on the buttonMarker Properties.
</p>
<p>This opens the dialog window on the right of Fig. 5.24. Select the option Point
</p>
<p>under Symbol, click on Accept, and then on OK.
</p>
<p>This will produce the matrix scatter plot shown in Fig. 5.25. The graph
</p>
<p>shows distinct bands, because the overall satisfaction variable takes on 7 values
</p>
<p>(1 to 7).
</p>
<p>The first scatter plot in the first row reveals the relationship between the
</p>
<p>number of flights and overall price satisfaction. Next to this on the first row,
the relationship between the number of flights and age is displayed. In the
second row, the relationship between the overall price satisfaction and number
of flights is shown and so on. In the same row, to the right, the relationship
</p>
<p>Fig. 5.23 Dialog box and scatter plot
</p>
<p>146 5 Descriptive Statistics</p>
<p/>
</div>
<div class="page"><p/>
<p>between the overall price satisfaction and age is displayed. Finally, the last
row displays the relationships between age and number of flights (first scatter
plot), as well as the relationship between age and overall price satisfaction
(second scatter plot).
</p>
<p>Fig. 5.24 Dialog box scatter plot matrix and the marker properties box
</p>
<p>Fig. 5.25 Matrix scatter plot
</p>
<p>5.10 Example 147</p>
<p/>
</div>
<div class="page"><p/>
<p>Cross Tabulation
</p>
<p>Cross tabulations are useful for understanding the relationship between two
</p>
<p>variables scaled on a nominal or ordinal scale. To create a crosstab, go to ►
</p>
<p>Statistics ► Summaries, tables, and tests ► Frequency tables ► Two-way table
</p>
<p>with measures of association. It is important that you specify which variable goes in
</p>
<p>the column and which in the rows. Choose country under Row variable and gender
under Column variable. Next, select the boxes Within-column relative
</p>
<p>frequencies andWithin-row relative frequencies under Cell Contents to display
</p>
<p>the column and row percentages. Click on OK and Stata produces a table similar to
</p>
<p>the one in Table 5.10.
</p>
<p>5.10.2.4 Bivariate Statistics: Correlation and Covariance
In Stata, we can calculate bivariate correlations by going to ► Statistics ►
</p>
<p>Summaries, tables, and tests ► Summary and descriptive statistics ► Correlations
</p>
<p>and covariances. In the dialog box that opens, select the variables to be considered
</p>
<p>in the analysis. For example, enter nflights, age, and overall_sat in the Variables
box. When you click on OK, Stata will produce a correlation matrix like the one in
</p>
<p>Table 5.11.
</p>
<p>Table 5.10 Example of a crosstab
</p>
<p>tabulate country gender, column row
</p>
<p>+-------------------+
| Key               |
|-------------------|
|     frequency     |
|  row percentage   |
| column percentage |
+-------------------+
</p>
<p>Home |  Gender
country |    female       male |     Total
</p>
<p>-----------+----------------------+----------
de |       180        515 |       695 
</p>
<p>|     25.90      74.10 |    100.00 
|     64.29      65.61 |     65.26 
</p>
<p>-----------+----------------------+----------
ch |        17         49 |        66 
</p>
<p>|     25.76      74.24 |    100.00 
|      6.07       6.24 |      6.20 
</p>
<p>-----------+----------------------+----------
at |        25         83 |       108 
</p>
<p>|     23.15      76.85 |    100.00 
|      8.93      10.57 |     10.14 
</p>
<p>-----------+----------------------+----------
fr |         1          0 |         1 
</p>
<p>|    100.00 0.00 |    100.00 
|      0.36       0.00 |      0.09 
</p>
<p>-----------+----------------------+----------
us |        57        138 |       195 
</p>
<p>|     29.23      70.77 |    100.00 
|     20.36      17.58 |     18.31 
</p>
<p>-----------+----------------------+----------
Total |       280        785 |     1,065 
</p>
<p>|     26.29      73.71 |    100.00 
|    100.00     100.00 |    100.00 
</p>
<p>148 5 Descriptive Statistics</p>
<p/>
</div>
<div class="page"><p/>
<p>The correlation matrix in Table 5.11 shows the correlation between each
</p>
<p>pairwise combination of three variables. For example, the correlation between
nflights and age is �0.1158, which is negative and rather weak. Conversely, the
relationship between age and overall_sat is positive (0.1207), but still rather weak.
</p>
<p>Alternatively, a covariance matrix as in Table 5.12, can be obtained as follows.
</p>
<p>Go to ► Statistics ► Summaries, tables, and tests ► Summary and descriptive
</p>
<p>statistics ► Correlations and covariances and ticking the box Display covariances
</p>
<p>in the Options tab. This will produce the following covariance matrix.
</p>
<p>5.11 Cadbury and the UK Chocolate Market (Case Study)
</p>
<p>The UK chocolate market is expected to be &pound;6.46 billion in 2019. Six subcategories
</p>
<p>of chocolates are used to identify the different chocolate segments: boxed choco-
</p>
<p>late, molded bars, seasonal chocolate, count lines, straight lines, and &ldquo;other.&rdquo;
</p>
<p>To understand the UK chocolate market for molded chocolate bars, we have a
</p>
<p>dataset (chocolate.dta) that includes a large supermarket&rsquo;s weekly sales of 100g
molded chocolate bars from January 2016 onwards. This data file can be
</p>
<p>downloaded from the book&rsquo;s Web Appendix (! Downloads). This file contains
a set of variables. Once you have opened the dataset, you will see the set of
</p>
<p>variables we discuss in the main Stata window under Variables (see Fig. 5.7).
The first variable is week, indicating the week of the year and starts with Week
</p>
<p>1 of January 2016. The last observation for 2016 ends with observation 52, but the
</p>
<p>variable continues to count onwards for 16 weeks in 2017.13 The next variable is
</p>
<p>Table 5.11 Correlation matrix produced in Stata
</p>
<p>correlate nflights age overall_sat
(obs=1,065)
</p>
<p>| nflights      age overal~t
-------------+---------------------------
</p>
<p>nflights |   1.0000
age |  -0.1158   1.0000
</p>
<p>overall_sat |  -0.1710   0.1207   1.0000
</p>
<p>Table 5.12 Covariance matrix produced in Stata
</p>
<p>correlate nflights age overall_sat, covariance
(obs=1,065)
</p>
<p>| nflights      age overal~t
-------------+---------------------------
</p>
<p>nflights |   409.11
age | -28.7408  150.667
</p>
<p>overall_sat | -5.62173  2.40806  2.64118
</p>
<p>13Note an ordinary year has 52 weeks and 1 day, while a leap year has 52 weeks and 2 days. This is
because 1 week comprises part of 2016 and part of 2017.
</p>
<p>5.11 Cadbury and the UK Chocolate Market (Case Study) 149</p>
<p/>
</div>
<div class="page"><p/>
<p>sales, which indicates the weekly sales of 100g Cadbury bars in ₤. Next, four price
variables are included, price1-price4, which indicate the price of Cadbury, Nestlé,
Guylian, and Milka in ₤. Next, advertising1-advertising4 indicate the amount of ₤
the supermarket spent on advertising each product during that week. A subsequent
</p>
<p>block of variables, pop1-pop4, indicate whether the products were promoted in the
supermarket by means of point of purchase advertising. This variable is measured
</p>
<p>as yes/no. Variables promo1-promo4 indicate whether the product was put at the
end of the supermarket aisle, where it is more noticeable. Lastly, temperature
indicates the weekly average temperature in degrees Celsius.
</p>
<p>You have been tasked with providing descriptive statistics for a client by means
</p>
<p>of this dataset. To help you with this task, the client has prepared a number of
</p>
<p>questions:
</p>
<p>1. Do Cadbury&rsquo;s chocolate sales vary substantially across different weeks? When
</p>
<p>are Cadbury&rsquo;s sales at their highest? Please create an appropriate graph to
</p>
<p>illustrate any patterns.
</p>
<p>2. Please tabulate point-of-purchase advertising for Cadbury against point-of-pur-
</p>
<p>chase advertising for Nestlé. In addition, create a few more crosstabs. What are
</p>
<p>the implications of these crosstabs?
</p>
<p>3. How do Cadbury&rsquo;s sales relate to the price of Cadbury? What is the strength of
</p>
<p>the relationship?
</p>
<p>4. Which descriptive statistics are appropriate for describing the usage of advertis-
</p>
<p>ing? Which statistics are appropriate for describing point-of-purchase
</p>
<p>advertising?
</p>
<p>5.12 Review Questions
</p>
<p>1. Imagine you are given a dataset on car sales in different regions and are asked to
</p>
<p>calculate descriptive statistics. How would you set up the analysis procedure?
</p>
<p>2. What summary statistics could best be used to describe the change in profits over
</p>
<p>the last 5 years? What types of descriptive statistics work best to determine the
</p>
<p>market shares of five different types of insurance providers? Should we use just
</p>
<p>one or multiple descriptive statistics?
</p>
<p>3. What information do we need to determine if a case is an outlier? What are the
</p>
<p>benefits and drawbacks of deleting outliers?
</p>
<p>4. Download the codebook of the Household Income and Labour Dynamics in
</p>
<p>Australia (HILDA) Survey at: http://melbourneinstitute.unimelb.edu.au/hilda/
</p>
<p>for-data-users/user-manuals. Is this codebook clear? What do you think of its
</p>
<p>structure?
</p>
<p>150 5 Descriptive Statistics</p>
<p/>
<div class="annotation"><a href="http://melbourneinstitute.unimelb.edu.au/hilda/for-data-users/user-manuals">http://melbourneinstitute.unimelb.edu.au/hilda/for-data-users/user-manuals</a></div>
<div class="annotation"><a href="http://melbourneinstitute.unimelb.edu.au/hilda/for-data-users/user-manuals">http://melbourneinstitute.unimelb.edu.au/hilda/for-data-users/user-manuals</a></div>
</div>
<div class="page"><p/>
<p>5.13 Further Readings
</p>
<p>https://www.stata.com/manuals13/mi.pdf
</p>
<p>This manual provides a hands-on application of multiple imputation in Stata.
http://www.stata.com/manuals13/u.pdf
</p>
<p>There is a detailed description of Stata&rsquo;s properties coupled with hands-on
examples in Stata&rsquo;s manual.
</p>
<p>http://www.ats.ucla.edu/stat/mult_pkg/whatstat/default.htm
</p>
<p>The following link provides a range of general guidelines regarding the type of
statistical analyses of and tips about the application of various methods using
Stata.
</p>
<p>https://www.iriseekhout.com/promotie/thesis/
</p>
<p>General strategies on how to deal with missing data.
https://eagereyes.org/pie-charts
</p>
<p>This blog provides a good description of the advantages and disadvantages related
to pie charts.
</p>
<p>This book provides an introduction to multivariate analysis and easy to follow
discussions of fundamental statistical concepts.
</p>
<p>Hair, J. F., Jr., Black, W. C., Babin, B. J., &amp; Anderson, R. E. (2013). Multivariate
data analysis. A global perspective (7th ed.). Upper Saddle River: Prentice-Hall.
</p>
<p>This book teaches you how to make high-quality graphs in Stata.
Mitchel, M.N. A Visual Guide to Stata Graphics. (2008). Stata Press: StataCorp LP.
This book teaches readers how to decipher the meaning of symbols, tables, and
</p>
<p>figures included in research reports in order to improve their ability to critically
assess such reports.
</p>
<p>Huck, W.S. (2014). Reading statistics and research (6th ed.). Harlow: Pearson.
</p>
<p>SticiGui at http://www.stat.berkeley.edu/~stark/SticiGui/Text/correlation.htm.
</p>
<p>These websites interactively demonstrate how strong the correlations between
different datasets are.
</p>
<p>References
</p>
<p>Agarwal, C. C. (2013). Outlier analysis. New York: Springer.
Agresti, A., &amp; Finlay, B. (2014). Statistical methods for the social sciences (4th ed.). London:
</p>
<p>Pearson.
Barchard, K. A., &amp; Pace, L. A. (2011). Preventing human error: The impact of data entry methods
</p>
<p>on data accuracy and statistical results. Computers in Human Behavior, 27(5), 1834&ndash;1839.
Barchard, K. A., &amp; Verenikina, Y. (2013). Improving data accuracy: Electing the best data
</p>
<p>checking technique. Computers in Human Behavior, 29(50), 1917&ndash;1912.
Baumgartner, H., &amp; Steenkamp, J.-B. E. M. (2001). Response styles in marketing research: A
</p>
<p>cross-national investigation. Journal of Marketing Research, 38(2), 143&ndash;156.
Carpenter, J., &amp; Kenward, M. (2013). Multiple imputation and its application. New York: Wiley.
Cohen, J. (1988). Statistical power analysis for the behavioral sciences (2nd ed.). Hillsdale:
</p>
<p>Lawrence Erlbaum Associates.
Drolet, A. L., &amp; Morrison, D. G. (2001). Do we really need multiple-item measures in service
</p>
<p>research? Journal of Service Research, 3(3), 196&ndash;204.
</p>
<p>References 151</p>
<p/>
<div class="annotation"><a href="https://www.stata.com/manuals13/mi.pdf">https://www.stata.com/manuals13/mi.pdf</a></div>
<div class="annotation"><a href="http://www.stata.com/manuals13/u.pdf">http://www.stata.com/manuals13/u.pdf</a></div>
<div class="annotation"><a href="http://www.ats.ucla.edu/stat/mult_pkg/whatstat/default.htm">http://www.ats.ucla.edu/stat/mult_pkg/whatstat/default.htm</a></div>
<div class="annotation"><a href="https://www.iriseekhout.com/promotie/thesis">https://www.iriseekhout.com/promotie/thesis</a></div>
<div class="annotation"><a href="https://eagereyes.org/pie-charts">https://eagereyes.org/pie-charts</a></div>
<div class="annotation"><a href="http://www.stat.berkeley.edu/~stark/SticiGui/Text/correlation.htm">http://www.stat.berkeley.edu/~stark/SticiGui/Text/correlation.htm</a></div>
</div>
<div class="page"><p/>
<p>Eekhout, I., de Vet, H. C. W., Twisk, J. W. R., Brand, J. P. L., de Boer, M. R., &amp; Heymans, M. W.
(2014). Missing data in a multi-item instrument were best handled by multiple imputation at
the item score level. Journal of Clinical Epidemiology, 67(3), 335&ndash;342.
</p>
<p>Gladwell, M. (2008). Outliers: The story of success. New York: Little, Brown, and Company.
Graham, J. W. (2012). Missing data: Analysis and design. Berlin et al.: Springer.
Hair, J. F., Jr., Black, W. C., Babin, B. J., &amp; Anderson, R. E. (2010). Multivariate data analysis.
</p>
<p>A global perspective (7th ed.). Upper Saddle River: Pearson.
Harzing, A. W. (2005). Response styles in cross-national survey research: A 26-country study.
</p>
<p>International Journal of Cross Cultural Management, 6(2), 243&ndash;266.
Johnson, T., Kulesa, P., Lic, I., Cho, Y. I., &amp; Shavitt, S. (2005). The relation between culture and
</p>
<p>response styles. Evidence from 19 countries. Journal of Cross-Cultural Psychology, 36(2),
264&ndash;277.
</p>
<p>Krippendorff, K. (2012). Content analysis: An introduction to its methodology. Thousand Oaks:
Sage.
</p>
<p>Little, R. J. A. (1998). A test of missing completely at random for multivariate data with missing
values. Journal of the American Statistical Association, 83(404), 1198&ndash;1202.
</p>
<p>Paulsen, A., Overgaard, S., &amp; Lauritsen, J. M. (2012). Quality of data entry using single entry,
double entry and automated forms processing &ndash; An example based on a study of patient-
reported outcomes. PloS One, 7(4), e35087.
</p>
<p>Rubin, D. B. (1987). Multiple imputation for nonresponse in surveys. New York: Wiley.
Sarstedt, M., Diamantopoulos, A., Salzberger, T., &amp; Baumgartner, P. (2016). Selecting single
</p>
<p>items to measure doubly-concrete constructs: A cautionary tale. Journal of Business Research,
69(8), 3159&ndash;3167.
</p>
<p>Schafer, J. L. (1997). Analysis of incomplete multivariate data. London: Chapman &amp; Hall.
White, I. R., Royston, P., &amp; Wood, A. M. (2011). Multiple imputation using chained equations:
</p>
<p>Issues and guidance for practice. Statistics in Medicine, 30(4), 377&ndash;399.
</p>
<p>152 5 Descriptive Statistics</p>
<p/>
</div>
<div class="page"><p/>
<p>Hypothesis Testing &amp; ANOVA 6
</p>
<p>Keywords
</p>
<p>α-Inflation &bull; α error &bull; Analysis of Variance (ANOVA) &bull; β error &bull; Bonferroni
</p>
<p>correction &bull; Degrees of freedom &bull; eta-squared &bull; F-test &bull; F-test of sample
</p>
<p>variance &bull; Factor variable &bull; Familywise error rate &bull; Independent samples t-
test &bull; Interaction effect &bull; Levene&rsquo;s test &bull; Mann-Whitney U test &bull; Main effect &bull;
</p>
<p>Nonparametric tests &bull; Null and alternative hypothesis &bull; Omega-squared &bull; One-
</p>
<p>sample t-test &bull; One-tailed test &bull; One-way ANOVA &bull; p-value &bull; Paired samples t-
test &bull; Parametric test &bull; Practical significance &bull; Post hoc tests &bull; Power of a test &bull;
</p>
<p>Shapiro-Wilk test &bull; Significance level &bull; Sampling error &bull; Statistical significance &bull;
</p>
<p>t-test &bull; Test statistic &bull; Tukey&rsquo;s honestly significant difference test &bull; Two-sample
t-test &bull; Two-tailed test &bull; Two-way ANOVA &bull; Type I and type II error &bull; Welch
correction &bull; Wilcoxon signed-rank test &bull; z-test
</p>
<p>Learning Objectives
</p>
<p>After reading this chapter, you should understand:
</p>
<p>&ndash; The logic of hypothesis testing.
</p>
<p>&ndash; The steps involved in hypothesis testing.
</p>
<p>&ndash; What a test statistic is.
</p>
<p>&ndash; Types of error in hypothesis testing.
</p>
<p>&ndash; Common types of t-tests, one-way, and two-way ANOVA.
&ndash; How to interpret Stata output.
</p>
<p>6.1 Introduction
</p>
<p>Do men or women spend more money on the Internet? Assume that the mean
</p>
<p>amount that a sample of men spends online is $200 per year against a women
</p>
<p>sample&rsquo;s mean of $250. When we compare mean values such as these, we always
</p>
<p># Springer Nature Singapore Pte Ltd. 2018
E. Mooi et al., Market Research, Springer Texts in Business and Economics,
DOI 10.1007/978-981-10-5218-7_6
</p>
<p>153</p>
<p/>
</div>
<div class="page"><p/>
<p>expect some difference. But, how can we determine if such differences are statisti-
</p>
<p>cally significant? Establishing statistical significance requires ascertaining whether
</p>
<p>such differences are attributable to chance or not. In this chapter, we will introduce
</p>
<p>hypothesis testing and how this helps determine statistical significance.
</p>
<p>6.2 Understanding Hypothesis Testing
</p>
<p>A hypothesis is a statement about a certain effect or parameter (such as a mean or
</p>
<p>correlation) that can be tested using a sample drawn from the population. A
</p>
<p>hypothesis may comprise a claim about the difference between two sample
</p>
<p>parameters (e.g., there is a difference between males&rsquo; and females&rsquo; mean spending).
</p>
<p>It can also be a test of a judgment (e.g., teenagers spend an average of 4 h per day
</p>
<p>on the Internet). Data from the sample are used to obtain evidence against, or in
</p>
<p>favor of, the statement.
</p>
<p>Hypothesis testing is performed to infer whether or not a certain effect is statisti-
</p>
<p>cally significant. Statistical significance means that the effect is so large that it is
</p>
<p>unlikely to have occured by chance. Whether results are statistically significant
</p>
<p>depends on several factors, including the size of the effect, the variation in the sample
</p>
<p>data, and the sample size (Agresti and Finlay 2014).When drawing a sample from the
</p>
<p>population, there is always some probability that we might reach the wrong conclu-
</p>
<p>sion due to a sampling error, which is the difference between the sample and the
</p>
<p>population characteristics. To determine whether the claim is true, we start by setting
</p>
<p>an acceptable probability (called the significance level) that we could incorrectly
</p>
<p>conclude there is an effect when, in fact, there is none. This significance level is
</p>
<p>typically set at 0.05, which corresponds to a 5% error probability. Next, subject to the
</p>
<p>claim made in the hypothesis, we should decide on the correct type of test to perform.
</p>
<p>This involves making decisions regarding four aspects.
</p>
<p>First, we should understand the testing situation. What exactly are we testing:
</p>
<p>Are we comparing one value against a fixed value, or are we comparing groups,
</p>
<p>and, if so, how many?
</p>
<p>Second, we need to specify the nature of the samples: Is our comparison based
</p>
<p>on paired samples or independent samples (the difference is discussed later in this
chapter)?
</p>
<p>Third, we should check assumptions about the distribution of our data to
</p>
<p>determine whether parametric or nonparametric tests are appropriate. Parametric
</p>
<p>tests make assumptions about the properties of the population distributions from
</p>
<p>which the data are drawn, while nonparametric tests are not based on any
</p>
<p>distributional assumptions.
</p>
<p>Fourth, we need to decide on the region where we can reject our hypothesis; that
</p>
<p>is, whether the region of rejection will be on one side or both sides of the sampling
</p>
<p>distribution.
</p>
<p>154 6 Hypothesis Testing &amp; ANOVA</p>
<p/>
</div>
<div class="page"><p/>
<p>Once these four aspects are sorted, we calculate the test statistic, which identifies
whether the sample supports or rejects the claim stated in the hypothesis. We can
</p>
<p>then decide to either reject or support the hypothesis. This decision enables us to
</p>
<p>draw market research conclusions in the final step. Figure 6.1 illustrates the six
</p>
<p>steps involved in hypothesis testing.
</p>
<p>To illustrate the process of hypothesis testing, consider the following example:
</p>
<p>A department store chain wants to evaluate the effectiveness of three different
</p>
<p>in-store promotion campaigns that drive the sales of a specific product. These
</p>
<p>campaigns comprise: (1) a point of sale display, (2) a free tasting stand, and
</p>
<p>(3) in-store announcements. To help with the evaluation, the management decides
</p>
<p>to conduct a one-week experiment during which 30 stores are randomly assigned to
</p>
<p>each campaign type. This random assignment is important to obtain reliable and
</p>
<p>generalizable results, because randomization should equalize the effect of system-
</p>
<p>atic factors not accounted for in the experimental design (see Chap. 4). Table 6.1
</p>
<p>shows the sales of the three different in-store promotion campaigns. The table also
</p>
<p>contains information on the service type (personal or self-service) in the first
</p>
<p>column and the marginal means representing the means of sales within stores in
the last column. The very last row also shows the marginal mean of the type of
</p>
<p>campaign, while the very last cell shows the grand mean across all the service types
</p>
<p>and campaigns.
</p>
<p>Step 1: Formulate the hypothesis
</p>
<p>Step 3: Select an appropriate test
</p>
<p>Step 4: Calculate the test statistic
</p>
<p>Step 5: Make the test decision
</p>
<p>Step 6: Interpret the results
</p>
<p>Step 2: Choose the significance level
</p>
<p>1. Define the testing situation
</p>
<p>2. Determine if samples are paired or independent
</p>
<p>3. Check assumptions and choose the test
</p>
<p>4. Specify the region of rejection
</p>
<p>Fig. 6.1 Steps involved in hypothesis testing
</p>
<p>6.2 Understanding Hypothesis Testing 155</p>
<p/>
</div>
<div class="page"><p/>
<p>We will use these data to carry out tests to compare the different in-store
</p>
<p>promotion campaigns&rsquo; mean sales separately, or in comparison to each other. We
</p>
<p>first discuss each test theoretically (including the formulas), followed by an empiri-
</p>
<p>cal illustration. You will realize that the formulas are not as complicated as
</p>
<p>you might have thought! These formulas contain Greek characters and we have
</p>
<p>therefore included a table describing each Greek character in the Web Appendix
</p>
<p>(! Downloads).
</p>
<p>6.3 Testing Hypotheses on One Mean
</p>
<p>6.3.1 Step 1: Formulate the Hypothesis
</p>
<p>Hypothesis testing starts with the formulation of a null and alternative hypothesis.
</p>
<p>A null hypothesis (indicated as H0) is a statement expecting no difference or effect.
</p>
<p>Conversely, an alternative hypothesis (indicated as H1) is the hypothesis against
</p>
<p>which the null hypothesis is tested (Everitt and Skrondal 2010). Examples of
</p>
<p>potential null and alternative hypotheses on the campaign types are:
</p>
<p>1. H0: The mean sales in stores that installed a point of sale display are equal to or
</p>
<p>lower than 45 units.
</p>
<p>H1: The mean sales in stores that installed a point of sale display are higher than
</p>
<p>45 units.
</p>
<p>Table 6.1 Sales data
</p>
<p>Sales (units)
</p>
<p>Marginal
</p>
<p>meanService type
</p>
<p>Point of sale display
</p>
<p>(stores 1&ndash;10)
</p>
<p>Free tasting stand
</p>
<p>(stores 11&ndash;20)
</p>
<p>In-store
</p>
<p>announcements
</p>
<p>(stores 21&ndash;30)
</p>
<p>Personal 50 55 45 50.00
</p>
<p>Personal 52 55 50 52.33
</p>
<p>Personal 43 49 45 45.67
</p>
<p>Personal 48 57 46 50.33
</p>
<p>Personal 47 55 42 48.00
</p>
<p>Self-service 45 49 43 45.67
</p>
<p>Self-service 44 48 42 44.67
</p>
<p>Self-service 49 54 45 49.33
</p>
<p>Self-service 51 54 47 50.67
</p>
<p>Self-service 44 44 42 43.33
</p>
<p>Marginal
</p>
<p>mean
</p>
<p>47.30 52.00 44.7 48.00
</p>
<p>Grand mean
</p>
<p>156 6 Hypothesis Testing &amp; ANOVA</p>
<p/>
</div>
<div class="page"><p/>
<p>2. H0: There&rsquo;s no difference in the mean sales of stores that installed a point of sale
</p>
<p>display and those that installed a free tasting stand (statistically, the average
</p>
<p>sales of the point of sale display &frac14; the average sales of the free tasting stand).
H1: There&rsquo;s a difference in the mean sales of stores that installed a point of sale
</p>
<p>display and those that installed a free tasting stand (statistically, the average
</p>
<p>sales of the point of sale display 6&frac14; the average sales of the free tasting stand).
</p>
<p>Hypothesis testing can have two outcomes: First, we do not reject the null hypoth-
</p>
<p>esis. This suggests there is no difference and that the null hypothesis can be
</p>
<p>retained. However, it would be incorrect to subsequently conclude that the null
</p>
<p>hypothesis is true, as it is not possible to &ldquo;prove&rdquo; the non-existence of a certain
</p>
<p>effect or condition. For example, one can examine any number of crows and find
</p>
<p>that they are all black, yet that would not make the statement &ldquo;There are no white
</p>
<p>crows&rdquo; true. Only sighting one white crow will prove its existence. Second, we
</p>
<p>could reject the null hypothesis, thus finding support for the alternative hypothesis
</p>
<p>in which some effect is expected. This outcome is, of course, desirable in most
</p>
<p>analyses, as we generally want to show that something (such as a promotion
</p>
<p>campaign) is related to a certain outcome (e.g., sales). Therefore, we frame the
</p>
<p>effect that we want to investigate as the alternative hypothesis.
</p>
<p>Inevitably, each hypothesis test has a certain degree of uncertainty so that
</p>
<p>even if we reject a null hypothesis, we can never be totally certain that this
</p>
<p>was the correct decision. Consequently, market researchers should use terms
</p>
<p>such as &ldquo;find support for the alternative hypothesis&rdquo; when they discuss their
</p>
<p>findings. Terms like &ldquo;prove&rdquo; should never be part of hypotheses testing.
</p>
<p>Returning to our initial example, the management only considers a campaign
</p>
<p>effective if the sales it generates are higher than the 45 units normally sold (you can
</p>
<p>choose any other value, the idea is to test the sample mean against a given standard).
</p>
<p>One way of formulating the null and alternative hypotheses in respect of this
</p>
<p>expectation is:
</p>
<p>H0: μ � 45
</p>
<p>H1: μ &gt; 45
</p>
<p>In words, the null hypothesis H0 states that the population mean, indicated by μ
</p>
<p>(pronounced as mu), is equal to or smaller than 45, whereas the alternative hypoth-
esis H1 states that the population mean is larger than 45. It is important to note that
</p>
<p>the hypothesis always refers to a population parameter, in this case, the population
</p>
<p>mean, represented by μ. It is practice for Greek characters to represent population
</p>
<p>parameters and for Latin characters to indicate sample statistics (e.g., the Latin �x ).
In this example, we state a directional hypothesis as the alternative hypothesis,
</p>
<p>6.3 Testing Hypotheses on One Mean 157</p>
<p/>
</div>
<div class="page"><p/>
<p>which is expressed in a direction (higher) relative to the standard of 45 units. Since
</p>
<p>we presume that during a campaign, the product sales are higher, we posit a right-
tailed hypothesis (as opposed to a left-tailed hypothesis) for the alternative hypoth-
esis H1.
</p>
<p>Alternatively, presume we are interested in determining whether the mean sales
</p>
<p>of the point of sale display (μ1) are equal to the mean sales of the free tasting
</p>
<p>stand (μ2). This implies a non-directional hypothesis, which can be written as:
</p>
<p>H0: μ1 &frac14; μ2
H1: μ1 6&frac14; μ2
</p>
<p>The difference between the two general types of hypotheses is that a directional
</p>
<p>hypothesis looks for an increase or a decrease in a parameter (such as a population
</p>
<p>mean) relative to a specific standard. A non-directional hypothesis tests for any
difference in the parameter, whether positive or negative.
</p>
<p>6.3.2 Step 2: Choose the Significance Level
</p>
<p>No type of hypothesis testing can evaluate the validity of a hypothesis with absolute
</p>
<p>certainty. In any study that involves drawing a sample from the population, there is
</p>
<p>always some probability that we will erroneously retain or reject the null hypothesis
</p>
<p>due to sampling error, which is a difference between the sample and the popula-
</p>
<p>tion. In statistical testing, two types of errors can occur (Fig. 6.2):
</p>
<p>1. a true null hypothesis is incorrectly rejected (type I or α error), and
</p>
<p>2. a false null hypothesis is not rejected (type II or β error).
</p>
<p>In our example, a type I error occurs if we conclude that the point of sale displays
</p>
<p>increased the sales beyond 45 units, when in fact it did not increase the sales, or may
</p>
<p>have even decreased them. A type II error occurs if we do not reject the null
</p>
<p>hypothesis, which suggests there was no increase in sales, even though the sales
</p>
<p>increased significantly.
</p>
<p>Correct
</p>
<p>decision
</p>
<p>Correct
</p>
<p>decision
</p>
<p>Type I
</p>
<p>error
</p>
<p>Type II
</p>
<p>error
</p>
<p>T
e
s
t 
</p>
<p>d
e
c
is
io
</p>
<p>n
</p>
<p>H
0
 r
</p>
<p>ej
ec
</p>
<p>te
d
</p>
<p>H
0
 n
</p>
<p>o
t 
</p>
<p>re
je
</p>
<p>ct
ed
</p>
<p>True state of H
0
</p>
<p>H0 falseH0 true
</p>
<p>Fig. 6.2 Type I and type II
errors
</p>
<p>158 6 Hypothesis Testing &amp; ANOVA</p>
<p/>
</div>
<div class="page"><p/>
<p>A problem with hypothesis testing is that we don&rsquo;t know the true state of the null
</p>
<p>hypothesis. Fortunately, we can establish a level of confidence that a true null
</p>
<p>hypothesis will not be erroneously rejected. This is the maximum probability of a
</p>
<p>type I error that we want to allow. The Greek character α (pronounced as alpha)
represents this probability and is called the significance level. In market research
reports, this is indicated by phrases such as &ldquo;this test result is significant at a 5%
</p>
<p>level.&rdquo; This means that the researcher allowed for a maximum chance of 5% of
</p>
<p>mistakenly rejecting a true null hypothesis.
</p>
<p>The selection of an α level depends on the research setting and the costs
</p>
<p>associated with a type I error. Usually, α is set to 0.05, which corresponds to a
</p>
<p>5% error probability. However, when researchers want to be conservative or strict
</p>
<p>in their testing, such as when conducting experiments, α is set to 0.01 (i.e., 1%). In
</p>
<p>exploratory studies, an α of 0.10 (i.e., 10%) is commonly used. An α-level of 0.10
</p>
<p>means that if you carry out ten tests and reject the null hypothesis every time, your
</p>
<p>decision in favor of the alternative hypothesis was, on average, wrong once. This
</p>
<p>might not sound too high a probability, but when much is at stake (e.g., withdrawing
</p>
<p>a product because of low satisfaction ratings) then 10% may be too high.
</p>
<p>Why don&rsquo;t we simply set α to 0.0001% to really minimize the probability of a
</p>
<p>type I error? Setting α to such a low level would obviously make the erroneous
</p>
<p>rejection of the null hypothesis very unlikely. Unfortunately, this approach
</p>
<p>introduces another problem. The probability of a type I error is inversely related
</p>
<p>to that of a type II error, so that the smaller the risk of a type I error, the higher the
</p>
<p>risk of a type II error! However, since a type I error is considered more severe than a
</p>
<p>type II error, we control the former directly by setting α to a desired level (Lehmann
</p>
<p>1993).
</p>
<p>Sometimes statistical significance can be established even when differences
</p>
<p>are very small and have little or no managerial implications. Practitioners,
</p>
<p>usually refer to &ldquo;significant&rdquo; as being practically significant rather than
</p>
<p>statistically significant. Practical significance refers to differences or effects
</p>
<p>that are large enough to influence the decision-making process. An analysis
</p>
<p>may disclose that a type of packaging increases sales by 10%, which could be
</p>
<p>practically significant. Whether results are practically significant depends on
</p>
<p>the management&rsquo;s perception of the difference or effect and whether this
</p>
<p>warrants action. It is important to separate statistical significance from prac-
</p>
<p>tical significance. Statistical significance does not imply practical
</p>
<p>significance.
</p>
<p>Another important concept related to this is the power of a statistical test
</p>
<p>(defined by 1 � β, where β is the probability of a type II error), which represents
the probability of rejecting a null hypothesis when it is, in fact, false. In other words,
</p>
<p>the power of a statistical test is the probability of rendering an effect significant
</p>
<p>when it is indeed significant. Researchers want the power of a test to be as high as
</p>
<p>6.3 Testing Hypotheses on One Mean 159</p>
<p/>
</div>
<div class="page"><p/>
<p>possible, but when maximizing the power and, therefore, reducing the probability
</p>
<p>of a type II error, the occurrence of a type I error increases (Everitt and Skrondal
</p>
<p>2010). Researchers generally view a statistical power of 0.80 (i.e., 80%) as satis-
</p>
<p>factory, because this level is assumed to achieve a balance between acceptable type
</p>
<p>I and II errors. A test&rsquo;s statistical power depends on many factors, such as the
</p>
<p>significance level, the strength of the effect, and the sample size. In Box 6.1 we
</p>
<p>discuss the statistical power concept in greater detail.
</p>
<p>6.3.3 Step 3: Select an Appropriate Test
</p>
<p>Selecting an appropriate statistical test is based on four aspects. First, we need to
</p>
<p>assess the testing situation: What are we comparing? Second, we need to assess the
</p>
<p>nature of the samples that are being compared: Do we have one sample with
</p>
<p>observations from the same object, firm or individual (paired), or do we have two
</p>
<p>different sets of samples (i.e., independent)? Third, we need to check the
</p>
<p>assumptions for normality to decide which type of test to use: Parametric (if we
</p>
<p>meet the test conditions) or non-parametric (if we fail to meet the test conditions)?
</p>
<p>This step may involve further analysis, such as testing the homogeneity of group
</p>
<p>variances. Fourth, we should decide on the region of rejection: Do we want to test
</p>
<p>one side or both sides of the sampling distribution? Table 6.2 summarizes these four
</p>
<p>aspects with the recommended choice of test indicated in the grey shaded boxes. In
</p>
<p>the following we will discuss each of these four aspects.
</p>
<p>Box 6.1 Statistical Power of a Test
</p>
<p>Market researchers encounter the common problem that they, given a
</p>
<p>predetermined level of α and some fixed parameters in the sample, have to
</p>
<p>calculate the sample size required to yield an effect of a specific size.
</p>
<p>Computing the required sample size (called a power analysis) can be compli-
cated, depending on the test or procedure used. Fortunately, Stata includes a
</p>
<p>power and sample size module that allows you to determine sample size
</p>
<p>under different conditions. In the Web Appendix (! Downloads), we use
data from our example to illustrate how to run a power analysis using Stata.
</p>
<p>An alternative is G * Power 3.0, which is sophisticated and yet easy-to-use. It
</p>
<p>can be downloaded freely from http://www.psycho.uni-duesseldorf.de/
</p>
<p>abteilungen/aap/gpower3/.
</p>
<p>If these tools are too advanced, Cohen (1992) suggests required sample
</p>
<p>sizes for different types of tests. For example, detecting the presence of
</p>
<p>differences between two independent sample means for α &frac14; 0.05 and a
power of β &frac14; 0.80 requires a sample size (n) of n &frac14; 26 for large differences,
n &frac14; 64 for medium differences, and n &frac14; 393 for small differences. This
demonstrates that sample size requirements increase disproportionally when
</p>
<p>the effect that needs to be detected becomes smaller.
</p>
<p>160 6 Hypothesis Testing &amp; ANOVA</p>
<p/>
<div class="annotation"><a href="http://www.psycho.uni-duesseldorf.de/abteilungen/aap/gpower3">http://www.psycho.uni-duesseldorf.de/abteilungen/aap/gpower3</a></div>
<div class="annotation"><a href="http://www.psycho.uni-duesseldorf.de/abteilungen/aap/gpower3">http://www.psycho.uni-duesseldorf.de/abteilungen/aap/gpower3</a></div>
</div>
<div class="page"><p/>
<p>T
a
b
le
</p>
<p>6
.2
</p>
<p>S
el
ec
ti
n
g
an
</p>
<p>ap
p
ro
p
ri
at
e
te
st
</p>
<p>T
es
t
</p>
<p>#
</p>
<p>T
es
ti
n
g
si
tu
at
io
n
</p>
<p>N
at
u
re
</p>
<p>o
f
sa
m
p
le
s
</p>
<p>C
h
o
ic
e
o
f
T
es
ta
</p>
<p>R
eg
io
n
o
f
</p>
<p>re
je
ct
io
n
</p>
<p>W
h
a
t
d
o
w
e
co
m
p
a
re
</p>
<p>P
a
ir
ed
</p>
<p>vs
.
In
d
ep
en
d
en
t
</p>
<p>A
ss
u
m
p
ti
o
n
s
</p>
<p>P
a
ra
m
et
ri
c
</p>
<p>N
o
n
-p
a
ra
m
et
ri
c
</p>
<p>O
n
e
o
r
</p>
<p>tw
o
-s
id
ed
</p>
<p>te
st
</p>
<p>1
O
n
e
g
ro
u
p
ag
ai
n
st
a
</p>
<p>fi
x
ed
</p>
<p>v
al
u
e
</p>
<p>N
o
t
ap
p
li
ca
b
le
</p>
<p>S
h
ap
ir
o
-W
</p>
<p>il
k
te
st
&frac14;
</p>
<p>n
o
rm
</p>
<p>al
O
n
e
sa
m
p
le
</p>
<p>t-
te
st
</p>
<p>O
n
e
o
r
</p>
<p>tw
o
-s
id
ed
</p>
<p>S
h
ap
ir
o
-W
</p>
<p>il
k
te
st
6&frac14;
</p>
<p>n
o
rm
</p>
<p>al
W
il
co
x
o
n
si
g
n
ed
-
</p>
<p>ra
n
k
te
st
</p>
<p>O
n
e
o
r
</p>
<p>tw
o
-s
id
ed
</p>
<p>2
O
u
tc
o
m
e
v
ar
ia
b
le
</p>
<p>ac
ro
ss
</p>
<p>tw
o
g
ro
u
p
s
</p>
<p>P
ai
re
d
sa
m
p
le
</p>
<p>S
h
ap
ir
o
-W
</p>
<p>il
k
te
st
&frac14;
</p>
<p>n
o
rm
</p>
<p>al
</p>
<p>&amp;
L
ev
en
e&rsquo;
s
te
st
:
σ
2 1
&frac14;
</p>
<p>σ
2 2
</p>
<p>P
ai
re
d
t-
te
st
</p>
<p>O
n
e
o
r
</p>
<p>tw
o
-s
id
ed
</p>
<p>S
h
ap
ir
o
-W
</p>
<p>il
k
te
st
6&frac14;
</p>
<p>n
o
rm
</p>
<p>al
</p>
<p>&amp;
L
ev
en
e&rsquo;
s
te
st
:
σ
2 1
&frac14;
</p>
<p>σ
2 2
</p>
<p>P
ai
re
d
t-
te
st
b
</p>
<p>O
n
e
o
r
</p>
<p>tw
o
-s
id
ed
</p>
<p>S
h
ap
ir
o
-W
</p>
<p>il
k
te
st
&frac14;
</p>
<p>n
o
rm
</p>
<p>al
</p>
<p>&amp;
L
ev
en
e&rsquo;
s
te
st
:
σ
2 1
6&frac14;
</p>
<p>σ
2 2
</p>
<p>P
ai
re
d
t-
te
st
w
it
h
</p>
<p>W
el
ch
&rsquo;s
</p>
<p>co
rr
ec
ti
o
n
</p>
<p>O
n
e
o
r
</p>
<p>tw
o
-s
id
ed
</p>
<p>S
h
ap
ir
o
-W
</p>
<p>il
k
te
st
6&frac14;
</p>
<p>n
o
rm
</p>
<p>al
</p>
<p>&amp;
L
ev
en
e&rsquo;
s
te
st
:
σ
2 1
6&frac14;
</p>
<p>σ
2 2
</p>
<p>W
il
co
x
o
n
m
at
ch
ed
-
</p>
<p>p
ai
rs
</p>
<p>si
g
n
ed
-r
an
k
</p>
<p>te
st
</p>
<p>O
n
e
o
r
</p>
<p>tw
o
-s
id
ed
</p>
<p>3
O
u
tc
o
m
e
v
ar
ia
b
le
</p>
<p>ac
ro
ss
</p>
<p>tw
o
g
ro
u
p
s
</p>
<p>In
d
ep
en
d
en
t
sa
m
p
le
s
</p>
<p>S
h
ap
ir
o
-W
</p>
<p>il
k
te
st
&frac14;
</p>
<p>n
o
rm
</p>
<p>al
</p>
<p>&amp;
L
ev
en
e&rsquo;
s
te
st
:
σ
2 1
&frac14;
</p>
<p>σ
2 2
</p>
<p>T
w
o
-s
am
</p>
<p>p
le
</p>
<p>t-
te
st
</p>
<p>O
n
e
o
r
</p>
<p>tw
o
-s
id
ed
</p>
<p>S
h
ap
ir
o
-W
</p>
<p>il
k
te
st
6&frac14;
</p>
<p>n
o
rm
</p>
<p>al
</p>
<p>&amp;
L
ev
en
e&rsquo;
s
te
st
:
σ
2 1
&frac14;
</p>
<p>σ
2 2
</p>
<p>T
w
o
-s
am
</p>
<p>p
le
</p>
<p>t-
te
st
b
</p>
<p>O
n
e
o
r
</p>
<p>tw
o
-s
id
ed
</p>
<p>S
h
ap
ir
o
-W
</p>
<p>il
k
te
st
&frac14;
</p>
<p>n
o
rm
</p>
<p>al
</p>
<p>&amp;
L
ev
en
e&rsquo;
s
te
st
:
σ
2 1
6&frac14;
</p>
<p>σ
2 2
</p>
<p>T
w
o
-s
am
</p>
<p>p
le
</p>
<p>t-
te
st
w
it
h
</p>
<p>W
el
ch
&rsquo;s
</p>
<p>co
rr
ec
ti
o
n
</p>
<p>O
n
e
o
r
</p>
<p>tw
o
-s
id
ed
</p>
<p>S
h
ap
ir
o
-W
</p>
<p>il
k
te
st
6&frac14;
</p>
<p>n
o
rm
</p>
<p>al
</p>
<p>&amp;
L
ev
en
e&rsquo;
s
te
st
:
σ
2 1
6&frac14;
</p>
<p>σ
2 2
</p>
<p>W
il
co
x
o
n
ra
n
k
-s
u
m
</p>
<p>te
st
</p>
<p>O
n
e
o
r
</p>
<p>tw
o
-s
id
ed
</p>
<p>4
O
u
tc
o
m
e
v
ar
ia
b
le
</p>
<p>ac
ro
ss
</p>
<p>th
re
e
o
r
m
o
re
</p>
<p>g
ro
u
p
s
</p>
<p>O
n
e
fa
ct
o
r
v
ar
ia
b
le
,
</p>
<p>in
d
ep
en
d
en
t
sa
m
p
le
s
</p>
<p>S
h
ap
ir
o
-W
</p>
<p>il
k
te
st
&frac14;
</p>
<p>n
o
rm
</p>
<p>al
</p>
<p>&amp;
L
ev
en
e&rsquo;
s
te
st
:
σ
2 1
&frac14;
</p>
<p>σ
2 2
</p>
<p>O
n
e-
w
ay
</p>
<p>A
N
O
V
A
:
F
-t
es
t
</p>
<p>T
w
o
-
</p>
<p>si
d
ed
*
c
</p>
<p>(c
o
n
ti
n
u
ed
)
</p>
<p>6.3 Testing Hypotheses on One Mean 161</p>
<p/>
</div>
<div class="page"><p/>
<p>T
a
b
le
</p>
<p>6
.2
</p>
<p>(c
o
n
ti
n
u
ed
)
</p>
<p>T
es
t
</p>
<p>#
</p>
<p>T
es
ti
n
g
si
tu
at
io
n
</p>
<p>N
at
u
re
</p>
<p>o
f
sa
m
p
le
s
</p>
<p>C
h
o
ic
e
o
f
T
es
ta
</p>
<p>R
eg
io
n
o
f
</p>
<p>re
je
ct
io
n
</p>
<p>W
h
a
t
d
o
w
e
co
m
p
a
re
</p>
<p>P
a
ir
ed
</p>
<p>vs
.
In
d
ep
en
d
en
t
</p>
<p>A
ss
u
m
p
ti
o
n
s
</p>
<p>P
a
ra
m
et
ri
c
</p>
<p>N
o
n
-p
a
ra
m
et
ri
c
</p>
<p>O
n
e
o
r
</p>
<p>tw
o
-s
id
ed
</p>
<p>te
st
</p>
<p>S
h
ap
ir
o
-W
</p>
<p>il
k
te
st
6&frac14;
</p>
<p>n
o
rm
</p>
<p>al
</p>
<p>&amp;
L
ev
en
e&rsquo;
s
te
st
:
σ
2 1
&frac14;
</p>
<p>σ
2 2
</p>
<p>O
n
e-
w
ay
</p>
<p>A
N
O
V
A
:
F
-t
es
t
</p>
<p>T
w
o
-
</p>
<p>si
d
ed
*
</p>
<p>S
h
ap
ir
o
-W
</p>
<p>il
k
te
st
&frac14;
</p>
<p>n
o
rm
</p>
<p>al
</p>
<p>&amp;
L
ev
en
e&rsquo;
s
te
st
:
σ
2 1
6&frac14;
</p>
<p>σ
2 2
</p>
<p>O
n
e-
w
ay
</p>
<p>A
N
O
V
A
:
F
-t
es
t
</p>
<p>w
it
h
W
el
ch
&rsquo;s
</p>
<p>co
rr
ec
ti
o
n
</p>
<p>T
w
o
-
</p>
<p>si
d
ed
*
</p>
<p>S
h
ap
ir
o
-W
</p>
<p>il
k
te
st
6&frac14;
</p>
<p>n
o
rm
</p>
<p>al
</p>
<p>&amp;
L
ev
en
e&rsquo;
s
te
st
:
σ
2 1
6&frac14;
</p>
<p>σ
2 2
</p>
<p>K
ru
sk
al
-W
</p>
<p>al
li
s
ra
n
k
</p>
<p>te
st
</p>
<p>T
w
o
-
</p>
<p>si
d
ed
*
</p>
<p>5
O
u
tc
o
m
e
v
ar
ia
b
le
</p>
<p>ac
ro
ss
</p>
<p>th
re
e
o
r
m
o
re
</p>
<p>g
ro
u
p
s
</p>
<p>T
w
o
fa
ct
o
r
v
ar
ia
b
le
s,
</p>
<p>in
d
ep
en
d
en
t
sa
m
p
le
s
</p>
<p>S
h
ap
ir
o
-W
</p>
<p>il
k
te
st
&frac14;
</p>
<p>n
o
rm
</p>
<p>al
</p>
<p>&amp;
L
ev
en
e&rsquo;
s
te
st
:
σ
2 1
&frac14;
</p>
<p>σ
2 2
</p>
<p>T
w
o
-w
</p>
<p>ay
A
N
O
V
A
:
F
-
</p>
<p>te
st
</p>
<p>T
w
o
-
</p>
<p>si
d
ed
*
</p>
<p>S
h
ap
ir
o
-W
</p>
<p>il
k
te
st
6&frac14;
</p>
<p>n
o
rm
</p>
<p>al
</p>
<p>&amp;
L
ev
en
e&rsquo;
s
te
st
:
σ
2 1
&frac14;
</p>
<p>σ
2 2
</p>
<p>T
w
o
-w
</p>
<p>ay
A
N
O
V
A
:F
-t
es
t
</p>
<p>T
w
o
-
</p>
<p>si
d
ed
*
</p>
<p>S
h
ap
ir
o
-W
</p>
<p>il
k
te
st
&frac14;
</p>
<p>n
o
rm
</p>
<p>al
</p>
<p>&amp;
L
ev
en
e&rsquo;
s
te
st
:
σ
2 1
6&frac14;
</p>
<p>σ
2 2
</p>
<p>T
w
o
-w
</p>
<p>ay
A
N
O
V
A
:
F
-
</p>
<p>te
st
w
it
h
W
el
ch
&rsquo;s
</p>
<p>co
rr
ec
ti
o
n
</p>
<p>T
w
o
-
</p>
<p>si
d
ed
*
</p>
<p>S
h
ap
ir
o
-W
</p>
<p>il
k
te
st
6&frac14;
</p>
<p>n
o
rm
</p>
<p>al
</p>
<p>&amp;
L
ev
en
e&rsquo;
s
te
st
:
σ
2 1
6&frac14;
</p>
<p>σ
2 2
</p>
<p>K
ru
sk
al
-W
</p>
<p>al
li
s
ra
n
k
</p>
<p>te
st
</p>
<p>T
w
o
-
</p>
<p>si
d
ed
*
</p>
<p>a
S
el
ec
ti
o
n
ap
p
li
es
</p>
<p>to
sa
m
p
le
</p>
<p>si
ze
s
&gt;
</p>
<p>3
0
</p>
<p>b
If
</p>
<p>th
e
sa
m
p
le
</p>
<p>si
ze
</p>
<p>is
le
ss
</p>
<p>th
an
</p>
<p>3
0
,
y
o
u
sh
o
u
ld
</p>
<p>tr
an
sf
o
rm
</p>
<p>y
o
u
r
o
u
tc
o
m
e
v
ar
ia
b
le
&mdash;
</p>
<p>th
ro
u
g
h
lo
g
ar
it
h
m
s,
</p>
<p>sq
u
ar
e
ro
o
t
o
r
p
o
w
er
</p>
<p>tr
an
sf
o
rm
</p>
<p>at
io
n
s&mdash;
</p>
<p>an
d
re
-r
u
n
th
e
</p>
<p>n
o
rm
</p>
<p>al
it
y
te
st
.
A
lt
er
n
at
iv
el
y
,
if
n
o
rm
</p>
<p>al
it
y
re
m
ai
n
s
an
</p>
<p>is
su
e,
y
o
u
sh
o
u
ld
</p>
<p>u
se
</p>
<p>a
n
o
n
-p
ar
am
</p>
<p>et
ri
c
te
st
th
at
</p>
<p>d
o
es
</p>
<p>n
o
t
re
ly
</p>
<p>o
n
th
e
n
o
rm
</p>
<p>al
it
y
as
su
m
p
ti
o
n
</p>
<p>c
*
&frac14;
</p>
<p>N
o
te
</p>
<p>th
at
</p>
<p>al
th
o
u
g
h
th
e
u
n
d
er
ly
in
g
al
te
rn
at
iv
e
h
y
p
o
th
es
is
in
</p>
<p>A
N
O
V
A
</p>
<p>is
tw
o
-s
id
ed
,
it
s
F
-s
ta
ti
st
ic
</p>
<p>is
b
as
ed
</p>
<p>o
n
th
e
F
-d
is
tr
ib
u
ti
o
n
,
w
h
ic
h
is
ri
g
h
t-
sk
ew
</p>
<p>ed
w
it
h
</p>
<p>ex
tr
em
</p>
<p>e
v
al
u
es
</p>
<p>o
n
ly
</p>
<p>in
th
e
ri
g
h
t
ta
il
o
f
th
e
d
is
tr
ib
u
ti
o
n
</p>
<p>162 6 Hypothesis Testing &amp; ANOVA</p>
<p/>
</div>
<div class="page"><p/>
<p>6.3.3.1 Define the Testing Situation
When we test hypotheses, we may find ourselves in one of three situations. First,
</p>
<p>we can test if we want to compare a group to a hypothetical value (test 1). In our
</p>
<p>example, this can be a pre-determined target of 45 units to establish whether a
</p>
<p>promotion campaign has been effective or not. Second, we may want to compare
</p>
<p>the outcome variable (e.g., sales) across two groups (tests 2 or 3). Third, we may
</p>
<p>wish to compare whether the outcome variable differs between three or more
</p>
<p>levels of a categorical variable (also called a factor variable) with three or more
</p>
<p>sub-groups (tests 4 or 5). The factor variable is the categorical variable that we
</p>
<p>use to define the groups (e.g., three types of promotion campaigns). Similarly, we
</p>
<p>may have situations in which we have two or more factor variables (e.g., three
</p>
<p>types of promotion campaigns for two types of service). Each of these situations
</p>
<p>leads to different tests. When assessing the testing situation, we also need to
</p>
<p>establish the nature of the dependent variable and whether it is measured on an
</p>
<p>interval or ratio scale. This is important, because parametric tests are based on
</p>
<p>the assumption that the dependent variable is measured on an interval or ratio
</p>
<p>scale. Note that we only discuss situations when the test variable is interval or
</p>
<p>ratio-scaled (see Chap. 3).
</p>
<p>6.3.3.2 Determine If Samples Are Paired or Independent
Next, we need to establish whether we compare paired samples or independent
samples. The rule of thumb for determining the samples&rsquo; nature is to ask if a
respondent (or object) was sampled once or multiple times. If a respondent was
</p>
<p>sampled only once, this means that the values of one sample reveal no information
</p>
<p>about the values of the other sample. If we sample the same respondent or object
</p>
<p>twice, it means that the reported values in one period may affect the values of the
</p>
<p>sample in the next period.1 Ignoring the &ldquo;nested&rdquo; nature of the data increases the
</p>
<p>probability of type I errors. We therefore need to understand the nature of our
</p>
<p>samples in order to select a test that takes the dependency between observations
</p>
<p>(i.e., paired versus independent samples tests) into account. In Table 6.2, test
</p>
<p>2 deals with paired samples, whereas tests 3, 4 and, 5 deal with independent
</p>
<p>samples.
</p>
<p>6.3.3.3 Check Assumptions and Choose the Test
Subsequently, we need to check the distributional properties and variation of our
</p>
<p>data before deciding whether to select a parametric or a non-parametric test.
</p>
<p>1In experimental studies, if respondents were paired with others (as in a matched case control
sample), each person would be sampled once, but it still would be a paired sample.
</p>
<p>6.3 Testing Hypotheses on One Mean 163</p>
<p/>
</div>
<div class="page"><p/>
<p>Normality Test
</p>
<p>To test whether the data are normally distributed, we conduct the Shapiro-Wilk
</p>
<p>test (Shapiro and Wilk 1965) that formally tests for normality. Without going into
</p>
<p>too much detail, the Shapiro-Wilk test compares the correlation between the
</p>
<p>observed sample scores (which take the covariance between the sample scores
</p>
<p>into account) with the scores expected under a standard normal distribution. The
</p>
<p>resulting ratio is called the W-statistic and its scaled version is known as the
</p>
<p>V-statistic. When the distribution is close to normal, the V statistic will be close
</p>
<p>to 1 and the associated p-value will be larger than 0.05. Large deviations from a V
of 1 will therefore be coupled with p-values that are smaller than 0.05, suggesting
that the sample scores are not normally distributed. The Kolmogorov-Smirnov test,
</p>
<p>which we discuss in Box 6.2, is another popular test to check for normality. An
</p>
<p>alternative strategy to check for normality is by means of visual inspection, which
</p>
<p>we discuss in Box 6.3.
</p>
<p>Equality of Variances Test
</p>
<p>We use Levene&rsquo;s test (Levene 1960), also known as the F-test of sample variance,
</p>
<p>to test for the equality of the variances between two or more groups of data. The null
</p>
<p>hypothesis is that population variances across the sub-samples are the same,
</p>
<p>whereas the alternative hypothesis is that they differ. If the p-value associated
with Levene&rsquo;s statistic (referred to as W0 in Stata) is lower than 0.05, we reject
</p>
<p>the null hypothesis, which implies that the variances are heterogeneous. Con-
</p>
<p>versely, a p-value larger than 0.05 indicates homogeneous variances. In Levene&rsquo;s
original paper, the formula for the test statistic is based on the sample mean (Levene
</p>
<p>Box 6.2 The Kolmogorov-Smirnov Test
</p>
<p>An important (nonparametric) test for normality is the one-sample
Kolmogorov&ndash;Smirnov (KS) test. We can use it to test whether or not a variable
is normally distributed. Technically, when assuming a normal distribution,
</p>
<p>the KS test compares the sample scores with an artificial set of normally
</p>
<p>distributed scores that has the same mean and standard deviation as the
</p>
<p>sample data. However, this approach is known to yield biased results,
</p>
<p>which are modified by means of the Lilliefors correction (1967). The
</p>
<p>Lilliefors correction takes into consideration that we do not know the true
</p>
<p>mean and standard deviation of the population. An issue with the KS test with
</p>
<p>the Lilliefors correction is that it is very sensitive when used on large samples
</p>
<p>and often rejects the null hypothesis if very small deviations are present. This
</p>
<p>also holds for Stata&rsquo;s version of the KS test, which only works well for very
</p>
<p>large sample sizes (i.e., at least 10,000 observations). Consequently, Stata
</p>
<p>does not recommend the use of a one-sample KS test (for more, read the
</p>
<p>information in Stata&rsquo;s help file on the KS test: https://www.stata.com/
</p>
<p>manuals14/rksmirnov.pdf).
</p>
<p>164 6 Hypothesis Testing &amp; ANOVA</p>
<p/>
<div class="annotation"><a href="https://www.stata.com/manuals14/rksmirnov.pdf">https://www.stata.com/manuals14/rksmirnov.pdf</a></div>
<div class="annotation"><a href="https://www.stata.com/manuals14/rksmirnov.pdf">https://www.stata.com/manuals14/rksmirnov.pdf</a></div>
</div>
<div class="page"><p/>
<p>1960), which performs well when the variances are symmetric and have moderate
</p>
<p>tailed distributions. For skewed data, Brown and Forsythe (1974) proposed a
</p>
<p>modification of Levene&rsquo;s test whereby the group sample&rsquo;s median (referred to as
</p>
<p>W50 in Stata) replaces the group sample mean. Alternatively, the group sample&rsquo;s
</p>
<p>trimmed mean can also replace the group sample mean, whereby 10% of the
</p>
<p>smallest and largest values of the sample are removed before calculating the
</p>
<p>mean (referred to as W10 in Stata). If the data are normally distributed, the p-
values associated with W0, W10, and W50 will all align, thus all be higher than
</p>
<p>0.05. If the data are not normally distributed, this might not be the case. In this
</p>
<p>situation, we should focus on the p-values associated with the sample&rsquo;s median
(W50), because this measure is robust for samples that are not normally distributed.
</p>
<p>Parametric Tests
</p>
<p>It is clear-cut that when the normality assumption is met, we should choose a
</p>
<p>parametric test. The most popular parametric test for examining one or two means
is the t-test, which can be used for different purposes. For example, the t-test can be
used to compare one mean with a given value (e.g., do males spend more than $150 a
</p>
<p>year online?). The one-sample t-test is an appropriate test. Alternatively, we can use
</p>
<p>a t-test to test the mean difference between two samples (e.g., do males spend more
time online than females?). In this case, a two-sample t-test is appropriate. The
</p>
<p>independent samples t-tests considers two distinct groups, such as males versus
</p>
<p>females, or users versus non-users. Conversely, the paired samples t-test relates to
</p>
<p>the same set of twice observed objects (usually respondents), as in a before-after
</p>
<p>experimental design discussed in Chap. 4. We are, however, often interested in
</p>
<p>Box 6.3 Visual Check for Normality
</p>
<p>You can also use plots to visually check for normality. The normal probabil-
ity plot visually contrasts the probability distribution of the test variable&rsquo;s
ordered sample values with the cumulative probabilities of a standard normal
</p>
<p>distribution. The probability plots are structured such that the cumulative
</p>
<p>frequency distribution of a set of normally distributed data falls in a straight
</p>
<p>line. This straight line serves as a reference line, meaning that sample values&rsquo;
</p>
<p>deviations from this straight line indicate departures from normality (Everitt
</p>
<p>and Skrondal 2010). The quantile plot is another type of probability plot,
which differs from the former by comparing the quantiles (Chap. 5) of the
</p>
<p>sorted sample values with the quantiles of a standard normal distribution.
</p>
<p>Here, again, the plotted data that do not follow the straight line reveal
</p>
<p>departures from normality. The normal probability plot assesses the normal-
</p>
<p>ity of the data in the middle of the distribution well. The quantile plot is better
</p>
<p>equipped to spot non-normality in the tails. Of the two, the quantile plots are
</p>
<p>most frequently used. Note that visual checks are fairly subjective and should
</p>
<p>always be used in combination with more formal checks for normality.
</p>
<p>6.3 Testing Hypotheses on One Mean 165</p>
<p/>
</div>
<div class="page"><p/>
<p>examining the differences between the means of more than two groups of
</p>
<p>respondents. Regarding our introductory example, we might be interested in
</p>
<p>evaluating the differences between the point of sale display, the free tasting stand,
</p>
<p>and the in-store announcements&rsquo; mean sales. Instead of making several paired
</p>
<p>comparisons by means of separate t-tests, we should use the Analysis of Variance
(ANOVA). The ANOVA is useful when three or more means are compared and,
</p>
<p>depending on how many variables define the groups to be compared (will be
</p>
<p>discussed later in this chapter), can come in different forms.
</p>
<p>The parametric tests introduced in this chapter are very robust against normality
</p>
<p>assumption violations, especially when the data are distributed symmetrically. That
</p>
<p>is, small departures from normality usually translate into marginal differences in
</p>
<p>the p-values, particularly when using sample sizes greater than 30 (Boneau 1960).
Thus, even if the Shapiro&ndash;Wilk test suggests the data are not normally distributed,
</p>
<p>we don&rsquo;t have to be concerned that the parametric test results are far off, provided
</p>
<p>we have sample sizes greater than 30. The same holds for the ANOVA in cases
</p>
<p>where the sample sizes per group exceed 30.
</p>
<p>In sum, with sample sizes greater than 30, we choose a parametric test even
</p>
<p>when the Shapiro-Wilk test suggests that the data are not normally distributed.
</p>
<p>When sample sizes are less than 30, we can transform the distribution of the
</p>
<p>outcome variable&mdash;through logarithms, square root, or power transformations
</p>
<p>(Chap. 5)&mdash;so that it approaches normality and re-run a normality test. If violations
</p>
<p>of normality remain an issue, you should use a non-parametric test that is not based
</p>
<p>on the normality assumption.
</p>
<p>Next, we can also have a situation in which the data are normally distributed, but
</p>
<p>the variances between two or more groups of data are unequal. This issue is
</p>
<p>generally unproblematic as long as the group-specific sample sizes are (nearly)
</p>
<p>equal. If group-specific sample sizes are different, we recommend using parametric
</p>
<p>tests, such as the two-sample t-tests and the ANOVA, in combination with tests that
withstand or correct the lack of equal group variances, such asWelch&rsquo;s correction.
</p>
<p>Welch&rsquo;s modified test statistic (Welch 1951) adjusts the underlying parametric tests
</p>
<p>if the variances are not homogenous in order to control for a type I error. This is
</p>
<p>particularly valuable when population variances differ and groups comprise very
</p>
<p>unequal sample sizes.2 In sum, when samples are normally distributed, but the
</p>
<p>equality of the variance assumption is violated (i.e., the outcome variable is not
</p>
<p>distributed equally across three or more groups), we choose a parametric test with
</p>
<p>Welch&rsquo;s correction. Depending on the testing situation this can be: a paired t-test
with Welch&rsquo;s correction, a one-way ANOVA F-test with Welch&rsquo;s correction, or a
two-way ANOVA F-test with Welch&rsquo;s correction.
</p>
<p>2Stata does not directly support Welch&rsquo;s correction for an ANOVA, but a user-written package
called wtest is readily available and can be installed (see Chap. 5 on how to install user-written
packages in Stata). This allows you to perform a test similar to the standard ANOVA test with
Welch&rsquo;s correction. For more information see Stata&rsquo;s help file: http://www.ats.ucla.edu/stat/stata/
ado/analysis/wtest.hlp
</p>
<p>166 6 Hypothesis Testing &amp; ANOVA</p>
<p/>
<div class="annotation"><a href="http://www.ats.ucla.edu/stat/stata/ado/analysis/wtest.hlp">http://www.ats.ucla.edu/stat/stata/ado/analysis/wtest.hlp</a></div>
<div class="annotation"><a href="http://www.ats.ucla.edu/stat/stata/ado/analysis/wtest.hlp">http://www.ats.ucla.edu/stat/stata/ado/analysis/wtest.hlp</a></div>
</div>
<div class="page"><p/>
<p>Finally, where both the normality and equality of variance assumptions are
</p>
<p>violated, non-parametric tests can be chosen directly. In the following, we briefly
</p>
<p>discuss these non-parametric tests.
</p>
<p>Non-parametric Tests
</p>
<p>As indicated in Table 6.2, there is a non-parametric equivalent for each parametric
</p>
<p>test. This would be important if the distributions are not symmetric. For single
</p>
<p>samples, the Wilcoxon signed-rank test is the equivalent of one sample t-test,
which is used to test the hypothesis that the population median is equal to a fixed
</p>
<p>value. For two-group comparisons with independent samples, the Mann-Whitney
</p>
<p>U test (also called theWilcoxon rank-sum test, orWilcoxon&ndash;Mann&ndash;Whitney test) is
the equivalent of the independent t-test, while, for paired samples, this is the
Wilcoxon matched-pairs signed-rank test. The Mann-Whitney U test uses the null
hypothesis that the distributions of the two independent groups being considered
</p>
<p>(e.g., randomly assigned high and low performing stores) have the same shape
</p>
<p>(Mann and Whitney 1947). In contrast to an independent sample t-test, the Mann-
Whitney U test does not compare the means, but the two groups&rsquo; median scores.
</p>
<p>Although we will not delve into the statistics behind the test, it is important to
</p>
<p>understand its logic. The Mann-Whitney U test is based on ranks and measures the
</p>
<p>differences in location (Liao 2002). The test works by first combining the separate
</p>
<p>groups into a single group. Subsequently, each outcome variable score (e.g., sales)
</p>
<p>is sorted and ranked in respect of each condition based on the values, with the
</p>
<p>lowest rank assigned to the smallest value. The ranks are then averaged based on the
</p>
<p>conditions (e.g., high versus low performing stores) and the test statistic
</p>
<p>U calculated. The test statistic represents the difference between the two rank totals.
That is, if the distribution of the two groups is identical, then the sum of the ranks in
</p>
<p>one group will be the same as in the other group. The smaller the p-value (which
will be discussed later in this chapter), the lower the likelihood that the two
</p>
<p>distributions&rsquo; similarities have occurred by chance; the opposite holds if otherwise.
</p>
<p>The Kruskal-Wallis rank test is the non-parametric equivalent of the ANOVA.
The null hypothesis of the Kruskal-Wallis rank test is that the distribution of the test
variable across group sub-samples is identical (Schuyler 2011). Given that the
</p>
<p>emphasis is on the distribution rather than on a point estimate, rejecting the null
</p>
<p>hypothesis implies that such distributions vary in their dispersion, central tendency
</p>
<p>and/or variability. According to Schuyler (2011) and Liao (2002), the following are
</p>
<p>the steps when conducting this test: First, single group categories are combined into
</p>
<p>one group with various categories. Next, objects in this variable (e.g., stores/
</p>
<p>campaigns) are sorted and ranked based on their associations with the dependent
</p>
<p>variable (e.g., sales), with the lowest rank assigned to the smallest value. Subse-
</p>
<p>quently, the categorical variable is subdivided to reestablish the original single
</p>
<p>comparison groups. Finally, each group&rsquo;s sum of its ranks is entered into a formula
</p>
<p>that yields the calculated test statistic. If this calculated statistic is higher than the
</p>
<p>critical value, the null hypothesis is rejected. The test statistic of the Kruskal-Wallis
</p>
<p>rank follows a χ2 distribution with k� 1 degrees of freedom. Use the Kruskal-Wallis
</p>
<p>6.3 Testing Hypotheses on One Mean 167</p>
<p/>
</div>
<div class="page"><p/>
<p>H test in situations where the group variances are not equal, as it corrects group
variances&rsquo; heterogeneity.
</p>
<p>6.3.3.4 Specify the Region of Rejection
Finally, depending on the formulated hypothesis (i.e., directional versus
</p>
<p>non-directional), we should decide on whether the region of rejection is on one
</p>
<p>side (i.e., one-tailed test) or on both sides (i.e., two-tailed test) of the sampling
</p>
<p>distribution. In statistical significance testing, a one-tailed test and a two-tailed test
</p>
<p>are alternative ways of computing the statistical significance of a test statistic,
</p>
<p>depending on whether the hypothesis is expressed directionally (i.e., &lt; or &gt; in
case of a one-tailed test) or not (i.e., 6&frac14; in case of a two-tailed test). The word tail is
used, because the extremes of distributions are often small, as in the normal
</p>
<p>distribution or bell curve shown in Fig. 6.3 later in this chapter. Instead of the
</p>
<p>word tail, the word &ldquo;sided&rdquo; is sometimes used.
</p>
<p>We need to use two-tailed tests for non-directional hypotheses. Even when
</p>
<p>directional hypotheses are used, two-tailed tests are used for 75% of directional
</p>
<p>hypotheses (van Belle 2008). This is because two-tailed tests have strong
</p>
<p>advantages; they are stricter (and therefore generally considered more appropriate)
</p>
<p>and can also reject a hypothesis when the effect is in an unexpected direction. The
</p>
<p>use of two-tailed testing for a directional hypothesis is also valuable, as it identifies
</p>
<p>significant effects that occur in the opposite direction from the one anticipated.
</p>
<p>Imagine that you have developed an advertising campaign that you believe is an
</p>
<p>improvement on an existing campaign. You wish to maximize your ability to detect
</p>
<p>the improvement and opt for a one-tailed test. In doing so, you do not test for the
</p>
<p>possibility that the new campaign is significantly less effective than the old cam-
</p>
<p>paign. As discussed in various studies (van Belle 2008; Ruxton and Neuhaeuser
</p>
<p>2010), one-tailed tests should only be used when the opposite direction is theoreti-
</p>
<p>cally meaningless or impossible (Kimmel 1957; Field 2013). Such an example
</p>
<p>would apply to controlled experiments where the intervention (i.e., the drug) can
</p>
<p>only have a positive and no negative outcome, because such differences are
</p>
<p>removed beforehand and have no possible meaning (e.g., Lichters et al. 2016).
</p>
<p>The use of two-tailed tests can seem counter to the idea of hypothesis testing,
</p>
<p>because two-tailed tests, by their very nature, do not reflect any directionality in a
</p>
<p>hypothesis. However, in many situations when we have clear expectations (e.g.,
</p>
<p>sales are likely to increase), the opposite is also a possibility.
</p>
<p>6.3.4 Step 4: Calculate the Test Statistic
</p>
<p>Having formulated the study&rsquo;s main hypothesis, the significance level, and the type
</p>
<p>of test, we can now proceed with calculating the test statistic by using the sample
</p>
<p>data at hand. The test statistic is a statistic, calculated by using the sample data, to
</p>
<p>assess the strength of evidence in support of the null hypothesis (Agresti and Finlay
</p>
<p>2014). In our example, we want to compare the mean with a given standard of
</p>
<p>168 6 Hypothesis Testing &amp; ANOVA</p>
<p/>
</div>
<div class="page"><p/>
<p>45 units. Hence, we make use of a one-sample t-test, whose test statistic is
computed as follows:
</p>
<p>t &frac14; �x � μ
s�x
</p>
<p>Here �x is the sample mean, μ is the hypothesized population mean, and s �x the
standard error (i.e., the standard deviation of the sampling distribution). Let&rsquo;s first
</p>
<p>look at the formula&rsquo;s numerator, which describes the difference between the sample
</p>
<p>mean �x and the hypothesized population mean μ. If the point of sale display was
highly successful, we would expect �x to be higher than μ, leading to a positive
difference between the two in the formula&rsquo;s numerator. Alternatively, if the point of
</p>
<p>sale display was not effective, we would expect the opposite to be true. This means
</p>
<p>that the difference between the hypothesized population mean and the sample mean
</p>
<p>can go either way, implying a two-sided test. Using the data from the second
</p>
<p>column of Table 6.1, we can compute the marginal mean of the point of sales
</p>
<p>display campaign as follows:
</p>
<p>�x &frac14; 1
n
</p>
<p>X
</p>
<p>n
</p>
<p>i&frac14;1
xi &frac14;
</p>
<p>1
</p>
<p>10
50&thorn; 52&thorn; . . .&thorn; 51&thorn; 44&eth; &THORN; &frac14; 47:30
</p>
<p>When comparing the calculated sample mean (47.30) with the hypothesized
</p>
<p>value of 45, we obtain a difference of 2.30:
</p>
<p>�x � μ &frac14; 47:30� 45 &frac14; 2:30
</p>
<p>At first sight, it appears as if the campaign was effective as sales during the time
</p>
<p>of the campaign were higher than those that the store normally experiences.
</p>
<p>However, as discussed before, we have not yet considered the variation in the
</p>
<p>sample. This variation is accounted for by the standard error of �x (indicated as s�x ),
which represents the uncertainty of the sample estimate.
</p>
<p>This sounds very abstract, so what does it mean? The sample mean is used as an
</p>
<p>estimator of the population mean; that is, we assume that the sample mean can be a
</p>
<p>substitute for the population mean. However, when drawing different samples from
</p>
<p>the same population, we are likely to obtain different sample means. The standard
</p>
<p>error tells us how much variance there probably is in the mean across different
</p>
<p>samples from the same population.
</p>
<p>Why do we have to divide the difference �x � μ by the standard error s �x ? We do
so, because when the standard error is very low (there is a low level of variation or
</p>
<p>uncertainty in the data), the value in the test statistic&rsquo;s denominator is also small,
</p>
<p>which results in a higher value for the t-test statistic. Higher t-values favor the
rejection of the null hypothesis. In other words, the lower the standard error s�x , the
greater the probability that the population represented by the sample truly differs
</p>
<p>from the hypothesized value of 45.
</p>
<p>6.3 Testing Hypotheses on One Mean 169</p>
<p/>
</div>
<div class="page"><p/>
<p>But how do we compute the standard error? We do so by dividing the sample
</p>
<p>standard deviation (s) by the square root of the number of observations (n), as
follows:
</p>
<p>s �x &frac14;
s
ffiffiffi
</p>
<p>n
p &frac14;
</p>
<p>ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
</p>
<p>1
n�1
</p>
<p>Pn
i&frac14;1 xi � �x&eth; &THORN;
</p>
<p>2
q
</p>
<p>ffiffiffi
</p>
<p>n
p
</p>
<p>As we can see, a low standard deviation s decreases the standard error (which
means less ambiguity when making inferences from these data). That is, less
</p>
<p>variation in the data decreases the standard error, thus favoring the rejection of
</p>
<p>the null hypothesis. Note that the standard error also depends on the sample size n.
By increasing the number of observations, we have more information available,
</p>
<p>thus reducing the standard error.
</p>
<p>If you understand this basic principle, you will have no problems understanding
</p>
<p>most other statistical tests. Let&rsquo;s go back to the example and compute the standard
</p>
<p>error as follows:
</p>
<p>s �x &frac14;
</p>
<p>ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
</p>
<p>1
10�1 50� 47:30&eth; &THORN;
</p>
<p>2&thorn; . . .&thorn; 44� 47:30&eth; &THORN;2
h i
</p>
<p>r
</p>
<p>ffiffiffiffiffi
</p>
<p>10
p &frac14; 3:199ffiffiffiffiffi
</p>
<p>10
p � 1:012
</p>
<p>Thus, the result of the test statistic is:
</p>
<p>t &frac14; �x � μ
s�x
</p>
<p>&frac14; 2:30
1:012
</p>
<p>� 2:274
</p>
<p>This test statistic applies when we compute a sample&rsquo;s standard deviation. In
</p>
<p>some situations, however, we might know the population&rsquo;s standard deviation,
</p>
<p>which requires the use of a different test, the z-test, (see Box 6.4).
</p>
<p>Box 6.4 The z-Test
</p>
<p>In the previous example, we used sample data to calculate the standard error s�x .
If we know the population&rsquo;s standard deviation beforehand, we should use the
</p>
<p>z-test. The z-test follows a normal (instead of a t-distribution).3 The z-test is also
used in situations when the sample size exceeds 30, because the t-distribution
and normal distribution are similar for n &gt; 30. As the t-test is slightly more
accurate (also when the sample size is greater than 30), Stata uses the t-test,
which can be accessed by going to► Statistics► Summaries, tables, and tests
</p>
<p>► Classical tests of hypotheses ► t test (mean-comparison test). We do not
</p>
<p>show the formulas associated with the z-test here, but have included these in the
Web Appendix (! Downloads).
</p>
<p>3The fundamental difference between the z- and t-distributions is that the t-distribution is depen-
dent on sample size n (which the z-distribution is not). The distributions become more similar with
larger values of n.
</p>
<p>170 6 Hypothesis Testing &amp; ANOVA</p>
<p/>
</div>
<div class="page"><p/>
<p>6.3.5 Step 5: Make the Test Decision
</p>
<p>Once we have calculated the test statistic, we can decide how likely it is that the
</p>
<p>claim stated in the hypothesis is correct. This is done by comparing the test statistic
</p>
<p>with the critical value that it must exceed (Option 1). Alternatively, we can
calculate the actual probability of making a mistake when rejecting the null
</p>
<p>hypothesis and compare this value with the significance level (Option 2). In the
following, we discuss both options.
</p>
<p>6.3.5.1 Option 1: Compare the Test Statistic with the Critical Value
To make a test decision, we must first determine the critical value, which the test
</p>
<p>statistic must exceed for the null hypothesis to be rejected. In our case, the critical
</p>
<p>value comes from a t-distribution and depends on three parameters:
</p>
<p>1. the significance level,
</p>
<p>2. the degrees of freedom, and
</p>
<p>3. one-tailed versus two-tailed testing.
</p>
<p>We have already discussed the first point, so let&rsquo;s focus on the second. The degrees
</p>
<p>of freedom (usually abbreviated as df) represent the amount of information avail-
able to estimate the test statistic. In general terms, an estimate&rsquo;s degrees of freedom
</p>
<p>are equal to the amount of independent information used (i.e., the number of
</p>
<p>observations) minus the number of parameters estimated. Field (2013) provides a
</p>
<p>great explanation, which we adapted and present in Box 6.5.
</p>
<p>In our example, we count n &ndash; 1 or 10 &ndash; 1&frac14; 9 degrees of freedom for the t-statistic
to test a two-sided hypothesis of one mean. Remember that for a two-tailed test,
</p>
<p>Box 6.5 Degrees of Freedom
</p>
<p>Suppose you have a soccer team and 11 slots on the playing field. When the
</p>
<p>first player arrives, you have the choice of 11 positions in which you can place
</p>
<p>him or her. By allocating the player to a position, this occupies one position.
</p>
<p>When the next player arrives, you can choose from 10 positions. With every
</p>
<p>additional player who arrives, you have fewer choices where to position him
</p>
<p>or her. With the very last player, you no longer have the freedom to choose
</p>
<p>where to put him or her&mdash;there is only one spot left. Thus, there are 10 degrees
</p>
<p>of freedom. You have some degree of choice with 10 players, but for 1 player
</p>
<p>you don&rsquo;t. The degrees of freedom are the number of players minus 1.
</p>
<p>6.3 Testing Hypotheses on One Mean 171</p>
<p/>
</div>
<div class="page"><p/>
<p>when α is 0.05, the cumulative probability distribution is 1 &ndash; α/2 or 1 &ndash; 0.05/
</p>
<p>2 &frac14; 0.975. We divide the significance level by two, because half of our alpha tests
the statistical significance in the lower tail of the distribution (bottom 2.5%) and
</p>
<p>half in the upper tail of the distribution (top 2.5%). If the value of the test statistic is
</p>
<p>greater than the critical value, we can reject the H0.
We can find critical values for combinations of significance levels and degrees of
</p>
<p>freedom in the t-distribution table, shown in Table A1 in the Web Appendix (!
Downloads). For 9 degrees of freedom and using a significance level of, for
</p>
<p>example, 5%, the critical value of the t-statistic is 2.262. Remember that we have
to look at the α &frac14; 0.05/2 &frac14; 0.025 column, because we use a two-tailed test. This
means that for the probability of a type I error (i.e., falsely rejecting the null
</p>
<p>hypothesis) to be less than or equal to 5%, the value of the test statistic must be
</p>
<p>2.262 or greater. In our case, the test statistic (2.274) exceeds the critical value
</p>
<p>(2.262), which suggests that we should reject the null hypothesis.4 Even though the
</p>
<p>difference between the values is very small, bear in mind that hypothesis testing is
</p>
<p>binary&mdash;we either reject or don&rsquo;t reject the null hypothesis. This is also the reason
</p>
<p>why a statement such as &ldquo;the result is highly significant&rdquo; is inappropriate.
</p>
<p>Figure 6.3 summarizes this concept graphically. In this figure, you can see that
</p>
<p>the critical value tcritical for an α-level of 5% with 9 degrees of freedoms equals
� 2.262 on both sides of the distribution. This is indicated by the two dark-shaded
rejection regions in the upper 2.5% and bottom 2.5% and the remaining white 95%
</p>
<p>non-rejection region in the middle. Since the test statistic ttest (indicated by the
dotted line) falls in the dark-shaded area, we reject the null hypothesis.
</p>
<p>Table 6.3 summarizes the decision rules for rejecting the null hypothesis for
</p>
<p>different types of t-tests, where ttest describes the test statistic and tcritical the critical
value for a specific significance level α. Depending on the test&rsquo;s formulation, test
</p>
<p>values may well be negative (e.g.,�2.262). However, due to the symmetry of the t-
distribution, only positive critical values are displayed.
</p>
<p>6.3.5.2 Option 2: Compare the p-Value with the Significance Level
The above might make you remember your introductory statistics course with
</p>
<p>horror. The good news is that we do not have to bother with statistical tables
</p>
<p>when working with Stata. Stata automatically calculates the probability of
</p>
<p>obtaining a test statistic that is at least as extreme as the actually observed one if
</p>
<p>the null hypothesis is supported. This probability is also referred to as the p-value or
</p>
<p>the probability of observing a more extreme departure from the null hypothesis
</p>
<p>(Everitt and Skrondal 2010).
</p>
<p>4To obtain the critical value, write display invt(9,1&ndash;0.05/2) in the command window.
</p>
<p>172 6 Hypothesis Testing &amp; ANOVA</p>
<p/>
</div>
<div class="page"><p/>
<p>In the previous example, the p-value is the answer to the following question: If
the population mean is not equal to 45 (i.e., therefore, the null hypothesis holds),
</p>
<p>what is the probability that random sampling could lead to a test statistic value of at
</p>
<p>least � 2.274? This description shows that there is a relationship between the
p-value and the test statistic. More precisely, these two measures are inversely
related; the higher the absolute value of the test statistic, the lower the p-value and
vice versa (see Fig. 6.3).
</p>
<p>Table 6.3 Decision rules for testing decisions
</p>
<p>Type of test Null hypothesis (H0) Alternative hypothesis (H1) Reject H0 if
</p>
<p>Right-tailed test μ � value μ &gt; value |ttest|&gt; tcritical (α)
Left-tailed test μ � value μ &lt; value |ttest|&gt; tcritical (α)
Two-tailed test μ &frac14; value μ 6&frac14; value ttestj j &gt; tcritical α2
</p>
<p>� �
</p>
<p>The t-distribution for 9 degrees of freedom
two-tailed test and α = .05
</p>
<p>95% of area
</p>
<p>under the curve
</p>
<p>a = 2.5%
</p>
<p>to the left
</p>
<p>0
.1
</p>
<p>.2
.3
</p>
<p>.4
</p>
<p>a = 2.5% to
</p>
<p>the right
</p>
<p>t value
</p>
<p>-2.262
</p>
<p>tcritical
</p>
<p>2.262
</p>
<p>tcritical
</p>
<p>t
test
</p>
<p>2.274
</p>
<p>P
ro
</p>
<p>b
a
b
ili
</p>
<p>ty
 d
</p>
<p>e
n
s
it
y
</p>
<p>Fig. 6.3 Relationship between test value, critical value, and p-value
</p>
<p>6.3 Testing Hypotheses on One Mean 173</p>
<p/>
</div>
<div class="page"><p/>
<p>The description of the p-value is similar to the significance level α, which
</p>
<p>describes the acceptable probability of rejecting a true null hypothesis. How-
</p>
<p>ever, the difference is that the p-value is calculated using the sample, and that α
is set by the researcher before the test outcome is observed.5 The p-value is not
the probability of the null hypothesis being supported! Instead, we should
</p>
<p>interpret it as evidence against the null hypothesis. The α-level is an arbitrary
</p>
<p>and subjective value that the researcher assigns to the level of risk of making a
</p>
<p>type I error; the p-value is calculated from the available data. Related to this
subjectivity, there has been a revived discussion in the literature on the
</p>
<p>usefulness of p-values (e.g., Nuzzo 2014; Wasserstein and Lazar 2016).
</p>
<p>The comparison of the p-value and the significance level allows the researcher to
decide whether or not to reject the null hypothesis. Specifically, if the p-value is
smaller than or equal to the significance level, we reject the null hypothesis. Thus,
</p>
<p>when examining test results, we should make use of the following decision rule&mdash;
</p>
<p>this should become second nature!6
</p>
<p>&ndash; p-value � α ! reject H0
&ndash; p-value &gt; α ! do not reject H0
</p>
<p>Note that this decision rule applies to two-tailed tests. If you apply a one-tailed
</p>
<p>test, you need to divide the p-value in half before comparing it to α, leading to the
following decision rule7:
</p>
<p>&ndash; p-value/2 � α ! reject H0
&ndash; p-value/2 &gt; α ! do not reject H0
</p>
<p>In our example, the actual two-tailed p-value is 0.049 for a test statistic of
� 2.274, just at the significance level of 0.05. We can therefore reject the null
hypothesis and find support for the alternative hypothesis.8
</p>
<p>5Unfortunately, there is some confusion about the difference between the α and p-value. See
Hubbard and Bayarri (2003) for a discussion.
6Note that this is convention and most textbooks discuss hypothesis testing in this way. Originally,
two testing procedures were developed, one by Neyman and Pearson and another by Fisher (for
more details, see Lehmann 1993). Agresti and Finlay (2014) explain the differences between the
convention and the two original procedures.
7Note that this doesn&rsquo;t apply, for instance, to exact tests for probabilities.
8We don&rsquo;t have to conduct manual calculations and tables when working with Stata. However, we
can easily compute the p-value ourselves by using the TDIST function in Microsoft Excel. The
function has the general form &ldquo;TDIST(t, df, tails)&rdquo;, where t describes the test value, df the degrees
of freedom, and tails specifies whether it&rsquo;s a one-tailed test (tails&frac14; 1) or two-tailed test (tails&frac14; 2).
Just open a new spreadsheet for our example and type in &ldquo;&frac14;TDIST(2.274,9,1)&rdquo;. Likewise, there
are several webpages with Java-based modules (e.g., http://graphpad.com/quickcalcs/pvalue1.
cfm) that calculate p-values and test statistic values.
</p>
<p>174 6 Hypothesis Testing &amp; ANOVA</p>
<p/>
<div class="annotation"><a href="http://graphpad.com/quickcalcs/pvalue1.cfm">http://graphpad.com/quickcalcs/pvalue1.cfm</a></div>
<div class="annotation"><a href="http://graphpad.com/quickcalcs/pvalue1.cfm">http://graphpad.com/quickcalcs/pvalue1.cfm</a></div>
</div>
<div class="page"><p/>
<p>6.3.6 Step 6: Interpret the Results
</p>
<p>The conclusion reached by hypothesis testing must be expressed in terms of the
</p>
<p>market research problem and the relevant managerial action that should be taken. In
</p>
<p>our example, we conclude that there is evidence that the point of sale display
</p>
<p>influenced the number of sales significantly during the week it was installed.
</p>
<p>6.4 Two-Samples t-Test
</p>
<p>6.4.1 Comparing Two Independent Samples
</p>
<p>Testing the relationship between two independent samples is very common in
</p>
<p>market research. Some common research questions are:
</p>
<p>&ndash; Do heavy and light users&rsquo; satisfaction with a product differ?
</p>
<p>&ndash; Do male customers spend more money online than female customers?
</p>
<p>&ndash; Do US teenagers spend more time on Facebook than Australian teenagers?
</p>
<p>Each of these hypotheses aim at evaluating whether two populations (e.g., heavy
</p>
<p>and light users), represented by samples, are significantly different in terms of
</p>
<p>certain key variables (e.g., satisfaction ratings).
</p>
<p>To understand the principles of the two independent samples t-test, let&rsquo;s recon-
sider the previous example of a promotion campaign in a department store. Specifi-
</p>
<p>cally, we want to test whether the population mean of the point of sale display&rsquo;s
</p>
<p>sales (μ1) differs in any (positive or negative) way from that of the free tasting stand
</p>
<p>(μ2). The resulting null and alternative hypotheses are now:
</p>
<p>H0: μ1 &frac14; μ2
</p>
<p>H1: μ1 6&frac14; μ2
The test statistic of the two independent samples t-test&mdash;which is distributed
</p>
<p>with n1 + n2�2 degrees of freedom&mdash;is similar to the one-sample t-test:
</p>
<p>t &frac14; �x 1 � �x 2&eth; &THORN; � μ1 � μ2&eth; &THORN;
s�x 1� �x 2
</p>
<p>,
</p>
<p>where �x 1 is the mean of the first sample (with n1 numbers of observations) and �x 2
is the mean of the second sample (with n2 numbers of observations). The term
μ1 � μ2 describes the hypothesized difference between the population means. In
this case, μ1 � μ2 is zero, as we assume that the means are equal, but we could use
any other value to hypothesize a specific difference in population means. Lastly,
</p>
<p>s �x 1� �x 2 describes the standard error, which comes in two forms:
</p>
<p>6.4 Two-Samples t-Test 175</p>
<p/>
</div>
<div class="page"><p/>
<p>&ndash; If we assume that the two populations have the same variance (i.e., σ21 &frac14; σ22), we
compute the standard error based on the so called pooled variance estimate:
</p>
<p>s �x 1��x 2 &frac14;
</p>
<p>ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
</p>
<p>n1 � 1&eth; &THORN; � s21 &thorn; n2 � 1&eth; &THORN; � s22
� �
</p>
<p>n1&thorn;n2 � 2
</p>
<p>s
</p>
<p>�
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
</p>
<p>1
</p>
<p>n1
&thorn; 1
n2
</p>
<p>r
</p>
<p>&ndash; Alternatively, if we assume that the population variances differ (i.e., σ21 6&frac14; σ22),
we compute the standard error as follows:
</p>
<p>s�x 1� �x 2 &frac14;
</p>
<p>ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
</p>
<p>s21
n1
</p>
<p>&thorn; s
2
2
</p>
<p>n2
</p>
<p>s
</p>
<p>How do we determine whether the two populations have the same variance? As
</p>
<p>discussed previously, this is done using Levene&rsquo;s test, which tests the following
</p>
<p>hypotheses:
</p>
<p>H0: σ
2
1 &frac14; σ22
</p>
<p>H1: σ
2
1 6&frac14; σ22
</p>
<p>The null hypothesis is that the two population variances are the same and the
</p>
<p>alternative hypothesis is that they differ. In this example, Levene&rsquo;s test provides
</p>
<p>support for the assumption that the variances in the population are equal, which
</p>
<p>allows us to proceed with the pooled variance estimate. First, we estimate the
</p>
<p>variances of the first and second group as follows:
</p>
<p>s21 &frac14;
1
</p>
<p>n1 � 1
X10
</p>
<p>i&frac14;1 &eth;x1 � �x 1&THORN;
2 &frac14; 1
</p>
<p>10� 1
h
</p>
<p>&eth;50� 47:30&THORN;2 &thorn; � � � &thorn; &eth;44� 47:30&THORN;2
i
</p>
<p>� 10:233
</p>
<p>s22 &frac14;
1
</p>
<p>n2 � 1
X10
</p>
<p>i&frac14;1 x2 � �x 2&eth; &THORN;
2 &frac14; 1
</p>
<p>10� 1 55� 52&eth; &THORN;
2 &thorn; . . .&thorn; 44� 52&eth; &THORN;2
</p>
<p>h i
</p>
<p>� 17:556:
</p>
<p>Using the variances as input, we can compute the standard error:
</p>
<p>s�x 1� �x 2 &frac14;
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
</p>
<p>10� 1&eth; &THORN; � 10:233&thorn; 10� 1&eth; &THORN; � 17:556&frac12; �
10&thorn; 10� 2
</p>
<p>r
</p>
<p>�
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
</p>
<p>1
</p>
<p>10
&thorn; 1
10
</p>
<p>r
</p>
<p>� 1:667
</p>
<p>176 6 Hypothesis Testing &amp; ANOVA</p>
<p/>
</div>
<div class="page"><p/>
<p>Inserting the estimated standard error into the test statistic results in:
</p>
<p>t &frac14; �x 1 � �x 2&eth; &THORN; � μ1 � μ2&eth; &THORN;
s �x 1��x 2
</p>
<p>&frac14; 47:30� 52&eth; &THORN; � 0
1:667
</p>
<p>� �2:819
</p>
<p>The test statistic follows a t-distribution with n1 &ndash; n2 degrees of freedom. In our
case, we have 10 &thorn; 10 &ndash; 2 &frac14; 18 degrees of freedom. Looking at the statistical
Table A1 in the Web Appendix (! Downloads), we can see that the critical value
of a two-sided test with a significance level of 5% is 2.101 (note that we should look
</p>
<p>at the column labeled α&frac14; 0.025 and the line labeled df&frac14; 18). The absolute value of
the test statistic (i.e., 2.819) is greater than the critical value of 2.101 and, thus, falls
</p>
<p>within the bottom 2.5% of the distribution. We can therefore reject the null
</p>
<p>hypothesis at a significance level of 5% and conclude that the absolute difference
</p>
<p>between means of the point of sale display&rsquo;s sales (μ1) and those of the free tasting
</p>
<p>stand (μ2) is significantly different from 0.
</p>
<p>6.4.2 Comparing Two Paired Samples
</p>
<p>In the previous example, we compared the mean sales of two independent samples.
</p>
<p>If the management wants to compare the difference in the units sold before and after
</p>
<p>they started the point of sale display campaign. The difference can be either way;
</p>
<p>that is, it can be higher or lower, indicating a two-sided test. We have sales data for
</p>
<p>the week before the point of sale display was installed, as well as for the following
</p>
<p>week when the campaign was in full swing (i.e., the point of sale display had been
</p>
<p>installed). Table 6.4 shows the sale figures of the 10 stores in respect of both
</p>
<p>experimental conditions. You can again assume that the data are normally
</p>
<p>distributed.
</p>
<p>At first sight, it appears that the point of sale display generated higher sales: The
</p>
<p>marginal mean of the sales in the week during which the point of sale display was
</p>
<p>Table 6.4 Sales data (extended)
</p>
<p>Store
</p>
<p>Sales (units)
</p>
<p>No point of sale display Point of sale display
</p>
<p>50461
</p>
<p>53512
</p>
<p>43403
</p>
<p>50484
</p>
<p>47465
</p>
<p>45456
</p>
<p>44427
</p>
<p>53518
</p>
<p>51499
</p>
<p>444310
</p>
<p>4846.10Marginal mean
</p>
<p>6.4 Two-Samples t-Test 177</p>
<p/>
</div>
<div class="page"><p/>
<p>installed (48) is slightly higher than in the week when it was not (46.10). However,
</p>
<p>the question is whether this difference is statistically significant.
</p>
<p>We cannot assume that we are comparing two independent samples, as each set
</p>
<p>of two samples originates from the same set of stores, but at different points in time
</p>
<p>and under different conditions. Hence, we should use a paired samples t-test. In this
example, we want to test whether the sales differ significantly with or without the
</p>
<p>installation of the point of sale display. We can express this by using the following
</p>
<p>hypotheses, where μd describes the population difference in sales; the null hypoth-
</p>
<p>esis assumes that the point of sale display made no difference, while the alternative
</p>
<p>hypothesis assumes a difference in sales:
</p>
<p>H0: μd &frac14; 0
</p>
<p>H1: μd 6&frac14; 0
</p>
<p>To carry out this test, we define a new variable di, which captures the differences
in sales between the two conditions (point of sale display &ndash; no point of sale display)
</p>
<p>in each of the stores. Thus:
</p>
<p>d1 &frac14; 50� 46 &frac14; 4
</p>
<p>d2 &frac14; 53� 51 &frac14; 2
</p>
<p>. . .
</p>
<p>d9 &frac14; 51� 49 &frac14; 2
</p>
<p>d10 &frac14; 44� 43 &frac14; 1
</p>
<p>Based on these results, we calculate the mean difference:
</p>
<p>�d &frac14; 1
n
</p>
<p>X
</p>
<p>10
</p>
<p>i&frac14;1
di &frac14;
</p>
<p>1
</p>
<p>10
4&thorn; 2&thorn; . . .&thorn; 2&thorn; 1&eth; &THORN; &frac14; 1:9
</p>
<p>as well as the standard error of this difference:
</p>
<p>s �d &frac14;
</p>
<p>ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
</p>
<p>1
</p>
<p>n� 1
X10
</p>
<p>i&frac14;1 di � �d&eth; &THORN;
2
</p>
<p>r
</p>
<p>ffiffiffi
</p>
<p>n
p
</p>
<p>&frac14;
</p>
<p>ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
</p>
<p>1
</p>
<p>9
4� 1:9&eth; &THORN;2 &thorn; 2� 1:9&eth; &THORN;2 &thorn; . . .&thorn; 2� 1:9&eth; &THORN;2 &thorn; 1� 1:9&eth; &THORN;2
</p>
<p>h i
</p>
<p>r
</p>
<p>ffiffiffiffiffi
</p>
<p>10
p � 0:383
</p>
<p>178 6 Hypothesis Testing &amp; ANOVA</p>
<p/>
</div>
<div class="page"><p/>
<p>Next, we compare the mean difference �d in our sample with the difference
expected under the null hypothesis μd and divide this difference by the standard
</p>
<p>error s �d . Thus, the test statistic is:
</p>
<p>t &frac14;
�d � μd
s �d
</p>
<p>&frac14; 1:9� 0
0:383
</p>
<p>� 4:960:
</p>
<p>The test statistic follows a t-distribution with n &ndash; 1 degrees of freedom, where
n is the number of pairs that we compare. Recall that for a two-tailed test, when α
is 0.05, we need to look at the column labeled α&frac14; 0.025 and the line labeled df&frac14; 9.
Looking at Table A1 in the Web Appendix (! Downloads), we can see that the
critical value of a two-sided test with a significance level of 5% is 2.262 for 9 degrees
</p>
<p>of freedom. Since the test value (4.960) is larger than the critical value, we can reject
</p>
<p>the null hypothesis and presume that the point of sale display makes a difference.
</p>
<p>6.5 Comparing More Than Two Means: Analysis of Variance
(ANOVA)
</p>
<p>Researchers are often interested in examining differences in the means between
</p>
<p>more than two groups. For example:
</p>
<p>&ndash; Do light, medium, and heavy internet users differ in respect of their monthly
</p>
<p>disposable income?
</p>
<p>&ndash; Do customers across four different types of demographic segments differ in their
</p>
<p>attitude towards a certain brand?
</p>
<p>&ndash; Is there a significant difference in hours spent on Facebook between US, UK,
</p>
<p>and Australian teenagers?
</p>
<p>Continuing with our previous example on promotion campaigns, we might be inter-
</p>
<p>ested in whether there are significant sales differences between the stores in which the
</p>
<p>three different types of campaigns were launched. One way to tackle this research
</p>
<p>question would be to carry out multiple pairwise comparisons of all the groups under
</p>
<p>consideration. In this example, doing so would require the following comparisons:
</p>
<p>1. the point of sale display versus the free tasting stand,
</p>
<p>2. the point of sale display versus the in-store announcements, and
</p>
<p>3. the free tasting stand versus the in-store announcements.
</p>
<p>While three comparisons seem manageable, you can imagine the difficulties when a
</p>
<p>greater number of groups are compared. For example, with ten groups, we would
</p>
<p>have to carry out 45 group comparisons.9
</p>
<p>9The number of pairwise comparisons is calculated as follows: k�(k � 1)/2, with k the number of
groups to compare.
</p>
<p>6.5 Comparing More Than Two Means: Analysis of Variance (ANOVA) 179</p>
<p/>
</div>
<div class="page"><p/>
<p>Making large numbers of comparisons induces the severe problem of α-infla-
</p>
<p>tion. This inflation refers to the more tests that you conduct at a certain significance
</p>
<p>level, the more likely you are to claim a significant result when this is not so (i.e., an
</p>
<p>increase or inflation in the type I error). Using a significance level of α &frac14; 0.05 and
making all possible pairwise comparisons of ten groups (i.e., 45 comparisons), the
</p>
<p>increase in the overall probability of a type I error (also referred to as familywise
</p>
<p>error rate) is:
</p>
<p>α
&lowast; &frac14; 1� 1� α&eth; &THORN;45 &frac14; 1� 1� 0:05&eth; &THORN;45 &frac14; 0:901
</p>
<p>That is, there is a 90.1% probability of erroneously rejecting your null hypothe-
</p>
<p>sis in at least some of your 45 t-tests&mdash;far greater than the 5% for a single
comparison! The problem is that you can never tell which of the comparisons&rsquo;
</p>
<p>results are wrong and which are correct.
</p>
<p>Instead of carrying out many pairwise tests, market researchers use ANOVA,
</p>
<p>which allows a comparison of three or more groups&rsquo; averages. In ANOVA, the
</p>
<p>variable that differentiates the groups is referred to as the factor variable (don&rsquo;t
confuse this with the factors of factor analysis discussed in Chap. 8!). The values of
</p>
<p>a factor (i.e., as found in respect of the different groups under consideration) are
</p>
<p>also referred to as factor levels.
In the previous example of promotion campaigns, we considered only one factor
</p>
<p>variable with three levels, indicating the type of campaign. This is the simplest form
</p>
<p>of an ANOVA and is called a one-way ANOVA. However, ANOVA allows us to
</p>
<p>consider more than one factor variable. For example, we might be interested in
</p>
<p>adding another grouping variable (e.g., the type of service offered), thus increasing
</p>
<p>the number of treatment conditions in our experiment. In this case, we should use a
</p>
<p>two-way ANOVA to analyze both factor variables&rsquo; effect on the sales (in isolation
</p>
<p>and jointly). ANOVA is even more flexible, because you can also integrate interval
</p>
<p>or ratio-scaled independent variables and even multiple dependent variables. We
</p>
<p>first introduce the one-way ANOVA, followed by a brief discussion of the two-way
</p>
<p>ANOVA.10 For a more detailed discussion of the latter, you can turn to the Web
</p>
<p>Appendix (! Downloads).
</p>
<p>6.6 Understanding One-Way ANOVA
</p>
<p>We now know ANOVA is used to examine the mean differences between more than
</p>
<p>two groups. In more formal terms, the objective of the one-way ANOVA is to test
</p>
<p>the null hypothesis that the population means of the groups (defined by the factor
</p>
<p>variable and its levels) are equal. If we compare three groups, as in the promotion
</p>
<p>campaign example, the null hypothesis is:
</p>
<p>10Mitchell (2015) provides a detailed introduction to other ANOVA types, such as the analysis of
covariance (ANCOVA).
</p>
<p>180 6 Hypothesis Testing &amp; ANOVA</p>
<p/>
</div>
<div class="page"><p/>
<p>H0: μ1 &frac14; μ2 &frac14; μ3
This hypothesis implies that the population means of all three promotion
</p>
<p>campaigns are identical (which is the same as saying, that the campaigns have the
</p>
<p>same effect on the mean sales). The alternative hypothesis is:
</p>
<p>H1: At least two of μ1, μ2, and μ3 are unequal:
</p>
<p>Before we even think of running an ANOVA, we should, of course, produce a
</p>
<p>problem formulation, which requires us to identify the dependent variable and the
</p>
<p>factor variable, as well as its levels. Once this task is done, we can dig deeper into
</p>
<p>ANOVA by following the steps described in Fig. 6.4. We next discuss each step in
</p>
<p>more detail.
</p>
<p>6.6.1 Check the Assumptions
</p>
<p>ANOVA is a parametric test that relies on the same distributional assumptions as
</p>
<p>discussed in Sect. 6.3.3.3. We may use ANOVA in situations when the dependent
</p>
<p>variable is measured on an ordinal scale and is not normally distributed, but we
</p>
<p>should then ensure that the group-specific sample sizes are similar. Thus, if
</p>
<p>possible, it is useful to collect samples of a similar size for each group. As discussed
</p>
<p>Calculate the test statistic
</p>
<p>Check the assumptions
</p>
<p>Make the test decision
</p>
<p>Carry out post hoc tests
</p>
<p>Measure the strength of the effects
</p>
<p>Interpret the results
</p>
<p>Plot the results (optional)
</p>
<p>Fig. 6.4 Steps in conducting an ANOVA
</p>
<p>6.6 Understanding One-Way ANOVA 181</p>
<p/>
</div>
<div class="page"><p/>
<p>previously, ANOVA is robust to departures from normality with sample sizes
</p>
<p>greater than 30, meaning that it can be performed even when the data are not
</p>
<p>normally distributed. Even though ANOVA is rather robust in this respect,
</p>
<p>violations of the assumption of the equality of variances can bias the results
</p>
<p>significantly, especially when the groups are of very unequal sample size.11 Conse-
</p>
<p>quently, we should always test for the equality of variances by using Levene&rsquo;s test.
</p>
<p>We already touched upon Levene&rsquo;s test and you can learn more about it in Web
</p>
<p>Appendix (! Chap. 6).
Finally, like any data analysis technique, the sample size must be sufficiently
</p>
<p>high to have sufficient statistical power. There is general agreement that the bare
</p>
<p>minimum sample size per group is 20. However, 30 or more observations per group
</p>
<p>are desirable. For more detail, see Box 6.1.
</p>
<p>6.6.2 Calculate the Test Statistic
</p>
<p>ANOVA examines the dependent variable&rsquo;s variation across groups and, based on
</p>
<p>this variation, determines whether there is reason to believe that the population
</p>
<p>means of the groups differ. Returning to our example, each store&rsquo;s sales are likely to
</p>
<p>deviate from the overall sales mean, as there will always be some variation. The
</p>
<p>question is therefore whether a specific promotion campaign is likely to cause the
</p>
<p>difference between each store&rsquo;s sales and the overall sales mean, or whether this is
</p>
<p>due to a natural variation in sales. To disentangle the effect of the treatment (i.e., the
</p>
<p>promotion campaign type) and the natural variation, ANOVA separates the total
</p>
<p>variation in the data (indicated by SST) into two parts:
</p>
<p>1. the between-group variation (SSB), and
2. the within-group variation (SSW).
</p>
<p>12
</p>
<p>These three types of variation are estimates of the population variation.
</p>
<p>Conceptually, the relationship between the three types of variation is expressed as:
</p>
<p>SST &frac14; SSB &thorn; SSW
</p>
<p>However, before we get into the math, let&rsquo;s see what SSB and SSW are all about.
</p>
<p>6.6.2.1 The Between-Group Variation (SSB)
SSB refers to the variation in the dependent variable as expressed in the variation in
the group means. In our example, it describes the variation in the mean values of
</p>
<p>sales across the three treatment conditions (i.e., point of sale display, free tasting
</p>
<p>11In fact, these two assumptions are interrelated, since unequal group sample sizes result in a
greater probability that we will violate the homogeneity assumption.
12SS is an abbreviation of &ldquo;sum of squares,&rdquo; because the variation is calculated using the squared
differences between different types of values.
</p>
<p>182 6 Hypothesis Testing &amp; ANOVA</p>
<p/>
</div>
<div class="page"><p/>
<p>stand, and in-store announcements) in relation to the overall mean. What does SSB
tell us? Imagine a situation in which all mean values across the treatment conditions
</p>
<p>are the same. In other words, regardless of which campaign we choose, the sales are
</p>
<p>the same, we cannot claim that the promotion campaigns had differing effects. This
</p>
<p>is what SSB expresses: it tells us how much variation the differences in observations
that truly stem from different groups can explain (for more, see Box 6.6). Since SSB
is the explained variation (explained by the grouping of data) and thus reflects
different effects, we would want it to be as high as possible. However, there is no
</p>
<p>given standard of how high SSB should be, as its magnitude depends on the scale
level used (e.g., are we looking at 7-point Likert scales or income in US$?).
</p>
<p>Consequently, we can only interpret the explained variation expressed by SSB in
relation to the variation that the grouping of data does not explain. This is where
</p>
<p>SSW comes into play.
</p>
<p>6.6.2.2 The Within-Group Variation (SSW)
As the name already suggests, SSW describes the variation in the dependent variable
within each of the groups. In our example, SSW simply represents the variation in
sales in each of the three treatment conditions. The smaller the variation within the
</p>
<p>groups, the greater the probability that the grouping of data can explain all the
</p>
<p>observed variation. It is obviously the ideal for this variation to be as small as
</p>
<p>possible. If there is much variation within some or all the groups, then some
</p>
<p>extraneous factor, not accounted for in the experiment, seems to cause this variation
</p>
<p>instead of the grouping of data. For this reason, SSW is also referred to as unex-
plained variation.
</p>
<p>Unexplained variation can occur if we fail to account for important factors in our
</p>
<p>experimental design. For example, in some of the stores, the product might have
</p>
<p>been sold through self-service, while personal service was available in others. This
</p>
<p>is a factor that we have not yet considered in our analysis, but which will be used
</p>
<p>Box 6.6 Types of Sums of Squares in Stata
</p>
<p>Stata allows two options to represent a model&rsquo;s sums of squares. The first, and
</p>
<p>also the default option, is the partial sums of squares. To illustrate what this
measure represents, presume we have a model with one independent variable
</p>
<p>var1 and we want to know what additional portion of our model&rsquo;s variation is
explained if we add independent variable var2, to the model. The partial sums
of squares indicates the portion of the variation that is explained by var2,
given var1. The second option is the sequential sums of squares, which adds
variables one at a time to the model in order to assess the model&rsquo;s incremental
</p>
<p>improvement with each newly added variable. Of the two options, the partial
</p>
<p>sums of squares is the simpler one and does not rely on the ordering of the
</p>
<p>variables in the model, but is not suitable for full factorial designs that include
</p>
<p>interactions between two variables.
</p>
<p>6.6 Understanding One-Way ANOVA 183</p>
<p/>
</div>
<div class="page"><p/>
<p>when we look at the two-way ANOVA later in the chapter. Nevertheless, some
</p>
<p>unexplained variation will always be present, regardless of how sophisticated our
</p>
<p>experimental design is and how many factors we consider. If the unexplained
</p>
<p>variation cannot be explained, it is called random noise or simply noise.
</p>
<p>6.6.2.3 Combining SSB and SSW into an Overall Picture
The comparison of SSB and SSW tells us whether the variation in the data is
attributable to the grouping, which is desirable, or due to sources of variation not
</p>
<p>captured by the grouping, which is not desirable. Figure 6.5 shows this relationship
</p>
<p>across the stores featuring our three different campaign types:
</p>
<p>&ndash; point of sale display (&bull;),
</p>
<p>&ndash; free tasting stand (▪), and
</p>
<p>&ndash; in-store announcements (~).
</p>
<p>We indicate the group mean of each level by dashed lines. If the group means are all
</p>
<p>the same, the three dashed lines are horizontally aligned and we then conclude that
</p>
<p>the campaigns have identical sales. Alternatively, if the dashed lines are very
</p>
<p>different, we conclude that the campaigns differ in their sales.
</p>
<p>At the same time, we would like the variation within each of the groups to be as
</p>
<p>small as possible; that is, the vertical lines connecting the observations and the
</p>
<p>dashed lines should be short. In the most extreme case, all observations would lie on
</p>
<p>2 4 6 8 10 12 14 16 18 20 22 24 26 28 30
</p>
<p>45
</p>
<p>50
</p>
<p>55
</p>
<p>60
</p>
<p>40
</p>
<p>Point of sale display
</p>
<p>In-store announcements
</p>
<p>Store
</p>
<p>Free tasting stand
</p>
<p>S
a
</p>
<p>le
s
</p>
<p>Fig. 6.5 Scatter plot of stores with different campaigns vs. sales
</p>
<p>184 6 Hypothesis Testing &amp; ANOVA</p>
<p/>
</div>
<div class="page"><p/>
<p>their group-specific dashed lines, implying that the grouping explains the variation
</p>
<p>in sales perfectly. This, however, hardly ever occurs.
</p>
<p>If the vertical bars were all, say, twice as long, it would be difficult to draw any
</p>
<p>conclusions about the effects of the different campaigns. Too great a variation
</p>
<p>within the groups then swamps the variation between the groups. Based on the
</p>
<p>discussion above, we can calculate the three types of variation.
</p>
<p>1. The total variation, computed by comparing each store&rsquo;s sales with the overall
</p>
<p>mean, which is equal to 48 in our example:13
</p>
<p>SST &frac14;
Xn
</p>
<p>i&frac14;1 xi � �x&eth; &THORN;
2
</p>
<p>&frac14; 50� 48&eth; &THORN;2 &thorn; 52� 48&eth; &THORN;2 &thorn; . . .&thorn; 47� 48&eth; &THORN;2 &thorn; 42� 48&eth; &THORN;2 &frac14; 584
</p>
<p>2. The between-group variation, computed by comparing each group&rsquo;s mean sales
</p>
<p>with the overall mean, is:
</p>
<p>SSB &frac14;
X k
</p>
<p>j&frac14;1 nj �x j � �x
� �2
</p>
<p>As you can see, besides index i, as previously discussed, we also have index j to
represent the group sales means. Thus, �x j describes the mean in the j-th group and nj
the number of observations in that group. The overall number of groups is denoted
</p>
<p>with k. The term nj is used as a weighting factor: Groups that have many
observations should be accounted for to a higher degree relative to groups with
</p>
<p>fewer observations. Returning to our example, the between-group variation is then
</p>
<p>given by:
</p>
<p>SSB &frac14; 10 � 47:30� 48&eth; &THORN;2 &thorn; 10 � 52� 48&eth; &THORN;2 &thorn; 10 � 44:70� 48&eth; &THORN;2 &frac14; 273:80
</p>
<p>3. The within-group variation, computed by comparing each store&rsquo;s sales with its
</p>
<p>group sales mean is:
</p>
<p>SSw &frac14;
X
</p>
<p>k
</p>
<p>j&frac14;1
</p>
<p>X
</p>
<p>nj
</p>
<p>i&frac14;1
&eth;xij � �x j&THORN;2
</p>
<p>13Note that the group-specific sample size in this example is too small to draw conclusions and is
only used to show the calculation of the statistics.
</p>
<p>6.6 Understanding One-Way ANOVA 185</p>
<p/>
</div>
<div class="page"><p/>
<p>Here, we should use two summation signs, because we want to compute the
</p>
<p>squared differences between each store&rsquo;s sales and its group sales&rsquo; mean for all
</p>
<p>k groups in our set-up. In our example, this yields the following:
</p>
<p>SSW &frac14; 50� 47:30&eth; &THORN;2 &thorn; . . .&thorn; 44� 47:30&eth; &THORN;2
h i
</p>
<p>&thorn; 55� 52&eth; &THORN;2 &thorn; . . .&thorn; 44� 52&eth; &THORN;2
h i
</p>
<p>&thorn; 45� 44:70&eth; &THORN;2 &thorn; . . .&thorn; 42� 44:70&eth; &THORN;2
h i
</p>
<p>&frac14; 310:20
</p>
<p>In the previous steps, we discussed the comparison of the between-group and
</p>
<p>within-group variation. The higher the between-group variation is in relation to the
</p>
<p>within-group variation, the more likely it is that the grouping of the data is
</p>
<p>responsible for the different levels in the stores&rsquo; sales instead of the natural
</p>
<p>variation in all the sales.
</p>
<p>A suitable way to describe this relation is by forming an index with SSB in the
numerator and SSW in the denominator. However, we do not use SSB and SSW
directly, because they are based on summed values and the scaling of the variables
</p>
<p>used therefore influence them. Therefore, we divide the values of SSB and SSW by
their degrees of freedom to obtain the true mean square values MSB (called
between-group mean squares) and MSW (called within-group mean squares). The
resulting mean square values are:
</p>
<p>MSB &frac14;
SSB
</p>
<p>k � 1 ,and MSW &frac14;
SSW
</p>
<p>n� k
We use these mean squares to compute the following test statistic, which we then
</p>
<p>compare with the critical value:
</p>
<p>F &frac14; MSB
MSW
</p>
<p>Turning back to our example, we calculate the test statistic as follows:
</p>
<p>F &frac14; MSB
MSW
</p>
<p>&frac14;
SSB=
</p>
<p>k�1
SSW=
</p>
<p>n�k
&frac14;
</p>
<p>273:80=
3�1
</p>
<p>310:20=
30�3
</p>
<p>� 11:916
</p>
<p>6.6.3 Make the Test Decision
</p>
<p>Making the test decision in ANOVA is like the t-tests discussed earlier, with the
difference that the test statistic follows an F-distribution (as opposed to a t-distri-
bution). Different from before, however, we don&rsquo;t have to divide α by 2 when
</p>
<p>186 6 Hypothesis Testing &amp; ANOVA</p>
<p/>
</div>
<div class="page"><p/>
<p>looking up the critical value, even though the underlying alternative hypothesis in
</p>
<p>ANOVA is two-sided! The reason for this is that an F-test statistic is the ratio of the
variation explained by systematic variance (i.e., between-group mean squares) to
</p>
<p>the unsystematic variance (i.e., within-group mean squares), which is always equal
</p>
<p>to or greater than 0, but never lower than 0. For this reason, and given that no
</p>
<p>negative values can be taken, it makes no sense to split the significance level in half,
</p>
<p>although you can always choose a more restrictive alpha (van Belle 2008).
</p>
<p>Unlike the t-distribution, the F-distribution depends on two degrees of freedom:
One corresponding to the between-group mean squares (k � 1) and the other
referring to the within-group mean squares (n � k). The degrees of freedom of
the promotion campaign example are 2 and 27; therefore, on examining Table A2 in
</p>
<p>the Web Appendix (! Downloads), we see a critical value of 3.354 for α &frac14; 0.05.
In our example, we reject the null hypothesis, because the F-test statistic of 11.916
is greater than the critical value of 3.354. Consequently, we can conclude that at
</p>
<p>least two of the population sales means of the three types of promotion campaigns
</p>
<p>differ significantly.
</p>
<p>At first sight, it appears that the free tasting stand is most successful, as it exhibits
</p>
<p>the highest mean sales (�x 2 &frac14; 52) compared to the point of sale display (�x 1 &frac14; 47.30)
and the in-store announcements (�x 3 &frac14; 44.70). However, rejecting the null hypothe-
sis does not mean that all the population means differ&mdash;it only means that at least
</p>
<p>two of the population means differ significantly! Market researchers often assume
</p>
<p>that all means differ significantly when interpreting ANOVA results, but this is
</p>
<p>wrong. How then do we determine which of the mean values differ significantly
</p>
<p>from the others? We deal with this problem by using post hoc tests, which is done in
</p>
<p>the next step of the analysis.
</p>
<p>6.6.4 Carry Out Post Hoc Tests
</p>
<p>Post hoc tests perform multiple comparison tests on each pair of groups and tests
</p>
<p>which of the groups differ significantly from each other. The basic idea underlying
</p>
<p>post hoc tests is to perform tests on each pair of groups and to correct the level of
</p>
<p>significance of each test. This way, the overall type I error rate across all the
</p>
<p>comparisons (i.e., the familywise error rate) remains constant at a certain level,
such as at α &frac14; 0.05 (i.e., α-inflation is avoided).
</p>
<p>There are several post hoc tests, the easiest of which is the Bonferroni correc-
</p>
<p>tion. This correction maintains the familywise error rate by calculating a new
</p>
<p>pairwise alpha that divides the statistical significance level of α by the number of
</p>
<p>comparisons made. How does this correction work? In our example, we can
</p>
<p>compare three groups pairwise: (1) Point of sale display vs. free tasting stand,
</p>
<p>(2) point of sale display vs. in-store announcements, and (3) free tasting stand
</p>
<p>vs. in-store announcements. Hence, we would use 0.05/3 � 0.017 as our criterion
for significance. Thus, to reject the null hypothesis that the two population means
</p>
<p>are equal, the p-value would have to be smaller than 0.017 (instead of 0.05!). The
Bonferroni adjustment is a very strict way of maintaining the familywise error rate.
</p>
<p>6.6 Understanding One-Way ANOVA 187</p>
<p/>
</div>
<div class="page"><p/>
<p>However, there is a trade-off between controlling the familywise error rate and
</p>
<p>increasing the type II error. By reducing the type I error rate, the type II error
</p>
<p>increases. Hence the statistical power decreases, potentially causing us to miss
</p>
<p>significant effects in the population.
</p>
<p>The good news is that there are alternatives to the Bonferroni method. The bad
</p>
<p>news is that there are numerous types of post hoc tests&mdash;Stata provides no less than
</p>
<p>nine such methods! All these post hoc tests are based on different assumptions and
</p>
<p>designed for different purposes, whose details are clearly beyond the scope of this
</p>
<p>book.14
</p>
<p>The most widely used post hoc test in market research is Tukey&rsquo;s honestly
</p>
<p>significant difference test, often simply referred to as Tukey&rsquo;s method. Tukey&rsquo;s
method is a very versatile test controlling for type I error, but is limited in terms of
</p>
<p>statistical power (Everitt and Skrondal 2010). The test divides the difference
</p>
<p>between the largest and smallest pairs of means by the data&rsquo;s standard error that
</p>
<p>combines all possible pairwise differences and produces a value called Tukey&rsquo;s
statistic. Where Tukey&rsquo;s statistic is larger than the critical value obtained from a
normal distribution, the pairwise differences are rendered statistically significant.
</p>
<p>Tukey&rsquo;s method relies on two important requirements:
</p>
<p>1. they require an equal number of observations for each group (differences of only
</p>
<p>a few observations are not problematic, though), and
</p>
<p>2. they assume that the population variances are equal.
</p>
<p>Alternative post hoc tests are available if these requirements are not met. When
</p>
<p>sample sizes clearly differ, we can draw on Scheffé&rsquo;s method, which is conservative
by nature and thus has low statistical power. Alternatively, we can use Dunnett&rsquo;s
method, which is useful when multiple pairwise comparisons (i.e., multiple treat-
ment groups) are made with reference to a single control group. This is standard in
</p>
<p>experiments that distinguish between control and treatment groups, as is often
</p>
<p>encountered in marketing research.
</p>
<p>Post hoc tests thus facilitate pairwise comparisons between groups while
</p>
<p>maintaining the familywise error rate. However, they do not allow for making
</p>
<p>statements regarding the strength of a factor variable&rsquo;s effects on the dependent
</p>
<p>variable. We can only do this after calculating the effect sizes, which we will
</p>
<p>do next.
</p>
<p>6.6.5 Measure the Strength of the Effects
</p>
<p>We can compute the η2 (the eta-squared) coefficient to determine the strength of
</p>
<p>the effect (also referred to as the effect size) that the factor variable exerts on the
dependent variable. The eta squared is the ratio of the between-group variation
</p>
<p>14The Stata help contrast function provides an overview and references.
</p>
<p>188 6 Hypothesis Testing &amp; ANOVA</p>
<p/>
</div>
<div class="page"><p/>
<p>(SSB) to the total variation (SST) and therefore indicates the variance accounted for
by the sample data. Since η2 is equal to the coefficient of determination (R2), known
from regression analysis (Chap. 7), Stata refers to it as R-squared in the output.
</p>
<p>η
2 can take on values between 0 and 1. If all groups have the samemean value, and
</p>
<p>we can thus assume that the factor has no influence on the dependent variable, η2
</p>
<p>is 0. Conversely, a high value implies that the factor exerts a strong influence on the
</p>
<p>dependent variable. In our example, η2 is:
</p>
<p>η2 &frac14; SSB
SST
</p>
<p>&frac14; 273:80
584
</p>
<p>� 0:469
</p>
<p>The outcome indicates that 46.9% of the total variation in sales is explained by the
</p>
<p>promotion campaigns. The η2 is often criticized as being too high for small sample
</p>
<p>sizes of 50 or less. We can compute ω2 (pronounced omega-squared), which
</p>
<p>corresponds to the Adjusted R2 from regression analysis (Chap. 7), to compensate
for small sample sizes:
</p>
<p>ω2 &frac14; SSB � k � 1&eth; &THORN; �MSW
SST &thorn;MSW
</p>
<p>&frac14; 273:80� 3� 1&eth; &THORN; � 11:916
584&thorn; 11:916 � 0:421
</p>
<p>This result indicates that 42.1% of the total variation in sales is accounted for by the
</p>
<p>promotion campaigns. Generally, you should use ω2 for n � 50 and η2 for n &gt; 50.
It is difficult to provide firm rules of thumb regarding which values are appro-
</p>
<p>priate for η2 or ω2, as this varies from research area to research area. However, since
</p>
<p>the η2 resembles Pearson&rsquo;s correlation coefficient (Chap. 7), we follow the
</p>
<p>suggestions provided in Chap. 7. Thus, we can consider values below 0.30 weak,
</p>
<p>values from 0.31 to 0.49 moderate, and values of 0.50 and higher strong.
</p>
<p>6.6.6 Interpret the Results and Conclude
</p>
<p>Just as in any other type of analysis, the final step is to interpret the results. Based on
</p>
<p>our results, we conclude that not all promotional activities have the same effect on
</p>
<p>sales. An analysis of the strength of the effects revealed that this association is
</p>
<p>moderate.
</p>
<p>6.6.7 Plotting the Results (Optional)
</p>
<p>In a final step, we can plot the estimated means of the dependent variable across the
</p>
<p>different samples. We could, for example, plot the mean of the sales across the
</p>
<p>stores with different types of promotion campaigns (i.e., point of sales display, free
</p>
<p>tasting stand or in-store display). When plotting the estimated group means, it is
</p>
<p>common to show the confidence interval, which is the interval within which the
mean estimate falls with a certain probability (e.g., 95%). In this way, we can see
</p>
<p>whether the mean of the outcome variable across the different groups differ
</p>
<p>6.6 Understanding One-Way ANOVA 189</p>
<p/>
</div>
<div class="page"><p/>
<p>significantly or not without examining the numbers in the table. We will illustrate
</p>
<p>this optional step in the case study later in this chapter.
</p>
<p>6.7 Going Beyond One-Way ANOVA: The Two-Way ANOVA
</p>
<p>A logical extension of the one-way ANOVA is to add a second factor variable to the
</p>
<p>analysis. For example, we could assume that, in addition to the different promotion
</p>
<p>campaigns, management also varied the type of service provided by offering either
</p>
<p>self-service or personal service (see column &ldquo;Service type&rdquo; in Table 6.1). The
</p>
<p>two-way ANOVA is similar to the one-way ANOVA, except that the inclusion of
</p>
<p>a second factor variable creates additional types of variation. Specifically, we need
</p>
<p>to account for two types of between-group variations:
</p>
<p>1. the between-group variation in factor variable 1 (i.e., promotion campaigns), and
</p>
<p>2. the between-group variation in factor variable 2 (i.e., service type).
</p>
<p>In its simplest form, the two-way ANOVA assumes that these factor variables
</p>
<p>are unrelated. However, in market research applications, this is rarely so, thereby
</p>
<p>requiring us to consider related factors. When we take two related factor variables
into account, we not only have to consider each factor variable&rsquo;s direct effect (also
</p>
<p>called themain effect) on the dependent variable, but also their interaction effect.
</p>
<p>Conceptually, an interaction effect is an additional effect due to combining two
</p>
<p>(or more) factors. Most importantly, this extra effect cannot be observed when
</p>
<p>considering each of the factor variables separately and thus reflects a concept
</p>
<p>known as synergy. There are many examples in everyday life where the whole is
</p>
<p>more than simply the sum of its parts as we know from cocktail drinks, music, and
</p>
<p>paintings. For an entertaining example of interaction, see Box 6.7.
</p>
<p>In our example, the free tasting stand might be the best promotion campaign
</p>
<p>when studied separately, but it could well be that when combined with personal
</p>
<p>service, the point of sale display produces higher sales. A significant interaction
</p>
<p>effect indicates that the combination of the two factor variables results in higher
</p>
<p>(or lower) sales than when each factor variable is considered separately. The
</p>
<p>computation of these effects, as well as a discussion of other technical aspects,
</p>
<p>lies beyond the scope of this book, but are discussed in the Web Appendix (!
Downloads).
</p>
<p>Table 6.5 provides an overview of the steps involved when carrying out the
</p>
<p>following tests in Stata: One-sample t-test, paired samples t-test, independent
samples t-test, and the one-way ANOVA. Owing to data limitations to accommo-
date all types of parametric tests in this chapter by means of Oddjobs Airways, we
</p>
<p>will use data from the case study in the theory section to illustrate the Stata
</p>
<p>commands. This data restriction applies only to this chapter.
</p>
<p>190 6 Hypothesis Testing &amp; ANOVA</p>
<p/>
</div>
<div class="page"><p/>
<p>Box 6.7 A Different Type of Interaction
</p>
<p>http://tinyurl.com/interact-coke
</p>
<p>Table 6.5 Steps involved in carrying out one, two, or more group comparisons with Stata
</p>
<p>Theory Action
</p>
<p>One-sample t-testa
</p>
<p>Formulate the hypothesis:
</p>
<p>Formulate the study&rsquo;s hypothesis: For example:
</p>
<p>H0: μ&frac14; #b
H1: μ 6&frac14; #
</p>
<p>Choose the significance level:
</p>
<p>Usually, α is set to 0.05, but:
if you want to be conservative, α is set to 0.01,
and:
in exploratory studies, α is set to 0.10. We
choose a significance level of 0.05.
</p>
<p>Select an appropriate test:
</p>
<p>What is the testing situation? Determine the fixed value again which that
you are comparing.
</p>
<p>Is the test variable measured on an interval or
ratio scale?
</p>
<p>Check Chap. 3 to determine the measurement
level of the variables.
</p>
<p>Are the observations independent? Consult Chap. 3 to determine whether the
observations are independent.
</p>
<p>Is the test variable normally distributed or is
n&gt;30 and are the group variances the same?
</p>
<p>Check for normality
</p>
<p>Carry out the Shapiro-Wilk normality test. Go
to ► Statistics ► Summaries, tables, and tests
►Distributional plots and tests ►Shapiro-
Wilk normality test. Select the test variable
outcome1 under Variables: and click on OK.
A p-value below 0.05 indicates non-normality.
</p>
<p>swilk outcome1
</p>
<p>(continued)
</p>
<p>6.7 Going Beyond One-Way ANOVA: The Two-Way ANOVA 191</p>
<p/>
<div class="annotation"><a href="http://tinyurl.com/interact-coke">http://tinyurl.com/interact-coke</a></div>
</div>
<div class="page"><p/>
<p>Table 6.5 (continued)
</p>
<p>Theory Action
</p>
<p>Specify the type of t-test Select the one-sample t-test.
</p>
<p>Is the test one or two-sided? Determine the region of rejection.
</p>
<p>Calculate the test statistic:
</p>
<p>Specify the test variable and the fixed value Go to ► Statistics ► Summaries, tables, and
tests ► Classical tests of hypotheses ► t test
(mean-comparison test). Select the first option
One-sample from the dialog box and specify
the test variable outcome1 under the Variable
name:. Next, enter the hypothetical mean
under Hypothesized mean:, choose the
confidence interval 95, which equates to a
statistical significance at&lt; 0.05 and clickOK.
</p>
<p>ttest outcome1&frac14;&frac14;#
Interpret the results:
</p>
<p>Look at the test results For two-sided tests: compare the p-value
under Ha: mean(diff) !&frac14; 0 with 0.05 and
decide whether the null hypothesis is
supported. The p-value under Pr(|T| &gt; |t|)
should be lower than 0.05 to reject the null
hypothesis.
</p>
<p>For one-sided tests, look at either Ha: mean
(diff) &lt; 0 (left-sided) or Ha: mean(diff) &gt;
0 (right-sided).
</p>
<p>What is your conclusion? Reject the null hypothesis that the population
mean of the outcome variable outcome1 is
equal to the hypothetical known parameter
against which you compare (i.e., #) if the p-
value is lower than 0.05.
</p>
<p>Paired samples t-test
</p>
<p>Formulate the hypothesis:
</p>
<p>Formulate the study&rsquo;s hypothesis: For example:
</p>
<p>H0: μ1&frac14; μ2
H1: μ1 6&frac14; μ2
</p>
<p>Choose the significance level:
</p>
<p>Usually, α is set to 0.05, but:
if you want to be conservative, α is set to 0.01,
and:
in exploratory studies, α is set to 0.10. We
choose a significance level of 0.05.
</p>
<p>Select an appropriate test:
</p>
<p>What is the testing situation? Determine the number of groups you are
comparing.
</p>
<p>Are the test variables measured on an interval
or ratio scale?
</p>
<p>Check Chap. 3 to determine the measurement
level of the variables.
</p>
<p>Are the observations dependent? Next, consult Chap. 3 to determine whether
the observations are independent.
</p>
<p>Check for normality
</p>
<p>(continued)
</p>
<p>192 6 Hypothesis Testing &amp; ANOVA</p>
<p/>
</div>
<div class="page"><p/>
<p>Table 6.5 (continued)
</p>
<p>Theory Action
</p>
<p>Are the test variables normally distributed or
is n &gt; 30 in each of the groups and are the
group variances the same?
</p>
<p>Run the Shapiro-Wilk normality test. Go to►
Statistics ► Summaries, tables, and tests
►Distributional plots and tests ►Shapiro-
Wilk normality test. Select the test variable
outcome1 under Variables. Next, under the
tab by/if/in tick the box Repeat command by
groups, specify the grouping variable
groupvar under Variables that define
groups: and click on OK. A p-value below
0.05 indicates non-normality.
</p>
<p>by groupvar,c sort: swilk outcome1
</p>
<p>Check for equality of variances assumptions
</p>
<p>To perform Levene&rsquo;s test, go to► Statistics►
Classical tests of hypotheses ► Robust equal-
variance test. Specify the dependent variable
under Variable outcome1, and the grouping
variable groupvar under Variable defining
comparison groups and click OK. To
validate the equality of variances, the
assumption p-values should lie above 0.05 for
W0, W50, and W10.
</p>
<p>robvar outcome1, by(groupvar)
</p>
<p>Specify the type of t-test The data appear to be normally distributed
with equal group variances. We can now
proceed with the paired sample t-test.
</p>
<p>Is the test one or two-sided? Determine the region of rejection.
</p>
<p>Calculate the test statistic:
</p>
<p>Select the paired test variables Go to ► Statistics ► Summaries, tables, and
tests ► Classical tests of hypotheses ► t test
(mean-comparison test). Select the fourth
option Paired from the dialog box and specify
the test variable &ldquo;variable1&rdquo; under the First
Variable. Next, enter the second comparison
group &ldquo;variable2&rdquo; under Second variable,
choose the confidence interval 95, which
equates to a statistical significance of
α &frac14; 0.05 and click OK.
ttest variable1&frac14;&frac14;variables2d
</p>
<p>Interpret the results:
</p>
<p>Look at the test results For two-sided tests: compare the p-value under
Ha: mean(diff) !&frac14; 0 with 0.05 and decide
whether the null hypothesis is supported. The
p-value under Pr(|T|&gt; |t|) should be lower than
0.05 to reject the null hypothesis.
</p>
<p>For one-sided tests, look at either Ha: mean
(diff) &lt; 0 (left-sided) or Ha: mean(diff) &gt;
0 (right-sided).
</p>
<p>What is your conclusion? Reject the null hypothesis if the p-value is
lower than 0.05.
</p>
<p>(continued)
</p>
<p>6.7 Going Beyond One-Way ANOVA: The Two-Way ANOVA 193</p>
<p/>
</div>
<div class="page"><p/>
<p>Table 6.5 (continued)
</p>
<p>Theory Action
</p>
<p>Independent sample t-test
</p>
<p>Formulate the hypothesis:
</p>
<p>Formulate the study&rsquo;s hypothesis: For example:
</p>
<p>H0: μ1&frac14; μ2
H1: μ1 6&frac14; μ2
</p>
<p>Choose the significance level:
</p>
<p>Usually, α is set to 0.05, but:
if you want to be conservative, α is set to 0.01,
and:
in exploratory studies, α is set to 0.10. We
choose a significance level of 0.05.
</p>
<p>Select an appropriate test:
</p>
<p>What is the testing situation? Determine the number of groups you are
comparing.
</p>
<p>Are the test variables measured on an interval
or ratio scale?
</p>
<p>Check Chap. 3 to determine the measurement
level of the variables.
</p>
<p>Are the observations dependent? Next, consult Chap. 3 to determine whether
the observations are independent.
</p>
<p>Are the test variables normally distributed or
is n &gt; 30 in each of the groups and are the
group variances the same?
</p>
<p>Check for normality
</p>
<p>Run the Shapiro-Wilk normality test. Go to►
Statistics► Summaries, tables, and tests►
Distributional plots and tests► Shapiro-Wilk
normality test. Select the test variable
overall_sat under Variables. Next, under the
tab by/if/in tick the box Repeat command by
groups, specify the grouping variable gender
under Variables that define groups and click
onOK. A p-value below 0.05 indicates
non-normality.
</p>
<p>by gender, sort: swilk
overall_sat
</p>
<p>Check for equality of variances assumptions
</p>
<p>Next, to perform Levene&rsquo;s test, go to ►
Statistics ► Classical tests of hypotheses
►Robust equal-variance test. Specify the
dependent variable overall_sat under
Variable: and the variable gender under
Variable defining comparison groups: aand
clickOK. The p-values ofW0,W50 andW10
should be above 0.05 to validate the equality
of the variances assumption.
</p>
<p>robvar overall_sat, by(gender)
</p>
<p>Specify the type of t-test The data appear to be normally distributed
with equal group variances. We can now
proceed with the two-sample t-test.
</p>
<p>Is the test one or two-sided? Determine the region of rejection.
</p>
<p>(continued)
</p>
<p>194 6 Hypothesis Testing &amp; ANOVA</p>
<p/>
</div>
<div class="page"><p/>
<p>Table 6.5 (continued)
</p>
<p>Theory Action
</p>
<p>Calculate the test statistic:
</p>
<p>Select the test variable and the grouping
variable
</p>
<p>Go to ► Statistics ► Summaries, tables, and
tests ► Classical tests of hypotheses ► t test
(mean-comparison test). Select the second
option Two-sample using groups from the
dialog box and specify the test variable
overall_sat under the Variable name. Next,
enter the variable gender under Group
variable name. Select the confidence interval
95, which equates to a statistical significance
of α &frac14; 0.05 and then click OK.
ttest overall_sat, by(gender)
</p>
<p>Interpret the results:
</p>
<p>Look at the test results For two-sided tests: Compare the p-value under
Ha: mean(diff)! &frac14; 0 with 0.05 and decide
whether the null hypothesis is supported. The
p-value under Pr(|T|&gt; |t|) should be lower than
0.05 to reject the null hypothesis.
</p>
<p>For one-sided tests, look either at Ha: mean
(diff) &lt; 0 (left-sided) or Ha: mean(diff) &gt;
0 (right-sided).
</p>
<p>What is your conclusion? Reject the null hypothesis that the population
mean of the overall satisfaction score among
female travelers is equal to the population
overall mean satisfaction score of male
traverlers if the p-value is lower than 0.05.
</p>
<p>One-way ANOVA
</p>
<p>Formulate the hypothesis:
</p>
<p>Formulate the study&rsquo;s hypothesis: For example:
</p>
<p>H0: μ1&frac14; μ2&frac14; μ3
H1: At least two of the population means are
different.
</p>
<p>Choose the significance level:
</p>
<p>Usually, α is set to 0.05, but:
if you want to be conservative, α is set to 0.01,
and:
in exploratory studies, α is set to 0.10. We
choose a significance level of 0.05.
</p>
<p>Select an appropriate test:
</p>
<p>What is the testing situation? Determine the number of groups you are
comparing.
</p>
<p>Are there at least 20 observations per group? Check Chap. 5 to determine the sample size in
each group.
</p>
<p>Is the dependent variable measured on an
interval or ratio scale?
</p>
<p>Determine the type of test that you need to use
for your analyses by checking the underlying
assumptions first. Check Chap. 3 to determine
the measurement level of the variables.
</p>
<p>(continued)
</p>
<p>6.7 Going Beyond One-Way ANOVA: The Two-Way ANOVA 195</p>
<p/>
</div>
<div class="page"><p/>
<p>Table 6.5 (continued)
</p>
<p>Theory Action
</p>
<p>Are the observations independent? Next, consult Chap. 3 to determine whether
the observations are independent.
</p>
<p>Is the test variable normally distributed or is
n larger than 30 per group and are the group
variances the same?
</p>
<p>Check for normality
</p>
<p>Carry out the Shapiro-Wilk normality test. Go
to ► Statistics ► Summaries, tables, and tests
►Distributional plots and tests ►Shapiro-
Wilk normality test. Select the test variable
overall_sat under Variables. Next, under the
tab by/if/in tick the box Repeat command by
groups, specify the grouping variable status
under Variables that define groups and click
onOK. Values (V) larger than 1 with p-values
below 0.05 indicate non-normality.
</p>
<p>by status, sort: swilk
overall_sat
</p>
<p>Check for Equality of Variances Assumption
</p>
<p>Perform Levene&rsquo;s test. Go to ► Statistics ►
Classical tests of hypotheses ►Robust equal-
variance test. Specify the dependent variable
overall_sat under Variable: aand the
grouping variable status under Variable
defining comparison groups: aand then click
on OK. The p-values of W0, W50 and W10
should be above 0.05 to validate the equality
of the variances assumption.
</p>
<p>robvar overall_sat, by (status)
</p>
<p>Select the type of the test Now that the assumption of normality and
equality of the variance are met, proceed with
the one-way ANOVA analysis.
</p>
<p>Calculate the test statistic:
</p>
<p>Specify the dependent variable and the factor
(grouping variable)
</p>
<p>Go to ►Statistics►Linear models and related
ANOVA/MANOVA ►Analysis of variance
and covariance. Specify the dependent
variable overall_sat under the Dependent
variable: aand the variable status under the
Model:. Next, select the Partial Sums of
squares and then click on OK.
</p>
<p>anova overall_sat status
</p>
<p>Interpret the results:
</p>
<p>Look at the test results Compare the p-value under Model with the
significance level. The p-value should be
lower than 0.05 to reject the null hypothesis.
</p>
<p>Carry out pairwise comparisons You can only carry out post hoc tests after you
have carried out the ANOVA analysis. After
the ANOVA analysis, go to ► Statistics ►
Postestimation. In the next window that
follows, go to ►Tests, contrasts, and
comparisons of parameter estimates ►
Pairwise comparisons and click on Launch.
</p>
<p>(continued)
</p>
<p>196 6 Hypothesis Testing &amp; ANOVA</p>
<p/>
</div>
<div class="page"><p/>
<p>Table 6.5 (continued)
</p>
<p>Theory Action
</p>
<p>Select the variable status under Factor terms
to compute pairwise comparisons for: aand
select Tukey&rsquo;s method option in theMultiple
comparisons box. Finally, go to the
Reporting tab, tick the box Show effects
table with confidence intervals and p-values
</p>
<p>and the box Sort the margins/differences in
each term and click on OK. Now check
whether the pairwise mean comparisons differ
significantly if the p-values (under P&gt;|t|) are
lower than 0.05.
</p>
<p>pwcompare status, effects sort
mcompare (tukey)
</p>
<p>If unequal variances are assumed, use
Scheffé&rsquo;s method.
</p>
<p>Look at the strength of the effects Check for the strengths of the effects under
R-squared and Adjusted R-squared in the
output.
</p>
<p>What is your conclusion? Based on pairwise comparisons: Check which
pairs differ significantly from each other. If the
p-values tied to the pairwise mean
comparisons are &lt; 0.05, reject the null
hypothesis that the mean comparisons
between the two groups are equal.
</p>
<p>Based on the output from the one-way
ANOVA table, reject the null hypothesis that
at least two population means are equal if the
p-value is lower than 0.05.
</p>
<p>Plotting the ANOVA results (optional) To plot the results from the one-way ANOVA,
go to ► Statistics ► Postestimation. In the
next window that follows, go to ► Marginal
analysis ► Marginal means and marginal
effects, fundamental analyses and click on
Launch. Enter the variable status under
Covariate, tick the box Draw profile plots of
results and then click on OK.
</p>
<p>aNote that the Oddjob Airways dataset is not well suited to perform (1) the one-sample t-test and
(2) the paired samples t-test. We therefore use hypothetical variables to illustrate the Stata
commands
b# &frac14; refers to a hypothetical constant (number) against which you want to compare
cOutcome1 refers to the dependent variable, with groupvar representing the two groups with and
without treatment
dVariable1 and Variable2 represent the outcome variable with and without treatment
</p>
<p>6.7 Going Beyond One-Way ANOVA: The Two-Way ANOVA 197</p>
<p/>
</div>
<div class="page"><p/>
<p>6.8 Example
</p>
<p>Let&rsquo;s now turn to the Oddjob Airways case study and apply the materials discussed in
</p>
<p>this chapter. Our aim is to identify the factors that influence customers&rsquo; overall price/
</p>
<p>performance satisfaction with the airline and explore the relevant target groups for
</p>
<p>future advertising campaigns. Based on discussions with the Oddjob Airways man-
</p>
<p>agement, answering the following three research questions will help achieve this aim:
</p>
<p>1. Does the overall price/performance satisfaction differ by gender?
</p>
<p>2. Does the overall price/performance satisfaction differ according to the traveler&rsquo;s
</p>
<p>status?
</p>
<p>3. Does the impact of the traveler&rsquo;s status on the overall price/performance satis-
</p>
<p>faction depend on the different levels of the variable gender?
</p>
<p>The following variables (variable names in parentheses) from the Oddjob Airways
</p>
<p>dataset ( Web Appendix ! Downloads) are central to this example:
</p>
<p>&ndash; overall price/performance satisfaction (overall_sat),
&ndash; respondent&rsquo;s gender (gender), and
&ndash; traveler&rsquo;s status (status).
</p>
<p>6.8.1 Independent Samples t-Test
</p>
<p>6.8.1.1 Formulate Hypothesis
We start by formulating a non-directional hypothesis. The null hypothesis of the
</p>
<p>first research question is that the overall price/performance satisfaction means of
</p>
<p>male and female travelers are the same (H0), while the alternative hypothesis (H1)
</p>
<p>expects the opposite.
</p>
<p>6.8.1.2 Choose the Significant Level
Next, we decide to use a significance level (α) of 0.05, which means that we allow a
</p>
<p>maximum chance of 5% of mistakenly rejecting a true null hypothesis.
</p>
<p>6.8.1.3 Select an Appropriate Test
We move to the next step to determine the type of test, which involves assessing the
</p>
<p>testing situation, the nature of the measurements, checking the assumptions, and
</p>
<p>selecting the region of rejection. We start by defining the testing situation of our
</p>
<p>analysis, which concerns comparing the mean overall price/performance satisfaction
</p>
<p>scores (measured on a ratio scale) of male and female travelers. In our example, we
</p>
<p>know that the sample is a random subset of the population and we also know that
</p>
<p>other respondents&rsquo; responses do not influence those of the respondents (i.e., they are
</p>
<p>independent). Next, we need to check if the dependent variable overall_sat is
normally distributed between male and female travelers (i.e., normality assumption)
</p>
<p>and whether male travelers show the same variance in their overall price satisfaction
</p>
<p>as female travelers (i.e., equality of variance assumption). We use the Shapiro-Wilk
</p>
<p>198 6 Hypothesis Testing &amp; ANOVA</p>
<p/>
</div>
<div class="page"><p/>
<p>test for the normality test. Go to ► Statistics ► Summaries, tables, and tests ►
</p>
<p>Distributional plots and tests► Shapiro-Wilk normality test. In theMain dialog box
</p>
<p>that follows (Fig. 6.6), select the variable overall_sat under Variables. Next, in the
by/if/in tab, tick the box Repeat command by groups and enter the variable gender
under Variables that define groups and then click on OK.
</p>
<p>Table 6.6 displays the Stata output that follows. Stata reports the Shapiro-Wilk
</p>
<p>test statistic in its original version (W) and scaled version (V) with their
</p>
<p>corresponding z-values (z) and p-values under (Prob &gt; z). Table 6.6 shows that
</p>
<p>Table 6.6 Shapiro-Wilk normality test output
</p>
<p>by gender, sort: swilk overall_sat
</p>
<p>--------------------------------------------------------------------------------------
---
-&gt; gender = female
</p>
<p>Shapiro-Wilk W test for normal data
</p>
<p>Variable |        Obs       W           V         z       Prob&gt;z
-------------+------------------------------------------------------
overall_sat |        280    0.98357      3.293     2.788    0.00265
</p>
<p>--------------------------------------------------------------------------------------
---
-&gt; gender = male
</p>
<p>Shapiro-Wilk W test for normal data
</p>
<p>Variable |        Obs       W           V         z       Prob&gt;z
-------------+------------------------------------------------------
overall_sat |        785    0.99050      4.805     3.848    0.00006
</p>
<p>Fig. 6.6 Shapiro-Wilk normality test dialog box
</p>
<p>6.8 Example 199</p>
<p/>
</div>
<div class="page"><p/>
<p>the p-values under (Prob &gt; z) of both female (0.00265) and male (0.00006)
samples are smaller than 0.05, indicating that the normality assumption is violated.
</p>
<p>Next, we need to check for the equality of the variances assumption. Go to ►
</p>
<p>Statistics► Summaries, tables, and tests► Classical tests of hypotheses► Robust
</p>
<p>equal-variance test. Enter the dependent variable overall_sat into the Variable box
and gender in the Variable defining the comparison groups box and click onOK.
</p>
<p>As you can see in Table 6.7, Stata calculates Levene&rsquo;s test for the mean (W0), the
</p>
<p>median (W50), and for the 10% trimmed mean replacement (W10), with their
</p>
<p>corresponding p-values (Pr &gt; F) at the bottom of the table. We can see that the p-
values of W0, W50, and W10 are higher than 0.05 and, thus, not significant. This
</p>
<p>means that there is no reason to think that the variances for male and female travelers
</p>
<p>are different. Overall, we conclude that the data are not normally distributed, but that
</p>
<p>the variances across the male and the female groups are equal, allowing us to utilize a
</p>
<p>parametric test for group differences, because these are robust against violations of
</p>
<p>normality when sample sizes are larger than 30. Having checked the underlying
</p>
<p>assumptions, we can now decide on the region of rejection of our study&rsquo;s main
</p>
<p>research questions. This relates does, of course, relate to our study&rsquo;s main hypothesis,
</p>
<p>which was formulated as non-directional, implying a two-sided test.
</p>
<p>6.8.1.4 Calculate the Test Statistic and Make the Test Decision
In the next step, and given that the equal variances assumption was tenable, we
</p>
<p>decide to use an independent samples t-test. To run this test, go to ► Statistics ►
Summaries, tables, and tests ► Classical tests of hypotheses ► t test (mean-
</p>
<p>comparison test). In the Main dialog box that follows (Fig. 6.7), select the second
</p>
<p>option Two-sample using groups from the dialog box and specify the outcome
</p>
<p>variable (overall_sat) under Variable name. Next, enter the grouping variable
gender under Group variable name. Select the confidence interval 95, which
equates to a significance level of 5% and then click OK.
</p>
<p>Table 6.7 Levene&rsquo;s test output
</p>
<p>robvar overall_sat, by(gender)
</p>
<p>| Summary of Overall, I am satisfied
| with the price performance ratio of
|           Oddjob Airways.
</p>
<p>Gender | Mean   Std. Dev.       Freq.
------------+------------------------------------
</p>
<p>female |         4.5   1.6461098         280
male |   4.2369427   1.6130561         785
</p>
<p>------------+------------------------------------
Total |   4.3061033   1.6251693       1,065
</p>
<p>W0  =  0.41763753   df(1, 1063)     Pr &gt; F = 0.51825772
W50 =  0.23527779   df(1, 1063)     Pr &gt; F = 0.62773769
W10 =  0.02419285   df(1, 1063)     Pr &gt; F = 0.87642474
</p>
<p>200 6 Hypothesis Testing &amp; ANOVA</p>
<p/>
</div>
<div class="page"><p/>
<p>The output that follows (Table 6.8) provides diverse information on the male and
</p>
<p>female travelers, including their mean overall price/performance satisfaction, the
</p>
<p>standard error, standard deviation, and the 95% confidence intervals (see Chap. 5).
</p>
<p>At the bottom of the table, Stata shows the results of the t-test for both one-sided
and two-sided tests, and leaves it to the researcher to decide which results
</p>
<p>to interpret. In our case, our main hypothesis was formulated non-directionally
</p>
<p>Table 6.8 Output of the independent sample t-test in Stata
</p>
<p>ttest overall_sat, by(gender)
</p>
<p>Two-sample t test with equal variances
------------------------------------------------------------------------------
</p>
<p>Group |     Obs        Mean    Std. Err.   Std. Dev.   [95% Conf. Interval]
---------+--------------------------------------------------------------------
female |     280         4.5    .0983739     1.64611    4.306351    4.693649
male |     785    4.236943    .0575724    1.613056    4.123928    4.349957
</p>
<p>---------+--------------------------------------------------------------------
combined |   1,065    4.306103    .0497994    1.625169    4.208387    4.403819
---------+--------------------------------------------------------------------
</p>
<p>diff |         .2630573    .1128905                .0415438    .4845708
------------------------------------------------------------------------------
</p>
<p>diff = mean(female) - mean(male)                              t =   2.3302
Ho: diff = 0                          degrees of freedom =     1063
</p>
<p>Ha: diff &lt; 0                 Ha: diff != 0                 Ha: diff &gt; 0
Pr(T &lt; t) = 0.9900         Pr(|T| &gt; |t|) = 0.0200          Pr(T &gt; t) = 0.0100
</p>
<p>Fig. 6.7 Dialog box, independent sample t-test
</p>
<p>6.8 Example 201</p>
<p/>
</div>
<div class="page"><p/>
<p>(i.e., two-sided) and we therefore focus on the test results under Ha: diff! &frac14; 0,
which are based on the two-tailed significance level. You can ignore the other test
</p>
<p>results, since these tests are for directional hypotheses (Ha: diff &lt; 0 for a one-(left)
sided hypothesis and Ha: diff &gt; 0 for a one-(right) sided hypothesis).
</p>
<p>6.8.1.5 Interpret the Results
When comparing the p-value under Pr(|T| &gt; |t|) with the significance level, we
learn that the p-value (0.020) is smaller than the significance level (0.05). Hence,
we conclude that that the overall price satisfaction differs significantly between
</p>
<p>female and male travelers.
</p>
<p>If the normality and equality of the variance assumptions were violated, and
</p>
<p>the sample sizes were small (i.e., &lt; 30 observations), the Mann-Whitney U
test, which Stata refers to as the Wilcoxon signed rank-sum test, should have
</p>
<p>been performed. This is the non-parametric counterpart of the independent
</p>
<p>sample t-test. To obtain the Wilcoxon signed rank-sum test, go to►Statistics
► Summaries, tables, and tests ► Non-parametric tests of hypotheses ►
</p>
<p>Wilcoxon rank-sum test. In theMain dialog box that follows, enter the depen-
</p>
<p>dent variable overall_sat underVariable and the variable gender underGroup-
ing variable. Stata lists the number of observations for female and male
</p>
<p>travelers separately, followed by their corresponding observed and expected
</p>
<p>rank sums. The test statistic can be found at the bottomof the table under theH0.
</p>
<p>The p-value under Prob&gt; |z| is 0.0125 and, thus, smaller than 0.05. This result
indicates that the medians of male and female travelers differ significantly.
</p>
<p>6.8.2 One-way ANOVA
</p>
<p>In the second research question, we examine whether customers&rsquo; membership
</p>
<p>status influences their overall price/performance satisfaction (i.e., overall_sat)
with Oddjob Airways. The membership can have three forms: Blue, Silver, and
Gold. Again, we start by formulating a null hypothesis that is again non-directional
in nature, expecting that the mean of the overall price/performance satisfaction is
</p>
<p>the same between the status groups, while the alternative hypothesis states that at
</p>
<p>least two status groups differ. Next, we decide to use a significance level (α) of 0.05.
</p>
<p>We have already established that a comparison of three or more groups involves a
</p>
<p>one-way ANOVA and we therefore follow the steps as indicated in Fig. 6.4.
</p>
<p>6.8.2.1 Check the Assumptions
In checking the assumptions, we already know that the sample is a random subset of
</p>
<p>the population and we also know that other respondents&rsquo; responses do not influence
</p>
<p>those of the respondents (i.e., they are independent). Next, we check the normality
</p>
<p>and equality of the variance assumptions by focusing directly on Stata&rsquo;s output
</p>
<p>tables (see previous research question for the menu options). We start with the
</p>
<p>results of the Shapiro-Wilk test displayed in Table 6.9.
</p>
<p>202 6 Hypothesis Testing &amp; ANOVA</p>
<p/>
</div>
<div class="page"><p/>
<p>We can see that the Shapiro-Wilk test produces significant effects for Blue
members (Prob &gt; z &frac14; 0.00000), but not for Silver members (Prob &gt; z &frac14; 0.37021)
and Gold members (Prob &gt; z &frac14; 0.39927). This means that in the three different
samples, the overall price satisfaction is normally distributed in the Silver and Gold
groups, but not in the Blue group. As we mentioned previously, the ANOVA is
robust to violations from normality when samples are greater than 30, meaning that
</p>
<p>we can move to the next step to test the equality of variance assumptions, even
</p>
<p>though one of our samples violates the normality assumption. The output of
</p>
<p>Levene&rsquo;s test is shown in Table 6.9.
</p>
<p>As we can see in Table 6.10, the p-values of W0, W50, and W10 under PR &gt;
F are higher than 0.05 (thus not significant), which means that the variances
</p>
<p>between travelers with different statuses are the same. Overall, we conclude that
</p>
<p>the equal variance assumption is tenable, we can therefore move ahead and test our
</p>
<p>study&rsquo;s second research question.
</p>
<p>6.8.2.2 Calculate the Test Statistic
To run an ANOVA, go to ► Statistics ► Linear models and related ► ANOVA/
</p>
<p>MANOVA ► Analysis of variance and covariance. In the dialog box that follows
</p>
<p>(Fig. 6.8), select overall_sat from the drop-down menu under Dependent variable
and status (status) from the drop-down menu under Model. Next, then click on
OK.
</p>
<p>Stata will produce the output as shown in Table 6.11. The top part of the table
</p>
<p>reports several measures of model fit, which we discuss in Box 6.6. Under Partial
</p>
<p>SS, Stata lists different types of variation. Model represents the between-group
</p>
<p>variation (SSB), whereas Residual indicates the within-group variation (SSW). Next
</p>
<p>Table 6.9 Stata ouput of the Shapiro-Wilk test
</p>
<p>by status, sort: swilk overall_sat
-------------------------------------------------------------------------
-&gt; status = Blue
</p>
<p>Shapiro-Wilk W test for normal data
</p>
<p>Variable |        Obs       W           V         z       Prob&gt;z
-------------+------------------------------------------------------
overall_sat |        677    0.98403      7.067     4.764    0.00000
</p>
<p>-------------------------------------------------------------------------
-&gt; status = Silver
</p>
<p>Shapiro-Wilk W test for normal data
</p>
<p>Variable |        Obs       W           V         z       Prob&gt;z
-------------+------------------------------------------------------
overall_sat |        245    0.99353      1.153  0.331    0.37021
</p>
<p>-------------------------------------------------------------------------
-&gt; status = Gold
</p>
<p>Shapiro-Wilk W test for normal data
</p>
<p>Variable |        Obs       W           V         z       Prob&gt;z
-------------+------------------------------------------------------
overall_sat |        143    0.98998      1.119     0.255    0.39927
</p>
<p>6.8 Example 203</p>
<p/>
</div>
<div class="page"><p/>
<p>to status, Stata lists this variable&rsquo;s partial contribution to the total variance. Given
</p>
<p>that the model has only one variable at this stage, the partial sums of squares
</p>
<p>explained by status (51.755064) is exactly the same as the partial sums of squares of
the model (51.755064).
</p>
<p>Fig. 6.8 ANOVA dialog box
</p>
<p>Table 6.10 Stata output of Levene&rsquo;s test
</p>
<p>robvar overall_sat, by(status)
</p>
<p>| Summary of Overall, I am satisfied
| with the price performance ratio of
</p>
<p>Traveler |           Oddjob Airways.
status |        Mean   Std. Dev.       Freq.
</p>
<p>------------+------------------------------------
Blue |   4.4726736   1.6411609         677
</p>
<p>Silver |   4.0326531   1.5599217         245
Gold |    3.986014   1.5563863         143
</p>
<p>------------+------------------------------------
Total |   4.3061033   1.6251693       1,065
</p>
<p>W0  =  0.90696260   df(2, 1062)     Pr &gt; F = 0.4040612
W50 =  0.06775398   df(2, 1062)     Pr &gt; F = 0.93449438
W10 =  0.88470222   df(2, 1062)     Pr &gt; F = 0.41314113
</p>
<p>204 6 Hypothesis Testing &amp; ANOVA</p>
<p/>
</div>
<div class="page"><p/>
<p>6.8.2.3 Make the Test Decision
Let&rsquo;s now focus on the F-test result with respect to the overall model. The model
has an F-value of 9.96, which yields a p-value of 0.0001 (see Prob&gt; F), suggesting
a statistically significant model.
</p>
<p>6.8.2.4 Carry Out Post Hoc Tests
Next, we carry out pairwise group comparisons using Tukey&rsquo;s method. In Stata, this
</p>
<p>is a post estimation command, which means that comparisons can only be carried
</p>
<p>out after estimating the ANOVA. To run Tukey&rsquo;s method, go to ► Statistics ►
</p>
<p>Postestimation. In the window that follows, go to ► Tests, contrasts, and
</p>
<p>comparisons of parameter estimates► Pairwise comparisons and click on Launch.
</p>
<p>In the dialog box that opens, select the variable status under Factor terms to
compute pairwise comparisons for and select the Tukey&rsquo;s method option from
</p>
<p>theMultiple comparisons drop-down menu. Next, go to the Reporting tab and first
</p>
<p>tick Specify additional tables (default is effects table with confidence intervals)
</p>
<p>and then tick Show effects table with confidence intervals and p-values. Finally, in
</p>
<p>the same window, tick the box Sort the margins/differences in each term, and then
</p>
<p>click on OK. This produces the following output as in Table 6.12.
</p>
<p>To check whether the means differ significantly, we need to inspect the p-values
under Tukey P &gt; |t|. The results in Table 6.12 indicate that the overall price/
performance satisfaction differs significantly between Gold and Blue members, as
well as between Silver and Blue members. The p-values of these pairwise
comparisons are respectively 0.003 and 0.001 and, thus, smaller than 0.05. In
</p>
<p>contrast, the pairwise mean difference between Gold and Silver members does
not differ significantly, as the p-value of 0.959 is above 0.05, indicating that
members from these two status groups share similar views about the overall
</p>
<p>price/performance satisfaction with Oddjob Airways.
</p>
<p>6.8.2.5 Measure the Strength of the Effects
The upper part of Table 6.11 reports the model fit. Besides the R-squared Stata
</p>
<p>reports the effect size η2 (0.0184). This means that differences in the travelers&rsquo;
</p>
<p>status explain 1.841% of the total variation in the overall price satisfaction. The ω2
</p>
<p>displayed under Adj R-squared is 0.0166.
</p>
<p>Table 6.11 One-way ANOVA
</p>
<p>anova overall_sat status
</p>
<p>Number of obs =      1,065    R-squared     =  0.0184
Root MSE      =    1.61165    Adj R-squared =  0.0166
</p>
<p>Source | Partial SS         df         MS        F    Prob&gt;F
-----------+----------------------------------------------------
</p>
<p>Model |  51.755064          2   25.877532      9.96  0.0001
status |  51.755064          2   25.877532      9.96  0.0001
</p>
<p>Residual |  2758.4553      1,062   2.5974155  
-----------+----------------------------------------------------
</p>
<p>Total |  2810.2103      1,064   2.6411751  
</p>
<p>6.8 Example 205</p>
<p/>
</div>
<div class="page"><p/>
<p>6.8.2.6 Interpret the Results
Overall, based on the outputs of the ANOVA in Tables 6.11 and 6.12, we conclude
</p>
<p>that:
</p>
<p>1. Gold and Bluemembers, as well as Silver and Bluemembers, differ significantly
in their mean overall price satisfaction.
</p>
<p>2. Membership status explains only a minimal share of the customers&rsquo; price
</p>
<p>satisfaction. Hence, other factors&mdash;presently not included in the model&mdash;explain
</p>
<p>the remaining variation in the outcome variable.
</p>
<p>6.8.2.7 Plot the Results
Next, we plot the results, but we first should save the estimated parameters. Note that
</p>
<p>these estimated parameters capture the instantaneous change in the overall satisfaction
</p>
<p>level in respect of every unit change in themembership status, also termed themarginal
effect (Greene 1997; Bartus 2005). Stata allows a fast and efficient way of saving the
estimated parameters from the ANOVA. To do so, go to► Statistics► Postestimation.
</p>
<p>In the dialog box that follows, go to ► Marginal analysis ► Marginal means and
</p>
<p>marginal effects, fundamental analyses and click on Launch. Enter the variable status
underCovariate, tick the boxDrawprofile plots of results and then click onOK. Stata
</p>
<p>will simultaneously produce Table 6.13 and the plot shown in Fig. 6.9.
</p>
<p>The plot depicts the predicted group means (listed in Table 6.13) surrounded by
</p>
<p>their confidence intervals, which the vertical lines through the dots in Fig. 6.9
</p>
<p>indicate. These intervals are very useful, because they immediately reveal whether
</p>
<p>the predicted group means differ significantly. More precisely, if the vertical bars
</p>
<p>do not overlap vertically with each other, we can say there is a significant differ-
ence. Overall, Fig. 6.9 indicates that there is a significant mean difference in the
</p>
<p>overall price satisfaction between Blue and Silver members and between Blue and
Gold members, but no significant difference between Silver and Gold members.
This is exactly what Tukey&rsquo;s pairwise comparison indicated in Table 6.12.
</p>
<p>Table 6.12 Output of Tukey&rsquo;s method
</p>
<p>pwcompare status, effects sort mcompare(tukey)  
</p>
<p>Pairwise comparisons of marginal linear predictions
Margins      : asbalanced
---------------------------
</p>
<p>|    Number of
|  Comparisons
</p>
<p>-------------+-------------
status |            3
</p>
<p>---------------------------
-------------------------------------------------------------------------------
</p>
<p>|                              Tukey                Tukey
|   Contrast   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
</p>
<p>----------------+--------------------------------------------------------------
status |
</p>
<p>Gold vs Blue  |  -.4866596   .1483253    -3.28   0.003    -.8347787   -.1385404
Silver vs Blue  |  -.4400205   .1201597    -3.66   0.001     -.722035    -.158006
Gold vs Silver  |  -.0466391   .1696038    -0.27   0.959    -.4446987    .3514205
---------------------------------------------------------------------------------
</p>
<p>206 6 Hypothesis Testing &amp; ANOVA</p>
<p/>
</div>
<div class="page"><p/>
<p>6.8.3 Two-way ANOVA
</p>
<p>The final research question asks whether the impact of status on overall price
satisfaction depends on the different levels of the variable gender (i.e., male versus
female). The two-way ANOVA allows answering this research question. The null
</p>
<p>hypothesis for this combined effect of status and gender (i.e., their interaction
</p>
<p>effect) is that the difference in the overall price satisfaction between Blue, Silver,
and Goldmembers is the same regardless of the travelers&rsquo; gender. We decide to use
a significance level (α) of 0.05 and directly calculate the test statistic, given that we
</p>
<p>already checked the ANOVA assumptions in the second research question.
</p>
<p>Fig. 6.9 Mean predicted level of overall satisfaction by flight frequency
</p>
<p>Table 6.13 Average marginal effects by flight frequency
</p>
<p>margins status
</p>
<p>Adjusted predictions                            Number of obs     =      1,065
</p>
<p>Expression   : Linear prediction, predict()
</p>
<p>------------------------------------------------------------------------------
|            Delta-method
|     Margin   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
</p>
<p>-------------+----------------------------------------------------------------
status |
Blue  |   4.472674   .0619407    72.21   0.000     4.351133    4.594214
</p>
<p>Silver  |   4.032653   .1029645    39.17   0.000     3.830616     4.23469
Gold  | 3.986014   .1347729    29.58   0.000     3.721562    4.250465
</p>
<p>------------------------------------------------------------------------------
</p>
<p>6.8 Example 207</p>
<p/>
</div>
<div class="page"><p/>
<p>6.8.3.1 Calculate the Test Statistic
In Stata, interaction effects between two variables are indicated by a hashtag (#)
</p>
<p>between the variables that we interact. Now, let us test for interaction effects. Go to
</p>
<p>► Statistics ► Linear models and related ► ANOVA/MANOVA ► Analysis of
</p>
<p>variance and covariance. The dialog box that opens is similar to that shown in
</p>
<p>Fig. 6.10, only this time we enter status##gender (instead of only status) in the
</p>
<p>Model box and click on OK. This produces the output shown in Table 6.14. Note
</p>
<p>that it is essential to have two hashtags (i.e., ##), as this tells Stata to include the two
</p>
<p>main variables status and gender, plus the interaction variable status#gender. The
</p>
<p>Fig. 6.10 ANOVA dialog box
</p>
<p>Table 6.14 Output ANOVA
</p>
<p>anova overall_sat status##gender
</p>
<p>Number of obs =      1,065    R-squared     =  0.0255
Root MSE      =     1.6081    Adj R-squared =  0.0209
</p>
<p>Source | Partial SS         df         MS        F    Prob&gt;F
--------------+----------------------------------------------------
</p>
<p>Model |  71.656085          5   14.331217      5.54  0.0000
|
</p>
<p>status |  53.277593          2   26.638796     10.30  0.0000
gender |   .2061234          1    .2061234      0.08  0.7777
</p>
<p>status#gender |  14.512817          2   7.2564083      2.81  0.0609
|
</p>
<p>Residual |  2738.5542      1,059   2.5859813  
--------------+----------------------------------------------------
</p>
<p>Total |  2810.2103      1,064   2.6411751  
</p>
<p>208 6 Hypothesis Testing &amp; ANOVA</p>
<p/>
</div>
<div class="page"><p/>
<p>reason for this is that status and gender are conditional main effects, whereby the
effect of the one main variable is conditional on the effect of the other main variable
</p>
<p>with a value of 0. Thus, in this example, the conditional main effect of status
represents the effect of statuswhen gender is equal to 0, while the conditional effect
of gender represents the effect of gender when status is equal to 0. When the
included variables have no 0 category, such conditional effects have no interpreta-
</p>
<p>tion. It is therefore important to set a meaningful 0 category.
</p>
<p>6.8.3.2 Make the Test Decision
The output in Table 6.14 shows an F-value of 2.81 with a corresponding p-value of
0.0609 for the interaction term (status#gender). This p-value is higher than 0.05
and thereby not statistically significant. Note that had we decided to use a signifi-
</p>
<p>cance level (α) of 0.10, we would have found a significant interaction effect!
</p>
<p>However, as this is not the case in our example, we conclude that the overall
</p>
<p>level of price satisfaction by membership status does not depend on gender.
</p>
<p>6.8.3.3 Carry Out Post Hoc Tests
To run Tukey&rsquo;s method, go to ► Statistics ► Postestimation. In the window that
</p>
<p>follows, go to ► Tests, contrasts, and comparisons of parameter estimates ►
</p>
<p>Pairwise comparisons and click on Launch. In the dialog box that opens, select
</p>
<p>the variable status#gender under Factor terms to compute pairwise comparisons
for and select the Tukey&rsquo;s method option from the Multiple comparisons drop-
</p>
<p>down menu. Finally, go to the Reporting tab and first tick Specify additional
</p>
<p>tables (default is effects table with confidence intervals) and then tick Show
</p>
<p>effects table with confidence intervals and p-values. Finally, in the same window,
</p>
<p>tick the box Sort the margins/differences in each term, and then click on OK.
</p>
<p>This produces the output as shown in Table 6.15.
</p>
<p>In this special case, post hoc tests are carried out across 15 distinct combinations
</p>
<p>within and between gender and membership status groups. We can check whether
</p>
<p>the pairwise mean comparisons differ significantly if the p-values (under Tukey
P&gt; |t|) are lower than 0.05. The results in Table 6.15 indicate that the overall price/
performance satisfaction is significantly lower between: (1) female travelers with a
</p>
<p>Silver and Blue membership status, (2) female travelers with a Gold and Blue
membership status, and (3) male and female travelers with a Silver membership
status. The p-values of these pairwise comparisons are respectively 0.006, 0.002,
and 0.002, thus smaller than 0.05. All the other effects have a p-value higher than
0.05 and are not significant.
</p>
<p>6.8.3.4 Measure the Strength of the Effects
In the next step, we focus on the model&rsquo;s effect strength. In Table 6.14 this is shown
</p>
<p>under R-squared and is 0.0255, indicating that our model, which includes the
</p>
<p>interaction term, explains 2.5% of the total variance in the overall price satisfaction.
</p>
<p>6.8 Example 209</p>
<p/>
</div>
<div class="page"><p/>
<p>6.8.3.5 Interpret the Results
The mere increase of 0.7% in the explained variance (from 1.8% in Table 6.11 to
</p>
<p>2.5% in Table 6.14) indicates that the interaction term does not add much to the
</p>
<p>model&rsquo;s fit. This is not surprising, as the interaction term reiterates the main effects
</p>
<p>in a slightly different form.
</p>
<p>6.8.3.6 Plot the Results
Finally, we move to plotting the results, which is optional. Like research question
</p>
<p>2, we go to ► Statistics ► Postestimation. In the dialog box that follows, go to ►
</p>
<p>Marginal analysis ► Marginal means and interaction analysis ► At sample means
</p>
<p>and click on Launch. Next, enter the first variable status under Covariate and tick
the box Interaction analysis with another covariate where you enter the second
</p>
<p>variable gender. Then tick the box Draw profile plots of results and click OK.
Stata will produce the output in Table 6.16 and the plot displayed in Fig. 6.11.
</p>
<p>Under status#gender in Table 6.16, we see the average marginal effects of
overall price satisfaction in respect of all status groups as they vary by gender. Here
</p>
<p>we find that the mean overall price satisfaction of female Blue status members
(indicated by Blue#female) is 4.68, while the mean of male Blue status members
equals 4.369469, and so on. Figure 6.11 depicts exactly these margins and their
</p>
<p>corresponding confidence intervals. Overall, we conclude that the relationship
</p>
<p>between the overall price satisfaction and travelers&rsquo; membership status does not
</p>
<p>vary by gender.
</p>
<p>Table 6.15 Output of Tukey&rsquo;s method
</p>
<p>pwcompare status#gender, mcompare(tukey) effects sort
</p>
<p>Pairwise comparisons of marginal linear predictions
</p>
<p>Margins      : asbalanced
</p>
<p>----------------------------
|    Number of
|  Comparisons
</p>
<p>--------------+-------------
status#gender |           15
----------------------------
---------------------------------------------------------------------------------------------------
</p>
<p>|                              Tukey                Tukey
|   Contrast   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
</p>
<p>----------------------------------+----------------------------------------------------------------
status#gender |
</p>
<p>(Silver#female) vs (Blue#female)  |  -.9876923   .2789273    -3.54   0.006    -1.784013   -.1913714
(Gold#female) vs (Blue#female) |     -.7425   .4160734    -1.78   0.476    -1.930365    .4453648
</p>
<p>(Gold#male) vs (Blue#female)  |   -.687874   .1784806    -3.85   0.002    -1.197425   -.1783227
(Silver#female) vs (Blue#male)  |  -.6771613   .2683811    -2.52   0.118    -1.443373 .0890507
(Silver#male) vs (Blue#female)  |  -.5829126   .1550695    -3.76   0.002    -1.025627   -.1401984
</p>
<p>(Gold#female) vs (Blue#male)  |   -.431969   .4090783    -1.06   0.899    -1.599863     .735925
(Gold#male) vs (Blue#male)  |   -.377343   .1615031    -2.34   0.180    -.8384248    .0837387
</p>
<p>(Blue#male) vs (Blue#female)  |   -.310531   .1312038    -2.37   0.169    -.6851101    .0640482
(Silver#male) vs (Blue#male)  |  -.2723816   .1351832    -2.01   0.334    -.6583217    .1135584
</p>
<p>(Gold#female) vs (Silver#male)  |  -.1595874   .4173454    -0.38   0.999    -1.351083    1.031909
(Gold#male) vs (Silver#male)  |  -.1049614   .1814259    -0.58   0.992    -.6229216    .4129988
(Gold#male) vs (Gold#female)  |    .054626    .426598     0.13   1.000    -1.163286    1.272538
</p>
<p>(Gold#female) vs (Silver#female)  |   .2451923   .4774212     0.51   0.996    -1.117817    1.608201
(Gold#male) vs (Silver#female)  |   .2998183   .2943965     1.02   0.912     -.540666    1.140303
</p>
<p>(Silver#male) vs (Silver#female)  |   .4047797   .2808212     1.44   0.702    -.3969479    1.206507
---------------------------------------------------------------------------------------------------
</p>
<p>210 6 Hypothesis Testing &amp; ANOVA</p>
<p/>
</div>
<div class="page"><p/>
<p>Table 6.16 Predicted average marginal effects of a combination between flight frequency and
gender
</p>
<p>margins status#gender, plot
</p>
<p>Adjusted predictions                            Number of obs     =      1,065
</p>
<p>Expression   : Linear prediction, predict()
at           : 1.status        =    .6356808 (mean)
</p>
<p>2.status        =    .2300469 (mean)
3.status        =    .1342723 (mean)
1.gender        =    .2629108 (mean)
2.gender        =    .7370892 (mean)
</p>
<p>-------------------------------------------------------------------------------
|            Delta-method
|     Margin   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
</p>
<p>---------------+---------------------------------------------------------------
status#gender |
Blue#female  |       4.68   .1072066    43.65   0.000     4.469639    4.890361
Blue#male  |   4.369469   .0756386    57.77   0.000      4.22105    4.517888
</p>
<p>Silver#female  |   3.692308   .2575019    14.34   0.000     3.187036     4.19758
Silver#male  |   4.097087   .1120415    36.57   0.000     3.877239    4.316936
Gold#female  |     3.9375   .4020247     9.79   0.000     3.148645    4.726355
Gold#male  |   3.992126   .1426957    27.98   0.000     3.712128    4.272124
</p>
<p>-----------------------------------------------------------------------------
</p>
<p>Fig. 6.11 Mean predicted level of overall satisfaction by membership status and gender
</p>
<p>6.8 Example 211</p>
<p/>
</div>
<div class="page"><p/>
<p>6.9 Customer Analysis at Crédit Samouel (Case Study)
</p>
<p>In 2017, Crédit Samouel, a globally operating bank underwent a massive
</p>
<p>re-branding campaign. In the course of this campaign, the bank&rsquo;s product range
</p>
<p>was also restructured and its service and customer orientation improved. In addi-
</p>
<p>tion, a comprehensive marketing campaign was launched, aimed at increasing the
</p>
<p>bank&rsquo;s customer base by one million new customers by 2025.
</p>
<p>In an effort to control the campaign&rsquo;s success and to align the marketing actions,
</p>
<p>the management decided to conduct an analysis of newly acquired customers.
</p>
<p>Specifically, the management is interested in evaluating the segment customers
</p>
<p>aged 30 and below. To do so, the marketing department surveyed the following
</p>
<p>characteristics of 251 randomly drawn new customers (variable names in
</p>
<p>parentheses):
</p>
<p>&ndash; Gender (gender: male/female).
&ndash; Bank deposit in Euro (deposit: ranging from 0 to 1,000,000).
&ndash; Does the customer currently attend school/university? (training: yes/no).
&ndash; Customer&rsquo;s age specified in three categories (age_cat: 16&ndash;20, 21&ndash;25, and 26&ndash;30).
</p>
<p>Use the data provided in bank.dta ( Web Appendix ! Downloads) to answer
the following research questions:
</p>
<p>1. Which test do we have to apply to find out whether there is a significant
</p>
<p>difference in bank deposits between male and female customers? Do we meet
</p>
<p>the assumptions required to conduct this test? Also use an appropriate normality
</p>
<p>test and interpret the result. Does the result give rise to any cause for concern?
</p>
<p>Carry out an appropriate test to answer the initial research question.
</p>
<p>2. Is there a significant difference in bank deposits between customers who are still
</p>
<p>studying and those who are not?
</p>
<p>3. Which type of test or procedure would you use to evaluate whether bank deposits
</p>
<p>differ significantly between the three age categories? Carry out this procedure
</p>
<p>and interpret the results.
</p>
<p>4. Reconsider the previous question and, using post hoc tests, evaluate whether
</p>
<p>there are significant differences between the three age groups.
</p>
<p>5. Is there a significant interaction effect between the variables training and
age_cat in terms of the customers&rsquo; deposit?
</p>
<p>212 6 Hypothesis Testing &amp; ANOVA</p>
<p/>
</div>
<div class="page"><p/>
<p>6. Estimate and plot the average marginal effects of bank deposits over the
</p>
<p>different combinations of training groups and the customers&rsquo; different age
</p>
<p>categories.
</p>
<p>7. Based on your analysis results, please provide recommendations for the man-
</p>
<p>agement team on how to align their future marketing actions.
</p>
<p>6.10 Review Questions
</p>
<p>1. Describe the steps involved in hypothesis testing in your own words.
</p>
<p>2. Explain the concept of the p-value and explain how it relates to the significance
level α.
</p>
<p>3. What level of α would you choose for the following types of market research
</p>
<p>studies? Give reasons for your answers.
</p>
<p>(a) An initial study on preferences for mobile phone colors.
</p>
<p>(b) The production quality of Rolex watches.
</p>
<p>(c) A repeat study on differences in preference for either Coca Cola or Pepsi.
</p>
<p>4. Write two hypotheses for each of the example studies in question 3, including
</p>
<p>the null hypothesis and alternative hypothesis.
</p>
<p>5. Describe the difference between independent and paired samples t-tests in your
own words and provide two examples of each type.
</p>
<p>6. What is the difference between an independent samples t-test and an ANOVA?
7. What are post hoc test and why is their application useful in ANOVA?
</p>
<p>6.11 Further Readings
</p>
<p>Hubbard, R., &amp; Bayarri, M. J. (2003). Confusion over measure of evidence (p&rsquo;s)
</p>
<p>versus errors (α&rsquo;s) in classical statistical testing. The American Statistician,
57(3), 171&ndash;178.
</p>
<p>The authors discuss the distinction between p-value and α and argue that there is
general confusion about these measures&rsquo; nature among researchers and
practitioners. A very interesting read!
</p>
<p>Kanji, G. K. (2006). 100 statistical tests (3rd ed.). London: Sage.
If you are interested in learning more about different tests, we recommend this best-
</p>
<p>selling book in which the author introduces various tests with information on
how to calculate and interpret their results using simple datasets.
</p>
<p>Mooi, E., &amp; Ghosh, M. (2010). Contract specificity and its performance
</p>
<p>implications. Journal of Marketing, 74(2), 105&ndash;120.
This is an interesting article that demonstrates how directional hypotheses are
</p>
<p>formulated based on theory-driven arguments about contract specificity and
performance implications.
</p>
<p>Stata.com (http://www.stata.com/manuals14/rmargins.pdf).
</p>
<p>6.11 Further Readings 213</p>
<p/>
</div>
<div class="page"><p/>
<p>Stata offers a very thorough explanation of marginal effects and the corresponding
Stata syntaxes for the estimation of marginal means, predictive margins, and
marginal effects.
</p>
<p>References
</p>
<p>Agresti, A., &amp; Finlay, B. (2014). Statistical methods for the social sciences (4th ed.). London:
Pearson.
</p>
<p>Bartus, T. (2005). Estimation of marginal effects using marge off. The Stata Journal, 5(3),
309&ndash;329.
</p>
<p>Boneau, C. A. (1960). The effects of violations of assumptions underlying the t test. Psychological
Bulletin, 57(1), 49&ndash;64.
</p>
<p>Brown, M. B., &amp; Forsythe, A. B. (1974). Robust tests for equality of variances. Journal of the
American Statistical Association, 69(346), 364&ndash;367.
</p>
<p>Cohen, J. (1992). A power primer. Psychological Bulletin, 112(1), 155&ndash;159.
Everitt, B. S., &amp; Skrondal, A. (2010). The Cambridge dictionary of statistics (4th ed.). Cambridge:
</p>
<p>Cambridge University Press.
Field, A. (2013). Discovering statistics using SPSS (4th ed.). London: Sage.
Greene, W. H. (1997). Econometric analysis (3rd ed.). Upper Saddle River: Prentice Hall.
Hubbard, R., &amp; Bayarri, M. J. (2003). Confusion over measure of evidence (p&rsquo;s) versus errors (α&rsquo;s)
</p>
<p>in classical statistical testing. The American Statistician, 57(3), 171&ndash;178.
Kimmel, H. D. (1957). Three criteria for the use of one-tailed tests. Psychological Bulletin, 54(4),
</p>
<p>351&ndash;353.
Lehmann, E. L. (1993). The Fischer, Neyman-Pearson theories of testing hypotheses: One theory
</p>
<p>or two? Journal of the American Statistical Association, 88(424), 1242&ndash;1249.
Levene, H. (1960). Robust tests for equality of variances. In I. Olkin (Ed.), Contributions to
</p>
<p>probability and statistics (pp. 278&ndash;292). Palo Alto: Stanford University Press.
Liao, T. F. (2002). Statistical group comparison. New York: Wiley-InterScience.
Lichters, M., Brunnlieb. C., Nave, G., Sarstedt, M., &amp; Vogt, B. (2016). The influence of serotonin
</p>
<p>defficiency on choice deferral and the compromise effect. Journal of Marketing Research,
53(2), 183&ndash;198.
</p>
<p>Lilliefors, H. W. (1967). On the Kolmogorov&ndash;Smirnov test for normality with mean and variance
unknown. Journal of the American Statistical Association, 62(318), 399&ndash;402.
</p>
<p>Mann, H. B., &amp; Whitney, D. R. (1947). On a test of whether one of two random variables is
stochastically larger than the other. The Annals of Mathematical Statistics, 18(1), 50&ndash;60.
</p>
<p>Mitchell, M. N. (2015). Stata for the behavioral sciences. College Station: Stata Press.
Nuzzo, R. (2014). Scientific method: Statistical errors. Nature, 506(7487), 150&ndash;152.
Ruxton, G. D., &amp; Neuhaeuser, M. (2010). When should we use one-tailed hypothesis testing?
</p>
<p>Methods in Ecology and Evolution, 1(2), 114&ndash;117.
Schuyler, W. H. (2011). Readings statistics and research (6th ed.). London: Pearson.
Shapiro, S. S., &amp; Wilk, M. B. (1965). An analysis of variance test for normality (complete
</p>
<p>samples). Biometrika, 52(3/4), 591&ndash;611.
Van Belle, G. (2008). Statistical rules of thumb (2nd ed.). Hoboken: Wiley.
Wasserstein, R. L., &amp; Lazar, N. A. (2016). The ASA&rsquo;s statement on p-values: Context, process,
</p>
<p>and purpose. The American Statistician, 70(2), 129&ndash;133.
Welch, B. L. (1951). On the comparison of several mean values: An alternative approach.
</p>
<p>Biometrika, 38(3/4), 330&ndash;336.
</p>
<p>214 6 Hypothesis Testing &amp; ANOVA</p>
<p/>
</div>
<div class="page"><p/>
<p>Regression Analysis 7
</p>
<p>Keywords
</p>
<p>Adjusted R2 &bull; Akaike information criterion (AIC) &bull; Autocorrelation &bull; Bayes
</p>
<p>information criterion (BIC) &bull; Binary logistic regression &bull; Breusch-Pagan test &bull;
</p>
<p>Coefficient of determination &bull; Constant &bull; Collinearity &bull; Cross validation &bull;
</p>
<p>Disturbance term &bull; Dummy variable &bull; Durbin-Watson test &bull; Error &bull; Estimation
</p>
<p>sample &bull; η2 (eta-squared) &bull; F-test &bull; Heteroskedasticity &bull; Interaction effects &bull;
</p>
<p>Intercept &bull; Moderation analysis &bull; (Multi)collinearity &bull; Multinomial logistic
</p>
<p>regression &bull; Multiple regression &bull; Nested models &bull; Ordinary least squares &bull;
</p>
<p>Outlier &bull; Ramsey&rsquo;s RESET test &bull; Residual &bull; R2 &bull; Robust regression &bull; Simple
</p>
<p>regression &bull; Split-sample validation &bull; Standard error &bull; Standardized effects &bull;
</p>
<p>Unstandardized effects &bull; Validation sample &bull; Variance inflation factor (VIF) &bull;
</p>
<p>White&rsquo;s test
</p>
<p>Learning Objectives
After reading this chapter, you should understand:
</p>
<p>&ndash; The basic concept of regression analysis.
</p>
<p>&ndash; How regression analysis works.
</p>
<p>&ndash; The requirements and assumptions of regression analysis.
</p>
<p>&ndash; How to specify a regression analysis model.
</p>
<p>&ndash; How to interpret regression analysis results.
</p>
<p>&ndash; How to predict and validate regression analysis results.
</p>
<p>&ndash; How to conduct regression analysis with Stata.
</p>
<p>&ndash; How to interpret regression analysis output produced by Stata.
</p>
<p># Springer Nature Singapore Pte Ltd. 2018
E. Mooi et al., Market Research, Springer Texts in Business and Economics,
DOI 10.1007/978-981-10-5218-7_7
</p>
<p>215</p>
<p/>
</div>
<div class="page"><p/>
<p>7.1 Introduction
</p>
<p>Regression analysis is one of the most frequently used analysis techniques in market
</p>
<p>research. It allows market researchers to analyze the relationships between dependent
</p>
<p>variables and independent variables (Chap. 3). In marketing applications, the depen-
</p>
<p>dent variable is the outcome we care about (e.g., sales), while we use the independent
</p>
<p>variables to achieve those outcomes (e.g., pricing or advertising). The key benefits of
</p>
<p>using regression analysis are it allows us to:
</p>
<p>1. Calculate if one independent variable or a set of independent variables has a
</p>
<p>significant relationship with a dependent variable.
</p>
<p>2. Estimate the relative strength of different independent variables&rsquo; effects on a
</p>
<p>dependent variable.
</p>
<p>3. Make predictions.
</p>
<p>Knowing whether independent variables have a significant effect on dependent
</p>
<p>variables, helps market researchers in many different ways. For example, this
</p>
<p>knowledge can help guide spending if we know promotional activities relate
</p>
<p>strongly to sales.
</p>
<p>Knowing effects&rsquo; relative strength is useful for marketers, because it may help
</p>
<p>answer questions such as: Do sales depend more on the product price or on product
</p>
<p>promotions? Regression analysis also allows us to compare the effects of variables
</p>
<p>measured on different scales, such as the effect of price changes (e.g., measured in
</p>
<p>dollars) and the effect of a specific number of promotional activities.
</p>
<p>Regression analysis can also help us make predictions. For example, if we have
</p>
<p>estimated a regression model by using data on the weekly supermarket sales of a
</p>
<p>brand of milk in dollars, the milk price (which changes with the season and supply),
</p>
<p>as well as an index of promotional activities (comprising product placement,
</p>
<p>advertising, and coupons), the results of the regression analysis could answer the
</p>
<p>question: what would happen to the sales if the prices were to increase by 5% and
</p>
<p>the promotional activities by 10%? Such answers help (marketing) managers make
</p>
<p>sound decisions. Furthermore, by calculating various scenarios, such as price
</p>
<p>increases of 5%, 10%, and 15%, managers can evaluate marketing plans and create
</p>
<p>marketing strategies.
</p>
<p>7.2 Understanding Regression Analysis
</p>
<p>In the previous paragraph, we briefly discussed what regression can do and why it is
</p>
<p>a useful market research tool. We now provide a more detailed discussion. Look at
</p>
<p>Fig. 7.1, which plots a dependent (y) variable (the weekly sales of a brand of milk in
</p>
<p>dollars) against an independent (x1) variable (an index of promotional activities).
</p>
<p>Regression analysis is a way of fitting a &ldquo;best&rdquo; line through a series of observations.
</p>
<p>With a &ldquo;best&rdquo; line we mean one that is fitted in such a way that it minimizes the sum
</p>
<p>of the squared differences between the observations and the line itself. It is
</p>
<p>216 7 Regression Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>important to know that the best line fitted by means of regression analysis is not
</p>
<p>necessarily the true line (i.e., the line that represents the population). Specifically, if
</p>
<p>we have data issues, or fail to meet the regression assumptions (discussed later), the
</p>
<p>estimated line may be biased.
</p>
<p>Before we discuss regression analysis further, we should discuss regression
</p>
<p>notation. Regression models are generally denoted as follows:
</p>
<p>y &frac14; α&thorn; β1x1 &thorn; e
</p>
<p>What does this mean? The y represents the dependent variable, which is the
</p>
<p>outcome you are trying to explain. In Fig. 7.1, we plot the dependent variable on the
</p>
<p>vertical axis. The α represents the constant (or intercept) of the regression model,
and indicates what your dependent variable would be if the independent variable
</p>
<p>were zero. In Fig. 7.1, you can see the constant is the value where the fitted straight
</p>
<p>(sloping) line crosses the y-axis. Thus, if the index of promotional activities is zero,
</p>
<p>we expect the weekly supermarket sales of a specific milk brand to be $2,500. It
</p>
<p>may not always be realistic to assume that independent variables are zero (prices
</p>
<p>are, after all, rarely zero), but the constant should always be included to ensure the
</p>
<p>regression model&rsquo;s best possible fit with the data.
</p>
<p>The independent variable is indicated by x1, while the &szlig;1 (pronounced beta)
</p>
<p>indicates its (regression) coefficient. This coefficient represents the slope of the
</p>
<p>line, or the slope of the diagonal grey line in Fig. 7.1. A positive &szlig;1 coefficient
</p>
<p>indicates an upward sloping regression line, while a negative &szlig;1 coefficient
</p>
<p>indicates a downward sloping line. In our example, the line slopes upward. This
</p>
<p>0
</p>
<p>0
5
</p>
<p>0
0
</p>
<p>0
1
</p>
<p>0
0
</p>
<p>0
0
</p>
<p>w
e
</p>
<p>e
k
ly
</p>
<p> s
a
</p>
<p>le
s
 i
n
</p>
<p> $
</p>
<p>1
5
</p>
<p>0
0
</p>
<p>0
</p>
<p>50 100 150
</p>
<p>Index of promotional activities
</p>
<p>Fig. 7.1 A visual explanation of regression analysis
</p>
<p>7.2 Understanding Regression Analysis 217</p>
<p/>
</div>
<div class="page"><p/>
<p>makes sense, since sales tend to increase with an increase in promotional activities.
</p>
<p>In our example, we estimate the &szlig;1 as 54.59, meaning that if we increase the
</p>
<p>promotional activities by one unit, the weekly supermarket sales of a brand of
</p>
<p>milk will go up by an average of $54.59. This &szlig;1 value has a degree of associated
</p>
<p>uncertainty called the standard error. This standard error is assumed to be
normally distributed. Using a t-test (see Chap. 6), we can test if the &szlig;1 is indeed
</p>
<p>significantly different from zero.
</p>
<p>The last element of the notation, the e, denotes the equation error (also called
the residual or disturbance term). The error is the distance between each obser-
vation and the best fitting line. To clarify what a regression error is, examine
</p>
<p>Fig. 7.1 again. The error is the difference between the regression line (which
</p>
<p>represents our regression prediction) and the actual observation (indicated by
</p>
<p>each dot). The predictions made by the &ldquo;best&rdquo; regression line are indicated by by
(pronounced y-hat). Thus, the error of each observation is:1
</p>
<p>e &frac14; y� by
</p>
<p>In the example above, we have only one independent variable. We call this
</p>
<p>simple regression. If we include multiple independent variables, we call this
multiple regression. The notation for multiple regression is similar to that of
simple regression. If we were to have two independent variables, say the price
</p>
<p>(x1), and an index of promotional activities (x2), our notation would be:
</p>
<p>y &frac14; α&thorn; β1x1 &thorn; β2x2 &thorn; e
</p>
<p>We need one regression coefficient for each independent variable (i.e., &szlig;1 and &szlig;2).
</p>
<p>Technically the &szlig;s indicate how a change in an independent variable influences the
</p>
<p>dependent variable if all other independent variables are held constant.2
</p>
<p>The Explained Visually webpage offers an excellent visualization of how
</p>
<p>regression analysis works, see http://setosa.io/ev/ordinary-least-squares-
</p>
<p>regression/
</p>
<p>Now that we have introduced a few regression analysis basics, it is time to
</p>
<p>discuss how to execute a regression analysis. We outline the key steps in Fig. 7.2.
</p>
<p>We first introduce the regression analysis data requirements, which will determine
</p>
<p>if regression analysis can be used. After this first step, we specify and estimate the
</p>
<p>regression model. Next, we discuss the basics, such as which independent variables
</p>
<p>to select. Thereafter, we discuss the assumptions of regression analysis, followed by
</p>
<p>1Strictly speaking, the difference between the predicted and the observed y-values is be.
2This only applies to the standardized βs.
</p>
<p>218 7 Regression Analysis</p>
<p/>
<div class="annotation"><a href="http://setosa.io/ev/ordinary-least-squares-regression">http://setosa.io/ev/ordinary-least-squares-regression</a></div>
<div class="annotation"><a href="http://setosa.io/ev/ordinary-least-squares-regression">http://setosa.io/ev/ordinary-least-squares-regression</a></div>
</div>
<div class="page"><p/>
<p>how to interpret and validate the regression results. The last step is to use the
</p>
<p>regression model to, for example, make predictions.
</p>
<p>7.3 Conducting a Regression Analysis
</p>
<p>7.3.1 Check the Regression Analysis Data Requirements
</p>
<p>Various data requirements must be taken into consideration before we undertake a
</p>
<p>regression analysis. These include the:
</p>
<p>&ndash; sample size,
</p>
<p>&ndash; variables need to vary,
</p>
<p>&ndash; scale type of the dependent variable, and
</p>
<p>&ndash; collinearity.
</p>
<p>We discuss each requirement in turn.
</p>
<p>7.3.1.1 Sample Size
The first data requirement is that we need an &ldquo;acceptable&rdquo; sample size. &ldquo;Accept-
</p>
<p>able&rdquo; relates to a sample size that gives you a good chance of finding significant
</p>
<p>results if they are possible (i.e., the analysis achieves a high degree of statistical
</p>
<p>power; see Chap. 6). There are two ways to calculate &ldquo;acceptable&rdquo; sample sizes.
</p>
<p>Check the regression analysis data requirements
</p>
<p>Specify and estimate the regression model
</p>
<p>Test the regression analysis assumptions
</p>
<p>Interpret the regression results
</p>
<p>Validate the regression results
</p>
<p>Use the regression model
</p>
<p>Fig. 7.2 Steps to conduct a regression analysis
</p>
<p>7.3 Conducting a Regression Analysis 219</p>
<p/>
</div>
<div class="page"><p/>
<p>&ndash; The first, formal, approach is a power analysis. As mentioned in Chap. 6 (Box 6.
</p>
<p>1), these calculations require you to specify several parameters, such as the
</p>
<p>expected effect size and the maximum type I error you want to allow for.
</p>
<p>Generally, you also have to set the power&mdash;0.80 is an acceptable level. A
</p>
<p>power level of 0.80 means there is an 80% probability of deciding that an effect
</p>
<p>will be significant, if it is indeed significant. Kelley and Maxwell (2003) discuss
</p>
<p>sample size requirements in far more detail, while: https://stats.idre.ucla.edu/
</p>
<p>stata/dae/multiple-regression-power-analysis/ discusses how to calculate sample
</p>
<p>sizes precisely.
</p>
<p>&ndash; The second approach is by using rules of thumb. These rules are not specific or
</p>
<p>precise, but are easy to apply. Green (1991) and VanVoorhis and Morgan (2007)
</p>
<p>suggest that if you want to test for individual parameters&rsquo; effect (i.e., whether one
</p>
<p>coefficient is significant or not), you need a sample size of 104 + k. Thus, if you
</p>
<p>have ten independent variables, you need 104 + 10&frac14; 114 observations. Note that
this rule of thumb is best applied when you have a small number of independent
</p>
<p>variables, say less than 10 and certainly less than 15. VanVoorhis and Morgan
</p>
<p>(2007) add that having at least 30 observations per variable (i.e., 30�k) allows for
detecting smaller effects (an expected R2 of 0.10 or smaller) better.
</p>
<p>7.3.1.2 Variables Need to Vary
A regression model cannot be estimated if the variables have no variation. If there is
</p>
<p>no variation in the dependent variable (i.e., it is constant), we also do not need
</p>
<p>regression, as we already know what the dependent variable&rsquo;s value is! Likewise, if
</p>
<p>an independent variable has no variation, it cannot explain any variation in the
</p>
<p>dependent variable.
</p>
<p>No variation can lead to epic failures! Consider the admission tests set by the
</p>
<p>University of Liberia: Not a single student passed the entry exams. In such
</p>
<p>situations, a regression analysis will clearly make no difference! http://www.
</p>
<p>independent.co.uk/student/news/epic-fail-all-25000-students-fail-university-
</p>
<p>entrance-exam-in-liberia-8785707.html
</p>
<p>7.3.1.3 Scale Type of the Dependent Variable
The third data requirement is that the dependent variable needs to be interval or
</p>
<p>ratio scaled (Chap. 3 discusses scaling). If the data are not interval or ratio scaled,
</p>
<p>alternative types of regression should be used. You should use binary logistic
regression if the dependent variable is binary and only takes two values (zero and
one). If the dependent variable is a nominal variable with more than two levels, you
</p>
<p>should use multinomial logistic regression. This should, for example, be used if
you want to explain why people prefer product A over B or C. We do not discuss
</p>
<p>these different methods in this chapter, but they are related to regression. For a
</p>
<p>discussion of regression methods with dependent variables measured on a nominal
</p>
<p>or ordinal scale, see Cameron and Trivedi (2010).
</p>
<p>220 7 Regression Analysis</p>
<p/>
<div class="annotation"><a href="https://stats.idre.ucla.edu/stata/dae/multiple-regression-power-analysis/">https://stats.idre.ucla.edu/stata/dae/multiple-regression-power-analysis/</a></div>
<div class="annotation"><a href="https://stats.idre.ucla.edu/stata/dae/multiple-regression-power-analysis/">https://stats.idre.ucla.edu/stata/dae/multiple-regression-power-analysis/</a></div>
<div class="annotation"><a href="http://www.independent.co.uk/student/news/epic-fail-all-25000-students-fail-university-entrance-exam-in-liberia-8785707.html">http://www.independent.co.uk/student/news/epic-fail-all-25000-students-fail-university-entrance-exam-in-liberia-8785707.html</a></div>
<div class="annotation"><a href="http://www.independent.co.uk/student/news/epic-fail-all-25000-students-fail-university-entrance-exam-in-liberia-8785707.html">http://www.independent.co.uk/student/news/epic-fail-all-25000-students-fail-university-entrance-exam-in-liberia-8785707.html</a></div>
<div class="annotation"><a href="http://www.independent.co.uk/student/news/epic-fail-all-25000-students-fail-university-entrance-exam-in-liberia-8785707.html">http://www.independent.co.uk/student/news/epic-fail-all-25000-students-fail-university-entrance-exam-in-liberia-8785707.html</a></div>
</div>
<div class="page"><p/>
<p>7.3.1.4 Collinearity
The last data requirement is that no or little collinearity should be present.3 Collin-
earity is a data issue that arises if two independent variables are highly correlated.
Perfect collinearity occurs if we enter two or more independent variables containing
</p>
<p>exactly the same information, therefore yielding a correlation of 1 or�1 (i.e., they are
perfectly correlated). Perfect collinearity may occur if you enter the same indepen-
</p>
<p>dent variable twice, or if one variable is a linear combination of another (e.g., one
</p>
<p>variable is a multiple of another variable, such as sales in units and sales in thousands
</p>
<p>of units). If this occurs, regression analysis cannot estimate one of the two
</p>
<p>coefficients; this means one coefficient will not be estimated. In practice, however,
</p>
<p>weaker forms of collinearity are common. For example, if we study what drives
</p>
<p>supermarket sales, variables such as price reductions and promotions are often used
</p>
<p>together. If this occurs very often, the variables price and promotion may be collinear,
</p>
<p>which means there is little uniqueness or new information in each of the variables.
</p>
<p>The problem with having collinearity is that it tends to regard significant parameters
</p>
<p>as insignificant. Substantial collinearity can even lead to sign changes in the regres-
</p>
<p>sion coefficients&rsquo; estimates. When three or more variables are strongly related to each
</p>
<p>other, we call this multicollinearity.
Fortunately, collinearity is relatively easy to detect by calculating the variance
</p>
<p>inflation factor (VIF). The VIF indicates the effect on the standard error of the
regression coefficient for each independent variable. Specifically, the square root of
</p>
<p>the VIF indicates you how much larger the standard error is, compared to if that
</p>
<p>variable were uncorrelated with all other independent variables in the regression
</p>
<p>model. Generally, a VIF of 10 or above indicates that (multi) collinearity is a
</p>
<p>problem (Hair et al. 2013).4 Some research now suggests that VIFs far above 10&mdash;
</p>
<p>such as 20 or 40&mdash;can be acceptable if the sample size is large and the R2 (discussed
</p>
<p>later) is high (say 0.90 or more) (O&rsquo;brien 2007). Conversely, if the sample sizes are
</p>
<p>below 200 and the R2 is low (0.25 or less), collinearity is more problematic (Mason
</p>
<p>and Perreault 1991). Consequently, in such situations, lower VIF values&mdash;such as
</p>
<p>5&mdash;should be the maximum.
</p>
<p>You can remedy collinearity in several ways. If perfect collinearity occurs, drop
</p>
<p>one of the perfectly overlapping variables. If weaker forms of collinearity occur,
</p>
<p>you can utilize two approaches to reduce collinearity (O&rsquo;brien 2007):
</p>
<p>&ndash; The first option is to use principal component or factor analysis on the collinear
</p>
<p>variables (see Chap. 8). By using principal component or factor analysis, you
</p>
<p>create a small number of factors that comprise most of the original variables&rsquo;
</p>
<p>3This is only a requirement if you are interested in the regression coefficients, which is the
dominant use of regression. If you are only interested in prediction, collinearity is not important.
4The VIF is calculated using a completely separate regression analysis. In this regression analysis,
the variable for which the VIF is calculated is regarded as a dependent variable and all other
independent variables are regarded as independents. The R2 that this model provides is deducted
from 1 and the reciprocal value of this sum (i.e., 1/(1 � R2)) is the VIF. The VIF is therefore an
indication of how much the regression model explains one independent variable. If the other
variables explain much of the variance (the VIF is larger than 10), collinearity is likely a problem.
</p>
<p>7.3 Conducting a Regression Analysis 221</p>
<p/>
</div>
<div class="page"><p/>
<p>information, but are uncorrelated. If you use factors, collinearity between the
</p>
<p>previously collinear variables is no longer an issue.
</p>
<p>&ndash; The second option is to re-specify the regression model by removing highly
</p>
<p>correlated variables. Which variables should you remove? If you create a
</p>
<p>correlation matrix of all the independent variables entered in the regression
</p>
<p>model, you should first focus on the variables that are most strongly correlated
</p>
<p>(see Chap. 5 for how to create a correlation matrix). First try removing one of the
</p>
<p>two most strongly correlated variables. The one you should remove depends on
</p>
<p>your research problem&mdash;pick the most relevant variable of the two.
</p>
<p>&ndash; The third option is not to do anything. In many cases removing collinear
</p>
<p>variables does not reduce the VIF values significantly. Even if we do, we run
</p>
<p>the risk of mis-specifying the regression model (see Box 7.1 for details). Given
</p>
<p>the trouble researchers go through to collect data and specify a regression model,
</p>
<p>it is often better to accept collinearity in all but the most extreme cases.
</p>
<p>7.3.2 Specify and Estimate the Regression Model
</p>
<p>We need to select the variables we want to include and decide how to estimate the
</p>
<p>model to conduct a regression analysis. In the following, we will discuss each step
</p>
<p>in detail.
</p>
<p>7.3.2.1 Model Specification
The model specification step involves choosing the variables to use. The regression
</p>
<p>model should be simple yet complete. To quote Albert Einstein: &ldquo;Everything should
</p>
<p>be made as simple as possible, but not simpler!&rdquo; How do we achieve this? By
</p>
<p>focusing on our ideas of what relates to the dependent variable of interest, the
</p>
<p>availability of data, client requirements, and prior regression models. For example,
</p>
<p>typical independent variables explaining the sales of a particular product include
</p>
<p>the price and promotions. When available, in-store advertising, competitors&rsquo; prices,
</p>
<p>and promotions are usually also included. Market researchers may, of course,
</p>
<p>choose different independent variables for other applications. Omitting important
</p>
<p>variables (see Box 7.1) has substantial implications for the regression model, so it is
</p>
<p>best to be inclusive. A few practical suggestions:
</p>
<p>&ndash; If you have many variables available in the data that overlap in terms of how
</p>
<p>they are defined&mdash;such as satisfaction with the waiter/waitress and with the
</p>
<p>speed of service&mdash;try to pick the variable that is most distinct or relevant for
</p>
<p>the client. Alternatively, you could conduct a principal component or factor
</p>
<p>analysis (see Chap. 8) first and use the factors as the regression analysis&rsquo;s
</p>
<p>independent variables.
</p>
<p>&ndash; If you expect to need a regression model for different circumstances, you should
</p>
<p>make sure that the independent variables are the same, which will allow you to
</p>
<p>compare the models. For example, temperature can drive the sales of some
</p>
<p>supermarket products (e.g., ice cream). In some countries, such as Singapore,
</p>
<p>the temperature is relatively constant, so including this variable is not important.
</p>
<p>222 7 Regression Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>In other countries, such as Germany, the temperature can fluctuate far more. If
</p>
<p>you are intent on comparing the ice cream sales in different countries, it is best to
</p>
<p>include variables that may be relevant to all the countries you want to compare
</p>
<p>(e.g., by including temperature, even if it is not very important in Singapore).
</p>
<p>&ndash; Consider the type of advice you want to provide. If you want to make concrete
</p>
<p>recommendations regarding how to use point-of-sales promotions and free prod-
</p>
<p>uct giveaways to boost supermarket sales, both variables need to be included.
</p>
<p>&ndash; Take the sample size rules of thumb into account. If practical issues limit the
</p>
<p>sample size to below the threshold that the rules of thumb recommend, use as
</p>
<p>few independent variables as possible. Larger sample sizes allow you more
</p>
<p>freedom to add independent variables, although they still need to be relevant.
</p>
<p>7.3.2.2 Model Estimation
Model estimation refers to how we estimate a regression model. The most common
</p>
<p>method of estimating regression models is ordinary least squares (OLS). OLS fits
a regression line to the data that minimizes the sum of the squared distances to
</p>
<p>it. These distances are squared to stop negative distances (i.e., below the regression
</p>
<p>line) from cancelling out positive distances (i.e., above the regression line), because
</p>
<p>squared values are always positive. Moreover, by using the square, we emphasize
</p>
<p>observations that are far from the regression much more, while observations close
</p>
<p>to the regression line carry very little weight. The rule to use squared distances is an
</p>
<p>effective (but also arbitrary) way of calculating the best fit between a set of
</p>
<p>observations and a regression line (Hill et al. 2008). If we return to Fig. 7.1., we
</p>
<p>see the vertical &ldquo;spikes&rdquo; from each observation to the regression line. OLS estima-
</p>
<p>tion is aimed at minimizing the squares of these spikes.
</p>
<p>Box 7.1 Omitting Relevant Variables
</p>
<p>Omitting key variables from a regression model can lead to biased results.
</p>
<p>Imagine that we want to explain weekly sales by only referring to promotions.
</p>
<p>From the introduction, we know the &szlig; of the regression model only containing
</p>
<p>promotions is estimated as 54.59. If we add the variable price (arguably a key
</p>
<p>variable), the estimated &szlig; of promotions drops to 42.27. As can be seen, the
</p>
<p>difference between the estimated &szlig;s in the two models (i.e., with and without
</p>
<p>price) is 12.32, suggesting that the &ldquo;true&rdquo; relationship between promotions
</p>
<p>and sales is weaker than in a model with only one independent variable. This
</p>
<p>example shows that omitting important independent variables leads to biases
</p>
<p>in the value of the estimated &szlig;s. That is, if we omit a relevant variable x2 from
</p>
<p>a regression model that only includes x1, we cause a bias in the &szlig;1 estimate.
</p>
<p>More precisely, the &szlig;1 is likely to be inflated, which means that the
</p>
<p>estimated value is higher than it should be. Thus, the &szlig;1 itself is biased
</p>
<p>because we omit x2!
</p>
<p>7.3 Conducting a Regression Analysis 223</p>
<p/>
</div>
<div class="page"><p/>
<p>We use the data behind Fig. 7.1&mdash;as shown in Table 7.1&mdash;to illustrate the
</p>
<p>method with which OLS regressions are calculated. This data has 30 observations,
</p>
<p>with information on the supermarket&rsquo;s sales of a brand of milk (sales), the price
</p>
<p>(price), and an index of promotional activities (promotion) for weeks 1&ndash;30. This
</p>
<p>dataset is small and only used to illustrate how OLS estimates are calculated. The
</p>
<p>data regression.dta can be downloaded, but are also included in Table 7.1. (see
</p>
<p>Web Appendix ! Downloads).
To estimate an OLS regression of the effect of price and promotion on sales, we
</p>
<p>need to calculate the &szlig;s, of which the estimate is noted as bβ (pronounced as beta-
hat). The bβ indicates the estimated association between each independent variable
(price and promotion) and the dependent variable sales. We can estimate bβ as
follows:
</p>
<p>Table 7.1 Regression
data
</p>
<p>Week Sales Price Promotion
</p>
<p>1 3,454 1.10 12.04
</p>
<p>2 3,966 1.08 22.04
</p>
<p>3 2,952 1.08 22.04
</p>
<p>4 3,576 1.08 22.04
</p>
<p>5 3,692 1.08 21.42
</p>
<p>6 3,226 1.08 22.04
</p>
<p>7 3,776 1.09 47.04
</p>
<p>8 14,134 1.05 117.04
</p>
<p>9 5,114 1.10 92.04
</p>
<p>10 4,022 1.08 62.04
</p>
<p>11 4,492 1.12 87.04
</p>
<p>12 10,186 1.02 92.04
</p>
<p>13 7,010 1.08 61.42
</p>
<p>14 4,162 1.06 72.04
</p>
<p>15 3,446 1.13 47.04
</p>
<p>16 3,690 1.05 37.04
</p>
<p>17 3,742 1.10 12.04
</p>
<p>18 7,512 1.08 62.04
</p>
<p>19 9,476 1.08 62.04
</p>
<p>20 3,178 1.08 22.04
</p>
<p>21 2,920 1.12 2.04
</p>
<p>22 8,212 1.04 42.04
</p>
<p>23 3,272 1.09 17.04
</p>
<p>24 2,808 1.11 7.04
</p>
<p>25 2,648 1.12 2.04
</p>
<p>26 3,786 1.11 7.04
</p>
<p>27 2,908 1.12 2.04
</p>
<p>28 3,395 1.08 62.04
</p>
<p>29 4,106 1.04 82.04
</p>
<p>30 8,754 1.02 132.04
</p>
<p>224 7 Regression Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>bβ &frac14; xTx
� ��1
</p>
<p>: xTy
</p>
<p>In this equation to solve the bβ, we first multiply the transposed matrix indicated as xT.
This matrix has three elements, a vector of 1s, which are added to estimate the
</p>
<p>intercept and two vectors of the independent variables price and promotion. Together,
</p>
<p>these form a 30 by 3 matrix. Next, we multiply this matrix with the untransposed
</p>
<p>matrix, indicated as x, consisting of the same elements (as a 3 by 30 matrix). This
</p>
<p>multiplication results in a 3�3 matrix of which we calculate the inverse, indicated by
the power of�1 in the equation. This also results in a 3 by 3 matrix (xTx)�1. Next, we
calculate xTy, which consists of the 30 by 3 matrix and the vector with the dependent
</p>
<p>variables&rsquo; observations (a 1 by 30 matrix). In applied form:5
</p>
<p>x &frac14;
</p>
<p>1 1:10 12:04
</p>
<p>1 1:08 22:04
</p>
<p>⋮ ⋮ ⋮
</p>
<p>1 1:02 132:04
</p>
<p>2
</p>
<p>664
</p>
<p>3
</p>
<p>775,
</p>
<p>xT &frac14;
1 1 . . . 1
</p>
<p>1:10 1:08 . . . 1:02
</p>
<p>12:04 22:04 . . . 132:04
</p>
<p>2
</p>
<p>4
</p>
<p>3
</p>
<p>5,
</p>
<p>xTx
� ��1
</p>
<p>&frac14;
77:97 �70:52 �0:04
</p>
<p>�70:52 63:86 0:03
� 0:04 0:03 0:00
</p>
<p>2
</p>
<p>4
</p>
<p>3
</p>
<p>5,
</p>
<p>xTy &frac14;
147615:00
</p>
<p>158382:64
</p>
<p>8669899:36
</p>
<p>2
</p>
<p>4
</p>
<p>3
</p>
<p>5,
</p>
<p>Hence, xTx
� ��1
</p>
<p>∙ xTy &frac14;
30304:05
</p>
<p>�25209:86
42:27
</p>
<p>2
</p>
<p>4
</p>
<p>3
</p>
<p>5:
</p>
<p>This last matrix indicates the estimated βs with 30304.05 representing the intercept,
</p>
<p>�25209.86 representing the effect of a one-unit increase in the price on sales, and
42.27 the effect of a one-unit increase in promotions on sales. This shows how the
</p>
<p>OLS estimator is calculated.
</p>
<p>5This term can be calculated manually, but also by using the function mmult in Microsoft Excel
where xTx is calculated. Once this matrix has been calculated, you can use the minverse function to
arrive at (xTx)�1 .
</p>
<p>7.3 Conducting a Regression Analysis 225</p>
<p/>
</div>
<div class="page"><p/>
<p>As discussed before, each bβ has a standard error, which expresses the uncertainty
associated with the estimate. This standard error can be expressed in standard
</p>
<p>deviations and, as discussed in Chap. 6, with more than 100 degrees of freedom
</p>
<p>and α&frac14; 0.05, t-values outside the critical value of �1.96 indicate that the estimated
effect is significant and that the null hypothesis can be rejected. If this the t-value
</p>
<p>falls within the range of � 1.96, the bβ is said to be insignificant.
While OLS is an effective estimator, there are alternatives that work better in
</p>
<p>specific situations. These situations occur if we violate one of the regression
</p>
<p>assumptions. For example, if the regression errors are heteroskedastic (discussed
</p>
<p>in Sect. 7.3.3.3), we need to account for this by, for example, using robust
regression (White 1980).6 Random-effects estimators allow for estimating a
model with correlated errors. There are many more estimators, but these are beyond
</p>
<p>the scope of this book. Greene (2011) discusses these and other estimation
</p>
<p>procedures in detail. Cameron and Trivedi (2010) discuss their implementation in
</p>
<p>Stata.
</p>
<p>7.3.3 Test the Regression Analysis Assumptions
</p>
<p>We have already discussed several issues that determine whether running a regres-
</p>
<p>sion analysis is useful. We now discuss regression analysis assumptions. If a
</p>
<p>regression analysis fails to meet its assumptions, it can provide invalid results.
</p>
<p>Four regression analysis assumptions are required to provide valid results:
</p>
<p>1. the regression model can be expressed linearly,
</p>
<p>2. the regression model&rsquo;s expected mean error is zero,
</p>
<p>3. the errors&rsquo; variance is constant (homoscedasticity), and
</p>
<p>4. the errors are independent (no autocorrelation).
</p>
<p>There is a fifth assumption, which is, however optional. If we meet this assumption,
</p>
<p>we have information on how the regression parameters are distributed, which
</p>
<p>allows straightforward conclusions regarding their significance. If the regression
</p>
<p>analysis fails to meet this assumption, the regression model will still be accurate,
</p>
<p>but it becomes we cannot rely on the standard errors (and t-values) to determine the
</p>
<p>regression parameters&rsquo; significance.
</p>
<p>5. The errors need to be approximately normally distributed.
</p>
<p>We next discuss these assumptions and how we can test each of them.
</p>
<p>6In Stata this can be done by using the, robust option.
</p>
<p>226 7 Regression Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>7.3.3.1 First Assumption: Linearity
The first assumption means that we can write the regression model as y&frac14; α +
β1x1 + e. Thus, non-linear relationships, such as β
</p>
<p>2
1x1, are not permissible. However,
</p>
<p>logarithmic expressions, such as log(x1), are possible as the regression model is still
</p>
<p>specified linearly. If you can write a model whose regression parameters (the &szlig;s) are
</p>
<p>linear, you satisfy this assumption.
</p>
<p>A separate issue is whether the relationship between the independent variable
</p>
<p>x and the dependent variable y is linear. You can check the linearity between x and
</p>
<p>y variables by plotting the independent variables against the dependent variable.
</p>
<p>Using a scatter plot, we can then assess whether there is some type of non-linear
</p>
<p>pattern. Fig. 7.3 shows such a plot. The straight, sloping line indicates a linear
</p>
<p>relationship between sales and promotions. For illustration purposes, we have also
</p>
<p>added a curved upward sloping line. This line corresponds to a x21 transformation. It
</p>
<p>visually seems that a linear line fits the data best. If we fail to identify non-linear
</p>
<p>relationships as such, our regression line does not fit the data well, as evidenced in a
</p>
<p>low model fit (e.g., the R2, which we will discuss later) and nonsignificant effects.
</p>
<p>After transforming x1 by squaring it (or using any other transformation), you still
</p>
<p>satisfy the assumption of specifying the regression model linearly, despite the
</p>
<p>non-linear relationship between x and y.
</p>
<p>Ramsey&rsquo;s RESET test is a specific linearity test (Ramsey 1969; Cook and
Weisberg 1983). This test includes the squared values of the independent variables
</p>
<p>0
</p>
<p>0
5
0
0
0
</p>
<p>1
0
0
0
0
</p>
<p>w
e
e
k
ly
</p>
<p> s
a
le
</p>
<p>s
 i
n
 $
</p>
<p>1
5
0
0
0
</p>
<p>2
0
0
0
0
</p>
<p>50 100
</p>
<p>index of promotional activities
</p>
<p>150
</p>
<p>Fig. 7.3 Different relationships between promotional activities and weekly sales
</p>
<p>7.3 Conducting a Regression Analysis 227</p>
<p/>
</div>
<div class="page"><p/>
<p>(i.e., x21 and third powers (i.e., x
3
1), and tests if these are significant (Baum 2006).
</p>
<p>7
</p>
<p>While this test can detect these specific types of non-linearities, it does not indicate
</p>
<p>which variable(s) has(ve) a non-linear relationship with the dependent variable.
</p>
<p>Sometimes this test is (falsely) called a test for omitted variables, but it actually
</p>
<p>tests for non-linearities.
</p>
<p>7.3.3.2 Second Assumption: Expected Mean Error is Zero
The second assumption is that the expected (not the estimated!) mean error is zero.
</p>
<p>If we do not expect the sum of the errors to be zero, we obtain a biased line. That is,
</p>
<p>we have a line that consistently overestimates or underestimates the true relation-
</p>
<p>ship. This assumption is not testable by means of statistics, as OLS always renders a
</p>
<p>best line with a calculated mean error of exactly zero. This assumption is important,
</p>
<p>because if the error&rsquo;s expected value is not zero, there is additional information in
</p>
<p>the data that has not been used in the regression model. For example, omitting
</p>
<p>important variables, as discussed in Box 7.1, or autocorrelation may cause the
</p>
<p>expected error to no longer be zero (see Sect. 7.3.3.4).
</p>
<p>7.3.3.3 Third Assumption: Homoscedasticity
The third assumption is that the errors&rsquo; variance is constant, a situation we call
</p>
<p>homoscedasticity. Imagine that we want to explain various supermarkets&rsquo; weekly
</p>
<p>sales in dollars. Large stores obviously have a far larger sales spread than small
</p>
<p>supermarkets. For example, if you have average weekly sales of $50,000, you might
</p>
<p>see a sudden jump to $60,000, or a fall to $40,000. However, a very large supermar-
</p>
<p>ket could see sales move from an average of $5 million to $7 million. This causes the
</p>
<p>weekly sales&rsquo; error variance of large supermarkets to be much larger than that of
</p>
<p>small supermarkets. We call this non-constant variance heteroskedasticity. If we
estimate regression models on data in which the variance is not constant, they will
</p>
<p>still result in correct βs. However, the associated standard errors are likely to be too
</p>
<p>large and may cause some βs to not be significant, although they actually are.
</p>
<p>Figure 7.4 provides a visualization of heteroskedasticity. As the dependent
</p>
<p>variable increases, the error variance also increases. If heteroskedasticity is an
</p>
<p>issue, the points are typically funnel shaped, displaying more (or less) variance as
</p>
<p>the independent variable increases (decreases). This funnel shape is typical of
</p>
<p>heteroskedasticity and indicates that, as a function of the dependent variable, the
</p>
<p>error variance changes.
</p>
<p>We can always try to visualize heteroskedasticity, whose presence is calculated
</p>
<p>by means of the errors, but it is often very difficult to determine visually whether
</p>
<p>heteroskedasticity is present. For example, when datasets are large, it is hard to see
</p>
<p>a funnel shape in the scatterplot. We can formally test for the presence of
</p>
<p>heteroskedasticity by using the Breusch-Pagan test and White&rsquo;s test.
The Breusch-Pagan test (1980) is the most frequently used test. This test
</p>
<p>determines whether the errors&rsquo; variance depends on the variables in the model by
</p>
<p>7The test also includes the predicted values squared and to the power of three.
</p>
<p>228 7 Regression Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>using a separate regression analysis (Greene 2011). This tests the null hypothesis
</p>
<p>that the errors&rsquo; variance does not depend on the variables in the regression model.8
</p>
<p>Rejecting this null hypothesis suggests that heteroskedasticity is present. If the
</p>
<p>Breusch-Pagan test indicates that heteroskedasticity is an issue, robust regression
</p>
<p>remedy for this (see Sect. 7.3.2.2). Note that to illustrate heteroskedasticity, we
</p>
<p>have used slightly different data than in the other examples.
</p>
<p>White&rsquo;s test, as extended by Cameron and Trivedi (1990), is a different test for
</p>
<p>heteroskedasticity. This test does not test if the error goes up or down, but adopts a
</p>
<p>more flexible approach whereby errors can first go down, then up (i.e., an hourglass
</p>
<p>shape), or first go up, then down (diabolo-shaped). Compared to the Breusch-Pagan
</p>
<p>test, White&rsquo;s test considers more shapes that can indicate heteroskedasticity. This has
</p>
<p>benefits in that more forms of heteroskedasticity can be detected, but in small samples;
</p>
<p>however, White&rsquo;s test may not detect heteroskedasticity, even if it is present. It is
</p>
<p>therefore best to use both tests, as they have slightly different strengths. Generally, the
</p>
<p>two tests are comparable, but if they are not, it is best to rely on White&rsquo;s test.
</p>
<p>7.3.3.4 Fourth Assumption: No Autocorrelation
The fourth assumption is that the regression model errors are independent; that is,
</p>
<p>the error terms are uncorrelated for any two observations. Imagine that you want to
</p>
<p>explain the supermarket sales of a brand of milk by using the previous week&rsquo;s sales
</p>
<p>Large 
</p>
<p>error 
</p>
<p>variance
</p>
<p>Small error variance
</p>
<p>Fig. 7.4 An example of heteroskedasticity
</p>
<p>8Specifically, in the mentioned regression model y&frac14; α+ β1x1+ β2x2+ β3x3+ e, the Breusch-Pagan
</p>
<p>test determines whether be2 &frac14; α&thorn; βBP1x1 &thorn; βBP2x2 &thorn; βBP3x3 &thorn; eBP.
</p>
<p>7.3 Conducting a Regression Analysis 229</p>
<p/>
</div>
<div class="page"><p/>
<p>of that milk. It is very likely that if sales increased last week, they will also increase
</p>
<p>this week. This may be due to, for example, the growing economy, an increasing
</p>
<p>appetite for milk, or other reasons that underlie the growth in supermarket sales of
</p>
<p>milk. This issue is called autocorrelation and means that regression errors are
correlated positively (or negatively) over time. For example, the data in Table 7.1
</p>
<p>are taken from weeks 1 to 30, which means they have a time component.
</p>
<p>We can identify the presence of autocorrelation by using the Durbin-Watson
(D-W) test (Durbin and Watson 1951). The D-W test assesses whether there is
autocorrelation by testing the null hypothesis of no autocorrelation, which is tested
</p>
<p>for negative autocorrelation against a lower and upper bound and for positive
</p>
<p>autocorrelation against a lower and upper bound. If we reject the null hypothesis
</p>
<p>of no autocorrelation, we find support for an alternative hypothesis that there is
</p>
<p>some degree of positive or negative autocorrelation. Essentially, there are four
</p>
<p>situations, which we indicate in Fig. 7.5.
</p>
<p>First, the errors may be positively related (called positive autocorrelation). This
</p>
<p>means that if we have observations over time, we observe that positive errors are
</p>
<p>generally followed by positive errors and negative errors by negative errors. For
</p>
<p>example, supermarket sales usually increase over certain time periods (e.g.,
</p>
<p>before Christmas) and decrease during other periods (e.g., the summer holidays).
</p>
<p>Second, if positive errors are commonly followed by negative errors and negative
</p>
<p>errors by positive errors, we have negative autocorrelation. Negative autocorre-
</p>
<p>lation is less common than positive autocorrelation, but also occurs. If we study,
</p>
<p>for example, how much time salespeople spend on shoppers, we may see that if
</p>
<p>they spend much time on one shopper, they spend less time on the next, allowing
</p>
<p>the salesperson to stick to his/her schedule, or to simply go home on time.
</p>
<p>Third, if no systematic pattern of errors occurs, we have no autocorrelation. This
</p>
<p>absence of autocorrelation is required to estimate standard (OLS) regression
</p>
<p>models.
</p>
<p>Fourth, the D-W values may fall between the lower and upper critical value. If this
</p>
<p>occur, the test is inconclusive.
</p>
<p>The situation that occurs depends on the interplay between the D-W test statistic
</p>
<p>(d) and the lower (dL) and upper (d
U) critical value.
</p>
<p>Indecision IndecisionPositive
</p>
<p>autocorrelation
</p>
<p>No
</p>
<p>autocorrelation
</p>
<p>Negative
</p>
<p>autocorrelation
</p>
<p>dL=1.352 4-dL=2.648d
U=1.489 4-dU=2.511
</p>
<p>Fig. 7.5 Durbin-Watson test values (n &frac14; 30, k &frac14; 1)
</p>
<p>230 7 Regression Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>1. If the test statistic is lower than the lower critical value (d &lt; dL), we have
</p>
<p>positive autocorrelation.
</p>
<p>2. If the test statistic is higher than 4 minus the lower critical value (d&gt; 4 &ndash; dL), we
</p>
<p>have negative autocorrelation.
</p>
<p>3. If the test statistic falls between the upper critical value and 4 minus the upper
</p>
<p>critical value (dU &lt; d &lt; 4 &ndash; dU), we have no autocorrelation.
</p>
<p>4. If the test statistic falls between the lower and upper critical value (dL&lt; d&lt; d
U),
</p>
<p>or it falls between 4 minus the upper critical value and 4 minus the lower critical
</p>
<p>value (4 &ndash; dU &lt; d &lt; 4 &ndash; dL), the test does not inform on the presence of
</p>
<p>autocorrelation and is undecided.
</p>
<p>The critical values dL and d
U can be found on the website accompanying this
</p>
<p>book ( Web Appendix! Downloads). From this table, you can see that the lower
critical value dL of a model with one independent variable and 30 observations is
</p>
<p>1.352 and the upper critical value dU is 1.489. Figure 7.5 shows the resulting
</p>
<p>intervals. Should the D-W test indicate autocorrelation, you should use models
</p>
<p>that account for this problem, such as panel and time series models. We do not
</p>
<p>discuss these methods in this book, but Cameron and Trivedi (2010) is a useful
</p>
<p>source of further information.
</p>
<p>7.3.3.5 Fifth (Optional) Assumption: Error Distribution
The fifth, optional, assumption is that the regression model errors are approximately
</p>
<p>normally distributed. If this is not the case, the t-values may be incorrect. However,
</p>
<p>even if the regression model errors are not normally distributed, the regression
</p>
<p>model still provides good estimates of the coefficients. Consequently, we consider
</p>
<p>this assumption an optional one. Potential reasons for regression errors being
</p>
<p>non-normally distributed include outliers (discussed in Chap. 5) and a non-linear
relationship between the independent and (a) dependent variable(s) as discussed in
</p>
<p>Sect. 7.3.3.1.
</p>
<p>There are two main ways of checking for normally distributed errors: you can
</p>
<p>use plots or carry out a formal test. Formal tests of normality include the Shapiro-
</p>
<p>Wilk test (see Chap. 6), which needs to be run on the saved errors. A formal test
</p>
<p>may indicate non-normality and provide absolute standards. However, formal test
</p>
<p>results reveal little about the source of non-normality. A histogram with a normality
</p>
<p>plot may can help assess why errors are non-normally distributed (see Chap. 5 for
</p>
<p>details). Such plots are easily explained and interpreted and may suggest the source
</p>
<p>of non-normality (if present).
</p>
<p>7.3.4 Interpret the Regression Results
</p>
<p>In the previous sections, we discussed how to specify a basic regression model and
</p>
<p>how to test regression assumptions. We now discuss the regression model fit,
</p>
<p>followed by the interpretation of individual variables&rsquo; effects.
</p>
<p>7.3 Conducting a Regression Analysis 231</p>
<p/>
</div>
<div class="page"><p/>
<p>7.3.4.1 Overall Model Fit
The model significance is the first aspect that should be determined. While model
</p>
<p>significance is not an indicator of (close) fit, it makes little sense to discuss model fit
</p>
<p>if the model itself is not significant. The F-test determines the model significance.
The test statistic&rsquo;s F-value is the result of a one-way ANOVA (see Chap. 6) that
</p>
<p>tests the null hypothesis that all the regression coefficients equal zero. Thus, the
</p>
<p>following null hypothesis is tested:9
</p>
<p>H0&frac14; β1 &frac14; β2 &frac14; β3 &frac14; . . . &frac14; 0
</p>
<p>If the regression coefficients are all equal to zero, then all the independent
</p>
<p>variables&rsquo; effect on the dependent variable is zero. In other words, there is no
</p>
<p>(zero) relationship between the dependent variable and the independent variables. If
</p>
<p>we do not reject the null hypothesis, we need to change the regression model, or, if
</p>
<p>this is not possible, report that the regression model is non-significant. A p-value of
</p>
<p>the F-test below 0.05 (i.e., the model is significant) does not, however, imply that all
</p>
<p>the regression coefficients are significant, or even that one of them is significant
</p>
<p>when considered in isolation. However, if the F-value is significant, it is highly
</p>
<p>likely that at least one or more regression coefficients are significant.
</p>
<p>If we find that the F-test is significant, we can interpret the model fit by using the
</p>
<p>R2. The R2 (also called the coefficient of determination) indicates the degree to
which the model, relative to the mean, explains the observed variation in the
</p>
<p>dependent variable. In Fig. 7.6, we illustrate this graphically by means of a scatter
</p>
<p>plot. The y-axis relates to the dependent variable sales (weekly sales in dollars) and
</p>
<p>the x-axis to the independent variable promotion. In the scatter plot, we see
</p>
<p>30 observations of sales and price (note that we use a small sample size for
</p>
<p>illustration purposes). The horizontal line (at about $5,000 sales per week) refers
</p>
<p>to the average sales in all 30 observations. This is also our benchmark. After all, if
</p>
<p>we were to have no regression line, our best estimate of the weekly sales would also
</p>
<p>be the average. The sum of all the squared differences between each observation
</p>
<p>and the average is the total variation or the total sum of squares (SST). We indicate
</p>
<p>the total variation in only one observation on the right of the scatter plot.
</p>
<p>The straight upward sloping line (starting at the y-axis at about $2,500 sales per
</p>
<p>week when there are no promotional activities) is the regression line that OLS
</p>
<p>estimates. If we want to understand what the regression model adds beyond the
</p>
<p>average (which is the benchmark for calculating the R2), we can calculate the
</p>
<p>difference between the regression line and the line indicating the average. We
</p>
<p>call this the regression sum of squares (SSR), as it is the variation in the data that
</p>
<p>the regression analysis explains. The final point we need to understand regarding
</p>
<p>how well a regression line fits the available data, is the unexplained sum of the
</p>
<p>squares. This is the difference between the observations (indicated by the dots) and
</p>
<p>the regression line. The squared sum of these differences refers to the regression
</p>
<p>error that we discussed previously and which is therefore denoted as the error sum
</p>
<p>9This hypothesis can also be read as that a model with only an intercept is sufficient.
</p>
<p>232 7 Regression Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>of squares (SSE). In more formal terms, we can describe these types of variation as
</p>
<p>follows:
</p>
<p>SST &frac14; SSR &thorn; SSE
</p>
<p>This is the same as:
</p>
<p>Xn
</p>
<p>i&frac14;1
</p>
<p>yi � �y&eth; &THORN;
2 &frac14;
</p>
<p>Xn
</p>
<p>i&frac14;1
</p>
<p>byi � �y&eth; &THORN;
2 &thorn;
</p>
<p>Xn
</p>
<p>i&frac14;1
</p>
<p>yi � byi&eth; &THORN;
2
</p>
<p>Here, n describes the number of observations, yi is the value of the independent
</p>
<p>variable for observation i,byi is the predicted value of observation i, and�y is the mean
value of y. As you can see, this description is like the one-way ANOVA we
</p>
<p>discussed in Chap. 6. A useful regression line should explain a substantial amount
</p>
<p>of variation (have a high SSR) relative to the total variation (SST):
</p>
<p>R2 &frac14;
SSR
</p>
<p>SST
</p>
<p>The R2 always lies between 0 and 1, with a higher R2 indicating a better model fit.
</p>
<p>When interpreting the R2, higher values indicate that the variation in x explains
</p>
<p>more of the variation in y. Therefore, relative to the SSR, the SSE is low.
</p>
<p>0
</p>
<p>0
5
0
0
0
</p>
<p>1
0
0
0
0
</p>
<p>W
e
e
k
ly
</p>
<p> s
a
le
</p>
<p>s
 i
n
 U
</p>
<p>S
D
</p>
<p>1
5
0
0
0
</p>
<p>50 100
</p>
<p>Index of promotional activities
</p>
<p>150
</p>
<p>Fig. 7.6 Explanation of the R2
</p>
<p>7.3 Conducting a Regression Analysis 233</p>
<p/>
</div>
<div class="page"><p/>
<p>It is difficult to provide rules of thumb regarding what R2 is appropriate, as
</p>
<p>this varies from research area to research area. For example, in longitudinal
</p>
<p>studies, R2s of 0.90 and higher are common. In cross-sectional designs,
</p>
<p>values of around 0.30 are common, while values of 0.10 are normal in
</p>
<p>cross-sectional data in exploratory research. In scholarly research focusing
</p>
<p>on marketing, R2 values of 0.50, 0.30, and 0.10 can, as a rough rule of thumb,
</p>
<p>be respectively described as substantial, moderate, and weak.
</p>
<p>If we use the R2 to compare different regression models (but with the same
</p>
<p>dependent variable), we run into problems. If we add irrelevant variables that are
</p>
<p>slightly correlated with the dependent variable, the R2 will increase. Thus, if we only
</p>
<p>use the R2 as the basis for understanding regression model fit, we are biased towards
</p>
<p>selecting regression models with many independent variables. Selecting a model only
</p>
<p>based on the R2 is generally not a good strategy, unless we are interested in making
</p>
<p>predictions. If we are interested in determining whether independent variables have a
</p>
<p>significant relationship with a dependent variable, or when we wish to estimate the
</p>
<p>relative strength of different independent variables&rsquo; effects, we need regression models
</p>
<p>that do a good job of explaining the data (which have a low SSE), but which also have a
</p>
<p>few independent variables. It is easier to recommend that a management should change
</p>
<p>a few key variables to improve an outcome than to recommend a long list of somewhat
</p>
<p>related variables. We also do not want too many independent variables, because they
</p>
<p>are likely to complicate the insights. Consequently, it is best to rely on simple models
</p>
<p>when possible. Relevant variables should, of course, always be included. To avoid a
</p>
<p>bias towards complex models, we can use the adjusted R2 to select regressionmodels.
The adjusted R2 only increases if the addition of another independent variable explains
</p>
<p>a substantial amount of the variance. We calculate the adjusted R2 as follows:
</p>
<p>R2adj &frac14; 1� 1� R
2
</p>
<p>� �
∙
</p>
<p>n� 1
</p>
<p>n� k� 1
</p>
<p>Here, n describes the number of observations and k the number of independent
</p>
<p>variables (not counting the constant α). This adjusted R2 is a relative measure and
</p>
<p>should be used to compare different but nested models with the same dependent
variable. Nested means that all of a simpler model&rsquo;s terms are included in a more
</p>
<p>complex model, as well as additional variables. You should pick the model with the
</p>
<p>highest adjusted R2 when comparing regression models. However, do not blindly
</p>
<p>use the adjusted R2 as a guide, but also look at each individual variable and see if it
</p>
<p>is relevant (practically) for the problem you are researching. Furthermore, it is
</p>
<p>important to note that we cannot interpret the adjusted R2 as the percentage of
</p>
<p>explained variance as we can with the regular R2. The adjusted R2 is only a measure
</p>
<p>of how much the model explains while controlling for model complexity.
</p>
<p>Because the adjusted R2 can only compare nested models, there are additional fit
</p>
<p>indices that can be used to compare models with the same dependent variable, but
</p>
<p>234 7 Regression Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>different independent variables (Treiman 2014). The Akaike information crite-
rion (AIC) and the Bayes information criterion (BIC) are such measures of
model fit. More precisely, AIC and BIC are relative measures indicating the
</p>
<p>difference in information when a set of candidate models with different indepen-
</p>
<p>dent variables is estimated. For example, we can use these criteria to compare two
</p>
<p>models where the first regression model explains the sales by using two indepen-
</p>
<p>dent variables (e.g., price and promotions) and the second model adds one more
</p>
<p>independent variable (e.g., price, promotions, and service quality). We can also use
</p>
<p>the AIC10 and BIC when we explain sales by using two different sets of independent
</p>
<p>variables.
</p>
<p>Both the AIC and BIC apply a penalty (the BIC a slightly larger one), as the
</p>
<p>number of independent variables increases with the sample size (Treiman 2014).
</p>
<p>Smaller values are better and, when comparing models, a rough guide is that when
</p>
<p>the more complex model&rsquo;s AIC (or BIC) is 10 lower than that of another model, the
</p>
<p>former model should be given strong preference (Fabozzi et al. 2014). When the
</p>
<p>difference is less than 2, the simpler model is preferred. For values between 2 and
</p>
<p>10, the evidence shifts towards the more complex model, although a specific cut-off
</p>
<p>point is hard to recommend. When interpreting these statistics, note that the AIC
</p>
<p>tends to point towards a more complex model than the BIC.
</p>
<p>7.3.4.2 Effects of Individual Variables
Having established that the overall model is significant and that the R2 is satisfac-
</p>
<p>tory, we need to interpret the effects of the various independent variables used to
</p>
<p>explain the dependent variable. If a regression coefficient&rsquo;s p-value is below 0.05,
</p>
<p>we generally say that the specific independent variable relates significantly to the
</p>
<p>dependent variable. To be precise, the null and alternative hypotheses tested for an
</p>
<p>individual parameter (e.g., β1) are:
</p>
<p>H0 : β1 &frac14; 0
</p>
<p>H1: β1 6&frac14; 0:
</p>
<p>If a coefficient is significant (i.e., the p-value is below 0.05), we reject the null
</p>
<p>hypothesis and support the alternative hypothesis, concluding that the parameter
</p>
<p>differs significantly from zero. For example, if we estimate a regression model on
</p>
<p>the data shown in Fig. 7.1, the (unstandardized) β1 coefficient of promotional
</p>
<p>activities&rsquo; effect on sales is 54.591, with a t-value of 5.35. This t-value results in
</p>
<p>a p-value less than 0.05, indicating that the effect is significantly different from
</p>
<p>zero. If we hypothesize a direction (i.e., smaller or larger than zero) instead of
</p>
<p>significantly different from zero, we should divide the corresponding p-value by
</p>
<p>10The AIC is specifically calculated as AIC &frac14; n�ln(SSE/n) + 2�k, where n is the number of
observations and k the number of independent variables, while the BIC is calculated as
BIC &frac14; n�ln(SSE/n) + k�ln(n).
</p>
<p>7.3 Conducting a Regression Analysis 235</p>
<p/>
</div>
<div class="page"><p/>
<p>two. This is the same as applying the t-test for a directional effect, which is
</p>
<p>explained in Chap. 6.
</p>
<p>The next step is to interpret the actual size of the β coefficients, which we can
</p>
<p>interpret in terms of unstandardized effects and standardized effects. The unstan-
dardized β coefficient indicates the effect that a one-unit increase in the independent
</p>
<p>variable (on the scale used to measure the original independent variable) has on the
</p>
<p>dependent variable. This effect is therefore the partial relationship between a
</p>
<p>change in a single independent variable and the dependent variable. For example,
</p>
<p>the unstandardized β1 coefficient of promotional activities&rsquo; effect on sales (54.59)
</p>
<p>indicates that a one-unit change in (the index of) promotional activities increases
</p>
<p>sales by 54.59 units. Importantly, if we have multiple independent variables, a
</p>
<p>variable&rsquo;s unstandardized coefficient is the effect of that independent variable&rsquo;s
</p>
<p>increase by one unit, but keeping the other independent variables constant. While
</p>
<p>this is a very simple example, we might run a multiple regression in which the
</p>
<p>independent variables are measured on different scales, such as in dollars, units
</p>
<p>sold, or on Likert scales. Consequently, the independent variables&rsquo; effects cannot be
</p>
<p>directly compared with one another, because their influence also depends on the
</p>
<p>type of scale used. Comparing the unstandardized β coefficients would, in any case,
</p>
<p>amount to comparing apples with oranges!
</p>
<p>Fortunately, the standardized βs allow us to compare the relative effect of
</p>
<p>differently measured independent variables by expressing the effect in terms of
</p>
<p>standard deviation changes from the mean. More precisely, the standardized β
</p>
<p>coefficient expresses the effect that a single standard deviation change in the
</p>
<p>independent variable has on the dependent variable. The standardized β is used to
</p>
<p>compare different independent variables&rsquo; effects. All we need to do is to find the
</p>
<p>highest absolute value, which indicates the variable that has the strongest effect on
</p>
<p>the dependent variable. The second highest absolute value indicates the second
</p>
<p>strongest effect, etc.
</p>
<p>Two further tips: First, only consider significant βs in this respect, as insignifi-
</p>
<p>cant βs do not (statistically) differ from zero! Second, while the standardized βs are
</p>
<p>helpful from a practical point of view, standardized βs only allow for comparing the
</p>
<p>coefficients within and not between models! Even if you just add a single variable to
</p>
<p>your regression model, the standardized βs may change substantially.
</p>
<p>When interpreting (standardized) β coefficients, you should always keep the
</p>
<p>effect size in mind. If a β coefficient is significant, it merely indicates an
</p>
<p>effect that differs from zero. This does not necessarily mean that the effect is
</p>
<p>managerially relevant. For example, we may find a $0.01 sales effect of
</p>
<p>spending $1 more on promotional activities that is statistically significant.
</p>
<p>Statistically, we could conclude that the effect of a $1 increase in promotional
</p>
<p>(continued)
</p>
<p>236 7 Regression Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>activities increases sales by an average of $0.01 (just one dollar cent). While
</p>
<p>this effect differs significantly from zero, we would probably not recommend
</p>
<p>increasing promotional activities in practice (we would lose money on the
</p>
<p>margin) as the effect size is just too small.11
</p>
<p>Another way to interpret the size of individual effects is to use the η2 (pronounced
</p>
<p>eta-squared), which, similar to the R2, is a measure of the variance accounted for.
There are two types of η2: The model η2, which is identical to the R2, and each
</p>
<p>variable&rsquo;s partial η2, which describes how much of the total variance is accounted
</p>
<p>for by that variable. Just like the R2, the η2 can only be used to compare variables
</p>
<p>within a regression model and cannot be used to compare them between regression
</p>
<p>models. The η2 relies on different rules of thumb regarding what are small, medium,
</p>
<p>and large effect sizes. Specifically, an effect of 0.02 is small, 0.15 is medium, and
</p>
<p>0.30 and over is large (Cohen 1992).
</p>
<p>There are also situations in which an effect is not constant for all observations,
</p>
<p>but depends on another variable&rsquo;s values. Researchers can run a moderation
analysis, which we discuss in Box 7.2, to estimate such effects.
</p>
<p>7.3.5 Validate the Regression Results
</p>
<p>Having checked for the assumptions of the regression analysis and interpreted the
</p>
<p>results, we need to assess the regression model&rsquo;s stability. Stability means that the
</p>
<p>results are stable over time, do not vary across different situations, and are not
</p>
<p>heavily dependent on the model specification. We can check for a regression
</p>
<p>model&rsquo;s stability in several ways:
</p>
<p>1. We can randomly split the dataset into two parts (called split-sample valida-
tion) and run the regression model again on each data subset. 70% of the
randomly chosen data are often used to estimate the regression model (called
</p>
<p>estimation sample) and the remaining 30% is used for comparison purposes
(called validation sample). We can only split the data if the remaining 30% still
meets the sample size rules of thumb discussed earlier. If the use of the two
</p>
<p>samples results in similar effects, we can conclude that the model is stable. Note
</p>
<p>that it is mere convention to use 70% and 30% and there is no specific reason for
</p>
<p>using these percentages.
</p>
<p>2. We can also cross-validate our findings on a new dataset and examine whether
</p>
<p>these findings are similar to the original findings. Again, similarity in the
</p>
<p>11Cohen&rsquo;s (1994) classical article &ldquo;The Earth is Round ( p &lt; 0.05)&rdquo; offers an interesting perspec-
tive on significance and effect sizes.
</p>
<p>7.3 Conducting a Regression Analysis 237</p>
<p/>
</div>
<div class="page"><p/>
<p>findings indicates stability and that our regression model is properly specified.
</p>
<p>Cross-validation does, of course, assume that we have a second dataset.
3. We can add several alternative variables to the model and examine whether the
</p>
<p>original effects change. For example, if we try to explain weekly supermarket
</p>
<p>sales, we could use several additional variables, such as the breadth of the
</p>
<p>assortment or the downtown/non-downtown location in our regression model.
</p>
<p>If the basic findings we obtained earlier continue to hold even when adding these
</p>
<p>two new variables, we conclude that the effects are stable. This analysis does, of
</p>
<p>Box 7.2 Moderation
</p>
<p>The discussion of individual variables&rsquo; effects assumes that there is only one
</p>
<p>effect. That is, that only one β parameter represents all observations well.
</p>
<p>This is often not true. For example, the link between sales and price has been
</p>
<p>shown to be stronger when promotional activities are higher. In other words,
</p>
<p>the effect of price (β1) is not constant, but with the level of promotional
</p>
<p>activities.
</p>
<p>Moderation analysis is one way of testing if such heterogeneity is present.
</p>
<p>A moderator variable, usually denoted by m, is a variable that changes the
</p>
<p>strength (or even direction) of the relationship between the independent
</p>
<p>variable (x1) and the dependent variable (y). You only need to create a new
</p>
<p>variable that is the multiplication of x1 and m (i.e., x1�m). The regression
model then takes the following form:
</p>
<p>y &frac14; α&thorn; β1x1 &thorn; β2m&thorn; β3x1 ∙m&thorn; e
</p>
<p>In words, a moderator analysis requires entering the independent variable x1,
the moderator variable m, and the product x1�m, which represents the interac-
tion between the independent variable and the moderator. Moderation analy-
</p>
<p>sis is therefore also commonly referred to as an analysis of interaction
effects. After estimating this regression model, you can interpret the signifi-
cance and sign of the β3 parameter. A significant effect suggests that:
</p>
<p>&ndash; when the sign of β3 is positive, the effect β1 increases as m increases,
</p>
<p>&ndash; when the sign of β3 is negative, the effect β1 decreases as m increases.
</p>
<p>For further details on moderation analysis, please see David Kenny&rsquo;s discus-
</p>
<p>sion on moderation (http://www.davidakenny.net/cm/moderation.htm), or the
</p>
<p>advanced discussion by Aiken and West (1991). Jeremy Dawson&rsquo;s website
</p>
<p>(http://www.jeremydawson.co.uk/slopes.htm) offers a tool for visualizing
</p>
<p>moderation effects. An example of a moderation analysis is found in Mooi
</p>
<p>and Frambach (2009).
</p>
<p>238 7 Regression Analysis</p>
<p/>
<div class="annotation"><a href="http://www.davidakenny.net/cm/moderation.htm">http://www.davidakenny.net/cm/moderation.htm</a></div>
<div class="annotation"><a href="http://www.jeremydawson.co.uk/slopes.htm">http://www.jeremydawson.co.uk/slopes.htm</a></div>
</div>
<div class="page"><p/>
<p>course, require us to have more variables available than those included in the
</p>
<p>original regression model.
</p>
<p>7.3.6 Use the Regression Model
</p>
<p>When we have found a useful regression model that satisfies regression analysis&rsquo;s
</p>
<p>assumptions, it is time to use it. Prediction is a key use of regression models.
</p>
<p>Essentially, prediction entails calculating the values of the dependent variables
</p>
<p>based on assumed values of the independent variables and their related, previously
</p>
<p>calculated, unstandardized β coefficients. Let us illustrate this by returning to our
</p>
<p>opening example. Imagine that we are trying to predict weekly supermarket sales
</p>
<p>(in dollars) (y) and have estimated a regression model with two independent
</p>
<p>variables: price (x1) and an index of promotional activities price (x2). The regres-
</p>
<p>sion model is as follows:
</p>
<p>y &frac14; α&thorn; β1x1 &thorn; β2x2 &thorn; e
</p>
<p>If we estimate this model on the previously used dataset, the estimated coefficients
</p>
<p>using regression analysis are 30,304.05 for the intercept, �25,209.86 for price, and
42.27 for promotions. We can use these coefficients to predict sales in different
</p>
<p>situations. Imagine, for example, that we set the price at $1.10 and the promotional
</p>
<p>activities at 50. Our expectation of the weekly sales would then be:
</p>
<p>y
_
</p>
<p>&frac14; 30, 304:05�25, 209:86 � $1:10&thorn; 42:27 � 50 &frac14; $4, 686:70:
</p>
<p>We could also build several scenarios to plan for different situations by, for
</p>
<p>example, increasing the price to $1.20 and reducing the promotional activities to
</p>
<p>40. By using regression models like this, one can, for example, automate stocking
</p>
<p>and logistical planning, or develop strategic marketing plans.
</p>
<p>Regression can also help by providing insight into variables&rsquo; specific effects. For
</p>
<p>example, if the effect of promotions is not significant, it may tell managers that the
</p>
<p>supermarket&rsquo;s sales are insensitive to promotions. Alternatively, if there is some
</p>
<p>effect, the strength and direction of promotional activities&rsquo; effect may help
</p>
<p>managers understand whether they are useful.
</p>
<p>Table 7.2 summarizes (on the left side) the major theoretical decisions we need
</p>
<p>to make if we want to run a regression model. On the right side, these decisions are
</p>
<p>then &ldquo;translated&rdquo; into Stata actions.
</p>
<p>7.3 Conducting a Regression Analysis 239</p>
<p/>
</div>
<div class="page"><p/>
<p>Table 7.2 Steps involved in carrying out a regression analysis
</p>
<p>Theory Action
</p>
<p>Consider the regression analysis data requirements
</p>
<p>Sufficient sample size Check if sample size is 104+k, where k indicates the
number of independent variables. If the expected
effects are weak (the R2 is .10 or lower), use at least
30 � k observations per independent variable.
</p>
<p>This can be done easily by calculating the
correlation matrix. Note the number of observations
(obs&frac14;. . .) immediately under the correlate
command to determine the sample size available for
regression.
</p>
<p>correlate commitment s9 s10 s19 s21
</p>
<p>s23 status age gender
</p>
<p>Do the dependent and independent
variables show variation?
</p>
<p>Calculate the standard deviation of the variables by
going to► Statistics► Summaries, tables, and tests
► Summary and descriptive statistics ► Summary
statistics (enter the dependent and independent
variables). At the very least, the standard deviation
(indicated by Std. Dev. in the output) should be
greater than 0.
</p>
<p>summarize commitment s9 s10 s19 s21
</p>
<p>s23 i.status age i.gender
</p>
<p>Is the dependent variable interval or ratio
scaled?
</p>
<p>See Chap. 3 to determine the measurement level.
</p>
<p>Is (multi)collinearity present? The presence of (multi)collinearity can only be
assessed after the regression analysis has been
conducted (to run a regression model; ► Statistics
► Linear models and related ► Linear regression.
Under Dependent variable enter the dependent
variable and add all the independent variables under
the box Independent variables and click on OK).
</p>
<p>Check the VIF: ► Statistics ► Postestimation ►
Specification, diagnostic, and goodness-of-fit
analysis► Variance inflation factors. Then click on
Launch and OK. The VIF should be below
10 (although it can be higher, or lower, in some
cases; see Sect. 7.3.1.4 for specifics).
</p>
<p>vif
</p>
<p>Specify and estimate the regression model
</p>
<p>Model specification 1. Pick distinct variables
</p>
<p>2. Try to build a robust model
</p>
<p>3. Consider the variables that are needed to give
advice
</p>
<p>4. Consider whether the number of independent
variables is in relation to the sample size
</p>
<p>Estimate the regression model ► Statistics ► Linear models and related ► Linear
regression. Under Dependent variable enter the
dependent variable and add all the independent
</p>
<p>(continued)
</p>
<p>240 7 Regression Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>Table 7.2 (continued)
</p>
<p>Theory Action
</p>
<p>variables under Independent variables and click
on OK.
</p>
<p>regress commitment s9 s10 s19 s21 s23
</p>
<p>i.status age gender
</p>
<p>Use robust regression when heteroskedasticity is
present:
</p>
<p>regress commitment s9 s10 s19 s21 s23
</p>
<p>i.status age gender, robust
</p>
<p>Test the regression analysis assumptions
</p>
<p>Can the regression model be specified
linearly?
</p>
<p>Consider whether you can write the regression
model as: y&frac14; α+ β1�x1+ β2�x2+ . . . + e
</p>
<p>Is the relationship between the
independent and dependent variables
linear?
</p>
<p>Plot the dependent variable against the independent
variable using a scatterplot matrix to see if the
relation (if any) appears to be linear.► Graphics ►
Scatterplot matrix. Then add all the variables and
click on Marker properties where, under Symbol,
you can choose Point for a clearer matrix. Note that
you cannot add variables that start with i. (i.e.,
categorical variables). Then click on OK.
</p>
<p>graph matrix commitment s9 s10 s19 s21
</p>
<p>s23 status age gender, msymbol(point)
</p>
<p>Conduct Ramsey&rsquo;s RESET test to test for
non-linearities. Go to► Statistics► Postestimation
► Specification, diagnostic, and goodness-of-fit
analysis ► Ramsey regression specification-error
test for omitted variables. Then click on Launch
and OK.
</p>
<p>estat ovtest
</p>
<p>Is the expected mean error of the
regression model zero?
</p>
<p>Choice made on theoretical grounds.
</p>
<p>Are the errors constant (homoscedastic)? Breusch-Pagan test: This can only be checked right
after running a regression model. Go to► Statistics
► Postestimation ► Specification, diagnostic, and
goodness-of-fit analysis ► Tests for
heteroskedasticity (hettest). Then click on Launch
and then OK. Check that the Breusch-Pagan /
Cook-Weisberg test for heteroskedasticity is not
significant. If it is, you can use robust regression to
remedy this.
</p>
<p>estat hettest
</p>
<p>White&rsquo;s test: This can only be checked right after
running a regression model. Go to ► Statistics ►
Postestimation ► Specification, diagnostic, and
goodness-of-fit analysis ► information matrix test
(imtest). Then click on Launch and then OK.
</p>
<p>estat imtest
</p>
<p>Are the errors correlated
(autocorrelation)?
</p>
<p>This can only be checked after running a regression
model and by declaring the time aspect. This means
</p>
<p>(continued)
</p>
<p>7.3 Conducting a Regression Analysis 241</p>
<p/>
</div>
<div class="page"><p/>
<p>Table 7.2 (continued)
</p>
<p>Theory Action
</p>
<p>you need a variable that indicates how the variables
are organized over time. This variable, for example,
week, should be declared in Stata using the tsset
command, for example, tsset week. Then
conduct the Durbin&ndash;Watson test. You can select
this test by going to ► Statistics ► Postestimation
► Specification, diagnostic, and goodness-of-fit
analysis ► Durbin-Watson statistic to test for first-
order serial correlation. Click on Launch and then
OK. The Durbin-Watson test for first-order serial
correlation should not be significant. The critical
values can be found on the website accompanying
</p>
<p>this book ( Web Appendix ! Downloads).
</p>
<p>tsset week
</p>
<p>estat dwatson
</p>
<p>Are the errors normally distributed? This can only be checked after running a regression
model. You should first save the errors by going to
► Statistics ► Postestimation ► Predictions ►
Predictions and their SEs, leverage statistics,
distance statistics, etc. Then click on Launch. Enter
the name of the error variable (we use error in this
chapter), making sure Residuals (equation-level
scores) is ticked, and click on OK.
</p>
<p>You should calculate the Shapiro-Wilk test to test
the normality of the errors. To select the Shapiro-
Wilk test, go to Statistics► Summaries, tables, and
tests ►Distributional plots and tests ► Shapiro-
Wilk normality test. Under Variables enter error
and click on OK. Check if the Shapiro-Wilk test
under Prob&gt;z reports a p-value greater than 0.05.
</p>
<p>To visualize, create a histogram of the errors
containing a standard normal curve: ► Graphics ►
Histogram and enter error. Under ► Density plots,
tick Add normal-density plot.
</p>
<p>predict error, res
</p>
<p>swilk error
</p>
<p>histogram error, normal
</p>
<p>Interpret the regression model
</p>
<p>Consider the overall model fit Check the R2 and significance of the F-value.
</p>
<p>Consider the effects of the independent
variables separately
</p>
<p>Check the (standardized) β. Also check the sign of
the β. Consider the significance of the t-value
(under P&gt;|t| in the regression table).
</p>
<p>To compare models Calculate the AIC and BIC ► Statistics ►
Postestimation ► Specification, diagnostic, and
goodness-of-fit analysis ► Information criteria &ndash;
AIC and BIC. Click on Launch and then OK.
</p>
<p>Check the AIC and BIC, and ascertain if the simpler
model has AIC or BIC values that are at least 2, but
</p>
<p>(continued)
</p>
<p>242 7 Regression Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>7.4 Example
</p>
<p>Let&rsquo;s go back to the Oddjob Airways case study and run a regression analysis on the
</p>
<p>data. Our aim is to explain commitment&mdash;the customer&rsquo;s intention to continue the
</p>
<p>relationship. This variable is formed from three items in the dataset: com1 (&ldquo;I am
</p>
<p>very committed to Oddjob Airways&rdquo;), com2 (&ldquo;My relationship with Oddjob
</p>
<p>Table 7.2 (continued)
</p>
<p>Theory Action
</p>
<p>preferably 10, lower than that of the more complex
model.
</p>
<p>estat ic
</p>
<p>Calculate the standardized effects Check Standardized beta coefficients under the
Reporting tab of the regression dialog box, which
can be found under ► Statistics ► Linear models
and related ► Linear regression ► Reporting
</p>
<p>Determine, sequentially, the highest absolute values
</p>
<p>Calculate the effect size Make sure you have used OLS regression (and not
robust regression). Then go to ► Statistics ►
Postestimation ► Specification, diagnostic, and
goodness-of-fit analysis ► Eta-squared and omega-
squared effect sizes. Then click on Launch and
OK.
</p>
<p>Interpret each eta squared as the percentage of
variance explained (i.e., as that variable&rsquo;s R2). An
effect of individual variables of 0.02 is small, 0.15
is medium, and 0.30 and greater is large.
</p>
<p>Validate the model
</p>
<p>Are the results robust? This can only be done easily using the command
window. First create a random variable.
</p>
<p>set seed 12345
</p>
<p>gen validate &frac14; runiform() &lt; 0.7
</p>
<p>Then run the regression model where you first select
70% and then last 30% of the cases. Do this by
going to ► Statistics ► Linear models and related
► Linear regression. Then click on by/if/in and
under If: (expression) enter validate&frac14;&frac14;1. Then
repeat and enter validate&frac14;&frac14;0.
</p>
<p>regress commitment s9 s10 s19 s21 s23
</p>
<p>i.status age gender, robust if
</p>
<p>validate&frac14;&frac14;1
</p>
<p>regress commitment s9 s10 s19 s21 s23
</p>
<p>i.status age gender, robust if
</p>
<p>validate&frac14;&frac14;0
</p>
<p>Compare the model results to ensure they are equal.
</p>
<p>7.4 Example 243</p>
<p/>
</div>
<div class="page"><p/>
<p>Airways means a lot to me&rdquo;), and com3 (&ldquo;If Oddjob Airways would not exist any
</p>
<p>longer, it would be a hard loss for me&rdquo;). Specifically, it is formed by taking the
</p>
<p>mean of these three variables.12
</p>
<p>Our task is to identify which variables relate to commitment to Oddjob Airways.
</p>
<p>Regression analysis can help us determine which variables relate significantly to
</p>
<p>commitment, while also identifying the relative strength of the different indepen-
</p>
<p>dent variables.
</p>
<p>The Oddjob Airways dataset ( Web Appendix ! Downloads) offers several
variables that may explain commitment (commitment). Based on prior research and
</p>
<p>discussions with Oddjob Airway&rsquo;s management, the following variables have been
</p>
<p>identified as promising candidates:
</p>
<p>&ndash; Oddjob Airways gives you a sense of safety (s9),
</p>
<p>&ndash; The condition of Oddjob Airways&rsquo; aircraft is immaculate (s10),
</p>
<p>&ndash; Oddjob Airways also pays attention to its service delivery&rsquo;s details. (s19),
</p>
<p>&ndash; Oddjob Airways makes traveling uncomplicated (s21), and
</p>
<p>&ndash; Oddjob Airways offers great value for money (s23).
</p>
<p>As additional variables, we add the following three categories to the model: the
</p>
<p>respondent&rsquo;s status (status), age (age), and gender (gender).
</p>
<p>7.4.1 Check the Regression Analysis Data Requirements
</p>
<p>Before we start, let&rsquo;s see if we have a sufficient sample size. The easiest way to do
</p>
<p>this is to correlate all the dependent and independent variables (see Chap. 5) by
</p>
<p>entering all the variables we intend to include in the regression analysis. To do so go
</p>
<p>to► Statistics► Summaries, tables, and tests► Summary and descriptive statistics
</p>
<p>► Correlations and covariances. In the dialog box, enter each variable separately
</p>
<p>(i.e., commitment, s9 s10, etc.) and click on OK.
As is indicated by the first line in Table 7.3, the number of observations is
</p>
<p>973 (obs&frac14;973). Green&rsquo;s (1991) rule of thumb suggests that we need at least 104 + k
observations, where k is the number of independent variables. Since we have
</p>
<p>9 independent variables, we satisfy this criterion. Note that if we treat status as
</p>
<p>the categorical variable, which it is, we need to estimate two parameters for
</p>
<p>status&mdash;one for the Silver and one for the Gold category&mdash;where Blue is the
</p>
<p>baseline (and cannot be estimated). We thus estimate 10 parameters in total (still
</p>
<p>satisfying this criterion). In fact, even if we apply VanVoorhis and Morgan&rsquo;s (2007)
</p>
<p>more stringent criteria of 30 observations per variable, we still have a sufficient
</p>
<p>sample size. In Table 7.3, we can also examine the pairwise correlations to get an
</p>
<p>idea of which independent variables relate to the dependent variable and which of
</p>
<p>them might be collinear.
</p>
<p>12Using the Stata command egen commitment&frac14;rowmean(com1 com2 com3)
</p>
<p>244 7 Regression Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>Next we should ascertain if our variables display some variation. This is very
</p>
<p>easy in Stata when using the summarize command (as described in Chap. 5) by
</p>
<p>going to ► Statistics ► Summaries, tables, and tests ► Summary and descriptive
</p>
<p>statistics ► Summary statistics and by entering the variables in the Variables box.
This, as shown in Table 7.4, results in output indicating the number of observations
</p>
<p>per variable and their means and standard deviations, along with their minimum and
</p>
<p>maximum values. Note that the number of observations in Table 7.3 is 973 and
indicates the number of cases in which we fully observe all the variables in the set.
</p>
<p>However, each variable has a larger number of non-missing observations, which is
</p>
<p>shown in Table 7.4.
</p>
<p>When working with categorical variables, we check whether all observations fall
</p>
<p>into one category. For example, status is coded using thee labels, Blue, Silver, and
</p>
<p>Gold, representing the values 1, 2, and 3. Having information on two categories makes
</p>
<p>information on the last category redundant (i.e., knowing that an observation does not
</p>
<p>fall into the Silver or Gold category implies it is Blue); Stata will therefore only show
</p>
<p>you one less than the total number of categories. The lowest value (here Blue) is
</p>
<p>Table 7.3 Correlation matrix to determine sample size
</p>
<p>correlate commitment s9 s10 s19 s21 s23 status age gender
</p>
<p>(obs=973)
</p>
<p>| commit~t       s9      s10      s19      s21      s23   status      age   gender
</p>
<p>-------------+---------------------------------------------------------------------------------
</p>
<p>commitment |   1.0000
</p>
<p>s9 |   0.3857   1.0000
</p>
<p>s10 |   0.3740   0.6318   1.0000
</p>
<p>s19 |   0.4655   0.5478   0.5673   1.0000
</p>
<p>s21 |   0.4951   0.5342   0.5135   0.6079   1.0000
</p>
<p>s23 |   0.4618   0.4528   0.5076   0.5682   0.5367   1.0000
</p>
<p>status |   0.0388   0.0114  -0.0237  -0.0042  -0.0149  -0.1324   1.0000
</p>
<p>age |   0.1552   0.1234   0.1738   0.0965   0.1083   0.1445   0.0187   1.0000
</p>
<p>gender |  -0.0743   0.0203   0.0388   0.0186  -0.0043  -0.0337   0.2128  -0.0047   1.0000
</p>
<p>Table 7.4 Descriptive statistics to determine variation
</p>
<p>summarize commitment s9 s10 s19 s21 s23 i.status age i.gender
</p>
<p>Variable |        Obs        Mean    Std. Dev.       Min        Max
-------------+---------------------------------------------------------
commitment |      1,065    4.163693    1.739216          1          7
</p>
<p>s9 |      1,036    72.23359    20.71326          1        100
s10 |      1,025    64.53854    21.40811          1        100
s19 |      1,013    57.21027    21.66066          1        100
s21 |      1,028    58.96498    22.68369          1        100
</p>
<p>-------------+---------------------------------------------------------
s23 |      1,065    48.93521   22.71068          1        100
</p>
<p>|
status |
Silver  |      1,065    .2300469    .4210604          0          1
Gold  |      1,065    .1342723    .3411048          0          1
</p>
<p>|
age |      1,065    50.41972    12.27464         19        101
</p>
<p>|
gender |
male  |      1,065    .7370892    .4404212          0          1
</p>
<p>7.4 Example 245</p>
<p/>
</div>
<div class="page"><p/>
<p>removed by default. Categories can be easier to use when the data are nominal or
</p>
<p>ordinal. Note that each category is coded 0 when absent and 1 when present. For
</p>
<p>example, looking at Table 7.4, we can see that .2300469 or 23% of the respondents fall
into the Silver category and .1342723 or 13% into the Gold category (implying that
64% fall into theBlue category).We also did this with the gender variable. You can tell
</p>
<p>Stata to show categories rather than the actual values by using i. in front of the
</p>
<p>variable&rsquo;s name.
</p>
<p>The scale of the dependent variable is interval or ratio scaled. Specifically, three
</p>
<p>7-point Likert scales create the mean of three items that form commitment. Most
</p>
<p>researchers would consider this to be interval or ratio scaled, which meets the OLS
</p>
<p>regression data assumptions.
</p>
<p>We should next check for collinearity. While having no collinearity is important,
</p>
<p>we can only check this assumption after having run a regression analysis. To do so,
</p>
<p>go to ► Statistics ► Linear models and related ► Linear regression. In the dialog
</p>
<p>box that follows (Fig. 7.7), enter the dependent variable commitment under Depen-
dent variable and s9 s10 s19 s21 s23 i.status age gender under Independent
variables. Note that because status has multiple levels, using i. is necessary to
tell you how many observations fall into each category, but be aware that the first
</p>
<p>level is not shown. Then click on OK.
Stata will show the regression output (Table 7.5). However, as the task is to
</p>
<p>check for collinearity, and not to interpret the regression results, we proceed by
</p>
<p>going to ► Statistics ► Postestimation ► Specification, diagnostic, and goodness-
</p>
<p>of-fit analysis ► Variance inflation factors. Then click on Launch and OK.
</p>
<p>Fig. 7.7 The regression dialog box
</p>
<p>246 7 Regression Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>As you can see in Table 7.6, the highest VIF value is 2.07, which is below 10 and
no reason for concern. Note that the individual VIF values are important and not the
</p>
<p>mean VIF, as individual variables might be problematic even though, on average,
</p>
<p>collinearity is not a concern. Note that Stata shows the two status levels Silver and
</p>
<p>Gold as 2 and 3.
Having met all the described requirements for a regression analysis, our next
</p>
<p>task is to interpret the regression analysis results. Since we already had to specify a
</p>
<p>regression model to check the requirement of no collinearity, we know what this
</p>
<p>regression model will look like!
</p>
<p>Table 7.5 Regression output
</p>
<p>regress commitment s9 s10 s19 s21 s23 i.status age gender
</p>
<p>Source |       SS           df       MS      Number of obs   =       973
-------------+---------------------------------- F(9, 963)       =     54.59
</p>
<p>Model |  966.268972         9  107.363219   Prob &gt; F        =    0.0000
Residual |  1893.84455       963  1.96660909   R-squared       =    0.3378
</p>
<p>-------------+---------------------------------- Adj R-squared   =    0.3317
Total |  2860.11352       972  2.94250363   Root MSE        =    1.4024
</p>
<p>------------------------------------------------------------------------------
commitment |      Coef.   Std. Err. t    P&gt;|t|     [95% Conf. Interval]
</p>
<p>-------------+----------------------------------------------------------------
s9 |   .0051594   .0029967     1.72   0.085    -.0007213    .0110401
s10 |   .0006685   .0029835     0.22   0.823   -.0051865    .0065235
s19 |   .0122601   .0030111     4.07   0.000      .006351    .0181691
s21 |   .0186644   .0027365     6.82   0.000     .0132942    .0240345
s23 |   .0157612   .0026255     6.00   0.000     .0106088    .0209135
</p>
<p>|
status |
</p>
<p>Silver  |    .183402   .1117365     1.64   0.101    -.0358732    .4026771
Gold  |   .4277363   .1377598     3.10   0.002     .1573922    .6980804
</p>
<p>|
age |   .0102835   .0038561     2.67   0.008     .0027162    .0178509
</p>
<p>gender |  -.3451731   .1050914    -3.28   0.001    -.5514077   -.1389385
_cons |   1.198751   .2998922     4.00   0.000     .6102336    1.787269
</p>
<p>------------------------------------------------------------------------------
</p>
<p>Table 7.6 Calculation of the variance inflation factors
</p>
<p>vif
</p>
<p>Variable |       VIF       1/VIF  
-------------+----------------------
</p>
<p>s9 |      1.92    0.521005
s10 |      2.01    0.497696
s19 |      2.07    0.483267
s21 |      1.88    0.532030
s23 |      1.75    0.570861
</p>
<p>status |
2  |     1.11    0.902297
3  |      1.12    0.896848
age |      1.05    0.951353
</p>
<p>gender |      1.06    0.946307
-------------+----------------------
</p>
<p>Mean VIF |      1.55
</p>
<p>7.4 Example 247</p>
<p/>
</div>
<div class="page"><p/>
<p>7.4.2 Specify and Estimate the Regression Model
</p>
<p>We know exactly which variables to select for this model: commitment, as the
</p>
<p>dependent variable, and s9, s10, s19, s21, s23, status, age, and gender as the
</p>
<p>independent variables. Run the regression analysis again by going to ► Statistics
</p>
<p>► Linear models and related ► Linear regression. Having entered the dependent
</p>
<p>and independent variables in the corresponding boxes, click on the SE/Robust tab.
Stata will show you several estimation options (Fig. 7.8). You should maintain
</p>
<p>the Default standard errors, which is identical toOrdinary least squares (OLS).
However, when heteroskedasticity is present, use Robust standard errors.
</p>
<p>Next, click on the Reporting tab (Fig. 7.9). Under this tab, you find several
options, including reporting the Standardized beta coefficients. You can also
change the confidence level to 0.90 or 0.99, as discussed in Chap. 6. Under Set
table formats, you can easily select how you want the regression results to be
reported, for example, the number of decimals, US or European notation (1,000.00
</p>
<p>vs. 1.000,00), and whether you want to see leading zeros (0.00 vs .00).
</p>
<p>Next click on OK. This produces the same output as in Table 7.5. Before we
interpret the output, let&rsquo;s first consider the assumptions.
</p>
<p>Fig. 7.8 The SE/Robust tab
</p>
<p>248 7 Regression Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>7.4.3 Test the Regression Analysis Assumptions
</p>
<p>The first assumption is whether the regression model can be expressed linearly.
</p>
<p>Since no variable transformations occurred, with the exception of categorizing the
</p>
<p>status variable, we meet this assumption, because we can write the regression
</p>
<p>model linearly as:
</p>
<p>commitment &frac14; α&thorn; β1s9&thorn; β2s10&thorn; β3s19&thorn; β4s21&thorn; β5s23&thorn; β6status Silver
&thorn; β7status Gold &thorn; β8age&thorn; β9gender &thorn; e
</p>
<p>Note that because status has three levels, two (three minus one) variables are
</p>
<p>used to estimate the effect of status; consequently, we have two βs.
</p>
<p>Separately, we also check whether the relationships between the independent
</p>
<p>and dependent variables are linear. To do this, create a scatterplot matrix of the
</p>
<p>dependent variable against all the independent variables. This matrix is a combina-
</p>
<p>tion of all scatterplots (in Chap. 5). To do this, go to ► Graphics ► Scatterplot
</p>
<p>matrix. Then add all the variables and click on Marker properties, where, under
Symbol, you can choose Point for a clearer matrix. Note that you cannot add
variables that start with i. (i.e., categorical variables) and we therefore just enter
</p>
<p>status (and not i.status). Then click on OK, after which Stata produces a graph
similar to Fig. 7.10. To interpret this graph, look at the first cell, which reads
</p>
<p>commitment. All the scatterplots in the first row (and not the column, as this shows
the transpose) show the relationship between the dependent variable and each
</p>
<p>Fig. 7.9 The Reporting tab
</p>
<p>7.4 Example 249</p>
<p/>
</div>
<div class="page"><p/>
<p>independent variable. The large number of dots makes it difficult to see whether the
</p>
<p>relationships are linear. However, linearity is also not clearly rejected. Note that the
</p>
<p>cells Traveler status and Gender show three (two) distinct bands. This is because
these variables take on three distinct values (Blue, Silver, and Gold) for status and
</p>
<p>two ( female and male) for gender. When an independent variable has a small
</p>
<p>number of categories, linearity is not important, although you can still see the form
</p>
<p>that the relationship might take.
</p>
<p>We can also test for the presence of nonlinear relationships between the inde-
</p>
<p>pendent and dependent variable by means of Ramsey&rsquo;s RESET test. Go to ►
</p>
<p>Statistics ► Postestimation ► Specification, diagnostic, and goodness-of-fit analy-
</p>
<p>sis ► Ramsey regression specification-error test for omitted variables. Then click
</p>
<p>on Launch and OK.
The results in Table 7.7 of this test under Prob&gt; F suggest no non-linearities are
</p>
<p>present, as the p-value (0.3270) is greater than 0.05. Bear in mind, however, that
this test does not consider all forms of non-linearities.
</p>
<p>To check the second assumption, we should assess whether the regression
</p>
<p>model&rsquo;s expected mean error is zero. Remember, this choice is made on theoretical
</p>
<p>grounds and there is no empirical test for this. We have a randomly drawn sample
</p>
<p>from the population and the model is similar in specification to other models
</p>
<p>Fig. 7.10 A scatterplot matrix of the dependent variable against all the independent variables
</p>
<p>250 7 Regression Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>explaining commitment. This makes it highly likely that the regression model&rsquo;s
</p>
<p>expected mean error is zero.
</p>
<p>The third assumption is that of homoscedasticity. To test for this, use the
</p>
<p>Breusch-Pagan test and go to ► Statistics ► Postestimation ► Specification,
</p>
<p>diagnostic, and goodness-of-fit analysis ► Tests for heteroskedasticity. Then
</p>
<p>click on Launch and OK.
The output in Table 7.8 shows the results of the Breusch-Pagan / Cook-Weisberg
</p>
<p>test for heteroskedasticity. With a p-value (Prob &gt; chi2) of 0.0034, we should
reject the null hypothesis that the error variance is constant, thus suggesting that the
</p>
<p>error variance is not constant.
</p>
<p>To test for heteroskedasticity by means of White&rsquo;s test, go to ► Statistics ►
</p>
<p>Postestimation ► Specification, diagnostic, and goodness-of-fit analysis ► Infor-
</p>
<p>mation matrix test (imtest). Then click on Launch and then OK.
As you can see in Table 7.9, the output consists of four tests. Under
</p>
<p>Heteroskedasticity, the first element is the most important, as it gives an indication
of whether heteroskedasticity is present. The null hypothesis is that there is no
</p>
<p>heteroskedasticity. In Table 7.9, this null hypothesis is rejected and the findings
</p>
<p>suggest that heteroskedasticity is present. Both the Breusch-Pagan and White&rsquo;s test
</p>
<p>are in agreement. Consequently, we should use a robust estimator; this option is
</p>
<p>shown in Fig. 7.8. as Robust. Please note that because we now use a robust
estimator, Stata no longer shows the adjusted R2 and we can only use the AIC
</p>
<p>and BIC to compare the models.
</p>
<p>If we had data with a time component, we would also perform the Durbin-
</p>
<p>Watson test to check for potential autocorrelation (fourth assumption). This
</p>
<p>Table 7.7 Ramsey&rsquo;s RESET test
</p>
<p>estat ovtest
</p>
<p>Ramsey RESET test using powers of the fitted values of commitment
Ho:  model has no omitted variables
</p>
<p>F(3, 960) =      1.15
Prob &gt; F =      0.3270
</p>
<p>Table 7.8 Breusch-Pagan test for heteroskedasticity
</p>
<p>estat hettest
</p>
<p>Breusch-Pagan / Cook-Weisberg test for heteroskedasticity 
Ho: Constant variance
Variables: fitted values of commitment
</p>
<p>chi2(1)      =     8.59
Prob &gt; chi2  =   0.0034
</p>
<p>7.4 Example 251</p>
<p/>
</div>
<div class="page"><p/>
<p>requires us to first specify a time component, which is absent in the dataset;
</p>
<p>however, if we had access to a time variable, say week, we could time-set the
</p>
<p>data by using tsset week. We can then use the command estat dwatson to
</p>
<p>calculate the Durbin-Watson d-statistic and check whether autocorrelation is pres-
</p>
<p>ent. However, since the data do not include any time component, we should not
</p>
<p>conduct this test.
</p>
<p>Lastly, we should explore how the errors are distributed. We first need to save
</p>
<p>the errors to do so by going to ► Statistics ► Postestimation ► Predictions ►
</p>
<p>Predictions and their SEs, leverage statistics, distance statistics, etc. Then click on
</p>
<p>Launch. In the dialog box that opens (Fig. 7.11), enter error under New variable
name and tick Residuals (equation-level scores), which is similar to what we
discussed at the beginning of this chapter. There are also several other types of
</p>
<p>variables that Stata can save. The first option, Linear prediction (xb), saves the
predicted values. The other options are more advanced and discussed in detail in the
</p>
<p>Stata Manual (StataCorp 2015).
</p>
<p>Next, click onOK. Stata now saves the errors so that they can be used to test and
visualize whether they are normally distributed. To test for normality, you should
</p>
<p>run the Shapiro-Wilk test (Chap. 6) by going go to ► Statistics ► Summaries,
</p>
<p>tables, and tests ► Distributional plots and tests ► Shapiro-Wilk normality test. In
</p>
<p>the dialog box that opens (Fig. 7.12), enter error under Variables and click onOK.
The output in Table 7.10 indicates a p-value (Prob &gt; z) of 0.08348, suggesting that
the errors are approximately normally distributed. Hence, we can interpret the
</p>
<p>regression parameters&rsquo; significance by using t-tests.
</p>
<p>We also create a histogram of the errors comprising a standard normal curve. To
</p>
<p>do so, go to► Graphics► Histogram and enter error under Variable. Click on the
Density plots tab and tick Add normal-density plot. The chart in Fig. 7.13 also
suggests that our data are normally distributed, as the bars indicating the frequency
</p>
<p>of the errors generally follow a normal curve.
</p>
<p>Table 7.9 White&rsquo;s test for heteroskedasticity
</p>
<p>estat imtest
</p>
<p>Cameron &amp; Trivedi's decomposition of IM-test
</p>
<p>---------------------------------------------------
Source |       chi2     df      p
</p>
<p>---------------------+-----------------------------
Heteroskedasticity |      78.04     50    0.0068
</p>
<p>Skewness |      30.73      9    0.0003
Kurtosis |       6.70      1    0.0097
</p>
<p>---------------------+-----------------------------
Total |     115.47     60    0.0000
</p>
<p>---------------------------------------------------
</p>
<p>252 7 Regression Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>Fig. 7.11 Saving the predicted errors
</p>
<p>Fig. 7.12 Test for the errors&rsquo; normality
</p>
<p>7.4 Example 253</p>
<p/>
</div>
<div class="page"><p/>
<p>7.4.4 Interpret the Regression Results
</p>
<p>Although we have already conducted a regression analysis to test the assumptions,
</p>
<p>let&rsquo;s run the analysis again, but this time with robust standard errors because we found
</p>
<p>evidence of heteroskedasticity. To run the regression, go to ► Statistics ► Linear
</p>
<p>models and related ► Linear regression. Under Dependent variable, enter the
dependentvariablecommitmentandaddall the independentvariables s9 s10s19s21s23
</p>
<p>i.status age gender under Independent variables. Then click on SE/Robust, select
Robust, followed by OK. Table 7.11 presents the regression analysis results.
</p>
<p>Table 7.11 has of two parts; on top, you find the overall model information
</p>
<p>followed by information on the individual parameters (separated by -----). In the
</p>
<p>section on the overall model, we first see that the number of observations is 973. Next
</p>
<p>is the F-test, whose p-value of 0.000 (less than 0.05) suggests a significant model.13
</p>
<p>Further down, we find that the model yields an R2 value of 0.3378, which seems
satisfactory and is above the value of 0.30 that is common for cross-sectional research.
</p>
<p>Table 7.10 The Shapiro-Wilk test for normality
</p>
<p>swilk error
</p>
<p>Shapiro-Wilk W test for normal data
</p>
<p>Variable |        Obs       W           V         z       Prob&gt;z
-------------+------------------------------------------------------
</p>
<p>error |        973    0.99716      1.748     1.382    0.08348
</p>
<p>-4
</p>
<p>0
.1
</p>
<p>.2
.3
</p>
<p>-2 0 2 4
</p>
<p>Residuals
</p>
<p>D
e
n
s
it
y
</p>
<p>Fig. 7.13 A histogram and normal curve to visualize the error distribution
</p>
<p>13Note that a p-value is never exactly zero, but has values different from zero in later decimal
places.
</p>
<p>254 7 Regression Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>In the section on the individual parameters, we find, from left to right, informa-
</p>
<p>tion on the included variables (with the dependent commitment listed on top), the
</p>
<p>coefficients, the robust standard errors, the t-values and associated p-values
</p>
<p>(indicated as P &gt; |t|), and the confidence intervals. First, you should look at the
individual coefficients. For example, for s19, we get an effect of 0.0122601,
suggesting that when variable s19 moves up by one unit, the dependent variable
</p>
<p>commitment goes up by .0122601 units. Under P &gt; |t|, we find that the regression
coefficient of s19 is significant, as the p-value is smaller 0.105 is greater than 0.05.
Conversely, s9 has no significant effect on commitment, as the corresponding p-
</p>
<p>value of 0.105 is greater than 0.05. Further analyzing the output, we find that the
variables s21, s23, status Gold, age, and gender have significant effects. Note that
</p>
<p>of all the status variables, only the coefficient of the tier status Gold is significant,
</p>
<p>whereas the coefficient of status Silver is not. A particular issue with these categor-
</p>
<p>ical variables is that if you change the base category to, for example, Silver, neither
</p>
<p>Gold nor Blue will be significant. Always interpret significant findings of categori-
</p>
<p>cal variables in relation to their base category. That is, you can claim that Gold
</p>
<p>status travelers have a significantly higher commitment than those of a Blue
</p>
<p>Table 7.11 Regression output
</p>
<p>regress commitment s9 s10 s19 s21 s23 i.status age gender, robust
</p>
<p>Linear regression                               Number of obs     =        973
</p>
<p>F(9, 963)         =      80.20
</p>
<p>Prob &gt; F          =     0.0000
</p>
<p>R-squared         =     0.3378
</p>
<p>Root MSE          =     1.4024
</p>
<p>------------------------------------------------------------------------------
</p>
<p>|               Robust
</p>
<p>commitment |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
</p>
<p>-------------+----------------------------------------------------------------
</p>
<p>s9 |   .0051594   .0031804     1.62   0.105    -.0010819    .0114007
</p>
<p>s10 |   .0006685   .0032532     0.21   0.837    -.0057157    .0070527
</p>
<p>s19 |   .0122601   .0032491     3.77   0.000      .005884    .0186361
</p>
<p>s21 |   .0186644    .002807     6.65   0.000     .0131558     .024173
</p>
<p>s23 |   .0157612   .0027849     5.66   0.000     .0102961    .0212263
</p>
<p>|
</p>
<p>status |
</p>
<p>Silver  |    .183402   .1152451     1.59   0.112    -.0427585    .4095624
</p>
<p>Gold  |   .4277363   .1333499     3.21   0.001     .1660463    .6894263
</p>
<p>|
</p>
<p>age |   .0102835    .003807     2.70   0.007     .0028125    .0177546
</p>
<p>gender |  -.3451731   .1029958    -3.35   0.001    -.5472952    -.143051
</p>
<p>_cons |   1.198751    .289718     4.14   0.000     .6301998    1.767303
</p>
<p>------------------------------------------------------------------------------
</p>
<p>7.4 Example 255</p>
<p/>
</div>
<div class="page"><p/>
<p>status.14 The coefficient of gender is significant and negative. Because the variable
</p>
<p>gender is scaled 0 (female) to 1 (male), this implies that males (the higher value)
</p>
<p>show less commitment to Oddjob Airways than females. Note that because the
</p>
<p>gender variable is measured binary (i.e., it is a dummy variable; see Chap. 5), it is
</p>
<p>always relative to the other gender and therefore always significant. Only when
</p>
<p>there are 3 or more categories does the interpretation issue, which we saw regarding
</p>
<p>status, occur. Specifically, on average, a male customer shows�.3451731 units less
commitment.
</p>
<p>In this example, we estimate one model as determined by prior research and the
</p>
<p>company management input. However, in other instances, we might have alterna-
</p>
<p>tive models, which we wish to compare in terms of their fit. In Box 7.3, we describe
</p>
<p>how to do this by using the relative fit statistics AIC and BIC.
</p>
<p>Next, we should check the standardized coefficients and effect sizes to get an
</p>
<p>idea of which variables are most important. This cannot be read from the t-values or
</p>
<p>p-values! To calculate the standardized β coefficients, return to ► Statistics ►
</p>
<p>Linear models and related ► Linear regression. In the Reporting tab, check the
Standardized beta coefficients and click on OK. This will produce the output in
Table 7.13. To interpret the standardized β coefficents, look at the largest absolute
</p>
<p>number, which is .4277363 for the variable status Gold. The second highest value
relates to gender (�.3451731) and is binary. While the third-highest is s21
(&ldquo;Oddjob Airways makes traveling uncomplicated&rdquo;). These variables contribute
</p>
<p>the most in this order.15 Note, however, that gender is not a variable that marketing
</p>
<p>Box 7.3 Model Comparison Using AIC and BIC
</p>
<p>When comparing different models with the same dependent variable (e.g.,
</p>
<p>commitment), but with different independent variables, we can compare the
</p>
<p>models&rsquo; adequacy by means of the AIC and BIC statistics. To do this, go to
</p>
<p>Statistics ► Postestimation ► Specification, diagnostic, and goodness-of-fit
</p>
<p>analysis ► Information criteria &ndash; AIC and BIC. Click on Launch and then
OK. Stata will then show the output as in Table 7.12. The AIC and BIC are
respectively listed as 3429.253 and 3478.057. Remember that the AIC and
</p>
<p>BIC can be used to compare different models. For example, we can drop age
</p>
<p>and gender from the previous model and calculate the AIC and BIC again.
</p>
<p>Although we do not show this output, the resultant AIC and BIC would then
</p>
<p>respectively be 3443.283 and 3482.326, which are higher, indicating worse fit
</p>
<p>and suggesting that our original specification is better.
</p>
<p>(continued)
</p>
<p>14Note that it is possible to show all categories for regression tables by typing set
showbaselevels on. This can be made permanent by typing set showbaselevels on,
permanent.
15Note that while the constant has the highest value (1.19), this is not a coefficient and should not
be interpreted as an effect size.
</p>
<p>256 7 Regression Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>managers can change and the number of people flying determines the status.
</p>
<p>Making flying with Oddjob airways less complicated may be something marketing
</p>
<p>managers can influence.
</p>
<p>To obtain a better understanding of the effect sizes, we can calculate the η2.
</p>
<p>Effect sizes can only be calculated when OLS regression and not robust regression
</p>
<p>is used. Therefore, run the regression model without the, robust option and go to
</p>
<p>► Statistics ► Postestimation ► Specification, diagnostic, and goodness-of-fit
</p>
<p>analysis ► Eta-squared and omega-squared effect sizes. Then click on Launch
and OK. Stata will calculate the effect sizes, as shown in Table 7.14. These effect
</p>
<p>Table 7.13 Regression output
</p>
<p>regress commitment s9 s10 s19 s21 s23 i.status age gender, vce(robust) beta
</p>
<p>Linear regression                               Number of obs     =        973
</p>
<p>F(9, 963)         =      80.20
</p>
<p>Prob &gt; F          =     0.0000
</p>
<p>R-squared         =     0.3378
</p>
<p>Root MSE          =     1.4024
</p>
<p>------------------------------------------------------------------------------
</p>
<p>|               Robust
</p>
<p>commitment |      Coef.   Std. Err.      t    P&gt;|t|                     Beta
</p>
<p>-------------+----------------------------------------------------------------
</p>
<p>s9 |   .0051594   .0031804     1.62   0.105                 .0625475
</p>
<p>s10 |   .0006685   .0032532     0.21   0.837                  .008328
</p>
<p>s19 |   .0122601   .0032491     3.77   0.000                 .1535821
</p>
<p>s21 |   .0186644    .002807     6.65   0.000                 .2451992
</p>
<p>s23 |   .0157612   .0027849     5.66   0.000                 .2083433
</p>
<p>|
</p>
<p>status |
</p>
<p>Silver  |    .183402   .1152451     1.59   0.112                 .0453108
</p>
<p>Gold  |   .4277363   .1333499     3.21   0.001                 .0859729
</p>
<p>|
</p>
<p>age |   .0102835    .003807     2.70   0.007                 .0716953
</p>
<p>gender |  -.3451731   .1029958    -3.35   0.001                -.0885363
</p>
<p>_cons |   1.198751    .289718     4.14   0.000                        .
</p>
<p>------------------------------------------------------------------------------
</p>
<p>Box 7.3 (continued)
</p>
<p>Table 7.12 Relative measures of fit
</p>
<p>estat ic
</p>
<p>Akaike's information criterion and Bayesian information criterion
</p>
<p>-----------------------------------------------------------------------------
</p>
<p>Model |        Obs  ll(null)  ll(model)      df         AIC        BIC
</p>
<p>-------------+---------------------------------------------------------------
</p>
<p>. |        973 -1905.187  -1704.627      10    3429.253   3478.057
</p>
<p>-----------------------------------------------------------------------------
</p>
<p>7.4 Example 257</p>
<p/>
</div>
<div class="page"><p/>
<p>sizes can be interpreted as the R2, but for each individual variable in that specific
</p>
<p>model. First, the overall η2 of .3378429 is identical to the model R2 as shown in
Table 7.11. We should consider the largest value of the individual variables (s21) as
</p>
<p>the most important variable, because it contributes the most to the explained
</p>
<p>variance (4.6% or, specifically, .0460813). Although this is the largest value,
Cohen&rsquo;s (1992) rules of thumb suggest this is a small effect size.16
</p>
<p>7.4.5 Validate the Regression Results
</p>
<p>Next, we need to validate the model. Let&rsquo;s first split-validate our model. This can
</p>
<p>only be done by means of easy instructions in the command window. First, we need
</p>
<p>to create a variable that helps us select two samples. A uniform distribution is very
</p>
<p>useful for this purpose, which we can make easily by typing set seed 12345 in
</p>
<p>the command window (press enter), followed by gen validate&frac14;runiform
() &lt; 0.7. The first command tells Stata to use random numbers, but since we fix
</p>
<p>the &ldquo;seed,&rdquo; we can replicate these numbers later.17 The second part makes a new
</p>
<p>variable called validate, which has the values zero and one. We can use this variable
</p>
<p>to help Stata select a random 70% and 30% of cases. This requires us to run the
</p>
<p>regression model again and selecting the first 70% and the last 30% of the cases.
</p>
<p>Let&rsquo;s first estimate our model over the 70% of cases. Do this by going to ►
</p>
<p>Statistics ► Linear models and related ► Linear regression. Then click on by/if/
</p>
<p>Table 7.14 Effect sizes
</p>
<p>estat esize
</p>
<p>Effect sizes for linear models
</p>
<p>-------------------------------------------------------------------
</p>
<p>Source |   Eta-Squared     df     [95% Conf. Interval]
</p>
<p>--------------------+----------------------------------------------
</p>
<p>Model |   .3378429         9      .286636    .3754686
</p>
<p>|
</p>
<p>s9 |   .0030688         1            .    .0138597
</p>
<p>s10 |   .0000521         1            .    .0041106
</p>
<p>s19 |   .0169237         1 .0045586    .0364154
</p>
<p>s21 |   .0460813         1     .0236554    .0743234
</p>
<p>s23 |   .0360722         1      .016498    .0619021
</p>
<p>status |   .0107431         2     .0010116    .0259672
</p>
<p>age |   .0073311         1     .0005013     .021729
</p>
<p>gender |   .0110784         1     .0017974    .0277598
</p>
<p>-------------------------------------------------------------------
</p>
<p>16Please note that only Stata 13 or above feature built-in routines to calculate η2.
17The seed specifies the initial value of the random-number generating process such that it can be
replicated later.
</p>
<p>258 7 Regression Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>in and under IF:(expression) enter validate&frac14;&frac14;1 and click on OK. Stata will now
estimate the regression model using 70% of the observations (i.e., the estimation
</p>
<p>sample). When repeating the process for the remaining 30%, enter validate&frac14;&frac14;0
under IF:(expression) (i.e., the validation sample). Table 7.15 shows the results of
these two model estimations. As we can see, the models are quite similar (also when
</p>
<p>compared to the original model), but the effect of age is not significant in the second
</p>
<p>Table 7.15 Assessing robustness
</p>
<p>regress commitment s9 s10 s19 s21 s23 i.status age gender if validate==1, vce(robust) 
</p>
<p>Linear regression                               Number of obs     =        687
</p>
<p>F(9, 677)         =      53.40
</p>
<p>Prob &gt; F          =     0.0000
</p>
<p>R-squared         =     0.3330
</p>
<p>Root MSE          =     1.3969
</p>
<p>------------------------------------------------------------------------------
</p>
<p>|               Robust
</p>
<p>commitment |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
</p>
<p>-------------+----------------------------------------------------------------
</p>
<p>s9 |   .0037881    .003764     1.01   0.315    -.0036024    .0111786
</p>
<p>s10 |   .0051292   .0038798     1.32   0.187    -.0024886     .012747
</p>
<p>s19 |   .0095681   .0037821     2.53   0.012     .0021421     .016994
</p>
<p>s21 |   .0176205   .0033582     5.25   0.000     .0110269    .0242142
</p>
<p>s23 |   .0155433   .0033412     4.65   0.000      .008983    .0221036
</p>
<p>|
</p>
<p>status |
</p>
<p>Blue  |          0  (base)
</p>
<p>Silver  |   .1400931   .1376231     1.02   0.309    -.1301264    .4103126
</p>
<p>Gold  |   .3958519   .1634049     2.42   0.016     .0750106    .7166931
</p>
<p>|
</p>
<p>age |   .0099883   .0044713     2.23   0.026     .001209    .0187676
</p>
<p>gender |  -.2638042    .124435    -2.12   0.034    -.5081291   -.0194794
</p>
<p>_cons |   1.177727   .3408402     3.46   0.001     .5084958    1.846958
</p>
<p>------------------------------------------------------------------------------
</p>
<p>regress commitment s9 s10 s19 s21 s23 i.status age gender if validate==0, vce(robust) 
</p>
<p>Linear regression                               Number of obs     =        286
</p>
<p>F(9, 276)         =      34.58
</p>
<p>Prob &gt; F          =     0.0000
</p>
<p>R-squared         =     0.3694
</p>
<p>Root MSE          =     1.4105
</p>
<p>------------------------------------------------------------------------------
</p>
<p>|               Robust
</p>
<p>commitment |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
</p>
<p>-------------+----------------------------------------------------------------
</p>
<p>s9 |   .0070651   .0060363     1.17   0.243     -.004818    .0189482
</p>
<p>s10 |  -.0091891   .0058937    -1.56   0.120    -.0207915    .0024132
</p>
<p>s19 |   .0186433   .0063824     2.92   0.004     .0060788    .0312078
</p>
<p>s21 |   .0223386   .0048513     4.60   0.000     .0127884    .0318888
</p>
<p>s23 |   .0157948    .005125     3.08   0.002     .0057057    .0258839
</p>
<p>|
</p>
<p>status |
</p>
<p>Blue  |          0  (base)
</p>
<p>Silver  |   .2740726   .2103388     1.30   0.194 -.1399995    .6881447
</p>
<p>Gold  |   .5126804   .2389762     2.15   0.033     .0422327    .9831281
</p>
<p>|
</p>
<p>age |   .0099737   .0071877     1.39   0.166    -.0041761    .0241234
</p>
<p>gender |  -.5085486   .1843601    -2.76   0.006    -.8714793   -.1456179
</p>
<p>_cons |   1.245528   .5731065     2.17   0.031     .1173124    2.373743
</p>
<p>------------------------------------------------------------------------------
</p>
<p>7.4 Example 259</p>
<p/>
</div>
<div class="page"><p/>
<p>model, although the coefficient is very similar. This suggests that the effects are
</p>
<p>robust.
</p>
<p>As we have no second dataset available, we cannot re-run the analysis to
</p>
<p>compare. We do, however, have access to other variables such as country. If we
</p>
<p>add this variable, all the variables that were significant at p &lt; 0.05 remain signifi-
</p>
<p>cant, while country is not significant. Based on this result, we can conclude that the
</p>
<p>results are stable.
</p>
<p>7.5 Farming with AgriPro (Case Study)
</p>
<p>AgriPro (http://www.agriprowheat.com) is a firm based in Colorado, USA, which
</p>
<p>does research on and produces genetically modified wheat seed. Every year,
</p>
<p>AgriPro conducts thousands of experiments on different varieties of wheat seeds
</p>
<p>in different USA locations. In these experiments, the agricultural and economic
</p>
<p>characteristics, regional adaptation, and yield potential of different varieties of
</p>
<p>wheat seeds are investigated. In addition, the benefits of the wheat produced,
</p>
<p>including the milling and baking quality, are examined. If a new variety of wheat
</p>
<p>seed with superior characteristics is identified, AgriPro produces and markets it
</p>
<p>throughout the USA and parts of Canada.
</p>
<p>AgriPro&rsquo;s product is sold to farmers through their distributors, known in the
</p>
<p>industry as growers. Growers buy wheat seed from AgriPro, grow wheat, harvest
</p>
<p>the seeds, and sell the seed to local farmers, who plant them in their fields. These
</p>
<p>growers also provide the farmers, who buy their seeds, with expert local knowledge
</p>
<p>about management and the environment.
</p>
<p>AgriPro sells its products to these growers in several geographically defined
</p>
<p>markets. These markets are geographically defined, because the different local
</p>
<p>conditions (soil, weather, and local plant diseases) force AgriPro to produce
</p>
<p>different products. One of these markets, the heartland region of the USA, is an
</p>
<p>important AgriPro market, but the company has been performing below the man-
</p>
<p>agement expectations in it. The heartland region includes the states of Ohio,
</p>
<p>Indiana, Missouri, Illinois, and Kentucky.
</p>
<p>To help AgriPro understand more about farmers in the heartland region, it
</p>
<p>commissioned a marketing research project involving the farmers in these states.
</p>
<p>AgriPro, together with a marketing research firm, designed a survey, which
</p>
<p>included questions regarding what farmers planting wheat find important, how
</p>
<p>they obtain information on growing and planting wheat, what is important for
</p>
<p>their purchasing decision, and their loyalty to and satisfaction with the top five
</p>
<p>wheat suppliers (including AgriPro). In addition, questions were asked about how
</p>
<p>many acres of farmland the respondents farm, how much wheat they planted, how
</p>
<p>old they were, and their level of education.
</p>
<p>This survey was mailed to 650 farmers from a commercial list that includes
</p>
<p>nearly all farmers in the heartland region. In all, 150 responses were received,
</p>
<p>resulting in a 23% response rate. The marketing research firm also assisted AgriPro
</p>
<p>260 7 Regression Analysis</p>
<p/>
<div class="annotation"><a href="http://www.agriprowheat.com">http://www.agriprowheat.com</a></div>
</div>
<div class="page"><p/>
<p>to assign variable names and labels. They did not delete any questions or
</p>
<p>observations due to nonresponse to items.
</p>
<p>Your task is to analyze the dataset further and, based on the dataset, provide the
</p>
<p>AgriPro management with advice. This dataset is labeled agripro.dta and is avail-
</p>
<p>able in the Web Appendix (! Chap. 7 ! Downloads). Note that the dataset
contains the variable names and labels matching those in the survey. In the Web
</p>
<p>Appendix ( Web Appendix ! Downloads), we also include the original survey.18
</p>
<p>To help you with this task, AgriPro has prepared several questions that it would like
</p>
<p>to see answered:
</p>
<p>1. What do these farmers find important when growing wheat? Please describe the
</p>
<p>variables import1 (&ldquo;Wheat fulfills my rotational needs&rdquo;), import2 (&ldquo;I double
</p>
<p>crop soybeans&rdquo;), import3 (&ldquo;Planting wheat improves my corn yield&rdquo;), import4
</p>
<p>(&ldquo;It helps me break disease and pest cycles&rdquo;), and import5 (&ldquo;It gives me summer
</p>
<p>cash flow&rdquo;) and interpret.
</p>
<p>2. What drives how much wheat these farmers grow (wheat)? Agripro management
</p>
<p>is interested in whether import1, import2, import3, import4, and import5 can
</p>
<p>explain wheat. Please run this regression model and test the assumptions. Can
</p>
<p>you report on this model to AgriPro&rsquo;s management? Please discuss.
</p>
<p>3. Please calculate the AIC and BIC for the model discussed in question 2. Then
</p>
<p>add the variables acre and age. Calculate the AIC and BIC. Which model is
</p>
<p>better? Should we present the model with or without acre and age to our client?
</p>
<p>4. AgriPro expects that farmers who are more satisfied with their products devote a
</p>
<p>greater percentage of their total number of acres to wheat (wheat). Please test
</p>
<p>this assumption by using regression analysis. The client has requested that you
</p>
<p>control for the number of acres of farmland (acre), the age of the respondent
</p>
<p>(age), the quality of the seed (var3), and the availability of the seed (var4), and
</p>
<p>check the assumptions of the regression analysis. Note that a smaller sample size
</p>
<p>is available for this analysis, which means the sample size requirement cannot be
</p>
<p>met. Proceed with the analysis nevertheless. Are all of the other assumptions
</p>
<p>satisfied? If not, is there anything we can do about this, or should we ignore the
</p>
<p>assumptions if they are not satisfied?
</p>
<p>5. Agripro wants you to consider which customers are most loyal to its biggest
</p>
<p>competitor Pioneer (loyal5). Use the number of acres (acre), number of acres
</p>
<p>planted with wheat (wheat), the age of the respondent (age), and this person&rsquo;s
</p>
<p>education. Use the i. operator for education to gain an understanding of the
</p>
<p>group differences. Does this regression model meet the requirements and
</p>
<p>assumptions?
</p>
<p>6. As an AgriPro&rsquo;s consultant, and based on this study&rsquo;s empirical findings, what
</p>
<p>marketing advice do you have for AgriPro&rsquo;s marketing team? Using bullet
</p>
<p>points, provide four or five carefully thought through suggestions.
</p>
<p>18We would like to thank Dr. D.I. Gilliland and AgriPro for making the data and case study
available.
</p>
<p>7.5 Farming with AgriPro (Case Study) 261</p>
<p/>
</div>
<div class="page"><p/>
<p>7.6 Review Questions
</p>
<p>1. Explain what regression analysis is in your own words.
</p>
<p>2. Imagine you are asked to use regression analysis to explain the profitability of
</p>
<p>new supermarket products, such as the introduction of a new type of jam or
</p>
<p>yoghurt, during the first year of their launch. Which independent variables would
</p>
<p>you use to explain these new products&rsquo; profitability?
</p>
<p>3. Imagine you have to present the findings of a regression model to a client. The
</p>
<p>client believes that the regression model is a &ldquo;black box&rdquo; and that anything can
</p>
<p>be made significant. What would your reaction be?
</p>
<p>4. I do not care about the assumptions&mdash;just give me the results! Please evaluate
</p>
<p>this statement in the context of regression analysis. Do you agree?
</p>
<p>5. Are all regression assumptions equally important? Please discuss.
</p>
<p>6. Using standardized βs, we can compare effects between different variables. Can
</p>
<p>we compare apples and oranges after all? Please discuss.
</p>
<p>7. Try adding or deleting variables from the regression model in the Oddjob
</p>
<p>Airways example and use the adjusted R2, as well as AIC and BIC statistics,
</p>
<p>to assess if these models are better.
</p>
<p>7.7 Further Readings
</p>
<p>Hair, J. F., Black, W. C., Babin, B. J., &amp; Anderson, R. E. (2013).Multivariate data
</p>
<p>analysis. A global perspective (7th ed.). Upper Saddle River: Pearson
</p>
<p>Prentice Hall.
</p>
<p>This is an excellent book which, in a highly accessible way, discusses many
</p>
<p>statistical terms from a theoretical perspective.
</p>
<p>Nielsen at http://www.nielsen.com
</p>
<p>This is the website for Nielsen, one of the world&rsquo;s biggest market research
</p>
<p>companies. They publish many reports that use regression analysis.
</p>
<p>The Food Marketing Institute at http://www.fmi.org
</p>
<p>This website contains data, some of which can be used for regression analysis.
</p>
<p>Treiman, D. J. (2014). Quantitative data analysis: Doing social research to test
</p>
<p>ideas. Hoboken: Wiley.
</p>
<p>This is a very good introduction to single and multiple regression. It discusses
</p>
<p>categorical independent variables in great detail while using Stata.
</p>
<p>http://www.ats.ucla.edu/stat/stata/topics/regression.htm
</p>
<p>This is an excellent and detailed website dealing with more advanced regression
</p>
<p>topics in Stata.
</p>
<p>262 7 Regression Analysis</p>
<p/>
<div class="annotation"><a href="http://www.nielsen.com/">http://www.nielsen.com/</a></div>
<div class="annotation"><a href="http://www.fmi.org/">http://www.fmi.org/</a></div>
</div>
<div class="page"><p/>
<p>References
</p>
<p>Aiken, L. S., &amp; West, S. G. (1991). Multiple regression: Testing and interpreting interactions.
Thousand Oaks: Sage.
</p>
<p>Baum, C. F. (2006). An introduction to modern econometrics using Stata. College Station: Stata
Press.
</p>
<p>Breusch, T. S., &amp; Pagan, A. R. (1980). The Lagrange multiplier test and its applications to model
specification in econometrics. Review of Economic Studies, 47(1), 239&ndash;253.
</p>
<p>Cameron, A.C. &amp; Trivedi, P.K. (1990). The information matrix test and its implied alternative
hypotheses. (Working Papers from California Davis &ndash; Institute of Governmental Affairs,
pp. 1&ndash;33).
</p>
<p>Cameron, A. C., &amp; Trivedi, P. K. (2010). Microeconometrics using stata (Revised ed.). College
Station: Stata Press.
</p>
<p>Cohen, J. (1992). A power primer. Psychological Bulletin, 112(1), 155&ndash;159.
Cohen, J. (1994). The earth is round (p &lt; .05). The American Psychologist, 49(912), 997&ndash;1003.
Cook, R. D., &amp; Weisberg, S. (1983). Diagnostics for heteroscedasticity in regression. Biometrika,
</p>
<p>70(1), 1&ndash;10.
Durbin, J., &amp; Watson, G. S. (1951). Testing for serial correlation in least squares regression,
</p>
<p>II. Biometrika, 38(1&ndash;2), 159&ndash;179.
Fabozzi, F. J., Focardi, S. M., Rachev, S. T., &amp; Arshanapalli, B. G. (2014). The basics of financial
</p>
<p>econometrics: Tools, concepts, and asset management applications. Hoboken: Wiley.
Green, S. B. (1991). How many subjects does it take to do a regression analysis? Multivariate
</p>
<p>Behavioral Research, 26(3), 499&ndash;510.
Greene, W. H. (2011). Econometric analysis (7th ed.). Upper Saddle River: Prentice Hall.
Hair, J. F., Jr., Black, W. C., Babin, B. J., &amp; Anderson, R. E. (2013). Multivariate data analysis.
</p>
<p>Upper Saddle River: Pearson.
Hill, C., Griffiths, W., &amp; Lim, G. C. (2008). Principles of econometrics (3rd ed.). Hoboken: Wiley.
Kelley, K., &amp; Maxwell, S. E. (2003). Sample size for multiple regression: Obtaining regression
</p>
<p>coefficients that are accurate, not simply significant. Psychological Methods, 8(3), 305&ndash;321.
Mason, C. H., &amp; Perreault, W. D., Jr. (1991). Collinearity, power, and interpretation of multiple
</p>
<p>regression analysis. Journal of Marketing Research, 28, 268&ndash;280.
Mooi, E. A., &amp; Frambach, R. T. (2009). A stakeholder perspective on buyer&ndash;supplier conflict.
</p>
<p>Journal of Marketing Channels, 16(4), 291&ndash;307.
O&rsquo;brien, R. M. (2007). A caution regarding rules of thumb for variance inflation factors. Quality
</p>
<p>and Quantity, 41(5), 673&ndash;690.
Ramsey, J. B. (1969). Test for specification errors in classical linear least-squares regression
</p>
<p>analysis. Journal of the Royal Statistical Society, Series B, 31(2), 350&ndash;371.
Sin, C., &amp; White, H. (1996). Information criteria for selecting possibly misspecified parametric
</p>
<p>models. Journal of Econometrics, 71(1&ndash;2), 207&ndash;225.
StataCorp. (2015). Stata 14 base reference manual. College Station: Stata Press.
Treiman, D. J. (2014). Quantitative data analysis: Doing social research to test ideas. Hoboken:
</p>
<p>Wiley.
VanVoorhis, C. R. W., &amp; Morgan, B. L. (2007). Understanding power and rules of thumb for
</p>
<p>determining sample sizes. Tutorial in Quantitative Methods for Psychology, 3(2), 43&ndash;50.
White, H. (1980). A heteroskedasticity-consistent covariance matrix estimator and a direct test for
</p>
<p>heteroskedasticity. Econometrica: Journal of the Econometric Society, 48(4), 817&ndash;838.
</p>
<p>References 263</p>
<p/>
</div>
<div class="page"><p/>
<p>Principal Component and Factor Analysis 8
</p>
<p>Keywords
</p>
<p>Akaike Information Criterion (AIC) &bull; Anti-image &bull; Bartlett method &bull; Bayes
</p>
<p>Information Criterion (BIC) &bull; Communality &bull; Components &bull; Confirmatory factor
</p>
<p>analysis &bull; Correlation residuals &bull; Covariance-based structural equation
</p>
<p>modeling &bull; Cronbach&rsquo;s alpha &bull; Eigenvalue &bull; Eigenvectors &bull; Exploratory factor
</p>
<p>analysis &bull; Factor analysis &bull; Factor loading &bull; Factor rotation &bull; Factor scores &bull;
</p>
<p>Factor weights &bull; Factors &bull; Heywood cases &bull; Internal consistency reliability &bull;
</p>
<p>Kaiser criterion &bull; Kaiser&ndash;Meyer&ndash;Olkin criterion &bull; Latent root criterion &bull;
</p>
<p>Measure of sampling adequacy &bull; Oblimin rotation &bull; Orthogonal rotation &bull;
</p>
<p>Oblique rotation &bull; Parallel analysis &bull; Partial least squares structural equation
</p>
<p>modeling &bull; Path diagram &bull; Principal axis factoring &bull; Principal components &bull;
</p>
<p>Principal component analysis &bull; Principal factor analysis &bull; Promax rotation &bull;
</p>
<p>Regression method &bull; Reliability analysis &bull; Scree plot &bull; Split-half reliability &bull;
</p>
<p>Structural equation modeling &bull; Test-retest reliability &bull; Uniqueness &bull; Varimax
</p>
<p>rotation
</p>
<p>Learning Objectives
</p>
<p>After reading this chapter, you should understand:
</p>
<p>&ndash; The basics of principal component and factor analysis.
</p>
<p>&ndash; The principles of exploratory and confirmatory factor analysis.
</p>
<p>&ndash; Key terms, such as communality, eigenvalues, factor loadings, factor scores, and
</p>
<p>uniqueness.
</p>
<p>&ndash; What rotation is.
</p>
<p>&ndash; The principles of exploratory and confirmatory factor analysis.
</p>
<p>&ndash; How to determine whether data are suitable for carrying out an exploratory
</p>
<p>factor analysis.
</p>
<p>&ndash; How to interpret Stata principal component and factor analysis output.
</p>
<p># Springer Nature Singapore Pte Ltd. 2018
</p>
<p>E. Mooi et al., Market Research, Springer Texts in Business and Economics,
DOI 10.1007/978-981-10-5218-7_8
</p>
<p>265</p>
<p/>
</div>
<div class="page"><p/>
<p>&ndash; The principles of reliability analysis and its execution in Stata.
</p>
<p>&ndash; The concept of structural equation modeling.
</p>
<p>8.1 Introduction
</p>
<p>Principal component analysis (PCA) and factor analysis (also called principal
</p>
<p>factor analysis or principal axis factoring) are two methods for identifying
</p>
<p>structure within a set of variables. Many analyses involve large numbers of
</p>
<p>variables that are difficult to interpret. Using PCA or factor analysis helps find
</p>
<p>interrelationships between variables (usually called items) to find a smaller number
</p>
<p>of unifying variables called factors. Consider the example of a soccer club whose
</p>
<p>management wants to measure the satisfaction of the fans. The management could,
</p>
<p>for instance, measure fan satisfaction by asking how satisfied the fans are with the
</p>
<p>(1) assortment of merchandise, (2) quality of merchandise, and (3) prices of
</p>
<p>merchandise. It is likely that these three items together measure satisfaction with
</p>
<p>the merchandise. Through the application of PCA or factor analysis, we can
</p>
<p>determine whether a single factor represents the three satisfaction items well.
</p>
<p>Practically, PCA and factor analysis are applied to understand much larger sets of
</p>
<p>variables, tens or even hundreds, when just reading the variables&rsquo; descriptions does
</p>
<p>not determine an obvious or immediate number of factors.
</p>
<p>PCA and factor analysis both explain patterns of correlations within a set of
</p>
<p>observed variables. That is, they identify sets of highly correlated variables and
</p>
<p>infer an underlying factor structure. While PCA and factor analysis are very similar
</p>
<p>in the way they arrive at a solution, they differ fundamentally in their assumptions
</p>
<p>of the variables&rsquo; nature and their treatment in the analysis. Due to these differences,
</p>
<p>the methods follow different research objectives, which dictate their areas of
</p>
<p>application. While the PCA&rsquo;s objective is to reproduce a data structure, as well
as possible only using a few factors, factor analysis aims to explain the variables&rsquo;
correlations by means of factors (e.g., Hair et al. 2013; Matsunaga 2010; Mulaik
</p>
<p>2009).1 We will discuss these differences and their implications in this chapter.
</p>
<p>Both PCA and factor analysis can be used for exploratory or confirmatory
</p>
<p>purposes. What are exploratory and confirmatory factor analyses? Comparing the
</p>
<p>left and right panels of Fig. 8.1 shows us the difference. Exploratory factor
</p>
<p>analysis, often simply referred to as EFA, does not rely on previous ideas on the
</p>
<p>factor structure we may find. That is, there may be relationships (indicated by the
</p>
<p>arrows) between each factor and each item. While some of these relationships may
</p>
<p>be weak (indicated by the dotted arrows), others are more pronounced, suggesting
</p>
<p>that these items represent an underlying factor well. The left panel of Fig. 8.1
</p>
<p>illustrates this point. Thus, an exploratory factor analysis reveals the number of
</p>
<p>factors and the items belonging to a specific factor. In a confirmatory factor
</p>
<p>1Other methods for carrying out factor analyses include, for example, unweighted least squares,
</p>
<p>generalized least squares, or maximum likelihood. However, these are statistically complex and
</p>
<p>inexperienced users should not consider them.
</p>
<p>266 8 Principal Component and Factor Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>analysis, usually simply referred to as CFA, there may only be relationships
</p>
<p>between a factor and specific items. In the right panel of Fig. 8.1, the first three
</p>
<p>items relate to factor 1, whereas the last two items relate to factor 2. Different from
</p>
<p>the exploratory factor analysis, in a confirmatory factor analysis, we have clear
</p>
<p>expectations of the factor structure (e.g., because researchers have proposed a scale
</p>
<p>that we want to adapt for our study) and we want to test for the expected structure.
</p>
<p>In this chapter, we primarily deal with exploratory factor analysis, as it conveys
</p>
<p>the principles that underlie all factor analytic procedures and because the two
</p>
<p>techniques are (almost) identical from a statistical point of view. Nevertheless,
</p>
<p>we will also discuss an important aspect of confirmatory factor analysis, namely
</p>
<p>reliability analysis, which tests the consistency of a measurement scale (see
</p>
<p>Chap. 3). We will also briefly introduce a specific confirmatory factor analysis
</p>
<p>approach called structural equation modeling (often simply referred to as SEM).
</p>
<p>Structural equation modeling differs statistically and practically from PCA and
</p>
<p>factor analysis. It is not only used to evaluate how well observed variables relate to
</p>
<p>factors but also to analyze hypothesized relationships between factors that the
</p>
<p>researcher specifies prior to the analysis based on theory and logic.
</p>
<p>8.2 Understanding Principal Component and Factor Analysis
</p>
<p>8.2.1 Why Use Principal Component and Factor Analysis?
</p>
<p>Researchers often face the problem of large questionnaires comprising many items.
For example, in a survey of a major German soccer club, the management was
</p>
<p>particularly interested in identifying and evaluating performance features that relate
</p>
<p>to soccer fans&rsquo; satisfaction (Sarstedt et al. 2014). Examples of relevant features
</p>
<p>include the stadium, the team composition and their success, the trainer, and the
</p>
<p>Fig. 8.1 Exploratory factor analysis (left) and confirmatory factor analysis (right)
</p>
<p>8.2 Understanding Principal Component and Factor Analysis 267</p>
<p/>
</div>
<div class="page"><p/>
<p>management. The club therefore commissioned a questionnaire comprising 99 pre-
</p>
<p>viously identified items by means of literature databases and focus groups of fans.
</p>
<p>All the items were measured on scales ranging from 1 (&ldquo;very dissatisfied&rdquo;) to
</p>
<p>7 (&ldquo;very satisfied&rdquo;). Table 8.1 shows an overview of some items considered in the
</p>
<p>study.
</p>
<p>As you can imagine, tackling such a large set of items is problematic, because it
</p>
<p>provides quite complex data. Given the task of identifying and evaluating perfor-
</p>
<p>mance features that relate to soccer fans&rsquo; satisfaction (measured by &ldquo;Overall, how
</p>
<p>satisfied are you with your soccer club&rdquo;), we cannot simply compare the items on a
</p>
<p>pairwise basis. It is far more reasonable to consider the factor structure first. For
</p>
<p>example, satisfaction with the condition of the stadium (x1), outer appearance of the
stadium (x2), and interior design of the stadium (x3) cover similar aspects that relate
to the respondents&rsquo; satisfaction with the stadium. If a soccer fan is generally very
</p>
<p>satisfied with the stadium, he/she will most likely answer all three items positively.
</p>
<p>Conversely, if a respondent is generally dissatisfied with the stadium, he/she is most
</p>
<p>likely to be rather dissatisfied with all the performance aspects of the stadium, such
</p>
<p>as the outer appearance and interior design. Consequently, these three items are
</p>
<p>likely to be highly correlated&mdash;they cover related aspects of the respondents&rsquo;
</p>
<p>overall satisfaction with the stadium. More precisely, these items can be interpreted
</p>
<p>Table 8.1 Items in the soccer fan satisfaction study
</p>
<p>Satisfaction with. . .
</p>
<p>Condition of the stadium Public appearances of the players
</p>
<p>Interior design of the stadium Number of stars in the team
</p>
<p>Outer appearance of the stadium Interaction of players with fans
</p>
<p>Signposting outside the stadium Volume of the loudspeakers in the stadium
</p>
<p>Signposting inside the stadium Choice of music in the stadium
</p>
<p>Roofing inside the stadium Entertainment program in the stadium
</p>
<p>Comfort of the seats Stadium speaker
</p>
<p>Video score boards in the stadium Newsmagazine of the stadium
</p>
<p>Condition of the restrooms Price of annual season ticket
</p>
<p>Tidiness within the stadium Entry fees
</p>
<p>Size of the stadium Offers of reduced tickets
</p>
<p>View onto the playing field Design of the home jersey
</p>
<p>Number of restrooms Design of the away jersey
</p>
<p>Sponsors&rsquo; advertisements in the stadium Assortment of merchandise
</p>
<p>Location of the stadium Quality of merchandise
</p>
<p>Name of the stadium Prices of merchandise
</p>
<p>Determination and commitment of the players Pre-sale of tickets
</p>
<p>Current success regarding matches Online-shop
</p>
<p>Identification of the players with the club Opening times of the fan-shops
</p>
<p>Quality of the team composition Accessibility of the fan-shops
</p>
<p>Presence of a player with whom fans can
</p>
<p>identify
</p>
<p>Behavior of the sales persons in the fan
</p>
<p>shops
</p>
<p>268 8 Principal Component and Factor Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>as manifestations of the factor capturing the &ldquo;joint meaning&rdquo; of the items related to
</p>
<p>it. The arrows pointing from the factor to the items in Fig. 8.1 indicate this point. In
</p>
<p>our example, the &ldquo;joint meaning&rdquo; of the three items could be described as satisfac-
tion with the stadium, since the items represent somewhat different, yet related,
aspects of the stadium. Likewise, there is a second factor that relates to the two
</p>
<p>items x4 and x5, which, like the first factor, shares a common meaning, namely
satisfaction with the merchandise.
</p>
<p>PCA and factor analysis are two statistical procedures that draw on item
</p>
<p>correlations in order to find a small number of factors. Having conducted the
</p>
<p>analysis, we can make use of few (uncorrelated) factors instead of many variables,
</p>
<p>thus significantly reducing the analysis&rsquo;s complexity. For example, if we find six
</p>
<p>factors, we only need to consider six correlations between the factors and overall
</p>
<p>satisfaction, which means that the recommendations will rely on six factors.
</p>
<p>8.2.2 Analysis Steps
</p>
<p>Like any multivariate analysis method, PCA and factor analysis are subject to
</p>
<p>certain requirements, which need to be met for the analysis to be meaningful. A
</p>
<p>crucial requirement is that the variables need to exhibit a certain degree of correla-
</p>
<p>tion. In our example in Fig. 8.1, this is probably the case, as we expect increased
</p>
<p>correlations between x1, x2, and x3, on the one hand, and between x4 and x5 on the
other. Other items, such as x1 and x4, are probably somewhat correlated, but to a
lesser degree than the group of items x1, x2, and x3 and the pair x4 and x5. Several
methods allow for testing whether the item correlations are sufficiently high.
</p>
<p>Both PCA and factor analysis strive to reduce the overall item set to a smaller set
</p>
<p>of factors. More precisely, PCA extracts factors such that they account for
</p>
<p>variables&rsquo; variance, whereas factor analysis attempts to explain the correlations
</p>
<p>between the variables. Whichever approach you apply, using only a few factors
</p>
<p>instead of many items reduces its precision, because the factors cannot represent all
</p>
<p>the information included in the items. Consequently, there is a trade-off between
</p>
<p>simplicity and accuracy. In order to make the analysis as simple as possible, we
</p>
<p>want to extract only a few factors. At the same time, we do not want to lose too
</p>
<p>much information by having too few factors. This trade-off has to be addressed in
</p>
<p>any PCA and factor analysis when deciding how many factors to extract from
</p>
<p>the data.
</p>
<p>Once the number of factors to retain from the data has been identified, we can
</p>
<p>proceed with the interpretation of the factor solution. This step requires us to
</p>
<p>produce a label for each factor that best characterizes the joint meaning of all the
</p>
<p>variables associated with it. This step is often challenging, but there are ways of
</p>
<p>facilitating the interpretation of the factor solution. Finally, we have to assess how
</p>
<p>well the factors reproduce the data. This is done by examining the solution&rsquo;s
</p>
<p>goodness-of-fit, which completes the standard analysis. However, if we wish to
</p>
<p>continue using the results in further analyses, we need to calculate the factor scores.
</p>
<p>8.2 Understanding Principal Component and Factor Analysis 269</p>
<p/>
</div>
<div class="page"><p/>
<p>Factor scores are linear combinations of the items and can be used as variables in
</p>
<p>follow-up analyses.
</p>
<p>Figure 8.2 illustrates the steps involved in the analysis; we will discuss these in
</p>
<p>more detail in the following sections. In doing so, our theoretical descriptions will
</p>
<p>focus on the PCA, as this method is easier to grasp. However, most of our
</p>
<p>descriptions also apply to factor analysis. Our illustration at the end of the chapter
</p>
<p>also follows a PCA approach but uses a Stata command (factor, pcf), which
</p>
<p>blends the PCA and factor analysis. This blending has several advantages, which
</p>
<p>we will discuss later in this chapter.
</p>
<p>8.3 Principal Component Analysis
</p>
<p>8.3.1 Check Requirements and Conduct Preliminary Analyses
</p>
<p>Before carrying out a PCA, we have to consider several requirements, which we can
</p>
<p>test by answering the following questions:
</p>
<p>&ndash; Are the measurement scales appropriate?
</p>
<p>&ndash; Is the sample size sufficiently large?
</p>
<p>&ndash; Are the observations independent?
</p>
<p>&ndash; Are the variables sufficiently correlated?
</p>
<p>Fig. 8.2 Steps involved in a PCA
</p>
<p>270 8 Principal Component and Factor Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>Are the measurement scales appropriate?
For a PCA, it is best to have data measured on an interval or ratio scale. In practical
</p>
<p>applications, items measured on an ordinal scale level have become common.
</p>
<p>Ordinal scales can be used if:
</p>
<p>&ndash; the scale points are equidistant, which means that the difference in the wording
</p>
<p>between scale steps is the same (see Chap. 3), and
&ndash; there are five or more response categories.
</p>
<p>Is the sample size sufficiently large?
Another point of concern is the sample size. As a rule of thumb, the number of
</p>
<p>(valid) observations should be at least ten times the number of items used for
</p>
<p>analysis. This only provides a rough indication of the necessary sample size.
</p>
<p>Fortunately, researchers have conducted studies to determine minimum sample
</p>
<p>size requirements, which depend on other aspects of the study. MacCallum et al.
</p>
<p>(1999) suggest the following:
</p>
<p>&ndash; When all communalities (we will discuss this term in Sect. 8.3.2.4) are above
</p>
<p>0.60, small sample sizes of below 100 are adequate.
</p>
<p>&ndash; With communalities around 0.50, sample sizes between 100 and 200 are
</p>
<p>sufficient.
</p>
<p>&ndash; When communalities are consistently low, with many or all under 0.50, a sample
</p>
<p>size between 100 and 200 is adequate if the number of factors is small and each
</p>
<p>of these is measured with six or more indicators.
</p>
<p>&ndash; When communalities are consistently low and the factors numbers are high or
</p>
<p>are measured with only few indicators (i.e., 3 or less), 300 observations are
</p>
<p>recommended.
</p>
<p>Are the observations independent?
We have to ensure that the observations are independent. This means that the
</p>
<p>observations need to be completely unrelated (see Chap. 3). If we use dependent
</p>
<p>observations, we would introduce &ldquo;artificial&rdquo; correlations, which are not due to an
</p>
<p>underlying factor structure, but simply to the same respondents answered the same
</p>
<p>questions multiple times.
</p>
<p>Are the variables sufficiently correlated?
As indicated before, PCA is based on correlations between items. Consequently,
</p>
<p>conducting a PCA only makes sense if the items correlate sufficiently. The problem
</p>
<p>is deciding what &ldquo;sufficient&rdquo; actually means.
</p>
<p>An obvious step is to examine the correlation matrix (Chap. 5). Naturally, we
</p>
<p>want the correlations between different items to be as high as possible, but they
</p>
<p>will not always be. In our previous example, we expect high correlations between
</p>
<p>x1, x2, and x3, on the one hand, and x4 and x5 on the other. Conversely, we might
</p>
<p>8.3 Principal Component Analysis 271</p>
<p/>
</div>
<div class="page"><p/>
<p>expect lower correlations between, for example, x1 and x4 and between x3 and x5.
Thus, not all of the correlation matrix&rsquo;s elements need to have high values. The
</p>
<p>PCA depends on the relative size of the correlations. Therefore, if single
correlations are very low, this is not necessarily problematic! Only when all the
</p>
<p>correlations are around zero is PCA no longer useful. In addition, the statistical
</p>
<p>significance of each correlation coefficient helps decide whether it differs signifi-
</p>
<p>cantly from zero.
</p>
<p>There are additional measures to determine whether the items correlate suffi-
</p>
<p>ciently. One is the anti-image. The anti-image describes the portion of an item&rsquo;s
</p>
<p>variance that is independent of another item in the analysis. Obviously, we want all
</p>
<p>items to be highly correlated, so that the anti-images of an item set are as small as
</p>
<p>possible. Initially, we do not interpret the anti-image values directly, but use a
</p>
<p>measure based on the anti-image concept: The Kaiser&ndash;Meyer&ndash;Olkin (KMO)
</p>
<p>statistic. The KMO statistic, also called the measure of sampling adequacy
</p>
<p>(MSA), indicates whether the other variables in the dataset can explain the
</p>
<p>correlations between variables. Kaiser (1974), who introduced the statistic,
</p>
<p>recommends a set of distinctively labeled threshold values for KMO and MSA,
</p>
<p>which Table 8.2 presents.
</p>
<p>To summarize, the correlation matrix with the associated significance levels
</p>
<p>provides a first insight into the correlation structures. However, the final decision of
</p>
<p>whether the data are appropriate for PCA should be primarily based on the KMO
</p>
<p>statistic. If this measure indicates sufficiently correlated variables, we can continue
</p>
<p>the analysis of the results. If not, we should try to identify items that correlate only
</p>
<p>weakly with the remaining items and remove them. In Box 8.1, we discuss how to
</p>
<p>do this.
</p>
<p>Box 8.1 Identifying Problematic Items
</p>
<p>Examining the correlation matrix and the significance levels of correlations
</p>
<p>allows identifying items that correlate only weakly with the remaining items.
</p>
<p>An even better approach is examining the variable-specific MSA values,
</p>
<p>which are interpreted like the overall KMO statistic (see Table 8.2). In fact,
</p>
<p>the KMO statistic is simply the overall mean of all variable-specific MSA
</p>
<p>values. Consequently, all the MSA values should also lie above the threshold
</p>
<p>(continued)
</p>
<p>Table 8.2 Threshold
</p>
<p>values for KMO and MSA
KMO/MSA value Adequacy of the correlations
</p>
<p>Below 0.50 Unacceptable
</p>
<p>0.50&ndash;0.59 Miserable
</p>
<p>0.60&ndash;0.69 Mediocre
</p>
<p>0.70&ndash;0.79 Middling
</p>
<p>0.80&ndash;0.89 Meritorious
</p>
<p>0.90 and higher Marvelous
</p>
<p>272 8 Principal Component and Factor Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>Box 8.1 (continued)
</p>
<p>level of 0.50. If this is not the case, consider removing this item from the
</p>
<p>analysis. An item&rsquo;s communality or uniqueness (see next section) can also
</p>
<p>serve as a useful indicators of how well the factors extracted represent an
</p>
<p>item. However, communalities and uniqueness are mostly considered when
</p>
<p>evaluating the solution&rsquo;s goodness-of-fit.
</p>
<p>8.3.2 Extract the Factors
</p>
<p>8.3.2.1 Principal Component Analysis vs. Factor Analysis
Factor analysis assumes that each variable&rsquo;s variance can be divided into common
</p>
<p>variance (i.e., variance shared with all the other variables in the analysis) and
</p>
<p>unique variance (Fig. 8.3), the latter of which can be further broken down into
</p>
<p>specific variance (i.e., variance associated with only one specific variable) and error
</p>
<p>variance (i.e., variance due to measurement error). The method, however, can only
</p>
<p>reproduce common variance. Thereby factor analysis explicitly recognizes the
</p>
<p>presence of error. Conversely, PCA assumes that all of each variable&rsquo;s variance is
</p>
<p>common variance, which factor extraction can explain fully (e.g., Preacher and
</p>
<p>MacCallum 2003). These differences entail different interpretations of the
</p>
<p>analysis&rsquo;s outcomes. PCA asks:
</p>
<p>Which umbrella term can we use to summarize a set of variables that loads highly on a
specific factor?
</p>
<p>Conversely, factor analysis asks:
</p>
<p>What is the common reason for the strong correlations between a set of variables?
</p>
<p>From a theoretical perspective, the assumption that there is a unique variance for
</p>
<p>which the factors cannot fully account, is generally more realistic, but simulta-
</p>
<p>neously more restrictive. Although theoretically sound, this restriction can some-
</p>
<p>times lead to complications in the analysis, which have contributed to the
</p>
<p>widespread use of PCA, especially in market research practice.
</p>
<p>Researchers usually suggest using PCA when data reduction is the primary
</p>
<p>concern; that is, when the focus is to extract a minimum number of factors that
</p>
<p>account for a maximum proportion of the variables&rsquo; total variance. In contrast, if the
</p>
<p>primary concern is to identify latent dimensions represented in the variables, factor
</p>
<p>analysis should be applied. However, prior research has shown that both approaches
</p>
<p>arrive at essentially the same result when:
</p>
<p>&ndash; more than 30 variables are used, or
&ndash; most of the variables&rsquo; communalities exceed 0.60.
</p>
<p>8.3 Principal Component Analysis 273</p>
<p/>
</div>
<div class="page"><p/>
<p>With 20 or fewer variables and communalities below 0.40&mdash;which are clearly
</p>
<p>undesirable in empirical research&mdash;the differences are probably pronounced
</p>
<p>(Stevens 2009).
</p>
<p>Apart from these conceptual differences in the variables&rsquo; nature, PCA and factor
</p>
<p>analysis differ in the aim of their analysis. Whereas the goal of factor analysis is to
</p>
<p>explain the correlations between the variables, PCA focuses on explaining the
</p>
<p>variables&rsquo; variances. That is, the PCA&rsquo;s objective is to determine the linear
</p>
<p>combinations of the variables that retain as much information from the original
</p>
<p>variables as possible. Strictly speaking, PCA does not extract factors, but
</p>
<p>components, which are labeled as such in Stata.
</p>
<p>Despite these differences, which have very little relevance in many common
</p>
<p>research settings in practice, PCA and factor analysis have many points in common.
</p>
<p>For example, the methods follow very similar ways to arrive at a solution and their
</p>
<p>interpretations of statistical measures, such as KMO, eigenvalues, or factor
</p>
<p>loadings, are (almost) identical. In fact, Stata blends these two procedures in its
</p>
<p>factor, pcf command, which runs a factor analysis but rescales the estimates
</p>
<p>such that they conform to a PCA. That way, the analysis assumes that the entire
</p>
<p>variance is common but produces (rotated) loadings (we will discuss factor rotation
</p>
<p>later in this chapter), which facilitate the interpretation of the factors. In contrast, if
</p>
<p>we would run a standard PCA, Stata would only offer us eigenvectors whose
</p>
<p>(unrotated) weights would not allow for a concluding interpretation of the factors.
</p>
<p>In fact, in many PCA analyses, researchers are not interested in the interpretation of
</p>
<p>the extracted factors but merely use the method for data reduction. For example, in
</p>
<p>sensory marketing research, researchers routinely use PCA to summarize a large set
</p>
<p>of sensory variables (e.g., haptic, smell, taste) to derive a set of factors whose scores
are then used as input for cluster analyses (Chap. 9). This approach allows for
</p>
<p>identifying distinct groups of products from which one or more representative
</p>
<p>products can then be chosen for a more detailed comparison using qualitative
</p>
<p>Fig. 8.3 Principal component analysis vs. factor analysis
</p>
<p>274 8 Principal Component and Factor Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>research or further assessment in field experiments (e.g., Carbonell et al. 2008;
</p>
<p>Vigneau and Qannari 2002).
</p>
<p>Despite the small differences of PCA and factor analysis in most research
</p>
<p>settings, researchers have strong feelings about the choice of PCA or factor
</p>
<p>analysis. Cliff (1987, p. 349) summarizes this issue well, by noting that proponents
</p>
<p>of factor analysis &ldquo;insist that components analysis is at best a common factor
</p>
<p>analysis with some error added and at worst an unrecognizable hodgepodge of
</p>
<p>things from which nothing can be determined.&rdquo; For further discussions on this
</p>
<p>topic, see also Velicer and Jackson (1990) and Widaman (1993).2
</p>
<p>8.3.2.2 How Does Factor Extraction Work?
PCA&rsquo;s objective is to reproduce a data structure with only a few factors. PCA does
</p>
<p>this by generating a new set of factors as linear composites of the original variables,
</p>
<p>which reproduces the original variables&rsquo; variance as best as possible. These linear
</p>
<p>composites are called principal components, but, for simplicity&rsquo;s sake, we refer to
</p>
<p>them as factors. More precisely, PCA computes eigenvectors. These eigenvectors
</p>
<p>include so called factor weights, which extract the maximum possible variance of
</p>
<p>all the variables, with successive factoring continuing until a significant share of the
</p>
<p>variance is explained.
</p>
<p>Operationally, the first factor is extracted in such a way that it maximizes the
</p>
<p>variance accounted for in the variables. We can visualize this easily by examining
</p>
<p>the vector space illustrated in Fig. 8.4. In this example, we have five variables (x1&ndash;
x5) represented by five vectors starting at the zero point, with each vector&rsquo;s length
standardized to one. To maximize the variance accounted for, the first factor F1 is
fitted into this vector space in such a way that the sum of all the angles between this
</p>
<p>factor and the five variables in the vector space is minimized. We do this to interpret
</p>
<p>the angle between two vectors as correlations. For example, if the factor&rsquo;s vector
</p>
<p>and a variable&rsquo;s vector are congruent, the angle between these two is zero,
</p>
<p>Fig. 8.4 Factor extraction
</p>
<p>(Note that Fig. 8.4 describes a
</p>
<p>special case, as the five
</p>
<p>variables are scaled down into
</p>
<p>a two-dimensional space. In
</p>
<p>this set-up, it would be
</p>
<p>possible for the two factors to
</p>
<p>explain all five items.
</p>
<p>However, in real-life, the five
</p>
<p>items span a five-dimensional
</p>
<p>vector space.)
</p>
<p>2Related discussions have been raised in structural equation modeling, where researchers have
</p>
<p>heatedly discussed the strengths and limitations of factor-based and component-based approaches
</p>
<p>(e.g., Sarstedt et al. 2016, Hair et al. 2017a, b).
</p>
<p>8.3 Principal Component Analysis 275</p>
<p/>
</div>
<div class="page"><p/>
<p>indicating that the factor and the variable correlate perfectly. On the other hand, if
</p>
<p>the factor and the variable are uncorrelated, the angle between these two is 90�. This
</p>
<p>correlation between a (unit-scaled) factor and a variable is called the factor
</p>
<p>loading. Note that factor weights and factor loadings essentially express the same
</p>
<p>thing&mdash;the relationships between variables and factors&mdash;but they are based on
</p>
<p>different scales.
</p>
<p>After extracting F1, a second factor (F2) is extracted, which maximizes the
remaining variance accounted for. The second factor is fitted at a 90� angle into
</p>
<p>the vector space (Fig. 8.4) and is therefore uncorrelated with the first factor.3 If we
</p>
<p>extract a third factor, it will explain the maximum amount of variance for which
</p>
<p>factors 1 and 2 have hitherto not accounted. This factor will also be fitted at a 90�
</p>
<p>angle to the first two factors, making it independent from the first two factors
</p>
<p>(we don&rsquo;t illustrate this third factor in Fig. 8.4, as this is a three-dimensional space).
</p>
<p>The fact that the factors are uncorrelated is an important feature, as we can use them
</p>
<p>to replace many highly correlated variables in follow-up analyses. For example,
</p>
<p>using uncorrelated factors as independent variables in a regression analysis helps
</p>
<p>solve potential collinearity issues (Chap. 7).
</p>
<p>The Explained Visually webpage offers an excellent illustration of two- and
</p>
<p>three-dimensional factor extraction, see http://setosa.io/ev/principal-compo
</p>
<p>nent-analysis/
</p>
<p>An important PCA feature is that it works with standardized variables (see
</p>
<p>Chap. 5 for an explanation of what standardized variables are). Standardizing
</p>
<p>variables has important implications for our analysis in two respects. First, we
</p>
<p>can assess each factor&rsquo;s eigenvalue, which indicates how much a specific factor
</p>
<p>extracts all of the variables&rsquo; variance (see next section). Second, the standardization
</p>
<p>of variables allows for assessing each variable&rsquo;s communality, which describes how
</p>
<p>much the factors extracted capture or reproduce each variable&rsquo;s variance. A related
</p>
<p>concept is the uniqueness, which is 1&ndash;communality (see Sect. 8.3.2.4).
</p>
<p>8.3.2.3 What Are Eigenvalues?
To understand the concept of eigenvalues, think of the soccer fan satisfaction study
</p>
<p>(Fig. 8.1). In this example, there are five variables. As all the variables are
</p>
<p>standardized prior to the analysis, each has a variance of 1. In a simplified way,
</p>
<p>we could say that the overall information (i.e., variance) that we want to reproduce
</p>
<p>by means of factor extraction is 5 units. Let&rsquo;s assume that we extract the two factors
</p>
<p>presented above.
</p>
<p>The first factor&rsquo;s eigenvalue indicates how much variance of the total variance
</p>
<p>(i.e., 5 units) this factor accounts for. If this factor has an eigenvalue of, let&rsquo;s say
</p>
<p>3Note that this changes when oblique rotation is used. We will discuss factor rotation later in this
</p>
<p>chapter.
</p>
<p>276 8 Principal Component and Factor Analysis</p>
<p/>
<div class="annotation"><a href="http://setosa.io/ev/principal-component-analysis/">http://setosa.io/ev/principal-component-analysis/</a></div>
<div class="annotation"><a href="http://setosa.io/ev/principal-component-analysis/">http://setosa.io/ev/principal-component-analysis/</a></div>
</div>
<div class="page"><p/>
<p>2.10, it covers the information of 2.10 variables or, put differently, accounts for
</p>
<p>2.10/5.00 &frac14; 42% of the overall variance (Fig. 8.5).
</p>
<p>Extracting a second factor will allow us to explain another part of the remaining
</p>
<p>variance (i.e., 5.00 &ndash; 2.10 &frac14; 2.90 units, Fig. 8.5). However, the eigenvalue of the
</p>
<p>second factor will always be smaller than that of the first factor. Assume that the
</p>
<p>second factor has an eigenvalue of 1.30 units. The second factor then accounts for
</p>
<p>1.30/5.00 &frac14; 26% of the overall variance. Together, these two factors explain
</p>
<p>(2.10 + 1.30)/5.00 &frac14; 68% of the overall variance. Every additional factor extracted
</p>
<p>increases the variance accounted for until we have extracted as many factors as
</p>
<p>there are variables. In this case, the factors account for 100% of the overall
</p>
<p>variance, which means that the factors reproduce the complete variance.
</p>
<p>Following the PCA approach, we assume that factor extraction can reproduce
</p>
<p>each variable&rsquo;s entire variance. In other words, we assume that each variable&rsquo;s
</p>
<p>variance is common; that is, the variance is shared with other variables. This differs
</p>
<p>in factor analysis, in which each variable can also have a unique variance.
</p>
<p>8.3.2.4 What Are Communality and Uniqueness?
Whereas the eigenvalue tells us how much variance each factor is accounts for, the
</p>
<p>communality indicates how much variance of each variable, factor extraction can
</p>
<p>reproduce. There is no commonly agreed threshold for a variable&rsquo;s communality, as
</p>
<p>this depends strongly on the complexity of the analysis at hand. However, gener-
</p>
<p>ally, the extracted factors should account for at least 50% of a variable&rsquo;s variance.
</p>
<p>Thus, the communalities should be above 0.50. Note that Stata does not indicate
</p>
<p>each variable&rsquo;s communality but its uniqueness, which is 1&ndash;communality. Hence,
</p>
<p>uniqueness gives the proportion of a variable&rsquo;s variance that the factors do not
capture. For uniqueness the same threshold as for communality applies. Thus, the
</p>
<p>Fig. 8.5 Total variance explained by variables and factors
</p>
<p>8.3 Principal Component Analysis 277</p>
<p/>
</div>
<div class="page"><p/>
<p>uniqueness values should be below 0.50. Every additional factor extracted will
</p>
<p>increase the explained variance, and if we extract as many factors as there are items
</p>
<p>(in our example five), each variable&rsquo;s communality would be 1.00 and its unique-
</p>
<p>ness equal to 0. The factors extracted would then fully explain each variable; that is,
</p>
<p>the first factor will explain a certain amount of each variable&rsquo;s variance, the second
</p>
<p>factor another part, and so on.
</p>
<p>However, since our overall objective is to reduce the number of variables
</p>
<p>through factor extraction, we should extract only a few factors that account for a
</p>
<p>high degree of overall variance. This raises the question of how to decide on the
</p>
<p>number of factors to extract from the data, which we discuss in the following
</p>
<p>section.
</p>
<p>8.3.3 Determine the Number of Factors
</p>
<p>Determining the number of factors to extract from the data is a crucial and
</p>
<p>challenging step in any PCA. Several approaches offer guidance in this respect,
</p>
<p>but most researchers do not pick just one method, but determine the number of
</p>
<p>factors resulting from the application of multiple methods. If multiple methods
</p>
<p>suggest the same number of factors, this leads to greater confidence in the results.
</p>
<p>8.3.3.1 The Kaiser Criterion
An intuitive way to decide on the number of factors is to extract all the factors with
</p>
<p>an eigenvalue greater than 1. The reason for this is that each factor with an
</p>
<p>eigenvalue greater than 1 accounts for more variance than a single variable
</p>
<p>(remember, we are looking at standardized variables, which is why each variable&rsquo;s
</p>
<p>variance is exactly 1). As the objective of PCA is to reduce the overall number of
</p>
<p>variables, each factor should of course account for more variance than a single
</p>
<p>variable can. If this occurs, then this factor is useful for reducing the set of variables.
</p>
<p>Extracting all the factors with an eigenvalue greater than 1 is frequently called the
</p>
<p>Kaiser criterion or latent root criterion and is commonly used to determine the
</p>
<p>number of factors. However, the Kaiser criterion is well known for overspecifying
</p>
<p>the number of factors; that is, the criterion suggests more factors than it should (e.g.,
</p>
<p>Russell 2002; Zwick and Velicer 1986).
</p>
<p>8.3.3.2 The Scree Plot
Another popular way to decide on the number of factors to extract is to plot each
</p>
<p>factor&rsquo;s eigenvalue (y-axis) against the factor with which it is associated (x-axis).
This results in a scree plot, which typically has a distinct break in it, thereby
</p>
<p>showing the &ldquo;correct&rdquo; number of factors (Cattell 1966). This distinct break is called
</p>
<p>the &ldquo;elbow.&rdquo; It is generally recommended that all factors should be retained above
this break, as they contribute most to the explanation of the variance in the dataset.
</p>
<p>Thus, we select one factor less than indicated by the elbow. In Box 8.2, we
</p>
<p>introduce a variant of the scree plot. This variant is however only available when
</p>
<p>using the pca command instead of the factor, pcf command, which serves as
</p>
<p>the basis for our case study illustration.
</p>
<p>278 8 Principal Component and Factor Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>Box 8.2 Confidence Intervals in the Scree Plot
</p>
<p>A variant of the scree plot includes the confidence interval (Chap. 6,
</p>
<p>Sect. 6.6.7) of each eigenvalue. These confidence intervals allow for
</p>
<p>identifying factors with an eigenvalue significantly greater than 1. If a
</p>
<p>confidence interval&rsquo;s lower bound is above the 1 threshold, this suggests
</p>
<p>that the factor should be extracted. Conversely, if an eigenvalue&rsquo;s confidence
</p>
<p>interval overlaps with the 1 threshold or falls completely below, this factor
</p>
<p>should not be extracted.
</p>
<p>8.3.3.3 Parallel Analysis
A large body of review papers and simulation studies has produced a prescriptive
</p>
<p>consensus that Horn&rsquo;s (1965) parallel analysis is the best method for deciding how
</p>
<p>many factors to extract (e.g., Dinno 2009; Hayton et al. 2004; Henson and Roberts
</p>
<p>2006; Zwick and Velicer 1986). The rationale underlying parallel analysis is that
</p>
<p>factors from real data with a valid underlying factor structure should have larger
</p>
<p>eigenvalues than those derived from randomly generated data (actually pseudoran-
</p>
<p>dom deviates) with the same sample size and number of variables.
</p>
<p>Parallel analysis involves several steps. First, a large number of datasets are
</p>
<p>randomly generated; they have the same number of observations and variables as
</p>
<p>the original dataset. Parallel PCAs are then run on each of the datasets (hence,
</p>
<p>parallel analysis), resulting in many slightly different sets of eigenvalues. Using
</p>
<p>these results as input, parallel analysis derives two relevant cues.
</p>
<p>First, parallel analysis adjusts the original eigenvalues for sampling error-
</p>
<p>induced collinearity among the variables to arrive at adjusted eigenvalues (Horn
</p>
<p>1965). Analogous to the Kaiser criterion, only factors with adjusted eigenvalues
</p>
<p>larger than 1 should be retained.
</p>
<p>Second, we can compare the randomly generated eigenvalues with those
</p>
<p>from the original analysis. Only factors whose original eigenvalues are larger
</p>
<p>than the 95th percentile of the randomly generated eigenvalues should be retained
</p>
<p>(Longman et al. 1989).
</p>
<p>8.3.3.4 Expectations
When, for example, replicating a previous market research study, we might have a
</p>
<p>priori information on the number of factors we want to find. For example, if a
</p>
<p>previous study suggests that a certain item set comprises five factors, we should
</p>
<p>extract the same number of factors, even if statistical criteria, such as the scree plot,
</p>
<p>suggest a different number. Similarly, theory might suggest that a certain number of
</p>
<p>factors should be extracted from the data.
</p>
<p>Strictly speaking, these are confirmatory approaches to factor analysis, which
</p>
<p>blur the distinction between these two factor analysis types. Ultimately however,
</p>
<p>we should not fully rely on the data, but keep in mind that the research results
</p>
<p>should be interpretable and actionable for market research practice.
</p>
<p>8.3 Principal Component Analysis 279</p>
<p/>
</div>
<div class="page"><p/>
<p>When using factor analysis, Stata allows for estimating two further criteria
</p>
<p>called the Akaike Information Criterion (AIC) and the Bayes Information
</p>
<p>Criterion (BIC). These criteria are relative measures of goodness-of-fit and
</p>
<p>are used to compare the adequacy of solutions with different numbers of
</p>
<p>factors. &ldquo;Relative&rdquo; means that these criteria are not scaled on a range of, for
</p>
<p>example, 0 to 1, but can generally take any value. Compared to an alternative
</p>
<p>solution with a different number of factors, smaller AIC or BIC values
</p>
<p>indicate a better fit. Stata computes solutions for different numbers of factors
</p>
<p>(up to the maximum number of factors specified before). We therefore need
</p>
<p>to choose the appropriate solution by looking for the smallest value in each
</p>
<p>criterion. When using these criteria, you should note that AIC is well known
</p>
<p>for overestimating the &ldquo;correct&rdquo; number of factors, while BIC has a slight
</p>
<p>tendency to underestimate this number.
</p>
<p>Whatever combination of approaches we use to determine the number of factors,
</p>
<p>the factors extracted should account for at least 50% of the total variance explained
</p>
<p>(75% or more is recommended). Once we have decided on the number of factors to
</p>
<p>retain from the data, we can start interpreting the factor solution.
</p>
<p>8.3.4 Interpret the Factor Solution
</p>
<p>8.3.4.1 Rotate the Factors
To interpret the solution, we have to determine which variables relate to each of the
</p>
<p>factors extracted. We do this by examining the factor loadings, which represent the
correlations between the factors and the variables and can take values ranging from
</p>
<p>�1 to +1. A high factor loading indicates that a certain factor represents a variable
</p>
<p>well. Subsequently, we look for high absolute values, because the correlation
between a variable and a factor can also be negative. Using the highest absolute
</p>
<p>factor loadings, we &ldquo;assign&rdquo; each variable to a certain factor and then produce a
</p>
<p>label for each factor that best characterizes the joint meaning of all the variables
</p>
<p>associated with it. This labeling is subjective, but a key PCA step. An example of a
</p>
<p>label is the respondents&rsquo; satisfaction with the stadium, which represents the items
</p>
<p>referring to its condition, outer appearance, and interior design (Fig. 8.1).
</p>
<p>We can make use of factor rotation to facilitate the factors&rsquo; interpretation.4 We
</p>
<p>do not have to rotate the factor solution, but it will facilitate interpreting the
</p>
<p>findings, particularly if we have a reasonably large number of items (say six or
</p>
<p>more). To understand what factor rotation is all about, once again consider the
</p>
<p>factor structure described in Fig. 8.4. Here, we see that both factors relate to the
</p>
<p>4Note that factor rotation primarily applies to factor analysis rather than PCA&mdash;see Preacher and
</p>
<p>MacCallum (2003) for details. However, our illustration draws on the factor, pcf command,
</p>
<p>which uses the factor analysis algorithm to compute PCA results for which rotation applies.
</p>
<p>280 8 Principal Component and Factor Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>variables in the set. However, the first factor appears to generally correlate more
</p>
<p>strongly with the variables, whereas the second factor only correlates weakly with
</p>
<p>the variables (to clarify, we look for small angles between the factors and
</p>
<p>variables). This implies that we &ldquo;assign&rdquo; all variables to the first factor without
</p>
<p>taking the second into consideration. This does not appear to be very meaningful, as
</p>
<p>we want both factors to represent certain facets of the variable set. Factor rotation
</p>
<p>can resolve this problem. By rotating the factor axes, we can create a situation in
</p>
<p>which a set of variables loads highly on only one specific factor, whereas another
</p>
<p>set loads highly on another. Figure 8.6 illustrates the factor rotation graphically.
</p>
<p>On the left side of the figure, we see that both factors are orthogonally rotated
</p>
<p>49�, meaning that a 90� angle is maintained between the factors during the rotation
</p>
<p>procedure. Consequently, the factors remain uncorrelated, which is in line with the
</p>
<p>PCA&rsquo;s initial objective. By rotating the first factor from F1 to F1
0, it is now strongly
</p>
<p>related to variables x1, x2, and x3, but weakly related to x4 and x5. Conversely, by
rotating the second factor from F2 to F2
</p>
<p>0 it is now strongly related to x4 and x5, but
weakly related to the remaining variables. The assignment of the variables is now
</p>
<p>much clearer, which facilitates the interpretation of the factors significantly.
</p>
<p>Various orthogonal rotation methods exist, all of which differ with regard to
</p>
<p>their treatment of the loading structure. The varimax rotation (the default option
</p>
<p>for orthogonal rotation in Stata) is the best-known one; this procedure aims at
</p>
<p>maximizing the dispersion of loadings within factors, which means a few variables
</p>
<p>will have high loadings, while the remaining variables&rsquo; loadings will be consider-
</p>
<p>ably smaller (Kaiser 1958).
</p>
<p>Alternatively, we can choose between several oblique rotation techniques. In
</p>
<p>oblique rotation, the 90� angle between the factors is not maintained during
</p>
<p>rotation, and the resulting factors are therefore correlated. Figure 8.6 (right side)
</p>
<p>illustrates an example of an oblique factor rotation. Promax rotation (the default
</p>
<p>option for oblique rotation in Stata) is a commonly used oblique rotation technique.
</p>
<p>The Promax rotation allows for setting an exponent (referred to as Promax power in
Stata) that needs to be greater than 1. Higher values make the loadings even more
</p>
<p>extreme (i.e., high loadings are amplified and weak loadings are reduced even
</p>
<p>further), which is at the cost of stronger correlations between the factors and less
</p>
<p>total variance explained (Hamilton 2013). The default value of 3 works well for
</p>
<p>most applications. Oblimin rotation is a popular alternative oblique rotation type.
</p>
<p>Fig. 8.6 Orthogonal and oblique factor rotation
</p>
<p>8.3 Principal Component Analysis 281</p>
<p/>
</div>
<div class="page"><p/>
<p>Oblimin is a class of rotation procedures whereby the degree of obliqueness can be
</p>
<p>specified. This degree is the gamma, which determines the level of the correlation
allowed between the factors. A gamma of zero (the default) ensures that the factors
</p>
<p>are&mdash;if at all&mdash;only moderately correlated, which is acceptable for most analyses.5
</p>
<p>Oblique rotation is used when factors are possibly related. It is, for example,
</p>
<p>very likely that the respondents&rsquo; satisfaction with the stadium is related to their
</p>
<p>satisfaction with other aspects of the soccer club, such as the number of stars in the
</p>
<p>team or the quality of the merchandise. However, relinquishing the initial objective
</p>
<p>of extracting uncorrelated factors can diminish the factors&rsquo; interpretability. We
</p>
<p>therefore recommend using the varimax rotation to enhance the interpretability of
</p>
<p>the results. Only if the results are difficult to interpret, should an oblique rotation be
</p>
<p>applied. Among the oblique rotation methods, researchers generally recommend
</p>
<p>the promax (Gorsuch 1983) or oblimin (Kim and Mueller 1978) methods but
</p>
<p>differences between the rotation types are typically marginal (Brown 2009).
</p>
<p>8.3.4.2 Assign the Variables to the Factors
After rotating the factors, we need to interpret them and give each factor a name,
</p>
<p>which has some descriptive value. Interpreting factors involves assigning each vari-
</p>
<p>able to a specific factor based on the highest absolute (!) loading. For example, if a
variable has a 0.60 loading with the first factor and a 0.20 loading with the second, we
</p>
<p>would assign this variable to the first factor. Loadings may nevertheless be very
</p>
<p>similar (e.g., 0.50 for the first factor and 0.55 for the second one), making the
</p>
<p>assignment ambiguous. In such a situation, we could assign the variable to another
</p>
<p>factor, even though it does not have the highest loading on this specific factor. While
</p>
<p>this step can help increase the results&rsquo; face validity (see Chap. 3), we shouldmake sure
</p>
<p>that the variable&rsquo;s factor loading with the designated factor is above an acceptable
</p>
<p>level. If very few factors have been extracted, the loading should be at least 0.50, but
</p>
<p>with a high number of factors, lower loadings of above 0.30 are acceptable. Alterna-
</p>
<p>tively, some simply ignore a certain variable if it does not fit with the factor structure.
</p>
<p>In such a situation, we should re-run the analysis without variables that do not load
</p>
<p>highly on a specific factor. In the end, the results should be interpretable and
</p>
<p>actionable, but keep in mind that this technique is, first and foremost, exploratory!
</p>
<p>8.3.5 Evaluate the Goodness-of-Fit of the Factor Solution
</p>
<p>8.3.5.1 Check the Congruence of the Initial and Reproduced
Correlations
</p>
<p>While PCA focuses on explaining the variables&rsquo; variances, checking how well the
</p>
<p>method approximates the correlation matrix allows for assessing the quality of the
</p>
<p>5When the gamma is set to 1, this is a special case, because the value of 1 represents orthogonality.
</p>
<p>The result of setting gamma to 1 is effectively a varimax rotation.
</p>
<p>282 8 Principal Component and Factor Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>solution (i.e., the goodness-of-fit) (Graffelman 2013). More precisely, to assess the
</p>
<p>solution&rsquo;s goodness-of-fit, we can make use of the differences between the
</p>
<p>correlations in the data and those that the factors imply. These differences are
</p>
<p>also called correlation residuals and should be as small as possible.
</p>
<p>In practice, we check the proportion of correlation residuals with an absolute
</p>
<p>value higher than 0.05. Even though there is no strict rule of thumb regarding the
</p>
<p>maximum proportion, a proportion of more than 50% should raise concern. How-
</p>
<p>ever, high residuals usually go hand in hand with an unsatisfactory KMO measure;
</p>
<p>consequently, this problem already surfaces when testing the assumptions.
</p>
<p>8.3.5.2 Check How Much of Each Variable&rsquo;s Variance Is Reproduced
by Means of Factor Extraction
</p>
<p>Another way to check the solution&rsquo;s goodness-of-fit is by evaluating how much of
</p>
<p>each variable&rsquo;s variance the factors reproduce (i.e., the communality). If several
</p>
<p>communalities exhibit low values, we should consider removing these variables.
</p>
<p>Considering the variable-specific MSA measures could help us make this decision.
</p>
<p>If there are more variables in the dataset, communalities usually become smaller;
</p>
<p>however, if the factor solution accounts for less than 50% of a variable&rsquo;s variance
</p>
<p>(i.e., the variable&rsquo;s communality is less than 0.50), it is worthwhile reconsidering
</p>
<p>the set-up. Since Stata does not provide communality but uniqueness values, we
</p>
<p>have to make this decision in terms of the variance that the factors do not reproduce.
That is, if several variables exhibit uniqueness values larger than 0.50, we should
</p>
<p>reconsider the analysis.
</p>
<p>8.3.6 Compute the Factor Scores
</p>
<p>After the rotation and interpretation of the factors, we can compute the factor
</p>
<p>scores, another element of the analysis. Factor scores are linear combinations of the
</p>
<p>items and can be used as separate variables in subsequent analyses. For example,
</p>
<p>instead of using many highly correlated independent variables in a regression
</p>
<p>analysis, we can use few uncorrelated factors to overcome collinearity problems.
</p>
<p>The simplest ways to compute factor scores for each observation is to sum all the
</p>
<p>scores of the items assigned to a factor. While easy to compute, this approach
</p>
<p>neglects the potential differences in each item&rsquo;s contribution to each factor
</p>
<p>(Sarstedt et al. 2016).
</p>
<p>Drawing on the eigenvectors that the PCA produces, which include the factor
</p>
<p>weights, is a more elaborate way of computing factor scores (Hershberger 2005).
</p>
<p>These weights indicate each item&rsquo;s relative contribution to forming the factor; we
</p>
<p>simply multiply the standardized variables&rsquo; values with the weights to get the factor
</p>
<p>scores. Factor scores computed on the basis of eigenvectors have a zero mean. This
</p>
<p>means that if a respondent has a value greater than zero for a certain factor, he/she
</p>
<p>scores above the above average in terms of the characteristic that this factor
</p>
<p>describes. Conversely, if a factor score is below zero, then this respondent exhibits
</p>
<p>the characteristic below average.
</p>
<p>8.3 Principal Component Analysis 283</p>
<p/>
</div>
<div class="page"><p/>
<p>Different from the PCA, a factor analysis does not produce determinate factor
</p>
<p>scores. In other words, the factor is indeterminate, which means that part of it
</p>
<p>remains an arbitrary quantity, capable of taking on an infinite range of values (e.g.,
</p>
<p>Grice 2001; Steiger 1979). Thus, we have to rely on other approaches to computing
</p>
<p>factor scores such as the regression method, which features prominently among
</p>
<p>factor analysis users. This method takes into account (1) the correlation between the
</p>
<p>factors and variables (via the item loadings), (2) the correlation between the
</p>
<p>variables, and (3) the correlation between the factors if oblique rotation has been
</p>
<p>used (DiStefano et al. 2009). The regression method z-standardizes each factor to
zero mean and unit standard deviation.6We can therefore interpret an observation&rsquo;s
</p>
<p>score in relation to the mean and in terms of the units of standard deviation from this
</p>
<p>mean. For example, an observation&rsquo;s factor score of 0.79 implies that this observa-
</p>
<p>tion is 0.79 standard deviations above the average with regard to the corresponding
</p>
<p>factor.
</p>
<p>Another popular approach is the Bartlett method, which is similar to the
</p>
<p>regression method. The method produces factor scores with zero mean and standard
</p>
<p>deviations larger than one. Owing to the way they are estimated, the factor scores
</p>
<p>that the Bartlett method produces are considered are considered more accurate
</p>
<p>(Hershberger 2005). However, in practical applications, both methods yield highly
</p>
<p>similar results. Because of the z-standardization of the scores, which facilitates the
comparison of scores across factors, we recommend using the regression method.
</p>
<p>In Table 8.3 we summarize the main steps that need to be taken when conducting
</p>
<p>a PCA in Stata. Our descriptions draw on Stata&rsquo;s factor, pcf command, which
</p>
<p>carries out a factor analysis but rescales the resulting factors such that the results
</p>
<p>conform to a standard PCA. This approach has the advantage that it follows the
</p>
<p>fundamentals of PCA, while allowing for analyses that are restricted to factor
</p>
<p>analysis (e.g., factor rotation, use of AIC and BIC).
</p>
<p>8.4 Confirmatory Factor Analysis and Reliability Analysis
</p>
<p>Many researchers and practitioners acknowledge the prominent role that explor-
</p>
<p>atory factor analysis plays in exploring data structures. Data can be analyzed
</p>
<p>without preconceived ideas of the number of factors or how these relate to the
</p>
<p>variables under consideration. Whereas this approach is, as its name implies,
</p>
<p>exploratory in nature, the confirmatory factor analysis allows for testing
hypothesized structures underlying a set of variables.
</p>
<p>In a confirmatory factor analysis, the researcher needs to first specify the
</p>
<p>constructs and their associations with variables, which should be based on previous
</p>
<p>measurements or theoretical considerations.
</p>
<p>6Note that this is not the case when using factor analysis if the standard deviations are different
</p>
<p>from one (DiStefano et al. 2009).
</p>
<p>284 8 Principal Component and Factor Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>Table 8.3 Steps involved in carrying out a PCA in Stata
</p>
<p>Theory Stata
</p>
<p>Check Assumptions and Carry Out Preliminary Analyses
</p>
<p>Select variables that should be reduced to a set
</p>
<p>of underlying factors (PCA) or should be used
</p>
<p>to identify underlying dimensions (factor
</p>
<p>analysis)
</p>
<p>► Statistics ► Multivariate analysis ► Factor
</p>
<p>and principal component analysis ► Factor
</p>
<p>analysis. Enter the variables in the Variables
</p>
<p>box.
</p>
<p>Are the variables interval or ratio scaled? Determine the measurement level of your
</p>
<p>variables (see Chap. 3). If ordinal variables are
</p>
<p>used, make sure that the scale steps are
</p>
<p>equidistant.
</p>
<p>Is the sample size sufficiently large? Check MacCallum et al.&rsquo;s (1999) guidelines
</p>
<p>for minimum sample size requirements,
</p>
<p>dependent on the variables&rsquo; communality. For
</p>
<p>example, if all the communalities are above
</p>
<p>0.60, small sample sizes of below 100 are
</p>
<p>adequate. With communalities around 0.50,
</p>
<p>sample sizes between 100 and 200 are
</p>
<p>sufficient.
</p>
<p>Are the observations independent? Determine whether the observations are
</p>
<p>dependent or independent (see Chap. 3).
</p>
<p>Are the variables sufficiently correlated? Check whether at least some of the variable
</p>
<p>correlations are significant. ► Statistics ►
</p>
<p>Summaries, tables, and tests ► Summary and
</p>
<p>descriptive statistics ► Pairwise correlations.
</p>
<p>Check Print number of observations for
</p>
<p>each entry and Print significance level for
</p>
<p>each entry. Select Use Bonferroni-adjusted
</p>
<p>significance level to maintain the familywise
</p>
<p>error rate (see Chap. 6).
</p>
<p>pwcorr s1 s2 s3 s4 s5 s6 s7 s8, obs
</p>
<p>sig bonferroni
</p>
<p>Is the KMO � 0.50? ► Statistics ►
</p>
<p>Postestimation ► Factor analysis reports and
</p>
<p>graphs ► Kaiser-Meyer-Olkin measure of
</p>
<p>sample adequacy. Then click on Launch and
</p>
<p>OK. Note that this analysis can only be run
</p>
<p>after the PCA has been conducted.
</p>
<p>estat kmo
</p>
<p>Extract the factors
</p>
<p>Choose the method of factor analysis ► Statistics ► Multivariate analysis ► Factor
</p>
<p>and principal component analysis ► Factor
</p>
<p>analysis. Click on the Model 2 tab and select
</p>
<p>Principal component factor.
</p>
<p>factor s1 s2 s3 s4 s5 s6 s7 s8, pcf
</p>
<p>Determine the number of factors
</p>
<p>Determine the number of factors Kaiser criterion: ► Statistics ► Multivariate
</p>
<p>analysis ► Factor and principal component
</p>
<p>analysis ► Factor analysis. Click on the
</p>
<p>Model 2 tab and enter 1 under Minimum
</p>
<p>value of eigenvalues to be retained.
</p>
<p>(continued)
</p>
<p>8.4 Confirmatory Factor Analysis and Reliability Analysis 285</p>
<p/>
</div>
<div class="page"><p/>
<p>Table 8.3 (continued)
</p>
<p>Theory Stata
</p>
<p>factor s1 s2 s3 s4 s5 s6 s7 s8 pcf
</p>
<p>mineigen (1)
</p>
<p>Parallel analysis: Download and install paran
(help paran) and enter paran s1 s2 s3
</p>
<p>s4 s5 s6 s7 s8, centile (95) q all
</p>
<p>graph
</p>
<p>Extract factors (1) with adjusted eigenvalues
</p>
<p>greater than 1, and (2) whose adjusted
</p>
<p>eigenvalues are greater than the random
</p>
<p>eigenvalues.
</p>
<p>Scree plot: ► Statistics ► Postestimation ►
</p>
<p>Factor analysis reports and graphs ► Scree
</p>
<p>plot of eigenvalues. Then click onLaunch and
</p>
<p>OK.
</p>
<p>screeplot
</p>
<p>Pre-specify the number of factors based on a
</p>
<p>priori information: ► Statistics ►
</p>
<p>Multivariate analysis ► Factor and principal
</p>
<p>component analysis ► Factor analysis. Under
</p>
<p>the Model 2 tab, tick Maximum number of
</p>
<p>factors to be retained and specify a value in
</p>
<p>the box below (e.g., 2).
</p>
<p>factor s1 s2 s3 s4 s5 s6 s7 s8,
</p>
<p>factors(2)
</p>
<p>Make sure that the factors extracted account
</p>
<p>for at least 50% of the total variance explained
</p>
<p>(75% or more is recommended): Check the
</p>
<p>Cumulative column in the PCA output.
</p>
<p>Interpret the Factor Solution
</p>
<p>Rotate the factors Use the varimax procedure or, if necessary,
</p>
<p>the promax procedure with gamma set to
</p>
<p>3 (both with Kaiser normalization): ►
</p>
<p>Statistics ► Postestimation ► Principal
</p>
<p>component analysis reports and graphs ►
</p>
<p>Rotate factor loadings. Select the
</p>
<p>corresponding option in the menu.
</p>
<p>Varimax: rotate, kaiser
</p>
<p>Promax: rotate, promax(3) oblique
</p>
<p>Kaiser
</p>
<p>Assign variables to factors Check the Factor loadings (pattern matrix)
</p>
<p>table in the output of the rotated solution.
</p>
<p>Assign each variable to a certain factor based
</p>
<p>on the highest absolute loading. To facilitate
</p>
<p>interpretation, you may also assign a variable
</p>
<p>to a different factor, but check that the loading
</p>
<p>is at an acceptable level (0.50 if only a few
</p>
<p>factors are extracted, 0.30 if many factors are
</p>
<p>extracted).
</p>
<p>Consider making a loadings plot: ► Statistics
</p>
<p>► Postestimation ► Factor analysis reports
</p>
<p>(continued)
</p>
<p>286 8 Principal Component and Factor Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>Instead of allowing the procedure to determine the number of factors, as is done
</p>
<p>in an exploratory factor analysis, a confirmatory factor analysis tells us how well the
</p>
<p>actual data fit the pre-specified structure. Reverting to our introductory example, we
</p>
<p>could, for example, assume that the construct satisfaction with the stadium can be
measured by means of the three items x1 (condition of the stadium), x2 (appearance
of the stadium), and x3 (interior design of the stadium). Likewise, we could
hypothesize that satisfaction with the merchandise can be adequately measured
using the items x4 and x5. In a confirmatory factor analysis, we set up a theoretical
model linking the items with the respective construct (note that in confirmatory
</p>
<p>factor analysis, researchers generally use the term construct rather than factor). This
</p>
<p>process is also called operationalization (see Chap. 3) and usually involves drawing
</p>
<p>a visual representation (called a path diagram) indicating the expected
</p>
<p>relationships.
</p>
<p>Figure 8.7 shows a path diagram&mdash;you will notice the similarity to the diagram
</p>
<p>in Fig. 8.1. Circles or ovals represent the constructs (e.g., Y1, satisfaction with the
stadium) and boxes represent the items (x1 to x5). Other elements include the
relationships between the constructs and respective items (the loadings l1 to l5),
the error terms (e1 to e5) that capture the extent to which a construct does not
explain a specific item, and the correlations between the constructs of interest (r12).
</p>
<p>Having defined the individual constructs and developed the path diagram, we
</p>
<p>can estimate the model. The relationships between the constructs and items (the
</p>
<p>loadings l1 to l5) and the item correlations (not shown in Fig. 8.7) are of particular
</p>
<p>Table 8.3 (continued)
</p>
<p>Theory Stata
</p>
<p>and graphs ► Plot of factor loadings. Under
</p>
<p>Plot all combinations of the following,
</p>
<p>indicate the number of factors for which you
</p>
<p>want to plot. Check which items load highly
</p>
<p>on which factor.
</p>
<p>Compute factor scores Save factor scores as new variables: ►
</p>
<p>Statistics ► Postestimation ► Predictions ►
</p>
<p>Regression and Bartlett scores. Under New
</p>
<p>variable names or variable stub* enter
</p>
<p>factor*. Select Factors scored by the
regression scoring method
</p>
<p>predict score*, regression
</p>
<p>Evaluate the Goodness-of-fit of the Factor Solution
</p>
<p>Check the congruence of the initial and
</p>
<p>reproduced correlations
</p>
<p>Create a reproduced correlation matrix: ►
</p>
<p>Statistics ► Postestimation ► Factor analysis
</p>
<p>reports and graphs ► Matrix of correlation
</p>
<p>residuals. Is the proportion of residuals greater
</p>
<p>than 0.05 � 50%?
</p>
<p>estat residuals
</p>
<p>Check how much of each variable&rsquo;s variance
</p>
<p>is reproduced by means of factor extraction
</p>
<p>Check the Uniqueness column in the PCA
</p>
<p>output. Are all the values lower than 0.50?
</p>
<p>8.4 Confirmatory Factor Analysis and Reliability Analysis 287</p>
<p/>
</div>
<div class="page"><p/>
<p>interest, as they indicate whether the construct has been reliably and validly
</p>
<p>measured.
</p>
<p>Reliability analysis is an important element of a confirmatory factor analysis and
essential when working with measurement scales. The preferred way to evaluate
</p>
<p>reliability is by taking two independent measurements (using the same subjects)
</p>
<p>and comparing these by means of correlations. This is also called test-retest
</p>
<p>reliability (see Chap. 3). However, practicalities often prevent researchers from
</p>
<p>surveying their subjects a second time.
</p>
<p>An alternative is to estimate the split-half reliability. In the split-half reliability,
</p>
<p>scale items are divided into halves and the scores of the halves are correlated to
</p>
<p>obtain an estimate of reliability. Since all items should be consistent regarding what
</p>
<p>they indicate about the construct, the halves can be considered approximations of
</p>
<p>alternative forms of the same scale. Consequently, instead of looking at the scale&rsquo;s
</p>
<p>test-retest reliability, researchers consider the scale&rsquo;s equivalence, thus showing the
</p>
<p>extent to which two measures of the same general trait agree. We call this type of
</p>
<p>reliability the internal consistency reliability.
</p>
<p>In the example of satisfaction with the stadium, we compute this scale&rsquo;s split-
half reliability manually by, for example, splitting up the scale into x1 on the one
side and x2 and x3 on the other. We then compute the sum of x2 and x3 (or calculate
the items&rsquo; average) to form a total score and correlate this score with x1. A high
correlation indicates that the two subsets of items measure related aspects of the
</p>
<p>same underlying construct and, thus, suggests a high degree of internal consistency.
</p>
<p>Fig. 8.7 Path diagram (confirmatory factor analysis)
</p>
<p>288 8 Principal Component and Factor Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>However, with many indicators, there are many different ways to split the variables
</p>
<p>into two groups.
</p>
<p>Cronbach (1951) proposed calculating the average of all possible split-half
</p>
<p>coefficients resulting from different ways of splitting the scale items. The
</p>
<p>Cronbach&rsquo;s Alpha coefficient has become by far the most popular measure of
</p>
<p>internal consistency. In the example above, this would comprise calculating the
</p>
<p>average of the correlations between (1) x1 and x2 + x3, (2) x2 and x1 + x3, as well as
(3) x3 and x1 + x2. The Cronbach&rsquo;s Alpha coefficient generally varies from 0 to
1, whereas a generally agreed lower limit for the coefficient is 0.70. However, in
</p>
<p>exploratory studies, a value of 0.60 is acceptable, while values of 0.80 or higher are
</p>
<p>regarded as satisfactory in the more advanced stages of research (Hair et al. 2011).
</p>
<p>In Box 8.3, we provide more advice on the use of Cronbach&rsquo;s Alpha. We illustrate a
</p>
<p>reliability analysis using the standard Stata module in the example at the end of this
</p>
<p>chapter.
</p>
<p>Box 8.3 Things to Consider When Calculating Cronbach&rsquo;s Alpha
</p>
<p>When calculating Cronbach&rsquo;s Alpha, ensure that all items are formulated in
</p>
<p>the same direction (positively or negatively worded). For example, in psy-
</p>
<p>chological measurement, it is common to use both negatively and positively
</p>
<p>worded items in a questionnaire. These need to be reversed prior to the
</p>
<p>reliability analysis. In Stata, this is done automatically when an item is
</p>
<p>negatively correlated with the other items. It is possible to add the option,
</p>
<p>reverse (variable name) to force Stata to reverse an item or you can
</p>
<p>stop Stata from reversing items automatically by using, asis. Furthermore,
</p>
<p>we have to be aware of potential subscales in our item set. Some multi-item
</p>
<p>scales comprise subsets of items that measure different facets of a multidi-
</p>
<p>mensional construct. For example, soccer fan satisfaction is a multidimen-
</p>
<p>sional construct that includes aspects such as satisfaction with the stadium,
</p>
<p>the merchandise (as described above), the team, and the coach, each
</p>
<p>measured with a different item set. It would be inappropriate to calculate
</p>
<p>one Cronbach&rsquo;s Alpha value for all 99 items. Cronbach&rsquo;s Alpha is always
</p>
<p>calculated over the items belonging to one construct and not all the items in
</p>
<p>the dataset!
</p>
<p>8.5 Structural Equation Modeling
</p>
<p>Whereas a confirmatory factor analysis involves testing if and how items relate to
</p>
<p>specific constructs, structural equation modeling involves the estimation of
relations between these constructs. It has become one of the most important
</p>
<p>methods in social sciences, including marketing research.
</p>
<p>There are broadly two approaches to structural equation modeling: Covariance-
</p>
<p>based structural equation modeling (e.g., J&euro;oreskog 1971) and partial least
</p>
<p>8.5 Structural Equation Modeling 289</p>
<p/>
</div>
<div class="page"><p/>
<p>squares structural equation modeling (e.g., Wold 1982), simply referred to as
</p>
<p>CB-SEM and PLS-SEM. Both estimation methods are based on the idea of an
</p>
<p>underlying model that allows the researcher to test relationships between multiple
</p>
<p>items and constructs.
</p>
<p>Figure 8.8 shows an example path diagram with four constructs (represented by
</p>
<p>circles or ovals) and their respective items (represented by boxes).7 A path model
</p>
<p>incorporates two types of constructs: (1) exogenous constructs (here, satisfaction
</p>
<p>with the stadium (Y1) and satisfaction with the merchandise (Y2)) that do not depend
on other constructs, and (2) endogenous constructs (here, overall satisfaction (Y3)
and loyalty (Y4)) that depend on one or more exogenous (or other endogenous)
constructs. The relations between the constructs (indicated with p) are called path
coefficients, while the relations between the constructs and their respective items
</p>
<p>(indicated with l ) are the indicator loadings. One can distinguish between the
structural model that incorporates the relations between the constructs and the
</p>
<p>(exogenous and endogenous) measurement models that represent the relationships
</p>
<p>between the constructs and their related items. Items that measure constructs are
</p>
<p>labeled x.
In the model in Fig. 8.8, we assume that the two exogenous constructs satisfac-
</p>
<p>tion with the stadium and satisfaction with the merchandise relate to the endoge-
nous construct overall satisfaction and that overall satisfaction relates to loyalty.
Depending on the research question, we could of course incorporate additional
</p>
<p>exogenous and endogenous constructs. Using empirical data, we could then test this
</p>
<p>model and, thus, evaluate the relationships between all the constructs and between
</p>
<p>each construct and its indicators. We could, for example, assess which of the two
</p>
<p>Fig. 8.8 Path diagram (structural equation modeling)
</p>
<p>7Note that we omitted the error terms for clarity&rsquo;s sake.
</p>
<p>290 8 Principal Component and Factor Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>constructs, Y1 or Y2, exerts the greater influence on Y3. The result would guide us
when developing marketing plans in order to increase overall satisfaction and,
</p>
<p>ultimately, loyalty by answering the research question whether we should rather
</p>
<p>concentrate on increasing the fans&rsquo; satisfaction with the stadium or with the
</p>
<p>merchandise.
</p>
<p>The evaluation of a path model analysis requires several steps that include the
</p>
<p>assessment of both measurement models and the structural model. Diamantopoulos
</p>
<p>and Siguaw (2000) and Hair et al. (2013) provide a thorough description of the
</p>
<p>covariance-based structural equation modeling approach and its application. Acock
</p>
<p>(2013) provides a detailed explanation of how to conduct covariance-based struc-
</p>
<p>tural equation modeling analyses in Stata. Hair et al. (2017a, b, 2018) provide a
</p>
<p>step-by-step introduction on how to set up and test path models using partial least
</p>
<p>squares structural equation modeling.
</p>
<p>8.6 Example
</p>
<p>In this example, we take a closer look at some of the items from the Oddjob
</p>
<p>Airways dataset ( Web Appendix ! Downloads). This dataset contains eight
</p>
<p>items that relate to the customers&rsquo; experience when flying with Oddjob Airways.
</p>
<p>For each of the following items, the respondents had to rate their degree of
</p>
<p>agreement from 1 (&ldquo;completely disagree&rdquo;) to 100 (&ldquo;completely agree&rdquo;). The vari-
</p>
<p>able names are included below:
</p>
<p>&ndash; with Oddjob Airways you will arrive on time (s1),
&ndash; the entire journey with Oddjob Airways will occur as booked (s2),
&ndash; in case something does not work out as planned, Oddjob Airways will find a
</p>
<p>good solution (s3),
&ndash; the flight schedules of Oddjob Airways are reliable (s4),
&ndash; Oddjob Airways provides you with a very pleasant travel experience (s5),
&ndash; Oddjob Airways&rsquo;s on board facilities are of high quality (s6),
&ndash; Oddjob Airways&rsquo;s cabin seats are comfortable (s7), and
&ndash; Oddjob Airways offers a comfortable on-board experience (s8).
</p>
<p>Our aim is to reduce the complexity of this item set by extracting several factors.
</p>
<p>Hence, we use these items to run a PCA using the factor, pcf procedure in
</p>
<p>Stata.
</p>
<p>8.6.1 Principal Component Analysis
</p>
<p>8.6.1.1 Check Requirements and Conduct Preliminary Analyses
All eight variables are interval scaled from 1 (&ldquo;very unsatisfied&rdquo;) to 100 (&ldquo;very
</p>
<p>satisfied&rdquo;), therefore meeting the requirements in terms of the measurement scale.
</p>
<p>8.6 Example 291</p>
<p/>
</div>
<div class="page"><p/>
<p>With 1,065 independent observations, the sample size requirements are clearly met,
</p>
<p>even if the analysis yields very low communality values.
</p>
<p>Determining if the variables are sufficiently correlated is easy if we go to ►
</p>
<p>Statistics ► Summaries, tables, and tests ► Summary and descriptive statistics ►
</p>
<p>Pairwise correlations. In the dialog box shown in Fig. 8.9, either enter each variable
</p>
<p>separately (i.e., s1 s2 s3 etc.) or simply write s1-s8 as the variables appear in this
order in the dataset. Then also tick Print number of observations for each entry,
</p>
<p>Print significance levels for each entry, as well as Use Bonferroni-adjusted
</p>
<p>significance level. The latter option corrects for the many tests we execute at the
</p>
<p>same time and is similar to what we discussed in Chap. 6.
</p>
<p>Table 8.4 shows the resulting output. The values in the diagonal are all 1.000,
</p>
<p>which is logical, as this is the correlation between a variable and itself! The
</p>
<p>off-diagonal cells correspond to the pairwise correlations. For example, the pairwise
</p>
<p>correlation between s1 and s2 is 0.7392. The value under it denotes the p-value
(0.000), indicating that the correlation is significant. To determine an absolute
</p>
<p>minimum standard, check if at least one correlation in all the off-diagonal cells is
</p>
<p>significant. The last value of 1037 indicates the sample size for the correlation
</p>
<p>between s1 and s2.
The correlation matrix in Table 8.4 indicates that there are several pairs of highly
</p>
<p>correlated variables. For example, not only s1 is highly correlated with s2 (correla-
tion &frac14; 0.7392), but also s3 is highly correlated with s1 (correlation &frac14; 0.6189), just
</p>
<p>Fig. 8.9 Pairwise correlations of variables
</p>
<p>292 8 Principal Component and Factor Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>like s4 (correlation &frac14; 0.7171). As these variables&rsquo; correlations with the remaining
ones are less pronounced, we suspect that these four variables constitute one factor.
</p>
<p>As you can see by just looking at the correlation matrix, we can already identify the
</p>
<p>factor structure that might result.
</p>
<p>However, at this point of the analysis, we are more interested in checking
</p>
<p>whether the variables are sufficiently correlated to conduct a PCA. When we
</p>
<p>examine Table 8.4, we see that all correlations have p-values below 0.05. This
result indicates that the variables are sufficiently correlated. However, for a
</p>
<p>concluding evaluation, we need to take the anti-image and related statistical
</p>
<p>measures into account. Most importantly, we should also check if the KMO
</p>
<p>values are at least 0.50. As we can only do this after the actual PCA, we will
discuss this point later.
</p>
<p>Table 8.4 Pairwise correlation matrix
</p>
<p>pwcorr s1-s8, obs sig bonferroni
</p>
<p>|       s1       s2       s3       s4       s5       s6       s7
-------------+---------------------------------------------------------------
</p>
<p>s1 |   1.0000 
|
|     1038
|
</p>
<p>s2 |   0.7392   1.0000 
|   0.0000
|     1037     1040
|
</p>
<p>s3 |   0.6189   0.6945   1.0000 
|   0.0000   0.0000
|      952      952      954
|
</p>
<p>s4 |   0.7171   0.7655   0.6447   1.0000 
|   0.0000   0.0000   0.0000
|     1033     1034      951     1035
|
</p>
<p>s5 |   0.5111   0.5394   0.5593   0.4901   1.0000 
|   0.0000   0.0000   0.0000   0.0000
|     1026     1027      945     1022     1041
|
</p>
<p>s6 |   0.4898   0.4984   0.4972   0.4321   0.8212   1.0000 
|   0.0000   0.0000   0.0000   0.0000   0.0000
|     1025     1027   943     1022     1032     1041
|
</p>
<p>s7 |   0.4530   0.4555   0.4598   0.3728   0.7873   0.8331   1.0000 
|   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000
|     1030     1032      947     1027     1038 1037     1048
|
</p>
<p>s8 |   0.5326   0.5329   0.5544   0.4822   0.8072   0.8401   0.7773 
|   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000
|     1018     1019      937     1014     1029     1025     1032
|
</p>
<p>|       s8
-------------+---------
</p>
<p>s8 |   1.0000 
|
|     1034
|
</p>
<p>8.6 Example 293</p>
<p/>
</div>
<div class="page"><p/>
<p>8.6.1.2 Extract the Factors and Determine the Number of Factors
To run the PCA, click on ► Statistics ► Multivariate analysis ► Factor and
</p>
<p>principal component analysis ► Factor analysis, which will open a dialog box
</p>
<p>similar to Fig. 8.10. Next, enter s1 s2 s3 s4 s5 s6 s7 s8 in the Variables box.
Alternatively, you can also simply enter s1-s8 in the box as the dataset includes
these eight variables in consecutive order.
</p>
<p>Under the Model 2 tab (see Fig. 8.11), we can choose the method for factor
</p>
<p>extraction, the maximum factors to be retained, as well as the minimum value for
</p>
<p>the eigenvalues to be retained. As the aim of our analysis is to reproduce the data
</p>
<p>structure, we choose Principal-component factor, which initiates the PCA based
</p>
<p>on Stata&rsquo;s factor, pcf procedure. By clicking on Minimum value of
</p>
<p>eigenvalues to be retained and entering 1 in the box below, we specify the Kaiser
</p>
<p>criterion. If we have a priori information on the factor structure, we can specify the
</p>
<p>number of factors manually by clicking on Maximum number of factors to be
</p>
<p>retained. Click on OK and Stata will display the results (Table 8.5).
</p>
<p>Before we move to the further interpretation of the PCA results in Table 8.5, let&rsquo;s
</p>
<p>first take a look at the KMO.Although we need to know if the KMO is larger than 0.50
</p>
<p>to interpret the PCA results with confidence, we can only do this after having run the
</p>
<p>PCA. We can calculate the KMO values by going to ► Statistics ► Postestimation
</p>
<p>► Principal component analysis reports and graphs ► Kaiser-Meyer-Olkin measure
</p>
<p>of sample adequacy (Fig. 8.12). In the dialog box that opens, simply click on OK to
</p>
<p>initiate the analysis.
</p>
<p>The analysis result at the bottom of Table 8.6 reveals that the KMO value is
</p>
<p>0.9073, which is &ldquo;marvelous&rdquo; (see Table 8.2). Likewise, the variable-specific MSA
</p>
<p>Fig. 8.10 Factor analysis dialog box
</p>
<p>294 8 Principal Component and Factor Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>values in the table are all above the threshold value of 0.50. For example, s1 has an
MSA value of 0.9166.
</p>
<p>The output in Table 8.5 shows three blocks. On the top right of the first block,
</p>
<p>Stata shows the number of observations used in the analysis (Number of obs&frac14; 921)
</p>
<p>and indicates that the analysis yielded two factors (Retained factors &frac14; 2). In the
</p>
<p>second block, Stata indicates the eigenvalues for each factor. With an eigenvalue of
</p>
<p>5.24886, the first factor extracts a large amount of variance, which accounts for
</p>
<p>5.24886/8 &frac14; 65.61% of the total variance (see column: Proportion). With an
</p>
<p>eigenvalue of 1.32834, factor two extracts less variance (16.60%). Using theKaiser
</p>
<p>criterion (i.e., eigenvalue &gt;1), we settle on two factors, because the third factor&rsquo;s
</p>
<p>eigenvalue is clearly lower than 1 (0.397267). The Cumulative column indicates
</p>
<p>the cumulative variance extracted. The two factors extract 0.8222 or 82.22% of the
</p>
<p>variance, which is highly satisfactory. The next block labeled Factor loadings
</p>
<p>(pattern matrix) and unique variances shows the factor loadings along with the
</p>
<p>Uniqueness, which indicates the amount of each variable&rsquo;s variance that the factors
</p>
<p>cannot reproduce (i.e., 1-communality) and is therefore lost in the process. All
</p>
<p>uniqueness values are very low, indicating that the factors reproduce the variables&rsquo;
</p>
<p>variance well. Specifically, with a value of 0.3086, s3 exhibits the highest unique-
ness value, which suggests a communality of 1&ndash;0.3086 &frac14; 0.6914 and is clearly
</p>
<p>above the 0.50 threshold.
</p>
<p>Beside the Kaiser criterion, the scree plot helps determine the number of factors.
</p>
<p>To create a scree plot, go to ► Statistics ► Postestimation ► Principal component
</p>
<p>analysis reports and graphs► Scree plot of eigenvalues. Then click on Launch and
</p>
<p>Fig. 8.11 Factor analysis dialog box (options)
</p>
<p>8.6 Example 295</p>
<p/>
</div>
<div class="page"><p/>
<p>OK. Stata will produce a graph as shown in Fig. 8.13. There is an &ldquo;elbow&rdquo; in the
</p>
<p>line at three factors. As the number of factors that the scree plot suggests is one
</p>
<p>factor less than the elbow indicates, we conclude that two factors are appropriate.
</p>
<p>This finding supports the conclusion based on the Kaiser criterion.
</p>
<p>Stata also allows for plotting each eigenvalue&rsquo;s confidence interval. However,
</p>
<p>to display such a scree plot requires running a PCA with a different command
</p>
<p>in combination with a postestimation command. The following syntax
</p>
<p>produces a scree plot for our example with a 95% confidence interval
</p>
<p>(heteroskedastic), as well as a horizontal reference line at the 1 threshold.
</p>
<p>pca s1 s2 s3 s4 s5 s6 s7 s8, mineigen(1)
</p>
<p>screeplot, recast(line) ci(heteroskedastic) yline(1)
</p>
<p>Table 8.5 PCA output
</p>
<p>factor s1 s2 s3 s4 s5 s6 s7 s8, pcf mineigen(1)
</p>
<p>(obs=921)
</p>
<p>Factor analysis/correlation                      Number of obs    =        921
Method: principal-component factors          Retained factors =          2
Rotation: (unrotated)                        Number of params =         15
</p>
<p>--------------------------------------------------------------------------
Factor  |   Eigenvalue   Difference        Proportion   Cumulative
</p>
<p>-------------+------------------------------------------------------------
Factor1  |      5.24886      3.92053            0.6561       0.6561
Factor2  |      1.32834      0.93107            0.1660       0.8222
Factor3  |      0.39727      0.13406  0.0497       0.8718
Factor4  |      0.26321      0.03202            0.0329       0.9047
Factor5  |      0.23119      0.03484            0.0289       0.9336
Factor6  |      0.19634      0.00360            0.0245       0.9582
Factor7  |      0.19274      0.05067            0.0241       0.9822
Factor8  |      0.14206            .            0.0178       1.0000
</p>
<p>--------------------------------------------------------------------------
LR test: independent vs. saturated:  chi2(28) = 6428.36 Prob&gt;chi2 = 0.0000
</p>
<p>Factor loadings (pattern matrix) and unique variances
</p>
<p>-------------------------------------------------
Variable |  Factor1   Factor2 |   Uniqueness 
</p>
<p>-------------+--------------------+--------------
s1 |   0.7855    0.3995 |      0.2235  
s2 |   0.8017    0.4420 |      0.1619  
s3 |   0.7672    0.3206 |      0.3086  
s4 |   0.7505    0.5090 |      0.1777  
s5 |   0.8587   -0.3307 |      0.1533  
s6 |   0.8444   -0.4203 |      0.1104  
s7 |   0.7993   -0.4662 |      0.1439  
s8 |   0.8649   -0.3291 |      0.1436  
</p>
<p>-------------------------------------------------
</p>
<p>296 8 Principal Component and Factor Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>Fig. 8.12 Postestimation dialog box
</p>
<p>Table 8.6 The KMO statistic
</p>
<p>estat kmo
</p>
<p>Kaiser-Meyer-Olkin measure of sampling adequacy
</p>
<p>-----------------------
Variable |     kmo 
</p>
<p>-------------+---------
s1 |  0.9166 
s2 |  0.8839 
s3 |  0.9427 
s4 |  0.8834 
s5 |  0.9308 
s6 |  0.8831 
s7 |  0.9036 
s8 |  0.9180 
</p>
<p>-------------+---------
Overall |  0.9073 
</p>
<p>8.6 Example 297</p>
<p/>
</div>
<div class="page"><p/>
<p>While the Kaiser criterion and the scree plot are helpful for determining the
</p>
<p>number of factors to extract, parallel analysis is a more robust criterion. Parallel
</p>
<p>analysis can only be accessed through the free add-on package paran. To install the
</p>
<p>package, type in help paran in the command window and follow the instructions
</p>
<p>to install the package. Having installed the package, type in paran s1 s2 s3 s4
</p>
<p>s5 s6 s7 s8, centile(95) q all graph in the command window and Stata
</p>
<p>will produce output similar to Table 8.7 and Fig. 8.14.
</p>
<p>Table 8.7 contains two rows of eigenvalues, with the first column (Adjusted
</p>
<p>Eigenvalue) indicating the sampling error-adjusted eigenvalues obtained by paral-
</p>
<p>lel analysis. Note that your results will look slightly different, as parallel analysis
</p>
<p>draws on randomly generated datasets. The second column (Unadjusted Eigen-
</p>
<p>value) contains the eigenvalues as reported in the PCA output (Table 8.5). Analo-
</p>
<p>gous to the original analysis, the two factors exhibit adjusted eigenvalues larger
</p>
<p>than 1, indicating a two-factor solution. The scree plot in Fig. 8.14 also supports this
</p>
<p>result, as the first two factors exhibit adjusted eigenvalues larger than the randomly
</p>
<p>generated eigenvalues. Conversely, the random eigenvalue of the third factor is
</p>
<p>clearly larger than the adjusted one.
</p>
<p>Finally, we can also request the model selection statistics AIC and BIC for
</p>
<p>different numbers of factors (see Fig. 8.15). To do so, go to ► Statistics ►
</p>
<p>Postestimation ► AIC and BIC for different numbers of factors. As our analysis
</p>
<p>draws on eight variables, we restrict the number of factors to consider to 4 (Specify
</p>
<p>the maximum number of factors to include in summary table: 4). Table 8.8
</p>
<p>shows the results of our analysis. As can be seen, AIC has the smallest value
</p>
<p>(53.69378) for a four-factor solution, whereas BIC&rsquo;s minimum value occurs for a
</p>
<p>Fig. 8.13 Scree plot of eigenvalues
</p>
<p>298 8 Principal Component and Factor Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>two-factor solution (141.8393). However, as AIC is well known to overspecify the
</p>
<p>number of factors, this result gives confidence that the two-factor solution as indicated
</p>
<p>by the BIC is appropriate. Note that Table 8.8 says no Heywood cases encountered.
</p>
<p>Table 8.7 Parallel analysis output
</p>
<p>paran s1 s2 s3 s4 s5 s6 s7 s8, centile (95) q all graph
</p>
<p>Computing: 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%
</p>
<p>Results of Horn's Parallel Analysis for principal components
240 iterations, using the p95 estimate
</p>
<p>--------------------------------------------------
Component   Adjusted    Unadjusted    Estimated
or Factor   Eigenvalue  Eigenvalue    Bias
--------------------------------------------------
1          5.1571075   5.2488644     .09175694
2          1.2542191   1.3283371     .07411802
3          .3409911    .39726683     .05627573
4          .23833934   .26320842     .02486908
5          .22882924   .23118517     .00235593
6          .23964682   .19634077     -.04330605
7          .2775334    .19273589     -.0847975
8          .26333341   .14206139     -.12127203
--------------------------------------------------
Criterion: retain adjusted components &gt; 1
</p>
<p>Fig. 8.14 Scree plot of parallel analysis
</p>
<p>8.6 Example 299</p>
<p/>
</div>
<div class="page"><p/>
<p>Heywood cases indicate negative estimates of variances or correlation estimates
</p>
<p>greater than one in absolute value. In the Stata output, they are typically noted as
</p>
<p>Beware: solution is a Heywood case.
</p>
<p>8.6.1.3 Interpret the Factor Solution
To facilitate the interpretation of the factor solution, we need to rotate the factors.
</p>
<p>To initiate this analysis, go to ► Statistics ► Postestimation ► Factor analysis
</p>
<p>reports and graphs ► Rotate factor loadings. In the dialog box that opens, select
</p>
<p>Varimax (default) under Orthogonal rotation and click on Apply the Kaiser
</p>
<p>normalization, followed by OK. Table 8.9 shows the resulting output.
</p>
<p>Table 8.8 Factor rotation output
</p>
<p>estat factors, factors(4)
</p>
<p>Factor analysis with different numbers of factors (maximum likelihood)
</p>
<p>----------------------------------------------------------
#factors |     loglik   df_m   df_r        AIC        BIC 
---------+------------------------------------------------
</p>
<p>1 |  -771.1381      8     20   1558.276    1596.88 
2 |  -19.72869     15     13   69.45738   141.8393 
3 |  -10.09471     21      7   62.18943   163.5241 
4 |  -.8468887     26      2   53.69378   179.1557 
</p>
<p>----------------------------------------------------------
no Heywood cases encountered
</p>
<p>Fig. 8.15 AIC and BIC
</p>
<p>300 8 Principal Component and Factor Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>The upper part of Table 8.9 is the same as the standard PCA output (Table 8.5),
</p>
<p>showing that the analysis draws on 921 observations and extracts two factors, which
</p>
<p>jointly capture 82.22% of the variance. As its name implies, the Rotated factor
</p>
<p>loadings block shows the factor loadings after rotation. Recall that rotation is
</p>
<p>carried out to facilitate the interpretation of the factor solution. To interpret the
</p>
<p>factors, we first &ldquo;assign&rdquo; each variable to a certain factor based on its maximum
</p>
<p>absolute factor loading. That is, if the highest absolute loading is negative, higher
</p>
<p>values of a particular variable relate negatively to the assigned factor. After that, we
</p>
<p>should find an umbrella term for each factor that best describes the set of variables
</p>
<p>associated with that factor. Looking at Table 8.9, we see that s1&ndash;s4 load highly on
the second factor, whereas s5-s8 load on the first factor. For example, s1 has a
0.2989 loading on the first factor, while its loading is much stronger on the second
</p>
<p>factor (0.8290). Finally, note that the uniqueness and, hence, the communality
</p>
<p>values are unaffected by the rotation (see Table 8.5).
</p>
<p>Table 8.9 Factor rotation output
</p>
<p>rotate, kaiser
</p>
<p>Factor analysis/correlation                      Number of obs    =        921
Method: principal-component factors          Retained factors =          2
Rotation: orthogonal varimax (Kaiser on)     Number of params =         15
</p>
<p>--------------------------------------------------------------------------
Factor  |     Variance   Difference        Proportion   Cumulative
</p>
<p>-------------+------------------------------------------------------------
Factor1  |      3.41063      0.24405            0.4263       0.4263
Factor2  |      3.16657            .            0.3958       0.8222
</p>
<p>--------------------------------------------------------------------------
LR test: independent vs. saturated:  chi2(28) = 6428.36 Prob&gt;chi2 = 0.0000
</p>
<p>Rotated factor loadings (pattern matrix) and unique variances
</p>
<p>-------------------------------------------------
Variable |  Factor1   Factor2 |   Uniqueness 
</p>
<p>-------------+--------------------+--------------
s1 |   0.2989    0.8290 |      0.2235  
s2 |   0.2817    0.8711 |      0.1619  
s3 |   0.3396    0.7590 |      0.3086  
s4 |   0.1984    0.8848 |      0.1777  
s5 |   0.8522    0.3470 |      0.1533  
s6 |   0.9031    0.2719 |      0.1104  
s7 |   0.9017    0.2076 |      0.1439  
s8 |   0.8557    0.3524 | 0.1436  
</p>
<p>-------------------------------------------------
</p>
<p>Factor rotation matrix
</p>
<p>--------------------------------
| Factor1  Factor2 
</p>
<p>-------------+------------------
Factor1 |  0.7288   0.6847 
Factor2 | -0.6847   0.7288 
</p>
<p>--------------------------------
</p>
<p>8.6 Example 301</p>
<p/>
</div>
<div class="page"><p/>
<p>To facilitate the identification of labels, we can plot each item&rsquo;s loading against
</p>
<p>each factor. To request a factor loadings plot, go to► Statistics► Postestimation►
</p>
<p>Factor analysis reports and graphs ► Plot of factor loadings and click on Launch.
</p>
<p>In the dialog box that follows, retain the default settings and click on OK. The
</p>
<p>resulting plot (Fig. 8.16) shows two cluster of variables, which strongly load on one
</p>
<p>factor while having low loadings on the other factor. This result supports our
</p>
<p>previous conclusion in terms of the variable assignments.
</p>
<p>Having identified which variables load highly on which factor in the rotated
</p>
<p>solution, we now need to identify labels for each factor. Looking at the variable
</p>
<p>labels, we learn that the first set of variables (s1&ndash;s4) relate to reliability aspects of
the journey and related processes, such as the booking. We could therefore label
</p>
<p>this factor (i.e., factor 2) reliability. The second set of variables (s5&ndash;s8) relate to
different aspects of the onboard facilities and the travel experience. Hence, we
</p>
<p>could label this factor (i.e., factor 1) onboard experience. The labeling of factors is
of course subjective and you could provide different descriptions.
</p>
<p>8.6.1.4 Evaluate the Goodness-of-fit of the Factor Solution
The last step involves assessing the analysis&rsquo;s goodness-of-fit. To do so, we first
</p>
<p>look at the residuals (i.e., the differences between observed and reproduced
</p>
<p>correlations) in the reproduced correlation matrix. To create this matrix, go to ►
</p>
<p>Statistics ► Postestimation ► Factor analysis reports and graphs ► Matrix of
</p>
<p>correlation residuals. In the dialog box that opens, select Matrix of correlation
</p>
<p>of covariance residuals (residuals) and click on OK. Table 8.10 shows the
</p>
<p>resulting output.
</p>
<p>Fig. 8.16 Factor loadings plot
</p>
<p>302 8 Principal Component and Factor Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>When examining the lower part of Table 8.10, we see that there are several
</p>
<p>residuals with absolute values larger than 0.05. A quick count reveals that 8 out of
</p>
<p>29 (i.e., 27.59%) residuals are larger than 0.05. As the percentage of increased
</p>
<p>residuals is well below 50%, we can presume a good model fit.
</p>
<p>Similarly, our previous analysis showed that the two factors reproduce a suffi-
</p>
<p>cient amount of each variable&rsquo;s variance. The uniqueness values are clearly below
</p>
<p>0.50 (i.e., the communalities are larger than 0.50), indicating that the factors
</p>
<p>account for more than 50% of the variables&rsquo; variance (Table 8.5).
</p>
<p>8.6.1.5 Compute the Factor Scores
The evaluation of the factor solution&rsquo;s goodness-of-fit completes the standard PCA
</p>
<p>analysis. However, if we wish to continue using the results for further analyses, we
</p>
<p>should calculate the factor scores. Go to ► Statistics ► Postestimation ►
</p>
<p>Predictions ► Regression and Bartlett scores and Stata will open a dialog box
</p>
<p>similar to Fig. 8.17. Under New variable names or variable stub* you can enter a
</p>
<p>prefix name, which Stata uses to name the saved factor scores. For example,
</p>
<p>specifying factor*, as shown in Fig. 8.17, will create two variables called factor1
and factor2. Next, select Factors scored by the regression scoring method and
click on OK. Stata will produce an output table showing the scoring coefficients,
</p>
<p>which are the weights used to compute the factor scores from the standardized data.
</p>
<p>However, we are more interested in the actual factor scores, which we can access by
</p>
<p>clicking on the Data Browser button in Stata&rsquo;s menu bar. Figure 8.18 shows the
</p>
<p>scores of factor1 and factor2 for the first ten observations.
Being z-standardized, the newly generated variables factor1 and factor2 have
</p>
<p>mean values (approximately) equal to zero and standard deviations equal to 1. Thus,
</p>
<p>the factor scores are estimated in units of standard deviations from their means. For
</p>
<p>example, the first observation is about 1.91 standard deviations below average on
</p>
<p>the onboard experience factor (i.e., factor 1) and about 0.91 standard deviations
above average on the reliability factor (i.e., factor 2). In contrast, the second
observation is clearly above average in terms of reliability and onboard experience.
Note that if the original variables include a missing value, the factor score will also
</p>
<p>be missing (i.e., only a &ldquo;.&rdquo; (dot) will be recorded).
</p>
<p>Table 8.10 Correlation residual matrix
</p>
<p>estat residuals
</p>
<p>Raw residuals of correlations (observed-fitted)
</p>
<p>--------------------------------------------------------------------------------------
Variable |      s1       s2       s3       s4       s5       s6       s7       s8 
</p>
<p>-------------+------------------------------------------------------------------------
s1 |  0.0000                                                
s2 | -0.0525   0.0000                                                       
s3 | -0.1089  -0.0602   0.0000                                              
s4 | -0.0598  -0.0557  -0.0907   0.0000    
s5 | -0.0174  -0.0063  -0.0029   0.0076   0.0000                            
s6 |  0.0046   0.0023  -0.0216   0.0159  -0.0464   0.0000                   
s7 |  0.0136   0.0182  -0.0118   0.0042  -0.0529  -0.0393   0.0000          
s8 | -0.0056  -0.0135  -0.0056   0.0052  -0.0404  -0.0265  -0.0645   0.0000 
</p>
<p>--------------------------------------------------------------------------------------
</p>
<p>8.6 Example 303</p>
<p/>
</div>
<div class="page"><p/>
<p>8.6.2 Reliability Analysis
</p>
<p>To illustrate its usage, let&rsquo;s carry out a reliability analysis of the first factor onboard
experience by calculating Cronbach&rsquo;s Alpha as a function of variables s5 to s8. To
run the reliability analysis, click on ► Statistics ► Multivariate analysis ►
</p>
<p>Cronbach&rsquo;s Alpha. A window similar to Fig. 8.19 will appear. Next, enter variables
</p>
<p>s5&ndash;s8 into the Variables box.
</p>
<p>Fig. 8.17 Saving factor scores as new variables
</p>
<p>Fig. 8.18 Overview of factor
</p>
<p>scores for the first ten
</p>
<p>observations
</p>
<p>304 8 Principal Component and Factor Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>TheOptions tab (Fig. 8.20) provides options for dealing with missing values and
</p>
<p>requesting descriptive statistics for each item and the entire scale or item
</p>
<p>correlations. Check Display item-test and item-rest correlations and click on
</p>
<p>OK.
</p>
<p>The results in Table 8.11 show that the scale exhibits a high degree of internal
</p>
<p>consistency reliability. With a value of 0.9439 (see row Test scale), the Cronbach&rsquo;s
</p>
<p>Alpha coefficient lies well above the commonly suggested threshold of 0.70. This
</p>
<p>result is not surprising, since we are simply testing a scale previously established by
</p>
<p>means of item correlations. Keep in mind that we usually carry out a reliability
</p>
<p>analysis to test a scale using a different sample&mdash;this example is only for illustration
</p>
<p>purposes! The rightmost column of Table 8.11 indicates what the Cronbach&rsquo;s Alpha
</p>
<p>would be if we deleted the item indicated in that row. When we compare each of the
</p>
<p>values with the overall Cronbach&rsquo;s Alpha value, we can see that any change in the
</p>
<p>scale&rsquo;s set-up would reduce the Cronbach&rsquo;s Alpha value. For example, by removing
</p>
<p>s5 from the scale, the Cronbach&rsquo;s Alpha of the new scale comprising only s6, s7,
and s8 would be reduced to 0.9284. Therefore, deleting this item (or any others)
makes little sense. In the leftmost column of Table 8.11, Stata indicates the number
</p>
<p>of observations (Obs), as well as whether that particular item correlates positively
</p>
<p>or negatively with the sum of the other items (Sign). This information is useful for
</p>
<p>determining whether reverse-coded items were also identified as such. Reverse-
</p>
<p>coded items should have a minus sign. The columns item-test, item-rest, and
</p>
<p>average interitem covariance are not needed for a basic interpretation.
</p>
<p>Fig. 8.19 Reliability analysis dialog box
</p>
<p>8.6 Example 305</p>
<p/>
</div>
<div class="page"><p/>
<p>8.7 Customer Satisfaction at Haver and Boecker (Case Study)
</p>
<p>Haver and Boecker (http://www.haverboecker.com) is one of the world&rsquo;s leading
</p>
<p>and most renowned machine producers in the fields of mineral processing, as well
</p>
<p>as the storing, conveying, packing, and loading of bulk material. The family-owned
</p>
<p>group operates through its global network of facilities, with manufacturing units,
</p>
<p>among others, in Germany, the UK, Belgium, US, Canada, Brazil, China, and India.
</p>
<p>Fig. 8.20 Options tab for Cronbach&rsquo;s alpha
</p>
<p>Table 8.11 Reliability statistics
</p>
<p>alpha s5 s6 s7 s8, item
</p>
<p>Test scale = mean(unstandardized items)
</p>
<p>average
item-test     item-rest       interitem
</p>
<p>Item         |  Obs  Sign   correlation   correlation     covariance      alpha
-------------+-----------------------------------------------------------------
s5           | 1041    +   0.9211        0.8578        421.5723      0.9284
s6           | 1041    +       0.9422        0.8964        412.3225      0.9168
s7           | 1048    +       0.9222        0.8506        399.2565      0.9330
s8           | 1034    +       0.9203      0.8620        434.3355      0.9282
-------------+-----------------------------------------------------------------
Test scale   |                                             416.8996      0.9439
-------------------------------------------------------------------------------
</p>
<p>306 8 Principal Component and Factor Analysis</p>
<p/>
<div class="annotation"><a href="http://www.haverboecker.com">http://www.haverboecker.com</a></div>
</div>
<div class="page"><p/>
<p>The company&rsquo;s relationships with its customers are usually long-term oriented
</p>
<p>and complex. Since the company&rsquo;s philosophy is to help customers and business
</p>
<p>partners solve their challenges or problems, they often customize their products and
</p>
<p>services to meet the buyers&rsquo; needs. Therefore, the customer is no longer a passive
</p>
<p>buyer, but an active partner. Given this background, the customers&rsquo; satisfaction
</p>
<p>plays an important role in establishing, developing, and maintaining successful
</p>
<p>customer relationships.
</p>
<p>Very early on, the company&rsquo;s management realized the importance of customer
</p>
<p>satisfaction and decided to commission a market research project in order to
</p>
<p>identify marketing activities that can positively contribute to the business&rsquo;s overall
</p>
<p>success. Based on a thorough literature review, as well as interviews with experts,
</p>
<p>the company developed a short survey to explore their customers&rsquo; satisfaction with
</p>
<p>specific performance features and their overall satisfaction. All the items were
</p>
<p>measured on 7-point scales, with higher scores denoting higher levels of satisfac-
</p>
<p>tion. A standardized survey was mailed to customers in 12 countries worldwide,
</p>
<p>which yielded 281 fully completed questionnaires. The following items (names in
</p>
<p>parentheses) were listed in the survey:
</p>
<p>&ndash; Reliability of the machines and systems. (s1)
&ndash; Life-time of the machines and systems. (s2)
&ndash; Functionality and user-friendliness operation of the machines and systems. (s3)
&ndash; Appearance of the machines and systems. (s4)
&ndash; Accuracy of the machines and systems. (s5)
</p>
<p>8.7 Customer Satisfaction at Haver and Boecker (Case Study) 307</p>
<p/>
</div>
<div class="page"><p/>
<p>&ndash; Timely availability of the after-sales service. (s6)
&ndash; Local availability of the after-sales service. (s7)
&ndash; Fast processing of complaints. (s8)
&ndash; Composition of quotations. (s9)
&ndash; Transparency of quotations. (s10)
&ndash; Fixed product prize for the machines and systems. (s11)
&ndash; Cost/performance ratio of the machines and systems. (s12)
&ndash; Overall, how satisfied are you with the supplier (overall)?
</p>
<p>Your task is to analyze the dataset to provide the management of Haver and
</p>
<p>Boecker with advice for effective customer satisfaction management. The dataset is
</p>
<p>labeled haver_and_boecker.dta ( Web Appendix ! Downloads).
</p>
<p>1. Determine the factors that characterize the respondents by means a factor
</p>
<p>analysis. Use items s1&ndash;s12 for this. Run a PCA with varimax rotation to facilitate
interpretation. Consider the following aspects:
</p>
<p>(a) Are all assumptions for carrying out a PCAmet? Pay special attention to the
</p>
<p>question whether the data are sufficiently correlated.
</p>
<p>(b) How many factors would you extract? Base your decision on the Kaiser
</p>
<p>criterion, the scree plot, parallel analysis, and the model selection statistics
</p>
<p>AIC and BIC.
</p>
<p>(c) Find suitable labels for the extracted factors.
</p>
<p>(d) Evaluate the factor solution&rsquo;s goodness-of-fit.
</p>
<p>2. Use the factor scores and regress the customers&rsquo; overall satisfaction (overall) on
these. Evaluate the strength of the model and compare it with the initial regres-
</p>
<p>sion. What should Haver and Boecker&rsquo;s management do to increase their
</p>
<p>customers&rsquo; satisfaction?
</p>
<p>3. Calculate the Cronbach&rsquo;s Alpha over items s1&ndash;s5 and interpret the results.
</p>
<p>For further information on the dataset and the study, see Festge and Schwaiger
</p>
<p>(2007), as well as Sarstedt et al. (2009).
</p>
<p>8.8 Review Questions
</p>
<p>1. What is factor analysis? Try to explain what factor analysis is in your own
</p>
<p>words.
</p>
<p>2. What is the difference between exploratory factor analysis and confirmatory
</p>
<p>factor analysis?
</p>
<p>3. What is the difference between PCA and factor analysis?
</p>
<p>4. Describe the terms communality, eigenvalue, factor loading, and uniqueness.
</p>
<p>How do these concepts relate to one another?
</p>
<p>5. Describe three approaches used to determine the number of factors.
</p>
<p>6. What are the purpose and the characteristic of a varimax rotation? Does a
</p>
<p>rotation alter eigenvalues or factor loadings?
</p>
<p>308 8 Principal Component and Factor Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>7. Re-run the Oddjob Airways case study by carrying out a factor analysis and
</p>
<p>compare the results with the example carried out using PCA. Are there any
</p>
<p>significant differences?
</p>
<p>8. What is reliability analysis and why is it important?
</p>
<p>9. Explain the basic principle of structural equation modeling.
</p>
<p>8.9 Further Readings
</p>
<p>Nunnally, J. C., &amp; Bernstein, I. H. (1993). Psychometric theory (3rd ed.). New York:
McGraw-Hill.
</p>
<p>Psychometric theory is a classic text and the most comprehensive introduction to
the fundamental principles of measurement. Chapter 7 provides an in-depth
discussion of the nature of reliability and its assessment.
</p>
<p>Sarstedt, M., Hair, J. F., Ringle, C. M., Thiele, K. O., &amp; Gudergan, S. P. (2016).
</p>
<p>Estimation issues with PLS and CBSEM: where the bias lies! Journal of
Business Research, 69(10), 3998&ndash;4010.
</p>
<p>This paper discusses the differences between covariance-based and partial least
squares structural equation modeling from a measurement perspective. This
discussion relates to the differentiation between factor analysis and PCA and
the assumptions underlying each approach to measure unobservable
phenomena.
</p>
<p>Stewart, D. W., (1981). The application and misapplication of factor analysis in
</p>
<p>marketing research. Journal of Marketing Research, 18(1), 51&ndash;62.
David Stewart discusses procedures for determining when data are appropriate for
</p>
<p>factor analysis, as well as guidelines for determining the number of factors to
extract, and for rotation.
</p>
<p>References
</p>
<p>Acock, A. C. (2013). Discovering structural equation modeling using Stata (Revised ed.). College
Station: Stata Press.
</p>
<p>Brown, J. D. (2009). Choosing the right type of rotation in PCA and EFA. JALT Testing &amp;
Evaluation SIG Newsletter, 13(3), 20&ndash;25.
</p>
<p>Carbonell, L., Izquierdo, L., Carbonell, I., &amp; Costell, E. (2008). Segmentation of food consumers
</p>
<p>according to their correlations with sensory attributes projected on preference spaces. Food
Quality and Preference, 19(1), 71&ndash;78.
</p>
<p>Cattell, R. B. (1966). The scree test for the number of factors. Multivariate Behavioral Research,
1(2), 245&ndash;276.
</p>
<p>Cliff, N. (1987). Analyzing multivariate data. New York: Harcourt Brace Jovanovich.
Cronbach, L. J. (1951). Coefficient alpha and the internal structure of tests. Psychometrika, 16(3),
</p>
<p>297&ndash;334.
</p>
<p>Diamantopoulos, A., &amp; Siguaw, J. A. (2000). Introducing LISREL: A guide for the uninitiated.
London: Sage.
</p>
<p>Dinno, A. (2009). Exploring the sensitivity of Horn&rsquo;s parallel analysis to the distributional form of
</p>
<p>random data. Multivariate Behavioral Research, 44(3), 362&ndash;388.
</p>
<p>References 309</p>
<p/>
</div>
<div class="page"><p/>
<p>DiStefano, C., Zhu, M., &amp; Mı̂ndriă, D. (2009). Understanding and using factor scores:
</p>
<p>Considerations fort he applied researcher. Practical Assessment, Research &amp; Evaluation,
14(20), 1&ndash;11.
</p>
<p>Festge, F., &amp; Schwaiger, M. (2007). The drivers of customer satisfaction with industrial goods: An
</p>
<p>international study. Advances in International Marketing, 18, 179&ndash;207.
Gorsuch, R. L. (1983). Factor analysis (2nd ed.). Hillsdale: Lawrence Erlbaum Associates.
Graffelman, J. (2013). Linear-angle correlation plots: New graphs for revealing correlation
</p>
<p>structure. Journal of Computational and Graphical Statistics, 22(1), 92&ndash;106.
Grice, J. W. (2001). Computing and evaluating factor scores. Psychological Methods, 6(4),
</p>
<p>430&ndash;450.
</p>
<p>Hair, J. F., Black, W. C., Babin, B. J., &amp; Anderson, R. E. (2013). Multivariate data analysis. A
global perspective (7th ed.). Upper Saddle River: Pearson Prentice Hall.
</p>
<p>Hair, J. F., Ringle, C. M., &amp; Sarstedt, M. (2011). PLS-SEM: Indeed a silver bullet. Journal of
Marketing Theory and Practice, 19(2), 139&ndash;151.
</p>
<p>Hair, J. F., Hult, G. T. M., Ringle, C. M., &amp; Sarstedt, M. (2017a). A primer on partial least squares
structural equation modeling (PLS-SEM) (2nd ed.). Thousand Oaks: Sage.
</p>
<p>Hair, J. F., Hult, G. T. M., Ringle, C. M., Sarstedt, M., &amp; Thiele, K. O. (2017b). Mirror, mirror on
</p>
<p>the wall. A comparative evaluation of composite-based structural equation modeling methods.
</p>
<p>Journal of the Academy of Marketing Science, 45(5), 616&ndash;632.
Hair, J. F., Sarstedt, M., Ringle, C. M., &amp; Gudergan, S. P. (2018). Advanced issues in partial least
</p>
<p>squares structural equation modeling (PLS-SEM). Thousand Oaks: Sage.
Hamilton, L. C. (2013), Statistics with Stata: Version 12: Cengage Learning.
</p>
<p>Hayton, J. C., Allen, D. G., &amp; Scarpello, V. (2004). Factor retention decisions in exploratory factor
</p>
<p>analysis: A tutorial on parallel analysis. Organizational Research Methods, 7(2), 191&ndash;205.
Henson, R. K., &amp; Roberts, J. K. (2006). Use of exploratory factor analysis in published research:
</p>
<p>Common errors and some comment on improved practice. Educational and Psychological
Measurement, 66(3), 393&ndash;416.
</p>
<p>Hershberger, S. L. (2005). Factor scores. In B. S. Everitt &amp; D. C. Howell (Eds.), Encyclopedia of
statistics in behavioral science (pp. 636&ndash;644). New York: John Wiley.
</p>
<p>Horn, J. L. (1965). A rationale and test for the number of factors in factor analysis. Psychometrika,
30(2), 179&ndash;185.
</p>
<p>J&euro;oreskog, K. G. (1971). Simultaneous factor analysis in several populations. Psychometrika, 36(4),
409&ndash;426.
</p>
<p>Kaiser, H. F. (1958). The varimax criterion for factor analytic rotation in factor analysis. Educa-
tional and Psychological Measurement, 23(3), 770&ndash;773.
</p>
<p>Kaiser, H. F. (1974). An index of factorial simplicity. Psychometrika, 39(1), 31&ndash;36.
Kim, J. O., &amp; Mueller, C. W. (1978). Introduction to factor analysis: What it is and how to do it.
</p>
<p>Thousand Oaks: Sage.
</p>
<p>Longman, R. S., Cota, A. A., Holden, R. R., &amp; Fekken, G. C. (1989). A regression equation for the
</p>
<p>parallel analysis criterion in principal components analysis: Mean and 95th percentile
</p>
<p>eigenvalues. Multivariate Behavioral Research, 24(1), 59&ndash;69.
MacCallum, R. C., Widaman, K. F., Zhang, S., &amp; Hong, S. (1999). Sample size in factor analysis.
</p>
<p>Psychological Methods, 4(1), 84&ndash;99.
Matsunga, M. (2010). How to factor-analyze your data right: Do&rsquo;s and don&rsquo;ts and how to&rsquo;s.
</p>
<p>International Journal of Psychological Research, 3(1), 97&ndash;110.
Mulaik, S. A. (2009). Foundations of factor analysis (2nd ed.). London: Chapman &amp; Hall.
Preacher, K. J., &amp; MacCallum, R. C. (2003). Repairing Tom Swift&rsquo;s electric factor analysis
</p>
<p>machine. Understanding Statistics, 2(1), 13&ndash;43.
Russell, D. W. (2002). In search of underlying dimensions: The use (and abuse) of factor analysis
</p>
<p>in Personality and Social Psychology Bulletin. Personality and Social Psychology Bulletin,
28(12), 1629&ndash;1646.
</p>
<p>Sarstedt, M., Schwaiger, M., &amp; Ringle, C. M. (2009). Do we fully understand the critical success
</p>
<p>factors of customer satisfaction with industrial goods? Extending Festge and Schwaiger&rsquo;s
</p>
<p>310 8 Principal Component and Factor Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>model to account for unobserved heterogeneity. Journal of Business Market Management,
3(3), 185&ndash;206.
</p>
<p>Sarstedt, M., Ringle, C. M., Raithel, S., &amp; Gudergan, S. (2014). In pursuit of understanding what
</p>
<p>drives fan satisfaction. Journal of Leisure Research, 46(4), 419&ndash;447.
Sarstedt, M., Hair, J. F., Ringle, C. M., Thiele, K. O., &amp; Gudergan, S. P. (2016). Estimation issues
</p>
<p>with PLS and CBSEM: Where the bias lies! Journal of Business Research, 69(10), 3998&ndash;4010.
Steiger, J. H. (1979). Factor indeterminacy in the 1930&rsquo;s and the 1970&rsquo;s some interesting parallels.
</p>
<p>Psychometrika, 44(2), 157&ndash;167.
Stevens, J. P. (2009). Applied multivariate statistics for the social sciences (5th ed.). Hillsdale:
</p>
<p>Erlbaum.
</p>
<p>Velicer, W. F., &amp; Jackson, D. N. (1990). Component analysis versus common factor analysis:
</p>
<p>Some issues in selecting an appropriate procedure. Multivariate Behavioral Research, 25(1),
1&ndash;28.
</p>
<p>Vigneau, E., &amp; Qannari, E. M. (2002). Segmentation of consumers taking account of external data.
</p>
<p>A clustering of variables approach. Food Quality and Preference, 13(7&ndash;8), 515&ndash;521.
Widaman, K. F. (1993). Common factor analysis versus principal component analysis: Differential
</p>
<p>bias in representing model parameters? Multivariate Behavioral Research, 28(3), 263&ndash;311.
Wold, H. O. A. (1982). Soft modeling: The basic design and some extensions. In K. G. J&euro;oreskog &amp;
</p>
<p>H. O. A. Wold (Eds.), Systems under indirect observations: Part II (pp. 1&ndash;54). Amsterdam:
North-Holland.
</p>
<p>Zwick, W. R., &amp; Velicer, W. F. (1986). Comparison of five rules for determining the number of
</p>
<p>components to retain. Psychological Bulletin, 99(3), 432&ndash;442.
</p>
<p>References 311</p>
<p/>
</div>
<div class="page"><p/>
<p>Cluster Analysis 9
</p>
<p>Keywords
</p>
<p>Agglomerative clustering &bull; Average linkage &bull; Canberra distance &bull; Centroid
</p>
<p>linkage &bull; Chaining effect &bull; Chebychev distance &bull; City-block distance &bull;
</p>
<p>Clusters &bull; Clustering variables &bull; Complete linkage &bull; Dendrogram &bull; Distance
</p>
<p>matrix &bull; Divisive clustering &bull; Duda-Hart index &bull; Euclidean distance &bull; Factor-
</p>
<p>cluster segmentation &bull; Gower&rsquo;s dissimilarity coefficient &bull; Hierarchical clustering
</p>
<p>methods &bull; Partitioning methods &bull; k-means &bull; k-medians &bull; k-means++ &bull;
</p>
<p>k-medoids &bull; Label switching &bull; Linkage algorithm &bull; Local optimum &bull;
</p>
<p>Mahalanobis distance &bull; Manhattan metric &bull; Market segmentation &bull; Matching
</p>
<p>coefficients &bull; Non-hierarchical clustering methods &bull; Profiling &bull; Russel and Rao
</p>
<p>coefficient &bull; Single linkage &bull; Simple matching coefficient &bull; Straight line
</p>
<p>distance &bull; Ties &bull; Variance ration criterion &bull; Ward&rsquo;s linkage &bull; Weighted
</p>
<p>average linkage
</p>
<p>Learning Objectives
</p>
<p>After reading this chapter, you should understand:
</p>
<p>&ndash; The basic concepts of cluster analysis.
</p>
<p>&ndash; How basic cluster algorithms work.
</p>
<p>&ndash; How to compute simple clustering results manually.
</p>
<p>&ndash; The different types of clustering procedures.
</p>
<p>&ndash; The Stata clustering outputs.
</p>
<p># Springer Nature Singapore Pte Ltd. 2018
</p>
<p>E. Mooi et al., Market Research, Springer Texts in Business and Economics,
DOI 10.1007/978-981-10-5218-7_9
</p>
<p>313</p>
<p/>
</div>
<div class="page"><p/>
<p>9.1 Introduction
</p>
<p>Market segmentation is one of the most fundamental marketing activities. Since
</p>
<p>consumers, customers, and clients have different needs, companies have to divide
</p>
<p>markets into groups (segments) of consumers, customers, and clients with similar
</p>
<p>needs and wants. Firms can then target each of these segments by positioning
</p>
<p>themselves in a unique segment (e.g., Ferrari in the high-end sports car market).
</p>
<p>Market segmentation &ldquo;is essential for marketing success: the most successful firms
</p>
<p>drive their businesses based on segmentation&rdquo; (Lilien and Rangaswamy 2004, p. 61)
</p>
<p>and &ldquo;tools such as segmentation [. . .] have the largest impact on marketing
</p>
<p>decisions&rdquo; (John et al. 2014, p. 127). While market researchers often form market
</p>
<p>segments based on practical grounds, industry practice and wisdom, cluster analysis
</p>
<p>uses data to form segments, making segmentation less dependent on subjectivity.
</p>
<p>9.2 Understanding Cluster Analysis
</p>
<p>Cluster analysis is a method for segmentation and identifies homogenous groups of
</p>
<p>objects (or cases, observations) called clusters. These objects can be individual
</p>
<p>customers, groups of customers, companies, or entire countries. Objects in a certain
</p>
<p>cluster should be as similar as possible to each other, but as distinct as possible from
</p>
<p>objects in other clusters.
</p>
<p>Let&rsquo;s try to gain a basic understanding of cluster analysis by looking at a simple
</p>
<p>example. Imagine that you are interested in segmenting your customer base in order
</p>
<p>to better target them through, for example, pricing strategies.
</p>
<p>The first step is to decide on the characteristics that you will use to segment your
</p>
<p>customers A to G. In other words, you have to decide which clustering variables
</p>
<p>will be included in the analysis. For example, you may want to segment a market
</p>
<p>based on customers&rsquo; price consciousness (x) and brand loyalty (y). These two
variables can be measured on a scale from 0 to 100 with higher values denoting a
</p>
<p>higher degree of price consciousness and brand loyalty. Table 9.1 and the scatter
</p>
<p>plot in Fig. 9.1 show the values of seven customers (referred to as objects).
</p>
<p>The aim of cluster analysis is to identify groups of objects (in this case,
</p>
<p>customers) that are very similar regarding their price consciousness and brand
</p>
<p>loyalty, and assign them to clusters. After having decided on the clustering
</p>
<p>variables (here, price consciousness and brand loyalty), we need to decide on the
</p>
<p>clustering procedure to form our groups of objects. This step is crucial for the
</p>
<p>analysis, as different procedures require different decisions prior to analysis. There
</p>
<p>is an abundance of different approaches and little guidance on which one to use in
</p>
<p>practice. We will discuss the most popular approaches in market research,
</p>
<p>including:
</p>
<p>&ndash; hierarchical methods, and
</p>
<p>&ndash; partitioning methods (more precisely k-means)
</p>
<p>314 9 Cluster Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>While the basic aim of these procedures is the same, namely grouping similar
</p>
<p>objects into clusters, they take different routes, which we will discuss in this
</p>
<p>chapter. An important consideration before starting the grouping is to determine
</p>
<p>how similarity should be measured. Most methods calculate measures of (dis)
</p>
<p>similarity by estimating the distance between pairs of objects. Objects with
</p>
<p>smaller distances between one another are considered more similar, whereas
</p>
<p>objects with larger distances are considered more dissimilar. The decision on
</p>
<p>how many clusters should be derived from the data is a fundamental issue in the
</p>
<p>application of cluster analysis. This question is explored in the next step of the
</p>
<p>analysis. In most instances, we do not know the exact number of clusters and then
</p>
<p>we face a trade-off. On the one hand, we want as few clusters as possible to make
</p>
<p>the clusters easy to understand and actionable. On the other hand, having many
</p>
<p>clusters allows us to identify subtle differences between objects.
</p>
<p>Table 9.1 Data Customer A B C D E F G
</p>
<p>x 33 82 66 30 79 50 10
</p>
<p>y 95 94 80 67 60 33 17
</p>
<p>Fig. 9.1 Scatter plot
</p>
<p>9.2 Understanding Cluster Analysis 315</p>
<p/>
</div>
<div class="page"><p/>
<p>Megabus is a hugely successful bus line in the US. They completely rethought
</p>
<p>the nature of their customers and concentrated on three specific segments of
</p>
<p>the market: College kids, women travelling in groups, and active seniors. To
</p>
<p>meet these customer segments&rsquo; needs, Megabus reimagined the entire driving
</p>
<p>experience by developing double-decker buses with glass roofs and big
</p>
<p>windows, and equipped with fast WiFi. In light of the success of Megabus&rsquo;s
</p>
<p>segmenting and targeting efforts, practitioners even talk about the &ldquo;Megabus
</p>
<p>Effect&rdquo;&mdash;how one company has shaped an entire industry.
</p>
<p>In the final step, we need to interpret the clustering solution by defining and
</p>
<p>labeling the obtained clusters. We can do so by comparing the mean values of the
</p>
<p>clustering variables across the different clusters, or by identifying explanatory
</p>
<p>variables to profile the clusters. Ultimately, managers should be able to identify
</p>
<p>customers in each cluster on the basis of easily measurable variables. This final step
</p>
<p>also requires us to assess the clustering solution&rsquo;s stability and validity. Figure 9.2
</p>
<p>illustrates the steps associated with a cluster analysis; we will discuss these steps in
</p>
<p>more detail in the following sections.
</p>
<p>9.3 Conducting a Cluster Analysis
</p>
<p>9.3.1 Select the Clustering Variables
</p>
<p>At the beginning of the clustering process, we have to select appropriate variables
</p>
<p>for clustering. Even though this choice is critical, it is rarely treated as such. Instead,
</p>
<p>a mixture of intuition and data availability guide most analyses in marketing
</p>
<p>316 9 Cluster Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>practice. However, faulty assumptions may lead to improper market segmentation
</p>
<p>and, consequently, to deficient marketing strategies. Thus, great care should be
</p>
<p>taken when selecting the clustering variables! There are several types of clustering
</p>
<p>variables, as shown in Fig. 9.3. Sociodemographic variables define clusters based
</p>
<p>on people&rsquo;s demographic (e.g., age, ethnicity, and gender), geographic (e.g., resi-
</p>
<p>dence in terms of country, state, and city), and socioeconomic (e.g., education,
</p>
<p>income, and social class) characteristics. Psychometric variables capture unobserv-
</p>
<p>able character traits such as people&rsquo;s personalities or lifestyles. Finally, behavioral
</p>
<p>clustering variables typically consider different facets of consumer behavior, such
</p>
<p>as the way people purchase, use, and dispose of products. Other behavioral cluster-
</p>
<p>ing variables capture specific benefits which different groups of consumers look for
</p>
<p>in a product.
</p>
<p>The types of variables used for cluster analysis provide different solutions and,
</p>
<p>thereby, influence targeting strategies. Over the last decades, attention has shifted
</p>
<p>from more traditional sociodemographic clustering variables towards behavioral
</p>
<p>and psychometric variables. The latter generally provide better guidance for
</p>
<p>decisions on marketing instruments&rsquo; effective specification. Generally, clusters
</p>
<p>based on psychometric variables are more homogenous and these consumers
</p>
<p>respond more consistently to marketing actions (e.g., Wedel and Kamakura
</p>
<p>2000). However, consumers in these clusters are frequently hard to identify as
</p>
<p>such variables are not easily measured. Conversely, clusters determined by
</p>
<p>sociodemographic variables are easy to identify but are also more heterogeneous,
</p>
<p>which complicates targeting efforts. Consequently, researchers frequently combine
</p>
<p>Fig. 9.2 Steps in a cluster analysis
</p>
<p>9.3 Conducting a Cluster Analysis 317</p>
<p/>
<div class="annotation"><a href="http://www.businessdictionary.com/definition/benefit.html">http://www.businessdictionary.com/definition/benefit.html</a></div>
<div class="annotation"><a href="http://www.businessdictionary.com/definition/group.html">http://www.businessdictionary.com/definition/group.html</a></div>
<div class="annotation"><a href="http://www.businessdictionary.com/definition/consumer.html">http://www.businessdictionary.com/definition/consumer.html</a></div>
<div class="annotation"><a href="http://www.businessdictionary.com/definition/product.html">http://www.businessdictionary.com/definition/product.html</a></div>
</div>
<div class="page"><p/>
<p>different variables such as lifestyle characteristics and demographic variables,
</p>
<p>benefiting from each one&rsquo;s strengths.
</p>
<p>In some cases, the choice of clustering variables is apparent because of the task
</p>
<p>at hand. For example, a managerial problem regarding corporate communications
</p>
<p>will have a fairly well defined set of clustering variables, including contenders such
</p>
<p>as awareness, attitudes, perceptions, and media habits. However, this is not always
</p>
<p>the case and researchers have to choose from a set of candidate variables. But how
</p>
<p>do we make this decision? To facilitate the choice of clustering variables, we should
</p>
<p>consider the following guiding questions:
</p>
<p>&ndash; Do the variables differentiate sufficiently between the clusters?
</p>
<p>&ndash; Is the relation between the sample size and the number of clustering variables
</p>
<p>reasonable?
</p>
<p>&ndash; Are the clustering variables highly correlated?
</p>
<p>&ndash; Are the data underlying the clustering variables of high quality?
</p>
<p>Fig. 9.3 Types of clustering variables
</p>
<p>318 9 Cluster Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>Do the variables differentiate sufficiently between the clusters?
</p>
<p>It is important to select those clustering variables that provide a clear-cut
</p>
<p>differentiation between the objects.1 More precisely, criterion validity is of special
</p>
<p>interest; that is, the extent to which the &ldquo;independent&rdquo; clustering variables are
</p>
<p>associated with one or more criterion variables not included in the analysis. Such
</p>
<p>criterion variables generally relate to an aspect of behavior, such as purchase
</p>
<p>intention or willingness-to-pay. Given this relationship, there should be significant
</p>
<p>differences between the criterion variable(s) across the clusters (e.g., consumers in
</p>
<p>one cluster exhibit a significantly higher willingness-to-pay than those in other
</p>
<p>clusters). These associations may or may not be causal, but it is essential that the
</p>
<p>clustering variables distinguish significantly between the variable(s) of interest.
</p>
<p>Is the relation between the sample size and the number of clustering variables
reasonable?
</p>
<p>When choosing clustering variables, the sample size is a point of concern. First
</p>
<p>and foremost, this relates to issues of managerial relevance as the cluster sizes need
</p>
<p>to be substantial to ensure that the targeted marketing programs are profitable. From
</p>
<p>a statistical perspective, every additional variable requires an over-proportional
</p>
<p>increase in observations to ensure valid results. Unfortunately, there is no generally
</p>
<p>accepted guideline regarding minimum sample sizes or the relationship between the
</p>
<p>objects and the number of clustering variables used. While early research suggested
</p>
<p>a minimum sample size of two to the power of the number of clustering variables
</p>
<p>(Formann 1984), more recent rules-of-thumb are as follows:
</p>
<p>&ndash; In the simplest case where clusters are of equal size, Qiu and Joe (2009)
</p>
<p>recommend a sample size at least ten times the number of clustering variables
</p>
<p>multiplied by the number of clusters.
</p>
<p>&ndash; Dolnicar et al. (2014) recommend using a sample size of 70 times the number of
</p>
<p>clustering variables.
</p>
<p>&ndash; Dolnicar et al. (2016) find that increasing the sample size from 10 to 30 times the
</p>
<p>number of clustering variables substantially improves the clustering solution.
</p>
<p>This improvement levels off subsequently, but is still noticeable up to a sample
</p>
<p>size of approximately 100 times the number of clustering variables.
</p>
<p>These rules-of-thumb provide only rough guidance as the required sample size
</p>
<p>depends on many factors, such as the survey data characteristics (e.g., nonresponse,
</p>
<p>sampling error, response styles), relative cluster sizes, and the degree to which the
</p>
<p>clusters overlap (Dolnicar et al. 2016). However, these rules also jointly suggest
</p>
<p>that a minimum of 10 times the number of clustering variables should be considered
</p>
<p>the bare minimum. Keep in mind that no matter how many variables are used and
</p>
<p>1Tonks (2009) provides a discussion of segment design and the choice of clustering variables in
</p>
<p>consumer markets.
</p>
<p>9.3 Conducting a Cluster Analysis 319</p>
<p/>
</div>
<div class="page"><p/>
<p>no matter how small the sample size, cluster analysis will almost always provide a
</p>
<p>result. At the same time, however, the quality of results shows decreasing marginal
</p>
<p>returns as the sample size increases. Since cluster analysis is an exploratory
</p>
<p>technique whose results should be interpreted by taking practical considerations
</p>
<p>into account, it is not necessary to increase the sample size massively.
</p>
<p>Are the clustering variables highly correlated?
</p>
<p>If there is strong correlation between the variables, they are not sufficiently
</p>
<p>unique to identify distinct market segments. If highly correlated variables are used
</p>
<p>for cluster analysis, the specific aspects that these variables cover will be overrepre-
</p>
<p>sented in the clustering solution. In this regard, absolute correlations above 0.90 are
</p>
<p>always problematic. For example, if we were to add another variable called brand
preference to our analysis, it would almost cover the same aspect as brand loyalty.
The concept of being attached to a brand would therefore be overrepresented in the
</p>
<p>analysis, because the clustering procedure does not conceptually differentiate
</p>
<p>between the clustering variables. Researchers frequently handle such correlation
</p>
<p>problems by applying cluster analysis to the observations&rsquo; factor scores derived
</p>
<p>from a previously carried out principal component or factor analysis. However, this
</p>
<p>factor-cluster segmentation approach is subject to several limitations, which we
</p>
<p>discuss in Box 9.1.
</p>
<p>Box 9.1 Issues with Factor-Cluster Segmentation
</p>
<p>Dolnicar and Gr&uuml;n (2009) identify several problems of the factor-cluster
</p>
<p>segmentation approach (see Chap. 8 for a discussion of principal component
</p>
<p>and factor analysis and related terminology):
</p>
<p>1. The data are pre-processed and the clusters are identified on the basis of
</p>
<p>transformed values, not on the original information, which leads to differ-
</p>
<p>ent results.
</p>
<p>2. In factor analysis, the factor solution does not explain all the variance;
</p>
<p>information is thus discarded before the clusters have been identified or
</p>
<p>constructed.
</p>
<p>3. Eliminating variables with low loadings on all the extracted factors means
</p>
<p>that, potentially, the most important pieces of information for the identifi-
</p>
<p>cation of niche clusters are discarded, making it impossible to ever identify
</p>
<p>such groups.
</p>
<p>4. The interpretations of clusters based on the original variables become
</p>
<p>questionable, given that these clusters were constructed by using factor
</p>
<p>scores.
</p>
<p>(continued)
</p>
<p>320 9 Cluster Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>Box 9.1 (continued)
</p>
<p>Several studies have shown that the factor-cluster segmentation reduces
</p>
<p>the success of finding useable clusters significantly.2 Consequently, you
</p>
<p>should reduce the number of items in the questionnaire&rsquo;s pre-testing phase,
</p>
<p>retaining a reasonable number of relevant, non-overlapping questions that
</p>
<p>you believe differentiate the clusters well. However, if you have doubts about
</p>
<p>the data structure, factor-clustering segmentation may still be a better option
</p>
<p>than discarding items.
</p>
<p>Are the data underlying the clustering variables of high quality?
</p>
<p>Ultimately, the choice of clustering variables always depends on contextual
</p>
<p>influences, such as the data availability or the resources to acquire additional
</p>
<p>data. Market researchers often overlook that the choice of clustering variables is
</p>
<p>closely connected to data quality. Only those variables that ensure that high quality
</p>
<p>data can be used should be included in the analysis (Dolnicar and Lazarevski 2009).
</p>
<p>Following our discussions in Chaps. 3, 4 and 5, data are of high quality if the
</p>
<p>questions. . .
</p>
<p>&ndash; . . . have a strong theoretical basis,
</p>
<p>&ndash; . . . are not contaminated by respondent fatigue or response styles, and
</p>
<p>&ndash; . . . reflect the current market situation (i.e., they are recent).
</p>
<p>The requirements of other functions in the organization often play a major role in
</p>
<p>the choice of clustering variables. Consequently, you have to be aware that the
</p>
<p>choice of clustering variables should lead to segments acceptable to the different
</p>
<p>functions in the organization.
</p>
<p>9.3.2 Select the Clustering Procedure
</p>
<p>By choosing a specific clustering procedure, we determine how clusters should be
</p>
<p>formed. This forming of clusters always involves optimizing some kind of criterion,
</p>
<p>such as minimizing the within-cluster variance (i.e., the clustering variables&rsquo;
</p>
<p>overall variance of the objects in a specific cluster), or maximizing the distance
</p>
<p>between the clusters. The procedure could also address the question of how to
</p>
<p>determine the (dis)similarity between objects in a newly formed cluster and the
</p>
<p>remaining objects in the dataset.
</p>
<p>There are many different clustering procedures and also many ways of
</p>
<p>classifying these (e.g., overlapping versus non-overlapping, unimodal versus
</p>
<p>2See Arabie and Hubert (1994), Sheppard (1996), and Dolnicar and Gr&uuml;n (2009).
</p>
<p>9.3 Conducting a Cluster Analysis 321</p>
<p/>
</div>
<div class="page"><p/>
<p>multimodal, exhaustive versus non-exhaustive). Wedel and Kamakura (2000),
</p>
<p>Dolnicar (2003), and Kaufman and Rousseeuw (2005) offer reviews of clustering
</p>
<p>techniques. A practical distinction is the differentiation between hierarchical and
</p>
<p>partitioning methods (especially k-means), which we will discuss in the next
sections.
</p>
<p>9.3.2.1 Hierarchical Clustering Methods
</p>
<p>Understanding Hierarchical Clustering Methods
</p>
<p>Hierarchical clustering methods are characterized by the tree-like structure
</p>
<p>established in the course of the analysis. Most hierarchical methods fall into a
</p>
<p>category called agglomerative clustering. In this category, clusters are consecu-
</p>
<p>tively formed from objects. Agglomerative clustering starts with each object
</p>
<p>representing an individual cluster. The objects are then sequentially merged to
</p>
<p>form clusters of multiple objects, starting with the two most similar objects.
</p>
<p>Similarity is typically defined in terms of the distance between objects. That is,
</p>
<p>objects with smaller distances between one another are considered more similar,
</p>
<p>whereas objects with larger distances are considered more dissimilar. After the
</p>
<p>merger of the first two most similar (i.e., closest) objects, the agglomerative
</p>
<p>clustering procedure continues by merging another pair of objects or adding another
</p>
<p>object to an already existing cluster. This procedure continues until all the objects
</p>
<p>have been merged into one big cluster. As such, agglomerative clustering
</p>
<p>establishes a hierarchy of objects from the bottom (where each object represents
</p>
<p>a distinct cluster) to the top (where all objects form one big cluster). The left-hand
</p>
<p>side of Fig. 9.4 shows how agglomerative clustering merges objects (represented by
</p>
<p>circles) step-by-step with other objects or clusters (represented by ovals).
</p>
<p>Hierarchical clustering can also be interpreted as a top-down process, where all
</p>
<p>objects are initially merged into a single cluster, which the algorithm then gradually
</p>
<p>splits up. This approach to hierarchical clustering is called divisive clustering. The
</p>
<p>right-hand side of Fig. 9.4 illustrates the divisive clustering concept. As we can see,
</p>
<p>in both agglomerative and divisive clustering, a cluster on a higher level of the
</p>
<p>hierarchy always encompasses all clusters from a lower level. This means that if an
</p>
<p>object is assigned to a certain cluster, there is no possibility of reassigning this
</p>
<p>object to another cluster (hence, hierarchical clustering). This is an important
</p>
<p>distinction between hierarchical and partitioning methods, such as k-means,
which we will explore later in this chapter.
</p>
<p>Divisive procedures are rarely used in market research and not implemented in
</p>
<p>statistical software programs such as Stata as they are computationally very inten-
</p>
<p>sive for all but small datasets.3 We therefore focus on (agglomerative) hierarchical
</p>
<p>clustering.
</p>
<p>3Whereas agglomerative methods have the large task of checking N�(N�1)/2 possible first
combinations of observations (note that N represents the number of observations in the dataset),
divisive methods have the almost impossible task of checking 2(N�1)�1 combinations.
</p>
<p>322 9 Cluster Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>Linkage algorithms
</p>
<p>When using agglomerative hierarchical clustering, you need to specify a linkage
</p>
<p>algorithm. Linkage algorithms define the distance from a newly formed cluster to a
</p>
<p>certain object, or to other clusters in the solution. The most popular linkage
</p>
<p>algorithms include the following:
</p>
<p>&ndash; Single linkage (nearest neighbor): The distance between two clusters
</p>
<p>corresponds to the shortest distance between any two members in the two
</p>
<p>clusters.
</p>
<p>&ndash; Complete linkage (furthest neighbor): The oppositional approach to single
</p>
<p>linkage assumes that the distance between two clusters is based on the longest
</p>
<p>distance between any two members in the two clusters.
</p>
<p>&ndash; Average linkage: The distance between two clusters is defined as the average
</p>
<p>distance between all pairs of the two clusters&rsquo; members. Weighted average
</p>
<p>linkage performs the same calculation, but weights distances based on the
</p>
<p>number of objects in the cluster. Thus, the latter method is preferred when
</p>
<p>clusters are not of approximately equal size.
</p>
<p>&ndash; Centroid linkage: In this approach, the geometric center (centroid) of each
</p>
<p>cluster is computed first. This is done by computing the clustering variables&rsquo;
</p>
<p>average values of all the objects in a certain cluster. The distance between the
</p>
<p>two clusters equals the distance between the two centroids.
</p>
<p>&ndash; Ward&rsquo;s linkage: This approach differs from the previous ones in that it does not
</p>
<p>combine the two closest or most similar objects successively. Instead, Ward&rsquo;s
</p>
<p>linkage combines those objects whose merger increases the overall within-
</p>
<p>Fig. 9.4 Agglomerative and divisive clustering
</p>
<p>9.3 Conducting a Cluster Analysis 323</p>
<p/>
</div>
<div class="page"><p/>
<p>cluster variance (i.e., the homogeneity of clusters) to the smallest possible
</p>
<p>degree. The approach is generally used in combination with (squared)
</p>
<p>Euclidean distances, but can be used in combination with any other
</p>
<p>(dis)similarity measure.
</p>
<p>Figures 9.5, 9.6, 9.7, 9.8 and 9.9 illustrate these linkage algorithms for two clusters,
</p>
<p>which are represented by white circles surrounding a set of objects. Each of these
</p>
<p>linkage algorithms can yield totally different results when used on the same dataset,
</p>
<p>as each has specific properties:
</p>
<p>&ndash; The single linkage algorithm is based on minimum distances; it tends to form
</p>
<p>one large cluster with the other clusters containing only one or a few objects
</p>
<p>each. We can make use of this chaining effect to detect outliers, as these will be
</p>
<p>merged with the remaining objects&mdash;usually at very large distances&mdash;in the last
</p>
<p>steps of the analysis. Single linkage is considered the most versatile algorithm.
</p>
<p>&ndash; The complete linkage method is strongly affected by outliers, as it is based on
</p>
<p>maximum distances. Clusters produced by this method are likely to be compact
</p>
<p>and tightly clustered.
</p>
<p>&ndash; The average linkage and centroid linkage algorithms tend to produce clusters
</p>
<p>with low within-cluster variance and with similar sizes. The average linkage is
</p>
<p>affected by outliers, but less than the complete linkage method.
</p>
<p>&ndash; Ward&rsquo;s linkage yields clusters of similar size with a similar degree of tightness.
</p>
<p>Prior research has shown that the approach generally performs very well.
</p>
<p>However, outliers and highly correlated variables have a strong bearing on the
</p>
<p>algorithm.
</p>
<p>To better understand how the linkage algorithms work, let&rsquo;s manually examine
</p>
<p>some calculation steps using single linkage as an example. Let&rsquo;s start by looking at
</p>
<p>the distance matrix in Table 9.2, which shows the distances between objects A-G
</p>
<p>from our initial example. In this distance matrix, the non-diagonal elements express
</p>
<p>the distances between pairs of objects based on the Euclidean distance&mdash;we will
</p>
<p>discuss this distance measure in the following section. The diagonal elements of the
</p>
<p>matrix represent the distance from each object to itself, which is, of course, 0. In our
</p>
<p>example, the distance matrix is an 8 � 8 table with the lines and rows representing
the objects under consideration (see Table 9.1). As the distance between objects B
</p>
<p>and C (in this case, 21.260 units) is the same as between C and B, the distance
</p>
<p>matrix is symmetrical. Furthermore, since the distance between an object and itself
</p>
<p>is 0, you only need to look at either the lower or upper non-diagonal elements.
</p>
<p>In the very first step, the two objects exhibiting the smallest distance in the
</p>
<p>matrix are merged. Since the smallest distance occurs between B and C (d(B,C) &frac14;
21.260; printed in bold in Table 9.2), we merge these two objects in the first step of
</p>
<p>the analysis.
</p>
<p>324 9 Cluster Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>Fig. 9.5 Single linkage
</p>
<p>Fig. 9.6 Complete linkage
</p>
<p>9.3 Conducting a Cluster Analysis 325</p>
<p/>
</div>
<div class="page"><p/>
<p>Fig. 9.7 Average linkage
</p>
<p>Fig. 9.8 Centroid linkage
</p>
<p>326 9 Cluster Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>Agglomerative clustering procedures always merge those objects with the
</p>
<p>smallest distance, regardless of the linkage algorithm used (e.g., single or
</p>
<p>complete linkage).
</p>
<p>In the next step, we form a new distance matrix by considering the single linkage
</p>
<p>decision rule as discussed above. Using this linkage algorithm, we need to compute
</p>
<p>the distance from the newly formed cluster [B,C] (clusters are indicated by squared
</p>
<p>brackets) to all the other objects. For example, with regard to the distance from the
</p>
<p>cluster [B,C] to object A, we need to check whether A is closer to object B or to
</p>
<p>object C. That is, we look for the minimum value in d(A,B) and d(A,C) from
</p>
<p>Table 9.2. As d(A,C) &frac14; 36.249 is smaller than d(A,B) &frac14; 49.010, the distance from
A to the newly formed cluster is equal to d(A,C); that is, 36.249. We also compute
</p>
<p>the distances from cluster [B,C] to all the other objects (i.e., D, E, F, G). For
</p>
<p>example, the distance between [B,C] and D is the minimum of d(B,D)&frac14; 58.592 and
d(C,D) &frac14; 38.275 (Table 9.2). Finally, there are several distances, such as d(D,E)
and d(E,F), which are not affected by the merger of B and C. These distances are
</p>
<p>simply copied into the new distance matrix. This yields the new distance matrix
</p>
<p>shown in Table 9.3.
</p>
<p>Continuing the clustering procedure, we simply repeat the last step by merging
</p>
<p>the objects in the new distance matrix that exhibit the smallest distance and
</p>
<p>calculate the distance from this new cluster to all the other objects. In our case,
</p>
<p>Fig. 9.9 Ward&rsquo;s linkage
</p>
<p>9.3 Conducting a Cluster Analysis 327</p>
<p/>
</div>
<div class="page"><p/>
<p>the smallest distance (23.854, printed in bold in Table 9.3) occurs between the
</p>
<p>newly formed cluster [B, C] and object E. The result of this step is described in
</p>
<p>Table 9.4.
</p>
<p>Try to calculate the remaining steps yourself and compare your solution with the
</p>
<p>distance matrices in the following Tables 9.5, 9.6 and 9.7.
</p>
<p>Table 9.2 Euclidean distance matrix
</p>
<p>Objects A B C D E F G
</p>
<p>A 0
</p>
<p>B 49.010 0
</p>
<p>C 36.249 21.260 0
</p>
<p>D 28.160 58.592 38.275 0
</p>
<p>E 57.801 34.132 23.854 40.497 0
</p>
<p>F 64.288 68.884 49.649 39.446 39.623 0
</p>
<p>G 81.320 105.418 84.291 53.852 81.302 43.081 0
</p>
<p>Note: Smallest distance is printed in bold
</p>
<p>Table 9.3 Distance matrix after first clustering step (single linkage)
</p>
<p>Objects A B, C D E F G
</p>
<p>A 0
</p>
<p>B, C 36.249 0
</p>
<p>D 28.160 38.275 0
</p>
<p>E 57.801 23.854 40.497 0
</p>
<p>F 64.288 49.649 39.446 39.623 0
</p>
<p>G 81.320 84.291 53.852 81.302 43.081 0
</p>
<p>Note: Smallest distance is printed in bold
</p>
<p>Table 9.4 Distance
</p>
<p>matrix after second
</p>
<p>clustering step (single
</p>
<p>linkage)
</p>
<p>Objects A B, C, E D F G
</p>
<p>A 0
</p>
<p>B, C, E 36.249 0
</p>
<p>D 28.160 38.275 0
</p>
<p>F 64.288 39.623 39.446 0
</p>
<p>G 81.320 81.302 53.852 43.081 0
</p>
<p>Note: Smallest distance is printed in bold
</p>
<p>Table 9.5 Distance
</p>
<p>matrix after third clustering
</p>
<p>step (single linkage)
</p>
<p>Objects A, D B, C, E F G
</p>
<p>A, D 0
</p>
<p>B, C, E 36.249 0
</p>
<p>F 39.446 39.623 0
</p>
<p>G 53.852 81.302 43.081 0
</p>
<p>Note: Smallest distance is printed in bold
</p>
<p>328 9 Cluster Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>By following the single linkage procedure, the last steps involve the merger of
</p>
<p>cluster [A,B,C,D,E,F] and object G at a distance of 43.081. Do you get the same
</p>
<p>results? As you can see, conducting a basic cluster analysis manually is not that
</p>
<p>hard at all&mdash;not if there are only a few objects.
</p>
<p>9.3.2.2 Partitioning Methods: k-means
Partitioning clustering methods are another important group of procedures. As
</p>
<p>with hierarchical clustering, there is a wide array of different algorithms; of these,
</p>
<p>k-means is the most popular for market research.
</p>
<p>Understanding k-means Clustering
</p>
<p>The k-means method follows an entirely different concept than the hierarchical
</p>
<p>methods discussed above. The initialization of the analysis is one crucial difference.
</p>
<p>Unlike with hierarchical clustering, we need to specify the number of clusters to
</p>
<p>extract from the data prior to the analysis. Using this information as input, k-means
then assigns all the objects to the number of clusters that the researcher specifies.
</p>
<p>This starting partition comes in different forms. Examples of these forms include:
</p>
<p>&ndash; randomly select k objects as starting centers for the k clusters (K unique random
observations in Stata),
</p>
<p>&ndash; use the first or last k objects as starting centers for the k clusters (First K
observations and Last K observations in Stata),
</p>
<p>&ndash; randomly allocate all the objects into k groups and compute the means
(or medians) of each group. These means (or medians) then serve as starting
</p>
<p>centers (Group means from K random partitions of the data in Stata), and
&ndash; provide an initial grouping variable that defines the groups among the objects to
</p>
<p>be clustered. The group means (or medians) of these groups are used as the
</p>
<p>starting centers (Group means from partitions defined by initial grouping
variables in Stata).
</p>
<p>After the initialization, k-means successively reassigns the objects to other
clusters with the aim of minimizing the within-cluster variation. This within-cluster
</p>
<p>variation is equal to the squared distance of each observation to the center of the
</p>
<p>associated cluster (i.e., the centroid). If the reallocation of an object to another
</p>
<p>cluster decreases the within-cluster variation, this object is reassigned to that cluster.
</p>
<p>Table 9.6 Distance
</p>
<p>matrix after fourth
</p>
<p>clustering step (single
</p>
<p>linkage)
</p>
<p>Objects A, B, C, D, E F G
</p>
<p>A, B, C, D, E 0
</p>
<p>F 39.446 0
</p>
<p>G 53.852 43.081 0
</p>
<p>Note: Smallest distance is printed in bold
</p>
<p>Table 9.7 Distance
</p>
<p>matrix after fifth clustering
</p>
<p>step (single linkage)
</p>
<p>Objects A, B, C, D, E, F G
</p>
<p>A, B, C, D, E, F 0
</p>
<p>G 43.081 0
</p>
<p>9.3 Conducting a Cluster Analysis 329</p>
<p/>
</div>
<div class="page"><p/>
<p>Since cluster affiliations can change in the course of the clustering process (i.e.,
</p>
<p>an object can move to another cluster in the course of the analysis), k-means does
not build a hierarchy, which hierarchical clustering does (Fig. 9.4). Therefore, k-
means belongs to the group of non-hierarchical clustering methods.
</p>
<p>For a better understanding of the approach, let&rsquo;s take a look at how it works in
</p>
<p>practice. Figures 9.10, 9.11, 9.12 and 9.13 illustrate the four steps of the k-means
clustering process&mdash;research has produced several variants of the original algo-
</p>
<p>rithm, which we briefly discuss in Box 9.2.
</p>
<p>&ndash; Step 1: The researcher needs to specify the number of clusters that k-means
should retain from the data. Using this number as the input, the algorithm selects
</p>
<p>a center for each cluster. In our example, two cluster centers are randomly
</p>
<p>initiated, which CC1 (first cluster) and CC2 (second cluster) represent in
</p>
<p>Fig. 9.10.
</p>
<p>&ndash; Step 2: Euclidean distances are computed from the cluster centers to every
</p>
<p>object. Each object is then assigned to the cluster center with the shortest
</p>
<p>distance to it. In our example (Fig. 9.11), objects A, B, and C are assigned to
</p>
<p>the first cluster, whereas objects D, E, F, and G are assigned to the second. We
</p>
<p>now have our initial partitioning of the objects into two clusters.
</p>
<p>&ndash; Step 3: Based on the initial partition in step 2, each cluster&rsquo;s geometric center
</p>
<p>(i.e., its centroid) is computed. This is done by computing the mean values of the
</p>
<p>objects contained in the cluster (e.g., A, B, C in the first cluster) in terms of each
</p>
<p>of the variables (price consciousness and brand loyalty). As we can see in
</p>
<p>Fig. 9.12, both clusters&rsquo; centers now shift to new positions (CC1&rsquo; in the first
</p>
<p>and CC2&rsquo; in the second cluster; the inverted comma indicates that the cluster
</p>
<p>center has changed).
</p>
<p>&ndash; Step 4: The distances are computed from each object to the newly located
</p>
<p>cluster centers and the objects are again assigned to a certain cluster on the
</p>
<p>basis of their minimum distance to other cluster centers (CC1&rsquo; and CC2&rsquo;). Since
</p>
<p>the cluster centers&rsquo; position changed with respect to the initial situation in the
</p>
<p>first step, this could lead to a different cluster solution. This is also true of our
</p>
<p>example, because object E is now&mdash;unlike in the initial partition&mdash;closer to the
</p>
<p>first cluster center (CC1&rsquo;) than to the second (CC2&rsquo;). Consequently, this object is
</p>
<p>now assigned to the first cluster (Fig. 9.13).
</p>
<p>The k-means procedure is now repeated until a predetermined number of iterations
are reached, or convergence is achieved (i.e., there is no change in the cluster
</p>
<p>affiliations).
</p>
<p>Three aspects are worth noting in terms of using k-means:
</p>
<p>&ndash; k-means is implicitly based on pairwise Euclidean distances, because the sum of
the squared distances from the centroid is equal to the sum of the pairwise
</p>
<p>squared Euclidean distances divided by the number of objects. Therefore, the
</p>
<p>method should only be used with metric and, in case of equidistant scales,
</p>
<p>ordinal variables. Similarly, you should only use (squared) Euclidean distances
</p>
<p>with k-means.
</p>
<p>330 9 Cluster Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>Fig. 9.10 k-means procedure (step 1: placing random cluster centers)
</p>
<p>Fig. 9.11 k-means procedure (step 2: assigning objects to the closest cluster center)
</p>
<p>9.3 Conducting a Cluster Analysis 331</p>
<p/>
</div>
<div class="page"><p/>
<p>&ndash; Results produced by k-means depend on the starting partition. That is, k-means
produce different results, depending on the starting partition chosen by the
</p>
<p>researcher or randomly initiated by the software. As a result, k-means may
converge in a local optimum, which means that the solution is only optimal
</p>
<p>Fig. 9.12 k-means procedure (step 3: recomputing cluster centers)
</p>
<p>Fig. 9.13 k-means procedure (step 4: reassigning objects to the closest cluster center)
</p>
<p>332 9 Cluster Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>compared to similar solutions, but not globally. Therefore, you should run k-
means multiple times using different options for generating a starting partition.
</p>
<p>&ndash; k-means is less computationally demanding than hierarchical clustering
techniques. The method is therefore generally preferred for sample sizes above
</p>
<p>500, and particularly for big data applications.
&ndash; Running k-means requires specifying the number of clusters to retain prior to
</p>
<p>running the analysis. We discuss this issue in the next section.
</p>
<p>Box 9.2 Variants of the Original k-means Method
</p>
<p>k-medians is a popular variant of k-means and has also been implemented in
Stata. This procedure essentially follows the same logic and procedure as k-
means. However, instead of using the cluster mean as a reference point for the
</p>
<p>calculation of the within cluster variance, k-medians minimizes the absolute
deviations from the cluster medians, which equals the city-block distance.
</p>
<p>Thus, k-medians does not optimize the squared deviations from the mean as
in k-means, but absolute distances. In this way, k-medians avoids the possible
effect of extreme values on the cluster solution. Further variants, which are
</p>
<p>not menu-accessible in Stata, use other cluster centers (e.g., k-medoids;
</p>
<p>Kaufman and Rousseeuw 2005; Park and Jun 2009), or optimize the
</p>
<p>initialization process (e.g., k-means++; Arthur and Vassilvitskii 2007).
</p>
<p>9.3.3 Select a Measure of Similarity or Dissimilarity
</p>
<p>In the previous section, we discussed different linkage algorithms used in agglom-
</p>
<p>erative hierarchical clustering as well as the k-means procedure. All these clustering
procedures rely on measures that express the (dis)similarity between pairs of
</p>
<p>objects. In the following section, we introduce different measures for metric,
</p>
<p>ordinal, nominal, and binary variables.
</p>
<p>9.3.3.1 Metric and Ordinal Variables
</p>
<p>Distance Measures
</p>
<p>A straightforward way to assess two objects&rsquo; proximity is by drawing a straight line
</p>
<p>between them. For example, on examining the scatter plot in Fig. 9.1, we can easily
</p>
<p>see that the length of the line connecting observations B and C is much shorter than
</p>
<p>the line connecting B and G. This type of distance is called Euclidean distance or
</p>
<p>straight line distance; it is the most commonly used type for analyzing metric
</p>
<p>variables and, if the scales are equidistant (Chap. 3), ordinal variables. Statistical
</p>
<p>software programs such as Stata simply refer to the Euclidean distance as L2, as it is
a specific type of the more general Minkowski distance metric with argument
</p>
<p>2 (Anderberg 1973). Researchers also often use the squared Euclidean distance,
</p>
<p>referred to as L2 squared in Stata. For k-means, using the squared Euclidean
</p>
<p>9.3 Conducting a Cluster Analysis 333</p>
<p/>
</div>
<div class="page"><p/>
<p>distance is more appropriate because of the way the method computes the distances
</p>
<p>from the objects to the centroids (see Section 9.3.2.2).
</p>
<p>In order to use a hierarchical clustering procedure, we need to express these
</p>
<p>distances mathematically. Using the data from Table 9.1, we can compute the
</p>
<p>Euclidean distance between customer B and customer C (generally referred to as
</p>
<p>d(B,C)) by using variables x and y with the following formula:
</p>
<p>dEuclidean B;C&eth; &THORN; &frac14;
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
</p>
<p>xB � xC&eth; &THORN;2 &thorn; yB � yC&eth; &THORN;2
q
</p>
<p>As can be seen, the Euclidean distance is the square root of the sum of the
</p>
<p>squared differences in the variables&rsquo; values. Using the data from Table 9.1, we
</p>
<p>obtain the following:
</p>
<p>dEuclidean B;C&eth; &THORN; &frac14;
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
</p>
<p>82� 66&eth; &THORN;2 &thorn; 94� 80&eth; &THORN;2 &frac14;
q
</p>
<p>ffiffiffiffiffiffiffiffi
</p>
<p>452
p
</p>
<p>� 21:260
</p>
<p>This distance corresponds to the length of the line that connects objects B and
</p>
<p>C. In this case, we only used two variables, but we can easily add more under the
</p>
<p>root sign in the formula. However, each additional variable will add a dimension to
</p>
<p>our research problem (e.g., with six clustering variables, we have to deal with six
</p>
<p>dimensions), making it impossible to represent the solution graphically. Similarly,
</p>
<p>we can compute the distance between customer B and G, which yields the
</p>
<p>following:
</p>
<p>dEuclidean B;G&eth; &THORN; &frac14;
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
</p>
<p>82� 10&eth; &THORN;2 &thorn; 94� 17&eth; &THORN;2 &frac14;
q
</p>
<p>ffiffiffiffiffiffiffiffiffiffiffiffiffiffi
</p>
<p>11, 113
p
</p>
<p>� 105:418
</p>
<p>Likewise, we can compute the distance between all other pairs of objects and
</p>
<p>summarize them in a distance matrix. Table 9.2, which we used as input to illustrate
</p>
<p>the single linkage algorithm, shows the Euclidean distance matrix for objects A-G.
</p>
<p>There are also alternative distance measures: The city-block distance (called L1
in Stata) uses the sum of the variables&rsquo; absolute differences. This distance measure
</p>
<p>is referred to as theManhattan metric as it is akin to the walking distance between
</p>
<p>two points in a city like New York&rsquo;s Manhattan district, where the distance equals
</p>
<p>the number of blocks in the directions North-South and East-West. Using the city-
</p>
<p>block distance to compute the distance between customers B and C (or C and B)
</p>
<p>yields the following:
</p>
<p>dCity�block B;C&eth; &THORN; &frac14; xB � xCj j &thorn; yB � yCj j &frac14; 82� 66j j &thorn; 94� 80j j &frac14; 30
</p>
<p>The resulting distance matrix is shown in Table 9.8.
</p>
<p>Lastly, when working with metric (or ordinal) data, researchers frequently use
</p>
<p>the Chebychev distance (called Linfinity in Stata), which is the maximum of the
</p>
<p>334 9 Cluster Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>absolute difference in the clustering variables&rsquo; values. In respect of customers B
</p>
<p>and C, this result is:
</p>
<p>dChebychev B;C&eth; &THORN; &frac14; max xB � xCj j; yB � yCj j&eth; &THORN; &frac14; max 82� 66j j; 94� 80j j&eth; &THORN; &frac14; 16
</p>
<p>Figure 9.14 illustrates the interrelation between these three distance measures
</p>
<p>regarding two objects (here: B and G) from our example.
</p>
<p>Research has brought forward a range of other distance measures suitable for
</p>
<p>specific research settings. For example, the Stata menu offers the Canberra
</p>
<p>distance, a weighted version of the city-block distance, which is typically used
</p>
<p>for clustering data scattered widely around an origin. Other distance measures, such
</p>
<p>as the Mahalanobis distance, which compensates for collinearity between the
</p>
<p>clustering variables, are accessible via Stata syntax.
</p>
<p>Table 9.8 City-block
</p>
<p>distance matrix
Objects A B C D E F G
</p>
<p>A 0
</p>
<p>B 50 0
</p>
<p>C 48 30 0
</p>
<p>D 31 79 49 0
</p>
<p>E 81 37 33 56 0
</p>
<p>F 79 93 63 54 56 0
</p>
<p>G 101 149 119 70 112 56 0
</p>
<p>Fig. 9.14 Distance measures
</p>
<p>9.3 Conducting a Cluster Analysis 335</p>
<p/>
</div>
<div class="page"><p/>
<p>Different distance measures typically lead to different cluster solutions. Thus,
</p>
<p>it is advisable to use several measures, check for the stability of results, and
</p>
<p>compare them with theoretical or known patterns.
</p>
<p>Association Measures
</p>
<p>The (dis)similarity between objects can also be expressed by means of association
measures (e.g., correlations). For example, suppose a respondent rated price con-
sciousness 2 and brand loyalty 3, a second respondent indicated 5 and 6, whereas a
</p>
<p>third rated these variables 3 and 3. Euclidean and city-block, distances indicate that
</p>
<p>the first respondent is more similar to the third than to the second. Nevertheless, one
</p>
<p>could convincingly argue that the first respondent&rsquo;s ratings are more similar to the
</p>
<p>second&rsquo;s, as both rate brand loyalty higher than price consciousness. This can be
</p>
<p>accounted for by computing the correlation between two vectors of values as a
</p>
<p>measure of similarity (i.e., high correlation coefficients indicate a high degree of
</p>
<p>similarity). Consequently, similarity is no longer defined by means of the difference
</p>
<p>between the answer categories, but by means of the similarity of the answering
</p>
<p>profiles.
</p>
<p>Whether you use one of the distance measures or correlations depends on
</p>
<p>whether you think the relative magnitude of the variables within an object
</p>
<p>(which favors correlation) matters more than the relative magnitude of each
</p>
<p>variable across the objects (which favors distance). Some researchers
</p>
<p>recommended using correlations when applying clustering procedures that
</p>
<p>are particularly susceptible to outliers, such as complete linkage, average
</p>
<p>linkage or centroid linkage. Furthermore, correlations implicitly standardize
</p>
<p>the data, as differences in the scale categories do not have a strong bearing on
</p>
<p>the interpretation of the response patterns. Nevertheless, distance measures
</p>
<p>are most commonly used for their intuitive interpretation. Distance measures
</p>
<p>best represent the concept of proximity, which is fundamental to cluster
</p>
<p>analysis. Correlations, although having widespread application in other
</p>
<p>techniques, represent patterns rather than proximity.
</p>
<p>Standardizing the Data
</p>
<p>In many analysis tasks, the variables under consideration are measured in different
</p>
<p>units with hugely different variance. This would be the case if we extended our set
</p>
<p>of clustering variables by adding another metric variable representing the
</p>
<p>customers&rsquo; gross annual income. Since the absolute variation of the income variable
</p>
<p>would be much higher than the variation of the remaining two variables (remember,
</p>
<p>x and y are measured on a scale from 0 to 100), this would clearly distort our
</p>
<p>336 9 Cluster Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>analysis results. We can resolve this problem by standardizing the data prior to the
</p>
<p>analysis (Chap. 5).
</p>
<p>Different standardization methods are available, such as z-standardization,
which rescales each variable to a mean of 0 and a standard deviation of
</p>
<p>1 (Chap. 5). In cluster analysis, however, range standardization (e.g., to a range
of 0 to 1) typically works better (Milligan and Cooper 1988).
</p>
<p>9.3.3.2 Binary and Nominal Variables
Whereas the distance measures presented thus far can be used for variables
</p>
<p>measured on a metric and, in general, on an ordinal scale, applying them to binary
</p>
<p>and nominal variables is problematic. When nominal variables are involved, you
</p>
<p>should rather select a similarity measure expressing the degree to which the
</p>
<p>variables&rsquo; values share the same category. These matching coefficients can take
</p>
<p>different forms, but rely on the same allocation scheme as shown in Table 9.9.
</p>
<p>In this crosstab, cell a is the number of characteristics present in both objects,
whereas cell d describes the number of characteristics absent in both objects. Cells
b and c describe the number of characteristics present in one, but not the other,
object (see Table 9.10 for an example).
</p>
<p>The allocation scheme in Table 9.9 applies to binary variables (i.e., nominal
</p>
<p>variables with two categories). For nominal variables with more than two
</p>
<p>categories, you need to convert the categorical variable into a set of binary variables
</p>
<p>in order to use matching coefficients. For example, a variable with three categories
</p>
<p>needs to be transformed into three binary variables, one for each category (see the
</p>
<p>following example).
</p>
<p>Based on the allocation scheme in Table 9.9, we can compute different matching
</p>
<p>coefficients, such as the simple matching (SM) coefficient (called Matching in
Stata):
</p>
<p>Table 9.9 Allocation scheme for matching coefficients
</p>
<p>Second object
</p>
<p>Presence of a
</p>
<p>characteristics (1)
</p>
<p>Absence of a
</p>
<p>characteristic (0)
</p>
<p>First
</p>
<p>object
</p>
<p>Presence of a
</p>
<p>characteristic (1)
</p>
<p>a b
</p>
<p>Absence of a
</p>
<p>characteristic (0)
</p>
<p>c d
</p>
<p>Table 9.10 Recoded measurement data
</p>
<p>Object Gender (binary) Customer (binary) Country of residence (binary)
</p>
<p>Male Female Yes No GER UK USA
</p>
<p>A 1 0 1 0 1 0 0
</p>
<p>B 1 0 0 1 0 0 1
</p>
<p>C 0 1 0 1 0 0 1
</p>
<p>9.3 Conducting a Cluster Analysis 337</p>
<p/>
</div>
<div class="page"><p/>
<p>SM &frac14; a&thorn; d
a&thorn; b&thorn; c&thorn; d
</p>
<p>This coefficient takes both the joint presence and the joint absence of a charac-
</p>
<p>teristic (as indicated by cells a and d in Table 9.9) into account. This feature makes
the simple matching coefficient particularly useful for symmetric variables where
</p>
<p>the joint presence and absence of a characteristic carry an equal degree of informa-
</p>
<p>tion. For example, the binary variable gender has the possible states &ldquo;male&rdquo; and
&ldquo;female.&rdquo; Both are equally valuable and carry the same weight when the simple
</p>
<p>matching coefficient is computed. However, when the outcomes of a binary vari-
</p>
<p>able are not equally important (i.e., the variable is asymmetric), the simple
</p>
<p>matching coefficient proves problematic. An example of an asymmetric variable
</p>
<p>is the presence, or absence, of a relatively rare attribute, such as customer
</p>
<p>complaints. While you say that two customers who complained have something
</p>
<p>in common, you cannot say that customers who did not complain have something in
</p>
<p>common. The most important outcome is usually coded as 1 (present) and the other
</p>
<p>is coded as 0 (absent). The agreement of two 1s (i.e., a positive match) is more
</p>
<p>significant than the agreement of two 0s (i.e., a negative match). Similarly, the
</p>
<p>simple matching coefficient proves problematic when used on nominal variables
</p>
<p>with many categories. In this case, objects may appear very similar, because they
</p>
<p>have many negative matches rather than positive matches.
</p>
<p>Given this issue, researchers have proposed several other matching coefficients,
</p>
<p>such as the Jaccard coefficient (JC) and the Russell and Rao coefficient (RR,
</p>
<p>called Russell in Stata), which (partially) omit the d cell from the calculation. Like
the simple matching coefficient, these coefficients range from 0 to 1 with higher
</p>
<p>values indicating a greater degree of similarity.4 They are defined as follows:
</p>
<p>JC &frac14; a
a&thorn; b&thorn; c
</p>
<p>RR &frac14; a
a&thorn; b&thorn; c&thorn; d
</p>
<p>To provide an example that compares the three coefficients, consider the fol-
</p>
<p>lowing three variables:
</p>
<p>&ndash; gender: male, female
&ndash; customer: yes, no
&ndash; country of residence: GER, UK, USA
</p>
<p>4There are many other matching coefficients, such as Yule&rsquo;s Q, Kulczynski, or Ochiai, which are
also menu-accessible in Stata. However, since most applications of cluster analysis rely on metric
</p>
<p>or ordinal data, we will not discuss these. See Wedel and Kamakura (2000) for more information
</p>
<p>on alternative matching coefficients.
</p>
<p>338 9 Cluster Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>We first transform the measurement data into binary data by recoding the
</p>
<p>original three variables into seven binary variables (i.e., two for gender and
customer; three for country of residence). Table 9.10 shows a binary data matrix
for three objects A, B, and C. Object A is a male customer from Germany; object B
</p>
<p>is a male non-customer from the United States; object C is a female non-customer,
</p>
<p>also from the United States.
</p>
<p>Using the allocation scheme from Table 9.9 to compare objects A and B yields
</p>
<p>the following results for the cells: a &frac14; 1, b &frac14; 2, c&frac14;2, and d &frac14; 2.
This means that the two objects have only one shared characteristic (a &frac14; 1), but
</p>
<p>two characteristics, which are absent from both objects (d &frac14; 2). Using this infor-
mation, we can now compute the three coefficients described earlier:
</p>
<p>SM A;B&eth; &THORN; &frac14; 1&thorn; 2
1&thorn; 2&thorn; 2&thorn; 2 &frac14; 0:571,
</p>
<p>JC A;B&eth; &THORN; &frac14; 1
1&thorn; 2&thorn; 2 &frac14; 0:2, and
</p>
<p>RR A;B&eth; &THORN; &frac14; 1
1&thorn; 2&thorn; 2&thorn; 2 &frac14; 0:143
</p>
<p>As can be seen, the simple matching coefficient suggests that objects A and B are
</p>
<p>reasonably similar. Conversely, the Jaccard coefficient, and particularly the Russel
</p>
<p>Rao coefficient, suggests that they are not.
</p>
<p>Try computing the distances between the other object pairs. Your computation
</p>
<p>should yield the following: SM(A,C) &frac14; 0.143, SM(B,C) &frac14; 0.714, JC(A,C) &frac14; 0, JC
(B,C) &frac14; 0.5, RR(A,C) &frac14; 0, and RR(B,C) &frac14; 0.286.
</p>
<p>9.3.3.3 Mixed Variables
Most datasets contain variables that are measured on multiple scales. For example,
</p>
<p>a market research questionnaire may require the respondent&rsquo;s gender, income
</p>
<p>category, and age. We therefore have to consider variables measured on a nominal,
</p>
<p>ordinal, and metric scale. How can we simultaneously incorporate these variables
</p>
<p>into an analysis?
</p>
<p>A common approach is to dichotomize all the variables and apply the matching
</p>
<p>coefficients discussed above. For metric variables, this involves specifying
</p>
<p>categories (e.g., low, medium, and high age) and converting these into sets of
</p>
<p>binary variables. In most cases, the specification of categories is somewhat arbi-
</p>
<p>trary. Furthermore, this procedure leads to a severe loss in precision, as we
</p>
<p>disregard more detailed information on each object. For example, we lose precise
</p>
<p>information on each respondent&rsquo;s age when scaling this variable down into age
</p>
<p>categories.
</p>
<p>Gower (1971) introduced a dissimilarity coefficient that works with a mix of
</p>
<p>binary and continuous variablesa.Gower&rsquo;s dissimilarity coefficient is a composite
</p>
<p>measure that combines several measures into one, depending on each variable&rsquo;s
</p>
<p>scale level. If binary variables are used, the coefficient takes the value 1 when two
</p>
<p>9.3 Conducting a Cluster Analysis 339</p>
<p/>
</div>
<div class="page"><p/>
<p>objects do not share a certain characteristic (cells b and c in Table 9.9), and 0 else
(cells a and d in Table 9.9). Thus, when all the variables are binary and symmetric,
Gower&rsquo;s dissimilarity coefficient reduces to the simple matching coefficient when
</p>
<p>expressed as a distance measure instead of a similarity measure (i.e., 1 &ndash; SM). If
binary and asymmetric variables are used, Gower&rsquo;s dissimilarity coefficient equals
</p>
<p>the Jaccard coefficient when expressed as a distance measure instead of a similarity
</p>
<p>measure (i.e., 1 &ndash; JC). If continuous variables are used, the coefficient is equal to the
city-block distance divided by each variable&rsquo;s range. Ordinal variables are treated
</p>
<p>as if they were continuous, which is fine when the scale is equidistant (see Chap. 3).
</p>
<p>Gower&rsquo;s dissimilarity coefficient welds the measures used for binary and continu-
</p>
<p>ous variables into one value that is an overall measure of dissimilarity.
</p>
<p>To illustrate Gower&rsquo;s dissimilarity coefficient, consider the following example
</p>
<p>with the two binary variables gender and customer, the ordinal variable income
category (1 &frac14; &ldquo;low&rdquo;, 2 &frac14; &ldquo;medium&rdquo;, 3 &frac14; &ldquo;high&rdquo;), and the metric variable age.
Table 9.11 shows the data for three objects A, B, and C.
</p>
<p>To compute Gower&rsquo;s dissimilarity coefficient for objects A and B, we first
</p>
<p>consider the variable gender. Since both objects A and B are male, they share
two characteristics (male&frac14; &ldquo;yes&rdquo;, female&frac14; &ldquo;no&rdquo;), which entails a distance of 0 for
both variable levels. With regard to the customer variable, the two objects have
different characteristics, hence a distance of 1 for each variable level. The ordinal
</p>
<p>variable income category is treated as continuous, using the city-block distance
</p>
<p>(here: |2&ndash;3|) divided by the variable&rsquo;s range (here: 3 &ndash; 1). Finally, the distance with
</p>
<p>regard to the age variable is |21� 37|/(37� 21)&frac14; 1. Hence, the resulting Gower
distance is:
</p>
<p>dGower A;B&eth; &THORN; &frac14;
1
</p>
<p>6
0&thorn; 0&thorn; 1&thorn; 1&thorn; 0:5&thorn; 1&eth; &THORN; � 0:583
</p>
<p>Computing the Gower distance between the other two object pairs yields
</p>
<p>dGower(A,C) � 0.833, and dGower(B,C) � 0.583.
</p>
<p>9.3.4 Decide on the Number of Clusters
</p>
<p>An important question we haven&rsquo;t yet addressed is how to decide on the number of
</p>
<p>clusters. A misspecified number of clusters results in under- or oversegmentation,
</p>
<p>which easily leads to inaccurate management decisions on, for example, customer
</p>
<p>Table 9.11 Recoded measurement data
</p>
<p>Object
</p>
<p>Gender (binary) Customer (binary)
</p>
<p>Income category (ordinal) Age (metric)Male Female Yes No
</p>
<p>A 1 0 1 0 2 21
</p>
<p>B 1 0 0 1 3 37
</p>
<p>C 0 1 0 1 1 29
</p>
<p>340 9 Cluster Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>targeting, product positioning, or determining the optimal marketing mix (Becker
</p>
<p>et al. 2015).
</p>
<p>We can select the number of clusters pragmatically, choosing a grouping that
</p>
<p>&ldquo;works&rdquo; for our analysis, but sometimes we want to select the &ldquo;best&rdquo; solution that
</p>
<p>the data suggest. However, different clustering methods require different
</p>
<p>approaches to decide on the number of clusters. Hence, we discuss hierarchical
</p>
<p>and portioning methods separately.
</p>
<p>9.3.4.1 Hierarchical Methods: Deciding on the Number of Clusters
To guide this decision, we can draw on the distances at which the objects were
</p>
<p>combined. More precisely, we can seek a solution in which an additional combina-
</p>
<p>tion of clusters or objects would occur at a greatly increased distance. This raises
</p>
<p>the issue of what a great distance is.
</p>
<p>We can seek an answer by plotting the distance level at which the mergers of
</p>
<p>objects and clusters occur by using a dendrogram. Figure 9.15 shows the dendro-
</p>
<p>gram for our example as produced by Stata. We read the dendrogram from the
</p>
<p>bottom to the top. The horizontal lines indicate the distances at which the objects
</p>
<p>were merged. For example, according to our calculations above, objects B and C
</p>
<p>were merged at a distance of 21.260. In the dendrogram, the horizontal line linking
</p>
<p>the two vertical lines that go from B and C indicates this merger. To decide on the
</p>
<p>number of clusters, we cut the dendrogram horizontally in the area where no merger
</p>
<p>has occurred for a long distance. In our example, this is done when moving from a
</p>
<p>four-cluster solution, which occurs at a distance of 28.160 (Table 9.4), to a three-
</p>
<p>cluster solution, which occurs at a distance of 36.249 (Table 9.5). This result
</p>
<p>suggests a four-cluster solution [A,D], [B,C,E], [F], and [G], but this conclusion
</p>
<p>is not clear-cut. In fact, the dendrogram often does not provide a clear indication,
</p>
<p>because it is generally difficult to identify where the cut should be made. This is
</p>
<p>particularly true of large sample sizes when the dendrogram becomes unwieldy.
</p>
<p>Research has produced several other criteria for determining the number of
</p>
<p>clusters in a dataset (referred to as stopping rules in Stata).5 One of the most
prominent criteria is Calinski and Harabasz&rsquo;s (1974) variance ratio criterion
</p>
<p>(VRC; also called Calinski-Harabasz pseudo-F in Stata). For a solution with
n objects and k clusters, the VRC is defined as:
</p>
<p>VRCk &frac14; SSB= k � 1&eth; &THORN;&eth; &THORN;= SSW= n� k&eth; &THORN;&eth; &THORN;,
</p>
<p>where SSB is the sum of the squares between the clusters and SSW is the sum of the
squares within the clusters. The criterion should seem familiar, as it is similar to the
</p>
<p>F-value of a one-way ANOVA (see Chap. 6). To determine the appropriate number
of clusters, you should choose the number that maximizes the VRC. However, as
</p>
<p>the VRC usually decreases with a greater number of clusters, you should compute
</p>
<p>5For details on the implementation of these stopping rules in Stata, see Halpin (2016).
</p>
<p>9.3 Conducting a Cluster Analysis 341</p>
<p/>
</div>
<div class="page"><p/>
<p>the difference in the VRC values ωk of each cluster solution, using the following
</p>
<p>formula:6
</p>
<p>ωk &frac14; VRCk&thorn;1 � VRCk&eth; &THORN; � VRCk � VRCk�1&eth; &THORN;:
</p>
<p>The number of clusters k that minimizes the value in ωk indicates the best cluster
solution. Prior research has shown that the VRC reliably identifies the correct
</p>
<p>number of clusters across a broad range of constellations (Miligan and Cooper
</p>
<p>1985). However, owing to the term VRCk�1, which is not defined for a one-cluster
solution, the minimum number of clusters that can be selected is three, which is a
</p>
<p>disadvantage when using the ωk statistic.
</p>
<p>Another criterion, which works well for determining the number of clusters (see
</p>
<p>Miligan and Cooper 1985) is the Duda-Hart index (Duda and Hart 1973). This
</p>
<p>index essentially performs the same calculation as the VRC, but compares the SSW
values in a pair of clusters to be split both before and after this split. More precisely,
</p>
<p>the Duda-Hart index is the SSW in the two clusters (Je(2)) divided by the SSW in one
cluster (Je(1)); that is:
</p>
<p>Duda� Hart &frac14; Je&eth;2&THORN;
Je&eth;1&THORN;
</p>
<p>Fig. 9.15 Dendrogram
</p>
<p>6In the Web Appendix (!Downloads), we offer a Stata.ado file to calculate the ωk called
chomega.ado. We also offer an Excel sheet (VRC.xlsx) to calculate the ωk manually.
</p>
<p>342 9 Cluster Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>To determine the number of clusters, you should choose the solution, which
</p>
<p>maximizes the Je(2)/Je(1) index value.
Duda et al. (2001) have also proposed a modified version of the index, which is
</p>
<p>called the pseudo T-squared. This index takes the number of observations in both
groups into account. Contrary to the Duda-Hart index, you should choose the
</p>
<p>number of clusters that minimizes the pseudo T-squared.
Two aspects are important when using the Duda-Hart indices:
</p>
<p>&ndash; The indices are not appropriate in combination with single linkage clustering, as
</p>
<p>chaining effects may occur. In this case, both indices will produce ambiguous
</p>
<p>results, as evidenced in highly similar values for different cluster solutions
</p>
<p>(Everitt and Rabe-Hesketh 2006).
</p>
<p>&ndash; The indices are considered &ldquo;local&rdquo; in that they do not consider the entire data
</p>
<p>structure in their computation, but only the SSW in the group being split. With
regard to our example above, the Je(2)/Je(1) index for a two-cluster solution
</p>
<p>would only consider the variation in objects A to F, but not G. This characteristic
</p>
<p>makes the Duda-Hart indices somewhat inferior to the VRC, which takes the
</p>
<p>entire variation into account (i.e., the criterion is &ldquo;global&rdquo;).
</p>
<p>In practice, you should combine the VRC and the Duda-Hart indices by
</p>
<p>selecting the number of clusters that yields a large VRC, a large Je(2)/Je
</p>
<p>(1) index, and a small pseudo T-squared value. These values do not necessar-
</p>
<p>ily have to be the maximum or minimum values. Note that the VRC and
</p>
<p>Duda-Hart indices become less informative as the number of objects in the
</p>
<p>clusters becomes smaller.
</p>
<p>Overall, the above criteria can often only provide rough guidance regarding the
</p>
<p>number of clusters that should be selected; consequently, you should instead take
</p>
<p>practical considerations into account. Occasionally, you might have a priori knowl-
</p>
<p>edge, or a theory on which you can base your choice. However, first and foremost,
</p>
<p>you should ensure that your results are interpretable and meaningful. Not only must
</p>
<p>the number of clusters be small enough to ensure manageability, but each segment
</p>
<p>should also be large enough to warrant strategic attention.
</p>
<p>9.3.4.2 Partitioning Methods: Deciding on the Number of Clusters
When running partitioning methods, such as k-means, you have to pre-specify the
number of clusters to retain from the data. There are varying ways of guiding this
</p>
<p>decision:
</p>
<p>&ndash; Compute the VRC (see discussion in the context of hierarchical clustering) for
</p>
<p>an alternating number of clusters and select the solution that maximizes the VRC
</p>
<p>or minimizes ωk. For example, compute the VRC for a three- to five-cluster
</p>
<p>9.3 Conducting a Cluster Analysis 343</p>
<p/>
</div>
<div class="page"><p/>
<p>solution and select the number of clusters that minimizes ωk. Note that the Duda-
</p>
<p>Hart indices are not applicable as they require a hierarchy of objects and
</p>
<p>mergers, which partitioning methods do not produce.
</p>
<p>&ndash; Run a hierarchical procedure to determine the number of clusters by using the
</p>
<p>dendrogram and run k-means afterwards.7 This approach also enables you to find
starting values for the initial cluster centers to handle a second problem, which
</p>
<p>relates to the procedure&rsquo;s sensitivity to the initial classification (we will follow
</p>
<p>this approach in the example application).
</p>
<p>&ndash; Rely on prior information, such as earlier research findings.
</p>
<p>9.3.5 Validate and Interpret the Clustering Solution
</p>
<p>Before interpreting the cluster solution, we need to assess the stability of the results.
</p>
<p>Stability means that the cluster membership of individuals does not change, or only
</p>
<p>changes a little when different clustering methods are used to cluster the objects.
</p>
<p>Thus, when different methods produce similar results, we claim stability.
</p>
<p>The aim of any cluster analysis is to differentiate well between the objects. The
</p>
<p>identified clusters should therefore differ substantially from each other and the
</p>
<p>members of different clusters should respond differently to different marketing-mix
</p>
<p>elements and programs.
</p>
<p>Lastly, we need to profile the cluster solution by using observable variables.
</p>
<p>Profiling ensures that we can easily assign new objects to clusters based on
</p>
<p>observable traits. For example, we could identify clusters based on loyalty to a
</p>
<p>product, but in order to use these different clusters, their membership should be
</p>
<p>identifiable according to tangible variables, such as income, location, or family size,
</p>
<p>in order to be actionable.
</p>
<p>The key to successful segmentation is to critically revisit the results of different
</p>
<p>cluster analysis set-ups (e.g., by using different algorithms on the same data) in
</p>
<p>terms of managerial relevance. The following criteria help identify a clustering
</p>
<p>solution (Kotler and Keller 2015; Tonks 2009).
</p>
<p>&ndash; Substantial: The clusters are large and sufficiently profitable to serve.
&ndash; Reliable: Only clusters that are stable over time can provide the necessary basis
</p>
<p>for a successful marketing strategy. If clusters change their composition quickly,
</p>
<p>or their members&rsquo; behavior, targeting strategies are not likely to succeed.
</p>
<p>Therefore, a certain degree of stability is necessary to ensure that marketing
</p>
<p>strategies can be implemented and produce adequate results. Reliability can be
</p>
<p>evaluated by critically revisiting and replicating the clustering results at a
</p>
<p>later date.
</p>
<p>&ndash; Accessible: The clusters can be effectively reached and served.
</p>
<p>7See Punj and Stewart (1983) for additional information on this sequential approach.
</p>
<p>344 9 Cluster Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>&ndash; Actionable: Effective programs can be formulated to attract and serve the
clusters.
</p>
<p>&ndash; Parsimonious: To be managerially meaningful, only a small set of substantial
clusters should be identified.
</p>
<p>&ndash; Familiar: To ensure management acceptance, the cluster composition should be
easy to relate to.
</p>
<p>&ndash; Relevant: Clusters should be relevant in respect of the company&rsquo;s competencies
and objectives.
</p>
<p>9.3.5.1 Stability
Stability is evaluated by using different clustering procedures on the same data and
</p>
<p>considering the differences that occur. For example, you may first run a hierarchical
</p>
<p>clustering procedure, followed by k-means clustering to check whether the cluster
affiliations of the objects change. Alternatively, running a hierarchical clustering
</p>
<p>procedure, you can use different distance measures and evaluate their effect on the
</p>
<p>stability of the results. However, note that it is common for results to change even
</p>
<p>when your solution is adequate. As a rule of thumb, if more than 20% of the cluster
</p>
<p>affiliations change from one technique to the other, you should reconsider the
</p>
<p>analysis and use, for example, a different set of clustering variables, or reconsider
</p>
<p>the number of clusters. Note, however, that this percentage is likely to increase with
</p>
<p>the number of clusters used.
</p>
<p>When the data matrix exhibits identical values (referred to as ties), the ordering
</p>
<p>of the objects in the dataset can influence the results of the hierarchical clustering
</p>
<p>procedure. For example, the distance matrix based on the city-block distance in
</p>
<p>Table 9.8 shows the distance of 56 for object pairs (D,E), (E,F), and (F,G). Ties can
</p>
<p>prove problematic when they occur for the minimum distance in a distance matrix,
</p>
<p>as the decision about which objects to merge then becomes ambiguous (i.e., should
</p>
<p>we merge objects D and E, E and F, or F and G if 56 was the smallest distance in the
</p>
<p>matrix?). To handle this problem, van der Kloot et al. (2005) recommend
</p>
<p>re-running the analysis with a different input order of the data. The downside of
</p>
<p>this approach is that the labels of a cluster may change from one analysis to the next.
</p>
<p>This issue is referred to as label switching. For example, in the first analysis, cluster
1 may correspond to cluster 2 in the second analysis. Ties are, however, more the
</p>
<p>exception than the rule in practical applications&mdash;especially when using (squared)
</p>
<p>Euclidean distances&mdash;and generally don&rsquo;t have a pronounced impact on the results.
</p>
<p>However, if changing the order of the objects also drastically changes the cluster
</p>
<p>compositions (e.g., in terms of cluster sizes), you should reconsider the set-up of the
</p>
<p>analysis and, for example, re-run it with different clustering variables.
</p>
<p>9.3.5.2 Differentiation of the Data
To examine whether the final partition differentiates the data well, we need to
</p>
<p>examine the cluster centroids. This step is highly important, as the analysis sheds
</p>
<p>light on whether the clusters are truly distinct. Only if objects across two (or more)
</p>
<p>clusters exhibit significantly different means in the clustering variables (or any
</p>
<p>other relevant variable) can they be distinguished from each other. This can be
</p>
<p>9.3 Conducting a Cluster Analysis 345</p>
<p/>
</div>
<div class="page"><p/>
<p>easily ascertained by comparing the means of the clustering variables across the
</p>
<p>clusters with independent t-tests or ANOVA (see Chap. 6).
Furthermore, we need to assess the solution&rsquo;s criterion validity. We do this by
</p>
<p>focusing on the criterion variables that have a theoretical relationship with the
</p>
<p>clustering variables, but were not included in the analysis. In market research,
</p>
<p>criterion variables are usually managerial outcomes, such as the sales per person, or
</p>
<p>willingness-to-pay. If these criterion variables differ significantly, we can conclude
</p>
<p>that the clusters are distinct groups with criterion validity.
</p>
<p>9.3.5.3 Profiling
As indicated at the beginning of the chapter, cluster analysis usually builds on
</p>
<p>unobservable clustering variables. This creates an important problem when work-
</p>
<p>ing with the final solution: How can we decide to which cluster a new object should
</p>
<p>be assigned if its unobservable characteristics, such as personality traits, personal
</p>
<p>values, or lifestyles, are unknown? We could survey these attributes and make a
</p>
<p>decision based on the clustering variables. However, this is costly and researchers
</p>
<p>therefore usually try to identify observable variables (e.g., demographics) that best
</p>
<p>mirror the partition of the objects. More precisely, these observable variables
</p>
<p>should partition the data into similar groups as the clustering variables do. Using
</p>
<p>these observable variables, it is then easy to assign a new object (whose cluster
</p>
<p>membership is unknown) to a certain cluster. For example, assume that we used a
</p>
<p>set of questions to assess the respondents&rsquo; values and learned that a certain cluster
</p>
<p>comprises respondents who appreciate self-fulfillment, enjoyment of life, and a
</p>
<p>sense of accomplishment, whereas this is not the case in another cluster. If we were
</p>
<p>able to identify explanatory variables, such as gender or age, which distinguish
</p>
<p>these clusters adequately, then we could assign a new person to a specific cluster on
</p>
<p>the basis of these observable variables whose value traits may still be unknown.
</p>
<p>9.3.5.4 Interpret the Clustering Solution
The interpretation of the solution requires characterizing each cluster by using the
</p>
<p>criterion or other variables (in most cases, demographics). This characterization
</p>
<p>should focus on criterion variables that convey why the cluster solution is relevant.
</p>
<p>For example, you could highlight that customers in one cluster have a lower
</p>
<p>willingness to pay and are satisfied with lower service levels, whereas customers
</p>
<p>in another cluster are willing to pay more for a superior service. By using this
</p>
<p>information, we can also try to find a meaningful name or label for each cluster; that
</p>
<p>is, one that adequately reflects the objects in the cluster. This is usually a challeng-
</p>
<p>ing task, especially when unobservable variables are involved.
</p>
<p>While companies develop their own market segments, they frequently use
</p>
<p>standardized segments, based on established buying trends, habits, and
</p>
<p>customers&rsquo; needs to position their products in different markets. The
</p>
<p>(continued)
</p>
<p>346 9 Cluster Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>PRIZM lifestyle by Nielsen is one of the most popular segmentation
</p>
<p>databases. It combines demographic, consumer behavior, and geographic
</p>
<p>data to help marketers identify, understand, and reach their customers and
</p>
<p>prospective customers. PRIZM defines every US household in terms of more
</p>
<p>than 60 distinct segments to help marketers discern these consumers&rsquo; likes,
</p>
<p>dislikes, lifestyles, and purchase behaviors.
</p>
<p>An example is the segment labeled &ldquo;Connected Bohemians,&rdquo; which Nielsen
</p>
<p>characterizes as a &ldquo;collection of mobile urbanites, Connected Bohemians
</p>
<p>represent the nation&rsquo;s most liberal lifestyles. Its residents are a progressive
</p>
<p>mix of tech savvy, young singles, couples, and families ranging from students
</p>
<p>to professionals. In their funky row houses and apartments, Bohemian Mixers
</p>
<p>are the early adopters who are quick to check out the latest movie, nightclub,
</p>
<p>laptop, and microbrew.&rdquo; Members of this segment are between 25 and 44 years
</p>
<p>old, have a midscale income, own a hybrid vehicle, eat at Starbucks, and go
</p>
<p>skiing/snowboarding. (http://www.MyBestSegments.com).
</p>
<p>Table 9.12 summarizes the steps involved in a hierarchical and k-means cluster-
ing when using Stata. The syntax code shown in the cells comes from the case
</p>
<p>study, which we introduce in the following section.
</p>
<p>Table 9.12 Steps involved in carrying out a cluster analysis in Stata
</p>
<p>Theory Action
</p>
<p>Research problem
</p>
<p>Identification of homogenous groups of objects in a population
</p>
<p>Select clustering variables to
</p>
<p>form segments
</p>
<p>Select relevant variables that potentially exhibit high degrees
</p>
<p>of criterion validity with regard to a specific managerial
</p>
<p>objective.
</p>
<p>Requirements
</p>
<p>Sufficient sample size Make sure that the relationship between the objects and the
</p>
<p>clustering variables is reasonable. Ten times the number of
</p>
<p>clustering variables is the bare minimum, but 30 to 70 times is
</p>
<p>recommended. Ensure that the sample size is large enough to
</p>
<p>guarantee substantial segments.
</p>
<p>Low levels of collinearity
</p>
<p>among the variables
</p>
<p>► Statistics ► Summaries, tables and tests ► Summary and
</p>
<p>descriptive statistics ► Pairwise correlations
</p>
<p>pwcorr e1 e5 e9 e21 e22
</p>
<p>In case of highly correlated variables (correlation coefficients
</p>
<p>&gt; 0.90), delete one variable of the offending pair.
</p>
<p>Specification
</p>
<p>Choose the clustering
</p>
<p>procedure
</p>
<p>If there is a limited number of objects in your dataset, rather
</p>
<p>use hierarchical clustering:
</p>
<p>► Statistics ► Multivariate analysis ► Cluster analysis ►
</p>
<p>Cluster Data ► Choose a linkage algorithm
</p>
<p>cluster wardslinkage e1 e5 e9 e21 e22, measure
(L2squared) name(wards_linkage)
</p>
<p>(continued)
</p>
<p>9.3 Conducting a Cluster Analysis 347</p>
<p/>
<div class="annotation"><a href="http://www.mybestsegments.com">http://www.mybestsegments.com</a></div>
</div>
<div class="page"><p/>
<p>Table 9.12 (continued)
</p>
<p>Theory Action
</p>
<p>If there are many observations (&gt; 500) in your dataset, rather
use k-means clustering:
► Statistics Multivariate analysis ► Cluster analysis ►
</p>
<p>Cluster Data ► kmeans
</p>
<p>cluster kmeans e1 e5 e9 e21 e22, k(2) measure
(L2squared) start(krandom) name(kmeans)
</p>
<p>Select a measure of (dis)
</p>
<p>similarity
</p>
<p>Hierarchical methods:
</p>
<p>Select from the (dis)similarity measure menu, depending on
</p>
<p>the clustering variables&rsquo; scale level.
</p>
<p>Depending on the scale level, select the measure; convert
</p>
<p>variables with multiple categories into a set of binary variables
</p>
<p>and use matching coefficients; Choose Gower&rsquo;s dissimilarity
</p>
<p>coefficient for mixed variables.
</p>
<p>When the variables are measured on different units,
</p>
<p>standardize the variables to a range from 0 to 1 prior to the
</p>
<p>analysis, using the following commands:
</p>
<p>summarize e1
</p>
<p>return list
</p>
<p>gen e1_rsdt &frac14;.
replace e1_rsdt &frac14; (e1- r(min)) / (r(max)-r
(min))
</p>
<p>Partitioning methods:
</p>
<p>Use the squared Euclidean distance from the (dis)
</p>
<p>similarity menu.
</p>
<p>Deciding on the number of
</p>
<p>clusters
</p>
<p>Hierarchical clustering:
</p>
<p>Examine the dendrogram:
</p>
<p>► Statistics ► Multivariate analysis ► Cluster analysis ►
</p>
<p>Postclustering ► Dendrogram
</p>
<p>cluster dendrogram wards_linkage, cutnumber
(10) showcount
</p>
<p>Examine the VRC and Duda-Hart indices:
</p>
<p>► Statistics Multivariate analysis ► Cluster analysis ►
</p>
<p>Postclustering ► Cluster analysis stopping rules.
</p>
<p>For VRC: cluster stop wards_linkage, rule
(calinski) groups(2/11)
</p>
<p>For Duda-Hart: cluster stop wards_linkage, rule
(duda) groups(1/10)
</p>
<p>Include practical considerations in your decision.
</p>
<p>Partitioning methods:
</p>
<p>Run a hierarchical cluster analysis and decide on the number
</p>
<p>of segments based on a dendrogram, the VRC, and the Duda-
</p>
<p>Hart indices; use the resulting partition as starting partition.
</p>
<p>► Statistics Multivariate analysis ► Cluster analysis ►
</p>
<p>Postclustering ► Cluster analysis stopping rules.
</p>
<p>cluster kmeans e1 e5 e9 e21 e22, k(3) measure
(L2squared) name(kmeans) start(group
(cluster_wl))
</p>
<p>Include practical considerations in your decision.
</p>
<p>(continued)
</p>
<p>348 9 Cluster Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>9.4 Example
</p>
<p>Let&rsquo;s go back to the Oddjob Airways case study and run a cluster analysis on the
</p>
<p>data. Our aim is to identify a manageable number of segments that differentiates
</p>
<p>the customer base well. To do so, we first select a set of clustering variables, taking
</p>
<p>the sample size and potential collinearity issues into account. Next, we apply
</p>
<p>hierarchical clustering based on the squared Euclidean distances, using theWard&rsquo;s
</p>
<p>linkage algorithm. This analysis will help us determine a suitable number of
</p>
<p>segments and a starting partition, which we will then use as the input for k-
means clustering.
</p>
<p>Table 9.12 (continued)
</p>
<p>Theory Action
</p>
<p>Validating and interpreting the cluster solution
</p>
<p>Stability Re-run the analysis using different clustering procedures,
</p>
<p>linkage algorithms or distance measures. For example,
</p>
<p>generate a cluster membership variable and use this grouping
</p>
<p>as starting partition for k-means clustering.
</p>
<p>cluster generate cluster_wl&frac14; groups(3), name
(wards_linkage) ties(error)
</p>
<p>cluster kmeans e1 e5 e9 e21 e22, k(3) measure
(L2squared) name (kmeans) start(group
(cluster_wl))
</p>
<p>Examine the overlap in the clustering solutions. If more than
</p>
<p>20% of the cluster affiliations change from one technique to
</p>
<p>the other, you should reconsider the set-up.
</p>
<p>tabulate cluster_wl kmeans
</p>
<p>Change the order of objects in the dataset (hierarchical
</p>
<p>clustering only).
</p>
<p>Differentiation of the data Compare the cluster centroids across the different clusters for
</p>
<p>significant differences.
</p>
<p>mean e1 e5 e9 e21 e22, over(cluster_wl)
</p>
<p>If possible, assess the solution&rsquo;s criterion validity.
</p>
<p>Profiling Identify observable variables (e.g., demographics) that best
</p>
<p>mirror the partition of the objects based on the clustering
</p>
<p>variables.
</p>
<p>tabulate cluster_wl flight_purpose, chi2 V
</p>
<p>Interpretating of the cluster
</p>
<p>solution
</p>
<p>Identify names or labels for each cluster and characterize each
</p>
<p>cluster by means of observable variables.
</p>
<p>9.4 Example 349</p>
<p/>
</div>
<div class="page"><p/>
<p>9.4.1 Select the Clustering Variables
</p>
<p>The Oddjob Airways dataset ( Web Appendix ! Downloads) offers several
variables for segmenting its customer base. Our analysis draws on the following
</p>
<p>set of variables, which we consider promising for identifying distinct segments
</p>
<p>based on customers&rsquo; expectations regarding the airline&rsquo;s service quality (variable
</p>
<p>names in parentheses):
</p>
<p>&ndash; . . . with Oddjob Airways you will arrive on time (e1),
&ndash; . . . Oddjob Airways provides you with a very pleasant travel experience (e5),
&ndash; . . . Oddjob Airways gives you a sense of safety (e9),
&ndash; . . . Oddjob Airways makes traveling uncomplicated (e21), and
&ndash; . . . Oddjob Airways provides you with interesting on-board entertainment,
</p>
<p>service, and information sources (e22).
</p>
<p>With five clustering variables, our analysis meets even the most conservative
</p>
<p>rule-of-thumb regarding minimum sample size requirements. Specifically,
</p>
<p>according to Dolnicar et al. (2016), the cluster analysis should draw on 100 times
</p>
<p>the number of clustering variables to optimize cluster recovery. As our sample size
</p>
<p>of 1,065 is clearly higher than 5�100&frac14; 500, we can proceed with the analysis. Note,
however, that the actual sample size used in the analysis may be substantially lower
</p>
<p>when using casewise deletion. This also applies to our analysis, which ultimately
</p>
<p>draws on 969 observations (i.e., after casewise deletion).
</p>
<p>To begin with, it is good practice to examine a graphical display of the data.
</p>
<p>With multivariate data such as ours, the best way to visualize the data is by means of
</p>
<p>a scatterplot matrix (see Chaps. 5 and 7). To generate a scatterplot matrix, go to ►
</p>
<p>Graphics ► Scatterplot matrix and enter the variables e1, e5, e9, e21, and e22 into
the Variables box (Fig. 9.16). To ensure that the variable labels fit the diagonal
</p>
<p>boxes of the scatterplot, enter 0.9 next to Scale text. Because there are so many
</p>
<p>observations in the dataset, we choose a different marker symbol. To do so, click on
</p>
<p>Marker properties and select Point next to Symbol. Confirm by clicking on
</p>
<p>Accept, followed by OK. Stata will generate a scatterplot similar to the one
</p>
<p>shown in Fig. 9.17.
</p>
<p>The resulting scatterplots do not suggest a clear pattern except that most
</p>
<p>observations are in the moderate to high range. But the scatterplots also assure us
</p>
<p>that all observations fall into the 0 to 100 range. Even though some observations
</p>
<p>with low values in (combinations of) expectation variables can be considered as
</p>
<p>extreme, we do not delete them, as they occur naturally in the dataset (see Chap. 5).
</p>
<p>In a further check, we examine the variable correlations by clicking on ►
</p>
<p>Statistics ► Summaries, tables and tests ► Summary and descriptive statistics ►
</p>
<p>Pairwise correlations. Next, enter all variables into the Variables box (Fig. 9.18).
</p>
<p>Click on OK and Stata will display the results (Table 9.13).
</p>
<p>The results show that collinearity is not at a critical level. The variables e1 and
e21 show the highest correlation of 0.6132, which is clearly lower than the 0.90
threshold. We can therefore proceed with the analysis, using all five clustering
</p>
<p>variables.
</p>
<p>350 9 Cluster Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>Note that all the variables used in our analysis are metric and are measured on a
</p>
<p>scale from 0 to 100. However, if the variables were measured in different units with
</p>
<p>Fig. 9.16 Scatterplot matrix dialog box
</p>
<p>Fig. 9.17 Scatterplot matrix
</p>
<p>9.4 Example 351</p>
<p/>
</div>
<div class="page"><p/>
<p>different variances, we would need to standardize them in order to avoid the
</p>
<p>variables with the highest variances dominating the analysis. In Box 9.3, we explain
</p>
<p>how to standardize the data in Stata.
</p>
<p>Box 9.3 Standardization in Stata
</p>
<p>Stata&rsquo;s menu-based dialog boxes only allow for z-standardization (see
Chap. 5), which you can access via ► Data ► Create or change data ►
</p>
<p>Create new variable (extended). In cluster analysis, however, the clustering
</p>
<p>variables should be standardized to a scale of 0 to 1. There is no menu option
</p>
<p>(continued)
</p>
<p>Fig. 9.18 Pairwise correlations dialog box
</p>
<p>Table 9.13 Pairwise correlations
</p>
<p>pwcorr e1 e5 e9 e21 e22
</p>
<p>|       e1       e5       e9      e21      e22
-------------+---------------------------------------------
</p>
<p>e1 |   1.0000 
e5 |   0.5151   1.0000 
e9 |   0.5330   0.5255   1.0000 
</p>
<p>e21 |   0.6132   0.5742   0.5221   1.0000 
e22 |   0.3700   0.5303   0.4167 0.4246   1.0000 
</p>
<p>352 9 Cluster Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>Box 9.3 (continued)
</p>
<p>or command to do this directly in Stata, but we can improvise by using the
</p>
<p>summarize command. When using this command, Stata saves the mini-
mum and maximum values of a certain variable as scalars. Stata refers to
</p>
<p>these scalars as r(max) and r(min), which we can use to calculate new
versions of the variables, standardized to a scale from 0 to 1. To run this
</p>
<p>procedure for the variable e1 type in the following:
</p>
<p>summarize e1
</p>
<p>Variable | Obs Mean Std. Dev. Min Max
</p>
<p>-----------+---&ndash;---------------------------------------------
</p>
<p>e1 | 1,038 86.08189 19.3953 1 100
</p>
<p>We can let Stata display the results of the summarize command by
typing return list in the command window.
</p>
<p>scalars:
</p>
<p>r(N) = 1038
</p>
<p>r(sum_w) = 1038
</p>
<p>r(mean) = 86.08188824662813
</p>
<p>r(Var) = 376.1774729981067
</p>
<p>r(sd) = 19.395295125316
</p>
<p>r(min) = 1
</p>
<p>r(max) = 100
</p>
<p>r(sum) = 89353
</p>
<p>Next, we compute a new variable called e1_rstd, which uses the minimum
and maximum values as input to compute a standardized version of e1 (see
Chap. 5 for the formula).
</p>
<p>gen e1_rsdt =.
</p>
<p>replace e1_rsdt = (e1- r(min)) / (r(max)-r(min))
</p>
<p>Similar commands create standardized versions of the other clustering
</p>
<p>variables.
</p>
<p>9.4.2 Select the Clustering Procedure and Measure of Similarity or
Dissimilarity
</p>
<p>To initiate hierarchical clustering, go to ► Statistics ► Multivariate analysis ►
</p>
<p>Cluster analysis ► Cluster data. The resulting menu offers a range of hierarchical
</p>
<p>and partitioning methods from which to choose. Because of its versatility and
</p>
<p>9.4 Example 353</p>
<p/>
</div>
<div class="page"><p/>
<p>general performance, we choose Ward&rsquo;s linkage. Clicking on the corresponding
</p>
<p>menu option opens a dialog box similar to Fig. 9.19.
</p>
<p>Enter the variables e1, e5, e9, e21, and e22 into the Variables box and select the
squared Euclidean distance (L2squared or squared Euclidean) as the (dis)simi-
</p>
<p>larity measure. Finally, specify a name, such as wards_linkage, in the Name this
cluster analysis box. Next, click on OK.
</p>
<p>Nothing seems to happen (aside from the following command, which gets
</p>
<p>issued: cluster wardslinkage e1 e5 e9 e21 e22, measure
(L2squared) name(wards_linkage)), although you might notice that our
dataset now contains three additional variables called wards_linkage_id,
wards_linkage_ord, and wards_linkage_hgt. While these new variables are not
directly of interest, Stata uses them as input to draw the dendrogram.
</p>
<p>9.4.3 Decide on the Number of Clusters
</p>
<p>To decide on the number of clusters, we start by examining the dendrogram. To
</p>
<p>display the dendrogram, go to ► Statistics ► Multivariate analysis ► Cluster
</p>
<p>analysis► Postclustering► Dendrogram. Given the great number of observations,
</p>
<p>we need to limit the display of the dendrogram (see Fig. 9.20). To do so, select Plot
</p>
<p>top branches only in the Branches menu. By specifying 10 next to Number of
</p>
<p>branches, we limit the view of the top 10 branches of the dendrogram, which Stata
</p>
<p>labels G1 to G10. When selecting Display number of observations for each
branch, Stata will display the number of observations in each of the ten groups.
</p>
<p>Fig. 9.19 Hierarchical clustering with Ward&rsquo;s linkage dialog box
</p>
<p>354 9 Cluster Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>After clicking on OK, Stata will open a new window with the dendrogram
</p>
<p>(Fig. 9.21).
</p>
<p>Reading the dendrogram from the bottom to the top, we see that clusters G1 to
G6 are merged in quick succession. Clusters G8 to G10 are merged at about the
</p>
<p>Fig. 9.20 Dendrogram dialog box
</p>
<p>Fig. 9.21 Dendrogram
</p>
<p>9.4 Example 355</p>
<p/>
</div>
<div class="page"><p/>
<p>same distance, while G7 initially remains separate. These three clusters remain
stable until, at a much higher distance, G7 merges with the first cluster. This result
clearly suggests a three-cluster solution, because reducing the cluster number to two
</p>
<p>requires merging the first cluster with G7, which is quite dissimilar to it. Increasing
the number of clusters appears unreasonable, as many mergers take place at about
</p>
<p>the same distance.
</p>
<p>The VRC and Duda-Hart indices allow us to further explore the number of
</p>
<p>clusters to extract. To request these measures in Stata, go to ► Statistics ►
</p>
<p>Multivariate analysis ► Cluster analysis ► Postclustering ► Cluster analysis
</p>
<p>stopping rules. In the dialog box that follows (Fig. 9.22), select Duda/Hart Je(2)/
</p>
<p>J2(1) index and tick the box next to Compute for groups up to. As we would like
</p>
<p>to consider a maximum number of ten clusters, enter 10 into the corresponding box
</p>
<p>and click on OK. Before interpreting the output, continue this procedure, but, this
</p>
<p>time, choose the Calinski/Harabasz pseudo F-index to request the VRC. Recall
</p>
<p>that we can also compute the VRC-based ωk statistic. As this statistic requires the
</p>
<p>VRCk+1 value as input, we need to enter 11 underCompute for groups up to. Next,
</p>
<p>click on OK. Tables 9.14 and 9.15 show the postclustering outputs.
</p>
<p>Looking at Table 9.14, we see that the Je(2)/Je(1) index yields the highest value
</p>
<p>for three clusters (0.8146), followed by a six-cluster solution (0.8111). Conversely,
</p>
<p>the lowest pseudo T-squared value (37.31) occurs for ten clusters. Looking at the
</p>
<p>VRC values in Table 9.15, we see that the index decreases with a greater number of
</p>
<p>clusters.
</p>
<p>To calculate the ωk criterion, we can use a file that has been specially
</p>
<p>programmed for this, called chomega.ado Web Appendix (! Downloads). This
</p>
<p>Fig. 9.22 Postclustering
</p>
<p>dialog box
</p>
<p>356 9 Cluster Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>file should first be run before we can use it, just like the add-on modules discussed in
</p>
<p>Chap. 5, Section 5.8.2. To do this, download the chomega.ado file and drag it into
</p>
<p>the Stata command box, and add do &ldquo; before and &ldquo; after the text that appears in the
</p>
<p>Table 9.14 Duda-Hart indices
</p>
<p>cluster stop wards_linkage, rule(duda) groups(1/10)
</p>
<p>+-----------------------------------------+
|             |         Duda/Hart         |
|  Number of  |             |  pseudo     |
|  clusters   | Je(2)/Je(1) |  T-squared  |
|-------------+-------------+-------------|
|      1      |   0.6955    |   423.29    |
|      2      |   0.6783    |   356.69    |
|      3      |   0.8146    |   100.83    |
|      4      |   0.7808    |    59.79    |
|      5      |   0.7785    |    58.59    |
|      6      |   0.8111    |    58.94    |
|      7      |   0.6652    |    47.82    |
|      8      |   0.7080    |    64.34    |
|      9      |   0.7127    |    75.79    |
|     10      |   0.7501    |    37.31    |
+-----------------------------------------+
</p>
<p>Table 9.15 VRC
</p>
<p>cluster stop wards_linkage, rule(calinski) groups(2/10)
</p>
<p>+---------------------------+
|             |  Calinski/  |
|  Number of  |  Harabasz   |
|  clusters   |  pseudo-F   |
|-------------+-------------|
|      2      |   423.29    |
|      3      |   406.02    |
|      4      |   335.05    |
|      5      |   305.39    |
|      6      |   285.26    |
|      7      |   273.24    |
|      8      |   263.12    |
|      9      |   249.61    |
|     10      |   239.73    |
|     11      |   233.17    |
+---------------------------+
</p>
<p>Table 9.16 ωk statistic
</p>
<p>chomega
</p>
<p>omega_3  is -53.691
omega_4  is  41.300
omega_5  is   9.534
omega_6  is   8.110
omega_7  is   1.899
omega_8  is  -3.394
omega_9  is   3.636
Minimum value of omega: -53.691 at 3 clusters
</p>
<p>9.4 Example 357</p>
<p/>
</div>
<div class="page"><p/>
<p>Stata Command window (see Chap. 5). Then click on enter. Then you should type
chomega. Note that this only works if you have first performed a cluster analysis.
</p>
<p>The output is included in Table 9.16. We find that the smallest ωk value of
</p>
<p>�53.691 occurs for a three-cluster solution. The smallest value is shown at the top
and bottom of Table 9.16. Note again that since ωk requires VRCk-1 as input, the
</p>
<p>statistic is only defined for three or more clusters. Taken jointly, our analyses of the
</p>
<p>dendrogram, the Duda-Hart indices, and the VRC clearly suggest a three-cluster
</p>
<p>solution.
</p>
<p>9.4.4 Validate and Interpret the Clustering Solution
</p>
<p>In the next step, we create a cluster membership variable, which indicates the
</p>
<p>cluster to which each observation belongs. To do so, go to ► Statistics ► Multi-
</p>
<p>variate analysis ► Cluster analysis ► Postclustering ► Generate summary
</p>
<p>variables from cluster. In the dialog box that opens (Fig. 9.23), enter a name,
</p>
<p>such as cluster_wl, for the variable to be created in the Generate variable(s)
box. In the dropdown list From cluster analysis, we can choose on which previ-
</p>
<p>ously run cluster analysis the cluster membership variable should be based. As this
</p>
<p>is our first analysis, we can only select wards_linkage. Finally, specify the number
of clusters to extract (3) under Number of groups to form and proceed by clicking
</p>
<p>OK.
</p>
<p>Stata generates a new variable cluster_wl, which indicates the group to which
each observation belongs. We can now use this variable to describe and profile the
</p>
<p>clusters. In a first step, we would like to tabulate the number of observations in each
</p>
<p>Fig. 9.23 Summary variables dialog box
</p>
<p>358 9 Cluster Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>cluster by going to► Statistics► Summary, tables, and tests► Frequency tables►
</p>
<p>One-way table. Simply select cluster_wl in the drop-down menu under Categorical
variable, tick the box next to Treat missing values like other values and click on
</p>
<p>OK. The output in Table 9.17 shows that the cluster analysis assigned
</p>
<p>969 observations to the three segments; 96 observations are not assigned to any
</p>
<p>segment due to missing values. The first cluster comprises 445 observations, the
</p>
<p>second cluster 309 observations, and the third cluster 215 observations.
</p>
<p>Next, we would like to compute the centroids of our clustering variables. To do
</p>
<p>so, go to ► Statistics ► Summaries, tables, and tests ► Other tables ► Compact
</p>
<p>table of summary statistics and enter e1 e5 e9 e21 e22 into theVariables box. Next,
click on Group statistics by variable and select cluster_wl from the list. Under
Statistics to display, tick the first box and select Mean, followed by OK.
</p>
<p>Table 9.18 shows the resulting output.
</p>
<p>Comparing the variable means across the three clusters, we find that respondents
</p>
<p>in the first cluster strongly emphasize punctuality (e1), while comfort (e5) and,
particularly, entertainment aspects (e22) are less important. Respondents in the
second cluster have extremely high expectations regarding all five performance
</p>
<p>features, as evidenced in average values well above 90. Finally, respondents in the
</p>
<p>third cluster do not express high expectations in general, except in terms of security
</p>
<p>(e9). Based on these results, we could label the first cluster &ldquo;on-time is enough,&rdquo; the
</p>
<p>Table 9.17 Cluster sizes
</p>
<p>tabulate cluster_wl, missing
</p>
<p>cluster_wl |      Freq.     Percent        Cum.
------------+-----------------------------------
</p>
<p>1 |        445       41.78       41.78
2 |        309       29.01       70.80
3 |        215       20.19       90.99
. |         96        9.01      100.00
</p>
<p>------------+-----------------------------------
Total |      1,065      100.00
</p>
<p>Table 9.18 Comparison of means
</p>
<p>tabstat e1 e5 e9 e21 e22, statistics( mean ) by(cluster_wl)
</p>
<p>Summary statistics: mean
by categories of: cluster_wl 
</p>
<p>cluster_wl |        e1        e5        e9       e21       e22
-----------+--------------------------------------------------
</p>
<p>1 |  92.39326  75.50562  89.74607   81.7191  62.33933
2 |   97.1068  95.54693  97.50809  96.63754  92.84466
3 |   59.4186  58.28372  71.62791  56.72558  58.03256
</p>
<p>-----------+--------------------------------------------------
Total |  86.57998  78.07534  88.20124  80.93086  71.11146
</p>
<p>--------------------------------------------------------------
</p>
<p>9.4 Example 359</p>
<p/>
</div>
<div class="page"><p/>
<p>second cluster &ldquo;the demanding traveler,&rdquo; and the third cluster &ldquo;no thrills.&rdquo; We could
</p>
<p>further check whether these differences in means are significant by using a one-way
</p>
<p>ANOVA as described in Chap. 6.
</p>
<p>In a further step, we can try to profile the clusters using sociodemographic
</p>
<p>variables. Specifically, we use crosstabs (see Chap. 5) to contrast our clustering
</p>
<p>with the variable flight_purpose, which indicates whether the respondents primarily
fly for business purposes ( flight_purpose&frac14;1) or private purposes
( flight_purpose&frac14;2). To do so, click on ► Statistics ► Summaries, tables, and
tests ► Frequency tables ► Two-way table with measures of association. In the
</p>
<p>dialog box that opens, enter cluster_wl into the Row variable box and
flight_purpose into the Column variable box. Select Pearson&rsquo;s chi-squared and
Cramer&rsquo;s V under Test statistics and click on OK. The results in Table 9.19 show
</p>
<p>that the majority of respondents in the first and third cluster are business travelers,
</p>
<p>whereas the second cluster primarily comprises private travelers. The χ2-test
</p>
<p>statistic (Pr &frac14; 0.004) indicates a significant relationship between these two
variables. However, the strength of the variables&rsquo; association is rather small, as
</p>
<p>indicated by the Cramer&rsquo;s V of 0.1065.
</p>
<p>The Oddjob Airways dataset offers various other variables such as age, gender,
or status, which could be used to further profile the cluster solution. However,
instead of testing these variables&rsquo; efficacy step-by-step, we proceed and assess the
</p>
<p>solution&rsquo;s stability by running an alternative clustering procedure on the data.
</p>
<p>Specifically, we apply the k-means method, using the grouping from the Ward&rsquo;s
linkage analysis as input for the starting partition. To do so, go to:
</p>
<p>► Statistics ► Multivariate statistics ► Cluster analysis ► Cluster data ►
</p>
<p>Kmeans. In the dialog box that opens, enter e1, e5, e9, e21, and e22 into the
Variables box, choose 3 clusters, and select L2squared or squared Euclidean
</p>
<p>under (Dis)similarity measure (Fig. 9.24). Under Name this cluster analysis,
</p>
<p>make sure that you specify an intuitive name, such as kmeans. When clicking on the
Options tab, we can choose between different options of how k-means should
derive a starting partition for the analysis. Since we want to use the clustering from
</p>
<p>Table 9.19 Crosstab
</p>
<p>tabulate cluster_wl flight_purpose, chi2 V
</p>
<p>|  Do you normaly fly
|    for business or
|   leisure purposes?
</p>
<p>cluster_wl |  Business    Leisure |     Total
-----------+----------------------+----------
</p>
<p>1 |       239        206 |       445 
2 |       130        179 |       309 
3 |       114        101 |       215 
</p>
<p>-----------+----------------------+----------
Total |       483        486 |       969 
</p>
<p>Pearson chi2(2) =  10.9943   Pr = 0.004
Cram&eacute;r's V =   0.1065
</p>
<p>360 9 Cluster Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>Fig. 9.24 k-means dialog box
</p>
<p>Fig. 9.25 Options in the k-means dialog box
</p>
<p>9.4 Example 361</p>
<p/>
</div>
<div class="page"><p/>
<p>our previous analysis by using Ward&rsquo;s linkage, we need to choose the last option
</p>
<p>and select the cluster_wl variable in the corresponding drop-down menu (Fig. 9.25).
Now click on OK.
</p>
<p>Stata only issues a command (cluster kmeans e1 e5 e9 e21 e22, k
(3) measure(L2squared) name(kmeans) start(group
(cluster_wl))) but also adds a new variable kmeans to the dataset, which
indicates each observation&rsquo;s cluster affiliation, analogous to the cluster_wl variable
for Ward&rsquo;s linkage. To explore the overlap in the two cluster solutions, we can
</p>
<p>contrast the results using crosstabs. To do so, go to► Statistics► Summary, tables,
</p>
<p>and tests ► Frequency tables ► Two-way table with measures of association and
</p>
<p>select cluster_wl under Row variable and kmeans under Column variable. After
clicking on OK, Stata will produce an output similar to Table 9.20.
</p>
<p>The results show that there is a strong degree of overlap between the two cluster
</p>
<p>analyses. For example, 307 observations that fall into the second cluster in the
</p>
<p>Ward&rsquo;s linkage analysis also fall into this cluster in the k-means clustering. Only
two observations from this cluster appear in the first k-means cluster. The diver-
gence in the clustering solutions is somewhat higher in the third and, especially, in
</p>
<p>the first cluster, but still low in absolute terms. Overall, the two analyses have an
</p>
<p>overlap of (320 + 307 + 169)/969 &frac14; 82.15%, which is very satisfactory as less than
20% of all observations appear in a different cluster when using k-means.
</p>
<p>This analysis concludes our cluster analysis. However, we could further explore
</p>
<p>the solution&rsquo;s stability by running other linkage algorithms, such as centroid or
</p>
<p>complete linkage, on the data. Similarly, we could use different (dis)similarity
</p>
<p>measures and assess their impact on the results. So go ahead and explore these
</p>
<p>options yourself!
</p>
<p>9.5 Oh, James! (Case Study)
</p>
<p>The James Bond movie series is one of the success stories of filmmaking. The
</p>
<p>movies are the longest continually running and the third-highest-grossing film
</p>
<p>series to date, which started in 1962 with Dr. No, starring Sean Connery as James
</p>
<p>Bond. As of 2016, there have been 24 movies with six actors having played James
</p>
<p>Table 9.20 Comparison of clustering results
</p>
<p>tabulate cluster_wl kmeans
</p>
<p>|              kmeans
cluster_wl |         1          2          3 |     Total
-----------+---------------------------------+----------
</p>
<p>1 |       320        107         18 |       445 
2 |         2        307          0 |       309 
3 |        36         10        169 |       215 
</p>
<p>-----------+---------------------------------+----------
Total |       358        424        187 |       969 
</p>
<p>362 9 Cluster Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>Bond. Interested in the factors that contributed to this running success, you decide
</p>
<p>to investigate the different James Bond movies&rsquo; characteristics. Specifically, you
</p>
<p>want to find out whether the movies can be grouped into clusters, which differ in
</p>
<p>their box-office revenues. To do so, you draw on Internet Movie Database (www.
</p>
<p>imdb.com) and collect data on all 24 movies based on the following variables
</p>
<p>(variable names in parentheses):
</p>
<p>&ndash; Title. (title)
&ndash; Actor playing James Bond. (actor)
&ndash; Year of publication. (year)
&ndash; Budget in USD, adjusted for inflation. (budget)
&ndash; Box-office revenues in the USA, adjusted for inflation. (gross_usa)
&ndash; Box-office revenues worldwide, adjusted for inflation. (gross_worldwide)
&ndash; Runtime in minutes. (runtime)
&ndash; Native country of the villain actor. (villain_country)
&ndash; Native country of the bondgirl. (bondgirl_country)
&ndash; Haircolor of the bondgirl. (bondgirl_hair)
</p>
<p>Use the dataset jamesbond.dta ( Web Appendix ! Downloads) to run a cluster
analysis&mdash;despite potential objections regarding the sample size. Answer the fol-
</p>
<p>lowing questions:
</p>
<p>1. Which clustering variables would you choose in light of the study objective,
</p>
<p>their levels of measurement, and correlations?
</p>
<p>2. Given the levels of measurement, which clustering method would you prefer?
</p>
<p>Carry out a cluster analysis using this procedure.
</p>
<p>3. Interpret and profile the obtained clusters by examining cluster centroids. Com-
</p>
<p>pare the differences across clusters on the box-office revenue variables.
</p>
<p>4. Use a different clustering method to test the stability of your results.
</p>
<p>9.6 Review Questions
</p>
<p>1. In your own words, explain the objective and basic concept of cluster analysis.
</p>
<p>2. What are the differences between hierarchical and partitioning methods? When
</p>
<p>do we use hierarchical or partitioning methods?
</p>
<p>3. Repeat the manual calculations of the hierarchical clustering procedure from the
</p>
<p>beginning of the chapter, but use complete linkage as the clustering method.
</p>
<p>Compare the results with those of the single linkage method.
</p>
<p>4. Explain the different options to decide on the number of clusters to extract from
</p>
<p>the data? Should you rather on statistical measures or rather on practical
</p>
<p>reasoning?
</p>
<p>5. Run the k-means analysis on the Oddjob Airways data again (oddjob.dta, Web
Appendix ! Downloads). Assume a three-cluster solution and try the different
</p>
<p>9.6 Review Questions 363</p>
<p/>
<div class="annotation"><a href="http://www.imdb.com">http://www.imdb.com</a></div>
<div class="annotation"><a href="http://www.imdb.com">http://www.imdb.com</a></div>
</div>
<div class="page"><p/>
<p>options for obtaining a starting partition that Stata offers. Compare the results
</p>
<p>with those obtained by the hierarchical clustering.
</p>
<p>6. Which clustering variables could be used to segment:
</p>
<p>&ndash; The market for smartphones?
</p>
<p>&ndash; The market for chocolate?
</p>
<p>&ndash; The market for car insurances?
</p>
<p>9.7 Further Readings
</p>
<p>Bottomley, P., &amp; Nairn, A. (2004). Blinded by science: The managerial
</p>
<p>consequences of inadequately validated cluster analysis solutions. International
Journal of Market Research, 46(2), 171&ndash;187.
</p>
<p>In this article, the authors investigate if managers could distinguish between cluster
analysis outputs derived from real-world and random data. They show that some
managers feel able to assign meaning to random data devoid of a meaningful
structure, and even feel confident formulating entire marketing strategies from
cluster analysis solutions generated from such data. As such, the authors provide
a reminder of the importance of validating clustering solutions with caution.
</p>
<p>Dolnicar, S., Gr&uuml;n, B., &amp; Leisch, F. (2016). Increasing sample size compensates for
</p>
<p>data problems in segmentation studies. Journal of Business Research, 69(2),
992&ndash;999.
</p>
<p>Using artificial datasets of known structure, the authors examine the effects of data
problems such as respondent fatigue, sampling error, and redundant items on
segment recovery. The study nicely shows how insufficient sample size of the
segmentation base can have serious negative consequences on segment recovery
and that increasing the sample size represents a simple measure to compensate
for the detrimental effects caused by poor data quality.
</p>
<p>Punj, G., &amp; Stewart, D. W. (1983). Cluster analysis in marketing research: Review
</p>
<p>and suggestions for application. Journal of Marketing Research, 20(2),134&ndash;148.
In this seminal article, the authors discuss several issues in applications of cluster
</p>
<p>analysis and provide further theoretical discussion of the concepts and rules of
thumb that we included in this chapter.
</p>
<p>Romesburg, C. (2004). Cluster analysis for researchers. Morrisville: Lulu Press.
Charles Romesburg nicely illustrates the most frequently used methods of hierar-
</p>
<p>chical cluster analysis for readers with limited backgrounds in mathematics and
statistics.
</p>
<p>Wedel, M., &amp; Kamakura, W. A. (2000). Market segmentation: Conceptual and
methodological foundations (2nd ed.). Boston: Kluwer Academic.
</p>
<p>This book is a clear, readable, and interesting presentation of applied market
segmentation techniques. The authors explain the theoretical concepts of recent
analysis techniques and provide sample applications. Probably the most com-
prehensive text in the market.
</p>
<p>364 9 Cluster Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>References
</p>
<p>Anderberg, M. R. (1973). Cluster analysis for applications. New York: Academic.
Arabie, P., &amp; Hubert, L. (1994). Cluster analysis in marketing research. In R. P. Bagozzi (Ed.),
</p>
<p>Advanced methods in marketing research (pp. 160&ndash;189). Cambridge: Basil Blackwell &amp; Mott,
Ltd..
</p>
<p>Arthur, D., &amp; Vassilvitskii, S. (2007). k-means++: The advantages of careful seeding. In
</p>
<p>Proceedings of the eighteenth annual ACM-SIAM symposium on discrete algorithms
(pp. 1027&ndash;1035). Philadelphia: Society for Industrial and Applied Mathematics.
</p>
<p>Becker, J.-M., Ringle, C. M., Sarstedt, M., &amp; V&euro;olckner, F. (2015). How collinearity affects mixture
</p>
<p>regression results. Marketing Letters, 26(4), 643&ndash;659.
Caliński, T., &amp; Harabasz, J. (1974). A dendrite method for cluster analysis. Communications in
</p>
<p>Statistics&mdash;Theory and Methods, 3(1), 1&ndash;27.
Dolnicar, S. (2003). Using cluster analysis for market segmentation&mdash;typical misconceptions,
</p>
<p>established methodological weaknesses and some recommendations for improvement.
</p>
<p>Australasian Journal of Market Research, 11(2), 5&ndash;12.
Dolnicar, S., &amp; Gr&uuml;n, B. (2009). Challenging &ldquo;factor-cluster segmentation&rdquo;. Journal of Travel
</p>
<p>Research, 47(1), 63&ndash;71.
Dolnicar, S., &amp; Lazarevski, K. (2009). Methodological reasons for the theory/practice divide in
</p>
<p>market segmentation. Journal of Marketing Management, 25(3&ndash;4), 357&ndash;373.
Dolnicar, S., Gr&uuml;n, B., Leisch, F., &amp; Schmidt, F. (2014). Required sample sizes for data-driven
</p>
<p>market segmentation analyses in tourism. Journal of Travel Research, 53(3), 296&ndash;306.
Dolnicar, S., Gr&uuml;n, B., &amp; Leisch, F. (2016). Increasing sample size compensates for data problems
</p>
<p>in segmentation studies. Journal of Business Research, 69(2), 992&ndash;999.
Duda, R. O., &amp; Hart, P. E. (1973). Pattern classification. Hoboken: Wiley.
Duda, R. O., Hart, P. E., &amp; Stork, D. G. (2001). Pattern classification (2nd ed.). Hoboken: Wiley.
Everitt, B. S., &amp; Rabe-Hesketh, S. (2006). Handbook of statistical analyses using Stata (4th ed.).
</p>
<p>Boca Raton: Chapman &amp; Hall/CRC.
</p>
<p>Formann, A. K. (1984). Die Latent-Class-Analyse: Einf&euro;uhrung in die Theorie und Anwendung.
Beltz: Weinheim.
</p>
<p>Gower, J. C. (1971). A general coefficient of similarity and some of its properties. Biometrics,
27(4), 857&ndash;871.
</p>
<p>Halpin, B. (2016). Cluster analysis stopping rules in Stata. University of Limerick. Department of
Sociology Working Paper Series, WP2016-01. http://ulsites.ul.ie/sociology/sites/default/files/
</p>
<p>wp2016-01.pdf
</p>
<p>Kaufman, L., &amp; Rousseeuw, P. J. (2005). Finding groups in data. An introduction to cluster
analysis. Hoboken: Wiley.
</p>
<p>Kotler, P., &amp; Keller, K. L. (2015).Marketing management (15th ed.). Upper Saddle River: Prentice
Hall.
</p>
<p>Milligan, G. W., &amp; Cooper, M. (1985). An examination of procedures for determining the number
</p>
<p>of clusters in a data set. Psychometrika, 50(2), 159&ndash;179.
Milligan, G. W., &amp; Cooper, M. (1988). A study of variable standardization. Journal of Classifica-
</p>
<p>tion, 5(2), 181&ndash;204.
Park, H.-S., &amp; Jun, C.-H. (2009). A simple and fast algorithm for K-medoids clustering. Expert
</p>
<p>Systems with Applications, 36(2), 3336&ndash;3341.
Punj, G., &amp; Stewart, D. W. (1983). Cluster analysis in marketing research: Review and suggestions
</p>
<p>for application. Journal of Marketing Research, 20(2), 134&ndash;148.
Qiu, W., &amp; Joe, H. (2009). Cluster generation: Random cluster generation (with specified degree
</p>
<p>of separation). R package version 1.2.7.
</p>
<p>Sheppard, A. (1996). The sequence of factor analysis and cluster analysis: Differences in segmen-
</p>
<p>tation and dimensionality through the use of raw and factor scores. Tourism Analysis, 1(1),
49&ndash;57.
</p>
<p>Tonks, D. G. (2009). Validity and the design of market segments. Journal of Marketing Manage-
ment, 25(3/4), 341&ndash;356.
</p>
<p>References 365</p>
<p/>
<div class="annotation"><a href="http://ulsites.ul.ie/sociology/sites/default/files/wp2016-01.pdf">http://ulsites.ul.ie/sociology/sites/default/files/wp2016-01.pdf</a></div>
<div class="annotation"><a href="http://ulsites.ul.ie/sociology/sites/default/files/wp2016-01.pdf">http://ulsites.ul.ie/sociology/sites/default/files/wp2016-01.pdf</a></div>
</div>
<div class="page"><p/>
<p>Wedel, M., &amp; Kamakura, W. A. (2000). Market segmentation: Conceptual and methodological
foundations (2nd ed.). Boston: Kluwer Academic.
</p>
<p>van der Kloot, W. A., Spaans, A. M. J., &amp; Heinser, W. J. (2005). Instability of hierarchical cluster
</p>
<p>analysis due to input order of the data: The PermuCLUSTER solution. Psychological Methods,
10(4), 468&ndash;476.
</p>
<p>Lilien, G. L., &amp; Rangaswamy, A. (2004). Marketing engineering. Computer-assisted marketing
analysis and planning (2nd ed.). Bloomington: Trafford Publishing.
</p>
<p>John H. R., Kayande, U., &amp; Stremersch, S. (2014). From academic research to marketing practice:
</p>
<p>Exploring the marketing science value chain. International Journal of Research in Marketing,
31(2), 127&ndash;140
</p>
<p>366 9 Cluster Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>Communicating the Results 10
</p>
<p>Keywords
</p>
<p>Ethics &bull; KISS principle &bull; Minto principle &bull; Pyramid structure for presentations &bull;
</p>
<p>Self-contained figure &bull; Self-contained table &bull; Visual aids
</p>
<p>Learning Objectives
After reading this chapter, you should understand:
</p>
<p>&ndash; Why communicating the results is a crucial element of every market research
</p>
<p>study.
</p>
<p>&ndash; The elements that should be included in a written research report and how to
</p>
<p>structure these elements.
</p>
<p>&ndash; How to communicate the findings in an oral presentation.
</p>
<p>&ndash; The ethical issues concerning communicating the report findings to the client.
</p>
<p>10.1 Introduction
</p>
<p>Communicating results is key in any market research project. This includes giving
</p>
<p>clear answers to the investigated research questions and recommending a course of
</p>
<p>action, where appropriate. The importance of communicating marketing research
</p>
<p>results should not be underestimated. Even if the research has been carefully
</p>
<p>conducted, the recipients will find it difficult to understand the implications of the
</p>
<p>results and to appreciate the study&rsquo;s quality if you spend too little time and energy
</p>
<p>on communicating these. Clear communication may also set the stage for follow-up
</p>
<p>research. If you communicate the findings effectively, the clients, who may know
</p>
<p>little about market research and may even be unfamiliar with the specific market
</p>
<p># Springer Nature Singapore Pte Ltd. 2018
E. Mooi et al., Market Research, Springer Texts in Business and Economics,
DOI 10.1007/978-981-10-5218-7_10
</p>
<p>367</p>
<p/>
</div>
<div class="page"><p/>
<p>research project, will understand them. Hence, the communication must be relevant
</p>
<p>for the addressed audience and provide a clear picture of the project.
</p>
<p>Market researchers usually present their findings in the form of an oral presenta-
</p>
<p>tion and written report. This report is the written evidence of the research effort and
</p>
<p>includes the details. Identifying the addressed audience is critical for both these
</p>
<p>points, as this determines how you can best communicate the findings. In this
</p>
<p>chapter, we discuss guidelines on how to effectively communicate research findings
</p>
<p>orally and in writing. We first discuss written reports before listing the basics of oral
</p>
<p>presentations. We also provide hints on how to acquire research follow-up. At the
</p>
<p>end of the chapter, we briefly review the ethical issues related to market research.
</p>
<p>10.2 Identify the Audience
</p>
<p>When providing reports (and presentations), you should keep the audience&rsquo;s
</p>
<p>characteristics and needs in mind and should tailor the report to their objectives.
</p>
<p>Imagine you are dealing with the marketing department of a company planning to
</p>
<p>launch a new product and needing to learn more about the potential customers&rsquo;
</p>
<p>buying behavior. The knowledge and level of interest in the study might differ
</p>
<p>greatly within the department. While the managers, who commissioned the study,
</p>
<p>are generally familiar with its objective and design, others, who might be unaware
</p>
<p>of the background (e.g., the marketing director or the sales staff), must be informed
</p>
<p>about the research to allow them to understand the research findings. When
</p>
<p>preparing the report, you should consider the following questions:
</p>
<p>&ndash; Who will read the report?
</p>
<p>&ndash; Why will they read the report?
</p>
<p>&ndash; Which parts of the report are of specific interest to them?
</p>
<p>&ndash; What do they already know about the study?
</p>
<p>&ndash; What information will be new to them?
</p>
<p>&ndash; What is the most important point for them to know after they have read the
</p>
<p>report?
</p>
<p>&ndash; What can be done with the research findings?
</p>
<p>These questions help you determine the level of detail that should be included in
</p>
<p>your report. Furthermore, they reveal information that requires specific focus
</p>
<p>during the project. Remember, a successful report meets its audience&rsquo;s needs!
</p>
<p>However, not everything that you consider appropriate for your audience is appro-
</p>
<p>priate. Nothing is worse than suggesting an idea that the audience finds unpalatable
</p>
<p>(e.g., saying that a specific senior management behavior or attitude is a major
</p>
<p>obstacle to success), or proposing a course of action that has been attempted before.
</p>
<p>Informal talks with the client are therefore vital before you present the results&mdash;
</p>
<p>never present findings formally without discussing them with the client first!
</p>
<p>Further, you need to ask clients about their expectations and the
</p>
<p>recommendations they think will be made early in the project. Why would clients
</p>
<p>368 10 Communicating the Results</p>
<p/>
</div>
<div class="page"><p/>
<p>spend $100,000 if you merely give them the answers they expect to get? Such
</p>
<p>discussions may help exceed clients&rsquo; expectations in a way that is useful to them.
</p>
<p>10.3 Guidelines for Written Reports
</p>
<p>You should always keep the people addressed in written report in mind. Decision
</p>
<p>makers are generally unfamiliar with statistical details, but would like to know how
</p>
<p>the findings can help them make decisions. You should therefore avoid research
</p>
<p>jargon and state the key insights clearly without omitting important facts. There are
</p>
<p>several major rules to consider when writing a report (Armstrong 2010; Churchill
</p>
<p>and Iacobucci 2009):
</p>
<p>1. The report must be complete; that is, it must contain all information that the
</p>
<p>reader needs in order to understand the research. Technical or ambiguous terms,
</p>
<p>as well as abbreviations and symbols, should be clearly defined and illustrated.
</p>
<p>Although you know what terms like heteroscedasticity or eigenvalue mean, the
</p>
<p>report reader probably won&rsquo;t! In addition, the report must provide enough detail
</p>
<p>to enable the reader to verify and replicate the findings if necessary. Bear in mind
</p>
<p>that the staff turnover in many organizations is high and that reports should
</p>
<p>therefore be stand-alone to allow those with little knowledge of the background
</p>
<p>to read and understand them.
</p>
<p>2. The report must be accurate. The readers will base their assessment of the entire
</p>
<p>research project&rsquo;s quality on the presented report. Consequently, the report must
</p>
<p>be well written. For example, grammar and spelling must be correct, no slang
</p>
<p>should be used, tables must be labeled correctly, and page numbers should be
</p>
<p>included. If there are small errors, the reader may believe they are due to your
</p>
<p>lack of care and generalize about your analysis! Therefore, proofread
</p>
<p>(a proofreader should preferably do this) to eliminate obvious errors. Lastly,
</p>
<p>objectivity is an important attribute of any report. This means that any subjective
</p>
<p>conclusions should be clearly stated as such.
</p>
<p>3. The report must be clear and language simple and concise:
</p>
<p>&ndash; Use short sentences.
</p>
<p>&ndash; Use simple and unambiguous words.
</p>
<p>&ndash; Use concrete examples to illustrate aspects of the research (e.g., unexpected
</p>
<p>findings). These can also be helpful if the audience has strong beliefs that are
</p>
<p>not consistent with your recommendation, which are often not implemented,
</p>
<p>because the client does not believe them.
</p>
<p>&ndash; Use the active voice to make the report easy to read and to help
</p>
<p>understanding.
</p>
<p>&ndash; Avoid negative words.
</p>
<p>&ndash; Use business language.
</p>
<p>&ndash; Avoid exclamation marks and do not use caps unnecessarily. Avoid the use of
</p>
<p>bold or italics for more than just a few words.
</p>
<p>10.3 Guidelines for Written Reports 369</p>
<p/>
</div>
<div class="page"><p/>
<p>4. Follow theKISS principle: Keep it short and simple! This principle requires the
report to be concise. And since it needs to be action-driven, the reader must
</p>
<p>immediately understand its purpose and the results, so start off with these. You
</p>
<p>should present the results clearly and simply. Important details can be shown in
</p>
<p>the appendix or appendices of the report, which should also not be overloaded
</p>
<p>with irrelevant material. In addition, keep in mind that each section&rsquo;s first
</p>
<p>sentences are the most important ones: They should summarize the main idea
</p>
<p>you want to convey in this section.
</p>
<p>5. The report must be structured logically. This applies to the general structure of
</p>
<p>the report (see Table 10.1) and to the line of argumentation in each section. Make
</p>
<p>sure you avoid style elements that may distract the reader:
</p>
<p>&ndash; Avoid cross-references. Having to search elsewhere for important results is
</p>
<p>disruptive. For example, do not put important tables in the appendix.
</p>
<p>&ndash; Use footnotes instead of endnotes and as few as possible.
</p>
<p>&ndash; The structure should not be too detailed. As a rule of thumb, you should avoid
</p>
<p>using more than four levels.
</p>
<p>&ndash; A new level must include at least two sections. For example, if there is a
</p>
<p>Sect. 3.1.1, there must also be a Sect. 3.1.2.
</p>
<p>10.4 Structure the Written Report
</p>
<p>When preparing a written report, a clear structure helps readers navigate it to
</p>
<p>quickly and easily find those elements that interest them. Although all reports
</p>
<p>differ, we include a suggested structure for a research report in Table 10.1.
</p>
<p>Table 10.1 Suggested
structure for a written
research report
</p>
<p>Title Page
</p>
<p>Executive Summary
</p>
<p>Table of Contents
</p>
<p>1. Introduction
</p>
<p>1.1 Problem definition
</p>
<p>1.2 Research objectives
</p>
<p>1.3 Research questions and/or hypothesesa
</p>
<p>2. Methodology
</p>
<p>2.1 Population, sampling method, and sample description
</p>
<p>2.2 Quantitative and qualitative methods used for data analysis
</p>
<p>3. Results
</p>
<p>4. Conclusions and Recommendations
</p>
<p>5. Limitations
</p>
<p>6. Appendix
aIn practice, the word hypotheses may be replaced by research
question(s) or proposition(s)
</p>
<p>370 10 Communicating the Results</p>
<p/>
</div>
<div class="page"><p/>
<p>10.4.1 Title Page
</p>
<p>The title page should state the title of the report, the name of the client who
</p>
<p>commissioned the report, the organization or researcher submitting it, and the
</p>
<p>date of release. The heading should clearly state the nature and scope of the report.
</p>
<p>It may simply describe the research (e.g., &ldquo;Survey of Mobile Phone Usage&rdquo;) or may
</p>
<p>outline the objectives of the study in the form of an action title (e.g., &ldquo;How to
</p>
<p>Increase the Adoption of Wearable Technologies&rdquo;).
</p>
<p>10.4.2 Executive Summary
</p>
<p>The executive summary should appear first and is essential, because it is often the
</p>
<p>only section that executives read. This summary helps set the expectations of those
</p>
<p>who read more. Hence, this section must be short to allow busy executives to read it
</p>
<p>and should give them the essence (findings and recommendations) of the research.
</p>
<p>As a rule of thumb, the executive summary should not exceed 150 words. It should
</p>
<p>contain key findings and recommendations, and help the reader understand the full
</p>
<p>study. The executive summary also requires more structure. A common way of
</p>
<p>giving structure is to tell a story. Begin with a description of the problem, thereafter
</p>
<p>introducing the issues that make this difficult or complicated and describing how
</p>
<p>these give rise to a number of questions. Finally, lead the reader through your line of
</p>
<p>reasoning to the answer:
</p>
<p>&ndash; Situation: Background information.
</p>
<p>&ndash; Difficulty or complication: A short window of opportunity; a change from the
</p>
<p>previously stable situation; lack of performance due to unknown causes (i.e., the
</p>
<p>reason for your research study).
</p>
<p>&ndash; Question: The scope and goal of your research study.
</p>
<p>&ndash; Answer: Your findings and conclusions (and if the client requires this, also your
</p>
<p>recommendations).
</p>
<p>10.4.3 Table of Contents
</p>
<p>The table of contents helps the reader locate specific aspects of the report. The table
</p>
<p>of contents should correspond to the main report headings. It should also include
</p>
<p>lists of tables and figures with page references.
</p>
<p>10.4 Structure the Written Report 371</p>
<p/>
</div>
<div class="page"><p/>
<p>10.4.4 Introduction
</p>
<p>This section should explain the project context to the reader. Questions to be
</p>
<p>answered include:
</p>
<p>&ndash; Why was the study undertaken?
</p>
<p>&ndash; What were the objectives and which key questions are answered?
</p>
<p>&ndash; Is the study related to other studies and, if so, which findings did they produce?
</p>
<p>&ndash; How is the report structured?
</p>
<p>Besides introducing the background and purpose of the research, the introduc-
</p>
<p>tion should briefly explain how the objectives and key questions are addressed. You
</p>
<p>should briefly mention the hypotheses or propositions tested during the research and
</p>
<p>how the research was approached (e.g., cluster analysis). You should ensure that
</p>
<p>Critical terms are defined. For example, aviation terms such as CASM (cost per
</p>
<p>available seat mile) require explanation. As a rule, the following three questions on
</p>
<p>the research should be answered in the introduction, but should be neither too
</p>
<p>detailed nor too technical:
</p>
<p>&ndash; What was done?
</p>
<p>&ndash; How was it done?
</p>
<p>&ndash; Why was it done?
</p>
<p>Keep in mind that the introduction should set the stage for the body of the report
</p>
<p>and the presentation of the results, but no more than this. You should only provide a
</p>
<p>detailed description of how you collected and analyzed the data in the next section
</p>
<p>of the report. Lastly, you should provide a brief summary of how the report is
</p>
<p>organized at the end of the introduction.
</p>
<p>10.4.5 Methodology
</p>
<p>In this section, you should describe the research procedure and the different
</p>
<p>(statistical) methods used to analyze the data. These must be presented precisely
</p>
<p>and coherently, allowing the reader to understand the analyses&rsquo; process and basic
</p>
<p>principles. Always keep your audience in mind! If the audience is familiar with
</p>
<p>research methodology, you can describe the procedures in detail and skip the basics.
</p>
<p>If the client has little knowledge of research methodology, you should introduce
</p>
<p>these briefly. If you have an audience of whom some have a little and others more
</p>
<p>knowledge, you might want to move the basics to an appendix.
</p>
<p>If not already stated in the previous section, you should define whether the study
</p>
<p>is exploratory, descriptive, or causal by nature and whether the results are based on
</p>
<p>primary or secondary data. If primary data are used, their source should be specified
</p>
<p>(e.g., observation or questionnaire). If a questionnaire was used, you should state
</p>
<p>whether it was administered by means of face-to-face interviews, telephone
</p>
<p>372 10 Communicating the Results</p>
<p/>
</div>
<div class="page"><p/>
<p>interviews, or through web or mail surveys. Also explain why you chose this
</p>
<p>specific method.
</p>
<p>The reader should also know the demographic or other relevant characteristics of
</p>
<p>the targeted survey population. This includes the geographical area, age group, and
</p>
<p>gender. While it is usually sufficient to describe the population in a few sentences,
</p>
<p>the sampling method needs more explanation: How was the sample selected?
</p>
<p>Which sampling frames were chosen (e.g., random, systematic, stratified)? In
</p>
<p>addition, information on the sample size, response rate, and sample characteristics
</p>
<p>is essential, as this indicates the results&rsquo; reliability and validity.
</p>
<p>You should include a copy of the actual instruments used, such as the question-
</p>
<p>naire or the interview guide, the data collection protocol, and the detailed statistical
</p>
<p>analyses of the results, in the appendix, or present them separately. Although these
</p>
<p>are important to fully understand the characteristics of the research project, includ-
</p>
<p>ing them in the main text would make reading the report more difficult.
</p>
<p>10.4.6 Results
</p>
<p>In this section, you need to present the findings and describe how they relate to a
</p>
<p>possible solution to the research problem and how they influence the
</p>
<p>recommendations. There are several ways of presenting the results logically. You
</p>
<p>could, for instance, use the different research objectives as a guideline to structure
</p>
<p>this section and then analyze them one by one.
</p>
<p>Another way is to first summarize the overall findings and then analyze them in
</p>
<p>relevant subgroups, such as the type of customer or geographical regions. Alterna-
</p>
<p>tively, you can classify the findings according to the data type or the research
</p>
<p>method if several were used. For example, you could first present the conclusions of
</p>
<p>the secondary data collection and then those derived from an analysis of the
</p>
<p>questionnaire.
</p>
<p>Use tables and graphs when presenting statistical data, as they make the report
</p>
<p>and the data more interesting. Tables and graphs also structure information, thus
</p>
<p>facilitating understanding. Graphs often allow the researcher to visually present
</p>
<p>complex data, which might not be possible when only using tables. However,
</p>
<p>graphs can also be misleading, as they may be adjusted to favor a specific viewpoint
</p>
<p>(see the next section for examples).
</p>
<p>Results are often presented in Excel or Word format. Fortunately, Stata has
</p>
<p>built-in capabilities to export its results to Excel by means of export excel
</p>
<p>and putexcel. You can also output results to Word, using a package called
</p>
<p>outreg2, which is a Stata add-on. The following videos offer step-by-step
</p>
<p>(continued)
</p>
<p>10.4 Structure the Written Report 373</p>
<p/>
</div>
<div class="page"><p/>
<p>introductions to exporting results to Excel (first mobile tag) and Word
</p>
<p>(second mobile tag):
</p>
<p>https://www.youtube.com/watch?v&frac14;MUQ3E8hIQZE
</p>
<p>https://www.youtube.com/watch?v&frac14;UemL7uYM6Lc
</p>
<p>10.4.6.1 Window Dressing with Graphs
While graphs have the advantage that they can present complex information in a
</p>
<p>way that is easily understandable, they can be used to mislead the reader. Experi-
</p>
<p>ence with generating and interpreting graphs will help you spot this. In this section,
</p>
<p>we show examples of how graphs can mislead. By shortening the x-axis in Fig. 10.1
</p>
<p>(i.e., removing the years 2003&ndash;2007), it suggests a growth in the units sold
</p>
<p>(Fig. 10.2).
</p>
<p>374 10 Communicating the Results</p>
<p/>
<div class="annotation"><a href="https://www.youtube.com/watch?v=MUQ3E8hIQZE">https://www.youtube.com/watch?v=MUQ3E8hIQZE</a></div>
<div class="annotation"><a href="https://www.youtube.com/watch?v=MUQ3E8hIQZE">https://www.youtube.com/watch?v=MUQ3E8hIQZE</a></div>
<div class="annotation"><a href="https://www.youtube.com/watch?v=UemL7uYM6Lc">https://www.youtube.com/watch?v=UemL7uYM6Lc</a></div>
<div class="annotation"><a href="https://www.youtube.com/watch?v=UemL7uYM6Lc">https://www.youtube.com/watch?v=UemL7uYM6Lc</a></div>
</div>
<div class="page"><p/>
<p>Likewise, we can modify the scale range (Fig. 10.2 vs. Fig. 10.3). Specifically,
</p>
<p>reducing the y-axis to a range from 68 to 100 units with 4 unit increments, suggests
</p>
<p>faster growth (Fig. 10.3). Another example is the &ldquo;floating&rdquo; y-axis (Fig. 10.4
</p>
<p>vs. Fig. 10.5), which increases the scale range along the y-axis from 0 to 190 with
</p>
<p>30-unit increments, thus making the drop in the number of units sold over the period
</p>
<p>2005 to 2008 less visually pronounced.
</p>
<p>20
03
</p>
<p>20
04
</p>
<p>20
05
</p>
<p>20
06
</p>
<p>20
07
</p>
<p>20
08
</p>
<p>20
09
</p>
<p>20
10
</p>
<p>20
11
</p>
<p>20
12
</p>
<p>20
13
</p>
<p>year
</p>
<p>U
n
it
s
 s
</p>
<p>o
ld
</p>
<p>6
0
</p>
<p>8
0
</p>
<p>1
0
0
</p>
<p>1
2
0
</p>
<p>1
4
0
</p>
<p>1
6
0
</p>
<p>Fig. 10.1 What year does the curve start? (I)
</p>
<p>20
08
</p>
<p>20
09
</p>
<p>20
10
</p>
<p>20
11
</p>
<p>20
12
</p>
<p>20
13
</p>
<p>year
</p>
<p>U
n
it
s
 s
</p>
<p>o
ld
</p>
<p>6
0
</p>
<p>7
0
</p>
<p>8
0
</p>
<p>9
0
</p>
<p>1
0
0
</p>
<p>1
1
0
</p>
<p>1
2
0
</p>
<p>Fig. 10.2 What year does the curve start? (II)
</p>
<p>10.4 Structure the Written Report 375</p>
<p/>
</div>
<div class="page"><p/>
<p>Data are often presented by means of three-dimensional figures, such as in
</p>
<p>Fig. 10.6. While these can be visually appealing, they are also subject to window-
</p>
<p>dressing. In this example, the lengths of all the edges were doubled to correspond to
</p>
<p>the 100% increase in turnover.
</p>
<p>However, the resulting area is not twice but four times as large as the original
</p>
<p>image, thus presenting a false picture of the increase. These are just some common
</p>
<p>examples; Huff&rsquo;s (1993) classical text offers more on this topic.
</p>
<p>20
08
</p>
<p>20
09
</p>
<p>20
10
</p>
<p>20
11
</p>
<p>20
12
</p>
<p>20
13
</p>
<p>year
</p>
<p>U
n
it
s
 s
</p>
<p>o
ld
</p>
<p>7
0
</p>
<p>7
4
</p>
<p>7
8
</p>
<p>8
2
</p>
<p>8
6
</p>
<p>9
0
</p>
<p>9
4
</p>
<p>9
8
</p>
<p>Fig. 10.3 Shortening the y-axis
</p>
<p>20
11
</p>
<p>20
10
</p>
<p>20
09
</p>
<p>20
08
</p>
<p>20
07
</p>
<p>20
06
</p>
<p>20
05
</p>
<p>20
04
</p>
<p>20
03
</p>
<p>20
12
</p>
<p>20
13
</p>
<p>year
</p>
<p>U
n
it
s
 s
</p>
<p>o
ld
</p>
<p>6
0
</p>
<p>8
0
</p>
<p>1
0
0
</p>
<p>1
2
0
</p>
<p>1
4
0
</p>
<p>1
6
0
</p>
<p>Fig. 10.4 The &ldquo;floating&rdquo; y-axis (I)
</p>
<p>376 10 Communicating the Results</p>
<p/>
</div>
<div class="page"><p/>
<p>Tables are generally less susceptible to manipulation, as they contain data in
</p>
<p>numbers, which the reader can check and understand. As a rule of thumb, each table
</p>
<p>or graph in the report should be numbered sequentially and have a meaningful title
</p>
<p>so that it can be understood without reading the text. This is called a self-contained
table or self-contained figure. Some rules of thumb:
</p>
<p>&ndash; Put data to be compared in columns, not rows.
</p>
<p>&ndash; Round data off to whole percentages, thousands or millions for sales, and two or
</p>
<p>three digits for academic purposes.
</p>
<p>&ndash; Highlight data to reinforce conclusions (e.g., making the key numbers bold).
</p>
<p>&ndash; Clearly state the units of measurement.
</p>
<p>20
11
</p>
<p>20
10
</p>
<p>20
09
</p>
<p>20
08
</p>
<p>20
07
</p>
<p>20
06
</p>
<p>20
05
</p>
<p>20
04
</p>
<p>20
03
</p>
<p>20
12
</p>
<p>20
13
</p>
<p>year
</p>
<p>U
n
it
s
 s
</p>
<p>o
ld
</p>
<p>0
3
</p>
<p>0
6
</p>
<p>0
9
</p>
<p>0
1
</p>
<p>2
0
</p>
<p>1
5
0
</p>
<p>1
8
0
</p>
<p>Fig. 10.5 The &ldquo;floating&rdquo; y-axis (II)
</p>
<p>Fig. 10.6 Doubling the edge
length quadruples the area
</p>
<p>10.4 Structure the Written Report 377</p>
<p/>
</div>
<div class="page"><p/>
<p>10.4.6.2 Presenting Statistical Data
In this section, we describe various ideas that you can use to convey statistical
</p>
<p>results in a reader-friendly manner. In the results section it is common to start
</p>
<p>presenting the descriptive statistics first. This comprises text that is visually
</p>
<p>supported by graphs, to offer information and context to those readers with the
</p>
<p>required background. Graphs offer two major advantages: first, they organize and
</p>
<p>simplify complex and dense information, which is especially useful with large data
</p>
<p>samples (Tufte 2001); second, graphs can summarize dense information efficiently.
</p>
<p>There are, of course, many kinds of graphs and each type has its advantages and
</p>
<p>disadvantages. The sample size and the nature of your data may constrain the
</p>
<p>choice of graphs. Sometimes your client may even have specific requirements.
</p>
<p>Here are some tips to help you with the presentation of your data:
</p>
<p>Summarize your results efficiently
</p>
<p>Graphs, like bar charts and especially dot charts, offer a useful way of summarizing
</p>
<p>descriptive data most efficiently; that is, by using less space (Cox 2008). Bar charts
</p>
<p>are generally useful where the objective is to depict how the outcome variable
</p>
<p>varies across two or more grouping variables. Figure 10.7 uses the Oddjob.dta
</p>
<p>dataset to illustrate this point by plotting the average overall satisfaction level with
</p>
<p>the price of the airline over the (1) respondents&rsquo; gender, (2) flight frequency, and
</p>
<p>(3) country of residence. Note that, in a complete presentation, it is important to
</p>
<p>include a title and subtitle, to label the y-axis and the x-axis, and to list the source of
</p>
<p>the data below the figure. Details of the syntax used are shown in the Web
</p>
<p>Appendix (! Downloads).
</p>
<p>Combine several plots
</p>
<p>Stata offers many more graphical options, many of which we discussed in Chap. 5.
</p>
<p>This chapter will therefore not review the many graphical options in Stata. How-
</p>
<p>ever, it is worth mentioning coefplot, a practical user-written program for
</p>
<p>plotting model estimates with Stata (Jann 2014). To install the program, type
</p>
<p>help coefplot in the command window and follow the instructions. The
</p>
<p>coefplot program offers many useful options for plotting statistical results
</p>
<p>innovatively. These options range from plotting descriptive data to more complex
</p>
<p>statistical outputs. It can also combine coefficients from different models by saving
</p>
<p>each estimated model separately before matching the coefficients and equations in a
</p>
<p>combined plot. Figure 10.8 offers an example and depicts the proportion of male
</p>
<p>and female travelers with different levels of disagreement or agreement with
</p>
<p>Oddjob Airways&rsquo; price along the seven different points of the overall satisfaction
</p>
<p>scale. The inclusion of confidence intervals in the graph&mdash;indicated by the vertical
</p>
<p>lines at the end of each bar&mdash;show that the proportion of differences between males
</p>
<p>and females are not statistically significant. This is because the vertical lines of each
</p>
<p>of the seven pairs of bars overlap. This matter could be unclear if the confidence
</p>
<p>intervals are not included and may lead to misinterpretation by knowledgeable
</p>
<p>readers, who assume that such differences are statistically significant. Details of the
</p>
<p>syntax used are included in the Web Appendix (! Downloads).
</p>
<p>378 10 Communicating the Results</p>
<p/>
</div>
<div class="page"><p/>
<p>Low Medium High
</p>
<p>de
</p>
<p>O
v
e
ra
</p>
<p>ll 
s
a
ti
s
fa
</p>
<p>c
ti
o
n
 l
e
v
e
l
</p>
<p>0
1
</p>
<p>2
3
</p>
<p>4
5
</p>
<p>Low Medium High
</p>
<p>ch
</p>
<p>Average Overall Satisfaction Level
by Gender, Flight Frequency and Country of Reisdence
</p>
<p>malefemale
</p>
<p>Source: Oddjob Airways Data
</p>
<p>Low Medium High
</p>
<p>at
</p>
<p>Fig. 10.7 Bar chart presentation
</p>
<p>fem
</p>
<p>Overall satisfaction level
</p>
<p>_prop_1 _prop_2 _prop_3 _prop_4 _prop_5 _prop_6 _prop_7
</p>
<p>0
.1
</p>
<p>.2
</p>
<p>P
ro
</p>
<p>p
o
rt
</p>
<p>io
n
</p>
<p>.3
.4
</p>
<p>male total
</p>
<p>Fig. 10.8 Combination of different estimations and charts using coefplot
</p>
<p>10.4 Structure the Written Report 379</p>
<p/>
</div>
<div class="page"><p/>
<p>Consider plotting regression coefficients
</p>
<p>It can be useful to plot the estimated regression coefficients, rather than show them
</p>
<p>in a table. This is not a substitute for presenting your careful data analysis in tables,
</p>
<p>but an aid and a complement. A graphical presentation of the regression coefficient
</p>
<p>estimates can be an efficient way of depicting both the significance and direction of
</p>
<p>the predicted effects.
</p>
<p>In Stata, regression coefficient estimates are plotted in three steps. First, a
</p>
<p>regression model is estimated. Then the regression coefficients are predicted in a
</p>
<p>second step. Third, the estimated coefficients with their corresponding confidence
</p>
<p>intervals are plotted in a profile plot. Figure 10.9 shows the result of these steps
</p>
<p>using the Oddjob.dta dataset. The figure depicts how commitment to fly with the
</p>
<p>airline relates to nflightsx (flight frequency), age, and the travelers&rsquo; gender. In the
</p>
<p>Web Appendix (! Downloads), you find details on how to plot regression
</p>
<p>coefficients yourself.
</p>
<p>Figure 10.9 shows that coefficients and their corresponding confidence intervals
</p>
<p>do not cross the 0 line on the x-axis (the vertical line on the right-hand side). This
means that each coefficient has a statistically significant association (at p &lt; 0.05)
</p>
<p>with commitment. The figure also shows the direction of the predicted effects, with
</p>
<p>negative effects left of the 0 line and positive effects right of the line. This means
that nflightsx and gender are negatively related to customers&rsquo; commitment, while
</p>
<p>age is positively related.
</p>
<p>-.5
</p>
<p>gender
</p>
<p>age
</p>
<p>nflightsx
</p>
<p>-.4 -.3 -.2
</p>
<p>Effects on Linear Prediction
</p>
<p>Average Marginal Effects with 95% Cls
</p>
<p>E
ff
e
c
ts
</p>
<p> w
it
h
 R
</p>
<p>e
s
p
e
c
t 
to
</p>
<p>-.1 0
</p>
<p>Fig. 10.9 Graphical presentation of regression coefficient estimates
</p>
<p>380 10 Communicating the Results</p>
<p/>
</div>
<div class="page"><p/>
<p>Finally, coefplot is not limited to regression, but can be extended to any other
</p>
<p>estimation method, varying from logistic regression models, in which the outcome
</p>
<p>variable is binary by nature, to multinomial and poisson regression models with
</p>
<p>categorical and count type outcome variables. See Jann (2014) for further details on
</p>
<p>the program.
</p>
<p>Make concise and clear (regression) tables with Stata
</p>
<p>Research projects often require running several analyses on the same set of data by
</p>
<p>using slightly different settings in the data analysis technique. For example,
</p>
<p>researchers generally run a regression analysis on multiple models, which differ
</p>
<p>with regard to the number of independent variables or the type of dependent
</p>
<p>variable. Typing the output tables for such multiple analyses by hand is time-
</p>
<p>consuming and often leads to errors. Fortunately, Stata offers different ways of
</p>
<p>creating tables that avoid such mistakes. Before showing how Stata can be
</p>
<p>helpful in this respect, it is important to understand what the different rows and
</p>
<p>columns of a table mean. When multiple models are estimated, these are usually
</p>
<p>presented in adjacent columns to make comparisons easier. In regression tables,
</p>
<p>each column represents the results of one regression analysis. The rows indicate
</p>
<p>each independent variable&rsquo;s influence on the dependent variable by means of the
</p>
<p>(standardized or unstandardized) regression coefficient estimates. To create a clear
</p>
<p>table, include the:
</p>
<p>1. Model Title: A first step in any table is to label each model that you are
</p>
<p>presenting. The label should be self-explanatory (&ldquo;Commitment to Oddjob
</p>
<p>Airlines&rdquo;). For academic purposes, a model number (e.g., Model 1, Model
</p>
<p>2, etc.) or a title that best represents the model (e.g., Baseline model; Extended
</p>
<p>model, etc.) is sometimes used. This is particularly useful when writing up the
</p>
<p>results as you can then refer to and compare the estimates of the different models
</p>
<p>in the text. The choice of model name depends on the audience and the
</p>
<p>formatting guidelines.
</p>
<p>2. Independent variables: In a (regression) table, the rows refer to the independent
</p>
<p>variables in the model. Give the variables a straightforward name to make it
</p>
<p>easier for the reader to understand. Make sure that these variable names are
</p>
<p>identical to those used in other tables and graphs (if any).
</p>
<p>3. Coefficient estimates: Depending on your audience, the (regression) table needs
</p>
<p>to specify whether standardized or unstandardized coefficient estimates are
</p>
<p>being presented. This can be included as a subtitle and explained above the
</p>
<p>table. In Table 10.2, the type of coefficient estimates is indicated as b/se.
Sometimes this is listed directly below the table.
</p>
<p>4. Significance level: Putting asterisks (*) behind the estimated regression
coefficients is a common way of presenting the coefficients&rsquo; significance levels.
</p>
<p>Usually, one asterisk indicates a significance level of 0.10, two asterisks a
</p>
<p>significance level of 0.05, and three asterisks a significance level of 0.01.
</p>
<p>Depending on the audience, researchers sometimes present only effects with a
</p>
<p>10.4 Structure the Written Report 381</p>
<p/>
</div>
<div class="page"><p/>
<p>significance of 0.05. Whatever strategy you choose, make sure you add a note
</p>
<p>below your table indicating the level of significance that the asterisks represent.
</p>
<p>5. Standard error or t-values: In addition to the significance levels, you need to
</p>
<p>present the corresponding t-value or standard error of the coefficient estimate,
</p>
<p>which is usually placed in brackets below the coefficient estimates. Both presen-
</p>
<p>tation methods are accepted and the choice depends on your audience and the
</p>
<p>formatting criteria.
</p>
<p>6. Sample size and degrees of freedom: For a complete presentation, you should
</p>
<p>also include the sample size and the models&rsquo; degrees of freedom after the model
</p>
<p>estimation. These statistics are respectively indicated as N and df_m in
Table 10.2. Reporting these statistics can reveal differences in the sample sizes
</p>
<p>of the different estimated models, which can be due to missing values in specific
</p>
<p>independent variables. If this happens, a comparison of the different models may
</p>
<p>make little sense, given that the models are based on different sample sizes with
</p>
<p>different characteristics. It is therefore important to understand what causes the
</p>
<p>large sample size differences between the models before taking any further
</p>
<p>action or drawing conclusions.
</p>
<p>7. Model fit: Finally, depending on the type of estimated model, statistics indicating
</p>
<p>the model significance, such as the R2 or η2 (eta-squared), and the relative model
</p>
<p>fit, such as the AIC or BIC statistics, should be part of the table. In Table 10.2 we
</p>
<p>have included the R-squared and BIC to indicate the model fit.
</p>
<p>You can produce tables with Stata by using the user-written package estout.
</p>
<p>As with any user-written program, you can install this Stata package immediately
</p>
<p>by typing help estout in the command window and then following the
</p>
<p>instructions. Table 10.2 shows an example of a regression table produced by
</p>
<p>using the estout command. It comprises three models that add several variables,
</p>
<p>Table 10.2 A regression table made using estout
</p>
<p>--------------------------------------------------------------------
Model 1         Model 2         Model 3   
</p>
<p>b/se            b/se            b/se   
--------------------------------------------------------------------
nflightsx                  -0.316***       -0.271*** -0.243***
</p>
<p>(0.06)          (0.06)          (0.06)   
Age                                         0.022***        0.022***
</p>
<p>(0.00)          (0.00)   
Gender                                                     -0.269*  
</p>
<p>(0.12)   
constant                    4.810***        3.614***        4.022***
</p>
<p>(0.14)          (0.27) (0.33)   
--------------------------------------------------------------------
R-squared 0.023           0.046           0.051   
N                            1065            1065            1065   
df_m                       1063.0          1062.0          1061.0   
BIC                        4189.6          4170.8          4172.7   
--------------------------------------------------------------------
* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001
</p>
<p>382 10 Communicating the Results</p>
<p/>
</div>
<div class="page"><p/>
<p>at a time, containing some of the key elements of the Table that we mentioned
</p>
<p>earlier. These are: the model title, indicated asModel 1,Model 2 andModel 3, with
a caption indicating that the (unstandardized) type of coefficient estimates and their
</p>
<p>pertaining standard errors b/se are presented. Next, the independent variables, with
the corresponding coefficients, standard errors (in brackets), and significance level
</p>
<p>are listed in the first column. For example, in Model 1, the first row presents the
unstandardized coefficient for nfligtsx (�0.316), together with the pertaining stan-
dard errors (e.g., (0.06)), and significance level (***) for which a caption is
included at the bottom of the table. Next, the sample size (N) and the models&rsquo;
degrees of freedom (df_m) in terms of each estimated model are shown in
Table 10.2. Finally, both the R-squared and the BIC values of each estimated
model are included to indicate the model fit.
</p>
<p>Further information that enables you to produce similar tables yourself can be
</p>
<p>found in the Web Appendix (! Downloads).
</p>
<p>10.4.7 Conclusion and Recommendations
</p>
<p>Having presented the findings, the next step is to summarize the most relevant
</p>
<p>points and interpret them in the light of the research objectives. You should write
</p>
<p>the conclusions in such a way that they present information that is relevant for
</p>
<p>managerial decision-making. Keep in mind that, for the client, the quality of the
</p>
<p>marketing research depends heavily on how well decision makers can use the
</p>
<p>information! The research must provide the client with clear benefits, which
</p>
<p>could lead to further research assignments.
</p>
<p>Researchers are increasingly asked to go beyond stating facts and to provide
</p>
<p>recommendations or to advise. Whereas conclusions based solely on the research
</p>
<p>should be unbiased and impersonal, specific recommendations are based on a
</p>
<p>personal and (at least partially) subjective opinion on how the results can be most
</p>
<p>favorably used in the client&rsquo;s interest. You should therefore make sure that
</p>
<p>recommendations are recognizable as such. During the negotiations prior to the
</p>
<p>start of a project, the client needs to determine the extent to which the research
</p>
<p>report should include recommendations. This will also depend on the researcher&rsquo;s
</p>
<p>expertise in the area. Researchers may provide logical recommendations based up
</p>
<p>the their findings, but these might be unrealistic or impossible for the client to
</p>
<p>implement due to issues such as insufficient budgets, predetermined methods, or
</p>
<p>specific policies, regulations, and politics. Make sure that you or another member of
</p>
<p>your research team is familiar with the overall context, including the regulatory and
</p>
<p>legal issues, to avoid such issues. Furthermore, before making recommendations,
</p>
<p>review them with the client to determine whether these are acceptable and action-
</p>
<p>able (see Box 10.1 for an example).
</p>
<p>10.4 Structure the Written Report 383</p>
<p/>
</div>
<div class="page"><p/>
<p>Box 10.1 Bad Recommendations
</p>
<p>A candy company wishes to know how it can increase its sales and has
</p>
<p>commissioned a research organization to gain insights into its different
</p>
<p>customer segments. The researchers find that teenagers are the most impor-
</p>
<p>tant target for the given brand and suggest that vending machines in schools
</p>
<p>would increase the company&rsquo;s revenue. Although this might boost sales, the
</p>
<p>recommendation does not help the company if vending machines are not
</p>
<p>allowed in schools. And even if they were allowed, they might lead to
</p>
<p>negative media reports.
</p>
<p>10.4.8 Limitations
</p>
<p>Finally, you should explain the extent to which the findings can be generalized. All
</p>
<p>research studies have limitations due to time, budget, and other constraints. Fur-
</p>
<p>thermore, errors might have occurred during the data collection. Not mentioning
</p>
<p>potential weaknesses (e.g., the use of a convenience sample, or a small sample size)
</p>
<p>for whatever reason reduces the research&rsquo;s credibility. Not disclosing important
</p>
<p>facts also violates common codes of industry conduct, such as those drafted by
</p>
<p>ESOMAR. Taking all these factors into regard, the research results should always
</p>
<p>be discussed objectively and in a balanced way. You should neither overly belittle
</p>
<p>the importance and validity of the research, nor try to conceal sources of errors and,
</p>
<p>hence, potentially mislead managers. Finally, some modesty is in order as, in
</p>
<p>hindsight, many reports have been proved inaccurate or even wrong. Few, for
</p>
<p>example, predicted the global financial crisis, the Trump presidency, or Brexit.
</p>
<p>10.4.9 Appendix
</p>
<p>All material not directly required for an understanding of the project, but still
</p>
<p>related to the study, should be included in the appendix or appendices. This includes
</p>
<p>questionnaires, interview guides, detailed data analyses, and other types of data or
</p>
<p>material.
</p>
<p>10.5 Guidelines for Oral Presentations
</p>
<p>Most clients want an oral presentation to accompany the written report. One could
</p>
<p>deliver such a presentation in the form of an interim report during the research, or at
</p>
<p>the end to explain the findings to the management and other staff. Members of the
</p>
<p>client staff often present the research findings to the management and do not ask the
</p>
<p>market research company to do so. Satisfaction with the delivered report may
</p>
<p>increase if a member of the client staff, such as an internal market researcher or
</p>
<p>384 10 Communicating the Results</p>
<p/>
</div>
<div class="page"><p/>
<p>business analyst, delivers the presentation, because the client feels they know and
</p>
<p>accept the content.
</p>
<p>If asked to deliver an oral presentation, you should keep the principles of a
</p>
<p>written report in mind. It is especially important to identify and understand your
</p>
<p>audience, and to prepare the presentation thoroughly. A professional and interesting
</p>
<p>presentation might increase interest in the written report! Furthermore, since the
</p>
<p>oral presentation allows for interaction, interesting points can be highlighted and
</p>
<p>discussed in more detail. However, if you are not well prepared for the presentation,
</p>
<p>nor understand your audience&rsquo;s expectations, needs, and wants, you could face an
</p>
<p>unpleasant situation. You should always keep the following golden rule in mind:
</p>
<p>Never deliver a presentation you wouldn&rsquo;t want to sit through!
</p>
<p>10.6 Visual Aids in Oral Presentations
</p>
<p>It is useful to provide the audience with a written summary or a handout so that they
</p>
<p>do not have to take notes, but can focus on the presentation. If focus group
</p>
<p>interviews were conducted, for example, you could show excerpts from the
</p>
<p>recordings to provide concrete examples in support of a finding. The saying &ldquo;a
</p>
<p>picture says more than a thousand words&rdquo; is also true of the oral presentation.
</p>
<p>Visual aids, such as overhead transparencies, flip charts, or computer slide shows
(e.g., PowerPoint or Prezi at http://www.prezi.com) not only help emphasize
</p>
<p>important points, but also facilitate the communication of difficult ideas. In the
</p>
<p>following, we summarize some suggestions (Armstrong 2010).
</p>
<p>Use of Visual Aids:
</p>
<p>&ndash; Use a simple master slide and avoid fancy animations.
</p>
<p>&ndash; Use a sufficiently large font size (as a rule of thumb, 16pt. or higher and never
</p>
<p>less than 12pt.) so that everyone attending the presentation can read the slides.
</p>
<p>&ndash; Use high contrasts for text. Use black and white. Do not write on illustrations or
</p>
<p>wallpapers.
</p>
<p>&ndash; Use contrasting colors to emphasize specific points, but not too many.
</p>
<p>&ndash; Use simple graphs, diagrams or short sentences rather than tables.
</p>
<p>Arranging Visual Aids:
</p>
<p>&ndash; Do not have too much information on one slide (generally, one key issue per
</p>
<p>slide). Never put a block of text on a page.
</p>
<p>&ndash; Organize the material so that the different modes reinforce one another. For
</p>
<p>example, you do not want people running ahead of you, so either explain each
</p>
<p>point as you discuss it on a slide, or use many simple slides.
</p>
<p>&ndash; Use a small number of slides compared to the time available for the presentation.
</p>
<p>The focus should be on the presenter and not on the slides. Having more slides
</p>
<p>than minutes available is not a good idea. Good presenters often use between
</p>
<p>3 and 5 min to discuss a slide.
</p>
<p>&ndash; Prepare (color) handouts for all members of the audience.
</p>
<p>10.6 Visual Aids in Oral Presentations 385</p>
<p/>
<div class="annotation"><a href="http://www.prezi.com/">http://www.prezi.com/</a></div>
</div>
<div class="page"><p/>
<p>&ndash; If you intend to use media elements in your presentation, make sure that the
</p>
<p>equipment supports them (e.g., that the sound equipment is working, or that your
</p>
<p>video formats are supported).
</p>
<p>10.7 Structure the Oral Presentation
</p>
<p>Be aware that an oral presentation cannot cover the same amount of information as
</p>
<p>a written report. You must be selective and structure the presentation content
</p>
<p>clearly and logically. There are two ways of creating a presentation:
</p>
<p>1. A common way of starting your presentation is by structuring the introduction in
</p>
<p>the classic narrative pattern of story-telling (situation ! difficulty or complica-
</p>
<p>tion ! question ! answer) introduced earlier in the context of written reports.
</p>
<p>Limit the introduction to what the audience can accept. Nothing could be worse
</p>
<p>than triggering resistance of what you are presenting right from the start of your
</p>
<p>oral presentation. Next, move on to the main part of your presentation. Based on
</p>
<p>a brief description of your major findings, capture the audience&rsquo;s attention by
</p>
<p>presenting answers to the logical questions that arise from the project, such as:
</p>
<p>&ldquo;How were these results achieved?&rdquo; or &ldquo;How did we reach this conclusion?&rdquo;
</p>
<p>2. An alternative is to follow the Minto principle, according to which
presentations have a pyramid structure, starting with the conclusion. This
raises question in the audience&rsquo;s mind that has to be subsequently answered.
</p>
<p>Figure 10.10 illustrates this concept by using the example of a mobile phone
</p>
<p>study, which found that a novel smartphone should be introduced in white.
</p>
<p>You begin by introducing the result of the study (i.e., the smartphone should be
</p>
<p>introduced in white) and then work your way down. Begin by explaining that a
</p>
<p>comprehensive market analysis was carried out, after which you discuss the
</p>
<p>elements of the analysis (i.e., focus group interviews, lead user interviews, and a
</p>
<p>customer survey). Finally, present the results of each element of the analysis (e.g.,
</p>
<p>that lead users perceived black as too conservative, silver as too cheap, while white
</p>
<p>was perceived as modern). Once at the bottom of the pyramid, it is time to pause
</p>
<p>and to provide a summary, before moving from the first key line, which you have
</p>
<p>just presented, to the next key line, and so on. This process forces you to only
</p>
<p>provide the information relevant to the question under consideration. Moving from
</p>
<p>top to bottom and then bottom to top, helps you answer the questions: &ldquo;Why so?&rdquo;
</p>
<p>and &ldquo;So what?,&rdquo; while being both exhaustive and mutually exclusive regarding the
</p>
<p>results and the concepts you have presented. Ensure you never provide findings that
</p>
<p>do not lead to specific conclusions and do not offer conclusions not based on
</p>
<p>findings. Ultimately, this pyramid approach helps the audience grasp the line of
</p>
<p>reasoning better. This technique is also frequently called the Minto principle or
</p>
<p>Minto pyramid after its creator Barbara Minto (2009).
</p>
<p>386 10 Communicating the Results</p>
<p/>
</div>
<div class="page"><p/>
<p>10.8 Follow-Up
</p>
<p>Having delivered the written report and oral presentation, two tasks remain: First,
</p>
<p>you may need to help the client implement the findings. This includes answering
</p>
<p>questions that may arise from the written report and oral presentation, helping select
</p>
<p>a product, advertising agency, marketing actions, etc., or incorporate information
</p>
<p>from the report into the firm&rsquo;s marketing information system or decision support
</p>
<p>system (see Chap. 3). This provides an opportunity to discuss other research
</p>
<p>projects. For example, you might agree on repeating the study after 1 year to see
</p>
<p>whether the marketing actions were effective. Second, you need to evaluate the
</p>
<p>market research project internally and with the client. Only (critical) feedback can
</p>
<p>disclose potential problems that may have occurred and, thus, provide the necessary
</p>
<p>grounds for improving your work. Using uniform questionnaires for the evaluation
</p>
<p>of different projects helps compare the feedback from different projects conducted
</p>
<p>simultaneously or at different points in time. However, some market research
</p>
<p>companies do not want to be involved in implementation.
</p>
<p>New smartphone
</p>
<p>should be white
</p>
<p>Competitor
</p>
<p>analysis
</p>
<p>Market
</p>
<p>analysis
</p>
<p>Customer
</p>
<p>survey
</p>
<p>Lead user
</p>
<p>interviews
</p>
<p>Focus group
</p>
<p>interviews
</p>
<p>Black:Too
</p>
<p>conservative
</p>
<p>Silver: Looks
</p>
<p>cheap
White:Very
</p>
<p>modern
</p>
<p>What has been done?
</p>
<p>How was it done?
</p>
<p>What was the result?
</p>
<p>Mutually exclusive &amp; collectively exhaustive
</p>
<p>S
o
 w
</p>
<p>h
a
t?
</p>
<p>W
h
y
 so
</p>
<p>?
</p>
<p>Fig. 10.10 Pyramid structure for presentations
</p>
<p>10.8 Follow-Up 387</p>
<p/>
</div>
<div class="page"><p/>
<p>10.9 Ethics in Research Reports
</p>
<p>Ethics is an important topic in marketing research, because research interacts with
human beings at several stages (e.g., during data collection and the communication
</p>
<p>of the findings). There are two &ldquo;problematic&rdquo; relations that can ultimately lead to
</p>
<p>ethical dilemmas. First, ethical issues arise when the researcher&rsquo;s interests conflict
</p>
<p>with those of the participants. For instance, the researcher&rsquo;s interest is to gather as
</p>
<p>much information as possible from the respondents, but they often require their
</p>
<p>answers to be treated confidentially and to remain anonymous. Second, in addition
</p>
<p>to researchers&rsquo; legal and professional responsibilities towards their respondents,
</p>
<p>they also have reporting responsibilities.
</p>
<p>For example, the European Society for Opinion and Marketing Research
</p>
<p>(ESOMAR) has established a code which sets minimum standards of ethical
</p>
<p>conduct to be followed by all researchers (ESOMAR 2007, p. 4):
</p>
<p>1. Market researchers shall conform to all relevant national and international laws.
2. Market researchers shall behave ethically and shall not do anything which might
</p>
<p>damage the reputation of market research.
3. Market researchers shall take special care when carrying out research among children
</p>
<p>and young people.
4. Respondents&rsquo; cooperation is voluntary and must be based on adequate, and not
</p>
<p>misleading, information about the general purpose and nature of the project when
their agreement to participate is being obtained and all such statements shall be
honoured.
</p>
<p>5. The rights of respondents as private individuals shall be respected by market researchers
and they shall not be harmed or adversely affected as the direct result of cooperating in a
market research project.
</p>
<p>6. Market researchers shall never allow personal data they collect in a market research
project to be used for any purpose other than market research.
</p>
<p>7. Market researchers shall ensure that projects and activities are designed, carried out,
reported and documented accurately, transparently and objectively.
</p>
<p>8. Market researchers shall conform to the accepted principles of fair competition.
</p>
<p>In practice, researchers face an ethical dilemma. They are paid by the client and
</p>
<p>may feel forced to deliver &ldquo;good&rdquo; results. In this sense, they might be tempted to
</p>
<p>interpret results in a way that fits the client&rsquo;s perspective or the client&rsquo;s presumed
</p>
<p>interests. For instance, researchers might ignore data because they would reveal an
</p>
<p>inconvenient truth (e.g., the client&rsquo;s brand has low awareness, or customers do not
</p>
<p>like the product design).
</p>
<p>Remember that researchers should never mislead the audience! For instance, it
</p>
<p>would be ethically questionable to modify the scales of a graph so that the results
</p>
<p>look more impressive, as shown in Figs. 10.1, 10.2, 10.3, and 10.4. Furthermore,
</p>
<p>researchers have a duty to treat information and research results confidentially, to
</p>
<p>store data securely, and to only use data for the research purpose agreed upon.
</p>
<p>Above all, you should keep in mind that marketing research is based on trust. Thus,
</p>
<p>when writing the report, you should respect the profession&rsquo;s ethical standards in
</p>
<p>order to maintain this trust.
</p>
<p>388 10 Communicating the Results</p>
<p/>
</div>
<div class="page"><p/>
<p>10.10 Review Questions
</p>
<p>1. What are the basic elements of any written research report?
</p>
<p>2. Revisit the case study on Oddjob Airlines in Chap. 7 and prepare an outline for a
</p>
<p>written research report.
</p>
<p>3. Consider the following situations. Do you think they confront the market
</p>
<p>researcher with ethical issues?
</p>
<p>(a) The client asks the researcher for a list of respondents to allow him/her to
</p>
<p>target selling activities at them.
</p>
<p>(b) The client asks the researcher not to disclose part of the research to his
</p>
<p>organization.
</p>
<p>(c) The client asks the researcher to present other recommendations.
</p>
<p>(d) The client asks the researcher to re-consider the analysis, because the
</p>
<p>findings seem implausible to him/her.
</p>
<p>(e) The client wishes to know the name of a particular customer who was very
</p>
<p>negative about the quality of service provided.
</p>
<p>10.11 Further Readings
</p>
<p>Huff D. (1993). How to lie with statistics. New York: Norton &amp; Company.
</p>
<p>First published in 1954, this book remains relevant as a wake-up call for people
</p>
<p>unaccustomed to the slippery world of means, correlations, and graphs.
</p>
<p>Although many of the examples used in the book are dated, the conclusions
</p>
<p>are timeless.
</p>
<p>Durate N. (2008). Slideology. The art and science of crafting great presentations.
</p>
<p>Sebastopol: O&rsquo;Reilly Media.
</p>
<p>In this book, the author presents a rich source for effective visual expression in
</p>
<p>presentations. It is full of practical approaches to visual story development that
</p>
<p>can be used to connect with your audience. The text provides good hints to fulfill
</p>
<p>the golden rule to never deliver a presentation you wouldn&rsquo;t want to sit through.
</p>
<p>Market Research Society at http://www.mrs.org.uk/standards/guidelines.htm
</p>
<p>Under this link you find the (ethical) guidelines of the Market Research Society. The
</p>
<p>guidelines discuss, for example, the ethical issues surrounding research using
</p>
<p>children or the elderly as participants.
</p>
<p>References
</p>
<p>Armstrong, J. S. (2010). Persuasive advertising: Evidence-based principles. New York: Palgrave
Macmillan.
</p>
<p>Churchill, G. A., Jr., &amp; Iacobucci, D. (2009). Marketing research: Methodological foundations
(10th ed.). Mason: South-Western College Publishers.
</p>
<p>Cox, N. J. (2008). Speaking Stata: Between tables and graphs. Stata Journal, 8(2), 269&ndash;289.
</p>
<p>References 389</p>
<p/>
<div class="annotation"><a href="http://www.mrs.org.uk/standards/guidelines.htm">http://www.mrs.org.uk/standards/guidelines.htm</a></div>
</div>
<div class="page"><p/>
<p>European Society for Opinion and Marketing Research (ESOMAR). (2007). ICC/ESOMAR
International Code On Market And Social Research. http://www.esomar.org/uploads/public/
knowledge-and-standards/codes-and-guidelines/ICCESOMAR_Code_English_.pdf
</p>
<p>Huff, D. (1993). How to lie with statistics. New York: W. W. Norton &amp; Company.
Jann, B. (2014). Plotting regression coefficients and other estimates. Stata Journal, 14(4),
</p>
<p>708&ndash;737.
Minto, B. (2009). The pyramid principle: Logic in writing and thinking (3rd ed.). Harlow: Pearson.
Tufte, E. R. (2001). The visual display of quantitative information (2nd ed.). Cheshire: Graphics
</p>
<p>Press.
</p>
<p>390 10 Communicating the Results</p>
<p/>
<div class="annotation"><a href="http://www.esomar.org/uploads/public/knowledge-and-standards/codes-and-guidelines/ICCESOMAR_Code_English_.pdf">http://www.esomar.org/uploads/public/knowledge-and-standards/codes-and-guidelines/ICCESOMAR_Code_English_.pdf</a></div>
<div class="annotation"><a href="http://www.esomar.org/uploads/public/knowledge-and-standards/codes-and-guidelines/ICCESOMAR_Code_English_.pdf">http://www.esomar.org/uploads/public/knowledge-and-standards/codes-and-guidelines/ICCESOMAR_Code_English_.pdf</a></div>
</div>
<div class="page"><p/>
<p>Glossary
</p>
<p>α-Error occurs when erroneously rejecting a true null hypothesis. Also referred to
as type I error.
</p>
<p>α-Inflation results when multiple tests are conducted simultaneously on the same
</p>
<p>data. The result is that you are more likely to claim a significant result when this
</p>
<p>is not so (i.e., an increase or inflation in the type I error).
</p>
<p>Acquiescence describes the tendency of respondents from different cultures to
</p>
<p>agree with statements (e.g., as formulated in a Likert scale item) regardless of
</p>
<p>their content.
</p>
<p>Adjusted coefficient of determination is a modified measure of goodness-of-fit
that takes the number of independent variables and the sample size into account.
</p>
<p>The statistic is useful for comparing regression models with different numbers of
</p>
<p>independent variables, sample sizes, or both.
</p>
<p>Adjusted R2 See Adjusted coefficient of determination.
</p>
<p>Agglomerative clustering is a type of hierarchical clustering method in which
</p>
<p>clusters are consecutively formed from objects. It starts with each object
</p>
<p>representing an individual cluster. The objects are then sequentially merged to
</p>
<p>form clusters of multiple objects, starting with the two most similar.
</p>
<p>Aggregation is a type of scale transformation in which variables measured at a
</p>
<p>lower level are taken to a higher level.
</p>
<p>Akaike information criterion (AIC) is a relative measure of goodness-of-fit,
</p>
<p>which can be used to assess the various statistical models such as regression or
</p>
<p>factor analysis. Compared to an alternative solution with a different number of
</p>
<p>variables or factors, smaller AIC values indicate a better fit.
</p>
<p>American Marketing Association (AMA) is the world&rsquo;s leading association for
</p>
<p>marketing professionals.
</p>
<p>Analysis of variance (ANOVA) is a multivariate data analysis technique that
</p>
<p>allows testing whether the means of (typically) three or more groups differ
</p>
<p>significantly on one (one-way ANOVA) or two (two-way ANOVA) metric
</p>
<p>dependent variable(s). There are numerous extensions to more dependent
</p>
<p>variables and to differently scaled independent variables.
</p>
<p>Anti-image is a measure used in principal component and factor analysis to
</p>
<p>determine whether the items correlate sufficiently. The anti-image describes
</p>
<p># Springer Nature Singapore Pte Ltd. 2018
</p>
<p>E. Mooi et al., Market Research, Springer Texts in Business and Economics,
</p>
<p>DOI 10.1007/978-981-10-5218-7
</p>
<p>391</p>
<p/>
</div>
<div class="page"><p/>
<p>the portion of an item&rsquo;s variance that is independent of another item in the
</p>
<p>analysis.
</p>
<p>Arithmetic mean See mean.
</p>
<p>Armstrong and Overton procedure is used to assess the degree of non-response
</p>
<p>bias. This procedure calls for comparing the first 50% respondents with the last
</p>
<p>50% with regard to key demographic variables. The concept behind this proce-
</p>
<p>dure is that later respondents more closely match the characteristics of
</p>
<p>non-respondents.
</p>
<p>Autocorrelation occurs when the residuals from a regression analysis are
</p>
<p>correlated.
</p>
<p>Average see mean.
</p>
<p>Average linkage is a linkage algorithm in hierarchical clustering methods in
</p>
<p>which the distance between two clusters is defined as the average distance
</p>
<p>between all pairs of objects in the two clusters.
</p>
<p>Average marginal effects average change (in percentage) of one variable when
</p>
<p>another variable increases by one unit.
</p>
<p>β-Error occurs when erroneously accepting a false null hypothesis. Also referred
</p>
<p>to as type II error.
</p>
<p>Back-translation is a translation method used in survey research in which a
</p>
<p>survey is being translated and then back-translated into the original language
</p>
<p>by another person.
</p>
<p>Balanced scale describes a scale with an equal number of positive and negative
</p>
<p>scale categories.
</p>
<p>Bar chart is a graphical representation of a single categorical variable indicating
each category&rsquo;s frequency of occurrence. Bar charts are primarily useful for
</p>
<p>describing nominal and ordinal variables.
</p>
<p>Bartlett method is a procedure to generate factor scores in principal component
</p>
<p>analysis. The resulting factor scores have a zero mean and a standard deviation
</p>
<p>larger than one.
</p>
<p>Bayes information criterion (BIC) is a relative measure of goodness-of-fit,
</p>
<p>which is similar to the AIC. Compared to the AIC, the BIC applies a greater
</p>
<p>penalty to statistical analysis that have a greater number of variables or factors.
</p>
<p>Big data refers to very large datasets, generally a mix of quantitative and qualita-
</p>
<p>tive data in very large volumes.
</p>
<p>Binary logistic regression is a type of regression method used when the depen-
</p>
<p>dent variable is binary and only takes two values.
</p>
<p>Bivariate statistics describes statistics that express the empirical relationship
</p>
<p>between two variables. Covariance and correlation are key measures that indi-
</p>
<p>cate (linear) associations between two variables.
</p>
<p>Bonferroni correction is a post hoc test typically used in an ANOVA that
maintains the familywise error rate by calculating a new pairwise alpha that
</p>
<p>divides the statistical significance level α by the number of comparisons made
</p>
<p>(see also familywise error rate and α-Inflation).
</p>
<p>Box-and-whisker plot See box plot.
</p>
<p>392 Glossary</p>
<p/>
</div>
<div class="page"><p/>
<p>Box plot shows the distribution of a variable. A box plot is a graph representing a
</p>
<p>variable&rsquo;s distribution and consists of elements expressing the dispersion of the
</p>
<p>data. Also referred to as box-and-whisker plot.
</p>
<p>Breusch-Pagan test is used to test for heteroskedasticity in regression analysis.
</p>
<p>Canberra distance is a distance measure used in cluster analysis. The Canberra
</p>
<p>distance is a weighted version of the city-block distance, typically used for
</p>
<p>clustering data scattered widely around an origin.
</p>
<p>Case is an object such as a customer, a company, or a country in statistical
analysis. Also referred to as observation.
</p>
<p>Causal research is used to understand the relationships between two or more
</p>
<p>variables. Causal research explains how variables relate.
</p>
<p>Census is a procedure of systematically acquiring and recording information about
</p>
<p>all the members of a given population.
</p>
<p>Centroid linkage is a linkage algorithm in hierarchical clustering methods in
</p>
<p>which the distance between two clusters is defined as the distance between
</p>
<p>their geometric centers (centroids).
</p>
<p>Chaining effect is a solution pattern typically observed when using a single
</p>
<p>linkage algorithm in cluster analysis.
</p>
<p>Chebychev distance is a distance measure used in cluster analysis that uses the
</p>
<p>maximum of the absolute difference in the clustering variables&rsquo; values.
</p>
<p>City-block distance is a distance measure used in cluster analysis that uses the
</p>
<p>sum of the variables&rsquo; absolute differences. Also referred to as Manhattan metric.
</p>
<p>Closed-ended questions is a type of question format in which respondents have a
</p>
<p>certain number of response categories from which to choose.
</p>
<p>Cluster analysis is a class of methods that groups a set of objects with the goal of
</p>
<p>obtaining high similarity within the formed groups and high dissimilarity
</p>
<p>between groups.
</p>
<p>Clustering variables are variables used in cluster analysis.
</p>
<p>Clusters are groups of objects with similar characteristics.
</p>
<p>Codebook contains essential details of a data file, such as variable names and
</p>
<p>summary statistics.
</p>
<p>Coefficient of determination (R2) is a measure used in regression analysis to
express the dependent variable&rsquo;s amount of variance that the independent
</p>
<p>variables explain.
</p>
<p>Collinearity arises when two variables are highly correlated.
</p>
<p>Communality describes the amount of a variable&rsquo;s variance that the extracted
</p>
<p>factors in a principal component and factor analysis reproduce.
</p>
<p>Complete linkage is a linkage algorithm in hierarchical clustering methods in
</p>
<p>which the distance between two clusters corresponds to the longest distance
</p>
<p>between any two members in the two clusters.
</p>
<p>Components are extracted in the course of a principal component analysis. They
</p>
<p>are also commonly referred to as factors.
</p>
<p>Computer-assisted web interviews (CAWI) See Web surveys.
</p>
<p>Glossary 393</p>
<p/>
</div>
<div class="page"><p/>
<p>Confidence interval provides the lower and upper limit of values within which a
</p>
<p>population parameter will fall with a certain probability (e.g., 95%).
</p>
<p>Confirmatory factor analysis is a special form of factor analysis used to test
</p>
<p>whether the measures of a construct are consistent with a researcher&rsquo;s under-
</p>
<p>standing of that construct.
</p>
<p>Constant sum scale is a type of scale that requires respondents to allocate a certain
</p>
<p>total number of points (typically 100) to a number of alternatives.
</p>
<p>Constant is a characteristic of an object whose value does not change.
Construct scores are composite scores that calculate a value for each construct of
</p>
<p>each observation. Construct scores are often computed by taking the mean of all
</p>
<p>the items associated with the construct.
</p>
<p>Construct validity is the degree of correspondence between a measure at the
</p>
<p>conceptual level and its empirical manifestation. Researchers often use this as
</p>
<p>an umbrella term for content, criterion, discriminant, face, and nomological
</p>
<p>validity.
</p>
<p>Construct measures a concept that is abstract, complex, and cannot be directly
observed by (multiple) items. Also referred to as latent variable.
</p>
<p>Content validity refers to the extent to which a measure represents all facets of a
</p>
<p>given construct.
</p>
<p>Correlation residuals are the differences between the original item correlations and
</p>
<p>the reproduced item correlations in a principal component and factor analysis.
</p>
<p>Correlation is a measure of how strongly two variables relate to each other.
</p>
<p>Correlation is a scaled version of the covariance.
</p>
<p>Covariance is a measure of how strongly two variables relate to each other.
Covariance-based structural equation modeling (CB-SEM) is an approach to
</p>
<p>structural equation modeling to test relationships between multiple items and
</p>
<p>constructs.
</p>
<p>Criterion validity measures how well one measure predicts the outcome of
</p>
<p>another measure when both are measured at the same time.
</p>
<p>Cronbach&rsquo;s alpha is a measure of internal consistency reliability. Cronbach&rsquo;s
</p>
<p>alpha generally varies between 0 and 1 with greater values indicating higher
</p>
<p>degrees of reliability.
</p>
<p>Cross validation entails comparing the results of an analysis with those obtained
</p>
<p>when using a new dataset.
</p>
<p>Crosstabs are tables in a matrix format that show the frequency distribution of
</p>
<p>nominal or ordinal variables.
</p>
<p>Customer relationship management (CRM) refers to a system of databases and
</p>
<p>software used to track and predict customer behavior.
</p>
<p>Data entry errors is a mistake in transcribing data during data entry. Erroneous
</p>
<p>values that fall outside a variable&rsquo;s standard range can easily be identified by
</p>
<p>means of descriptive statistics (minimum, maximum, and range).
</p>
<p>Degrees of freedom (df) represents the amount of information available to esti-
</p>
<p>mate a test statistic. In general terms, an estimate&rsquo;s degrees of freedom are equal
</p>
<p>to the amount of independent information used (i.e., the number of observations)
</p>
<p>minus the number of parameters estimated.
</p>
<p>394 Glossary</p>
<p/>
</div>
<div class="page"><p/>
<p>Dendrogram visualizes the results of a cluster analysis. Horizontal lines in a
</p>
<p>dendrogram indicate the distances at which the objects have been merged.
</p>
<p>Dependence of observations is the degree to which observations are related.
</p>
<p>Dependent variables are the concepts a researcher wants to understand, explain,
</p>
<p>or predict.
</p>
<p>Depth interview is an interview type typically used in exploratory research that
</p>
<p>allows one-to-one probing to foster interaction between the interviewer and the
</p>
<p>respondent.
</p>
<p>Descriptive research is used to detail certain phenomena, characteristics, or
</p>
<p>functions. Descriptive research often builds on previous exploratory research.
</p>
<p>Discriminant validity ensures that a measure is empirically unique and represents
</p>
<p>phenomena of interest that other measures in a model do not capture.
</p>
<p>Distance matrix expresses the distances between pairs of objects.
</p>
<p>Disturbance term see Residual.
</p>
<p>Divisive clustering is a type of hierarchical clustering method in which all objects
</p>
<p>are initially merged into a single cluster, which the algorithm then gradually
</p>
<p>splits up.
</p>
<p>Double-barreled questions are survey questions to which respondents can agree
</p>
<p>with one part but not with the other. Also refers to survey questions that cannot
</p>
<p>be answered without accepting an assumption.
</p>
<p>Duda-Hart index is a statistic used in cluster analysis to determine the number of
</p>
<p>clusters to extract from the data. The statistic compares the sum of the squares in
</p>
<p>a pair of clusters to be split both before and after this extraction.
</p>
<p>Dummy variables are binary variables that indicate whether a certain trait is
present or not.
</p>
<p>Durbin-Watson test is a test for autocorrelation used in regression analysis.
</p>
<p>Eigenvalue indicates the amount of variance reproduced by a specific component
</p>
<p>or factor.
</p>
<p>Eigenvectors are the results of a principal component analysis and include the
</p>
<p>factor weights.
</p>
<p>Equidistance is indicated when the (psychological) distances between a scale&rsquo;s
</p>
<p>categories are identical.
</p>
<p>Equidistant scale is a scale whose scale categories are equidistant.
</p>
<p>Error is the difference between the regression line (which represents the regres-
</p>
<p>sion prediction) and the actual observation.
</p>
<p>Error sum of squares quantifies the difference between the observations and the
</p>
<p>regression line.
</p>
<p>ESOMAR is the world organization for market, consumer, and societal research.
</p>
<p>Estimation sample is the sample used to run a statistical analysis.
</p>
<p>Eta-squared (η2) is a statistic used in an ANOVA to describe the ratio of the
between-group variation to the total variation, thereby indicating the variance
</p>
<p>accounted for by the sample data. There are two types of η 2: the model η 2,
</p>
<p>which is identical to the R2, and each variable&rsquo;s partial η 2, which describes the
</p>
<p>percentage of the total variance accounted for by that variable. Eta squared also
</p>
<p>refers to the percentage of variance explained by a single variable in regression
</p>
<p>analysis.
</p>
<p>Glossary 395</p>
<p/>
</div>
<div class="page"><p/>
<p>Ethics are a system of morals and principles which defines a research
</p>
<p>organization&rsquo;s obligations, for example, with regard to the findings they release
</p>
<p>being an accurate portrayal of the survey data.
</p>
<p>Ethnography is a type of qualitative research in which the researcher interacts
</p>
<p>with consumers over a period to observe and question them.
</p>
<p>Euclidean distance is a distance measure commonly used in cluster analysis. It is
</p>
<p>the square root of the sum of the squared differences in the variables&rsquo; values.
</p>
<p>Also referred to as straight-line distance.
</p>
<p>Experimental design describes which treatment variables to administer and how
</p>
<p>these relate to dependent variables. Prominent experimental designs include the
</p>
<p>one-shot case study, the before-after design, the before-after design with a
</p>
<p>control group, and the Solomon four-group design.
</p>
<p>Experiments are study designs commonly used in causal research in which a
</p>
<p>researcher controls for a potential cause and observes corresponding changes
</p>
<p>in hypothesized effects via treatment variables.
</p>
<p>Exploratory factor analysis is a type of factor analysis that derives factors from a
set of correlated indicator variables without the researcher having to prespecify a
</p>
<p>factor structure.
</p>
<p>Exploratory research is conducted when the researcher has little or no informa-
</p>
<p>tion about a particular problem or opportunity. It is used to refine research
</p>
<p>questions, discover new relationships, patterns, themes, and ideas or to inform
</p>
<p>measurement development.
</p>
<p>External secondary data are compiled outside a company for a variety of purposes.
</p>
<p>Sources of secondary data include, for example, governments, trade associations,
</p>
<p>market research firms, consulting firms, (literature) databases, and social networks.
</p>
<p>External validity is the extent to which the study results can be generalized to
</p>
<p>real-world settings.
</p>
<p>Extreme response styles occur when respondents systematically select the
</p>
<p>endpoints of a response scale.
</p>
<p>Face validity is the extent to which a test is subjectively viewed as covering the
</p>
<p>concept it purports to measure.
</p>
<p>Face-to-face interview See Personal interview.
Factor analysis is a statistical procedure that uses the correlation patterns among a
</p>
<p>set of indicator variables to derive factors that represent most of the original
</p>
<p>variables&rsquo; variance. Also referred to as Principal axis factoring.
</p>
<p>Factor loading is the correlation between a (unit-scaled) factor and a variable.
</p>
<p>Factor rotation is a technique used to facilitate the interpretation of solutions in
</p>
<p>principal component and factor analysis.
</p>
<p>Factors are (1) independent variables in an ANOVA and (2) the resulting
</p>
<p>variables of a principal component and factor analysis that summarize the
</p>
<p>information from a set of indicator variables.
</p>
<p>Factor scores are composite scores that calculate a value for each factor of each
</p>
<p>observation.
</p>
<p>Factor variable is a categorical variable used to define the groups (e.g., three
</p>
<p>types of promotion campaigns) in an ANOVA.
</p>
<p>396 Glossary</p>
<p/>
</div>
<div class="page"><p/>
<p>Factor weights express the relationships between variables and factors.
</p>
<p>Familywise error rate is the probability of making one or more false discoveries
</p>
<p>or type I errors when performing multiple hypotheses tests (see also α-inflation).
</p>
<p>Field experiments are experiments in which the manipulation of a treatment
</p>
<p>variable occurs in a natural setting, thereby emphasizing the external validity,
</p>
<p>but potentially compromising internal validity.
</p>
<p>Field service firms are companies that focus on conducting surveys, determining
</p>
<p>samples and sample sizes, and collecting data. Some of these firms also translate
</p>
<p>surveys or provide addresses and contact details.
</p>
<p>Focus groups is a method of data collection in which four to six participants
</p>
<p>discuss a defined topic under the leadership of a moderator.
</p>
<p>Forced-choice scale is an answer scale that omits a neutral category, thereby
</p>
<p>forcing the respondents to make a positive or negative assessment.
</p>
<p>Formative construct is a type of measurement in which the indicators form the
</p>
<p>construct.
</p>
<p>Free-choice scale is an answer scale that includes a neutral choice category.
Respondents are therefore not forced to make a positive or negative assessment.
</p>
<p>Frequency table is a table that displays the absolute, relative, and cumulative
</p>
<p>frequencies of one or more variables.
</p>
<p>F-test of sample variance see Levene&rsquo;s test.
</p>
<p>F-test a test statistic used in an ANOVA and regression analysis to test the overall
</p>
<p>model&rsquo;s significance.
</p>
<p>Full service providers are large market research companies, such as The Nielsen
</p>
<p>Company, Kantar, or GfK, that offer syndicated and customized services.
</p>
<p>Gower&rsquo;s dissimilarity coefficient a dissimilarity coefficient used in cluster anal-
</p>
<p>ysis that works with a mix of binary and continuous variables.
</p>
<p>Heteroskedasticity refers to a situation in regression analysis in which the vari-
</p>
<p>ance of the residuals is not constant.
</p>
<p>Heywood cases negative estimates of variances or correlation estimates greater
</p>
<p>than one in absolute value.
</p>
<p>Hierarchical clustering methods develop a treelike structure of objects in the
</p>
<p>course of the clustering process, which can be top-down (divisive clustering) or
</p>
<p>bottom-up (agglomerative clustering).
</p>
<p>Histogram is a graph that shows how frequently categories derived from a con-
</p>
<p>tinuous variable occur.
</p>
<p>Hypotheses are claims made about effects or relationships in a population.
</p>
<p>Inconsistent answers are a respondent&rsquo;s contradictory answer patterns.
</p>
<p>Independent samples t-test a test using the t-statistic that establishes whether two
</p>
<p>means collected from independent samples differ significantly.
</p>
<p>Independent variables are variables that explain or predict a dependent variable.
In-depth interview is a qualitative conversation with participants on a specific
</p>
<p>topic.
</p>
<p>Index consists of a set of variables that defines the meaning of the resulting
</p>
<p>composite.
</p>
<p>Index construction is the procedure of combining several items to form an index.
</p>
<p>Glossary 397</p>
<p/>
</div>
<div class="page"><p/>
<p>Indicators See items.
</p>
<p>Interaction effect refers to how the effect of one variable on another variable is
</p>
<p>influenced by a third variable.
</p>
<p>Intercept is the expected mean value of the dependent variable in a regression
</p>
<p>analysis, when the independent variables are zero. Also referred to as a constant.
</p>
<p>Internal consistency reliability is a form of reliability used to judge the consis-
</p>
<p>tency of results across items in the same test. It determines whether the items
</p>
<p>measuring a construct are highly correlated. The most prominent measure of
</p>
<p>internal consistency reliability is Cronbach&rsquo;s alpha.
</p>
<p>Internal secondary data are data that companies compile for various reporting
</p>
<p>and analysis purposes.
</p>
<p>Internal validity is the extent to which causal claims can be made in respect of the
</p>
<p>study results.
</p>
<p>Interquartile range is the difference between the third and first quartile.
</p>
<p>Inter-rater reliability is the degree of agreement between raters expressed by the
</p>
<p>amount of consensus in their judgment.
</p>
<p>Interviewer fraud is an issue in data collection resulting from interviewers
</p>
<p>making up data or even falsifying entire surveys.
</p>
<p>Item non-response occurs when people do not provide answers to certain
</p>
<p>questions, for example, because they refuse to answer, or forgot to answer.
</p>
<p>Items represent measurable characteristics in conceptual models and statistical
</p>
<p>analysis. Also referred to as indicators.
</p>
<p>Kaiser criterion is a statistic used in principal component and factor analysis to
</p>
<p>determine the number of factor to extract from the data. According to this
</p>
<p>criterion, researchers should extract all factors with an eigenvalue greater than
</p>
<p>one. Also referred to as latent root criterion.
</p>
<p>Kaiser-Meyer-Olkin criterion is an index used to assess the adequacy of the data
</p>
<p>for a principal component and factor analysis. High values indicate that the data
</p>
<p>are sufficiently correlated. Also referred to as measure of sampling adequacy
</p>
<p>(MSA).
</p>
<p>KISS principle the abbreviation of &ldquo;keep it short and simple!&rdquo; and implies that
</p>
<p>any research report should be as concise as possible.
</p>
<p>k-means is a group of clustering methods that starts with an initial partitioning of
</p>
<p>all the objects into a prespecified number of clusters and then gradually
</p>
<p>reallocates objects in order to minimize the overall within-cluster variation.
</p>
<p>k-means++ is a variant of the k-means method that uses an improved initialization
</p>
<p>process.
</p>
<p>k-medians is a popular variant of k-means that aims at minimizing the absolute
</p>
<p>deviations from the cluster medians.
</p>
<p>k-medoids is a variant of k-means that uses other cluster centers rather than the
mean or median.
</p>
<p>Lab experiments are performed in controlled environments (usually in a company
</p>
<p>or academic lab) to isolate the effects of one or more treatment variables on an
</p>
<p>outcome.
</p>
<p>398 Glossary</p>
<p/>
</div>
<div class="page"><p/>
<p>Label switching a situation in which the labels of clusters change from one
</p>
<p>analysis to the other.
</p>
<p>Laddering is an interviewing technique where the interviewer pushes a seemingly
</p>
<p>simple response to a question in order to find subconscious motives. It is
</p>
<p>typically used in the means-end approach.
</p>
<p>Latent concepts represent broad ideas or thoughts about certain phenomena that
</p>
<p>researchers have established and want to measure in their research.
</p>
<p>Latent root criterion See Kaiser criterion.
Latent variable measures a concept that is abstract, complex, and cannot be
</p>
<p>directly observed by (multiple) items. Also referred to as construct.
</p>
<p>Levene&rsquo;s test tests the equality of the variances between two or more groups of
</p>
<p>data. Also referred to as F-test of sample variance.
</p>
<p>Likert scale is a type of answering scale in which respondents have to indicate
</p>
<p>their degree of agreement to a statement. The degree of agreement is usually set
</p>
<p>by the scale endpoints, which range from strongly disagree to strongly agree.
</p>
<p>Limited service providers are market research companies that specialize in one
or more services.
</p>
<p>Line chart is a type of chart in which measurement points are ordered (typically
</p>
<p>according to their x-axis value) and joined with straight-line segments.
</p>
<p>Linkage algorithm defines the distance from a newly formed cluster to a certain
</p>
<p>object or to other clusters in the solution.
</p>
<p>Listwise deletion entails deleting cases with one or more missing value(s) in any
</p>
<p>of the variables used in an analysis.
</p>
<p>Little&rsquo;s MCAR test is used to analyze the patterns of missing data by comparing
the observed data with the pattern expected if the data were missing completely
</p>
<p>at random.
</p>
<p>Local optimum is an optimal solution when compared with similar solutions, but
</p>
<p>not a global optimum.
</p>
<p>Log transformation is a type of scale transformation commonly used to handle
</p>
<p>skewed data.
</p>
<p>Mahalanobis distance is a distance measure used in cluster analysis that
</p>
<p>compensates for the collinearity between the clustering variables.
</p>
<p>Mail surveys are paper-based surveys sent to respondents via regular mail.
</p>
<p>Main effect is the effect of one independent variable (i.e., factor) on the dependent
</p>
<p>variable, ignoring the effects of all the other independent variables in a two-way
</p>
<p>ANOVA.
</p>
<p>Manhattan metric See City-block distance.
</p>
<p>Manipulation checks a type of analysis in experiments to check whether the
</p>
<p>experimental treatment was effective.
</p>
<p>Mann-Whitney U test is the nonparametric equivalent of the independent
samples t-test used to assess whether two sample means are equal or not.
</p>
<p>Marginal mean represents the mean value of one category in respect of each of
</p>
<p>the other types of categories.
</p>
<p>Market segmentation is the segmenting of markets into groups (segments) of
</p>
<p>objects (e.g., consumers) with similar characteristics (e.g., needs and wants).
</p>
<p>Glossary 399</p>
<p/>
</div>
<div class="page"><p/>
<p>Market segments are groups of objects with similar characteristics.
</p>
<p>Matching coefficients are similarity measures that express the degree to which the
</p>
<p>clustering variables&rsquo; values fall into the same category.
</p>
<p>Mean is the most common method of defining a typical value of a list of numbers.
</p>
<p>It is equal to the sum of a variable&rsquo;s values divided by the number of
</p>
<p>observations. Also referred to as arithmetic mean or simply average.
</p>
<p>Means-end approach a method used to identify the ends consumers aim to satisfy
</p>
<p>and the means (consumption) they use to do so.
</p>
<p>Measure of sampling adequacy (MSA) See Kaiser-Meyer-Olkin criterion.
</p>
<p>Measurement scaling refers to (1) the level at which a variable is measured
</p>
<p>(nominal, ordinal, interval, or ratio scale) and (2) the general act of using a set
</p>
<p>of variables to measure a construct.
</p>
<p>Measures of centrality are statistical indices of a typical or average value of a list
</p>
<p>of numbers. There are two main types of measures of centrality, the median and
</p>
<p>the mean.
</p>
<p>Measures of dispersion provide researchers with information about the variability
of the data (i.e., how far the values are spread out). There are four main types of
</p>
<p>measures of dispersion: the range, interquartile range, variance, and standard
</p>
<p>deviation.
</p>
<p>Median is a value that separates the lowest 50% of values from the highest 50% of
</p>
<p>values.
</p>
<p>Middle response styles a systematic way of responding to survey items describing
</p>
<p>respondents&rsquo; tendency to choose the midpoints of a response scale.
</p>
<p>Minto principle a guideline for presentations that starts with the conclusion,
raising questions in the audience&rsquo;s mind about the way this conclusion was
</p>
<p>reached. The presenter subsequently explains the steps involved in the analysis.
</p>
<p>Missing at random (MAR) is a missing values pattern in which the probability
</p>
<p>that data points are missing varies from respondent to respondent.
</p>
<p>Missing completely at random (MCAR) is a missing values pattern in which the
</p>
<p>probability that data points are missing is unrelated to any other measured
</p>
<p>variable and to the variable with the missing values.
</p>
<p>Missing data occur when entire observations are missing (survey non-response) or
respondents have not answered all the items (item non-response).
</p>
<p>Mixed mode is the act of combining different ways of administering surveys.
</p>
<p>Moderation analysis involves assessing whether the effect of an independent
</p>
<p>variable on a dependent variable depends on the values of a third variable,
</p>
<p>referred to as a moderator variable.
</p>
<p>(Multi)collinearity is a data issue that arises in regression analysis when two or
</p>
<p>more independent variables are highly correlated.
</p>
<p>Multi-item construct is a measurement of an abstract concept that uses several
items.
</p>
<p>Multinomial logistic regression is a type of regression analysis used when the
</p>
<p>dependent variable is nominal and takes more than two values.
</p>
<p>400 Glossary</p>
<p/>
</div>
<div class="page"><p/>
<p>Multiple imputation is a simulation-based statistical technique that replaces
</p>
<p>missing observations with a set of possible values (as opposed to a single
</p>
<p>value) representing the uncertainty about the missing data&rsquo;s true value.
</p>
<p>Multiple regression is a type of regression analysis that includes multiple inde-
</p>
<p>pendent variables.
</p>
<p>Mystery shopping is a type of observational study in which a trained researcher
</p>
<p>visits a store or restaurant and consumes their products/services.
</p>
<p>Nested models are simpler versions of a complex model.
Net Promoter Score (NPS) is a measure of customer loyalty that uses the single
</p>
<p>question: &ldquo;How likely are you to recommend our company/product/service to a
</p>
<p>friend or colleague?&rdquo;
</p>
<p>Nomological validity is the degree to which a construct behaves as it should in
</p>
<p>a system of related constructs.
</p>
<p>Nonhierarchical clustering methods see Partitioning methods.
</p>
<p>Nonparametric tests are statistical tests for hypothesis testing that do not assume
</p>
<p>a specific distribution of the data (typically a normal distribution).
</p>
<p>Non-probability sampling is a sampling technique that does not give every
</p>
<p>individual in the population an equal chance of being included in the sample.
</p>
<p>The resulting sample is not representative of the population.
</p>
<p>Nonrandom missing is a missing values pattern in which the probability that data
</p>
<p>points are missing depends on the variable and on other unobserved factors.
</p>
<p>Null and alternative hypothesis the null hypothesis (indicated as H0) is a state-
</p>
<p>ment expecting no difference or no effect. The alternative hypothesis (indicated
</p>
<p>as H1) is the hypothesis against which the null hypothesis is tested.
</p>
<p>Oblimin rotation is a popular oblique rotation method used in principal compo-
</p>
<p>nent and factor analysis and principal component analysis.
</p>
<p>Oblique rotation is a technique used to facilitate the interpretation of the factor
</p>
<p>solution in which the independence of a factor to all other factors is not
</p>
<p>maintained.
</p>
<p>Observation is an object, such as a customer, a company, or a country, in statisti-
</p>
<p>cal analysis. Also referred to as case.
</p>
<p>Observational studies are procedures for gathering data in which the researcher
observes people&rsquo;s behavior in a certain context. Observational studies are
</p>
<p>normally used to understand what people are doing rather than why they are
</p>
<p>doing it.
</p>
<p>Omega-squared (ω2) is a statistic used in an ANOVA to describe the ratio of the
</p>
<p>between-group variation to the total variation, thereby indicating the variance
</p>
<p>accounted for by the data. It is commonly used for sample sizes of 50 or less and
</p>
<p>corresponds to the adjusted R2 of regression analysis. Omega squared is also
</p>
<p>used to indicate effect sizes of individual variables in regression analysis.
</p>
<p>One-sample t-test is a parametric test used to compare one mean with a given
</p>
<p>value.
</p>
<p>One-tailed tests are a class of statistical tests frequently used when the hypothesis
</p>
<p>is expressed directionally (i.e., &lt; or &gt;). The region of rejection is on one side of
</p>
<p>the sampling distribution.
</p>
<p>Glossary 401</p>
<p/>
</div>
<div class="page"><p/>
<p>One-way ANOVA is a type of ANOVA that involves a single metric dependent
</p>
<p>variable and one factor variable with three (or more) levels.
</p>
<p>Open-ended questions are a type of question format that provides little or no
</p>
<p>structure for respondents&rsquo; answers. Generally, the researcher asks a question and
</p>
<p>the respondent writes down his or her answer in a box. Also referred to as
</p>
<p>verbatim items.
</p>
<p>Operationalization is the process of defining a set of variables to measure a
</p>
<p>construct. The process defines latent concepts and allows them to be measured
</p>
<p>empirically.
</p>
<p>Ordinary least squares (OLS) is the estimation approach commonly used in
</p>
<p>regression analysis and involves minimizing the squared deviations from the
</p>
<p>observations to the regression line (i.e., the residuals).
</p>
<p>Orthogonal rotation is a technique used to facilitate the interpretation of a factor
</p>
<p>solution in which a factor&rsquo;s independence is maintained from all other factors.
</p>
<p>The correlation between the factors is determined as zero.
</p>
<p>Outliers are observations that differ substantially from other observations in
respect of one or more characteristics.
</p>
<p>Paired samples t-test is a statistical procedure used to determine whether there is
</p>
<p>a significant mean difference between observations measured at two points
</p>
<p>in time.
</p>
<p>Parallel analysis is a statistic used in principal component and factor analysis to
</p>
<p>determine the number of factors to extract from the data. According to this
</p>
<p>criterion, researchers should extract all factors whose eigenvalues are larger than
</p>
<p>those derived from randomly generated data with the same sample size and
</p>
<p>number of variables.
</p>
<p>Parametric tests are statistical tests that assume a specific data distribution
</p>
<p>(typically normal).
</p>
<p>Partial least squares structural equation modeling (PLS-SEM) is a variance-
</p>
<p>based method to estimate structural equation models. The goal is to maximize
</p>
<p>the explained variance of the dependent latent variables.
</p>
<p>Partial sums of squares is a statistic in an ANOVA indicating the additional
</p>
<p>portion of variance explained when another variable is added to the analysis.
</p>
<p>Partitioning method is a group of clustering procedures that does not establish a
</p>
<p>treelike structure of objects and clusters, but exchanges objects between clusters
</p>
<p>to optimize a certain goal criterion. The most popular type of partitioning
</p>
<p>method is k-means.
</p>
<p>Path diagram is a visual representation of expected relationships tested in a
</p>
<p>structural equation modeling analysis.
</p>
<p>Personal interview is an interview technique that involves face-to-face contact
</p>
<p>between the interviewer and the respondent. Also referred to as face-to-face
</p>
<p>interviews.
</p>
<p>Pie chart displays the relative frequencies of a variable&rsquo;s values.
</p>
<p>Population is a group of objects (e.g., consumers, companies, or products) that a
</p>
<p>researcher wants to assess.
</p>
<p>402 Glossary</p>
<p/>
</div>
<div class="page"><p/>
<p>Post hoc tests are a group of tests used for paired comparisons in an ANOVA. Post
</p>
<p>hoc tests maintain the familywise error rate (i.e., they prevent excessive type I
</p>
<p>error).
</p>
<p>Power of a test represents the probability of rejecting a null hypothesis when it is
</p>
<p>in fact false. In other words, the power of a statistical test is the probability of
</p>
<p>rendering an effect significant when it is indeed significant (defined by 1 � β,
</p>
<p>where β is the probability of a type II error).
</p>
<p>Practical significance refers to whether differences or effects are large enough to
influence decision-making processes.
</p>
<p>Predictive validity measures how well one measure predicts the outcome of
</p>
<p>another measure when both are measured at a later point in time.
</p>
<p>Primary data are data gathered for a specific research project.
</p>
<p>Principal axis factoring See Factor analysis.
</p>
<p>Principal component analysis is a statistical procedure that uses correlation
</p>
<p>patterns among a set of indicator variables to derive factors that represent most
</p>
<p>of the original variables&rsquo; variance. Different from factor analysis, the procedure
</p>
<p>uses all the variance in the variables.
</p>
<p>Principal components are linear composites of original variables that reproduce
</p>
<p>the original variables&rsquo; variance as well as possible.
</p>
<p>Principal factor analysis See Factor analysis.
</p>
<p>Probability sampling is a sampling technique that gives every individual in the
</p>
<p>population an equal chance, different from zero, of being included in the sample.
</p>
<p>Profiling is a step in market segmentation that identifies observable variables (e.g.,
</p>
<p>demographics) that characterize the segments.
</p>
<p>Projective technique is a special type of testing procedure, usually used as part of
</p>
<p>in-depth interviews. This technique provides the participants with a stimulus
</p>
<p>(e.g., pictures, words) and then gauges their responses (e.g., through sentence
</p>
<p>completion).
</p>
<p>Promax rotation is a popular oblique rotation method used in principal compo-
</p>
<p>nent and factor analysis and principal component analysis.
</p>
<p>p-value is the probability of erroneously rejecting a true null hypothesis in a given
</p>
<p>statistical test.
</p>
<p>Pyramid structure for presentations See Minto principle.
</p>
<p>Qualitative data are audio, pictorial, or textual information that researchers use to
</p>
<p>answer research questions.
</p>
<p>Qualitative research is primarily used to gain an understanding of why certain
</p>
<p>things happen. It can be used in an exploratory context by defining problems in
</p>
<p>more detail or by developing hypotheses to be tested in subsequent research.
</p>
<p>Quantitative data are data to which numbers are assigned to represent specific
</p>
<p>characteristics.
</p>
<p>R2 See Coefficient of determination.
</p>
<p>Ramsey&rsquo;s RESET test is a test for linearity used in regression analysis.
</p>
<p>Range standardization is a type of scale transformation in which the values of a
</p>
<p>scale are standardized to a specific range that the researcher has set.
</p>
<p>Glossary 403</p>
<p/>
</div>
<div class="page"><p/>
<p>Range is the difference between the highest and the lowest value in a variable
</p>
<p>measured, at least, on an ordinal scale.
</p>
<p>Rank order scale is an ordinal scale that asks respondents to rank a set of objects
</p>
<p>or characteristics in terms of, for example, importance, preference, or similarity.
</p>
<p>Reflective constructs is a type of measurement in which the indicators are con-
</p>
<p>sidered manifestations of the underlying construct.
</p>
<p>Regression method is a procedure to generate factor scores in principal compo-
</p>
<p>nent analysis. The resulting factor scores have a zero mean and unit standard
</p>
<p>deviation.
</p>
<p>Regression sum of squares quantifies the difference between the regression line
</p>
<p>and the line indicating the average. It represents the variation in the data that the
</p>
<p>regression analysis explains.
</p>
<p>Reliability is the degree to which a measure is free from random error.
</p>
<p>Reliability analysis is an important element of a confirmatory factor analysis and
</p>
<p>essential when working with measurement scales. See Reliability.
</p>
<p>Research design describes the general approach to answer a research question
related to a marketing opportunity or problem. There are three broad types of
</p>
<p>research design: exploratory research, descriptive research, and causal research.
</p>
<p>Residual is the unexplained variance in a regression model. Also referred to as
</p>
<p>disturbance term.
</p>
<p>Reverse-scaled items are items whose statement (if a Likert scale is used) or word
</p>
<p>pair (if a semantic differential scale is used) is reversed when compared to the
</p>
<p>other items in the set.
</p>
<p>Robust regression is a variant of regression analysis used when heteroskedasticity
is present.
</p>
<p>Russell and Rao coefficient is a similarity coefficient used in cluster analysis.
</p>
<p>Sample size is the number of observations drawn from a population.
</p>
<p>Sampling error occurs when the sample and population structure differ on rele-
</p>
<p>vant characteristics.
</p>
<p>Sampling is the process through which objects are selected from a population.
</p>
<p>Scale development is the process of defining a set of variables to measure a
</p>
<p>construct and which follows an iterative process with several steps and feedback
</p>
<p>loops. Also referred to as operationalization, or, in the case of an index, index
</p>
<p>construction.
</p>
<p>Scale transformation is the act of changing a variable&rsquo;s values to ensure compa-
</p>
<p>rability with other variables or to make the data suitable for analysis.
</p>
<p>Scanner data are collected at the checkout of a supermarket where details about
</p>
<p>each product sold are entered into a database.
</p>
<p>Scatter plot is a graph that represents the relationship between two variables, thus
</p>
<p>portraying the joint values of each observation in a two-dimensional graph.
</p>
<p>Scree plot is a graph used in principal component and factor analysis that plots the
</p>
<p>number of factors against the eigenvalues, resulting in a distinct break (elbow)
</p>
<p>that indicates the number of factors to extract. Following the same principle, the
</p>
<p>404 Glossary</p>
<p/>
</div>
<div class="page"><p/>
<p>scree plot is also used in hierarchical cluster analysis to plot the number of
</p>
<p>clusters against the distances at which objects were merged.
</p>
<p>Secondary data are data that have already been gathered, often for a different
</p>
<p>research purpose and some time ago. Secondary data comprise internal second-
</p>
<p>ary data, external secondary data, or a mix of both.
</p>
<p>Segment specialists are companies that concentrate on specific market segments,
</p>
<p>such as a particular industry or type of customer.
</p>
<p>Self-contained figure is a graph in a market research report that should be
numbered sequentially and have a meaningful title so that it can be understood
</p>
<p>without reading the text.
</p>
<p>Self-contained table is a table in a market research report that should be numbered
</p>
<p>sequentially and have a meaningful title so that it can be understood without
</p>
<p>reading the text.
</p>
<p>Semantic differential scales is a type of answering scale that comprises opposing
</p>
<p>pairs of words, normally adjectives (e.g., young/old, masculine/feminine)
</p>
<p>constituting the endpoints of the scale. Respondents then indicate how well
</p>
<p>one of the word in each pair describes how he or she feels about the object to
</p>
<p>be rated (e.g., a company or brand).
</p>
<p>Sentence completion is a type of projective technique that provides respondents
</p>
<p>with beginnings of sentences that they have to complete in ways that are
</p>
<p>meaningful to them.
</p>
<p>Sequential sums of squares is a statistic in an ANOVA that indicates the addi-
</p>
<p>tional portion of variance explained when a set of variables is added to the
</p>
<p>analysis.
</p>
<p>Shapiro-Wilk test is a test for normality (i.e., whether the data are normally
</p>
<p>distributed).
</p>
<p>Significance level is the probability that an effect is incorrectly assumed when
</p>
<p>there is in fact none. The researcher sets the significance level prior to the
</p>
<p>analysis.
</p>
<p>Simple matching coefficient is a similarity coefficient used in cluster analysis.
</p>
<p>Simple regression is the simplest type of regression analysis with one dependent
</p>
<p>and one independent variable.
</p>
<p>Single-item constructs is a measurement of a concept that uses only one item.
</p>
<p>Single linkage is a linkage algorithm in hierarchical clustering methods in which
</p>
<p>the distance between two clusters corresponds to the shortest distance between
</p>
<p>any two members in the two clusters.
</p>
<p>Skewed data occur if a variable is asymmetrically distributed. A positive skew
</p>
<p>(also called right skewed) occurs when many observations are concentrated on
</p>
<p>the left side of the distribution, producing a long right tail (the opposite is called
</p>
<p>negative skew or left skewed).
</p>
<p>Social desirability bias occurs when respondents provide socially desirable
</p>
<p>answers (e.g., by reporting higher or lower incomes than are actually true) or
</p>
<p>Glossary 405</p>
<p/>
</div>
<div class="page"><p/>
<p>take a position that they believe society favors (e.g., not smoking or drinking).
</p>
<p>Social media analytics are methods for analyzing social networking data and
</p>
<p>comprise text mining, social network analysis, and trend analysis.
</p>
<p>Social networking data reflect how people would like others to perceive them
</p>
<p>and, thus, indicate consumers&rsquo; intentions. Product or company-related social
</p>
<p>networking data are of specific interest to market researchers.
</p>
<p>Specialized service firms are market research companies that focus on particular
</p>
<p>products, markets, or market research techniques.
</p>
<p>Split-half reliability is a type of reliability assessment in which scale items are
</p>
<p>divided into halves, and the scores of the halves are correlated.
</p>
<p>Split-sample validation involves splitting the dataset into two samples, running
</p>
<p>the analysis on both samples, and comparing the results.
</p>
<p>Stability of the measurement See Test-retest reliability.
</p>
<p>Standard deviation describes the sample distribution values&rsquo; variability from the
</p>
<p>mean. It is the square root of the variance and, therefore, a variant.
</p>
<p>Standard error is the sampling distribution of a statistic&rsquo;s standard deviation,
mostly from the mean.
</p>
<p>Standardized effects express the relative effects of differently measured indepen-
</p>
<p>dent variables in a regression analysis by expressing them in terms of standard
</p>
<p>deviation changes from the mean.
</p>
<p>Standardizing variables have been rescaled (typically to a zero mean and unit
</p>
<p>standard deviation) to facilitate comparisons between differently scaled
</p>
<p>variables.
</p>
<p>Stata computer package specializing in quantitative data analysis.
Statistical inference is the process of drawing conclusions about populations
</p>
<p>from data.
</p>
<p>Statistical significance occurs when an effect is so large that it is unlikely to have
</p>
<p>occurred by chance. Statistical significance depends on several factors, including
</p>
<p>the size of the effect, the variation in the sample data, and the number of
</p>
<p>observations.
</p>
<p>Straight-line distance See Euclidean distance.
</p>
<p>Straight-lining occurs when a respondent marks the same response in almost all
the items.
</p>
<p>Structural equation modeling is a multivariate data analysis technique used to
</p>
<p>measure relationships between constructs, as well as between constructs and
</p>
<p>their associated indicators.
</p>
<p>Survey non-response occurs when entire responses are missing. Survey
</p>
<p>non-response rates are usually 75%&ndash;95%.
</p>
<p>Surveys are often used for gathering primary data. Designing surveys involves a
</p>
<p>six-step process: (1) Determine the survey goal, (2) determine the type of
</p>
<p>questionnaire required and the administration method, (3) decide on the
</p>
<p>questions and (4) the scale, (5) design the questionnaire, and (6) pretest and
</p>
<p>administer the questionnaire.
</p>
<p>Suspicious response patterns are issues in response styles in respect of straight-
</p>
<p>lining and inconsistent answers that a researcher needs to address in the analysis.
</p>
<p>406 Glossary</p>
<p/>
</div>
<div class="page"><p/>
<p>Syndicated data are data sold to multiple clients, allowing them to compare key
</p>
<p>measures with those of the rest of the market.
</p>
<p>Telephone interviews allow researchers to collect data quickly and facilitate
</p>
<p>open-ended responses, although not as well as personal interviews.
</p>
<p>Test markets are a type of field experiment that evaluates a new product or
</p>
<p>promotional campaign under real market conditions.
</p>
<p>Test statistic is calculated from the sample data to assess the strength of the
</p>
<p>evidence in support of the null hypothesis.
</p>
<p>Test-retest reliability is a type of reliability assessment in which the researcher
</p>
<p>obtains repeated measurement of the same respondent or group of respondents,
</p>
<p>using the same instrument and under similar conditions. Also referred to as
</p>
<p>stability of the measurement.
</p>
<p>Ties are identical values in a distance matrix used in cluster analysis.
</p>
<p>Total sum of squares quantifies the difference between the observations and the
</p>
<p>line indicating the average.
</p>
<p>Transforming data is an optional step in workflow of data, involving variable
respecification and scale transformation.
</p>
<p>Treatments are elements in an experiment that are used to manipulate the
</p>
<p>participants by subjecting them to different situations. A simple form of treat-
</p>
<p>ment could be an advertisement with and without humor.
</p>
<p>t-test is the most popular type of parametric test for comparing a mean with a given
</p>
<p>standard and for comparing the means of independent samples (independent
</p>
<p>samples t-test) or the means of paired samples (paired samples t-test).
</p>
<p>Tukey&rsquo;s honestly significant difference test is a popular post hoc test used in an
ANOVA that controls for type I errors, but is limited in terms of statistical
</p>
<p>power. Often simply referred to as Tukey&rsquo;s method.
</p>
<p>Tukey&rsquo;s method See Tukey&rsquo;s honestly significant difference test.
</p>
<p>Two-sample t-test is the most popular type of parametric test for comparing the
</p>
<p>means of independent or paired samples.
</p>
<p>Two-tailed tests are a class of statistical tests frequently used when the hypothesis
</p>
<p>is not expressed directionally (i.e., 6&frac14;). The region of rejection is on two sides of
</p>
<p>the sampling distribution.
</p>
<p>Two-way ANOVA is a type of ANOVA that involves a single metric dependent
</p>
<p>variable and two factor variables with three (or more) levels.
</p>
<p>Type I error occurs when erroneously rejecting a true null hypothesis. Also
</p>
<p>referred to as α error.
</p>
<p>Type II error occurs when erroneously accepting a false null hypothesis. Also
</p>
<p>referred to as β error.
</p>
<p>Unbalanced scale describes a scale with an unequal number of positive and
</p>
<p>negative scale categories.
</p>
<p>Uniqueness is a statistic used in principal component analysis and factor analy-
</p>
<p>sis that indicates the proportion of a variable&rsquo;s variance that the factors do not
</p>
<p>capture. The uniqueness equals 1 &ndash; communality.
</p>
<p>Glossary 407</p>
<p/>
</div>
<div class="page"><p/>
<p>Unit of analysis is the level at which a variable is measured. Typical measurement
</p>
<p>levels include that of the respondents, customers, stores, companies, or
</p>
<p>countries.
</p>
<p>Univariate statistics are statistics that describe the centrality and dispersion of a
</p>
<p>single variable.
</p>
<p>Unstandardized effects express the absolute effects that one-unit increases in the
</p>
<p>independent variables have on the dependent variable in a regression analysis.
</p>
<p>Validation sample is a random subsample of the original dataset used for valida-
tion testing.
</p>
<p>Validity is the degree to which a researcher measures what (s)he wants to measure.
</p>
<p>It is the degree to which a measure is free from systematic error.
</p>
<p>Variable represents a measurable characteristic whose value can change.
</p>
<p>Variable names should be clear and short so that they can be read in the dialog
</p>
<p>boxes (e.g., loyalty1, loyalty2, etc.).
</p>
<p>Variable respecification involves transforming data to create new variables or to
</p>
<p>modify existing ones.
</p>
<p>Variance a measure of dispersion computed by the sum of the squared differences
</p>
<p>of each value and a variable&rsquo;s mean, divided by the sample size minus 1.
</p>
<p>Variance inflation factor (VIF) quantifies the degree of collinearity between the
</p>
<p>independent variables in a regression analysis.
</p>
<p>Variance ratio criterion is a statistic used in cluster analysis to determine the
</p>
<p>number of clusters. The criterion compares the within- and between-cluster
</p>
<p>variation of different numbers of clusters.
</p>
<p>Varimax rotation is the most popular orthogonal rotation method used in princi-
pal component analysis and factor analysis.
</p>
<p>Verbatim items See Open-ended questions.
</p>
<p>Visual aids include overhead transparencies, flip charts, or slides (e.g.,
</p>
<p>PowerPoint or Prezi) that help emphasize important points and facilitate the
</p>
<p>communication of difficult ideas in a presentation of market research results.
</p>
<p>Visual analogue scale is a type of answering scale in which respondents use levers
</p>
<p>that allow scaling on a continuum. This scale does not provide response
</p>
<p>categories.
</p>
<p>Ward&rsquo;s linkage is a linkage algorithm in hierarchical clustering methods that
</p>
<p>combines those objects whose merger increases the overall within-cluster vari-
</p>
<p>ance by the smallest possible degree.
</p>
<p>Web surveys are less expensive to administer and can be fast in terms of data
</p>
<p>collection, because they can be set up very quickly. Also referred to as computer-
</p>
<p>assisted web interviews (CAWI).
</p>
<p>Weighted average linkage is a variant of the average linkage algorithm used in
</p>
<p>cluster analysis that weights the distances according to the number of objects in
</p>
<p>the cluster.
</p>
<p>Welch correction is a statistical test used in an ANOVA to assess the significance
</p>
<p>of the overall model when the group variances differ significantly and the groups
</p>
<p>differ in size.
</p>
<p>408 Glossary</p>
<p/>
</div>
<div class="page"><p/>
<p>White&rsquo;s test is a statistical test that detects the presence of heteroskedasticity in
</p>
<p>regression analysis.
</p>
<p>Wilcoxon matched-pairs signed-rank test is the nonparametric equivalent of the
</p>
<p>paired samples t-test.
</p>
<p>Wilcoxon signed-rank test is the nonparametric equivalent of the independent
</p>
<p>samples t-test.
</p>
<p>Wilcoxon test is the nonparametric equivalent of the one sample t-test.
</p>
<p>Workflow is a strategy to keep track of the entering, cleaning, describing, and
transforming of data.
</p>
<p>z-standardization is a type of scale transformation in which the values of a scale
</p>
<p>are standardized to a zero mean and unit standard deviation.
</p>
<p>z-test is any statistical test for which the distribution of the test statistic under the
</p>
<p>null hypothesis can be approximated by a normal distribution.
</p>
<p>Glossary 409</p>
<p/>
</div>
<div class="page"><p/>
<p>Index
</p>
<p>A
</p>
<p>Acquiescence, 72, 101
</p>
<p>Adaptive questioning, 68
</p>
<p>Adjusted R2, 189, 234
</p>
<p>Agglomerative clustering, 322, 327
</p>
<p>Aggregating data, 33
</p>
<p>Aggregation, 122
</p>
<p>Agresti, A., 118
</p>
<p>Akaike information criterion (AIC), 235, 280
</p>
<p>α error, 158
</p>
<p>α-inflation, 180
</p>
<p>Alternative hypothesis, 156
</p>
<p>American Customer Satisfaction Index, 60
</p>
<p>Analysis of variance (ANOVA), 166
</p>
<p>Anti-image, 272
</p>
<p>Arithmetic mean, 113
</p>
<p>Armstrong, J.S., 385
</p>
<p>Autocorrelation, 230
</p>
<p>Average, 113
</p>
<p>Average linkage, 323, 324
</p>
<p>B
</p>
<p>Back-translation, 72
</p>
<p>Balanced scale, 77
</p>
<p>Bar chart, 111
</p>
<p>Bartlett method, 284
</p>
<p>Bayes information criterion (BIC), 235, 280
</p>
<p>Before-after design, 88
</p>
<p>Before-after experiment with a control
</p>
<p>group, 89
</p>
<p>Before measurement effect, 89
</p>
<p>β error, 158
</p>
<p>Between-group mean squares, 186
</p>
<p>Between-group variation, 182&ndash;183
</p>
<p>Big data, 57
</p>
<p>Binary logistic regression, 220
</p>
<p>Bivariate graphs, 115
</p>
<p>Bivariate statistics, 117
</p>
<p>Bonferroni correction, 187
</p>
<p>Box-and-whisker plot, 112
</p>
<p>Box plot, 112
</p>
<p>Breusch-Pagan, 228
</p>
<p>Bubble plot, 116
</p>
<p>C
</p>
<p>Calinski/Harabasz pseudo F, 341, 356
</p>
<p>Canberra distance, 335
</p>
<p>Case, 28
</p>
<p>Causality, 18
</p>
<p>Causal research, 18, 86
</p>
<p>Census, 43
</p>
<p>Centroid linkage, 323, 324
</p>
<p>Chaining effect, 324
</p>
<p>Chebychev distance, 334
</p>
<p>χ
2
-test, 108, 116
</p>
<p>CIA World Fact Book, 54
</p>
<p>City-block distance, 334
</p>
<p>Closed-ended questions, 71
</p>
<p>Cluster analysis, 314
</p>
<p>Clustering variables, 314
</p>
<p>Clusters, 314
</p>
<p>Cluster sampling, 45
</p>
<p>Codebook, 123
</p>
<p>Coefficient of determination, 232
</p>
<p>Collinearity, 221, 276
</p>
<p>Communality, 276, 277
</p>
<p>Company records, 53
</p>
<p>Complete linkage, 323, 324
</p>
<p>Composite measure, 29, 121
</p>
<p>Computer-assisted personal
</p>
<p>interviews (CAPI), 66
</p>
<p>Computer-assisted self-interviews (CASI), 66
</p>
<p>Computer-assisted telephone interviews
</p>
<p>(CATI), 67
</p>
<p>Computer-assisted web interviews (CAWI), 67
</p>
<p>Confidence interval, 189
</p>
<p># Springer Nature Singapore Pte Ltd. 2018
</p>
<p>E. Mooi et al., Market Research, Springer Texts in Business and Economics,
</p>
<p>DOI 10.1007/978-981-10-5218-7
</p>
<p>411</p>
<p/>
</div>
<div class="page"><p/>
<p>Confirmatory factor analysis, 266
</p>
<p>Constant, 28, 217
</p>
<p>Constant sum scale, 74
</p>
<p>Constructs, 28, 121
</p>
<p>Construct score, 121
</p>
<p>Construct validity, 39
</p>
<p>Consulting firms, 55
</p>
<p>Content validity, 40
</p>
<p>Contingency coefficient, 119
</p>
<p>Contingency tables, 116
</p>
<p>Convenience sampling, 46
</p>
<p>Conversion rate, 56
</p>
<p>Correlation, 117
</p>
<p>Correlation residuals, 283
</p>
<p>Covariance, 117
</p>
<p>Covariance-based structural equation
</p>
<p>modeling, 289
</p>
<p>Cox, N. J., 378
</p>
<p>Cramer&rsquo;s V, 119, 360
</p>
<p>Criterion validity, 40, 319
</p>
<p>Cronbach&rsquo;s alpha, 289
</p>
<p>Crosstabs, 116
</p>
<p>Cross-validation, 238
</p>
<p>Customer relationship management, 53
</p>
<p>D
</p>
<p>Data entry errors, 102
</p>
<p>Degrees of freedom, 171
</p>
<p>Dendrogram, 341, 354
</p>
<p>Dependence of observations, 34, 35
</p>
<p>Dependent variables, 35, 216
</p>
<p>Depth interviews, 15
</p>
<p>Descriptive research, 17
</p>
<p>Directional hypothesis, 157
</p>
<p>Directly observed qualitative data, 82
</p>
<p>Discriminant validity, 40
</p>
<p>Disturbance term, 218
</p>
<p>Divisive clustering, 322
</p>
<p>&ldquo;Don&rsquo;t know&rdquo; option, 77
</p>
<p>Double-barreled questions, 71
</p>
<p>Duda-Hart index, 342, 343, 357
</p>
<p>Dummies, 120
</p>
<p>Dummy variables, 120, 256
</p>
<p>Dunnett&rsquo;s method, 188
</p>
<p>Durbin-Watson (D-W) test, 230
</p>
<p>E
</p>
<p>Effect size, 188
</p>
<p>Eigenvalues, 276
</p>
<p>Eigenvectors, 275
</p>
<p>Enterprise Resource Planning, 53
</p>
<p>Equidistance, 37
</p>
<p>Equidistant scale, 78
</p>
<p>Error, 218
</p>
<p>Error sum of squares, 232
</p>
<p>Estimation sample, 237
</p>
<p>Eta squared, 188
</p>
<p>Ethics, 388
</p>
<p>Ethnographies, 16
</p>
<p>Ethnography, 63
</p>
<p>Euclidean distance, 333
</p>
<p>Existing research studies, 54
</p>
<p>Experimental design, 87
</p>
<p>Experimental research, 86&ndash;89
</p>
<p>Experiments, 86
</p>
<p>Expert validity, 40
</p>
<p>Exploratory factor analysis, 266
</p>
<p>Exploratory research, 14
</p>
<p>External secondary data, 54
</p>
<p>External validity, 87
</p>
<p>Extraneous variables, 86
</p>
<p>Extreme response styles, 101
</p>
<p>F
</p>
<p>Facebook, 56
</p>
<p>Face-to-face interviews, 66
</p>
<p>Face validity, 40
</p>
<p>Factor analysis, 266
</p>
<p>Factor-cluster segmentation, 320
</p>
<p>Factor extraction, 275&ndash;276
</p>
<p>Factor levels, 180
</p>
<p>Factor loading, 276, 295
</p>
<p>Factor rotation, 280
</p>
<p>Factors, 266
</p>
<p>Factor scores, 283
</p>
<p>Factor variable, 163, 180
</p>
<p>Factor weights, 275
</p>
<p>Familywise error rate, 180, 187
</p>
<p>Field experiments, 21, 87
</p>
<p>Field service, 7
</p>
<p>Finlay, B., 118
</p>
<p>Focus groups, 15, 84
</p>
<p>Forced-choice scale, 76
</p>
<p>Formative constructs, 30
</p>
<p>Free-choice scale, 76
</p>
<p>Frequency table, 113
</p>
<p>F-test, 232
</p>
<p>F-test of sample variance, 164
</p>
<p>Full factorial design, 86
</p>
<p>Full service providers, 6
</p>
<p>Funnel approach, 78
</p>
<p>Furthest neighbor, 323
</p>
<p>Fused market research, 34
</p>
<p>412 Index</p>
<p/>
</div>
<div class="page"><p/>
<p>G
</p>
<p>Global representativeness, 42
</p>
<p>Governments, 54
</p>
<p>Gower&rsquo;s dissimilarity coefficient, 339, 340
</p>
<p>H
</p>
<p>Hershberger, S.L., 284
</p>
<p>Heywood cases, 299
</p>
<p>Hierarchical clustering, 322
</p>
<p>Histogram, 111
</p>
<p>Hybrid market research, 34
</p>
<p>Hypothesis, 16, 154
</p>
<p>I
</p>
<p>Inconsistent answers, 101
</p>
<p>Independent observations, 35
</p>
<p>Independent samples, 163
</p>
<p>Independent samples t-test, 165, 175
</p>
<p>Independent variables, 35, 216
</p>
<p>In-depth interviews, 82
</p>
<p>Index, 29, 121
</p>
<p>Index construction, 29
</p>
<p>Indicators, 28
</p>
<p>Indirectly observed qualitative data, 82
</p>
<p>Instagram, 56
</p>
<p>Interaction effect, 190
</p>
<p>Intercept, 217
</p>
<p>Internal consistency reliability, 41, 288
</p>
<p>Internal secondary data, 53
</p>
<p>Internal validity, 87
</p>
<p>Internet data, 56
</p>
<p>Interquartile range, 114
</p>
<p>Inter-rater reliability, 41
</p>
<p>Interval scale, 37
</p>
<p>Interviewer bias, 66, 68
</p>
<p>Interviewer fraud, 100
</p>
<p>Item content, 70
</p>
<p>Item non-response, 105
</p>
<p>Items, 28
</p>
<p>Item wording, 71
</p>
<p>J
</p>
<p>Jaccard coefficient (JC), 338, 339
</p>
<p>Judgmental sampling, 45
</p>
<p>K
</p>
<p>Kaiser criterion, 278, 295
</p>
<p>Kaiser&ndash;Meyer&ndash;Olkin (KMO), 272
</p>
<p>Kendall&rsquo;s tau, 119
</p>
<p>KISS principle, 370
</p>
<p>k-means, 329, 330
</p>
<p>k-means++, 333
</p>
<p>k-medians, 333
</p>
<p>k-medoids, 333
</p>
<p>Kruskal-Wallis H test, 167
</p>
<p>Kruskal-Wallis rank test, 167
</p>
<p>L
</p>
<p>L1, 334
</p>
<p>Lab experiments, 21, 87
</p>
<p>Laddering, 83
</p>
<p>Latent concepts, 28
</p>
<p>Latent root criterion, 278
</p>
<p>Latent variable, 28
</p>
<p>Lead users, 103
</p>
<p>Left-skewed, 122
</p>
<p>Left-tailed hypothesis, 158
</p>
<p>Levels of measurement, 35
</p>
<p>Levene&rsquo;s test, 164
</p>
<p>Likert, 98
</p>
<p>Likert scale, 73
</p>
<p>Limited service providers, 7
</p>
<p>Line chart, 116
</p>
<p>Linfinity, 334
</p>
<p>Linkage algorithm, 323
</p>
<p>LinkedIn, 56
</p>
<p>Listwise deletion, 105, 108, 109
</p>
<p>Literature databases, 55
</p>
<p>Little&rsquo;s MCAR test, 107
</p>
<p>Local optimum, 332
</p>
<p>Log transformation, 122
</p>
<p>Lower adjacent value, 112
</p>
<p>L2 squared, 333
</p>
<p>M
</p>
<p>Mahalanobis distance, 335
</p>
<p>Mail surveys, 68
</p>
<p>Main effect, 190
</p>
<p>Manhattan metric, 334
</p>
<p>Manipulation checks, 86
</p>
<p>Mann-Whitney U test, 167
</p>
<p>Marginal effect, 206
</p>
<p>Marginal means, 155
</p>
<p>Marketing opportunities, 12
</p>
<p>Marketing symptoms, 12
</p>
<p>Market research firms, 54
</p>
<p>Market segmentation, 17, 314
</p>
<p>Matching, 337
</p>
<p>Matching coefficients, 337
</p>
<p>Mean, 113
</p>
<p>Index 413</p>
<p/>
</div>
<div class="page"><p/>
<p>Means-end approach, 83
</p>
<p>Measurement error, 38
</p>
<p>Measurement scaling, 35
</p>
<p>Measure of sampling adequacy (MSA), 272
</p>
<p>Measures of centrality, 113
</p>
<p>Measures of central tendency, 113
</p>
<p>Measures of dispersion, 114
</p>
<p>Median, 112, 113
</p>
<p>Middle response styles, 101
</p>
<p>Minto principle, 386
</p>
<p>Misresponse rates, 72
</p>
<p>Missing at random (MAR), 106
</p>
<p>Missing completely at random (MCAR), 105
</p>
<p>Missing data, 104
</p>
<p>Mixed methodology, 34
</p>
<p>Mixed mode, 69
</p>
<p>Mobile phone surveys, 67
</p>
<p>Moderation analysis, 237
</p>
<p>Multicollinearity, 221
</p>
<p>Multi-item constructs, 30
</p>
<p>Multi-item scale, 28
</p>
<p>Multinomial logistic regression, 220
</p>
<p>Multiple imputation, 105, 106, 108, 109
</p>
<p>Multiple regression, 218
</p>
<p>Mystery shopping, 63
</p>
<p>N
</p>
<p>Negative skew, 122
</p>
<p>Nested models, 234
</p>
<p>Net Promoter Score (NPS), 30
</p>
<p>Noise, 184
</p>
<p>Nominal scale, 36
</p>
<p>Non-directional hypothesis, 158
</p>
<p>Non-hierarchical clustering methods, 330
</p>
<p>Nonparametric tests, 154
</p>
<p>Non-probability sampling, 45
</p>
<p>Non-random missing (NRM), 107
</p>
<p>Normal probability plot, 165
</p>
<p>Null hypothesis, 156
</p>
<p>Number of answer categories, 74
</p>
<p>O
</p>
<p>Oblimin rotation, 281
</p>
<p>Oblique rotation, 281
</p>
<p>Observation, 28
</p>
<p>Observational studies, 15, 62
</p>
<p>Omega squared, 189
</p>
<p>One-sample t-test, 165, 169
</p>
<p>One-shot case study, 87
</p>
<p>One-tailed test, 168
</p>
<p>One-way ANOVA, 180
</p>
<p>Open-ended questions, 71
</p>
<p>Operationalization, 29
</p>
<p>Oral presentation, 368
</p>
<p>Ordinal scale, 36
</p>
<p>Ordinary least squares (OLS), 223
</p>
<p>Orthogonal rotation, 281
</p>
<p>Outliers, 102, 231
</p>
<p>Outside values, 112
</p>
<p>P
</p>
<p>Page requests, 56
</p>
<p>Paired samples, 163
</p>
<p>Paired samples t-test, 165, 178
</p>
<p>Parallel analysis, 279
</p>
<p>Parametric tests, 154
</p>
<p>Partial least squares structural equation
</p>
<p>modeling, 289
</p>
<p>Partial sums of squares, 183
</p>
<p>Partitioning clustering methods, 329
</p>
<p>Path diagram, 287
</p>
<p>Pearson&rsquo;s chi-squared, 360
</p>
<p>Pearson&rsquo;s correlation coefficient, 117
</p>
<p>Personal interviews, 66
</p>
<p>Phi, 119
</p>
<p>Pie chart, 113
</p>
<p>5-Point scales, 75
</p>
<p>7-Point scales, 75
</p>
<p>Population, 41
</p>
<p>Positive skew, 122
</p>
<p>Post hoc tests, 187
</p>
<p>Potential Ratings Index by Zip Markets
</p>
<p>(PRIZM), 346
</p>
<p>Power analysis, 160
</p>
<p>Power of a statistical test, 159
</p>
<p>Practical significance, 159
</p>
<p>Practice effects, 41
</p>
<p>Predictive validity, 40
</p>
<p>Presentations, 386
</p>
<p>Pretest, 80&ndash;82
</p>
<p>Primary data, 31, 62
</p>
<p>Principal axis factoring, 266
</p>
<p>Principal component analysis (PCA), 266
</p>
<p>Principal-component factor, 294
</p>
<p>Principal components, 275
</p>
<p>Principal factor analysis, 266
</p>
<p>Probability sampling, 43
</p>
<p>Profiling, 344
</p>
<p>Projective techniques, 15, 84
</p>
<p>Promax rotation, 281
</p>
<p>Pseudo T-squared, 343
</p>
<p>p-value, 172
</p>
<p>Pyramid structure, 386
</p>
<p>414 Index</p>
<p/>
</div>
<div class="page"><p/>
<p>Q
</p>
<p>Qualitative data, 32
</p>
<p>Qualitative research, 34, 82
</p>
<p>Qualtrics, 67
</p>
<p>Quantile plot, 165
</p>
<p>Quantitative data, 32
</p>
<p>Quantitative research, 34
</p>
<p>Quartile, 114
</p>
<p>Questionnaire design, 78
</p>
<p>Question order, 78
</p>
<p>Quota sampling, 46
</p>
<p>R
</p>
<p>R2, 189, 232
</p>
<p>Ramsey&rsquo;s RESET test, 227
</p>
<p>Random error, 38
</p>
<p>Random noise, 184
</p>
<p>Range, 114
</p>
<p>Rank order scales, 73
</p>
<p>Ratio scale, 37
</p>
<p>Recoding, 120
</p>
<p>Reflective constructs, 30
</p>
<p>Regression method, 284
</p>
<p>Regression sum of squares, 232
</p>
<p>Related factors, 190
</p>
<p>Reliability, 38
</p>
<p>Reliability analysis, 267
</p>
<p>Representative sample, 42
</p>
<p>Research design, 13
</p>
<p>Research problem, 12
</p>
<p>Residual, 218
</p>
<p>Respondent bias, 66
</p>
<p>Response categories, 76
</p>
<p>Response category labeling, 77
</p>
<p>Response rates, 81
</p>
<p>Reverse-scaled items, 72, 100
</p>
<p>Right-skewed, 122
</p>
<p>Right-tailed hypothesis, 158
</p>
<p>Robust regression, 226
</p>
<p>Russell and Rao coefficient, 338, 339
</p>
<p>S
</p>
<p>Sales reports, 53
</p>
<p>Sample size, 47
</p>
<p>Sampling, 41
</p>
<p>Sampling error, 44, 158
</p>
<p>Sampling frame, 43
</p>
<p>Scale development, 29
</p>
<p>Scale properties, 74
</p>
<p>Scale transformation, 121
</p>
<p>Scale types, 73
</p>
<p>Scanner data, 17
</p>
<p>Scatter plot, 104, 115
</p>
<p>Scheffé&rsquo;s method, 188
</p>
<p>Scree plot, 278
</p>
<p>Screening questions and Screeners, 78
</p>
<p>Search engines, 58
</p>
<p>Secondary data, 31
</p>
<p>Segment specialists, 7
</p>
<p>Self-selection, 88
</p>
<p>Semantic differential scales, 73, 98
</p>
<p>Sensory variables, 274
</p>
<p>Sentence completion, 84
</p>
<p>Sequential sums of squares, 183
</p>
<p>Sessions, 56
</p>
<p>Set the Scale, 73&ndash;78
</p>
<p>Shapiro-Wilk test, 164
</p>
<p>Shelf tests, 22
</p>
<p>Significance level, 154
</p>
<p>Simple matching (SM) coefficient, 337
</p>
<p>Simple random sampling, 44
</p>
<p>Simple regression, 218
</p>
<p>Single-item constructs, 30
</p>
<p>Single linkage, 323, 324
</p>
<p>Skewed data, 122
</p>
<p>Snowball sampling, 45
</p>
<p>Social desirability bias, 70
</p>
<p>Social media analytics, 56
</p>
<p>Social network analysis, 56
</p>
<p>Social networking data, 56
</p>
<p>Solomon four-group design, 89
</p>
<p>Spearman&rsquo;s correlation coefficient, 119
</p>
<p>Specialized service, 7
</p>
<p>Specific representativeness, 42
</p>
<p>Split-half reliability, 288
</p>
<p>Split-sample validation, 237
</p>
<p>Squared Euclidean distance, 333
</p>
<p>Stability of the measurement, 41
</p>
<p>Standard deviation, 115
</p>
<p>Standard error, 169, 218
</p>
<p>Standardized effects, 236
</p>
<p>Standardizing, 336&ndash;337
</p>
<p>Standardizing variables, 121
</p>
<p>Stata, 124
</p>
<p>Statistical significance, 154
</p>
<p>Straight line distance, 333
</p>
<p>Straight-lining, 100
</p>
<p>Strata, 44
</p>
<p>Stratified sampling, 44
</p>
<p>Structural equation modeling, 267
</p>
<p>Survey non-response, 104
</p>
<p>Surveys, 64
</p>
<p>Suspicious response patterns, 100
</p>
<p>Syndicated data, 6, 54
</p>
<p>Index 415</p>
<p/>
</div>
<div class="page"><p/>
<p>Systematic error, 38
</p>
<p>Systematic sampling, 44
</p>
<p>T
</p>
<p>Tables, 115
</p>
<p>Telephone interviews, 66
</p>
<p>Ten-point scales, 75
</p>
<p>Testing effect, 89
</p>
<p>Test markets, 22, 63
</p>
<p>Test-retest reliability, 41, 288
</p>
<p>Test statistic, 155, 168
</p>
<p>Text mining, 56
</p>
<p>Ties, 345
</p>
<p>Total sum of the squares, 232
</p>
<p>Tracking cookie, 56
</p>
<p>Trade associations, 54
</p>
<p>Transforming data, 120
</p>
<p>Treatments, 86
</p>
<p>Trend analysis, 56
</p>
<p>Tufte, E.R., 378
</p>
<p>Tukey&rsquo;s honestly significant
</p>
<p>difference test, 188
</p>
<p>Tukey&rsquo;s method, 188
</p>
<p>Tukey&rsquo;s statistic, 188
</p>
<p>Two-sample t-test, 165
</p>
<p>Two-tailed test, 168
</p>
<p>Two-way ANOVA, 180
</p>
<p>Type I, 158
</p>
<p>Type II, 158
</p>
<p>Types of research problems, 13
</p>
<p>U
</p>
<p>Unbalanced scale, 78
</p>
<p>Unexplained variation, 183
</p>
<p>Uniqueness, 276, 277, 295
</p>
<p>Unit of analysis, 33
</p>
<p>Unit non-response, 104
</p>
<p>Univariate graphs, 110
</p>
<p>Univariate statistics, 113
</p>
<p>Univariate tables, 110
</p>
<p>Unstandardized effects, 236
</p>
<p>Upper adjacent value, 112
</p>
<p>V
</p>
<p>Vague quantifiers, 72
</p>
<p>Validation sample, 237
</p>
<p>Validity, 38
</p>
<p>Variable, 28
</p>
<p>Variable coding, 98
</p>
<p>Variable names, 98
</p>
<p>Variable respecification, 120
</p>
<p>Variance, 114
</p>
<p>Variance inflation factor (VIF), 221
</p>
<p>Variance ratio criterion, 341
</p>
<p>Varimax rotation, 281
</p>
<p>Verbatim items, 71
</p>
<p>Visual analogue scale, 75
</p>
<p>W
</p>
<p>Ward&rsquo;s linkage, 323, 324
</p>
<p>Web surveys, 67
</p>
<p>Weighted average linkage, 323
</p>
<p>Whisker, 112
</p>
<p>White&rsquo;s test, 228
</p>
<p>Wilcoxon&ndash;Mann&ndash;Whitney test, 167
</p>
<p>Wilcoxon matched-pairs signed-rank test, 167
</p>
<p>Wilcoxon rank-sum test, 167
</p>
<p>Wilcoxon signed-rank test, 167
</p>
<p>Within-group mean squares, 186
</p>
<p>Within-group variation, 183&ndash;184
</p>
<p>Workflow, 96
</p>
<p>Written report, 368
</p>
<p>Z
</p>
<p>z-scores, 121
</p>
<p>z-standardization, 121
</p>
<p>z-test, 170
</p>
<p>416 Index</p>
<p/>
</div>
<ul>	<li>Preface</li>
<ul>	<li>How to Use Mobile Tags</li>
	<li>How to Use This Book</li>
	<li>For Instructors</li>
</ul>
	<li>Acknowledgments</li>
<ul>	<li>References</li>
</ul>
	<li>Contents</li>
	<li>1: Introduction to Market Research</li>
<ul>	<li>1.1 Introduction</li>
	<li>1.2 What Is Market and Marketing Research?</li>
	<li>1.3 Market Research by Practitioners and Academics</li>
	<li>1.4 When Should Market Research (Not) Be Conducted?</li>
	<li>1.5 Who Provides Market Research?</li>
	<li>1.6 Review Questions</li>
	<li>1.7 Further Readings</li>
	<li>References</li>
</ul>
	<li>2: The Market Research Process</li>
<ul>	<li>2.1 Introduction</li>
	<li>2.2 Identify and Formulate the Problem</li>
	<li>2.3 Determine the Research Design</li>
<ul>	<li>2.3.1 Exploratory Research</li>
	<li>2.3.2 Uses of Exploratory Research</li>
	<li>2.3.3 Descriptive Research</li>
	<li>2.3.4 Uses of Descriptive Research</li>
	<li>2.3.5 Causal Research</li>
<ul>	<li>Box 2.1 Correlation Does Not Automatically Imply Causation</li>
</ul>
	<li>2.3.6 Uses of Causal Research</li>
</ul>
	<li>2.4 Design the Sample and Method of Data Collection</li>
	<li>2.5 Collect the Data</li>
	<li>2.6 Analyze the Data</li>
	<li>2.7 Interpret, Discuss, and Present the Findings</li>
	<li>2.8 Follow-Up</li>
	<li>2.9 Review Questions</li>
	<li>2.10 Further Readings</li>
	<li>References</li>
</ul>
	<li>3: Data</li>
<ul>	<li>3.1 Introduction</li>
	<li>3.2 Types of Data</li>
<ul>	<li>Box 3.1 Types of Constructs</li>
	<li>3.2.1 Primary and Secondary Data</li>
	<li>3.2.2 Quantitative and Qualitative Data</li>
<ul>	<li>Box 3.2 Coding Qualitative Data</li>
</ul>
</ul>
	<li>3.3 Unit of Analysis</li>
<ul>	<li>Box 3.3 Quantitative Research and Qualitative Research</li>
</ul>
	<li>3.4 Dependence of Observations</li>
	<li>3.5 Dependent and Independent Variables</li>
	<li>3.6 Measurement Scaling</li>
	<li>3.7 Validity and Reliability</li>
<ul>	<li>3.7.1 Types of Validity</li>
	<li>3.7.2 Types of Reliability</li>
</ul>
	<li>3.8 Population and Sampling</li>
<ul>	<li>Box 3.4 Do I Have a Representative Sample?</li>
	<li>Box 3.5 The US Census</li>
	<li>3.8.1 Probability Sampling</li>
	<li>3.8.2 Non-probability Sampling</li>
	<li>3.8.3 Probability or Non-probability Sampling?</li>
</ul>
	<li>3.9 Sample Sizes</li>
	<li>3.10 Review Questions</li>
	<li>3.11 Further Readings</li>
	<li>References</li>
</ul>
	<li>4: Getting Data</li>
<ul>	<li>4.1 Introduction</li>
	<li>4.2 Secondary Data</li>
<ul>	<li>4.2.1 Internal Secondary Data</li>
	<li>4.2.2 External Secondary Data</li>
</ul>
	<li>Box 4.1 GfK Spex Retail</li>
	<li>4.3 Conducting Secondary Data Research</li>
<ul>	<li>4.3.1 Assess Availability of Secondary Data</li>
<ul>	<li>Box 4.2 Using Google to Searching for Secondary Data</li>
</ul>
	<li>4.3.2 Assess Inclusion of Key Variables</li>
	<li>4.3.3 Assess Construct Validity</li>
	<li>4.3.4 Assess Sampling</li>
</ul>
	<li>4.4 Conducting Primary Data Research</li>
<ul>	<li>4.4.1 Collecting Primary Data Through Observations</li>
<ul>	<li>Box 4.3 Using Mystery Shopping to Improve Customer Service</li>
</ul>
	<li>4.4.2 Collecting Quantitative Data: Designing Surveys</li>
<ul>	<li>4.4.2.1 Set the Survey Goal</li>
	<li>4.4.2.2 Determine the Type of Questionnaire and Method of Administration</li>
	<li>4.4.2.3 Design the Items</li>
<ul>	<li>Item Content</li>
	<li>Item Wording</li>
</ul>
	<li>4.4.2.4 Set the Scale</li>
<ul>	<li>Type of Scale</li>
	<li>Properties of the Scale</li>
<ul>	<li>Box 4.4 Oddly Chosen Response Categories</li>
</ul>
</ul>
	<li>4.4.2.5 Design the Questionnaire</li>
	<li>4.4.2.6 Pretest the Questionnaire and Execution</li>
<ul>	<li>Box 4.5 Dillman et al.&acute;s (2014) Recommendations on How to Increase Response Rates</li>
</ul>
</ul>
</ul>
	<li>4.5 Basic Qualitative Research</li>
<ul>	<li>4.5.1 In-Depth Interviews</li>
	<li>4.5.2 Projective Techniques</li>
	<li>4.5.3 Focus Groups</li>
</ul>
	<li>4.6 Collecting Primary Data Through Experimental Research</li>
<ul>	<li>4.6.1 Principles of Experimental Research</li>
	<li>4.6.2 Experimental Designs</li>
</ul>
	<li>4.7 Review Questions</li>
	<li>4.8 Further Readings</li>
	<li>References</li>
</ul>
	<li>5: Descriptive Statistics</li>
<ul>	<li>5.1 The Workflow of Data</li>
	<li>5.2 Create Structure</li>
	<li>5.3 Enter Data</li>
	<li>5.4 Clean Data</li>
<ul>	<li>5.4.1 Interviewer Fraud</li>
	<li>5.4.2 Suspicious Response Patterns</li>
</ul>
	<li>Box 5.1 An Example of a Scale with Reverse-Scaled Items (in Bold)</li>
<ul>	<li>5.4.3 Data Entry Errors</li>
	<li>5.4.4 Outliers</li>
<ul>	<li>5.4.4.1 Types of Outliers</li>
	<li>5.4.4.2 Detecting Outliers</li>
<ul>	<li>Univariate Detection</li>
	<li>Bivariate Detection</li>
</ul>
	<li>5.4.4.3 Dealing with Outliers</li>
</ul>
	<li>5.4.5 Missing Data</li>
<ul>	<li>5.4.5.1 The Three Types of Missing Data: Paradise, Purgatory, and Hell</li>
	<li>5.4.5.2 Testing for the Type of Missing Data</li>
	<li>5.4.5.3 Dealing with Missing Data</li>
</ul>
</ul>
	<li>5.5 Describe Data</li>
<ul>	<li>5.5.1 Univariate Graphs and Tables</li>
	<li>5.5.2 Univariate Statistics</li>
<ul>	<li>5.5.2.1 Measures of Centrality</li>
	<li>5.5.2.2 Measures of Dispersion</li>
</ul>
	<li>5.5.3 Bivariate Graphs and Tables</li>
	<li>5.5.4 Bivariate Statistics</li>
</ul>
	<li>Box 5.2 Sample Calculation of Univariate and Bivariate Statistics</li>
	<li>5.6 Transform Data (Optional)</li>
<ul>	<li>5.6.1 Variable Respecification</li>
	<li>5.6.2 Scale Transformation</li>
</ul>
	<li>5.7 Create a Codebook</li>
	<li>5.8 The Oddjob Airways Case Study</li>
<ul>	<li>5.8.1 Introduction to Stata</li>
	<li>5.8.2 Finding Your Way in Stata</li>
<ul>	<li>5.8.2.1 The Stata Main/Start Up Window and the Toolbar</li>
	<li>5.8.2.2 The Menu Bar</li>
<ul>	<li>File</li>
<ul>	<li>Format TypesFormat Types</li>
	<li>Stata .do FilesStata .do Files</li>
</ul>
	<li>Edit</li>
	<li>View</li>
	<li>Data</li>
	<li>Graphics</li>
	<li>Statistics</li>
	<li>User</li>
	<li>Window</li>
	<li>Help</li>
</ul>
</ul>
</ul>
	<li>5.9 Data Management in Stata</li>
<ul>	<li>5.9.1 Restrict Observations</li>
	<li>5.9.2 Create a New Variable from Existing Variable(s)</li>
	<li>5.9.3 Recode Variables</li>
</ul>
	<li>5.10 Example</li>
<ul>	<li>5.10.1 Clean Data</li>
	<li>5.10.2 Describe Data</li>
<ul>	<li>5.10.2.1 Univariate Graphs and Tables</li>
<ul>	<li>Bar Charts</li>
	<li>Histograms</li>
	<li>Box Plot</li>
	<li>Pie Charts</li>
	<li>Frequency Tables</li>
</ul>
	<li>5.10.2.2 Univariate Statistics</li>
	<li>5.10.2.3 Bivariate Graphs and Tables</li>
<ul>	<li>Scatter Plots and Matrix Scatter Plots</li>
	<li>Cross Tabulation</li>
</ul>
	<li>5.10.2.4 Bivariate Statistics: Correlation and Covariance</li>
</ul>
</ul>
	<li>5.11 Cadbury and the UK Chocolate Market (Case Study)</li>
	<li>5.12 Review Questions</li>
	<li>5.13 Further Readings</li>
	<li>References</li>
</ul>
	<li>6: Hypothesis Testing and ANOVA</li>
<ul>	<li>6.1 Introduction</li>
	<li>6.2 Understanding Hypothesis Testing</li>
	<li>6.3 Testing Hypotheses on One Mean</li>
<ul>	<li>6.3.1 Step 1: Formulate the Hypothesis</li>
	<li>6.3.2 Step 2: Choose the Significance Level</li>
	<li>6.3.3 Step 3: Select an Appropriate Test</li>
<ul>	<li>6.3.3.1 Define the Testing Situation</li>
	<li>6.3.3.2 Determine If Samples Are Paired or Independent</li>
	<li>6.3.3.3 Check Assumptions and Choose the Test</li>
<ul>	<li>Normality Test</li>
	<li>Equality of Variances Test</li>
	<li>Parametric Tests</li>
	<li>Non-parametric Tests</li>
</ul>
	<li>6.3.3.4 Specify the Region of Rejection</li>
</ul>
	<li>6.3.4 Step 4: Calculate the Test Statistic</li>
	<li>6.3.5 Step 5: Make the Test Decision</li>
<ul>	<li>6.3.5.1 Option 1: Compare the Test Statistic with the Critical Value</li>
	<li>6.3.5.2 Option 2: Compare the p-Value with the Significance Level</li>
</ul>
	<li>6.3.6 Step 6: Interpret the Results</li>
</ul>
	<li>6.4 Two-Samples t-Test</li>
<ul>	<li>6.4.1 Comparing Two Independent Samples</li>
	<li>6.4.2 Comparing Two Paired Samples</li>
</ul>
	<li>6.5 Comparing More Than Two Means: Analysis of Variance (ANOVA)</li>
	<li>6.6 Understanding One-Way ANOVA</li>
<ul>	<li>6.6.1 Check the Assumptions</li>
	<li>6.6.2 Calculate the Test Statistic</li>
<ul>	<li>6.6.2.1 The Between-Group Variation (SSB)</li>
	<li>6.6.2.2 The Within-Group Variation (SSW)</li>
	<li>6.6.2.3 Combining SSB and SSW into an Overall Picture</li>
</ul>
	<li>6.6.3 Make the Test Decision</li>
	<li>6.6.4 Carry Out Post Hoc Tests</li>
	<li>6.6.5 Measure the Strength of the Effects</li>
	<li>6.6.6 Interpret the Results and Conclude</li>
	<li>6.6.7 Plotting the Results (Optional)</li>
</ul>
	<li>6.7 Going Beyond One-Way ANOVA: The Two-Way ANOVA</li>
	<li>6.8 Example</li>
<ul>	<li>6.8.1 Independent Samples t-Test</li>
<ul>	<li>6.8.1.1 Formulate Hypothesis</li>
	<li>6.8.1.2 Choose the Significant Level</li>
	<li>6.8.1.3 Select an Appropriate Test</li>
	<li>6.8.1.4 Calculate the Test Statistic and Make the Test Decision</li>
	<li>6.8.1.5 Interpret the Results</li>
</ul>
	<li>6.8.2 One-way ANOVA</li>
<ul>	<li>6.8.2.1 Check the Assumptions</li>
	<li>6.8.2.2 Calculate the Test Statistic</li>
	<li>6.8.2.3 Make the Test Decision</li>
	<li>6.8.2.4 Carry Out Post Hoc Tests</li>
	<li>6.8.2.5 Measure the Strength of the Effects</li>
	<li>6.8.2.6 Interpret the Results</li>
	<li>6.8.2.7 Plot the Results</li>
</ul>
	<li>6.8.3 Two-way ANOVA</li>
<ul>	<li>6.8.3.1 Calculate the Test Statistic</li>
	<li>6.8.3.2 Make the Test Decision</li>
	<li>6.8.3.3 Carry Out Post Hoc Tests</li>
	<li>6.8.3.4 Measure the Strength of the Effects</li>
	<li>6.8.3.5 Interpret the Results</li>
	<li>6.8.3.6 Plot the Results</li>
</ul>
</ul>
	<li>6.9 Customer Analysis at Cr&eacute;dit Samouel (Case Study)</li>
	<li>6.10 Review Questions</li>
	<li>6.11 Further Readings</li>
	<li>References</li>
</ul>
	<li>7: Regression Analysis</li>
<ul>	<li>7.1 Introduction</li>
	<li>7.2 Understanding Regression Analysis</li>
	<li>7.3 Conducting a Regression Analysis</li>
<ul>	<li>7.3.1 Check the Regression Analysis Data Requirements</li>
<ul>	<li>7.3.1.1 Sample Size</li>
	<li>7.3.1.2 Variables Need to Vary</li>
	<li>7.3.1.3 Scale Type of the Dependent Variable</li>
	<li>7.3.1.4 Collinearity</li>
</ul>
	<li>7.3.2 Specify and Estimate the Regression Model</li>
<ul>	<li>7.3.2.1 Model Specification</li>
	<li>7.3.2.2 Model Estimation</li>
</ul>
</ul>
	<li>Box 7.1 Omitting Relevant Variables</li>
<ul>	<li>7.3.3 Test the Regression Analysis Assumptions</li>
<ul>	<li>7.3.3.1 First Assumption: Linearity</li>
	<li>7.3.3.2 Second Assumption: Expected Mean Error is Zero</li>
	<li>7.3.3.3 Third Assumption: Homoscedasticity</li>
	<li>7.3.3.4 Fourth Assumption: No Autocorrelation</li>
	<li>7.3.3.5 Fifth (Optional) Assumption: Error Distribution</li>
</ul>
	<li>7.3.4 Interpret the Regression Results</li>
<ul>	<li>7.3.4.1 Overall Model Fit</li>
	<li>7.3.4.2 Effects of Individual Variables</li>
</ul>
	<li>7.3.5 Validate the Regression Results</li>
</ul>
	<li>Box 7.2 Moderation</li>
<ul>	<li>7.3.6 Use the Regression Model</li>
</ul>
	<li>7.4 Example</li>
<ul>	<li>7.4.1 Check the Regression Analysis Data Requirements</li>
	<li>7.4.2 Specify and Estimate the Regression Model </li>
	<li>7.4.3 Test the Regression Analysis Assumptions</li>
	<li>7.4.4 Interpret the Regression Results</li>
</ul>
	<li>Box 7.3 Model Comparison Using AIC and BIC</li>
<ul>	<li>7.4.5 Validate the Regression Results</li>
</ul>
	<li>7.5 Farming with AgriPro (Case Study)</li>
	<li>7.6 Review Questions</li>
	<li>7.7 Further Readings</li>
	<li>References</li>
</ul>
	<li>8: Principal Component and Factor Analysis</li>
<ul>	<li>8.1 Introduction</li>
	<li>8.2 Understanding Principal Component and Factor Analysis</li>
<ul>	<li>8.2.1 Why Use Principal Component and Factor Analysis?</li>
	<li>8.2.2 Analysis Steps</li>
</ul>
	<li>8.3 Principal Component Analysis</li>
<ul>	<li>8.3.1 Check Requirements and Conduct Preliminary Analyses</li>
<ul>	<li>Box 8.1 Identifying Problematic Items</li>
</ul>
	<li>8.3.2 Extract the Factors</li>
<ul>	<li>8.3.2.1 Principal Component Analysis vs. Factor Analysis</li>
	<li>8.3.2.2 How Does Factor Extraction Work?</li>
	<li>8.3.2.3 What Are Eigenvalues?</li>
	<li>8.3.2.4 What Are Communality and Uniqueness?</li>
</ul>
	<li>8.3.3 Determine the Number of Factors</li>
<ul>	<li>8.3.3.1 The Kaiser Criterion</li>
	<li>8.3.3.2 The Scree Plot</li>
<ul>	<li>Box 8.2 Confidence Intervals in the Scree Plot</li>
</ul>
	<li>8.3.3.3 Parallel Analysis</li>
	<li>8.3.3.4 Expectations</li>
</ul>
	<li>8.3.4 Interpret the Factor Solution</li>
<ul>	<li>8.3.4.1 Rotate the Factors</li>
	<li>8.3.4.2 Assign the Variables to the Factors</li>
</ul>
	<li>8.3.5 Evaluate the Goodness-of-Fit of the Factor Solution</li>
<ul>	<li>8.3.5.1 Check the Congruence of the Initial and Reproduced Correlations</li>
	<li>8.3.5.2 Check How Much of Each Variable&acute;s Variance Is Reproduced by Means of Factor Extraction</li>
</ul>
	<li>8.3.6 Compute the Factor Scores</li>
</ul>
	<li>8.4 Confirmatory Factor Analysis and Reliability Analysis</li>
<ul>	<li>Box 8.3 Things to Consider When Calculating Cronbach&acute;s Alpha</li>
</ul>
	<li>8.5 Structural Equation Modeling</li>
	<li>8.6 Example</li>
<ul>	<li>8.6.1 Principal Component Analysis</li>
<ul>	<li>8.6.1.1 Check Requirements and Conduct Preliminary Analyses</li>
	<li>8.6.1.2 Extract the Factors and Determine the Number of Factors</li>
	<li>8.6.1.3 Interpret the Factor Solution</li>
	<li>8.6.1.4 Evaluate the Goodness-of-fit of the Factor Solution</li>
	<li>8.6.1.5 Compute the Factor Scores</li>
</ul>
	<li>8.6.2 Reliability Analysis</li>
</ul>
	<li>8.7 Customer Satisfaction at Haver and Boecker (Case Study)</li>
	<li>8.8 Review Questions</li>
	<li>8.9 Further Readings</li>
	<li>References</li>
</ul>
	<li>9: Cluster Analysis</li>
<ul>	<li>9.1 Introduction</li>
	<li>9.2 Understanding Cluster Analysis</li>
	<li>9.3 Conducting a Cluster Analysis</li>
<ul>	<li>9.3.1 Select the Clustering Variables</li>
<ul>	<li>Box 9.1 Issues with Factor-Cluster Segmentation</li>
</ul>
	<li>9.3.2 Select the Clustering Procedure</li>
<ul>	<li>9.3.2.1 Hierarchical Clustering Methods</li>
<ul>	<li>Understanding Hierarchical Clustering Methods</li>
	<li>Linkage algorithms</li>
</ul>
	<li>9.3.2.2 Partitioning Methods: k-means</li>
<ul>	<li>Understanding k-means Clustering</li>
<ul>	<li>Box 9.2 Variants of the Original k-means Method</li>
</ul>
</ul>
</ul>
	<li>9.3.3 Select a Measure of Similarity or Dissimilarity</li>
<ul>	<li>9.3.3.1 Metric and Ordinal Variables</li>
<ul>	<li>Distance Measures</li>
	<li>Association Measures</li>
	<li>Standardizing the Data</li>
</ul>
	<li>9.3.3.2 Binary and Nominal Variables</li>
	<li>9.3.3.3 Mixed Variables</li>
</ul>
	<li>9.3.4 Decide on the Number of Clusters</li>
<ul>	<li>9.3.4.1 Hierarchical Methods: Deciding on the Number of Clusters</li>
	<li>9.3.4.2 Partitioning Methods: Deciding on the Number of Clusters</li>
</ul>
	<li>9.3.5 Validate and Interpret the Clustering Solution</li>
<ul>	<li>9.3.5.1 Stability</li>
	<li>9.3.5.2 Differentiation of the Data</li>
	<li>9.3.5.3 Profiling</li>
	<li>9.3.5.4 Interpret the Clustering Solution</li>
</ul>
</ul>
	<li>9.4 Example</li>
<ul>	<li>9.4.1 Select the Clustering Variables</li>
<ul>	<li>Box 9.3 Standardization in Stata</li>
</ul>
	<li>9.4.2 Select the Clustering Procedure and Measure of Similarity or Dissimilarity</li>
	<li>9.4.3 Decide on the Number of Clusters</li>
	<li>9.4.4 Validate and Interpret the Clustering Solution</li>
</ul>
	<li>9.5 Oh, James! (Case Study)</li>
	<li>9.6 Review Questions</li>
	<li>9.7 Further Readings</li>
	<li>References</li>
</ul>
	<li>10: Communicating the Results</li>
<ul>	<li>10.1 Introduction</li>
	<li>10.2 Identify the Audience</li>
	<li>10.3 Guidelines for Written Reports</li>
	<li>10.4 Structure the Written Report</li>
<ul>	<li>10.4.1 Title Page</li>
	<li>10.4.2 Executive Summary</li>
	<li>10.4.3 Table of Contents</li>
	<li>10.4.4 Introduction</li>
	<li>10.4.5 Methodology</li>
	<li>10.4.6 Results</li>
<ul>	<li>10.4.6.1 Window Dressing with Graphs</li>
	<li>10.4.6.2 Presenting Statistical Data</li>
</ul>
	<li>10.4.7 Conclusion and Recommendations</li>
<ul>	<li>Box 10.1 Bad Recommendations</li>
</ul>
	<li>10.4.8 Limitations</li>
	<li>10.4.9 Appendix</li>
</ul>
	<li>10.5 Guidelines for Oral Presentations</li>
	<li>10.6 Visual Aids in Oral Presentations</li>
	<li>10.7 Structure the Oral Presentation</li>
	<li>10.8 Follow-Up</li>
	<li>10.9 Ethics in Research Reports</li>
	<li>10.10 Review Questions</li>
	<li>10.11 Further Readings</li>
	<li>References</li>
</ul>
	<li>Glossary</li>
	<li>Index</li>
</ul>
</body></html>