<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Untitled</title>
</head>
<body><div class="page"><p/>
<p>Benjamin A. Stickler&nbsp;&middot; Ewald&nbsp;Schachinger
</p>
<p>Basic 
Concepts in 
Computational 
Physics
 Second Edition </p>
<p/>
</div>
<div class="page"><p/>
<p>Basic Concepts in Computational Physics</p>
<p/>
</div>
<div class="page"><p/>
<p>Benjamin A. Stickler &bull; Ewald Schachinger
</p>
<p>Basic Concepts
in Computational Physics
</p>
<p>Second Edition
</p>
<p>123</p>
<p/>
</div>
<div class="page"><p/>
<p>Benjamin A. Stickler
Faculty of Physics
University of Duisburg-Essen
Duisburg
Germany
</p>
<p>Ewald Schachinger
Institute of Theoretical and Computational
</p>
<p>Physics
Graz University of Technology
Graz, Austria
</p>
<p>Supplementary material and data can be found on extras.springer.com
</p>
<p>ISBN 978-3-319-27263-4 ISBN 978-3-319-27265-8 (eBook)
DOI 10.1007/978-3-319-27265-8
</p>
<p>Library of Congress Control Number: 2015959954
</p>
<p>&copy; Springer International Publishing Switzerland 2014, 2016
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of
the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microfilms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology
now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a specific statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this book
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or
the editors give a warranty, express or implied, with respect to the material contained herein or for any
errors or omissions that may have been made.
</p>
<p>Printed on acid-free paper
</p>
<p>This Springer imprint is published by Springer Nature
The registered company is Springer International Publishing AG Switzerland</p>
<p/>
<div class="annotation"><a href="extras.springer.com">extras.springer.com</a></div>
</div>
<div class="page"><p/>
<p>Preface
</p>
<p>Traditionally physics is divided into two fields of activities: theoretical and experi-
mental. As a consequence of the stunning increase in computer power and of the
development of more powerful numerical techniques, a new branch of physics
was established over the last decades: Computational Physics. This new branch
was introduced as a spin-off of what nowadays is commonly called computer
simulations. They play an increasingly important role in physics and in related
sciences as well as in industrial applications and serve two purposes, namely:
</p>
<p>&bull; Direct simulation of physical processes such as
</p>
<p>ı Molecular dynamics or
ı Monte Carlo simulation of physical processes
</p>
<p>&bull; Solution of complex mathematical problems such as
</p>
<p>ı Differential equations
ı Minimization problems
ı High-dimensional integrals or sums
This book addresses all these scenarios on a very basic level. It is addressed
</p>
<p>to lecturers who will have to teach a basic course/basic courses in Computational
Physics or numerical methods and to students as a companion in their first steps into
the realm of this fascinating field of modern research. Following these intentions
this book was divided into two parts. Part I deals with deterministic methods in
Computational Physics. We discuss, in particular, numerical differentiation and
integration, the treatment of ordinary differential equations, and we present some
notes on the numerics of partial differential equations. Each section within this part
of the book is complemented by numerous applications. Part II of this book provides
an introduction to stochastic methods in Computational Physics. In particular, we
will examine how to generate random numbers following a given distribution,
summarize the basics of stochastics in order to establish the necessary background
to understand techniques like MARKOV-Chain Monte Carlo. Finally, algorithms of
stochastic optimization are discussed. Again, numerous examples out of physics like
</p>
<p>v</p>
<p/>
</div>
<div class="page"><p/>
<p>vi Preface
</p>
<p>diffusion processes or the POTTS model are investigated exhaustively. Finally, this
book contains an appendix that augments the main parts of the book with a detailed
discussion of supplementary topics.
</p>
<p>This book is not meant to be just a collection of algorithms which can
immediately be applied to various problems which may arise in Computational
Physics. On the contrary, the scope of this book is to provide the reader with a
mathematically well-founded glance behind the scene of Computational Physics.
Thus, particular emphasis is on a clear analysis of the various topics and to even
provide in some cases the necessary means to understand the very background
of these methods. Although there is a barely comprehensible amount of excellent
literature on Computational Physics, most of these books seem to concentrate either
on deterministicmethods or on stochastic methods. It is not our goal to competewith
these rather specific works. On the contrary, it is the particular focus of this book to
discuss deterministic methods on par with stochastic methods and to motivate these
methods by concrete examples out of physics and/or engineering.
</p>
<p>Nevertheless, a certain overlap with existing literature was unavoidable and we
apologize if we were not able to cite appropriately all existing works which are of
importance and which influenced this book. However, we believe that by putting the
emphasis on an exact mathematical analysis of both, deterministic and stochastic
methods, we created a stimulating presentation of the basic concepts applied in
Computational Physics.
</p>
<p>If we assume two basic courses in Computational Physics to be part of the cur-
riculum, nicknamed here Computational Physics 101 and Computational Physics
102, then we would like to suggest to present/study the various topics of this book
according to the following syllabus:
</p>
<p>&bull; Computational Physics 101:
</p>
<p>&ndash; Chapter 1: Some Basic Remarks
&ndash; Chapter 2: Numerical Differentiation
&ndash; Chapter 3: Numerical Integration
&ndash; Chapter 4: The KEPLER Problem
&ndash; Chapter 5: Ordinary Differential Equations: Initial Value Problems
&ndash; Chapter 6: The Double Pendulum
&ndash; Chapter 7: Molecular Dynamics
&ndash; Chapter 8: Numerics of Ordinary Differential Equations: Boundary Value
</p>
<p>Problems
&ndash; Chapter 9: The One-Dimensional Stationary Heat Equation
&ndash; Chapter 10: The One-Dimensional Stationary SCHR&Ouml;DINGER Equation
&ndash; Chapter 12: Pseudo-random Number Generators
</p>
<p>&bull; Computational Physics 102:
</p>
<p>&ndash; Chapter 11: Partial Differential Equations
&ndash; Chapter 13: Random Sampling Methods
&ndash; Chapter 14: A Brief Introduction to Monte Carlo Methods
&ndash; Chapter 15: The ISING Model</p>
<p/>
</div>
<div class="page"><p/>
<p>Preface vii
</p>
<p>&ndash; Chapter 16: Some Basics of Stochastic Processes
&ndash; Chapter 17: The RandomWalk and Diffusion Theory
&ndash; Chapter 18: MARKOV-Chain Monte Carlo and the POTTS Model
&ndash; Chapter 19: Data Analysis
&ndash; Chapter 20: Stochastic Optimization
</p>
<p>The various chapters are augmented by problems of medium complexity which
help to understand better the numerical part of the topics discussed within this book.
</p>
<p>Although the manuscript has been carefully checked several times, we cannot
exclude that some errors escaped our scrutiny. We apologize in advance and would
highly appreciate reports of potential mistakes or typos.
</p>
<p>Throughout the book SI-units are used except stated otherwise.
</p>
<p>Graz, Austria Benjamin A. Stickler
July 2015 Ewald Schachinger</p>
<p/>
</div>
<div class="page"><p/>
<p>Acknowledgments
</p>
<p>The authors are grateful to Profs. Dr. C. Lang and Dr. C. Gattringer (Karl-Franzens
Universit&auml;t, Graz, Austria) and to Profs. Dr. W. von der Linden, Dr. H.-G. Evertz,
and Dr. H. Sormann (Graz University of Technology, Austria). They inspired this
book with their lectures on various topics of Computational Physics, computer sim-
ulation, and numerical analysis. Last but not least, the authors thank Dr. Chris Theis
(CERN) for meticulously reading the manuscript, for pointing out inconsistencies,
and for suggestions to improve the text.
</p>
<p>ix</p>
<p/>
</div>
<div class="page"><p/>
<p>Contents
</p>
<p>1 Some Basic Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.1 Motivation .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.2 Roundoff Errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
1.3 Methodological Errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
1.4 Stability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
1.5 Concluding Remarks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
References .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
</p>
<p>Part I Deterministic Methods
</p>
<p>2 Numerical Differentiation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
2.2 Finite Differences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
2.3 Finite Difference Derivatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
2.4 A Systematic Approach: The Operator Technique . . . . . . . . . . . . . . . . . 23
2.5 Concluding Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
References .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
</p>
<p>3 Numerical Integration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
3.2 Rectangular Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
3.3 Trapezoidal Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
3.4 The SIMPSON Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
3.5 General Formulation: The NEWTON-COTES Rules . . . . . . . . . . . . . . . . 38
3.6 GAUSS-LEGENDRE Quadrature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
3.7 An Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
3.8 Concluding Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
References .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
</p>
<p>xi</p>
<p/>
</div>
<div class="page"><p/>
<p>xii Contents
</p>
<p>4 The KEPLER Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
4.2 Numerical Treatment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
References .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
</p>
<p>5 Ordinary Differential Equations: Initial Value Problems . . . . . . . . . . . . . 63
5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
5.2 Simple Integrators. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
5.3 RUNGE-KUTTA Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
5.4 Hamiltonian Systems: Symplectic Integrators . . . . . . . . . . . . . . . . . . . . . . 73
5.5 An Example: The KEPLER Problem, Revisited . . . . . . . . . . . . . . . . . . . . 76
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
References .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
</p>
<p>6 The Double Pendulum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
6.1 HAMILTON&rsquo;s Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
6.2 Numerical Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
6.3 Numerical Analysis of Chaos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
References .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
</p>
<p>7 Molecular Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
7.2 Classical Molecular Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
7.3 Numerical Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114
References .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116
</p>
<p>8 Numerics of Ordinary Differential Equations: Boundary
Value Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
8.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
8.2 Finite Difference Approach.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
8.3 Shooting Methods .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
References .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
</p>
<p>9 The One-Dimensional Stationary Heat Equation . . . . . . . . . . . . . . . . . . . . . . . 131
9.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
9.2 Finite Differences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
9.3 A Second Scenario . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134</p>
<p/>
</div>
<div class="page"><p/>
<p>Contents xiii
</p>
<p>Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
References .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
</p>
<p>10 The One-Dimensional Stationary SCHR&Ouml;DINGER
Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
10.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
10.2 A Simple Example: The Particle in a Box . . . . . . . . . . . . . . . . . . . . . . . . . . 143
10.3 Numerical Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
10.4 Another Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
References .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
</p>
<p>11 Partial Differential Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
11.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
11.2 The POISSON Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158
11.3 The Time-Dependent Heat Equation .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
11.4 The Wave Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
11.5 The Time-Dependent SCHR&Ouml;DINGER Equation .. . . . . . . . . . . . . . . . . . . 170
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
References .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
</p>
<p>Part II Stochastic Methods
</p>
<p>12 Pseudo-random Number Generators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
12.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
12.2 Different Methods. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186
12.3 Quality Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
References .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195
</p>
<p>13 Random Sampling Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197
13.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197
13.2 Inverse Transformation Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200
13.3 Rejection Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202
13.4 Probability Mixing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208
References .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209
</p>
<p>14 A Brief Introduction to Monte-Carlo Methods . . . . . . . . . . . . . . . . . . . . . . . . . 211
14.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211
14.2 Monte-Carlo Integration .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213
14.3 The METROPOLIS Algorithm: An Introduction . . . . . . . . . . . . . . . . . . . . 219</p>
<p/>
</div>
<div class="page"><p/>
<p>xiv Contents
</p>
<p>Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 222
References .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223
</p>
<p>15 The ISING Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225
15.1 The Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225
15.2 Numerics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236
15.3 Selected Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
References .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246
</p>
<p>16 Some Basics of Stochastic Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247
16.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247
16.2 Stochastic Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248
16.3 MARKOV Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251
16.4 MARKOV-Chains. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259
16.5 Continuous-Time MARKOV-Chains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269
References .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269
</p>
<p>17 The RandomWalk and Diffusion Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 271
17.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 271
17.2 The RandomWalk . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273
17.3 The WIENER Process and Brownian Motion . . . . . . . . . . . . . . . . . . . . . . . 279
17.4 Generalized Diffusion Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 294
References .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 294
</p>
<p>18 MARKOV-Chain Monte Carlo and the POTTS Model . . . . . . . . . . . . . . . . . 297
18.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297
18.2 MARKOV-Chain Monte Carlo Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . 298
18.3 The POTTS Model. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 302
18.4 Advanced Algorithms for the POTTS Model . . . . . . . . . . . . . . . . . . . . . . . 306
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 308
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 309
References .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 309
</p>
<p>19 Data Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 311
19.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 311
19.2 Calculation of Errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 311
19.3 Auto-Correlations .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 315
19.4 The Histogram Technique . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 320
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321
References .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321</p>
<p/>
</div>
<div class="page"><p/>
<p>Contents xv
</p>
<p>20 Stochastic Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323
20.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323
20.2 Hill Climbing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 325
20.3 Simulated Annealing.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327
20.4 Genetic Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 334
20.5 Some Further Methods. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 338
References .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 338
</p>
<p>A The Two-Body Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341
</p>
<p>B Solving Non-linear Equations: The NEWTON Method . . . . . . . . . . . . . . . . 347
</p>
<p>C Numerical Solution of Linear Systems of Equations . . . . . . . . . . . . . . . . . . . 349
C.1 The LU Decomposition .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 350
C.2 The GAUSS-SEIDEL Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353
</p>
<p>D Fast Fourier Transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357
</p>
<p>E Basics of Probability Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 363
E.1 Classical Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 363
E.2 Random Variables and Moments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 364
E.3 Binomial Distribution and Limit Theorems . . . . . . . . . . . . . . . . . . . . . . . . 366
E.4 POISSON Distribution and Counting Experiments . . . . . . . . . . . . . . . . . 367
E.5 Continuous Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 368
E.6 BAYES&rsquo; Theorem.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 369
E.7 Normal Distribution.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 370
E.8 Central Limit Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 370
E.9 Characteristic Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 371
E.10 The Correlation Coefficient . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 371
E.11 Stable Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373
</p>
<p>F Phase Transitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 375
F.1 Some Basics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 375
F.2 LANDAU Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 376
</p>
<p>G Fractional Integrals and Derivatives in 1D . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 379
</p>
<p>H Least Squares Fit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 381
H.1 Motivation .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 381
H.2 Linear Least Squares Fit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383
H.3 Nonlinear Least Squares Fit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384</p>
<p/>
</div>
<div class="page"><p/>
<p>xvi Contents
</p>
<p>I Deterministic Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 387
I.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 387
I.2 Steepest Descent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 388
I.3 Conjugate Gradients . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 390
References .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 399
</p>
<p>Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 401</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 1
Some Basic Remarks
</p>
<p>1.1 Motivation
</p>
<p>Computational Physics aims at solving physical problems by means of numerical
methods developed in the field of numerical analysis [1, 2]. According to I. JACQUES
and C. JUDD [3], it is defined as:
</p>
<p>Numerical analysis is concerned with the development and analysis of methods for the
numerical solution of practical problems.
</p>
<p>Although the term practical problems remained unspecified in this definition, it
is certainly necessary to reflect on ways to find approximate solutions to complex
problems which occur regularly in natural sciences. In fact, in most cases it is not
possible to find analytic solutions and one must rely on good approximations. Let
us give some examples.
</p>
<p>Consider the definite integral
</p>
<p>Z b
</p>
<p>a
</p>
<p>dx exp
�
</p>
<p>�x2
�
</p>
<p>; (1.1)
</p>
<p>which, for instance, may occur when it is required to calculate the probability that
an event following a normal distribution takes on a value within the interval Œa; b&#141;,
where a; b 2 R. In contrast to the much simpler integral
</p>
<p>Z b
</p>
<p>a
</p>
<p>dx exp .x/ D exp .b/ � exp .a/ ; (1.2)
</p>
<p>the integral (1.1) cannot be solved analytically because there is no elementary
function which differentiates to exp
</p>
<p>�
</p>
<p>�x2
�
</p>
<p>. Hence, we have to approximate this
integral in such a way that the approximation is accurate enough for our purpose.
This example illustrates that even mathematical expressions which appear quite
</p>
<p>&copy; Springer International Publishing Switzerland 2016
B.A. Stickler, E. Schachinger, Basic Concepts in Computational Physics,
DOI 10.1007/978-3-319-27265-8_1
</p>
<p>1</p>
<p/>
</div>
<div class="page"><p/>
<p>2 1 Some Basic Remarks
</p>
<p>simple at first glance may need a closer inspection when a numerical estimate
for the expression is required. In fact, most numerical methods we will encounter
within this book have been designed before the invention of modern computers or
calculators. However, the applicability of these methods has increased and is still
increasing drastically with the development of even more powerful machines. We
give another example, namely the oscillation of a pendulum. We know from basic
mechanics [4&ndash;8] that the time evolution of a frictionless pendulum of mass m and
length ` in a gravitational field is modeled by the differential equation
</p>
<p>R� C g
`
sin .�/ D 0 : (1.3)
</p>
<p>The solution of this equation describes the oscillatory motion of the pendulum
around the origin O within a two-dimensional plane (Fig. 1.1). Here � is the angular
displacement and g is the acceleration due to gravity. Furthermore, a common
situation is described by initial conditions of the form:
</p>
<p>(
�.0/ D �0 ;
P�.0/ D 0 :
</p>
<p>(1.4)
</p>
<p>For small initial angular displacements, �0 � 1, we set in Eq. (1.3) sin .�/ � �
and obtain the differential equation of the harmonic oscillator:
</p>
<p>R� C g
`
� D 0 : (1.5)
</p>
<p>Together with the initial conditions (1.4) we arrive at the solution
</p>
<p>�.t/ D �0 cos.!t/ ; (1.6)
</p>
<p>Fig. 1.1 Schematic
illustration of the pendulum</p>
<p/>
</div>
<div class="page"><p/>
<p>1.1 Motivation 3
</p>
<p>with ! D
p
</p>
<p>g=`. The period � of the pendulum follows immediately:
</p>
<p>� D 2�
s
</p>
<p>`
</p>
<p>g
: (1.7)
</p>
<p>However, if the approximation of a small angular displacement �0 � 1 is not
applicable, expressions (1.6) and (1.7) will not be valid. Thus, it is advisable to
apply energy conservation in order to arrive at analytic results. The total energy of
the pendulum is given by:
</p>
<p>E D 1
2
</p>
<p>mv2 C mg` Œ1 � cos .�/&#141; D 1
2
</p>
<p>mv20 C mg` Œ1 � cos .�0/&#141; : (1.8)
</p>
<p>Here v is the velocity of the point mass m and v0 and �0 are defined by the initial
conditions (1.4). Since P�.0/ D 0 we have
</p>
<p>E D mg` Œ1 � cos .�0/&#141;
</p>
<p>D 2mg` sin2
�
�0
</p>
<p>2
</p>
<p>�
</p>
<p>; (1.9)
</p>
<p>where we made use of the relation: 1 � cos.x/ D 2 sin2.x=2/. We use this result in
Eq. (1.8) and arrive at:
</p>
<p>1
</p>
<p>2
v2 D 2g`
</p>
<p>�
</p>
<p>sin2
�
�0
</p>
<p>2
</p>
<p>�
</p>
<p>� sin2
�
�
</p>
<p>2
</p>
<p>��
</p>
<p>: (1.10)
</p>
<p>Since v2 D `2 P�2 we have
</p>
<p>P� D 2
r
</p>
<p>g
</p>
<p>`
</p>
<p>s
</p>
<p>sin2
�
�0
</p>
<p>2
</p>
<p>�
</p>
<p>� sin2
�
�
</p>
<p>2
</p>
<p>�
</p>
<p>: (1.11)
</p>
<p>Separation of variables yields
</p>
<p>r
</p>
<p>g
</p>
<p>`
t D 1
</p>
<p>2k
</p>
<p>Z �
</p>
<p>0
</p>
<p>d'
q
</p>
<p>1 � 1
k2
sin2
</p>
<p>�
'
</p>
<p>2
</p>
<p>�
; (1.12)
</p>
<p>with k D sin .�0=2/ : For t D � we have � D �0 and we obtain for the period
</p>
<p>� D 2
k
</p>
<p>s
</p>
<p>`
</p>
<p>g
</p>
<p>Z �0
</p>
<p>0
</p>
<p>d'
q
</p>
<p>1 � 1
k2
sin2
</p>
<p>�
'
</p>
<p>2
</p>
<p>�
: (1.13)</p>
<p/>
</div>
<div class="page"><p/>
<p>4 1 Some Basic Remarks
</p>
<p>Let us transform the above integral into a more convenient form with help of
the substitution k sin.˛/ D sin .'=2/. Thus, ˛ 2 Œ0; �=2&#141; and a straightforward
calculation yields:
</p>
<p>� D 4
s
</p>
<p>`
</p>
<p>g
</p>
<p>Z �
2
</p>
<p>0
</p>
<p>d˛
p
</p>
<p>1 � k2 sin2 .˛/
</p>
<p>D 4
s
</p>
<p>`
</p>
<p>g
K1.k/ : (1.14)
</p>
<p>The function K1.k/ introduced in (1.14) for k 2 R is referred to as the complete
elliptic integral of the first kind [9&ndash;12]. All these manipulations did not really result
in a simplification of the problem at hand because we are still confronted with
the integral in Eq. (1.14) which cannot be evaluated without the use of additional
approximationswhich will, in the end, result in a numerical solution of the problem.
A natural way to proceed would be to expand the complete elliptic integral in a
power series up to order N, where N is chosen in such a way that the truncation
error RN.k/ becomes negligible. We can find the desired expression in any text on
special functions [9, 11, 12]. It reads
</p>
<p>K1.k/ D
�
</p>
<p>2
</p>
<p>1
X
</p>
<p>nD0
</p>
<p>�
.2n/Š
</p>
<p>22n.nŠ/2
</p>
<p>�2
</p>
<p>k2n
</p>
<p>D �
2
</p>
<p>N
X
</p>
<p>nD0
</p>
<p>�
.2n/Š
</p>
<p>22n.nŠ/2
</p>
<p>�2
</p>
<p>k2n C RN.k/ : (1.15)
</p>
<p>Imagine now the inverse problem: the period � is given and the initial angle �0
is unknown. Again, we could expand the integrand in a power series and solve
the corresponding polynomial for �0. However, such an approach would be very
inefficient due to two reasons: first of all, we are confronted with the impossibility
of finding analytically the roots of a polynomial of order N &gt; 41 and, secondly, at
which value of N should we truncate the power series if �0 is unknown? A glance in
a book on special functions might give us a better, i.e. more convenient, alternative.
Indeed, the inverse function of the elliptic integral K1.k/ with respect to k can be
given explicitly in terms of JACOBI elliptic functions [9&ndash;12]. Series expansions of
these functions have been developed such that we can approximate �0 by truncating
the respective series.
</p>
<p>This example helped to illustrate that we depend on numerical approximations
of definite expressions in a multitude of cases. Even if an numerically approximate
solution has been found for a particular problem it will be adamant to check quite
</p>
<p>1The roots of a real valued polynomial of order N D 3 or 4 are referred to as CARDANO&rsquo;s or
FERRARI&rsquo;s solutions [13], respectively.</p>
<p/>
</div>
<div class="page"><p/>
<p>1.1 Motivation 5
</p>
<p>carefully if the approach was (i) justified within the required accuracy, and (ii) if it
allowed to improve the induced error of the result. The second point is known as the
stability of a routine. We will discuss this topic in more detail in Sect. 1.4.
</p>
<p>Throughout this book we will be confronted with numerous methods which will
allow approximate solutions of problems similar to the two examples illustrated
above. First of all, we would like to specify the properties we expect these methods
to have. Primarily, the method is to be formulated as an unambiguous mathematical
recipe which can be applied to the set of problems it was designed for. Its
applicability should be well defined and it should allow to determine an estimate for
the error. Moreover, infinite repetition of the procedure should approximate the exact
result to arbitrary accuracy. In other words, we want the method to be well defined in
algorithmic form. Consequently, let us define an algorithm as a sequence of logical
and arithmetic operations (addition, subtraction, multiplication or division) which
</p>
<p>allows to approximate the solution of the problem under consideration within any
</p>
<p>accuracy desired. This implies, of course, that numerical errors will be unavoidable.
Let us classify the occurring errors based on the structure every numerical
</p>
<p>routine follows: We have input-errors, algorithmic-errors, and output-errors as
indicated schematically in Fig. 1.2. This structural classification can be refined:
input-errors are divided into roundoff errors and measurement errors contained in
the input data; algorithmic-errors consist of roundoff errors during evaluation and
of methodological errors due to mathematical approximations; finally, output errors
are, in fact, roundoff errors. In Sects. 1.2 and 1.3 we will concentrate on roundoff
errors and methodological errors. Since in most cases measurement errors cannot
be influenced by the theoretical physicist concerned with numerical modeling, this
particular part will not be discussed in this book. However, we will discuss the
stability of numerical routines, i.e. the influence of slight modifications of the input
parameters on the outcome of a particular algorithm in Sect. 1.4.
</p>
<p>Fig. 1.2 Schematic
classification of the errors
occurring within a numerical
procedure</p>
<p/>
</div>
<div class="page"><p/>
<p>6 1 Some Basic Remarks
</p>
<p>1.2 Roundoff Errors
</p>
<p>In fact, since every number is stored in a computer using a finite number of digits, we
have to truncate every non-terminating number at some point. For instance, consider
2
3
D 0:666666666666 : : : which will be stored as 0:6666666667 if the machine
</p>
<p>allows only ten digits. Actually, computers use binary arithmetic (for which even
0:110 D 0:000110011001100 : : :2 is problematic2) but for the moment we shall
ignore this fact since the above example suffices to illustrate the crucial point. Let
Fl.x/ denote the floating-point form of a number x within the numerical range of the
machine. For the above example, i.e. a ten digit storage, we have
</p>
<p>Fl
</p>
<p>�
2
</p>
<p>3
</p>
<p>�
</p>
<p>D 0:6666666667 : (1.16)
</p>
<p>This has the consequence that, for instance, Fl.
p
3/ � Fl.
</p>
<p>p
3/ &curren; Fl.
</p>
<p>p
3 �
</p>
<p>p
3/D3.
</p>
<p>However, Fl.
p
3/ � Fl.
</p>
<p>p
3/ � 3 within the defined range. Before we continue our
</p>
<p>discussion on roundoff errors we have to introduce the concepts of the absolute and
the relative error. We denote the true value of a quantity by y and its approximate
value by y. Then the absolute error �a is defined as
</p>
<p>�a D jy � yj ; (1.17)
</p>
<p>while the relative error �r is given by
</p>
<p>�r D
ˇ
ˇ
ˇ
ˇ
</p>
<p>y � y
y
</p>
<p>ˇ
ˇ
ˇ
ˇ
D �ajyj ; (1.18)
</p>
<p>provided that y &curren; 0. In most applications, the relative error is more significant. This
is illustrated in Table 1.1, where it is intuitively obvious that in the second case the
approximate value is much better although the absolute error is the same for both
examples.
</p>
<p>Let us have a look at the relative error of an arbitrary number stored to the k-th
digit: We can write an arbitrary number y in the form y D 0:d1d2d3 : : : dkdkC1 : : : 10n
with d1 &curren; 0 and n 2 Z. Accordingly, we write its approximate value as
y D 0:d1d2d3 : : : dk10n, where k is the maximum number of digits stored by the
</p>
<p>Table 1.1 Illustration of the
significance of the relative
error
</p>
<p>y y �a �r
</p>
<p>(1) 0:1 0:09 0.01 0.1
</p>
<p>(2) 1000:0 999:99 0.01 0.00001
</p>
<p>2A disastrous effect of this binary approximation of 0.1 was discussed by T. Chartier [14].</p>
<p/>
</div>
<div class="page"><p/>
<p>1.3 Methodological Errors 7
</p>
<p>machine. Hence we obtain for the relative error
</p>
<p>�r D
ˇ
ˇ
ˇ
ˇ
</p>
<p>0:d1d2d3 : : : dkdkC1 : : : 10n � 0:d1d2d3 : : : dk10n
0:d1d2d3 : : : dkdkC1 : : : 10n
</p>
<p>ˇ
ˇ
ˇ
ˇ
</p>
<p>D
ˇ
ˇ
ˇ
ˇ
</p>
<p>0:dkC1dkC2 : : : 10n�k
</p>
<p>0:d1d2d3 : : : 10n
</p>
<p>ˇ
ˇ
ˇ
ˇ
</p>
<p>D
ˇ
ˇ
ˇ
ˇ
</p>
<p>0:dkC1dkC2 : : :
</p>
<p>0:d1d2d3 : : :
</p>
<p>ˇ
ˇ
ˇ
ˇ
10�k
</p>
<p>� 1
0:1
10�k
</p>
<p>D 10�kC1 : (1.19)
</p>
<p>In the last steps we employed that, since d1 &curren; 0, we have 0:d1d2d3 : : : � 0:1
and accordingly 0:dkC1dkC2 : : : &lt; 1. If the last digit would have been rounded (for
dkC1 � 5 we set dk D dk C 1 otherwise dk remains unchanged) instead of a simple
truncation, the relative error of a variable y would be �r D 0:5 � 10�kC1.
</p>
<p>Whenever an arithmetic operation is performed, the errors of the variables
involved is transferred to the result [15]. This can occur in an advantageous or
disadvantageous way, where we understand disadvantageous as an increase in
the relative error. Particular care is required when two nearly identical numbers
are subtracted (subtractive cancellation) or when a large number is divided by
a, in comparison, small number. In such cases the roundoff error will increase
dramatically. We note that it might be necessary to avoid such operations in our
aim to design an algorithm which is required to produce reasonable results. An
illustrative example and its remedy will be discussed in Sect. 1.3. However, before
proceeding to the next section we introduce a lower bound to the accuracy which is
achievable with a non-ideal computer, the machine-number. The machine-number
is smallest positive number � which can be added to another number, such that a
change in the result is observed. In particular,
</p>
<p>� D min
ı
</p>
<p>n
</p>
<p>ı &gt; 0
ˇ
ˇ
ˇ1C ı &gt; 1
</p>
<p>o
</p>
<p>: (1.20)
</p>
<p>For a (nonexistent) super-computer, which is capable of saving as much digits
as desired, � would be arbitrarily small. A typical value for double-precision in
FORTRAN or C is � � 10�16.
</p>
<p>1.3 Methodological Errors
</p>
<p>A methodological error is introduced into the routine whenever a complex mathe-
matical expression is replaced by an approximate, simpler one. We already came
across an example when we regarded the series representation of the elliptic</p>
<p/>
</div>
<div class="page"><p/>
<p>8 1 Some Basic Remarks
</p>
<p>integral (1.12) in Sect. 1.1. Although we could evaluate the series up to an arbitrary
order N, we are definitely not able to sum up the coefficients to infinite order.
Hence, it is not possible to get rid of methodological errors whenever we have to
deal with expressions we cannot evaluate analytically. Another intriguing example
is the numerical differentiation of a given function. The standard approximation of
a derivative reads
</p>
<p>f 0.x0/ D
d
</p>
<p>dx
f .x/
</p>
<p>ˇ
ˇ
ˇ
ˇ
xDx0
</p>
<p>� f .x0 C h/� f .x0/
h
</p>
<p>: (1.21)
</p>
<p>This approximation is referred to as finite difference and will be discussed in more
detail in Chap. 2. One would, in a first guess, expect that the obtained value gets
closer to the true value of the derivative f 0.x0/ with decreasing values of h. From a
calculus point of view, this is correct since by definition
</p>
<p>d
</p>
<p>dx
f .x/
</p>
<p>ˇ
ˇ
ˇ
ˇ
xDx0
</p>
<p>D lim
h!0
</p>
<p>f .x0 C h/� f .x0/
h
</p>
<p>: (1.22)
</p>
<p>However, this is not the case numerically. In particular, one can find a value Oh
for which the relative error is minimal, while for values h &lt; Oh and h &gt; Oh the
approximation obtained is worse in comparison. The reason is that for small values
of h the roundoff errors dominate the result since f .x0 C h/ and f .x0/ almost cancel
while 1=h is very small. For h &gt; Oh, the methodological error, i.e. the replacement of
a derivative by a finite difference, controls the result.
</p>
<p>We give one further example [16] in order to illustrate the interplay between
methodological errors and roundoff errors. We regard the, apparently nonhazardous,
numerical solution of a quadratic equation
</p>
<p>ax2 C bx C c D 0 ; (1.23)
</p>
<p>where a; b; c 2 R, a &curren; 0. The well known solutions read
</p>
<p>x1 D
�b C
</p>
<p>p
b2 � 4ac
2a
</p>
<p>and x2 D
�b �
</p>
<p>p
b2 � 4ac
2a
</p>
<p>: (1.24)
</p>
<p>Cautious because of the above examples, we immediately diagnose the danger of a
subtractive cancellation in the expression of x1 for b &gt; 0 or in x2 for b &lt; 0, and
rewrite the above expression for x1:
</p>
<p>x1 D
.�b C
</p>
<p>p
b2 � 4ac/
2a
</p>
<p>.�b �
p
</p>
<p>b2 � 4ac/
.�b �
</p>
<p>p
b2 � 4ac/
</p>
<p>D 2c
�b �
</p>
<p>p
b2 � 4ac
</p>
<p>: (1.25)</p>
<p/>
</div>
<div class="page"><p/>
<p>1.4 Stability 9
</p>
<p>For x2 we obtain
</p>
<p>x2 D
2c
</p>
<p>�b C
p
</p>
<p>b2 � 4ac
: (1.26)
</p>
<p>Consequently, if b &gt; 0 x1 should be calculated using Eq. (1.25) and if b &lt; 0
Eq. (1.26) should be used to calculate x2. Moreover, the above expressions can be
cast into one expression by setting
</p>
<p>x1 D
q
</p>
<p>a
and x2 D
</p>
<p>c
</p>
<p>q
; (1.27)
</p>
<p>with
</p>
<p>q D �1
2
</p>
<p>h
</p>
<p>b C sgn.b/
p
</p>
<p>b2 � 4ac
i
</p>
<p>: (1.28)
</p>
<p>Thus, Eqs. (1.27) and (1.28) can be used to calculate x1 and x2 for any sign of b.
</p>
<p>1.4 Stability
</p>
<p>When a new numerical method is designed stability is the third crucial point after
roundoff errors and methodological errors [17]. We give an introductory definition:
</p>
<p>An algorithm, equation or, even more general, a problem is referred to as unstable or ill-
conditioned if small changes in the input cause a large change in the output.
</p>
<p>It will be followed by a couple of elucidating examples [3].3 To be more specific,
let us now, for instance, consider the following system of equations
</p>
<p>x C y D 2:0;
x C 1:01y D 2:01 : (1.29)
</p>
<p>These equations are easily solved and give x D 1:0 and y D 1:0. To make our
point we consider now the case in which the right hand side of the second equation
of (1.29) is subjected to a small perturbation, i.e. we consider in particular the
following system of equations
</p>
<p>x C y D 2:0;
x C 1:01y D 2:02 : (1.30)
</p>
<p>3Although unstable behavior is not desirable in the first place the discovery of unstable systems
was the birth of a specific branch in physics called Chaos Theory. We briefly comment on this
point at the end of this section.</p>
<p/>
</div>
<div class="page"><p/>
<p>10 1 Some Basic Remarks
</p>
<p>The corresponding solution is x D 0:0 and y D 2:0. We observe that a relative
change of 0:05% on the right hand side of the second equation in (1.29) resulted
in a 100% relative change of the solution. Moreover, if the coefficient of y in the
second equation of (1.29) were 1:0 instead of 1:01, which corresponds to a relative
change of 1%, the equations would be unsolvable. This is a behavior typical for
ill-conditioned problems which, for obvious reasons, should be avoided whenever
possible.
</p>
<p>We give a second example: We consider the following initial value problem
</p>
<p>(
</p>
<p>Ry � 10Py � 11y D 0 ;
y.0/ D 1; Py.0/ D �1 :
</p>
<p>(1.31)
</p>
<p>The general solution is readily obtained to be of the form
</p>
<p>y D A exp .�x/C B exp .11x/ ; (1.32)
</p>
<p>with numerical constants A and B. The initial conditions yield the unique solution
</p>
<p>y D exp .�x/ : (1.33)
</p>
<p>The initial conditions are now changed by two small parameters ı; � &gt; 0 to give:
</p>
<p>y.0/ D 1C ı and Py.0/ D �1C � : (1.34)
</p>
<p>The unique solution which satisfies these initial conditions is:
</p>
<p>y D
�
</p>
<p>1C 11ı
12
</p>
<p>� �
12
</p>
<p>�
</p>
<p>exp.�x/C
�
ı
</p>
<p>12
C �
12
</p>
<p>�
</p>
<p>exp .11x/ : (1.35)
</p>
<p>We calculate the relative error
</p>
<p>�r D
ˇ
ˇ
ˇ
ˇ
</p>
<p>y � y
y
</p>
<p>ˇ
ˇ
ˇ
ˇ
</p>
<p>D
�
11ı
</p>
<p>12
� �
12
</p>
<p>�
</p>
<p>C
�
ı
</p>
<p>12
C �
12
</p>
<p>�
</p>
<p>exp .12x/ ; (1.36)
</p>
<p>which indicates that the problem is ill-conditioned since for large values of x the
second term definitely overrules the first one.
</p>
<p>Another, but not less serious kind of problem is induced instability:
</p>
<p>A method is referred to as induced unstable if a small error at one point of the calculation
induces a large error at some subsequent point.
</p>
<p>Induced instability is particularly dangerous since small roundoff errors are
unavoidable in most calculations. Hence, if some part of the whole algorithm is</p>
<p/>
</div>
<div class="page"><p/>
<p>1.4 Stability 11
</p>
<p>ill-conditioned, the final output will be dominated by the error induced in such a
way. Again, an example will help to illustrate such behavior. The definite integral
</p>
<p>In D
Z 1
</p>
<p>0
</p>
<p>dx xn exp.x � 1/ ; (1.37)
</p>
<p>is considered. Integration by parts yields
</p>
<p>In D 1 � nIn�1 : (1.38)
</p>
<p>This expression can be used to recursively calculate In from I0, where
</p>
<p>I0 D 1 � exp .�1/ : (1.39)
</p>
<p>Although the recursion formula (1.38) is exact we will run into massive problems
using it. The reason is easily illustrated:
</p>
<p>In D 1� nIn�1
D 1� n C n.n � 1/In�2
D 1� n C n.n � 1/� n.n � 1/.n � 2/In�3
:::
</p>
<p>D 1C
n�1X
</p>
<p>kD1
.�1/k nŠ
</p>
<p>.n � k/Š C .�1/
n�1nŠI0 : (1.40)
</p>
<p>Thus, the initial roundoff error included in the numerical value of I0 is multiplied
with nŠ. Note that for large n we have according to STIRLING&rsquo;s approximation
</p>
<p>nŠ �
p
2�nnC
</p>
<p>1
2 exp .�n/ ; (1.41)
</p>
<p>i.e. an initial error increases almost as nn.
However, Eq. (1.38) can be reformulated to give
</p>
<p>In D
1
</p>
<p>n C 1 .1 � InC1/ ; (1.42)
</p>
<p>and this opens an alternative method for a recursive calculation of In. We can start
with some value N � n and simply set IN D 0. The error introduced in such a way
may in the end not be acceptable, nevertheless, it decreases with every iteration step
due to the division by n in Eq. (1.42).
</p>
<p>Having discussed some basic features of stability in numerical algorithms we
would like to add a few remarks on Chaos Theory. Chaos theory investigates
dynamical processes which are very sensitive to initial conditions. One of the</p>
<p/>
</div>
<div class="page"><p/>
<p>12 1 Some Basic Remarks
</p>
<p>best known examples for such a behavior is the weather prediction. Although,
POINCAR&Eacute; already observed chaotic behavior while working on the three body
problem, one of the pioneers of chaos theory was E.N. LORENZ [18] (not to be
confusedwith H. LORENTZ, who introduced the LORENTZ transformation). In 1961
he ran weather simulations on a computer of restricted capacity. However, when
he tried to reproduce one particular result by restarting the calculation with new
parameters calculated the days before, he observed that the outcomewas completely
different [19]. The reason was that the equations he dealt with were ill-conditioned,
and the roundoff error he introduced by simply typing in the numbers of the
graphical output, increased drastically, and, hence, produced a completely different
result. Nowadays, various physical systems are known which indeed behave in such
a way. Further examples are turbulences in fluids, oscillations in electrical circuits,
oscillating chemical reactions, population growth in ecology, the time evolution of
the magnetic field of celestial bodies, . . . .
</p>
<p>It is important to note, that chaotic behavior induced in such systems is determin-
istic, yet unpredictable. This is due to the impossibility of an exact knowledge of
the initial conditions required to predict, for instance, the weather over a reasonably
long period. A feature which is referred to as the butterfly effect: a hurricane can
form because a butterfly flapped its wings several weeks before. However, these
effects have nothing to do with intrinsically probabilistic properties which are solely
a feature of quantum mechanics. In contrast to this, in chaos theory, the future is
uniquely determined by initial conditions, however, still unpredictable. This is often
referred to as deterministic chaos.
</p>
<p>It has to be emphasized that chaos in physical systems is a consequence of the
equations describing the processes and not a consequence of the numerical method
used for modeling. Therefore, it is important to distinguish between the stability of
a numerical method and the stability of a physical system in general.
</p>
<p>We will come across chaotic behavior again in Sect. 6.3 where we discuss chaotic
behavior in the dynamics of the double pendulum [4&ndash;8].
</p>
<p>1.5 Concluding Remarks
</p>
<p>In this chapter we dealt with the basic features of numerical errors one is always
confronted with when developing an algorithm. One point we neglected in our
discussion is the computational cost, i.e. the time a program needs to be executed.
Although this is a very important point, it is beyond the scope of this book. However,
one has to find a balance between the need of achieving the most accurate result and
the computing time required to achieve it. The most accurate result is useless if
the programmer does not get the result within his lifetime. D. ADAMS [20] put in
a nutshell: the super-computer Deep Thought was asked to compute the answer to</p>
<p/>
</div>
<div class="page"><p/>
<p>References 13
</p>
<p>&ldquo;The Ultimate Question of Life, the Universe and Everything&rdquo;, quote:
</p>
<p>&ldquo;How long?&rdquo; he said.
&ldquo;Seven and a half million years.&rdquo;
</p>
<p>Another quite crucial point, which we neglected so far, is the error analysis of
a computational method which is based on random numbers (in fact it is pseudo-
random numbers and this point will be discussed in the second part of this book). In
this case, the situation changes completely, because, similar to experimental results,
the observed values are distributed around a mean with a certain variance. Such
results have to be interpreted within a statistical context. However, it turns out
that for many problems the computational efficiency can be significantly increased
using such methods. Typical applications are estimates of integrals or solutions to
optimization problems. Such topics will be treated in the second part of this book.
</p>
<p>References
</p>
<p>1. S&uuml;li, E., Mayers, D.: An Introduction to Numerical Analysis. Cambridge University Press,
Cambridge (2003)
</p>
<p>2. Gautschi, W.: Numerical Analysis. Springer, Berlin/Heidelberg (2012)
3. Jacques, I., Judd, C.: Numerical Analysis. Chapman and Hall, London (1987)
4. Arnol&rsquo;d, V.I.: Mathematical Methods of Classical Mechanics, 2nd edn. Graduate Texts in
</p>
<p>Mathematics, vol. 60. Springer, Berlin/Heidelberg (1989)
5. Fetter, A.L., Walecka, J.D.: Theoretical Mechanics of Particles and Continua. Dover, New York
</p>
<p>(2004)
6. Scheck, F.: Mechanics, 5th edn. Springer, Berlin/Heidelberg (2010)
7. Goldstein, H., Poole, C., Safko, J.: Classical Mechanics, 3rd edn. Addison-Wesley, Menlo Park
</p>
<p>(2013)
8. Flie&szlig;bach, T.: Mechanik, 7th edn. Lehrbuch zur Theoretischen Physik I. Springer,
</p>
<p>Berlin/Heidelberg (2015)
9. Abramovitz, M., Stegun, I.A. (eds.): Handbook of Mathemathical Functions. Dover, New York
</p>
<p>(1965)
10. Olver, F.W.J., Lozier, D.W., Boisvert, R.F., Clark, C.W.: NIST Handbook of Mathematical
</p>
<p>Functions. Cambridge University Press, Cambridge (2010)
11. Mathai, A.M., Haubold, H.J.: Special Functions for Applied Scientists. Springer,
</p>
<p>Berlin/Heidelberg (2008)
12. Beals, R., Wong, R.: Special Functions. Cambridge Studies in Advanced Mathematics.
</p>
<p>Cambridge University Press, Cambridge (2010)
13. Clark, A.: Elements of Abstract Algebra. Dover, New York (1971)
14. Chartier, T.: Devastating roundoff error. Math. Horiz. 13, 11 (2006). http://www.jstor.org/
</p>
<p>stable/25678616
15. Ueberhuber, C.W.: Numerical Computation 1: Methods, Software and Analysis. Springer,
</p>
<p>Berlin/Heidelberg (1997)
16. Burden, R.L., Faires, J.D.: Numerical Analysis. PWS-Kent Publishing Comp., Boston (1993)
17. Higham, N.J.: Accuracy and Stability of Numerical Algorithms, 2nd edn. Society for Industrial
</p>
<p>and Applied Mathematics (SIAM), Philadelphia (2002)
18. Lorenz, E.N.: Deterministic nonperiodic flow. J. Atmos. Sci. 20, 130&ndash;141 (1963)
19. Roulstone, I., Norbury, J.: Invisible in the Storm: The Role of Mathematics in Understanding
</p>
<p>Weather. Prinecton University Press, Princeton (2013)
20. Adams, D.: The Hitchhiker&rsquo;s Guide to the Galaxy. Pan Books, London (1979)</p>
<p/>
<div class="annotation"><a href="http://www.jstor.org/stable/25678616">http://www.jstor.org/stable/25678616</a></div>
<div class="annotation"><a href="http://www.jstor.org/stable/25678616">http://www.jstor.org/stable/25678616</a></div>
</div>
<div class="page"><p/>
<p>Part I
Deterministic Methods</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 2
Numerical Differentiation
</p>
<p>2.1 Introduction
</p>
<p>This chapter is the first of two systematic introductions to the numerical treatment
of differential equations. Differential equations and, thus, derivatives and integrals
are of eminent importance in the modern formulation of natural sciences and
in particular of physics. Very often the complexity of the expressions involved
does not allow an analytical approach, although modern symbolic software can
ease a physicists life significantly. Thus, in many cases a numerical treatment is
unavoidable and one should be prepared.
</p>
<p>We introduce here the notion of finite differences as a basic concept of numerical
differentiation [1&ndash;3]. In contrast, the next chapter will deal with the concepts
of numerical quadrature. Together, these two chapters will set the stage for a
comprehensive discussion of algorithms designed to solve numerically differential
equations. In particular, the solution of ordinary differential equations will always
be based on an integration.
</p>
<p>This chapter is composed of four sections. The first repeats some basic concepts
of calculus and introduces formally finite differences. The second formulates
approximates to derivatives based on finite differences, while the third section
includes a more systematic approach based on an operator technique. It allows
an arbitrarily close approximation of derivatives with the advantage that the
expressions discussed in this section can immediately be applied to the problems
at hand. The chapter is concluded with a discussion of some additional aspects.
</p>
<p>&copy; Springer International Publishing Switzerland 2016
B.A. Stickler, E. Schachinger, Basic Concepts in Computational Physics,
DOI 10.1007/978-3-319-27265-8_2
</p>
<p>17</p>
<p/>
</div>
<div class="page"><p/>
<p>18 2 Numerical Differentiation
</p>
<p>2.2 Finite Differences
</p>
<p>Let us consider a smooth function f .x/ on the finite interval Œa; b&#141; � R of the real
axis. The interval Œa; b&#141; is divided into N � 1 2 N equally spaced sub-intervals of
the form Œxi; xiC1&#141; where x1 D a, xN D b. Obviously, xi is then given by
</p>
<p>xi D a C .i � 1/
b � a
N � 1; i D 1; : : : ;N : (2.1)
</p>
<p>We introduce the distance h between two grid-points xi by:
</p>
<p>h D xiC1 � xi D
b � a
N � 1 ; 8i D 1; : : : ;N � 1 : (2.2)
</p>
<p>For the sake of a more compact notation we restrict our discussion to equally spaced
grid-points keeping in mind that the extension to arbitrarily spaced grid-points by
replacing h by hi is straight forward and leaves the discussion essentially unchanged.
</p>
<p>Note that the number of grid-points and, thus, their distance h, has to be chosen
in such a way that the function f .x/ can be sufficiently well approximated by its
function values f .xi/ as indicated in Fig. 2.1. We understand by sufficiently well
approximated that some interpolation scheme in the interval Œxi; xiC1&#141;will reproduce
the function f .x/ within a required accuracy. In cases where the function is strongly
varying within some sub-interval Œc; d&#141; � Œa; b&#141; and is slowly varying within
</p>
<p>Fig. 2.1 We define equally spaced grid-points xi on a finite interval on the real axis in such a
way that the function f .x/ is sufficiently well approximated by its functional values f .xi/ at these
grid-points</p>
<p/>
</div>
<div class="page"><p/>
<p>2.2 Finite Differences 19
</p>
<p>Œa; b&#141; n Œc; d&#141; it might be advisable to use variable grid-spacing in order to reduce
the computational cost of the procedure.
</p>
<p>We introduce the following notation: The function value of f .x/ at the grid-point
xi will be denoted by fi � f .xi/ and its n-th derivative:
</p>
<p>f
.n/
i � f .n/.xi/ D
</p>
<p>dnf .x/
</p>
<p>dxn
</p>
<p>ˇ
ˇ
ˇ
ˇ
xDxi
</p>
<p>: (2.3)
</p>
<p>Furthermore, we define for arbitrary � 2 Œxi; xiC1/
</p>
<p>f
.n/
iC� D f .n/.�/ ; (2.4)
</p>
<p>where f .0/iC� � fiC� and � is chosen to give:
</p>
<p>� D xi C �h ; � 2 Œ0; 1/ : (2.5)
</p>
<p>Let us remember some basics from calculus: The first derivative, denoted f 0.x/
of a function f .x/ which is smooth within the interval Œa; b&#141;, i.e. f .x/ 2 C1Œa; b&#141; for
arbitrary x 2 Œa; b&#141;, is defined as
</p>
<p>f 0.x/ WD lim
h!0
</p>
<p>f .x C h/� f .x/
h
</p>
<p>D lim
h!0
</p>
<p>f .x/ � f .x � h/
h
</p>
<p>D lim
h!0
</p>
<p>f .x C h/� f .x � h/
2h
</p>
<p>: (2.6)
</p>
<p>However, it is impossible to draw numerically the limit h ! 0 as discussed in
Sect. 1.3, Eq. (1.22). This manifests itself in a non-negligible error due to subtractive
cancellation.
</p>
<p>This problem is circumvented by the use of TAYLOR&rsquo;s theorem. It states that if
there is a function which is .nC 1/-times continuously differentiable on the interval
Œa; b&#141; then f .x/ can be expressed in terms of a series expansion at point x0 2 Œa; b&#141;:
</p>
<p>f .x/ D
n
X
</p>
<p>kD0
</p>
<p>f .k/.x0/
</p>
<p>kŠ
.x � x0/k C
</p>
<p>f .nC1/Œ�.x/&#141;
</p>
<p>.n C 1/Š .x � x0/
nC1; 8x 2 Œa; b&#141; : (2.7)
</p>
<p>Here, �.x/ takes on a value between x and x0.1 The last term on the right hand side
of Eq. (2.7) is commonly referred to as truncation error. (A more general definition
of this error was given in Sect. 1.1.)
</p>
<p>1Note that for x0 D 0 the series expansion (2.7) is referred to as MCLAURIN series.</p>
<p/>
</div>
<div class="page"><p/>
<p>20 2 Numerical Differentiation
</p>
<p>We introduce now the finite difference operators
</p>
<p>
Cfi D fiC1 � fi ; (2.8a)
</p>
<p>as the forward difference,
</p>
<p>
�fi D fi � fi�1 ; (2.8b)
</p>
<p>as the backward difference, and
</p>
<p>
c fi D fiC1 � fi�1 ; (2.8c)
</p>
<p>as the central difference.2 The derivative of f .x/ can be approximated with the help
of TAYLOR&rsquo;s theorem (2.7). In a first step we consider (restricting to third order in
h)
</p>
<p>fiC1 D f .xi/C hf 0.xi/C
h2
</p>
<p>2
f 00.xi/C
</p>
<p>h3
</p>
<p>6
f 000Œ�.xi C h/&#141;
</p>
<p>D fi C hf 0i C
h2
</p>
<p>2
f 00i C
</p>
<p>h3
</p>
<p>6
f 000iC�� ; (2.9a)
</p>
<p>with fiC1 � f .xi C h/. Here �� is the fractional part � which has to be determined
according to �.xi C h/. In analogue we find for fi�1
</p>
<p>fi�1 D fi � hf 0i C
h2
</p>
<p>2
f 00i �
</p>
<p>h3
</p>
<p>6
f 000iC�� : (2.9b)
</p>
<p>Solving Eqs. (2.9) for the derivative f 0i leads directly to the definition of finite
difference derivatives.
</p>
<p>2.3 Finite Difference Derivatives
</p>
<p>We define the finite difference derivative or difference approximations
</p>
<p>DCfi D

Cfi
</p>
<p>h
D fiC1 � fi
</p>
<p>h
; (2.10a)
</p>
<p>as the forward difference derivative,
</p>
<p>2Please note that the symbols 
C,
�, and 
c in Eqs. (2.8) are linear operators acting on fi. For a
basic introduction to the theory of linear operators see for instance [4, 5].</p>
<p/>
</div>
<div class="page"><p/>
<p>2.3 Finite Difference Derivatives 21
</p>
<p>Fig. 2.2 Graphical
illustration of different finite
difference derivatives. The
solid line labeled f 0i
represents the real derivative
for comparison
</p>
<p>D�fi D

�fi
</p>
<p>h
D fi � fi�1
</p>
<p>h
; (2.10b)
</p>
<p>as the backward difference derivative, and
</p>
<p>Dcfi D

cfi
</p>
<p>2h
D fiC1 � fi�1
</p>
<p>2h
; (2.10c)
</p>
<p>as the central difference derivative.3 A graphical interpretation of these expressions
is straight forward and is presented in Fig. 2.2.
</p>
<p>Using the above definitions (2.10) together with the expansions (2.9) we obtain
</p>
<p>f 0i D DCfi �
h
</p>
<p>2
f 00i �
</p>
<p>h2
</p>
<p>6
f 000iC��
</p>
<p>D D�fi C
h
</p>
<p>2
f 00i �
</p>
<p>h2
</p>
<p>6
f 000iC��
</p>
<p>D Dcfi �
h2
</p>
<p>6
f 000iC�� : (2.11)
</p>
<p>We observe that in the central difference approximation of f 0i the truncation error
scales like h2 while it scales like h in the other two approximations; thus the central
</p>
<p>3The central difference derivative is related to the forward and backward difference derivatives via:
</p>
<p>Dc D
1
</p>
<p>2
.DC C D�/:</p>
<p/>
</div>
<div class="page"><p/>
<p>22 2 Numerical Differentiation
</p>
<p>difference approximation should have the smallest methodological error. Note that
the error is usually not dominated by the derivatives of f .x/ since we assumed that
f .x/ is a smooth function and sufficiently well approximated on the grid within
Œa; b&#141;. Furthermore we have to emphasize that the central difference approximation
is essentially a three point approximation, including fi�1, fi and fiC1, although fi
cancels. Thus, we can improve our approximation by taking even more grid-points
into account. For instance, we could combine the above finite difference derivatives.
Let us prepare this step by expanding Eqs. (2.9) to higher order derivatives. We then
obtain for the forward difference derivative
</p>
<p>DCfi D f 0i C
h
</p>
<p>2
f 00i C
</p>
<p>h2
</p>
<p>6
f 000i C
</p>
<p>h3
</p>
<p>24
f IVi C
</p>
<p>h4
</p>
<p>120
f Vi C : : : ; (2.12)
</p>
<p>for the backward difference derivative
</p>
<p>D�fi D f 0i �
h
</p>
<p>2
f 00i C
</p>
<p>h2
</p>
<p>6
f 000i �
</p>
<p>h3
</p>
<p>24
f IVi C
</p>
<p>h4
</p>
<p>120
f Vi � : : : ; (2.13)
</p>
<p>and, finally, for the central difference derivative
</p>
<p>Dcfi D f 0i C
h2
</p>
<p>6
f 000i C
</p>
<p>h4
</p>
<p>120
f Vi C : : : : (2.14)
</p>
<p>In order to improve the method we have to combine DCfi, D�fi and Dcfi from
different grid-points in such a way that at least the terms proportional to h2 cancel.
This can be achieved by observing that4
</p>
<p>8Dcfi � Dcfi�1 � DcfiC1 D 6f 0i �
h4
</p>
<p>5
f ViC�� ; (2.15)
</p>
<p>which gives
</p>
<p>f 0i D
1
</p>
<p>6
.8Dcfi � DcfiC1 � Dcfi�1/C
</p>
<p>h4
</p>
<p>30
f Vi
</p>
<p>D 1
12h
</p>
<p>. fi�2 � 8fi�1 C 8fiC1 � fiC2/C
h4
</p>
<p>30
f Vi : (2.16)
</p>
<p>Note that this simple combination yields an improvement of two orders in h ! One
can even improve the approximation in a similar fashion by simply calculating the
derivative from even more points, for instance fi˙3.
</p>
<p>4Please note that the TAYLOR expansion of .Dcfi�1CDcfiC1/=2 D . fiC2�fi�2/=.4h/ is equivalent
to the expansion (2.14) of Dcfi with h replaced by 2h.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.4 A Systematic Approach: The Operator Technique 23
</p>
<p>2.4 A Systematic Approach: The Operator Technique
</p>
<p>We would like to obtain a general expression which will allow to calculate the finite
difference derivatives of arbitrary order up to arbitrary order of h in the truncation
error. We achieve this goal by introducing the shift operator T and its inverse
operator T�1 as5
</p>
<p>Tfi D fiC1 ; (2.18)
</p>
<p>and
</p>
<p>T�1fi D fi�1 ; (2.19)
</p>
<p>where TT�1 D 1 is the unity operator. We can write these operators in terms of the
forward and backward difference operators
C and 
� of Eqs. (2.8), in particular
</p>
<p>T D 1C
C ; (2.20)
</p>
<p>and
</p>
<p>T�1 D 1 �
� : (2.21)
</p>
<p>Moreover, if D � d=dx denotes the derivative operator and if the n-th power of this
operator D is understood as the n-th successive application of it, we can rewrite the
TAYLOR expansions (2.9) as
</p>
<p>fiC1 D
�
</p>
<p>1C hD C 1
2
</p>
<p>h2D2 C 1
3Š
</p>
<p>h3D3 C : : :
�
</p>
<p>fi
</p>
<p>� exp .hD/ fi ; (2.22)
</p>
<p>5We note in passing that the shift operators form the discrete translational group, a very important
group in theoretical physics. Let T.n/ D Tn denote the shift by n 2 N grid-points. We then have
</p>
<p>T.n/T.m/ D T.n C m/ ; (2.17a)
T.0/ D 1 ; (2.17b)
</p>
<p>and
</p>
<p>T.n/�1 D T.�n/ ; (2.17c)
</p>
<p>which are the properties required to form a group. Here 1 denotes unity. Moreover, we have
</p>
<p>T.n/T.m/ D T.m/T.n/ ; (2.17d)
</p>
<p>i.e. it is an Abelian group. The group of discrete translations is usually denoted by Td [6].</p>
<p/>
</div>
<div class="page"><p/>
<p>24 2 Numerical Differentiation
</p>
<p>and
</p>
<p>fi�1 D
�
</p>
<p>1 � hD C 1
2
</p>
<p>h2D2 � 1
3Š
</p>
<p>h3D3 ˙ : : :
�
</p>
<p>fi
</p>
<p>� exp .�hD/ fi ; (2.23)
</p>
<p>Hence, we find that [7]6
</p>
<p>T D 1C
C � exp .hD/ ; (2.24)
</p>
<p>and, accordingly, that
</p>
<p>T�1 D 1 �
� � exp .�hD/ : (2.25)
</p>
<p>Finally, we obtain the central difference operator:
</p>
<p>
c D T � T�1 D exp .hD/ � exp .�hD/ � 2 sinh .hD/ : (2.26)
</p>
<p>Equations (2.24), (2.25) and (2.26) can be inverted for hD:
</p>
<p>hD D
</p>
<p>8
</p>
<p>ˆ̂
ˆ̂
ˆ̂
</p>
<p>&lt;̂
</p>
<p>ˆ̂
ˆ̂
ˆ̂
</p>
<p>:̂
</p>
<p>ln .1C
C/ D 
C �
1
</p>
<p>2

2C C
</p>
<p>1
</p>
<p>3

3C � : : : ;
</p>
<p>� ln .1 �
�/ D 
� C
1
</p>
<p>2

2� C
</p>
<p>1
</p>
<p>3

3� C : : : ;
</p>
<p>sinh�1
�

c
</p>
<p>2
</p>
<p>�
</p>
<p>D 
c
2
</p>
<p>� 1
3Š
</p>
<p>�

c
</p>
<p>2
</p>
<p>�3
</p>
<p>C 3
2
</p>
<p>5Š
</p>
<p>�

c
</p>
<p>2
</p>
<p>�5
</p>
<p>� : : : :
</p>
<p>(2.27)
</p>
<p>Again, the n-th power of an operator K (with K D 
C; 
�; 
c) Knfi is understood
as the n-th successive action of the operator K on fi, i.e. Kn�1 .Kfi/. Expres-
sion (2.27) allows to approximate the derivatives up to arbitrary order using finite
differences. Furthermore, we can take the k-th power of Eq. (2.27) in order to get an
approximate k-th derivative, .hD/k [7].
</p>
<p>However, it turns out that the expansion (2.27) in terms of the central difference

c does not optimally use the grid because it contains only odd powers of 
c. For
instance, the third power 
3c fi includes the function values fi˙3 and fi˙1 at &lsquo;odd&rsquo;
grid-points but ignores the function values fi and fi˙2 at &lsquo;even&rsquo; grid-points. Since
this is true for all odd powers of 
c we observe that the expansion (2.27) uses only
half of the grid. On the other hand, if one computes the square .hD/2 of (2.27) only
&lsquo;even&rsquo; grid-points are used, while the &lsquo;odd&rsquo; grid-points are ignored. This reduces
the accuracy of the method and an improvement is required. The easiest remedy
</p>
<p>6This representation of the shift operator T explains why the derivative operator D is frequently
referred to as the infinitesimal generator of translations [6].</p>
<p/>
</div>
<div class="page"><p/>
<p>2.4 A Systematic Approach: The Operator Technique 25
</p>
<p>is to formally introduce function values T˙
1
2 fi
</p>
<p>ŠD fi˙1=2 at intermediate grid-points7
xi˙1=2 D xi˙h=2. This definition allows to introduce the central difference operator
ıc of intermediate grid-points,
</p>
<p>ıc D T
1
2 � T� 12 D 2 sinh
</p>
<p>�
hD
</p>
<p>2
</p>
<p>�
</p>
<p>; (2.28)
</p>
<p>and the average operator:
</p>
<p>� D 1
2
</p>
<p>�
</p>
<p>T
1
2 C T� 12
</p>
<p>�
</p>
<p>D cosh
�
</p>
<p>hD
</p>
<p>2
</p>
<p>�
</p>
<p>: (2.29)
</p>
<p>The central difference operator
c on the grid is connected to the central difference
operator ıc of intermediate grid-points by:
</p>
<p>
c D 2�ıc: (2.30)
</p>
<p>To avoid the problem of Eq. (2.27) that only odd or even grid-points are accounted
for we replace all shift operators 
c=2 by ıc and then multiply the right hand side
of Eq. (2.27) by �. This ensures that function values at intermediate grid-points will
not appear in the final expression. Hence, we obtain for the first order derivative
operator:
</p>
<p>D D 1
h
</p>
<p>8
</p>
<p>ˆ̂
ˆ̂
ˆ̂
</p>
<p>&lt;̂
</p>
<p>ˆ̂
ˆ̂
ˆ̂
</p>
<p>:̂
</p>
<p>
C �
1
</p>
<p>2

2C C
</p>
<p>1
</p>
<p>3

3C � : : : ;
</p>
<p>
� C
1
</p>
<p>2

2� C
</p>
<p>1
</p>
<p>3

3� C : : : ;
</p>
<p>�ıc �
1
</p>
<p>3Š
�ı3c C
</p>
<p>32
</p>
<p>5Š
�ı5c � : : :
</p>
<p>(2.31)
</p>
<p>When higher order derivatives are calculated, we replace, again, 
c=2 by ıc and
multiply odd powers of ıc by �. This procedure results, for instance, in the second
order derivative operator:
</p>
<p>D2 D 1
h2
</p>
<p>8
</p>
<p>ˆ̂
ˆ̂
ˆ̂
&lt;
</p>
<p>ˆ̂
ˆ̂
ˆ̂
:
</p>
<p>
2C �
3C C
11
</p>
<p>12

4C � : : : ;
</p>
<p>
2� C
3� C
11
</p>
<p>12

4� C : : : ;
</p>
<p>ı2c �
1
</p>
<p>3
ı4c C
</p>
<p>8
</p>
<p>45
ı6c � : : : :
</p>
<p>(2.32)
</p>
<p>7These intermediate grid-points are virtual, auxiliary grid-points which will be eliminated in due
course.</p>
<p/>
</div>
<div class="page"><p/>
<p>26 2 Numerical Differentiation
</p>
<p>In particular, we obtain for the central difference derivative
</p>
<p>f 0i D
fiC1 � fi�1
</p>
<p>2h
C O.h2/ ; (2.33)
</p>
<p>and
</p>
<p>f 00i D
fiC1 � 2fi C fi�1
</p>
<p>h2
C O.h2/ : (2.34)
</p>
<p>Here, O.h2/ indicates that this term is of the order of h2 and we get the important
result that the truncation error is of the order O.h2/.8
</p>
<p>2.5 Concluding Discussion
</p>
<p>First of all, although Eq. (2.27) allows to approximate a derivative of any order k
arbitrarily close, it is still an infinite series which leaves us with the decision at
which order to truncate. This choice will highly depend on the choice of h which in
turn depends on the function we would like to differentiate. Consider, for instance,
the periodic function
</p>
<p>f .x/ D exp .i!x/ ; (2.35)
</p>
<p>where !; x 2 R and i is the imaginary unit with i2 D �1. Its first derivative is
</p>
<p>f 0.x/ D i! exp .i!x/ : (2.36)
</p>
<p>We now introduce grid-points by
</p>
<p>xk D x0 C kh ; (2.37)
</p>
<p>where h is the grid-spacing and x0 is some finite starting point on the real axis.
Accordingly,
</p>
<p>fk D exp Œi!.x0 C kh/&#141; ; (2.38)
</p>
<p>8The leading order of the truncation error can be determined by inserting the dominant contribution
of Eqs. (2.28) and (2.29) into the remainder of Eqs. (2.31) and (2.32), respectively. For instance,
it follows from Eq. (2.29) that � � O.1/ and from Eq. (2.28) that ıc � O.h/ and, hence, we
find with the help of Eq. (2.31) that �ı3c=h � O.h2/. In analogue, we obtain from Eq. (2.32) that
ı4c=h
</p>
<p>2 � O.h2/.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.5 Concluding Discussion 27
</p>
<p>and the exact value of the first derivative is
</p>
<p>f 0k D i! exp Œi!.x0 C kh/&#141; D i!fk : (2.39)
</p>
<p>We calculate the forward, backward, and central difference derivatives according to
Eqs. (2.10) and obtain
</p>
<p>DCfk D i!fk exp
�
</p>
<p>ih!
</p>
<p>2
</p>
<p>�
</p>
<p>sinc
</p>
<p>�
h!
</p>
<p>2
</p>
<p>�
</p>
<p>; (2.40a)
</p>
<p>with sinc.x/ D sin.x/=x and
</p>
<p>D�fk D i!fk exp
�
</p>
<p>� ih!
2
</p>
<p>�
</p>
<p>sinc
</p>
<p>�
h!
</p>
<p>2
</p>
<p>�
</p>
<p>; (2.40b)
</p>
<p>and
</p>
<p>Dcfk D i!fksinc.h!/ : (2.40c)
</p>
<p>We divide the approximate derivatives by the true value (2.39) and take the modulus.
We get
</p>
<p>ˇ
ˇ
ˇ
ˇ
</p>
<p>DCfk
f 0k
</p>
<p>ˇ
ˇ
ˇ
ˇ
D
ˇ
ˇ
ˇ
ˇ
</p>
<p>D�fk
f 0k
</p>
<p>ˇ
ˇ
ˇ
ˇ
D sinc
</p>
<p>�
h!
</p>
<p>2
</p>
<p>�
</p>
<p>; (2.41)
</p>
<p>and
ˇ
ˇ
ˇ
ˇ
</p>
<p>Dcfk
</p>
<p>f 0k
</p>
<p>ˇ
ˇ
ˇ
ˇ
D sinc.h!/ : (2.42)
</p>
<p>Since j sin.x/j � jxj, 8x 2 R we obtain that in all three cases this ratio is less
than one independent of h, unless ! D 0. (Please keep in mind that sinc.x/ ! 1
as x ! 0.) Hence, the first order finite difference approximations underestimate
the true value of the derivative. The reason is easily found: f .x/ oscillates with
frequency ! while the finite difference derivatives applied here approximate the
derivative linearly. Higher order corrections will, of course, improve the approxi-
mation significantly. Furthermore, we observe that the one-sided finite difference
derivatives (2.40a) and (2.40b) are exactly zero if h! D 2n� , n 2 N, i.e. if the grid-
spacing h matches a multiple of the frequency 2�! of the function f .x/. The same
occurs when central derivatives (2.40c) are used, but now for h! D �n. This is not
really a problem in our example because we choose the grid-spacing h � 2�=! in
order to approximate the function f .x/ sufficiently well. However, in many cases the
analytic form of the function is unknown and we only have its representation on the
grid. In this case one has to check carefully by changing h whether the function is
periodic or not.</p>
<p/>
</div>
<div class="page"><p/>
<p>28 2 Numerical Differentiation
</p>
<p>We discuss, finally, how to approximate partial derivatives of functions which
depend on more than one variable. Basically this can be achieved by independently
discretisizing the function of interest in each particular variable and then by defining
the corresponding finite difference derivatives. We will briefly discuss the case of
two variables and the extension to evenmore variables is straight forward.We regard
a function g.x; y/ where .x; y/ 2 Œa; b&#141; � Œc; d&#141;. We denote the grid-spacing in x-
direction by hx and in y-direction by hy. The evaluation of derivatives of the form
@n
</p>
<p>@xn
g.x; y/ or @
</p>
<p>n
</p>
<p>@yn
g.x; y/ for arbitrary n are approximated with the help of the schemes
</p>
<p>discussed above, only the respective grid-spacing has to be accounted for. We will
now briefly discuss mixed partial derivatives, in particular the derivative @
</p>
<p>2
</p>
<p>@x@y
g.x; y/.
</p>
<p>Higher orders can be easily obtained in the same fashion. Here, we will restrict to
the case of the central difference derivative. Again, the extension to the other two
forms of derivatives is straight forward. We would like to approximate the derivative
at the point .aC ihx; cC jhy/, which will be abbreviated by .i; j/. Hence, we compute
</p>
<p>@
</p>
<p>@y
</p>
<p>@
</p>
<p>@x
g.x; y/
</p>
<p>ˇ
ˇ
ˇ
ˇ
.i;j/
</p>
<p>D 1
2hx
</p>
<p>"
</p>
<p>@
</p>
<p>@y
g.x; y/
</p>
<p>ˇ
ˇ
ˇ
ˇ
.iC1;j/
</p>
<p>� @
@y
</p>
<p>g.x; y/
</p>
<p>ˇ
ˇ
ˇ
ˇ
.i�1;j/
</p>
<p>#
</p>
<p>C O.h2x/
</p>
<p>D 1
2hx
</p>
<p>"
</p>
<p>giC1;jC1 � giC1;j�1
2hy
</p>
<p>C O.h2y/
ˇ
ˇ
ˇ
ˇ
.iC1;j/
</p>
<p>� gi�1;jC1 � gi�1;j�1
2hy
</p>
<p>� O.h2y/
ˇ
ˇ
ˇ
ˇ
.i�1;j/
</p>
<p>#
</p>
<p>C O.h2x/ ; (2.43)
</p>
<p>where we made use of the notation gi;j � g.xi; yj/. Neglecting higher order
contributions yields
</p>
<p>@
</p>
<p>@y
</p>
<p>@
</p>
<p>@x
g.x; y/
</p>
<p>ˇ
ˇ
ˇ
ˇ
.i;j/
</p>
<p>� 1
2hx
</p>
<p>giC1;jC1 � giC1;j�1 � gi�1;jC1 C gi�1;j�1
2hy
</p>
<p>: (2.44)
</p>
<p>This simple approximation is easily improved with the help of methods developed
in the previous sections.
</p>
<p>It should be noted that there are also other methods to approximate derivatives.
One of the most powerful methods, is the method of finite elements [8]. The
conceptual difference to the method of finite differences is that one divides the
domain in finite sub-domains (elements) rather than by replacing these by sets of
discrete grid-points. The function of interest, say g.x; y/, is then replacedwithin each
element by an interpolating polynomial. However, this method is quite complex
and definitely beyond the scope of this book. Another interesting method, which is
particularly useful for the solution of hyperbolic differential equations, is the method
of finite volumes. The interested reader is referred to the book by R. J. LEVEQUE
[9].</p>
<p/>
</div>
<div class="page"><p/>
<p>2.5 Concluding Discussion 29
</p>
<p>Summary
</p>
<p>In a first step the notion of finite differences was introduced: All functions
are approximated only by their functional values at discrete grid-points and by
interpolation schemes between these points. This served as a basis for the definition
of finite difference derivatives. Three different types were discussed: the forward,
the backward, and the central difference derivative. A more systematic approach to
finite difference derivatives was then offered by the operator technique. It provided
ready to use equations which allowed to approximate a particular derivative of
arbitrary order to arbitrary order of grid-spacing. The two methodological errors
introduced by this method, namely the subtractive cancellation error due to too
dense a grid and the truncation error due to too coarse a grid were discussed in
detail.
</p>
<p>Problems
</p>
<p>1. Derive Eq. (2.32).
2. Calculate numerically the derivative of the function
</p>
<p>f .x/ D cos.!1x/C exp.�x2=2/ sin.!2x/;
</p>
<p>with !2 D 0:5 and !2 D 10!1. Use a non-uniform grid. Calculate locally the
relative error of your approximation.
</p>
<p>3. Extend your code of the previous example to arbitrary !1 � 10!2 and !2 D 0:5
by implementing an adaptive grid-spacing. In particular, write a routine which
recursively finds a suitable grid-spacing.
</p>
<p>4. Consider the finite interval I D Œ�5; 5&#141; on the real axis. Define N equally spaced
grid-points xi D x1 C .i � 1/h, i D 1; : : : ;N. Investigate the functions
</p>
<p>g.x/ D exp
�
</p>
<p>�x2
�
</p>
<p>and h.x/ D sin.x/:
</p>
<p>a. Plot these functions within the interval I by defining these functions on the
grid-points xi.
</p>
<p>b. Plot the first derivative of these functions by analytical differentiation.
c. Calculate and plot the first derivatives of these functions by employing the first
</p>
<p>order backward, forward, and central difference derivatives. For the central
difference derivative use an algorithm which is based on the grid-points xi�1
and xiC1 rather than the method based on intermediate grid-points xi˙ 12 .
</p>
<p>d. Calculate and plot the first central difference derivatives of these functions
by employing second order corrections. These corrections can be obtained by
applying the sum representation of the derivative operator defined in Sect. 2.4,
last line of Eq. (2.31), i.e. take the term proportional to ı3c into account!</p>
<p/>
</div>
<div class="page"><p/>
<p>30 2 Numerical Differentiation
</p>
<p>e. Calculate the absolute and the relative error of the above methods. Note that
the exact values are known analytically.
</p>
<p>f. Repeat the above steps for the second derivative of the function h.x/. For
the second order correction of the central difference derivative take the term
proportional to ı4c in Eq. (2.32) into account.
</p>
<p>g. Try different values of N.
</p>
<p>5. Consider the function:
</p>
<p>f .x; y/ D cos.x/ exp.�y2/:
</p>
<p>a. Calculate numerically its gradient rf .x; y/ and compare with the analytical
result.
</p>
<p>b. Demonstrate numerically that gradient fields are curl-free, i.e. r�rf .x; y/ D
0 for all x and y.
</p>
<p>References
</p>
<p>1. Jordan, C.: Calculus of Finite Differences, 3rd edn. AMSChelsea Publishing, Providence (1965)
2. S&uuml;li, E., Mayers, D.: An Introduction to Numerical Analysis. Cambridge University Press,
</p>
<p>Cambridge (2003)
3. Gautschi, W.: Numerical Analysis. Springer, Berlin/Heidelberg (2012)
4. Weidmann, J.: Lineare Operatoren in Hilbertr&auml;umen, vol. I: Grundlagen. Springer,
</p>
<p>Berlin/Heidelberg (2000)
5. Davies, E.B.: Linear Operators and Their Spectra. Cambridge Studies in Advanced Mathemat-
</p>
<p>ics. Cambridge University Press, Cambridge (2007)
6. Tung, W.K.: Group Theory in Physics. World Scientific, Hackensack (2003)
7. Lapidus, L., Pinder, G.F.: Numerical Solution of Partial Differential Equations. Wiley, NewYork
</p>
<p>(1982)
8. Gockenbach, M.S.: Understanding and Implementing the Finite Element Method. Cambridge
</p>
<p>University Press, Cambridge (2006)
9. LeVeque, R.J.: Finite Volume Methods for Hyperbolic Problems. Cambridge Texts in Applied
</p>
<p>Mathematics. Cambridge University Press, Cambridge (2002)</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 3
Numerical Integration
</p>
<p>3.1 Introduction
</p>
<p>Numerical integration is certainly one of the most important concepts in computa-
tional analysis since it plays a major role in the numerical treatment of differential
equations. Given a function f .x/ which is continuous on the interval Œa; b&#141;, one
wishes to approximate the integral by a discrete sum of the form
</p>
<p>Z b
</p>
<p>a
</p>
<p>dx f .x/ �
N
X
</p>
<p>iD1
!i f .xi/; (3.1)
</p>
<p>where the !i are referred to as weights and xi are the grid-points at which
the function needs to be evaluated. Such methods are commonly referred to as
quadrature [1, 2].
</p>
<p>We will mainly discuss two different approaches to the numerical integration of
arbitrary functions.We start with a rather simple approach, the rectangular rule. The
search of an improvement of this method will lead us first to the trapezoidal rule,
then to the SIMPSON rule and, finally, to a general formulation of the method, the
NEWTON-COTES quadrature. This will be followed by a more advanced technique,
the GAUSS-LEGENDRE quadrature. At the end of the chapter we will discuss an
elucidating example and briefly sketch extensions of all methods to more general
problems, such as integration of non-differentiable functions or the evaluation of
multiple integrals.
</p>
<p>Another very important approach, which is based on random sampling methods,
is the so calledMonte-Carlo integration. This methodwill be presented in Sect. 14.2.
</p>
<p>&copy; Springer International Publishing Switzerland 2016
B.A. Stickler, E. Schachinger, Basic Concepts in Computational Physics,
DOI 10.1007/978-3-319-27265-8_3
</p>
<p>31</p>
<p/>
</div>
<div class="page"><p/>
<p>32 3 Numerical Integration
</p>
<p>3.2 Rectangular Rule
</p>
<p>The straight forward approach to numerical integration is to employ the concept of
finite differences developed in Sect. 2.2.We regard a smooth function f .x/within the
interval Œa; b&#141;, i.e. f .x/ 2 C1Œa; b&#141;. The RIEMANN definition of the proper integral
of f .x/ from a to b states that:
</p>
<p>Z b
</p>
<p>a
</p>
<p>dx f .x/ D lim
N!1
</p>
<p>b � a
N
</p>
<p>N
X
</p>
<p>iD0
f
</p>
<p>�
</p>
<p>a C i b � a
N
</p>
<p>�
</p>
<p>: (3.2)
</p>
<p>We approximate the right hand side of this relation using equally spaced grid-points
xi 2 Œa; b&#141; according to Eq. (2.1) and find
</p>
<p>Z b
</p>
<p>a
</p>
<p>dx f .x/ � h
N�1
X
</p>
<p>iD1
fi: (3.3)
</p>
<p>It is clear that the quality of this approach strongly depends on the discretization
chosen, i.e. on the values of xi as illustrated schematically in Fig. 3.1. Again, a non-
uniform grid may be of advantage. We can estimate the error of this approximation
by expanding f .x/ into a TAYLOR series.
</p>
<p>We note that
</p>
<p>Z b
</p>
<p>a
</p>
<p>dx f .x/ D
N�1
X
</p>
<p>iD1
</p>
<p>Z xiC1
</p>
<p>xi
</p>
<p>dx f .x/; (3.4)
</p>
<p>Fig. 3.1 Illustration of the
numerical approximation of a
proper integral according to
Eq. (3.3)</p>
<p/>
</div>
<div class="page"><p/>
<p>3.2 Rectangular Rule 33
</p>
<p>hence, the approximation (3.3) is equivalent to an estimate of the area in the unit
interval, the elemental area:
</p>
<p>Z xiC1
</p>
<p>xi
</p>
<p>dx f .x/ � hfi: (3.5)
</p>
<p>Furthermore, we find following Eq. (2.9a):
</p>
<p>Z xiC1
</p>
<p>xi
</p>
<p>dx f .x/ D
Z xiC1
</p>
<p>xi
</p>
<p>dx
h
</p>
<p>fi C .x � xi/f 0iC"�
i
</p>
<p>D fih C O.h2/: (3.6)
</p>
<p>In this last step we applied the first mean value theorem for integration which states
that if f .x/ is continuous in Œa; b&#141;, then there exists a � 2 Œa; b&#141; such that
</p>
<p>Z b
</p>
<p>a
</p>
<p>dx f .x/ D .b � a/f .�/: (3.7)
</p>
<p>(We shall come back to the mean value theorem in the course of our discussion
of Monte-Carlo integration in Chap. 14.) Consequently, the error we make with
approximation (3.3) can be seen from Eq. (3.6) to be of the order O.h2/.
</p>
<p>This procedure corresponds to a forward difference approach and, equivalently,
backward differences can be used. This results in:
</p>
<p>Z b
</p>
<p>a
</p>
<p>dx f .x/ D h
N
X
</p>
<p>iD2
fi C O.h2/: (3.8)
</p>
<p>Let us now define the forward and backward rectangular rule by
</p>
<p>iI
C
iC1 D hfi ; (3.9)
</p>
<p>and
</p>
<p>iI
�
iC1 D hfiC1; (3.10)
</p>
<p>respectively. Thus, we obtain from TAYLOR&rsquo;s expansion that:
</p>
<p>Z xiC1
</p>
<p>xi
</p>
<p>dx f .x/ D iICiC1 C
h2
</p>
<p>2
f 0i C
</p>
<p>h3
</p>
<p>3Š
f 00i C : : :
</p>
<p>D iI�iC1 �
h2
</p>
<p>2
f 0iC1 C
</p>
<p>h3
</p>
<p>3Š
f 00iC1 � : : : : (3.11)</p>
<p/>
</div>
<div class="page"><p/>
<p>34 3 Numerical Integration
</p>
<p>However, the use of central differences gives more accurate results as has already
been observed in Chap. 2 in which the differential operator was approximated. We
make use of the concept of intermediate grid-points (see Sect. 2.4) and consider the
integral
</p>
<p>Z xiC1
</p>
<p>xi
</p>
<p>dx f .x/; (3.12)
</p>
<p>expand f .x/ in a TAYLOR series around the midpoint xiC 12 , and obtain:
</p>
<p>Z xiC1
</p>
<p>xi
</p>
<p>dx f .x/ D
Z xiC1
</p>
<p>xi
</p>
<p>dx
n
</p>
<p>fiC 12 C
�
</p>
<p>x � xiC 12
�
</p>
<p>f 0
iC 12
</p>
<p>C
</p>
<p>�
</p>
<p>x � xiC 12
�2
</p>
<p>2
f 00
iC 12
</p>
<p>C O
��
</p>
<p>x � xiC 12
�3
�
</p>
<p>9
</p>
<p>&gt;=
</p>
<p>&gt;;
</p>
<p>D hfiC 12 C
h3
</p>
<p>24
f 00iC��
</p>
<p>D iIiC1 C
h3
</p>
<p>24
f 00iC�� : (3.13)
</p>
<p>Thus, the error generated by this method, the central rectangular rule, scales as
O.h3/ which is a significant improvement in comparison to Eqs. (3.3) and (3.8).1
</p>
<p>We obtain
</p>
<p>Z b
</p>
<p>a
</p>
<p>dx f .x/ D h
N�1
X
</p>
<p>iD1
fiC 12 C O.h
</p>
<p>3/: (3.14)
</p>
<p>This approximation is known as the rectangular rule. It is illustrated in Fig. 3.2.
Note that the boundary points x1 D a and xN D b do not enter Eq. (3.14). Such a
procedure is commonly referred to as an open integration rule. On the other hand,
if the end-points are taken into account by the method it is referred to as a closed
integration rule.
</p>
<p>1In this context the intermediate position xiC1=2 is understood as a true grid-point. If, on the other
hand, the function value fiC1=2 is approximated by �fiC1=2, Eq. (2.29), the method is referred to as
the trapezoidal rule.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.3 Trapezoidal Rule 35
</p>
<p>Fig. 3.2 Scheme of the
rectangular integration rule
according to Eq. (3.14). Note
that boundary points do not
enter the evaluation of the
elemental areas
</p>
<p>3.3 Trapezoidal Rule
</p>
<p>An elegant alternative to the rectangular rule is found when the area between two
grid-points is approximated by a trapezoid as is shown schematically in Fig. 3.3.
The trapezoidal rule is obtained when the function values fiC1=2 at intermediate grid-
points on the right hand side of the central rectangular rule (3.13) are approximated
with the help of �fiC1=2, Eq. (2.29). Thus, the elemental area is calculated from
</p>
<p>Z xiC1
</p>
<p>xi
</p>
<p>dx f .x/ � h
2
. fi C fiC1/ : (3.15)
</p>
<p>and we obtain:
</p>
<p>Z b
</p>
<p>a
</p>
<p>dx f .x/ � h
2
</p>
<p>N�1
X
</p>
<p>iD1
.fi C fiC1/
</p>
<p>D h
�
</p>
<p>f1
</p>
<p>2
C f2 C : : :C fN�1 C
</p>
<p>fN
</p>
<p>2
</p>
<p>�
</p>
<p>D h
2
.f1 C fN/C h
</p>
<p>N�1
X
</p>
<p>iD2
fi
</p>
<p>D 1ITN : (3.16)
</p>
<p>Note that this integration rule is closed, although the boundary points f1 and fN
enter the summation (3.16) only with half the weight in comparison to all other
function values fi. This stems from the fact that the function values f1 and fN
contribute only to one elemental area, the first and the last one. Another noticeable</p>
<p/>
</div>
<div class="page"><p/>
<p>36 3 Numerical Integration
</p>
<p>Fig. 3.3 Sketch of how the
elemental areas under the
curve f .x/ are approximated
by trapezoids
</p>
<p>feature of the trapezoidal rule is that, in contrast to the rectangular rule (3.14), only
function values at grid-points enter the summation, which can be desirable in some
cases.
</p>
<p>The error of this method can be estimated by inserting expansion (2.9a) into
Eq. (3.16). One obtains for an elemental area:
</p>
<p>iI
T
iC1 D
</p>
<p>h
</p>
<p>2
.fi C fiC1/
</p>
<p>D hfi C
h2
</p>
<p>2
f 0i C
</p>
<p>h3
</p>
<p>4
f 00i C : : : : (3.17)
</p>
<p>On the other hand, we know from Eq. (3.6) that
</p>
<p>hfi D
Z xiC1
</p>
<p>xi
</p>
<p>dx f .x/� h
2
</p>
<p>2
f 0i �
</p>
<p>h3
</p>
<p>3Š
f 00i � : : : ; (3.18)
</p>
<p>which, when inserted into (3.17), yields
</p>
<p>iI
T
iC1 D
</p>
<p>Z xiC1
</p>
<p>xi
</p>
<p>dx f .x/C h
3
</p>
<p>12
f 00i C : : : : (3.19)
</p>
<p>Hence, we observe that the error induced by the trapezoidal rule is comparable to
the error of the rectangular rule, namely O.h3/. However, since we do not have to
compute function values at intermediate grid-points, this rule may be advantageous
in many cases.
</p>
<p>We remember from Chap. 2 that a more accurate estimate of a derivative was
achieved by increasing the number of grid-points involved which in the case of
integration leads us to the SIMPSON rule.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.4 The SIMPSON Rule 37
</p>
<p>3.4 The SIMPSON Rule
</p>
<p>The basic idea of the SIMPSON rule is to include higher order derivatives into the
expansion of the integrand. These higher order derivatives, which are primarily
unknown, are then approximated by expressions we obtained within the context of
finite difference derivatives. Let us discuss this procedure in greater detail. To this
purpose we will study the integral of f .x/ within the interval Œxi�1; xiC1&#141; and expand
the integrand around the midpoint xi:
</p>
<p>Z xiC1
</p>
<p>xi�1
</p>
<p>dx f .x/ D
Z xiC1
</p>
<p>xi�1
</p>
<p>dx
</p>
<p>�
</p>
<p>fi C .x � xi/f 0i C
.x � xi/2
2Š
</p>
<p>f 00i
</p>
<p>C .x � xi/
3
</p>
<p>3Š
f 000i C : : :
</p>
<p>�
</p>
<p>D 2hfi C
h3
</p>
<p>3
f 00i C O.h5/: (3.20)
</p>
<p>Inserting Eq. (2.34) for f 00i yields
</p>
<p>Z xiC1
</p>
<p>xi�1
</p>
<p>dx f .x/ D 2hfi C
h
</p>
<p>3
.fiC1 � 2fi C fi�1/C O.h5/
</p>
<p>D h
�
1
</p>
<p>3
fi�1 C
</p>
<p>4
</p>
<p>3
fi C
</p>
<p>1
</p>
<p>3
fiC1
</p>
<p>�
</p>
<p>C O.h5/: (3.21)
</p>
<p>Note that in contrast to the trapezoidal rule, the procedure described here is a three
point method since the function values at three different points enter the expression.
We can immediately write down the resulting integral from a to b. Since,
</p>
<p>Z b
</p>
<p>a
</p>
<p>dx f .x/ D
Z x2
</p>
<p>x0
</p>
<p>dx f .x/C
Z x4
</p>
<p>x2
</p>
<p>dx f .x/C : : :C
Z xN
</p>
<p>xN�2
</p>
<p>dx f .x/; (3.22)
</p>
<p>where we assumed that N is even and employed the discretization xi D x0C ih with
x0 D a and xN D b. We obtain:
Z b
</p>
<p>a
</p>
<p>dx f .x/ D h
3
.f0 C 4f1 C 2f2 C 4f3 C : : :C 2fN�2 C 4fN�1 C fN/C O.h5/:
</p>
<p>(3.23)
</p>
<p>This expression is exact for polynomials of degree n � 3 since the first term in
the error expansion involves the fourth derivative. Hence, whenever the integrand is
satisfactorily reproduceable by a polynomial of degree three or less, the SIMPSON
rule might give almost exact estimates, independent of the discretization h.</p>
<p/>
</div>
<div class="page"><p/>
<p>38 3 Numerical Integration
</p>
<p>The arguments applied above allow for a straightforward extension to four- or
even more-point rules. We find, for instance,
</p>
<p>Z xiC3
</p>
<p>xi
</p>
<p>dx f .x/ D 3h
8
.fi C 3fiC1 C 3fiC2 C fiC3/C O.h5/; (3.24)
</p>
<p>which is usually called SIMPSON&rsquo;s three-eight rule.
It is important to note that all the methods discussed so far are special cases of
</p>
<p>a more general formulation, the NEWTON-COTES rules [2] which will be discussed
in the next section.
</p>
<p>3.5 General Formulation: The NEWTON-COTES Rules
</p>
<p>We define the LAGRANGE interpolating polynomial pn�1.x/ of degree n � 1 [3&ndash;5]
to a function f .x/ as2
</p>
<p>pn�1.x/ D
n
X
</p>
<p>jD1
fjL
</p>
<p>.n�1/
j .x/; (3.25)
</p>
<p>where
</p>
<p>L
.n�1/
j .x/ D
</p>
<p>n
Y
</p>
<p>kD1
k&curren;j
</p>
<p>x � xk
xj � xk
</p>
<p>: (3.26)
</p>
<p>An arbitrary smooth function f .x/ can then be expressed with the help of a
LAGRANGE polynomial of degree n by
</p>
<p>f .x/ D pn�1.x/C
f .n/Œ�.x/&#141;
</p>
<p>nŠ
.x � x1/.x � x2/ : : : .x � xn/: (3.27)
</p>
<p>If we neglect the second term on the right hand side of this equation and integrate
the LAGRANGE polynomial of degree n � 1 over the n grid-points from x1 to xn we
obtain the closed n-point NEWTON-COTES formulas. For instance, if we set n D 2,
</p>
<p>2The LAGRANGE polynomial pn�1.x/ to the function f .x/ is the polynomial of degree n � 1 that
satisfies the n equations pn�1.xj/ D f .xj/ for j D 1; : : : ; n, where xj denotes arbitrary but distinct
grid-points.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.5 General Formulation: The NEWTON-COTES Rules 39
</p>
<p>then
</p>
<p>p1.x/ D f1L.1/1 .x/C f2L
.1/
2 .x/
</p>
<p>D f1
x � x2
x1 � x2
</p>
<p>C f2
x � x1
x2 � x1
</p>
<p>D 1
h
Œx.f2 � f1/ � x1f2 C x2f1&#141; ; (3.28)
</p>
<p>with f1 � f .x1/ and f2 � f .x2/. Integration over the respective interval yields
Z x2
</p>
<p>x1
</p>
<p>dx p1.x/ D
1
</p>
<p>h
</p>
<p>�
x2
</p>
<p>2
.f2 � f1/C x.x2f1 � x1f2/
</p>
<p>�ˇ
ˇ
ˇ
ˇ
</p>
<p>x2
</p>
<p>x1
</p>
<p>D h
2
Œf2 C f1&#141; ; (3.29)
</p>
<p>which is exactly the trapezoidal rule. By setting n D 3 one obtains SIMPSON&rsquo;s rule
and setting n D 4 gives the SIMPSON&rsquo;s three-eight rule.
</p>
<p>The open NEWTON-COTES rule can be obtained by integrating the polynomial
pn�1.x/ of degree n � 1 which includes the grid-points x1; : : : ; xn from x0 to xnC1.
The fact that these relations are open means that the function values at the boundary
points x0 D x1 � h and xnC1 D xn C h do not enter the final expressions. The
simplest open NEWTON-COTES formula is the central integral approximationwhich
we encountered as the rectangular rule (3.14). A second order approximation is
easily found with help of the two-point LAGRANGE polynomial (3.28)
</p>
<p>Z x3
</p>
<p>x0
</p>
<p>dx p1.x/ D
1
</p>
<p>h
</p>
<p>�
x2
</p>
<p>2
.f2 � f1/C x.x2f1 � x1f2/
</p>
<p>�ˇ
ˇ
ˇ
ˇ
</p>
<p>x3
</p>
<p>x0
</p>
<p>D 3h
2
Œf2 C f1&#141; : (3.30)
</p>
<p>Higher order approximations can be obtained in a similar fashion. To conclude
this section let us briefly discuss an idea which is referred to as ROMBERG&rsquo;s
method [6].
</p>
<p>So far, we approximated all integrals by expressions of the form
</p>
<p>I D I N C O.hm/; (3.31)
</p>
<p>where I is the exact, unknown, value of the integral, I N is the estimate obtained
from an integration scheme using N grid-points, and m is the leading order of the
error. Let us review the error of the trapezoidal approximation: we learned that the</p>
<p/>
</div>
<div class="page"><p/>
<p>40 3 Numerical Integration
</p>
<p>error for the integral over the interval Œxi; xiC1&#141; scales like h3. Since we have N such
intervals, we conclude that the total error behaves like .b�a/h2. Similarly, the error
of the three-point SIMPSON rule is for each sub-interval proportional to h5 and this
gives in total .b � a/h4. We assume that this trend can be generalized and conclude
that the error of an n-point method with the estimate In behaves like h2n�2. Since,
h / N�1 we have
</p>
<p>I D I Nn C
CN
</p>
<p>N2n�2
; (3.32)
</p>
<p>where CN depends on the number of grid-points N. Let us double the amount of
grid-points and we obtain:
</p>
<p>I D I 2Nn C
C2N
</p>
<p>.2N/2n�2
: (3.33)
</p>
<p>Obviously, Eqs. (3.32) and (3.33) can be regarded as a linear system of equations in
I and C if CN � C2N � C. Solving Eqs. (3.32) and (3.33) for I yields
</p>
<p>I � 1
4n�1 � 1
</p>
<p>�
</p>
<p>4n�1I 2Nn � I Nn
�
</p>
<p>: (3.34)
</p>
<p>It has to be emphasized that in the above expression I is no longer the exact value
because of the approximation CN � C. However, it is an improvement of the
solution and it is possible to demonstrate that this new estimate is exactly the value
one would have obtained with an integral approximation of order n C 1 and 2N
grid-points! Thus
</p>
<p>I
2N
</p>
<p>nC1 D
1
</p>
<p>4n�1 � 1
�
</p>
<p>4n�1I 2Nn � I Nn
�
</p>
<p>: (3.35)
</p>
<p>This suggests a very elegant and rapid procedure: We simply calculate the integrals
using two point rules and add the results according to Eq. (3.35) to obtain more-point
results. For instance, calculate I 22 and I
</p>
<p>4
2 , add these according to Eq. (3.35) and
</p>
<p>getI 43 . Now calculateI
8
2 , add I
</p>
<p>4
2 , get I
</p>
<p>8
3 , addI
</p>
<p>4
3 and get I
</p>
<p>8
4 . This pyramid-like
</p>
<p>procedure can be continued until convergence is achieved, that is jI Nm �I NmC1j &lt; �
where � &gt; 0 can be chosen arbitrarily. An illustration of this method is given in
Fig. 3.4.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.6 GAUSS-LEGENDRE Quadrature 41
</p>
<p>Fig. 3.4 Illustration of the ROMBERG method. Here, the I .m; n/ are synonyms for integrals I nm
where the first index m refers to the order of the quadrature while the second index n refers to the
number of grid-points used. Note that we only have to use a second order integration scheme (left
row inside the box), all other values are determined via Eq. (3.35) as indicated by the arrows
</p>
<p>3.6 GAUSS-LEGENDRE Quadrature
</p>
<p>In preparation for the GAUSS-LEGENDRE quadrature we introduce a set of orthogo-
nal LEGENDRE polynomialsP`.x/ [3, 4, 7, 8] which are solutions of the LEGENDRE
differential equation
</p>
<p>�
</p>
<p>1 � x2
�
</p>
<p>P00` .x/ � 2xP0`.x/C `.`C 1/P`.x/ D 0: (3.36)
</p>
<p>This equation occurs, for instance, when the LAPLACE equation 
f .x/ D 0 is
transformed to spherical coordinates. Here, we will introduce the most important
properties of LEGENDRE polynomials which will be required for an understanding
of the GAUSS-LEGENDRE quadrature.
</p>
<p>LEGENDRE polynomials are given by
</p>
<p>P`.x/ D
1
X
</p>
<p>kD0
ak;`x
</p>
<p>k; (3.37)
</p>
<p>where the coefficients ak;` can be determined recursively:
</p>
<p>akC2;` D
k.k C 1/� `.`C 1/
.k C 1/.k C 2/ ak;`: (3.38)
</p>
<p>Hence, for even values of ` the LEGENDRE polynomial involves only even powers
of x and for odd ` only odd powers of x. Note also that according to Eq. (3.38) for
k � ` the coefficients are equal to zero and, thus, it follows from Eq. (3.37) that the</p>
<p/>
</div>
<div class="page"><p/>
<p>42 3 Numerical Integration
</p>
<p>P`.x/ are polynomials of degree `. Furthermore, the LEGENDRE polynomials fulfill
the orthonomality condition
</p>
<p>Z 1
</p>
<p>�1
dxP`.x/P`0.x/ D
</p>
<p>2
</p>
<p>2`0 C 1ı``
0 ; (3.39)
</p>
<p>where ıij is KRONECKER&rsquo;s delta. One obtains, in particular,
</p>
<p>P0.x/ D 1; (3.40)
</p>
<p>and
</p>
<p>P1.x/ D x : (3.41)
</p>
<p>Another convenient way to calculate LEGENDRE polynomials is based on
RODRIGUES&rsquo; formula
</p>
<p>P`.x/ D
1
</p>
<p>2``Š
</p>
<p>d`
</p>
<p>dx`
�
</p>
<p>x2 � 1
�`
: (3.42)
</p>
<p>We focus now on the core of the GAUSS-LEGENDRE quadrature and introduce
the function F.x/ as a transform of the function f .x/
</p>
<p>F.x/ D b � a
2
</p>
<p>f
</p>
<p>�
b � a
2
</p>
<p>x C b C a
2
</p>
<p>�
</p>
<p>; (3.43)
</p>
<p>in such a way that we can rewrite the integral of interest as:
</p>
<p>Z b
</p>
<p>a
</p>
<p>dxf .x/ D
Z 1
</p>
<p>�1
dxF.x/: (3.44)
</p>
<p>If the function F.x/ can be well approximated by some polynomial of degree 2n�1,
like
</p>
<p>F.x/ � p2n�1.x/ ; (3.45)
</p>
<p>then this means that according to TAYLOR&rsquo;s theorem (2.7) the error introduced
by this approximation is proportional to F.2n/.x/. If the polynomial p2n�1.x/ is
explicitly given then we can apply the methods discussed in the previous sections
to approximate the integral (3.44). However, even if the polynomial is not explicitly
given we write the integral (3.44) as
</p>
<p>Z 1
</p>
<p>�1
dxF.x/ D
</p>
<p>nX
</p>
<p>iD1
!iF.xi/ ; (3.46)</p>
<p/>
</div>
<div class="page"><p/>
<p>3.6 GAUSS-LEGENDRE Quadrature 43
</p>
<p>with weights !i and grid-points xi, i D 1; : : : ; n which are yet undetermined!
Therefore, we will determine the weights!i and grid-points xi in such a way, that the
integral is well approximated even if the polynomial p2n�1 in Eq. (3.45) is unknown.
For this purpose we decompose p2n�1.x/ into
</p>
<p>p2n�1.x/ D pn�1.x/Pn.x/C qn�1.x/ ; (3.47)
</p>
<p>where Pn.x/ is the LEGENDRE polynomial of degree n and pn�1.x/ and qn�1.x/ are
polynomials of degree n � 1. Since pn�1.x/ itself is a polynomial of degree n � 1, it
can also be expanded in LEGENDRE polynomials of degrees up to n � 1 by
</p>
<p>pn�1.x/ D
n�1X
</p>
<p>iD0
aiPi.x/ : (3.48)
</p>
<p>Using Eq. (3.48) in (3.47) we obtain together with normalization relation (3.39)
</p>
<p>Z 1
</p>
<p>�1
dx p2n�1.x/ D
</p>
<p>n�1
X
</p>
<p>iD0
ai
</p>
<p>Z 1
</p>
<p>�1
dx Pi.x/Pn.x/C
</p>
<p>Z 1
</p>
<p>�1
dx qn�1.x/ D
</p>
<p>Z 1
</p>
<p>�1
dx qn�1.x/ :
</p>
<p>(3.49)
</p>
<p>Moreover, since Pn.x/ is a LEGENDRE polynomial of degree n it has n-zeros in the
interval Œ�1; 1&#141; and Eq. (3.47) results in
</p>
<p>p2n�1.xi/ D qn�1.xi/ ; (3.50)
</p>
<p>where x1; x2; : : : ; xn denote the zeros of Pn.x/ and these zeros determine the grid-
points of our integration routine. It is interesting to note, that these zeros are
independent of the function F.x/ we want to integrate. We also expand qn�1.x/ in
terms of LEGENDRE polynomials
</p>
<p>qn�1.x/ D
n�1
X
</p>
<p>iD0
biPi.x/ ; (3.51)
</p>
<p>and use it in Eq. (3.50) to obtain
</p>
<p>p2n�1.xi/ D
n�1
X
</p>
<p>kD0
bkPk.xi/ ; i D 1; : : : ; n ; (3.52)</p>
<p/>
</div>
<div class="page"><p/>
<p>44 3 Numerical Integration
</p>
<p>which can be written in a more compact form by defining pi � p2n�1.xi/ and Pki �
Pk.xi/:
</p>
<p>pi D
n�1X
</p>
<p>kD0
bkPki ; i D 1; : : : ; n : (3.53)
</p>
<p>It has to be emphasized again that the grid-points xi are independent of the
polynomial p2n�1.x/ and, therefore, independent of F.x/. Furthermore, we can
replace pi � F.xi/ � Fi according to Eq. (3.45). We recognize that Eq. (3.53)
corresponds to a system of linear equations which can be solved for the weights
bk. We obtain
</p>
<p>bk D
n
X
</p>
<p>iD1
Fi
�
</p>
<p>P�1
�
</p>
<p>ik
; (3.54)
</p>
<p>where P is the matrix P D fPijg, which is known to be non-singular. We can now
rewrite the integral (3.44) with the help of Eqs. (3.45), (3.49), and (3.51) together
with the properties of the zeros of LEGENDRE polynomials [7, 8] as
</p>
<p>Z 1
</p>
<p>�1
dx F.x/ �
</p>
<p>Z 1
</p>
<p>�1
dx p2n�1.x/ D
</p>
<p>n�1
X
</p>
<p>kD0
bk
</p>
<p>Z 1
</p>
<p>�1
dx Pk.x/ : (3.55)
</p>
<p>Since P0.x/ D 1 according to Eq. (3.40), we deduce from Eq. (3.39)
Z 1
</p>
<p>�1
dx Pk.x/ D
</p>
<p>Z 1
</p>
<p>�1
dx Pk.x/P0.x/ D
</p>
<p>2
</p>
<p>2k C 1ık0 D 2ık0 : (3.56)
</p>
<p>Hence, Eq. (3.55) reads
</p>
<p>Z 1
</p>
<p>�1
dx F.x/ � 2b0 D 2
</p>
<p>n
X
</p>
<p>iD1
Fi
�
</p>
<p>P�1
�
</p>
<p>i0
: (3.57)
</p>
<p>By defining
</p>
<p>!i D 2
�
</p>
<p>P�1
�
</p>
<p>i0
; (3.58)
</p>
<p>we arrive at the desired expansion
</p>
<p>Z 1
</p>
<p>�1
dxF.x/ �
</p>
<p>n
X
</p>
<p>iD1
!iFi : (3.59)</p>
<p/>
</div>
<div class="page"><p/>
<p>3.6 GAUSS-LEGENDRE Quadrature 45
</p>
<p>Moreover, since we approximated F.x/ by a polynomial of degree 2n � 1, the
GAUSS-LEGENDRE quadrature is exact for polynomials of degree 2n � 1, i.e.
the error is proportional to a derivative of F.x/ of order 2n. Furthermore, expres-
sion (3.58) can be put in a more convenient form. One can show that
</p>
<p>!i D
2
</p>
<p>.1� x2i /
�
</p>
<p>P0n.xi/
�2
; (3.60)
</p>
<p>where
</p>
<p>P0n.xi/ D
d
</p>
<p>dx
Pn.x/
</p>
<p>ˇ
ˇ
ˇ
ˇ
xDxi
</p>
<p>: (3.61)
</p>
<p>Let us make some concluding remarks. The grid-points xi as well as the weights
!i are independent of the actual function F.x/ we want to integrate. This means,
that one can table these values once and for all [7, 8] and use them for different
types of problems. The grid-points xi are symmetrically distributed around the
point x D 0, i.e. for every xj there is a �xj. Furthermore, these two grid-points
have the same weight !j. The density of grid-points increases approaching the
boundary, however, the boundary points themselves are not included, which means
that the GAUSS-LEGENDRE quadrature is an open method. Furthermore, it has to be
emphasized that low order GAUSS-LEGENDRE parameters can easily be calculated
by employing relation (3.42). This makes the GAUSS-LEGENDRE quadrature the
predominant integration method. In comparison to the trapezoidal rule or even
the ROMBERG method, it needs in many cases a smaller number of grid-points,
is simpler to implement, converges faster and yields more accurate results. One
drawback of this method is that one has to compute the function F.x/ at the zeros of
the LEGENDRE polynomial xi. This can be a problem if the integrand at hand is not
known analytically.
</p>
<p>It is important to note at this point that comparable procedures exist which
use other types of orthogonal polynomials, such as HERMITE polynomials. This
procedure is known as the GAUSS-HERMITE quadrature.
</p>
<p>Table 3.1 lists the methods, discussed in the previous sections, which allow to
calculate numerically an estimate of integrals of the form:
</p>
<p>Z b
</p>
<p>a
</p>
<p>dx f .x/ : (3.62)
</p>
<p>Equal grid-spacing h is assumed, with the GAUSS-LEGENDRE method as the only
exception. The particular value of h depends on the order of the method employed
and is given in Table 3.1.</p>
<p/>
</div>
<div class="page"><p/>
<p>46 3 Numerical Integration
</p>
<p>Table 3.1 Summary of the quadrature methods discussed in this chapter applied to the integral
R b
</p>
<p>a dxf .x/. For a detailed description consult the corresponding sections. Equal grid-spacing is
assumed for all methods except for the GAUSS-LEGENDRE quadrature. The explicit values of h
depend on the order of the method and are listed in the table. Furthermore, we use xi D aC ih and
denote f .xi/ D fi. The function P.m/.x/ which appears in the description of the NEWTON-COTES
rules denotes the m-th order LAGRANGE interpolating polynomial and Pm.x/ is the m-th degree
LEGENDRE polynomial
</p>
<p>n h I Method Comment
</p>
<p>1 b�a
2
</p>
<p>hf1 Rectangular Open
</p>
<p>2 b � a h
2
.f0 C f1/ Trapezoidal Closed
</p>
<p>3 b�a
2
</p>
<p>h
3
.f0 C 4f1 C f2/ SIMPSON Closed
</p>
<p>4 b�a
3
</p>
<p>3h
8
.f0 C 3f1 C 3f2 C f3/ SIMPSON 38 Closed
</p>
<p>m b�a
m�1
</p>
<p>R xm�1
x0
</p>
<p>dxP.m/.x/ NEWTON-COTES Closed
</p>
<p>m b�a
mC1
</p>
<p>R xmC1
x0
</p>
<p>dxP.m/.x/ NEWTON-COTES Open
</p>
<p>m Pm.xj/ D 0 b�a2
Pm
</p>
<p>jD1 !jf
�
</p>
<p>zj
�
</p>
<p>GAUSS-LEGENDRE Open
</p>
<p>zj D aCb2 C a�b2 xj
!j D 2
</p>
<p>.1�xj/2ŒP0m.xj/&#141;
2
</p>
<p>3.7 An Example
</p>
<p>Let us discuss as an example the following proper integral:
</p>
<p>I D
Z 1
</p>
<p>�1
</p>
<p>dx
</p>
<p>x C 2 D ln.3/� ln.1/ � 1:09861 : (3.63)
</p>
<p>We will now apply the various methods of Table 3.1 to approximate Eq. (3.63). Note
that these methods could give better results if a finer grid had been chosen. However,
since this is only an illustrative example, we wanted to keep it as simple as possible.
The rectangular rule gives
</p>
<p>IR D 1 �
1
</p>
<p>2
D 0:5 ; (3.64)
</p>
<p>the trapezoidal rule
</p>
<p>IT D
2
</p>
<p>2
</p>
<p>�
1
</p>
<p>1
C 1
3
</p>
<p>�
</p>
<p>D 4
3
D 1:333 : : : ; (3.65)
</p>
<p>and an application of the SIMPSON rule yields
</p>
<p>IS D
1
</p>
<p>3
</p>
<p>�
1
</p>
<p>1
C 4
2
C 1
3
</p>
<p>�
</p>
<p>D 10
9
</p>
<p>D 1:111 : : : : (3.66)</p>
<p/>
</div>
<div class="page"><p/>
<p>3.7 An Example 47
</p>
<p>Finally, we apply the GAUSS-LEGENDRE quadrature in a second order approxima-
tion. We could look up the parameters in [7, 8], however, for illustrative reasons we
will calculate those in this simple case. For a second order approximation we need
the LEGENDRE polynomial of second degree. It can be obtained from RODRIGUES&rsquo;
formula (3.42):
</p>
<p>P2.x/ D
1
</p>
<p>222Š
</p>
<p>d2
</p>
<p>dx2
�
</p>
<p>x2 � 1
�2
</p>
<p>D 1
2
</p>
<p>�
</p>
<p>3x2 � 1
�
</p>
<p>: (3.67)
</p>
<p>In a next step the zeros x1 and x2 of P2.x/ are determined from Eq. (3.67) which
results immediately in:
</p>
<p>x1;2 D ˙
1p
3
� ˙0:57735 : (3.68)
</p>
<p>The weights !1 and !2 can now be evaluated according to Eq. (3.60):
</p>
<p>!i D
2
</p>
<p>.1 � x2i /
�
</p>
<p>P02.xi/
�2
: (3.69)
</p>
<p>It follows from Eq. (3.67) that
</p>
<p>P02.x/ D 3x ; (3.70)
</p>
<p>and, thus,
</p>
<p>P02.x1/ D �
p
3 and P02.x2/ D
</p>
<p>p
3 : (3.71)
</p>
<p>This is used to calculate the weights from Eq. (3.69):
</p>
<p>!1 D !2 D 1 : (3.72)
</p>
<p>We combine the results (3.68) and (3.72) to arrive at the GAUSS-LEGENDRE
estimate of the integral (3.63):
</p>
<p>IGL D
1
</p>
<p>� 1p
3
C 2
</p>
<p>C 1
1p
3
C 2
</p>
<p>D 1:090909 : : : : (3.73)
</p>
<p>Obviously, a second order GAUSS-LEGENDRE approximation results already in a
much better estimate of the integral (3.63) than the trapezoidal rule which is also
of second order. It is also better than the estimate by the SIMPSON rule which is of
third order.</p>
<p/>
</div>
<div class="page"><p/>
<p>48 3 Numerical Integration
</p>
<p>3.8 Concluding Discussion
</p>
<p>Let us briefly discuss some further aspects of numerical integration. In many cases
one is confronted with improper integrals of the form
</p>
<p>Z 1
</p>
<p>a
</p>
<p>dx f .x/;
Z a
</p>
<p>�1
dx f .x/; or
</p>
<p>Z 1
</p>
<p>�1
dx f .x/ : (3.74)
</p>
<p>The question arises whether or not we can treat such an integral with the methods
discussed so far. The answer is yes, it is possible as we will demonstrate using the
integral
</p>
<p>I D
Z 1
</p>
<p>a
</p>
<p>dx f .x/ : (3.75)
</p>
<p>as an example; other integrals can be treated in a similar fashion. We rewrite
Eq. (3.75) as
</p>
<p>I D lim
b!1
</p>
<p>Z b
</p>
<p>a
</p>
<p>dx f .x/ D lim
b!1
</p>
<p>I.b/ : (3.76)
</p>
<p>One now calculates I.b1/ for some b1 &gt; a and I.b2/ for some b2 &gt; b1. If jI.b2/ �
I.b1/j &lt; �, where � &gt; 0 is the required accuracy, the resulting value I.b2/ can
be regarded as the appropriate estimate to I.3 However, in many cases it is easier
to perform an integral transform in order to map the infinite interval onto a finite
interval. For instance, consider [9]
</p>
<p>I D
Z 1
</p>
<p>0
</p>
<p>dx
1
</p>
<p>.1C x2/
4
3
</p>
<p>: (3.77)
</p>
<p>The transformation
</p>
<p>t D 1
1C x (3.78)
</p>
<p>gives
</p>
<p>I D
Z 1
</p>
<p>0
</p>
<p>dt
t
2
3
</p>
<p>Œt2 C .1 � t/2&#141;
4
3
</p>
<p>: (3.79)
</p>
<p>3Particular care is required when dealing with periodic functions!</p>
<p/>
</div>
<div class="page"><p/>
<p>3.8 Concluding Discussion 49
</p>
<p>Thus, we mapped the interval Œ0;1/ ! Œ0; 1&#141;. Integral (3.79) can now be
approximatedwith help of the methods discussed in the previous sections. These can
also be applied to approximate convergent integrals whose integrand shows singular
behavior within Œa; b&#141;.
</p>
<p>If the integrand f .x/ is not smooth within the interval I W x 2 Œa; b&#141;we can split the
total integral into a sum over sub-intervals. For instance, if we consider the function
</p>
<p>f .x/ D
�
</p>
<p>x cos.x/; x &lt; 0 ;
x sin.x/; x � 0 ;
</p>
<p>we can calculate the integral over the interval I W x 2 Œ�10; 10&#141; as
Z 10
</p>
<p>�10
dx f .x/ D
</p>
<p>Z 0
</p>
<p>�10
dx x cos.x/C
</p>
<p>Z 10
</p>
<p>0
</p>
<p>dx x sin.x/ :
</p>
<p>We generalize this result and write
</p>
<p>Z
</p>
<p>I
</p>
<p>dx f .x/ D
X
</p>
<p>k
</p>
<p>Z
</p>
<p>Ik
</p>
<p>dx f .x/ ; (3.80)
</p>
<p>with sub-intervals Ik 2 I;8k and the integrand f .x/ is assumed to be smooth within
each sub-interval Ik but not necessarily within the interval I. We can then apply one
of the methods discussed in this chapter to calculate an estimate of the integral over
any of the sub-intervals Ik.
</p>
<p>Similar to the discussion in Sect. 2.5 about the approximation of partial deriva-
tives on the basis of finite differences, one can apply the rules of quadrature
developed here for different dimensions to obtain an estimate of multi-dimensional
integrals. However, the complexity of the problem is significantly increased if the
integration boundaries are functions of the variables rather than constants. For
instance,
</p>
<p>Z b
</p>
<p>a
</p>
<p>dx
Z '2.x/
</p>
<p>'1.x/
</p>
<p>dy f .x; y/ : (3.81)
</p>
<p>Such cases are rather difficult to handle and the method to choose depends highly
on the form of the functions '1.x/, '2.x/ and f .x; y/. We will not deal with integrals
of this kind because this is beyond the scope of this book. The interested reader is
referred to books by DAHLQUIST and BJ&Ouml;RK [10] and by PRESS et al. [11].
</p>
<p>In a final remark we would like to point out that it can be of advantage to utilize
the properties of FOURIER transforms when integrals of the convolution type are to
be approximated numerically (see Appendix D).</p>
<p/>
</div>
<div class="page"><p/>
<p>50 3 Numerical Integration
</p>
<p>Summary
</p>
<p>The starting point was the concept of finite differences (Sect. 2.2). Based on this
concept proper integrals over smooth functions f .x/ were approximated by a sum
over elemental areas with the elemental area defined as the area under f .x/ between
two consecutive grid-points. The simplest method, the rectangular rule, was based
on forward/backward differences. It was a closed method, i.e. the functional values
at the boundaries were included. On the other hand, a rectangular rule based on
central differences was an open method, i.e. the functional values at the boundaries
were not included. Application of the TAYLOR expansion (2.7) revealed that the
methodological error of the rectangular rule was of order O.h2/. With the elemental
area approximated by a trapezoid we arrived at the trapezoidal rule. It was a closed
method and the methodological error was of order O.h3/. The inclusion of higher
order derivatives of f .x/ allowed the derivation of the SIMPSON rules of quadrature.
They resulted a remarkable reduction of the methodological error. A more general
formulation of all these methods was based on the interpolation of the function f .x/
using LAGRANGE interpolating polynomials of degree n and resulted in the class
of NEWTON-COTES rules. For various orders of n of the interpolating polynomial
all the above rules were derived. Within this context a particularly useful method,
the ROMBERG method, was discussed. By adding diligently only two-point rules
the error of the numerical estimate of the integral has been made arbitrarily small.
An even more general approach was offered by the GAUSS-LEGENDRE quadrature
which used LEGENDRE polynomials of degree ` to approximate the function f .x/.
The grid-points were defined by the zeros of the `-th degree polynomial and the
weights !i in Eq. (3.1) were proportional to the square of the inverse first derivative
of the polynomial. This method had the enormous advantage that the grid-points
and weights were independent of the function f .x/ and, thus, could be determined
once and for all for any polynomial degree `. Error analysis proved that this method
had the smallest methodological error.
</p>
<p>Problems
</p>
<p>We consider the interval I D Œ�5; 5&#141; together with the functions g.x/ and h.x/:
</p>
<p>g.x/ D exp
�
</p>
<p>�x2
�
</p>
<p>and h.x/ D sin.x/ :
</p>
<p>We discretize the interval I by introducing N equally spaced grid-points. The
correspondingN�1 sub-intervals are denoted by Ij, j D 1; : : :N�1. In the following
we wish to calculate estimates of the integrals
</p>
<p>I1 D
Z
</p>
<p>I
</p>
<p>dx g.x/ and I2 D
Z
</p>
<p>I
</p>
<p>dx h.x/ :</p>
<p/>
</div>
<div class="page"><p/>
<p>3.8 Concluding Discussion 51
</p>
<p>Furthermore, we add a third integral of the form
</p>
<p>I3 D
Z
</p>
<p>I
</p>
<p>dx h2.x/ D
Z
</p>
<p>I
</p>
<p>dx sin2.x/ ;
</p>
<p>to our discussion.
</p>
<p>1. Evaluate I1 with the help of the error function erf.x/, which should be supplied
by the environment you use as an intrinsic function. Note that the error function
is defined as
</p>
<p>erf.x/ D 2p
�
</p>
<p>Z x
</p>
<p>0
</p>
<p>dz exp.�z2/: (3.82)
</p>
<p>Hence you should be able to express I1 in terms of erf.x/.
2. Calculate I2 and I3 analytically.
3. In order to approximate I1, I2 and I3 with the help of the two second order
</p>
<p>methods we discussed in this chapter, employ the following strategy: First the
integrals are rewritten as
</p>
<p>Z
</p>
<p>I
</p>
<p>dx � D
X
</p>
<p>i
</p>
<p>Z
</p>
<p>Ii
</p>
<p>dx � ;
</p>
<p>where � is a placeholder for g.x/, h.x/ and h2.x/ and Ii, i D 1; : : : ;N are suitable
intervals. In a second step the integrals are approximated with (i) the central
rectangular rule and (ii) the trapezoidal rule.
</p>
<p>4. In addition, we approximate the integrals I1, I2 and I3 by employing SIMP-
SON&rsquo;s rule for odd N. Here
</p>
<p>Z
</p>
<p>I
</p>
<p>dx � D
Z
</p>
<p>I1[I2
dx � C
</p>
<p>Z
</p>
<p>I3[I4
dx � C : : :C
</p>
<p>Z
</p>
<p>IN�2[IN�1
dx � ;
</p>
<p>is used as it was discussed in Sect. 3.4.
5. Compare the results obtained with different algorithms and different numbers of
</p>
<p>grid-points, N. Plot the absolute and the relative error as a function of N.
6. Approximate numerically the integral
</p>
<p>I D
Z 1
</p>
<p>�1
dx g.x/:
</p>
<p>7. Calculate the integral over the function
</p>
<p>f .x; y/ D exp
�
</p>
<p>�y
2
</p>
<p>2
</p>
<p>�
</p>
<p>cos.x/</p>
<p/>
</div>
<div class="page"><p/>
<p>52 3 Numerical Integration
</p>
<p>within the intervals x 2 Œ�10�; 10�&#141; and y 2 Œ�10; 10&#141; with the help of an
approximation of your choice.
</p>
<p>8. Demonstrate numerically that the line integral over closed loops C of the
function f .x; y/ of the previous problem vanishes:
</p>
<p>I
</p>
<p>C
</p>
<p>ds � rf .x; y/ D 0:
</p>
<p>References
</p>
<p>1. Jordan, C.: Calculus of Finite Differences, 3rd edn. AMS Chelsea, Providence (1965)
2. Ueberhuber, C.W.: Numerical Computation 2: Methods, Software and Analysis. Springer,
</p>
<p>Berlin/Heidelberg (1997)
3. Mathai, A.M., Haubold, H.J.: Special Functions for Applied Scientists. Springer,
</p>
<p>Berlin/Heidelberg (2008)
4. Beals, R., Wong, R.: Special Functions. Cambridge Studies in Advanced Mathematics.
</p>
<p>Cambridge University Press, Cambridge (2010)
5. Fornberg, B.: A Practical Guide to Pseudospectral Methods. Cambridge Monographs on
</p>
<p>Applied and Computational Mathematics. Cambridge University Press, Cambridge (1999)
6. Stoer, J., Bulirsch, R.: Introduction to Numerical Analysis, 2nd edn. Springer,
</p>
<p>Berlin/Heidelberg (1993)
7. Abramovitz, M., Stegun, I.A. (eds.): Handbook of Mathemathical Functions. Dover, New York
</p>
<p>(1965)
8. Olver, F.W.J., Lozier, D.W., Boisvert, R.F., Clark, C.W.: NIST Handbook of Mathematical
</p>
<p>Functions. Cambridge University Press, Cambridge (2010)
9. Sormann, H.: Numerische Methoden in der Physik. Lecture Notes. Institute of Theoretical and
</p>
<p>Computational Physics, Graz University of Technology (2011)
10. Dahlquist, G., Bj&ouml;rk, &Aring;.: Numerical Methods in Scientific Computing. Cambridge University
</p>
<p>Press, Cambridge (2008)
11. Press, W.H., Teukolsky, S.A., Vetterling, W.T., Flannery, B.P.: Numerical Recipes in C++, 2nd
</p>
<p>edn. Cambridge University Press, Cambridge (2002)</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 4
The KEPLER Problem
</p>
<p>4.1 Introduction
</p>
<p>The KEPLER problem [1&ndash;6] is certainly one of the most important problems in the
history of physics and natural sciences in general. We will study this problem for
several reasons: (i) it is a nice demonstration of the applicability of the methods
introduced in the previous chapters, (ii) important concepts of the numerical
treatment of ordinary differential equations can be introduced quite naturally, and
(iii) it allows to revisit some of the most important aspects of classical mechanics.
</p>
<p>The KEPLER problem is a special case of the two-body problem which is
discussed in Appendix A. Let us summarize the main results. We consider two
point particles interacting via the rotationally symmetric two body potential U
which is solely a function of the distance between the particles. The symmetries
of this problem allow several simplifications: (i) The problem can be reduced to
the two dimensional motion of a point particle with reduced mass m in the central
potential U. (ii) By construction, the total energy E is conserved. (iii) The length `
of the angular momentum vector is also conserved because of the symmetry of the
potential U. Due to this rotational symmetry it is a natural choice to describe the
particle&rsquo;s motion in polar coordinates .�; '/.
</p>
<p>The final differential equations which have to be solved are of the form
</p>
<p>P' D `
m�2
</p>
<p>; (4.1)
</p>
<p>and
</p>
<p>P� D ˙
s
</p>
<p>2
</p>
<p>m
</p>
<p>�
</p>
<p>E � U.�/ � `
2
</p>
<p>2m�2
</p>
<p>�
</p>
<p>: (4.2)
</p>
<p>&copy; Springer International Publishing Switzerland 2016
B.A. Stickler, E. Schachinger, Basic Concepts in Computational Physics,
DOI 10.1007/978-3-319-27265-8_4
</p>
<p>53</p>
<p/>
</div>
<div class="page"><p/>
<p>54 4 The KEPLER Problem
</p>
<p>Here, one usually defines the effective potential
</p>
<p>Ueff.�/ D U.�/C
`2
</p>
<p>2m�2
; (4.3)
</p>
<p>as the sum of the interaction potential and the centrifugal barrier Umom.�/ D
`2=2m�2. Equation (4.2) can be transformed into an implicit equation for �
</p>
<p>t D t0 ˙
Z �
</p>
<p>�0
</p>
<p>d�0
�
2
</p>
<p>m
</p>
<p>�
</p>
<p>E � Ueff.�0/
�
�� 12
</p>
<p>; (4.4)
</p>
<p>with �0 � �.t0/ the initial condition at time t0. Furthermore, the angle ' is related
to the radius � by
</p>
<p>' D '0 ˙
Z �
</p>
<p>�0
</p>
<p>d�0
`
</p>
<p>m�02
</p>
<p>�
2
</p>
<p>m
</p>
<p>�
</p>
<p>E � Ueff.�0/
�
�� 12
</p>
<p>; (4.5)
</p>
<p>with the initial condition '0 � '.t0/.
The KEPLER problem is defined by the gravitational interaction potential
</p>
<p>U.�/ D �˛
�
; ˛ &gt; 0: (4.6)
</p>
<p>For this case, we show in Fig. 4.1 schematically the effective potential (4.3) (solid
black line), together with the gravitational potential U.�/ (dashed-dotted line) and
the centrifugal barrier Umom (dashed line). The gravitational potential (4.6) is now
</p>
<p>Fig. 4.1 Schematic illustration of the effective potential Ueff.�/=Ueff.�0/ vs �=�0 (solid line, right
hand scale). Here, �0 is the distance of the minimum in Ueff.�/. Ugrav.�/ (dashed-dotted line)
denotes the gravitational contribution while Umom.�/ (dashed line) denotes the centrifugal barrier.
Both potentials are normalized to Ueff.�0/ (Left hand scale applies)</p>
<p/>
</div>
<div class="page"><p/>
<p>4.1 Introduction 55
</p>
<p>inserted into Eq. (4.5):
</p>
<p>' D '0 ˙
Z �
</p>
<p>�0
</p>
<p>d�0
`
</p>
<p>m�02
</p>
<p>�
2
</p>
<p>m
</p>
<p>�
</p>
<p>E C ˛
�0
</p>
<p>� `
2
</p>
<p>2m�02
</p>
<p>��� 12
: (4.7)
</p>
<p>The substitution u D 1=� simplifies Eq. (4.7) to
</p>
<p>' D '0 �
Z u2
</p>
<p>u1
</p>
<p>du
</p>
<p>�
2mE
</p>
<p>`2
C 2m˛
</p>
<p>`2
u � u2
</p>
<p>�� 12
; (4.8)
</p>
<p>where the integration boundaries u1 and u2 are 1=�0 and 1=�, respectively. The
integral can now be evaluated with the help of a simple substitution1 and we obtain
the angle ' as a function of �:
</p>
<p>' D '0 ˙ cos�1
</p>
<p>0
</p>
<p>B
@
</p>
<p>`
�
� m˛
</p>
<p>`
q
</p>
<p>2mE C m2˛2
`2
</p>
<p>1
</p>
<p>C
AC const : (4.9)
</p>
<p>This solution can conveniently be characterized by the introduction of two parame-
ters, namely
</p>
<p>a D `
2
</p>
<p>m˛
(4.10)
</p>
<p>and the eccentricity e
</p>
<p>e D
</p>
<p>s
</p>
<p>1C 2E`
2
</p>
<p>m˛2
: (4.11)
</p>
<p>Hence, by neglecting the integration constant and setting '0 D 0 we arrive at
</p>
<p>a
</p>
<p>�
D 1C e cos .'/ (4.12)
</p>
<p>as the final form of Eq. (4.9). It describes for e &gt; 1 a hyperbola, for e D 1 a parabola,
and for e &lt; 1 an ellipse. The case e D 0 is a special case of the ellipse and describes
a circle with radius � D a. A more detailed discussion of this result, in particular the
</p>
<p>1In particular, we substitute
</p>
<p>w D
�
</p>
<p>u � m˛
`2
</p>
<p>��2mE
</p>
<p>`2
C m
</p>
<p>2˛2
</p>
<p>`4
</p>
<p>�� 12
</p>
<p>:</p>
<p/>
</div>
<div class="page"><p/>
<p>56 4 The KEPLER Problem
</p>
<p>derivation of KEPLER&rsquo;s laws can be found in any textbook on classical mechanics
[1&ndash;6]. We discuss now some numerical aspects.
</p>
<p>4.2 Numerical Treatment
</p>
<p>In the previous section we solved the KEPLER problem by evaluating the inte-
grand (4.7) expressing the angle ' as a function of the radius �. However, in this
section we aim at solving the integral equation (4.4) numerically with the help of
the methods discussed in the previous chapter. Remember that Eq. (4.4) expresses
the time t as a function of the radius �. This equation has to be inverted, in order to
obtain �.t/, which, in turn, is then inserted into Eq. (4.1) in order to determine the
angle '.t/ as a function of time. This discussion will lead us in a natural way to the
most common techniques applied to solve ordinary differential equations, which is
of no surprise since Eq. (4.4) is the integral representation of Eq. (4.2).
</p>
<p>We give a short outline of what we plan to do: We discretize the time axis in
equally spaced time steps 
t, i.e. tn D t0 C n
t. Accordingly, we define the radius
� at time t D tn as �.tn/ � �n. We can use the methods introduced in Chap. 3 to
approximate the integral (4.4) from some �n to �nC1. According to this chapter the
absolute error introduced will behave like ı D j�n ��nC1jK where the explicit value
of K depends on the method used. However, since the radius � changes continuously
with time t we know that for sufficiently small values of 
t the error ı will also
become arbitrarily small. If we start from some initial values t0 and �0, we can
successively calculate the values �1, �2 , . . . , by applying a small time step 
t.
</p>
<p>Let us start by rewriting Eq. (4.4) as:
</p>
<p>t � t0 D
Z �
</p>
<p>�0
</p>
<p>d�0 f .�0/ : (4.13)
</p>
<p>As we discretized the time axis in equally spaced increments and defined �n � �.tn/,
we can rewrite (4.13) as
</p>
<p>
t D tn � tn�1 D
Z �nC1
</p>
<p>�n
</p>
<p>d�0f .�0/ : (4.14)
</p>
<p>The forward rectangular rule, (3.9), results in the approximation
</p>
<p>
t D .�nC1 � �n/ f .�n/ : (4.15)</p>
<p/>
</div>
<div class="page"><p/>
<p>4.2 Numerical Treatment 57
</p>
<p>We solve this equation for �nC1 and obtain
</p>
<p>�nC1 D h.�n/
t C �n ; (4.16)
</p>
<p>where we defined
</p>
<p>h.�/ D 1
f .�/
</p>
<p>D
r
</p>
<p>2
</p>
<p>m
ŒE � Ueff.�/&#141; ; (4.17)
</p>
<p>following Eqs. (4.2) and (4.3). As Eq. (4.4) is the integral representation of the ordi-
nary differential equation (4.2), approximation (4.16) corresponds to the approxi-
mation
</p>
<p>DC�n D h.�n/ ; (4.18)
</p>
<p>where DC�n is the forward difference derivative (2.10a). Since the left hand side
of the discretized differential equation (4.18) is independent of �nC1, this method
is referred to as an explicit method. In particular, consider an ordinary differential
equation of the form
</p>
<p>Py D F.y/ : (4.19)
</p>
<p>Then the approximation method is referred to as an explicit EULER method if it is
of the form
</p>
<p>ynC1 D yn C F.yn/
t : (4.20)
</p>
<p>Note that y might be a vector.
Let us use the backward rectangular rule (3.10) to solve Eq. (4.14). We obtain
</p>
<p>tnC1 � tn D .�nC1 � �n/ f .�nC1/ ; (4.21)
</p>
<p>or equivalently
</p>
<p>�nC1 D �n C h.�nC1/
t : (4.22)
</p>
<p>Again, this corresponds to an approximation of the differential equation (4.2) by
</p>
<p>D��nC1 D h.�nC1/ ; (4.23)
</p>
<p>where D�.�nC1/ is the backward difference derivative (2.10b). In this case the
quantity of interest �nC1 still appears in the argument of the function h.�/ and
Eq. (4.22) is an implicit equation for �nC1 which has to be solved. In general, if
the problem (4.19) is approximated by an algorithm of the form
</p>
<p>ynC1 D yn C F.ynC1/
t ; (4.24)</p>
<p/>
</div>
<div class="page"><p/>
<p>58 4 The KEPLER Problem
</p>
<p>it is referred to as an implicit EULER method. Note that the implicit equation (4.24)
might be analytically unsolvable. Hence, one has to employ a numerical method to
solve (4.24) which will also imply a numerical error. However, in the particular case
of Eq. (4.22) we can solve it analytically since it is a fourth order polynomial in �nC1
of the form
</p>
<p>�4nC1 � 2�n�3nC1 C
�
</p>
<p>�2n �
2E
t2
</p>
<p>m
</p>
<p>�
</p>
<p>�2nC1 �
2˛
t2
</p>
<p>m
�nC1 C
</p>
<p>`2
t2
</p>
<p>m2
D 0 : (4.25)
</p>
<p>The solution of this equation is quite tedious and will not be discussed here,
however, the method one employs is referred to as FERRARI&rsquo;s method [7].
</p>
<p>A natural way to proceed is to regard the central rectangular rule (3.13) in a next
step. Within this approximation we obtain for Eq. (4.13)
</p>
<p>
t D .�nC1 � �n/f
�
�nC1 C �n
</p>
<p>2
</p>
<p>�
</p>
<p>; (4.26)
</p>
<p>which is equivalent to the implicit equation
</p>
<p>�nC1 D �n C h
�
�nC1 C �n
</p>
<p>2
</p>
<p>�
</p>
<p>
t : (4.27)
</p>
<p>It can be written as an approximation to Eq. (4.2) with help of the central difference
derivative Dc�nC 12 :
</p>
<p>Dc�nC 12 D h
�
�nC1 C �n
</p>
<p>2
</p>
<p>�
</p>
<p>: (4.28)
</p>
<p>In general, for a problem of the form (4.19) a method of the form
</p>
<p>ynC1 D yn C F
�
</p>
<p>ynC1 C yn
2
</p>
<p>�
</p>
<p>
t ; (4.29)
</p>
<p>is referred to as the implicit midpoint rule. We note that this method might be more
accurate since the error of the rectangular rule scales like O.
t2/ while the error of
the forward and backward rectangular rules scale like O.
t/. Nevertheless, in case
of the KEPLER problem, one can solve the implicit equation (4.27) analytically for
�nC1 which is certainly of advantage.
</p>
<p>In this chapter the KEPLER problem was instrumental in introducing three
commonmethods which can be employed to solve numerically ordinary differential
equations of the form (4.19). More general and advanced methods to solve ordinary
differential equations and a more systematic description of these methods will be
offered in the next chapter.
</p>
<p>However, let us discuss another point before proceeding to the chapter on
the numerics of ordinary differential equations. As demonstrated in Sect. 1.3 the</p>
<p/>
</div>
<div class="page"><p/>
<p>4.2 Numerical Treatment 59
</p>
<p>approximation of the integral (4.4) involves a numerical error. What will be the con-
sequence of this error? Since we demonstrated that the approximationswe discussed
result in a differential equation in finite difference form, i.e. Eqs. (4.18), (4.23),
and (4.27), we know that the derivative P� will exhibit an error. Consequently, energy
conservation [see Appendix A, Eq. (A.27)] will be violated with the implication that
deviations from the trajectory (4.12) can be expected. This is definitely not desirable.
</p>
<p>A solution is provided by a special class of methods, known as symplectic
integrators, which were specifically designed for such cases. They are based on
a formulation of the problem using HAMILTON&rsquo;s equations of motion. (See, for
instance, Refs. [1&ndash;5].) In the particular case of the KEPLER problem the HAMILTON
function is equivalent to the total energy of the system and reads (in some scaled
units):
</p>
<p>H.p; q/ D 1
2
</p>
<p>�
</p>
<p>p21 C p22
�
</p>
<p>� 1q
q21 C q22
</p>
<p>: (4.30)
</p>
<p>Here p D .p1; p2/ are the generalized momentum coordinates of the point particle
in the two-dimensional plane and .q1; q2/ are the generalized position coordinates.
From this HAMILTON&rsquo;s equations of motion
</p>
<p>�
</p>
<p>Pq
Pp
</p>
<p>�
</p>
<p>D
�
</p>
<p>rpH.p; q/
�rqH.p; q/
</p>
<p>�
</p>
<p>D
�
</p>
<p>a.q; p/
</p>
<p>b.q; p/
</p>
<p>�
</p>
<p>; (4.31)
</p>
<p>follow, where the functions a.q; p/ and b.q; p/ have been introduced for a more
convenient notation. Note that these functions are two dimensional vectors in the
case of KEPLER&rsquo;s problem. The so called symplectic EULER method is given by
</p>
<p>qnC1 D qn C a.qn; pnC1/
t ;
pnC1 D pn C b.qn; pnC1/
t : (4.32)
</p>
<p>Obviously, the first equation is explicit while the second is implicit. An alternative
formulation reads
</p>
<p>qnC1 D qn C a.qnC1; pn/
t ;
pnC1 D pn C b.qnC1; pn/
t ; (4.33)
</p>
<p>where the first equation is implicit and the second equation is explicit. Of course,
Eq. (4.31) may be solved with the help of the explicit EULER method (4.20), the
implicit EULER method (4.24) or the implicit midpoint rule (4.29). The solution
should be equivalent to solving Eq. (4.4) with the respective method and then
calculating (4.1) successively. Again, a more systematic discussion of symplectic
integrators can be found in the following chapters.</p>
<p/>
</div>
<div class="page"><p/>
<p>60 4 The KEPLER Problem
</p>
<p>Let us conclude this chapter with a final remark. We decided to solve Eqs. (4.4)
and (4.1) because we wanted to reproduce the dynamics of the system, i.e. we
wanted to obtain �.t/ and '.t/. This directed us to the numerical solution of
two integrals. If we wanted to employ symplectic methods, which provide several
advantages, we would have to solve four differential equations (4.31) instead of
two integrals. Moreover, if we are not interested in the time evolution of the
system but in the form of the trajectory in general, we could simply evaluate the
integral (4.5) analytically or, if an analytical solution is not feasible for the potential
U.�/ one is interested in, numerically. Methods to approximate such an integral
were extensively discussed in Chap. 3.
</p>
<p>Summary
</p>
<p>KEPLER&rsquo;s two-body problem was used as an incentive to introduce intuitively
numerical methods to solve ordinary first order differential equations. To serve this
purpose the basic differential equations were transformed into integral form. These
integrals were then solved with the help of the rules discussed in Sect. 3.2. Three
basic methods have been identified, namely the explicit EULER method (based
on the forward difference derivative), the implicit EULER method (based on the
backward difference derivative), and the explicit midpoint rule (based on the central
rectangular rule). Shortcomings of such methods have been discussed briefly as
were remedies to overcome these.
</p>
<p>Problems
</p>
<p>1. Planetary-Orbits: Apply the methods of numerical integration to the integral (4.4)
and compare it to the analytical result. Identify the three different cases of elliptic,
parabolic or hyperbolic orbits by varying the initial conditions.
</p>
<p>2. LENNARD-JONES Scattering: Consider the scattering of two point particles
which interact via the LENNARD-JONES potential U.r/ D 4�Œ."=r/12 � ."=r/6&#141;
with �; " &gt; 0 (see Chapter 7). Calculate the orbit '.�/.
</p>
<p>3. Harmonic-Motion: Consider the motion of a point particle in the radial har-
monic oscillator U.�/ D m!2�2=2. According to BERTRAND&rsquo;s theorem (see
Refs. [1&ndash;3]) the particle&rsquo;s trajectories should be closed orbits. Demonstrate this
numerically as well as analytically.</p>
<p/>
</div>
<div class="page"><p/>
<p>References 61
</p>
<p>References
</p>
<p>1. Arnol&rsquo;d, V.I.: Mathematical Methods of Classical Mechanics, 2nd edn. Graduate Texts in
Mathematics, vol. 60. Springer, Berlin/Heidelberg (1989)
</p>
<p>2. Fetter, A.L., Walecka, J.D.: Theoretical Mechanics of Particles and Continua. Dover, New York
(2004)
</p>
<p>3. Scheck, F.: Mechanics, 5th edn. Springer, Berlin/Heidelberg (2010)
4. Goldstein, H., Poole, C., Safko, J.: Classical Mechanics, 3rd edn. Addison-Wesley, Menlo Park
</p>
<p>(2013)
5. Flie&szlig;bach, T.: Mechanik, 7th edn. Lehrbuch zur Theoretischen Physik I. Springer,
</p>
<p>Berlin/Heidelberg (2015)
6. &Oacute;&rsquo;Math&uacute;na, D.: Integrable Systems in Celestial Mechanics. Progress in Mathematical Physics,
</p>
<p>vol. 51. Birkh&auml;user Basel, Basel (2008)
7. Clark, A.: Elements of Abstract Algebra. Dover, New York (1971)</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 5
Ordinary Differential Equations: Initial Value
Problems
</p>
<p>5.1 Introduction
</p>
<p>This chapter introduces common numeric methods designed to solve initial value
problems. The discussion of the KEPLER problem in the previous chapter allowed
the introduction of three concepts, namely the implicit EULER method, the explicit
EULER method, and the implicit midpoint rule. Furthermore, we mentioned the
symplectic EULER method. In this chapter we plan to put these methods into a more
general context and to discuss more advanced techniques.
</p>
<p>Let us define the problem: We consider initial value problems of the form
</p>
<p>(
</p>
<p>Py.t/ D f .y; t/ ;
y.0/ D y0 ;
</p>
<p>(5.1)
</p>
<p>where y.t/ � y is an n-dimensional vector and y0 is referred to as the initial value
of y. Some remarks about the form of Eq. (5.1) are required:
</p>
<p>(i) We note that by posing Eq. (5.1), we assume that the differential equation is
explicit in Py, i.e. initial value problems of the form
</p>
<p>(
</p>
<p>G.Py/ D f .y; t/ ;
y.0/ D y0 ;
</p>
<p>(5.2)
</p>
<p>are only considered if G.Py/ is analytically invertible. For instance, we will not
deal with differential equations of the form
</p>
<p>Py C log .Py/ D 1 : (5.3)
</p>
<p>&copy; Springer International Publishing Switzerland 2016
B.A. Stickler, E. Schachinger, Basic Concepts in Computational Physics,
DOI 10.1007/978-3-319-27265-8_5
</p>
<p>63</p>
<p/>
</div>
<div class="page"><p/>
<p>64 5 Ordinary Differential Equations: Initial Value Problems
</p>
<p>(ii) We note that Eq. (5.1) is a first order differential equation in y. However, this
is in fact not a restriction since we can transform every explicit differential
equation of order n into a coupled set of explicit first order differential
equations. Let us demonstrate this. We regard an explicit differential equation
of the form
</p>
<p>y.n/ D f .tI y; Py; Ry; : : : ; y.n�1// ; (5.4)
</p>
<p>where we defined y.k/ � dkdtk y. This equation is equivalent to the set
</p>
<p>Py1 D y2 ;
Py2 D y3 ;
:::
</p>
<p>:::
</p>
<p>Pyn�1 D yn ;
Pyn D f .t; y1; y2; : : : ; yn/ ; (5.5)
</p>
<p>which can be written as Eq. (5.1). Hence, we can attenuate the criterion
discussed in point (i), that the differential equation has to be explicit in Py, to
the criterion that the differential equation of order n has to be explicit in the
n-th derivative of y, namely y.n/.
</p>
<p>There is another point required to be discussed before moving on. The numerical
treatment of initial value problems is of eminent importance in physics because
many differential equations, which appear unspectacular at first glance, cannot be
solved analytically. For instance, consider a first order differential equation of the
type:
</p>
<p>Py D t2 C y2 : (5.6)
</p>
<p>Although this equation appears to be simple, one has to rely on numerical methods
to obtain a solution. However, Eq. (5.6) is not well posed since the solution is
ambiguous as long as no initial values are given. A numerical solution is only
possible if the problem is completely defined. In many cases, one uses numerical
methods although the problem is solvable with the help of analytic methods, simply
because the solution would be too complicated. A numerical approach might be
justified, however, one should always remember that, quote [1]:
</p>
<p>Numerical methods are no excuse for poor analysis.
</p>
<p>This chapter will be augmented by a chapter on the double pendulum, which
will serve as a demonstration of the applicability of RUNGE-KUTTA methods and
by a chapter on molecular dynamics which will demonstrate the applicability of the
leap-frog algorithm.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.2 Simple Integrators 65
</p>
<p>5.2 Simple Integrators
</p>
<p>We start by reintroducing the methods already discussed in the previous chapter.
Again, we discretize the time coordinate t via the relation tn D t0 C n
t and
define fn � f .tn/ accordingly. In the following we will refrain from noting the initial
condition explicitly for a more compact notation. We investigate Eq. (5.1) at some
particular time tn:
</p>
<p>Pyn D f .yn; tn/ : (5.7)
</p>
<p>Integrating both sides of (5.7) over the interval Œtn; tnC1&#141; gives
</p>
<p>ynC1 D yn C
Z tnC1
</p>
<p>tn
</p>
<p>dt0f Œy.t0/; t0&#141; : (5.8)
</p>
<p>Note that Eq. (5.8) is exact and it will be our starting point in the discussion of
several paths to a numeric solution of initial value problems. These solutions will
be based on an approximation of the integral on the right hand side of Eq. (5.8) with
the help of the methods already discussed in Chap. 3.
</p>
<p>In the following we list four of the best known simple integration methods for
initial value problems:
</p>
<p>(1) Applying the forward rectangular rule (3.9) to Eq. (5.8) yields
</p>
<p>ynC1 D yn C f .yn; tn/
t C O.
t2/ ; (5.9)
</p>
<p>which is the explicit EULER method we encountered already in Sect. 4.2. This
method is also referred to as the forward EULER method. In accordance to
the forward rectangular rule, the leading term of the error of this method is
proportional to 
t2 as was pointed out in Sect. 3.2.
</p>
<p>(2) We use the backward rectangular rule (3.10) in Eq. (5.8) and obtain
</p>
<p>ynC1 D yn C f .ynC1; tnC1/
t C O.
t2/ ; (5.10)
</p>
<p>which is the implicit EULER method, also referred to as backward EULER
method. As already highlighted in Sect. 4.2, it may be necessary to solve
Eq. (5.10) numerically for ynC1. (Some notes on the numeric solution of non-
linear equations can be found in Appendix B.)
</p>
<p>(3) The central rectangular rule (3.13) approximates Eq. (5.8) by
</p>
<p>ynC1 D yn C f .ynC 12 ; tnC 12 /
t C O.
t
3/ ; (5.11)
</p>
<p>and we rewrite this equation in the form:
</p>
<p>ynC1 D yn�1 C 2f .yn; tn/
t C O.
t3/ : (5.12)</p>
<p/>
</div>
<div class="page"><p/>
<p>66 5 Ordinary Differential Equations: Initial Value Problems
</p>
<p>This method is sometimes referred to as the leap-frog routine or ST&Ouml;RMER-
VERLET method. We will come back to this point in Chap. 7. Note that the
approximation
</p>
<p>ynC 12 �
yn C ynC1
</p>
<p>2
; (5.13)
</p>
<p>in Eq. (5.11) gives the implicit midpoint rule as it was introduced in Sect. 4.2.
(4) Employing the trapezoidal rule (3.15) in an approximation to Eq. (5.8) yields
</p>
<p>ynC1 D yn C

t
</p>
<p>2
Œ f .yn; tn/C f .ynC1; tnC1/&#141;C O.
t3/ : (5.14)
</p>
<p>This is an implicit method which has to be solved for ynC1. It is generally known
as the CRANK-NICOLSON method [2] or simply as trapezoidal method.
</p>
<p>Methods (1), (2), and (4) are also known as one-step methods, since only function
values at times tn and tnC1 are used to propagate in time. In contrast, the leap-
frog method is already a multi-step method since three different times appear in
the expression. Basically, there are three different strategies to improve these rather
simple methods:
</p>
<p>&bull; TAYLOR series methods: Use more terms in the TAYLOR expansion of ynC1.
&bull; Linear Multi-Step methods: Use data from previous time steps yk, k &lt; n in order
</p>
<p>to cancel terms in the truncation error.
&bull; RUNGE-KUTTA method: Use intermediate points within one time step.
</p>
<p>We will briefly discuss the first two alternatives and then turn our attention to the
RUNGE-KUTTA methods in the next section.
</p>
<p>TAYLOR Series Methods
</p>
<p>From Chap. 2 we are already familiar with the TAYLOR expansion (2.7) of the
function ynC1 around the point yn,
</p>
<p>ynC1 D yn C
t Pyn C

t2
</p>
<p>2
Ryn C O.
t3/ : (5.15)
</p>
<p>We insert Eq. (5.7) into Eq. (5.15) and obtain
</p>
<p>ynC1 D yn C
t f .yn; tn/C

t2
</p>
<p>2
Ryn C O.
t3/ : (5.16)</p>
<p/>
</div>
<div class="page"><p/>
<p>5.2 Simple Integrators 67
</p>
<p>So far nothing has been gained since the truncation error is still proportional to 
t2.
However, calculating Ryn with the help of Eq. (5.7) gives
</p>
<p>Ryn D
d
</p>
<p>dt
f .yn; tn/ D Pf .yn; tn/C f 0.yn; tn/Pyn D Pf .yn; tn/C f 0.yn; tn/f .yn; tn/ ; (5.17)
</p>
<p>and this results together with Eq. (5.16) in:
</p>
<p>ynC1 D ynC
t f .yn; tn/C

t2
</p>
<p>2
</p>
<p>�Pf .yn; tn/C f 0.yn; tn/f .yn; tn/
�
</p>
<p>CO.
t3/ : (5.18)
</p>
<p>This manipulation reduced the local truncation error to orders of 
t3. The deriva-
tives of f .yn; tn/, f 0.yn; tn/ and Pf .yn; tn/ can be approximated with the help of the
methods discussed in Chap. 2, if an analytic differentiation is not feasible. The above
procedure can be repeated up to arbitrary order in the TAYLOR expansion (5.15).
</p>
<p>Linear Multi-step Methods
</p>
<p>A k-th order linear multi-step method is defined by the approximation
</p>
<p>ynC1 D
k
X
</p>
<p>jD0
ajyn�j C
t
</p>
<p>kC1
X
</p>
<p>jD0
bj f .ynC1�j; tnC1�j/ ; (5.19)
</p>
<p>of Eq. (5.8). The coefficients aj and bj have to be determined in such a way that the
local truncation error is reduced. Two of the best known techniques are the so called
second order ADAMS-BASHFORD method
</p>
<p>ynC1 D yn C

t
</p>
<p>2
Œ3f .yn; tn/ � f .yn�1; tn�1/&#141; ; (5.20)
</p>
<p>and the second order rule (backward differentiation formula)
</p>
<p>ynC1 D
1
</p>
<p>3
</p>
<p>�
</p>
<p>4yn � yn�1 C

t
</p>
<p>2
f .ynC1; tnC1/
</p>
<p>�
</p>
<p>: (5.21)
</p>
<p>(For details please consult Refs. [3&ndash;5].)
We note in passing that the backward differentiation formula of arbitrary order
</p>
<p>can easily be obtainedwith the help of the operator technique introduced in Sect. 2.4,
Eq. (2.27). One simply inserts the backward difference series (2.27) to arbitrary
order into the right hand side of the differential equation (5.7).</p>
<p/>
</div>
<div class="page"><p/>
<p>68 5 Ordinary Differential Equations: Initial Value Problems
</p>
<p>In many cases, multi-step methods are based on the interpolation of previously
computed values yk by LAGRANGE polynomials. This interpolation is then inserted
into Eq. (5.8) and integrated. However, a detailed discussion of such procedures is
beyond the scope of this book. The interested reader is referred to Refs. [6, 7].
</p>
<p>Nevertheless, let us make one last point. We note that Eq. (5.19) is explicit for
b0 D 0 and implicit for b0 &curren; 0. In many numerical realizations one combines
implicit and explicit multi-step methods in such a way that the explicit result [solve
Eq. (5.19) with b0 D 0] is used as a guess to solve the implicit equation [solve
Eq. (5.19) with b0 &curren; 0]. Hence, the explicit method predicts the value ynC1 and the
implicit method corrects it. Such methods yield very good results and are commonly
referred to as predictor&ndash;corrector methods [8].
</p>
<p>5.3 RUNGE-KUTTA Methods
</p>
<p>In contrast to linear multi-step methods, the idea in RUNGE-KUTTA methods (see,
for instance, Ref. [6]) is to improve the accuracy by calculating intermediate grid-
points within the interval Œtn; tnC1&#141;. We note that the approximation (5.11) resulting
from the central rectangular rule is already such a method since the function value
ynC1=2 at the grid-point tnC1=2 D tnC
t=2 is taken into account. We investigate this
in more detail and rewrite Eq. (5.11):
</p>
<p>ynC1 D yn C f .ynC 12 ; tnC 12 /
t C O.
t
3/ : (5.22)
</p>
<p>We now have to find appropriate approximations to ynC1=2 which will increase
the accuracy of Eq. (5.11). Our first choice is to replace ynC1=2 with the help of the
explicit EULER method, Eq. (5.9),
</p>
<p>ynC 12 D yn C

t
</p>
<p>2
Pyn D yn C
</p>
<p>
t
</p>
<p>2
f .yn; tn/ ; (5.23)
</p>
<p>which, inserted into Eq. (5.22) yields
</p>
<p>ynC1 D yn C f
�
</p>
<p>yn C

t
</p>
<p>2
f .yn; tn/; tn C
</p>
<p>
t
</p>
<p>2
</p>
<p>�
</p>
<p>
t C O.
t2/ : (5.24)
</p>
<p>We note that Eq. (5.24) is referred to as the explicit midpoint rule. In analogy
we could have approximated ynC1=2 with the help of the averaged function value
�ynC1=2 which results in
</p>
<p>ynC1 D yn C f
�
</p>
<p>yn C ynC1
2
</p>
<p>; tn C

t
</p>
<p>2
</p>
<p>�
</p>
<p>
t C O.
t2/ : (5.25)</p>
<p/>
</div>
<div class="page"><p/>
<p>5.3 RUNGE-KUTTA Methods 69
</p>
<p>This equation is referred to as the implicit midpoint rule. Let us explain how we
obtain an estimate for the error in Eqs. (5.24) and (5.25). In case of Eq. (5.24) we
investigate the term
</p>
<p>ynC1 � yn � f
�
</p>
<p>yn C

t
</p>
<p>2
f .yn; tn/; tn C
</p>
<p>
t
</p>
<p>2
</p>
<p>�
</p>
<p>
t :
</p>
<p>The TAYLOR expansion of ynC1 and f .�/ around the point
t D 0 yields
</p>
<p>
t ŒPyn � f .yn; tn/&#141;C

t2
</p>
<p>2
</p>
<p>�
</p>
<p>Ry � Pf .yn; tn/ � f 0.yn; tn/Pyn
�
</p>
<p>C : : : : (5.26)
</p>
<p>We observe that the first term cancels because of Eq. (5.7). Consequently, the error
is of order
t2. A similar argument holds for Eq. (5.25).
</p>
<p>Let us introduce a more convenient notation for the above examples before we
concentrate on a more general topic. It is presented in algorithmic form, i.e. it defines
the sequence in which one should calculate the various terms. This is convenient for
two reasons, first of all it increases the readability of complex methods such as
Eq. (5.25) and, secondly, it is easy to identify which part of the method involves
an implicit step and which part has to be solved separately for the corresponding
variable. For this purpose let us introduce variables Yi of some index i � 1 and
we use a simple example to illustrate this notation. Consider the explicit EULER
method (5.9). It can be written as
</p>
<p>Y1 D yn ;
ynC1 D yn C f .Y1; tn/
t : (5.27)
</p>
<p>In a similar fashion we write the implicit EULER method (5.10) as
</p>
<p>Y1 D yn C f .Y1; tnC1/
t ;
ynC1 D yn C f .Y1; tnC1/
t : (5.28)
</p>
<p>It is understood that the first equation of (5.28) has to be solved for Y1 first and this
result is then plugged into the second equation in order to obtain ynC1. One further
example: the CRANK-NICOLSON (5.14) method can be rewritten as
</p>
<p>Y1 D yn ;
</p>
<p>Y2 D yn C

t
</p>
<p>2
Œ f .Y1; tn/C f .Y2; tnC1/&#141; ;
</p>
<p>ynC1 D yn C

t
</p>
<p>2
Œ f .Y1; tn/C f .Y2; tnC1/&#141; ; (5.29)
</p>
<p>where the second equation is to be solved for Y2 in the second step.</p>
<p/>
</div>
<div class="page"><p/>
<p>70 5 Ordinary Differential Equations: Initial Value Problems
</p>
<p>In analogy, the algorithmic form of the explicit midpoint rule (5.24) is defined as
</p>
<p>Y1 D yn ;
</p>
<p>Y2 D yn C

t
</p>
<p>2
f
</p>
<p>�
</p>
<p>Y1; tn C

t
</p>
<p>2
</p>
<p>�
</p>
<p>;
</p>
<p>ynC1 D yn C

t
</p>
<p>2
f
</p>
<p>�
</p>
<p>Y2; tn C

t
</p>
<p>2
</p>
<p>�
</p>
<p>; (5.30)
</p>
<p>and we find for the implicit midpoint rule (5.25):
</p>
<p>Y1 D yn C

t
</p>
<p>2
f
</p>
<p>�
</p>
<p>Y1; tn C

t
</p>
<p>2
</p>
<p>�
</p>
<p>;
</p>
<p>ynC1 D yn C
t f
�
</p>
<p>Y1; tn C

t
</p>
<p>2
</p>
<p>�
</p>
<p>: (5.31)
</p>
<p>The above algorithms are all examples of the so called RUNGE-KUTTA methods.
We introduce the general representation of a d-stage RUNGE-KUTTA method:
</p>
<p>Yi D yn C
t
d
X
</p>
<p>jD1
aij f
</p>
<p>�
</p>
<p>Yj; tn C cj
t
�
</p>
<p>; i D 1; : : : ; d ;
</p>
<p>ynC1 D yn C
t
d
X
</p>
<p>jD1
bj f
</p>
<p>�
</p>
<p>Yj; tn C cj
t
�
</p>
<p>: (5.32)
</p>
<p>We note that Eq. (5.32) it is completely determined by the coefficients aij, bj and
cj. In particular a D faijg is a d � d matrix, while b D fbjg and c D fcjg are d
dimensional vectors.
</p>
<p>BUTCHER tableaus are a very useful tool to characterize such methods. They
provide a structured representation of the coefficient matrix a and the coefficient
vectors b and c:
</p>
<p>c1 a11 a12 : : : a1d
c2 a21 a22 : : : a2d
:::
</p>
<p>:::
:::
: : :
</p>
<p>:::
</p>
<p>cd ad1 ad2 : : : add
</p>
<p>b1 b2 : : : bd
</p>
<p>(5.33)</p>
<p/>
</div>
<div class="page"><p/>
<p>5.3 RUNGE-KUTTA Methods 71
</p>
<p>We note that the RUNGE-KUTTA method (5.32) or (5.33) is explicit if the matrix a is
zero on and above the diagonal, i.e. aij D 0 for j � i. Let us rewrite all the methods
described here in the form of BUTCHER tableaus:
</p>
<p>Explicit EULER:
</p>
<p>0 0
</p>
<p>1
(5.34)
</p>
<p>Implicit EULER:
</p>
<p>1 1
</p>
<p>1
(5.35)
</p>
<p>CRANK-NICOLSON:
</p>
<p>0 0 0
</p>
<p>1 1
2
1
2
</p>
<p>1
2
1
2
</p>
<p>(5.36)
</p>
<p>Explicit Midpoint:
</p>
<p>0 0 0
1
2
1
2
0
</p>
<p>1
2
1
2
</p>
<p>(5.37)
</p>
<p>Implicit Midpoint:
</p>
<p>1
2
1
2
</p>
<p>1
(5.38)</p>
<p/>
</div>
<div class="page"><p/>
<p>72 5 Ordinary Differential Equations: Initial Value Problems
</p>
<p>With the help of RUNGE-KUTTA methods of the general form (5.32) one can
develop methods of arbitrary accuracy. One of the most popular methods is the
explicit four stage method (we will call it e-RK-4) which is defined by the algorithm:
</p>
<p>Y1 D yn;
</p>
<p>Y2 D yn C

t
</p>
<p>2
f .Y1; tn/ ;
</p>
<p>Y3 D yn C

t
</p>
<p>2
f
</p>
<p>�
</p>
<p>Y2; tn C

t
</p>
<p>2
</p>
<p>�
</p>
<p>;
</p>
<p>Y4 D yn C
t f
�
</p>
<p>Y3; tn C

t
</p>
<p>2
</p>
<p>�
</p>
<p>;
</p>
<p>ynC1 D yn C

t
</p>
<p>6
</p>
<p>�
</p>
<p>f .Y1; tn/C 2f
�
</p>
<p>Y2; tn C

t
</p>
<p>2
</p>
<p>�
</p>
<p>C2f
�
</p>
<p>Y3; tn C

t
</p>
<p>2
</p>
<p>�
</p>
<p>C f .Y4; tn/
�
</p>
<p>: (5.39)
</p>
<p>This method is an analogue to the SIMPSON rule of numerical integration as
discussed in Sect. 3.4. However, a detailed compilation of the coefficient array a
and coefficient vectors b, and c is quite complicated. A closer inspection reveals
that the methodological error of this method behaves as
t5. The algorithm e-RK-4,
Eq. (5.39), is represented by a BUTCHER tableau of the form
</p>
<p>0 0 0 0 0
1
2
1
2
0 0 0
</p>
<p>1
2
0 1
2
0 0
</p>
<p>1 0 0 1 0
1
6
1
3
1
3
1
6
</p>
<p>(5.40)
</p>
<p>Another quite popular method is given by the BUTCHER tableau
</p>
<p>1
2
�
</p>
<p>p
3
6
</p>
<p>1
4
</p>
<p>1
4
�
</p>
<p>p
3
6
</p>
<p>1
2
C
</p>
<p>p
3
6
</p>
<p>1
4
C
</p>
<p>p
3
6
</p>
<p>1
4
</p>
<p>1
2
</p>
<p>1
2
</p>
<p>(5.41)
</p>
<p>We note that this method is implicit and mention that it corresponds to the two point
GAUSS-LEGENDRE quadrature of Sect. 3.6.
</p>
<p>A further improvement of implicit RUNGE-KUTTA methods can be achieved by
choosing the Yi in such a way that they correspond to solutions of the differential
equation (5.7) at intermediate time steps. The intermediate time steps at which
one wants to reproduce the function are referred to as collocation points. At these
points the functions are approximated by interpolation on the basis of LAGRANGE</p>
<p/>
</div>
<div class="page"><p/>
<p>5.4 Hamiltonian Systems: Symplectic Integrators 73
</p>
<p>polynomials, which can easily be integrated analytically. However, the discussion
of such collocation methods [8] is far beyond the scope of this book.
</p>
<p>In general RUNGE-KUTTA methods are very useful. However one always has to
keep in mind that there could be better methods for the problem at hand. Let us close
this section with a quote from the book by PRESS et al. [9]:
</p>
<p>For many scientific users, fourth-order Runge-Kutta is not just the first word on ODE
integrators, but the last word as well. In fact, you can get pretty far on this old workhorse,
especially if you combine it with an adaptive step-size algorithm. Keep in mind, however,
that the old workhorse&rsquo;s last trip may well take you to the poorhouse: Bulirsch-Stoer or
predictor-corrector methods can be very much more efficient for problems where high
accuracy is a requirement. Those methods are the high-strung racehorses. Runge-Kutta is
for ploughing the fields.
</p>
<p>5.4 Hamiltonian Systems: Symplectic Integrators
</p>
<p>Let us define a symplectic integrator as a numerical integration in which the
mapping
</p>
<p>˚
t W yn 7! ynC1 ; (5.42)
</p>
<p>is symplectic. Here ˚
t is referred to as the numerical flow of the method. If we
regard the initial value problem (5.1) we can define in an analogous way the flow of
the system 't as
</p>
<p>'t.y0/ D y.t/ : (5.43)
</p>
<p>For instance, if we consider the initial value problem
</p>
<p>(
</p>
<p>Py D Ay ;
y.0/ D y0 ;
</p>
<p>(5.44)
</p>
<p>where y 2 Rn and A 2 Rn�n, then the flow 't of the system is given by:
</p>
<p>'t.y0/ D exp.At/y0 : (5.45)
</p>
<p>On the other hand, if we regard two vectors v;w 2 R2, we can express the area
! of the parallelogram spanned by these vectors as
</p>
<p>!.v;w/ D det.vw/ D v
�
</p>
<p>0 1
</p>
<p>�1 0
</p>
<p>�
</p>
<p>w D ad � bc ; (5.46)</p>
<p/>
</div>
<div class="page"><p/>
<p>74 5 Ordinary Differential Equations: Initial Value Problems
</p>
<p>where we put v D .a; b/T and w D .c; d/T . More generally, if v;w 2 R2d, we have
</p>
<p>!.v;w/ D v
�
</p>
<p>0 I
</p>
<p>�I 0
</p>
<p>�
</p>
<p>w � vJw ; (5.47)
</p>
<p>where I is the d � d dimensional unity matrix. Hence (5.47) represents the sum of
the projected areas of the form
</p>
<p>det
</p>
<p>�
</p>
<p>vi wi
viCd wiCd
</p>
<p>�
</p>
<p>: (5.48)
</p>
<p>If we regard a mapping M W R2d 7! R2d and require that
</p>
<p>!.Mv;Mw/ D !.v;w/ ; (5.49)
</p>
<p>i.e. the area is preserved, we obtain the condition that
</p>
<p>MTJM D J ; (5.50)
</p>
<p>which is equivalent to det.M/ D 1. Finally, a differentiable mapping f W R2d 7! R2d
is referred to as symplectic if the linear mapping f 0.x/ (JACOBI matrix) conserves
! for all x 2 R2d. One can easily prove that the flow of Hamiltonian systems
is symplectic, i.e. area preserving in phase space. Every Hamiltonian system is
characterized by its HAMILTON function H.p; q/ and the corresponding HAMILTON
equations of motion [10&ndash;14]:
</p>
<p>Pp D �rqH.p; q/ and Pq D rpH.p; q/ : (5.51)
</p>
<p>We define the flow of the system via
</p>
<p>'t.x0/ D x.t/ ; (5.52)
</p>
<p>where
</p>
<p>x0 D
�
</p>
<p>p0
</p>
<p>q0
</p>
<p>�
</p>
<p>and x.t/ D
�
</p>
<p>p.t/
</p>
<p>q.t/
</p>
<p>�
</p>
<p>: (5.53)
</p>
<p>Hence we rewrite (5.51) as
</p>
<p>Px D J�1rxH.x/ ; (5.54)
</p>
<p>and note that x � x.t; x0/ is a function of time and initial conditions. In a next step
we define the Jacobian of the flow via
</p>
<p>Pt.x0/ D rx0't.x0/ ; (5.55)</p>
<p/>
</div>
<div class="page"><p/>
<p>5.4 Hamiltonian Systems: Symplectic Integrators 75
</p>
<p>and calculate
</p>
<p>PPt.x0/ D rx0 Px
D J�1rx0rxH.x/
D J�1
xH.x/rx0x
D J�1
xH.x/Pt.x0/
</p>
<p>D
�
</p>
<p>�rqpH.p; q/ �rqqH.p; q/
rppH.p; q/ rpqH.p; q/
</p>
<p>�
</p>
<p>Pt.x0/ : (5.56)
</p>
<p>Hence, Pt is given by the solution of the equation
</p>
<p>PPt D J�1
xH.x/Pt : (5.57)
</p>
<p>Symplecticity ensures that the area
</p>
<p>PTt JPt D const ; (5.58)
</p>
<p>which can be verified by calculating ddt
�
</p>
<p>PTt JPt
�
</p>
<p>where we keep in mind that JT D
�J. Hence,
</p>
<p>d
</p>
<p>dt
PTt JPt D PPTt JPt C PTt J PPt
</p>
<p>D PTt 
xH.x/.J�1/TJPt C PTt JJ�1
xH.x/Pt
D 0 ; (5.59)
</p>
<p>even if the HAMILTON function is not conserved. This means that the flow of a
Hamiltonian system is symplectic, i.e. area preserving in phase space [10, 11, 13].
</p>
<p>Since this conservation law is violated bymethods like e-RK-4 or explicit EULER,
one introduces so called symplectic integrators, which have been particularly
designed as a remedy to this shortcoming. A detailed investigation of these
techniques is far too engaged for this book. The interested reader is referred to Refs.
[12, 15&ndash;17].
</p>
<p>However, we provide a list of the most important integrators.
</p>
<p>Symplectic EULER
</p>
<p>qnC1 D qn C a.qn; pnC1/
t ; (5.60a)
pnC1 D pn C b.qn; pnC1/
t : (5.60b)</p>
<p/>
</div>
<div class="page"><p/>
<p>76 5 Ordinary Differential Equations: Initial Value Problems
</p>
<p>Here a.p; q/ D rpH.p; q/ and b.p; q/ D �rqH.p; q/ have already been defined
in Sect. 4.2.
</p>
<p>Symplectic RUNGE-KUTTA
</p>
<p>It can be demonstrated that a RUNGE-KUTTA method is symplectic if the coeffi-
cients fulfill
</p>
<p>biaij C bjaji D bibj ; (5.61)
</p>
<p>for all i; j [16, 18]. This is a property of the collocation methods based on GAUSS
points ci.
</p>
<p>5.5 An Example: The KEPLER Problem, Revisited
</p>
<p>It has already been discussed in Sect. 4.2 that the HAMILTON function of this system
takes on the form [19]
</p>
<p>H.p; q/ D 1
2
</p>
<p>�
</p>
<p>p21 C p22
�
</p>
<p>� 1q
q21 C q22
</p>
<p>; (5.62)
</p>
<p>and HAMILTON&rsquo;s equations of motion read
</p>
<p>Pp1 D �rq1H.p; q/ D �
q1
</p>
<p>.q21 C q22/
3
2
</p>
<p>; (5.63a)
</p>
<p>Pp2 D �rq2H.p; q/ D �
q2
</p>
<p>.q21 C q22/
3
2
</p>
<p>; (5.63b)
</p>
<p>Pq1 D rp1H.p; q/ D p1 ; (5.63c)
Pq2 D rp2H.p; q/ D p2 : (5.63d)
</p>
<p>We now introduce the time instances tn D t0 C n
t and define qni � qi.tn/ and
pni � pi.tn/ for i D 1; 2. In the following we give the discretized recursion relation
for three differentmethods, namely explicit EULER, implicit EULER, and symplectic
EULER.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.5 An Example: The KEPLER Problem, Revisited 77
</p>
<p>Explicit EULER
</p>
<p>In case of the explicit EULER method we have simple recursion relations
</p>
<p>pnC11 D pn1 �
qn1
t
</p>
<p>Œ.qn1/
2 C .qn2/2&#141;
</p>
<p>3
2
</p>
<p>; (5.64a)
</p>
<p>pnC12 D pn2 �
qn2
t
</p>
<p>Œ.qn1/
2 C .qn2/2&#141;
</p>
<p>3
2
</p>
<p>; (5.64b)
</p>
<p>qnC11 D qn1 C pn1
t ; (5.64c)
qnC12 D qn2 C pn2
t : (5.64d)
</p>
<p>Implicit EULER
</p>
<p>We obtain the implicit equations
</p>
<p>pnC11 D pn1 �
qnC11 
t
</p>
<p>Œ.qnC11 /
2 C .qnC12 /2&#141;
</p>
<p>3
2
</p>
<p>; (5.65a)
</p>
<p>pnC12 D pn2 �
qnC12 
t
</p>
<p>Œ.qnC11 /
2 C .qnC12 /2&#141;
</p>
<p>3
2
</p>
<p>; (5.65b)
</p>
<p>qnC11 D qn1 C pnC11 
t ; (5.65c)
qnC12 D qn2 C pnC12 
t : (5.65d)
</p>
<p>These implicit equations can be solved, for instance, by the use of the NEWTON
method discussed in Appendix B.
</p>
<p>Symplectic EULER
</p>
<p>Employing Eqs. (5.60) gives
</p>
<p>pnC11 D pn1 �
qn1
t
</p>
<p>Œ.qn1/
2 C .qn2/2&#141;
</p>
<p>3
2
</p>
<p>; (5.66a)
</p>
<p>pnC12 D pn2 �
qn2
t
</p>
<p>Œ.qn1/
2 C .qn2/2&#141;
</p>
<p>3
2
</p>
<p>; (5.66b)</p>
<p/>
</div>
<div class="page"><p/>
<p>78 5 Ordinary Differential Equations: Initial Value Problems
</p>
<p>qnC11 D qn1 C pnC11 
t ; (5.66c)
qnC12 D qn2 C pnC12 
t : (5.66d)
</p>
<p>These implicit equations can be solved analytically and we obtain
</p>
<p>pnC11 D pn1 �
qn1
t
</p>
<p>Œ.qn1/
2 C .qn2/2&#141;
</p>
<p>3
2
</p>
<p>; (5.67a)
</p>
<p>pnC12 D pn2 �
qn2
t
</p>
<p>Œ.qn1/
2 C .qn2/2&#141;
</p>
<p>3
2
</p>
<p>; (5.67b)
</p>
<p>qnC11 D qn1 C pn1
t �
qn1
t
</p>
<p>2
</p>
<p>Œ.qn1/
2 C .qn2/2&#141;
</p>
<p>3
2
</p>
<p>; (5.67c)
</p>
<p>qnC12 D qn2 C pn2
t �
qn2
t
</p>
<p>2
</p>
<p>Œ.qn1/
2 C .qn2/2&#141;
</p>
<p>3
2
</p>
<p>: (5.67d)
</p>
<p>A second possibility of the symplectic EULER is given by Eq. (4.33). It reads
</p>
<p>pnC11 D pn1 �
qnC11 
t
</p>
<p>Œ.qnC11 /
2 C .qnC12 /2&#141;
</p>
<p>3
2
</p>
<p>; (5.68a)
</p>
<p>pnC12 D pn2 �
qnC12 
t
</p>
<p>Œ.qnC11 /
2 C .qnC12 /2&#141;
</p>
<p>3
2
</p>
<p>; (5.68b)
</p>
<p>qnC11 D qn1 C pn1
t ; (5.68c)
qnC12 D qn2 C pn2
t : (5.68d)
</p>
<p>The trajectories calculated using these four methods are presented in Figs. 5.1
and 5.2, the time evolution of the total energy of the system is plotted in Fig. 5.3.
The initial conditions were [16]
</p>
<p>p1.0/ D 0; q1.0/ D 1 � e ; (5.69)
</p>
<p>and
</p>
<p>p2.0/ D
r
</p>
<p>1C e
1 � e ; q2.0/ D 0 ; (5.70)
</p>
<p>with e D 0:6 which gives H D �1=2. Furthermore, we set 
t D 0:01 for the
symplectic EULER methods and 
t D 0:005 for the forward and backward EULER
methods in order to reduce the methodological error. The implicit equations were
solved with help of the NEWTON method as discussed in Appendix B. The JACOBI</p>
<p/>
</div>
<div class="page"><p/>
<p>5.5 An Example: The KEPLER Problem, Revisited 79
</p>
<p>Fig. 5.1 KEPLER trajectories in position space for the initial values defined in Eqs. (5.69)
and (5.70). They are indicated by a solid square. Solutions have been generated (a) by the explicit
EULER method (5.64), (b) by the implicit EULER method (5.65), (c) by the symplectic EULER
method (5.67), and (d) by the symplectic EULER method (5.68)
</p>
<p>matrix was calculated analytically, hence no methodological error enters because
approximations of derivatives were unnecessary.
</p>
<p>According to theory [19] the q-space and p-space projections of the phase space
trajectory are ellipses. Furthermore, energy and angular momentum are conserved.
Thus, the numerical solutions of HAMILTON&rsquo;s equations of motion (5.63) should
reflect these properties. Figures 5.1a, b and 5.2a, b present the results of the
explicit EULER method, Eqs. (5.64), and the implicit EULER method, Eqs. (5.65),
respectively. Obviously, the result does not agree with the theoretical expectation
and the trajectories are open instead of closed. The reason for this behavior is
the methodological error of the method which is accumulative and, thus, causes
a violation of energy conservation. This violation becomes apparent in Fig. 5.3
where the total energy H.t/ is plotted vs time t. Neither the explicit EULER method
(dashed line) nor the implicit EULER method (short dashed line) conform to the
requirement of energy conservation. We also see step-like structures of H.t/. At the
center of these steps an open diamond symbol and in the case of the implicit EULER
method an additional open circle indicate the position in time of the perihelion
of the point-mass (point of closest approach to the center of attraction). It is</p>
<p/>
</div>
<div class="page"><p/>
<p>80 5 Ordinary Differential Equations: Initial Value Problems
</p>
<p>Fig. 5.2 KEPLER trajectories in momentum space for the initial values defined in Eqs. (5.69)
and (5.70). They are indicated by a solid square. Solutions have been generated (a) by the explicit
EULER method (5.64), (b) by the implicit EULER method (5.65), (c) by the symplectic EULER
method (5.67), and (d) by the symplectic EULER method (5.68)
</p>
<p>Fig. 5.3 Time evolution of the total energy H calculated with the help of the four methods
discussed in the text. The initial values are given by Eqs. (5.69) and (5.70). Solutions have been
generated (i) by the explicit EULER method (5.64) (dashed line), (ii) by the implicit EULER
method (5.65) (dotted line), (iii) by the symplectic EULER method (5.67) (solid line), and (iv)
by the symplectic EULER method (5.68) (dashed-dotted line)</p>
<p/>
</div>
<div class="page"><p/>
<p>5.5 An Example: The KEPLER Problem, Revisited 81
</p>
<p>indicated by the same symbols in Fig. 5.1a, b. At this point the point-mass reaches
its maximum velocity, the pericenter velocity, and it covers the biggest distances
along its trajectory per time interval 
t. Consequently, the methodological error is
biggest in this part of the trajectory which manifests itself in those steps in H.t/. As
the point-mass moves &lsquo;faster&rsquo; when the implicit EULER method is applied, again,
the distances covered per time interval are greater than those covered by the point-
mass in the explicit EULER method. Thus, it is not surprising that the error of the
implicit EULER method is bigger as well when H.t/ is determined.
</p>
<p>These results are in strong contrast to the numerical solutions of Eqs. (5.63)
obtained with the help of symplectic EULER methods which are presented in
Figs. 5.1c, d and 5.2c, d. The trajectories are almost perfect ellipses for both
symplectic methods which follow Eqs. (5.67) and (5.68). Moreover, the total energy
H.t/ (solid and dashed-dotted lines in Fig. 5.3) varies very little as a function of t.
Deviations from the mean value can only be observed around the perihelion which
is indicated by a solid square. Moreover, these deviations compensate because of the
symplectic nature of the method. This demonstrates that symplectic integrators are
the appropriate technique to solve the equations of motion of Hamiltonian systems.
</p>
<p>Summary
</p>
<p>We concentrated on numericalmethods to solve the initial value problem of ordinary
differential equations. The methods discussed here rely heavily on the various
methods developed for numerical integration because we can always find an integral
representation of this kind of equations. The simple integrators known from Chap. 4
were augmented by the more general CRANK-NICHOLSON method which was
based on the trapezoidal rule introduced in Sect. 3.3. The simple single-step methods
were improved in their methodological error by TAYLOR series methods, linear
multi-step methods, and by the RUNGE-KUTTAmethod. The latter took intermediate
points within the time interval Œtn; tnC1&#141; into account. In principle, it is possible
to achieve almost arbitrary accuracy with such a method. Nevertheless, all those
methods had the disadvantage that because of their methodological error energy
conservation was violated when applied to Hamiltonian systems. As this problem
can be remedied by symplectic integrators a short introduction into this topic was
provided and the most important symplectic integrators were presented. The final
discussion of KEPLER&rsquo;s two-body problem elucidated the various points discussed
throughout this chapter.</p>
<p/>
</div>
<div class="page"><p/>
<p>82 5 Ordinary Differential Equations: Initial Value Problems
</p>
<p>Problems
</p>
<p>1. Write a program to solve numerically the KEPLER problem. The HAMILTON
function of the problem is defined as
</p>
<p>H.p; q/ D 1
2
</p>
<p>�
</p>
<p>p21 C p22
�
</p>
<p>� 1q
q21 C q22
</p>
<p>;
</p>
<p>and the initial conditions are given by
</p>
<p>p1.0/ D 0; q1.0/ D 1� e; p2.0/ D
r
</p>
<p>1C e
1 � e ; q2.0/ D 0;
</p>
<p>where e D 0:6. Derive HAMILTON&rsquo;s equations of motion and implement an
algorithm which solves these equations based on the following methods
</p>
<p>(a) Explicit EULER,
(b) Symplectic EULER.
</p>
<p>2. Plot the trajectories and the total energy as a function of time. You can use the
results presented in Figs. 5.1 and 5.2 to check your code. Modify the initial
conditions and discuss the results! Try to confirm KEPLER&rsquo;s laws of planetary
motion with the help of your algorithm.
</p>
<p>3. Use a symplectic integrator to study LENNARD-JONES scattering; see Problems
of Chap. 4
</p>
<p>4. Solve the differential equation (5.6) numerically with different methods. Use also
the TAYLOR series method (5.18).
</p>
<p>References
</p>
<p>1. Dorn, W.S., McCracken, D.D.: Numerical Methods with Fortran IV Case Studies. Wiley,
New York (1972)
</p>
<p>2. Crank, J., Nicolson, P.: A practical method for numerical evaluation of solutions of partial
differential equations of the heat-conduction type. Proc. Camb. Philos. Soc. 43, 50&ndash;67 (1947).
doi:10.1017/S0305004100023197
</p>
<p>3. Hairer, E., Wanner, G.: Solving Ordinary Differential Equations II. Springer Series in
Computational Mathematics, vol. 14. Springer, Berlin/Heidelberg (1991)
</p>
<p>4. Hairer, E., N&oslash;rsett, S.P., Wanner, G.: Solving Ordinary Differential Equations I, 2nd edn.
Springer Series in Computational Mathematics, vol. 8. Springer, Berlin/Heidelberg (1993)
</p>
<p>5. S&uuml;li, E., Mayers, D.: An Introduction to Numerical Analysis. Cambridge University Press,
Cambridge (2003)
</p>
<p>6. Collatz, L.: The Numerical Treatment of Differential Equations. Springer, Berlin/Heidelberg
(1960)
</p>
<p>7. van Winckel, G.: Numerical methods for differential equations. Lecture Notes, Karl-Franzens
Universit&auml;t Graz (2012)</p>
<p/>
</div>
<div class="page"><p/>
<p>References 83
</p>
<p>8. Ascher, U.M., Petzold, L.R.: Computer Methods for Ordinary Differential Equations
and Differential-Algebraic Equations. Society for Industrial and Applied Mathematics,
Philadelphia (1998)
</p>
<p>9. Press, W.H., Teukolsky, S.A., Vetterling, W.T., Flannery, B.P.: Numerical Recipes in C++, 2nd
edn. Cambridge University Press, Cambridge (2002)
</p>
<p>10. Arnol&rsquo;d, V.I.: Mathematical Methods of Classical Mechanics, 2nd edn. Graduate Texts in
Mathematics, vol. 60. Springer, Berlin/Heidelberg (1989)
</p>
<p>11. Fetter, A.L., Walecka, J.D.: Theoretical Mechanics of Particles and Continua. Dover, New York
(2004)
</p>
<p>12. Scheck, F.: Mechanics, 5th edn. Springer, Berlin/Heidelberg (2010)
13. Goldstein, H., Poole, C., Safko, J.: Classical Mechanics, 3rd edn. Addison-Wesley, Menlo Park
</p>
<p>(2013)
14. Flie&szlig;bach, T.: Mechanik, 7th edn. Lehrbuch zur Theoretischen Physik I. Springer,
</p>
<p>Berlin/Heidelberg (2015)
15. Guillemin, V., Sternberg, S.: Symplectic Techniques in Physics. Cambridge University Press,
</p>
<p>Cambridge (1990)
16. Hairer, E.: Geometrical Integration &ndash; Symplectic Integrators. Lecture Notes, TU M&uuml;nchen
</p>
<p>(2010)
17. Levi, D., Oliver, P., Thomova, Z., Winteritz, P. (eds.): Symmetries and Integrability of Dif-
</p>
<p>ference Equations. London Mathematical Society Lecture Note Series. Cambridge University
Press, Cambridge (2011)
</p>
<p>18. Feng, K., Qin, M.: Symplectic Runge-Kutta methods. In: Feng, K., Qin, M. (eds.) Symplectic
Geometric Algorithms for Hamiltonian Systems, pp. 277&ndash;364. Springer, Berlin/Heidelberg
(2010)
</p>
<p>19. &Oacute;&rsquo;Math&uacute;na, D.: Integrable Systems in Celestial Mechanics. Progress in Mathematical Physics,
vol. 51. Birkh&auml;user Basel, Basel (2008)</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 6
The Double Pendulum
</p>
<p>6.1 HAMILTON&rsquo;s Equations
</p>
<p>We investigate the dynamics of the double pendulum in two spacial dimensions
as illustrated schematically in Fig. 6.1. It is the aim of this section to derive
HAMILTON&rsquo;s equations of motion for this system. In a first step we introduce
generalized coordinates and determine the LAGRANGE function of the system from
its kinetic and potential energy [1&ndash;5]. We then introduce generalized momenta
and, finally, derive the HAMILTON function from which HAMILTON&rsquo;s equations of
motion follow. They will serve as a starting point for the formulation of a numerical
method.
</p>
<p>From Fig. 6.1 we find the coordinates of the two point masses m:
</p>
<p>x1 D ` sin.'1/ ; z1 D 2` � ` cos.'1/ ; (6.1)
</p>
<p>and
</p>
<p>x2 D ` Œsin.'1/C sin.'2/&#141; ; z2 D 2` � ` Œcos.'1/C cos.'2/&#141; : (6.2)
</p>
<p>Here, 2` is the pendulum&rsquo;s total length. The angles 'i, i D 1; 2 are defined in
Fig. 6.1.
</p>
<p>We note that ` D const and obtain the time derivatives of the coordinates (6.1)
and (6.2):
</p>
<p>Px1 D ` P'1 cos.'1/ ; (6.3)
Pz1 D ` P'1 sin.'1/ ; (6.4)
Px2 D ` Œ P'1 cos.'1/C P'2 cos.'2/&#141; ; (6.5)
Pz2 D ` Œ P'1 sin.'1/C P'2 sin.'2/&#141; : (6.6)
</p>
<p>&copy; Springer International Publishing Switzerland 2016
B.A. Stickler, E. Schachinger, Basic Concepts in Computational Physics,
DOI 10.1007/978-3-319-27265-8_6
</p>
<p>85</p>
<p/>
</div>
<div class="page"><p/>
<p>86 6 The Double Pendulum
</p>
<p>Fig. 6.1 Schematic
illustration of the double
pendulum. m are the
point-masses, 2` is the total
length of the pendulum and
'1, '2 are the corresponding
angles
</p>
<p>The LAGRANGE function of the system is defined by
</p>
<p>L D T � U ; (6.7)
</p>
<p>with the kinetic energy T and the potential U. The kinetic energy T is given by1
</p>
<p>T D m
2
</p>
<p>�
</p>
<p>Px21 C Pz21 C Px22 C Pz22
�
</p>
<p>D m`
2
</p>
<p>2
</p>
<p>�
</p>
<p>2 P'21 C P'22 C 2 P'1 P'2 cos.'1 � '2/
�
</p>
<p>: (6.8)
</p>
<p>The potential energy U is determined by the gravitational force
</p>
<p>U D mgz1 C mgz2
D mg` Œ4 � 2 cos.'1/ � cos.'2/&#141; ; (6.9)
</p>
<p>where g is the acceleration due to gravity. Hence, we get for the LAGRANGE
function L:
</p>
<p>L D m`
2
</p>
<p>2
</p>
<p>�
</p>
<p>2 P'21 C P'22 C 2 P'1 P'2 cos.'1 � '2/
�
</p>
<p>� mg` Œ4 � 2 cos.'1/ � cos.'2/&#141; :
(6.10)
</p>
<p>1We make use of the relation:
</p>
<p>sin.x/ sin.y/C cos.x/ cos.y/ D cos.x � y/ :</p>
<p/>
</div>
<div class="page"><p/>
<p>6.1 HAMILTON&rsquo;s Equations 87
</p>
<p>We find a description of the motion in phase space by calculating the generalized
momenta pi, i D 1; 2 as
</p>
<p>p1 D
@
</p>
<p>@ P'1
L D m`2 Œ2 P'1 C P'2 cos.'1 � '2/&#141; ; (6.11)
</p>
<p>and
</p>
<p>p2 D
@
</p>
<p>@ P'2
L D m`2 Œ P'2 C P'1 cos.'1 � '2/&#141; : (6.12)
</p>
<p>The aim is now to express the kinetic energy (6.8) in terms of generalized
momenta p1 and p2. To accomplish this we solve in a first step Eq. (6.12) for P'2
and obtain
</p>
<p>P'2 D
p2
</p>
<p>m`2
� P'1 cos.'1 � '2/ : (6.13)
</p>
<p>This is used to rewrite Eq. (6.11). Solving for P'1 gives:
</p>
<p>P'1 D
�
</p>
<p>2 � cos2.'1 � '2/
��1 h p1
</p>
<p>m`2
� p2
</p>
<p>m`2
cos.'1 � '2/
</p>
<p>i
</p>
<p>: (6.14)
</p>
<p>The trigonometric identity cos2.x/C sin2.x/ D 1 changes Eq. (6.14) into
</p>
<p>P'1 D
1
</p>
<p>m`2
p1 � p2 cos.'1 � '2/
1C sin2.'1 � '2/
</p>
<p>: (6.15)
</p>
<p>This is then used to transform Eq. (6.13) into
</p>
<p>P'2 D
1
</p>
<p>m`2
</p>
<p>�
</p>
<p>p2 �
p1 cos.'1 � '2/ � p2 cos2.'1 � '2/
</p>
<p>1C sin2.'1 � '2/
</p>
<p>�
</p>
<p>D 1
m`2
</p>
<p>2p2 � p1 cos.'1 � '2/
1C sin2.'1 � '2/
</p>
<p>: (6.16)
</p>
<p>Hence, with help of Eqs. (6.15) and (6.16) we can reevaluate the kinetic
energy (6.8) to give
</p>
<p>T D m`
2
</p>
<p>2
</p>
<p>�
</p>
<p>2 P'21 C P'22 C 2 P'1 P'2 cos.'1 � '2/
�
</p>
<p>D 1
2m`2
</p>
<p>p21 C 2p22 � 2p1p2 cos.'1 � '2/
1C sin2.'1 � '2/
</p>
<p>: (6.17)</p>
<p/>
</div>
<div class="page"><p/>
<p>88 6 The Double Pendulum
</p>
<p>The HAMILTON functionH.p1; p2; '1; '2/ is the sum of the kinetic energy (6.17)
and the potential energy (6.9) and we get:
</p>
<p>H D T C U
</p>
<p>D 1
2m`2
</p>
<p>p21 C 2p22 � 2p1p2 cos.'1 � '2/
1C sin2.'1 � '2/
</p>
<p>Cmg` Œ4 � 2 cos.'1/� cos.'2/&#141; : (6.18)
</p>
<p>Thus, we are now, finally, in a position to formulate HAMILTON&rsquo;s equations of
motion from
</p>
<p>P'i D
@
</p>
<p>@pi
H ; Ppi D �
</p>
<p>@
</p>
<p>@'i
H ; i D 1; 2; (6.19)
</p>
<p>and the dynamics of the double pendulum are determined by the solutions of the
following set of differential equations:
</p>
<p>P'1 D
1
</p>
<p>m`2
p1 � p2 cos.'1 � '2/
1C sin2.'1 � '2/
</p>
<p>; (6.20a)
</p>
<p>P'2 D
1
</p>
<p>m`2
2p2 � p1 cos.'1 � '2/
1C sin2.'1 � '2/
</p>
<p>; (6.20b)
</p>
<p>Pp1 D
1
</p>
<p>m`2
1
</p>
<p>1C sin2.'1 � '2/
</p>
<p>"
</p>
<p>�p1p2 sin.'1 � '2/
</p>
<p>Cp
2
1 C 2p22 � 2p1p2 cos.'1 � '2/
</p>
<p>1C sin2.'1 � '2/
cos.'1 � '2/ sin.'1 � '2/
</p>
<p>#
</p>
<p>�2mg` sin.'1/ ; (6.20c)
</p>
<p>and
</p>
<p>Pp2 D
1
</p>
<p>m`2
1
</p>
<p>1C sin2.'1 � '2/
</p>
<p>h
</p>
<p>p1p2 sin.'1 � '2/
</p>
<p>�p
2
1 C 2p22 � 2p1p2 cos.'1 � '2/
</p>
<p>1C sin2.'1 � '2/
sin.'1 � '2/ cos.'1 � '2/
</p>
<p>#
</p>
<p>�mg` sin.'2/ : (6.20d)</p>
<p/>
</div>
<div class="page"><p/>
<p>6.2 Numerical Solution 89
</p>
<p>The following section is dedicated to the numerical solution of Eqs. (6.20) with
the help of the explicit RUNGE-KUTTA algorithm e-RK-4 introduced in Sect. 5.3.
</p>
<p>6.2 Numerical Solution
</p>
<p>In a first step we recognize that Eqs. (6.20) are of the form
</p>
<p>Py D F.y/ ; (6.21)
</p>
<p>where y 2 R4. Let us define
</p>
<p>y D
</p>
<p>0
</p>
<p>B
B
@
</p>
<p>y1
</p>
<p>y2
</p>
<p>y3
</p>
<p>y4
</p>
<p>1
</p>
<p>C
C
A
</p>
<p>�
</p>
<p>0
</p>
<p>B
B
@
</p>
<p>'1
'2
</p>
<p>p1
</p>
<p>p2
</p>
<p>1
</p>
<p>C
C
A
; (6.22)
</p>
<p>and consequently
</p>
<p>0
</p>
<p>B
B
@
</p>
<p>P'1
P'2
Pp1
Pp2
</p>
<p>1
</p>
<p>C
C
A
</p>
<p>D F.y/ �
</p>
<p>0
</p>
<p>B
B
@
</p>
<p>f1.y/
</p>
<p>f2.y/
</p>
<p>f3.y/
</p>
<p>f4.y/
</p>
<p>1
</p>
<p>C
C
A
: (6.23)
</p>
<p>We introduce time instances tn D n
t, n 2 N and use the notation yn � y.tn/ D
.yn1; y
</p>
<p>n
2; y
</p>
<p>n
3; y
</p>
<p>n
4/
</p>
<p>T . Furthermore, F.y/ is not an explicit function of time t and we
reformulate the e-RK-4 algorithm of Eq. (5.39) as:
</p>
<p>Y1 D yn ;
</p>
<p>Y2 D yn C

t
</p>
<p>2
F.Y1/ ;
</p>
<p>Y3 D yn C

t
</p>
<p>2
F.Y2/ ;
</p>
<p>Y4 D yn C
tF.Y3/ ;
</p>
<p>ynC1 D yn C

t
</p>
<p>6
ŒF.Y1/C 2F.Y2/C 2F.Y3/C F.Y4/&#141; : (6.24)</p>
<p/>
</div>
<div class="page"><p/>
<p>90 6 The Double Pendulum
</p>
<p>Fig. 6.2 Numerical solution of the double pendulum with initial conditions '1.0/ D '2.0/ D 0:0,
p1.0/ D 4:0 and p2.0/ D 2:0. (a) Trajectory in '-space, (b) trajectory in p-space, and (c) trajectory
in local .x; z/-space. The solid circles numbered 1 and 2 represent the two masses in their initial
configuration
</p>
<p>Hence, the only remaining challenge is to correctly implement the function F.y/ D
Œ f1.y/; f2.y/; f3.y/; f4.y/&#141;
</p>
<p>T according to Eqs. (6.20).
The following graphs discuss the dynamics (trajectories in '- and p-space, as
</p>
<p>well as in configuration space) of the pendulum and for this purpose we defined the
parameters m D ` D 1 and g D 9:8067. The time step was chosen to be
t D 0:001
and we calculated N D 60;000 time steps.
</p>
<p>We start with Fig. 6.2. The two masses numbered 1 and 2 are initially in the
equilibrium position (solid circles). Both masses are pushed to the right but the push
on mass 1 [p1.0/ D 4:0] is much stronger than the one mass 2 experiences [p2.0/ D
2:0]. Thus, mass 2 is &lsquo;dragged&rsquo; along in the process. This is made transparent by two
&lsquo;snapshots&rsquo; indicated by solid light gray circles and solid gray circles. The motion
of the whole system is quite regular.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.2 Numerical Solution 91
</p>
<p>Fig. 6.3 Numerical solution of the double pendulum with initial conditions '1.0/ D 1:0; '2.0/ D
0:0, p1.0/ D 0:0 and p2.0/ D 3:0. (a) Trajectory in '-space, (b) trajectory in p-space, and (c)
trajectory in local .x; z/-space. The solid circles numbered 1 and 2 represent the two masses in
their initial configuration
</p>
<p>We proceed with Fig. 6.3. In this case mass 1 is displaced from its position by
the initial angular displacement '1 D 1:0. This initial configuration is indicated
by the solid circles numbered 1 and 2 representing the two point-masses. Mass
2 is then pushed to the right with p2.0/ D 3:0. Again, mass 1 remains on a
trajectory centered around the point .0;2/ in configuration space. But in contrast
to the previous situation it follows now mass 2. Mass 2, on the other hand, develops
a very lively trajectory, Fig. 6.3c. Two snapshots indicated by solid light gray circles
and solid gray circles illustrate configurations of particular interest.
</p>
<p>The dynamics depicted in Fig. 6.4 is quite similar to the one already discussed
in Fig. 6.2. Initially both masses are in the equilibrium position and then mass 2 is
pushed to the right [p2.0/ D 4:0]. Thus, mass 1 is trailing behind. In contrast to the
previous Fig. 6.3 the trajectory of mass 2 will now be symmetric around the z-axis
given enough time. Again, snapshots indicated by solid light gray circles and solid
gray circles indicate interesting configurations.</p>
<p/>
</div>
<div class="page"><p/>
<p>92 6 The Double Pendulum
</p>
<p>Fig. 6.4 Numerical solution of the double pendulum with initial conditions '1.0/ D '2.0/ D 0:0,
p1.0/ D 0:0 and p2.0/ D 4:0. (a) Trajectory in '-space, (b) trajectory in p-space, and (c) trajectory
in local .x; z/-space. The solid circles numbered 1 and 2 represent the two masses in their initial
configuration
</p>
<p>The initial condition which resulted in the trajectory shown in Fig. 6.5 differs
only for mass 2 from the initial conditions which lead to the trajectory in Fig. 6.4.
Mass 2 is now pushed evenmore strongly to the right [p2.0/ D 5:0]. Of course, mass
1 is again dragging behind mass 2. In contrast to Fig. 6.4 the initial momentum of
mass 2 is now sufficient to allow mass 2 to pass through the center of the inner
mass&rsquo; circular trajectory. Snapshots indicated by light gray solid circles and solid
gray circles emphasize interesting configurations.
</p>
<p>The situation shown in Fig. 6.6 differs from the one of Fig. 6.5 only by the initial
condition for mass 2. It is now pushed even more strongly to the right [p2.0/ D 6:5]
and this initial momentum is sufficient to cause mass 1 to rotate around the point
.0;2/. Nevertheless, mass 1 is permanently dragging behind mass 2. Two interesting
configurations are depicted by snapshots (solid light gray circles and solid gray
circles).</p>
<p/>
</div>
<div class="page"><p/>
<p>6.2 Numerical Solution 93
</p>
<p>Fig. 6.5 Numerical solution of the double pendulum with initial conditions '1.0/ D '2.0/ D 0:0,
p1.0/ D 0:0 and p2.0/ D 5:0. (a) Trajectory in '-space, (b) trajectory in p-space, and (c) trajectory
in local .x; z/-space. The solid circles numbered 1 and 2 represent the two masses in their initial
configuration (The angles '2 &gt; � correspond to complete rotations of the pendulum)
</p>
<p>A comparison between trajectories as a result of different initial conditions
reveals that the physical system is highly sensitive to the choice of the initial
conditions y0 D Œ'1.0/; '2.0/; p1.0/; p2.0/&#141;T . For instance, consider Figs. 6.4, 6.5,
and 6.6. In all three cases we chose y0 in such a way that the initial angles
'1.0/ D '2.0/ D 0 and the generalized momentum coordinate p1.0/ D 0. The
only difference is that we used different values for the initial value of the second
momentum coordinate p2. However, the resulting dynamics of '1 vs. '2 as well as
p1 vs. p2 are entirely different and so are the local .x; z/-space trajectories. Hence,
the system is chaotic. In the following section we will briefly discuss a method
designed to characterize chaotic behavior of physical systems [6&ndash;10].</p>
<p/>
</div>
<div class="page"><p/>
<p>94 6 The Double Pendulum
</p>
<p>Fig. 6.6 Numerical solution of the double pendulum with initial conditions '1.0/ D '2.0/ D 0:0,
p1.0/ D 0:0 and p2.0/ D 6:5. (a) Trajectory in '-space, (b) trajectory in p-space, and (c) trajectory
in configuration space. The solid circles numbered 1 and 2 represent the two masses in their initial
configuration (The angles '2 &gt; � correspond to complete rotations of the pendulum)
</p>
<p>6.3 Numerical Analysis of Chaos
</p>
<p>It is the aim of this section to analyze in more detail the chaotic behavior
observed in the dynamics of the double pendulum. This requires the introduction
of some basic notations. We consider a physical system with f degrees of freedom
where q1.t/; : : : ; qf .t/ denote the generalized coordinates and p1.t/; : : : ; pf .t/ denote
the corresponding generalized momenta. Together, both fully characterize the
state of the system at time t. Consequently, the f -dimensional vector q.t/ D
Œq1.t/; q2.t/; : : : ; qf .t/&#141;
</p>
<p>T describes a point in configuration space of the physical
system. In case of a pendulum consisting of f point-masses connected in a
similar fashion as the double pendulum discussed above, which corresponds to the
particular case f D 2, the configuration space is constrained to values 'i 2 .��; �&#141;,
i D 1; : : : ; f . This resembles an f -dimensional torus.
</p>
<p>The 2f -dimensional vector x.t/ D Œq1.t/; : : : ; qf .t/; p1.t/; : : : ; pf .t/&#141;T describes
a point in the phase space of the physical system at some particular time t. The</p>
<p/>
</div>
<div class="page"><p/>
<p>6.3 Numerical Analysis of Chaos 95
</p>
<p>time evolution of a physical system is represented by its phase space trajectory. Of
course, the phase space trajectories x.t/ are differentiable with respect to t.2
</p>
<p>We define an autonomous system as a system which is time-invariant, i.e. the
HAMILTON function H.x; t/ does not depend explicitly on time t, H.x; t/ � H.x/.
Hence, a physical system is referred to as autonomous if the HAMILTON function
H.x; t/ of the system obeys
</p>
<p>@
</p>
<p>@t
H.x; t/ D 0 : (6.25)
</p>
<p>Thus, the total energy is conserved.
An autonomous system is referred to as integrable if it has f independent
</p>
<p>invariants I1; : : : ; If
</p>
<p>Ij.x/ D Ij D const; j D 1; : : : ; f : (6.26)
</p>
<p>One of these is the energy. Each particular invariant Ij reduces the dimension
of the manifold on which the phase space trajectories can propagate. Hence, an
integrable system propagates on an f -dimensional subspace of the 2f -dimensional
phase space. We note that a one-dimensional autonomous system is integrable since
the conservation of energy delivers the required invariant.
</p>
<p>On the other hand, non-integrable systems can show chaotic behavior. In this
case the trajectories develop a strong dependence on the initial conditions which
makes an analytic calculation of the dynamics extremely difficult. However, since
the trajectories can be computed without problems by numeric means, we discuss
now how to characterize chaotic behavior on the computer.
</p>
<p>For this sake we investigate the dynamics of an autonomous Hamiltonian system
starting with one of two initial conditions, namely x0 and x00. Then the system arrives
at time t at the phase space points x.t/ D 't.x0/ and x0.t/ D 't.x00/, respectively, as
a solution of HAMILTON&rsquo;s equations of motion. Here 't.x0/ denotes the flow of the
system as defined in Sect. 5.4. Since the trajectories in a chaotic system strongly
depend on the initial conditions x0 and x00 we introduce the separation between
the two trajectories 't.x0/ and 't.x00/ at time t as d.t/ D j't.x0/ � 't.x00/j where
j � j denotes some suitable norm. This length can now, for instance, be used to
characterize the stability of the trajectory 't.x0/ [11]. In particular, a solution 't.x0/
is referred to as stable if
</p>
<p>8� &gt; 0 9ı.�/ &gt; 0 W 8x00 W d.0/ &lt; ı ) d.t/ &lt; �; 8t &gt; 0 : (6.27)
</p>
<p>In words: We speak of a stable solution if the trajectory 't.x00/ which corresponds
to the perturbed initial condition x00 stays within a tube of radius � around the
</p>
<p>2The symplectic mapping 't W x0 7! x.t/ from the initial conditions x0 to the phase space point
x.t/ at time t is referred to as Hamiltonian flow of the system. This was discussed in Sect. 5.4.</p>
<p/>
</div>
<div class="page"><p/>
<p>96 6 The Double Pendulum
</p>
<p>unperturbed trajectory 't.x0/ for all t &gt; 0. Alternatively, a solution is referred
to as asymptotically stable if the distance to adjacent trajectories tends to zero,
i.e. d.t/ ! 0 as t ! 1. Such solutions tend to attract trajectories from their
neighborhood and, hence, they are referred to as attractors. Finally, a periodic orbit
is defined as a trajectory for which one can find a time � such that:
</p>
<p>'� .x/ D x ; 8x: (6.28)
</p>
<p>To find an easy answer to the question whether or not a particular solution
of a non-integrable system is stable, the clear, topological method of POINCAR&Eacute;
maps was introduced. The idea was to reduce the investigation of the complete
2f -dimensional phase space trajectory x.t/ D 't.x0/ to the investigation of its
intersection points through a plane ˙ which is transverse to the flow of the system.
This plane is a subspace of dimension 2f � 1 and is commonly referred to as
POINCAR&Eacute; section [3]. The transversality of the POINCAR&Eacute; section ˙ means that
periodic flows intersect this section and never flow parallel to or within it.
</p>
<p>Consider a trajectory which is bound to a finite domain, i.e. it does not tend
to infinity in any phase space coordinate. In this case it is possible to define the
POINCAR&Eacute; section in such a way that the trajectory will intersect this section not
only once but several times. Thus, a POINCAR&Eacute; map is then the mapping of one
intersection point P onto the next intersection point P0.
</p>
<p>Let us substantiate this idea: we consider the initial condition x0 for which a
trajectory � is periodic. We choose the initial time t D 0 in such a way that x0 2 ˙ ,
where ˙ is the POINCAR&Eacute; section, Fig. 6.7. We suppose that after a time �.x0/
the trajectory intersects this POINCAR&Eacute; section again.3 Since we demanded that the
trajectory which started in x0 is periodic, we deduce that it intersects the POINCAR&Eacute;
section again at some point '�.x0/.x0/ D x0. We consider now a slightly perturbed
initial condition x0 2 U0.x0/, where U0.x0/ is referred to as the neighborhood of x0.
In this case the trajectory will in general not be periodic, and the next intersection
point '�.x0/.x0/ &curren; x0. The mapping from one intersection point x0 onto the next
intersection point '�.x0/.x0/ is called the POINCAR&Eacute; map P.x0/ D '�.x0/.x0/. We note
that the particular point x0 is a fixed point of this mapping,P.x0/ D x0. Furthermore,
we note that if x0 2 U0.x0/ we will have P.x0/ 2 U1.x0/, where U1.x0/ is the
neighborhood of first return. This is indicated schematically in Fig. 6.7.
</p>
<p>3Note that we denoted � � �.x0/ in order to emphasize that the recurrence time � will depend on
the initial condition x0.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.3 Numerical Analysis of Chaos 97
</p>
<p>Fig. 6.7 Schematic
illustration of the
neighborhood U0.x0/ and the
neighborhood of first return
U1.x0/ of a periodic
trajectory � . The intersection
point x0 of � with˙ is a
fixed point of this mapping
</p>
<p>We utilize now these concepts and analyze the dynamics of the double pendulum.
We have four generalized coordinates which, with the help of conservation of
energy, are constrained to a three-dimensionalmanifold within the four-dimensional
phase space. Since the investigation of these three-dimensional trajectories is very
complex we consider a two-dimensional POINCAR&Eacute; section. For instance, the
coordinates Œ'1.t/; p1.t/&#141;T can be &lsquo;measured&rsquo; whenever '2.t/ D 0 and p2 &gt; 0. Thus,
the system&rsquo;s state is registered whenever mass 2 crosses the vertical plane from the
left-hand side.
</p>
<p>We discuss now some of the most typical scenarios for POINCAR&Eacute; plots. (Such
a plot represents the POINCAR&Eacute; section together with all intersection points of
a particular trajectory.) Note that this discussion is, of course, not restricted to
the case of the double pendulum. Two different scenarios can be distinguished
for integrable systems: (i) the set of intersection points .�1; �2; : : : ; �N/ is finite.
(ii) In the more general case, the dimension N of the set of intersection points is
infinite. In both cases the intersection points form one-dimensional lines which
do not have to be connected. Figure 6.8a, b discuss this schematically. However,
if the system is non-integrable, a third scenario is possible: chaotic behavior. In
this case the intersection points appear to be randomly distributed on the two-
dimensional POINCAR&Eacute; section and one observes space-filling behavior. This is
illustrated schematically in Fig. 6.8c. Whether one observes chaotic behavior or not
depends on the choice of the initial conditions.</p>
<p/>
</div>
<div class="page"><p/>
<p>98 6 The Double Pendulum
</p>
<p>Fig. 6.8 Schematic illustration of the three types of POINCAR&Eacute; plots as discussed in the text. (a)
Finite number of intersection points, (b) infinite number of intersection points which, however,
form closed lines, (c) space-filling and, consequently, chaotic behavior
</p>
<p>Fig. 6.9 POINCAR&Eacute; plot of the double pendulum with initial conditions '1.0/ D '2.0/ D 0:0,
p1.0/ D 4:0 and p2.0/ D 2:0. It corresponds to the situation discussed in Fig. 6.2
</p>
<p>In Figs. 6.9, 6.10, and 6.11 we present POINCAR&Eacute; plots of the double pendulum.
The graphs were obtained with help of the method discussed above, i.e. '2 D 0 and
p2 &gt; 0. Again, we set m D ` D 1 and g D 9:8067. The time step was chosen to
be 
t D 0:001 and we calculated N D 36 � 104 time steps. In Figs. 6.9 and 6.10</p>
<p/>
</div>
<div class="page"><p/>
<p>6.3 Numerical Analysis of Chaos 99
</p>
<p>Fig. 6.10 POINCAR&Eacute; plot of
the double pendulum with
initial conditions
'1.0/ D 1:0; '2.0/ D 0:0,
p1.0/ D 0:0 and p2.0/ D 3:0.
It corresponds to the situation
discussed in Fig. 6.3
</p>
<p>Fig. 6.11 POINCAR&Eacute; plot of
the double pendulum with
initial conditions
'1.0/ D '2.0/ D 0:0,
p1.0/ D 0:0 and p2.0/ D 4:0.
It corresponds to the situation
discussed in Fig. 6.4
</p>
<p>we observe regular behavior as it was illustrated in Fig. 6.8b. In Fig. 6.11 the points
are space filling and, consequently, chaotic behavior is observed in this particular
case. Keeping in mind that this particular POINCAR&Eacute; plot refers to the initial value
problem of Fig. 6.4 we conclude that all problems of this series, i.e. Figs. 6.4, 6.5,
and 6.6, are non-integrable and chaotic.
</p>
<p>Summary
</p>
<p>The dynamics of the double pendulum is described by a system of four ordinary
first order differential equations. It is a typical initial value problem and, thus,
the methods introduced in Chap. 5 are all candidates to find a numerical solution.
Here we concentrated on the explicit RUNGE-KUTTA algorithm e-RK-4 of Sect. 5.3.</p>
<p/>
</div>
<div class="page"><p/>
<p>100 6 The Double Pendulum
</p>
<p>Solutions were studied in detail for several classes of initial conditions. One of the
results was that rather small changes of the initial conditions could result in rather
strong, chaotic reactions of the outer mass. This triggered the obvious question about
the stability of a numerical analysis and of physical dynamics in general. While the
stability of numerical methods has already been discussed in Chap. 1 we focused
here on the chaotic behavior of Hamiltonian systems. Consequently, a short section
on the numerical analysis of chaos was added. It contained the most important
concepts and in particular the concept of the stability of a phase space trajectory
against variation of initial conditions. Finally, the importance of POINCAR&Eacute; plots
in recognizing whether a system is integrable or non-integrable was explained.
Non-integrable systems can develop chaotic behavior. Thus, POINCAR&Eacute; plots are
an important tool to study chaos in mechanics.
</p>
<p>Problems
</p>
<p>1. Verify HAMILTON&rsquo;s equations of motion derived in Sect. 6.1. Implement the
e-RK-4 algorithm discussed in Sects. 5.3 and 6.2 to integrate the equations of
motion. Plot the trajectories for various initial conditions. Use the examples
illustrated in Sect. 6.2 to check the code.
</p>
<p>2. Produce POINCAR&Eacute; plots by plotting .'1; p1/ whenever '2 D 0 and p2 &gt; 0.
The condition '2 D 0 is substituted by j'2j &lt; � in the numerically realization.
Note that if the points are space filling the dynamics are chaotic, as discussed in
Sect. 6.3. Try to find different initial conditions which result in regular behavior
and different initial conditions which produce chaotic dynamics.
</p>
<p>3. Let x.t/ D Œ'1.t/; '2.t/; p1.t/; p2.t/&#141;T and x0.t/ D Œ' 01.t/; ' 02.t/; p01.t/; p02.t/&#141;T be
two trajectories which correspond to different initial conditions x0 and x00. In this
case the distance between trajectories is defined as
</p>
<p>d.t/ D
q
</p>
<p>Œ'1.t/ � '01.t/&#141;2 C Œ'2.t/ � '02.t/&#141;2 C Œp1.t/� p01.t/&#141;2 C Œp2.t/ � p02.t/&#141;2:
</p>
<p>Plot the distance d.t/ as a function of time t for two different initial conditions.
4. Extend the code of the double pendulum of equal mass and equal length to cover
</p>
<p>the case when the lengths and masses of the individual pendula are different,
`1 &curren; `2 and m1 &curren; m2. What happens? For instance, one can choose a certain
initial condition and keep `1 and `2 fixed. What is the influence of `1 and `2 on
the dynamics?
</p>
<p>5. Show that the dynamics become integrable in the absence of a gravitational force,
i.e. g D 0. What are the conserved quantities? How do the POINCAR&Eacute; plots look
like? Again, try different initial conditions.</p>
<p/>
</div>
<div class="page"><p/>
<p>References 101
</p>
<p>References
</p>
<p>1. Arnol&rsquo;d, V.I.: Mathematical Methods of Classical Mechanics, 2nd edn. Graduate Texts in
Mathematics, vol. 60. Springer, Berlin/Heidelberg (1989)
</p>
<p>2. Fetter, A.L., Walecka, J.D.: Theoretical Mechanics of Particles and Continua. Dover, New York
(2004)
</p>
<p>3. Scheck, F.: Mechanics, 5th edn. Springer, Berlin/Heidelberg (2010)
4. Goldstein, H., Poole, C., Safko, J.: Classical Mechanics, 3rd edn. Addison-Wesley, Menlo Park
</p>
<p>(2013)
5. Flie&szlig;bach, T.: Mechanik, 7th edn. Lehrbuch zur Theoretischen Physik I. Springer,
</p>
<p>Berlin/Heidelberg (2015)
6. Arnol&rsquo;d, V.I.: Catastrophe Theory. Springer, Berlin/Heidelberg (1992)
7. McCauley, J.L.: Chaos, Dynamics and Fractals. Cambridge University Press, Cambridge
</p>
<p>(1994)
8. Devaney, R.L.: An Introduction to Chaotic Dynamical Systems, 2nd edn. Westview, Boulder
</p>
<p>(2003)
9. Schuster, H.G., Just, W.: Deterministic Chaos, 4th edn. Wiley, New York (2006)
10. Teschl, G.: Ordinary Differential Equations and Dynamical Systems. Graduate Studies in
</p>
<p>Mathematics, vol. 140. American Mathematical Society, Providence (2012)
11. Lyapunov, A.M.: The General Problem of Stability of Motion. Taylor &amp; Francis, London
</p>
<p>(1992)</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 7
Molecular Dynamics
</p>
<p>7.1 Introduction
</p>
<p>It is the aim of many branches of research in physics to describe macroscopic
properties of matter on the basis of microscopic dynamics. However, a description
of the simultaneousmotion of a large number of interacting particles is in most cases
not feasible by analytic methods. Moreover, a description is particularly difficult if
the interaction between the particles is strong. Within the framework of statistical
mechanics one tries to remedy these difficulties by employing some simplifying
assumptions and by treating the system from a statistical point of view [1&ndash;4].
However, most of these simplifying assumptions are only justified within certain
limits, such as the weak coupling limit or the low density limit. Nevertheless, it is
not easy to establish how the solutions acquired are influenced by these limits and
how the physics beyond these limits can be perceived. This makes the necessity of
numerical solutions quite apparent. There are essentially two methods to determine
physical quantities over a restricted set of states, namely molecular dynamics [5&ndash;7]
and Monte Carlo methods. The technique of molecular dynamics will be discussed
within this chapter while an introduction into some basic features of Monte Carlo
algorithms is postponed to the second part of this book.
</p>
<p>We strictly focus on a particular sub-field of molecular dynamics, namely on
classical molecular dynamics, i.e. the treatment of classical physical systems.
Extensions to quantum mechanical systems, which are commonly referred to as
quantum molecular dynamics [8], will not be discussed here.
</p>
<p>&copy; Springer International Publishing Switzerland 2016
B.A. Stickler, E. Schachinger, Basic Concepts in Computational Physics,
DOI 10.1007/978-3-319-27265-8_7
</p>
<p>103</p>
<p/>
</div>
<div class="page"><p/>
<p>104 7 Molecular Dynamics
</p>
<p>7.2 Classical Molecular Dynamics
</p>
<p>The classical model system for molecular dynamics consists of N particles with
positions ri � ri.t/, velocities vi � vi.t/ D Pri.t/ and masses mi, where i D
1; 2; : : : ;N. We note that ri and vi are vectors of the same dimension. We can write
NEWTON&rsquo;s equations of motion as
</p>
<p>miRri D fi.r1; r2; : : : ; rN/ ; (7.1)
</p>
<p>where we introduced the forces fi � fi.r1; r2; : : : ; rN/. Again, we note that the
forces fi are vectors of the same dimension as ri and vi. We specify the forces fi
by demanding them to be conservative. Thus, we write
</p>
<p>fi.r1; r2; : : : ; rN/ D �riU.r1; r2; : : : ; rN/ ; (7.2)
</p>
<p>where ri is the gradient pertaining to the spatial components of the i-th particle
and U.r1; r2; : : : ; rN/ is some potential which we will abbreviate by dropping its
arguments: U � U.r1; r2; : : : rN/. We then specify this potential U as the sum of
two-particle interactions Uij and some external potential Uext as, for instance, the
gravitational field or a static electric potential applied to the system:
</p>
<p>U D 1
2
</p>
<p>X
</p>
<p>i
</p>
<p>X
</p>
<p>j&curren;i
Uij C Uext : (7.3)
</p>
<p>In our discussion of the two-body problem (Appendix A) and, in particular,
of the KEPLER problem in Chap. 4 we considered a central potential, which was
proportional to �1=r. Due to the conservation of angular momentum, it was
convenient to introduce an effective potential Ueff as the sum of an attractive and
repulsive part as it was defined in Eq. (4.3) and illustrated in Fig. 4.1. In contrast, in
molecular dynamics the most prominent two-body interaction potential is known as
the LENNARD-JONES potential [9]. It is of the form
</p>
<p>U.jrj/ D 4�
"�
</p>
<p>�
</p>
<p>jrj
</p>
<p>�12
</p>
<p>�
�
�
</p>
<p>jrj
</p>
<p>�6
#
</p>
<p>; (7.4)
</p>
<p>where � and � are real parameters and jrj is the distance between two particles.
The significance of the parameters � and � as well as the form of U.jrj/ defined by
Eq. (7.4) is illustrated in Fig. 7.1. The LENNARD-JONES potential was particularly
developed to model the interaction between neutral atoms or molecules. The
repulsive term, which is proportional to jrj�12, describes the PAULI repulsion while
the attractive jrj�6 term accounts for attractive VAN DER WAALS forces.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.2 Classical Molecular Dynamics 105
</p>
<p>Fig. 7.1 Illustration of the
LENNARD-JONES potential,
Eq. (7.4). � describes the
depth of the potential well
and � is the position of the
root of the LENNARD-JONES
potential
</p>
<p>We introduce the distance between particles i and j via
</p>
<p>rij D jri � rjj D jrj � rij D rji ; (7.5)
</p>
<p>and define the two-body potential
</p>
<p>Uij � U.rij/ ; (7.6)
</p>
<p>where U is approximated by the LENNARD-JONES potential (7.4). Furthermore, we
deduce from Eq. (7.4) that
</p>
<p>f .jrj/ D �rrU.jrj/ D
24�
</p>
<p>jrj2
</p>
<p>"
</p>
<p>2
</p>
<p>�
�
</p>
<p>jrj
</p>
<p>�12
</p>
<p>�
�
�
</p>
<p>jrj
</p>
<p>�6
#
</p>
<p>r ; (7.7)
</p>
<p>where we keep in mind that r is a vector. Hence, we write the forces fi which appear
in NEWTON&rsquo;s equations of motion (7.1) with the help of (7.3) in the form
</p>
<p>fi D �riU
</p>
<p>D �ri
</p>
<p>0
</p>
<p>@
1
</p>
<p>2
</p>
<p>X
</p>
<p>k
</p>
<p>X
</p>
<p>l&curren;k
Ukl C Uext
</p>
<p>1
</p>
<p>A
</p>
<p>D �
X
</p>
<p>j&curren;i
riUij � riUext
</p>
<p>D
X
</p>
<p>j&curren;i
f .rij/C f iext
</p>
<p>D
X
</p>
<p>j&curren;i
fij C f iext ; (7.8)</p>
<p/>
</div>
<div class="page"><p/>
<p>106 7 Molecular Dynamics
</p>
<p>where we implicitly defined the external force f iext acting on particle i and the two-
particle forces fij acting between particle i and j. We want to make the road visible,
which guides us to a numerical solution of NEWTON&rsquo;s equations of motion (7.1),
and introduce the vectors R D .r1; r2; : : : ; rN/T , V D .v1; v2; : : : ; vN/T D PR, and
F D . f1=m1; f2=m2; : : : ; fN=mN/T . This transforms Eq. (7.1) into the very compact
form
</p>
<p>RR D F ; (7.9)
</p>
<p>which is equivalent to a set of two first order ordinary differential equations:
</p>
<p>� PR
PV
</p>
<p>�
</p>
<p>D
�
</p>
<p>V
</p>
<p>F
</p>
<p>�
</p>
<p>: (7.10)
</p>
<p>This set is already of the standard form (5.1) of initial value problems.
We are now in a position to proceedwith a discussion of some numericalmethods
</p>
<p>which have been developed in Chap. 5 to solve this initial value problem. For this
sake, we regard discrete time instances tk D k
t, where k 2 N and function values
at these discrete time instances tk are denoted by a subscript k, as for instance Rk �
R.tk/.
</p>
<p>(i) In a first approximation we apply the symplectic EULER method [see
Eq. (4.33)] to Eq. (7.10) and obtain
</p>
<p>�
</p>
<p>RkC1
VkC1
</p>
<p>�
</p>
<p>D
�
</p>
<p>Rk
</p>
<p>Vk
</p>
<p>�
</p>
<p>C
�
</p>
<p>VkC1
Fk
</p>
<p>�
</p>
<p>
t : (7.11)
</p>
<p>Inserting the second into the first equation results in
</p>
<p>RkC1 D Rk C Vk
t C Fk
t2 : (7.12)
</p>
<p>The velocity Vk at time tk is then approximated by the backward difference
derivative (2.10b) and we find the recursion relation:
</p>
<p>RkC1 D 2Rk � Rk�1 C Fk
t2 : (7.13)
</p>
<p>We note that it is only valid for k � 1. The initialization step necessary to
complete the analysis is found by expanding R1 in a TAYLOR series up to
second order:
</p>
<p>R1 D R0 C
tV0 C
1
</p>
<p>2
F0
t
</p>
<p>2 : (7.14)
</p>
<p>This method is referred to as the ST&Ouml;RMER-VERLET algorithm [10]. Note
that Eq. (7.14) serves as the initialization of the sequence of time steps.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.2 Classical Molecular Dynamics 107
</p>
<p>Furthermore, we remark that Eq. (7.13) could have also been obtained using
the central difference derivative to approximate the second time derivative in
Eq. (7.1):
</p>
<p>RRk �
RkC1 � 2Rk C Rk�1
</p>
<p>
t2
D Fk : (7.15)
</p>
<p>In summary, the VERLET or ST&Ouml;RMER-VERLET algorithm is defined by the
following set of equations:
</p>
<p>RkC1 D 2Rk � Rk�1 C Fk
t2 ; k � 1;
</p>
<p>R1 D R0 C
tV0 C
1
</p>
<p>2
F0
t
</p>
<p>2 : (7.16)
</p>
<p>(ii) We employ the central rectangular rule of integration (Sect. 3.2) in order
to obtain approximations which are formally equivalent to Eq. (5.11). In
particular, we obtain from Eq. (7.10):
</p>
<p>RkC1 D Rk C VkC 12
t : (7.17)
</p>
<p>We note that the value of VkC1=2 is yet undetermined. However, it can be
determined in a similar fashion via
</p>
<p>VkC 12 D Vk� 12 C Fk
t : (7.18)
</p>
<p>This method is referred to as the leap-frog algorithm and is initialized by the
relation
</p>
<p>V 1
2
D V0 C
</p>
<p>
t
</p>
<p>2
F0 : (7.19)
</p>
<p>This equation can also be obtained by expanding V1=2 in a TAYLOR series up
to first order around the point t0 D 0 and by noting that PVk D Fk. In summary
we write the leap-frog algorithm as
</p>
<p>RkC1 D Rk C VkC 12
t ;
</p>
<p>VkC 12 D Vk� 12 C Fk
t ;
</p>
<p>V 1
2
D V0 C
</p>
<p>1
</p>
<p>2
F0
t : (7.20)</p>
<p/>
</div>
<div class="page"><p/>
<p>108 7 Molecular Dynamics
</p>
<p>(iii) A third, very elegant alternative is the so-called velocity VERLET algorithm.
We expand RkC1:
</p>
<p>RkC1 D Rk C Vk
t C
1
</p>
<p>2
Fk
t
</p>
<p>2 : (7.21)
</p>
<p>This allows to calculate the spatial coordinates at time tkC1 if Rk and Vk are
given. Note that Fk � F.Rk/ is completely determined by the positions Rk.
Nevertheless, we need one more relation in order to determine the velocities
at times tkC1. Again, we expand VkC1 in a TAYLOR series. However, we
approximate the remainder by the arithmetic mean between tk and tkC1:
</p>
<p>VkC1 D Vk C
1
</p>
<p>2
.Fk C FkC1/
t : (7.22)
</p>
<p>The strategy is clear: we calculate the positions RkC1 from Eq. (7.21) for given
values of Rk and Vk. With the help of RkC1 we compute FkC1, which is then
inserted into Eq. (7.22) which determines VkC1. In summary, the complete
algorithm of the velocity VERLET method is defined by the steps:
</p>
<p>RkC1 D Rk C Vk
t C
1
</p>
<p>2
Fk
t
</p>
<p>2 ;
</p>
<p>VkC1 D Vk C
1
</p>
<p>2
.Fk C FkC1/
t : (7.23)
</p>
<p>We note some properties of these methods. The ST&Ouml;RMER-VERLET algorithm
of Eq. (7.16) is time-reversal symmetric (invariant under the transformation 
t !
�
t), hence reversible. This is a direct consequence of its relation to the symplectic
EULER method. Moreover, the positions Rk obtained with this method are highly
accurate, however, the procedure suffers under an inaccurate approximation of the
velocitiesVk. This shortcoming is clearly remedied by the leap-frog algorithm (7.20)
or the velocity VERLET algorithm (7.23). However, these methods are not time-
reversal invariant. Hence, one has to decide whether or not very accurate values
for the velocities are required for the problem at hand. In many cases the velocity
VERLET algorithm is the most popular choice.
</p>
<p>7.3 Numerical Implementation
</p>
<p>The rough structure of a molecular dynamics code consists of three crucial steps,
namely
</p>
<p>&bull; Initialization,
&bull; start simulation and equilibrate,
&bull; continue simulation and store results.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.3 Numerical Implementation 109
</p>
<p>In the following we discuss some of the most important subtleties associated with
these three parts. In particular we will focus on the choice of appropriate boundary
conditions and on the choice of the scales of characteristic quantities.
</p>
<p>Boundary Conditions
</p>
<p>Basically, there are two possibilities: (i) The system is of finite size and the
implementation of boundary conditions might be straightforward. For instance, let
us assume that we regard N particles within a finite box of reflecting boundaries,
we simply propagate the particle-coordinates in time and if a particle tries to leave
the box, we correct its trajectory according to a reflection law. The velocity is
adjusted accordingly. This is illustrated in Fig. 7.2 for a two-dimensional case and
the particular situation that the particle is reflected from the right hand boundary of
the box. The corresponding equations read
</p>
<p>rkC1 D
�
</p>
<p>xkC1
ykC1
</p>
<p>�
</p>
<p>D
�
</p>
<p>L � .QxkC1 � L/
QykC1
</p>
<p>�
</p>
<p>; (7.24)
</p>
<p>and
</p>
<p>vkC1 D
�
</p>
<p>vkC1;x
vkC1;y
</p>
<p>�
</p>
<p>D
�
</p>
<p>�QvkC1;x
QvkC1;y
</p>
<p>�
</p>
<p>: (7.25)
</p>
<p>Fig. 7.2 Illustration of the
reflection principle for a box
of finite dimension with
reflecting boundaries</p>
<p/>
</div>
<div class="page"><p/>
<p>110 7 Molecular Dynamics
</p>
<p>Here, L denotes the length of the box and QxkC1, QykC1, QvkC1;x and QvkC1;y are the
positions and velocities one would have obtained in the absence of the boundary,
see Fig. 7.2.
</p>
<p>(ii) The system is not confined. Then the situation is entirely different. Of course,
one could approximate the infinite volume by a large but finite volume. In such a
case the influence of a constraint to finite size is usually not negligible. The induced
errors are referred to as finite volume effects. A very popular choice are so called
periodic boundary conditions which means that a finite system is surrounded by an
infinite number of completely identical replicas of the system, where the forces
are allowed to act across the boundaries. Because of this, calculating the force
on one particle requires the evaluation of an infinite sum. This is numerically not
manageable and we have to find ways to truncate the sum. For instance, it might
be a good approximation to restrict the sum to nearest-neighbor cells. However, the
applicability of such an approach highly depends on the properties of the system
under investigation and, in particular, on the range of the interaction potential.
In case of a LENNARD-JONES potential the quantity defining the range of the
interaction potential is �, see Fig. 7.1.
</p>
<p>If a particle leaves the box, it enters the box at the same time on the opposite
side. More generally, due to the requirement of identical replicas, we have for all
observables O.r/ that O.r C nK/ D O.r/, where r lies within the central box, K is
a lattice vector pointing to one of the neighboring cells and n 2 Z.
</p>
<p>There is another crucial point concerning periodic boundary conditions. In case
of a closed system, the system is definitely at rest. However, if periodic boundary
conditions are imposed it is possible that the particles move with constant velocity
from one cell to another, which, in our case, resembles circling trajectories. This is
definitely not desirable since the total velocity is a measure of the kinetic energy
and, therefore, of the temperature of the system. However, one can shift the total
velocity in order to remedy this problem. In particular, if
</p>
<p>vtot D
N
X
</p>
<p>iD1
vi &curren; 0 ; (7.26)
</p>
<p>the shift
</p>
<p>v0i D vi �
1
</p>
<p>N
vtot ; (7.27)
</p>
<p>yields the desired result. We note that in a case where all masses are identical, i.e.
m1 D m2 D : : : D mN � m, this is equivalent to ptot D mvtot D 0.
</p>
<p>In conclusion, we remark that the choice of boundary conditions is not the only
item to be considered in the definition of the system. Another quite crucial point
might be the size of the box. If an infinite system is modeled using finite systems,
the dimension of the box must fairly exceed the mean free path of the particles.
Otherwise, the influence of the boundaries is going to perturb significantly the
outcome of the numerical experiment.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.3 Numerical Implementation 111
</p>
<p>Initialization and Equilibration
</p>
<p>We remember from statistical physics [1&ndash;4] that every degree of freedom in the
system (7.1) contributes just kBT=2 to the total kinetic energy because of the
equipartition theorem. Here kB is BOLTZMANN&rsquo;s constant and T is the temperature.
If we regard N particles, which move in a d-dimensional space, we have d.N � 1/
degrees of freedom, if we demand that vtot D 0. Hence, we have
</p>
<p>Ekin D
1
</p>
<p>2
</p>
<p>N
X
</p>
<p>iD1
miv
</p>
<p>2
i D
</p>
<p>d.N � 1/
2
</p>
<p>kBT ; (7.28)
</p>
<p>which gives a relation from which we can determine the temperature of the system:
</p>
<p>kBT D
1
</p>
<p>d.N � 1/
</p>
<p>NX
</p>
<p>iD1
miv
</p>
<p>2
i : (7.29)
</p>
<p>However, in many applications the system is supposed to be simulated at a given
temperature, i.e. the temperature T is an input rather than an output parameter and
is supposed to stay constant during the simulation. We can control the temperature
by rescaling the velocities and this might be necessary at several times during the
simulation in order to guarantee a constant temperature. We define
</p>
<p>v0i D �vi ; (7.30)
</p>
<p>where � is a rescaling parameter. The temperature associated with the velocities v0i
is given by
</p>
<p>kBT
0 D �
</p>
<p>2
</p>
<p>d.N � 1/
</p>
<p>N
X
</p>
<p>iD1
miv
</p>
<p>2
i : (7.31)
</p>
<p>This allows to determine how to choose � in order to obtain a certain temperature
T 0:
</p>
<p>� D
</p>
<p>s
</p>
<p>d.N � 1/kBT 0
2Ekin
</p>
<p>: (7.32)
</p>
<p>We note that if the total velocity, which is the sum of all velocities vi, is zero, the
total velocity corresponding to the rescaled velocities v0i is also equal to zero since
</p>
<p>NX
</p>
<p>iD1
v0i D �
</p>
<p>NX
</p>
<p>iD1
vi D 0 : (7.33)</p>
<p/>
</div>
<div class="page"><p/>
<p>112 7 Molecular Dynamics
</p>
<p>This ensures that rescaling of the velocities does not induce a bias.
The choice of the initial conditions highly influences the time the system needs
</p>
<p>to reach thermal equilibrium. For instance, if a gas is to be simulated at a given
temperature T it might be advantageous to choose the initial velocities according to
a MAXWELL-BOLTZMANN distribution. The MAXWELL-BOLTZMANN distribution
states that the probability [more precisely: the pdf (probability density function
describing the probability, see Appendix E)] that a particle with mass m has velocity
v is proportional to
</p>
<p>p.jvj/ / jvj2 exp
�
</p>
<p>�mjvj
2
</p>
<p>2kBT
</p>
<p>�
</p>
<p>: (7.34)
</p>
<p>Another intriguing question is how to check whether or not thermal equilibrium
has been reached. In statistical mechanics one is usually confrontedwith expectation
values of observables O.t/ as a function of time. The expectation value hOi is
defined as
</p>
<p>hOi D lim
�!1
</p>
<p>1
</p>
<p>�
</p>
<p>Z �
</p>
<p>0
</p>
<p>dtO.t/ : (7.35)
</p>
<p>Since O.t/ is not known analytically one replaces the mean value by its arithmetic
mean
</p>
<p>hOi � O D 1
n
</p>
<p>kCn
X
</p>
<p>jDkC1
O.tj/ : (7.36)
</p>
<p>If n and k are sufficiently large, the average value can be regarded as converged.
In particular, one has to choose n reasonably large and then find k in such a way,
that for all values k0 � k the same result for O is obtained. Hence, equilibrium has
been reached after k time-steps and it is now possible to &lsquo;measure&rsquo; the observables
by calculating their mean values. A more detailed discussion of such a procedure,
as, for instance, the influence of time correlations or a discussion of more advanced
techniques is postponed to Chap. 19.
</p>
<p>There is one last point: In many cases the natural units of the physical system
might be disadvantageous because they are likely to induce numerical instabilities.
In such cases a common technique is to switch to rescaled variables by introducing
new units, which are characteristic quantities for the system and all physical
quantities are expressed in these new units. For instance one might introduce the
length L of the box as the unit of space. The new spatial coordinates would then be
given by
</p>
<p>r0 D r
L
: (7.37)</p>
<p/>
</div>
<div class="page"><p/>
<p>7.3 Numerical Implementation 113
</p>
<p>Hence all coordinates take on values within the interval r0 2 Œ0; 1&#141;. However, one
cannot introduce an arbitrary set of characteristic quantities due to the physical
relations they have to obey. For instance, one might introduce a characteristic
energy E0, a characteristic length �, and a characteristic mass m. In this case the
characteristic temperature QT is determined via
</p>
<p>QT D E0
kB
: (7.38)
</p>
<p>Moreover the characteristic time � is fixed to the value
</p>
<p>� D
</p>
<p>s
</p>
<p>m�2
</p>
<p>E0
; (7.39)
</p>
<p>which results from the relation between the kinetic energy and the velocity.
To illustrate a molecular dynamics simulation we study a set of N D 100 particles
</p>
<p>of mass m D 1which are subject to a LENNARD-JONES potential (7.4) characterized
by � D � D 1 and to a gravitational force mg, g D 9:81. At initialization the
particles are placed in a 10 � 10 lattice starting with the lower left hand edge at
x D 10:5 and y D 10. The particles are equally spaced with 
x D 
y D �.
This initial configuration is shown in Fig. 7.3a. Furthermore, the left hand side, the
right hand side, and the bottom of the confinement .L D 30/ are described by
reflecting boundary conditions, Eqs. (7.24) and (7.25). The confinement is open at
the top, i.e. it extends to infinity. The time step is given by
t D 10�3. Figure 7.3b&ndash;
d demonstrate how the system developed after 1200, 1800, and 3000 time steps,
respectively.
</p>
<p>This chapter closes our discussion of the numerics of initial value problems. In
the following chapters we will introduce some of the basic concepts developed to
solve boundary value problems with numerical methods.
</p>
<p>Summary
</p>
<p>This chapter dealt with the classical dynamics of many particles (not neces-
sarily identical particles) which are confined in a box of finite dimension or
which are allowed to roam freely in infinite space. The particles are subject
to a particle-particle interaction and to an external force. The discussion was
restricted to classical molecular dynamics. From NEWTON&rsquo;s equations of motion
for N interacting particles numerical methods were developed which allowed the
simulation of the particles&rsquo; dynamics. Based on the symplectic EULER method the
ST&Ouml;RMER-VERLET algorithm was derived. Another approach was based on the
central rectangular rule and resulted in the leap-frog algorithm. Finally, the velocity
VERLET algorithm was introduced. All three methods do have their merits. The
first gives very accurate results for the particles&rsquo; positions but calculates inaccurate</p>
<p/>
</div>
<div class="page"><p/>
<p>114 7 Molecular Dynamics
</p>
<p>Fig. 7.3 (a) Initial configuration: the particles are placed in a 10�10 equally spaced lattice starting
with x D 10:5 and y D 10:0; g D 9:81. The initial velocities are equal to zero. (b) Configuration
after 1200 time steps. (c) Configuration after 1800 time steps. (d) Configuration after 3000 time
steps
</p>
<p>velocities. It has the advantage that it is time reversible. The other two methods
lack this property but give very accurate estimates of the particles&rsquo; velocities. The
final part of this chapter was dedicated to the discussion of various subtleties of
the numerical implementation of these algorithms as there were: (i) definition of
boundary conditions, (ii) initialization of the algorithm, (iii) equilibration to a given
temperature, (iv) ensuring constant temperature throughout the simulation, and (v)
transformation to rescaled variables.
</p>
<p>Problems
</p>
<p>1. We investigate the pendulum of Chap. 1 and write its equation of motion as
</p>
<p>Rx C !2x D 0 ;
</p>
<p>with ! D
p
</p>
<p>g=`. The ST&Ouml;RMER-VERLET algorithm is applied to simulate the
pendulum&rsquo;s motion and to compare the numerical results with the exact solution.</p>
<p/>
</div>
<div class="page"><p/>
<p>Problems 115
</p>
<p>Demonstrate that the result is very sensitive to the choice of the time step
t and,
in particular, of the product !
t. Note that in this particular case the ST&Ouml;RMER-
VERLET algorithm can also be studied analytically!What happens for the choice
!
t D 1 or !
t � 2? Which conclusions can be drawn from this example for a
proper choice of the time discretization?
</p>
<p>Try the other two methods to simulate the pendulum&rsquo;s dynamics.
2. Write a molecular dynamics code with the help of the following instructions. You
</p>
<p>can use either the leap-frog or the velocity VERLET algorithm. We consider the
following system:
</p>
<p>&bull; There are N D 100 particles in a two-dimensional box with side length
L D 30. The boundaries at the bottom, at the left- and at right-hand side
are considered as reflecting, as in Fig. 7.2. The top of the box is regarded as
open (no periodic boundary condition or reflecting boundary is imposed).
</p>
<p>&bull; The particles interact through a LENNARD-JONES potential of the form (7.4)
where � and � define the interaction.
</p>
<p>&bull; Furthermore, a gravitational force Fext D �mgey acts on each particle, where
m is the particle&rsquo;s mass, g is the acceleration due to gravity, and ey denotes the
unit vector in y-direction.
</p>
<p>&bull; As an initial condition, the particles can be placed within the box on a regular
lattice, where the distance between the particles is the characteristic distance
according to the LENNARD-JONES potential, i.e. �. The form and position of
this lattice is arbitrary. This is illustrated in Fig. 7.3.
</p>
<p>We measure the velocities and the positions of all particles. Since the particle&rsquo;s
velocities and positions are to be analyzed with the help of an extra program, the
data are written to external files (it is not necessary to save all time steps!).
</p>
<p>Perform the following analysis:
</p>
<p>&bull; Determine the temperature T from the kinetic energy as discussed in in this
chapter. Note that in this particular case we do not demand that vtot D 0!
</p>
<p>&bull; Try different initial conditions. For instance, set the initial velocity equal to
zero and stack the particles in different geometric configurations (rectangle,
triangle, . . . , one can also use more than one configurations at the same time!).
The nearest neighbor distance between the particles can be set equal to �.
Choose one configuration and place it at different positions in the box. What
happens?
</p>
<p>&bull; Set � D � D m D 1 (we change the units) and set in the initial condition to
the inter-atomic distance of 2
</p>
<p>1
6 �. (Why?) Vary the gravitational acceleration g
</p>
<p>(different systems of units) in order to simulate different states of matter. The
reference program developed solid behavior for g � 0, liquid behavior for
g � 0:1 and gaseous behavior for g &gt; 1. Explain this behavior!
</p>
<p>&bull; Measure the particle density �.h/ as a function of the height h. You should be
able to reproduce the barometric formula:
</p>
<p>� / �0 expf�&#13;h=Tg; &#13; &gt; 0 :</p>
<p/>
</div>
<div class="page"><p/>
<p>116 7 Molecular Dynamics
</p>
<p>&bull; Determine the momentum distribution (pi D mvi) of the particles and
demonstrate that it follows a MAXWELL-BOLTZMANN distribution
</p>
<p>p.jvj/ / jvj2 expf�&#13; jvj2=Tg ; &#13; &gt; 0 ;
</p>
<p>with jvj D
q
</p>
<p>v2x C v2y , the Euclidean norm.
&bull; Illustrate the results of the simulation graphically.
</p>
<p>References
</p>
<p>1. Mandl, F.: Statistical Physics, 2nd edn. Wiley, New York (1988)
2. Schwabl, F.: Statistical Mechanics. Advanced Texts in Physics. Springer, Berlin/Heidelberg
</p>
<p>(2006)
3. Halley, J.W.: Statistical Mechanics. Cambridge University Press, Cambridge (2006)
4. Hardy, R.J., Binek, C.: Thermodynamics and Statistical Mechanics: An Integrated Approach.
</p>
<p>Wiley, New York (2014)
5. Hoover, W.G.: Molecular Dynamics. Springer, Berlin/Heidelberg (1986)
6. Griebel, M., Knapek, S., Zumbesch, G.: Numerical Simulation in Molecular Dynamics. Texts
</p>
<p>in Computational Science and Engeneering, vol. 5. Springer, Berlin/Heidelberg (2007)
7. Marx, D., Hutter, J.: Ab Initio Molecular Dynamics. Cambridge University Press, Cambridge
</p>
<p>(2012)
8. Gatti, F. (ed.): Molecular Quantum Dynamics. Springer, Berlin/Heidelberg (2014)
9. Jones, J.E.: On the determination of molecular fields. II. From the equation of state of a gas.
</p>
<p>Proc. R. Soc. A 106, 463&ndash;477 (1924). doi:10.1098/rspa.1924.0082
10. Hairer, E., Lubich, C., Wanner, G.: Geometric numerical integration illustrated by the St&ouml;rmer-
</p>
<p>Verlet method. Acta Numer. 12, 399&ndash;450 (2003). doi:10.1017/S0962492902000144</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 8
Numerics of Ordinary Differential Equations:
Boundary Value Problems
</p>
<p>8.1 Introduction
</p>
<p>It is the aim of this chapter to introduce some of the basics methods developed to
solve boundary value problems. Since a treatment of all available concepts is far
too extensive, we will concentrate on two approaches, namely the finite difference
approach and shooting methods [1&ndash;5]. Furthermore, we will strictly focus on linear
boundary value problems defined on a finite interval Œa; b&#141; � R. A boundary value
problem is referred to as linear if both the differential equation and the boundary
conditions are linear. Such a problem of order n is of the form
</p>
<p>(
</p>
<p>LŒy&#141; D f .x/; x 2 Œa; b&#141; ;
U� Œy&#141; D ��; � D 1; : : : ; n :
</p>
<p>(8.1)
</p>
<p>Here, LŒy&#141; is a linear operator
</p>
<p>LŒy&#141; D
nX
</p>
<p>kD0
ak.x/y
</p>
<p>.k/.x/ ; (8.2)
</p>
<p>where y.k/.x/ denotes the k-th spatial derivative of y.x/, i.e. y.k/ � dky.x/=dxk and
f .x/ as well as the ak.x/ are given functions which we assume to be continuous.
Accordingly, linear boundary conditions U� Œy&#141; can be formulated as
</p>
<p>U� Œy&#141; D
n�1
X
</p>
<p>kD0
</p>
<p>�
</p>
<p>˛�ky
.k/.a/C ˇ�ky.k/.b/
</p>
<p>�
</p>
<p>D �� ; (8.3)
</p>
<p>&copy; Springer International Publishing Switzerland 2016
B.A. Stickler, E. Schachinger, Basic Concepts in Computational Physics,
DOI 10.1007/978-3-319-27265-8_8
</p>
<p>117</p>
<p/>
</div>
<div class="page"><p/>
<p>118 8 Numerics of Ordinary Differential Equations: Boundary Value Problems
</p>
<p>where the ˛�k, ˇ�k and �� are given constants. The question in which cases a solution
to the boundary value problem (8.1) exists and whether or not this solution will be
unique [6], will not be discussed here.
</p>
<p>Let us introduce some further conventions: The differential equation in the first
line of Eq. (8.1) is referred to as homogeneous if the function f .x/ D 0 for all
x 2 Œa; b&#141;. In analogy, the boundary conditions are referred to as homogeneous if the
constants �� D 0 for all � D 1; : : : ; n. Finally, the boundary value problem (8.1)
is referred to as homogeneous if the differential equation is homogeneous and the
boundary conditions are homogeneous as well. In all other cases it is referred to as
inhomogeneous. Moreover, the boundary conditions are said to be decoupled if the
function values at the two different boundaries do not mix.
</p>
<p>One of the most important types of boundary value problems in physics are linear
second order boundary value problems with decoupled boundary conditions. They
are of the form:
</p>
<p>a2.x/y
00.x/C a1.x/y0.x/C a0.x/y.x/ D f .x/ ; x 2 Œa; b&#141; ; (8.4a)
</p>
<p>˛0y.a/C ˛1y0.a/ D �1; j˛0j C j˛1j &curren; 0 ; (8.4b)
ˇ0y.b/C ˇ1y0.b/ D �2; jˇ0j C jˇ1j &curren; 0 : (8.4c)
</p>
<p>This chapter focuses mainly on problems of this kind.
In particular, for second order differential equations, boundary conditions of the
</p>
<p>form
</p>
<p>y.a/ D ˛ ; y.b/ D ˇ ; (8.5)
</p>
<p>are referred to as boundary conditions of the first kind or DIRICHLET boundary
conditions. On the other hand, boundary conditions of the form
</p>
<p>y0.a/ D ˛ ; y0.b/ D ˇ ; (8.6)
</p>
<p>are referred to as boundary conditions of the second kind or NEUMANN boundary
conditions and boundary conditions of the form (8.4) are referred to as boundary
conditions of the third kind or STURM boundary conditions.
</p>
<p>We note, that the particular case of decoupled boundary conditions does not
include problems like
</p>
<p>y.a/ D y.b/ &curren; 0 : (8.7)
</p>
<p>We encountered such a condition in Sect. 7.3 where we introduced boundary
conditions of this form as periodic boundary conditions.
</p>
<p>In the following section the method of finite differences will be applied to solve
boundary value problems of the form (8.4). On the other hand, shooting methods, in
particular the method developed by NUMEROV (see, for instance, [7] and references
therein), will be the topic of the third section.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.2 Finite Difference Approach 119
</p>
<p>A common alternative in the case of constant coefficients is to solve the
differential equation with the help of FOURIER transform techniques. A brief
introduction to the numerical implementation of the FOURIER transform is given
in Appendix D.
</p>
<p>8.2 Finite Difference Approach
</p>
<p>For illustrative purposes, we regard a boundary value problem of the form (8.4). The
extension to more complex problems might be tedious but follows the same line of
arguments. We discretize the interval Œa; b&#141; according to the recipe introduced in
Chap. 2: the positions xk are given by xk D a C .k � 1/h, where the grid-spacing h
is determined via the maximum number of grid-points N as h D .b � a/=.N � 1/.
Hence, we have x1 D a and xN D b. Furthermore, we use the notation yk � y.xk/
for all k D 1; : : : ;N. It will be used for all functions which appear in Eqs. (8.4).
</p>
<p>Let us now employ the central difference derivative (2.10c) in order to approxi-
mate
</p>
<p>y00k � y00.xk/ �
ykC1 � 2yk C yk�1
</p>
<p>h2
; (8.8)
</p>
<p>for k D 2; : : : ;N � 1 and
</p>
<p>y0k � y0.xk/ �
ykC1 � yk�1
</p>
<p>2h
: (8.9)
</p>
<p>The boundary points x1 and xN will be treated in a separate step. In order to
abbreviate the notation we will rewrite the differential equation (8.4) without the
indices as
</p>
<p>a.x/y00.x/C b.x/y0.x/C c.x/y.x/ D f .x/ : (8.10)
</p>
<p>Equations (8.8) and (8.9) are then applied and we arrive at the difference equation
</p>
<p>ak
ykC1 � 2yk C yk�1
</p>
<p>h2
C bk
</p>
<p>ykC1 � yk�1
2h
</p>
<p>C ckyk D fk ; (8.11)
</p>
<p>where k D 2; : : : ;N � 1. Sorting the yk yields
�
</p>
<p>ak
</p>
<p>h2
� bk
2h
</p>
<p>�
</p>
<p>yk�1 C
�
</p>
<p>ck �
2ak
</p>
<p>h2
</p>
<p>�
</p>
<p>yk C
�
</p>
<p>ak
</p>
<p>h2
C bk
2h
</p>
<p>�
</p>
<p>ykC1 D fk ; (8.12)
</p>
<p>and this equation is only valid for k D 2; : : : ;N�1 because we definedN grid-points
within the interval Œa; b&#141;.</p>
<p/>
</div>
<div class="page"><p/>
<p>120 8 Numerics of Ordinary Differential Equations: Boundary Value Problems
</p>
<p>A final step is necessary in which the boundary conditions are incorporated. This
will then enable us to reduce the whole problem to a system of linear equations.
Decoupled boundary conditions of a second order differential equation for the left-
hand boundary (8.4b) are of the general form:
</p>
<p>˛0y.a/C ˛1y0.a/ D �1; j˛0j; j˛1j &curren; 0 : (8.13)
</p>
<p>In analogy, we find for the right-hand boundary (8.4c):
</p>
<p>ˇ0y.b/C ˇ1y0.b/ D �2; jˇ0j; jˇ1j &curren; 0 : (8.14)
</p>
<p>We discretize y0.a/ as
</p>
<p>y01 � y0.a/ �
y2 � y0
2h
</p>
<p>; (8.15)
</p>
<p>and set y1 D y.a/. Note that the function value y0 in Eq. (8.15) is unknown since
the virtual point x0 D a � h is not within our interval Œa; b&#141;. Nevertheless, we use
Eq. (8.15) in Eq. (8.13) and obtain:
</p>
<p>˛0y1 C ˛1
y2 � y0
2h
</p>
<p>D �1 : (8.16)
</p>
<p>We solve now Eq. (8.16) for y0 under the premise that ˛1 &curren; 0,
</p>
<p>y0 D y2 �
2h
</p>
<p>˛1
.�1 � ˛0y1/ ; (8.17)
</p>
<p>rewrite Eq. (8.12) for k D 1,
�
</p>
<p>a1
</p>
<p>h2
� b1
2h
</p>
<p>�
</p>
<p>y0 C
�
</p>
<p>c1 �
2a1
</p>
<p>h2
</p>
<p>�
</p>
<p>y1 C
�
</p>
<p>a1
</p>
<p>h2
C b1
2h
</p>
<p>�
</p>
<p>y2 D f1 ; (8.18)
</p>
<p>and insert (8.17) into (8.18):
</p>
<p>�
</p>
<p>c1 �
2a1
</p>
<p>h2
C ˛0
˛1
</p>
<p>�
2a1
</p>
<p>h
� b1
</p>
<p>��
</p>
<p>y1 C
2a1
</p>
<p>h2
y2 D f1 �
</p>
<p>�1
</p>
<p>˛1
</p>
<p>�
</p>
<p>b1 �
2a1
</p>
<p>h
</p>
<p>�
</p>
<p>: (8.19)
</p>
<p>On the other hand, in the specific case of ˛1 D 0 we immediately obtain from
Eq. (8.16):
</p>
<p>y1 D
�1
</p>
<p>˛0
: (8.20)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.2 Finite Difference Approach 121
</p>
<p>The same strategy can be applied to incorporate the right-hand side boundary
condition, Eq. (8.14): We discretize Eq. (8.14) by introducing the function value
yNC1 at the virtual grid-point xNC1 D N C h outside the interval Œa; b&#141; via:
</p>
<p>ˇ0yN C ˇ1
yNC1 � yN�1
</p>
<p>2h
D �2 : (8.21)
</p>
<p>This equation is solved for yNC1 under the premise that ˇ1 &curren; 0
</p>
<p>yNC1 D yN�1 C
2h
</p>
<p>ˇ1
.�2 � ˇ0yN/ ; (8.22)
</p>
<p>and insert this into Eq. (8.12) for k D N. This results in:
</p>
<p>2aN
</p>
<p>h2
yN�1 C
</p>
<p>�
</p>
<p>cN �
2aN
</p>
<p>h2
� ˇ0
ˇ1
</p>
<p>�
</p>
<p>bN C
2aN
</p>
<p>h
</p>
<p>��
</p>
<p>yN D fN �
�2
</p>
<p>ˇ1
</p>
<p>�
</p>
<p>bN C
2aN
</p>
<p>h
</p>
<p>�
</p>
<p>:
</p>
<p>(8.23)
In the specific case ˇ1 D 0, the value yN is fixed at the boundary and one obtains
from Eq. (8.14):
</p>
<p>yN D
�2
</p>
<p>ˇ0
: (8.24)
</p>
<p>All these manipulations reduced the boundary value problem to a system of
inhomogeneous linear equations, namely Eqs. (8.12), (8.19), and (8.23). It can be
written as
</p>
<p>Ay D F ; (8.25)
</p>
<p>where we introduced the vector y D .y1; y2; : : : ; yN/T , the vector F
</p>
<p>F D
</p>
<p>0
</p>
<p>B
B
B
B
B
B
B
B
@
</p>
<p>f1 � �1˛1
�
</p>
<p>b1 � 2a1h
�
</p>
<p>f2
</p>
<p>f3
:::
</p>
<p>fN�1
fN � �2ˇ1
</p>
<p>�
</p>
<p>bN C 2aNh
�
</p>
<p>1
</p>
<p>C
C
C
C
C
C
C
C
A
</p>
<p>; (8.26)</p>
<p/>
</div>
<div class="page"><p/>
<p>122 8 Numerics of Ordinary Differential Equations: Boundary Value Problems
</p>
<p>and the tridiagonal matrix A:
</p>
<p>A D
</p>
<p>0
</p>
<p>B
B
B
B
B
B
B
B
B
@
</p>
<p>B1 C1 0 � � � 0
</p>
<p>A2 B2 C2 � � �
:::
</p>
<p>0
: : :
</p>
<p>: : :
: : :
</p>
<p>:::
: : :
</p>
<p>: : :
: : : 0
</p>
<p>AN�1 BN�1 CN�1
0 � � � 0 AN BN
</p>
<p>1
</p>
<p>C
C
C
C
C
C
C
C
C
A
</p>
<p>: (8.27)
</p>
<p>Here we defined
</p>
<p>Ak D
</p>
<p>8
</p>
<p>ˆ̂
&lt;
</p>
<p>ˆ̂
:
</p>
<p>�
ak
</p>
<p>h2
� bk
2h
</p>
<p>�
</p>
<p>k D 2; : : : ;N � 1 ;
</p>
<p>2aN
</p>
<p>h2
k D N ;
</p>
<p>(8.28)
</p>
<p>Bk D
</p>
<p>8
</p>
<p>ˆ̂
ˆ̂
ˆ̂
ˆ̂
&lt;
</p>
<p>ˆ̂
ˆ̂
ˆ̂
ˆ̂
:
</p>
<p>�
</p>
<p>c1 �
2a1
</p>
<p>h2
C ˛0
˛1
</p>
<p>�
2a1
</p>
<p>h
� b1
</p>
<p>��
</p>
<p>k D 1 ;
�
</p>
<p>ck �
2ak
</p>
<p>h2
</p>
<p>�
</p>
<p>k D 2; : : :N � 1 ;
�
</p>
<p>cN �
2aN
</p>
<p>h2
� ˇ0
ˇ1
</p>
<p>�
</p>
<p>bN C
2aN
</p>
<p>h
</p>
<p>��
</p>
<p>k D N ;
</p>
<p>(8.29)
</p>
<p>and, finally,
</p>
<p>Ck D
</p>
<p>8
</p>
<p>ˆ̂
&lt;
</p>
<p>ˆ̂
:
</p>
<p>2a1
</p>
<p>h2
k D 1 ;
</p>
<p>�
ak
</p>
<p>h2
C bk
2h
</p>
<p>�
</p>
<p>k D 2; : : : ;N � 1 :
(8.30)
</p>
<p>The remaining task is now to solve this linear system of equations (8.25). (A
brief introduction to the numerical treatment of linear systems of equations can
be found in Appendix C.) Very effective methods exist for cases where the matrix
A is tridiagonal [8] as it is the case here. Although we discussed the method of
finite differences for the particular case of a second order differential equation with
decoupled boundary conditions, the same strategy can be employed to derive similar
methods for higher order boundary value problems. However, these methods will, in
general, be more complex. Furthermore, we note that in cases where ˛1 D ˇ1 D 0
the function values at the boundaries y1 and yN are fixed and the corresponding
system of linear equations reduces to .N � 2/ equations.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.2 Finite Difference Approach 123
</p>
<p>Let us briefly investigate the differential equation which corresponds to the
problem (8.4) together with periodic boundary conditions of the form (8.7). In this
case we have to consider that
</p>
<p>y1 D yN ; (8.31)
</p>
<p>and, for a solution to exist, we have necessarily
</p>
<p>a1 D aN ; b1 D bN ; and c1 D cN : (8.32)
</p>
<p>The finite difference approximations (8.8) and (8.9) are again applied to derive
Eqs. (8.12) for k D 2; : : : ;N � 1. For k D 2 Eq. (8.12) becomes
</p>
<p>�
a2
</p>
<p>h2
� b2
2h
</p>
<p>�
</p>
<p>y1 C
�
</p>
<p>c2 �
2a2
</p>
<p>h2
</p>
<p>�
</p>
<p>y2 C
�
</p>
<p>a2
</p>
<p>h2
C b2
2h
</p>
<p>�
</p>
<p>y3 D f2 ; (8.33)
</p>
<p>and we have for k D N � 1
�
</p>
<p>aN�1
h2
</p>
<p>� bN�1
2h
</p>
<p>�
</p>
<p>yN�2 C
�
</p>
<p>cN�1 �
2aN�1
</p>
<p>h2
</p>
<p>�
</p>
<p>yN�1 C
�
</p>
<p>aN�1
h2
</p>
<p>C bN�1
2h
</p>
<p>�
</p>
<p>yN D fN�1 :
(8.34)
</p>
<p>Since y1 D yN this can be rewritten as
�
</p>
<p>aN�1
h2
</p>
<p>� bN�1
2h
</p>
<p>�
</p>
<p>yN�2 C
�
</p>
<p>cN�1 �
2aN�1
</p>
<p>h2
</p>
<p>�
</p>
<p>yN�1 C
�
</p>
<p>aN�1
h2
</p>
<p>C bN�1
2h
</p>
<p>�
</p>
<p>y1 D fN�1 :
(8.35)
</p>
<p>Finally, Eq. (8.12) results for k D 1 in
�
</p>
<p>a1
</p>
<p>h2
� b1
2h
</p>
<p>�
</p>
<p>yN�1 C
�
</p>
<p>c1 �
2a1
</p>
<p>h2
</p>
<p>�
</p>
<p>y1 C
�
</p>
<p>a1
</p>
<p>h2
C b1
2h
</p>
<p>�
</p>
<p>y2 D f1 ; (8.36)
</p>
<p>where we identified y0 D y.x1 � h/ � y.xN � h/ D yN�1. All this results in a closed
system of N � 1 equations, which is of the form (8.25)
</p>
<p>Ay D F ; (8.37)
</p>
<p>where y D .y1; y2; : : : ; yN�1/T ,
</p>
<p>F D
</p>
<p>0
</p>
<p>B
B
B
@
</p>
<p>f1
</p>
<p>f2
:::
</p>
<p>fN�1
</p>
<p>1
</p>
<p>C
C
C
A
; (8.38)</p>
<p/>
</div>
<div class="page"><p/>
<p>124 8 Numerics of Ordinary Differential Equations: Boundary Value Problems
</p>
<p>and the .N � 1/ � .N � 1/ matrix A is given by
</p>
<p>A D
</p>
<p>0
</p>
<p>B
B
B
B
B
B
B
B
B
@
</p>
<p>B1 C1 0 � � � 0 A1
A2 B2 C2 � � � 0
</p>
<p>0
: : :
</p>
<p>: : :
: : :
</p>
<p>:::
:::
</p>
<p>: : :
: : :
</p>
<p>: : : 0
</p>
<p>0 AN�2 BN�2 CN�2
CN�1 0 � � � 0 AN�1 BN�1
</p>
<p>1
</p>
<p>C
C
C
C
C
C
C
C
C
A
</p>
<p>: (8.39)
</p>
<p>Here, we defined
</p>
<p>Ak D
�
</p>
<p>ak
</p>
<p>h2
� bk
2h
</p>
<p>�
</p>
<p>; k D 1; : : : ;N � 1 ; (8.40)
</p>
<p>Bk D
�
</p>
<p>ck �
2ak
</p>
<p>h2
</p>
<p>�
</p>
<p>; k D 1; : : : ;N � 1 ; (8.41)
</p>
<p>and
</p>
<p>Ck D
�
</p>
<p>ak
</p>
<p>h2
C bk
2h
</p>
<p>�
</p>
<p>; k D 1; : : : ;N � 1 : (8.42)
</p>
<p>In contrast to the matrix (8.27) the matrix (8.39) is not tridiagonal since the
matrix elements .A/1;N�1 and .A/N�1;1 are non-zero. Nevertheless, it was possible
to reduce the boundary value problem to a system of linear equations which can be
solved iteratively.
</p>
<p>8.3 Shooting Methods
</p>
<p>For illustrative purposes, we restrict here the discussion to a second order boundary
value problem with decoupled boundary conditions of the form (8.4). The essential
idea of shooting methods is to treat the boundary value problem as an initial value
problem. The resulting equations can then be solved with the help of methods
discussed in Chap. 5. Of course, such an approach is ill-defined because no initial
conditions but only boundary conditions are given. The trick is, that one modifies the
initial conditions iteratively in such a way that in the end the boundary conditions
are fulfilled. Let us put this train of thoughts into a mathematical form: We rewrite
the second order differential equation (8.4a) as
</p>
<p>y00 D f .y; y0; x/ ; (8.43)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.3 Shooting Methods 125
</p>
<p>which can be reduced to a set of first order differential equations as was demon-
strated in Chap. 5. We note that Eq. (8.43) is not yet well posed since the initial
conditions have not been defined. The boundary condition on the left-hand side
reads:
</p>
<p>˛0y.a/C ˛1y0.a/ D �1 : (8.44)
</p>
<p>We now assume that y0.a/ D z, where z is some number. This gives the well posed
initial value problem
</p>
<p>8
</p>
<p>ˆ̂
ˆ̂
&lt;
</p>
<p>ˆ̂
ˆ̂
:
</p>
<p>y00 D f .y; y0; x/ ;
</p>
<p>y.a/ D �1
˛0
</p>
<p>� ˛1
˛0
</p>
<p>z ;
</p>
<p>y0.a/ D z ;
</p>
<p>(8.45)
</p>
<p>under the assumption that ˛0 &curren; 0. The solution of this problem will be written as
y.xI z/ in order to indicate its dependence on the particular choice y0.a/ D z. We
remember, that the boundary condition at the right-hand boundary is defined as:
</p>
<p>ˇ0y.b/C ˇ1y0.b/ D �2 : (8.46)
</p>
<p>Let us introduce the function:
</p>
<p>F.z/ D ˇ0y.bI z/C ˇ1y0.bI z/ � �2 : (8.47)
</p>
<p>We observe that the solution of the equation
</p>
<p>F.z/ D 0 ; (8.48)
</p>
<p>gives the desired solution to the boundary value problem (8.4), because in this case
the second boundary condition (8.46) is fulfilled. In practice, one tries several values
of z until relation (8.48) is fulfilled. However, from a numerical point of view this
method is very inefficient since usually several initial value problems have to be
solved until the correct value of z is found. Nevertheless, in some cases shooting
methods proved to be very useful [7].
</p>
<p>For instance, shooting methods are particularly effective if a solution to an
eigenvalue problem of the form
</p>
<p>a.x/y00.x/C b.x/y0.x/C c.x/y.x/ D �y.x/ ; (8.49a)
</p>
<p>in combination with homogeneous boundary conditions,
</p>
<p>˛0y.a/C ˛1y0.a/ D 0 ; (8.49b)</p>
<p/>
</div>
<div class="page"><p/>
<p>126 8 Numerics of Ordinary Differential Equations: Boundary Value Problems
</p>
<p>and
</p>
<p>ˇ0y.b/C ˇ1y0.b/ D 0 ; (8.49c)
</p>
<p>is to be found. We note that Eq. (8.49a) has the trivial solution y.x/ � 0 for all
values of �. However a non-trivial solution will only exist for particular values of �.
These particular values will be indexed by �n and are referred to as eigenvalues of
Eq. (8.49a) [9]. The corresponding functions yn.x/ are referred to as eigenfunctions.
We note that the differential equation (8.49a) in combination with the boundary
conditions (8.49b) and (8.49c) define a homogeneousboundary value problem. Such
a problem is commonly referred to as an eigenvalue problem [9]. Furthermore, we
note the following property of homogeneous boundary value problems: Suppose
that y.x/ is a solution of the boundary value problem (8.49), then Qy.x/ D &#13;y.x/, with
&#13; D const will also be a solution of (8.49). Hence, the solution of a homogeneous
boundary value problem is not unique but invariant under multiplication by a
constant &#13; . Typically, the multiplicative factor &#13; is fixed by some additional
condition, such as a normalization condition of the form
</p>
<p>Z b
</p>
<p>a
</p>
<p>dxjy.x/j2 D 1 : (8.50)
</p>
<p>We now employ this property and choose y.a/ D 1. Inserting this choice
into (8.49b) yields
</p>
<p>y0.a/ D �˛0
˛1
: (8.51)
</p>
<p>Note that for ˛0 D 0 or ˛1 D 0, we are restricted to the choices y0.a/ D 0 and
y.a/ is arbitrary or y.a/ D 0 and y0.a/ is arbitrary, respectively. If we assume that
a.x/ &curren; 0 for all x 2 Œa; b&#141;, we can solve the initial value problem
</p>
<p>8
</p>
<p>ˆ̂
ˆ̂
</p>
<p>&lt;̂
</p>
<p>ˆ̂
ˆ̂
</p>
<p>:̂
</p>
<p>y00.x/ D �b.x/
a.x/
</p>
<p>y0.x/ � c.x/ � �
a.x/
</p>
<p>y.x/ ;
</p>
<p>y.a/ D 1 ;
</p>
<p>y0.a/ D �˛0
˛1
:
</p>
<p>(8.52)
</p>
<p>The solutions are denoted by y.xI�/ in order to emphasize that they will highly
depend on the choice of the parameter �. The strategy is to solve the initial value
problem (8.52) for several values of � and whenever one finds that
</p>
<p>F.�n/ D ˇ0y.bI�n/C ˇ1y0.bI�n/ � �n D 0 ; (8.53)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.3 Shooting Methods 127
</p>
<p>is satisfied, an eigenvalue �n with the corresponding eigenfunction yn.x/ D y.xI�n/
of the eigenvalue problem (8.49) has been found.
</p>
<p>However, this strategy is also very time consuming. The most common applica-
tion of the shooting method is its combination with a very fast and accurate solution
of initial value problems. This method is known as the NUMEROV method [7]. It is
applicable whenever one is confronted with a differential equation of the form
</p>
<p>y00.x/C k.x/y.x/ D 0 ; (8.54)
</p>
<p>in combinationwith homogeneous boundary conditions. Here k.x/ is some function.
If we are particularly interested in eigenvalue problems then k.x/ has the form
k.x/ D q.x/ � �, where q.x/ is some function and � is the eigenvalue [see the
discussion after Eq. (8.49)]. For instance, consider the one-dimensional stationary
SCHR&Ouml;DINGER equation [10&ndash;12],
</p>
<p> 00.x/C 2m&bdquo;2 ŒE � V.x/&#141;  .x/ D 0 ; (8.55)
</p>
<p>where  .x/ is the wave-function, m is the mass, &bdquo; denotes the reduced PLANCK
constant, E is the energy, and V.x/ is some potential. In this case we identify
</p>
<p>k.x/ D 2m&bdquo;2 ŒE � V.x/&#141; : (8.56)
</p>
<p>We note that Eq. (8.55) together with its boundary conditions defines an eigenvalue
problem with eigenvalues En, the possible energies of the system. We remember
from Chap. 2, Eq. (2.34), that
</p>
<p>y00j D
yjC1 � 2yj C yj�1
</p>
<p>h2
� h
</p>
<p>2
</p>
<p>12
y
.4/
j � : : : D �kjyj : (8.57)
</p>
<p>Here we made use of Eq. (8.54) and introduced kj � k.xj/. Furthermore, we write
the fourth derivative of y.x/ at point x D xj as
</p>
<p>y
.4/
j �
</p>
<p>y00jC1 � 2y00j C y00j�1
h2
</p>
<p>D �kjC1yjC1 C 2kjyj � kj�1yj�1
h2
</p>
<p>; (8.58)
</p>
<p>where we employed Eq. (8.54). Truncating (8.57) after the fourth order derivative
y
.4/
j , inserting relation (8.58), and solving for yjC1 yields
</p>
<p>yjC1 D
2
�
</p>
<p>1 � 5h2
12
</p>
<p>kj
</p>
<p>�
</p>
<p>yj �
�
</p>
<p>1C h2
12
</p>
<p>kj�1
�
</p>
<p>yj�1
</p>
<p>1C h2
12
</p>
<p>kjC1
: (8.59)
</p>
<p>This gives a very fast algorithm to solve the differential equation (8.54) with some
initial values of the form (8.52). The remaining strategy is the same as discussed</p>
<p/>
</div>
<div class="page"><p/>
<p>128 8 Numerics of Ordinary Differential Equations: Boundary Value Problems
</p>
<p>above, i.e. one screens the parameter � in order to find the eigenvalues �n and
eigenfunctions yn.x/. In case of the SCHR&Ouml;DINGER equation, one can screen the
energy E in order to obtain the energy eigenvalues En which satisfy a condition of
the form (8.53).
</p>
<p>Before we present two illustrating examples in the next two chapters let us
conclude this section with two important remarks on the NUMEROV method. We
note from Eq. (8.59) that in order to compute y3 one already needs the function
values y1 and y2. Usually, one obtains these values from the boundary conditions
in combination with some additional condition for the problem at hand. Such an
additional condition might be, for instance, the normalization of the function y.x/,
like Eq. (8.50). We also emphasized that one has to run the NUMEROV algorithm
several times for different trial values of the parameter �. In order to reduce the
computational cost of the method it is in many cases advantageous to store the
function values qi, where ki D qi � �, in an array which is then regarded as an
input argument of the NUMEROV algorithm.
</p>
<p>Summary
</p>
<p>We focused on linear boundary value problems defined on a finite interval Œa; b&#141; �
R. Most important for physics are second order boundary value problems with
decoupled boundary conditions, i.e. the boundary conditions at the two different
boundaries do not mix. The numerical treatment of the second order differential
equation together with its boundary conditions concentrated either on the applica-
tion of finite differences or on shooting methods. In the finite difference approach
the methods developed in Chap. 2 were applied and the boundary conditions were
incorporated directly. This resulted in a set of linear algebraic equations which
was to be solved for each grid-point of the discretisized interval Œa; b&#141;. The case
of periodic boundary conditions was also discussed in detail.
</p>
<p>The shooting methods, on the other hand, try to link the decoupled boundary
value problem to an initial value problem. This allowed the application of the
methods discussed in Chap. 5. The idea was to start with some initial value at one
of the two boundaries, solve the differential equation numerically and to modify
the initial value iteratively until it agreed with the original boundary condition
within some predefined error. Such a procedure is rather time consuming. Never-
theless, shooting methods, in particular its NUMEROV variation, proved to be very
useful in the numerical solution of eigenvalue problems. This was demonstrated
using the homogeneous boundary value problem of the one-dimensional stationary
SCHR&Ouml;DINGER equation as an example.</p>
<p/>
</div>
<div class="page"><p/>
<p>References 129
</p>
<p>Problems
</p>
<p>1. Solve the linear second order boundary value problem
</p>
<p>y00.x/C y.x/ D x;
</p>
<p>for x 2 Œ0; �=2&#141; with the boundary conditions y.0/ D 0 and y.�=2/ D 1
analytically and then numerically with the help of finite differences.
</p>
<p>2. Solve the linear, second order boundary value problem
</p>
<p>y00.x/ � 2 cos.2x/y.x/ D 0;
</p>
<p>on the interval Œ��=2; �=2&#141; and with the boundary conditions y.˙�=2/ D 1.
Use the finite difference method.
</p>
<p>Comment: The solution can be expressed analytically in terms of so-called
MATHIEU functions [13, 14] which might be intrinsically available from your
computing environment. If this happens to be the case, it might be a good idea to
compare the numerical solution with the analytical result.
</p>
<p>References
</p>
<p>1. Collatz, L.: The Numerical Treatment of Differential Equations. Springer, Berlin/Heidelberg
(1960)
</p>
<p>2. Lapidus, L., Pinder, G.F.: Numerical Solution of Partial Differential Equations. Wiley,
New York (1982)
</p>
<p>3. Stoer, J., Bulirsch, R.: Introduction to Numerical Analysis, 2nd edn. Springer,
Berlin/Heidelberg (1993)
</p>
<p>4. Asher, U.M., Mattheij, R.M.M., Russell, R.D.: Numerical Solution of Boundary Value
Problems for Ordinary Differential Eqautions. Classics in Applied Mathematics, vol. 13.
Cambridge University Press, Cambridge (1995)
</p>
<p>5. Powers, D.L.: Boundary Value Problems, 6th edn. Academic, San Diego (2009)
6. Polyanin, A.D., Zaitsev, V.F.: Boundary Value Problems, 2nd edn. Chapman &amp;Hall/CRC, Boca
</p>
<p>Raton (2003)
7. Hairer, E., N&oslash;rsett, S.P., Wanner, G.: Solving Ordinary Differential Equations I, 2nd edn.
</p>
<p>Springer Series in Computational Mathematics, vol. 8. Springer, Berlin/Heidelberg (1993)
8. Press, W.H., Teukolsky, S.A., Vetterling, W.T., Flannery, B.P.: Numerical Recipes in C++, 2nd
</p>
<p>edn. Cambridge University Press, Cambridge (2002)
9. Courant, R., Hilbert, D.: Methods of Mathematical Physics, vol. 1. Wiley, New York (1989)
10. Baym, G.: Lectures on Quantum Mechanics. Lecture Notes and Supplements in Physics. The
</p>
<p>Benjamin/Cummings, London/Amsterdam (1969)
11. Cohen-Tannoudji, C., Diu, B., Lalo&euml;, F.: Quantum Mechanics, vol. I. Wiley, New York (1977)
12. Sakurai, J.J.: Modern Quantum Mechanics. Addison-Wesley, Menlo Park (1985)
13. Abramovitz, M., Stegun, I.A. (eds.): Handbook of Mathemathical Functions. Dover, New York
</p>
<p>(1965)
14. Olver, F.W.J., Lozier, D.W., Boisvert, R.F., Clark, C.W.: NIST Handbook of Mathematical
</p>
<p>Functions. Cambridge University Press, Cambridge (2010)</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 9
The One-Dimensional Stationary Heat Equation
</p>
<p>9.1 Introduction
</p>
<p>This is the first of two chapters which illustrate the applicability of the methods
introduced in Chap. 8. Within this chapter the finite difference approach is employed
to solve the stationary heat equation. Let us motivate briefly this particular problem.
We consider a rod of length L which is supposed to be kept at constant temperatures
T0 and TN at its ends as illustrated in Fig. 9.1. The homogeneous heat equation is a
linear partial differential equation of the form
</p>
<p>@
</p>
<p>@t
T.x; t/ D �
T.x; t/ : (9.1)
</p>
<p>Here T.x; t/ is the temperature as a function of space x 2 R3 and time t 2 R,

 D r2 D @2x C @2y C @2z is the LAPLACE operator, and � D const is the thermal
diffusivity.
</p>
<p>We remark, that Eq. (9.1) is a partial differential equation together with initial
and boundary conditions. Moreover, we note in passing that the heat equation is
equivalent to the diffusion equation [1]
</p>
<p>@
</p>
<p>@t
�.x; t/ D D
�.x; t/ ; (9.2)
</p>
<p>with particle density �.x; t/ and the diffusion coefficient D D const. Here we restrict
ourselves to a simplified situation in order to test the validity of the finite difference
approach discussed in Sect. 8.2. The general solution of the heat or diffusion
equation will be discussed in Sect. 11.3. (The problem of the one-dimensional heat
equation was studied in all conceivable detail by J. R. CANNON [2].)
</p>
<p>If we assume that the cylindrical surface of the rod is perfectly isolated, we can
restrict the problem to a one-dimensional problem. Furthermore, we assume that the
</p>
<p>&copy; Springer International Publishing Switzerland 2016
B.A. Stickler, E. Schachinger, Basic Concepts in Computational Physics,
DOI 10.1007/978-3-319-27265-8_9
</p>
<p>131</p>
<p/>
</div>
<div class="page"><p/>
<p>132 9 The One-Dimensional Stationary Heat Equation
</p>
<p>Fig. 9.1 We consider a rod
of length L. Its ends are kept
at constant temperatures T0
and TN , respectively
</p>
<p>steady-state has been reached, i.e. @
@t
</p>
<p>T.x; t/ D 0. Hence, the remaining boundary
value problem is of the form
</p>
<p>8
</p>
<p>ˆ̂
ˆ̂
&lt;
</p>
<p>ˆ̂
ˆ̂
:
</p>
<p>d2
</p>
<p>dx2
T.x/ D 0; x 2 Œ0;L&#141; ;
</p>
<p>T.0/ D T0 ;
T.L/ D TN :
</p>
<p>(9.3)
</p>
<p>The solution can easily be found analytically and one obtains
</p>
<p>T.x/ D T0 C .TN � T0/
x
</p>
<p>L
: (9.4)
</p>
<p>In the following section we will apply the approach of finite differences to the
boundary value problem (9.3) as discussed in Sect. 8.2.
</p>
<p>9.2 Finite Differences
</p>
<p>We discretize the interval Œ0;L&#141; according Chap. 2 by the introduction of NC1 grid-
points xn D nh, with h D L=N, x0 D 0, and xN D L. Furthermore, Tn � T.xn/ and,
in particular, we refer to the boundary conditions (9.3) as T0 and TN , respectively.
</p>
<p>On the basis of this discretization, we approximate Eq. (9.3) by
</p>
<p>TnC1 � 2Tn C Tn�1
h2
</p>
<p>D 0 ; (9.5)
</p>
<p>or equivalently
</p>
<p>TnC1 � 2Tn C Tn�1 D 0 : (9.6)
</p>
<p>We can rewrite this as a matrix equation,
</p>
<p>AT D F ; (9.7)</p>
<p/>
</div>
<div class="page"><p/>
<p>9.2 Finite Differences 133
</p>
<p>where the boundary conditions have already been included. In Eq. (9.7) the vector
T D .T1;T2; : : : ;TN�1/T . Furthermore, the tridiagonal matrix A is given by
</p>
<p>A D
</p>
<p>0
</p>
<p>B
B
B
B
B
@
</p>
<p>�2 1 0 : : : 0
1 �2 1 0 : : : 0
0 1 �2 1
:::
</p>
<p>: : :
: : :
</p>
<p>: : :
</p>
<p>0 : : : 1 �2
</p>
<p>1
</p>
<p>C
C
C
C
C
A
</p>
<p>; (9.8)
</p>
<p>and the vector F by
</p>
<p>F D
</p>
<p>0
</p>
<p>B
B
B
B
B
@
</p>
<p>�T0
0
:::
</p>
<p>0
</p>
<p>�TN
</p>
<p>1
</p>
<p>C
C
C
C
C
A
</p>
<p>: (9.9)
</p>
<p>It is an easy task to solve Eq. (9.7) analytically. It follows from Eq. (9.6) that
</p>
<p>TnC1 D 2Tn � Tn�1; n D 1; : : : ;N � 1 : (9.10)
</p>
<p>We insert n D 1; 2; 3 in order to obtain
</p>
<p>T2 D 2T1 � T0 ; (9.11)
T3 D 2T2 � T1 ;
</p>
<p>D 3T1 � 2T0 ; (9.12)
T4 D 2T3 � T2 ;
</p>
<p>D 4T1 � 3T0 : (9.13)
</p>
<p>We recognize the pattern and conclude that Tn has the general form
</p>
<p>Tn D nT1 � .n � 1/T0 ; (9.14)
</p>
<p>which we prove by complete induction:
</p>
<p>TnC1 D 2Tn � Tn�1
D 2ŒnT1 � .n � 1/T0&#141;� Œ.n � 1/T1 � .n � 2/T0&#141;
D .n C 1/T1 � nT0 : (9.15)</p>
<p/>
</div>
<div class="page"><p/>
<p>134 9 The One-Dimensional Stationary Heat Equation
</p>
<p>Hence, expression (9.14) is valid for all n D 1; : : : ;N. However, since TN is kept
constant according to the boundary condition, we can determine T1 from
</p>
<p>TN D NT1 � NT0 C T0 ; (9.16)
</p>
<p>which yields
</p>
<p>T1 D
TN � T0
</p>
<p>N
C T0 : (9.17)
</p>
<p>Inserting (9.17) into (9.14) gives
</p>
<p>Tn D T0 C .TN � T0/
n
</p>
<p>N
</p>
<p>D T0 C .TN � T0/
nh
</p>
<p>L
; (9.18)
</p>
<p>which is exactly the discretized version of Eq. (9.4). Hence the finite difference
approach to the boundary value problem (9.3) is exact and independent of the
grid-spacing h. This is not surprising since we proved already in Chap. 2 that finite
difference derivatives are exact for linear functions.
</p>
<p>9.3 A Second Scenario
</p>
<p>We consider the inhomogeneous heat equation
</p>
<p>@
</p>
<p>@t
T.x; t/ D �
T.x; t/ � � .x; t/ : (9.19)
</p>
<p>Here � .x; t/ � � .x/ is some heat source or heat drain, which is assumed to be
independent of time t. Again, we consider the one dimensional, stationary case, i.e.
</p>
<p>d2
</p>
<p>dx2
T.x/ D 1
</p>
<p>�
� .x/ ; (9.20)
</p>
<p>with the same boundary conditions as in Eq. (9.4). Furthermore, we assume � .x/ to
be of the form
</p>
<p>� .x/ D �
`
exp
</p>
<p>"
</p>
<p>�
�
</p>
<p>x � L
2
</p>
<p>�2
</p>
<p>`2
</p>
<p>#
</p>
<p>; (9.21)
</p>
<p>i.e. � .x/ has the form of a GAUSS peak which is centered at x D L=2 and has a width
determined by the parameter ` and a maximum height given by the constant�. Such</p>
<p/>
</div>
<div class="page"><p/>
<p>9.3 A Second Scenario 135
</p>
<p>a situation might occur, for instance, when the rod is heated with some kind of a heat
gun or cooled by a cold spot. (In cases where the diffusion equation (9.2) is used
to describe the random motion of electrons in some device, one can imagine, that
the density of electrons � is constant at the contacts at x D 0 and x D L. The
source/drain term � .x/ then accounts for a constant generation or recombination
rate of electrons, for instance, through incoming light or intrinsic traps, respectively
[3].)
</p>
<p>Furthermore, we note that in the limiting case `! 0 we have
</p>
<p>lim
`!0
</p>
<p>� .x/ / �ı
�
</p>
<p>x � L
2
</p>
<p>�
</p>
<p>; (9.22)
</p>
<p>where ı.�/ is the DIRAC ı-distribution; in this case the spatial extension of the
source/drain term � .x/ is infinitesimal.
</p>
<p>We now employ the results of Sect. 8.2 and rewrite the system of equations in the
familiar form1
</p>
<p>AT D F ; (9.23)
</p>
<p>where A has already been defined in Eq. (9.8), T D .T1;T2; : : : ;TN�1/T , and F is
given by
</p>
<p>F D h
2
</p>
<p>�
</p>
<p>0
</p>
<p>B
B
B
B
B
B
@
</p>
<p>�1 � �h2 T0
�2
:::
</p>
<p>�N�2
�N�1 � �h2 TN
</p>
<p>1
</p>
<p>C
C
C
C
C
C
A
</p>
<p>: (9.24)
</p>
<p>Here we used the notation �n � � .xn/.
The system is solved numerically quite easily using methods discussed by PRESS
</p>
<p>et al. [4] for the solution of sets of algebraic equations of the kind (9.24) with
tridiagonal matrix A. We chose L D 10, � D 1, � D �0:4, ` D 1, T0 D 0
and TN D 2. The resulting temperature profiles T.x/ (solid line) for different values
of N can be found in Figs. 9.2, 9.3, and 9.4 as well as the respective form of the
function � .x/ (dashed line). With increasing number of steps we see, as it was to be
expected, a refinement of the temperature profile. Its maximum does not quite agree
with the minimum of � .x/, it is shifted slightly towards the end of the rod because
of the boundary conditions, i.e. T0 &lt; TN .
</p>
<p>1We note that Eq. (9.20) can also be solved with the help of FOURIER transforms, see Appendix D.</p>
<p/>
</div>
<div class="page"><p/>
<p>136 9 The One-Dimensional Stationary Heat Equation
</p>
<p>Fig. 9.2 Temperature profile
T.x/ (solid line, left hand
scale) and the source function
� .x/ (dashed line, right hand
scale) for N D 5
</p>
<p>Fig. 9.3 Temperature profile
T.x/ (solid line, left hand
scale) and the source function
� .x/ (dashed line, right hand
scale) for N D 10
</p>
<p>Fig. 9.4 Temperature profile
T.x/ (solid line, left hand
scale) and the source function
� .x/ (dashed line, right hand
scale) for N D 100</p>
<p/>
</div>
<div class="page"><p/>
<p>9.3 A Second Scenario 137
</p>
<p>Summary
</p>
<p>The methods of Sect. 8.2 were applied to find the numerical solution of the
stationary heat equation with DIRICHLET boundary conditions. We studied the
particular case of an isolated rod of length L. This reduced the dimensionality of the
differential equation to one. The length of the rod was then divided into N discrete
grid-points. Using finite differences the one-dimensional ordinary second order
differential equation which described this particular problem was transformed into
a set of linear algebraic equations which determined the temperatures at each grid-
point. This set of algebraic equations was characterized by a tridiagonal coefficient
matrix. Solutions have been studied without and with a heat source which was
described as a &lsquo;point&rsquo; source characterized by a Gaussian of given width and
amplitude. In the first case analytic solutions were easily derived. They described a
linear temperature profile increasing (decreasing) from T0 to TN . In the latter case
solutions were generated numerically using specific algorithms designed for sets of
algebraic equations with a tridiagonal coefficient matrix A.
</p>
<p>Problems
</p>
<p>1. Calculate the stationary temperature profile across the cylindrical rod of Fig. 9.1
which is exposed to a heat sink centered around x D L=2. This heat sink is
described by a function � .x/ which is of rectangular shape of width a and depth
� . Both ends of the rod are kept at constant temperatures T0 and TN , respectively.
</p>
<p>2. Investigate the three cases T0 &gt; TN , T0 &lt; TN , T0 D TN &gt; 0, and study the
influence of the width a of the heat sink on the temperature profile.
</p>
<p>3. Consider the one-dimensional drift-diffusion equation [5]
</p>
<p>@
</p>
<p>@t
�.x; t/ D �D1
</p>
<p>@
</p>
<p>@x
�.x; t/C D2
</p>
<p>@2
</p>
<p>@x2
�.x; t/C � .x; t/
</p>
<p>where D1 is the drift constant and D2 the diffusion constant. For instance, if
we want to model the electron density in an electronic device, the drift constant
would be in the simplest case D1 D �E, where � is the charge carrier mobility
and E is the x-component of the electric field.
</p>
<p>Discretize the above equation for the stationary case and solve it numerically
for different values of D1. The boundary conditions are �.0/ &gt; 0 and �.L/ &gt;
�.0/, where L denotes the length of the device. Solve it numerically and analyt-
ically for � .x/ D 0 and compare the results. Investigate also a scenario with a
generation rate � given by an exponential function like � .x/ D �0 exp .��x/
with � &gt; 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>138 9 The One-Dimensional Stationary Heat Equation
</p>
<p>References
</p>
<p>1. Cussler, E.L.: Diffusion. Cambridge University Press, Cambridge (2009)
2. Cannon, J.R.: The One-Dimensional Heat Equation. Encyclopedia of Mathematics and Its
</p>
<p>Applications. Cambridge University Press, Cambridge (1985)
3. Glicksman, M.E.: Diffusion in Solids. Wiley, New York (1999)
4. Press, W.H., Teukolsky, S.A., Vetterling, W.T., Flannery, B.P.: Numerical Recipes in C++, 2nd
</p>
<p>edn. Cambridge University Press, Cambridge (2002)
5. J&uuml;ngel, A.: Drift-Diffusion Equations. Lecture Notes in Physics, vol. 773. Springer,
</p>
<p>Berlin/Heidelberg (2009)</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 10
The One-Dimensional Stationary
SCHR&Ouml;DINGER Equation
</p>
<p>10.1 Introduction
</p>
<p>The numerical solution of the stationary SCHR&Ouml;DINGER equation is discussed to
illustrate the application of NUMEROV&rsquo;s shooting method introduced in Sect. 8.3.
</p>
<p>We start the discussion with a brief survey of basic quantum mechanics. Of
course, this chapter is not supposed to give a self-contained introduction into this
field and the reader not familiar with quantum mechanics should, therefore, regard
the following discussion from a purely mathematical point of view. For more in-
depth reading on quantum mechanics we refer to the books [1&ndash;4] to name a few.
</p>
<p>A quantum-mechanical wave-function � � �.x; t/ 2 C is a function of time
t 2 RC and space x 2 R3 and obeys the SCHR&Ouml;DINGER equation:
</p>
<p>i&bdquo; d
dt
� D H� : (10.1)
</p>
<p>Here, &bdquo; D h=.2�/ is the reduced PLANCK constant, i is the imaginary unit, and
H is the HAMILTON operator or Hamiltonian. If H &curren; H.t/, i.e. the Hamiltonian is
independent of time t, we can employ a product ansatz
</p>
<p>�.x; t/ D exp
�
</p>
<p>� i&bdquo;Et
�
</p>
<p> .x/ ; (10.2)
</p>
<p>&copy; Springer International Publishing Switzerland 2016
B.A. Stickler, E. Schachinger, Basic Concepts in Computational Physics,
DOI 10.1007/978-3-319-27265-8_10
</p>
<p>139</p>
<p/>
</div>
<div class="page"><p/>
<p>140 10 The One-Dimensional Stationary SCHR&Ouml;DINGER Equation
</p>
<p>where E is the energy and  .x/ is the time-independent part of the wave-function.
This ansatz transforms Eq. (10.1) into
</p>
<p>i&bdquo; d
dt
</p>
<p>�
</p>
<p>exp
</p>
<p>�
</p>
<p>� i&bdquo;Et
�
</p>
<p> .x/
</p>
<p>�
</p>
<p>D i&bdquo;
�
</p>
<p>� i&bdquo;E
�
</p>
<p>exp
</p>
<p>�
</p>
<p>� i&bdquo;Et
�
</p>
<p> .x/
</p>
<p>D exp
�
</p>
<p>� i&bdquo;Et
�
</p>
<p>H .x/ ; (10.3)
</p>
<p>and  .x/ is determined by the eigenvalue problem [5]
</p>
<p>H D E ; (10.4)
</p>
<p>augmented by appropriate boundary conditions. We already came across Eq. (10.4)
when we discussed shooting methods in Sect. 8.3.
</p>
<p>The one-particle Hamiltonian is of the general form
</p>
<p>H D T C V D P
2
</p>
<p>2m
C V ; (10.5)
</p>
<p>with the kinetic energy operator T, the potential operator V , the momentum operator
P, and the particle&rsquo;s mass m. If the system is not exposed to an external magnetic
field, P can be expressed in position space by
</p>
<p>P D �i&bdquo;r ; (10.6)
</p>
<p>and the potential operator V by V.x/. Thus we get for Eq. (10.5):
</p>
<p>H D � &bdquo;
2
</p>
<p>2m

C V.x/ : (10.7)
</p>
<p>Hence, we have to solve the linear, second order partial differential equation:
</p>
<p>� &bdquo;
2
</p>
<p>2m

 .x/C V.x/ .x/ D E .x/ : (10.8)
</p>
<p>This equation will certainly not have solutions for arbitrary values of the
energy E. The particular values E D En1 for which it has a solution are referred to as
eigenenergies and the corresponding solution  n.x/ is referred to as eigenfunction
</p>
<p>1It depends on the problem on hand whether or not the index n will be continuous or discrete. For
simplicity, we assume here n to be discrete.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.1 Introduction 141
</p>
<p>to the eigenenergy En [1&ndash;3, 5]. To emphasize this point we rewrite Eq. (10.8) as:
</p>
<p>� &bdquo;
2
</p>
<p>2m

 n.x/C V.x/ n.x/ D En n.x/ ; 8n 2 N : (10.9)
</p>
<p>It is the purpose of this chapter to develop a numerical procedure which will, in the
end, allow to calculate numerically the eigenvalues En and eigenfunctions  n.x/ as
solutions of this equation.
</p>
<p>We proceed in our analysis by defining the scalar product between two functions
�.x/ and '.x/
</p>
<p>h�j'i D
Z
</p>
<p>dx��.x/'.x/ ; (10.10)
</p>
<p>where ��.x/ denotes the complex conjugate of �.x/. The corresponding L2-norm
reads
</p>
<p>j�j D
p
</p>
<p>h�j�i D
s
Z
</p>
<p>dxj�.x/j2 : (10.11)
</p>
<p>The expectation value of an operator O in the quantum mechanical state � is given
by
</p>
<p>hOi D h� jOj�ih� j� i D
R
</p>
<p>dx��.x/O�.x/
R
</p>
<p>dxj�.x/j2 : (10.12)
</p>
<p>It follows from Eq. (10.4) that the energy is the expectation value of the Hamilto-
nian H
</p>
<p>hHi D
R
</p>
<p>dx��.x/H�.x/
R
</p>
<p>dxj�.x/j2 D E : (10.13)
</p>
<p>We quote now some important properties; a detailed discussion can be found in
any textbook on quantum mechanics.
</p>
<p>&bull; The expectation value hOi of a Hermitian operator O, O&#142; D O, is real, i.e. hOi D
hOi�. Here O&#142; denotes the adjoint of O, i.e. O&#142; D .O�/T .
</p>
<p>&bull; Every real expectation value can be described by a Hermitian operator.
&bull; All observables can be described by Hermitian operators, in particular, the
</p>
<p>Hamiltonian has to be a Hermitian operator to ensure that the eigenenergies En
are real, En 2 R.</p>
<p/>
</div>
<div class="page"><p/>
<p>142 10 The One-Dimensional Stationary SCHR&Ouml;DINGER Equation
</p>
<p>&bull; It follows from the hermiticity of H that the eigenfunctions  n.x/ form a
complete, orthogonal basis in HILBERT space [5]. Furthermore, the functions
can be normalized and the relation
</p>
<p>h nj mi D ınm ; (10.14)
</p>
<p>holds, with ınm the KRONECKER-ı.
</p>
<p>Thus, with the help of Eq. (10.14) we rewrite the expectation value (10.12) of a
Hermitian operator O as:
</p>
<p>hOi D
Z
</p>
<p>dx��.x/O�.x/ : (10.15)
</p>
<p>In a next step we define the wave function �n.x; t/ following the ansatz (10.2)
</p>
<p>�n.x; t/ D exp
�
</p>
<p>� i&bdquo;Ent
�
</p>
<p> n.x/ ; (10.16)
</p>
<p>and the total wave-function�.x; t/ is then a superposition of wave-functions�n.x; t/
</p>
<p>�.x; t/ D
X
</p>
<p>n
</p>
<p>cn�n.x; t/ ; (10.17)
</p>
<p>because the �n.x; t/ constitute a complete, orthogonal basis.2 Furthermore, we
demand �.x; t/ to be normalized for all t. Employing Eq. (10.14) in Eq. (10.17)
yields
</p>
<p>Z
</p>
<p>dxj�.x; t/j2 D
X
</p>
<p>n
</p>
<p>jcnj2 D 1 : (10.18)
</p>
<p>We quote BORN&rsquo;s interpretation of the squared modulus of the total wave-
function (referred to as BORN&rsquo;s rule):
</p>
<p>j�.x; t/j2dx D The probability that the particle described by
the wave-function �.x; t/ can be found at time
</p>
<p>t within a volume dx around the point x. (10.19)
</p>
<p>2This is only possible because the SCHR&Ouml;DINGER equation is linear.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.2 A Simple Example: The Particle in a Box 143
</p>
<p>This interpretation justifies the requirement of a normalization of the wave-function
�.x; t/
</p>
<p>Z
</p>
<p>dxj�.x; t/j2 ŠD 1 ; (10.20)
</p>
<p>because, by definition, the particle has to be found somewhere anytime.
Suppose we start with an initial state �.x/ D �.x; t D 0/. Since the functions
</p>
<p> n.x/ form a complete basis in HILBERT space, �.x/ may be written with the help
of Eq. (10.17) as
</p>
<p>�.x/ D
X
</p>
<p>n
</p>
<p>cn n.x/ : (10.21)
</p>
<p>We deduce from Eq. (10.14) that
</p>
<p>h mj�i D
X
</p>
<p>n
</p>
<p>cn
</p>
<p>Z
</p>
<p>dx �m.x/ n.x/ D cm : (10.22)
</p>
<p>Consequently, jcmj2 is the probability that the particle was initially in state m.
This allows us to interpret Eq. (10.17) in the following way: The coefficients cm
determine the composition of the initial state. The exponential factor describes an
oscillation and we note that different eigenfunctions, which correspond to different
eigenenergies, oscillate with different frequencies. This can, for instance, induce the
diffluence of a wave packet.
</p>
<p>10.2 A Simple Example: The Particle in a Box
</p>
<p>We concentrate on the one-dimensional SCHR&Ouml;DINGER equation and discuss a
simple problem which will then be solved numerically in Sect. 10.3. We rewrite
the one-dimensional SCHR&Ouml;DINGER equation (10.7), with x 2 R, as
</p>
<p>� &bdquo;
2
</p>
<p>2m
</p>
<p>d2
</p>
<p>dx2
 n.x/C V.x/ n.x/ D En n.x/ ; (10.23)
</p>
<p>and specify
</p>
<p>V.x/ D
(
</p>
<p>0 0 � x � L;
1 elsewhere,
</p>
<p>(10.24)</p>
<p/>
</div>
<div class="page"><p/>
<p>144 10 The One-Dimensional Stationary SCHR&Ouml;DINGER Equation
</p>
<p>together with the boundary conditions
</p>
<p> n.0/ D  n.L/ D 0 ; (10.25)
</p>
<p>and the normalization condition
</p>
<p>Z
</p>
<p>dxj n.x/j2 D
Z L
</p>
<p>0
</p>
<p>dxj n.x/j2 D 1 : (10.26)
</p>
<p>We note that the boundary conditions are dictated by the particular form of the
potential (10.24) which requires that  n.x/ D 0 for x &hellip; Œ0;L&#141;. This problem is
commonly referred to as the particle in a one-dimensional box.
</p>
<p>Let us introduce dimensionless variables in order to simplify the numerics of
Eq. (10.23). We define new variables
</p>
<p>s D x
L
; "n D
</p>
<p>En
</p>
<p>E
; (10.27)
</p>
<p>where L is the length scale and E is the energy scale. The energy scale E is fully
determined by the relation
</p>
<p>E D &bdquo;
2
</p>
<p>mL2
: (10.28)
</p>
<p>We note that s 2 Œ0; 1&#141;, hence the rescaled wave-function is given by
</p>
<p>'n.s/ D
p
</p>
<p>L n.x/ ; (10.29)
</p>
<p>which satisfies the normalization condition
</p>
<p>Z L
</p>
<p>0
</p>
<p>dxj n.x/j2 D
Z 1
</p>
<p>0
</p>
<p>dsj'n.s/j2 D 1 : (10.30)
</p>
<p>The rescaled SCHR&Ouml;DINGER equation can be obtained by multiplying Eq. (10.23)
with 1=E:
</p>
<p>� &bdquo;
2
</p>
<p>2mE
</p>
<p>d2
</p>
<p>dx2
 n.x/C
</p>
<p>V.x/
</p>
<p>E
 n.x/ D �
</p>
<p>L2
</p>
<p>2
</p>
<p>d2
</p>
<p>dx2
 n.x/C v.s/ n.x/
</p>
<p>D �1
2
</p>
<p>d2
</p>
<p>ds2
 n.x/C v.s/ n.x/
</p>
<p>D En
E
 n.x/
</p>
<p>D "n n.x/ : (10.31)</p>
<p/>
</div>
<div class="page"><p/>
<p>10.2 A Simple Example: The Particle in a Box 145
</p>
<p>Here we introduced the rescaled potential v.s/
</p>
<p>v.s/ D
(
</p>
<p>0 0 � s � 1;
1 elsewhere.
</p>
<p>(10.32)
</p>
<p>Hence, the rescaled wave-function (10.29) is a solution of the differential equation:
</p>
<p>� 1
2
</p>
<p>d2
</p>
<p>ds2
'n.s/C v.s/'n.s/ D "n'n.s/ : (10.33)
</p>
<p>The form (10.32) of the potential implies that 'n.s/ D 0 for all s &hellip; Œ0; 1&#141; and the
complete boundary value problem is defined as:
</p>
<p>8
</p>
<p>ˆ̂
ˆ̂
&lt;
</p>
<p>ˆ̂
ˆ̂
:
</p>
<p>�1
2
</p>
<p>d2
</p>
<p>ds2
'n.s/ D "n'n.s/ ; s 2 Œ0; 1&#141; ;
</p>
<p>'n.0/ D 0 ;
'n.1/ D 0 :
</p>
<p>(10.34)
</p>
<p>It is an easy task to solve this boundary value problem analytically. For s 2 Œ0; 1&#141;
we choose the ansatz
</p>
<p>'n.s/ D An sin.kns/C Bn cos.kns/ ; (10.35)
</p>
<p>where An and Bn are some constants and kn is given by
</p>
<p>kn D
p
</p>
<p>2"n : (10.36)
</p>
<p>From the boundary conditions we obtain
</p>
<p>'n.0/ D Bn D 0 ; (10.37)
</p>
<p>and
</p>
<p>'n.1/ D An sin.kn/ D 0 : (10.38)</p>
<p/>
</div>
<div class="page"><p/>
<p>146 10 The One-Dimensional Stationary SCHR&Ouml;DINGER Equation
</p>
<p>Thus,
</p>
<p>kn D n� ; (10.39)
</p>
<p>and the eigenenergies "n are quantized:
</p>
<p>"n D
n2�2
</p>
<p>2
: (10.40)
</p>
<p>The corresponding eigenfunctions 'n.s/ are then given by:
</p>
<p>'n.s/ D
(
</p>
<p>An sin.n�s/ s 2 Œ0; 1&#141; ;
0 elsewhere.
</p>
<p>(10.41)
</p>
<p>The constants An are determined from the normalization condition (10.30)3
</p>
<p>Z 1
</p>
<p>0
</p>
<p>dsj'n.s/j2 D A2n
Z 1
</p>
<p>0
</p>
<p>ds sin2.n�s/
</p>
<p>D A
2
n
</p>
<p>2
</p>
<p>ŠD 1 ; (10.43)
</p>
<p>and:
</p>
<p>An D
p
2 : (10.44)
</p>
<p>Finally, we apply the relations (10.27), (10.28), and (10.29) and obtain
</p>
<p> n.x/ D
1p
L
'n
</p>
<p>� x
</p>
<p>L
</p>
<p>�
</p>
<p>D
</p>
<p>8
</p>
<p>&lt;̂
</p>
<p>:̂
</p>
<p>r
</p>
<p>2
</p>
<p>L
sin
�n�x
</p>
<p>L
</p>
<p>�
</p>
<p>x 2 Œ0;L&#141; ;
</p>
<p>0 elsewhere;
</p>
<p>(10.45)
</p>
<p>3 Here we make use of
Z
</p>
<p>du sin2.u/ D 1
2
Œu � cos.u/ sin.u/&#141; : (10.42)</p>
<p/>
</div>
<div class="page"><p/>
<p>10.3 Numerical Solution 147
</p>
<p>and
</p>
<p>En D "nE D
&bdquo;2�2n2
2mL2
</p>
<p>: (10.46)
</p>
<p>In most cases expectation values of some observables are to be determined. We
might, for instance, be interested in the expectation value hxi of the position operator
x or its variance var .x/ D
</p>
<p>˝
</p>
<p>.x � hxi/2
˛
</p>
<p>(see also Appendix E). It follows from
Eq. (10.27) that
</p>
<p>hxi D L hsi ; and
˝
</p>
<p>.x � hxi/2
˛
</p>
<p>D L2
˝
</p>
<p>.s � hsi/2
˛
</p>
<p>D L2var .s/ : (10.47)
</p>
<p>Definition (10.15) gives together with solution (10.41) the expectation value hsi:
</p>
<p>hsi D 2
Z 1
</p>
<p>0
</p>
<p>ds sin2.n�s/s D 1
2
: (10.48)
</p>
<p>Thus, the expectation value of the position operator is independent of the quantum
number n. Furthermore, we obtain for
</p>
<p>˝
</p>
<p>s2
˛
</p>
<p>:
</p>
<p>˝
</p>
<p>s2
˛
</p>
<p>D 2
Z 1
</p>
<p>0
</p>
<p>ds sin2.n�s/s2 D 1
3
� 1
2n2�2
</p>
<p>: (10.49)
</p>
<p>Hence, the variance var .s/ is determined by
</p>
<p>˝
</p>
<p>.s � hsi/2
˛
</p>
<p>D
˝
</p>
<p>s2
˛
</p>
<p>� hsi2 D 1
3
� 1
2n2�2
</p>
<p>� 1
4
D 1
12
</p>
<p>�
</p>
<p>1 � 6
n2�2
</p>
<p>�
</p>
<p>: (10.50)
</p>
<p>We note that the variance increases with increasing n.
In the next section these results are reproduced numerically by the NUMEROV
</p>
<p>shooting method. (See Sect. 8.3 and, for instance, Ref. [6].) Moreover, this numeri-
cal formulation will allow us to find solutions for more complex potentials V.x/.
</p>
<p>10.3 Numerical Solution
</p>
<p>The following discussion is based on the scaled SCHR&Ouml;DINGER equation (10.33),
but we consider now a more general potential of the form
</p>
<p>v.s/ D
(
</p>
<p>Qv.s/ 0 � s � 1 ;
1 elsewhere,
</p>
<p>(10.51)</p>
<p/>
</div>
<div class="page"><p/>
<p>148 10 The One-Dimensional Stationary SCHR&Ouml;DINGER Equation
</p>
<p>which results in the boundary value problem:
</p>
<p>8
</p>
<p>ˆ̂
ˆ̂
&lt;
</p>
<p>ˆ̂
ˆ̂
:
</p>
<p>�1
2
</p>
<p>d2
</p>
<p>ds2
'n.s/C Qv.s/'n.s/ D "n'n.s/ s 2 Œ0; 1&#141;; n 2 N ;
</p>
<p>'n.0/ D 0 ;
'n.1/ D 0 :
</p>
<p>(10.52)
</p>
<p>As our numerical treatment will be based on shooting methods, discussed in
Sect. 8.3, the second order differential equation in Eq. (10.52) will be transformed
into a form which corresponds to Eq. (8.54), namely:
</p>
<p>' 00n .s/C 2 Œ"n � Qv.s/&#141; 'n.s/ D 0 : (10.53)
</p>
<p>The interval Œ0; 1&#141; is discretized using N C 1 grid-points s` D `=N, ` D
0; 1; 2; : : : ;N (h D 1=N) and we denote with 'n.s`/ and Qv.s`/ � v` the values
of 'n.s/ and Qv.s/ at those grid-points. Thus, Eq. (8.59) can immediately be applied
and we get:
</p>
<p>'n.s`C1/ D
2
�
</p>
<p>1 � 5
6N2
</p>
<p>."n � Qv`/
�
</p>
<p>'n.s`/�
�
</p>
<p>1C 1
6N2
</p>
<p>."n � Qv`�1/
�
</p>
<p>'n.s`�1/
</p>
<p>1C 1
6N2
</p>
<p>."n � Qv`C1/
:
</p>
<p>(10.54)
</p>
<p>We use the initial conditions 'n.s0/ D 0 and ' 0n.s0/ D 1, which is always
possible, since (10.52) is a homogeneous boundary value problem. This gives
</p>
<p>' 0n.s0/ �
'n.s1/ � 'n.s�1/
</p>
<p>2h
D 1 ) 'n.s1/ D
</p>
<p>2
</p>
<p>N
: (10.55)
</p>
<p>The normalization of the wave-function (10.30) is then approximated with the help
of the forward rectangular rule (3.9):
</p>
<p>Z 1
</p>
<p>0
</p>
<p>dsj'n.s/j2 � h
N
X
</p>
<p>`D0
Œ'n.s`/&#141;
</p>
<p>2 ŠD 1 : (10.56)
</p>
<p>Consistently, we approximate the expectation value hsi via
</p>
<p>Z 1
</p>
<p>0
</p>
<p>ds sj'n.s/j2 � h
N
X
</p>
<p>`D0
Œ'n.s`/&#141;
</p>
<p>2
s` : (10.57)</p>
<p/>
</div>
<div class="page"><p/>
<p>10.3 Numerical Solution 149
</p>
<p>Table 10.1 Comparison
between analytic and
numerical eigenenergies for
the particle in a box for
N D 100
</p>
<p>n "n-analytic "n-numeric
</p>
<p>1 4.934802 4.934802
</p>
<p>2 19.739209 19.739208
</p>
<p>3 44.413219 44.413205
</p>
<p>4 78.956835 78.956753
</p>
<p>5 123.370055 123.369742
</p>
<p>6 177.652879 177.651943
</p>
<p>7 241.805308 241.802947
</p>
<p>8 315.827341 315.822077
</p>
<p>9 399.718978 399.708300
</p>
<p>10 493.480220 493.460113
</p>
<p>The NUMEROV shooting algorithm is then defined by the following steps:
</p>
<p>1. Choose two trial energies "a and "b and define the required accuracy �.
2. Calculate '.sN I "a/ � 'a and '.sN I "b/ � 'b using Eq. (10.54).
3. If 'a'b &gt; 0, choose new values for "a or "b and go to step 1.
4. If 'a'b &lt; 0, calculate "c D ."a C "b/ =2 and determine '.sN I "c/ � 'c using
</p>
<p>Eq. (10.54).
5. If 'a'c &lt; 0, set "b D "c and go to step 4.
6. If 'c'b &lt; 0, set "a D "c and go to step 4.
7. Terminate the loop when j"a � "bj &lt; �.
</p>
<p>These steps have been carried out for 100 grid-points, a potential Qv D 0, and a
required accuracy of � D 10�10. The first ten eigenenergies are given in Table 10.1
and are compared with analytic results (10.40).
</p>
<p>In addition, Fig. 10.1 presents the first five eigenvalues "n (right hand scale) as
horizontal straight lines. Aligned with these eigenvalues we find on the right hand
side of this figure the corresponding normalized eigenfunctions calculated using
N D 100 grid-points. The agreement with the analytic result of Eq. (10.41) is
excellent.</p>
<p/>
</div>
<div class="page"><p/>
<p>150 10 The One-Dimensional Stationary SCHR&Ouml;DINGER Equation
</p>
<p>Fig. 10.1 The first five numerically determined eigenvalues "n of Table 10.1 are presented
as horizontal lines (left hand scale). Aligned with these eigenvalues are the corresponding
eigenfunctions 'n.s/ vs s for N D 100 (right hand scales)</p>
<p/>
</div>
<div class="page"><p/>
<p>10.4 Another Case 151
</p>
<p>10.4 Another Case
</p>
<p>Here we discuss briefly some results achievedwith the help of NUMEROV&rsquo;s shooting
algorithm. In particular, we discuss the particle in the box for three different
potentials Qv.s/
</p>
<p>Qv1.s/ D 50 cos.�s/ ; Qv2.s/ D 50 exp
"
</p>
<p>�
�
</p>
<p>s � 1
2
</p>
<p>�2
</p>
<p>0:08
</p>
<p>#
</p>
<p>; Qv3.s/ D 50s :
</p>
<p>(10.58)
The potentials are illustrated in Fig. 10.2. All calculations were carried out with
N D 100 grid-points and an accuracy � D 10�10. The first five eigenenergies "n are
shown in Figs. 10.3, 10.4, and 10.5, respectively, as horizontal lines (left hand scale).
The numerically determined normalized eigenfunctions 'n.s/ vs s (solid lines) are
presented on the right hand side of these figures and are aligned with their respective
eigenvalues. They are also compared with the eigenfunctions (dotted lines) of the
particle in a box, i.e. Qv.s/ D 0. In all cases the eigenfunctions reflect the symmetry
of the various potentials Qv.s/which becomes particularly transparent in Fig. 10.3 for
the potential Qv1.s/. The eigenfunctions develop an additional node in comparison to
the eigenfunctions calculated for Qv.s/ D 0. In the other two cases only the very
first eigenfunctions n � 3 appear to be affected by the potential. Moreover, in all
three cases, the respective eigenvalues are shifted towards higher values which is
consistent with a general result of quantum mechanical perturbation theory.
</p>
<p>Fig. 10.2 The three different
potentials Qv.s/ defined in
Eq. (10.58)</p>
<p/>
</div>
<div class="page"><p/>
<p>152 10 The One-Dimensional Stationary SCHR&Ouml;DINGER Equation
</p>
<p>Fig. 10.3 Numerically determined eigenvalues "n (left hand scale) and eigenfunctions 'n.s/ vs
s (right hand scales) for the potential Qv1.s/. The first five eigenvalues are presented as straight
horizontal lines. Aligned with these lines the eigenfunctions are shown on the right hand side of
this figure. The dotted lines indicate the eigenfunctions of the particle in the box with Qv.s/ D 0
(see Fig. 10.1)</p>
<p/>
</div>
<div class="page"><p/>
<p>10.4 Another Case 153
</p>
<p>Fig. 10.4 Numerically determined eigenvalues "n (left hand scale) and eigenfunctions 'n.s/ vs
s (right hand scales) for the potential Qv2.s/. The first five eigenvalues are presented as straight
horizontal lines. Aligned with these lines the eigenfunctions are shown on the right hand side of
this figure. The dotted lines indicate the eigenfunctions of the particle in the box with Qv.s/ D 0
(see Fig. 10.1)</p>
<p/>
</div>
<div class="page"><p/>
<p>154 10 The One-Dimensional Stationary SCHR&Ouml;DINGER Equation
</p>
<p>Fig. 10.5 Numerically determined eigenvalues "n (left hand scale) and eigenfunctions 'n.s/ vs
s (right hand scales) for the potential Qv3.s/. The first five eigenvalues are presented as straight
horizontal lines. Aligned with these lines the eigenfunctions are shown on the right hand side of
this figure. The dotted lines indicate the eigenfunctions of the particle in the box with Qv.s/ D 0
(see Fig. 10.1)</p>
<p/>
</div>
<div class="page"><p/>
<p>References 155
</p>
<p>Summary
</p>
<p>The quantum-mechanical problem of a particle in a box was described by a
homogeneous boundary value problem which could be solved analytically if the
box&rsquo; potential Qv.s/ D 0. On the other hand, NUMEROV&rsquo;s shooting algorithm was
particularly designed to treat effectively homogeneous boundary value problems.
Consequently, the problem of the particle in the box was used to design a NUMEROV
shooting algorithmwhichwas then tested against the analytic results. The agreement
between numerics and analytical results turned out to be excellent and proved the
quality of the method. For illustrative purposes the problem of the particle in the
box was then solved numerically for three different, more complex, box-potentials
Qv.s/.
</p>
<p>Problems
</p>
<p>1. Solve the one-dimensional stationary SCHR&Ouml;DINGER equation in an infinitely
deep potential well by employing the shooting method according to NUMEROV
of Sect. 8.3. The total potential v.s/ is assumed to be of the form (10.51). Choose
different potentials Qv.s/ within the well.
</p>
<p>You can check your code by reproducing the results presented in Sects. 10.3
and 10.4. In addition, determine numerically the expectation value hxi and the
variance var .x/ of the position operator x for the first five eigenfunctions. This
can be achieved by employing the rectangular rule of Chap. 3, as illustrated in
Eq. (10.57).
</p>
<p>2. Solve the SCHR&Ouml;DINGER equation for some potential Qv.s/ of your choice and
plot the first five eigenfunctions. This potential should not be equal to one of the
potentials discussed in this chapter. Again, calculate hxi and var .x/ for the first
five eigenfunctions.
</p>
<p>3. Solve the stationary SCHR&Ouml;DINGER equation (10.4) for the harmonic potential
V.x/ D m!2x2=2. The algorithm discussed in this chapter can be applied by
choosing the box length L sufficiently large, so that the harmonic oscillator
potential is well within the box for all energies of interest.
</p>
<p>4. Solve the SCHR&Ouml;DINGER equation for a double well potential which can be
obtained by adding two mutually displaced harmonic potentials.
</p>
<p>References
</p>
<p>1. Baym, G.: Lectures on Quantum Mechanics. Lecture Notes and Supplements in Physics. The
Benjamin/Cummings Publ. Comp., Inc., London/Amsterdam (1969)
</p>
<p>2. Cohen-Tannoudji, C., Diu, B., Lalo&euml;, F.: Quantum Mechanics, vol. I. Wiley, New York (1977)
3. Sakurai, J.J.: Modern Quantum Mechanics. Addison-Wesley, Menlo Park (1985)</p>
<p/>
</div>
<div class="page"><p/>
<p>156 10 The One-Dimensional Stationary SCHR&Ouml;DINGER Equation
</p>
<p>4. Ballentine, L.E.: Quantum Mechanics. World Scientific, Hackensack (1998)
5. Courant, R., Hilbert, D.: Methods of Mathematical Physics, vol. 1. Wiley, New York (1989)
6. Hairer, E., N&oslash;rsett, S.P., Wanner, G.: Solving Ordinary Differential Equations I. Springer Series
</p>
<p>in Computational Mathematics, vol. 8, 2nd edn. Springer, Berlin/Heidelberg (1993)</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 11
Partial Differential Equations
</p>
<p>11.1 Introduction
</p>
<p>This section discusses some fundamental aspects of the numerics of partial differ-
ential equations and it will be restricted to methods already encountered in previous
chapters, i.e. on finite difference methods. These are particularly useful to find
solutions of linear partial differential equations (PDEs). Nonlinear PDEs, such as
the NAVIER-STOKES equations, require more advanced techniques as there are
finite element methods or finite volume methods for conservation laws. A detailed
discussion of a wide spectrum of methods can be found in many textbooks on the
numerics of PDEs the interested reader is referred to [1&ndash;7].
</p>
<p>Since we already introduced the concepts of finite difference derivatives in
Chap. 2 and their application to boundary value problems of ordinary differential
equations in Sect. 8.2, we concentrate mainly on the application of these methods
to specific types of PDEs. In detail, we investigate the POISSON equation as an
example for elliptic PDEs, the time dependent heat equation as an example for
parabolic PDEs, and the wave equation as an example for hyperbolic PDEs. The
concepts presented here are, of course, also applicable to other problems. However,
in contrast to the numerics of ordinary differential equations, there exists no general
recipe for the solution of PDEs.
</p>
<p>Another important point to note is that, as in the theory of ordinary differential
equations, the problem is only fully determined when initial and/or boundary
conditions have been defined. For instance, in the case of the POISSON equation
only boundary conditions are required, while for the time-dependent heat equation
initial conditions are required as well. In general, pure boundary value problems
are easier from a numerical point of view because the question whether or not the
algorithm is stable does not play such an important role. For combined boundary
</p>
<p>&copy; Springer International Publishing Switzerland 2016
B.A. Stickler, E. Schachinger, Basic Concepts in Computational Physics,
DOI 10.1007/978-3-319-27265-8_11
</p>
<p>157</p>
<p/>
</div>
<div class="page"><p/>
<p>158 11 Partial Differential Equations
</p>
<p>and initial value problems it is essential to check carefully that the discretization of
the time axis is not in conflict with the discretization of the space domain. This is
of particular importance in the numerical treatment of hyperbolic PDEs, where the
so called COURANT-FRIEDRICHS-LEWY (CFL) condition determines the stability
of the algorithm. We shall come back to this point in Sects. 11.3 and 11.4. Finally,
we conclude this chapter with a discussion of the numerical solution of the time-
dependent SCHR&Ouml;DINGER equation.
</p>
<p>11.2 The POISSON Equation
</p>
<p>We consider the POISSON equation as a model for an elliptic PDE [8, 9]. Neverthe-
less, we review briefly some basics of electrodynamics [10, 11]. The force F.r; t/
as a function of position r 2 R3 and time t 2 RC acting on a particle with charge
q, which moves with velocity v within an electromagnetic field described by the
electric field E.r; t/ and the magnetic field B.r; t/, is determined from:
</p>
<p>F.r; t/ D q ŒE.r; t/C v � B.r; t/&#141; : (11.1)
</p>
<p>We consider here the electrostatic case which is characterized by a zero magnetic
field [B.r; t/ D 0] and a time independent electric field. The electric field E itself is
described by the equation
</p>
<p>r � E.r/ D 1
�0
�.r/ ; (11.2)
</p>
<p>where the charge density �.r; t/ acts as the source of the electric field E.r; t/.
Here �0 is the dielectric permittivity of vacuum. Furthermore, the electric field E
is connected to the electrostatic potential '.r/ via
</p>
<p>E.r/ D �r'.r/ : (11.3)
</p>
<p>Thus, Eq. (11.2) is reformulated as:
</p>
<p>
'.r/ D ��.r/
�0
</p>
<p>: (11.4)
</p>
<p>This equation is referred to as the POISSON equation and in the particular case of
�.r/ D 0 it is referred to as the LAPLACE equation [12].</p>
<p/>
</div>
<div class="page"><p/>
<p>11.2 The POISSON Equation 159
</p>
<p>We focus now on the numerical solution of the two dimensional POISSON
equation (11.4) on a rectangular domain˝ D Œ0;Lx&#141;�Œ0;Ly&#141; together with boundary
conditions '.x; y/ D g.x; y/ on @˝ . In detail, we want to solve the two-dimensional
boundary value problem
</p>
<p>8
</p>
<p>&lt;̂
</p>
<p>:̂
</p>
<p>@2
</p>
<p>@x2
'.x; y/C @
</p>
<p>2
</p>
<p>@y2
'.x; y/ D ��.x; y/ ; .x; y/ 2 ˝ ;
</p>
<p>'.x; y/ D g.x; y/ ; .x; y/ 2 @˝ ;
(11.5)
</p>
<p>where we absorbed �0 into the charge density �.x; y/. Note that a treatment of the
three dimensional case can be carried out in analogue.
</p>
<p>We employ a finite difference approximation to the derivatives which appear in
Eq. (11.5) (see Chap. 2) and we define grid-points in x and y direction via
</p>
<p>xi D x0 C ihx; i D 0; 1; 2; : : : ; n ; (11.6a)
</p>
<p>yj D y0 C jhy; j D 0; 1; 2; : : : ;m ; (11.6b)
</p>
<p>where hx and hy denote the grid-spacing in x- and y-direction, respectively. As
discussed in Chap. 2 we consider only equally spaced grid-points. An extension to
non-uniform grids is straight forward.
</p>
<p>We define the function values on the grid-points as
</p>
<p>'i;j � '.xi; yj/ ; (11.7)
</p>
<p>and similarly �i;j � �.xi; yj/. Consequently, we find the finite difference approxima-
tion of Eq. (11.5):
</p>
<p>'i�1;j � 2'i;j C 'iC1;j
h2x
</p>
<p>C 'i;j�1 � 2'i;j C 'i;jC1
h2y
</p>
<p>D ��i;j : (11.8)
</p>
<p>The boundary conditions (11.5) can be written as
</p>
<p>'0;j D g0;j ; j D 0; 1; : : : ;m ; (11.9a)
'n;j D gn;j ; j D 0; 1; : : : ;m ; (11.9b)
'i;0 D gi;0 ; i D 1; 2; : : : ; n � 1 ; (11.9c)
'i;m D gi;m ; i D 1; 2; : : : ; n � 1 : (11.9d)</p>
<p/>
</div>
<div class="page"><p/>
<p>160 11 Partial Differential Equations
</p>
<p>Equation (11.8) is multiplied by �h2xh2y=2 and we obtain after rearranging terms
</p>
<p>�
</p>
<p>h2x C h2y
�
</p>
<p>'i;j �
1
</p>
<p>2
</p>
<p>�
</p>
<p>h2y
�
</p>
<p>'i�1;j C 'iC1;j
�
</p>
<p>C h2x
�
</p>
<p>'i;j�1 C 'i;jC1
��
</p>
<p>D .hxhy/
2
</p>
<p>2
�i;j ;
</p>
<p>(11.10)
</p>
<p>for i D 1; : : : ; n and j D 1; : : : ;m. There are different strategies how this set of
equations might be solved. The common strategy is to employ the assignments
</p>
<p>'1;1 ! '1 ;
'1;2 ! '2 ;
:::
</p>
<p>:::
</p>
<p>'n;m ! '` ; (11.11)
</p>
<p>where ` D nm. Equation (11.10) is then rewritten as a matrix equation with a
vector of unknowns ' D .'1; '2; : : : ; '`/T according to Eq. (11.11). The boundary
conditions are to be included in the matrix. This matrix equation is then solved either
by direct or iterative methods as they are discussed in Appendix C.
</p>
<p>It is our plan to solve Eq. (11.10) iteratively. This requires the introduction of
a superscript iteration index t and ' ti;j denotes the function value '.xi; yj/ after t-
iteration steps. There are two different implementations of an iterative solution,
namely the GAUSS-SEIDEL or the JACOBI method (Appendix C). They differ only
in the update procedure of the function values ' ti;j at the grid-points. The basic idea
is to develop an update algorithm which expresses the function values ' ti;j with the
help of function values at already updated grid-points and of function values ' t�1i;j
determined in the preceding iteration step [Appendix, Eq. (C.27)].
</p>
<p>We formulate this iteration rule as
</p>
<p>' tC1i;j D
.hxhy/
</p>
<p>2
</p>
<p>2.h2x C h2y/
�i;j C
</p>
<p>1
</p>
<p>2.h2x C h2y/
h
</p>
<p>h2y
</p>
<p>�
</p>
<p>' tC1i�1;j C ' tiC1;j
�
</p>
<p>Ch2x
�
</p>
<p>' tC1i;j�1 C ' ti;jC1
�i
</p>
<p>; (11.12)
</p>
<p>where we abstained from incorporating a relaxation parameter (see Appendix C).
Note that by using the iteration rule (11.12) the boundary conditions have to be
accounted for in an additional step.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.2 The POISSON Equation 161
</p>
<p>Let us specify the boundary conditions for a concrete problem: We we want
to determine the electrostatic potential of an electric monopole, dipole, and
quadrupole, respectively. They are placed inside a grounded box of dimensions
Lx and Ly. Thus, we have to impose DIRICHLET boundary conditions '.0; y/ D
'.Lx; y/ D 0 in x-direction and '.x; 0/ D '.x;Ly/ D 0 in y-direction. In
this particular case the boundary conditions can be made part of Eq. (11.12) by
restricting the loop over the x-grid (y-grid) to i D 2; : : : ;N � 1 which leaves the
boundary points '.0; y/ ['.x; 0/] and '.Lx; y/ ['.x;Ly/] unchanged. Furthermore
we set Lx D Ly D 10, the number of grid-points on both axes to n D m D 100, and
define the domains:
</p>
<p>˝1 D
�
</p>
<p>x n
2�10; x n2
</p>
<p>�
</p>
<p>�
�
</p>
<p>y m
2 �10; y m2
</p>
<p>�
</p>
<p>; (11.13a)
</p>
<p>˝2 D
�
</p>
<p>x n
2
; x n
</p>
<p>2C10
�
</p>
<p>�
�
</p>
<p>y m
2 �10; y m2
</p>
<p>�
</p>
<p>; (11.13b)
</p>
<p>˝3 D
�
</p>
<p>x n
2�10; x n2
</p>
<p>�
</p>
<p>�
�
</p>
<p>y m
2
; y m
</p>
<p>2C10
�
</p>
<p>; (11.13c)
</p>
<p>˝4 D
�
</p>
<p>x n
2
; x n
</p>
<p>2C10
�
</p>
<p>�
�
</p>
<p>y m
2
; y m
</p>
<p>2C10
�
</p>
<p>: (11.13d)
</p>
<p>The charge density �.x; y/ is described by three different scenarios, namely the
electric monopole
</p>
<p>�1.x; y/ D
(
</p>
<p>50 .x; y/ 2 ˝1 [˝2 [˝3 [˝4 ;
0 elsewhere;
</p>
<p>(11.14a)
</p>
<p>the electric dipole
</p>
<p>�2.x; y/ D
</p>
<p>8
</p>
<p>ˆ̂
&lt;
</p>
<p>ˆ̂
:
</p>
<p>50 .x; y/ 2 ˝1 [˝2 ;
�50 .x; y/ 2 ˝3 [˝4 ;
0 elsewhere;
</p>
<p>(11.14b)
</p>
<p>and the electric quadrupole:
</p>
<p>�3.x; y/ D
</p>
<p>8
</p>
<p>ˆ̂
&lt;
</p>
<p>ˆ̂
:
</p>
<p>50 .x; y/ 2 ˝1 [˝4 ;
�50 .x; y/ 2 ˝2 [˝3 ;
0 elsewhere:
</p>
<p>(11.14c)
</p>
<p>These charge densities are illustrated in Fig. 11.1.</p>
<p/>
</div>
<div class="page"><p/>
<p>162 11 Partial Differential Equations
</p>
<p>Fig. 11.1 The electric monopole, dipole, and quadrupole charge densities (a) �1.x; y/, (b) �2.x; y/,
and (c) �3.x; y/, respectively, as defined in Eq. (11.14)
</p>
<p>The solution of Eq. (11.12) is regarded to be converged if the potential '.x; y/
does not change significantly between two consecutive iteration steps, i.e.
</p>
<p>max
i;j
</p>
<p>�
</p>
<p>j' ti;j � ' t�1i;j j
�
</p>
<p>&lt; � ; (11.15)
</p>
<p>where � D 10�4 is the required accuracy. A criterion to check the relative
change can be formulated in a similar fashion. The resulting potential profiles
'.x; y/ are presented in Fig. 11.2. They reflect perfectly the symmetries of the
charge densities �1.x; y/, �2.x; y/, and �3.x; y/, respectively. Finally, standard finite
differencemethods can be applied to calculate, based on Eq. (11.3), the electric field
E.x; y/ from the potential profiles '.x; y/.1
</p>
<p>1We note that the electrostatic potentials that we calculated here numerically can also be
determined analytically with the method of mirror charges [10].</p>
<p/>
</div>
<div class="page"><p/>
<p>11.3 The Time-Dependent Heat Equation 163
</p>
<p>Fig. 11.2 Potential profile '.x; y/ obtained for charge density (a) �1.x; y/, (b) �2.x; y/, and
(c) �3.x; y/
</p>
<p>11.3 The Time-Dependent Heat Equation
</p>
<p>We discuss here the numerical solution of the time-dependent heat equation [13,
14] which is a representative of parabolic PDEs. This equation has already been
introduced in Sect. 9.1, Eq. (9.1), and is, reduced to the one-dimensional case, of the
form
</p>
<p>@
</p>
<p>@t
T.x; t/ D � @
</p>
<p>2
</p>
<p>@x2
T.x; t/ ; (11.16)
</p>
<p>with the thermal diffusivity �. It is augmented by appropriate boundary and initial
conditions. Again, we will not discuss the extension to higher dimensions since it
is straight forward, however, maybe tedious in the general case. We approximate
the right hand side of Eq. (11.16) with the help of the central finite difference
approximation (Sect. 2.2) and obtain
</p>
<p>@
</p>
<p>@t
Tk.t/ D �
</p>
<p>Tk�1.t/ � 2Tk.t/C TkC1.t/
h2
</p>
<p>; (11.17)</p>
<p/>
</div>
<div class="page"><p/>
<p>164 11 Partial Differential Equations
</p>
<p>with the usual discretization xk D x0 C kh, k D 0; : : : ;N, in combination with the
notation Tk.t/ � T.xk; t/.
</p>
<p>The time derivative in Eq. (11.17) can be approximated with the help of methods
already discussed in Chap. 5. In particular, one has to decide whether the solution
of Eq. (11.17) should be approximated by an explicit or an implicit integrator. In
order to emphasize the differences between the two methods, the application of the
explicit EULER and of the implicit EULER method will be studied. However, more
complex integrators may be applied as well. In particular, the CRANK-NICOLSON
method [15] proved to be very useful for the solution of parabolic differential
equations.
</p>
<p>We define tn D t0 C n
t and Tnk � Tk.tn/ and employ the explicit EULER
scheme (5.9) in Eq. (11.17) to get
</p>
<p>TnC1k � Tnk

t
</p>
<p>D �
Tnk�1 � 2Tnk C TnkC1
</p>
<p>h2
; (11.18)
</p>
<p>with the solution:
</p>
<p>TnC1k D Tnk C �
t
Tnk�1 � 2Tnk C TnkC1
</p>
<p>h2
: (11.19)
</p>
<p>The right hand side of this equation depends only on temperatures of the previous
time step, since we used an explicit method. Although this might seem advantageous
on a first glance, it turns out that the above scheme is not stable for arbitrary choices
of
t and h. In particular, it is possible to prove that the above discretization is stable
only for
</p>
<p>�
t
</p>
<p>h2
� 1
2
: (11.20)
</p>
<p>A detailed discussion and proof of this property can be found in any advanced
textbook on numerics of PDEs [1&ndash;5].
</p>
<p>On the other hand, if we apply the implicit EULER method (5.10) to solve
Eq. (11.17) we obtain
</p>
<p>TnC1k � Tnk

t
</p>
<p>D �
TnC1k�1 � 2TnC1k C TnC1kC1
</p>
<p>h2
; (11.21)
</p>
<p>which is unconditionally stable. However, Eq. (11.21) is an implicit equation, i.e.
the function values TnC1kC1 and T
</p>
<p>nC1
k�1 are required in order to evaluate T
</p>
<p>nC1
k . Hence,
</p>
<p>Eq. (11.21) has to be solved as a system of linear equations. This system may be
written as
</p>
<p>ATnC1 D Tn C F ; (11.22)</p>
<p/>
</div>
<div class="page"><p/>
<p>11.3 The Time-Dependent Heat Equation 165
</p>
<p>with the vector Tn D .Tn0 ;Tn1 ; : : : ;TnN/T and the tridiagonal matrix A:
</p>
<p>A D
</p>
<p>0
</p>
<p>B
B
B
B
B
B
@
</p>
<p>: : :
: : :
</p>
<p>: : :
</p>
<p>� �
t
h2
1C 2�
t
</p>
<p>h2
� �
t
</p>
<p>h2
</p>
<p>: : :
: : :
</p>
<p>: : :
</p>
<p>1
</p>
<p>C
C
C
C
C
C
A
</p>
<p>: (11.23)
</p>
<p>The boundary conditions are incorporated in the matrix A and in the vector F. (See
Sects. 9.2 and 9.3.) The linear system of equations (11.22) can be solved numerically
using a direct or an iterative method. Employing an iterative method, imposes a third
index t on the function values of the temperature T which accounts for the iteration
step.
</p>
<p>Let us give a brief numerical example. We consider the time-dependent homo-
geneous heat equation (11.16) on a finite interval Œ0;L&#141; together with the boundary
conditions of Sect. 9.1:
</p>
<p>T.0/ D T0; T.L/ D TN : (11.24)
</p>
<p>In addition we introduce the initial condition
</p>
<p>T.x; 0/ D 0; x 2 Œ0;L&#141; : (11.25)
</p>
<p>Figure 11.3 presents the time evolution of T.x; t/ at six different time-steps as
it was obtained with the explicit EULER method (11.19). Here we chose T0 D 0,
TN D 2, N D 20, L D 10, � D 1 as well as 
t � 0:5. Note that for this choice of
parameters, the condition (11.20) is fulfilled since h � 1:05 and therefore
</p>
<p>�
t
</p>
<p>h2
� 0:45 � 1
</p>
<p>2
: (11.26)
</p>
<p>Figure 11.4 corresponds to Fig. 11.3 but now 
t was chosen to be approximately
0:7 and:
</p>
<p>�
t
</p>
<p>h2
� 0:63 &gt; 1
</p>
<p>2
: (11.27)
</p>
<p>Thus, the stability criterion was violated and the solutions became unstable. Finally,
Fig. 11.5 presents results obtained with the same parameters as for Fig. 11.4 but with
the help of the implicit EULER method (11.21). Obviously, this procedure provides
a stable solution of the problem.</p>
<p/>
</div>
<div class="page"><p/>
<p>166 11 Partial Differential Equations
</p>
<p>Fig. 11.3 Solutions of the time-dependent heat equation T.x/ vs x generated by the explicit EULER
method. The stability criterion (11.20) is fulfilled. Results after n D 25, 50, 100, 150, and 300 time
steps are presented. n D 0 represents the initial conditions
</p>
<p>Fig. 11.4 Solutions of the time-dependent heat equation T.x/ vs x generated by the explicit EULER
method. The stability criterion (11.20) is not fulfilled and, therefore, the solution is apparently
unstable. Results after n D 25, 50, 100, 150, and 200 time steps are presented. n D 0 represents
the initial conditions</p>
<p/>
</div>
<div class="page"><p/>
<p>11.4 The Wave Equation 167
</p>
<p>Fig. 11.5 Solutions of the time-dependent heat equation T.x/ vs x generated by the implicit
EULER method. Results after n D 25, 50, 100, 150, and 300 time steps are presented. n D 0
represents the initial conditions
</p>
<p>11.4 The Wave Equation
</p>
<p>As a model hyperbolic PDE we consider briefly the wave equation [16]. Again, we
regard only the one-dimensional case:
</p>
<p>@2
</p>
<p>@t2
u.x; t/ D c2 @
</p>
<p>2
</p>
<p>@x2
u.x; t/ : (11.28)
</p>
<p>Here, c is the speed at which the wave u.x; t/ propagates. Equation (11.28) is to
be augmented by appropriate boundary and initial conditions. A finite difference
approach similar to the one discussed in Sect. 11.3 will be employed and the
discussion will be restricted to the explicit EULER approximation. Consequently,
Eq. (11.28) is replaced by
</p>
<p>un�1k � 2unk C unC1k

t2
</p>
<p>D c2
unk�1 � 2unk C unkC1
</p>
<p>h2
: (11.29)
</p>
<p>We define the parameter � D c
t
h
</p>
<p>and solve Eq. (11.29) for unC1k :
</p>
<p>unC1k D 2.1� �2/unk � un�1k C �2.unk�1 C unkC1/ : (11.30)</p>
<p/>
</div>
<div class="page"><p/>
<p>168 11 Partial Differential Equations
</p>
<p>We note two important points: (i) The solution for time step n C 1 can only be
determined if the solutions for the time steps n and n � 1 are known. In particular,
the solutions for n D 0 and n D 1 are required to obtain the solution for n D 2. The
function values for n D 1 can be obtained from the initial conditions which must
include a first order time derivative of u.x; t/ since Eq. (11.28) is a second order
differential equation with respect to time t. (ii) As in the case of parabolic problems,
the explicit EULER approximation (11.30) will not be stable for arbitrary values of
�. It is only stable for
</p>
<p>� D c
t
h
</p>
<p>� 1 : (11.31)
</p>
<p>This condition is referred to as the COURANT-FRIEDRICHS-LEWY or CFL condition
[17, 18]. Its importance stems from the fact, that this condition is not limited to the
wave equation but holds for hyperbolic problems in general. In particular, since the
wave equation can always be viewed as a combination of a right- and a left-going
advection equation, i.e.
</p>
<p>@
</p>
<p>@t
u.x; t/ D ˙c @
</p>
<p>@x
u.x; t/ ; (11.32)
</p>
<p>we gain the very important property that explicit time integrators applied to solve
equations of the type (11.32) are only stable if relation (11.31) is obeyed.
</p>
<p>Let us return to the discretization (11.30). Suppose we have initial conditions of
the form
</p>
<p>u.x; 0/ D f .x/; @
@t
</p>
<p>u.x; 0/ D g.x/ : (11.33)
</p>
<p>They can be approximated by
</p>
<p>u0k D fk;
u1k � u0k

t
</p>
<p>D gk ; (11.34)
</p>
<p>and the solution of the second relation in (11.34) yields the desired function values
for n D 1:
</p>
<p>u1k D u0k C
tgk : (11.35)
</p>
<p>However, in many cases it is beneficial to take higher order terms into account. This
can be achieved by employing a TAYLOR expansion of the form (Chap. 2):
</p>
<p>u1k � u0k

t
</p>
<p>D @
@t
</p>
<p>u.x; 0/C 
t
2
</p>
<p>@2
</p>
<p>@t2
u.x; 0/C O.
t2/ : (11.36)</p>
<p/>
</div>
<div class="page"><p/>
<p>11.4 The Wave Equation 169
</p>
<p>We make now use of the initial conditions (11.33), employ the wave equa-
tion (11.28), and solve for u1k . This gives
</p>
<p>u1k D u0k C
tgk C

t2c2
</p>
<p>2
f 00k C O.
t3/ : (11.37)
</p>
<p>Here we assumed that the second spatial derivative f 00k D @
2
</p>
<p>@x2
f .xk/ of the initial
</p>
<p>condition f .x/ exists. It may then be approximated by a finite difference approach.
To be specific we consider a vibrating string of length L, which is fixed at its
</p>
<p>ends, i.e. u.0; t/ D u.L; t/ D 0. Furthermore, we assume that the string was initially
at rest, i.e.
</p>
<p>@
</p>
<p>@t
u.x; 0/ D 0 ; (11.38)
</p>
<p>and impose initial conditions
</p>
<p>u.x; 0/ D
</p>
<p>8
</p>
<p>&lt;̂
</p>
<p>:̂
</p>
<p>sin
</p>
<p>�
2�x
</p>
<p>L
</p>
<p>�
</p>
<p>x 2
�
</p>
<p>L
</p>
<p>2
;L
</p>
<p>�
</p>
<p>;
</p>
<p>0 elsewhere.
</p>
<p>(11.39)
</p>
<p>Figure 11.6 presents results obtained with L D 1; c D 2, N D 100. 
t was
chosen in such a way that � D 0:5. On the other hand, Fig. 11.7 presents calculations
</p>
<p>Fig. 11.6 Solutions of the wave equation u.x/ vs x generated by the explicit EULER method with
� D 0:5. Results after n D 25, 50, 100, 150, and 200 time steps are presented. n D 0 represents
the initial conditions</p>
<p/>
</div>
<div class="page"><p/>
<p>170 11 Partial Differential Equations
</p>
<p>Fig. 11.7 Solutions of the wave equation u.x/ vs x generated by the explicit EULER method with
� D 1:01. Results after n D 25, 50, 100, 150, and 200 time steps are presented. n D 0 represents
the initial conditions
</p>
<p>performed with the same parameters but now � was set to 1:01. Thus, the CFL
condition (11.31) was violated and the solutions become unstable.
</p>
<p>In general, the numerical solution of hyperbolic PDEs can be very difficult
to obtain since in many cases these equations represent conservation laws. A
very popular class of methods in this context is referred to as finite volume
methods. A detailed discussion of these methods can be found in the book by
R. J. LEVEQUE [6].
</p>
<p>11.5 The Time-Dependent SCHR&Ouml;DINGER Equation
</p>
<p>We already came across the time-dependent SCHR&Ouml;DINGER equation in Chap. 10.
It reads
</p>
<p>i&bdquo; @
@t
�.x; t/ D H�.x; t/ ; (11.40)
</p>
<p>where &bdquo; is the reduced PLANCK constant, �.x; t/ is the wave function, and H is the
HAMILTON operator. Since the SCHR&Ouml;DINGER equation contains a complex coef-
ficient, it cannot be categorized as a PDE of one of the familiar types, i.e. elliptic,
parabolic or hyperbolic. In fact, the SCHR&Ouml;DINGER equation shows parabolic as
well as hyperbolic behavior (it is of the form of the diffusion equation but allows
for wave solutions). We discuss here briefly a very elegant method developed to</p>
<p/>
</div>
<div class="page"><p/>
<p>11.5 The Time-Dependent SCHR&Ouml;DINGER Equation 171
</p>
<p>numerically approximate solutions of the time-dependent SCHR&Ouml;DINGER equation,
A prominent alternative method, the split operator technique, is briefly explained in
Appendix D.
</p>
<p>We note that Eq. (11.40) has the formal solution
</p>
<p>�.x; t/ D exp
�
</p>
<p>� it&bdquo;H
�
</p>
<p>�.x; 0/ D U.t/�.x; 0/ ; (11.41)
</p>
<p>where we assumed that H is independent of time t. We note that the operator U.t/
on the right hand side of Eq. (11.41) propagates the solution in time. Furthermore, it
is a unitary operator and therefore preserves the norm of the wave-function �.x; t/.
U.t/ is usually referred to as the unitary time-evolution operator [19].2
</p>
<p>We employ relation (11.41) and obtain
</p>
<p>�.x; tC
t/ D exp
�
</p>
<p>� i.t C
t/&bdquo; H
�
</p>
<p>�.x; 0/ D exp
�
</p>
<p>� i
t&bdquo; H
�
</p>
<p>�.x; t/ : (11.42)
</p>
<p>Expanding the exponential in this equation in its series representation and truncating
the series after the second term results in the approximation
</p>
<p>�.x; t C
t/ �
�
</p>
<p>1 � i
t&bdquo; H
�
</p>
<p>�.x; t/ : (11.43)
</p>
<p>Again, we introduce grid-spacing xk D k
x; k 2 N and the correspondingly indexed
functions � nk � �.xk; n
t/ which results in
</p>
<p>� nC1k D
�
</p>
<p>1 � i
t&bdquo; H
�
</p>
<p>� nk : (11.44)
</p>
<p>Using Eq. (10.23) for the Hamiltonian in its position space representation in the one-
dimensional case and by approximating the second derivative with the help of the
central difference approximation we arrive at
</p>
<p>� nC1k D � nk �
i
t
</p>
<p>&bdquo;
</p>
<p>�
</p>
<p>� &bdquo;
2
</p>
<p>2m
</p>
<p>� nk�1 � 2� nk C � nkC1

x2
</p>
<p>C Vk� nk
�
</p>
<p>; (11.45)
</p>
<p>where we defined Vk � V.xk/.
The iteration scheme (11.45) resembles the explicit EULER approxima-
</p>
<p>tion (11.18) of the heat equation with the difference that we have here an
imaginary coefficient. An implicit procedure for the time-dependent SCHR&Ouml;DINGER
</p>
<p>2We remember that unitary means that UU&#142; D U&#142;U D 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>172 11 Partial Differential Equations
</p>
<p>equation (11.40) can be obtained by inversion of Eq. (11.42):
</p>
<p>�.x; t/ D U&#142;.
t/�.x; t C
t/ D exp
�
</p>
<p>i
t
</p>
<p>&bdquo; H
�
</p>
<p>�.x; t C
t/ : (11.46)
</p>
<p>A series expansion of the exponential results in the desired relation:
</p>
<p>� nk D
�
</p>
<p>1C i
t&bdquo; H
�
</p>
<p>� nC1k : (11.47)
</p>
<p>We emphasize that the unitarity of the time-evolution operator is of fundamental
importance since it preserves the norm of the wave-function. However, in truncating
the series representation of the unitary time evolution operator U.
t/ we certainly
violate the unitarity of U.
t/. This problem can be remedied by imposing unitarity
of the time evolution as an additional requirement. This requirement can be
incorporated by normalizing the wave-function after each time step.
</p>
<p>We demonstrate now that the CRANK-NICOLSON scheme [15] can be applied
successfully to solve Eq. (11.40) numerically for a particular potential. The CRANK-
NICOLSON scheme can be obtained by realizing that
</p>
<p>U.
t/ D exp
�
</p>
<p>� i
t&bdquo; H
�
</p>
<p>D exp
�
</p>
<p>� i
t
2&bdquo; H
</p>
<p>�
</p>
<p>exp
</p>
<p>�
</p>
<p>� i
t
2&bdquo; H
</p>
<p>�
</p>
<p>D exp
�
</p>
<p>i
t
</p>
<p>2&bdquo; H
��1
</p>
<p>exp
</p>
<p>�
</p>
<p>� i
t
2&bdquo; H
</p>
<p>�
</p>
<p>D
�
</p>
<p>U&#142;
�

t
</p>
<p>2
</p>
<p>���1
U
</p>
<p>�

t
</p>
<p>2
</p>
<p>�
</p>
<p>: (11.48)
</p>
<p>Hence, we obtain from Eq. (11.45)
</p>
<p>U&#142;
�

t
</p>
<p>2
</p>
<p>�
</p>
<p>� nC1k D U
�

t
</p>
<p>2
</p>
<p>�
</p>
<p>� nk ; (11.49)
</p>
<p>or by expanding U in a series and truncating after the second term
</p>
<p>�
</p>
<p>1C i
t
2&bdquo; H
</p>
<p>�
</p>
<p>� nC1k D
�
</p>
<p>1 � i
t
2&bdquo; H
</p>
<p>�
</p>
<p>� nk : (11.50)</p>
<p/>
</div>
<div class="page"><p/>
<p>11.5 The Time-Dependent SCHR&Ouml;DINGER Equation 173
</p>
<p>Inserting the finite difference approximation of the Hamiltonian H and rearranging
terms yields
</p>
<p>�
</p>
<p>1C i
t
2&bdquo;
</p>
<p>� &bdquo;2
m
x2
</p>
<p>C Vk
��
</p>
<p>� nC1k �
i
t &bdquo;
4m
x2
</p>
<p>�
</p>
<p>� nC1k�1 C � nC1kC1
�
</p>
<p>D Ő nk ; (11.51)
</p>
<p>where we defined Ő nk as
</p>
<p>Ő n
k D
</p>
<p>�
</p>
<p>1 � i
t
2&bdquo;
</p>
<p>� &bdquo;2
m
x2
</p>
<p>C Vk
��
</p>
<p>� nk C
i
t &bdquo;
4m
x2
</p>
<p>�
</p>
<p>� nk�1 C � nkC1
�
</p>
<p>: (11.52)
</p>
<p>Both sides of Eq. (11.51) are now multiplied by i4m
x2=.&bdquo;
t/ and this gives
</p>
<p>� nC1k�1 C 2
�
</p>
<p>i2m
x2
</p>
<p>
t &bdquo; � 1 �
m
x2
</p>
<p>&bdquo;2 Vk
�
</p>
<p>� nC1k C � nC1kC1 D ˝nk ; (11.53)
</p>
<p>where
</p>
<p>˝nk D �� nk�1 C 2
�
</p>
<p>i2m
x2
</p>
<p>
t &bdquo; C 1C
m
x2
</p>
<p>&bdquo;2 Vk
�
</p>
<p>� nk � � nkC1 : (11.54)
</p>
<p>We recognize that Eq. (11.53) establishes a system of linear equations and rewrite
it in matrix form:
</p>
<p>A� nC1 D ˝n : (11.55)
</p>
<p>Here, we defined the vectors � n D
�
</p>
<p>� n0 ; �
n
1 ; : : : ; �
</p>
<p>n
N
</p>
<p>�T
, ˝nD
</p>
<p>�
</p>
<p>˝n0 ;˝
n
1 ; : : : ;˝
</p>
<p>n
N
</p>
<p>�T
</p>
<p>and the tridiagonal matrix
</p>
<p>A D
</p>
<p>0
</p>
<p>B
B
B
B
B
B
@
</p>
<p>: : :
: : :
</p>
<p>: : :
</p>
<p>1 �k 1
: : :
</p>
<p>: : :
: : :
</p>
<p>1
</p>
<p>C
C
C
C
C
C
A
</p>
<p>; (11.56)
</p>
<p>with �k for k D 1; 2; : : : ;N given by
</p>
<p>�k D 2
�
</p>
<p>i2m
x2
</p>
<p>
t &bdquo; � 1 �
m
x2
</p>
<p>&bdquo;2 Vk
�
</p>
<p>; (11.57)
</p>
<p>according to Eq. (11.53).</p>
<p/>
</div>
<div class="page"><p/>
<p>174 11 Partial Differential Equations
</p>
<p>The system (11.56) is solved iteratively. However, in this case we employ a more
elegant ansatz which is allowed for tridiagonal matrices. We set
</p>
<p>� nC1kC1 D ak� nC1k C bnk ; (11.58)
</p>
<p>and apply it to Eq. (11.53). After rearranging terms we arrive at:
</p>
<p>2
</p>
<p>�
</p>
<p>1C m
x
2
</p>
<p>&bdquo;2 Vk �
i2m
x2
</p>
<p>
t &bdquo; �
ak
</p>
<p>2
</p>
<p>�
</p>
<p>� nC1k D � nC1k�1 C bnk �˝nk : (11.59)
</p>
<p>We define
</p>
<p>˛k D 2
�
</p>
<p>1C m
x
2
</p>
<p>&bdquo;2 Vk �
i2m
x2
</p>
<p>
t &bdquo; �
ak
</p>
<p>2
</p>
<p>�
</p>
<p>; (11.60)
</p>
<p>and obtain from Eq. (11.59)
</p>
<p>� nC1k D
1
</p>
<p>˛k
� nC1k�1 C
</p>
<p>bnk �˝nk
˛k
</p>
<p>: (11.61)
</p>
<p>However, due to the ansatz (11.58) we also have
</p>
<p>� nC1k D ak�1� nC1k�1 C bnk�1 ; (11.62)
</p>
<p>which results in the relations
</p>
<p>ak�1 D
1
</p>
<p>˛k
; (11.63)
</p>
<p>and
</p>
<p>bnk�1 D
bnk �˝nk
˛k
</p>
<p>D
�
</p>
<p>bnk �˝nk
�
</p>
<p>ak�1 : (11.64)
</p>
<p>Equation (11.63) leads to the recursion relation
</p>
<p>ak D 2
�
</p>
<p>1C m
x
2
</p>
<p>&bdquo;2 Vk �
i2m
x2
</p>
<p>
t &bdquo;
</p>
<p>�
</p>
<p>� 1
ak�1
</p>
<p>; (11.65)
</p>
<p>and we derive from Eq. (11.64):
</p>
<p>bnk D
bnk�1
ak�1
</p>
<p>C˝nk : (11.66)</p>
<p/>
</div>
<div class="page"><p/>
<p>11.5 The Time-Dependent SCHR&Ouml;DINGER Equation 175
</p>
<p>The remaining question is how to choose a0 and bn0. We impose the boundary
conditions � n0 D 0 and � nN D 0 and derive from Eq. (11.53):
</p>
<p>˝n1 D 2
�
</p>
<p>i2m
x2
</p>
<p>
t &bdquo; � 1 �
m
x2
</p>
<p>&bdquo;2 V1
�
</p>
<p>� nC11 C � nC12 : (11.67)
</p>
<p>A comparison of this equation with the ansatz (11.58), i.e. � nC12 D a1� nC11 C bn1,
reveals that
</p>
<p>a1 D 2
�
</p>
<p>1C m
x
2
</p>
<p>&bdquo;2 V1 �
i2m
x2
</p>
<p>
t &bdquo;
</p>
<p>�
</p>
<p>; (11.68)
</p>
<p>and
</p>
<p>bn1 D ˝n1 : (11.69)
</p>
<p>These expressions are equivalent to a0 D 1 and it is, thus, impossible to
calculate � nC11 from �
</p>
<p>nC1
0 . However, we can determine the function values �
</p>
<p>nC1
k
</p>
<p>via a backward recursion
</p>
<p>� nC1k D
1
</p>
<p>ak
</p>
<p>�
</p>
<p>� nC1kC1 � bnk
�
</p>
<p>; (11.70)
</p>
<p>which is initialized with the boundary condition � nC1N D 0. We can now summarize
the algorithm:
</p>
<p>1. Choose the initial conditions � 0k , k D 0; 1; : : : ;N which satisfy the boundary
conditions � 00 D 0 and � 0N D 0.
</p>
<p>2. Set
</p>
<p>a1 D 2
�
</p>
<p>1C m
x
2
</p>
<p>&bdquo;2 V1 �
i2m
x2
</p>
<p>
t &bdquo;
</p>
<p>�
</p>
<p>; (11.71)
</p>
<p>and calculate for k D 2; : : : ;N � 1
</p>
<p>ak D 2
�
</p>
<p>1C m
x
2
</p>
<p>&bdquo;2 Vk �
i2m
x2
</p>
<p>
t &bdquo;
</p>
<p>�
</p>
<p>� 1
ak�1
</p>
<p>: (11.72)
</p>
<p>3. Start the time loop: n D 0; 1; : : : ;M, with M the maximum number of time steps.
4. Calculate for k D 1; 2; : : : ;N � 1
</p>
<p>˝nk D �� nk�1 C 2
�
</p>
<p>i2m
x2
</p>
<p>
t &bdquo; C 1C
m
x2
</p>
<p>&bdquo;2 Vk
�
</p>
<p>� nk � � nkC1 : (11.73)</p>
<p/>
</div>
<div class="page"><p/>
<p>176 11 Partial Differential Equations
</p>
<p>5. Set
</p>
<p>bn1 D ˝n1 ; (11.74)
</p>
<p>and calculate for k D 2; : : : ;N � 1
</p>
<p>bnk D
bnk�1
ak�1
</p>
<p>C˝nk : (11.75)
</p>
<p>6. Calculate for k D N � 1;N � 2; : : : ; 1
</p>
<p>� nC1k D
1
</p>
<p>ak
</p>
<p>�
</p>
<p>� nC1kC1 � bnk
�
</p>
<p>; (11.76)
</p>
<p>where the boundary conditions � n0 D � nN D 0 are to be considered.
7. Set n D n C 1 and go to step 4.
</p>
<p>The application of this algorithm is now elucidated with the help of a specific
example, the quantummechanical tunneling effect. The initial condition is described
by a GAUSS wave packet
</p>
<p>�.x; 0/ D exp .iqx/ exp
�
</p>
<p>� .x � x0/
2
</p>
<p>2�2
</p>
<p>�
</p>
<p>; (11.77)
</p>
<p>centered at x D x0 which propagates in positive x-direction with momentum q. This
wave-function is not yet normalized. Furthermore, we regard the single potential
barrier
</p>
<p>V1.x/ D
(
</p>
<p>V0 x 2 Œa; b&#141; ;
0 elsewhere;
</p>
<p>(11.78)
</p>
<p>or the double potential barrier
</p>
<p>V2.x/ D
(
</p>
<p>V0 x 2 Œa; b&#141;[ Œc; d&#141; ;
0 elsewhere:
</p>
<p>(11.79)
</p>
<p>The scales and parameters are chosen in the following way: L D 500, 
x D 1,

t D 0:1, m D &bdquo; D 1, x0 D 200, q D 2, � D 20, V0 D 0:7, a D 250, b D 260, c D
300, and d D 310. Figure 11.8 presents the time evolution of the square modulus of
the resulting wave-function j�.x; n
t/j2 vs x (solid line, left hand scale) at different
time steps n D 500, 1000, and 1500. The time step n D 0 corresponds to the initial</p>
<p/>
</div>
<div class="page"><p/>
<p>11.5 The Time-Dependent SCHR&Ouml;DINGER Equation 177
</p>
<p>Fig. 11.8 Time evolution of the square modulus of the wave-function j .x/j2 vs x (solid line, left
hand scale). The potential V.x/ D V1.x/ is also plotted vs x (dashed line, right hand scale). We
present the results for n D 500, 1000, and 1500 time steps. The graph labeled by n D 0 represents
the initial configuration
</p>
<p>condition. The potential V1.x/ vs x is also plotted (dashed line, right hand scale).
Figure 11.9 corresponds to Fig. 11.8 but now the potential is described by V2.x/ and
additional time steps for n D 2000 and 2500 have been added.
</p>
<p>In both figures a typical quantum mechanical effect which is referred to as
tunneling can be observed. In particular, there exists a finite probability that the
potential barrier can be crossed, although, from a classical point of view, the
particle&rsquo;s energy is not sufficient to overcome the barrier. A detailed discussion of
this effect and its technological importance can be found in any standard textbook
on quantum mechanics [19&ndash;21].
</p>
<p>In conclusion we remark that a very prominent method to solve numerically the
time-dependent SCHR&Ouml;DINGER equation is based on the FOURIER transformation.
The numerical implementation of the FOURIER transformation as well as its
application to the SCHR&Ouml;DINGER equation is briefly discussed in Appendix D.</p>
<p/>
</div>
<div class="page"><p/>
<p>178 11 Partial Differential Equations
</p>
<p>Fig. 11.9 Time evolution of the square modulus of the wave-function j .x/j2 vs x (solid line, left
hand scale). The potential V.x/ D V2.x/ is also plotted vs x (dashed line, right hand scale). We
present the results for n D 500, 1000, 1500, 2000, and 2500 time steps. The graph labeled by
n D 0 represents the initial configuration
</p>
<p>Summary
</p>
<p>This chapter was about linear PDEs and how to find solutions numerically. The
dominating theme was the application of the various finite difference methods.
The two-dimensional POISSON equation served as an example for an elliptic PDE.
The algorithm to solve this equation developed here was based on the central
difference derivative. Parabolic PDEs were represented by the time-dependent one-
dimensional heat equation. The numerical solution proved to be possible by either
using an explicit or an implicit EULER scheme. For the explicit EULER scheme the
appropriate choice of time and space discretization proved to be essential for the
stability of the algorithm. The one-dimensional wave equation was introduced as
an example of a hyperbolic PDE. The solution was found by employing an explicit</p>
<p/>
</div>
<div class="page"><p/>
<p>References 179
</p>
<p>EULER approximation. Again time and space discretization had to follow a specific
stability criterion, the COURANT-FRIEDRICHS-LEWY condition. Finally, the one-
dimensional time-dependent SCHR&Ouml;DINGER equation was studied. It does not fit
into any of the above categories. The algorithm to find a numerical solution was
developed here on the basis of a CRANK-NICOLSON scheme and it was tested with
the quantum mechanical tunneling effect.
</p>
<p>Problems
</p>
<p>1. Write a program which solves the two-dimensional POISSON equation for an
arbitrary charge density distribution �.x; y/. Use the numerical method discussed
in Sect. 11.2.
</p>
<p>a. Impose DIRICHLET boundary conditions '.x; 0/ D '.x;Ly/ D '.0; y/ D
'.Lx; y/ D 0 as described in Sect. 11.2. Test the program by first reproducing
Fig. 11.2.
</p>
<p>b. Solve the POISSON equation for different charge densities of your choice.
c. Calculate the electric field E.x; y/ with the help of Eq. (11.3).
</p>
<p>2. Calculate the time evolution of the temperature distribution T.x; t/ along a
cylindrical rod described in Sect. 9.3. The rod is kept at constant temperatures
T0 and TN at its ends. The parameters used in Sect. 9.3 stay unchanged. Study
also the case of a heat sink as suggested in the Problems section of Chap. 9.
</p>
<p>3. Calculate the time evolution of the square modulus of the wave-function j .x/j2
vs x for a potential V1.x/ according to Eq. (11.78) with V0 &lt; 0 (quantum well).
In a second step, modify the potential according to
</p>
<p>V.x/ D
</p>
<p>8
</p>
<p>ˆ̂
&lt;
</p>
<p>ˆ̂
:
</p>
<p>V1 x 2 Œa; b&#141;[ Œc; d&#141;
V2 x 2 Œb; c&#141;
0 elsewhere;
</p>
<p>with V1 &gt; 0, V2 &lt; 0, and jV1j &lt; jV2j.
</p>
<p>References
</p>
<p>1. Lapidus, L., Pinder, G.F.: Numerical Solution of Partial Differential Equations. Wiley,
New York (1982)
</p>
<p>2. Morton, K.W., Mayers, D.F.: Numerical Solution of Partial Differential Equations. Cambridge
University Press, Cambridge (2005)
</p>
<p>3. Li, T., Qin, T.: Physics and Partial Differential Equations, vol. 1. Cambridge University Press,
Cambridge (2012)</p>
<p/>
</div>
<div class="page"><p/>
<p>180 11 Partial Differential Equations
</p>
<p>4. Li, T., Qin, T.: Physics and Partial Differential Equations, vol. 2. Cambridge University Press,
Cambridge (2014)
</p>
<p>5. Lui, S.H.: Numerical Analysis of Partial Differential Equations. Wiley, New York (2012)
6. LeVeque, R.J.: Finite Volume Methods for Hyperbolic Problems. Cambridge Texts in Applied
</p>
<p>Mathematics. Cambridge University Press, Cambridge (2002)
7. Gockenbach, M.S.: Understanding and Implementing the Finite Element Method. Cambridge
</p>
<p>University Press, Cambridge (2006)
8. Selvadurai, A.: Partial Differential Equations inMechanics, vol. 2. Springer, Berlin/Heidelberg
</p>
<p>(2000)
9. Sleeman, B.D.: Partial differential equations, poisson equation. In: Dubitzky, W.,Wolkenhauer,
</p>
<p>O., Cho, K.H., Yokota, H. (eds.) Encyclopedia of Systems Biology, pp. 1635&ndash;1638. Springer,
Berlin/Heidelberg (2013)
</p>
<p>10. Jackson, J.D.: Classical Electrodynamics, 3rd edn. Wiley, New York (1998)
11. Greiner, W.: Classical Electrodynamics. Springer, Berlin/Heidelberg (1998)
12. Polyanin, A.D.: Handbook of Linear Partial Differential Equations for Engineers and Scien-
</p>
<p>tists. Chapman &amp; Hall/CRC, Boca Raton (2002)
13. Cannon, J.R.: The One-Dimensional Heat Equation. Encyclopedia of Mathematics and Its
</p>
<p>Applications. Cambridge University Press, Cambridge (1985)
14. Carslaw, H.S., Jaeger, J.C.: Conduction of Heat in Solids, 2nd edn. Oxford University Press,
</p>
<p>Oxford (1986)
15. Crank, J., Nicolson, P.: A practical method for numerical evaluation of solutions of partial
</p>
<p>differential equations of the heat-conduction type. Proc. Camb. Philos. Soc. 43, 50&ndash;67 (1947).
doi:10.1017/S0305004100023197
</p>
<p>16. Zwillinger, D.: Handbook of Differential Equations, 3nd edn. Academic, San Diego (1997)
17. Courant, R., Friedrichs, K., Lewy, H.: On the partial difference equations of mathematical
</p>
<p>physics. IBM J. Res. Dev. 11, 215&ndash;234 (1967 [1928])
18. Bakhvalov, N.S.: Courant-friedrichs-lewy condition. In: Hazewinkel, M. (ed.) Encyclopaedia
</p>
<p>of Mathematics. Springer, Berlin/Heidelberg (1994)
19. Sakurai, J.J.: Modern Quantum Mechanics. Addison-Wesley, Menlo Park (1985)
20. Baym, G.: Lectures on Quantum Mechanics. Lecture Notes and Supplements in Physics. The
</p>
<p>Benjamin/Cummings Publ. Comp., Inc., London/Amsterdam (1969)
21. Cohen-Tannoudji, C., Diu, B., Lalo&euml;, F.: Quantum Mechanics, vol. I. Wiley, New York (1977)</p>
<p/>
</div>
<div class="page"><p/>
<p>Part II
Stochastic Methods</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 12
Pseudo-random Number Generators
</p>
<p>12.1 Introduction
</p>
<p>Stochastic methods in Computational Physics are based on the availability of
random numbers and on the concepts of probability theory. (Readers not familiar
with the basic concepts of probability theory are highly encouraged to study
Appendix E before proceeding.) The required random numbers are provided by
numerical random number generators and, thus, we have to speak, more precisely,
of pseudo-random numbers. Let us now motivate the problem and discuss some
preliminary items.
</p>
<p>A popular example of randomness in physical systems is certainly the outcome
of a dice-throw or the drawing of lotto numbers. Even though the outcome of a
dice-throw is completely determined by the initial conditions, it is effectively unpre-
dictable because the initial conditions cannot be determined accurately enough. A
probabilistic description, which assigns the random variables 1; 2; 3; 4; 5; and 6 a
probability of 1=6, respectively, is much more convenient and promising. It has to
be kept in mind, of course, that all predictions obtained on the basis of such an
approach are also clearly probabilistic in their nature.
</p>
<p>Another example is Brownian motion or diffusion which describes the random
motion of dust particles on a fluid surface. It is in this case particularly obvious
that a description with the help of a stochastic differential equation, such as the
LANGEVIN equation [1], is completely sufficient and more to the point than a
description based on the dynamics of a large number of interacting particles.
</p>
<p>Stochastic methods are not confined to physics: They are applied very success-
fully in many other fields of expertise, like biology [2], economics [3, 4], medicine
[5], etc. Finally, an interesting and purely mathematical application can be found in
the evaluation of integrals as an alternative to the methods discussed in Chap. 3. This
method is referred to as Monte-Carlo integration and will be addressed in Chap. 14
together with a basic introduction to stochastics and its applications in physics.
</p>
<p>&copy; Springer International Publishing Switzerland 2016
B.A. Stickler, E. Schachinger, Basic Concepts in Computational Physics,
DOI 10.1007/978-3-319-27265-8_12
</p>
<p>183</p>
<p/>
</div>
<div class="page"><p/>
<p>184 12 Pseudo-random Number Generators
</p>
<p>From the numerical point of view, there is one common denominator to all
these applications: Random numbers are an essential tool and, consequently, so are
random number generators. Thus, a closer inspection of randomness in general and
the generation of random numbers or sequences in particular is required.We have to
explain what we understand by randomness and how it can be measured. Moreover,
based on this discussion we have to formulate requirements to be imposed on the
random number generators to deliver useful random numbers.
</p>
<p>Although, we might have an intuitive picture of randomness it is hard to
formulate without relying on mathematics. For instance, consider the sequence s1
which consists of N elements:
</p>
<p>s1 D 0; 0; 0; 0; 0; : : : ; N elements. (12.1)
</p>
<p>Is it random? The question cannot be answered without further information.
Suppose, the numbers in sequence s1 were drawn from some set S . If this set is
of the form
</p>
<p>S1 D f0g ; (12.2)
</p>
<p>then the above sequence s1 is certainly not random since there is only one possible
outcome. However, suppose the numbers of the sequence s1 were drawn from the
set S2
</p>
<p>S2 D f0; 1g ; (12.3)
</p>
<p>with the events 0 and 1 together with assigned probabilities P.0/ and P.1/. These
probabilities describe the probability that the outcome of a measurement on the set
S2 yields either the event 0 or 1, respectively. For instance, in the case of tossing a
coin the event 0 may correspond to heads while 1 stands for tails. (To register this
result is, within this context, a measurement.) In this case the probabilities are given
by P.0/ D P.1/ D 1=2 under the premise that the coin is perfectly ideal and has
not been manipulated. Even, if we know that the coin has not been manipulated,
sequence (12.1) is still a possible outcome, although it is rather improbable for a
large number N of measurements (repeated tosses of the coin).
</p>
<p>A literal definition of randomness within the context of a random sequence was
given by G. J. CHAITIN [6]:
</p>
<p>[. . . ] a series of numbers is random if the smallest algorithm capable of specifying it to a
computer has about the same number of bits of information as the series itself.
</p>
<p>This definition seems to include the most important features of randomness
which we are used to from our experience. Since, no universal trend is observable,
reproducing the sequence requires the knowledge of every single constituent. Hence,
one may employ the sloppy definition: Randomness is the lack of a universal trend.
</p>
<p>But how can we test whether or not a given sequence really follows a certain dis-
tribution? Of course, one can simply exploit the statistical definition of probability,</p>
<p/>
</div>
<div class="page"><p/>
<p>12.1 Introduction 185
</p>
<p>Appendix, Eq. (E.4): The probability of a certain outcome is measured by counting
the particular results. This procedure seems to be quite promising, however, it has
to be kept in mind that the statistical definition of probability is only valid in the
limit m ! 1, where m is the number of measurements. Hence, it is fundamentally
impossible to determine whether or not a sequence is random because an infinite
number of elements would have to be evaluated and analyzed. More promising
appears to be the calculation of moments or correlations (see Appendix, Sects. E.2
and E.10) from the sequence and to compare such a result with known values
for real random numbers. These statistical tests will be discussed in Sect. 12.3.
If we consider the sequence (12.1) drawn from the set S2, [Eq. (12.3), uniform
distribution assumed] we can deduce that it is a very improbable result for large
N, although it is certainly a possible outcome. Methods based on this train of
thoughts are known as hypothesis testings and we discuss the �2-test as a simple
representative of such tests in Sect. 12.3.
</p>
<p>We are now in a position to clarify what we understand by a random number:We
regard a random sequence drawn from the set
</p>
<p>S3 D f0; 1; 2; 3; 4; 5; 6; 7; 8; 9g : (12.4)
</p>
<p>If the random number is to be uniformly distributed, we assign probabilities P.k/ D
1=10, k D 0; 1; : : : ; 9 and if we would like to obtain a random number out of the
interval
</p>
<p>˝1 D Œ0; 1/ ; (12.5)
</p>
<p>we may simply draw the sequence s2 D a1; a2; a3; : : : from S3 and compose the
random number r as
</p>
<p>r D 0:a1a2a3 : : : : (12.6)
</p>
<p>Section 12.2 is dedicated to the discussion of different methods of how to
generate so called pseudo-random numbers. A pseudo-random number is a number
generated with the help of a deterministic algorithm, however, it shows a behavior
as if it were random. This implicates that its statistical properties are close to that of
true random numbers. In contrast to pseudo-random numbers, real random numbers
are truly random. A real random number can be obtained from experiments. One
could, for instance, simply toss a coin and register the resulting sequence of zeros
and ones. A more sophisticated method is to exploit the radioactive decay of a
nucleus, which is believed to be purely stochastic. There are also more exotic
ideas, such as using higher digits of � which are assumed to behave as if they
were random. However, all these methods have in common that they are far too
slow for computational purposes. Moreover, an experimental approach is obviously
not reproducible in the sense, that a random sequence cannot be reproduced on
demand, but the reproducibility of a random number sequence is essential for many
applications.</p>
<p/>
</div>
<div class="page"><p/>
<p>186 12 Pseudo-random Number Generators
</p>
<p>This leads us to the formulation of several criteria a random number generator
will have to comply with. It should
</p>
<p>&bull; produce pseudo-random numbers whose statistical properties are as close as
possible to those of real random numbers.
</p>
<p>&bull; have a long period: It should generate a non-repeating sequence of random
numbers which is sufficiently long for computational purposes.
</p>
<p>&bull; be reproducible in the sense defined above, as well as restartable from an arbitrary
break-point.
</p>
<p>&bull; be fast and parallelizable: It should not be the limiting component in simulations.
</p>
<p>We restrict, within this chapter, our discussion to random numbers that are
uniformly distributed over a finite set. Thus, we assign to all possible outcomes of
a measurement the same probability. The generation of non-uniformly distributed
random numbers from uniformly distributed random numbers is not a difficult task
and will be discussed in more detail in Chap. 13.
</p>
<p>12.2 Different Methods
</p>
<p>We discuss here different types of pseudo-random number generators [7&ndash;9] which
generate a pseudo-random number r which is uniformly distributed within the
interval Œ0; 1/. Hence, its probability density function (pdf) is given by
</p>
<p>p.r/ D
(
</p>
<p>1 r 2 Œ0; 1/ ;
0 elsewhere;
</p>
<p>(12.7)
</p>
<p>and from this follows the cumulative distribution function (cdf; see Appendix E):
</p>
<p>P.r/ D
Z r
</p>
<p>0
</p>
<p>dr0p.r0/ D
</p>
<p>8
</p>
<p>ˆ̂
&lt;
</p>
<p>ˆ̂
:
</p>
<p>0 r &lt; 0 ;
</p>
<p>r 0 � r &lt; 1 ;
1 r � 1 :
</p>
<p>(12.8)
</p>
<p>We introduce here only some of the most basic concepts for pseudo-random
number generators. However, in huge simulations based on random numbers
standard pseudo-random number generators provided by the various compilers may
not be sufficient due to their rather short period and bad statistical properties. In
this case it is, therefore, recommended to consult the literature [7, 10] and use more
advanced techniques in order to obtain reliable results.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.2 Different Methods 187
</p>
<p>Linear Congruential Generators
</p>
<p>Linear congruential generators are the simplest and most prominent random number
generators. They produce a sequence of integers fxng, n 2 N following the rule
</p>
<p>xnC1 D .axn C c/ mod m ; (12.9)
</p>
<p>where a, c and m are positive integers which obey 0 � a; c &lt; m. Furthermore,
the generator is initialized by its seed x0, which is also a positive (in most cases
odd) integer in the range 0 � x0 &lt; m. The seed is commonly taken from, for
instance, the system time in order to avoid repetition at a restart of the sequence. In
many environments it is, therefore, necessary to fix the seed artificially whenever
one wants to perform reproducible tests.
</p>
<p>We note that the sequence resulting from Eq. (12.9) is bounded to the interval
xn D Œ0;m � 1&#141; and, hence, its maximum period is m. However, the actual period
of the sequence highly depends on the choices of the parameters a, c and m as
well as on the seed x0. In general, linear congruential generators are very fast and
simple, however, they have rather short periods. Moreover, they are very susceptible
to correlations since the value xnC1 is calculated from xn only. (This is obviously a
property which does not apply to real independent random numbers and it should
therefore be eliminated!) In Sect. 12.3 we will discuss some simple methods which
allow to identify such correlations.
</p>
<p>One of the most prominent choices for the parameters in Eq. (12.9) are the PARK-
MILLER parameters [10]:
</p>
<p>a D 75; c D 0; m D 231 � 1 : (12.10)
</p>
<p>Note that one has to be particularly careful when choosing c D 0. It follows from
Eq. (12.9) that if c D 0 and if for any n, xn D 0 one obtains xk D 0 for all k &gt; n.
</p>
<p>The random numbers rn described by the pdf (12.7) are obtained via
</p>
<p>rn D
xn
</p>
<p>m
2 Œ0; 1/ : (12.11)
</p>
<p>Let us briefly discuss two famous improvements which concentrate on the
reduction of correlations and an elongation of the period: The first idea which is
referred to as shuffling [10] includes a second pseudo-random step. One calculates
N numbers rn from Eqs. (12.9) and (12.11) and stores these numbers in an array.
If a random number is needed by the executing program, a second random integer
k 2 Œ1;N&#141; is drawn and the k-th element is taken from this array. In order to avoid
that the same random number is used again, the k-th element of the array is replaced
by a new random number which, again, is calculated from (12.9) and (12.11).</p>
<p/>
</div>
<div class="page"><p/>
<p>188 12 Pseudo-random Number Generators
</p>
<p>The second idea to improve the linear congruential generator is simply to include
more previous elements of the sequence:
</p>
<p>xnC1 D
 
X̀
</p>
<p>kD0
akxn�k
</p>
<p>!
</p>
<p>mod m ; (12.12)
</p>
<p>where ` &gt; 0 and a` &curren; 0. Again, the periodicity depends highly on the choice of
the parameters and on the seed. A specific variation of random number generators
using Eq. (12.12) are the FIBONACCI generators.
</p>
<p>FIBONACCI Generators
</p>
<p>The FIBONACCI sequence is given by
</p>
<p>xnC1 D xn C xn�1; x0 D 0; x1 D 1 ; (12.13)
</p>
<p>which results for n � 1 in
</p>
<p>1; 1; 2; 3; 5; 8; 13; 21; 34; 55; 89; : : : : (12.14)
</p>
<p>Choosing in Eq. (12.12) m D 10, ` D 1 and a0 D a1 D 1 simply leaves the last
digits of the sequence (12.14):
</p>
<p>1; 1; 2; 3; 5; 8; 3; 1; 4; 5; 9; : : : : (12.15)
</p>
<p>This suggests the definition of a pseudo-random number generator based on the
FIBONACCI sequence [11]. It is of the form
</p>
<p>xnC1 D .xn C xn�1/ mod m ; (12.16)
</p>
<p>which, according to our previous discussion, allows a periodicity exceeding m. A
straightforward generalization results in the so called lagged FIBONACCI genera-
tors:
</p>
<p>xnC1 D .xn�p ˝ xn�q/ mod m ; (12.17)
</p>
<p>where p; q 2 N and the operator˝ stands for any binary operation, such as addition,
subtraction, multiplication or some logical operation. Two of the most popular
lagged FIBONACCI generators are the shift register generator and the MARSAGLIA-
ZAMAN generator.
</p>
<p>The shift register generator is based on the exclusive or (XOR; ˚) operation,
which acts on each bit of the numbers xn�p and xn�q. In particular, the recurrence</p>
<p/>
</div>
<div class="page"><p/>
<p>12.2 Different Methods 189
</p>
<p>relation reads
</p>
<p>xn D xn�p ˚ xn�q : (12.18)
</p>
<p>The XOR operation˚ is shown in the following multiplication table:
</p>
<p>a b a ˚ b
0 0 0
</p>
<p>1 0 1
</p>
<p>0 1 1
</p>
<p>1 1 0
</p>
<p>Hence, suppose that the binary representation of xn�p is of the form 01001110 : : :
and for xn�q we have 11110011 : : : Then we get from Eq. (12.18):
</p>
<p>xn�p 0 1 0 0 1 1 1 0 : : :
</p>
<p>xn�q 1 1 1 1 0 0 1 1 : : :
</p>
<p>xn 1 0 1 1 1 1 0 1 : : :
</p>
<p>A very prominent choice is given by p D 250 and q D 103 which yields a
superior periodicity of order O.1075/. The algorithm is initialized with the help
of, for instance, a linear congruential generator.
</p>
<p>In contrast, the MARSAGLIA-ZAMAN generator (subtract-with-borrow scheme)
[12] uses the subtraction operation and may be written by introducing the so called
carry bit 
 as
</p>
<p>
 D xn�p � xn�q � cn�1 ; (12.19)
</p>
<p>where xi 2 Œ0;m&#141; for all i. Then,
</p>
<p>xn D
(
</p>
<p>
 
 � 0 ;

C m 
 &lt; 0 :
</p>
<p>(12.20)
</p>
<p>and cn is obtained via
</p>
<p>cn D
(
</p>
<p>0 
 � 0 ;
1 
 &lt; 0 :
</p>
<p>(12.21)
</p>
<p>For the particular choice p D 10, q D 24 and m D 224 one finds an amazingly large
periodicity of order O.10171/. The random numbers xn are integers in the interval
xn 2 Œ0;m&#141;, hence dividing the random numbers by m gives rn 2 Œ0; 1&#141;.</p>
<p/>
</div>
<div class="page"><p/>
<p>190 12 Pseudo-random Number Generators
</p>
<p>12.3 Quality Tests
</p>
<p>Here, we discuss some tests to check whether or not a given, finite sequence of
numbers xn consists of uniformly distributed random numbers out of the interval
xn 2 Œ0; 1&#141;.1
</p>
<p>Statistical Tests
</p>
<p>Statistical tests are generally the most simple methods to arrive at a first idea of the
quality of a pseudo-random number generator. Statistical tests are typically based
on the calculation of moments or correlations. Since we regard the simplified case
of uniformly distributed, uncorrelated random numbers within the interval Œ0; 1&#141;, the
moments can be calculated immediately from (see Appendix, Sect. E.2)
</p>
<p>˝
</p>
<p>Xk
˛
</p>
<p>D
Z
</p>
<p>dx xkp.x/ D
Z 1
</p>
<p>0
</p>
<p>dx xk D 1
k C 1 ; (12.22)
</p>
<p>for k 2 N. These moments are approximated using the generated finite sequence of
numbers fxngnD1;:::;N via
</p>
<p>˝
</p>
<p>Xk
˛
</p>
<p>� xk D 1
N
</p>
<p>N
X
</p>
<p>nD1
xkn : (12.23)
</p>
<p>As illustrated in Appendix, Sect. E.2, the error of this approximation is of order
</p>
<p>O
</p>
<p>�
</p>
<p>1=
p
</p>
<p>N
�
</p>
<p>and
</p>
<p>˝
</p>
<p>Xk
˛
</p>
<p>D xk C O
�
1p
N
</p>
<p>�
</p>
<p>: (12.24)
</p>
<p>Another method studies correlations (see Appendix, Sects. E.2 and E.10)
between the random numbers of the sequence and compare it with the analytical
result. We obtain for uncorrelated random numbers:
</p>
<p>hXnXnCki D hXni2 D
1
</p>
<p>4
: (12.25)
</p>
<p>1From now on we define quite generally the interval out of which random numbers xn are drawn by
xn 2 Œ0; 1&#141; keeping in mind that this interval depends on the actual method applied. This method
determines whether zero or one is contained in the interval.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.3 Quality Tests 191
</p>
<p>Fig. 12.1 Spectral test for a linear congruential generator. We used the PARK-MILLER parameters,
(a) a D 75, c D 0, and m D 231 � 1, (b) a D 137, c D 0, and m D 211, and plotted N D 103
subsequent pairs .xn; xnC1/ of random numbers. In frame (a) the random numbers evolve nicely
distributed within the unit square, showing no obvious correlations. On the other hand, in frame
(b) subsequent random numbers lie on hyperplanes and, thus, develop correlations: They do not
fill the unit square uniformly
</p>
<p>Another, quite evident test, is the analysis of the symmetry of the distribution. If
Xn 2 Œ0; 1&#141; is uniformly distributed then it follows that .1 � Xn/ 2 Œ0; 1&#141; should also
be uniformly distributed.
</p>
<p>Finally, we discuss a graphical test, known as the spectral test [7]. The spectral
test consists of plotting subsequent random numbers xn vs xnC1 and of visual
inspection of the result. One expects the random numbers to uniformly fill the unit-
square, however, if correlations exist, particular patterns might evolve. We illustrate
this method in Fig. 12.1 where it is applied to a linear congruential generator (12.9)
with two different sets of parameters.
</p>
<p>Hypothesis Testing
</p>
<p>Basically, one could employ different hypothesis tests, such as the KOLMOGOROV-
SMIRNOV test, to test random numbers. These tests are rather basic and are
discussed in numerous books on statistics. In what follows we shall briefly mention
the �2-test; for more advanced techniques we refer the reader to the literature
[13, 14].
</p>
<p>The �2-test tests the pdf directly. One starts by sorting the N elements of the
sequence into a histogram. Suppose we would like to have M bins and, hence,
the width of every bin is given by 1=M. We now count the number of elements
which lie within bin k, i.e. within the interval Œ.k � 1/=M; k=M&#141;, and denote this
number by nk. The histogram array h is given by h D c.n1; n2; : : : ; nM/T where</p>
<p/>
</div>
<div class="page"><p/>
<p>192 12 Pseudo-random Number Generators
</p>
<p>Fig. 12.2 Histograms for
N D 105 , N D 106 and
N D 107 , M D 100 bins as
obtained with the
PARK-MILLER linear
congruential generator,
a D 75 , c D 0 and
m D 231 � 1
</p>
<p>the constant c D M=N normalizes the histogram. In Fig. 12.2 we show three
different histograms for N D 105, N D 106 and N D 107 uniformly distributed
random numbers as obtained with the PARK-MILLER linear congruential generator.
In Fig. 12.3 we present a histogram for N D 107 obtained with the bad linear
congruential generator defined in Fig. 12.1b. We recognize numerous empty bins
which are a clear indication that the random numbers are not uniformly distributed.
</p>
<p>Let us briefly remember some points from probability theory [15, 16]. One can
show, that if numbers Qn are normally distributed random variables, their sum
</p>
<p>x D
�
X
</p>
<p>nD1
Q2n ; (12.26)</p>
<p/>
</div>
<div class="page"><p/>
<p>12.3 Quality Tests 193
</p>
<p>Fig. 12.3 Histogram for N D 107 and M D 100 bins as obtained with a linear congruential
generator with parameters a D 137, c D 0 and m D 211
</p>
<p>follows a �2-distribution where � is the number of degrees of freedom. The pdf of
the �2-distribution is given by
</p>
<p>p.xI �/ D x
�
2�1e�
</p>
<p>x
2
</p>
<p>2
�
2 �
</p>
<p>�
�
2
</p>
<p>� ; x � 0 ; (12.27)
</p>
<p>where � .�/ denotes the � -function. The probability of finding the variable x within
the interval Œa; b&#141; � RC can be calculated as
</p>
<p>P.x 2 Œa; b&#141;I �/ D
Z b
</p>
<p>a
</p>
<p>dx p.xI �/ ; (12.28)
</p>
<p>and in particular for a D 0 we obtain
</p>
<p>P.x &lt; bI �/ D
Z b
</p>
<p>0
</p>
<p>dx p.xI �/ D F.bI �/ : (12.29)
</p>
<p>Here we introduced the cdf F.bI �/. Let us consider the inverse problem: the
probability that x � b is equal to ˛, i.e. F.bI �/ D ˛. We then calculate the upper
bound b by inverting Eq. (12.29) and obtain:
</p>
<p>b D F�1.˛I �/ : (12.30)
</p>
<p>These values are tabulated [17, 18].
We return to our particular example: the hypothesis is that the sequence fxng
</p>
<p>generated by some pseudo-random number generator complies to a uniform distri-
bution. It is a consequence of the central limit theorem (see Appendix, Sect. E.8) that
the deviations from the theoretically expected values nthk obey a normal distribution.</p>
<p/>
</div>
<div class="page"><p/>
<p>194 12 Pseudo-random Number Generators
</p>
<p>We define the variable
</p>
<p>x D �2 D
M
X
</p>
<p>kD1
</p>
<p>.nk � nthk /2
nthk
</p>
<p>: (12.31)
</p>
<p>If our hypothesis is true, �2 follows a �2-distribution with � D M � 1 because the
requirement that the sum of all numbers nk is equal to N reduces the degrees of
freedom by one. We employ relation (12.30) for ˛ D 0:85 and � D 99 and obtain
b D 113. Hence, values �2 &lt; b are very likely if �2 really follows a �2 distribution,
while values �2 &gt; b are unlikely and therefore the hypothesis may require a review.
However, it has to be emphasized that it is fundamentally impossible to verify
a hypothesis. It can only be falsified or strengthened. We note that the resulting
value of �2 will highly depend on the seed number of the generator as long as the
maximum period has not been reached.
</p>
<p>Summary
</p>
<p>We first concentrated on a possible definition of randomness and on a mathematical
definition of random numbers and sequences. As the generation of random numbers
was the main topic of this section we moved on to describe the requirements an
&lsquo;ideal&rsquo; random number generator will have to obey. On the other hand, the numerics
of computational physics demanded reproducible sequences of random numbers
and this resulted in the notion of &lsquo;pseudo&rsquo; random numbers which will be generated
by deterministic methods and, thus, cannot possibly be &lsquo;ideal&rsquo;. A number of rather
simple but quite effective pseudo-random number generators was discussed before
the question of how to test the quality (randomness) of these numbers was raised.
We discussed statistical tests and demonstrated the simple spectral test using a linear
congruential generator. More sophisticated is the method of quality testing. The
histogram technique as a direct test for the probability density function from which
the random numbers are drawn was discussed in detail. Finally, some basics of the
�2-test have been presented.
</p>
<p>Problems
</p>
<p>1. Write the computer code for a linear congruential generator. This generator is
described by
</p>
<p>xjC1 D .axj C c/ mod m :</p>
<p/>
</div>
<div class="page"><p/>
<p>References 195
</p>
<p>The random numbers rj 2 Œ0; 1&#141; can be obtained by normalizing xj as was
discussed in Sect. 12.2. Use the following parameters
</p>
<p>a. a D 16807 ; c D 0 ; m D 231 � 1 ; x0 D 3141549,
b. a D 5 ; c D 0 ; m D 27 ; x0 D 1.
</p>
<p>2. Perform the following analysis:
</p>
<p>a. Compute the mean hri and the variance var .r/ for random numbers generated
in N steps. Plot the result.
</p>
<p>b. Plot two successive random numbers rk versus rkC1 for k D 1; 2; : : : ;N � 1 in
a two dimensional plot.
</p>
<p>c. Repeat the above steps for random numbers generated by your system&rsquo;s
software.
</p>
<p>d. Discuss the results!
</p>
<p>References
</p>
<p>1. Coffey, W.T., Kalmykov, Y.P.: The Langevin Equation, 3rd edn. World Scientific Series in
Contemporary Chemical Physics: Volume 27. World Scientific, Hackensack (2012)
</p>
<p>2. Dubitzky, W., Wolkenhauer, O., Cho, K.H., Yokota, H. (eds.): Encyclopedia of Systems
Biology, p. 1596. Springer, Berlin/Heidelberg (2013)
</p>
<p>3. Tapiero, C.S.: Risk and Financial Management: Mathematical and Computational Methods.
Wiley, New York (2004)
</p>
<p>4. Lax, M., Cai, W., Xu, M.: Random Processes in Physics and Finance. Oxford Finance Series.
Oxford University Press, Oxford (2013)
</p>
<p>5. Laing, C., Lord, G.J. (eds.): Stochastic Methods in Neuroscience. Oxford University Press,
Oxford (2009)
</p>
<p>6. Chaitin, G.J.: Randomness and mathematical proof. Sci. Am. 232, 47 (1975)
7. Knuth, D.: The Art of Computer Programming, vol. II, 3rd edn. Addison Wesley, Menlo Park
</p>
<p>(1998)
8. Gentle, J.E.: Random Number Generation and Monte Carlo Methods. Statistics and Comput-
</p>
<p>ing. Springer, Berlin/Heidelberg (2003)
9. Ripley, B.D.: Stochastic Simulation. Wiley, New York (2006)
10. Press, W.H., Teukolsky, S.A., Vetterling, W.T., Flannery, B.P.: Numerical Recipes in C++, 2nd
</p>
<p>edn. Cambridge University Press, Cambridge (2002)
11. Knuth, D.: The Art of Computer Programming, vol. IV. Addison Wesley, Menlo Park (2011)
12. Marsaglia, G., Zaman, A.: A new class of random number generators. Ann. Appl. Prob. 1,
</p>
<p>462&ndash;480 (1991)
13. Iversen, G.P., Gergen, I.: Statistics. Springer Undergraduate Textbooks in Statistics. Springer,
</p>
<p>Berlin/Heidelberg (1997)
14. Keener, R.W.: Theoretical Statistics. Springer, Berlin/Heidelberg (2010)
15. Chow, Y.S., Teicher, H.: Probability Theory, 3rd edn. Springer Texts in Statistics. Springer,
</p>
<p>Berlin/Heidelberg (1997)
16. Kienke, A.: Probability Theory. Universitext. Springer, Berlin/Heidelberg (2008)
17. Abramovitz, M., Stegun, I.A. (eds.): Handbook of Mathemathical Functions. Dover, New York
</p>
<p>(1965)
18. Olver, F.W.J., Lozier, D.W., Boisvert, R.F., Clark, C.W.: NIST Handbook of Mathematical
</p>
<p>Functions. Cambridge University Press, Cambridge (2010)</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 13
Random Sampling Methods
</p>
<p>13.1 Introduction
</p>
<p>Most applications require random number generators that follow a particular
probability density function (pdf) which is not a uniform distribution on the interval
Œ0; 1&#141;. We present in this chapter methods to generate random numbers that follow
some arbitrary pdf. As a source will serve uniformly distributed random numbers as
they are generated with the help of the methods we discussed in Chap. 12.
</p>
<p>The two most prominent techniques to generate random numbers from an
arbitrary distribution, are the inverse transformation method and the rejection
method. They will be discussed in Sects. 13.2 and 13.3, respectively. In addition,
we comment in Sect. 13.4 briefly on sampling from piecewise defined pdfs and
combined pdfs. It has to be emphasized that these methods are in many cases not
sufficient and a more powerful approach is required. One of these is based on the
idea of importance sampling and is referred to as the METROPOLIS method. It will
be discussed briefly in Chap. 14.
</p>
<p>Nevertheless, it is also possible to obtain quite easily random numbers for some
specific pdfs by direct sampling [1]. For instance, suppose x1; x2 are two uniformly
distributed random numbers. Hence, their pdf is given by
</p>
<p>pu.x/ D
(
</p>
<p>1 x 2 Œ0; 1&#141; ;
0 elsewhere.
</p>
<p>(13.1)
</p>
<p>and the corresponding cumulative distribution function (cdf) follows:
</p>
<p>Pu.x/ D
Z x
</p>
<p>0
</p>
<p>d x0pu.x0/ D
</p>
<p>8
</p>
<p>ˆ̂
&lt;
</p>
<p>ˆ̂
:
</p>
<p>0 x &lt; 0 ;
</p>
<p>x x 2 Œ0; 1&#141; ;
1 x &gt; 1 :
</p>
<p>(13.2)
</p>
<p>&copy; Springer International Publishing Switzerland 2016
B.A. Stickler, E. Schachinger, Basic Concepts in Computational Physics,
DOI 10.1007/978-3-319-27265-8_13
</p>
<p>197</p>
<p/>
</div>
<div class="page"><p/>
<p>198 13 Random Sampling Methods
</p>
<p>One can prove that the new random number y
</p>
<p>y D max.x1; x2/ ; (13.3)
</p>
<p>conforms to the cdf
</p>
<p>F.y/ D y2 ; (13.4)
</p>
<p>and to the pdf1:
</p>
<p>f .y/ D 2y : (13.5)
</p>
<p>The consequence is an elegant method to generate random numbers z which follow
the pdf
</p>
<p>g.z/ D kzk�1 ; (13.6)
</p>
<p>by defining
</p>
<p>z D max.x1; x2; : : : ; xk/ : (13.7)
</p>
<p>Here, the random numbers xi are uniformly distributed and can be obtained with the
help of the methods introduced in Chap. 12.
</p>
<p>Another equally elegant method can be employed to calculate random numbers
which follow a normal distribution:
</p>
<p>p.z/ D 1p
2�
</p>
<p>exp
</p>
<p>�
</p>
<p>� z
2
</p>
<p>2
</p>
<p>�
</p>
<p>: (13.8)
</p>
<p>Again, we act on the assumption that the random numbers xi are uniformly
distributed within the unit interval Œ0; 1&#141;. We take two random numbers .x1; x2/ and
construct two random numbers .z1; z2/ using the transformation:
</p>
<p>z1 D cos.2�x2/
p
</p>
<p>�2 ln x1; z2 D sin.2�x2/
p
</p>
<p>�2 ln x1 : (13.9)
</p>
<p>1This follows from the transformation of pdfs (see Chap. 14):
</p>
<p>f .y/ D
Z 1
</p>
<p>0
</p>
<p>d x1
</p>
<p>Z 1
</p>
<p>0
</p>
<p>d x2 ıŒy �max.x1; x2/&#141; D 2y:</p>
<p/>
</div>
<div class="page"><p/>
<p>13.1 Introduction 199
</p>
<p>It is easy to demonstrate that .z1; z2/ follow the pdf (13.8). We introduce the
joint distribution pu.x1; x2/ D pu.x1/pu.x2/ (assumption of no correlations). The
transformation of probabilities [2&ndash;4] gives
</p>
<p>p.z1; z2/dz1dz2 D pu.x1; x2/dx1dx2 ; (13.10)
</p>
<p>or, equivalently, the JACOBIAN determinant
</p>
<p>p.z1; z2/ D
@.x1; x2/
</p>
<p>@.z1; z2/
; (13.11)
</p>
<p>where we employed Eq. (13.1). We recognize that Eq. (13.9) is equivalent to
</p>
<p>x1 D exp
�
</p>
<p>� z
2
1 C z22
2
</p>
<p>�
</p>
<p>; x2 D
1
</p>
<p>2�
tan�1
</p>
<p>�
z2
</p>
<p>z1
</p>
<p>�
</p>
<p>: (13.12)
</p>
<p>The JACOBIAN determinant is readily evaluated and gives2:
</p>
<p>@.x1; x2/
</p>
<p>@.z1; z2/
D
ˇ
ˇ
ˇ
ˇ
ˇ
</p>
<p>@x1
@z1
</p>
<p>@x1
@z2
</p>
<p>@x2
@z1
</p>
<p>@x2
@z2
</p>
<p>ˇ
ˇ
ˇ
ˇ
ˇ
</p>
<p>D
ˇ
ˇ
ˇ
ˇ
ˇ
</p>
<p>�z1x1 �z2x1
� z2
2�.z21Cz22/
</p>
<p>z1
</p>
<p>2�.z21Cz22/
</p>
<p>ˇ
ˇ
ˇ
ˇ
ˇ
</p>
<p>D x1
2�
</p>
<p>D 1
2�
</p>
<p>exp
</p>
<p>�
</p>
<p>� z
2
1 C z22
2
</p>
<p>�
</p>
<p>D p.z1/p.z2/ : (13.13)
</p>
<p>This is the product of two normal distributions and, thus, z1 and z2 follow indeed a
normal distribution.
</p>
<p>2We make use of:
</p>
<p>d
</p>
<p>dx
tan�1.x/ D 1
</p>
<p>1C x2 :</p>
<p/>
</div>
<div class="page"><p/>
<p>200 13 Random Sampling Methods
</p>
<p>13.2 Inverse Transformation Method
</p>
<p>The inverse transformation method is one of the simplest and most useful methods
to sample random variables from an arbitrary pdf [1, 5&ndash;7]. Let p.x/, x 2 Œxmin; xmax&#141;,
denote the pdf fromwhich we want to obtain our random numbers. The correspond-
ing cdf will be denoted by
</p>
<p>P.x/ D
Z x
</p>
<p>xmin
</p>
<p>d x0p.x0/ : (13.14)
</p>
<p>It follows immediately from the positivity and the normalization condition of pdfs
(Appendix, Sect. E.5) that P.x/ is monotonically increasing and, furthermore, that
P.xmin/ D 0 and P.xmax/ D 1. Let � denote some random number uniformly
distributed within the interval Œ0; 1&#141;. We obtain from the conservation of probability
[2&ndash;4]
</p>
<p>pu.�/d� D p.x/d x H) 1 D pu.�/ D p.x/
�
d�
</p>
<p>d x
</p>
<p>��1
; (13.15)
</p>
<p>which can be solved by the choice � D P.x/, since
</p>
<p>d
</p>
<p>d x
P.x/ D p.x/ : (13.16)
</p>
<p>Hence, we arrive at
</p>
<p>x D P�1.�/ (13.17)
</p>
<p>where P�1 denotes the inverse of P. It is an obvious caveat of this method
that it requires the inverse P�1.�/ to exist and that P.x/ must be calculated and
inverted analytically. This is, for instance, not possible in the case of the normal
distribution (13.8).
</p>
<p>Let us illustrate this method with two simple examples:
</p>
<p>1. Suppose we want to draw random numbers which are uniformly distributed
within the interval Œa; b&#141;. The corresponding pdf reads
</p>
<p>p.x/ D 1
b � a ; (13.18)
</p>
<p>and the cdf takes on the form
</p>
<p>P.x/ D x � a
b � a ; (13.19)</p>
<p/>
</div>
<div class="page"><p/>
<p>13.2 Inverse Transformation Method 201
</p>
<p>where we set, in this particular example, xmin D a. Hence, we have
</p>
<p>� D x � a
b � a ; (13.20)
</p>
<p>which is uniformly distributed within Œ0; 1&#141;. Consequently, we determine random
numbers x 2 Œa; b&#141; uniformly distributed via
</p>
<p>x D a C .b � a/� : (13.21)
</p>
<p>2. We are interested in randomnumbers x drawn from a pdf given by the exponential
distribution:
</p>
<p>p.x/ D 1
�
exp
</p>
<p>�
</p>
<p>� x
�
</p>
<p>�
</p>
<p>: (13.22)
</p>
<p>Here, � &gt; 0 and x 2 Œ0;1/. These could, for instance, describe the free path
x of a particle between interactions, where the mean free path hxi D �. From
Eq. (13.17) we obtain
</p>
<p>� D 1
�
</p>
<p>Z x
</p>
<p>0
</p>
<p>d x0 exp
</p>
<p>�
</p>
<p>�x
0
</p>
<p>�
</p>
<p>�
</p>
<p>D 1 � exp
�
</p>
<p>� x
�
</p>
<p>�
</p>
<p>; (13.23)
</p>
<p>and consequently the relation
</p>
<p>x D �� ln.1 � �/ ; (13.24)
</p>
<p>gives random variables x which comply to the exponential distribution (13.22) if
� follows the pdf pu.�/ of Eq. (13.1). Moreover, it follows from the symmetry of
the uniform distribution that
</p>
<p>x D �� ln.�/ ; (13.25)
</p>
<p>without affecting the resulting random numbers. In Fig. 13.1 we show a
histogram with random numbers drawn according to (13.25).
</p>
<p>We pointed out already that it is certainly a caveat of this method that the cdf
P.x/ has to be calculated and inverted analytically. Even if P.x/ is not analytically
invertible, it is possible to employ the inverse transformation method by calculating
P.x/ for certain grid-points xi and then interpolating P.x/ piecewise between
these points with the help of an invertible function. However, in many cases it is
advantageous to employ the rejection method, which will be discussed next.</p>
<p/>
</div>
<div class="page"><p/>
<p>202 13 Random Sampling Methods
</p>
<p>Fig. 13.1 The histogram representation of the pdf p.x/ vs x generated by random numbers drawn
from an exponential distribution Eq. (13.22) with the help of the inverse transformation sampling
method. Two different values for � have been considered, namely (a) � D 1 and (b) � D 5.
N D 105 random numbers have been sampled. The solid line corresponds to the pdf p.x/ according
to Eq. (13.22)
</p>
<p>13.3 Rejection Method
</p>
<p>The rejection method is particularly suitable if the inverse transformation method
fails [1, 6, 7]. One of the most prominent versions of the rejection method is the
METROPOLIS algorithm. It will be introduced in Sect. 14.3.
</p>
<p>The basic idea of the rejection method is to draw random numbers x from another,
preferably analytically invertible pdf h.x/ and check whether or not they lie within
the desired pdf p.x/. If this is the case the random number x is accepted, otherwise
it will be rejected. This is also the basic idea of the hit and miss version of Monte-
Carlo integration which will be discussed in Sect. 14.2.
</p>
<p>We specify the rejection method: Let p.x/ denote the pdf from which we want
to draw random numbers. Furthermore, let h.x/ be another pdf, which can easily
be sampled (for instance with the help of the inverse transformation method) and
which is chosen in a such a way that the inequality
</p>
<p>p.x/ � c h.x/ ; (13.26)
</p>
<p>holds for all x 2 Œxmin; xmax&#141;, where c � 1 is some constant. The function c h.x/
is referred to as the envelope of p.x/ within the interval Œxmin; xmax&#141;. The strategy
is clear: we sample a random variable xt (trial state) from h.x/ and accept it with
probability p.x/=Œc h.x/&#141;. This procedure is sketched in Fig. 13.2. Let p.Ajx/ denote
the probability that a given value x is accepted and g.x/ denotes the probability that</p>
<p/>
</div>
<div class="page"><p/>
<p>13.3 Rejection Method 203
</p>
<p>Fig. 13.2 Schematic
illustration of the rejection
method. The trial state xt is
accepted with probability
p.x/=Œc h.x/&#141;
</p>
<p>we produce a variable x with the help of this algorithm. Furthermore, P.x D xt/
stands for the probability that a trial state xt is generated. We have
</p>
<p>g.x/ / P.x D xt/p.Ajxt/
</p>
<p>D h.xt/ p.x
t/
</p>
<p>c h.xt/
</p>
<p>/ p.xt/ : (13.27)
</p>
<p>Hence, we indeed generate random numbers which follow the pdf p.x/. We may
also calculate the probability P.A/ that an arbitrary trial state xt is accepted. This is
done with the help of the marginalization rule (E.39):
</p>
<p>P.A/ D
Z
</p>
<p>d xtp.A ^ xt/
</p>
<p>D
Z
</p>
<p>d xtp.Ajxt/P.x D xt/
</p>
<p>D
Z
</p>
<p>d xt
p.xt/
</p>
<p>c h.xt/
h.xt/
</p>
<p>D 1
c
</p>
<p>Z
</p>
<p>d xtp.xt/
</p>
<p>D 1
c
: (13.28)
</p>
<p>More generally, the probability P.A/ to accept a d-dimensional random variable is
given by:
</p>
<p>P.A/ D 1
cd
: (13.29)</p>
<p/>
</div>
<div class="page"><p/>
<p>204 13 Random Sampling Methods
</p>
<p>We deduce that the bigger c the worse is the acceptance probability of the rejection
method. It is therefore advisable to choose the envelope h.x/ very carefully.
</p>
<p>As an example we aim at sampling the normal distribution (E.43) for x 2 R
</p>
<p>p.x/ D 1p
2��2
</p>
<p>exp
</p>
<p>�
</p>
<p>� x
2
</p>
<p>2�2
</p>
<p>�
</p>
<p>; (13.30)
</p>
<p>with expectation value hxi � x0 D 0 and variance �2. In a first step we restrict our
investigation to x 2 Œ0;1/ due to the symmetry of the pdf. The slightly modified
pdf for the right-half axis reads
</p>
<p>q.x/ D
r
</p>
<p>2
</p>
<p>��2
exp
</p>
<p>�
</p>
<p>� x
2
</p>
<p>2�2
</p>
<p>�
</p>
<p>; x 2 Œ0;1/ ; (13.31)
</p>
<p>where we adjusted the normalization. The complete normal distribution (13.30) is
re-obtained by sampling the sign of x in an additional step. We use as an envelope
h.x/ the exponential distribution Eq. (13.22). Furthermore, � and c are chosen in
such a way that the acceptance probability (13.28) has a maximum under the
constraint (13.26). Since this is equivalent to c ! min we have to solve the
optimization problem
</p>
<p>c � q.x/
h.x/
</p>
<p>! max : (13.32)
</p>
<p>The resulting cmin is then given by
</p>
<p>cmin D
q.xopt/
</p>
<p>h.xopt/
(13.33)
</p>
<p>and xopt is the yet unknown optimal value for x. We obtain
</p>
<p>d
</p>
<p>d x
</p>
<p>q.x/
</p>
<p>h.x/
D
r
</p>
<p>2�2
</p>
<p>��2
d
</p>
<p>d x
exp
</p>
<p>�
x
</p>
<p>�
� x
</p>
<p>2
</p>
<p>2�2
</p>
<p>�
</p>
<p>D
r
</p>
<p>2�2
</p>
<p>��2
exp
</p>
<p>�
x
</p>
<p>�
� x
</p>
<p>2
</p>
<p>2�2
</p>
<p>��
1
</p>
<p>�
� x
�2
</p>
<p>�
</p>
<p>ŠD 0 ; (13.34)
</p>
<p>and, therefore,
</p>
<p>xopt D
�2
</p>
<p>�
: (13.35)</p>
<p/>
</div>
<div class="page"><p/>
<p>13.3 Rejection Method 205
</p>
<p>Consequently, we have
</p>
<p>cmin D
r
</p>
<p>2�2
</p>
<p>��2
exp
</p>
<p>�
�2
</p>
<p>2�2
</p>
<p>�
</p>
<p>: (13.36)
</p>
<p>The above relation gives the minimum value of c for arbitrary values of �. However,
since h.x/ is our envelope, we can choose � in such a way, that cmin ! min. This is
achieved in a second step
</p>
<p>d
</p>
<p>d�
cmin D
</p>
<p>r
</p>
<p>2
</p>
<p>��2
exp
</p>
<p>�
�2
</p>
<p>2�2
</p>
<p>��
</p>
<p>1 � �
2
</p>
<p>�2
</p>
<p>�
</p>
<p>ŠD 0 : (13.37)
</p>
<p>which results in the optimum value �opt D � . This, finally, results together with
Eq. (13.36) in:
</p>
<p>cmin D
r
</p>
<p>2e
</p>
<p>�
: (13.38)
</p>
<p>The algorithm is executed in the following steps:
</p>
<p>1. Draw a uniformly distributed random number � 2 Œ0; 1&#141;.
2. Calculate xt D ��opt ln.�/, where �opt D � .
3. Draw a uniformly distributed random number r 2 Œ0; 1&#141;. If r � q.xt/=Œcminh.xt/&#141;,
</p>
<p>then x D xt is accepted and if r &gt; q.xt/=Œcminh.xt/&#141;, xt is rejected and we return
to step 1.
</p>
<p>4. If xt was accepted, we draw a uniformly distributed random number r 2 Œ0; 1&#141;
and only if r &lt; 0:5 we set x D �x otherwise x stays as is.
</p>
<p>5. We repeat steps 1&ndash;4 until the number N of desired random numbers has been
reached.
</p>
<p>Figure 13.3 shows random numbers obtained with the help of this method in a
histogram representation.We calculated (a) N D 103, (b) N D 104, and (c) N D 105
random numbers for � D 1. It is quite obvious that the original pdf (13.30) is the
better approximated the bigger the numberN of sampled random numbers becomes.</p>
<p/>
</div>
<div class="page"><p/>
<p>206 13 Random Sampling Methods
</p>
<p>Fig. 13.3 The histogram
representation of the pdf p.x/
vs x generated by random
numbers drawn from the
normal distribution
Eq. (13.30) (� D 1) with the
help of the rejection method.
We sampled (a) N D 103 , (b)
N D 104 , and (c) N D 105
random numbers. The solid
line represents the pdf
p.x/ (13.30)
</p>
<p>13.4 Probability Mixing
</p>
<p>The method of probability mixing was developed to offer an algorithm which allows
to generate random numbers by sampling piecewise defined or composite pdfs. Such
a pdf is of the general form
</p>
<p>p.x/ D
N
X
</p>
<p>iD1
˛ifi.x/; ˛i &curren; 0 ; (13.39)
</p>
<p>where the sub-pdfs fi.x/ fulfill the normalization requirement
</p>
<p>Z
</p>
<p>d x0fi.x0/ D 1 ; (13.40)</p>
<p/>
</div>
<div class="page"><p/>
<p>13.4 Probability Mixing 207
</p>
<p>and are non-negative, i.e.
</p>
<p>fi.x/ � 0 ; (13.41)
</p>
<p>for all i D 1; : : : ;N. It follows that
</p>
<p>N
X
</p>
<p>iD1
˛i D 1 ; (13.42)
</p>
<p>which ensures that
Z
</p>
<p>d x0p.x0/ D 1 ; (13.43)
</p>
<p>is fulfilled. The question is how to sample random numbers from such a pdf, since
in most cases it might be hard to invert the sum (inverse transformation method)
or find a suitable envelope (rejection method). However, this question can easily be
answered: We define
</p>
<p>qi D
i
X
</p>
<p>`D1
˛` : (13.44)
</p>
<p>Thus, qN D 1 and the interval Œ0; 1&#141; has been divided according to:
</p>
<p>The index i of the relevant pdf is determined by the condition
</p>
<p>qi�1 &lt; r &lt; qi ; (13.45)
</p>
<p>where r 2 Œ0; 1&#141; is a uniformly distributed random number. We then draw the
required random number x from the sub-pdf fi.x/ with any of the methods discussed
above.
</p>
<p>This procedure is quite plausible, since the coefficients ˛i give the relative weight
of the sub-pdfs fi.x/. In particular, ˛i determines the importance of the sub-pdf fi.x/.
It is, therefore, a natural approach to use ˛i as a measure of the probability that a
random variable is to be sampled from the particular sub-pdf fi.x/.</p>
<p/>
</div>
<div class="page"><p/>
<p>208 13 Random Sampling Methods
</p>
<p>Summary
</p>
<p>To generate random numbers is essential for many application in Computational
Physics. This chapter concentrated on basic methods to generate the desired random
numbers: (a) the direct sampling method used transformations of the uniform
distribution to generate the required random numbers; (b) the inverse transformation
method was based on the availability of an inverse cdf which was in most cases
required to be calculated analytically; finally, (c) the rejection method which
was basically a hit or miss method. It used an easily invertible pdf h.x/ which
enveloped the desired pdf p.x/ completely within some interval x 2 Œxmin; xmax&#141;. The
effectiveness of this method depended on how &lsquo;well&rsquo; the envelope h.x/ enclosed
the original pdf p.x/. In a last step the method of probability mixing was discussed.
It was an easily verifiable method which allowed to sample random numbers from
composite pdfs.
</p>
<p>Problems
</p>
<p>Draw random numbers from the following pdfs:
</p>
<p>1. Direct Sampling:
Sample the normal distribution with hxi D 0 and � D 1 with the help of the
</p>
<p>method discussed in Sect. 13.1. Check the result by plotting the random numbers
against the pdf p.x/ in a histogram.
</p>
<p>2. Inverse Transformation Method:
Write a function which samples random numbers from the exponential
</p>
<p>distribution with the help of the inverse transformation method as discussed in
Sect. 13.2. Compare the generated random numbers to the pdf in a histogram.
</p>
<p>3. Rejection Method:
Sample the normal distribution with hxi D 0 and � D 1 with the help of the
</p>
<p>exponential distribution as discussed in Sect. 13.3. Compare the generated ran-
dom numbers with the pdf in a histogram. Determine the acceptance probability
numerically.
</p>
<p>4. Probability Mixing:
We choose an alternative envelope for the normal distribution with hxi D 0
</p>
<p>and � D 1. This envelope is chosen to be constant for all jxj &lt; x0, and decays
exponentially for jxj � x0. (x0 is a parameter of your choice.) The parameters
do not need to optimize the acceptance probability. Again, plot the generated
random numbers in a histogram and compare the acceptance probability with the
acceptance probability of point 3.</p>
<p/>
</div>
<div class="page"><p/>
<p>References 209
</p>
<p>References
</p>
<p>1. Devroye, L.: Non-uniform Random Variate Generation. Springer, Berlin/Heidelberg (1986)
2. Chow, Y.S., Teicher, H.: Probability Theory, 3rd edn. Springer Texts in Statistics. Springer,
</p>
<p>Berlin/Heidelberg (1997)
3. Kienke, A.: Probability Theory. Universitext. Springer, Berlin/Heidelberg (2008)
4. Stroock, D.W.: Probability Theory. Cambridge University Press, Cambridge (2011)
5. Bratley, P., Fox, B.L., Schrage, L.E.: A Guide to Simulation. Springer, Berlin/Heidelberg (1987)
6. Knuth, D.: The Art of Computer Programming, vol. II, 3rd edn. Addison Wesley, Menlo Park
</p>
<p>(1998)
7. Press, W.H., Teukolsky, S.A., Vetterling, W.T., Flannery, B.P.: Numerical Recipes in C++, 2nd
</p>
<p>edn. Cambridge University Press, Cambridge (2002)</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 14
A Brief Introduction to Monte-Carlo Methods
</p>
<p>14.1 Introduction
</p>
<p>This chapter presents a brief introduction to Monte-Carlo methods in general, and
to Monte-Carlo integration as well as to the METROPOLIS-HASTINGS algorithm in
particular. A detailed discussion of the fundamental concepts involved is postponed
to Chap. 16. The introduction given here is not supposed to be self-contained and
methods will be introduced without reference to their background.
</p>
<p>The notion of Monte-Carlo methods, Monte-Carlo algorithms or Monte-Carlo
techniques is not well defined. In particular, the term Monte-Carlo summarizes a
wide field of methods which are based on the sampling of random numbers [1&ndash;
3]. In general, the advantage of Monte-Carlo algorithms lies in their computational
strength. In many cases it is simply not feasible to employ deterministicmethods due
to their very high computational cost. However, in many cases the use of methods
based on random sampling is also motivated by the nature of the processes to be
described.Wementioned in the previous chapter as a typical example the radioactive
decay of some nucleus. This process is believed to be purely stochastic in nature.
</p>
<p>The development of Monte-Carlo techniques was initialized in the 1940s by J.
VON NEUMANN, S. M. ULAM and N. METROPOLIS who coined the term Monte-
Carlo methods. One of the earliest illustrations of the principle of Monte-Carlo
techniques in general, and of Monte-Carlo integration in particular is the Monte-
Carlo approximation of � . The discussion which follows now includes the essential
ideas of Monte-Carlo integration.
</p>
<p>We regard the unit square characterized by the corner points .0; 0/, .0; 1/, .1; 0/,
and .1; 1/. The areaAs of this square is one.We insert a quarter-circle of radius r D 1
which, consequently, possesses the area Ac D �=4. Suppose, we are throwing darts
on this unit square in such a way that the impact points are uniformly distributed;
</p>
<p>&copy; Springer International Publishing Switzerland 2016
B.A. Stickler, E. Schachinger, Basic Concepts in Computational Physics,
DOI 10.1007/978-3-319-27265-8_14
</p>
<p>211</p>
<p/>
</div>
<div class="page"><p/>
<p>212 14 A Brief Introduction to Monte-Carlo Methods
</p>
<p>then the probability P that a certain dart becomes stuck within the interior of the
quarter-circle is given by
</p>
<p>P D Ac
As
</p>
<p>D Ac D
�
</p>
<p>4
D 0:785398 : : : : (14.1)
</p>
<p>From a probabilistic point of view, we have after N throws of which n hit the
interior of the quarter-circle the probability:
</p>
<p>P D lim
N!1
</p>
<p>n
</p>
<p>N
: (14.2)
</p>
<p>The strategy is clear: we draw uniformly distributed random numbers xi, yi from
the interval Œ0; 1&#141;. These are the intersection points of the darts. Repeating this
experiment several times and counting the number of hits n within the quarter-circle
allows us to approximate � via
</p>
<p>P D �
4
� n
</p>
<p>N
: (14.3)
</p>
<p>The resulting approximation of � will be strongly influenced by the number
of experiments N as well as by the performance of the random number generator
used. Table 14.1 lists computed approximations of � for different numbers of
experimentsN as they were obtainedwith the help of a linear congruential generator.
Linear congruential generators have been introduced and discussed in Sect. 12.2.
The parameters used to initialize the generators are given in the caption of the table.
Furthermore, Fig. 14.1 illustrates the result after N D 103 experiments for both
generators.
</p>
<p>Table 14.1 Approximate values � .i/a obtained with the method discussed in the text. The linear
congruential generators are initialized by the following parameters: generator (1): a D 75, c D 0,
m D 231 � 1, and x0 D 281 (PARK-MILLER) and generator (2): a D 75, c D 0, m D 211, and
x0 D 281. We also give the absolute errors j� .i/a � �j
N �
</p>
<p>.1/
a j� .1/a � �j � .2/a j� .2/a � �j
</p>
<p>10 2.8000 0.34159 2.8000 0.34159
</p>
<p>102 2.9200 0.22159 3.1600 0.01841
</p>
<p>103 3.1600 0.01841 3.1840 0.04241
</p>
<p>104 3.1304 0.01119 3.1868 0.04521
</p>
<p>105 3.1358 0.00579 3.1875 0.04589
</p>
<p>106 3.1393 0.00229 3.1875 0.04599
</p>
<p>107 3.1413 0.00028 3.1875 0.04599</p>
<p/>
</div>
<div class="page"><p/>
<p>14.2 Monte-Carlo Integration 213
</p>
<p>Fig. 14.1 N D 103 uniformly distributed random numbers within the unit-square. Frame (a) gives
the results for generator (1) while frame (b) is for generator (2). The number of elements within
the quarter-circle indicated by the solid line determines the value of � .i/a . The inferior result of the
approximation obtained with generator (2) [frame (b)] originates in correlations between the x and
y coordinates
</p>
<p>14.2 Monte-Carlo Integration
</p>
<p>We generalize the ideas formulated above and consider a function f .x/ � 0 for
x 2 Œa; b&#141; � R where the area of interest is
</p>
<p>A D
Z b
</p>
<p>a
</p>
<p>dxf .x/ : (14.4)
</p>
<p>We denote
</p>
<p>� D max
x2Œa;b&#141;
</p>
<p>f .x/ ; (14.5)
</p>
<p>and obtain using the above example
</p>
<p>A D As lim
N!1
</p>
<p>n
</p>
<p>N
; (14.6)
</p>
<p>where n is the number of random points under the curve indicated schematically in
Fig. 14.2. The area As is given by
</p>
<p>As D .b � a/� ; (14.7)
</p>
<p>and the random numbers ri D .xi; yi/ are uniformly distributed within the intervals
xi 2 Œa; b&#141; and yi 2 Œ0; �&#141;. This method is referred to as hit and miss integration [4].</p>
<p/>
</div>
<div class="page"><p/>
<p>214 14 A Brief Introduction to Monte-Carlo Methods
</p>
<p>Fig. 14.2 Schematic
illustration of the
Monte-Carlo integration
technique
</p>
<p>Another way to perform a Monte-Carlo integration is the so called mean-value
integration. It is essentially based on the mean value theorem of calculus which we
already employed in our discussion of quadrature in Chap. 3. We restate it here for
the sake of a more transparent presentation: The mean-value theorem states that if
f .x/ is a continuous function for x 2 Œa; b&#141; then there exists a z 2 .a; b/ such that
</p>
<p>Z b
</p>
<p>a
</p>
<p>dx f .x/ D f .z/.b � a/ : (14.8)
</p>
<p>The function value f .z/ � h f i is referred to as the expectation value or mean value
of f .x/. We know from probability theory [5&ndash;7] that the expectation value can be
approximated by the arithmetic mean f
</p>
<p>1
</p>
<p>b � a
</p>
<p>Z b
</p>
<p>a
</p>
<p>dx0f .x0/ ' f ˙
</p>
<p>s
</p>
<p>f 2 � f 2
</p>
<p>N
; (14.9)
</p>
<p>with the error given by the standard error, Eq. (E.14). The arithmetic mean f , on the
other hand, is given by
</p>
<p>f D 1
N
</p>
<p>N
X
</p>
<p>iD1
f .xi/ ; (14.10)
</p>
<p>and consequently
</p>
<p>f 2 D 1
N
</p>
<p>N
X
</p>
<p>iD1
f 2.xi/ : (14.11)
</p>
<p>Note that here the variables xi are assumed to be uniformly distributed random
numbers within the interval Œa; b&#141;. (This result will immediately be discussed in</p>
<p/>
</div>
<div class="page"><p/>
<p>14.2 Monte-Carlo Integration 215
</p>
<p>more detail.) However, first of all we note from the law of large numbers, Eq. (E.25),
that this approach is exact in the limit N ! 1:
</p>
<p>1
</p>
<p>b � a
</p>
<p>Z b
</p>
<p>a
</p>
<p>dx0f .x0/ D lim
N!1
</p>
<p>1
</p>
<p>N
</p>
<p>N
X
</p>
<p>iD1
f .xi/ : (14.12)
</p>
<p>Let us now consider the more general case which, in the end, will guide us to
a very prominent formulation of Monte-Carlo integration. We want to estimate the
expectation value
</p>
<p>h f i D
Z
</p>
<p>dx f .x/p.x/ ; (14.13)
</p>
<p>where x 2 Rd and p.x/ is a pdf. A typical example is the calculation of the thermal
expectation value in statistical physics where the pdf p.x/ is given by the normalized
BOLTZMANN distribution
</p>
<p>p.x/ D 1
Z
exp
</p>
<p>�
</p>
<p>�E.x/
kBT
</p>
<p>�
</p>
<p>: (14.14)
</p>
<p>Here E.x/ denotes the energy as a function of the parameter x 2 Rd, kB stands
for BOLTZMANN&rsquo;s constant, T is the temperature, and the normalization factor Z is
referred to as the canonical partition function [8&ndash;11].
</p>
<p>Equation (14.13) may be rewritten as
</p>
<p>h f i D
Z
</p>
<p>dx f .x/p.x/ D
Z
</p>
<p>df f q. f / ; (14.15)
</p>
<p>where we introduced the probability density q. f / of f via
</p>
<p>q. f / D
Z
</p>
<p>dx ı Œ f � f .x/&#141; p.x/ ; (14.16)
</p>
<p>with ı.�/ DIRAC&rsquo;s ı-distribution. Let us briefly explain how we arrived at this
definition. Let the cdf P.x/ be defined by1
</p>
<p>P.x/ D Pr.X � x/ D
Z x
</p>
<p>�1
dx p.x/ : (14.17)
</p>
<p>1Please note that according to the conventions established in Appendix E capital letters denote
random variables.</p>
<p/>
</div>
<div class="page"><p/>
<p>216 14 A Brief Introduction to Monte-Carlo Methods
</p>
<p>We define in analogy the cdf Q. f /:
</p>
<p>Q. f / D Pr.F � f / D PrŒ f .X/ � f &#141; : (14.18)
</p>
<p>Note that we distinguish between the function f .X/ of the random variable X [which
follows the pdf p.X/] and the particular function value f 2 R. Furthermore, the
probability PrŒ f .X/ � f &#141; can be rewritten as
</p>
<p>PrŒ f .X/ � f &#141; D
X
</p>
<p>n
</p>
<p>Pr.an � X � bn/ ; (14.19)
</p>
<p>where the values an &lt; bn are the ordered intersection points a1 &lt; b1 &lt; a2 &lt; b2 &lt;
: : : &lt; aN &lt; bN chosen in such a way that
</p>
<p>f .an/ D f .bn/ D f ; and f Œx 2 .an; bn/&#141; &lt; f : (14.20)
</p>
<p>It is a matter of the particular form of f .x/ whether or not the boundary points have
to be included. Equation (14.19) can be rewritten:
</p>
<p>Pr.an � X � bn/ D P.bn/ � P.an/ D
Z bn
</p>
<p>an
</p>
<p>dx p.x/ : (14.21)
</p>
<p>The pdf q. f / is related to the cdf Q. f / via
</p>
<p>q. f / D d
df
</p>
<p>Q. f / ; (14.22)
</p>
<p>and we obtain
</p>
<p>q. f / D
X
</p>
<p>n
</p>
<p>d
</p>
<p>df
Pr.an � X � bn/
</p>
<p>D
X
</p>
<p>n
</p>
<p>d
</p>
<p>df
</p>
<p>Z bn
</p>
<p>an
</p>
<p>dx p.x/
</p>
<p>D
X
</p>
<p>n
</p>
<p>�
dbn
df
</p>
<p>p.bn/�
dan
df
</p>
<p>p.an/
</p>
<p>�
</p>
<p>D
X
</p>
<p>n
</p>
<p>"�
df .x/
</p>
<p>dx
</p>
<p>��1
p.x/
</p>
<p>ˇ
ˇ
ˇ
ˇ
ˇ
xDbn
</p>
<p>�
�
df .x/
</p>
<p>dx
</p>
<p>��1
p.x/
</p>
<p>ˇ
ˇ
ˇ
ˇ
ˇ
xDan
</p>
<p>#
</p>
<p>:
</p>
<p>(14.23)</p>
<p/>
</div>
<div class="page"><p/>
<p>14.2 Monte-Carlo Integration 217
</p>
<p>However, we know from Eq. (14.20) that:
</p>
<p>df .x/
</p>
<p>dx
</p>
<p>ˇ
ˇ
ˇ
ˇ
xDbn
</p>
<p>Š
&gt; 0 and
</p>
<p>df .x/
</p>
<p>dx
</p>
<p>ˇ
ˇ
ˇ
ˇ
xDan
</p>
<p>Š
&lt; 0 : (14.24)
</p>
<p>We introduce the intersection points xk where x1 &lt; x2 &lt; : : : &lt; xK and K D 2N (if
the boundary points are not included) for which f .x1/ D f .x2/ D : : : D f .xK/ D f .
Hence, Eq. (14.23) may be rewritten as
</p>
<p>q. f / D
X
</p>
<p>k
</p>
<p>ˇ
ˇ
ˇ
ˇ
</p>
<p>df .x/
</p>
<p>dx
</p>
<p>ˇ
ˇ
ˇ
ˇ
</p>
<p>�1
ˇ
ˇ
ˇ
ˇ
ˇ
xDxk
</p>
<p>p.xk/
</p>
<p>D
X
</p>
<p>k
</p>
<p>p.xk/
</p>
<p>jf 0.xk/j
: (14.25)
</p>
<p>We want to improve this result and remember that the DIRAC ı-distribution of an
arbitrary function g.y/ can be expressed as [12]
</p>
<p>ıŒg.y/&#141; D
X
</p>
<p>i
</p>
<p>ı.y � yi/
jg0.yi/j
</p>
<p>; (14.26)
</p>
<p>where the yi are the zeros of g.y/, i.e. g.yi/ D 0. Hence, we arrive at the final form
of Eq. (14.23)2:
</p>
<p>q. f / D
Z
</p>
<p>dxı Œ f � f .x/&#141; p.x/ : (14.27)
</p>
<p>We note, furthermore, that:
</p>
<p>Z
</p>
<p>df q. f / D
Z
</p>
<p>df
Z
</p>
<p>dxı Œ f � f .x/&#141; p.x/ D
Z
</p>
<p>dx p.x/ D 1 : (14.28)
</p>
<p>2We give an example. Suppose f .x/ D exp.x/. Then we deduce that
</p>
<p>ı Œ f � exp.x/&#141; D ı.x � ln f /
f
</p>
<p>;
</p>
<p>and, consequently,
</p>
<p>q. f / D p.ln f /
f
</p>
<p>:
</p>
<p>A second example was given in Chap. 13 where we derived the pdf (13.5).</p>
<p/>
</div>
<div class="page"><p/>
<p>218 14 A Brief Introduction to Monte-Carlo Methods
</p>
<p>As a result, the variance of f , var . f /, can be expressed as
</p>
<p>var . f / D
Z
</p>
<p>dx Œ f .x/ � h f i&#141;2 p.x/ D
Z
</p>
<p>df Œ f � h f i&#141;2 q. f / : (14.29)
</p>
<p>Let us define in a next step the arithmetic mean of f .X/
</p>
<p>F D 1
N
</p>
<p>N
X
</p>
<p>iD1
f .xi/ D
</p>
<p>1
</p>
<p>N
</p>
<p>N
X
</p>
<p>iD1
fi ; (14.30)
</p>
<p>calculated with the help of N random numbers. Hence, we have
</p>
<p>hF i D h f i ; (14.31)
</p>
<p>and
</p>
<p>var .F / D var . f /
N
</p>
<p>; (14.32)
</p>
<p>according to Appendix E. It follows from the central limit theorem, Appendix
Sect. E.8, that for large values of N, the pdf of F , p.F / converges to a normal
distribution (E.43) with hF i and var .F /:
</p>
<p>p.F / � N
�
</p>
<p>F
</p>
<p>ˇ
ˇ
ˇ
ˇ
h f i ; var . f /
</p>
<p>N
</p>
<p>�
</p>
<p>: (14.33)
</p>
<p>Based on this property h f i can be estimated from:
</p>
<p>h f i D F ˙
r
</p>
<p>var . f /
</p>
<p>N
D 1
</p>
<p>N
</p>
<p>N
X
</p>
<p>iD1
f .xi/˙
</p>
<p>r
</p>
<p>var . f /
</p>
<p>N
: (14.34)
</p>
<p>Here, the random numbers xi are sampled from the pdf p.x/. This method is the
most prominent formulation of Monte-Carlo integration.
</p>
<p>We shall briefly discuss some properties of this method. We deduce from
Eq. (14.34) that the error scales like N�
</p>
<p>1
2 . In contrast to the integration methods
</p>
<p>we discussed in Chap. 3, N is no longer the number of grid-points but the number
of random numbers sampled.3 In principle, the error scaling is worse than in the
case of classical integrators. For instance, in the case of the central rectangular rule
(Sect. 3.2) we had an error scaling of N�2 when summed over the whole interval.
However, we obtained this result for the one-dimensional case, in higher dimensions
we will certainly need much more grid-points. On the other hand, in Eq. (14.34) N
</p>
<p>3Nevertheless, there is certainly some conceptual similarity between grid-points and random
numbers within this context.</p>
<p/>
</div>
<div class="page"><p/>
<p>14.3 The METROPOLIS Algorithm: An Introduction 219
</p>
<p>corresponds to the number of d-dimensional random numbers x. Hence, Monte-
Carlo integration can be of advantage whenever one has to deal with complicated,
high dimensional integrals. In contrast, restricted to one dimension it is in most
cases not an improvement of the methods discussed already.
</p>
<p>Monte-Carlo integration can also be of advantage whenever the integrand f .x/ is
not well behaved. In such a case a very fine grid would be required to compute a
reasonable estimate of the true value of the integral. Monte-Carlo integration offers
a very convenient alternative due to its conceptual simplicity [13].
</p>
<p>It is certainly a drawback of Monte-Carlo integration in its formulation (14.34),
that the error is also proportional to
</p>
<p>p
</p>
<p>var . f / which is a yet unknown quantity.
One has to approximate it with an adequate estimator, for instance with the help of
the sampling variance. Moreover, if the variance var . f / diverges, the central limit
theorem does not hold and the procedure (14.34) is no longer justified and will fail
for sure.
</p>
<p>Closely related to the problem of how to determine var . f /, is the question of
how many random numbers should be drawn. In most cases an iterative approach
is the most promising strategy. In a first step N random numbers are drawn and the
integral is computed using Eq. (14.34). Then another set of N random numbers is
sampled and Eq. (14.34) is reevaluated now using all 2N random numbers. If the
change in the resulting estimate of the integral is less than some given tolerance �,
the loop is terminated otherwise another set of N random numbers is added.
</p>
<p>We mention that this form of Monte-Carlo integration can be improved partic-
ularly by sampling only from points which dominantly contribute to the integral.
This method is referred to as importance sampling [13&ndash;16] and will be discussed in
more detail later on.
</p>
<p>14.3 The METROPOLIS Algorithm: An Introduction
</p>
<p>The METROPOLIS algorithm is a more sophisticated method to produce random
numbers from given distributions. In fact, the METROPOLIS algorithm is a special
form of the rejection method (Sect. 13.3). This section introduces the algorithm on
a very basic level which will, in the end, allow a first glance at an interesting model
out of statistical physics, namely the ISING model. It will be discussed in Chap. 15
and a more detailed discussion of the METROPOLIS algorithm will be postponed to
Sect. 16.4.
</p>
<p>The METROPOLIS algorithm is particularly useful to treat problems in statistical
physics where thermodynamic expectation values of some observable O are of
interest [8&ndash;11]. They are defined as
</p>
<p>hOi D
Z
</p>
<p>dxO.x/q.x/ ; (14.35)</p>
<p/>
</div>
<div class="page"><p/>
<p>220 14 A Brief Introduction to Monte-Carlo Methods
</p>
<p>where x is a set of parameters and q.x/ is the BOLTZMANN distribution (14.14).
The set of parameters x could be, for instance, the position- and momentum-space
coordinates of N different particles. In most cases x is a high dimensional object
which makes classical numerical integration (Chap. 3) cumbersome. InsteadMonte-
Carlo integration is employed and the integral (14.35) is approximated with the help
of Eq. (14.34) by
</p>
<p>hOi � 1
N
</p>
<p>N
X
</p>
<p>iD1
O.xi/˙
</p>
<p>r
</p>
<p>var .O/
</p>
<p>N
; (14.36)
</p>
<p>where the uncorrelated random numbers xi, i D 1; 2; : : : ;N are sampled from the
pdf, Eq. (14.14). We recognize immediately the problem: we need to know the exact
functional form of q.x/ if we want to apply either the inverse transformation method
or the rejection method discussed in Chap. 13. However, the partition function Z
itself is determined by an integral which can be approximated using Eq. (14.36). We
set
</p>
<p>q.x/ D p.x/
Z
</p>
<p>; (14.37)
</p>
<p>and
</p>
<p>Z D
Z
</p>
<p>dx p.x/ (14.38)
</p>
<p>follows from the normalization of q.x/. The METROPOLIS algorithm was
designed to avoid precisely this problem. We concentrate on a pdf which is of
the form (14.37), but q.x/ must not necessarily be described by a normalized
BOLTZMANN distribution, Eq. (14.14). Thus, p.x/ is arbitrary but it ensures that
</p>
<p>Z
</p>
<p>dx q.x/ D 1&rdquo;
Z
</p>
<p>dx p.x/ D Z ; (14.39)
</p>
<p>and q.x/ � 0 for all x. In other words, q.x/ is a pdf. Suppose we already have a
sequence x0; x1; : : : ; xn D fxng of parameters which indeed follows the pdf q.x/.4
We now add to the last element of this sequence xn a small perturbation ı and set
</p>
<p>xt D xn C ı : (14.40)
</p>
<p>Note that the perturbation ı is of the same dimension as the vector x. Similar to the
rejection method we seek for a criterion which helps us to decide whether or not the
test value xt can be accepted as the next element of the sequence fxng.
</p>
<p>4The question of how one can obtain such a sequence will be discussed in Sect. 16.3.</p>
<p/>
</div>
<div class="page"><p/>
<p>14.3 The METROPOLIS Algorithm: An Introduction 221
</p>
<p>The METROPOLIS method proposes an acceptance probability of the form
</p>
<p>Pr.Ajxt; xn/ D
</p>
<p>8
</p>
<p>ˆ̂
</p>
<p>&lt;̂
</p>
<p>ˆ̂
</p>
<p>:̂
</p>
<p>1 if
q.xt/
</p>
<p>q.xn/
� 1 ;
</p>
<p>q.xt/
</p>
<p>q.xn/
otherwise:
</p>
<p>(14.41)
</p>
<p>Hence, if Pr.Ajxt; xn/ D 1, we set xnC1 D xt, and if Pr.Ajxt; xn/ &lt; 1, we draw a
random number r 2 Œ0; 1&#141; and accept xt if r � Pr.Ajxt; xn/ and reject xt otherwise.
We note that in this formulation the knowledge of the normalization factor Z is no
longer required since it follows from Eq. (14.37) that
</p>
<p>q.xt/
</p>
<p>q.xn/
D p.xt/
</p>
<p>p.xn/
: (14.42)
</p>
<p>Consequently we rewrite Eq. (14.41) as
</p>
<p>Pr.Ajxt; xn/ D min
�
</p>
<p>p.xt/
</p>
<p>p.xn/
; 1
</p>
<p>�
</p>
<p>D p.xtjxn/ ; (14.43)
</p>
<p>where we introduced in the last step a more compact notation.
A discussion of the underlying concepts and why the choice (14.41) indeed
</p>
<p>samples random numbers according to the pdf q.x/ requires some basic knowledge
of stochastics in general and of MARKOV-chains in particular. This is the reason
why we postponed this discussion to Chap. 16. Nevertheless, there is a particular
property, referred to as detailed balance which requires our attention because it
is crucial for the METROPOLIS algorithm: Let p.xtjxn/ denote the pdf for the
probability that a random number xt is generated from the random number xn as
defined in Eq. (14.43). Then the condition of detailed balance is defined as
</p>
<p>p.xtjxn/q.xn/ D p.xnjxt/q.xt/ : (14.44)
</p>
<p>In words: The probability p.xtjxn/ that a random number xt is generated from a
random number xn times the probability q.xn/ that the random number xn occurred
at all is equal to the probability p.xnjxt/ that the random number xn is generated
from xt times the probability q.xt/ that xt occurred. Detailed balance is motivated by
physics and is a condition of thermodynamic equilibrium.
</p>
<p>Let us briefly demonstrate that the METROPOLIS algorithm (14.43) satisfies
detailed balance: We distinguish three different cases: (i) Suppose that p.xtjxn/ D
p.xnjxt/ D 1. From Eq. (14.43) we note that this is only possible if p.xt/ D p.xn/
and therefore q.xt/ D q.xn/ which is already Eq. (14.44) for this particular case. (ii)</p>
<p/>
</div>
<div class="page"><p/>
<p>222 14 A Brief Introduction to Monte-Carlo Methods
</p>
<p>We assume that p.xtjxn/ D 1 but p.xnjxt/ &curren; 1. It follows from Eq. (14.43) that
</p>
<p>p.xnjxt/q.xt/ D
p.xn/
</p>
<p>p.xt/
q.xt/
</p>
<p>D q.xn/ : (14.45)
</p>
<p>This corresponds to Eq. (14.44) for p.xtjxn/ D 1. Note that we made use of
definition (14.37) in order to achieve this result. (iii) Finally, we find for p.xnjxt/ D 1
and p.xtjxn/ &curren; 1 that
</p>
<p>p.xtjxn/q.xn/ D
p.xt/
</p>
<p>p.xn/
q.xn/
</p>
<p>D q.xt/ ; (14.46)
</p>
<p>which, again, is Eq. (14.44). Hence, the METROPOLIS algorithm (14.43) indeed
obeys detailed balance.
</p>
<p>So far the question of how to choose the initialization point x0 of the sequence
stayed unanswered. This is clearly not a trivial problem and it is strongly related
to one of the major disadvantages of the METROPOLIS algorithm, namely that
subsequent random numbers .xn; xnC1/ are strongly correlated. One of the most
pragmatic approaches is to choose a starting point x0 at random out of the parameter
space and then discard it together with the first few members of the sequence.
This approach is strongly motivated by a clear physical picture: The sequence of
random numbers resembles the evolution of the physical system from an arbitrary
initial point x0 toward equilibriumwhich manifests itself in the condition of detailed
balance. Hence, the approach of discarding the first few members of the sequence is
referred to as thermalization.
</p>
<p>The integral of interest, Eq. (14.35) is then approximated with the help of
Eq. (14.36), where the random numbers xk; xkC1; : : : ; xkCN are used, if the ther-
malization required k steps. There is a remedy which helps to reduce correlations
between subsequent random numbers within the sequence which is based on a
similar strategy. In particular, the modified sequence
</p>
<p>xk; xkC`; xkC2`; : : : ; (14.47)
</p>
<p>generated by discarding ` intermediate random numbers will reduce correlations
between the members of this final sequence of random numbers.
</p>
<p>Summary
</p>
<p>This chapter set the stage for an important numerical tool in Computational Physics:
the Monte-Carlo techniques. It started with the conceptual transparent task of how
to calculate � using a sequence of uniformly distributed random numbers of the</p>
<p/>
</div>
<div class="page"><p/>
<p>References 223
</p>
<p>range Œ0; 1&#141;. This established the so-called hit and miss technique. It moved on to a
discussion of Monte-Carlo integration in a more formal way and discussed in detail
the error involved by this type of integration as opposed to the error experienced
by deterministic methods. The conclusion was, that Monte-Carlo integration was
certainly preferable whenever estimates of high dimensional integrals were required
and it also had advantages when the integrand was heavily structured. The second
part of this chapter dealt with theMETROPOLIS algorithmwhich allowed to generate
a sequence of random numbers from some pdf p.x/. It was conceptually similar
to the rejection method discussed earlier. The mathematical background which is
more involved was not discussed within this first contact with the METROPOLIS
algorithm. Instead, the emphasis was to demonstrate that this algorithm obeyed
detailed balance a property purely based on physics as a condition of thermo-
dynamic equilibrium. It was, furthermore, pointed out that the random numbers
generated by this algorithm were highly correlated and some strategies to remedy
this problem were discussed.
</p>
<p>References
</p>
<p>1. Gentle, J.E.: Random Number Generation and Monte Carlo Methods. Statistics and Comput-
ing. Springer, Berlin/Heidelberg (2003)
</p>
<p>2. Kalos, M.H., Whitlock, P.A.: Monte Carlo Methods, 2nd edn. Wiley, New York (2008)
3. Kroese, D.P., Taimre, T., Botev, Z.I.: Handbook of Monte Carlo Methods. Wiley, New York
</p>
<p>(2011)
4. Fishman, G.S.: Monte Carlo: Concepts, Algorithms and Applications. Springer Series in
</p>
<p>Operations Research. Springer, Berlin/Heidelberg (1996)
5. Chow, Y.S., Teicher, H.: Probability Theory. Springer Texts in Statistics, 3rd edn. Springer,
</p>
<p>Berlin/Heidelberg (1997)
6. Kienke, A.: Probability Theory. Universitext. Springer, Berlin/Heidelberg (2008)
7. Stroock, D.W.: Probability Theory. Cambridge University Press, Cambridge (2011)
8. Schwabl, F.: Statistical Mechanics. Advanced Texts in Physics. Springer, Berlin/Heidelberg
</p>
<p>(2006)
9. Halley, J.W.: Statistical Mechanics. Cambridge University Press, Cambridge (2006)
10. Pathria, R.K., Beale, P.D.: Statistical Mechanics, 3rd edn. Academic, San Diego (2011)
11. Hardy, R.J., Binek, C.: Thermodynamics and Statistical Mechanics: An Integrated Approach.
</p>
<p>Wiley, New York (2014)
12. Ballentine, L.E.: Quantum Mechanics. World Scientific, Hackensack (1998)
13. Press, W.H., Teukolsky, S.A., Vetterling, W.T., Flannery, B.P.: Numerical Recipes in C++, 2nd
</p>
<p>edn. Cambridge University Press, Cambridge (2002)
14. Doucet, A., de Freitas, N., Gordon, N. (eds.): Sequential Monte Carlo Methods in Practice.
</p>
<p>Information Science and Statistics. Springer, Berlin/Heidelberg (2001)
15. Ripley, B.D.: Stochastic Simulation. Wiley, New York (2006)
16. Landau, D.P., Binder, K.: A Guide to Monte Carlo Simulations in Statistical Physics, 3rd edn.
</p>
<p>Cambridge University Press, Cambridge (2009)</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 15
The ISING Model
</p>
<p>15.1 The Model
</p>
<p>Ferromagneticmaterials are materials which develop a non-vanishingmagnetization
M even in the absence of an external magnetic field B. It is an experimental obser-
vation, that this magnetization decreases smoothly with increasing temperature, and
vanishes above the critical temperature TC, referred to as CURIE temperature [1].
Above this temperature the magnetization is zero and the material is no longer
ferromagnetic but paramagnetic. This typical situation is illustrated in Fig. 15.1 and
it is the signature of a phase transition. In a theoretical description of this transition
the magnetization M serves as an order parameter.1 At T D TC the system exhibits
a second order phase transition: The magnetization is not differentiable with respect
to T; it is, however, continuous.
</p>
<p>The microscopic origin of this macroscopic phenomenon is based on the
exchange interaction between identical particles, the atoms or molecules forming
the material. The exchange interaction is a purely quantum-mechanical effect which
is a consequence of the COULOMB interaction in combination with the PAULI
exclusion principle.2 For more detailed information please consult Refs. [2&ndash;8].
</p>
<p>Given two atoms or molecules with spins S1 and S2, where S1; S2 2 R3, the
exchange interaction energy is of the form3
</p>
<p>E D JS1 � S2 ; (15.1)
</p>
<p>1For a short introduction to phase transitions in general please consult Appendix F.
2The statement that magnetism is a purely quantum-mechanical phenomenon that cannot explained
in classical terms is known as the BOHR-VAN LEEUWEN theorem [3, 4].
3In this discussion we regard the spin as a classical quantity. In the quantum mechanic case one
has to replace the vectors by vector operators Si.
</p>
<p>&copy; Springer International Publishing Switzerland 2016
B.A. Stickler, E. Schachinger, Basic Concepts in Computational Physics,
DOI 10.1007/978-3-319-27265-8_15
</p>
<p>225</p>
<p/>
</div>
<div class="page"><p/>
<p>226 15 The ISING Model
</p>
<p>Fig. 15.1 Schematic
illustration of the
magnetization M as a
function of temperature T in a
ferromagnetic material
</p>
<p>Fig. 15.2 Schematic
illustration of the
spin-orientation in a (a)
ferromagnetic (J &lt; 0) or (b)
antiferromagnetic (J &gt; 0)
two-dimensional crystal
</p>
<p>with the exchange constant J. The magnitude of J as well as its sign are determined
by overlap integrals which include the COULOMB interaction. If J &lt; 0 a parallel
orientation of the spins is energetically favorable and ferromagnetism arises if T &lt;
TC. On the other hand, if J &gt; 0, an antiparallel orientation is established as long as
the temperature does not exceed the N&Eacute;EL temperature TN . However, in both cases
the system undergoes a phase transition to a paramagnetic state if the temperature
T exceeds the CURIE temperature (ferromagnetic case) or the N&Eacute;EL temperature
(antiferromagnetic case). A schematic illustration of ferro- and antiferromagnetism
for a two-dimensional crystal is illustrated in Fig. 15.2. We summarize the different
scenarios:
</p>
<p>8
</p>
<p>ˆ̂
&lt;
</p>
<p>ˆ̂
:
</p>
<p>J &lt; 0 ferromagnetic;
</p>
<p>J &gt; 0 antiferromagnetic;
</p>
<p>J D 0 non-interacting:
</p>
<p>We concentrate on a cubic crystal lattice in which the atoms are localized at
positions x`. The spin of atom ` will be denoted by S` 2 R3 and the exchange
parameter between atom ` and atom `0 by J``0 . Furthermore, we consider the
ferromagnetic case with J``0 &lt; 0. The HAMILTON function [9&ndash;11] is of the form
</p>
<p>H D 1
2
</p>
<p>X
</p>
<p>``0
</p>
<p>J``0S` � S`0 D
1
</p>
<p>2
</p>
<p>X
</p>
<p>``0
</p>
<p>J`�`0S` � S`0 : (15.2)
</p>
<p>Here J``0 was replaced by J`�`0 D J`0�` to account for translational invariance.
Moreover, we define that J`` D 0, otherwise we would have to exclude the</p>
<p/>
</div>
<div class="page"><p/>
<p>15.1 The Model 227
</p>
<p>contributions ` D `0 from the above sum. The HAMILTON function (15.2) is genuine
to the HEISENBERG model [1]. We note that in this model there is no distinguished
direction of spin orientation and, consequently, the HAMILTON function is invariant
under a rotation of all spin vectors S`. The actual spin orientation may be determined
by an external magnetic field or by an anisotropy of the crystal lattice. Furthermore,
the restriction of the spin orientation to the positive or negative z-direction is the
characteristic of the ISING model.
</p>
<p>In a quantum mechanical description the HAMILTON operator (Hamiltonian) of
the ISING model is defined by
</p>
<p>H D 1
2
</p>
<p>X
</p>
<p>``0
</p>
<p>J`�`0S
z
`S
</p>
<p>z
`0 ; (15.3)
</p>
<p>where Sz` are the spin operators in z-direction. If spin 1=2 particles are described by
this Hamiltonian, the spin operators Sz` are replaced by .&bdquo;=2/�
</p>
<p>z
` with �
</p>
<p>z
` the PAULI
</p>
<p>matrix and &bdquo; the reduced PLANCK&rsquo;s constant. Furthermore, we redefine J0
`�`0 D
</p>
<p>�.&bdquo;2=4/J`�`0 , J0`�`0 &gt; 0, and represent the Hamiltonian in the basis of eigenstates
of the operators � z` . These eigenstates have eigenvalues �` D ˙1 which correspond
to spin up and spin down states, respectively. We obtain in this representation
</p>
<p>H D �1
2
</p>
<p>X
</p>
<p>``0
</p>
<p>J`�`0�`�`0 � h
X
</p>
<p>`
</p>
<p>�` ; (15.4)
</p>
<p>where we dropped the prime on the exchange parameter J`�`0 for the sake of a more
compact notation. We added, furthermore, a term which accounts for the possible
coupling of the spins to an external magnetic field,4 where h stands for the reduced
field h D ��BgB=2.5
</p>
<p>There are some special cases in which the ISING model can be solved analytically
[12, 13]. For instance, one can solve the general case described by Eq. (15.4) with
the help of the mean field approximation: The contribution h` acting on site `
</p>
<p>h` D h C
X
</p>
<p>`0
</p>
<p>J`�`0�`0 ; (15.5)
</p>
<p>is replaced by its mean value
</p>
<p>hh`i D h C QJm ; (15.6)
</p>
<p>4We note in passing that the Hamiltonian (15.4) is invariant under a spin flip of all spins if h D 0
(Z2 symmetry). This symmetry is broken if h &curren; 0, i.e. the spins align with the external field h.
5We note that H / � �B where B is the magnetic field and � is the magnetic moment. Furthermore,
� can be expressed as � D ��BgS=&bdquo; D ��Bg�=2, where �B is the BOHR magneton, g is the
LAND&Eacute; g-factor and � is the vector of PAULI matrices. The sign is convention.</p>
<p/>
</div>
<div class="page"><p/>
<p>228 15 The ISING Model
</p>
<p>where m D h�`i and QJ D
P
</p>
<p>` J`. (The term QJm is commonly referred to as the
molecular field.) With the help of this ansatz it is, for instance, possible to reproduce
the experimentally observed CURIE-WEISS &ndash; law of ferromagnetic materials: The
temperature dependence of the magnetic susceptibility � for T &gt; TC can be
described by:
</p>
<p>� / 1
T � TC
</p>
<p>: (15.7)
</p>
<p>Another very interesting special case of the general model (15.4) is the restriction
to nearest neighbor (n. n.) interaction with the assumption that the interaction
between non-nearest neighbor spins is negligible. One step further goes the
approximation that J`�`0 � J for nearest neighbors. Hence, we have
</p>
<p>J`�`0 D
(
</p>
<p>J if `; `0 n. n. ;
</p>
<p>0 otherwise:
(15.8)
</p>
<p>In this case Eq. (15.4) is rewritten as
</p>
<p>H D �J
2
</p>
<p>X
</p>
<p>h``0i
�`�`0 � h
</p>
<p>X
</p>
<p>`
</p>
<p>�` ; (15.9)
</p>
<p>where
P
</p>
<p>h``0i denotes the sum over all nearest neighbors. This model can be solved
analytically in one and two dimensions if the system is assumed to be spatially
unlimited. The solution in one dimension was published by E. ISING [14]. The
solution in two dimensions, which is much more involved, was reported by L.
ONSAGER [15].
</p>
<p>We briefly discuss ISING&rsquo;s solution in one dimension. The Hamiltonian (15.9)
for N-particles aligned in a one-dimensional chain is rewritten as
</p>
<p>H D �J
N
X
</p>
<p>`D1
�`�`C1 � h
</p>
<p>N
X
</p>
<p>`D1
�` ; (15.10)
</p>
<p>where we applied periodic boundary conditions, �NC1 D �1, and the factor 1=2was
absorbed into J. Let us now briefly elaborate on the kind of observables we would
like to describe within this model. (We note in passing that the following discussion
is not restricted to the one-dimensional case.) Given a particular spin configuration
C D f�ig, we assume that the probability of finding the system in this configuration</p>
<p/>
</div>
<div class="page"><p/>
<p>15.1 The Model 229
</p>
<p>is given by the BOLTZMANN distribution p.C /6:
</p>
<p>p.C / D 1
ZN
</p>
<p>exp
</p>
<p>�
</p>
<p>�E.C /
kBT
</p>
<p>�
</p>
<p>: (15.11)
</p>
<p>Here, T is the temperature and kB is BOLTZMANN&rsquo;s constant. The energy E.C /
associated with configuration C is given by Eq. (15.10). Please note that now,
obviously, we have to treat the model in the classical sense, although we consider
spin degrees of freedom. The partition function ZN is given by the sum over all
possible configurations C [3, 4, 16]:
</p>
<p>ZN D
X
</p>
<p>C
</p>
<p>exp
</p>
<p>�
</p>
<p>�E.C /
kBT
</p>
<p>�
</p>
<p>: (15.12)
</p>
<p>In general, the task of solving the ISING problem is a problem of how to evaluate the
sum (15.12). This is certainly not trivial since, for instance, in the one dimensional
case with N D 100 grid-points one has 2N D 2100 � 1:3 � 1030 different
configurationsC . On the other hand, once ZN has been determinedmore information
about the properties of the system can be derived [2, 12, 13]. For instance, the
expectation value of the energy7 is given by
</p>
<p>hEi D
X
</p>
<p>C
</p>
<p>p.C /E.C / D kBT2
@
</p>
<p>@T
lnZN ; (15.13)
</p>
<p>and the expectation value of the magnetization follows from
</p>
<p>hMi D
X
</p>
<p>C
</p>
<p>p.C /M .C / D kBT
@
</p>
<p>@h
lnZN ; (15.14)
</p>
<p>where we defined the magnetization M .C / of a configuration C via:
</p>
<p>M .C / D
 
X
</p>
<p>`
</p>
<p>�`
</p>
<p>!
</p>
<p>C
</p>
<p>: (15.15)
</p>
<p>The term
P
</p>
<p>` �` was placed within parenthesis indexed by C to emphasize
its dependence on the particular configuration C . From the observables (15.13)
and (15.14) the fluctuation quantities, namely, the magnetic susceptibility, �, and
</p>
<p>6In particular we assume ergodicity of the system as will be explained in Chap. 16.
7hEi is also referred to as internal energy U.</p>
<p/>
</div>
<div class="page"><p/>
<p>230 15 The ISING Model
</p>
<p>the heat capacity, ch, can be derived. The following relations hold:
</p>
<p>� D @
@h
</p>
<p>hMi and ch D
@
</p>
<p>@T
hEi : (15.16)
</p>
<p>Equation (15.13) is applied to rewrite the expression for the heat capacity:
</p>
<p>ch D
X
</p>
<p>C
</p>
<p>E.C /
@
</p>
<p>@T
p.C / : (15.17)
</p>
<p>Here we made use of the fact that E.C / is independent of temperature T. We
evaluate, furthermore, the derivative of p.C / with respect to temperature T:
</p>
<p>@
</p>
<p>@T
p.C / D @
</p>
<p>@T
</p>
<p>2
</p>
<p>4
</p>
<p>exp
�
</p>
<p>�E.C /
kBT
</p>
<p>�
</p>
<p>ZN
</p>
<p>3
</p>
<p>5
</p>
<p>D p.C /
kBT2
</p>
<p>ŒE.C / � hEi&#141; : (15.18)
</p>
<p>This is inserted into Eq. (15.17) and results in a final expression for the heat capacity:
</p>
<p>ch D
1
</p>
<p>kB
T2
X
</p>
<p>C
</p>
<p>p.C /
�
</p>
<p>E2.C / � E.C / hEi
�
</p>
<p>D 1
kBT2
</p>
<p>�˝
</p>
<p>E2
˛
</p>
<p>� hEi2
�
</p>
<p>D 1
kBT2
</p>
<p>var .E/ : (15.19)
</p>
<p>This result justifies why the heat capacity is referred to as a fluctuation quantity.
We determine now, following the same ideas, the magnetic susceptibility using
</p>
<p>relation (15.14):
</p>
<p>� D
X
</p>
<p>C
</p>
<p>M .C /
@
</p>
<p>@h
p.C / : (15.20)
</p>
<p>We note that
</p>
<p>@
</p>
<p>@h
E.C / D �M .C / ; (15.21)</p>
<p/>
</div>
<div class="page"><p/>
<p>15.1 The Model 231
</p>
<p>and obtain:
</p>
<p>@
</p>
<p>@h
p.C / D @
</p>
<p>@h
</p>
<p>2
</p>
<p>4
</p>
<p>exp
�
</p>
<p>�E.C /
kBT
</p>
<p>�
</p>
<p>ZN
</p>
<p>3
</p>
<p>5
</p>
<p>D p.C /
kBT
</p>
<p>ŒM .C / � hMi&#141; : (15.22)
</p>
<p>This results in a final expression for the magnetic susceptibility � which relates it to
the variance of the magnetization M:
</p>
<p>� D 1
kBT
</p>
<p>X
</p>
<p>C
</p>
<p>p.C /
�
</p>
<p>M
2.C /� M .C / hMi
</p>
<p>�
</p>
<p>D 1
kBT
</p>
<p>�˝
</p>
<p>M2
˛
</p>
<p>� hMi2
�
</p>
<p>D 1
kBT
</p>
<p>var .M/ : (15.23)
</p>
<p>After this excursion, we return to the analytic treatment of the infinite one-
dimensional ISING model with nearest neighbor interaction, Eq. (15.10). If it were
possible to evaluate the partition function ZN , the required observables would be
directly accessible via the above relations. In most cases this task is not analytically
feasible. Nevertheless, in our particular case it appears to be possible because we
recognize that we can actually evaluate Eq. (15.12) explicitly by keeping in mind
Eq. (15.9):
</p>
<p>ZN D
X
</p>
<p>C
</p>
<p>p.C /
</p>
<p>D
X
</p>
<p>C
</p>
<p>exp
</p>
<p>"
</p>
<p>1
</p>
<p>kBT
</p>
<p> 
</p>
<p>J
</p>
<p>N
X
</p>
<p>`D1
�`�`C1 C h
</p>
<p>N
X
</p>
<p>`D1
�`
</p>
<p>!#
</p>
<p>D
X
</p>
<p>C
</p>
<p>N
Y
</p>
<p>`D1
exp
</p>
<p>�
J
</p>
<p>kBT
�`�`C1 C
</p>
<p>h
</p>
<p>2kBT
.�` C �`C1/
</p>
<p>�
</p>
<p>: (15.24)
</p>
<p>In the last step the sum over �` was replaced by an alternative sum
</p>
<p>N
X
</p>
<p>`D1
�` D
</p>
<p>1
</p>
<p>2
</p>
<p>N
X
</p>
<p>`D1
.�` C �`C1/ ; (15.25)</p>
<p/>
</div>
<div class="page"><p/>
<p>232 15 The ISING Model
</p>
<p>which is a consequence of the periodic boundary conditions �NC1 D �1. Equa-
tion (15.24) can be rewritten as
</p>
<p>ZN D tr
�
</p>
<p>T
N
�
</p>
<p>; (15.26)
</p>
<p>where tr .�/ denotes the trace operation and we introduced the transfer matrix:
</p>
<p>T�;� 0 D exp
�
</p>
<p>J
</p>
<p>kBT
�� 0 C h
</p>
<p>2kBT
</p>
<p>�
</p>
<p>� C � 0
�
�
</p>
<p>: (15.27)
</p>
<p>Let us briefly clarify this point: The trace operation in the basis of the spin
eigenvalues � D ˙1 results in
</p>
<p>tr .T / D
X
</p>
<p>�
</p>
<p>T�;� D T�1;�1 C T1;1 : (15.28)
</p>
<p>Hence, we have
</p>
<p>tr
�
</p>
<p>T
N
�
</p>
<p>D
X
</p>
<p>�
</p>
<p>�
</p>
<p>T
N
�
</p>
<p>��
</p>
<p>D
X
</p>
<p>�
</p>
<p>X
</p>
<p>f�ig
iD1;:::;N�1
</p>
<p>T�;�1T�1;�2 � � �T�N�1 ;�
</p>
<p>D
X
</p>
<p>f�ig
iD1;:::;N
</p>
<p>T�1;�2T�2;�3 � � �T�N ;�1 : (15.29)
</p>
<p>In the last step we redefined the sum indices and we used the notation f�ig to indicate
that the sum runs over all possible values of �1; �2; : : : ; �N in order to abbreviate
the notation. However, the sum over all possible values of �1; �2; : : : ; �N can be
replaced by a sum over all configurations C where one configuration is a specific
combination of definite values �1; �2; : : : ; �N . For these definite values the product
of transfer matrices in Eq. (15.29) is equivalent to the product of exponentials in
Eq. (15.24) due to our definition of the transfer matrixT�;� 0 . Hence we demonstrated
that expression (15.26) is indeed equivalent to Eq. (15.24).
</p>
<p>It follows from definition (15.27) that
</p>
<p>T D
</p>
<p>0
</p>
<p>@
exp
</p>
<p>�
JCh
kBT
</p>
<p>�
</p>
<p>exp
�
</p>
<p>� J
kBT
</p>
<p>�
</p>
<p>exp
�
</p>
<p>� J
kBT
</p>
<p>�
</p>
<p>exp
�
</p>
<p>J�h
kBT
</p>
<p>�
</p>
<p>1
</p>
<p>A : (15.30)</p>
<p/>
</div>
<div class="page"><p/>
<p>15.1 The Model 233
</p>
<p>It is an easy task to determine the eigenvalues of this matrix [17, 18]. The
characteristic polynomial
</p>
<p>det
</p>
<p>0
</p>
<p>@
exp
</p>
<p>�
JCh
kBT
</p>
<p>�
</p>
<p>� � exp
�
</p>
<p>� J
kBT
</p>
<p>�
</p>
<p>exp
�
</p>
<p>� J
kBT
</p>
<p>�
</p>
<p>exp
�
</p>
<p>J�h
kBT
</p>
<p>�
</p>
<p>� �
</p>
<p>1
</p>
<p>A (15.31)
</p>
<p>is of the form
�
</p>
<p>exp
</p>
<p>�
J C h
kBT
</p>
<p>�
</p>
<p>� �
� �
</p>
<p>exp
</p>
<p>�
J � h
kBT
</p>
<p>�
</p>
<p>� �
�
</p>
<p>� exp
�
</p>
<p>� 2J
kBT
</p>
<p>�
</p>
<p>D �2 � 2� exp
�
</p>
<p>J
</p>
<p>kBT
</p>
<p>�
</p>
<p>cosh
</p>
<p>�
h
</p>
<p>kBT
</p>
<p>�
</p>
<p>C 2 sinh
�
2J
</p>
<p>kBT
</p>
<p>�
</p>
<p>ŠD 0 ; (15.32)
</p>
<p>which is easily solved. We get for the two eigenvalues �1;2
</p>
<p>�1;2 D exp
�
</p>
<p>J
</p>
<p>kBT
</p>
<p>�
</p>
<p>cosh
</p>
<p>�
h
</p>
<p>kBT
</p>
<p>�
</p>
<p>˙
s
</p>
<p>exp
</p>
<p>�
2J
</p>
<p>kBT
</p>
<p>�
</p>
<p>sinh2
�
</p>
<p>h
</p>
<p>kBT
</p>
<p>�
</p>
<p>C exp
�
</p>
<p>� 2J
kBT
</p>
<p>�
</p>
<p>; (15.33)
</p>
<p>and note that �1 � �2 for all temperatures T � 0.
We now make use of the fact that the trace is invariant under a basis transforma-
</p>
<p>tion � . Hence we can express the transfer matrix in a basis in which it is diagonal
and set
</p>
<p>T
0 D � T � �1 D
</p>
<p>�
</p>
<p>�1 0
</p>
<p>0 �2
</p>
<p>�
</p>
<p>; (15.34)
</p>
<p>which immediately results in:
</p>
<p>ZN D �N1 C �N2 : (15.35)
</p>
<p>Everything required to calculate the expectation value of energy per particle h"i
</p>
<p>h"i D kBT
2
</p>
<p>N
</p>
<p>@
</p>
<p>@T
lnZN ; (15.36)</p>
<p/>
</div>
<div class="page"><p/>
<p>234 15 The ISING Model
</p>
<p>in the thermodynamic limit N ! 1 is now in place and, thus, we can investigate
the possibility of a phase transition. First, we consider the limit
</p>
<p>lim
N!1
</p>
<p>1
</p>
<p>N
ZN D lim
</p>
<p>N!1
1
</p>
<p>N
ln
�
</p>
<p>�N1 C �N2
�
</p>
<p>D ln�1 ; (15.37)
</p>
<p>since �1 � �2 for all T � 0.8
If there is no external field, i.e. h D 0, we have
</p>
<p>lim
N!1
</p>
<p>1
</p>
<p>N
ZN D ln
</p>
<p>�
</p>
<p>2 cosh
</p>
<p>�
J
</p>
<p>kBT
</p>
<p>��
</p>
<p>; (15.38)
</p>
<p>which is a smooth function of T for T � 0. Consequently, we do not observe a
phase transition in the one dimensional ISING model. Even more information about
the system can be provided by the spin correlation function h�`�`0i
</p>
<p>h�`�`0i D
X
</p>
<p>C
</p>
<p>p.C /�`�`0 : (15.39)
</p>
<p>A basic, however, tedious calculation shows that in the thermodynamic limit it is
described by
</p>
<p>h�`�`0i D
�
�2
</p>
<p>�1
</p>
<p>�`�`0
</p>
<p>; (15.40)
</p>
<p>with the result that the spin correlation decreases with increasing distance ` � `0
since �2 &lt; �1 for T &gt; 0.
</p>
<p>We move on and briefly sketch the solution of the infinite two-dimensional ISING
model according to L. ONSAGER [15]. The HAMILTON function (15.10) changes
into:
</p>
<p>H D �J
X
</p>
<p>``0
</p>
<p>�`;`0 .�`C1;`0 C �`�1;`0 C �`;`0�1 C �`;`0C1/ � h
X
</p>
<p>`;`0
</p>
<p>�`;`0 : (15.41)
</p>
<p>8We transform
</p>
<p>�N1 C �N2 D �N1
</p>
<p>"
</p>
<p>1C
�
�2
</p>
<p>�1
</p>
<p>�N
#
</p>
<p>;
</p>
<p>and use that
�
�2
</p>
<p>�1
</p>
<p>�N
</p>
<p>! 0; as N ! 1 :</p>
<p/>
</div>
<div class="page"><p/>
<p>15.1 The Model 235
</p>
<p>The strategy developed for the one-dimensional case can again be applied: The
system is treated as a classical system with spin degrees of freedom. The HAMILTON
function (15.41) is inserted into the expression, Eq. (15.12) for the partition function
ZN . With the help of the correct basis ZN can be described by the trace over a product
of transfer matrices. However, in this case the transfer matrix T is of dimension
2N�2N rather than 2�2. It is quite obvious that the search for the largest eigenvalue
for arbitrary values of N is not a trivial task. Therefore, we limit our discussion to a
summary of the most important results for the particular case h D 0.
</p>
<p>In the two-dimensional case a phase transition is indeed observed: The magnetic
susceptibility becomes singular at a particular temperature TC. This temperature is
given as the solution of equation:
</p>
<p>2 tanh2
�
2J
</p>
<p>kBTC
</p>
<p>�
</p>
<p>D 1 : (15.42)
</p>
<p>The expectation value of the energy per particle takes on the form
</p>
<p>h"i D �J coth
�
2J
</p>
<p>kBT
</p>
<p>��
</p>
<p>1C 2
�
</p>
<p>K1.�/
</p>
<p>�
</p>
<p>2 tanh2
�
2J
</p>
<p>kBT
</p>
<p>�
</p>
<p>� 1
��
</p>
<p>; (15.43)
</p>
<p>where K1.�/ is the complete elliptic integral of the first kind [see Eq. (1.14)] with
the argument:
</p>
<p>� D
2 sinh
</p>
<p>�
2J
</p>
<p>kBT
</p>
<p>�
</p>
<p>cosh2
�
2J
</p>
<p>kBT
</p>
<p>� : (15.44)
</p>
<p>The magnetization per particle hmi is proved to be determined from
</p>
<p>hmi D
</p>
<p>8
</p>
<p>&lt;̂
</p>
<p>:̂
</p>
<p>.1C z2/ 14 .1 � 6z2 C z4/ 18p
1 � z2
</p>
<p>for T &lt; TC ;
</p>
<p>0 for T &gt; TC ;
</p>
<p>(15.45)
</p>
<p>with
</p>
<p>z D exp
�
</p>
<p>� 2J
kBT
</p>
<p>�
</p>
<p>:
</p>
<p>Equation (15.45) clearly describes a phase transition at T D TC .</p>
<p/>
</div>
<div class="page"><p/>
<p>236 15 The ISING Model
</p>
<p>15.2 Numerics
</p>
<p>We study a finite two-dimensional ISING model on a square lattice ˝ with grid-
points .xi; yj/, i; j D 1; 2; : : : ;N, which will be denoted by .i; j/. We write the
HAMILTON function in the form
</p>
<p>H D �J
X
</p>
<p>�
</p>
<p>ij
</p>
<p>i0j0
</p>
<p>�
</p>
<p>�i;j�i0 ;j0 � h
X
</p>
<p>ij
</p>
<p>�i;j ; (15.46)
</p>
<p>where the �i;j 2 f�1; 1g are treated as &lsquo;classical&rsquo; spins. We consider nearest
neighbor interaction and regard the exchange parameter as independent of the actual
positions i; j. The problem is easily motivated:We calculate numerically observables
like the expectation value of the energy or of the magnetization which will then be
compared with analytic results. Such a procedure provides a rather simple check
of the quality of the numerical approach which can then be extended to similar
models which cannot any longer be treated analytically.We need numericalmethods
because summing over all possible configurations in a calculation of the partition
function ZN is simply no longer feasible since, for instance, for N D 100 we have
2N
</p>
<p>2 D 210000 � 103000 possible configurations which will have to be considered as
follows from Eqs. (15.12), (15.13), and (15.14). A more convenient approach would
be to approximate the sums with the help of methods we encountered within the
context of Monte-Carlo integration in Sect. 14.2. For instance, the estimate of the
energy expectation value is given by
</p>
<p>hEi D 1
M
</p>
<p>M
X
</p>
<p>iD1
E.Ci/˙
</p>
<p>r
</p>
<p>var .E/
</p>
<p>M
: (15.47)
</p>
<p>Here, Ci, i D 1; 2; : : : ;M are M configurations drawn from the pdf (15.11), the
BOLTZMANN distribution. Equation (15.47) is referred to as the estimator of the
internal energy. We note that we also have to calculate an estimate of the variance
of E using a similar approach in order to determine the error induced by this
approximation.9
</p>
<p>Hence, there remains the task to find configurations Ci which follow the
BOLTZMANN distribution (15.11). The inverse transformation method of Sect. 13.2
cannot be applied since E.Ci/ is not invertible. Furthermore, the rejection method
</p>
<p>9In particular var .E/ D
˝
</p>
<p>E2
˛
</p>
<p>�hEi2 is to be determined and only the second term is already known.
The first term,
</p>
<p>˝
</p>
<p>E2
˛
</p>
<p>, is then estimated with the help of
</p>
<p>˝
</p>
<p>E2
˛
</p>
<p>D 1
M
</p>
<p>M
X
</p>
<p>iD1
</p>
<p>E2i :</p>
<p/>
</div>
<div class="page"><p/>
<p>15.2 Numerics 237
</p>
<p>is useless since we would need the partition function ZN to make it work. However,
calculating the partition function is a task as difficult as calculating the internal
energy (15.13) without any approximations. Therefore, the method of choice will
be the METROPOLIS algorithm discussed in Sect. 14.3.
</p>
<p>Let C be a given spin configuration on the two-dimensional square lattice
˝ . We modify the spin on one particular grid-point .i; j/ and obtain a trial spin
configuration C t. According to our discussion in Sect. 14.3, the probability of
accepting the new configuration C t is then given by
</p>
<p>Pr.AjC t;C / D min
�
</p>
<p>p.C t/
</p>
<p>p.C /
; 1
</p>
<p>�
</p>
<p>D min
�
</p>
<p>exp
</p>
<p>�
</p>
<p>�E.C
t/ � E.C /
kBT
</p>
<p>�
</p>
<p>; 1
</p>
<p>�
</p>
<p>D min
�
</p>
<p>exp
</p>
<p>�
</p>
<p>�
Eij
kBT
</p>
<p>�
</p>
<p>; 1
</p>
<p>�
</p>
<p>: (15.48)
</p>
<p>The spin orientation was changed only on one grid-point .i; j/, with �i;j ! O�i;j D
��i;j; thus, the energy difference
Eij is easily evaluated using
</p>
<p>
Eij D 2J�i;j
�
</p>
<p>�iC1;j C �i�1;j C �i;j�1 C �i;jC1
�
</p>
<p>C 2h�i;j : (15.49)
</p>
<p>with �i;j the original spin orientation.
We focus now on numerical details, some particular to the numerical treatment of
</p>
<p>the ISING model [19], and some of rather general nature which should be considered
whenever a Monte-Carlo simulation is planned.
</p>
<p>(1) Lattice Geometry
</p>
<p>We regard a two-dimensional N � N square lattice with periodic boundary condi-
tions10 in order to reduce finite volume effects. It is of advantage to write a program
code which will help to identify the nearest neighbors of some grid-point, since
we will need this information in the METROPOLIS run whenever we calculate the
energy difference due to a spin flip according to Eq. (15.49). To help with this task
a matrix neighbor.site; i/ will be generated only once for each choice of the system
size N. Here i D 1; 2; 3; 4 are the directions to the neighboring grid-points of the
grid-point site. In a first step the sites of the square lattice are relabeled following
</p>
<p>10Periodic boundary conditions in two dimensions imply that
</p>
<p>�NC1;j D �1;j and �i;NC1 D �i;1 ;
</p>
<p>for all i; j.</p>
<p/>
</div>
<div class="page"><p/>
<p>238 15 The ISING Model
</p>
<p>the scheme11:
</p>
<p>N.N � 1/C 1 � � � N2
:::
</p>
<p>:::
:::
</p>
<p>N C 1 � � � 2N
1 2 � � � N :
</p>
<p>(15.50)
</p>
<p>In the next step, the matrix neighbor is initialized as an array of size N2 � 4.
Every site has four nearest neighbors: up, right, down, and left. The corresponding
matrix elements for periodic boundary conditions can be evaluated according to the
following scheme:
</p>
<p>&bull; For up we have:
</p>
<p>(a) If site C N � N2: up D site C N,
(b) else if site C N &gt; N2: up D site � N.N � 1/.
</p>
<p>&bull; For right we have:
</p>
<p>(a) If mod.site;N/ &curren; 0: right D site C 1,
(b) else if mod.site;N/ D 0: right D site � N C 1.
</p>
<p>&bull; For down we have:
</p>
<p>(a) If site � N � 1: down D site � N,
(b) else if site � N &lt; 1: down D site C N.N � 1/
</p>
<p>&bull; For left we have:
</p>
<p>(a) If mod.site � 1;N/ &curren; 0: left D site � 1,
(b) else if mod.site � 1;N/ D 0: left D site C N � 1.
</p>
<p>In a final step, the array elements are rearranged according to
</p>
<p>neighbor.site; W/ D Œup; right; down; left&#141; ; (15.51)
</p>
<p>where site D 1; 2; : : : ;N2.
</p>
<p>(2) Initialization
</p>
<p>It has already been discussed in Sect. 14.3 that the quality of random numbers
generated with the help of the METROPOLIS algorithm is highly dependent on the
choice of initial conditions. This is, in our case, the initial spin configuration C0.
</p>
<p>11In the following we will refer to the notation .i/, i D 1; 2; : : : ;N2 as the single-index notation
while the notation .i; j/, i; j D 1; 2; : : : ;N will be referred to as the double-index notation.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.2 Numerics 239
</p>
<p>Of course, it would be favorable to start with a configuration which was already
drawn from the BOLTZMANN distribution p.C /. However, in practice this is not
feasible ab initio. But, as will be elucidated in Chap. 16, theMETROPOLIS algorithm
produces configurations which become independent of the initial state and follow
the BOLTZMANN distribution. Hence we can simply start with some arbitrary
configuration and discard it together with the first n constituents of the sequence
C0;C1; : : : ;Cn. This method is referred to as thermalization.12 The question arises:
can n be determined to ensure that the sequence starting with CnC1 will conform to
the pdf p.C /?
</p>
<p>There are two different ways to approach this problem: (i) The first is to measure
auto-correlations between configurations Ci where it has to be ensured that the
set of states is sufficiently large to allow for a significant conclusion. We will
discuss auto-correlations in more detail in Chap. 19. (ii) The second approach is
to empirically check whether equilibrium has been reached or not. For instance,
one could simply plot some selected observables and check when the initial bias
vanishes. In this case the observable reaches some saturation value as a function of
the number of measurements. A particularly useful method is to start the algorithm
with at least two different configurations. As soon as equilibrium has been reached,
the observables should approach the same saturation values after a certain (finite)
number of measurements. Typical choices are the cold start and the hot start. Cold
start means that the temperature is initially below the critical temperature, i.e. in
the ISING model all spins are aligned (ferromagnetic state). Hot start means that the
temperature is well above the critical temperature and for the ISING model the spin
orientation is chosen at random for any site (paramagnetic state).
</p>
<p>(3) Execution of the Code
</p>
<p>The METROPOLIS algorithm for the ISING model is executed in the following
steps:
</p>
<p>1. Choose an initial configuration C0.
2. We migrate through the lattice sites systematically.13 Suppose we just reached
</p>
<p>site .i; j/ (we use the double-index notation i; j D 1; 2; : : : ;N, to improve the
readability) and our current configuration is Ck. Then k configurations have been
accepted so far. We generate a new configuration C t from Ck by replacing in Ck
the entry �i;j by ��i;j.
</p>
<p>12The number of configurations discarded is referred to as the thermalization length.
13A migration through all lattice sites is referred to as a sweep.</p>
<p/>
</div>
<div class="page"><p/>
<p>240 15 The ISING Model
</p>
<p>3. The new configuration is accepted with probability
</p>
<p>Pr.AjC t;Ck/ D min
�
</p>
<p>exp
</p>
<p>�
</p>
<p>�
Eij
kBT
</p>
<p>�
</p>
<p>; 1
</p>
<p>�
</p>
<p>; (15.52)
</p>
<p>where 
Eij is determined from Eq. (15.49). C t is accepted if Pr.AjC t;Ck/ is
equal to one or if Pr.AjC t;Ck/ � r 2 Œ0; 1&#141; otherwise C t is rejected. If C t was
accepted we set CkC1 D C t.
</p>
<p>4. Go to the next lattice site [step 2].
</p>
<p>We note that instead of sampling the lattice sites sequentially as suggested in step
2 the lattice sites can also be sampled randomly with the help of
</p>
<p>i D int.rN2/C 1 ; (15.53)
</p>
<p>where r 2 Œ0; 1&#141; is a uniformly distributed random number and int.�/ denotes the
integer part of a given quantity. Obviously, Eq. (15.53) is only useful in the single-
index notation i D 1; 2; : : : ;N2.
</p>
<p>(4) Measurement
</p>
<p>As soon as thermalization was achieved the procedure to measure interesting
observables can be started. Such a procedure consists of collecting the data required
and in calculating expectation values as was illustrated in Eq. (15.47) for the case of
the expectation value of the energy. A more detailed study of estimator techniques is
postponed to Chap. 19. However, there is one crucial point one should be aware of:
We already mentioned in our discussion of theMETROPOLIS algorithm in Sect. 14.3
that subsequent configurations Ck may be strongly correlated. This problem can be
circumvented by simply neglecting intermediate configurations. For instance, one
may allow a couple of &lsquo;empty&rsquo; sweeps between two measurements.
</p>
<p>In the following we discuss some selected results obtained with the numerical
approach described above.
</p>
<p>15.3 Selected Results
</p>
<p>We investigate the two-dimensional ISING model with periodic boundary conditions
and we chose h D 0 and J D 0:5 for all following illustrations.
</p>
<p>In a first experiment we plan to check the thermalization process and, thus,
measure after every single sampling step and skip thermalization. The observables
of interest, the expectation value of the energy per particle, h"i, and the expectation
value of the magnetization per particle, hmi, are illustrated in Fig. 15.3 for 30 sweeps</p>
<p/>
</div>
<div class="page"><p/>
<p>15.3 Selected Results 241
</p>
<p>Fig. 15.3 Time evolution of
(a) the expectation value of
the energy per particle h"i
and (b) of the expectation
value of the magnetization
per particle hmi vs the
number of measurements M.
We used a cold start (solid
line) and a hot start (dashed
line) to achieve these results
</p>
<p>Fig. 15.4 Typical spin
configuration for a
temperature well above the
critical temperature TC . Black
shaded areas correspond to
spin up sites while the white
areas are spin down sites
</p>
<p>in a system of the size N D 50 which corresponds to m � 8 � 104 measurements.
Moreover, we set kBT D 3 which should be well above TC according to Eq. (15.42).
Hence, we expect paramagnetic behavior, i.e. hmi D 0 in the equilibrium since the
acceptance probability is rather large because the spins are randomly orientated. In
addition, Fig. 15.4 shows a typical spin configuration for a temperature well above
TC.
</p>
<p>According to Fig. 15.3b the expectation value of the magnetization per particle
hmi approaches indeed zero after a rather short thermalization period independent
of the starting procedure. This is certainly not the case for the energy expectation
value per particle h"i, Fig. 15.3a, which does not approach saturation even after
M � 8 � 104 measurements for both starting procedures. The consequence is that
the thermalization period certainly needs to be longer than only 30 sweeps.</p>
<p/>
</div>
<div class="page"><p/>
<p>242 15 The ISING Model
</p>
<p>Keeping this result in mind we move on to perform the next check of our
numerics, namely to study the influence of the system size N on the numerical
results we get for the observables h"i, hmi as well as ch and � as functions of
temperature T. Let us outline the strategy: a thermalization period of 500 sweeps
will be used and 10 sweeps between each measurement will be discarded.Moreover,
we start with the hot start configuration and at a temperature kBT0 D 3 well above
TC. After the measurements at T0 have been finished, the temperature is slightly
decreased, T1 &lt; T0.
</p>
<p>One more point should be addressed: We perform a simulation using the
strategy outlined above and obtain as a result some observable O as a function of
temperatures fTng, with T0 the initial temperature well above TC and TnC1 &lt; Tn.
From the physics point of view, this temperature dependence will, of course, be
most interesting for temperatures T � TC. Thus, what we need is an adaptive
cooling strategy designed in such a way that the temperature is decreased rapidly
for temperatures T � TC or T � TC , but for T � TC the temperature is modified
only minimally. [This question will also be a very important point in the discussion
of simulated annealing, a stochastic optimization strategy (see Sect. 20.3).] At the
moment we are satisfied with equally spaced temperatures, i.e. TkC1 D Tk�ı, where
ı D const because we are mainly interested to study the influence of the system size
N on our calculations.
</p>
<p>The error bars of the calculated expectation values have been obtained with the
help of Eq. (15.47). The error estimates for the heat capacity ch as well as for the
magnetic susceptibility � are more complex to evaluate. The method employed is
referred to as statistical bootstrap, where M D 100 samples have been generated.
This method will be explained in some detail in Chap. 19.
</p>
<p>In Fig. 15.5 we compare the expectation value of the energy per particle, h"i, the
absolute value of the magnetization per particle jhmij, the overall heat capacity ch
and the overall magnetic susceptibility � for four system sizes N D 5; 20; 50; 100.
Furthermore, in Fig. 15.6 we show the curves for the system size of N D 50 together
with corresponding error bars.
</p>
<p>We observe that the phase transition becomes sharper with increasing system
size. In fact we know, that the phase transition is infinitely sharp as N ! 1 from
the analytic solution given by ONSAGER. It is a quite obvious result of this study
that the system size N should be greater than 20 to achieve acceptable results.
</p>
<p>Furthermore,we presented the absolute value of the magnetization rather than the
magnetization itself. The reason is that for T &lt; TC the ground state is degenerate. In
particular, the state with all spins up or all spins down is equally probable since we
set the external magnetic field h D 0. This is a manifestation of the Z2 symmetry of
the Hamiltonian discussed in Sect. 15.1.
</p>
<p>Of particular interest is the region around the critical temperature, referred to
as the critical region. In this region, the spins are not perfectly aligned and not ran-
domly orientated either. In this region the spins align in so called magnetic domains,
which are also referred to as WEISS domains [1]. A typical spin configuration which
exhibits such domains is presented in Fig. 15.7.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.3 Selected Results 243
</p>
<p>Fig. 15.5 (a) The expectation value of the energy per particle h"i, (b) the absolute value of the
expectation value of the magnetization per particle jhmij, (c) the heat capacity ch, and (d) the
magnetic susceptibility � vs temperature kBT for the two-dimensional ISING model. The system
sizes are N D 5; 20; 50; 100
</p>
<p>We conclude this chapter with an interesting note: Fig. 15.6 makes it quite clear
that the error of the expectation value of the magnetization and of the energy is
biggest for values around the transition temperature. In fact, if we increase the
system size the error will become even larger. The reason is quite obvious: The error
of our Monte-Carlo integration is proportional to the square root of the variance
of the investigated observable. However, since we deal with a second order phase
transition, this variance tends to infinity as N ! 1 [4]. There is one cure to the
problem:We are dealing here with finite-sized systems, thus, the variance will never
actually be infinitely large. Furthermore, according to Eq. (15.47) we can decrease
the error by increasing the number of measurements. Hence, if one is confronted
with large systems, one has also to perform many measurements in order to reduce
the error.14
</p>
<p>14We note from Eq. (15.47) that we have to perform four times as many measurements in order to
reduce the error by a factor 2.</p>
<p/>
</div>
<div class="page"><p/>
<p>244 15 The ISING Model
</p>
<p>Fig. 15.6 (a) The expectation value of the energy per particle h"i, (b) the expectation value of
the magnetization per particle jhmij, (c) the heat capacity ch , and (d) the magnetic susceptibility �
with error bars vs temperature kBT obtained for the two-dimensional ISING model of size N D 50
</p>
<p>Fig. 15.7 For T � TC the spins organize in WEISS domains. Here we show a typical spin
configuration for N D 100 and kBT D 1:15. The black shaded areas correspond to spin up sites
while the white areas indicate spin down sites</p>
<p/>
</div>
<div class="page"><p/>
<p>15.3 Selected Results 245
</p>
<p>Summary
</p>
<p>The ISINGmodel is a rather simple model which describes effectively a second order
phase transition. Such phase transitions are the topic of extensive numerical studies
and, therefore, this model served here as a tool to demonstrate how to proceed from
the problem analysis to a numerical algorithm which will allow to simulate the
physics. The advantage of the ISING model was that under certain simplifications
solutions could be derived analytically. In the course of this analysis the important
concept of observables was introduced. Observables are certain physical properties
of a system which characterize the specific phenomenon of interest. Numerically,
observables are certain variables which are to be &lsquo;measured&rsquo; within the course of
a simulation. After the extensive analysis of the ISING model the transition to the
numerical analysis of the two-dimensional ISING model was a rather easy part. The
required modification of spin configurations turned out to be the key element of
the simulation and this suggested the application of the METROPOLIS algorithm
for sampling. Finally, important problems like initialization of the simulation,
thermalization, finite size effects, measurement of observables, and the prevention
of correlations between subsequent spin configurations caused by the METROPOLIS
algorithm have been discussed on the basis of concrete calculations. The first part
of this chapter was motivated by W. S. DORN and D. D. MCCRACKEN [20]:
</p>
<p>Numerical methods are no excuse for poor analysis.
</p>
<p>Problems
</p>
<p>1. Write a program to simulate the two-dimensional ISING model with periodic
boundary conditions with the help of the METROPOLIS algorithm. Follow the
strategy outlined in Sect. 15.2 and try to reproduce the results illustrated in
Sect. 15.3 for N D 5; 20; 50.
</p>
<p>In particular, as a first step write a routine which stores the nearest neighbors
of the square lattice in an array. As a second step, write a programwhich performs
a sweep through the lattice geometry. You can either choose the lattice sites
systematically or at random. As a third step, set up the main program which calls
the sweep routine. Choose some initial configuration and thermalize the system.
Measure the expectation value of the energy per particle as well as the absolute
value of the expectation value of the magnetization for different temperatures kBT
and determine the respective errors, see Eq. (15.47). Calculate also the overall
magnetic susceptibility and the overall heat capacity. The determination of the
error is more complicated in this case and can therefore be neglected for the
moment.
</p>
<p>Good parameters to start with are J D 0:5, Ntherm D 500, Nskip D 10 and
h D 0:0.
</p>
<p>2. Try also different values of J and h &curren; 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>246 15 The ISING Model
</p>
<p>References
</p>
<p>1. White, R.M.: Quantum Theory of Magnetism, 3rd edn. Springer Series in Solid-State Sciences.
Springer, Berlin/Heidelberg (2007)
</p>
<p>2. Landau, L.D., Lifshitz, E.M.: Course of Theoretical Physics. Statistical Physics, vol. 5.
Pergamon Press, London (1963)
</p>
<p>3. Halley, J.W.: Statistical Mechanics. Cambridge University Press, Cambridge (2006)
4. Pathria, R.K., Beale, P.D.: Statistical Mechanics, 3rd edn. Academic, San Diego (2011)
5. Baym, G.: Lectures on Quantum Mechanics. Lecture Notes and Supplements in Physics. The
</p>
<p>Benjamin/Cummings, London/Amsterdam (1969)
6. Cohen-Tannoudji, C., Diu, B., Lalo&euml;, F.: Quantum Mechanics, vol. I. Wiley, New York (1977)
7. Sakurai, J.J.: Modern Quantum Mechanics. Addison-Wesley, Menlo Park (1985)
8. Ballentine, L.E.: Quantum Mechanics. World Scientific, Hackensack (1998)
9. Arnol&rsquo;d, V.I.: Mathematical Methods of Classical Mechanics, 2nd edn. Graduate Texts in
</p>
<p>Mathematics, vol. 60. Springer, Berlin/Heidelberg (1989)
10. Goldstein, H., Poole, C., Safko, J.: Classical Mechanics, 3rd edn. Addison-Wesley, Menlo Park
</p>
<p>(2013)
11. Scheck, F.: Mechanics, 5th edn. Springer, Berlin/Heidelberg (2010)
12. Mandl, F.: Statistical Physics, 2nd edn. Wiley, New York (1988)
13. Schwabl, F.: Statistical Mechanics. Advanced Texts in Physics. Springer, Berlin/Heidelberg
</p>
<p>(2006)
14. Ising, E.: Beitrag zur Theorie des Ferromagnetismus. Z. Phys. 31, 253 (1925)
15. Onsager, L.: Crystal statistics, I. A two-dimensional model with an order-disorder transition.
</p>
<p>Phys. Rev. 65, 117&ndash;149 (1944). doi:10.1103/PhysRev.65.117
16. Hardy, R.J., Binek, C.: Thermodynamics and Statistical Mechanics: An Integrated Approach.
</p>
<p>Wiley, New York (2014)
17. Kwak, J.H., Hong, S.: Linear Algebra. Springer, Berlin/Heidelberg (2004)
18. Strang, G.: Introduction to Linear Algebra, 4th edn. Cambridge University Press, Cambridge
</p>
<p>(2009)
19. Stauffer, D., Hehl, F.W., Ito, N., Winkelmann, V., Zabolitzky, J.G.: Computer Simulation and
</p>
<p>Computer Algebra, pp. 79&ndash;84. Springer, Berlin/Heidelberg (1993)
20. Dorn, W.S., McCracken, D.D.: Numerical Methods with Fortran IV Case Studies. Wiley,
</p>
<p>New York (1972)</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 16
Some Basics of Stochastic Processes
</p>
<p>16.1 Introduction
</p>
<p>This chapter is devoted to an introduction to some basic concepts of stochastic
processes. This introduction serves two purposes: First of all, it allows for a more
systematic treatment of non-deterministic methods in Computational Physics which
is certainly necessary if we really aim at an understanding of these methods.
The second reason can be found in the elementary importance of stochastics in
modern theoretical physics and chemistry in general. Hence, many of the concepts
elaborated within this chapter will be of profound importance in subsequent
chapters. For instance, we present a discussion of diffusion theory in Chap. 17 as
a motivating example.
</p>
<p>The reader not familiar with the basics of probability theory [1&ndash;4] is highly
encouraged to at least consult Appendix E before proceeding. In particular, we are
going to apply the notation introduced in Appendix E throughout this chapter.
</p>
<p>The basics of stochastic processes will be discussed within five sections including
this introduction. In Sect. 16.2 we introduce basic definitions associated with
stochastic processes in general. Here we discuss concepts which will serve as a
basis for an understanding of the methods presented within the subsequent sections.
Section 16.3 deals with a special class of stochastic processes, the so called
MARKOV processes. As we shall see, these processes are of fundamental importance
for statistical physics and for computational methods. Moreover, in Sect. 16.4
we consider so called MARKOV-chains which are discrete MARKOV processes
defined on a discrete time span. This will serve as the basis of a very important
method in computational physics, the so called MARKOV-Chain Monte Carlo
technique. We already encountered a simple example of this method in Sect. 14.3
and in Chap. 15, the METROPOLIS algorithm. Finally, in Sect. 16.5 continuous-time
MARKOV-chains will be discussed, in particular, discrete MARKOV processes on a
continuous time span. These processes are very important, for instance, in diffusion
theory as will be demonstrated in Chap. 17.
</p>
<p>&copy; Springer International Publishing Switzerland 2016
B.A. Stickler, E. Schachinger, Basic Concepts in Computational Physics,
DOI 10.1007/978-3-319-27265-8_16
</p>
<p>247</p>
<p/>
</div>
<div class="page"><p/>
<p>248 16 Some Basics of Stochastic Processes
</p>
<p>A discussion of detailed balance will also be included in the section on MARKOV
processes, Sect. 16.3, although detailed balance follows from physical arguments.
Detailed balance has already been introduced in our discussion of the METROPOLIS
algorithm, Sect. 14.3.
</p>
<p>16.2 Stochastic Processes
</p>
<p>The following discussion is primarily restricted to one-dimensional processes.
A stochastic process is a time dependent process depending on randomness [5&ndash;
</p>
<p>7]. From a mathematical point of view, a stochastic process YX.t/ is a random
variable Y which is a function of another random variable X and time t � 0:
</p>
<p>YX.t/ D f .X; t/ : (16.1)
</p>
<p>Here we apply the notation of Appendix E and denote random variables by
capital letters, such as X, and their realization by lower case characters, such as
x. Consequently, the realization of a stochastic process is described by
</p>
<p>Yx.t/ D f .x; t/ : (16.2)
</p>
<p>The set of all possible realizations of YX.t/ spans the state space of the stochastic
process. We note that it is in principle not necessary to define t as the time in a
classical sense. It suffices to denote t 2 T, where T is a totally ordered set such as,
for instance, T D N the natural numbers. The set T is referred to as the time span.
We distinguish four different scenarios:
</p>
<p>&bull; discrete state space, discrete time span,
&bull; continuous state space, discrete time span,
&bull; discrete state space, continuous time span,
&bull; continuous state space, continuous time span.
</p>
<p>Stochastic processes on a continuous time span are referred to as continuous-time
stochastic processes.
</p>
<p>Suppose the random variable X follows the pdf pX.x/. It is then an easy task to
calculate averages hY.t/i of the stochastic process YX.t/ via
</p>
<p>hY.t/i D
Z
</p>
<p>dx Yx.t/pX.x/ : (16.3)
</p>
<p>This concept is easily extended to multiple times t1; t2; : : : ; tn by
</p>
<p>hY.t1/Y.t2/ � � �Y.tn/i D
Z
</p>
<p>dx Yx.t1/Yx.t2/ � � �Yx.tn/pX.x/ ; (16.4)</p>
<p/>
</div>
<div class="page"><p/>
<p>16.2 Stochastic Processes 249
</p>
<p>which defines the moments of the stochastic process [1, 5, 8]. Similar to the concept
of the correlation coefficient (see Appendix, Sect. E.10) we define the so called auto-
correlation function �.t1; t2/:
</p>
<p>�.t1; t2/ D
hŒY.t1/ � hY.t1/i&#141; ŒY.t2/ � hY.t2/i&#141;i
</p>
<p>r
D
</p>
<p>ŒY.t1/ � hY.t1/i&#141;2
E D
</p>
<p>ŒY.t2/ � hY.t2/i&#141;2
E
</p>
<p>D hY.t1/Y.t2/i � hY.t1/i hY.t2/ip
varŒY.t1/&#141;varŒY.t2/&#141;
</p>
<p>D &#13;ŒY.t1/;Y.t2/&#141;p
varŒY.t1/&#141;varŒY.t2/&#141;
</p>
<p>: (16.5)
</p>
<p>The function &#13;ŒY.t1/;Y.t2/&#141; is referred to as the auto-covariance function and is
defined as
</p>
<p>&#13;ŒY.t1/;Y.t2/&#141; D covŒY.t1/;Y.t2/&#141; : (16.6)
</p>
<p>We proceed by defining the pdf of a stochastic process YX.t/. The pdf p1.y; t/,
which describes the probability that the stochastic process YX.t/ takes on its
representation y at time t, is given by (see Sect. 14.2)
</p>
<p>p1.y; t/ D
Z
</p>
<p>dx ıŒ y � Yx.t/&#141;pX.x/ : (16.7)
</p>
<p>We define, in analogy, the pdf pn.y1; t1; y2; t2; : : : ; yn; tn/ which describes the
probability that the stochastic process takes on the realization y1 at time t1, y2 at
time t2, . . . , and yn at time tn for arbitrary n:
</p>
<p>pn.y1; t1; y2; t2; : : : ; yn; tn/ D
Z
</p>
<p>dx ıŒ y1 � Yx.t1/&#141;ıŒ y2 � Yx.t2/&#141; � � �
</p>
<p>�ıŒ yn � Yx.tn/&#141;pX.x/ : (16.8)
</p>
<p>This is referred to as the hierarchy of pdfs. We note the following important
properties of the pdf pn.y1; t1; y2; t2; : : : ; yn; tn/ [8]:
</p>
<p>&bull; pn.y1; t1; y2; t2; : : : ; yn; tn/ � 0 ; (16.9)
&bull; pn.: : : ; yk; tk : : : ; y`; t`; : : :/ D pn.: : : ; y`; t` : : : ; yk; tk; : : :/ ; (16.10)
&bull;
Z
</p>
<p>dyn pn.y1; t1; : : : ; yn; tn/ D pn�1.y1; t1; : : : ; yn�1; tn�1/ ; (16.11)
</p>
<p>&bull;
Z
</p>
<p>dyp1.y; t/ D 1 : (16.12)</p>
<p/>
</div>
<div class="page"><p/>
<p>250 16 Some Basics of Stochastic Processes
</p>
<p>The moments defined in Eq. (16.4) can also be expressed with the help of the pdfs
pn by
</p>
<p>hY.t1/Y.t2/ � � �Y.tn/i D
Z
</p>
<p>dy1 : : : dyny1 � � � ynpn.y1; t1; : : : ; yn; tn/ : (16.13)
</p>
<p>Conditional pdfs p`jk can also be introduced. They describe the probability that
we have ykC1 at tkC1, . . . , ykC` at tkC` if there existed y1 at t1, . . . , yk at tk via
</p>
<p>p`jk.ykC1; tkC1; : : : ; ykC`; tkC`jy1; t1; : : : ; yk; tk/ D
pkC`.y1; t1; : : : ; ykC`; tkC`/
</p>
<p>pk.y1t1; : : : ; yk; tk/
:
</p>
<p>(16.14)
</p>
<p>It follows that
Z
</p>
<p>dy2 p1j1.y2; t2jy1; t1/ D 1 : (16.15)
</p>
<p>Let us give some further definitions [8]:
</p>
<p>&bull; A stochastic process is referred to as a stationary process if the moments defined
in Eq. (16.4) are invariant under a time-shift 
t:
</p>
<p>hY.t1/Y.t2/ � � �Y.tn/i D hY.t1 C
t/Y.t2 C
t/ � � �Y.tn C
t/i : (16.16)
</p>
<p>In particular, one has hY.t/i D const and the auto-covariance depends only on
the time difference jt1 � t2j:
</p>
<p>&#13;.t1; t2/ D covŒY.t1/;Y.t2/&#141; D covŒY.0/;Y.jt1 � t2j/&#141; � &#13;.t1 � t2/ : (16.17)
</p>
<p>It is understood that &#13;.t/ D &#13;.�t/. Moreover, we have
</p>
<p>pn.y1; t1 C
t; : : : ; yn; tn C
t/ D pn.y1; t1; : : : ; yn; tn/ ; (16.18)
</p>
<p>and in particular, p1.y; t/ D p1.y/.
&bull; A time-homogeneous process is a stochastic process whose conditional pdfs are
</p>
<p>stationary
</p>
<p>p1j1.y2; t2jy1; t2 � �/ D p1j1.y2; s2jy1; s2 � �/ ; (16.19)
</p>
<p>for all t2; �; s2. The pdf p1j1 is referred to as transition probability.
&bull; A process of stationary increments is a stochastic process YX.t/ for which the
</p>
<p>difference YX.t2/�YX.t1/ is stationary for all t2�t1, with t2 &gt; t1 � 0. This means,
in particular, that the pdf of this process depends only on the time difference
t2 � t1. The quantities YX.t2/ � YX.t1/ are referred to as increments.</p>
<p/>
</div>
<div class="page"><p/>
<p>16.3 MARKOV Processes 251
</p>
<p>&bull; A process of independent increments is a stochastic process YX.t/ for which the
differences
</p>
<p>YX.t2/� YX.t1/;YX.t3/ � YX.t2/; : : : ;YX.tn/ � YX.tn�1/ ;
</p>
<p>are independent for all tn &gt; tn�1 &gt; : : : &gt; t2 &gt; t1.
&bull; A L&Eacute;VY process is a continuous-time stochastic process with stationary
</p>
<p>independent increments which starts with YX.0/ D 0.
&bull; A Gaussian process is a stochastic process YX.t/ for which all finite linear
</p>
<p>combinations of YX.t/, t 2 T follow a normal distribution (see Appendix,
Sect. E.7). We shall come back to this kind of process in Chap. 17.
</p>
<p>&bull; A WIENER process is a continuous-time stochastic process with independent
increments which starts with YX.0/ D 0 and for which the increments YX.t2/ �
YX.t1/ follow a normal distribution with mean 0 and variance t2�t1. TheWIENER
process is a special case of a L&Eacute;VY process. One of the main applications of the
WIENER process is to study Brownian motion or diffusion. This process will be
discussed in more detail in Sect. 16.3 and in Chap. 17.
</p>
<p>&bull; The random walk is the discrete analogy to the WIENER process [9&ndash;11]. This
means in particular that if the step size of the random walk goes to zero, the
WIENER process is reestablished. This point will be elucidated in Chap. 17.
</p>
<p>After stating the most important definitions, we proceed to the next section
in which the attention is on a special class of stochastic processes, the so called
MARKOV processes.
</p>
<p>16.3 MARKOV Processes
</p>
<p>A MARKOV process is a stochastic process YX.t/ for which the conditional pdf
p1jn�1 satisfies for arbitrary n and t1 &lt; t2 &lt; � � � &lt; tn the relation
</p>
<p>p1jn�1.yn; tnjy1; t1; : : : ; yn�1; tn�1/ D p1j1.yn; tnjyn�1; tn�1/ : (16.20)
</p>
<p>Hence, a MARKOV process is a process in which any state yn; tn is uniquely defined
by its precursor state yn�1; tn�1 and is independent of the entire rest of the past [12].
MARKOV processes are of particular importance in natural sciences because of their
rather simple structure. This will become clear throughout the rest of this book.
</p>
<p>We note in passing that a process with independent increments is always a
MARKOV process because
</p>
<p>YX.tnC1/ D YX.tn/C ŒYX.tnC1/ � YX.tn/&#141; ; (16.21)</p>
<p/>
</div>
<div class="page"><p/>
<p>252 16 Some Basics of Stochastic Processes
</p>
<p>is satisfied. Since the increment YX.tnC1/ � YX.tn/ is independent of all previous
increments which gave rise to YX.tn/ by definition, YX.tnC1/ depends only on YX.tn/,
which is exactly the MARKOV property (16.20).
</p>
<p>The quantity p1j1.yn; tnjyn�1; tn�1/ which appears in Eq. (16.20) is referred to as
transition probability. Given the transition probability p1j1 together with the pdf p1,
one can construct the whole hierarchy of pdfs (16.8) of the MARKOV process by
calculating successively [8]:
</p>
<p>p2.y1; t1; y2; t2/ D p1j1.y2; t2jy1; t1/p1.y1; t1/ ;
p3.y1; t1; y2; t2; y3; t3/ D p1j2.y3; t3jy1; t1; y2; t2/p2.y1; t1; y2; t2/ ;
</p>
<p>D p1j1.y3; t3jy2; t2/p1j1.y2; t2jy1; t1/p1.y1; t1/ ;
:::
</p>
<p>::: (16.22)
</p>
<p>Here we employed definition (16.14) and in the second step of the second equation
we employed for p1j2 the MARKOV property (16.20).
</p>
<p>The fact that the whole hierarchy of pdfs can be constructed by repeating the steps
illustrated in Eq. (16.22) reveals the rather simple structure of MARKOV processes.
However, Eq. (16.22) contains another useful information. We regard the pdf p3
of (16.22) for three successive times t1 &lt; t2 &lt; t3. First we integrate the left-hand
side with respect to y2 which yields with the help of property (16.10):
</p>
<p>Z
</p>
<p>dy2 p3.y1; t1; y2; t2; y3; t3/ D p2.y1; t1; y3; t3/ : (16.23)
</p>
<p>Hence, we have
</p>
<p>p2.y1; t1; y3; t3/ D p1.y1; t1/
Z
</p>
<p>dy2 p1j1.y3; t3jy2; t2/p1j1.y2; t2jy1; t1/ ; (16.24)
</p>
<p>or after dividing both sides by p1.y1; t1/ and by keeping in mind Eq. (16.14) we
arrive at:
</p>
<p>p1j1.y3; t3jy1; t1/ D
Z
</p>
<p>dy2 p1j1.y3; t3jy2; t2/p1j1.y2; t2jy1; t1/ : (16.25)
</p>
<p>This equation is known as the CHAPMAN-KOLMOGOROV equation [8, 13]. The
interpretation of this equation is straight-forward: the transition probability from
.y1; t1/ to .y3; t3/ is equivalent to the transition probability from .y1; t1/ to .y2; t2/
multiplied by the transition probability from .y2; t2/ to .y3; t3/ when summed over
all intermediate positions y2. This is illustrated in Fig. 16.1.</p>
<p/>
</div>
<div class="page"><p/>
<p>16.3 MARKOV Processes 253
</p>
<p>Fig. 16.1 Illustration of the
CHAPMANN-KOLMOGOROV
equation
</p>
<p>We state a very important theorem: Any two non-negative functions p1j1 and
p1 uniquely define a MARKOV process if the CHAPMAN-KOLMOGOROV equa-
tion (16.25) is obeyed and if
</p>
<p>p1.y2; t2/ D
Z
</p>
<p>dy1p1j1.y2; t2jy1; t1/p1.y1; t1/ ; (16.26)
</p>
<p>which follows immediately from the first equation in Eqs. (16.22) in combination
with property (16.10).
</p>
<p>As a first example we consider one of the most important MARKOV processes
in physics, the WIENER process [10]. Its importance stems from its application to
the description of Brownian motion, the random motion of dust particles on a fluid
surface. (In Chap. 17 we take a closer look at diffusion phenomena.) The transition
probability of the WIENER process is of the form1
</p>
<p>p1j1.y2; t2jy1; t1/ D
1
</p>
<p>p
</p>
<p>2�.t2 � t1/
exp
</p>
<p>�
</p>
<p>� .y2 � y1/
2
</p>
<p>2.t2 � t1/
</p>
<p>�
</p>
<p>: (16.27)
</p>
<p>The initial condition is given by
</p>
<p>p1.y1; t1 D 0/ D ı.y1/ : (16.28)
</p>
<p>A straight-forward calculation proves that (16.27) indeed obeys the CHAPMAN-
KOLMOGOROV equation (16.25). Moreover, we deduce from Eqs. (16.28) together
with (16.26) that
</p>
<p>p1.y; t/ D
1p
2�t
</p>
<p>exp
</p>
<p>�
</p>
<p>�y
2
</p>
<p>2t
</p>
<p>�
</p>
<p>: (16.29)
</p>
<p>1This form is equivalent to the above definition of the WIENER process, in particular to the
requirement of normally distributed increments with variance t2 � t1.</p>
<p/>
</div>
<div class="page"><p/>
<p>254 16 Some Basics of Stochastic Processes
</p>
<p>Fig. 16.2 Three possible
realizations of the WIENER
process
</p>
<p>The WIENER process is easily realized on the computer. We regard the one-
dimensional case and start at the origin YX.0/ D 0. As per definition the increments
YX.t C dt/ � YX.t/ follow a normal distribution N .dyj0; dt/ of mean zero and
variance dt. Hence, we start with y0 D 0 at time t0 D 0, sample the displacement
dy within a time-step dt from N .dyj0; dt/ and calculate the next position at time
t1 D t0 C dt which is given by: y1 D y0 C dy.2 This process is repeated until a
certain time limit has been reached. Figure 16.2 presents the result of three such
calculations which have been started using different seeds.
</p>
<p>Let us mention a second very important MARKOV process, the POISSON process.
The POISSON process is particularly interesting for problems involving waiting
times, such as the decay of some radioactive nucleus. However, we shall also
come across the POISSON process within the context of diffusion in Chap. 17. The
transition probability of the POISSON process is defined as
</p>
<p>p1j1.n2; t2jn1; t1/ D
.t2 � t1/n2�n1
.n2 � n1/Š
</p>
<p>exp Œ�.t2 � t1/&#141; : (16.30)
</p>
<p>Here it is understood that n1; n2 2 N and n2 &gt; n1. Hence, the POISSON process
counts the number of occurrences n2 of a certain event until the time t2 is reached
under the premise that n1 events have already occurred at time t1. The POISSON
process is initialized by the pdf
</p>
<p>p1.n1; t1 D 0/ D ın10 ; (16.31)
</p>
<p>here ıij is the KRONECKER-ı. Hence we have according to Eq. (16.26)
</p>
<p>p1.n; t/ D
X
</p>
<p>n1
</p>
<p>p1j1.n; tjn1; t1 D 0/p1.n1; t1 D 0/ D
tn
</p>
<p>nŠ
exp .�t/ ; (16.32)
</p>
<p>2Alternatively, we may sample dy from a normal distribution with variance 1 and multiply it byp
dt. This follows from a simple transformation of variables.</p>
<p/>
</div>
<div class="page"><p/>
<p>16.3 MARKOV Processes 255
</p>
<p>Fig. 16.3 Three possible
realizations of a POISSON
process
</p>
<p>which is a POISSON distribution (see Appendix, Sect. E.4 and, for instance,
Ref. [14]). Let us briefly consider the time between two events. Suppose we had
n1 events at time t1. Then, we calculate the probability that at time t2 D t1 C � we
still counted n2 D n1 events, thus, nothing happened. We have
</p>
<p>p1j1.n1; t1 C � jn1; t1/ D exp .��/ (16.33)
</p>
<p>and the waiting times are independent and follow an exponential distribution. We
may simulate the POISSON process by starting at t1 D 0 with n1 D 0 and by
increasing n2; n3; : : : by one, i.e. niC1 D ni C 1 after successive waiting times
�1; �2; : : : which we sample from the exponential distribution (see Sect. 13.2) until
a final count N has been reached. The result of such a procedure is illustrated in
Fig. 16.3 where n.t/ vs t has been plotted for three runs started by different seeds.
</p>
<p>Finally, we remark that for a time-homogeneousMARKOV process the transition
probability p1j1.y2; t2jy1; t1/ depends by definition on the time difference t2� t1 � �
rather than explicitly on the two times t1 and t2 and is usually denoted by T� .y2; y1/.
</p>
<p>We turn now our attention to another very important general concept of MARKOV
processes, the master equation [8]. This equation is in fact the differential form of
the CHAPMAN-KOLMOGOROV equation.We regard the CHAPMAN-KOLMOGOROV
equation (16.25) for three successive times t1 &lt; t2 &lt; t3 D t2C� where � is assumed
to be small, i.e. � � 1. We expand the conditional pdf p1j1 in a TAYLOR series with
respect to � :
</p>
<p>p1j1.y3; t2 C � jy2; t2/ D p1j1.y3; t2jy2; t2/C �
@
</p>
<p>@�
p1j1.y3; t2 C � jy2; t2/
</p>
<p>ˇ
ˇ
ˇ
�D0
</p>
<p>CO.�2/ :
(16.34)
</p>
<p>In order to transform this equation into a more transparent form, we introduce the
transition rate w.y3jy2; t2/ from y2 to y3, with y2 &curren; y3:
</p>
<p>w.y3jy2; t2/ D
@
</p>
<p>@�
p1j1.y3; t2 C � jy2; t2/
</p>
<p>ˇ
ˇ
ˇ
�D0
</p>
<p>: (16.35)</p>
<p/>
</div>
<div class="page"><p/>
<p>256 16 Some Basics of Stochastic Processes
</p>
<p>In addition, we note that the first term on the right-hand side of Eq. (16.34) has to
be of the form:
</p>
<p>p1j1.y3; t2jy2; t2/ D ı.y3 � y2/ : (16.36)
</p>
<p>On the other hand, we defined the transition rate (16.35) only for elements y2 &curren; y3
and, thus, we denote the remaining part (i.e. y2 D y3) by a.y2; t2/. All this allows us
to rewrite Eq. (16.34) as
</p>
<p>p1j1.y3; t2 C � jy2; t2/ D Œ1C a.y2; t2/&#141;ı.y3 � y2/C �w.y3jy2; t2/ ; (16.37)
</p>
<p>where we neglected terms of order O.�2/. The pdf p1j1.y3; t2 C � jy2; t2/ is subject
to the normalization (16.15) and this provides us with the required condition to
determine a.y2; t2/:
</p>
<p>a.y2; t2/ D ��
Z
</p>
<p>dy3w.y3jy2; t2/ : (16.38)
</p>
<p>Hence, the term 1C a.y2; t2/ describes the probability that no event occurs within
the time interval Œt2; t2 C �&#141;. The expansion (16.37) is inserted into the CHAPMAN-
KOLMOGOROV equation (16.25) with the result:
</p>
<p>p1j1.y3; t2 C � jy1; t1/� p1j1.y3; t2jy1; t1/
�
</p>
<p>D
Z
</p>
<p>dy2
�
</p>
<p>w.y3jy2; t2/p1j1.y2; t2jy1; t1/
</p>
<p>�w.y2jy3; t2/p1j1.y3; t2jy1; t1/
�
</p>
<p>:
</p>
<p>(16.39)
</p>
<p>Finally, we arrive, in the limit � ! 0, at the master equation:
</p>
<p>@
</p>
<p>@t
p1j1.y; tjy0; t0/ D
</p>
<p>Z
</p>
<p>dy00
�
</p>
<p>w.yjy00; t/p1j1.y00; tjy0; t0/
</p>
<p>�w.y00jy; t/p1j1.y; tjy0; t0/
�
</p>
<p>: (16.40)
</p>
<p>We multiply this equation by p1.y0; t0/ and integrate over y0. This results in the
master equation for the pdf p1.y; t/
</p>
<p>@
</p>
<p>@t
p1.y; t/ D
</p>
<p>Z
</p>
<p>dy0
�
</p>
<p>w.yjy0; t/p1.y0; t/ � w.y0jy; t/p1.y; t/
�
</p>
<p>; (16.41)
</p>
<p>where we made use of the property (16.26).
Let us briefly discuss this result: In its derivation we assumed the state space to
</p>
<p>be continuous. However, if a master equation for a discrete state space is required
the integral is to be replaced by a sum over the discrete states. On the other hand, the
physical interpretation of such an equation is straight-forward: The time evolution</p>
<p/>
</div>
<div class="page"><p/>
<p>16.3 MARKOV Processes 257
</p>
<p>of the quantity p1.y; t/ is governed by the sum over all transitions into state y
(first term) minus all transitions out of state y. We remark that master equations
occur commonly in physical applications; for instance, the collision integral of the
BOLTZMANN equation is of a similar form. The transitions rates w.yjy0; t/ can be
determined explicitly in many applications in physics.3
</p>
<p>Furthermore, if the system is in a stationary state then it is described by a
stationary distribution p1.y; t/ D p1.y/ and we obtain from (16.41) the relation
</p>
<p>Z
</p>
<p>dy0w.yjy0; t/p1.y0/ D
Z
</p>
<p>dy0w.y0jy; t/p1.y/ ; (16.42)
</p>
<p>which is referred to as global balance. The much stronger condition
</p>
<p>w.yjy0; t/p1.y0/ D w.y0jy; t/p1.y/ ; (16.43)
</p>
<p>is referred to as detailed balance and will be discussed next.
The task now is to prove that the equilibrium distribution function pe.X/ of
</p>
<p>a classical physical system will, under certain restrictions, indeed fulfill detailed
balance. (This proof was given by N.G. VAN KAMPEN [5].) The next steps of the
proof become more transparent if a vector x D .qk; pk/T 2 R6N is introduced which
represents the phase space trajectory of the N particles constituting the system under
investigation. This trajectory is determined by HAMILTON&rsquo;s equations of motion
[16&ndash;18]:
</p>
<p>Pqk D
@
</p>
<p>@pk
H.x/ ; Ppk D �
</p>
<p>@
</p>
<p>@qk
H.x/ : (16.44)
</p>
<p>Furthermore, YX.t/ denotes a stochastic process which describes some observable
of the physical system. We require that YX.t/ is invariant under time reversal. We
also assume the equilibrium distribution function pe.x/ to be invariant under time
reversal, which in most cases is equivalent to the requirement that the HAMILTON
function H.x/ is invariant under time reversal. The operation of time reversal will
be indicated by bared variables:
</p>
<p>t D �t ; x D .qk;�pk/T : (16.45)
</p>
<p>3As an example we quote FERMI&rsquo;s golden rule [15], where the transition rate wnn0 from unperturbed
states n to n0 is of the form
</p>
<p>wnn0 D
2�
</p>
<p>&bdquo; jH
0
nn0 j�.En/ ;
</p>
<p>where H0nn0 are the matrix elements of the perturbation Hamiltonian H
0 and �.En/ denotes the
</p>
<p>density of states of the unperturbed system.</p>
<p/>
</div>
<div class="page"><p/>
<p>258 16 Some Basics of Stochastic Processes
</p>
<p>Hence, the above assumptions result in
</p>
<p>Yx.t/ D Yx.t/ D Yx.�t/ D Yx.t/ ; (16.46)
</p>
<p>and
</p>
<p>pe.x/ D pe.x/ D pe.x/ : (16.47)
</p>
<p>In particular, we deduce from Eq. (16.46) that
</p>
<p>Yx.0/ D Yx.0/ ; (16.48)
</p>
<p>and
</p>
<p>Yx.t/ D Yx.�t/ : (16.49)
</p>
<p>We calculate the pdf p2 from
</p>
<p>p2.y1; 0; y2; t/ D
Z
</p>
<p>dxıŒ y1 � Yx.0/&#141;ıŒy2 � Yx.t/&#141;pe.x/ : (16.50)
</p>
<p>However, since we integrate over the whole phase space we recognize that the
volume is invariant under a change dx ! dx. Thus, we can change the variable
of integration from x to x and this results in:
</p>
<p>p2.y1; 0; y2; t/ D
Z
</p>
<p>dxıŒ y1 � Yx.0/&#141;ıŒy2 � Yx.t/&#141;pe.x/
</p>
<p>D
Z
</p>
<p>dxıŒ y1 � Yx.0/&#141;ıŒy2 � Yx.�t/&#141;pe.x/
</p>
<p>D p2.y2;�t; y1; 0/
D p2.y2; 0; y1; t/ : (16.51)
</p>
<p>We obtain immediately:
</p>
<p>p1j1.y2; tjy1; 0/pe.y1/ D p1j1.y1; tjy2; 0/pe.y2/ : (16.52)
</p>
<p>Differentiation of this equation with respect to t together with definition (16.35)
yields for small values of t
</p>
<p>w.y2jy1/pe.y1/ D w.y1jy2/pe.y2/ ; (16.53)
</p>
<p>which is the condition of detailed balance, Eq. (16.43), for stationary distributions.
It should be noted at this point that detailed balance in physical systems is
</p>
<p>strongly connected to the entropy growth (the H-theorem by BOLTZMANN [19]).</p>
<p/>
</div>
<div class="page"><p/>
<p>16.4 MARKOV-Chains 259
</p>
<p>Here, detailed balance was based on the condition that the stochastic process YX.t/
was invariant under time reversal and that the equilibrium distribution pe.x/ had the
same property. This has in most cases the consequence that the HAMILTON function
is also invariant under time reversal transformations. However, a detailed discussion
of these properties is far beyond the scope of this book.
</p>
<p>We continue our presentation with so called MARKOV-chains which are a special
class of MARKOV processes. MARKOV-chains will prove to be very important
for the understanding of MARKOV-chain Monte Carlo techniques, such as the
METROPOLIS algorithm.
</p>
<p>16.4 MARKOV-Chains
</p>
<p>A MARKOV-chain is a time-homogeneous MARKOV process defined on a discrete
time span and in a discrete state space [20&ndash;22]. Hence, we express the time instances
by integers T D N, tn D n where n 2 N and possible outcomes are indexed by
integers YX.tn/ 2 fmg where m 2 N. As a first consequence of the discreteness of
the state space we replace all pdfs p.�/ by probabilities Pr.�/. Hence the MARKOV
property (16.20) reads
</p>
<p>Pr.YnC1 D yjYn D yn; : : : ;Y1 D y1/ D Pr.YnC1 D yjYn D yn/ ; (16.54)
</p>
<p>where we applied the notation Yn � YX.tn/ and yn 2 fmg is one particular realization
out of the discrete state space. Since we assume the transition probabilities to be
independent of the actual time, we can define a transition matrix P D fpijg via
</p>
<p>pij D Pr.YnC1 D jjYn D i/ : (16.55)
</p>
<p>Consequently, we write
</p>
<p>Pr.Yn D in;Yn�1 D in�1; : : : ;Y0 D i0/ D Pr.Y0 D i0/pi0i1pi1i2 � � � pin�1in :
(16.56)
</p>
<p>We note that the transition matrix is a stochastic matrix, a matrix with only non-
negative elements such that the sum of each row is equal to one. Furthermore, one
can prove that the product of two stochastic matrices results, again, in a stochastic
matrix.
</p>
<p>We define the state vector at time n, �.n/ D f� .n/i g as
</p>
<p>�
.n/
i D Pr.Yn D i/ : (16.57)</p>
<p/>
</div>
<div class="page"><p/>
<p>260 16 Some Basics of Stochastic Processes
</p>
<p>From the marginalization rule (Appendix, Sect. E.6) follows for the particular case
n D 1
</p>
<p>Pr.Y1 D i/ D
X
</p>
<p>k
</p>
<p>Pr.Y0 D k/ Pr.Y1 D ijY0 D k/ ; (16.58)
</p>
<p>or with the help of the definitions (16.55) and (16.57):
</p>
<p>�
.1/
i D
</p>
<p>X
</p>
<p>k
</p>
<p>pki�
.0/
k ; 8i: (16.59)
</p>
<p>Hence, we get for n D 1
</p>
<p>� .1/ D � .0/P ; (16.60)
</p>
<p>and for n D 2
</p>
<p>� .2/ D � .1/P D � .0/P2 : (16.61)
</p>
<p>Obviously,
</p>
<p>� .n/ D � .0/Pn ; (16.62)
</p>
<p>follows for arbitrary n. Hence the probability matrix for an n step transition P.n/ is
given by P.n/ D Pn. We immediately deduce that the CHAPMAN-KOLMOGOROV
equation for MARKOV-chains is fulfilled since
</p>
<p>P.n/P.m/ D PnPm D PnCm D P.nCm/ ; (16.63)
</p>
<p>for two integers n and m.
Let us cite some further definitions in order to classify MARKOV-chains [5, 8, 20&ndash;
</p>
<p>22]:
</p>
<p>&bull; The notation i ! j means state i leads to state j and is true whenever there
is a path of length n, i0 D i; i1; : : : ; in D j such that all pikikC1 &gt; 0 for k D
0; 1; : : : ; n � 1. This is equivalent to .Pn/ij &gt; 0.
</p>
<p>&bull; The notation i $ j means state i communicates with state j. This relation is true
whenever i ! j and j ! i.
</p>
<p>&bull; A class of states is given if (i) all states within one class communicate with each
other and (ii) two states of different classes never communicate with each other.
These classes are referred to as the irreducible classes of the MARKOV-chain.
</p>
<p>&bull; An irreducible MARKOV-chain is a MARKOV-chain in which the whole state
space forms an irreducible class, i.e. all states communicate with each other.
</p>
<p>&bull; A closed set of states is a set of states which never leads to states which are
outside of this set.</p>
<p/>
</div>
<div class="page"><p/>
<p>16.4 MARKOV-Chains 261
</p>
<p>&bull; An absorbing state is a state which does not lead to any other states: It forms
itself a closed set. An absorbing state can be reached from the outside but there
is no escape from it.
</p>
<p>&bull; A state is referred to as transient if the probability of returning to the state is less
than one.
</p>
<p>&bull; A state is referred to as recurrent if the probability of returning to the state is
equal to one.
</p>
<p>&bull; Furthermore, we call a state positive recurrent if the expectation value of the
first return time is less than infinity and null recurrent if it is infinity. We may
formulate this in a more mathematical language: The time of first return to state
i is defined via
</p>
<p>Ti D inf .n � 1 W Xn D ijX0 D i/ : (16.64)
</p>
<p>The probability that we return to state i for the first time after n steps is defined as
</p>
<p>f nii D Pr.Ti D n/ : (16.65)
</p>
<p>Hence, a state is referred to as recurrent if
</p>
<p>Fi D
X
</p>
<p>n
</p>
<p>f nii D 1 ; (16.66)
</p>
<p>positive recurrent if
</p>
<p>hTii D
X
</p>
<p>n
</p>
<p>nf nii &lt;1 ; (16.67)
</p>
<p>and null recurrent if
</p>
<p>hTii D
X
</p>
<p>n
</p>
<p>nf nii D 1 : (16.68)
</p>
<p>We note that we also have hTii D 1 if state i is transient. Furthermore, one can
show that a state is only recurrent if
</p>
<p>X
</p>
<p>n
</p>
<p>pnii D 1 : (16.69)
</p>
<p>&bull; A state is referred to as periodic if the return time of the state can only be a
multiple of some integer d &gt; 1.
</p>
<p>&bull; A state is referred to as aperiodic if d D 1.
&bull; We call a state ergodic if it is positive recurrent and aperiodic.
&bull; A MARKOV-chain is called ergodic if all its states are ergodic.</p>
<p/>
</div>
<div class="page"><p/>
<p>262 16 Some Basics of Stochastic Processes
</p>
<p>We give some useful theorems in the context of the above definitions: First of
all, it can be proved that if a MARKOV-chain is irreducible it follows that either all
states are transient, or all states are null recurrent, or all states are positive recurrent.
</p>
<p>Furthermore, a theorem by KOLMOGOROV states that if a MARKOV-chain is
irreducible and aperiodic then the limit
</p>
<p>�j D lim
n!1
</p>
<p>�
.n/
j D
</p>
<p>1
˝
</p>
<p>Tj
˛ ; (16.70)
</p>
<p>exists. It follows from the above discussion that if all states j are transient or null
recurrent we have
</p>
<p>�j D 0 ; (16.71)
</p>
<p>and if all states j are positive recurrent, we have
</p>
<p>�j &curren; 0 : (16.72)
</p>
<p>In this case the state vector � D f�jg is referred to as the stationary distribution or
equilibrium distribution. We note that in this context the term equilibrium does not
mean that nothing changes, but that the system forgets its own past. In particular, as
soon as the system reaches the stationary distribution, it is independent of the initial
state � .0/.
</p>
<p>We concentrate now on equilibrium distributions. It follows from Eq. (16.62) that
� satisfies:
</p>
<p>� D �P : (16.73)
</p>
<p>Thus, � is the left-eigenvector to the transition probability matrix P with eigenvalue
1. We note that Eq. (16.73) states a homogeneous eigenvalue problem: The solution
is only determined up to a constant multiplicator (see Sect. 8.3). However, it is clear
that the vector � satisfies
</p>
<p>X
</p>
<p>j
</p>
<p>�j D 1 : (16.74)
</p>
<p>One can prove that the unique solution of the eigenvalue problem (16.73) together
with the normalization condition (16.74) for n states can be written as
</p>
<p>� D e � .P � E � I/�1 ; (16.75)
</p>
<p>where e is an n-element row vector containing only ones, E is a n � n matrix
containing only ones and I is the n � n identity.
</p>
<p>Let us briefly elaborate on this point: if it is possible to construct a MARKOV-
chain which possesses a unique stationary distribution, we know that it will</p>
<p/>
</div>
<div class="page"><p/>
<p>16.4 MARKOV-Chains 263
</p>
<p>definitely reach this distribution independent of the choice of initial conditions. The
existence as well as the form of the stationary distribution is clearly determined
by the transition probabilities pij. A sufficient condition for a unique stationary
distribution to exist is the requirement of reversibility. A MARKOV-chain is referred
to as reversible if
</p>
<p>pij�i D pji�j ; 8i; j; (16.76)
</p>
<p>i.e. if the transition probabilities ensure detailed balance for the stationary distribu-
tion � .
</p>
<p>Now we are in a position to understand better why detailed balance was
such an important concept of the METROPOLIS algorithm discussed in Sect. 14.3:
Invoking the detailed balance condition ensures that for all possible initial states the
MARKOV-chain converges toward the equilibrium distribution for which detailed
balance is fulfilled. Of course, the convergence time will highly depend on the
choice of the initial state as well as on the choice of the transition matrix. Hence,
we can generate random numbers with the help of such a MARKOV-chain and after
a thermalization period these numbers will follow the required pdf. Methods based
on this concept are commonly referred to as MARKOV-chain Monte Carlo sampling
methods [23&ndash;26].
</p>
<p>We give a brief example, the spread of a rumor. Let Z1 and Z2 be two distinct
versions of a report. If a person receives report Z1 it will pass this report on as Z1
with probability .1� p/ or as Z2 with probability p. An alternative is that the person
receives Z2 and passes it on as Z2 with probability .1 � q/ or modifies it to Z1 with
probability q. We summarize
</p>
<p>&bull; Pr.Z1 ! Z1/ D .1 � p/ D p11 ,
&bull; Pr.Z1 ! Z2/ D p D p12 ,
&bull; Pr.Z2 ! Z1/ D q D p21 ,
&bull; Pr.Z2 ! Z2/ D .1 � q/ D p22 .
The transition matrix is of the form
</p>
<p>P D
�
</p>
<p>1 � p p
q 1 � q
</p>
<p>�
</p>
<p>: (16.77)
</p>
<p>We note that the two states communicate with each other Z1 $ Z2, hence the
MARKOV-chain is irreducible. Furthermore, since the process can reach either
state Z1 or Z2 within a single time step, it is clearly aperiodic. Let us briefly
investigate the probabilities of first recurrence f nii after n steps. Due to the theorem by
KOLMOGOROV it is sufficient to investigate the state Z1 since the MARKOV-chain is
irreducible and it follows that also Z2 has the same recurrence properties. We note
the following possible paths for a first return to state Z1:
</p>
<p>&bull; 1 W Pr.Z1 ! Z1/ D .1 � p/ D f 111 ,
&bull; 2 W Pr.Z1 ! Z2 ! Z1/ D pq D f 211 ,</p>
<p/>
</div>
<div class="page"><p/>
<p>264 16 Some Basics of Stochastic Processes
</p>
<p>&bull; 3 W Pr.Z1 ! Z2 ! Z2 ! Z1/ D p.1 � q/q D f 311 ,
&bull; n W Pr.Z1 ! Z2 ! � � � ! Z2 ! Z1/ D p.1 � q/n�2q D f n11 .
The probability of returning to Z1 is, see Eq. (16.66),
</p>
<p>F1 D
1
X
</p>
<p>nD1
f n11
</p>
<p>D .1 � p/C pq
1
X
</p>
<p>nD0
.1 � q/n
</p>
<p>D .1 � p/C pq 1
1� .1 � q/
</p>
<p>D 1 ; (16.78)
</p>
<p>where we employed that 0 &lt; .1�q/ &lt; 1 as well as the convergence of the geometric
series. Hence state Z1 is recurrent and, therefore, also state Z2. We calculate the
expectation value of the first return time hT1i:
</p>
<p>hT1i D
1
X
</p>
<p>nD1
nf n11
</p>
<p>D .1 � p/C pq
1
X
</p>
<p>nD0
.n C 2/.1 � q/n
</p>
<p>D .1 � p/C 2pq
1X
</p>
<p>nD0
.1 � q/n
</p>
<p>&bdquo; ƒ&sbquo; &hellip;
</p>
<p>D 1
q
</p>
<p>Cpq
1X
</p>
<p>nD0
n.1 � q/n
</p>
<p>D 1C p � pq.1� q/ d
dq
</p>
<p>1
X
</p>
<p>nD0
.1 � q/n
</p>
<p>&bdquo; ƒ&sbquo; &hellip;
</p>
<p>D 1
q
</p>
<p>D 1C p C p
q
.1 � q/
</p>
<p>D p C q
q
</p>
<p>: (16.79)
</p>
<p>Hence, the states Z1 and Z2 are positive recurrent as long as p &curren; 0 and q &curren; 0. This
means that an equilibriumdistribution exists and it can be obtained from Eq. (16.70).</p>
<p/>
</div>
<div class="page"><p/>
<p>16.4 MARKOV-Chains 265
</p>
<p>We have
</p>
<p>�1 D
1
</p>
<p>hT1i
D q
</p>
<p>p C q : (16.80)
</p>
<p>Due to the normalization condition (16.74) we obtain
</p>
<p>�2 D 1 � �1 D
p
</p>
<p>p C q ; (16.81)
</p>
<p>and, therefore,
</p>
<p>hT2i D
p C q
</p>
<p>p
: (16.82)
</p>
<p>Since all states are positive recurrent and aperiodic, the above MARKOV-chain is
ergodic. Finally, we remark that this example also fulfills detailed balance since
</p>
<p>�1p12 D
qp
</p>
<p>p C q D �2p21 : (16.83)
</p>
<p>Let us briefly interpret this example: Suppose the original, true version Z1 of
a report is &lsquo;Mr. X is going to resign&rsquo; while Z2 is just the opposite: &lsquo;Mr. X is not
going to resign&rsquo;. The property of irreducibility of the MARKOV-chain reflects the
fact that there is no version of the report which cannot be reached or modified.
Moreover, we just demonstrated that the process is positive recurrent: Even if the
probability p that Z1 was modified to Z2 is very small and the probability q that Z2
was modified to Z1 is very high, the report will infinitely often return to version Z2
with probability one. This means that the public will be told infinitely often that
Mr. X is not going to resign with probability one. The equilibrium probabilities �1
and �2 display the asymptotic probability of versions one and two of the report,
respectively. However, as has already been emphasized, this does not mean that
the report cannot be modified in equilibrium, it simply displays the fact that the
probabilities reached a steady state. Finally, we note an interesting effect in passing:
Suppose that the probabilities that any of the two versions is modified is very small
but equal, i.e. p D q � 1. Then the equilibrium distribution is
</p>
<p>�1 D �2 D
1
</p>
<p>2
; (16.84)
</p>
<p>and the public will believe Z1 and Z2 with the same probability after some time
independently of the initial version and also independent of the actual decision of
Mr. X. Detailed balance expresses the property that the probability of receiving Z2
and passing it on as Z1 is the same as the probability of receiving Z2 and passing it
on as Z1.</p>
<p/>
</div>
<div class="page"><p/>
<p>266 16 Some Basics of Stochastic Processes
</p>
<p>We close this section with a final remark: It is an easy task to generalize the
ideas of MARKOV-chains to continuous state spaces since we already introduced
the required tools in Sect. 16.3. Let �.x/ denote the stationary distribution density
and p.xjy/ the accompanying transition rate pdf. Then relation (16.73) transforms
into
</p>
<p>�.x/ D
Z
</p>
<p>dy�.y/p.xjy/ ; (16.85)
</p>
<p>together with
</p>
<p>Z
</p>
<p>dx�.x/ D 1 ; (16.86)
</p>
<p>the usual normalization of pdfs. In this case, the condition of detailed balance is
given by
</p>
<p>�.x/p.yjx/ D �.y/p.xjy/ ; (16.87)
</p>
<p>which is equivalent to Eq. (16.52).
</p>
<p>16.5 Continuous-Time MARKOV-Chains
</p>
<p>A generalization of the results of the previous sections to a continuous time span
is straight-forward. We define the continuous-time MARKOV-chain as a time-
homogeneous MARKOV process on a discrete state space but with a continuous
time span, t � 0. Thus
</p>
<p>PrŒX.t C s/ D njX.s/ D m&#141; D pnm.t/ ; (16.88)
</p>
<p>is independent of s � 0. In this case the transition matrix P.t/ D fpij.t/g is an
explicit function of time t. Its elements pnm.t/ have the following four properties:
</p>
<p>(a) All matrix elements pnm.t/ of the transition matrix P are positive:
</p>
<p>pnm.t/ � 0 ; 8t &gt; 0 : (16.89)
</p>
<p>(b) The usual normalization of the rows of the transition matrix P is valid:
</p>
<p>X
</p>
<p>m
</p>
<p>pnm.t/ D 1 ; 8n and t &gt; 0 : (16.90)</p>
<p/>
</div>
<div class="page"><p/>
<p>16.5 Continuous-Time MARKOV-Chains 267
</p>
<p>(c) As for every MARKOV process, the transition matrix of the continuous time
MARKOV-chain obeys the CHAPMAN-KOLMOGOROV equation:
</p>
<p>X
</p>
<p>k
</p>
<p>pnk.t/pkm.t
0/ D pnm.t C t0/ ; (16.91)
</p>
<p>which can alternatively be expressed as
</p>
<p>P.t C t0/ D P.t/P.t0/ : (16.92)
</p>
<p>(d) We assume pnm.t/ to be a continuous function of t and that:
</p>
<p>lim
t!0
</p>
<p>pnm.t/ D
(
</p>
<p>1 for n D m ;
0 for n &curren; m :
</p>
<p>(16.93)
</p>
<p>It follows from this equation that the matrix elements pnm.t/ can be written as
</p>
<p>pnm.t/ D
(
</p>
<p>1C qnn t C O.t2/ for n D m ;
qnm t C O.t2/ for n &curren; m ;
</p>
<p>(16.94)
</p>
<p>where we introduced with fqnmg D Q the transition rate matrix. The transition rate
matrix Q obeys:
</p>
<p>(a) All off-diagonal elements qnm, n &curren; m, are non-negative since
</p>
<p>qnm D lim
t!0
</p>
<p>pnm.t/
</p>
<p>t
� 0 for n &curren; m : (16.95)
</p>
<p>(b) All diagonal elements qnn are non-positive since
</p>
<p>qnn D � lim
t!0
</p>
<p>1 � pnn.t/
t
</p>
<p>� 0 : (16.96)
</p>
<p>(c) Differentiating Eq. (16.90) with respect to t yields that the sum over all elements
in a row is equal to zero. Therefore, we conclude:
</p>
<p>qnn D �
X
</p>
<p>m
</p>
<p>n&curren;m
</p>
<p>qnm : (16.97)
</p>
<p>Moreover, differentiating the CHAPMAN-KOLMOGOROV equation with respect to t
or t0 gives the KOLMOGOROV forward &ndash; or KOLMOGOROV backward equations
</p>
<p>PP.t/ D P.t/Q and PP.t/ D QP.t/ ; (16.98)</p>
<p/>
</div>
<div class="page"><p/>
<p>268 16 Some Basics of Stochastic Processes
</p>
<p>respectively. We obtain P.t/ D exp .Qt/ where the exponential function of a matrix
has to be interpreted as
</p>
<p>exp .Qt/ D
1
X
</p>
<p>kD0
</p>
<p>tk
</p>
<p>kŠ
Qk ; (16.99)
</p>
<p>where Q0 D I is the identity matrix.
We define s as the time of the first jump of our process for the particular case
</p>
<p>X.0/ D n
</p>
<p>s D min ŒtjX.t/ &curren; X.0/&#141; : (16.100)
</p>
<p>It can be shown that Pn.s &gt; t/, the probability that the jump occurs at some time
s &gt; t, is given by
</p>
<p>Pn.s &gt; t/ D exp.qnnt/ ; (16.101)
</p>
<p>where we note that qnn � 0. Moreover,
</p>
<p>PnŒX.s/ D m&#141; D �
qnm
</p>
<p>qnn
; (16.102)
</p>
<p>and the process starts again at time s and in state m. This means that in a
continuous-time MARKOV-chain the waiting times between two consecutive jumps
are exponentially distributed. One of the simplest examples of a continuous time
MARKOV-chain is the POISSON process, discussed in Sect. 16.3.
</p>
<p>Summary
</p>
<p>This chapter introduced the concept of stochastic processes YX.t/ as &lsquo;time&rsquo; depen-
dent processes depending on randomness. Y was a random variable which depended
on another random variable X and t, the time. All realizations of YX.t/ spanned
the state space. Each stochastic process was coupled to a pdf which described the
probability that the process took on the realization y at time t. In the course of
this introduction a series of general properties which classify such processes were
defined. This was followed by the discussion of a particular class of stochastic
processes, the MARKOV processes. They had the remarkable property that a future
realization of the process solely depended on its current realization and not on
the history how this current realization had been reached (MARKOV property). A
huge class of processes in physics and related sciences is Markovian in nature.
The next refinement in our discussion was the introduction of MARKOV-chains.
These were processes defined on a discrete time span and in a discrete state space.</p>
<p/>
</div>
<div class="page"><p/>
<p>References 269
</p>
<p>This allowed to replace the pdfs by probabilities. Again, various specific properties
of MARKOV-chains opened the possibility of a distinctive classification. A very
important observation was that under certain conditions a MARKOV-chain reached a
stationary or equilibrium distribution and that it definitely arrived at this distribution
independent of the choice of initial conditions. Moreover, detailed balance was
obeyed by this equilibrium condition. This observation was the backbone of
MARKOV-chain Monte Carlo sampling methods, in particular of the METROPOLIS
algorithm. Finally, continuous-timeMARKOV-chains were discussed.
</p>
<p>Problems
</p>
<p>1. Write a program to simulate the WIENER process in one dimension. Follow the
method explained in Sect. 16.2 and perform the following analysis:
</p>
<p>a. Illustrate graphically some typical sample paths.
b. Calculate the mean hx.t/i and the variance varŒx.t/&#141; by restarting the process
</p>
<p>several times with different seeds and plot the result.
c. Measure the position x of the particle at a particular time t for several runs
</p>
<p>(with different seeds) and illustrate the result p.x; t/ graphically.
</p>
<p>2. Realize numerically a POISSON process according to the instructions given
in Sect. 16.2. Again, plot some typical sample paths. Moreover, calculate the
mean waiting time h�i as well as the variance var .�/ numerically as well as
analytically.
</p>
<p>References
</p>
<p>1. Chow, Y.S., Teicher, H.: Probability Theory, 3rd edn. Springer Texts in Statistics. Springer,
Berlin/Heidelberg (1997)
</p>
<p>2. Kienke, A.: Probability Theory. Universitext. Springer, Berlin/Heidelberg (2008)
3. Stroock, D.W.: Probability Theory. Cambridge University Press, Cambridge (2011)
4. von der Linden, W., Dose, V., von Toussaint, U.: Bayesian Probability Theory. Cambridge
</p>
<p>University Press, Cambridge (2014)
5. van Kampen, N.G.: Stochastic Processes in Physics and Chemistry. Elsevier, Amsterdam
</p>
<p>(2008)
6. Gardiner, C.: Stochastic Methods. Springer Series in Synergetics. Springer, Berlin/Heidelberg
</p>
<p>(2009)
7. Chiasson, J.: Introduction to Probability Theory and Stochastic Processes. Wiley, New York
</p>
<p>(2013)
8. Breuer, H.P., Petruccione, F.: Open Quantum Systems, chap. 1. Clarendon Press, Oxford (2010)
9. Montroll, E.W., Schlesinger, M.F.: The wonderful world of random walks. In: Lebowitz, J.L.,
</p>
<p>Montroll, E.W. (eds.) Non-equilibrium Phenomena II. Studies in Statistical Mechanics, vol. 11.
North-Holland, Amsterdam (1984)
</p>
<p>10. Rudnik, J., Gaspari, G.: Elements of the Random Walk. Cambridge University Press,
Cambridge (2010)</p>
<p/>
</div>
<div class="page"><p/>
<p>270 16 Some Basics of Stochastic Processes
</p>
<p>11. Ibe, O.C.: Elements of Random Walk and Diffusion Processes. Wiley, New York (2013)
12. Marcus, M.B., Rosen, J.: Markov Processes, Gaussian Processes, and Local Times. Cambridge
</p>
<p>University Press, Cambridge (2011)
13. Papoulis, A., Pillai, S.: Probability, Random Variables and Stochastic Processes. McGraw Hill,
</p>
<p>New York (2001)
14. Knuth, D.: The Art of Computer Programming, vol. II, 3rd edn. Addison Wesley, Menlo Park
</p>
<p>(1998)
15. Ballentine, L.E.: Quantum Mechanics. World Scientific, Hackensack (1998)
16. Arnol&rsquo;d, V.I.: Mathematical Methods of Classical Mechanics, 2nd edn. Graduate Texts in
</p>
<p>Mathematics, vol. 60. Springer, Berlin/Heidelberg (1989)
17. Scheck, F.: Mechanics, 5th edn. Springer, Berlin/Heidelberg (2010)
18. Goldstein, H., Poole, C., Safko, J.: Classical Mechanics, 3rd edn. Addison-Wesley, Menlo Park
</p>
<p>(2013)
19. Pathria, R.K., Beale, P.D.: Statistical Mechanics, 3rd edn. Academic, San Diego (2011)
20. Norris, J.R.: Markov Chains. Cambridge Series in Statistical and Probabilistic. Cambridge
</p>
<p>University Press, Cambridge (1998)
21. Modica, G., Poggiolini, L.: A First Course in Probability and Markov Chains. Wiley, New
</p>
<p>York (2012)
22. Graham, C.: Markov Chains: Analytic and Monte Carlo Computations. Wiley, New York
</p>
<p>(2014)
23. Fishman, G.S.: Monte Carlo: Concepts, Algorithms and Applications. Springer Series in
</p>
<p>Operations Research. Springer, Berlin/Heidelberg (1996)
24. Doucet, A., de Freitas, N., Gordon, N. (eds.): Sequential Monte Carlo Methods in Practice.
</p>
<p>Information Science and Statistics. Springer, Berlin/Heidelberg (2001)
25. Kalos, M.H., Whitlock, P.A.: Monte Carlo Methods, 2nd edn. Wiley, New York (2008)
26. Kroese, D.P., Taimre, T., Botev, Z.I.: Handbook of Monte Carlo Methods. Wiley, New York
</p>
<p>(2011)</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 17
The Random Walk and Diffusion Theory
</p>
<p>17.1 Introduction
</p>
<p>Diffusion is one of the most widely spread processes in science. Its occurrence
ranges from random motion of dust particles on fluid surfaces, historically known
as Brownian motion, to the motion of particles in numerous physical systems [1, 2],
the spreading of malaria by migration of mosquitoes [3], or even to the description
of fluctuations in stock markets [4].
</p>
<p>For instance, let us regard N neutral, identical, classical particles which solely
interact through collisions, for instance an H2-gas in a box, where N D NA �
6:022 � 1023. We are interested in the dynamics of one particle under the influence
of all others and under no influence by an external force; we expect that diffusion
will be the dominating process. From the microscopic point of view such a situation
can be described with the help of N coupled NEWTON&rsquo;s equations of motion. (See
Chap. 7.) Anyhow, such a task will not be feasible due to the size of the system
&ndash; the magnitude of N. However, a statistical description can be obtained from
BOLTZMANN&rsquo;s equation [5]
</p>
<p>d
</p>
<p>dt
f .r; �; t/ D @
</p>
<p>@t
f .r; �; t/
</p>
<p>ˇ
ˇ
ˇ
coll.
</p>
<p>; (17.1)
</p>
<p>where f .r; �; t/ is the phase space distribution function. Hence, f .r; �; t/drd� is the
number of particles of momentum � within the phase-space volume drd� which is
centered around position r at time t. We have, in particular:
</p>
<p>@
</p>
<p>@t
f .r; �; t/C �
</p>
<p>m
� @
@r
</p>
<p>f .r; �; t/C F � @
@�
</p>
<p>f .r; �; t/ D CŒ f &#141;.r; �; t/ : (17.2)
</p>
<p>Here CŒ f &#141;.r; �; t/ is the collision integral and F describes an external force. In cases
where collisions result solely from two-body interactions between particles that
</p>
<p>&copy; Springer International Publishing Switzerland 2016
B.A. Stickler, E. Schachinger, Basic Concepts in Computational Physics,
DOI 10.1007/978-3-319-27265-8_17
</p>
<p>271</p>
<p/>
</div>
<div class="page"><p/>
<p>272 17 The Random Walk and Diffusion Theory
</p>
<p>are assumed to be uncorrelated prior to the collision,1 the collision integral can be
described by
</p>
<p>CŒ f &#141;.r; �; t/ D
Z
</p>
<p>d�1
</p>
<p>Z
</p>
<p>d�2
</p>
<p>Z
</p>
<p>d�3g.�1; �2; �3; �/ Œf .r; �1; t/f .r; �2; t/
</p>
<p>�f .r; �; t/f .r; �3; t/&#141; ; (17.3)
</p>
<p>where g.�1; �2; �3; �/ accounts for the probability that a collision between two
particles of initial moments �1 and �2 and final momenta �3 and � occurs. This
function depends on the particular type of particles under investigation and has, in
general, to be determined from a microscopic theory.2 We now define the particle
density �.r; t/ as a function of space r and time t via
</p>
<p>�.r; t/ D
Z
</p>
<p>d� f .r; �; t/ : (17.4)
</p>
<p>A complicated mathematical analysis of Eq. (17.1) results in a diffusion equation of
the well-known form
</p>
<p>@
</p>
<p>@t
�.r; t/ D D @
</p>
<p>2
</p>
<p>@r2
�.r; t/ ; (17.5)
</p>
<p>if collisions dominate the dynamics (diffusion limit). Here D D const is the diffusion
coefficient of dimension length2� time�1. Note that
</p>
<p>Z
</p>
<p>dr�.r; t/ D N ; (17.6)
</p>
<p>is the number of particles within our system.3 Thus, in our example we can interpret
diffusion as the average evolution of the integrated phase space distribution function
governed by collisions between particles. Such an interpretation will certainly not
hold in the case of fluctuations in stock markets or in the case of the spreading of
malaria because typically mosquitoes do not collide with humans.
</p>
<p>It is the aim of the first part of this chapter to present a purely stochastic
approach to diffusion, the so called random walk model [7, 8]. This stochastic
</p>
<p>1This assumption is known as the approximation of molecular chaos. In fact it represents the
MARKOV approximation to the dynamics of a many particle system.
2For instance, one can employ FERMI&rsquo;s golden rule [6] to obtain this function on a quantum
mechanical level. We already came across an expression of the form (17.3) on the right hand side of
the master equation, see Sect. 16.3, Eq. (16.42). However, the collision integral of the BOLTZMANN
equation is non-linear.
3The function �.r; t/ is referred to as a physical distribution function due to the normalization
condition (17.6). This is in contrast to distribution functions we encountered so far within this
book, which are normalized to unity.</p>
<p/>
</div>
<div class="page"><p/>
<p>17.2 The Random Walk 273
</p>
<p>description will prove to have several precious advantages: (i) We will be able to
identify criteria for the validity of the diffusion model even for systems lacking a
straight-forward physical interpretation. (ii) The stochastic formulation will give us
the opportunity to perform diffusion &lsquo;experiments&rsquo; on the computer without much
computational effort as the methods employed are based on algorithms discussed in
previous chapters. (iii) Within this framework it will be an easy task to generalize the
approach to stochastic models of anomalous diffusion [9]: The fractal time random
walk and L&Eacute;VY flight models [10]. These models play an increasingly important
role in modern statistical physics.
</p>
<p>17.2 The RandomWalk
</p>
<p>The random walk is one of the classical examples of MARKOV-chains [11&ndash;13].
In this section we discuss some of the basic properties of random walks in one
dimension. For convenience,we are going to use the familiar picture of one diffusing
particle.
</p>
<p>Basics
</p>
<p>The random walk [8] is defined as the motion of a single particle which moves at
the time instances
</p>
<p>0;
t; 2
t; : : : ; n
t; : : : ; (17.7)
</p>
<p>between grid-points
</p>
<p>: : : ;�n
x; : : : ;�
x; 0;
x; : : : ; n
x; : : : : (17.8)
</p>
<p>For a more transparent notation the lattice point n
x, with n 2 Z, will be denoted
by xn and the instance k
t, with k 2 N, will be denoted by tk. This notation follows
the conventions of Chap. 2. The initial position is given by
</p>
<p>PrŒX.t0 D 0/ D xi&#141; D ıi0 ; (17.9)
</p>
<p>and the transition rates pij from position i to position j within a single time step 
t
are defined as
</p>
<p>PrŒX.tnC1/ D xijX.tn/ D xj&#141; D pıji�1 C qıjiC1 C rıij : (17.10)
</p>
<p>Here p denotes the probability that the particle jumps to the neighboring grid-point
on the right-hand side, q stands for the probability that the particle jumps to the</p>
<p/>
</div>
<div class="page"><p/>
<p>274 17 The Random Walk and Diffusion Theory
</p>
<p>neighboring grid-point on the left-hand side, and r denotes the probability of staying
at the same-grid point within this time step. Naturally, we have
</p>
<p>p C q C r D 1 : (17.11)
</p>
<p>Consequently, we have a MARKOV-chain with time instances tn and a state space
spanned by the positions xk. Moreover, we note that the stochastic process is clearly
irreducible since all states communicate with each other (see Sect. 16.4). Hence, it
follows that either all states are recurrent or all states are transient. Furthermore, in
the case that r &curren; 0 the MARKOV-chain is aperiodic, otherwise the chain is periodic
with periodicity d D 2 because it takes at least two steps to return to the starting
position.
</p>
<p>We concentrate first on the classical random walk that is a one-dimensional
random walk with 
t D 
x D 1, r D 0, and p C q D 1. This ensures that the
probability of remaining in the actual position within one time step is equal to zero.
If, furthermore, p D q D 1=2 the random walk is referred to as unbiased and for
p &curren; q we call it biased. We write the position X.tn/ D xn at time tn D n as
</p>
<p>xn D
nX
</p>
<p>iD1
�i ; (17.12)
</p>
<p>where �i 2 f�1; 1g and Pr.�i D C1/ D p, Pr.�i D �1/ D q. Let us assume that
within these n steps the particle moved m times to the right and, consequently, n�m
times to the left. The actual position xn after n steps can then be determined from
</p>
<p>xn D m � .n � m/ D 2m � n � k ; (17.13)
</p>
<p>where we used that x0 D 0. It is interesting to calculate the probability Pr.xn D k/
to find the particle after n time steps at some particular position k. This is simply the
sum over all paths along which the particle moved m D .nC k/=2 times to the right
and n � m D .n � k/=2 times to the left multiplied by the probability for m steps
to the right and n � m steps to the left. In total, this yields
</p>
<p>�
n
</p>
<p>m
</p>
<p>�
</p>
<p>D
�
</p>
<p>n
</p>
<p>.nCk/=2
�
</p>
<p>different
contributions and we have
</p>
<p>Pr.xn D k/ D
 
</p>
<p>n
</p>
<p>m
</p>
<p>!
</p>
<p>pmqn�m
</p>
<p>D
 
</p>
<p>n
</p>
<p>.n C k/=2
</p>
<p>!
</p>
<p>p
nCk
2 q
</p>
<p>n�k
2 : (17.14)
</p>
<p>In particular, we find for the unbiased random walk:
</p>
<p>Pr.xn D k/ D
 
</p>
<p>n
</p>
<p>.n C k/=2
</p>
<p>!�
1
</p>
<p>2
</p>
<p>�n
</p>
<p>: (17.15)</p>
<p/>
</div>
<div class="page"><p/>
<p>17.2 The Random Walk 275
</p>
<p>Due to the aperiodicity of the classical randomwalk, k can only take on the values
k D �n;�nC2; : : : ; n�2; n. Consequently n˙k has to be even. For all other values
of k we have Pr.xn D k/ D 0. Furthermore,
</p>
<p>n
X
</p>
<p>kD�n
n˙k even
</p>
<p>Pr.xn D k/ D
n
X
</p>
<p>mD0
</p>
<p> 
</p>
<p>n
</p>
<p>m
</p>
<p>!
</p>
<p>pmqn�m
</p>
<p>D . p C q/n
</p>
<p>D 1 ; (17.16)
</p>
<p>and the probability of finding the particle at time n within Œ�n; n&#141; is equal to one. A
simple algorithm to simulate the one-dimensional biased random walk consists of
the following steps:
</p>
<p>1. Define values x0, p, and q D 1 � p .
2. Draw a uniformly distributed random number r 2 Œ0; 1&#141; .
3. If r &lt; p set xnC1 D xn C 1 , otherwise set xnC1 D xn � 1 .
4. Return to step 2.
</p>
<p>In Fig. 17.1 we present three different realizations of an unbiased one-
dimensional random walk for (a) N D 50, (b) N D 100, and (c) N D 1000
consecutive steps.
</p>
<p>Comparison between Figs. 17.1 and 16.2 already suggests a connection between
the random walk and the WIENER process and we shall come back to this point in
the course of this chapter.
</p>
<p>Moments
</p>
<p>Let us briefly elaborate on the moments of the random walk (see Appendix,
Sect. E.2). The first moment or expectation value hxni is given by
</p>
<p>hxni D
n
X
</p>
<p>kD�n
n˙k even
</p>
<p>k
</p>
<p> 
</p>
<p>n
</p>
<p>.n C k/=2
</p>
<p>!
</p>
<p>p.nCk/=2q.n�k/=2
</p>
<p>D
n
X
</p>
<p>mD0
.2m � n/
</p>
<p> 
</p>
<p>n
</p>
<p>m
</p>
<p>!
</p>
<p>pmqn�m
</p>
<p>D .2 hmi � n/
D n.2p � 1/ : (17.17)</p>
<p/>
</div>
<div class="page"><p/>
<p>276 17 The Random Walk and Diffusion Theory
</p>
<p>Fig. 17.1 Three different realizations of an unbiased one-dimensional random walk for (a) N D
50, (b) N D 100, and (c) N D 1000 time steps and different seeds
</p>
<p>We now introduce a bias v such that
</p>
<p>p D 1
2
.1C v/ and q D 1
</p>
<p>2
.1 � v/ ; (17.18)
</p>
<p>and obtain
</p>
<p>hxni D nv : (17.19)
</p>
<p>We calculate the second moment
˝
</p>
<p>x2n
˛
</p>
<p>using the above method and get:
</p>
<p>˝
</p>
<p>x2n
˛
</p>
<p>D n.1 � v2/C n2v2 : (17.20)
</p>
<p>The variance var .xn/ follows immediately:
</p>
<p>var .xn/ D
˝
</p>
<p>x2n
˛
</p>
<p>� hxni2 D n.1 � v2/ : (17.21)
</p>
<p>We note the following: The expectation value hxni moves according to Eq. (17.19)
with a uniform velocity defined by the bias v D p�q. In particular, for the unbiased
random walk v D 0 and, thus, hxni D 0 for all n. Furthermore, we observe that
var .xn/ increases linearly with time n &ndash; a property we already noted for the WIENER</p>
<p/>
</div>
<div class="page"><p/>
<p>17.2 The Random Walk 277
</p>
<p>process in Sect. 16.3 &ndash; and it is maximal for v D 0. For v D ˙1, which describes
a pure drift motion in the positive or negative x direction, the variance is equal to
zero.
</p>
<p>Recurrence
</p>
<p>Let us briefly investigate the recurrence behavior of the random walk. We are
interested in the probability f .2`/00 of a first return to the origin x0 D 0 after 2` steps.
We already know that f .2`/00 / p`q` from our previous analysis. In the very first time
step the particle moves either to x1 D 1 or to x1 D �1 and, consequently, within the
following 2`� 2 steps it must not cross or touch the line xk D 0 and the particle has
to terminate at position x2`�1 D x1. Therefore, the walker performs ` � 1 steps to
the left and ` � 1 steps to the right within these 2` � 2 steps. The total number of
possible paths N from x1 to x2`�1 D x1 is, thus, given by
</p>
<p>N D
 
</p>
<p>2` � 2
` � 1
</p>
<p>!
</p>
<p>: (17.22)
</p>
<p>Moreover, N may also be written as the sum of Nc paths which cross or touch the
line xk D 0 and Nnc paths which do not cross or touch the line xk D 0, i.e.
</p>
<p>N D Nc C Nnc : (17.23)
</p>
<p>Obviously, we are only interested in the paths which do not cross or touch the line
xk D 0. We employ the reflection principle to solve this problem. In general, the
number of paths which go from x1 D i &gt; 0 to xkC1 D j &gt; 0 within k-steps and
cross the line x` D 0 is equal to the total number of paths which go from x1 D �i to
xkC1 D j, as is schematically illustrated in Fig. 17.2.
</p>
<p>Let us regard the case x1 D 1: The walker moved in the first step to the right.
Hence, from the reflection principle we obtain that the number of paths from x1 to
x2`�2 D x1 in 2`� 2 steps which cross or touch the line xk D 0 is given by the total
</p>
<p>Fig. 17.2 Illustration of the
reflection principle</p>
<p/>
</div>
<div class="page"><p/>
<p>278 17 The Random Walk and Diffusion Theory
</p>
<p>number of paths from �x1 to x2`�2 D x1. Thus, Nc is determined by:
</p>
<p>Nc D
 
</p>
<p>2` � 2
`
</p>
<p>!
</p>
<p>: (17.24)
</p>
<p>We note that in this picture, the walker moves ` steps to the right and ` � 2 steps to
the left. Hence, we obtain that the number of paths which do not cross or touch the
line xk D 0 is given by
</p>
<p>2Nnc D 2.N � Nc/ D
1
</p>
<p>2` � 1
</p>
<p> 
</p>
<p>2`
</p>
<p>`
</p>
<p>!
</p>
<p>: (17.25)
</p>
<p>The prefactor 2 accounts for the fact that the walker can move in its first step either
to x1 D �1 or to x1 D 1. Thus, the probability for the first return of the particle after
2` steps is described by:
</p>
<p>f
.2`/
00 D
</p>
<p>1
</p>
<p>2` � 1
</p>
<p> 
</p>
<p>2`
</p>
<p>`
</p>
<p>!
</p>
<p>p`q` : (17.26)
</p>
<p>We calculate the recurrence probability according to Eq. (16.66) and this results
in
</p>
<p>1X
</p>
<p>`D0
f
.2`/
00 D
</p>
<p>8
</p>
<p>ˆ̂
&lt;
</p>
<p>ˆ̂
:
</p>
<p>1 for p D q D 1
2
;
</p>
<p>2p for p &lt; q ;
</p>
<p>2q for p &gt; q ;
</p>
<p>(17.27)
</p>
<p>with the consequence that the one-dimensional randomwalk is only recurrent in the
unbiased case v D 0.
</p>
<p>Another possibility to demonstrate the recurrence of the unbiased one-
dimensional random walk is provided by Eq. (16.69). The probability that a walker
returns to x0 D 0 after 2n steps is given by
</p>
<p>P.2n/.x0/ D
 
</p>
<p>2n
</p>
<p>n
</p>
<p>!
</p>
<p>pnqn D .2n/Š
nŠnŠ
</p>
<p>.pq/n : (17.28)
</p>
<p>In this case we are not interested in the question whether or not it is the particle&rsquo;s
first return. By STIRLING&rsquo;s approximation [Appendix, Eq. (E.20)]
</p>
<p>nŠ / nnC 12 e�n
p
2� ; (17.29)</p>
<p/>
</div>
<div class="page"><p/>
<p>17.3 The WIENER Process and Brownian Motion 279
</p>
<p>and we obtain for P.2n/.x0/:
</p>
<p>P.2n/.x0/ /
.4pq/np
</p>
<p>n�
: (17.30)
</p>
<p>We assume p � 1=2 and since pq D p.1 � p/ � 1=4 one gets
</p>
<p>1X
</p>
<p>nD0
P.2n/.x0/! 1 only for p D q D
</p>
<p>1
</p>
<p>2
: (17.31)
</p>
<p>The same argument holds for p &gt; 1=2 since we can write pq D .1 � q/q � 1=4.
According to Eq. (16.69) this means that the process is recurrent only for p D q, in
accordance with our previous result (17.27), and transient otherwise. We note that
this agrees also with the more physical picture of an external force inducing a bias
or drift velocity v &curren; 0.
</p>
<p>It should be noted that the unbiased random walk in two dimensions is also
recurrent while it can be proved to be transient in higher dimensions. For instance,
the recurrence probability is approximately 0:344 in 3D.
</p>
<p>17.3 The WIENER Process and Brownian Motion
</p>
<p>It is the purpose of this section to demonstrate that theWIENER process is the scaling
limit of the randomwalk. Moreover, we discuss briefly the LANGEVIN equation and
derive the diffusion equation.
</p>
<p>As a starting point we consider the one-dimensional unbiased randomwalk on an
equally spaced grid according to Eq. (17.8) and time instances given by Eq. (17.7).
We denote the stochastic process by Xn D X.tn/ and it is described by
</p>
<p>Xn D
n
X
</p>
<p>iD1
�i
x ; (17.32)
</p>
<p>where � 2 f�1; 1g together with X0 D 0. Since we regard the unbiased case Pr.�i D
˙1/ D 1=2, h�ii D 0, and var .�i/ D 1. This is equivalent to
</p>
<p>hXni D 0 and var .Xn/ D n
x2 ; (17.33)
</p>
<p>4This is one of P&Oacute;LYA&rsquo;s random walk constants [14&ndash;16].</p>
<p/>
</div>
<div class="page"><p/>
<p>280 17 The Random Walk and Diffusion Theory
</p>
<p>as we already demonstrated in the previous section, Eq. (17.21). The variance
var .Xn/ can be reformulated as
</p>
<p>var .Xn/ D tn

x2
</p>
<p>
t
; (17.34)
</p>
<p>using the definition tn � n
t. The simultaneous limit
t; 
x ! 0 is now performed
in such a way that
</p>
<p>lim

t!0

x!0
</p>
<p>
x2
</p>
<p>
t
D D D const ; (17.35)
</p>
<p>with D the diffusion coefficient. This limit is known as the continuous limit and it
will be denoted by the operatorL . Hence, in the continuous limit Eq. (17.34) results
in
</p>
<p>L Œvar .Xn/&#141; D Dt ; (17.36)
</p>
<p>where we renamed tn � t. We also note that the limit 
t ! 0 for constant t is
equivalent to n ! 1 and we obtain in accordance with the central limit theorem
(see Appendix, Sect. E.8):
</p>
<p>L .Xn/! Wt � N .0;Dt/ : (17.37)
</p>
<p>Here N .0;Dt/ denotes the normal distribution of mean zero and variance Dt,
Appendix Eq. (E.43). Furthermore, the symbol Wt was introduced to represent the
WIENER process and the symbol &lsquo;�&rsquo; stands, within this context, for the notion
follows the distribution. If Wt describes a WIENER process it is necessary to prove
that Wt has independent increments Wt2 �Wt1 which follow, according to Sect. 16.3,
a normal distribution with mean zero and a variance proportional to t2 � t1. This is
demonstrated quite easily: We learn from our discussion of the random walk that
</p>
<p>Xn � Xm D
nX
</p>
<p>iD1
�i �
</p>
<p>mX
</p>
<p>iD1
�i D
</p>
<p>nX
</p>
<p>iDmC1
�i ; (17.38)
</p>
<p>and, therefore, Xn � Xm and Xm � Xk are clearly independent for n &gt; m &gt; k and it
follows that also Wt � Ws and Ws � Wu are independent. Furthermore, we have
</p>
<p>Xn � Xm dD Xn�m ; (17.39)</p>
<p/>
</div>
<div class="page"><p/>
<p>17.3 The WIENER Process and Brownian Motion 281
</p>
<p>where the symbol &lsquo;
dD&rsquo; stands for the notion to follow the same distribution or to be
</p>
<p>distributionally equivalent. Therefore, in the limit L for t &gt; s
</p>
<p>Wt � Ws dD Wt�s � N Œ0;D.t � s/&#141; ; (17.40)
</p>
<p>which completes the proof. We note that the particular case D D 1 is commonly
referred to as the standard WIENER process. We remark that in many cases the
terms WIENER process and Brownian motion are used as synonyms for a stochastic
process satisfying the above properties. However, strictly speaking, the stochastic
process is the WIENER process while Brownian motion is the physical phenomenon
which can be described by the WIENER process.
</p>
<p>If we suppose that p &curren; q then
</p>
<p>L .hXni/ D �t ; (17.41)
</p>
<p>with the drift constant �, describes a WIENER process with a drift term
</p>
<p>L .Xn/! QWt D �t C Wt : (17.42)
</p>
<p>This process behaves like Wt with the only difference that it fluctuates around mean
�t instead of mean zero. Note that for � &gt; 0 the mean
</p>
<p>˝ QWt
˛
</p>
<p>increases, while for
� &lt; 0 it decreases with time t.
</p>
<p>Another interesting property of the WIENER process is its self-similarity. In
particular, we have the property that for ˛ &gt; 0
</p>
<p>Wt
dD ˛� 12 W˛t ; (17.43)
</p>
<p>with the consequence that it is completely sufficient to study the properties of the
WIENER process for t 2 Œ0; 1&#141; to know its properties for arbitrary time intervals.
Relation (17.43) follows from the fact that Wt � N .0;Dt/.
</p>
<p>Furthermore,white noise, �.t/, is defined as the formal derivative of the WIENER
process Wt with respect to time. We give its most important properties without going
into details5:
</p>
<p>h�.t/i D 0; and h�.t/�.s/i D ı.t � s/ : (17.44)
</p>
<p>5In fact, it can be shown that Wt is non-differentiable with probability one. This is the reason why
it is defined as the formal derivative of Wt. Let '.t/ be a test function and f .t/ an arbitrary function
which does not need to be differentiable with respect to t. Then the formal derivative Pf .t/ is defined
by
</p>
<p>Z 1
</p>
<p>0
</p>
<p>dt Pf .t/'.t/ D �
Z 1
</p>
<p>0
</p>
<p>dt f .t/ P'.t/ :</p>
<p/>
</div>
<div class="page"><p/>
<p>282 17 The Random Walk and Diffusion Theory
</p>
<p>Fig. 17.3 Three different
realizations of the standard
WIENER process with drift
� D 1 according to
Eq. (17.42). The expectation
value hxi D �t of the process
is presented as a dashed line
</p>
<p>White noise is referred to as Gaussian white noise if �.t/ follows a normal
distribution.
</p>
<p>Figure 17.3 presents three different realizations of the standard WIENER process
with drift according to Eq. (17.42). The curves in this figure were generated using
the procedure outlined in Sect. 16.3 in connection with Fig. 16.2.
</p>
<p>Let us derive the diffusion equation from the randomwalk model. The probability
Pr.x; t/ of finding the particle at time t at position x is expressed by
</p>
<p>Pr.x; t/ D Pr.x; t �
t/r C Pr.x �
x; t �
t/p
CPr.x C
x; t �
t/q
</p>
<p>D Pr.x; t �
t/.1 � p � q/C Pr.x �
x; t �
t/p
CPr.x C
x; t �
t/q ; (17.45)
</p>
<p>where we made use of relation (17.11). The interpretation of this equation is
straight-forward: The probability to find the particle at the position-time point .x; t/
is the sum of three terms. The first term describes the probability that the particle
arrived already at position x in the previous time step t�
t and that it will stay there
during the next time step. The remaining two terms describe the probability that the
particle arrived at position x�
x (xC
x) in the previous time step and that it will
move one step to the right (left) in the next time step. Each particular term is now
expanded into a TAYLOR series up to order O.
x2/ and O.
t/, respectively. This
requires the transition from a discrete to a continuous state space and, consequently,
the probabilities Pr.�/ are replaced by pdfs p.�/. We get
</p>
<p>p.x; t/ D .1 � p � q/
h
</p>
<p>p.x; t/ �
t@p.x; t/
@t
</p>
<p>i
</p>
<p>Cp
h
</p>
<p>p.x; t/�
t@p.x; t/
@t
</p>
<p>�
x@p.x; t/
@x</p>
<p/>
</div>
<div class="page"><p/>
<p>17.3 The WIENER Process and Brownian Motion 283
</p>
<p>C1
2

x2
</p>
<p>@2p.x; t/
</p>
<p>@x2
</p>
<p>i
</p>
<p>Cq
h
</p>
<p>p.x; t/ �
t@p.x; t/
@t
</p>
<p>C
x@p.x; t/
@x
</p>
<p>C1
2

x2
</p>
<p>@2p.x; t/
</p>
<p>@x2
</p>
<p>i
</p>
<p>; (17.46)
</p>
<p>and furthermore:
</p>
<p>@p.x; t/
</p>
<p>@t
D � . p � q/
x
</p>
<p>
t
</p>
<p>@p.x; t/
</p>
<p>@x
C . p C q/
x
</p>
<p>2
</p>
<p>2
t
</p>
<p>@2p.x; t/
</p>
<p>@x2
: (17.47)
</p>
<p>We draw the continuous limit and define the drift constant
</p>
<p>� D L
�
</p>
<p>. p � q/
x

t
</p>
<p>�
</p>
<p>D lim

t!0

x!0
</p>
<p>.q � p/

t
</p>
<p>
x ; (17.48)
</p>
<p>the diffusion constant
</p>
<p>D D L
�
</p>
<p>. p C q/
x
2
</p>
<p>2
t
</p>
<p>�
</p>
<p>D lim

t!0

x!0
</p>
<p>. p C q/
2
t
</p>
<p>
x2 ; (17.49)
</p>
<p>and arrive at the one-dimensional diffusion equation with drift term:
</p>
<p>@p.x; t/
</p>
<p>@t
D � @p.x; t/
</p>
<p>@x
C D@
</p>
<p>2p.x; t/
</p>
<p>@x2
: (17.50)
</p>
<p>This equation is referred to as a FOKKER-PLANCK equation [17]. In the specific case
p D q the drift term disappears and we obtain, as expected, the classical diffusion
equation
</p>
<p>@
</p>
<p>@t
p.x; t/ D D @
</p>
<p>2
</p>
<p>@x2
p.x; t/ ; (17.51)
</p>
<p>which we solved already numerically in Chaps. 9 and 11. It follows from this
discussion that the position of a diffusing particle can be described as a stochastic
process where, in the continuous limit, the jump-lengths follow a normal distribu-
tion. In addition, we know from our discussion of continuous-timeMARKOV-chains
in Sect. 16.5, that the waiting times between two successive jumps will certainly
follow an exponential distribution. These insights will serve as a starting point in
the discussion of general diffusion models in Sect. 17.4. Moreover, we note that the
anisotropy of the jump-length distribution is a model for the presence of an external
field which manifests itself in a drift term.</p>
<p/>
</div>
<div class="page"><p/>
<p>284 17 The Random Walk and Diffusion Theory
</p>
<p>An alternative approach to the formal description of Brownian motion goes back
to LANGEVIN. He considered the classical equation of motion of a particle in a fluid
which reads
</p>
<p>Pv D �ˇv ; (17.52)
</p>
<p>where ˇ denotes the friction coefficient and we set the particle&rsquo;s mass m equal to
one. LANGEVIN argued that this equation may only be valid for the average motion
of the particle which corresponds to the long time behavior of the motion of massive
particles. However, if the particle is not heavy at all its trajectory can be highly
affected by collisions with solvent&rsquo;s molecules. He supposed that a reasonable
generalization of Eq. (17.52) should be of the form [18]
</p>
<p>Pv D �ˇv C F.t/ ; (17.53)
</p>
<p>where F.t/ is a random force. In particular, F.t/ is a stochastic process which
satisfies
</p>
<p>hF.t/i D 0 and hF.t/F.s/i D Aı.t � t0/ ; (17.54)
</p>
<p>where A is a constant and we obtain
</p>
<p>F.t/
dD
p
</p>
<p>A�.t/ : (17.55)
</p>
<p>Equation (17.53) is referred to as the LANGEVIN equation and it is the prototype
stochastic differential equation. Based on the definition of white noise �.t/ the
LANGEVIN equation can be rewritten:
</p>
<p>dv D �ˇvdt C
p
</p>
<p>AdWt : (17.56)
</p>
<p>The solution of the LANGEVIN equation describes a stochastic process referred to
as the ORNSTEIN-UHLENBECK process [19]. This process is essentially the only
stochastic process which is stationary, Gaussian and Markovian. Its master equation
is a FOKKER-PLANCK equation of the form [17]
</p>
<p>@
</p>
<p>@t
p.v; t/ D ˇ @
</p>
<p>@v
vp.v; t/C A
</p>
<p>2
</p>
<p>@2
</p>
<p>@v2
p.v; t/ ; (17.57)
</p>
<p>where p.v; t/ is the pdf of the ORNSTEIN-UHLENBECK process. If the initial
velocity v0 is given then the pdf p.v; t/ can be proved to be
</p>
<p>p.v; t/ D
p
</p>
<p>ˇ
q
</p>
<p>�A
�
</p>
<p>1 � e�2ˇt
�
exp
</p>
<p>"
</p>
<p>�
ˇ
�
</p>
<p>v � v0e�ˇt
�2
</p>
<p>A
�
</p>
<p>1 � e�2ˇt
�
</p>
<p>#
</p>
<p>: (17.58)</p>
<p/>
</div>
<div class="page"><p/>
<p>17.4 Generalized Diffusion Models 285
</p>
<p>It is possible to solve the LANGEVIN equation (17.53) analytically with the result:
</p>
<p>v.t/ D v0 exp .�ˇt/C
p
</p>
<p>A
</p>
<p>Z t
</p>
<p>0
</p>
<p>dt0�.t0/ exp
�
</p>
<p>�ˇ.t � t0/
�
</p>
<p>: (17.59)
</p>
<p>We write in particular
</p>
<p>v.tnC1/ D v.tn/ exp .�ˇ
t/C Zn ; (17.60)
</p>
<p>with Zn defined as:
</p>
<p>Zn D
p
</p>
<p>A
</p>
<p>Z 
t
</p>
<p>0
</p>
<p>dt0�.tn C t0/ exp
�
</p>
<p>�ˇ.
t � t0/
�
</p>
<p>: (17.61)
</p>
<p>Since �.t/was assumed to be Gaussian white noise, Zn can be proved to be described
by
</p>
<p>Zn � N
�
</p>
<p>0;
A
</p>
<p>2ˇ
Œ1 � exp .�2ˇ
t/&#141;
</p>
<p>�
</p>
<p>; (17.62)
</p>
<p>which offers a very convenient way to simulate the ORNSTEIN-UHLENBECK
process. This particular formulation of Brownian motion allows to model this
process by sampling changes in the velocity Zn from the normal distribution with
mean zero and the variance given in Eq. (17.62). The walker&rsquo;s position x.t/ can then
be obtained by approximating the velocity v D Px with the help of finite difference
derivatives, as described in Chap. 2. In conclusion we remark that although the
LANGEVIN equationwas introduced in a heuristicmanner, it represents a very useful
tool due to its rather simple interpretation.
</p>
<p>Figure 17.4 presents three different realizations of the ORNSTEIN-UHLENBECK
process based on three different initial velocities v0. The corresponding random
trajectories x.t/ of the Brownian particle are illustrated in Fig. 17.5.
</p>
<p>17.4 Generalized Diffusion Models
</p>
<p>We formulate now a very general approach to diffusive behavior which is based on
continuous random variables. We start with the introduction of the pdf �.x; t/. Its
purpose is to describe the event that a particle arrives at time t at position x. It can
be expressed as [20, 21]
</p>
<p>�.x; t/ D
Z
</p>
<p>dx
Z t
</p>
<p>0
</p>
<p>dt0 .x; tI x0; t0/�.x0; t0/ ; (17.63)
</p>
<p>where  .x; tI x0; t0/ is the jump pdf. We offer the following interpretation:
 .x; tI x0; t0/ describes the probability for an event that a particle which arrived</p>
<p/>
</div>
<div class="page"><p/>
<p>286 17 The Random Walk and Diffusion Theory
</p>
<p>Fig. 17.4 Three different realizations of the ORNSTEIN-UHLENBECK process v.t/ vs t. For this
simulation we chose ˇ D 1, A D 5, dt D 10�2 and N D 103 time steps. Furthermore, we chose
three different initial velocities, i.e. v0 D 0 (black), v0 D 5 (gray) and v0 D 10 (light gray)
</p>
<p>Fig. 17.5 Random trajectories x.t/ vs t of the Brownian particle which correspond to the velocities
v.t/ illustrated in Fig. 17.4 with initial position x0 D 0. Note that we used for this figure N D 105
time steps
</p>
<p>at time t0 at position x0 &ndash; with pdf�.x0; t0/ &ndash; waited at position x0 until the time t was
reached and then jumped within an infinitesimal time interval from position x0 to x.
If we regard a space and time homogeneous process then  .x; tI x0; t0/ is replaced
by  .x � x0; t � t0/. This allows the introduction of a jump length pdf p.x/ and of a
waiting time pdf q.t/. They are related to the jump pdf by
</p>
<p>p.x/ D
Z 1
</p>
<p>0
</p>
<p>dt0 .x; t0/ and q.t/ D
Z 1
</p>
<p>�1
dx0 .x0; t/ : (17.64)</p>
<p/>
</div>
<div class="page"><p/>
<p>17.4 Generalized Diffusion Models 287
</p>
<p>If the jump length pdf and the waiting time pdf are conditionally independent one
can simply write  .x; t/ D p.x/q.t/. The probability '.x; t/ of finding a particle at
position x at time t is, furthermore, given by
</p>
<p>'.x; t/ D
Z t
</p>
<p>0
</p>
<p>dt0�.x; t0/�.t � t0/ ; (17.65)
</p>
<p>where �.t/ is the probability, that a particle stayed at least for a time interval t at the
same position, i.e.
</p>
<p>�.t/ D 1 �
Z t
</p>
<p>0
</p>
<p>dt0q.t � t0/ : (17.66)
</p>
<p>Finally, the jump length variance �2 and the characteristic waiting time � are
given by
</p>
<p>�2 D
Z 1
</p>
<p>�1
dx0x02p.x0/ and � D
</p>
<p>Z 1
</p>
<p>0
</p>
<p>dt0t0q.t0/ : (17.67)
</p>
<p>We conclude from our discussion of the WIENER process that for Brownian
motion the jump length pdf is a Gaussian and the waiting time pdf is an exponential
distribution:
</p>
<p>p.x/ D 1p
2��2
</p>
<p>exp
</p>
<p>�
</p>
<p>� x
2
</p>
<p>2�2
</p>
<p>�
</p>
<p>and q.t/ D 1
�
exp
</p>
<p>�
</p>
<p>� t
�
</p>
<p>�
</p>
<p>: (17.68)
</p>
<p>The characteristic function [Appendix Eq. (E.54)] of the waiting time pdf q.t/ is
given by
</p>
<p>Oq.s/ D
Z 1
</p>
<p>0
</p>
<p>dt q.t/e�st D 1
s� C 1 ; (17.69)
</p>
<p>and we find for the jump length pdf p.x/:
</p>
<p>Op.k/ D
Z
</p>
<p>dxe�ikxp.x/ D exp
�
</p>
<p>��2k2=2
�
</p>
<p>: (17.70)
</p>
<p>For x; t ! 1, i.e. k; s ! 0, the characteristic functions Oq.s/ and Op.k/ develop the
asymptotic behavior
</p>
<p>lim
s!0
</p>
<p>1
</p>
<p>1C s� � 1 � s� C O.s
2/ ; (17.71)</p>
<p/>
</div>
<div class="page"><p/>
<p>288 17 The Random Walk and Diffusion Theory
</p>
<p>and
</p>
<p>lim
k!0
</p>
<p>exp
�
</p>
<p>��2k2=2
�
</p>
<p>� 1 � �2k2=2C O.k4/ : (17.72)
</p>
<p>In fact, it can be shown that any pair of jump length and waiting time pdfs lead in
first order to the same asymptotic behavior, namely O.�/ and O.�2/, as long as the
moments � and �2 exist.
</p>
<p>However, there is a variety of processes which cannot be accounted for within
the basic framework of Brownian motion. Such processes are described within the
concept of anomalous diffusion [9, 20]. Examples are, for instance, the foraging
behavior of spider monkeys, particle trajectories in a rotating flow, diffusion of
proteins across cell membranes, diffusion of tracers in polymer-like breakable
micelles, the traveling behavior of humans, charge carrier transport in disordered
organic molecules, etc.
</p>
<p>We concentrate now on two particular models of anomalous diffusion. The first
model can, from a qualitative point of view, be characterized as a diffusion process
which consists of small clustering jumps which are intersected by very long flights.
Such behavior is, for instance, encountered in the context of human travel behavior,
Fig. 17.6 [22], charge carrier transport in disordered solids, etc. The incorporation
of these long jumps on a stochastic level is referred to as L&Eacute;VY flight [10]. The
second model, which is referred to as the fractal time random walk incorporates
</p>
<p>Fig. 17.6 Traveling behavior of humans (Adapted from [22]. Copyright &copy; 2006, Rights Managed
by Nature Publishing Group)</p>
<p/>
</div>
<div class="page"><p/>
<p>17.4 Generalized Diffusion Models 289
</p>
<p>anomalously long waiting times between two successive jumps. In particular, these
long waiting times account for non-Markovian effects which could be due to, for
instance, trapping processes of charge carriers in disordered solids. It has to be
emphasized at this point that the resulting diffusion models are still linear in the
pdf '.x; t/. The inclusion of non-linear effects will not be discussed here, however,
can be achieved within the framework of non-extensive thermodynamics [23].
</p>
<p>Let us start with L&Eacute;VY flights. In this case one modifies the asymptotic behavior
of the characteristic function of the jump length pdf according to
</p>
<p>Op.k/ / 1 � .� jkj/˛ ; (17.73)
</p>
<p>where ˛ 2 .0; 2&#141;. We recognize that this is the asymptotic behavior jkj ! 0 of the
characteristic function of a symmetric L&Eacute;VY ˛-stable distribution [19] following
Appendix Eq. (E.69). In the limit ˛ ! 2 normal, Gaussian behavior is recovered.
According to Appendix Eq. (E.70) the characteristic function (17.73) corresponds
to a jump length pdf:
</p>
<p>p.x/ / jxj�˛�1 for jxj ! 1 : (17.74)
</p>
<p>It is commonly referred to as a fat-tailed jump length pdf because of its asymptotic
behavior.
</p>
<p>A L&Eacute;VY flight is, in principle, a random walk where the length of the jumps
at discrete time instances tn follow the pdf (17.74). In the continuous time limit,
the waiting times are distributed exponentially as was illustrated in Sect. 16.5. It
has to be noted that in such a case the jump length variance diverges, i.e. ˙2 !
1. Consequently, L&Eacute;VY ˛-stable distributions are not subject to the central limit
theorem (see Appendix, Sect. E.8). In particular, the distance from the origin after
some finite time t follows a L&Eacute;VY ˛-stable distribution. Moreover, we note that
if 0 &lt; ˛ &lt; 1 even the mean jump length hxi diverges. A detailed mathematical
analysis proves, that L&eacute;vy flights result in a diffusion equation of the form
</p>
<p>@
</p>
<p>@t
p.x; t/ D D˛D˛jxjp.x; t/ ; (17.75)
</p>
<p>where D˛ is the fractional diffusion coefficient of dimension length˛� time�1 and
D˛jxj is the symmetric RIESZ fractional derivative operator of order ˛ 2 .1; 2/6:
</p>
<p>D
˛
jxjf .x/ D
</p>
<p>1
</p>
<p>2� .2 � ˛/ cos
�
˛�
2
</p>
<p>�
</p>
<p>Z
</p>
<p>dx0
f 00.x0/
</p>
<p>jx � x0j˛�1 ; (17.76)
</p>
<p>where f 00.x/ is the second spatial derivative of f .
</p>
<p>6A short introduction to fractional derivatives and integrals can be found in Appendix G.</p>
<p/>
</div>
<div class="page"><p/>
<p>290 17 The Random Walk and Diffusion Theory
</p>
<p>Fig. 17.7 Three different
realizations of the one
dimensional L&eacute;vy flight. The
parameters are `D 0:001,
˛ D 1:3 and we performed
N D 1000 time steps
</p>
<p>Fig. 17.8 Comparison
between the two-dimensional
WIENER process (solid
up-triangles) and the
two-dimensional L&Eacute;VY flight
(open squares) for ˛ D 1:3.
The minimal flight length of
the L&Eacute;VY flight as well as the
jump length variance of the
WIENER process were set
`D ˙2 D 0:1 and we
performed N D 100 time
steps
</p>
<p>Figure 17.7 illustrates a one-dimensional L&Eacute;VY flight and Fig. 17.8 presents
a comparison between a two-dimensional L&Eacute;VY flight and a two-dimensional
WIENER process. These figures were generated by sampling an exponential dis-
tribution with mean hti D 1 for the waiting times. On the other hand a jump length
pdf
</p>
<p>p.x/ D ˛`˛�.x � `/
x˛C1
</p>
<p>; x &gt; 0 : (17.77)
</p>
<p>was sampled for the jump length of the L&Eacute;VY flight. Here ˛ is referred to as the
L&Eacute;VY index, �.�/ denotes the HEAVISIDE � function and ` &gt; 0 is the minimal
flight length. We introduced this particular form of the pdf because it can rather
easily be sampled with the help of the inverse transformation method &ndash; Sect. 13.2 &ndash;
and it obeys the asymptotic behavior, Eq. (17.74). Moreover, it can be proved that it</p>
<p/>
</div>
<div class="page"><p/>
<p>17.4 Generalized Diffusion Models 291
</p>
<p>gives the correct behavior in the limit ` ! 0. Finally, the direction of the jump has
to be sampled in an additional step. Figure 17.8 is particularly instructive because
the different physics described by these two models becomes immediately apparent.
</p>
<p>Let us turn our attention to the second scenario, the fractal time random walk. In
this case the asymptotic behavior of the waiting time pdf is modified according to
</p>
<p>Oq.s/ / 1 � .Ts/ˇ ; (17.78)
</p>
<p>where ˇ 2 .0; 1&#141; and for ˇ ! 1 regular behavior, an exponentially distributed
waiting time, is recovered. A pdf of such a form is commonly referred to as a fat-
tailed waiting time pdf. After an inverse LAPLACE transform we obtain
</p>
<p>q.t/ / t�ˇ�1 for t ! 1 : (17.79)
</p>
<p>We note that in this case the mean waiting time T D hti diverges for ˇ &lt; 1.
This clearly indicates a non-Markovian time evolution since we demonstrated in
Sect. 16.5 that every Markovian discrete time process converges in the continuous
time limit to a process with exponentially distributed waiting times. Again, the
ansatz
</p>
<p>q.t/ D ˇ�ˇ�.t � �/
tˇC1
</p>
<p>; (17.80)
</p>
<p>is employed, where � &gt; 0 is the minimal waiting time. The process is essentially a
random walk with waiting times distributed according to Eq. (17.80), i.e. the jump
length
x is constant. In the continuous space limit
x ! 0 the jump lengths follow
a Gaussian, as in the case of a regular random walk. A detailed analysis proves that
in the limit � ! 0 the corresponding diffusion equation is given by
</p>
<p>CD
ˇ
t p.x; t/ D Dˇ
</p>
<p>@2
</p>
<p>@x2
p.x; t/ ; (17.81)
</p>
<p>where the diffusion constant Dˇ is of dimension length2� time�ˇ . Here, CDˇt is the
CAPUTO fractional time derivative of order ˇ 2 .0; 1/ (see Appendix G). It is of the
form
</p>
<p>CD
ˇ
t f .t/ D
</p>
<p>1
</p>
<p>� .1 � ˇ/
</p>
<p>Z t
</p>
<p>0
</p>
<p>dt0
Pf .t0/
</p>
<p>.t � t0/ˇ : (17.82)
</p>
<p>It follows from the properties of fractional derivatives that an alternative form of
Eq. (17.81) can be found, namely
</p>
<p>@
</p>
<p>@t
p.x; t/ D Dˇ
</p>
<p>@2
</p>
<p>@x2
D
ˇ
t p.x; t/ ; (17.83)</p>
<p/>
</div>
<div class="page"><p/>
<p>292 17 The Random Walk and Diffusion Theory
</p>
<p>Fig. 17.9 Three different
realizations of the fractal time
random walk in one
dimension for ˇ D 0:8 and
� D 0:1
</p>
<p>where Dˇt is the RIEMANN-LIOUVILLE fractional derivative of order ˇ (see
Appendix G).
</p>
<p>Figure 17.9 presents three different realizations of the fractal time random walk.
The waiting times were sampled from the pdf (17.80) with the help of the inverse
transformation method &ndash; Sect. 13.2 &ndash; and the jump lengths were sampled from a
normal distribution with jump length variance˙2 D 1.
</p>
<p>It is a straight-forward task to combine fractal time random walks and L&Eacute;VY
flights to so called fractal time L&Eacute;VY flights. The resulting diffusion equation can
be written as
</p>
<p>CD
ˇ
t p.x; t/ D D˛ˇD˛jxjp.x; t/ ; (17.84)
</p>
<p>where the diffusion constant D˛ˇ has units length˛� time�ˇ and CDˇt and D˛jxj are
the fractional CAPUTO and RIESZ derivatives, respectively.
</p>
<p>Figure 17.10 illustrates three different realizations of such a diffusion process.
The waiting times were sampled from the pdf (17.80) where we set � D 0:1 and
ˇ D 0:8. The jump lengths were sampled from the pdf (17.77) with ˛ D 1:3 and
` D 0:01. Finally, the direction of the jump was sampled in an additional step.
</p>
<p>We close this chapter with a short discussion: The description of diffusion
processes with the help of stochastics proofed to be one of the most powerful
methods in modern theoretical physics. Within this chapter we discussed several
different paths toward a description of Brownian motion, namely the random
walk, the WIENER process, and the LANGEVIN equation, as well as models which
describe phenomena beyond Brownian motion. It has to be emphasized that the field
of anomalous diffusion in general is still developing rapidly, however, its importance
for the description of various phenomena in science is already impressive. We
refer the interested reader to the excellent review articles by R. METZLER and J.
KLAFTER on anomalous diffusion [20, 21].</p>
<p/>
</div>
<div class="page"><p/>
<p>17.4 Generalized Diffusion Models 293
</p>
<p>Fig. 17.10 Three possible
realizations of the fractal time
L&Eacute;VY flight in one
dimension. The parameters
are � D 0:1, ˇ D 0:8,
`D 0:01 and ˛ D 1:3
</p>
<p>Summary
</p>
<p>The random walk, a classical example of MARKOV-chains, was used to open the
door to the realm of diffusion theory. Random walks have been used for a long
time to simulate Brownian motion and related problems. From a theoretical point of
view randomwalks were described by the scaling limit of the WIENER process. The
biased WIENER process was then used to demonstrate that the FOKKER-PLANCK
equation followed in the limit of a continuous state space, as the classical diffusion
equation followed from the unbiased WIENER process in the same limit. Brownian
motion was also the basis for the rather heuristic introduction of the stochastic
differential equation by LANGEVIN. A direct consequence of this equation was the
ORNSTEIN-UHLENBECK process with its master equation, the FOKKER-PLANCK
equation. It was the only stationary, Gaussian, and Markovian process in this
class of stochastic diffusion processes. An extension of these processes was then
possible by the introduction of a jump pdf which in turn allowed to define a jump
length pdf and a waiting time pdf. These two pdfs resulted in a more general
description of diffusion processes in a space and time homogeneous environment.
Furthermore, the observation that many diffusive processes (not only in physics)
cannot be understood within the framework of &lsquo;classical&rsquo; Brownian motion resulted
in the introduction of L&Eacute;VY flights. This was particularly motivated by the need
for a process whose jump-length variance diverges which enabled, for instance the
simulation of human travel behavior. In the very last step the fractal time random
walk was introduced. It was characterized by a specific form of the waiting time pdf
which made it possible to describe on a stochastic level anomalously long waiting
times between two consecutive jumps. Such behavior can, for instance, be observed
by trapping phenomena in solids. The combination of both extensions resulted in
the fractal time L&Eacute;VY flight.</p>
<p/>
</div>
<div class="page"><p/>
<p>294 17 The Random Walk and Diffusion Theory
</p>
<p>Problems
</p>
<p>1. Write a program which simulates different realizations of the following stochas-
tic processes in one spatial dimension:
</p>
<p>a. A random walk.
b. A standard WIENER process and a WIENER process with drift.
c. An OHRNSTEIN-UHLENBECK process.
d. A L&Eacute;VY flight.
e. A fractal time random walk.
f. A fractal time L&Eacute;VY flight.
</p>
<p>Illustrate three different sample paths graphically for each process. Furthermore,
perform the following tests:
</p>
<p>a. Calculate the expectation value hxni and the variance var .xn/ of the random
walk numerically by restarting the process several times with different seeds.
</p>
<p>b. In a similar fashion, calculate numerically hWti and var .Wt/.
c. Try different parameters ˛; ˇ for L&Eacute;VY flights and fractal time random walks.
</p>
<p>2. Write a program which simulates the WIENER process in two dimensions. This
can be achieved by drawing the jump length from a normal distribution and
sampling the jump angle, i.e. the direction, in an additional step. Augment this
program with L&Eacute;VY flight jump lengths pdfs.
</p>
<p>References
</p>
<p>1. Glicksman, M.E.: Diffusion in Solids. Wiley, New York (1999)
2. Cussler, E.L.: Diffusion. Cambridge University Press, Cambridge (2009)
3. Dubitzky, W., Wolkenhauer, O., Cho, K.H., Yokota, H. (eds.): Encyclopedia of Systems
</p>
<p>Biology, p. 1596. Springer, Berlin/Heidelberg (2013)
4. Tapiero, C.S.: Risk and Financial Management: Mathematical and Computational Methods.
</p>
<p>Wiley, New York (2004)
5. Harris, S.: An Introduction to the Theory of the Boltzmann Equation. Dover, Mineola (2004)
6. Ballentine, L.E.: Quantum Mechanics. World Scientific, Hackensack (1998)
7. Rudnik, J., Gaspari, G.: Elements of the Random Walk. Cambridge University Press,
</p>
<p>Cambridge (2010)
8. Ibe, O.C.: Elements of Random Walk and Diffusion Processes. Wiley, New York (2013)
9. Pekalski, A., Sznajd-Weron, K. (eds.): Anomalous Diffusion. Lecture Notes in Physics.
</p>
<p>Springer, Berlin/Heidelberg (1999)
10. Shlesinger, M.F., Zaslavsky, G.M., Frisch, U. (eds.): L&eacute;vy Flights and Related Topics in
</p>
<p>Physics. Lecture Notes in Physics. Springer, Berlin/Heidelberg (1995)
11. Norris, J.R.: Markov Chains. Cambridge Series in Statistical and Probabilistic. Cambridge
</p>
<p>University Press, Cambridge (1998)
12. Modica, G., Poggiolini, L.: A First Course in Probability and Markov Chains. Wiley,
</p>
<p>New York (2012)
13. Graham, C.: Markov Chains: Analytic and Monte Carlo Computations. Wiley, New York
</p>
<p>(2014)</p>
<p/>
</div>
<div class="page"><p/>
<p>References 295
</p>
<p>14. P&oacute;lya, G.: &Uuml;ber eine Aufgabe der Wahrscheinlichkeitsrechnung betreffend die Irrfahrt im
Stra&szlig;ennetz. Math. Ann. 84, 149&ndash;160 (1921)
</p>
<p>15. Finch, S.R. (ed.): Mathematical Constants, pp. 322&ndash;331. Cambridge University Press,
Cambridge (2003)
</p>
<p>16. Weisstein, E.W.: P&oacute;lya&rsquo;s random walk constants. http://mathworld.wolfram.com/
PolyasRandomWalkConstants.html
</p>
<p>17. Risken, H., Frank, T.: The Fokker-Planck Equation. Springer Series in Synergetics.
Springer, Berlin/Heidelberg (1996)
</p>
<p>18. Coffey, W.T., Kalmykov, Y.P.: The Langevin Equation, 3rd edn. World Scientific Series in
Contemporary Chemical Physics: Volume 27. World Scientific, Hackensack (2012)
</p>
<p>19. Breuer, H.P., Petruccione, F.: Open Quantum Systems, chap. 1. Clarendon Press, Oxford (2010)
20. Metzler, R., Klafter, J.: The random walk&rsquo;s guide to anomalous diffusion: a fractional dynamics
</p>
<p>approach. Phys. Rep. 339, 1&ndash;77 (2000)
21. Metzler, R., Klafter, J.: The restaurant at the end of random walk: recent developments in the
</p>
<p>description of anomalous transport by fractional dynamics. J. Phys. A 37, R161 (2004)
22. Brockmann, D., Hufnagl, L., Geisl, T.: The scaling laws of human travel. Nature (London)
</p>
<p>439, 462&ndash;465 (2006). doi:10.1038/nature04292
23. Tsallis, C.: Introduction to Nonextensive Statistical Mechanics. Springer, Berlin/Heidelberg
</p>
<p>(2009)</p>
<p/>
<div class="annotation"><a href="http://mathworld.wolfram.com/PolyasRandomWalkConstants.html">http://mathworld.wolfram.com/PolyasRandomWalkConstants.html</a></div>
<div class="annotation"><a href="http://mathworld.wolfram.com/PolyasRandomWalkConstants.html">http://mathworld.wolfram.com/PolyasRandomWalkConstants.html</a></div>
</div>
<div class="page"><p/>
<p>Chapter 18
MARKOV-Chain Monte Carlo and the POTTS
Model
</p>
<p>18.1 Introduction
</p>
<p>This chapter discusses in more detail the concept of MARKOV-chain Monte Carlo
techniques [1&ndash;4]. We already came across the METROPOLIS algorithm in Sect. 14.3,
where the condition of detailed balance proved to be the crucial point of the method.
The reason for imposing such a condition was explained in all required detail during
our discussion of MARKOV-chains within Sect. 16.4. The ISING model, analyzed in
Chap. 15, served as a first illustration of the applicability of MARKOV-chain Monte
Carlo methods in physics.
</p>
<p>Let us briefly summarize what we learned so far: We discussed several methods
to sample pseudo random numbers from a given distribution in Chap. 13. The
two most important methods, the inverse transformation method and the rejection
method, were based on an exact knowledge of the analytic form of the distribution
function which the random numbers were supposed to follow. However, when
simulating the physics of the ISING model it was required to draw random
configurations from the equilibrium distribution of the system and, unfortunately,
the exact analytic form of this distribution was unknown. On the other hand, in the
discussion of MARKOV-chains we came across the condition of detailed balance.
Invoking this condition ensured that the constructed MARKOV-chain converged
toward a stationary distribution, independent of the initial conditions. Consequently,
we can also sample random numbers by constructing a MARKOV-chain with a
stationary distributionwhich is equal to the distribution fromwhich we would like to
obtain our random numbers. In such a case the distribution function has to be known,
at least in principle. However, the formulation of the METROPOLIS algorithm
allowed for an unknown normalization constant of the distribution function which,
in turn, makes this method such a powerful tool in computational physics.
</p>
<p>Here we plan to discuss MARKOV-chain Monte Carlo techniques in greater
detail. We start with the introduction of the concept of importance sampling, review
the METROPOLIS algorithm, and discuss the straight forward generalization to the
</p>
<p>&copy; Springer International Publishing Switzerland 2016
B.A. Stickler, E. Schachinger, Basic Concepts in Computational Physics,
DOI 10.1007/978-3-319-27265-8_18
</p>
<p>297</p>
<p/>
</div>
<div class="page"><p/>
<p>298 18 MARKOV-Chain Monte Carlo and the POTTS Model
</p>
<p>METROPOLIS-HASTINGS algorithm. Finally, the applicability of the METROPOLIS-
HASTINGS algorithm will be demonstrated by simulating the physics of the q-states
POTTS model [5] which is closely related to the ISING model. This chapter is
closed with a brief presentation of some of the more advanced techniques within
this context.
</p>
<p>18.2 MARKOV-Chain Monte Carlo Methods
</p>
<p>Before turning our focus toward the MARKOV-chain Monte Carlo methods we shall
briefly discuss importance sampling. Let p.x/ be a certain pdf fromwhich we would
like to draw a sequence of random numbers fxig, i � 1. Furthermore, let f .x/ be
some arbitrary function and we would like to estimate its expectation value h f ip
which is determined by the integral
</p>
<p>h f ip D
Z
</p>
<p>dx f .x/p.x/ : (18.1)
</p>
<p>But h f ip can also be regarded as the expectation value haiu of the function a.x/ WD
f .x/p.x/, with u.x/ the pdf of the uniform distribution. Hence, we may evaluate
haiu by drawing uniformly distributed random numbers on a given interval Œa; b&#141; �
R and by estimating the expectation value by its arithmetic mean as discussed in
Sect. 14.2. This approach is the easiest version of a method referred to as simple
sampling. On the other hand, we might approximate h f ip by sampling xi according
to p.x/ and by employing the central limit theorem (see Appendix, Sect. E.8) as
was demonstrated in Sect. 14.2. The basic idea of importance sampling, however, is
to improve this approach by sampling from a different distribution q.x/ which is in
most cases chosen in such a way that the expectation value h f ip is easier to evaluate.
</p>
<p>Let g.x/ be some function with g.x/ &gt; 0 for all x. Then
</p>
<p>h f ip D
Z
</p>
<p>dx f .x/p.x/ D
Z
</p>
<p>dx
f .x/
</p>
<p>g.x/
p.x/g.x/ D c
</p>
<p>�
f
</p>
<p>g
</p>
<p>�
</p>
<p>q
</p>
<p>; (18.2)
</p>
<p>where we defined the function q.x/ D p.x/g.x/=c and c is chosen in such a way
that
</p>
<p>R
</p>
<p>dxq.x/ D 1. We note that g.x/ can be any positive function. Hence, such an
approach might be interesting in two different scenarios: (i) if it is easier to sample
from the distribution q.x/ rather than from p.x/ and, (ii) if such a sampling results
in a variance reduction which is equivalent to a decrease in error, and less random
numbers are to be sampled to obtain comparable results.
</p>
<p>Let us briefly elaborate on this point: we have
</p>
<p>var
</p>
<p>�
f
</p>
<p>g
</p>
<p>�
</p>
<p>q
</p>
<p>D
* 
</p>
<p>f
</p>
<p>g
�
�
</p>
<p>f
</p>
<p>g
</p>
<p>�
</p>
<p>q
</p>
<p>!2+
</p>
<p>q
</p>
<p>; (18.3)</p>
<p/>
</div>
<div class="page"><p/>
<p>18.2 MARKOV-Chain Monte Carlo Methods 299
</p>
<p>and for the particular choice g.x/ � f .x/ we obtain that
</p>
<p>var
</p>
<p>�
f
</p>
<p>g
</p>
<p>�
</p>
<p>q
</p>
<p>D 0 ; (18.4)
</p>
<p>and the error of our Monte Carlo integration vanishes. However, the ideal case of
g.x/ � f .x/ is unrealistic because we obtain for the normalization constant c
</p>
<p>c D
Z
</p>
<p>dx p.x/g.x/ D hgip � h f ip ; (18.5)
</p>
<p>which is exactly the integral we want to evaluate. Nevertheless, the function g.x/
can be adapted to improve the result. We choose g.x/ in such a way that the integral
hgip is easily evaluated and that g.x/ follows f .x/ as closely as possible; in other
words, the quotient f .x/=g.x/ becomes as constant as possible. This means that we
no longer sample from p.x/ within a given interval but only from points which are
of importance for the particular function f .x/. Such an approach is referred to as
importance sampling [6&ndash;9].
</p>
<p>The attentive reader might have observed that, on a first glance, importance
sampling has nothing to do with MARKOV-chain Monte Carlo methods in general.
Nevertheless, it can be demonstrated that MARKOV-chain Monte Carlo methods
correspond indeed to importance sampling.
</p>
<p>To prove this, we remember that MARKOV-chain Monte Carlo techniques are
based on the generation of a sequence of configurations S.n/:
</p>
<p>S.1/ ! S.2/ ! : : :! S.n/ ! : : : : (18.6)
</p>
<p>Each individual configuration S.n/ is generated from the previous configuration
S.n�1/ at random with a certain transition probability P.S.n�1/ ! S.n//. These
transition probabilities obey
</p>
<p>P.S ! S0/ � 0 and
X
</p>
<p>S0
</p>
<p>P.S ! S0/ D 1 ; (18.7)
</p>
<p>and this property ensures that the sequence (18.6) is a MARKOV-chain. In Sect. 16.4
we observed that the condition of detailed balance for a stationary distribution P.S/
</p>
<p>P.S/P.S ! S0/ D P.S0/P.S0 ! S/ (18.8)
</p>
<p>guarantees convergence of the MARKOV-chain toward the stationary distribution.
Hence, the remaining task is to find transition probabilities which fulfill detailed
balance. In a typical situation, the transition probabilities can be written as
</p>
<p>P.S ! S0/ D Pp.S ! S0/Pa.S ! S0/ ; (18.9)</p>
<p/>
</div>
<div class="page"><p/>
<p>300 18 MARKOV-Chain Monte Carlo and the POTTS Model
</p>
<p>where Pp.S ! S0/ is the probability that a configuration S0 is proposed and Pa.S !
S0/ is the probability that the proposed configuration is accepted. In many cases one
simplifies the situation by assuming that
</p>
<p>Pp.S ! S0/ D Pp.S0 ! S/ ; (18.10)
</p>
<p>and, thus, the condition of detailed balance changes into
</p>
<p>P.S/Pa.S ! S0/ D P.S0/Pa.S0 ! S/ : (18.11)
</p>
<p>The METROPOLIS algorithm uses one possible choice for the acceptance probabil-
ity, namely
</p>
<p>Pa.S ! S0/ D min
�
</p>
<p>1;
P.S0/
</p>
<p>P.S/
</p>
<p>�
</p>
<p>: (18.12)
</p>
<p>It was demonstrated in Sect. 14.3 that Eq. (18.12) indeed fulfills Eq. (18.11). The
execution of the algorithm has already been illustrated in Chap. 15 in its application
to the numerics of the ISING model.
</p>
<p>A rather straight forward generalization of the METROPOLIS algorithm (18.12)
is found when an asymmetric proposal probability Pp.S ! S0/ is considered. It is
easily demonstrated that the choice
</p>
<p>Pa.S ! S0/ D min
�
</p>
<p>1;
P.S0/
</p>
<p>P.S/
</p>
<p>Pp.S
0 ! S/
</p>
<p>Pp.S ! S0/
</p>
<p>�
</p>
<p>; (18.13)
</p>
<p>also fulfills detailed balance (18.8). The choice (18.13) is referred to as the
METROPOLIS-HASTINGS algorithm [10].1
</p>
<p>By exploiting the MARKOV property in order to sample configurations according
to the BOLTZMANN distribution we perform importance sampling as illustrated
above. An alternative approachwould be to select different configurations according
to a uniform distributionwhich obviously increases the numerical cost of the method
bymagnitudes. Hence, sampling with the help of aMARKOV-chain yields a variance
reduction in comparison to the crude approach of simple sampling. Furthermore, the
algorithm can be optimized by a clever choice of Pp.S ! S0/ which does not need
to be symmetric. Clearly, this choice will have to depend on the particular problem
at hand.
</p>
<p>We shall briefly discuss two alternative approaches to MARKOV-chain Monte
Carlo sampling, namely GIBBS sampling [11] and slice sampling [12]: Suppose we
want to sample a sequence of m-dimensional variables x.n/ D .x.n/1 ; x
</p>
<p>.n/
2 ; : : : ; x
</p>
<p>.n/
m /
</p>
<p>T
</p>
<p>from a multivariate distribution function p.x/ D p.x1; x2; : : : ; xm/. In such a case
</p>
<p>1Please note that it is common in the literature to refer even to Eq. (18.12) as a METROPOLIS-
HASTINGS algorithm, despite the fact that here Pp.S0 ! S/ D Pp.S ! S0/.</p>
<p/>
</div>
<div class="page"><p/>
<p>18.2 MARKOV-Chain Monte Carlo Methods 301
</p>
<p>GIBBS sampling is particularly interesting if the joint distribution function is well-
known and simple to sample. The acceptance probability for a particular component
of the vector x.n/ is set to:
</p>
<p>Pa.x
.nC1/
j jx.n// D p.x
</p>
<p>.nC1/
j jx
</p>
<p>.nC1/
1 ; : : : ; x
</p>
<p>.nC1/
j�1 ; x
</p>
<p>.n/
jC1; : : : ; x
</p>
<p>.n/
m / : (18.14)
</p>
<p>This is possible as we have
</p>
<p>p.xjjx1; : : : ; xj�1; xjC1; : : : ; xm/ D
p.x1; : : : ; xm/
</p>
<p>p.x1; : : : ; xj�1; xjC1; : : : ; xm/
</p>
<p>/ p.x1; : : : ; xm/ ; (18.15)
</p>
<p>because the denominator of the left hand side of Eq. (18.15) is independent of xj. It
can, therefore, be treated as a normalization constant when xj is sampled.
</p>
<p>Let us briefly discuss slice sampling: For reasons of simplicity we shall regard
the uni-variate case where p.x/ denotes the pdf fromwhich we would like to sample.
We apply the following algorithm:
</p>
<p>1. Choose some initial value x0.
2. Sample a uniformly distributed random variable y0 from the interval Œ0; p.x0/&#141;.
3. Sample the next random variable x1 uniformly from the slice S Œp�1.y0/&#141;.
4. Sample a uniformly distributed random number y1 2 Œ0; p.x1/&#141;.
5. Sample the next random variable x2 uniformly from the slice S Œp�1.y1/&#141;.
6. . . .
:::
</p>
<p>The final sequence fxjg is constructed by ignoring the yn-values (even steps). This
procedure is illustrated schematically in Fig. 18.1.
</p>
<p>Fig. 18.1 Schematic
illustration of slice sampling
for the uni-variate case
described by the pdf p.x/.
The relevant steps of the
algorithm are indicated by
solid asterisks</p>
<p/>
</div>
<div class="page"><p/>
<p>302 18 MARKOV-Chain Monte Carlo and the POTTS Model
</p>
<p>18.3 The POTTS Model
</p>
<p>We studied in Chap. 15 the two-dimensional ISING model as an example for the
METROPOLIS algorithm. Here we expand on this discussion and investigate the q-
states POTTS model [5] which can be regarded as a generalization of the ISING
model. The model is characterized by the HAMILTON function
</p>
<p>H D �
X
</p>
<p>ij
</p>
<p>Jijı�i�j ; (18.16)
</p>
<p>where the notation used for the ISING model applies. In particular, we shall regard
the case Jij D J for i; j nearest neighbors and Jij D 0 otherwise. In contrast to
the ISING model, the spin realizations �i on grid-point i can take integer values
�i D 1; 2; : : : ; q. For q D 2, the POTTS model is equivalent to the ISING model
which can be easily proved by rewriting the HAMILTON function as
</p>
<p>H D J
2
</p>
<p>X
</p>
<p>hiji
2
</p>
<p>�
1
</p>
<p>2
� ı�i�j
</p>
<p>�
</p>
<p>� J
X
</p>
<p>i
</p>
<p>1
</p>
<p>2
: (18.17)
</p>
<p>Here hiji denotes sum over nearest neighbors. We observe that 2
�
1
2
� ı�i�j
</p>
<p>�
</p>
<p>is equal
to �1 for �i D �j and C1 for �i &curren; �j. Moreover, the constant energy shift in
Eq. (18.17) can be neglected and we recover the ISING model of Chap. 15.
</p>
<p>The method to calculate the observables of interest, like hEi, hMi, ch and �, and
the basic algorithm can be adopted from Sect. 15.2 as is.2 There is one important
difference. It occurs in step 3 of the algorithm: Instead of setting �ij D ��ij we
sample the new value of �ij uniformly distributed from 1; 2; : : : ; q under exclusion
of the old value of �ij.
</p>
<p>Figures 18.2, 18.3, 18.4, and 18.5 display the mean energy per particle, the mean
magnetization per particle hm1i [with Q D 1 in Eq. (18.18)], the heat capacity ch as
well as the magnetic susceptibility � for q D 1; 2; : : : ; 8 and J D 0:5 [Eq. (18.16)]
vs temperature kBT. The size of the system was N � N with N D 40. We performed
104 measurements per temperature and 10 sweeps where discarded between two
successive measurements in order to reduce correlations. The equilibration time was
set to 103 sweeps. A typical spin configuration for q D 4 and kBT D 0:47 can be
found in Fig. 18.6.
</p>
<p>A number of interesting details can be observed in Fig. 18.3. First of all, we
recognize that the mean magnetization hm1i above the critical temperature decreases
</p>
<p>2We calculate the magnetization in a particular spin Q via
</p>
<p>MQ.C / D
 
X
</p>
<p>i
</p>
<p>ı�i ;Q
</p>
<p>!
</p>
<p>C
</p>
<p>: (18.18)</p>
<p/>
</div>
<div class="page"><p/>
<p>18.3 The POTTS Model 303
</p>
<p>Fig. 18.2 The mean free energy per particle h"i vs temperature kBT for a q-states POTTSmodel on
a 40�40 square lattice, with q D 1; 2; : : : ; 8 and J D 0:5. 104 measurements have been performed
</p>
<p>Fig. 18.3 The mean magnetization per particle hm1i vs temperature kBT for a q-states POTTS
model on a 40 � 40 square lattice, for q D 1; 2; : : : ; 8 and J D 0:5. 104 measurements have been
performed
</p>
<p>with increasing values of q. The reason is that the mean magnetization hm1i
represents for T � Tc the probability of finding a particular spin in state �i D 1.
This is equivalent to 1=q for a uniform distribution and therefore decreases with
increasing q. Please note that the expectation value of the magnetization hmQi is
restricted to take the values from f0; 1g for T � TC due to the modified definition
of MQ.C /. This is in contrast to the ISING model where hmi 2 f�1; 1g for T &lt; Tc.
Hence, we obtain for T � TC hm1i D 0 with probability .q � 1/=q and hm1i D 1
with probability 1=q. However, the particular definition (18.18) of MQ.C / is not</p>
<p/>
</div>
<div class="page"><p/>
<p>304 18 MARKOV-Chain Monte Carlo and the POTTS Model
</p>
<p>Fig. 18.4 The heat capacity ch vs temperature kBT for a q-states POTTSmodel on a 40�40 square
lattice, with q D 1; 2; : : : ; 8 and J D 0:5. 104 measurements have been performed. The inset shows
the specific heat ch on a logarithmic scale in the region around the transition temperature
</p>
<p>Fig. 18.5 The magnetic susceptibility � vs temperature kBT for a q-states POTTS model on a
40 � 40 square grid, with q D 1; 2; : : : ; 8 and J D 0:5. 104 measurements have been performed.
The inset shows the magnetic susceptibility � on a logarithmic scale in the region around the
transition temperature
</p>
<p>important since the physically relevant property of the POTTS model HAMILTON
function (18.16) is its Zq symmetry with a degenerate ground state.
</p>
<p>A second interesting feature is the observation that the critical temperature Tc
also decreases with increasing values of q which becomes particularly transparent
from Figs. 18.4 and 18.5. The critical temperatures are quoted in Table 18.1. Finally,
we deduce from Fig. 18.2 that the phase transition is smoother for q D 2 and
becomes discontinuous for large values of q. In particular, the q-states POTTS model</p>
<p/>
</div>
<div class="page"><p/>
<p>18.3 The POTTS Model 305
</p>
<p>Fig. 18.6 A typical spin
configuration �i;j for a
q-states POTTS model on a
40� 40 square lattice with
q D 4, J D 0:5, and
kBT D 0:47
</p>
<p>Table 18.1 List of the
critical values ˛c D J=.kBTc/
of the q-states POTTS model
for q D 2; 3; : : : ; 8
</p>
<p>q ˛c
</p>
<p>2 0.89
</p>
<p>3 1.00
</p>
<p>4 1.09
</p>
<p>5 1.16
</p>
<p>6 1.22
</p>
<p>7 1.28
</p>
<p>8 1.35
</p>
<p>exhibits a second order phase transition for q D 2; 3; 4 and a first order phase
transition for q &gt; 4 which is hard to see from Figs. 18.2 and 18.3. However,
there is another method to unambiguously identify a first order phase transition.
It is referred to as the histogram technique. The mean energies of consecutive
measurements near the critical temperature are simply collected in a histogram.
If only one peak is observed, the system fluctuates around a single phase, and a
second order phase transition was observed. However, the existence of two or more
peaks means that the system fluctuates between two or more different phases and,
therefore, exhibits a first order phase transition. Figure 18.7 displays two histograms
for q D 2 (kBT D 0:56) and q D 8 (kBT D 0:37) from 104 measurements to prove
our case.
</p>
<p>One possible realization of the q D 3 states POTTS model was discussed by M.
KARDAR and A.N. BERKER [13]. They studied the over saturated adsorption of
Krypton atoms on a graphite surface. A detailed analysis of this system revealed
that three energetically degenerate sublattices are formed. Furthermore, the authors
demonstrated that the thermodynamic properties of this system can be explained by
a q D 3 states POTTS model. For a more detailed discussion we refer to the original
paper. More applications of the POTTS model were discussed in the review by F.Y.
WU [14].</p>
<p/>
</div>
<div class="page"><p/>
<p>306 18 MARKOV-Chain Monte Carlo and the POTTS Model
</p>
<p>Fig. 18.7 (a) Histogram of 104 measurements of the absolute value of the mean free energy per
particle jh"ij for a q-states POTTS model on a 40 � 40 square lattice at temperature kBT D 0:56
with q D 2. We observe one single peak which indicates that the system exhibits a second order
phase transition. (b) The same as (a) but for q D 8 and temperature kBT D 0:37. We observe two
well separated peaks, thus the system exhibits a first order phase transition
</p>
<p>The attentive reader may have noticed that our results do not carry error-bars.
We neglected error-bars for a clearer illustration. A short discussion of methods
used to calculate numerical errors was presented in Sect. 15.2 for the ISING model
and they can easily be adapted for the POTTS model. More advanced techniques will
be introduced in the next chapter.
</p>
<p>18.4 Advanced Algorithms for the POTTS Model
</p>
<p>We discuss briefly some advanced techniques for the POTTS model. Although these
algorithms are applicable for arbitrary q we restrict our discussion to the case q D 2,
the ISING model, for reasons of simplicity. Let us briefly motivate the need for more
advancedmethods: For large values of N we observe the formation of spin domains3
</p>
<p>for temperatures T � Tc. In such a case the specific METROPOLIS algorithm used
so far is disadvantageous because single spin flips will only affect the boundaries of
these domains (critical slowing down). It is therefore necessary to perform many
sweeps in order to produce configurations which are entirely different. It might
</p>
<p>3This are regions in which all spins point in the same direction, the so-called WEISS domains [15].</p>
<p/>
</div>
<div class="page"><p/>
<p>18.4 Advanced Algorithms for the POTTS Model 307
</p>
<p>Fig. 18.8 Schematic illustration of the identification of clusters according to the SWENDSEN-
WANG algorithm. 1 and 2 denote two different spin orientations, bonds are denoted by solid lines
and all bonded spins form clusters
</p>
<p>therefore be a better approach to flip a whole spin cluster at once. Such algorithms
are referred to as cluster algorithms. The main problem is the identification of
clusters as well as the assignment of a probability to the flip of a particular cluster.
</p>
<p>As a first example we shall discuss the SWENDSEN-WANG algorithm [16]. The
algorithm is executed in the following steps:
</p>
<p>1. Identify all links between two neighboring identical spins.
2. Define a bond between two linked spins with probability
</p>
<p>P D 1 � exp .�2ˇJ/ ; (18.19)
</p>
<p>with ˇ D 1=.kBT/.
3. Identify all clusters which are built from spins connected by bonds, see Fig. 18.8.
4. Flip every cluster with probability 1=2.
5. Delete the bonds and restart the iteration for the next spin configuration.
</p>
<p>We note the following properties of the SWENDSEN-WANG algorithm:
</p>
<p>&bull; The algorithm is ergodic because every spin forms a cluster on its own with a
non-vanishing probability according to Eq. (18.19).
</p>
<p>&bull; The algorithm fulfills detailed balance for the BOLTZMANN distribution and thus
reproduces the correct stationary distribution.
</p>
<p>Since the algorithm breaks domain walls or flips whole clusters, this algorithm
can be regarded to be very efficient from a numerical point of view. However, it
outperforms the single spin METROPOLIS algorithm only for temperatures near the
critical temperature because only then spin domains dominate the observables.
</p>
<p>A simpler version of this algorithm consists of the following four steps:
</p>
<p>1. Randomly pick a lattice site.
2. Find all neighbors with the same spin and form bonds with probability (18.19).
3. Move to the boundary of the cluster and repeat step 2, i.e. the cluster grows.
4. If no new bond is formed, flip the cluster with probability 1.
</p>
<p>In this simplified version the identification of clusters is not necessary because each
cluster is built dynamically during the simulation. Such a formulation of a cluster</p>
<p/>
</div>
<div class="page"><p/>
<p>308 18 MARKOV-Chain Monte Carlo and the POTTS Model
</p>
<p>algorithm is the WOLFF algorithm [17] which is essentially a generalization of the
work of SWENDSEN and WANG [16]. But why is it allowed to accept every step, i.e.
flip every formed cluster, without contradicting the condition of detailed balance?
The explanation is found in the definition of the probability of bond-formation,
Eq. (18.19). An even more effective extension of the WOLFF algorithm to quantum
systems is the loop algorithm [18]. For a more detailed discussion of all these
methods we refer the interested reader to the literature [19] and to the particular
papers cited here.
</p>
<p>Before proceeding to the next chapter, let us briefly mention that there is also
an entirely different approach to improve the METROPOLIS algorithm for quantum
systems, the so called worm algorithms [18]. However, a detailed discussion of such
algorithms is beyond the scope of this book.
</p>
<p>Summary
</p>
<p>The dominant topic of this chapter was importance sampling, a method to improve
Monte Carlo methods by reducing the variance. In this method some hard to sample
pdf was approximated as closely as possible by another, easy to sample pdf and one
concentrated on intervals which particularly matter for an as accurate as possible
estimate of, for instance, an expectation value of some property f .x/. In this sense
MARKOV-chain Monte Carlo techniques corresponded to importance sampling as
long as detailed balance was obeyed. In this particular case the MARKOV-chain
was known to approach the equilibrium distribution which must not necessarily
be known in detail. The METROPOLIS algorithm with its symmetric acceptance
probability was one possible realization of MARKOV-chains which obeyed detailed
balance. Another method was the METROPOLIS-HASTINGS algorithm with its
asymmetric acceptance probability. It also obeyed detailed balance and improved
the variance over the &lsquo;classical&rsquo; METROPOLIS algorithm. The second part of this
chapter was dedicated to the simulation of the q-state POTTS model, an extension of
the ISING model. The POTTS model had the feature that it developed a second order
phase transition for q � 4 and a first order phase transition for q &gt; 4. Moreover, the
transition temperature was q-dependent. The numerical simulation of the physics of
this model proved to be able to pick up on all these particular features. Finally, some
advanced algorithms developed for a more precise handling of various properties of
spin-models, particularly around the phase transition, have been presented without
going into great detail.</p>
<p/>
</div>
<div class="page"><p/>
<p>References 309
</p>
<p>Problems
</p>
<p>1. Modify the program designed to solve the ISING model (see Problems in
Chap. 15) in such a way that the physics of the q-states POTTS model can
be simulated for arbitrary values of q. Try to reproduce the figures presented
within this chapter. In order to investigate the order of the phase transition, plot
the internal energy per particle h"i for T � Tc in a histogram for different
measurements.
</p>
<p>The critical temperatures listed in Table 18.1 for q D 2; 3; : : : ; 8 can be used
to validate your code.
</p>
<p>2. Include a non-zero external field h and study its influence on the physics of the
q-states POTTS model for different values of q.
</p>
<p>References
</p>
<p>1. Norris, J.R.: Markov Chains. Cambridge Series in Statistical and Probabilistic. Cambridge
University Press, Cambridge (1998)
</p>
<p>2. Kendall, W.S., Liang, F., Wang, J.S.: Markov Chain Monte Carlo: Innovations and Applica-
tions. Lecture Notes Series, vol. 7. Institute for Mathematical Sciences, National University of
Singapore. World Scientific, Singapore (2005)
</p>
<p>3. Modica, G., Poggiolini, L.: A First Course in Probability and Markov Chains. Wiley, New York
(2012)
</p>
<p>4. Graham, C.: Markov Chains: Analytic and Monte Carlo Computations. Wiley, New York
(2014)
</p>
<p>5. Potts, R.B.: Some generalized order-disorder transformations. Math. Proc. Camb. Philos. Soc.
48, 106&ndash;109 (1952). doi:10.1017/S0305004100027419
</p>
<p>6. Doucet, A., de Freitas, N., Gordon, N. (eds.): Sequential Monte Carlo Methods in Practice.
Information Science and Statistics. Springer, Berlin/Heidelberg (2001)
</p>
<p>7. Press, W.H., Teukolsky, S.A., Vetterling, W.T., Flannery, B.P.: Numerical Recipes in C++, 2nd
edn. Cambridge University Press, Cambridge (2002)
</p>
<p>8. Kalos, M.H., Whitlock, P.A.: Monte Carlo Methods, 2nd edn. Wiley, New York (2008)
9. von der Linden, W., Dose, V., von Toussaint, U.: Bayesian Probability Theory. Cambridge
</p>
<p>University Press, Cambridge (2014)
10. Berg, B.A.: Markov Chain Monte Carlo Simulations and Their Statistical Analysis. World
</p>
<p>Scientific, Singapore (2004)
11. German, S.: Stochastic relaxation, gibbs distributions, and the bayesian restoration of images.
</p>
<p>IEEE Trans. Pattern Anal. Mach. Intell. 6, 721&ndash;741 (1984). doi:10.1109/TPAMI.1984.4767596
12. Neal, R.M.: Slice sampling. Ann. Stat. 31, 705&ndash;767 (2003). doi:10.1214/aos/1056562461
13. Kardar, M., Berker, A.N.: Commensurate-incommensurate phase diagrams for
</p>
<p>overlayers from a helical potts model. Phys. Rev. Lett. 48, 1552&ndash;1555 (1982).
doi:10.1103/PhysRevLett.48.1552
</p>
<p>14. Wu, F.Y.: The potts model. Rev. Mod. Phys. 54, 235&ndash;268 (1982).
doi:10.1103/RevModPhys.54.235
</p>
<p>15. White, R.M.: Quantum Theory of Magnetism, 3rd edn. Springer Series in Solid-State Sciences.
Springer, Berlin/Heidelberg (2007)
</p>
<p>16. Swendsen, R.H., Wang, J.S.: Nonuniversal critical dynamics in monte carlo simulations. Phys.
Rev. Lett. 58, 86&ndash;88 (1987). doi:10.1103/PhysRevLett.58.86</p>
<p/>
</div>
<div class="page"><p/>
<p>310 18 MARKOV-Chain Monte Carlo and the POTTS Model
</p>
<p>17. Wolff, U.: Collective monte carlo updating for spin systems. Phys. Rev. Lett. 62, 361&ndash;364
(1989). doi:10.1103/PhysRevLett.62.361
</p>
<p>18. Evertz, H.G.: The loop algorithm. Adv. Phys. 52, 1&ndash;66 (2003).
doi:10.1080/0001873021000049195
</p>
<p>19. Newman, M.E.J., Barkema, G.T.:Monte Carlo Methods in Statistical Physics. Clarendon Press,
Oxford (1999)</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 19
Data Analysis
</p>
<p>19.1 Introduction
</p>
<p>It is the aim of this chapter to present some of the most important techniques of
statistical data analysis which is of interest for experimental as well as theoretical
sciences. In particular, the superstition that numerically generated data sets do
not need to be analyzed with statistical methods is certainly not justified if the
data was generated by Monte Carlo methods. Some simple methods of statistical
analysis have already been discussed in previous chapters. For instance, in Chap. 12
we discussed simple quality tests for random number generators, in Chap. 15 we
calculated the errors associated with the observables of the ISING model. Here, these
simple methods will be summarized and some more advanced techniques will be
introduced on a basic level. For a more advanced discussion of this topic we refer
the interested reader to Refs. [1&ndash;5].
</p>
<p>19.2 Calculation of Errors
</p>
<p>We repeat briefly the basics of simple estimators which we made use of previously.
We approximate the expectation value hxi of some variable x
</p>
<p>hxi D
Z
</p>
<p>dx xp.x/ ; (19.1)
</p>
<p>where p.x/ is a pdf, by its arithmetic mean
</p>
<p>hxi � x D 1
N
</p>
<p>NX
</p>
<p>iD1
xi ; (19.2)
</p>
<p>&copy; Springer International Publishing Switzerland 2016
B.A. Stickler, E. Schachinger, Basic Concepts in Computational Physics,
DOI 10.1007/978-3-319-27265-8_19
</p>
<p>311</p>
<p/>
</div>
<div class="page"><p/>
<p>312 19 Data Analysis
</p>
<p>where the numbers xi follow the distribution p.x/. It is of conceptual importance
to distinguish between the expectation value hxi which is a c-number, while the
estimator x is a random number fluctuating around hxi. The error of approximating
hxi by x can be estimated by calculating the variance
</p>
<p>var .x/ D var .x/
N
</p>
<p>D
˝
</p>
<p>x2
˛
</p>
<p>� hxi2
</p>
<p>N
; (19.3)
</p>
<p>if the random numbers xi are uncorrelated (see Appendix E). In case of correlated
data the treatment becomes more involved and this will be discussed in Sect. 19.3.
The expectation values
</p>
<p>˝
</p>
<p>x2
˛
</p>
<p>and hxi in Eq. (19.3) may again be replaced by the
corresponding estimators x2 and x in order to obtain a reasonable estimate of the
variance var .x/. In particular, we approximate
</p>
<p>˝
</p>
<p>x2
˛
</p>
<p>� x2 D 1
N
</p>
<p>N
X
</p>
<p>iD1
x2i : (19.4)
</p>
<p>This approximation has already been applied in our investigation of the ISING
model, Chap. 15. When dealing with MARKOV-chain Monte Carlo simulations, the
result (19.3) can be interpreted in a rather trivial way: Repeating the simulation
under identical conditions results in roughly 68% of all simulations to yield a mean
value x 2 Œx � �x; x C �x&#141;, where �x D
</p>
<p>p
</p>
<p>var .x/ is the standard error.
We consider now the, in the meanwhile, quite familiar situation in which the
</p>
<p>underlying pdf p.x/ of a sequence of random numbers fxig is unknown. In such a
case one cannot simply use a particular estimator without some knowledge of the
particular form of p.x/. A common way to proceed is the poor person&rsquo;s assumption:
The underlying distribution is symmetric. This assumption has its origin in the
central limit theorem (see Appendix, Sect. E.8). However, some intuitive checks
may be required if fatal misconceptions are to be avoided. Is the data set reasonably
large one can retrieve essential information from collecting the data points in form
of a histogram or, if the index i refers to time instances, by plotting a time sequence.
</p>
<p>We can deduce a first idea about the form of the underlying pdf from a histogram.
For instance, if the data set displays only one peak, as in Fig. 19.1, quantities like
the mean or the variance could be useful. But if there are two (or more) separate
peaks, as in Fig. 19.2, it does not necessarily make sense to calculate the mean or
variance by summing over all the data points. Such a situation can, for instance,
occur in statistical spin models, with two phases, as we observed it in the q-state
POTTS model, Fig. 18.7a, b.
</p>
<p>Time series, in which the data points xi are plotted as a function of discrete
time instances ti, can also reveal important information about the properties of the
data set. For instance, systematic trends, outliers, or hints for correlations may be
observed.</p>
<p/>
</div>
<div class="page"><p/>
<p>19.2 Calculation of Errors 313
</p>
<p>Fig. 19.1 Histogram generated by random sampling of a Gaussian of mean zero and variance one
</p>
<p>Fig. 19.2 Histogram generated by random sampling of two Gaussians of mean zero and variance
one, displaced by C3 and �3, respectively
</p>
<p>Let us turn our attention to some more advanced estimator techniques. So far
we discussed the sample mean and sample variance as candidates for unbiased
estimators.1 In a more general context the calculation of observables from data sets
might be more complex. In the following we assume a data set of N data points
.x1; x2; : : : ; xN/. Basically, we would like to estimate a quantity of the form f .hxi/
</p>
<p>1Since mean and variance are calculated from the same data points, they are usually not unbiased.
Therefore a common choice is the so called bias corrected variance var .x/B which is given by
var .x/B D NN�1var .x/ where N is the number of data points. A more detailed discussion can be
found in any textbook on statistics [6&ndash;9].</p>
<p/>
</div>
<div class="page"><p/>
<p>314 19 Data Analysis
</p>
<p>where f is some particular function (for instance hxi2). A bad (biased) estimate
would be to calculate
</p>
<p>f D 1
N
</p>
<p>X
</p>
<p>f .xi/ ; (19.5)
</p>
<p>which is definitely not the quantity we are interested in because for N ! 1 we
have f ! h f i and not f .hxi/. A better estimate would be to calculate
</p>
<p>f .x/ D f
�
1
</p>
<p>N
</p>
<p>X
</p>
<p>xi
</p>
<p>�
</p>
<p>; (19.6)
</p>
<p>which converges to f .hxi/ for N ! 1. We discuss here two different methods to
calculate the error attached to f .x/, namely the Jackknife method and the statistical
bootstrap method.
</p>
<p>We define Jackknife averages
</p>
<p>xJi D
1
</p>
<p>N � 1
X
</p>
<p>j&curren;i
xj ; (19.7)
</p>
<p>and xJi is the average of all values xj &curren; xi. Moreover, we define
</p>
<p>f Ji � f .xJi / ; (19.8)
</p>
<p>and this opens the possibility to estimate f .hxi/ following
</p>
<p>f .hxi/ � f J D 1
N
</p>
<p>X
</p>
<p>i
</p>
<p>f Ji ; (19.9)
</p>
<p>with the statistical error
</p>
<p>�2
f
</p>
<p>J D .N � 1/
h
</p>
<p>. f J/2 � .f J/2
i
</p>
<p>; (19.10)
</p>
<p>which can be written as
</p>
<p>�2
f
</p>
<p>J D
N � 1
</p>
<p>N
</p>
<p>X
</p>
<p>i
</p>
<p>. f Ji � f
J
/2 ; (19.11)
</p>
<p>for uncorrelated f Ji (see Appendix E).
In the case of the statistical bootstrap we consider again a set of N data-
</p>
<p>points fxig. We randomly choose N elements from this data set without removal
which constitutes the set fx.i/j g and calculate for these N points the observable
fi D f .1=N
</p>
<p>P
</p>
<p>j x
.i/
j /. This procedure is repeated M-times and we get
</p>
<p>f .hxi/ � f BS D
1
</p>
<p>M
</p>
<p>X
</p>
<p>i
</p>
<p>fi ; (19.12)</p>
<p/>
</div>
<div class="page"><p/>
<p>19.3 Auto-Correlations 315
</p>
<p>and
</p>
<p>�2
f BS
</p>
<p>D 1
M
</p>
<p>X
</p>
<p>i
</p>
<p>�
</p>
<p>fi � f BS
�2
: (19.13)
</p>
<p>This method was applied in Chap. 15 to determine estimates for the error-bars
of the various observables as a function of temperature in Fig. 15.6. The methods
discussed here can, of course, also be employed to derive estimates for the errors
attached to the various observables studied in the POTTS model, Chap. 18.
</p>
<p>Let us close this section with a short comment on systematic errors. As already
highlighted within Chap. 1 one also has to be aware of possible systematic errors.
Like in experimental data, these errors are more easily overlooked in numerical data
since they are rather hard to identify. In general, there is no method available to
investigate systematic errors. For instance, in the simulation of the ISING model, the
main source of errors was that the MARKOV-chain was not allowed to completely
equilibrate which would have been equivalent to running the simulation forever.
The introduction of the concept of an auto-correlation time will, at least, allow for a
systematic investigation of this fundamental problem.
</p>
<p>19.3 Auto-Correlations
</p>
<p>The situation becomes more involved whenever the random numbers of the
sequence fxig are correlated, i.e. cov
</p>
<p>�
</p>
<p>xi; xj
�
</p>
<p>&curren; 0 for i &curren; j [see Appendix,
Eq. (E.16)], where the elements of the series fxig are successive members of a
time series. Hence, existing covariances between elements xi and xj account for
auto-correlations of a certain observable between different time steps. We rewrite
Eq. (19.3):
</p>
<p>var .x/ D
D
</p>
<p>x2
E
</p>
<p>� hxi2
</p>
<p>D 1
N2
</p>
<p>N
X
</p>
<p>i;jD1
</p>
<p>˝
</p>
<p>xixj
˛
</p>
<p>� 1
N2
</p>
<p>N
X
</p>
<p>i;jD1
hxii
</p>
<p>˝
</p>
<p>xj
˛
</p>
<p>D 1
N2
</p>
<p>N
X
</p>
<p>iD1
</p>
<p>�˝
</p>
<p>x2i
˛
</p>
<p>� hxii2
�
</p>
<p>C 1
N2
</p>
<p>X
</p>
<p>i&curren;j
</p>
<p>�˝
</p>
<p>xixj
˛
</p>
<p>� hxii
˝
</p>
<p>xj
˛�
</p>
<p>: (19.14)</p>
<p/>
</div>
<div class="page"><p/>
<p>316 19 Data Analysis
</p>
<p>The first term on the right-hand side of Eq. (19.14) is identified as var .xi/ =N which
is assumed to be identical for all i, i.e. var .xi/ � var .x/. Furthermore, we rewrite
the sum
</p>
<p>X
</p>
<p>i&curren;j
� D 2
</p>
<p>N
X
</p>
<p>iD1
</p>
<p>N
X
</p>
<p>jDiC1
� ;
</p>
<p>and obtain
</p>
<p>var .x/ D 1
N
</p>
<p>2
</p>
<p>4var .x/C 2
N
</p>
<p>N
X
</p>
<p>iD1
</p>
<p>N
X
</p>
<p>jDiC1
cov
</p>
<p>�
</p>
<p>xi; xj
�
</p>
<p>3
</p>
<p>5 : (19.15)
</p>
<p>Let us assume time translational invariance:
</p>
<p>cov
�
</p>
<p>xi; xj
�
</p>
<p>� C. j � i/ ; for j &gt; i: (19.16)
</p>
<p>We apply this relation to Eq. (19.15) and obtain
</p>
<p>var .x/ D 1
N
</p>
<p>2
</p>
<p>4var .x/C 2
N
</p>
<p>N
X
</p>
<p>iD1
</p>
<p>N
X
</p>
<p>jDiC1
C. j � i/
</p>
<p>3
</p>
<p>5
</p>
<p>D 1
N
</p>
<p>"
</p>
<p>var .x/C 2
N
</p>
<p>NX
</p>
<p>kD1
C.k/ .N � k/
</p>
<p>#
</p>
<p>D 1
N
</p>
<p>"
</p>
<p>var .x/C 2
N
X
</p>
<p>kD1
C.k/
</p>
<p>�
</p>
<p>1 � k
N
</p>
<p>�
#
</p>
<p>; (19.17)
</p>
<p>which can be reformulated as:
</p>
<p>var .x/ D 2var .x/ O�
i
x
</p>
<p>N
: (19.18)
</p>
<p>We introduced here the (proper) integrated auto-correlation time O� ix
</p>
<p>O� ix D
1
</p>
<p>2
C
</p>
<p>NX
</p>
<p>kD1
A.k/
</p>
<p>�
</p>
<p>1 � k
N
</p>
<p>�
</p>
<p>; (19.19)
</p>
<p>and the normalized auto-correlation function
</p>
<p>A.k/ D C.k/
C.0/
</p>
<p>D cov .xi; xiCk/
var .xi/
</p>
<p>: (19.20)</p>
<p/>
</div>
<div class="page"><p/>
<p>19.3 Auto-Correlations 317
</p>
<p>In most cases we are interested in the limit N ! 1 of Eq. (19.19):
</p>
<p>� ix D lim
N!1
</p>
<p>O� ix D
1
</p>
<p>2
C
</p>
<p>1
X
</p>
<p>kD1
A.k/ : (19.21)
</p>
<p>The form of the auto-correlation function A.x/ can be approximated using the
results of Sect. 16.4. There, we observed that the stationary distribution � was the
left-eigenvector of the transition matrix P with eigenvalue 1, Eq. (16.73). Let f'`g
denote the set of all left-eigenvectors of the matrix P with eigenvalues �`, i.e. '`P D
�`'`.2 Then some arbitrary state q.0/ can be expressed in this basis as:
</p>
<p>q.0/ D
X
</p>
<p>i
</p>
<p>˛i'i : (19.22)
</p>
<p>After n consecutive time-steps we arrive at state q.n/
</p>
<p>q.n/ D q.0/Pn D
X
</p>
<p>i
</p>
<p>˛i'iP
n D
</p>
<p>X
</p>
<p>i
</p>
<p>˛i�
n
i 'i ; (19.23)
</p>
<p>which follows from Eq. (16.62). We denote the observable we want to calculate by
O.n/ and expand it according to Ref. [10]
</p>
<p>O.n/ D
X
</p>
<p>i
</p>
<p>Œq.n/&#141;ioi D
X
</p>
<p>i
</p>
<p>˛i�
n
i oi ; (19.24)
</p>
<p>where oi stands for the expectation value of O in the i-th eigenstate 'i. For large n
the value of O.n/ will be dominated by the largest eigenvalue of P, say �0, and we
denote this value by O.1/ D ˛0o0. This allows us to rewrite Eq. (19.24) as
</p>
<p>O.n/ D O.1/C
X
</p>
<p>i&curren;0
˛ioi�
</p>
<p>n
i : (19.25)
</p>
<p>Let �1 2 R be the second largest eigenvalue and let us define the exponential auto-
correlation time � ex via
</p>
<p>� ex D �
1
</p>
<p>log.�1/
; (19.26)
</p>
<p>2Note that since P is a stochastic matrix, it follows that j�`j � 1 for all `. Furthermore, it can be
shown that the largest eigenvalue of a stochastic matrix is equal to 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>318 19 Data Analysis
</p>
<p>and the value of O.n/ can, for large values of n, be approximated by
</p>
<p>O.n/ � O.1/C ˇ exp
�
</p>
<p>� n
� ex
</p>
<p>�
</p>
<p>; (19.27)
</p>
<p>where ˇ is some constant. Hence, the auto-correlation obeys
</p>
<p>C.n/ / ŒO.0/ � O.1/&#141; ŒO.n/ � O.1/&#141; / ˇ exp
�
</p>
<p>� n
� ex
</p>
<p>�
</p>
<p>; (19.28)
</p>
<p>and we can simply set for the auto-correlation function A.k/
</p>
<p>A.k/ D &#13; exp
�
</p>
<p>� k
� ex
</p>
<p>�
</p>
<p>; (19.29)
</p>
<p>where &#13; is some constant. We use this result in the expression for the integrated
auto-correlation time (19.21) and arrive at:
</p>
<p>� ix D
1
</p>
<p>2
C &#13;
</p>
<p>1
X
</p>
<p>kD1
</p>
<p>�
</p>
<p>exp
</p>
<p>�
</p>
<p>� 1
� ex
</p>
<p>��k
</p>
<p>D 1
2
C &#13;
</p>
<p>exp
�
</p>
<p>� 1
�ex
</p>
<p>�
</p>
<p>1 � exp
�
</p>
<p>� 1
�ex
</p>
<p>� : (19.30)
</p>
<p>For � ex � 1 the exponential function can be expanded into a TAYLOR series.
Keeping terms up to first order results in:
</p>
<p>� ix D
1
</p>
<p>2
C &#13;
</p>
<p>1 � 1
�ex
</p>
<p>1
�ex
</p>
<p>D 1
2
C &#13;
</p>
<p>�
</p>
<p>� ex � 1
�
</p>
<p>/ &#13;� ex : (19.31)
</p>
<p>However, we note that in general relation (19.31) is only a poor approximation
because usually the exponential auto-correlation time is very different from the
integrated auto-correlation time.
</p>
<p>Let us briefly discuss our results. A comparison between Eqs. (19.3) and (19.18)
reveals that due to correlations in the time series, the number of effective (or useful)
data points Neff can be determined from
</p>
<p>Neff D
N
</p>
<p>2� ix
: (19.32)
</p>
<p>In the limit � ex ! 0 we obtain � ix D 1=2 and, thus, recover Eq. (19.3). The effective
number of measurements is the relevant quantity whenever the error of a Monte
Carlo integration is calculated.</p>
<p/>
</div>
<div class="page"><p/>
<p>19.4 The Histogram Technique 319
</p>
<p>In another approach, one can determine the exponential auto-correlation time � ex
and use it to estimate the number of steps that should be neglected between two
successive measurements. This can be achieved by fitting the auto-correlation A.k/
with an exponential function. (A brief introduction to least squares fits can be found
in Appendix H.) We note that in one and the same system the auto-correlation times
may be very different for different observables.
</p>
<p>19.4 The Histogram Technique
</p>
<p>The histogram technique is a method which allows to approximate the expectation
value of some observable for temperatures near a given temperature T0 without
performing further MARKOV-chain Monte Carlo simulations. The basic idea is
easily sketched. Suppose the observable O is solely a function of energy E. We
perform a MARKOV-chain Monte Carlo simulation for a given temperature T0 and
measure the energy E several times. The resulting measurements are sorted in a
histogram with bin width 
E as was demonstrated in Sect. 18.3. If n.E/ denotes
the number of configurations measured within the interval .E;E C 
E/, then the
probability that some energy is measured to lay within the interval .E;E C 
E/ is
given by
</p>
<p>PH.E;T0/ D
n.E/
</p>
<p>M
; (19.33)
</p>
<p>where the index H refers to histogram and M D
P
</p>
<p>E n.E/ is the number of
measurements. However, we note that this probability can also be expressed by the
BOLTZMANN distribution
</p>
<p>P.E;T/ D
N.E/ exp
</p>
<p>�
</p>
<p>� E
kBT
</p>
<p>�
</p>
<p>P
</p>
<p>E N.E/ exp
�
</p>
<p>� E
kBT
</p>
<p>� ; (19.34)
</p>
<p>where N.E/ denotes the number of micro-states within the interval .E;E C 
E/.
N.E/ is independent of the temperature T and relation (19.34) is valid for all
temperatures T. In particular, for T D T0
</p>
<p>PH.E;T0/ D P.E;T0/ ; (19.35)
</p>
<p>which immediately yields
</p>
<p>N.E/ D ˛n.E/ exp
�
</p>
<p>E
</p>
<p>kBT0
</p>
<p>�
</p>
<p>; (19.36)</p>
<p/>
</div>
<div class="page"><p/>
<p>320 19 Data Analysis
</p>
<p>where ˛ is some constant and we emphasize that n.E/ was measured at T0. Inserting
Eq. (19.36) into (19.34) yields
</p>
<p>P.E;T/ D
n.E/ exp
</p>
<p>h
</p>
<p>�
�
</p>
<p>1
kBT
</p>
<p>� 1
kBT0
</p>
<p>�
</p>
<p>E
i
</p>
<p>P
</p>
<p>E n.E/ exp
h
</p>
<p>�
�
</p>
<p>1
kBT
</p>
<p>� 1
kBT0
</p>
<p>�
</p>
<p>E
i ; (19.37)
</p>
<p>for arbitrary T. The expectation value hOiT of the observable O at some temperature
T can now be determined from
</p>
<p>hOiT D
X
</p>
<p>E
</p>
<p>O.E/P.E;T/
</p>
<p>D
P
</p>
<p>E O.E/n.E/ exp
h
</p>
<p>�
�
</p>
<p>1
kBT
</p>
<p>� 1
kBT0
</p>
<p>�
</p>
<p>E
i
</p>
<p>P
</p>
<p>E n.E/ exp
h
</p>
<p>�
�
</p>
<p>1
kBT
</p>
<p>� 1
kBT0
</p>
<p>�
</p>
<p>E
i : (19.38)
</p>
<p>This result implies, that it is not necessary to run an additional MARKOV-chain
Monte Carlo simulation in an attempt to compute the expectation value hOiT for
temperature T if T is in the vicinity of T0. However, if T deviates strongly from
T0, the above procedure (19.38) does not provide a good approximation because
the relevant configurations at T may have been very improbable at T0 and may,
therefore, not have been reproduced sufficiently often in the original MARKOV-
chain Monte Carlo simulation.
</p>
<p>Summary
</p>
<p>Data analysis is an important but often neglected part of natural sciences and
in particular of numerical simulations. It consists mainly of consistency checks
and error analysis. This chapter concentrated in a first step on error analysis. It
discussed the most common methods to arrive at an estimate of the error involved
whenever expectation values of some property are analyzed. These went beyond
all those methods which have already been discussed in some detail throughout
this book. In a second step auto-correlations have been discussed. They should be
part of consistency checks and give valuable information about possible systematic
errors. The auto-correlation analysis was of particular importance whenever the
quality of the sequence of random numbers was crucial to a particular simulation.
(Experiments in which the events are expected to be random, like radioactive decay,
fall also into this category.) Nevertheless, this method proved to be very useful in
MARKOV-chain Monte Carlo simulations as it allowed to define and determine an
auto-correlation timewhich could serve as a measure of the number of sweeps which
have to be neglected between two consecutive measurements. Finally, the histogram
technique was introduced as a method of data interpolation. It allowed, in addition</p>
<p/>
</div>
<div class="page"><p/>
<p>References 321
</p>
<p>to applications which have already been presented within this book, to derive the
expectation value of some property at some &lsquo;temperature&rsquo; T from the already known
expectation value of this same property at some other temperature T0 if T � T0 and
if the equilibrium distribution was known.
</p>
<p>Problems
</p>
<p>1. Calculate the auto-correlation function for random numbers generated by the two
linear congruential generators discussed in Sect. 12.2. Check also the random
number generator provided by your system. Discuss the results.
</p>
<p>2. POTTS model: Calculate the error attached to the specific heat ch and the
susceptibility � using the Jackknife method for all values of q D 1; : : : ; 8. Plot
the corresponding diagrams and discuss the results. Determine the exponential
and integrated correlation time.
</p>
<p>References
</p>
<p>1. Gaul, W., Opitz, O., Schader, M. (eds.): Data Analysis. Springer, Berlin/Heidelberg (2000)
2. Sivia, D., Skilling, J.: Data Analysis, 2nd edn. Oxford University Press, Oxford (2006)
3. Ad&egrave;r, H.J., Mellenbergh, G.J., Hand, D.J. (eds.): Advising on Research Methods: A Consul-
</p>
<p>tant&rsquo;s Companion, chap. 14, 15. Johannes van Kessel, Huizen (2008)
4. Brandt, S.: Data Analysis. Springer, Berlin/Heidelberg (2014)
5. von der Linden, W., Dose, V., von Toussaint, U.: Bayesian Probability Theory. Cambridge
</p>
<p>University Press, Cambridge (2014)
6. Iversen, G.P., Gergen, I.: Statistics. Springer Undergraduate Textbooks in Statistics. Springer,
</p>
<p>Berlin/Heidelberg (1997)
7. Wilcox, R.R.: Basic Statistics. Oxford University Press, New York (2009)
8. Monahan, J.F.: Numerical Methods of Statistics. Cambridge Series in Statistical and Proba-
</p>
<p>bilistic Mathematics. Cambridge University Press, Cambridge (2011)
9. Wood, S.: Core Statistics. Institute of Mathematical Statistics Textbooks. Cambridge Univer-
</p>
<p>sity Press, Cambridge (2015)
10. Sokal, A.D.: Monte Carlo Methods in Statistical Mechanics: Foundations and NewAlgorithms.
</p>
<p>Department of Physics. New York University, New York (1996). www.stat.unc.edu/faculty/cji/
Sokal.pdf</p>
<p/>
<div class="annotation"><a href="www.stat.unc.edu/faculty/cji/Sokal.pdf">www.stat.unc.edu/faculty/cji/Sokal.pdf</a></div>
<div class="annotation"><a href="www.stat.unc.edu/faculty/cji/Sokal.pdf">www.stat.unc.edu/faculty/cji/Sokal.pdf</a></div>
</div>
<div class="page"><p/>
<p>Chapter 20
Stochastic Optimization
</p>
<p>20.1 Introduction
</p>
<p>Suppose x 2 S is some vector in an n-dimensional search space S and let H W
S ! R be a mapping from the search space S onto the real axis R. The function H
plays a particular role and is usually referred to as the cost function. A minimization
problem can be defined in a very compact form:
</p>
<p>Find x0 2 S, such that H.x0/ is the global minimum of the cost function H.
</p>
<p>In analogue, a maximization problem with cost function H defines a minimiza-
tion problem with cost function G D �H. The class of both problems is referred to
as the class of optimization problems [1&ndash;3] and only minimization problems will be
discussed here.
</p>
<p>The reader might be aware that there are numerous applications in physics and
related sciences. We list a few in order to remind ourselves of their fundamental
importance:
</p>
<p>&bull; The set of linear equationsAx D b is often regarded as the minimization problem:
H.x/ D kAx � bk2 which can be beneficial for high dimensional problems.
</p>
<p>&bull; The quantum mechanical ground state energy E0 is given by
</p>
<p>E0 D min
�
</p>
<p>h� jH j� i
h� j� i ; (20.1)
</p>
<p>where j� i denotes the wave function and H is the Hamiltonian of the system.
&bull; High dimensional and highly non-linear least squares fits. (More details can be
</p>
<p>found in Appendix H.)
&bull; The equilibrium crystal structure of solids is obtained by minimization of the free
</p>
<p>energy.
&bull; Protein folding is described by minimization of the forces in a molecular
</p>
<p>dynamics problem.
</p>
<p>&copy; Springer International Publishing Switzerland 2016
B.A. Stickler, E. Schachinger, Basic Concepts in Computational Physics,
DOI 10.1007/978-3-319-27265-8_20
</p>
<p>323</p>
<p/>
</div>
<div class="page"><p/>
<p>324 20 Stochastic Optimization
</p>
<p>Whenever the cost function is at least once differentiable, methods of deter-
ministic optimization can be applied [4]. (Two simple deterministic optimization
methods are presented in Appendix I.) On the other hand, if H is not differentiable
or too complex, due to a huge search space S or many local minima, methods of
stochastic optimization [5] can be employed. The term stochastic optimization is
used for methods which contain at least one step which is based on random number
generation. Let us briefly give some examples of problems for which deterministic
methods fail:
</p>
<p>&bull; The Traveling Salesperson Problem [6, 7]: A traveling salesperson has to visit L
cities in a tour as short as possible under the constraint that he/she has to return
to the starting point in the end. Each city has to be visited only once, hence the
cities have to be ordered in such a way that the travel length becomes a global
minimum. In particular, the cost function
</p>
<p>H.fig/ D
L
X
</p>
<p>`D1
jxi`C1 � xi` j ; (20.2)
</p>
<p>has to be minimized. Here fig denotes a certain configuration of cities and we set
iLC1 D i1. Obviously, we cannot calculate the first derivative of H with respect
to fig, set it zero, and solve the problem in the classical way. On the other hand,
a brute force approach of calculating H.fig/ for all possible arrangements fig is
not possible since we have LŠ different possible routes. Since for one particular
choice all L starting points and both travel directions yield the same result, we
have to calculate LŠ=.2L/ D .L � 1/Š=2 different configurations fig. We would
have about 10155 different choices for L D 100 cities! This clearly makes such
an approach intractable.
</p>
<p>&bull; The arrangement of timetables under certain constraints. In particular, the design
of timetables in schools, universities or at airports. This problem is also referred
to as the Nurse Scheduling Problem [8].
</p>
<p>&bull; The ISING spin glass [9]: In contrast to the classical ISING model, the ISING spin
glass is characterized by nearest neighbor interactions Jij which are, in the most
simple case, chosen to be Jij D C1 and Jij D �1 with the same probability.
In this case the ground state below the critical temperature is not simply given
by a configuration in which all spins point in the same direction. Of course, the
ground state configuration in such a case can be highly degenerate. The fact that
such a model can be simulated using MARKOV-chain Monte Carlo methods as
they have been discussed within Chaps. 15 and 18 gives us some idea of how one
may employ stochastic methods to solve optimization problems.
</p>
<p>&bull; The N-Queens Problem [10]: Place N queens on a N�N chessboard in such a way
that no two queens attack each other. In particular, this means that two queens are
not allowed to share the same row, the same column, and the same diagonal. It
can be shown that the problem possesses solutions for N � 4. One defines a
function H.fng/ which counts the number of attacks in a certain configuration</p>
<p/>
</div>
<div class="page"><p/>
<p>20.2 Hill Climbing 325
</p>
<p>fng. For instance, for N D 4, the configuration
</p>
<p>4
</p>
<p>3
</p>
<p>2
</p>
<p>1
</p>
<p>a b c d
</p>
<p>has H.fng/ D 2. On the contrary, the configuration
</p>
<p>4
</p>
<p>3
</p>
<p>2
</p>
<p>1
</p>
<p>a b c d
</p>
<p>solves the 4-queens problem and H.fng/ D 0.
We concentrate here on some of the most basic methods of stochastic optimiza-
</p>
<p>tion: the method of hill climbing, the method of simulated annealing, and genetic
algorithms. Ideas on which several more advanced techniques are based will be
sketched in Sect. 20.5.
</p>
<p>20.2 Hill Climbing
</p>
<p>The method of hill climbing [11] is probably one of the most simple methods
of stochastic optimization. Given a cost function H.x/, we execute the following
steps:
</p>
<p>1. Choose an initial position x0.
2. Randomly pick a new xn from the neighborhood of xn�1.
3. Keep xn if H.xn/ � H.xn�1/.
4. Terminate the search if no new xn can be found in the neighborhood of xn�1.</p>
<p/>
</div>
<div class="page"><p/>
<p>326 20 Stochastic Optimization
</p>
<p>We note that the algorithm requires a neighborhood relation. This relation
is to be defined for each particular problem. For instance, in the case of the
traveling salesperson problem it is by no means clear what a configuration in the
neighborhood of a certain route fig should mean. To elaborate on this problem we
concentrate here on two particular problems which help to demonstrate how such a
neighborhood relation can be defined.
</p>
<p>In the traveling salesperson problem or in the ISING spin glass model the
neighborhood of a route fig or of a configuration C can be defined as the set
of all routes fig in which two cities have been interchanged or as the set of all
configurations C in which one spin has been flipped.
</p>
<p>On the other hand, if the search space S D Rn we may define the neighborhood
as the number of points within an n-sphere of radius r centered at z � xn�1. It is
rather simple to sample points from an n-sphere centered at the origin by applying
the method of G. MARSAGLIA [12]: For an n-dimensional vector we sample all
components x1; : : : ; xn from the normal distribution N .0; 1/ with mean zero and
variance one. The points are then transformed according to
</p>
<p>xj ! x0j D
r
</p>
<p>kxkxj C zj ; (20.3)
</p>
<p>where kxk denotes the Euclidean norm of the vector x. The points given by
Eq. (20.3) lie on the surface of the n-sphere with radius r. In order to obtain
uniformly distributed random points within a sphere with radius r we draw a random
number u 2 Œ0; 1&#141; and calculate
</p>
<p>xj ! x0j D u
1
n xj ; (20.4)
</p>
<p>where the factor 1=n in the exponent of u ensures that the points are uniformly
distributed.
</p>
<p>Let us briefly summarize the most important properties of the method of hill
climbing:
</p>
<p>&bull; The way the algorithm is defined it will terminate in a local minimum and not
in the global minimum. A classical remedy is the restart of the algorithm from
various different initial positions. Information gathered from previous runs can
help to make a good choice for the initial positions of restarts.
</p>
<p>&bull; It depends highly on the choice of initial conditions if and how the global
minimum is found. This situation is very similar to the application of determin-
istic methods of optimization (see Appendix I). Sometimes it may even be of
advantage to accept points which result in a slight increase of the cost function&rsquo;s
value just to escape a local minimum.
</p>
<p>&bull; For most problems this method is very expensive from a computational point of
view.
</p>
<p>We apply the method of hill climbing to the N-queens problem for N D 8. The
algorithm is executed in the following way: In the initial configuration the queens</p>
<p/>
</div>
<div class="page"><p/>
<p>20.3 Simulated Annealing 327
</p>
<p>are set randomly on the chessboard and we place only one queen in each row and
column. It is then checked whether or not two queens attack each other. If they do,
a new configuration is generated by picking two queens at random and by changing
their respective positions. This is repeated until a configuration arises in which none
of the queens attacks another. Such an algorithm resembles a random walk in a
parameter space which spans all possible configurations under the constraint that
only one queen is placed in each row and column. The iteration is terminated as soon
as no queen is attacked by any other queen. It is rather obvious that this strategy is
not very fast, however, one possible solution to the problem for N D 8 can easily be
found within a few iteration steps:
</p>
<p>However, for large values of N hill climbing is definitely not a recommendable
method to solve the N-queens problem.
</p>
<p>20.3 Simulated Annealing
</p>
<p>Let us turn our attention to simulated annealing [7, 13, 14]. The name of this
algorithm stems from the annealing process in metallurgy in which a metal is
first heated and then slowly cooled in order to reduce the amount of defects in
the material. The reasoning behind this method can quite easily be reconstructed
with the help of the ISING model which we discussed in detail in Chap. 15. There
we learned from thermodynamics that the equilibrium distribution of possible
configurations P.C ;T/ at a certain temperature T is a BOLTZMANN distribution
</p>
<p>P.C ;T/ D 1
Z
exp
</p>
<p>�
</p>
<p>�H.C /
kBT
</p>
<p>�
</p>
<p>; (20.5)
</p>
<p>where H.C / is the HAMILTON function of the system. In particular, we expect that
the system is in its ground state (let us assume a non-degenerate ground-state for</p>
<p/>
</div>
<div class="page"><p/>
<p>328 20 Stochastic Optimization
</p>
<p>the time being) with probability one in the limit T ! 0, provided that we cooled
sufficiently slowly so that the system had enough time to equilibrate. This can be
used to solve the optimization problem: We take the cost function H.x/ and define
the probability for the realization of a particular state (configuration) in the search
space x0 2 S by
</p>
<p>P.x0;T/ D
1
</p>
<p>Z
exp
</p>
<p>�
</p>
<p>�H.x0/
T
</p>
<p>�
</p>
<p>; (20.6)
</p>
<p>where T is some external parameter, which we refer to as temperature for reasons
of convenience, and Z denotes the normalization constant:
</p>
<p>Z D
P
Z
</p>
<p>x2S
dx exp
</p>
<p>�
</p>
<p>�H.x/
T
</p>
<p>�
</p>
<p>: (20.7)
</p>
<p>We start the procedure at some finite initial temperature T0 &curren; 0 and construct
a MARKOV-chain of states fxng which converges towards the distribution (20.6).
We choose, of course, a sampling technique which does not require the explicit
knowledge of the normalization Z, such as the METROPOLIS-HASTINGS algorithm
of Sect. 18.2. As soon as the MARKOV-chain reaches its stationary distribution for a
given temperature T, we slightly decrease the temperature and restart the MARKOV-
chain with the last state of the previous temperature. By slowly cooling the search
MARKOV-chain, we exclude unimportant parts of the search space by decreasing
their acceptance probability. Nevertheless, the chain is given enough time to explore
the whole remaining search space at each temperature. This procedure is commonly
referred to as the classical version of simulated annealing.
</p>
<p>It is of advantage to start with an initial temperature which allows to cover the
largest part of possible states in the search space S. Thus, the acceptance probability
for a new state in the MARKOV-chain is almost equal to one for all x 2 S. If this
were not the case, some regions of the search space might be excluded from our
search routine right away due to an unlucky choice of the initial configuration. In
particular, the result might be a state in the neighborhood of the initial state of the
MARKOV-chain and it is, therefore, most likely a local minimum rather than the
global minimum.
</p>
<p>We note that the algorithm consists of the following essential ingredients: (i) a
proposal probability for new states x within the search space S, (ii) an acceptance
probability Pa.x ! x0/ for a proposed x0 from a previous state x, and (iii) a cooling
strategy T D T.t/, where t is time. Let us briefly elaborate on these points.
</p>
<p>(i) Proposal Probability
</p>
<p>The question of how to generate new states x from a previous state x0 within the
search space S has already been answered in the case of hill climbing, Sect. 20.2 by</p>
<p/>
</div>
<div class="page"><p/>
<p>20.3 Simulated Annealing 329
</p>
<p>defining the neighborhood of a state in search space. The corresponding proposal
probability will be denoted by Pp.x ! x0/.
</p>
<p>(ii) Acceptance of Probability
</p>
<p>The acceptance probability has to be chosen in such a way that the sequence of
generated states constitutes a MARKOV-chain which converges toward the distribu-
tion (20.6). Hence, detailed balance has to be imposed and the implications of this
requirement have been discussed extensively in Chap. 18. Note that the proposal
probability has to be included into the definition of the acceptance probability as
was outlined in Sect. 18.2.
</p>
<p>One particular choice of a METROPOLIS-HASTINGS acceptance probability
</p>
<p>Pa.x ! x0;T/ D min
�
</p>
<p>1;
P.x0;T/
</p>
<p>P.x;T/
</p>
<p>Pp.x
0 ! x/
</p>
<p>Pp.x ! x0/
</p>
<p>�
</p>
<p>; (20.8)
</p>
<p>appears to be quite natural for several reasons:
</p>
<p>&bull; It is very general and can, thus, also handle asymmetric proposal probabilities.
&bull; In the symmetric case Pp.x ! x0/ D Pp.x0 ! x/ and H.x0/ � H.x/ we get
</p>
<p>P.x0;T/
</p>
<p>P.x;T/
D exp
</p>
<p>�
1
</p>
<p>T
</p>
<p>�
</p>
<p>H.x/�H.x0/
�
�
</p>
<p>� 1 ; (20.9)
</p>
<p>according to our choice (20.6) and the state x0 is accepted with probability one.
On the other hand, for H.x0/ &gt; H.x/, x0 may still be accepted with some
finite probability Pa.x ! x0;T/ which offers an opportunity to escape a local
minimum.
</p>
<p>(iii) Cooling Strategy
</p>
<p>The design of a proper cooling strategy includes both, the choice of an appropriate
initial temperature T0 as well as the formulation of a mathematical rule which
defines TnC1 D f .Tn/ where TnC1 &lt; Tn.
</p>
<p>First of all we discuss the choice of the initial temperature. A common choice
is to choose it in such a way that at least 80% of all generated states are accepted.
The simplest procedure to determine this temperature starts with some arbitrary
value T0 &gt; 0 and generates N states. If the number of rejected states Nr is greater
than 0:2N, then the temperature T0 is doubled and the number of rejected states is
measured again.</p>
<p/>
</div>
<div class="page"><p/>
<p>330 20 Stochastic Optimization
</p>
<p>Another more sophisticated choice is based on the following idea: The best
choice would be T0 ! 1 because then the acceptance probability would be one for
all possible states independent ofH.x/. This corresponds to a randomwalk in search
space S and we calculate the mean value hHi1 and the variance var .H/1. Thus,
the function valuesH fluctuate between ŒhHi1�
</p>
<p>p
</p>
<p>var .H/1; hHi1C
p
</p>
<p>var .H/1&#141;.
We consider now the expectation value hHiT0 for large values of T0. We define the
small parameter � D 1=T0 � 1 and find with p.x; �/ D P.x;T/
</p>
<p>hHi� D
Z
</p>
<p>dx p.x; �/H.x/
</p>
<p>D hHi0 � �
h˝
</p>
<p>H
2
˛
</p>
<p>0
� hHi20
</p>
<p>i
</p>
<p>: (20.10)
</p>
<p>Re-substituting T0 D 1=� results, finally, in:
</p>
<p>hHiT0 � hHi1 �
var .H/1
</p>
<p>T0
: (20.11)
</p>
<p>The initial temperature T0 is now chosen in such a way that the expectation
value hHiT0 borders the infinite temperature fluctuations from below and we set
consequently
</p>
<p>hHiT0 D hHi1 �
p
</p>
<p>var .H/1 ; (20.12)
</p>
<p>with the implication that
</p>
<p>T0 D
p
</p>
<p>var .H/1 : (20.13)
</p>
<p>We are now in a position to investigate appropriate cooling strategies: The
geometric cooling schedule
</p>
<p>Tn D T0qn ; (20.14)
</p>
<p>with 0 � q &lt; 1 is very often used. However, particular cost functions H.x/ may
develop several phase transitions in the course of the cooling process. Naturally, the
expectation value hHi changes rapidly in the region T � Tc, with Tc the temperature
at which the phase transition occurs. It is, therefore, certainly of advantage to take
such a possibility into account and to design the cooling strategy accordingly.
</p>
<p>Hence, a more appropriate strategy is to use temperature changes which cause
only slightly modified acceptance probabilities. In particular, we demand that
</p>
<p>1
</p>
<p>1C ı &lt;
P.x;Tn/
</p>
<p>P.x;TnC1/
&lt; 1C ı ; (20.15)</p>
<p/>
</div>
<div class="page"><p/>
<p>20.3 Simulated Annealing 331
</p>
<p>with 0 &lt; ı � 1. Assuming a BOLTZMANN type distribution for P.x;Tn/, we obtain
</p>
<p>exp
</p>
<p>�
</p>
<p>�H.x/
�
1
</p>
<p>Tn
� 1
</p>
<p>TnC1
</p>
<p>��
</p>
<p>&lt; 1C ı ; (20.16)
</p>
<p>or
</p>
<p>TnC1 &gt;
Tn
</p>
<p>1C Tn
H.x/
</p>
<p>ln.1C ı/
: (20.17)
</p>
<p>Hence, we can choose
</p>
<p>TnC1 �
Tn
</p>
<p>1C Tn
3
p
</p>
<p>var.H/Tn
ln.1C ı/
</p>
<p>; (20.18)
</p>
<p>where we replaced H.x/ � 3
p
</p>
<p>var .H/Tn . This choice is plausible if one recognizes
that we can replace H.x/ ! H.x/ � Hmin in the above calculations, where Hmin
represents the (unknown) minimum of H.x/. This cooling schedule is known as the
AARTS schedule.
</p>
<p>Finally, we have to discuss how to terminate the algorithm. Typically, there are
several choices and we present briefly the most popular ones. The obvious choice is
to terminate the algorithm as soon as the acceptance ratio is below some predefined
threshold value. A more sophisticated choice is to terminate the algorithm whenever
the mean value hHi reaches some constant value. A quite different and more formal
approach would be to initially define a maximum number of iterations or to set
the final temperature Tf to some reasonable value. Nevertheless, the termination
condition has to be defined for each particular problem individually.
</p>
<p>Before presenting an example, we note some further results associated with
cooling strategies. It was demonstrated by S. KIRKPATRIK et al. [15] that the optimal
cooling strategy for a BOLTZMANN type distribution is of the form
</p>
<p>Tn /
1
</p>
<p>ln.n/
; (20.19)
</p>
<p>where n labels the temperature steps. In this case the global minimum is found
with probability one. However, the convergence is rather slow. In addition, several
extensions of classical simulated annealing have been suggested in the literature.
For instance, fast simulated annealing uses a CAUCHY distribution
</p>
<p>P.x;T/ D T
.x2 C T2/
</p>
<p>dC1
2
</p>
<p>(20.20)
</p>
<p>instead of a BOLTZMANN distribution. Here d is the dimension of the search space
S. The optimal cooling strategy for such a distribution function is of the form
</p>
<p>Tn /
1
</p>
<p>n
; (20.21)</p>
<p/>
</div>
<div class="page"><p/>
<p>332 20 Stochastic Optimization
</p>
<p>Fig. 20.1 (a) Initial route of the traveling salesperson for 36 cities on a regular grid. (b) One of
many optimal routes of the traveling salesperson for 36 cities on a regular grid
</p>
<p>which signifies a considerable increase in convergence speed in comparison to
Eq. (20.19). Another generalization is referred to as generalized simulated anneal-
ing and is based on the TSALLIS distribution which depends on an external
parameter �:
</p>
<p>P�.x;T/ D
1
</p>
<p>Z
</p>
<p>�
</p>
<p>1C �H.x/
kBT
</p>
<p>�� 1�
: (20.22)
</p>
<p>It can be demonstrated that P� converges toward the BOLTZMANN distribution for
� ! 0. We mention in passing that the concept of the TSALLIS distribution is
closely intertwined with the definition of the TSALLIS entropy and the formulation
of non-extensive thermodynamics by C. TSALLIS [16].
</p>
<p>As a first illustrative example we discuss the traveling salesperson problem for
N D 36 cities on a regular grid because in this case the optimal route is easily
identified. We calculate the initial temperature from Eq. (20.13) and employ the
geometric cooling schedule (20.14) with q D 0:99 together with a termination
criterion of the form
</p>
<p>hHiTn � hHiTn�1 &lt; � ; (20.23)
</p>
<p>where � is the required accuracy. Figure 20.1a presents one route for the initial
temperature and Fig. 20.1b displays one of many optimal routes after convergence
has been reached. This case will be called the first scenario. In the second scenario
we place 36 cities in four equally spaced clusters. Results for the optimal route are
presented in Fig. 20.2b.
</p>
<p>The possibility of phase transitions to occur during the cooling process has
already been mentioned. In a genuine physical system the question whether a phase
transition is possible at all or if it is of first or second order is solely determined by</p>
<p/>
</div>
<div class="page"><p/>
<p>20.3 Simulated Annealing 333
</p>
<p>Fig. 20.2 (a) Initial route of the traveling salesperson for 36 cities placed in four equally spaced
clusters. (b) One of many optimal routes of the traveling salesperson for 36 cities placed in four
equally spaced clusters
</p>
<p>Fig. 20.3 (a) The expectation value hHiT and (b) the &lsquo;specific heat&rsquo; ch vs temperature T for
scenario one
</p>
<p>the HAMILTON function H.x/ of the system. As an intriguing example we refer to
the q-states POTTS model of Sect. 18.3 where a second order phase transition was
observed for q � 4 and a first order phase transition for q &gt; 4. In analogy, the order
of a &lsquo;phase transition&rsquo; during the iteration process toward the global minimum in
simulated annealing is completely determined by the particular properties of the cost
function H.x/. We want to study such a possibility and determine the expectation
values hHiT and the &lsquo;specific heat&rsquo; ch as functions of temperature T for the two
scenarios of the traveling salesperson problem. Figure 20.3 presents the results for
scenario one and Fig. 20.4 those for scenario two. The second scenario develops
two second order phase transitions while in the first scenario only one second order
phase transition can be observed. The first phase transition of the second scenario</p>
<p/>
</div>
<div class="page"><p/>
<p>334 20 Stochastic Optimization
</p>
<p>Fig. 20.4 The same as
Fig. 20.3 but for scenario two.
Two second order phase
transitions are observed. They
are indicated by down arrows
labeled (1) and (2)
</p>
<p>(at T � 3:5) can be related to the optimization of the clusters&rsquo; sequence while in
the second phase transition (at T � 0:42) the sequence of cities within the clusters
becomes finalized. These two transitions are indicated by down arrows labeled .1/
and .2/ in Fig. 20.4.
</p>
<p>20.4 Genetic Algorithms
</p>
<p>The sparkling idea of genetic algorithms has originally been lent from natures
survival of the fittest [17]. The basic intentions are quickly summarized by remem-
bering the natural evolution of a particular species within a hostile environment: The
individuals of the species reproduce from one generation to another. During this
process the genes of the individuals are modified by local mutations. Individuals
best accustomed to the environment then survive with higher probability. This very
last process is referred to as selection. By iterating this process for large populations
the individuals of the whole species will adjust their properties to the environment
on average,1 and, thus, the individuals will be better equipped for survival within
the hostile environment. A large population is compulsory in order to obtain a huge
variety in the phenotype of the individuals. Algorithms based on such a scheme are
referred to as genetic algorithms.
</p>
<p>We are not going into the details of the implementation of genetic algorithms
because this is beyond the scope of this book. However, the ideas sketched above
will be applied to the problem of the traveling salesperson passing through m-cities
just to illustrate the method. Let s D .s1; : : : ; sm/ 2 Nm denote a list of m integers,
which obey si � i. For instance, for m D 10, s might be given by
</p>
<p>1Note that in the real world the environment (in particular the natural enemies of a species) develop
as well. Moreover, we do not consider any communication within a species, like the formation of
societies, learning, and related processes.</p>
<p/>
</div>
<div class="page"><p/>
<p>20.4 Genetic Algorithms 335
</p>
<p>Table 20.1 Sample tour to illustrate the recovery of the order of cities within a genetic algorithm.
Elements indicated by Œx&#141; are &lsquo;selected&rsquo; elements which are added to the column Tour
</p>
<p>Os 1 2 3 4 5 6 7 8 9 10 Tour
9 1 2 3 4 5 6 7 8 [9] 10 ! 9
4 1 2 3 [4] 5 6 7 8 10 ! 4
3 1 2 [3] 5 6 7 8 10 ! 3
3 1 2 [5] 6 7 8 10 ! 5
5 1 2 6 7 [8] 10 ! 8
1 [1] 2 6 7 10 ! 1
4 2 6 7 [10] ! 10
2 2 [6] 7 ! 6
2 2 [7] ! 7
1 [2] ! 2
</p>
<p>s D .1; 2; 2; 4; 1; 5; 3; 3; 4; 9/ : (20.24)
</p>
<p>The order of cities is then recovered by setting Os D .sm; : : : ; s1/ and performing the
steps illustrated in Table 20.1.
</p>
<p>In words: The vector Os labels the elements taken from the list .1; 2; : : : ;m/ with
removal. The resulting list Tour specifies the optimum sequence of the cities. The
genetic algorithm is executed in the following steps:
</p>
<p>&bull; Define M initial individuals.
&bull; Mutation: for each individual we introduce a single random local modification
</p>
<p>with probability pmut.
&bull; Reproduction: We produce M additional individuals by pairwise combining the
</p>
<p>parents. This is performed by
</p>
<p>(a) Pick two individuals at random.
(b) Draw a random integer r 2 Œ1;m � 1&#141; and replace the first r genes of the first
</p>
<p>individual by the first r genes of the second individual and vice versa.
</p>
<p>In this way, we obtain 2M individuals.
&bull; Selection: The M individuals with the highest fitness which corresponds to the
</p>
<p>lowest value of the cost function survive.
</p>
<p>The above steps are repeated until the desired number of generations has been
achieved.
</p>
<p>In Fig. 20.5 we show the optimal path for the traveling salesperson problem
discussed in the previous section, but now for N D 30 cities. It was obtained with
the genetic algorithm described here. The number of individuals was chosen to be
M D 5000 and the number of generations to be G D 5000.
</p>
<p>Some remarks are appropriate: First of all we note that there are many different
permutations of how a genetic algorithm can be realized. In particular, it is the
problem which determines the most convenient form to implement the essential</p>
<p/>
</div>
<div class="page"><p/>
<p>336 20 Stochastic Optimization
</p>
<p>Fig. 20.5 (a) The random route of one individual out of the population of 5000. (b) One of many
optimal routes of the traveling salesperson for N D 30 cities as obtained by a genetic algorithm
</p>
<p>ingredients: mutation, reproduction, and selection. However, particular care is
required in formulating the algorithm in such a way that it does not produce
individuals which are too similar. In such a case the algorithm is very likely to
terminate in a local minimum.
</p>
<p>Another remark comments on how to treat optimization problems with contin-
uous variables x. Here it might be advantageous to represent the variable x in its
binary form because it makes the reproduction step particularly simple.
</p>
<p>20.5 Some Further Methods
</p>
<p>We briefly list some alternative stochastic optimization techniques without going
into detail. Two famous alternatives which are closely related to simulated annealing
are:
</p>
<p>&bull; Threshold Accepting Algorithms: The new configuration x0 is accepted with
probability one if H.x0/ � H.x/ C T. During the simulation the temperature or
threshold level T is continuously decreased. The above choice of an acceptance
probability is very effective to allow for an escape from local minima.
</p>
<p>&bull; Deluge Algorithms: These algorithms are very similar to threshold accepting
algorithms. We present it in the original formulation which is suited to find
the global maximum of a function G.x/. The global minimum of H.x/ can be
found by searching the maximum of G.x/ D �H.x/. One accepts a new state
x0 with probability one if G.x0/ &gt; T, where T is continuously increased during
the simulation. Hence, the whole landscape of G.x/ is flooded with increasing T
until only the summits of G.x/ are left. Finally, only the biggest mountain will
reach out of the water and the global maximum has been found.</p>
<p/>
</div>
<div class="page"><p/>
<p>20.5 Some Further Methods 337
</p>
<p>Two famous ideas which are closely related to genetic algorithms are:
</p>
<p>&bull; Grouping Genetic Algorithms: The idea is to put the individuals of the population
in distinct groups. These groups may for instance be formed by comparing the
genes or grouping individuals with similar cost function values. All members
of a group have one part of the genes in common, and all operators acting on
genes act on the whole group. Such an approach can significantly improve the
convergence rate of a classical genetic algorithm.
</p>
<p>&bull; Ant Colony Optimization: The idea is, again, borrowed from nature, in particular
from an ant colony searching the optimum path between two or more fixed or
variable points. In a real world an ant travels from one point to another randomly,
leaving a trail of pheromone on its traveled path. Following ants are very likely
to follow the pheromone trail, however, some random nature remains. The key
point is that with time the pheromone trail starts to evaporate, hence its impact
on the path of following ants is reduced if the path is not traveled frequently
or often enough so that the pheromones evaporated. In this way one prevents the
algorithm to get stuck in a local minimum and the global minimummay be found
by sending out artificial ants.
</p>
<p>There are many further methods available in the literature (see, for instance,
Refs. [2, 18]) to which we refer the interested reader.
</p>
<p>Summary
</p>
<p>The local maximum/minimum of some cost function H.x/ within a search space
S can be determined using stochastic methods, thus establishing a particular class
of algorithms known as Stochastic Optimization. The most straightforward method
was the algorithm of hill climbing which resembled a controlled random walk
within a restricted search space S called neighborhood. Because of this feature
hill climbing will find in general local minima within this neighborhood and the
global minimum has to be found under variation of initial conditions. This made
this method too expensive for more complex problems from a computational point
of view. To move from a random walk formulation to a formulation on the basis
of MARKOV-chain Monte Carlo was the logical next step. The method of choice
was named simulated annealing. It used the METROPOLIS-HASTINGS algorithm to
generate new configurations within a search space S from a temperature dependent
equilibrium distribution. A cooling strategy was used to slowly restrict the search
space to the neighborhoodof the globalminimum. This global minimumwas always
found, albeit rather slowly. We mentioned some flavors of this basic algorithm
which either differed in the definition of the acceptance probability or in the cooling
strategy. A completely different class of algorithms was established with the so-
called genetic algorithms. They were adapted from nature&rsquo;s concept of the survival
of the fittest. They were based on the notions of: (i) Mutation, a single random local
modification of a certain probability. (ii) Reproduction, additional &lsquo;individuals&rsquo;were</p>
<p/>
</div>
<div class="page"><p/>
<p>338 20 Stochastic Optimization
</p>
<p>generated by pairwise combining parents. (iii) Selection: Individuals with the lowest
value of the cost function survived and mutation started again. Genetic algorithms
established a very versatile class of solvers to cover a huge body of optimization
problems.
</p>
<p>Problems
</p>
<p>Solve the traveling salesperson problem for N D 20 cities on a regular grid with
the help of simulated annealing. As a cooling schedule, use the geometric cooling
as explained in Sect. 20.3. Determine the initial temperature by demanding an
acceptance rate of 90% and terminate the algorithm if the mean value of the cost
function hHi remains unchanged for at least 10 successive temperatures. Calculate
the expectation value hHiT for different temperatures and identify the transition
temperature. In a second step produce a list of 20 cities which are randomly
distributed on a two-dimensional grid. Optimize this problem as well. Note that
you should produce the list of cities only once in order to obtain comparable and
reproducible results.
</p>
<p>References
</p>
<p>1. Panos, M.P., Resende, M.G.C. (eds.): Handbook of Applied Optimization. Oxford University
Press, New York (2002)
</p>
<p>2. Hartmann, A.H., Rieger, H.: Optimization Algorithms in Physics. Wiley &ndash; VCH, Berlin (2002)
3. Locatelli, M., Schoen, F.: Global Optimization. MPS-SIAM Series on Optimization. Cam-
</p>
<p>bridge University Press, Cambridge (2013)
4. Scholz, D.: Deterministic Global Optimization. Springer, Berlin/Heidelberg (2012)
5. Schneider, J.J., Kirkpatrick, S. (eds.): Stochastic Optimization. Springer, Berlin/Heidelberg
</p>
<p>(2006)
6. Lawler, E.L., Lenstra, K.K., Rinnooy Kan, A.H.G., Shmoys, D.B.: The Traveling Salesman
</p>
<p>Problem: A Guided Tour of Combinatorial Optimization. Wiley, New York (1985)
7. Press, W.H., Teukolsky, S.A., Vetterling, W.T., Flannery, B.P.: Numerical Recipes in C++, 2nd
</p>
<p>edn. Cambridge University Press, Cambridge (2002)
8. Kundu, S., Acharyya, S.: Stochastic local search approaches in solving the nurse scheduling
</p>
<p>problem. In: Chaki, N., Cortesi, A. (eds.) Computer Information Systems &ndash; Analysis and
Technologies, Communications in Computer and Information Science, vol. 245, pp. 202&ndash;211.
Springer, Berlin/Heidelberg (2011)
</p>
<p>9. Marinari, E., Parisi, G., Ritort, F.: On the 3d Ising spin glass. J. Phys. A: Math. Gen. 27, 2687
(1994). doi:10.1088/0305-4470/27/8/008
</p>
<p>10. Watkins, J.J.: Across the Board: The Mathematics of Chessboard Problems. Princeton
University Press, Princeton (2012)
</p>
<p>11. Skiena, S.S.: The Algorithm Design Manual. Springer, Berlin/Heidelberg (2008)
12. Marsaglia, G.: Choosing a point from the surface of a sphere. Ann. Math. Stat. 43, 645&ndash;646
</p>
<p>(1972). doi:10.1214/aoms/1177692644
13. Bertsimas, D., Tsitsiklis, J.: Simulated annealing. Stat. Sci. 8, 10&ndash;15 (1993)</p>
<p/>
</div>
<div class="page"><p/>
<p>References 339
</p>
<p>14. Salamon, P., Sibani, P., Frost, R.: Facts, Conjectures, and Improvements for Simulated
Annealing. Cambridge University Press, Cambridge (2002)
</p>
<p>15. Kirkpatrik, S., Gellat, C.D., Jr., Vecchi, M.P.: Simulated annealing. Science 220, 671 (1983)
16. Tsallis, C.: Introduction to Nonextensive Statistical Mechanics. Springer, Berlin/Heidelberg
</p>
<p>(2009)
17. Man, K.F., Tang, K.S., Kwong, S.: Genetic Algorithms. Springer, Berlin/Heidelberg (1999)
18. B&auml;ck, T.: Evolutionary Algorithms in Theory and Practice. Oxford University Press, New York
</p>
<p>(1996)</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix A
The Two-Body Problem
</p>
<p>Consider two mass points with positions ri.t/ 2 R3, i D 1; 2 and masses mi,
i D 1; 2. It is assumed that the point masses interact through a central potential
U D U.jr1.t/ � r2.t/j/ and that external forces are neglected. Thus, the system is
closed. The explicit notation of time t is now omitted for the sake of a more compact
presentation. Furthermore, we introduce with pi 2 R3, i D 1; 2 the point mass&rsquo;
momentum and the LAGRANGE function [1&ndash;5] of the system takes on the form
</p>
<p>L.r1; r2; p1; p2/ D
p21
</p>
<p>2m1
C p
</p>
<p>2
2
</p>
<p>2m2
� U.jr1 � r2j/ : (A.1)
</p>
<p>The moments pi are replaced by
</p>
<p>pi D miPri; i D 1; 2 ; (A.2)
</p>
<p>and this yields for the LAGRANGE function (A.1)
</p>
<p>L.r1; r2; Pr1; Pr2/ D
m1
</p>
<p>2
Pr21 C
</p>
<p>m2
</p>
<p>2
Pr22 � U.jr1 � r2j/ ; (A.3)
</p>
<p>where Pri denotes the time derivative of ri. We note the following symmetries:
the LAGRANGE function is (i) translational invariant, (ii) rotational invariant, and
(iii) time invariant. We know from classical mechanics that each symmetry of
the LAGRANGE function corresponds to a constant of motion (a quantity that
is conserved throughout the motion) and, thus, results in a reduction of the
dimensionality of the 12-dimensional phase space.
</p>
<p>Let us demonstrate these symmetries: In order to prove translational invariance,
we transform to center of mass coordinates which are defined as
</p>
<p>R D m1r1 C m2r2
m1 C m2
</p>
<p>and r D r2 � r1 : (A.4)
</p>
<p>&copy; Springer International Publishing Switzerland 2016
B.A. Stickler, E. Schachinger, Basic Concepts in Computational Physics,
DOI 10.1007/978-3-319-27265-8
</p>
<p>341</p>
<p/>
</div>
<div class="page"><p/>
<p>342 A The Two-Body Problem
</p>
<p>It is easily verified that we can express the original coordinates r1 and r2 with the
help of (A.4) as
</p>
<p>r1 D R C
m2
</p>
<p>m1 C m2
r and r2 D R �
</p>
<p>m1
</p>
<p>m1 C m2
r : (A.5)
</p>
<p>The LAGRANGE function (A.3) is rewritten in these new coordinates (A.4) and this
yields
</p>
<p>L.r;R; Pr; PR/ D M
2
PR2 C m
</p>
<p>2
Pr2 � U.jrj/
</p>
<p>� L.r; Pr; PR/ ; (A.6)
</p>
<p>where we introduced the total mass M and the reduced mass m:
</p>
<p>M D m1 C m2 and m D
m1m2
</p>
<p>m1 C m2
: (A.7)
</p>
<p>Obviously, the center of mass coordinate R plays in Eq. (A.6) the role of a cyclic
coordinate: It does not appear explicitly in the LAGRANGE function. This means
that the system is translational invariant and we can deduce from LAGRANGE&rsquo;s
equations that
</p>
<p>d
</p>
<p>dt
</p>
<p>@
</p>
<p>@ PR
L D @
</p>
<p>@R
L D 0 ; (A.8)
</p>
<p>and the center of mass momentum is conserved. Hence, we obtain that
</p>
<p>@
</p>
<p>@ PR
L D M PR D const ; (A.9)
</p>
<p>with the solution
</p>
<p>R.t/ D At C B ; (A.10)
</p>
<p>where A;B 2 R3 are constants determined by the initial conditions of the problem.
As a result, the center of mass moves along a straight line with constant velocity.
We collect all results and reformulate the LAGRANGE function (A.6) as
</p>
<p>L.r; Pr/ D M
2
</p>
<p>A2 C m
2
Pr2 � U.jrj/
</p>
<p>� QL.r; Pr/C const : (A.11)</p>
<p/>
</div>
<div class="page"><p/>
<p>A The Two-Body Problem 343
</p>
<p>Hence, the problem was reduced to a one-body problem with the LAGRANGE
function QL.r; Pr/. In what follows the tilde is omitted and the LAGRANGE function
</p>
<p>L.r; Pr/ D m
2
Pr2 � U.jrj/ ; (A.12)
</p>
<p>is now studied instead of Eq. (A.11). It is an effective one-body LAGRANGE
function.
</p>
<p>In the next step the effect of rotational invariance is investigated. Equation (A.12)
resembles the LAGRANGE function of a particle of mass m which is located at
position r and moves in the field of a central force F 2 R3. This force points to the
center of the coordinate system (or points from the center of the coordinate system
to the particle). This situation is clearly invariant under a rotation of the coordinate
system since U D U.jrj/ depends only on the modulus of r. Consequently, r is
parallel to F for all t � 0. In such a case the vector of angular momentum ` 2 R3 is
conserved, since
</p>
<p>d
</p>
<p>dt
` D M D r � F D 0 ; ! ` D const ; (A.13)
</p>
<p>where M is the torque. This allows us to arbitrarily rotate our coordinate system.
We take advantage if this property and rotate it in such a way that
</p>
<p>` D j`jez ; (A.14)
</p>
<p>where ez is the unit vector in z-direction. Moreover, since the angular momentum `
is given by
</p>
<p>` D mr � Pr D const ; (A.15)
</p>
<p>and because `kez we conclude that r?ez. This allows us to set z D 0 which
means that the whole motion of the point mass can be described in the x � y
plane. Rotational invariance led us to the conservation of angular momentum and
this made the reduction from a three-dimensional problem to a two dimensional
problem possible. The particular form (A.12) of the LAGRANGE function suggests
the introduction of polar coordinates .�; '/:
</p>
<p>L.�; P�; P'/ D m
2
</p>
<p>�
</p>
<p>P�2 C �2 P'2
�
</p>
<p>� U.�/: (A.16)
</p>
<p>We solve now LAGRANGE&rsquo;s equations (A.6) on the basis of Eq. (A.16): The first
step deals with the differential equation for the radius �
</p>
<p>d
</p>
<p>dt
</p>
<p>@
</p>
<p>@ P�L D m R� D
@
</p>
<p>@�
L D m� P'2 � @
</p>
<p>@�
U.�/ ; (A.17)</p>
<p/>
</div>
<div class="page"><p/>
<p>344 A The Two-Body Problem
</p>
<p>thus
</p>
<p>m R� � m� P'2 C d
d�
</p>
<p>U.�/ D 0 : (A.18)
</p>
<p>The differential equation for the angle ' follows from
</p>
<p>d
</p>
<p>dt
</p>
<p>@
</p>
<p>@ P' L D
d
</p>
<p>dt
m�2 P' D @
</p>
<p>@'
L D 0 ; (A.19)
</p>
<p>which corresponds to
</p>
<p>d
</p>
<p>dt
</p>
<p>�
</p>
<p>m�2 P'
�
</p>
<p>D 0 : (A.20)
</p>
<p>Equation (A.20) is trivially fulfilled since according to Eq. (A.15)
</p>
<p>m�2 P' D j`j D const : (A.21)
</p>
<p>However, we solve Eq. (A.21) for P'
</p>
<p>P' D j`j
m�2
</p>
<p>; (A.22)
</p>
<p>plug (A.22) into (A.18), and obtain
</p>
<p>m R� � j`j
2
</p>
<p>m�3
C d
</p>
<p>d�
U.�/ D 0 : (A.23)
</p>
<p>We make use of the time invariance of the LAGRANGE function (A.16). This
equation does not explicitly depend on time t and we have
</p>
<p>@
</p>
<p>@t
L D 0 : (A.24)
</p>
<p>This implies conservation of energy, as can easily be demonstrated. For this purpose,
we regard the total time derivative of the LAGRANGE function L
</p>
<p>d
</p>
<p>dt
L D P� @
</p>
<p>@�
L C R� @
</p>
<p>@ P�L C R'
@
</p>
<p>@ P' L C
@
</p>
<p>@t
L ; (A.25)
</p>
<p>and solve for @
@t
</p>
<p>L
</p>
<p>d
</p>
<p>dt
</p>
<p>�
</p>
<p>P� @
@ P�L C P'
</p>
<p>@
</p>
<p>@ P' L � L
�
</p>
<p>D � @
@t
</p>
<p>L D 0 : (A.26)</p>
<p/>
</div>
<div class="page"><p/>
<p>A The Two-Body Problem 345
</p>
<p>Consequently
</p>
<p>P� @
@ P�L C P'
</p>
<p>@
</p>
<p>@ P' L � L D const ; (A.27)
</p>
<p>which states the conservation of energy. We evaluate this expression with the help
of Eq. (A.16). We obtain
</p>
<p>P� @
@ P�L C P'
</p>
<p>@
</p>
<p>@ P' L � L D
m
</p>
<p>2
</p>
<p>�
</p>
<p>P�2 C �2 P'2
�
</p>
<p>C U.�/
</p>
<p>D m
2
P�2 C j`j
</p>
<p>2
</p>
<p>2m�2
C U.�/
</p>
<p>D E : (A.28)
</p>
<p>Here we employed, in the second step, relation (A.22). In summary, time invariance
resulted in:
</p>
<p>m
</p>
<p>2
P�2 C j`j
</p>
<p>2
</p>
<p>2m�2
C U.�/ D E : (A.29)
</p>
<p>This is a first order differential equation in �.
The necessary step required for a solution of the two-body problem can now be
</p>
<p>outlined: (i) Calculate R.t/ according to Eq. (A.10), (ii) solve Eq. (A.29) in order to
obtain �.t/, (iii) plug �.t/ into Eq. (A.22) and solve for '.t/, (iv) since z.t/ D 0, the
original vectors r1.t/, r2.t/ can be constructed from �.t/ and '.t/. All integration
constants are uniquely determined by the initial conditions of the problem at hand.
</p>
<p>From Eq. (A.29) we obtain
</p>
<p>P� D ˙
s
</p>
<p>2
</p>
<p>m
</p>
<p>�
</p>
<p>E � U.�/ � j`j
2
</p>
<p>2m�2
</p>
<p>�
</p>
<p>; (A.30)
</p>
<p>which results in an implicit equation for �
</p>
<p>t D t0 C
Z �
</p>
<p>�0
</p>
<p>d�0
m�0
</p>
<p>p
</p>
<p>2m�02 ŒE � U.�0/&#141; � j`j2
; (A.31)
</p>
<p>where we defined �0 � �.t0/, t0 is some initial time, and we neglected the negative
root. Equation (A.31) defines t as a function of �, t D t.�/, which has to be inverted
to, finally, obtain the required solution � D �.t/. Whether Eq. (A.31) can be solved
analytically depends on the particular form of the potential U.�/. If Eq. (A.31)
cannot be solved analytically one has to employ numerical approximations.</p>
<p/>
</div>
<div class="page"><p/>
<p>346 A The Two-Body Problem
</p>
<p>Finally, the angle ' can be expressed as a function of the radius �, i.e. ' D '.�/.
We get from Eqs. (A.22) and (A.30)
</p>
<p>d'
</p>
<p>d�
D d'
</p>
<p>dt
</p>
<p>dt
</p>
<p>d�
D ˙ j`j
</p>
<p>m�2
</p>
<p>�
2
</p>
<p>m
</p>
<p>�
</p>
<p>E � U.�/� j`j
2
</p>
<p>2m�2
</p>
<p>��� 12
; (A.32)
</p>
<p>integrate over �, and find the desired relation
</p>
<p>' D '0 ˙ j`j
Z �
</p>
<p>�0
</p>
<p>d�0
</p>
<p>�0
p
</p>
<p>2m�02 ŒE � U.�0/&#141; � j`j2
; (A.33)
</p>
<p>where '0 � '.t0/.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix B
Solving Non-linear Equations: The NEWTON
Method
</p>
<p>We give a brief introduction into the solution of non-linear equations with the help
of NEWTON&rsquo;s method.1 We regard a differentiable function F.x/ and we would like
to find the solution of the equation
</p>
<p>F.x/ D 0 : (B.1)
</p>
<p>The simplest approach is to transform the equation into an equation of the form
</p>
<p>x D f .x/ ; (B.2)
</p>
<p>which is always possible. This equation could be solved iteratively by simply
repeating
</p>
<p>xtC1 D f .xt/ ; (B.3)
</p>
<p>where we start with some initial value x0. If this method converges, one can
approximate the solution arbitrarily close, however, convergence is not guaranteed
and will in fact depend on the transformation from Eqs. (B.1) to (B.2). A more
advanced technique is the so called NEWTON method [6, 7]. It is based on the
definition of f .x/ as
</p>
<p>f .x/ D x � F.x/
F0.x/
</p>
<p>; (B.4)
</p>
<p>which allows the iteration
</p>
<p>xtC1 D xt �
F.xt/
</p>
<p>F0.xt/
: (B.5)
</p>
<p>1This method is also referred to as the NEWTON-RAPHSON method.
</p>
<p>&copy; Springer International Publishing Switzerland 2016
B.A. Stickler, E. Schachinger, Basic Concepts in Computational Physics,
DOI 10.1007/978-3-319-27265-8
</p>
<p>347</p>
<p/>
</div>
<div class="page"><p/>
<p>348 B Solving Non-linear Equations: The NEWTON Method
</p>
<p>Here F0.x/ denotes the derivative of F.x/ with respect to x. The convergence
behavior of the iteration (B.5) highly depends on the form of the function F.x/
and on the choice of the starting point x0. The routine can be regarded as converged
if jxtC1 � xtj &lt; �, where � is the accuracy required.
</p>
<p>If F.x/ is not differentiable one can use the regula falsi or employ stochastic
methods which are discussed in the second part of this book. The iteration of the
method known as regula falsi is [6, 7]
</p>
<p>xtC1 D xt � F.xt/
xt � xt�1
</p>
<p>F.xt/� F.xt�1/
: (B.6)
</p>
<p>A more detailed discussion on methods to solve transcendental equations numer-
ically can be found in any textbook on numerical methods, see for instance
Refs. [8, 9]. We shall also briefly introduce the case of a non-linear system of
equations of the form (B.1) where F.x/ 2 RN and x 2 RN . In this case the iteration
scheme is given by
</p>
<p>xtC1 D xt � J�1.xt/F.xt/ ; (B.7)
</p>
<p>where
</p>
<p>J.x/ D rxF.x/ D
</p>
<p>0
</p>
<p>B
B
B
B
@
</p>
<p>@F1.x/
</p>
<p>@x1
</p>
<p>@F1.x/
</p>
<p>@x2
: : :
</p>
<p>@F1.x/
</p>
<p>@xN
@F2.x/
</p>
<p>@x1
</p>
<p>@F2.x/
</p>
<p>@x2
: : :
</p>
<p>@F2.x/
</p>
<p>@xN
:::
</p>
<p>:::
: : :
</p>
<p>:::
@FN .x/
</p>
<p>@x1
</p>
<p>@FN .x/
</p>
<p>@x2
: : :
</p>
<p>@FN .x/
</p>
<p>@xN
</p>
<p>1
</p>
<p>C
C
C
C
A
</p>
<p>: (B.8)
</p>
<p>is the JACOBI matrix of F.x/. We can also make use of the methods discussed in
Chap. 2 to calculate numerically the derivatives in Eqs. (B.5) or (B.8).</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix C
Numerical Solution of Linear Systems
of Equations
</p>
<p>We discuss briefly two of the most important methods to solve non-homogeneous
systems of linear equations applying numerical methods. We consider a system of n
equations of the form
</p>
<p>a11x1 C a12x2 C : : :C a1nxn D b1 ;
a21x1 C a22x2 C : : :C a2nxn D b2 ;
</p>
<p>:::
:::
</p>
<p>an1x1 C an2x2 C : : :C annxn D bn ; (C.1)
</p>
<p>which is usually transformed into a matrix equation,
</p>
<p>Ax D b : (C.2)
</p>
<p>The coefficients of the matrix A D faijg as well as the vector b D fbig are assumed
to be real valued and, furthermore, if
</p>
<p>nX
</p>
<p>iD1
jbij &curren; 0 ; (C.3)
</p>
<p>the problem (C.2) is referred to as non-homogeneous (inhomogeneous). The
solution of non-homogeneous linear systems of equations is one of the central
problems in numerical analysis, since numerous numerical methods, such as the
finite difference approach to a boundary value problem, see Chap. 8, can be reduced
to such a problem.
</p>
<p>&copy; Springer International Publishing Switzerland 2016
B.A. Stickler, E. Schachinger, Basic Concepts in Computational Physics,
DOI 10.1007/978-3-319-27265-8
</p>
<p>349</p>
<p/>
</div>
<div class="page"><p/>
<p>350 C Numerical Solution of Linear Systems of Equations
</p>
<p>The solution of (C.2) is well defined as long as the matrix A is non-singular, i.e.
as long as
</p>
<p>det.A/ &curren; 0 : (C.4)
</p>
<p>Then the unique solution of (C.2) can be written as
</p>
<p>x D A�1b : (C.5)
</p>
<p>However, the inversion of matrix A is very complex for n � 4 and one would prefer
methods which are computationally more effective. Basically, one distinguishes
between direct and iterative methods. Since a complete discussion of this huge topic
would be too extensive, we will mainly focus on two methods.
</p>
<p>In contrast to iterative procedures, direct procedures do not contain any method-
ological errors and can, therefore, be regarded as exact. However, these methods
are often computationally very extensive and rounding errors are in many cases
not negligible. As an example we will discuss the LU decomposition. On the other
hand, many iterative methods are fast and rounding errors can be controlled easily.
However, it is not guaranteed that an iterative procedure converges, even in cases
where the system of equations is known to have unique solutions. Moreover, the
result is an approximate solution. As an illustration for an iterative procedure we
will discuss the GAUSS-SEIDEL method.
</p>
<p>C.1 The LU Decomposition
</p>
<p>The LU decomposition [6, 10] is essentially a numerical realization of GAUSSIAN
elimination which is based on a fundamental property of linear systems of equa-
tions (C.2). This property states the system (C.2) to remain unchanged when a linear
combination of rows is added to one particular row. This property is then employed
in order to obtain a matrix in triangular form. It was demonstrated by DOOLITTLE
and CROUT [6, 10, 11] that the GAUSSIAN elimination can be formulated as a
decomposition of the matrix A into two matrices L and U:
</p>
<p>A D LU : (C.6)
</p>
<p>Here, U is an upper triangular matrix and L is a lower triangular matrix. In
particular, U is of the form
</p>
<p>U D
</p>
<p>0
</p>
<p>B
B
B
@
</p>
<p>u11 u12 : : : u1n
0 u22 : : : u2n
:::
</p>
<p>:::
</p>
<p>0 0 : : : unn
</p>
<p>1
</p>
<p>C
C
C
A
; (C.7)</p>
<p/>
</div>
<div class="page"><p/>
<p>C Numerical Solution of Linear Systems of Equations 351
</p>
<p>and L is of the form
</p>
<p>L D
</p>
<p>0
</p>
<p>B
B
B
B
B
@
</p>
<p>1 0 : : : 0
</p>
<p>m21 1 0 : : : 0
</p>
<p>m31 m32 1 : : : 0
:::
</p>
<p>:::
</p>
<p>mn1 mn2 mn3 : : : 1
</p>
<p>1
</p>
<p>C
C
C
C
C
A
</p>
<p>: (C.8)
</p>
<p>The factorization (C.6) is referred to as LU decomposition. The corresponding
procedure can be easily identified by equating the elements in (C.6). One can
show that the following operations yield the desired result: For j D 1; 2; : : : ; n one
computes
</p>
<p>uij D aij �
i�1X
</p>
<p>kD1
mikukj i D 1; 2; : : : ; j ; (C.9)
</p>
<p>mij D
1
</p>
<p>ujj
</p>
<p> 
</p>
<p>aij �
j�1
X
</p>
<p>kD1
mikukj
</p>
<p>!
</p>
<p>i D j C 1; j C 2; : : : ; n ; (C.10)
</p>
<p>with the requirement that ujj &curren; 0. Note that in this notation we used the convention
that the contribution of the sum is equal to zero if the upper boundary is less than the
lower boundary. We rewrite Eq. (C.2) with the help of the LU decomposition (C.6)
</p>
<p>Ax D LUx D b ; (C.11)
</p>
<p>and by defining y D Ux, we retrieve a system of equations for the variable y:
</p>
<p>Ly D b : (C.12)
</p>
<p>The particular form of L allows to solve the system (C.12) immediately by forward
substitution. We find the solution
</p>
<p>yi D bi �
i�1
X
</p>
<p>kD1
mikyk ; i D 1; 2; : : : ; n ; (C.13)
</p>
<p>and the equation
</p>
<p>Ux D y ; (C.14)
</p>
<p>remains. It can solved by backward substitution:
</p>
<p>xi D
1
</p>
<p>uii
</p>
<p> 
</p>
<p>yi �
n
X
</p>
<p>kDiC1
uikxk
</p>
<p>!
</p>
<p>; i D n; n � 1; : : : ; 1 : (C.15)</p>
<p/>
</div>
<div class="page"><p/>
<p>352 C Numerical Solution of Linear Systems of Equations
</p>
<p>We note that this method can also be employed to invert the matrix A. The
strategy is based on the relation
</p>
<p>AX D I ; (C.16)
</p>
<p>where X D A�1 is to be determined and I is the n-dimensional identity. Equa-
tion (C.16) is equivalent to the following system of equations:
</p>
<p>Ax1 D
</p>
<p>0
</p>
<p>B
B
B
@
</p>
<p>1
</p>
<p>0
:::
</p>
<p>0
</p>
<p>1
</p>
<p>C
C
C
A
;
</p>
<p>Ax2 D
</p>
<p>0
</p>
<p>B
B
B
B
B
@
</p>
<p>0
</p>
<p>1
</p>
<p>0
:::
</p>
<p>0
</p>
<p>1
</p>
<p>C
C
C
C
C
A
</p>
<p>;
</p>
<p>:::
:::
</p>
<p>Axn D
</p>
<p>0
</p>
<p>B
B
B
@
</p>
<p>0
:::
</p>
<p>0
</p>
<p>1
</p>
<p>1
</p>
<p>C
C
C
A
; (C.17)
</p>
<p>where the vectors xi are the rows of the unknownmatrix X, i.e. X D .x1; x2; : : : ; xn/.
The n equations of the system (C.17) can be solved with the help of the LU
decomposition.
</p>
<p>Furthermore, one can easily calculate the determinant of A using the LU
decomposition. We note that
</p>
<p>det.A/ D det.LU/ D det.L/ det.U/ D det.U/ ; (C.18)
</p>
<p>since L and U are triangular matrices, the determinants are equal to the product of
the diagonal elements, which yields det.L/ D 1. Hence we have
</p>
<p>det.A/ D det.U/ D
n
Y
</p>
<p>iD1
uii : (C.19)
</p>
<p>In conclusion we remark that there are many specialized methods which have
been designed particularly for matrices of specific forms, such as tridiagonal
matrices, symmetric matrices, block-matrices, . . . . Such matrices commonly appear</p>
<p/>
</div>
<div class="page"><p/>
<p>C Numerical Solution of Linear Systems of Equations 353
</p>
<p>in physics applications. For instance, we remember that the matrix we encountered
in Sect. 8.2 within the context of a finite difference approximation of boundary
value problems, was tridiagonal. These specialized methods are usually the first
choice if one has a matrix of such a specific form because they are much faster and
more stable than methods developed for matrices of more general form. Since a full
treatment of these methods is beyond the scope of this book, we refer the interested
reader to books on numerical linear algebra, for instance Refs. [10, 11].
</p>
<p>C.2 The GAUSS-SEIDEL Method
</p>
<p>The GAUSS-SEIDEL method is an iterative procedure to approximate the solution
of non-homogeneous systems of linear equations [6, 12]. The advantage of an
iterative procedure, in contrast to a direct approach, is that its formulation is in
general much simpler. However, one might have problems with the convergence
of the method, even in cases where a solution exists and is unique. We note that
the GAUSS-SEIDEL method is of particular interest whenever one has to deal with
sparse coefficient matrices.1 This requirement is not too restrictive since most of
the matrices encountered in physical applications are indeed sparse. As an example
we remember the matrices arising in the context of a finite difference approach to
boundary value problems, Sect. 8.2.
</p>
<p>Again, we use Eq. (C.1) as a starting point for our discussion. It is a requirement
of the GAUSS-SEIDEL method that all diagonal elements of A are non-zero.We then
solve each row of (C.1) for xi. This creates the following hierarchy
</p>
<p>x1 D �
1
</p>
<p>a11
.a12x2 C a13x3 C : : :C a1nxn � f1/ ;
</p>
<p>x2 D �
1
</p>
<p>a22
.a21x1 C a23x3 C : : :C a2nxn � f2/ ;
</p>
<p>:::
:::
</p>
<p>xn D �
1
</p>
<p>ann
.an1x1 C an2x2 C : : :C an;n�1xn�1 � fn/ ; (C.20)
</p>
<p>or in general for i D 1; : : : ; n
</p>
<p>xi D �
1
</p>
<p>aii
</p>
<p>0
</p>
<p>B
B
@
</p>
<p>n
X
</p>
<p>jD1
j&curren;i
</p>
<p>aijxj � fi
</p>
<p>1
</p>
<p>C
C
A
: (C.21)
</p>
<p>1A matrix A is referred to as sparse, when the matrix is populated primarily by zeros.</p>
<p/>
</div>
<div class="page"><p/>
<p>354 C Numerical Solution of Linear Systems of Equations
</p>
<p>We note that Eq. (C.21) can be rewritten as a matrix equation
</p>
<p>x D Cx C b ; (C.22)
</p>
<p>where we defined the matrix C D fcijg via
</p>
<p>cij D
</p>
<p>8
</p>
<p>&lt;
</p>
<p>:
</p>
<p>�aij
aii
</p>
<p>i &curren; j ;
</p>
<p>0 i D j ;
(C.23)
</p>
<p>and the vector b D fbig as
</p>
<p>bi D
fi
</p>
<p>aii
: (C.24)
</p>
<p>We recognize that Eq. (C.21) can be transformed into an iterative form with the help
of a trivial manipulation
</p>
<p>xi D xi �
</p>
<p>2
</p>
<p>6
6
4
</p>
<p>xi C
1
</p>
<p>aii
</p>
<p>0
</p>
<p>B
B
@
</p>
<p>n
X
</p>
<p>jD1
j&curren;i
</p>
<p>aijxj � fi
</p>
<p>1
</p>
<p>C
C
A
</p>
<p>3
</p>
<p>7
7
5
; (C.25)
</p>
<p>or
</p>
<p>x
.tC1/
i D x
</p>
<p>.t/
i �
x
</p>
<p>.t/
i ; (C.26)
</p>
<p>where
</p>
<p>
x
.t/
i D x
</p>
<p>.t/
i C
</p>
<p>1
</p>
<p>aii
</p>
<p>0
</p>
<p>@
</p>
<p>i�1X
</p>
<p>jD1
aijx
</p>
<p>.tC1/
j C
</p>
<p>nX
</p>
<p>jDiC1
aijx
</p>
<p>.t/
j � fi
</p>
<p>1
</p>
<p>A : (C.27)
</p>
<p>Equation (C.26) in combination with (C.27) produces a sequence of vectors
</p>
<p>x.0/ ! x.1/ ! x.2/ ! : : :! x.m/ ; (C.28)
</p>
<p>where x.0/ is referred to as the initialization vector or trial vector. One can prove that
if this sequence converges, it approaches the exact solution x arbitrarily close:
</p>
<p>lim
t!1
</p>
<p>x.t/ D x : (C.29)
</p>
<p>We remark that if the terms x.tC1/i on the right hand side of Eq. (C.27) are replaced
</p>
<p>by x.t/i the method is referred to as the JACOBI method.</p>
<p/>
</div>
<div class="page"><p/>
<p>C Numerical Solution of Linear Systems of Equations 355
</p>
<p>To terminate the GAUSS-SEIDEL method, we need an exit condition: One should
terminate the iteration whenever:
</p>
<p>� The approximate solution x.t/ obeys the required accuracy � or Q�, for instance
</p>
<p>max
�
</p>
<p>jx.t/i � x
.t�1/
i j
</p>
<p>�
</p>
<p>� � ; (C.30)
</p>
<p>where � is the absolute error, or
</p>
<p>max
</p>
<p> 
</p>
<p>jx.t/i � x
.t�1/
i j
</p>
<p>jx.t/i j
</p>
<p>!
</p>
<p>� Q� ; (C.31)
</p>
<p>where Q� is the relative error.
� When a maximum number of iterations is reached. This condition may be
</p>
<p>interpreted as an emergency exit which ensures that the iteration terminates even
if the process is not convergent or has still not converged.
</p>
<p>Let us discuss one final, however, crucial point of this section: In many cases
the convergence of the GAUSS-SEIDEL method can be significantly improved by
including a relaxation parameter ! to the iterative process. In this case the update
routine (C.26) takes on the form
</p>
<p>x
.tC1/
i D x
</p>
<p>.t/
i � !
x
</p>
<p>.t/
i : (C.32)
</p>
<p>If the relaxation parameter ! obeys ! &gt; 1 one speaks of over-relaxation, if ! &lt; 1
of under-relaxation and if ! D 1 the regular GAUSS-SEIDEL method is recovered.
An appropriate choice of the relaxation parameter may fasten the convergence of
the method significantly. The best result will certainly be obtained if the ideal value
of !, !i were known. Unfortunately, it is impossible to determine !i prior to the
iteration in the general case. We remark the following properties:
</p>
<p>� The method (C.32) is only convergent for 0 &lt; ! � 2.
� If the matrix C is positive definite and 0 &lt; ! &lt; 2, the GAUSS-SEIDEL method
</p>
<p>converges for any choice of x.0/ (OSTROWSKI-REICH theorem, [13]).
� In many cases, 1 � !i � 2. We note that this inequality holds only under
</p>
<p>particular restrictions for the matrixC [see Eq. (C.23)]. However, we note without
going into detail, that these restrictions are almost always fulfilled when one is
confronted with applications in physics.
</p>
<p>� If C is positive definite and tridiagonal, the ideal value !i can be calculated using
</p>
<p>!i D
2
</p>
<p>1C
p
1 � �2
</p>
<p>; (C.33)
</p>
<p>where � is the largest eigenvalue of C, Eq. (C.23).</p>
<p/>
</div>
<div class="page"><p/>
<p>356 C Numerical Solution of Linear Systems of Equations
</p>
<p>� Since the calculation of � is in many cases quite complex, one could employ the
following idea: It is possible to prove that
</p>
<p>lim
t!1
</p>
<p>j
x.tC1/j
j
x.t/j ! �
</p>
<p>2 : (C.34)
</p>
<p>Hence, one may start with ! D 1, perform t0 (20 &lt; t0 &lt; 100) iterations and then
approximate !i with the help of Eq. (C.33) and
</p>
<p>�2 � j
x
.t0/j
</p>
<p>j
x.t0�1/j : (C.35)
</p>
<p>The iteration is then continued with the approximated value of !i until conver-
gence is reached.
</p>
<p>In conclusion we remark that numerous numerical libraries contain sophisticated
routines to solve linear systems of equations. In many cases it is, thus, advisable to
rely on such routines.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix D
Fast Fourier Transform
</p>
<p>Integral transforms are indispensable in modern mathematics and natural science
because they can be employed to simplify complex mathematical problems. In this
Appendix we will discuss the FOURIER transform as one prominent representative
of integral transforms in general. Loosely speaking, the FOURIER transform is the
unambiguous decomposition of a function f .x/ into its frequency components. Its
applications range from the harmonic analysis of periodic signals to the solution of
differential equations and the description of wave phenomena in classical mechanics
[2, 4], electrodynamics [14&ndash;16], quantummechanics [17&ndash;19], andmanymore. Here,
we briefly discuss its numerical implementation, the fast FOURIER transform (FFT)
and its applications in Computational Physics.
</p>
<p>We start by recalling the concept of FOURIER series: It is asserted by FOURIER&rsquo;s
theorem that every square-integrable, d-periodic function f .x/, f .xC d/ D f .x/, can
be (uniquely) represented as1
</p>
<p>f .x/ D
X
</p>
<p>n2Z
</p>
<p>Ofn exp
�
</p>
<p>i
2�nx
</p>
<p>d
</p>
<p>�
</p>
<p>; (D.1)
</p>
<p>where the complex coefficients Ofn 2 C are related to f .x/ by the inverse transform
</p>
<p>Ofn D
1
</p>
<p>d
</p>
<p>Z d
</p>
<p>0
</p>
<p>dx f .x/ exp
</p>
<p>�
</p>
<p>�i2�nx
d
</p>
<p>�
</p>
<p>: (D.2)
</p>
<p>1In other words, the plane waves exp.in2�x=d/ with period d form a complete, orthonormal basis
in the space of d-periodic, square integrable functions with the scalar product (10.10). We remark
that this also applies to functions which are defined on a compact interval of length d [20].
</p>
<p>&copy; Springer International Publishing Switzerland 2016
B.A. Stickler, E. Schachinger, Basic Concepts in Computational Physics,
DOI 10.1007/978-3-319-27265-8
</p>
<p>357</p>
<p/>
</div>
<div class="page"><p/>
<p>358 D Fast Fourier Transform
</p>
<p>The representation (D.1) of f .x/ is referred to as the FOURIER series of f .x/ and
the coefficient (D.2) is the FOURIER coefficient of order n. Equation (D.1) is an
unambiguous expansion of the function f .x/ into contributions which oscillate with
an integer multiple of the frequency 2�=d. There are numerous important properties,
examples and applications of FOURIER series for which we refer to the literature
[21&ndash;24].
</p>
<p>The concept of FOURIER series can be generalized to the idea of the FOURIER
transform of a square integrable function f .x/ by formally letting d ! 1 [23]. The
FOURIER transform relates the function f .x/ to its transform Of .k/, k 2 R, via2
</p>
<p>f .x/ D
Z 1
</p>
<p>�1
dk Of .k/ exp.ikx/; (D.3)
</p>
<p>and the inverse transform is obtained as
</p>
<p>Of .k/ D 1
2�
</p>
<p>Z 1
</p>
<p>�1
dx f .x/ exp.�ikx/: (D.4)
</p>
<p>The transform (D.3) and its inverse (D.4) can be used to considerably simplify
mathematical problems. For instance, a linear differential equation for the function
f .x/ is mapped onto a linear algebraic equation for Of .k/. The solution of the
differential equation is then obtained by back-transforming the solution Of .k/ of the
algebraic equation. Again, we refer to the literature for further applications and the
various properties of the transforms (D.3) and (D.4). Instead, let us concentrate on
the question of how to compute the FOURIER transform (D.2) numerically.
</p>
<p>It appears to be reasonable to start with the concepts developed in Chap. 3.3
</p>
<p>For this purpose, we assume that the function f .x/ is solely known on a grid of N
equidistant grid-points x`, ` D 0; : : : ;N � 1. In addition, we note that it is sufficient
to limit our discussion to 2�-periodic functions. Thus, we can choose our grid-
points to be x` D x0C`h where x0 D 0 and h D 2�=N, so that xN�1 D 2�.1�1=N/.
</p>
<p>Approximating the integral (D.2) with the help of the forward rectangular rule,
Chap. 3, yields
</p>
<p>Ofn D
1
</p>
<p>N
</p>
<p>N�1
X
</p>
<p>`D0
f` exp
</p>
<p>�
</p>
<p>�2�n`
N
</p>
<p>�
</p>
<p>C O.h2/: (D.5)
</p>
<p>It follows from this equation that the coefficients Ofn are periodic in n with period
N due to the finite number of grid-points. Hence, the maximal number of distinct
</p>
<p>2We work here with the asymmetric definition of the FOURIER transform. For other definitions,
the pre-factors have to be adapted consistently.
3If f .x/ is not periodic we have to truncate the integral (D.4) and restrict the integration to a suitable
finite interval so that the problem again reduces to the evaluation of Eq. (D.2).</p>
<p/>
</div>
<div class="page"><p/>
<p>D Fast Fourier Transform 359
</p>
<p>coefficients is equal to the number of grid-points. The inversion of Eq. (D.5) follows
directly from Eq. (D.1) and reads4
</p>
<p>f` D
N�1
X
</p>
<p>nD0
</p>
<p>Ofn exp
�
</p>
<p>i
2�n`
</p>
<p>N
</p>
<p>�
</p>
<p>: (D.6)
</p>
<p>The transforms (D.5) and (D.6) are referred to as the discrete FOURIER transform
(DFT) and its inverse, respectively. We cast these relations into matrix form by
defining vectors F D . f0; : : : ; fN�1/T and OF D . Of0; : : : ; OfN�1/T together with the
matrix W of elements:
</p>
<p>Wnm D !nmN : (D.7)
</p>
<p>Here, !N D exp
�
2� i
N
</p>
<p>�
</p>
<p>denotes the N-th root of unity. The transformation matrix
W is known as the FOURIER matrix or DFT matrix and it is easy to prove that its
inverse W�1 has the elements
</p>
<p>�
</p>
<p>W�1
�
</p>
<p>nm
D !�nmN : (D.8)
</p>
<p>All this allows to rewrite Eqs. (D.5) and (D.6) in compact form:
</p>
<p>OF D 1
N
</p>
<p>W�1F; and F D W OF: (D.9)
</p>
<p>Thus, we reduced the problem of numerically implementing the FOURIER
transform (D.2) to the task of multiplying the N � N complex matrix W with the N-
element vector F. This means that we have to perform N2 complex multiplications
and N.N � 1/ complex additions. However, the symmetry Wnm D Wmn already
suggests that there is further room for improvement. In fact, there are methods that
do much better and these algorithms are known as fast FOURIER transform (FFT)
algorithms.
</p>
<p>We limit our presentation to the version proposed by COOLEY and TUKEY [6, 25,
26] which is, with some variations, the most common algorithm. In its simplest form
it is based on the observation that one can always split the FOURIER transform (D.5)
into an even and an odd part
</p>
<p>Ofn D
1
</p>
<p>N
</p>
<p>N=2
X
</p>
<p>`D0
f2`!
</p>
<p>2n`
N C
</p>
<p>!nN
N
</p>
<p>N=2
X
</p>
<p>`D0
f2`C1!
</p>
<p>2n`
N ; (D.10)
</p>
<p>4It follows directly from the summation rule of the geometric series that
</p>
<p>N�1
X
</p>
<p>mD0
</p>
<p>exp
</p>
<p>�
</p>
<p>i
2�mn
</p>
<p>N
</p>
<p>�
</p>
<p>D Nın0:</p>
<p/>
</div>
<div class="page"><p/>
<p>360 D Fast Fourier Transform
</p>
<p>provided that N is even.5 Since !2kN D !nN=2, we can interpret Eq. (D.10) as the linear
combination of two FOURIER transforms of length N=2. Denoting the FOURIER
coefficients of the function values f2` on even grid-points by OAn and of the values
f2`C1 on odd grid-points by OBn, we obtain for n D 1; : : : ;N=2
</p>
<p>Ofn D OAn C !nN OBn: (D.11)
</p>
<p>We now make use of the property that the FOURIER coefficients are periodic, i.e.
OAnCN=2 D OAn, OBnCN=2 D OBn, and that !nCN=2N D �!nN . Thus, we can calculate the
remaining coefficients Ofn, n D N=2C 1; : : : ;N with the help of:
</p>
<p>OfnCN=2 D OAn � !nN OBn: (D.12)
</p>
<p>Because of Eqs. (D.11) and (D.12) the N FOURIER coefficients can be computed
as a linear combination of two FOURIER transforms of size N=2. The recursive
application of the very same scheme to OAn and OBn constitutes the core of the FFT
algorithm in its simplest variation [6]. It is also the efficiency of this algorithm that
makes the FOURIER transform an attractive tool for numerical calculations [27]. In
fact, there are several problems in this book where an algorithm based on FFT could
have been evoked. Let us discuss two examples in more detail in order to illustrate
this.
</p>
<p>(i) In Chap. 9.3 we could have solved the stationary inhomogeneous heat
equation for its FOURIER coefficients followed by the back-transform.6 Denoting
by OTn the FOURIER coefficients of the temperature T.x/ and by O�n the FOURIER
coefficients of the heat source/drain, we obtain from Eq. (9.20)
</p>
<p>OTn D �
O�n
</p>
<p>.n!/2
; (D.13)
</p>
<p>where ! D 2�=L. Performing the inverse FFT on Eq. (D.13) and adding the
homogeneous solution (9.4) immediately gives the required temperature profile
T.x/. In a similar fashion, FFT could have been used for solving the partial
differential equations discussed in Chap. 11. From the examples in this chapter, the
time-dependent SCHR&Ouml;DINGER equation serves as our second application.
</p>
<p>(ii) The Hamiltonian of a free point particle (for simplicity in one dimension) is
diagonal in momentum space, H D P2=2m, with the momentum operator (10.6).
Given the position-space representation of the initial state  .x; t/, we can then
</p>
<p>5This is not a limitation because we can always choose N to be even.
6Here we use the fact that the plane waves exp.in2�x=d/ form a complete, orthonormal basis of
the functions defined on a compact interval of length L [20].</p>
<p/>
</div>
<div class="page"><p/>
<p>D Fast Fourier Transform 361
</p>
<p>compute the time evolved wave packet  .x; t C
t/ for arbitrary
t &gt; 0 according
to Eq. (10.17) as
</p>
<p> .x; t C
t/ D 1
2�&bdquo;
</p>
<p>Z
</p>
<p>dp exp
</p>
<p>�
</p>
<p>� i
t&bdquo;
p2
</p>
<p>2m
C i&bdquo;px
</p>
<p>�
</p>
<p>O .p; t/; (D.14)
</p>
<p>where O .p; t/ is the momentum space representation of the initial state  .x; t/, i.e.
its FOURIER transform with k D p=&bdquo;. Hence, the time evolution of the free wave
packet is readily computed numerically with the help of the FFT and its inverse.
</p>
<p>It is now certainly interesting to investigate whether or not a similar approach
can be applied to solve Eq. (10.1) in the presence of a potential V.x/ which is
diagonal in position space. Although this can be achieved by solving the full
stationary eigenvalue problem (10.9) followed by the application of the eigenvector
expansion (10.17), we present here a more efficient but approximate solution valid
for small time steps 
t. In order to see this, we transform Eq. (D.14) into a slightly
more compact form. Denoting by F the FOURIER transform operator, O .p/ D
F .x/, we can write Eq. (D.14) as
</p>
<p> .x; t C
t/ D F�1U
tF .x; t/; (D.15)
</p>
<p>where U
t D exp
�
</p>
<p>�i
tp2=2&bdquo;m
�
</p>
<p>is the unitary time evolution operator for the
time interval 
t.7 The correct result can not be obtained by multiplying Eq. (D.15)
with the position-space time evolution of the potential V
t D exp.�i
tV.x/=&bdquo;/
because the operators V and P do not commute. However, by applying the BAKER-
CAMPBELL-HAUSDORFF formula8 [17, 19, 28], we can approximate the time
evolution  .x; t/ !  .x; t C
t/ for a small time step 
t by:
</p>
<p> .x; t C
t/ D V
tF�1U
tF .x; t/ C O.
t2/: (D.16)
</p>
<p>An even better approximation is obtained by the symmetrized form
</p>
<p> .x; t C
t/ D F�1U
t=2FV
tF�1U
t=2F .x; t/ C O.
t3/: (D.17)
</p>
<p>This method, known as the split operator technique [29], is a frequently used
method to numerically solve time dependent problems in quantum mechanics with
the help of FFT.
</p>
<p>7Ut is the momentum space representation of the free unitary time evolution operator U D
exp.�itP2=2&bdquo;m/.
8The BAKER-CAMPBELL-HAUSDORFF formula states how the exponential function exp.X C Y/
of two non-commuting operators X and Y can be expanded in terms of products of exponentials of
their commutators [17, 28].</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix E
Basics of Probability Theory
</p>
<p>E.1 Classical Definition
</p>
<p>It is the aim of this Appendix to summarize the most important definitions and
results from basic probability theory as required within this book. For a more in
depth presentation we refer to the literature [30&ndash;34].
</p>
<p>The classical probability P.A/ for an event A is defined by the number of
favorable results n, divided by the number of possible results m,
</p>
<p>P.A/ D n
m
: (E.1)
</p>
<p>For two events A and B we can deduce the following rules1
</p>
<p>P.A _ B/ D P.A/C P.B/ � P.A ^ B/ ; (E.2a)
</p>
<p>P.Z/ D 0 impossible event; Z . . . zero element ; (E.2b)
</p>
<p>P.I/ D 1 certain event; I . . . identity element ; (E.2c)
</p>
<p>0 � P.A/ � 1 ; (E.2d)
</p>
<p>P.AjB/ D P.A ^ B/
P.B/
</p>
<p>; (E.2e)
</p>
<p>1Here we use the symbols _ and ^ to denote the Boolean operators OR and AND, respectively.
</p>
<p>&copy; Springer International Publishing Switzerland 2016
B.A. Stickler, E. Schachinger, Basic Concepts in Computational Physics,
DOI 10.1007/978-3-319-27265-8
</p>
<p>363</p>
<p/>
</div>
<div class="page"><p/>
<p>364 E Basics of Probability Theory
</p>
<p>where P.AjB/ is the probability for the event A under the constraint that event B is
true. Moreover, if A is the complementary event2 to A we have
</p>
<p>P.A/ D 1� P.A/ : (E.3)
</p>
<p>The statistical definition of the probability for an event A is given by:
</p>
<p>P.A/ D lim
m!1
</p>
<p>n
</p>
<p>m
: (E.4)
</p>
<p>E.2 Random Variables and Moments
</p>
<p>A random variable is a functional which assigns to an event ! a real number x from
the set of possible outcomes ˝: x D X.!/ [31].3 Roughly speaking it is a variable
whose value is assigned to the observation of some random process. The mean value
of a discrete random variable X is defined by
</p>
<p>hXi D
X
</p>
<p>!2˝
X.!/P! ; (E.5)
</p>
<p>where P! is the probability for the event !. For instance, in case of a dice-throw
X.!/ � n D 1; 2; : : : ; 6.
</p>
<p>We restrict ourselves now to discrete random variables and, thus, x can only
take on discrete values. Furthermore, we introduce the function of random variables
Y D f .X/ and define quite generally its mean value:
</p>
<p>h f .X/i � h f i D
X
</p>
<p>i
</p>
<p>f .xi/Pi : (E.6)
</p>
<p>Note that
</p>
<p>h1i �
X
</p>
<p>i
</p>
<p>Pi D 1 : (E.7)
</p>
<p>Moments of order k of a random variable X are defined by
</p>
<p>mk WD
˝
</p>
<p>Xk
˛
</p>
<p>; (E.8)
</p>
<p>2This means that A _ A D I and A ^ A D Z.
3A more exact formulation will follow in the course of this Appendix.</p>
<p/>
</div>
<div class="page"><p/>
<p>E Basics of Probability Theory 365
</p>
<p>and central moments are introduced via the relation
</p>
<p>�k WD
˝
</p>
<p>.
X/k
˛
</p>
<p>D
˝
</p>
<p>.X � hXi/k
˛
</p>
<p>: (E.9)
</p>
<p>Of particular interest is the second central moment, the variance:
</p>
<p>var .X/ WD
˝
</p>
<p>.X � hXi/2
˛
</p>
<p>D
˝
</p>
<p>X2
˛
</p>
<p>� hXi2 : (E.10)
</p>
<p>Finally, the standard deviation � is defined as the square root of the variance:
</p>
<p>� WD std.X/ D
p
</p>
<p>var .X/ : (E.11)
</p>
<p>We study now a discrete set of observations xi where i D 1; : : : ;N. Then the sample
mean value is given by
</p>
<p>x D 1
N
</p>
<p>X
</p>
<p>i
</p>
<p>xi ; (E.12)
</p>
<p>and the error (standard deviation) of x (standard error) can be determined from:
</p>
<p>var .x / D var
 
</p>
<p>1
</p>
<p>N
</p>
<p>X
</p>
<p>i
</p>
<p>xi
</p>
<p>!
</p>
<p>D �
2
</p>
<p>N
: (E.13)
</p>
<p>We assumed here the xi to be uncorrelated with the consequence that cov
�
</p>
<p>xi; xj
�
</p>
<p>D
var .xi/ ıij [defined in Eq. (E.16)]. Therefore,
</p>
<p>standard error D �x D
�p
N
; (E.14)
</p>
<p>where � is the standard deviation of the observations as defined above.
In the case of multiple random variables we can proceed as above. For instance,
</p>
<p>the expectation value of a function of two random variables is given by
</p>
<p>h f .X;Y/i WD
X
</p>
<p>i;j
</p>
<p>f .xi; yj/Pij ; (E.15)
</p>
<p>and the covariance between two random variables:
</p>
<p>cov .X;Y/ WD h.X � hXi/.Y � hYi/i D hXYi � hXi hYi : (E.16)</p>
<p/>
</div>
<div class="page"><p/>
<p>366 E Basics of Probability Theory
</p>
<p>Fig. E.1 Uncorrelated (left panel) and positively correlated (right panel) variables X and Y
</p>
<p>The value of the covariance together with its sign determines important properties
of the random variables X and Y in their relation to each other:
</p>
<p>cov .X;Y/ D
</p>
<p>8
</p>
<p>ˆ̂
ˆ̂
ˆ̂
ˆ̂
</p>
<p>&lt;̂
</p>
<p>ˆ̂
ˆ̂
ˆ̂
ˆ̂
</p>
<p>:̂
</p>
<p>&gt; 0 for Y � hYi &gt; 0) X � hXi &gt; 0 ;
(positive linear correlation)
</p>
<p>&lt; 0 for Y � hYi &gt; 0) X � hXi &lt; 0 ;
&lt; 0 for X � hXi &gt; 0) Y � hYi &lt; 0 ;
</p>
<p>(negative linear correlation)
</p>
<p>D 0 no linear dependence between X and Y .
</p>
<p>(E.17)
</p>
<p>Random variables whose covariance is zero are called uncorrelated. [This property
was used in the derivation of Eq. (E.13).] To give an example, Fig. E.1 compares
schematically uncorrelated and positively correlated random variables X and Y.
</p>
<p>E.3 Binomial Distribution and Limit Theorems
</p>
<p>The binomial distribution is given by
</p>
<p>P.kjn; p/ D
 
</p>
<p>n
</p>
<p>k
</p>
<p>!
</p>
<p>pk.1 � p/n�k ; (E.18)
</p>
<p>where
�
</p>
<p>n
</p>
<p>k
</p>
<p>�
</p>
<p>is the binomial coefficient
</p>
<p> 
</p>
<p>n
</p>
<p>k
</p>
<p>!
</p>
<p>D nŠ
kŠ.n � k/Š : (E.19)</p>
<p/>
</div>
<div class="page"><p/>
<p>E Basics of Probability Theory 367
</p>
<p>For large values of n STIRLING&rsquo;s approximation can be applied to calculate an
estimate of nŠ:
</p>
<p>nŠ D nnC 12 e�n
p
2�
�
</p>
<p>1C O.n�1/
�
</p>
<p>: (E.20)
</p>
<p>Furthermore, it is easy to prove that the mean value and the variance of the binomial
distribution are given by
</p>
<p>hki D np ; (E.21)
var .k/ D np.1� p/ : (E.22)
</p>
<p>The DE MOIVRE-LAPLACE theorem states that for var.k/� 1
</p>
<p>P.kjn; p/ � g.kjk0; �/ D
1p
2��2
</p>
<p>exp
</p>
<p>�
</p>
<p>� .k � k0/
2
</p>
<p>2�2
</p>
<p>�
</p>
<p>; (E.23)
</p>
<p>where k0 D hki and � D
p
</p>
<p>var .k/. We can also deduce that
</p>
<p>P.k D npjn; p/ D 1p
2�np.1� p/
</p>
<p>�! 0 ; (E.24)
</p>
<p>for n ! 1. From this, BERNOULLI&rsquo;s law of large numbers follows
</p>
<p>P.jk=n � pj &lt; �jn; p/! 1 8 � &gt; 0 : (E.25)
</p>
<p>E.4 POISSON Distribution and Counting Experiments
</p>
<p>If the mean expectation value � is independent of the number of experiments n, i.e.
np D � � const, it follows from Eq. (E.18) that
</p>
<p>lim
n!1
</p>
<p>P
�
</p>
<p>k
</p>
<p>ˇ
ˇ
ˇn; p D
</p>
<p>�
</p>
<p>n
</p>
<p>�
</p>
<p>D exp.��/�
k
</p>
<p>kŠ
DW P.kj�/ : (E.26)
</p>
<p>The distribution P.kj�/ is called POISSON distribution. We obtain for the POISSON
distribution:
</p>
<p>hki D � ; (E.27)
var .k/ D � : (E.28)</p>
<p/>
</div>
<div class="page"><p/>
<p>368 E Basics of Probability Theory
</p>
<p>It is important to note that counting experiments, as for instance radioactive decay,
follow the Poisson statistics. A typical counting experiment observes within the
time interval t (in average) � events. This time interval is now divided into n sub-
intervals with 
t D t=n. If the counting events can be assumed to be independent,
the process follows a binomial distribution and we have � D np. This is equivalent
to p D �=n. We return to the case of radioactive decay: We count � signals within
one minute which are uniformly distributed over the time interval. The experiment
is now reduced to a time interval of one second and the probability of detecting a
signal consequently reduces to�=60. For p � 1 but np =�1 the binomial distribution
P.kjn; p/ can be approximated by P.kj�/ and we can use for large values of �
</p>
<p>P.kj�/ D 1p
2��2
</p>
<p>exp
</p>
<p>�
</p>
<p>� .k � �/
2
</p>
<p>2�2
</p>
<p>�
</p>
<p>; (E.29)
</p>
<p>with
</p>
<p>� D p� �
p
</p>
<p>k : (E.30)
</p>
<p>In most experimentally relevant cases is � unknown and is approximated by:
</p>
<p>� D k ˙
p
</p>
<p>k : (E.31)
</p>
<p>E.5 Continuous Variables
</p>
<p>We define the cumulative distribution function (cdf) [31, 32], F.x/, of a continuous
variable x by4
</p>
<p>F.x/ WD P.X � xjB/ ; (E.32)
</p>
<p>where B is a generalized condition (condition complex). Moreover, we define the
probability density function (pdf), p.x/ by
</p>
<p>p.x/ D d
dx
</p>
<p>F.x/ : (E.33)
</p>
<p>It follows that
</p>
<p>p.x/dx D ŒF.x C dx/ � F.x/&#141; ŠD P.x � X � x C dxjB/ : (E.34)
</p>
<p>4For convenience we use here the notation F.x/ for the cumulative distribution function in contrast
to the notation P.x/ used throughout the second part of this book.</p>
<p/>
</div>
<div class="page"><p/>
<p>E Basics of Probability Theory 369
</p>
<p>Hence,
</p>
<p>F.x/ D
Z x
</p>
<p>�1
dx0p.x0/ : (E.35)
</p>
<p>Note that the pdf is normalized
</p>
<p>Z
</p>
<p>dx0p.x0/ D F.1/ D P.X � 1jB/ D 1 ; (E.36)
</p>
<p>and non-negative
</p>
<p>p.x/ � 0 : (E.37)
</p>
<p>E.6 BAYES&rsquo; Theorem
</p>
<p>We regard a set of discrete events Ai under the generalized condition B. Then we
have the normalization condition
</p>
<p>X
</p>
<p>i
</p>
<p>P.AijB/ D 1 ; (E.38)
</p>
<p>and the marginalization rule
</p>
<p>P.BjB/ D
X
</p>
<p>i
</p>
<p>P.BjAi;B/P.AijB/ : (E.39)
</p>
<p>BAYES&rsquo; theorem [33, 35] for discrete variables follows from Eq. (E.2e) since P.A^
B/ D P.B ^ A/:
</p>
<p>P.AjB;B/ D P.BjA;B/P.AjB/
P.BjB/ : (E.40)
</p>
<p>In case of continuous variables the above equations modify accordingly. The
marginalization and BAYES&rsquo; theorem for pdfs are then given by
</p>
<p>P.BjB/ D
Z
</p>
<p>dxP.Bjx;B/p.xjB/ ; (E.41)
</p>
<p>and
</p>
<p>p.yjx;B/ D p.xjy;B/p.yjB/
p.xjB/ : (E.42)</p>
<p/>
</div>
<div class="page"><p/>
<p>370 E Basics of Probability Theory
</p>
<p>E.7 Normal Distribution
</p>
<p>The normal distribution (GAUSS distribution) is defined by the pdf:
</p>
<p>p.x/ D N .xjx0; �/ D
1p
2��2
</p>
<p>exp
</p>
<p>�
</p>
<p>� .x � x0/
2
</p>
<p>2�2
</p>
<p>�
</p>
<p>: (E.43)
</p>
<p>The corresponding cdf
</p>
<p>F.x/ D 1p
2��2
</p>
<p>Z x
</p>
<p>�1
dx0 exp
</p>
<p>�
</p>
<p>� .x
0 � x0/2
2�2
</p>
<p>�
</p>
<p>D ˚
�x � x0
</p>
<p>�
</p>
<p>�
</p>
<p>D 1
2
C 1
2
erf
</p>
<p>�
x � x0p
2�2
</p>
<p>�
</p>
<p>; (E.44)
</p>
<p>follows. Here ˚.x/ is given by
</p>
<p>˚.x/ D 1p
2�
</p>
<p>Z x
</p>
<p>�1
dx0e�x
</p>
<p>02=2 ; (E.45)
</p>
<p>and erf.x/ is the error function [36, 37]:
</p>
<p>erf.x/ D 2p
�
</p>
<p>Z x
</p>
<p>0
</p>
<p>dx0e�x
02
</p>
<p>: (E.46)
</p>
<p>Furthermore, we obtain
</p>
<p>hxi D x0 ; (E.47)
</p>
<p>var .x/ D �2 : (E.48)
</p>
<p>E.8 Central Limit Theorem
</p>
<p>Let S denote a random variable defined by
</p>
<p>S D
N
X
</p>
<p>iD1
ciXi ; (E.49)
</p>
<p>where the Xi are independent and identically distributed random numbers with mean
� and variance �2 and
</p>
<p>lim
N!1
</p>
<p>1
</p>
<p>N
</p>
<p>N
X
</p>
<p>iD1
cki D const ; 8k 2 Z : (E.50)</p>
<p/>
</div>
<div class="page"><p/>
<p>E Basics of Probability Theory 371
</p>
<p>Then,
</p>
<p>p.SjN;B/ � N ŒSj hSi ; var .S/&#141; ; (E.51)
</p>
<p>with
</p>
<p>hSi D �
NX
</p>
<p>iD1
ci ; (E.52)
</p>
<p>and
</p>
<p>var .S/ D �2
NX
</p>
<p>iD1
c2i ; (E.53)
</p>
<p>for large values of N. The theorem of DE MOIVRE-LAPLACE is a special case of the
central limit theorem, with the result that the Xi are binomial distributed.
</p>
<p>E.9 Characteristic Function
</p>
<p>The characteristic function G.k/ of a stochastic variable X is defined by [31, 32]
</p>
<p>G.k/ D
˝
</p>
<p>eikX
˛
</p>
<p>D
Z
</p>
<p>I
</p>
<p>dxeikxp.x/ ; (E.54)
</p>
<p>where I denotes the range of the pdf p.x/. It follows that
</p>
<p>G.0/ D 1 and jG.k/j � 1 : (E.55)
</p>
<p>Expanding Eq. (E.54) in a Taylor series with respect to k yields
</p>
<p>G.k/ D
X
</p>
<p>m
</p>
<p>.ik/m
</p>
<p>mŠ
</p>
<p>Z
</p>
<p>I
</p>
<p>dx xmp.x/ �
X
</p>
<p>m
</p>
<p>.ik/m
</p>
<p>mŠ
hXmi : (E.56)
</p>
<p>Hence, the characteristic function is a moment generating function.
</p>
<p>E.10 The Correlation Coefficient
</p>
<p>We shall briefly define and discuss the correlation coefficient. Two random variables
X and Y form a random vector Z D .X;Y/ which follows the pdf p.Z/ D p.X;Y/
with the normalization
</p>
<p>Z
</p>
<p>dxdy p.x; y/ D 1 : (E.57)</p>
<p/>
</div>
<div class="page"><p/>
<p>372 E Basics of Probability Theory
</p>
<p>The correlation coefficient r is now defined as
</p>
<p>r D cov .X;Y/p
var .X/ var .Y/
</p>
<p>; (E.58)
</p>
<p>where cov .X;Y/ is the covariance (E.16) of X and Y while var .�/ denotes the
variance (E.10) of the respective argument. It follows from the CAUCHY-SCHWARZ
inequality that 0 � r2 � 1 and, therefore, �1 � r � 1.5
</p>
<p>The random variables X and Y are said to be the stronger correlated the bigger
r2 becomes because for statistically independent (uncorrelated) variables we have
p.x; y/ D q1.x/q2.y/ with the consequence that cov .X;Y/ D 0 and, thus, r D 0.
</p>
<p>The definition of the correlation coefficient is commonly motivated by the
problem of linear regression: Suppose we have a set of data points Y associated
with data points X. We would like to find a linear function f .X/ D a C bX which
approximates the data points Y as good as possible. The problem may be stated as
</p>
<p>˝
</p>
<p>ŒY � f .X/&#141;2
˛
</p>
<p>D
˝
</p>
<p>.Y � a � bX/2
˛
</p>
<p>! min ; (E.60)
</p>
<p>where a and b are real constants. This corresponds to GAUSS&rsquo;s method of minimiz-
ing the square of errors. We have
</p>
<p>@
</p>
<p>@a
</p>
<p>˝
</p>
<p>.Y � a � bX/2
˛
</p>
<p>D �2 hY � a � bXi D 0 ; (E.61)
</p>
<p>and
</p>
<p>@
</p>
<p>@b
</p>
<p>˝
</p>
<p>.Y � a � bX/2
˛
</p>
<p>D �2 h.Y � a � bX/Xi D 0 : (E.62)
</p>
<p>Equations (E.61) and (E.62) result in:
</p>
<p>a C b hXi D hYi ; (E.63)
a hXi C b
</p>
<p>˝
</p>
<p>X2
˛
</p>
<p>D hXYi : (E.64)
</p>
<p>Both are easily solved for a and b and one obtains
</p>
<p>a D hYi � b hXi ; (E.65)
</p>
<p>5One defines the scalar product between random variables .X; Y/ D cov .X; Y/ and therefore
kXk2 D .X;X/ D var .X/. The CAUCHY-SCHWARZ inequality reads
</p>
<p>j.X; Y/j2 �k X k2k Y k2 ; (E.59)
</p>
<p>and therefore 0 � r2 � 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>E Basics of Probability Theory 373
</p>
<p>where
</p>
<p>b D hXYi � hXi hYi
hX2i � hXi2
</p>
<p>D cov .X;Y/
var .X/
</p>
<p>: (E.66)
</p>
<p>Thus, the linear function f .X/ which approximates the data points Y optimally is
given by
</p>
<p>f .X/ D hYi � cov .X;Y/
var .X/
</p>
<p>.X � hXi/
</p>
<p>D hYi � r
s
</p>
<p>var .Y/
</p>
<p>var .X/
.X � hXi/ ; (E.67)
</p>
<p>and it follows immediately for the squared error:
</p>
<p>˝
</p>
<p>Œy � F.x/&#141;2
˛
</p>
<p>D var .Y/ .1 � r2/ : (E.68)
</p>
<p>Hence, the best result is achieved for r D ˙1 in which case the association of the
data points Y with the data points X is really linear while the worst result is found
when r D 0 (no association what so ever).
</p>
<p>E.11 Stable Distributions
</p>
<p>A stable distribution is a distribution which reproduces itself [32]. In particular,
consider two random variables X1 and X2 which are independent copies of the
random variable X following the distribution pX .6
</p>
<p>The pdf pX is referred to as a stable distribution if for arbitrary constants a and
b the random variable aX1 C bX2 has the same distribution as the random variable
cX C d for some positive c and some d 2 R.
</p>
<p>For this case one can write down the characteristic function analytically. We will
give a special case, the so called symmetric L&Eacute;VY distributions [38]:
</p>
<p>G˛.k/ D exp .�� jkj˛/ : (E.69)
</p>
<p>Here � &gt; 0 and 0 &lt; ˛ � 2. The pdf of such a distribution shows the asymptotic
behavior
</p>
<p>p˛.x/ /
˛
</p>
<p>jxj1C˛ ; jxj ! 1 : (E.70)
</p>
<p>6Independent copies of a random variable, are random variables, which are independent and follow
the same distribution as the original random variable.</p>
<p/>
</div>
<div class="page"><p/>
<p>374 E Basics of Probability Theory
</p>
<p>The normal distribution follows from Eq. (E.69) for ˛ D 2. Moreover, we
observe from Eq. (E.70) that the variance diverges for all ˛ &lt; 2. However,
the existence of the variance was the criterion for the validity of the central
limit theorem formulated in Sect. E.8. We note that stable distributions reproduce
themselves and are attractors for sums of independent identical distributed random
variables. This is referred to as the generalized central limit theorem.
</p>
<p>We remark, in conclusion, that for ˛ D 1 the CAUCHY distribution results from
Eq. (E.69), and note that stable distributions are also referred to as L&Eacute;VY ˛-stable
distributions [32].</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix F
Phase Transitions
</p>
<p>F.1 Some Basics
</p>
<p>In many systems transitions between different phases can be observed if an external
parameter, such as the temperature or the particle density, changes. Familiar
examples are the liquid-gaseous phase transition or the ferromagnetic-paramagnetic
transition. The two phases exhibit different properties and often develop a different
physical structure, like in disorder-to-order transitions. This suggests the intro-
duction of an order parameter ' which is zero in one phase and takes on some
finite value ' &curren; 0 in the other one.1 For instance, in the case of paramagnetic-
ferromagnetic transitions the magnetization plays the role of the order parameter
[43].
</p>
<p>In order to classify phase transitions we briefly repeat some basics from statistical
mechanics [39&ndash;42, 44, 45]. In a canonical ensemble the probability to find the
system in micro-state r (as a function of the external parameters temperature T,
volume V and number of particles N) is proportional to the BOLTZMANN-factor
</p>
<p>Pr.T;V;N/ D
1
</p>
<p>Z.T;V;N/
exp Œ�ˇEr.V;N/&#141; : (F.1)
</p>
<p>Here, ˇ D 1=.kBT/, where kB is the BOLTZMANN constant, T is the temperature,
and Er is the energy of micro-state r. The canonical partition function Z.T;V;N/,
</p>
<p>Z.T;V;N/ D
X
</p>
<p>r
</p>
<p>exp Œ�ˇEr.V;N/&#141; ; (F.2)
</p>
<p>1The choice of the order parameter may not be unique [39&ndash;42].
</p>
<p>&copy; Springer International Publishing Switzerland 2016
B.A. Stickler, E. Schachinger, Basic Concepts in Computational Physics,
DOI 10.1007/978-3-319-27265-8
</p>
<p>375</p>
<p/>
</div>
<div class="page"><p/>
<p>376 F Phase Transitions
</p>
<p>ensures the normalization of Pr.T;V;N/ and determines the free energy F.T;V;N/
according to
</p>
<p>F.T;V;N/ D � 1
ˇ
lnZ.T;V;N/ : (F.3)
</p>
<p>The EHRENFEST classification [46] of phase transitions is based on the behavior
of F near the transition point: If F is a continuous function of its variables at
the transition point and its first derivative with respect to some thermodynamic
variable is discontinuous we call it a first order phase transition. For instance,
transitions from the liquid to the gaseous phase are classified as first order phase
transitions because the density, which is proportional to the first derivative of the
free energy with respect to the chemical potential, changes discontinuously at the
boiling temperature T D TB. We remark the following characteristics of first order
phase transitions:
</p>
<p>1. The transition involves a latent heat 
Q: The system absorbs or releases energy.
A familiar example is the latent heat of fusion in the case of melting or freezing.
</p>
<p>2. Both phases can coexist at the transition point.
3. A metastable phase can be observed.
</p>
<p>In a second order phase transition the first derivative of the free energy F with
respect to some thermodynamic variable is continuous but the second derivative
of F exhibits a discontinuity. For instance, in a ferromagnetic phase transition the
magnetization (first derivative of F with respect to the external magnetic field B)
changes continuously while the magnetic susceptibility � (the second derivative of
F with respect to B) is discontinuous at the CURIE temperature Tc [43].
</p>
<p>The modern classification is based on the behavior of the order parameter near
the critical point. The order parameter changes discontinuously for first order
phase transitions while it changes continuously for second and higher order phase
transitions. Second order transitions are typically related to spontaneous symmetry
breaking, as for instance in the paramagnetic-ferromagnetic transition. Based on
this observation, LANDAU developed a general description of second order phase
transitions which we briefly discuss in the following section.
</p>
<p>F.2 LANDAU Theory
</p>
<p>We regard a second order phase transition characterized by the scalar order
parameter ' [47]. Since h'i changes continuously at T D Tc, it is convenient to
define ' in such a way that h'i jT�Tc D 0 while h'i jT&lt;Tc &curren; 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>F Phase Transitions 377
</p>
<p>For the free energy F.T; h; '/, one chooses
</p>
<p>F.T; h; '/ D F0.T/C V
�
</p>
<p>a.T � Tc/
2
</p>
<p>'2 C b
4
'4 � h'
</p>
<p>�
</p>
<p>; (F.4)
</p>
<p>where a and b are some material constants and h denotes the external field. This
ansatz is motivated by the theory of the paramagnetic-ferromagnetic phase transition
[43]. Thus, in equilibrium we have
</p>
<p>ıF
</p>
<p>ı'
D 0 ; (F.5)
</p>
<p>which results in
</p>
<p>a.T � Tc/' C b'3 D h : (F.6)
</p>
<p>For h D 0 and T &lt; Tc we obtain
</p>
<p>h'0i D
r
</p>
<p>a
</p>
<p>b
.Tc � T/ � .Tc � T/&#13; ; (F.7)
</p>
<p>where &#13; D 1=2 is called the critical exponent. For T � Tc we have h'0i D 0. We
now regard a weak external field h. The order parameter will change
</p>
<p>' D h'0i C ı' : (F.8)
</p>
<p>Again, we obtain for equilibrium:
</p>
<p>ıF
</p>
<p>ı'
D a.T � Tc/.h'0i C ı'/C b.h'0i C ı'/3 � h D 0 : (F.9)
</p>
<p>Neglecting contributions of order O.ı'2/ yields for the susceptibility
</p>
<p>� D @
@h
</p>
<p>h'i D hı'i
h
</p>
<p>� jT � Tcjı ; (F.10)
</p>
<p>where ı D �1 is a second critical exponent. This is the CURIE-WEISS law [43].
Finally for T D Tc we obtain from Eq. (F.6)
</p>
<p>' D
�
</p>
<p>h
</p>
<p>b
</p>
<p>� 1
3
</p>
<p>� h 1� ; (F.11)</p>
<p/>
</div>
<div class="page"><p/>
<p>378 F Phase Transitions
</p>
<p>with the third critical exponent �. The LANDAU theory is a mean-field approxima-
tion since local fluctuations of the order parameter are neglected.2 Although the
critical exponents obtained with LANDAU&rsquo;s approach deviate from experimental
values, the theory is qualitatively correct. We remark that the critical exponents are
universal (a property referred to as universality [40]) as they depend only on the
dimensionality and the symmetry of the interaction.
</p>
<p>2The extension to space dependent order parameters is referred to as GINZBURG-LANDAU theory
[48].</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix G
Fractional Integrals and Derivatives in 1D
</p>
<p>This section introduces briefly the common definitions and notations associated with
fractional calculus in one dimension [49].
</p>
<p>The RIEMANN-LIOUVILLE fractional integrals of order ˛ 2 C ŒR.˛/ &gt; 0&#141;,
I˛aCf .x/ and I
</p>
<p>˛
b�f .x/ on a finite interval Œa; b&#141; on the real axis R are given by
</p>
<p>I˛aCf .x/ WD
1
</p>
<p>� .˛/
</p>
<p>Z x
</p>
<p>a
</p>
<p>dx0
f .x0/
</p>
<p>.x � x0/1�˛ for .x &gt; a; R.˛/ &gt; 0/; (G.1a)
</p>
<p>I˛b�f .x/ WD
1
</p>
<p>� .˛/
</p>
<p>Z b
</p>
<p>x
</p>
<p>dx0
f .x0/
</p>
<p>.x0 � x/1�˛ for .x &lt; b; R.˛/ &gt; 0/ ; (G.1b)
</p>
<p>where � .x/ denotes the Gamma function [36, 37], R.˛/ is the real part of ˛, and
f .x/ is a sufficiently well behaved continuous, differentiable function for which
the integrals in (G.1) exist. The corresponding RIEMANN-LIOUVILLE fractional
derivatives D˛aCf .x/ and D
</p>
<p>˛
b�f .x/ of order ˛ 2 C ŒR.˛/ � 0&#141; are defined by
</p>
<p>D˛aCf .x/ WD
�
</p>
<p>d
</p>
<p>dx
</p>
<p>�n
</p>
<p>.In�˛aC f /.x/
</p>
<p>D 1
� .n � ˛/
</p>
<p>�
d
</p>
<p>dx
</p>
<p>�n Z x
</p>
<p>a
</p>
<p>dx0
f .x0/
</p>
<p>.x � x0/˛�nC1 for x &gt; a ; (G.2a)
</p>
<p>and
</p>
<p>D˛b�f .x/ WD
�
</p>
<p>� d
dx
</p>
<p>�n
</p>
<p>.In�˛b� f /.x/
</p>
<p>D 1
� .n � ˛/
</p>
<p>�
</p>
<p>� d
dx
</p>
<p>�n Z b
</p>
<p>x
</p>
<p>dx0
f .x0/
</p>
<p>.x0 � x/˛�nC1 for x &lt; b ; (G.2b)
</p>
<p>&copy; Springer International Publishing Switzerland 2016
B.A. Stickler, E. Schachinger, Basic Concepts in Computational Physics,
DOI 10.1007/978-3-319-27265-8
</p>
<p>379</p>
<p/>
</div>
<div class="page"><p/>
<p>380 G Fractional Integrals and Derivatives in 1D
</p>
<p>with n D ŒR.˛/&#141;C 1. Here ŒR.˛/&#141; denotes the integer part of R.˛/. For a ! �1
and b ! C1 the RIEMANN-LIOUVILLE fractional integrals and derivatives are
referred to as WEYL fractional integrals and derivatives. In what follows, they will
be denoted by I˛˙ and D
</p>
<p>˛
˙, respectively.
</p>
<p>If ˛ 2 C ŒR.˛/ � 0&#141; and Œa; b&#141; 2 R is a finite interval, then the left- and right-
sided CAPUTO fractional derivatives CD˛aCf .x/ and
</p>
<p>CD˛b�f .x/ are defined by
</p>
<p>CD˛aCf .x/ D D˛aCf .x/ �
n�1
X
</p>
<p>kD0
</p>
<p>f .k/.a/
</p>
<p>� .k � ˛ C 1/.x � a/
k�˛ ; (G.3a)
</p>
<p>and
</p>
<p>CD˛b�f .x/ D D˛b�f .x/ �
n�1
X
</p>
<p>kD0
</p>
<p>.�1/kf .k/.b/
� .k � ˛ C 1/.b � x/
</p>
<p>k�˛ ; (G.3b)
</p>
<p>with
</p>
<p>n D
(
</p>
<p>ŒR.˛/&#141;C 1 ˛ &hellip; N ;
˛ ˛ 2 N0 :
</p>
<p>(G.3c)
</p>
<p>This is, however, equivalent to
</p>
<p>CD˛aCf .x/ D
1
</p>
<p>� .n � ˛/
</p>
<p>Z x
</p>
<p>a
</p>
<p>dx0
f .n/.x0/
</p>
<p>.x � x0/˛�nC1
</p>
<p>D .In�˛aC Dnf /.x/ ; (G.4a)
</p>
<p>and
</p>
<p>CD˛b�f .x/ D
.�1/n
</p>
<p>� .n � ˛/
</p>
<p>Z b
</p>
<p>x
</p>
<p>dx0
f .n/.x0/
</p>
<p>.x0 � x/˛�nC1
</p>
<p>D .�1/n.In�˛b� Dnf /.x/ : (G.4b)
</p>
<p>The symmetric fractional integrals I˛jxj and derivatives D
˛
jxj are referred to as
</p>
<p>RIESZ fractional integrals or derivatives and are of the form
</p>
<p>I˛jxj D
I˛C C I˛�
2 cos
</p>
<p>�
˛�
2
</p>
<p>� ; (G.5)
</p>
<p>for ˛ 2 .0; 1/ and
</p>
<p>D
˛
jxj D
</p>
<p>8
</p>
<p>ˆ̂
&lt;
</p>
<p>ˆ̂
:
</p>
<p>.�1/ n2 D
˛
C
CD˛�
</p>
<p>2 cos .˛�=2/ for n D ŒR.˛/&#141;C 1 � 2k; k 2 N0 ;
</p>
<p>.�1/ n�12 D
˛
C
�D˛�
</p>
<p>2 sin .˛�=2/ for n D ŒR.˛/&#141;C 1 � 2k C 1; k 2 N0 :
(G.6)</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix H
Least Squares Fit
</p>
<p>H.1 Motivation
</p>
<p>In numerous physics applications a set of corresponding data points .xk; yk/ was
measured or calculated and a set of certain parameters f˛jg characterizing a function
f .xk; f˛jg/ is to be determined in such a way that
</p>
<p>�2 D
X
</p>
<p>k
</p>
<p>ck
�
</p>
<p>yk � f .xk; f˛jg/
�2 ! min : (H.1)
</p>
<p>This is referred to as a least squares fit problem [6, 7]. Here, ck � 0 are weights,
which indicate the relevance of a certain data point .xk; yk/ for the fitting routine, and
f .x; f˛jg/ is referred to as the model function. Besides numerous applications within
the context of experimentally obtained data points, we already came across such a
problem in our discussion of data analysis in Chap. 19. Here it was of interest to
determine the experimental auto-correlation time by fitting an exponential function
to the measured auto-correlation coefficientA.k/, discussed in Sect. 19.3. Hence, we
note that in many applications the parameters f˛jg can be associated with a physical
property of interest.
</p>
<p>We distinguish between two different cases: (i) the function f .xk; f˛jg/ is a linear
function of the parameters f˛jg and (ii) the function f .xk; f˛jg/ is not linear in its
parameters f˛jg. It should be emphasized that in both cases the function does not
need to be linear in xk. This section will discuss methods for linear as well as non-
linear least squares fits. However, before proceeding some comments on the data
points fykg seem to be required.
</p>
<p>&copy; Springer International Publishing Switzerland 2016
B.A. Stickler, E. Schachinger, Basic Concepts in Computational Physics,
DOI 10.1007/978-3-319-27265-8
</p>
<p>381</p>
<p/>
</div>
<div class="page"><p/>
<p>382 H Least Squares Fit
</p>
<p>Suppose the points .xk; yk/ stem from a measurement which has been repeated
N-times. In this case for every value xk we have N different values fy jkg and we may
use the arithmetic mean
</p>
<p>yk D
1
</p>
<p>N
</p>
<p>X
</p>
<p>j
</p>
<p>y
j
</p>
<p>k ; (H.2)
</p>
<p>instead of yk in expression (H.1). We may also calculate the variance var .yk/ via1
</p>
<p>var .yk/ D
1
</p>
<p>N
</p>
<p>X
</p>
<p>j
</p>
<p>.y
j
</p>
<p>k � yk/2 : (H.3)
</p>
<p>If we assume that the data points y jk follow a normal distribution with mean hyki
and variance var .yk/ we may proceed in the following way: The weights ck are
chosen as
</p>
<p>ck D
1
</p>
<p>var .yk/
: (H.4)
</p>
<p>The resulting fit parameters f˛jg are then regarded as mean values of parameters
where the variances var .˛i/ as well as the covariances cov
</p>
<p>�
</p>
<p>˛i; ˛j
�
</p>
<p>can be obtained
from the matrix
</p>
<p>Nij D
1
</p>
<p>2
</p>
<p>@2�2
</p>
<p>@˛i@˛j
; (H.5)
</p>
<p>via inversion, i.e.
</p>
<p>C D N�1 ; (H.6)
</p>
<p>and
</p>
<p>Cij D cov
�
</p>
<p>˛i; ˛j
�
</p>
<p>: (H.7)
</p>
<p>The matrix C is commonly referred to as covariance matrix.
</p>
<p>1In many cases one employs the bias corrected variance var .yk/B D NN�1var .yk/. For a detailed
discussion of the bias corrected variance the interested reader is encouraged to consult a statistics
textbook [45, 50&ndash;53].</p>
<p/>
</div>
<div class="page"><p/>
<p>H Least Squares Fit 383
</p>
<p>H.2 Linear Least Squares Fit
</p>
<p>In this particular case the model function f .xk; f˛jg/ is defined as
</p>
<p>f .xk; f˛jg/ D
X
</p>
<p>j
</p>
<p>˛j'j.xk/ ; (H.8)
</p>
<p>where 'j.xk/ are linear independent basis functions, which do not have to be linear
in xk. The particular case of a linear regression, discussed in Sect. E.10, is included.
Equation (H.8) specifies the model function f .xk; f˛jg/ in (H.1) and this yields
</p>
<p>�2 D
X
</p>
<p>k
</p>
<p>ck
</p>
<p>2
</p>
<p>4yk �
X
</p>
<p>j
</p>
<p>˛j'j.xk/
</p>
<p>3
</p>
<p>5
</p>
<p>2
</p>
<p>; (H.9)
</p>
<p>which is supposed to tend to a minimum. We calculate
</p>
<p>@�2
</p>
<p>@˛`
D �2
</p>
<p>X
</p>
<p>k
</p>
<p>ck'`.xk/
</p>
<p>2
</p>
<p>4yk �
X
</p>
<p>j
</p>
<p>˛j'j.xk/
</p>
<p>3
</p>
<p>5
ŠD 0 ; (H.10)
</p>
<p>and arrive at:
</p>
<p>X
</p>
<p>j
</p>
<p>˛j
X
</p>
<p>k
</p>
<p>ck'`.xk/'j.xk/ D
X
</p>
<p>k
</p>
<p>ckyk'`.xk/ ; 8`: (H.11)
</p>
<p>This equation can be reformulated as the linear equation
</p>
<p>M˛ D ˇ ; (H.12)
</p>
<p>where the vectors ˛ D .˛1; ˛2; : : :/T and ˇ D .ˇ1; ˇ2; : : :/T with
</p>
<p>ˇi D
X
</p>
<p>k
</p>
<p>ckyk'i.xk/ ; (H.13)
</p>
<p>and the matrix M
</p>
<p>Mij D
X
</p>
<p>k
</p>
<p>ck'i.xk/'j.xk/ ; (H.14)
</p>
<p>have been introduced.</p>
<p/>
</div>
<div class="page"><p/>
<p>384 H Least Squares Fit
</p>
<p>Equation (H.12) can, for instance, be solved with the help of the methods
discussed in Appendix C. It is also particularly simple to determine the covariances
because we have
</p>
<p>Nij D
1
</p>
<p>2
</p>
<p>@2�2
</p>
<p>@˛i@˛j
D Mij ; (H.15)
</p>
<p>and the covariances follow from Eqs. (H.6) and (H.7).
</p>
<p>H.3 Nonlinear Least Squares Fit
</p>
<p>Before we discuss the most general case of a completely arbitrary model function
f .xk; f˛jg/ we want to point out that it is in most cases of advantage to linearize the
model function if at all possible. For instance, if the model function is an exponential
function, it may be linearized by taking the data points ln.yk/ instead of yk.
</p>
<p>However, if this is not possible there are numerous alternatives to find a solution
of the problem. For instance, the GAUSS-NEWTON method can be employed if
the model function f .xk; f˛jg/ and its derivatives with respect to the parameters
˛j are known analytically. Another possibility is offered by the application of an
deterministic optimization algorithm as they will be introduced in Appendix I. If
even this method is not applicable, the methods of stochastic optimization, discussed
in Chap. 20, might be an obvious choice.
</p>
<p>We describe now the GAUSS-NEWTON method which is essentially a general-
ization of the NEWTON method presented in Appendix B. The GAUSS-NEWTON
method is a method developed to minimize the expression (H.1) iteratively. The
derivatives
</p>
<p>@f .xk; f˛jg/
@˛`
</p>
<p>; (H.16)
</p>
<p>are assumed to be known analytically. This is an iterative algorithm and, thus, an
iteration index is introduced and indicated by a superscript index n like in ˛nj . The
algorithm is described by the following steps:
</p>
<p>1. Choose a set of initial values f˛0j g for the iteration.
2. Linearize the function f .xl; f˛nj g/ and insert the result into Eq. (H.1):
</p>
<p>�2 �
X
</p>
<p>k
</p>
<p>ck
</p>
<p>(
</p>
<p>yk � f .xk; f˛nj g/ �
X
</p>
<p>`
</p>
<p>�
@f .xk; f˛jg/
</p>
<p>@˛`
</p>
<p>�
</p>
<p>f˛jgDf˛nj g
.˛` � ˛n` /
</p>
<p>) 2
</p>
<p>:
</p>
<p>(H.17)</p>
<p/>
</div>
<div class="page"><p/>
<p>H Least Squares Fit 385
</p>
<p>We introduce the following abbreviations for a more compact notation:
</p>
<p>df nk;` D
�
@f .xk; f˛jg/
</p>
<p>@˛`
</p>
<p>�
</p>
<p>f˛jgDf˛nj g
; (H.18)
</p>
<p>and
</p>
<p>f nk D f .xk; f˛nj g/ : (H.19)
</p>
<p>3. We have to solve
</p>
<p>@�2
</p>
<p>@˛i
D �2
</p>
<p>X
</p>
<p>k
</p>
<p>ckdf nk;i
</p>
<p>"
</p>
<p>yk � fk �
X
</p>
<p>`
</p>
<p>df nk;`.˛` � ˛n` /
#
</p>
<p>ŠD 0 ; (H.20)
</p>
<p>for all parameters f˛jg. Therefore, we introduce vectors ˛ D .˛1; ˛2; : : :/T , ˇ D
.ˇ1; ˇ2; : : :/
</p>
<p>T with
</p>
<p>ˇi D
X
</p>
<p>k
</p>
<p>ck.yk � f nk /df nk;i ; (H.21)
</p>
<p>and the matrix M with elements:
</p>
<p>Mij D
X
</p>
<p>k
</p>
<p>ckdf nk;idf
n
k;j : (H.22)
</p>
<p>This transforms Eq. (H.20) into a linear system of equations
</p>
<p>M.˛ � ˛n/ D ˇ ; (H.23)
</p>
<p>which is solved for
˛n D ˛� ˛n. Please note that ˛n denotes the vector ˛ after
n iterations. The vector ˛nC1 for the next iteration step is guessed from:
</p>
<p>˛nC1 D ˛n C
˛n : (H.24)
</p>
<p>4. The iteration is terminated if for all parameters the desired accuracy was
achieved. For instance, the condition j˛nC1j � ˛nj j � � can be used with � a
small parameter. A criterion for the relative error can be formulated in analogue.
</p>
<p>Some comments concerning the covariance matrix are in order: It is more
complicated in the nonlinear case because we also have to consider the second
partial derivatives of the model function f .xk; f˛`g/. However, if these can for some
reason be neglected we obtain, again, that Nij D Mij, as in Appendix, Sect. H.2.
Another, more serious problem is found in the fact that the GAUSS-NEWTON
method suffers from severe instability problems. However, a possible remedy was
formulated by D.MARQUART [54] who suggested to multiply the diagonal elements</p>
<p/>
</div>
<div class="page"><p/>
<p>386 H Least Squares Fit
</p>
<p>of the matrix M with a factor .1C �/ where � &gt; 0. A detailed analysis shows that
one can choose � sufficiently large and in such a way that the value of �2n decreases
monotonically, i.e. �2nC1 � �2n for all iteration steps n. However, an increase of �
decreases the convergence rate and more iterations are necessary until the required
accuracy was obtained. It is therefore desirable to choose � values in such a way
that the error decreases monotonically but that, at the same time, a convergence
rate is maintained which is as large as possible. A possible strategy is to start with
some given value of � and to reduce it after every iteration step by a constant rate.
However, if at some point the error �2 increases, i.e. �2nC1 &gt; �
</p>
<p>2
n, then � has again to
</p>
<p>be increased.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix I
Deterministic Optimization
</p>
<p>I.1 Introduction
</p>
<p>We use the term deterministic optimization to distinguish these particular optimiza-
tion methods from the stochastic optimizationmethods discussed in Chap. 20. There
are numerous different deterministic methods designed to find the minimum (or
maximum) of a given function f .x/, where x can be a vector. Roughly speaking,
we can distinguish between methods which require the knowledge of the Hessian,1
</p>
<p>methodswhich need gradients only, andmethodswhich are based on function values
only. For instance, if the gradient of a function is known analytically onemay exploit
NEWTON&rsquo;s method, as it was introduced in Appendix B. Note that such an approach
requires the Hessian of the function f .x/.
</p>
<p>We plan to discuss here in some detail two specific methods, namely the method
of steepest descent and the method of conjugate gradients. Both methods require
the knowledge of the gradient of the function, however, the gradient can also be
approximated with the help of finite differences (see Chap. 2). A discussion of
additional methods is beyond the scope of this book and the interested reader is
referred to the available literature [55].
</p>
<p>However, before discussing these two methods in more detail, let us briefly
consider the quadratic problem which can be solved analytically. In this case the
function f .x/ can be written as
</p>
<p>f .x/ D 1
2
</p>
<p>xTAx � bTx C c ; (I.1)
</p>
<p>1The Hessian, or HESSE matrix, H 2 RN�N of a function f .x/, x 2 RN is the Jacobian of the
Jacobian J.x/ of f .x/ defined in Eq. (B.8). Thus, it is the matrix of second order partial derivatives
of a function. It describes the local curvature of a function of many variables.
</p>
<p>&copy; Springer International Publishing Switzerland 2016
B.A. Stickler, E. Schachinger, Basic Concepts in Computational Physics,
DOI 10.1007/978-3-319-27265-8
</p>
<p>387</p>
<p/>
</div>
<div class="page"><p/>
<p>388 I Deterministic Optimization
</p>
<p>where x 2 RN , A 2 RN�N , b 2 RN and c 2 R where we restrict the discussion to real
valued functions for reasons of simplicity. We demonstrate now that for symmetric
and positive definite matrices A, i.e. AT D A and xTAx &gt; 0 for all x &curren; 0, the
minimum of f .x/ is given by x D A�1b. The gradient of f .x/ is readily evaluated
and is given by2:
</p>
<p>rf .x/ D 1
2
</p>
<p>ATx C 1
2
</p>
<p>Ax � b : (I.2)
</p>
<p>This immediately yields the desired result:
</p>
<p>Ax D b : (I.3)
</p>
<p>It follows that x D A�1b is a minimum because we assumed A to be positive
definite. It is possible to solve the optimization problem even if A is not symmetric
by inverting the symmetrized matrix
</p>
<p>�
</p>
<p>A C AT
�
</p>
<p>=2. Finally, the linear equation (I.3)
can be solved with the methods discussed in Appendix C.
</p>
<p>I.2 Steepest Descent
</p>
<p>The most simple gradient based method is the method of steepest descent [6]. It
is based on the rather straight forward idea of moving in each iteration step into
the opposite direction of the gradient, i.e. downhill. Hence, we may formulate it
mathematically in the following way: Let xn be the current position of our search
for the minimum. Then we choose
</p>
<p>xnC1 D xn � ˛nrf .xn/ ; (I.4)
</p>
<p>where the step-size in direction of the negative gradient, ˛n, has to be determined in
an additional step. The step-size should be chosen in such a way that we reach the
line minimum in direction rf .xn/:
</p>
<p>d
</p>
<p>d˛n
f ŒxnC1.˛n/&#141; D �rf .xnC1/ � rf .xn/ ŠD 0 : (I.5)
</p>
<p>2We remember from vector analysis that
</p>
<p>rx
�
</p>
<p>xT Ax
�
</p>
<p>D rx
�
</p>
<p>xT A
�
</p>
<p>&bdquo; ƒ&sbquo; &hellip;
</p>
<p>DA
</p>
<p>x Crx
�
</p>
<p>xT AT
�
</p>
<p>&bdquo; ƒ&sbquo; &hellip;
</p>
<p>DAT
</p>
<p>x D .A C AT /x :</p>
<p/>
</div>
<div class="page"><p/>
<p>I Deterministic Optimization 389
</p>
<p>Hence, we observe that for an optimal choice of ˛n the search directions are
orthogonal. In practice ˛n is estimated with the help of a separate minimization
technique, such as bisection. This technique has already been used in our discussion
of the shooting methods in Chap. 10.
</p>
<p>We provide an example which is supposed to make the method more transparent
and to help in the discussion of its caveats: We want to determine the global
minimum of the function
</p>
<p>f .x; y/ D cos.2x/C sin.4y/C exp.1:5x2 C 0:7y2/C 2x : (I.6)
</p>
<p>Its gradient is easily evaluated
</p>
<p>@f .x; y/
</p>
<p>@x
D �2 sin.2x/C 3x exp.1:5x2 C 0:7y2/C 2 ; (I.7)
</p>
<p>and
</p>
<p>@f .x; y/
</p>
<p>@y
D 4 cos.4y/C 1:4y exp.1:5x2 C 0:7y2/ : (I.8)
</p>
<p>We define the algorithm steepest descent with the following steps:
</p>
<p>1. Choose some initial values x0 and y0.
2. Calculate the gradient rf .xn; yn/ in iteration step n.
3. Determine ˛n in such a way that
</p>
<p>f ŒxnC1.˛n/; ynC1.˛n/&#141;! min ; (I.9)
</p>
<p>which is equivalent to
</p>
<p>g.˛n/ WD rf ŒxnC1.˛n/; ynC1.˛n/&#141; � rf .xn; yn/ D 0 : (I.10)
</p>
<p>This is achieved by a bisection technique similar to the one employed in
Sect. 10.3,
</p>
<p>a. Set ˛an D 0 and chose ˛bn arbitrary.
b. Increase ˛bn until g.˛
</p>
<p>a
n/g.˛
</p>
<p>b
n/ &lt; 0.
</p>
<p>c. Define
</p>
<p>˛cn D
˛an C ˛bn
</p>
<p>2
; (I.11)
</p>
<p>and determine g.˛cn/.</p>
<p/>
</div>
<div class="page"><p/>
<p>390 I Deterministic Optimization
</p>
<p>Fig. I.1 Iteration sequence
of the method of steepest
descent for three different
starting points
</p>
<p>d. If g.˛an/g.˛
c
n/ &lt; 0, set ˛
</p>
<p>b
n D ˛cn and return to step c. Otherwise, set ˛an D ˛cn
</p>
<p>and return to step c.
e. The bisection is terminated if jg.˛an/j &lt; �, with � some required accuracy for
</p>
<p>the bisection part.
</p>
<p>4. Check whether j f .xnC1; ynC1/ � f .xn; yn/j � � with � some required accuracy.
Return to step 2 for the next iteration step if the algorithm is not converged.
</p>
<p>The above algorithm was executed for the function f .x; y/ given by Eq. (I.6) for
three different starting points, (0:8; �0:75), (0:8; 1:05), and (�1:05; 1:05). The
function f .x; y/ as well as the iteration sequence towards the minimum for all three
starting points is illustrated in Fig. I.1.
</p>
<p>We note the following properties of the method: First of all it is a rather slow
method due to the orthogonality of subsequent search directions. Moreover, as we
observe from Fig. I.1, we can only find the local minimum closest to the starting
point and not the global minimum of the function f .x; y/. The convergence rate is
also highly affected by the choice of the initial position. However, it is a very simple
method which works in spaces of arbitrary dimension.
</p>
<p>I.3 Conjugate Gradients
</p>
<p>The method of conjugate gradients [6, 55] is based on the definition of N orthogonal
search directions f ig in an N dimensional space. In contrast to steepest descent it
is designed in such a way that we take only one step in each search direction and the
minimum is found after at most N steps, if the function f .x/ is of the quadratic
form (I.1). In the more general case, however, it will take more steps but will,
nevertheless, be much more efficient than the method of steepest descent. Let us
formulate the method for a general function f .x/.</p>
<p/>
</div>
<div class="page"><p/>
<p>I Deterministic Optimization 391
</p>
<p>We approximate the function f .x/, with x 2 RN , in the vicinity of the reference
point xn in the n-th iteration step up to second order and name the resulting function
Of .x/:
</p>
<p>Ofn.x/ WD f .xn/Crf .xn/ � .x � xn/C
1
</p>
<p>2
.x � xn/ � Œ
f .xn/.x � xn/&#141;
</p>
<p>� f .xn/ � bTn .x � xn/C
1
</p>
<p>2
.x � xn/TAn.x � xn/ : (I.12)
</p>
<p>Here, An denotes the Hessian3 at position xn and bn is the negative gradient at xn.
In particular, for a quadratic function f .x/ the equality Of .x/ D f .x/ holds. We now
write the minimum Ox of f .x/ as a linear combination of search directions f ig with
coefficients �i and the initial point x0:
</p>
<p>Ox D x0 C
M
X
</p>
<p>iD0
�i i : (I.13)
</p>
<p>Note that in the quadratic case (I.1) this sum will be restricted to M D N � 1. At
each iteration instance we have the relation
</p>
<p>xnC1 D xn C �n n ; (I.14)
</p>
<p>together with the goal
</p>
<p>xM
ŠD Ox : (I.15)
</p>
<p>Let us define now a couple of useful quantities. The deviation from the minimum at
iteration step n C 1, ınC1, is given by
</p>
<p>ınC1 D xnC1 � Ox
D xn C �n n � Ox
D ın C �n n : (I.16)
</p>
<p>We define, furthermore, the residual
</p>
<p>rnC1 WD �rOfn.xnC1/
D bn � An.xnC1 � xn/
D bn � �nAn n ; (I.17)
</p>
<p>3Note that the Hessian is always symmetric for real valued functions f .x/ due to the symmetry of
second order derivatives.</p>
<p/>
</div>
<div class="page"><p/>
<p>392 I Deterministic Optimization
</p>
<p>where we employed that
</p>
<p>1
</p>
<p>2
r
�
</p>
<p>.x � xn/TAn.x � xn/
�
</p>
<p>D An.x � xn/ : (I.18)
</p>
<p>Finding the minimum of the quadratic approximation Of .x/ of f .x/ around xn is
equivalent to the condition
</p>
<p>rnC1 D 0 : (I.19)
</p>
<p>In particular, we have to find the product �n n in such a way that rnC1 D 0. Of
course, we could invert the Hessian An in order to obtain this result. However,
this would be too expensive from a computational point of view. The idea is to
apply the ideal search strategy for quadratic functions to Ofn.x/ in order to obtain
xnC1. Hence, the method of conjugate gradients executes packages of N steps,
where each package solves the quadratic problem around xn, until the minimum
of the original function f .x/ has been found. Therefore, we have to generalize the
relations (I.14), (I.16), and (I.17) for iterations within step n.
</p>
<p>We have, in particular, for every iteration step n
</p>
<p>xnC1 D xn C
N�1
X
</p>
<p>`D0
�`n 
</p>
<p>`
n ; (I.20)
</p>
<p>together with the definitions
</p>
<p>x`C1n D x`n C �`n `n ; (I.21)
</p>
<p>where xnC1 � xNn . Furthermore, we define the deviation
</p>
<p>ı`C1n D x`C1n � xnC1 D ı`n C �`n `n ; (I.22)
</p>
<p>and the residual
</p>
<p>r`C1n D �rOfn.x`C1n /
D bn � An.x`C1n � xn/ : (I.23)
</p>
<p>In contrast to relation (I.17), Eq. (I.23) features the difference .x`C1n �xn/ rather than
.xnC1 � xn/. We insert the recurrence (I.21) and obtain
</p>
<p>r`C1n D bn � An.x`n � xn/ � �`nAn `n
D r`n � �`nAn `n : (I.24)</p>
<p/>
</div>
<div class="page"><p/>
<p>I Deterministic Optimization 393
</p>
<p>Hence, in contrast to relation (I.17) Eq. (I.24) defines a recurrence relation. Again,
we want to choose the search directions `n and the step length �
</p>
<p>`
n in such a way that
</p>
<p>we find the minimum as quickly as possible. Suppose we already knew the search
direction  `n . The line minimum in this direction is then given by
</p>
<p>d
</p>
<p>d�`n
Ofn.x`C1n / D r Of .x`C1n / �  `n
</p>
<p>D �r`C1n � `n
D �.r`n � �`nAn `n /T `n
D �.r`n/T `n C �`n. `n /TAn `n
ŠD 0 ; (I.25)
</p>
<p>and we have
</p>
<p>�`n D
.r`n/
</p>
<p>T `n
. `n /
</p>
<p>TAn `n
: (I.26)
</p>
<p>Hence, the remaining unknown quantities in our algorithm are the search directions
 `n . So far, the only information we obtained is that the search direction  
</p>
<p>`
n is
</p>
<p>orthogonal to the residual r`C1n , see Eq. (I.25).
However, we also know that
</p>
<p>0 D An .xnC1 � xn/� bn
</p>
<p>D An
N�1
X
</p>
<p>`D0
�`n 
</p>
<p>`
n � bn ; (I.27)
</p>
<p>and therefore
</p>
<p>0 D
�
</p>
<p> kn
�T
</p>
<p>An
</p>
<p>N�1
X
</p>
<p>`D0
�`n 
</p>
<p>`
n �
</p>
<p>�
</p>
<p> kn
�T
</p>
<p>bn ; (I.28)
</p>
<p>for arbitrary k. A sufficient condition to ensure the validity of relation (I.28) is to
impose An-orthogonality:
</p>
<p>˝
</p>
<p> kn
ˇ
ˇ `n
</p>
<p>˛
</p>
<p>An
� . kn/TAn `n D ık;`
</p>
<p>˝
</p>
<p> kn
ˇ
ˇ kn
</p>
<p>˛
</p>
<p>An
: (I.29)
</p>
<p>We note that
˝
</p>
<p> kn
ˇ
ˇ `n
</p>
<p>˛
</p>
<p>A
constitutes indeed a scalar product since An is positive
</p>
<p>definite in the neighborhood of a minimum.</p>
<p/>
</div>
<div class="page"><p/>
<p>394 I Deterministic Optimization
</p>
<p>Let us briefly demonstrate that the choice (I.29) fulfills Eq. (I.28). First of all we
note that we obtain from Eq. (I.24)
</p>
<p>r`C1n D bn �
X̀
</p>
<p>kD0
�knAn 
</p>
<p>k
n ; (I.30)
</p>
<p>and, therefore, we derive the coefficients �`n from Eq. (I.26) in the convenient form:
</p>
<p>�`n D
bTn 
</p>
<p>`
n
</p>
<p>˝
</p>
<p> `n
ˇ
ˇ `n
</p>
<p>˛
</p>
<p>An
</p>
<p>: (I.31)
</p>
<p>The condition of orthogonality (I.29) is used to rewrite Eq. (I.28) as
</p>
<p>0 D �kn
˝
</p>
<p> kn
ˇ
ˇ kn
</p>
<p>˛
</p>
<p>An
�
�
</p>
<p> kn
�T
</p>
<p>bn ; (I.32)
</p>
<p>which together with Eq. (I.31) proves the equality (I.28). Hence, the strategy is clear:
We choose an initial direction  0n and then construct the further directions in such
a way that they fulfill An-orthogonality (I.29). Before discussing the construction of
search directions in more detail we observe the following property:
</p>
<p>. kn /
Tr`n D . kn /Tbn �
</p>
<p>`�1
X
</p>
<p>mD0
�mn
˝
</p>
<p> kn
ˇ
ˇ mn
</p>
<p>˛
</p>
<p>An
D
(
</p>
<p>. kn/
Tbn for k � ` ;
</p>
<p>0 else.
(I.33)
</p>
<p>This means that all search directions  kn for k � `� 1 are orthogonal to the residual
r`n, or in other words, all residuals r
</p>
<p>`
n are orthogonal (in the classical sense) to all
</p>
<p>previous search directions.
We shall now briefly outline the resulting update algorithm for search directions:
</p>
<p>Let f'`ng be a set of linear independent vectors that span our search space for Ofn.x/.4
We write the search direction  kn as
</p>
<p> kn D 'kn C
k�1
X
</p>
<p>`D0
ˇk`n  
</p>
<p>`
n ; (I.34)
</p>
<p>together with
</p>
<p> 0n D '0n : (I.35)
</p>
<p>4In principle these linear independent vectors f'`ng do not need to depend on the index n, i.e. on
the actual position xn. However, we consider here the most general case as will soon become clear.</p>
<p/>
</div>
<div class="page"><p/>
<p>I Deterministic Optimization 395
</p>
<p>The expansion coefficients ˇ`kn can be determined recursively by imposing An-
orthogonality for all ` &lt; k:
</p>
<p>0 D
˝
</p>
<p> kn
ˇ
ˇ `n
</p>
<p>˛
</p>
<p>An
</p>
<p>D
˝
</p>
<p>'kn
ˇ
ˇ `n
</p>
<p>˛
</p>
<p>An
C
</p>
<p>k�1
X
</p>
<p>mD0
ˇkmn
</p>
<p>˝
</p>
<p> mn
ˇ
ˇ `n
</p>
<p>˛
</p>
<p>An
</p>
<p>D
˝
</p>
<p>'kn
ˇ
ˇ `n
</p>
<p>˛
</p>
<p>An
C ˇk`n
</p>
<p>˝
</p>
<p> `n
ˇ
ˇ `n
</p>
<p>˛
</p>
<p>An
; (I.36)
</p>
<p>and, therefore:
</p>
<p>ˇk`n D �
˝
</p>
<p>'kn
ˇ
ˇ `n
</p>
<p>˛
</p>
<p>An
˝
</p>
<p> `n
ˇ
ˇ `n
</p>
<p>˛
</p>
<p>An
</p>
<p>: (I.37)
</p>
<p>This procedure is known as the GRAM-SCHMIDT conjugation [6, 12].
Now, the question arises how one should choose the basis vectors '`n and whether
</p>
<p>or not it is advantageous to choose the '`n as a function of n. A particularly clever
choice is to take the residuals, i.e.
</p>
<p>'`n D r`n : (I.38)
</p>
<p>In this case we have for ` &lt; k
</p>
<p>ˇk`n D �
˝
</p>
<p>rkn
</p>
<p>ˇ
ˇ `n
</p>
<p>˛
</p>
<p>An
˝
</p>
<p> `n
ˇ
ˇ `n
</p>
<p>˛
</p>
<p>An
</p>
<p>D � .r
k
n/
</p>
<p>TAn 
`
n
</p>
<p>˝
</p>
<p> `n
ˇ
ˇ `n
</p>
<p>˛
</p>
<p>An
</p>
<p>D � .r
k
n/
</p>
<p>T
</p>
<p>˝
</p>
<p> `n
ˇ
ˇ `n
</p>
<p>˛
</p>
<p>An
</p>
<p>�
r`n � r`C1n
�`n
</p>
<p>�
</p>
<p>; (I.39)
</p>
<p>where we used recurrence (I.24). We now calculate with the help of Eq. (I.34)
</p>
<p>.rkn/
T.r`n/ D .rkn/T `n � .rkn/T
</p>
<p>`�1X
</p>
<p>mD0
ˇ`mn  
</p>
<p>m
n D 0 ; (I.40)</p>
<p/>
</div>
<div class="page"><p/>
<p>396 I Deterministic Optimization
</p>
<p>for ` &lt; k due to the orthogonality of the search direction and the residuals, see
Eq. (I.33). Hence, we obtain for all ` &lt; k
</p>
<p>ˇk`n D
1
</p>
<p>�k�1n
</p>
<p>.rkn/
Trknı`C1;k
</p>
<p>˝
</p>
<p> k�1n
ˇ
ˇ k�1n
</p>
<p>˛
</p>
<p>An
</p>
<p>D .r
k
n/
</p>
<p>Trkn
</p>
<p>.rk�1n /
Trk�1n
</p>
<p>ı`;k�1 : (I.41)
</p>
<p>Hence, the name conjugated gradients.
We are now in a position to describe the algorithm for the method of conjugated
</p>
<p>gradients:
</p>
<p>1. Choose an initial position x0.
2. Determine the vector bn and the matrix An for a given position xn.
3. Perform the following N steps in order to calculate xnC1:
</p>
<p>a. Set
</p>
<p> 0n D r0n D bn and �0n D
bTn 
</p>
<p>0
n
</p>
<p>˝
</p>
<p> 0n
ˇ
ˇ 0n
</p>
<p>˛
</p>
<p>An
</p>
<p>; (I.42)
</p>
<p>as well as
</p>
<p>xnC1 D xn C �0n 0n : (I.43)
</p>
<p>b. Calculate for k D 1; : : : ;N � 1 the residuals,
</p>
<p>rkn D rk�1n � �k�1n An k�1n ; (I.44)
</p>
<p>the new search directions
</p>
<p> kn D rkn C
.rkn/
</p>
<p>Trkn
</p>
<p>.rk�1n /
Trk�1n
</p>
<p> k�1n ; (I.45)
</p>
<p>the step lengths
</p>
<p>�kn D
bTn 
</p>
<p>k
</p>
<p>˝
</p>
<p> kn
ˇ
ˇ kn
</p>
<p>˛
</p>
<p>An
</p>
<p>; (I.46)
</p>
<p>and, finally, the modified positions
</p>
<p>xnC1 D xn C �kn kn : (I.47)</p>
<p/>
</div>
<div class="page"><p/>
<p>I Deterministic Optimization 397
</p>
<p>4. If j f .xnC1/ � f .xn/j &lt; �, with � some required accuracy, terminate the iteration,
otherwise return to step 2. In case of a convex function f .x/ terminate also after
N steps.
</p>
<p>Strictly speaking, this algorithm is only valid for convex functions because we
note that one might get into trouble whenever a position is reached at which the
Hessian is not positive definite. It is therefore desirable to exclude the Hessian from
the algorithm. This can be achieved by an algorithm developed by FLETCHER and
REEVES [56]. Based on our previous discussion the generalization is rather obvious:
If we do not want to use the Hessian explicitly, we have to determine the step length
�`n by minimizing f .x
</p>
<p>`
n C �`n `n / for a given search direction  `n numerically. The
</p>
<p>residuals are then taken to be the exact gradient of the function f .x`n/ rather than of
Ofn.x`n/. The next search direction  kC1n is then determined via
</p>
<p> kC1n D �rf .xkC1n /C
krf .xkC1n /k2
krf .xkn/k2
</p>
<p> kn : (I.48)
</p>
<p>Hence, we have the following algorithm (FLETCHER-REEVES algorithm):
</p>
<p>1. Choose an initial position x0.
2. Perform the following N steps in order to calculate xnC1:
</p>
<p>a. Set
</p>
<p> 0n D �rf .xn/ : (I.49)
</p>
<p>b. Calculate for k D 0; : : : ;N � 1 �kn by minimizing f .xkn C �kn kn /, the new
position xkC1n D xkn C �kn kn , and the new search direction via
</p>
<p> kn D �rf .xkC1n /C
krf .xkC1n /k2
krf .xkn/k2
</p>
<p> kn : (I.50)
</p>
<p>3. If j f .xnC1/ � f .xn/j &lt; �, with � some required accuracy, terminate the iteration,
otherwise return to step 2.
</p>
<p>The resulting sequence of steps towards the minimum for the same function and
initial conditions as were used for Fig. I.1 is illustrated in Fig. I.2. In comparing
Figs. I.1 and I.2 we note immediately that the search strategy developed for the
method of conjugate gradients superbly outperforms the search strategy of the
method of steepest descent. In particular, if the ratio between the gradient in x and
y direction is large, a strategy of orthogonal search directions is disadvantageous.
This particular case is illustrated in Fig. I.3 for both, steepest descent and conjugate
gradients. Here we investigate the convex function
</p>
<p>f .x; y/ D x2 C 10y2 ; (I.51)</p>
<p/>
</div>
<div class="page"><p/>
<p>398 I Deterministic Optimization
</p>
<p>Fig. I.2 Iteration sequence of the method of conjugated gradients for three different starting points
</p>
<p>Fig. I.3 Comparison of the iteration sequence between the method of steepest descent and the
method of conjugated gradients
</p>
<p>together with an initial position .x0; y0/ D .1:9; 0:4/. The resulting sequence
of points towards the minimum is illustrated in Fig. I.3. In the case of steepest
descent the sequence approaches the minimum rather slowly since subsequent
search directions have to be orthogonal to each other in the classical sense.
The advantage of conjugate gradients is that An-orthonormality accelerates the
convergence towards the minimum. In this example we reach it within two steps
and a required absolute accuracy of � D 10�7.
</p>
<p>As a final remark we note that also the method of conjugate gradients will
only find the local minimum closest to the initial position. Hence, the outcome of
the method highly depends on the choice of x0. Moreover, the calculation of the
gradients may be very tedious and time-consuming from a numerical point of view.</p>
<p/>
</div>
<div class="page"><p/>
<p>I Deterministic Optimization 399
</p>
<p>References
</p>
<p>1. Arnol&rsquo;d, V.I.: Mathematical Methods of Classical Mechanics, 2nd edn. Graduate Texts in
Mathematics, vol. 60. Springer, Berlin/Heidelberg (1989)
</p>
<p>2. Fetter, A.L., Walecka, J.D.: Theoretical Mechanics of Particles and Continua. Dover, New
York (2004)
</p>
<p>3. Scheck, F.: Mechanics, 5th edn. Springer, Berlin/Heidelberg (2010)
4. Goldstein, H., Poole, C., Safko, J.: Classical Mechanics, 3rd edn. Addison-Wesley, Menlo Park
</p>
<p>(2013)
5. Flie&szlig;bach, T.: Mechanik, 7th edn. Lehrbuch zur Theoretischen Physik I. Springer,
</p>
<p>Berlin/Heidelberg (2015)
6. Press, W.H., Teukolsky, S.A., Vetterling, W.T., Flannery, B.P.: Numerical Recipes in C++, 2nd
</p>
<p>edn. Cambridge University Press, Cambridge (2002)
7. Ralston, A., Rabinowitz, P.: A First Course in Numerical Analysis, 2nd edn. Dover, New York
</p>
<p>(2001)
8. Jacques, I., Judd, C.: Numerical Analysis. Chapman and Hall, London (1987)
9. Burden, R.L., Faires, J.D.: Numerical Analysis. PWS-Kent Publishing Comp., Boston (1993)
10. Stoer, J., Bulirsch, R.: Introduction to Numerical Analysis, 2nd edn. Springer,
</p>
<p>Berlin/Heidelberg (1993)
11. Westlake, J.R.: A Handbook of Numerical Matrix Inversion and Solution of Linear Equations.
</p>
<p>Wiley, New York (1968)
12. Hazewinkel, M. (ed.): Encyclopaedia of Mathematics. Springer, Berlin/Heidelberg (1994)
13. Varga, R.S.: Matrix Iterative Analysis, 2nd edn. Springer Series in Computational Mathemat-
</p>
<p>ics, vol. 27. Springer, Berlin/Heidelberg (2000)
14. Jackson, J.D.: Classical Electrodynamics, 3rd edn. Wiley, New York (1998)
15. Greiner, W.: Classical Electrodynamics. Springer, Berlin/Heidelberg (1998)
16. Griffiths, D.J.: Introduction to Electrodynamics, 4th edn. Addison-Wesley, Menlo Park (2013)
17. Sakurai, J.J.: Modern Quantum Mechanics. Addison-Wesley, Menlo Park (1985)
18. Sakurai, J.J.: Advanced Quantum Mechanics. Addison-Wesley, Menlo Park (1987)
19. Ballentine, L.E.: Quantum Mechanics. World Scientific, Hackensack (1998)
20. Courant, R., Hilbert, D.: Methods of Mathematical Physics, vol. 1. Wiley, New York (1989)
21. Sch&uuml;cker, T.: Distributions, Fourier Transforms and Some of Their Applications to Physics.
</p>
<p>Lecture Notes in Physics, vol. 37. World Scientific, Hackensack (1991)
22. Kammler, D.W.: A First Course in Fourier Analysis. Cambridge Univerity Press, Cambridge
</p>
<p>(2008)
23. Hansen, E.W.: Fourier Transforms: Principles and Applications. Wiley, New York (2014)
24. Bernatz, R.: Fourier Series and Numerical Methods for Partial Differential Equations. Wiley,
</p>
<p>New York (2010)
25. Cooley, J.W., Tukey, J.W.: An algorithm for the machine calculation of complex Fourier series.
</p>
<p>Math. Comput. 19, 297&ndash;301 (1965). doi:10.1090/S0025-5718-1965-0178586-1
26. Nussbaumer, H.J.: Fast Fourier Transform and Convolution Algorithms. Springer Series in
</p>
<p>Information Sciences, vol. 2. Springer, Berlin/Heidelberg (1981)
27. Frigo, M., Johnson, S.G.: FFTW, The Fastest Fourier Transform in theWest,Vers. 3.3.4 (2015).
</p>
<p>http://www.fftw.org
28. Bakhturin, Y.A.: Campell-Hausdorff formula. In: Hazewinkel, M. (ed.) Encyclopaedia of
</p>
<p>Mathematics. Springer, Berlin/Heidelberg (1994)
29. McLachlan, R.I., Quispel, G.R.W.: Splitting methods. Acta Numer. 11, 341&ndash;434 (2002).
</p>
<p>doi:10.1017/S0962492902000053
30. Rotar, V.: Probability Theory. World Scientific, Singapore (1998)
31. Papoulis, A., Pillai, S.: Probability, Random Variables and Stochastic Processes. McGraw Hill,
</p>
<p>New York (2001)
32. Breuer, H.P., Petruccione, F.: Open Quantum Systems, chap. 1. Clarendon Press, Oxford (2010)</p>
<p/>
<div class="annotation"><a href="http://www.fftw.org">http://www.fftw.org</a></div>
</div>
<div class="page"><p/>
<p>400 I Deterministic Optimization
</p>
<p>33. von der Linden, W., Dose, V., von Toussaint, U.: Bayesian Probability Theory. Cambridge
University Press, Cambridge (2014)
</p>
<p>34. Klenke, A.: Probability Theory. Universitext. Springer, Berlin/Heidelberg (2014)
35. Lee, P.M.: Bayesian Statistics: An Introduction. Wiley, New York (2012)
36. Abramovitz, M., Stegun, I.A. (eds.): Handbook of Mathemathical Functions. Dover, New York
</p>
<p>(1965)
37. Olver, F.W.J., Lozier, D.W., Boisvert, R.F., Clark, C.W.: NIST Handbook of Mathematical
</p>
<p>Functions. Cambridge University Press, Cambridge (2010)
38. Applebaum, D.: L&eacute;vy Processes and Stochastic Calculus. Cambridge Studies in Advanced
</p>
<p>Mathematics. Cambridge University Press, Cambridge (2006)
39. Yeomans, J.M.: Statistical Mechanics of Phase Transitions. Clarendon Press, Oxford (1992)
40. Cardy, J.: Scaling and Renormalization in Statistical Physics. Cambridge Lecture Notes in
</p>
<p>Physics. Cambridge University Press, Cambridge (1996)
41. Flie&szlig;bach, T.: Statistische Physik. Lehrbuch zur Theoretischen Physik IV. Springer,
</p>
<p>Berlin/Heidelberg (2010)
42. Pathria, R.K., Beale, P.D.: Statistical Mechanics, 3rd edn. Academic, San Diego (2011)
43. White, R.M.: Quantum Theory of Magnetism, 3rd edn. Springer Series in Solid-State Sciences.
</p>
<p>Springer, Berlin/Heidelberg (2007)
44. Mandl, F.: Statistical Physics, 2nd edn. Wiley, New York (1988)
45. Keener, R.W.: Theoretical Statistics. Springer, Berlin/Heidelberg (2010)
46. Jaeger, G.: The Ehrenfest classification of phase transitions: introduction and evolution. Arch.
</p>
<p>Hist. Exact Sci. 51&ndash;81 (1998). doi:10.1007/s004070050021
47. Landau, L.D., Lifshitz, E.M.: Course of Theoretical Physics, vol. 5: Statistical Physics.
</p>
<p>Pergamon Press, London (1963)
48. Ter Haar, D.: Collected Papers of L. D. Landau, p. 546. Pergamon Press, London (1965)
49. Kilbas, A.A., Srivastava, H.M., Trujillo, J.J.: Theory and Applications of Fractal Differential
</p>
<p>Equations. Elsevier, Amsterdam (2006)
50. Iversen, G.P., Gergen, I.: Statistics. Springer Undergraduate Textbooks in Statistics. Springer,
</p>
<p>Berlin/Heidelberg (1997)
51. Wilcox, R.R.: Basic Statistics. Oxford University Press, New York (2009)
52. Monahan, J.F.: Numerical Methods of Statistics. Cambridge Series in Statistical and
</p>
<p>Probabilistic Mathematics. Cambridge University Press, Cambridge (2011)
53. Wood, S.: Core Statistics. Institute of Mathematical Statistics Textbooks. Cambridge Univer-
</p>
<p>sity Press, Cambridge (2015)
54. Marquart, D.: An algorithm for least-squares estimation of nonlinear parameters. J. Soc. Ind.
</p>
<p>Appl. Math. 11, 431&ndash;441 (1963). doi:10.1137/0111030
55. Avriel, M.: Nonlinear Programming: Analysis and Methods. Dover, New York (2003)
56. Fletcher, R., Reeves, C.M.: Function minimization by conjugate gradients. Comput. J. 7, 149&ndash;
</p>
<p>154 (1964). doi:10.1093/comjnl/7.2.149</p>
<p/>
</div>
<div class="page"><p/>
<p>Index
</p>
<p>A
</p>
<p>Absolute error See Error
ADAMS-BASHFORDmethod See Integrator
Algorithm 5
Anomalous diffusion 288
Antiferromagnetism 226
Auto-correlation function 249
Auto-correlation time
</p>
<p>exponential 317
integrated 316, 318
</p>
<p>Auto-correlations 315
Autonomous system 95
</p>
<p>B
</p>
<p>Backward difference 20, 33
BAKER-CAMPBELL-HAUSDORFF formula
</p>
<p>361
Barometric formula 115
BAYES&rsquo; theorem 369
BERNOULLI&rsquo;s law of large numbers 367
Binomial distribution See Probability density
</p>
<p>function
BOLTZMANN distribution See Probability
</p>
<p>density function
BOLTZMANN equation 271
</p>
<p>collision integral 271
particle density 272
</p>
<p>Boundary conditions
decoupled 118
DIRICHLET conditions 118, 161
homogeneous 118
</p>
<p>NEUMANN conditions 118
of first kind 118
of second kind 118
of third kind 118
periodic 110, 123
STURM conditions 118
</p>
<p>Brownian motion 183, 251
BUTCHER tableau See Integrator
</p>
<p>C
</p>
<p>Calculus
mean value theorem 214
</p>
<p>Canonical partition function 215, 220, 229
CAPUTO fractional derivative 291, 292, 380
CAUCHY distribution See Probability density
</p>
<p>function
Cdf See Cumulative distribution function
Central difference 20
Central limit theorem 193, 218, 371
</p>
<p>generalized 374
Chaos
</p>
<p>attractor 96
characteristic length 95
deterministic 12
periodic orbit 96
POINCAR&Eacute;map 96
POINCAR&Eacute; section 96
stability 95
theory 11
</p>
<p>CHAPMAN - KOLMOGOROV equation 252,
255, 260, 267
</p>
<p>�2 distribution 193
</p>
<p>&copy; Springer International Publishing Switzerland 2016
B.A. Stickler, E. Schachinger, Basic Concepts in Computational Physics,
DOI 10.1007/978-3-319-27265-8
</p>
<p>401</p>
<p/>
</div>
<div class="page"><p/>
<p>402 Index
</p>
<p>Closed integration rule 34
Cluster algorithm 307
Computational cost 12
Configuration space 94
Conjugate gradients See Deterministic
</p>
<p>optimization
COOLEY and TUKEY algorithm 359
COURANT-FRIEDRICHS-LEWY condition
</p>
<p>158
Covariance 365
CRANK-NICOLSONmethod See Integrator
Cumulative distribution function 186, 197,
</p>
<p>368
</p>
<p>D
</p>
<p>DE MOIVRE-LAPLACE theorem 367, 371
Derivative
</p>
<p>finite difference 157, 159, 167, 173, 285
backward 21, 23, 57, 106
central 21, 22, 58, 163, 171
forward 20, 23, 57
operator technique 23
shift operator 23
three point approximation 22
</p>
<p>fractional
CAPUTO fractional derivative 291,
</p>
<p>292, 380
RIEMANN-LIOUVILLE fractional
</p>
<p>derivative 292, 379
RIESZ fractional derivative 292, 380
WEYL fractional derivative 380
</p>
<p>partial 28
Detailed balance 221, 257, 258, 263, 297,
</p>
<p>299
Deterministic optimization 387
</p>
<p>conjugate gradients 390
An-orthogonality 393
FLETCHER-REEVES algorithm 397
GRAM-SCHMIDT conjugation 395
line minimum 393
residual 391
</p>
<p>Hessian matrix 391
steepest descent 388
</p>
<p>convergence rate 390
line minimum 388
</p>
<p>DFT See FOURIER
Differential equation
</p>
<p>harmonic oscillator 2
LEGENDRE 41
</p>
<p>Diffusion equation 131, 272, 283
diffusion coefficient 131, 272
particle density 131
</p>
<p>DIRAC ı-distribution 135, 215, 217
Double pendulum 85
</p>
<p>chaotic 93
generalized momenta 87
HAMILTON equations of motion 88
HAMILTON function 88
kinetic energy 86, 87
LAGRANGE function 86
POINCAR&Eacute; plot 98
potential energy 86
RUNGE-KUTTA algorithm 89
trajectories 90
</p>
<p>E
</p>
<p>e-RK-4 72, 89
Eccentricity 55
Error
</p>
<p>absolute 6
algorithmic 5
input 5
measurement 5
methodological 5, 7, 22, 78, 79
output 5
relative 6
roundoff 5, 6
standard 365
truncation 4, 19, 21, 23, 26, 66, 67
</p>
<p>Estimator 236
energy expectation value 236
error 312
internal energy 236
</p>
<p>EULER methods 57&ndash;59, 65, 68, 69, 75, 77
Exchange interaction 225
</p>
<p>F
</p>
<p>FERRARI&rsquo;s method 58
Ferromagnetism 226
FFT See FOURIER
FIBONACCI sequence 188
Finite difference 8, 17, 18, 32, 131
</p>
<p>backward 20, 33
central 20
forward 20, 33
</p>
<p>Finite volume effects 110
FLETCHER-REEVES algorithm See
</p>
<p>Deterministic optimization
Floating-point form 6
Fluctuation quantities 229</p>
<p/>
</div>
<div class="page"><p/>
<p>Index 403
</p>
<p>FOKKER - PLANCK equation 283
Forward difference 20, 33
FOURIER
</p>
<p>coefficient 358, 360
DFT 359
FFT 359
matrix 359
series 357
transform 119, 135, 177, 358
</p>
<p>convolution integral 49
discrete 359
fast 359
inverse 358
operator 361
</p>
<p>Fractal random walk 291
diffusion equation 291
fractal time random walk 288
</p>
<p>Fractional integral
RIEMANN-LIOUVILLE fractional integral
</p>
<p>379
RIESZ fractional integral 380
WEYL fractional integral 380
</p>
<p>G
</p>
<p>GAUSS-HERMITE quadrature 45
GAUSS-LEGENDRE quadrature 41
GAUSS-SEIDL method 160, 353, 355
GAUSS distribution See Probability density
</p>
<p>function
GAUSS peak 134
GAUSS-NEWTON method 384
Gaussian process 251
Generalized diffusion model
</p>
<p>characteristic waiting time 287
jump length pdf 286
jump pdf 285
length variance 287
waiting time pdf 286
</p>
<p>Genetic algorithm See Stochastic
optimization
</p>
<p>GIBBS sampling See MARKOV-chain
Monte-Carlo sampling
</p>
<p>Grid-point 18, 119, 132, 148, 161, 274
distance between 18, 119, 159, 171
equally spaced 18, 32, 159
variable spaced 18
</p>
<p>H
</p>
<p>HAMILTON
equations of motion 74
</p>
<p>function 74, 75, 327
HAMILTON operator See Operator
Heat capacity 230, 302
Heat equation
</p>
<p>heat source/drain 134
homogeneous 131
inhomogeneous 134, 360
stationary, one-dimensional 131
thermal diffusivity 131
time-dependent heat equation see Partial
</p>
<p>differential equation
HEISENBERG model 227
Hessian matrix See Deterministic
</p>
<p>optimization
Hill climbing See Stochastic optimization
Histogram technique 191, 312, 319
Hit and miss integration 213
</p>
<p>I
</p>
<p>Ill-conditioned 9
problems 10
</p>
<p>Implicit midpoint rule 58
Importance sampling 219, 297&ndash;299
Induced instability 10
Integrable system 95
Integration See Quadrature
Integrator
</p>
<p>ADAMS-BASHFORDmethod 67
backward EULER method 65
BUTCHER tableau 70
CRANK-NICOLSONmethod 66, 164, 172
</p>
<p>algorithmic form 69
explicit EULER method 57, 65, 68
</p>
<p>algorithmic form 69
explicit midpoint rule 68
</p>
<p>algorithmic form 70
forward EULER method 65
implicit EULER method 58, 65
</p>
<p>algorithmic form 69
implicit midpoint rule 58, 66, 69
leap-frog method 66, 107
linear multi-step methods 66, 67
predictor - corrector method 68
RUNGE-KUTTA method 66, 68
</p>
<p>e-RK-4 72, 89
e-RK-4, algorithmic form 72
explicit, algorithmic form 70
</p>
<p>simple integrators 65
ST&Ouml;RMER-VERLET method 66, 106
symplectic integrators
</p>
<p>EULER method 59, 75, 106</p>
<p/>
</div>
<div class="page"><p/>
<p>404 Index
</p>
<p>flow of the system 73
RUNGE - KUTTA method 76
</p>
<p>velocity VERLET algorithm 108
Inverse transformation method 200, 220
ISING model 219
</p>
<p>antiferromagnetism 226
exchange interaction 225
expectation value
</p>
<p>energy 229
energy per particle 233
</p>
<p>ferromagnetism 226
HAMILTON function 226
HAMILTON operator 227
</p>
<p>one-dimensional chain 228
Z2 symmetry 242
</p>
<p>heat capacity 230
magnetic susceptibility 229, 230
magnetization 229, 236
mean field approximation 227
molecular field 228
nearest neighbor interaction 228
numerics
</p>
<p>auto-correlation 239
cold start 239
cooling strategy 242
error on expectation values 242
hot start 239
initial configuration 238
lattice geometry 237
measurement 240
size effect 242
thermalization 239, 240
thermalization length 239
</p>
<p>paramagnetism 226
phase transition 234
spin correlation function 234
thermodynamic limit 234
transfer matrix 232
</p>
<p>eigenvalue 233
two-dimensional solution 234
</p>
<p>J
</p>
<p>Jackknife averages 314
JACOBI determinant 199
JACOBI matrix 74, 79, 348
JACOBI method 160, 354
</p>
<p>K
</p>
<p>KEPLER problem
absolute error 56
</p>
<p>conservation
of energy 344
</p>
<p>differential equation 53
explicit EULER 77
HAMILTON equations of motion 59, 76
HAMILTON function 59, 76
implicit EULER 77
LAGRANGE equation 343
LAGRANGE function 341&ndash;343
pericenter velocity 81
perihelion 79
rotational invariance 343
symplectic EULER 77
translational invariance 341
</p>
<p>L
</p>
<p>L2-norm 141
LAGRANGE polynomial 38, 39, 68
LANDAU theory 376
LAND&Eacute; factor 227
LANGEVIN equation See Stochastic
</p>
<p>differential equation
LAPLACE equation See Partial differential
</p>
<p>equation
Law of large numbers 215
Leap-frog See Integrator and/or Molecular
</p>
<p>dynamics
Least squares fit 381
</p>
<p>linear 383
model/test function 381
nonlinear
</p>
<p>GAUSS-NEWTON method 384
LEGENDRE polynomial 41
</p>
<p>orthonormality condition 42
RODRIGUES formula 42
</p>
<p>LENNARD-JONES potential 104, 110
L&Eacute;VY flight 288
</p>
<p>diffusion equation 289
fat-tailed jump length pdf 289
jump length pdf 289
</p>
<p>L&Eacute;VY process 251
Linear equations 120, 124, 173
</p>
<p>GAUSS-SEIDL method 160, 353
relaxation parameter 355
</p>
<p>inhomogeneous 121
JACOBI method 160, 354
LU decomposition 350
non-homogeneous 349
sparse matrix 353
tridiagonal matrix 124, 133, 135, 173,
</p>
<p>174, 352</p>
<p/>
</div>
<div class="page"><p/>
<p>Index 405
</p>
<p>Linear multi-step methods 66, 67
Loop algorithm 308
LU decomposition 350
</p>
<p>M
</p>
<p>Machine-number 7
Magnetic susceptibility 229, 230, 302
Magnetization 229, 236, 302
Marginalization rule 203, 260, 369
MARKOV process 251
</p>
<p>CHAPMAN-KOLMOGOROV equation
252, 255
</p>
<p>detailed balance 257, 258
equilibrium distribution function 257
global balance 257
HAMILTON&rsquo;s equations of motion 257
hierarchy of pdfs 252
MARKOV property 252
master equation 255, 256
POISSON process 254
</p>
<p>transition probability 254
waiting time 254
</p>
<p>precursor state 251
time-homogeneous 255
transition probability 252
transition rate 255
WIENER process 253
</p>
<p>transition probability 253
MARKOV-chain 221, 259
</p>
<p>absorbing state 261
aperiodic state 261, 274
CHAPMAN-KOLMOGOROV equation
</p>
<p>260, 267
closed set of states 260
continuous state space 266
continuous time 266
detailed balance 221, 263, 297, 299
equilibrium distribution 262, 263
ergodic state 261
irreducible 260, 262
irreducible class 260
MARKOV property 259
null recurrent state 261
periodic state 261, 274
positive recurrent state 261
recurrent state 261
reversible 263
stationary distribution 262, 297
transient state 261
transition matrix 259
</p>
<p>transition probability 299
MARKOV-chain Monte-Carlo sampling
</p>
<p>GIBBS sampling 301
METROPOLIS algorithm 219, 237, 297
</p>
<p>acceptance probability 221, 237, 300
asymmetric proposal probability 300
correlations 222
initialization 222
thermalization 222
</p>
<p>METROPOLIS-HASTINGS algorithm 298,
300, 328
</p>
<p>slice sampling 301
MAXWELL - BOLTZMANN distribution 112
Mean-value integration See Quadrature
Methodological error See Error
METROPOLIS-HASTINGS algorithm See
</p>
<p>MARKOV-chain Monte-Carlo
sampling
</p>
<p>METROPOLIS algorithm See MARKOV-
chain Monte-Carlo sampling
</p>
<p>Midpoint rules 66, 68, 69
Molecular dynamics 103
</p>
<p>barometric formula 115
boundary conditions 109
constant temperature 111
external potential 104
finite volume effects 110
forces 105
initial conditions 112
leap-frog method see Integrator
LENNARD-JONES potential 104
natural units 112
NEWTON equations of motion 104,
</p>
<p>105
ST&Ouml;RMER - VERLET method 106
system temperature 111
thermal equilibrium 112
time-reversal symmetry 108
total kinetic energy 111
total velocity shift 110
two-particle interaction 104
velocity VERLET algorithm 108
</p>
<p>Monte-Carlo integration See Quadrature
</p>
<p>N
</p>
<p>N&Eacute;EL temperature 226
NEWTON method 78, 347
NEWTON-COTES rules 38
Normal distribution See Probability density
</p>
<p>function
Normalization condition 126</p>
<p/>
</div>
<div class="page"><p/>
<p>406 Index
</p>
<p>O
</p>
<p>Open integration rule 34
Operator
</p>
<p>expectation value 141
HAMILTON operator 139, 171, 360
Hermitian 141
kinetic energy 140
LAPLACE operator 131
momentum 140
position 147
potential energy 140
time-evolution operator 171, 361
</p>
<p>Ordinary differential equation 53, 57
collocation point 72
eigenvalue problem 125, 126, 140
explicit 63, 64
homogeneous 118
homogeneous boundary value problem
</p>
<p>126, 148
initial value problem 63
integrators see Integrator
linear boundary value problem 117
</p>
<p>ORNSTEIN-UHLENBECK process 284
master equation 284
</p>
<p>P
</p>
<p>Paramagnetism 226
Partial differential equation
</p>
<p>diffusion equation see Diffusion equation
elliptic 157, 158
hyperbolic 157, 167
LAPLACE equation
</p>
<p>charge density 158
parabolic 157, 163
POISSON equation 158
</p>
<p>charge density 158, 161
convergence condition 162
electric field 158
electrostatic potential 158
iterative solution 160
</p>
<p>split operator technique 171, 359, 361
time-dependent heat equation 163
</p>
<p>CRANK-NICOLSONmethod 164
explicit EULER method 164, 165
implicit EULER method 164, 165
stability 164
</p>
<p>time-dependent SCHR&Ouml;DINGER equation
see SCHR&Ouml;DINGER equation
</p>
<p>wave equation
explicit EULER method 167
</p>
<p>one-dimensional 167
PAULI matrix 227
Pdf See Probability density function. See
</p>
<p>Stochastic process
Pendulum 2
</p>
<p>period 3
Phase space 94
Phase transition
</p>
<p>critical exponent 377
CURIE-WEISS law 228, 377
CURIE temperature 225, 226, 376
first order 376
modern classification 376
N&Eacute;EL temperature 226
second order 225, 376
</p>
<p>LANDAU theory 376
order parameter 225, 376
</p>
<p>universality 378
POINCAR&Eacute;map 96
POINCAR&Eacute; section 96
POISSON distribution See Probability density
</p>
<p>function
POISSON equation See Partial differential
</p>
<p>equation
POISSON process 254
Poor person&rsquo;s assumption 312
POTTS model 302
</p>
<p>HAMILTON function 302
heat capacity 302
magnetic susceptibility 302
magnetization 302
phase transition
</p>
<p>first order 305
histogram technique 305
second order 305
</p>
<p>Predictor-corrector method See Integrator
Probability
</p>
<p>classical 363
conservation 200
correlation coefficient 371
event 363
</p>
<p>certain 363
complimentary 364
impossible 363
</p>
<p>Probability density function 112, 186
binomial distribution 366
BOLTZMANN distribution 215, 220, 229,
</p>
<p>236, 307, 319, 327, 331
CAUCHY distribution 331
�2 distribution 193
composite pdf 206
exponential distribution 208
GAUSS distribution 370
L&Eacute;VY ˛-stable distributions 374</p>
<p/>
</div>
<div class="page"><p/>
<p>Index 407
</p>
<p>normal distribution 198, 204, 218, 254,
285, 370
</p>
<p>piecewise defined 206
POISSON distribution 255, 367
stable distribution 373
TSALLIS distribution 332
</p>
<p>Q
</p>
<p>Quadrature 31
backward rectangular rule 33, 57, 65
central rectangular rule 34, 58, 65, 68,
</p>
<p>107, 218
closed integration rule 34, 35
elemental area 33, 35
forward rectangular rule 33, 56, 65, 148,
</p>
<p>358
GAUSS-HERMITE 45
GAUSS-LEGENDRE 41
</p>
<p>error 45
grid-point 43, 45
weight 43, 45
</p>
<p>improper integrals 48
integral transform 48
Monte-Carlo integration 211, 218, 220,
</p>
<p>236
approximation of � 211
error 218, 219
expectation value 214
hit and miss 213
mean-value 214
mean-value integration 214
</p>
<p>multiple integrals 49
NEWTON-COTES rules 38
</p>
<p>closed 38
open 39
</p>
<p>open integration rule 34
rectangular rule 32, 34, 39
</p>
<p>error 33, 34
ROMBERG method 39
SIMPSON rule 37, 39
</p>
<p>error 40
three-eight rule 38, 39
total error 40
</p>
<p>trapezoidal rule 35, 39, 66
error 36, 39
total error 40
</p>
<p>R
</p>
<p>Random number 184
non-uniform distribution 186, 197
pseudo 185
real 185
sequence
</p>
<p>correlation 185, 190
moments 185, 190
moments error 190
</p>
<p>uniform distribution 185, 298
Random number generator
</p>
<p>criteria 186
FIBONACCI 188
</p>
<p>lagged 188
linear congruential 187, 212
</p>
<p>PARK-MILLER parameters 187
shuffling 187
</p>
<p>MARSAGLIA-ZAMAN 188
carry bit 189
</p>
<p>period 186
quality
</p>
<p>�2 test 191
hypothesis test 191
spectral test 191
statistical tests 190
</p>
<p>seed 187
shift register 188
</p>
<p>Random sampling See also MARKOV-chain
Monte-Carlo sampling
</p>
<p>direct method 197
importance sampling 219, 297&ndash;299
inverse transformation 200, 220
n-sphere 326
probability mixing 206
rejection method 202, 219, 220
</p>
<p>acceptance probability 202
envelope 202
histogram test 205
</p>
<p>simple sampling 298
Random variable 183, 248, 364
</p>
<p>central moments 365
characteristic function 371
mean value 364
moments 364
standard deviation 365
uncorrelated 366
variance 365
</p>
<p>Random walk 273
biased 274</p>
<p/>
</div>
<div class="page"><p/>
<p>408 Index
</p>
<p>definition 273
moments 275
probability of first return 278
recurrence 277
recurrence probability 278
transition rate 273
unbiased 274
variance 276
</p>
<p>Randomness 183
definition
</p>
<p>CHAITIN 184
event 184
measurement 184
probability 184
</p>
<p>Rectangular rules 33, 34, 56&ndash;58, 65, 68, 107,
148, 218
</p>
<p>Reflection principle 277
Regula falsi 348
Rejection method 202, 205, 219, 220
Relative error See Error
RIEMANN-LIOUVILLE fractional derivative
</p>
<p>292, 379
RIEMANN-LIOUVILLE fractional integral
</p>
<p>379
RIESZ fractional derivative 292, 380
RIESZ fractional integral 380
ROMBERG method 39
Roundoff error See Error
RUNGE-KUTTA methods 66, 68, 70, 72, 76
</p>
<p>S
</p>
<p>SCHR&Ouml;DINGER equation
basis 142
dimensionless variables 144
eigenenergy 140
eigenfunction 140
GAUSS wave packet 176
stationary
</p>
<p>one-dimensional 127, 143
time-dependent 170, 360
</p>
<p>CRANK-NICOLSONmethod 172
explicit EULER method 171
</p>
<p>time-evolution operator 171, 361
total wave-function 142
wave-function 139
</p>
<p>normalization 143
Series expansion
</p>
<p>LAGRANGE polynomial 38, 73
LEGENDRE polynomials 43
TAYLOR 19, 23, 32, 34, 66, 69, 106&ndash;108,
</p>
<p>168, 255, 318
</p>
<p>Shooting method 124
NUMEROV method 127, 147, 149
</p>
<p>SIMPSON rule 37, 39
Simulated annealing See Stochastic
</p>
<p>optimization
Slice sampling See MARKOV-chain
</p>
<p>Monte-Carlo sampling
Split operator technique 359, 361
Stability 5, 9, 157
</p>
<p>COURANT-FRIEDRICHS-LEWY condition
158, 168
</p>
<p>definition 9
Standard deviation 365
Standard error See Error
Statistical bootstrap 242, 314
Steepest descent See Deterministic
</p>
<p>optimization
STIRLING approximation 11, 278, 367
Stochastic differential equation 183, 284
</p>
<p>random force 284
Stochastic matrix 259
Stochastic optimization
</p>
<p>ant colony optimization 337
cost function 323
deluge algorithms 336
genetic algorithm 334
</p>
<p>traveling salesperson problem 335
grouping genetic algorithms 337
hill climbing 325
</p>
<p>N-queens problem 326
simulated annealing 327
</p>
<p>AARTS schedule 331
acceptance probability 329
fast 331
generalized 332
geometric cooling schedule 330
initial temperature 329
traveling salesperson problem 332
</p>
<p>threshold algorithms 336
Stochastic process
</p>
<p>auto-correlation function 249
auto-covariance function 249
conditional pdf 250
definition 248
Gaussian process 251
hierarchy of pdfs 249
independent increments 251
L&Eacute;VY process 251
moments 249
pdf 249
random variable 248
</p>
<p>realization 248
random walk 251
state space 248</p>
<p/>
</div>
<div class="page"><p/>
<p>Index 409
</p>
<p>stationary increments 250
stationary process 250
time span 248
time-homogeneous process 250
transition probability 250
WIENER process 251
</p>
<p>Stochastic variable See Random variable
ST&Ouml;RMER-VERLET method 66, 106
Subtractive cancellation 7, 19
SWENDSEN-WANG algorithm 307
Symplectic integrators 59, 73, 75&ndash;77
Symplectic mapping 74
</p>
<p>T
</p>
<p>TAYLOR theorem See Series expansion
Thermodynamic equilibrium 221
Thermodynamic expectation value 219
Time series plot 312
Time-dependent heat equation See Partial
</p>
<p>differential equation
Trapezoidal rule 35, 39, 66
Traveling salesperson problem 332, 335
Truncation error See Error
Two-body problem See KEPLER problem
</p>
<p>V
</p>
<p>Variance 147, 218, 365
velocity VERLET algorithm 108
Violation of energy conservation 75, 79
</p>
<p>W
</p>
<p>Wave equation See Partial differential
equation
</p>
<p>Wave-function See SCHR&Ouml;DINGER equation
WEYL fractional derivative 380
WEYL fractional integral 380
White noise 281
</p>
<p>Gaussian 282
WIENER process 251, 253, 279
</p>
<p>continuous limit 280
drift term 281
independent increments 280
self-similarity 281
standard process 281
</p>
<p>WOLFF algorithm 308</p>
<p/>
</div>
<ul>	<li>Preface</li>
	<li>Acknowledgments</li>
	<li>Contents</li>
	<li>1 Some Basic Remarks </li>
<ul>	<li>1.1 Motivation</li>
	<li>1.2 Roundoff Errors</li>
	<li>1.3 Methodological Errors</li>
	<li>1.4 Stability</li>
	<li>1.5 Concluding Remarks</li>
	<li>References</li>
</ul>
	<li>Part I Deterministic Methods</li>
<ul>	<li>2 Numerical Differentiation </li>
<ul>	<li>2.1 Introduction</li>
	<li>2.2 Finite Differences</li>
	<li>2.3 Finite Difference Derivatives</li>
	<li>2.4 A Systematic Approach: The Operator Technique</li>
	<li>2.5 Concluding Discussion</li>
	<li>Summary</li>
	<li>Problems</li>
	<li>References</li>
</ul>
	<li>3 Numerical Integration </li>
<ul>	<li>3.1 Introduction</li>
	<li>3.2 Rectangular Rule</li>
	<li>3.3 Trapezoidal Rule</li>
	<li>3.4 The Simpson Rule</li>
	<li>3.5 General Formulation: The Newton-Cotes Rules</li>
	<li>3.6 Gauss-Legendre Quadrature</li>
	<li>3.7 An Example</li>
	<li>3.8 Concluding Discussion</li>
	<li>Summary</li>
	<li>Problems</li>
	<li>References</li>
</ul>
	<li>4 The Kepler Problem </li>
<ul>	<li>4.1 Introduction</li>
	<li>4.2 Numerical Treatment</li>
	<li>Summary</li>
	<li>Problems</li>
	<li>References</li>
</ul>
	<li>5 Ordinary Differential Equations: Initial Value Problems </li>
<ul>	<li>5.1 Introduction</li>
	<li>5.2 Simple Integrators</li>
<ul>	<li>Taylor Series Methods</li>
	<li>Linear Multi-step Methods</li>
</ul>
	<li>5.3 Runge-Kutta Methods</li>
<ul>	<li>Explicit Euler:</li>
	<li>Implicit Euler:</li>
	<li>Crank-Nicolson:</li>
	<li>Explicit Midpoint:</li>
	<li>Implicit Midpoint:</li>
</ul>
	<li>5.4 Hamiltonian Systems: Symplectic Integrators</li>
<ul>	<li>Symplectic Euler</li>
	<li>Symplectic Runge-Kutta</li>
</ul>
	<li>5.5 An Example: The Kepler Problem, Revisited</li>
<ul>	<li>Explicit Euler</li>
	<li>Implicit Euler</li>
	<li>Symplectic Euler</li>
</ul>
	<li>Summary</li>
	<li>Problems</li>
	<li>References</li>
</ul>
	<li>6 The Double Pendulum </li>
<ul>	<li>6.1 Hamilton's Equations</li>
	<li>6.2 Numerical Solution</li>
	<li>6.3 Numerical Analysis of Chaos</li>
	<li>Summary</li>
	<li>Problems</li>
	<li>References</li>
</ul>
	<li>7 Molecular Dynamics </li>
<ul>	<li>7.1 Introduction</li>
	<li>7.2 Classical Molecular Dynamics</li>
	<li>7.3 Numerical Implementation</li>
<ul>	<li>Boundary Conditions</li>
<ul>	<li>Initialization and Equilibration</li>
</ul>
</ul>
	<li>Summary</li>
	<li>Problems</li>
	<li>References</li>
</ul>
	<li>8 Numerics of Ordinary Differential Equations: Boundary Value Problems </li>
<ul>	<li>8.1 Introduction</li>
	<li>8.2 Finite Difference Approach</li>
	<li>8.3 Shooting Methods</li>
	<li>Summary</li>
	<li>Problems</li>
	<li>References</li>
</ul>
	<li>9 The One-Dimensional Stationary Heat Equation</li>
<ul>	<li>9.1 Introduction</li>
	<li>9.2 Finite Differences</li>
	<li>9.3 A Second Scenario</li>
	<li>Summary</li>
	<li>Problems</li>
	<li>References</li>
</ul>
	<li>10 The One-Dimensional Stationary Schr&ouml;dinger Equation </li>
<ul>	<li>10.1 Introduction</li>
	<li>10.2 A Simple Example: The Particle in a Box</li>
	<li>10.3 Numerical Solution</li>
	<li>10.4 Another Case</li>
	<li>Summary</li>
	<li>Problems</li>
	<li>References</li>
</ul>
	<li>11 Partial Differential Equations </li>
<ul>	<li>11.1 Introduction</li>
	<li>11.2 The Poisson Equation</li>
	<li>11.3 The Time-Dependent Heat Equation</li>
	<li>11.4 The Wave Equation</li>
	<li>11.5 The Time-Dependent Schr&ouml;dinger Equation</li>
	<li>Summary</li>
	<li>Problems</li>
	<li>References</li>
</ul>
</ul>
	<li>Part II Stochastic Methods</li>
<ul>	<li>12 Pseudo-random Number Generators </li>
<ul>	<li>12.1 Introduction</li>
	<li>12.2 Different Methods</li>
<ul>	<li>Linear Congruential Generators</li>
	<li>Fibonacci Generators</li>
</ul>
	<li>12.3 Quality Tests</li>
<ul>	<li>Statistical Tests</li>
	<li>Hypothesis Testing</li>
</ul>
	<li>Summary</li>
	<li>Problems</li>
	<li>References</li>
</ul>
	<li>13 Random Sampling Methods </li>
<ul>	<li>13.1 Introduction</li>
	<li>13.2 Inverse Transformation Method</li>
	<li>13.3 Rejection Method</li>
	<li>13.4 Probability Mixing</li>
	<li>Summary</li>
	<li>Problems</li>
	<li>References</li>
</ul>
	<li>14 A Brief Introduction to Monte-Carlo Methods </li>
<ul>	<li>14.1 Introduction</li>
	<li>14.2 Monte-Carlo Integration</li>
	<li>14.3 The Metropolis Algorithm: An Introduction</li>
	<li>Summary</li>
	<li>References</li>
</ul>
	<li>15 The Ising Model </li>
<ul>	<li>15.1 The Model</li>
	<li>15.2 Numerics</li>
<ul>	<li>(1) Lattice Geometry</li>
	<li>(2) Initialization</li>
	<li>(3) Execution of the Code</li>
	<li>(4) Measurement</li>
</ul>
	<li>15.3 Selected Results</li>
	<li>Summary</li>
	<li>Problems</li>
	<li>References</li>
</ul>
	<li>16 Some Basics of Stochastic Processes </li>
<ul>	<li>16.1 Introduction</li>
	<li>16.2 Stochastic Processes</li>
	<li>16.3 Markov Processes</li>
	<li>16.4 Markov-Chains</li>
	<li>16.5 Continuous-Time Markov-Chains</li>
	<li>Summary</li>
	<li>Problems</li>
	<li>References</li>
</ul>
	<li>17 The Random Walk and Diffusion Theory </li>
<ul>	<li>17.1 Introduction</li>
	<li>17.2 The Random Walk</li>
<ul>	<li>Basics</li>
	<li>Moments</li>
	<li>Recurrence</li>
</ul>
	<li>17.3 The Wiener Process and Brownian Motion</li>
	<li>17.4 Generalized Diffusion Models</li>
	<li>Summary</li>
	<li>Problems</li>
	<li>References</li>
</ul>
	<li>18 Markov-Chain Monte Carlo and the Potts Model </li>
<ul>	<li>18.1 Introduction</li>
	<li>18.2 Markov-Chain Monte Carlo Methods</li>
	<li>18.3 The Potts Model</li>
	<li>18.4 Advanced Algorithms for the Potts Model</li>
	<li>Summary</li>
	<li>Problems</li>
	<li>References</li>
</ul>
	<li>19 Data Analysis </li>
<ul>	<li>19.1 Introduction</li>
	<li>19.2 Calculation of Errors</li>
	<li>19.3 Auto-Correlations</li>
	<li>19.4 The Histogram Technique</li>
	<li>Summary</li>
	<li>Problems</li>
	<li>References</li>
</ul>
	<li>20 Stochastic Optimization </li>
<ul>	<li>20.1 Introduction</li>
	<li>20.2 Hill Climbing</li>
	<li>20.3 Simulated Annealing</li>
<ul>	<li>(i) Proposal Probability</li>
	<li>(ii) Acceptance of Probability</li>
	<li>(iii) Cooling Strategy</li>
</ul>
	<li>20.4 Genetic Algorithms</li>
	<li>20.5 Some Further Methods</li>
	<li>Summary</li>
	<li>Problems</li>
	<li>References</li>
</ul>
</ul>
	<li>A The Two-Body Problem </li>
	<li>B Solving Non-linear Equations: The Newton Method </li>
	<li>C Numerical Solution of Linear Systems of Equations </li>
<ul>	<li>C.1 The LU Decomposition</li>
	<li>C.2 The Gauss-Seidel Method</li>
</ul>
	<li>D Fast Fourier Transform </li>
	<li>E Basics of Probability Theory </li>
<ul>	<li>E.1 Classical Definition</li>
	<li>E.2 Random Variables and Moments</li>
	<li>E.3 Binomial Distribution and Limit Theorems</li>
	<li>E.4 Poisson Distribution and Counting Experiments</li>
	<li>E.5 Continuous Variables</li>
	<li>E.6 Bayes' Theorem</li>
	<li>E.7 Normal Distribution</li>
	<li>E.8 Central Limit Theorem</li>
	<li>E.9 Characteristic Function</li>
	<li>E.10 The Correlation Coefficient</li>
	<li>E.11 Stable Distributions</li>
</ul>
	<li>F Phase Transitions </li>
<ul>	<li>F.1 Some Basics</li>
	<li>F.2 Landau Theory</li>
</ul>
	<li>G Fractional Integrals and Derivatives in 1D </li>
	<li>H Least Squares Fit </li>
<ul>	<li>H.1 Motivation</li>
	<li>H.2 Linear Least Squares Fit</li>
	<li>H.3 Nonlinear Least Squares Fit</li>
</ul>
	<li>I Deterministic Optimization </li>
<ul>	<li>I.1 Introduction</li>
	<li>I.2 Steepest Descent</li>
	<li>I.3 Conjugate Gradients</li>
	<li>References</li>
</ul>
	<li>Index</li>
</ul>
</body></html>