<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Untitled</title>
</head>
<body><div class="page"><p/>
<p>Wolfgang&nbsp;Karl&nbsp;H&auml;rdle
L&eacute;opold&nbsp;Simar
</p>
<p>Applied 
Multivariate 
Statistical 
Analysis
 Fourth Edition </p>
<p/>
</div>
<div class="page"><p/>
<p>Applied Multivariate Statistical Analysis</p>
<p/>
</div>
<div class="page"><p/>
</div>
<div class="page"><p/>
<p>Wolfgang Karl HRardle &bull; L&eacute;opold Simar
</p>
<p>Applied Multivariate
Statistical Analysis
</p>
<p>Fourth Edition
</p>
<p>123</p>
<p/>
</div>
<div class="page"><p/>
<p>Wolfgang Karl HRardle
C.A.S.E. Centre f. Appl. Stat. &amp; Econ.
</p>
<p>School of Business and Economics
Humboldt-UniversitRat zu Berlin
Berlin, Germany
</p>
<p>L&eacute;opold Simar
Center of Operations Research &amp;
</p>
<p>Econometrics (CORE)
Katholieke Univeristeit Leuven Inst.
</p>
<p>Statistics
Leuven, Belgium
</p>
<p>The majority of chapters have quantlet codes in Matlab or R. These quantlets may be
downloaded from http://extras.springer.com or via a link on http://springer.com/978-3-662-
45170-0 and from www.quantlet.de
</p>
<p>ISBN 978-3-662-45170-0 ISBN 978-3-662-45171-7 (eBook)
DOI 10.1007/978-3-662-45171-7
</p>
<p>Library of Congress Control Number: 2015933294
</p>
<p>Mathematics Subject Classification (2000): 62H10, 62H12, 62H15, 62H17, 62H20, 62H25,
62H30, 62F25
</p>
<p>Springer Heidelberg New York Dordrecht London
&copy; Springer-Verlag Berlin Heidelberg 2003, 2007, 2012, 2015
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of
the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microfilms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology
now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a specific statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this book
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or
the editors give a warranty, express or implied, with respect to the material contained herein or for any
errors or omissions that may have been made.
</p>
<p>Printed on acid-free paper
</p>
<p>Springer-Verlag GmbH Berlin Heidelberg is part of Springer Science+Business Media (www.springer.
com)</p>
<p/>
<div class="annotation"><a href="http://extras.springer.com">http://extras.springer.com</a></div>
<div class="annotation"><a href="http://springer.com/978-3-662-45170-0">http://springer.com/978-3-662-45170-0</a></div>
<div class="annotation"><a href="http://springer.com/978-3-662-45170-0">http://springer.com/978-3-662-45170-0</a></div>
<div class="annotation"><a href="www.quantlet.de">www.quantlet.de</a></div>
<div class="annotation"><a href="www.springer.com">www.springer.com</a></div>
<div class="annotation"><a href="www.springer.com">www.springer.com</a></div>
</div>
<div class="page"><p/>
<p>Preface to the Fourth Edition
</p>
<p>The fourth edition of this book on Applied Multivariate Statistical Analysis offers
</p>
<p>a new sub-chapter on Variable Selection by using least absolute shrinkage and
</p>
<p>selection operator (LASSO) and its general form the so-called Elastic Net.
</p>
<p>All pictures and numerical examples have been now calculated in the (almost)
</p>
<p>standard language R &amp; MATLAB. The code for each picture is indicated with
</p>
<p>a small sign near the picture, e.g. MVAdenbank denotes the corresponding
</p>
<p>quantlet for reproduction of Fig. 1.9, where we display the densities of the diagonal
</p>
<p>of genuine and counterfeit bank notes. We believe that these publicly available
</p>
<p>quantlets (see also http://sfb649.wiwi.hu-berlin.de/quantnet/) create a valuable
</p>
<p>contribution to distribution of knowledge in the statistical science. The symbols and
</p>
<p>notations have also been standardised. In the preparation of the fourth edition, we
</p>
<p>received valuable input from Dedy Dwi Prastyo, Petra Burdejova, Sergey Nasekin
</p>
<p>and Awdesch Melzer. We would like to thank them.
</p>
<p>Berlin, Germany Wolfgang Karl H&auml;rdle
</p>
<p>Louvain la Neuve, Belgium L&eacute;opold Simar
</p>
<p>January 2014
</p>
<p>v</p>
<p/>
<div class="annotation"><a href="http://sfb649.wiwi.hu-berlin.de/quantnet/">http://sfb649.wiwi.hu-berlin.de/quantnet/</a></div>
</div>
<div class="page"><p/>
</div>
<div class="page"><p/>
<p>Preface to the Third Edition
</p>
<p>The third edition of this book on Applied Multivariate Statistical Analysis offers the
</p>
<p>following new features.
</p>
<p>1. A new Chap. 8 on Regression Models has been added.
</p>
<p>2. Almost all numerical examples have been reproduced in MATLAB or R.
</p>
<p>The chapter on regression models focuses on a core business of multivariate
</p>
<p>statistical analysis. This contribution has not been subject of a prominent discussion
</p>
<p>in earlier editions of this book. We now take the opportunity to cover classical
</p>
<p>themes of ANOVA and ANCOVA analysis. Categorical responses are presented in
</p>
<p>Sect. 8.2. The spectrum of log linear models for contingency tables is presented in
</p>
<p>Sect. 8.2.2, and applications to count data, e.g. in the economic and medical science
</p>
<p>are presented there. Logit models are discussed in great detail, and the numerical
</p>
<p>implementation in terms of matrix manipulations is presented.
</p>
<p>The majority of pictures and numerical examples has been now calculated in the
</p>
<p>(almost) standard language R &amp; MATLAB. The code for each picture is indicated
</p>
<p>with a small sign near the picture, e.g. MVAdenbank denotes the corresponding
</p>
<p>quantlet for reproduction of Fig. 1.9, where we display the densities of the diagonal
</p>
<p>of genuine and counterfeit bank notes. We believe that these publicly available
</p>
<p>quantlets (see also www.quantlet.com) create a valuable contribution to distribution
</p>
<p>of knowledge in the statistical science. The symbols and notations have also been
</p>
<p>standardised. In the preparation of the third edition, we received valuable input from
</p>
<p>Song Song, Weining Wang and Mengmeng Guo. We would like to thank them.
</p>
<p>Berlin, Germany Wolfgang Karl H&auml;rdle
</p>
<p>Louvain la Neuve, Belgium L&eacute;opold Simar
</p>
<p>June 2011
</p>
<p>vii</p>
<p/>
<div class="annotation"><a href="www.quantlet.com">www.quantlet.com</a></div>
</div>
<div class="page"><p/>
</div>
<div class="page"><p/>
<p>Contents
</p>
<p>Part I Descriptive Techniques
</p>
<p>1 Comparison of Batches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
</p>
<p>1.1 Boxplots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
</p>
<p>1.2 Histograms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
</p>
<p>1.3 Kernel Densities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
</p>
<p>1.4 Scatterplots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
</p>
<p>1.5 Chernoff-Flury Faces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
</p>
<p>1.6 Andrews&rsquo; Curves. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
</p>
<p>1.7 Parallel Coordinates Plots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
</p>
<p>1.8 Hexagon Plots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
</p>
<p>1.9 Boston Housing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
</p>
<p>1.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
</p>
<p>Part II Multivariate Random Variables
</p>
<p>2 A Short Excursion into Matrix Algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
</p>
<p>2.1 Elementary Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
</p>
<p>2.2 Spectral Decompositions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
</p>
<p>2.3 Quadratic Forms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
</p>
<p>2.4 Derivatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
</p>
<p>2.5 Partitioned Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
</p>
<p>2.6 Geometrical Aspects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
</p>
<p>2.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
</p>
<p>3 Moving to Higher Dimensions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
</p>
<p>3.1 Covariance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
</p>
<p>3.2 Correlation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
</p>
<p>3.3 Summary Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
</p>
<p>3.4 Linear Model for Two Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
</p>
<p>3.5 Simple Analysis of Variance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
</p>
<p>ix</p>
<p/>
</div>
<div class="page"><p/>
<p>x Contents
</p>
<p>3.6 Multiple Linear Model. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
</p>
<p>3.7 Boston Housing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
</p>
<p>3.8 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
</p>
<p>4 Multivariate Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
</p>
<p>4.1 Distribution and Density Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
</p>
<p>4.2 Moments and Characteristic Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
</p>
<p>4.3 Transformations .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
</p>
<p>4.4 The Multinormal Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
</p>
<p>4.5 Sampling Distributions and Limit Theorems . . . . . . . . . . . . . . . . . . . . . . 142
</p>
<p>4.6 Heavy-Tailed Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
</p>
<p>4.7 Copulae .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166
</p>
<p>4.8 Bootstrap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176
</p>
<p>4.9 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
</p>
<p>5 Theory of the Multinormal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
</p>
<p>5.1 Elementary Properties of the Multinormal. . . . . . . . . . . . . . . . . . . . . . . . . 183
</p>
<p>5.2 The Wishart Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191
</p>
<p>5.3 Hotelling&rsquo;s T 2-Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193
</p>
<p>5.4 Spherical and Elliptical Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195
</p>
<p>5.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197
</p>
<p>6 Theory of Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201
</p>
<p>6.1 The Likelihood Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202
</p>
<p>6.2 The Cramer&ndash;Rao Lower Bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206
</p>
<p>6.3 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210
</p>
<p>7 Hypothesis Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213
</p>
<p>7.1 Likelihood Ratio Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214
</p>
<p>7.2 Linear Hypothesis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224
</p>
<p>7.3 Boston Housing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242
</p>
<p>7.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246
</p>
<p>Part III Multivariate Techniques
</p>
<p>8 Regression Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253
</p>
<p>8.1 General ANOVA and ANCOVA Models . . . . . . . . . . . . . . . . . . . . . . . . . . 255
</p>
<p>8.1.1 ANOVA Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255
</p>
<p>8.1.2 ANCOVA Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 260
</p>
<p>8.1.3 Boston Housing .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 262
</p>
<p>8.2 Categorical Responses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263
</p>
<p>8.2.1 Multinomial Sampling and Contingency Tables . . . . . . . . . . 263
</p>
<p>8.2.2 Log-Linear Models for Contingency Tables . . . . . . . . . . . . . . 264
</p>
<p>8.2.3 Testing Issues with Count Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268
</p>
<p>8.2.4 Logit Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 271
</p>
<p>8.3 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279</p>
<p/>
</div>
<div class="page"><p/>
<p>Contents xi
</p>
<p>9 Variable Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281
</p>
<p>9.1 Lasso. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282
</p>
<p>9.1.1 Lasso in the Linear Regression Model . . . . . . . . . . . . . . . . . . . . 282
</p>
<p>9.1.2 Lasso in High Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292
</p>
<p>9.1.3 Lasso in Logit Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293
</p>
<p>9.2 Elastic Net . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297
</p>
<p>9.2.1 Elastic Net in Linear Regression Model . . . . . . . . . . . . . . . . . . . 298
</p>
<p>9.2.2 Elastic Net in Logit Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299
</p>
<p>9.3 Group Lasso . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 300
</p>
<p>9.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304
</p>
<p>10 Decomposition of Data Matrices by Factors . . . . . . . . . . . . . . . . . . . . . . . . . . . . 305
</p>
<p>10.1 The Geometric Point of View . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 306
</p>
<p>10.2 Fitting the p-Dimensional Point Cloud . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307
</p>
<p>10.3 Fitting the n-Dimensional Point Cloud . . . . . . . . . . . . . . . . . . . . . . . . . . . . 310
</p>
<p>10.4 Relations Between Subspaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312
</p>
<p>10.5 Practical Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 314
</p>
<p>10.6 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317
</p>
<p>11 Principal Components Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319
</p>
<p>11.1 Standardised Linear Combination.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 320
</p>
<p>11.2 Principal Components in Practice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 324
</p>
<p>11.3 Interpretation of the PCs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327
</p>
<p>11.4 Asymptotic Properties of the PCs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 331
</p>
<p>11.5 Normalised Principal Components Analysis . . . . . . . . . . . . . . . . . . . . . . 335
</p>
<p>11.6 Principal Components as a Factorial Method. . . . . . . . . . . . . . . . . . . . . . 336
</p>
<p>11.7 Common Principal Components . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 342
</p>
<p>11.8 Boston Housing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 346
</p>
<p>11.9 More Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 348
</p>
<p>11.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357
</p>
<p>12 Factor Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 359
</p>
<p>12.1 The Orthogonal Factor Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 360
</p>
<p>12.2 Estimation of the Factor Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367
</p>
<p>12.3 Factor Scores and Strategies. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 376
</p>
<p>12.4 Boston Housing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 378
</p>
<p>12.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382
</p>
<p>13 Cluster Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 385
</p>
<p>13.1 The Problem.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 386
</p>
<p>13.2 The Proximity Between Objects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 387
</p>
<p>13.3 Cluster Algorithms .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 392
</p>
<p>13.4 Boston Housing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 400
</p>
<p>13.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 404</p>
<p/>
</div>
<div class="page"><p/>
<p>xii Contents
</p>
<p>14 Discriminant Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 407
</p>
<p>14.1 Allocation Rules for Known Distributions . . . . . . . . . . . . . . . . . . . . . . . . 407
</p>
<p>14.2 Discrimination Rules in Practice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415
</p>
<p>14.3 Boston Housing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 421
</p>
<p>14.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 423
</p>
<p>15 Correspondence Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 425
</p>
<p>15.1 Motivation .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 426
</p>
<p>15.2 Chi-Square Decomposition .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 428
</p>
<p>15.3 Correspondence Analysis in Practice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 432
</p>
<p>15.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 441
</p>
<p>16 Canonical Correlation Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 443
</p>
<p>16.1 Most Interesting Linear Combination .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 443
</p>
<p>16.2 Canonical Correlation in Practice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 448
</p>
<p>16.3 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 454
</p>
<p>17 Multidimensional Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455
</p>
<p>17.1 The Problem.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455
</p>
<p>17.2 Metric MDS .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 460
</p>
<p>17.3 Nonmetric MDS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 465
</p>
<p>17.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 472
</p>
<p>18 Conjoint Measurement Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 473
</p>
<p>18.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 473
</p>
<p>18.2 Design of Data Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 475
</p>
<p>18.3 Estimation of Preference Orderings .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 478
</p>
<p>18.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 485
</p>
<p>19 Applications in Finance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 487
</p>
<p>19.1 Portfolio Choice. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 487
</p>
<p>19.2 Efficient Portfolio . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 488
</p>
<p>19.3 Efficient Portfolios in Practice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 496
</p>
<p>19.4 The Capital Asset Pricing Model. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 497
</p>
<p>19.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 499
</p>
<p>20 Computationally Intensive Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 501
</p>
<p>20.1 Simplicial Depth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 502
</p>
<p>20.2 Projection Pursuit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 505
</p>
<p>20.3 Sliced Inverse Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 511
</p>
<p>20.4 Support Vector Machines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 519
</p>
<p>20.5 Classification and Regression Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 534
</p>
<p>20.6 Boston Housing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 552
</p>
<p>20.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 554</p>
<p/>
</div>
<div class="page"><p/>
<p>Contents xiii
</p>
<p>Part IV Appendix
</p>
<p>21 Symbols and Notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 557
</p>
<p>22 Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 561
</p>
<p>22.1 Boston Housing Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 561
</p>
<p>22.2 Swiss Bank Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 562
</p>
<p>22.3 Car Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 562
</p>
<p>22.4 Classic Blue Pullovers Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 563
</p>
<p>22.5 US Companies Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 563
</p>
<p>22.6 French Food Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 563
</p>
<p>22.7 Car Marks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 564
</p>
<p>22.8 French Baccalaur&eacute;at Frequencies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 564
</p>
<p>22.9 Journaux Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 564
</p>
<p>22.10 US Crime Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 565
</p>
<p>22.11 Plasma Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 566
</p>
<p>22.12 WAIS Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 566
</p>
<p>22.13 ANOVA Data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 567
</p>
<p>22.14 Timebudget Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 567
</p>
<p>22.15 Geopol Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 568
</p>
<p>22.16 US Health Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 569
</p>
<p>22.17 Vocabulary Data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 570
</p>
<p>22.18 Athletic Records Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 570
</p>
<p>22.19 Unemployment Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 570
</p>
<p>22.20 Annual Population Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 570
</p>
<p>22.21 Bankruptcy Data I. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 571
</p>
<p>22.22 Bankruptcy Data II . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 571
</p>
<p>References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 573
</p>
<p>Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 577</p>
<p/>
</div>
<div class="page"><p/>
<p>Part I
</p>
<p>Descriptive Techniques</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 1
</p>
<p>Comparison of Batches
</p>
<p>Multivariate statistical analysis is concerned with analysing and understanding data
</p>
<p>in high dimensions. We suppose that we are given a set fxi gniD1 of n observations
of a variable vector X in Rp . That is, we suppose that each observation xi has p
</p>
<p>dimensions:
</p>
<p>xi D .xi1; xi2; : : : ; xip/;
</p>
<p>and that it is an observed value of a variable vector X 2 Rp . Therefore, X is
composed of p random variables:
</p>
<p>X D .X1; X2; : : : ; Xp/
</p>
<p>where Xj , for j D 1; : : : ; p, is a one-dimensional random variable. How do
we begin to analyse this kind of data? Before we investigate questions on what
</p>
<p>inferenceswe can reach from the data, we should think about how to look at the data.
</p>
<p>This involves descriptive techniques. Questions that we could answer by descriptive
</p>
<p>techniques are:
</p>
<p>&bull; Are there components of X that are more spread out than others?
</p>
<p>&bull; Are there some elements of X that indicate sub-groups of the data?
</p>
<p>&bull; Are there outliers in the components of X?
</p>
<p>&bull; How &ldquo;normal&rdquo; is the distribution of the data?
</p>
<p>&bull; Are there &ldquo;low-dimensional&rdquo; linear combinations of X that show &ldquo;non-normal&rdquo;
</p>
<p>behaviour?
</p>
<p>One difficulty of descriptive methods for high-dimensional data is the human
</p>
<p>perceptional system. Point clouds in two dimensions are easy to understand and to
</p>
<p>interpret. With modern interactive computing techniques we have the possibility
</p>
<p>to see real time 3D rotations and thus to perceive also three-dimensional data.
</p>
<p>A &ldquo;sliding technique&rdquo; as described in H&auml;rdle and Scott (1992) may give insight
</p>
<p>&copy; Springer-Verlag Berlin Heidelberg 2015
W.K. H&auml;rdle, L. Simar, Applied Multivariate Statistical Analysis,
DOI 10.1007/978-3-662-45171-7_1
</p>
<p>3</p>
<p/>
</div>
<div class="page"><p/>
<p>4 1 Comparison of Batches
</p>
<p>into four-dimensional structures by presenting dynamic 3D density contours as the
</p>
<p>fourth variable is changed over its range.
</p>
<p>A qualitative jump in presentation difficulties occurs for dimensions greater
</p>
<p>than or equal to 5, unless the high-dimensional structure can be mapped into
</p>
<p>lower-dimensional components (Klinke &amp; Polzehl, 1995). Features like clustered
</p>
<p>sub-groups or outliers, however, can be detected using a purely graphical analysis.
</p>
<p>In this chapter, we investigate the basic descriptive and graphical techniques
</p>
<p>allowing simple exploratory data analysis. We begin the exploration of a data
</p>
<p>set using boxplots. A boxplot is a simple univariate device that detects outliers
</p>
<p>component by component and that can compare distributions of the data among
</p>
<p>different groups. Next, several multivariate techniques are introduced (Flury faces,
</p>
<p>Andrews&rsquo; curves and parallel coordinates plots (PCPs)) which provide graphical
</p>
<p>displays addressing the questions formulated above. The advantages and the
</p>
<p>disadvantages of each of these techniques are stressed.
</p>
<p>Two basic techniques for estimating densities are also presented: histograms and
</p>
<p>kernel densities. A density estimate gives a quick insight into the shape of the
</p>
<p>distribution of the data. We show that kernel density estimates (KDEs) overcome
</p>
<p>some of the drawbacks of the histograms.
</p>
<p>Finally, scatterplots are shown to be very useful for plotting bivariate or
</p>
<p>trivariate variables against each other: they help to understand the nature of the
</p>
<p>relationship among variables in a data set and allow for the detection of groups or
</p>
<p>clusters of points. Draftman plots or matrix plots are the visualisation of several
</p>
<p>bivariate scatterplots on the same display. They help detect structures in conditional
</p>
<p>dependencies by brushing across the plots. Outliers and observations that need
</p>
<p>special attention may be discovered with Andrews curves and PCPs. This chapter
</p>
<p>ends with an explanatory analysis of the Boston Housing data.
</p>
<p>1.1 Boxplots
</p>
<p>Example 1.1 The Swiss bank data (see Chap. 22, Sect. 22.2) consists of 200
</p>
<p>measurements on Swiss bank notes. The first half of these measurements are from
</p>
<p>genuine bank notes, the other half are from counterfeit bank notes.
</p>
<p>The authorities measured, as indicated in Fig. 1.1,
</p>
<p>X1 D length of the bill
X2 D height of the bill (left)
X3 D height of the bill (right)
X4 D distance of the inner frame to the lower border
X5 D distance of the inner frame to the upper border
X6 D length of the diagonal of the central picture.</p>
<p/>
</div>
<div class="page"><p/>
<p>1.1 Boxplots 5
</p>
<p>X 1
</p>
<p>X 2 X 3
</p>
<p>X 4
</p>
<p>X 5
</p>
<p>Fig. 1.1 An old Swiss 1000-franc bank note
</p>
<p>These data are taken from Flury and Riedwyl (1988). The aim is to study
</p>
<p>how these measurements may be used in determining whether a bill is genuine or
</p>
<p>counterfeit.
</p>
<p>The boxplot is a graphical technique that displays the distribution of variables. It
</p>
<p>helps us see the location, skewness, spread, tail length and outlying points.
</p>
<p>It is particularly useful in comparing different batches. The boxplot is a graphical
</p>
<p>representation of the Five Number Summary. To introduce the Five Number
</p>
<p>Summary, let us consider for a moment a smaller, one-dimensional data set:
</p>
<p>the population of the 15 largest world cities in 2006 (Table 1.1).
</p>
<p>In the Five Number Summary, we calculate the upper quartileFU , the lower quar-
</p>
<p>tile FL, the median and the extremes. Recall that order statistics fx.1/; x.2/; : : : ; x.n/g
are a set of ordered values x1; x2; : : : ; xn where x.1/ denotes the minimum and x.n/
the maximum. The median M typically cuts the set of observations in two equal
</p>
<p>parts, and is defined as
</p>
<p>M D
</p>
<p>8
&lt;
:
</p>
<p>x� nC1
2
</p>
<p>� n odd
</p>
<p>1
2
</p>
<p>n
x. n2 /
</p>
<p>C x. n2C1/
o
n even
</p>
<p>: (1.1)</p>
<p/>
</div>
<div class="page"><p/>
<p>6 1 Comparison of Batches
</p>
<p>Table 1.1 The 15 largest
world cities in 2006
</p>
<p>City Country Pop. (10,000) Order statistics
</p>
<p>Tokyo Japan 3,420 x.15/
</p>
<p>Mexico city Mexico 2,280 x.14/
</p>
<p>Seoul South Korea 2,230 x.13/
</p>
<p>New York USA 2,190 x.12/
</p>
<p>Sao Paulo Brazil 2,020 x.11/
</p>
<p>Bombay India 1,985 x.10/
</p>
<p>Delhi India 1,970 x.9/
</p>
<p>Shanghai China 1,815 x.8/
</p>
<p>Los Angeles USA 1,800 x.7/
</p>
<p>Osaka Japan 1,680 x.6/
</p>
<p>Jakarta Indonesia 1,655 x.5/
</p>
<p>Calcutta India 1,565 x.4/
</p>
<p>Cairo Egypt 1,560 x.3/
</p>
<p>Manila Philippines 1,495 x.2/
</p>
<p>Karachi Pakistan 1,430 x.1/
</p>
<p>The quartiles cut the set into four equal parts, which are often called fourths (that is
</p>
<p>why we use the letter F ). Using a definition that goes back to Hoaglin, Mosteller,
</p>
<p>and Tukey (1983) the definition of a median can be generalised to fourths, eights,
</p>
<p>etc. Considering the order statistics we can define the depth of a data value x.i/
as minfi; n � i C 1g. If n is odd, the depth of the median is nC1
</p>
<p>2
. If n is even,
</p>
<p>nC1
2
</p>
<p>is a fraction. Thus, the median is determined to be the average between
</p>
<p>the two data values belonging to the next larger and smaller order statistics, i.e.
</p>
<p>M D 1
2
</p>
<p>n
x. n2 /
</p>
<p>C x. n2C1/
o
. In our example, we have n D 15 hence the median
</p>
<p>M D x.8/ D 1;815.
We proceed in the same way to get the fourths. Take the depth of the median and
</p>
<p>calculate
</p>
<p>depth of fourth D Œdepth of median&#141;C 1
2
</p>
<p>with Œz&#141; denoting the largest integer smaller than or equal to z. In our example this
</p>
<p>gives 4:5 and thus leads to the two fourths
</p>
<p>FL D
1
</p>
<p>2
</p>
<p>˚
x.4/ C x.5/
</p>
<p>�
</p>
<p>FU D
1
</p>
<p>2
</p>
<p>˚
x.11/ C x.12/
</p>
<p>�
</p>
<p>(recalling that a depth which is a fraction corresponds to the average of the two
</p>
<p>nearest data values).</p>
<p/>
</div>
<div class="page"><p/>
<p>1.1 Boxplots 7
</p>
<p>Table 1.2 Five number
summary
</p>
<p># 15 World cities
</p>
<p>M 8 1,815
</p>
<p>F 4.5 1,610 2,105
</p>
<p>1 1,430 3,420
</p>
<p>The F -spread, dF , is defined as dF D FU � FL. The outside bars
</p>
<p>FU C 1:5dF (1.2)
FL � 1:5dF (1.3)
</p>
<p>are the borders beyond which a point is regarded as an outlier. For the number of
</p>
<p>points outside these bars see Exercise 1.3. For the n D 15 data points the fourths are
1610 D 1
</p>
<p>2
</p>
<p>˚
x.4/ C x.5/
</p>
<p>�
and 2105 D 1
</p>
<p>2
</p>
<p>˚
x.11/ C x.12/
</p>
<p>�
. Therefore the F -spread and
</p>
<p>the upper and lower outside bars in the above example are calculated as follows:
</p>
<p>dF D FU � FL D 2105� 1610 D 495 (1.4)
FL � 1:5dF D 1610� 1:5 � 495 D 867:5 (1.5)
FU C 1:5dF D 2105C 1:5 � 495 D 2847:5: (1.6)
</p>
<p>Since Tokyo is beyond the outside bars it is considered to be an outlier. The mini-
</p>
<p>mum and the maximum are called the extremes. The mean is defined as
</p>
<p>x D n�1
nX
</p>
<p>iD1
xi ;
</p>
<p>which is 1;939:7 in our example. The mean is a measure of location. The median
</p>
<p>(1815), the fourths (1610;2105) and the extremes (1430;3420) constitute basic
</p>
<p>information about the data. The combination of these five numbers leads to the Five
</p>
<p>Number Summary as shown in Table 1.2. The depths of each of the five numbers
</p>
<p>have been added as an additional column.
</p>
<p>Construction of the Boxplot
</p>
<p>1. Draw a box with borders (edges) at FL and FU (i.e. 50% of the data are in this
</p>
<p>box).
</p>
<p>2. Draw the median as a solid line (j) and the mean as a dotted line ().
3. Draw &ldquo;whiskers&rdquo; from each end of the box to the most remote point that is NOT
</p>
<p>an outlier.
</p>
<p>4. Show outliers as either &ldquo;?&rdquo; or &ldquo;�&rdquo;depending on whether they are outside ofFUL˙
1:5dF or FUL˙3dF respectively (this feather is not contained in some software).
Label them if possible.</p>
<p/>
</div>
<div class="page"><p/>
<p>8 1 Comparison of Batches
</p>
<p>World Cities
</p>
<p>1500
</p>
<p>2000
</p>
<p>2500
</p>
<p>3000
</p>
<p>3500
Boxplot
</p>
<p>Fig. 1.2 Boxplot for world cities MVAboxcity
</p>
<p>In the world cities example, the cut-off points (outside bars) are at 867:5 and
</p>
<p>2847.5, hence we can draw whiskers to Karachi and Mexico City. We can see from
</p>
<p>Fig. 1.2 that the data are very skew: The upper half of the data (above the median)
</p>
<p>is more spread out than the lower half (below the median), the data contains one
</p>
<p>outlier marked as a circle and the mean (as a non-robust measure of location) is
</p>
<p>pulled away from the median.
</p>
<p>Boxplots are very useful tools in comparing batches. The relative location of
</p>
<p>the distribution of different batches tells us a lot about the batches themselves.
</p>
<p>Before we come back to the Swiss bank data, let us compare the fuel economy
</p>
<p>of vehicles from different countries, see Fig. 1.3 and Table 22.3.
</p>
<p>Example 1.2 The data are from the second column of Table 22.3 and show
</p>
<p>the mileage (miles per gallon) of American, Japanese and European cars.
</p>
<p>The five-number summaries for these data sets are f12; 16:8; 18:8; 22; 30g,
f18; 22; 25; 30:5; 35g and f14; 19; 23; 25; 28g for American, Japanese and European
cars, respectively. This reflects the information shown in Fig. 1.3. The following
</p>
<p>conclusions can be made:
</p>
<p>&bull; Japanese cars achieve higher fuel efficiency than US and European cars.
</p>
<p>&bull; There is one outlier, a very fuel-efficient car (VW-Rabbit Golf Diesel).
</p>
<p>&bull; The main body of the US car data (the box) lies below the Japanese car data.
</p>
<p>&bull; The worst Japanese car is more fuel-efficient than almost 50% of the US cars.
</p>
<p>&bull; The spread of the Japanese and the US cars are almost equal.
</p>
<p>&bull; The median of the Japanese data is above that of the European data and the US
</p>
<p>data.</p>
<p/>
</div>
<div class="page"><p/>
<p>1.1 Boxplots 9
</p>
<p>Fig. 1.3 Boxplot for the
mileage of American,
Japanese and European cars
(from left to right)
MVAboxcar
</p>
<p>US JAPAN EU
</p>
<p>15
</p>
<p>20
</p>
<p>25
</p>
<p>30
</p>
<p>35
</p>
<p>40
</p>
<p>Car Data
</p>
<p>Fig. 1.4 The X6 variable of
Swiss bank data (diagonal of
bank notes)
MVAboxbank6
</p>
<p>GENUINE COUNTERFEIT
</p>
<p>138
</p>
<p>139
</p>
<p>140
</p>
<p>141
</p>
<p>142
</p>
<p>Swiss Bank Notes
</p>
<p>Table 1.3 Five number
summary
</p>
<p># 100 Genuine bank notes
</p>
<p>M 50.5 141.5
</p>
<p>F 25.75 141.25 141.8
</p>
<p>1 140.65 142.4
</p>
<p>Now let us apply the boxplot technique to the bank data set. In Fig. 1.4 we
</p>
<p>show the parallel boxplot of the diagonal variable X6. On the left is the value of
</p>
<p>the genuine bank notes and on the right the value of the counterfeit bank notes. The
</p>
<p>five number summary is reported in Table 1.3 and 1.4.</p>
<p/>
</div>
<div class="page"><p/>
<p>10 1 Comparison of Batches
</p>
<p>Table 1.4 Five number
summary
</p>
<p># 100 Counterfeit bank notes
</p>
<p>M 50.5 139.5
</p>
<p>F 25.75 139.2 139.8
</p>
<p>1 138.3 140.65
</p>
<p>Fig. 1.5 The X1 variable of
Swiss bank data (length of
bank notes)
MVAboxbank1
</p>
<p>GENUINE COUNTERFEIT
</p>
<p>214
</p>
<p>214.5
</p>
<p>215
</p>
<p>215.5
</p>
<p>216
</p>
<p>Swiss Bank Notes
</p>
<p>One sees that the diagonals of the genuine bank notes tend to be larger. It is
</p>
<p>harder to see a clear distinction when comparing the length of the bank notes X1,
</p>
<p>see Fig. 1.5. There are a few outliers in both plots. Almost all the observations of
</p>
<p>the diagonal of the genuine notes are above the ones from the counterfeit notes.
</p>
<p>There is one observation in Fig. 1.4 of the genuine notes that is almost equal to
</p>
<p>the median of the counterfeit notes. Can the parallel boxplot technique help us
</p>
<p>distinguish between the two types of bank notes?
</p>
<p>Summary
</p>
<p>,! The median and mean bars are measures of locations.
</p>
<p>,! The relative location of the median (and the mean) in the box is a
measure of how skewed it is.
</p>
<p>,! The length of the box and whiskers are a measure of spread.
</p>
<p>,! The length of the whiskers indicate the tail length of the distribu-
tion.
</p>
<p>,! The outlying points are indicated with a &ldquo;?&rdquo; or &ldquo;�&rdquo; depending on
if they are outside of FUL ˙ 1:5dF or FUL ˙ 3dF respectively.</p>
<p/>
</div>
<div class="page"><p/>
<p>1.2 Histograms 11
</p>
<p>Summary (continued)
</p>
<p>,! The boxplots do not indicate multi-modality or clusters.
</p>
<p>,! If we compare the relative size and location of the boxes, we are
comparing distributions.
</p>
<p>1.2 Histograms
</p>
<p>Histograms are density estimates. A density estimate gives a good impression of the
</p>
<p>distribution of the data. In contrast to boxplots, density estimates show possible
</p>
<p>multimodality of the data. The idea is to locally represent the data density by
</p>
<p>counting the number of observations in a sequence of consecutive intervals (bins)
</p>
<p>with origin x0. Let Bj .x0; h/ denote the bin of length h which is the element of a
</p>
<p>bin grid starting at x0:
</p>
<p>Bj .x0; h/ D Œx0 C .j � 1/h; x0 C jh/; j 2 Z;
</p>
<p>where Œ:; :/ denotes a left closed and right open interval. If fxi gniD1 is an i.i.d. sample
with density f , the histogram is defined as follows:
</p>
<p>Ofh.x/ D n�1h�1
X
</p>
<p>j2Z
</p>
<p>nX
</p>
<p>iD1
Ifxi 2 Bj .x0; h/g Ifx 2 Bj .x0; h/g: (1.7)
</p>
<p>In sum (1.7) the first indicator function Ifxi 2 Bj .x0; h/g (see Symbols and
Notation in Chap. 21) counts the number of observations falling into bin Bj .x0; h/.
</p>
<p>The second indicator function is responsible for &ldquo;localising&rdquo; the counts around x.
</p>
<p>The parameter h is a smoothing or localising parameter and controls the width of
</p>
<p>the histogram bins. An h that is too large leads to very big blocks and thus to a
</p>
<p>very unstructured histogram. On the other hand, an h that is too small gives a very
</p>
<p>variable estimate with many unimportant peaks.
</p>
<p>The effect of h is given in detail in Fig. 1.6. It contains the histogram (upper
</p>
<p>left) for the diagonal of the counterfeit bank notes for x0 D 137:8 (the minimum
of these observations) and h D 0:1. Increasing h to h D 0:2 and using the same
origin, x0 D 137:8, results in the histogram shown in the lower left of the figure.
This density histogram is somewhat smoother due to the larger h. The binwidth is
</p>
<p>next set to h D 0:3 (upper right). From this histogram, one has the impression that
the distribution of the diagonal is bimodal with peaks at about 138.5 and 139.9.</p>
<p/>
</div>
<div class="page"><p/>
<p>12 1 Comparison of Batches
</p>
<p>138 139 140 141
0
</p>
<p>2
</p>
<p>4
</p>
<p>6
</p>
<p>8
</p>
<p>10
</p>
<p>h = 0.1
</p>
<p>138 139 140 141
0
</p>
<p>5
</p>
<p>10
</p>
<p>15
</p>
<p>20
</p>
<p>h = 0.2
</p>
<p>138 139 140 141
0
</p>
<p>5
</p>
<p>10
</p>
<p>15
</p>
<p>20
</p>
<p>25
</p>
<p>30
</p>
<p>h = 0.3
</p>
<p>138 139 140 141
0
</p>
<p>10
</p>
<p>20
</p>
<p>30
</p>
<p>40
</p>
<p>h = 0.4
</p>
<p>Fig. 1.6 Diagonal of counterfeit bank notes. Histograms with x0 D 137:8 and h D 0:1 (upper
left), h D 0:2 (lower left), h D 0:3 (upper right), h D 0:4 (lower right) MVAhisbank1
</p>
<p>The detection of modes requires fine tuning of the binwidth. Using methods from
</p>
<p>smoothing methodology (H&auml;rdle, M&uuml;ller, Sperlich, &amp; Werwatz, 2004) one can find
</p>
<p>an &ldquo;optimal&rdquo; binwidth h for n observations:
</p>
<p>hopt D
�
24
p
�
</p>
<p>n
</p>
<p>�1=3
:
</p>
<p>Unfortunately, the binwidth h is not the only parameter determining the shapes of Of .
In Fig. 1.7, we show histograms with x0 D 137:65 (upper left), x0 D 137:75
</p>
<p>(lower left), with x0 D 137:85 (upper right), and x0 D 137:95 (lower right). All
the graphs have been scaled equally on the y-axis to allow comparison. One sees
</p>
<p>that&mdash;despite the fixed binwidth h&mdash;the interpretation is not facilitated. The shift
</p>
<p>of the origin x0 (to four different locations) created four different histograms. This</p>
<p/>
</div>
<div class="page"><p/>
<p>1.2 Histograms 13
</p>
<p>138 139 140 141
0
</p>
<p>20
</p>
<p>40
</p>
<p>x
0
 = 137.65
</p>
<p>138 139 140 141
0
</p>
<p>20
</p>
<p>40
</p>
<p>x
0
 = 137.75
</p>
<p>138 139 140 141
0
</p>
<p>20
</p>
<p>40
</p>
<p>x
0
</p>
<p> = 137.85
</p>
<p>138 139 140 141
0
</p>
<p>20
</p>
<p>40
</p>
<p>x
0
 = 137.95
</p>
<p>Fig. 1.7 Diagonal of counterfeit bank notes. Histogram with h D 0:4 and origins x0 D 137:65
(upper left), x0 D 137:75 (lower left), x0 D 137:85 (upper right), x0 D 137:95 (lower right)
MVAhisbank2
</p>
<p>property of histograms strongly contradicts the goal of presenting data features.
</p>
<p>Obviously, the same data are represented quite differently by the four histograms. A
</p>
<p>remedy has been proposed by Scott (1985): &ldquo;Average the shifted histograms!&rdquo;. The
</p>
<p>result is presented in Fig. 1.8.
</p>
<p>Here all bank note observations (genuine and counterfeit) have been used. The
</p>
<p>(so-called) averaged shifted histogram is no longer dependent on the origin and
</p>
<p>shows a clear bimodality of the diagonals of the Swiss bank notes.</p>
<p/>
</div>
<div class="page"><p/>
<p>14 1 Comparison of Batches
</p>
<p>138 139 140 141 142
0
</p>
<p>0.1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>Swiss Bank Notes
</p>
<p>2 shifts
</p>
<p>D
ia
</p>
<p>g
o
</p>
<p>n
a
l
</p>
<p>138 139 140 141 142
0
</p>
<p>0.1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>Swiss Bank Notes
</p>
<p>8 shifts
</p>
<p>D
ia
</p>
<p>g
o
</p>
<p>n
a
l
</p>
<p>138 139 140 141 142
0
</p>
<p>0.1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>Swiss Bank Notes
</p>
<p>4 shifts
</p>
<p>D
ia
</p>
<p>g
o
</p>
<p>n
a
l
</p>
<p>138 139 140 141 142
0
</p>
<p>0.1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>Swiss Bank Notes
</p>
<p>16 shifts
</p>
<p>D
ia
</p>
<p>g
o
</p>
<p>n
a
l
</p>
<p>Fig. 1.8 Averaged shifted histograms based on all (counterfeit and genuine) Swiss bank notes:
there are 2 shifts (upper left), 4 shifts (lower left), 8 shifts (upper right) and 16 shifts (lower right)
MVAashbank
</p>
<p>Summary
</p>
<p>,! Modes of the density are detected with a histogram.
</p>
<p>,! Modes correspond to strong peaks in the histogram.
</p>
<p>,! Histograms with the same h need not be identical. They also
depend on the origin x0 of the grid.
</p>
<p>,! The influence of the origin x0 is drastic. Changing x0 creates
different looking histograms.
</p>
<p>,! The consequence of an h that is too large is an unstructured
histogram that is too flat.
</p>
<p>,! A binwidth h that is too small results in an unstable histogram.</p>
<p/>
</div>
<div class="page"><p/>
<p>1.3 Kernel Densities 15
</p>
<p>Summary (continued)
</p>
<p>,! There is an &ldquo;optimal&rdquo; h D .24p�=n/1=3.
</p>
<p>,! It is recommended to use averaged histograms. They are kernel
densities.
</p>
<p>1.3 Kernel Densities
</p>
<p>The major difficulties of histogram estimation may be summarised in four cri-
</p>
<p>tiques:
</p>
<p>&bull; determination of the binwidth h, which controls the shape of the histogram,
</p>
<p>&bull; choice of the bin origin x0, which also influences to some extent the shape,
</p>
<p>&bull; loss of information since observations are replaced by the central point of the
</p>
<p>interval in which they fall,
</p>
<p>&bull; the underlying density function is often assumed to be smooth, but the histogram
</p>
<p>is not smooth.
</p>
<p>Rosenblatt (1956), Whittle (1958) and Parzen (1962) developed an approach
</p>
<p>which avoids the last three difficulties. First, a smooth kernel function rather than
</p>
<p>a box is used as the basic building block. Second, the smooth function is centred
</p>
<p>directly over each observation. Let us study this refinement by supposing that x is
</p>
<p>the centre value of a bin. The histogram can in fact be rewritten as
</p>
<p>Ofh.x/ D n�1h�1
nX
</p>
<p>iD1
I
</p>
<p>�
jx � xi j �
</p>
<p>h
</p>
<p>2
</p>
<p>�
: (1.8)
</p>
<p>If we define K.u/ D I.juj � 1
2
/, then (1.8) changes to
</p>
<p>Ofh.x/ D n�1h�1
nX
</p>
<p>iD1
K
�x � xi
</p>
<p>h
</p>
<p>�
: (1.9)
</p>
<p>This is the general form of the kernel estimator. Allowing smoother kernel functions
</p>
<p>like the quartic kernel,
</p>
<p>K.u/ D 15
16
.1 � u2/2 I.juj � 1/;
</p>
<p>and computing x not only at bin centers gives us the kernel density estimator.
</p>
<p>Kernel estimators can also be derived via weighted averaging of rounded points
</p>
<p>(WARPing) or by averaging histograms with different origins, see Scott (1985).
</p>
<p>Table 1.5 introduces some commonly used kernels.</p>
<p/>
</div>
<div class="page"><p/>
<p>16 1 Comparison of Batches
</p>
<p>Table 1.5 Kernel functions K.�/ Kernel
K.u/ D 1
</p>
<p>2
I.juj � 1/ Uniform
</p>
<p>K.u/ D .1� juj/ I.juj � 1/ Triangle
K.u/ D 3
</p>
<p>4
.1� u2/ I.juj � 1/ Epanechnikov
</p>
<p>K.u/ D 15
16
.1� u2/2 I.juj � 1/ Quartic (Biweight)
</p>
<p>K.u/ D 1p
2�
</p>
<p>exp.� u2
2
/ D '.u/ Gaussian
</p>
<p>Different kernels generate different shapes of the estimated density. The most
</p>
<p>important parameter is the so-called bandwidth h, and can be optimised, for exam-
</p>
<p>ple, by cross-validation; see H&auml;rdle (1991) for details. The cross-validation method
</p>
<p>minimises the integrated squared error. This measure of discrepancy is based on
</p>
<p>the squared differences
n
Ofh.x/ � f .x/
</p>
<p>o2
. Averaging these squared deviations over
</p>
<p>a grid of points fxlgLlD1 leads to
</p>
<p>L�1
LX
</p>
<p>lD1
</p>
<p>n
Ofh.xl / � f .xl /
</p>
<p>o2
:
</p>
<p>Asymptotically, if this grid size tends to zero, we obtain the integrated squared error:
</p>
<p>Z n
Ofh.x/ � f .x/
</p>
<p>o2
dx:
</p>
<p>In practice, it turns out that the method consists of selecting a bandwidth that
</p>
<p>minimises the cross-validation function
</p>
<p>Z
Of 2h � 2
</p>
<p>nX
</p>
<p>iD1
</p>
<p>Ofh;i .xi /;
</p>
<p>where Ofh;i is the density estimate obtained by using all datapoints except for the i -th
observation. Both terms in the above function involve double sums. Computation
</p>
<p>may therefore be slow. There are many other density bandwidth selection methods.
</p>
<p>Probably the fastest way to calculate this is to refer to some reasonable reference
</p>
<p>distribution. The idea of using the Normal distribution as a reference, for example,
</p>
<p>goes back to Silverman (1986). The resulting choice of h is called the rule of thumb.
</p>
<p>For the Gaussian kernel from Table 1.5 and a Normal reference distribution, the
</p>
<p>rule of thumb is to choose
</p>
<p>hG D 1:06 O� n�1=5 (1.10)</p>
<p/>
</div>
<div class="page"><p/>
<p>1.3 Kernel Densities 17
</p>
<p>Fig. 1.9 Densities of the
diagonals of genuine and
counterfeit bank notes.
Automatic density
estimates MVAdenbank
</p>
<p>137 138 139 140 141 142 143
0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>Swiss bank notes
</p>
<p>Counterfeit / Genuine
</p>
<p>where O� D
p
n�1
</p>
<p>Pn
iD1.xi � x/2 denotes the sample standard deviation. This
</p>
<p>choice of hG optimises the integrated squared distance between the estimator and
</p>
<p>the true density. For the quartic kernel, we need to transform (1.10). The modified
</p>
<p>rule of thumb is:
</p>
<p>hQ D 2:62 � hG : (1.11)
</p>
<p>Figure 1.9 shows the automatic density estimates for the diagonals of the coun-
</p>
<p>terfeit and genuine bank notes. The density on the left is the density corresponding
</p>
<p>to the diagonal of the counterfeit data. The separation is clearly visible, but there is
</p>
<p>also an overlap. The problem of distinguishing between the counterfeit and genuine
</p>
<p>bank notes is not solved by just looking at the diagonals of the notes. The question
</p>
<p>arises whether a better separation could be achieved using not only the diagonals,
</p>
<p>but one or two more variables of the data set. The estimation of higher dimensional
</p>
<p>densities is analogous to that of one dimensional. We show a two-dimensional
</p>
<p>density estimate for X4 and X5 in Fig. 1.10. The contour lines indicate the height
</p>
<p>of the density. One sees two separate distributions in this higher dimensional space,
</p>
<p>but they still overlap to some extent.
</p>
<p>We can add one more dimension and give a graphical representation of a three-
</p>
<p>dimensional density estimate, or more precisely an estimate of the joint distribution
</p>
<p>of X4, X5 and X6. Figure 1.11 shows the contour areas at three different levels of
</p>
<p>the density: 0:2 (green), 0:4 (red) and 0:6 (blue) of this three-dimensional density
</p>
<p>estimate. One can clearly recognise two &ldquo;ellipsoids&rdquo; (at each level), but as before,
</p>
<p>they overlap. In Chap. 14 we will learn how to separate the two ellipsoids and how
</p>
<p>to develop a discrimination rule to distinguish between these data points.</p>
<p/>
</div>
<div class="page"><p/>
<p>18 1 Comparison of Batches
</p>
<p>Fig. 1.10 Contours of the
density of X5 and X6 of
genuine and counterfeit bank
notes MVAcontbank2
</p>
<p>0.02
</p>
<p>0.06
</p>
<p>0.08
</p>
<p>0.1
</p>
<p>0.1
</p>
<p>0.12
</p>
<p>0.12
</p>
<p>0.14
</p>
<p>0.14
</p>
<p>0.16
</p>
<p>0.16
</p>
<p>0.18
</p>
<p>9 10 11 12
</p>
<p>1
3
</p>
<p>8
1
</p>
<p>3
9
</p>
<p>1
4
</p>
<p>0
1
</p>
<p>4
1
</p>
<p>1
4
</p>
<p>2
1
</p>
<p>4
3
</p>
<p>0.04
</p>
<p>Fig. 1.11 Contours of the
density of X4; X5; X6 of
genuine and counterfeit bank
notes MVAcontbank3</p>
<p/>
</div>
<div class="page"><p/>
<p>1.4 Scatterplots 19
</p>
<p>Summary
</p>
<p>,! Kernel densities estimate distribution densities by the kernel
method.
</p>
<p>,! The bandwidth h determines the degree of smoothness of the
estimate Of .
</p>
<p>,! Kernel densities are smooth functions and they can graphically
represent distributions (up to three dimensions).
</p>
<p>,! A simple (but not necessarily correct) way to find a good bandwidth
is to compute the rule of thumb bandwidth hG D 1:06 O�n�1=5:
This bandwidth is to be used only in combination with a Gaussian
</p>
<p>kernel '.
</p>
<p>,! Kernel density estimates are a good descriptive tool for seeing
modes, location, skewness, tails, asymmetry, etc.
</p>
<p>1.4 Scatterplots
</p>
<p>Scatterplots are bivariate or trivariate plots of variables against each other. They help
</p>
<p>us understand relationships among the variables of a data set. A downward-sloping
</p>
<p>scatter indicates that as we increase the variable on the horizontal axis, the variable
</p>
<p>on the vertical axis decreases. An analogous statement can be made for upward-
</p>
<p>sloping scatters.
</p>
<p>Figure 1.12 plots the 5th column (upper inner frame) of the bank data against
</p>
<p>the 6th column (diagonal). The scatter is downward-sloping. As we already know
</p>
<p>from the previous section on marginal comparison (e.g. Fig. 1.9) a good separation
</p>
<p>between genuine and counterfeit bank notes is visible for the diagonal variable.
</p>
<p>The sub-cloud in the upper half (circles) of Fig. 1.12 corresponds to the true bank
</p>
<p>notes. As noted before, this separation is not distinct, since the two groups overlap
</p>
<p>somewhat.
</p>
<p>This can be verified in an interactive computing environment by showing the
</p>
<p>index and coordinates of certain points in this scatterplot. In Fig. 1.12, the 70th
</p>
<p>observation in the merged data set is given as a thick circle, and it is from a genuine
</p>
<p>bank note. This observation lies well embedded in the cloud of counterfeit bank
</p>
<p>notes. One straightforward approach that could be used to tell the counterfeit from
</p>
<p>the genuine bank notes is to draw a straight line and define notes above this value as
</p>
<p>genuine.We would of course misclassify the 70th observation, but can we do better?</p>
<p/>
</div>
<div class="page"><p/>
<p>20 1 Comparison of Batches
</p>
<p>7 8 9 10 11 12 13
</p>
<p>138
</p>
<p>139
</p>
<p>140
</p>
<p>141
</p>
<p>142
</p>
<p>Swiss bank notes
</p>
<p>Fig. 1.12 2D scatterplot for X5 vs. X6 of the bank notes. Genuine notes are circles, counterfeit
notes are stars MVAscabank56
</p>
<p>8
</p>
<p>10
</p>
<p>12
</p>
<p>14 7
8
</p>
<p>9
10
</p>
<p>11
12
</p>
<p>139
</p>
<p>140
</p>
<p>141
</p>
<p>142
</p>
<p>Upper inner frame (X5)
</p>
<p>Swiss bank notes
</p>
<p>Lower inner frame (X4)
</p>
<p>D
ia
</p>
<p>g
o
</p>
<p>n
a
</p>
<p>l 
(X
</p>
<p>6
)
</p>
<p>Fig. 1.13 3D scatterplot of the bank notes for .X4; X5; X6/. Genuine notes are circles, counterfeit
are stars MVAscabank456
</p>
<p>If we extend the two-dimensional scatterplot by adding a third variable, e.g. X4
(lower distance to inner frame), we obtain the scatterplot in three dimensions as
</p>
<p>shown in Fig. 1.13. It becomes apparent from the location of the point clouds that a
</p>
<p>better separation is obtained. We have rotated the three-dimensional data until this
</p>
<p>satisfactory 3D view was obtained. Later, we will see that the rotation is the same
</p>
<p>as bundling a high-dimensional observation into one or more linear combinations</p>
<p/>
</div>
<div class="page"><p/>
<p>1.4 Scatterplots 21
</p>
<p>VAR 3
</p>
<p>VAR 4
</p>
<p>VAR 5
</p>
<p>VAR 6
</p>
<p>Fig. 1.14 Draftman&rsquo;s plot of the bank notes. The pictures in the left-hand column show
.X3; X4/, .X3; X5/ and .X3; X6/, in the middle we have .X4; X5/ and .X4; X6/, and in the
lower right .X5; X6/. The upper right half contains the corresponding density contour plots
MVAdrafbank4
</p>
<p>of the elements of the observation vector. In other words, the &ldquo;separation line"
</p>
<p>parallel to the horizontal coordinate axis in Fig. 1.12 is, in Fig. 1.13, a plane and
</p>
<p>no longer parallel to one of the axes. The formula for such a separation plane is a
</p>
<p>linear combination of the elements of the observation vector:
</p>
<p>a1x1 C a2x2 C � � � C a6x6 D const: (1.12)
</p>
<p>The algorithm that automatically finds the weights (a1; : : : ; a6) will be investigated
</p>
<p>later on in Chap. 14.
</p>
<p>Let us study yet another technique: the scatterplot matrix. If we want to draw all
</p>
<p>possible two-dimensional scatterplots for the variables, we can create a so-called
</p>
<p>draftman&rsquo;s plot (named after a draftman who prepares drafts for parliamentary
</p>
<p>discussions). Similar to a draftman&rsquo;s plot the scatterplot matrix helps in creating
</p>
<p>new ideas and in building knowledge about dependencies and structure.
</p>
<p>Figure 1.14 shows a draftman&rsquo;s plot applied to the last four columns of the full
</p>
<p>bank data set. For ease of interpretation we have distinguished between the group of
</p>
<p>counterfeit and genuine bank notes by a different colour. As discussed several times</p>
<p/>
</div>
<div class="page"><p/>
<p>22 1 Comparison of Batches
</p>
<p>earlier, the separability of the two types of notes is different for different scatterplots.
</p>
<p>Not only is it difficult to perform this separation on, say, scatterplot X3 vs. X4, in
</p>
<p>addition the &ldquo;separation line&rdquo; is no longer parallel to one of the axes. The most
</p>
<p>obvious separation happens in the scatterplot in the lower right-hand side where
</p>
<p>indicated, as in Fig. 1.12, X5 vs. X6. The separation line here would be upward-
</p>
<p>sloping with an intercept at about X6 D 139. The upper right half of the draftman&rsquo;s
plot shows the density contours that we introduced in Sect. 1.3.
</p>
<p>The power of the draftman&rsquo;s plot lies in its ability to show the internal
</p>
<p>connections of the scatter diagrams. Define a brush as a re-scalable rectangle that we
</p>
<p>can move via keyboard or mouse over the screen. Inside the brush we can highlight
</p>
<p>or colour observations. Suppose the technique is installed in such a way that as we
</p>
<p>move the brush in one scatter, the corresponding observations in the other scatters
</p>
<p>are also highlighted. By moving the brush, we can study conditional dependence.
</p>
<p>If we brush (i.e. highlight or colour the observation with the brush), the X5 vs.
</p>
<p>X6 plot and move through the upper point cloud, we see that in other plots (e.g. X3
vs. X4), the corresponding observations are more embedded in the other sub-cloud.
</p>
<p>Summary
</p>
<p>,! Scatterplots in two and three dimensions helps in identifying
separated points, outliers or sub-clusters.
</p>
<p>,! Scatterplots help us in judging positive or negative dependencies.
</p>
<p>,! Draftman scatterplot matrices help detect structures conditioned on
values of other variables.
</p>
<p>,! As the brush of a scatterplot matrix moves through a point cloud,
we can study conditional dependence.
</p>
<p>1.5 Chernoff-Flury Faces
</p>
<p>If we are given data in numerical form, we tend to also display it numerically. This
</p>
<p>was done in the preceding sections: an observation x1 D .1; 2/ was plotted as
the point .1; 2/ in a two-dimensional coordinate system. In multivariate analysis
</p>
<p>we want to understand data in low dimensions (e.g. on a 2D computer screen)
</p>
<p>although the structures are hidden in high dimensions. The numerical display of
</p>
<p>data structures using coordinates therefore ends at dimensions greater than three.</p>
<p/>
</div>
<div class="page"><p/>
<p>1.5 Chernoff-Flury Faces 23
</p>
<p>If we are interested in condensing a structure into 2D elements, we have to
</p>
<p>consider alternative graphical techniques. The Chernoff-Flury faces, for example,
</p>
<p>provide such a condensation of high-dimensional information into a simple &ldquo;face&rdquo;.
</p>
<p>In fact faces are a simple way of graphically displaying high-dimensional data. The
</p>
<p>size of the face elements like pupils, eyes, upper and lower hair line, etc. are assigned
</p>
<p>to certain variables. The idea of using faces goes back to Chernoff (1973) and has
</p>
<p>been further developed by Bernhard Flury. We follow the design described in Flury
</p>
<p>and Riedwyl (1988) which uses the following characteristics.
</p>
<p>1. right eye size
</p>
<p>2. right pupil size
</p>
<p>3. position of right pupil
</p>
<p>4. right eye slant
</p>
<p>5. horizontal position of right eye
</p>
<p>6. vertical position of right eye
</p>
<p>7. curvature of right eyebrow
</p>
<p>8. density of right eyebrow
</p>
<p>9. horizontal position of right eyebrow
</p>
<p>10. vertical position of right eyebrow
</p>
<p>11. right upper hair line
</p>
<p>12. right lower hair line
</p>
<p>13. right face line
</p>
<p>14. darkness of right hair
</p>
<p>15. right hair slant
</p>
<p>16. right nose line
</p>
<p>17. right size of mouth
</p>
<p>18. right curvature of mouth
</p>
<p>19&ndash;36. like 1&ndash;18, only for the left side.
</p>
<p>First, every variable that is to be coded into a characteristic face element is
</p>
<p>transformed into a .0; 1/ scale, i.e. the minimum of the variable corresponds to 0 and
</p>
<p>the maximum to 1. The extreme positions of the face elements therefore correspond
</p>
<p>to a certain &ldquo;grin&rdquo; or &ldquo;happy&rdquo; face element. Dark hair might be coded as 1, and
</p>
<p>blond hair as 0 and so on.
</p>
<p>As an example, consider the observations 91&ndash;110 of the bank data. Recall that
</p>
<p>the bank data set consists of 200 observations of dimension 6 where, for example,
</p>
<p>X6 is the diagonal of the note. If we assign the six variables to the following face
</p>
<p>elements
</p>
<p>X1 D 1, 19 (eye sizes)
X2 D 2, 20 (pupil sizes)
X3 D 4, 22 (eye slants)</p>
<p/>
</div>
<div class="page"><p/>
<p>24 1 Comparison of Batches
</p>
<p>91 92 93 94 95
</p>
<p>96 97 98 99 100
</p>
<p>101 102 103 104 105
</p>
<p>106 107 108 109 110
</p>
<p>Fig. 1.15 Chernoff-Flury faces for observations 91&ndash;110 of the bank notes MVAfacebank10
</p>
<p>X4 D 11, 29 (upper hair lines)
X5 D 12, 30 (lower hair lines)
X6 D 13, 14, 31, 32 (face lines and darkness of hair),
</p>
<p>we obtain Fig. 1.15. Also recall that observations 1&ndash;100 correspond to the genuine
</p>
<p>notes, and that observations 101&ndash;200 correspond to the counterfeit notes. The
</p>
<p>counterfeit bank notes then correspond to the upper half of Fig. 1.15. In fact the
</p>
<p>faces for these observations look more grim and less happy. The variable X6
(diagonal) already worked well in the boxplot in Fig. 1.4 in distinguishing between
</p>
<p>the counterfeit and genuine notes. Here, this variable is assigned to the face line and
</p>
<p>the darkness of the hair. That is why we clearly see a good separation within these
</p>
<p>20 observations.</p>
<p/>
</div>
<div class="page"><p/>
<p>1.5 Chernoff-Flury Faces 25
</p>
<p>1 2 3 4 5 6 7 8 9 10
</p>
<p>11 12 13 14 15 16 17 18 19 20
</p>
<p>21 22 23 24 25 26 27 28 29 30
</p>
<p>31 32 33 34 35 36 37 38 39 40
</p>
<p>41 42 43 44 45 46 47 48 49 50
</p>
<p>Observations 1 to 50
</p>
<p>Fig. 1.16 Chernoff-Flury faces for observations 1&ndash;50 of the bank notes MVAfacebank50
</p>
<p>What happens if we include all 100 genuine and all 100 counterfeit bank notes
</p>
<p>in the Chernoff-Flury face technique? Figures 1.16 and 1.17 show the faces of
</p>
<p>the genuine bank notes with the same assignments as used before, and Figs. 1.18
</p>
<p>and 1.19 show the faces of the counterfeit bank notes. Comparing Figs. 1.16
</p>
<p>and 1.18 one clearly sees that the diagonal (face line) is longer for genuine bank
</p>
<p>notes. Equivalently coded is the hair darkness (diagonal) which is lighter (shorter)
</p>
<p>for the counterfeit bank notes. One sees that the faces of the genuine bank notes
</p>
<p>have a much darker appearance and have broader face lines. The faces in Figs. 1.16
</p>
<p>and 1.17 are obviously different from the ones in Figs. 1.18 and 1.19.</p>
<p/>
</div>
<div class="page"><p/>
<p>26 1 Comparison of Batches
</p>
<p>51 52 53 54 55 56 57 58 59 60
</p>
<p>61 62 63 64 65 66 67 68 69 70
</p>
<p>71 72 73 74 75 76 77 78 79 80
</p>
<p>81 82 83 84 85 86 87 88 89 90
</p>
<p>91 92 93 94 95 96 97 98 99 100
</p>
<p>Observations 51 to 100
</p>
<p>Fig. 1.17 Chernoff-Flury faces for observations 51&ndash;100 of the bank notes MVAfacebank50</p>
<p/>
</div>
<div class="page"><p/>
<p>1.5 Chernoff-Flury Faces 27
</p>
<p>101 102 103 104 105 106 107 108 109 110
</p>
<p>111 112 113 114 115 116 117 118 119 120
</p>
<p>121 122 123 124 125 126 127 128 129 130
</p>
<p>131 132 133 134 135 136 137 138 139 140
</p>
<p>141 142 143 144 145 146 147 148 149 150
</p>
<p>Observations 101 to 150
</p>
<p>Fig. 1.18 Chernoff-Flury faces for observations 101&ndash;150 of the bank notes MVAfacebank50</p>
<p/>
</div>
<div class="page"><p/>
<p>28 1 Comparison of Batches
</p>
<p>151 152 153 154 155 156 157 158 159 160
</p>
<p>161 162 163 164 165 166 167 168 169 170
</p>
<p>171 172 173 174 175 176 177 178 179 180
</p>
<p>181 182 183 184 185 186 187 188 189 190
</p>
<p>191 192 193 194 195 196 197 198 199 200
</p>
<p>Observations 151 to 200
</p>
<p>Fig. 1.19 Chernoff-Flury faces for observations 151&ndash;200 of the bank notes MVAfacebank50</p>
<p/>
</div>
<div class="page"><p/>
<p>1.6 Andrews&rsquo; Curves 29
</p>
<p>Summary
</p>
<p>,! Faces can be used to detect sub-groups in multivariate data.
</p>
<p>,! Sub-groups are characterised by similar looking faces.
</p>
<p>,! Outliers are identified by extreme faces, e.g. dark hair, smile or a
happy face.
</p>
<p>,! If one element of X is unusual, the corresponding face element
significantly changes in shape.
</p>
<p>1.6 Andrews&rsquo; Curves
</p>
<p>The basic problem of graphical displays of multivariate data is the dimensionality.
</p>
<p>Scatterplots work well up to three dimensions (if we use interactive displays).
</p>
<p>More than three dimensions have to be coded into displayable 2D or 3D structures
</p>
<p>(e.g. faces). The idea of coding and representing multivariate data by curves was
</p>
<p>suggested by Andrews (1972). Each multivariate observationXi D .Xi;1; : : : ; Xi;p/
is transformed into a curve as follows:
</p>
<p>fi .t/ D
</p>
<p>8
ˆ̂̂
&lt;
ˆ̂̂
:
</p>
<p>Xi;1p
2
CXi;2 sin.t/CXi;3 cos.t/C � � �
CXi;p�1 sin
</p>
<p>�
p�1
2
t
�
CXi;p cos
</p>
<p>�
p�1
2
t
�
</p>
<p>for p odd
</p>
<p>Xi;1p
2
CXi;2 sin.t/CXi;3 cos.t/C � � � CXi;p sin
</p>
<p>�
p
</p>
<p>2
t
�
</p>
<p>for p even
</p>
<p>(1.13)
</p>
<p>the observation represents the coefficients of a so-called Fourier series (t 2 Œ��; �&#141;).
Suppose that we have three-dimensional observations: X1 D .0; 0; 1/, X2 D
</p>
<p>.1; 0; 0/ and X3 D .0; 1; 0/. Here p D 3 and the following representations
correspond to the Andrews&rsquo; curves:
</p>
<p>f1.t/ D cos.t/
</p>
<p>f2.t/ D
1p
2
</p>
<p>and
</p>
<p>f3.t/ D sin.t/:
</p>
<p>These curves are indeed quite distinct, since the observations X1, X2, and X3 are
</p>
<p>the 3D unit vectors: each observation has mass only in one of the three dimensions.
</p>
<p>The order of the variables plays an important role.</p>
<p/>
</div>
<div class="page"><p/>
<p>30 1 Comparison of Batches
</p>
<p>0 0.2 0.4 0.6 0.8 1
-2
</p>
<p>-1
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>3
Andrews curves (Bank data)
</p>
<p>Fig. 1.20 Andrews&rsquo; curves of the observations 96&ndash;105 from the Swiss bank note data. The order
of the variables is 1,2,3,4,5,6 MVAandcur
</p>
<p>Example 1.3 Let us take the 96th observation of the Swiss bank note data set,
</p>
<p>X96 D .215:6; 129:9; 129:9; 9:0; 9:5; 141:7/:
</p>
<p>The Andrews&rsquo; curve is by (1.13):
</p>
<p>f96.t/ D
215:6p
2
C 129:9 sin.t/C 129:9 cos.t/C 9:0 sin.2t/
</p>
<p>C 9:5 cos.2t/C 141:7 sin.3t/:
</p>
<p>Figure 1.20 shows the Andrews&rsquo; curves for observations 96&ndash;105 of the Swiss
</p>
<p>bank note data set. We already know that the observations 96&ndash;100 represent genuine
</p>
<p>bank notes, and that the observations 101&ndash;105 represent counterfeit bank notes. We
</p>
<p>see that at least four curves differ from the others, but it is hard to tell which curve
</p>
<p>belongs to which group.
</p>
<p>We know from Fig. 1.4 that the sixth variable is an important one. Therefore, the
</p>
<p>Andrews&rsquo; curves are calculated again using a reversed order of the variables.
</p>
<p>Example 1.4 Let us consider again the 96th observation of the Swiss bank note data
</p>
<p>set,
</p>
<p>X96 D .215:6; 129:9; 129:9; 9:0; 9:5; 141:7/:</p>
<p/>
</div>
<div class="page"><p/>
<p>1.6 Andrews&rsquo; Curves 31
</p>
<p>Fig. 1.21 Andrews&rsquo; curves
of the observations 96&ndash;105
from the Swiss bank note
data. The order of the
variables is 6,5,4,3,2,1
MVAandcur2
</p>
<p>0 0.2 0.4 0.6 0.8 1
-2
</p>
<p>-1
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>Andrews curves (Bank data)
</p>
<p>The Andrews&rsquo; curve is computed using the reversed order of variables:
</p>
<p>f96.t/ D
141:7p
2
C 9:5 sin.t/C 9:0 cos.t/C 129:9 sin.2t/
</p>
<p>C 129:9 cos.2t/C 215:6 sin.3t/:
</p>
<p>In Fig. 1.21 the curves f96&ndash;f105 for observations 96&ndash;105 are plotted. Instead of a
</p>
<p>difference in high frequency, nowwe have a difference in the intercept, which makes
</p>
<p>it more difficult for us to see the differences in observations.
</p>
<p>This shows that the order of the variables plays an important role in the
</p>
<p>interpretation. If X is high-dimensional, then the last variables will only have a
</p>
<p>small visible contribution to the curve: they fall into the high frequency part of
</p>
<p>the curve. To overcome this problem Andrews suggested using an order which
</p>
<p>is suggested by Principal Component Analysis. This technique will be treated in
</p>
<p>detail in Chap. 11. In fact, the sixth variable will appear there as the most important
</p>
<p>variable for discriminating between the two groups. If the number of observations
</p>
<p>is more than 20, there may be too many curves in one graph. This will result in
</p>
<p>an over plotting of curves or a bad &ldquo;signal-to-ink-ratio&rdquo;, see Tufte (1983). It is
</p>
<p>therefore advisable to present multivariate observations via Andrews&rsquo; curves only
</p>
<p>for a limited number of observations.</p>
<p/>
</div>
<div class="page"><p/>
<p>32 1 Comparison of Batches
</p>
<p>Summary
</p>
<p>,! Outliers appear as single Andrews&rsquo; curves that look different from
the rest.
</p>
<p>,! A sub-group of data is characterised by a set of similar curves.
</p>
<p>,! The order of the variables plays an important role for interpretation.
</p>
<p>,! The order of variables may be optimised by Principal Component
Analysis.
</p>
<p>,! For more than 20 observations we may obtain a bad &ldquo;signal-to-ink
ratio&rdquo;, i.e. too many curves are overlaid in one picture.
</p>
<p>1.7 Parallel Coordinates Plots
</p>
<p>PCP is a method for representing high-dimensional data, see Inselberg (1985).
</p>
<p>Instead of plotting observations in an orthogonal coordinate system, PCP draws
</p>
<p>coordinates in parallel axes and connects themwith straight lines. This method helps
</p>
<p>in representing data with more than four dimensions.
</p>
<p>One first scales all variables to max D 1 and min D 0. The coordinate index
j is drawn onto the horizontal axis, and the scaled value of variable xij is mapped
</p>
<p>onto the vertical axis. This way of representation is very useful for high-dimensional
</p>
<p>data. It is however also sensitive to the order of the variables, since certain trends in
</p>
<p>the data can be shown more clearly in one ordering than in another.
</p>
<p>Example 1.5 Take, once again, the observations 96&ndash;105 of the Swiss bank notes.
</p>
<p>These observations are six dimensional, so we can&rsquo;t show them in a six-dimensional
</p>
<p>Cartesian coordinate system. Using the PCP technique, however, they can be plotted
</p>
<p>on parallel axes. This is shown in Fig. 1.22.
</p>
<p>PCP can also be used for detecting linear dependencies between variables:
</p>
<p>if all the lines are of almost parallel dimensions (p D 2), there is a positive
linear dependence between them. In Fig. 1.23 we display the two variables weight
</p>
<p>and displacement for the car data set in Sect. 22.3. The correlation coefficient �
</p>
<p>introduced in Sect. 3.2 is 0.9. If all lines intersect visibly in the middle, there is
</p>
<p>evidence of a negative linear dependence between these two variables, see Fig. 1.24.
</p>
<p>In fact the correlation is � D �0:82 between two variables mileage and weight: The
more the weight, the less the mileage.</p>
<p/>
</div>
<div class="page"><p/>
<p>1.7 Parallel Coordinates Plots 33
</p>
<p>1 2 3 4 5 6
0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>Parallel coordinate plot (Bank data)
</p>
<p>Fig. 1.22 Parallel coordinates plot of observations 96&ndash;105 MVAparcoo1
</p>
<p>weight displacement
0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>Parallel Coordinate Plot (Car Data)
</p>
<p>Fig. 1.23 Parallel coordinates plot indicating strong positive dependence with � D 0:9, X1 D
weight, X2 D displacement MVApcp2
</p>
<p>Another use of PCP is sub-groups detection. Lines converging to different
</p>
<p>discrete points indicate sub-groups. Figure 1.25 shows the last three variables&mdash;
</p>
<p>displacement, gear ratio for high gear and company&rsquo;s headquarters of the car
</p>
<p>data; we see convergence to the last variable. This last variable is the company&rsquo;s
</p>
<p>headquarters with three discrete values: USA, Japan and Europe. PCP can also
</p>
<p>be used for outlier detection. Figure 1.26 shows the variables headroom, rear seat</p>
<p/>
</div>
<div class="page"><p/>
<p>34 1 Comparison of Batches
</p>
<p>Fig. 1.24 Parallel
coordinates plot showing
strong negative dependence
with � D �0:82, X1 D
mileage, X2 D weight
MVApcp3
</p>
<p>mileage weight
0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>Parallel Coordinate Plot(Car Data)
</p>
<p>Fig. 1.25 Parallel
coordinates plot with
sub-groups MVApcp4
</p>
<p>displacement gear ratio headquarters
0
</p>
<p>0.1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>0.6
</p>
<p>0.7
</p>
<p>0.8
</p>
<p>0.9
</p>
<p>1
</p>
<p>C
o
</p>
<p>o
rd
</p>
<p>in
a
</p>
<p>te
 V
</p>
<p>a
lu
</p>
<p>e
</p>
<p>Parallel Coordinate Plot (Car Data)
</p>
<p>clearance and trunk (boot) space in the car data set. There are two outliers visible.
</p>
<p>The boxplot Fig. 1.27 confirms this.
</p>
<p>PCPs have also possible shortcomings:We cannot distinguish observations when
</p>
<p>two lines cross at one point unless we distinguish them clearly (e.g. by different line
</p>
<p>style). In Fig. 1.28, observation A and B both have the same value at j D 2. Two
lines cross at one point here. At the 3rd and 4th dimension we cannot tell which line
</p>
<p>belongs to which observation. A dotted line for A and solid line for B could have
</p>
<p>helped there.
</p>
<p>To solve this problem one uses an interpolation curve instead of straight lines, e.g.
</p>
<p>cubic curves as in Graham and Kennedy (2003). Figure 1.29 is a variant of Fig. 1.28.
</p>
<p>In Fig. 1.29, with a natural cubic spline, it is evident how to follow the curves
</p>
<p>and distinguish the observations. The real power of PCP comes though through
</p>
<p>colouring sub-groups.</p>
<p/>
</div>
<div class="page"><p/>
<p>1.7 Parallel Coordinates Plots 35
</p>
<p>Fig. 1.26 PCP for
X1 D headroom, X2 D rear
seat clearance and
X3 D trunk space
MVApcp5
</p>
<p>headroom rear seat trunk space
0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>Parallel Coordinate Plot (Car Data)
</p>
<p>Fig. 1.27 Boxplots for
headroom, rear seat clearance
and trunk space
MVApcp6
</p>
<p>headroom rear seat trunk space
</p>
<p>5
</p>
<p>15
</p>
<p>25
</p>
<p>35
</p>
<p>Boxplot (Car Data)
</p>
<p>Example 1.6 Data in Fig. 1.30 are coloured according to X13&mdash;car company&rsquo;s
</p>
<p>headquarters. Red stands for European car, green for Japan and black for US. This
</p>
<p>PCP with colouring can provide some information for us:
</p>
<p>1. US cars (black) tend to have large value in X7, X8, X9, X10, X11 (trunk (boot)
</p>
<p>space, weight, length, turning diameter, displacement), which means US cars are
</p>
<p>generally larger.
</p>
<p>2. Japanese cars (green) have large value in X3, X4 (both for repair record), which
</p>
<p>means Japanese cars tend to be repaired less.</p>
<p/>
</div>
<div class="page"><p/>
<p>36 1 Comparison of Batches
</p>
<p>Fig. 1.28 PCP with
intersection for given data
points A D Œ0; 2; 3; 2&#141; and
BD Œ3; 2; 2; 1&#141; MVApcp7
</p>
<p>1 2 3 4
0
</p>
<p>0.5
</p>
<p>1
</p>
<p>1.5
</p>
<p>2
</p>
<p>2.5
</p>
<p>3
</p>
<p>Parallel Coordinate Plot
</p>
<p>A
</p>
<p>B
</p>
<p>Fig. 1.29 PCP with cubic
spline interpolation
MVApcp8
</p>
<p>1 2 3 4
0
</p>
<p>0.5
</p>
<p>1
</p>
<p>1.5
</p>
<p>2
</p>
<p>2.5
</p>
<p>3
</p>
<p>Parallel Coordinate Plot with Cubic Spline
</p>
<p>A
</p>
<p>B</p>
<p/>
</div>
<div class="page"><p/>
<p>1.8 Hexagon Plots 37
</p>
<p>1 2 3 4 5 6 7 8 9 10 11 12 13
0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>Parallel Coordinate Plot (Car Data)
</p>
<p>Fig. 1.30 Parallel coordinates plot for car data MVApcp1
</p>
<p>Summary
</p>
<p>,! Parallel coordinates plots overcome the visualisation problem of
the Cartesian coordinate system for dimensions greater than 4.
</p>
<p>,! Outliers are visible as outlying polygon curves.
</p>
<p>,! The order of variables is important, especially in the detection of
sub-groups.
</p>
<p>,! Sub-groups may be screened by selective colouring.
</p>
<p>1.8 Hexagon Plots
</p>
<p>This section closely follows the presentation of Lewin-Koh (2006). In geometry, a
</p>
<p>hexagon is a polygon with six edges and six vertices. Hexagon binning is a type of
</p>
<p>bivariate histogram with hexagon borders. It is useful for visualising the structure</p>
<p/>
</div>
<div class="page"><p/>
<p>38 1 Comparison of Batches
</p>
<p>of data sets entailing a large number of observations n. The concept of hexagon
</p>
<p>binning is as follows:
</p>
<p>1. The xy plane over the set (range(x), range(y)) is tessellated by a regular grid of
</p>
<p>hexagons.
</p>
<p>2. The number of points falling in each hexagon is counted.
</p>
<p>3. The hexagons with count &gt; 0 are plotted by using a colour ramp or varying the
</p>
<p>radius of the hexagon in proportion to the counts.
</p>
<p>This algorithm is extremely fast and effective for displaying the structure of data
</p>
<p>sets even for n � 106. If the size of the grid and the cuts in the colour ramp are
chosen in a clever fashion, then the structure inherent in the data should emerge in
</p>
<p>the binned plot. The same caveats apply to hexagon binning as histograms. Variance
</p>
<p>and bias vary in opposite directions with bin width, so we have to settle for finding
</p>
<p>the value of the bin width that yields the optimal compromise between variance and
</p>
<p>bias reduction. Clearly, if we increase the size of the grid, the hexagon plot appears
</p>
<p>to be smoother, but without some reasonable criterion on hand it remains difficult
</p>
<p>to say which bin width provides the &ldquo;optimal&rdquo; degree of smoothness. The default
</p>
<p>number of bins suggested by standard software is 30.
</p>
<p>Applications to some data sets are shown as follows. The data is taken from
</p>
<p>ALLBUS (2006)[ZANo.3762]. The number of respondents is 2,946. The following
</p>
<p>nine variables have been selected to analyse the relation between each pair of
</p>
<p>variables.
</p>
<p>X1: Age
</p>
<p>X2: Net income
</p>
<p>X3: Time for television per day in minutes
</p>
<p>X4: Time for work per week in hours
</p>
<p>X5: Time for computer per week in hours
</p>
<p>X6: Days for illness yearly
</p>
<p>X7: Living space (square metres)
</p>
<p>X8: Size
</p>
<p>X9: Weight
</p>
<p>Firstly, we consider two variables X1 D Age and X2 D Net income in Fig. 1.31.
The top left picture is a scatter plot. The second one is a hexagon plot with borders
</p>
<p>making it easier to see the separation between hexagons. Looking at these plots one
</p>
<p>can see that almost all individuals have a net monthly income of less than 2,000
</p>
<p>EUR. Only two individuals earn more than 10,000 EUR per month.
</p>
<p>Figure 1.32 shows the relation between X1 and X5. About 40% of respondents
</p>
<p>from 20 to 80 years old do not use a computer at least once per week. The
</p>
<p>respondent who deals with a computer 105 h each week was actually not in full-
</p>
<p>time employment.</p>
<p/>
</div>
<div class="page"><p/>
<p>1.8 Hexagon Plots 39
</p>
<p>Age
</p>
<p>N
e
t 
in
</p>
<p>c
o
m
</p>
<p>e
</p>
<p>0
</p>
<p>2000
</p>
<p>4000
</p>
<p>6000
</p>
<p>8000
</p>
<p>10000
</p>
<p>20 40 60 80
</p>
<p>Counts
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>5
</p>
<p>7
</p>
<p>9
</p>
<p>11
</p>
<p>14
</p>
<p>17
</p>
<p>21
</p>
<p>24
</p>
<p>28
</p>
<p>33
</p>
<p>37
</p>
<p>42
</p>
<p>47
</p>
<p>53
</p>
<p>a b
</p>
<p>Fig. 1.31 Hexagon plots between X1 and X2 MVAageIncome
</p>
<p>Age
</p>
<p>C
o
m
</p>
<p>p
u
te
</p>
<p>r 
ti
m
</p>
<p>e
</p>
<p>0
</p>
<p>20
</p>
<p>40
</p>
<p>60
</p>
<p>80
</p>
<p>100
</p>
<p>20 40 60 80
</p>
<p>Counts
</p>
<p>1
</p>
<p>3
</p>
<p>5
</p>
<p>8
</p>
<p>13
</p>
<p>17
</p>
<p>23
</p>
<p>30
</p>
<p>37
</p>
<p>45
</p>
<p>54
</p>
<p>64
</p>
<p>75
</p>
<p>86
</p>
<p>98
</p>
<p>111
</p>
<p>125
</p>
<p>Fig. 1.32 Hexagon plot between X1 and X5 MVAageCom
</p>
<p>Clearly, people who earn modest incomes live in smaller flats. The trend here
</p>
<p>is relatively clear in Fig. 1.33. The larger the net income, the larger the flat. A few
</p>
<p>people do however earn high incomes but live in small flats.</p>
<p/>
</div>
<div class="page"><p/>
<p>40 1 Comparison of Batches
</p>
<p>Fig. 1.33 Hexagon plot
between X2 and X7
MVAincomeLi
</p>
<p>Income
</p>
<p>F
la
</p>
<p>t 
s
iz
</p>
<p>e
</p>
<p>0
</p>
<p>100
</p>
<p>200
</p>
<p>300
</p>
<p>400
</p>
<p>500
</p>
<p>0 2000 4000 6000 8000 10000
</p>
<p>Counts
</p>
<p>1
</p>
<p>3
</p>
<p>6
</p>
<p>9
</p>
<p>14
</p>
<p>20
</p>
<p>27
</p>
<p>34
</p>
<p>43
</p>
<p>52
</p>
<p>63
</p>
<p>74
</p>
<p>87
</p>
<p>100
</p>
<p>114
</p>
<p>130
</p>
<p>146
</p>
<p>Summary
</p>
<p>,! Hexagon binning is a type of bivariate histogram, used for visual-
ising large data.
</p>
<p>,! Variance and bias vary in opposite directions with bin width.
</p>
<p>,! Hexagons have the property of &ldquo;symmetry of the nearest neigh-
bours&rdquo; which lacks in square bins.
</p>
<p>,! Hexagons are visually less biased for displaying densities than
other regular tessellations.
</p>
<p>1.9 Boston Housing
</p>
<p>Aim of the Analysis
</p>
<p>The Boston Housing data set was analysed by Harrison and Rubinfeld (1978) who
</p>
<p>wanted to find out whether &ldquo;clean air&rdquo; had an influence on house prices. We will
</p>
<p>use this data set in this chapter and in most of the following chapters to illustrate the
</p>
<p>presented methodology. The data are described in Sect. 22.1.</p>
<p/>
</div>
<div class="page"><p/>
<p>1.9 Boston Housing 41
</p>
<p>Fig. 1.34 Parallel
coordinates plot for Boston
housing data
MVApcphousing
</p>
<p>1 2 3 4 5 6 7 8 9 10 11 12 13 14
0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>Boston Housing
</p>
<p>What Can Be Seen from the PCPs
</p>
<p>In order to highlight the relations of X14 to the remaining 13 variables, we colour
</p>
<p>all of the observations with X14 &gt;median.X14/ as red lines in Fig. 1.34. Some of
</p>
<p>the variables seem to be strongly related. The most obvious relation is the negative
</p>
<p>dependence between X13 and X14. It can also be argued that a strong dependence
</p>
<p>exists between X12 and X14 since no red lines are drawn in the lower part of X12.
</p>
<p>The opposite can be said aboutX11: there are only red lines plotted in the lower part
</p>
<p>of this variable. Low values of X11 induce high values of X14.
</p>
<p>For the PCP, the variables have been rescaled over the interval Œ0; 1&#141; for better
</p>
<p>graphical representations. The PCP shows that the variables are not distributed in
</p>
<p>a symmetric manner. It can be clearly seen that the values of X1 and X9 are much
</p>
<p>more concentrated around 0. Therefore it makes sense to consider transformations
</p>
<p>of the original data.
</p>
<p>The Scatterplot Matrix
</p>
<p>One characteristic of PCPs is that many lines are drawn on top of each other. This
</p>
<p>problem is reduced by depicting the variables in pairs of scatterplots. Including all
</p>
<p>14 variables in one large scatterplot matrix is possible, but makes it hard to see
</p>
<p>anything from the plots. Therefore, for illustratory purposes we will analyse only
</p>
<p>one such matrix from a subset of the variables in Fig. 1.35. On the basis of the PCP
</p>
<p>and the scatterplot matrix we would like to interpret each of the 13 variables and
</p>
<p>their eventual relation to the 14th variable. Included in the figure are images for</p>
<p/>
</div>
<div class="page"><p/>
<p>42 1 Comparison of Batches
</p>
<p>Fig. 1.35 Scatterplot matrix
for variables X1; : : : ; X5 and
X14 of the Boston housing
data MVAdrafthousing
</p>
<p>Fig. 1.36 Scatterplot matrix
for variables QX1; : : : ; QX5 and
QX14 of the Boston housing
data
MVAdrafthousingt
</p>
<p>X1&ndash;X5 and X14, although each variable is discussed in detail below. All references
</p>
<p>made to scatterplots in the following refer to Fig. 1.35.
</p>
<p>Per-Capita Crime Rate X1
</p>
<p>Taking the logarithm makes the variable&rsquo;s distribution more symmetric. This can be
</p>
<p>seen in the boxplot of QX1 in Fig. 1.37 which shows that the median and the mean
have moved closer to each other than they were for the original X1. Plotting the
</p>
<p>KDE of QX1 D log .X1/ would reveal that two sub-groups might exist with different
mean values. However, taking a look at the scatterplots in Fig. 1.36 of the logarithms</p>
<p/>
</div>
<div class="page"><p/>
<p>1.9 Boston Housing 43
</p>
<p>which include X1 does not clearly reveal such groups. Given that the scatterplot of
</p>
<p>log .X1/ vs. log .X14/ shows a relatively strong negative relation, it might be the
</p>
<p>case that the two sub-groups of X1 correspond to houses with two different price
</p>
<p>levels. This is confirmed by the two boxplots shown to the right of the X1 vs. X2
scatterplot (in Fig. 1.35): the right boxplot&rsquo;s shape differs a lot from the black one&rsquo;s,
</p>
<p>having a much higher median and mean.
</p>
<p>Proportion of Residential Area Zoned for Large Lots X2
</p>
<p>It strikes the eye in Fig. 1.35 that there is a large cluster of observations for which
</p>
<p>X2 is equal to 0. It also strikes the eye that&mdash;as the scatterplot ofX1 vs.X2 shows&mdash;
</p>
<p>there is a strong, though non-linear, negative relation betweenX1 andX2; almost all
</p>
<p>observations for which X2 is high have an X1-value close to zero, and vice versa,
</p>
<p>many observations for which X2 is zero have quite a high per-capita crime rate X1.
</p>
<p>This could be due to the location of the areas, e.g. urban districts might have a
</p>
<p>higher crime rate and at the same time it is unlikely that any residential land would
</p>
<p>be zoned in a generous manner.
</p>
<p>As far as the house prices are concerned, it can be said that there seems to
</p>
<p>be no clear (linear) relation between X2 and X14, but it is obvious that the more
</p>
<p>expensive houses are situated in areas where X2 is large (this can be seen from the
</p>
<p>two boxplots on the second position of the diagonal, where the red one has a clearly
</p>
<p>higher mean/median than the black one).
</p>
<p>Proportion of Non-retail Business AcresX3
</p>
<p>The PCP (in Fig. 1.34) as well as the scatterplot of X3 vs. X14 shows an obvious
</p>
<p>negative relation between X3 and X14. The relationship between the logarithms of
</p>
<p>both variables seems to be almost linear. This negative relation might be explained
</p>
<p>by the fact that non-retail business sometimes causes annoying sounds and other
</p>
<p>pollution. Therefore, it seems reasonable to use X3 as an explanatory variable for
</p>
<p>the prediction of X14 in a linear-regression analysis.
</p>
<p>As far as the distribution of X3 is concerned, it can be said that the KDE of X3
clearly has two peaks, which indicates that there are two sub-groups. According to
</p>
<p>the negative relation between X3 and X14 it could be the case that one sub-group
</p>
<p>corresponds to the more expensive houses and the other one to the cheaper houses.
</p>
<p>Charles River Dummy Variable X4
</p>
<p>The observation made from the PCP that there are more expensive houses than
</p>
<p>cheap houses situated on the banks of the Charles River is confirmed by inspecting
</p>
<p>the scatterplot matrix. Still, we might have some doubt that proximity to the river
</p>
<p>influences house prices. Looking at the original data set, it becomes clear that the</p>
<p/>
</div>
<div class="page"><p/>
<p>44 1 Comparison of Batches
</p>
<p>observations for which X4 equals one are districts that are close to each other.
</p>
<p>Apparently, the Charles River does not flow through very many different districts.
</p>
<p>Thus, it may be pure coincidence that the more expensive districts are close to the
</p>
<p>Charles River&mdash;their high values might be caused by many other factors such as the
</p>
<p>pupil/teacher ratio or the proportion of non-retail business acres.
</p>
<p>Nitric Oxides ConcentrationX5
</p>
<p>The scatterplot of X5 vs. X14 and the separate boxplots of X5 for more and less
</p>
<p>expensive houses reveal a clear negative relation between the two variables. As it
</p>
<p>was the main aim of the authors of the original study to determine whether pollution
</p>
<p>had an influence on housing prices, it should be considered very carefully whether
</p>
<p>X5 can serve as an explanatory variable for price X14. A possible reason against it
</p>
<p>being an explanatory variable is that people might not like to live in areas where the
</p>
<p>emissions of nitric oxides are high. Nitric oxides are emitted mainly by automobiles,
</p>
<p>by factories and from heating private homes. However, as one can imagine there are
</p>
<p>many good reasons besides nitric oxides not to live in urban or industrial areas.
</p>
<p>Noise pollution, for example, might be a much better explanatory variable for the
</p>
<p>price of housing units. As the emission of nitric oxides is usually accompanied by
</p>
<p>noise pollution, using X5 as an explanatory variable for X14 might lead to the false
</p>
<p>conclusion that people run away from nitric oxides, whereas in reality it is noise
</p>
<p>pollution that they are trying to escape.
</p>
<p>Average Number of Rooms per Dwelling X6
</p>
<p>The number of rooms per dwelling is a possible measure of the size of the houses.
</p>
<p>Thus we expect X6 to be strongly correlated with X14 (the houses&rsquo; median price).
</p>
<p>Indeed&mdash;apart from some outliers&mdash;the scatterplot ofX6 vs.X14 shows a point cloud
</p>
<p>which is clearly upward-sloping and which seems to be a realisation of a linear
</p>
<p>dependence of X14 on X6. The two boxplots of X6 confirm this notion by showing
</p>
<p>that the quartiles, the mean and the median are all much higher for the red than for
</p>
<p>the black boxplot.
</p>
<p>Proportion of Owner-Occupied Units Built Prior to 1940X7
</p>
<p>There is no clear connection visible between X7 and X14. There could be a weak
</p>
<p>negative correlation between the two variables, since the (red) boxplot of X7 for the
</p>
<p>districts whose price is above the median price indicates a lower mean and median
</p>
<p>than the (black) boxplot for the district whose price is below the median price. The
</p>
<p>fact that the correlation is not so clear could be explained by two opposing effects.
</p>
<p>On the one hand, house prices should decrease if the older houses are not in a good
</p>
<p>shape. On the other hand, prices could increase, because people often like older</p>
<p/>
</div>
<div class="page"><p/>
<p>1.9 Boston Housing 45
</p>
<p>houses better than newer houses, preferring their atmosphere of space and tradition.
</p>
<p>Nevertheless, it seems reasonable that the age of the houses has an influence on their
</p>
<p>price X14.
</p>
<p>Raising X7 to the power of 2.5 reveals again that the data set might consist of
</p>
<p>two sub-groups. But in this case it is not obvious that the sub-groups correspond
</p>
<p>to more expensive or cheaper houses. One can furthermore observe a negative
</p>
<p>relation between X7 and X8. This could reflect the way the Boston metropolitan
</p>
<p>area developed over time; the districts with the newer buildings are further away
</p>
<p>from employment centers and industrial facilities.
</p>
<p>Weighted Distance to Five Boston Employment Centers X8
</p>
<p>Since most people like to live close to their place of work, we expect a negative
</p>
<p>relation between the distances to the employment centers and house prices. The
</p>
<p>scatterplot hardly reveals any dependence, but the boxplots ofX8 indicate that there
</p>
<p>might be a slightly positive relation as the red boxplot&rsquo;s median and mean are higher
</p>
<p>than the black ones. Again, there might be two effects in opposite directions at work
</p>
<p>here. The first is that living too close to an employment centre might not provide
</p>
<p>enough shelter from the pollution created there. The second, as mentioned above, is
</p>
<p>that people do not travel very far to their workplace.
</p>
<p>Index of Accessibility to Radial HighwaysX9
</p>
<p>The first obvious thing one can observe from the scatterplots, as well in the
</p>
<p>histograms and the KDEs, is that there are two sub-groups of districts containingX9
values which are close to the respective group&rsquo;s mean. The scatterplots deliver no
</p>
<p>hint as to what might explain the occurrence of these two sub-groups. The boxplots
</p>
<p>indicate that for the cheaper and for the more expensive houses the average of X9 is
</p>
<p>almost the same.
</p>
<p>Full-Value Property Tax X10
</p>
<p>X10 shows behaviour similar to that of X9: two sub-groups exist. A downward-
</p>
<p>sloping curve seems to underlie the relation of X10 and X14. This is confirmed by
</p>
<p>the two boxplots drawn for X10: the red one has a lower mean and median than the
</p>
<p>black one.
</p>
<p>Pupil/Teacher RatioX11
</p>
<p>The red and black boxplots ofX11 indicate a negative relation betweenX11 andX14.
</p>
<p>This is confirmed by inspection of the scatterplot of X11 vs. X14: The point cloud is</p>
<p/>
</div>
<div class="page"><p/>
<p>46 1 Comparison of Batches
</p>
<p>downward sloping, i.e. the less teachers there are per pupil, the less people pay on
</p>
<p>median for their dwellings.
</p>
<p>Proportion of African-AmericanB, X12 D 1000.B � 0:63/
2 I.B &lt; 0:63/
</p>
<p>Interestingly, X12 is negatively&mdash;though not linearly&mdash;correlated with X3, X7 and
</p>
<p>X11, whereas it is positively related withX14. Looking at the data set reveals that for
</p>
<p>almost all districts X12 takes on a value around 390. Since B cannot be larger than
</p>
<p>0.63, such values can only be caused by B close to zero. Therefore, the higher X12
is, the lower the actual proportion of African-Americans is. Among observations
</p>
<p>405&ndash;470 there are quite a few that have a X12 that is much lower than 390. This
</p>
<p>means that in these districts the proportion of African-Americans is above zero.
</p>
<p>We can observe two clusters of points in the scatterplots of log .X12/: one cluster
</p>
<p>for which X12 is close to 390 and a second one for which X12 is between 3 and
</p>
<p>100. When X12 is positively related with another variable, the actual proportion of
</p>
<p>African-Americans is negatively correlated with this variable and vice versa. This
</p>
<p>means that African-Americans live in areas where there is a high proportion of non-
</p>
<p>retail business land, where there are older houses and where there is a high (i.e. bad)
</p>
<p>pupil/teacher ratio. It can be observed that districts with housing prices above the
</p>
<p>median can only be found where the proportion of African-Americans is virtually
</p>
<p>zero.
</p>
<p>Proportion of Lower Status of the Population X13
</p>
<p>Of all the variablesX13 exhibits the clearest negative relation with X14&mdash;hardly any
</p>
<p>outliers show up. Taking the square root of X13 and the logarithm ofX14 transforms
</p>
<p>the relation into a linear one.
</p>
<p>Transformations
</p>
<p>Since most of the variables exhibit an asymmetry with a higher density on the left-
</p>
<p>hand side, the following transformations are proposed:
</p>
<p>fX1 D log .X1/
fX2 D X2=10
fX3 D log .X3/
fX4 none, since X4 is binary
fX5 D log .X5/</p>
<p/>
</div>
<div class="page"><p/>
<p>1.9 Boston Housing 47
</p>
<p>fX6 D log .X6/
fX7 D X72:5=10000
fX8 D log .X8/
fX9 D log .X9/
eX10 D log .X10/
eX11 D exp .0:4 �X11/=1000
eX12 D X12=100
eX13 D
</p>
<p>p
X13
</p>
<p>eX14 D log .X14/
</p>
<p>Taking the logarithm or raising the variables to the power of something smaller
</p>
<p>than one helps to reduce the asymmetry. This is due to the fact that lower values
</p>
<p>move further away from each other, whereas the distance between greater values is
</p>
<p>reduced by these transformations.
</p>
<p>Figure 1.37 displays boxplots for the original mean variance scaled variables as
</p>
<p>well as for the proposed transformed variables. The transformed variables&rsquo; boxplots
</p>
<p>are more symmetric and have less outliers than the original variables&rsquo; boxplots.
</p>
<p>Fig. 1.37 Boxplots for all of
the variables from the Boston
housing data before and after
the proposed transformations
MVAboxbhd
</p>
<p>Boston Housing Data
</p>
<p>Transformed Boston Housing Data</p>
<p/>
</div>
<div class="page"><p/>
<p>48 1 Comparison of Batches
</p>
<p>1.10 Exercises
</p>
<p>Exercise 1.1 Is the upper extreme always an outlier?
</p>
<p>Exercise 1.2 Is it possible for the mean or the median to lie outside of the fourths
</p>
<p>or even outside of the outside bars?
</p>
<p>Exercise 1.3 Assume that the data are normally distributedN.0; 1/. What percent-
</p>
<p>age of the data do you expect to lie outside the outside bars?
</p>
<p>Exercise 1.4 What percentage of the data do you expect to lie outside the outside
</p>
<p>bars if we assume that the data are normally distributed N.0; �2/ with unknown
</p>
<p>variance �2?
</p>
<p>Exercise 1.5 How would the five-number summary of the 15 largest US cities differ
</p>
<p>from that of the 50 largest US cities? How would the five-number summary of 15
</p>
<p>observations ofN.0; 1/-distributed data differ from that of 50 observations from the
</p>
<p>same distribution?
</p>
<p>Exercise 1.6 Is it possible that all five numbers of the five-number summary could
</p>
<p>be equal? If so, under what conditions?
</p>
<p>Exercise 1.7 Suppose we have 50 observations of X � N.0; 1/ and another 50
observations of Y � N.2; 1/. What would the 100 Flury faces look like if you had
defined as face elements the face line and the darkness of hair? Do you expect any
</p>
<p>similar faces? How many faces do you think should look like observations of Y even
</p>
<p>though they are X observations?
</p>
<p>Exercise 1.8 Draw a histogram for the mileage variable of the car data
</p>
<p>(Sect. 22.3). Do the same for the three groups (USA, Japan, and Europe). Do
</p>
<p>you obtain a similar conclusion as in the parallel boxplot in Fig. 1.3 for these data?
</p>
<p>Exercise 1.9 Use some bandwidth selection criterion to calculate the optimally
</p>
<p>chosen bandwidth h for the diagonal variable of the bank notes. Would it be better
</p>
<p>to have one bandwidth for the two groups?
</p>
<p>Exercise 1.10 In Fig. 1.9 the densities overlap in the region of diagonal� 140:4.
We partially observed this in the boxplot of Fig. 1.4. Our aim is to separate the two
</p>
<p>groups. Will we be able to do this effectively on the basis of this diagonal variable
</p>
<p>alone?
</p>
<p>Exercise 1.11 Draw a parallel coordinates plot for the car data.
</p>
<p>Exercise 1.12 How would you identify discrete variables (variables with only a
</p>
<p>limited number of possible outcomes) on a parallel coordinates plot?
</p>
<p>Exercise 1.13 True or false: the height of the bars of a histogram are equal to the
</p>
<p>relative frequency with which observations fall into the respective bins.
</p>
<p>Exercise 1.14 True or false: kernel density estimates must always take on a value
</p>
<p>between 0 and 1. (Hint: Which quantity connected with the density function has to</p>
<p/>
</div>
<div class="page"><p/>
<p>1.10 Exercises 49
</p>
<p>be equal to 1? Does this property imply that the density function has to always be
</p>
<p>less than 1?)
</p>
<p>Exercise 1.15 Let the following data set represent the heights of 13 students taking
</p>
<p>the Applied Multivariate Statistical Analysis course:
</p>
<p>1:72; 1:83; 1:74; 1:79; 1:94; 1:81; 1:66; 1:60; 1:78; 1:77; 1:85; 1:70; 1:76:
</p>
<p>1. Find the corresponding five-number summary.
</p>
<p>2. Construct the boxplot.
</p>
<p>3. Draw a histogram for this data set.
</p>
<p>Exercise 1.16 Describe the unemployment data (see Table 22.19) that contain
</p>
<p>unemployment rates of all German Federal States using various descriptive tech-
</p>
<p>niques.
</p>
<p>Exercise 1.17 Using yearly population data (see Sect. 22.20), generate
</p>
<p>1. a boxplot (choose one of variables)
</p>
<p>2. an Andrew&rsquo;s Curve (choose ten data points)
</p>
<p>3. a scatterplot
</p>
<p>4. a histogram (choose one of the variables)
</p>
<p>What do these graphs tell you about the data and their structure?
</p>
<p>Exercise 1.18 Make a draftman plot for the car data with the variables
</p>
<p>X1 D price;
X2 D mileage;
X8 D weight;
X9 D length:
</p>
<p>Move the brush into the region of heavy cars. What can you say about price, mileage
</p>
<p>and length? Move the brush onto high fuel economy. Mark the Japanese, European
</p>
<p>and American cars. You should find the same condition as in boxplot Fig. 1.3.
</p>
<p>Exercise 1.19 What is the form of a scatterplot of two independent random
</p>
<p>variables X1 and X2 with standard normal distribution?
</p>
<p>Exercise 1.20 Rotate a three-dimensional standard normal point cloud in 3D
</p>
<p>space. Does it &ldquo;almost look the same from all sides&rdquo;? Can you explain why or
</p>
<p>why not?
</p>
<p>Exercise 1.21 There are many reasons for using hexagons to visualise the structure
</p>
<p>of data.
</p>
<p>1. Hexagons have the property of &ldquo;symmetry of nearest neighbours&rdquo; which lacks in
</p>
<p>square bins.</p>
<p/>
</div>
<div class="page"><p/>
<p>50 1 Comparison of Batches
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>&minus;3 &minus;1 0 1 2 3
</p>
<p>&minus;
2
</p>
<p>0
1
</p>
<p>2
3
</p>
<p>X
</p>
<p>Y
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>&minus;3 &minus;1 0 1 2 3
</p>
<p>&minus;
2
</p>
<p>0
1
</p>
<p>2
3
</p>
<p>X
</p>
<p>Y
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>&minus;3 &minus;1 0 1 2 3
</p>
<p>&minus;
2
</p>
<p>0
1
</p>
<p>2
3
</p>
<p>X
</p>
<p>Y
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>&minus;3 &minus;1 0 1 2 3
</p>
<p>&minus;
2
</p>
<p>0
1
</p>
<p>2
3
</p>
<p>X
</p>
<p>Y
</p>
<p>Fig. 1.38 Hexagon binning algorithm MVAhexaAl
</p>
<p>2. Hexagons have the maximum number of sides that a polygon can have for a
</p>
<p>regular tessellation of the plane.
</p>
<p>3. Hexagons are visually less biased for displaying densities than other regular
</p>
<p>tessellations.
</p>
<p>The hexagon binning algorithm is as follows:
</p>
<p>1. Decrease y-axis variable by a factor of
p
3 (making the calculation more
</p>
<p>quickly)
</p>
<p>2. Create a dual lattice (circle and star lines in Fig. 1.38)
</p>
<p>3. Bin each point into a pair of near neighbour rectangles
</p>
<p>4. Choose the closest of the rectangle centers (adjusting for
p
3)
</p>
<p>The rectangles created from dual lattice have length hx (bin width of hexagons) and
</p>
<p>height hy D
p
3hx . From these rectangles we can get hexagons with bin width hx .
</p>
<p>The first point of the star lattice has coordinates x0 and y0. The other star points
</p>
<p>will have coordinates x0 C k1hx and y0 C l1hy , where k1; l1 D 1; 2; : : : The first
point of the circle lattice has coordinates x0C hx2 and y0C
</p>
<p>p
3hx
2
</p>
<p>. Other circle points
</p>
<p>are calculated like star points. Suppose an arbitrary point with coordinates x; y lies
</p>
<p>in the intersection of two near neighbour rectangles. What&rsquo;s the distance from this
</p>
<p>point to one of two corners?</p>
<p/>
</div>
<div class="page"><p/>
<p>Part II
</p>
<p>Multivariate Random Variables</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 2
</p>
<p>A Short Excursion into Matrix Algebra
</p>
<p>This chapter serves as a reminder of basic concepts of matrix algebra, which
</p>
<p>are particularly useful in multivariate analysis. It also introduces the notations
</p>
<p>used in this book for vectors and matrices. Eigenvalues and eigenvectors play an
</p>
<p>important role in multivariate techniques. In Sects. 2.2 and 2.3, we present the
</p>
<p>spectral decomposition of matrices and consider the maximisation (minimisation)
</p>
<p>of quadratic forms given some constraints.
</p>
<p>In analysing the multivariate normal distribution, partitioned matrices appear
</p>
<p>naturally. Some of the basic algebraic properties are given in Sect. 2.5. These
</p>
<p>properties will be heavily used in Chaps. 4 and 5.
</p>
<p>The geometry of the multinormal and the geometric interpretation of the
</p>
<p>multivariate techniques (Part III) intensively uses the notion of angles between two
</p>
<p>vectors, the projection of a point on a vector and the distances between two points.
</p>
<p>These ideas are introduced in Sect. 2.6.
</p>
<p>2.1 Elementary Operations
</p>
<p>A matrixA is a system of numbers with n rows and p columns:
</p>
<p>A D
</p>
<p>0
BBBBBBBBBB@
</p>
<p>a11 a12 : : : : : : : : : a1p
::: a22
</p>
<p>:::
:::
</p>
<p>:::
: : :
</p>
<p>:::
:::
</p>
<p>:::
: : :
</p>
<p>:::
:::
</p>
<p>:::
: : :
</p>
<p>:::
</p>
<p>an1 an2 : : : : : : : : : anp
</p>
<p>1
CCCCCCCCCCA
</p>
<p>:
</p>
<p>&copy; Springer-Verlag Berlin Heidelberg 2015
W.K. H&auml;rdle, L. Simar, Applied Multivariate Statistical Analysis,
DOI 10.1007/978-3-662-45171-7_2
</p>
<p>53</p>
<p/>
</div>
<div class="page"><p/>
<p>54 2 A Short Excursion into Matrix Algebra
</p>
<p>Table 2.1 Special matrices and vectors
</p>
<p>Name Definition Notation Example
</p>
<p>Scalar p D n D 1 a 3
</p>
<p>Column vector p D 1 a
 
1
</p>
<p>3
</p>
<p>!
</p>
<p>Row vector n D 1 a&gt;
�
1 3
</p>
<p>�
</p>
<p>Vector of ones .1; : : : ; 1&bdquo; ƒ&sbquo; &hellip;
n
</p>
<p>/&gt; 1n
</p>
<p> 
1
</p>
<p>1
</p>
<p>!
</p>
<p>Vector of zeros .0; : : : ; 0&bdquo; ƒ&sbquo; &hellip;
n
</p>
<p>/&gt; 0n
</p>
<p> 
0
</p>
<p>0
</p>
<p>!
</p>
<p>Square matrix n D p A.p � p/
 
2 0
</p>
<p>0 2
</p>
<p>!
</p>
<p>Diagonal matrix aij D 0, i 6D j , n D p diag.aii/
 
1 0
</p>
<p>0 2
</p>
<p>!
</p>
<p>Identity matrix diag.1; : : : ; 1&bdquo; ƒ&sbquo; &hellip;
p
</p>
<p>/ Ip
</p>
<p> 
1 0
</p>
<p>0 1
</p>
<p>!
</p>
<p>Unit matrix aij D 1, n D p 1n1&gt;n
</p>
<p> 
1 1
</p>
<p>1 1
</p>
<p>!
</p>
<p>Symmetric matrix aij D aji
 
1 2
</p>
<p>2 3
</p>
<p>!
</p>
<p>Null matrix aij D 0 0
 
0 0
</p>
<p>0 0
</p>
<p>!
</p>
<p>Upper triangular matrix aij D 0; i &lt; j
</p>
<p>0
B@
1 2 4
</p>
<p>0 1 3
</p>
<p>0 0 1
</p>
<p>1
CA
</p>
<p>Idempotent matrix AA D A
</p>
<p>0
B@
1 0 0
</p>
<p>0 1
2
1
2
</p>
<p>0 1
2
1
2
</p>
<p>1
CA
</p>
<p>Orthogonal matrix A&gt;A D I D AA&gt;
 
</p>
<p>1p
2
</p>
<p>1p
2
</p>
<p>1p
2
� 1p
</p>
<p>2
</p>
<p>!
</p>
<p>We also write .aij/ for A and A.n � p/ to indicate the numbers of rows and
columns. Vectors are matrices with one column and are denoted as x or x.p � 1/.
Special matrices and vectors are defined in Table 2.1. Note that we use small letters
</p>
<p>for scalars as well as for vectors.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.1 Elementary Operations 55
</p>
<p>Matrix Operations
</p>
<p>Elementary operations are summarised below:
</p>
<p>A&gt; D .aji/
AC B D .aij C bij/
A � B D .aij � bij/
c �A D .c � aij/
</p>
<p>A � B D A.n � p/ B.p �m/ D C.n �m/ D .cij/ D
</p>
<p>0
@
</p>
<p>pX
</p>
<p>jD1
aijbjk
</p>
<p>1
A :
</p>
<p>Properties of Matrix Operations
</p>
<p>AC B D B CA
A.B C C/ D AB CAC
</p>
<p>A.BC/ D .AB/C
.A&gt;/&gt; D A
.AB/&gt; D B&gt;A&gt;
</p>
<p>Matrix Characteristics
</p>
<p>Rank
</p>
<p>The rank, rank.A/, of a matrix A.n � p/ is defined as the maximum number of
linearly independent rows (columns). A set of k rows aj of A.n � p/ are said to
be linearly independent if
</p>
<p>Pk
jD1 cj aj D 0p implies cj D 0;8j , where c1; : : : ; ck
</p>
<p>are scalars. In other words no rows in this set can be expressed as a nontrivial linear
</p>
<p>combination of the .k � 1/ remaining rows.</p>
<p/>
</div>
<div class="page"><p/>
<p>56 2 A Short Excursion into Matrix Algebra
</p>
<p>Trace
</p>
<p>The trace of a matrix A.p � p/ is the sum of its diagonal elements
</p>
<p>tr.A/ D
pX
</p>
<p>iD1
aii:
</p>
<p>Determinant
</p>
<p>The determinant is an important concept of matrix algebra. For a square matrix A,
</p>
<p>it is defined as:
</p>
<p>det.A/ D jAj D
X
</p>
<p>.�1/j� j a1�.1/ : : : ap�.p/;
</p>
<p>the summation is over all permutations � of f1; 2; : : : ; pg, and j� j D 0 if the
permutation can be written as a product of an even number of transpositions and
</p>
<p>j� j D 1 otherwise. Some properties of determinant of a matrix are:
</p>
<p>jA&gt;j D jAj
jABj D jAj � jAj
jcAj D cnjAj:
</p>
<p>Example 2.1 In the case of p D 2, A D
�
a11 a12
a21 a22
</p>
<p>�
and we can permute the digits
</p>
<p>&ldquo;1&rdquo; and &ldquo;2&rdquo; once or not at all. So,
</p>
<p>jAj D a11 a22 � a12 a21:
</p>
<p>Transpose
</p>
<p>For A.n � p/ and B.p � n/
</p>
<p>.A&gt;/&gt; D A; and .AB/&gt; D B&gt;A&gt;:
</p>
<p>Inverse
</p>
<p>If jAj 6D 0 and A.p � p/, then the inverseA�1 exists:
</p>
<p>A A�1 D A�1 A D Ip:</p>
<p/>
</div>
<div class="page"><p/>
<p>2.1 Elementary Operations 57
</p>
<p>For small matrices, the inverse of A D .aij/ can be calculated as
</p>
<p>A�1 D CjAj ;
</p>
<p>where C D .cij/ is the adjoint matrix ofA. The elements cji of C&gt; are the co-factors
of A:
</p>
<p>cji D .�1/iCj
</p>
<p>ˇ̌
ˇ̌
ˇ̌
ˇ̌
ˇ̌
ˇ̌
ˇ̌
ˇ
</p>
<p>a11 : : : a1.j�1/ a1.jC1/ : : : a1p
:::
</p>
<p>a.i�1/1 : : : a.i�1/.j�1/ a.i�1/.jC1/ : : : a.i�1/p
a.iC1/1 : : : a.iC1/.j�1/ a.iC1/.jC1/ : : : a.iC1/p
:::
</p>
<p>ap1 : : : ap.j�1/ ap.jC1/ : : : app
</p>
<p>ˇ̌
ˇ̌
ˇ̌
ˇ̌
ˇ̌
ˇ̌
ˇ̌
ˇ
</p>
<p>:
</p>
<p>The relationship between determinant and inverse of matrixA is jA�1j D jAj�1.
</p>
<p>G-Inverse
</p>
<p>A more general concept is the G-inverse (Generalised Inverse) A� which satisfies
the following:
</p>
<p>A A�A D A:
</p>
<p>Later we will see that there may be more than one G-inverse.
</p>
<p>Example 2.2 The generalised inverse can also be calculated for singular matrices.
</p>
<p>We have:
</p>
<p>�
1 0
</p>
<p>0 0
</p>
<p>��
1 0
</p>
<p>0 0
</p>
<p>��
1 0
</p>
<p>0 0
</p>
<p>�
D
�
1 0
</p>
<p>0 0
</p>
<p>�
;
</p>
<p>which means that the generalised inverse of A D
�
1 0
</p>
<p>0 0
</p>
<p>�
is A� D
</p>
<p>�
1 0
</p>
<p>0 0
</p>
<p>�
even
</p>
<p>though the inverse matrix of A does not exist in this case.
</p>
<p>Eigenvalues, Eigenvectors
</p>
<p>Consider a (p � p) matrixA. If there a scalar � and a vector &#13; exists such as
</p>
<p>A&#13; D �&#13;; (2.1)</p>
<p/>
</div>
<div class="page"><p/>
<p>58 2 A Short Excursion into Matrix Algebra
</p>
<p>then we call
</p>
<p>� an eigenvalue
</p>
<p>&#13; an eigenvector.
</p>
<p>It can be proven that an eigenvalue � is a root of the p-th order polynomial jA �
�Ipj D 0. Therefore, there are up to p eigenvalues �1; �2; : : : ; �p of A. For each
eigenvalue �j , a corresponding eigenvector &#13;j exists given by Eq. (2.1) . Suppose
</p>
<p>the matrix A has the eigenvalues �1; : : : ; �p . Let ƒ D diag.�1; : : : ; �p).
The determinant jAj and the trace tr.A/ can be rewritten in terms of the
</p>
<p>eigenvalues:
</p>
<p>jAj D jƒj D
pY
</p>
<p>jD1
�j (2.2)
</p>
<p>tr.A/ D tr.ƒ/ D
pX
</p>
<p>jD1
�j : (2.3)
</p>
<p>An idempotent matrixA (see the definition in Table 2.1) can only have eigenvalues
</p>
<p>in f0; 1g therefore tr.A/ D rank.A/ D number of eigenvalues&curren; 0.
</p>
<p>Example 2.3 Let us consider the matrix A D
</p>
<p>0
@
1 0 0
</p>
<p>0 1
2
1
2
</p>
<p>0 1
2
1
2
</p>
<p>1
A. It is easy to verify that
</p>
<p>AA D A which implies that the matrix A is idempotent.
We know that the eigenvalues of an idempotent matrix are equal to 0 or 1. In this
</p>
<p>case, the eigenvalues ofA are �1 D 1, �2 D 1, and �3 D 0 since
</p>
<p>0
@
1 0 0
</p>
<p>0 1
2
1
2
</p>
<p>0 1
2
1
2
</p>
<p>1
A
0
@
1
</p>
<p>0
</p>
<p>0
</p>
<p>1
A D
</p>
<p>1
</p>
<p>0
@
1
</p>
<p>0
</p>
<p>0
</p>
<p>1
A,
</p>
<p>0
@
1 0 0
</p>
<p>0 1
2
1
2
</p>
<p>0 1
2
1
2
</p>
<p>1
A
</p>
<p>0
B@
</p>
<p>0p
2
2p
2
2
</p>
<p>1
CA D 1
</p>
<p>0
B@
</p>
<p>0p
2
2p
2
2
</p>
<p>1
CA, and
</p>
<p>0
@
1 0 0
</p>
<p>0 1
2
1
2
</p>
<p>0 1
2
1
2
</p>
<p>1
A
</p>
<p>0
B@
</p>
<p>0p
2
2
</p>
<p>�
p
2
2
</p>
<p>1
CA D 0
</p>
<p>0
B@
</p>
<p>0p
2
2
</p>
<p>�
p
2
2
</p>
<p>1
CA.
</p>
<p>Using formulas (2.2) and (2.3), we can calculate the trace and the determinant
</p>
<p>of A from the eigenvalues: tr.A/ D �1 C �2 C �3 D 2, jAj D �1�2�3 D 0, and
rank.A/ D 2.
</p>
<p>Properties of Matrix Characteristics
</p>
<p>A.n � n/; B.n � n/; c 2 R
</p>
<p>tr.AC B/ D trAC trB (2.4)
tr.cA/ D c trA (2.5)</p>
<p/>
</div>
<div class="page"><p/>
<p>2.1 Elementary Operations 59
</p>
<p>jcAj D cnjAj (2.6)
jABj D jBAj D jAjjBj (2.7)
</p>
<p>A.n � p/; B.p � n/
</p>
<p>tr.A�B/ D tr.B�A/ (2.8)
rank.A/ � min.n; p/
rank.A/ � 0 (2.9)
rank.A/ D rank.A&gt;/ (2.10)
</p>
<p>rank.A&gt;A/ D rank.A/ (2.11)
rank.AC B/ � rank.A/C rank.B/ (2.12)
</p>
<p>rank.AB/ � minfrank.A/; rank.B/g (2.13)
</p>
<p>A.n � p/; B.p � q/; C.q � n/
</p>
<p>tr.ABC/ D tr.BCA/
D tr.CAB/ (2.14)
</p>
<p>rank.ABC/ D rank.B/ for nonsingularA; C (2.15)
</p>
<p>A.p � p/
</p>
<p>jA�1j D jAj�1 (2.16)
rank.A/ D p if and only if A is nonsingular. (2.17)
</p>
<p>Summary
</p>
<p>,! The determinant jAj is the product of the eigenvalues of A.
</p>
<p>,! The inverse of a matrixA exists if jAj &curren; 0.
</p>
<p>,! The trace tr.A/ is the sum of the eigenvalues of A.
</p>
<p>,! The sum of the traces of two matrices equals the trace of the sum
of the two matrices.
</p>
<p>,! The trace tr.AB/ equals tr.BA/.
</p>
<p>,! The rank.A/ is the maximal number of linearly independent rows
(columns) of A.</p>
<p/>
</div>
<div class="page"><p/>
<p>60 2 A Short Excursion into Matrix Algebra
</p>
<p>2.2 Spectral Decompositions
</p>
<p>The computation of eigenvalues and eigenvectors is an important issue in the
</p>
<p>analysis of matrices. The spectral decomposition or Jordan decomposition links the
</p>
<p>structure of a matrix to the eigenvalues and the eigenvectors.
</p>
<p>Theorem 2.1 (Jordan Decomposition) Each symmetric matrix A.p � p/ can be
written as
</p>
<p>A D &#128; ƒ &#128;&gt; D
pX
</p>
<p>jD1
�j&#13;j &#13;
</p>
<p>&gt;
j
</p>
<p>(2.18)
</p>
<p>where
</p>
<p>ƒ D diag.�1; : : : ; �p/
</p>
<p>and where
</p>
<p>&#128; D .&#13;1 ; &#13;2 ; : : : ; &#13;p /
</p>
<p>is an orthogonal matrix consisting of the eigenvectors &#13;j of A.
</p>
<p>Example 2.4 Suppose that A D
�
1
2
2
3
</p>
<p>�
. The eigenvalues are found by solving jA�
</p>
<p>�Ij D 0. This is equivalent to
ˇ̌
ˇ̌ 1 � � 2
2 3 � �
</p>
<p>ˇ̌
ˇ̌ D .1 � �/.3 � �/ � 4 D 0:
</p>
<p>Hence, the eigenvalues are �1 D 2C
p
5 and �2 D 2 �
</p>
<p>p
5. The eigenvectors are
</p>
<p>&#13;1 D .0:5257; 0:8506/&gt; and &#13;2 D .0:8506;�0:5257/&gt;. They are orthogonal since
&#13;&gt;1 &#13;2 D 0.
</p>
<p>Using spectral decomposition, we can define powers of a matrix A.p � p/.
SupposeA is a symmetric matrix with positive eigenvalues. Then by Theorem 2.1
</p>
<p>A D &#128;ƒ&#128;&gt;;
</p>
<p>and we define for some ˛ 2 R
</p>
<p>A˛ D &#128;ƒ˛&#128;&gt;; (2.19)</p>
<p/>
</div>
<div class="page"><p/>
<p>2.2 Spectral Decompositions 61
</p>
<p>where ƒ˛ D diag.�˛1 ; : : : ; �˛p/. In particular, we can easily calculate the inverse of
the matrix A. Suppose that the eigenvalues of A are positive. Then with ˛ D �1,
we obtain the inverse of A from
</p>
<p>A�1 D &#128;ƒ�1&#128;&gt;: (2.20)
</p>
<p>Another interesting decomposition which is later used is given in the following
</p>
<p>theorem.
</p>
<p>Theorem 2.2 (Singular Value Decomposition) Each matrixA.n�p/ with rank r
can be decomposed as
</p>
<p>A D &#128; ƒ &#129;&gt;;
</p>
<p>where &#128;.n� r/ and&#129;.p � r/. Both &#128; and&#129; are column orthonormal, i.e. &#128;&gt;&#128; D
&#129;&gt;&#129; D Ir and ƒ D diag
</p>
<p>�
�
1=2
1 ; : : : ; �
</p>
<p>1=2
r
</p>
<p>�
, �j &gt; 0. The values �1; : : : ; �r are
</p>
<p>the nonzero eigenvalues of the matrices AA&gt; and A&gt;A. &#128; and &#129; consist of the
corresponding r eigenvectors of these matrices.
</p>
<p>This is obviously a generalisation of Theorem 2.1 (Jordan decomposition). With
</p>
<p>Theorem 2.2, we can find a G-inverse A� of A. Indeed, define A� D &#129; ƒ�1 &#128;&gt;.
ThenA A� A D &#128; ƒ &#129;&gt; D A. Note that the G-inverse is not unique.
Example 2.5 In Example 2.2, we showed that the generalised inverse of A D�
1 0
</p>
<p>0 0
</p>
<p>�
is A�
</p>
<p>�
1 0
</p>
<p>0 0
</p>
<p>�
. The following also holds
</p>
<p>�
1 0
</p>
<p>0 0
</p>
<p>��
1 0
</p>
<p>0 8
</p>
<p>��
1 0
</p>
<p>0 0
</p>
<p>�
D
�
1 0
</p>
<p>0 0
</p>
<p>�
</p>
<p>which means that the matrix
</p>
<p>�
1 0
</p>
<p>0 8
</p>
<p>�
is also a generalised inverse of A.
</p>
<p>Summary
</p>
<p>,! The Jordan decomposition gives a representation of a symmetric
matrix in terms of eigenvalues and eigenvectors.
</p>
<p>,! The eigenvectors belonging to the largest eigenvalues indicate the
&ldquo;main direction&rdquo; of the data.</p>
<p/>
</div>
<div class="page"><p/>
<p>62 2 A Short Excursion into Matrix Algebra
</p>
<p>Summary (continued)
</p>
<p>,! The Jordan decomposition allows one to easily compute the power
of a symmetric matrix A: A˛ D &#128;ƒ˛&#128;&gt;.
</p>
<p>,! The singular value decomposition (SVD) is a generalisation of the
Jordan decomposition to non-quadratic matrices.
</p>
<p>2.3 Quadratic Forms
</p>
<p>A quadratic form Q.x/ is built from a symmetric matrix A.p � p/ and a vector
x 2 Rp:
</p>
<p>Q.x/ D x&gt; A x D
pX
</p>
<p>iD1
</p>
<p>pX
</p>
<p>jD1
aijxixj : (2.21)
</p>
<p>Definiteness of Quadratic Forms and Matrices
</p>
<p>Q.x/ &gt; 0 for all x 6D 0 positive definite
Q.x/ � 0 for all x 6D 0 positive semidefinite
</p>
<p>A matrix A is called positive definite (semidefinite) if the corresponding quadratic
</p>
<p>formQ.:/ is positive definite (semidefinite). We write A &gt; 0 .� 0/.
Quadratic forms can always be diagonalised, as the following result shows.
</p>
<p>Theorem 2.3 IfA is symmetric andQ.x/ D x&gt;Ax is the corresponding quadratic
form, then there exists a transformation x 7! &#128;&gt;x D y such that
</p>
<p>x&gt; A x D
pX
</p>
<p>iD1
�iy
</p>
<p>2
i ;
</p>
<p>where �i are the eigenvalues ofA.
</p>
<p>Proof A D &#128; ƒ &#128;&gt;. By Theorem 2.1 and y D &#128;&gt;˛ we have that x&gt;Ax D
x&gt;&#128;ƒ&#128;&gt;x D y&gt;ƒy D
</p>
<p>Pp
iD1 �iy
</p>
<p>2
i : ut
</p>
<p>Positive definiteness of quadratic forms can be deduced from positive eigenval-
</p>
<p>ues.
</p>
<p>Theorem 2.4 A &gt; 0 if and only if all �i &gt; 0, i D 1; : : : ; p.
Proof 0 &lt; �1y
</p>
<p>2
1 C � � � C �py2p D x&gt;Ax for all x &curren; 0 by Theorem 2.3. ut</p>
<p/>
</div>
<div class="page"><p/>
<p>2.3 Quadratic Forms 63
</p>
<p>Corollary 2.1 If A &gt; 0, then A�1 exists and jAj &gt; 0.
Example 2.6 The quadratic formQ.x/ D x21 C x22 corresponds to the matrix A D�
1
</p>
<p>0
</p>
<p>0
</p>
<p>1
</p>
<p>�
with eigenvalues �1 D �2 D 1 and is thus positive definite. The quadratic
</p>
<p>formQ.x/ D .x1 � x2/2 corresponds to the matrix A D
�
</p>
<p>1
�1
�1
1
</p>
<p>�
with eigenvalues
</p>
<p>�1 D 2; �2 D 0 and is positive semidefinite. The quadratic formQ.x/ D x21 � x22
with eigenvalues �1 D 1; �2 D �1 is indefinite.
</p>
<p>In the statistical analysis of multivariate data, we are interested in maximising
</p>
<p>quadratic forms given some constraints.
</p>
<p>Theorem 2.5 If A and B are symmetric and B &gt; 0, then the maximum of x
&gt;Ax
x&gt;Bx
</p>
<p>is
</p>
<p>given by the largest eigenvalue of B�1A. More generally,
</p>
<p>max
x
</p>
<p>x&gt;Ax
</p>
<p>x&gt;Bx
D �1 � �2 � � � � � �p D min
</p>
<p>x
</p>
<p>x&gt;Ax
</p>
<p>x&gt;Bx
;
</p>
<p>where �1; : : : ; �p denote the eigenvalues of B
�1A. The vector which maximises
</p>
<p>(minimises) x
&gt;Ax
x&gt;Bx
</p>
<p>is the eigenvector of B�1A which corresponds to the largest
</p>
<p>(smallest) eigenvalue of B�1A. If x&gt;Bx D 1, we get
</p>
<p>max
x
x&gt;Ax D �1 � �2 � � � � � �p D min
</p>
<p>x
x&gt;Ax
</p>
<p>Proof Denote norm of vector x as kxk D
p
x&gt;x. By definition, B1=2 D
</p>
<p>&#128;B ƒ
1=2
</p>
<p>B &#128;
&gt;
B is symmetric. Then x
</p>
<p>&gt;Bx D
&#13;&#13;x&gt;B1=2
</p>
<p>&#13;&#13;2 D
&#13;&#13;B1=2x
</p>
<p>&#13;&#13;2. Set y D
B1=2x
</p>
<p>kB1=2xk , then
</p>
<p>max
x
</p>
<p>x&gt;Ax
</p>
<p>x&gt;Bx
D max
fyWy&gt;yD1g
</p>
<p>y&gt;B�1=2 AB�1=2y: (2.22)
</p>
<p>From Theorem 2.1, let
</p>
<p>B�1=2 A B�1=2 D &#128; ƒ &#128;&gt;
</p>
<p>be the spectral decomposition of B�1=2 A B�1=2. Set
</p>
<p>z D &#128;&gt;y; then z&gt;z D y&gt;&#128; &#128;&gt; y D y&gt;y:
</p>
<p>Thus (2.22) is equivalent to
</p>
<p>max
fzWz&gt;zD1g
</p>
<p>z&gt; ƒ z D max
fzWz&gt;zD1g
</p>
<p>pX
</p>
<p>iD1
�iz
</p>
<p>2
i :</p>
<p/>
</div>
<div class="page"><p/>
<p>64 2 A Short Excursion into Matrix Algebra
</p>
<p>But
</p>
<p>max
z
</p>
<p>X
�i z
</p>
<p>2
i � �1max
</p>
<p>z
</p>
<p>X
z2i
</p>
<p>&bdquo; ƒ&sbquo; &hellip;
D1
</p>
<p>D �1:
</p>
<p>The maximum is thus obtained by z D .1; 0; : : : ; 0/&gt;, i.e.
</p>
<p>y D &#13;1 ; hencex D B�1=2&#13;1 :
</p>
<p>Since B�1A and B�1=2 A B�1=2 have the same eigenvalues, the proof is complete.
To maximise (minimise) x&gt;Ax under x&gt;Bx D 1, below is another proof using
</p>
<p>the Lagrange method.
</p>
<p>max
x
x&gt;Ax D max
</p>
<p>x
Œx&gt;Ax � �
</p>
<p>�
x&gt;Bx � 1
</p>
<p>�
&#141;:
</p>
<p>The first derivative of it in respect to x is equal to 0:
</p>
<p>2Ax � 2�Bx D 0:
</p>
<p>so
</p>
<p>B�1Ax D �x
</p>
<p>By the definition of eigenvector and eigenvalue, our maximiser x� is B�1A&rsquo;s
eigenvector corresponding to eigenvalue �. So
</p>
<p>max
fxWx&gt;BxD1g
</p>
<p>x&gt;Ax D max
fxWx&gt;BxD1g
</p>
<p>x&gt;BB�1Ax D max
fxWx&gt;BxD1g
</p>
<p>x&gt;B�x D max�
</p>
<p>which is just the maximum eigenvalue of B�1A, and we choose the corresponding
eigenvector as our maximiser x�. ut
</p>
<p>Example 2.7 Consider the matricesA D
�
1 2
</p>
<p>2 3
</p>
<p>�
and B D
</p>
<p>�
1 0
</p>
<p>0 1
</p>
<p>�
,
</p>
<p>we calculate B�1A D
�
1 2
</p>
<p>2 3
</p>
<p>�
: The biggest eigenvalue of the matrix B�1A is 2C
</p>
<p>p
5. This means that the maximum of x&gt;Ax under the constraint x&gt;Bx D 1 is
</p>
<p>2C
p
5. Notice that the constraint x&gt;Bx D 1 corresponds to our choice of B, to the
</p>
<p>points which lie on the unit circle x21 C x22 D 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.4 Derivatives 65
</p>
<p>Summary
</p>
<p>,! A quadratic form can be described by a symmetric matrixA.
</p>
<p>,! Quadratic forms can always be diagonalised.
</p>
<p>,! Positive definiteness of a quadratic form is equivalent to positive-
ness of the eigenvalues of the matrixA.
</p>
<p>,! The maximum and minimum of a quadratic form given some
constraints can be expressed in terms of eigenvalues.
</p>
<p>2.4 Derivatives
</p>
<p>For later sections of this book, it will be useful to introduce matrix notation for
</p>
<p>derivatives of a scalar function of a vector x, i.e. f .x/, with respect to x. Consider
</p>
<p>f W Rp ! R and a .p � 1/ vector x, then @f .x/
@x
</p>
<p>is the column vector of partial
</p>
<p>derivatives
n
@f .x/
</p>
<p>@xj
</p>
<p>o
; j D 1; : : : ; p and @f .x/
</p>
<p>@x&gt;
is the row vector of the same derivative
</p>
<p>�
@f .x/
</p>
<p>@x
is called the gradient of f
</p>
<p>�
.
</p>
<p>We can also introduce second order derivatives: @
2f .x/
</p>
<p>@x@x&gt;
is the .p � p/ matrix of
</p>
<p>elements @
2f .x/
</p>
<p>@xi @xj
; i D 1; : : : ; p and j D 1; : : : ; p
</p>
<p>�
@2f .x/
</p>
<p>@x@x&gt;
is called the Hessian of f
</p>
<p>�
.
</p>
<p>Suppose that a is a .p � 1/ vector and thatA D A&gt; is a .p � p/ matrix. Then
</p>
<p>@a&gt;x
</p>
<p>@x
D @x
</p>
<p>&gt;a
</p>
<p>@x
D a; (2.23)
</p>
<p>@x&gt;Ax
</p>
<p>@x
D 2Ax: (2.24)
</p>
<p>The Hessian of the quadratic formQ.x/ D x&gt;Ax is:
</p>
<p>@2x&gt;Ax
</p>
<p>@x@x&gt;
D 2A: (2.25)
</p>
<p>Example 2.8 Consider the matrix
</p>
<p>A D
�
1 2
</p>
<p>2 3
</p>
<p>�
:</p>
<p/>
</div>
<div class="page"><p/>
<p>66 2 A Short Excursion into Matrix Algebra
</p>
<p>From formulas (2.24) and (2.25) it immediately follows that the gradient ofQ.x/ D
x&gt;Ax is
</p>
<p>@x&gt;Ax
</p>
<p>@x
D 2Ax D 2
</p>
<p>�
1 2
</p>
<p>2 3
</p>
<p>�
x D
</p>
<p>�
2x 4x
</p>
<p>4x 6x
</p>
<p>�
</p>
<p>and the Hessian is
</p>
<p>@2x&gt;Ax
</p>
<p>@x@x&gt;
D 2A D 2
</p>
<p>�
1 2
</p>
<p>2 3
</p>
<p>�
D
�
2 4
</p>
<p>4 6
</p>
<p>�
:
</p>
<p>2.5 Partitioned Matrices
</p>
<p>Very often we will have to consider certain groups of rows and columns of a matrix
</p>
<p>A.n � p/. In the case of two groups, we have
</p>
<p>A D
�
A11 A12
</p>
<p>A21 A22
</p>
<p>�
;
</p>
<p>whereAij.ni � pj /; i; j D 1; 2; n1 C n2 D n and p1 C p2 D p.
If B.n � p/ is partitioned accordingly, we have:
</p>
<p>AC B D
�
A11 C B11 A12 C B12
A21 C B21 A22 C B22
</p>
<p>�
</p>
<p>B&gt; D
�
B&gt;11 B
</p>
<p>&gt;
21
</p>
<p>B&gt;12 B
&gt;
22
</p>
<p>�
</p>
<p>AB&gt; D
�
A11B
</p>
<p>&gt;
11 CA12B&gt;12 A11B&gt;21 CA12B&gt;22
</p>
<p>A21B
&gt;
11 CA22B&gt;12 A21B&gt;21 CA22B&gt;22
</p>
<p>�
:
</p>
<p>An important particular case is the square matrix A.p � p/, partitioned in such a
way that A11 and A22 are both square matrices (i.e. nj D pj ; j D 1; 2). It can be
verified that when A is non-singular (AA�1 D Ip):
</p>
<p>A�1 D
�
A11 A12
</p>
<p>A21 A22
</p>
<p>�
(2.26)
</p>
<p>where
</p>
<p>8
ˆ̂̂
&lt;
ˆ̂̂
:
</p>
<p>A11 D .A11 �A12A�122 A21/�1
defD .A11�2/�1
</p>
<p>A12 D �.A11�2/�1A12A�122
A21 D �A�122 A21.A11�2/�1
A22 D A�122 CA�122 A21.A11�2/�1A12A�122 :</p>
<p/>
</div>
<div class="page"><p/>
<p>2.5 Partitioned Matrices 67
</p>
<p>An alternative expression can be obtained by reversing the positions ofA11 andA22
in the original matrix.
</p>
<p>The following results will be useful if A11 is non-singular:
</p>
<p>jAj D jA11jjA22 �A21A�111 A12j D jA11jjA22�1j: (2.27)
</p>
<p>If A22 is non-singular, we have that:
</p>
<p>jAj D jA22jjA11 �A12A�122 A21j D jA22jjA11�2j: (2.28)
</p>
<p>A useful formula is derived from the alternative expressions for the inverse and
</p>
<p>the determinant. For instance let
</p>
<p>B D
�
1 b&gt;
</p>
<p>a A
</p>
<p>�
</p>
<p>where a and b are .p � 1/ vectors and A is non-singular. We then have:
</p>
<p>jBj D jA� ab&gt;j D jAjj1� b&gt;A�1aj (2.29)
</p>
<p>and equating the two expressions for B22, we obtain the following:
</p>
<p>.A � ab&gt;/�1 D A�1 C A
�1ab&gt;A�1
</p>
<p>1 � b&gt;A�1a : (2.30)
</p>
<p>Example 2.9 Let&rsquo;s consider the matrix
</p>
<p>A D
�
1 2
</p>
<p>2 2
</p>
<p>�
:
</p>
<p>We can use formula (2.26) to calculate the inverse of a partitionedmatrix, i.e.A11 D
�1;A12 D A21 D 1;A22 D �1=2. The inverse of A is
</p>
<p>A�1 D
�
�1 1
1 �0:5
</p>
<p>�
:
</p>
<p>It is also easy to calculate the determinant of A:
</p>
<p>jAj D j1jj2� 4j D �2:
</p>
<p>Let A.n � p/ and B.p � n/ be any two matrices and suppose that n � p.
From (2.27) and (2.28) we can conclude that
</p>
<p>ˇ̌
ˇ̌��In �A
</p>
<p>B Ip
</p>
<p>ˇ̌
ˇ̌ D .��/n�pjBA� �Ipj D jAB � �Inj: (2.31)</p>
<p/>
</div>
<div class="page"><p/>
<p>68 2 A Short Excursion into Matrix Algebra
</p>
<p>Since both determinants on the right-hand side of (2.31) are polynomials in �, we
</p>
<p>find that the n eigenvalues ofAB yield the p eigenvalues of BA plus the eigenvalue
</p>
<p>0, n � p times.
The relationship between the eigenvectors is described in the next theorem.
</p>
<p>Theorem 2.6 ForA.n�p/ and B.p�n/, the nonzero eigenvalues ofAB and BA
are the same and have the same multiplicity. If x is an eigenvector of AB for an
</p>
<p>eigenvalue � &curren; 0, then y D Bx is an eigenvector of BA.
Corollary 2.2 ForA.n � p/, B.q � n/, a.p � 1/, and b.q � 1/ we have
</p>
<p>rank.Aab&gt;B/ � 1:
</p>
<p>The nonzero eigenvalue, if it exists, equals b&gt;BAa (with eigenvectorAa).
</p>
<p>Proof Theorem 2.6 asserts that the eigenvalues of Aab&gt;B are the same as those of
b&gt;BAa. Note that the matrix b&gt;BAa is a scalar and hence it is its own eigenvalue
�1.
</p>
<p>ApplyingAab&gt;B to Aa yields
</p>
<p>.Aab&gt;B/.Aa/ D .Aa/.b&gt;BAa/ D �1Aa:
</p>
<p>ut
</p>
<p>2.6 Geometrical Aspects
</p>
<p>Distance
</p>
<p>Let x; y 2 Rp . A distance d is defined as a function
</p>
<p>d W R2p ! RC which fulfills
</p>
<p>8
&lt;
:
</p>
<p>d.x; y/ &gt; 0 8x &curren; y
d.x; y/ D 0 if and only if x D y
d.x; y/ � d.x; z/C d.z; y/ 8x; y; z
</p>
<p>:
</p>
<p>A Euclidean distance d between two points x and y is defined as
</p>
<p>d 2.x; y/ D .x � y/TA.x � y/ (2.32)
</p>
<p>whereA is a positive definite matrix .A &gt; 0/. A is called a metric.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.6 Geometrical Aspects 69
</p>
<p>Fig. 2.1 Distance d
</p>
<p>Fig. 2.2 Iso-distance sphere
</p>
<p>Fig. 2.3 Iso-distance
ellipsoid
</p>
<p>Example 2.10 A particular case is whenA D Ip , i.e.
</p>
<p>d 2.x; y/ D
pX
</p>
<p>iD1
.xi � yi /2: (2.33)
</p>
<p>Figure 2.1 illustrates this definition for p D 2.
Note that the sets Ed D fx 2 Rp j .x � x0/&gt;.x � x0/ D d 2g , i.e. the spheres
</p>
<p>with radius d and centre x0, are the Euclidean Ip iso-distance curves from the point
</p>
<p>x0 (see Fig. 2.2).
</p>
<p>The more general distance (2.32) with a positive definite matrixA .A &gt; 0/ leads
</p>
<p>to the iso-distance curves
</p>
<p>Ed D fx 2 Rp j .x � x0/&gt;A.x � x0/ D d 2g; (2.34)
</p>
<p>i.e. ellipsoids with centre x0, matrixA and constant d (see Fig. 2.3).</p>
<p/>
</div>
<div class="page"><p/>
<p>70 2 A Short Excursion into Matrix Algebra
</p>
<p>Let &#13;1; &#13;2; : : : ; &#13;p be the orthonormal eigenvectors of A corresponding to the
</p>
<p>eigenvalues �1 � �2 � � � � � �p . The resulting observations are given in the next
theorem.
</p>
<p>Theorem 2.7 (i) The principal axes of Ed are in the direction of &#13;i I i D
1; : : : ; p.
</p>
<p>(ii) The half-lengths of the axes are
</p>
<p>q
d 2
</p>
<p>�i
; i D 1; : : : ; p.
</p>
<p>(iii) The rectangle surrounding the ellipsoid Ed is defined by the following
</p>
<p>inequalities:
</p>
<p>x0i �
p
d 2aii � xi � x0i C
</p>
<p>p
d 2aii; i D 1; : : : ; p;
</p>
<p>where aii is the .i; i/ element ofA�1. By the rectangle surrounding the ellipsoid
Ed we mean the rectangle whose sides are parallel to the coordinate axis.
</p>
<p>It is easy to find the coordinates of the tangency points between the ellipsoid and
</p>
<p>its surrounding rectangle parallel to the coordinate axes. Let us find the coordinates
</p>
<p>of the tangency point that are in the direction of the j -th coordinate axis (positive
</p>
<p>direction).
</p>
<p>For ease of notation, we suppose the ellipsoid is centred around the origin .x0 D
0/. If not, the rectangle will be shifted by the value of x0.
</p>
<p>The coordinate of the tangency point is given by the solution to the following
</p>
<p>problem:
</p>
<p>x D arg max
x&gt;AxDd 2
</p>
<p>e&gt;j x (2.35)
</p>
<p>where e&gt;j is the j -th column of the identity matrix Ip . The coordinate of the
tangency point in the negative direction would correspond to the solution of the
</p>
<p>min problem: by symmetry, it is the opposite value of the former.
</p>
<p>The solution is computed via the Lagrangian L D e&gt;j x � �.x&gt;Ax � d 2/ which
by (2.23) leads to the following system of equations:
</p>
<p>@L
</p>
<p>@x
D ej � 2�Ax D 0 (2.36)
</p>
<p>@L
</p>
<p>@�
D x&gt;Ax � d 2 D 0: (2.37)
</p>
<p>This gives x D 1
2�
A�1ej , or componentwise
</p>
<p>xi D
1
</p>
<p>2�
aij; i D 1; : : : ; p (2.38)
</p>
<p>where aij denotes the .i; j /-th element of A�1.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.6 Geometrical Aspects 71
</p>
<p>Premultiplying (2.36) by x&gt;, we have from (2.37):
</p>
<p>xj D 2�d 2:
</p>
<p>Comparing this to the value obtained by (2.38), for i D j we obtain 2� D
q
</p>
<p>ajj
</p>
<p>d 2
.
</p>
<p>We choose the positive value of the square root because we are maximising e&gt;j x. A
minimum would correspond to the negative value. Finally, we have the coordinates
</p>
<p>of the tangency point between the ellipsoid and its surrounding rectangle in the
</p>
<p>positive direction of the j -th axis:
</p>
<p>xi D
r
d 2
</p>
<p>ajj
aij; i D 1; : : : ; p: (2.39)
</p>
<p>The particular case where i D j provides statement (iii) in Theorem 2.7.
</p>
<p>Remark: Usefulness of Theorem 2.7
</p>
<p>Theorem 2.7 will prove to be particularly useful in many subsequent chapters. First,
</p>
<p>it provides a helpful tool for graphing an ellipse in two dimensions. Indeed, knowing
</p>
<p>the slope of the principal axes of the ellipse, their half-lengths and drawing the
</p>
<p>rectangle inscribing the ellipse, allows one to quickly draw a rough picture of the
</p>
<p>shape of the ellipse.
</p>
<p>In Chap. 7, it is shown that the confidence region for the vector� of a multivariate
</p>
<p>normal population is given by a particular ellipsoid whose parameters depend
</p>
<p>on sample characteristics. The rectangle inscribing the ellipsoid (which is much
</p>
<p>easier to obtain) will provide the simultaneous confidence intervals for all of the
</p>
<p>components in �.
</p>
<p>In addition it will be shown that the contour surfaces of the multivariate normal
</p>
<p>density are provided by ellipsoids whose parameters depend on the mean vector
</p>
<p>and on the covariance matrix. We will see that the tangency points between the
</p>
<p>contour ellipsoids and the surrounding rectangle are determined by regressing one
</p>
<p>component on the .p � 1/ other components. For instance, in the direction of the
j -th axis, the tangency points are given by the intersections of the ellipsoid contours
</p>
<p>with the regression line of the vector of .p � 1/ variables (all components except
the j -th) on the j -th component.</p>
<p/>
</div>
<div class="page"><p/>
<p>72 2 A Short Excursion into Matrix Algebra
</p>
<p>Norm of a Vector
</p>
<p>Consider a vector x 2 Rp . The norm or length of x (with respect to the metric Ip)
is defined as
</p>
<p>kxk D d.0p; x/ D
p
x&gt;x:
</p>
<p>If kxk D 1; x is called a unit vector. A more general norm can be defined with
respect to the metric A:
</p>
<p>kxkA D
p
x&gt;Ax:
</p>
<p>Angle Between Two Vectors
</p>
<p>Consider two vectors x and y 2 Rp . The angle � between x and y is defined by the
cosine of � :
</p>
<p>cos � D x
&gt;y
</p>
<p>kxk kyk ; (2.40)
</p>
<p>see Fig. 2.4. Indeed for p D 2, x D
 
x1
</p>
<p>x2
</p>
<p>!
and y D
</p>
<p> 
y1
</p>
<p>y2
</p>
<p>!
, we have
</p>
<p>kxk cos �1 D x1 I kyk cos �2 D y1
kxk sin �1 D x2 I kyk sin �2 D y2;
</p>
<p>(2.41)
</p>
<p>therefore,
</p>
<p>cos � D cos �1 cos �2 C sin �1 sin �2 D
x1y1 C x2y2
kxk kyk D
</p>
<p>x&gt;y
</p>
<p>kxk kyk :
</p>
<p>Fig. 2.4 Angle between
vectors</p>
<p/>
</div>
<div class="page"><p/>
<p>2.6 Geometrical Aspects 73
</p>
<p>Fig. 2.5 Projection
</p>
<p>Remark 2.1 If x&gt;y D 0, then the angle � is equal to �
2
. From trigonometry, we
</p>
<p>know that the cosine of � equals the length of the base of a triangle (jjpxjj) divided
by the length of the hypotenuse (jjxjj). Hence, we have
</p>
<p>jjpx jj D jjxjjj cos� j D
jx&gt;yj
kyk ; (2.42)
</p>
<p>where px is the projection of x on y (which is defined below). It is the coordinate
</p>
<p>of x on the y vector, see Fig. 2.5.
</p>
<p>The angle can also be defined with respect to a general metric A
</p>
<p>cos � D x
&gt;Ay
</p>
<p>kxkA kykA
: (2.43)
</p>
<p>If cos � D 0 then x is orthogonal to y with respect to the metric A.
Example 2.11 Assume that there are two centred (i.e. zero mean) data vectors. The
</p>
<p>cosine of the angle between them is equal to their correlation (defined in (3.8)).
</p>
<p>Indeed for x and y with x D y D 0 we have
</p>
<p>rXY D
P
xiyiqP
x2i
P
y2i
</p>
<p>D cos �
</p>
<p>according to formula (2.40).
</p>
<p>Rotations
</p>
<p>When we consider a point x 2 Rp , we generally use a p-coordinate system to obtain
its geometric representation, like in Fig. 2.1 for instance. There will be situations in
</p>
<p>multivariate techniques where we will want to rotate this system of coordinates by
</p>
<p>the angle � .
</p>
<p>Consider for example the point P with coordinates x D .x1; x2/&gt; in R2 with
respect to a given set of orthogonal axes. Let &#128; be a .2 � 2/ orthogonal matrix
where
</p>
<p>&#128; D
�
</p>
<p>cos � sin �
</p>
<p>� sin � cos �
</p>
<p>�
: (2.44)</p>
<p/>
</div>
<div class="page"><p/>
<p>74 2 A Short Excursion into Matrix Algebra
</p>
<p>If the axes are rotated about the origin through an angle � in a clockwise direction,
</p>
<p>the new coordinates of P will be given by the vector y
</p>
<p>y D &#128; x; (2.45)
</p>
<p>and a rotation through the same angle in a anti-clockwise direction gives the new
</p>
<p>coordinates as
</p>
<p>y D &#128;&gt; x: (2.46)
</p>
<p>More generally, premultiplying a vector x by an orthogonal matrix &#128; geomet-
</p>
<p>rically corresponds to a rotation of the system of axes, so that the first new axis is
</p>
<p>determined by the first row of &#128; . This geometric point of view will be exploited in
</p>
<p>Chaps. 11 and 12.
</p>
<p>Column Space and Null Space of a Matrix
</p>
<p>Define for X .n � p/
</p>
<p>Im.X /
defD C.X / D fx 2 Rn j 9a 2 Rp so that Xa D xg;
</p>
<p>the space generated by the columns of X or the column space of X . Note that
</p>
<p>C.X / � Rn and dimfC.X /g D rank.X / D r � min.n; p/:
</p>
<p>Ker.X /
defD N.X / D fy 2 Rp j Xy D 0g
</p>
<p>is the null space of X . Note that N.X / � Rp and that dimfN.X /g D p � r:
Remark 2.2 N.X&gt;/ is the orthogonal complement of C.X / in Rn, i.e. given a
vector b 2 Rn it will hold that x&gt;b D 0 for all x 2 C.X /, if and only if b 2 N.X&gt;/.
</p>
<p>Example 2.12 Let X D
</p>
<p>0
BB@
</p>
<p>2 3 5
</p>
<p>4 6 7
</p>
<p>6 8 6
</p>
<p>8 2 4
</p>
<p>1
CCA : It is easy to show (e.g. by calculating the
</p>
<p>determinant of X ) that rank.X / D 3. Hence, the column space of X is C.X / D R3.
The null space of X contains only the zero vector .0; 0; 0/&gt; and its dimension is
equal to rank.X / � 3 D 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.6 Geometrical Aspects 75
</p>
<p>For X D
</p>
<p>0
BB@
</p>
<p>2 3 1
</p>
<p>4 6 2
</p>
<p>6 8 3
</p>
<p>8 2 4
</p>
<p>1
CCA ; the third column is a multiple of the first one and the
</p>
<p>matrix X cannot be of full rank. Noticing that the first two columns of X are
</p>
<p>independent, we see that rank.X / D 2. In this case, the dimension of the columns
space is 2 and the dimension of the null space is 1.
</p>
<p>Projection Matrix
</p>
<p>A matrix P.n � n/ is called an (orthogonal) projection matrix in Rn if and only if
P D P&gt; D P2 (P is idempotent). Let b 2 Rn. Then a D Pb is the projection of b
on C.P/.
</p>
<p>Projection on C.X /
</p>
<p>Consider X .n � p/ and let
</p>
<p>P D X .X&gt;X /�1X&gt; (2.47)
</p>
<p>andQ D In � P . It&rsquo;s easy to check that P and Q are idempotent and that
</p>
<p>PX D X andQX D 0: (2.48)
</p>
<p>Since the columns of X are projected onto themselves, the projection matrix P
</p>
<p>projects any vector b 2 Rn onto C.X /. Similarly, the projection matrix Q projects
any vector b 2 Rn onto the orthogonal complement of C.X /.
Theorem 2.8 Let P be the projection (2.47) and Q its orthogonal complement.
</p>
<p>Then:
</p>
<p>(i) x D Pb entails x 2 C.X /,
(ii) y D Qb means that y&gt;x D 0 8x 2 C.X /.
Proof (i) holds, since x D X .X&gt;X /�1X&gt;b D Xa, where a D .X&gt;X /�1X&gt;b 2
</p>
<p>R
p .
</p>
<p>(ii) follows from y D b � Pb and x D Xa: Hence y&gt;x D b&gt;Xa �
b&gt;X .X&gt;X /�1X&gt;Xa D 0.
</p>
<p>ut</p>
<p/>
</div>
<div class="page"><p/>
<p>76 2 A Short Excursion into Matrix Algebra
</p>
<p>Remark 2.3 Let x; y 2 Rn and consider px 2 Rn, the projection of x on y (see
Fig. 2.5). With X D y we have from (2.47)
</p>
<p>px D y.y&gt;y/�1y&gt;x D
y&gt;x
</p>
<p>kyk2 y (2.49)
</p>
<p>and we can easily verify that
</p>
<p>kpxk D
q
p&gt;x px D
</p>
<p>jy&gt;xj
kyk :
</p>
<p>See again Remark 2.1.
</p>
<p>Summary
</p>
<p>,! A distance between two p-dimensional points x and y is a
quadratic form .x � y/&gt;A.x � y/ in the vectors of differences
.x � y/. A distance defines the norm of a vector.
</p>
<p>,! Iso-distance curves of a point x0 are all those points that have the
same distance from x0. Iso-distance curves are ellipsoids whose
</p>
<p>principal axes are determined by the direction of the eigenvectors
</p>
<p>ofA. The half-length of principal axes is proportional to the inverse
</p>
<p>of the roots of the eigenvalues of A.
</p>
<p>,! The angle between two vectors x and y is given by cos � D
x&gt;Ay
</p>
<p>kxkA kykA w.r.t. the metric A.
</p>
<p>,! For the Euclidean distance with A D I the correlation between
two centred data vectors x and y is given by the cosine of the angle
</p>
<p>between them, i.e. cos � D rXY .
,! The projection P D X .X&gt;X /�1X&gt; is the projection onto the
</p>
<p>column space C.X / of X .
</p>
<p>,! The projection of x 2 Rn on y 2 Rn is given by px D y
&gt;x
kyk2 y:
</p>
<p>2.7 Exercises
</p>
<p>Exercise 2.1 Compute the determinant for a .3 � 3/ matrix.
Exercise 2.2 Suppose thatjAj D 0. Is it possible that all eigenvalues of A are
positive?</p>
<p/>
</div>
<div class="page"><p/>
<p>2.7 Exercises 77
</p>
<p>Exercise 2.3 Suppose that all eigenvalues of some (square) matrix A are different
</p>
<p>from zero. Does the inverse A�1 of A exist?
</p>
<p>Exercise 2.4 Write a program that calculates the Jordan decomposition of the
</p>
<p>matrix
</p>
<p>A D
</p>
<p>0
@
1 2 3
</p>
<p>2 1 2
</p>
<p>3 2 1
</p>
<p>1
A :
</p>
<p>Check Theorem 2.1 numerically.
</p>
<p>Exercise 2.5 Prove (2.23), (2.24) and (2.25).
</p>
<p>Exercise 2.6 Show that a projection matrix only has eigenvalues in f0; 1g.
Exercise 2.7 Draw some iso-distance ellipsoids for the metric A D &dagger;�1 of
Example 3.13.
</p>
<p>Exercise 2.8 Find a formula for jA C aa&gt;j and for .A C aa&gt;/�1: (Hint: use the
</p>
<p>inverse partitioned matrix with B D
�
1 �a&gt;
a A
</p>
<p>�
:)
</p>
<p>Exercise 2.9 Prove the Binomial inverse theorem for two non-singular matrices
</p>
<p>A.p �p/ and B.p� p/: .ACB/�1 D A�1 �A�1.A�1CB�1/�1A�1: (Hint: use
</p>
<p>(2.26) with C D
�
A Ip
�Ip B�1
</p>
<p>�
:)</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 3
</p>
<p>Moving to Higher Dimensions
</p>
<p>We have seen in the previous chapters how very simple graphical devices can help in
</p>
<p>understanding the structure and dependency of data. The graphical tools were based
</p>
<p>on either univariate (bivariate) data representations or on &ldquo;slick&rdquo; transformations
</p>
<p>of multivariate information perceivable by the human eye. Most of the tools are
</p>
<p>extremely useful in a modelling step, but unfortunately, do not give the full picture
</p>
<p>of the data set. One reason for this is that the graphical tools presented capture
</p>
<p>only certain dimensions of the data and do not necessarily concentrate on those
</p>
<p>dimensions or sub-parts of the data under analysis that carry the maximum structural
</p>
<p>information. In Part III of this book, powerful tools for reducing the dimension of
</p>
<p>a data set will be presented. In this chapter, as a starting point, simple and basic
</p>
<p>tools are used to describe dependency. They are constructed from elementary facts
</p>
<p>of probability theory and introductory statistics (e.g. the covariance and correlation
</p>
<p>between two variables).
</p>
<p>Sections 3.1 and 3.2 show how to handle these concepts in a multivariate setup
</p>
<p>and how a simple test on correlation between two variables can be derived. Since
</p>
<p>linear relationships are involved in these measures, Sect. 3.4 presents the simple
</p>
<p>linear model for two variables and recalls the basic t-test for the slope. In Sect. 3.5,
</p>
<p>a simple example of one-factorial analysis of variance introduces the notations for
</p>
<p>the well-known F -test.
</p>
<p>Due to the power of matrix notation, all of this can easily be extended to a more
</p>
<p>general multivariate setup. Section 3.3 shows how matrix operations can be used to
</p>
<p>define summary statistics of a data set and for obtaining the empirical moments of
</p>
<p>linear transformations of the data. These results will prove to be very useful in most
</p>
<p>of the chapters in Part III.
</p>
<p>Finally, matrix notation allows us to introduce the flexible multiple linear model,
</p>
<p>where more general relationships among variables can be analysed. In Sect. 3.6, the
</p>
<p>least squares adjustment of the model and the usual test statistics are presented with
</p>
<p>their geometric interpretation. Using these notations, the ANOVA model is just a
</p>
<p>particular case of the multiple linear model.
</p>
<p>&copy; Springer-Verlag Berlin Heidelberg 2015
W.K. H&auml;rdle, L. Simar, Applied Multivariate Statistical Analysis,
DOI 10.1007/978-3-662-45171-7_3
</p>
<p>79</p>
<p/>
</div>
<div class="page"><p/>
<p>80 3 Moving to Higher Dimensions
</p>
<p>3.1 Covariance
</p>
<p>Covariance is a measure of dependency between random variables. Given two
</p>
<p>(random) variables X and Y the (theoretical) covariance is defined by:
</p>
<p>�XY D Cov.X; Y / D E.XY/ � .EX/.EY /: (3.1)
</p>
<p>The precise definition of expected values is given in Chap. 4. If X and Y are
</p>
<p>independent of each other, the covariance Cov.X; Y / is necessarily equal to zero,
</p>
<p>see Theorem 3.1. The converse is not true. The covariance of X with itself is the
</p>
<p>variance:
</p>
<p>�XX D Var.X/ D Cov.X;X/:
</p>
<p>If the variable X is p-dimensional multivariate, e.g. X D
</p>
<p>0
B@
X1
:::
</p>
<p>Xp
</p>
<p>1
CA, then the
</p>
<p>theoretical covariances among all the elements are put into matrix form, i.e. the
</p>
<p>covariance matrix:
</p>
<p>&dagger; D
</p>
<p>0
B@
�X1X1 : : : �X1Xp
:::
</p>
<p>: : :
:::
</p>
<p>�XpX1 : : : �XpXp
</p>
<p>1
CA :
</p>
<p>Properties of covariance matrices will be detailed in Chap. 4. Empirical versions of
</p>
<p>these quantities are:
</p>
<p>sXY D
1
</p>
<p>n
</p>
<p>nX
</p>
<p>iD1
.xi � x/.yi � y/ (3.2)
</p>
<p>sXX D
1
</p>
<p>n
</p>
<p>nX
</p>
<p>iD1
.xi � x/2: (3.3)
</p>
<p>For small n, say n � 20, we should replace the factor 1
n
in (3.2) and (3.3) by 1
</p>
<p>n�1 in
order to correct for a small bias. For a p-dimensional random variable, one obtains
</p>
<p>the empirical covariance matrix (see Sect. 3.3 for properties and details)
</p>
<p>S D
</p>
<p>0
B@
sX1X1 : : : sX1Xp
:::
</p>
<p>: : :
:::
</p>
<p>sXpX1 : : : sXpXp
</p>
<p>1
CA :</p>
<p/>
</div>
<div class="page"><p/>
<p>3.1 Covariance 81
</p>
<p>For a scatterplot of two variables the covariances measure &ldquo;how close the scatter
</p>
<p>is to a line&rdquo;. Mathematical details follow but it should already be understood here
</p>
<p>that in this sense covariance measures only &ldquo;linear dependence&rdquo;.
</p>
<p>Example 3.1 If X is the entire bank data set, one obtains the covariance matrix S
</p>
<p>as indicated below:
</p>
<p>S D
</p>
<p>0
BBBBBBB@
</p>
<p>0:14 0:03 0:02 �0:10 �0:01 0:08
0:03 0:12 0:10 0:21 0:10 �0:21
0:02 0:10 0:16 0:28 0:12 �0:24
�0:10 0:21 0:28 2:07 0:16 �1:03
�0:01 0:10 0:12 0:16 0:64 �0:54
0:08 �0:21 �0:24 �1:03 �0:54 1:32
</p>
<p>1
CCCCCCCA
</p>
<p>: (3.4)
</p>
<p>The empirical covariance between X4 and X5, i.e. sX4X5 , is found in row 4 and
</p>
<p>column 5. The value is sX4X5 D 0:16. Is it obvious that this value is positive? In
Exercise 3.1 we will discuss this question further.
</p>
<p>If Xf denotes the counterfeit bank notes, we obtain:
</p>
<p>Sf D
</p>
<p>0
BBBBBBB@
</p>
<p>0:123 0:031 0:023 �0:099 0:019 0:011
0:031 0:064 0:046 �0:024 �0:012 �0:005
0:024 0:046 0:088 �0:018 0:000 0:034
�0:099 �0:024 �0:018 1:268 �0:485 0:236
0:019 �0:012 0:000 �0:485 0:400 �0:022
0:011 �0:005 0:034 0:236 �0:022 0:308
</p>
<p>1
CCCCCCCA
� (3.5)
</p>
<p>For the genuine Xg , we have:
</p>
<p>Sg D
</p>
<p>0
BBBBBBB@
</p>
<p>0:149 0:057 0:057 0:056 0:014 0:005
</p>
<p>0:057 0:131 0:085 0:056 0:048 �0:043
0:057 0:085 0:125 0:058 0:030 �0:024
0:056 0:056 0:058 0:409 �0:261 �0:000
0:014 0:049 0:030 �0:261 0:417 �0:074
0:005 �0:043 �0:024 �0:000 �0:074 0:198
</p>
<p>1
CCCCCCCA
</p>
<p>� (3.6)
</p>
<p>Note that the covariance between X4 (distance of the frame to the lower border)
</p>
<p>and X5 (distance of the frame to the upper border) is negative in both (3.5) and
</p>
<p>(3.6). Why would this happen? In Exercise 3.2 we will discuss this question in more
</p>
<p>detail.
</p>
<p>At first sight, the matrices Sf and Sg look different, but they create almost the
</p>
<p>same scatterplots (see the discussion in Sect. 1.4). Similarly, the common principal
</p>
<p>component analysis in Chap. 11 suggests a joint analysis of the covariance structure
</p>
<p>as in Flury and Riedwyl (1988).</p>
<p/>
</div>
<div class="page"><p/>
<p>82 3 Moving to Higher Dimensions
</p>
<p>Fig. 3.1 Scatterplot of
variables X4 vs. X5 of the
entire bank data set
MVAscabank45
</p>
<p>7 8 9 10 11 12 13
</p>
<p>8
</p>
<p>9
</p>
<p>10
</p>
<p>11
</p>
<p>12
</p>
<p>13
Swiss bank notes
</p>
<p>Scatterplots with point clouds that are &ldquo;upward-sloping&rdquo;, like the one in the
</p>
<p>upper left of Fig. 1.14, show variables with positive covariance. Scatterplots with
</p>
<p>&ldquo;downward-sloping&rdquo; structure have negative covariance. In Fig. 3.1 we show the
</p>
<p>scatterplot of X4 vs. X5 of the entire bank data set. The point cloud is upward-
</p>
<p>sloping. However, the two sub-clouds of counterfeit and genuine bank notes are
</p>
<p>downward-sloping.
</p>
<p>Example 3.2 A textile shop manager is studying the sales of &ldquo;classic blue&rdquo;
</p>
<p>pullovers over ten different periods. He observes the number of pullovers sold
</p>
<p>(X1), variation in price (X2, in EUR), the advertisement costs in local newspapers
</p>
<p>(X3, in EUR) and the presence of a sales assistant (X4, in hours per period). Over
</p>
<p>the periods, he observes the following data matrix:
</p>
<p>X D
</p>
<p>0
BBBBBBBBBBBBBBB@
</p>
<p>230 125 200 109
</p>
<p>181 99 55 107
</p>
<p>165 97 105 98
</p>
<p>150 115 85 71
</p>
<p>97 120 0 82
</p>
<p>192 100 150 103
</p>
<p>181 80 85 111
</p>
<p>189 90 120 93
</p>
<p>172 95 110 86
</p>
<p>170 125 130 78
</p>
<p>1
CCCCCCCCCCCCCCCA
</p>
<p>:
</p>
<p>He is convinced that the price must have a large influence on the number of pullovers
</p>
<p>sold. So he makes a scatterplot of X2 vs. X1, see Fig. 3.2. A rough impression</p>
<p/>
</div>
<div class="page"><p/>
<p>3.1 Covariance 83
</p>
<p>Fig. 3.2 Scatterplot of
variables X2 vs. X1 of the
pullovers data set
MVAscapull1
</p>
<p>80 90 100 110 120
</p>
<p>100
</p>
<p>150
</p>
<p>200
</p>
<p>Pullovers Data
</p>
<p>Price (X2)
</p>
<p>S
a
</p>
<p>le
s
</p>
<p> (
X
</p>
<p>1
)
</p>
<p>is that the cloud is somewhat downward-sloping. A computation of the empirical
</p>
<p>covariance yields
</p>
<p>sX1X2 D
1
</p>
<p>9
</p>
<p>10X
</p>
<p>iD1
</p>
<p>�
X1i � NX1
</p>
<p>� �
X2i � NX2
</p>
<p>�
D �80:02;
</p>
<p>a negative value as expected.
</p>
<p>Note: The covariance function is scale dependent. Thus, if the prices in this
</p>
<p>example were in Japanese Yen (JPY), we would obtain a different answer (see
</p>
<p>Exercise 3.16). A measure of (linear) dependence independent of the scale is the
</p>
<p>correlation, which we introduce in the next section.
</p>
<p>Summary
</p>
<p>,! The covariance is a measure of dependence.
</p>
<p>,! Covariance measures only linear dependence.
</p>
<p>,! Covariance is scale dependent.
</p>
<p>,! There are non-linear dependencies that have zero covariance.
</p>
<p>,! Zero covariance does not imply independence.</p>
<p/>
</div>
<div class="page"><p/>
<p>84 3 Moving to Higher Dimensions
</p>
<p>Summary (continued)
</p>
<p>,! Independence implies zero covariance.
</p>
<p>,! Negative covariance corresponds to downward-sloping scatter-
plots.
</p>
<p>,! Positive covariance corresponds to upward-sloping scatterplots.
</p>
<p>,! The covariance of a variable with itself is its variance
Cov.X;X/ D �XX D �2X .
</p>
<p>,! For small n, we should replace the factor 1
n
in the computation of
</p>
<p>the covariance by 1
n�1 .
</p>
<p>3.2 Correlation
</p>
<p>The correlation between two variables X and Y is defined from the covariance as
</p>
<p>the following:
</p>
<p>�XY D
Cov.X; Y /p
</p>
<p>Var.X/Var.Y /
� (3.7)
</p>
<p>The advantage of the correlation is that it is independent of the scale, i.e. changing
</p>
<p>the variables&rsquo; scale of measurement does not change the value of the correlation.
</p>
<p>Therefore, the correlation is more useful as a measure of association between two
</p>
<p>random variables than the covariance. The empirical version of �XY is as follows:
</p>
<p>rXY D
sXYp
sXXsYY
</p>
<p>� (3.8)
</p>
<p>The correlation is in absolute value always less than 1. It is zero if the covariance
</p>
<p>is zero and vice versa. For p-dimensional vectors .X1; : : : ; Xp/
&gt; we have the
</p>
<p>theoretical correlation matrix
</p>
<p>P D
</p>
<p>0
B@
�X1X1 : : : �X1Xp
:::
</p>
<p>: : :
:::
</p>
<p>�XpX1 : : : �XpXp
</p>
<p>1
CA ;
</p>
<p>and its empirical version, the empirical correlation matrix which can be calculated
</p>
<p>from the observations,
</p>
<p>R D
</p>
<p>0
B@
rX1X1 : : : rX1Xp
:::
</p>
<p>: : :
:::
</p>
<p>rXpX1 : : : rXpXp
</p>
<p>1
CA :</p>
<p/>
</div>
<div class="page"><p/>
<p>3.2 Correlation 85
</p>
<p>Example 3.3 We obtain the following correlation matrix for the genuine bank
</p>
<p>notes:
</p>
<p>Rg D
</p>
<p>0
BBBBBBB@
</p>
<p>1:00 0:41 0:41 0:22 0:05 0:03
</p>
<p>0:41 1:00 0:66 0:24 0:20 �0:25
0:41 0:66 1:00 0:25 0:13 �0:14
0:22 0:24 0:25 1:00 �0:63 �0:00
0:05 0:20 0:13 �0:63 1:00 �0:25
0:03 �0:25 �0:14 �0:00 �0:25 1:00
</p>
<p>1
CCCCCCCA
; (3.9)
</p>
<p>and for the counterfeit bank notes:
</p>
<p>Rf D
</p>
<p>0
BBBBBBB@
</p>
<p>1:00 0:35 0:24 �0:25 0:08 0:06
0:35 1:00 0:61 �0:08 �0:07 �0:03
0:24 0:61 1:00 �0:05 0:00 0:20
�0:25 �0:08 �0:05 1:00 �0:68 0:37
0:08 �0:07 0:00 �0:68 1:00 �0:06
0:06 �0:03 0:20 0:37 �0:06 1:00
</p>
<p>1
CCCCCCCA
: (3.10)
</p>
<p>As noted before for Cov.X4; X5/, the correlation betweenX4 (distance of the frame
</p>
<p>to the lower border) and X5 (distance of the frame to the upper border) is negative.
</p>
<p>This is natural, since the covariance and correlation always have the same sign (see
</p>
<p>also Exercise 3.17).
</p>
<p>Why is the correlation an interesting statistic to study? It is related to indepen-
</p>
<p>dence of random variables, which we shall define more formally later on. For the
</p>
<p>moment we may think of independence as the fact that one variable has no influence
</p>
<p>on another.
</p>
<p>Theorem 3.1 If X and Y are independent, then �.X; Y / D Cov.X; Y / D 0:
</p>
<p>!
In general, the converse is not true, as the following example shows.
</p>
<p>Example 3.4 Consider a standard normally-distributed random variable X and a
</p>
<p>random variable Y D X2, which is surely not independent of X . Here we have
</p>
<p>Cov.X; Y / D E.XY/ � E.X/E.Y / D E.X3/ D 0
</p>
<p>(becauseE.X/ D 0 andE.X2/ D 1). Therefore �.X; Y / D 0, as well. This example
also shows that correlations and covariances measure only linear dependence. The
</p>
<p>quadratic dependence of Y D X2 on X is not reflected by these measures of
dependence.</p>
<p/>
</div>
<div class="page"><p/>
<p>86 3 Moving to Higher Dimensions
</p>
<p>Remark 3.1 For two normal random variables, the converse of Theorem 3.1
</p>
<p>is true: zero covariance for two normally distributed random variables implies
</p>
<p>independence. This will be shown later in Corollary 5.2.
</p>
<p>Theorem 3.1 enables us to check for independence between the components of
</p>
<p>a bivariate normal random variable. That is, we can use the correlation and test
</p>
<p>whether it is zero. The distribution of rXY for an arbitrary .X; Y / is unfortunately
</p>
<p>complicated. The distribution of rXY will be more accessible if .X; Y / are jointly
</p>
<p>normal (see Chap. 5). If we transform the correlation by Fisher&rsquo;s Z-transformation,
</p>
<p>W D 1
2
log
</p>
<p>�
1C rXY
1 � rXY
</p>
<p>�
; (3.11)
</p>
<p>we obtain a variable that has a more accessible distribution. Under the hypothesis
</p>
<p>that � D 0, W has an asymptotic normal distribution. Approximations of the
expectation and variance ofW are given by the following:
</p>
<p>E.W / � 1
2
log
</p>
<p>�
1C�XY
1��XY
</p>
<p>�
</p>
<p>Var.W / � 1
.n�3/ �
</p>
<p>(3.12)
</p>
<p>The distribution is given in Theorem 3.2.
</p>
<p>Theorem 3.2
</p>
<p>Z D W � E.W /p
Var.W /
</p>
<p>L�! N.0; 1/: (3.13)
</p>
<p>The symbol &ldquo;
L�!&rdquo; denotes convergence in distribution, which will be explained
</p>
<p>in more detail in Chap. 4.
</p>
<p>Theorem 3.2 allows us to test different hypotheses on correlation. We can fix the
</p>
<p>level of significance ˛ (the probability of rejecting a true hypothesis) and reject the
</p>
<p>hypothesis if the difference between the hypothetical value and the calculated value
</p>
<p>of Z is greater than the corresponding critical value of the normal distribution. The
</p>
<p>following example illustrates the procedure.
</p>
<p>Example 3.5 Let&rsquo;s study the correlation between mileage (X2) and weight (X8) for
</p>
<p>the car data set (22.3) where n D 74. We have rX2X8 D �0:823. Our conclusions
from the boxplot in Fig. 1.3 (&ldquo;Japanese cars generally have better mileage than the
</p>
<p>others&rdquo;) needs to be revised. From Fig. 3.3 and rX2X8 , we can see that mileage is
</p>
<p>highly correlated with weight, and that the Japanese cars in the sample are in fact
</p>
<p>all lighter than the others.
</p>
<p>If we want to know whether �X2X8 is significantly different from �0 D 0, we
apply Fisher&rsquo;s Z-transform (3.11). This gives us
</p>
<p>w D 1
2
log
</p>
<p>�
1C rX2X8
1 � rX2X8
</p>
<p>�
D �1:166 and z D �1:166� 0q
</p>
<p>1
71
</p>
<p>D �9:825;</p>
<p/>
</div>
<div class="page"><p/>
<p>3.2 Correlation 87
</p>
<p>Fig. 3.3 Mileage (X2) vs.
weight (X8) of US (star),
European (plus signs) and
Japanese (circle) cars
MVAscacar
</p>
<p>10 20 30 40 50
</p>
<p>2000
</p>
<p>3000
</p>
<p>4000
</p>
<p>5000
</p>
<p>Mileage (X2)
</p>
<p>W
e
ig
</p>
<p>h
t 
</p>
<p>(X
8
)
</p>
<p>Car Data
</p>
<p>i.e. a highly significant value to reject the hypothesis that � D 0 (the 2.5% and
97.5% quantiles of the normal distribution are �1:96 and 1:96, respectively). If we
want to test the hypothesis that, say, �0 D �0:75, we obtain:
</p>
<p>z D �1:166� .�0:973/q
1
71
</p>
<p>D �1:627:
</p>
<p>This is a non-significant value at the ˛ D 0:05 level for z since it is between the
critical values at the 5% significance level (i.e. �1:96 &lt; z &lt; 1:96).
Example 3.6 Let us consider again the pullovers data set from Example 3.2.
</p>
<p>Consider the correlation between the presence of the sales assistants (X4) vs. the
</p>
<p>number of sold pullovers (X1) (see Fig. 3.4). Here we compute the correlation as
</p>
<p>rX1X4 D 0:633:
</p>
<p>The Z-transform of this value is
</p>
<p>w D 1
2
log
</p>
<p>�
1C rX1X4
1� rX1X4
</p>
<p>�
D 0:746: (3.14)
</p>
<p>The sample size is n D 10, so for the hypothesis �X1X4 D 0, the statistic to
consider is:
</p>
<p>z D
p
7.0:746� 0/ D 1:974 (3.15)
</p>
<p>which is just statistically significant at the 5% level (i.e. 1.974 is just a little larger
</p>
<p>than 1.96).</p>
<p/>
</div>
<div class="page"><p/>
<p>88 3 Moving to Higher Dimensions
</p>
<p>Fig. 3.4 Hours of sales
assistants (X4) vs. sales (X1)
of pullovers
MVAscapull2
</p>
<p>70 80 90 100 110 120
</p>
<p>100
</p>
<p>150
</p>
<p>200
</p>
<p>Pullovers Data
</p>
<p>Sales Assistants (X4)
</p>
<p>S
a
</p>
<p>le
s
</p>
<p> (
X
</p>
<p>1
)
</p>
<p>Remark 3.2 The normalising and variance stabilising properties of W are asymp-
</p>
<p>totic. In addition the use of W in small samples (for n � 25) is improved by
Hotelling&rsquo;s transform (Hotelling, 1953):
</p>
<p>W � D W � 3W C tanh.W /
4.n� 1/ with Var.W
</p>
<p>�/ D 1
n � 1:
</p>
<p>The transformed variableW � is asymptotically distributed as a normal distribution.
</p>
<p>Example 3.7 From the preceding remark, we obtain w� D 0:6663 andp
10� 1w� D 1:9989 for the preceding Example 3.6. This value is significant
</p>
<p>at the 5% level.
</p>
<p>Remark 3.3 Note that the Fisher&rsquo;s Z-transform is the inverse of the hyperbolic
</p>
<p>tangent function:W D tanh�1.rXY/; equivalently rXY D tanh.W / D e
2W �1
e2WC1 .
</p>
<p>Remark 3.4 Under the assumptions of normality of X and Y , we may test their
</p>
<p>independence (�XY D 0) using the exact t-distribution of the statistic
</p>
<p>T D rXY
</p>
<p>s
n � 2
1 � r2XY
</p>
<p>�XYD0� tn�2:
</p>
<p>Setting the probability of the first error type to ˛, we reject the null hypothesis
</p>
<p>�XY D 0 if jT j � t1�˛=2In�2.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.3 Summary Statistics 89
</p>
<p>Summary
</p>
<p>,! The correlation is a standardised measure of dependence.
</p>
<p>,! The absolute value of the correlation is always less or equal to one.
</p>
<p>,! Correlation measures only linear dependence.
</p>
<p>,! There are non-linear dependencies that have zero correlation.
</p>
<p>,! Zero correlation does not imply independence. For two normal
random variables, it does.
</p>
<p>,! Independence implies zero correlation.
</p>
<p>,! Negative correlation corresponds to downward-sloping scatter-
plots.
</p>
<p>,! Positive correlation corresponds to upward-sloping scatterplots.
</p>
<p>,! Fisher&rsquo;s Z-transform helps us in testing hypotheses on correlation.
</p>
<p>,! For small samples, Fisher&rsquo;s Z-transform can be improved by the
transformationW � D W � 3WCtanh.W /
</p>
<p>4.n�1/ .
</p>
<p>3.3 Summary Statistics
</p>
<p>This section focuses on the representation of basic summary statistics (means,
</p>
<p>covariances and correlations) in matrix notation, since we often apply linear
</p>
<p>transformations to data. The matrix notation allows us to derive instantaneously
</p>
<p>the corresponding characteristics of the transformed variables. The Mahalanobis
</p>
<p>transformation is a prominent example of such linear transformations.
</p>
<p>Assume that we have observed n realisations of a p-dimensional random
</p>
<p>variable; we have a data matrix X .n � p/:
</p>
<p>X D
</p>
<p>0
BBBB@
</p>
<p>x11 � � � x1p
:::
</p>
<p>:::
:::
</p>
<p>:::
</p>
<p>xn1 � � � xnp
</p>
<p>1
CCCCA
: (3.16)
</p>
<p>The rows xi D .xi1; : : : ; xip/ 2 Rp denote the i th observation of a p-dimensional
random variable X 2 Rp.</p>
<p/>
</div>
<div class="page"><p/>
<p>90 3 Moving to Higher Dimensions
</p>
<p>The statistics that were briefly introduced in Sects. 3.1 and 3.2 can be rewritten in
</p>
<p>matrix form as follows. The &ldquo;centre of gravity&rdquo; of the n observations in Rp is given
</p>
<p>by the vector x of the means xj of the p variables:
</p>
<p>x D
</p>
<p>0
B@
x1
:::
</p>
<p>xp
</p>
<p>1
CA D n�1X&gt;1n: (3.17)
</p>
<p>The dispersion of the n observations can be characterised by the covariance
</p>
<p>matrix of the p variables. The empirical covariances defined in (3.2) and (3.3) are
</p>
<p>the elements of the following matrix:
</p>
<p>S D n�1X&gt;X � x x&gt; D n�1.X&gt;X � n�1X&gt;1n1&gt;n X /: (3.18)
</p>
<p>Note that this matrix is equivalently defined by
</p>
<p>S D 1
n
</p>
<p>nX
</p>
<p>iD1
.xi � x/.xi � x/&gt;:
</p>
<p>The covariance formula (3.18) can be rewritten as S D n�1X&gt;HX with the
centering matrix
</p>
<p>H D In � n�11n1&gt;n : (3.19)
</p>
<p>Note that the centering matrix is symmetric and idempotent. Indeed,
</p>
<p>H2 D .In � n�11n1&gt;n /.In � n�11n1&gt;n /
D In � n�11n1&gt;n � n�11n1&gt;n C .n�11n1&gt;n /.n�11n1&gt;n /
D In � n�11n1&gt;n D H:
</p>
<p>As a consequence S is positive semidefinite, i.e.
</p>
<p>S � 0: (3.20)
</p>
<p>Indeed for all a 2 Rp ,
</p>
<p>a&gt;Sa D n�1a&gt;X&gt;HXa
D n�1.a&gt;X&gt;H&gt;/.HXa/ since H&gt;H D H;
</p>
<p>D n�1y&gt;y D n�1
pX
</p>
<p>jD1
y2j � 0</p>
<p/>
</div>
<div class="page"><p/>
<p>3.3 Summary Statistics 91
</p>
<p>for y D HXa. It is well known from the one-dimensional case that n�1
Pn
</p>
<p>iD1.xi �
x/2 as an estimate of the variance exhibits a bias of the order n�1 (Breiman, 1973).
In the multi-dimensional case, Su D nn�1 S is an unbiased estimate of the true
covariance. (This will be shown in Example 4.15.)
</p>
<p>The sample correlation coefficient between the i th and j th variables is rXiXj , see
</p>
<p>(3.8). If D D diag.sXiXi /, then the correlation matrix is
</p>
<p>R D D�1=2SD�1=2; (3.21)
</p>
<p>where D�1=2 is a diagonal matrix with elements .sXiXi /
�1=2 on its main diagonal.
</p>
<p>Example 3.8 The empirical covariances are calculated for the pullover data set.
</p>
<p>The vector of the means of the four variables in the dataset is x D
.172:7; 104:6; 104:0; 93:8/&gt;.
</p>
<p>The sample covariance matrix is S D
</p>
<p>0
BB@
</p>
<p>1037:2 �80:2 1430:7 271:4
�80:2 219:8 92:1 �91:6
1430:7 92:1 2624 210:3
</p>
<p>271:4 �91:6 210:3 177:4
</p>
<p>1
CCA :
</p>
<p>The unbiased estimate of the variance (n D 10) is equal to
</p>
<p>Su D
10
</p>
<p>9
S D
</p>
<p>0
BB@
</p>
<p>1152:5 �88:9 1589:7 301:6
�88:9 244:3 102:3 �101:8
1589:7 102:3 2915:6 233:7
</p>
<p>301:6 �101:8 233:7 197:1
</p>
<p>1
CCA :
</p>
<p>The sample correlation matrix isR D
</p>
<p>0
BB@
</p>
<p>1 �0:17 0:87 0:63
�0:17 1 0:12 �0:46
0:87 0:12 1 0:31
</p>
<p>0:63 �0:46 0:31 1
</p>
<p>1
CCA :
</p>
<p>Linear Transformation
</p>
<p>In many practical applications we need to study linear transformations of the
</p>
<p>original data. This motivates the question of how to calculate summary statistics
</p>
<p>after such linear transformations.
</p>
<p>Let A be a (q � p) matrix and consider the transformed data matrix
</p>
<p>Y D XA&gt; D .y1; : : : ; yn/&gt;: (3.22)</p>
<p/>
</div>
<div class="page"><p/>
<p>92 3 Moving to Higher Dimensions
</p>
<p>The row yi D .yi1; : : : ; yiq/ 2 Rq can be viewed as the i th observation
of a q-dimensional random variable Y D AX . In fact we have yi D xiA&gt;.
We immediately obtain the mean and the empirical covariance of the variables
</p>
<p>(columns) forming the data matrix Y:
</p>
<p>y D 1
n
Y&gt;1n D
</p>
<p>1
</p>
<p>n
AX&gt;1n D Ax (3.23)
</p>
<p>SY D
1
</p>
<p>n
Y&gt;HY D 1
</p>
<p>n
AX&gt;HXA&gt; D ASXA&gt;: (3.24)
</p>
<p>Note that if the linear transformation is non-homogeneous, i.e.
</p>
<p>yi D Axi C b where b.q � 1/;
</p>
<p>only (3.23) changes: y D Ax C b. The formulas (3.23) and (3.24) are useful in the
particular case of q D 1, i.e. y D Xa, i.e. yi D a&gt;xi I i D 1; : : : ; n:
</p>
<p>y D a&gt;x
Sy D a&gt;SX a:
</p>
<p>Example 3.9 Suppose that X is the pullover data set. The manager wants to
</p>
<p>compute his mean expenses for advertisement (X3) and sales assistant (X4).
</p>
<p>Suppose that the sales assistant charges an hourly wage of 10 EUR. Then the
</p>
<p>shop manager calculates the expenses Y as Y D X3 C 10X4. Formula (3.22) says
that this is equivalent to defining the matrixA.4 � 1/ as:
</p>
<p>A D .0; 0; 1; 10/:
</p>
<p>Using formulas (3.23) and (3.24), it is now computationally very easy to obtain the
</p>
<p>sample mean y and the sample variance Sy of the overall expenses:
</p>
<p>y D Ax D .0; 0; 1; 10/
</p>
<p>0
BB@
</p>
<p>172:7
</p>
<p>104:6
</p>
<p>104:0
</p>
<p>93:8
</p>
<p>1
CCA D 1042:0
</p>
<p>SY D ASXA&gt; D .0; 0; 1; 10/
</p>
<p>0
BB@
</p>
<p>1152:5 �88:9 1589:7 301:6
�88:9 244:3 102:3 �101:8
1589:7 102:3 2915:6 233:7
</p>
<p>301:6 �101:8 233:7 197:1
</p>
<p>1
CCA
</p>
<p>0
BB@
</p>
<p>0
</p>
<p>0
</p>
<p>1
</p>
<p>10
</p>
<p>1
CCA
</p>
<p>D 2915:6C 4674C 19710D 27299:6:</p>
<p/>
</div>
<div class="page"><p/>
<p>3.4 Linear Model for Two Variables 93
</p>
<p>Mahalanobis Transformation
</p>
<p>A special case of this linear transformation is
</p>
<p>zi D S�1=2.xi � x/; i D 1; : : : ; n: (3.25)
</p>
<p>Note that for the transformed data matrix Z D .z1; : : : ; zn/&gt;,
</p>
<p>SZ D n�1Z&gt;HZ D Ip: (3.26)
</p>
<p>So the Mahalanobis transformation eliminates the correlation between the variables
</p>
<p>and standardises the variance of each variable. If we apply (3.24) usingA D S�1=2,
we obtain the identity covariance matrix as indicated in (3.26).
</p>
<p>Summary
</p>
<p>,! The centre of gravity of a data matrix is given by its mean vector
x D n�1X&gt;1n.
</p>
<p>,! The dispersion of the observations in a data matrix is given by the
empirical covariance matrix S D n�1X&gt;HX .
</p>
<p>,! The empirical correlation matrix is given byR D D�1=2SD�1=2.
</p>
<p>,! A linear transformation Y D XA&gt; of a data matrix X has mean
Ax and empirical covarianceASXA
</p>
<p>&gt;.
,! The Mahalanobis transformation is a linear transformation zi D
</p>
<p>S�1=2.xi � x/ which gives a standardised, uncorrelated data
matrix Z .
</p>
<p>3.4 Linear Model for Two Variables
</p>
<p>We have looked several times now at downward and upward-sloping scatterplots.
</p>
<p>What does the eye define here as a slope? Suppose that we can construct a line
</p>
<p>corresponding to the general direction of the cloud. The sign of the slope of this
</p>
<p>line would correspond to the upward and downward directions. Call the variable on
</p>
<p>the vertical axis Y and the one on the horizontal axis X . A slope line is a linear
</p>
<p>relationship between X and Y :
</p>
<p>yi D ˛ C ˇxi C "i ; i D 1; : : : ; n: (3.27)</p>
<p/>
</div>
<div class="page"><p/>
<p>94 3 Moving to Higher Dimensions
</p>
<p>Here, ˛ is the intercept and ˇ is the slope of the line. The errors (or deviations from
</p>
<p>the line) are denoted as "i and are assumed to have zero mean and finite variance
</p>
<p>�2. The task of finding .˛; ˇ/ in (3.27) is referred to as a linear adjustment.
</p>
<p>In Sect. 3.6 we shall derive estimators for ˛ and ˇ more formally, as well as
</p>
<p>accurately describe what a &ldquo;good&rdquo; estimator is. For now, one may try to find a
</p>
<p>&ldquo;good&rdquo; estimator . Ǫ ; Ǒ/ via graphical techniques. A very common numerical and
statistical technique is to use those Ǫ and Ǒ that minimise:
</p>
<p>. Ǫ ; Ǒ/ D argmin
.˛;ˇ/
</p>
<p>nX
</p>
<p>iD1
.yi � ˛ � ˇxi /2: (3.28)
</p>
<p>The solution to this task are the estimators:
</p>
<p>Ǒ D sXY
sXX
</p>
<p>(3.29)
</p>
<p>Ǫ D y � Ǒx: (3.30)
</p>
<p>The variance of Ǒ is:
</p>
<p>Var. Ǒ/ D �
2
</p>
<p>n � sXX
: (3.31)
</p>
<p>The standard error (SE) of the estimator is the square root of (3.31),
</p>
<p>SE. Ǒ/ D fVar. Ǒ/g1=2 D �
.n � sXX/1=2
</p>
<p>: (3.32)
</p>
<p>We can use this formula to test the hypothesis that ˇ D 0. In an application the
variance �2 has to be estimated by an estimator O�2 that will be given below. Under
a normality assumption of the errors, the t-test for the hypothesis ˇ D 0 works as
follows.
</p>
<p>One computes the statistic
</p>
<p>t D
Ǒ
</p>
<p>SE. Ǒ/
(3.33)
</p>
<p>and rejects the hypothesis at a 5% significance level if j t j� t0:975In�2, where the
97.5% quantile of the Student&rsquo;s tn�2 distribution is clearly the 95% critical value
for the two-sided test. For n � 30, this can be replaced by 1.96, the 97.5% quantile
of the normal distribution. An estimator O�2 of �2 will be given in the following.
Example 3.10 Let us apply the linear regression model (3.27) to the &ldquo;classic blue&rdquo;
</p>
<p>pullovers. The sales manager believes that there is a strong dependence on the</p>
<p/>
</div>
<div class="page"><p/>
<p>3.4 Linear Model for Two Variables 95
</p>
<p>80 90 100 110 120
</p>
<p>100
</p>
<p>150
</p>
<p>200
</p>
<p>Pullovers Data
</p>
<p>Price (X2)
</p>
<p>S
a
</p>
<p>le
s
</p>
<p> (
X
</p>
<p>1
)
</p>
<p>Fig. 3.5 Regression of sales (X1) on price (X2) of pullovers MVAregpull
</p>
<p>number of sales as a function of price. He computes the regression line as shown in
</p>
<p>Fig. 3.5.
</p>
<p>How good is this fit? This can be judged via goodness-of-fit measures. Define
</p>
<p>byi D Ǫ C Ǒxi ; (3.34)
</p>
<p>as the predicted value of y as a function of x. With Oy the textile shop manager in
the above example can predict sales as a function of prices x. The variation in the
</p>
<p>response variable is:
</p>
<p>nsYY D
nX
</p>
<p>iD1
.yi � y/2: (3.35)
</p>
<p>The variation explained by the linear regression (3.27) with the predicted values
</p>
<p>(3.34) is:
</p>
<p>nX
</p>
<p>iD1
.byi � y/2: (3.36)
</p>
<p>The residual sum of squares, the minimum in (3.28), is given by:
</p>
<p>RSS D
nX
</p>
<p>iD1
.yi �byi /2: (3.37)</p>
<p/>
</div>
<div class="page"><p/>
<p>96 3 Moving to Higher Dimensions
</p>
<p>An unbiased estimatorb�2 of �2 is given by RSS=.n � 2/.
The following relation holds between (3.35) and (3.37):
</p>
<p>nX
</p>
<p>iD1
.yi � y/2 D
</p>
<p>nX
</p>
<p>iD1
.byi � y/2 C
</p>
<p>nX
</p>
<p>iD1
.yi �by i/2; (3.38)
</p>
<p>Total variation D Explained variationC Unexplained variation:
</p>
<p>The coefficient of determination is r2:
</p>
<p>r2 D
</p>
<p>nP
iD1
.byi � y/2
</p>
<p>nP
iD1
.yi � y/2
</p>
<p>D explained variation
total variation
</p>
<p>� (3.39)
</p>
<p>The coefficient of determination increases with the proportion of explained variation
</p>
<p>by the linear relation (3.27). In the extreme cases where r2 D 1, all of the variation
is explained by the linear regression (3.27). The other extreme, r2 D 0, is where
the empirical covariance is sXY D 0. The coefficient of determination can be
rewritten as
</p>
<p>r2 D 1 �
</p>
<p>nP
iD1
.yi �byi /2
</p>
<p>nP
iD1
.yi � y/2
</p>
<p>: (3.40)
</p>
<p>From (3.39), it can be seen that in the linear regression (3.27), r2 D r2XY is the
square of the correlation between X and Y .
</p>
<p>Example 3.11 For the above pullover example, we estimate
</p>
<p>Ǫ D 210:774 and Ǒ D �0:364:
</p>
<p>The coefficient of determination is
</p>
<p>r2 D 0:028:
</p>
<p>The textile shop manager concludes that sales are not influenced very much by the
</p>
<p>price (in a linear way).
</p>
<p>The geometrical representation of formula (3.38) can be graphically evaluated
</p>
<p>using Fig. 3.6. This plot shows a section of the linear regression of the &ldquo;sales&rdquo;
</p>
<p>on &ldquo;price&rdquo; for the pullovers data. The distance between any point and the overall
</p>
<p>mean is given by the distance between the point and the regression line and the
</p>
<p>distance between the regression line and the mean. The sums of these two distances
</p>
<p>represent the total variance (solid blue lines from the observations to the overall</p>
<p/>
</div>
<div class="page"><p/>
<p>3.4 Linear Model for Two Variables 97
</p>
<p>Fig. 3.6 Regression of sales
(X1) on price (X2) of
pullovers. The overall mean is
given by the dashed line
MVAregzoom
</p>
<p>90 95 100
</p>
<p>170
</p>
<p>180
</p>
<p>190
</p>
<p>Pullover Data
</p>
<p>Price (X2)
</p>
<p>S
a
</p>
<p>le
s
</p>
<p> (
X
</p>
<p>1
)
</p>
<p>Fig. 3.7 Regression of X5
(upper inner frame) on X4
(lower inner frame) for
genuine bank notes
MVAregbank
</p>
<p>7 8 9 10
</p>
<p>8
</p>
<p>9
</p>
<p>10
</p>
<p>11
</p>
<p>12
</p>
<p>Swiss bank notes
</p>
<p>Lower inner frame (X4), genuine
</p>
<p>U
p
</p>
<p>p
e
</p>
<p>r 
in
</p>
<p>n
e
</p>
<p>r 
fr
</p>
<p>a
m
</p>
<p>e
 (
</p>
<p>X
5
</p>
<p>),
 g
</p>
<p>e
n
</p>
<p>u
in
</p>
<p>e
</p>
<p>mean), i.e. the explained variance (distance from the regression curve to the mean)
</p>
<p>and the unexplained variance (distance from the observation to the regression line),
</p>
<p>respectively.
</p>
<p>In general the regression of Y on X is different from that of X on Y . We will
</p>
<p>demonstrate this, once again, using the Swiss bank notes data.
</p>
<p>Example 3.12 The least squares fit of the variables X4 (X ) and X5 (Y ) from
</p>
<p>the genuine bank notes are calculated. Figure 3.7 shows the fitted line if X5 is
</p>
<p>approximated by a linear function of X4. In this case the parameters are
</p>
<p>Ǫ D 15:464 and Ǒ D �0:638:</p>
<p/>
</div>
<div class="page"><p/>
<p>98 3 Moving to Higher Dimensions
</p>
<p>If we predict X4 by a function of X5 instead, we would arrive at a different
</p>
<p>intercept and slope
</p>
<p>Ǫ D 14:666 and Ǒ D �0:626:
</p>
<p>The linear regression of Y on X is given by minimising (3.28), i.e. the vertical
</p>
<p>errors "i . The linear regression of X on Y does the same, but here the errors
</p>
<p>to be minimised in the least squares sense are measured horizontally. As seen in
</p>
<p>Example 3.12, the two least squares lines are different although both measure (in a
</p>
<p>certain sense) the slope of the cloud of points.
</p>
<p>As shown in the next example, there is still one other way to measure the
</p>
<p>main direction of a cloud of points: it is related to the spectral decomposition of
</p>
<p>covariance matrices.
</p>
<p>Example 3.13 Suppose that we have the following covariance matrix:
</p>
<p>&dagger; D
�
1 �
</p>
<p>� 1
</p>
<p>�
:
</p>
<p>Figure 3.8 shows a scatterplot of a sample of two normal random variables with
</p>
<p>such a covariance matrix (with � D 0:8).
The eigenvalues of &dagger; are, as was shown in Example 2.4, solutions to:
</p>
<p>ˇ̌
ˇ̌1 � � �
� 1 � �
</p>
<p>ˇ̌
ˇ̌ D 0:
</p>
<p>Fig. 3.8 Scatterplot for a
sample of two correlated
normal random variables
(sample size n D 150,
� D 0:8) MVAcorrnorm
</p>
<p>-3 -2 -1 0 1 2 3
-3
</p>
<p>-2
</p>
<p>-1
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>Normal sample, n=150
</p>
<p>X
</p>
<p>Y</p>
<p/>
</div>
<div class="page"><p/>
<p>3.4 Linear Model for Two Variables 99
</p>
<p>Hence, �1 D 1 C � and �2 D 1 � �. Therefore ƒ D diag.1 C �; 1 � �/. The
eigenvector corresponding to �1 D 1 C � can be computed from the system of
linear equations:
</p>
<p>�
1 �
</p>
<p>� 1
</p>
<p>��
x1
x2
</p>
<p>�
D .1C �/
</p>
<p>�
x1
x2
</p>
<p>�
</p>
<p>or
</p>
<p>x1 C �x2 D x1 C �x1
�x1 C x2 D x2 C �x2
</p>
<p>and thus
</p>
<p>x1 D x2:
</p>
<p>The first (standardised) eigenvector is
</p>
<p>&#13;1 D
�
1
ıp
</p>
<p>2
</p>
<p>1
ıp
</p>
<p>2
</p>
<p>�
:
</p>
<p>The direction of this eigenvector is the diagonal in Fig. 3.8 and captures the main
</p>
<p>variation in this direction. We shall come back to this interpretation in Chap. 11. The
</p>
<p>second eigenvector (orthogonal to &#13;1) is
</p>
<p>&#13;2 D
�
</p>
<p>1
ıp
</p>
<p>2
</p>
<p>�1
ıp
</p>
<p>2
</p>
<p>�
:
</p>
<p>So finally
</p>
<p>&#128; D .&#13;1 ; &#13;2/ D
�
1
ıp
</p>
<p>2 1
ıp
</p>
<p>2
</p>
<p>1
ıp
</p>
<p>2 �1
ıp
</p>
<p>2
</p>
<p>�
</p>
<p>and we can check our calculation by
</p>
<p>&dagger; D &#128; ƒ &#128;&gt; :
</p>
<p>The first eigenvector captures the main direction of a point cloud. The linear
</p>
<p>regression of Y on X and X on Y accomplished, in a sense, the same thing. In
</p>
<p>general the direction of the eigenvector and the least squares slope are different.
</p>
<p>The reason is that the least squares estimator minimises either vertical or horizontal
</p>
<p>errors (in (3.28)), whereas the first eigenvector corresponds to a minimisation that
</p>
<p>is orthogonal to the eigenvector (see Chap. 11).</p>
<p/>
</div>
<div class="page"><p/>
<p>100 3 Moving to Higher Dimensions
</p>
<p>Summary
</p>
<p>,! The linear regression y D ˛ C ˇx C " models a linear relation
between two one-dimensional variables.
</p>
<p>,! The sign of the slope Ǒ is the same as that of the covariance and the
correlation of x and y.
</p>
<p>,! A linear regression predicts values of Y given a possible observa-
tion x of X .
</p>
<p>,! The coefficient of determination r2 measures the amount of varia-
tion in Y which is explained by a linear regression on X .
</p>
<p>,! If the coefficient of determination is r2 D 1, then all points lie on
one line.
</p>
<p>,! The regression line of X on Y and the regression line of Y on X
are in general different.
</p>
<p>,! The t-test for the hypothesis ˇ D 0 is t D Ǒ
SE. Ǒ/ , where SE.
</p>
<p>Ǒ/ D
O�
</p>
<p>.n�sXX/1=2 .
</p>
<p>,! The t-test rejects the null hypothesis ˇ D 0 at the level of
significance ˛ if j t j� t1�˛=2In�2 where t1�˛In�2 is the 1 � ˛=2
quantile of the Student&rsquo;s t-distribution with .n � 2/ degrees of
freedom.
</p>
<p>,! The standard error SE. Ǒ/ increases/decreases with less/more
spread in the X variables.
</p>
<p>,! The direction of the first eigenvector of the covariance matrix of
a two-dimensional point cloud is different from the least squares
</p>
<p>regression line.
</p>
<p>3.5 Simple Analysis of Variance
</p>
<p>In a simple (i.e. one-factorial) analysis of variance (ANOVA), it is assumed that
</p>
<p>the average values of the response variable y are induced by one simple factor.
</p>
<p>Suppose that this factor takes on p values and that for each factor level, we have
</p>
<p>m D n=p observations. The sample is of the form given in Table 3.1, where all of
the observations are independent.
</p>
<p>The goal of a simple ANOVA is to analyse the observation structure
</p>
<p>ykl D �l C "kl for k D 1; : : : ; m; and l D 1; : : : ; p: (3.41)
</p>
<p>Each factor has a mean value �l . Each observation ykl is assumed to be a sum of the
</p>
<p>corresponding factor mean value �l and a zero mean random error "kl. The linear</p>
<p/>
</div>
<div class="page"><p/>
<p>3.5 Simple Analysis of Variance 101
</p>
<p>Table 3.1 Observation
structure of a simple ANOVA
</p>
<p>Sample element Factor levels l
</p>
<p>1 y11 � � � y1l � � � y1p
</p>
<p>2
:
:
:
</p>
<p>:
:
:
</p>
<p>:
:
:
</p>
<p>:
:
:
</p>
<p>:
:
:
</p>
<p>:
:
:
</p>
<p>:
:
:
</p>
<p>k yk1 � � � ykl � � � ykp
:
:
:
</p>
<p>:
:
:
</p>
<p>:
:
:
</p>
<p>:
:
:
</p>
<p>m D n=p ym1 � � � yml � � � ymp
</p>
<p>Table 3.2 Pullover sales as
function of marketing
strategy
</p>
<p>Shop Marketing strategy
</p>
<p>k Factor l
</p>
<p>1 2 3
</p>
<p>1 9 10 18
</p>
<p>2 11 15 14
</p>
<p>3 10 11 17
</p>
<p>4 12 15 9
</p>
<p>5 7 15 14
</p>
<p>6 11 13 17
</p>
<p>7 12 7 16
</p>
<p>8 10 15 14
</p>
<p>9 11 13 17
</p>
<p>10 13 10 15
</p>
<p>regression model falls into this scheme with m D 1, p D n and �i D ˛ C ˇxi ,
where xi is the i th level value of the factor.
</p>
<p>Example 3.14 The &ldquo;classic blue&rdquo; pullover company analyses the effect of three
</p>
<p>marketing strategies
</p>
<p>1. advertisement in local newspaper,
</p>
<p>2. presence of sales assistant,
</p>
<p>3. luxury presentation in shop windows.
</p>
<p>All of these strategies are tried in ten different shops. The resulting sale
</p>
<p>observations are given in Table 3.2.
</p>
<p>There are p D 3 factors and n D mp D 30 observations in the data. The &ldquo;classic
blue&rdquo; pullover company wants to know whether all three marketing strategies have
</p>
<p>the same mean effect or whether there are differences. Having the same effect
</p>
<p>means that all �l in (3.41) equal one value, �. The hypothesis to be tested is
</p>
<p>therefore
</p>
<p>H0 W �l D � for l D 1; : : : ; p:</p>
<p/>
</div>
<div class="page"><p/>
<p>102 3 Moving to Higher Dimensions
</p>
<p>The alternative hypothesis, that the marketing strategies have different effects, can
</p>
<p>be formulated as
</p>
<p>H1 W �l &curren; �l 0 for some l and l 0:
</p>
<p>This means that one marketing strategy is better than the others.
</p>
<p>The method used to test this problem is to compute as in (3.38) the total variation
</p>
<p>and to decompose it into the sources of variation. This gives:
</p>
<p>pX
</p>
<p>lD1
</p>
<p>mX
</p>
<p>kD1
.ykl � Ny/2 D m
</p>
<p>pX
</p>
<p>lD1
. Nyl � Ny/2 C
</p>
<p>pX
</p>
<p>lD1
</p>
<p>mX
</p>
<p>kD1
.ykl � Nyl /2 (3.42)
</p>
<p>The total variation (sum of squaresDSS) is:
</p>
<p>SS.reduced/ D
pX
</p>
<p>lD1
</p>
<p>mX
</p>
<p>kD1
.ykl � Ny/2 (3.43)
</p>
<p>where Ny D n�1
Pp
</p>
<p>lD1
Pm
</p>
<p>kD1 ykl is the overall mean. Here the total variation is
denoted as SS(reduced), since in comparison with the model under the alternative
</p>
<p>H1, we have a reduced set of parameters. In fact there is 1 parameter � D �l
underH0. Under H1, the &ldquo;full&rdquo; model, we have three parameters, namely the three
</p>
<p>different means �l .
</p>
<p>The variation underH1 is therefore:
</p>
<p>SS.full/ D
pX
</p>
<p>lD1
</p>
<p>mX
</p>
<p>kD1
.ykl � Nyl /2 (3.44)
</p>
<p>where Nyl D m�1
Pm
</p>
<p>kD1 ykl is the mean of each factor l . The hypothetical modelH0
is called reduced, since it has (relative toH1) fewer parameters.
</p>
<p>The F -test of the linear hypothesis is used to compare the difference in the
</p>
<p>variations under the reduced model H0 (3.43) and the full model H1 (3.44) to the
</p>
<p>variation under the full modelH1:
</p>
<p>F D fSS.reduced/� SS.full/g=fdf .r/ � df .f /g
SS.full/=df .f /
</p>
<p>: (3.45)
</p>
<p>Here df .f / and df .r/ denote the degrees of freedom under the full model and
</p>
<p>the reduced model, respectively. The degrees of freedom are essential in spec-
</p>
<p>ifying the shape of the F -distribution. They have a simple interpretation: df .�/
is equal to the number of observations minus the number of parameters in the
</p>
<p>model.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.5 Simple Analysis of Variance 103
</p>
<p>From Example 3.14, p D 3 parameters are estimated under the full model, i.e.
df .f / D n � p D 30 � 3 D 27. Under the reduced model, there is one parameter
to estimate, namely the overall mean, i.e. df .r/ D n � 1 D 29. We can compute
</p>
<p>SS.reduced/ D 260:3
</p>
<p>and
</p>
<p>SS.full/ D 157:7:
</p>
<p>The F -statistic (3.45) is therefore
</p>
<p>F D .260:3� 157:7/=2
157:7=27
</p>
<p>D 8:78:
</p>
<p>This value needs to be compared to the quantiles of the F2;27 distribution. Looking
</p>
<p>up the critical values in a F -distribution shows that the test statistic above is highly
</p>
<p>significant. We conclude that the marketing strategies have different effects.
</p>
<p>The F -Test in a Linear Regression Model
</p>
<p>The t-test of a linear regression model can be put into this framework. For a linear
</p>
<p>regression model (3.27), the reduced model is the one with ˇ D 0:
</p>
<p>yi D ˛ C 0 � xi C "i :
</p>
<p>The reduced model has n�1 degrees of freedom and one parameter, the intercept ˛.
The full model is given by ˇ &curren; 0,
</p>
<p>yi D ˛ C ˇ � xi C "i ;
</p>
<p>and has n � 2 degrees of freedom, since there are two parameters .˛; ˇ/.
The SS(reduced) equals
</p>
<p>SS.reduced/ D
nX
</p>
<p>iD1
.yi � Ny/2 D total variation:
</p>
<p>The SS(full) equals
</p>
<p>SS.full/ D
nX
</p>
<p>iD1
.yi � Oyi /2 D RSS D unexplained variation:</p>
<p/>
</div>
<div class="page"><p/>
<p>104 3 Moving to Higher Dimensions
</p>
<p>The F -test is therefore, from (3.45),
</p>
<p>F D .total variation� unexplained variation/ =1
(unexplained variation)=.n � 2/ (3.46)
</p>
<p>D explained variation
(unexplained variation)=.n� 2/ : (3.47)
</p>
<p>Using the estimators Ǫ and Ǒ the explained variation is:
</p>
<p>nX
</p>
<p>iD1
. Oyi � Ny/2 D
</p>
<p>nX
</p>
<p>iD1
</p>
<p>�
Ǫ C Ǒxi � Ny
</p>
<p>�2
</p>
<p>D
nX
</p>
<p>iD1
</p>
<p>n
. Ny � Ǒ Nx/C Ǒxi � Ny
</p>
<p>o2
</p>
<p>D
nX
</p>
<p>iD1
</p>
<p>Ǒ2.xi � Nx/2
</p>
<p>D Ǒ2nsXX :
</p>
<p>From (3.32) the F -ratio (3.46) is therefore:
</p>
<p>F D
Ǒ2nsXX
</p>
<p>RSS=.n � 2/ (3.48)
</p>
<p>D
 
Ǒ
</p>
<p>SE. Ǒ/
</p>
<p>!2
: (3.49)
</p>
<p>The t-test statistic (3.33) is just the square root of the F -statistic (3.49).
</p>
<p>Note, using (3.39) the F -statistic can be rewritten as
</p>
<p>F D r
2=1
</p>
<p>.1 � r2/=.n� 2/ :
</p>
<p>In the pullover Example 3.11, we obtain F D 0:028
0:972
</p>
<p>8
1
D 0:2305, so that the null
</p>
<p>hypothesis ˇ D 0 cannot be rejected. We conclude therefore that there is only a
minor influence of prices on sales.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.6 Multiple Linear Model 105
</p>
<p>Summary
</p>
<p>,! Simple ANOVA models an output Y as a function of one factor.
</p>
<p>,! The reduced model is the hypothesis of equal means.
</p>
<p>,! The full model is the alternative hypothesis of different means.
</p>
<p>,! The F -test is based on a comparison of the sum of squares under
the full and the reduced models.
</p>
<p>,! The degrees of freedom are calculated as the number of observa-
tions minus the number of parameters.
</p>
<p>,! The F -statistic is
</p>
<p>F D fSS.reduced/ � SS.full/g=fdf .r/� df .f /g
SS.full/=df .f /
</p>
<p>:
</p>
<p>,! The F -test rejects the null hypothesis if the F -statistic is larger
than the 95% quantile of the Fdf .r/�df .f /;df .f / distribution.
</p>
<p>,! The F -test statistic for the slope of the linear regression model
yi D ˛ C ˇxi C "i is the square of the t-test statistic.
</p>
<p>3.6 Multiple Linear Model
</p>
<p>The simple linear model and the analysis of variance model can be viewed as a
</p>
<p>particular case of a more general linear model where the variations of one variable y
</p>
<p>are explained by p explanatory variables x respectively. Let y .n�1/ and X .n�p/
be a vector of observations on the response variable and a data matrix on the p
</p>
<p>explanatory variables. An important application of the developed theory is the least
</p>
<p>squares fitting. The idea is to approximate y by a linear combination Oy of columns
of X , i.e. Oy 2 C.X /. The problem is to find Ǒ 2 Rp such that Oy D X Ǒ is the best
fit of y in the least-squares sense. The linear model can be written as
</p>
<p>y D Xˇ C "; (3.50)
</p>
<p>where " are the errors. The least squares solution is given by Ǒ:
</p>
<p>Ǒ D argmin
ˇ
.y � Xˇ/&gt;.y � Xˇ/ D argmin
</p>
<p>ˇ
"&gt;": (3.51)</p>
<p/>
</div>
<div class="page"><p/>
<p>106 3 Moving to Higher Dimensions
</p>
<p>Suppose that .X&gt;X / is of full rank and thus invertible. Minimising the expres-
sion (3.51) with respect to ˇ yields:
</p>
<p>Ǒ D .X&gt;X /�1X&gt;y: (3.52)
</p>
<p>The fitted value Oy D X Ǒ D X .X&gt;X /�1X&gt;y D Py is the projection of y onto
C.X / as computed in (2.47).
</p>
<p>The least squares residuals are
</p>
<p>e D y � Oy D y � X Ǒ D Qy D .In � P/y:
</p>
<p>The vector e is the projection of y onto the orthogonal complement of C.X /.
</p>
<p>Remark 3.5 A linear model with an intercept ˛ can also be written in this
</p>
<p>framework. The approximating equation is:
</p>
<p>yi D ˛ C ˇ1xi1 C � � � C ˇpxip C "i I i D 1; : : : ; n:
</p>
<p>This can be written as:
</p>
<p>y D X �ˇ� C "
</p>
<p>where X � D .1n X / (we add a column of ones to the data). We have by (3.52):
</p>
<p>Ǒ� D
� Ǫ
Ǒ
</p>
<p>�
D .X �&gt;X �/�1X �&gt;y:
</p>
<p>Example 3.15 Let us come back to the &ldquo;classic blue&rdquo; pullovers example. In
</p>
<p>Example 3.11, we considered the regression fit of the sales X1 on the price X2
and concluded that there was only a small influence of sales by changing the prices.
</p>
<p>A linear model incorporating all three variables allows us to approximate sales as
</p>
<p>a linear function of price (X2), advertisement (X3) and presence of sales assistants
</p>
<p>(X4) simultaneously. Adding a column of ones to the data (in order to estimate the
</p>
<p>intercept ˛) leads to
</p>
<p>Ǫ D 65:670 and b̌1 D �0:216; b̌2 D 0:485; b̌3 D 0:844:
</p>
<p>The coefficient of determination is computed as before in (3.40) and is:
</p>
<p>r2 D 1 � e
&gt;e
</p>
<p>P
.yi � y/2
</p>
<p>D 0:907:
</p>
<p>We conclude that the variation of X1 is well approximated by the linear relation.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.6 Multiple Linear Model 107
</p>
<p>Remark 3.6 The coefficient of determination is influenced by the number of
</p>
<p>regressors. For a given sample size n, the r2 value will increase by adding more
</p>
<p>regressors into the linear model. The value of r2 may therefore be high even if
</p>
<p>possibly irrelevant regressors are included. An adjusted coefficient of determination
</p>
<p>for p regressors and a constant intercept (p C 1 parameters) is
</p>
<p>r2adj D r2 �
p.1 � r2/
n � .p C 1/ : (3.53)
</p>
<p>Example 3.16 The corrected coefficient of determination for Example 3.15 is
</p>
<p>r2adj D 0:907�
3.1� 0:9072/
10 � 3 � 1
</p>
<p>D 0:818:
</p>
<p>This means that 81:8% of the variation of the response variable is explained by the
</p>
<p>explanatory variables.
</p>
<p>Note that the linear model (3.50) is very flexible and can model non-linear
</p>
<p>relationships between the response y and the explanatory variables x. For example,
</p>
<p>a quadratic relation in one variable x could be included. Then yi D ˛ C ˇ1xi C
ˇ2x
</p>
<p>2
i C "i could be written in matrix notation as in (3.50), y D Xˇ C " where
</p>
<p>X D
</p>
<p>0
BBB@
</p>
<p>1 x1 x
2
1
</p>
<p>1 x2 x
2
2
</p>
<p>:::
:::
:::
</p>
<p>1 xn x
2
n
</p>
<p>1
CCCA :
</p>
<p>Properties of Ǒ
</p>
<p>When yi is the i th observation of a random variable Y , the errors are also random.
</p>
<p>Under standard assumptions (independence, zero mean and constant variance �2),
</p>
<p>inference can be conducted on ˇ. Using the properties of Chap. 4, it is easy to prove:
</p>
<p>E. Ǒ/ D ˇ
</p>
<p>Var. Ǒ/ D �2.X&gt;X /�1:
</p>
<p>The analogue of the t-test for the multivariate linear regression situation is
</p>
<p>t D
b̌
j
</p>
<p>SE.b̌j /
:</p>
<p/>
</div>
<div class="page"><p/>
<p>108 3 Moving to Higher Dimensions
</p>
<p>The standard error of each coefficient b̌j is given by the square root of the diagonal
elements of the matrix Var. Ǒ/. In standard situations, the variance �2 of the error "
is not known. For linear model with intercept, one may estimate it by
</p>
<p>O�2 D 1
n � .p C 1/.y � Oy/
</p>
<p>&gt;.y � Oy/;
</p>
<p>where p is the dimension of ˇ. In testing ˇj D 0 we reject the hypothesis at the
significance level ˛ if jt j � t1�˛=2In�.pC1/. More general issues on testing linear
models are addressed in Chap. 7.
</p>
<p>The ANOVA Model in Matrix Notation
</p>
<p>The simple ANOVA problem (Sect. 3.5) may also be rewritten in matrix terms.
</p>
<p>Recall the definition of a vector of ones from (2.1) and define a vector of zeros
</p>
<p>as 0n. Then construct the following (n � p) matrix (here p D 3),
</p>
<p>X D
</p>
<p>0
@
1m 0m 0m
</p>
<p>0m 1m 0m
0m 0m 1m
</p>
<p>1
A ; (3.54)
</p>
<p>wherem D 10. Equation (3.41) then reads as follows.
The parameter vector is ˇ D .�1; �2; �3/&gt;. The data set from Example 3.14 can
</p>
<p>therefore be written as a linear model y D Xˇ C " where y 2 Rn with n D m � p
is the stacked vector of the columns of Table 3.1. The projection into the column
</p>
<p>space C.X / of (3.54) yields the least-squares estimator Ǒ D .X&gt;X /�1X&gt;y. Note
that .X&gt;X /�1 D .1=10/I3 and that X&gt;y D .106; 124; 151/&gt; is the sum
</p>
<p>Pm
kD1 ykj
</p>
<p>for each factor, i.e. the three column sums of Table 3.1. The least squares estimator
</p>
<p>is therefore the vector ǑH1 D . O�1; O�2; O�3/ D .10:6; 12:4; 15:1/&gt; of sample means
for each factor level j D 1; 2; 3. Under the null hypothesis of equal mean values
�1 D �2 D �3 D �, we estimate the parameters under the same constraints. This
can be put into the form of a linear constraint:
</p>
<p>��1 C �2 D 0
��1 C �3 D 0:
</p>
<p>This can be written as Aˇ D a, where
</p>
<p>a D
�
0
</p>
<p>0
</p>
<p>�</p>
<p/>
</div>
<div class="page"><p/>
<p>3.6 Multiple Linear Model 109
</p>
<p>and
</p>
<p>A D
�
�1 1 0
�1 0 1
</p>
<p>�
:
</p>
<p>The constrained least-squares solution can be shown (Exercise 3.24) to be given by:
</p>
<p>Ǒ
H0 D ǑH1 � .X&gt;X /�1A&gt;fA.X&gt;X /�1A&gt;g�1.A ǑH1 � a/: (3.55)
</p>
<p>It turns out that (3.55) amounts to simply calculating the overall mean Ny D 12:7 of
the response variable y: ǑH0 D .12:7; 12:7; 12:7/&gt;.
</p>
<p>The F -test that has already been applied in Example 3.14 can be written as
</p>
<p>F D fjjy � X
Ǒ
H0 jj2 � jjy � X ǑH1 jj2g=2
jjy � X ǑH1 jj2=27
</p>
<p>(3.56)
</p>
<p>which gives the same significant value 8:78. Note that again we compare the RSSH0
of the reduced model to the RSSH1 of the full model. It corresponds to comparing
</p>
<p>the lengths of projections into different column spaces. This general approach in
</p>
<p>testing linear models is described in detail in Chap. 7.
</p>
<p>Summary
</p>
<p>,! The relation y D Xˇ C e models a linear relation between a one-
dimensional variable Y and a p-dimensional variable X . Py gives
</p>
<p>the best linear regression fit of the vector y onto C.X /. The least
</p>
<p>squares parameter estimator is Ǒ D .X&gt;X /�1X&gt;y.
,! The simple ANOVA model can be written as a linear model.
</p>
<p>,! The ANOVA model can be tested by comparing the length of the
projection vectors.
</p>
<p>,! The test statistic of the F -test can be written as
</p>
<p>fjjy � X ǑH0 jj2 � jjy � X ǑH1 jj2g=fdf .r/ � df .f /g
jjy � X ǑH1 jj2=df .f /
</p>
<p>:
</p>
<p>,! The adjusted coefficient of determination is
</p>
<p>r2adj D r2 �
p.1 � r2/
n � .p C 1/ :</p>
<p/>
</div>
<div class="page"><p/>
<p>110 3 Moving to Higher Dimensions
</p>
<p>3.7 Boston Housing
</p>
<p>The main statistics presented so far can be computed for the data matrixX .506�14/
from our Boston Housing data set. The sample means and the sample medians
</p>
<p>of each variable are displayed in Table 3.3. The table also provides the unbiased
</p>
<p>estimates of the variance of each variable and the corresponding standard deviations.
</p>
<p>The comparison of the means and the medians confirms the asymmetry of the
</p>
<p>components of X that was pointed out in Sect. 1.9.
</p>
<p>The (unbiased) sample covariance matrix is given by the following .14 � 14/
matrix Sn:
</p>
<p>0
BBBBBBBBBBBBBBBBBBBBBBBBBBBB@
</p>
<p>73:99 �40:22 23:99�0:12 0:42 �1:33 85:41 �6:88 46:85 844:82 5:40 �302:38 27:99 �30:72
�40:22 543:94 �85:41�0:25�1:40 5:11�373:90 32:63 �63:35�1236:45�19:78 373:72 �68:78 77:32
23:99 �85:41 47:06 0:11 0:61 �1:89 124:51 �10:23 35:55 833:36 5:69 �223:58 29:58 �30:52
�0:12 �0:25 0:11 0:06 0:00 0:02 0:62 �0:05 �0:02 �1:52 �0:07 1:13 �0:10 0:41
0:42 �1:40 0:61 0:00 0:01 �0:02 2:39 �0:19 0:62 13:05 0:05 �4:02 0:49 �0:46
</p>
<p>�1:33 5:11 �1:89 0:02�0:02 0:49 �4:75 0:30 �1:28 �34:58 �0:54 8:22 �3:08 4:49
85:41 �373:90 124:51 0:62 2:39 �4:75 792:36 �44:33 111:77 2402:69 15:94 �702:94 121:08 �97:59
�6:88 32:63 �10:23�0:05�0:19 0:30 �44:33 4:43 �9:07 �189:66 �1:06 56:04 �7:47 4:84
46:85 �63:35 35:55�0:02 0:62 �1:28 111:77 �9:07 75:82 1335:76 8:76 �353:28 30:39 �30:56
844:82�1236:45 833:36�1:52 13:05�34:58 2402:69�189:66 1335:76 28404:76 168:15�6797:91 654:71�726:26
5:40 �19:78 5:69�0:07 0:05 �0:54 15:94 �1:06 8:76 168:15 4:69 �35:06 5:78 �10:11
</p>
<p>�302:38 373:72�223:58 1:13�4:02 8:22�702:94 56:04�353:28�6797:91�35:06 8334:75�238:67 279:99
27:99 �68:78 29:58�0:10 0:49 �3:08 121:08 �7:47 30:39 654:71 5:78 �238:67 50:99 �48:45
</p>
<p>�30:72 77:32 �30:52 0:41�0:46 4:49 �97:59 4:84 �30:56 �726:26�10:11 279:99 �48:45 84:59
</p>
<p>1
CCCCCCCCCCCCCCCCCCCCCCCCCCCCA
</p>
<p>;
</p>
<p>and the corresponding correlation matrixR.14 � 14/ is:
</p>
<p>Table 3.3 Descriptive
statistics for the Boston
Housing data set
MVAdescbh
</p>
<p>X x Median.X/ Var.X/ Std.X/
</p>
<p>X1 3:61 0:26 73:99 8:60
</p>
<p>X2 11:36 0:00 543:94 23:32
</p>
<p>X3 11:14 9:69 47:06 6:86
</p>
<p>X4 0:07 0:00 0:06 0:25
</p>
<p>X5 0:55 0:54 0:01 0:12
</p>
<p>X6 6:28 6:21 0:49 0:70
</p>
<p>X7 68:57 77:50 792:36 28:15
</p>
<p>X8 3:79 3:21 4:43 2:11
</p>
<p>X9 9:55 5:00 75:82 8:71
</p>
<p>X10 408:24 330:00 28;405:00 168:54
</p>
<p>X11 18:46 19:05 4:69 2:16
</p>
<p>X12 356:67 391:44 8;334:80 91:29
</p>
<p>X13 12:65 11:36 50:99 7:14
</p>
<p>X14 22:53 21:20 84:59 9:20</p>
<p/>
</div>
<div class="page"><p/>
<p>3.7 Boston Housing 111
</p>
<p>0
BBBBBBBBBBBBBBBBBBBBBBBBB@
</p>
<p>1:00�0:20 0:41�0:06 0:42�0:22 0:35�0:38 0:63 0:58 0:29�0:39 0:46�0:39
�0:20 1:00�0:53�0:04�0:52 0:31�0:57 0:66�0:31�0:31�0:39 0:18�0:41 0:36
0:41�0:53 1:00 0:06 0:76�0:39 0:64�0:71 0:60 0:72 0:38�0:36 0:60�0:48
�0:06�0:04 0:06 1:00 0:09 0:09 0:09�0:10�0:01�0:04�0:12 0:05�0:05 0:18
0:42�0:52 0:76 0:09 1:00�0:30 0:73�0:77 0:61 0:67 0:19�0:38 0:59�0:43
�0:22 0:31�0:39 0:09�0:30 1:00�0:24 0:21�0:21�0:29�0:36 0:13�0:61 0:70
0:35�0:57 0:64 0:09 0:73�0:24 1:00�0:75 0:46 0:51 0:26�0:27 0:60�0:38
�0:38 0:66�0:71�0:10�0:77 0:21�0:75 1:00�0:49�0:53�0:23 0:29�0:50 0:25
0:63�0:31 0:60�0:01 0:61�0:21 0:46�0:49 1:00 0:91 0:46�0:44 0:49�0:38
0:58�0:31 0:72�0:04 0:67�0:29 0:51�0:53 0:91 1:00 0:46�0:44 0:54�0:47
0:29�0:39 0:38�0:12 0:19�0:36 0:26�0:23 0:46 0:46 1:00�0:18 0:37�0:51
�0:39 0:18�0:36 0:05�0:38 0:13�0:27 0:29�0:44�0:44�0:18 1:00�0:37 0:33
0:46�0:41 0:60�0:05 0:59�0:61 0:60�0:50 0:49 0:54 0:37�0:37 1:00�0:74
�0:39 0:36�0:48 0:18�0:43 0:70�0:38 0:25�0:38�0:47�0:51 0:33�0:74 1:00
</p>
<p>1
CCCCCCCCCCCCCCCCCCCCCCCCCA
</p>
<p>:
</p>
<p>AnalysingR confirms most of the comments made from examining the scatterplot
</p>
<p>matrix in Chap. 1. In particular, the correlation betweenX14 (the value of the house)
</p>
<p>and all the other variables is given by the last row (or column) of R. The highest
</p>
<p>correlations (in absolute values) are in decreasing order X13; X6; X11; X10; etc.
</p>
<p>Using the Fisher&rsquo;s Z-transform on each of the correlations between X14 and the
</p>
<p>other variables would confirm that all are significantly different from zero, except
</p>
<p>the correlation between X14 and X4 (the indicator variable for the Charles River).
</p>
<p>We know, however, that the correlation and Fisher&rsquo;sZ-transform are not appropriate
</p>
<p>for binary variable.
</p>
<p>The same descriptive statistics can be calculated for the transformed variables
</p>
<p>(transformations were motivated in Sect. 1.9). The results are given in Table 3.4
</p>
<p>and as can be seen, most of the variables are now more symmetric. Note that the
</p>
<p>Table 3.4 Descriptive
statistics for the Boston
Housing data set after the
transformation
MVAdescbh
</p>
<p>QX Qx Median. QX/ Var. QX/ Std. QX/
eX1 �0:78 �1:36 4:67 2:16
eX2 1:14 0:00 5:44 2:33
eX3 2:16 2:27 0:60 0:78
eX4 0:07 0:00 0:06 0:25
eX5 �0:61 �0:62 0:04 0:20
eX6 1:83 1:83 0:01 0:11
eX7 5:06 5:29 12:72 3:57
eX8 1:19 1:17 0:29 0:54
eX9 1:87 1:61 0:77 0:87
eX10 5:93 5:80 0:16 0:40
eX11 2:15 2:04 1:86 1:36
eX12 3:57 3:91 0:83 0:91
eX13 3:42 3:37 0:97 0:99
eX14 3:03 3:05 0:17 0:41</p>
<p/>
</div>
<div class="page"><p/>
<p>112 3 Moving to Higher Dimensions
</p>
<p>covariances and the correlations are sensitive to these non-linear transformations.
</p>
<p>For example, the correlation matrix is now
</p>
<p>0
BBBBBBBBBBBBBBBBBBBBBBBBB@
</p>
<p>1:00�0:52 0:74 0:03 0:81�0:32 0:70�0:74 0:84 0:81 0:45�0:48 0:62�0:57
�0:52 1:00�0:66�0:04�0:57 0:31�0:53 0:59�0:35�0:31�0:35 0:18�0:45 0:36
0:74�0:66 1:00 0:08 0:75�0:43 0:66�0:73 0:58 0:66 0:46�0:33 0:62�0:55
0:03�0:04 0:08 1:00 0:08 0:08 0:07�0:09 0:01�0:04�0:13 0:05�0:06 0:16
0:81�0:57 0:75 0:08 1:00�0:32 0:78�0:86 0:61 0:67 0:34�0:38 0:61�0:52
�0:32 0:31�0:43 0:08�0:32 1:00�0:28 0:28�0:21�0:31�0:32 0:13�0:64 0:61
0:70�0:53 0:66 0:07 0:78�0:28 1:00�0:80 0:47 0:54 0:38�0:29 0:64�0:48
�0:74 0:59�0:73�0:09�0:86 0:28�0:80 1:00�0:54�0:60�0:32 0:32�0:56 0:41
0:84�0:35 0:58 0:01 0:61�0:21 0:47�0:54 1:00 0:82 0:40�0:41 0:46�0:43
0:81�0:31 0:66�0:04 0:67�0:31 0:54�0:60 0:82 1:00 0:48�0:43 0:53�0:56
0:45�0:35 0:46�0:13 0:34�0:32 0:38�0:32 0:40 0:48 1:00�0:20 0:43�0:51
�0:48 0:18�0:33 0:05�0:38 0:13�0:29 0:32�0:41�0:43�0:20 1:00�0:36 0:40
0:62�0:45 0:62�0:06 0:61�0:64 0:64�0:56 0:46 0:53 0:43�0:36 1:00�0:83
�0:57 0:36�0:55 0:16�0:52 0:61�0:48 0:41�0:43�0:56�0:51 0:40�0:83 1:00
</p>
<p>1
CCCCCCCCCCCCCCCCCCCCCCCCCA
</p>
<p>:
</p>
<p>Notice that some of the correlations between eX14 and the other variables have
increased.
</p>
<p>If we want to explain the variations of the price eX14 by the variation of all the
other variables eX1; : : : ; eX13 we could estimate the linear model
</p>
<p>eX14 D ˇ0 C
13X
</p>
<p>jD1
ˇjeX j C ": (3.57)
</p>
<p>The result is given in Table 3.5.
</p>
<p>Table 3.5 Linear regression
results for all variables of
Boston Housing data set
MVAlinregbh
</p>
<p>Variable Ǒj SE. Ǒj / t p-Value
Constant 4:1769 0:3790 11:020 0:0000
</p>
<p>eX 1 �0:0146 0:0117 �1:254 0:2105
eX 2 0:0014 0:0056 0:247 0:8051
eX 3 �0:0127 0:0223 �0:570 0:5692
eX 4 0:1100 0:0366 3:002 0:0028
eX 5 �0:2831 0:1053 �2:688 0:0074
eX 6 0:4211 0:1102 3:822 0:0001
eX 7 0:0064 0:0049 1:317 0:1885
eX 8 �0:1832 0:0368 �4:977 0:0000
eX 9 0:0684 0:0225 3:042 0:0025
eX 10 �0:2018 0:0484 �4:167 0:0000
eX 11 �0:0400 0:0081 �4:946 0:0000
eX 12 0:0445 0:0115 3:882 0:0001
eX 13 �0:2626 0:0161 �16:320 0:0000</p>
<p/>
</div>
<div class="page"><p/>
<p>3.8 Exercises 113
</p>
<p>The value of r2 (0.765) and r2adj (0.759) show that most of the variance of X14 is
</p>
<p>explained by the linear model (3.57).
</p>
<p>Again we see that the variations of eX14 are mostly explained by (in decreasing
order of the absolute value of the t-statistic) eX13; eX8; eX11;eX10; eX12; eX6; eX9; eX4
and eX5. The other variables eX1; eX2; eX3 and eX7 seem to have little influence on
the variations of eX14. This will be confirmed by the testing procedures that will be
developed in Chap. 7.
</p>
<p>3.8 Exercises
</p>
<p>Exercise 3.1 The covariance sX4X5 between X4 and X5 for the entire bank data
</p>
<p>set is positive. Given the definitions of X4 and X5, we would expect a negative
</p>
<p>covariance. Using Fig. 3.1 can you explain why sX4X5 is positive?
</p>
<p>Exercise 3.2 Consider the two sub-clouds of counterfeit and genuine bank notes in
</p>
<p>Fig. 3.1 separately. Do you still expect sX4X5 (now calculated separately for each
</p>
<p>cloud) to be positive?
</p>
<p>Exercise 3.3 We remarked that for two normal random variables, zero covariance
</p>
<p>implies independence. Why does this remark not apply to Example 3.4?
</p>
<p>Exercise 3.4 Compute the covariance between the variables
</p>
<p>X2 D miles per gallon,
X8 D weight
</p>
<p>from the car data set (Table 22.3). What sign do you expect the covariance to have?
</p>
<p>Exercise 3.5 Compute the correlation matrix of the variables in Example 3.2.
</p>
<p>Comment on the sign of the correlations and test the hypothesis
</p>
<p>�X1X2 D 0:
</p>
<p>Exercise 3.6 Suppose you have observed a set of observations fxi gniD1 with x D 0,
sXX D 1 and n�1
</p>
<p>Pn
iD1.xi � x/3 D 0. Define the variable yi D x2i . Can you
</p>
<p>immediately tell whether rXY &curren; 0?
</p>
<p>Exercise 3.7 Find formulas (3.29) and (3.30) for Ǫ and Ǒ by differentiating the
objective function in (3.28) w.r.t. ˛ and ˇ.
</p>
<p>Exercise 3.8 Howmany sales does the textile manager expect with a &ldquo;classic blue&rdquo;
</p>
<p>pullover price of x D 105?
Exercise 3.9 What does a scatterplot of two random variables look like for r2 D 1
and r2 D 0?</p>
<p/>
</div>
<div class="page"><p/>
<p>114 3 Moving to Higher Dimensions
</p>
<p>Exercise 3.10 Prove the variance decomposition (3.38) and show that the coeffi-
</p>
<p>cient of determination is the square of the simple correlation between X and Y .
</p>
<p>Exercise 3.11 Make a boxplot for the residuals "i D yi � Ǫ � Ǒxi for the &ldquo;classic
blue&rdquo; pullovers data. If there are outliers, identify them and run the linear regression
</p>
<p>again without them. Do you obtain a stronger influence of price on sales?
</p>
<p>Exercise 3.12 Under what circumstances would you obtain the same coefficients
</p>
<p>from the linear regression lines of Y on X and of X on Y ?
</p>
<p>Exercise 3.13 Treat the design of Example 3.14 as if there were thirty shops and
</p>
<p>not ten. Define xi as the index of the shop, i.e. xi D i; i D 1; 2; : : : ; 30. The
null hypothesis is a constant regression line, EY D �. What does the alternative
regression curve look like?
</p>
<p>Exercise 3.14 Perform the test in Exercise 3.13 for the shop example with a 0:99
</p>
<p>significance level. Do you still reject the hypothesis of equal marketing strategies?
</p>
<p>Exercise 3.15 Compute an approximate confidence interval for �X1X4 in Exam-
</p>
<p>ple 3.2. Hint: start from a confidence interval for tanh�1.�X1X4/ and then apply
the inverse transformation.
</p>
<p>Exercise 3.16 In Example 3.2, using the exchange rate of 1 EUR D 106 JPY,
compute the same empirical covariance using prices in Japanese Yen rather than
</p>
<p>in Euros. Is there a significant difference? Why?
</p>
<p>Exercise 3.17 Why does the correlation have the same sign as the covariance?
</p>
<p>Exercise 3.18 Show that rank.H/ D tr.H/ D n� 1.
Exercise 3.19 Show that X� D HXD�1=2 is the standardised data matrix, i.e.
x� D 0 and SX� D RX .
Exercise 3.20 Compute for the pullovers data the regression of X1 on X2; X3 and
</p>
<p>of X1 on X2; X4. Which one has the better coefficient of determination?
</p>
<p>Exercise 3.21 Compare for the pullovers data the coefficient of determination for
</p>
<p>the regression of X1 on X2 (Example 3.11), of X1 on X2; X3 (Exercise 3.20) and of
</p>
<p>X1 on X2; X3; X4 (Example 3.15). Observe that this coefficient is increasing with
</p>
<p>the number of predictor variables. Is this always the case?
</p>
<p>Exercise 3.22 Consider the ANOVA problem (Sect. 3.5) again. Establish the con-
</p>
<p>straint Matrix A for testing �1 D �2. Test this hypothesis via an analog of (3.55)
and (3.56).
</p>
<p>Exercise 3.23 Prove (3.52). (Hint, let f .ˇ/ D .y � xˇ/&gt;.y � xˇ/ and solve
@f .ˇ/
</p>
<p>@ˇ
D 0.)
</p>
<p>Exercise 3.24 Consider the linear model Y D Xˇ C " where Ǒ D argmin
ˇ
"&gt;" is
</p>
<p>subject to the linear constraints A Ǒ D a where A.q �p/; .q�p/ is of rank q and</p>
<p/>
</div>
<div class="page"><p/>
<p>3.8 Exercises 115
</p>
<p>a is of dimension .q � 1/. Show that Ǒ D ǑOLS � .X&gt;X /�1A&gt;
�
A.X&gt;X /�1A&gt;
</p>
<p>��1
�
A ǑOLS � a
</p>
<p>�
where ǑOLS D .X&gt;X /�1X&gt;y. (Hint, let f .ˇ; �/ D .y � xˇ/&gt;.y �
</p>
<p>xˇ/ � �&gt;.Aˇ � a/ where � 2 Rq and solve @f .ˇ;�/
@ˇ
D 0 and @f .ˇ;�/
</p>
<p>@�
D 0.)
</p>
<p>Exercise 3.25 Compute the covariance matrix S D Cov.X / where X denotes the
matrix of observations on the counterfeit bank notes. Make a Jordan decomposition
</p>
<p>of S. Why are all of the eigenvalues positive?
</p>
<p>Exercise 3.26 Compute the covariance of the counterfeit notes after they are
</p>
<p>linearly transformed by the vector a D .1; 1; 1; 1; 1; 1/&gt;.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 4
</p>
<p>Multivariate Distributions
</p>
<p>The preceding chapter showed that by using the two first moments of a multivariate
</p>
<p>distribution (the mean and the covariance matrix), a lot of information on the
</p>
<p>relationship between the variables can be made available. Only basic statistical
</p>
<p>theory was used to derive tests of independence or of linear relationships. In this
</p>
<p>chapter we give an introduction to the basic probability tools useful in statistical
</p>
<p>multivariate analysis.
</p>
<p>Means and covariances share many interesting and useful properties, but they
</p>
<p>represent only part of the information on a multivariate distribution. Section 4.1
</p>
<p>presents the basic probability tools used to describe a multivariate random variable,
</p>
<p>including marginal and conditional distributions and the concept of independence.
</p>
<p>In Sect. 4.2, basic properties on means and covariances (marginal and conditional
</p>
<p>ones) are derived.
</p>
<p>Since many statistical procedures rely on transformations of a multivariate
</p>
<p>random variable, Sect. 4.3 proposes the basic techniques needed to derive the
</p>
<p>distribution of transformations with a special emphasis on linear transforms. As
</p>
<p>an important example of a multivariate random variable, Sect. 4.4 defines the
</p>
<p>multinormal distribution. It will be analysed in more detail in Chap. 5 along
</p>
<p>with most of its &ldquo;companion&rdquo; distributions that are useful in making multivariate
</p>
<p>statistical inferences.
</p>
<p>The normal distribution plays a central role in statistics because it can be viewed
</p>
<p>as an approximation and limit of many other distributions. The basic justification
</p>
<p>relies on the central limit theorem presented in Sect. 4.5. We present this central
</p>
<p>theorem in the framework of sampling theory. A useful extension of this theorem is
</p>
<p>also given: it is an approximate distribution to transformations of asymptotically
</p>
<p>normal variables. The increasing power of computers today makes it possible
</p>
<p>to consider alternative approximate sampling distributions. These are based on
</p>
<p>resampling techniques and are suitable for many general situations. Section 4.8
</p>
<p>gives an introduction to the ideas behind bootstrap approximations.
</p>
<p>&copy; Springer-Verlag Berlin Heidelberg 2015
W.K. H&auml;rdle, L. Simar, Applied Multivariate Statistical Analysis,
DOI 10.1007/978-3-662-45171-7_4
</p>
<p>117</p>
<p/>
</div>
<div class="page"><p/>
<p>118 4 Multivariate Distributions
</p>
<p>4.1 Distribution and Density Function
</p>
<p>Let X D .X1; X2; : : : ; Xp/&gt; be a random vector. The cumulative distribution
function (cdf) of X is defined by
</p>
<p>F.x/ D P.X � x/ D P.X1 � x1; X2 � x2; : : : ; Xp � xp/:
</p>
<p>For continuousX , a nonnegative probability density function (pdf) f exists that
</p>
<p>F.x/ D
Z x
</p>
<p>�1
f .u/du: (4.1)
</p>
<p>Note that
</p>
<p>Z 1
</p>
<p>�1
f .u/ du D 1:
</p>
<p>Most of the integrals appearing below are multidimensional. For instance,R x
�1 f .u/du means
</p>
<p>R xp
�1 : : :
</p>
<p>R x1
�1 f .u1; : : : ; up/du1 : : : dup: Note also that the cdf
</p>
<p>F is differentiable with
</p>
<p>f .x/ D @
pF.x/
</p>
<p>@x1 � � � @xp
:
</p>
<p>For discrete X , the values of this random variable are concentrated on a countable
</p>
<p>or finite set of points fcj gj2J , the probability of events of the form fX 2 Dg can
then be computed as
</p>
<p>P.X 2 D/ D
X
</p>
<p>fj Wcj2Dg
P.X D cj /:
</p>
<p>If we partition X as X D .X1; X2/&gt; with X1 2 Rk and X2 2 Rp�k , then the
function
</p>
<p>FX1.x1/ D P.X1 � x1/ D F.x11; : : : ; x1k ;1; : : : ;1/ (4.2)
</p>
<p>is called the marginal cdf. F D F.x/ is called the joint cdf. For continuous X
the marginal pdf can be computed from the joint density by &ldquo;integrating out&rdquo; the
</p>
<p>variable not of interest.
</p>
<p>fX1.x1/ D
Z 1
</p>
<p>�1
f .x1; x2/dx2: (4.3)</p>
<p/>
</div>
<div class="page"><p/>
<p>4.1 Distribution and Density Function 119
</p>
<p>The conditional pdf of X2 given X1 D x1 is given as
</p>
<p>f .x2 j x1/ D
f .x1; x2/
</p>
<p>fX1.x1/
� (4.4)
</p>
<p>Example 4.1 Consider the pdf
</p>
<p>f .x1; x2/ D
�
1
2
x1 C 32x2 0 � x1; x2 � 1;
0 otherwise.
</p>
<p>f .x1; x2/ is a density since
</p>
<p>Z
f .x1; x2/dx1dx2 D
</p>
<p>1
</p>
<p>2
</p>
<p>�
x21
2
</p>
<p>�1
</p>
<p>0
</p>
<p>C 3
2
</p>
<p>�
x22
2
</p>
<p>�1
</p>
<p>0
</p>
<p>D 1
4
C 3
4
D 1:
</p>
<p>The marginal densities are
</p>
<p>fX1.x1/ D
Z
f .x1; x2/dx2 D
</p>
<p>Z 1
</p>
<p>0
</p>
<p>�
1
</p>
<p>2
x1 C
</p>
<p>3
</p>
<p>2
x2
</p>
<p>�
dx2 D
</p>
<p>1
</p>
<p>2
x1 C
</p>
<p>3
</p>
<p>4
I
</p>
<p>fX2.x2/ D
Z
f .x1; x2/dx1 D
</p>
<p>Z 1
</p>
<p>0
</p>
<p>�
1
</p>
<p>2
x1 C
</p>
<p>3
</p>
<p>2
x2
</p>
<p>�
dx1 D
</p>
<p>3
</p>
<p>2
x2 C
</p>
<p>1
</p>
<p>4
�
</p>
<p>The conditional densities are therefore
</p>
<p>f .x2 j x1/ D
1
2
x1 C 32x2
1
2
x1 C 34
</p>
<p>and f .x1 j x2/ D
1
2
x1 C 32x2
3
2
x2 C 14
</p>
<p>�
</p>
<p>Note that these conditional pdf&rsquo;s are nonlinear in x1 and x2 although the joint pdf
</p>
<p>has a simple (linear) structure.
</p>
<p>Independence of two random variables is defined as follows.
</p>
<p>Definition 4.1 X1 and X2 are independent iff f .x/ D f .x1; x2/ D
fX1.x1/fX2.x2/.
</p>
<p>That is, X1 and X2 are independent if the conditional pdf&rsquo;s are equal to the
</p>
<p>marginal densities, i.e. f .x1 j x2/ D fX1.x1/ and f .x2 j x1/ D fX2.x2/.
Independence can be interpreted as follows: knowing X2 D x2 does not change the
probability assessments on X1, and conversely.
</p>
<p>!
Different joint pdf&rsquo;s may have the same marginal pdf&rsquo;s.</p>
<p/>
</div>
<div class="page"><p/>
<p>120 4 Multivariate Distributions
</p>
<p>Example 4.2 Consider the pdf&rsquo;s
</p>
<p>f .x1; x2/ D 1; 0 &lt; x1; x2 &lt; 1;
</p>
<p>and
</p>
<p>f .x1; x2/ D 1C ˛.2x1 � 1/.2x2 � 1/; 0 &lt; x1; x2 &lt; 1; �1 � ˛ � 1:
</p>
<p>We compute in both cases the marginal pdf&rsquo;s as
</p>
<p>fX1.x1/ D 1; fX2.x2/ D 1:
</p>
<p>Indeed
</p>
<p>Z 1
</p>
<p>0
</p>
<p>1C ˛.2x1 � 1/.2x2 � 1/dx2 D 1C ˛.2x1 � 1/Œx22 � x2&#141;10 D 1:
</p>
<p>Hence we obtain identical marginals from different joint distributions.
</p>
<p>Let us study the concept of independence using the bank notes example. Consider
</p>
<p>the variablesX4 (lower inner frame) and X5 (upper inner frame). From Chap. 3, we
</p>
<p>already know that they have significant correlation, so they are almost surely not
</p>
<p>independent. Kernel estimates of the marginal densities, OfX4 and OfX5 , are given in
Fig. 4.1. In Fig. 4.2 (left) we show the product of these two densities. The kernel
</p>
<p>density technique was presented in Sect. 1.3. If X4 and X5 are independent, this
</p>
<p>product OfX4 � OfX5 should be roughly equal to Of .x4; x5/, the estimate of the joint
density of .X4; X5/. Comparing the two graphs in Fig. 4.2 reveals that the two
</p>
<p>densities are different. The two variablesX4 and X5 are therefore not independent.
</p>
<p>An elegant concept of connecting marginals with joint cdfs is given by copulae.
</p>
<p>Copulae are important in Value-at-Risk calculations and are an essential tool in
</p>
<p>quantitative finance (H&auml;rdle, Hautsch, &amp; Overbeck, 2009).
</p>
<p>For simplicity of presentation we concentrate on the p D 2 dimensional case.
A two-dimensional copula is a function C W Œ0; 1&#141;2 ! Œ0; 1&#141; with the following
properties:
</p>
<p>&bull; For every u 2 Œ0; 1&#141;: C.0; u/ D C.u; 0/ D 0.
&bull; For every u 2 Œ0; 1&#141;: C.u; 1/ D u and C.1; u/ D u.
&bull; For every .u1; u2/; .v1; v2/ 2 Œ0; 1&#141; � Œ0; 1&#141; with u1 � v1 and u2 � v2:
</p>
<p>C.v1; v2/� C.v1; u2/� C.u1; v2/C C.u1; u2/ � 0 :
</p>
<p>The usage of the name &ldquo;copula&rdquo; for the function C is explained by the following
</p>
<p>theorem.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.1 Distribution and Density Function 121
</p>
<p>6 8 10 12
0
</p>
<p>0.1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>Swiss Bank Notes
</p>
<p>Lower Inner Frame (X4)
</p>
<p>D
e
n
</p>
<p>s
it
</p>
<p>y
</p>
<p>6 8 10 12
0
</p>
<p>0.1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>Swiss Bank Notes
</p>
<p>Upper Inner Frame (X5)
</p>
<p>D
e
n
</p>
<p>s
it
</p>
<p>y
</p>
<p>Fig. 4.1 Univariate estimates of the density of X4 (left) and X5 (right) of the bank notes
MVAdenbank2
</p>
<p>Fig. 4.2 The product of univariate density estimates (left) and the joint density estimate (right) for
X4 (left) and X5 of the bank notes MVAdenbank3
</p>
<p>Theorem 4.1 (Sklar&rsquo;s Theorem) Let F be a joint distribution function with
</p>
<p>marginal distribution functions FX1 and FX2 . Then a copula C exists with
</p>
<p>F.x1; x2/ D C fFX1.x1/; FX2.x2/g (4.5)</p>
<p/>
</div>
<div class="page"><p/>
<p>122 4 Multivariate Distributions
</p>
<p>for every x1; x2 2 R. If FX1 and FX2 are continuous, then C is unique. On the other
hand, if C is a copula and FX1 and FX2 are distribution functions, then the function
</p>
<p>F defined by (4.5) is a joint distribution function with marginals FX1 and FX2 .
</p>
<p>With Sklar&rsquo;s Theorem, the use of the name &ldquo;copula&rdquo; becomes obvious. It was
</p>
<p>chosen to describe &ldquo;a function that links a multidimensional distribution to its one-
</p>
<p>dimensional margins&rdquo; and appeared in the mathematical literature for the first time
</p>
<p>in Sklar (1959).
</p>
<p>Example 4.3 The structure of independence implies that the product of the distri-
</p>
<p>bution functions FX1 and FX2 equals their joint distribution function F ,
</p>
<p>F.x1; x2/ D FX1.x1/ � FX2.x2/: (4.6)
</p>
<p>Thus, we obtain the independence copula C D &hellip; from
</p>
<p>&hellip;.u1; : : : ; un/ D
nY
</p>
<p>iD1
ui :
</p>
<p>Theorem 4.2 Let X1 and X2 be random variables with continuous distribution
</p>
<p>functions FX1 and FX2 and the joint distribution function F . Then X1 and X2 are
</p>
<p>independent if and only if CX1;X2 D &hellip;.
Proof From Sklar&rsquo;s Theorem we know that there exists an unique copula C with
</p>
<p>P.X1 � x1; X2 � x2/ D F.x1; x2/ D C fFX1.x1/; FX2.x2/g : (4.7)
</p>
<p>Independence can be seen using (4.5) for the joint distribution function F and the
</p>
<p>definition of&hellip;,
</p>
<p>F.x1; x2/ D C fFX1.x1/; FX2.x2/g D FX1.x1/FX2.x2/ : (4.8)
</p>
<p>ut
Example 4.4 The Gumbel&ndash;Hougaard family of copulae (Nelsen, 1999) is given by
</p>
<p>the function
</p>
<p>C� .u; v/ D exp
h
�
˚
.� log u/� C .� log v/�
</p>
<p>�1=�i
: (4.9)
</p>
<p>The parameter � may take all values in the interval Œ1;1/. The Gumbel&ndash;Hougaard
copulae are suited to describe bivariate extreme value distributions.
</p>
<p>For � D 1, the expression (4.9) reduces to the product copula, i.e. C1.u; v/ D
&hellip;.u; v/ D u v. For � !1 one finds for the Gumbel&ndash;Hougaard copula:
</p>
<p>C� .u; v/�!min.u; v/ DM.u; v/;</p>
<p/>
</div>
<div class="page"><p/>
<p>4.2 Moments and Characteristic Functions 123
</p>
<p>where the function M is also a copula such that C.u; v/ � M.u; v/ for arbitrary
copula C . The copulaM is called the Fr&eacute;chet&ndash;Hoeffding upper bound.
</p>
<p>Similarly, we obtain the Fr&eacute;chet&ndash;Hoeffding lower bound W.u; v/ D max.u C
v � 1; 0/ which satisfiesW.u; v/ � C.u; v/ for any other copula C .
</p>
<p>Summary
</p>
<p>,! The cumulative distribution function (cdf) is defined as F.x/ D
P.X &lt; x/.
</p>
<p>,! If a probability density function (pdf) f exists then F.x/ DR x
�1 f .u/du.
</p>
<p>,! The pdf integrates to one, i.e.
R1
�1 f .x/dx D 1.
</p>
<p>,! Let X D .X1; X2/&gt; be partitioned into sub-vectors X1 and
X2 with joint cdf F . Then FX1.x1/ D P.X1 � x1/ is the
marginal cdf of X1. The marginal pdf of X1 is obtained by
</p>
<p>fX1.x1/ D
R1
�1 f .x1; x2/dx2. Different joint pdf&rsquo;s may have the
</p>
<p>same marginal pdf&rsquo;s.
</p>
<p>,! The conditional pdf of X2 given X1 D x1 is defined as f .x2 j
x1/ D f .x1; x2/fX1.x1/
</p>
<p>�
</p>
<p>,! Two random variables X1 and X2 are called independent iff
f .x1; x2/ D fX1.x1/fX2.x2/. This is equivalent to f .x2 j x1/ D
fX2.x2/.
</p>
<p>,! Different joint pdf&rsquo;s may have identical marginal pdf&rsquo;s.
</p>
<p>,! Copula is a function which connects marginals to form joint cdfs.
</p>
<p>4.2 Moments and Characteristic Functions
</p>
<p>Moments: Expectation and Covariance Matrix
</p>
<p>If X is a random vector with density f .x/ then the expectation of X is
</p>
<p>EX D
</p>
<p>0
B@
</p>
<p>EX1
:::
</p>
<p>EXp
</p>
<p>1
CA D
</p>
<p>Z
xf .x/dx D
</p>
<p>0
B@
</p>
<p>R
x1f .x/dx
</p>
<p>:::R
xpf .x/dx
</p>
<p>1
CA D �: (4.10)</p>
<p/>
</div>
<div class="page"><p/>
<p>124 4 Multivariate Distributions
</p>
<p>Accordingly, the expectation of a matrix of random elements has to be understood
</p>
<p>component by component. The operation of forming expectations is linear:
</p>
<p>E .˛X C ˇY / D ˛ EX C ˇ EY: (4.11)
</p>
<p>If A.q � p/ is a matrix of real numbers, we have:
</p>
<p>E.AX/ D AEX: (4.12)
</p>
<p>When X and Y are independent,
</p>
<p>E.XY&gt;/ D EX EY &gt;: (4.13)
</p>
<p>The matrix
</p>
<p>Var.X/ D &dagger; D E.X � �/.X � �/&gt; (4.14)
</p>
<p>is the (theoretical) covariance matrix. We write for a vector X with mean vector �
</p>
<p>and covariance matrix &dagger;,
</p>
<p>X � .�;&dagger;/: (4.15)
</p>
<p>The .p � q/ matrix
</p>
<p>&dagger;XY D Cov.X; Y / D E.X � �/.Y � �/&gt; (4.16)
</p>
<p>is the covariance matrix of X � .�;&dagger;XX/ and Y � .�;&dagger;YY/. Note that &dagger;XY D &dagger;&gt;YX
and that Z D
</p>
<p>�
X
</p>
<p>Y
</p>
<p>�
has covariance&dagger;ZZ D
</p>
<p>�
&dagger;XX
&dagger;YX
</p>
<p>&dagger;XY
&dagger;YY
</p>
<p>�
. From
</p>
<p>Cov.X; Y / D E.XY&gt;/ � ��&gt; D E.XY&gt;/ � EX E Y &gt; (4.17)
</p>
<p>it follows that Cov.X; Y / D 0 in the case where X and Y are independent. We
often say that � D E.X/ is the first order moment of X and that E.XX&gt;/ provides
the second order moments of X :
</p>
<p>E.XX&gt;/ D fE.XiXj /g; for i D 1; : : : ; p and j D 1; : : : ; p: (4.18)
</p>
<p>Properties of the Covariance Matrix &dagger; D Var.X/
</p>
<p>&dagger; D .�XiXj /; �XiXj D Cov.Xi ; Xj /; �XiXi D Var.Xi/ (4.19)
</p>
<p>&dagger; D E.XX&gt;/ � ��&gt; (4.20)
&dagger; � 0 (4.21)</p>
<p/>
</div>
<div class="page"><p/>
<p>4.2 Moments and Characteristic Functions 125
</p>
<p>Properties of Variances and Covariances
</p>
<p>Var.a&gt;X/ D a&gt;Var.X/a D
X
</p>
<p>i;j
</p>
<p>aiaj �XiXj (4.22)
</p>
<p>Var.AX C b/ D AVar.X/A&gt; (4.23)
Cov.X C Y;Z/ D Cov.X;Z/C Cov.Y;Z/ (4.24)
Var.X C Y / D Var.X/C Cov.X; Y /C Cov.Y;X/C Var.Y / (4.25)
Cov.AX;BY / D ACov.X; Y /B&gt;: (4.26)
</p>
<p>Let us compute these quantities for a specific joint density.
</p>
<p>Example 4.5 Consider the pdf of Example 4.1. The mean vector � D
�
�1
�2
</p>
<p>�
is
</p>
<p>�1 D
Z Z
</p>
<p>x1f .x1; x2/dx1dx2 D
Z 1
</p>
<p>0
</p>
<p>Z 1
</p>
<p>0
</p>
<p>x1
</p>
<p>�
1
</p>
<p>2
x1 C
</p>
<p>3
</p>
<p>2
x2
</p>
<p>�
dx1dx2
</p>
<p>D
Z 1
</p>
<p>0
</p>
<p>x1
</p>
<p>�
1
</p>
<p>2
x1 C
</p>
<p>3
</p>
<p>4
</p>
<p>�
dx1 D
</p>
<p>1
</p>
<p>2
</p>
<p>�
x31
3
</p>
<p>�1
</p>
<p>0
</p>
<p>C 3
4
</p>
<p>�
x21
2
</p>
<p>�1
</p>
<p>0
</p>
<p>D 1
6
C 3
8
D 4C 9
</p>
<p>24
D 13
24
;
</p>
<p>�2 D
Z Z
</p>
<p>x2f .x1; x2/dx1dx2 D
Z 1
</p>
<p>0
</p>
<p>Z 1
</p>
<p>0
</p>
<p>x2
</p>
<p>�
1
</p>
<p>2
x1 C
</p>
<p>3
</p>
<p>2
x2
</p>
<p>�
dx1dx2
</p>
<p>D
Z 1
</p>
<p>0
</p>
<p>x2
</p>
<p>�
1
</p>
<p>4
C 3
2
x2
</p>
<p>�
dx2 D
</p>
<p>1
</p>
<p>4
</p>
<p>�
x22
2
</p>
<p>�1
</p>
<p>0
</p>
<p>C 3
2
</p>
<p>�
x32
3
</p>
<p>�1
</p>
<p>0
</p>
<p>D 1
8
C 1
2
D 1C 4
</p>
<p>8
D 5
8
�
</p>
<p>The elements of the covariance matrix are
</p>
<p>�X1X1 D EX21 � �21 with
</p>
<p>EX21 D
Z 1
</p>
<p>0
</p>
<p>Z 1
</p>
<p>0
</p>
<p>x21
</p>
<p>�
1
</p>
<p>2
x1 C
</p>
<p>3
</p>
<p>2
x2
</p>
<p>�
dx1dx2 D
</p>
<p>1
</p>
<p>2
</p>
<p>�
x41
4
</p>
<p>�1
</p>
<p>0
</p>
<p>C 3
4
</p>
<p>�
x31
3
</p>
<p>�1
</p>
<p>0
</p>
<p>D 3
8
</p>
<p>�X2X2 D EX22 � �22 with
</p>
<p>EX22 D
Z 1
</p>
<p>0
</p>
<p>Z 1
</p>
<p>0
</p>
<p>x22
</p>
<p>�
1
</p>
<p>2
x1 C
</p>
<p>3
</p>
<p>2
x2
</p>
<p>�
dx1dx2 D
</p>
<p>1
</p>
<p>4
</p>
<p>�
x32
3
</p>
<p>�1
</p>
<p>0
</p>
<p>C 3
2
</p>
<p>�
x42
4
</p>
<p>�1
</p>
<p>0
</p>
<p>D 11
24
</p>
<p>�X1X2 D E.X1X2/ � �1�2 with</p>
<p/>
</div>
<div class="page"><p/>
<p>126 4 Multivariate Distributions
</p>
<p>E.X1X2/ D
Z 1
</p>
<p>0
</p>
<p>Z 1
</p>
<p>0
</p>
<p>x1x2
</p>
<p>�
1
</p>
<p>2
x1 C
</p>
<p>3
</p>
<p>2
x2
</p>
<p>�
dx1dx2 D
</p>
<p>Z 1
</p>
<p>0
</p>
<p>�
1
</p>
<p>6
x2 C
</p>
<p>3
</p>
<p>4
x22
</p>
<p>�
dx2
</p>
<p>D 1
6
</p>
<p>�
x22
2
</p>
<p>�1
</p>
<p>0
</p>
<p>C 3
4
</p>
<p>�
x32
3
</p>
<p>�1
</p>
<p>0
</p>
<p>D 1
3
:
</p>
<p>Hence the covariance matrix is
</p>
<p>&dagger; D
�
0:0815 0:0052
</p>
<p>0:0052 0:0677
</p>
<p>�
:
</p>
<p>Conditional Expectations
</p>
<p>The conditional expectations are
</p>
<p>E.X2 j x1/ D
Z
x2f .x2 j x1/ dx2 and E.X1 j x2/ D
</p>
<p>Z
x1f .x1 j x2/ dx1:
</p>
<p>(4.27)
</p>
<p>E.X2jx1/ represents the location parameter of the conditional pdf of X2 given that
X1 D x1. In the same way, we can define Var.X2jX1 D x1/ as a measure of the
dispersion of X2 given that X1 D x1. We have from (4.20) that
</p>
<p>Var.X2jX1 D x1/ D E.X2 X&gt;2 jX1 D x1/ � E.X2jX1 D x1/ E.X&gt;2 jX1 D x1/:
</p>
<p>Using the conditional covariance matrix, the conditional correlations may be
</p>
<p>defined as:
</p>
<p>�X2 X3jX1Dx1 D
Cov.X2; X3jX1 D x1/p
</p>
<p>Var.X2jX1 D x1/ Var.X3jX1 D x1/
:
</p>
<p>These conditional correlations are known as partial correlations betweenX2 andX3,
</p>
<p>conditioned on X1 being equal to x1.
</p>
<p>Example 4.6 Consider the following pdf
</p>
<p>f .x1; x2; x3/ D
2
</p>
<p>3
.x1 C x2 C x3/ where 0 &lt; x1; x2; x3 &lt; 1:</p>
<p/>
</div>
<div class="page"><p/>
<p>4.2 Moments and Characteristic Functions 127
</p>
<p>Note that the pdf is symmetric in x1; x2 and x3 which facilitates the computations.
</p>
<p>For instance,
</p>
<p>f .x1; x2/ D 23 .x1 C x2 C
1
2
/ 0 &lt; x1; x2 &lt; 1
</p>
<p>f .x1/ D 23 .x1 C 1/ 0 &lt; x1 &lt; 1
</p>
<p>and the other marginals are similar. We also have
</p>
<p>f .x1; x2jx3/ D
x1 C x2 C x3
x3 C 1
</p>
<p>; 0 &lt; x1; x2 &lt; 1
</p>
<p>f .x1jx3/ D
x1 C x3 C 12
x3 C 1
</p>
<p>; 0 &lt; x1 &lt; 1:
</p>
<p>It is easy to compute the following moments:
</p>
<p>E.Xi / D 59 I E.X2i / D
7
18
I E.XiXj / D 1136 .i 6D j and i; j D 1; 2; 3/
</p>
<p>E.X1jX3 D x3/ D E.X2jX3 D x3/ D 112
�
6x3C7
x3C1
</p>
<p>�
I
</p>
<p>E.X21 jX3 D x3/ D E.X22 jX3 D x3/ D 112
�
4x3C5
x3C1
</p>
<p>�
</p>
<p>and
</p>
<p>E.X1X2jX3 D x3/ D 112
�
3x3C4
x3C1
</p>
<p>�
:
</p>
<p>Note that the conditional means of X1 and of X2, given X3 D x3, are not linear
in x3. From these moments we obtain:
</p>
<p>&dagger; D
</p>
<p>0
@
</p>
<p>13
162
� 1
324
� 1
324
</p>
<p>� 1
324
</p>
<p>13
162
� 1
324
</p>
<p>� 1
324
� 1
324
</p>
<p>13
162
</p>
<p>1
A in particular �X1X2 D �
</p>
<p>1
</p>
<p>26
� �0:0385:
</p>
<p>The conditional covariance matrix of X1 and X2, given X3 D x3 is
</p>
<p>Var
</p>
<p>  
X1
</p>
<p>X2
</p>
<p>!
j X3 D x3
</p>
<p>!
D
</p>
<p>0
@
</p>
<p>12x23C24x3C11
144.x3C1/2
</p>
<p>�1
144.x3C1/2
</p>
<p>�1
144.x3C1/2
</p>
<p>12x23C24x3C11
144.x3C1/2
</p>
<p>1
A :
</p>
<p>In particular, the partial correlation between X1 and X2, given that X3 is fixed at x3,
</p>
<p>is given by �X1X2jX3Dx3 D � 112x23C24x3C11 which ranges from �0:0909 to �0:0213
when x3 goes from 0 to 1. Therefore, in this example, the partial correlation may be
</p>
<p>larger or smaller than the simple correlation, depending on the value of the condition
</p>
<p>X3 D x3.</p>
<p/>
</div>
<div class="page"><p/>
<p>128 4 Multivariate Distributions
</p>
<p>Example 4.7 Consider the following joint pdf
</p>
<p>f .x1; x2; x3/ D 2x2.x1 C x3/I 0 &lt; x1; x2; x3 &lt; 1:
</p>
<p>Note the symmetry of x1 and x3 in the pdf and that X2 is independent of .X1; X3/.
</p>
<p>It immediately follows that
</p>
<p>f .x1; x3/ D .x1 C x3/ 0 &lt; x1; x3 &lt; 1
</p>
<p>f .x1/ D x1 C
1
</p>
<p>2
I
</p>
<p>f .x2/ D 2x2I
</p>
<p>f .x3/ D x3 C
1
</p>
<p>2
:
</p>
<p>Simple computations lead to
</p>
<p>E.X/ D
</p>
<p>0
BBB@
</p>
<p>7
12
</p>
<p>2
3
</p>
<p>7
12
</p>
<p>1
CCCA and &dagger; D
</p>
<p>0
@
</p>
<p>11
144
</p>
<p>0 � 1
144
</p>
<p>0 1
18
</p>
<p>0
</p>
<p>� 1
144
</p>
<p>0 11
144
</p>
<p>1
A :
</p>
<p>Let us analyse the conditional distribution of .X1; X2/ given X3 D x3. We have
</p>
<p>f .x1; x2jx3/ D
4.x1 C x3/x2
2x3 C 1
</p>
<p>0 &lt; x1; x2 &lt; 1
</p>
<p>f .x1jx3/ D 2
�
x1 C x3
2x3 C 1
</p>
<p>�
0 &lt; x1 &lt; 1
</p>
<p>f .x2jx3/ D f .x2/ D 2x2 0 &lt; x2 &lt; 1
</p>
<p>so that again X1 and X2 are independent conditional on X3 D x3. In this case
</p>
<p>E
</p>
<p>  
X1
</p>
<p>X2
</p>
<p>!
jX3 D x3
</p>
<p>!
D
 
1
3
</p>
<p>�
2C3x3
1C2x3
</p>
<p>�
</p>
<p>2
3
</p>
<p>!
</p>
<p>Var
</p>
<p>  
X1
</p>
<p>X2
</p>
<p>!
jX3 D x3
</p>
<p>!
D
 
</p>
<p>1
18
</p>
<p>�
6x23C6x3C1
.2x3C1/2
</p>
<p>�
0
</p>
<p>0 1
18
</p>
<p>!
:</p>
<p/>
</div>
<div class="page"><p/>
<p>4.2 Moments and Characteristic Functions 129
</p>
<p>Properties of Conditional Expectations
</p>
<p>Since E.X2jX1 D x1/ is a function of x1, say h.x1/, we can define the random
variable h.X1/ D E.X2jX1/. The same can be done when defining the random
variableVar.X2jX1/. These two random variables share some interesting properties:
</p>
<p>E.X2/ D EfE.X2jX1/g (4.28)
Var.X2/ D EfVar.X2jX1/g C VarfE.X2jX1/g: (4.29)
</p>
<p>Example 4.8 Consider the following pdf
</p>
<p>f .x1; x2/ D 2e�
x2
x1 I 0 &lt; x1 &lt; 1; x2 &gt; 0:
</p>
<p>It is easy to show that
</p>
<p>f .x1/ D 2x1 for 0 &lt; x1 &lt; 1I E.X1/ D
2
</p>
<p>3
and Var.X1/ D
</p>
<p>1
</p>
<p>18
</p>
<p>f .x2jx1/ D
1
</p>
<p>x1
e
� x2x1 for x2 &gt; 0I E.X2jX1/ D X1 and Var.X2jX1/ D X21 :
</p>
<p>Without explicitly computing f .x2/, we can obtain:
</p>
<p>E.X2/ D E fE.X2jX1/g D E.X1/ D
2
</p>
<p>3
</p>
<p>Var.X2/ D E fVar.X2jX1/g C Var fE.X2jX1/g
</p>
<p>D E.X21 /C Var.X1/ D
2
</p>
<p>4
C 1
18
D 10
18
:
</p>
<p>The conditional expectation E.X2jX1/ viewed as a function h.X1/ ofX1 (known
as the regression function of X2 on X1), can be interpreted as a conditional
</p>
<p>approximation of X2 by a function of X1. The error term of the approximation is
</p>
<p>then given by:
</p>
<p>U D X2 � E.X2jX1/:
</p>
<p>Theorem 4.3 Let X1 2 Rk and X2 2 Rp�k and U D X2 � E.X2jX1/. Then we
have:
</p>
<p>1. E.U / D 0
2. E.X2jX1/ is the best approximation of X2 by a function h.X1/ of X1 where h W
</p>
<p>R
k �! Rp�k . &ldquo;Best&rdquo; is the minimum mean squared error (MSE) sense, where
</p>
<p>MSE.h/ D EŒfX2 � h.X1/g&gt; fX2 � h.X1/g&#141;:</p>
<p/>
</div>
<div class="page"><p/>
<p>130 4 Multivariate Distributions
</p>
<p>Characteristic Functions
</p>
<p>The characteristic function (cf) of a random vectorX 2 Rp (respectively its density
f .x/) is defined as
</p>
<p>'X.t/ D E.eit
&gt;X / D
</p>
<p>Z
eit
</p>
<p>&gt;xf .x/ dx; t 2 Rp;
</p>
<p>where i is the complex unit: i2 D �1. The cf has the following properties:
</p>
<p>'X .0/ D 1 and j'X.t/j � 1: (4.30)
</p>
<p>If ' is absolutely integrable, i.e. the integral
R1
�1 j'.x/jdx exists and is finite, then
</p>
<p>f .x/ D 1
.2�/p
</p>
<p>Z 1
</p>
<p>�1
e�it
</p>
<p>&gt;x'X .t/ dt: (4.31)
</p>
<p>If X D .X1; X2; : : : ; Xp/&gt;, then for t D .t1; t2; : : : ; tp/&gt;
</p>
<p>'X1.t1/ D 'X .t1; 0; : : : ; 0/; : : : ; 'Xp .tp/ D 'X .0; : : : ; 0; tp/: (4.32)
</p>
<p>If X1; : : : ; Xp are independent random variables, then for t D .t1; t2; : : : ; tp/&gt;
</p>
<p>'X.t/ D 'X1.t1/� : : : � 'Xp.tp/: (4.33)
</p>
<p>If X1; : : : ; Xp are independent random variables, then for t 2 R
</p>
<p>'X1C���CXp .t/ D 'X1.t/� : : : � 'Xp .t/: (4.34)
</p>
<p>The characteristic function can recover all the cross-product moments of any order:
</p>
<p>8jk � 0; k D 1; : : : ; p and for t D .t1; : : : ; tp/&gt; we have
</p>
<p>E
�
X
j1
1 � : : : �X
</p>
<p>jp
p
</p>
<p>�
D 1
</p>
<p>ij1C���Cjp
</p>
<p>"
@'X .t/
</p>
<p>@t
j1
1 : : : @t
</p>
<p>jp
p
</p>
<p>#
</p>
<p>tD0
: (4.35)
</p>
<p>Example 4.9 The cf of the density in Example 4.5 is given by
</p>
<p>'X .t/ D
Z 1
</p>
<p>0
</p>
<p>Z 1
</p>
<p>0
</p>
<p>eit
&gt;xf .x/dx
</p>
<p>D
Z 1
</p>
<p>0
</p>
<p>Z 1
</p>
<p>0
</p>
<p>fcos.t1x1 C t2x2/C i sin.t1x1 C t2x2/g
�
1
</p>
<p>2
x1 C
</p>
<p>3
</p>
<p>2
x2
</p>
<p>�
dx1dx2;</p>
<p/>
</div>
<div class="page"><p/>
<p>4.2 Moments and Characteristic Functions 131
</p>
<p>D
0:5 ei t1
</p>
<p>�
3 i t1 � 3 i ei t2 t1 C i t2 � i ei t2 t2 C t1 t2 � 4 ei t2 t1 t2
</p>
<p>�
</p>
<p>t12 t22
</p>
<p>�
0:5
</p>
<p>�
3 i t1 � 3 i ei t2 t1 C i t2 � i ei t2 t2 � 3 ei t2 t1 t2
</p>
<p>�
</p>
<p>t12 t22
:
</p>
<p>Example 4.10 Suppose X 2 R1 follows the density of the standard normal
distribution
</p>
<p>fX .x/ D
1p
2�
</p>
<p>exp
</p>
<p>�
�x
</p>
<p>2
</p>
<p>2
</p>
<p>�
</p>
<p>(see Sect. 4.4) then the cf can be computed via
</p>
<p>'X.t/ D
1p
2�
</p>
<p>Z 1
</p>
<p>�1
eitx exp
</p>
<p>�
�x
</p>
<p>2
</p>
<p>2
</p>
<p>�
dx
</p>
<p>D 1p
2�
</p>
<p>Z 1
</p>
<p>�1
exp
</p>
<p>�
�1
2
.x2 � 2itx C i2t2/
</p>
<p>�
exp
</p>
<p>�
1
</p>
<p>2
i2t2
</p>
<p>�
dx
</p>
<p>D exp
�
� t
</p>
<p>2
</p>
<p>2
</p>
<p>� Z 1
</p>
<p>�1
</p>
<p>1p
2�
</p>
<p>exp
</p>
<p>�
� .x � it/
</p>
<p>2
</p>
<p>2
</p>
<p>�
dx
</p>
<p>D exp
�
� t
</p>
<p>2
</p>
<p>2
</p>
<p>�
;
</p>
<p>since i2 D �1 and
R
</p>
<p>1p
2�
</p>
<p>exp
n
� .x�it /2
</p>
<p>2
</p>
<p>o
dx D 1.
</p>
<p>A variety of distributional characteristics can be computed from 'X.t/. The
</p>
<p>standard normal distribution has a very simple cf, as was seen in Example 4.10.
</p>
<p>Deviations from normal covariance structures can be measured by the deviations
</p>
<p>from the cf (or characteristics of it). In Table 4.1 we give an overview of the cf&rsquo;s for
</p>
<p>a variety of distributions.
</p>
<p>Theorem 4.4 (Cramer&ndash;Wold) The distribution of X 2 Rp is completely deter-
mined by the set of all (one-dimensional) distributions of t&gt;X where t 2 Rp .
</p>
<p>Table 4.1 Characteristic functions for some common distributions
</p>
<p>pdf cf
</p>
<p>Uniform f .x/D I.x 2 Œa; b&#141;/=.b � a/ 'X .t / D .eibt � eiat /=.b � a/it
N1.�; �
</p>
<p>2/ f .x/D .2��2/�1=2expf�.x � �/2=2�2g 'X .t / D ei�t��2 t2=2
�2.n/ f .x/D I.x &gt; 0/xn=2�1e�x=2=f&#128;.n=2/2n=2g 'X .t / D .1� 2it /�n=2
Np.�;&dagger;/ f .x/D j2�&dagger;j�1=2expf�.x � �/&gt;&dagger;.x � �/=2g 'X .t / D eit&gt;��t&gt;&dagger;t=2</p>
<p/>
</div>
<div class="page"><p/>
<p>132 4 Multivariate Distributions
</p>
<p>This theorem says that we can determine the distribution of X in Rp by
</p>
<p>specifying all of the one-dimensional distributions of the linear combinations
</p>
<p>pX
</p>
<p>jD1
tjXj D t&gt;X; t D .t1; t2; : : : ; tp/&gt;:
</p>
<p>Cumulant Functions
</p>
<p>Moments mk D
R
xkf .x/dx often help in describing distributional characteristics.
</p>
<p>The normal distribution in d D 1 dimension is completely characterised by its
standard normal density f D ' and the moment parameters are � D m1 and
�2 D m2 � m21 . Another helpful class of parameters are the cumulants or semi-
invariants of a distribution. In order to simplify notation we concentrate here on the
</p>
<p>one-dimensional (d D 1) case.
For a given one-dimensional random variable X with density f and finite
</p>
<p>moments of order k the characteristic function 'X.t/ D E.eitX / has the derivative
</p>
<p>1
</p>
<p>ij
</p>
<p>�
@j log f'X.t/g
</p>
<p>@tj
</p>
<p>�
</p>
<p>tD0
D �j ; j D 1; : : : ; k:
</p>
<p>The values �j are called cumulants or semi-invariants since �j does not change
</p>
<p>(for j &gt; 1) under a shift transformation X 7! X C a. The cumulants are natural
parameters for dimension reduction methods, in particular the Projection Pursuit
</p>
<p>method (see Sect. 20.2).
</p>
<p>The relationship between the first k moments m1; : : : ; mk and the cumulants is
</p>
<p>given by
</p>
<p>�k D .�1/k�1
</p>
<p>ˇ̌
ˇ̌
ˇ̌
ˇ̌
ˇ̌
ˇ̌
ˇ̌
</p>
<p>m1 1 : : : 0
</p>
<p>m2
</p>
<p>�
1
</p>
<p>0
</p>
<p>�
m1 : : :
</p>
<p>:::
:::
</p>
<p>: : :
:::
</p>
<p>mk
</p>
<p>�
k � 1
0
</p>
<p>�
mk�1 : : :
</p>
<p>�
k � 1
k � 2
</p>
<p>�
m1
</p>
<p>ˇ̌
ˇ̌
ˇ̌
ˇ̌
ˇ̌
ˇ̌
ˇ̌
</p>
<p>: (4.36)
</p>
<p>Example 4.11 Suppose that k D 1, then formula (4.36) above yields
</p>
<p>�1 D m1:</p>
<p/>
</div>
<div class="page"><p/>
<p>4.2 Moments and Characteristic Functions 133
</p>
<p>For k D 2 we obtain
</p>
<p>�2 D �
</p>
<p>ˇ̌
ˇ̌
ˇ̌
m1 1
</p>
<p>m2
</p>
<p>�
1
</p>
<p>0
</p>
<p>�
m1
</p>
<p>ˇ̌
ˇ̌
ˇ̌ D m2 �m21:
</p>
<p>For k D 3 we have to calculate
</p>
<p>�3 D
</p>
<p>ˇ̌
ˇ̌
ˇ̌
m1 1 0
</p>
<p>m2 m1 1
</p>
<p>m3 m2 2m1
</p>
<p>ˇ̌
ˇ̌
ˇ̌ :
</p>
<p>Calculating the determinant we have:
</p>
<p>�3 D m1
ˇ̌
ˇ̌m1 1
m2 2m1
</p>
<p>ˇ̌
ˇ̌�m2
</p>
<p>ˇ̌
ˇ̌ 1 0
m2 2m1
</p>
<p>ˇ̌
ˇ̌Cm3
</p>
<p>ˇ̌
ˇ̌ 1 0
m1 1
</p>
<p>ˇ̌
ˇ̌
</p>
<p>D m1.2m21 �m2/ �m2.2m1/Cm3
D m3 � 3m1m2 C 2m31: (4.37)
</p>
<p>Similarly one calculates
</p>
<p>�4 D m4 � 4m3m1 � 3m22 C 12m2m21 � 6m41: (4.38)
</p>
<p>The same type of process is used to find the moments from the cumulants:
</p>
<p>m1 D �1
m2 D �2 C �21
m3 D �3 C 3�2�1 C �31
m4 D �4 C 4�3�1 C 3�22 C 6�2�21 C �41 : (4.39)
</p>
<p>A very simple relationship can be observed between the semi-invariants and the
</p>
<p>central moments �k D E.X � �/k , where � D m1 as defined before. In fact,
�2 D �2, �3 D �3 and �4 D �4 � 3�22.
</p>
<p>Skewness &#13;3 and kurtosis &#13;4 are defined as:
</p>
<p>&#13;3 D E.X � �/3=�3
</p>
<p>&#13;4 D E.X � �/4=�4: (4.40)</p>
<p/>
</div>
<div class="page"><p/>
<p>134 4 Multivariate Distributions
</p>
<p>The skewness and kurtosis determine the shape of one-dimensional distributions.
</p>
<p>The skewness of a normal distribution is 0 and the kurtosis equals 3. The relation of
</p>
<p>these parameters to the cumulants is given by:
</p>
<p>&#13;3 D
�3
</p>
<p>�
3=2
2
</p>
<p>(4.41)
</p>
<p>From (4.39) and Example 4.11
</p>
<p>&#13;4 D
�4 C 3�22 C �41 �m41
</p>
<p>�4
D �4 C 3�
</p>
<p>2
2
</p>
<p>�22
D �4
�22
C 3: (4.42)
</p>
<p>These relations will be used later in Sect. 20.2 on Projection Pursuit to determine
</p>
<p>deviations from normality.
</p>
<p>Summary
</p>
<p>,! The expectation of a random vector X is � D
R
xf.x/ dx, the
</p>
<p>covariance matrix&dagger; D Var.X/ D E.X ��/.X ��/&gt;. We denote
X � .�;&dagger;/.
</p>
<p>,! Expectations are linear, i.e. E.˛X C ˇY / D ˛ EX C ˇ EY . If X
and Y are independent, then E.XY&gt;/ D EX EY &gt;.
</p>
<p>,! The covariance between two random vectors X and Y is &dagger;XY D
Cov.X; Y / D E.X � EX/.Y � EY /&gt; D E.XY&gt;/ � EX E Y &gt;.
If X and Y are independent, then Cov.X; Y / D 0.
</p>
<p>,! The characteristic function (cf) of a random vector X is 'X .t/ D
E.eit
</p>
<p>&gt;X /.
</p>
<p>,! The distribution of a p-dimensional random variable X is com-
pletely determined by all one-dimensional distributions of t&gt;X
where t 2 Rp (Theorem of Cramer&ndash;Wold).
</p>
<p>,! The conditional expectation E.X2jX1/ is the MSE best approxima-
tion of X2 by a function of X1.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.3 Transformations 135
</p>
<p>4.3 Transformations
</p>
<p>Suppose that X has pdf fX .x/. What is the pdf of Y D 3X? Or if X D
.X1; X2; X3/
</p>
<p>&gt;, what is the pdf of
</p>
<p>Y D
</p>
<p>0
@
</p>
<p>3X1
X1 � 4X2
</p>
<p>X3
</p>
<p>1
A&lsaquo;
</p>
<p>This is a special case of asking for the pdf of Y when
</p>
<p>X D u.Y / (4.43)
</p>
<p>for a one-to-one transformation u: Rp ! Rp . Define the Jacobian of u as
</p>
<p>J D
�
@xi
</p>
<p>@yj
</p>
<p>�
D
�
@ui .y/
</p>
<p>@yj
</p>
<p>�
</p>
<p>and let abs.jJ j/ be the absolute value of the determinant of this Jacobian. The pdf
of Y is given by
</p>
<p>fY .y/ D abs.jJ j/ � fX fu.y/g: (4.44)
</p>
<p>Using this we can answer the introductory questions, namely
</p>
<p>.x1; : : : ; xp/
&gt; D u.y1; : : : ; yp/ D
</p>
<p>1
</p>
<p>3
.y1; : : : ; yp/
</p>
<p>&gt;
</p>
<p>with
</p>
<p>J D
</p>
<p>0
B@
</p>
<p>1
3
</p>
<p>0
</p>
<p>: : :
</p>
<p>0 1
3
</p>
<p>1
CA
</p>
<p>and hence abs.jJ j/ D
�
1
3
</p>
<p>�p
. So the pdf of Y is
</p>
<p>1
</p>
<p>3p
fX
</p>
<p>�y
3
</p>
<p>�
.
</p>
<p>This introductory example is a special case of
</p>
<p>Y D AX C b; whereA is nonsingular.
</p>
<p>The inverse transformation is
</p>
<p>X D A�1.Y � b/:</p>
<p/>
</div>
<div class="page"><p/>
<p>136 4 Multivariate Distributions
</p>
<p>Therefore
</p>
<p>J D A�1;
</p>
<p>and hence
</p>
<p>fY .y/ D abs.jAj�1/fX fA�1.y � b/g: (4.45)
</p>
<p>Example 4.12 Consider X D .X1; X2/ 2 R2 with density fX .x/ D fX .x1; x2/,
</p>
<p>A D
�
1 1
</p>
<p>1 �1
</p>
<p>�
; b D
</p>
<p>�
0
</p>
<p>0
</p>
<p>�
:
</p>
<p>Then
</p>
<p>Y D AX C b D
�
X1 CX2
X1 �X2
</p>
<p>�
</p>
<p>and
</p>
<p>jAj D �2; abs.jAj�1/ D 1
2
; A�1 D �1
</p>
<p>2
</p>
<p>�
�1 �1
�1 1
</p>
<p>�
:
</p>
<p>Hence
</p>
<p>fY .y/ D abs.jAj�1/ � fX .A�1y/
</p>
<p>D 1
2
fX
</p>
<p>�
1
</p>
<p>2
</p>
<p>�
1 1
</p>
<p>1 �1
</p>
<p>��
y1
</p>
<p>y2
</p>
<p>��
</p>
<p>D 1
2
fX
</p>
<p>�
1
</p>
<p>2
.y1 C y2/;
</p>
<p>1
</p>
<p>2
.y1 � y2/
</p>
<p>�
: (4.46)
</p>
<p>Example 4.13 Consider X 2 R1 with density fX .x/ and Y D exp.X/. According
to (4.43) x D u.y/ D log.y/ and hence the Jacobian is
</p>
<p>J D dx
dy
D 1
y
:
</p>
<p>The pdf of Y is therefore:
</p>
<p>fY .y/ D
1
</p>
<p>y
fX flog.y/g:</p>
<p/>
</div>
<div class="page"><p/>
<p>4.4 The Multinormal Distribution 137
</p>
<p>Summary
</p>
<p>,! If X has pdf fX .x/, then a transformed random vector Y , i.e. X D
u.Y /, has pdf fY .y/ D abs.jJ j/ � fX fu.y/g, where J denotes the
Jacobian J D
</p>
<p>�
@u.yi /
</p>
<p>@yj
</p>
<p>�
.
</p>
<p>,! In the case of a linear relation Y D AX C b the pdf&rsquo;s of X and Y
are related via fY .y/ D abs.jAj�1/fX fA�1.y � b/g.
</p>
<p>4.4 The Multinormal Distribution
</p>
<p>The multinormal distribution with mean � and covariance&dagger; &gt; 0 has the density
</p>
<p>f .x/ D j2�&dagger;j�1=2 exp
�
�1
2
.x � �/&gt;&dagger;�1.x � �/
</p>
<p>�
: (4.47)
</p>
<p>We write X � Np.�;&dagger;/.
How is this multinormal distribution with mean � and covariance &dagger; related to
</p>
<p>the multivariate standard normal Np.0; Ip/? Through a linear transformation using
</p>
<p>the results of Sect. 4.3, as shown in the next theorem.
</p>
<p>Theorem 4.5 Let X �Np.�;&dagger;/ and Y D &dagger;�1=2.X � �/ (Mahalanobis transfor-
mation). Then
</p>
<p>Y � Np.0; Ip/;
</p>
<p>i.e. the elements Yj 2 R are independent, one-dimensionalN.0; 1/ variables.
Proof Note that .X � �/&gt;&dagger;�1.X � �/ D Y &gt;Y . Application of (4.45) gives J D
&dagger;1=2, hence
</p>
<p>fY .y/ D .2�/�p=2 exp
�
�1
2
y&gt;y
</p>
<p>�
(4.48)
</p>
<p>which is by (4.47) the pdf of a Np.0; Ip/. ut</p>
<p/>
</div>
<div class="page"><p/>
<p>138 4 Multivariate Distributions
</p>
<p>Note that the above Mahalanobis transformation yields in fact a random variable
</p>
<p>Y D .Y1; : : : ; Yp/&gt; composed of independent one-dimensionalYj � N1.0; 1/ since
</p>
<p>fY .y/ D
1
</p>
<p>.2�/p=2
exp
</p>
<p>�
�1
2
y&gt;y
</p>
<p>�
</p>
<p>D
pY
</p>
<p>jD1
</p>
<p>1p
2�
</p>
<p>exp
</p>
<p>�
�1
2
y2j
</p>
<p>�
</p>
<p>D
pY
</p>
<p>jD1
fYj .yj /:
</p>
<p>Here each fYj .y/ is a standard normal density
1p
2�
</p>
<p>exp
�
� y2
</p>
<p>2
</p>
<p>�
. From this it is clear
</p>
<p>that E.Y / D 0 and Var.Y / D Ip .
How can we create Np.�;&dagger;/ variables on the basis of Np.0; Ip/ variables? We
</p>
<p>use the inverse linear transformation
</p>
<p>X D &dagger;1=2Y C �: (4.49)
</p>
<p>Using (4.11) and (4.23) we can also check that E.X/ D � and Var.X/ D &dagger;. The
following theorem is useful because it presents the distribution of a variable after it
</p>
<p>has been linearly transformed. The proof is left as an exercise.
</p>
<p>Theorem 4.6 Let X � Np.�;&dagger;/ andA.p � p/; c 2 Rp , where A is nonsingular.
Then Y D AX C c is again a p-variate Normal, i.e.
</p>
<p>Y � Np.A�C c;A&dagger;A&gt;/: (4.50)
</p>
<p>Geometry of the Np.�;&dagger;/ Distribution
</p>
<p>From (4.47) we see that the density of the Np.�;&dagger;/ distribution is constant on
</p>
<p>ellipsoids of the form
</p>
<p>.x � �/&gt;&dagger;�1.x � �/ D d 2: (4.51)
</p>
<p>Example 4.14 Figure 4.3 shows the contour ellipses of a two-dimensional normal
</p>
<p>distribution. Note that these contour ellipses are the iso-distance curves (2.34) from
</p>
<p>the mean of this normal distribution corresponding to the metric &dagger;�1.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.4 The Multinormal Distribution 139
</p>
<p>0 2 4 6
-4
</p>
<p>-2
</p>
<p>0
</p>
<p>2
</p>
<p>4
</p>
<p>6
</p>
<p>8
</p>
<p>Normal sample
</p>
<p>X1
</p>
<p>X
2
</p>
<p>Contour Ellipses
</p>
<p>X
2
</p>
<p>X1
</p>
<p>-3
</p>
<p>-2
</p>
<p>-1
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>4
</p>
<p>5
</p>
<p>6
</p>
<p>7
</p>
<p>1 2 3 4 5
</p>
<p>Fig. 4.3 Scatterplot of a normal sample and contour ellipses for � D
�
3
2
</p>
<p>�
and &dagger; D
</p>
<p>�
1
</p>
<p>�1:5
�1:5
4
</p>
<p>�
</p>
<p>MVAcontnorm
</p>
<p>According to Theorem 2.7 in Sect. 2.6 the half-lengths of the axes in the contour
</p>
<p>ellipsoid are
p
d 2�i where �i are the eigenvalues of &dagger;. If &dagger; is a diagonal matrix,
</p>
<p>the rectangle circumscribing the contour ellipse has sides with length 2d�i and is
</p>
<p>thus naturally proportional to the standard deviations of Xi .i D 1; 2/.
The distribution of the quadratic form in (4.51) is given in the next theorem.
</p>
<p>Theorem 4.7 If X � Np.�;&dagger;/, then the variable U D .X ��/&gt;&dagger;�1.X ��/ has
a �2p distribution.
</p>
<p>Theorem 4.8 The characteristic function (cf) of a multinormal Np.�;&dagger;/ is
</p>
<p>given by
</p>
<p>'X.t/ D exp
�
</p>
<p>i t&gt;� � 1
2
t&gt;&dagger;t
</p>
<p>�
: (4.52)
</p>
<p>We can check Theorem 4.8 by transforming the cf back:
</p>
<p>f .x/ D 1
.2�/p
</p>
<p>Z
exp
</p>
<p>�
�it&gt;x C it&gt;� � 1
</p>
<p>2
t&gt;&dagger;t
</p>
<p>�
dt
</p>
<p>D 1
j2�&dagger;�1j1=2j2�&dagger;j1=2
</p>
<p>Z
exp
</p>
<p>�
�1
2
ft&gt;&dagger;t C 2it&gt;.x � �/ � .x � �/&gt;&dagger;�1.x � �/g
</p>
<p>�</p>
<p/>
</div>
<div class="page"><p/>
<p>140 4 Multivariate Distributions
</p>
<p>� exp
�
�1
2
f.x � �/&gt;&dagger;�1.x � �/g
</p>
<p>�
dt
</p>
<p>D 1
j2�&dagger;j1=2
</p>
<p>exp
</p>
<p>�
�1
2
f.x � �/&gt;&dagger;.x � �/g
</p>
<p>�
</p>
<p>since
</p>
<p>Z
1
</p>
<p>j2�&dagger;�1j1=2 exp
�
�1
2
ft&gt;&dagger;t C 2it&gt;.x � �/ � .x � �/&gt;&dagger;�1.x � �/g
</p>
<p>�
dt
</p>
<p>D
Z
</p>
<p>1
</p>
<p>j2�&dagger;�1j1=2 exp
�
�1
2
f.t C i&dagger;�1.x � �//&gt;&dagger;.t C i&dagger;�1.x � �//g
</p>
<p>�
dt
</p>
<p>D 1:
</p>
<p>Note that if Y � Np.0; Ip/, then
</p>
<p>'Y .t/ D exp
�
�1
2
t&gt;Ipt
</p>
<p>�
D exp
</p>
<p> 
�1
2
</p>
<p>pX
</p>
<p>iD1
t2i
</p>
<p>!
</p>
<p>D 'Y1.t1/ � : : : � 'Yp .tp/
</p>
<p>which is consistent with (4.33).
</p>
<p>Singular Normal Distribution
</p>
<p>Suppose that we have rank.&dagger;/ D k &lt; p, where p is the dimension of X . We define
the (singular) density of X with the aid of the G-Inverse&dagger;� of &dagger;,
</p>
<p>f .x/ D .2�/
�k=2
</p>
<p>.�1 � � ��k/1=2
exp
</p>
<p>�
�1
2
.x � �/&gt;&dagger;�.x � �/
</p>
<p>�
(4.53)
</p>
<p>where
</p>
<p>1. x lies on the hyperplaneN&gt;.x ��/ D 0 withN .p � .p � k// W N&gt;&dagger; D 0 and
N&gt;N D Ik .
</p>
<p>2. &dagger;� is the G-Inverse of &dagger;, and �1; : : : ; �k are the nonzero eigenvalues of &dagger;.
</p>
<p>What is the connection to a multinormal with k-dimensions? If
</p>
<p>Y � Nk.0;ƒ1/ and ƒ1 D diag.�1; : : : ; �k/; (4.54)
</p>
<p>then an orthogonal matrix B.p�k/with B&gt;B D Ik exists that meansX D BY C�
where X has a singular pdf of the form (4.53).</p>
<p/>
</div>
<div class="page"><p/>
<p>4.4 The Multinormal Distribution 141
</p>
<p>Gaussian Copula
</p>
<p>In Examples 4.3 and 4.4 we have introduced copulae. Another important copula is
</p>
<p>the Gaussian or normal copula,
</p>
<p>C�.u; v/ D
Z ˆ�11 .u/
</p>
<p>�1
</p>
<p>Z ˆ�12 .v/
</p>
<p>�1
f�.x1; x2/dx2dx1 ; (4.55)
</p>
<p>see Embrechts, McNeil, and Straumann (1999). In (4.55), f� denotes the bivariate
</p>
<p>normal density function with correlation � for n D 2. The functions ˆ1 and ˆ2
in (4.55) refer to the corresponding one-dimensional standard normal cdfs of the
</p>
<p>margins.
</p>
<p>In the case of vanishing correlation, � D 0, the Gaussian copula becomes
</p>
<p>C0.u; v/ D
Z ˆ�11 .u/
</p>
<p>�1
fX1.x1/dx1
</p>
<p>Z ˆ�12 .v/
</p>
<p>�1
fX2.x2/dx2
</p>
<p>D u v
D &hellip;.u; v/ :
</p>
<p>Summary
</p>
<p>,! The pdf of a p-dimensional multinormalX � Np.�;&dagger;/ is
</p>
<p>f .x/ D j2�&dagger;j�1=2 exp
�
�1
2
.x � �/&gt;&dagger;�1.x � �/
</p>
<p>�
:
</p>
<p>The contour curves of a multinormal are ellipsoids with half-
</p>
<p>lengths proportional to
p
�i , where �i denotes the eigenvalues of
</p>
<p>&dagger; (i D 1; : : : ; p).
,! The Mahalanobis transformation transforms X � Np.�;&dagger;/ to
</p>
<p>Y D &dagger;�1=2.X � �/ � Np.0; Ip/. Going in the other direction,
one can create a X � Np.�;&dagger;/ from Y � Np.0; Ip/ via X D
&dagger;1=2Y C �.
</p>
<p>,! If the covariance matrix &dagger; is singular (i.e. rank.&dagger;/ &lt; p), then it
defines a singular normal distribution.</p>
<p/>
</div>
<div class="page"><p/>
<p>142 4 Multivariate Distributions
</p>
<p>Summary (continued)
</p>
<p>,! The Gaussian copula is given by
</p>
<p>C�.u; v/ D
Z ˆ�11 .u/
</p>
<p>�1
</p>
<p>Z ˆ�12 .v/
</p>
<p>�1
f�.x1; x2/dx2dx1 :
</p>
<p>,! The density of a singular normal distribution is given by
</p>
<p>.2�/�k=2
</p>
<p>.�1 � � ��k/1=2
exp
</p>
<p>�
�1
2
.x � �/&gt;&dagger;�.x � �/
</p>
<p>�
:
</p>
<p>4.5 Sampling Distributions and Limit Theorems
</p>
<p>In multivariate statistics, we observe the values of a multivariate random variable
</p>
<p>X and obtain a sample fxi gniD1, as described in Chap. 3. Under random sampling,
these observations are considered to be realisations of a sequence of i.i.d. random
</p>
<p>variablesX1; : : : ; Xn, where eachXi is a p-variate random variable which replicates
</p>
<p>the parent or population random variable X . Some notational confusion is hard to
</p>
<p>avoid: Xi is not the i th component of X , but rather the i th replicate of the p-variate
</p>
<p>random variable X which provides the i th observation xi of our sample.
</p>
<p>For a given random sample X1; : : : ; Xn, the idea of statistical inference is to
</p>
<p>analyse the properties of the population variable X . This is typically done by
</p>
<p>analysing some characteristic � of its distribution, like the mean, covariance matrix,
</p>
<p>etc. Statistical inference in a multivariate setup is considered in more detail in
</p>
<p>Chaps. 6 and 7.
</p>
<p>Inference can often be performed using some observable function of the sample
</p>
<p>X1; : : : ; Xn, i.e. a statistics. Examples of such statistics were given in Chap. 3: the
</p>
<p>sample mean x, the sample covariance matrix S. To get an idea of the relationship
</p>
<p>between a statistics and the corresponding population characteristic, one has to
</p>
<p>derive the sampling distribution of the statistic. The next example gives some insight
</p>
<p>into the relation of .x; S/ to .�;&dagger;/.
</p>
<p>Example 4.15 Consider an iid sample of n random vectors Xi 2 Rp where
E.Xi / D � and Var.Xi/ D &dagger;. The sample mean x and the covariance matrix
S have already been defined in Sect. 3.3. It is easy to prove the following results:
</p>
<p>E.x/ D n�1
nP
iD1
</p>
<p>E.Xi/ D �
</p>
<p>Var.x/ D n�2
nP
iD1
</p>
<p>Var.Xi/ D n�1&dagger; D E.x x&gt;/� ��&gt;</p>
<p/>
</div>
<div class="page"><p/>
<p>4.5 Sampling Distributions and Limit Theorems 143
</p>
<p>E.S/ D n�1 E
�
</p>
<p>nP
iD1
.Xi � x/.Xi � x/&gt;
</p>
<p>�
</p>
<p>D n�1 E
�
</p>
<p>nP
iD1
</p>
<p>XiX
&gt;
i � n x x&gt;
</p>
<p>�
</p>
<p>D n�1
˚
n
�
&dagger;C ��&gt;
</p>
<p>�
� n
</p>
<p>�
n�1&dagger;C ��&gt;
</p>
<p>��
</p>
<p>D n�1
n
&dagger;:
</p>
<p>This shows in particular that S is a biased estimator of &dagger;. By contrast, Su D nn�1S
is an unbiased estimator of &dagger;.
</p>
<p>Statistical inference often requires more than just the mean and/or the variance
</p>
<p>of a statistic. We need the sampling distribution of the statistics to derive confidence
</p>
<p>intervals or to define rejection regions in hypothesis testing for a given significance
</p>
<p>level. Theorem 4.9 gives the distribution of the sample mean for a multinormal
</p>
<p>population.
</p>
<p>Theorem 4.9 Let X1; : : : ; Xn be i.i.d. with Xi � Np.�;&dagger;/. Then x �
Np.�; n
</p>
<p>�1&dagger;/.
</p>
<p>Proof x D n�1
Pn
</p>
<p>iD1Xi is a linear combination of independent normal variables,
so it has a normal distribution (see Chap. 5). The mean and the covariance matrix
</p>
<p>were given in the preceding example. ut
With multivariate statistics, the sampling distributions of the statistics are often
</p>
<p>more difficult to derive than in the preceding Theorem. In addition they might
</p>
<p>be so complicated that approximations have to be used. These approximations
</p>
<p>are provided by limit theorems. Since they are based on asymptotic limits, the
</p>
<p>approximations are only valid when the sample size is large enough. In spite of this
</p>
<p>restriction, they make complicated situations rather simple. The following central
</p>
<p>limit theorem shows that even if the parent distribution is not normal, when the
</p>
<p>sample size n is large, the sample mean Nx has an approximate normal distribution.
Theorem 4.10 (Central Limit Theorem (CLT)) Let X1; X2; : : : ; Xn be i.i.d. with
</p>
<p>Xi � .�;&dagger;/. Then the distribution of
p
n.x � �/ is asymptotically Np.0;&dagger;/, i.e.
</p>
<p>p
n.x � �/ L�! Np.0;&dagger;/ as n �!1:
</p>
<p>The symbol &ldquo;
L�!&rdquo; denotes convergence in distribution which means that the
</p>
<p>distribution function of the random vector
p
n. Nx � �/ converges to the distribution
</p>
<p>function of Np.0;&dagger;/.</p>
<p/>
</div>
<div class="page"><p/>
<p>144 4 Multivariate Distributions
</p>
<p>Example 4.16 Assume that X1; : : : ; Xn are i.i.d. and that they have Bernoulli
</p>
<p>distributions where p D 1
2
(this means that P.Xi D 1/ D 12 ; P.Xi D 0/ D
</p>
<p>1
2
/.
</p>
<p>Then � D p D 1
2
and &dagger; D p.1 � p/ D 1
</p>
<p>4
. Hence,
</p>
<p>p
n
</p>
<p>�
x � 1
</p>
<p>2
</p>
<p>�
L�! N1
</p>
<p>�
0;
1
</p>
<p>4
</p>
<p>�
as n �!1:
</p>
<p>The results are shown in Fig. 4.4 for varying sample sizes.
</p>
<p>Example 4.17 Now consider a two-dimensional random sample X1; : : : ; Xn that is
</p>
<p>i.i.d. and created from two independent Bernoulli distributions with p D 0:5. The
joint distribution is given by P.Xi D .0; 0/&gt;/ D 14 ; P.Xi D .0; 1/&gt;/ D
</p>
<p>1
4
; P.Xi D
</p>
<p>.1; 0/&gt;/ D 1
4
; P.Xi D .1; 1/&gt;/ D 14 . Here we have
</p>
<p>p
n
</p>
<p>(
x �
</p>
<p> 
1
2
1
2
</p>
<p>!)
D N2
</p>
<p>  
0
</p>
<p>0
</p>
<p>!
;
</p>
<p> 
1
4
</p>
<p>0
</p>
<p>0
1
4
</p>
<p>!!
as n �!1:
</p>
<p>Figure 4.5 displays the estimated two-dimensional density for different sample
</p>
<p>sizes.
</p>
<p>The asymptotic normal distribution is often used to construct confidence intervals
</p>
<p>for the unknown parameters. A confidence interval at the level 1 � ˛; ˛ 2 .0; 1/, is
an interval that covers the true parameter with probability 1 � ˛:
</p>
<p>P.� 2 Œb� l ;b� u&#141;/ D 1 � ˛;
</p>
<p>where � denotes the (unknown) parameter and b� l and b�u are the lower and upper
confidence bounds, respectively.
</p>
<p>Example 4.18 Consider the i.i.d. random variables X1; : : : ; Xn with Xi � .�; �2/
and �2 known. Since we have
</p>
<p>p
n.x��/ L! N.0; �2/ from the CLT, it follows that
</p>
<p>P
</p>
<p>�
�u1�˛=2 �
</p>
<p>p
n
.x � �/
�
</p>
<p>� u1�˛=2
�
�! 1 � ˛; as n �!1
</p>
<p>where u1�˛=2 denotes the .1 � ˛=2/-quantile of the standard normal distribution.
Hence the interval
</p>
<p>�
x � �p
</p>
<p>n
u1�˛=2; x C
</p>
<p>�p
n
u1�˛=2
</p>
<p>�
</p>
<p>is an approximate .1 � ˛/-confidence interval for �.
But what can we do if we do not know the variance �2? The following corollary
</p>
<p>gives the answer.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.5 Sampling Distributions and Limit Theorems 145
</p>
<p>Fig. 4.4 The CLT for
Bernoulli distributed random
variables. Sample size n D 5
(up) and n D 35 (down)
MVAcltbern
</p>
<p>-3 -2 -1 0 1 2 3
</p>
<p>0
.0
</p>
<p>0
.1
</p>
<p>0
.2
</p>
<p>0
.3
</p>
<p>0
.4
</p>
<p>1000 Random Samples
</p>
<p>E
s
ti
m
</p>
<p>a
te
</p>
<p>d
 a
</p>
<p>n
d
</p>
<p> N
o
</p>
<p>rm
a
</p>
<p>l 
D
</p>
<p>e
n
</p>
<p>s
it
y
</p>
<p>Asymptotic Distribution, n=5
</p>
<p>-4 -2 0 2 4
</p>
<p>0
.0
</p>
<p>0
.1
</p>
<p>0
.2
</p>
<p>0
.3
</p>
<p>0
.4
</p>
<p>1000 Random Samples
</p>
<p>E
s
ti
m
</p>
<p>a
te
</p>
<p>d
 a
</p>
<p>n
d
</p>
<p> N
o
</p>
<p>rm
a
</p>
<p>l 
D
</p>
<p>e
n
</p>
<p>s
it
y
</p>
<p>Asymptotic Distribution, n=35
</p>
<p>Corollary 4.1 If O&dagger; is a consistent estimate for &dagger;, then the CLT still holds, namely
</p>
<p>p
n b&dagger;�1=2.x � �/ L�! Np.0; I/ as n �!1:</p>
<p/>
</div>
<div class="page"><p/>
<p>146 4 Multivariate Distributions
</p>
<p>Fig. 4.5 The CLT in the two-dimensional case. Sample size n D 5 (left) and n D 85 (right)
MVAcltbern2
</p>
<p>Example 4.19 Consider the i.i.d. random variables X1; : : : ; Xn with Xi � .�; �2/,
and nowwith an unknown variance �2. FromCorollary 4.1 usingb�2 D 1
</p>
<p>n
</p>
<p>Pn
iD1.xi�
</p>
<p>x/2 we obtain
</p>
<p>p
n
</p>
<p>�
x � �
O�
</p>
<p>�
L�! N.0; 1/ as n �!1:
</p>
<p>Hence we can construct an approximate .1� ˛/-confidence interval for � using the
variance estimate O�2:
</p>
<p>C1�˛ D
�
x � O�p
</p>
<p>n
u1�˛=2; x C
</p>
<p>O�p
n
u1�˛=2
</p>
<p>�
:
</p>
<p>Note that by the CLT
</p>
<p>P.� 2 C1�˛/ �! 1 � ˛ as n �!1:
</p>
<p>Remark 4.1 One may wonder how large should n be in practice to provide
</p>
<p>reasonable approximations. There is no definite answer to this question: it mainly
</p>
<p>depends on the problem at hand (the shape of the distribution of the Xi and the
</p>
<p>dimension of Xi ). If the Xi are normally distributed, the normality of x is achieved
</p>
<p>from n D 1. In most situations, however, the approximation is valid in one-
dimensional problems for n larger than, say, 50.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.5 Sampling Distributions and Limit Theorems 147
</p>
<p>Transformation of Statistics
</p>
<p>Often in practical problems, one is interested in a function of parameters for
</p>
<p>which one has an asymptotically normal statistic. Suppose for instance that we are
</p>
<p>interested in a cost function depending on the mean � of the process: f .�/ D
�&gt;A� where A &gt; 0 is given. To estimate � we use the asymptotically normal
statistic x. The question is: how does f .x/ behave? More generally, what happens
</p>
<p>to a statistic t that is asymptotically normal when we transform it by a function
</p>
<p>f .t/? The answer is given by the following theorem.
</p>
<p>Theorem 4.11 If
p
n.t � �/ L�! Np.0;&dagger;/ and if f D .f1; : : : ; fq/&gt; W Rp !
</p>
<p>R
q are real valued functions which are differentiable at � 2 Rp , then f .t/ is
</p>
<p>asymptotically normal with mean f .�/ and covarianceD&gt;&dagger;D, i.e.
</p>
<p>p
nff .t/ � f .�/g L�! Nq.0;D&gt;&dagger;D/ for n �!1; (4.56)
</p>
<p>where
</p>
<p>D D
�
@fj
</p>
<p>@ti
</p>
<p>�
.t/
</p>
<p>ˇ̌
ˇ̌
tD�
</p>
<p>is the .p � q/ matrix of all partial derivatives.
Example 4.20 We are interested in seeing how f .x/ D x&gt;Ax behaves asymp-
totically with respect to the quadratic cost function of �; f .�/ D �&gt;A�, where
A &gt; 0.
</p>
<p>D D @f .x/
@x
</p>
<p>ˇ̌
ˇ̌
xD�
D 2A�:
</p>
<p>By Theorem 4.11 we have
</p>
<p>p
n.x&gt;Ax � �&gt;A�/ L�! N1 .0; 4�&gt;A&dagger;A�/:
</p>
<p>Example 4.21 Suppose
</p>
<p>Xi � .�;&dagger;/I � D
 
0
</p>
<p>0
</p>
<p>!
; &dagger; D
</p>
<p>�
1 0:5
</p>
<p>0:5 1
</p>
<p>�
; p D 2:
</p>
<p>We have by the CLT (Theorem 4.10) for n!1 that
</p>
<p>p
n.x � �/ L�! N.0;&dagger;/:</p>
<p/>
</div>
<div class="page"><p/>
<p>148 4 Multivariate Distributions
</p>
<p>Suppose that we would like to compute the distribution of
</p>
<p>�
x21 � x2
x1 C 3x2
</p>
<p>�
. According
</p>
<p>to Theorem 4.11 we have to consider f D .f1; f2/&gt; with
</p>
<p>f1.x1; x2/ D x21 � x2; f2.x1; x2/ D x1 C 3x2; q D 2:
</p>
<p>Given this f .�/ D
�
0
</p>
<p>0
</p>
<p>�
and
</p>
<p>D D .dij/; dij D
�
@fj
</p>
<p>@xi
</p>
<p>�ˇ̌
ˇ̌
xD�
D
�
2x1 1
</p>
<p>�1 3
</p>
<p>�ˇ̌
ˇ̌
xD0
</p>
<p>:
</p>
<p>Thus
</p>
<p>D D
�
</p>
<p>0 1
</p>
<p>�1 3
</p>
<p>�
:
</p>
<p>The covariance is
</p>
<p>�
0 �1
1 3
</p>
<p>� �
1 1
2
</p>
<p>1
2
1
</p>
<p>� �
0 1
</p>
<p>�1 3
</p>
<p>�
D
�
0 �1
1 3
</p>
<p>� �
� 1
2
5
2
</p>
<p>�1 7
2
</p>
<p>�
D
�
</p>
<p>1 � 7
2
</p>
<p>� 7
2
13
</p>
<p>�
</p>
<p>D&gt; &dagger; D D&gt; &dagger;D D&gt;&dagger;D
;
</p>
<p>which yields
</p>
<p>p
n
</p>
<p>�
x21 � x2
x1 C 3x2
</p>
<p>�
L�! N2
</p>
<p>  
0
</p>
<p>0
</p>
<p>!
;
</p>
<p>�
1 � 7
</p>
<p>2
</p>
<p>� 7
2
13
</p>
<p>�!
:
</p>
<p>Example 4.22 Let us continue the previous example by adding one more compo-
</p>
<p>nent to the function f . Since q D 3 &gt; p D 2, we might expect a singular normal
distribution. Consider f D .f1; f2; f3/&gt; with
</p>
<p>f1.x1; x2/ D x21 � x2; f2.x1; x2/ D x1 C 3x2; f3 D x32 ; q D 3:
</p>
<p>From this we have that
</p>
<p>D D
�
</p>
<p>0 1 0
</p>
<p>�1 3 0
</p>
<p>�
and thus D&gt;&dagger;D D
</p>
<p>0
@
</p>
<p>1 � 7
2
0
</p>
<p>� 7
2
13 0
</p>
<p>0 0 0
</p>
<p>1
A :
</p>
<p>The limit is in fact a singular normal distribution!</p>
<p/>
</div>
<div class="page"><p/>
<p>4.6 Heavy-Tailed Distributions 149
</p>
<p>Summary
</p>
<p>,! If X1; : : : ; Xn are i.i.d. random vectors with Xi � Np.�;&dagger;/, then
Nx � Np.�; 1n&dagger;/.
</p>
<p>,! If X1; : : : ; Xn are i.i.d. random vectors with Xi � .�;&dagger;/, then the
distribution of
</p>
<p>p
n.x��/ is asymptoticallyN.0;&dagger;/ (Central Limit
</p>
<p>Theorem).
</p>
<p>,! If X1; : : : ; Xn are i.i.d. random variables with Xi � .�; �/, then
an asymptotic confidence interval can be constructed by the CLT:
</p>
<p>x ˙ O�p
n
u1�˛=2.
</p>
<p>,! If t is a statistic that is asymptotically normal, i.e.pn.t � �/ L�!
Np.0;&dagger;/, then this holds also for a function f .t/, i.e.
</p>
<p>p
nff .t/ �
</p>
<p>f .�/g is asymptotically normal.
</p>
<p>4.6 Heavy-Tailed Distributions
</p>
<p>Heavy-tailed distributions were first introduced by the Italian-born Swiss economist
</p>
<p>Pareto and extensively studied by Paul L&eacute;vy. Although in the beginning these
</p>
<p>distributions were mainly studied theoretically, nowadays they have found many
</p>
<p>applications in areas as diverse as finance, medicine, seismology, structural engi-
</p>
<p>neering. More concretely, they have been used to model returns of assets in
</p>
<p>financial markets, stream flow in hydrology, precipitation and hurricane damage
</p>
<p>in meteorology, earthquake prediction in seismology, pollution, material strength,
</p>
<p>teletraffic and many others.
</p>
<p>A distribution is called heavy-tailed if it has higher probability density in its
</p>
<p>tail area compared with a normal distribution with same mean � and variance �2.
</p>
<p>Figure 4.6 demonstrates the differences of the pdf curves of a standard Gaussian
</p>
<p>distribution and a Cauchy distribution with location parameter � D 0 and scale
parameter � D 1. The graphic shows that the probability density of the Cauchy
distribution is much higher than that of the Gaussian in the tail part, while in the
</p>
<p>area around the centre, the probability density of the Cauchy distribution is much
</p>
<p>lower.
</p>
<p>In terms of kurtosis, a heavy-tailed distribution has kurtosis greater than 3 (see
</p>
<p>Chap. 4, formula (4.40)), which is called leptokurtic, in contrast to mesokurtic dis-
</p>
<p>tribution (kurtosisD 3) and platykurtic distribution (kurtosis&lt; 3). Since univariate
heavy-tailed distributions serve as basics for their multivariate counterparts and their
</p>
<p>density properties have been proved useful even in multivariate cases, we will start
</p>
<p>from introducing some univariate heavy-tailed distributions. Then we will move on
</p>
<p>to analyse their multivariate counterparts and their tail behaviour.</p>
<p/>
</div>
<div class="page"><p/>
<p>150 4 Multivariate Distributions
</p>
<p>&minus;6 &minus;4 &minus;2 0 2 4 6
</p>
<p>0
.0
</p>
<p>0
.1
</p>
<p>0
.2
</p>
<p>0
.3
</p>
<p>0
.4
</p>
<p>X
</p>
<p>Y
&minus;2f &minus;1f 1f 2f
</p>
<p>●●
</p>
<p>●
</p>
<p>Gaussy
Cauchy
</p>
<p>Distribution Comparison
</p>
<p>Fig. 4.6 Comparison of the pdf of a standard Gaussian (blue) and a Cauchy distribution (red) with
location parameter 0 and scale parameter 1 MVAgausscauchy
</p>
<p>&minus;6 &minus;4 &minus;2 0 2 4 6
</p>
<p>0
.0
</p>
<p>0
.1
</p>
<p>0
.2
</p>
<p>0
.3
</p>
<p>0
.4
</p>
<p>0
.5
</p>
<p>X
</p>
<p>Y
</p>
<p>PDF of GH, HYP and NIG
</p>
<p>&minus;6 &minus;4 &minus;2 0 2 4 6
</p>
<p>0
.0
</p>
<p>0
.2
</p>
<p>0
.4
</p>
<p>0
.6
</p>
<p>0
.8
</p>
<p>1
.0
</p>
<p>X
</p>
<p>Y
</p>
<p>CDF of GH, HYP and NIG
</p>
<p>Fig. 4.7 pdf (left) and cdf (right) of GH (� D 0:5, black), HYP (red), and NIG (blue) with
˛ D 1; ˇ D 0; ı D 1; � D 0 MVAghdis
</p>
<p>Generalised Hyperbolic Distribution
</p>
<p>The generalised hyperbolic distribution was introduced by Barndorff-Nielsen and at
</p>
<p>first applied to model grain size distributions of wind blown sands. Today one of
</p>
<p>its most important uses is in stock price modelling and market risk measurement.
</p>
<p>The name of the distribution is derived from the fact that its log-density forms a
</p>
<p>hyperbola, while the log-density of the normal distribution is a parabola (Fig. 4.7).</p>
<p/>
</div>
<div class="page"><p/>
<p>4.6 Heavy-Tailed Distributions 151
</p>
<p>The density of a one-dimensional generalised hyperbolic (GH) distribution for
</p>
<p>x 2 R is
</p>
<p>fGH.xI�; ˛; ˇ; ı; �/
</p>
<p>D
�p
˛2 � ˇ2=ı
</p>
<p>��
p
2�K�.ı
</p>
<p>p
˛2 � ˇ2/
</p>
<p>K��1=2
˚
˛
p
ı2 C .x � �/2
</p>
<p>�
p
ı2 C .x � �/2=˛/1=2��
</p>
<p>eˇ.x��/ (4.57)
</p>
<p>whereK� is a modified Bessel function of the third kind with index �
</p>
<p>K�.x/ D
1
</p>
<p>2
</p>
<p>Z 1
</p>
<p>0
</p>
<p>y��1e�
x
2 .yCy�1/dy (4.58)
</p>
<p>The domain of variation of the parameters is � 2 R and
</p>
<p>ı � 0; jˇj &lt; ˛; if � &gt; 0
ı &gt; 0; jˇj &lt; ˛; if � D 0
ı &gt; 0; jˇj � ˛; if � &lt; 0
</p>
<p>The generalised hyperbolic distribution has the following mean and variance
</p>
<p>EŒX&#141; D �C ıˇp
˛2 � ˇ2
</p>
<p>K�C1.ı
p
˛2 � ˇ2/
</p>
<p>K�.ı
p
˛2 � ˇ2/
</p>
<p>(4.59)
</p>
<p>VarŒX&#141; D ı2
"
</p>
<p>K�C1.ı
p
˛2 � ˇ2/
</p>
<p>ı
p
˛2 � ˇ2K�.ı
</p>
<p>p
˛2 � ˇ2/
</p>
<p>C ˇ
2
</p>
<p>˛2 � ˇ2
�
K�C2.ı
</p>
<p>p
˛2 � ˇ2/
</p>
<p>K�.ı
p
˛2 � ˇ2/
</p>
<p>�
�
K�C1.ı
</p>
<p>p
˛2 � ˇ2/
</p>
<p>K�.ı
p
˛2 � ˇ2/
</p>
<p>� 2�#
(4.60)
</p>
<p>Where � and ı play important roles in the density&rsquo;s location and scale respectively.
</p>
<p>With specific values of �, we obtain different sub-classes of GH such as hyperbolic
</p>
<p>(HYP) or normal-inverse Gaussian (NIG) distribution.
</p>
<p>For � D 1 we obtain the hyperbolic distributions (HYP)
</p>
<p>fHYP.xI˛; ˇ; ı; �/ D
p
˛2 � ˇ2
</p>
<p>2˛ıK1.ı
p
˛2 � ˇ2/
</p>
<p>ef�˛
p
ı2C.x��/2Cˇ.x��/g (4.61)
</p>
<p>where x; � 2 R; ı � 0 and jˇj &lt; ˛. For � D �1=2 we obtain the NIG distribution
</p>
<p>fNIG.xI˛; ˇ; ı; �/ D
˛ı
</p>
<p>�
</p>
<p>K1
�
˛
p
.ı2 C .x � �/2/
</p>
<p>�
p
ı2 C .x � �/2
</p>
<p>efı
p
˛2�ˇ2Cˇ.x��/g (4.62)</p>
<p/>
</div>
<div class="page"><p/>
<p>152 4 Multivariate Distributions
</p>
<p>Student&rsquo;s t-Distribution
</p>
<p>The t-distribution was first analysed by Gosset (1908) who published it under
</p>
<p>pseudonym &ldquo;Student&rdquo; by request of his employer. Let X be a normally distributed
</p>
<p>random variable with mean � and variance �2, and Y be the random variable such
</p>
<p>that Y 2=�2 has a chi-square distribution with n degrees of freedom. Assume that X
</p>
<p>and Y are independent, then
</p>
<p>t
defD X
</p>
<p>p
n
</p>
<p>Y
(4.63)
</p>
<p>is distributed as Student&rsquo;s t with n degrees of freedom. The t-distribution has the
</p>
<p>following density function
</p>
<p>ft .xIn/ D
&#128;
�
nC1
2
</p>
<p>�
</p>
<p>p
n�&#128;
</p>
<p>�
n
2
</p>
<p>�
 
1C x
</p>
<p>2
</p>
<p>n
</p>
<p>!� nC12
(4.64)
</p>
<p>where n is the number of degrees of freedom, �1 &lt; x &lt;1, and &#128; is the gamma
function:
</p>
<p>&#128;.˛/ D
Z 1
</p>
<p>0
</p>
<p>x˛�1e�xdx: (4.65)
</p>
<p>The mean, variance, skewness and kurtosis of Student&rsquo;s t-distribution .n &gt; 4/ are:
</p>
<p>� D 0
</p>
<p>�2 D n
n � 2
</p>
<p>Skewness D 0
</p>
<p>Kurtosis D 3C 6
n � 4 :
</p>
<p>The t-distribution is symmetric around 0, which is consistent with the fact that its
</p>
<p>mean is 0 and skewness is also 0 (Fig. 4.8).
</p>
<p>Student&rsquo;s t-distribution approaches the normal distribution as n increases, since
</p>
<p>lim
n!1
</p>
<p>ft .xIn/ D
1p
2�
e�
</p>
<p>x2
</p>
<p>2 : (4.66)
</p>
<p>In practice the t-distribution is widely used, but its flexibility of modelling is
</p>
<p>restricted because of the integer-valued tail index.
</p>
<p>In the tail area of the t-distribution, x is proportional to jxj�.nC1/. In Fig. 4.13
we compared the tail-behaviour of t-distribution with different degrees of freedom.
</p>
<p>With higher degree of freedom, the t-distribution decays faster.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.6 Heavy-Tailed Distributions 153
</p>
<p>&minus;4 &minus;2 0 2 4
</p>
<p>0
.0
</p>
<p>0
.1
</p>
<p>0
.2
</p>
<p>0
.3
</p>
<p>0
.4
</p>
<p>X
</p>
<p>Y
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>t3
</p>
<p>t6
</p>
<p>t30
</p>
<p>PDF of t&minus;distribution
</p>
<p>&minus;4 &minus;2 0 2 4
</p>
<p>0
.0
</p>
<p>0
.2
</p>
<p>0
.4
</p>
<p>0
.6
</p>
<p>0
.8
</p>
<p>1
.0
</p>
<p>X
</p>
<p>Y
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>t3
</p>
<p>t6
</p>
<p>t30
</p>
<p>CDF of t&minus;distribution
</p>
<p>Fig. 4.8 pdf (left) and cdf (right) of t -distribution with different degrees of freedom (t3 stands for
t -distribution with degree of freedom 3) MVAtdis
</p>
<p>Laplace Distribution
</p>
<p>The univariate Laplace distribution with mean zero was introduced by Laplace
</p>
<p>(1774). The Laplace distribution can be defined as the distribution of differences
</p>
<p>between two independent variates with identical exponential distributions. There-
</p>
<p>fore it is also called the double exponential distribution (Fig. 4.9).
</p>
<p>The Laplace distribution with mean � and scale parameter � has the pdf
</p>
<p>fLaplace.xI�; �/ D
1
</p>
<p>2�
e�
</p>
<p>jx��j
� (4.67)
</p>
<p>and the cdf
</p>
<p>FLaplace.xI�; �/ D
1
</p>
<p>2
</p>
<p>�
1C sign.x � �/.1 � e�
</p>
<p>jx��j
� /
</p>
<p>�
; (4.68)
</p>
<p>where sign is sign function. The mean, variance, skewness and kurtosis of the
</p>
<p>Laplace distribution are
</p>
<p>� D �
�2 D 2�2
</p>
<p>Skewness D 0
Kurtosis D 6
</p>
<p>With mean 0 and � D 1, we obtain the standard Laplace distribution</p>
<p/>
</div>
<div class="page"><p/>
<p>154 4 Multivariate Distributions
</p>
<p>&minus;6 &minus;4 &minus;2 0 2 4 6
</p>
<p>0
.0
</p>
<p>0
.1
</p>
<p>0
.2
</p>
<p>0
.3
</p>
<p>0
.4
</p>
<p>0
.5
</p>
<p>X
</p>
<p>Y
</p>
<p>L1
</p>
<p>L1.5
</p>
<p>L2
</p>
<p>PDF of Laplace distribution
</p>
<p>&minus;6 &minus;4 &minus;2 0 2 4 6
</p>
<p>0
.0
</p>
<p>0
.2
</p>
<p>0
.4
</p>
<p>0
.6
</p>
<p>0
.8
</p>
<p>1
.0
</p>
<p>X
</p>
<p>Y
</p>
<p>L1
</p>
<p>L1.5
</p>
<p>L2
</p>
<p>CDF of Laplace distribution
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>Fig. 4.9 pdf (left) and cdf (right) of Laplace distribution with zero mean and different scale
parameters (L1 stands for Laplace distribution with � D 1) MVAlaplacedis
</p>
<p>f .x/ D e
�jxj
</p>
<p>2
(4.69)
</p>
<p>F.x/ D
�
ex
</p>
<p>2
for x &lt; 0
</p>
<p>1 � e�x
2
</p>
<p>for x � 0 (4.70)
</p>
<p>Cauchy Distribution
</p>
<p>The Cauchy distribution is motivated by the following example.
</p>
<p>Example 4.23 A gangster has just robbed a bank. As he runs to a point s metres
</p>
<p>away from the wall of the bank, a policeman reaches the crime scene behind the
</p>
<p>wall of the bank. The robber turns back and starts to shoot but he is such a poor
</p>
<p>shooter that the angle of his fire (marked in Fig. 4.10 as ˛) is uniformly distributed.
</p>
<p>The bullets hit the wall at distance x (from the centre). Obviously the distribution
</p>
<p>of x, the random variable where the bullet hits the wall, is of vital knowledge to the
</p>
<p>policeman in order to identify the location of the gangster. (Should the policeman
</p>
<p>calculate the mean or the median of the observed bullet hits fxi gniD1 in order to
identify the location of the robber?)
</p>
<p>Since ˛ is uniformly distributed:
</p>
<p>f .˛/ D 1
�
</p>
<p>I.˛ 2 Œ��=2; �=2&#141;/</p>
<p/>
</div>
<div class="page"><p/>
<p>4.6 Heavy-Tailed Distributions 155
</p>
<p>Fig. 4.10 Introduction to Cauchy distribution&mdash;robber vs. policeman
</p>
<p>and
</p>
<p>tan ˛ D x
s
</p>
<p>˛ D arctan
�x
s
</p>
<p>�
</p>
<p>d˛ D 1
s
</p>
<p>1
</p>
<p>1C . x
s
/2
dx
</p>
<p>For a small interval d˛, the probability is given by
</p>
<p>f .˛/d˛ D 1
�
d˛
</p>
<p>D 1
s�
</p>
<p>1
</p>
<p>1C
�
x
s
</p>
<p>�2 dx
</p>
<p>with
</p>
<p>Z �
2
</p>
<p>� �2
</p>
<p>1
</p>
<p>�
d˛ D 1</p>
<p/>
</div>
<div class="page"><p/>
<p>156 4 Multivariate Distributions
</p>
<p>Z 1
</p>
<p>�1
</p>
<p>1
</p>
<p>s�
</p>
<p>1
</p>
<p>1C
�
x
s
</p>
<p>�2 dx D
1
</p>
<p>�
</p>
<p>n
arctan
</p>
<p>�x
s
</p>
<p>�o1
�1
</p>
<p>D 1
�
</p>
<p>n�
2
�
�
� �
2
</p>
<p>�o
</p>
<p>D 1
</p>
<p>So the pdf of x can be written as:
</p>
<p>f .x/ D 1
s�
</p>
<p>1
</p>
<p>1C . x
s
/2
</p>
<p>The general formula for the pdf and cdf of the Cauchy distribution is
</p>
<p>fCauchy.xIm; s/ D
1
</p>
<p>s�
</p>
<p>1
</p>
<p>1C . x�m
s
/2
</p>
<p>(4.71)
</p>
<p>FCauchy.xIm; s/ D
1
</p>
<p>2
C 1
�
arctan
</p>
<p>�
x �m
s
</p>
<p>�
(4.72)
</p>
<p>where m and s are location and scale parameter respectively. The case in the above
</p>
<p>example where m D 0 and s D 1 is called the standard Cauchy distribution with
pdf and cdf as following,
</p>
<p>fCauchy.x/ D
1
</p>
<p>�.1C x2/ (4.73)
</p>
<p>FCauchy.xIm; s/ D
1
</p>
<p>2
C arctan.x/
</p>
<p>�
(4.74)
</p>
<p>The mean, variance, skewness and kurtosis of Cauchy distribution are all undefined,
</p>
<p>since its moment generating function diverges. But it has mode and median, both
</p>
<p>equal to the location parameterm (Fig. 4.11).
</p>
<p>Mixture Model
</p>
<p>Mixture modelling concerns modelling a statistical distribution by a mixture (or
</p>
<p>weighted sum) of different distributions. For many choices of component density
</p>
<p>functions, the mixture model can approximate any continuous density to arbitrary
</p>
<p>accuracy, provided that the number of component density functions is sufficiently
</p>
<p>large and the parameters of the model are chosen correctly. The pdf of a mixture
</p>
<p>distribution consists of n distributions and can be written as:</p>
<p/>
</div>
<div class="page"><p/>
<p>4.6 Heavy-Tailed Distributions 157
</p>
<p>&minus;6 &minus;4 &minus;2 0 2 4 6
</p>
<p>0
.0
</p>
<p>0
.1
</p>
<p>0
.2
</p>
<p>0
.3
</p>
<p>0
.4
</p>
<p>X
</p>
<p>Y
</p>
<p>C1
</p>
<p>C1.5
</p>
<p>C2
</p>
<p>PDF of Cauchy distribution
</p>
<p>&minus;6 &minus;4 &minus;2 0 2 4 6
</p>
<p>0
.2
</p>
<p>0
.4
</p>
<p>0
.6
</p>
<p>0
.8
</p>
<p>X
</p>
<p>Y
</p>
<p>C1
</p>
<p>C1.5
</p>
<p>C2
</p>
<p>CDF of Cauchy distribution
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>Fig. 4.11 pdf (left) and cdf (right) of Cauchy distribution with m D 0 and different scale
parameters (C1 stands for Cauchy distribution with s D 1) MVAcauchy
</p>
<p>f .x/ D
LX
</p>
<p>lD1
wlpl.x/ (4.75)
</p>
<p>under the constraints:
</p>
<p>0 � wl � 1
LX
</p>
<p>lD1
wl D 1
</p>
<p>Z
pl.x/dx D 1
</p>
<p>where pl.x/ is the pdf of the l&rsquo;th component density and wl is a weight. The mean,
</p>
<p>variance, skewness and kurtosis of a mixture are
</p>
<p>� D
LX
</p>
<p>lD1
wl�l (4.76)
</p>
<p>�2 D
LX
</p>
<p>lD1
wlf�2l C .�l � �/2g (4.77)
</p>
<p>Skewness D
LX
</p>
<p>lD1
wl
</p>
<p>(�
�l
</p>
<p>�
</p>
<p>�3
SKl C
</p>
<p>3�2l .�l � �/
�3
</p>
<p>C
�
�l � �
�
</p>
<p>�3)
(4.78)</p>
<p/>
</div>
<div class="page"><p/>
<p>158 4 Multivariate Distributions
</p>
<p>Kurtosis D
LX
</p>
<p>lD1
wl
</p>
<p>(�
�l
</p>
<p>�
</p>
<p>�4
Kl C
</p>
<p>6.�l � �/2�2l
�4
</p>
<p>C 4.�l � �/�
3
l
</p>
<p>�4
SKl
</p>
<p>C
�
�l � �
�
</p>
<p>�4)
; (4.79)
</p>
<p>where �l ; �l ; SKl and Kl are respectively mean, variance, skewness and kurtosis of
</p>
<p>l&rsquo;th distribution.
</p>
<p>Mixture models are ubiquitous in virtually every facet of statistical analysis,
</p>
<p>machine learning and data mining. For data sets comprising continuous vari-
</p>
<p>ables, the most common approach involves mixture distributions having Gaussian
</p>
<p>components.
</p>
<p>The pdf for a Gaussian mixture is:
</p>
<p>fGM.x/ D
LX
</p>
<p>lD1
</p>
<p>wlp
2��l
</p>
<p>e
� .x��l /
</p>
<p>2
</p>
<p>2�2
l : (4.80)
</p>
<p>For a Gaussian mixture consisting of Gaussian distributions with mean 0, this can
</p>
<p>be simplified to:
</p>
<p>fGM.x/ D
LX
</p>
<p>lD1
</p>
<p>wlp
2��l
</p>
<p>e
� x2
2�2
l ; (4.81)
</p>
<p>with variance, skewness and kurtosis
</p>
<p>�2 D
LX
</p>
<p>lD1
wl�
</p>
<p>2
l (4.82)
</p>
<p>Skewness D 0 (4.83)
</p>
<p>Kurtosis D
LX
</p>
<p>lD1
wl
</p>
<p>�
�l
</p>
<p>�
</p>
<p>�4
3 (4.84)
</p>
<p>Example 4.24 Consider a Gaussian Mixture which is 80% N(0,1) and 20% N(0,9).
</p>
<p>The pdf of N(0,1) and N(0,9) are (Fig. 4.12):
</p>
<p>fN.0;1/.x/ D
1p
2�
e�
</p>
<p>x2
</p>
<p>2
</p>
<p>fN.0;9/.x/ D
1
</p>
<p>3
p
2�
e�
</p>
<p>x2
</p>
<p>18
</p>
<p>so the pdf of the Gaussian Mixture is</p>
<p/>
</div>
<div class="page"><p/>
<p>4.6 Heavy-Tailed Distributions 159
</p>
<p>&minus;6 &minus;4 &minus;2 0 2 4 6
</p>
<p>0
.0
</p>
<p>0
.1
</p>
<p>0
.2
</p>
<p>0
.3
</p>
<p>0
.4
</p>
<p>X
</p>
<p>Y
</p>
<p>●
</p>
<p>Gaussian Mixture
</p>
<p>Gaussian
</p>
<p>Pdf of a Gaussian mixture and Gaussian
</p>
<p>&minus;6 &minus;4 &minus;2 0 2 4 6
</p>
<p>0
.0
</p>
<p>0
.2
</p>
<p>0
.4
</p>
<p>0
.6
</p>
<p>0
.8
</p>
<p>1
.0
</p>
<p>X
Y
</p>
<p>●
</p>
<p>Gaussian Mixture
</p>
<p>Gaussian
</p>
<p>Cdf of a Gaussian mixture and Gaussian
</p>
<p>●
</p>
<p>●
</p>
<p>Fig. 4.12 pdf (left) and cdf (right) of a Gaussian mixture (Example 4.24) MVAmixture
</p>
<p>Table 4.2 Basic statistics of
t , Laplace and Cauchy
distribution
</p>
<p>t Laplace Cauchy
</p>
<p>Mean 0 � Not defined
</p>
<p>Variance n
n�2 2�
</p>
<p>2 Not defined
</p>
<p>Skewness 0 0 Not defined
</p>
<p>Kurtosis 3C 6
n�4 6 Not defined
</p>
<p>fGM.x/ D
1
</p>
<p>5
p
2�
</p>
<p>�
4e�
</p>
<p>x2
</p>
<p>2 C 1
3
e�
</p>
<p>x2
</p>
<p>18
</p>
<p>�
</p>
<p>Notice that the Gaussian Mixture is not a Gaussian distribution:
</p>
<p>� D 0
�2 D 0:8 � 1C 0:2 � 9 D 2:6
</p>
<p>Skewness D 0
</p>
<p>Kurtosis D 0:8 �
�
</p>
<p>1p
2:6
</p>
<p>�4
� 3C 0:2 �
</p>
<p>� p
9p
2:6
</p>
<p>�4
� 3 D 7:54
</p>
<p>The kurtosis of this Gaussian mixture is higher than 3.
</p>
<p>A summary of the basic statistics is given in Tables 4.2 and 4.3.</p>
<p/>
</div>
<div class="page"><p/>
<p>160 4 Multivariate Distributions
</p>
<p>Table 4.3 Basic statistics of GH distribution and mixture model
</p>
<p>GH
</p>
<p>Mean �C ıˇp
˛2Cˇ2
</p>
<p>K�C1.ı
p
</p>
<p>˛2Cˇ2/
K�.ı
p
˛2Cˇ2/
</p>
<p>Variance ı2
</p>
<p>"
K�C1.ı
</p>
<p>p
˛2Cˇ2/
</p>
<p>ı
p
˛2Cˇ2K�.ı
</p>
<p>p
˛2Cˇ2/
</p>
<p>C ˇ2
˛2Cˇ2
</p>
<p>�
K�C2.ı
</p>
<p>p
˛2Cˇ2/
</p>
<p>K�.ı
p
˛2Cˇ2/
</p>
<p>�
�
K�C1.ı
</p>
<p>p
˛2Cˇ2/
</p>
<p>K�.ı
p
˛2Cˇ2/
</p>
<p>� 2�#
</p>
<p>Mixture
</p>
<p>Mean
PL
</p>
<p>lD1 wl�l
</p>
<p>Variance
PL
</p>
<p>lD1 wlf�2l C .�l � �/2g
</p>
<p>Skewness
PL
</p>
<p>lD1 wl
</p>
<p>(�
�l
�
</p>
<p>�3
SKl C 3�
</p>
<p>2
l .�l��/
�3
</p>
<p>C
�
�l��
�
</p>
<p>�3)
</p>
<p>Kurtosis
PL
</p>
<p>lD1 wl
</p>
<p>(�
�l
�
</p>
<p>�4
Kl C 6.�l��/
</p>
<p>2�2l
�4
</p>
<p>C 4.�l��/�
3
l
</p>
<p>�4
SKl C
</p>
<p>�
�l��
�
</p>
<p>�4)
</p>
<p>Multivariate Generalised Hyperbolic Distribution
</p>
<p>The multivariate Generalised Hyperbolic Distribution (GHd ) has the following pdf
</p>
<p>fGHd .xI�; ˛; ˇ; ı;&#129;;�/ D ad
K�� d2
</p>
<p>n
˛
p
ı2 C .x � �/&gt;&#129;�1.x � �/
</p>
<p>o
</p>
<p>n
˛�1
</p>
<p>p
ı2 C .x � �/&gt;&#129;�1.x � �/
</p>
<p>o d
2��
</p>
<p>eˇ
&gt;.x��/
</p>
<p>(4.85)
</p>
<p>ad D ad .�; ˛; ˇ; ı;&#129;/ D
</p>
<p>�p
˛2 � ˇ&gt;&#129;ˇ=ı
</p>
<p>��
</p>
<p>.2�/
d
2K�.ı
</p>
<p>p
˛2 � ˇ&gt;&#129;ˇ
</p>
<p>; (4.86)
</p>
<p>and characteristic function
</p>
<p>�.t/ D
�
</p>
<p>˛2 � ˇ&gt;&#129;ˇ
˛2 � ˇ&gt;&#129;ˇ C 1
</p>
<p>2
t&gt;&#129;t � iˇ&gt;&#129;t
</p>
<p>� �
2
</p>
<p>�
K�
</p>
<p>�
ı
</p>
<p>q
˛2 � ˇ&gt;&#129;ˇ&gt; C 1
</p>
<p>2
t&gt;&#129;t � iˇ&gt;&#129;t
</p>
<p>�
</p>
<p>K�
</p>
<p>�
ı
p
˛2 � ˇ&gt;&#129;ˇ&gt;
</p>
<p>� (4.87)</p>
<p/>
</div>
<div class="page"><p/>
<p>4.6 Heavy-Tailed Distributions 161
</p>
<p>These parameters have the following domain of variation:
</p>
<p>� 2 R; ˇ; � 2 Rd
ı &gt; 0; ˛ &gt; ˇ&gt;&#129;ˇ
&#129; 2 Rd�d positive definite matrix
j&#129;j D 1
</p>
<p>For � D dC1
2
</p>
<p>we obtain the multivariate hyperbolic (HYP) distribution; for � D � 1
2
</p>
<p>we get the multivariate normal inverse Gaussian (NIG) distribution.
</p>
<p>Bl&aelig;sild and Jensen (1981) introduced a second parameterisation (�;&hellip;;&dagger;), where
</p>
<p>� D ı
q
˛2 � ˇ&gt;&#129;ˇ (4.88)
</p>
<p>&hellip; D ˇ
s
</p>
<p>&#129;
</p>
<p>˛2 � ˇ&gt;&#129;ˇ (4.89)
</p>
<p>&dagger; D ı2&#129; (4.90)
</p>
<p>The mean and variance of X � GHd
</p>
<p>EŒX&#141; D �C ıR�.�/&hellip;&#129;
1
2 (4.91)
</p>
<p>VarŒX&#141; D ı2
˚
��1R�.�/&#129;C S�.�/.&hellip;&#129;
</p>
<p>1
2 /&gt;.&hellip;&#129;
</p>
<p>1
2 /
�
</p>
<p>(4.92)
</p>
<p>where
</p>
<p>R�.x/ D
K�C1.x/
</p>
<p>K�.x/
(4.93)
</p>
<p>S�.x/ D
K�C2.x/K�.x/ �K2�C1.x/
</p>
<p>K2�.x/
(4.94)
</p>
<p>Theorem 4.12 Suppose thatX is a d -dimensional variate distributed according to
</p>
<p>the generalised hyperbolic distribution GHd . Let .X1; X2/ be a partitioning of X ,
</p>
<p>let r and k denote the dimensions of X1 and X2, respectively, and let .ˇ1; ˇ2/ and
</p>
<p>.�1; �2/ be similar partitions of ˇ and �, let
</p>
<p>&#129; D
�
&#129;11 &#129;12
&#129;21 &#129;22
</p>
<p>�
(4.95)</p>
<p/>
</div>
<div class="page"><p/>
<p>162 4 Multivariate Distributions
</p>
<p>be a partition of&#129; such that&#129;11 is a r � r matrix. Then one has the following
1. The distribution of X1 is the r-dimensional generalised hyperbolic distribution,
</p>
<p>GHr .�
�; ˛�; ˇ�; ı�; ��; &#129;�/, where
</p>
<p>�� D �
˛� D j&#129;11j�
</p>
<p>1
2r f˛2 � ˇ2.&#129;22 �&#129;21&#129;�111 &#129;12/ˇ&gt;2 g
</p>
<p>1
2
</p>
<p>ˇ� D ˇ1 C ˇ2&#129;21&#129;�111
</p>
<p>ı� D ıj&#129;11j
1
2�
</p>
<p>�� D �1
&#129;� D j&#129;j� 1r&#129;11
</p>
<p>2. The conditional distribution of X2 given X1 D x1 is the k-dimensional
generalised hyperbolic distribution GHk. Q�; Q̨ ; Q̌; Qı; Q�; Q&#129;),where
</p>
<p>Q� D � � r
2
</p>
<p>Q̨ D ˛j&#129;11j
1
2k
</p>
<p>Q̌ D ˇ2
Qı D j&#129;11j�
</p>
<p>1
2k fı2 C .x1 � �1/&#129;�111 .x1 � �1/&gt;g
</p>
<p>1
2
</p>
<p>Q� D �2 C .x1 � �1/&#129;�111 &#129;12
Q&#129; D j&#129;11j
</p>
<p>1
k .&#129;22 �&#129;21&#129;�111 &#129;12/
</p>
<p>3. Let Y D XA C B be a regular affine transformation of X and let
jjAjj denote the absolute value of the determinant of A. The distri-
bution of Y is the d -dimensional generalised hyperbolic distribution
</p>
<p>GHd .�
C; ˛C; ˇC; ıC; �C; &#129;C/,where
</p>
<p>�C D �
˛C D ˛jjAjj� 1d
</p>
<p>ˇC D ˇ.A�1/&gt;
</p>
<p>ıC D jjAjj 1d
</p>
<p>�C D �ACB
</p>
<p>&#129;C D jjAjj� 2d A&gt;&#129;A</p>
<p/>
</div>
<div class="page"><p/>
<p>4.6 Heavy-Tailed Distributions 163
</p>
<p>Multivariate t-Distribution
</p>
<p>If X and Y are independent and distributed as Np.�;&dagger;/ and X
2
n respectively, and
</p>
<p>X
p
n=Y D t � �, then the pdf of t is given by
</p>
<p>ft .t In;&dagger;;�/ D
&#128; f.nC p/=2g
</p>
<p>&#128;.n=2/np=2�p=2 j&dagger;j1=2
˚
1C 1
</p>
<p>n
.t � �/&gt;&dagger;�1.t � �/
</p>
<p>�.nCp/=2
</p>
<p>(4.96)
</p>
<p>The distribution of t is the noncentral t-distribution with n degrees of freedom and
</p>
<p>the noncentrality parameter �, Giri (1996).
</p>
<p>Multivariate Laplace Distribution
</p>
<p>Let g and G be the pdf and cdf of a d -dimensional Gaussian distribution Nd .0;&dagger;/,
</p>
<p>the pdf and cdf of a multivariate Laplace distribution can be written as
</p>
<p>fMLaplaced .xIm;&dagger;/ D
Z 1
</p>
<p>0
</p>
<p>g.z�
1
2 x � z 12m/z� d2 e�zdz (4.97)
</p>
<p>FMLaplaced .x;m;&dagger;/ D
Z 1
</p>
<p>0
</p>
<p>G.z�
1
2 x � z 12m/e�zdz (4.98)
</p>
<p>the pdf can also be described as
</p>
<p>fMLaplaced .xIm;&dagger;/ D
2ex
</p>
<p>&gt;&dagger;�1m
</p>
<p>.2�/
d
2 j&dagger;j 12
</p>
<p>�
x&gt;&dagger;�1x
</p>
<p>2Cm&gt;&dagger;�1m
</p>
<p>� �
2
</p>
<p>�K�
�q
</p>
<p>.2Cm&gt;&dagger;�1m/.x&gt;&dagger;�1x/
�
</p>
<p>(4.99)
</p>
<p>where � D 2�d
2
</p>
<p>andK�.x/ is the modified Bessel function of the third kind
</p>
<p>K�.x/ D
1
</p>
<p>2
</p>
<p>�
x
</p>
<p>2
</p>
<p>�� Z 1
</p>
<p>0
</p>
<p>t���1e�t�
x2
</p>
<p>4t dt; x &gt; 0 (4.100)
</p>
<p>Multivariate Laplace distribution has mean and variance
</p>
<p>EŒX&#141; D m (4.101)
CovŒX&#141; D &dagger;C mm&gt; (4.102)</p>
<p/>
</div>
<div class="page"><p/>
<p>164 4 Multivariate Distributions
</p>
<p>2.5 3.0 3.5 4.0
</p>
<p>0
.0
</p>
<p>0
0
</p>
<p>.0
1
</p>
<p>0
.0
</p>
<p>2
0
</p>
<p>.0
3
</p>
<p>0
.0
</p>
<p>4
</p>
<p>X
</p>
<p>Y
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>t1
</p>
<p>t3
</p>
<p>t9
</p>
<p>t45
</p>
<p>Gaussian
</p>
<p>Tail comparison of t&minus;distribution
</p>
<p>Fig. 4.13 Tail comparison of t -distribution MVAtdistail
</p>
<p>Multivariate Mixture Model
</p>
<p>A multivariate mixture model comprises multivariate distributions, e.g. the pdf of a
</p>
<p>multivariate Gaussian distribution can be written as
</p>
<p>f .x/ D
LX
</p>
<p>lD1
</p>
<p>wl
</p>
<p>j2�&dagger;l j
1
2
</p>
<p>e�
1
2 .x��l /&gt;&dagger;�1.x��l / (4.103)
</p>
<p>Generalised Hyperbolic Distribution
</p>
<p>The GH distribution has an exponential decaying speed
</p>
<p>fGH.xI�; ˛; ˇ; ı; � D 0/ � x��1e�.˛�ˇ/x as x !1; (4.104)
</p>
<p>As a comparison to tail behaviour of t-distribution depicted in Fig. 4.13, the
</p>
<p>Fig. 4.14 illustrates the tail behaviour of GH distributions with different value of
</p>
<p>� with ˛ D 1; ˇ D 0; ı D 1; � D 0. It is clear that among the four distributions,
GH with � D 1:5 has the lowest decaying speed, while NIG decays faster.
</p>
<p>In Fig. 4.15, Chen, H&auml;rdle, and Jeong (2008), four distributions and especially
</p>
<p>their tail-behaviour are compared. In order to keep the comparability of these
</p>
<p>distributions, we specified the means to 0 and standardised the variances to 1.
</p>
<p>Furthermore we used one important subclass of the GH distribution: the NIG</p>
<p/>
</div>
<div class="page"><p/>
<p>4.6 Heavy-Tailed Distributions 165
</p>
<p>4.0 4.2 4.4 4.6 4.8 5.0
</p>
<p>0
.0
</p>
<p>0
0
</p>
<p>0
.0
</p>
<p>0
5
</p>
<p>0
.0
</p>
<p>1
0
</p>
<p>0
.0
</p>
<p>1
5
</p>
<p>0
.0
</p>
<p>2
0
</p>
<p>X
</p>
<p>Y
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>GH(f=0.5)
</p>
<p>GH(f=1.5)
</p>
<p>NIG
</p>
<p>HYP
</p>
<p>Tail comparison &minus; GH
</p>
<p>Fig. 4.14 Tail comparison of GH distribution (pdf) MVAghdistail
</p>
<p>&minus;6 &minus;4 &minus;2 0 2 4 6
</p>
<p>0
.0
</p>
<p>0
.1
</p>
<p>0
.2
</p>
<p>0
.3
</p>
<p>0
.4
</p>
<p>0
.5
</p>
<p>X
</p>
<p>Y
</p>
<p>●●●●●●●●●●
●●●●●●
</p>
<p>●●●●●
●●●●
</p>
<p>●●●
●●
●●
●●
●●
●
●
●
●
●
●
●
●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
●
●
●
●
●
●
●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>Laplace
</p>
<p>NIG
</p>
<p>Cauchy
</p>
<p>Gaussian
</p>
<p>Distribution comparison
</p>
<p>&minus;5.0 &minus;4.8 &minus;4.6 &minus;4.4 &minus;4.2 &minus;4.0
</p>
<p>0
.0
</p>
<p>0
0
</p>
<p>0
.0
</p>
<p>0
5
</p>
<p>0
.0
</p>
<p>1
0
</p>
<p>0
.0
</p>
<p>1
5
</p>
<p>0
.0
</p>
<p>2
0
</p>
<p>X
</p>
<p>Y
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>NIG
</p>
<p>Laplace
</p>
<p>Cauchy
</p>
<p>Gaussian
</p>
<p>Tail comparison
</p>
<p>Fig. 4.15 Graphical comparison of the NIG distribution (line), standard normal distribution
MVAghadatail
</p>
<p>distribution with � D � 1
2
introduced above. On the left panel, the complete forms
</p>
<p>of these distributions are revealed. The Cauchy (dots) distribution has the lowest
</p>
<p>peak and the fattest tails. In other words, it has the flattest distribution. The NIG
</p>
<p>distribution decays second fast in the tails although it has the highest peak, which is
</p>
<p>more clearly displayed on the right panel.</p>
<p/>
</div>
<div class="page"><p/>
<p>166 4 Multivariate Distributions
</p>
<p>4.7 Copulae
</p>
<p>The cumulative distribution function (cdf) of a two-dimensional vector .X1; X2/ is
</p>
<p>given by
</p>
<p>F .x1; x2/ D P .X1 � x1; X2 � x2/ : (4.105)
</p>
<p>For the case that X1 and X2 are independent, their joint cumulative distribution
</p>
<p>function F.x1; x2/ can be written as a product of their one-dimensional marginals:
</p>
<p>F.x1; x2/ D FX1 .x1/ FX2 .x2/ D P .X1 � x1/ P .X2 � x2/ : (4.106)
</p>
<p>But how can we model dependence of X1 and X2? Most people would suggest
</p>
<p>linear correlation. Correlation is though an appropriate measure of dependence only
</p>
<p>when the random variables have an elliptical or spherical distribution, which include
</p>
<p>the normal multivariate distribution. Although the terms &ldquo;correlation&rdquo; and &ldquo;depen-
</p>
<p>dency&rdquo; are often used interchangeably, correlation is actually a rather imperfect
</p>
<p>measure of dependency, and there are many circumstances where correlation should
</p>
<p>not be used.
</p>
<p>Copulae represent an elegant concept of connecting marginals with joint cumula-
</p>
<p>tive distribution functions. Copulae are functions that join or &ldquo;couple&rdquo; multivariate
</p>
<p>distribution functions to their one-dimensional marginal distribution functions. Let
</p>
<p>us consider a d -dimensional vector X D .X1; : : : ; Xd /&gt;. Using copulae, the
marginal distribution functions FXi .i D 1; : : : ; d / can be separately modelled
from their dependence structure and then coupled together to form the multivariate
</p>
<p>distribution FX . Copula functions have a long history in probability theory and
</p>
<p>statistics. Their application in finance is very recent. Copulae are important in Value-
</p>
<p>at-Risk calculations and constitute an essential tool in quantitative finance (H&auml;rdle
</p>
<p>et al., 2009).
</p>
<p>First let us concentrate on the two-dimensional case, then we will extend this
</p>
<p>concept to the d -dimensional case, for a random variable in Rd with d � 1. To be
able to define a copula function, first we need to represent a concept of the volume
</p>
<p>of a rectangle, a 2-increading function and a grounded function.
</p>
<p>Let U1 and U2 be two sets in R D R[fC1g[ f�1g and consider the function
F W U1 � U2 �! R:
Definition 4.2 The F -volume of a rectangle B D Œx1; x2&#141; � Œy1; y2&#141; � U1 � U2 is
defined as:
</p>
<p>VF .B/ D F.x2; y2/ � F.x1; y2/� F.x2; y1/C F.x1; y1/ (4.107)</p>
<p/>
</div>
<div class="page"><p/>
<p>4.7 Copulae 167
</p>
<p>Definition 4.3 F is said to be a 2-increasing function if for every B D Œx1; x2&#141; �
Œy1; y2&#141; � U1 � U2,
</p>
<p>VF .B/ � 0 (4.108)
</p>
<p>Remark 4.2 Note that &ldquo;to be 2-increasing function&rdquo; neither implies nor is implied
</p>
<p>by &ldquo;to be increasing in each argument&rdquo;.
</p>
<p>The following lemmas (Nelsen, 1999) will be very useful later for establishing
</p>
<p>the continuity of copulae.
</p>
<p>Lemma 4.1 Let U1 and U2 be non-empty sets in R and let F W U1 �U2 �! R be a
two-increasing function. Let x1, x2 be in U1 with x1 � x2, and y1, y2 be in U2 with
y1 � y2. Then the function t 7! F.t; y2/ � F.t; y1/ is non-decreasing on U1 and
the function t 7! F.x2; t/ � F.x1; t/ is non-decreasing on U2.
Definition 4.4 If U1 and U2 have a smallest element minU1 and minU2 respec-
</p>
<p>tively, then we say that a function F W U1 � U2 �! R is grounded if :
</p>
<p>for all x 2 U1 : F.x;minU2/ D 0 and (4.109)
for all y 2 U2 : F.minU1; y/ D 0 (4.110)
</p>
<p>In the following, we will refer to this definition of a cdf.
</p>
<p>Definition 4.5 A cdf is a function from R
2 7! Œ0; 1&#141; which
</p>
<p>(i) is grounded
</p>
<p>(ii) is 2-increasing
</p>
<p>(iii) satisfies F .1;1/ D 1
Lemma 4.2 Let U1 and U2 be non-empty sets in R and let F W U1 � U2 �! R be
a grounded two-increasing function. Then F is non-decreasing in each argument.
</p>
<p>Definition 4.6 If U1 and U2 have a greatest element maxU1 and maxU2 respec-
</p>
<p>tively, then we say that a function F W U1 � U2 �! R has margins and that the
margins of F are given by:
</p>
<p>F.x/ D F.x;maxU2/ for all x 2 U1 (4.111)
F.y/ D F.maxU1; y/ for all y 2 U2 (4.112)
</p>
<p>Lemma 4.3 Let U1 and U2 be non-empty sets in R and let F W U1 � U2 �! R
be a grounded two-increasing function which has margins. Let .x1; y1/, .x2; y2/ 2
S1 � S2. Then
</p>
<p>jF.x2; y2/ � F.x1; y1/j � jF.x2/ � F.x1/j C jF.y2/ � F.y1/j (4.113)</p>
<p/>
</div>
<div class="page"><p/>
<p>168 4 Multivariate Distributions
</p>
<p>Definition 4.7 A two-dimensional copula is a functionC defined on the unit square
</p>
<p>I 2 D I � I with I D Œ0; 1&#141; such that
(i) for every u 2 I holds: C.u; 0/ D C.0; v/ D 0, i.e. C is grounded.
(ii) for every u1; u2; v1; v2 2 I with u1 � u2 and v1 � v2 holds:
</p>
<p>C.u2; v2/ � C.u2; v1/ � C.u1; v2/C C.u1; v1/ � 0; (4.114)
</p>
<p>i.e. C is 2-increasing.
</p>
<p>(iii) for every u 2 I holds C.u; 1/ D u and C.1; v/ D v.
Informally, a copula is a joint distribution function defined on the unit square Œ0; 1&#141;2
</p>
<p>which has uniformmarginals. That means that if FX1.x1/ and FX2.x2/ are univariate
</p>
<p>distribution functions, then C fFX1.x1/; FX2.x2/g is a two-dimensional distribution
function with marginals FX1.x1/ and FX2.x2/.
</p>
<p>Example 4.25 The functions max.uCv�1; 0/, uv, min.u; v/ can be easily checked
to be copula functions. They are called respectively the minimum, product and
</p>
<p>maximum copula.
</p>
<p>Example 4.26 Consider the function
</p>
<p>CGauss� .u; v/ D ˆ�
˚
ˆ�1.u/; ˆ�1.v/
</p>
<p>�
(4.115)
</p>
<p>D
Z ˆ�11 .u/
</p>
<p>�1
</p>
<p>Z ˆ�12 .v/
</p>
<p>�1
f�.x1; x2/dx2dx1
</p>
<p>where ˆ� is the joint two-dimensional standard normal distribution function with
</p>
<p>correlation coefficient �, while ˆ1 and ˆ2 refer to standard normal cdfs and
</p>
<p>f�.x1; x2/ D
1
</p>
<p>2�
p
1 � �2
</p>
<p>exp
</p>
<p>�
�x
</p>
<p>2
1 � 2�x1x2 C x22
2.1� �2/
</p>
<p>�
(4.116)
</p>
<p>denotes the bivariate normal pdf.
</p>
<p>It is easy to see that CGauss is a copula, the so-called Gaussian or normal copula,
</p>
<p>since it is 2-increasing and
</p>
<p>ˆ�
˚
ˆ�1.u/; ˆ�1.0/
</p>
<p>�
D ˆ�
</p>
<p>˚
ˆ�1.0/;ˆ�1.v/
</p>
<p>�
D 0 (4.117)
</p>
<p>ˆ�
˚
ˆ�1.u/; ˆ�1.1/
</p>
<p>�
D u and ˆ�
</p>
<p>˚
ˆ�1.1/;ˆ�1.v/
</p>
<p>�
D v (4.118)</p>
<p/>
</div>
<div class="page"><p/>
<p>4.7 Copulae 169
</p>
<p>w
</p>
<p>Y
</p>
<p>Z
</p>
<p>Fig. 4.16 Surface plot of the Gumbel&ndash;Hougaard copula, � D 3 MVAghsurface
</p>
<p>A simple and useful way to represent the graph of a copula is the contour diagram
</p>
<p>that is, graphs of its level sets&mdash;the sets in I 2 given by C.u; v/ D a constant.
In Figs. 4.16 and 4.17 we present the countour diagrams of the Gumbel&ndash;Hougard
</p>
<p>copula (Example 4.4) for different values of the copula parameter � .
</p>
<p>For � D 1 the Gumbel&ndash;Hougaard copula reduces to the product copula, i.e.
</p>
<p>C1.u; v/ D &hellip;.u; v/ D uv (4.119)
</p>
<p>For � !1, one finds for the Gumbel&ndash;Hougaard copula:
</p>
<p>C� .u; v/ �! min.u; v/ DM.u; v/ (4.120)
</p>
<p>where M is also a copula such that C.u; v/ � M.u; v/ for an arbitrary copula C .
The copulaM is called the Fr&eacute;chet&ndash;Hoeffding upper bound.
</p>
<p>The two-dimensional function W.u; v/ D max.u C v � 1; 0/ defines a copula
withW.u; v/ � C.u; v/ for any other copula C .W is called the Fr&eacute;chet&ndash;Hoeffding
lower bound.
</p>
<p>In Fig. 4.18 we show an example of Gumbel&ndash;Hougaard copula sampling for fixed
</p>
<p>parameters �1 D 1, �2 D 1 and � D 3.
One can demonstrate the so-called Fr&eacute;chet&ndash;Hoeffding inequality, which we have
</p>
<p>already used in Example 1.3, and which states that each copula function is bounded
</p>
<p>by the minimum and maximum one:
</p>
<p>W.u; v/ D max.uC v � 1; 0/ � C.u; v/ � min.u; v/ DM.u; v/ (4.121)</p>
<p/>
</div>
<div class="page"><p/>
<p>170 4 Multivariate Distributions
</p>
<p>Theta 1.000
</p>
<p>u
</p>
<p>v
</p>
<p> 0.1 
</p>
<p> 0.2 
</p>
<p> 0.3 
</p>
<p> 0.4 
</p>
<p> 0.5 
</p>
<p> 0.6 
</p>
<p> 0.7 
</p>
<p> 0
.8
</p>
<p> 
</p>
<p>0.0 0.4 0.8
</p>
<p>0
.0
</p>
<p>0
.4
</p>
<p>0
.8
</p>
<p>Theta 2.000
</p>
<p>u
</p>
<p>v
 0.1 
</p>
<p> 0.2 
</p>
<p> 0.3 
</p>
<p> 0.4 
</p>
<p> 0.5 
</p>
<p> 0.6 
</p>
<p> 0.7 
</p>
<p> 0.8 
</p>
<p> 0.9 
</p>
<p>0.0 0.4 0.8
</p>
<p>0
.0
</p>
<p>0
.4
</p>
<p>0
.8
</p>
<p>Theta 3.000
</p>
<p>u
</p>
<p>v
</p>
<p> 0.1 
</p>
<p> 0.2 
</p>
<p> 0.3 
</p>
<p> 0.4 
</p>
<p> 0.5 
</p>
<p> 0.6 
</p>
<p> 0.7 
</p>
<p> 0.8 
</p>
<p> 0.9 
</p>
<p>0.0 0.4 0.8
</p>
<p>0
.0
</p>
<p>0
.4
</p>
<p>0
.8
</p>
<p>Theta 10.000
</p>
<p>u
</p>
<p>v
</p>
<p> 0.1 
</p>
<p> 0.2 
</p>
<p> 0.3 
</p>
<p> 0.4 
</p>
<p> 0.5 
</p>
<p> 0.6 
</p>
<p> 0.7 
</p>
<p> 0.8 
</p>
<p>0.0 0.4 0.8
</p>
<p>0
.0
</p>
<p>0
.4
</p>
<p>0
.8
</p>
<p>Theta 30.000
</p>
<p>u
</p>
<p>v
</p>
<p> 0.1 
</p>
<p> 0.2 
</p>
<p> 0.3 
</p>
<p> 0.4 
</p>
<p> 0.5 
</p>
<p> 0.6 
</p>
<p> 0.7 
</p>
<p> 0.8 
</p>
<p>0.0 0.4 0.8
</p>
<p>0
.0
</p>
<p>0
.4
</p>
<p>0
.8
</p>
<p>Theta 100.000
</p>
<p>u
</p>
<p>v
</p>
<p> 0.1 
</p>
<p> 0.2 
</p>
<p> 0.3 
</p>
<p> 0.4 
</p>
<p> 0.5 
</p>
<p> 0.6 
</p>
<p> 0.7 
</p>
<p> 0.8 
</p>
<p>0.0 0.4 0.8
</p>
<p>0
.0
</p>
<p>0
.4
</p>
<p>0
.8
</p>
<p>Fig. 4.17 Contour plots of the Gumbel&ndash;Hougard copula MVAghcontour
</p>
<p>The full relationship between copula and joint cdf depends on Sklar theorem.
</p>
<p>Example 4.27 Let us verify that the Gaussian copula satisfies Sklar&rsquo;s theorem in
</p>
<p>both directions. On the one side, let
</p>
<p>F.x1; x2/ D
Z x1
�1
</p>
<p>Z x2
�1
</p>
<p>1
</p>
<p>2�
p
1� �2
</p>
<p>exp
</p>
<p>�
�u
</p>
<p>2
1 � 2�u1u2 C u22
2.1� �2/
</p>
<p>�
du2du1:
</p>
<p>(4.122)
</p>
<p>be a two-dimensional normal distribution function with standard normal cdf&rsquo;s
</p>
<p>FX1.x1/ and FX2.x2/. Since FX1.x1/ and FX2.x2/ are continuous, a unique copula</p>
<p/>
</div>
<div class="page"><p/>
<p>4.7 Copulae 171
</p>
<p>Fig. 4.18 10,000-sample
output for �1 D 1, �2 D 1,
� D 3 MVAsample1000
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●● ●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
● ●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ● ●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>● ●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
●
</p>
<p>●
●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●●
●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>&minus;4 &minus;2 0 2 4
</p>
<p>&minus;
4
</p>
<p>&minus;
2
</p>
<p>0
2
</p>
<p>4
</p>
<p>u
</p>
<p>v
</p>
<p>C exists such that for all x1, x2 2 R
2
a two-dimensional distribution function can
</p>
<p>be written as a copula in FX1.x1/ and FX2.x2/:
</p>
<p>F .x1; x2/ D C fˆX1 .x1/ ; ˆX2 .x2/g (4.123)
</p>
<p>The Gaussian copula satisfies the above equality, therefore it is the unique copula
</p>
<p>mentioned in Sklar&rsquo;s theorem. This proves that the Gaussian copula, together with
</p>
<p>Gaussian marginals, gives the two-dimensional normal distribution.
</p>
<p>Conversely, if C is a copula and FX1 and FX2 are standard normal distribution
</p>
<p>functions, then
</p>
<p>C fFX1.x1/; FX2.x2/g D
Z ��11 fFX1 .x1/g
</p>
<p>�1
</p>
<p>Z ��12 fFX2 .x2/g
</p>
<p>�1
</p>
<p>1
</p>
<p>2�
p
1 � �2
</p>
<p>� exp
�
�x
</p>
<p>2
1 � 2�x1x2 C x22
2.1� �2/
</p>
<p>�
dx2dx1 (4.124)
</p>
<p>is evidently a joint (two-dimensional) distribution function. Its margins are
</p>
<p>C fFX1.x1/; FX2.C1/g D ˆ�
�
ˆ�1 fFX1.x1/g ;C1
</p>
<p>�
D FX1.x1/ (4.125)
</p>
<p>C fFX1.C1/; FX2.x2/g D ˆ�
�
C1; ˆ�1 fFX2.x2/g
</p>
<p>�
D FX2.x2/ (4.126)
</p>
<p>The following proposition shows one attractive feature of the copula represen-
</p>
<p>tation of dependence, i.e. that the dependence structure described by a copula</p>
<p/>
</div>
<div class="page"><p/>
<p>172 4 Multivariate Distributions
</p>
<p>is invariant under increasing and continuous transformations of the marginal
</p>
<p>distributions.
</p>
<p>Theorem 4.13 If .X1; X2/ have copula C and set g1; g2 two continuously increas-
</p>
<p>ing functions, then fg1 .X1/ ; g2 .X2/g have the copula C, too.
Example 4.28 Independence implies that the product of the cdf&rsquo;s FX1 and FX2
equals the joint distribution function F , i.e.:
</p>
<p>F.x1; x2/ D FX1.x1/FX2.x2/: (4.127)
</p>
<p>Thus, we obtain the independence or product copula C D &hellip;.u; v/ D uv.
While it is easily understood how a product copula describes an independence
</p>
<p>relationship, the converse is also true. Namely, the joint distribution function of two
</p>
<p>independent random variables can be interpreted as a product copula. This concept
</p>
<p>is formalised in the following theorem:
</p>
<p>Theorem 4.14 Let X1 and X2 be random variables with continuous distribution
</p>
<p>functions FX1 and FX2 and the joint distribution function F . Then X1 and X2 are
</p>
<p>independent if and only if CX1;X2 D &hellip;.
Example 4.29 Let us consider the Gaussian copula for the case � D 0, i.e. vanishing
correlation. In this case the Gaussian copula becomes
</p>
<p>CGauss0 .u; v/ D
Z ˆ�11 .u/
</p>
<p>�1
'.x1/dx1
</p>
<p>Z ˆ�12 .v/
</p>
<p>�1
'.x2/dx2
</p>
<p>D uv (4.128)
D &hellip;.u; v/:
</p>
<p>The following theorem, which follows directly from Lemma 4.3, establishes the
</p>
<p>continuity of copulae .
</p>
<p>Theorem 4.15 Let C be a copula. Then for any u1; v1; u2; v2 2 I holds
</p>
<p>jC.u2; v2/ � C.u1; v1/j � ju2 � u1j C jv2 � v1j (4.129)
</p>
<p>From (4.129) it follows that every copula C is uniformly continuous on its
</p>
<p>domain.
</p>
<p>A further important property of copulae concerns the partial derivatives of a
</p>
<p>copula with respect to its variables:
</p>
<p>Theorem 4.16 Let C.u; v/ be a copula. For any u 2 I , the partial derivative @C.u;v/
@v
</p>
<p>exists for almost all u 2 I . For such u and v one has:
</p>
<p>@C.u; v/
</p>
<p>@v
2 I (4.130)</p>
<p/>
</div>
<div class="page"><p/>
<p>4.7 Copulae 173
</p>
<p>The analogous statement is true for the partial derivative
@C.u;v/
</p>
<p>@u
:
</p>
<p>@C.u; v/
</p>
<p>@u
2 I (4.131)
</p>
<p>Moreover, the functions
</p>
<p>u 7! Cv.u/ defD @C.u; v/=@v and
</p>
<p>v 7! Cu.v/ defD @C.u; v/=@u
</p>
<p>are defined and non-increasing almost everywhere on I .
</p>
<p>Until now, we have considered copulae only in a two-dimensional setting. Let
</p>
<p>us now extend this concept to the d -dimensional case, for a random variable in Rd
</p>
<p>with d � 1.
Let U1; U2; : : : ; Ud be non-empty sets in R and consider the function F W U1 �
</p>
<p>U2� � � � �Ud �! R. For a D .a1; a2; : : : ; ad / and b D .b1; b2; : : : ; bd / with a � b
(i.e. ak � bk for all k) let B D Œa; b&#141; D Œa1; b1&#141; � Œa2; b2&#141; � � � � � Œan; bn&#141; be the
d -box with vertices c D .c1; c2; : : : ; cd /. It is obvious that each ck is either equal to
ak or to bk .
</p>
<p>Definition 4.8 The F -volume of a d -box B D Œa; b&#141; D Œa1; b1&#141; � Œa2; b2&#141; � � � � �
Œad ; bd &#141; � U1 � U2 � � � � � Ud is defined as follows:
</p>
<p>VF .B/ D
dX
</p>
<p>kD1
sign.ck/F.ck/ (4.132)
</p>
<p>where sign.ck/ D 1, if ck D ak for even k and sign.ck/ D �1, if ck D ak for odd k.
Example 4.30 For the case d D 3, theF -volume of a 3-boxB D Œa; b&#141; D Œx1; x2&#141;�
Œy1; y2&#141; � Œz1; z2&#141; is defined as:
</p>
<p>VF .B/ D F.x2; y2; z2/ � F.x2; y2; z1/ � F.x2; y1; z2/� F.x1; y2; z2/
CF.x2; y1; z1/C F.x1; y2; z1/C F.x1; y1; z2/ � F.x1; y1; z1/
</p>
<p>Definition 4.9 F is said to be a d -increasing function if for all d -boxes B with
</p>
<p>vertices in U1 � U2 � � � � � Ud holds:
</p>
<p>VF .B/ � 0 (4.133)</p>
<p/>
</div>
<div class="page"><p/>
<p>174 4 Multivariate Distributions
</p>
<p>Definition 4.10 If U1; U2; : : : ; Ud have a smallest element minU1;minU2; : : :,
</p>
<p>minUd respectively, then we say that a function F W U1 � U2 � � � � � Ud �! R
is grounded if :
</p>
<p>F.x/ D 0 for all x 2 U1 � U2 � � � � � Ud (4.134)
</p>
<p>such that xk D minUk for at least one k.
The lemmas, which we presented for the two-dimensional case, have analogous
</p>
<p>multivariate versions, see Nelsen (1999).
</p>
<p>Definition 4.11 A d -dimensional copula (or d -copula) is a function C defined on
</p>
<p>the unit d -cube I d D I � I � � � � � I such that
(i) for every u 2 I d holds: C.u/ D 0, if at least one coordinate of u is equal to 0;
</p>
<p>i.e. C is grounded.
</p>
<p>(ii) for every a; b 2 I d with a � b holds:
</p>
<p>VC .Œa; b&#141;/ � 0I (4.135)
</p>
<p>i.e. C is 2-increasing.
</p>
<p>(iii) for every u 2 I d holds: C.u/ D uk, if all coordinates of u are 1 except uk .
Analogously to the two-dimensional setting, let us state the Sklar&rsquo;s theorem for
</p>
<p>the d -dimensional case.
</p>
<p>Theorem 4.17 (Sklar&rsquo;s Theorem in d -Dimensional Case) Let F be a d -
</p>
<p>dimensional distribution function with marginal distribution functions
</p>
<p>FX1 ; FX2 ; : : : ; FXd . Then a d -copula C exists such that for all x1; : : : ; xd 2 R
d
:
</p>
<p>F .x1; x2; : : : ; xd / D C fFX1 .x1/ ; FX2 .x2/ ; : : : ; FXd .xd /g (4.136)
</p>
<p>Moreover, if FX1 ; FX2 ; : : : ; FXd are continuous then C is unique. Otherwise C is
</p>
<p>uniquely determined on the Cartesian product Im.FX1/�Im.FX2/�� � ��Im.FXd /.
Conversely, if C is a copula and FX1 ; FX2 ; : : : ; FXd are distribution functions
</p>
<p>then F defined by (4.136) is a d -dimensional distribution function with marginals
</p>
<p>FX1 ; FX2 ; : : : ; FXd .
</p>
<p>In order to illustrate the d -copulae we present the following examples:
</p>
<p>Example 4.31 Let ˆ denote the univariate standard normal distribution function
</p>
<p>and ˆ&dagger;;d the d -dimensional standard normal distribution function with correlation
</p>
<p>matrix &dagger;. Then the function
</p>
<p>CGauss� .u; &dagger;/ D ˆ&dagger;;d
˚
ˆ�1.u1/; : : : ; ˆ
</p>
<p>�1.ud /
�
</p>
<p>D
Z ��11 .ud /
</p>
<p>�1
: : :
</p>
<p>Z ��12 .u1/
</p>
<p>�1
f&dagger;.x1; : : : ; xn/dx1 : : : dxd (4.137)</p>
<p/>
</div>
<div class="page"><p/>
<p>4.7 Copulae 175
</p>
<p>is the d -dimensional Gaussian or normal copula with correlation matrix &dagger;. The
</p>
<p>function
</p>
<p>f�.x1; : : : ; xd / D
1p
</p>
<p>det.&dagger;/
</p>
<p>� exp
n
� .ˆ�1.u1/;:::;ˆ�1.ud //&gt;.&dagger;�1�Id /.ˆ�1.u1/;:::;ˆ�1.ud //
</p>
<p>2
</p>
<p>o (4.138)
</p>
<p>is a copula density function. The copula dependence parameter ˛ is the collection of
</p>
<p>all unknown correlation coefficients in &dagger;. If ˛ &curren; 0, then the corresponding normal
copula allows to generate joint symmetric dependence. However, it is not possible
</p>
<p>to model a tail dependence, i.e. joint extreme events have a zero probability.
</p>
<p>Example 4.32 Let us consider the following function
</p>
<p>CGH� .u1; : : : ; ud / D exp
</p>
<p>2
64�
</p>
<p>8
&lt;
:
</p>
<p>dX
</p>
<p>jD1
</p>
<p>�
� log uj
</p>
<p>��
9
=
;
</p>
<p>1=�
3
75 (4.139)
</p>
<p>One recognise this function is as the d -dimensional Gumbel&ndash;Hougaard copula
</p>
<p>function. Unlike the Gaussian copula, the copula (4.139) can generate an upper tail
</p>
<p>dependence.
</p>
<p>Example 4.33 As in the two-dimensional setting, let us consider the d -dimensional
</p>
<p>Gumbel&ndash;Hougaard copula for the case � D 1. In this case the Gumbel&ndash;Hougaard
copula reduces to the d -dimensional product copula, i.e.
</p>
<p>C1.u1; : : : ; ud / D
dY
</p>
<p>jD1
uj D &hellip;d .u/ (4.140)
</p>
<p>The extension of the two-dimensional copula M , which one gets from the d -
</p>
<p>dimensional Gumbel&ndash;Hougaard copula for � !1 is denotedM d .u/:
</p>
<p>C� .u1; : : : ud / �! min.u1; : : : ; ud / DM d .u/ (4.141)
</p>
<p>The d -dimensional function
</p>
<p>W d .u/ D max.u1 C u2 C � � � C ud � d C 1; 0/ (4.142)
</p>
<p>defines a copula with W.u/ � C.u/ for any other d -dimensional copula function
C.u/.W d .u/ is the Fr&eacute;chet&ndash;Hoeffding lower bound in the d -dimensional case.</p>
<p/>
</div>
<div class="page"><p/>
<p>176 4 Multivariate Distributions
</p>
<p>The functionsM d and &hellip;d are d -copulae for all d � 2, whereas the function W d
fails to be a d -copula for any d &gt; 2 (Nelsen, 1999). However, the d -dimensional
</p>
<p>version of the Fr&eacute;chet&ndash;Hoeffding inequality can be written as follows:
</p>
<p>W d .u/ � C.u/ �M d .u/ (4.143)
</p>
<p>As we have already mentioned, copula functions have been widely applied in
</p>
<p>empirical finance.
</p>
<p>Summary
</p>
<p>,! The cumulative distribution function (cdf) is defined as F.x/ D
P.X &lt; x/.
</p>
<p>,! If a probability density function (pdf) f exists then F.x/ DR x
�1 f .u/du.
</p>
<p>,! The pdf integrates to one, i.e.
R1
�1 f .x/dx D 1.
</p>
<p>4.8 Bootstrap
</p>
<p>Recall that we need large sample sizes in order to sufficiently approximate the
</p>
<p>critical values computable by the CLT. Here large means n &gt; 50 for one-
</p>
<p>dimensional data. How can we construct confidence intervals in the case of smaller
</p>
<p>sample sizes? One way is to use a method called the Bootstrap. The Bootstrap
</p>
<p>algorithm uses the data twice:
</p>
<p>1. estimate the parameter of interest,
</p>
<p>2. simulate from an estimated distribution to approximate the asymptotic distribu-
</p>
<p>tion of the statistics of interest.
</p>
<p>In detail, bootstrap works as follows. Consider the observations x1; : : : ; xn of the
</p>
<p>sample X1; : : : ; Xn and estimate the empirical distribution function (EDF) Fn. In
</p>
<p>the case of one-dimensional data
</p>
<p>Fn.x/ D
1
</p>
<p>n
</p>
<p>nX
</p>
<p>iD1
I.Xi � x/: (4.144)
</p>
<p>This is a step function which is constant between neighbouring data points.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.8 Bootstrap 177
</p>
<p>-3 -2 -1 0 1 2 3
0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>X
</p>
<p>E
D
</p>
<p>F
(X
</p>
<p>),
 C
</p>
<p>D
F
</p>
<p>(X
)
</p>
<p>EDF and CDF, n=100
</p>
<p>Fig. 4.19 The standard normal cdf (thick line) and the empirical distribution function (thin line)
for n D 100 MVAedfnormal
</p>
<p>&minus;4 &minus;2 0 2 4
0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>X
</p>
<p>E
D
</p>
<p>F
(X
</p>
<p>),
 C
</p>
<p>D
F
</p>
<p>(X
)
</p>
<p>EDF and CDF,   n=1000
</p>
<p>Fig. 4.20 The standard normal cdf (thick line) and the empirical distribution function (thin line)
for n D 1;000 MVAedfnormal
</p>
<p>Example 4.34 Suppose that we have n D 100 standard normal N.0; 1/ data points
Xi , i D 1; : : : ; n. The cdf of X is ˆ.x/ D
</p>
<p>R x
�1 '.u/du and is shown in Fig. 4.19 as
</p>
<p>the thin, solid line. The EDF is displayed as a thick step function line. Figure 4.20
</p>
<p>shows the same setup for n D 1;000 observations.</p>
<p/>
</div>
<div class="page"><p/>
<p>178 4 Multivariate Distributions
</p>
<p>Now draw with replacement a new sample from this empirical distribution. That
</p>
<p>is we sample with replacement n� observations X�1 ; : : : ; X
�
n� from the original
</p>
<p>sample. This is called a Bootstrap sample. Usually one takes n� D n.
Since we sample with replacement, a single observation from the original sample
</p>
<p>may appear several times in the Bootstrap sample. For instance, if the original
</p>
<p>sample consists of the three observations x1; x2; x3, then a Bootstrap sample might
</p>
<p>look like X�1 D x3; X�2 D x2; X�3 D x3: Computationally, we find the Bootstrap
sample by using a uniform random number generator to draw from the indices
</p>
<p>1; 2; : : : ; n of the original samples.
</p>
<p>The Bootstrap observations are drawn randomly from the empirical distribution,
</p>
<p>i.e. the probability for each original observation to be selected into the Bootstrap
</p>
<p>sample is 1=n for each draw. It is easy to compute that
</p>
<p>EFn.X
�
i / D
</p>
<p>1
</p>
<p>n
</p>
<p>nX
</p>
<p>iD1
xi D x:
</p>
<p>This is the expected value given that the cdf is the original mean of the sample
</p>
<p>x1; : : : ; xn. The same holds for the variance, i.e.
</p>
<p>VFn.X
�
i / D b�2;
</p>
<p>whereb�2 D n�1P.xi � x/2. The cdf of the bootstrap observations is defined as in
(4.144). Figure 4.21 shows the cdf of the n D 100 original observations as a solid
line and two bootstrap cdf&rsquo;s as thin lines.
</p>
<p>The CLT holds for the bootstrap sample. Analogously to Corollary 4.1 we have
</p>
<p>the following corollary.
</p>
<p>Fig. 4.21 The cdf Fn (thick
line) and two bootstrap cdf&rsquo;s
F �n (thin lines)
MVAedfbootstrap
</p>
<p>-3 -2 -1 0 1 2 3
0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>EDF and 2 bootstrap EDF's, n=100
</p>
<p>X
</p>
<p>e
d
</p>
<p>fs
[1
</p>
<p>..
3
</p>
<p>](
x
</p>
<p>)</p>
<p/>
</div>
<div class="page"><p/>
<p>4.9 Exercises 179
</p>
<p>Corollary 4.2 If X�1 ; : : : ; X
�
n is a bootstrap sample from X1; : : : ; Xn, then the
</p>
<p>distribution of
</p>
<p>p
n
</p>
<p>�
x� � x
b��
</p>
<p>�
</p>
<p>also becomes N.0; 1/ asymptotically, where x� D n�1
Pn
</p>
<p>iD1X
�
i and .b��/2 D
</p>
<p>n�1
Pn
</p>
<p>iD1.X
�
i � x�/2.
</p>
<p>How do we find a confidence interval for � using the Bootstrap method? Recall
</p>
<p>that the quantile u1�˛=2 might be bad for small sample sizes because the true
</p>
<p>distribution of
p
n
�
x��
O�
</p>
<p>�
might be far away from the limit distributionN.0; 1/. The
</p>
<p>Bootstrap idea enables us to &ldquo;simulate&rdquo; this distribution by computing
p
n
�
x��x
b��
</p>
<p>�
</p>
<p>for many Bootstrap samples . In this way we can estimate an empirical (1 � ˛=2)-
quantile u�1�˛=2. The bootstrap improved confidence interval is then
</p>
<p>C �1�˛ D
�
x � O�p
</p>
<p>n
u�1�˛=2; x C
</p>
<p>O�p
n
u�1�˛=2
</p>
<p>�
:
</p>
<p>By Corollary 4.2 we have
</p>
<p>P.� 2 C �1�˛/ �! 1 � ˛ as n!1;
</p>
<p>but with an improved speed of convergence, see Hall (1992).
</p>
<p>Summary
</p>
<p>,! For small sample sizes the bootstrap improves the precision of the
confidence interval.
</p>
<p>,! The bootstrap distribution L
˚p
n.x� � x/= O��
</p>
<p>�
converges to the
</p>
<p>same asymptotic limit as the distribution L
˚p
n.x� � x/= O�
</p>
<p>�
.
</p>
<p>4.9 Exercises
</p>
<p>Exercise 4.1 Assume that the random vector Y has the following normal distribu-
</p>
<p>tion: Y � Np.0; I/. Transform it according to (4.49) to create X � N.�;&dagger;/ with
mean � D .3; 2/&gt; and &dagger; D
</p>
<p>�
1
�1:5
</p>
<p>�1:5
4
</p>
<p>�
. How would you implement the resulting
</p>
<p>formula on a computer?</p>
<p/>
</div>
<div class="page"><p/>
<p>180 4 Multivariate Distributions
</p>
<p>Exercise 4.2 Prove Theorem 4.7 using Theorem 4.5.
</p>
<p>Exercise 4.3 Suppose that X has mean zero and covariance &dagger; D
�
1
0
0
2
</p>
<p>�
. Let Y D
</p>
<p>X1CX2. Write Y as a linear transformation, i.e. find the transformation matrix A.
Then compute Var.Y / via (4.26). Can you obtain the result in another fashion?
</p>
<p>Exercise 4.4 Calculate the mean and the variance of the estimate Ǒ in (3.50).
Exercise 4.5 Compute the conditional moments E.X2jx1/ and E.X1jx2/ for the pdf
of Example 4.5.
</p>
<p>Exercise 4.6 Prove the relation (4.28).
</p>
<p>Exercise 4.7 Prove the relation (4.29).
</p>
<p>Hint: Note that Var.E.X2jX1// D E.E.X2jX1/ E.X&gt;2 jX1// � E.X2/ E.X&gt;2 // and
that
</p>
<p>E.Var.X2jX1// D EŒE.X2X&gt;2 jX1/� E.X2jX1/ E.X&gt;2 jX1/&#141;.
Exercise 4.8 Compute (4.46) for the pdf of Example 4.5.
</p>
<p>Exercise 4.9
</p>
<p>Show that fY .y/ D
(
1
2
y1 � 14y2 0 � y1 � 2; jy2j � 1 � j1� y1j
0 otherwise
</p>
<p>is a pdf:
</p>
<p>Exercise 4.10 Compute (4.46) for a two-dimensional standard normal distribution.
</p>
<p>Show that the transformed random variables Y1 and Y2 are independent. Give a
</p>
<p>geometrical interpretation of this result based on iso-distance curves.
</p>
<p>Exercise 4.11 Consider the Cauchy distribution which has no moment, so that the
</p>
<p>CLT cannot be applied. Simulate the distribution of x (for different n&rsquo;s). What can
</p>
<p>you expect for n!1?
Hint: The Cauchy distribution can be simulated by the quotient of two indepen-
</p>
<p>dent standard normally distributed random variables.
</p>
<p>Exercise 4.12 A European car company has tested a new model and reports the
</p>
<p>consumption of petrol (X1/ and oil (X2). The expected consumption of petrol is 8 l
</p>
<p>per 100 km (�1) and the expected consumption of oil is 1 l per 10,000km (�2).
</p>
<p>The measured consumption of petrol is 8.1 l per 100 km (x1) and the measured
</p>
<p>consumption of oil is 1.1 l per 10,000 km (x2). The asymptotic distribution ofp
n
n�
x1
x2
</p>
<p>�
�
�
�1
�2
</p>
<p>�o
is N
</p>
<p>��
0
0
</p>
<p>�
;
�
0:1
0:05
</p>
<p>0:05
0:1
</p>
<p>��
.
</p>
<p>For the American market the basic measuring units are miles (1 mile � 1.6 km)
and gallons (1 gallon � 3.8 l). The consumptions of petrol (Y1) and oil (Y2) are
usually reported in miles per gallon. Can you express y1 and y2 in terms of x1 and
</p>
<p>x2? Recompute the asymptotic distribution for the American market.
</p>
<p>Exercise 4.13 Consider the pdf f .x1; x2/ D e�.x1Cx2/; x1; x2 &gt; 0 and let U1 D
X1 CX2 and U2 D X1 � X2. Compute f .u1; u2/.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.9 Exercises 181
</p>
<p>Exercise 4.14 Consider the pdf&rsquo;s
</p>
<p>f .x1; x2/ D 4x1x2e�x
2
1 x1; x2 &gt; 0;
</p>
<p>f .x1; x2/ D 1 0 &lt; x1; x2 &lt; 1 and x1 C x2 &lt; 1
f .x1; x2/ D 12e�x1 x1 &gt; jx2j:
</p>
<p>For each of these pdf&rsquo;s compute E.X/;Var.X/;E.X1jX2/;E.X2jX1/;Var.X1jX2/
and Var.X2jX1/:
</p>
<p>Exercise 4.15 Consider the pdf f .x1; x2/ D 34x
� 12
1 ; 0 &lt; x1 &lt; x2 &lt; 1. Compute
</p>
<p>P.X1 &lt; 0:25/; P.X2 &lt; 0:25/ and P.X2 &lt; 0:25jX1 &lt; 0:25/:
Exercise 4.16 Consider the pdf f .x1; x2/ D 12� ; 0 &lt; x1 &lt; 2�; 0 &lt; x2 &lt; 1:
Let U1 D sinX1
</p>
<p>p�2 logX2 and U2 D cosX1
p�2 logX2. Compute f .u1; u2/.
</p>
<p>Exercise 4.17 Consider f .x1; x2; x3/ D k.x1 C x2x3/I 0 &lt; x1; x2; x3 &lt; 1:
(a) Determine k so that f is a valid pdf of .X1; X2; X3/ D X:
(b) Compute the .3 � 3/ matrix &dagger;X .
(c) Compute the .2 � 2/ matrix of the conditional variance of .X2; X3/ given
</p>
<p>X1 D x1.
</p>
<p>Exercise 4.18 Let X � N2
��
</p>
<p>1
2
</p>
<p>�
;
</p>
<p>�
2 a
</p>
<p>a 2
</p>
<p>��
.
</p>
<p>(a) Represent the contour ellipses for a D 0I � 1
2
I C 1
</p>
<p>2
I 1:
</p>
<p>(b) For a D 1
2
find the regions of X centred on � which cover the area of the true
</p>
<p>parameter with probability 0:90 and 0:95.
</p>
<p>Exercise 4.19 Consider the pdf
</p>
<p>f .x1; x2/ D
1
</p>
<p>8x2
e
�
�
x1
2x2
C x24
</p>
<p>�
</p>
<p>x1; x2 &gt; 0:
</p>
<p>Compute f .x2/ and f .x1jx2/. Also give the best approximation of X1 by a function
of X2. Compute the variance of the error of the approximation.
</p>
<p>Exercise 4.20 Prove Theorem 4.6.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 5
</p>
<p>Theory of the Multinormal
</p>
<p>In the preceding chapter we saw how the multivariate normal distribution comes into
</p>
<p>play in many applications. It is useful to know more about this distribution, since
</p>
<p>it is often a good approximate distribution in many situations. Another reason for
</p>
<p>considering the multinormal distribution relies on the fact that it has many appealing
</p>
<p>properties: it is stable under linear transforms, zero correlation corresponds to
</p>
<p>independence, the marginals and all the conditionals are also multivariate normal
</p>
<p>variates, etc. The mathematical properties of the multinormal make analyses much
</p>
<p>simpler.
</p>
<p>In this chapter we will first concentrate on the probabilistic properties of
</p>
<p>the multinormal, then we will introduce two &ldquo;companion&rdquo; distributions of the
</p>
<p>multinormal which naturally appear when sampling from a multivariate normal
</p>
<p>population: the Wishart and the Hotelling distributions. The latter is particularly
</p>
<p>important for most of the testing procedures proposed in Chap. 7.
</p>
<p>5.1 Elementary Properties of the Multinormal
</p>
<p>Let us first summarise some properties which were already derived in the previous
</p>
<p>chapter.
</p>
<p>&bull; The pdf of X � Np.�;&dagger;/ is
</p>
<p>f .x/ D j2�&dagger;j�1=2 exp
�
�1
2
.x � �/&gt;&dagger;�1.x � �/
</p>
<p>�
: (5.1)
</p>
<p>&copy; Springer-Verlag Berlin Heidelberg 2015
W.K. H&auml;rdle, L. Simar, Applied Multivariate Statistical Analysis,
DOI 10.1007/978-3-662-45171-7_5
</p>
<p>183</p>
<p/>
</div>
<div class="page"><p/>
<p>184 5 Theory of the Multinormal
</p>
<p>The expectation is E.X/ D �, the covariance can be calculated as
Var.X/ D E.X � �/.X � �/&gt; D &dagger;.
</p>
<p>&bull; Linear transformations turn normal random variables into normal random
</p>
<p>variables. If X � Np.�;&dagger;/ and A.p � p/; c 2 Rp , then Y D AX C c is
p-variate Normal, i.e.
</p>
<p>Y � Np.A�C c;A&dagger;A&gt;/: (5.2)
</p>
<p>&bull; If X � Np.�;&dagger;/, then the Mahalanobis transformation is
</p>
<p>Y D &dagger;�1=2.X � �/ � Np.0; Ip/ (5.3)
</p>
<p>and it holds that
</p>
<p>Y &gt;Y D .X � �/&gt; &dagger;�1.X � �/ � �2p: (5.4)
</p>
<p>Often it is interesting to partition X into sub-vectors X1 and X2. The following
</p>
<p>theorem tells us how to correct X2 to obtain a vector which is independent of X1.
</p>
<p>Theorem 5.1 Let X D
�
X1
X2
</p>
<p>�
� Np.�;&dagger;/, X1 2 Rr , X2 2 Rp�r . Define X2:1 D
</p>
<p>X2 �&dagger;21&dagger;�111 X1 from the partitioned covariance matrix
</p>
<p>&dagger; D
�
&dagger;11 &dagger;12
&dagger;21 &dagger;22
</p>
<p>�
:
</p>
<p>Then
</p>
<p>X1 � Nr.�1; &dagger;11/; (5.5)
X2:1 � Np�r .�2:1; &dagger;22:1/ (5.6)
</p>
<p>are independent with
</p>
<p>�2:1 D �2 �&dagger;21&dagger;�111 �1; &dagger;22:1 D &dagger;22 �&dagger;21&dagger;�111 &dagger;12: (5.7)
</p>
<p>Proof
</p>
<p>X1 D AX with A D . Ir ; 0 /
X2:1 D BX with B D . �&dagger;21&dagger;�111 ; Ip�r /:</p>
<p/>
</div>
<div class="page"><p/>
<p>5.1 Elementary Properties of the Multinormal 185
</p>
<p>Then, by (5.2) X1 and X2:1 are both normal. Note that
</p>
<p>Cov.X1; X2:1/ D A&dagger;B&gt; D
</p>
<p>0
BB@
</p>
<p>1 0
: : :
</p>
<p>0 1
</p>
<p>0
</p>
<p>1
CCA
�
&dagger;11 &dagger;12
&dagger;21 &dagger;22
</p>
<p>�
</p>
<p>0
BBBBBBB@
</p>
<p>.�&dagger;21&dagger;�111 /&gt;
</p>
<p>1 0
: : :
</p>
<p>0 1
</p>
<p>1
CCCCCCCA
;
</p>
<p>A&dagger; D .Ir 0/
�
&dagger;11 &dagger;12
&dagger;21 &dagger;22
</p>
<p>�
D .&dagger;11 &dagger;12/ ;
</p>
<p>hence, A&dagger;B&gt; D .&dagger;11 &dagger;12/
 �
�&dagger;21&dagger;�111
</p>
<p>�&gt;
Ip�r
</p>
<p>!
</p>
<p>D
�
�&dagger;11
</p>
<p>�
&dagger;21&dagger;
</p>
<p>�1
11
</p>
<p>�&gt; C&dagger;12
�
:
</p>
<p>Recall that &dagger;21 D .&dagger;12/&gt;. Hence A&dagger;B&gt; D �&dagger;11&dagger;�111 &dagger;12 C&dagger;12 � 0.
Using (5.2) again we also have the joint distribution of (X1; X2:1), namely
</p>
<p>�
X1
</p>
<p>X2:1
</p>
<p>�
D
�
A
</p>
<p>B
</p>
<p>�
X � Np
</p>
<p>��
�1
</p>
<p>�2:1
</p>
<p>�
;
</p>
<p>�
&dagger;11 0
</p>
<p>0 &dagger;22:1
</p>
<p>��
:
</p>
<p>With this block diagonal structure of the covariance matrix, the joint pdf of
</p>
<p>(X1; X2:1) can easily be factorised into
</p>
<p>f .x1; x2:1/ D j2�&dagger;11j�
1
2 exp
</p>
<p>�
�1
2
.x1 � �1/&gt;&dagger;�111 .x1 � �1/
</p>
<p>�
</p>
<p>�j2�&dagger;22:1j�
1
2 exp
</p>
<p>�
�1
2
.x2:1 � �2:1/&gt;&dagger;�122:1.x2:1 � �2:1/
</p>
<p>�
</p>
<p>from which the independence between X1 and X2:1 follows. ut
The next two corollaries are direct consequences of Theorem 5.1.
</p>
<p>Corollary 5.1 Let X D
�
X1
</p>
<p>X2
</p>
<p>�
� Np.�;&dagger;/, &dagger; D
</p>
<p>�
&dagger;11 &dagger;12
</p>
<p>&dagger;21 &dagger;22
</p>
<p>�
. &dagger;12 D 0 if and
</p>
<p>only if X1 is independent of X2.
</p>
<p>The independence of two linear transforms of a multinormalX can be shown via
</p>
<p>the following corollary.
</p>
<p>Corollary 5.2 If X � Np.�;&dagger;/ and given some matrices A and B , then AX and
BX are independent if and only if A&dagger;B&gt; D 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>186 5 Theory of the Multinormal
</p>
<p>The following theorem is also useful. It generalises Theorem 4.6. The proof is
</p>
<p>left as an exercise.
</p>
<p>Theorem 5.2 If X � Np.�;&dagger;/, A.q � p/, c 2 Rq and q � p, then Y D AX C c
is a q-variate Normal, i.e.
</p>
<p>Y � Nq.A�C c;A&dagger;A&gt;/:
</p>
<p>The conditional distribution of X2 given X1 is given by the next theorem.
</p>
<p>Theorem 5.3 The conditional distribution of X2 given X1 D x1 is normal with
mean �2 C&dagger;21&dagger;�111 .x1 � �1/ and covariance&dagger;22:1, i.e.
</p>
<p>.X2 j X1 D x1/ � Np�r .�2 C&dagger;21&dagger;�111 .x1 � �1/;&dagger;22:1/: (5.8)
</p>
<p>Proof SinceX2 D X2:1C&dagger;21&dagger;�111 X1, for a fixed value ofX1 D x1,X2 is equivalent
to X2:1 plus a constant term:
</p>
<p>.X2jX1 D x1/ D .X2:1 C&dagger;21&dagger;�111 x1/;
</p>
<p>which has the normal distribution N.�2:1 C&dagger;21&dagger;�111 x1; &dagger;22:1/. ut
Note that the conditional mean of .X2 j X1/ is a linear function ofX1 and that the
</p>
<p>conditional variance does not depend on the particular value ofX1. In the following
</p>
<p>example we consider a specific distribution.
</p>
<p>Example 5.1 Suppose that p D 2, r D 1, � D
�
0
</p>
<p>0
</p>
<p>�
and &dagger; D
</p>
<p>�
1
</p>
<p>�0:8
�0:8
2
</p>
<p>�
.
</p>
<p>Then &dagger;11 D 1, &dagger;21 D �0:8 and &dagger;22:1 D &dagger;22 � &dagger;21&dagger;�111 &dagger;12 D 2 � .0:8/2 D 1:36.
Hence the marginal pdf of X1 is
</p>
<p>fX1.x1/ D
1p
2�
</p>
<p>exp
</p>
<p>�
�x
</p>
<p>2
1
</p>
<p>2
</p>
<p>�
</p>
<p>and the conditional pdf of .X2 j X1 D x1/ is given by
</p>
<p>f .x2 j x1/ D
1p
</p>
<p>2�.1:36/
exp
</p>
<p>�
� .x2 C 0:8x1/
</p>
<p>2
</p>
<p>2 � .1:36/
</p>
<p>�
:
</p>
<p>As mentioned above, the conditional mean of .X2 j X1/ is linear in X1. The shift in
the density of .X2 j X1/ can be seen in Fig. 5.1.
</p>
<p>Sometimes it will be useful to reconstruct a joint distribution from the marginal
</p>
<p>distribution of X1 and the conditional distribution .X2jX1/. The following theorem
shows under which conditions this can be easily done in the multinormal framework.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.1 Elementary Properties of the Multinormal 187
</p>
<p>Fig. 5.1 Shifts in the conditional density MVAcondnorm
</p>
<p>Theorem 5.4 IfX1 � Nr.�1; &dagger;11/ and .X2jX1 D x1/ � Np�r .Ax1Cb;&#127;/ where
&#127; does not depend on x1, then X D
</p>
<p>�
X1
X2
</p>
<p>�
� Np.�;&dagger;/, where
</p>
<p>� D
 
</p>
<p>�1
</p>
<p>A�1 C b
</p>
<p>!
</p>
<p>&dagger; D
�
&dagger;11 &dagger;11A
</p>
<p>&gt;
</p>
<p>A&dagger;11 &#127;CA&dagger;11A&gt;
�
:
</p>
<p>Example 5.2 Consider the following random variables
</p>
<p>X1 � N1.0; 1/;
</p>
<p>X2jX1 D x1 � N2
��
</p>
<p>2x1
x1 C 1
</p>
<p>�
;
</p>
<p>�
1 0
</p>
<p>0 1
</p>
<p>��
:
</p>
<p>Using Theorem (5.4), where A D .2 1/&gt;, b D .0 1/&gt; and &#127; D I2, we easily
obtain the following result:
</p>
<p>X D
�
X1
X2
</p>
<p>�
� N3
</p>
<p>0
@
0
@
0
</p>
<p>0
</p>
<p>1
</p>
<p>1
A ;
</p>
<p>0
@
1 2 1
</p>
<p>2 5 2
</p>
<p>1 2 2
</p>
<p>1
A
1
A :</p>
<p/>
</div>
<div class="page"><p/>
<p>188 5 Theory of the Multinormal
</p>
<p>In particular, the marginal distribution of X2 is
</p>
<p>X2 � N2
��
</p>
<p>0
</p>
<p>1
</p>
<p>�
;
</p>
<p>�
5 2
</p>
<p>2 2
</p>
<p>��
;
</p>
<p>thus conditional on X1, the two components of X2 are independent but marginally
</p>
<p>they are not.
</p>
<p>Note that the marginal mean vector and covariance matrix of X2 could have
</p>
<p>also been computed directly by using (4.28)&ndash;(4.29). Using the derivation above,
</p>
<p>however, provides us with useful properties: we have multinormality.
</p>
<p>Conditional Approximations
</p>
<p>As we saw in Chap. 4 (Theorem 4.3), the conditional expectation E.X2jX1/ is the
mean squared error (MSE) best approximation of X2 by a function of X1. We have
</p>
<p>in this case
</p>
<p>X2 D E.X2jX1/C U D �2 C&dagger;21&dagger;�111 .X1 � �1/C U: (5.9)
</p>
<p>Hence, the best approximation of X2 2 Rp�r by X1 2 Rr is the linear
approximation that can be written as:
</p>
<p>X2 D ˇ0 C BX1 C U (5.10)
</p>
<p>with B D &dagger;21&dagger;�111 , ˇ0 D �2 � B�1 and U � N.0;&dagger;22:1/.
Consider now the particular case where r D p � 1. Now X2 2 R and B is a row
</p>
<p>vector ˇ&gt; of dimension .1 � r/
</p>
<p>X2 D ˇ0 C ˇ&gt; X1 C U: (5.11)
</p>
<p>This means, geometrically speaking, that the best MSE approximation of X2 by a
</p>
<p>function of X1 is a hyperplane. The marginal variance of X2 can be decomposed
</p>
<p>via (5.11):
</p>
<p>�22 D ˇ&gt;&dagger;11ˇ C �22:1 D �21&dagger;�111 �12 C �22:1: (5.12)
</p>
<p>The ratio
</p>
<p>�22:1:::r D
�21&dagger;
</p>
<p>�1
11 �12
</p>
<p>�22
(5.13)
</p>
<p>is known as the square of the multiple correlation between X2 and the r variables
</p>
<p>X1. It is the percentage of the variance of X2 which is explained by the linear</p>
<p/>
</div>
<div class="page"><p/>
<p>5.1 Elementary Properties of the Multinormal 189
</p>
<p>approximation ˇ0 C ˇ&gt;X1. The last term in (5.12) is the residual variance
of X2. The square of the multiple correlation corresponds to the coefficient of
</p>
<p>determination introduced in Sect. 3.4, see (3.39), but here it is defined in terms of
</p>
<p>the r.v. X1 and X2. It can be shown that �2:1:::r is also the maximum correlation
</p>
<p>attainable between X2 and a linear combination of the elements of X1, the optimal
</p>
<p>linear combination being precisely given by ˇ&gt;X1. Note that when r D 1, the
multiple correlation �2:1 coincides with the usual simple correlation �X2X1 between
</p>
<p>X2 and X1.
</p>
<p>Example 5.3 Consider the &ldquo;classic blue&rdquo; pullover example (Example 3.15) and
</p>
<p>suppose that X1 (sales), X2 (price), X3 (advertisement) and X4 (sales assistants)
</p>
<p>are normally distributed with
</p>
<p>� D
</p>
<p>0
BB@
</p>
<p>172:7
</p>
<p>104:6
</p>
<p>104:0
</p>
<p>93:8
</p>
<p>1
CCA and &dagger; D
</p>
<p>0
BB@
</p>
<p>1037:21
</p>
<p>�80:02 219:84
1430:70 92:10 2624:00
</p>
<p>271:44 �91:58 210:30 177:36
</p>
<p>1
CCA :
</p>
<p>(These are in fact the sample mean and the sample covariance matrix but in this
</p>
<p>example we pretend that they are the true parameter values.)
</p>
<p>The conditional distribution ofX1 given .X2; X3; X4/ is thus an univariate normal
</p>
<p>with mean
</p>
<p>�1 C �12&dagger;�122
</p>
<p>0
@
X2 � �2
X3 � �3
X4 � �4
</p>
<p>1
A D 65:670� 0:216X2 C 0:485X3C 0:844X4
</p>
<p>and variance
</p>
<p>�11:2 D �11 � �12&dagger;�122 �21 D 96:761
</p>
<p>The linear approximation of the sales .X1/ by the price .X2/, advertisement .X3/
</p>
<p>and sales assistants .X4/ is provided by the conditional mean above. (Note that
</p>
<p>this coincides with the results of Example 3.15 due to the particular choice of
</p>
<p>� and &dagger;.) The quality of the approximation is given by the multiple correlation
</p>
<p>�21:234 D
�12&dagger;
</p>
<p>�1
22 �21
�11
</p>
<p>D 0:907. (Note again that this coincides with the coefficient of
determination r2 found in Example 3.15.)
</p>
<p>This example also illustrates the concept of partial correlation. The correlation
</p>
<p>matrix between the four variables is given by
</p>
<p>P D
</p>
<p>0
BB@
</p>
<p>1 �0:168 0:867 0:633
�0:168 1 0:121 �0:464
0:867 0:121 1 0:308
</p>
<p>0:633 �0:464 0:308 1
</p>
<p>1
CCA ;</p>
<p/>
</div>
<div class="page"><p/>
<p>190 5 Theory of the Multinormal
</p>
<p>so that the correlation betweenX1 (sales) andX2 (price) is�0:168:We can compute
the conditional distribution of .X1; X2/ given .X3; X4/, which is a bivariate normal
with mean:
</p>
<p> 
�1
</p>
<p>�2
</p>
<p>!
C
 
�13 �14
</p>
<p>�23 �24
</p>
<p>! 
�33 �34
</p>
<p>�43 �44
</p>
<p>!�1  
X3 � �3
X4 � �4
</p>
<p>!
D
 
32:516C 0:467X3 C 0:977X4
153:644C 0:085X3 � 0:617X4
</p>
<p>!
</p>
<p>and covariance matrix:
</p>
<p>�
�11 �12
�21 �22
</p>
<p>�
�
�
�13 �14
�23 �24
</p>
<p>��
�33 �34
�43 �44
</p>
<p>��1 �
�31 �32
�41 �42
</p>
<p>�
D
�
104:006
</p>
<p>�33:574 155:592
</p>
<p>�
:
</p>
<p>In particular, the last covariance matrix allows the partial correlation between
</p>
<p>X1 and X2 to be computed for a fixed level of X3 and X4:
</p>
<p>�X1X2jX3X4 D
�33:574p
</p>
<p>104:006 � 155:592
D �0:264;
</p>
<p>so that in this particular example with a fixed level of advertisement and sales
</p>
<p>assistance, the negative correlation between price and sales is more important than
</p>
<p>the marginal one.
</p>
<p>MVAbluepullover
</p>
<p>Summary
</p>
<p>,! If X � Np.�;&dagger;/, then a linear transformationAX C c, A.q �p/,
where c 2 Rq , has distribution Nq.A�C c;A&dagger;A&gt;/.
</p>
<p>,! Two linear transformations AX and BX with X � Np.�;&dagger;/ are
independent if and only if A&dagger;B&gt; D 0.
</p>
<p>,! If X1 and X2 are partitions of X � Np.�;&dagger;/, then the conditional
distribution of X2 given X1 D x1 is again normal.
</p>
<p>,! In the multivariate normal case,X1 is independent ofX2 if and only
if &dagger;12 D 0.
</p>
<p>,! The conditional expectation of .X2jX1/ is a linear function if�
X1
X2
</p>
<p>�
� Np.�;&dagger;/.
</p>
<p>,! The multiple correlation coefficient is defined as �22:1:::r D
�21&dagger;
</p>
<p>�1
11 �12
�22
</p>
<p>:
</p>
<p>,! The multiple correlation coefficient is the percentage of the vari-
ance of X2 explained by the linear approximation ˇ0 C ˇ&gt;X1.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.2 The Wishart Distribution 191
</p>
<p>5.2 The Wishart Distribution
</p>
<p>The Wishart distribution (named after its discoverer) plays a prominent role in the
</p>
<p>analysis of estimated covariance matrices. If the mean of X � Np.�;&dagger;/ is known
to be � D 0, then for a data matrix X .n � p/ the estimated covariance matrix is
proportional to X&gt;X . This is the point where the Wishart distribution comes in,
becauseM.p � p/ D X&gt;X D
</p>
<p>Pn
iD1 xix
</p>
<p>&gt;
i has a Wishart distributionWp.&dagger;; n/.
</p>
<p>Example 5.4 Set p D 1, then forX � N1.0; �2/ the data matrix of the observations
</p>
<p>X D .x1; : : : ; xn/&gt; with M D X&gt;X D
nX
</p>
<p>iD1
xixi
</p>
<p>leads to the Wishart distribution W1.�
2; n/ D �2�2n. The one-dimensional Wishart
</p>
<p>distribution is thus in fact a �2 distribution.
</p>
<p>When we talk about the distribution of a matrix, we mean of course the joint
</p>
<p>distribution of all its elements. More exactly: since M D X&gt;X is symmetric we
only need to consider the elements of the lower triangular matrix
</p>
<p>M D
</p>
<p>0
BBB@
</p>
<p>m11
m21 m22
:::
</p>
<p>:::
: : :
</p>
<p>mp1 mp2 : : : mpp
</p>
<p>1
CCCA : (5.14)
</p>
<p>Hence the Wishart distribution is defined by the distribution of the vector
</p>
<p>.m11; : : : ; mp1; m22; : : : ; mp2; : : : ; mpp/
&gt;: (5.15)
</p>
<p>Linear transformations of the data matrix X also lead to Wishart matrices.
</p>
<p>Theorem 5.5 If M � Wp.&dagger;; n/ and B.p � q/, then the distribution of B&gt;MB is
Wishart Wq.B
</p>
<p>&gt;&dagger;B; n/.
</p>
<p>With this theorem we can standardise Wishart matrices since with BD&dagger;�1=2
the distribution of &dagger;�1=2M&dagger;�1=2 is Wp.I; n/. Another connection to the
�2-distribution is given by the following theorem.
</p>
<p>Theorem 5.6 If M � Wp.&dagger;;m/, and a 2 Rp with a&gt;&dagger;a &curren; 0, then the
</p>
<p>distribution of
a&gt;Ma
</p>
<p>a&gt;&dagger;a
is �2m.
</p>
<p>This theorem is an immediate consequence of Theorem 5.5 if we apply the linear
</p>
<p>transformation x 7! a&gt;x. Central to the analysis of covariance matrices is the next
theorem.</p>
<p/>
</div>
<div class="page"><p/>
<p>192 5 Theory of the Multinormal
</p>
<p>Theorem 5.7 (Cochran) Let X .n�p/ be a data matrix from a Np.0;&dagger;/ distribu-
tion and let C.n � n/ be a symmetric matrix.
(a) X&gt;CX has the distribution of weighted Wishart random variables, i.e.
</p>
<p>X&gt;CX D
nX
</p>
<p>iD1
�iWp.&dagger;; 1/;
</p>
<p>where �i , i D 1; : : : ; n, are the eigenvalues of C.
(b) X&gt;CX is Wishart if and only if C2 D C. In this case
</p>
<p>X&gt;CX � Wp.&dagger;; r/;
</p>
<p>and r D rank.C/ D tr.C/:
(c) nS D X&gt;HX is distributed as Wp.&dagger;; n � 1/ (note that S is the sample
</p>
<p>covariance matrix).
</p>
<p>(d) Nx and S are independent.
The following properties are useful:
</p>
<p>1. IfM � Wp.&dagger;; n/, then E.M/ D n&dagger;.
2. IfMi are independentWishartWp.&dagger;; ni / i D 1; : : : ; k, thenM D
</p>
<p>Pk
iD1Mi �
</p>
<p>Wp.&dagger;; n/ where n D
Pk
</p>
<p>iD1 ni .
3. The density ofWp.&dagger;; n � 1/ for a positive definiteM is given by:
</p>
<p>f&dagger;;n�1.M/ D
jMj 12 .n�p�2/e� 12 tr.M&dagger;�1/
</p>
<p>2
1
2p.n�1/�
</p>
<p>1
4p.p�1/j&dagger;j 12 .n�1/
</p>
<p>Qp
iD1 &#128;f n�i2 g
</p>
<p>; (5.16)
</p>
<p>where &#128; is the gamma function: &#128;.z/ D
R1
0
t z�1e�tdt .
</p>
<p>For further details on the Wishart distribution, see Mardia, Kent, and Bibby
</p>
<p>(1979).
</p>
<p>Summary
</p>
<p>,! The Wishart distribution is a generalisation of the �2-distribution.
In particularW1.�
</p>
<p>2; n/ D �2�2n.
,! The empirical covariance matrix S has a 1
</p>
<p>n
Wp.&dagger;; n � 1/ distribu-
</p>
<p>tion.
,! In the normal case, Nx and S are independent.
</p>
<p>,! ForM � Wp.&dagger;;m/;
a&gt;Ma=a&gt;&dagger;a � �2m.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.3 Hotelling&rsquo;s T 2-Distribution 193
</p>
<p>5.3 Hotelling&rsquo;s T 2-Distribution
</p>
<p>Suppose that Y 2 Rp is a standard normal random vector, i.e. Y � Np.0; I/,
independent of the random matrix M � Wp.I; n/. What is the distribution of
Y &gt;M�1Y ? The answer is provided by the Hotelling T 2-distribution: n Y &gt;M�1Y
is Hotelling T 2p;n distributed.
</p>
<p>The Hotelling T 2-distribution is a generalisation of the Student t-distribution.
</p>
<p>The general multinormal distribution N.�;&dagger;/ is considered in Theorem 5.8.
</p>
<p>The Hotelling T 2-distribution will play a central role in hypothesis testing in
</p>
<p>Chap. 7.
</p>
<p>Theorem 5.8 If X � Np.�;&dagger;/ is independent ofM � Wp.&dagger;; n/, then
</p>
<p>n.X � �/&gt;M�1.X � �/ � T 2p;n:
</p>
<p>Corollary 5.3 If x is the mean of a sample drawn from a normal population
</p>
<p>Np.�;&dagger;/ and S is the sample covariance matrix, then
</p>
<p>.n � 1/.x � �/&gt;S�1.x � �/ D n.x � �/&gt;S�1u .x � �/ � T 2p;n�1: (5.17)
</p>
<p>Recall that Su D nn�1S is an unbiased estimator of the covariance matrix.
A connection between the Hotelling T 2- and the F -distribution is given by the next
</p>
<p>theorem.
</p>
<p>Theorem 5.9
</p>
<p>T 2p;n D
np
</p>
<p>n � p C 1 Fp;n�pC1:
</p>
<p>Example 5.5 In the univariate case (p D 1), this theorem boils down to the
well-known result:
</p>
<p>� Nx � �p
Su=
p
n
</p>
<p>�2
� T 21;n�1 D F1;n�1 D t2n�1
</p>
<p>For further details on Hotelling T 2-distribution see Mardia et al. (1979). The next
</p>
<p>corollary follows immediately from (3.23), (3.24) and from Theorem 5.8. It will be
</p>
<p>useful for testing linear restrictions in multinormal populations.</p>
<p/>
</div>
<div class="page"><p/>
<p>194 5 Theory of the Multinormal
</p>
<p>Corollary 5.4 Consider a linear transform of X � Np.�;&dagger;/; Y D AX where
A.q�p/ with .q � p/: If x and SX are the sample mean and the covariance matrix,
we have
</p>
<p>y D Ax � Nq
�
A�;
</p>
<p>1
</p>
<p>n
A&dagger;A&gt;
</p>
<p>�
</p>
<p>nSY D nASXA&gt; � Wq.A&dagger;A&gt;; n � 1/
</p>
<p>.n � 1/.Ax �A�/&gt;.ASXA&gt;/�1.Ax �A�/ � T 2q;n�1
</p>
<p>The T 2 distribution is closely connected to the univariate t-statistic.
</p>
<p>In Example 5.4 we described the manner in which the Wishart distribution
</p>
<p>generalises the �2-distribution. We can write (5.17) as:
</p>
<p>T 2 D
p
n.x � �/&gt;
</p>
<p> Pn
jD1.xj � x/.xj � x/&gt;
</p>
<p>n � 1
</p>
<p>!�1p
n.x � �/
</p>
<p>which is of the form
</p>
<p>�
multivariate normal
</p>
<p>random vector
</p>
<p>�&gt;
0
BBB@
</p>
<p>Wishart random
</p>
<p>matrix
</p>
<p>degrees of freedom
</p>
<p>1
CCCA
</p>
<p>�1
�
multivariate normal
</p>
<p>random vector
</p>
<p>�
:
</p>
<p>This is analogous to
</p>
<p>t2 D
p
n.x � �/.s2/�1
</p>
<p>p
n.x � �/
</p>
<p>or
</p>
<p>�
normal
</p>
<p>random variable
</p>
<p>�
0
BBB@
</p>
<p>�2-random
</p>
<p>variable
</p>
<p>degrees of freedom
</p>
<p>1
CCCA
</p>
<p>�1
�
</p>
<p>normal
</p>
<p>random variable
</p>
<p>�
</p>
<p>for the univariate case. Since the multivariate normal and Wishart random variables
</p>
<p>are independently distributed, their joint distribution is the product of the marginal
</p>
<p>normal and Wishart distributions. Using calculus, the distribution of T 2 as given
</p>
<p>above can be derived from this joint distribution.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.4 Spherical and Elliptical Distributions 195
</p>
<p>Summary
</p>
<p>,! Hotelling&rsquo;s T 2-distribution is a generalisation of the t-distribution.
In particular T 21;n D tn.
</p>
<p>,! .n � 1/.x � �/&gt;S�1.x � �/ has a T 2p;n�1 distribution.
</p>
<p>,! The relation between Hotelling&rsquo;s T 2- and Fisher&rsquo;s F -distribution is
given by T 2p;n D npn�pC1 Fp;n�pC1:
</p>
<p>5.4 Spherical and Elliptical Distributions
</p>
<p>The multinormal distribution belongs to the large family of elliptical distributions
</p>
<p>which has recently gained a lot of attention in financial mathematics. Elliptical
</p>
<p>distributions are often used, particularly in risk management.
</p>
<p>Definition 5.1 A .p � 1/ random vector Y is said to have a spherical distribution
Sp.�/ if its characteristic function  Y .t/ satisfies:  Y .t/ D �.t&gt;t/ for some
scalar function �.:/ which is then called the characteristic generator of the spherical
</p>
<p>distribution Sp.�/. We will write Y � Sp.�/.
This is only one of several possible ways to define spherical distributions.We can
</p>
<p>see spherical distributions as an extension of the standard multinormal distribution
</p>
<p>Np.0; Ip/.
</p>
<p>Theorem 5.10 Spherical random variables have the following properties:
</p>
<p>1. All marginal distributions of a spherically distributed random vector are
</p>
<p>spherical.
</p>
<p>2. All the marginal characteristic functions have the same generator.
</p>
<p>3. LetX � Sp.�/, thenX has the same distribution as ru.p/ where u.p/ is a random
vector distributed uniformly on the unit sphere surface in Rp and r � 0 is a
random variable independent of u.p/. If E.r2/ &lt;1, then
</p>
<p>E.X/ D 0 ; Cov.X/ D E.r
2/
</p>
<p>p
Ip:
</p>
<p>The random radius r is related to the generator � by a relation described in Fang,
</p>
<p>Kotz, and Ng (1990, p. 29). The moments of X � Sp.�/, provided that they exist,
can be expressed in terms of one-dimensional integral.
</p>
<p>A spherically distributed random vector does not, in general, necessarily possess
</p>
<p>a density. However, if it does, the marginal densities of dimension smaller than
</p>
<p>p � 1 are continuous and the marginal densities of dimension smaller than p � 2</p>
<p/>
</div>
<div class="page"><p/>
<p>196 5 Theory of the Multinormal
</p>
<p>are differentiable (except possibly at the origin in both cases). Univariate marginal
</p>
<p>densities for p greater than 2 are non-decreasing on .�1; 0/ and non-increasing
on .0;1/.
Definition 5.2 A .p � 1/ random vector X is said to have an elliptical distribution
with parameters�.p�1/ and&dagger;.p�p/ ifX has the same distribution as �CA&gt;Y ,
where Y � Sk.�/ andA is a .k�p/matrix such thatA&gt;A D &dagger;with rank.&dagger;/ D k.
We shall write X � ECp.�;&dagger;; �/.
Remark 5.1 The elliptical distribution can be seen as an extension of Np.�;&dagger;/.
</p>
<p>Example 5.6 The multivariate t-distribution. Let Z � Np.0; Ip/ and s � �2m be
independent. The random vector
</p>
<p>Y D
p
m
Z
</p>
<p>s
</p>
<p>has a multivariate t-distribution with m degrees of freedom. Moreover the
</p>
<p>t-distribution belongs to the family of p-dimensional spherical distributions.
</p>
<p>Example 5.7 The multinormal distribution. Let X � Np.�;&dagger;/. Then
X �ECp.�;&dagger;; �/ and �.u/ D exp .�u=2/. Figure 4.3 shows a density surface
of the multivariate normal distribution: f .x/D det.2�&dagger;/� 12 expf � 1
</p>
<p>2
.x��/&gt;&dagger;�1
</p>
<p>.x��/g with &dagger; D
�
1 0:6
</p>
<p>0:6 1
</p>
<p>�
and � D
</p>
<p>�
0
</p>
<p>0
</p>
<p>�
Notice that the density is constant on
</p>
<p>ellipses. This is the reason for calling this family of distributions &ldquo;elliptical&rdquo;.
</p>
<p>Theorem 5.11 Elliptical random vectors X have the following properties:
</p>
<p>1. Any linear combination of elliptically distributed variables are elliptical.
</p>
<p>2. Marginal distributions of elliptically distributed variables are elliptical.
</p>
<p>3. A scalar function �.:/ can determine an elliptical distribution ECp.�;&dagger;; �/ for
</p>
<p>every � 2 Rp and &dagger; � 0 with rank.&dagger;/ D k iff �.t&gt;t/ is a p-dimensional
characteristic function.
</p>
<p>4. Assume that X is non-degenerate. If X � ECp.�;&dagger;; �/ and X �ECp
.��; &dagger;�; ��/, then a constant c &gt; 0 exists that
</p>
<p>� D ��; &dagger; D c&dagger;�; ��.:/ D �.c�1:/:
</p>
<p>In other words &dagger;;�;A are not unique, unless we impose the condition that
</p>
<p>det.&dagger;/ D 1.
5. The characteristic function of X; .t/ D E.eit&gt;X / is of the form
</p>
<p> .t/ D eit&gt;��.t&gt;&dagger;t/
</p>
<p>for a scalar function �.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.5 Exercises 197
</p>
<p>6. X � ECp.�;&dagger;; �/ with rank.&dagger;/ D k iff X has the same distribution as:
</p>
<p>�C rA&gt;u.k/ (5.18)
</p>
<p>where r � 0 is independent of u.k/ which is a random vector distributed
uniformly on the unit sphere surface in Rk and A is a .k � p/ matrix such that
A&gt;A D &dagger;.
</p>
<p>7. Assume that X � ECp.�;&dagger;; �/ and E.r2/ &lt;1. Then
</p>
<p>E.X/ D � Cov.X/ D E.r
2/
</p>
<p>rank.&dagger;/
&dagger; D �2�&gt;.0/&dagger;:
</p>
<p>8. Assume that X � ECp.�;&dagger;; �/ with rank.&dagger;/ D k. Then
</p>
<p>Q.X/ D .X � �/&gt;&dagger;�1.X � �/
</p>
<p>has the same distribution as r2 in Eq. (5.18).
</p>
<p>5.5 Exercises
</p>
<p>Exercise 5.1 Consider X � N2.�;&dagger;/ with � D .2; 2/&gt; and &dagger; D
�
1
</p>
<p>0
</p>
<p>0
</p>
<p>1
</p>
<p>�
and the
</p>
<p>matrices A D
 
1
</p>
<p>1
</p>
<p>!&gt;
, B D
</p>
<p> 
1
</p>
<p>�1
</p>
<p>!&gt;
. Show that AX and BX are independent.
</p>
<p>Exercise 5.2 Prove Theorem 5.4.
</p>
<p>Exercise 5.3 Prove proposition (c) of Theorem 5.7.
</p>
<p>Exercise 5.4 Let
</p>
<p>X � N2
��
</p>
<p>1
</p>
<p>2
</p>
<p>�
;
</p>
<p>�
2 1
</p>
<p>1 2
</p>
<p>��
</p>
<p>and
</p>
<p>Y j X � N2
��
</p>
<p>X1
X1 CX2
</p>
<p>�
;
</p>
<p>�
1 0
</p>
<p>0 1
</p>
<p>��
:
</p>
<p>(a) Determine the distribution of Y2 j Y1.
(b) Determine the distribution of W D X � Y .</p>
<p/>
</div>
<div class="page"><p/>
<p>198 5 Theory of the Multinormal
</p>
<p>Exercise 5.5 Consider
</p>
<p>0
@
X
</p>
<p>Y
</p>
<p>Z
</p>
<p>1
A � N3.�;&dagger;/: Compute � and &dagger; knowing that
</p>
<p>Y j Z � N1.�Z; 1/
</p>
<p>�ZjY D �
1
</p>
<p>3
� 1
3
Y
</p>
<p>X j Y;Z � N1.2C 2Y C 3Z; 1/:
</p>
<p>Determine the distributions of X j Y and of X j Y CZ.
Exercise 5.6 Knowing that
</p>
<p>Z � N1.0; 1/
Y j Z � N1.1CZ; 1/
</p>
<p>X j Y;Z � N1.1 � Y; 1/
</p>
<p>(a) find the distribution of
</p>
<p>0
@
X
</p>
<p>Y
</p>
<p>Z
</p>
<p>1
A and of Y j X;Z.
</p>
<p>(b) find the distribution of
</p>
<p>�
U
</p>
<p>V
</p>
<p>�
D
�
1CZ
1 � Y
</p>
<p>�
:
</p>
<p>(c) compute E.Y j U D 2/.
</p>
<p>Exercise 5.7 Suppose
</p>
<p>�
X
</p>
<p>Y
</p>
<p>�
� N2.�;&dagger;/ with &dagger; positive definite. Is it possible
</p>
<p>that
</p>
<p>(a) �X jY D 3Y 2,
(b) �XX jY D 2C Y 2,
(c) �X jY D 3 � Y , and
(d) �XX jY D 5?</p>
<p/>
</div>
<div class="page"><p/>
<p>5.5 Exercises 199
</p>
<p>Exercise 5.8 Let X � N3
</p>
<p>0
@
0
@
1
</p>
<p>2
</p>
<p>3
</p>
<p>1
A ;
</p>
<p>0
@
11 �6 2
�6 10 �4
2 �4 6
</p>
<p>1
A
1
A.
</p>
<p>(a) Find the best linear approximation ofX3 by a linear function ofX1 andX2 and
</p>
<p>compute the multiple correlation between X3 and .X1; X2/.
</p>
<p>(b) Let Z1 D X2 � X3; Z2 D X2 C X3 and .Z3 j Z1; Z2/ � N1.Z1 C Z2; 10/.
</p>
<p>Compute the distribution of
</p>
<p>0
@
Z1
</p>
<p>Z2
Z3
</p>
<p>1
A.
</p>
<p>Exercise 5.9 Let .X; Y;Z/&gt; be a trivariate normal r.v. with
</p>
<p>Y j Z � N1.2Z; 24/
Z j X � N1.2X C 3; 14/
</p>
<p>X � N1.1; 4/
and �XY D 0:5:
</p>
<p>Find the distribution of .X; Y;Z/&gt; and compute the partial correlation between
X and Y for fixed Z. Do you think it is reasonable to approximate X by a linear
</p>
<p>function of Y and Z?
</p>
<p>Exercise 5.10 Let X � N4
</p>
<p>0
BB@
</p>
<p>0
BB@
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>4
</p>
<p>1
CCA ;
</p>
<p>0
BB@
</p>
<p>4 1 2 4
</p>
<p>1 4 2 1
</p>
<p>2 2 16 1
</p>
<p>4 1 1 9
</p>
<p>1
CCA
</p>
<p>1
CCA :
</p>
<p>(a) Give the best linear approximation ofX2 as a function of .X1; X4/ and evaluate
</p>
<p>the quality of the approximation.
</p>
<p>(b) Give the best linear approximation of X2 as a function of .X1; X3; X4/ and
</p>
<p>compare your answer with part (a).
</p>
<p>Exercise 5.11 Prove Theorem 5.2.
</p>
<p>(Hint: complete the linear transformation Z D
�
</p>
<p>A
</p>
<p>Ip�q
</p>
<p>�
X C
</p>
<p>�
c
</p>
<p>0p�q
</p>
<p>�
and then use
</p>
<p>Theorem 5.1 to get the marginal of the first q components of Z.)
</p>
<p>Exercise 5.12 Prove Corollaries 5.1 and 5.2.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 6
</p>
<p>Theory of Estimation
</p>
<p>We know from our basic knowledge of statistics that one of the objectives in
</p>
<p>statistics is to better understand and model the underlying process which generates
</p>
<p>data. This is known as statistical inference: we infer from information contained
</p>
<p>in sample properties of the population from which the observations are taken.
</p>
<p>In multivariate statistical inference, we do exactly the same. The basic ideas
</p>
<p>were introduced in Sect. 4.5 on sampling theory: we observed the values of a
</p>
<p>multivariate random variableX and obtained a sampleX D fxigniD1. Under random
sampling, these observations are considered to be realisations of a sequence of i.i.d.
</p>
<p>random variables X1; : : : ; Xn where each Xi is a p-variate random variable which
</p>
<p>replicates the parent or population random variableX . In this chapter, for notational
</p>
<p>convenience, we will no longer differentiate between a random variable Xi and an
</p>
<p>observation of it, xi , in our notation. We will simply write xi and it should be clear
</p>
<p>from the context whether a random variable or an observed value is meant.
</p>
<p>Statistical inference infers from the i.i.d. random sample X the properties of
</p>
<p>the population: typically, some unknown characteristic � of its distribution. In
</p>
<p>parametric statistics, � is a k-variate vector � 2 Rk characterising the unknown
properties of the population pdf f .xI �/: this could be the mean, the covariance
matrix, kurtosis, etc.
</p>
<p>The aim will be to estimate � from the sample X through estimators O� which
are functions of the sample: O� D O�.X /. When an estimator O� is proposed, we must
derive its sampling distribution to analyse its properties.
</p>
<p>In this chapter the basic theoretical tools are developed which are needed to
</p>
<p>derive estimators and to determine their properties in general situations. We will
</p>
<p>basically rely on the maximum likelihood theory in our presentation. In many
</p>
<p>situations, the maximum likelihood estimators (MLEs) indeed share asymptotic
</p>
<p>optimal properties which make their use easy and appealing.
</p>
<p>We will illustrate the multivariate normal population and also the linear regres-
</p>
<p>sion model where the applications are numerous and the derivations are easy to
</p>
<p>do. In multivariate setups, the MLE is at times too complicated to be derived
</p>
<p>&copy; Springer-Verlag Berlin Heidelberg 2015
W.K. H&auml;rdle, L. Simar, Applied Multivariate Statistical Analysis,
DOI 10.1007/978-3-662-45171-7_6
</p>
<p>201</p>
<p/>
</div>
<div class="page"><p/>
<p>202 6 Theory of Estimation
</p>
<p>analytically. In such cases, the estimators are obtained using numerical methods
</p>
<p>(nonlinear optimisation). The general theory and the asymptotic properties of
</p>
<p>these estimators remain simple and valid. The following Chap. 7 concentrates on
</p>
<p>hypothesis testing and confidence interval issues.
</p>
<p>6.1 The Likelihood Function
</p>
<p>Suppose that fxi gniD1 is an i.i.d. sample from a population with pdf f .xI �/. The
aim is to estimate � 2 Rk which is a vector of unknown parameters. The likelihood
function is defined as the joint density L.X I �/ of the observations xi considered as
a function of � :
</p>
<p>L.X I �/ D
nY
</p>
<p>iD1
f .xi I �/; (6.1)
</p>
<p>where X denotes the sample of the data matrix with the observations x&gt;1 ; : : : ; x
&gt;
n in
</p>
<p>each row. The MLE of � is defined as
</p>
<p>O� D argmax
�
L.X I �/:
</p>
<p>Often it is easier to maximise the log-likelihood function
</p>
<p>`.X I �/ D logL.X I �/; (6.2)
</p>
<p>which is equivalent since the logarithm is a monotone one-to-one function. Hence
</p>
<p>O� D argmax
�
L.X I �/ D argmax
</p>
<p>�
`.X I �/:
</p>
<p>The following examples illustrate cases where the maximisation process can be
</p>
<p>performed analytically, i.e., we will obtain an explicit analytical expression for O� .
Unfortunately, in other situations, the maximisation process can be more intricate,
</p>
<p>involving nonlinear optimisation techniques. In the latter case, given a sample X
</p>
<p>and the likelihood function, numerical methods will be used to determine the value
</p>
<p>of � maximising L.X I �/ or `.X I �/. These numerical methods are typically based
on Newton&ndash;Raphson iterative techniques.
</p>
<p>Example 6.1 Consider a sample fxi gniD1 from Np.�; I/, i.e., from the pdf
</p>
<p>f .xI �/ D .2�/�p=2 exp
�
�1
2
.x � �/&gt;.x � �/
</p>
<p>�
;</p>
<p/>
</div>
<div class="page"><p/>
<p>6.1 The Likelihood Function 203
</p>
<p>where � D � 2 Rp is the mean vector parameter. The log-likelihood is in this case
</p>
<p>`.X I �/ D
nX
</p>
<p>iD1
logff .xi I �/g D log .2�/�np=2 �
</p>
<p>1
</p>
<p>2
</p>
<p>nX
</p>
<p>iD1
.xi � �/&gt;.xi � �/: (6.3)
</p>
<p>The term .xi � �/&gt;.xi � �/ equals
</p>
<p>.xi � x/&gt;.xi � x/C .x � �/&gt;.x � �/C 2.x � �/&gt;.xi � x/:
</p>
<p>Summing this term over i D 1; : : : ; n we see that
</p>
<p>nX
</p>
<p>iD1
.xi � �/&gt;.xi � �/ D
</p>
<p>nX
</p>
<p>iD1
.xi � x/&gt;.xi � x/C n.x � �/&gt;.x � �/:
</p>
<p>Hence
</p>
<p>`.X I �/ D log.2�/�np=2 � 1
2
</p>
<p>nX
</p>
<p>iD1
.xi � x/&gt;.xi � x/ �
</p>
<p>n
</p>
<p>2
.x � �/&gt;.x � �/:
</p>
<p>Only the last term depends on � and is obviously maximised for
</p>
<p>O� D O� D x:
</p>
<p>Thus x is the MLE of � for this family of pdfs f .x; �/.
</p>
<p>A more complex example is the following one where we derive the MLEs for �
</p>
<p>and &dagger;.
</p>
<p>Example 6.2 Suppose fxigniD1 is a sample from a normal distribution Np.�;&dagger;/.
Here � D .�;&dagger;/ with &dagger; interpreted as a vector. Due to the symmetry of &dagger; the
unknown parameter � is in fact fp C 1
</p>
<p>2
p.p C 1/g-dimensional. Then
</p>
<p>L.X I �/ D j2�&dagger;j�n=2 exp
(
�1
2
</p>
<p>nX
</p>
<p>iD1
.xi � �/&gt;&dagger;�1.xi � �/
</p>
<p>)
(6.4)
</p>
<p>and
</p>
<p>`.X I �/ D �n
2
log j2�&dagger;j � 1
</p>
<p>2
</p>
<p>nX
</p>
<p>iD1
.xi � �/&gt;&dagger;�1.xi � �/: (6.5)
</p>
<p>The term .xi � �/&gt;&dagger;�1.xi � �/ equals
</p>
<p>.xi � x/&gt;&dagger;�1.xi � x/C .x � �/&gt;&dagger;�1.x � �/C 2.x � �/&gt;&dagger;�1.xi � x/:</p>
<p/>
</div>
<div class="page"><p/>
<p>204 6 Theory of Estimation
</p>
<p>Summing this term over i D 1; : : : ; n we see that
</p>
<p>nX
</p>
<p>iD1
.xi ��/&gt;&dagger;�1.xi ��/ D
</p>
<p>nX
</p>
<p>iD1
.xi � x/&gt;&dagger;�1.xi � x/C n.x ��/&gt;&dagger;�1.x ��/:
</p>
<p>Note that from (2.14)
</p>
<p>.xi � x/&gt;&dagger;�1.xi � x/ D tr
˚
.xi � x/&gt;&dagger;�1.xi � x/
</p>
<p>�
</p>
<p>D tr
˚
&dagger;�1.xi � x/.xi � x/&gt;
</p>
<p>�
:
</p>
<p>Therefore, by summing over the index i we finally arrive at
</p>
<p>nX
</p>
<p>iD1
.xi � �/&gt;&dagger;�1.xi � �/ D tr
</p>
<p>(
&dagger;�1
</p>
<p>nX
</p>
<p>iD1
.xi � x/.xi � x/&gt;
</p>
<p>)
</p>
<p>Cn.x � �/&gt;&dagger;�1.x � �/
D trf&dagger;�1nSg C n.x � �/&gt;&dagger;�1.x � �/:
</p>
<p>Thus the log-likelihood function for Np.�;&dagger;/ is
</p>
<p>`.X I �/ D �n
2
log j2�&dagger;j � n
</p>
<p>2
trf&dagger;�1Sg � n
</p>
<p>2
.x � �/&gt;&dagger;�1.x � �/: (6.6)
</p>
<p>We can easily see that the third term is maximised by � D Nx. In fact the MLEs are
given by
</p>
<p>O� D x; O&dagger; D S:
</p>
<p>The derivation of O&dagger; is a lot more complicated. It involves derivatives with respect
to matrices with their notational complexities and will not be presented here; for
</p>
<p>more elaborate proof, see Mardia, Kent and Bibby (1979, pp. 103&ndash;104). Note that
</p>
<p>the unbiased covariance estimator Su D nn�1S is not the MLE of &dagger;!
</p>
<p>Example 6.3 Consider the linear regressionmodel yi D ˇ&gt;xiC"i for i D 1; : : : ; n,
where "i is i.i.d. and N.0; �
</p>
<p>2/ and where xi 2 Rp . Here � D .ˇ&gt;; �/ is a .p C 1/-
dimensional parameter vector. Denote
</p>
<p>y D
</p>
<p>0
B@
y1
:::
</p>
<p>yn
</p>
<p>1
CA ; X D
</p>
<p>0
B@
x&gt;1
:::
</p>
<p>x&gt;n
</p>
<p>1
CA :</p>
<p/>
</div>
<div class="page"><p/>
<p>6.1 The Likelihood Function 205
</p>
<p>Then
</p>
<p>L.y;X I �/ D
nY
</p>
<p>iD1
</p>
<p>1p
2��
</p>
<p>exp
</p>
<p>�
� 1
2�2
</p>
<p>.yi � ˇ&gt;xi /2
�
</p>
<p>and
</p>
<p>`.y;X I �/ D log
�
</p>
<p>1
</p>
<p>.2�/n=2�n
</p>
<p>�
� 1
2�2
</p>
<p>nX
</p>
<p>iD1
.yi � ˇ&gt;xi /2
</p>
<p>D �n
2
log.2�/ � n log � � 1
</p>
<p>2�2
.y � Xˇ/&gt;.y � Xˇ/
</p>
<p>D �n
2
log.2�/ � n log � � 1
</p>
<p>2�2
.y&gt;y C ˇ&gt;X&gt;Xˇ � 2ˇ&gt;X&gt;y/:
</p>
<p>Differentiating w.r.t. the parameters yields
</p>
<p>@
</p>
<p>@̌
` D � 1
</p>
<p>2�2
.2X&gt;Xˇ � 2X&gt;y/
</p>
<p>@
</p>
<p>@�
` D �n
</p>
<p>�
C 1
�3
</p>
<p>˚
.y � Xˇ/&gt;.y � Xˇ/
</p>
<p>�
:
</p>
<p>Note that @
@ˇ
` denotes the vector of the derivatives w.r.t. all components of ˇ (the
</p>
<p>gradient). Since the first equation only depends on ˇ, we start with deriving Ǒ.
</p>
<p>X&gt;X Ǒ D X&gt;y; hence Ǒ D .X&gt;X /�1X&gt;y
</p>
<p>Plugging Ǒ into the second equation gives
</p>
<p>n
</p>
<p>O� D
1
</p>
<p>O�3 .y � X
Ǒ/&gt;.y � X Ǒ/; hence O�2 D 1
</p>
<p>n
jjy � X Ǒjj2;
</p>
<p>where jj � jj2 denotes the Euclidean vector norm from Sect. 2.6. We see that the MLE
Ǒ is identical with the least squares estimator (3.52). The variance estimator
</p>
<p>O�2 D 1
n
</p>
<p>nX
</p>
<p>iD1
.yi � Ǒ&gt;xi /2
</p>
<p>is nothing else than the residual sum of squares (RSS) from (3.37) generalised to the
</p>
<p>case of multivariate xi . Note that when the xi are considered to be fixed, we have
</p>
<p>E.y/ D Xˇ and Var.y/ D �2In:</p>
<p/>
</div>
<div class="page"><p/>
<p>206 6 Theory of Estimation
</p>
<p>Then, using the properties of moments from Sect. 4.2 we have
</p>
<p>E. Ǒ/ D .X&gt;X /�1X&gt; E.y/ D ˇ; (6.7)
</p>
<p>Var. Ǒ/ D �2.X&gt;X /�1: (6.8)
</p>
<p>Summary
</p>
<p>,! If fxi gniD1 is an i.i.d. sample from a distribution with pdf f .xI �/,
then L.X I �/ D
</p>
<p>Qn
iD1 f .xi I �/ is the likelihood function. The
</p>
<p>MLE is that value of � which maximises L.X I �/. Equivalently
one can maximise the log-likelihood `.X I �/.
</p>
<p>,! The MLEs of � and &dagger; from a Np.�;&dagger;/ distribution are O� D x
and O&dagger; D S. Note that the MLE of &dagger; is not unbiased.
</p>
<p>,! The MLEs of ˇ and � in the linear model y D Xˇ C
"; " � Nn.0; �2I/ are given by the least squares estimator
Ǒ D .X&gt;X /�1X&gt;y and O�2 D 1
</p>
<p>n
jjy � X Ǒjj2. E. Ǒ/ D ˇ and
</p>
<p>Var. Ǒ/ D �2.X&gt;X /�1.
</p>
<p>6.2 The Cramer&ndash;Rao Lower Bound
</p>
<p>As pointed out above, an important question in estimation theory is whether an
</p>
<p>estimator O� has certain desired properties, in particular, if it converges to the
unknown parameter � it is supposed to estimate. One typical property we want for
</p>
<p>an estimator is unbiasedness, meaning that on the average, the estimator hits its
</p>
<p>target: E. O�/ D � . We have seen for instance (see Example 6.2) that x is an unbiased
estimator of � and S is a biased estimator of &dagger; in finite samples. If we restrict
</p>
<p>ourselves to unbiased estimation, then the natural question is whether the estimator
</p>
<p>shares some optimality properties in terms of its sampling variance. Since we focus
</p>
<p>on unbiasedness, we look for an estimator with the smallest possible variance.
</p>
<p>In this context, the Cramer&ndash;Rao lower bound will give the minimal achievable
</p>
<p>variance for any unbiased estimator. This result is valid under very general regularity
</p>
<p>conditions (discussed below). One of the most important applications of the
</p>
<p>Cramer&ndash;Rao lower bound is that it provides the asymptotic optimality property
</p>
<p>of MLEs. The Cramer&ndash;Rao theorem involves the score function and its properties
</p>
<p>which will be derived first.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.2 The Cramer&ndash;Rao Lower Bound 207
</p>
<p>The score function s.X I �/ is the derivative of the log likelihood function w.r.t.
� 2 Rk
</p>
<p>s.X I �/ D @
@�
`.X I �/ D 1
</p>
<p>L.X I �/
@
</p>
<p>@�
L.X I �/: (6.9)
</p>
<p>The covariance matrix Fn D Varfs.X I �/g is called the Fisher information matrix.
In what follows, we will give some interesting properties of score functions.
</p>
<p>Theorem 6.1 If s D s.X I �/ is the score function and if O� D t D t.X ; �/ is any
function of X and � , then under regularity conditions
</p>
<p>E.st&gt;/ D @
@�
</p>
<p>E.t&gt;/ � E
�
@t&gt;
</p>
<p>@�
</p>
<p>�
� (6.10)
</p>
<p>The proof is left as an exercise (see Exercise 6.9). The regularity conditions required
</p>
<p>for this theorem are rather technical and ensure that the expressions (expectations
</p>
<p>and derivations) appearing in (6.10) are well defined. In particular, the support of the
</p>
<p>density f .xI �/ should not depend on � . The next corollary is a direct consequence.
</p>
<p>Corollary 6.1 If s D s.X I �/ is the score function, and O� D t D t.X / is any
unbiased estimator of � (i.e., E.t/ D �), then
</p>
<p>E.st&gt;/ D Cov.s; t/ D Ik : (6.11)
</p>
<p>Note that the score function has mean zero (see Exercise 6.10).
</p>
<p>Efs.X I �/g D 0: (6.12)
</p>
<p>Hence, E.ss&gt;/ D Var.s/ D Fn and by setting s D t in Theorem 6.1 it follows that
</p>
<p>Fn D �E
�
</p>
<p>@2
</p>
<p>@�@�&gt;
`.X I �/
</p>
<p>�
:
</p>
<p>Remark 6.1 If x1; : : : ; xn are i.i.d., Fn D nF1 where F1 is the Fisher information
matrix for sample size n D 1.
Example 6.4 Consider an i.i.d. sample fxi gniD1 from Np.�; I/. In this case the
parameter � is the mean �. It follows from (6.3) that
</p>
<p>s.X I �/ D @
@�
`.X I �/
</p>
<p>D �1
2
</p>
<p>@
</p>
<p>@�
</p>
<p>(
nX
</p>
<p>iD1
.xi � �/&gt;.xi � �/
</p>
<p>)
</p>
<p>D n.x � �/:</p>
<p/>
</div>
<div class="page"><p/>
<p>208 6 Theory of Estimation
</p>
<p>Hence, the information matrix is
</p>
<p>Fn D Varfn.x � �/g D nIp:
</p>
<p>Howwell can we estimate �? The answer is given in the following theoremwhich
</p>
<p>is from Cramer and Rao. As pointed out above, this theorem gives a lower bound
</p>
<p>for unbiased estimators. Hence, all estimators, which are unbiased and attain this
</p>
<p>lower bound, are minimum variance estimators.
</p>
<p>Theorem 6.2 (Cramer&ndash;Rao) If O� D t D t.X / is any unbiased estimator for � ,
then under regularity conditions
</p>
<p>Var.t/ � F�1n ; (6.13)
</p>
<p>where
</p>
<p>Fn D Efs.X I �/s.X I �/&gt;g D Varfs.X I �/g (6.14)
</p>
<p>is the Fisher information matrix.
</p>
<p>Proof Consider the correlation �Y;Z between Y and Z where Y D a&gt;t , Z D c&gt;s.
Here s is the score and the vectors a, c 2 Rp. By Corollary 6.1 Cov.s; t/ D I and
thus
</p>
<p>Cov.Y;Z/ D a&gt; Cov.t; s/c D a&gt;c
Var.Z/ D c&gt; Var.s/c D c&gt;Fnc:
</p>
<p>Hence,
</p>
<p>�2Y;Z D
Cov2.Y;Z/
</p>
<p>Var.Y /Var.Z/
D .a
</p>
<p>&gt;c/2
</p>
<p>a&gt; Var.t/a� c&gt;Fnc
� 1: (6.15)
</p>
<p>In particular, this holds for any c &curren; 0. Therefore it holds also for the maximum of
the left-hand side of (6.15) with respect to c. Since
</p>
<p>max
c
</p>
<p>c&gt;aa&gt;c
</p>
<p>c&gt;Fnc
D max
</p>
<p>c&gt;FncD1
c&gt;aa&gt;c
</p>
<p>and
</p>
<p>max
c&gt;FncD1
</p>
<p>c&gt;aa&gt;c D a&gt;F�1n a</p>
<p/>
</div>
<div class="page"><p/>
<p>6.2 The Cramer&ndash;Rao Lower Bound 209
</p>
<p>by our maximisation Theorem 2.5, we have
</p>
<p>a&gt;F�1n a
</p>
<p>a&gt; Var.t/a
� 1 8 a 2 Rp; a &curren; 0;
</p>
<p>i.e.,
</p>
<p>a&gt;fVar.t/ � F�1n ga � 0 8 a 2 Rp; a &curren; 0;
</p>
<p>which is equivalent to Var.t/ � F�1n . ut
MLEs attain the lower bound if the sample size n goes to infinity. The next
</p>
<p>Theorem 6.3 states this and, in addition, gives the asymptotic sampling distribution
</p>
<p>of the maximum likelihood estimation, which turns out to be multinormal.
</p>
<p>Theorem 6.3 Suppose that the sample fxi gniD1 is i.i.d. If O� is the MLE for � 2 Rk ,
i.e., O� D argmax
</p>
<p>�
L.X I �/, then under some regularity conditions, as n!1:
</p>
<p>p
n. O� � �/ L�! Nk.0;F�11 / (6.16)
</p>
<p>where F1 denotes the Fisher information for sample size n D 1.
As a consequence of Theorem 6.3 we see that under regularity conditions the MLE
</p>
<p>is asymptotically unbiased, efficient (minimum variance) and normally distributed.
</p>
<p>Also it is a consistent estimator of � .
</p>
<p>Note that from property (5.4) of the multinormal it follows that asymptotically
</p>
<p>n. O� � �/&gt;F1. O� � �/
L! �2p: (6.17)
</p>
<p>If OF1 is a consistent estimator of F1 .e:g: OF1 D F1. O�//, we have equivalently
</p>
<p>n. O� � �/&gt; OF1. O� � �/
L! �2p: (6.18)
</p>
<p>This expression is sometimes useful in testing hypotheses about � and in construct-
</p>
<p>ing confidence regions for � in a very general setup. These issues will be raised
</p>
<p>in more detail in the next chapter, but from (6.18) it can be seen, for instance, that
</p>
<p>when n is large,
</p>
<p>P
n
n. O� � �/&gt; OF1. O� � �/ � �21�˛Ip
</p>
<p>o
� 1 � ˛;
</p>
<p>where �2�Ip denotes the �-quantile of a �
2
p random variable. So, the ellipsoid n.
</p>
<p>O� �
�/&gt; OF1. O� � �/ � �21�˛Ip provides in Rp an asymptotic .1 � ˛/-confidence region
for � .</p>
<p/>
</div>
<div class="page"><p/>
<p>210 6 Theory of Estimation
</p>
<p>Summary
</p>
<p>,! The score function is the derivative s.X I �/ D @
@�
`.X I �/ of the
</p>
<p>log-likelihood with respect to � . The covariance matrix of s.X I �/
is the Fisher information matrix.
</p>
<p>,! The score function has mean zero: Efs.X I �/g D 0.
</p>
<p>,! The Cramer&ndash;Rao bound says that any unbiased estimator O� D t D
t.X / has a variance that is bounded from below by the inverse of
</p>
<p>the Fisher information. Thus, an unbiased estimator, which attains
</p>
<p>this lower bound, is a minimum variance estimator.
</p>
<p>,! For i.i.d. data fxi gniD1 the Fisher information matrix is: Fn D nF1.
</p>
<p>,! MLEs attain the lower bound in an asymptotic sense, i.e.,
</p>
<p>p
n. O� � �/ L�! Nk.0;F�11 /
</p>
<p>if O� is the MLE for � 2 Rk , i.e., O� D argmax
�
L.X I �/.
</p>
<p>6.3 Exercises
</p>
<p>Exercise 6.1 Consider a uniform distribution on the interval Œ0; �&#141;. What is the
</p>
<p>MLE of �? (Hint: the maximisation here cannot be performed by means of
</p>
<p>derivatives. Here the support of x depends on � .)
</p>
<p>Exercise 6.2 Consider an i.i.d. sample of size n from the bivariate population with
</p>
<p>pdf f .x1; x2/ D .�1�2/�1 exp.�x1=�1 � x2=�2/, x1; x2 &gt; 0. Compute the MLE of
� D .�1; �2/. Find the Cramer&ndash;Rao lower bound. Is it possible to derive a minimal
variance unbiased estimator of �?
</p>
<p>Exercise 6.3 Show that the MLE of Example 6.1, O� D x, is a minimal variance
estimator for any finite sample size n (i.e., without applying Theorem 6.3).
</p>
<p>Exercise 6.4 We know from Example 6.4 that the MLE of Example 6.1 has F1 D
Ip . This leads to
</p>
<p>p
n.x � �/ L�! Np.0; I/
</p>
<p>by Theorem 6.3. Can you give an analogous result for the square x2 for the case
</p>
<p>p D 1?</p>
<p/>
</div>
<div class="page"><p/>
<p>6.3 Exercises 211
</p>
<p>Exercise 6.5 Consider an i.i.d. sample of size n from the bivariate population
</p>
<p>with pdf f .x1; x2/ D .�21�2x2/�1 exp.�x1=�1x2 � x2=�1�2/, x1; x2 &gt; 0. Compute
the MLE of � D .�1; �2/. Find the Cramer&ndash;Rao lower bound and the asymptotic
variance of O� .
Exercise 6.6 Consider a sample fxigniD1 from Np.�;&dagger;0/ where &dagger;0 is known.
Compute the Cramer&ndash;Rao lower bound for �. Can you derive a minimal unbiased
</p>
<p>estimator for �?
</p>
<p>Exercise 6.7 Let X � Np.�;&dagger;/ where &dagger; is unknown but we know
&dagger; D diag.�11; �22; : : : ; �pp/. From an i.i.d. sample of size n, find the MLE of � and
of &dagger;.
</p>
<p>Exercise 6.8 Reconsider the setup of the previous exercise. Suppose that
</p>
<p>&dagger; D diag.�11; �22; : : : ; �pp/:
</p>
<p>Can you derive in this case the Cramer&ndash;Rao lower bound for �&gt; D
.�1 : : : �p; �11 : : : �pp/?
</p>
<p>Exercise 6.9 Prove Theorem 6.1. Hint: start from @
@�
</p>
<p>E.t&gt;/ D @
@�
</p>
<p>R
t&gt;.X I �/
</p>
<p>L.X I �/dX , then permute integral and derivatives and note that s.X I �/ D
1
</p>
<p>L.X I�/
@
@�
L.X I �/.
</p>
<p>Exercise 6.10 Prove expression (6.12).
</p>
<p>(Hint: start from Efs.X I �/g D
R
</p>
<p>1
L.X I�/
</p>
<p>@
@�
L.X I �/L.X I �/@X and then permute
</p>
<p>integral and derivative.)</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 7
</p>
<p>Hypothesis Testing
</p>
<p>In the preceding chapter, the theoretical basis of estimation theory was presented.
</p>
<p>Now we turn our interest towards testing issues: we want to test the hypothesisH0
that the unknown parameter � belongs to some subspace of Rq . This subspace is
</p>
<p>called the null set and will be denoted by &#127;0 � Rq .
In many cases, this null set corresponds to restrictions which are imposed on the
</p>
<p>parameter space:H0 corresponds to a &ldquo;reduced model&rdquo;. As we have already seen in
</p>
<p>Chap. 3, the solution to a testing problem is in terms of a rejection region R which
</p>
<p>is a set of values in the sample space which leads to the decision of rejecting the
</p>
<p>null hypothesisH0 in favour of an alternativeH1, which is called the &ldquo;full model&rdquo;.
</p>
<p>In general, we want to construct a rejection region R which controls the size of
</p>
<p>the type I error, i.e. the probability of rejecting the null hypothesis when it is true.
</p>
<p>More formally, a solution to a testing problem is of predetermined size ˛ if:
</p>
<p>P.RejectingH0 j H0 is true/ D ˛:
</p>
<p>In fact, since H0 is often a composite hypothesis, it is achieved by finding R such
</p>
<p>that
</p>
<p>sup
�2&#127;0
</p>
<p>P.X 2 R j �/ D ˛:
</p>
<p>In this chapter we will introduce a tool which allows us to build a rejection
</p>
<p>region in general situations; it is based on the likelihood ratio principle. This is
</p>
<p>a very useful technique because it allows us to derive a rejection region with an
</p>
<p>asymptotically appropriate size ˛. The technique will be illustrated through various
</p>
<p>testing problems and examples. We concentrate on multinormal populations and
</p>
<p>linear models where the size of the test will often be exact even for finite sample
</p>
<p>sizes n.
</p>
<p>Section 7.1 gives the basic ideas and Sect. 7.2 presents the general problem of
</p>
<p>testing linear restrictions. This allows us to propose solutions to frequent types
</p>
<p>&copy; Springer-Verlag Berlin Heidelberg 2015
W.K. H&auml;rdle, L. Simar, Applied Multivariate Statistical Analysis,
DOI 10.1007/978-3-662-45171-7_7
</p>
<p>213</p>
<p/>
</div>
<div class="page"><p/>
<p>214 7 Hypothesis Testing
</p>
<p>of analyses (including comparisons of several means, repeated measurements and
</p>
<p>profile analysis). Each case can be viewed as a simple specific case of testing linear
</p>
<p>restrictions. Special attention is devoted to confidence intervals and confidence
</p>
<p>regions for means and for linear restrictions on means in a multinormal setup.
</p>
<p>7.1 Likelihood Ratio Test
</p>
<p>Suppose that the distribution of fxi gniD1, xi 2 Rp, depends on a parameter vector � .
We will consider two hypotheses:
</p>
<p>H0 W � 2 &#127;0
H1 W � 2 &#127;1:
</p>
<p>The hypothesisH0 corresponds to the &ldquo;reduced model&rdquo; andH1 to the &ldquo;full model&rdquo;.
</p>
<p>This notation was already used in Chap. 3.
</p>
<p>Example 7.1 Consider a multinormal Np.�; I/. To test if � equals a certain fixed
</p>
<p>value �0 we construct the test problem:
</p>
<p>H0 W � D �0
H1 W no constraints on �
</p>
<p>or, equivalently,&#127;0 D f�0g,&#127;1 D Rp.
Define L�j D max
</p>
<p>�2&#127;j
L.X I �/, the maxima of the likelihood for each of the
</p>
<p>hypotheses. Consider the likelihood ratio (LR)
</p>
<p>�.X / D L
�
0
</p>
<p>L�1
: (7.1)
</p>
<p>One tends to favour H0 if the LR is high and H1 if the LR is low. The likelihood
</p>
<p>ratio test (LRT) tells us when exactly to favour H0 over H1. A LRT of size ˛ for
</p>
<p>testingH0 againstH1 has the rejection region
</p>
<p>R D fX W �.X / &lt; cg;
</p>
<p>where c is determined so that sup
�2&#127;0
</p>
<p>P� .X 2 R/ D ˛. The difficulty here is to express
</p>
<p>c as a function of ˛, because �.X / might be a complicated function of X .
</p>
<p>Instead of � we may equivalently use the log-likelihood
</p>
<p>�2 log� D 2.`�1 � `�0 /:</p>
<p/>
</div>
<div class="page"><p/>
<p>7.1 Likelihood Ratio Test 215
</p>
<p>In this case the rejection region will be R D fX W �2 log�.X / &gt; kg: What is the
distribution of � or of �2 log� from which we need to compute c or k?
Theorem 7.1 If &#127;1 � Rq is a q-dimensional space and if &#127;0 � &#127;1 is an r-
dimensional subspace, then under regularity conditions
</p>
<p>8 � 2 &#127;0 W �2 log�
L�! �2q�r as n!1:
</p>
<p>An asymptotic rejection region can now be given by simply computing the 1 � ˛
quantile k D �21�˛Iq�r . The LRT rejection region is therefore
</p>
<p>R D fX W �2 log�.X / &gt; �21�˛Iq�r g:
</p>
<p>Theorem 7.1 is thus very helpful: it gives a general way of building rejection regions
</p>
<p>into many problems. Unfortunately, it is only an asymptotic result, meaning that
</p>
<p>the size of the test is only approximately equal to ˛, although the approximation
</p>
<p>becomes better when the sample size n increases. The question is &ldquo;how large should
</p>
<p>n be?&rdquo;. There is no definite rule: we encounter here the same problem that was
</p>
<p>already discussed with respect to the Central Limit Theorem in Chap. 4.
</p>
<p>Fortunately, in many standard circumstances, we can derive exact tests even for
</p>
<p>finite samples because the test statistic �2 log�.X / or a simple transformation of it
turns out to have a simple form. This is the case in most of the following standard
</p>
<p>testing problems. All of them can be viewed as an illustration of the likelihood ratio
</p>
<p>principle.
</p>
<p>Test Problem 1 is an amuse-bouche: in testing the mean of a multinormal
</p>
<p>population with a known covariance matrix the likelihood ratio statistic has a very
</p>
<p>simple quadratic form with a known distribution underH0.
</p>
<p>Test Problem 1. Suppose that X1; : : : ; Xn is an i.i.d. random sample from a
</p>
<p>Np.�;&dagger;/ population.
</p>
<p>H0 W � D �0; &dagger; known versusH1 W no constraints.
</p>
<p>In this case H0 is a simple hypothesis, i.e. &#127;0 D f�0g and therefore the
dimension r of&#127;0 equals 0. Since we have imposed no constraints inH1, the space
</p>
<p>&#127;1 is the whole R
p which leads to q D p. From (6.6) we know that
</p>
<p>`�0 D `.�0; &dagger;/ D �
n
</p>
<p>2
log j2�&dagger;j � 1
</p>
<p>2
n tr.&dagger;�1S/ � 1
</p>
<p>2
n.x � �0/&gt;&dagger;�1.x � �0/:</p>
<p/>
</div>
<div class="page"><p/>
<p>216 7 Hypothesis Testing
</p>
<p>UnderH1 the maximum of `.�;&dagger;/ is
</p>
<p>`�1 D `.x;&dagger;/ D �
n
</p>
<p>2
log j2�&dagger;j � 1
</p>
<p>2
n tr.&dagger;�1S/:
</p>
<p>Therefore,
</p>
<p>� 2 log� D 2.`�1 � `�0 / D n.x � �0/&gt;&dagger;�1.x � �0/ (7.2)
</p>
<p>which, by Theorem 4.7, has a �2p-distribution underH0.
</p>
<p>Example 7.2 Consider the bank data again. Let us test whether the population mean
</p>
<p>of the forged bank notes is equal to
</p>
<p>�0 D .214:9; 129:9; 129:7; 8:3; 10:1; 141:5/&gt;:
</p>
<p>(This is in fact the sample mean of the genuine bank notes.) The sample mean of
</p>
<p>the forged bank notes is
</p>
<p>x D .214:8; 130:3; 130:2; 10:5; 11:1; 139:4/&gt;:
</p>
<p>Suppose for the moment that the estimated covariance matrix Sf given in (3.5) is
</p>
<p>the true covariance matrix&dagger;. We construct the LRT and obtain
</p>
<p>�2 log� D 2.`�1 � `�0 / D n.x � �0/&gt;&dagger;�1.x � �0/
D 7362:32;
</p>
<p>the quantile k D �20:95I6 equals 12:592. The rejection consists of all values in the
sample space which lead to values of the LRT statistic larger than 12:592. Under
</p>
<p>H0 the value of �2 log� is therefore highly significant. Hence, the true mean of the
forged bank notes is significantly different from �0!
</p>
<p>Test Problem 2 is the same as the preceding one but in a more realistic situation
</p>
<p>where the covariance matrix is unknown; here the Hotelling&rsquo;s T 2-distribution will
</p>
<p>be useful to determine an exact test and a confidence region for the unknown�.
</p>
<p>Test Problem 2. Suppose that X1; : : : ; Xn is an i.i.d. random sample from a
</p>
<p>Np.�;&dagger;/ population.
</p>
<p>H0 W � D �0; &dagger; unknown versusH1 W no constraints.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.1 Likelihood Ratio Test 217
</p>
<p>UnderH0 it can be shown that
</p>
<p>S0 D
1
</p>
<p>n
</p>
<p>h
x � 1n�&gt;0 � 1nx&gt; C 1nx&gt;
</p>
<p>i&gt; h
x � 1n�&gt;0 � 1nx&gt; C 1nx&gt;
</p>
<p>i
</p>
<p>D S C .x � �0/ .x � �0/&gt;
</p>
<p>`�0 D `.�0;S C dd&gt;/; d D .x � �0/ (7.3)
</p>
<p>and underH1 we have
</p>
<p>`�1 D `.x;S/:
</p>
<p>This leads after some calculation to
</p>
<p>�2 log� D 2.`�1 � `�0 /
</p>
<p>D �n log jSj � n tr.S�1S/ � n .x � x/&gt; S�1 .x � x/C n log jS C dd&gt;j
</p>
<p>Cn tr
h�
S C dd&gt;
</p>
<p>��1
S
i
C n .x � �0/&gt; .S C dd&gt;/�1 .x � �0/
</p>
<p>D n log
ˇ̌
ˇ̌
ˇ
S C dd&gt;
</p>
<p>S
</p>
<p>ˇ̌
ˇ̌
ˇC n tr
</p>
<p>�
.S C dd&gt;/�1S
</p>
<p>�
C nd&gt;.S C dd&gt;/�1d � np
</p>
<p>D n log
ˇ̌
ˇ̌
ˇ
S C dd&gt;
</p>
<p>S
</p>
<p>ˇ̌
ˇ̌
ˇC n tr
</p>
<p>�
.S C dd&gt;/�1.dd&gt; C S/
</p>
<p>�
� np
</p>
<p>D n log
ˇ̌
ˇ̌
ˇ
S C dd&gt;
</p>
<p>S
</p>
<p>ˇ̌
ˇ̌
ˇ
</p>
<p>D n log j1C S�1=2dd&gt;S�1=2j:
</p>
<p>By using the result for the determinant of a partitioned matrix, it equals to
</p>
<p>n log
</p>
<p>ˇ̌
ˇ̌ 1 �d&gt;S�1=2
S�1=2d I
</p>
<p>ˇ̌
ˇ̌
</p>
<p>D n log
</p>
<p>ˇ̌
ˇ̌
ˇ̌
ˇ̌
ˇ̌
ˇ
</p>
<p>1 �d&gt;S�1=21 �d&gt;S�1=22 : : : �d&gt;S�1=2p
S�1=2d 1 1 0 : : : 0
S�1=2d 2 0 1 0
</p>
<p>:::
:::
</p>
<p>: : :
</p>
<p>S�1=2dp 0 0 : : : 1
</p>
<p>ˇ̌
ˇ̌
ˇ̌
ˇ̌
ˇ̌
ˇ</p>
<p/>
</div>
<div class="page"><p/>
<p>218 7 Hypothesis Testing
</p>
<p>D n log 1C n log
pX
</p>
<p>iD1
�d&gt;S�1=2i .�1/1C.iC1/
</p>
<p>ˇ̌
ˇ̌
ˇ̌
ˇ̌
ˇ̌
ˇ̌
ˇ̌
ˇ
</p>
<p>S�1=2d 1 1 0 : : : 0
S�1=2d 2 0 1 : : : 0
</p>
<p>:::
: : :
</p>
<p>S�1=2d i 0 0 : : : 0
:::
</p>
<p>S�1=2dp 0 0 : : : 1
</p>
<p>ˇ̌
ˇ̌
ˇ̌
ˇ̌
ˇ̌
ˇ̌
ˇ̌
ˇ
</p>
<p>D n log 1C
pX
</p>
<p>iD1
�d&gt;S�1=2i .�1/2CiS�1=2d i .�1/iC1
</p>
<p>D n log.1C d&gt;S�1d/: (7.4)
</p>
<p>This statistic is a monotone function of .n�1/d&gt;S�1d . This means that�2 log� &gt;
k if and only if .n�1/d&gt;S�1d &gt; k0. The latter statistic has by Corollary 5.3, under
H0; a Hotelling&rsquo;s T
</p>
<p>2-distribution. Therefore,
</p>
<p>.n � 1/. Nx � �0/&gt;S�1. Nx � �0/ � T 2p;n�1; (7.5)
</p>
<p>or equivalently
</p>
<p>�
n � p
p
</p>
<p>�
. Nx � �0/&gt;S�1. Nx � �0/ � Fp;n�p : (7.6)
</p>
<p>In this case an exact rejection region may be defined as
</p>
<p>�
n � p
p
</p>
<p>�
. Nx � �0/&gt;S�1. Nx � �0/ &gt; F1�˛Ip;n�p :
</p>
<p>Alternatively, we have from Theorem 7.1 that underH0 the asymptotic distribution
</p>
<p>of the test statistic is
</p>
<p>�2 log�
L
</p>
<p>�! �2p; as n!1
</p>
<p>which leads to the (asymptotically valid) rejection region
</p>
<p>n logf1C . Nx � �0/&gt;S�1. Nx � �0/g &gt; �21�˛Ip ;
</p>
<p>but of course, in this case, we would prefer to use the exact F -test provided just
</p>
<p>above.
</p>
<p>Example 7.3 Consider the problem of Example 7.2 again. We know that Sf is the
</p>
<p>empirical analogue for&dagger;f , the covariance matrix for the forged banknotes. The test
</p>
<p>statistic (7.5) has the value 1,153.4 or its equivalent for the F distribution in (7.6)</p>
<p/>
</div>
<div class="page"><p/>
<p>7.1 Likelihood Ratio Test 219
</p>
<p>is 182.5 which is highly significant (F0:95I6;94 D 2:1966) so that we conclude that
�f 6D �0.
</p>
<p>Confidence Region for �
</p>
<p>When estimating a multidimensional parameter � 2 Rk from a sample, we saw in
Chap. 6 how to determine the estimator O� D O�.X /. For the observed data we end
up with a point estimate, which is the corresponding observed value of O� . We know
O�.X / is a random variable and we often prefer to determine a confidence region for
� . A confidence region (CR) is a random subset of Rk (determined by appropriate
</p>
<p>statistics) such that we are &ldquo;confident&rdquo;, at a certain given level 1�˛, that this region
contains � :
</p>
<p>P.� 2 CR/ D 1 � ˛:
</p>
<p>This is just a multidimensional generalisation of the basic univariate confidence
</p>
<p>interval. Confidence regions are particularly useful when a hypothesis H0 on �
</p>
<p>is rejected, because they eventually help in identifying which component of � is
</p>
<p>responsible for the rejection.
</p>
<p>There are only a few cases where confidence regions can be easily assessed, and
</p>
<p>include most of the testing problems on mean presented in this section.
</p>
<p>Corollary 5.3 provides a pivotal quantity which allows confidence regions for �
</p>
<p>to be constructed. Since
�
n�p
p
</p>
<p>�
. Nx � �/&gt;S�1. Nx � �/ � Fp;n�p , we have
</p>
<p>P
</p>
<p>��
n � p
p
</p>
<p>�
.� � Nx/&gt;S�1.� � Nx/ &lt; F1�˛Ip;n�p
</p>
<p>�
D 1 � ˛:
</p>
<p>Then,
</p>
<p>CR D
�
� 2 Rp j .� � Nx/&gt;S�1.�� Nx/ � p
</p>
<p>n � pF1�˛Ip;n�p
�
</p>
<p>is a confidence region at level (1-˛) for �. It is the interior of an iso-distance
</p>
<p>ellipsoid in Rp centred at Nx, with a scaling matrix S�1 and a distance constant�
p
</p>
<p>n�p
</p>
<p>�
F1�˛Ip;n�p . When p is large, ellipsoids are not easy to handle for practical
</p>
<p>purposes. One is thus interested in finding confidence intervals for �1; �2; : : : ; �p
so that simultaneous confidence on all the intervals reaches the desired level of say,
</p>
<p>1 � ˛.
Below, we consider a more general problem. We construct simultaneous confi-
</p>
<p>dence intervals for all possible linear combinations a&gt;�, a 2 Rp of the elements
of �.</p>
<p/>
</div>
<div class="page"><p/>
<p>220 7 Hypothesis Testing
</p>
<p>Suppose for a moment that we fix a particular projection vector a. We are back
</p>
<p>to a standard univariate problem of finding a confidence interval for the mean a&gt;�
of a univariate random variable a&gt;X . We can use the t-statistics and an obvious
confidence interval for a&gt;� is given by the values a&gt;� such that
</p>
<p>ˇ̌
ˇ̌
ˇ
</p>
<p>p
n � 1.a&gt;� � a&gt; Nx/p
</p>
<p>a&gt;Sa
</p>
<p>ˇ̌
ˇ̌
ˇ � t1� ˛2 In�1
</p>
<p>or equivalently
</p>
<p>t2.a/ D
.n � 1/
</p>
<p>˚
a&gt;.� � Nx/
</p>
<p>�2
</p>
<p>a&gt;Sa
� F1�˛I1;n�1:
</p>
<p>This provides the (1� ˛) confidence interval for a&gt;�:
0
@a&gt; Nx �
</p>
<p>s
</p>
<p>F1�˛I1;n�1
a&gt;Sa
</p>
<p>n � 1 � a
&gt;� � a&gt; Nx C
</p>
<p>s
</p>
<p>F1�˛I1;n�1
a&gt;Sa
</p>
<p>n � 1
</p>
<p>1
A :
</p>
<p>Now it is easy to prove (using Theorem 2.5) that:
</p>
<p>max
a
t2.a/ D .n � 1/. Nx � �/&gt;S�1. Nx � �/ � T 2p;n�1:
</p>
<p>Therefore, simultaneously for all a 2 Rp , the interval
�
a&gt; Nx �
</p>
<p>p
K˛a&gt;Sa; a
</p>
<p>&gt; Nx C
p
K˛a&gt;Sa
</p>
<p>�
; (7.7)
</p>
<p>whereK˛ D pn�pF1�˛Ip;n�p , will contain a&gt;� with probability (1 � ˛).
A particular choice of a are the columns of the identity matrix Ip , providing
</p>
<p>simultaneous confidence intervals for �1; : : : ; �p . We therefore have with probabil-
</p>
<p>ity (1 � ˛) for j D 1; : : : ; p
</p>
<p>Nxj �
r
</p>
<p>p
</p>
<p>n � pF1�˛Ip;n�psjj � �j � Nxj C
r
</p>
<p>p
</p>
<p>n � pF1�˛Ip;n�psjj: (7.8)
</p>
<p>It should be noted that these intervals define a rectangle inscribing the confidence
</p>
<p>ellipsoid for � given above. They are particularly useful when a null hypothesis
</p>
<p>H0 of the type described above is rejected and one would like to see which
</p>
<p>component(s) are mainly responsible for the rejection.
</p>
<p>Example 7.4 The 95%confidence region for�f , the mean of the forged banknotes,
</p>
<p>is given by the ellipsoid:
</p>
<p>�
� 2 R6
</p>
<p>ˇ̌
ˇ.� � Nxf /&gt;S�1f .� � Nxf / �
</p>
<p>6
</p>
<p>94
F0:95I6;94
</p>
<p>�
:</p>
<p/>
</div>
<div class="page"><p/>
<p>7.1 Likelihood Ratio Test 221
</p>
<p>The 95% simultaneous confidence intervals are given by (we use F0:95I6;94 D
2:1966)
</p>
<p>214:692 � �1 � 214:954
130:205 � �2 � 130:395
130:082 � �3 � 130:304
10:108 � �4 � 10:952
10:896 � �5 � 11:370
139:242 � �6 � 139:658:
</p>
<p>Comparing the inequalities with �0 D .214:9; 129:9; 129:7; 8:3; 10:1; 141:5/&gt;
shows that almost all components (except the first one) are responsible for the
</p>
<p>rejection of �0 in Examples 7.2 and 7.3.
</p>
<p>In addition, the method can provide other confidence intervals. We have at the
</p>
<p>same level of confidence (choosing a&gt; D .0; 0; 0; 1; �1; 0/)
</p>
<p>�1:211 � �4 � �5 � 0:005
</p>
<p>showing that for the forged bills, the lower border is essentially smaller than the
</p>
<p>upper border.
</p>
<p>Remark 7.1 It should be noted that the confidence region is an ellipsoid whose
</p>
<p>characteristics depend on the whole matrix S. In particular, the slope of the axis
</p>
<p>depends on the eigenvectors of S and therefore on the covariances sij. However, the
</p>
<p>rectangle inscribing the confidence ellipsoid provides the simultaneous confidence
</p>
<p>intervals for �j ; j D 1; : : : ; p. They do not depend on the covariances sij, but
only on the variances sjj [see (7.8)]. In particular, it may happen that a tested value
</p>
<p>�0 is covered by the confidence ellipsoid but not covered by the intervals (7.8). In
</p>
<p>this case, �0 is rejected by a test based on the simultaneous confidence intervals
</p>
<p>but not rejected by a test based on the confidence ellipsoid. The simultaneous
</p>
<p>confidence intervals are easier to handle than the full ellipsoid but we have lost some
</p>
<p>information, namely the covariance between the components (see Exercise 7.14).
</p>
<p>The following problem concerns the covariance matrix in a multinormal popula-
</p>
<p>tion: in this situation the test statistic has a slightly more complicated distribution.
</p>
<p>We will therefore invoke the approximation of Theorem 7.1 in order to derive a test
</p>
<p>of approximate size ˛.
</p>
<p>Test Problem 3. Suppose that X1; : : : ; Xn is an i.i.d. random sample from a
</p>
<p>Np.�;&dagger;/ population.
</p>
<p>H0 W &dagger; D &dagger;0; � unknown versusH1 W no constraints:</p>
<p/>
</div>
<div class="page"><p/>
<p>222 7 Hypothesis Testing
</p>
<p>UnderH0 we have O� D x, and &dagger; D &dagger;0, whereas underH1 we have O� D x, and
O&dagger; D S. Hence
</p>
<p>`�0 D `.x;&dagger;0/ D �
1
</p>
<p>2
n log j2�&dagger;0j �
</p>
<p>1
</p>
<p>2
n tr.&dagger;�10 S/
</p>
<p>`�1 D `.x;S/ D �
1
</p>
<p>2
n log j2�Sj � 1
</p>
<p>2
np
</p>
<p>and thus
</p>
<p>�2 log� D 2.`�1 � `�0 /
D n tr.&dagger;�10 S/ � n log j&dagger;�10 Sj � np:
</p>
<p>Note that this statistic is a function of the eigenvalues of &dagger;�10 S. Unfortunately, the
exact finite sample distribution of �2 log� is very complicated. Asymptotically, we
have underH0
</p>
<p>�2 log� L! �2m as n!1
</p>
<p>with m D 1
2
fp.p C 1/g, since a .p � p/ covariance matrix has only these m
</p>
<p>parameters as a consequence of its symmetry.
</p>
<p>Example 7.5 Consider the US companies data set (Table 22.5) and suppose we
</p>
<p>are interested in the companies of the energy sector, analysing their assets .X1/
</p>
<p>and sales .X2/. The sample is of size 15 and provides the value of S D 107 ��
1:6635 1:2410
</p>
<p>1:2410 1:3747
</p>
<p>�
. We want to test if Var
</p>
<p>�
X1
X2
</p>
<p>�
D 107 �
</p>
<p>�
1:2248 1:1425
</p>
<p>1:1425 1:5112
</p>
<p>�
D &dagger;0.
</p>
<p>(&dagger;0 is in fact the empirical variance matrix for X1 and X2 for the manufacturing
</p>
<p>sector). The test statistic ( MVAusenergy) turns out to be �2 log� D 5:4046
which is not significant for �23 (p-value D 0:1445). So we cannot conclude that
&dagger; 6D &dagger;0.
</p>
<p>In the next testing problem, we address a question that was already stated in
</p>
<p>Chap. 3, Sect. 3.6: testing a particular value of the coefficients ˇ in a linear model.
</p>
<p>The presentation is carried out in general terms so that it can be built on in the next
</p>
<p>section where we will test linear restrictions on ˇ.
</p>
<p>Test Problem 4. Suppose that Y1; : : : ; Yn are independent r.v.&rsquo;s with
</p>
<p>Yi � N1.ˇ&gt;xi ; �2/; xi 2 Rp .
</p>
<p>H0 W ˇ D ˇ0; �2 unknown versusH1 W no constraints:</p>
<p/>
</div>
<div class="page"><p/>
<p>7.1 Likelihood Ratio Test 223
</p>
<p>Under H0 we have ˇ D ˇ0; O�20 D 1n jjy � Xˇ0jj2 and under H1 we have Ǒ D
.X&gt;X /�1X&gt;y; O�2 D 1
</p>
<p>n
jjy � X Ǒjj2 (see Example 6.3). Hence by Theorem 7.1
</p>
<p>�2 log� D 2.`�1 � `�0 /
</p>
<p>D n log
 
jjy � Xˇ0jj2
</p>
<p>jjy � X Ǒjj2
</p>
<p>!
</p>
<p>L�! �2p:
</p>
<p>We draw upon the result (3.45) which gives us
</p>
<p>F D .n � p/
p
</p>
<p> 
jjy � Xˇ0jj2
</p>
<p>jjy � X Ǒjj2
� 1
</p>
<p>!
� Fp;n�p ;
</p>
<p>so that in this case we again have an exact distribution.
</p>
<p>Example 7.6 Let us consider our &ldquo;classic blue&rdquo; pullovers again. In Example 3.11
</p>
<p>we tried to model the dependency of sales on prices. As we have seen in Fig. 3.5
</p>
<p>the slope of the regression curve is rather small, hence we might ask if
�
˛
ˇ
</p>
<p>�
D
�
211
0
</p>
<p>�
.
</p>
<p>Here
</p>
<p>y D
</p>
<p>0
B@
y1
:::
</p>
<p>y10
</p>
<p>1
CA D
</p>
<p>0
B@
x1;1
:::
</p>
<p>x10;1
</p>
<p>1
CA ; X D
</p>
<p>0
B@
1 x1;2
:::
</p>
<p>:::
</p>
<p>1 x10;2
</p>
<p>1
CA :
</p>
<p>The test statistic for the LR test is
</p>
<p>�2 log� D 9:10
</p>
<p>which under the �22 distribution is significant. The exact F -test statistic
</p>
<p>F D 5:93
</p>
<p>is also significant under the F2;8 distribution .F2;8I0:95 D 4:46/.
</p>
<p>Summary
</p>
<p>,! The hypotheses H0 W � 2 &#127;0 against H1 W � 2 &#127;1 can be tested
using the LRT. The likelihood ratio (LR) is the quotient �.X / D
L�0 =L
</p>
<p>�
1 where the L
</p>
<p>�
j are the maxima of the likelihood for each of
</p>
<p>the hypotheses.</p>
<p/>
</div>
<div class="page"><p/>
<p>224 7 Hypothesis Testing
</p>
<p>Summary (continued)
</p>
<p>,! The test statistic in the LRT is �.X / or equivalently its logarithm
log�.X /. If &#127;1 is q-dimensional and &#127;0 � &#127;1 r-dimensional,
then the asymptotic distribution of�2 log� is �2q�r . This allowsH0
to be tested against H1 by calculating the test statistic �2 log� D
2.`�1 � `�0 / where `�j D logL�j .
</p>
<p>,! The hypothesis H0 W � D �0 for X � Np.�;&dagger;/, where &dagger; is
known, leads to �2 log� D n.x � �0/&gt;&dagger;�1.x � �0/ � �2p:
</p>
<p>,! The hypothesis H0 W � D �0 for X � Np.�;&dagger;/, where &dagger;
is unknown, leads to �2 log� D n logf1 C .x � �0/&gt;S�1.x �
�0/g �! �2p , and
.n � 1/. Nx � �0/&gt;S�1. Nx � �0/ � T 2p;n�1:
</p>
<p>,! The hypothesis H0 W &dagger; D &dagger;0 for X � Np.�;&dagger;/, where � is
unknown, leads to�2 log� D n tr
</p>
<p>�
&dagger;�10 S
</p>
<p>�
�n log j&dagger;�10 Sj�np �!
</p>
<p>�2m; m D 12p.p C 1/:
,! The hypothesisH0 W ˇ D ˇ0 for Yi � N1.ˇ&gt;xi ; �2/, where �2 is
</p>
<p>unknown, leads to �2 log� D n log
�
jjy�Xˇ0jj2
jjy�X Ǒjj2
</p>
<p>�
�! �2p .
</p>
<p>7.2 Linear Hypothesis
</p>
<p>In this section, we present a very general procedurewhich allows a linear hypothesis
</p>
<p>to be tested, i.e. a linear restriction, either on a vector mean � or on the coefficient
</p>
<p>ˇ of a linear model. The presented technique covers many of the practical testing
</p>
<p>problems on means or regression coefficients.
</p>
<p>Linear hypotheses are of the form A� D a with known matrices A.q � p/ and
a.q � 1/ with q � p.
Example 7.7 Let � D .�1; �2/&gt;. The hypothesis that �1 D �2 can be equivalently
written as:
</p>
<p>A� D
�
1 �1
</p>
<p>� ��1
�2
</p>
<p>�
D 0 D a:
</p>
<p>The general idea is to test a normal populationH0 W A� D a (restricted model)
against the full modelH1 where no restrictions are put on�. Due to the properties of
</p>
<p>the multinormal, we can easily adapt the Test Problems 1 and 2 to this new situation.
</p>
<p>Indeed we know, from Theorem 5.2, that yi D Axi � Nq.�y ; &dagger;y/, where �y D
A� and &dagger;y D A&dagger;A&gt;.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.2 Linear Hypothesis 225
</p>
<p>Testing the null H0 W A� D a, is the same as testing H0 W �y D a. The
appropriate statistics are Ny and Sy which can be derived from the original statistics
Nx and S available from X :
</p>
<p>Ny D A Nx; Sy D ASA&gt;:
</p>
<p>Here the difference between the translated sample mean and the tested value is d D
A Nx � a. We are now in the situation to proceed to Test Problems 5 and 6.
</p>
<p>Test Problem 5. Suppose X1; : : : ; Xn is an i.i.d. random sample from a
</p>
<p>Np.�;&dagger;/ population.
</p>
<p>H0 W A� D a; &dagger; known versusH1 W no constraints.
</p>
<p>By (7.2) we have that, underH0:
</p>
<p>n.A Nx � a/&gt;.A&dagger;A&gt;/�1.A Nx � a/ � X 2q ;
</p>
<p>and we rejectH0 if this test statistic is too large at the desired significance level.
</p>
<p>Example 7.8 We consider hypotheses on partitioned mean vectors � D
�
�1
�2
</p>
<p>�
. Let
</p>
<p>us first look at
</p>
<p>H0 W �1 D �2; versusH1 W no constraints,
</p>
<p>for N2p.
�
�1
�2
</p>
<p>�
;
�
&dagger;
0
0
&dagger;
</p>
<p>�
/ with known &dagger;. This is equivalent to A D .I;�I/, a D
</p>
<p>.0; : : : ; 0/&gt; 2 Rp and leads to
</p>
<p>�2 log� D n.x1 � x2/.2&dagger;/�1.x1 � x2/ � �2p:
</p>
<p>Another example is the test whether �1 D 0, i.e.
</p>
<p>H0 W �1 D 0; versusH1 W no constraints,
</p>
<p>for N2p
</p>
<p>��
�1
�2
</p>
<p>�
;
�
&dagger;
0
0
&dagger;
</p>
<p>� �
with known &dagger;. This is equivalent to A� D a with A D
</p>
<p>.I; 0/, and a D .0; : : : ; 0/&gt; 2 Rp . Hence
</p>
<p>�2 log� D nx1&dagger;�1x1 � �2p :</p>
<p/>
</div>
<div class="page"><p/>
<p>226 7 Hypothesis Testing
</p>
<p>Test Problem 6. Suppose X1; : : : ; Xn is an i.i.d. random sample from a
</p>
<p>Np.�;&dagger;/ population.
</p>
<p>H0 W A� D a; &dagger; unknown versusH1 W no constraints.
</p>
<p>From Corollary (5.4) and underH0 it follows immediately that
</p>
<p>.n � 1/.Ax � a/&gt;.ASA&gt;/�1.Ax � a/ � T 2q;n�1 (7.9)
</p>
<p>since indeed underH0,
</p>
<p>Ax � Nq.a; n�1A&dagger;A&gt;/
</p>
<p>is independent of
</p>
<p>nASA&gt; � Wq.A&dagger;A&gt;; n � 1/:
</p>
<p>Example 7.9 Let&rsquo;s come back again to the bank data set and suppose that we want
</p>
<p>to test if �4 D �5, i.e. the hypothesis that the lower border mean equals the larger
border mean for the forged bills. In this case:
</p>
<p>A D .0 0 0 1 � 1 0/
a D 0:
</p>
<p>The test statistic is:
</p>
<p>99.A Nx/&gt;.ASfA&gt;/�1.A Nx/ � T 21;99 D F1;99:
</p>
<p>The observed value is 13:638 which is significant at the 5% level.
</p>
<p>Repeated Measurements
</p>
<p>In many situations, n independent sampling units are observed at p different times
</p>
<p>or under p different experimental conditions (different treatments, etc.). So here we
</p>
<p>repeat p one-dimensional measurements on n different subjects. For instance, we
</p>
<p>observe the results from n students taking p different exams. We end up with a
</p>
<p>.n � p/ matrix. We can thus consider the situation where we have X1; : : : ; Xn i.i.d.
from a normal distributionNp.�;&dagger;/ when there are p repeated measurements. The
</p>
<p>hypothesis of interest in this case is that there are no treatment effects, H0 W �1 D</p>
<p/>
</div>
<div class="page"><p/>
<p>7.2 Linear Hypothesis 227
</p>
<p>�2 D � � � D �p . This hypothesis is a direct application of Test Problem 6. Indeed,
introducing an appropriate matrix transform on � we have
</p>
<p>H0 W C� D 0 where C..p � 1/ � p/ D
</p>
<p>0
BBB@
</p>
<p>1 &ndash;1 0 � � � 0
0 1 &ndash;1 � � � 0
:::
:::
</p>
<p>:::
:::
:::
</p>
<p>0 � � � 0 1 &ndash;1
</p>
<p>1
CCCA : (7.10)
</p>
<p>Note that in many cases one of the experimental conditions is the &ldquo;control&rdquo; (a
</p>
<p>placebo, standard drug or reference condition). Suppose it is the first component.
</p>
<p>In that case one is interested in studying differences to the control variable. The
</p>
<p>matrix C has therefore a different form
</p>
<p>C..p � 1/ � p/ D
</p>
<p>0
BBB@
</p>
<p>1 &ndash;1 0 � � � 0
1 0 &ndash;1 � � � 0
:::
:::
:::
:::
:::
</p>
<p>1 0 0 � � � &ndash;1
</p>
<p>1
CCCA :
</p>
<p>By (7.9) the null hypothesis will be rejected if:
</p>
<p>.n� p C 1/
p � 1 Nx
</p>
<p>&gt;C&gt;.CSC&gt;/�1C Nx &gt; F1�˛Ip�1;n�pC1:
</p>
<p>As a matter of fact, C� is the mean of the random variable yi D Cxi
</p>
<p>yi � Np�1.C�; C&dagger;C&gt;/:
</p>
<p>Simultaneous confidence intervals for linear combinations of the mean of yi have
</p>
<p>been derived above in (7.7). For all a 2 Rp�1, with probability .1 � ˛/ we have
</p>
<p>a&gt;C� 2 a&gt;C Nx ˙
s
</p>
<p>.p � 1/
n � p C 1F1�˛Ip�1;n�pC1a
</p>
<p>&gt;CSC&gt;a:
</p>
<p>Due to the nature of the problem here, the row sums of the elements in C are zero:
</p>
<p>C1p D 0, therefore a&gt;C is a vector having sum of elements equals to 0 . This is
</p>
<p>called a contrast. Let b D C&gt;a. We have b&gt;1p D
pP
jD1
</p>
<p>bj D 0. The result above
</p>
<p>thus provides for all contrasts of �, and b&gt;� simultaneous confidence intervals at
level .1 � ˛/
</p>
<p>b&gt;� 2 b&gt; Nx ˙
s
</p>
<p>.p � 1/
n � p C 1F1�˛Ip�1;n�pC1b
</p>
<p>&gt;Sb:</p>
<p/>
</div>
<div class="page"><p/>
<p>228 7 Hypothesis Testing
</p>
<p>Examples of contrasts for p D 4 are b&gt; D .1 � 1 0 0/ or .1 0 0 � 1/ or even
.1 � 1
</p>
<p>3
� 1
</p>
<p>3
� 1
</p>
<p>3
/ when the control is to be compared with the mean of three
</p>
<p>different treatments.
</p>
<p>Example 7.10 Bock (1975) considers the evolution of the vocabulary of children
</p>
<p>from the eighth through eleventh grade. The data set contains the scores of a
</p>
<p>vocabulary test of 40 randomly chosen children. This is a repeated measurement
</p>
<p>situation, .n D 40; p D 4/, since the same children were observed from grades 8 to
11. The statistics of interest are:
</p>
<p>Nx D .1:086; 2:544; 2:851; 3:420/&gt;
</p>
<p>S D
</p>
<p>0
BB@
</p>
<p>2:902 2:438 2:963 2:183
</p>
<p>2:438 3:049 2:775 2:319
</p>
<p>2:963 2:775 4:281 2:939
</p>
<p>2:183 2:319 2:939 3:162
</p>
<p>1
CCA :
</p>
<p>Suppose we are interested in the yearly evolution of the children. Then the matrix C
</p>
<p>providing successive differences of �j is:
</p>
<p>C D
</p>
<p>0
@
1 �1 0 0
0 1 �1 0
0 0 1 �1
</p>
<p>1
A :
</p>
<p>The value of the test statistic is Fobs D 53:134 which is highly significant for
F3:37: There are significant differences between the successive means. However,
</p>
<p>the analysis of the contrasts shows the following simultaneous 95% confidence
</p>
<p>intervals
</p>
<p>�1:958 � �1 � �2 � �0:959
�0:949 � �2 � �3 � 0:335
�1:171 � �3 � �4 � 0:036:
</p>
<p>Thus, the rejection of H0 is mainly due to the difference between the childrens&rsquo;
</p>
<p>performances in the first and second year. The confidence intervals for the following
</p>
<p>contrasts may also be of interest:
</p>
<p>�2:283 � �1 � 13 .�2 C �3 C �4/ � �1:423
�1:777 � 1
</p>
<p>3
.�1 C �2 C �3/ � �4 � �0:742
</p>
<p>�1:479 � �2 � �4 � �0:272:
</p>
<p>They show that �1 is different from the average of the 3 other years (the same being
</p>
<p>true for �4) and �4 turns out to be higher than �2 (and of course higher than �1).</p>
<p/>
</div>
<div class="page"><p/>
<p>7.2 Linear Hypothesis 229
</p>
<p>Test Problem 7 illustrates how the likelihood ratio can be applied to testing a
</p>
<p>linear restriction on the coefficient ˇ of a linear model. It is also shown how a
</p>
<p>transformation of the test statistic leads to an exact F test as presented in Chap. 3.
</p>
<p>Test Problem 7. Suppose Y1; : : : ; Yn, are independent with
</p>
<p>Yi � N1.ˇ&gt;xi ; �2/, and xi 2 Rp .
</p>
<p>H0 W Aˇ D a; �2 unknown versusH1 W no constraints.
</p>
<p>To get the constrained maximum likelihood estimators under H0, let f .ˇ; �/ D
.y � xˇ/&gt;.y � xˇ/ � �&gt;.Aˇ � a/ where � 2 Rq and solve @f .ˇ;�/
</p>
<p>@ˇ
D 0 and
</p>
<p>@f .ˇ;�/
</p>
<p>@�
D 0 (Exercise 3.24), thus we obtain:
</p>
<p>Q̌ D Ǒ � .X&gt;X /�1A&gt;fA.X&gt;X /�1A&gt;g�1.A Ǒ � a/
</p>
<p>for ˇ and Q�2 D 1
n
.y � X Q̌/&gt;.y � X Q̌/. The estimate Ǒ denotes the unconstrained
</p>
<p>MLE as before. Hence, the LR statistic is
</p>
<p>�2 log� D 2.`�1 � `�0 /
</p>
<p>D n log
 
jjy � X Q̌jj2
</p>
<p>jjy � X Ǒjj2
</p>
<p>!
</p>
<p>L�! �2q ;
</p>
<p>where q is the number of elements of a. This problem also has an exact F -test since
</p>
<p>n � p
q
</p>
<p> 
jjy � X Q̌jj2
</p>
<p>jjy � X Ǒjj2
� 1
</p>
<p>!
</p>
<p>D n � p
q
</p>
<p>.A Ǒ � a/&gt;fA.X&gt;X /�1A&gt;g�1.A Ǒ � a/
.y � X Ǒ/&gt;.y � X Ǒ/
</p>
<p>� Fq;n�p:
</p>
<p>Example 7.11 Let us continue with the &ldquo;classic blue&rdquo; pullovers. We can once more
</p>
<p>test if ˇ D 0 in the regression of sales on prices. It holds that
</p>
<p>ˇ D 0 iff .0 1/
 
˛
</p>
<p>ˇ
</p>
<p>!
D 0:
</p>
<p>The LR statistic here is
</p>
<p>�2 log� D 0:284</p>
<p/>
</div>
<div class="page"><p/>
<p>230 7 Hypothesis Testing
</p>
<p>which is not significant for the �21 distribution. The F -test statistic
</p>
<p>F D 0:231
</p>
<p>is also not significant. Hence, we can assume independence of sales and prices
</p>
<p>(alone). Recall that this conclusion has to be revised if we consider the prices
</p>
<p>together with advertising costs and hours sales manager hours.
</p>
<p>Recall the different conclusion that was made in Example 7.6 when we rejected
</p>
<p>H0 W ˛ D 211 and ˇ D 0. The rejection there came from the fact that the pair of
values was rejected. Indeed, if ˇ D 0 the estimator of ˛ would be Ny D 172:70 and
this is too far from 211.
</p>
<p>Example 7.12 Let us now consider the multivariate regression in the &ldquo;classic blue&rdquo;
</p>
<p>pullovers example. From Example 3.15 we know that the estimated parameters in
</p>
<p>the model
</p>
<p>X1 D ˛ C ˇ1X2 C ˇ2X3 C ˇ3X4 C "
</p>
<p>are
</p>
<p>Ǫ D 65:670; Ǒ1 D �0:216; Ǒ2 D 0:485; Ǒ3 D 0:844:
</p>
<p>Hence, we could postulate the approximate relation:
</p>
<p>ˇ1 � �
1
</p>
<p>2
ˇ2;
</p>
<p>which means in practice that augmenting the price by 20 EUR requires the
</p>
<p>advertising costs to increase by 10 EUR in order to keep the number of pullovers
</p>
<p>sold constant. Vice versa, reducing the price by 20 EUR yields the same result as
</p>
<p>before if we reduced the advertising costs by 10 EUR. Let us now test whether the
</p>
<p>hypothesis
</p>
<p>H0 W ˇ1 D �
1
</p>
<p>2
ˇ2
</p>
<p>is valid. This is equivalent to
</p>
<p>�
0 1
</p>
<p>1
</p>
<p>2
0
</p>
<p>�
0
BB@
</p>
<p>˛
</p>
<p>ˇ1
</p>
<p>ˇ2
ˇ3
</p>
<p>1
CCA D 0:
</p>
<p>The LR statistic in this case is equal to ( MVAlrtest)
</p>
<p>�2 log� D 0:012;</p>
<p/>
</div>
<div class="page"><p/>
<p>7.2 Linear Hypothesis 231
</p>
<p>the F statistic is
</p>
<p>F D 0:007:
</p>
<p>Hence, in both cases we will not reject the null hypothesis.
</p>
<p>Comparison of Two Mean Vectors
</p>
<p>In many situations, we want to compare two groups of individuals for whom a set
</p>
<p>of p characteristics has been observed. We have two random samples fxi1gn1iD1 and
fxj 2gn2jD1 from two distinct p-variate normal populations. Several testing issues can
be addressed in this framework. In Test Problem 8 we will first test the hypothesis
</p>
<p>of equal mean vectors in the two groups under the assumption of equality of the two
</p>
<p>covariance matrices. This task can be solved by adapting Test Problem 2.
</p>
<p>In Test Problem 9 a procedure for testing the equality of the two covariance
</p>
<p>matrices is presented. If the covariance matrices differ, the procedure of Test
</p>
<p>Problem 8 is no longer valid. If the equality of the covariancematrices is rejected, an
</p>
<p>easy rule for comparing two means with no restrictions on the covariance matrices
</p>
<p>is provided in Test Problem 10.
</p>
<p>Test Problem 8. Assume that Xi1 � Np.�1; &dagger;/, with i D 1; : : : ; n1 and
Xj 2 � Np.�2; &dagger;/, with j D 1; : : : ; n2, where all the variables are independent.
</p>
<p>H0 W �1 D �2; versusH1 W no constraints.
</p>
<p>Both samples provide the statistics Nxk and Sk , k D 1; 2. Let ı D �1 � �2. We
have
</p>
<p>. Nx1 � Nx2/ � Np
�
ı;
n1 C n2
n1n2
</p>
<p>&dagger;
</p>
<p>�
(7.11)
</p>
<p>n1S1 C n2S2 � Wp.&dagger;; n1 C n2 � 2/: (7.12)
</p>
<p>Let S D .n1 C n2/�1.n1S1 C n2S2/ be the weighted mean of S1 and S2. Since the
two samples are independent and since Sk is independent of Nxk (for k D 1; 2) it
follows that S is independent of . Nx1� Nx2/: Hence, Theorem 5.8 applies and leads to
a T 2-distribution:
</p>
<p>n1n2.n1 C n2 � 2/
.n1 C n2/2
</p>
<p>f. Nx1 � Nx2/� ıg&gt; S�1 f. Nx1 � Nx2/ � ıg/ � T 2p;n1Cn2�2
(7.13)</p>
<p/>
</div>
<div class="page"><p/>
<p>232 7 Hypothesis Testing
</p>
<p>or
</p>
<p>f. Nx1 � Nx2/� ıg&gt; S�1 f. Nx1 � Nx2/ � ıg �
p.n1 C n2/2
</p>
<p>.n1 C n2 � p � 1/n1n2
Fp;n1Cn2�p�1:
</p>
<p>This result, as in Test Problem 2, can be used to test H0 W ı D 0 or to construct a
confidence region for ı 2 Rp . The rejection region is given by:
</p>
<p>n1n2.n1 C n2 � p � 1/
p.n1 C n2/2
</p>
<p>. Nx1 � Nx2/&gt; S�1 . Nx1 � Nx2/ � F1�˛Ip;n1Cn2�p�1: (7.14)
</p>
<p>A .1� ˛/ confidence region for ı is given by the ellipsoid centred at . Nx1 � Nx2/
</p>
<p>fı� . Nx1�Nx2/g&gt; S�1 fı� . Nx1�Nx2/g �
p.n1 C n2/2
</p>
<p>.n1 C n2�p�1/.n1n2/
F1�˛Ip;n1Cn2�p�1;
</p>
<p>and the simultaneous confidence intervals for all linear combinations a&gt;ı of the
elements of ı are given by
</p>
<p>a&gt;ı 2 a&gt;. Nx1 � Nx2/˙
s
</p>
<p>p.n1 C n2/2
.n1 C n2 � p � 1/.n1n2/
</p>
<p>F1�˛Ip;n1Cn2�p�1a&gt;Sa:
</p>
<p>In particular we have at the .1 � ˛/ level, for j D 1; : : : ; p,
</p>
<p>ıj 2 . Nx1j � Nx2j /˙
s
</p>
<p>p.n1 C n2/2
.n1 C n2 � p � 1/.n1n2/
</p>
<p>F1�˛Ip;n1Cn2�p�1sjj: (7.15)
</p>
<p>Example 7.13 Let us come back to the questions raised in Example 7.5. We
</p>
<p>compare the means of assets (X1) and of sales (X2) for two sectors, energy (group
</p>
<p>1) and manufacturing (group 2). With n1 D 15, n2 D 10, and p D 2 we obtain the
statistics:
</p>
<p>Nx1 D
�
4084:0
</p>
<p>2580:5
</p>
<p>�
; Nx2 D
</p>
<p>�
4307:2
</p>
<p>4925:2
</p>
<p>�
</p>
<p>and
</p>
<p>S1 D 107
�
1:6635 1:2410
</p>
<p>1:2410 1:3747
</p>
<p>�
;S2 D 107
</p>
<p>�
1:2248 1:1425
</p>
<p>1:1425 1:5112
</p>
<p>�
;
</p>
<p>so that
</p>
<p>S D 107
�
1:4880 1:2016
</p>
<p>1:2016 1:4293
</p>
<p>�
:</p>
<p/>
</div>
<div class="page"><p/>
<p>7.2 Linear Hypothesis 233
</p>
<p>The observed value of the test statistic (7.14) is F D 2:7036. Since F0:95I2;22 D
3:4434 the hypothesis of equal means of the two groups is not rejected although
</p>
<p>it would be rejected at a less severe level (F &gt; F0:90I2;22 D 2:5613). By directly
applying (7.15), the 95% simultaneous confidence intervals for the differences (
</p>
<p>MVAsimcidif) are obtained as:
</p>
<p>�4628:6 � �1a � �2a � 4182:2
�6662:4 � �1s � �2s � 1973:0:
</p>
<p>Example 7.14 In order to illustrate the presented test procedures it is interesting to
</p>
<p>analyse some simulated data. This simulation will point out the importance of the
</p>
<p>covariances in testing means. We created two independent normal samples in R4 of
</p>
<p>sizes n1 D 30 and n2 D 20 with:
</p>
<p>�1 D .8; 6; 10; 10/&gt;
</p>
<p>�2 D .6; 6; 10; 13/&gt;:
</p>
<p>One may consider this as an example of X D .X1; : : : ; Xn/&gt; being the students&rsquo;
scores from four tests, where the two groups of students were subjected to two
</p>
<p>different methods of teaching. First we simulate the two samples with &dagger; D I4 and
obtain the statistics:
</p>
<p>Nx1 D .7:607; 5:945; 10:213; 9:635/&gt;
</p>
<p>Nx2 D .6:222; 6:444; 9:560; 13:041/&gt;
</p>
<p>S1 D
</p>
<p>0
BB@
</p>
<p>0:812 �0:229 �0:034 0:073
�0:229 1:001 0:010 �0:059
�0:034 0:010 1:078 �0:098
0:073 �0:059 �0:098 0:823
</p>
<p>1
CCA
</p>
<p>S2 D
</p>
<p>0
BB@
</p>
<p>0:559 �0:057 �0:271 0:306
�0:057 1:237 0:181 0:021
�0:271 0:181 1:159 �0:130
0:306 0:021 �0:130 0:683
</p>
<p>1
CCA :
</p>
<p>The test statistic (7.14) takes the value F D 60:65 which is highly significant:
the small variance allows the difference to be detected even with these relatively
</p>
<p>moderate sample sizes. We conclude (at the 95% level) that:
</p>
<p>0:6213 � ı1 � 2:2691
�1:5217 � ı2 � 0:5241
�0:3766 � ı3 � 1:6830
�4:2614 � ı4 � �2:5494
</p>
<p>which confirms that the means for X1 and X4 are different.</p>
<p/>
</div>
<div class="page"><p/>
<p>234 7 Hypothesis Testing
</p>
<p>Consider now a different simulation scenario where the standard deviations are
</p>
<p>four times larger:&dagger; D 16I4. Here we obtain:
</p>
<p>Nx1 D .7:312; 6:304; 10:840; 10:902/&gt;
</p>
<p>Nx2 D .6:353; 5:890; 8:604; 11:283/&gt;
</p>
<p>S1 D
</p>
<p>0
BB@
</p>
<p>21:907 1:415 �2:050 2:379
1:415 11:853 2:104 �1:864
�2:050 2:104 17:230 0:905
2:379 �1:864 0:905 9:037
</p>
<p>1
CCA
</p>
<p>S2 D
</p>
<p>0
BB@
</p>
<p>20:349 �9:463 0:958 �6:507
�9:463 15:502 �3:383 �2:551
0:958 �3:383 14:470 �0:323
�6:507 �2:551 �0:323 10:311
</p>
<p>1
CCA :
</p>
<p>Now the test statistic takes the value 1.54 which is no longer significant (F0:95;4;45 D
2:58). Now we cannot reject the null hypothesis (which we know to be false!) since
</p>
<p>the increase in variances prohibits the detection of differences of such magnitude.
</p>
<p>The following situation illustrates once more the role of the covariances between
</p>
<p>covariates. Suppose that &dagger; D 16I4 as above but with �14 D �41 D �3:999 (this
corresponds to a negative correlation r41 D �0:9997). We have:
</p>
<p>Nx1 D .8:484; 5:908; 9:024; 10:459/&gt;
</p>
<p>Nx2 D .4:959; 7:307; 9:057; 13:803/&gt;
</p>
<p>S1 D
</p>
<p>0
BB@
</p>
<p>14:649 �0:024 1:248 �3:961
�0:024 15:825 0:746 4:301
1:248 0:746 9:446 1:241
</p>
<p>�3:961 4:301 1:241 20:002
</p>
<p>1
CCA
</p>
<p>S2 D
</p>
<p>0
BB@
</p>
<p>14:035 �2:372 5:596 �1:601
�2:372 9:173 �2:027 �2:954
5:596 �2:027 9:021 �1:301
�1:601 �2:954 �1:301 9:593
</p>
<p>1
CCA :
</p>
<p>The value of F is 3:853 which is significant at the 5% level (p-value D 0:0089).
So the null hypothesis ı D �1 � �2 D 0 is outside the 95% confidence ellipsoid.
However, the simultaneous confidence intervals, which do not take the covariances
</p>
<p>into account are given by:
</p>
<p>�0:1837 � ı1 � 7:2343
�4:9452 � ı2 � 2:1466
�3:0091 � ı3 � 2:9438
�7:2336 � ı4 � 0:5450:</p>
<p/>
</div>
<div class="page"><p/>
<p>7.2 Linear Hypothesis 235
</p>
<p>They contain the null value (see Remark 7.1 above) although they are very
</p>
<p>asymmetric for ı1 and ı4.
</p>
<p>Example 7.15 Let us compare the vectors of means of the forged and the genuine
</p>
<p>bank notes. The matrices Sf and Sg were given in Example 3.1 and since here
</p>
<p>nf D ng D 100, S is the simple average of Sf and Sg W S D 12
�
Sf C Sg
</p>
<p>�
.
</p>
<p>Nxg D .214:97; 129:94; 129:72; 8:305; 10:168; 141:52/&gt;
</p>
<p>Nxf D .214:82; 130:3; 130:19; 10:53; 11:133; 139:45/&gt;:
</p>
<p>The test statistic is given by (7.14) and turns out to be F D 391:92 which is highly
significant forF6;193. The 95% simultaneous confidence intervals for the differences
</p>
<p>ıj D �gj � �fj ; j D 1; : : : ; p are:
</p>
<p>�0:0443 � ı1 � 0:3363
�0:5186 � ı2 � �0:1954
�0:6416 � ı3 � �0:3044
�2:6981 � ı4 � �1:7519
�1:2952 � ı5 � �0:6348
1:8072 � ı6 � 2:3268:
</p>
<p>All of the components (except for the first one) show significant differences in the
</p>
<p>means. The main effects are taken by the lower border .X4/ and the diagonal .X6/.
</p>
<p>The preceding test implicitly uses the fact that the two samples are extracted
</p>
<p>from two different populations with common variance &dagger;. In this case, the test
</p>
<p>statistic (7.14) measures the distance between the two centers of gravity of the two
</p>
<p>groups w.r.t. the commonmetric given by the pooled variance matrix S. If&dagger;1 6D &dagger;2
no such matrix exists. There are no satisfactory test procedures for testing the
</p>
<p>equality of variancematrices which are robust with respect to normality assumptions
</p>
<p>of the populations. The following test extends Bartlett&rsquo;s test for equality of variances
</p>
<p>in the univariate case. But this test is known to be very sensitive to departures from
</p>
<p>normality.
</p>
<p>Test Problem 9 (Comparison of CovarianceMatrices). LetXih � Np.�h; &dagger;h/,
i D 1; : : : ; nh, h D 1; : : : ; k be independent random variables,
</p>
<p>H0 W &dagger;1 D &dagger;2 D � � � D &dagger;k versusH1 W no constraints.
</p>
<p>Each sub-sample provides Sh, an estimator of &dagger;h, with
</p>
<p>nhSh � Wp.&dagger;h; nh � 1/:</p>
<p/>
</div>
<div class="page"><p/>
<p>236 7 Hypothesis Testing
</p>
<p>Under H0,
Pk
</p>
<p>hD1 nhSh � Wp.&dagger;; n � k/ (Sect. 5.2), where &dagger; is the common
covariance matrix Xih and n D
</p>
<p>Pk
hD1 nh. Let S D n1S1C���CnkSkn be the weighted
</p>
<p>average of the Sh (this is in fact the MLE of &dagger; whenH0 is true). The LRT leads to
</p>
<p>the statistic
</p>
<p>� 2 log� D n log j S j �
kX
</p>
<p>hD1
nh log j Sh j (7.16)
</p>
<p>which underH0 is approximately distributed as a X
2
m wherem D 12 .k�1/p.pC1/.
</p>
<p>Example 7.16 Let&rsquo;s come back to Example 7.13, where the mean of assets and
</p>
<p>sales have been compared for companies from the energy and manufacturing sector
</p>
<p>assuming that &dagger;1 D &dagger;2. The test of &dagger;1 D &dagger;2 leads to the value of the test statistic
</p>
<p>� 2 log� D 0:9076 (7.17)
</p>
<p>which is not significant (p-value for a �23 D 0:82). We cannot reject H0 and the
comparison of the means performed above is valid.
</p>
<p>Example 7.17 Let us compare the covariancematrices of the forged and the genuine
</p>
<p>bank notes (the matrices Sf and Sg are shown in Example 3.1). A first look seems
</p>
<p>to suggest that &dagger;1 &curren; &dagger;2. The pooled variance S is given by S D 12
�
Sf C Sg
</p>
<p>�
</p>
<p>since here nf D ng . The test statistic here is �2 log� D 127:21, which is highly
significant �2 with 21 degrees of freedom. As expected, we reject the hypothesis
</p>
<p>of equal covariance matrices, and as a result the procedure for comparing the two
</p>
<p>means in Example 7.15 is not valid.
</p>
<p>What can we do with unequal covariance matrices? When both n1 and n2 are large,
</p>
<p>we have a simple solution:
</p>
<p>Test Problem 10 (Comparison of Two Means, Unequal Covariance Matrices,
</p>
<p>Large Samples). Assume that Xi1 � Np.�1; &dagger;1/, with i D 1; : : : ; n1 and
Xj 2 � Np.�2; &dagger;2/, with j D 1; : : : ; n2 are independent random variables.
</p>
<p>H0 W �1 D �2 versusH1 W no constraints.
</p>
<p>Letting ı D �1 � �2, we have
</p>
<p>. Nx1 � Nx2/ � Np
�
ı;
&dagger;1
</p>
<p>n1
C &dagger;2
n2
</p>
<p>�
:</p>
<p/>
</div>
<div class="page"><p/>
<p>7.2 Linear Hypothesis 237
</p>
<p>Therefore, by (5.4)
</p>
<p>. Nx1 � Nx2/&gt;
�
&dagger;1
</p>
<p>n1
C &dagger;2
n2
</p>
<p>��1
. Nx1 � Nx2/ � �2p:
</p>
<p>Since Si is a consistent estimator of &dagger;i for i D 1; 2, we have
</p>
<p>. Nx1 � Nx2/&gt;
�
S1
</p>
<p>n1
C S2
n2
</p>
<p>��1
. Nx1 � Nx2/
</p>
<p>L! �2p : (7.18)
</p>
<p>This can be used in place of (7.13) for testing H0, defining a confidence region for
</p>
<p>ı or constructing simultaneous confidence intervals for ıj ; j D 1; : : : ; p.
For instance, the rejection region at the level ˛ will be
</p>
<p>. Nx1 � Nx2/&gt;
�
S1
</p>
<p>n1
C S2
n2
</p>
<p>��1
. Nx1 � Nx2/ &gt; �21�˛Ip (7.19)
</p>
<p>and the .1 � ˛/ simultaneous confidence intervals for ıj , j D 1; : : : ; p are:
</p>
<p>ıj 2 . Nx1 � Nx2/˙
</p>
<p>vuut�21�˛Ip
</p>
<p> 
s
.1/
jj
</p>
<p>n1
C
s
.2/
jj
</p>
<p>n2
</p>
<p>!
; (7.20)
</p>
<p>where s
.i/
jj is the .j; j / element of the matrix Si . This may be compared to (7.15)
</p>
<p>where the pooled variance was used.
</p>
<p>Remark 7.2 We see, by comparing the statistics (7.19) with (7.14), that we measure
</p>
<p>here the distance between Nx1 and Nx2 using the metric
�
S1
n1
C S2
</p>
<p>n2
</p>
<p>�
. It should be
</p>
<p>noted that when n1 D n2, the two methods are essentially the same since then
S D 1
</p>
<p>2
.S1 C S2/. If the covariances are different but have the same eigenvectors
</p>
<p>(different eigenvalues), one can apply the common principal component (CPC)
</p>
<p>technique, see Chap. 11.
</p>
<p>Example 7.18 Let us use the last test to compare the forged and the genuine bank
</p>
<p>notes again (n1 and n2 are both large). The test statistic (7.19) turns out to be 2,436.8
</p>
<p>which is again highly significant. The 95% simultaneous confidence intervals are:
</p>
<p>�0:0389 � ı1 � 0:3309
�0:5140 � ı2 � �0:2000
�0:6368 � ı3 � �0:3092
�2:6846 � ı4 � �1:7654
�1:2858 � ı5 � �0:6442
1:8146 � ı6 � 2:3194
</p>
<p>showing that all the components except the first are different from zero, the largest
</p>
<p>difference coming from X6 (length of the diagonal) and X4 (lower border). The</p>
<p/>
</div>
<div class="page"><p/>
<p>238 7 Hypothesis Testing
</p>
<p>results are very similar to those obtained in Example 7.15. This is due to the fact
</p>
<p>that here n1 D n2 as we already mentioned in the remark above.
</p>
<p>Profile Analysis
</p>
<p>Another useful application of Test Problem 6 is the repeated measurements problem
</p>
<p>applied to two independent groups. This problem arises in practice when we
</p>
<p>observe repeated measurements of characteristics (or measures of the same type
</p>
<p>under different experimental conditions) on the different groups which have to be
</p>
<p>compared. It is important that the p measures (the &ldquo;profile&rdquo;) are comparable, and,
</p>
<p>in particular, are reported in the same units. For instance, they may be measures
</p>
<p>of blood pressure at p different points in time, one group being the control group
</p>
<p>and the other the group receiving a new treatment. The observations may be the
</p>
<p>scores obtained from p different tests of two different experimental groups. One is
</p>
<p>then interested in comparing the profiles of each group: the profile being just the
</p>
<p>vectors of the means of the p responses (the comparison may be visualised in a
</p>
<p>two-dimensional graph using the parallel coordinate plot introduced in Sect. 1.7).
</p>
<p>We are thus in the same statistical situation as for the comparison of two means:
</p>
<p>Xi1 � Np .�1; &dagger;/ i D 1; : : : ; n1
</p>
<p>Xi2 � Np .�2; &dagger;/ i D 1; : : : ; n2;
</p>
<p>where all variables are independent. Suppose the two population profiles look like
</p>
<p>in Fig. 7.1.
</p>
<p>The following questions are of interest:
</p>
<p>1. Are the profiles similar in the sense of being parallel (which means no interaction
</p>
<p>between the treatments and the groups)?
</p>
<p>2. If the profiles are parallel, are they at the same level?
</p>
<p>3. If the profiles are parallel, is there any treatment effect, i.e. are the profiles
</p>
<p>horizontal (profiles remain the same no matter which treatment received)?
</p>
<p>The above questions are easily translated into linear constraints on the means and a
</p>
<p>test statistic can be obtained accordingly.
</p>
<p>Parallel Profiles
</p>
<p>Let C be a .p � 1/ � p matrix defined as C D
</p>
<p>0
BBB@
</p>
<p>1 �1 0 � � � 0
0 1 �1 � � � 0
:::
</p>
<p>:::
:::
:::
</p>
<p>:::
</p>
<p>0 � � � 0 1 �1
</p>
<p>1
CCCA :</p>
<p/>
</div>
<div class="page"><p/>
<p>7.2 Linear Hypothesis 239
</p>
<p>1 2 3 4 5
0
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>4
</p>
<p>5
</p>
<p>Population Profiles
</p>
<p>Treatment
</p>
<p>M
e
</p>
<p>a
n
</p>
<p>Fig. 7.1 Example of population profiles MVAprofil
</p>
<p>The hypothesis to be tested is
</p>
<p>H
.1/
0 W C.�1 � �2/ D 0:
</p>
<p>From (7.11), (7.12) and Corollary 5.4 we know that underH0:
</p>
<p>n1n2
</p>
<p>.n1 C n2/2
.n1 C n2 � 2/ fC. Nx1 � Nx2/g&gt; .CSC&gt;/�1C. Nx1 � Nx2/ � T 2p�1;n1Cn2�2;
</p>
<p>(7.21)
</p>
<p>where S is the pooled covariance matrix. The hypothesis is rejected if
</p>
<p>n1n2.n1 C n1 � p/
.n1 C n2/2.p � 1/
</p>
<p>.C Nx/&gt;
�
CSC&gt;
</p>
<p>��1
C Nx &gt; F1�˛Ip�1;n1Cn2�p:
</p>
<p>Equality of Two Levels
</p>
<p>The question of equality of the two levels is meaningful only if the two profiles are
</p>
<p>parallel. In the case of interactions (rejection of H
.1/
0 ), the two populations react
</p>
<p>differently to the treatments and the question of the level has no meaning.
</p>
<p>The equality of the two levels can be formalised as
</p>
<p>H
.2/
0 W 1&gt;p .�1 � �2/ D 0</p>
<p/>
</div>
<div class="page"><p/>
<p>240 7 Hypothesis Testing
</p>
<p>since
</p>
<p>1&gt;p . Nx1 � Nx2/ � N1
�
1&gt;p .�1 � �2/;
</p>
<p>n1 C n2
n1n2
</p>
<p>1&gt;p&dagger;1p
</p>
<p>�
</p>
<p>and
</p>
<p>.n1 C n2/1&gt;p S1p � W1.1&gt;p&dagger;1p; n1 C n2 � 2/:
</p>
<p>Using Corollary 5.4 we have that:
</p>
<p>n1n2
</p>
<p>.n1 C n2/2
.n1 C n2 � 2/
</p>
<p>n
1&gt;p . Nx1 � Nx2/
</p>
<p>o2
</p>
<p>1&gt;p S1p
� T 21;n1Cn2�2 (7.22)
</p>
<p>D F1;n1Cn2�2:
</p>
<p>The rejection region is
</p>
<p>n1n2.n1 C n2 � 2/
.n1 C n2/2
</p>
<p>n
1&gt;p . Nx1 � Nx2/
</p>
<p>o2
</p>
<p>1&gt;p S1p
&gt; F1�˛I1;n1Cn2�2:
</p>
<p>Treatment Effect
</p>
<p>If it is rejected that the profiles are parallel, then two independent analyses should
</p>
<p>be done on the two groups using the repeated measurement approach. But if it is
</p>
<p>accepted that they are parallel, then we can exploit the information contained in
</p>
<p>both groups (possibly at different levels) to test a treatment effect, i.e. if the two
</p>
<p>profiles are horizontal. This may be written as:
</p>
<p>H
.3/
0 W C.�1 C �2/ D 0:
</p>
<p>Consider the average profile Nx
</p>
<p>Nx D n1 Nx1 C n2 Nx2
n1 C n2
</p>
<p>:
</p>
<p>Clearly,
</p>
<p>Nx � Np
�
n1�1 C n2�2
n1 C n2
</p>
<p>;
1
</p>
<p>n1 C n2
&dagger;
</p>
<p>�
:</p>
<p/>
</div>
<div class="page"><p/>
<p>7.2 Linear Hypothesis 241
</p>
<p>Now it is not hard to prove thatH
.3/
0 with H
</p>
<p>.1/
0 implies that
</p>
<p>C
</p>
<p>�
n1�1 C n2�2
n1 C n2
</p>
<p>�
D 0:
</p>
<p>So under parallel, horizontal profiles we have
</p>
<p>p
n1 C n2C Nx � Np.0; C&dagger;C&gt;/:
</p>
<p>From Corollary 5.4 we again obtain
</p>
<p>.n1 C n2 � 2/.C Nx/&gt;.CSC&gt;/�1C Nx � T 2p�1;n1Cn2�2: (7.23)
</p>
<p>This leads to the rejection region ofH
.3/
0 , namely
</p>
<p>n1 C n2 � p
p � 1 .C Nx/
</p>
<p>&gt;.CSC&gt;/�1C Nx &gt; F1�˛Ip�1;n1Cn2�p:
</p>
<p>Example 7.19 Morrison (1990) proposed a test in which the results of four sub-tests
</p>
<p>of the Wechsler Adult Intelligence Scale (WAIS) are compared for two categories of
</p>
<p>people: group 1 contains n1 D 37 people who do not have a senile factor and group
2 contains n2 D 12 people who have a senile factor. The four WAIS sub-tests areX1
(information), X2 (similarities), X3 (arithmetic) and X4 (picture completion). The
</p>
<p>relevant statistics are
</p>
<p>Nx1 D .12:57; 9:57; 11:49; 7:97/&gt;
</p>
<p>Nx2 D .8:75; 5:33; 8:50; 4:75/&gt;
</p>
<p>S1 D
</p>
<p>0
BB@
</p>
<p>11:164 8:840 6:210 2:020
</p>
<p>8:840 11:759 5:778 0:529
</p>
<p>6:210 5:778 10:790 1:743
</p>
<p>2:020 0:529 1:743 3:594
</p>
<p>1
CCA
</p>
<p>S2 D
</p>
<p>0
BB@
</p>
<p>9:688 9:583 8:875 7:021
</p>
<p>9:583 16:722 11:083 8:167
</p>
<p>8:875 11:083 12:083 4:875
</p>
<p>7:021 8:167 4:875 11:688
</p>
<p>1
CCA :
</p>
<p>The test statistic for testing if the two profiles are parallel is F D 0:4634, which
is not significant (p-value D 0:71). Thus it is accepted that the two are parallel.
The second test statistic (testing the equality of the levels of the two profiles) is
</p>
<p>F D 17:21, which is highly significant (p-value � 10�4). The global level of the
test for the non-senile people is superior to the senile group. The final test (testing
</p>
<p>the horizontality of the average profile) has the test statistic F D 53:32, which is</p>
<p/>
</div>
<div class="page"><p/>
<p>242 7 Hypothesis Testing
</p>
<p>also highly significant (p-value � 10�14). This implies that there are substantial
differences among the means of the different subtests.
</p>
<p>Summary
</p>
<p>,! Hypotheses about � can often be written as A� D a, with matrix
A, and vector a.
</p>
<p>,! The hypothesis H0 W A� D a for X � Np.�;&dagger;/ with &dagger; known
leads to �2 log� D n.Ax�a/&gt;.A&dagger;A&gt;/�1.Ax�a/ � �2q , where
q is the number of elements in a.
</p>
<p>,! The hypothesisH0 W A� D a for X � Np.�;&dagger;/ with &dagger; unknown
leads to�2 log� D n logf1C.Ax�a/&gt;.ASA&gt;/�1.Ax�a/g �!
�2q , where q is the number of elements in a and we have an exact
</p>
<p>test .n� 1/.A Nx � a/&gt;.ASA&gt;/�1.A Nx � a/ � T 2q;n�1:
,! The hypothesis H0 W Aˇ D a for Yi � N1.ˇ&gt;xi ; �2/ with �2
</p>
<p>unknown leads to �2 log� D n
2
log
</p>
<p>�
jjy�X Q̌jj2
jjy�X Ǒjj2 � 1
</p>
<p>�
�! �2q , with
</p>
<p>q being the length of a and with
</p>
<p>n � p
q
</p>
<p>�
A Ǒ � a
</p>
<p>� n
A
�
X&gt;X
</p>
<p>��1
A&gt;
</p>
<p>o�1 �
A Ǒ � a
</p>
<p>�
</p>
<p>�
y � X Ǒ
</p>
<p>�&gt; �
y � X Ǒ
</p>
<p>� � Fq;n�p:
</p>
<p>7.3 Boston Housing
</p>
<p>Returning to the Boston Housing data set, we are now in a position to test if the
</p>
<p>means of the variables vary according to their location, for example, when they are
</p>
<p>located in a district with high valued houses. In Chap. 1, we built two groups of
</p>
<p>observations according to the value of X14 being less than or equal to the median of
</p>
<p>X14 (a group of 256 districts) and greater than the median (a group of 250 districts).
</p>
<p>In what follows, we use the transformed variables motivated in Sect. 1.9.
</p>
<p>Testing the equality of the means from the two groups was proposed in a
</p>
<p>multivariate setup, so we restrict the analysis to the variables X1; X5; X8; X11, and
</p>
<p>X13 to see if the differences between the two groups that were identified in Chap. 1
</p>
<p>can be confirmed by a formal test. As in Test Problem 8, the hypothesis to be tested is
</p>
<p>H0 W �1 D �2; where �1 2 R5; n1 D 256; and n2 D 250:</p>
<p/>
</div>
<div class="page"><p/>
<p>7.3 Boston Housing 243
</p>
<p>&dagger; is not known. The F -statistic given in (7.13) is equal to 126.30, which is much
</p>
<p>higher than the critical value F0:95I5;500 D 2:23. Therefore, we reject the hypothesis
of equal means.
</p>
<p>To see which component, X1; X5; X8; X11, or X13, is responsible for this
</p>
<p>rejection, take a look at the simultaneous confidence intervals defined in (7.14):
</p>
<p>ı1 2 . 1:4020; 2:5499/
ı5 2 . 0:1315; 0:2383/
ı8 2 .�0:5344;�0:2222/
ı11 2 . 1:0375; 1:7384/
ı13 2 . 1:1577; 1:5818/:
</p>
<p>These confidence intervals confirm that all of the ıj are significantly different from
</p>
<p>zero (note there is a negative effect for X8: weighted distances to employment
</p>
<p>centers) MVAsimcibh.
</p>
<p>We could also check if the factor &ldquo;being bounded by the river&rdquo; (variable
</p>
<p>X4) has some effect on the other variables. To do this compare the means of
</p>
<p>.X5; X8; X9; X12; X13; X14/
&gt;. There are two groups: n1 D 35 districts bounded
</p>
<p>by the river and n2 D 471 districts not bounded by the river. Test Problem 8
(H0 W �1 D �2) is applied again with p D 6. The resulting test statistic, F D 5:81,
is highly significant (F0:95I6;499 D 2:12). The simultaneous confidence intervals
indicate that only X14 (the value of the houses) is responsible for the hypothesis
</p>
<p>being rejected. At a significance level of 0.95
</p>
<p>ı5 2 .�0:0603; 0:1919/
ı8 2 .�0:5225; 0:1527/
ı9 2 .�0:5051; 0:5938/
ı12 2 .�0:3974; 0:7481/
ı13 2 .�0:8595; 0:3782/
ı14 2 . 0:0014; 0:5084/:
</p>
<p>Testing Linear Restrictions
</p>
<p>In Chap. 3 a linear model was proposed that explained the variations of the priceX14
by the variations of the other variables. Using the same procedure that was shown
</p>
<p>in Testing Problem 7, we are in a position to test a set of linear restrictions on the
</p>
<p>vector of regression coefficients ˇ.</p>
<p/>
</div>
<div class="page"><p/>
<p>244 7 Hypothesis Testing
</p>
<p>The model we estimated in Sect. 3.7 provides the following ( MVAlinregbh):
</p>
<p>Variable Ǒj SE . Ǒj / t p-Value
Constant 4:1769 0:3790 11:020 0:0000
</p>
<p>X1 �0:0146 0:0117 �1:254 0:2105
X2 0:0014 0:0056 0:247 0:8051
</p>
<p>X3 �0:0127 0:0223 �0:570 0:5692
X4 0:1100 0:0366 3:002 0:0028
</p>
<p>X5 �0:2831 0:1053 �2:688 0:0074
X6 0:4211 0:1102 3:822 0:0001
</p>
<p>X7 0:0064 0:0049 1:317 0:1885
</p>
<p>X8 �0:1832 0:0368 �4:977 0:0000
X9 0:0684 0:0225 3:042 0:0025
</p>
<p>X10 �0:2018 0:0484 �4:167 0:0000
X11 �0:0400 0:0081 �4:946 0:0000
X12 0:0445 0:0115 3:882 0:0001
</p>
<p>X13 �0:2626 0:0161 �16:320 0:0000
</p>
<p>Recall that the estimated residuals Y � X Ǒ did not show a big departure
from normality, which means that the testing procedure developed above can be
</p>
<p>used.
</p>
<p>1. First a global test of significance for the regression coefficients is performed,
</p>
<p>H0 W .ˇ1; : : : ; ˇ13/ D 0:
</p>
<p>This is obtained by definingA D .013; I13/ and a D 013 so thatH0 is equivalent
to Aˇ D a where ˇ D .ˇ0; ˇ1; : : : ; ˇ13/&gt;. Based on the observed values F D
123:20. This is highly significant (F0:95I13;492 D 1:7401), thus we rejectH0. Note
that underH0 ǑH0 D .3:0345; 0; : : : ; 0/ where 3:0345 D y.
</p>
<p>2. Since we are interested in the effect that being located close to the river has on
</p>
<p>the value of the houses, the second test isH0 W ˇ4 D 0. This is done by fixing
</p>
<p>A D .0; 0; 0; 0; 1; 0; 0; 0; 0; 0; 0; 0; 0; 0/&gt;
</p>
<p>and a D 0 to obtain the equivalent hypothesisH0 W Aˇ D a. The result is again
significant: F D 9:0125 (F0:95I1;492 D 3:8604) with a p-value of 0.0028. Note
that this is the same p-value obtained in the individual test ˇ4 D 0 in Chap. 3,
computed using a different setup.
</p>
<p>3. A third test notices the fact that some of the regressors in the full model (3.57)
</p>
<p>appear to be insignificant (that is they have high individual p-values). It can
</p>
<p>be confirmed from a joint test if the corresponding reduced model, formulated</p>
<p/>
</div>
<div class="page"><p/>
<p>7.4 Exercises 245
</p>
<p>Table 7.1 Linear regression
for Boston housing data set
MVAlinreg2bh
</p>
<p>Variable Ǒj SE t p-Value
Const 4:1582 0:3628 11:462 0:0000
</p>
<p>X4 0:1087 0:0362 2:999 0:0028
</p>
<p>X5 �0:3055 0:0973 �3:140 0:0018
X6 0:4668 0:1059 4:407 0:0000
</p>
<p>X8 �0:1855 0:0327 �5:679 0:0000
X9 0:0492 0:0183 2:690 0:0074
</p>
<p>X10 �0:2096 0:0446 �4:705 0:0000
X11 �0:0410 0:0078 �5:280 0:0000
X12 0:0481 0:0112 4:306 0:0000
</p>
<p>X13 �0:2588 0:0149 �17:396 0:0000
</p>
<p>by deleting the insignificant variables, is rejected by the data. We want to test
</p>
<p>H0 W ˇ1 D ˇ2 D ˇ3 D ˇ7 D 0. Hence,
</p>
<p>A D
</p>
<p>0
BB@
</p>
<p>0 1 0 0 0 0 0 0 0 0 0 0 0 0
</p>
<p>0 0 1 0 0 0 0 0 0 0 0 0 0 0
</p>
<p>0 1 0 1 0 0 0 0 0 0 0 0 0 0
</p>
<p>0 1 0 0 0 0 0 1 0 0 0 0 0 0
</p>
<p>1
CCA
</p>
<p>and a D 04. The test statistic is 0.9344, which is not significant for F4;492. Given
that the p-value is equal to 0.44, we cannot reject the null hypothesis nor the
</p>
<p>corresponding reduced model. The value of Ǒ under the null hypothesis is
</p>
<p>Ǒ
H0 D .4:16; 0; 0; 0; 0:11;�0:31; 0:47; 0;�0:19; 0:05;�0:20;�0:04; 0:05;�0:26/&gt; :
</p>
<p>A possible reduced model is
</p>
<p>X14 D ˇ0 C ˇ4X4 C ˇ5X5 C ˇ6X6 C ˇ8X8 C � � � C ˇ13X13 C ":
</p>
<p>Estimating this reduced model using OLS, as was done in Chap. 3, provides the
</p>
<p>results shown in Table 7.1.
</p>
<p>Note that the reducedmodel has r2 D 0:763which is very close to r2 D 0:765
obtained from the full model. Clearly, including variables X1; X2; X3, and X7
does not provide valuable information in explaining the variation of X14, the
</p>
<p>price of the houses.</p>
<p/>
</div>
<div class="page"><p/>
<p>246 7 Hypothesis Testing
</p>
<p>7.4 Exercises
</p>
<p>Exercise 7.1 Use Theorem 7.1 to derive a test for testing the hypothesis that a dice
</p>
<p>is balanced, based on n tosses of that dice. (Hint: use the multinomial probability
</p>
<p>function.)
</p>
<p>Exercise 7.2 ConsiderN3.�;&dagger;/. Formulate the hypothesisH0 W �1 D �2 D �3 in
terms of A� D a.
</p>
<p>Exercise 7.3 Simulate a normal sample with � D
�
1
2
</p>
<p>�
and &dagger; D
</p>
<p>�
1
0:5
</p>
<p>0:5
2
</p>
<p>�
and test
</p>
<p>H0 W 2�1 � �2 D 0:2 first with &dagger; known and then with &dagger; unknown. Compare the
results.
</p>
<p>Exercise 7.4 Derive expression (7.3) for the LRT statistic in Test Problem 2.
</p>
<p>Exercise 7.5 With the simulated data set of Example 7.14, test the hypothesis of
</p>
<p>equality of the covariance matrices.
</p>
<p>Exercise 7.6 In the US companies data set, test the equality of means between the
</p>
<p>energy and manufacturing sectors, taking the full vector of observations X1 to X6.
</p>
<p>Derive the simultaneous confidence intervals for the differences.
</p>
<p>Exercise 7.7 Let X � N2.�;&dagger;/ where &dagger; is known to be &dagger; D
�
</p>
<p>2 �1
�1 2
</p>
<p>�
. We
</p>
<p>have an i.i.d. sample of size n D 6 providing Nx&gt; D
�
1 1
2
</p>
<p>�
. Solve the following test
</p>
<p>problems (˛ D 0:05):
</p>
<p>(a) H0 W � D
�
2; 2
</p>
<p>3
</p>
<p>�&gt;
H1 W � &curren;
</p>
<p>�
2; 2
</p>
<p>3
</p>
<p>�&gt;
</p>
<p>(b) H0 W �1 C �2 D 72 H1 W �1 C �2 &curren;
7
2
</p>
<p>(c) H0 W �1 � �2 D 12 H1 W �1 � �2 &curren;
1
2
</p>
<p>(d) H0 W �1 D 2 H1 W �1 &curren; 2
For each case, represent the rejection region graphically (comment).
</p>
<p>Exercise 7.8 Repeat the preceding exercise with &dagger; unknown and S D
�
</p>
<p>2 �1
�1 2
</p>
<p>�
.
</p>
<p>Compare the results.
</p>
<p>Exercise 7.9 Consider X � N3.�;&dagger;/. An i.i.d. sample of size n D 10 provides:
</p>
<p>Nx D .1; 0; 2/&gt;
</p>
<p>S D
</p>
<p>0
@
3 2 1
</p>
<p>2 3 1
</p>
<p>1 1 4
</p>
<p>1
A :</p>
<p/>
</div>
<div class="page"><p/>
<p>7.4 Exercises 247
</p>
<p>(a) Knowing that the eigenvalues of S are integers, describe a 95% confidence
</p>
<p>region for �. (Hint: to compute eigenvalues use jS j D
3Q
</p>
<p>jD1
�j and tr.S/ D
</p>
<p>3P
jD1
</p>
<p>�j .)
</p>
<p>(b) Calculate the simultaneous confidence intervals for �1; �2 and �3.
</p>
<p>(c) Can we assert that �1 is an average of �2 and �3?
</p>
<p>Exercise 7.10 Consider two independent i.i.d. samples, each of size 10, from two
</p>
<p>bivariate normal populations. The results are summarised below:
</p>
<p>Nx1 D .3; 1/&gt;I Nx2 D .1; 1/&gt;
</p>
<p>S1 D
�
</p>
<p>4 �1
�1 2
</p>
<p>�
I S2 D
</p>
<p>�
2 �2
�2 4
</p>
<p>�
:
</p>
<p>Provide a solution to the following tests:
</p>
<p>(a) H0 W �1 D �2 H1 W �1 6D �2
(b) H0 W �11 D �21 H1 W �11 6D �21
(c) H0 W �12 D �22 H1 W �12 6D �22
</p>
<p>Compare the solutions and comment.
</p>
<p>Exercise 7.11 Prove expression (7.4) in the Test Problem 2 with log-likelihoods `�0
and `�1 . [Hint: use (2.29).]
</p>
<p>Exercise 7.12 Assume that X � Np.�;&dagger;/ where &dagger; is unknown.
(a) Derive the log LRT for testing the independence of the p components, that is
</p>
<p>H0 W &dagger; is a diagonal matrix. (Solution: �2 log� D �n log jRj where R is the
correlation matrix, which is asymptotically a �21
</p>
<p>2p.p�1/
underH0.)
</p>
<p>(b) Assume that&dagger; is a diagonal matrix (all the variables are independent). Can an
</p>
<p>asymptotic test forH0 W � D �o againstH1 W � &curren; �o be derived? How would
this compare to p independent univariate t-tests on each �j ?
</p>
<p>(c) Show an easy derivation of an asymptotic test for testing the equality of the p
</p>
<p>means [Hint: use .C NX/&gt;.CSC&gt;/�1C NX ! �2p�1 where S D diag.s11; : : : ; spp/
and C is defined as in (7.10)]. Compare this to the simple ANOVA procedure used
</p>
<p>in Sect. 3.5.
</p>
<p>Exercise 7.13 The yields of wheat have been measured in 30 parcels that have been
</p>
<p>randomly attributed to three lots prepared by one of three different fertiliser A, B and
</p>
<p>C. The data are</p>
<p/>
</div>
<div class="page"><p/>
<p>248 7 Hypothesis Testing
</p>
<p>Fertiliser yield A B C
</p>
<p>1 4 6 2
</p>
<p>2 3 7 1
</p>
<p>3 2 7 1
</p>
<p>4 5 5 1
</p>
<p>5 4 5 3
</p>
<p>6 4 5 4
</p>
<p>7 3 8 3
</p>
<p>8 3 9 3
</p>
<p>9 3 9 2
</p>
<p>10 1 6 2
</p>
<p>Using Exercise 7.12,
</p>
<p>(a) test the independence between the three variables.
</p>
<p>(b) test whether �&gt; D Œ2 6 4&#141; and compare this to the three univariate t-tests.
(c) test whether �1 D �2 D �3 using simple ANOVA and the �2 approximation.
Exercise 7.14 Consider an i.i.d. sample of size n D 5 from a bivariate normal
distribution
</p>
<p>X � N2
�
�;
</p>
<p>�
3 �
</p>
<p>� 1
</p>
<p>��
;
</p>
<p>where � is a known parameter. Suppose Nx&gt; D .1 0/. For what value of � would the
hypothesisH0 W �&gt; D .0 0/ be rejected in favour of H1 W �&gt; 6D .0 0/ (at the 5%
level)?
</p>
<p>Exercise 7.15 Using Example 7.14, test the last two cases described there and test
</p>
<p>the sample number one (n1 D 30), to see if they are from a normal population with
&dagger; D 4I4 (the sample covariance matrix to be used is given by S1).
Exercise 7.16 Consider the bank data set. For the counterfeit bank notes, we want
</p>
<p>to know if the length of the diagonal (X6) can be predicted by a linear model in X1
toX5. Estimate the linear model and test if the coefficients are significantly different
</p>
<p>from zero.
</p>
<p>Exercise 7.17 In Example 7.10, can you predict the vocabulary score of the
</p>
<p>children in eleventh grade, by knowing the results from grades 8&ndash;9 and 10? Estimate
</p>
<p>a linear model and test its significance.
</p>
<p>Exercise 7.18 Test the equality of the covariance matrices from the two groups in
</p>
<p>the WAIS subtest (Example 7.19).
</p>
<p>Exercise 7.19 Prove expressions (7.21)&ndash;(7.23).</p>
<p/>
</div>
<div class="page"><p/>
<p>7.4 Exercises 249
</p>
<p>Exercise 7.20 Using Theorem 6.3 and expression (7.16), construct an asymptotic
</p>
<p>rejection region of size ˛ for testing, in a general model f .x; �/, with � 2 Rk ;
H0 W � D �0 againstH1 W � 6D �0.
</p>
<p>Exercise 7.21 Exercise 6.5 considered the pdf f .x1; x2/ D 1�21 �22 x2 e
�
�
</p>
<p>x1
�1x2
C x2�1�2
</p>
<p>�
</p>
<p>x1; x2 &gt; 0. Solve the problem of testing H0 W �&gt; D .�01; �02/ from an iid sample of
size n on x D .x1; x2/&gt;, where n is large.
Exercise 7.22 In Olkin and Veath (1980), the evolution of citrate concentrations
</p>
<p>in plasma is observed at three different times of day, X1 (8 am), X2 (11 am) and
</p>
<p>X3 (3 pm), for two groups of patients who follow different diets. (The patients were
</p>
<p>randomly attributed to each group under a balanced design n1 D n2 D 5.)
The data are:
</p>
<p>Group X1 (8 am) X2 (11 am) X3 (3 pm)
</p>
<p>125 137 121
</p>
<p>144 173 147
</p>
<p>I 105 119 125
</p>
<p>151 149 128
</p>
<p>137 139 109
</p>
<p>93 121 107
</p>
<p>116 135 106
</p>
<p>II 109 83 100
</p>
<p>89 95 83
</p>
<p>116 128 100
</p>
<p>Test if the profiles of the groups are parallel, if they are at the same level and if
</p>
<p>they are horizontal.</p>
<p/>
</div>
<div class="page"><p/>
<p>Part III
</p>
<p>Multivariate Techniques</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 8
</p>
<p>Regression Models
</p>
<p>The aim of regression models is to model the variation of a quantitative response
</p>
<p>variable y in terms of the variation of one or several explanatory variables
</p>
<p>.x1; : : : ; xp/
&gt;. We have already introduced such models in Chaps. 3 and 7 where
</p>
<p>linear models were written in (3.50) as
</p>
<p>y D Xˇ C ";
</p>
<p>where y.n � 1/ is the vector of observation for the response variable, X .n � p/ is
the data matrix of the p explanatory variables and " are the errors. Linear models
</p>
<p>are not restricted to handle only linear relationships between y and x. Curvature is
</p>
<p>allowed by including appropriate higher order terms in the design matrix X .
</p>
<p>Example 8.1 If y represents response and x1; x2 are two factors that explain the
</p>
<p>variation of y via the quadratic response model:
</p>
<p>yi D ˇ0 C ˇ1xi1 C ˇ2xi2 C ˇ3x2i1 C ˇ4x2i2 C ˇ5xi1xi2 C "i ; i D 1; : : : ; n:
(8.1)
</p>
<p>This model (8.1) belongs to the class of linear models because it is linear in ˇ. The
</p>
<p>data matrix X is:
</p>
<p>X D
</p>
<p>0
BBB@
</p>
<p>1 x11 x12 x
2
11 x
</p>
<p>2
12 x11x12
</p>
<p>1 x21 x22 x
2
21 x
</p>
<p>2
22 x21x22
</p>
<p>:::
:::
</p>
<p>:::
:::
</p>
<p>:::
:::
</p>
<p>1 xn1 xn2 x
2
n1 x
</p>
<p>2
n2 xn1xn2
</p>
<p>1
CCCA
</p>
<p>For a given value of ˇ, the response surface can be represented in a three-
</p>
<p>dimensional plot as in Fig. 8.1 where we display y D 20 C 1x1 C 2x2 � 8x21 �
6x22 C 6x1x2, i.e. ˇ D .20; 1; 2;�8;�6;C6/&gt;.
</p>
<p>&copy; Springer-Verlag Berlin Heidelberg 2015
W.K. H&auml;rdle, L. Simar, Applied Multivariate Statistical Analysis,
DOI 10.1007/978-3-662-45171-7_8
</p>
<p>253</p>
<p/>
</div>
<div class="page"><p/>
<p>254 8 Regression Models
</p>
<p>&minus;1
&minus;0.5
</p>
<p>0
0.5
</p>
<p>1
</p>
<p>&minus;1
</p>
<p>&minus;0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
&minus;5
</p>
<p>0
</p>
<p>5
</p>
<p>10
</p>
<p>15
</p>
<p>20
</p>
<p>25
</p>
<p>X
1
</p>
<p>X
2
</p>
<p>Y
</p>
<p>Fig. 8.1 A 3-D response surface MVAresponsesurface
</p>
<p>Note also that pure non-linear models can sometimes be rewritten as a linear
</p>
<p>model by choosing an appropriate transformation of the coordinates of the variables.
</p>
<p>For instance the Cobb&ndash;Douglas production function
</p>
<p>yi D k xˇ1i1 x
ˇ2
i2 x
</p>
<p>ˇ3
i3 ;
</p>
<p>where y is the level of the production of a plant and .x1; x2; x3/
&gt; are three factors of
</p>
<p>production (e.g. labour, capital and energy), can be transformed into a linear model
</p>
<p>in the log scale. We have indeed
</p>
<p>logyi D ˇ0 C ˇ1 logxi1 C ˇ2 logxi2 C ˇ3 logxi3;
</p>
<p>where ˇ0 D log k and the ˇj ; j D 1; : : : ; 3 are the elasticities (ˇj D
@ logy=@ logxj ).
</p>
<p>Linear models are flexible and cover a wide class of models. If X has full
</p>
<p>rank, they can easily be estimated by least squares Ǒ D .X&gt;X /�1X&gt;y and linear
restrictions on the ˇ&rsquo;s can be tested using the tools developed in Chap. 7.
</p>
<p>In Chap. 3, we saw that even qualitative explanatory variables can be used by
</p>
<p>defining appropriate coding of the nominal values of x. In this chapter, we will
</p>
<p>extend our toolbox by showing how to code these qualitative factors in a way
</p>
<p>which allows the introduction of several qualitative factors including the possibility
</p>
<p>of interactions. This covers more general ANOVA models than those introduced
</p>
<p>in Chap. 3. This includes the ANCOVA models where qualitative and quantitative
</p>
<p>variables are both present in the explanatory variables.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.1 General ANOVA and ANCOVA Models 255
</p>
<p>When the response variable is qualitative or categorical (for instance, an indi-
</p>
<p>vidual can be employed or unemployed, a company may be bankrupt or not, the
</p>
<p>opinion of one person relative to a particular issue can be &ldquo;in favour&rdquo;, &ldquo;against&rdquo; or
</p>
<p>&ldquo;indifferent to&rdquo;, etc.), linear models have to be adapted to this particular situation.
</p>
<p>The most useful models for these cases will be presented in the second part of the
</p>
<p>chapter; this covers the log-linear models for contingency tables (where we analyse
</p>
<p>the relations between several categorical variables) and the logit model for quantal
</p>
<p>or binomial responses where we analyse the probability of being in one state as a
</p>
<p>function of explanatory variables.
</p>
<p>8.1 General ANOVA and ANCOVA Models
</p>
<p>8.1.1 ANOVA Models
</p>
<p>One-Factor Models
</p>
<p>In Sect. 3.5, we introduced the example of analysing the effect of one factor (three
</p>
<p>possible marketing strategies) on the sales of a product (a pullover), see Table 3.2.
</p>
<p>The standard way to present one factor ANOVA models with p levels is as follows
</p>
<p>yk` D �C ˛` C "k`; k D 1; : : : ; n`; and ` D 1; : : : ; p; (8.2)
</p>
<p>all the "k` being independent. Here ` is the label which indicates the level of the
</p>
<p>factor and ˛` is the effect of the `th level: it measures the deviation from �, the
</p>
<p>global mean of y, due to this level of the factor. In this notation, we need to
</p>
<p>impose the restriction
Pp
</p>
<p>`D1 ˛` D 0 in order to identify � as the mean of y. This
presentation is equivalent, but slightly different, to the one presented in Chap. 3
</p>
<p>(compare with Eq. (3.41)), but it allows for easier extension to the multiple factors
</p>
<p>case. Note also that here we allow different sample sizes for each level of the factor
</p>
<p>(an unbalanced design, more general than the balanced design presented in Chap. 3).
</p>
<p>To simplify the presentation, assume as in the pullover example that p D 3. In
this case, one could be tempted to write the model (8.2) under the general form of a
</p>
<p>linear model by using three indicator variables
</p>
<p>yi D �C ˛1xi1 C ˛2xi2 C ˛3xi3 C "i ;
</p>
<p>where xi` is equal to 1 or 0 according to the i th observation and belongs (or not) to
</p>
<p>the level ` of the factor. In matrix notation and letting, for simplicity, n1 D n2 D
n3 D 2 we have with ˇ D .�; ˛1; ˛2; ˛3/&gt;
</p>
<p>y D Xˇ C "; (8.3)</p>
<p/>
</div>
<div class="page"><p/>
<p>256 8 Regression Models
</p>
<p>where the design matrix X is given by:
</p>
<p>X D
</p>
<p>0
BBBBBBB@
</p>
<p>1 1 0 0
</p>
<p>1 1 0 0
</p>
<p>1 0 1 0
</p>
<p>1 0 1 0
</p>
<p>1 0 0 1
</p>
<p>1 0 0 1
</p>
<p>1
CCCCCCCA
:
</p>
<p>Unfortunately, this type of coding is not useful because the matrix X is not of full
</p>
<p>rank (the sum of each row is equal to the same constant 2) and therefore the matrix
</p>
<p>X&gt;X is not invertible. One way to overcome this problem is to change the coding
by introducing the additional constraint that the effects add up to zero. There are
</p>
<p>many ways to achieve this. Noting that ˛3 D �˛1 �˛2, we do not need to introduce
˛3 explicitly in the model. The linear model could indeed be written as
</p>
<p>yi D �C ˛1xi1 C ˛2xi2 C "i ;
</p>
<p>with a design matrix defined as
</p>
<p>X D
</p>
<p>0
BBBBBBB@
</p>
<p>1 1 0
</p>
<p>1 1 0
</p>
<p>1 0 1
</p>
<p>1 0 1
</p>
<p>1 �1 �1
1 �1 �1
</p>
<p>1
CCCCCCCA
</p>
<p>;
</p>
<p>which automatically implies that ˛3 D �.˛1 C ˛2/. The linear model (8.3) is now
correct with ˇ D .�; ˛1; ˛2/&gt;. The least squares estimator Ǒ D .X&gt;X /�1X&gt;y
can be computed providing the estimator of the ANOVA parameters � and ˛`; ` D
1; : : : ; 3. Any linear constraint on ˇ can be tested by using the techniques described
</p>
<p>in Chap. 7. For instance, the null hypothesis of no factor effect H0 W ˛1 D ˛2 D
</p>
<p>˛3 D 0 can be written asH0 W Aˇ D a, where A D
�
0 1 0
</p>
<p>0 0 1
</p>
<p>�
and a D .0 0/&gt;.
</p>
<p>Multiple-Factors Models
</p>
<p>The coding above can be extended to more general situations with many qualitative
</p>
<p>variables (factors) and with the possibility of interactions between the factors.
</p>
<p>Suppose that in a marketing example, the sales of a product can be explained by
</p>
<p>two factors: the marketing strategy with three levels (as in the pullover example) but
</p>
<p>also the location of the shop that may be either in a big shopping centre or in a less
</p>
<p>commercial location (two levels for this factor). We might also think that there is an</p>
<p/>
</div>
<div class="page"><p/>
<p>8.1 General ANOVA and ANCOVA Models 257
</p>
<p>Table 8.1 A two factor
ANOVA data set, factor A,
three levels of the marketing
strategy and factor B , two
levels for the location
</p>
<p>B1 B2
</p>
<p>A1 18 15
</p>
<p>15 20
</p>
<p>25
</p>
<p>30
</p>
<p>A2 5 10
</p>
<p>8 12
</p>
<p>8
</p>
<p>A3 10 20
</p>
<p>14 25
</p>
<p>The figures represent the resulting
sales during the same period
</p>
<p>interaction between the two factors: the marketing strategy might have a different
</p>
<p>effect in a shopping centre than in a small quiet area. To fix the idea the data are
</p>
<p>collected as in Table 8.1.
</p>
<p>The general two factor model with interactions can be written as
</p>
<p>yijk D �C ˛i C &#13;j C .˛&#13;/ij C "ijkI i D 1; : : : ; r; j D 1; : : : ; s; k D 1; : : : ; nij
(8.4)
</p>
<p>where the identification constraints are:
</p>
<p>rX
</p>
<p>iD1
˛i D 0 and
</p>
<p>sX
</p>
<p>jD1
&#13;j D 0
</p>
<p>rX
</p>
<p>iD1
.˛&#13;/ij D 0; j D 1; : : : ; s
</p>
<p>sX
</p>
<p>jD1
.˛&#13;/ij D 0; i D 1; : : : ; r:
</p>
<p>(8.5)
</p>
<p>In our example of Table 8.1 we have r D 3 and s D 2. The ˛&rsquo;s measure the
effect of the marketing strategy (three levels) and the &#13; &rsquo;s the effect of the location
</p>
<p>(two levels). A positive (negative) value of one of these parameters would indicate
</p>
<p>a favourable (unfavourable) effect on the expected sales; the global average of
</p>
<p>sales being represented by the parameter �. The interactions are measured by the
</p>
<p>parameters .˛&#13;/ij; i D 1; : : : ; r; j D 1; : : : ; s, again identification constraints
implies the .r C s/ constraints in (8.5) on the interactions terms.
</p>
<p>For example, a positive value of .˛&#13;/11 would indicate that the effect of the sale
</p>
<p>strategy A1 (advertisement in local newspaper), if any, is more favourable on the
</p>
<p>sales in the location B1 (in a big commercial centre) than in the location B2 (not
</p>
<p>a commercial centre) with the relation .˛&#13;/11 D �.˛&#13;/12. As another example,
a negative value of .˛&#13;/31 would indicate that the marketing strategy A3 (luxury</p>
<p/>
</div>
<div class="page"><p/>
<p>258 8 Regression Models
</p>
<p>presentation in shop windows) has less effect, if any, in location typeB1 than in B2:
</p>
<p>again .˛&#13;/31 D �.˛&#13;/32, etc.
The nice thing is that it is easy to extend the coding rule for one-factor model
</p>
<p>to this general situation, in order to present the model a standard linear model with
</p>
<p>the appropriate design matrix X . To build the columns of X for the effect of each
</p>
<p>factor, we will need, as above, r � 1 (and s � 1) variables for coding a qualitative
variable with r (and s, respectively) levels with the convention defined above in
</p>
<p>the one-factor case. For the interactions between a r between a r level factor and
</p>
<p>a s level factor, we will need .r � 1/ � .s � 1/ additional columns that will be
obtained by performing the product, element by element, of the correspondingmain
</p>
<p>effect columns. So, at the end, for a full model with all the interactions, we have
</p>
<p>f1C r � 1 C s � 1 C .r � 1/.s � 1/g D rs parameters where the first column of
1&rsquo;s is for the intercept (the constant �). We illustrate this for our marketing example
</p>
<p>where r D 3 and s D 2. We first describe a model without interactions.
1. Model without interactions
</p>
<p>Without the interactions (all the .˛&#13;/ij D 0) the model could be written with
3 D .r � 1/C .s � 1/ coded variables in a simple linear model form as in (8.3),
with the matrices:
</p>
<p>y D
</p>
<p>0
BBBBBBBBBBBBBBBBBBBBBBBBBB@
</p>
<p>18
</p>
<p>15
</p>
<p>15
</p>
<p>20
</p>
<p>25
</p>
<p>30
</p>
<p>5
</p>
<p>8
</p>
<p>8
</p>
<p>10
</p>
<p>12
</p>
<p>10
</p>
<p>14
</p>
<p>20
</p>
<p>25
</p>
<p>1
CCCCCCCCCCCCCCCCCCCCCCCCCCA
</p>
<p>; X D
</p>
<p>0
BBBBBBBBBBBBBBBBBBBBBBBBBB@
</p>
<p>1 1 0 1
</p>
<p>1 1 0 1
</p>
<p>1 1 0 �1
1 1 0 �1
1 1 0 �1
1 1 0 �1
1 0 1 1
</p>
<p>1 0 1 1
</p>
<p>1 0 1 1
</p>
<p>1 0 1 �1
1 0 1 �1
1 �1 �1 1
1 �1 �1 1
1 �1 �1 �1
1 �1 �1 �1
</p>
<p>1
CCCCCCCCCCCCCCCCCCCCCCCCCCA
</p>
<p>;
</p>
<p>and ˇ D .�; ˛1; ˛2; &#13;1/&gt;. Then, ˛3 D �.˛1 C ˛2/ and &#13;2 D �&#13;1.
2. Model with interactions
</p>
<p>A model with interaction between A and B is obtained by adding new columns
</p>
<p>to the design matrix. We need 2 D .r � 1/� .s � 1/ new coding variables which
are defined as the product, element-by-element, of the corresponding columns
</p>
<p>obtained for the main effects. For instance for the interaction parameter .˛&#13;/11,
</p>
<p>we multiply the column used for coding ˛1 by the column defined for coding &#13;1,
</p>
<p>where the product is element-by-element. The same is done for the parameter</p>
<p/>
</div>
<div class="page"><p/>
<p>8.1 General ANOVA and ANCOVA Models 259
</p>
<p>.˛&#13;/21. No other columns are necessary, since the remaining interactions are
</p>
<p>derived from the identification constraints (8.5). We obtain
</p>
<p>X D
</p>
<p>0
BBBBBBBBBBBBBBBBBBBBBBBBBB@
</p>
<p>1 1 0 1 1 0
</p>
<p>1 1 0 1 1 0
</p>
<p>1 1 0 �1 �1 0
1 1 0 �1 �1 0
1 1 0 �1 �1 0
1 1 0 �1 �1 0
1 0 1 1 0 1
</p>
<p>1 0 1 1 0 1
</p>
<p>1 0 1 1 0 1
</p>
<p>1 0 1 �1 0 �1
1 0 1 �1 0 �1
1 �1 �1 1 �1 �1
1 �1 �1 1 �1 �1
1 �1 �1 �1 1 1
1 �1 �1 �1 1 1
</p>
<p>1
CCCCCCCCCCCCCCCCCCCCCCCCCCA
</p>
<p>;
</p>
<p>with ˇ D .�; ˛1; ˛2; &#13;1; .˛&#13;/11; .˛&#13;/21/&gt;. The other interactions can indeed be
derived from (8.5)
</p>
<p>.˛&#13;/12 D �.˛&#13;/11
</p>
<p>.˛&#13;/22 D �.˛&#13;/21
</p>
<p>.˛&#13;/31 D � ..˛&#13;/11 C .˛&#13;/21/
</p>
<p>.˛&#13;/32 D �.˛&#13;/31:
</p>
<p>The estimation of ˇ is again simply given by the least squares solution Ǒ D
.X&gt;X /�1X&gt;y.
</p>
<p>Example 8.2 Let us come back to the marketing data provided by the two-way
</p>
<p>Table 8.1. The values of Ǒ in the full model, with interactions, are given in Table 8.2.
The p-values in the right column are for the individual tests: it appears that the
</p>
<p>interactions do not provide additional significant explanation of y, but the effect of
</p>
<p>the two factors seems significant.
</p>
<p>Using the techniques of Chap. 7, we can test some reduced model corresponding
</p>
<p>to linear constraints on the ˇ&rsquo;s. The full model is the model with all the parameters,
</p>
<p>including all the interactions. The overall fit test H0 : all the parameters, except �,
</p>
<p>are equal to zero, gives the value Fobserved D 6:5772 with a p-value of 0.0077 for
a F5;9, so that H0 is rejected. In this case, the RSSreduced D 735:3333. So there is
some effect by the factors.</p>
<p/>
</div>
<div class="page"><p/>
<p>260 8 Regression Models
</p>
<p>Table 8.2 Estimation of the
two factors ANOVA model
with data from Table 8.1
</p>
<p>Ǒ p-Values
� 15.25
</p>
<p>˛1 4.25 0.0218
</p>
<p>˛2 �6.25 0.0033
&#13;1 �3.42 0.0139
.˛&#13;/11 0.42 0.7922
</p>
<p>.˛&#13;/21 1.42 0.8096
</p>
<p>RSSfull 158.00
</p>
<p>We then test a less reduced model. We can test if the interaction terms are
</p>
<p>significantly different to zero. This is a linear constraint on ˇ with
</p>
<p>A D
�
0 0 0 0 1 0
</p>
<p>0 0 0 0 0 1
</p>
<p>�
I a D
</p>
<p>�
0
</p>
<p>0
</p>
<p>�
:
</p>
<p>Under the null we obtain:
</p>
<p>Ǒ
H0 D
</p>
<p>0
BBBBBBB@
</p>
<p>15:3035
</p>
<p>4:0975
</p>
<p>�6:0440
�3:2972
</p>
<p>0
</p>
<p>0
</p>
<p>1
CCCCCCCA
</p>
<p>;
</p>
<p>and RSSreduced D 181:8019. The observed value of F D 0:6779 which is not
significant (r D 11; f D 9) the p-valueD P.F2;9 � 0:6779/ D 0:5318, confirming
the absence of interactions.
</p>
<p>Now taking the model without the interactions as the full model, we can test
</p>
<p>if one of the main effects ˛ (marketing strategy) or &#13; (location) or both are
</p>
<p>significantly different from zero. We leave this as an exercise for the reader.
</p>
<p>8.1.2 ANCOVA Models
</p>
<p>ANCOVA (ANalysis of COVAriances) are mixed models where some variables are
</p>
<p>qualitative and others are quantitative. The same coding of the ANOVA will be used
</p>
<p>for the qualitative variable. The design matrix X is completed by the columns for
</p>
<p>the quantitative explanatory variables x. Interactions between a qualitative variable
</p>
<p>(a factor with r levels) and a quantitative one x is also possible, this corresponds to
</p>
<p>situations where the effect of x on the response y is different according to the level
</p>
<p>of the factor. This is achieved by adding into the design matrix X , a new column
</p>
<p>obtained by the product, element-by-element, of the quantitative variable with the</p>
<p/>
</div>
<div class="page"><p/>
<p>8.1 General ANOVA and ANCOVA Models 261
</p>
<p>coded variables for the factor (r � 1 interaction variables if the categorical variable
has r levels).
</p>
<p>For instance consider a simple model where a response y is explained by one
</p>
<p>explanatory variable x and one factor with two levels (for instance the gender level
</p>
<p>1 for men and level 2 for women), we would have in the case n1 D n2 D 3
</p>
<p>X D
</p>
<p>0
BBBBBBB@
</p>
<p>1 x1 1 x1
1 x2 1 x2
</p>
<p>1 x3 1 x3
1 x4 �1 �x4
1 x5 �1 �x5
1 x6 �1 �x6
</p>
<p>1
CCCCCCCA
;
</p>
<p>with ˇ D .ˇ1; ˇ2; ˇ3; ˇ4/&gt;. The intercept and the slope are .ˇ1Cˇ3/ and .ˇ1Cˇ4/
for men and .ˇ1 � ˇ3/ and .ˇ1 � ˇ4/ for women. This situation is displayed in
Fig. 8.2.
</p>
<p>Example 8.3 Consider the Car Data provided in Sect. 22.3. We want to analyse the
</p>
<p>effect of the weight (W ), the displacement (D) on the mileage (M ). But we would
</p>
<p>like to test if the origin of the car (the factor C ) has some effect on the response
</p>
<p>and if the effect of the continuous variables is different for the different levels of the
</p>
<p>factor.
</p>
<p>From the regression results in Table 8.3, we observe that only the weight affects
</p>
<p>the mileage, while the displacement does not. We also consider the origin of the car,
</p>
<p>however, both the displacement and the factor are not significant. Table 8.4 is for
</p>
<p>different factor levels.
</p>
<p>*
</p>
<p>*
*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>*
</p>
<p>* *
</p>
<p>*
</p>
<p>*
</p>
<p>*
*
</p>
<p>*
</p>
<p>*
</p>
<p>*
*
</p>
<p>*
</p>
<p>*
</p>
<p>*
*
</p>
<p>*
</p>
<p>*
</p>
<p>*
*
</p>
<p>*
</p>
<p>*
</p>
<p>* *
</p>
<p>x
</p>
<p>y
model for men
</p>
<p>model for women
</p>
<p>&deg;
&deg;
&deg;
&deg;
</p>
<p>&deg;
</p>
<p>&deg;
</p>
<p>&deg;
&deg;
</p>
<p>&deg;
&deg;
</p>
<p>&deg;
</p>
<p>&deg;
&deg;
&deg;
&deg;
</p>
<p>&deg;
</p>
<p>&deg;
</p>
<p>&deg;
&deg;
</p>
<p>&deg;
&deg;
</p>
<p>&deg;
&deg;
</p>
<p>&deg;
&deg;
&deg;
</p>
<p>&deg;
</p>
<p>&deg;
</p>
<p>&deg;
&deg;
</p>
<p>&deg;
&deg;
</p>
<p>&deg;
</p>
<p>&deg;
&deg;
&deg;
&deg;
</p>
<p>&deg;
</p>
<p>&deg;
</p>
<p>&deg;
&deg;
</p>
<p>&deg;
&deg;
</p>
<p>&deg;
</p>
<p>Fig. 8.2 A model with interaction</p>
<p/>
</div>
<div class="page"><p/>
<p>262 8 Regression Models
</p>
<p>Table 8.3 Estimation of the effects of weight and displacement on the mileage
MVAcareffect
</p>
<p>Ǒ p-Values Q̌ p-Values
� 41.0066 0.0000 43.4031 0.0000
</p>
<p>W �0.0073 0.0000 �0.0074 0.0000
D 0.0118 0.2250 0.0081 0.4140
</p>
<p>C �0.9675 0.1250
</p>
<p>Table 8.4 Different factor levels on the response MVAcareffect
</p>
<p>� p-Values W p-Values D p-Values
</p>
<p>c D 1 40.043 0.0000 �0.0065 0.0000 0.0058 0.3790
c D 2 47.557 0.0005 0.0081 0.3666 �0.3582 0.0160
c D 3 44.174 0.0002 0.0039 0.7556 �0.2650 0.3031
</p>
<p>8.1.3 Boston Housing
</p>
<p>In Chaps. 3 and 7, linear models were used to analyse if the variations of the price
</p>
<p>(the variables were transformed in Sect. 1.9) could be explained by other variables.
</p>
<p>A reduced model was obtained in Sect. 7.3 with the results shown in Table 7.1, with
</p>
<p>r2 D 0:763. The model was:
</p>
<p>X14 D ˇ0 C ˇ4X4 C ˇ5X5 C ˇ6X6 C ˇ8X8 C ˇ9X9 C ˇ10X10 C ˇ11X11
Cˇ12X12 C ˇ13X13
</p>
<p>One factor (X4) was coded as a binary variable (1, if the house is close to the
</p>
<p>Charles River and 0 if it is not). Taking advantage of the ANCOVAmodels described
</p>
<p>above, we would like to add to a new factor built from the original quantitative
</p>
<p>variableX9 D index of accessibility to radial highways. So we will transformX4 as
being 1 if close to the Charles River and �1 if not, and we will replace X9 by a new
factor coded X15 D 1 if X9 � median.X9/ and X15 D �1 if X9 &lt; median.X9/. We
also want to consider the interaction of X4 with X12 (proportion of blacks) and the
</p>
<p>interaction of X4 with the new factor X15. The results are shown in Table 8.5.
</p>
<p>Summary
</p>
<p>,! ANOVA models can be dividend into one-factor models and
multiple factor models.
</p>
<p>,! Multiple factor models analyse many qualitative variables and the
interactions between them.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.2 Categorical Responses 263
</p>
<p>Table 8.5 Estimation of the
ANCOVA model using the
Boston housing data
MVAboshousing
</p>
<p>Ǒ p-Values Q̌ p-Values
ˇ0 32.27 0.00 27.65 0.00
</p>
<p>ˇ4 1.54 0.00 �3.19 0.32
ˇ5 �17.59 0.00 �16.50 0.00
ˇ6 4.27 0.00 4.23 0.00
</p>
<p>ˇ8 �1.13 0.00 �1.10 0.00
ˇ10 0.00 0.97 0.00 0.95
</p>
<p>ˇ11 �0.97 0.00 �0.97 0.00
ˇ12 0.01 0.00 0.02 0.01
</p>
<p>ˇ13 �0.54 0.00 �0.54 0.00
ˇ15 0.21 0.46 0.23 0.66
</p>
<p>ˇ4�14 0.01 0.13
</p>
<p>ˇ4�15 0.03 0.95
</p>
<p>Summary (continued)
</p>
<p>,! ANCOVA models are mixed models with qualitative and quantita-
tive variables, and can also incorporate the interaction between a
</p>
<p>qualitative and a quantitative variable.
</p>
<p>8.2 Categorical Responses
</p>
<p>8.2.1 Multinomial Sampling and Contingency Tables
</p>
<p>In many applications, the response variable of interest is qualitative or categorical,
</p>
<p>in the sense that the response can take its nominal value in one of, say, K classes
</p>
<p>or categories. Often we observe counts yk , the number of observations in category
</p>
<p>k D 1; : : : ; K . If the total number of observations n D
PK
</p>
<p>kD1 yk is fixed and we
may assume independence of the observations, we obtain a multinomial sampling
</p>
<p>process.
</p>
<p>If we denote bypk the probability of observing the kth categorywith
PK
</p>
<p>kD1 pk D
1, we have E.Yk/ D mk D npk . The likelihood of the sample can then be written as:
</p>
<p>L D nŠQK
kD1 ykŠ
</p>
<p>KY
</p>
<p>kD1
</p>
<p>�mk
n
</p>
<p>�yk
: (8.6)</p>
<p/>
</div>
<div class="page"><p/>
<p>264 8 Regression Models
</p>
<p>In contingency tables, the categories are defined by several qualitative variables.
</p>
<p>For example in a .J�K/ two-way table, the observations (counts) yjk, j D 1; : : : ; J
and k D 1; : : : ; K are reported for row j and column k. Here n D
</p>
<p>PJ
jD1
</p>
<p>PK
kD1 yjk.
</p>
<p>Log-linear models introduce a linear structure on the logarithms of the expected
</p>
<p>frequenciesmjk D E.yjk/ D npjk, with
PJ
</p>
<p>jD1
PK
</p>
<p>kD1 pjk D 1. Log-linear structures
on mjk will impose the same structure for the pjk, the estimation of the model will
</p>
<p>then be obtained by constrained maximum likelihood. Three-way tables .J �K�L/
may be analysed in the same way.
</p>
<p>Sometimes additional information is available on explanatory variables x. In this
</p>
<p>case, the logit model will be appropriate when the categorical response is binary
</p>
<p>(K D 2). We will introduce these models when the main response of interest is
binary (for instance tables .2 � K/ or .2 � K � L/). Further, we will show how
they can be adapted to the case of contingency tables. Contingency tables are also
</p>
<p>analysed by multivariate descriptive tools in Chap. 15.
</p>
<p>8.2.2 Log-Linear Models for Contingency Tables
</p>
<p>Two-Way Tables
</p>
<p>Consider a .J � K/ two-way table, where yjk is the number of observations
having the nominal value j for the first qualitative character and nominal value
</p>
<p>k for the second character. Since the total number of observations is fixed n DPJ
jD1
</p>
<p>PK
kD1 yjk, there are JK�1 free cells in the table. The multinomial likelihood
</p>
<p>can be written as in (8.6)
</p>
<p>L D nŠQJ
jD1
</p>
<p>QK
kD1 yjkŠ
</p>
<p>JY
</p>
<p>jD1
</p>
<p>KY
</p>
<p>kD1
</p>
<p>�mjk
n
</p>
<p>�yjk
; (8.7)
</p>
<p>where we now introduce a log-linear structure to analyse the role of the rows and
</p>
<p>the columns to determine the parametersmjk D E.yjk/ (or pjk).
1. Model without interaction
</p>
<p>Suppose that there is no interaction between the rows and the columns: this
</p>
<p>corresponds to the hypothesis of independence between the two qualitative
</p>
<p>characters. In other words, pjk D pjpk for all j; k. This implies the log-linear
model:
</p>
<p>logmjk D �C ˛j C &#13;k for j D 1; : : : ; J; k D 1; : : : ; K; (8.8)
</p>
<p>where, as in ANOVAmodels for identification purposes
PJ
</p>
<p>jD1 ˛j D
PK
</p>
<p>kD1 &#13;k D
0. Using the same coding devices as above, the model can be written as
</p>
<p>logm D Xˇ: (8.9)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.2 Categorical Responses 265
</p>
<p>For a .2 � 3/ table we have:
</p>
<p>logm D
</p>
<p>0
BBBBBBB@
</p>
<p>logm11
logm12
logm13
logm21
logm22
logm23
</p>
<p>1
CCCCCCCA
; X D
</p>
<p>0
BBBBBBB@
</p>
<p>1 1 1 0
</p>
<p>1 1 0 1
</p>
<p>1 1 �1 �1
1 �1 1 0
1 �1 0 1
1 �1 �1 �1
</p>
<p>1
CCCCCCCA
; ˇ D
</p>
<p>0
BB@
</p>
<p>ˇ0
ˇ1
ˇ2
</p>
<p>ˇ3
</p>
<p>1
CCA
</p>
<p>where the first column of X is for the constant term, the second column is the
</p>
<p>coded column for the 2-levels row effect and the two last columns are the coded
</p>
<p>columns for the 3-levels column effect. The estimation is obtained bymaximising
</p>
<p>the log-likelihood which is equivalent to maximising the function L.ˇ/ in ˇ:
</p>
<p>L.ˇ/ D
JX
</p>
<p>jD1
</p>
<p>KX
</p>
<p>kD1
yjk logmjk: (8.10)
</p>
<p>The maximisation is under the constraint
P
</p>
<p>j;k mjk D n. In summary we have
1C .J � 1/C .K � 1/� 1 free parameters for JK � 1 free cells. The number of
degrees of freedom in the model is the number of free cells minus the number of
</p>
<p>free parameters. It is given by
</p>
<p>r D JK � 1 � .J � 1/ � .K � 1/ D .J � 1/ .K � 1/:
</p>
<p>In the example above, we have therefore .3�1/�.2�1/ D 2 degrees of freedom.
The original parameters of the model can then be estimated as:
</p>
<p>˛1 D ˇ1
˛2 D �ˇ1
&#13;1 D ˇ2
&#13;2 D ˇ3
&#13;3 D �.ˇ2 C ˇ3/: (8.11)
</p>
<p>2. Model with interactions
</p>
<p>In two-way tables the interactions between the two variables are of interest. This
</p>
<p>corresponds to the general (full) model
</p>
<p>logmjk D �C ˛j C &#13;k C .˛&#13;/jk; j D 1; : : : ; J; k D 1; : : : ; K; (8.12)</p>
<p/>
</div>
<div class="page"><p/>
<p>266 8 Regression Models
</p>
<p>where in addition, we have the J CK restrictions
</p>
<p>KX
</p>
<p>kD1
.˛&#13;/jk D 0; for j D 1; : : : ; J
</p>
<p>JX
</p>
<p>jD1
.˛&#13;/jk D 0; for k D 1; : : : ; K (8.13)
</p>
<p>As in the ANOVAmodel, the interactionsmay be coded by adding .J�1/.K�1/
columns to X , obtained by the product of the corresponding coded variables. In
</p>
<p>our example for the .2�3/ table the design matrixX is completed with two more
columns:
</p>
<p>X D
</p>
<p>0
BBBBBBB@
</p>
<p>1 1 1 0 1 0
</p>
<p>1 1 0 1 0 1
</p>
<p>1 1 �1 �1 �1 �1
1 �1 1 0 �1 0
1 �1 0 1 0 �1
1 �1 �1 �1 1 1
</p>
<p>1
CCCCCCCA
; ˇ D
</p>
<p>0
BBBBBBB@
</p>
<p>ˇ0
</p>
<p>ˇ1
ˇ2
ˇ3
</p>
<p>ˇ4
ˇ5
</p>
<p>1
CCCCCCCA
:
</p>
<p>Now the interactions are determined by using (8.13):
</p>
<p>.˛&#13;/11 D ˇ4
</p>
<p>.˛&#13;/12 D ˇ5
</p>
<p>.˛&#13;/13 D �f.˛&#13;/11 C .˛&#13;/12g D �.ˇ4 C ˇ5/
</p>
<p>.˛&#13;/21 D �.˛&#13;/11 D �ˇ4
</p>
<p>.˛&#13;/22 D �.˛&#13;/12 D �ˇ5
</p>
<p>.˛&#13;/23 D �.˛&#13;/13 D ˇ4 C ˇ5
</p>
<p>We have again a log-linear model as in (8.9) and the estimation of ˇ goes through
</p>
<p>the maximisation in ˇ of L.ˇ/ given by (8.10) under the same constraint.
</p>
<p>The model with all the interaction terms is called the saturated model. In
</p>
<p>this model there are no degrees of freedom, the number of free parameters to
</p>
<p>be estimated equals the number of free cells. The parameters of interest are the
</p>
<p>interactions. In particular, we are interested in testing their significance. These
</p>
<p>issues will be addressed below.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.2 Categorical Responses 267
</p>
<p>Three-Way Tables
</p>
<p>The models presented above for two-way tables can be extended to higher order
</p>
<p>tables but at a cost of notational complexity. We show how to adapt to three-
</p>
<p>way tables. This deserves special attention due to the presence of higher-order
</p>
<p>interactions in the saturated model.
</p>
<p>A .J �K �L/ three-way table may be constructed under multinomial sampling
as follows: each of the n observations falls in one, and only one, category of each
</p>
<p>of three categorical variables having J;K and L modalities respectively. We end
</p>
<p>up with a three-dimensional table with JKL cells containing the counts yjk` where
</p>
<p>n D
P
</p>
<p>j;k;` yjk`. The expected counts depend on the unknown probabilities pjk` in
</p>
<p>the usual way:
</p>
<p>mjk` D npjk`; j D 1; : : : ; J; k D 1; : : : ; K; ` D 1; : : : ; L:
</p>
<p>1. The saturated model
</p>
<p>A full saturated log-linear model reads as follows:
</p>
<p>logmjk` D �C ˛j C ˇk C &#13;` C .˛ˇ/jk C .˛&#13;/j ` C .ˇ&#13;/k` C .˛ˇ&#13;/jk`;
j D 1; : : : ; J; k D 1; : : : ; K; ` D 1; : : : ; L: (8.14)
</p>
<p>The restrictions are the following (using the &ldquo;dot&rdquo; notation for summation on the
</p>
<p>corresponding indices):
</p>
<p>˛.�/ D ˇ.�/ D &#13;.�/ D 0
.˛ˇ/j� D .˛&#13;/j� D .ˇ&#13;/k� D 0
.˛ˇ/�k D .˛&#13;/�` D .ˇ&#13;/�` D 0
.˛ˇ&#13;/jk� D .˛ˇ&#13;/j�` D .˛ˇ&#13;/�k` D 0
</p>
<p>The parameters .˛ˇ/jk; .˛&#13;/j `; .ˇ&#13;/k` are called first-order interactions. The
</p>
<p>second-order interactions are the parameters .˛ˇ&#13;/jk`, they allow to take into
</p>
<p>account heterogeneities in the interactions between two of the three variables.
</p>
<p>For instance, let ` stand for the two gender categories .L D 2/, if we suppose
that .˛ˇ&#13;/jk1 D �.˛ˇ&#13;/jk2 &curren; 0; we mean that the interactions between the
variable J and K are not the same for both gender categories.
</p>
<p>The estimation of the parameters of the saturated model are obtained through
</p>
<p>maximisation of the log-likelihood. In the multinomial sampling scheme, it
</p>
<p>corresponds to maximising the function:
</p>
<p>L D
X
</p>
<p>j;k;`
</p>
<p>yjk` logmjk`;
</p>
<p>under the constraint
P
</p>
<p>j;k;`mjk` D n.</p>
<p/>
</div>
<div class="page"><p/>
<p>268 8 Regression Models
</p>
<p>The number of degrees of freedom in the saturated model is again zero.
</p>
<p>Indeed, the number of free parameters in the model is
</p>
<p>1C .J � 1/C .K � 1/C .L� 1/C .J � 1/.K � 1/C .J � 1/.L� 1/
C.K � 1/.L� 1/C .J � 1/.K � 1/.L� 1/ � 1 D JKL � 1:
</p>
<p>This is indeed equal to the number of free cells in the table and so, there is no
</p>
<p>degree of freedom.
</p>
<p>2. Hierarchical non-saturated models
</p>
<p>As illustrated above, a saturated model has no degrees of freedom. Non-saturated
</p>
<p>models correspond to reduced models where some parameters are fixed to be
</p>
<p>equal to zero. They are thus particular cases of the saturated model (8.14). The
</p>
<p>hierarchical non-saturated models that we will consider here, are models where
</p>
<p>once a set of parameters is set equal to zero, all the parameters of higher-order
</p>
<p>containing the same indices are also set equal to zero.
</p>
<p>For instance if we suppose ˛1 D 0, we only consider non-saturated models
where also .˛&#13;/1` D .˛ˇ/1k D .˛ˇ&#13;/1k` D 0 for all values of k and `. If we
only suppose that .˛ˇ/12 D 0, we also assume that .˛ˇ&#13;/12` D 0 for all `.
</p>
<p>Hierarchical models have the advantage of being more easily interpretable.
</p>
<p>Indeed without this hierarchy, the models would be difficult to interpret. What
</p>
<p>would be, for instance, the meaning of the parameter .˛ˇ&#13;/12`, if we know that
</p>
<p>.˛ˇ/12 D 0? The estimation of the non-saturated models will be achieved by the
usual way i.e. by maximising the log-likelihood function L as above but under
</p>
<p>the new constraints of the reduced model.
</p>
<p>8.2.3 Testing Issues with Count Data
</p>
<p>One of the main practical interests in regression models for contingency tables is
</p>
<p>to test restrictions on the parameters of a more complete model. These testing ideas
</p>
<p>are created in the same spirit as in Sect. 3.5 where we tested restrictions in ANOVA
</p>
<p>models.
</p>
<p>In linear models, the test statistics is based on the comparison of the goodness
</p>
<p>of fit for the full model and for the reduced model. Goodness of fit is measured by
</p>
<p>the residual sum of squares (RSS). The idea here will be the same here but with a
</p>
<p>more appropriate measure for goodness of fit. Once a model has been estimated, we
</p>
<p>can compute the predicted value under that model for each cell of the table. We will
</p>
<p>denote, as above, the observed value in a cell by yk and Omk will denote the expected
value predicted by the model. The goodness of fit may be appreciated by measuring,
</p>
<p>in some way, the distance between the series of observed and of predicted values.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.2 Categorical Responses 269
</p>
<p>Two statistics are proposed: the Pearson chi-square X2 and the Deviance notedG2.
</p>
<p>They are defined as follows:
</p>
<p>X2 D
KX
</p>
<p>kD1
</p>
<p>.yk � Omk/2
Omk
</p>
<p>(8.15)
</p>
<p>G2 D 2
KX
</p>
<p>kD1
yk log
</p>
<p>�
yk
</p>
<p>Omk
</p>
<p>�
(8.16)
</p>
<p>where K is the total number of cells of the table. The deviance is directly related
</p>
<p>to the log-likelihood ratio statistic and is usually preferred because it can be used to
</p>
<p>compare nested models as we usually do in this context.
</p>
<p>Under the hypothesis that the model used to compute the predicted value is true,
</p>
<p>both statistics (for large samples) are approximately distributed as a �2 variable
</p>
<p>with degrees of freedom d:f: depending on the model. The d:f: can be computed
</p>
<p>as follows:
</p>
<p>d:f: D # free cells � # free parameters estimated: (8.17)
</p>
<p>For saturated models, the fit is perfect: X2 D G2 D 0 with d:f: D 0.
Suppose now that we want to test a reducedmodel which is a restricted version of
</p>
<p>a full model. The deviance can then be used as the F statistics in linear regression.
</p>
<p>The test procedure is straightforward:
</p>
<p>H0 W reduced model with r degrees of freedom
H1 W full model with f degrees of freedom: (8.18)
</p>
<p>Since, the full model contains more parameters, we expect the deviance to be
</p>
<p>smaller. We reject the H0 if this reduction is significant, i.e. if G
2
H0
� G2H1 is large
</p>
<p>enough. UnderH0 one has:
</p>
<p>G2H0 �G
2
H1
� �2r�f :
</p>
<p>We rejectH0 if the p-value:
</p>
<p>P
n
�2r�f &gt;
</p>
<p>�
G2H0 �G
</p>
<p>2
H1
</p>
<p>�o
:
</p>
<p>is small. Suppose we want to test the independence in a .J �K/ two-way table (no
interaction). Here the full model is the saturated one with no degrees of freedom
</p>
<p>.f D 0/ and the restricted model has r D .J � 1/ .K � 1/ degrees of freedom. We
rejectH0 if the p-value ofH0 Pf�2r &gt;
</p>
<p>�
G2H0
</p>
<p>�
g is too small.
</p>
<p>This test is equivalent to the Pearson chi-square test for independence in two-way
</p>
<p>tables (G2H0 � X
2
H0
</p>
<p>when n is large).</p>
<p/>
</div>
<div class="page"><p/>
<p>270 8 Regression Models
</p>
<p>Table 8.6 A three-way contingency table: top table for men and bottom table for women
MVAdrug
</p>
<p>M A1 A2 A3 A4 A5
</p>
<p>DY 21 32 70 43 19
</p>
<p>DN 683 596 705 295 99
</p>
<p>F A1 A2 A3 A4 A5
</p>
<p>DY 46 89 169 98 51
</p>
<p>DN 738 700 847 336 196
</p>
<p>Table 8.7 Coefficient estimates based on the saturated model MVAdrug
</p>
<p>Ǒ Ǒ
Ǒ
0 intercept 5.0089 Ǒ10 0.0205
Ǒ
1 gender: M �0.2867 Ǒ11 0.0482
Ǒ
2 drug: DY �1.0660 Ǒ12 drug*age �0.4983
Ǒ
3 age �0.0080 Ǒ13 �0.1807
Ǒ
4 0.2151 Ǒ14 0.0857
Ǒ
5 0.6607 Ǒ15 0.2766
Ǒ
6 �0.0463 Ǒ16 gender*drug*age �0.0134
Ǒ
7 gender*drug �0.1632 Ǒ17 �0.0523
Ǒ
8 gender*age 0.0713 Ǒ18 �0.0112
Ǒ
9 �0.0092 Ǒ19 �0.0102
</p>
<p>Example 8.4 Everitt and Dunn (1998) provide a three-dimensional .2�2�5/ count
table of n D 5;833 interviewed people. The count were on prescribed psychotropic
drugs in the fortnight prior to the interview as a function of age and gender. The
</p>
<p>data are summarised in Table 8.6, where the categories for the three factors are M
</p>
<p>for male, F for female, DY for &ldquo;yes&rdquo; having taken drugs, DN for &ldquo;no&rdquo; not having
</p>
<p>taking drugs and the five age categories: A1 (16&ndash;29), A2 (30&ndash;44), A3 (45&ndash;64), A4
</p>
<p>(65&ndash;74), A5 for over 74. The table provides the observed frequencies yjk` in each
</p>
<p>of the cells of the three-way table: where j stands for gender, k for drug and ` for
</p>
<p>age categories. The design matrix X for the full saturated model can be found in the
</p>
<p>quantlet MVAdrug.
</p>
<p>The saturated model gives the estimates displayed in Table 8.7.
</p>
<p>We see for instance that Ǒ1 &lt; 0, so there are fewer men than women in the study,
since Ǒ7 is also negative it seems that the tendency of men taking the drug is less
important than for women. Also, note that Ǒ12 to Ǒ15 forms an increasing sequence,
so that the age factor seems to increase the tendency to take the drug. Note that in this
</p>
<p>saturated model, there are no degrees of freedom and the fit is perfect, Omjk` D yjk`
for all the cells of the table.
</p>
<p>The second order interactions have a lower order of magnitude, so we want to
</p>
<p>test if they are significantly different to zero. We consider a restricted model where</p>
<p/>
</div>
<div class="page"><p/>
<p>8.2 Categorical Responses 271
</p>
<p>Table 8.8 Coefficients estimates based on the maximum likelihood method MVAdrug-
3waysTab
</p>
<p>Ǒ Ǒ
Ǒ
0 intercept 5.0051 Ǒ8 gender*age 0.0795
Ǒ
1 gender: M �0.2919 Ǒ9 0.0321
Ǒ
2 drug: DY �1.0717 Ǒ10 0.0265
Ǒ
3 age �0.0030 Ǒ11 0.0534
Ǒ
4 0.2358 Ǒ12 drug*age �0.4915
Ǒ
5 0.6649 Ǒ13 �0.1576
Ǒ
6 �0.0425 Ǒ14 0.0917
Ǒ
7 gender*drug �0.1734 Ǒ15 0.2822
</p>
<p>.˛ˇ&#13;/jk` are all set to zero. This can be achieved by testing H0 W ˇ16 D ˇ17 D
ˇ18 D ˇ19 D 0. The maximum likelihood estimators of the restricted model are
obtained by deleting the last four columns in the design matrix X . The results are
</p>
<p>given in Table 8.8.
</p>
<p>We have J D 2,K D 2 and L D 5, this makes JKL� 1 D 19 free cells. The full
model has f D 0 degrees of freedom and the reduced model has r D 4 degrees of
freedom. The G2 deviance is given by 2.3004; it has 4 degrees of freedom (the chi-
</p>
<p>square statistics is 2:3745). The p-value of the restricted model is 0.6807, so we do
</p>
<p>not reject the null hypothesis (the restricted model without 2nd order interaction). In
</p>
<p>others words, age does not interfere with the interactions between gender and drugs,
</p>
<p>or equivalently, gender does not interfere in the interactions between age and drugs.
</p>
<p>The reader can verify that the first order interactions are significant, by taking, for
</p>
<p>instance, the model without interactions of the second order as the new full model
</p>
<p>and testing a reduced model where all the first order interactions are all set to zero.
</p>
<p>MVAdrug3waysTab
</p>
<p>8.2.4 Logit Models
</p>
<p>Logit models are useful to analyse how explanatory variables influence a binary
</p>
<p>response y. The response y may take the two values 1 and 0 to denote the presence
</p>
<p>or absence of a certain qualitative trait (a person can be employed or unemployed,
</p>
<p>a firm can be bankrupt or not, a patient can be affected by a certain disease or not,
</p>
<p>etc.). Logit models are designed to estimate the probability of y D 1 as a logistic
function of linear combinations of x. Logit models can be adapted to the analysis of
</p>
<p>contingency tables where one of the qualitative variables is binary. One obtains the
</p>
<p>probability of being in one of the two states of this binary variable as a function of
</p>
<p>the other variables. We concentrate here on .2 �K/ and .2 �K � L/ tables.</p>
<p/>
</div>
<div class="page"><p/>
<p>272 8 Regression Models
</p>
<p>Logit Models for Binary Response
</p>
<p>Consider the vector y .n� 1/ of observations on a binary response variable (a value
of &ldquo;1&rdquo; indicating the presence of a particular qualitative trait and a value of &ldquo;0&rdquo;, its
</p>
<p>absence). The logit model makes the assumption that the probability for observing
</p>
<p>yi D 1 given a particular value of xi D .xi1; : : : ; xip/&gt; is given by the logistic
function of a &ldquo;score&rdquo;, a linear combination of x:
</p>
<p>p .xi / D P.yi D 1 j xi/ D
exp.ˇ0 C
</p>
<p>Pp
jD1 ˇjxij/
</p>
<p>1C exp.ˇ0 C
Pp
</p>
<p>jD1 ˇjxij/
: (8.19)
</p>
<p>This entails the probability of the absence of the trait:
</p>
<p>1 � p .xi / D P.yi D 0 j xi/ D
1
</p>
<p>1C exp.ˇ0 C
Pp
</p>
<p>jD1 ˇjxij/
;
</p>
<p>which implies
</p>
<p>log
</p>
<p>�
p .xi /
</p>
<p>1 � p .xi /
</p>
<p>�
D ˇ0 C
</p>
<p>pX
</p>
<p>jD1
ˇjxij: (8.20)
</p>
<p>This indicates that the logit model is equivalent to a log-linear model for the odds
</p>
<p>ratio p .xi /=f1� p .xi /g. A positive value of ˇj indicates an explanatory variable
xj that will favour the presence of the trait since it improves the odds. A zero value
</p>
<p>of ˇj corresponds to the absence of an effect of this variable on the appearance of
</p>
<p>the qualitative trait.
</p>
<p>For i.i.d observations the likelihood function is:
</p>
<p>L.ˇ0; ˇ/ D
nY
</p>
<p>iD1
p .xi /
</p>
<p>yi f1� p .xi /g1�yi :
</p>
<p>The maximum likelihood estimators of the ˇ&rsquo;s are obtained as the solution of the
</p>
<p>non-linear maximisation problem . Ǒ0; Ǒ/ D argmaxˇ0;ˇ logL.ˇ0; ˇ/ where
</p>
<p>logL.ˇ0; ˇ/ D
nX
</p>
<p>iD1
Œyi logp .xi /C .1 � yi / logf1� p .xi /g&#141; :
</p>
<p>The asymptotic theory of the MLE of Chap. 6 (see Theorem 6.3) applies and thus
</p>
<p>asymptotic inference on ˇ is available (test of hypothesis or confidence intervals).
</p>
<p>Example 8.5 In the bankruptcy data set (see Sect. 22.22), we have measures on 5
</p>
<p>financial characteristics on 66 banks, 33 among them being bankrupt and the other
</p>
<p>33 still being solvent. The logit model can be used to evaluate the probability of</p>
<p/>
</div>
<div class="page"><p/>
<p>8.2 Categorical Responses 273
</p>
<p>Table 8.9 Probabilities of
the bankruptcies with the
logit model
MVAbankrupt
</p>
<p>Ǒ p-Values
ˇ0 3.6042 0.0660
</p>
<p>ˇ3 �0.2031 0.0037
ˇ4 �0.0205 0.0183
</p>
<p>Table 8.10 A .2�K/
contingency table
</p>
<p>1 � � � k � � � K Total
1 y11 � � � y1k � � � y1K y1
2 y21 � � � y2k � � � y2K y2
Total y�1 � � � y�k � � � y�K y� D n
</p>
<p>bankruptcy as a function of these financial ratios. We obtain the results summarised
</p>
<p>in Table 8.9. We observe that only ˇ3 and ˇ4 are significant.
</p>
<p>Logit Models for Contingency Tables
</p>
<p>The logit model may contain quantitative and qualitative explanatory variables. In
</p>
<p>the latter case, the variable may be coded according to the rules described in the
</p>
<p>ANOVA/ANCOVA sections above. This enables a revisit to the contingency tables
</p>
<p>where one of the variables is binary and is the variable of interest. How can the
</p>
<p>probability of taking one of the two nominal values be evaluated as a function of
</p>
<p>the other variables? We keep the notations of Sect. 8.1 and suppose, without loss of
</p>
<p>generality, that the first variable with J D 2 is the binary variable of interest. In the
drug Example 8.4, we have a .2�2�5/ table and one is interested in the probability
of taking a drug as a function of age and gender.
</p>
<p>.2 �K/ Tables with Binomial Sampling
</p>
<p>In Table 8.10 we have displayed the situation. Let pk be the probability of falling
</p>
<p>into the first row for the k-th column, k D 1; : : : ; K . Since we are mainly interested
in the probabilities pk as a function of k, we suppose here that y�k are fixed for k D
1; : : : ; K (or we work conditionally on the observed value of these column totals),
</p>
<p>where y�k D
PJ
</p>
<p>jD1 yjk. Therefore, we haveK independent binomial processes with
parameters .y�k ; pk/. Since the column variable is nominal we can use an ANOVA
model to analyse the effect of the column variable on pk through the logs of the
</p>
<p>odds
</p>
<p>log
</p>
<p>�
pk
</p>
<p>1 � pk
</p>
<p>�
D �0 C �k ; k D 1; : : : ; K; (8.21)</p>
<p/>
</div>
<div class="page"><p/>
<p>274 8 Regression Models
</p>
<p>where
PK
</p>
<p>kD1 �k D 0. As in the ANOVA models, one of the interests will be to test
H0 W �1 D � � � D �K D 0. The log-linear model for the odds has its equivalent in a
logit formulation for pk
</p>
<p>pk D
exp.�0 C �k/
</p>
<p>1C exp.�0 C �k/
; k D 1; : : : ; K: (8.22)
</p>
<p>Note that we can code the RHS of (8.21) as a linear model X� , where for instance,
</p>
<p>for a .2 � 4/ table (K D 4) we have:
</p>
<p>X D
</p>
<p>0
BB@
</p>
<p>1 1 0 0
</p>
<p>1 0 1 0
</p>
<p>1 0 0 1
</p>
<p>1 �1 �1 �1
</p>
<p>1
CCA ; � D
</p>
<p>0
BB@
</p>
<p>ˇ0
ˇ1
</p>
<p>ˇ2
ˇ3
</p>
<p>1
CCA ;
</p>
<p>where �0 D ˇ0; �1 D ˇ1; �2 D ˇ2; �3 D ˇ3 and �4 D �.ˇ1 C ˇ2 C ˇ3/. The logit
model for pk ; k D 1; : : : ; K can now be written, with some abuse of notation, as
the K-vector
</p>
<p>p D exp.X�/
1C exp.X�/ ;
</p>
<p>where the division has to be understood as being element-by-element. The MLE of
</p>
<p>� is obtained by maximising the log-likelihood
</p>
<p>L.�/ D
KX
</p>
<p>kD1
fy1k logpk C y2k log.1 � pk/g; (8.23)
</p>
<p>where the pk are elements of theK-vector p.
</p>
<p>This logit model is a saturated model. Indeed the number of free parameters is
</p>
<p>K , the dimension of � , and the number of free cells is also equal to K since we
</p>
<p>consider the column totals y�k as being fixed. So, there are no degrees of freedom
in this model. It can be proven that this logit model is equivalent to the saturated
</p>
<p>model for a table .2 � K/ presented in Sect. 8.2.2 where all the interactions are
present in the model. The hypothesis of all interactions .˛&#13;/jk being equal to zero
</p>
<p>(independence case) is equivalent to the hypothesis that the �k ; k D 1; : : : ; K are
all equal to zero (no column effect on the probabilities pk).
</p>
<p>The main interest of the logit presentation is its flexibility when the variable
</p>
<p>defining the column categories is a quantitative variable (age group, number of
</p>
<p>children, etc.). Indeed, when this is the case, the logit model allows to quantify
</p>
<p>the effect of the column category by using less parameters and a more flexible
</p>
<p>relationship than a linear relation. Suppose that we could attach a representative
</p>
<p>value xk to each column category for this class (for instance, it could be the median</p>
<p/>
</div>
<div class="page"><p/>
<p>8.2 Categorical Responses 275
</p>
<p>value, or the average value of the class category). We can then choose the following
</p>
<p>logit model for pk
</p>
<p>pk D
exp.�0 C �1xk/
</p>
<p>1C exp.�0 C �1xk/
; k D 1; : : : ; K; (8.24)
</p>
<p>where we now have only two free parameters for K free cells, so we have K � 2
degrees of freedom. We could even introduce a quadratic term to allow some
</p>
<p>curvature effect of x on the odds
</p>
<p>pk D
exp.�0 C �1xk C �2x2k/
</p>
<p>1C exp.�0 C �1xk C �2x2k/
; k D 1; : : : ; K:
</p>
<p>In this latter case, we would still haveK � 3 degrees of freedom.
We can follow the same idea for a three-way table when we want to model
</p>
<p>the behaviour of the first binary variable as a function of the two other variables
</p>
<p>defining the table. In the drug example, one is interested in analysing the tendency
</p>
<p>of taking a psychotropic drug as a function of the gender category and of the age.
</p>
<p>Fix the number of observations in each cell k` (i.e. y�k`), so that we have a binomial
sampling process with an unknown parameter pk` for each cell. As for the two-way
</p>
<p>case above, we can either use ANOVA-like models for the logarithm of the odds and
</p>
<p>ANCOVA-like models when one (or both) of the two qualitative variables defining
</p>
<p>the K and/or L categories is a quantitative variable.
</p>
<p>One may study the following ANOVA model for the logarithms of the odds
</p>
<p>log
</p>
<p>�
pk`
</p>
<p>1 � pk`
</p>
<p>�
D �C �k C �`; k D 1; : : : ; K; ` D 1; : : : ; L;
</p>
<p>with � D � D 0. As another example, if x` is a representative value (like the average
age of the group) of the `th level of the third categorical variable, one might think of:
</p>
<p>log
</p>
<p>�
pk`
</p>
<p>1 � pk`
</p>
<p>�
D �C �k C �x`; k D 1; : : : ; K; ` D 1; : : : ; L; (8.25)
</p>
<p>with the constraint � D 0. Here also, interactions and the curvature effect for x`
can be introduced, as shown in the following example. Since the cell totals y�k` are
considered as fixed, the log-likelihood to be maximised is:
</p>
<p>KX
</p>
<p>kD1
</p>
<p>LX
</p>
<p>`D1
fy1k` logpk` C y2k` log.1 � pk`/g; (8.26)
</p>
<p>where pk` follows the appropriate logistic model.</p>
<p/>
</div>
<div class="page"><p/>
<p>276 8 Regression Models
</p>
<p>Example 8.6 Consider again Example 8.4. One is interested in the influence of
</p>
<p>gender and age on drug prescription. Take the number of observations for each
</p>
<p>&ldquo;gender-age group&rdquo; combination, y�k` as fixed. A logit model (8.25) can be used for
the odds-ratios of the probability of taking drugs, where the value x` is the average
</p>
<p>age of the group. In the linear form it may be written as one of the two following
</p>
<p>equivalent forms:
</p>
<p>log
</p>
<p>�
p
</p>
<p>1 � p
</p>
<p>�
D X�;
</p>
<p>p D exp.X�/
1C exp.X�/ ;
</p>
<p>where � D .ˇ0; ˇ1; ˇ2/&gt; and the design matrix X is given by
</p>
<p>X D
</p>
<p>0
BBBBBBBBBBBBBBB@
</p>
<p>1:0 1:0 23:2
</p>
<p>1:0 1:0 36:5
</p>
<p>1:0 1:0 54:3
</p>
<p>1:0 1:0 69:2
</p>
<p>1:0 1:0 79:5
</p>
<p>1:0 �1:0 23:2
1:0 �1:0 36:5
1:0 �1:0 54:3
1:0 �1:0 69:2
1:0 �1:0 79:5
</p>
<p>1
CCCCCCCCCCCCCCCA
</p>
<p>The first column of X is for the intercept, the second is the coded variable for
</p>
<p>the two gender categories and the last column is the average of the ages for the
</p>
<p>corresponding age-group. Then we estimate ˇ by maximising the log-likelihood
</p>
<p>function (8.26). We obtain:
</p>
<p>Ǒ
0 D �3:5612
Ǒ
1 D �0:3426
Ǒ
2 D 0:0280;
</p>
<p>the intercept for men is Ǒ0C Ǒ1 D �3:9038 and for women is Ǒ0 � Ǒ1 D �3:2186,
indicating a gender effect and the common slope for the positive age effect being
Ǒ
2 D 0:0280. The fit appears to be reasonably good. There are K � L D 2 �
5 D 10 free cells in the table. A saturated &ldquo;full&rdquo; model with ten parameters and
a zero degree of freedom would involve a constant (one parameter) plus an effect
</p>
<p>for gender (one parameter) plus an effect for age (four parameters) and finally the</p>
<p/>
</div>
<div class="page"><p/>
<p>8.2 Categorical Responses 277
</p>
<p>20 30 40 50 60 70 80
&minus;3.5
</p>
<p>&minus;3
</p>
<p>&minus;2.5
</p>
<p>&minus;2
</p>
<p>&minus;1.5
</p>
<p>&minus;1
</p>
<p>&minus;0.5
</p>
<p>Age category
</p>
<p>lo
g
</p>
<p> o
f 
o
</p>
<p>d
d
</p>
<p>s
&minus;
</p>
<p>ra
ti
o
</p>
<p>s
</p>
<p>Fig. 8.3 Fit of the log of the odds-ratios for taking drugs: linear model for age effect with a &ldquo;gen-
der&rdquo; effect (no interaction). Men are the stars and women are the circles MVAdruglogistic
</p>
<p>interactions between gender and age (four parameters). The model retained above
</p>
<p>is a &ldquo;reduced model&rdquo; with only three parameters that can be tested against the most
</p>
<p>general saturated model. We obtain the value of the deviance G2H0 D 11:5584 with
7 degrees of freedom .7 D 10 � 3/, whereas, G2H1 D 0 with no degree of freedom.
This gives a p-valueD 0.1160, so we cannot reject the reduced model.
</p>
<p>Figure 8.3 shows how well the model fits the data. It displays the fitted values of
</p>
<p>the log of the odds-ratios by the linear model for the men and the women along with
</p>
<p>the log of the odds-ratios computed from the observed corresponding frequencies.
</p>
<p>It seems that the age effect shows a curvature. So we fit a model introducing the
</p>
<p>square of the ages. This gives the following design matrix:
</p>
<p>X D
</p>
<p>0
BBBBBBBBBBBBBBB@
</p>
<p>1:0 1:0 23:2 538:24
</p>
<p>1:0 1:0 36:5 1332:25
</p>
<p>1:0 1:0 54:3 2948:49
</p>
<p>1:0 1:0 69:2 4788:64
</p>
<p>1:0 1:0 79:5 6320:25
</p>
<p>1:0 �1:0 23:2 538:24
1:0 �1:0 36:5 1332:25
1:0 �1:0 54:3 2948:49
1:0 �1:0 69:2 4788:64
1:0 �1:0 79:5 6320:25
</p>
<p>1
CCCCCCCCCCCCCCCA</p>
<p/>
</div>
<div class="page"><p/>
<p>278 8 Regression Models
</p>
<p>The maximum likelihood estimators are:
</p>
<p>Ǒ
0 D �4:4996
Ǒ
1 D �0:3457
Ǒ
2 D 0:0697
Ǒ
3 D �0:0004:
</p>
<p>MVAdruglogistic
</p>
<p>The fit is better for this more flexible alternative, giving a deviance G2H1 D
3:3251 with 6 degrees of freedom (6 D 10 � 4). If we test H0: no curvature for
the age effect againstH1: curvature for the age effect, the reduction of the deviance
</p>
<p>is G2H0 � G
2
H1
D 11:5584 � 3:3251 D 8:2333 with one degree of freedom. The
</p>
<p>p-value D 0.0041, so we reject the reduced model (no curvature) in favour of the
more general model with a curvature term.
</p>
<p>We know already from Example 8.4 that second order interactions are not
</p>
<p>significant for this data set (the influence of age on taking a drug is the same for
</p>
<p>both gender categories), so we can keep this model as a final reasonable model to
</p>
<p>analyse the probability of taking the drug as a function of the gender and of the
</p>
<p>age. To summarise this analysis we end up saying that the probability of taking a
</p>
<p>psychotropic drug can be modelled as (with some abuse of notation)
</p>
<p>log
</p>
<p>�
p
</p>
<p>1 � p
</p>
<p>�
D ˇ0 C ˇ1 � SexC ˇ2 � AgeC ˇ3 � Age2: (8.27)
</p>
<p>Summary
</p>
<p>,! In contingency tables, the categories are defined by the qualitative
variables.
</p>
<p>,! The saturated model has all of the interaction terms, and 0 degree
of freedom.
</p>
<p>,! The non-saturated model is a reduced model since it fixes some
parameters to be zero.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.3 Exercises 279
</p>
<p>Summary (continued)
</p>
<p>,! Two statistics to test for the full model and the reduced model are:
</p>
<p>X2 D
KX
</p>
<p>kD1
.yk � Omk/2= Omk
</p>
<p>G2 D 2
KX
</p>
<p>kD1
yk log .yk= Omk/
</p>
<p>,! The logit models allow the column categories to be a quantitative
variable, and quantify the effect of the column category by using
</p>
<p>fewer parameters and incorporating more flexible relationships
</p>
<p>than just a linear one.
</p>
<p>,! The logit model is equivalent to a log-linear model.
</p>
<p>log Œp .xi /=f1� p .xi /g&#141; D ˇ0 C
pX
</p>
<p>jD1
ˇjxij
</p>
<p>8.3 Exercises
</p>
<p>Exercise 8.1 For the one factor ANOVA model, show that if the model is &ldquo;bal-
</p>
<p>anced&rdquo; .n1 D n2 D n3/, we have O� D Ny. If the model is not balanced, show that
Ny D O�C n1 Ǫ1 C n2 Ǫ2 C n3 Ǫ3:
Exercise 8.2 Redo the calculations of Example 8.2 and test if the main effects of
</p>
<p>the marketing strategy and of the location are significant.
</p>
<p>Exercise 8.3 Redo the calculations of Example 8.3 with the Car Data set.
</p>
<p>Exercise 8.4 Calculate the prediction interval for &ldquo;classic blue&rdquo; pullover sales
</p>
<p>(Example 3.2) corresponding to price D 120.
Exercise 8.5 Redo the calculations of the Boston housing example in Sect. 8.1.3
</p>
<p>Exercise 8.6 We want to analyse the variations in the consumption of packs of
</p>
<p>cigarettes per month as a function of the brand (A or B), of the price per pack
</p>
<p>and as a function of the gender of the smoker (M or F). The data are below.</p>
<p/>
</div>
<div class="page"><p/>
<p>280 8 Regression Models
</p>
<p>y Price Gender Brand
</p>
<p>30 3.5 M A
</p>
<p>4 4 F B
</p>
<p>20 4.1 F B
</p>
<p>15 3.75 M A
</p>
<p>24 3.25 F A
</p>
<p>11 5 F B
</p>
<p>8 4.1 F B
</p>
<p>9 3.5 M A
</p>
<p>17 4.5 M B
</p>
<p>1 4 F B
</p>
<p>23 3.65 M A
</p>
<p>13 3.5 M A
</p>
<p>1. In addition to the effects of brand, price and gender, test if there is an interaction
</p>
<p>between the brand and the price.
</p>
<p>2. How would the design matrix of a full model with all the interactions between
</p>
<p>the variables appear? What would be the number of degrees of freedom of such
</p>
<p>a model?
</p>
<p>3. We would like to introduce a curvature term for the price variable. How can we
</p>
<p>proceed? Test if this coefficient is significant.
</p>
<p>Exercise 8.7 In the drug Example 8.4, test if the first order interactions are
</p>
<p>significant.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 9
</p>
<p>Variable Selection
</p>
<p>Variable selection is very important in statistical modelling. We are frequently
</p>
<p>not only interested in using a model for prediction but also need to correctly
</p>
<p>identify the relevant variables, that is, to recover the correct model under given
</p>
<p>assumptions. It is known that under certain conditions, the ordinary least squares
</p>
<p>(OLS) method produces poor prediction results and does not yield a parsimonious
</p>
<p>model causing overfitting. Therefore the objective of the variable selection methods
</p>
<p>is to find the variables which are the most relevant for prediction. Such methods are
</p>
<p>particularly important when the true underlying model has a sparse representation
</p>
<p>(many parameters close to zero). The identification of relevant variables will reduce
</p>
<p>the noise and therefore improve the prediction performance of the fitted model.
</p>
<p>Some popular regularisation methods used are the ridge regression, subset
</p>
<p>selection, L1 norm penalisation and their modifications and combinations. Ridge
</p>
<p>regression, for instance, which minimises a penalised residual sum of squares using
</p>
<p>the squared L2 norm penalty, is employed to improve the OLS estimate through
</p>
<p>a bias-variance trade-off. However, ridge regression has a drawback that it cannot
</p>
<p>yield a parsimonious model since it keeps all predictors in the model and therefore
</p>
<p>creates an interpretability problem. It also gives prediction errors close to those from
</p>
<p>the OLS model.
</p>
<p>Another approach proposed for variable selection is the so-called &ldquo;least absolute
</p>
<p>shrinkage and selection operator&rdquo; (Lasso), aims at combining the features of ridge
</p>
<p>regression and subset selection either retaining (and shrinking) the coefficients or
</p>
<p>setting them to zero. This method received several extensions such as the Elastic
</p>
<p>net, a combination of Lasso and ridge regression or the Group Lasso used when
</p>
<p>predictors are divided into groups. This chapter describes the application of Lasso,
</p>
<p>Group Lasso as well as the Elastic net in linear regression model with continuous
</p>
<p>and binary response (logit model) variables.
</p>
<p>&copy; Springer-Verlag Berlin Heidelberg 2015
W.K. H&auml;rdle, L. Simar, Applied Multivariate Statistical Analysis,
DOI 10.1007/978-3-662-45171-7_9
</p>
<p>281</p>
<p/>
</div>
<div class="page"><p/>
<p>282 9 Variable Selection
</p>
<p>9.1 Lasso
</p>
<p>Tibshirani (1996) first introduced Lasso for generalised linear models, where the
</p>
<p>response variable y is continuous rather than categorical. Lasso has two important
</p>
<p>characteristics. First, it has an L1-penalty term which performs shrinkage on
</p>
<p>coefficients in a way similar to ridge regression, where an L2 penalty is used.
</p>
<p>Second, unlike ridge regression, Lasso performs variable subset selection driving
</p>
<p>some coefficients to exactly zero due to the nature of the constraint, where the
</p>
<p>objective function may touch the quadratic constraint area at a corner. For this
</p>
<p>reason, the Lasso is able to produce sparse solutions and is therefore able to combine
</p>
<p>good features of both ridge regression and subset selection procedure. It yields
</p>
<p>interpretable models and has the stability of ridge regression.
</p>
<p>9.1.1 Lasso in the Linear Regression Model
</p>
<p>The linear regression model can be written as follows:
</p>
<p>y D Xˇ C ";
</p>
<p>where y is an .n � 1/ vector of observations for the response variable, X D
.x&gt;1 ; : : : ; x
</p>
<p>&gt;
n /
&gt;, xi 2 Rp , i D 1; : : : ; n is a data matrix of p explanatory variables,
</p>
<p>and " D ."1; : : : ; "n/&gt; is a vector of errors where E."i/ D 0 and Var."i/ D �2,
i D 1; : : : ; n.
</p>
<p>In this framework,E .yjX / D Xˇ with ˇ D
�
ˇ1; : : : ; ˇp
</p>
<p>�&gt;
. Further assume that
</p>
<p>the columns of X are standardised such that n�1
Pn
</p>
<p>iD1 xij D 0 and n�1
Pn
</p>
<p>iD1 x
2
ij D
</p>
<p>1. The Lasso estimate Ǒ can then be defined as follows
</p>
<p>Ǒ D argmin
ˇ
</p>
<p>(
nX
</p>
<p>iD1
</p>
<p>�
yi � x&gt;i ˇ
</p>
<p>�2
)
; subject to
</p>
<p>pX
</p>
<p>jD1
jˇj j � s; (9.1)
</p>
<p>where s � 0 is the tuning parameter which controls the amount of shrinkage. For
the OLS estimate Ǒ0 D .X&gt;X /�1X&gt;y a choice of tuning parameter s &lt; s0, where
s0 D
</p>
<p>Pp
jD1 j Ǒ0j j, will cause shrinkage of the solutions towards 0, and ultimately
</p>
<p>some coefficients may be exactly equal to 0. For values s � s0 the Lasso coefficients
are equal to the unpenalised OLS coefficients.
</p>
<p>An alternative representation of (9.1) is:
</p>
<p>Ǒ D argmin
ˇ
</p>
<p>8
&lt;
:
</p>
<p>nX
</p>
<p>iD1
</p>
<p>�
yi � x&gt;i ˇ
</p>
<p>�2 C �
pX
</p>
<p>jD1
jˇj j
</p>
<p>9
=
; ; (9.2)</p>
<p/>
</div>
<div class="page"><p/>
<p>9.1 Lasso 283
</p>
<p>with a tuning parameter � � 0. As � increases, the Lasso estimates are continuously
shrunk toward zero. Then if � is quite large, some coefficients are exactly zero. For
</p>
<p>� D 0 the Lasso coefficients coincide with the OLS estimate. In fact, if the solution
to (9.1) is denoted as Ǒs and the solution to (9.2) as Ǒ�, then 8� &gt; 0 and the
resulting solution Ǒ� 9s� such that Ǒ� D Ǒs� and vice versa which implies a one-to-
one correspondence between these parameters. However, this does not hold if it is
</p>
<p>required that � � 0 only and not � &gt; 0, because if, for instance, � D 0, then Ǒ� is
the same for any s � k Ǒk1 and the correspondence is no longer one-to-one.
</p>
<p>Geometrical Aspects in R2
</p>
<p>The Lasso estimate under the least squares loss function solves a quadratic program-
</p>
<p>ming problem with linear inequality constraints. The criterion
Pn
</p>
<p>iD1
�
yi � x&gt;i ˇ
</p>
<p>�2
yields the quadratic form objective function
</p>
<p>.ˇ � Ǒ0/&gt;W.ˇ � Ǒ0/ (9.3)
</p>
<p>with W D X&gt;X . For the special case when p D 2, ˇ D .ˇ1; ˇ2/&gt;, the
resulting elliptical contour lines are centred around the OLS estimate and the linear
</p>
<p>constraints are represented by square (shaded area) shown in Fig. 9.1. The Lasso
</p>
<p>solution is the first place that the contours touch the square, and this sometimes
</p>
<p>occurs at a corner, corresponding to a zero coefficient. The nature of the Lasso
</p>
<p>shrinkage may not occur completely obvious. In the work by Efron, Hastie,
</p>
<p>Johnstone, and Tibshirani (2004) the Least Angle Regression (LAR) algorithm
</p>
<p>Fig. 9.1 Lasso in the general
design case for s D 4 and
OLS estimate Ǒ0 D .6; 7/&gt;
MVAlassocontour
</p>
<p>0 5 10
</p>
<p>0
5
</p>
<p>1
0
</p>
<p>β1
</p>
<p>β
2
</p>
<p>&minus;4 &minus;3 &minus;2 &minus;1 1 2 3 4 6 7 8 9 11 12
</p>
<p>&minus;
4
</p>
<p>&minus;
3
</p>
<p>&minus;
2
</p>
<p>&minus;
1
</p>
<p>1
2
</p>
<p>3
4
</p>
<p>6
7
</p>
<p>8
9
</p>
<p>1
1
</p>
<p>1
2
</p>
<p>●
</p>
<p>βOLS</p>
<p/>
</div>
<div class="page"><p/>
<p>284 9 Variable Selection
</p>
<p>with a Lasso modification was described which computes the whole path of Lasso
</p>
<p>solutions and gives a better understanding of the shrinkage nature.
</p>
<p>The LAR Algorithm and Lasso Solution Paths
</p>
<p>The LAR algorithm may be introduced in the simple three-dimensional case as
</p>
<p>follows (assume that the number of covariates p D 3):
&bull; first, standardise all the covariates to have mean 0 and unit length as well as make
</p>
<p>the response variable have mean zero;
</p>
<p>&bull; start with Ǒ D 0;
&bull; initialise the algorithm with the first two covariates: let X D .x1; x2/ and
</p>
<p>calculate the prediction vector Oy0 D X Ǒ D 0;
&bull; calculate y2 the projection of y onto L.x1; x2/, the linear space spanned by x1
</p>
<p>and x2;
</p>
<p>&bull; compute the vector of current correlations between the covariates X and the
</p>
<p>two-dimensional current residual vector: C Oy0 D X&gt;.y2 � Oy0/ D .c
Oy0
1 ; c
</p>
<p>Oy0
2 /
&gt;.
</p>
<p>According to Fig. 9.2, the current residual y2 � Oy0 makes a smaller angle with
x1, than with x2, therefore c
</p>
<p>Oy0
1 &gt; c
</p>
<p>Oy0
2 ;
</p>
<p>&bull; augment Oy0 in the direction of x1 so that Oy1 D Oy0 C O&#13;1x1 with O&#13;1 chosen such
that c
</p>
<p>Oy0
1 D c
</p>
<p>Oy0
2 which means that the new current residual y2 � Oy1 makes equal
</p>
<p>angles (is equiangular) with x1 and x2;
</p>
<p>&bull; suppose that another regressor x3 enters the model: calculate a new projection y3
of y onto L.x1; x2; x3/;
</p>
<p>&bull; recompute the current correlations vector C Oy1 D .c Oy11 ; c
Oy1
2 ; c
</p>
<p>Oy1
3 /
&gt; with X D
</p>
<p>.x1; x2; x3/, y3 and Oy1;
&bull; augment Oy1 in the equiangular direction so that Oy2 D Oy1 C O&#13;2u2 with O&#13;2
</p>
<p>chosen such that c
Oy1
1 D c
</p>
<p>Oy1
2 D c
</p>
<p>Oy1
3 , then the new current residual y3 � Oy2 goes
</p>
<p>Fig. 9.2 Illustration of LARS algorithm</p>
<p/>
</div>
<div class="page"><p/>
<p>9.1 Lasso 285
</p>
<p>equiangularly between x1, x2 and x3 (here u2 is the unit vector lying along the
</p>
<p>equiangular direction Oy2);
&bull; the three-dimensional algorithm is terminated with the calculation of the final
</p>
<p>prediction vector Oy3 D Oy2 C O&#13;3u3 with O&#13;3 chosen such that Oy3 D y3 .
In the case of p &gt; 3 covariates, Oy3 would be smaller than y3 initiating another
</p>
<p>change of direction, as illustrated in Fig. 9.2.
</p>
<p>In this setup, it is important that the covariate vectors x1, x2, x3 are linearly
</p>
<p>independent. The LAR algorithm &ldquo;moves&rdquo; the variable coefficients to their least
</p>
<p>squares values. So the Lasso adjustment necessary for the sparse solution is that
</p>
<p>if a nonzero coefficient happens to return to zero, it should be dropped from the
</p>
<p>current (&ldquo;active&rdquo;) set of variables and not be considered in further computations.
</p>
<p>The general LAR algorithm for p predictors can be summarised as follows.
</p>
<p>Least Angle Regression Algorithm
</p>
<p>1. The covariates are standardised to have mean 0 and unit length 1 and the
</p>
<p>response has mean 0:
</p>
<p>nX
</p>
<p>iD1
yi D 0;
</p>
<p>nX
</p>
<p>iD1
xij D 0;
</p>
<p>nX
</p>
<p>iD1
x2ij D 1I j D 1; 2; : : : ; p:
</p>
<p>The task is to construct the fit Ǒ D . Ǒ1; : : : ; Ǒp/&gt; by iteratively changing
the prediction vector Oy D
</p>
<p>Pp
jD1 xj
</p>
<p>Ǒ
j D X Ǒ.
</p>
<p>2. Denote A equal to a subset of the indices f1; 2; : : : ; pg, begin with OyA D
Oyo D 0 and calculate the vector of current correlations
</p>
<p>Oc D X&gt;.y � OyA/:
</p>
<p>3. Then review the current set A D fj W j Ocj j D OC g as the set of
indices corresponding to the covariates with the greatest absolute current
</p>
<p>correlations, where OC D max
j
fj Ocj jg; let sj D signf Ocj g for j 2 A and
</p>
<p>compute the matrix XA D .sjxj /j2A, the scalar AA D .1&gt;AG�1A 1A/�
1
2 with
</p>
<p>GA D X&gt;AXA and 1&gt;A being a vector of ones of length jAj, and the so-called
equiangular vector uA D XAwA with wA D AAG�1A 1A which makes equal
angles, each less than 90ı, with the columns of XA.
</p>
<p>4. Calculate the inner product vector a
defD X&gt;uA and the direction
</p>
<p>O&#13; D minC
j2Ac
</p>
<p>(
OC � Ocj
AA � aj
</p>
<p>;
OC C Ocj
AA C aj
</p>
<p>)</p>
<p/>
</div>
<div class="page"><p/>
<p>286 9 Variable Selection
</p>
<p>5. Define Od to be the m-vector equaling sjwAj for j 2 A and zero elsewhere
and &#13;j D � Ǒj = Odj yielding Q&#13; D min
</p>
<p>&#13;j&gt;0
</p>
<p>˚
&#13;j
�
</p>
<p>(a) If Q&#13; &lt; O&#13; , calculate the next LARS step as
</p>
<p>OyAC D OyA C Q&#13;uA
</p>
<p>with AC D A � f Qj g.
(b) Else: calculate the next step as
</p>
<p>OyAC D OyA C O&#13;uA
</p>
<p>6. Iterate until all p predictors have been entered, some of which are ultimately
</p>
<p>dropped from the active set A.
</p>
<p>This algorithm can be implemented on a grid from 0 to 1 of the standardised
</p>
<p>coefficients constraint s resulting in the complete paths of the Lasso coefficients
</p>
<p>and illustrating the nature of Lasso shrinkage.
</p>
<p>Once the Lasso solution paths have been obtained, it is important to decide on a
</p>
<p>rule how to choose the &ldquo;optimal&rdquo; solution, or, equally, the regularisation parameter
</p>
<p>�. There are several existing methods to do this and the most popular examples
</p>
<p>are theK-fold cross-validation, generalised cross-validation, Schwartz&rsquo;s (Bayesian)
</p>
<p>Information Criterion (BIC). All these methods can be viewed as degrees-of-
</p>
<p>freedom adjustments to the residual squared error (RSE) which underestimates the
</p>
<p>true prediction error
</p>
<p>RSE
defD
</p>
<p>nX
</p>
<p>iD1
.yi � Oyi /2:
</p>
<p>Consider the generalised cross-validation statistic:
</p>
<p>GCV.�/ D n�1RSE�= f1 � df.�/=ng2 ; (9.4)
</p>
<p>where RSE� is the residual sum of squares for the constrained fit with a particular
</p>
<p>regularisation parameter �. An alternative is the BIC
</p>
<p>BIC D n log. O�2/C log.n/ � df.�/ (9.5)
</p>
<p>with the estimation of error variance O�2 D n�1
Pn
</p>
<p>iD1.yi � Oyi /2.</p>
<p/>
</div>
<div class="page"><p/>
<p>9.1 Lasso 287
</p>
<p>The degrees of freedom of the predicted vector Oy in the Lasso problem with the
linear Gaussian model with normally distributed errors having zero expectation and
</p>
<p>variance �2, written "i � N.0; �2/, can be defined as follows:
</p>
<p>df.�/
defD ��2
</p>
<p>nX
</p>
<p>iD1
Cov. Oyi ; yi /; (9.6)
</p>
<p>which can actually be used for both linear and non-linear models. This expression
</p>
<p>for df.�/ can be viewed as a quantitative measure of the prediction error bias
</p>
<p>dependence on how much each yi affects its fitted value Oyi . The estimate Ǒ
minimising the GCV statistic can then be chosen. The following example shows
</p>
<p>how to compute df.�/.
</p>
<p>Example 9.1 (Calculation of df.�/) As no closed-form solution exists for the Lasso
</p>
<p>problem, an approximation should be calculated. The constraint
P
jˇj j � s can
</p>
<p>be rewritten as
P
ˇ2j =jˇj j � s. Using the duality between the constrained and
</p>
<p>unconstrained problems and one-to-one correspondence between s and �, the Lasso
</p>
<p>solution is computed as the ridge regression estimate
</p>
<p>Ǒ D .X&gt;X C �B�1/�1X&gt;y;
</p>
<p>where B D diag.j Ǒj j/. Then it follows that
</p>
<p>Oy D X Ǒ;
</p>
<p>D X .X&gt;X C �B�1/�1X&gt;y:
</p>
<p>Then, to calculate Cov. Oyi ; yi /, one could use Cov. Oyi ; yi / D Cov.e&gt;i Oy; e&gt;i y/ D
e&gt;i Cov. Oy; y/ei , where ei is a vector where the i 0th entry is 1 and the rest are zero.
Furthermore, each entry in the sum of (9.6) can be calculated to be
</p>
<p>Cov. Oyi ; yi / D e&gt;i Cov. Oy; y/ei (9.7)
</p>
<p>D e&gt;i X .X&gt;X C �B�1/�1X&gt; Cov.y; y/ei (9.8)
</p>
<p>D �2.X&gt;ei /&gt;.X&gt;X C �B�1/�1.X&gt;ei/ (9.9)
</p>
<p>D �2x&gt;i .X&gt;X C �B�1/�1xi : (9.10)
</p>
<p>Using the fact that (9.10) are scalars for all i as well as the properties of the trace of
</p>
<p>a matrix and matrix multiplication rules mentioned in Chap. 2, one obtains the final
</p>
<p>closed-form expression for the effective degrees of freedom in the Lasso problem:
</p>
<p>df.�/ D 1
�2
</p>
<p>nX
</p>
<p>iD1
tr
˚
�2x&gt;i .X
</p>
<p>&gt;X C �B�1/�1xi
�</p>
<p/>
</div>
<div class="page"><p/>
<p>288 9 Variable Selection
</p>
<p>D
nX
</p>
<p>iD1
tr
˚
xix
&gt;
i .X
</p>
<p>&gt;X C �B�1/�1
�
</p>
<p>D tr
( 
</p>
<p>nX
</p>
<p>iD1
xix
&gt;
i
</p>
<p>!
.X&gt;X C �B�1/�1
</p>
<p>)
</p>
<p>D tr
˚
X&gt;X .X&gt;X C �B�1/�1
</p>
<p>�
</p>
<p>D tr
˚
X .X&gt;X C �B�1/�1X&gt;
</p>
<p>�
:
</p>
<p>It should be noted that the formula for the effective degrees of freedom derived
</p>
<p>above is valid in the case of the underlying model with non-random regressors.
</p>
<p>When the random design is used and the set of nonzero predictors is not fixed,
</p>
<p>another estimator should be used.
</p>
<p>Orthonormal Design Case
</p>
<p>A computationally convenient special case is the so-called orthonormal design
</p>
<p>framework. In the orthonormal design case X&gt;X is a diagonal matrix that X&gt;X D
I. Here the explicit Lasso estimate is
</p>
<p>Ǒ
j D sign
</p>
<p>�
Ǒ0
j
</p>
<p>� �
j Ǒ0j j � &#13;
</p>
<p>�C
; (9.11)
</p>
<p>&#13; D �
2
</p>
<p>subject to
</p>
<p>pX
</p>
<p>jD1
j Ǒj j D s: (9.12)
</p>
<p>The formula shows what was already mentioned in the beginning, namely that the
</p>
<p>Lasso estimate is a compromise between subset selection and ridge regression, the
</p>
<p>estimate is either shrunk by &#13; or is set to zero. As a consequence Lasso coefficients
</p>
<p>can take values between zero and Ǒ0j .
</p>
<p>Example 9.2 (Orthonormal Design Case for p D 2) Let Ǒ D
�
b̌
1; b̌2
</p>
<p>�&gt;
</p>
<p>w.l.o.g. be in the first quadrant, i.e. b̌1 � 0 and b̌2 � 0. This gives us
the first condition. The orthonormal design ensures that the elliptical contour
</p>
<p>lines describe circles around the OLS estimate. Thus we get a linear function
</p>
<p>going through the point Ǒ0 and being orthogonal (if possible) to the first
condition. Equalising both conditions
</p>
<p>Ǒ
1 C Ǒ2 D s (9.13)
Ǒ
2 D Ǒ1 C
</p>
<p>�
Ǒ0
2 � Ǒ01
</p>
<p>�
(9.14)</p>
<p/>
</div>
<div class="page"><p/>
<p>9.1 Lasso 289
</p>
<p>the Lasso estimate can now be accurately determined:
</p>
<p>Ǒ
1 D
</p>
<p> 
s
</p>
<p>2
C
Ǒ0
1 � Ǒ02
2
</p>
<p>!C
(9.15)
</p>
<p>Ǒ
2 D
</p>
<p> 
s
</p>
<p>2
�
Ǒ0
1 � Ǒ02
2
</p>
<p>!C
: (9.16)
</p>
<p>For cases in which
</p>
<p>�
s
2
C Ǒ
</p>
<p>0
1� Ǒ02
2
</p>
<p>�
� 0 or
</p>
<p>�
s
2
� Ǒ
</p>
<p>0
1� Ǒ02
2
</p>
<p>�
� 0 the corresponding Lasso
</p>
<p>estimates will always be zero as the position of the Ǒ01 and corresponding contour
lines do not make it possible to get the orthogonality condition mentioned above.
</p>
<p>Let Ǒ0 D .6; 7/&gt; and tuning parameter s D 4. In this case the Lasso estimator is
given by, as shown in Fig. 9.3:
</p>
<p>Ǒ
1 D
</p>
<p>4
</p>
<p>2
C 6 � 7
</p>
<p>2
D 1:5; (9.17)
</p>
<p>Ǒ
2 D
</p>
<p>4
</p>
<p>2
� 6 � 7
</p>
<p>2
D 2:5: (9.18)
</p>
<p>0 5 10
</p>
<p>0
5
</p>
<p>1
0
</p>
<p>β1
</p>
<p>β
2
</p>
<p>&minus;4 &minus;3 &minus;2 &minus;1 1 2 3 4 6 7 8 9 11 12 13
</p>
<p>&minus;
4
</p>
<p>&minus;
3
</p>
<p>&minus;
2
</p>
<p>&minus;
1
</p>
<p>1
2
</p>
<p>3
4
</p>
<p>6
7
</p>
<p>8
9
</p>
<p>1
1
</p>
<p>1
2
</p>
<p>1
3
</p>
<p>●
</p>
<p>βOLS
</p>
<p>Fig. 9.3 Lasso in the orthonormal design case for s D 4 and OLS estimate Ǒ0 D .6; 7/&gt;
MVAlassocontour</p>
<p/>
</div>
<div class="page"><p/>
<p>290 9 Variable Selection
</p>
<p>In terms of �, the Lasso solution (9.11) in the orthonormal design case can be
</p>
<p>calculated in a usual unconstrained minimisation problem. Note that in this case the
</p>
<p>least squares solution is given by
</p>
<p>Ǒ0 D .X&gt;X /�1X&gt;y D X&gt;y:
</p>
<p>Then the minimisation problem is written as
</p>
<p>Ǒ D arg min
ˇ2Rp
ky � Xˇk22 C �kˇk1
</p>
<p>D arg min
ˇ2Rp
</p>
<p>.y � Xˇ/&gt;.y � Xˇ/C �
pX
</p>
<p>jD1
jˇj j
</p>
<p>D arg min
ˇ2Rp
� 2y&gt;Xˇ C ˇ&gt;ˇ C �
</p>
<p>pX
</p>
<p>jD1
jˇj j
</p>
<p>D arg min
ˇ2Rp
� 2 Ǒ0&gt;ˇ C ˇ&gt;ˇ C �
</p>
<p>pX
</p>
<p>jD1
jˇj j
</p>
<p>D arg min
ˇ2Rp
</p>
<p>pX
</p>
<p>jD1
</p>
<p>�
�2 Ǒ0jˇj C ˇ2j C �jˇj j
</p>
<p>�
:
</p>
<p>The objective function can now be minimised by separate minimisation of its j th
</p>
<p>element. To solve
</p>
<p>min
ˇ
.�2 Ǒ0ˇ C ˇ2 � �jˇj/; (9.19)
</p>
<p>where the index j was dropped for simplicity, let&rsquo;s first assume that Ǒ0 &gt; 0,
then ˇ � 0, because a lower value for the objective function may be obtained by
changing the sign. Then the solution for the modified problem
</p>
<p>min
ˇ
.�2 Ǒ0ˇ C ˇ2 C �ˇ/ (9.20)
</p>
<p>is, obviously, Ǒ D Ǒ0 � &#13; , where &#13; D �=2, as in (9.11). To ensure the sign
consistency for this case, one could see that the solution is
</p>
<p>Ǒ D . Ǒ0 � &#13;/C D sign. Ǒ0/.j Ǒ0j � &#13;/C: (9.21)
</p>
<p>Now let us take Ǒ0 � 0, then ˇ � 0 as well and the solution for the new problem
</p>
<p>min
ˇ
.�2 Ǒ0ˇ C ˇ2 � �ˇ/ (9.22)</p>
<p/>
</div>
<div class="page"><p/>
<p>9.1 Lasso 291
</p>
<p>is Ǒ D Ǒ0 C &#13; , but the sign consistency requires that
</p>
<p>Ǒ D . Ǒ0 C &#13;/�
</p>
<p>D �.� Ǒ0 � &#13;/C
</p>
<p>D sign. Ǒ0/.j Ǒ0j � &#13;/C:
</p>
<p>As the solutions are the same in both cases, the expression sign. Ǒ0/.j Ǒ0j � &#13;/C is
indeed the solution to the original Lasso problem.
</p>
<p>General Lasso Solution
</p>
<p>For a fixed s � 0 the Lasso estimation problem is a least squares problem
subjected to 2p linear inequality constraints as there are 2p different possible signs
</p>
<p>for ˇ D
�
ˇ1; : : : ; ˇp
</p>
<p>�&gt;
. Lawson and Hansen (1974) suggested solving the least
</p>
<p>squares problem subject to a general linear inequality constraint Gˇ � h where
G.m � p/ corresponds to the m D 2p constraints and h D s1m. As m could be
very large, this procedure is not very fast computationally. Therefore Lawson and
</p>
<p>Hansen (1974) introduced the inequality constraints sequentially in their algorithm,
</p>
<p>seeking a feasible solution.
</p>
<p>Let g.ˇ/ D
Pn
</p>
<p>iD1
�
yi � x&gt;i ˇ
</p>
<p>�2
and let ık ; k D 1; : : : ; 2p , be column vectors of
</p>
<p>p-tuples of the form .˙1; : : : ;˙1/. It follows that the linear inequality condition
can be equivalently described as ı&gt;k ˇ � s; k D 1; : : : ; 2p. Now let E D fkjı&gt;k ˇ D
sg the equality set, mE the number of elements of E and GE D
</p>
<p>�
ı&gt;k
�
k2E a matrix
</p>
<p>whose rows are all ık&rsquo;s for k 2 E . Now the algorithm works as follows, see
Tibshirani (1996):
</p>
<p>1. Find OLS estimate Ǒ0 and let ık0 D sign. Ǒ0/, E D fk0g.
2. Find Ǒ to minimise g.ˇ/ subject to GEˇ � s1mE .
3. If
</p>
<p>Pp
jD1 j Ǒj j � s the computation is complete.
</p>
<p>4. If
Pp
</p>
<p>jD1 j Ǒj j &gt; s add k to the set E where ık D sign. Ǒ/ and go back to step 2.
5. The final iteration is a solution to the original problem.
</p>
<p>As the number of steps is limited by m D 2p , the algorithm has to converge
in finite time. The average number of iterations in practice is between 0:5p and
</p>
<p>0:75p.
</p>
<p>Example 9.3 Let us consider the car data set (Table 22.3) where n D 74. We
want to study in-what way the price .X1/ depends on the 12 other variables
</p>
<p>.X2/; : : : ; .X13/, which are represented by j D 1; 2; : : : ; 12, using Lasso regres-
sion. In Fig. 9.4 one can clearly see that coefficients become nonzero one at a
</p>
<p>time, that means the variables enter the regression equation sequentially as the
</p>
<p>scaled shrinkage parameter Os D s=k Ǒ0k1 increases, in order j D 6; 11; 9; 3; : : :</p>
<p/>
</div>
<div class="page"><p/>
<p>292 9 Variable Selection
</p>
<p>* * * * * ** * * *
** **
</p>
<p>0.0 0.2 0.4 0.6 0.8 1.0
</p>
<p>&minus;
2
</p>
<p>0
0
</p>
<p>0
0
</p>
<p>0
1
</p>
<p>0
0
</p>
<p>0
0
</p>
<p>3
0
</p>
<p>0
0
</p>
<p>0
</p>
<p>Scaled Parameter
</p>
<p>S
ta
</p>
<p>n
d
</p>
<p>a
rd
</p>
<p>iz
e
</p>
<p>d
 C
</p>
<p>o
e
</p>
<p>ff
ic
</p>
<p>ie
n
</p>
<p>ts
</p>
<p>* * * * * ** * * *
** **
</p>
<p>* * * * * ** * * * ** **
</p>
<p>* * * * * ** * *
*
</p>
<p>** **
* * * * * ** * * * ** ***
</p>
<p>*
*
</p>
<p>* *
</p>
<p>**
* *
</p>
<p>*
</p>
<p>**
**
</p>
<p>* * * * * ** * *
</p>
<p>*
</p>
<p>** **
</p>
<p>* * * * *
</p>
<p>** * * *
** **
</p>
<p>* * *
</p>
<p>* * ** * * * ** **
* * * * * ** * * * ** *** *
</p>
<p>*
</p>
<p>* * ** * * * ** **
</p>
<p>7
8
</p>
<p>3
4
</p>
<p>1
1
</p>
<p>6
</p>
<p>0 1 2 3 5 8 9 10 12
</p>
<p>Fig. 9.4 Lasso estimates of standardised regression Ǒj for car data with n D 74 and p D 12
MVAlassoregress
</p>
<p>(representing X7; X12; X10; X4; : : :), hence the L1 penalty results in variable selec-
</p>
<p>tion and the variables which are most relevant are shrunk less. In this example,
</p>
<p>an optimal Os can be found such that the fitted model gives the smallest residual
(see Exercise 9.3).
</p>
<p>9.1.2 Lasso in High Dimensions
</p>
<p>The problemwith the algorithm by Tibshirani to calculate the Lasso solutions is that
</p>
<p>it is initialised from an OLS solution of the unconstrained problem which does not
</p>
<p>correspond to the true model. Another problem is that for the case of p &gt; n, this
</p>
<p>computation is infeasible. Therefore it may be optimal to start with a small initial
</p>
<p>guess for ˇ and iterate through a different kind of an algorithm to obtain the Lasso
</p>
<p>solutions. Such an algorithm is based on the properties of the Lasso problem as
</p>
<p>a convex programming one. Osborne et al. (2000) showed that the original Lasso
</p>
<p>estimate problem (9.1) can be rewritten as:
</p>
<p>Ǒ D arg min
ˇ2Rp
</p>
<p>1
</p>
<p>2
.y � Xˇ/&gt; .y � Xˇ/ defD 1
</p>
<p>2
r&gt;r; subject to s � kˇk1 � 0;
</p>
<p>(9.23)
</p>
<p>where r
defD .y � Xˇ/. Let J D fi1; : : : ; ipg be the set of indices such that
</p>
<p>j.X&gt;r/ij j D kX&gt;rk1, for j D 1; : : : ; p; so indices in J correspond to nonzero
elements of ˇ. Also let P be the permutation matrix that permutes the elements
</p>
<p>of the coefficient vector ˇ so that the first elements are the nonzero elements:</p>
<p/>
</div>
<div class="page"><p/>
<p>9.1 Lasso 293
</p>
<p>ˇ D P&gt; .ˇJ ; 0/&gt;. Denote �J D sign.ˇJ / be equal to 1 if the correspond-
ing element of ˇJ is positive and �1 otherwise. Further denoting f .ˇ/ D
.y � Xˇ/&gt; .y � Xˇ/ the following optimisation algorithm is based on the local
linearisation of (9.1) around ˇ:
</p>
<p>Ǒ D arg min
h
f .ˇ C h/; subject to �&gt;J .ˇJ C hJ /� s and hDP&gt; .hJ ; 0/&gt; ;
</p>
<p>(9.24)
</p>
<p>the solution for which can be shown to be equal to
</p>
<p>hJ D .X&gt;J XJ /�1fX&gt;J .y � XJˇJ /� ��J g;
</p>
<p>where
</p>
<p>� D max
(
0;
�&gt;J .X
</p>
<p>&gt;
J XJ /
</p>
<p>�1X&gt;J y � s
�&gt;J .X
</p>
<p>&gt;
J XJ /
</p>
<p>�1�J
</p>
<p>)
:
</p>
<p>The procedure as a whole is implemented as shown in the &ldquo;Lasso solution-path
</p>
<p>optimisation&rdquo; algorithm. As shown in the algorithm, indices may enter and leave the
</p>
<p>set J , which makes the Lasso problem similar to other subset selection techniques.
</p>
<p>Moreover, one can compute the whole path of Lasso solutions for 0 � s � s0, each
time taking the solution for the previous s as a starting point for the next one.
</p>
<p>9.1.3 Lasso in Logit Model
</p>
<p>The Lasso model can be extended to generalised linear models, one of the most
</p>
<p>common of which is the logistic regression (logit) model. Coefficients in the logit
</p>
<p>model have probabilistic interpretation. In the logit model, the linear predictor Xˇ
</p>
<p>is related to the conditional mean � of the response variable y via the logit link
</p>
<p>logf�=.1 � �/g. As the response variable is binary, it is binomial-distributed and
� D p.xi /. Therefore, as defined in (9.25), the logit model for y 2 f0; 1g of .n� 1/
observations on a binary response variable and xi D .xi1; : : : ; xip/&gt; is,
</p>
<p>log
</p>
<p>�
p .xi /
</p>
<p>1 � p .xi /
</p>
<p>�
D
</p>
<p>pX
</p>
<p>jD1
ˇjxij;
</p>
<p>where
</p>
<p>p .xi / D P.yi D 1 j xi/ D
exp.
</p>
<p>Pp
jD1 ˇjxij/
</p>
<p>1C exp.
Pp
</p>
<p>jD1 ˇjxij/
: (9.25)</p>
<p/>
</div>
<div class="page"><p/>
<p>294 9 Variable Selection
</p>
<p>Algorithm Lasso solution-path optimisation
</p>
<p>1: procedure FIND(optimal ˇ)
2: Choose initial ˇ and J (e.g. ˇ 0, J  ;)
3: repeat
</p>
<p>4: Solve (9.23) to obtain h
</p>
<p>5: Set Ǒ  ˇC h
6: if sign. ǑJ / D �J then
7: Obtain the solution ˇ D Ǒ
8: else
9: repeat
10: Find the smallest &#13; , 0 &lt; &#13; &lt; 1, k 2 J such that 0D ˇk C &#13;hk
11: Set ˇ D ˇC &#13;h
12: Set �k D ��k
13: Solve (9.23) again to obtain a new h
14: if �&gt;J .ˇJ C hJ / � s then
15: Ǒ D ˇC h
16: else
17: Update J  J�k
18: Recompute ˇJ , �J , h
19: end if
20: until sign. ǑJ / D �J
21: end if
22: Compute Ov X&gt; Or=kX&gt;J Ork1 D P&gt; . Ov1; Ov2/
</p>
<p>&gt; F here Or D y � X Ǒ
23: if �1 � . Ov2/{ � 1 for 1 � { � p � jJ j then
24: Ǒ is a solution
25: else
26: Find | such that j. Ov2/| j is maximised
27: Update J  .J ; | /
28: Update ǑJ  . ǑJ ; 0/&gt;
29: Update �J  .�J ; sign. Ov2/| /&gt;
30: end if
31: Set ˇ Ǒ
32: until �1 � . Ov2/{ � 1 for 1 � { � p � jJ j
33: end procedure
</p>
<p>The Lasso estimate for the logit model is obtained by solving the following
</p>
<p>optimisation problem:
</p>
<p>Ǒ D argmin
ˇ
</p>
<p>(
nX
</p>
<p>iD1
g
�
�yix&gt;i ˇ
</p>
<p>�
)
; subject to
</p>
<p>pX
</p>
<p>jD1
jˇj j � s; (9.26)
</p>
<p>with tuning parameter s � 0 and log-loss function g.u/ D log f1C exp.u/g. An
alternative representation of the Lasso estimate Ǒ in the logit model is:
</p>
<p>argmin
ˇ
</p>
<p>8
&lt;
:
</p>
<p>nX
</p>
<p>iD1
g
�
�yix&gt;i ˇ
</p>
<p>�
C �
</p>
<p>pX
</p>
<p>jD1
jˇj j
</p>
<p>9
=
; : (9.27)</p>
<p/>
</div>
<div class="page"><p/>
<p>9.1 Lasso 295
</p>
<p>Shevade and Keerthi (2003) developed a simple and efficient algorithm to solve
</p>
<p>the optimisation in (9.27) based on the Gauss&ndash;Seidel method using coordinate-
</p>
<p>wise descent approach. The algorithm is asymptotically convergent and easy to
</p>
<p>implement. Firstly, define the following terms,
</p>
<p>ui D �yix&gt;i ˇ;
</p>
<p>Fj D
nX
</p>
<p>iD1
</p>
<p>exp.ui/
</p>
<p>exp.1C ui/
yixij: (9.28)
</p>
<p>The first order optimality conditions for (9.27) are:
</p>
<p>Fj D 0 if j D 0;
Fj D � if ˇj &gt; 0; j &gt; 0;
Fj D �� if ˇj &lt; 0; j &gt; 0;
</p>
<p>�� � Fj � � if ˇj D 0; j &gt; 0:
</p>
<p>A new variable is defined
</p>
<p>vj D jFj j if j D 0;
D j� � Fj j if ˇj &gt; 0; j &gt; 0;
D j�C Fj j if ˇj &lt; 0; j &gt; 0;
D  j if ˇj D 0; j &gt; 0:
</p>
<p>where j D maxf.Fj��/; .���Fj /; 0g. Thus, the first-order optimality conditions
can be written as
</p>
<p>vj D 0 8j: (9.29)
</p>
<p>It is difficult to obtain exact optimality condition, so the stopping criterion for (9.27)
</p>
<p>is defined as follows (for some small "),
</p>
<p>vj � " 8j: (9.30)
</p>
<p>To write the algorithm, let us define Iz D fj W ˇj D 0; j &gt; 0g and Inz D
fj W ˇj &curren; 0; j &gt; 0g for sets of zero estimates and sets of nonzero estimates,
respectively, and I D Iz [ Inz. The algorithm consists of two loops. The first loop
runs over the variables in Iz to choose the maximum violator, v. In the second loop
</p>
<p>W is optimised with respect to ˇv , therefore the set Inz is modified and maximum
</p>
<p>violator in Inz is obtained. The second loop is repeated until no violators are found
</p>
<p>in Inz. The algorithm alternates between the first and second loop until no violators
</p>
<p>exist in both Iz and Inz.</p>
<p/>
</div>
<div class="page"><p/>
<p>296 9 Variable Selection
</p>
<p>Algorithm Lasso in logit model
</p>
<p>1: procedure FIND(optimal Lasso estimate Ǒ)
2: Set ˇj D 0 for all j
3: while an optimality violator exists in Iz do
4: Find the maximum violator (v) in Iz
5: repeat
6: Optimise W with respect to ˇv
7: Find the maximum violator (v) in Inz
8: until no violator exists in Inz
9: end while
10: end procedure
</p>
<p>Another way to obtain the lasso estimate in the logit model is by maximising the
</p>
<p>likelihood function of logit model with lasso constraint. The log-likelihood function
</p>
<p>of logit model is written as
</p>
<p>logL.ˇ/ D
nX
</p>
<p>iD1
Œyi logp .xi /C .1 � yi / logf1 � p .xi /g&#141; : (9.31)
</p>
<p>Suppose `.ˇ/ D logL.ˇ/, with ˇ D .ˇ1; : : : ; ˇp/&gt;, the Lasso estimates are
obtained by maximising the penalised log likelihood for logit model as follows
</p>
<p>Ǒ D argmax
ˇ
</p>
<p>(
n�1
</p>
<p>nX
</p>
<p>iD1
`.ˇ/
</p>
<p>)
; subject to
</p>
<p>pX
</p>
<p>jD1
jˇj j � s: (9.32)
</p>
<p>It can solved by a general non-linear programming procedure or by using iteratively
</p>
<p>reweighted least squares (IRLS). Friedman, Hastie, and Tibshirani (2010) developed
</p>
<p>an algorithm to solve the problem in (9.32). An alternative representation of the
</p>
<p>Lasso problem is defined as follows:
</p>
<p>Ǒ D argmax
ˇ
</p>
<p>8
&lt;
:n
�1
</p>
<p>nX
</p>
<p>iD1
`.ˇ/ � �
</p>
<p>pX
</p>
<p>jD1
jˇj j
</p>
<p>9
=
; : (9.33)
</p>
<p>Example 9.4 Following Example 9.3, the price .X1/ of car data set (Table 22.3) has
</p>
<p>average 6;192:28. We now define a new categorical variable which takes the value
</p>
<p>0 if X1 � 6,000 and otherwise is equal to 1. We want to study in what way the price
.X1/ depends on the 12 other variables .X2; : : : ; X13/ using Lasso in logit model.
</p>
<p>In Fig. 9.5 one can see that coefficients&rsquo; dynamics depends on the shrinkage
</p>
<p>parameter s D k Ǒ.�/k1, the L1 norm of estimated coefficients. An optimal s can be
chosen such that the fitted model gives the smallest residual (see Exercise 9.4).</p>
<p/>
</div>
<div class="page"><p/>
<p>9.2 Elastic Net 297
</p>
<p>Fig. 9.5 Lasso estimates Ǒj of logit model for car data with n D 74 and p D 12
MVAlassologit
</p>
<p>9.2 Elastic Net
</p>
<p>Although the Lasso is widely used in variable selection, it has several drawbacks.
</p>
<p>Zou and Hastie (2005) stated that:
</p>
<p>1. if p &gt; n, the Lasso selects at most n variables before it saturates;
</p>
<p>2. if there is a group of variables which has very high correlation, then the Lasso
</p>
<p>tends to select only one variable from this group;
</p>
<p>3. for usual n &gt; p condition, if there are high correlations between predictors,
</p>
<p>the prediction performance of the Lasso is dominated by ridge regression, see
</p>
<p>Tibshirani (1996).
</p>
<p>Zou and Hastie (2005) introduced the Elastic net which combines good features
</p>
<p>of the L1-norm and L2-norm penalties. The Elastic net is a regularised regression
</p>
<p>method which overcomes the limitations of the Lasso. This method is very useful
</p>
<p>when p � n or there are many correlated variables. The advantages are: (1) a group
of correlated variables can be selected without arbitrary omissions, (2) the number
</p>
<p>of selected variables is no longer limited by the sample size.</p>
<p/>
</div>
<div class="page"><p/>
<p>298 9 Variable Selection
</p>
<p>9.2.1 Elastic Net in Linear Regression Model
</p>
<p>We describe the Elastic net in linear regression model. For simplicity reason we
</p>
<p>assume that the xij are standardised such that
Pn
</p>
<p>iD1 xij D 0 and n�1
Pn
</p>
<p>iD1 x
2
ij D 1.
</p>
<p>The Elastic net penalty P˛.ˇ/ leads to the following modification of the problem to
</p>
<p>obtain the estimator Ǒ
</p>
<p>argmin
ˇ
</p>
<p>(
.2n/�1
</p>
<p>nX
</p>
<p>iD1
</p>
<p>�
yi � x&gt;i ˇ
</p>
<p>�2 C �P˛.ˇ/
)
; (9.34)
</p>
<p>where
</p>
<p>P˛.ˇ/ D
1
</p>
<p>2
.1 � ˛/ kˇk22 C ˛ kˇk1
</p>
<p>D
pX
</p>
<p>jD1
</p>
<p>�
1
</p>
<p>2
.1 � ˛/ˇ2j C ˛jˇj j
</p>
<p>�
: (9.35)
</p>
<p>The penalty P˛.ˇ/ is a compromise between ridge regression and the Lasso. If
</p>
<p>˛ D 0 then the criterion is the ridge regression and if ˛ D 1 the method will be the
Lasso. Practically, for small " &gt; 0, the Elastic net with ˛ D 1 � " performs like the
Lasso, but removes degeneracies and erratic variable selection behaviour caused by
</p>
<p>extreme correlation. Given a specific �, as ˛ increases from 0 to 1, the sparsity of
</p>
<p>the Elastic net solutions increases monotonically from 0 to the sparsity of the Lasso
</p>
<p>solutions.
</p>
<p>The Elastic net optimisation problem can be represented as the usual Lasso
</p>
<p>problem, using modified X and y vectors, as shown in the following example.
</p>
<p>Example 9.5 To turn the Elastic net optimisation problem into the usual Lasso
</p>
<p>one, one should first augment y with p additional zeros to obtain Qy D .y; 0/&gt;.
Then, augment X with the multiple of the p � p identity matrix
</p>
<p>p
�˛I to get
</p>
<p>QX D
�
X&gt;;
p
�˛I
</p>
<p>�&gt;
. Next, define Q� D �.1 � ˛/ and solve the original Lasso
</p>
<p>minimisation problem in terms of the new input Qy, QX and Q�. This new problem is
equivalent to the original Elastic net problem:
</p>
<p>k Qy � QXˇk22 C Q�kˇk1 D
&#13;&#13;&#13;&#13;
�
y
</p>
<p>0
</p>
<p>�
�
�
</p>
<p>Xˇp
�˛Iˇ
</p>
<p>�&#13;&#13;&#13;&#13;
2
</p>
<p>2
</p>
<p>C �.1 � ˛/kˇk1;
</p>
<p>D ky � Xˇk22 � �˛kˇk22 C �kˇk1 � �˛kˇk1;
</p>
<p>D ky � Xˇk22 C �
˚
˛kˇk22 C .1� ˛/kˇk1
</p>
<p>�
;
</p>
<p>which is equivalent to the original Elastic net problem.
</p>
<p>We follow the idea of Friedman et al. (2010) who used a coordinate descent
</p>
<p>algorithm to solve the optimisation problem in (9.34). Let us suppose to have</p>
<p/>
</div>
<div class="page"><p/>
<p>9.2 Elastic Net 299
</p>
<p>estimates Q̌k for k &curren; j . Then we optimise (9.34) partially with respect to ˇj by
computing the gradient at ˇj D Q̌j , which only exists if Q̌j &curren; 0. Having the soft-
thresholding operator S.z; &#13;/ as
</p>
<p>sign.z/ .jzj � &#13;/C D
</p>
<p>8
ˆ̂&lt;
ˆ̂:
</p>
<p>z� &#13; if z &gt; 0 and &#13; &lt; jzj;
zC &#13; if z &lt; 0 and &#13; &lt; jzj;
0 if &#13; � jzj:
</p>
<p>(9.36)
</p>
<p>it can be shown that the coordinate-wise update has the following form
</p>
<p>f̌
j D
</p>
<p>S
n
n�1
</p>
<p>Pn
iD1 xij
</p>
<p>�
yi � Qy.j /i
</p>
<p>�
; �˛
</p>
<p>o
</p>
<p>1C �.1 � ˛/ ; (9.37)
</p>
<p>where Qy.j /i D
P
</p>
<p>k&curren;j xik Q̌k is a fitted value which excludes the contribution xij,
therefore yi � Qy.j /i is partial residual for fitting ˇj .
</p>
<p>The algorithm computes the least square estimate for the partial residual yi �
Qy.j /i , then applies the soft-thresholding rule to perform the Lasso contribution to
the penalty P˛.ˇ/. Afterwards, a proportional shrinkage is applied to ridge penalty.
</p>
<p>There are several methods used to update the current estimate Q̌. We describe the
simplest updating method, the so-called naive update.
</p>
<p>The partial residual can be rewritten as follows:
</p>
<p>yi � Qy.j /i D yi � Oyi C xijf̌j
D ri C xijf̌j ; (9.38)
</p>
<p>with byi being the current fit and ri the current residual. As xj is standardised,
therefore
</p>
<p>1
</p>
<p>n
</p>
<p>nX
</p>
<p>iD1
xij
</p>
<p>�
yi � Qy.j /i
</p>
<p>�
D 1
n
</p>
<p>nX
</p>
<p>iD1
xijri C f̌j : (9.39)
</p>
<p>Note that the first term on the right-hand side of the new partial residual is the
</p>
<p>gradient of the loss with respect to ˇj .
</p>
<p>9.2.2 Elastic Net in Logit Model
</p>
<p>The Elastic net penalty can similarly be applied to the logit model. Recall the log-
</p>
<p>likelihood function of the logit model in (9.31),
</p>
<p>logL.ˇ/ D
nX
</p>
<p>iD1
Œyi logp .xi /C .1 � yi / logf1 � p .xi /g&#141; :</p>
<p/>
</div>
<div class="page"><p/>
<p>300 9 Variable Selection
</p>
<p>Penalised log-likelihood for the logit model using Elastic net has the following form
</p>
<p>max
ˇ
</p>
<p>(
n�1
</p>
<p>nX
</p>
<p>iD1
`.ˇ/ � �P˛.ˇ/
</p>
<p>)
; (9.40)
</p>
<p>with `.ˇ/ D logL.ˇ/. The solution of (9.40) can be found by means of a
Newton algorithm. For a fixed � and given a current parameter Q̌, the quadratic
approximation (Taylor expansion) is updated about current estimates Q̌ as follows:
</p>
<p>`Q.ˇ/ D �.2n/�1
nX
</p>
<p>iD1
wi .zi � x&gt;i ˇ/2 C C. Q̌/2; (9.41)
</p>
<p>where working response and weight, respectively, are:
</p>
<p>zi D x&gt;i Q̌ C
yi � Qp.xi /
</p>
<p>Qp.xi /f1 � Qp.xi /g
;
</p>
<p>wi D Qp.xi / f1 � Qp.xi /g :
</p>
<p>A Newton update is obtained by minimising `Q.ˇ/.
</p>
<p>Friedman et al. (2010) proposed similar approach creating an outer loop for
</p>
<p>each value of �, which computes a quadratic approximation in (9.41) about current
</p>
<p>estimates Q̌. Afterwards, a coordinate descent algorithm is used to solve the
following penalised weighted least squares problem (PWLS)
</p>
<p>min
ˇ
</p>
<p>˚
�`Q.ˇ/C �P˛.ˇ/
</p>
<p>�
: (9.42)
</p>
<p>This inner coordinate descent loop continues until the maximum change in (9.42) is
</p>
<p>less than a very small threshold.
</p>
<p>9.3 Group Lasso
</p>
<p>The Group Lasso was first introduced by Yuan and Lin (2006) and was motivated
</p>
<p>by the fact that the predictor variables can occur in several groups and one could
</p>
<p>want a parsimonious model which uses only a few of these groups. That is, assume
</p>
<p>that there are K groups and the vector of coefficients is structured as follows
</p>
<p>ˇG D .ˇ&gt;1 ; : : : ; ˇ&gt;K /&gt; 2 R
P
k pk ;
</p>
<p>where pk is the coefficient vector dimension of the kth group, k D 1; : : : ; K . A
sparse set of groups is produced, although within each group either all entries of ˇk ,</p>
<p/>
</div>
<div class="page"><p/>
<p>9.3 Group Lasso 301
</p>
<p>k D 1; : : : ; K , a corresponding element of the whole vector ˇG are zero or all of
them are nonzero. The Group Lasso problem can be formulated in general as
</p>
<p>arg min
ˇ2R
</p>
<p>P
k pk
</p>
<p>n�1
&#13;&#13;&#13;&#13;&#13;y �
</p>
<p>KX
</p>
<p>kD1
Xkˇk
</p>
<p>&#13;&#13;&#13;&#13;&#13;
</p>
<p>2
</p>
<p>2
</p>
<p>C �
KX
</p>
<p>kD1
</p>
<p>p
pkkˇkk2; (9.43)
</p>
<p>where Xk is the kth component of the matrix X with columns corresponding to
</p>
<p>the predictors in the group k, ˇk is the coefficient vector for that group and pk is
</p>
<p>the cardinality of the group, i.e. the size of the coefficient vector which serves as
</p>
<p>a balancing weight in the case of widely differing group sizes. It is obvious that if
</p>
<p>groups consist of single elements, i.e. pk D 1 8k, then the Group Lasso problem is
reduced to the usual Lasso one.
</p>
<p>The computation of the Group Lasso solution involves calculating the necessary
</p>
<p>and sufficient subgradient KKT conditions for ǑG D . Ǒ&gt;1 ; : : : ; Ǒ&gt;K /&gt; to be a
solution for (9.43)
</p>
<p>� X&gt;k
</p>
<p> 
y �
</p>
<p>KX
</p>
<p>kD1
Xkˇk
</p>
<p>!
C
�ˇk
p
pk
</p>
<p>kˇkk
D 0; (9.44)
</p>
<p>if ˇk &curren; 0; otherwise, for ˇk D 0, it holds that
&#13;&#13;&#13;&#13;&#13;&#13;
X&gt;k
</p>
<p>0
@y �
</p>
<p>X
</p>
<p>l&curren;k
Xl Ǒl
</p>
<p>1
A
&#13;&#13;&#13;&#13;&#13;&#13;
� �ppk: (9.45)
</p>
<p>Expressions (9.44) and (9.45) allow to calculate the solution, the so-called update
</p>
<p>step which can be used to implement an iterative algorithm to solve the prob-
</p>
<p>lem (9.43). The solution resulting from the KKT conditions is readily shown to
</p>
<p>be the following:
</p>
<p>Ǒ
k D
</p>
<p>��
�
p
pkk Ǒkk�1 C X&gt;k Xk
</p>
<p>��1�C
X&gt;k Ork ; (9.46)
</p>
<p>where the residual Ork is defined as Ork defD y�
P
</p>
<p>l&curren;k Xl Ǒl . As a special (orthonormal)
case, when X&gt;l Xl D I, the solution is simplified to the Ǒk D .�
</p>
<p>p
pkk Ǒkk�1 C
</p>
<p>1/X&gt;k Ork. To obtain a full solution to this problem, Yuan and Lin (2006) suggest
using a blockwise coordinate descent algorithm which iteratively applies the
</p>
<p>estimate (9.46) to k D 1; : : : ; K .
Meier, van de Geer, and B&uuml;hlmann (2008) extended the Group Lasso to the case
</p>
<p>of logistic regression and demonstrated convergence of several algorithms for the
</p>
<p>computation of the solution as well as outlined consistency results for the Group
</p>
<p>Lasso logit estimator. The general setup for that model involves a binary response</p>
<p/>
</div>
<div class="page"><p/>
<p>302 9 Variable Selection
</p>
<p>variable yi 2 f0; 1g and K groups predictor variable xi D .x&gt;i1 ; : : : ; x&gt;ik /&gt;, both
xi and yi are i.i.d., i D 1; : : : ; n. Then the logistic linear regression model may be
written as before:
</p>
<p>log
</p>
<p>�
p.xi /
</p>
<p>1 � p.xi /
</p>
<p>�
D �.xi / defD ˇ0 C
</p>
<p>KX
</p>
<p>kD1
x&gt;ik ˇk ; (9.47)
</p>
<p>where the conditional probability p.xi / D P.yi D 1jxi/. The Group Lasso logit
estimator Ǒ then minimises the objective function
</p>
<p>Ǒ D arg min
ˇ2RpC1
</p>
<p>(
�`.ˇ/C �
</p>
<p>KX
</p>
<p>kD1
</p>
<p>p
pkkˇkk2
</p>
<p>)
; (9.48)
</p>
<p>where `.�/ is the log-likelihood function
</p>
<p>`.ˇ/ D
nX
</p>
<p>iD1
yi�.xi / � logŒ1C expf�.xi /g&#141;:
</p>
<p>The problem is solved through a group-wise minimisation of the penalised objective
</p>
<p>function by, for example, the block-coordinate descent method.
</p>
<p>Example 9.6 The Group Lasso results can be illustrated by an application to the
</p>
<p>MEMset Donor dataset of human donor splice sites with a sequence length of 7
</p>
<p>base pairs. The full dataset (training and test parts) consists of 12.623 true (yi D 1)
and 269.155 false (yi D 0) human donor sites. Each element of data represents a
sequence of DNA within a window of the splice site which consists of the last three
</p>
<p>positions of the exon and first 4 positions of the intron; so the strings of length 7
</p>
<p>are made up of 4 characters A, C, T, G and therefore the predictor variables are 7
</p>
<p>factors, each having 4 levels. False splice sites are sequences on the DNA which
</p>
<p>match the consensus sequence at position four and five. Figure 9.6 shows how the
</p>
<p>Group Lasso does shrinkage on the level of groups built by DNA letters.
</p>
<p>As is seen from Example 9.6, the solution to the Group Lasso problem yields a
</p>
<p>sparse solution only regarding the &ldquo;between&rdquo; case, that is, it excludes some of the
</p>
<p>groups from the model but then all coefficients in the remaining groups are nonzero.
</p>
<p>To ensure both the sparsity of groups and within each group, Simon, Friedman,
</p>
<p>Hastie, and Tibshirani (2013) proposed the so-called &ldquo;sparse Group Lasso&rdquo; which
</p>
<p>uses a more general penalty which yields sparsity an both inter- and intragroup level.
</p>
<p>The sparse Group Lasso estimate solves the problem
</p>
<p>Ǒ D arg min
ˇ2Rp
</p>
<p>&#13;&#13;&#13;&#13;&#13;y �
KX
</p>
<p>kD1
Xkˇk
</p>
<p>&#13;&#13;&#13;&#13;&#13;
</p>
<p>2
</p>
<p>2
</p>
<p>C �1
KX
</p>
<p>kD1
kˇkk2 C �2kˇk1; (9.49)
</p>
<p>where ˇ D .ˇ1; ˇ2; : : : ; ˇK/&gt; is the entire parameter vector.</p>
<p/>
</div>
<div class="page"><p/>
<p>9.3 Group Lasso 303
</p>
<p>70 60 50 40 30 20
</p>
<p>&minus;
1
.0
</p>
<p>&minus;
0
.5
</p>
<p>0
.0
</p>
<p>0
.5
</p>
<p>1
.0
</p>
<p>Coefficient paths
</p>
<p>Lambda
</p>
<p>C
o
</p>
<p>e
ff
</p>
<p>ic
ie
</p>
<p>n
ts
</p>
<p>(I
n
te
</p>
<p>rc
e
p
t)
</p>
<p>P
o
s
.2
</p>
<p>3
P
</p>
<p>o
s
.1
</p>
<p>2
P
</p>
<p>o
s
.4
</p>
<p>1
</p>
<p>Fig. 9.6 Lasso estimates of standardised regression Ǒj for car data with n D 74 and p D 12
MVAgrouplasso
</p>
<p>Summary
</p>
<p>,! Lasso gives a sparse solution. Lasso estimate combines best of both
ridge regression and subset regression.
</p>
<p>,! If there is a group of variables which has very high correlation, then
the Lasso tends to select only one variable from the group.
</p>
<p>,! The LARS algorithm computes the whole path of Lasso solutions
and is feasible for the high-dimensional case p � n.
</p>
<p>,! Elastic net combines good features of L1-norm and L2-norm
penalties.
</p>
<p>,! The Elastic net is very useful when p � n or there are many
correlated variables.
</p>
<p>,! The Sparse Group Lasso can perform shrinkage both on inter- and
intragroup level.</p>
<p/>
</div>
<div class="page"><p/>
<p>304 9 Variable Selection
</p>
<p>9.4 Exercises
</p>
<p>Exercise 9.1 Derive the explicit Lasso estimate in (9.11) for the orthonormal
</p>
<p>design case.
</p>
<p>Exercise 9.2 Compare Lasso orthonormal design case for p D 2 graphically to
ridge regression, i.e. to the problem Ǒ D argmin
</p>
<p>nPn
iD1
</p>
<p>�
yi � x&gt;i ˇ
</p>
<p>�2o
subject to
</p>
<p>Pp
jD1 ˇj
</p>
<p>2 � s.
Why does Lasso produce variable selection and ridge regression does not?
</p>
<p>Exercise 9.3 Optimise the value of s such that the fitted model in Example 9.3
</p>
<p>produces the smallest residual.
</p>
<p>Exercise 9.4 Optimise the value of s such that the fitted model in Example 9.4
</p>
<p>produces the smallest residual.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 10
</p>
<p>Decomposition of Data Matrices by Factors
</p>
<p>In Chap. 1 basic descriptive techniques were developed which provided tools
</p>
<p>for &ldquo;looking&rdquo; at multivariate data. They were based on adaptations of bivariate
</p>
<p>or univariate devices used to reduce the dimensions of the observations. In the
</p>
<p>following three chapters, issues of reducing the dimension of a multivariate data
</p>
<p>set will be discussed. The perspectives will be different but the tools will be related.
</p>
<p>In this chapter, we take a descriptive perspective and show how using a
</p>
<p>geometrical approach provides a &ldquo;best&rdquo; way of reducing the dimension of a data
</p>
<p>matrix. It is derived with respect to a least-squares criterion. The result will be low
</p>
<p>dimensional graphical pictures of the data matrix. This involves the decomposition
</p>
<p>of the data matrix into &ldquo;factors&rdquo;. These &ldquo;factors&rdquo; will be sorted in decreasing
</p>
<p>order of importance. The approach is very general and is the core idea of many
</p>
<p>multivariate techniques. We deliberately use the word &ldquo;factor&rdquo; here as a tool or
</p>
<p>transformation for structural interpretation in an exploratory analysis. In practice,
</p>
<p>the matrix to be decomposed will be some transformation of the original data
</p>
<p>matrix and as shown in the following chapters, these transformations provide easier
</p>
<p>interpretations of the obtained graphs in lower dimensional spaces.
</p>
<p>Chapter 11 addresses the issue of reducing the dimensionality of a multivariate
</p>
<p>random variable by using linear combinations (the principal components). The
</p>
<p>identified principal components are ordered in decreasing order of importance.
</p>
<p>When applied in practice to a data matrix, the principal components will turn out to
</p>
<p>be the factors of a transformed data matrix (the data will be centred and eventually
</p>
<p>standardised).
</p>
<p>Factor analysis is discussed in Chap. 12. The same problem of reducing the
</p>
<p>dimension of a multivariate random variable is addressed but in this case the
</p>
<p>number of factors is fixed from the start. Each factor is interpreted as a latent
</p>
<p>characteristic of the individuals revealed by the original variables. The non-
</p>
<p>uniqueness of the solutions is dealt with by searching for the representation with
</p>
<p>the easiest interpretation for the analysis.
</p>
<p>Summarising, this chapter can be seen as a foundation since it develops a basic
</p>
<p>tool for reducing the dimension of a multivariate data matrix.
</p>
<p>&copy; Springer-Verlag Berlin Heidelberg 2015
W.K. H&auml;rdle, L. Simar, Applied Multivariate Statistical Analysis,
DOI 10.1007/978-3-662-45171-7_10
</p>
<p>305</p>
<p/>
</div>
<div class="page"><p/>
<p>306 10 Decomposition of Data Matrices by Factors
</p>
<p>10.1 The Geometric Point of View
</p>
<p>As a matter of introducing certain ideas, assume that the data matrix X .n � p/ is
composed of n observations (or individuals) of p variables.
</p>
<p>There are in fact two ways of looking at X , row by row or column by column:
</p>
<p>1. Each row (observation) is a vector x&gt;i D .xi1; : : : ; xip/ 2 Rp .
From this point of view our data matrix X is representable as a cloud of n
</p>
<p>points in Rp as shown in Fig. 10.1.
</p>
<p>2. Each column (variable) is a vector xŒj &#141; D .x1j ; : : : ; xnj/&gt; 2 Rn.
From this point of view the data matrix X is a cloud of p points in Rn as
</p>
<p>shown in Fig. 10.2.
</p>
<p>When n and/or p are large (larger than 2 or 3), we cannot produce interpretable
</p>
<p>graphs of these clouds of points. Therefore, the aim of the factorial methods to be
</p>
<p>developed here is twofold. We shall try to simultaneously approximate the column
</p>
<p>spaceC.X / and the row spaceC.X&gt;/with smaller subspaces. The hope is of course
that this can be done without losing too much information about the variation and
</p>
<p>structure of the point clouds in both spaces. Ideally, this will provide insights into
</p>
<p>the structure of X through graphs in R, R2 or R3. The main focus then is to find the
</p>
<p>dimension reducing factors.
</p>
<p>Fig. 10.1 Cloud of n points in Rp
</p>
<p>Fig. 10.2 Cloud of p points in Rn</p>
<p/>
</div>
<div class="page"><p/>
<p>10.2 Fitting the p-Dimensional Point Cloud 307
</p>
<p>Summary
</p>
<p>,! Each row (individual) of X is a p-dimensional vector. From this
point of view X can be considered as a cloud of n points in Rp.
</p>
<p>,! Each column (variable) of X is a n-dimensional vector. From this
point of view X can be considered as a cloud of p points in Rn.
</p>
<p>10.2 Fitting the p-Dimensional Point Cloud
</p>
<p>Subspaces of Dimension 1
</p>
<p>In this section X is represented by a cloud of n points in Rp (considering each row).
</p>
<p>The question is how to project this point cloud onto a space of lower dimension. To
</p>
<p>begin consider the simplest problem, namely finding a subspace of dimension 1. The
</p>
<p>problem boils down to finding a straight line F1 through the origin. The direction of
</p>
<p>this line can be defined by a unit vector u1 2 Rp. Hence, we are searching for the
vector u1 which gives the &ldquo;best&rdquo; fit of the initial cloud of n points. The situation is
</p>
<p>depicted in Fig. 10.3.
</p>
<p>The representation of the i th individual xi 2 Rp on this line is obtained by the
projection of the corresponding point onto u1, i.e. the projection point pxi . We know
</p>
<p>from (2.42) that the coordinate of xi on F1 is given by
</p>
<p>pxi D x&gt;i
u1
</p>
<p>ku1k
D x&gt;i u1: (10.1)
</p>
<p>Fig. 10.3 Projection of point cloud onto u space of lower dimension</p>
<p/>
</div>
<div class="page"><p/>
<p>308 10 Decomposition of Data Matrices by Factors
</p>
<p>We define the best line F1 in the following &ldquo;least-squares&rdquo; sense: Find u1 2 Rp
which minimises
</p>
<p>nX
</p>
<p>iD1
kxi � pxi k2: (10.2)
</p>
<p>Since kxi � pxi k2 D kxik2 � kpxi k2 by Pythagoras&rsquo;s theorem, the problem of
minimising (10.2) is equivalent to maximising
</p>
<p>Pn
iD1 kpxi k2. Thus the problem is
</p>
<p>to find u1 2 Rp that maximises
Pn
</p>
<p>iD1 kpxi k2 under the constraint ku1k D 1.
With (10.1) we can write
</p>
<p>0
BBB@
</p>
<p>px1
px2
:::
</p>
<p>pxn
</p>
<p>1
CCCA D
</p>
<p>0
BBB@
</p>
<p>x&gt;1 u1
x&gt;2 u1
:::
</p>
<p>x&gt;n u1
</p>
<p>1
CCCA D Xu1
</p>
<p>and the problem can finally be reformulated as: find u1 2 Rp with ku1k D 1 that
maximises the quadratic form .Xu1/
</p>
<p>&gt;.Xu1/ or
</p>
<p>max
u&gt;1 u1D1
</p>
<p>u&gt;1 .X
&gt;X /u1: (10.3)
</p>
<p>The solution is given by Theorem 2.5 (using A D X&gt;X and B D I in the
theorem).
</p>
<p>Theorem 10.1 The vector u1 which minimises (10.2) is the eigenvector of X
&gt;X
</p>
<p>associated with the largest eigenvalue �1 of X
&gt;X .
</p>
<p>Note that if the data have been centred, i.e. x D 0, then X D Xc , where Xc is
the centred data matrix, and 1
</p>
<p>n
X&gt;X is the covariance matrix. Thus Theorem 10.1
</p>
<p>says that we are searching for a maximum of the quadratic form (10.3) w.r.t. the
</p>
<p>covariance matrix SX D n�1X&gt;X .
</p>
<p>Representation of the Cloud on F1
</p>
<p>The coordinates of the n individuals on F1 are given by Xu1. Xu1 is called the first
</p>
<p>factorial variable or the first factor and u1 the first factorial axis. The n individuals,
</p>
<p>xi , are now represented by a new factorial variable z1 D Xu1. This factorial variable
is a linear combination of the original variables .xŒ1&#141;; : : : ; xŒp&#141;/ whose coefficients
</p>
<p>are given by the vector u1, i.e.
</p>
<p>z1 D u11xŒ1&#141; C � � � C up1xŒp&#141;: (10.4)</p>
<p/>
</div>
<div class="page"><p/>
<p>10.2 Fitting the p-Dimensional Point Cloud 309
</p>
<p>Fig. 10.4 Representation of the individuals x1; : : : ; xn as a two-dimensional point cloud
</p>
<p>Subspaces of Dimension 2
</p>
<p>If we approximate the n individuals by a plane (dimension 2), it can be shown via
</p>
<p>Theorem 2.5 that this space contains u1. The plane is determined by the best linear
</p>
<p>fit (u1) and a unit vector u2 orthogonal to u1 which maximises the quadratic form
</p>
<p>u&gt;2 .X
&gt;X /u2 under the constraints
</p>
<p>ku2k D 1; and u&gt;1 u2 D 0:
</p>
<p>Theorem 10.2 The second factorial axis, u2, is the eigenvector of X
&gt;X corre-
</p>
<p>sponding to the second largest eigenvalue �2 of X
&gt;X .
</p>
<p>The unit vector u2 characterises a second line, F2, on which the points are
</p>
<p>projected. The coordinates of the n individuals on F2 are given by z2 D Xu2.
The variable z2 is called the second factorial variable or the second factor. The
</p>
<p>representation of the n individuals in two-dimensional space (z1 D Xu1 vs.
z2 D Xu2) is shown in Fig. 10.4.
</p>
<p>Subspaces of Dimension q .q � p/
</p>
<p>In the case of q dimensions the task is again to minimise (10.2) but with projection
</p>
<p>points in a q-dimensional subspace. Following the same argument as above, it can
</p>
<p>be shown via Theorem 2.5 that this best subspace is generated by u1; u2; : : : ; uq , the
</p>
<p>orthonormal eigenvectors of X&gt;X associated with the corresponding eigenvalues</p>
<p/>
</div>
<div class="page"><p/>
<p>310 10 Decomposition of Data Matrices by Factors
</p>
<p>�1 � �2 � � � � � �q . The coordinates of the n individuals on the kth factorial
axis, uk , are given by the kth factorial variable zk D Xuk for k D 1; : : : ; q. Each
factorial variable zk D .z1k ; z2k ; : : : ; znk/&gt; is a linear combination of the original
variables xŒ1&#141;; xŒ2&#141;; : : : ; xŒp&#141; whose coefficients are given by the elements of the kth
</p>
<p>vector uk W zik D
Pp
</p>
<p>mD1 ximumk.
</p>
<p>Summary
</p>
<p>,! The p-dimensional point cloud of individuals can be graphically
represented by projecting each element into spaces of smaller
</p>
<p>dimensions.
,! The first factorial axis is u1 and defines a line F1 through the origin.
</p>
<p>This line is found by minimising the orthogonal distances (10.2).
</p>
<p>The factor u1 equals the eigenvector of X
&gt;X corresponding to its
</p>
<p>largest eigenvalue. The coordinates for representing the point cloud
</p>
<p>on a straight line are given by z1 D Xu1.
,! The second factorial axis is u2, where u2 denotes the eigenvector
</p>
<p>of X&gt;X corresponding to its second largest eigenvalue. The
coordinates for representing the point cloud on a plane are given
</p>
<p>by z1 D Xu1 and z2 D Xu2.
,! The factor directions 1; : : : ; q are u1; : : : ; uq , which denote the
</p>
<p>eigenvectors of X&gt;X corresponding to the q largest eigenvalues.
The coordinates for representing the point cloud of individuals on
</p>
<p>a q-dimensional subspace are given by z1 D Xu1; : : : ; zq D Xuq .
</p>
<p>10.3 Fitting the n-Dimensional Point Cloud
</p>
<p>Subspaces of Dimension 1
</p>
<p>Suppose that X is represented by a cloud of p points (variables) in Rn (considering
</p>
<p>each column). How can this cloud be projected into a lower dimensional space? We
</p>
<p>start as before with one dimension. In other words, we have to find a straight line
</p>
<p>G1, which is defined by the unit vector v1 2 Rn, and which gives the best fit of the
initial cloud of p points.
</p>
<p>Algebraically, this is the same problem as above (replace X by X&gt; and follow
Sect. 10.2): the representation of the j th variable xŒj &#141; 2 Rn is obtained by the
projection of the corresponding point onto the straight line G1 or the direction v1.
</p>
<p>Hence we have to find v1 such that
Pp
</p>
<p>jD1 kpxŒj &#141;k2 is maximised, or equivalently, we
have to find the unit vector v1 which maximises .X
</p>
<p>&gt;v1/&gt;.Xv1/ D v&gt;1 .XX&gt;/v1.
The solution is given by Theorem 2.5.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.3 Fitting the n-Dimensional Point Cloud 311
</p>
<p>Theorem 10.3 v1 is the eigenvector of XX
&gt; corresponding to the largest eigen-
</p>
<p>value �1 of XX
&gt;:
</p>
<p>Representation of the Cloud on G1
</p>
<p>The coordinates of the p variables onG1 are given by w1 D X&gt;v1, the first factorial
axis. The p variables are now represented by a linear combination of the original
</p>
<p>individuals x1; : : : ; xn, whose coefficients are given by the vector v1, i.e. for j D
1; : : : ; p
</p>
<p>w1j D v11x1j C � � � C v1nxnj: (10.5)
</p>
<p>Subspaces of Dimension q .q � n/
</p>
<p>The representation of the p variables in a subspace of dimension q is done in the
</p>
<p>same manner as for the n individuals above. The best subspace is generated by the
</p>
<p>orthonormal eigenvectors v1; v2; : : : ; vq of XX
&gt; associated with the eigenvalues
</p>
<p>�1 � �2 � � � � � �q . The coordinates of the p variables on the kth factorial
axis are given by the factorial variables wk D X&gt;vk ; k D 1; : : : ; q. Each
factorial variable wk D .wk1;wk2; : : : ;wkp/&gt; is a linear combination of the original
individuals x1; x2; : : : ; xn whose coefficients are given by the elements of the kth
</p>
<p>vector vk W wkj D
Pn
</p>
<p>mD1 vkmxmj. The representation in a subspace of dimension
q D 2 is depicted in Fig. 10.5.
</p>
<p>Fig. 10.5 Representation of the variables xŒ1&#141;; : : : ; xŒp&#141; as a two-dimensional point cloud</p>
<p/>
</div>
<div class="page"><p/>
<p>312 10 Decomposition of Data Matrices by Factors
</p>
<p>Summary
</p>
<p>,! The n-dimensional point cloud of variables can be graphically
represented by projecting each element into spaces of smaller
</p>
<p>dimensions.
,! The first factor direction is v1 and defines a line G1 through the
</p>
<p>origin. The vector v1 equals the eigenvector ofXX
&gt; corresponding
</p>
<p>to the largest eigenvalue of XX&gt;. The coordinates for representing
the point cloud on a straight line are w1 D X&gt;v1.
</p>
<p>,! The second factor direction is v2, where v2 denotes the eigenvector
of XX&gt; corresponding to its second largest eigenvalue. The
coordinates for representing the point cloud on a plane are given
</p>
<p>by w1 D X&gt;v1 and w2 D X&gt;v2.
,! The factor directions 1; : : : ; q are v1; : : : ; vq , which denote the
</p>
<p>eigenvectors of XX&gt; corresponding to the q largest eigenvalues.
The coordinates for representing the point cloud of variables on a q-
</p>
<p>dimensional subspace are given by w1 D X&gt;v1; : : : ;wq D X&gt;vq .
</p>
<p>10.4 Relations Between Subspaces
</p>
<p>The aim of this section is to present a duality relationship between the two
</p>
<p>approaches shown in Sects. 10.2 and 10.3. Consider the eigenvector equations in Rn
</p>
<p>.XX&gt;/vk D �kvk (10.6)
</p>
<p>for k � r , where r D rank.XX&gt;/ D rank.X / � min.p; n/. Multiplying by X&gt;,
we have
</p>
<p>X&gt;.XX&gt;/vk D �kX&gt;vk (10.7)
or .X&gt;X /.X&gt;vk/ D �k.X&gt;vk/ (10.8)
</p>
<p>so that each eigenvector vk ofXX
&gt; corresponds to an eigenvector .X&gt;vk/ ofX&gt;X
</p>
<p>associated with the same eigenvalue �k . This means that every nonzero eigenvalue
</p>
<p>of XX&gt; is an eigenvalue of X&gt;X . The corresponding eigenvectors are related by
</p>
<p>uk D ckX&gt;vk ;
</p>
<p>where ck is some constant.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.4 Relations Between Subspaces 313
</p>
<p>Now consider the eigenvector equations in Rp:
</p>
<p>.X&gt;X /uk D �kuk (10.9)
</p>
<p>for k � r:Multiplying by X , we have
</p>
<p>.XX&gt;/.Xuk/ D �k.Xuk/; (10.10)
</p>
<p>i.e. each eigenvector uk of X
&gt;X corresponds to an eigenvector Xuk of XX&gt;
</p>
<p>associated with the same eigenvalue �k. Therefore, every nonzero eigenvalue of
</p>
<p>.X&gt;X / is an eigenvalue of XX&gt;. The corresponding eigenvectors are related by
</p>
<p>vk D dkXuk ;
</p>
<p>where dk is some constant. Now, since u
&gt;
k uk D v&gt;k vk D 1we have ck D dk D 1p�k .
</p>
<p>This lead to the following result:
</p>
<p>Theorem 10.4 (Duality Relations) Let r be the rank of X . For k � r , the
eigenvalues �k of X
</p>
<p>&gt;X and XX&gt; are the same and the eigenvectors (uk and vk ,
respectively) are related by
</p>
<p>uk D
1p
�k
</p>
<p>X&gt;vk (10.11)
</p>
<p>vk D
1p
�k
</p>
<p>Xuk : (10.12)
</p>
<p>Note that the projection of the p variables on the factorial axis vk is given by
</p>
<p>wk D X&gt;vk D
1p
�k
</p>
<p>X&gt;Xuk D
p
�k uk : (10.13)
</p>
<p>Therefore, the eigenvectors vk do not have to be explicitly recomputed to get wk .
</p>
<p>Note that uk and vk provide the SVD of X (see Theorem 2.2). Letting
</p>
<p>U D Œu1 u2 : : : ur &#141;; V D Œv1 v2 : : : vr &#141; and ƒ D diag.�1; : : : ; �r / we have
</p>
<p>X D V ƒ1=2 U&gt;
</p>
<p>so that
</p>
<p>xij D
rX
</p>
<p>kD1
�
1=2
</p>
<p>k vik ujk: (10.14)
</p>
<p>In the following section this method is applied in analysing consumption
</p>
<p>behaviour across different household types.</p>
<p/>
</div>
<div class="page"><p/>
<p>314 10 Decomposition of Data Matrices by Factors
</p>
<p>Summary
</p>
<p>,! The matrices X&gt;X and XX&gt; have the same nonzero eigenvalues
�1; : : : ; �r , where r D rank.X /.
</p>
<p>,! The eigenvectors of X&gt;X can be calculated from the eigenvectors
of XX&gt; and vice versa:
</p>
<p>uk D
1p
�k
</p>
<p>X&gt;vk and vk D
1p
�k
</p>
<p>Xuk :
</p>
<p>,! The coordinates representing the variables (columns) of X in a
q-dimensional subspace can be easily calculated by wk D
</p>
<p>p
�kuk.
</p>
<p>10.5 Practical Computation
</p>
<p>The practical implementation of the techniques introduced begins with the compu-
</p>
<p>tation of the eigenvalues �1 � �2 � � � � � �p and the corresponding eigenvectors
u1; : : : ; up of X
</p>
<p>&gt;X . (Since p is usually less than n, this is numerically less
involved than computing vk directly for k D 1; : : : ; p.) The representation of the
n individuals on a plane is then obtained by plotting z1 D Xu1 versus z2 D Xu2
(z3 D Xu3 may eventually be added if a third dimension is helpful). Using the
Duality Relation (10.13) representations for the p variables can easily be obtained.
</p>
<p>These representations can be visualised in a scatterplot of w1 D
p
�1 u1 against
</p>
<p>w2 D
p
�2u2 (and eventually against w3 D
</p>
<p>p
�3 u3). Higher dimensional factorial
</p>
<p>resolutions can be obtained (by computing zk and wk for k &gt; 3) but, of course,
</p>
<p>cannot be plotted.
</p>
<p>A standard way of evaluating the quality of the factorial representations in a
</p>
<p>subspace of dimension q is given by the ratio
</p>
<p>�q D
�1 C �2 C � � � C �q
�1 C �2 C � � � C �p
</p>
<p>; (10.15)
</p>
<p>where 0 � �q � 1. In general, the scalar product y&gt;y is called the inertia of y 2 Rn
w.r.t. the origin. Therefore, the ratio �q is usually interpreted as the percentage of
</p>
<p>the inertia explained by the first q factors. Note that �j D .Xuj /&gt;.Xuj / D z&gt;j zj .
Thus, �j is the inertia of the j th factorial variable w.r.t. the origin. The denominator
</p>
<p>in (10.15) is a measure of the total inertia of the p variables, xŒj &#141;. Indeed, by (2.3)
</p>
<p>pX
</p>
<p>jD1
�j D tr.X&gt;X / D
</p>
<p>pX
</p>
<p>jD1
</p>
<p>nX
</p>
<p>iD1
x2ij D
</p>
<p>pX
</p>
<p>jD1
x&gt;Œj &#141;xŒj &#141;:</p>
<p/>
</div>
<div class="page"><p/>
<p>10.5 Practical Computation 315
</p>
<p>Remark 10.1 It is clear that the sum
Pq
</p>
<p>jD1 �j is the sum of the inertia of the first q
factorial variables z1; z2; : : : ; zq .
</p>
<p>Example 10.1 We consider the data set in Table 22.6 which gives the food
</p>
<p>expenditures of various French families (manual workersDMA, employeesDEM,
managersDCA) with varying numbers of children (2, 3, 4 or 5 children). We are
interested in investigatingwhether certain household types prefer certain food types.
</p>
<p>We can answer this question using the factorial approximations developed here.
</p>
<p>The correlation matrix corresponding to the data is
</p>
<p>R D
</p>
<p>0
BBBBBBBBB@
</p>
<p>1:00 0:59 0:20 0:32 0:25 0:86 0:30
</p>
<p>0:59 1:00 0:86 0:88 0:83 0:66 �0:36
0:20 0:86 1:00 0:96 0:93 0:33 �0:49
0:32 0:88 0:96 1:00 0:98 0:37 �0:44
0:25 0:83 0:93 0:98 1:00 0:23 �0:40
0:86 0:66 0:33 0:37 0:23 1:00 0:01
</p>
<p>0:30 �0:36 �0:49 �0:44 �0:40 0:01 1:00
</p>
<p>1
CCCCCCCCCA
</p>
<p>�
</p>
<p>We observe a rather high correlation (0.98) between meat and poultry, whereas
</p>
<p>the correlation for expenditure for milk and wine (0.01) is rather small. Are there
</p>
<p>household types that prefer, say, meat over bread?
</p>
<p>We shall now represent food expenditures and households simultaneously using
</p>
<p>two factors. First, note that in this particular problem the origin has no specific
</p>
<p>meaning (it represents a &ldquo;zero&rdquo; consumer). So it makes sense to compare the
</p>
<p>consumption of any family to that of an &ldquo;average family&rdquo; rather than to the origin.
</p>
<p>Therefore, the data is first centred (the origin is translated to the centre of gravity,
</p>
<p>x). Furthermore, since the dispersions of the seven variables are quite different each
</p>
<p>variable is standardised so that each has the same weight in the analysis (mean
</p>
<p>0 and variance 1). Finally, for convenience, we divide each element in the matrix
</p>
<p>by
p
n D
</p>
<p>p
12. (This will only change the scaling of the plots in the graphical
</p>
<p>representation.)
</p>
<p>The data matrix to be analysed is
</p>
<p>X� D
1p
n
HXD�1=2;
</p>
<p>where H is the centering matrix and D D diag.sXiXi / (see Sect. 3.3). Note that by
standardising by
</p>
<p>p
n, it follows that X&gt;� X� D R whereR is the correlation matrix
</p>
<p>of the original data. Calculating
</p>
<p>� D .4:33; 1:83; 0:63; 0:13; 0:06; 0:02; 0:00/&gt;
</p>
<p>shows that the directions of the first two eigenvectors play a dominant role (�2 D
88%), whereas the other directions contribute less than 15% of inertia. A two-
</p>
<p>dimensional plot should suffice for interpreting this data set.</p>
<p/>
</div>
<div class="page"><p/>
<p>316 10 Decomposition of Data Matrices by Factors
</p>
<p>-1 -0.5 0 0.5
-1
</p>
<p>-0.5
</p>
<p>0
</p>
<p>0.5
Food
</p>
<p>W[.1]
</p>
<p>W
[.
</p>
<p>2
]
</p>
<p>bread
</p>
<p>veget
</p>
<p>fruit
</p>
<p>meat
poult
</p>
<p>milk
wine
</p>
<p>-2 0 2
</p>
<p>-1
</p>
<p>0
</p>
<p>1
Families
</p>
<p>Z[.1]
</p>
<p>Z
[.
</p>
<p>2
] ma2
</p>
<p>em2
</p>
<p>ca2
</p>
<p>ma3
</p>
<p>em3
</p>
<p>ca3
</p>
<p>ma4
</p>
<p>em4
</p>
<p>ca4
</p>
<p>ma5
</p>
<p>em5
</p>
<p>ca5
</p>
<p>bread -0.49873 -0.84162
</p>
<p>veget -0.96975 -0.13310
</p>
<p>fruit -0.92913 0.27791
</p>
<p>meat -0.96210 0.19107
</p>
<p>poult -0.91125 0.26590
</p>
<p>milk -0.58434 -0.70690
</p>
<p>wine 0.42820 -0.64815
</p>
<p>ma2 1.12853 0.14353
</p>
<p>em2 0.74582 0.70752
</p>
<p>ca2 0.04654 0.28641
</p>
<p>ma3 0.80563 -0.12763
</p>
<p>em3 0.66887 0.06423
</p>
<p>ca3 -0.66886 0.53487
</p>
<p>ma4 0.36826 -0.54151
</p>
<p>em4 0.09955 -0.24969
</p>
<p>ca4 -0.63184 0.68533
</p>
<p>ma5 -0.08726 -1.09621
</p>
<p>em5 -0.77026 -0.44656
</p>
<p>ca5 -1.70498 0.03971
</p>
<p>Fig. 10.6 Representation of food expenditures and family types in two dimensions
MVAdecofood
</p>
<p>The coordinates of the projected data points are given in the two lower windows
</p>
<p>of Fig. 10.6. Let us first examine the food expenditure window. In this window we
</p>
<p>see the representation of the p D 7 variables given by the first two factors. The
plot shows the factorial variables w1 and w2 in the same fashion as Fig. 10.4. We
</p>
<p>see that the points for meat, poultry, vegetables and fruits are close to each other in
</p>
<p>the lower left of the graph. The expenditures for bread and milk can be found in the
</p>
<p>upper left, whereas wine stands alone in the upper right. The first factor, w1, may
</p>
<p>be interpreted as the meat/fruit factor of consumption, the second factor, w2, as the
</p>
<p>bread/wine component.
</p>
<p>In the lower window on the right-hand side, we show the factorial variables z1 and
</p>
<p>z2 from the fit of the n D 12 household types. Note that by the Duality Relations
of Theorem 10.4, the factorial variables zj are linear combinations of the factors
</p>
<p>wk from the left window. The points displayed in the consumer window (graph on
</p>
<p>the right) are plotted relative to an average consumer represented by the origin.
</p>
<p>The manager families are located in the lower left corner of the graph whereas the
</p>
<p>manual workers and employees tend to be in the upper right. The factorial variables
</p>
<p>for CA5 (managers with five children) lie close to the meat/fruit factor. Relative to
</p>
<p>the average consumer this household type is a large consumer of meat/poultry and</p>
<p/>
</div>
<div class="page"><p/>
<p>10.6 Exercises 317
</p>
<p>fruits/vegetables. In Chap. 11, we will return to these plots interpreting them in a
</p>
<p>much deeper way. At this stage, it suffices to notice that the plots provide a graphical
</p>
<p>representation in R2 of the information contained in the original, high-dimensional
</p>
<p>(12 � 7) data matrix.
</p>
<p>Summary
</p>
<p>,! The practical implementation of factor decomposition of matrices
consists of computing the eigenvalues�1; : : : ; �p and the eigenvec-
</p>
<p>tors u1; : : : ; up of X
&gt;X . The representation of the n individuals is
</p>
<p>obtained by plotting z1 D Xu1 vs. z2 D Xu2 (and, if necessary,
vs. z3 D Xu3). The representation of the p variables is obtained
by plotting w1 D
</p>
<p>p
�1u1 vs. w2 D
</p>
<p>p
�2u2 (and, if necessary, vs.
</p>
<p>w3 D
p
�3u3).
</p>
<p>,! The quality of the factorial representation can be evaluated using �q
which is the percentage of inertia explained by the first q factors.
</p>
<p>10.6 Exercises
</p>
<p>Exercise 10.1 Prove that n�1Z&gt;Z is the covariance of the centred data matrix,
where Z is the matrix formed by the columns zk D Xuk .
Exercise 10.2 Compute the SVD of the French food data (Table 22.6).
</p>
<p>Exercise 10.3 Compute �3; �4; : : : for the French food data (Table 22.6).
</p>
<p>Exercise 10.4 Apply the factorial techniques to the Swiss bank notes (Sect. 22.2).
</p>
<p>Exercise 10.5 Apply the factorial techniques to the time budget data (Table 22.14).
</p>
<p>Exercise 10.6 Assume that you wish to analyse p independent identically dis-
</p>
<p>tributed random variables. What is the percentage of the inertia explained by the
</p>
<p>first factor? What is the percentage of the inertia explained by the first q factors?
</p>
<p>Exercise 10.7 Assume that you have p i.i.d. r.v.&rsquo;s. What does the eigenvector,
</p>
<p>corresponding to the first factor, look like.
</p>
<p>Exercise 10.8 Assume that you have two random variables, X1 and X2 D 2X1.
What do the eigenvalues and eigenvectors of their correlation matrix look like? How
</p>
<p>many eigenvalues are nonzero?</p>
<p/>
</div>
<div class="page"><p/>
<p>318 10 Decomposition of Data Matrices by Factors
</p>
<p>Exercise 10.9 What percentage of inertia is explained by the first factor in the
</p>
<p>previous exercise?
</p>
<p>Exercise 10.10 How do the eigenvalues and eigenvectors in Example 10.1 change
</p>
<p>if we take the prices in USD instead of in EUR? Does it make a difference if some
</p>
<p>of the prices are in EUR and others in USD?</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 11
</p>
<p>Principal Components Analysis
</p>
<p>Chapter 10 presented the basic geometric tools needed to produce a lower dimen-
</p>
<p>sional description of the rows and columns of a multivariate data matrix. Principal
</p>
<p>components analysis (PCA) has the same objective with the exception that the rows
</p>
<p>of the data matrix X will now be considered as observations from a p-variate
</p>
<p>random variable X . The principle idea of reducing the dimension of X is achieved
</p>
<p>through linear combinations. Low dimensional linear combinations are often easier
</p>
<p>to interpret and serve as an intermediate step in a more complex data analysis. More
</p>
<p>precisely one looks for linear combinations which create the largest spread among
</p>
<p>the values of X . In other words, one is searching for linear combinations with the
</p>
<p>largest variances.
</p>
<p>Section 11.1 introduces the basic ideas and technical elements behind principal
</p>
<p>components. No particular assumption will be made on X except that the mean
</p>
<p>vector and the covariance matrix exist. When reference is made to a data matrix X
</p>
<p>in Sect. 11.2, the empirical mean and covariance matrix will be used. Section 11.3
</p>
<p>shows how to interpret the principal components by studying their correlations
</p>
<p>with the original components of X . Often analyses are performed in practice by
</p>
<p>looking at two-dimensional scatterplots. Section 11.4 develops inference techniques
</p>
<p>on principal components. This is particularly helpful in establishing the appropriate
</p>
<p>dimension reduction and thus in determining the quality of the resulting lower
</p>
<p>dimensional representations. Since principal component analysis is performed on
</p>
<p>covariance matrices, it is not scale invariant. Often, the measurement units of
</p>
<p>the components of X are quite different, so it is reasonable to standardise the
</p>
<p>measurement units. The normalised version of principal components is defined in
</p>
<p>Sect. 11.5. In Sect. 11.6 it is discovered that the empirical principal components are
</p>
<p>the factors of appropriate transformations of the data matrix. The classical way
</p>
<p>of defining principal components through linear combinations with respect to the
</p>
<p>largest variance is described here in geometric terms, i.e. in terms of the optimal fit
</p>
<p>within subspaces generated by the columns and/or the rows of X as was discussed
</p>
<p>in Chap. 10. Section 11.9 concludes with additional examples.
</p>
<p>&copy; Springer-Verlag Berlin Heidelberg 2015
W.K. H&auml;rdle, L. Simar, Applied Multivariate Statistical Analysis,
DOI 10.1007/978-3-662-45171-7_11
</p>
<p>319</p>
<p/>
</div>
<div class="page"><p/>
<p>320 11 Principal Components Analysis
</p>
<p>11.1 Standardised Linear Combination
</p>
<p>The main objective of PCA is to reduce the dimension of the observations. The
</p>
<p>simplest way of dimension reduction is to take just one element of the observed
</p>
<p>vector and to discard all others. This is not a very reasonable approach, as we have
</p>
<p>seen in the earlier chapters, since strength may be lost in interpreting the data. In
</p>
<p>the bank notes example we have seen that just one variable (e.g. X1 D length)
had no discriminatory power in distinguishing counterfeit from genuine bank notes.
</p>
<p>An alternative method is to weight all variables equally, i.e. to consider the simple
</p>
<p>average p�1
Pp
</p>
<p>jD1Xj of all the elements in the vector X D .X1; : : : ; Xp/&gt;. This
again is undesirable, since all of the elements of X are considered with equal
</p>
<p>importance (weight).
</p>
<p>A more flexible approach is to study a weighted average, namely
</p>
<p>ı&gt;X D
pX
</p>
<p>jD1
ıjXj ; such that
</p>
<p>pX
</p>
<p>jD1
ı2j D 1: (11.1)
</p>
<p>The weighting vector ı D .ı1; : : : ; ıp/&gt; can then be optimised to investigate
and to detect specific features. We call (11.1) a standardised linear combination
</p>
<p>(SLC). Which SLC should we choose? One aim is to maximise the variance of the
</p>
<p>projection ı&gt;X , i.e. to choose ı according to
</p>
<p>max
fıWkıkD1g
</p>
<p>Var.ı&gt;X/ D max
fıWkıkD1g
</p>
<p>ı&gt; Var.X/ı: (11.2)
</p>
<p>The interesting &ldquo;directions&rdquo; of ı are found through the spectral decomposition of
</p>
<p>the covariance matrix. Indeed, from Theorem 2.5, the direction ı is given by the
</p>
<p>eigenvector &#13;1 corresponding to the largest eigenvalue �1 of the covariance matrix
</p>
<p>&dagger; D Var.X/.
Figures 11.1 and 11.2 show two such projections (SLCs) of the same data set
</p>
<p>with zero mean. In Fig. 11.1 an arbitrary projection is displayed. The upper window
</p>
<p>shows the data point cloud and the line onto which the data are projected. The
</p>
<p>middle window shows the projected values in the selected direction. The lower
</p>
<p>window shows the variance of the actual projection and the percentage of the total
</p>
<p>variance that is explained.
</p>
<p>Figure 11.2 shows the projection that captures the majority of the variance in the
</p>
<p>data. This direction is of interest and is located along the main direction of the point
</p>
<p>cloud. The same line of thought can be applied to all data orthogonal to this direction
</p>
<p>leading to the second eigenvector. The SLC with the highest variance obtained from
</p>
<p>maximising (11.2) is the first principal component (PC) y1 D &#13;&gt;1 X . Orthogonal to
the direction &#13;1 we find the SLC with the second highest variance: y2 D &#13;&gt;2 X , the
second PC.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.1 Standardised Linear Combination 321
</p>
<p>&minus;3 &minus;2 &minus;1 0 1 2 3
&minus;5
</p>
<p>0
</p>
<p>5
</p>
<p>Direction in Data
</p>
<p>&minus;3 &minus;2 &minus;1 0 1 2 3
&minus;1
</p>
<p>0
</p>
<p>1
Projection
</p>
<p>Explained variance 0.50520
</p>
<p>Total variance 1.96569
</p>
<p>Explained percentage 0.25701
</p>
<p>Fig. 11.1 An arbitrary SLC MVApcasimu
</p>
<p>&minus;3 &minus;2 &minus;1 0 1 2 3
&minus;5
</p>
<p>0
</p>
<p>5
Direction in Data
</p>
<p>&minus;3 &minus;2 &minus;1 0 1 2 3
&minus;1
</p>
<p>0
</p>
<p>1
Projection
</p>
<p>Explained variance 1.46049
</p>
<p>Total variance 1.96569
</p>
<p>Explained percentage 0.74299
</p>
<p>Fig. 11.2 The most interesting SLC MVApcasimu
</p>
<p>Proceeding in this way and writing in matrix notation, the result for a random
</p>
<p>variable X with E.X/ D � and Var.X/ D &dagger; D &#128;ƒ&#128;&gt; is the PC transformation
which is defined as
</p>
<p>Y D &#128;&gt;.X � �/: (11.3)
</p>
<p>Here we have centred the variable X in order to obtain a zero mean PC variable Y .</p>
<p/>
</div>
<div class="page"><p/>
<p>322 11 Principal Components Analysis
</p>
<p>Example 11.1 Consider a bivariate normal distribution N.0;&dagger;/ with &dagger; D
�
1
�
�
1
</p>
<p>�
</p>
<p>and � &gt; 0 (see Example 3.13). Recall that the eigenvalues of this matrix are �1 D
1C � and �2 D 1 � � with corresponding eigenvectors
</p>
<p>&#13;1 D
1p
2
</p>
<p>�
1
</p>
<p>1
</p>
<p>�
; &#13;2 D
</p>
<p>1p
2
</p>
<p>�
1
</p>
<p>�1
</p>
<p>�
:
</p>
<p>The PC transformation is thus
</p>
<p>Y D &#128;&gt;.X � �/ D 1p
2
</p>
<p>�
1 1
</p>
<p>1 �1
</p>
<p>�
X
</p>
<p>or �
Y1
Y2
</p>
<p>�
D 1p
</p>
<p>2
</p>
<p>�
X1 CX2
X1 � X2
</p>
<p>�
:
</p>
<p>So the first principal component is
</p>
<p>Y1 D
1p
2
.X1 CX2/
</p>
<p>and the second is
</p>
<p>Y2 D
1p
2
.X1 � X2/:
</p>
<p>Let us compute the variances of these PCs using formulas (4.22)&ndash;(4.26):
</p>
<p>Var.Y1/ D Var
�
1p
2
.X1 CX2/
</p>
<p>�
D 1
2
</p>
<p>Var.X1 CX2/
</p>
<p>D 1
2
fVar.X1/C Var.X2/C 2Cov.X1; X2/g
</p>
<p>D 1
2
.1C 1C 2�/ D 1C �
</p>
<p>D �1:
</p>
<p>Similarly we find that
</p>
<p>Var.Y2/ D �2:
</p>
<p>This can be expressed more generally and is given in the next theorem.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.1 Standardised Linear Combination 323
</p>
<p>Theorem 11.1 For a given X � .�;&dagger;/ let Y D &#128;&gt;.X � �/ be the PC
transformation. Then
</p>
<p>EYj D 0; j D 1; : : : ; p (11.4)
Var.Yj / D �j ; j D 1; : : : ; p (11.5)
Cov.Yi ; Yj / D 0; i &curren; j (11.6)
Var.Y1/ � Var.Y2/ � � � � � Var.Yp/ � 0 (11.7)
pX
</p>
<p>jD1
Var.Yj / D tr.&dagger;/ (11.8)
</p>
<p>pY
</p>
<p>jD1
Var.Yj / D j&dagger;j: (11.9)
</p>
<p>Proof To prove (11.6), we use &#13;i to denote the i th column of &#128; . Then
</p>
<p>Cov.Yi ; Yj / D &#13;&gt;i Var.X � �/&#13;j D &#13;&gt;i Var.X/&#13;j :
</p>
<p>As Var.X/ D &dagger; D &#128;ƒ&#128;&gt;; &#128;&gt;&#128; D I; we obtain via the orthogonality of &#128;:
</p>
<p>&#13;&gt;i &#128;ƒ&#128;
&gt;&#13;j D
</p>
<p>(
0 i &curren; j;
�i i D j:
</p>
<p>In fact, as Yi D &#13;&gt;i .X � �/ lies in the eigenvector space corresponding to &#13;i , and
eigenvector spaces corresponding to different eigenvalues are orthogonal to each
</p>
<p>other, we can directly see Yi and Yj are orthogonal to each other, so their covariance
</p>
<p>is 0. ut
The connection between the PC transformation and the search for the best SLC is
</p>
<p>made in the following theorem,which follows directly from (11.2) and Theorem 2.5.
</p>
<p>Theorem 11.2 There exists no SLC that has larger variance than �1 D Var.Y1/.
Theorem 11.3 If Y D a&gt;X is an SLC that is not correlated with the first k PCs of
X , then the variance of Y is maximised by choosing it to be the .k C 1/-st PC.
</p>
<p>Summary
</p>
<p>,! An SLC is a weighted average ı&gt;X D
Pp
</p>
<p>jD1 ıjXj where ı is a
vector of length 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>324 11 Principal Components Analysis
</p>
<p>Summary (continued)
</p>
<p>,! Maximising the variance of ı&gt;X leads to the choice ı D &#13;1, the
eigenvector corresponding to the largest eigenvalue �1 of &dagger; D
Var.X/.
</p>
<p>This is a projection of X into the one-dimensional space, where
</p>
<p>the components of X are weighted by the elements of &#13;1. Y1 D
&#13;&gt;1 .X � �/
is called the first principal component (PC).
</p>
<p>,! This projection can be generalised for higher dimensions. The PC
transformation is the linear transformation Y D &#128;&gt;.X��/, where
&dagger; D Var.X/ D &#128;ƒ&#128;&gt; and � D EX .
Y1; Y2; : : : ; Yp are called the first, second,. . . , and p-th PCs.
</p>
<p>,! The PCs have zero means, variance Var.Yj / D �j , and zero
covariances. From �1 � � � � � �p it follows that Var.Y1/ �
� � � � Var.Yp/. It holds that
</p>
<p>Pp
jD1 Var.Yj / D tr.&dagger;/ andQp
</p>
<p>jD1 Var.Yj / D j&dagger;j.
,! If Y D a&gt;X is an SLC which is not correlated with the first k PCs
</p>
<p>of X , then the variance of Y is maximised by choosing it to be the
</p>
<p>.k C 1/-st PC.
</p>
<p>11.2 Principal Components in Practice
</p>
<p>In practice the PC transformation has to be replaced by the respective estimators: �
</p>
<p>becomes x, &dagger; is replaced by S, etc. If g1 denotes the first eigenvector of S, the first
</p>
<p>principal component is given by y1 D .X �1nx&gt;/g1. More generally if S D GLG&gt;
is the spectral decomposition of S, then the PCs are obtained by
</p>
<p>Y D .X � 1nx&gt;/G: (11.10)
</p>
<p>Note that with the centering matrix H D I � .n�11n1&gt;n / and H1nx&gt; D 0 we can
write
</p>
<p>SY D n�1Y&gt;HY D n�1G&gt;.X � 1nx&gt;/&gt;H.X � 1nx&gt;/G
D n�1G&gt;X&gt;HXG D G&gt;SG D L (11.11)
</p>
<p>where L D diag.`1; : : : ; `p/ is the matrix of eigenvalues of S. Hence the variance
of yi equals the eigenvalue `i !
</p>
<p>The PC technique is sensitive to scale changes. If we multiply one variable by a
</p>
<p>scalar we obtain different eigenvalues and eigenvectors. This is due to the fact that</p>
<p/>
</div>
<div class="page"><p/>
<p>11.2 Principal Components in Practice 325
</p>
<p>1 2 3 4 5 6
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>Index
</p>
<p>L
a
m
</p>
<p>b
d
</p>
<p>a
</p>
<p>Eigenvalues of S
</p>
<p>44 46 48 50 52
42
</p>
<p>44
</p>
<p>46
</p>
<p>48
</p>
<p>50
</p>
<p>PC1
</p>
<p>P
C
</p>
<p>2
</p>
<p>First vs. Second PC
</p>
<p>42 44 46 48 50
238
</p>
<p>239
</p>
<p>240
</p>
<p>241
</p>
<p>242
</p>
<p>PC2
</p>
<p>P
C
</p>
<p>3
</p>
<p>Second vs. Third PC
</p>
<p>44 46 48 50 52
238
</p>
<p>239
</p>
<p>240
</p>
<p>241
</p>
<p>242
</p>
<p>PC1
</p>
<p>P
C
</p>
<p>3
</p>
<p>First vs. Third PC
</p>
<p>Fig. 11.3 Principal components of the bank data MVApcabank
</p>
<p>an eigenvalue decomposition is performed on the covariance matrix and not on the
</p>
<p>correlation matrix (see Sect. 11.5). The following warning is therefore important:
</p>
<p>!
The PC transformation should be applied to data that have approximately
</p>
<p>the same scale in each variable.
</p>
<p>Example 11.2 Let us apply this technique to the bank data set. In this example we
</p>
<p>do not standardise the data. Figure 11.3 shows some PC plots of the bank data set.
</p>
<p>The genuine and counterfeit bank notes are marked by &ldquo;o&rdquo; and &ldquo;+&rdquo;, respectively.
</p>
<p>Recall that the mean vector of X is
</p>
<p>x D .214:9; 130:1; 129:9; 9:4; 10:6; 140:5/&gt; :
</p>
<p>The vector of eigenvalues of S is
</p>
<p>` D .2:985; 0:931; 0:242; 0:194; 0:085; 0:035/&gt; :</p>
<p/>
</div>
<div class="page"><p/>
<p>326 11 Principal Components Analysis
</p>
<p>The eigenvectors gj are given by the columns of the matrix
</p>
<p>G D
</p>
<p>0
BBBBBBB@
</p>
<p>�0:044 0:011 0:326 0:562 �0:753 0:098
0:112 0:071 0:259 0:455 0:347 �0:767
0:139 0:066 0:345 0:415 0:535 0:632
</p>
<p>0:768 �0:563 0:218 �0:186 �0:100 �0:022
0:202 0:659 0:557 �0:451 �0:102 �0:035
�0:579 �0:489 0:592 �0:258 0:085 �0:046
</p>
<p>1
CCCCCCCA
:
</p>
<p>The first column of G is the first eigenvector and gives the weights used in the linear
</p>
<p>combination of the original data in the first PC.
</p>
<p>Example 11.3 To see how sensitive the PCs are to a change in the scale of the
</p>
<p>variables, assume that X1; X2; X3 and X6 are measured in cm and that X4 and X5
remain in mm in the bank data set. This leads to:
</p>
<p>Nx D .21:49; 13:01; 12:99; 9:41; 10:65; 14:05/&gt;:
</p>
<p>The covariance matrix can be obtained from S in (3.4) by dividing rows 1, 2, 3, 6
</p>
<p>and columns 1, 2, 3, 6 by 10. We obtain:
</p>
<p>` D .2:101; 0:623; 0:005; 0:002; 0:001; 0:0004/&gt;
</p>
<p>which clearly differs from Example 11.2. Only the first two eigenvectors are given:
</p>
<p>g1 D .�0:005; 0:011; 0:014; 0:992; 0:113; �0:052/&gt;
</p>
<p>g2 D .�0:001; 0:013; 0:016; �0:117; 0:991; �0:069/&gt;:
</p>
<p>Comparing these results to the first two columns of G from Example 11.2, a
</p>
<p>completely different story is revealed. Here the first component is dominated by X4
(lower margin) and the second byX5 (upper margin), while all of the other variables
</p>
<p>have much less weight. The results are shown in Fig. 11.4. Section 11.5 will show
</p>
<p>how to select a reasonable standardisation of the variables when the scales are too
</p>
<p>different.
</p>
<p>Summary
</p>
<p>,! The scale of the variables should be roughly the same for PC
transformations.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.3 Interpretation of the PCs 327
</p>
<p>Summary (continued)
</p>
<p>,! For the practical implementation of PCA we replace� by the mean
x and &dagger; by the empirical covariance S. Then we compute the
</p>
<p>eigenvalues `1; : : : ; `p and the eigenvectors g1; : : : ; gp of S. The
</p>
<p>graphical representation of the PCs is obtained by plotting the first
</p>
<p>PC vs. the second (and eventually vs. the third).
</p>
<p>,! The components of the eigenvectors gi are the weights of the
original variables in the PCs.
</p>
<p>1 2 3 4 5 6
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>Index
</p>
<p>L
a
m
</p>
<p>b
d
</p>
<p>a
</p>
<p>Eigenvalues of S
</p>
<p>-14 -12 -10 -8 -6
</p>
<p>-10
</p>
<p>-8
</p>
<p>-6
</p>
<p>PC1
</p>
<p>P
C
</p>
<p>2
</p>
<p>First vs. Second PC
</p>
<p>-12 -10 -8 -6 -4
13.8
</p>
<p>14
</p>
<p>14.2
</p>
<p>PC2
</p>
<p>P
C
</p>
<p>3
</p>
<p>Second vs. Third PC
</p>
<p>-14 -12 -10 -8 -6
13.8
</p>
<p>14
</p>
<p>14.2
</p>
<p>PC1
</p>
<p>P
C
</p>
<p>3
</p>
<p>First vs. Third PC
</p>
<p>Fig. 11.4 Principal components of the rescaled bank data MVApcabankr
</p>
<p>11.3 Interpretation of the PCs
</p>
<p>Recall that the main idea of PC transformations is to find the most informative
</p>
<p>projections that maximise variances. The most informative SLC is given by the
</p>
<p>first eigenvector. In Sect. 11.2 the eigenvectors were calculated for the bank data.
</p>
<p>In particular, with centred x&rsquo;s, we had:
</p>
<p>y1 D �0:044x1 C 0:112x2 C 0:139x3 C 0:768x4 C 0:202x5 � 0:579x6
y2 D 0:011x1 C 0:071x2 C 0:066x3 � 0:563x4 C 0:659x5 � 0:489x6</p>
<p/>
</div>
<div class="page"><p/>
<p>328 11 Principal Components Analysis
</p>
<p>and
</p>
<p>x1 D length
x2 D left height
x3 D right height
x4 D bottom frame
x5 D top frame
x6 D diagonal:
</p>
<p>Hence, the first PC is essentially the difference between the bottom frame
</p>
<p>variable and the diagonal. The second PC is best described by the difference between
</p>
<p>the top frame variable and the sum of bottom frame and diagonal variables.
</p>
<p>The weighting of the PCs tells us in which directions, expressed in original
</p>
<p>coordinates, the best variance explanation is obtained. A measure of how well the
</p>
<p>first q PCs explain variation is given by the relative proportion:
</p>
<p> q D
</p>
<p>qX
</p>
<p>jD1
�j
</p>
<p>pX
</p>
<p>jD1
�j
</p>
<p>D
</p>
<p>qX
</p>
<p>jD1
Var.Yj /
</p>
<p>pX
</p>
<p>jD1
Var.Yj /
</p>
<p>: (11.12)
</p>
<p>Referring to the bank data Example 11.2, the (cumulative) proportions of
</p>
<p>explained variance are given in Table 11.1. The first PC .q D 1/ already explains
67% of the variation. The first three .q D 3/ PCs explain 93% of the variation.
Once again it should be noted that PCs are not scale invariant, e.g. the PCs derived
</p>
<p>from the correlation matrix give different results than the PCs derived from the
</p>
<p>covariance matrix (see Sect. 11.5).
</p>
<p>A good graphical representation of the ability of the PCs to explain the variation
</p>
<p>in the data is given by the scree plot shown in the lower right-hand window of
</p>
<p>Fig. 11.3. The screeplot can be modified by using the relative proportions on the
</p>
<p>y-axis, as is shown in Fig. 11.5 for the bank data set.
</p>
<p>Table 11.1 Proportion of
variance of PC&rsquo;s
</p>
<p>Eigenvalue Proportion of variance Cumulated proportion
</p>
<p>2:985 0:67 0:67
</p>
<p>0:931 0:21 0:88
</p>
<p>0:242 0:05 0:93
</p>
<p>0:194 0:04 0:97
</p>
<p>0:085 0:02 0:99
</p>
<p>0:035 0:01 1:00</p>
<p/>
</div>
<div class="page"><p/>
<p>11.3 Interpretation of the PCs 329
</p>
<p>Fig. 11.5 Relative
proportion of variance
explained by PCs
MVApcabanki
</p>
<p>1 2 3 4 5 6
</p>
<p>0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>Index
</p>
<p>V
a
</p>
<p>ri
a
</p>
<p>n
c
</p>
<p>e
 E
</p>
<p>x
p
</p>
<p>la
in
</p>
<p>e
d
</p>
<p>Swiss Bank Notes
</p>
<p>The covariance between the PC vector Y and the original vector X is calculated
</p>
<p>with the help of (11.4) as follows:
</p>
<p>Cov.X; Y / D E.XY &gt;/ � EX EY &gt; D E.XY &gt;/
D E.XX&gt;&#128;/ � ��&gt;&#128; D Var.X/&#128;
D &dagger;&#128; (11.13)
D &#128;ƒ&#128;&gt;&#128;
D &#128;ƒ:
</p>
<p>Hence, the correlation, �XiYj , between variable Xi and the PC Yj is
</p>
<p>�XiYj D
&#13;ij�j
</p>
<p>.�XiXi�j /
1=2
D &#13;ij
</p>
<p>�
�j
</p>
<p>�XiXi
</p>
<p>�1=2
: (11.14)
</p>
<p>Using actual data, this of course translates into
</p>
<p>rXiYj D gij
�
`j
</p>
<p>sXiXi
</p>
<p>�1=2
: (11.15)
</p>
<p>The correlations can be used to evaluate the relations between the PCs Yj where
</p>
<p>j D 1; : : : ; q, and the original variables Xi where i D 1; : : : ; p. Note that
</p>
<p>pX
</p>
<p>jD1
r2XiYj D
</p>
<p>Pp
jD1 `jg
</p>
<p>2
ij
</p>
<p>sXiXi
D sXiXi
sXiXi
</p>
<p>D 1: (11.16)</p>
<p/>
</div>
<div class="page"><p/>
<p>330 11 Principal Components Analysis
</p>
<p>Fig. 11.6 The correlation of
the original variable with the
PCs MVApcabanki
</p>
<p>&minus;1.0 &minus;0.5 0.0 0.5 1.0
</p>
<p>&minus;
1
.0
</p>
<p>&minus;
0
.5
</p>
<p>0
.0
</p>
<p>0
.5
</p>
<p>1
.0
</p>
<p>Swiss Bank Notes
</p>
<p>First PC
</p>
<p>S
e
</p>
<p>c
o
</p>
<p>n
d
</p>
<p> P
C
</p>
<p>X1
</p>
<p>X2X3
</p>
<p>X4
</p>
<p>X5
</p>
<p>X6
</p>
<p>Indeed,
Pp
</p>
<p>jD1 `jg
2
ij D g&gt;i Lgi is the .i; i/-element of the matrix GLG&gt; D S, so
</p>
<p>that r2XiYj may be seen as the proportion of variance of Xi explained by Yj .
</p>
<p>In the space of the first two PCs we plot these proportions, i.e. rXiY1 versus rXiY2 .
</p>
<p>Figure 11.6 shows this for the bank notes example. This plot shows which of the
</p>
<p>original variables are most strongly correlated with PC Y1 and Y2.
</p>
<p>From (11.16) it obviously follows that r2XiY1 C r
2
XiY2
� 1 so that the points are
</p>
<p>always inside the circle of radius 1. In the bank notes example, the variablesX4, X5
and X6 correspond to correlations near the periphery of the circle and are thus well
</p>
<p>explained by the first two PCs. Recall that we have interpreted the first PC as being
</p>
<p>essentially the difference betweenX4 andX6. This is also reflected in Fig. 11.6 since
</p>
<p>the points corresponding to these variables lie on different sides of the vertical axis.
</p>
<p>An analogous remark applies to the second PC. We had seen that the second PC is
</p>
<p>well described by the difference between X5 and the sum of X4 and X6. Now we
</p>
<p>are able to see this result again from Fig. 11.6 since the point corresponding to X5
lies above the horizontal axis and the points corresponding to X4 and X6 lie below.
</p>
<p>The correlations of the original variables Xi and the first two PCs are given
</p>
<p>in Table 11.2 along with the cumulated percentage of variance of each variable
</p>
<p>explained by Y1 and Y2. This table confirms the above results. In particular, it
</p>
<p>confirms that the percentage of variance of X1 (and X2; X3) explained by the first
</p>
<p>two PCs is relatively small and so are their weights in the graphical representation
</p>
<p>of the individual bank notes in the space of the first two PCs (as can be seen in
</p>
<p>the upper left plot in Fig. 11.3). Looking simultaneously at Fig. 11.6 and the upper
</p>
<p>left plot of Fig. 11.3 shows that the genuine bank notes are roughly characterised by
</p>
<p>large values of X6 and smaller values of X4. The counterfeit bank notes show larger
</p>
<p>values of X5 (see Example 7.15).</p>
<p/>
</div>
<div class="page"><p/>
<p>11.4 Asymptotic Properties of the PCs 331
</p>
<p>Table 11.2 Correlation
between the original variables
and the PCs
</p>
<p>rXiY1 rXiY2 r
2
XiY1
C r2XiY2
</p>
<p>X1 length �0:201 0:028 0:041
X2 left h. 0:538 0:191 0:326
</p>
<p>X3 right h. 0:597 0:159 0:381
</p>
<p>X4 lower 0:921 �0:377 0:991
X5 upper 0:435 0:794 0:820
</p>
<p>X6 diagonal �0:870 �0:410 0:926
</p>
<p>Summary
</p>
<p>,! The weighting of the PCs tells us in which directions, expressed
in original coordinates, the best explanation of the variance is
</p>
<p>obtained. Note that the PCs are not scale invariant.
,! A measure of how well the first q PCs explain variation is given
</p>
<p>by the relative proportion  q D
Pq
</p>
<p>jD1 �j =
Pp
</p>
<p>jD1 �j . A good
graphical representation of the ability of the PCs to explain the
</p>
<p>variation in the data is the scree plot of these proportions.
</p>
<p>,! The correlation between PC Yj and an original variable Xi is
�XiYj D &#13;ij
</p>
<p>�
�j
</p>
<p>�Xi Xi
</p>
<p>�1=2
. For a data matrix this translates into
</p>
<p>r2XiYj D
`j g
</p>
<p>2
ij
</p>
<p>sXi Xi
. r2XiYj can be interpreted as the proportion of variance
</p>
<p>of Xi explained by Yj . A plot of rXiY1 vs. rXiY2 shows which of
</p>
<p>the original variables are most strongly correlated with the PCs,
</p>
<p>namely those that are close to the periphery of the circle of radius 1.
</p>
<p>11.4 Asymptotic Properties of the PCs
</p>
<p>In practice, PCs are computed from sample data. The following theorem yields
</p>
<p>results on the asymptotic distribution of the sample PCs.
</p>
<p>Theorem 11.4 Let &dagger; &gt; 0 with distinct eigenvalues, and let U � m�1Wp.&dagger;;m/
with spectral decompositions&dagger; D &#128;ƒ&#128;&gt;, and U D GLG&gt;. Then
</p>
<p>(a)
p
m.` � �/ L�! Np.0; 2ƒ2/,
</p>
<p>where ` D .`1; : : : ; `p/&gt; and � D .�1; : : : ; �p/&gt; are the diagonals of L and
ƒ,
</p>
<p>(b)
p
m.gj � &#13;j /
</p>
<p>L�! Np.0;Vj /,
with Vj D �j
</p>
<p>X
</p>
<p>k&curren;j
</p>
<p>�k
</p>
<p>.�k � �j /2
&#13;k&#13;
&gt;
k ,</p>
<p/>
</div>
<div class="page"><p/>
<p>332 11 Principal Components Analysis
</p>
<p>(c) Cov.gj ; gk/ D Vjk,
where the .r; s/-element of the matrix Vjk.p � p/ is �
</p>
<p>�j�k&#13;rk&#13;sj
</p>
<p>m.�j � �k/2
,
</p>
<p>(d) the elements in ` are asymptotically independent of the elements in G.
</p>
<p>Example 11.4 Since nS � Wp.&dagger;; n � 1/ if X1; : : : ; Xn are drawn from N.�;&dagger;/,
we have that
</p>
<p>p
n � 1.`j � �j /
</p>
<p>L�! N.0; 2�2j /; j D 1; : : : ; p: (11.17)
</p>
<p>Since the variance of (11.17) depends on the true mean �j a log transformation
</p>
<p>is useful. Consider f .`j / D log.`j /. Then dd`j f j`jD�j D
1
�j
</p>
<p>and by the
</p>
<p>Transformation Theorem 4.11 we have from (11.17) that
</p>
<p>p
n � 1.log `j � log�j /
</p>
<p>L�! N.0; 2/: (11.18)
</p>
<p>Hence,
</p>
<p>r
n � 1
2
</p>
<p>�
log `j � log�j
</p>
<p>� L�! N.0; 1/
</p>
<p>and a two-sided confidence interval at the 1 � ˛ D 0:95 significance level is given
by
</p>
<p>log.`j /� 1:96
r
</p>
<p>2
</p>
<p>n � 1 � log�j � log.`j /C 1:96
r
</p>
<p>2
</p>
<p>n� 1 :
</p>
<p>In the bank data example we have that
</p>
<p>`1 D 2:98:
</p>
<p>Therefore,
</p>
<p>log.2:98/˙ 1:96
r
</p>
<p>2
</p>
<p>199
D log.2:98/˙ 0:1965:
</p>
<p>It can be concluded for the true eigenvalue that
</p>
<p>P f�1 2 .2:448; 3:62/g � 0:95:</p>
<p/>
</div>
<div class="page"><p/>
<p>11.4 Asymptotic Properties of the PCs 333
</p>
<p>Variance Explained by the First q PCs
</p>
<p>The variance explained by the first q PCs is given by
</p>
<p> D �1 C � � � C �q
pP
jD1
</p>
<p>�j
</p>
<p>�
</p>
<p>In practice this is estimated by
</p>
<p>O D `1 C � � � C `q
pP
jD1
</p>
<p>`j
</p>
<p>�
</p>
<p>From Theorem 11.4 we know the distribution of
p
n � 1.` � �/. Since  is a non-
</p>
<p>linear function of �, we can again apply the Transformation Theorem 4.11 to obtain
</p>
<p>that
</p>
<p>p
n � 1. O �  / L�! N.0;D&gt;VD/
</p>
<p>where V D 2ƒ2 (from Theorem 11.4) and D D .d1; : : : ; dp/&gt; with
</p>
<p>dj D
@ 
</p>
<p>@�j
D
</p>
<p>8
&lt;̂
</p>
<p>:̂
</p>
<p>1 �  
tr.&dagger;/
</p>
<p>for 1 � j � q;
� 
tr.&dagger;/
</p>
<p>for q C 1 � j � p:
</p>
<p>Given this result, the following theorem can be derived.
</p>
<p>Theorem 11.5
</p>
<p>p
n � 1. O �  / L�! N.0; !2/;
</p>
<p>where
</p>
<p>!2 D D&gt;VD D 2ftr.&dagger;/g2
n
.1 �  /2.�21 C � � � C �2q/C  2.�2qC1 C � � � C �2p/
</p>
<p>o
</p>
<p>D 2 tr.&dagger;
2/
</p>
<p>ftr.&dagger;/g2 . 
2 � 2ˇ C ˇ/
</p>
<p>and
</p>
<p>ˇ D
�21 C � � � C �2q
�21 C � � � C �2p
</p>
<p>:</p>
<p/>
</div>
<div class="page"><p/>
<p>334 11 Principal Components Analysis
</p>
<p>Example 11.5 From Sect. 11.3 it is known that the first PC for the Swiss bank notes
</p>
<p>resolves 67% of the variation. It can be tested whether the true proportion is actually
</p>
<p>75%. Computing
</p>
<p>Ǒ D `
2
1
</p>
<p>`21 C � � � C `2p
D .2:985/
</p>
<p>2
</p>
<p>.2:985/2 C .0:931/2 C � � � .0:035/2 D 0:902
</p>
<p>tr.S/ D 4:472
</p>
<p>tr.S2/ D
pX
</p>
<p>jD1
`2j D 9:883
</p>
<p>O!2 D 2 tr.S
2/
</p>
<p>ftr.S/g2 .
O 2 � 2 Ǒ O C Ǒ/
</p>
<p>D 2 � 9:883
.4:472/2
</p>
<p>f.0:668/2 � 2.0:902/.0:668/C 0:902g D 0:142:
</p>
<p>Hence, a confidence interval at a significance of level 1 � ˛ D 0.95 is given by
</p>
<p>0:668˙ 1:96
r
0:142
</p>
<p>199
D .0:615; 0:720/:
</p>
<p>Clearly the hypothesis that  D 75% can be rejected!
</p>
<p>Summary
</p>
<p>,! The eigenvalues `j and eigenvectors gj are asymptotically, nor-
mally distributed, in particular
</p>
<p>p
n � 1.` � �/ L�! Np.0; 2ƒ2/.
</p>
<p>,! For the eigenvalues it holds that
q
</p>
<p>n�1
2
</p>
<p>�
log `j � log�j
</p>
<p>� L�!
N.0; 1/.
</p>
<p>,! Given an asymptotic, normal distribution approximate confidence
intervals and tests can be constructed for the proportion of variance
</p>
<p>which is explained by the first q PCs. The two-sided confidence
</p>
<p>interval at the 1�˛ D 0:95 level is given by log.`j /�1:96
q
</p>
<p>2
n�1 �
</p>
<p>log�j � log.`j /C 1:96
q
</p>
<p>2
n�1 :</p>
<p/>
</div>
<div class="page"><p/>
<p>11.5 Normalised Principal Components Analysis 335
</p>
<p>Summary (continued)
</p>
<p>,! It holds for O , the estimate of  (the proportion of the variance
explained by the first q PCs) that
</p>
<p>p
n � 1. O �  / L�! N.0; !2/,
</p>
<p>where ! is given in Theorem 11.5.
</p>
<p>11.5 Normalised Principal Components Analysis
</p>
<p>In certain situations the original variables can be heterogeneous w.r.t. their vari-
</p>
<p>ances. This is particularly true when the variables are measured on heterogeneous
</p>
<p>scales (such as years, kilograms, dollars, . . . ). In this case a description of the
</p>
<p>information contained in the data needs to be provided which is robust w.r.t. the
</p>
<p>choice of scale. This can be achieved through a standardisation of the variables,
</p>
<p>namely
</p>
<p>XS D HXD�1=2 (11.19)
</p>
<p>whereD D diag.sX1X1 ; : : : ; sXpXp /. Note that xS D 0 and SXS D R, the correlation
matrix of X . The PC transformations of the matrix XS are referred to as the
</p>
<p>Normalised Principal Components (NPCs). The spectral decomposition ofR is
</p>
<p>R D GRLRG&gt;R; (11.20)
</p>
<p>where LR D diag.`R1 ; : : : ; `Rp / and `R1 � � � � � `Rp are the eigenvalues of R with
corresponding eigenvectors gR1 ; : : : ; g
</p>
<p>R
p (note that here
</p>
<p>Pp
jD1 `
</p>
<p>R
j D tr.R/ D p).
</p>
<p>The NPCs, Zj , provide a representation of each individual, and is given by
</p>
<p>Z D XSGR D .z1; : : : ; zp/: (11.21)
</p>
<p>After transforming the variables, once again, we have that
</p>
<p>z D 0; (11.22)
SZ D G&gt;RSXSGR D G&gt;RRGR D LR: (11.23)
</p>
<p>!
The NPCs provide a perspective similar to that of the PCs, but in terms of
</p>
<p>the relative position of individuals, NPC gives each variable the same weight (with
</p>
<p>the PCs the variable with the largest variance received the largest weight).</p>
<p/>
</div>
<div class="page"><p/>
<p>336 11 Principal Components Analysis
</p>
<p>Computing the covariance and correlation betweenXi andZj is straightforward:
</p>
<p>SXS ;Z D
1
</p>
<p>n
X&gt;S Z D GRLR; (11.24)
</p>
<p>RXS ;Z D GRLRL
�1=2
R D GRL
</p>
<p>1=2
R : (11.25)
</p>
<p>The correlations between the original variablesXi and the NPCs Zj are:
</p>
<p>rXiZj D
q
`jgR;ij (11.26)
</p>
<p>pX
</p>
<p>jD1
r2XiZj D 1 (11.27)
</p>
<p>(compare this to (11.15) and (11.16)). The resulting NPCs, the Zj , can be
</p>
<p>interpreted in terms of the original variables and the role of each PC in explaining
</p>
<p>the variation in variable Xi can be evaluated.
</p>
<p>11.6 Principal Components as a Factorial Method
</p>
<p>The empirical PCs (normalised or not) turn out to be equivalent to the factors that
</p>
<p>one would obtain by decomposing the appropriate data matrix into its factors (see
</p>
<p>Chap. 10). It will be shown that the PCs are the factors representing the rows
</p>
<p>of the centred data matrix and that the NPCs correspond to the factors of the
</p>
<p>standardised data matrix. The representation of the columns of the standardised
</p>
<p>data matrix provides (at a scale factor) the correlations between the NPCs and the
</p>
<p>original variables. The derivation of the (N)PCs presented above will have a nice
</p>
<p>geometric justification here since they are the best fit in subspaces generated by the
</p>
<p>columns of the (transformed) data matrix X . This analogy provides complementary
</p>
<p>interpretations of the graphical representations shown above.
</p>
<p>Assume, as in Chap. 10, that we want to obtain representations of the individuals
</p>
<p>(the rows of X ) and of the variables (the columns of X ) in spaces of smaller
</p>
<p>dimension. To keep the representations simple, some prior transformations are
</p>
<p>performed. Since the origin has no particular statistical meaning in the space of
</p>
<p>individuals, we will first shift the origin to the centre of gravity, x, of the point
</p>
<p>cloud. This is the same as analysing the centred data matrix XC D HX . Now all of
the variables have zero means, thus the technique used in Chap. 10 can be applied
</p>
<p>to the matrix XC . Note that the spectral decomposition of X
&gt;
C XC is related to that
</p>
<p>of SX , namely
</p>
<p>X&gt;C XC D X&gt;H&gt;HX D nSX D nGLG&gt;: (11.28)</p>
<p/>
</div>
<div class="page"><p/>
<p>11.6 Principal Components as a Factorial Method 337
</p>
<p>The factorial variables are obtained by projecting XC on G,
</p>
<p>Y D XCG D .y1; : : : ; yp/: (11.29)
</p>
<p>These are the same principal components obtained above, see formula (11.10).
</p>
<p>(Note that the y&rsquo;s here correspond to the z&rsquo;s in Sect. 10.2.) Since HXC D XC , it
immediately follows that
</p>
<p>y D 0; (11.30)
SY D G&gt;SXG D L D diag.`1; : : : ; `p/: (11.31)
</p>
<p>The scatterplot of the individuals on the factorial axes are thus centred around the
</p>
<p>origin and are more spread out in the first direction (first PC has variance `1) than
</p>
<p>in the second direction (second PC has variance `2).
</p>
<p>The representation of the variables can be obtained using the Duality Rela-
</p>
<p>tions (10.11), and (10.12). The projections of the columns of XC onto the eigen-
</p>
<p>vectors vk of XCX
&gt;
C are
</p>
<p>X&gt;C vk D
1p
n`k
</p>
<p>X&gt;C XCgk D
p
n`kgk : (11.32)
</p>
<p>Thus the projections of the variables on the first p axes are the columns of the matrix
</p>
<p>X&gt;C V D
p
nGL1=2: (11.33)
</p>
<p>Considering the geometric representation, there is a nice statistical interpretation of
</p>
<p>the angle between two columns of XC . Given that
</p>
<p>x&gt;CŒj &#141;xCŒk&#141; D nsXjXk ; (11.34)
</p>
<p>jjxCŒj &#141;jj2 D nsXjXj ; (11.35)
</p>
<p>where xCŒj &#141; and xCŒk&#141; denote the j -th and k-th column of XC , it holds that in the
</p>
<p>full space of the variables, if �jk is the angle between two variables, xCŒj &#141; and xCŒk&#141;,
</p>
<p>then
</p>
<p>cos �jk D
x&gt;CŒj &#141;xCŒk&#141;
</p>
<p>kxCŒj &#141;k kxCŒk&#141;k
D rXjXk : (11.36)
</p>
<p>(Example 2.11 shows the general connection that exists between the angle and
</p>
<p>correlation of two variables). As a result, the relative positions of the variables in
</p>
<p>the scatterplot of the first columns of X&gt;C V may be interpreted in terms of their
correlations; the plot provides a picture of the correlation structure of the original
</p>
<p>data set. Clearly, one should take into account the percentage of variance explained
</p>
<p>by the chosen axes when evaluating the correlation.</p>
<p/>
</div>
<div class="page"><p/>
<p>338 11 Principal Components Analysis
</p>
<p>The NPCs can also be viewed as a factorial method for reducing the dimension.
</p>
<p>The variables are again standardised so that each one has mean zero and unit
</p>
<p>variance and is independent of the scale of the variables. The factorial analysis of
</p>
<p>XS provides the NPCs. The spectral decomposition of X
&gt;
S XS is related to that of
</p>
<p>R, namely
</p>
<p>X&gt;S XS D D�1=2X&gt;HXD�1=2 D nR D nGRLRG&gt;R:
</p>
<p>The NPCs Zj , given by (11.21), may be viewed as the projections of the rows of
</p>
<p>XS onto GR.
</p>
<p>The representation of the variables are again given by the columns of
</p>
<p>X&gt;S VR D
p
nGRL
</p>
<p>1=2
R : (11.37)
</p>
<p>Comparing (11.37) and (11.25) we see that the projections of the variables in the
</p>
<p>factorial analysis provide the correlation between the NPCs Zk and the original
</p>
<p>variables xŒj &#141; (up to the factor
p
n which could be the scale of the axes).
</p>
<p>This implies that a deeper interpretation of the representation of the individuals
</p>
<p>can be obtained by looking simultaneously at the graphs plotting the variables. Note
</p>
<p>that
</p>
<p>x&gt;SŒj &#141;xSŒk&#141; D nrXjXk ; (11.38)
</p>
<p>kxSŒj &#141;k2 D n; (11.39)
</p>
<p>where xSŒj &#141; and xSŒk&#141; denote the j -th and k-th column of XS . Hence, in the full
</p>
<p>space, all the standardised variables (columns of XS ) are contained within the
</p>
<p>&ldquo;sphere&rdquo; in Rn, which is centred at the origin and has radius
p
n (the scale of the
</p>
<p>graph). As in (11.36), given the angle �jk between two columns xSŒj &#141; and xSŒk&#141;, it
</p>
<p>holds that
</p>
<p>cos �jk D rXjXk : (11.40)
</p>
<p>Therefore, when looking at the representation of the variables in the spaces of
</p>
<p>reduced dimension (for instance the first two factors), we have a picture of the
</p>
<p>correlation structure between the original Xi &rsquo;s in terms of their angles. Of course,
</p>
<p>the quality of the representation in those subspaces has to be taken into account,
</p>
<p>which is presented in the next section.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.6 Principal Components as a Factorial Method 339
</p>
<p>Quality of the Representations
</p>
<p>As said before, an overall measure of the quality of the representation is given by
</p>
<p> D `1 C `2 C � � � C `q
pP
jD1
</p>
<p>`j
</p>
<p>:
</p>
<p>In practice, q is chosen to be equal to 1, 2 or 3. Suppose for instance that  D 0:93
for q D 2. This means that the graphical representation in two dimensions captures
93% of the total variance. In other words, there is minimal dispersion in a third
</p>
<p>direction (no more than 7%).
</p>
<p>It can be useful to check if each individual is well represented by the PCs. Clearly,
</p>
<p>the proximity of two individuals on the projected space may not necessarily coincide
</p>
<p>with the proximity in the full original space Rp, which may lead to erroneous
</p>
<p>interpretations of the graphs. In this respect, it is worth computing the angle #ik
between the representation of an individual i and the k-th PC or NPC axis. This can
</p>
<p>be done using (2.40), i.e.
</p>
<p>cos#ik D
y&gt;i ek
kyikkekk
</p>
<p>D yikkxCik
</p>
<p>for the PCs or analogously
</p>
<p>cos �ik D
z&gt;i ek
kzikkekk
</p>
<p>D zikkxSik
</p>
<p>for the NPCs, where ek denotes the k-th unit vector ek D .0; : : : ; 1; : : : ; 0/&gt;. An
individual i will be represented on the k-th PC axis if its corresponding angle is
</p>
<p>small, i.e. if cos2 #ik for k D 1; : : : ; p is close to one. Note that for each individual i ,
</p>
<p>pX
</p>
<p>kD1
cos2 #ik D
</p>
<p>y&gt;i yi
x&gt;CixCi
</p>
<p>D x
&gt;
CiGG
</p>
<p>&gt;xCi
x&gt;CixCi
</p>
<p>D 1:
</p>
<p>The values cos2 #ik are sometimes called the relative contributions of the k-th axis
</p>
<p>to the representation of the i -th individual, e.g. if cos2 #i1 C cos2 #i2 is large (near
one), we know that the individual i is well represented on the plane of the first two
</p>
<p>principal axes since its corresponding angle with the plane is close to zero.
</p>
<p>We already know that the quality of the representation of the variables can be
</p>
<p>evaluated by the percentage of Xi &rsquo;s variance that is explained by a PC, which is
</p>
<p>given by r2XiYj or r
2
XiZj
</p>
<p>according to (11.16) and (11.27) respectively.</p>
<p/>
</div>
<div class="page"><p/>
<p>340 11 Principal Components Analysis
</p>
<p>-1.5 -1 -0.5 0 0.5 1
-0.6
</p>
<p>-0.4
</p>
<p>-0.2
</p>
<p>0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
French Food Data
</p>
<p>First Factor - Families
</p>
<p>S
e
</p>
<p>c
o
</p>
<p>n
d
</p>
<p> F
a
</p>
<p>c
to
</p>
<p>r 
- 
</p>
<p>F
a
</p>
<p>m
ili
</p>
<p>e
s
</p>
<p>MA2
</p>
<p>EM2
</p>
<p>CA2
</p>
<p>MA3
</p>
<p>EM3
</p>
<p>CA3
</p>
<p>MA4
</p>
<p>EM4
</p>
<p>CA4
</p>
<p>MA5
</p>
<p>EM5
</p>
<p>CA5
</p>
<p>Fig. 11.7 Representation of the individuals MVAnpcafood
</p>
<p>Example 11.6 Let us return to the French food expenditure example, see Sect. 22.6.
</p>
<p>This yields a two-dimensional representation of the individuals as shown in
</p>
<p>Fig. 11.7.
</p>
<p>Calculating the matrix GR we have
</p>
<p>GR D
</p>
<p>0
BBBBBBBBB@
</p>
<p>�0:240 0:622 �0:011 �0:544 0:036 0:508
�0:466 0:098 �0:062 �0:023 �0:809 �0:301
�0:446 �0:205 0:145 0:548 �0:067 0:625
�0:462 �0:141 0:207 �0:053 0:411 �0:093
�0:438 �0:197 0:356 �0:324 0:224 �0:350
�0:281 0:523 �0:444 0:450 0:341 �0:332
0:206 0:479 0:780 0:306 �0:069 �0:138
</p>
<p>1
CCCCCCCCCA
</p>
<p>;
</p>
<p>which gives the weights of the variables (milk, vegetables, etc.). The eigenvalues `j
and the proportions of explained variance are given in Table 11.3.
</p>
<p>The interpretation of the principal components are best understood when looking
</p>
<p>at the correlations between the original Xi &rsquo;s and the PCs. Since the first two PCs
</p>
<p>explain 88.1% of the variance, we limit ourselves to the first two PCs. The results
</p>
<p>are shown in Table 11.4. The two-dimensional graphical representation of the
</p>
<p>variables in Fig. 11.8 is based on the first two columns of Table 11.4.
</p>
<p>The plots are the projections of the variables into R2. Since the quality of the
</p>
<p>representation is good for all the variables (except maybe X7), their relative angles
</p>
<p>give a picture of their original correlation: wine is negatively correlated with the</p>
<p/>
</div>
<div class="page"><p/>
<p>11.6 Principal Components as a Factorial Method 341
</p>
<p>Table 11.3 Eigenvalues and
explained variance
</p>
<p>Eigenvalues Proportion of variance Cumulated proportion
</p>
<p>4:333 0:6190 61:9
</p>
<p>1:830 0:2620 88:1
</p>
<p>0:631 0:0900 97:1
</p>
<p>0:128 0:0180 98:9
</p>
<p>0:058 0:0080 99:7
</p>
<p>0:019 0:0030 99:9
</p>
<p>0:001 0:0001 100:0
</p>
<p>Table 11.4 Correlations
with PCs
</p>
<p>rXiZ1 rXiZ2 r
2
XiZ1
C r2XiZ2
</p>
<p>X1: bread �0:499 0:842 0:957
X2: vegetables �0:970 0:133 0:958
X3: fruits �0:929 �0:278 0:941
X4: meat �0:962 �0:191 0:962
X5: poultry �0:911 �0:266 0:901
X6: milk �0:584 0:707 0:841
X7: wine 0:428 0:648 0:604
</p>
<p>Fig. 11.8 Representation of
the variables
MVAnpcafood
</p>
<p>&minus;1 &minus;0.5 0 0.5 1
</p>
<p>&minus;1
</p>
<p>&minus;0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>French Food Data
</p>
<p>First Factor &minus; Goods
</p>
<p>S
e
c
o
n
d
</p>
<p> F
a
</p>
<p>c
to
</p>
<p>r 
&minus;
</p>
<p> G
o
</p>
<p>o
d
</p>
<p>s
</p>
<p>bread
</p>
<p>vegetables
</p>
<p>fruit
meat
</p>
<p>poultry
</p>
<p>milk
wine
</p>
<p>vegetables, fruits, meat and poultry groups (� &gt; 90ı), whereas taken individually
this latter grouping of variables are highly positively correlatedwith each other (� �
0). Bread and milk are positively correlated but poorly correlated with meat, fruits
</p>
<p>and poultry (� � 90ı).
Now the representation of the individuals in Fig. 11.7 can be interpreted
</p>
<p>better. From Fig. 11.8 and Table 11.4 we can see that the first factor Z1 is a
</p>
<p>vegetable&ndash;meat&ndash;poultry&ndash;fruit factor (with a negative sign), whereas the second</p>
<p/>
</div>
<div class="page"><p/>
<p>342 11 Principal Components Analysis
</p>
<p>factor is a milk&ndash;bread&ndash;wine factor (with a positive sign). Note that this corresponds
</p>
<p>to the most important weights in the first columns of GR. In Fig. 11.7 lines were
</p>
<p>drawn to connect families of the same size and families of the same professional
</p>
<p>types. A grid can clearly be seen (with a slight deformation by the manager families)
</p>
<p>that shows the families with higher expenditures (higher number of children) on
</p>
<p>the left.
</p>
<p>Considering both figures together explainswhat types of expenditures are respon-
</p>
<p>sible for similarities in food expenditures. Bread, milk and wine expenditures are
</p>
<p>similar for manual workers and employees. Families of managers are characterised
</p>
<p>by higher expenditures on vegetables, fruits, meat and poultry. Very often when
</p>
<p>analysing NPCs (and PCs), it is illuminating to use such a device to introduce
</p>
<p>qualitative aspects of individuals in order to enrich the interpretations of the graphs.
</p>
<p>Summary
</p>
<p>,! NPCs are PCs applied to the standardised (normalised) data matrix
XS .
</p>
<p>,! The graphical representation of NPCs provides a similar type of
picture as that of PCs, the difference being in the relative position
</p>
<p>of individuals, i.e. each variable in NPCs has the same weight (in
</p>
<p>PCs, the variable with the largest variance has the largest weight).
</p>
<p>,! The quality of the representation is evaluated by  D
.
Pp
</p>
<p>jD1 `j /
�1.`1 C `2 C � � � C `q/:
</p>
<p>,! The quality of the representation of a variable can be evaluated by
the percentage ofXi &rsquo;s variance that is explained by a PC, i.e. r
</p>
<p>2
XiYj
</p>
<p>.
</p>
<p>11.7 Common Principal Components
</p>
<p>In many applications a statistical analysis is simultaneously done for groups of data.
</p>
<p>In this section a technique is presented that allows us to analyse group elements that
</p>
<p>have common PCs. From a statistical point of view, estimating PCs simultaneously
</p>
<p>in different groups will result in a joint dimension reducing transformation. This
</p>
<p>multi-group PCA, the so-called common principle components analysis (CPCA),
</p>
<p>yields the joint eigenstructure across groups.
</p>
<p>In addition to traditional PCA, the basic assumption of CPCA is that the space
</p>
<p>spanned by the eigenvectors is identical across several groups, whereas variances
</p>
<p>associated with the components are allowed to vary.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.7 Common Principal Components 343
</p>
<p>More formally, the hypothesis of common principle components can be stated in
</p>
<p>the following way (Flury, 1988):
</p>
<p>HCPC W &dagger;i D &#128;ƒi&#128;&gt;; i D 1; : : : ; k
</p>
<p>where &dagger;i is a positive definite p � p population covariance matrix for every
i , &#128; D .&#13;1; : : : ; &#13;p/ is an orthogonal p � p transformation matrix and ƒi D
diag
</p>
<p>�
�i1; : : : ; �ip
</p>
<p>�
is the matrix of eigenvalues. Moreover, assume that all �i are
</p>
<p>distinct.
</p>
<p>Let S be the (unbiased) sample covariance matrix of an underlying p-variate
</p>
<p>normal distribution Np.�;&dagger;/ with sample size n. Then the distribution of nS has
</p>
<p>n�1 degrees of freedom and is known as the Wishart distribution (Muirhead, 1982,
p. 86):
</p>
<p>nS �Wp.&dagger;; n � 1/:
</p>
<p>The density is given in (5.16). Hence, for a given Wishart matrix Si with sample
</p>
<p>size ni , the likelihood function can be written as
</p>
<p>L.&dagger;1; : : : ; &dagger;k/ D C
kY
</p>
<p>iD1
exp
</p>
<p>h
tr
</p>
<p>�
�1
2
.ni � 1/&dagger;�1i Si
</p>
<p>�i
j&dagger;i j�
</p>
<p>1
2 .ni�1/ (11.41)
</p>
<p>where C is a constant independent of the parameters&dagger;i . Maximising the likelihood
</p>
<p>is equivalent to minimising the function
</p>
<p>g.&dagger;1; : : : ; &dagger;k/ D
kX
</p>
<p>iD1
.ni � 1/
</p>
<p>n
log j&dagger;i j C tr.&dagger;�1i Si /
</p>
<p>o
:
</p>
<p>Assuming that HCPC holds, i.e. in replacing &dagger;i by &#128;ƒi&#128;
&gt;, after some manipu-
</p>
<p>lations one obtains
</p>
<p>g.&#128;;ƒ1; : : : ; ƒk/ D
kX
</p>
<p>iD1
.ni � 1/
</p>
<p>pX
</p>
<p>jD1
</p>
<p> 
log�ij C
</p>
<p>&#13;&gt;j Si&#13;j
</p>
<p>�ij
</p>
<p>!
:
</p>
<p>As we know from Sect. 2.2, the vectors &#13;j in &#128; have to be orthogonal.
</p>
<p>Orthogonality of the vectors &#13;j is achieved using the Lagrange method, i.e. we
</p>
<p>impose the p constraints &#13;&gt;j &#13;j D 1 using the Lagrange multipliers �j ; and the
remaining p.p � 1/=2 constraints &#13;&gt;h &#13;j D 0 for h &curren; j using the multiplier 2�hj
(Flury, 1988). This yields
</p>
<p>g�.&#128;;ƒ1; : : : ; ƒk/ D g.�/�
pX
</p>
<p>jD1
�j .&#13;
</p>
<p>&gt;
j &#13;j � 1/� 2
</p>
<p>pX
</p>
<p>hD1
</p>
<p>pX
</p>
<p>jDhC1
�hj&#13;
</p>
<p>&gt;
h &#13;j :</p>
<p/>
</div>
<div class="page"><p/>
<p>344 11 Principal Components Analysis
</p>
<p>Taking partial derivatives with respect to all �im and &#13;m, it can be shown that the
</p>
<p>solution of the CPC model is given by the generalised system of characteristic
</p>
<p>equations
</p>
<p>&#13;&gt;m
</p>
<p>(
kX
</p>
<p>iD1
.ni � 1/
</p>
<p>�im � �ij
�im�ij
</p>
<p>Si
</p>
<p>)
&#13;j D 0; m; j D 1; : : : ; p; m &curren; j:
</p>
<p>(11.42)
</p>
<p>This system can be solved using
</p>
<p>�im D &#13;&gt;m Si&#13;m; i D 1; : : : ; k; m D 1; : : : ; p
</p>
<p>under the constraints
</p>
<p>&#13;&gt;m &#13;j D
(
0 m &curren; j
1 m D j
</p>
<p>:
</p>
<p>Flury (1988) proves existence and uniqueness of the maximum of the likelihood
</p>
<p>function, and Flury and Gautschi (1986) provide a numerical algorithm.
</p>
<p>Example 11.7 As an example we provide the data sets XFGvolsurf01,
</p>
<p>XFGvolsurf02 and XFGvolsurf03 that have been used in Fengler, H&auml;rdle, and
</p>
<p>Villa (2003) to estimate common principle components for the implied volatility
</p>
<p>surfaces of the DAX 1999. The data has been generated by smoothing an implied
</p>
<p>volatility surface day by day. Next, the estimated grid points have been grouped into
</p>
<p>maturities of � D 1, � D 2 and � D 3 months and transformed into a vector of time
series of the &ldquo;smile&rdquo;, i.e. each element of the vector belongs to a distinct moneyness
</p>
<p>ranging from 0.85 to 1.10.
</p>
<p>Figure 11.9 shows the first three eigenvectors in a parallel coordinate plot. The
</p>
<p>basic structure of the first three eigenvectors is not altered. We find a shift, a slope
</p>
<p>and a twist structure. This structure is common to all maturity groups, i.e. when
</p>
<p>exploiting PCA as a dimension reducing tool, the same transformation applies to
</p>
<p>each group! However, by comparing the size of eigenvalues among groups we find
</p>
<p>that variability is decreasing across groups as we move from the short-term contracts
</p>
<p>to long-term contracts.
</p>
<p>Before drawing conclusions we should convince ourselves that the CPC model
</p>
<p>is truly a good description of the data. This can be done by using a likelihood ratio
</p>
<p>test. The likelihood ratio statistic for comparing a restricted (the CPC) model against
</p>
<p>the unrestricted model (the model where all covariances are treated separately) is
</p>
<p>given by
</p>
<p>T.n1;n2;:::;nk / D �2 log
L. O&dagger;1; : : : ; O&dagger;k/
L.S1; : : : ;Sk/
</p>
<p>:</p>
<p/>
</div>
<div class="page"><p/>
<p>11.7 Common Principal Components 345
</p>
<p>Fig. 11.9 Factor loadings of
the first (thick), the second
(medium), and the third (thin)
PC MVAcpcaiv
</p>
<p>1 2 3 4 5 6
</p>
<p>&minus;
1
.0
</p>
<p>&minus;
0
.5
</p>
<p>0
.0
</p>
<p>0
.5
</p>
<p>1
.0
</p>
<p>PCP for CPCA, 3 eigenvectors
</p>
<p>moneyness
</p>
<p>lo
a
</p>
<p>d
in
</p>
<p>g
</p>
<p>Inserting the likelihood function, we find that this is equivalent to
</p>
<p>T.n1;n2;:::;nk / D
kX
</p>
<p>iD1
.ni � 1/
</p>
<p>det . O&dagger;i/
det .Si /
</p>
<p>;
</p>
<p>which has a �2 distribution as min.ni / tends to infinity with
</p>
<p>k
n1
2
p.p � 1/C 1
</p>
<p>o
�
n1
2
p.p � 1/C kp
</p>
<p>o
D 1
2
.k � 1/p.p � 1/
</p>
<p>degrees of freedom. This test is included in the quantlet MVAcpcaiv.
</p>
<p>The calculations yield T.n1;n2;:::;nk/ D 31:836, which corresponds to the p-value
p D 0:37512 for the �2.30/ distribution. Hence we cannot reject the CPC model
against the unrestricted model, where PCA is applied to each maturity separately.
</p>
<p>Using the methods in Sect. 11.3, we can estimate the amount of variability, �l ,
</p>
<p>explained by the first l principal components: (only a few factors, three at the
</p>
<p>most, are needed to capture a large amount of the total variability present in the
</p>
<p>data). Since the model now captures the variability in both the strike and maturity
</p>
<p>dimensions, this is a suitable starting point for a simplified VaR calculation for
</p>
<p>delta-gamma neutral option portfolios using Monte Carlo methods, and is hence
</p>
<p>a valuable insight in risk management.</p>
<p/>
</div>
<div class="page"><p/>
<p>346 11 Principal Components Analysis
</p>
<p>11.8 Boston Housing
</p>
<p>A set of transformationswere defined in Chap. 1 for the Boston Housing data set that
</p>
<p>resulted in &ldquo;regular&rdquo; marginal distributions. The usefulness of principal component
</p>
<p>analysis with respect to such high-dimensional data sets will now be shown. The
</p>
<p>variable X4 is dropped because it is a discrete 0&ndash;1 variable. It will be used later,
</p>
<p>however, in the graphical representations. The scale difference of the remaining 13
</p>
<p>variables motivates a NPCA based on the correlation matrix.
</p>
<p>The eigenvalues and the percentage of explained variance are given in
</p>
<p>Table 11.5.
</p>
<p>The first principal component explains 56% of the total variance and the first
</p>
<p>three components together explain more than 75%. These results imply that it is
</p>
<p>sufficient to look at 2, maximum 3, principal components.
</p>
<p>Table 11.6 provides the correlations between the first three PCs and the original
</p>
<p>variables. These can be seen in Fig. 11.10.
</p>
<p>The correlations with the first PC show a very clear pattern. The variables
</p>
<p>X2; X6; X8; X12, and X14 are strongly positively correlated with the first PC,
</p>
<p>whereas the remaining variables are highly negatively correlated. The minimal
</p>
<p>correlation in the absolute value is 0.5. The first PC axis could be interpreted as
</p>
<p>a quality of life and house indicator. The second axis, given the polarities of X11
and X13 and of X6 and X14, can be interpreted as a social factor explaining only
</p>
<p>10% of the total variance. The third axis is dominated by a polarity betweenX2 and
</p>
<p>X12.
</p>
<p>The set of individuals from the first two PCs can be graphically interpreted
</p>
<p>if the plots are colour coded with respect to some particular variable of interest.
</p>
<p>Table 11.5 Eigenvalues and
percentage of explained
variance for Boston Housing
data MVAnpcahousi
</p>
<p>Eigenvalue Percentages Cumulated percentages
</p>
<p>7:2852 0:5604 0:5604
</p>
<p>1:3517 0:1040 0:6644
</p>
<p>1:1266 0:0867 0:7510
</p>
<p>0:7802 0:0600 0:8111
</p>
<p>0:6359 0:0489 0:8600
</p>
<p>0:5290 0:0407 0:9007
</p>
<p>0:3397 0:0261 0:9268
</p>
<p>0:2628 0:0202 0:9470
</p>
<p>0:1936 0:0149 0:9619
</p>
<p>0:1547 0:0119 0:9738
</p>
<p>0:1405 0:0108 0:9846
</p>
<p>0:1100 0:0085 0:9931
</p>
<p>0:0900 0:0069 1:0000</p>
<p/>
</div>
<div class="page"><p/>
<p>11.8 Boston Housing 347
</p>
<p>Table 11.6 Correlations of
the first three PC&rsquo;s with the
original variables
MVAnpcahous
</p>
<p>PC1 PC2 PC3
</p>
<p>X1 �0:9076 0:2247 0:1457
X2 0:6399 �0:0292 0:5058
X3 �0:8580 0:0409 �0:1845
X5 �0:8737 0:2391 �0:1780
X6 0:5104 0:7037 0:0869
</p>
<p>X7 �0:7999 0:1556 �0:2949
X8 0:8259 �0:2904 0:2982
X9 �0:7531 0:2857 0:3804
X10 �0:8114 0:1645 0:3672
X11 �0:5674 �0:2667 0:1498
X12 0:4906 �0:1041 �0:5170
X13 �0:7996 �0:4253 �0:0251
X14 0:7366 0:5160 �0:1747
</p>
<p>-1 0 1
</p>
<p>-1
</p>
<p>0
</p>
<p>1
</p>
<p>Boston Housing
</p>
<p>First PC
</p>
<p>S
e
c
o
</p>
<p>n
d
</p>
<p> P
C
</p>
<p>X1
</p>
<p>X3
</p>
<p>X5
X6
</p>
<p>X7
</p>
<p>X8
</p>
<p>X9
</p>
<p>X10
</p>
<p>X11
X12
</p>
<p>X13
</p>
<p>X14
</p>
<p>-1 0 1
</p>
<p>-1
</p>
<p>0
</p>
<p>1
</p>
<p>Boston Housing
</p>
<p>Third PC
</p>
<p>S
e
c
o
</p>
<p>n
d
</p>
<p> P
C
</p>
<p>X1
</p>
<p>X3
</p>
<p>X5
X6
</p>
<p>X7
</p>
<p>X8
</p>
<p>X9
</p>
<p>X10
</p>
<p>X11
X12
</p>
<p>X13
</p>
<p>X14
</p>
<p>-1 0 1
</p>
<p>-1
</p>
<p>0
</p>
<p>1
</p>
<p>Boston Housing
</p>
<p>First PC
</p>
<p>T
h
</p>
<p>ir
d
</p>
<p> P
C
</p>
<p>X1
X3
</p>
<p>X5
</p>
<p>X6X7
</p>
<p>X8
</p>
<p>X9
</p>
<p>X10
X11X12
</p>
<p>X13
</p>
<p>X14
</p>
<p>-1 0 1
</p>
<p>-1
</p>
<p>0
</p>
<p>1
</p>
<p>X
</p>
<p>Y X1
X3
</p>
<p>X5
X6
</p>
<p>X7
</p>
<p>X8
</p>
<p>X9
</p>
<p>X10
</p>
<p>X11
X12
</p>
<p>X13
</p>
<p>X14
</p>
<p>Fig. 11.10 NPCA for the Boston housing data, correlations of first three PCs with the original
variables MVAnpcahousi
</p>
<p>Figure 11.11 colour codes X14 &gt; median as red points. Clearly the first and second
</p>
<p>PCs are related to house value. The situation is less clear in Fig. 11.12 where the
</p>
<p>colour code corresponds toX4, the Charles River indicator, i.e. houses near the river
</p>
<p>are coloured red.</p>
<p/>
</div>
<div class="page"><p/>
<p>348 11 Principal Components Analysis
</p>
<p>Fig. 11.11 NPC analysis for
the Boston housing data,
scatterplot of the first two
PCs. More expensive houses
are marked with red colour
MVAnpcahous
</p>
<p>-6 -4 -2 0 2 4 6
</p>
<p>-4
</p>
<p>-3
</p>
<p>-2
</p>
<p>-1
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>PC1
</p>
<p>P
C
</p>
<p>2
</p>
<p>First vs. Second PC
</p>
<p>Fig. 11.12 NPC analysis for
the Boston housing data,
scatterplot of the first two
PCs. Houses close to the
Charles River are indicated
with red squares
MVAnpcahous
</p>
<p>-6 -4 -2 0 2 4 6
</p>
<p>-4
</p>
<p>-3
</p>
<p>-2
</p>
<p>-1
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>PC1
</p>
<p>P
C
</p>
<p>2
</p>
<p>First vs. Second PC
</p>
<p>11.9 More Examples
</p>
<p>Example 11.8 Let us now apply the PCA to the standardised bank data set
</p>
<p>(Sect. 22.2). Figure 11.13 shows some PC plots of the bank data set. The genuine
</p>
<p>and counterfeit bank notes are marked by &ldquo;o&rdquo; and &ldquo;+&rdquo;, respectively.
</p>
<p>The vector of eigenvalues ofR is
</p>
<p>` D .2:946; 1:278; 0:869; 0:450; 0:269; 0:189/&gt; :</p>
<p/>
</div>
<div class="page"><p/>
<p>11.9 More Examples 349
</p>
<p>2 4 6
0
</p>
<p>0.5
</p>
<p>1
</p>
<p>1.5
</p>
<p>2
</p>
<p>2.5
</p>
<p>3
</p>
<p>Index
</p>
<p>L
a
m
</p>
<p>b
d
</p>
<p>a
</p>
<p>Eigenvalues of S
</p>
<p>-4 -2 0 2 4
</p>
<p>0
</p>
<p>PC1
</p>
<p>P
C
</p>
<p>2
</p>
<p>First vs. Second PC
</p>
<p>-4 -2 0 2 4
</p>
<p>0
</p>
<p>PC2
</p>
<p>P
C
</p>
<p>3
</p>
<p>Second vs. Third PC
</p>
<p>-4 -2 0 2 4
-4
</p>
<p>-2
</p>
<p>0
</p>
<p>2
</p>
<p>4
</p>
<p>PC1
</p>
<p>P
C
</p>
<p>3
</p>
<p>First vs. Third PC
</p>
<p>Fig. 11.13 Principal components of the standardised bank data MVAnpcabank
</p>
<p>Table 11.7 Eigenvalues and
proportions of explained
</p>
<p>variance
</p>
<p>`j Proportion of variances Cumulated proportion
</p>
<p>2:946 0:491 49:1
</p>
<p>1:278 0:213 70:4
</p>
<p>0:869 0:145 84:9
</p>
<p>0:450 0:075 92:4
</p>
<p>0:264 0:045 96:9
</p>
<p>0:189 0:032 100:0
</p>
<p>The eigenvectors gj are given by the columns of the matrix
</p>
<p>G D
</p>
<p>0
BBBBBBB@
</p>
<p>�0:007 �0:815 0:018 0:575 0:059 0:031
0:468 �0:342 �0:103 �0:395 �0:639 �0:298
0:487 �0:252 �0:123 �0:430 0:614 0:349
0:407 0:266 �0:584 0:404 0:215 �0:462
0:368 0:091 0:788 0:110 0:220 �0:419
�0:493 �0:274 �0:114 �0:392 0:340 �0:632
</p>
<p>1
CCCCCCCA
:
</p>
<p>Each original variable has the same weight in the analysis and the results are
</p>
<p>independent of the scale of each variable.
</p>
<p>The proportions of explained variance are given in Table 11.7. It can be
</p>
<p>concluded that the representation in two dimensions should be sufficient. The</p>
<p/>
</div>
<div class="page"><p/>
<p>350 11 Principal Components Analysis
</p>
<p>Fig. 11.14 The correlations
of the original variable with
the PCs MVAnpcabanki
</p>
<p>-1 -0.5 0 0.5 1
</p>
<p>-1
</p>
<p>-0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>Swiss Bank Notes
</p>
<p>First PC
</p>
<p>S
e
c
o
</p>
<p>n
d
</p>
<p> P
C
</p>
<p>X1
</p>
<p>X2
</p>
<p>X3
</p>
<p>X4
</p>
<p>X5
</p>
<p>X6
</p>
<p>Table 11.8 Correlations
with PCs
</p>
<p>rXiZ1 rXiZ2 r
2
XiZ1
C r2XiZ2
</p>
<p>X1: length �0:012 �0:922 0:85
X2: left height 0:803 �0:387 0:79
X3: right height 0:835 �0:285 0:78
X4: lower 0:698 0:301 0:58
</p>
<p>X5: upper 0:631 0:104 0:41
</p>
<p>X6: diagonal �0:847 �0:310 0:81
</p>
<p>correlations leading to Fig. 11.14 are given in Table 11.8. The picture is different
</p>
<p>from the one obtained in Sect. 11.3 (see Table 11.2). Here, the first factor is mainly
</p>
<p>a left&ndash;right vs. diagonal factor and the second one is a length factor (with negative
</p>
<p>weight). Take another look at Fig. 11.13, where the individual bank notes are
</p>
<p>displayed. In the upper left graph it can be seen that the genuine bank notes are for
</p>
<p>the most part in the south-eastern portion of the graph featuring a larger diagonal,
</p>
<p>smaller height (Z1 &lt; 0) and also a larger length (Z2 &lt; 0). Note also that Fig. 11.14
</p>
<p>gives an idea of the correlation structure of the original data matrix.
</p>
<p>Example 11.9 Consider the data of 79 US companies given in Table 22.5. The data
</p>
<p>is first standardised by subtracting the mean and dividing by the standard deviation.
</p>
<p>Note that the data set contains six variables: assets .X1/, sales .X2/, market value
</p>
<p>.X3/, profits .X4/, cash flow .X5/, number of employees .X6/.
</p>
<p>Calculating the corresponding vector of eigenvalues gives
</p>
<p>` D .5:039; 0:517; 0:359; 0:050; 0:029; 0:007/&gt;</p>
<p/>
</div>
<div class="page"><p/>
<p>11.9 More Examples 351
</p>
<p>1 2 3 4 5 6
</p>
<p>0
</p>
<p>2
</p>
<p>4
</p>
<p>6
</p>
<p>Index
</p>
<p>L
a
m
</p>
<p>b
d
</p>
<p>a
</p>
<p>Eigenvalues of S
</p>
<p>0 5 10 15
</p>
<p>-4
</p>
<p>-2
</p>
<p>0
</p>
<p>2
First vs. Second PC
</p>
<p>PC 1
</p>
<p>P
C
</p>
<p> 2
</p>
<p>H
</p>
<p>H
</p>
<p>E
</p>
<p>EE
E
E
</p>
<p>EEE
EEE
</p>
<p>E
</p>
<p>EEEFF
F
</p>
<p>F
</p>
<p>F
</p>
<p>F
</p>
<p>F
F
FF
</p>
<p>F
</p>
<p>F
F
</p>
<p>F
</p>
<p>F
F
</p>
<p>F
</p>
<p>H
H
</p>
<p>H
</p>
<p>H
</p>
<p>H
</p>
<p>H
</p>
<p>HHMMM
M
</p>
<p>M
</p>
<p>M M
</p>
<p>M
</p>
<p>M
</p>
<p>M
</p>
<p>*
</p>
<p>*
** *
</p>
<p>*
</p>
<p>**** *
R
</p>
<p>RR
R
</p>
<p>R
RRRR R**
</p>
<p>**
*
</p>
<p>*
</p>
<p>Fig. 11.15 Principal components of the US company data MVAnpcausco
</p>
<p>and the matrix of eigenvectors is
</p>
<p>G D
</p>
<p>0
BBBBBBB@
</p>
<p>0:340 �0:849 �0:339 0:205 0:077 �0:006
0:423 �0:170 0:379 �0:783 �0:006 �0:186
0:434 0:190 �0:192 0:071 �0:844 0:149
0:420 0:364 �0:324 0:156 0:261 �0:703
0:428 0:285 �0:267 �0:121 0:452 0:667
0:397 0:010 0:726 0:548 0:098 0:065
</p>
<p>1
CCCCCCCA
:
</p>
<p>Using this information the graphical representations of the first two principal
</p>
<p>components are given in Fig. 11.15. The different sectors are marked by the
</p>
<p>following symbols:
</p>
<p>H . . . Hi Tech and Communication
</p>
<p>E . . . Energy
</p>
<p>F . . . Finance
</p>
<p>M . . . Manufacturing
</p>
<p>R . . . Retail
</p>
<p>? . . . all other sectors.
</p>
<p>The two outliers in the right-hand side of the graph are IBM and General Electric
</p>
<p>(GE), which differ from the other companies with their high market values. As can</p>
<p/>
</div>
<div class="page"><p/>
<p>352 11 Principal Components Analysis
</p>
<p>1 2 3 4 5 6
</p>
<p>0
</p>
<p>2
</p>
<p>4
</p>
<p>6
</p>
<p>Index
</p>
<p>L
a
m
</p>
<p>b
d
</p>
<p>a
</p>
<p>Eigenvalues of S
</p>
<p>-2 0 2 4 6 8
</p>
<p>-4
</p>
<p>0
</p>
<p>4
</p>
<p>8
First vs. Second PC
</p>
<p>PC 1
</p>
<p>P
C
</p>
<p> 2
</p>
<p>H
</p>
<p>H
E
</p>
<p>EE
E
</p>
<p>E
</p>
<p>EEE
E
EE EE E
</p>
<p>EFF F
F
</p>
<p>F
</p>
<p>F
FF FF
</p>
<p>F
</p>
<p>FF
</p>
<p>F
FFF
</p>
<p>H H
H
</p>
<p>HH
H M
</p>
<p>M M
M
</p>
<p>M
</p>
<p>M
M
</p>
<p>M
M
</p>
<p>M
</p>
<p>* *** *
</p>
<p>*
</p>
<p>**** *
</p>
<p>R
</p>
<p>RR
</p>
<p>R
</p>
<p>R
</p>
<p>R
R RR R
</p>
<p>*
</p>
<p>*
</p>
<p>**
</p>
<p>*
</p>
<p>*
</p>
<p>Fig. 11.16 Principal components of the US company data (without IBM and General Electric)
MVAnpcausco2
</p>
<p>be seen in the first column of G, market value has the largest weight in the first
</p>
<p>PC, adding to the isolation of these two companies. If IBM and GE were to be
</p>
<p>excluded from the data set, a completely different picture would emerge, as shown
</p>
<p>in Fig. 11.16. In this case the vector of eigenvalues becomes
</p>
<p>` D .3:191; 1:535; 0:791; 0:292; 0:149; 0:041/&gt; ;
</p>
<p>and the corresponding matrix of eigenvectors is
</p>
<p>G D
</p>
<p>0
BBBBBBB@
</p>
<p>0:263 �0:408 �0:800 �0:067 0:333 0:099
0:438 �0:407 0:162 �0:509 �0:441 �0:403
0:500 �0:003 �0:035 0:801 �0:264 �0:190
0:331 0:623 �0:080 �0:192 0:426 �0:526
0:443 0:450 �0:123 �0:238 �0:335 0:646
0:427 �0:277 0:558 0:021 0:575 0:313
</p>
<p>1
CCCCCCCA
</p>
<p>:
</p>
<p>The percentage of variation explained by each component is given in Table 11.9.
</p>
<p>The first two components explain almost 79% of the variance. The interpretation of
</p>
<p>the factors (the axes of Fig. 11.16) is given in the table of correlations (Table 11.10).
</p>
<p>The first two columns of this table are plotted in Fig. 11.17.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.9 More Examples 353
</p>
<p>Table 11.9 Eigenvalues and
proportions of explained
variance
</p>
<p>`j Proportion of variance Cumulated proportion
</p>
<p>3:191 0:532 0:532
</p>
<p>1:535 0:256 0:788
</p>
<p>0:791 0:132 0:920
</p>
<p>0:292 0:049 0:968
</p>
<p>0:149 0:025 0:993
</p>
<p>0:041 0:007 1:000
</p>
<p>Table 11.10 Correlations
with PCs
</p>
<p>rXiZ1 rXiZ2 r
2
XiZ1
C r2XiZ2
</p>
<p>X1: assets 0:47 �0:510 0:48
X2: sales 0:78 �0:500 0:87
X3: market value 0:89 �0:003 0:80
X4: profits 0:59 0:770 0:95
</p>
<p>X5: cash flow 0:79 0:560 0:94
</p>
<p>X6: employees 0:76 �0:340 0:70
</p>
<p>-1 -0.5 0 0.5 1
</p>
<p>-1
</p>
<p>-0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>U.S. Company Data
</p>
<p>First PC
</p>
<p>S
e
c
o
</p>
<p>n
d
</p>
<p> P
C
</p>
<p>X1 X2
</p>
<p>X3
</p>
<p>X4
</p>
<p>X5
</p>
<p>X6
</p>
<p>Fig. 11.17 The correlation of the original variables with the PCs MVAnpcausco2i
</p>
<p>From Fig. 11.17 (and Table 11.10) it appears that the first factor is a &ldquo;size effect&rdquo;,
</p>
<p>it is positively correlated with all the variables describing the size of the activity of
</p>
<p>the companies. It is also a measure of the economic strength of the firms. The second
</p>
<p>factor describes the &ldquo;shape&rdquo; of the companies (&ldquo;profit-cash flow&rdquo; vs. &ldquo;assets-sales&rdquo;
</p>
<p>factor), which is more difficult to interpret from an economic point of view.</p>
<p/>
</div>
<div class="page"><p/>
<p>354 11 Principal Components Analysis
</p>
<p>Example 11.10 Volle (1985) analyses data on 28 individuals (Table 22.14). For
</p>
<p>each individual, the time spent (in hours) on 10 different activities has been recorded
</p>
<p>over 100 days, as well as informative statistics such as the individual&rsquo;s sex, country
</p>
<p>of residence, professional activity and matrimonial status. The results of a NPCA
</p>
<p>are given below.
</p>
<p>The eigenvalues of the correlation matrix are given in Table 11.11. Note that the
</p>
<p>last eigenvalue is exactly zero since the correlation matrix is singular (the sum of all
</p>
<p>the variables is always equal to 2;400 D 24� 100). The results of the 4 first PCs are
given in Tables 11.12 and 11.13.
</p>
<p>From these tables (and Figs. 11.18 and 11.19), it appears that the professional
</p>
<p>and household activities are strongly contrasted in the first factor. Indeed on the
</p>
<p>horizontal axis of Fig. 11.18 it can be seen that all the active men are on the right
</p>
<p>and all the inactive women are on the left. Active women and/or single women are
</p>
<p>in between. The second factor contrasts meal/sleeping vs. toilet/shopping (note the
</p>
<p>high correlation between meal and sleeping). Along the vertical axis of Fig. 11.18
</p>
<p>we see near the bottom of the graph the people from Western-European countries,
</p>
<p>who spend more time on meals and sleeping than people from the US (who can be
</p>
<p>found close to the top of the graph). The other categories are in between.
</p>
<p>Table 11.11 Eigenvalues of
correlation matrix for the
time budget data
</p>
<p>`j Proportion of variance Cumulated proportion
</p>
<p>4:59 0:459 0:460
</p>
<p>2:12 0:212 0:670
</p>
<p>1:32 0:132 0:800
</p>
<p>1:20 0:120 0:920
</p>
<p>0:47 0:047 0:970
</p>
<p>0:20 0:020 0:990
</p>
<p>0:05 0:005 0:990
</p>
<p>0:04 0:004 0:999
</p>
<p>0:02 0:002 1:000
</p>
<p>0:00 0:000 1:000
</p>
<p>Table 11.12 Correlation of
variables with PCs
</p>
<p>rXiW1 rXiW2 rXiW3 rXiW4
</p>
<p>X1: prof 0:9772 �0:1210 �0:0846 0:0669
X2: tran 0:9798 0:0581 �0:0084 0:4555
X3: hous �0:8999 0:0227 0:3624 0:2142
X4: kids �0:8721 0:1786 0:0837 0:2944
X5: shop �0:5636 0:7606 �0:0046 �0:1210
X6: pers �0:0795 0:8181 �0:3022 �0:0636
X7: eati �0:5883 �0:6694 �0:4263 0:0141
X8: slee �0:6442 �0:5693 �0:1908 �0:3125
X9: tele �0:0994 0:1931 �0:9300 0:1512
X10: leis �0:0922 0:1103 0:0302 �0:9574</p>
<p/>
</div>
<div class="page"><p/>
<p>11.9 More Examples 355
</p>
<p>Table 11.13 PCs for time
budget data
</p>
<p>Z1 Z2 Z3 Z4
</p>
<p>maus 0:0633 0:0245 �0:0668 0:0205
waus 0:0061 0:0791 �0:0236 0:0156
wnus �0:1448 0:0813 �0:0379 �0:0186
mmus 0:0635 0:0105 �0:0673 0:0262
wmus �0:0934 0:0816 �0:0285 0:0038
msus 0:0537 0:0676 �0:0487 �0:0279
wsus 0:0166 0:1016 �0:0463 �0:0053
mawe 0:0420 �0:0846 �0:0399 �0:0016
wawe �0:0111 �0:0534 �0:0097 0:0337
wnwe �0:1544 �0:0583 �0:0318 �0:0051
mmwe 0:0402 �0:0880 �0:0459 0:0054
wmwe �0:1118 �0:0710 �0:0210 0:0262
mswe 0:0489 �0:0919 �0:0188 �0:0365
wswe �0:0393 �0:0591 �0:0194 �0:0534
mayo 0:0772 �0:0086 0:0253 �0:0085
wayo 0:0359 0:0064 0:0577 0:0762
</p>
<p>wnyo �0:1263 �0:0135 0:0584 �0:0189
mmyo 0:0793 �0:0076 0:0173 �0:0039
wmyo �0:0550 �0:0077 0:0579 0:0416
msyo 0:0763 0:0207 0:0575 �0:0778
wsyo 0:0120 0:0149 0:0532 �0:0366
maes 0:0767 �0:0025 0:0047 0:0115
waes 0:0353 0:0209 0:0488 0:0729
</p>
<p>wnes �0:1399 0:0016 0:0240 �0:0348
mmes 0:0742 �0:0061 �0:0152 0:0283
wmes �0:0175 0:0073 0:0429 0:0719
mses 0:0903 0:0052 0:0379 �0:0701
fses 0:0020 0:0287 0:0358 �0:0346
</p>
<p>In Fig. 11.19 the variables television and other leisure activities hardly play
</p>
<p>any role (look at Table 11.12). The variable television appears in Z3 (negatively
</p>
<p>correlated). Table 11.13 shows that this factor contrasts people from Eastern
</p>
<p>countries and Yugoslavia with men living in the US The variable other leisure
</p>
<p>activities is the factor Z4. It merely distinguishes between men and women in
</p>
<p>Eastern countries and in Yugoslavia. These last two factors are orthogonal to
</p>
<p>the preceding axes and of course their contribution to the total variation is less
</p>
<p>important.</p>
<p/>
</div>
<div class="page"><p/>
<p>356 11 Principal Components Analysis
</p>
<p>-0.15 -0.1 -0.05 0 0.05 0.1
</p>
<p>-0.1
</p>
<p>-0.05
</p>
<p>0
</p>
<p>0.05
</p>
<p>0.1
</p>
<p>Time Budget Data
</p>
<p>First Factor - Individuals
</p>
<p>S
e
</p>
<p>c
o
</p>
<p>n
d
</p>
<p> F
a
</p>
<p>c
to
</p>
<p>r 
- 
</p>
<p>In
d
</p>
<p>iv
id
</p>
<p>u
a
</p>
<p>ls
</p>
<p>maus
</p>
<p>wauswnus
</p>
<p>mmus
</p>
<p>wmus
</p>
<p>msus
</p>
<p>wsus
</p>
<p>mawe
</p>
<p>wawe
wnwe
</p>
<p>mmwe
</p>
<p>wmwe
</p>
<p>mswe
</p>
<p>wswe
</p>
<p>mayo
</p>
<p>wayo
</p>
<p>wnyo
mmyowmyo
</p>
<p>msyo
wsyo
</p>
<p>maes
</p>
<p>waes
</p>
<p>wnes
mmes
</p>
<p>wmes mses
</p>
<p>wses
</p>
<p>Fig. 11.18 Representation of the individuals MVAnpcatime
</p>
<p>-1 -0.5 0 0.5 1
</p>
<p>-1
</p>
<p>-0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>Time Budget Data
</p>
<p>First Factor - Expenditures
</p>
<p>S
e
</p>
<p>c
o
</p>
<p>n
d
</p>
<p> F
a
</p>
<p>c
to
</p>
<p>r 
- 
</p>
<p>E
x
</p>
<p>p
e
</p>
<p>n
d
</p>
<p>it
u
</p>
<p>re
s
</p>
<p>prof
</p>
<p>tran hous
</p>
<p>kids
</p>
<p>shop
pers
</p>
<p>eati
</p>
<p>slee
</p>
<p>tele
leis
</p>
<p>Fig. 11.19 Representation of the variables MVAnpcatime</p>
<p/>
</div>
<div class="page"><p/>
<p>11.10 Exercises 357
</p>
<p>11.10 Exercises
</p>
<p>Exercise 11.1 Prove Theorem 11.1. (Hint: use (4.23).)
</p>
<p>Exercise 11.2 Interpret the results of the PCA of the US companies. Use the
</p>
<p>analysis of the bank notes in Sect. 11.3 as a guide. Compare your results with those
</p>
<p>in Example 11.9.
</p>
<p>Exercise 11.3 Test the hypothesis that the proportion of variance explained by the
</p>
<p>first two PCs for the US companies is  D 0:75.
Exercise 11.4 Apply the PCA to the car data (Table 22.7). Interpret the first two
</p>
<p>PCs. Would it be necessary to look at the third PC?
</p>
<p>Exercise 11.5 Take the athletic records for 55 countries (Sect. 22.18) and apply the
</p>
<p>NPCA. Interpret your results.
</p>
<p>Exercise 11.6 Apply a PCA to &dagger; D
�
1 �
</p>
<p>� 1
</p>
<p>�
, where � &gt; 0. Now change the scale
</p>
<p>ofX1, i.e. consider the covariance of cX1 andX2. How do the PC directions change
</p>
<p>with the screeplot?
</p>
<p>Exercise 11.7 Suppose that we have standardised some data using the Maha-
</p>
<p>lanobis transformation. Would it be reasonable to apply a PCA?
</p>
<p>Exercise 11.8 Apply a NPCA to the US CRIME data set (Table 22.10). Interpret the
</p>
<p>results. Would it be necessary to look at the third PC? Can you see any difference
</p>
<p>between the four regions? Redo the analysis excluding the variable &ldquo;area of the
</p>
<p>state&rdquo;.
</p>
<p>Exercise 11.9 Repeat Exercise 11.8 using the US HEALTH data set (Table 22.16).
</p>
<p>Exercise 11.10 Do a NPCA on the GEOPOL data set (see Table 22.15) which
</p>
<p>compares 41 countries w.r.t. different aspects of their development. Why or why
</p>
<p>not would a PCA be reasonable here?
</p>
<p>Exercise 11.11 Let U be an uniform r.v. on Œ0; 1&#141;. Let a 2 R3 be a vector of
constants. Suppose that X D Ua&gt; D .X1; X2; X3/. What do you expect the NPCs
of X to be?
</p>
<p>Exercise 11.12 Let U1 and U2 be two independent uniform random variables on
</p>
<p>Œ0; 1&#141;. Suppose that X D .X1; X2; X3; X4/&gt; where X1 D U1, X2 D U2, X3 D
U1 C U2 and X4 D U1 � U2. Compute the correlation matrix P of X . How many
PCs are of interest? Show that &#13;1 D
</p>
<p>�
1p
2
; 1p
</p>
<p>2
; 1; 0
</p>
<p>�&gt;
and &#13;2 D
</p>
<p>�
1p
2
; �1p
</p>
<p>2
; 0; 1
</p>
<p>�&gt;
</p>
<p>are eigenvectors of P corresponding to the non trivial �&lsquo;s. Interpret the first two
</p>
<p>NPCs obtained.
</p>
<p>Exercise 11.13 Simulate a sample of size n D 50 for the r.v. X in Exercise 11.12
and analyse the results of a NPCA.</p>
<p/>
</div>
<div class="page"><p/>
<p>358 11 Principal Components Analysis
</p>
<p>Exercise 11.14 Bouroche and Saporta (1980) reported the data on the state
</p>
<p>expenses of France from the period 1872 to 1971 (24 selected years) by noting the
</p>
<p>percentage of 11 categories of expenses. Do a NPCA of this data set. Do the three
</p>
<p>main periods (before WWI, between WWI and WWII, and after WWII) indicate a
</p>
<p>change in behaviour w.r.t. state expenses?</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 12
</p>
<p>Factor Analysis
</p>
<p>A frequently applied paradigm in analysing data frommultivariate observations is to
</p>
<p>model the relevant information (represented in a multivariate variableX ) as coming
</p>
<p>from a limited number of latent factors. In a survey on household consumption, for
</p>
<p>example, the consumption levels, X , of p different goods during 1 month could be
</p>
<p>observed. The variations and covariations of the p components of X throughout the
</p>
<p>survey might in fact be explained by two or three main social behaviour factors
</p>
<p>of the household. For instance, a basic desire of comfort or the willingness to
</p>
<p>achieve a certain social level or other social latent concepts might explain most of
</p>
<p>the consumption behaviour. These unobserved factors are much more interesting to
</p>
<p>the social scientist than the observed quantitative measures (X ) themselves, because
</p>
<p>they give a better understanding of the behaviour of households. As shown in the
</p>
<p>examples below, the same kind of factor analysis is of interest in many fields such
</p>
<p>as psychology, marketing, economics, and politic sciences.
</p>
<p>How can we provide a statistical model addressing these issues and how
</p>
<p>can we interpret the obtained model? This is the aim of factor analysis. As in
</p>
<p>Chaps. 10 and 11, the driving statistical theme of this chapter is to reduce the
</p>
<p>dimension of the observed data. The perspective used, however, is different: we
</p>
<p>assume that there is a model (it will be called the &ldquo;Factor Model&rdquo;) stating that most
</p>
<p>of the covariances between the p elements of X can be explained by a limited
</p>
<p>number of latent factors. Section 12.1 defines the basic concepts and notations
</p>
<p>of the orthogonal factor model, stressing the non-uniqueness of the solutions. We
</p>
<p>show how to take advantage of this non-uniqueness to derive techniques which
</p>
<p>lead to easier interpretations. This will involve (geometric) rotations of the factors.
</p>
<p>Section 12.2 presents an empirical approach to factor analysis. Various estimation
</p>
<p>procedures are proposed and an optimal rotation procedure is defined. Many
</p>
<p>examples are used to illustrate the method.
</p>
<p>&copy; Springer-Verlag Berlin Heidelberg 2015
W.K. H&auml;rdle, L. Simar, Applied Multivariate Statistical Analysis,
DOI 10.1007/978-3-662-45171-7_12
</p>
<p>359</p>
<p/>
</div>
<div class="page"><p/>
<p>360 12 Factor Analysis
</p>
<p>12.1 The Orthogonal Factor Model
</p>
<p>The aim of factor analysis is to explain the outcome of p variables in the data
</p>
<p>matrix X using fewer variables, the so-called factors. Ideally all the information in
</p>
<p>X can be reproduced by a smaller number of factors. These factors are interpreted
</p>
<p>as latent (unobserved) common characteristics of the observed x 2 Rp . The case
just described occurs when every observed x D .x1; : : : ; xp/&gt; can be written as
</p>
<p>xj D
kX
</p>
<p>`D1
qj `f` C �j ; j D 1; : : : ; p: (12.1)
</p>
<p>Here f`, for ` D 1; : : : ; k denotes the factors. The number of factors, k, should
always be much smaller than p. For instance, in psychology x may represent p
</p>
<p>results of a test measuring intelligence scores. One common latent factor explaining
</p>
<p>x 2 Rp could be the overall level of &ldquo;intelligence&rdquo;. In marketing studies, x may
consist of p answers to a survey on the levels of satisfaction of the customers. These
</p>
<p>p measures could be explained by common latent factors like the attraction level of
</p>
<p>the product or the image of the brand, and so on. Indeed it is possible to create a
</p>
<p>representation of the observations that is similar to the one in (12.1) by means of
</p>
<p>principal components, but only if the last p � k eigenvalues corresponding to the
covariance matrix are equal to zero. Consider a p-dimensional random vector X
</p>
<p>with mean � and covariance matrix Var.X/ D &dagger;. A model similar to (12.1) can be
written for X in matrix notation, namely
</p>
<p>X D QF C �; (12.2)
</p>
<p>where F is the k-dimensional vector of the k factors. When using the factor
</p>
<p>model (12.2) it is often assumed that the factors F are centred, uncorrelated and
</p>
<p>standardised: E.F / D 0 and Var.F / D Ik . We will now show that if the last
p � k eigenvalues of &dagger; are equal to zero, we can easily express X by the factor
model (12.2).
</p>
<p>The spectral decomposition of&dagger; is given by &#128;ƒ&#128;&gt;. Suppose that only the first k
eigenvalues are positive, i.e. �kC1 D � � � D �p D 0. Then the (singular) covariance
matrix can be written as
</p>
<p>&dagger; D
kX
</p>
<p>`D1
�`&#13;`&#13;
</p>
<p>&gt;
` D .&#128;1&#128;2/
</p>
<p>�
ƒ1 0
</p>
<p>0 0
</p>
<p>� 
&#128;&gt;1
&#128;&gt;2
</p>
<p>!
:
</p>
<p>In order to show the connection to the factor model (12.2), recall that the PCs are
</p>
<p>given by Y D &#128;&gt;.X � �/. Rearranging we have X � � D &#128;Y D &#128;1Y1 C &#128;2Y2,
where the components of Y are partitioned according to the partition of &#128; above,
</p>
<p>namely</p>
<p/>
</div>
<div class="page"><p/>
<p>12.1 The Orthogonal Factor Model 361
</p>
<p>Y D
�
Y1
Y2
</p>
<p>�
D
�
&#128;&gt;1
&#128;&gt;2
</p>
<p>�
.X � �/; where
</p>
<p>�
&#128;&gt;1
&#128;&gt;2
</p>
<p>�
.X � �/ �
</p>
<p>�
0;
</p>
<p>�
ƒ1 0
</p>
<p>0 0
</p>
<p>��
:
</p>
<p>In other words, Y2 has a singular distribution with mean and covariancematrix equal
</p>
<p>to zero. Therefore,X �� D &#128;1Y1C&#128;2Y2 implies that X �� is equivalent to &#128;1Y1,
which can be written as
</p>
<p>X D &#128;1ƒ1=21 ƒ
�1=2
1 Y1 C �:
</p>
<p>DefiningQ D &#128;1ƒ1=21 and F D ƒ
�1=2
1 Y1, we obtain the factor model (12.2).
</p>
<p>Note that the covariance matrix of model (12.2) can be written as
</p>
<p>&dagger; D E.X � �/.X � �/&gt; D QE.FF&gt;/Q&gt; D QQ&gt; D
kX
</p>
<p>jD1
�j&#13;j&#13;
</p>
<p>&gt;
j : (12.3)
</p>
<p>We have just shown how the variableX can be completely determined by a weighted
</p>
<p>sum of k (where k &lt; p) uncorrelated factors. The situation used in the derivation,
</p>
<p>however, is too idealistic. In practice the covariance matrix is rarely singular.
</p>
<p>It is a common praxis in factor analysis to split the influences of the factors into
</p>
<p>common and specific ones. There are, for example, highly informative factors that
</p>
<p>are common to all of the components of X and factors that are specific to certain
</p>
<p>components. The factor analysis model used in praxis is a generalisation of (12.2):
</p>
<p>X D QF C U C �; (12.4)
</p>
<p>where Q is a .p � k/ matrix of the (non-random) loadings of the common factors
F.k�1/ andU is a .p�1/matrix of the (random) specific factors. It is assumed that
the factor variables F are uncorrelated random vectors and that the specific factors
</p>
<p>are uncorrelated and have zero covariance with the common factors. More precisely,
</p>
<p>it is assumed that:
</p>
<p>EF D 0;
Var.F / D Ik ;
</p>
<p>EU D 0; (12.5)
Cov.Ui ; Uj / D 0; i &curren; j
</p>
<p>Cov.F; U / D 0:
</p>
<p>Define
</p>
<p>Var.U / D &permil; D diag. 11; : : : ;  pp/:</p>
<p/>
</div>
<div class="page"><p/>
<p>362 12 Factor Analysis
</p>
<p>The generalised factor model (12.4) together with the assumptions given in (12.5)
</p>
<p>constitute the orthogonal factor model.
</p>
<p>Orthogonal Factor Model
</p>
<p>X D Q F C U C �
(p � 1) (p � k) (k � 1) (p � 1) (p � 1)
</p>
<p>�j D mean of variable j
Uj D j th specific factor
F` D `th common factor
qj ` D loading of the j th variable on the `th factor
</p>
<p>The random vectors F and U are unobservable and uncorrelated.
</p>
<p>Note that (12.4) implies for the components of X D .X1; : : : ; Xp/&gt; that
</p>
<p>Xj D
kX
</p>
<p>`D1
qj `F` C Uj C �j ; j D 1; : : : ; p: (12.6)
</p>
<p>Using (12.5) we obtain �XjXj D Var.Xj / D
Pk
</p>
<p>`D1 q
2
j ` C  jj. The quantity
</p>
<p>h2j D
Pk
</p>
<p>`D1 q
2
j ` is called the communality and  jj the specific variance. Thus the
</p>
<p>covariance of X can be rewritten as
</p>
<p>&dagger; D E.X � �/.X � �/&gt; D E.QF C U /.QF C U /&gt;
</p>
<p>D QE.FF&gt;/Q&gt; C E.UU&gt;/ D QVar.F /Q&gt; C Var.U /
D QQ&gt; C&permil;: (12.7)
</p>
<p>In a sense, the factor model explains the variations ofX for the most part by a small
</p>
<p>number of latent factors F common to its p components and entirely explains all
</p>
<p>the correlation structure between its components, plus some &ldquo;noise&rdquo;U which allows
</p>
<p>specific variations of each component to enter. The specific factors adjust to capture
</p>
<p>the individual variance of each component. Factor analysis relies on the assumptions
</p>
<p>presented above. If the assumptions are not met, the analysis could be spurious.
</p>
<p>Although principal components analysis and factor analysis might be related (this
</p>
<p>was hinted at in the derivation of the factor model), they are quite different in nature.
</p>
<p>PCs are linear transformations of X arranged in decreasing order of variance and
</p>
<p>used to reduce the dimension of the data set, whereas in factor analysis, we try to
</p>
<p>model the variations of X using a linear transformation of a fixed, limited number
</p>
<p>of latent factors. The objective of factor analysis is to find the loadings Q and
</p>
<p>the specific variance &permil;. Estimates of Q and &permil; are deduced from the covariance
</p>
<p>structure (12.7).</p>
<p/>
</div>
<div class="page"><p/>
<p>12.1 The Orthogonal Factor Model 363
</p>
<p>Interpretation of the Factors
</p>
<p>Assume that a factor model with k factors was found to be reasonable, i.e. most
</p>
<p>of the (co)variations of the p measures in X were explained by the k fixed latent
</p>
<p>factors. The next natural step is to try to understand what these factors represent. To
</p>
<p>interpret F`, it makes sense to compute its correlations with the original variables
</p>
<p>Xj first. This is done for ` D 1; : : : ; k and for j D 1; : : : ; p to obtain the matrix
PXF. The sequence of calculations used here are in fact the same that were used to
</p>
<p>interpret the PCs in the principal components analysis.
</p>
<p>The following covariance between X and F is obtained via (12.5),
</p>
<p>&dagger;XF D Ef.QF C U /F&gt;g D Q:
</p>
<p>The correlation is
</p>
<p>PXF D D�1=2Q; (12.8)
</p>
<p>where D D diag.�X1X1 ; : : : ; �XpXp /. Using (12.8) it is possible to construct a
figure analogous to Fig. 11.6 and thus to consider which of the original variables
</p>
<p>X1; : : : ; Xp play a role in the unobserved common factors F1; : : : ; Fk .
</p>
<p>Returning to the psychology example where X are the observed scores to p
</p>
<p>different intelligence tests (the WAIS data set in Table 22.12 provides an example),
</p>
<p>we would expect a model with one factor to produce a factor that is positively
</p>
<p>correlated with all of the components in X . For this example the factor represents
</p>
<p>the overall level of intelligence of an individual. A model with two factors could
</p>
<p>produce a refinement in explaining the variations of the p scores. For example,
</p>
<p>the first factor could be the same as before (overall level of intelligence), whereas
</p>
<p>the second factor could be positively correlated with some of the tests, Xj , that
</p>
<p>are related to the individual&rsquo;s ability to think abstractly and negatively correlated
</p>
<p>with other tests, Xi , that are related to the individual&rsquo;s practical ability. The second
</p>
<p>factor would then concern a particular dimension of the intelligence stressing the
</p>
<p>distinctions between the &ldquo;theoretical&rdquo; and &ldquo;practical&rdquo; abilities of the individual.
</p>
<p>If the model is true, most of the information coming from the p scores can be
</p>
<p>summarised by these two latent factors. Other practical examples are given below.
</p>
<p>Invariance of Scale
</p>
<p>What happens if we change the scale of X to Y D CX with C D diag.c1; : : : ; cp/?
If the k-factor model (12.6) is true for X with Q D QX , &permil; D &permil;X , then, since
</p>
<p>Var.Y / D C&dagger;C&gt; D CQXQ&gt;XC&gt; C C&permil;XC&gt;;</p>
<p/>
</div>
<div class="page"><p/>
<p>364 12 Factor Analysis
</p>
<p>the same k-factor model is also true for Y with QY D CQX and &permil;Y D C&permil;XC&gt;.
In many applications, the search for the loadings Q and for the specific variance
</p>
<p>&permil; will be done by the decomposition of the correlation matrix of X rather than the
</p>
<p>covariancematrix&dagger;. This corresponds to a factor analysis of a linear transformation
</p>
<p>of X (i.e. Y D D�1=2.X � �//. The goal is to try to find the loadings QY and the
specific variance &permil;Y such that
</p>
<p>P D QY Q&gt;Y C&permil;Y : (12.9)
</p>
<p>In this case the interpretation of the factorsF immediately follows from (12.8) given
</p>
<p>the following correlation matrix:
</p>
<p>PXF D PYF D QY : (12.10)
</p>
<p>Because of the scale invariance of the factors, the loadings and the specific variance
</p>
<p>of the model, where X is expressed in its original units of measure, are given by
</p>
<p>QX D D1=2QY
&permil;X D D1=2&permil;YD1=2:
</p>
<p>It should be noted that although the factor analysis model (12.4) enjoys the scale
</p>
<p>invariance property, the actual estimated factors could be scale dependent. We will
</p>
<p>come back to this point later when we discuss the method of principal factors.
</p>
<p>Non-uniqueness of Factor Loadings
</p>
<p>The factor loadings are not unique! Suppose that G is an orthogonal matrix. Then X
</p>
<p>in (12.4) can also be written as
</p>
<p>X D .QG/.G&gt;F /C U C �:
</p>
<p>This implies that, if a k-factor of X with factors F and loadings Q is true, then
</p>
<p>the k-factor model with factors G&gt;F and loadings QG is also true. In practice, we
will take advantage of this non-uniqueness. Indeed, referring back to Sect. 2.6 we
</p>
<p>can conclude that premultiplying a vector F by an orthogonal matrix corresponds
</p>
<p>to a rotation of the system of axis, the direction of the first new axis being given by
</p>
<p>the first row of the orthogonal matrix. It will be shown that choosing an appropriate
</p>
<p>rotation will result in a matrix of loadings QG that will be easier to interpret. We
</p>
<p>have seen that the loadings provide the correlations between the factors and the
</p>
<p>original variables; therefore, it makes sense to search for rotations that give factors
</p>
<p>that are maximally correlated with various groups of variables.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.1 The Orthogonal Factor Model 365
</p>
<p>From a numerical point of view, the non-uniqueness is a drawback. We have
</p>
<p>to find loadings Q and specific variances &permil; satisfying the decomposition &dagger; D
QQ&gt; C&permil;, but no straightforward numerical algorithm can solve this problem due
to the multiplicity of the solutions. An acceptable technique is to impose some
</p>
<p>chosen constraints in order to get&mdash;in the best case&mdash;an unique solution to the
</p>
<p>decomposition. Then, as suggested above, once we have a solution we will take
</p>
<p>advantage of the rotations in order to obtain a solution that is easier to interpret.
</p>
<p>An obvious question is: what kind of constraints should we impose in order to
</p>
<p>eliminate the non-uniqueness problem? Usually, we impose additional constraints
</p>
<p>where
</p>
<p>Q&gt;&permil;�1Q is diagonal (12.11)
</p>
<p>or
</p>
<p>Q&gt;D�1Q is diagonal. (12.12)
</p>
<p>How many parameters does the model (12.7) have without constraints?
</p>
<p>Q.p � k/ has p � k parameters, and
&permil;.p � p/ has p parameters.
</p>
<p>Hence we have to determine pk C p parameters! Conditions (12.11) respec-
tively (12.12) introduce 1
</p>
<p>2
fk.k � 1/g constraints, since we require the matrices to
</p>
<p>be diagonal. Therefore, the degrees of freedom of a model with k factors is:
</p>
<p>d D (# parameters for &dagger; unconstrained)� (# parameters for &dagger; constrained)
D 1
</p>
<p>2
p.p C 1/� .pkC p � 1
</p>
<p>2
k.k � 1//
</p>
<p>D 1
2
.p � k/2 � 1
</p>
<p>2
.p C k/:
</p>
<p>If d &lt; 0, then the model is undetermined: there are infinitely many solutions
</p>
<p>to (12.7). This means that the number of parameters of the factorial model is larger
</p>
<p>than the number of parameters of the original model, or that the number of factors
</p>
<p>k is &ldquo;too large&rdquo; relative to p. In some cases d D 0: there is a unique solution to
the problem (except for rotation). In practice we usually have that d &gt; 0: there are
</p>
<p>more equations than parameters, thus an exact solution does not exist. In this case
</p>
<p>approximate solutions are used. An approximation of&dagger;, for example, isQQ&gt;C&permil;.
The last case is the most interesting since the factorial model has less parameters
</p>
<p>than the original one. Estimation methods are introduced in the next section.
</p>
<p>Evaluating the degrees of freedom,d , is particularly important, because it already
</p>
<p>gives an idea of the upper bound on the number of factors we can hope to identify in
</p>
<p>a factor model. For instance, if p D 4, we could not identify a factor model with two
factors (this results in d D �1 which has infinitely many solutions). With p D 4,</p>
<p/>
</div>
<div class="page"><p/>
<p>366 12 Factor Analysis
</p>
<p>only a one factor model gives an approximate solution (d D 2). When p D 6,
models with 1 and 2 factors provide approximate solutions and a model with three
</p>
<p>factors results in an unique solution (up to the rotations) since d D 0. A model with
four or more factors would not be allowed, but of course, the aim of factor analysis
</p>
<p>is to find suitable models with a small number of factors, i.e. smaller than p. The
</p>
<p>next two examples give more insights into the notion of degrees of freedom.
</p>
<p>Example 12.1 Let p D 3 and k D 1, then d D 0 and
</p>
<p>&dagger; D
</p>
<p>0
@
�11 �12 �13
</p>
<p>�21 �22 �23
�31 �32 �33
</p>
<p>1
A D
</p>
<p>0
@
q21 C  11 q1q2 q1q3
q1q2 q
</p>
<p>2
2 C  22 q2q3
</p>
<p>q1q3 q2q3 q
2
3 C  33
</p>
<p>1
A
</p>
<p>with Q D
</p>
<p>0
@
q1
q2
</p>
<p>q3
</p>
<p>1
A and &permil; D
</p>
<p>0
@
 11 0 0
</p>
<p>0  22 0
</p>
<p>0 0  33
</p>
<p>1
A. Note that here the constraint (12.11)
</p>
<p>is automatically verified since k D 1. We have
</p>
<p>q21 D
�12�13
</p>
<p>�23
I q22 D
</p>
<p>�12�23
</p>
<p>�13
I q23 D
</p>
<p>�13�23
</p>
<p>�12
</p>
<p>and
</p>
<p> 11 D �11 � q21 I  22 D �22 � q22 I  33 D �33 � q23 :
</p>
<p>In this particular case (k D 1), the only rotation is defined by G D �1, so the other
solution for the loadings is provided by �Q.
Example 12.2 Suppose now p D 2 and k D 1, then d &lt; 0 and
</p>
<p>&dagger; D
�
1 �
</p>
<p>� 1
</p>
<p>�
D
�
q21 C  11 q1q2
q1q2 q
</p>
<p>2
2 C  22
</p>
<p>�
:
</p>
<p>We have infinitely many solutions: for any ˛ .� &lt; ˛ &lt; 1/, a solution is provided by
</p>
<p>q1 D ˛I q2 D �=˛I  11 D 1 � ˛2I  22 D 1 � .�=˛/2:
</p>
<p>The solution in Example 12.1 may be unique (up to a rotation), but it is not proper
</p>
<p>in the sense that it cannot be interpreted statistically. Exercise 12.5 gives an example
</p>
<p>where the specific variance  11 is negative.
</p>
<p>!
Even in the case of a unique solution .d D 0/, the solution may be
</p>
<p>inconsistent with statistical interpretations.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.2 Estimation of the Factor Model 367
</p>
<p>Summary
</p>
<p>,! The factor analysis model aims to describe how the original p vari-
ables in a data set depend on a small number of latent factors k &lt;p,
</p>
<p>i.e. it assumes that X DQF CU C�. The (k-dimensional) ran-
dom vector F contains the common factors, the (p-dimensional)
</p>
<p>U contains the specific factors and Q.p � k/ contains the factor
loadings.
</p>
<p>,! It is assumed that F and U are uncorrelated and have zero means,
i.e. F � .0; I/, U � .0;&permil;/ where &permil; is diagonal matrix and
Cov.F; U / D 0.
This leads to the covariance structure&dagger; D QQ&gt; C&permil;.
</p>
<p>,! The interpretation of the factor F is obtained through the correla-
tion PXF D D�1=2Q.
</p>
<p>,! A normalised analysis is obtained by the model P D QQ&gt; C &permil;.
The interpretation of the factors is given directly by the loadings
</p>
<p>Q W PXF D Q.
,! The factor analysis model is scale invariant. The loadings are not
</p>
<p>unique (only up to multiplication by an orthogonal matrix).
</p>
<p>,! Whether a model has an unique solution or not is determined by
the degrees of freedom d D 1=2.p � k/2 � 1=2.pC k/.
</p>
<p>12.2 Estimation of the Factor Model
</p>
<p>In practice, we have to find estimates OQ of the loadings Q and estimates O&permil; of the
specific variances&permil; such that analogously to (12.7)
</p>
<p>S D OQ OQ&gt; C O&permil;;
</p>
<p>where S denotes the empirical covariance of X . Given an estimate OQ of Q, it is
natural to set
</p>
<p>O jj D sXjXj �
kX
</p>
<p>`D1
Oq2j `:
</p>
<p>We have that Oh2j D
Pk
</p>
<p>`D1 Oq2j ` is an estimate for the communality h2j .
In the ideal case d D 0, there is an exact solution. However, d is usually greater
</p>
<p>than zero, thereforewe have to find OQ and O&permil; such that S is approximated by OQ OQ&gt;C
O&permil;. As mentioned above, it is often easier to compute the loadings and the specific
variances of the standardised model.</p>
<p/>
</div>
<div class="page"><p/>
<p>368 12 Factor Analysis
</p>
<p>Define Y D HXD�1=2, the standardisation of the data matrix X , where D D
diag.sX1X1 ; : : : ; sXpXp / and the centering matrix H D I � n�11n1&gt;n (recall from
Chap. 2 that S D 1
</p>
<p>n
X&gt;HX ). The estimated factor loading matrix OQY and the
</p>
<p>estimated specific variance O&permil;Y of Y are
</p>
<p>OQY D D�1=2 OQX and O&permil;Y D D�1 O&permil;X :
</p>
<p>For the correlation matrixR of X , we have that
</p>
<p>R D OQY OQ&gt;Y C O&permil;Y :
</p>
<p>The interpretations of the factors are formulated from the analysis of the loadings
OQY .
Example 12.3 Let us calculate the matrices just defined for the car data given in
</p>
<p>Table 22.7. This data set consists of the averaged marks (from 1 Dlow to 6 Dhigh)
for 24 car types. Considering the three variables price, security and easy handling,
</p>
<p>we get the following correlation matrix:
</p>
<p>R D
</p>
<p>0
@
</p>
<p>1 0:975 0:613
</p>
<p>0:975 1 0:620
</p>
<p>0:613 0:620 1
</p>
<p>1
A :
</p>
<p>We will first look for one factor, i.e. k D 1. Note that (# number of parameters of&dagger;
unconstrained &ndash; # parameters of&dagger; constrained) is equal to 1
</p>
<p>2
.p�k/2� 1
</p>
<p>2
.pCk/ D
</p>
<p>1
2
.3� 1/2� 1
</p>
<p>2
.3C 1/ D 0. This implies that there is an exact solution! The equation
</p>
<p>0
@
</p>
<p>1 rX1X2 rX1X3
rX1X2 1 rX2X3
rX1X3 rX2X3 1
</p>
<p>1
A D R D
</p>
<p>0
@
Oq21 C O 11 Oq1 Oq2 Oq1 Oq3
Oq1 Oq2 Oq22 C O 22 Oq2 Oq3
Oq1 Oq3 Oq2 Oq3 Oq23 C O 33
</p>
<p>1
A
</p>
<p>yields the communalities Oh2i D Oq2i , where
</p>
<p>Oq21 D
rX1X2rX1X3
rX2X3
</p>
<p>; Oq22 D
rX1X2rX2X3
rX1X3
</p>
<p>and Oq23 D
rX1X3rX2X3
rX1X2
</p>
<p>:
</p>
<p>Combining this with the specific variances O 11 D 1 � Oq21 , O 22 D 1 � Oq22 and
O 33 D 1 � Oq23 , we obtain the following solution
</p>
<p>Oq1 D 0:982 Oq2 D 0:993 Oq3 D 0:624
O 11 D 0:035 O 22 D 0:014 O 33 D 0:610:</p>
<p/>
</div>
<div class="page"><p/>
<p>12.2 Estimation of the Factor Model 369
</p>
<p>Since the first two communalities ( Oh2i D Oq2i ) are close to one, we can conclude that
the first two variables, namely price and security, are explained by the single factor
</p>
<p>quite well. This factor can be interpreted as a &ldquo;price+security&rdquo; factor.
</p>
<p>The Maximum Likelihood Method
</p>
<p>Recall fromChap. 6 the log-likelihood function ` for a data matrixX of observations
</p>
<p>of X � Np.�;&dagger;/:
</p>
<p>`.X I�;&dagger;/ D �n
2
log j 2�&dagger; j �1
</p>
<p>2
</p>
<p>nX
</p>
<p>iD1
.xi � �/&dagger;�1.xi � �/&gt;
</p>
<p>D �n
2
log j 2�&dagger; j �n
</p>
<p>2
tr.&dagger;�1S/ � n
</p>
<p>2
.x � �/&dagger;�1.x � �/&gt;:
</p>
<p>This can be rewritten as
</p>
<p>`.X I O�;&dagger;/ D �n
2
</p>
<p>˚
log j 2�&dagger; j C tr.&dagger;�1S/
</p>
<p>�
:
</p>
<p>Replacing � by O� D x and substituting&dagger; D QQ&gt; C&permil; this becomes
</p>
<p>`.X I O�;Q; &permil;/ D �n
2
</p>
<p>�
logfj 2�.QQ&gt; C&permil;/ jg C trf.QQ&gt; C&permil;/�1Sg
</p>
<p>�
:
</p>
<p>(12.13)
</p>
<p>Even in the case of a single factor (k D 1), these equations are rather complicated
and iterative numerical algorithms have to be used [for more details see Mardia,
</p>
<p>Kent &amp; Bibby, 1979, p. 263]. A practical computation scheme is also given in
</p>
<p>Supplement 9A of Johnson and Wichern (1998).
</p>
<p>Likelihood Ratio Test for the Number of Common Factors
</p>
<p>Using the methodology of Chap. 7, it is easy to test the adequacy of the factor
</p>
<p>analysis model by comparing the likelihood under the null (factor analysis) and
</p>
<p>alternative (no constraints on covariance matrix) hypotheses.
</p>
<p>Assuming that OQ and O&permil; are the maximum likelihood estimates corresponding
to (12.13), we obtain the following LR test statistic:
</p>
<p>� 2 log
�
maximised likelihood underH0
</p>
<p>maximised likelihood
</p>
<p>�
D n log
</p>
<p> 
j OQ OQ&gt; C O&permil;j
jSj
</p>
<p>!
; (12.14)
</p>
<p>which asymptotically has the �21
2 f.p�k/2�p�kg
</p>
<p>distribution.</p>
<p/>
</div>
<div class="page"><p/>
<p>370 12 Factor Analysis
</p>
<p>The �2 approximation can be improved if we replace n by n�1�.2pC4kC5/=6
in (12.14) (Bartlett, 1954). Using Bartlett&rsquo;s correction, we reject the factor analysis
</p>
<p>model at the ˛ level if
</p>
<p>fn� 1� .2pC 4kC 5/=6g log
 
j OQ OQ&gt; C O&permil;j
jSj
</p>
<p>!
&gt; �2
</p>
<p>1�˛If.p�k/2�p�kg=2; (12.15)
</p>
<p>and if the number of observations n is large and the number of common factors k is
</p>
<p>such that the �2 statistic has a positive number of degrees of freedom.
</p>
<p>The Method of Principal Factors
</p>
<p>The method of principal factors concentrates on the decomposition of the correla-
</p>
<p>tion matrix R or the covariance matrix S. For simplicity, only the method for the
</p>
<p>correlation matrix R will be discussed. As pointed out in Chap. 11, the spectral
</p>
<p>decompositions of R and S yield different results and therefore, the method of
</p>
<p>principal factors may result in different estimators. The method can be motivated as
</p>
<p>follows: Suppose we know the exact &permil;, then the constraint (12.12) implies that the
</p>
<p>columns of Q are orthogonal since D D I and it implies that they are eigenvectors
ofQQ&gt; D R�&permil;. Furthermore, assume that the first k eigenvalues are positive. In
this case we could calculate Q by means of a spectral decomposition of QQ&gt; and
k would be the number of factors.
</p>
<p>The principal factors algorithm is based on good preliminary estimators Oh2j of the
communalities h2j , for j D 1; : : : ; p. There are two traditional proposals:
</p>
<p>&bull; Oh2j , defined as the square of the multiple correlation coefficient of Xj with .Xl/,
for l 6D j , i.e. �2.V;W Ǒ/ with V D Xj , W D .X`/`&curren;j and where Ǒ is the least
squares regression parameter of a regression of V onW .
</p>
<p>&bull; Oh2j D max
`&curren;j
jrXjX` j, whereR D .rXjX`/ is the correlation matrix of X .
</p>
<p>Given Q jj D 1 � Qh2j we can construct the reduced correlation matrix, R � O&permil;. The
Spectral Decomposition Theorem says that
</p>
<p>R � O&permil; D
pX
</p>
<p>`D1
�`&#13;`&#13;
</p>
<p>&gt;
` ;
</p>
<p>with eigenvalues �1 � � � � � �p . Assume that the first k eigenvalues �1; : : : ; �k are
positive and large compared to the others. Then we can set
</p>
<p>Oq` D
p
�` &#13;` ; ` D 1; : : : ; k</p>
<p/>
</div>
<div class="page"><p/>
<p>12.2 Estimation of the Factor Model 371
</p>
<p>or
</p>
<p>OQ D &#128;1ƒ1=21
</p>
<p>with
</p>
<p>&#128;1 D .&#13;1; : : : ; &#13;k/ and ƒ1 D diag.�1; : : : ; �k/:
</p>
<p>In the next step set
</p>
<p>O jj D 1 �
kX
</p>
<p>`D1
Oq2j ` ; j D 1; : : : ; p:
</p>
<p>Note that the procedure can be iterated: from O jj we can compute a new reduced
correlation matrixR� O&permil; following the same procedure. The iteration usually stops
when the O jj have converged to a stable value.
Example 12.4 Consider once again the car data given in Table 22.7. From Exer-
</p>
<p>cise 11.4 we know that the first PC is mainly influenced by X2&ndash;X7. Moreover, we
</p>
<p>know that most of the variance is already captured by the first PC. Thus we can
</p>
<p>conclude that the data are mainly determined by one factor (k D 1).
The eigenvalues ofR� O&permil; for O&permil; D .max
</p>
<p>j&curren;i
jrXiXj j/ are
</p>
<p>.4:628; 1:340; 1:201; 1:045; 1:007; 0:993; 0:980;�4:028/&gt; :
</p>
<p>It would suffice to choose only one factor. Nevertheless, we have computed two
</p>
<p>factors. The result (the factor loadings for two factors) is shown in Fig. 12.1.
</p>
<p>-1.5 -1 -0.5 0 0.5 1
</p>
<p>-0.6
</p>
<p>-0.4
</p>
<p>-0.2
</p>
<p>0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>economic
</p>
<p>service
</p>
<p>value
</p>
<p>price
</p>
<p>look
sporty
</p>
<p>security
easy
</p>
<p>Car Marks Data
</p>
<p>First Factor
</p>
<p>S
e
</p>
<p>c
o
</p>
<p>n
d
</p>
<p> F
a
</p>
<p>c
to
</p>
<p>r
</p>
<p>Fig. 12.1 Loadings of the evaluated car qualities, factor analysis with k D 2 MVAfactcarm</p>
<p/>
</div>
<div class="page"><p/>
<p>372 12 Factor Analysis
</p>
<p>We can clearly see a cluster of points to the right, which contain the factor
</p>
<p>loadings for the variables X2&ndash;X7. This shows, as did the PCA, that these variables
</p>
<p>are highly dependent and are thus more or less equivalent. The factor loadings for
</p>
<p>X1 (economy) and X8 (easy handling) are separate, but note the different scales on
</p>
<p>the horizontal and vertical axes! Although there are two or three sets of variables
</p>
<p>in the plot, the variance is already explained by the first factor, the &ldquo;price+security&rdquo;
</p>
<p>factor.
</p>
<p>The Principal Component Method
</p>
<p>The principal factor method (PFM) involves finding an approximation Q&permil; of &permil;, the
matrix of specific variances, and then correcting R, the correlation matrix of X ,
</p>
<p>by Q&permil;. The principal component method (PCM) starts with an approximation OQ
of Q, the factor loadings matrix. The sample covariance matrix is diagonalised,
</p>
<p>S D &#128;ƒ&#128;&gt;. Then the first k eigenvectors are retained to build
</p>
<p>OQ D .
p
�1&#13;1; : : : ;
</p>
<p>p
�k&#13;k/: (12.16)
</p>
<p>The estimated specific variances are provided by the diagonal elements of the
</p>
<p>matrix S � OQ OQ&gt;,
</p>
<p>O&permil; D
</p>
<p>0
BBB@
</p>
<p>O 11 0
O 22
</p>
<p>: : :
</p>
<p>0 O pp
</p>
<p>1
CCCA with
</p>
<p>O jj D sXjXj �
kX
</p>
<p>`D1
Oq2j `: (12.17)
</p>
<p>By definition, the diagonal elements of S are equal to the diagonal elements of
OQ OQ&gt;C O&permil;. The off-diagonal elements are not necessarily estimated. How good then
is this approximation? Consider the residual matrix
</p>
<p>S � . OQ OQ&gt; C O&permil;/
</p>
<p>resulting from the principal component solution. Analytically we have that
</p>
<p>X
</p>
<p>i;j
</p>
<p>.S � OQ OQ&gt; � O&permil;/2ij � �2kC1 C � � � C �2p:
</p>
<p>This implies that a small value of the neglected eigenvalues can result in a small
</p>
<p>approximation error. A heuristic device for selecting the number of factors is to</p>
<p/>
</div>
<div class="page"><p/>
<p>12.2 Estimation of the Factor Model 373
</p>
<p>consider the proportion of the total sample variance due to the j th factor. This
</p>
<p>quantity is in general equal to
</p>
<p>(a) �j =
Pp
</p>
<p>jD1 sjj for a factor analysis of S,
(b) �j =p for a factor analysis ofR.
</p>
<p>Example 12.5 This example uses a consumer-preference study from Johnson
</p>
<p>and Wichern (1998). Customers were asked to rate several attributes of a new
</p>
<p>product. The responses were tabulated and the following correlation matrix R was
</p>
<p>constructed:
</p>
<p>Attribute (Variable)
</p>
<p>Taste 1
</p>
<p>Good buy for money 2
</p>
<p>Flavor 3
</p>
<p>Suitable for snack 4
</p>
<p>Provides lots of energy 5
</p>
<p>0
BBBBB@
</p>
<p>1:00 0:02 0:96 0:42 0:01
</p>
<p>0:02 1:00 0:13 0:71 0:85
</p>
<p>0:96 0:13 1:00 0:50 0:11
</p>
<p>0:42 0:71 0:50 1:00 0:79
</p>
<p>0:01 0:85 0:11 0:79 1:00
</p>
<p>1
CCCCCA
</p>
<p>The bold entries ofR show that variables 1 and 3 and variables 2 and 5 are highly
</p>
<p>correlated. Variable 4 is more correlated with variables 2 and 5 than with variables
</p>
<p>1 and 3. Hence, a model with 2 (or 3) factors seems to be reasonable.
</p>
<p>The first two eigenvalues�1 D 2:85 and �2 D 1:81 ofR are the only eigenvalues
greater than one. Moreover, k D 2 common factors account for a cumulative
proportion
</p>
<p>�1 C �2
p
</p>
<p>D 2:85C 1:81
5
</p>
<p>D 0:93
</p>
<p>of the total (standardised) sample variance. Using the PCM, the estimated
</p>
<p>factor loadings, communalities, and specific variances are calculated from
</p>
<p>formulas (12.16) and (12.17), and the results are given in Table 12.1.
</p>
<p>Table 12.1 Estimated factor loadings, communalities, and specific variances
</p>
<p>Estimated factor Specific
</p>
<p>loadings Communalities variances
</p>
<p>Variable Oq1 Oq2 Oh2j O jj D 1� Oh2j
1. Taste 0.56 0:82 0.98 0.02
</p>
<p>2. Good buy for money 0.78 �0:53 0.88 0.12
3. Flavor 0.65 0:75 0.98 0.02
</p>
<p>4. Suitable for snack 0.94 �0:11 0.89 0.11
5. Provides lots of energy 0.80 �0:54 0.93 0.07
Eigenvalues 2.85 1:81
</p>
<p>Cumulative proportion of total
(standardised) sample variance
</p>
<p>0.571 0:932</p>
<p/>
</div>
<div class="page"><p/>
<p>374 12 Factor Analysis
</p>
<p>Take a look at:
</p>
<p>OQ OQ&gt; C O&permil; D
</p>
<p>0
BBBBB@
</p>
<p>0:56 0:82
</p>
<p>0:78 �0:53
0:65 0:75
</p>
<p>0:94 �0:11
0:80 �0:54
</p>
<p>1
CCCCCA
</p>
<p>�
0:56 0:78 0:65 0:94 0:80
</p>
<p>0:82 �0:53 0:75 �0:11 �0:54
</p>
<p>�
</p>
<p>C
</p>
<p>0
BBBBB@
</p>
<p>0:02 0 0 0 0
</p>
<p>0 0:12 0 0 0
</p>
<p>0 0 0:02 0 0
</p>
<p>0 0 0 0:11 0
</p>
<p>0 0 0 0 0:07
</p>
<p>1
CCCCCA
D
</p>
<p>0
BBBBB@
</p>
<p>1:00 0:01 0:97 0:44 0:00
</p>
<p>0:01 1:00 0:11 0:79 0:91
</p>
<p>0:97 0:11 1:00 0:53 0:11
</p>
<p>0:44 0:79 0:53 1:00 0:81
</p>
<p>0:00 0:91 0:11 0:81 1:00
</p>
<p>1
CCCCCA
:
</p>
<p>This nearly reproduces the correlation matrix R. We conclude that the two-
</p>
<p>factor model provides a good fit of the data. The communalities .0:98; 0:88; 0:98;
</p>
<p>0:89; 0:93/ indicate that the two factors account for a large percentage of the
</p>
<p>sample variance of each variable. Due to the nonuniqueness of factor loadings, the
</p>
<p>interpretationmight be enhanced by rotation. This is the topic of the next subsection.
</p>
<p>Rotation
</p>
<p>The constraints (12.11) and (12.12) are given as a matter of mathematical con-
</p>
<p>venience (to create unique solutions) and can therefore complicate the problem
</p>
<p>of interpretation. The interpretation of the loadings would be very simple if the
</p>
<p>variables could be split into disjoint sets, each being associated with one factor.
</p>
<p>A well-known analytical algorithm to rotate the loadings is given by the varimax
</p>
<p>rotation method proposed by Kaiser (1985). In the simplest case of k D 2 factors, a
rotation matrix G is given by
</p>
<p>G.�/ D
�
</p>
<p>cos � sin �
</p>
<p>� sin � cos �
</p>
<p>�
;
</p>
<p>representing a clockwise rotation of the coordinate axes by the angle � . The
</p>
<p>corresponding rotation of loadings is calculated via OQ� D OQG.�/. The idea of
the varimax method is to find the angle � that maximises the sum of the variances
</p>
<p>of the squared loadings Oq�ij within each column of OQ�. More precisely, defining
Qq�jl D Oq�jl = Oh�j , the varimax criterion chooses � so that</p>
<p/>
</div>
<div class="page"><p/>
<p>12.3 Factor Scores and Strategies 375
</p>
<p>V D 1
p
</p>
<p>kX
</p>
<p>`D1
</p>
<p>2
64
</p>
<p>pX
</p>
<p>jD1
</p>
<p>�
Oq�jl
�4 �
</p>
<p>8
&lt;
:
1
</p>
<p>p
</p>
<p>pX
</p>
<p>jD1
</p>
<p>�
Oq�jl
�2
9
=
;
</p>
<p>2
3
75
</p>
<p>is maximised.
</p>
<p>Example 12.6 Let us return to the marketing example of Johnson and Wichern
</p>
<p>(1998) (Example 12.5). The basic factor loadings given in Table 12.1 of the
</p>
<p>first factor and a second factor are almost identical making it difficult to inter-
</p>
<p>pret the factors. Applying the varimax rotation we obtain the loadings Qq1 D
.0:02; 0:94; 0:13; 0:84; 0:97/&gt; and Qq2 D .0:99;�0:01; 0:98; 0:43;�0:02/&gt;. The
high loadings, indicated as bold entries, show that variables 2, 4, 5 define factor 1,
</p>
<p>a nutritional factor. Variables 1 and 3 define factor 2 which might be referred to as a
</p>
<p>taste factor.
</p>
<p>Summary
</p>
<p>,! In practice, Q and &permil; have to be estimated from S D OQ OQ&gt; C O&permil;.
The number of parameters is d D 1
</p>
<p>2
.p � k/2 � 1
</p>
<p>2
.p C k/.
</p>
<p>,! If d D 0, then there exists an exact solution. In practice, d is
usually greater than 0, thus approximations must be considered.
</p>
<p>,! The maximum-likelihood method assumes a normal distribution
for the data. A solution can be found using numerical algorithms.
</p>
<p>,! The method of principal factors is a two-stage method which
calculates OQ from the reduced correlation matrix R � O&permil;, where
O&permil; is a pre-estimate of &permil;. The final estimate of &permil; is found by
O ii D 1 �
</p>
<p>Pk
jD1 Oq2ij.
</p>
<p>,! The PCM is based on an approximation, OQ, of Q.
</p>
<p>,! Often a more informative interpretation of the factors can be found
by rotating the factors.
</p>
<p>,! The varimax rotation chooses a rotation � that maximises
</p>
<p>V D 1
p
</p>
<p>Pk
`D1
</p>
<p>"
Pp
</p>
<p>jD1
</p>
<p>�
Qq�jl
�4
�
�
1
p
</p>
<p>Pp
jD1
</p>
<p>�
Qq�jl
�2� 2
</p>
<p>#
.</p>
<p/>
</div>
<div class="page"><p/>
<p>376 12 Factor Analysis
</p>
<p>12.3 Factor Scores and Strategies
</p>
<p>Up to now strategies have been presented for factor analysis that have concentrated
</p>
<p>on the estimation of loadings and communalities and on their interpretations. This
</p>
<p>was a logical step since the factors F were considered to be normalised random
</p>
<p>sources of information and were explicitly addressed as nonspecific (common
</p>
<p>factors). The estimated values of the factors, called the factor scores, may also be
</p>
<p>useful in the interpretation as well as in the diagnostic analysis. To be more precise,
</p>
<p>the factor scores are estimates of the unobserved random vectors Fl , l D 1; : : : ; k,
for each individual xi , i D 1; : : : ; n. Johnson and Wichern (1998) describe three
methods which in practice yield very similar results. Here, we present the regression
</p>
<p>method which has the advantage of being the simplest technique and is easy to
</p>
<p>implement.
</p>
<p>The idea is to consider the joint distribution of .X��/ andF , and then to proceed
with the regression analysis presented in Chap. 5. Under the factor model (12.4), the
</p>
<p>joint covariance matrix of .X � �/ and F is:
</p>
<p>Var
</p>
<p>�
X � �
F
</p>
<p>�
D
�
QQ&gt; C&permil; Q
</p>
<p>Q&gt; Ik
</p>
<p>�
: (12.18)
</p>
<p>Note that the upper left entry of this matrix equals &dagger; and that the matrix has size
</p>
<p>.p C k/ � .p C k/.
Assuming joint normality, the conditional distribution of F jX is multinormal,
</p>
<p>see Theorem 5.1, with
</p>
<p>E.F jX D x/ D Q&gt;&dagger;�1.X � �/ (12.19)
</p>
<p>and using (5.7) the covariance matrix can be calculated:
</p>
<p>Var.F jX D x/ D Ik �Q&gt;&dagger;�1Q: (12.20)
</p>
<p>In practice, we replace the unknown Q, &dagger; and � by corresponding estimators,
</p>
<p>leading to the estimated individual factor scores:
</p>
<p>Ofi D OQ&gt;S�1.xi � x/: (12.21)
</p>
<p>We prefer to use the original sample covariance matrix S as an estimator of &dagger;,
</p>
<p>instead of the factor analysis approximation OQ OQ&gt; C O&permil;, in order to be more robust
against incorrect determination of the number of factors.
</p>
<p>The same rule can be followed when usingR instead of S. Then (12.18) remains
</p>
<p>valid when standardised variables, i.e.Z D D�1=2&dagger; .X ��/, are considered if D&dagger; D
diag.�11; : : : ; �pp/. In this case the factors are given by
</p>
<p>Ofi D OQ&gt;R�1.zi /; (12.22)</p>
<p/>
</div>
<div class="page"><p/>
<p>12.3 Factor Scores and Strategies 377
</p>
<p>where zi D D�1=2S .xi � x/, OQ is the loading obtained with the matrixR, and DS D
diag.s11; : : : ; spp/.
</p>
<p>If the factors are rotated by the orthogonal matrix G, the factor scores have to be
</p>
<p>rotated accordingly, that is
</p>
<p>Of �i D G&gt; Ofi : (12.23)
</p>
<p>A practical example is presented in Sect. 12.4 using the Boston Housing data.
</p>
<p>Practical Suggestions
</p>
<p>No one method outperforms another in the practical implementation of factor
</p>
<p>analysis. However, by applying a t&acirc;tonnement process, the factor analysis view of
</p>
<p>the data can be stabilised. This motivates the following procedure.
</p>
<p>1. Fix a reasonable number of factors, say k D 2 or 3, based on the correlation
structure of the data and/or screeplot of eigenvalues.
</p>
<p>2. Perform several of the presented methods, including rotation. Compare the
</p>
<p>loadings, communalities, and factor scores from the respective results.
</p>
<p>3. If the results show significant deviations, check for outliers (based on factor
</p>
<p>scores), and consider changing the number of factors k.
</p>
<p>For larger data sets, cross-validation methods are recommended. Such methods
</p>
<p>involve splitting the sample into a training set and a validation data set. On the
</p>
<p>training sample one estimates the factor model with the desired methodology and
</p>
<p>uses the obtained parameters to predict the factor scores for the validation data set.
</p>
<p>The predicted factor scores should be comparable to the factor scores obtained using
</p>
<p>only the validation data set. This stability criterion may also involve the loadings and
</p>
<p>communalities.
</p>
<p>Factor Analysis Versus PCA
</p>
<p>Factor analysis and principal component analysis use the same set of mathematical
</p>
<p>tools (spectral decomposition, projections, : : : ). One could conclude, on first sight,
</p>
<p>that they share the same view and strategy and therefore yield very similar results.
</p>
<p>This is not true. There are substantial differences between these two data analysis
</p>
<p>techniques that we would like to describe here.
</p>
<p>The biggest difference between PCA and factor analysis comes from the model
</p>
<p>philosophy. Factor analysis imposes a strict structure of a fixed number of common
</p>
<p>(latent) factors whereas the PCA determines p factors in decreasing order of
</p>
<p>importance. The most important factor in PCA is the one that maximises the</p>
<p/>
</div>
<div class="page"><p/>
<p>378 12 Factor Analysis
</p>
<p>projected variance. The most important factor in factor analysis is the one that (after
</p>
<p>rotation) gives the maximal interpretation. Often this is different from the direction
</p>
<p>of the first principal component.
</p>
<p>From an implementation point of view, the PCA is based on a well-defined,
</p>
<p>unique algorithm (spectral decomposition), whereas fitting a factor analysis model
</p>
<p>involves a variety of numerical procedures. The non-uniqueness of the factor
</p>
<p>analysis procedure opens the door for subjective interpretation and yields therefore
</p>
<p>a spectrum of results. This data analysis philosophy makes factor analysis difficult
</p>
<p>especially if the model specification involves cross-validation and a data-driven
</p>
<p>selection of the number of factors.
</p>
<p>12.4 Boston Housing
</p>
<p>To illustrate how to implement factor analysis we will use the Boston Housing data
</p>
<p>set and the by now well-known set of transformations. Once again, the variable X4
(Charles River indicator) will be excluded. As before, standardised variables are
</p>
<p>used and the analysis is based on the correlation matrix.
</p>
<p>In Sect. 12.3, we described a practical implementation of factor analysis. Based
</p>
<p>on principal components, three factors were chosen and factor analysis was applied
</p>
<p>using the maximum likelihood method (MLM), the PFM, and the PCM. For
</p>
<p>illustration, the MLM will be presented with and without varimax rotation.
</p>
<p>Table 12.2 gives the MLM factor loadings without rotation and Table 12.3 gives
</p>
<p>the varimax version of this analysis. The corresponding graphical representations
</p>
<p>of the loadings are displayed in Figs. 12.2 and 12.3. We can see that the varimax
</p>
<p>Table 12.2 Estimated factor loadings, communalities, and specific variances, MLM
MVAfacthous
</p>
<p>Estimated factor Specific
</p>
<p>loadings Communalities variances
</p>
<p>Oq1 Oq2 Oq3 Oh2j O jj D 1� Oh2j
1 Crime 0:9295 0:1653 0:1107 0.9036 0.0964
</p>
<p>2 Large lots �0:5823 0:0379 0:2902 0.4248 0.5752
3 Nonretail acres 0:8192 �0:0296 �0:1378 0.6909 0.3091
5 Nitric oxides 0:8789 0:0987 �0:2719 0.8561 0.1439
6 Rooms �0:4447 0:5311 �0:0380 0.4812 0.5188
7 Prior 1940 0:7837 �0:0149 �0:3554 0.7406 0.2594
8 Empl. centers �0:8294 �0:1570 0:4110 0.8816 0.1184
9 Accessibility 0:7955 0:3062 0:4053 0.8908 0.1092
</p>
<p>10 Tax-rate 0:8262 0:1401 0:2906 0.7867 0.2133
</p>
<p>11 Pupil/teacher 0:5051 �0:1850 0:1553 0.3135 0.6865
12 African American 0:4701 �0:0227 �0:1627 0.2480 0.7520
13 Lower status 0:7601 �0:5059 �0:0070 0.8337 0.1663
14 Value �0:6942 0:5904 �0:1798 0.8628 0.1371</p>
<p/>
</div>
<div class="page"><p/>
<p>12.4 Boston Housing 379
</p>
<p>Table 12.3 Estimated factor loadings, communalities, and specific variances, MLM, varimax
rotation MVAfacthous
</p>
<p>Estimated factor Specific
</p>
<p>loadings Communalities variances
</p>
<p>Oq1 Oq2 Oq3 Oh2j O jj D 1� Oh2j
1 Crime 0:7247 �0:2705 �0:5525 0.9036 0.0964
2 Large lots �0:1570 0:2377 0:5858 0.4248 0.5752
3 Nonretail acres 0:4195 �0:3566 �0:6287 0.6909 0.3091
5 Nitric oxides 0:4141 �0:2468 �0:7896 0.8561 0.1439
6 Rooms �0:0799 0:6691 0:1644 0.4812 0.5188
7 Prior 1940 0:2518 �0:2934 �0:7688 0.7406 0.2594
8 Empl. centers �0:3164 0:1515 0:8709 0.8816 0.1184
9 Accessibility 0:8932 �0:1347 �0:2736 0.8908 0.1092
10 Tax-rate 0:7673 �0:2772 �0:3480 0.7867 0.2133
11 Pupil/teacher 0:3405 �0:4065 �0:1800 0.3135 0.6865
12 African American �0:3917 0:2483 0:1813 0.2480 0.7520
13 Lower status 0:2586 �0:7752 �0:4072 0.8337 0.1663
14 Value �0:3043 0:8520 0:2111 0.8630 0.1370
</p>
<p>0.5 0.0 0.5
</p>
<p>0
.6
</p>
<p>0
.2
</p>
<p>0
.2
</p>
<p>0
.6
</p>
<p>Factors21  theta21
</p>
<p>x
</p>
<p>y
</p>
<p>X1
</p>
<p>X2
X3
</p>
<p>X5
</p>
<p>X6
</p>
<p>X7
</p>
<p>X8
</p>
<p>X9
</p>
<p>X10
</p>
<p>X11
</p>
<p>X12
</p>
<p>X13
</p>
<p>X14
</p>
<p>0.5 0.0 0.5
</p>
<p>0
.4
</p>
<p>0
.2
</p>
<p>0
.0
</p>
<p>0
.2
</p>
<p>0
.4
</p>
<p>Factors31  theta31
</p>
<p>x
</p>
<p>y
</p>
<p>X1
</p>
<p>X2
</p>
<p>X3
</p>
<p>X5
</p>
<p>X6
</p>
<p>X7
</p>
<p>X8 X9
</p>
<p>X10
</p>
<p>X11
</p>
<p>X12
</p>
<p>X13
</p>
<p>X14
</p>
<p>0.6 0.2 0.2 0.4 0.6
</p>
<p>0
.4
</p>
<p>0
.2
</p>
<p>0
.0
</p>
<p>0
.2
</p>
<p>0
.4
</p>
<p>Factors32  theta32
</p>
<p>x
</p>
<p>y
</p>
<p>X1
</p>
<p>X2
</p>
<p>X3
</p>
<p>X5
</p>
<p>X6
</p>
<p>X7
</p>
<p>X8 X9
</p>
<p>X10
</p>
<p>X11
</p>
<p>X12
</p>
<p>X13
</p>
<p>X14
</p>
<p>Fig. 12.2 Factor analysis for Boston housing data, MLM MVAfacthous</p>
<p/>
</div>
<div class="page"><p/>
<p>380 12 Factor Analysis
</p>
<p>1.0 0.5 0.0 0.5 1.0
</p>
<p>0
.5
</p>
<p>0
.0
</p>
<p>0
.5
</p>
<p>Factors21  theta21
</p>
<p>x
</p>
<p>y
</p>
<p>X1
</p>
<p>X2
</p>
<p>X3
X5
</p>
<p>X6
</p>
<p>X7
</p>
<p>X8
</p>
<p>X9
X10
</p>
<p>X11
</p>
<p>X12
</p>
<p>X13
</p>
<p>X14
</p>
<p>1.0 0.5 0.0 0.5 1.0
</p>
<p>0
.5
</p>
<p>0
.0
</p>
<p>0
.5
</p>
<p>Factors31  theta31
</p>
<p>x
</p>
<p>y
</p>
<p>X1
</p>
<p>X2
</p>
<p>X3
X5
</p>
<p>X6
</p>
<p>X7
</p>
<p>X8
</p>
<p>X9
X10
</p>
<p>X11
</p>
<p>X12
</p>
<p>X13
</p>
<p>X14
</p>
<p>1.0 0.5 0.0 0.5 1.0
</p>
<p>0
.5
</p>
<p>0
.0
</p>
<p>0
.5
</p>
<p>Factors32  theta32
</p>
<p>x
</p>
<p>y
</p>
<p>X1
</p>
<p>X2
</p>
<p>X3
X5
</p>
<p>X6
</p>
<p>X7
</p>
<p>X8
</p>
<p>X9
X10
</p>
<p>X11
</p>
<p>X12
</p>
<p>X13
</p>
<p>X14
</p>
<p>Fig. 12.3 Factor analysis for Boston housing data, MLM after varimax rotation
MVAfacthous
</p>
<p>does not significantly change the interpretation of the factors obtained by the MLM.
</p>
<p>Factor 1 can be roughly interpreted as a &ldquo;quality of life factor&rdquo; because it is
</p>
<p>positively correlated with variables like X11 and negatively correlated withX8, both
</p>
<p>having low specific variances. The second factor may be interpreted as a &ldquo;residential
</p>
<p>factor&rdquo;, since it is highly correlated with variables X6, and X13. The most striking
</p>
<p>difference between the results with and without varimax rotation can be seen by
</p>
<p>comparing the lower left corners of Figs. 12.2 and 12.3. There is a clear separation
</p>
<p>of the variables in the varimax version of the MLM. Given this arrangement of the
</p>
<p>variables in Fig. 12.3, we can interpret factor 3 as an employment factor, since we
</p>
<p>observe high correlations with X8 and X5.
</p>
<p>We now turn to the PCM and PFM analyses. The results are presented in
</p>
<p>Tables 12.4 and 12.5 and in Figs. 12.4 and 12.5. We would like to focus on the
</p>
<p>PCM, because this three-factormodel yields only one specific variance (unexplained
</p>
<p>variation) above 0.5. Looking at Fig. 12.4, it turns out that factor 1 remains a &ldquo;quality
</p>
<p>of life factor&rdquo; which is clearly visible from the clustering of X5, X3, X10 and X1
on the right-hand side of the graph, while the variables X8, X2, X14, X12 and X6
are on the left-hand side. Again, the second factor is a &ldquo;residential factor&rdquo;, clearly</p>
<p/>
</div>
<div class="page"><p/>
<p>12.4 Boston Housing 381
</p>
<p>Table 12.4 Estimated factor loadings, communalities, and specific variances, PCM, varimax
rotation MVAfacthous
</p>
<p>Estimated factor Specific
</p>
<p>loadings Communalities variances
</p>
<p>Oq1 Oq2 Oq3 Oh2j O jj D 1� Oh2j
1 Crime 0:6034 �0:2456 0:6864 0.8955 0.1045
2 Large lots �0:7722 0:2631 0:0270 0.6661 0.3339
3 Nonretail acres 0:7183 �0:3701 0:3449 0.7719 0.2281
5 Nitric oxides 0:7936 �0:2043 0:4250 0.8521 0.1479
6 Rooms �0:1601 0:8585 0:0218 0.7632 0.2368
7 Prior 1940 0:7895 �0:2375 0:2670 0.7510 0.2490
8 Empl. centers �0:8562 0:1318 �0:3240 0.8554 0.1446
9 Accessibility 0:3681 �0:1268 0:8012 0.7935 0.2065
10 Tax-rate 0:3744 �0:2604 0:7825 0.8203 0.1797
11 Pupil/teacher 0:1982 �0:5124 0:3372 0.4155 0.5845
12 African American 0:1647 0:0368 �0:7002 0.5188 0.4812
13 Lower status 0:4141 �0:7564 0:2781 0.8209 0.1791
14 Value �0:2111 0:8131 �0:3671 0.8394 0.1606
</p>
<p>Table 12.5 Estimated factor loadings, communalities, and specific variances, PFM, varimax
rotation MVAfacthous
</p>
<p>Estimated factor Specific
</p>
<p>loadings Communalities variances
</p>
<p>Oq1 Oq2 Oq3 Oh2j O jj D 1� Oh2j
1 Crime 0:5477 �0:2558 �0:7387 0.9111 0.0889
2 Large lots �0:6148 0:2668 0:1281 0.4655 0.5345
3 Nonretail acres 0:6523 �0:3761 �0:3996 0.7266 0.2734
5 Nitric oxides 0:7723 �0:2291 �0:4412 0.8439 0.1561
6 Rooms �0:1732 0:6783 0:1296 0.0699 0.5046
7 Prior 1940 0:7390 �0:2723 �0:2909 0.7049 0.2951
8 Empl. centers �0:8565 0:1485 0:3395 0.8708 0.1292
9 Accessibility 0:2855 �0:1359 �0:8460 0.8156 0.1844
10 Tax-rate 0:3062 �0:2656 �0:8174 0.8325 0.1675
11 Pupil/teacher 0:2116 �0:3943 �0:3297 0.3090 0.6910
12 African American 0:1994 0:0666 0:4217 0.2433 0.7567
</p>
<p>13 Lower status 0:4005 �0:7743 �0:2706 0.8333 0.1667
14 Value �0:1885 0:8400 0:3473 0.8611 0.1389</p>
<p/>
</div>
<div class="page"><p/>
<p>382 12 Factor Analysis
</p>
<p>0.5 0.0 0.5
</p>
<p>0
.5
</p>
<p>0
.0
</p>
<p>0
.5
</p>
<p>Factors21  theta21
</p>
<p>x
</p>
<p>y
</p>
<p>X1
</p>
<p>X2
</p>
<p>X3
</p>
<p>X5
</p>
<p>X6
</p>
<p>X7
</p>
<p>X8
</p>
<p>X9
X10
</p>
<p>X11
</p>
<p>X12
</p>
<p>X13
</p>
<p>X14
</p>
<p>0.5 0.0 0.5
</p>
<p>0
.5
</p>
<p>0
.0
</p>
<p>0
.5
</p>
<p>Factors31  theta31
</p>
<p>x
</p>
<p>y
</p>
<p>X1
</p>
<p>X2
</p>
<p>X3
X5
</p>
<p>X6
</p>
<p>X7
</p>
<p>X8
</p>
<p>X9X10
</p>
<p>X11
</p>
<p>X12
</p>
<p>X13
</p>
<p>X14
</p>
<p>0.5 0.0 0.5
</p>
<p>0
.5
</p>
<p>0
.0
</p>
<p>0
.5
</p>
<p>Factors32  theta32
</p>
<p>x
</p>
<p>y
</p>
<p>X1
</p>
<p>X2
</p>
<p>X3
X5
</p>
<p>X6
</p>
<p>X7
</p>
<p>X8
</p>
<p>X9X10
</p>
<p>X11
</p>
<p>X12
</p>
<p>X13
</p>
<p>X14
</p>
<p>Fig. 12.4 Factor analysis for Boston housing data, PCM after varimax rotation MVAfacthous
</p>
<p>demonstrated by the location of variablesX6, X14, X11, and X13. The interpretation
</p>
<p>of the third factor is more difficult because all of the loadings (except for X12) are
</p>
<p>very small.
</p>
<p>12.5 Exercises
</p>
<p>Exercise 12.1 In Example 12.4 we have computed OQ and O&permil; using the method of
principal factors. We used a two-step iteration for O&permil;. Perform the third iteration step
and compare the results (i.e. use the given OQ as a pre-estimate to find the final &permil;).
Exercise 12.2 Using the bank data set, how many factors can you find with the
</p>
<p>Method of Principal Factors?
</p>
<p>Exercise 12.3 Repeat Exercise 12.2 with the US company data set!
</p>
<p>Exercise 12.4 Generalise the two-dimensional rotation matrix in Sect. 12.2 to
</p>
<p>n-dimensional space.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.5 Exercises 383
</p>
<p>0.5 0.0 0.5
</p>
<p>0
.5
</p>
<p>0
.0
</p>
<p>0
.5
</p>
<p>Factors21  theta21
</p>
<p>x
</p>
<p>y
</p>
<p>X1
</p>
<p>X2
</p>
<p>X3
X5
</p>
<p>X6
</p>
<p>X7
</p>
<p>X8
</p>
<p>X9
X10
</p>
<p>X11
</p>
<p>X12
</p>
<p>X13
</p>
<p>X14
</p>
<p>0.5 0.0 0.5
</p>
<p>1
.0
</p>
<p>0
.5
</p>
<p>0
.0
</p>
<p>0
.5
</p>
<p>1
.0
</p>
<p>Factors31  theta31
</p>
<p>x
</p>
<p>y
</p>
<p>X1
</p>
<p>X2
</p>
<p>X3X5
</p>
<p>X6
</p>
<p>X7
</p>
<p>X8
</p>
<p>X9X10
</p>
<p>X11
</p>
<p>X12
</p>
<p>X13
</p>
<p>X14
</p>
<p>0.5 0.0 0.5
</p>
<p>1
.0
</p>
<p>0
.5
</p>
<p>0
.0
</p>
<p>0
.5
</p>
<p>1
.0
</p>
<p>Factors32  theta32
</p>
<p>x
</p>
<p>y
</p>
<p>X1
</p>
<p>X2
</p>
<p>X3X5
</p>
<p>X6
</p>
<p>X7
</p>
<p>X8
</p>
<p>X9X10
</p>
<p>X11
</p>
<p>X12
</p>
<p>X13
</p>
<p>X14
</p>
<p>Fig. 12.5 Factor analysis for Boston housing data, PFM after varimax rotation MVAfacthous
</p>
<p>Exercise 12.5 Compute the orthogonal factor model for
</p>
<p>&dagger; D
</p>
<p>0
@
1 0:9 0:7
</p>
<p>0:9 1 0:4
</p>
<p>0:7 0:4 1
</p>
<p>1
A :
</p>
<p>[Solution:  11 D �0:575; q11 D 1:255]
Exercise 12.6 Perform a factor analysis on the type of families in the French food
</p>
<p>data set. Rotate the resulting factors in a way which provides the most reasonable
</p>
<p>interpretation. Compare your result with the varimax method.
</p>
<p>Exercise 12.7 Perform a factor analysis on the variables X3 to X9 in the US crime
</p>
<p>data set (Table 22.10). Would it make sense to use all of the variables for the
</p>
<p>analysis?
</p>
<p>Exercise 12.8 Analyse the athletic records data set (Table 22.18). Can you recog-
</p>
<p>nise any patterns if you sort the countries according to the estimates of the factor
</p>
<p>scores?</p>
<p/>
</div>
<div class="page"><p/>
<p>384 12 Factor Analysis
</p>
<p>Exercise 12.9 Perform a factor analysis on the US health data set (Table 22.16)
</p>
<p>and estimate the factor scores.
</p>
<p>Exercise 12.10 Redo Exercise 12.9 using the US crime data in Table 22.10.
</p>
<p>Compare the estimated factor scores of the two data sets.
</p>
<p>Exercise 12.11 Analyse the vocabulary data given in Table 22.17.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 13
</p>
<p>Cluster Analysis
</p>
<p>The next two chapters address classification issues from two varying perspectives.
</p>
<p>When considering groups of objects in a multivariate data set, two situations can
</p>
<p>arise. Given a data set containing measurements on individuals, in some cases we
</p>
<p>want to see if some natural groups or classes of individuals exist, and in other
</p>
<p>cases, we want to classify the individuals according to a set of existing groups.
</p>
<p>Cluster analysis develops tools and methods concerning the former case, that is,
</p>
<p>given a data matrix containing multivariate measurements on a large number of
</p>
<p>individuals (or objects), the objective is to build some natural sub-groups or clusters
</p>
<p>of individuals. This is done by grouping individuals that are &ldquo;similar&rdquo; according to
</p>
<p>some appropriate criterion. Once the clusters are obtained, it is generally useful to
</p>
<p>describe each group using some descriptive tool from Chaps. 1, 10 or 11 to create a
</p>
<p>better understanding of the differences that exist among the formulated groups.
</p>
<p>Cluster analysis is applied in many fields such as the natural sciences, the medical
</p>
<p>sciences, economics, marketing, etc. In marketing, for instance, it is useful to
</p>
<p>build and describe the different segments of a market from a survey on potential
</p>
<p>consumers. An insurance company, on the other hand, might be interested in the
</p>
<p>distinction among classes of potential customers so that it can derive optimal prices
</p>
<p>for its services. Other examples are provided below.
</p>
<p>Discriminant analysis presented in Chap. 14 addresses the other issue of clas-
</p>
<p>sification. It focuses on situations where the different groups are known a priori.
</p>
<p>Decision rules are provided in classifying a multivariate observation into one of the
</p>
<p>known groups.
</p>
<p>Section 13.1 states the problem of cluster analysis where the criterion chosen to
</p>
<p>measure the similarity among objects clearly plays an important role. Section 13.2
</p>
<p>&copy; Springer-Verlag Berlin Heidelberg 2015
W.K. H&auml;rdle, L. Simar, Applied Multivariate Statistical Analysis,
DOI 10.1007/978-3-662-45171-7_13
</p>
<p>385</p>
<p/>
</div>
<div class="page"><p/>
<p>386 13 Cluster Analysis
</p>
<p>shows how to precisely measure the proximity between objects. Finally, Sect. 13.3
</p>
<p>provides some algorithms. We will concentrate on hierarchical algorithms only
</p>
<p>where the number of clusters is not known in advance.
</p>
<p>13.1 The Problem
</p>
<p>Cluster analysis is a set of tools for building groups (clusters) from multivariate
</p>
<p>data objects. The aim is to construct groups with homogeneous properties out of
</p>
<p>heterogeneous large samples. The groups or clusters should be as homogeneous as
</p>
<p>possible and the differences among the various groups as large as possible. Cluster
</p>
<p>analysis can be divided into two fundamental steps.
</p>
<p>1. Choice of a proximity measure:
</p>
<p>One checks each pair of observations (objects) for the similarity of their values.
</p>
<p>A similarity (proximity) measure is defined to measure the &ldquo;closeness&rdquo; of the
</p>
<p>objects. The &ldquo;closer&rdquo; they are, the more homogeneous they are.
</p>
<p>2. Choice of group-building algorithm:
</p>
<p>On the basis of the proximity measures the objects assigned to groups so that
</p>
<p>differences between groups become large and observations in a group become as
</p>
<p>close as possible.
</p>
<p>In marketing, for example, cluster analysis is used to select test markets. Other
</p>
<p>applications include the classification of companies according to their organisational
</p>
<p>structures, technologies and types. In psychology, cluster analysis is used to find
</p>
<p>types of personalities on the basis of questionnaires. In archaeology, it is applied
</p>
<p>to classify art objects in different time periods. Other scientific branches that use
</p>
<p>cluster analysis are medicine, sociology, linguistics and biology. In each case a
</p>
<p>heterogeneous sample of objects are analysed with the aim to identify homogeneous
</p>
<p>sub-groups.
</p>
<p>Summary
</p>
<p>,! Cluster analysis is a set of tools for building groups (clusters) from
multivariate data objects.
</p>
<p>,! The methods used are usually divided into two fundamental steps:
The choice of a proximity measure and the choice of a group-
</p>
<p>building algorithm.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.2 The Proximity Between Objects 387
</p>
<p>13.2 The Proximity Between Objects
</p>
<p>The starting point of a cluster analysis is a data matrix X .n � p/ with n
measurements (objects) of p variables. The proximity (similarity) among objects
</p>
<p>is described by a matrix D.n � n/
</p>
<p>D D
</p>
<p>0
BBBBBBBBBB@
</p>
<p>d11 d12 : : : : : : : : : d1n
::: d22
</p>
<p>:::
:::
</p>
<p>:::
: : :
</p>
<p>:::
:::
</p>
<p>:::
: : :
</p>
<p>:::
:::
</p>
<p>:::
: : :
</p>
<p>:::
</p>
<p>dn1 dn2 : : : : : : : : : dnn
</p>
<p>1
CCCCCCCCCCA
</p>
<p>: (13.1)
</p>
<p>ThematrixD containsmeasures of similarity or dissimilarity among the n objects. If
</p>
<p>the values dij are distances, then theymeasure dissimilarity. The greater the distance,
</p>
<p>the less similar are the objects. If the values dij are proximity measures, then the
</p>
<p>opposite is true, i.e. the greater the proximity value, the more similar are the objects.
</p>
<p>A distance matrix, for example, could be defined by the L2-norm: dij D kxi �xjk2,
where xi and xj denote the rows of the data matrixX . Distance and similarity are of
</p>
<p>course dual. If dij is a distance, then d
0
ij D maxi;j fdijg � dij is a proximity measure.
</p>
<p>The nature of the observations plays an important role in the choice of proximity
</p>
<p>measure. Nominal values (like binary variables) lead in general to proximity values,
</p>
<p>whereas metric values lead (in general) to distance matrices. We first present
</p>
<p>possibilities for D in the binary case and then consider the continuous case.
</p>
<p>Similarity of Objects with Binary Structure
</p>
<p>In order to measure the similarity between objects we always compare pairs of
</p>
<p>observations .xi ; xj /where x
&gt;
i D .xi1; : : : ; xip/, x&gt;j D .xj1; : : : ; xjp/, and xik; xjk 2
</p>
<p>f0; 1g. Obviously there are four cases:
</p>
<p>xik D xjk D 1;
xik D 0; xjk D 1;
xik D 1; xjk D 0;
xik D xjk D 0:</p>
<p/>
</div>
<div class="page"><p/>
<p>388 13 Cluster Analysis
</p>
<p>Define
</p>
<p>a1 D
pX
</p>
<p>kD1
I.xik D xjk D 1/;
</p>
<p>a2 D
pX
</p>
<p>kD1
I.xik D 0; xjk D 1/;
</p>
<p>a3 D
pX
</p>
<p>kD1
I.xik D 1; xjk D 0/;
</p>
<p>a4 D
pX
</p>
<p>kD1
I.xik D xjk D 0/:
</p>
<p>Note that each al ; l D 1; : : : ; 4, depends on the pair .xi ; xj /.
The following proximity measures are used in practice:
</p>
<p>dij D
a1 C ıa4
</p>
<p>a1 C ıa4 C �.a2 C a3/
(13.2)
</p>
<p>where ı and � are weighting factors. Table 13.1 shows some similarity measures for
</p>
<p>given weighting factors.
</p>
<p>These measures provide alternative ways of weighting mismatching and positive
</p>
<p>(presence of a common character) or negative (absence of a common character)
</p>
<p>matchings. In principle, we could also consider the Euclidean distance. However,
</p>
<p>the disadvantage of this distance is that it treats the observations 0 and 1 in the same
</p>
<p>way. If xik D 1 denotes, say, knowledge of a certain language, then the contrary,
xik D 0 (not knowing the language) should eventually be treated differently.
Example 13.1 Let us consider binary variables computed from the car data set
</p>
<p>(Table 22.7). We define the new binary data by
</p>
<p>yik D
�
1 if xik &gt; xk ;
</p>
<p>0 otherwise,
</p>
<p>Table 13.1 The common
similarity coefficients
</p>
<p>Name ı � Definition
</p>
<p>Jaccard 0 1
a1
</p>
<p>a1 C a2 C a3
Tanimoto 1 2
</p>
<p>a1 C a4
a1 C 2.a2 C a3/C a4
</p>
<p>Simple matching (M) 1 1
a1 C a4
p
</p>
<p>Russel and Rao (RR) &ndash; &ndash;
a1
p
</p>
<p>Dice 0 0.5
2a1
</p>
<p>2a1 C .a2 C a3/
Kulczynski &ndash; &ndash;
</p>
<p>a1
a2 C a3</p>
<p/>
</div>
<div class="page"><p/>
<p>13.2 The Proximity Between Objects 389
</p>
<p>for i D 1; : : : ; n and k D 1; : : : ; p. This means that we transform the observations
of the k-th variable to 1 if it is larger than the mean value of all observations of the
</p>
<p>k-th variable. Let us only consider the data points 17 to 19 (Renault 19, Rover and
</p>
<p>Toyota Corolla) which lead to .3� 3/ distance matrices. The Jaccard measure gives
the similarity matrix
</p>
<p>D D
</p>
<p>0
@
1:000 0:000 0:400
</p>
<p>1:000 0:167
</p>
<p>1:000
</p>
<p>1
A ;
</p>
<p>the Tanimoto measure yields
</p>
<p>D D
</p>
<p>0
@
1:000 0:000 0:455
</p>
<p>1:000 0:231
</p>
<p>1:000
</p>
<p>1
A ;
</p>
<p>whereas the Simple Matching measure gives
</p>
<p>D D
</p>
<p>0
@
1:000 0:000 0:625
</p>
<p>1:000 0:375
</p>
<p>1:000
</p>
<p>1
A :
</p>
<p>Distance Measures for Continuous Variables
</p>
<p>A wide variety of distance measures can be generated by the Lr -norms, r � 1,
</p>
<p>dij D jjxi � xj jjr D
(
</p>
<p>pX
</p>
<p>kD1
jxik � xjkjr
</p>
<p>) 1=r
: (13.3)
</p>
<p>Here xik denotes the value of the k-th variable on object i . It is clear that dii D 0 for
i D 1; : : : ; n. The class of distances (13.3) for varying r measures the dissimilarity
of different weights. The L1-metric, for example, gives less weight to outliers than
</p>
<p>the L2-norm (Euclidean norm). It is common to consider the squared L2-norm.
</p>
<p>Example 13.2 Suppose we have x1 D .0; 0/; x2 D .1; 0/ and x3 D .5; 5/. Then the
distance matrix for the L1-norm is
</p>
<p>D1 D
</p>
<p>0
@
</p>
<p>0 1 10
</p>
<p>1 0 9
</p>
<p>10 9 0
</p>
<p>1
A ;</p>
<p/>
</div>
<div class="page"><p/>
<p>390 13 Cluster Analysis
</p>
<p>and for the squared L2- or Euclidean norm
</p>
<p>D2 D
</p>
<p>0
@
</p>
<p>0 1 50
</p>
<p>1 0 41
</p>
<p>50 41 0
</p>
<p>1
A :
</p>
<p>One can see that the third observation x3 receives much more weight in the squared
</p>
<p>L2-norm than in the L1-norm.
</p>
<p>An underlying assumption in applying distances based on Lr -norms is that the
</p>
<p>variables are measured on the same scale. If this is not the case, a standardisation
</p>
<p>should first be applied. This corresponds to using a more general L2- or Euclidean
</p>
<p>norm with a metric A, where A &gt; 0 (see Sect. 2.6):
</p>
<p>d 2ij D kxi � xj kA D .xi � xj /&gt;A.xi � xj /: (13.4)
</p>
<p>L2-norms are given by A D Ip , but if a standardisation is desired, then the
weight matrix A D diag.s�1X1X1 ; : : : ; s
</p>
<p>�1
XpXp
</p>
<p>/ may be suitable. Recall that sXkXk is
</p>
<p>the variance of the k-th component. Hence we have
</p>
<p>d 2ij D
pX
</p>
<p>kD1
</p>
<p>.xik � xjk/2
sXkXk
</p>
<p>: (13.5)
</p>
<p>Here each component has the same weight in the computation of the distances and
</p>
<p>the distances do not depend on a particular choice of the units of measure.
</p>
<p>Example 13.3 Consider the French Food expenditures (Table 22.6). The Euclidean
</p>
<p>distance matrix (squared L2-norm) is
</p>
<p>D D 104�
</p>
<p>0
BBBBBBBBBBBBBBBBBBBBB@
</p>
<p>0:00 5:82 58:19 3:54 5:15 151:44 16:91 36:15 147:99 51:84 102:56 271:83
</p>
<p>0:00 41:73 4:53 2:93 120:59 13:52 25:39 116:31 43:68 76:81 226:87
</p>
<p>0:00 44:14 40:10 24:12 29:95 8:17 25:57 20:81 20:30 88:62
</p>
<p>0:00 0:76 127:85 5:62 21:70 124:98 31:21 72:97 231:57
</p>
<p>0:00 121:05 5:70 19:85 118:77 30:82 67:39 220:72
</p>
<p>0:00 96:57 48:16 1:80 60:52 28:90 29:56
</p>
<p>0:00 9:20 94:87 11:07 42:12 179:84
</p>
<p>0:00 46:95 6:17 18:76 113:03
</p>
<p>0:00 61:08 29:62 31:86
</p>
<p>0:00 15:83 116:11
</p>
<p>0:00 53:77
</p>
<p>0:00
</p>
<p>1
CCCCCCCCCCCCCCCCCCCCCA
</p>
<p>:</p>
<p/>
</div>
<div class="page"><p/>
<p>13.2 The Proximity Between Objects 391
</p>
<p>Taking the weight matrixA D diag.s�1X1X1 ; : : : ; s
�1
X7X7
</p>
<p>/, we obtain the distance matrix
</p>
<p>(squared L2-norm)
</p>
<p>D D
</p>
<p>0
BBBBBBBBBBBBBBBBBBBB@
</p>
<p>0:00 6:85 10:04 1:68 2:66 24:90 8:28 8:56 24:61 21:55 30:68 57:48
</p>
<p>0:00 13:11 6:59 3:75 20:12 13:13 12:38 15:88 31:52 25:65 46:64
</p>
<p>0:00 8:03 7:27 4:99 9:27 3:88 7:46 14:92 15:08 26:89
</p>
<p>0:00 0:64 20:06 2:76 3:82 19:63 12:81 19:28 45:01
</p>
<p>0:00 17:00 3:54 3:81 15:76 14:98 16:89 39:87
</p>
<p>0:00 17:51 9:79 1:58 21:32 11:36 13:40
</p>
<p>0:00 1:80 17:92 4:39 9:93 33:61
</p>
<p>0:00 10:50 5:70 7:97 24:41
</p>
<p>0:00 24:75 11:02 13:07
</p>
<p>0:00 9:13 29:78
</p>
<p>0:00 9:39
</p>
<p>0:00
</p>
<p>1
CCCCCCCCCCCCCCCCCCCCA
</p>
<p>:
</p>
<p>(13.6)
</p>
<p>When applied to contingency tables, a �2-metric is suitable to compare (and
</p>
<p>cluster) rows and columns of a contingency table.
</p>
<p>If X is a contingency table, row i is characterised by the conditional frequency
</p>
<p>distribution
xij
xi�
</p>
<p>, where xi� D
Pp
</p>
<p>jD1 xij indicates the marginal distributions over
the rows: xi�
</p>
<p>x��
; x�� D
</p>
<p>Pn
iD1 xi�. Similarly, column j of X is characterised by the
</p>
<p>conditional frequencies
xij
x�j
</p>
<p>, where x�j D
Pn
</p>
<p>iD1 xij. The marginal frequencies of
</p>
<p>the columns are
x�j
x��
</p>
<p>.
</p>
<p>The distance between two rows, i1 and i2, corresponds to the distance between
</p>
<p>their respective frequency distributions. It is common to define this distance using
</p>
<p>the �2-metric:
</p>
<p>d 2.i1; i2/ D
pX
</p>
<p>jD1
</p>
<p>1�
x�j
x��
</p>
<p>�
�
xi1j
</p>
<p>xi1�
� xi2j
xi2�
</p>
<p>�2
: (13.7)
</p>
<p>Note that this can be expressed as a distance between the vectors x1 D
�
xi1j
x��
</p>
<p>�
and
</p>
<p>x2 D
�
xi2j
</p>
<p>x��
</p>
<p>�
as in (13.4) with weighting matrix A D
</p>
<p>n
diag
</p>
<p>�
x�j
x��
</p>
<p>�o�1
. Similarly, if
</p>
<p>we are interested in clusters among the columns, we can define:
</p>
<p>d 2.j1; j2/ D
nX
</p>
<p>iD1
</p>
<p>1�
xi�
x��
</p>
<p>�
�
xij1
x�j1
� xij2
x�j2
</p>
<p>�2
:</p>
<p/>
</div>
<div class="page"><p/>
<p>392 13 Cluster Analysis
</p>
<p>Apart from the Euclidean and the Lr -norm measures one can use a proximity
</p>
<p>measure such as theQ-correlation coefficient
</p>
<p>dij D
Pp
</p>
<p>kD1.xik � xi /.xjk � xj /˚Pp
kD1.xik � xi /2
</p>
<p>Pp
kD1.xjk � xj /2
</p>
<p>�1=2 : (13.8)
</p>
<p>Here xi denotes the mean over the variables .xi1; : : : ; xip/.
</p>
<p>Summary
</p>
<p>,! The proximity between data points is measured by a distance
or similarity matrix D whose components dij give the similarity
</p>
<p>coefficient or the distance between two points xi and xj .
</p>
<p>,! A variety of similarity (distance) measures exist for binary data
(e.g. Jaccard, Tanimoto, Simple Matching coefficients) and for
</p>
<p>continuous data (e.g. Lr -norms).
</p>
<p>,! The nature of the data could impose the choice of a particular
metricA in defining the distances (standardisation, �2-metric etc.).
</p>
<p>13.3 Cluster Algorithms
</p>
<p>There are essentially two types of clustering methods: hierarchical algorithms and
</p>
<p>partitioning algorithms. The hierarchical algorithms can be divided into agglomer-
</p>
<p>ative and splitting procedures. The first type of hierarchical clustering starts from
</p>
<p>the finest partition possible (each observation forms a cluster) and groups them. The
</p>
<p>second type starts with the coarsest partition possible: one cluster contains all of the
</p>
<p>observations. It proceeds by splitting the single cluster up into smaller sized clusters.
</p>
<p>The partitioning algorithms start from a given group definition and proceed by
</p>
<p>exchanging elements between groups until a certain score is optimised. The main
</p>
<p>difference between the two clustering techniques is that in hierarchical clustering
</p>
<p>once groups are found and elements are assigned to the groups, this assignment
</p>
<p>cannot be changed. In partitioning techniques, on the other hand, the assignment of
</p>
<p>objects into groups may change during the algorithm application.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.3 Cluster Algorithms 393
</p>
<p>Hierarchical Algorithms, Agglomerative Techniques
</p>
<p>Agglomerative algorithms are used quite frequently in practice. The algorithm
</p>
<p>consists of the following steps:
</p>
<p>Algorithm Hierarchical algorithms-agglomerative technique
</p>
<p>1: Construct the finest partition
2: Compute the distance matrix D.
3: repeat
4: Find the two clusters with the closest distance
5: Put those two clusters into one cluster
6: Compute the distance between the new groups and obtain a reduced distance matrix D
7: until all clusters are agglomerated into X
</p>
<p>If two objects or groups say, P and Q, are united, one computes the distance
</p>
<p>between this new group (object) P CQ and group R using the following distance
function:
</p>
<p>d.R;P CQ/ D ı1d.R;P /C ı2d.R;Q/C ı3d.P;Q/C ı4jd.R;P /� d.R;Q/j:
(13.9)
</p>
<p>The ıj &rsquo;s are weighting factors that lead to different agglomerative algorithms as
</p>
<p>described in Table 13.2. Here nP D
Pn
</p>
<p>iD1 I.xi 2 P/ is the number of objects in
group P . The values of nQ and nR are defined analogously.
</p>
<p>For the most common used Single and Complete linkages, below are the modified
</p>
<p>agglomerative algorithm steps:
</p>
<p>As instead of computing new distance matrixes every step, a linear search in the
</p>
<p>original distancematrix is enough for clustering in the modified algorithm, it is more
</p>
<p>efficient in practice.
</p>
<p>Table 13.2 Computations of group distances
</p>
<p>Name ı1 ı2 ı3 ı4
</p>
<p>Single linkage 1/2 1/2 0 �1/2
Complete linkage 1/2 1/2 0 1/2
</p>
<p>Average linkage (unweighted) 1/2 1/2 0 0
</p>
<p>Average linkage (weighted)
nP
</p>
<p>nP C nQ
nQ
</p>
<p>nP C nQ 0 0
</p>
<p>Centroid
nP
</p>
<p>nP C nQ
nQ
</p>
<p>nP C nQ �
nPnQ
</p>
<p>.nP C nQ/2
0
</p>
<p>Median 1/2 1/2 �1=4 0
Ward
</p>
<p>nR C nP
nR C nP C nQ
</p>
<p>nR C nQ
nR C nP C nQ �
</p>
<p>nR
nR C nP C nQ 0</p>
<p/>
</div>
<div class="page"><p/>
<p>394 13 Cluster Analysis
</p>
<p>Algorithm Modified hierarchical algorithms-agglomerative technique
</p>
<p>1: Construct the finest partition
2: Compute the distance matrix D.
</p>
<p>3: repeat
4: Find the smallest (Single linkage)/ largest (Complete linkage) value d (between objects m
</p>
<p>and n) in D
5: Ifm and n are not in the same cluster, combine the clusters m and n belonging to together,
</p>
<p>and delete the smallest value
6: until all clusters are agglomerated into X or the value d exceeds the preset level
</p>
<p>Example 13.4 Let us examine the agglomerative algorithm for the three points in
</p>
<p>Example 13.2, x1 D .0; 0/, x2 D .1; 0/ and x3 D .5; 5/, and the squared Euclidean
distance matrix with single linkage weighting. The algorithm starts with N D 3
clusters: P D fx1g, Q D fx2g and R D fx3g. The distance matrix D2 is given in
Example 13.2. The smallest distance in D2 is the one between the clusters P and
</p>
<p>Q. Therefore, applying step 4 in the above algorithm we combine these clusters to
</p>
<p>form P C Q D fx1; x2g. The single linkage distance between the remaining two
clusters is from Table 13.2 and (13.9) equal to
</p>
<p>d.R;P CQ/ D 1
2
d.R;P /C 1
</p>
<p>2
d.R;Q/� 1
</p>
<p>2
jd.R;P / � d.R;Q/j
</p>
<p>D 1
2
d13 C
</p>
<p>1
</p>
<p>2
d23 �
</p>
<p>1
</p>
<p>2
� jd13 � d23j
</p>
<p>D 50
2
C 41
</p>
<p>2
� 1
2
� j50� 41j
</p>
<p>D 41: (13.10)
</p>
<p>The reduced distance matrix is then
�
0
41
41
0
</p>
<p>�
. The next and last step is to unite the
</p>
<p>clusters R and P CQ into a single cluster X , the original data matrix.
When there are more data points than in the example above, a visualisation of
</p>
<p>the implication of clusters is desirable. A graphical representation of the sequence
</p>
<p>of clustering is called a dendrogram. It displays the observations, the sequence of
</p>
<p>clusters and the distances between the clusters. The vertical axis displays the indices
</p>
<p>of the points, whereas the horizontal axis gives the distance between the clusters.
</p>
<p>Large distances indicate the clustering of heterogeneous groups. Thus, if we choose
</p>
<p>to &ldquo;cut the tree&rdquo; at a desired level, the branches describe the corresponding clusters.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.3 Cluster Algorithms 395
</p>
<p>Fig. 13.1 The 8-point
example MVAclus8p
</p>
<p>&minus;2 0 2 4
</p>
<p>&minus;
4
</p>
<p>&minus;
2
</p>
<p>0
2
</p>
<p>4
</p>
<p>8 points
</p>
<p>first coordinate
</p>
<p>s
e
</p>
<p>c
o
</p>
<p>n
d
</p>
<p> c
o
</p>
<p>o
rd
</p>
<p>in
a
</p>
<p>te
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>4
</p>
<p>5
</p>
<p>6
</p>
<p>7
</p>
<p>8
</p>
<p>Example 13.5 Here we describe the single linkage algorithm for the eight data
</p>
<p>points displayed in Fig. 13.1. The distance matrix (L2-norms) is
</p>
<p>D D
</p>
<p>0
BBBBBBBBBBB@
</p>
<p>0 10 53 73 50 98 41 65
</p>
<p>0 25 41 20 80 37 65
</p>
<p>0 2 1 25 18 34
</p>
<p>0 5 17 20 32
</p>
<p>0 36 25 45
</p>
<p>0 13 9
</p>
<p>0 4
</p>
<p>0
</p>
<p>1
CCCCCCCCCCCA
</p>
<p>and the dendrogram is shown in Fig. 13.2.
</p>
<p>If we decide to cut the tree at the level 10, three clusters are defined: f1; 2g,
f3; 4; 5g and f6; 7; 8g.
</p>
<p>The single linkage algorithm defines the distance between two groups as the
</p>
<p>smallest value of the individual distances. Table 13.2 shows that in this case
</p>
<p>d.R;P CQ/ D minfd.R;P /; d.R;Q/g: (13.11)
</p>
<p>This algorithm is also called the Nearest Neighbour algorithm. As a consequence
</p>
<p>of its construction, single linkage tends to build large groups. Groups that differ but
</p>
<p>are not well separated may thus be classified into one group as long as they have</p>
<p/>
</div>
<div class="page"><p/>
<p>396 13 Cluster Analysis
</p>
<p>Fig. 13.2 The dendrogram
for the 8-point example,
single linkage algorithm
MVAclus8p
</p>
<p>1 2 4 3 5 6 7 8
</p>
<p>0
5
</p>
<p>1
0
</p>
<p>1
5
</p>
<p>2
0
</p>
<p>Single Linkage Dendrogram &minus; 8 points
</p>
<p>S
q
</p>
<p>u
a
</p>
<p>re
d
</p>
<p> E
u
</p>
<p>c
lid
</p>
<p>e
a
</p>
<p>n
 D
</p>
<p>is
ta
</p>
<p>n
c
e
</p>
<p>two approximate points. The complete linkage algorithm tries to correct this kind
</p>
<p>of grouping by considering the largest (individual) distances. Indeed, the complete
</p>
<p>linkage distance can be written as
</p>
<p>d.R;P CQ/ D maxfd.R;P /; d.R;Q/g: (13.12)
</p>
<p>It is also called the Farthest Neighbour algorithm. This algorithm will cluster
</p>
<p>groups where all the points are proximate, since it compares the largest distances.
</p>
<p>The average linkage algorithm (weighted or unweighted) proposes a compromise
</p>
<p>between the two preceding algorithms, in that it computes an average distance:
</p>
<p>d.R;P CQ/ D nP
nP C nQ
</p>
<p>d.R;P /C nQ
nP C nQ
</p>
<p>d.R;Q/: (13.13)
</p>
<p>The centroid algorithm is quite similar to the average linkage algorithm and uses
</p>
<p>the natural geometrical distance between R and the weighted centre of gravity of P
</p>
<p>andQ (see Fig. 13.3):
</p>
<p>d.R;P CQ/ D nP
nP C nQ
</p>
<p>d.R;P /C nQ
nP C nQ
</p>
<p>d.R;Q/� nP nQ
.nP C nQ/2
</p>
<p>d.P;Q/:
</p>
<p>(13.14)
</p>
<p>TheWard clustering algorithm computes the distance between groups according
</p>
<p>to the formula in Table 13.2. The main difference between this algorithm and the
</p>
<p>linkage procedures is in the unification procedure. The Ward algorithm does not put
</p>
<p>together groups with smallest distance. Instead, it joins groups that do not increase
</p>
<p>a given measure of heterogeneity &ldquo;too much&rdquo;. The aim of the Ward procedure is</p>
<p/>
</div>
<div class="page"><p/>
<p>13.3 Cluster Algorithms 397
</p>
<p>Fig. 13.3 The centroid
algorithm
</p>
<p>to unify groups such that the variation inside these groups does not increase too
</p>
<p>drastically: the resulting groups are as homogeneous as possible.
</p>
<p>The heterogeneity of group R is measured by the inertia inside the group. This
</p>
<p>inertia is defined as follows:
</p>
<p>IR D
1
</p>
<p>nR
</p>
<p>nRX
</p>
<p>iD1
d 2.xi ; xR/ (13.15)
</p>
<p>where xR is the centre of gravity (mean) over the groups. IR clearly provides a
</p>
<p>scalar measure of the dispersion of the group around its centre of gravity. If the
</p>
<p>usual Euclidean distance is used, then IR represents the sum of the variances of the
</p>
<p>p components of xi inside groupR.
</p>
<p>When two objects or groups P and Q are joined, the new group P CQ has a
larger inertia IPCQ. It can be shown that the corresponding increase of inertia is
given by
</p>
<p>&#129;.P;Q/ D nPnQ
nP C nQ
</p>
<p>d 2.P;Q/: (13.16)
</p>
<p>In this case, the Ward algorithm is defined as an algorithm that &ldquo;joins the groups
</p>
<p>that give the smallest increase in &#129;.P;Q/&rdquo;. It is easy to prove that when P andQ
</p>
<p>are joined, the new criterion values are given by (13.9) along with the values of ıi
given in Table 13.2, when the centroid formula is used to modify d 2.R; P C Q/.
So, the Ward algorithm is related to the centroid algorithm, but with an &ldquo;inertial&rdquo;
</p>
<p>distance &#129; rather than the &ldquo;geometric&rdquo; distance d 2.
</p>
<p>As pointed out in Sect. 13.2, all the algorithms above can be adjusted by the
</p>
<p>choice of the metric A defining the geometric distance d 2. If the results of a
</p>
<p>clustering algorithm are illustrated as graphical representations of individuals in
</p>
<p>spaces of low dimension (using principal components (normalised or not) or using
</p>
<p>a correspondence analysis for contingency tables), it is important to be coherent in
</p>
<p>the choice of the metric used.</p>
<p/>
</div>
<div class="page"><p/>
<p>398 13 Cluster Analysis
</p>
<p>Fig. 13.4 PCA for 20
randomly chosen bank notes
MVAclusbank
</p>
<p>&minus;4 &minus;2 0 2 4
</p>
<p>&minus;
4
</p>
<p>&minus;
2
</p>
<p>0
2
</p>
<p>4
</p>
<p>20 Swiss bank notes
</p>
<p>first PC
</p>
<p>s
e
</p>
<p>c
o
</p>
<p>n
d
</p>
<p> P
C 34
</p>
<p>161
</p>
<p>7765
119
</p>
<p>118
</p>
<p>25
</p>
<p>57
</p>
<p>111
</p>
<p>121
</p>
<p>98
</p>
<p>96
</p>
<p>101
</p>
<p>105
</p>
<p>162
</p>
<p>15421
</p>
<p>129 164
51
</p>
<p>Fig. 13.5 The dendrogram
for the 20 bank notes, Ward
algorithm MVAclusbank
</p>
<p>1
1
</p>
<p>8
</p>
<p>1
5
</p>
<p>4
</p>
<p>1
0
</p>
<p>5
</p>
<p>1
6
</p>
<p>4
</p>
<p>1
6
</p>
<p>2
</p>
<p>1
2
</p>
<p>1
</p>
<p>1
0
</p>
<p>1
</p>
<p>1
1
</p>
<p>9
</p>
<p>1
2
</p>
<p>9
</p>
<p>1
6
</p>
<p>1
</p>
<p>1
1
</p>
<p>1
</p>
<p>3
4
</p>
<p>7
7
</p>
<p>6
5
</p>
<p>5
7
</p>
<p>9
6
</p>
<p>9
8
</p>
<p>2
1
</p>
<p>2
5
</p>
<p>5
1
</p>
<p>0
2
</p>
<p>0
4
</p>
<p>0
6
</p>
<p>0
8
</p>
<p>0
1
</p>
<p>0
0
</p>
<p>Dendrogram for 20 Swiss bank notes
</p>
<p>Ward algorithm
</p>
<p>S
q
</p>
<p>u
a
</p>
<p>re
d
</p>
<p> E
u
</p>
<p>c
lid
</p>
<p>e
a
</p>
<p>n
 D
</p>
<p>is
ta
</p>
<p>n
c
e
</p>
<p>Example 13.6 As an example we randomly select 20 observations from the bank
</p>
<p>notes data and apply the Ward technique using Euclidean distances. Figure 13.4
</p>
<p>shows the first two PCs of these data, Fig. 13.5 displays the dendrogram.
</p>
<p>Example 13.7 Consider the French food expenditures. As in Chap. 11 we use
</p>
<p>standardised data which is equivalent to using A D diag.s�1X1X1 ; : : : ; s
�1
X7X7
</p>
<p>/ as
</p>
<p>the weight matrix in the L2-norm. The NPCA plot of the individuals was given</p>
<p/>
</div>
<div class="page"><p/>
<p>13.3 Cluster Algorithms 399
</p>
<p>Fig. 13.6 The dendrogram
for the French food
expenditures, Ward algorithm
MVAclusfood
</p>
<p>C
A
</p>
<p>2
</p>
<p>C
A
</p>
<p>3
</p>
<p>C
A
</p>
<p>4
</p>
<p>E
M
</p>
<p>5
</p>
<p>C
A
</p>
<p>5
</p>
<p>M
A
</p>
<p>5
</p>
<p>M
A
</p>
<p>4
</p>
<p>E
M
</p>
<p>4
</p>
<p>E
M
</p>
<p>2
</p>
<p>M
A
</p>
<p>2
</p>
<p>M
A
</p>
<p>3
</p>
<p>E
M
</p>
<p>3
</p>
<p>0
1
</p>
<p>0
2
</p>
<p>0
3
</p>
<p>0
4
</p>
<p>0
5
</p>
<p>0
6
</p>
<p>0
7
</p>
<p>0
</p>
<p>Ward Dendrogram for French Food
</p>
<p>S
q
</p>
<p>u
a
</p>
<p>re
d
</p>
<p> E
u
</p>
<p>c
lid
</p>
<p>e
a
</p>
<p>n
 D
</p>
<p>is
ta
</p>
<p>n
c
e
</p>
<p>in Fig. 11.7. The Euclidean distance matrix is of course given by (13.6). The
</p>
<p>dendrogram obtained by using the Ward algorithm is shown in Fig. 13.6.
</p>
<p>If the aim was to have only two groups, as can be seen in Fig. 13.6, they would
</p>
<p>be fCA2, CA3, CA4, CA5, EM5g and fMA2, MA3, MA4, MA5, EM2, EM3,
EM4g. Clustering three groups is somewhat arbitrary (the levels of the distances
are too similar). If we were interested in four groups, we would obtain fCA2,
CA3, CA4g, fEM2, MA2, EM3, MA3g, fEM4, MA4, MA5g and fEM5, CA5g.
This grouping shows a balance between socio-professional levels and size of the
</p>
<p>families in determining the clusters. The four groups are clearly well represented in
</p>
<p>the NPCA plot in Fig. 11.7.
</p>
<p>Summary
</p>
<p>,! The class of clustering algorithms can be divided into two types:
hierarchical and partitioning algorithms. Hierarchical algorithms
</p>
<p>start with the finest (coarsest) possible partition and put groups
</p>
<p>together (split groups apart) step by step. Partitioning algorithms
</p>
<p>start from a preliminary clustering and exchange group elements
</p>
<p>until a certain score is reached.
,! Hierarchical agglomerative techniques are frequently used in prac-
</p>
<p>tice. They start from the finest possible structure (each data point
</p>
<p>forms a cluster), compute the distance matrix for the clusters
</p>
<p>and join the clusters that have the smallest distance. This step is
</p>
<p>repeated until all points are united in one cluster.</p>
<p/>
</div>
<div class="page"><p/>
<p>400 13 Cluster Analysis
</p>
<p>Summary (continued)
</p>
<p>,! The agglomerative procedure depends on the definition of the
distance between two clusters. Single linkage, complete linkage,
</p>
<p>and Ward distance are frequently used distances.
</p>
<p>,! The process of the unification of clusters can be graphically
represented by a dendrogram.
</p>
<p>13.4 Boston Housing
</p>
<p>Presented multivariate techniques are now applied to the Boston Housing data. We
</p>
<p>focus our attention to 14 transformed and standardised variables, see e.g. Fig. 13.7
</p>
<p>that provides descriptive statistics via boxplots for two clusters, as discussed in the
</p>
<p>2
1
</p>
<p>0
1
</p>
<p>2
</p>
<p>X1
</p>
<p>0
1
</p>
<p>2
3
</p>
<p>X2
</p>
<p>3
2
</p>
<p>1
0
</p>
<p>1
</p>
<p>X3
</p>
<p>0
1
</p>
<p>2
3
</p>
<p>X4
</p>
<p>1
0
</p>
<p>1
2
</p>
<p>X5
</p>
<p>4
2
</p>
<p>0
2
</p>
<p>X6
</p>
<p>1
.5
</p>
<p>1
.0
</p>
<p>0
.5
</p>
<p>0
.0
</p>
<p>0
.5
</p>
<p>1
.0
</p>
<p>X7
</p>
<p>2
1
</p>
<p>0
1
</p>
<p>2
</p>
<p>X8
</p>
<p>2
1
</p>
<p>0
1
</p>
<p>X9
</p>
<p>1
.5
</p>
<p>0
.5
</p>
<p>0
.0
</p>
<p>0
.5
</p>
<p>1
.0
</p>
<p>1
.5
</p>
<p>X10
</p>
<p>1
0
</p>
<p>1
2
</p>
<p>3
</p>
<p>X11
</p>
<p>4
3
</p>
<p>2
1
</p>
<p>0
</p>
<p>X12
</p>
<p>2
1
</p>
<p>0
1
</p>
<p>2
</p>
<p>X13
</p>
<p>3
2
</p>
<p>1
0
</p>
<p>1
2
</p>
<p>X14
</p>
<p>Fig. 13.7 Boxplots of the 14 standardised variables of the Boston housing data MVAclusbh</p>
<p/>
</div>
<div class="page"><p/>
<p>13.4 Boston Housing 401
</p>
<p>Fig. 13.8 Dendrogram of the
Boston housing data using the
Ward algorithm
MVAclusbh
</p>
<p>0
1
0
0
</p>
<p>2
0
0
</p>
<p>3
0
0
</p>
<p>4
0
0
</p>
<p>5
0
0
</p>
<p>Ward method
</p>
<p>d
is
</p>
<p>ta
n
c
e
</p>
<p>Table 13.3 Means and
standard errors of the 13
standardised variables for
Cluster 1 (251 observations)
and Cluster 2 (255
observations)
MVAclusbh
</p>
<p>Variable Mean C1 SE C1 Mean C2 SE C2
</p>
<p>1 �0:7105 0:0332 0:6994 0:0535
2 0:4848 0:0786 �0:4772 0:0047
3 �0:7665 0:0510 0:7545 0:0279
5 �0:7672 0:0365 0:7552 0:0447
6 0:4162 0:0571 �0:4097 0:0576
7 �0:7730 0:0429 0:7609 0:0378
8 0:7140 0:0472 �0:7028 0:0417
9 �0:5429 0:0358 0:5344 0:0656
10 �0:6932 0:0301 0:6823 0:0569
11 �0:5464 0:0469 0:5378 0:0582
12 0:3547 0:0080 �0:3491 0:0824
13 �0:6899 0:0401 0:6791 0:0509
14 0:5996 0:0431 �0:5902 0:0570
</p>
<p>sequel. A dendrogram for 13 variables(excluding the dummy variable QX4&mdash;Charles
River indicator) using the Ward method is displayed in Fig. 13.8. One observes two
</p>
<p>dominant clusters. A further refinement of say, four clusters, could be considered at
</p>
<p>a lower level of distance.
</p>
<p>To interpret the two clusters, we present the mean values and their respective
</p>
<p>standard errors of the 13 QX variables by groups in Table 13.3. Comparison of
the mean values for both groups shows that all the differences in the means are
</p>
<p>individually significant. Moreover, cluster one corresponds to housing districts with
</p>
<p>better living quality and higher house prices, whereas cluster two corresponds to less
</p>
<p>favored districts in Boston. This can be confirmed, for instance, by a lower crime
</p>
<p>rate, a higher proportion of residential land, lower proportion of African American,</p>
<p/>
</div>
<div class="page"><p/>
<p>402 13 Cluster Analysis
</p>
<p>4 0 4
</p>
<p>4
0
</p>
<p>4
</p>
<p>X1
</p>
<p>0
4
</p>
<p>8
</p>
<p>X2
</p>
<p>0
2
</p>
<p>X3
</p>
<p>0
.0
</p>
<p>0
.6
</p>
<p>X4
</p>
<p>0
.8
</p>
<p>0
.2
</p>
<p>X5
</p>
<p>1
.4
</p>
<p>2
.0
</p>
<p>X6
</p>
<p>4 0 4
</p>
<p>0
4
</p>
<p>8
</p>
<p>0 4 8 0 2 0.0 0.6 0.8 0.2 1.4 2.0 0 4 8
</p>
<p>0
4
</p>
<p>8
</p>
<p>X7
</p>
<p>Fig. 13.9 Scatterplot matrix for variables QX1 to QX7 of the Boston housing data MVAclusbh
</p>
<p>etc. for cluster one. Cluster two is identified by a higher proportion of older houses,
</p>
<p>a higher pupil/teacher ratio and a higher percentage of the lower status population.
</p>
<p>This interpretation is underlined by visual inspection of all the variables via
</p>
<p>scatterplot matrices, see e.g. Figs. 13.9 and 13.10. For example, the lower right
</p>
<p>boxplot of Fig. 13.7 and the correspondingly coloured clusters in the last row
</p>
<p>of Fig. 13.10 confirm the role of each variable in determining the clusters. This
</p>
<p>interpretation perfectly coincides with the previous PC analysis (Fig. 11.11). The
</p>
<p>quality of life factor is clearly visible in Fig. 13.11, where cluster membership is</p>
<p/>
</div>
<div class="page"><p/>
<p>13.4 Boston Housing 403
</p>
<p>0.5 2.0
</p>
<p>0
.5
</p>
<p>2
.0
</p>
<p>X8
</p>
<p>0
.0
</p>
<p>1
.5
</p>
<p>3
.0
</p>
<p>X9
</p>
<p>5
.2
</p>
<p>6
.0
</p>
<p>X10
</p>
<p>0
2
</p>
<p>4
6
</p>
<p>X11
</p>
<p>0
2
</p>
<p>4
</p>
<p>X12
</p>
<p>2
4
</p>
<p>6
</p>
<p>X13
</p>
<p>0.5 2.0
</p>
<p>2
.0
</p>
<p>3
.5
</p>
<p>0.0 1.5 3.05.2 6.0 0 2 4 6 0 2 4 2 4 6 2.0 3.5
</p>
<p>2
.0
</p>
<p>3
.5
</p>
<p>X14
</p>
<p>Fig. 13.10 Scatterplot matrix for variables QX8 to QX14 of the Boston housing data MVAclusbh
</p>
<p>distinguished by the shape and colour of the points graphed according to the first
</p>
<p>two principal components. Clearly, the first PC completely separates the two clusters
</p>
<p>and corresponds, as we have discussed in Chap. 11, to a quality of life and house
</p>
<p>indicator.</p>
<p/>
</div>
<div class="page"><p/>
<p>404 13 Cluster Analysis
</p>
<p>Fig. 13.11 Scatterplot of the
first two PCs displaying the
two clusters MVAclusbh
</p>
<p>6 4 2 0 2 4 6
</p>
<p>3
2
</p>
<p>1
0
</p>
<p>1
2
</p>
<p>3
4
</p>
<p>first vs. second PC
</p>
<p>PC1
</p>
<p>P
C
</p>
<p>2
</p>
<p>13.5 Exercises
</p>
<p>Exercise 13.1 Prove formula (13.16).
</p>
<p>Exercise 13.2 Prove that IR D tr.SR/, where SR denotes the empirical covariance
matrix of the observations contained in R.
</p>
<p>Exercise 13.3 Prove that
</p>
<p>&#129;.R;P CQ/ D nR C nP
nR C nP C nQ
</p>
<p>&#129;.R;P /C nR C nQ
nR C nP C nQ
</p>
<p>&#129;.R;Q/
</p>
<p>� nR
nR C nP C nQ
</p>
<p>&#129;.P;Q/;
</p>
<p>when the centroid formula is used to define d 2.R; P CQ/.
Exercise 13.4 Repeat the 8-point example (Example 13.5) using the complete
</p>
<p>linkage and the Ward algorithm. Explain the difference to single linkage.
</p>
<p>Exercise 13.5 Explain the differences between various proximity measures by
</p>
<p>means of an example.
</p>
<p>Exercise 13.6 Repeat the bank notes example (Example 13.6) with another random
</p>
<p>sample of 20 notes.
</p>
<p>Exercise 13.7 Repeat the bank notes example (Example 13.6) with another cluster-
</p>
<p>ing algorithm.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.5 Exercises 405
</p>
<p>Exercise 13.8 Repeat the bank notes example (Example 13.6) or the 8-point
</p>
<p>example (Example 13.5) with the L1-norm.
</p>
<p>Exercise 13.9 Analyse the US companies example (Table 22.5) using the Ward
</p>
<p>algorithm and the L2-norm.
</p>
<p>Exercise 13.10 Analyse the US crime data set (Table 22.10) with the Ward algo-
</p>
<p>rithm and the L2-norm on standardised variables (use only the crime variables).
</p>
<p>Exercise 13.11 Repeat Exercise 13.10 with the US health data set (use only the
</p>
<p>number of deaths variables).
</p>
<p>Exercise 13.12 Redo Exercise 13.10 with the �2-metric. Compare the results.
</p>
<p>Exercise 13.13 Redo Exercise 13.11 with the �2-metric and compare the results.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 14
</p>
<p>Discriminant Analysis
</p>
<p>Discriminant analysis is used in situations where the clusters are known a priori. The
</p>
<p>aim of discriminant analysis is to classify an observation, or several observations,
</p>
<p>into these known groups. For instance, in credit scoring, a bank knows from
</p>
<p>past experience that there are good customers (who repay their loan without any
</p>
<p>problems) and bad customers (who showed difficulties in repaying their loan).When
</p>
<p>a new customer asks for a loan, the bank has to decide whether or not to give the
</p>
<p>loan. The past records of the bank provides two data sets: multivariate observations
</p>
<p>xi on the two categories of customers (including for example age, salary, marital
</p>
<p>status, the amount of the loan, etc.). The new customer is a new observation x with
</p>
<p>the same variables. The discrimination rule has to classify the customer into one of
</p>
<p>the two existing groups and the discriminant analysis should evaluate the risk of a
</p>
<p>possible &ldquo;bad decision&rdquo;.
</p>
<p>Many other examples are described below, and in most applications, the groups
</p>
<p>correspond to natural classifications or to groups known from history (like in the
</p>
<p>credit scoring example). These groups could have been formed by a cluster analysis
</p>
<p>performed on past data.
</p>
<p>Section 14.1 presents the allocation rules when the populations are known, i.e.
</p>
<p>when we know the distribution of each population. As described in Sect. 14.2
</p>
<p>in practice the population characteristics have to be estimated from history. The
</p>
<p>methods are illustrated in several examples.
</p>
<p>14.1 Allocation Rules for Known Distributions
</p>
<p>Discriminant analysis is a set of methods and tools used to distinguish between
</p>
<p>groups of populations&hellip;j and to determine how to allocate new observations into
</p>
<p>groups. In one of our running examples we are interested in discriminating between
</p>
<p>counterfeit and true bank notes on the basis of measurements of these bank notes,
</p>
<p>&copy; Springer-Verlag Berlin Heidelberg 2015
W.K. H&auml;rdle, L. Simar, Applied Multivariate Statistical Analysis,
DOI 10.1007/978-3-662-45171-7_14
</p>
<p>407</p>
<p/>
</div>
<div class="page"><p/>
<p>408 14 Discriminant Analysis
</p>
<p>see Sect. 22.2. In this case we have two groups (counterfeit and genuine bank
</p>
<p>notes) and we would like to establish an algorithm (rule) that can allocate a new
</p>
<p>observation (a new bank note) into one of the groups.
</p>
<p>Another example is the detection of &ldquo;fast&rdquo; and &ldquo;slow&rdquo; consumers of a newly
</p>
<p>introduced product. Using a consumer&rsquo;s characteristics like education, income,
</p>
<p>family size, amount of previous brand switching, we want to classify each consumer
</p>
<p>into the two groups just identified.
</p>
<p>In poetry and literary studies the frequencies of spoken or written words and
</p>
<p>lengths of sentences indicate profiles of different artists and writers. It can be of
</p>
<p>interest to attribute unknown literary or artistic works to certain writers with a
</p>
<p>specific profile. Anthropological measures on ancient sculls help in discriminating
</p>
<p>between male and female bodies. Good and poor credit risk ratings constitute a
</p>
<p>discrimination problem that might be tackled using observations on income, age,
</p>
<p>number of credit cards, family size, etc.
</p>
<p>In general we have populations &hellip;j ; j D 1; 2; : : : ; J and we have to allocate
an observation x to one of these groups. A discriminant rule is a separation of the
</p>
<p>sample space (in general Rp) into sets Rj such that if x 2 Rj , it is identified as a
member of population&hellip;j .
</p>
<p>The main task of discriminant analysis is to find &ldquo;good&rdquo; regionsRj such that the
</p>
<p>error of misclassification is small. In the following we describe such rules when the
</p>
<p>population distributions are known.
</p>
<p>Maximum Likelihood Discriminant Rule
</p>
<p>Denote the densities of each population &hellip;j by fj .x/. The maximum likelihood
</p>
<p>discriminant rule (ML rule) is given by allocating x to &hellip;j maximising the
</p>
<p>likelihood Lj .x/ D fj .x/ D argmaxi fi .x/.
If several fi give the same maximum then any of them may be selected.
</p>
<p>Mathematically, the sets Rj given by the ML discriminant rule are defined as
</p>
<p>Rj D fx W Lj .x/ &gt; Li .x/ for i D 1; : : : ; J; i &curren; j g: (14.1)
</p>
<p>By classifying the observation into a certain group we may encounter a misclas-
</p>
<p>sification error. For J D 2 groups the probability of putting x into group 2 although
it is from population 1 can be calculated as
</p>
<p>p21 D P.X 2 R2j&hellip;1/ D
Z
</p>
<p>R2
</p>
<p>f1.x/dx: (14.2)</p>
<p/>
</div>
<div class="page"><p/>
<p>14.1 Allocation Rules for Known Distributions 409
</p>
<p>Similarly the conditional probability of classifying an object as belonging to the first
</p>
<p>population&hellip;1 although it actually comes from&hellip;2 is
</p>
<p>p12 D P.X 2 R1j&hellip;2/ D
Z
</p>
<p>R1
</p>
<p>f2.x/dx: (14.3)
</p>
<p>The misclassified observations create a cost C.i jj / when a &hellip;j observation is
assigned to Ri . In the credit risk example, this might be the cost of a &ldquo;sour&rdquo; credit.
</p>
<p>The cost structure can be pinned down in a cost matrix:
</p>
<p>Classified population
</p>
<p>&hellip;1 &hellip;2
</p>
<p>&hellip;1 0 C.2j1/
True population
</p>
<p>&hellip;2 C.1j2/ 0
</p>
<p>Let �j be the prior probability of population &hellip;j , where &ldquo;prior&rdquo; means the a
</p>
<p>priori probability that an individual selected at random belongs to &hellip;j (i.e. before
</p>
<p>looking to the value x). Prior probabilities should be considered if it is clear ahead
</p>
<p>of time that an observation is more likely to stem from a certain population&hellip;j : An
</p>
<p>example is the classification of musical tunes. If it is known that during a certain
</p>
<p>period of time a majority of tunes were written by a certain composer, then there is
</p>
<p>a higher probability that a certain tune was composed by this composer. Therefore,
</p>
<p>he should receive a higher prior probability when tunes are assigned to a specific
</p>
<p>group.
</p>
<p>The expected cost of misclassification .ECM/ is given by
</p>
<p>ECM D C.2j1/p21�1 C C.1j2/p12�2: (14.4)
</p>
<p>We will be interested in classification rules that keep the ECM small or minimise
</p>
<p>it over a class of rules. The discriminant rule minimising the ECM (14.4) for two
</p>
<p>populations is given below.
</p>
<p>Theorem 14.1 For two given populations, the rule minimising the ECM is given by
</p>
<p>R1 D
�
x W f1.x/
</p>
<p>f2.x/
�
�
C.1j2/
C.2j1/
</p>
<p>��
�2
</p>
<p>�1
</p>
<p>��
</p>
<p>R2 D
�
x W f1.x/
</p>
<p>f2.x/
&lt;
</p>
<p>�
C.1j2/
C.2j1/
</p>
<p>��
�2
</p>
<p>�1
</p>
<p>��</p>
<p/>
</div>
<div class="page"><p/>
<p>410 14 Discriminant Analysis
</p>
<p>The ML discriminant rule is thus a special case of the ECM rule for equal
</p>
<p>misclassification costs and equal prior probabilities. For simplicity the unity cost
</p>
<p>case, C.1j2/ D C.2j1/ D 1, and equal prior probabilities, �2 D �1, are assumed in
the following.
</p>
<p>Theorem 14.1 will be proven by an example from credit scoring.
</p>
<p>Example 14.1 Suppose that&hellip;1 represents the population of bad clients who create
</p>
<p>the cost C.2j1/ if they are classified as good clients. Analogously, define C.1j2/ as
the cost of loosing a good client classified as a bad one. Let &#13; denote the gain of the
</p>
<p>bank for the correct classification of a good client. The total gain of the bank is then
</p>
<p>G.R2/ D �C.2j1/�1
Z
</p>
<p>I.x 2 R2/f1.x/dx
</p>
<p>�C.1j2/�2
Z
f1 � I.x 2 R2/gf2.x/dxC &#13; �2
</p>
<p>Z
I.x 2 R2/f2.x/dx
</p>
<p>D �C.1j2/�2 C
Z
</p>
<p>I.x 2 R2/f�C.2j1/�1f1.x/
</p>
<p>C.C.1j2/C &#13;/�2f2.x/gdx
</p>
<p>Since the first term in this equation is constant, the maximum is obviously obtained
</p>
<p>for
</p>
<p>R2 D f x W �C.2j1/�1f1.x/C fC.1j2/C &#13;g�2f2.x/ � 0 g:
</p>
<p>This is equivalent to
</p>
<p>R2 D
�
x W f2.x/
</p>
<p>f1.x/
� C.2j1/�1fC.1j2/C &#13;g�2
</p>
<p>�
;
</p>
<p>which corresponds to the set R2 in Theorem 14.1 for a gain of &#13; D 0:
Example 14.2 Suppose x 2 f0; 1g and
</p>
<p>&hellip;1 W P.X D 0/ D P.X D 1/ D
1
</p>
<p>2
</p>
<p>&hellip;2 W P.X D 0/ D
1
</p>
<p>4
D 1 � P.X D 1/:
</p>
<p>The sample space is the set f0; 1g. The ML discriminant rule is to allocate x D 0 to
&hellip;1 and x D 1 to&hellip;2, defining the sets R1 D f0g, R2 D f1g and R1 [R2 D f0; 1g.
Example 14.3 Consider two normal populations
</p>
<p>&hellip;1 W N.�1; �21 /;
&hellip;2 W N.�2; �22 /:</p>
<p/>
</div>
<div class="page"><p/>
<p>14.1 Allocation Rules for Known Distributions 411
</p>
<p>Then
</p>
<p>Li .x/ D .2��2i /�1=2 exp
(
�1
2
</p>
<p>�
x � �i
�i
</p>
<p>�2)
:
</p>
<p>Hence x is allocated to &hellip;1 (x 2 R1) if L1.x/ � L2.x/. Note that L1.x/ � L2.x/
is equivalent to
</p>
<p>�2
</p>
<p>�1
exp
</p>
<p>"
�1
2
</p>
<p>(�
x � �1
�1
</p>
<p>�2
�
�
x � �2
�2
</p>
<p>�2)#
� 1
</p>
<p>or
</p>
<p>x2
�
1
</p>
<p>�21
� 1
�22
</p>
<p>�
� 2x
</p>
<p>�
�1
</p>
<p>�21
� �2
�22
</p>
<p>�
C
�
�21
�21
� �
</p>
<p>2
2
</p>
<p>�22
</p>
<p>�
� 2 log �2
</p>
<p>�1
: (14.5)
</p>
<p>Suppose that �1 D 0, �1 D 1 and �2 D 1, �2 D 12 . Formula (14.5) leads to
</p>
<p>R1 D
�
x W x � 1
</p>
<p>3
</p>
<p>�
4 �
</p>
<p>p
4C 6 log.2/
</p>
<p>�
or x � 1
</p>
<p>3
</p>
<p>�
4C
</p>
<p>p
4C 6 log.2/
</p>
<p>��
;
</p>
<p>R2 D R n R1:
</p>
<p>This situation is shown in Fig. 14.1.
</p>
<p>Fig. 14.1 Maximum
likelihood rule for normal
distributions
MVAdisnorm -3 -2 -1 0 1 2 3
</p>
<p>0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
2 Normal Distributions
</p>
<p>D
e
</p>
<p>n
s
</p>
<p>it
ie
</p>
<p>s
</p>
<p>R1R2R1</p>
<p/>
</div>
<div class="page"><p/>
<p>412 14 Discriminant Analysis
</p>
<p>The situation simplifies in the case of equal variances �1 D �2. The discriminant
rule (14.5) is then (for �1 &lt; �2)
</p>
<p>x ! &hellip;1; if x 2 R1 D fx W x � 12 .�1 C �2/g;
x ! &hellip;2; if x 2 R2 D fx W x &gt; 12 .�1 C �2/g:
</p>
<p>(14.6)
</p>
<p>Theorem 14.2 shows that the ML discriminant rule for multinormal observations
</p>
<p>is intimately connected with the Mahalanobis distance. The discriminant rule is
</p>
<p>based on linear combinations and belongs to the family of linear discriminant
</p>
<p>analysis (LDA) methods.
</p>
<p>Theorem 14.2 Suppose&hellip;i D Np.�i ; &dagger;/.
(a) The ML rule allocates x to &hellip;j , where j 2 f1; : : : ; J g is the value minimising
</p>
<p>the square Mahalanobis distance between x and �i :
</p>
<p>ı2.x; �i / D .x � �i /&gt;&dagger;�1.x � �i / ; i D 1; : : : ; J :
</p>
<p>(b) In the case of J D 2,
</p>
<p>x 2 R1 &rdquo; ˛&gt;.x � �/ � 0 ;
</p>
<p>where ˛ D &dagger;�1.�1 � �2/ and � D 12 .�1 C �2/.
Proof Part (a) of the theorem follows directly from comparison of the likelihoods.
</p>
<p>For J D 2, part (a) says that x is allocated to&hellip;1 if
</p>
<p>.x � �1/&gt;&dagger;�1.x � �1/ � .x � �2/&gt;&dagger;�1.x � �2/
</p>
<p>Rearranging terms leads to
</p>
<p>�2�&gt;1 &dagger;�1x C 2�&gt;2 &dagger;�1x C �&gt;1 &dagger;�1�1 � �&gt;2 &dagger;�1�2 � 0;
</p>
<p>which is equivalent to
</p>
<p>2.�2 � �1/&gt;&dagger;�1x C .�1 � �2/&gt;&dagger;�1.�1 C �2/ � 0;
</p>
<p>.�1 � �2/&gt;&dagger;�1
�
x � 1
</p>
<p>2
.�1 C �2/
</p>
<p>�
� 0;
</p>
<p>˛&gt;.x � �/ � 0:
</p>
<p>ut</p>
<p/>
</div>
<div class="page"><p/>
<p>14.1 Allocation Rules for Known Distributions 413
</p>
<p>Bayes Discriminant Rule
</p>
<p>We have seen an example where prior knowledge on the probability of classifi-
</p>
<p>cation into &hellip;j was assumed. Denote the prior probabilities by �j and note thatPJ
jD1 �j D 1. The Bayes rule of discrimination allocates x to the&hellip;j that gives the
</p>
<p>largest value of �ifi .x/, �jfj .x/ D maxi �ifi .x/. Hence, the discriminant rule is
defined by Rj D fx W �jfj .x/ � �ifi .x/ for i D 1; : : : ; J g. Obviously the Bayes
rule is identical to the ML discriminant rule for �j D 1=J .
</p>
<p>A further modification is to allocate x to &hellip;j with a certain probability �j .x/,
</p>
<p>such that
PJ
</p>
<p>jD1 �j .x/ D 1 for all x. This is called a randomised discriminant rule.
A randomised discriminant rule is a generalisation of deterministic discriminant
</p>
<p>rules since
</p>
<p>�j .x/ D
�
1 if �jfj .x/ D maxi �ifi .x/;
0 otherwise
</p>
<p>reflects the deterministic rules.
</p>
<p>Which discriminant rules are good? We need a measure of comparison. Denote
</p>
<p>pij D
Z
�i .x/fj .x/dx (14.7)
</p>
<p>as the probability of allocating x to &hellip;i if it in fact belongs to &hellip;j . A discriminant
</p>
<p>rule with probabilitiespij is as good as any other discriminant rule with probabilities
</p>
<p>p0ij if
</p>
<p>pii � p0ii for all i D 1; : : : ; J: (14.8)
</p>
<p>We call the first rule better if the strict inequality in (14.8) holds for at least one i . A
</p>
<p>discriminant rule is called admissible if there is no better discriminant rule.
</p>
<p>Theorem 14.3 All Bayes discriminant rules (including the ML rule) are admissible.
</p>
<p>Probability of Misclassification for the ML Rule (J D 2)
</p>
<p>Suppose that &hellip;i D Np.�i ; &dagger;/. In the case of two groups, it is not difficult to
derive the probabilities of misclassification for the ML discriminant rule. Consider
</p>
<p>for instance p12 D P.x 2 R1 j &hellip;2/. By part (b) in Theorem 14.2 we have
</p>
<p>p12 D Pf˛&gt;.x � �/ &gt; 0 j &hellip;2g:</p>
<p/>
</div>
<div class="page"><p/>
<p>414 14 Discriminant Analysis
</p>
<p>If X 2 R2, ˛&gt;.X � �/ � N
�
� 1
2
ı2; ı2
</p>
<p>�
where ı2 D .�1 � �2/&gt;&dagger;�1.�1 � �2/ is
</p>
<p>the squared Mahalanobis distance between the two populations, we obtain
</p>
<p>p12 D ˆ
�
�1
2
ı
</p>
<p>�
:
</p>
<p>Similarly, the probability of being classified into population 2 although x stems from
</p>
<p>&hellip;1 is equal to p21 D ˆ
�
� 1
2
ı
�
.
</p>
<p>Classification with Different Covariance Matrices
</p>
<p>The minimum ECM depends on the ratio of the densities f1.x/
f2.x/
</p>
<p>or equivalently on the
</p>
<p>difference logff1.x/g� logff2.x/g. When the covariance for both density functions
differ, the allocation rule becomes more complicated:
</p>
<p>R1 D
�
x W �1
</p>
<p>2
x&gt;.&dagger;�11 �&dagger;�12 /x C .�&gt;1 &dagger;�11 � �&gt;2 &dagger;�12 /x � k
</p>
<p>� log
��
C.1j2/
C.2j1/
</p>
<p>��
�2
</p>
<p>�1
</p>
<p>���
;
</p>
<p>R2 D
�
x W �1
</p>
<p>2
x&gt;.&dagger;�11 �&dagger;�12 /x C .�&gt;1 &dagger;�11 � �&gt;2 &dagger;�12 /x � k
</p>
<p>&lt; log
</p>
<p>��
C.1j2/
C.2j1/
</p>
<p>��
�2
</p>
<p>�1
</p>
<p>���
;
</p>
<p>where k D 1
2
log
</p>
<p>�
j&dagger;1j
j&dagger;2j
</p>
<p>�
C 1
</p>
<p>2
.�&gt;1 &dagger;
</p>
<p>�1
1 �1 � �&gt;2 &dagger;�12 �2/. The classification regions
</p>
<p>are defined by quadratic functions. Therefore they belong to the family of quadratic
</p>
<p>discriminant analysis (QDA) methods. This quadratic classification rule coincides
</p>
<p>with the rules used when &dagger;1 D &dagger;2, since the term 12x&gt;.&dagger;�11 �&dagger;�12 /x disappears.
</p>
<p>Summary
</p>
<p>,! Discriminant analysis is a set of methods used to distinguish among
groups in data and to allocate new observations into the existing
</p>
<p>groups.
</p>
<p>,! Given that data are from populations &hellip;j with densities fj , j D
1; : : : ; J , the maximum likelihood discriminant rule (ML rule)
</p>
<p>allocates an observation x to that population &hellip;j which has the
</p>
<p>maximum likelihood Lj .x/ D fj .x/ D maxi fi .x/.</p>
<p/>
</div>
<div class="page"><p/>
<p>14.2 Discrimination Rules in Practice 415
</p>
<p>Summary (continued)
</p>
<p>,! Given prior probabilities �j for populations &hellip;j , Bayes discrim-
inant rule allocates an observation x to the population &hellip;j that
</p>
<p>maximises �ifi .x/ with respect to i . All Bayes discriminant rules
</p>
<p>(incl. the ML rule) are admissible.
</p>
<p>,! For the ML rule and J D 2 normal populations, the probabilities
of misclassification are given by p12 D p21 D ˆ
</p>
<p>�
� 1
2
ı
�
where ı is
</p>
<p>the Mahalanobis distance between the two populations.
</p>
<p>,! Classification of two normal populations with different covariance
matrices (ML rule) leads to regions defined by a quadratic function.
</p>
<p>,! Desirable discriminant rules have a low ECM.
</p>
<p>14.2 Discrimination Rules in Practice
</p>
<p>The ML rule is used if the distribution of the data is known up to parameters.
</p>
<p>Suppose for example that the data come from multivariate normal distributions
</p>
<p>Np.�j ; &dagger;/. If we have J groups with nj observations in each group, we use xj
to estimate �j , and Sj to estimate&dagger;. The common covariance may be estimated by
</p>
<p>Su D
JX
</p>
<p>jD1
nj
</p>
<p>�
Sj
</p>
<p>n � J
</p>
<p>�
; (14.9)
</p>
<p>with n D
PJ
</p>
<p>jD1 nj . Thus the empirical version of the ML rule of Theorem 14.2 is
to allocate a new observation x to&hellip;j such that j minimises
</p>
<p>.x � xi /&gt;S�1u .x � xi / for i 2 f1; : : : ; J g:
</p>
<p>Example 14.4 Let us apply this rule to the Swiss bank notes. The 20 randomly
</p>
<p>chosen bank notes which we had clustered into two groups in Example 13.6 are
</p>
<p>used. First the covariance &dagger; is estimated by the average of the covariances of &hellip;1
(cluster 1) and&hellip;2 (cluster 2). The hyperplane Ǫ&gt;.x � x/ D 0 which separates the
two populations is given by
</p>
<p>Ǫ D S�1u .x1 � x2/ D .�12:18; 20:54;�19:22;�15:55;�13:06; 21:43/&gt; ;
</p>
<p>x D 1
2
.x1 C x2/ D .214:79; 130:05; 129:92; 9:23; 10:48; 140:46/&gt; :</p>
<p/>
</div>
<div class="page"><p/>
<p>416 14 Discriminant Analysis
</p>
<p>Now let us apply the discriminant rule to the entire bank notes data set. Counting
</p>
<p>the number of misclassifications by
</p>
<p>100X
</p>
<p>iD1
If Ǫ&gt;.xi � x/ &lt; 0g;
</p>
<p>200X
</p>
<p>iD101
If Ǫ&gt;.xi � x/ &gt; 0g;
</p>
<p>we obtain 1 misclassified observation for the counterfeit bank notes and 0 misclas-
</p>
<p>sification for the genuine bank notes.
</p>
<p>When J D 3 groups, the allocation regions can be calculated using
</p>
<p>h12.x/ D .x1 � x2/&gt;S�1u
�
x � 1
</p>
<p>2
.x1 C x2/
</p>
<p>�
</p>
<p>h13.x/ D .x1 � x3/&gt;S�1u
�
x � 1
</p>
<p>2
.x1 C x3/
</p>
<p>�
</p>
<p>h23.x/ D .x2 � x3/&gt;S�1u
�
x � 1
</p>
<p>2
.x2 C x3/
</p>
<p>�
:
</p>
<p>The rule is to allocate x to
</p>
<p>8
&lt;
:
</p>
<p>&hellip;1 if h12.x/ � 0 and h13.x/ � 0
&hellip;2 if h12.x/ &lt; 0 and h23.x/ � 0
&hellip;3 if h13.x/ &lt; 0 and h23.x/ &lt; 0:
</p>
<p>Estimation of the Probabilities of Misclassifications
</p>
<p>Misclassification probabilities are given by (14.7) and can be estimated by replacing
</p>
<p>the unknown parameters by their corresponding estimators.
</p>
<p>For the ML rule for two normal populations we obtain
</p>
<p>Op12 D Op21 D ˆ
�
�1
2
Oı
�
</p>
<p>where Oı2 D . Nx1 � Nx2/&gt;S�1u . Nx1 � Nx2/ is the estimator for ı2.
</p>
<p>The probabilities of misclassification may also be estimated by the re-substitution
</p>
<p>method. We reclassify each original observation xi , i D 1; : : : ; n into &hellip;1; : : : ;&hellip;J
according to the chosen rule. Then denoting the number of individuals coming from
</p>
<p>&hellip;j which have been classified into &hellip;i by nij, we have Opij D nijnj , an estimator of
pij. Clearly, this method leads to too optimistic estimators of pij, but it provides a</p>
<p/>
</div>
<div class="page"><p/>
<p>14.2 Discrimination Rules in Practice 417
</p>
<p>rough measure of the quality of the discriminant rule. The matrix . Opij/ is called the
confusion matrix in Johnson and Wichern (1998).
</p>
<p>Example 14.5 In the above classification problem for the Swiss bank notes
</p>
<p>(Sect. 22.2), we have the following confusion matrix: MVAaper
</p>
<p>true membership
</p>
<p>genuine (˘1) counterfeit (˘2)
</p>
<p>˘1 100 1
</p>
<p>predicted
</p>
<p>˘2 0 99
</p>
<p>The apparent error rate (APER) is defined as the fraction of observations that
</p>
<p>are misclassified. The APER, expressed as a percentage, is
</p>
<p>APER D
�
1
</p>
<p>200
</p>
<p>�
100% D 0:5%:
</p>
<p>For the calculation of the APER we use the observations twice: the first time to
</p>
<p>construct the classification rule and the second time to evaluate this rule. An APER
</p>
<p>of 0:5% might therefore be too optimistic. An approach that corrects for this bias
</p>
<p>is based on the holdout procedure of Lachenbruch and Mickey (1968). For two
</p>
<p>populations this procedure is as follows:
</p>
<p>1. Start with the first population &hellip;1. Omit one observation and develop the
</p>
<p>classification rule based on the remaining n1 � 1; n2 observations.
2. Classify the &ldquo;holdout&rdquo; observation using the discrimination rule in Step 1.
</p>
<p>3. Repeat steps 1 and 2 until all of the &hellip;1 observations are classified. Count the
</p>
<p>number n021 of misclassified observations.
4. Repeat steps 1 through 3 for population&hellip;2. Count the number n
</p>
<p>0
12 of misclassi-
</p>
<p>fied observations.
</p>
<p>Estimates of the misclassification probabilities are given by
</p>
<p>Op012 D
n012
n2
</p>
<p>and
</p>
<p>Op021 D
n021
n1
:
</p>
<p>A more realistic estimator of the actual error rate (AER) is given by
</p>
<p>n012 C n021
n2 C n1
</p>
<p>: (14.10)</p>
<p/>
</div>
<div class="page"><p/>
<p>418 14 Discriminant Analysis
</p>
<p>Statisticians favor the AER (for its unbiasedness) over the APER. In large samples,
</p>
<p>however, the computational costs might counterbalance the statistical advantage.
</p>
<p>This is not a real problem since the two misclassification measures are asymptoti-
</p>
<p>cally equivalent.
</p>
<p>Fisher&rsquo;s Linear Discrimination Function
</p>
<p>Another approach stems fromR.A. Fisher. His idea was to base the discriminant rule
</p>
<p>on a projection a&gt;x such that a good separation was achieved. This LDA projection
method is called Fisher&rsquo;s linear discrimination function. If
</p>
<p>Y D Xa
</p>
<p>denotes a linear combination of observations, then the total sum of squares of y,Pn
iD1.yi � Ny/2, is equal to
</p>
<p>Y&gt;HY D a&gt;X&gt;HXa D a&gt;T a (14.11)
</p>
<p>with the centering matrixH D I � n�11n1&gt;n and T D X&gt;HX .
Suppose we have samples Xj , j D 1; : : : ; J , from J populations. Fisher&rsquo;s
</p>
<p>suggestion was to find the linear combination a&gt;x which maximises the ratio of
the between-group-sum of squares to the within-group-sum of squares.
</p>
<p>The within-group-sum of squares is given by
</p>
<p>JX
</p>
<p>jD1
Y&gt;j HjYj D
</p>
<p>JX
</p>
<p>jD1
a&gt;X&gt;j HjXj a D a&gt;Wa; (14.12)
</p>
<p>where Yj denotes the j -th sub-matrix of Y corresponding to observations of group
</p>
<p>j andHj denotes the .nj �nj / centering matrix. The within-group-sum of squares
measures the sum of variations within each group.
</p>
<p>The between-group-sum of squares is
</p>
<p>JX
</p>
<p>jD1
nj .yj � y/2 D
</p>
<p>JX
</p>
<p>jD1
nj fa&gt;.xj � x/g2 D a&gt;Ba; (14.13)
</p>
<p>where yj and xj denote the means of Yj and Xj and y and x denote the sample
</p>
<p>means of Y and X . The between-group-sum of squares measures the variation of
</p>
<p>the means across groups.</p>
<p/>
</div>
<div class="page"><p/>
<p>14.2 Discrimination Rules in Practice 419
</p>
<p>The total sum of squares (14.11) is the sum of the within-group-sum of squares
</p>
<p>and the between-group-sum of squares, i.e.
</p>
<p>a&gt;T a D a&gt;WaC a&gt;Ba:
</p>
<p>Fisher&rsquo;s idea was to select a projection vector a that maximises the ratio
</p>
<p>a&gt;Ba
</p>
<p>a&gt;Wa
: (14.14)
</p>
<p>The solution is found by applying Theorem 2.5.
</p>
<p>Theorem 14.4 The vector a that maximises (14.14) is the eigenvector of W�1B
that corresponds to the largest eigenvalue.
</p>
<p>Now a discrimination rule is easy to obtain:
</p>
<p>classify x into group j where a&gt; Nxj is closest to a&gt;x, i.e.
</p>
<p>x ! &hellip;j where j D argmin
i
ja&gt;.x � Nxi /j:
</p>
<p>When J D 2 groups, the discriminant rule is easy to compute. Suppose that
group 1 has n1 elements and group 2 has n2 elements. In this case
</p>
<p>B D
�n1n2
n
</p>
<p>�
dd&gt;;
</p>
<p>where d D .x1 � x2/. W�1B has only one eigenvalue which equals
</p>
<p>tr.W�1B/ D
�n1n2
n
</p>
<p>�
d&gt;W�1d;
</p>
<p>and the corresponding eigenvector is a D W�1d . The corresponding discriminant
rule is
</p>
<p>x ! &hellip;1 if a&gt;fx � 12 .x1 C x2/g &gt; 0;
x ! &hellip;2 if a&gt;fx � 12 .x1 C x2/g � 0:
</p>
<p>(14.15)
</p>
<p>The Fisher LDA is closely related to projection pursuit (Chap. 20) since the
</p>
<p>statistical technique is based on a one-dimensional index a&gt;x.
</p>
<p>Example 14.6 Consider the bank notes data again. Let us use the subscript &ldquo;g&rdquo; for
</p>
<p>the genuine and &ldquo;f &rdquo; for the counterfeit bank notes, e.g.Xg denotes the first hundred
</p>
<p>observations of X and Xf the second hundred. In the context of the bank data set
</p>
<p>the &ldquo;between-group-sum of squares&rdquo; is defined as
</p>
<p>100
˚
.yg � y/2 C .yf � y/2
</p>
<p>�
D a&gt;Ba (14.16)</p>
<p/>
</div>
<div class="page"><p/>
<p>420 14 Discriminant Analysis
</p>
<p>for some matrix B. Here, yg and yf denote the means for the genuine and
</p>
<p>counterfeit bank notes and y D 1
2
.yg C yf /. The &ldquo;within-group-sum of squares&rdquo; is
</p>
<p>100X
</p>
<p>iD1
f.yg/i � ygg2 C
</p>
<p>100X
</p>
<p>iD1
f.yf /i � yf g2 D a&gt;Wa; (14.17)
</p>
<p>with .yg/i D a&gt;xi and .yf /i D a&gt;xiC100 for i D 1; : : : ; 100.
The resulting discriminant rule consists of allocating an observation x0 to the
</p>
<p>genuine sample space if
</p>
<p>a&gt;.x0 � x/ &gt; 0;
</p>
<p>with a DW�1.xg � xf / (see Exercise 14.8) and of allocating x0 to the counterfeit
sample space when the opposite is true. In our case
</p>
<p>a D .0:000; 0:029;�0:029;�0:039;�0:041; 0:054/&gt; �
</p>
<p>One genuine and no counterfeit bank notes are misclassified. Figure 14.2 shows the
</p>
<p>estimated densities for yg D a&gt;Xg and yf D a&gt;Xf . They are separated better than
those of the diagonals in Fig. 1.9.
</p>
<p>Note that the allocation rule (14.15) is exactly the same as the ML rule for J D 2
groups and for normal distributions with the same covariance. For J D 3 groups
this rule will be different, except for the special case of collinear sample means.
</p>
<p>Fig. 14.2 Densities of
projections of genuine and
counterfeit bank notes by
Fisher&rsquo;s discrimination rule
MVAdisfbank -0.2 -0.1 0 0.1 0.2
</p>
<p>0
</p>
<p>2
</p>
<p>4
</p>
<p>6
</p>
<p>8
</p>
<p>10
</p>
<p>12
</p>
<p>14
Swiss Bank Notes
</p>
<p>D
e
n
</p>
<p>s
it
</p>
<p>ie
s
 o
</p>
<p>f 
P
</p>
<p>ro
je
</p>
<p>c
ti
</p>
<p>o
n
</p>
<p>s
</p>
<p>Forged
</p>
<p>Genuine</p>
<p/>
</div>
<div class="page"><p/>
<p>14.3 Boston Housing 421
</p>
<p>Summary
</p>
<p>,! A discriminant rule is a separation of the sample space into setsRj .
An observation x is classified as coming from population&hellip;j if it
</p>
<p>lies in Rj .
</p>
<p>,! The ECM for two populations is given by ECM D C.2j1/p21�1 C
C.1j2/p12�2.
</p>
<p>,! The ML rule is applied if the distributions in the populations are
known up to parameters, e.g. for normal distributionsNp.�j ; &dagger;/.
</p>
<p>,! The ML rule allocates x to the population that exhibits the smallest
Mahalanobis distance
</p>
<p>ı2.xI�i / D .x � �i /&gt;&dagger;�1.x � �i /:
</p>
<p>,! The probability of misclassification is given by
</p>
<p>p12 D p21 D ˆ
�
�1
2
ı
</p>
<p>�
;
</p>
<p>where ı is the Mahalanobis distance between �1 and �2.
</p>
<p>,! Classification for different covariance structures in the two popula-
tions leads to quadratic discrimination rules.
</p>
<p>,! A different approach is Fisher&rsquo;s linear discrimination rule which
finds a linear combination a&gt;x that maximises the ratio of the
&ldquo;between-group-sum of squares&rdquo; and the &ldquo;within-group-sum of
</p>
<p>squares&rdquo;. This rule turns out to be identical to the ML rule when
</p>
<p>J D 2 for normal populations.
</p>
<p>14.3 Boston Housing
</p>
<p>One interesting application of discriminant analysis with respect to the Boston
</p>
<p>housing data is the classification of the districts according to the house values.
</p>
<p>The rationale behind this is that certain observable must determine the value of a
</p>
<p>district, as in Sect. 3.7 where the house value was regressed on the other variables.
</p>
<p>Two groups are defined according to the median value of houses QX14: in group&hellip;1
the value of QX14 is greater than or equal to the median of QX14 and in group&hellip;2 the
value of QX14 is less than the median of QX14.</p>
<p/>
</div>
<div class="page"><p/>
<p>422 14 Discriminant Analysis
</p>
<p>Table 14.1 APER for price
of Boston houses
MVAdiscbh
</p>
<p>True
</p>
<p>&hellip;1 &hellip;2
</p>
<p>&hellip;1 216 40
</p>
<p>Predicted
</p>
<p>&hellip;2 34 216
</p>
<p>Table 14.2 AER for price of
Boston houses MVAaerbh
</p>
<p>True
</p>
<p>&hellip;1 &hellip;2
</p>
<p>&hellip;1 211 42
</p>
<p>Predicted
</p>
<p>&hellip;2 39 214
</p>
<p>Table 14.3 APER for
clusters of Boston houses
MVAdiscbh
</p>
<p>True
</p>
<p>&hellip;1 &hellip;2
</p>
<p>&hellip;1 244 13
</p>
<p>Predicted
</p>
<p>&hellip;2 7 242
</p>
<p>Table 14.4 AER for clusters
of Boston houses
MVAaerbh
</p>
<p>True
</p>
<p>&hellip;1 &hellip;2
</p>
<p>&hellip;1 244 14
</p>
<p>Predicted
</p>
<p>&hellip;2 7 241
</p>
<p>The linear discriminant rule, defined on the remaining 12 variables (excluding QX4
and QX14) is applied. After reclassifying the 506 observations, we obtain an APER
of 0.146. The details are given in Table 14.1. The more appropriate error rate, given
</p>
<p>by the AER, is 0.160 (see Table 14.2).
</p>
<p>Let us now turn to a group definition suggested by the Cluster Analysis in
</p>
<p>Sect. 13.4. Group &hellip;1 was defined by higher quality of life and house. We define
</p>
<p>the linear discriminant rule using the 13 variables from eX excluding QX4. Then
we reclassify the 506 observations and we obtain an APER of 0.0395. Details are
</p>
<p>summarised in Table 14.3. The AER turns out to be 0.0415 (see Table 14.4).
</p>
<p>Figure 14.3 displays the values of the linear discriminant scores (see Theo-
</p>
<p>rem 14.2) for all of the 506 observations, coloured by groups. One can clearly see
</p>
<p>the APER is derived from the seven observations from group &hellip;1 with a negative
</p>
<p>score and the 13 observations from group&hellip;2 with positive score.</p>
<p/>
</div>
<div class="page"><p/>
<p>14.4 Exercises 423
</p>
<p>Fig. 14.3 Discrimination scores for the two clusters created from the Boston housing data
MVAdiscbh
</p>
<p>14.4 Exercises
</p>
<p>Exercise 14.1 Prove Theorem 14.2 (a) and 14.2 (b).
</p>
<p>Exercise 14.2 Apply the rule from Theorem 14.2 (b) for p D 1 and compare the
result with that of Example 14.3.
</p>
<p>Exercise 14.3 Calculate the ML discrimination rule based on observations of a
</p>
<p>one-dimensional variable with an exponential distribution.
</p>
<p>Exercise 14.4 Calculate the ML discrimination rule based on observations of a
</p>
<p>two-dimensional random variable, where the first component has an exponential
</p>
<p>distribution and the other has an alternative distribution. What is the difference
</p>
<p>between the discrimination rule obtained in this exercise and the Bayes discrimina-
</p>
<p>tion rule?
</p>
<p>Exercise 14.5 Apply the Bayes rule to the car data (Table 22.3) in order to
</p>
<p>discriminate between Japanese, European and US cars, i.e. J D 3. Consider
only the &ldquo;miles per gallon&rdquo; variable and take the relative frequencies as prior
</p>
<p>probabilities.
</p>
<p>Exercise 14.6 Compute Fisher&rsquo;s linear discrimination function for the 20 bank
</p>
<p>notes from Example 13.6. Apply it to the entire bank data set. How many obser-
</p>
<p>vations are misclassified?
</p>
<p>Exercise 14.7 Use the Fisher&rsquo;s linear discrimination function on the WAIS data
</p>
<p>set (Table 22.12) and evaluate the results by re-substitution the probabilities of
</p>
<p>misclassification.
</p>
<p>Exercise 14.8 Show that in Example 14.6
</p>
<p>(a) W D 100
�
Sg C Sf
</p>
<p>�
, where Sg and Sf denote the empirical covariances (3.6)
</p>
<p>and (3.5) w.r.t. the genuine and counterfeit bank notes,
</p>
<p>(b) B D 100
˚
.xg � x/.xg � x/&gt; C .xf � x/.xf � x/&gt;
</p>
<p>�
; where x D 1
</p>
<p>2
.xg C
</p>
<p>xf /,
</p>
<p>(c) a DW�1.xg � xf /:</p>
<p/>
</div>
<div class="page"><p/>
<p>424 14 Discriminant Analysis
</p>
<p>Exercise 14.9 Recalculate Example 14.3 with the prior probability �1 D 13 and
C.2j1/ D 2C.1j2/.
Exercise 14.10 Explain the effect of changing�1 or C.1j2/ on the relative location
of the region Rj ; j D 1; 2.
Exercise 14.11 Prove that Fisher&rsquo;s linear discrimination function is identical to the
</p>
<p>ML rule when the covariance matrices are identical .J D 2/.
Exercise 14.12 Suppose that x 2 f0; 1; 2; 3; 4; 5; 6; 7; 8; 9; 10g and
</p>
<p>&hellip;1 W X � Bi.10; 0:2/ with the prior probability �1 D 0:5I
&hellip;2 W X � Bi.10; 0:3/ with the prior probability �2 D 0:3I
&hellip;3 W X � Bi.10; 0:5/ with the prior probability �3 D 0:2:
</p>
<p>Determine the sets R1, R2 and R3. (Use the Bayes discriminant rule.)</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 15
</p>
<p>Correspondence Analysis
</p>
<p>Correspondence analysis provides tools for analysing the associations between rows
</p>
<p>and columns of contingency tables. A contingency table is a two-entry frequency
</p>
<p>table where the joint frequencies of two qualitative variables are reported. For
</p>
<p>instance a .2�2/ table could be formed by observing from a sample of n individuals
two qualitative variables: the individual&rsquo;s sex and whether the individual smokes.
</p>
<p>The table reports the observed joint frequencies. In general .n � p/ tables may be
considered.
</p>
<p>The main idea of correspondence analysis is to develop simple indices that will
</p>
<p>show the relations between the row and the columns categories. These indices will
</p>
<p>tell us simultaneously which column categories have more weight in a row category
</p>
<p>and vice versa. Correspondence analysis is also related to the issue of reducing the
</p>
<p>dimension of the table, similar to principal component analysis in Chap. 11, and to
</p>
<p>the issue of decomposing the table into its factors as discussed in Chap. 10. The
</p>
<p>idea is to extract the indices in decreasing order of importance so that the main
</p>
<p>information of the table can be summarised in spaces with smaller dimensions. For
</p>
<p>instance, if only two factors (indices) are used, the results can be shown in two-
</p>
<p>dimensional graphs, showing the relationship between the rows and the columns of
</p>
<p>the table.
</p>
<p>Section 15.1 defines the basic notation and motivates the approach and Sect. 15.2
</p>
<p>gives the basic theory. The indices will be used to describe the �2 statistic measuring
</p>
<p>the associations in the table. Several examples in Sect. 15.3 show how to provide
</p>
<p>and interpret, in practice, the two-dimensional graphs displaying the relationship
</p>
<p>between the rows and the columns of a contingency table.
</p>
<p>&copy; Springer-Verlag Berlin Heidelberg 2015
W.K. H&auml;rdle, L. Simar, Applied Multivariate Statistical Analysis,
DOI 10.1007/978-3-662-45171-7_15
</p>
<p>425</p>
<p/>
</div>
<div class="page"><p/>
<p>426 15 Correspondence Analysis
</p>
<p>15.1 Motivation
</p>
<p>The aim of correspondence analysis is to develop simple indices that show relations
</p>
<p>between the row and columns of a contingency tables. Contingency tables are very
</p>
<p>useful to describe the association between two variables in very general situations.
</p>
<p>The two variables can be qualitative (nominal), in which case they are also referred
</p>
<p>to as categorical variables. Each row and each column in the table represents one
</p>
<p>category of the corresponding variable. The entry xij in the table X (with dimension
</p>
<p>.n� p/) is the number of observations in a sample which simultaneously fall in the
i th row category and the j th column category, for i D 1; : : : ; n and j D 1; : : : ; p.
Sometimes a &ldquo;category&rdquo; of a nominal variable is also called a &ldquo;modality&rdquo; of the
</p>
<p>variable.
</p>
<p>The variables of interest can also be discrete quantitative variables, such as the
</p>
<p>number of family members or the number of accidents an insurance company had to
</p>
<p>cover during 1 year, etc. Here, each possible value that the variable can have defines
</p>
<p>a row or a column category. Continuous variables may be taken into account by
</p>
<p>defining the categories in terms of intervals or classes of values which the variable
</p>
<p>can take on. Thus contingency tables can be used in many situations, implying that
</p>
<p>correspondence analysis is a very useful tool in many applications.
</p>
<p>The graphical relationships between the rows and the columns of the table X
</p>
<p>that result from correspondence analysis are based on the idea of representing all
</p>
<p>the row and column categories and interpreting the relative positions of the points
</p>
<p>in terms of the weights corresponding to the column and the row. This is achieved
</p>
<p>by deriving a system of simple indices providing the coordinates of each row and
</p>
<p>each column. These row and column coordinates are simultaneously represented in
</p>
<p>the same graph. It is then clear to see which column categories are more important
</p>
<p>in the row categories of the table (and the other way around).
</p>
<p>As was already eluded to, the construction of the indices is based on an idea sim-
</p>
<p>ilar to that of PCA. Using PCA the total variance was partitioned into independent
</p>
<p>contributions stemming from the principal components. Correspondence analysis,
</p>
<p>on the other hand, decomposes a measure of association, typically the total �2 value
</p>
<p>used in testing independence, rather than decomposing the total variance.
</p>
<p>Example 15.1 The French &ldquo;baccalaur&eacute;at&rdquo; frequencies have been classified into
</p>
<p>regions and different baccalaur&eacute;at categories, see Chap. 22, Table 22.8. Altogether
</p>
<p>n D 202;100 baccalaur&eacute;ats were observed. The joint frequency of the region
Ile-de-France and the modality Philosophy, for example, is 9,724. That is, 9,724
</p>
<p>baccalaur&eacute;ats were in Ile-de-France and the category Philosophy.
</p>
<p>The question is whether certain regions prefer certain baccalaur&eacute;at types. If we
</p>
<p>consider, for instance, the region Lorraine, we have the following percentages:
</p>
<p>A B C D E F G H
</p>
<p>20.5 7.6 15.3 19.6 3.4 14.5 18.9 0.2</p>
<p/>
</div>
<div class="page"><p/>
<p>15.1 Motivation 427
</p>
<p>The total percentages of the different modalities of the variable baccalaur&eacute;at are
</p>
<p>as follows:
</p>
<p>A B C D E F G H
</p>
<p>22.6 10.7 16.2 22.8 2.6 9.7 15.2 0.2
</p>
<p>One might argue that the region Lorraine seems to prefer the modalities E, F,
</p>
<p>G and dislike the specialisations A, B, C, D relative to the overall frequency of
</p>
<p>baccalaur&eacute;at type.
</p>
<p>In correspondence analysis we try to develop an index for the regions so that
</p>
<p>this over- or underrepresentation can be measured in just one single number.
</p>
<p>Simultaneously we try to weight the regions so that we can see in which region
</p>
<p>certain baccalaur&eacute;at types are preferred.
</p>
<p>Example 15.2 Consider n types of companies and p locations of these companies.
</p>
<p>Is there a certain type of company that prefers a certain location? Or is there a
</p>
<p>location index that corresponds to a certain type of company?
</p>
<p>Assume that n D 3, p D 3, and that the frequencies are as follows:
</p>
<p>X D
</p>
<p>0
@
4 0 2
</p>
<p>0 1 1
</p>
<p>1 1 4
</p>
<p>1
A
 Finance
 Energy
 HiTech
</p>
<p>" Frankfurt
" Berlin
" Munich
</p>
<p>The frequencies imply that four type three companies (HiTech) are in location 3
</p>
<p>(Munich), and so on. Suppose there is a (company) weight vector r D .r1; : : : ; rn/&gt;
such that a location index sj could be defined as
</p>
<p>sj D c
nX
</p>
<p>iD1
ri
xij
</p>
<p>x�j
; (15.1)
</p>
<p>where x�j D
Pn
</p>
<p>iD1 xij is the number of companies in location j and c is a constant.
s1, for example, would give the average weighted frequency (by r) of companies in
</p>
<p>location 1 (Frankfurt).
</p>
<p>Given a location weight vector s� D
�
s�1 ; : : : ; s
</p>
<p>�
p
</p>
<p>�&gt;
, we can define a company
</p>
<p>index in the same way as
</p>
<p>r�i D c�
pX
</p>
<p>jD1
s�j
xij
</p>
<p>xi�
; (15.2)</p>
<p/>
</div>
<div class="page"><p/>
<p>428 15 Correspondence Analysis
</p>
<p>where c� is a constant and xi� D
Pp
</p>
<p>jD1 xij is the sum of the i th row of X , i.e. the
number of type i companies. Thus r�2 , for example, would give the averageweighted
frequency (by s�) of energy companies.
</p>
<p>If (15.1) and (15.2) can be solved simultaneously for a &ldquo;row weight&rdquo; vector
</p>
<p>r D .r1; : : : ; rn/&gt; and a &ldquo;column weight&rdquo; vector s D .s1; : : : ; sp/&gt;, we may
represent each row category by ri ; i D 1; : : : ; n and each column category by
sj ; j D 1; : : : ; p in a one-dimensional graph. If in this graph ri and sj are in
close proximity (far from the origin), this would indicate that the i th row category
</p>
<p>has an important conditional frequency xij=x�j in (15.1) and that the j th column
category has an important conditional frequency xij=xi� in (15.2). This would
indicate a positive association between the i th row and the j th column. A similar
</p>
<p>line of argument could be used if ri was very far away from sj (and far from
</p>
<p>the origin). This would indicate a small conditional frequency contribution, or a
</p>
<p>negative association between the i th row and the j th column.
</p>
<p>Summary
</p>
<p>,! The aim of correspondence analysis is to develop simple indices
that show relations among qualitative variables in a contingency
</p>
<p>table.
,! The joint representation of the indices reveals relations among the
</p>
<p>variables.
</p>
<p>15.2 Chi-Square Decomposition
</p>
<p>An alternative way of measuring the association between the row and column
</p>
<p>categories is a decomposition of the value of the �2-test statistic. The well-
</p>
<p>known �2-test for independence in a two-dimensional contingency table consists
</p>
<p>of two steps. First the expected value of each cell of the table is estimated under
</p>
<p>the hypothesis of independence. Second, the corresponding observed values are
</p>
<p>compared to the expected values using the statistic
</p>
<p>t D
nX
</p>
<p>iD1
</p>
<p>pX
</p>
<p>jD1
.xij � Eij/2=Eij; (15.3)
</p>
<p>where xij is the observed frequency in cell .i; j / and Eij is the corresponding
</p>
<p>estimated expected value under the assumption of independence, i.e.
</p>
<p>Eij D
xi� x�j
x��
</p>
<p>: (15.4)</p>
<p/>
</div>
<div class="page"><p/>
<p>15.2 Chi-Square Decomposition 429
</p>
<p>Here x�� D
Pn
</p>
<p>iD1 xi�. Under the hypothesis of independence, t has a �
2
.n�1/.p�1/
</p>
<p>distribution. In the industrial location example introduced above the value of t D
6:26 is almost significant at the 5% level. It is therefore worth investigating the
</p>
<p>special reasons for departure from independence.
</p>
<p>The method of �2 decomposition consists of finding the SVD of the matrix C .n�
p/ with elements
</p>
<p>cij D .xij � Eij/=E1=2ij : (15.5)
</p>
<p>The elements cij may be viewed as measuring the (weighted) departure between the
</p>
<p>observed xij and the theoretical values Eij under independence. This leads to the
</p>
<p>factorial tools of Chap. 10 which describe the rows and the columns of C.
</p>
<p>For simplification define the matricesA .n � n/ and B .p � p/ as
</p>
<p>A D diag.xi�/ and B D diag.x�j /: (15.6)
</p>
<p>These matrices provide the marginal row frequencies a.n � 1/ and the marginal
column frequencies b.p � 1/:
</p>
<p>a D A1n and b D B1p: (15.7)
</p>
<p>It is easy to verify that
</p>
<p>C
p
b D 0 and C&gt;
</p>
<p>p
a D 0; (15.8)
</p>
<p>where the square root of the vector is taken element by element andR D rank.C/ �
minf.n� 1/; .p � 1/g. From (10.14) of Chap. 10, the SVD of C yields
</p>
<p>C D &#128;ƒ&#129;&gt;; (15.9)
</p>
<p>where &#128; contains the eigenvectors of CC&gt;, &#129; the eigenvectors of C&gt;C and ƒ D
diag.�
</p>
<p>1=2
1 ; : : : ; �
</p>
<p>1=2
R / with �1 � �2 � � � � � �R (the eigenvalues of CC&gt;).
</p>
<p>Equation (15.9) implies that
</p>
<p>cij D
RX
</p>
<p>kD1
�
1=2
</p>
<p>k &#13;ikıjk: (15.10)
</p>
<p>Note that (15.3) can be rewritten as
</p>
<p>tr.CC&gt;/ D
RX
</p>
<p>kD1
�k D
</p>
<p>nX
</p>
<p>iD1
</p>
<p>pX
</p>
<p>jD1
c2ij D t: (15.11)</p>
<p/>
</div>
<div class="page"><p/>
<p>430 15 Correspondence Analysis
</p>
<p>This relation shows that the SVD of C decomposes the total �2 value rather than,
</p>
<p>as in Chap. 10, the total variance.
</p>
<p>The duality relations between the row and the column space (10.11) are now for
</p>
<p>k D 1; : : : ; R given by
</p>
<p>ık D 1p�k C
&gt;&#13;k ;
</p>
<p>&#13;k D 1p�k Cık :
(15.12)
</p>
<p>The projections of the rows and the columns of C are given by
</p>
<p>Cık D
p
�k&#13;k ;
</p>
<p>C&gt;&#13;k D
p
�kık :
</p>
<p>(15.13)
</p>
<p>Note that the eigenvectors satisfy
</p>
<p>ı&gt;k
p
b D 0; &#13;&gt;k
</p>
<p>p
a D 0: (15.14)
</p>
<p>From (15.10) we see that the eigenvectors ık and &#13;k are the objects of interest when
</p>
<p>analysing the correspondence between the rows and the columns. Suppose that the
</p>
<p>first eigenvalue in (15.10) is dominant so that
</p>
<p>cij � �1=21 &#13;i1ıj1: (15.15)
</p>
<p>In this case when the coordinates &#13;i1 and ıj1 are both large (with the same sign)
</p>
<p>relative to the other coordinates, then cij will be large as well, indicating a positive
</p>
<p>association between the i th row and the j th column category of the contingency
</p>
<p>table. If &#13;i1 and ıj1 were both large with opposite signs, then there would be a
</p>
<p>negative association between the i th row and j th column.
</p>
<p>In many applications, the first two eigenvalues, �1 and �2, dominate and the
</p>
<p>percentage of the total �2 explained by the eigenvectors &#13;1 and &#13;2 and ı1 and ı2 is
</p>
<p>large. In this case (15.13) and .&#13;1; &#13;2/ can be used to obtain a graphical display of the
</p>
<p>n rows of the table (.ı1; ı2/ play a similar role for the p columns of the table). The
</p>
<p>interpretation of the proximity between row and column points will be interpreted
</p>
<p>as above with respect to (15.10).
</p>
<p>In correspondence analysis, we use the projections of weighted rows of C and
</p>
<p>the projections of weighted columns of C for graphical displays. Let rk.n � 1/ be
the projections of A�1=2C on ık and sk.p � 1/ be the projections of B�1=2C&gt; on &#13;k
(k D 1; : : : ; R):
</p>
<p>rk D A�1=2Cık D
p
�kA
</p>
<p>�1=2&#13;k ;
sk D B�1=2C&gt;&#13;k D
</p>
<p>p
�kB
</p>
<p>�1=2ık :
(15.16)</p>
<p/>
</div>
<div class="page"><p/>
<p>15.2 Chi-Square Decomposition 431
</p>
<p>These vectors have the property that
</p>
<p>r&gt;k a D 0;
s&gt;k b D 0:
</p>
<p>(15.17)
</p>
<p>The obtained projections on each axis k D 1; : : : ; R are centred at zero with the
natural weights given by a (the marginal frequencies of the rows of X ) for the row
</p>
<p>coordinates rk and by b (the marginal frequencies of the columns of X ) for the
</p>
<p>column coordinates sk (compare this to expression (15.14)). As a result, the origin
</p>
<p>is the centre of gravity for all of the representations. We also know from (15.16) and
</p>
<p>the SVD of C that
</p>
<p>r&gt;k Ark D �k ;
s&gt;k Bsk D �k :
</p>
<p>(15.18)
</p>
<p>From the duality relation between ık and &#13;k (see (15.12)) we obtain
</p>
<p>rk D 1p�kA
�1=2CB1=2sk ;
</p>
<p>sk D 1p�k B
�1=2C&gt;A1=2rk ;
</p>
<p>(15.19)
</p>
<p>which can be simplified to
</p>
<p>rk D
q
</p>
<p>x��
�k
</p>
<p>A�1X sk ;
</p>
<p>sk D
q
</p>
<p>x��
�k
</p>
<p>B�1X&gt;rk:
(15.20)
</p>
<p>These vectors satisfy the relations (15.1) and (15.2) for each k D 1; : : : ; R
simultaneously.
</p>
<p>As in Chap. 10, the vectors rk and sk are referred to as factors (row factor and
</p>
<p>column factor respectively). They have the following means and variances:
</p>
<p>rk D 1x�� r
&gt;
k a D 0;
</p>
<p>sk D 1x�� s
&gt;
k b D 0;
</p>
<p>(15.21)
</p>
<p>Var.rk/ D 1x��
Pn
</p>
<p>iD1 xi�r
2
ki D
</p>
<p>r&gt;k Ark
x��
</p>
<p>D �k
x��
;
</p>
<p>Var.sk/ D 1x��
Pp
</p>
<p>jD1 x�j s
2
kj D
</p>
<p>s&gt;k Bsk
x��
D �k
</p>
<p>x��
:
</p>
<p>(15.22)
</p>
<p>Hence, �k=
Pj
</p>
<p>kD1 �j , which is the part of the kth factor in the decomposition of the
�2 statistic t , may also be interpreted as the proportion of the variance explained by
</p>
<p>the factor k. The proportions
</p>
<p>Ca.i; rk/ D
xi�r2ki
�k
</p>
<p>; for i D 1; : : : ; n; k D 1; : : : ; R (15.23)</p>
<p/>
</div>
<div class="page"><p/>
<p>432 15 Correspondence Analysis
</p>
<p>are called the absolute contributions of row i to the variance of the factor rk. They
</p>
<p>show which row categories are most important in the dispersion of the kth row
</p>
<p>factors. Similarly, the proportions
</p>
<p>Ca.j; sk/ D
x�j s2kj
�k
</p>
<p>; for j D 1; : : : ; p; k D 1; : : : ; R (15.24)
</p>
<p>are called the absolute contributions of column j to the variance of the column
</p>
<p>factor sk . These absolute contributions may help to interpret the graph obtained by
</p>
<p>correspondence analysis.
</p>
<p>15.3 Correspondence Analysis in Practice
</p>
<p>The graphical representations on the axes k D 1; 2; : : : ; R of the n rows and of the p
columns ofX are provided by the elements of rk and sk . Typically, two-dimensional
</p>
<p>displays are often satisfactory if the cumulated percentage of variance explained by
</p>
<p>the first two factors, &permil;2 D �1C�2PR
kD1 �k
</p>
<p>, is sufficiently large.
</p>
<p>The interpretation of the graphs may be summarised as follows:
</p>
<p>&ndash; The proximity of two rows (two columns) indicates a similar profile in these
</p>
<p>two rows (two columns), where &ldquo;profile&rdquo; refers to the conditional frequency
</p>
<p>distribution of a row (column); those two rows (columns) are almost proportional.
</p>
<p>The opposite interpretation applies when the two rows (two columns) are far
</p>
<p>apart.
</p>
<p>&ndash; The proximity of a particular row to a particular column indicates that this row
</p>
<p>(column) has a particularly important weight in this column (row). In contrast to
</p>
<p>this, a row that is quite distant from a particular column indicates that there are
</p>
<p>almost no observations in this column for this row (and vice versa). Of course, as
</p>
<p>mentioned above, these conclusions are particularly true when the points are far
</p>
<p>away from 0.
</p>
<p>&ndash; The origin is the average of the factors rk and sk . Hence, a particular point (row
</p>
<p>or column) projected close to the origin indicates an average profile.
</p>
<p>&ndash; The absolute contributions are used to evaluate the weight of each row (column)
</p>
<p>in the variances of the factors.
</p>
<p>&ndash; All the interpretations outlined abovemust be carried out in view of the quality of
</p>
<p>the graphical representation which is evaluated, as in PCA, using the cumulated
</p>
<p>percentage of variance.
</p>
<p>Remark 15.1 Note that correspondence analysis can also be applied to more general
</p>
<p>.n � p/ tables X which in a &ldquo;strict sense&rdquo; are not contingency tables.
As long as statistical (or natural) meaning can be given to sums over rows and
</p>
<p>columns, Remark 15.1 holds. This implies, in particular, that all of the variables
</p>
<p>are measured in the same units. In that case, x�� constitutes the total frequency</p>
<p/>
</div>
<div class="page"><p/>
<p>15.3 Correspondence Analysis in Practice 433
</p>
<p>of the observed phenomenon, and is shared between individuals (n rows) and
</p>
<p>between variables (p columns). Representations of the rows and columns of X , rk
and sk , have the basic property (15.19) and show which variables have important
</p>
<p>weights for each individual and vice versa. This type of analysis is used as an
</p>
<p>alternative to PCA. PCA is mainly concerned with covariances and correlations,
</p>
<p>whereas correspondence analysis analyses a more general kind of association. (See
</p>
<p>Exercises 15.3 and 15.11.)
</p>
<p>Example 15.3 A survey of Belgium citizens who regularly read a newspaper was
</p>
<p>conducted in the 1980s. They were asked where they lived. The possible answers
</p>
<p>were ten regions: seven provinces (Antwerp, Western Flanders, Eastern Flan-
</p>
<p>ders, Hainant, Li&egrave;ge, Limbourg, Luxembourg) and three regions around Brussels
</p>
<p>(Flemish-Brabant, Wallon-Brabant and the city of Brussels). They were also asked
</p>
<p>what kind of newspapers they read on a regular basis. There were 15 possible
</p>
<p>answers split up into three classes: Flemish newspapers (label begins with the letter
</p>
<p>v), French newspapers (label begins with f ) and both languages together (label
</p>
<p>begins with b). The data set is given in Table 22.9. The eigenvalues of the factorial
</p>
<p>correspondence analysis are given in Table 15.1.
</p>
<p>Two-dimensional representations will be quite satisfactory since the first two
</p>
<p>eigenvalues account for 81% of the variance. Figure 15.1 shows the projections
</p>
<p>of the rows (the 15 newspapers) and of the columns (the ten regions).
</p>
<p>As expected, there is a high association between the regions and the type
</p>
<p>of newspapers which is read. In particular, vb (Gazet van Antwerp) is almost
</p>
<p>exclusively read in the province of Antwerp (this is an extreme point in the graph).
</p>
<p>The points on the left all belong to Flanders, whereas those on the right all belong to
</p>
<p>Wallonia. Notice that the Wallon-Brabant and the Flemish-Brabant are not far from
</p>
<p>Brussels. Brussels is close to the centre (average) and also close to the bilingual
</p>
<p>newspapers. It is shifted a little to the right of the origin due to the majority of
</p>
<p>French speaking people in the area.
</p>
<p>The absolute contributions of the first three factors are listed in Tables 15.2
</p>
<p>and 15.3. The row factors rk are in Table 15.2 and the column factors sk are in
</p>
<p>Table 15.3.
</p>
<p>Table 15.1 Eigenvalues and
percentages of the variance
(Example 15.3)
</p>
<p>�j Percentage of variance Cumulated percentage
</p>
<p>183.40 0:653 0:653
</p>
<p>43.75 0:156 0:809
</p>
<p>25.21 0:090 0:898
</p>
<p>11.74 0:042 0:940
</p>
<p>8.04 0:029 0:969
</p>
<p>4.68 0:017 0:985
</p>
<p>2.13 0:008 0:993
</p>
<p>1.20 0:004 0:997
</p>
<p>0.82 0:003 1:000
</p>
<p>0.00 0:000 1:000</p>
<p/>
</div>
<div class="page"><p/>
<p>434 15 Correspondence Analysis
</p>
<p>-1 -0.5 0 -0.5 1 1.5
</p>
<p>-1
</p>
<p>-0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>va
</p>
<p>vb
</p>
<p>vc
</p>
<p>vd
</p>
<p>ve
</p>
<p>ff fg
</p>
<p>fh
</p>
<p>fibj
</p>
<p>bk
</p>
<p>bl
vm fn
</p>
<p>fo
brw
</p>
<p>bxl
</p>
<p>anv
</p>
<p>brffoc
</p>
<p>for
</p>
<p>hai
</p>
<p>lig
</p>
<p>lim
</p>
<p>lux
</p>
<p>Journal Data
</p>
<p>r
1
, s
</p>
<p>1
</p>
<p>r 2
, 
s
</p>
<p>2
</p>
<p>Fig. 15.1 Projection of rows (the 15 newspapers) and columns (the ten regions)
MVAcorrjourn
</p>
<p>Table 15.2 Absolute
contributions of row factors
</p>
<p>rk
</p>
<p>Ca.i; r1/ Ca.i; r2/ Ca.i; r3/
</p>
<p>va 0:0563 0:0008 0:0036
</p>
<p>vb 0:1555 0:5567 0:0067
</p>
<p>vc 0:0244 0:1179 0:0266
</p>
<p>vd 0:1352 0:0952 0:0164
</p>
<p>ve 0:0253 0:1193 0:0013
</p>
<p>ff 0:0314 0:0183 0:0597
</p>
<p>fg 0:0585 0:0162 0:0122
</p>
<p>fh 0:1086 0:0024 0:0656
</p>
<p>fi 0:1001 0:0024 0:6376
</p>
<p>bj 0:0029 0:0055 0:0187
</p>
<p>bk 0:0236 0:0278 0:0237
</p>
<p>bl 0:0006 0:0090 0:0064
</p>
<p>vm 0:1000 0:0038 0:0047
</p>
<p>fn 0:0966 0:0059 0:0269
</p>
<p>f0 0:0810 0:0188 0:0899
</p>
<p>Total 1:0000 1:0000 1:0000
</p>
<p>They show, for instance, the important role of Antwerp and the newspaper
</p>
<p>vb in determining the variance of both factors. Clearly, the first axis expresses
</p>
<p>linguistic differences between the three parts of Belgium. The second axis shows
</p>
<p>a larger dispersion between the Flemish region than the French speaking regions.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.3 Correspondence Analysis in Practice 435
</p>
<p>Table 15.3 Absolute
contributions of column
factors sk
</p>
<p>Ca.j; s1/ Ca.j; s2/ Ca.j; s3/
</p>
<p>brw 0:0887 0:0210 0:2860
</p>
<p>bxl 0:1259 0:0010 0:0960
</p>
<p>anv 0:2999 0:4349 0:0029
</p>
<p>brf 0:0064 0:2370 0:0090
</p>
<p>foc 0:0729 0:1409 0:0033
</p>
<p>for 0:0998 0:0023 0:0079
</p>
<p>hai 0:1046 0:0012 0:3141
</p>
<p>lig 0:1168 0:0355 0:1025
</p>
<p>lim 0:0562 0:1162 0:0027
</p>
<p>lux 0:0288 0:0101 0:1761
</p>
<p>Total 1:0000 1:0000 1:0000
</p>
<p>-0.2 -0.1 0 0.1 0.2 0.3
</p>
<p>-0.1
</p>
<p>0
</p>
<p>0.1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>Baccalaureat Data
</p>
<p>r
1
,s
</p>
<p>1
</p>
<p>r 2
,s
</p>
<p>2
</p>
<p>ildf
</p>
<p>cham
pica
</p>
<p>hnor
</p>
<p>cent
</p>
<p>bnor
</p>
<p>bour
</p>
<p>nopc
</p>
<p>lorr
</p>
<p>alsa
</p>
<p>frac
</p>
<p>payl
</p>
<p>bret
</p>
<p>pcha
aqui
</p>
<p>midi
</p>
<p>limorhoa
</p>
<p>auve
</p>
<p>laro prov
</p>
<p>cors
</p>
<p>A
</p>
<p>B
C
</p>
<p>D
</p>
<p>E
F
</p>
<p>G
</p>
<p>H
ildf
</p>
<p>cham
pica
</p>
<p>hnor
</p>
<p>cent
</p>
<p>bnor
</p>
<p>bour
</p>
<p>nopc
</p>
<p>lorr
</p>
<p>alsa
</p>
<p>frac
</p>
<p>payl
</p>
<p>bret
</p>
<p>pcha
aqui
</p>
<p>midi
</p>
<p>limorhoa
</p>
<p>auve
</p>
<p>laro prov
</p>
<p>cors
</p>
<p>A
</p>
<p>B
C
</p>
<p>D
</p>
<p>E
F
</p>
<p>G
</p>
<p>H
ildf
</p>
<p>cham
pica
</p>
<p>hnor
</p>
<p>cent
</p>
<p>bnor
</p>
<p>bour
</p>
<p>nopc
</p>
<p>lorr
</p>
<p>alsa
</p>
<p>frac
</p>
<p>payl
</p>
<p>bret
</p>
<p>pcha
aqui
</p>
<p>midi
</p>
<p>limorhoa
</p>
<p>auve
</p>
<p>laro prov
</p>
<p>cors
</p>
<p>A
</p>
<p>B
C
</p>
<p>D
</p>
<p>E
F
</p>
<p>G
</p>
<p>H
</p>
<p>Fig. 15.2 Correspondence analysis including Corsica MVAcorrbac
</p>
<p>Note also that the third axis shows an important role of the category &ldquo;fi&rdquo; (other
</p>
<p>French newspapers) with the Wallon-Brabant &ldquo;brw&rdquo; and the Hainant &ldquo;hai&rdquo; showing
</p>
<p>the most important contributions. The coordinate of &ldquo;fi&rdquo; on this axis is negative
</p>
<p>(not shown here) so are the coordinates of &ldquo;brw&rdquo; and &ldquo;hai&rdquo;. Apparently, these
</p>
<p>two regions also seem to feature a greater proportion of readers of more local
</p>
<p>newspapers.
</p>
<p>Example 15.4 Applying correspondence analysis to the French baccalaur&eacute;at data
</p>
<p>(Table 22.8) leads to Fig. 15.2. Excluding Corsica we obtain Fig. 15.3. The different</p>
<p/>
</div>
<div class="page"><p/>
<p>436 15 Correspondence Analysis
</p>
<p>-0.2 -0.1 0 0.1 0.2
</p>
<p>-0.15
</p>
<p>-0.1
</p>
<p>-0.05
</p>
<p>0
</p>
<p>0.05
</p>
<p>0.1
</p>
<p>Baccalaureat Data
</p>
<p>r
1
,s
</p>
<p>1
</p>
<p>r 2
,s
</p>
<p>2
</p>
<p>ildf
</p>
<p>cham
</p>
<p>pica
</p>
<p>hnor
</p>
<p>cent
</p>
<p>bnor
</p>
<p>bour
</p>
<p>nopc
</p>
<p>lorr
</p>
<p>alsa
</p>
<p>frac
</p>
<p>payl
</p>
<p>bret
</p>
<p>pcha
</p>
<p>aqui
</p>
<p>midi
</p>
<p>limo
rhoa
</p>
<p>auve
</p>
<p>laro
</p>
<p>provA
</p>
<p>B
</p>
<p>C
</p>
<p>D
</p>
<p>E
</p>
<p>F
</p>
<p>G
</p>
<p>H
</p>
<p>Fig. 15.3 Correspondence analysis excluding Corsica MVAcorrbac
</p>
<p>Table 15.4 Eigenvalues and percentages of explained variance (including Corsica)
</p>
<p>Eigenvalues � Percentage of variances Cumulated percentage
</p>
<p>2,436.2 0:5605 0:561
</p>
<p>1,052.4 0:2421 0:803
</p>
<p>341.8 0:0786 0:881
</p>
<p>229.5 0:0528 0:934
</p>
<p>152.2 0:0350 0:969
</p>
<p>109.1 0:0251 0:994
</p>
<p>25.0 0:0058 1:000
</p>
<p>0.0 0:0000 1:000
</p>
<p>modalities are labeled A, . . . , H and the regions are labeled ILDF, . . . , CORS.
</p>
<p>The results of the correspondence analysis are given in Table 15.4 and Fig. 15.2.
</p>
<p>The first two factors explain 80% of the total variance. It is clear from Fig. 15.2
</p>
<p>that Corsica (in the upper left) is an outlier. The analysis is therefore redone without
</p>
<p>Corsica and the results are given in Table 15.5 and Fig. 15.3. Since Corsica has such
</p>
<p>a small weight in the analysis, the results have not changed much.
</p>
<p>The projections on the first three axes, along with their absolute contribution
</p>
<p>to the variance of the axis, are summarised in Table 15.6 for the regions and in
</p>
<p>Table 15.7 for baccalaur&eacute;ats.
</p>
<p>The interpretation of the results may be summarised as follows. Table 15.7 shows
</p>
<p>that the baccalaur&eacute;ats B on one side and F on the other side are most strongly</p>
<p/>
</div>
<div class="page"><p/>
<p>15.3 Correspondence Analysis in Practice 437
</p>
<p>Table 15.5 Eigenvalues and percentages of explained variance (excluding Corsica)
</p>
<p>Eigenvalues � Percentage of variances Cumulated percentage
</p>
<p>2,408.6 0:5874 0:587
</p>
<p>909.5 0:2218 0:809
</p>
<p>318.5 0:0766 0:887
</p>
<p>195.9 0:0478 0:935
</p>
<p>149.3 0:0304 0:971
</p>
<p>96.1 0:0234 0:994
</p>
<p>22.8 0:0056 1:000
</p>
<p>0.0 0:0000 1:000
</p>
<p>Table 15.6 Coefficients and absolute contributions for regions, Example 15.4
</p>
<p>Region r1 r2 r3 Ca.i; r1/ Ca.i; r2/ Ca.i; r3/
</p>
<p>ILDF 0:1464 0:0677 0:0157 0:3839 0:2175 0:0333
</p>
<p>CHAM �0:0603 �0:0410 �0:0187 0:0064 0:0078 0:0047
PICA 0:0323 �0:0258 �0:0318 0:0021 0:0036 0:0155
HNOR �0:0692 0:0287 0:1156 0:0096 0:0044 0:2035
CENT �0:0068 �0:0205 �0:0145 0:0001 0:0030 0:0043
BNOR �0:0271 �0:0762 0:0061 0:0014 0:0284 0:0005
BOUR �0:1921 0:0188 0:0578 0:0920 0:0023 0:0630
NOPC �0:1278 0:0863 �0:0570 0:0871 0:1052 0:1311
LORR �0:2084 0:0511 0:0467 0:1606 0:0256 0:0608
ALSA �0:2331 0:0838 0:0655 0:1283 0:0439 0:0767
FRAC �0:1304 �0:0368 �0:0444 0:0265 0:0056 0:0232
PAYL �0:0743 �0:0816 �0:0341 0:0232 0:0743 0:0370
BRET 0:0158 0:0249 �0:0469 0:0011 0:0070 0:0708
PCHA �0:0610 �0:1391 �0:0178 0:0085 0:1171 0:0054
AQUI 0:0368 �0:1183 0:0455 0:0055 0:1519 0:0643
MIDI 0:0208 �0:0567 0:0138 0:0018 0:0359 0:0061
LIMO �0:0540 0:0221 �0:0427 0:0033 0:0014 0:0154
RHOA �0:0225 0:0273 �0:0385 0:0042 0:0161 0:0918
AUVE 0:0290 �0:0139 �0:0554 0:0017 0:0010 0:0469
LARO 0:0290 �0:0862 �0:0177 0:0383 0:0595 0:0072
PROV 0:0469 �0:0717 0:0279 0:0142 0:0884 0:0383
</p>
<p>responsible for the variation on the first axis. The second axis mostly characterises
</p>
<p>an opposition between baccalaur&eacute;ats A and C. Regarding the regions, Ile de France
</p>
<p>plays an important role on each axis. On the first axis, it is opposed to Lorraine
</p>
<p>and Alsace, whereas on the second axis, it is opposed to Poitou-Charentes and
</p>
<p>Aquitaine. All of this is confirmed in Fig. 15.3.
</p>
<p>On the right side are the more classical baccalaur&eacute;ats and on the left, more
</p>
<p>technical ones. The regions on the left side have thus larger weights in the technical</p>
<p/>
</div>
<div class="page"><p/>
<p>438 15 Correspondence Analysis
</p>
<p>Table 15.7 Coefficients and absolute contributions for baccalaur&eacute;ats, Example 15.4
</p>
<p>Baccal s1 s2 s3 Ca.j; s1/ Ca.j; s2/ Ca.j; s3/
</p>
<p>A 0:0447 �0:0679 0:0367 0:0376 0:2292 0:1916
B 0:1389 0:0557 0:0011 0:1724 0:0735 0:0001
</p>
<p>C 0:0940 0:0995 0:0079 0:1198 0:3556 0:0064
</p>
<p>D 0:0227 �0:0495 �0:0530 0:0098 0:1237 0:4040
E �0:1932 0:0492 �0:1317 0:0825 0:0141 0:2900
F �0:2156 0:0862 0:0188 0:3793 0:1608 0:0219
G �0:1244 �0:0353 0:0279 0:1969 0:0421 0:0749
H �0:0945 0:0438 �0:0888 0:0017 0:0010 0:0112
</p>
<p>Table 15.8 Eigenvalues and
explained proportion of
variance, Example 15.5
</p>
<p>�j Percentage of variance Cumulated percentage
</p>
<p>4,399.0 0:4914 0:4914
</p>
<p>2,213.6 0:2473 0:7387
</p>
<p>1,382.4 0:1544 0:8932
</p>
<p>870.7 0:0973 0:9904
</p>
<p>51.0 0:0057 0:9961
</p>
<p>34.8 0:0039 1:0000
</p>
<p>0.0 0:0000 0:0000
</p>
<p>baccalaur&eacute;ats. Note also that most of the southern regions of France are concentrated
</p>
<p>in the lower part of the graph near the baccalaur&eacute;at A.
</p>
<p>Finally, looking at the third axis, we see that it is dominated by the baccalaur&eacute;at
</p>
<p>E (negative sign) and to a lesser degree by H (negative) (as opposed to A (positive
</p>
<p>sign)). The dominating regions are HNOR (positive sign), opposed to NOPC and
</p>
<p>AUVE (negative sign). For instance, HNOR is particularly poor in baccalaur&eacute;at D.
</p>
<p>Example 15.5 The US crime data set (Table 22.10) gives the number of crimes in
</p>
<p>the 50 states of the US classified in 1985 for each of the following seven categories:
</p>
<p>murder, rape, robbery, assault, burglary, larceny and auto-theft. The analysis of the
</p>
<p>contingency table, limited to the first two factors, provides the following results (see
</p>
<p>Table 15.8).
</p>
<p>Looking at the absolute contributions (not reproduced here, see Exercise 15.6), it
</p>
<p>appears that the first axis is robbery (C) versus larceny (�) and auto-theft (�) axis
and that the second factor contrasts assault (�) to auto-theft (C). The dominating
states for the first axis are the North-Eastern States MA (C) and NY (C) contrasting
the Western States WY (�) and ID (�). For the second axis, the differences are
seen between the Northern States (MA (C) and RI (C)) and the Southern States
AL (�), MS (�) and AR (�). These results can be clearly seen in Fig. 15.4 where
all the states and crimes are reported. The figure also shows in which states the
</p>
<p>proportion of a particular crime category is higher or lower than the national average
</p>
<p>(the origin).</p>
<p/>
</div>
<div class="page"><p/>
<p>15.3 Correspondence Analysis in Practice 439
</p>
<p>-0.4 -0.2 0 0.2 0.4 0.6
</p>
<p>-0.2
</p>
<p>0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>mur
</p>
<p>rap
</p>
<p>rob
</p>
<p>ass
</p>
<p>bur
</p>
<p>lar
</p>
<p>aut
</p>
<p>ME
</p>
<p>NH
</p>
<p>VT
</p>
<p>MA
RI
</p>
<p>CT
</p>
<p>NY
</p>
<p>NJ
</p>
<p>PA
</p>
<p>OH
IN
</p>
<p>IL
</p>
<p>MI
</p>
<p>WI MN
</p>
<p>IA
</p>
<p>MO
</p>
<p>ND
</p>
<p>SD
</p>
<p>NE
</p>
<p>KS
</p>
<p>DE
</p>
<p>MDVA
</p>
<p>VW
</p>
<p>NC
</p>
<p>SC
</p>
<p>GA
</p>
<p>FL
</p>
<p>KY
</p>
<p>TN
</p>
<p>AL
</p>
<p>MS
</p>
<p>AR
</p>
<p>LA
</p>
<p>OK
TX
</p>
<p>MT
</p>
<p>ID
WY
</p>
<p>CO
</p>
<p>NM
</p>
<p>AZ
</p>
<p>UT
</p>
<p>NV
WA
</p>
<p>OR
CA
</p>
<p>AK
</p>
<p>HI
</p>
<p>US Crime Data
</p>
<p>r
1
,s
</p>
<p>1
</p>
<p>r 2
,s
</p>
<p>2
</p>
<p>Fig. 15.4 Projection of rows (the 50 states) and columns (the seven crime categories)
MVAcorrcrime
</p>
<p>Biplots
</p>
<p>The biplot is a low-dimensional display of a data matrix X where the rows and
</p>
<p>columns are represented by points. The interpretation of a biplot is specifically
</p>
<p>directed towards the scalar products of lower dimensional factorial variables and
</p>
<p>is designed to approximately recover the individual elements of the data matrix in
</p>
<p>these scalar products. Suppose that we have a (10�5) data matrix with elements xij.
The idea of the biplot is to find 10 row points qi 2 Rk (k &lt; p; i D 1; : : : ; 10) and
5 column points tj 2 Rk (j D 1; : : : ; 5) such that the 50 scalar products between
the row and the column vectors closely approximate the 50 corresponding elements
</p>
<p>of the data matrix X . Usually we choose k D 2. For example, the scalar product
between q7 and t4 should approximate the data value x74 in the seventh row and
</p>
<p>the fourth column. In general, the biplot models the data xij as the sum of a scalar
</p>
<p>product in some low-dimensional subspace and a residual &ldquo;error&rdquo; term:
</p>
<p>xij D q&gt;i tj C eij
</p>
<p>D
X
</p>
<p>k
</p>
<p>qiktjk C eij: (15.25)</p>
<p/>
</div>
<div class="page"><p/>
<p>440 15 Correspondence Analysis
</p>
<p>To understand the link between correspondence analysis and the biplot, we need to
</p>
<p>introduce a formula which expresses xij from the original data matrix (see (15.3)) in
</p>
<p>terms of row and column frequencies. One such formula, known as the &ldquo;reconstitu-
</p>
<p>tion formula&rdquo;, is (15.10):
</p>
<p>xij D Eij
</p>
<p>0
B@1C
</p>
<p>PR
kD1 �
</p>
<p>1
2
</p>
<p>k &#13;ikıjkq
xi�x�j
x��
</p>
<p>1
CA (15.26)
</p>
<p>Consider now the row profiles xij=xi� (the conditional frequencies) and the average
row profile xi�=x��. From (15.26) we obtain the difference between each row profile
and this average:
</p>
<p>�
xij
</p>
<p>xi�
� xi�
x��
</p>
<p>�
D
</p>
<p>RX
</p>
<p>kD1
�
1
2
</p>
<p>k &#13;ik
</p>
<p>�r
x�j
xi�x��
</p>
<p>�
ıjk: (15.27)
</p>
<p>By the same argument we can also obtain the difference between each column
</p>
<p>profile and the average column profile:
</p>
<p>�
xij
</p>
<p>x�j
� x�j
x��
</p>
<p>�
D
</p>
<p>RX
</p>
<p>kD1
�
1
2
</p>
<p>k &#13;ik
</p>
<p>�r
xi�
</p>
<p>x�jx��
</p>
<p>�
ıjk: (15.28)
</p>
<p>Now, if �1 � �2 � �3 : : :, we can approximate these sums by a finite number of
K terms (usuallyK D 2) using (15.16) to obtain
</p>
<p>�
xij
</p>
<p>x�j
� xi�
x��
</p>
<p>�
D
</p>
<p>KX
</p>
<p>kD1
</p>
<p>�
x�ip
�kx��
</p>
<p>rki
</p>
<p>�
skj C eij; (15.29)
</p>
<p>�
xij
</p>
<p>xi�
� x�j
x��
</p>
<p>�
D
</p>
<p>KX
</p>
<p>kD1
</p>
<p>�
x�jp
�kx��
</p>
<p>skj
</p>
<p>�
rki C e0ij; (15.30)
</p>
<p>where eij and e
0
ij are error terms. Equation (15.30) shows that if we consider
</p>
<p>displaying the differences between the row profiles and the average profile, then
</p>
<p>the projection of the row profile rk and a rescaled version of the projections of the
</p>
<p>column profile sk constitute a biplot of these differences. Equation (15.29) implies
</p>
<p>the same for the differences between the column profiles and this average.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.4 Exercises 441
</p>
<p>Summary
</p>
<p>,! Correspondence analysis is a factorial decomposition of contin-
gency tables. The p-dimensional individuals and the n-dimensional
</p>
<p>variables can be graphically represented by projecting onto spaces
</p>
<p>of smaller dimension.
,! The practical computation consists of first computing a spectral
</p>
<p>decomposition ofA�1XB�1X&gt; andB�1X&gt;A�1X which have the
same first p eigenvalues. The graphical representation is obtained
</p>
<p>by plotting
p
�1r1 vs.
</p>
<p>p
�2r2 and
</p>
<p>p
�1s1 vs.
</p>
<p>p
�2s2. Both plots
</p>
<p>maybe displayed in the same graph taking into account the appro-
</p>
<p>priate orientation of the eigenvectors ri ; sj .
</p>
<p>,! Correspondence analysis provides a graphical display of the asso-
ciation measure cij D .xij � Eij/2=Eij.
</p>
<p>,! Biplot is a low-dimensional display of a data matrix where the rows
and columns are represented by points
</p>
<p>15.4 Exercises
</p>
<p>Exercise 15.1 Show that the matrices A�1XB�1X&gt; and B�1X&gt;A�1X have an
eigenvalue equal to 1 and that the corresponding eigenvectors are proportional to
</p>
<p>.1; : : : ; 1/&gt;.
</p>
<p>Exercise 15.2 Verify the relations in (15.8), (15.14) and (15.17).
</p>
<p>Exercise 15.3 Do a correspondence analysis for the car marks data (Table 22.7)!
</p>
<p>Explain how this table can be considered as a contingency table.
</p>
<p>Exercise 15.4 Compute the �2-statistic of independence for the French baccalau-
</p>
<p>r&eacute;at data.
</p>
<p>Exercise 15.5 Prove that C D A�1=2.X � E/B�1=2px�� and E D ab
&gt;
</p>
<p>x��
and
</p>
<p>verify (15.20).
</p>
<p>Exercise 15.6 Do the full correspondence analysis of the US crime data
</p>
<p>(Table 22.10), and determine the absolute contributions for the first three axes.
</p>
<p>How can you interpret the third axis? Try to identify the states with one of the four
</p>
<p>regions to which it belongs. Do you think the four regions have a different behaviour
</p>
<p>with respect to crime?
</p>
<p>Exercise 15.7 Repeat Exercise 15.6 with the US health data (Table 22.16). Only
</p>
<p>analyse the columns indicating the number of deaths per state.</p>
<p/>
</div>
<div class="page"><p/>
<p>442 15 Correspondence Analysis
</p>
<p>Exercise 15.8 Consider a .n � n/ contingency table being a diagonal matrix X .
What do you expect the factors rk ; sk to be like?
</p>
<p>Exercise 15.9 Assume that after some reordering of the rows and the columns, the
</p>
<p>contingency table has the following structure:
</p>
<p>X D
J1 J2
</p>
<p>I1 � 0
I2 0 �
</p>
<p>That is, the rows Ii only have weights in the columns Ji , for i D 1; 2. What do you
expect the graph of the first two factors to look like?
</p>
<p>Exercise 15.10 Redo Exercise 15.9 using the following contingency table:
</p>
<p>X D
</p>
<p>J1 J2 J3
</p>
<p>I1 � 0 0
I2 0 � 0
I3 0 0 �
</p>
<p>Exercise 15.11 Consider the French food data (Table 22.6). Given that all of the
</p>
<p>variables are measured in the same units (Francs), explain how this table can be
</p>
<p>considered as a contingency table. Perform a correspondence analysis and compare
</p>
<p>the results to those obtained in the NPCA analysis in Chap. 11.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 16
</p>
<p>Canonical Correlation Analysis
</p>
<p>Complex multivariate data structures are better understood by studying low-
</p>
<p>dimensional projections. For a joint study of two data sets, we may ask what type
</p>
<p>of low-dimensional projection helps in finding possible joint structures for the two
</p>
<p>samples. The canonical correlation analysis (CCA) is a standard tool of multivariate
</p>
<p>statistical analysis for discovery and quantification of associations between two sets
</p>
<p>of variables.
</p>
<p>The basic technique is based on projections. One defines an index (projected
</p>
<p>multivariate variable) that maximally correlates with the index of the other variable
</p>
<p>for each sample separately. The aim of CCA is to maximise the association
</p>
<p>(measured by correlation) between the low-dimensional projections of the two data
</p>
<p>sets. The canonical correlation vectors are found by a joint covariance analysis
</p>
<p>of the two variables. The technique is applied to a marketing example where the
</p>
<p>association of a price factor and other variables (like design, sportiness etc.) is
</p>
<p>analysed. Tests are given on how to evaluate the significance of the discovered
</p>
<p>association.
</p>
<p>16.1 Most Interesting Linear Combination
</p>
<p>The associations between two sets of variables may be identified and quantified by
</p>
<p>CCA. The technique was originally developed by Hotelling (1935) who analysed
</p>
<p>how arithmetic speed and arithmetic power are related to reading speed and reading
</p>
<p>power. Other examples are the relation between governmental policy variables
</p>
<p>and economic performance variables and the relation between job and company
</p>
<p>characteristics.
</p>
<p>&copy; Springer-Verlag Berlin Heidelberg 2015
W.K. H&auml;rdle, L. Simar, Applied Multivariate Statistical Analysis,
DOI 10.1007/978-3-662-45171-7_16
</p>
<p>443</p>
<p/>
</div>
<div class="page"><p/>
<p>444 16 Canonical Correlation Analysis
</p>
<p>Suppose we are given two random variablesX 2 Rq and Y 2 Rp . The idea is to
find an index describing a (possible) link betweenX and Y . CCA is based on linear
</p>
<p>indices, i.e. linear combinations
</p>
<p>a&gt;X and b&gt;Y
</p>
<p>of the random variables. CCA searches for vectors a and b such that the relation
</p>
<p>of the two indices a&gt;x and b&gt;y is quantified in some interpretable way. More
precisely, one is looking for the &ldquo;most interesting&rdquo; projections a and b in the sense
</p>
<p>that they maximise the correlation
</p>
<p>�.a; b/ D �a&gt;X b&gt;Y (16.1)
</p>
<p>between the two indices.
</p>
<p>Let us consider the correlation �.a; b/ between the two projections in more detail.
</p>
<p>Suppose that
</p>
<p> 
X
</p>
<p>Y
</p>
<p>!
�
  
</p>
<p>�
</p>
<p>�
</p>
<p>!
;
</p>
<p>�
&dagger;XX
</p>
<p>&dagger;YX
</p>
<p>&dagger;XY
</p>
<p>&dagger;YY
</p>
<p>� !
</p>
<p>where the sub-matrices of this covariance structure are given by
</p>
<p>Var.X/ D &dagger;XX .q � q/
Var.Y / D &dagger;YY .p � p/
</p>
<p>Cov.X; Y / D E.X � �/.Y � �/&gt; D &dagger;XY D &dagger;&gt;YX .q � p/:
</p>
<p>Using (3.7) and (4.26),
</p>
<p>�.a; b/ D a
&gt;&dagger;XYb
</p>
<p>.a&gt;&dagger;XXa/1=2 .b&gt;&dagger;YYb/1=2
� (16.2)
</p>
<p>Therefore, �.ca; b/ D �.a; b/ for any c 2 RC. Given the invariance of scale we
may rescale projections a and b and thus we can equally solve
</p>
<p>max
a;b
</p>
<p>a&gt;&dagger;XYb
</p>
<p>under the constraints
</p>
<p>a&gt;&dagger;XXa D 1
b&gt;&dagger;YYb D 1:</p>
<p/>
</div>
<div class="page"><p/>
<p>16.1 Most Interesting Linear Combination 445
</p>
<p>For this problem, define
</p>
<p>K D &dagger;�1=2XX &dagger;XY&dagger;
�1=2
YY : (16.3)
</p>
<p>Recall the singular value decomposition ofK.q�p/ from Theorem 2.2. The matrix
K may be decomposed as
</p>
<p>K D &#128;ƒ&#129;&gt;
</p>
<p>with
</p>
<p>&#128; D .&#13;1; : : : ; &#13;k/
&#129; D .ı1; : : : ; ık/ (16.4)
</p>
<p>ƒ D diag.�1=21 ; : : : ; �
1=2
</p>
<p>k /
</p>
<p>where by (16.3) and (2.15),
</p>
<p>k D rank.K/ D rank.&dagger;XY/ D rank.&dagger;YX/ ;
</p>
<p>and �1 � �2 � � � ��k are the nonzero eigenvalues of N1 D KK&gt; and N2 D K&gt;K
and &#13;i and ıj are the standardised eigenvectors of N1 and N2 respectively.
</p>
<p>Define now for i D 1; : : : ; k the vectors
</p>
<p>ai D &dagger;�1=2XX &#13;i ; (16.5)
</p>
<p>bi D &dagger;�1=2YY ıi ; (16.6)
</p>
<p>which are called the canonical correlation vectors. Using these canonical correla-
</p>
<p>tion vectors we define the canonical correlation variables
</p>
<p>�i D a&gt;i X (16.7)
'i D b&gt;i Y: (16.8)
</p>
<p>The quantities �i D �1=2i for i D 1; : : : ; k are called the canonical correlation
coefficients.
</p>
<p>From the properties of the singular value decomposition given in (16.4) we have
</p>
<p>Cov.�i ; �j / D a&gt;i &dagger;XXaj D &#13;&gt;i &#13;j D
�
1 i D j;
0 i &curren; j: (16.9)
</p>
<p>The same is true for Cov.'i ; 'j /. The following theorem tells us that the canonical
</p>
<p>correlation vectors are the solution to the maximisation problem of (16.1).</p>
<p/>
</div>
<div class="page"><p/>
<p>446 16 Canonical Correlation Analysis
</p>
<p>Theorem 16.1 For any given r , 1 � r � k, the maximum
</p>
<p>C.r/ D max
a;b
</p>
<p>a&gt;&dagger;XYb (16.10)
</p>
<p>subject to
</p>
<p>a&gt;&dagger;XXa D 1; b&gt;&dagger;YYb D 1
</p>
<p>and
</p>
<p>a&gt;i &dagger;XXa D 0 for i D 1; : : : ; r � 1
</p>
<p>is given by
</p>
<p>C.r/ D �r D �1=2r
</p>
<p>and is attained when a D ar and b D br .
Proof The proof is given in three steps.
</p>
<p>(i) Fix a and maximise over b, i.e. solve:
</p>
<p>max
b
</p>
<p>�
a&gt;&dagger;XYb
</p>
<p>�2 D max
b
</p>
<p>�
b&gt;&dagger;YXa
</p>
<p>� �
a&gt;&dagger;XYb
</p>
<p>�
</p>
<p>subject to b&gt;&dagger;YYb D 1. By Theorem 2.5 the maximum is given by the largest
eigenvalue of the matrix
</p>
<p>&dagger;�1YY&dagger;YXaa
&gt;&dagger;XY :
</p>
<p>By Corollary 2.2, the only nonzero eigenvalue equals
</p>
<p>a&gt;&dagger;XY&dagger;
�1
YY&dagger;YXa: (16.11)
</p>
<p>(ii) Maximise (16.11) over a subject to the constraints of the theorem. Put &#13; D
&dagger;
1=2
XX a and observe that (16.11) equals
</p>
<p>&#13;&gt;&dagger;�1=2XX &dagger;XY&dagger;
�1
YY&dagger;YX&dagger;
</p>
<p>�1=2
XX &#13; D &#13;&gt;K&gt;K&#13;:
</p>
<p>Thus, solve the equivalent problem
</p>
<p>max
&#13;
&#13;&gt;N1&#13; (16.12)
</p>
<p>subject to &#13;&gt;&#13; D 1, &#13;&gt;i &#13; D 0 for i D 1; : : : ; r � 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>16.1 Most Interesting Linear Combination 447
</p>
<p>Note that the &#13;i &rsquo;s are the eigenvectors ofN1 corresponding to its first r � 1
largest eigenvalues. Thus, as in Theorem 11.3, the maximum in (16.12) is
</p>
<p>obtained by setting &#13; equal to the eigenvector corresponding to the r th largest
</p>
<p>eigenvalue, i.e. &#13; D &#13;r or equivalently a D ar . This yields
</p>
<p>C 2.r/ D &#13;&gt;r N1&#13;r D �r&#13;&gt;r &#13; D �r :
</p>
<p>(iii) Show that the maximum is attained for a D ar and b D br . From the SVD of
K we conclude that Kır D �r&#13;r and hence
</p>
<p>a&gt;r &dagger;XYbr D &#13;&gt;r Kır D �r&#13;&gt;r &#13;r D �r :
ut
</p>
<p>Let
</p>
<p>�
X
</p>
<p>Y
</p>
<p>�
�
��
</p>
<p>�
</p>
<p>�
</p>
<p>�
;
</p>
<p>�
&dagger;XX &dagger;XY
&dagger;YX &dagger;YY
</p>
<p>��
:
</p>
<p>The canonical correlation vectors
</p>
<p>a1 D &dagger;�1=2XX &#13;1;
</p>
<p>b1 D &dagger;�1=2YY ı1
</p>
<p>maximise the correlation between the canonical variables
</p>
<p>�1 D a&gt;1 X;
</p>
<p>'1 D b&gt;1 Y:
</p>
<p>The covariance of the canonical variables � and ' is given in the next theorem.
</p>
<p>Theorem 16.2 Let �i and 'i be the i th canonical correlation variables (i D
1; : : : ; k). Define � D .�1; : : : ; �k/ and ' D .'1; : : : ; 'k/. Then
</p>
<p>Var
</p>
<p>�
�
</p>
<p>'
</p>
<p>�
D
�
Ik ƒ
</p>
<p>ƒ Ik
</p>
<p>�
</p>
<p>with ƒ given in (16.4).
</p>
<p>This theorem shows that the canonical correlation coefficients, �i D �1=2i , are
the covariances between the canonical variables �i and 'i and that the indices �1 D
a&gt;1 X and '1 D b&gt;1 Y have the maximum covariance
</p>
<p>p
�1 D �1.
</p>
<p>The following theorem shows that canonical correlations are invariant w.r.t. linear
</p>
<p>transformations of the original variables.</p>
<p/>
</div>
<div class="page"><p/>
<p>448 16 Canonical Correlation Analysis
</p>
<p>Theorem 16.3 Let X� D U&gt;X C u and Y � D V&gt;Y C v where U and V are
nonsingular matrices. Then the canonical correlations between X� and Y � are the
same as those between X and Y . The canonical correlation vectors of X� and Y �
</p>
<p>are given by
</p>
<p>a�i D U�1ai ;
b�i D V�1bi : (16.13)
</p>
<p>Summary
</p>
<p>,! CCA aims to identify possible links between two (sub-)sets of
variables X 2 Rq and Y 2 Rp . The idea is to find indices a&gt;X
and b&gt;Y such that the correlation �.a; b/ D �a&gt;Xb&gt;Y is maximal.
</p>
<p>,! The maximum correlation (under constraints) is attained by setting
ai D &dagger;�1=2XX &#13;i and bi D &dagger;
</p>
<p>�1=2
YY ıi , where &#13;i and ıi denote the eigen-
</p>
<p>vectors of KK&gt; and K&gt;K, K D &dagger;�1=2XX &dagger;XY&dagger;
�1=2
YY respectively.
</p>
<p>,! The vectors ai and bi are called canonical correlation vectors.
</p>
<p>,! The indices �i D a&gt;i X and 'i D b&gt;i Y are called canonical
correlation variables.
</p>
<p>,! The values �1 D
p
�1; : : : ; �k D
</p>
<p>p
�k , which are the square
</p>
<p>roots of the nonzero eigenvalues of KK&gt; and K&gt;K, are called
the canonical correlation coefficients. The covariance between
</p>
<p>the canonical correlation variables is Cov.�i ; 'i/ D
p
�i , i D
</p>
<p>1; : : : ; k.
</p>
<p>,! The first canonical variables, �1 D a&gt;1 X and '1 D b&gt;1 Y , have the
maximum covariance
</p>
<p>p
�1.
</p>
<p>,! Canonical correlations are invariant w.r.t. linear transformations of
the original variablesX and Y .
</p>
<p>16.2 Canonical Correlation in Practice
</p>
<p>In practice we have to estimate the covariance matrices &dagger;XX, &dagger;XY and &dagger;YY . Let us
</p>
<p>apply the CCA to the car marks data (see Table 22.7). In the context of this data
</p>
<p>set one is interested in relating price variables with variables such as sportiness
</p>
<p>and safety. In particular, we would like to investigate the relation between the two
</p>
<p>variables non-depreciation of value and price of the car and all other variables.</p>
<p/>
</div>
<div class="page"><p/>
<p>16.2 Canonical Correlation in Practice 449
</p>
<p>Example 16.1 We perform the CCA on the data matrices X and Y that correspond
</p>
<p>to the set of values fPrice, Value Stabilityg and fEconomy, Service, Design, Sporty
car, Safety, Easy handlingg, respectively. The estimated covariance matrix S is
given by
</p>
<p>Price Value Econ. Serv. Design Sport. Safety Easy h.
</p>
<p>S D
</p>
<p>0
BBBBBBBBBBBBB@
</p>
<p>1:41 �1:11 j 0:78 �0:71 �0:90 �1:04 �0:95 0:18
�1:11 1:19 j �0:42 0:82 0:77 0:90 1:12 0:11
� �� � �� j � � � � �� � �� � �� � � � �� �
0:78 �0:42 j 0:75 �0:23 �0:45 �0:42 �0:28 0:28
�0:71 0:82 j �0:23 0:66 0:52 0:57 0:85 0:14
�0:90 0:77 j �0:45 0:52 0:72 0:77 0:68 �0:10
�1:04 0:90 j �0:42 0:57 0:77 1:05 0:76 �0:15
�0:95 1:12 j �0:28 0:85 0:68 0:76 1:26 0:22
0:18 0:11 j 0:28 0:14 �0:10 �0:15 0:22 0:32
</p>
<p>1
CCCCCCCCCCCCCA
</p>
<p>:
</p>
<p>Hence,
</p>
<p>SXX D
�
</p>
<p>1:41 �1:11
�1:11 1:19
</p>
<p>�
; SXY D
</p>
<p>�
0:78 �0:71 �0:90 �1:04 �0:95 0:18
�0:42 0:82 0:77 0:90 1:12 0:11
</p>
<p>�
;
</p>
<p>SYY D
</p>
<p>0
BBBBBBB@
</p>
<p>0:75 �0:23 �0:45 �0:42 �0:28 0:28
�0:23 0:66 0:52 0:57 0:85 0:14
�0:45 0:52 0:72 0:77 0:68 �0:10
�0:42 0:57 0:77 1:05 0:76 �0:15
�0:28 0:85 0:68 0:76 1:26 0:22
0:28 0:14 �0:10 �0:15 0:22 0:32
</p>
<p>1
CCCCCCCA
:
</p>
<p>It is interesting to see that value stability and price have a negative covariance. This
</p>
<p>makes sense since highly priced vehicles tend to loose their market value at a faster
</p>
<p>pace than medium priced vehicles.
</p>
<p>Now we estimate K D &dagger;�1=2XX &dagger;XY &dagger;
�1=2
YY by
</p>
<p>OK D S�1=2XX SXY S
�1=2
YY
</p>
<p>and perform a singular value decomposition of OK:
</p>
<p>OK D GLD&gt; D .g1; g2/ diag.`1=21 ; `
1=2
2 / .d1; d2/
</p>
<p>&gt;</p>
<p/>
</div>
<div class="page"><p/>
<p>450 16 Canonical Correlation Analysis
</p>
<p>8 9 10 11 12 13 14
4
</p>
<p>5
</p>
<p>6
</p>
<p>7
</p>
<p>8
</p>
<p>Audi
BMW
</p>
<p>Citroen
</p>
<p>Ferrari
</p>
<p>Fiat
</p>
<p>Ford
Hyundai
</p>
<p>Jaguar
</p>
<p>Lada
</p>
<p>Mazda
</p>
<p>Mercedes
</p>
<p>Mitsubishi
</p>
<p>Nissan
</p>
<p>Opel Corsa 
</p>
<p>Opel Vectra
</p>
<p>Peugeot
Renault
</p>
<p>Rover
</p>
<p>Toyota
</p>
<p>Trabant
</p>
<p>VW Golf
</p>
<p>VW Passat
</p>
<p>Wartburg
</p>
<p>Car Marks Data
</p>
<p>η
1
</p>
<p>f
1
</p>
<p>Fig. 16.1 The second canonical variables for the car marks data textttMVAcancarm
</p>
<p>where the `i &rsquo;s are the eigenvalues of OK OK&gt; and OK&gt; OK with rank. OK/ D 2, and gi and
di are the eigenvectors of OK OK&gt; and OK&gt; OK, respectively. The canonical correlation
coefficients are
</p>
<p>r1 D `1=21 D 0:98; r2 D `
1=2
2 D 0:89:
</p>
<p>The high correlation of the second two canonical variables can be seen in Fig. 16.1.
</p>
<p>The second canonical variables are
</p>
<p>O�1 D Oa&gt;1 x D 1:602 x1 C 1:686 x2
O'1 D Ob&gt;1 y D 0:568 y1 C 0:544 y2 � 0:012 y3 � 0:096 y4 � 0:014 y5 C 0:915 y6:
</p>
<p>Note that the variables y1 (economy), y2 (service) and y6 (easy handling) have
</p>
<p>positive coefficients on O'1. The variables y3 (design), y4 (sporty car) and y5 (safety)
have a negative influence on O'1.
</p>
<p>The canonical variable �1 may be interpreted as a price and value index. The
</p>
<p>canonical variable '1 is mainly formed from the qualitative variables economy,
</p>
<p>service and handling with negative weights on design, safety and sportiness. These
</p>
<p>variables may therefore be interpreted as an appreciation of the value of the car. The
</p>
<p>sportiness has a negative effect on the price and value index, as do the design and
</p>
<p>the safety features.</p>
<p/>
</div>
<div class="page"><p/>
<p>16.2 Canonical Correlation in Practice 451
</p>
<p>Testing the Canonical Correlation Coefficients
</p>
<p>The hypothesis that the two sets of variablesX andY are uncorrelatedmay be tested
</p>
<p>(under normality assumptions) with Wilks likelihood ratio statistic (Gibbins, 1985):
</p>
<p>T 2=n D
ˇ̌
I � S�1YY SYXS�1XX SXY
</p>
<p>ˇ̌
D
</p>
<p>kY
</p>
<p>iD1
.1 � `i /:
</p>
<p>This statistic unfortunately has a rather complicated distribution. Bartlett (1939)
</p>
<p>provides an approximation for large n:
</p>
<p>� fn� .p C q C 3/=2g log
kY
</p>
<p>iD1
.1� `i / � �2pq : (16.14)
</p>
<p>A test of the hypothesis that only s of the canonical correlation coefficients are
</p>
<p>nonzero may be based (asymptotically) on the statistic
</p>
<p>� fn� .p C q C 3/=2g log
kY
</p>
<p>iDsC1
.1 � `i / � �2.p�s/.q�s/: (16.15)
</p>
<p>Example 16.2 Consider Example 16.1 again. There are n D 40 persons that have
rated the cars according to different categories with p D 2 and q D 6. The canonical
correlation coefficients were found to be r1 D 0:98 and r2 D 0:89. Bartlett&rsquo;s
statistic (16.14) is therefore
</p>
<p>�f40 � .2C 6C 3/=2g logf.1 � 0:982/.1� 0:892/g D 165:59 � �212
</p>
<p>which is highly significant (the 99% quantile of the �212 is 26.23). The hypothesis
</p>
<p>of no correlation between the variables X and Y is therefore rejected.
</p>
<p>Let us now test whether the second canonical correlation coefficient is different
</p>
<p>from zero. We use Bartlett&rsquo;s statistic (16.15) with s D 1 and obtain
</p>
<p>�f40 � .2C 6C 3/=2g logf.1 � 0:892/g D 54:19 � �25
</p>
<p>which is again highly significant with the �25 distribution.
</p>
<p>CCA with Qualitative Data
</p>
<p>The canonical correlation technique may also be applied to qualitative data.
</p>
<p>Consider for example the contingency tableN of the French baccalaur&eacute;at data. The
</p>
<p>dataset is given in Table 22.8 in Chap. 22. The CCA cannot be applied directly to</p>
<p/>
</div>
<div class="page"><p/>
<p>452 16 Canonical Correlation Analysis
</p>
<p>this contingency table since the table does not correspond to the usual data matrix
</p>
<p>structure. We may wish, however, to explain the relationship between the row r and
</p>
<p>column c categories. It is possible to represent the data in a .n�.rCc// data matrix
Z D .X ;Y/ where n is the total number of frequencies in the contingency tableN
and X and Y are matrices of zero-one dummy variables. More precisely, let
</p>
<p>xki D
�
1 if the kth individual belongs to the i th row category
</p>
<p>0 otherwise
</p>
<p>and
</p>
<p>ykj D
�
1 if the kth individual belongs to the j th column category
</p>
<p>0 otherwise
</p>
<p>where the indices range from k D 1; : : : ; n, i D 1; : : : ; r and j D 1; : : : ; c. Denote
the cell frequencies by nij so thatN D .nij/ and note that
</p>
<p>x&gt;.i/y.j / D nij;
</p>
<p>where x.i/ (y.j /) denotes the i th (j th) column of X (Y).
</p>
<p>X D
</p>
<p>0
BBBBBBBBBBBBBBB@
</p>
<p>1 0
</p>
<p>1 0
</p>
<p>1 0
</p>
<p>1 0
</p>
<p>1 0
</p>
<p>0 1
</p>
<p>0 1
</p>
<p>0 1
</p>
<p>0 1
</p>
<p>0 1
</p>
<p>1
CCCCCCCCCCCCCCCA
</p>
<p>; Y D
</p>
<p>0
BBBBBBBBBBBBBBB@
</p>
<p>1 0
</p>
<p>1 0
</p>
<p>1 0
</p>
<p>0 1
</p>
<p>0 1
</p>
<p>1 0
</p>
<p>0 1
</p>
<p>0 1
</p>
<p>0 1
</p>
<p>0 1
</p>
<p>1
CCCCCCCCCCCCCCCA
</p>
<p>; Z D .X ;Y/ D
</p>
<p>0
BBBBBBBBBBBBBBB@
</p>
<p>1 0 1 0
</p>
<p>1 0 1 0
</p>
<p>1 0 1 0
</p>
<p>1 0 0 1
</p>
<p>1 0 0 1
</p>
<p>0 1 1 0
</p>
<p>0 1 0 1
</p>
<p>0 1 0 1
</p>
<p>0 1 0 1
</p>
<p>0 1 0 1
</p>
<p>1
CCCCCCCCCCCCCCCA
</p>
<p>:
</p>
<p>Example 16.3 Consider the following example where
</p>
<p>N D
�
3 2
</p>
<p>1 4
</p>
<p>�
:
</p>
<p>The matrices X , Y and Z are therefore
</p>
<p>The element n12 ofN may be obtained by multiplying the first column ofX with
</p>
<p>the second column of Y to yield
</p>
<p>x&gt;.1/y.2/ D 2:</p>
<p/>
</div>
<div class="page"><p/>
<p>16.2 Canonical Correlation in Practice 453
</p>
<p>The purpose is to find the canonical variables � D a&gt;x and ' D b&gt;y that are
maximally correlated. Note, however, that x has only one nonzero component and
</p>
<p>therefore an &ldquo;individual&rdquo; may be directly associated with its canonical variables
</p>
<p>or score .ai ; bj /. There will be nij points at each .ai ; bj / and the correlation
</p>
<p>represented by these points may serve as a measure of dependence between the
</p>
<p>rows and columns ofN .
</p>
<p>Let Z D .X ;Y/ denote a data matrix constructed from a contingency table N .
Similar to Chap. 14 define
</p>
<p>c D xi� D
cX
</p>
<p>jD1
nij;
</p>
<p>d D x�j D
rX
</p>
<p>iD1
nij;
</p>
<p>and define C D diag.c/ andD D diag.d/. Suppose that xi� &gt; 0 and x�j &gt; 0 for all
i and j . It is not hard to see that
</p>
<p>nS D Z&gt;HZ D Z&gt;Z � nNzNz&gt; D
�
nSXX nSXY
</p>
<p>nSYX nSYY
</p>
<p>�
</p>
<p>D
� n
n � 1
</p>
<p>��
C � n�1cc&gt; N � ON
N&gt; ON&gt; D � n�1dd&gt;
</p>
<p>�
</p>
<p>where ON D cd&gt;=n is the estimated value of N under the assumption of
independence of the row and column categories.
</p>
<p>Note that
</p>
<p>.n � 1/SXX1r D C1r � n�1cc&gt;1r D c � c.n�1c&gt;1r/ D c � c.n�1n/ D 0
</p>
<p>and therefore S�1XX does not exist. The same is true for S
�1
YY . One way out of this
</p>
<p>difficulty is to drop one column from both X and Y , say the first column. Let Nc and
Nd denote the vectors obtained by deleting the first component of c and d .
Define NC, ND and NSXX , NSYY , NSXY accordingly and obtain
</p>
<p>.n NSXX/�1 D NC�1 C n�1i� 1r1&gt;r
</p>
<p>.n NSYY /�1 D ND�1 C n�1�j 1c1&gt;c
</p>
<p>so that (16.3) exists. The score associated with an individual contained in the first
</p>
<p>row (column) category ofN is 0.
</p>
<p>The technique described here for purely qualitative data may also be used when
</p>
<p>the data is a mixture of qualitative and quantitative characteristics. One has to &ldquo;blow
</p>
<p>up&rdquo; the data matrix by dummy zero-one values for the qualitative data variables.</p>
<p/>
</div>
<div class="page"><p/>
<p>454 16 Canonical Correlation Analysis
</p>
<p>Summary
</p>
<p>,! In practice we estimate &dagger;XX, &dagger;XY , &dagger;YY by the empirical covari-
ances and use them to compute estimates `i , gi , di for �i , &#13;i , ıi
</p>
<p>from the SVD of OK D S�1=2XX SXYS
�1=2
YY .
</p>
<p>,! The signs of the coefficients of the canonical variables tell us the
direction of the influence of these variables.
</p>
<p>16.3 Exercises
</p>
<p>Exercise 16.1 Show that the eigenvalues of KK&gt; and K&gt;K are identical. (Hint:
Use Theorem 2.6.)
</p>
<p>Exercise 16.2 Perform the CCA for the following subsets of variables: X corre-
</p>
<p>sponding to fpriceg and Y corresponding to feconomy, easy handlingg from the car
marks data (Table 22.7).
</p>
<p>Exercise 16.3 Calculate the first canonical variables for Example 16.1. Interpret
</p>
<p>the coefficients.
</p>
<p>Exercise 16.4 Use the SVD of matrix K to show that the canonical variables �1
and �2 are not correlated.
</p>
<p>Exercise 16.5 Verify that the number of nonzero eigenvalues of matrix K is equal
</p>
<p>to rank.&dagger;XY/.
</p>
<p>Exercise 16.6 Express the singular value decomposition of matrices K and K&gt;
</p>
<p>using eigenvalues and eigenvectors of matrices K&gt;K and KK&gt;.
</p>
<p>Exercise 16.7 What will be the result of CCA for Y D X?
Exercise 16.8 What will be the results of CCA for Y D 2X and for Y D �X?
Exercise 16.9 What results do you expect if you perform CCA for X and Y such
</p>
<p>that &dagger;XY D 0? What if &dagger;XY D Ip?</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 17
</p>
<p>Multidimensional Scaling
</p>
<p>One major aim of multivariate data analysis is dimension reduction. For data
</p>
<p>measured in Euclidean coordinates, Factor Analysis and Principal Component
</p>
<p>Analysis are dominantly used tools. In many applied sciences data is recorded as
</p>
<p>ranked information. For example, in marketing, one may record &ldquo;product A is better
</p>
<p>than product B&rdquo;. High-dimensional observations therefore often have mixed data
</p>
<p>characteristics and contain relative information (w.r.t. a defined standard) rather
</p>
<p>than absolute coordinates that would enable us to employ one of the multivariate
</p>
<p>techniques presented so far.
</p>
<p>Multidimensional scaling (MDS) is a method based on proximities between
</p>
<p>objects, subjects, or stimuli used to produce a spatial representation of these
</p>
<p>items. Proximities express the similarity or dissimilarity between data objects. It
</p>
<p>is a dimension reduction technique since the aim is to find a set of points in
</p>
<p>low dimension (typically two dimensions) that reflect the relative configuration
</p>
<p>of the high-dimensional data objects. The metric MDS is concerned with such a
</p>
<p>representation in Euclidean coordinates. The desired projections are found via an
</p>
<p>appropriate spectral decomposition of a distance matrix.
</p>
<p>The metric MDS solution may result in projections of data objects that conflict
</p>
<p>with the ranking of the original observations. The nonmetric MDS solves this
</p>
<p>problem by iterating between a monotising algorithmic step and a least squares
</p>
<p>projection step. The examples presented in this chapter are based on reconstructing
</p>
<p>a map from a distance matrix and on marketing concerns such as ranking of the
</p>
<p>outfit of cars.
</p>
<p>17.1 The Problem
</p>
<p>MDS is a mathematical tool that uses proximities between objects, subjects or
</p>
<p>stimuli to produce a spatial representation of these items. The proximities are
</p>
<p>defined as any set of numbers that express the amount of similarity or dissimilarity
</p>
<p>&copy; Springer-Verlag Berlin Heidelberg 2015
W.K. H&auml;rdle, L. Simar, Applied Multivariate Statistical Analysis,
DOI 10.1007/978-3-662-45171-7_17
</p>
<p>455</p>
<p/>
</div>
<div class="page"><p/>
<p>456 17 Multidimensional Scaling
</p>
<p>between pairs of objects, subjects or stimuli. In contrast to the techniques considered
</p>
<p>so far, MDS does not start from the raw multivariate data matrix X , but from a
</p>
<p>.n�n/ dissimilarity or distance matrix,D, with the elements ıij and dij respectively.
Hence, the underlying dimensionality of the data under investigation is in general
</p>
<p>not known .
</p>
<p>MDS is a data reduction technique because it is concerned with the problem of
</p>
<p>finding a set of points in low dimension that represents the &ldquo;configuration&rdquo; of data
</p>
<p>in high dimension. The &ldquo;configuration&rdquo; in high dimension is represented by the
</p>
<p>distance or dissimilarity matrix D.
</p>
<p>MDS-techniques are often used to understand how people perceive and evaluate
</p>
<p>certain signals and information. For instance, political scientists use MDS tech-
</p>
<p>niques to understand why political candidates are perceived by voters as being
</p>
<p>similar or dissimilar. Psychologists use MDS to understand the perceptions and
</p>
<p>evaluations of speech, colours and personality traits, among other things. Last but
</p>
<p>not least, in marketing researchers use MDS techniques to shed light on the way
</p>
<p>consumers evaluate brands and to assess the relationship between product attributes.
</p>
<p>In short, the primary purpose of all MDS-techniques is to uncover structural
</p>
<p>relations or patterns in the data and to represent it in a simple geometrical model
</p>
<p>or picture. One of the aims is to determine the dimension of the model (the goal is a
</p>
<p>low-dimensional, easily interpretable model) by finding the d -dimensional space in
</p>
<p>which there is maximum correspondence between the observed proximities and the
</p>
<p>distances between points measured on a metric scale.
</p>
<p>MDS based on proximities is usually referred to as metric MDS, whereas the
</p>
<p>more popular nonmetric MDS is used when the proximities are measured on an
</p>
<p>ordinal scale.
</p>
<p>Example 17.1 A good example of how MDS works is given by Dillon and
</p>
<p>Goldstein (1984) (page 108). Suppose one is confronted with a map of Germany
</p>
<p>and asked to measure, with the use of a ruler and the scale of the map, some inter-
</p>
<p>city distances. Admittedly this is quite an easy exercise. However, let us now reverse
</p>
<p>the problem: One is given a set of distances, as in Table 17.1, and is asked to recreate
</p>
<p>the map itself. This is a far more difficult exercise, though it can be solved with a
</p>
<p>Table 17.1 Inter-city distances
</p>
<p>Berlin Dresden Hamburg Koblenz Munich Rostock
</p>
<p>Berlin 0 214 279 610 596 237
</p>
<p>Dresden 0 492 533 496 444
</p>
<p>Hamburg 0 520 772 140
</p>
<p>Koblenz 0 521 687
</p>
<p>Munich 0 771
</p>
<p>Rostock 0</p>
<p/>
</div>
<div class="page"><p/>
<p>17.1 The Problem 457
</p>
<p>Fig. 17.1 Metric MDS
solution for the inter-city
road distances
MVAMDScity1
</p>
<p>-400 -200 0 200 400 600
</p>
<p>-400
</p>
<p>-200
</p>
<p>0
</p>
<p>200
</p>
<p>Initial Configuration
</p>
<p>NORTH - SOUTH - DIRECTION in km
</p>
<p>E
A
</p>
<p>S
T
</p>
<p> -
 W
</p>
<p>E
S
</p>
<p>T
 -
</p>
<p> D
IR
</p>
<p>E
C
</p>
<p>T
IO
</p>
<p>N
 i
n
</p>
<p> k
m
</p>
<p>Berlin
Dresden
</p>
<p>Hamburg
</p>
<p>Koblenz
</p>
<p>Muenchen
</p>
<p>Rostock
</p>
<p>Fig. 17.2 Metric MDS
solution for the inter-city road
distances after reflection and
90ı rotation
MVAMDScity2
</p>
<p>0 200 400 600 800
0
</p>
<p>200
</p>
<p>400
</p>
<p>600
</p>
<p>800
</p>
<p>Map of German Cities
</p>
<p>EAST - WEST - DIRECTION in km
</p>
<p>N
O
</p>
<p>R
T
</p>
<p>H
 -
</p>
<p> S
O
</p>
<p>U
T
</p>
<p>H
 -
</p>
<p> D
IR
</p>
<p>E
C
</p>
<p>T
IO
</p>
<p>N
 i
n
</p>
<p> k
m
</p>
<p>Berlin
</p>
<p>Dresden
</p>
<p>Hamburg
</p>
<p>Koblenz
</p>
<p>Muenchen
</p>
<p>Rostock
</p>
<p>ruler and a compass in two dimensions. MDS is a method for solving this reverse
</p>
<p>problem in arbitrary dimensions. In Figure 17.1 and 17.2 you can see the graphical
</p>
<p>representation of the metric MDS solution to Table 17.1 after rotating and reflecting
</p>
<p>the points representing the cities. Note that the distances given in Table 17.1 are
</p>
<p>road distances that in general do not correspond to Euclidean distances. In real-life
</p>
<p>applications, the problems are exceedingly more complex: there are usually errors
</p>
<p>in the data and the dimensionality is rarely known in advance.</p>
<p/>
</div>
<div class="page"><p/>
<p>458 17 Multidimensional Scaling
</p>
<p>Table 17.2 Dissimilarities for cars
</p>
<p>Audi 100 BMW 5 Citroen AX Ferrari . . .
</p>
<p>Audi 100 0 2.232 3.451 3.689 . . .
</p>
<p>BMW 5 2.232 0 5.513 3.167 . . .
</p>
<p>Citroen AX 3.451 5.513 0 6.202 . . .
</p>
<p>Ferrari 3.689 3.167 6.202 0 . . .
:
:
:
</p>
<p>:
:
:
</p>
<p>:
:
:
</p>
<p>:
:
:
</p>
<p>:
:
:
</p>
<p>: : :
</p>
<p>Fig. 17.3 MDS solution on
the car data MVAmdscarm
</p>
<p>4 2 0 2 4
</p>
<p>1
0
</p>
<p>1
2
</p>
<p>3
Metric MDS
</p>
<p>x
</p>
<p>y
</p>
<p>audi
bmw
</p>
<p>citroen
</p>
<p>ferrari
</p>
<p>fiatford
</p>
<p>hyundai
</p>
<p>aguar
</p>
<p>lada
</p>
<p>mazda
</p>
<p>mercedes
</p>
<p>mitsubishi
</p>
<p>nissan
</p>
<p>opel_corsa
</p>
<p>opel_vectra
</p>
<p>peugeot
renault
</p>
<p>rover
</p>
<p>toyota
</p>
<p>traban
</p>
<p>vw_golf
</p>
<p>vw_passat
</p>
<p>wartburg
</p>
<p>Example 17.2 A further example is given in Table 17.2 where consumers noted
</p>
<p>their impressions of the dissimilarity of certain cars. The dissimilarities in this table
</p>
<p>were in fact computed from Table 22.7 as Euclidean distances
</p>
<p>dij D
</p>
<p>vuut
8X
</p>
<p>lD1
.xil � xjl/2:
</p>
<p>MDS produces Fig. 17.3 which shows a non-linear relationship for all the cars in the
</p>
<p>projection. This enables us to build a non-linear (quadratic) index with theWartburg
</p>
<p>and the Trabant on the left and the Ferrari and the Jaguar on the right. We can
</p>
<p>construct an order or ranking of the cars based on the subjective impression of the
</p>
<p>consumers.</p>
<p/>
</div>
<div class="page"><p/>
<p>17.1 The Problem 459
</p>
<p>Fig. 17.4 Correlation
between the MDS direction
and the variables
MVAmdscarm
</p>
<p>1.0 0.5 0.0 0.5 1.0
</p>
<p>1
.0
</p>
<p>0
.5
</p>
<p>0
.0
</p>
<p>0
.5
</p>
<p>1
.0
</p>
<p>Correlations MDS/Variables
</p>
<p>x
</p>
<p>y
</p>
<p>economic
</p>
<p>service
</p>
<p>value
price
</p>
<p>looksporty
</p>
<p>security
</p>
<p>easy
</p>
<p>What does the ranking describe? The answer is given by Fig. 17.4 which shows
</p>
<p>the correlation between the MDS projection and the variables. Apparently, the
</p>
<p>first MDS direction is highly correlated with service(�), value(�), design(�),
sportiness(�), safety(�) and price(C). We can interpret the first direction as the
price direction since a bad mark in price (&ldquo;high price&rdquo;) obviously corresponds with
</p>
<p>a good mark, say, in sportiness (&ldquo;very sportive&rdquo;). The second MDS direction is
</p>
<p>highly positively correlated with practicability. We observe from this data an almost
</p>
<p>orthogonal relationship between price and practicability.
</p>
<p>In MDS a map is constructed in Euclidean space that corresponds to given
</p>
<p>distances. Which solution can we expect? The solution is determined only up to
</p>
<p>rotation, reflection and shifts. In general, if P1; : : : ; Pn with coordinates xi D
.xi1; : : : ; xip/
</p>
<p>&gt; for i D 1; : : : ; n represents a MDS solution in p dimensions, then
yi D Axi C b with an orthogonal matrix A and a shift vector b also represents a
MDS solution. A comparison of Figs. 17.1 and 17.2 illustrates this fact.
</p>
<p>Solution methods that use only the rank order of the distances are termed
</p>
<p>nonmetric methods of MDS. Methods aimed at finding the points Pi directly from a
</p>
<p>distance matrix like the one in the Table 17.2 are called metric methods.</p>
<p/>
</div>
<div class="page"><p/>
<p>460 17 Multidimensional Scaling
</p>
<p>Summary
</p>
<p>,! MDS is a set of techniques which use distances or dissimilarities
to project high-dimensional data into a low-dimensional space
</p>
<p>essential in understanding respondents perceptions and evaluations
</p>
<p>for all sorts of items.
,! MDS starts with a .n � n/ proximity matrix D consisting of
</p>
<p>dissimilarities ıi;j or distances dij.
</p>
<p>,! MDS is an explorative technique and focuses on data reduction.
</p>
<p>,! The MDS-solution is indeterminate with respect to rotation, reflec-
tion and shifts.
</p>
<p>,! The MDS-techniques are divided into metric MDS and nonmetric
MDS.
</p>
<p>17.2 Metric MDS
</p>
<p>Metric MDS begins with a .n�n/ distance matrixD with elements dij where i; j D
1; : : : ; n. The objective of metric MDS is to find a configuration of points in p-
</p>
<p>dimensional space from the distances between the points such that the coordinates
</p>
<p>of the n points along the p dimensions yield a Euclidean distance matrix whose
</p>
<p>elements are as close as possible to the elements of the given distance matrix D.
</p>
<p>The Classical Solution
</p>
<p>The classical solution is based on a distance matrix that is computed from a
</p>
<p>Euclidean geometry.
</p>
<p>Definition 17.1 A .n�n/ distance matrixD D .dij/ is Euclidean if for some points
x1; : : : ; xn 2 RpI d 2ij D .xi � xj /&gt;.xi � xj /.
</p>
<p>The following result tells us whether a distance matrix is Euclidean or not.
</p>
<p>Theorem 17.1 Define A D .aij/; aij D � 12d 2ij and B D HAH where H is the
centering matrix. D is Euclidean if and only if B is positive semidefinite. If D is
</p>
<p>the distance matrix of a data matrix X , then B D HXX&gt;H. B is called the inner
product matrix.</p>
<p/>
</div>
<div class="page"><p/>
<p>17.2 Metric MDS 461
</p>
<p>Recovery of Coordinates
</p>
<p>The task of MDS is to find the original Euclidean coordinates from a given distance
</p>
<p>matrix. Let the coordinates of n points in a p dimensional Euclidean space be given
</p>
<p>by xi (i D 1; : : : ; n) where xi D .xi1; : : : ; xip/&gt;. Call X D .x1; : : : ; xn/&gt; the
coordinate matrix and assume x D 0. The Euclidean distance between the i th and
j th points is given by:
</p>
<p>d 2ij D
pX
</p>
<p>kD1
.xik � xjk/2: (17.1)
</p>
<p>The general bij term of B is given by:
</p>
<p>bij D
pX
</p>
<p>kD1
xikxjk D x&gt;i xj : (17.2)
</p>
<p>It is possible to derive B from the known squared distances dij, and then from B the
</p>
<p>unknown coordinates.
</p>
<p>d 2ij D x&gt;i xi C x&gt;j xj � 2x&gt;i xj
D bii C bjj � 2bij: (17.3)
</p>
<p>Centering of the coordinate matrix X implies that
Pn
</p>
<p>iD1 bij D 0. Summing (17.3)
over i and j , we find:
</p>
<p>1
</p>
<p>n
</p>
<p>nX
</p>
<p>iD1
d 2ij D
</p>
<p>1
</p>
<p>n
</p>
<p>nX
</p>
<p>iD1
bii C bjj
</p>
<p>1
</p>
<p>n
</p>
<p>nX
</p>
<p>jD1
d 2ij D bii C
</p>
<p>1
</p>
<p>n
</p>
<p>nX
</p>
<p>jD1
bjj
</p>
<p>1
</p>
<p>n2
</p>
<p>nX
</p>
<p>iD1
</p>
<p>nX
</p>
<p>jD1
d 2ij D
</p>
<p>2
</p>
<p>n
</p>
<p>nX
</p>
<p>iD1
bii: (17.4)
</p>
<p>Solving (17.3) and (17.4) gives:
</p>
<p>bij D �
1
</p>
<p>2
.d 2ij � d 2i� � d 2�j C d 2��/: (17.5)</p>
<p/>
</div>
<div class="page"><p/>
<p>462 17 Multidimensional Scaling
</p>
<p>With aij D � 12d 2ij , and
</p>
<p>ai� D
1
</p>
<p>n
</p>
<p>nX
</p>
<p>jD1
aij
</p>
<p>a�j D
1
</p>
<p>n
</p>
<p>nX
</p>
<p>iD1
aij
</p>
<p>a�� D
1
</p>
<p>n2
</p>
<p>nX
</p>
<p>iD1
</p>
<p>nX
</p>
<p>jD1
aij (17.6)
</p>
<p>we get:
</p>
<p>bij D aij � ai� � a�j C a��: (17.7)
</p>
<p>Define the matrix A as .aij/, and observe that:
</p>
<p>B D HAH: (17.8)
</p>
<p>The inner product matrix B can be expressed as:
</p>
<p>B D XX&gt;; (17.9)
</p>
<p>where X D .x1; : : : ; xn/&gt; is the .n � p/ matrix of coordinates. The rank of B is
then
</p>
<p>rank.B/ D rank.XX&gt;/ D rank.X / D p: (17.10)
</p>
<p>As required in Theorem 17.1 the matrix B is symmetric, positive semidefinite and
</p>
<p>of rank p, and hence it has p non-negative eigenvalues and n� p zero eigenvalues.
B can now be written as:
</p>
<p>B D &#128;ƒ&#128;&gt; (17.11)
</p>
<p>where ƒ D diag.�1; : : : ; �p/, the diagonal matrix of the eigenvalues of B, and
&#128; D .&#13;1; : : : ; &#13;p/, the matrix of corresponding eigenvectors. Hence the coordinate
matrix X containing the point configuration in Rp is given by:
</p>
<p>X D &#128;ƒ 12 : (17.12)</p>
<p/>
</div>
<div class="page"><p/>
<p>17.2 Metric MDS 463
</p>
<p>HowMany Dimensions?
</p>
<p>The number of desired dimensions is small in order to provide practical interpreta-
</p>
<p>tions, and is given by the rank of B or the number of nonzero eigenvalues �i . If B is
</p>
<p>positive semidefinite, then the number of nonzero eigenvalues gives the number of
</p>
<p>eigenvalues required for representing the distances dij.
</p>
<p>The proportion of variation explained by p dimensions is given by
</p>
<p>Pp
iD1 �iPn�1
iD1 �i
</p>
<p>: (17.13)
</p>
<p>It can be used for the choice of p. If B is not positive semidefinite we can
</p>
<p>modify (17.13) to
</p>
<p>Pp
iD1 �iP
</p>
<p>.&ldquo;positive eigenvalues&rdquo;/
: (17.14)
</p>
<p>In practice the eigenvalues �i are almost always unequal to zero. To be able to
</p>
<p>represent the objects in a space with dimensions as small as possible we may modify
</p>
<p>the distance matrix to:
</p>
<p>D� D d�ij (17.15)
</p>
<p>with
</p>
<p>d�ij D
�
0 I i D j
dij C e � 0 I i &curren; j
</p>
<p>(17.16)
</p>
<p>where e is determined such that the inner product matrix B becomes positive
</p>
<p>semidefinite with a small rank.
</p>
<p>Similarities
</p>
<p>In some situations we do not start with distances but with similarities. The standard
</p>
<p>transformation (see Chap. 13) from a similarity matrix C to a distance matrix D is:
</p>
<p>dij D .cii � 2cij C cjj/
1
2 : (17.17)
</p>
<p>Theorem 17.2 If C � 0, then the distance matrixD defined by (17.17) is Euclidean
with centred inner product matrix B D HCH.</p>
<p/>
</div>
<div class="page"><p/>
<p>464 17 Multidimensional Scaling
</p>
<p>Relation to Factorial Analysis
</p>
<p>Suppose that the (n � p) data matrix X is centred so that X&gt;X equals a multiple
of the covariance matrix nS. Suppose that the p eigenvalues �1; : : : ; �p of nS are
</p>
<p>distinct and non zero. Using the duality Theorem 10.4 of factorial analysis we see
</p>
<p>that �1; : : : ; �p are also eigenvalues ofXX
&gt; D B whenD is the Euclidean distance
</p>
<p>matrix between the rows of X . The k-dimensional solution to the metric MDS
</p>
<p>problem is thus given by the k first principal components of X .
</p>
<p>Optimality Properties of the Classical MDS Solution
</p>
<p>Let X be a .n � p/ data matrix with some inter-point distance matrix D. The
objective of MDS is thus to find X1, a representation of X in a lower dimensional
</p>
<p>Euclidean space Rk whose inter-point distance matrix D1 is not far from D. Let
</p>
<p>L D .L1;L2/ be a .p � p/ orthogonal matrix where L1 is .p � k/. X1 D XL1
represents a projection of X on the column space of L1; in other words, X1 may be
</p>
<p>viewed as a fitted configuration of X in Rk . A measure of discrepancy between D
</p>
<p>and D1 D .d .1/ij / is given by
</p>
<p>� D
nX
</p>
<p>i;jD1
.dij � d .1/ij /2: (17.18)
</p>
<p>Theorem 17.3 Among all projections XL1 of X onto k-dimensional subspaces
</p>
<p>of Rp the quantity � in (17.18) is minimised when X is projected onto its first k
</p>
<p>principal factors.
</p>
<p>We see therefore that the metric MDS is identical to principal factor analysis as we
</p>
<p>have defined it in Chap. 10.
</p>
<p>Summary
</p>
<p>,! Metric MDS starts with a distance matrix D.
</p>
<p>,! The aim of metric MDS is to construct a map in Euclidean space
that corresponds to the given distances.</p>
<p/>
</div>
<div class="page"><p/>
<p>17.3 Nonmetric MDS 465
</p>
<p>Summary (continued)
</p>
<p>,! A practical algorithm is given as:
1. start with distances dij
2. define A D � 1
</p>
<p>2
d 2ij
</p>
<p>3. put B D
�
aij � ai� � a�j C a��
</p>
<p>�
</p>
<p>4. find the eigenvalues �1; : : : ; �p and the associated eigenvec-
</p>
<p>tors &#13;1; : : : ; &#13;p where the eigenvectors are normalised so that
</p>
<p>&#13;&gt;i &#13;i D 1.
5. Choose an appropriate number of dimensions p (ideally p D 2)
6. The coordinates of the n points in the Euclidean space are given
</p>
<p>by xij D &#13;ij�1=2j for i D 1; : : : ; n and j D 1; : : : ; p.
</p>
<p>,! Metric MDS is identical to principal components analysis.
</p>
<p>17.3 Nonmetric MDS
</p>
<p>The object of nonmetric MDS, as well as of metric MDS, is to find the coordinates
</p>
<p>of the points in p-dimensional space, so that there is a good agreement between the
</p>
<p>observed proximities and the inter-point distances. The development of nonmetric
</p>
<p>MDS was motivated by two main weaknesses in the metric MDS (Fahrmeir &amp;
</p>
<p>Hamerle, 1984, p. 679):
</p>
<p>1. the definition of an explicit functional connection between dissimilarities and
</p>
<p>distances in order to derive distances out of given dissimilarities, and
</p>
<p>2. the restriction to Euclidean geometry in order to determine the object configura-
</p>
<p>tions.
</p>
<p>The idea of a nonmetric MDS is to demand a less rigid relationship between the
</p>
<p>dissimilarities and the distances. Suppose that an unknown monotonic increasing
</p>
<p>function f ,
</p>
<p>dij D f .ıij/; (17.19)
</p>
<p>is used to generate a set of distances dij as a function of given dissimilarities ıij. Here
</p>
<p>f has the property that if ıij &lt; ırs, then f .ıij/ &lt; f .ırs/. The scaling is based on the
</p>
<p>rank order of the dissimilarities. Nonmetric MDS is therefore ordinal in character.
</p>
<p>The most common approach used to determine the elements dij and to obtain
</p>
<p>the coordinates of the objects x1; x2; : : : ; xn given only rank order information is an
</p>
<p>iterative process commonly referred to as the Shepard&ndash;Kruskal algorithm.</p>
<p/>
</div>
<div class="page"><p/>
<p>466 17 Multidimensional Scaling
</p>
<p>Shepard&ndash;Kruskal Algorithm
</p>
<p>In a first step, called the initial phase, we calculate Euclidean distances d
.0/
ij from
</p>
<p>an arbitrarily chosen initial configuration X0 in dimension p
�, provided that all
</p>
<p>objects have different coordinates. One might use metric MDS to obtain these
</p>
<p>initial coordinates. The second step or nonmetric phase determines disparities Od .0/ij
from the distances d
</p>
<p>.0/
ij by constructing a monotone regression relationship between
</p>
<p>the d
.0/
ij &rsquo;s and ıij&rsquo;s, under the requirement that if ıij &lt; ırs , then
</p>
<p>Od .0/ij � Od
.0/
rs .
</p>
<p>This is called the weak monotonicity requirement. To obtain the disparities Od .0/ij ,
a useful approximation method is the pool-adjacent violators (PAV) algorithm (see
</p>
<p>Fig. 17.5). Let
</p>
<p>.i1; j1/ &gt; .i2; j2/ &gt; � � � &gt; .ik ; jk/ (17.20)
</p>
<p>be the rank order of dissimilarities of the k D n.n � 1/=2 pairs of objects. This
corresponds to the points in Fig. 17.6. The PAV algorithm is described as follows:
</p>
<p>&ldquo;beginningwith the lowest ranked value of ıij, the adjacent d
.0/
ij values are compared
</p>
<p>for each ıij to determine if they are monotonically related to the ıij&rsquo;s. Whenever
</p>
<p>a block of consecutive values of d
.0/
ij are encountered that violate the required
</p>
<p>monotonicity property the d
.0/
ij values are averaged together with the most recent
</p>
<p>non-violator d
.0/
ij value to obtain an estimator. Eventually this value is assigned to
</p>
<p>all points in the particular block&rdquo;.
</p>
<p>Fig. 17.5 Pool-adjacent
violators algorithm
MVAMDSpooladj
</p>
<p>5 10 15
</p>
<p>5
</p>
<p>10
</p>
<p>15
</p>
<p>Pool-Adjacent-Violator-Algorithm
</p>
<p>Rank
</p>
<p>D
is
</p>
<p>ta
n
</p>
<p>c
e</p>
<p/>
</div>
<div class="page"><p/>
<p>17.3 Nonmetric MDS 467
</p>
<p>Fig. 17.6 Ranks and
distances
MVAMDSnonmstart
</p>
<p>5 10 15
</p>
<p>5
</p>
<p>10
</p>
<p>15
</p>
<p>Monotonic Regression
</p>
<p>Rank
</p>
<p>D
is
</p>
<p>ta
n
</p>
<p>c
e
</p>
<p>Table 17.3 Dissimilarities
ıij for car marks
</p>
<p>1 2 3 4
</p>
<p>i j Mercedes Jaguar Ferrari VW
</p>
<p>1 Mercedes &ndash;
</p>
<p>2 Jaguar 3 &ndash;
</p>
<p>3 Ferrari 2 1 &ndash;
</p>
<p>4 VW 5 4 6 &ndash;
</p>
<p>In a third step, called the metric phase, the spatial configuration ofX0 is altered to
</p>
<p>obtain X1. From X1 the new distances d
.1/
ij can be obtained which are more closely
</p>
<p>related to the disparities Od .0/ij from step two.
Example 17.3 Consider a small example with 4 objects based on the car marks data
</p>
<p>set, see (Table 17.3). Our aim is to find a representation with p� D 2 via MDS.
Suppose that we choose as an initial configuration (Fig. 17.7) of X0 the coordinates
</p>
<p>given in Table 17.4. The corresponding distances dij D
p
.xi � xj /&gt;.xi � xj / are
</p>
<p>calculated in Table 17.5
</p>
<p>A plot of the dissimilarities of Table 17.5 against the distance yields Fig. 17.8.
</p>
<p>This relation is not satisfactory since the ranking of the ıij did not result in a
</p>
<p>monotone relation of the corresponding distances dij. We apply therefore the PAV
</p>
<p>algorithm.
</p>
<p>The first violator of monotonicity is the second point .1; 3/. Thereforewe average
</p>
<p>the distances d13 and d23 to obtain the disparities
</p>
<p>Od13 D Od23 D
d13 C d23
</p>
<p>2
D 2:2C 4:1
</p>
<p>2
D 3:17:</p>
<p/>
</div>
<div class="page"><p/>
<p>468 17 Multidimensional Scaling
</p>
<p>Fig. 17.7 Initial
configuration of the MDS
of the car data
MVAnmdscar1
</p>
<p>0 2 4 6 8 10 12
0
</p>
<p>2
</p>
<p>4
</p>
<p>6
</p>
<p>8
</p>
<p>Mercedes
</p>
<p>Jaguar
</p>
<p>Ferrari
</p>
<p>VW
</p>
<p>Initial Configuration
</p>
<p>Table 17.4 Initial
coordinates for MDS
</p>
<p>i xi1 xi2
</p>
<p>1 Mercedes 3 2
</p>
<p>2 Jaguar 2 7
</p>
<p>3 Ferrari 1 3
</p>
<p>4 VW 10 4
</p>
<p>Table 17.5 Ranks and
distances
</p>
<p>i; j dij rank.dij/ ıij
</p>
<p>1,2 5.1 3 3
</p>
<p>1,3 2.2 1 2
</p>
<p>1,4 7.3 4 5
</p>
<p>2,3 4.1 2 1
</p>
<p>2,4 8.5 5 4
</p>
<p>3,4 9.1 6 6
</p>
<p>Applying the same procedure to .2; 4/ and .1; 4/ we obtain Od24 D Od14 D 7:9. The
plot of ıij versus the disparities Odij represents a monotone regression relationship.
</p>
<p>In the initial configuration (Fig. 17.7), the third point (Ferrari) could be moved
</p>
<p>so that the distance to object 2 (Jaguar) is reduced. This procedure however also
</p>
<p>alters the distance between objects 3 and 4. Care should be given when establishing
</p>
<p>a monotone relation between ıij and dij.
</p>
<p>In order to assess how well the derived configuration fits the given dissimilarities
</p>
<p>Kruskal suggests a measure called STRESS1 that is given by
</p>
<p>STRESS1 D
 P
</p>
<p>i&lt;j .dij � Odij/2P
i&lt;j d
</p>
<p>2
ij
</p>
<p>! 1
2
</p>
<p>: (17.21)</p>
<p/>
</div>
<div class="page"><p/>
<p>17.3 Nonmetric MDS 469
</p>
<p>0 1 2 3 4 5 6 7
0
</p>
<p>2
</p>
<p>4
</p>
<p>6
</p>
<p>8
</p>
<p>10
Dissimilarities and Distances
</p>
<p>Dissimilarity
</p>
<p>D
is
</p>
<p>ta
n
</p>
<p>c
e
</p>
<p>(2,3)
</p>
<p>(1,3)
</p>
<p>(1,2)
</p>
<p>(2,4)
</p>
<p>(1,4)
</p>
<p>(3,4)
</p>
<p>Fig. 17.8 Scatterplot of dissimilarities against distances MVAnmdscar2
</p>
<p>Table 17.6 STRESS calculations for car marks example
</p>
<p>.i; j / ıij dij Odij .dij � Odij/2 d 2ij .dij � Nd/2
(2,3) 1 4.1 3.15 0.9 16.8 3.8
</p>
<p>(1,3) 2 2.2 3.15 0.9 4.8 14.8
</p>
<p>(1,2) 3 5.1 5.1 0 26.0 0.9
</p>
<p>(2,4) 4 8.5 7.9 0.4 72.3 6.0
</p>
<p>(1,4) 5 7.3 7.9 0.4 53.3 1.6
</p>
<p>(3,4) 6 9.1 9.1 0 82.8 9.3
</p>
<p>&dagger; 36.3 2.6 256.0 36.4
</p>
<p>An alternative stress measure is given by
</p>
<p>STRESS2 D
 P
</p>
<p>i&lt;j .dij � Odij/2P
i&lt;j .dij � Nd/2
</p>
<p>! 1
2
</p>
<p>; (17.22)
</p>
<p>where Nd denotes the average distance.
Example 17.4 Table 17.6 presents the STRESS calculations for the car example.
</p>
<p>The average distance is Nd D 36:4=6 D 6:1. The corresponding STRESS
measures are:
</p>
<p>STRESS1 D
p
2:6=256D 0:1
</p>
<p>STRESS2 D
p
2:6=36:4 D 0:27:</p>
<p/>
</div>
<div class="page"><p/>
<p>470 17 Multidimensional Scaling
</p>
<p>The goal is to find a point configuration that balances the effects STRESS and
</p>
<p>non monotonicity. This is achieved by an iterative procedure. More precisely, one
</p>
<p>defines a new position of object i relative to object j by
</p>
<p>xNEWil D xil C ˛
 
1 �
Odij
dij
</p>
<p>!
.xjl � xil/; l D 1; : : : ; p�: (17.23)
</p>
<p>Here ˛ denotes the step width of the iteration.
</p>
<p>By (17.23) the configuration of object i is improved relative to object j . In order
</p>
<p>to obtain an overall improvement relative to all remaining points one uses:
</p>
<p>xNEWil D xil C
˛
</p>
<p>n � 1
</p>
<p>nX
</p>
<p>jD1;j&curren;i
</p>
<p> 
1 �
Odij
dij
</p>
<p>!
.xjl � xil/; l D 1; : : : ; p�: (17.24)
</p>
<p>The choice of step width ˛ is crucial. Kruskal proposes a starting value of ˛ D 0:2.
The iteration is continued by a numerical approximation procedure, such as steepest
</p>
<p>descent or the Newton&ndash;Raphson procedure.
</p>
<p>In a fourth step, the evaluation phase, the STRESS measure is used to evaluate
</p>
<p>whether or not its change as a result of the last iteration is sufficiently small that
</p>
<p>the procedure is terminated. At this stage the optimal fit has been obtained for a
</p>
<p>given dimension. Hence, the whole procedure needs to be carried out for several
</p>
<p>dimensions.
</p>
<p>Example 17.5 Let us compute the new point configuration for i D 3 (Ferrari)
(Fig. 17.9). The initial coordinates from Table 17.4 are
</p>
<p>x31 D 1 and x32 D 3:
</p>
<p>Applying (17.24) yields (for ˛ D 3):
</p>
<p>xNEW31 D 1C
3
</p>
<p>4 � 1
</p>
<p>4X
</p>
<p>jD1;j&curren;3
</p>
<p> 
1 �
Od3j
d3j
</p>
<p>!
.xj1 � 1/
</p>
<p>D 1C
�
1 � 3:15
</p>
<p>2:2
</p>
<p>�
.3 � 1/C
</p>
<p>�
1 � 3:15
</p>
<p>4:1
</p>
<p>�
.2 � 1/C
</p>
<p>�
1 � 9:1
</p>
<p>9:1
</p>
<p>�
.10 � 1/
</p>
<p>D 1 � 0:86C 0:23C 0
D 0:37:
</p>
<p>Similarly we obtain xNEW32 D 4:36.
To find the appropriate number of dimensions, p�, a plot of the minimum
</p>
<p>STRESS value as a function of the dimensionality is made. One possible criterion
</p>
<p>in selecting the appropriate dimensionality is to look for an elbow in the plot. A rule</p>
<p/>
</div>
<div class="page"><p/>
<p>17.3 Nonmetric MDS 471
</p>
<p>0 2 4 6 8 10 12
0
</p>
<p>2
</p>
<p>4
</p>
<p>6
</p>
<p>8
</p>
<p>Mercedes
</p>
<p>Jaguar
</p>
<p>Ferrari Init
</p>
<p>Ferrari New
</p>
<p>VW
</p>
<p>First Iteration for Ferrari
</p>
<p>Fig. 17.9 First iteration for Ferrari using Shepard-Kruskal algorithm MVAnmdscar3
</p>
<p>of thumb that can be used to decide if a STRESS value is sufficiently small or not is
</p>
<p>provided by Kruskal:
</p>
<p>S &gt; 20%; poorI S D 10%; fairI S &lt; 5%; goodI S D 0; perfect: (17.25)
</p>
<p>Summary
</p>
<p>,! Nonmetric MDS is only based on the rank order of dissimilarities.
</p>
<p>,! The object of nonmetric MDS is to create a spatial representation
of the objects with low dimensionality.
</p>
<p>,! A practical algorithm is given as:
1. Choose an initial configuration.
</p>
<p>2. Find dij from the configuration.
</p>
<p>3. Fit Odij, the disparities, by the PAV algorithm.
4. Find a new configurationXnC1 by using the steepest descent.
5. Go to 2.</p>
<p/>
</div>
<div class="page"><p/>
<p>472 17 Multidimensional Scaling
</p>
<p>17.4 Exercises
</p>
<p>Exercise 17.1 Apply the MDS method to the Swiss bank note data. What do you
</p>
<p>expect to see?
</p>
<p>Exercise 17.2 Using (17.6), show that (17.7) can be written in the form (17.2).
</p>
<p>Exercise 17.3 Show that
</p>
<p>1. bii D a�� � 2ai�I bij D aij � ai� � a�j C a��I i 6D j
2. B D
</p>
<p>Pp
iD1 xix
</p>
<p>&gt;
i
</p>
<p>3.
Pn
</p>
<p>iD1 �i D
Pn
</p>
<p>iD1 bii D 12nPni;jD1 d 2ij .
</p>
<p>Exercise 17.4 Redo a careful analysis of the car marks data based on the following
</p>
<p>dissimilarity matrix:
</p>
<p>j 1 2 3 4
</p>
<p>i Nissan Kia BMW Audi
</p>
<p>1 Nissan &ndash;
</p>
<p>2 Kia 2 &ndash;
</p>
<p>3 BMW 4 6 &ndash;
</p>
<p>4 Audi 3 5 1 &ndash;
</p>
<p>Exercise 17.5 Apply the MDS method to the US health data. Is the result in
</p>
<p>accordance with the geographic location of the US states?
</p>
<p>Exercise 17.6 Redo Exercise 17.5 with the US crime data.
</p>
<p>Exercise 17.7 Perform the MDS analysis on the Athletic Records data in
</p>
<p>Sect. 22.18. Can you see which countries are &ldquo;close to each other&rdquo;?</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 18
</p>
<p>Conjoint Measurement Analysis
</p>
<p>Conjoint Measurement Analysis plays an important role in marketing. In the design
</p>
<p>of new products it is valuable to know which components carry what kind of utility
</p>
<p>for the customer.Marketing and advertisement strategies are based on the perception
</p>
<p>of the new product&rsquo;s overall utility. It can be valuable information for a car producer
</p>
<p>to knowwhether a change in sportiness or a change in safety or comfort equipment is
</p>
<p>perceived as a higher increase in overall utility. The ConjointMeasurement Analysis
</p>
<p>is a method for attributing utilities to the components (part worths) on the basis of
</p>
<p>ranks given to different outcomes (stimuli) of the product. An important assumption
</p>
<p>is that the overall utility is decomposed as a sum of the utilities of the components.
</p>
<p>In Sect. 18.1 we introduce the idea of Conjoint Measurement Analysis. We give
</p>
<p>two examples from the food and car industries. In Sect. 18.2 we shed light on the
</p>
<p>problem of designing questionnaires for ranking different product outcomes. In
</p>
<p>Sect. 18.3 we see that the metric solution of estimating the part-worths is given
</p>
<p>by solving a least squares problem. The estimated preference ordering may be
</p>
<p>nonmonotone. The nonmetric solution strategy takes care of this inconsistency by
</p>
<p>iterating between a least squares solution and the pool adjacent violators algorithm.
</p>
<p>18.1 Introduction
</p>
<p>In the design and perception of new products it is important to specify the contri-
</p>
<p>butions made by different facets or elements. The overall utility and acceptance of
</p>
<p>such a new product can then be estimated and understood as a possibly additive
</p>
<p>function of the elementary utilities. Examples are the design of cars, a food article
</p>
<p>or the program of a political party. For a new type of margarine one may ask
</p>
<p>&copy; Springer-Verlag Berlin Heidelberg 2015
W.K. H&auml;rdle, L. Simar, Applied Multivariate Statistical Analysis,
DOI 10.1007/978-3-662-45171-7_18
</p>
<p>473</p>
<p/>
</div>
<div class="page"><p/>
<p>474 18 Conjoint Measurement Analysis
</p>
<p>whether a change in taste or presentation will enhance the overall perception of
</p>
<p>the product. The elementary utilities are here the presentation style and the taste
</p>
<p>(e.g. calory content). For a party program one may want to investigate whether a
</p>
<p>stronger ecological or a stronger social orientation gives a better overall profile of
</p>
<p>the party. For the marketing of a new car one may be interested in whether this new
</p>
<p>car should have a stronger active safety or comfort equipment or a more sporty note
</p>
<p>or combinations of both.
</p>
<p>In Conjoint Measurement Analysis one assumes that the overall utility can be
</p>
<p>explained as an additive decomposition of the utilities of different elements. In a
</p>
<p>sample of questionnaires people ranked the product types and thus revealed their
</p>
<p>preference orderings. The aim is to find the decomposition of the overall utility on
</p>
<p>the basis of observed data and to interpret the elementary or marginal utilities.
</p>
<p>Example 18.1 A car producer plans to introduce a new car with features that appeal
</p>
<p>to the customer and that may help in promoting future sales. The new elements
</p>
<p>that are considered are comfort/safety components (e.g. active steering or GPS) and
</p>
<p>a sporty look (leather steering wheel and additional kW of the engine). The car
</p>
<p>producer has thus four lines of cars.
</p>
<p>car 1: basic safety equipment and low sportiness
</p>
<p>car 2: basic safety equipment and high sportiness
</p>
<p>car 3: high safety equipment and low sportiness
</p>
<p>car 4: high safety equipment and high sportiness
</p>
<p>For the car producer it is important to rank these cars and to find out customers&rsquo;
</p>
<p>attitudes toward a certain product line in order to develop a suitable marketing
</p>
<p>scheme. A tester may rank the cars as described in Table 18.1.
</p>
<p>The elementary utilities here are the comfort equipment and the level of
</p>
<p>sportiness. Conjoint Measurement Analysis aims at explaining the rank order given
</p>
<p>by the test person as a function of these elementary utilities.
</p>
<p>Example 18.2 A food producer plans to create a new margarine and varies the
</p>
<p>product characteristics &ldquo;calories&rdquo; (low vs. high) and &ldquo;presentation&rdquo; (a plastic pot
</p>
<p>vs. paper package) (Backhaus, Erichson, Plinke, &amp; Weiber, 1996). We can view this
</p>
<p>in fact as ranking four products.
</p>
<p>product 1: low calories and plastic pot
</p>
<p>product 2: low calories and paper package
</p>
<p>product 3: high calories and plastic pot
</p>
<p>product 4: high calories and paper package</p>
<p/>
</div>
<div class="page"><p/>
<p>18.2 Design of Data Generation 475
</p>
<p>Table 18.1 Tester&rsquo;s ranking
of cars
</p>
<p>Car 1 2 3 4
</p>
<p>Ranking 1 2 4 3
</p>
<p>Table 18.2 Tester&rsquo;s ranking
of margarine
</p>
<p>Product 1 2 3 4
</p>
<p>Ranking 3 4 1 2
</p>
<p>These four fictive products may now be ordered by a set of sample testers as
</p>
<p>described in Table 18.2.
</p>
<p>The Conjoint Measurement Analysis aims to explain such a preference ranking
</p>
<p>by attributing part-worths to the different elements of the product. The part-worths
</p>
<p>are the utilities of the elementary components of the product.
</p>
<p>In interpreting the part-worths one may find that for a test person one of the
</p>
<p>elements has a higher value or utility. This may lead to a new design or to the
</p>
<p>decision that this utility should be emphasised in advertisement schemes.
</p>
<p>Summary
</p>
<p>,! Conjoint Measurement Analysis is used in the design of new
products.
</p>
<p>,! ConjointMeasurement Analysis tries to identify part-worth utilities
that contribute to an overall utility.
</p>
<p>,! The part-worths enter additively into an overall utility.
</p>
<p>,! The interpretation of the part-worths gives insight into the percep-
tion and acceptance of the product.
</p>
<p>18.2 Design of Data Generation
</p>
<p>The product is defined through the properties of the components. A stimulus is
</p>
<p>defined as a combination of the different components. Examples 18.1 and 18.2 had
</p>
<p>four stimuli each. In the margarine example they were the possible combinations of
</p>
<p>the factors X1 (calories) and X2 (presentation). If a product property such as
</p>
<p>X3(usage) D
</p>
<p>8
&lt;
:
</p>
<p>1 bread
</p>
<p>2 cooking
</p>
<p>3 universal
</p>
<p>is added, then there are 3 � 2 � 2 D 12 stimuli.</p>
<p/>
</div>
<div class="page"><p/>
<p>476 18 Conjoint Measurement Analysis
</p>
<p>For the automobile Example 18.1 additional characteristics may be engine power
</p>
<p>and the number of doors. Suppose that the engines offered for the new car have
</p>
<p>50; 70; 90kW and that the car may be produced in 2-, 4-, or 5-door versions. These
</p>
<p>categories may be coded as
</p>
<p>X3.power of engine/ D
</p>
<p>8
&lt;
:
</p>
<p>1 50 kW
</p>
<p>2 70 kW
</p>
<p>3 90 kW
</p>
<p>and
</p>
<p>X4.doors/ D
</p>
<p>8
&lt;
:
</p>
<p>1 2 doors
</p>
<p>2 4 doors
</p>
<p>3 5 doors
</p>
<p>:
</p>
<p>Both X3 and X4 have three factor levels each, whereas the first two factors X1
(safety) andX2 (sportiness) have only two levels. Altogether 2 � 2 � 3 � 3D 36 stimuli
are possible. In a questionnaire a tester would have to rank all 36 different products.
</p>
<p>The profile method asks for the utility of each stimulus. This may be time
</p>
<p>consuming and tiring for a test person if there are too many factors and factor levels.
</p>
<p>Suppose that there are six properties of components with three levels each. This
</p>
<p>results in 36 D 729 stimuli (i.e. 729 different products) that a tester would have to
rank.
</p>
<p>The two factor method is a simplification and considers only two factors
</p>
<p>simultaneously. It is also called trade-off analysis. The idea is to present just two
</p>
<p>stimuli at a time and then to recombine the information. Trade-off analysis is
</p>
<p>performed by defining the trade-off matrices corresponding to stimuli of two factors
</p>
<p>only.
</p>
<p>The trade-off matrices for the levelsX1, X2 andX3 from the margarine Example
</p>
<p>18.2 are given in Table 18.3. The trade-off matrices for the new car outfit are
</p>
<p>described in Tabel 18.4.
</p>
<p>The choice between the profile method and the trade-off analysis should be
</p>
<p>guided by consideration of the following aspects:
</p>
<p>1. requirements on the test person,
</p>
<p>2. time consumption,
</p>
<p>3. product perception.
</p>
<p>Table 18.3 Trade-off
matrices for margarine
</p>
<p>X3 X1
</p>
<p>1 1 2
</p>
<p>2 1 2
</p>
<p>3 1 2
</p>
<p>X3 X2
</p>
<p>1 1 2
</p>
<p>2 1 2
</p>
<p>3 1 2
</p>
<p>X1 X2
</p>
<p>1 1 2
</p>
<p>2 1 2</p>
<p/>
</div>
<div class="page"><p/>
<p>18.2 Design of Data Generation 477
</p>
<p>Table 18.4 Trade-off
matrices for car design
</p>
<p>X4 X3
</p>
<p>1 1 2 3
</p>
<p>2 1 2 3
</p>
<p>3 1 2 3
</p>
<p>X4 X2
</p>
<p>1 1 2
</p>
<p>2 1 2
</p>
<p>3 1 2
</p>
<p>X4 X1
</p>
<p>1 1 2
</p>
<p>2 1 2
</p>
<p>3 1 2
</p>
<p>X3 X2
</p>
<p>1 1 2
</p>
<p>2 1 2
</p>
<p>3 1 2
</p>
<p>X3 X1
</p>
<p>1 1 2
</p>
<p>2 1 2
</p>
<p>3 1 2
</p>
<p>X2 X1
</p>
<p>1 1 2
</p>
<p>2 1 2
</p>
<p>The first aspect relates to the ability of the test person to judge the different stimuli.
</p>
<p>It is certainly an advantage of the trade-off analysis that one only has to consider
</p>
<p>two factors simultaneously. The two factor method can be carried out more easily
</p>
<p>in a questionnaire without an interview.
</p>
<p>The profile method incorporates the possibility of a complete product perception
</p>
<p>since the test person is not confronted with an isolated aspect (2 factors) of the
</p>
<p>product. The stimuli may be presented visually in its final form (e.g. as a picture).
</p>
<p>With the number of levels and properties the number of stimuli rise exponentially
</p>
<p>with the profile method. The time to complete a questionnaire is therefore a factor
</p>
<p>in the choice of method.
</p>
<p>In general the product perception is the most important aspect and is therefore
</p>
<p>the profile method that is used the most. The time consumption aspect speaks for the
</p>
<p>trade-off analysis. There exist, however, clever strategies on selecting representation
</p>
<p>subsets of all profiles that bound the time investment. We therefore concentrate on
</p>
<p>the profile method in the following.
</p>
<p>Summary
</p>
<p>,! A stimulus is a combination of different properties of a product.
</p>
<p>,! Conjoint measurement analysis is based either on a list of all factors
(profile method) or on trade-off matrices (two factor method).
</p>
<p>,! Trade-off matrices are used if there are too many factor levels.
</p>
<p>,! Presentation of trade-off matrices makes it easier for testers since
only two stimuli have to be ranked at a time.</p>
<p/>
</div>
<div class="page"><p/>
<p>478 18 Conjoint Measurement Analysis
</p>
<p>18.3 Estimation of Preference Orderings
</p>
<p>On the basis of the reported preference values for each stimulus conjoint analysis
</p>
<p>determines the part-worths. Conjoint analysis uses an additive model of the form
</p>
<p>Yk D
JX
</p>
<p>jD1
</p>
<p>LjX
</p>
<p>lD1
ˇjl I.Xj D xjl/C �; for k D 1; : : : ; K and 8 j
</p>
<p>LjX
</p>
<p>lD1
ˇjl D 0:
</p>
<p>(18.1)
</p>
<p>Xj (j D 1; : : : ; J ) denote the factors, xjl (l D 1; : : : ; Lj ) are the levels of each
factor Xj and the coefficients ˇjl are the part-worths. The constant � denotes an
</p>
<p>overall level and Yk is the observed preference for each stimulus and the total
</p>
<p>number of stimuli are:
</p>
<p>K D
JY
</p>
<p>jD1
Lj :
</p>
<p>Equation (18.1) is without an error term for the moment. In order to explain
</p>
<p>how (18.1) may be written in the standard linear model form we first concentrate on
</p>
<p>J D 2 factors. Suppose that the factors engine power and airbag safety equipment
have been ranked as follows:
</p>
<p>Airbag
</p>
<p>1 2
</p>
<p>Engine 50 kW 1 1 3
</p>
<p>70 kW 2 2 6
</p>
<p>90 kW 3 4 5
</p>
<p>There are K D 6 preferences altogether. Suppose that the stimuli have been
sorted so that Y1 corresponds to engine level 1 and airbag level 1, Y2 corresponds to
</p>
<p>engine level 1 and airbag level 2, and so on. Then model (18.1) reads:
</p>
<p>Y1 D ˇ11 C ˇ21 C �
Y2 D ˇ11 C ˇ22 C �
Y3 D ˇ12 C ˇ21 C �
Y4 D ˇ12 C ˇ22 C �
Y5 D ˇ13 C ˇ21 C �
Y6 D ˇ13 C ˇ22 C �:
</p>
<p>Now we would like to estimate the part-worths ˇjl.</p>
<p/>
</div>
<div class="page"><p/>
<p>18.3 Estimation of Preference Orderings 479
</p>
<p>Table 18.5 Ranked products X2 (calories)
</p>
<p>Low High
</p>
<p>1 2
</p>
<p>X1 (usage) Bread 1 2 1
</p>
<p>Cooking 2 3 4
</p>
<p>Universal 3 6 5
</p>
<p>Example 18.3 In the margarine example let us consider the part-worths of X1 D
usage and X2 D calories. We have x11 D 1, x12 D 2, x13 D 3, x21 D 1 and
x22 D 2. (We momentarily re-labeled the factors: X3 became X1.) Hence L1 D 3
and L2 D 2. Suppose that a person has ranked the six different products as in
Table 18.5.
</p>
<p>If we order the stimuli as follows:
</p>
<p>Y1 D Utility .X1 D 1 ^X2 D 1/
Y2 D Utility .X1 D 1 ^X2 D 2/
Y3 D Utility .X1 D 2 ^X2 D 1/
Y4 D Utility .X1 D 2 ^X2 D 2/
Y5 D Utility .X1 D 3 ^X2 D 1/
Y6 D Utility .X1 D 3 ^X2 D 2/ ;
</p>
<p>we obtain from Eq. (18.1) the same decomposition as above:
</p>
<p>Y1 D ˇ11 C ˇ21 C �
Y2 D ˇ11 C ˇ22 C �
Y3 D ˇ12 C ˇ21 C �
Y4 D ˇ12 C ˇ22 C �
Y5 D ˇ13 C ˇ21 C �
Y6 D ˇ13 C ˇ22 C �:
</p>
<p>Our aim is to estimate the part-worths ˇjl as well as possible from a collection of
</p>
<p>tables like Table 18.5 that have been generated by a sample of test persons. First, the
</p>
<p>so-called metric solution to this problem is discussed and then a non-metric solution.
</p>
<p>Metric Solution
</p>
<p>The problem of conjoint measurement analysis can be solved by the technique of
</p>
<p>Analysis of Variance (ANOVA). An important assumption underlying this technique
</p>
<p>is that the &ldquo;distance&rdquo; between any two adjacent preference orderings corresponds to
</p>
<p>the same difference in utility. That is, the difference in utility between the products
</p>
<p>ranked 1st and 2nd is the same as the difference in utility between the products</p>
<p/>
</div>
<div class="page"><p/>
<p>480 18 Conjoint Measurement Analysis
</p>
<p>Table 18.6 Metric solution
for car example
</p>
<p>X2 (airbags)
</p>
<p>1 2 Npx1� ˇ1l
X1(engine) 50 kW 1 1 3 2 �1.5
</p>
<p>70 kW 2 2 6 4 0.5
</p>
<p>90 kW 3 4 5 4.5 1
</p>
<p>Npx2� 2.33 4.66 3.5
ˇ2l �1.16 1.16
</p>
<p>ranked 4th and 5th. Put differently, we treat the ranking of the products&mdash;which is a
</p>
<p>cardinal variable&mdash;as if it were a metric variable.
</p>
<p>Introducing a mean utility � Eq. (18.1) can be rewritten. The mean utility in the
</p>
<p>above Example 18.3 is � D .1C 2C 3 C 4 C 5C 6/=6 D 21=6 D 3:5. In order
to check the deviations of the utilities from this mean, we enlarge Table 18.5 by the
</p>
<p>mean utility Npxj� , given a certain level of the other factor. The metric solution for
the car example is given in Table 18.6.
</p>
<p>Example 18.4 In the margarine example the resulting part-worths for � D 3:5 are
</p>
<p>ˇ11 D �2 ˇ21 D 0:16
ˇ12 D 0 ˇ22 D �0:16
ˇ13 D 2
</p>
<p>:
</p>
<p>Note that
LjP
lD1
</p>
<p>ˇjl D 0 (j D 1; : : : ; J ). The estimated utility OY1 for the product with
low calories and usage of bread, for example, is:
</p>
<p>OY1 D ˇ11 C ˇ21 C � D �2C 0:16C 3:5 D 1:66:
</p>
<p>The estimated utility OY4 for product 4 (cooking (X1 D 2) and high calories (X2 D
2)) is:
</p>
<p>OY4 D ˇ12 C ˇ22 C � D 0 � 0:16C 3:5 D 3:33:
</p>
<p>The coefficients ˇjl are computed as Npxjl��, where Npxjl is the average preference
ordering for each factor level. For instance, Npx11 D 1=2 � .2C 1/ D 1:5.
</p>
<p>The fit can be evaluated by calculating the deviations of the fitted values to the
</p>
<p>observed preference orderings. In the rightmost column of Table 18.8 the quadratic
</p>
<p>deviations between the observed rankings (utilities) Yk and the estimated utilities OYk
are listed.</p>
<p/>
</div>
<div class="page"><p/>
<p>18.3 Estimation of Preference Orderings 481
</p>
<p>Table 18.7 Metric solution
for Table 18.5
</p>
<p>X2 (calories)
</p>
<p>Low High
</p>
<p>1 2 Npx1� ˇ1l
X1 (usage) Bread 1 2 1 1.5 �2
</p>
<p>Cooking 2 3 4 3.5 0
</p>
<p>Universal 3 6 5 5.5 2
</p>
<p>Npx2� 3.66 3.33 3.5
ˇ2l 0.16 �0.16
</p>
<p>Table 18.8 Deviations
between model and data
</p>
<p>Stimulus Yk OYk Yk � OYk .Yk � OYk/2
1 2 1.66 0.33 0.11
</p>
<p>2 1 1.33 �0.33 0.11
3 3 3.66 �0.66 0.44
4 4 3.33 0.66 0.44
</p>
<p>5 6 5.66 0.33 0.11
</p>
<p>6 5 5.33 �0.33 0.11P
21 21 0 1.33
</p>
<p>The technique described that generated Table 18.7 is in fact the solution to a least
</p>
<p>squares problem. The conjoint measurement problem (18.1) may be rewritten as a
</p>
<p>linear regression model (with error " D 0):
</p>
<p>Y D Xˇ C " (18.2)
</p>
<p>with X being a design matrix with dummy variables. X has the row dimension
</p>
<p>K D
JQ
jD1
</p>
<p>Lj (the number of stimuli) and the column dimensionD D
JP
jD1
</p>
<p>Lj � J .
</p>
<p>The reason for the reduced column number is that per factor only (Lj � 1) vectors
are linearly independent.Without loss of generality we may standardise the problem
</p>
<p>so that the last coefficient of each factor is omitted. The error term " is introduced
</p>
<p>since even for one person the preference orderings may not fit the model (18.1).
</p>
<p>Example 18.5 If we rewrite the ˇ coefficients in the form
</p>
<p>0
BB@
</p>
<p>ˇ1
ˇ2
ˇ3
</p>
<p>ˇ4
</p>
<p>1
CCA D
</p>
<p>0
BB@
</p>
<p>�C ˇ13 C ˇ22
ˇ11 � ˇ13
ˇ12 � ˇ13
ˇ21 � ˇ22
</p>
<p>1
CCA (18.3)</p>
<p/>
</div>
<div class="page"><p/>
<p>482 18 Conjoint Measurement Analysis
</p>
<p>and define the design matrix X as
</p>
<p>X D
</p>
<p>0
BBBBBBB@
</p>
<p>1 1 0 1
</p>
<p>1 1 0 0
</p>
<p>1 0 1 1
</p>
<p>1 0 1 0
</p>
<p>1 0 0 1
</p>
<p>1 0 0 0
</p>
<p>1
CCCCCCCA
; (18.4)
</p>
<p>then Eq. (18.1) leads to the linear model (with error " D 0):
</p>
<p>Y D Xˇ C ": (18.5)
</p>
<p>The least squares solution to this problem is the technique used for Table 18.7.
</p>
<p>In practice we have more than one person to answer the utility rank question for
</p>
<p>the different factor levels. The design matrix is then obtained by stacking the above
</p>
<p>design matrix n times. Hence, for n persons we have as a final design matrix:
</p>
<p>X � D 1n ˝ X D
</p>
<p>0
BBBB@
</p>
<p>X
:::
:::
</p>
<p>X
</p>
<p>1
CCCCA
</p>
<p>9
&gt;&gt;&gt;&gt;=
&gt;&gt;&gt;&gt;;
n � times
</p>
<p>which has dimension .nK/.L�J / (whereL D
JP
jD1
</p>
<p>Lj ) and Y
� D .Y &gt;1 ; : : : ; Y &gt;n /&gt;.
</p>
<p>The linear model (18.5) can now be written as:
</p>
<p>Y � D X �ˇ C "�: (18.6)
</p>
<p>Given that the test people assign different rankings, the error term "� is a necessary
part of the model.
</p>
<p>Example 18.6 If we take the ˇ vector as defined in (18.3) and the design matrix X
</p>
<p>from (18.4), we obtain the coefficients:
</p>
<p>Ǒ
1 D 5:33 D O�C Ǒ13 C Ǒ22
Ǒ
2 D �4 D Ǒ11 � Ǒ13
Ǒ
3 D �2 D Ǒ12 � Ǒ13
Ǒ
4 D 0:33 D Ǒ21 � Ǒ22
LjP
lD1
Ǒ
jl D 0:
</p>
<p>(18.7)</p>
<p/>
</div>
<div class="page"><p/>
<p>18.3 Estimation of Preference Orderings 483
</p>
<p>Solving (18.7) we have:
</p>
<p>Ǒ
11 D Ǒ2 � 13
</p>
<p>�
Ǒ
2 C Ǒ3
</p>
<p>�
D �2
</p>
<p>Ǒ
12 D Ǒ3 � 13
</p>
<p>�
Ǒ
2 C Ǒ3
</p>
<p>�
D 0
</p>
<p>Ǒ
13 D � 13
</p>
<p>�
Ǒ
2 C Ǒ3
</p>
<p>�
D 2
</p>
<p>Ǒ
21 D Ǒ4 � 12 Ǒ4 D
</p>
<p>1
2
Ǒ
4 D 0:16
</p>
<p>Ǒ
31 D � 12 Ǒ4 D �0:16
O� D Ǒ1 C 13
</p>
<p>�
Ǒ
2 C Ǒ3
</p>
<p>�
C 1
</p>
<p>2
. Ǒ4/ D 3:5:
</p>
<p>(18.8)
</p>
<p>In fact, we obtain the same estimated part-worths as in Table 18.7. The stimulus
</p>
<p>k D 2 corresponds to adding up ˇ11; ˇ22; and � (see (18.3)). Adding Ǒ1 and
Ǒ
2 gives:
</p>
<p>OY2 D 5:33� 4 D 1:33:
</p>
<p>Nonmetric Solution
</p>
<p>If we drop the assumption that utilities are measured on a metric scale, we have
</p>
<p>to use (18.1) to estimate the coefficients from an adjusted set of estimated utilities.
</p>
<p>More precisely, we may use the monotone ANOVA as developed by Kruskal (1965).
</p>
<p>The procedure works as follows. First, one estimates model (18.1) with the ANOVA
</p>
<p>technique described above. Then one applies a monotone transformation OZ D f . OY /
to the estimated stimulus utilities. The monotone transformation f is used because
</p>
<p>the fitted values OYk from (18.2) of the reported preference orderings Yk may not be
monotone. The transformation OZk D f . OYk/ is introduced to guaranteemonotonicity
of preference orderings. For the car example the reported Yk values were Y D
.1; 3; 2; 6; 4; 5/&gt;. The estimated values are computed as:
</p>
<p>OY1 D �1:5 � 1:16C 3:5 D 0:84
OY2 D �1:5C 1:16C 3:5 D 3:16
OY3 D �0:5 � 1:16C 3:5 D 2:84
OY4 D �0:5C 1:16C 3:5 D 5:16
OY5 D 1:5 � 1:16C 3:5 D 3:34
OY6 D 1:5C 1:16C 3:5 D 5:66:
</p>
<p>If we make a plot of the estimated preference orderings against the revealed ones,
</p>
<p>we obtain Fig. 18.1.</p>
<p/>
</div>
<div class="page"><p/>
<p>484 18 Conjoint Measurement Analysis
</p>
<p>Fig. 18.1 Plot of estimated
preference orderings
vs. revealed rankings and
PAV fit
MVAcarrankings
</p>
<p>1 2 3 4 5 6
</p>
<p>1
2
</p>
<p>3
4
</p>
<p>5
6
</p>
<p>Car rankings
</p>
<p>revealed rankings
</p>
<p>e
s
ti
m
</p>
<p>a
te
</p>
<p>d
 r
</p>
<p>a
n
</p>
<p>k
in
</p>
<p>g
s
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>car1
</p>
<p>car2
</p>
<p>car3
car4
</p>
<p>car5
</p>
<p>car6
</p>
<p>We see that the estimated OY6 D 5:16 is below the estimated OY5 D 5:66 and
thus an inconsistency in ranking the utilities occurs. The monotone transformation
OZk D f . OYk/ is introduced to make the relationship in Fig. 18.1 monotone. A very
simple procedure consists of averaging the &ldquo;violators&rdquo; OY6 and OY5 to obtain 5:41. The
relationship is then monotone but the model (18.1) may now be violated. The idea
</p>
<p>is therefore to iterate these two steps. This procedure is iterated until the stress
</p>
<p>measure (see Chap. 17)
</p>
<p>STRESS D
</p>
<p>KP
kD1
. OZk � OYk/2
</p>
<p>KP
kD1
. OYk � NOY /2
</p>
<p>(18.9)
</p>
<p>is minimised over ˇ and the monotone transformation f . The monotone transfor-
</p>
<p>mation can be computed by the so-called pool-adjacent-violators (PAV) algorithm.</p>
<p/>
</div>
<div class="page"><p/>
<p>18.4 Exercises 485
</p>
<p>Summary
</p>
<p>,! The part-worths are estimated via the least squares method.
</p>
<p>,! The metric solution corresponds to analysis of variance in a linear
model.
</p>
<p>,! The non-metric solution iterates between a monotone regression
curve fitting and determining the part-worths by ANOVA method-
</p>
<p>ology.
</p>
<p>,! The fitting of data to a monotone function is done via the PAV
algorithm.
</p>
<p>18.4 Exercises
</p>
<p>Exercise 18.1 Compute the part-worths for the following table of rankings
</p>
<p>X2
</p>
<p>1 2
</p>
<p>X1 1 1 2
</p>
<p>2 4 3
</p>
<p>3 6 5
</p>
<p>.
</p>
<p>Exercise 18.2 Consider again Example 18.5. Rewrite the design matrix X and the
</p>
<p>parameter vector ˇ so that the overall mean effect � is part of X and ˇ, i.e. find the
</p>
<p>matrix X 0 and ˇ0 such that Y D X 0ˇ0.
Exercise 18.3 Compute the design matrix for Example 18.5 for n D 3 persons
ranking the margarine with X1 and X2.
</p>
<p>Exercise 18.4 Construct an analog for Table 18.8 for the car example.
</p>
<p>Exercise 18.5 Compute the part-worths on the basis of the following tables of
</p>
<p>rankings observed on n D 3 persons.
</p>
<p>X2
</p>
<p>X1 1 1 2
</p>
<p>2 4 3
</p>
<p>3 6 5
</p>
<p>X2
</p>
<p>X1 1 3
</p>
<p>4 2
</p>
<p>5 6
</p>
<p>X2
</p>
<p>X1 3 1
</p>
<p>5 2
</p>
<p>6 4</p>
<p/>
</div>
<div class="page"><p/>
<p>486 18 Conjoint Measurement Analysis
</p>
<p>Exercise 18.6 Suppose that in the car example a person has ranked cars by the
</p>
<p>profile method on the following characteristics:
</p>
<p>X1 D motor
X2 D safety
X3 D doors
</p>
<p>X1 X2 X3 Preference
</p>
<p>1 1 1 1
</p>
<p>1 1 2 3
</p>
<p>1 1 3 2
</p>
<p>1 2 1 5
</p>
<p>1 2 2 4
</p>
<p>1 2 3 6
</p>
<p>X1 X2 X3 Preference
</p>
<p>2 1 1 7
</p>
<p>2 1 2 8
</p>
<p>2 1 3 9
</p>
<p>2 2 1 10
</p>
<p>2 2 2 12
</p>
<p>2 2 3 11
</p>
<p>X1 X2 X3 Preference
</p>
<p>3 1 1 13
</p>
<p>3 1 2 15
</p>
<p>3 1 3 14
</p>
<p>3 2 1 16
</p>
<p>3 2 2 17
</p>
<p>3 2 3 18
</p>
<p>There are k D 18 stimuli.
Estimate and analyse the part-worths.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 19
</p>
<p>Applications in Finance
</p>
<p>A portfolio is a linear combination of assets. Each asset contributes with a weight
</p>
<p>cj to the portfolio. The performance of such a portfolio is a function of the various
</p>
<p>returns of the assets and of the weights c D .c1; : : : ; cp/&gt;. In this chapter we
investigate the &ldquo;optimal choice&rdquo; of the portfolio weights c. The optimality criterion
</p>
<p>is the mean-variance efficiency of the portfolio. Usually investors are risk-averse,
</p>
<p>therefore, we can define a mean-variance efficient portfolio to be a portfolio that
</p>
<p>has a minimal variance for a given desired mean return. Equivalently, we could
</p>
<p>try to optimise the weights for the portfolios with maximal mean return for a
</p>
<p>given variance (risk structure). We develop this methodology in the situations of
</p>
<p>(non)existence of riskless assets and discuss relations with the capital asset pricing
</p>
<p>model (CAPM).
</p>
<p>19.1 Portfolio Choice
</p>
<p>Suppose that one has a portfolio of p assets. The price of asset j at time i is denoted
</p>
<p>as pij. The return from asset j in a single time period (day, month, year etc.) is:
</p>
<p>xij D
pij � pi�1;j
pi�1;j
</p>
<p>�
</p>
<p>We observe the vectors xi D .xi1; : : : ; xip/&gt; (i.e. the returns of the assets which are
contained in the portfolio) over several time periods. We stack these observations
</p>
<p>into a data matrix X D .xij/ consisting of observations of a random variable
</p>
<p>X � .�;&dagger;/:
</p>
<p>&copy; Springer-Verlag Berlin Heidelberg 2015
W.K. H&auml;rdle, L. Simar, Applied Multivariate Statistical Analysis,
DOI 10.1007/978-3-662-45171-7_19
</p>
<p>487</p>
<p/>
</div>
<div class="page"><p/>
<p>488 19 Applications in Finance
</p>
<p>The return of the portfolio is the weighted sum of the returns of the p assets:
</p>
<p>Q D c&gt;X; (19.1)
</p>
<p>where c D .c1; : : : ; cp/&gt; (with
Pp
</p>
<p>jD1 cj D 1) denotes the proportions of the assets
in the portfolio. The mean return of the portfolio is given by the expected value
</p>
<p>of Q, which is c&gt;�. The risk or variance (squared volatility) of the portfolio is
given by the variance ofQ (Theorem 4.6), which is equal to two times
</p>
<p>1
</p>
<p>2
c&gt;&dagger;c: (19.2)
</p>
<p>The reason for taking half of the variance ofQ is merely technical. The optimisation
</p>
<p>of (19.2) with respect to c is of course equivalent to minimising c&gt;&dagger;c. Our aim is to
maximise the portfolio returns (19.1) given a bound on the volatility (19.2) or vice
</p>
<p>versa to minimise risk given a (desired) mean return of the portfolio.
</p>
<p>Summary
</p>
<p>,! Given a matrix of returns X from p assets in n time periods, and
that the underlying distribution is stationary, i.e. X � .�;&dagger;/, then
the (theoretical) return of the portfolio is a weighted sum of the
</p>
<p>returns of the p assets, namelyQ D c&gt;X .
,! The expected value of Q is c&gt;�. For technical reasons one
</p>
<p>considers optimising 1
2
c&gt;&dagger;c. The risk or squared volatility is
</p>
<p>c&gt;&dagger;c D Var.c&gt;X/.
,! The portfolio choice, i.e. the selection of c, is such that the return
</p>
<p>is maximised for a given risk bound.
</p>
<p>19.2 Efficient Portfolio
</p>
<p>A variance efficient portfolio is one that keeps the risk (19.2) minimal under the
</p>
<p>constraint that the weights sum to 1, i.e. c&gt;1p D 1. For a variance efficient portfolio,
we therefore try to find the value of c that minimises the Lagrangian
</p>
<p>L D 1
2
c&gt;&dagger;c � �.c&gt;1p � 1/: (19.3)
</p>
<p>A mean-variance efficient portfolio is defined as one that has minimal variance
</p>
<p>among all portfolios with the same mean. More formally, we have to find a vector</p>
<p/>
</div>
<div class="page"><p/>
<p>19.2 Efficient Portfolio 489
</p>
<p>of weights c such that the variance of the portfolio is minimal subject to two
</p>
<p>constraints:
</p>
<p>1. a certain, pre-specified mean return N� has to be achieved,
2. the weights have to sum to one.
</p>
<p>Mathematically speaking, we are dealing with an optimisation problem under two
</p>
<p>constraints.
</p>
<p>The Lagrangian function for this problem is given by
</p>
<p>L D c&gt;&dagger;c C �1. N� � c&gt;�/C �2.1 � c&gt;1p/:
</p>
<p>With tools presented in Sect. 2.4 we can calculate the first order condition for a
</p>
<p>minimum:
</p>
<p>@L
</p>
<p>@c
D 2&dagger;c � �1� � �21p D 0: (19.4)
</p>
<p>Example 19.1 Figure 19.1 shows the monthly returns from January 2000 to
</p>
<p>December 2009 of six stocks. The data is from Yahoo Finance. For each stock
</p>
<p>0 20 40 60 80 100 120
</p>
<p>0
.5
</p>
<p>0
.5
</p>
<p>IBM
</p>
<p>0 20 40 60 80 100 120
</p>
<p>0
.5
</p>
<p>0
.5
</p>
<p>Apple
</p>
<p>0 20 40 60 80 100 120
</p>
<p>0
.5
</p>
<p>0
.5
</p>
<p>BAC
</p>
<p>0 20 40 60 80 100 120
</p>
<p>0
.5
</p>
<p>0
.5
</p>
<p>Forward Industries
</p>
<p>0 20 40 60 80 100 120
</p>
<p>0
.5
</p>
<p>0
.5
</p>
<p>Consolidated Edison
</p>
<p>0 20 40 60 80 100 120
</p>
<p>0
.5
</p>
<p>0
.5
</p>
<p>Morgan Stanley
</p>
<p>Fig. 19.1 Returns of six firms from January 2000 to December 2009 MVAreturns</p>
<p/>
</div>
<div class="page"><p/>
<p>490 19 Applications in Finance
</p>
<p>we have chosen the same scale on the vertical axis (which gives the return of
</p>
<p>the stock). Note how the return of some stocks, such as Forward Industries and
</p>
<p>Apple, are much more volatile than the returns of other stocks, such as IBM or
</p>
<p>Consolidated Edison (Electric utilities).
</p>
<p>As a very simple example consider two differently weighted portfolios
</p>
<p>containing only two assets, IBM and Forward Industries. Figure 19.2 displays the
</p>
<p>monthly returns of the two portfolios. The portfolio in the upper panel consists of
</p>
<p>approximately 10% Forward Industries assets and 90% IBM assets. The portfolio
</p>
<p>in the lower panel contains an equal proportion of each of the assets. The text
</p>
<p>windows on the right of Fig. 19.2 show the exact weights which were used. We can
</p>
<p>clearly see that the returns of the portfolio with a higher share of the IBM assets
</p>
<p>(which have a low variance) are much less volatile.
</p>
<p>For an exact analysis of the optimisation problem (19.4) we distinguish between
</p>
<p>two cases: the existence and nonexistence of a riskless asset. A riskless asset is an
</p>
<p>asset such as a zero bond, i.e. a financial instrument with a fixed nonrandom return
</p>
<p>(Franke, H&auml;rdle &amp; Hafner, 2011).
</p>
<p>Fig. 19.2 Portfolio of IBM
and Forward Industries
assets, equal and efficient
weights MVAportfolIBMFord
</p>
<p>0 20 40 60 80 100 120
</p>
<p>&minus;
0
.6
</p>
<p>0
.0
</p>
<p>0
.6
</p>
<p>Equally Weighted Portfolio
</p>
<p>X
</p>
<p>Y
</p>
<p>Weights
</p>
<p>0.500, IBM
0.500, Ford
</p>
<p>0 20 40 60 80 100 120
</p>
<p>&minus;
0
.6
</p>
<p>0
.0
</p>
<p>0
.6
</p>
<p>Optimal Weighted Portfolio
</p>
<p>X
</p>
<p>Y
</p>
<p>Weights
</p>
<p>0.895, IBM
0.105, Ford</p>
<p/>
</div>
<div class="page"><p/>
<p>19.2 Efficient Portfolio 491
</p>
<p>Nonexistence of a Riskless Asset
</p>
<p>Assume that the covariance matrix &dagger; is invertible (which implies positive
</p>
<p>definiteness). This is equivalent to the nonexistence of a portfolio c with variance
</p>
<p>c&gt;&dagger;c D 0. If all assets are uncorrelated, &dagger; is invertible if all of the asset returns
have positive variances. A riskless asset (uncorrelated with all other assets) would
</p>
<p>have zero variance since it has fixed, nonrandom returns. In this case &dagger; would not
</p>
<p>be positive definite.
</p>
<p>The optimal weights can be derived from the first order condition (19.4) as
</p>
<p>c D 1
2
&dagger;�1.�1�C �21p/: (19.5)
</p>
<p>Multiplying this by a .p � 1/ vector 1p of ones, we obtain
</p>
<p>1 D 1&gt;p c D
1
</p>
<p>2
1&gt;p&dagger;
</p>
<p>�1.�1�C �21p/;
</p>
<p>which can be solved for �2 to get:
</p>
<p>�2 D
2 � �11&gt;p&dagger;�1�
1&gt;p&dagger;
</p>
<p>�11p
:
</p>
<p>Plugging this expression into (19.5) yields
</p>
<p>c D 1
2
�1
</p>
<p> 
&dagger;�1� �
</p>
<p>1&gt;p&dagger;
�1�
</p>
<p>1&gt;p&dagger;
�11p
</p>
<p>&dagger;�11p
</p>
<p>!
C &dagger;
</p>
<p>�11p
1&gt;p&dagger;
</p>
<p>�11p
: (19.6)
</p>
<p>For the case of a variance efficient portfolio there is no restriction on the mean of
</p>
<p>the portfolio .�1 D 0/. The optimal weights are therefore:
</p>
<p>c D &dagger;
�11p
</p>
<p>1&gt;p&dagger;
�11p
</p>
<p>: (19.7)
</p>
<p>This formula is identical to the solution of (19.3). Indeed, differentiation with
</p>
<p>respect to c gives
</p>
<p>&dagger;c D �1p
</p>
<p>c D �&dagger;�11p:</p>
<p/>
</div>
<div class="page"><p/>
<p>492 19 Applications in Finance
</p>
<p>If we plug this into (19.3), we obtain
</p>
<p>L D 1
2
�21&gt;p&dagger;
</p>
<p>�11p � �.�1&gt;p&dagger;�11p � 1/
</p>
<p>D � � 1
2
�21&gt;p&dagger;
</p>
<p>�11p:
</p>
<p>This quantity is a function of � and is minimal for
</p>
<p>� D .1&gt;p&dagger;�11p/�1
</p>
<p>since
</p>
<p>@2L
</p>
<p>@c&gt;@c
D &dagger; &gt; 0:
</p>
<p>Theorem 19.1 The variance efficient portfolio weights for returns X � .�;&dagger;/ are
</p>
<p>copt D
&dagger;�11p
1&gt;p&dagger;
</p>
<p>�11p
: (19.8)
</p>
<p>Existence of a Riskless Asset
</p>
<p>If an asset exists with variance equal to zero, then the covariance matrix &dagger; is not
</p>
<p>invertible. The notation can be adjusted for this case as follows: denote the return of
</p>
<p>the riskless asset by r (under the absence of arbitrage this is the interest rate), and
</p>
<p>partition the vector and the covariancematrix of returns such that the last component
</p>
<p>is the riskless asset. Thus, the last equation of the system (19.4) becomes
</p>
<p>2Cov.r; X/ � �1r � �2 D 0;
</p>
<p>and, because the covariance of the riskless asset with any portfolio is zero, we have
</p>
<p>�2 D �r�1: (19.9)
</p>
<p>Let us for a moment modify the notation in such a way that in each vector and matrix
</p>
<p>the components corresponding to the riskless asset are excluded. For example, c
</p>
<p>is the weight vector of the risky assets (i.e. assets with positive variance), and c0
denotes the proportion invested in the riskless asset. Obviously, c0 D 1�1&gt;p c, and&dagger;
the covariance matrix of the risky assets, is assumed to be invertible. Solving (19.4)
</p>
<p>using (19.9) gives
</p>
<p>c D �1
2
&dagger;�1.� � r1p/: (19.10)</p>
<p/>
</div>
<div class="page"><p/>
<p>19.2 Efficient Portfolio 493
</p>
<p>This equation may be solved for �1 by plugging it into the condition �
&gt;c D N�.
</p>
<p>This is the mean-variance efficient weight vector of the risky assets if a riskless asset
</p>
<p>exists. The final solution is:
</p>
<p>c D N�&dagger;
�1.�� r1p/
</p>
<p>�&gt;&dagger;�1.�� r1p/
: (19.11)
</p>
<p>The variance optimal weighting of the assets in the portfolio depends on the
</p>
<p>structure of the covariance matrix as the following corollaries show.
</p>
<p>Corollary 19.1 A portfolio of uncorrelated assets whose returns have equal
</p>
<p>variances (&dagger; D �2Ip) needs to be weighted equally:
</p>
<p>copt D p�11p:
</p>
<p>Proof Here we obtain 1&gt;p&dagger;
�11p D ��21&gt;p 1p D ��2p and therefore c D
</p>
<p>��21p
��2p D
</p>
<p>p�11p . ut
Corollary 19.2 A portfolio of correlated assets whose returns have equal vari-
</p>
<p>ances, i.e.
</p>
<p>&dagger; D �2
</p>
<p>0
BBB@
</p>
<p>1 � � � � �
� 1 � � � �
:::
:::
: : :
</p>
<p>:::
</p>
<p>� � � � � 1
</p>
<p>1
CCCA ; �
</p>
<p>1
</p>
<p>p � 1 &lt; � &lt; 1
</p>
<p>needs to be weighted equally:
</p>
<p>copt D p�11p:
</p>
<p>Proof &dagger; can be rewritten as &dagger; D �2
n
.1 � �/Ip C �1p1&gt;p
</p>
<p>o
: The inverse is
</p>
<p>&dagger;�1 D Ip
�2.1 � �/ �
</p>
<p>�1p1
&gt;
p
</p>
<p>�2.1 � �/f1C .p � 1/�g
</p>
<p>since for a .p � p/ matrix A of the form A D .a � b/Ip C b1p1&gt;p the inverse is
generally given by
</p>
<p>A�1 D Ip
.a � b/ �
</p>
<p>b 1p1
&gt;
p
</p>
<p>.a � b/faC .p � 1/bg �</p>
<p/>
</div>
<div class="page"><p/>
<p>494 19 Applications in Finance
</p>
<p>Hence
</p>
<p>&dagger;�11p D
1p
</p>
<p>�2.1 � �/
�
</p>
<p>�1p1
&gt;
p 1p
</p>
<p>�2.1 � �/f1C .p � 1/�g
</p>
<p>D Œf1C .p � 1/�g � �p&#141;1p
�2.1 � �/f1C .p � 1/�g
</p>
<p>D f1 � �g1p
�2.1 � �/f1C .p � 1/�g
</p>
<p>D 1p
�2f1C .p � 1/�g
</p>
<p>which yields
</p>
<p>1&gt;p&dagger;
�11p D
</p>
<p>p
</p>
<p>�2f1C .p � 1/�g
</p>
<p>and thus c D p�11p . ut
Let us now consider assets with different variances. We will see that in this case
</p>
<p>the weights are adjusted to the risk.
</p>
<p>Corollary 19.3 A portfolio of uncorrelated assets with returns of different vari-
</p>
<p>ances, i.e. &dagger; D diag.�21 ; : : : ; �2p/, has the following optimal weights
</p>
<p>cj;opt D
��2j
pP
lD1
</p>
<p>��2l
</p>
<p>; j D 1; : : : ; p:
</p>
<p>Proof From &dagger;�1 D diag.��21 ; : : : ; ��2p / we have 1&gt;p&dagger;�11p D
Pp
</p>
<p>lD1 �
�2
l and
</p>
<p>therefore the optimal weights are cj D ��2j =
pP
lD1
</p>
<p>��2l . ut
</p>
<p>This result can be generalised for covariance matrices with block structures.
</p>
<p>Corollary 19.4 A portfolio of assets with returns X � .�;&dagger;/, where the
covariance matrix has the form:
</p>
<p>&dagger; D
</p>
<p>0
BBBB@
</p>
<p>&dagger;1 0 : : : 0
</p>
<p>0 &dagger;2
: : :
</p>
<p>:::
:::
: : :
</p>
<p>: : :
:::
</p>
<p>0 : : : 0 &dagger;r
</p>
<p>1
CCCCA</p>
<p/>
</div>
<div class="page"><p/>
<p>19.2 Efficient Portfolio 495
</p>
<p>has optimal weights c D .c1; : : : ; cr /&gt; given by
</p>
<p>cj;opt D
&dagger;�1j 1
</p>
<p>1&gt;&dagger;�1j 1
; j D 1; : : : ; r:
</p>
<p>Summary
</p>
<p>,! An efficient portfolio is one that keeps the risk minimal under the
constraint that a given mean return is achieved and that the weights
</p>
<p>sum to 1, i.e. that minimises L D c&gt;&dagger;cC �1. N�� c&gt;�/C �2.1�
c&gt;1p/:
</p>
<p>,! If a riskless asset does not exist, the variance efficient portfolio
weights are given by
</p>
<p>c D &dagger;
�11p
</p>
<p>1&gt;p&dagger;
�11p
</p>
<p>:
</p>
<p>,! If a riskless asset exists, the mean-variance efficient portfolio
weights are given by
</p>
<p>c D N�&dagger;
�1.� � r1p/
</p>
<p>�&gt;&dagger;�1.� � r1p/
:
</p>
<p>,! The efficient weighting depends on the structure of the covariance
matrix &dagger;. Equal variances of the assets in the portfolio lead to
</p>
<p>equal weights, different variances lead to weightings proportional
</p>
<p>to these variances:
</p>
<p>cj;opt D
��2j
pP
lD1
</p>
<p>��2l
</p>
<p>; j D 1; : : : ; p:</p>
<p/>
</div>
<div class="page"><p/>
<p>496 19 Applications in Finance
</p>
<p>19.3 Efficient Portfolios in Practice
</p>
<p>We can now demonstrate the usefulness of this technique by applying our method to
</p>
<p>the monthly market returns computed on the basis of transactions at the New York
</p>
<p>stock market and the NASDAQ stock market between January 2000 and December
</p>
<p>2009.
</p>
<p>Example 19.2 Recall that we had shown the portfolio returns with uniform and
</p>
<p>optimal weights in Fig. 19.2. The covariance matrix of the returns of IBM and
</p>
<p>Forward Industries is
</p>
<p>S D
�
0:0073 0:0023
</p>
<p>0:0023 0:0454
</p>
<p>�
:
</p>
<p>Hence by (19.7) the optimal weighting is
</p>
<p>Oc D S
�112
</p>
<p>1&gt;2 S
�112
</p>
<p>D .0:8952; 0:1048/&gt;:
</p>
<p>The effect of efficient weighting becomes even clearer when we expand the
</p>
<p>portfolio to six assets. The covariance matrix for the returns of all six firms
</p>
<p>introduced in Example 19.1 is
</p>
<p>S D 10�3
</p>
<p>0
BBBBBBB@
</p>
<p>7:3 6:2 3:1 2:3 �0:1 5:2
6:2 23:9 4:3 2:1 0:4 6:4
</p>
<p>3:1 4:3 19:5 �0:9 1:1 3:7
2:3 2:1 �0:9 45:4 �2:1 0:8
�0:1 0:4 1:1 �2:1 2:4 �0:1
5:2 6:4 3:7 0:8 �0:1 14:7
</p>
<p>1
CCCCCCCA
:
</p>
<p>Hence the optimal weighting is
</p>
<p>Oc D S
�116
</p>
<p>1&gt;6 S
�116
</p>
<p>D .0:1894;�0:0139; 0:0094; 0:0580; 0:7112; 0:0458/&gt;:
</p>
<p>As we can clearly see, the optimal weights are quite different from the equal
</p>
<p>weights (cj D 1=6). The weights which were used are shown in text windows on
the right hand side of Fig. 19.3.
</p>
<p>This efficient weighting assumes stable covariances between the assets over time.
</p>
<p>Changing covariance structure over time implies weights that depend on time as
</p>
<p>well. This is part of a large body of literature on multivariate volatility models.
</p>
<p>For a review refer to Franke et al. (2011).</p>
<p/>
</div>
<div class="page"><p/>
<p>19.4 The Capital Asset Pricing Model 497
</p>
<p>Fig. 19.3 Portfolio of all six
assets, equal and efficient
weights MVAportfol
</p>
<p>0 20 40 60 80 100 120
</p>
<p>&minus;
0
.6
</p>
<p>0
.0
</p>
<p>0
.6
</p>
<p>Equally Weighted Portfolio
</p>
<p>X
Y
</p>
<p>Weights
</p>
<p>0.167, IBM
0.167, Apple
0.167, BAC
0.167, Ford
0.167, Edison
0.167, Stanley
</p>
<p>0 20 40 60 80 100 120
</p>
<p>&minus;
0
.6
</p>
<p>0
.0
</p>
<p>0
.6
</p>
<p>Optimal Weighted Portfolio
</p>
<p>X
</p>
<p>Y
</p>
<p>Weights
</p>
<p>0.189, IBM
&minus;0.014, Apple
0.009, BAC
0.058, Ford
0.711, Edison
0.046, Stanley
</p>
<p>Summary
</p>
<p>,! Efficient portfolio weighting in practice consists of estimating the
covariances of the assets in the portfolio and then computing
</p>
<p>efficient weights from this empirical covariance matrix.
</p>
<p>,! Note that this efficient weighting assumes stable covariances
between the assets over time.
</p>
<p>19.4 The Capital Asset Pricing Model
</p>
<p>The CAPM considers the relation between a mean-variance efficient portfolio and
</p>
<p>an asset uncorrelated with this portfolio. Let us denote this specific asset return by
</p>
<p>y0. The riskless asset with constant return y0 D r may be such an asset. Recall
from (19.4) the condition for a mean-variance efficient portfolio:
</p>
<p>2&dagger;c � �1� � �21p D 0:</p>
<p/>
</div>
<div class="page"><p/>
<p>498 19 Applications in Finance
</p>
<p>In order to eliminate �2, we can multiply (19.4) by c
&gt; to get:
</p>
<p>2c&gt;&dagger;c � �1 N� D �2:
</p>
<p>Plugging this into (19.4), we obtain:
</p>
<p>2&dagger;c � �1� D 2c&gt;&dagger;c1p � �1 N�1p
</p>
<p>� D N�1p C
2
</p>
<p>�1
.&dagger;c � c&gt;&dagger;c1p/: (19.12)
</p>
<p>For the asset that is uncorrelated with the portfolio, Eq. (19.12) can be written as:
</p>
<p>y0 D N� �
2
</p>
<p>�1
c&gt;&dagger;c
</p>
<p>since y0 D r is the mean return of this asset and is otherwise uncorrelated with the
risky assets. This yields:
</p>
<p>�1 D 2
c&gt;&dagger;c
</p>
<p>N�� y0
(19.13)
</p>
<p>and if (19.13) is plugged into (19.12):
</p>
<p>� D N�1p C
N� � y0
c&gt;&dagger;c
</p>
<p>.&dagger;c � c&gt;&dagger;c1p/
</p>
<p>� D y01p C
&dagger;c
</p>
<p>c&gt;&dagger;c
. N� � y0/
</p>
<p>� D y01p C ˇ. N� � y0/ (19.14)
</p>
<p>with
</p>
<p>ˇ
defD &dagger;c
c&gt;&dagger;c
</p>
<p>:
</p>
<p>The relation (19.14) holds if there exists any asset that is uncorrelated with
</p>
<p>the mean-variance efficient portfolio c. The existence of a riskless asset is not a
</p>
<p>necessary condition for deriving (19.14). However, for this special case we arrive at
</p>
<p>the well-known expression
</p>
<p>� D r1p C ˇ. N� � r/; (19.15)
</p>
<p>which is known as the CAPM, see Franke et al. (2011). The beta factor ˇ measures
</p>
<p>the relative performance with respect to riskless assets or an index. It reflects the
</p>
<p>sensitivity of an asset with respect to the whole market. The beta factor is close to</p>
<p/>
</div>
<div class="page"><p/>
<p>19.5 Exercises 499
</p>
<p>1 for most assets. A factor of 1.16, for example, means that the asset reacts in relation
</p>
<p>to movements of the whole market (expressed through an index like DAX or DOW
</p>
<p>JONES) 16% stronger than the index. This is of course true for both positive and
</p>
<p>negative fluctuations of the whole market.
</p>
<p>Summary
</p>
<p>,! The weights of the mean-variance efficient portfolio satisfy 2&dagger;c �
�1�� �21p D 0:
</p>
<p>,! In the CAPM the mean of X depends on the riskless asset and the
pre-specified mean � as follows � D r1p C ˇ.�� r/:
</p>
<p>,! The beta factor ˇ measures the relative performance with respect
to riskless assets or an index and reflects the sensitivity of an asset
</p>
<p>with respect to the whole market.
</p>
<p>19.5 Exercises
</p>
<p>Exercise 19.1 Prove that the inverse of A D .a � b/Ip C b1p1&gt;p is given by
</p>
<p>A�1 D Ip
.a � b/ �
</p>
<p>b 1p1
&gt;
p
</p>
<p>.a � b/faC .p � 1/bg �
</p>
<p>Exercise 19.2 The empirical covariance between the 120 returns of IBM and
</p>
<p>Forward Industries is 0:0023 (see Example 19.2). Test if the true covariance is zero.
</p>
<p>Hint: Use Fisher&rsquo;s Z-transform.
</p>
<p>Exercise 19.3 Explain why in both Figs. 19.2 and 19.3 the portfolios have negative
</p>
<p>returns just before the end of the series, regardless of whether they are optimally
</p>
<p>weighted or not! (What happened in the mid 2007?)
</p>
<p>Exercise 19.4 Apply the method used in Example 19.2 on the same data
</p>
<p>(Table 22.5) including also the Digital Equipment company. Obviously one of
</p>
<p>the weights is negative. Is this an efficient weighting?
</p>
<p>Exercise 19.5 In the CAPM the ˇ value tells us about the performance of the
</p>
<p>portfolio relative to the riskless asset. Calculate the ˇ value for each single stock
</p>
<p>price series relative to the &ldquo;riskless&rdquo; asset IBM.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 20
</p>
<p>Computationally Intensive Techniques
</p>
<p>It is generally accepted that training in statistics must include some exposure to the
</p>
<p>mechanics of computational statistics. This exposure to computational methods is of
</p>
<p>an essential nature when we consider extremely high-dimensional data. Computer-
</p>
<p>aided techniques can help us to discover dependencies in high dimensions without
</p>
<p>complicated mathematical tools. A draftman&rsquo;s plot (i.e. a matrix of pairwise
</p>
<p>scatterplots like in Fig. 1.14) may lead us immediately to a theoretical hypothesis
</p>
<p>(on a lower dimensional space) on the relationship of the variables. Computer-aided
</p>
<p>techniques are therefore at the heart of multivariate statistical analysis.
</p>
<p>With the rapidly increasing amount of data statistics faces a new challenge.
</p>
<p>While in the twentieth century the focus was on the mathematical precision of
</p>
<p>statistical modelling, the twenty-first century relies more and more on data analytic
</p>
<p>procedures that provide information (even for extremely large data bases) on the
</p>
<p>fingertip. This demand on fast availability of condensed statistical information has
</p>
<p>changed the statistical paradigm and has shifted energy from mathematical analysis
</p>
<p>to computational analysis of course without loosing sight of the statistical core
</p>
<p>questions.
</p>
<p>In this chapter we first present the concept of Simplicial Depth&mdash;a multivariate
</p>
<p>extension of the data depth concept of Sect. 1.1. We then present Projection
</p>
<p>Pursuit&mdash;a semiparametric technique which is based on a one-dimensional, flexible
</p>
<p>regression or on the idea of density smoothing applied to principal component
</p>
<p>analysis (PCA) type projections. A similar model is underlying the Sliced Inverse
</p>
<p>Regression (SIR) technique which we discuss in Sect. 20.3.
</p>
<p>The next technique is called support vector machines (SVMs) and is motivated
</p>
<p>by non-linear classification (discrimination) problems. SVMs are classification
</p>
<p>methods based on statistical learning theory. A quadratic optimisation problem
</p>
<p>determines so-called support vectors with high margin that guarantee maximal
</p>
<p>separability. Non-linear classification is achieved by mapping the data into a feature
</p>
<p>space and finding a linear separating hyperplane in this feature space. Another
</p>
<p>&copy; Springer-Verlag Berlin Heidelberg 2015
W.K. H&auml;rdle, L. Simar, Applied Multivariate Statistical Analysis,
DOI 10.1007/978-3-662-45171-7_20
</p>
<p>501</p>
<p/>
</div>
<div class="page"><p/>
<p>502 20 Computationally Intensive Techniques
</p>
<p>advanced technique is CART&mdash;Classification and Regression Trees, a decision tree
</p>
<p>procedure developed by Breiman, Friedman, Olshen, and Stone (1984).
</p>
<p>20.1 Simplicial Depth
</p>
<p>Simplicial depth generalises the notion of data depth as introduced in Sect. 1.1.
</p>
<p>This general definition allows us to define a multivariate median and to visually
</p>
<p>present high-dimensional data in low dimension. For univariate data we have well
</p>
<p>known parameters of location which describe the centre of a distribution of a random
</p>
<p>variable X . These parameters are for example the mean
</p>
<p>Nx D 1
n
</p>
<p>nX
</p>
<p>iD1
xi ; (20.1)
</p>
<p>or the mode
</p>
<p>xmod D argmax
x
</p>
<p>Of .x/;
</p>
<p>where Of is the estimated density function of X (see Sect. 1.3). The median
</p>
<p>xmed D
</p>
<p>8
&lt;
:
xf.nC1/=2g if n odd
</p>
<p>x.n=2/Cx.n=2C1/
2
</p>
<p>otherwise;
</p>
<p>where x.i/ is the order statistics of the n observations xi , is yet another measure of
</p>
<p>location.
</p>
<p>The first two parameters can be easily extended to multivariate random variables.
</p>
<p>The mean in higher dimensions is defined as in (20.1) and the mode accordingly,
</p>
<p>xmod D argmax
x
</p>
<p>Of .x/
</p>
<p>with Of the estimated multidimensional density function of X (see Sect. 1.3). The
median poses a problem though since in a multivariate sense we cannot interpret the
</p>
<p>element-wise median
</p>
<p>xmed;j D
</p>
<p>8
&lt;
:
xf.nC1/=2g;j if n odd
</p>
<p>x.n=2/;jCx.n=2C1/;j
2
</p>
<p>otherwise
(20.2)
</p>
<p>as a point that is &ldquo;most central&rdquo;. The same argument applies to other observations
</p>
<p>of a sample that have a certain &ldquo;depth&rdquo; as defined in Sect. 1.1. The &ldquo;fourths&rdquo; or the</p>
<p/>
</div>
<div class="page"><p/>
<p>20.1 Simplicial Depth 503
</p>
<p>&ldquo;extremes&rdquo; are not defined in a straightforward way in higher (not even for two)
</p>
<p>dimensions.
</p>
<p>An equivalent definition of the median in one dimension is given by the simplicial
</p>
<p>depth. It is defined as follows: For each pair of datapoints xi and xj we generate a
</p>
<p>closed interval, a one-dimensional simplex, which contains xi and xj as border
</p>
<p>points. Redefine the median as the datapoint xmed, which is enclosed in the
</p>
<p>maximum number of intervals:
</p>
<p>xmed D argmax
i
</p>
<p>#fk; l I xi 2 Œxk ; xl &#141;g: (20.3)
</p>
<p>With this definition of the median, the median is the &ldquo;deepest&rdquo; and &ldquo;most
</p>
<p>central&rdquo; point in a data set as discussed in Sect. 1.1. This definition involves a
</p>
<p>computationally intensive operation since we generate n.n � 1/=2 intervals for n
observations.
</p>
<p>In two dimensions, the computation is even more intensive since the interval
</p>
<p>Œxk ; xl &#141; is replaced by a triangle constructed from three different datapoints. The
</p>
<p>median as the deepest point is then defined by that datapoint that is covered by
</p>
<p>the maximum number of triangles. In three dimensions triangles become pyramids
</p>
<p>formed from 4 points and the median is that datapoint that lies in the maximum
</p>
<p>number of pyramids.
</p>
<p>An example for the depth in two dimensions is given by the constellation of
</p>
<p>points given in Fig. 20.1. If we build for example the triangle of the points 1, 3, 5
</p>
<p>(denoted as4 135 in Table 20.1), it contains the point 4. From Table 20.1 we count
the number of coverages to obtain the simplicial depth values of Table 20.2.
</p>
<p>Simplicial Depth Example
1
</p>
<p>6
</p>
<p>3 4
</p>
<p>5
</p>
<p>2
</p>
<p>Fig. 20.1 Construction of simplicial depth MVAsimdep1</p>
<p/>
</div>
<div class="page"><p/>
<p>504 20 Computationally Intensive Techniques
</p>
<p>Table 20.1 Coverages for
artificial configuration of
points
</p>
<p>Triangle Coverages
</p>
<p>1 4 123 1 2 3
2 4 124 1 2 4
3 4 125 1 2 5
4 4 126 1 2 3 4 6
5 4 134 1 3 4
6 4 135 1 3 4 5
7 4 136 1 3 6
8 4 145 1 4 5
9 4 146 1 3 4 6
10 4 156 1 3 4 5 6
11 4 234 2 3 4
12 4 235 2 3 4 5
13 4 236 2 3 4 6
14 4 245 2 4 5
15 4 246 2 4 6
16 4 256 2 5 6
17 4 345 3 4 5
18 4 346 3 4 6
19 4 356 3 5 6
20 4 456 4 5 6
</p>
<p>Table 20.2 Simplicial
depths for artificial
configuration of points
</p>
<p>Point 1 2 3 4 5 6
</p>
<p>Depth 10 10 12 14 8 8
</p>
<p>In arbitrary dimension p, we look for datapoints that lie inside a simplex (or
</p>
<p>convex hull) formed from p C 1 points. We therefore extend the definition of the
median to the multivariate case as follows
</p>
<p>xmed D argmax
i
</p>
<p>#fk0; : : : ; kpI xi 2 hull.xk0 ; : : : ; xkp /g: (20.4)
</p>
<p>Here k0; : : : ; kp denote the indices of p C 1 datapoints. Thus for each datapoint
we have a multivariate data depth. If we compute all the necessary simplices
</p>
<p>hull.xk0 ; : : : ; xkp /, the computing time will unfortunately be exponential as the
</p>
<p>dimension increases.
</p>
<p>In Fig. 20.2 we calculate the simplicial depth for a two-dimensional, 10 point
</p>
<p>distribution according to depth. It contains 100 data points with corresponding
</p>
<p>parameters controlling its spread. The deepest point, the two-dimensional median,
</p>
<p>is indicated as a big star in the centre. The points with less depth are indicated via
</p>
<p>grey shades.</p>
<p/>
</div>
<div class="page"><p/>
<p>20.2 Projection Pursuit 505
</p>
<p>Fig. 20.2 10 point
distribution according to
depth with the median shown
as a big star in the centre
MVAsimdepex
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>&minus;4 &minus;2 0 2 4
</p>
<p>&minus;
4
</p>
<p>&minus;
2
</p>
<p>0
2
</p>
<p>4
</p>
<p>Simplicial depth
</p>
<p>X
</p>
<p>Y
</p>
<p>Summary
</p>
<p>,! The &ldquo;depth&rdquo; of a datapoint in one dimension can be computed by
counting all (closed) intervals of two datapoints which contain the
</p>
<p>datapoint
</p>
<p>,! The &ldquo;deepest&rdquo; datapoint is the central point of the distribution, the
median
</p>
<p>,! The &ldquo;depth&rdquo; of a datapoint in arbitrary dimension p is defined as
the number of simplices (constructed from p C 1 points) covering
this point. It is called simplicial depth
</p>
<p>,! A multivariate extension of the median is to take the &ldquo;deepest&rdquo;
datapoint of the distribution
</p>
<p>,! In the bivariate case we count all triangles of datapoints which
contain the datapoint to compute its depth
</p>
<p>20.2 Projection Pursuit
</p>
<p>&ldquo;Projection Pursuit&rdquo; stands for a class of exploratory projection techniques.
</p>
<p>This class contains statistical methods designed for analysing high-dimensional
</p>
<p>data using low-dimensional projections. The aim of projection pursuit is to</p>
<p/>
</div>
<div class="page"><p/>
<p>506 20 Computationally Intensive Techniques
</p>
<p>reveal possible non-linear and therefore interesting structures hidden in the high-
</p>
<p>dimensional data. To what extent these structures are &ldquo;interesting&rdquo; is measured by
</p>
<p>an index. Exploratory Projection Pursuit (EPP) goes back to Kruskal (1969, 1972).
</p>
<p>The approach was successfully implemented for exploratory purposes by various
</p>
<p>other authors. The idea has been applied to regression analysis, density estimation,
</p>
<p>classification and discriminant analysis.
</p>
<p>Exploratory Projection Pursuit
</p>
<p>In EPP, we try to find &ldquo;interesting&rdquo; low-dimensional projections of the data. For
</p>
<p>this purpose, a suitable index function I.˛/, depending on a normalised projection
</p>
<p>vector ˛, is used. This function will be defined such that &ldquo;interesting&rdquo; views
</p>
<p>correspond to local and global maxima of the function. This approach naturally
</p>
<p>accompanies the technique of PCA of the covariance structure of a randomvectorX .
</p>
<p>In PCA we are interested in finding the axes of the covariance ellipsoid. The index
</p>
<p>function I.˛/ is in this case the variance of a linear combination ˛&gt;X subject to the
normalising constraint ˛&gt;˛ D 1 (see Theorem 11.2). If we analyse a sample with a
p-dimensional normal distribution, the &ldquo;interesting&rdquo; high-dimensional structure we
</p>
<p>find by maximising this index is of course linear.
</p>
<p>There are many possible projection indices, for simplicity the kernel based and
</p>
<p>polynomial based indices are reported. Assume that the p-dimensional random
</p>
<p>variable X is sphered and centred, that is, E.X/ D 0 and Var.X/ D Ip . This
will remove the effect of location, scale, and correlation structure. This covariance
</p>
<p>structure can be achieved easily by the Mahalanobis transformation (3.26).
</p>
<p>Friedman and Tukey (1974) proposed to investigate the high-dimensional distri-
</p>
<p>bution of X by considering the index
</p>
<p>IFT;h.˛/ D n�1
nX
</p>
<p>iD1
</p>
<p>Ofh;˛.˛&gt;Xi / (20.5)
</p>
<p>where Ofh;˛ denotes the kernel estimator (see Sect. 1.3)
</p>
<p>Ofh;˛.z/ D n�1
nX
</p>
<p>jD1
Kh.z� ˛&gt;Xj / (20.6)
</p>
<p>of the projected data. Note that (20.5) is an estimate of
R
f 2.z/d z where z D ˛&gt;X
</p>
<p>is a one-dimensional random variable with mean zero and unit variance. If the high-
</p>
<p>dimensional distribution of X is normal, then each projection z D ˛&gt;X is standard
normal since jj˛jj D 1 and since X has been centred and sphered by, e.g. the
Mahalanobis transformation.</p>
<p/>
</div>
<div class="page"><p/>
<p>20.2 Projection Pursuit 507
</p>
<p>The index should therefore be stable as a function of ˛ if the high-dimensional
</p>
<p>data is in fact normal. Changes in IFT;h.˛/ with respect to ˛ therefore indicate
</p>
<p>deviations from normality. Hodges and Lehman (1956) showed that, given a mean
</p>
<p>of zero and unit variance, the (compact support) density which minimises
R
f 2 is
</p>
<p>uniquely given by
</p>
<p>f .z/ D maxf0; c.b2 � z2/g;
</p>
<p>where c D 3=.20
p
5/ and b D
</p>
<p>p
5. This is a parabolic density function, which is
</p>
<p>equal to zero outside the interval (�
p
5;
p
5). A high value of the Friedman&ndash;Tukey
</p>
<p>index indicates a larger departure from the parabolic form.
</p>
<p>An alternative index is based on the negative of the entropy measure, i.e.R
�f logf . The density for zero mean and unit variance which minimises the index
</p>
<p>Z
f logf
</p>
<p>is the standard normal density, a far more plausible candidate than the parabolic
</p>
<p>density as a norm from which departure is to be regarded as &ldquo;interesting&rdquo;. Thus
</p>
<p>in using
R
f logf as a projection index we are really implementing the viewpoint
</p>
<p>of seeing &ldquo;interesting&rdquo; projections as departures from normality. Yet another index
</p>
<p>could be based on the Fisher information (see Sect. 6.2)
</p>
<p>Z
.f 0/2=f:
</p>
<p>To optimise the entropy index, it is necessary to recalculate it at each step of the
</p>
<p>numerical procedure. There is no method of obtaining the index via summary
</p>
<p>statistics of the multivariate data set, so the workload of the calculation at each
</p>
<p>iteration is determined by the number of observations. It is therefore interesting to
</p>
<p>look for approximations to the entropy index. Jones and Sibson (1987) suggested
</p>
<p>that deviations from the normal density should be considered as
</p>
<p>f .x/ D '.x/f1C ".x/g (20.7)
</p>
<p>where the function " satisfies
</p>
<p>Z
'.u/".u/u�rdu D 0; for r D 0; 1; 2: (20.8)
</p>
<p>In order to develop the Jones and Sibson (1987) index it is convenient to think in
</p>
<p>terms of cumulants �3 D �3 D E.X3/, �4 D �4 D E.X4/ � 3 (see Sect. 1.3). The
standard normal density satisfies �3 D �4 D 0, an index with any hope of tracking
the entropy index must at least incorporate information up to the level of symmetric
</p>
<p>departures (�3 or �4 not zero) from normality. The simplest of such indices is a</p>
<p/>
</div>
<div class="page"><p/>
<p>508 20 Computationally Intensive Techniques
</p>
<p>positive definite quadratic form in �3 and �4. It must be invariant under sign-reversal
</p>
<p>of the data since both ˛&gt;X and�˛&gt;X should show the same kind of departure from
normality. Note that �3 is odd under sign-reversal, i.e. �3.˛
</p>
<p>&gt;X/ D ��3.�˛&gt;X/.
The cumulant �4 is even under sign-reversal, i.e. �4.˛
</p>
<p>&gt;X/ D �4.�˛&gt;X/. The
quadratic form in �3 and �4 measuring departure from normality cannot include
</p>
<p>a mixed �3�4 term.
</p>
<p>For the density (20.7) one may conclude with (20.8) that
</p>
<p>Z
f .u/ log.u/du � 1
</p>
<p>2
</p>
<p>Z
'.u/".u/du:
</p>
<p>Now if f is expressed as a Gram&ndash;Charli&eacute;r expansion
</p>
<p>f .x/'.x/ D f1C �3H3.x/=6C �4H4.x/=24C � � � g (20.9)
</p>
<p>(Kendall &amp; Stuart, 1977, p. 169) where Hr is the r-th Hermite polynomial, then
</p>
<p>the truncation of (20.9) and use of orthogonality and normalisation properties of
</p>
<p>Hermite polynomials with respect to ' yields
</p>
<p>1
</p>
<p>2
</p>
<p>Z
'.x/"2.x/dx D
</p>
<p>�
�23 C �24=4
</p>
<p>�
=12:
</p>
<p>The index proposed by Jones and Sibson (1987) is therefore
</p>
<p>IJS.˛/ D f�23.˛&gt;X/C �24.˛&gt;X/=4g=12:
</p>
<p>This index measures in fact the negative entropy difference
R
f logf �
</p>
<p>R
' log'.
</p>
<p>Example 20.1 The EPP is used on the Swiss bank note data. For 50 randomly
</p>
<p>chosen one-dimensional projections of this six-dimensional dataset we calculate the
</p>
<p>Friedman&ndash;Tukey index to evaluate how &ldquo;interesting&rdquo; their structures are.
</p>
<p>Figure 20.3 shows the density for the standard, normally distributed data (green)
</p>
<p>and the estimated densities for the best (red) and the worst (blue) projections found.
</p>
<p>A dotplot of the projections is also presented. In the lower part of the figure we
</p>
<p>see the estimated value of the Friedman&ndash;Tukey index for each computed projection.
</p>
<p>From this information we can judge the non normality of the bank note data set
</p>
<p>since there is a lot of variation across the 50 random projections.
</p>
<p>Projection Pursuit Regression
</p>
<p>The problem in projection pursuit regression is to estimate a response surface
</p>
<p>f .x/ D E.Y j x/</p>
<p/>
</div>
<div class="page"><p/>
<p>20.2 Projection Pursuit 509
</p>
<p>-4 -2 0 2 4
</p>
<p>-0
.2
</p>
<p>-0
.1
</p>
<p>0
.0
</p>
<p>0
.1
</p>
<p>0
.2
</p>
<p>0
.3
</p>
<p>0
.4
</p>
<p>X
</p>
<p>Y
</p>
<p>50 directions
</p>
<p>0 10 20 30 40 50
</p>
<p>0
.1
</p>
<p>5
0
</p>
<p>.2
5
</p>
<p>Fig. 20.3 Exploratory Projection Pursuit for the Swiss bank notes data (greenD standard normal,
red D best, blueD worst) MVAppexample
</p>
<p>via approximating functions of the form:
</p>
<p>Of .x/ D
MX
</p>
<p>kD1
gk.ƒ
</p>
<p>&gt;
k x/
</p>
<p>with non-parametric regression functions gk and projection indices ƒk . Given
</p>
<p>observations f.x1; y1/; : : : ; .xn; yn/g with xi 2 Rp and yi 2 R the basic algorithm
works as follows.
</p>
<p>1. Set r
.0/
i D yi and k D 1.
</p>
<p>2. Minimise
</p>
<p>Ek D
nX
</p>
<p>iD1
</p>
<p>n
r
.k�1/
i � gk.ƒ&gt;k xi /
</p>
<p>o2
</p>
<p>where ƒk is an orthogonal projection matrix and gk is a non-parametric
</p>
<p>regression estimator.
</p>
<p>3. Compute new residuals
</p>
<p>r
.k/
i D r
</p>
<p>.k�1/
i � gk.ƒ&gt;k xi /:</p>
<p/>
</div>
<div class="page"><p/>
<p>510 20 Computationally Intensive Techniques
</p>
<p>4. Increase k and repeat the last two steps until Ek becomes small.
</p>
<p>Although this approach seems to be simple, we encounter some problems. One
</p>
<p>of the most serious is that the decomposition of a function into sums of functions of
</p>
<p>projections may not be unique. An example is
</p>
<p>z1z2 D
1
</p>
<p>4ab
f.az1 C bz2/2 � .az1 � bz2/2g:
</p>
<p>Numerical improvements of this algorithm were suggested by Friedman and
</p>
<p>Stuetzle (1981).
</p>
<p>Summary
</p>
<p>,! Exploratory Projection Pursuit is a technique used to find inter-
esting structures in high-dimensional data via low-dimensional
</p>
<p>projections. Since the Gaussian distribution represents a standard
</p>
<p>situation, we define the Gaussian distribution as the most uninter-
</p>
<p>esting
</p>
<p>,! The search for interesting structures is done via a projection score
like the Friedman&ndash;Tukey index IFT.˛/ D
</p>
<p>R
f 2. The parabolic
</p>
<p>distribution has the minimal score. We maximise this score over
</p>
<p>all projections
</p>
<p>,! The Jones&ndash;Sibson index maximises
</p>
<p>IJS.˛/ D f�3.˛&gt;X/C �24.˛&gt;X/=4g=12
</p>
<p>as a function of ˛
,! The entropy index maximises
</p>
<p>IE.˛/ D
Z
f .˛&gt;X/ logf .˛&gt;X/
</p>
<p>where f is the density of ˛&gt;X
,! In Projection Pursuit Regression the idea is to represent the
</p>
<p>unknown function by a sum of non-parametric regression functions
</p>
<p>on projections. The key problem is in choosing the number of terms
</p>
<p>and often the interpretability</p>
<p/>
</div>
<div class="page"><p/>
<p>20.3 Sliced Inverse Regression 511
</p>
<p>20.3 Sliced Inverse Regression
</p>
<p>SIR is a dimension reduction method proposed by Duan and Li (1991). The idea is
</p>
<p>to find a smooth regression function that operates on a variable set of projections.
</p>
<p>Given a response variable Y and a (random) vector X 2 Rp of explanatory
variables, SIR is based on the model:
</p>
<p>Y D m.ˇ&gt;1 X; : : : ; ˇ&gt;k X; "/; (20.10)
</p>
<p>where ˇ1; : : : ; ˇk are unknown projection vectors, k is unknown and assumed to be
</p>
<p>less than p, m W RkC1 ! R is an unknown function, and " is the noise random
variable with E ." jX/ D 0.
</p>
<p>Model (20.10) describes the situation where the response variable Y depends
</p>
<p>on the p-dimensional variable X only through a k-dimensional subspace. The
</p>
<p>unknown ˇi &rsquo;s, which span this space, are called effective dimension reduction
</p>
<p>directions (EDR-directions). The span is denoted as effective dimension reduction
</p>
<p>space (EDR-space). The aim is to estimate the base vectors of this space, for which
</p>
<p>neither the length nor the direction can be identified. Only the space in which they
</p>
<p>lie is identifiable.
</p>
<p>SIR tries to find this k-dimensional subspace of Rp which under the
</p>
<p>model (20.10) carries the essential information of the regression between X and Y .
</p>
<p>SIR also focuses on small k, so that nonparametric methods can be applied for the
</p>
<p>estimation of m. A direct application of nonparametric smoothing to X is for high
</p>
<p>dimension p generally not possible due to the sparseness of the observations. This
</p>
<p>fact is well known as the curse of dimensionality, see Huber (1985).
</p>
<p>The name of SIR comes from computing the inverse regression (IR) curve. That
</p>
<p>means instead of looking for E .Y jX D x/, we investigate E .X jY D y/, a curve
in Rp consisting of p one-dimensional regressions. What is the connection between
</p>
<p>the IR and the SIR model (20.10)? The answer is given in the following theorem
</p>
<p>from Li (1991).
</p>
<p>Theorem 20.1 Given the model (20.10) and the assumption
</p>
<p>8b 2 Rp W E
�
b&gt;X jˇ&gt;1 X D ˇ&gt;1 x; : : : ; ˇ&gt;k X D ˇ&gt;k x
</p>
<p>�
D c0 C
</p>
<p>kX
</p>
<p>iD1
ciˇ
&gt;
i x;
</p>
<p>(20.11)
</p>
<p>the centred IR curve E.X jY D y/ � E.X/ lies in the linear subspace spanned by
the vectors &dagger;ˇi , i D 1; : : : ; k; where &dagger; D Cov.X/.
</p>
<p>Assumption (20.11) is equivalent to the fact that X has an elliptically symmetric
</p>
<p>distribution, see Cook and Weisberg (1991). Hall and Li (1993) have shown that
</p>
<p>assumption (20.11) only needs to hold for the EDR-directions.</p>
<p/>
</div>
<div class="page"><p/>
<p>512 20 Computationally Intensive Techniques
</p>
<p>It is easy to see that for the standardised variable Z D &dagger;�1=2fX � E.X/g the
IR curve m1.y/ D E.Z j Y D y/ lies in span.�1; : : : ; �k/, where �i D &dagger;1=2ˇi .
This means that the conditional expectation m1.y/ is moving in span.�1; : : : ; �k/
</p>
<p>depending on y. With b orthogonal to span.�1; : : : ; �k/, it follows that
</p>
<p>b&gt;m1.y/ D 0;
</p>
<p>and further that
</p>
<p>m1.y/m1.y/
&gt;b D Covfm1.y/gb D 0:
</p>
<p>As a consequence CovfE.Z jy/g is degenerated in each direction orthogonal to all
EDR-directions �i of Z. This suggests the following algorithm.
</p>
<p>First, estimate Covfm1.y/g and then calculate the orthogonal directions of this
matrix (for example, with eigenvalue/eigenvector decomposition). In general, the
</p>
<p>estimated covariance matrix will have full rank because of random variability,
</p>
<p>estimation errors and numerical imprecision. Therefore, we investigate the eigen-
</p>
<p>values of the estimate and ignore eigenvectors having small eigenvalues. These
</p>
<p>eigenvectors O�i are estimates for the EDR-direction �i of Z. We can easily rescale
them to estimates Ǒi for the EDR-directions ofX by multiplying by O&dagger;�1=2, but then
they are not necessarily orthogonal. SIR is strongly related to PCA. If all of the data
</p>
<p>falls into a single interval, which means that bCovfm1.y/g is equal to bCov.Z/, SIR
coincides with PCA. Obviously, in this case any information about y is ignored.
</p>
<p>The SIR Algorithm
</p>
<p>The algorithm to estimate the EDR-directions via SIR is as follows:
</p>
<p>1. Standardise x:
</p>
<p>zi D O&dagger;�1=2.xi � Nx/:
</p>
<p>2. Divide the range of yi into S nonoverlapping intervals (slices)Hs , s D 1; : : : ; S .
ns denotes the number of observations within slice Hs , and IHs the indicator
</p>
<p>function for this slice:
</p>
<p>ns D
nX
</p>
<p>iD1
IHs .yi /:
</p>
<p>3. Compute the mean of zi over all slices. This is a crude estimate Om1 for the inverse
regression curve m1:
</p>
<p>Nzs D ns�1
nX
</p>
<p>iD1
zi IHs .yi /:</p>
<p/>
</div>
<div class="page"><p/>
<p>20.3 Sliced Inverse Regression 513
</p>
<p>4. Calculate the estimate for Covfm1.y/g:
</p>
<p>OV D n�1
SX
</p>
<p>sD1
ns Nzs Nz&gt;s :
</p>
<p>5. Identify the eigenvalues O�i and eigenvectors O�i of OV .
6. Transform the standardised EDR-directions O�i back to the original scale. Now
</p>
<p>the estimates for the EDR-directions are given by
</p>
<p>Ǒ
i D O&dagger;�1=2 O�i :
</p>
<p>Remark 20.1 The number of different eigenvalues unequal to zero depends on the
</p>
<p>number of slices. The rank of OV cannot be greater than the number of slices �1 (the
zi sum up to zero). This is a problem for categorical response variables, especially
</p>
<p>for a binary response&mdash;where only one direction can be found.
</p>
<p>SIR II
</p>
<p>In the previous section we learned that it is interesting to consider the IR curve,
</p>
<p>that is, E.X j y/. In some situations however SIR does not find the EDR-direction.
We overcome this difficulty by considering the conditional covariance Cov.X j y/
instead of the IR curve. An example where the EDR directions are not found via the
</p>
<p>SIR curve is given below.
</p>
<p>Example 20.2 Suppose that .X1; X2/
&gt; � N.0; I2/ and Y D X21 . Then E.X2 j
</p>
<p>y/ D 0 because of independence and E.X1 j y/ D 0 because of symmetry. Hence,
the EDR-direction ˇ D .1; 0/&gt; is not found when the IR curve E.X j y/ D 0 is
considered.
</p>
<p>The conditional variance
</p>
<p>Var.X1 jY D y/ D E.X21 jY D y/ D y;
</p>
<p>offers an alternative way to find ˇ. It is a function of y while Var.X2 j y/ is a
constant.
</p>
<p>The idea of SIR II is to consider the conditional covariances. The principle of
</p>
<p>SIR II is the same as before: investigation of the IR curve (here the conditional
</p>
<p>covariance instead of the conditional expectation). Unfortunately, the theory of SIR
</p>
<p>II is more complicated. The assumption of the elliptical symmetrical distribution of
</p>
<p>X has to be more restrictive, i.e. assuming the normality of X .
</p>
<p>Given this assumption, one can show that the vectors with the largest distance to
</p>
<p>Cov.Z j Y D y/ � EfCov.Z j Y D y/g for all y are the most interesting for the</p>
<p/>
</div>
<div class="page"><p/>
<p>514 20 Computationally Intensive Techniques
</p>
<p>EDR-space. An appropriate measure for the overall mean distance is, according to
</p>
<p>Li (1992),
</p>
<p>E
�
jj ŒCov.Z jY D y/ � EfCov.Z jY D y/g&#141; bjj2
</p>
<p>�
</p>
<p>D b&gt; E
�
jjCov.Z jy/ � EfCov.Z jy/gjj2
</p>
<p>�
b: (20.12)
</p>
<p>Equipped with this distance, we conduct again an eigensystem decomposition, this
</p>
<p>time for the above expectation E
�
jjCov.Z jy/ � EfCov.Z jy/gjj2
</p>
<p>�
. Then we take
</p>
<p>the rescaled eigenvectors with the largest eigenvalues as estimates for the unknown
</p>
<p>EDR-directions.
</p>
<p>The SIR II Algorithm
</p>
<p>The algorithm of SIR II is very similar to the one for SIR, it differs in only
</p>
<p>two steps. Instead of merely computing the mean, the covariance of each slice
</p>
<p>has to be computed. The estimate for the above expectation (20.12) is calculated
</p>
<p>after computing all slice covariances. Finally, decomposition and rescaling are
</p>
<p>conducted, as before.
</p>
<p>1. Do steps 1&ndash;3 of the SIR algorithm.
</p>
<p>2. Compute the slice covariance matrix OVs:
</p>
<p>OVs D .ns � 1/�1
nX
</p>
<p>iD1
IHs .yi /zi z
</p>
<p>&gt;
i � ns Nzs Nz&gt;s :
</p>
<p>3. Calculate the mean over all slice covariances:
</p>
<p>NV D n�1
SX
</p>
<p>sD1
ns OVs:
</p>
<p>4. Compute an estimate for (20.12):
</p>
<p>OV D n�1
SX
</p>
<p>sD1
ns
</p>
<p>�
OVs � NV
</p>
<p>�2
D n�1
</p>
<p>SX
</p>
<p>sD1
ns OV 2s � NV 2:
</p>
<p>5. Identify the eigenvectors and eigenvalues of OV and scale back the eigenvectors.
This gives estimates for the SIR II EDR-directions:
</p>
<p>Ǒ
i D O&dagger;�1=2 O�i :</p>
<p/>
</div>
<div class="page"><p/>
<p>20.3 Sliced Inverse Regression 515
</p>
<p>3 2 1 0 1 2
</p>
<p>1
5
0
</p>
<p>5
0
</p>
<p>0
5
0
</p>
<p>1
0
0
</p>
<p>1
5
0
</p>
<p>XBeta1 vs Response
</p>
<p>first index
</p>
<p>re
s
p
o
n
s
e
</p>
<p>3 2 1 0 1 2
</p>
<p>1
5
0
</p>
<p>5
0
</p>
<p>0
5
0
</p>
<p>1
0
0
</p>
<p>1
5
0
</p>
<p>XBeta2 vs Response
</p>
<p>second index
</p>
<p>re
s
p
o
n
s
e
</p>
<p>XBeta1 XBeta2 Response
</p>
<p>3 2 1  0  1  2  3
</p>
<p>1
5
</p>
<p>0
1
</p>
<p>0
0
</p>
<p> 
5
</p>
<p>0
  
</p>
<p> 0
  
</p>
<p>5
0
</p>
<p> 1
0
</p>
<p>0
 1
</p>
<p>5
0
</p>
<p>4
3
</p>
<p>2
1
</p>
<p> 0
 1
</p>
<p> 2
 
</p>
<p>first index
</p>
<p>s
e
c
o
n
d
 i
n
d
e
x
</p>
<p>re
s
p
o
n
s
e
</p>
<p>1.0 1.5 2.0 2.5 3.0
</p>
<p>0
.2
</p>
<p>0
.4
</p>
<p>0
.6
</p>
<p>0
.8
</p>
<p>1
.0
</p>
<p>Scree Plot
</p>
<p>K
</p>
<p>P
s
i(
k
) 
</p>
<p>E
ig
</p>
<p>e
n
v
a
lu
</p>
<p>e
s
</p>
<p>Fig. 20.4 SIR: The left plots show the response versus the estimated EDR-directions. The upper
right plot is a three-dimensional plot of the first two directions and the response. The lower right
</p>
<p>plot shows the eigenvalues O�i (asterisk) and the cumulative sum (open circle) MVAsirdata
</p>
<p>Example 20.3 The result of SIR is visualised in four plots in Fig. 20.4: the left two
</p>
<p>show the response variable versus the first respectively second direction. The upper
</p>
<p>right plot consists of a three-dimensional plot of the first two directions and the
</p>
<p>response. The last picture shows O&permil;k , the ratio of the sum of the first k eigenvalues
and the sum of all eigenvalues, similar to PCA.
</p>
<p>The data are generated according to the following model:
</p>
<p>yi D ˇ&gt;1 xi C .ˇ&gt;1 xi /3 C 4
�
ˇ&gt;2 xi
</p>
<p>�2 C "i ;
</p>
<p>where the xi &rsquo;s follow a three-dimensional normal distribution with zero mean, the
</p>
<p>covariance equal to the identity matrix, ˇ2 D .1;�1;�1/&gt;, and ˇ1 D .1; 1; 1/&gt;.</p>
<p/>
</div>
<div class="page"><p/>
<p>516 20 Computationally Intensive Techniques
</p>
<p>Fig. 20.5 Plot of the true
response versus the true first
index. The monotonic and the
convex shapes can be clearly
seen MVAsirdata
</p>
<p>4 2 0 2 4
</p>
<p>1
5
</p>
<p>0
1
</p>
<p>0
0
</p>
<p>5
0
</p>
<p>0
5
</p>
<p>0
1
</p>
<p>0
0
</p>
<p>1
5
</p>
<p>0
</p>
<p>True index vs Response
</p>
<p>first index
</p>
<p>re
s
p
</p>
<p>o
n
</p>
<p>s
e
</p>
<p>Fig. 20.6 Plot of the true
response versus the true
second index. The monotonic
and the convex shapes can be
</p>
<p>clearly seen MVAsirdata
</p>
<p>4 2 0 2
</p>
<p>1
5
0
</p>
<p>1
0
0
</p>
<p>5
0
</p>
<p>0
5
</p>
<p>0
1
</p>
<p>0
0
</p>
<p>1
5
</p>
<p>0
</p>
<p>True index vs Response
</p>
<p>second index
</p>
<p>re
s
p
</p>
<p>o
n
</p>
<p>s
e
</p>
<p>"i is standard, normally distributed and n D 300. Corresponding to model (20.10),
m.u; v; "/ D uC u3 C v2 C ". The situation is depicted in Figs. 20.5 and 20.6.
</p>
<p>Both algorithms were conducted using the slicing method with 20 elements in
</p>
<p>each slice. The goal was to find ˇ1 and ˇ2 with SIR. The data are designed such
</p>
<p>that SIR can detect ˇ1 because of the monotonic shape of fˇ&gt;1 xi C .ˇ&gt;1 xi /3g, while
SIR II will search for ˇ2, as in this direction the conditional variance on y is varying.</p>
<p/>
</div>
<div class="page"><p/>
<p>20.3 Sliced Inverse Regression 517
</p>
<p>Table 20.3 SIR:
EDR-directions for simulated
data
</p>
<p>Ǒ
1
</p>
<p>Ǒ
2
</p>
<p>Ǒ
3
</p>
<p>0:452 0:881 0:040
</p>
<p>0:571 �0:349 �0:787
0:684 �0:320 0:615
</p>
<p>Table 20.4 SIR II:
EDR-directions for simulated
data
</p>
<p>Ǒ
1
</p>
<p>Ǒ
2
</p>
<p>Ǒ
3
</p>
<p>�0.272 0:964 �0:001
0.670 0:100 0:777
</p>
<p>0.690 0:244 �0:630
</p>
<p>If we normalise the eigenvalues for the EDR-directions in Table 20.3 such that
</p>
<p>they sum up to one, the resulting vector is .0:852; 0:086; 0:062/. As can be seen in
</p>
<p>the upper left plot of Fig. 20.4, there is a functional relationship found between the
</p>
<p>first index Ǒ&gt;1 x and the response. Actually, ˇ1 and Ǒ1 are nearly parallel, that is, the
normalised inner product Ǒ&gt;1 ˇ1=fjj Ǒ1jjjjˇ1jjg D 0:9894 is very close to one.
</p>
<p>The second direction along ˇ2 is probably found due to the good approximation,
</p>
<p>but SIR does not provide it clearly, because it is &ldquo;blind&rdquo; with respect to the change
</p>
<p>of variance, as the second eigenvalue indicates.
</p>
<p>For SIR II, the normalised eigenvalues are .0:706; 0:185; 0:108/, that is, about
</p>
<p>69% of the variance is explained by the first EDR-direction (Table 20.4). Here, the
</p>
<p>normalised inner product of ˇ2 and Ǒ1 is 0:9992. The estimator Ǒ1 estimates in fact
ˇ2 of the simulated model. In this case, SIR II found the direction where the second
</p>
<p>moment varies with respect to ˇ&gt;2 x (Fig. 20.7).
</p>
<p>In summary, SIR has found the direction which shows a strong relation regarding
</p>
<p>the conditional expectation between ˇ&gt;1 x and y, and SIR II has found the direction
where the conditional variance is varying, namely, ˇ&gt;2 x.
</p>
<p>The behaviour of the two SIR algorithms is as expected. In addition, we have
</p>
<p>seen that it is worthwhile to apply both versions of SIR. It is possible to combine
</p>
<p>SIR and SIR II (Cook &amp; Weisberg, 1991; Li, 1991; Schott, 1994) directly, or to
</p>
<p>investigate higher conditional moments. For the latter it seems to be difficult to
</p>
<p>obtain theoretical results.
</p>
<p>Summary
</p>
<p>,! SIR serves as a dimension reduction tool for regression problems
</p>
<p>,! Inverse regression avoids the curse of dimensionality
</p>
<p>,! The dimension reduction can be conducted without estimation of
the regression function y D m.x/</p>
<p/>
</div>
<div class="page"><p/>
<p>518 20 Computationally Intensive Techniques
</p>
<p>Summary (continued)
</p>
<p>,! SIR searches for the effective dimension reduction (EDR) by
computing the inverse regression IR
</p>
<p>,! SIR II uses the EDR on computing the inverse conditional variance
</p>
<p>,! SIR might miss EDR directions that are found by SIR II
</p>
<p>3 2 1 0 1 2
</p>
<p>1
5
0
</p>
<p>5
0
</p>
<p>0
5
0
</p>
<p>1
0
0
</p>
<p>1
5
0
</p>
<p>XBeta1 vs Response
</p>
<p>first index
</p>
<p>re
s
p
o
n
s
e
</p>
<p>3 2 1 0 1 2
</p>
<p>1
5
0
</p>
<p>5
0
</p>
<p>0
5
0
</p>
<p>1
0
0
</p>
<p>1
5
0
</p>
<p>XBeta2 vs Response
</p>
<p>second index
</p>
<p>re
s
p
o
n
s
e
</p>
<p>XBeta1 XBeta2 Response
</p>
<p>3 2 1  0  1  2  3
</p>
<p>1
5
</p>
<p>0
1
</p>
<p>0
0
</p>
<p>5
0
</p>
<p>  
 0
</p>
<p>  
5
</p>
<p>0
 1
</p>
<p>0
0
</p>
<p> 1
5
</p>
<p>0
</p>
<p>4
3
</p>
<p>2
1
</p>
<p> 0
 1
</p>
<p> 2
</p>
<p>first index
</p>
<p>s
e
c
o
n
d
 i
n
d
e
x
</p>
<p>re
s
p
o
n
s
e
</p>
<p>1.0 1.5 2.0 2.5 3.0
</p>
<p>0
.2
</p>
<p>0
.4
</p>
<p>0
.6
</p>
<p>0
.8
</p>
<p>1
.0
</p>
<p>Scree Plot
</p>
<p>K
</p>
<p>P
s
i(
k
) 
</p>
<p>E
ig
</p>
<p>e
n
v
a
lu
</p>
<p>e
s
</p>
<p>Fig. 20.7 SIR II mainly sees the direction ˇ2. The left plots show the response versus the
estimated EDR-directions. The upper right plot is a three-dimensional plot of the first two
</p>
<p>directions and the response. The lower right plot shows the eigenvalues O�i (asterisk) and the
cumulative sum (open circle) MVAsir2data</p>
<p/>
</div>
<div class="page"><p/>
<p>20.4 Support Vector Machines 519
</p>
<p>20.4 Support Vector Machines
</p>
<p>The purpose of this section is to introduce one of the most promising among recently
</p>
<p>developed multivariate non-linear statistical techniques: the SVM. The SVM is
</p>
<p>a classification method that is based on statistical learning theory. It has been
</p>
<p>successfully applied to optical character recognition, early medical diagnostics, and
</p>
<p>text classification. One application where SVMs outperformed other methods is
</p>
<p>electric load prediction (EUNITE, 2001), another one is optical character recogni-
</p>
<p>tion (Vapnik, 1995). In a variety of applications SVMs produce better classification
</p>
<p>results than parametric methods (e.g. logit analysis) and are outperforming widely
</p>
<p>used nonparametric techniques, such as neural networks. Here we apply SVMs to
</p>
<p>corporate bankruptcy analysis.
</p>
<p>Classification Methodology
</p>
<p>In order to illustrate the classification methodology we focus for the moment on
</p>
<p>a company rating example that we will treat further in more detail. Investment
</p>
<p>risks are evaluated via the default probability (PD) for a company. Each company is
</p>
<p>described by a set of variables (predictors) x, such as financial ratios, and its class y
</p>
<p>that can be either y D �1 (&ldquo;successful&rdquo;) or y D 1 (&ldquo;bankrupt&rdquo;). Financial ratios are
constructed from the variables like net income, total assets, interest payments, etc.
</p>
<p>A training set represents a sample of data for companies which are known to have
</p>
<p>survived or gone bankrupt. From the training set one estimates a classifier function
</p>
<p>f that is then applied to computing PDs. These PDs can be uniquely translated into
</p>
<p>a company rating.
</p>
<p>Classical discriminant analysis is based on the assumption that each group of
</p>
<p>observations is normally distributed with the same variance&ndash;covariance matrix but
</p>
<p>different means. Under such a formulation the discriminating function will be linear,
</p>
<p>see Theorem 14.2. Figure 20.8 displays this situation: if some linear combination of
</p>
<p>predictors (called Z-score in the context of bankruptcy analysis) is greater than
</p>
<p>a particular threshold value z0 the observation under consideration is regarded as
</p>
<p>belonging to y D 1; if Z &lt; z0 the observation would belong to y D �1
(successful). One can change the labels &ldquo;�1,C1&rdquo; to the more standard notation
&ldquo;0,1&rdquo;. The current labeling is done only for mathematical convenience.
</p>
<p>The Z-score is:
</p>
<p>Zi D a1xi1 C a2xi2 C : : :C apxip D a&gt;xi ;
</p>
<p>where xi D .xi1; : : : ; xip/&gt; 2 Rp are predictors for the i -th company. The
classification based on the Z-score are necessarily linear and, therefore, may not
</p>
<p>handle more complex situations as in Fig. 20.9 when non-linear classifiers, such as
</p>
<p>those generated by SVMs, can produce better results.</p>
<p/>
</div>
<div class="page"><p/>
<p>520 20 Computationally Intensive Techniques
</p>
<p>Fig. 20.8 A linear
classification function in the
case of linearly separable data
</p>
<p>X
1
</p>
<p>X
2 Surviving
</p>
<p>companies
</p>
<p>x
</p>
<p>o
</p>
<p>o
</p>
<p>Failing
</p>
<p>companies
</p>
<p>x
</p>
<p>x
</p>
<p>x
</p>
<p>xx
</p>
<p>x
</p>
<p>x
</p>
<p>x x
</p>
<p>x
</p>
<p>x
</p>
<p>x
</p>
<p>x
</p>
<p>x
x x
</p>
<p>x
</p>
<p>x
</p>
<p>x
</p>
<p>x
</p>
<p>x
</p>
<p>x
</p>
<p>x x
</p>
<p>x
</p>
<p>x x
</p>
<p>xx
</p>
<p>o
</p>
<p>o
</p>
<p>o
o
</p>
<p>o
</p>
<p>o
</p>
<p>o o
</p>
<p>o
oo
</p>
<p>o
</p>
<p>o
</p>
<p>o
</p>
<p>o o
</p>
<p>o
</p>
<p>o
o
</p>
<p>o
</p>
<p>o
</p>
<p>o
</p>
<p>o
</p>
<p>o
</p>
<p>oo
</p>
<p>o
</p>
<p>o
</p>
<p>o
</p>
<p>o
</p>
<p>o
o
</p>
<p>o
</p>
<p>o
</p>
<p>o
o
</p>
<p>x
</p>
<p>x
</p>
<p>x
</p>
<p>x
</p>
<p>Fig. 20.9 Different linear
classification functions (1)
and (2) and a non-linear one
(3) in the linearly
non-separable case
</p>
<p>X
1
</p>
<p>X
2
</p>
<p>Surviving
</p>
<p>companies
</p>
<p>x
</p>
<p>x
</p>
<p>x
</p>
<p>x
</p>
<p>x
xx
</p>
<p>x
</p>
<p>x
</p>
<p>x x
</p>
<p>x
</p>
<p>x
</p>
<p>x
x
</p>
<p>x
</p>
<p>x
</p>
<p>x x
</p>
<p>x
</p>
<p>x
</p>
<p>x
</p>
<p>x
</p>
<p>o
</p>
<p>o
</p>
<p>o
</p>
<p>o
</p>
<p>oo
</p>
<p>o
</p>
<p>o
</p>
<p>o
</p>
<p>o
o
</p>
<p>o o
</p>
<p>o
</p>
<p>o
</p>
<p>o
</p>
<p>o
</p>
<p>o
</p>
<p>oo
</p>
<p>o
</p>
<p>o
</p>
<p>o
o o
</p>
<p>o
o
</p>
<p>o
</p>
<p>o
</p>
<p>o
</p>
<p>o
</p>
<p>o
</p>
<p>o
</p>
<p>o
</p>
<p>o
</p>
<p>o
</p>
<p>x
</p>
<p>x
</p>
<p>x
</p>
<p>x
</p>
<p>x
</p>
<p>x
</p>
<p>x
</p>
<p>Failing
</p>
<p>companies
</p>
<p>x
</p>
<p>3 12
</p>
<p>o
</p>
<p>o
</p>
<p>x
</p>
<p>Expected vs. Empirical Risk Minimisation
</p>
<p>A non-linear classifier function f may be described by a function classF .F is fixed
</p>
<p>a priori, e.g. it can be the class of linear classifiers (hyperplanes). A good classifier
</p>
<p>optimises some criterion that tells us how well f separates the classes. As in (14.4)
</p>
<p>one considers the minimisation of the expected risk:
</p>
<p>R .f / D
Z
1
</p>
<p>2
jf .x/ � yj dF.x; y/: (20.13)
</p>
<p>The joint distributionF.x; y/, however, is never known in practical applications and
</p>
<p>must be estimated from the training set fxi ; yi gniD1. By replacing F.x; y/ with the
empirical cdf Fn.x; y/ one obtains the empirical risk:
</p>
<p>OR .f / D 1
n
</p>
<p>nX
</p>
<p>iD1
</p>
<p>1
</p>
<p>2
jf .xi / � yi j : (20.14)</p>
<p/>
</div>
<div class="page"><p/>
<p>20.4 Support Vector Machines 521
</p>
<p>The empirical risk is an average value of loss over the training set, while the
</p>
<p>expected risk is the expected value of loss under the true probability measure. The
</p>
<p>loss is given by:
</p>
<p>L.x; y/ D 1
2
jf .x/ � yj D
</p>
<p>(
0; if classification is correct,
</p>
<p>1; if classification is wrong.
</p>
<p>One sees here that it is convenient to work with the labels &ldquo;�1; 1&rdquo; for y. The
solutions to the problems of expected and empirical risk minimisation:
</p>
<p>fopt D arg min
f 2F
</p>
<p>R .f / ; (20.15)
</p>
<p>Ofn D arg min
f 2F
</p>
<p>OR .f / ; (20.16)
</p>
<p>generally do not coincide (Fig. 20.10), although converge as n ! 1 if F is not
too large. According to statistical learning theory (Vapnik, 1995), it is possible to
</p>
<p>get a uniform upper bound on the difference between R .f / and OR .f / via the
Vapnik&ndash;Chervonenkis (VC) theory. The VC bound states that there is a function
</p>
<p>� (monotone increasing in h) so that for all f 2 F with a probability 1 � �:
</p>
<p>R .f / � OR .f /C �
�
h
</p>
<p>n
;
log.�/
</p>
<p>n
</p>
<p>�
: (20.17)
</p>
<p>Here h denotes the VC dimension, a measure of complexity of the involved function
</p>
<p>class F . For a linear classification rule g.x/ D sign.x&gt;wC b/:
</p>
<p>�
</p>
<p>�
h
</p>
<p>n
;
log.�/
</p>
<p>n
</p>
<p>�
D
</p>
<p>s
h
�
log 2n
</p>
<p>h
</p>
<p>�
� log �
</p>
<p>4
</p>
<p>n
; (20.18)
</p>
<p>Fig. 20.10 The minima fopt
</p>
<p>and Ofn of the expected (R)
and empirical ( OR) risk
functions generally do not
coincide
</p>
<p>Function class
</p>
<p>Risk
</p>
<p>f f
opt
</p>
<p>f
n
</p>
<p>R
R
</p>
<p>emp
</p>
<p>R
emp
</p>
<p>(f)
</p>
<p>R (f)
</p>
<p>ˆ
</p>
<p>ˆ
</p>
<p>ˆ</p>
<p/>
</div>
<div class="page"><p/>
<p>522 20 Computationally Intensive Techniques
</p>
<p>where h is the VC dimension. By plotting the function � .u; v/ D
˚
� u � log 2uC
</p>
<p>log 4�v
��1=2
</p>
<p>for small u one sees the monotonicity of � .u; v/. In fact one can show
</p>
<p>that
</p>
<p>@�
�
h
n
;
log.�/
</p>
<p>n
</p>
<p>�
</p>
<p>@h
&gt; 0
</p>
<p>if and only if 2n &gt; h. For a linear classifier with h D p C 1 this is an easy condition
to meet.
</p>
<p>The VC dimension of a set F of functions in a d -dimensional space is h if some
</p>
<p>function f 2 F can shatter h objects
˚
xi 2 Rd ; i D 1; : : : ; h
</p>
<p>�
, in all 2h possible
</p>
<p>configurations and no set
˚
xj 2 Rd ; j D 1; : : : ; q
</p>
<p>�
with q &gt; h, exists that satisfies
</p>
<p>this property. For example, three points on a plane (d D 2) can be shattered by linear
indicator functions in 2h D 23 D 8 ways, whereas 4 points can not be shattered in
2q D 24 D 16 ways. Thus, the VC dimension of the set of linear indicator functions
in a two-dimensional space is h D 3, see Fig. 20.11. The expression for the VC
bound (20.17) involves the VC dimension h, a parameter controlling complexity
</p>
<p>of F . The term �
n
h
n
;
log.�/
</p>
<p>n
</p>
<p>o
introduces a penalty for excessive complexity of a
</p>
<p>classifier function. The higher is the complexity of f 2 F the higher are h and
therefore �. There is a trade-off between the number of classification errors on the
</p>
<p>training set and the complexity of the classifier function. If the complexity were
</p>
<p>not controlled for, it would be possible to construct a classifier function with no
</p>
<p>classification errors on the training set notwithstanding how low its generalisation
</p>
<p>ability would be.
</p>
<p>Fig. 20.11 Eight possible ways of shattering 3 points on the plane with a linear indicator function</p>
<p/>
</div>
<div class="page"><p/>
<p>20.4 Support Vector Machines 523
</p>
<p>The SVM in the Linearly Separable Case
</p>
<p>First we will describe the SVM in the linearly separable case. The family F of
</p>
<p>classification functions in the data space is given by:
</p>
<p>F D
˚
x&gt;wC b;w 2 Rp; b 2 R
</p>
<p>�
(20.19)
</p>
<p>In order to determine the support vectors we choose f 2 F (or equivalently .w; b/)
such that the so-called margin&mdash;the corridor between the separating hyperplanes&mdash;
</p>
<p>is maximal. This situation is illustrated in Fig. 20.12. Themargin is equal to d�CdC.
The classification function is a hyperplane plus the margin zone, where, in the
</p>
<p>separable case, no observations can lie. It separates the points from both classes with
</p>
<p>the highest &ldquo;safest&rdquo; distance (margin) between them. It can be shown that margin
</p>
<p>maximisation corresponds to the reduction of complexity as given by the VC-
</p>
<p>dimension of the SVM classifier. Apparently, the separating hyperplane is defined
</p>
<p>only by the support vectors that hold the hyperplanes parallel to the separating one.
</p>
<p>In Fig. 20.12 there are three support vectors that are marked with bold style: two
</p>
<p>crosses and one circle. We come now to the description of the SVM selection.
</p>
<p>Let x&gt;wC b D 0 be a separating hyperplane. Then dC .d�/ will be the shortest
distance to the closest objects from the classes C1 .�1/. Since the separation can
be done without errors, all observations i D 1; 2; : : : ; n must satisfy:
</p>
<p>x&gt;i wC b � C1 for yi D C1
x&gt;i wC b � �1 for yi D �1
</p>
<p>Fig. 20.12 The separating
hyperplane x&gt;wC b D 0
and the margin in the linearly
separable case</p>
<p/>
</div>
<div class="page"><p/>
<p>524 20 Computationally Intensive Techniques
</p>
<p>We can combine both constraints into one:
</p>
<p>yi .x
&gt;
i wC b/� 1 � 0 i D 1; 2; : : : ; n (20.20)
</p>
<p>The canonical hyperplanes x&gt;i wCb D ˙1 are parallel and the distance between
each of them and the separating hyperplane is dC D d� D 1=kwk. To maximise the
margin dC C d� D 2=kwk one therefore minimises the Euclidean norm kwk or its
square kwk2.
</p>
<p>The Lagrangian for the primal problem that corresponds to margin maximisation
</p>
<p>subject to constraint (20.20) is:
</p>
<p>LP .w; b/ D
1
</p>
<p>2
kwk2 �
</p>
<p>nX
</p>
<p>iD1
˛i fyi .x&gt;i wC b/� 1g (20.21)
</p>
<p>The Karush&ndash;Kuhn&ndash;Tucker (KKT) (Gale et al., 1951) first order optimality
</p>
<p>conditions are:
</p>
<p>@LP
</p>
<p>@w
D 0 W w �
</p>
<p>nX
</p>
<p>iD1
˛iyixi D 0
</p>
<p>@LP
</p>
<p>@b
D 0 W
</p>
<p>nX
</p>
<p>iD1
˛iyi D 0
</p>
<p>yi .x
&gt;
i wC b/ � 1 � 0; i D 1; : : : ; n
</p>
<p>˛i � 0
˛i fyi .x&gt;i wC b/ � 1g D 0
</p>
<p>From these first order condition, we can derive w D
Pn
</p>
<p>iD1 ˛iyixi and therefore
the summands in (20.21) read:
</p>
<p>1
</p>
<p>2
kwk2 D 1
</p>
<p>2
</p>
<p>nX
</p>
<p>iD1
</p>
<p>nX
</p>
<p>jD1
˛i˛jyiyjx
</p>
<p>&gt;
i xj
</p>
<p>�
nX
</p>
<p>iD1
˛i fyi.x&gt;i wC b/ � 1g D �
</p>
<p>nX
</p>
<p>iD1
˛iyix
</p>
<p>&gt;
i
</p>
<p>nX
</p>
<p>jD1
˛jyjxj C
</p>
<p>nX
</p>
<p>iD1
˛i
</p>
<p>D �
nX
</p>
<p>iD1
</p>
<p>nX
</p>
<p>jD1
˛i˛jyiyjx
</p>
<p>&gt;
i xj C
</p>
<p>nX
</p>
<p>iD1
˛i</p>
<p/>
</div>
<div class="page"><p/>
<p>20.4 Support Vector Machines 525
</p>
<p>Substituting this into (20.21) we obtain the Lagrangian for the dual problem:
</p>
<p>LD .˛/ D
nX
</p>
<p>iD1
˛i �
</p>
<p>1
</p>
<p>2
</p>
<p>nX
</p>
<p>iD1
</p>
<p>nX
</p>
<p>jD1
˛i˛jyiyjx
</p>
<p>&gt;
i xj : (20.22)
</p>
<p>The primal and dual problems are:
</p>
<p>min
w;b
</p>
<p>LP .w; b/
</p>
<p>max
˛
LD .˛/ s.t. ˛i � 0;
</p>
<p>nX
</p>
<p>iD1
˛iyi D 0:
</p>
<p>Since the optimisation problem is convex the dual and primal formulations give the
</p>
<p>same solution.
</p>
<p>Those points i for which the equation yi .x
&gt;
i wC b/ D 1 holds are called support
</p>
<p>vectors. After &ldquo;training the SVM&rdquo; i.e. solving the dual problem above and deriving
</p>
<p>Lagrange multipliers (they are equal to 0 for non-support vectors) one can classify
</p>
<p>a company. One uses the classification rule:
</p>
<p>g.x/ D sign
�
x&gt;wC b
</p>
<p>�
; (20.23)
</p>
<p>where w D
Pn
</p>
<p>iD1 ˛iyixi and b D 12 .xC1 C x�1/w. xC1 and x�1 are two support
vectors belonging to different classes for which y.x&gt;wC b/ D 1. The value of the
classification function (the score of a company) can be computed as
</p>
<p>f .x/ D x&gt;wC b: (20.24)
</p>
<p>Each score f .x/ uniquely corresponds to a default probability (PD). The higher
</p>
<p>f .x/ the higher the PD.
</p>
<p>SVMs in the Linearly Non-separable Case
</p>
<p>In the linearly non-separable case the situation is like in Fig. 20.13. The slack
</p>
<p>variables �i represent the violation from strict separation. In this case the following
</p>
<p>inequalities can be induced from Fig. 20.13:
</p>
<p>x&gt;i wC b � 1 � �i for yi D 1;
x&gt;i wC b � �1C �i for yi D �1;
</p>
<p>�i � 0:</p>
<p/>
</div>
<div class="page"><p/>
<p>526 20 Computationally Intensive Techniques
</p>
<p>Fig. 20.13 The separating
hyperplane x&gt;wC b D 0
and the margin in the linearly
non-separable case
</p>
<p>They can be combined into two constraints:
</p>
<p>yi .x
&gt;
i wC b/ � 1� �i (20.25)
�i � 0: (20.26)
</p>
<p>SVM classification again maximises the margin given a family of classification
</p>
<p>functions F .
</p>
<p>The penalty for misclassification, the classification error �i � 0, is related to
the distance from a misclassified point xi to the canonical hyperplane bounding its
</p>
<p>class. If �i &gt; 0, an error in separating the two sets occurs. The objective function
</p>
<p>corresponding to penalised margin maximisation is then formulated as:
</p>
<p>1
</p>
<p>2
kwk2 C C
</p>
<p>nX
</p>
<p>iD1
�i ; (20.27)
</p>
<p>where the parameter C characterises the weight given to the classification errors.
</p>
<p>The minimisation of the objective function with constraint (20.25) and (20.26) pro-
</p>
<p>vides the highest possible margin in the case when classification errors are inevitable
</p>
<p>due to the linearity of the separating hyperplane. Under such a formulation the
</p>
<p>problem is convex.
</p>
<p>The Lagrange function for the primal problem is:
</p>
<p>LP .w; b; �/ D
1
</p>
<p>2
kwk2 C C
</p>
<p>nX
</p>
<p>iD1
�i �
</p>
<p>nX
</p>
<p>iD1
˛i fyi
</p>
<p>�
x&gt;i wC b
</p>
<p>�
� 1C �i g �
</p>
<p>nX
</p>
<p>iD1
�i�i ;
</p>
<p>(20.28)</p>
<p/>
</div>
<div class="page"><p/>
<p>20.4 Support Vector Machines 527
</p>
<p>where ˛i � 0 and �i � 0 are Lagrange multipliers. The primal problem is
formulated as:
</p>
<p>min
w;b;�
</p>
<p>LP .w; b; �/ :
</p>
<p>The first order conditions in this case are:
</p>
<p>@LP
</p>
<p>@w
D 0 W w �
</p>
<p>nX
</p>
<p>iD1
˛iyixi D 0
</p>
<p>@LP
</p>
<p>@b
D 0 W
</p>
<p>nX
</p>
<p>iD1
˛iyi D 0
</p>
<p>@LP
</p>
<p>@�i
D 0 W C � ˛i � �i D 0
</p>
<p>With the conditions for the Lagrange multipliers:
</p>
<p>˛i � 0
�i � 0
˛i fyi.x&gt;i wC b/ � 1C �i g D 0
�i �i D 0
</p>
<p>Note that
Pn
</p>
<p>iD1 ˛iyib D 0 therefore similar to the linear separable case the primal
problem translates into:
</p>
<p>LD .˛/ D
1
</p>
<p>2
</p>
<p>nX
</p>
<p>iD1
</p>
<p>nX
</p>
<p>jD1
˛i˛jyiyjx
</p>
<p>&gt;
i xj �
</p>
<p>nX
</p>
<p>iD1
˛iyix
</p>
<p>&gt;
i
</p>
<p>nX
</p>
<p>jD1
˛jyjxj
</p>
<p>CC
nX
</p>
<p>iD1
�i C
</p>
<p>nX
</p>
<p>iD1
˛i �
</p>
<p>nX
</p>
<p>iD1
˛i �i �
</p>
<p>nX
</p>
<p>iD1
�i�i
</p>
<p>D
nX
</p>
<p>iD1
˛i �
</p>
<p>1
</p>
<p>2
</p>
<p>nX
</p>
<p>iD1
</p>
<p>nX
</p>
<p>jD1
˛i˛jyiyjx
</p>
<p>&gt;
i xj C
</p>
<p>nX
</p>
<p>iD1
�i .C � ˛i � �i /
</p>
<p>Since the last term is 0 we derive the dual problem as:
</p>
<p>LD .˛/ D
nX
</p>
<p>iD1
˛i �
</p>
<p>1
</p>
<p>2
</p>
<p>nX
</p>
<p>iD1
</p>
<p>nX
</p>
<p>jD1
˛i˛jyiyjx
</p>
<p>&gt;
i xj ; (20.29)</p>
<p/>
</div>
<div class="page"><p/>
<p>528 20 Computationally Intensive Techniques
</p>
<p>and the dual problem is posed as:
</p>
<p>max
˛
LD .˛/ ;
</p>
<p>subject to:
</p>
<p>0 � ˛i � C;
nX
</p>
<p>iD1
˛iyi D 0:
</p>
<p>Non-linear Classification
</p>
<p>The SVMs can also be generalised to the non-linear case. In order to obtain non-
</p>
<p>linear classifiers as in Fig. 20.14 one maps the data with a non-linear structure via a
</p>
<p>function&permil; W Rp 7! H into a very large dimensional spaceHwhere the classification
rule is (almost) linear. Note that all the training vectors xi appear in LD (20.29)
</p>
<p>only as scalar products of the form x&gt;i xj . In the non-linear SVM situations this
transforms to  .xi /
</p>
<p>&gt;  
�
xj
�
.
</p>
<p>The so-called kernel trick is to compute this scalar product via a kernel function.
</p>
<p>These kernel functions are actually related to those we presented in Sect. 1.3.
</p>
<p>If a kernel function K exists such that K.xi ; xj / D &permil;.xi /&gt;&permil;.xj /, then it
can be used without knowing the transformation &permil; explicitly. A necessary and
</p>
<p>sufficient condition for a symmetric function K.xi ; xj / to be a kernel is given
</p>
<p>by Mercer&rsquo;s theorem (Mercer, 1909). It requires positive definiteness, i.e. for
</p>
<p>Feature SpaceData Space
</p>
<p>x
x
</p>
<p>x
</p>
<p>x
</p>
<p>x
x
</p>
<p>x
</p>
<p>x
</p>
<p>x
</p>
<p>x
</p>
<p>x
</p>
<p>x
</p>
<p>x
</p>
<p>x
</p>
<p>x
</p>
<p>x
</p>
<p>x
</p>
<p>xx
</p>
<p>x
x
</p>
<p>x
</p>
<p>x
x
</p>
<p>x
x
</p>
<p>x
</p>
<p>x
</p>
<p>x
</p>
<p>o
o o
</p>
<p>o
</p>
<p>o
</p>
<p>o
</p>
<p>o
o
</p>
<p>o o
</p>
<p>o
</p>
<p>o
</p>
<p>o
o
</p>
<p>o
</p>
<p>o
</p>
<p>o
</p>
<p>o
</p>
<p>o
</p>
<p>o
</p>
<p>o
</p>
<p>o
</p>
<p>o
</p>
<p>o
</p>
<p>o
o
</p>
<p>o
</p>
<p>o
</p>
<p>o
o
</p>
<p>o
o
</p>
<p>x
</p>
<p>x
</p>
<p>x
</p>
<p>x
</p>
<p>x
</p>
<p>o
</p>
<p>o
</p>
<p>o
</p>
<p>o
</p>
<p>o
</p>
<p>Fig. 20.14 Mapping into a three-dimensional feature space from a two-dimensional data space
</p>
<p>R
2 7! R3. The transformation &permil;.x1; x2/ D .x21 ;
</p>
<p>p
2x1x2; x
</p>
<p>2
2/
</p>
<p>&gt; corresponds to the kernel function
K.xi ; xj / D .x&gt;i xj /2</p>
<p/>
</div>
<div class="page"><p/>
<p>20.4 Support Vector Machines 529
</p>
<p>any data set x1; : : : ; xn and any real numbers �1; : : : ; �n the function K must
</p>
<p>satisfy
</p>
<p>nX
</p>
<p>iD1
</p>
<p>nX
</p>
<p>jD1
�i�jK.xi ; xj / � 0: (20.30)
</p>
<p>Some examples of kernel functions are:
</p>
<p>&ndash; K.xi ; xj / D e�kxi�xjk=2�
2
&mdash;the isotropic Gaussian kernel with constant �
</p>
<p>&ndash; K.xi ; xj / D e�.xi�xj /
&gt;r�2&dagger;�1.xi�xj /=2&mdash;the stationary Gaussian kernel with an
</p>
<p>anisotropic radial basis with constant r and variance&ndash;covariance matrix &dagger; from
</p>
<p>training set
</p>
<p>&ndash; K.xi ; xj / D .x&gt;i xj C 1/p&mdash;the polynomial kernel of degree p
&ndash; K.xi ; xj / D tanh.kx&gt;i xj � ı/&mdash;the hyperbolic tangent kernel with constant k
</p>
<p>and ı.
</p>
<p>SVMs for Simulated Data
</p>
<p>The basic parameters of SVMs are on the scaling r of the anisotropic radial basis
</p>
<p>functions (in the stationary Gaussian kernel) and the capacity C . The parameter r
</p>
<p>controls the local resolution of the SVM in the sense that smaller r create smaller
</p>
<p>curvature of the margin. The capacity C controls the amount of slack to allow for
</p>
<p>unclassified observations. A large C would create a very rough and curved margin
</p>
<p>where C close to zero makes the margin more smooth.
</p>
<p>One of the guinea pig tests for a classification algorithm is the data described
</p>
<p>as &ldquo;orange peel&rdquo;, i.e. when two groups of observations have similar means,
</p>
<p>their variance, however, being different. The classification results in this case are
</p>
<p>presented in Fig. 20.15. An SVM with a radial basis kernel is highly suitable for
</p>
<p>such a kind of data.
</p>
<p>Another popular non-linear test is the classification of &ldquo;spiral data&rdquo;.We generated
</p>
<p>two spirals with the distance between them equal 1.0 that span over 3� radian. The
</p>
<p>SVM was chosen with r D 0:1 and C D 10=n. The SVM was able to separate the
classes without an error if noise with parameters "i � N.0; 0:12I/ was injected into
the pure spiral data (Fig. 20.16). Obviously, both the &ldquo;orange peel&rdquo; and the &ldquo;spiral
</p>
<p>data&rdquo; are not linearly separable.
</p>
<p>Solution of the SVM Classification Problem
</p>
<p>The standard SVM optimisation problem (20.29), which is a quadratic optimisa-
</p>
<p>tion problem, is usually solved by means of quadratic programming (QP). This</p>
<p/>
</div>
<div class="page"><p/>
<p>530 20 Computationally Intensive Techniques
</p>
<p>Fig. 20.15 SVM
classification results for the
&ldquo;orange peel&rdquo; data, n D 200,
d D 2, n�1 D nC1 D 100,
xC1;i � N..0; 0/&gt;; 22I/,
x�1;i � N..0; 0/&gt;; 0:52I/
with SVM parameters
r D 0:5 and C D 20=200
MVAsvmOrangePeel
</p>
<p>&minus;1.0
</p>
<p>&minus;0.5
</p>
<p>0.0
</p>
<p>0.5
</p>
<p>1.0
</p>
<p>&minus;3 &minus;2 &minus;1 0 1 2 3
</p>
<p>&minus;3
</p>
<p>&minus;2
</p>
<p>&minus;1
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>SVM classification plot
</p>
<p>X1
</p>
<p>X
2
</p>
<p>Fig. 20.16 SVM
classification results for the
noisy spiral data. The spirals
spread over 3� radian; the
distance between the spirals
equals 1.0. d D 2,
n�1 D nC1 D 100, n D 200.
The noise was injected with
the parameters
"i � N.0; 0:12I/. The
separation is perfect with
SVM parameters r D 0:1 and
C D 10=200
MVAsvmSpiral
</p>
<p>&minus;0.2
</p>
<p>&minus;0.1
</p>
<p>0.0
</p>
<p>0.1
</p>
<p>0.2
</p>
<p>&minus;4 &minus;2 0 2 4
</p>
<p>&minus;4
</p>
<p>&minus;2
</p>
<p>0
</p>
<p>2
</p>
<p>4
</p>
<p>●●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●●
</p>
<p>●●
●
</p>
<p>●
●●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
●
●
●
</p>
<p>●
●●
</p>
<p>●
</p>
<p>●
●●
</p>
<p>●●
</p>
<p>●
●●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
●
● ●
</p>
<p>●
● ●
</p>
<p>●
● ● ●
</p>
<p>●
</p>
<p>●●
</p>
<p>●●
●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●●
●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●●●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>SVM classification plot
</p>
<p>X1
</p>
<p>X
2
</p>
<p>technique, however, is notorious for (i) its bad scaling properties (the time required
</p>
<p>to solve the problem is proportional to n3, where n is the number of observations),
</p>
<p>(ii) implementation difficulty and (iii) enormous memory requirements. With the
</p>
<p>QP technique the whole kernel matrix of the size n � n has to be fit in the memory,
which, assuming that each variable takes up 10 bytes of memory, will require
</p>
<p>10 � n � n bytes. This means that 1 million observation (which is not unusual for
practical applications such as credit scoring) will require 12,000 TBytes (terabytes)</p>
<p/>
</div>
<div class="page"><p/>
<p>20.4 Support Vector Machines 531
</p>
<p>or 10,000,000 MBytes of operating memory to store. With a typical size of the
</p>
<p>computer memory of 512 MBytes no more than around 5,000 observations can
</p>
<p>be processed. Thus, the main emphasis in designing new algorithms was made
</p>
<p>on using special properties of SVMs to speed up the solution and reduce memory
</p>
<p>requirements.
</p>
<p>Scoring Companies
</p>
<p>For our illustration we selected the largest bankrupt companies with the capitalisa-
</p>
<p>tion of no less than 1 billion USD. The dataset used in this work is from the Credit
</p>
<p>reform database provided by the Research Data Center (RDC) of the Humboldt
</p>
<p>Universit&auml;t zu Berlin. It contains financial information from about 20,000 solvent
</p>
<p>and 1,000 insolvent German companies. The period spans from 1996 to 2002 and in
</p>
<p>the case of the insolvent companies the information is gathered 2 years before the
</p>
<p>insolvency took place. The last annual report of a company before it goes bankrupt
</p>
<p>receives the indicator y D 1 and for the rest (solvent) companies y D �1.
We are given 28 variables, i.e. cash, inventories, equity, EBIT, number of
</p>
<p>employees, and branch code. From the original data, we create common financial
</p>
<p>indicators which are denoted as x1; : : : ; x25. These ratios can be grouped into four
</p>
<p>categories such as profitability, leverage, liquidity, and activity.
</p>
<p>Obviously, data for the year of 1996 are missing and we will exclude them for
</p>
<p>further calculations. In order to reduce the effect of the outliers on the results,
</p>
<p>all observations that exceeded the upper limit of IQ (Inter-quartile range) or the
</p>
<p>lower limit of IQ were replaced with these values. To demonstrate how performance
</p>
<p>changes, we will use the Accounts Payable (AP) turnover (named X24) and ratio
</p>
<p>of Operating Income (OI) and Total Asset (TA) (namedX3). We choose randomnly
</p>
<p>50 solvent and 50 insolvent companies. The statistical description of financial ratios
</p>
<p>is summarized in Table 20.5.
</p>
<p>Keep in mind that different kernels will influence performance. We will use one
</p>
<p>of the most common ones, the isotropic Gaussian kernel. Triangles and circles
</p>
<p>in Fig. 20.17 represent successful and failing companies from the training set,
</p>
<p>respectively. The coloured background corresponds to different score values f . The
</p>
<p>more blue the area, the higher the score and the greater the probability of default.
</p>
<p>Most successful companies lying in the red area have positive profitability and a
</p>
<p>reasonable activity.
</p>
<p>Figure 20.17 presents the classification results for an SVM using isotropic
</p>
<p>Gaussian kernel with � D 100 and the fixed capacity C D 1. With given priors,
the SVM has trouble classifying between solvent and insolvent company. The radial
</p>
<p>base � , which determines the minimum radius of a group, is too large. Notice that
</p>
<p>SVM do a poor job of distinguishing between groups even thoughmost observations
</p>
<p>are used as support vector.
</p>
<p>The applied SVMs differed in two aspects: (i) their capacity that is controlled
</p>
<p>by the coefficient C in (20.28) and (ii) the complexity of classifier functions
</p>
<p>controlled in our case by the isotropic radial basis in the Gaussian kernel. In</p>
<p/>
</div>
<div class="page"><p/>
<p>532 20 Computationally Intensive Techniques
</p>
<p>Fig. 20.17 Ratings of
companies in two dimensions.
Low complexity of classifier
functions with � D 100 and
C D 1. Percentage of
misclassification is 0.43
MVAsvmSig100C1
</p>
<p>&minus;2.0
</p>
<p>&minus;1.5
</p>
<p>&minus;1.0
</p>
<p>&minus;0.5
</p>
<p>0.0
</p>
<p>0.5
</p>
<p>0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35
</p>
<p>&minus;0.2
</p>
<p>&minus;0.1
</p>
<p>0.0
</p>
<p>0.1
</p>
<p>0.2
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>SVM classification plot
</p>
<p>x24
</p>
<p>x
3
</p>
<p>Table 20.5 Descriptive
statistics for financial ratios
</p>
<p>Ratio q0:05 Med. q0:95 IQR
</p>
<p>OI/TA �0:22 0:00 0:10 0:06
AP/sales 0:03 0:14 0:36 0:10
</p>
<p>Fig. 20.18 Ratings of
companies in two
</p>
<p>dimensions. The case of an
average complexity of
classifier functions with
� D 2 and capacity is fixed at
C D 1. Percentage of
misclassification is reduced to
0.27 MVAsvmSig2C1
</p>
<p>&minus;1.5
</p>
<p>&minus;1.0
</p>
<p>&minus;0.5
</p>
<p>0.0
</p>
<p>0.5
</p>
<p>1.0
</p>
<p>0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35
</p>
<p>&minus;0.2
</p>
<p>&minus;0.1
</p>
<p>0.0
</p>
<p>0.1
</p>
<p>0.2
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>SVM classification plot
</p>
<p>x24
</p>
<p>x
3
</p>
<p>Fig. 20.18 the value � is reduced to 2 while C remains the same. SVM start
</p>
<p>recognising the difference between solvent and insolvent companies resulting in
</p>
<p>sharper cluster. Figure 20.19 demonstrate the effect of the changing capacity to the
</p>
<p>classification result. The optimisation of SVM parameters (C and �) can be done</p>
<p/>
</div>
<div class="page"><p/>
<p>20.4 Support Vector Machines 533
</p>
<p>&minus;10
</p>
<p>&minus;5
</p>
<p>0
</p>
<p>5
</p>
<p>0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35
</p>
<p>&minus;0.2
</p>
<p>&minus;0.1
</p>
<p>0.0
</p>
<p>0.1
</p>
<p>0.2
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>SVM classification plot
</p>
<p>x24
</p>
<p>x
3
</p>
<p>Fig. 20.19 Ratings of companies in two dimensions. High capacity (C D 200) with radial basis
is fixed at � D 0:5. Percentage of misclassification is 0.10 MVAsvmSig05C200
</p>
<p>Fig. 20.20 Cumulative accuracy profile (CAP) curve
</p>
<p>by using grid search method or an other advance algorithm the so-called Genetic
</p>
<p>Algorithm.
</p>
<p>Figure 20.20 shows a Cumulative Accuracy Profile (CAP) curve which is
</p>
<p>particularly useful in that it simultaneously measures Type I and Type II errors.
</p>
<p>In statistical terms, the CAP curve represents the cumulative probability of default
</p>
<p>events for different percentiles of the risk score scale. Now, we introduce Accuracy
</p>
<p>Ratio (AR) derived from CAP curve for measuring and comparing the performance
</p>
<p>of credit risk model. Therefore, AR is defined as the ratio of the area between a
</p>
<p>model CAP curve and the random curve to the area between the perfect CAP curve</p>
<p/>
</div>
<div class="page"><p/>
<p>534 20 Computationally Intensive Techniques
</p>
<p>and the random CAP curve (see Fig. 20.20). Perfect classification is attained if the
</p>
<p>value of AR is equal to one.
</p>
<p>Summary
</p>
<p>,! SVM classification is done by mapping the data into feature space
and finding a separating hyperplane there
</p>
<p>,! The support vectors are determined via a quadratic optimisation
problem
</p>
<p>,! SVM produces highly non-linear classification boundaries
</p>
<p>20.5 Classification and Regression Trees
</p>
<p>Classification and Regression Trees (CART) is a method of data analysis developed
</p>
<p>by a group of American statisticians (Breiman et al. , 1984). The aim of CART is to
</p>
<p>classify observations into a subset of known classes or to predict levels of regression
</p>
<p>functions. CART is a non-parametric tool which is designed to represent decision
</p>
<p>rules in a form of the so-called binary trees. Binary trees split a learning sample
</p>
<p>parallel to the coordinate axis and represent the resulting data clusters hierarchically
</p>
<p>starting from a root node for the whole learning sample itself and ending with
</p>
<p>relatively homogenous buckets of observations.
</p>
<p>Regression trees are constructed in a similar way but the final buckets do not
</p>
<p>represent classes but rather approximations to an unknown regression functions at
</p>
<p>a particular point of the independent variable. In this sense regression trees are
</p>
<p>estimates via a non-parametric regression model. Here we provide an outlook of
</p>
<p>how decision trees are created, what challenges arise during practical applications
</p>
<p>and, of course, a number of examples will illustrate the power of CART.
</p>
<p>How Does CART Work?
</p>
<p>Consider the example of how high risk patients (those who will not survive at least
</p>
<p>30 days after a heart attack is admitted) were identified at San DiegoMedical Center,
</p>
<p>University of California on the basis of initial 24-h data. A classification rule using
</p>
<p>at most three decisions (questions) is presented in Fig. 20.21. Left branches of the
</p>
<p>tree represent cases of positive answers, right branches&mdash;negative ones so that e.g.
</p>
<p>if minimum systolic blood pressure over the last 24 h is less or equal 91, then the</p>
<p/>
</div>
<div class="page"><p/>
<p>20.5 Classification and Regression Trees 535
</p>
<p>Fig. 20.21 Decision tree for
low/high patients
</p>
<p>Is minimum systolic blood pressure
over the initial 24 hours &gt; 91?
</p>
<p>Is age &gt; 62.5?
</p>
<p>Is sinus tachycardia present?
</p>
<p>High risk Low risk
</p>
<p>Low risk
</p>
<p>High risk
</p>
<p>patient belongs to the high risk group. In this example the dependant variable is
</p>
<p>binary: low risk (0) and high risk (1).
</p>
<p>A different situation occurs when we are interested in the expected amount of
</p>
<p>days the patient will be able to survive. The decision tree will probably change and
</p>
<p>the terminal nodes will now indicate a mean expected number of days the patient
</p>
<p>will survive. This situation describes a regression tree rather than a classification
</p>
<p>tree.
</p>
<p>In a more formal setup let Y be a dependent variable&mdash;binary or continuous and
</p>
<p>X 2 Rd . We are interested in approximating
</p>
<p>f .x/ D E .Y jX D x/
</p>
<p>For the definition of conditional expectations we refer to Sect. 4.2. CART estimates
</p>
<p>this function f by a step function that is constructed via splits along the coordinate
</p>
<p>axis. An illustration is given in Fig. 20.22. The regression function f .x/ is
</p>
<p>approximated by the values of the step function. The splits along the coordinate
</p>
<p>axes are to be determined from the data.
</p>
<p>The following simple one-dimensional example shows that the choice of
</p>
<p>splits points involves some decisions. Suppose that f .x/ D I .x 2 Œ0; 1&#141;/ C
2 I .x 2 Œ1; 2&#141;/ is a simple step function with a step at x D 1. Assume now that
one observes Yi D f .xi / C "i ; Xi � U Œ0; 2&#141;; "i � N .0; 1/. By going through
the X data points as possible split points one sees that in the neighbourhood of
</p>
<p>x D 1 one has two possibilities: one simply takes the Xi left to 1 or the observation
right to 1. In order to make such splits unique one averages these neighbouring
</p>
<p>points.</p>
<p/>
</div>
<div class="page"><p/>
<p>536 20 Computationally Intensive Techniques
</p>
<p>0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
</p>
<p>0.1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>0.6
</p>
<p>0.7
</p>
<p>0.8
</p>
<p>0.9
</p>
<p>1
</p>
<p>X
</p>
<p>Y
</p>
<p>Fig. 20.22 CART orthogonal splitting example where each colour corresponds to one cluster
</p>
<p>Impurity Measures
</p>
<p>A more formal framework on how to split and where to split needs to be developed.
</p>
<p>Suppose there are n observations in the learning sample and nj is the overall number
</p>
<p>of observations belonging to class j , j D 1; : : : ; J . The class probabilities are:
</p>
<p>� .j / D nj
n
; j D 1; : : : ; J (20.31)
</p>
<p>� .j / is the proportion of observations belonging to a particular class. Let n.t/
</p>
<p>be the number of observations at node t and nj .t/&mdash;the number of observations
</p>
<p>belonging to the j -th class at t . The frequency of the event that an observation of
</p>
<p>the j -th class falls into node t is:
</p>
<p>p.j; t/ D �.j /nj .t/
nj
</p>
<p>(20.32)
</p>
<p>The proportion of observations at t are p.t/ D
JP
jD1
</p>
<p>p.j; t/ the conditional
</p>
<p>probability of an observation to belong to class j given that it is at node t is:
</p>
<p>p.j j t/ D p.j; t/
p.t/
</p>
<p>D nj .t/
n.t/
</p>
<p>(20.33)</p>
<p/>
</div>
<div class="page"><p/>
<p>20.5 Classification and Regression Trees 537
</p>
<p>Define now a degree of class homogeneity in a given node. This characteristic&mdash;
</p>
<p>an impurity measure i.t/&mdash;will represent a class homogeneity indicator for a given
</p>
<p>tree node and hence will help to find optimal splits. Define an impurity function �.t/
</p>
<p>which is determined on .p1; : : : ; pJ / 2 Œ0; 1&#141;J with
JP
jD1
</p>
<p>pj D 1 so that:
</p>
<p>1. � has a unique maximum at point
�
1
J
; 1
J
; : : : ; 1
</p>
<p>J
</p>
<p>�
;
</p>
<p>2. � has a unique minimum at points .1; 0; 0; : : : ; 0/, .0; 1; 0; : : : ; 0/, : : :,
</p>
<p>.0; 0; 0; : : : ; 1/;
</p>
<p>3. � is a symmetric function of p1; : : : ; pJ
</p>
<p>Each function satisfying these conditions is called an impurity function. Given �,
</p>
<p>define the impurity measure i.t/ for a node t as:
</p>
<p>i.t/ D � fp.1j t/; p.2j t/; : : : ; p.J j t/g (20.34)
</p>
<p>Denote an arbitrary data split by s, then for a given node t which we will call
</p>
<p>a parent node two child nodes described in Fig. 20.23 arise: tL and tR representing
</p>
<p>observations meeting and not meeting the split criterion s. A fraction pL of data
</p>
<p>from t falls to the left child node and pR D 1 � pL is the share of data in tR.
A quality measure of how well split s works is:
</p>
<p>&#129;i.s; t/ D i.t/ � pLi.tL/ � pRi.tR/ (20.35)
</p>
<p>The higher the value of &#129;i.s; t/ the better split we have since data impurity is
</p>
<p>reduced. In order to find an optimal split s it is natural to maximise &#129;i.s; t/. Note
</p>
<p>that in (20.35) for different splits s, the value of i.t/ remains constant, hence it is
</p>
<p>equivalent to find
</p>
<p>s� D argmax
s
</p>
<p>&#129;i .s; t/
</p>
<p>D argmax
s
</p>
<p>f�pLi .tL/� pRi .tR/g
</p>
<p>D argmax
s
</p>
<p>fpLi .tL/C pRi .tR/g
</p>
<p>Fig. 20.23 Parent and child
nodes hierarchy
</p>
<p>Node t
</p>
<p>Node tL Node tR</p>
<p/>
</div>
<div class="page"><p/>
<p>538 20 Computationally Intensive Techniques
</p>
<p>where tL and tR are implicit functions of s. This splitting procedure is repeated until
</p>
<p>one arrives at a minimal bucket size. Classes are then assigned to terminal nodes
</p>
<p>using the following rule:
</p>
<p>If p.j j t/ D max
i
p.i j t/; then j �.t/ D j (20.36)
</p>
<p>If the maximum is not unique, then j �.t/ is assigned randomly to those classes
for which p.i j t/ takes its maximum value. The crucial question is of course to
define an impurity function i .t/. A natural definition of impurity is via a variance
</p>
<p>measure: Assign 1 to all observations at node t belonging to class j and 0 to others.
</p>
<p>A sample variance estimate for node t observations is p.j j t/ f1 � p.j j t/g.
Summing over all J classes we obtain the Gini index:
</p>
<p>i .t/ D
JX
</p>
<p>jD1
p.j j t/ f1 � p.j j t/g D 1 �
</p>
<p>JX
</p>
<p>jD1
p2.j j t/ (20.37)
</p>
<p>The Gini index is an impurity function �.p1; : : : ; pJ /, pj D p.j j t/. It is not
hard see that the Gini index is a convex function. Since pL C pR D 1, we get:
</p>
<p>i.tL/pL C i.tR/pR D � fp.1j tL/; : : : ; p.J j tL/gpL C � fp.1j tR/; : : : ; p.J j tR/gpR
� � fpLp.1j tL/C pRp.1j tR/; : : : ; pLp.J j tL/C pRp.J j tR/g
</p>
<p>where inequality becomes an equality in case p.j j tL/ D p.j j tR/, j D 1; : : : ; J .
Recall that
</p>
<p>p.j; tL/
</p>
<p>p.t/
D p.tL/
</p>
<p>p.t/
� p.j; tL/
p.tL/
</p>
<p>D pLp.j j tL/
</p>
<p>and since
</p>
<p>p.j j t/ D p.j; tL/C p.j; tR/
p.t/
</p>
<p>D pLp.j j tL/C pRp.j j tR/
</p>
<p>we can conclude that
</p>
<p>i.tL/pL C i.tR/pR � i.t/ (20.38)
</p>
<p>Hence each variant of data split leads to &#129;i.s; t/ &gt; 0 unless p.j j tR/ D
p.j j tL/ D p.j j t/, i.e. when no split decreases class heterogeneity.
</p>
<p>Impurity measures can be defined in a number of different ways, for practical
</p>
<p>applications the so-called twoing rule can be considered. Instead of maximising
</p>
<p>impurity change at a particular node, the twoing rule tries to balance as if the
</p>
<p>learning sample had only two classes. The reason for such an algorithm is that such</p>
<p/>
</div>
<div class="page"><p/>
<p>20.5 Classification and Regression Trees 539
</p>
<p>a decision rule is able to distinguish observations between general factors on top
</p>
<p>levels of the tree and take into account specific data characteristics at lower levels.
</p>
<p>If S D f1; : : : ; J g is the set of learning sample classes, divide it into two subsets
</p>
<p>S1 D fj1; : : : ; jng ; and S2 D SnS1
</p>
<p>All observations belonging to S1 get dummy class 1, and the rest dummy class 2.
</p>
<p>The next step is to calculate&#129;i.s; t/ for different s as if there were only two (dummy)
</p>
<p>classes. Since actually &#129;i.s; t/ depends on S1, the value&#129;i.s; t; S1/ is maximised.
</p>
<p>Now apply a two-step procedure: first, find s�.S1/ maximising &#129;i.s; t; S1/ and
second, find a superclass S�1 maximising&#129;i fs�.S1/; t; S1g. In other words the idea
of twoing is to find a combination of superclasses at each node that maximises the
</p>
<p>impurity increment for two classes.
</p>
<p>This method provides one big advantage: it finds the so-called strategic nodes,
</p>
<p>i.e. nodes filtering observations in the way that they are different to the maximum
</p>
<p>feasible extent. Although applying the twoing rule may seem to be desirable espe-
</p>
<p>cially for data with a big number of classes, another challenge arises: computational
</p>
<p>speed. Let&rsquo;s assume that the learning sample has J classes, then a set S can be
</p>
<p>split into S1 and S2 by 2
J�1 ways. For 11 classes data this will create more than
</p>
<p>1,000 combinations. Fortunately the following result helps to reduce drastically the
</p>
<p>amount of computations.
</p>
<p>It can be proven (Breiman et al. , 1984) that in a classification task with two
</p>
<p>classes and impurity measure p.1j t/p.2j t/ for an arbitrary split s a superclass
S1.s/ is determined by:
</p>
<p>S1.s/ D fj W p.j j tL/ � p.j j tR/g ;
</p>
<p>max
S1
&#129;i.s; t; S1/ D
</p>
<p>pLpR
</p>
<p>4
</p>
<p>8
&lt;
:
</p>
<p>JX
</p>
<p>jD1
jp.j j tL/ � p.j j tR/j
</p>
<p>9
=
;
</p>
<p>2
</p>
<p>(20.39)
</p>
<p>Hence the twoing rule can be applied in practice as well as Gini index, although
</p>
<p>the first criterion works a bit slower.
</p>
<p>Gini Index and Twoing Rule in Practice
</p>
<p>In this section we look at practical issues of using these two rules. Consider
</p>
<p>a learning dataset from Salford Systems with 400 observations characterising
</p>
<p>automobiles: their make, type, colour, technical parameters, age etc. The aim is to
</p>
<p>build a decision tree splitting different cars by their characteristics based on feasible
</p>
<p>relevant parameters. The classification tree constructed using the Gini index is given
</p>
<p>in Fig. 20.24.</p>
<p/>
</div>
<div class="page"><p/>
<p>540 20 Computationally Intensive Techniques
</p>
<p>400 makes, models and vehical types
</p>
<p>Other makes and models
</p>
<p>Other makes and models
</p>
<p>............
Ford F-150
</p>
<p>Honda Accord
</p>
<p>Ford Taurus
</p>
<p>Fig. 20.24 Classification tree constructed by Gini index
</p>
<p>400 makes, models and vehical types
</p>
<p>Passenger vehicles
</p>
<p>Luxury
</p>
<p>Mid-size
Large
</p>
<p>......... .........
</p>
<p>Economy
</p>
<p>Trucks and vans
</p>
<p>Trucks
</p>
<p>Light
Heavy
</p>
<p>......... .........
</p>
<p>Vans
</p>
<p>Fig. 20.25 Classification tree constructed by twoing index
</p>
<p>A particular feature here is that at each node observations belonging to one make
</p>
<p>are filtered out, i.e. observations with most striking characteristics are separated. As
</p>
<p>a result a decision tree is able to pick out automobile makes quite easily.
</p>
<p>The twoing rule based tree Fig. 20.25 for the same data is different. Instead of
</p>
<p>specifying particular car makes at each node, application of the twoing rule results
</p>
<p>in strategic nodes, i.e. questions which distinguish between different car classes to
</p>
<p>the maximum extent. This feature can be vital when high-dimensional datasets with
</p>
<p>a big number of classes are processed.</p>
<p/>
</div>
<div class="page"><p/>
<p>20.5 Classification and Regression Trees 541
</p>
<p>Optimal Size of a Decision Tree
</p>
<p>Up to now we were interested in determining the best split s� at a particular node.
The next and perhaps more important question is how to determine the optimal tree
</p>
<p>size, i.e. when to stop splitting. If each terminal node has only class homogenous
</p>
<p>dataset, then every point of the learning sample can be flawlessly classified using
</p>
<p>this maximum tree. But can be such an approach fruitful?
</p>
<p>The maximum tree is a case of overspecification. Some criterion is required to
</p>
<p>stop data splitting. Since tree building is dependent on&#129;i.s; t/, a criterion is to stop
</p>
<p>data splitting if
</p>
<p>&#129;i.s; t/ &lt; Ň (20.40)
</p>
<p>where Ň is some threshold value.
The value of Ň is to be chosen in a subjective way and this is unfortunately
</p>
<p>a drawback. Empirical simulations show that the impurity increment is frequently
</p>
<p>non-monotone, that is why even for small Ň the tree may be underparametrised.
Setting even smaller values for Ň will probably remedy the situation but at the cost
of tree overparametrisation.
</p>
<p>Another way to determine the adequate shape of a decision tree is to demand
</p>
<p>a minimum number of observations N (bucked size) at each terminal node. A
</p>
<p>disadvantage is that if at terminal node t the number of observations is higher
</p>
<p>N.t/ &gt; N (20.41)
</p>
<p>then this node is also being split as data are still not supposed to be clustered well
</p>
<p>enough.
</p>
<p>Cross-Validation for Tree Pruning
</p>
<p>Cross-validation is a procedure which uses the bigger data part as a training set and
</p>
<p>the rest as a test set. Then the process is looped so that different parts of the data
</p>
<p>become learning and training set, so that at the end each datapoint was employed
</p>
<p>both as a member of test and learning sets. The aim of this procedure is to extract
</p>
<p>maximum information from the learning sample especially in the situations of data
</p>
<p>scarceness.
</p>
<p>The procedure is implemented in the following way. First, the learning sample
</p>
<p>is randomly divided into V parts. Using the training set from the union of .V � 1/
subsets a decision tree is constructed while the test set is used to verify the tree
</p>
<p>quality. This procedure is looped over all possible subsets.
</p>
<p>Unfortunately for small values of V cross-validation estimates can be unstable
</p>
<p>since each iteration a cluster of data is selected randomly and the number of</p>
<p/>
</div>
<div class="page"><p/>
<p>542 20 Computationally Intensive Techniques
</p>
<p>iterations itself is relatively small, thus the overall estimation result is somewhat
</p>
<p>random. Nowadays cross-validation with V D 10 is an industry standard and for
many applications a good balance between computational complexity and statistical
</p>
<p>precision.
</p>
<p>Cost-Complexity Function and Cross-Validation
</p>
<p>Another method taken into account is tree complexity, i.e. the number of terminal
</p>
<p>nodes. The maximum tree will get a penalty for its big size, on the other hand it
</p>
<p>will be able to make perfect in-sample predictions. Small trees will, of course, get
</p>
<p>lower penalty for their size but their prediction abilities are limited. Optimisation
</p>
<p>procedure based on such a trade-off criterion could determine a good decision tree.
</p>
<p>Define the internal misclassification error of an arbitrary observation at node
</p>
<p>t as e.t/ D 1 � max
j
p.j j t/, define also E.t/ D e.t/p.t/. Then internal
</p>
<p>misclassification tree error is E.T / D
P
t2 QT
</p>
<p>E.t/ where QT is a set of terminal
</p>
<p>nodes. The estimates are called internal because they are based solely on the
</p>
<p>learning sample. It may seem that E.T / as a tree quality measure is sufficient but
</p>
<p>unfortunately it is not so. Consider the case of the maximum tree, here E.TMAX/ D
0, i.e. the tree is of best configuration.
</p>
<p>For any subtree T .� TMAX/ define the number of terminal nodes
ˇ̌ QT
ˇ̌
as a
</p>
<p>measure of its complexity. The following cost-complexity function can be used:
</p>
<p>E˛.T / D E.T /C ˛
ˇ̌ QT
ˇ̌
</p>
<p>(20.42)
</p>
<p>where ˛ � 0 is a complexity parameter and ˛
ˇ̌ QT
ˇ̌
is a cost component. The more
</p>
<p>complex the tree (high number of terminal nodes) the lower is E.T / but at the same
</p>
<p>time the higher is the penalty ˛
ˇ̌ QT
ˇ̌
and vice versa.
</p>
<p>The number of subtrees of TMAX is finite. Hence pruning of TMAX leads to
</p>
<p>creation of a subtree sequence T1; T2; T3; : : : with a decreasing number of terminal
</p>
<p>nodes.
</p>
<p>An important question is if a subtree T � TMAX for a given ˛ minimisingE˛.T /
always exists and whether it is unique?
</p>
<p>In Breiman et al. (1984) it is shown that for 8˛ � 0 there exists an optimal tree
T .˛/ in the sense that
</p>
<p>1. E˛ fT .˛/g D min
T�TMAX
</p>
<p>E˛.T / D min
T�TMAX
</p>
<p>˚
E.T /C ˛
</p>
<p>ˇ̌ QT
ˇ̌�
</p>
<p>2. if E˛.T / D E˛ fT .˛/g then T .˛/ � T .
This result is a proof of existence, but also a proof of uniqueness: consider
</p>
<p>another subtree T 0 so that T and T 0 both minimise E˛ and are not nested, then
T .˛/ does not exist in accordance with second condition.</p>
<p/>
</div>
<div class="page"><p/>
<p>20.5 Classification and Regression Trees 543
</p>
<p>The idea of introducing cost-complexity function at this stage is to check only a
</p>
<p>subset of different subtrees of TMAX: optimal subtrees for different values of ˛. The
</p>
<p>starting point is to define the first optimal subtree in the sequence so that E.T1/ D
E.TMAX/ and the size of T1 is minimum among other subtrees with the same cost
</p>
<p>level. To get T1 out of TMAX for each terminal node of TMAX it is necessary to
</p>
<p>verify the condition E.t/ D E.tL/C E.tR/ and if it is fulfilled&mdash;node t is pruned.
The process is looped until no extra pruning is available&mdash;the resulting tree T .0/
</p>
<p>becomes T1.
</p>
<p>Define a node t as an ancestor of t 0 and t 0 as descendant of t if there is a
connected path down the tree leading from t to t 0. Consider Fig. 20.26 where nodes
t4, t5, t8, t9, t10 and t11 are descendants of t2 while nodes t6 and t7 are not descendants
</p>
<p>of t2 although they are positioned lower since it is not possible to connect them with
</p>
<p>a path from t2 to these nodes without engaging t1. Nodes t4, t2 and t1 are ancestors
</p>
<p>of t9 and t3 is not ancestor of t9.
</p>
<p>Define the branch Tt of the tree T as a subtree based on node t and all its
</p>
<p>descendants. An example is given in Fig. 20.27. Pruning a branch Tt from a tree
</p>
<p>T means deleting all descendant nodes of t . Denote the transformed tree as T � Tt .
Pruning the branch Tt2 results in the tree described in Fig. 20.28.
</p>
<p>For any branch Tt define the internal misclassification estimate as:
</p>
<p>E.Tt/ D
X
</p>
<p>t 02 QTt
</p>
<p>E.t 0/ (20.43)
</p>
<p>where QTt is the set of terminal nodes of Tt . Hence for an arbitrary node t of Tt :
</p>
<p>Node t7
</p>
<p>Node t1
</p>
<p>Node t2 Node t3
</p>
<p>Node t4 Node t5
Node t6
</p>
<p>Node t8 Node t9 Node t10 Node t11
</p>
<p>Fig. 20.26 Decision tree hierarchy</p>
<p/>
</div>
<div class="page"><p/>
<p>544 20 Computationally Intensive Techniques
</p>
<p>Fig. 20.27 The branch Tt2 of
the original tree T
</p>
<p>Node t2
</p>
<p>Node t4 Node t5
</p>
<p>Node t8 Node t9 Node t10 Node t12
</p>
<p>Fig. 20.28 T � Tt2 the
pruned tree T
</p>
<p>Node t1
</p>
<p>Node t3
Node t2
</p>
<p>Node t6 Node t7
</p>
<p>E.t/ &gt; E.Tt/ (20.44)
</p>
<p>Consider now the cost-complexity misclassification estimate for branches or
</p>
<p>single nodes. Define for a single node ftg:
</p>
<p>E .ftg/ D E.t/C ˛ (20.45)
</p>
<p>and for a branch:
</p>
<p>E˛.Tt / D E.Tt/C ˛
ˇ̌ QTt
ˇ̌
</p>
<p>(20.46)
</p>
<p>WhenE˛.Tt / &lt; E˛ .ftg/ the branch Tt is preferred to a single node ftg according
to cost-complexity. For some ˛ both (20.45) and (20.46) will become equal. This
</p>
<p>critical value of ˛ can be determined from:
</p>
<p>E˛.Tt / &lt; E˛ .ftg/ (20.47)
</p>
<p>which is equivalent to
</p>
<p>˛ &lt;
E.t/ � E.Tt/ˇ̌ QTt
</p>
<p>ˇ̌
� 1
</p>
<p>(20.48)
</p>
<p>where ˛ &gt; 0 since E.t/ &gt; E.Tt/</p>
<p/>
</div>
<div class="page"><p/>
<p>20.5 Classification and Regression Trees 545
</p>
<p>To obtain the next member of the subtrees sequence, i.e. T2 out of T1 a special
</p>
<p>node called weak link is determined. For this purpose a function g1.t/, t 2 T1 is
defined as
</p>
<p>g1.t/ D
(
</p>
<p>E.t/�E.Tt /
j QTt j�1 ; t &hellip;
</p>
<p>QT1
C1; t 2 QT1
</p>
<p>(20.49)
</p>
<p>Node Nt1 is a weak link in T1 if
</p>
<p>g1.Nt1/ D min
t2T1
</p>
<p>g1.t/ (20.50)
</p>
<p>and a new value for ˛2 is defined as
</p>
<p>˛2 D g1.Nt1/ (20.51)
</p>
<p>A new tree T2 � T1 in the sequence is obviously defined by pruning the branch
TNt1 , i.e.
</p>
<p>T2 D T1 � TNt1 (20.52)
</p>
<p>The process is looped until root node ft0g&mdash;the final member of sequence&mdash;is
reached.When there are multiple weak links detected, for instance gk.Ntk/ D gk.Nt 0k/,
then both branches are pruned, i.e. TkC1 D Tk � TNtk � TNt 0k .
</p>
<p>In this way it is possible to get the sequence of optimal subtrees TMAX � T1 �
T2 � T3 � � � � � ft0g for which it is possible to prove that the sequence f˛kg is
increasing, i.e. ˛k &lt; ˛kC1, k � 1 and ˛1 D 0. For k � 1: ˛k � ˛ &lt; ˛kC1 and
T .˛/ D T .˛k/ D Tk .
</p>
<p>Practically this tells us how to implement the search algorithm. First, the
</p>
<p>maximum tree TMAX is taken, then T1 is found and a weak link Nt1 is detected and
branch TNt1 is pruned off, ˛2 is calculated and the process is continued.
</p>
<p>When the algorithm is applied to T1, the number of pruned nodes is usually
</p>
<p>quite significant. For instance, consider the following typical empirical evidence
</p>
<p>(see Table 20.6). When the trees become smaller, the difference in the number of
</p>
<p>terminal nodes also gets smaller.
</p>
<p>Finally, it is worth mentioning that the sequence of optimally pruned subtrees
</p>
<p>is a subset of trees which might be constructed using direct method of internal
</p>
<p>misclassification estimator minimisation given a fixed number of terminal nodes.
</p>
<p>Table 20.6 Typical pruning speed
</p>
<p>Tree T1 T2 T3 T4 T5 T6 T7 T8 T9 T10 T11 T12 T13ˇ̌ QTk
ˇ̌
</p>
<p>71 63 58 40 34 19 10 9 7 6 5 2 1</p>
<p/>
</div>
<div class="page"><p/>
<p>546 20 Computationally Intensive Techniques
</p>
<p>Consider an example of tree T .˛/ with 7 terminal nodes, then there is no other
</p>
<p>subtree T with 7 terminal nodes having lower E.T /. Otherwise
</p>
<p>E˛.T / D E.T /C 7˛ &lt; E˛ fT .˛/g D min
T�TMAX
</p>
<p>E˛.T /
</p>
<p>which is impossible by definition.
</p>
<p>Applying the method of V -fold cross-validation to the sequence TMAX � T1 �
T2 � T3 � � � � � ft0g, an optimal tree is determined. On the other hand it is
frequently pointed out that choice of tree with minimum value of ECV.T / is not
</p>
<p>always adequate sinceECV.T / is not too robust, i.e. there is a whole range of values
</p>
<p>ECV.T / satisfying ECV.T / &lt; E
CV
</p>
<p>MIN.T / C " for small " &gt; 0. Moreover, when
V &lt; N a simple change of random generator seed will definitely result in changed
</p>
<p>values of
ˇ̌ QTk
ˇ̌
minimising OE.TK/. Hence a so-called one standard error empirical
</p>
<p>rule is applied which states that if Tk0 is the tree minimising E
CV.Tk0/ from the
</p>
<p>sequence TMAX � T1 � T2 � T3 � � � � � ft0g, then a value k1 and a correspondent
tree Tk1 are selected so that
</p>
<p>argmax
k1
</p>
<p>OE.Tk1/ � OE.Tk0/C �
n
OE.Tk0/
</p>
<p>o
(20.53)
</p>
<p>where �.�/ denotes sample estimate of standard error and OE.�/&mdash;the relevant sample
estimators.
</p>
<p>The dotted line in Fig. 20.29 shows the area where the values of OE.Tk/ only
slightly differ from min
</p>
<p>j QTkj
OE.Tk/. The left edge which is roughly equivalent to 16
</p>
<p>terminal nodes shows the application of one standard error rule. The use of one
</p>
<p>0 8 16 24 32 40
0
</p>
<p>1
</p>
<p>T̃k
</p>
<p>Ê(Tk)
</p>
<p>Fig. 20.29 The example of relationship between OE.Tk/ and number of terminal nodes</p>
<p/>
</div>
<div class="page"><p/>
<p>20.5 Classification and Regression Trees 547
</p>
<p>standard error rule allows not only to achieve more robust results but also to get
</p>
<p>trees of lower complexity given the error comparable with min
j QTkj
OE.Tk/.
</p>
<p>Regression Trees
</p>
<p>Up to now we concentrate on classification trees. Although regression trees
</p>
<p>share a similar logical framework, there are some differences which need to be
</p>
<p>addressed. The important difference between classification and regression trees is
</p>
<p>the type of dependent variable Y . When Y is discrete, a decision tree is called a
</p>
<p>classification tree, a regression tree is a decision tree with a continuous dependent
</p>
<p>variable.
</p>
<p>Gini index and twoing rule discussed in previous sections assume that the
</p>
<p>number of classes is finite and hence introduce some measures based mainly
</p>
<p>on p.j jt/ for arbitrary class j and node t . But since in case of continuous
dependent variable there are no more classes, this approach cannot be used
</p>
<p>anymore unless groups of continuous values are effectively substituted with artificial
</p>
<p>classes. Since there are no classes anymore&mdash;how can be the maximum regression
</p>
<p>tree determined? Analogously with discrete case, absolute homogeneity can be
</p>
<p>then described only after some adequate impurity measure for regression trees is
</p>
<p>introduced.
</p>
<p>Recall the idea ofGini index, then it becomes quite natural to use the variance as
</p>
<p>impurity indicator. Since for each node data variance can be easily computed, then
</p>
<p>splitting criterion for an arbitrary node t can be written as
</p>
<p>s� D argmax
s
</p>
<p>ŒpLvar ftL.s/g C pRvar ftR.s/g&#141; (20.54)
</p>
<p>where tL and tR are emerging child nodes which are, of course, directly dependent
</p>
<p>on the choice of s�.
Hence the maximum regression tree can be easily defined as a structure where
</p>
<p>each node has only the same predicted values. It is important to point out that since
</p>
<p>continuous data have much higher chances to take different values comparing with
</p>
<p>discrete ones, the size of maximum regression tree is usually very big.
</p>
<p>When the maximum regression tree is properly defined, it is then of no problem
</p>
<p>to get an optimally size tree. Like with classification trees, maximum regression tree
</p>
<p>is usually supposed to be upwardly prunedwith the help of cost-complexity function
</p>
<p>and cross-validation. That is why the majority of results presented above is applied
</p>
<p>to regression trees as well.</p>
<p/>
</div>
<div class="page"><p/>
<p>548 20 Computationally Intensive Techniques
</p>
<p>Fig. 20.30 Decision tree for
bankruptcy dataset: Gini
index, NN D 30
MVACARTBan1
</p>
<p>X1 &lt; 0.028
(42,42)
</p>
<p>X2 &lt; 0.641
(13,35)
</p>
<p>CLASS -1
</p>
<p>(7,5)
</p>
<p>X2 &lt; 1.044
(6,30)
</p>
<p>X1 &lt; 0.006
(4,28)
</p>
<p>CLASS 1
</p>
<p>(1,20)
CLASS 1
</p>
<p>(3,8)
</p>
<p>CLASS -1
</p>
<p>(2,2)
</p>
<p>X1 &lt; 0.057
(29,7)
</p>
<p>CLASS -1
</p>
<p>(16,6)
CLASS -1
</p>
<p>(13,1)
</p>
<p>Bankruptcy Analysis
</p>
<p>This section provides a practical study on bankruptcy data involving decision trees.
</p>
<p>A dataset with 84 observations representing different companies is constituted by
</p>
<p>three variables:
</p>
<p>&ndash; net income to total assets ratio
</p>
<p>&ndash; total liabilities to total assets ratio
</p>
<p>&ndash; company status (�1 if bankrupt and 1 if not)
The data is from SEC (2004).
</p>
<p>The goal is to predict and describe the company status given the two primary
</p>
<p>financial ratios. Since no additional information like the functional form of possible
</p>
<p>relationship is available, the use of a classification tree is an active alternative.
</p>
<p>The tree given in Fig. 20.30 was constructed using the Gini index and a NN D
30 constraint, i.e. the number of points in each of the terminal nodes can not be
</p>
<p>more than 30. Numbers in parentheses displayed on terminal nodes are observation
</p>
<p>quantities belonging to Class 1 and Class �1.
If we loose the constraint to NN D 10, the decision rule changes, see Fig. 20.31.
</p>
<p>How exactly did the situation change? Consider the Class 1 terminal nodes of the
</p>
<p>tree on Fig. 20.30. The first one contains 21 observations and thus was split for
NN D 10. When it was split two new nodes of different classes emerged and for both
of them the impurity measure has decreased.
</p>
<p>We may conclude that NN � 10 is a good choice and analysing the tree produced
we can state that for this particular example the net income to total assets .X1/
</p>
<p>ratio appears to be an important class indicator. The successful classification ratio</p>
<p/>
</div>
<div class="page"><p/>
<p>20.5 Classification and Regression Trees 549
</p>
<p>X1 &lt; 0.028
(42,42)
</p>
<p>X2 &lt; 0.641
(13,35)
</p>
<p>X1 &lt; -0.002
(7,5)
</p>
<p>CLASS 1
(2,4)
</p>
<p>CLASS -1
(5,1)
</p>
<p>X2 &lt; 1.044
(6,30)
</p>
<p>X1 &lt; 0.006
(4,28)
</p>
<p>X1 &lt; -0.078
(1,20)
</p>
<p>CLASS 1
(1,6)
</p>
<p>CLASS 1
(0,14)
</p>
<p>X1 &lt; 0.018
(3,8)
</p>
<p>CLASS -1
(2,0)
</p>
<p>CLASS 1
(1,8)
</p>
<p>CLASS -1
(2,2)
</p>
<p>X1 &lt; 0.057
(29,7)
</p>
<p>X1 &lt; 0.056
(16,6)
</p>
<p>X1 &lt; 0.038
(16,5)
</p>
<p>CLASS -1
(8,1)
</p>
<p>X1 &lt; 0.045
(8,4)
</p>
<p>CLASS 1
(1,3)
</p>
<p>CLASS -1
(7,1)
</p>
<p>CLASS 1
(0,1)
</p>
<p>X2 &lt; 0.755
(13,1)
</p>
<p>CLASS -1
(13,0)
</p>
<p>CLASS 1
(0,1)
</p>
<p>Fig. 20.31 Decision tree for bankruptcy dataset: Gini index, NN D 10 MVACARTBan2
</p>
<p>MinSize parameter
</p>
<p>C
la
</p>
<p>s
s
if
ic
</p>
<p>a
ti
o
</p>
<p>n
 R
</p>
<p>a
ti
o
</p>
<p>0 10 20 30 40
</p>
<p>0
.4
</p>
<p>0
.6
</p>
<p>0
.8
</p>
<p>1
</p>
<p>Classification ratio by minsize parameter
</p>
<p>Fig. 20.32 Successful classification ratio dynamic over the number of terminal nodes: cross-
validation MVAbancrupcydis
</p>
<p>dynamic over the number of terminal nodes is shown in Fig. 20.32. It is chosen by
</p>
<p>cross-validation method.
</p>
<p>For this example with relatively small sample size we construct two maximum
</p>
<p>trees&mdash;using the Gini and twoing rules, see Figs. 20.33 and 20.34. Looking at both
</p>
<p>decision trees we see that the choice of impurity measure is not so important as the
</p>
<p>right choice of tree size.</p>
<p/>
</div>
<div class="page"><p/>
<p>550 20 Computationally Intensive Techniques
</p>
<p>X
1
&lt;
</p>
<p>0
.0
</p>
<p>2
8
</p>
<p>(4
2
,4
</p>
<p>2
)
</p>
<p>X
2
&lt;
</p>
<p>0
.6
</p>
<p>4
1
</p>
<p>(1
3
,3
</p>
<p>5
)
</p>
<p>X
1
&lt;
</p>
<p>-0
.0
</p>
<p>0
2
</p>
<p>(7
,5
</p>
<p>)
</p>
<p>X
1
&lt;
</p>
<p>-0
.0
</p>
<p>3
2
</p>
<p>(2
,4
</p>
<p>)
</p>
<p>X
1
&lt;
</p>
<p>-0
.0
</p>
<p>6
9
</p>
<p>(2
,1
</p>
<p>)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>-1
(1
</p>
<p>,0
)
</p>
<p>X
1
&lt;
</p>
<p>-0
.0
</p>
<p>4
3
</p>
<p>(1
,1
</p>
<p>)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>1
(0
</p>
<p>,1
)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>-1
(1
</p>
<p>,0
)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>1
(0
</p>
<p>,3
)
</p>
<p>X
1
&lt;
</p>
<p>0
.0
</p>
<p>2
4
</p>
<p>(5
,1
</p>
<p>)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>-1
(5
</p>
<p>,0
)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>1
(0
</p>
<p>,1
)
</p>
<p>X
2
&lt;
</p>
<p>1
.0
</p>
<p>4
4
</p>
<p>(6
,3
</p>
<p>0
)
</p>
<p>X
1
&lt;
</p>
<p>0
.0
</p>
<p>0
6
</p>
<p>(4
,2
</p>
<p>8
)
</p>
<p>X
1
&lt;
</p>
<p>-0
.0
</p>
<p>7
8
</p>
<p>(1
,2
</p>
<p>0
)
</p>
<p>X
1
&lt;
</p>
<p>-0
.0
</p>
<p>9
2
</p>
<p>(1
,6
</p>
<p>)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>1
(0
</p>
<p>,6
)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>-1
(1
</p>
<p>,0
)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>1
(0
</p>
<p>,1
4
)
</p>
<p>X
1
&lt;
</p>
<p>0
.0
</p>
<p>1
8
</p>
<p>(3
,8
</p>
<p>)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>-1
(2
</p>
<p>,0
)
</p>
<p>X
2
&lt;
</p>
<p>0
.7
</p>
<p>1
0
</p>
<p>(1
,8
</p>
<p>)
</p>
<p>X
1
&lt;
</p>
<p>0
.0
</p>
<p>2
4
</p>
<p>(1
,1
</p>
<p>)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>-1
(1
</p>
<p>,0
)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>1
(0
</p>
<p>,1
)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>1
(0
</p>
<p>,7
)
</p>
<p>X
1
&lt;
</p>
<p>-0
.1
</p>
<p>9
2
</p>
<p>(2
,2
</p>
<p>)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>1
(0
</p>
<p>,2
)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>-1
(2
</p>
<p>,0
)
</p>
<p>X
1
&lt;
</p>
<p>0
.0
</p>
<p>5
7
</p>
<p>(2
9
,7
</p>
<p>)
</p>
<p>X
1
&lt;
</p>
<p>0
.0
</p>
<p>5
6
</p>
<p>(1
6
,6
</p>
<p>)
</p>
<p>X
1
&lt;
</p>
<p>0
.0
</p>
<p>3
8
</p>
<p>(1
6
,5
</p>
<p>)
</p>
<p>X
1
&lt;
</p>
<p>0
.0
</p>
<p>3
0
</p>
<p>(8
,1
</p>
<p>)
</p>
<p>X
1
&lt;
</p>
<p>0
.0
</p>
<p>3
0
</p>
<p>(2
,1
</p>
<p>)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>-1
(2
</p>
<p>,0
)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>1
(0
</p>
<p>,1
)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>-1
(6
</p>
<p>,0
)
</p>
<p>X
1
&lt;
</p>
<p>0
.0
</p>
<p>4
5
</p>
<p>(8
,4
</p>
<p>)
</p>
<p>X
2
&lt;
</p>
<p>0
.3
</p>
<p>8
1
</p>
<p>(1
,3
</p>
<p>)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>-1
(1
</p>
<p>,0
)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>1
(0
</p>
<p>,3
)
</p>
<p>X
2
&lt;
</p>
<p>0
.5
</p>
<p>5
5
</p>
<p>(7
,1
</p>
<p>)
</p>
<p>X
1
&lt;
</p>
<p>0
.0
</p>
<p>4
7
</p>
<p>(2
,1
</p>
<p>)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>-1
(2
</p>
<p>,0
)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>1
(0
</p>
<p>,1
)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>-1
(5
</p>
<p>,0
)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>1
(0
</p>
<p>,1
)
</p>
<p>X
2
&lt;
</p>
<p>0
.7
</p>
<p>5
5
</p>
<p>(1
3
,1
</p>
<p>)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>-1
(1
</p>
<p>3
,0
</p>
<p>)
C
</p>
<p>L
A
</p>
<p>S
S
</p>
<p>1
(0
</p>
<p>,1
)
</p>
<p>F
ig
.
2
0
.3
3
</p>
<p>M
ax
im
</p>
<p>u
m
</p>
<p>tr
ee
</p>
<p>co
n
st
ru
ct
ed
</p>
<p>em
p
lo
y
in
g
G
in
i
in
d
ex
</p>
<p>M
V
A
C
A
R
T
G
i
n
i
T
r
e
e
1</p>
<p/>
</div>
<div class="page"><p/>
<p>20.5 Classification and Regression Trees 551
</p>
<p>X
1
&lt;
</p>
<p>0
.0
</p>
<p>2
8
</p>
<p>(4
2
,4
</p>
<p>2
)
</p>
<p>X
2
&lt;
</p>
<p>0
.6
</p>
<p>4
1
</p>
<p>(1
3
,3
</p>
<p>5
)
</p>
<p>X
1
&lt;
</p>
<p>-0
.0
</p>
<p>0
2
</p>
<p>(7
,5
</p>
<p>)
</p>
<p>X
1
&lt;
</p>
<p>-0
.0
</p>
<p>3
2
</p>
<p>(2
,4
</p>
<p>)
</p>
<p>X
1
&lt;
</p>
<p>-0
.0
</p>
<p>6
9
</p>
<p>(2
,1
</p>
<p>)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>-1
(1
</p>
<p>,0
)
</p>
<p>X
1
&lt;
</p>
<p>-0
.0
</p>
<p>4
3
</p>
<p>(1
,1
</p>
<p>)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>1
(0
</p>
<p>,1
)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>-1
(1
</p>
<p>,0
)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>1
(0
</p>
<p>,3
)
</p>
<p>X
1
&lt;
</p>
<p>0
.0
</p>
<p>2
4
</p>
<p>(5
,1
</p>
<p>)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>-1
(5
</p>
<p>,0
)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>1
(0
</p>
<p>,1
)
</p>
<p>X
1
&lt;
</p>
<p>0
.0
</p>
<p>0
6
</p>
<p>(6
,3
</p>
<p>0
)
</p>
<p>X
1
&lt;
</p>
<p>-0
.0
</p>
<p>7
8
</p>
<p>(2
,2
</p>
<p>2
)
</p>
<p>X
1
&lt;
</p>
<p>-0
.0
</p>
<p>9
2
</p>
<p>(2
,8
</p>
<p>)
</p>
<p>X
2
&lt;
</p>
<p>1
.0
</p>
<p>4
4
</p>
<p>(1
,8
</p>
<p>)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>1
(0
</p>
<p>,6
)
</p>
<p>X
1
&lt;
</p>
<p>-0
.1
</p>
<p>9
2
</p>
<p>(1
,2
</p>
<p>)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>1
(0
</p>
<p>,2
)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>-1
(1
</p>
<p>,0
)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>-1
(1
</p>
<p>,0
)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>1
(0
</p>
<p>,1
4
)
</p>
<p>X
1
&lt;
</p>
<p>0
.0
</p>
<p>1
8
</p>
<p>(4
,8
</p>
<p>)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>-1
(3
</p>
<p>,0
)
</p>
<p>X
2
&lt;
</p>
<p>0
.7
</p>
<p>1
0
</p>
<p>(1
,8
</p>
<p>)
</p>
<p>X
1
&lt;
</p>
<p>0
.0
</p>
<p>2
4
</p>
<p>(1
,1
</p>
<p>)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>-1
(1
</p>
<p>,0
)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>1
(0
</p>
<p>,1
)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>1
(0
</p>
<p>,7
)
</p>
<p>X
1
&lt;
</p>
<p>0
.0
</p>
<p>5
7
</p>
<p>(2
9
,7
</p>
<p>)
</p>
<p>X
1
&lt;
</p>
<p>0
.0
</p>
<p>5
6
</p>
<p>(1
6
,6
</p>
<p>)
</p>
<p>X
1
&lt;
</p>
<p>0
.0
</p>
<p>3
8
</p>
<p>(1
6
,5
</p>
<p>)
</p>
<p>X
1
&lt;
</p>
<p>0
.0
</p>
<p>3
0
</p>
<p>(8
,1
</p>
<p>)
</p>
<p>X
1
&lt;
</p>
<p>0
.0
</p>
<p>3
0
</p>
<p>(2
,1
</p>
<p>)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>-1
(2
</p>
<p>,0
)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>1
(0
</p>
<p>,1
)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>-1
(6
</p>
<p>,0
)
</p>
<p>X
1
&lt;
</p>
<p>0
.0
</p>
<p>4
5
</p>
<p>(8
,4
</p>
<p>)
</p>
<p>X
2
&lt;
</p>
<p>0
.3
</p>
<p>8
1
</p>
<p>(1
,3
</p>
<p>)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>-1
(1
</p>
<p>,0
)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>1
(0
</p>
<p>,3
)
</p>
<p>X
2
&lt;
</p>
<p>0
.5
</p>
<p>5
5
</p>
<p>(7
,1
</p>
<p>)
</p>
<p>X
1
&lt;
</p>
<p>0
.0
</p>
<p>4
7
</p>
<p>(2
,1
</p>
<p>)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>-1
(2
</p>
<p>,0
)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>1
(0
</p>
<p>,1
)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>-1
(5
</p>
<p>,0
)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>1
(0
</p>
<p>,1
)
</p>
<p>X
2
&lt;
</p>
<p>0
.7
</p>
<p>5
5
</p>
<p>(1
3
,1
</p>
<p>)
</p>
<p>C
L
A
</p>
<p>S
S
</p>
<p>-1
(1
</p>
<p>3
,0
</p>
<p>)
C
</p>
<p>L
A
</p>
<p>S
S
</p>
<p>1
(0
</p>
<p>,1
)
</p>
<p>F
ig
.
2
0
.3
4
</p>
<p>M
ax
im
</p>
<p>u
m
</p>
<p>tr
ee
</p>
<p>co
n
st
ru
ct
ed
</p>
<p>em
p
lo
y
in
g
tw
o
in
g
ru
le
</p>
<p>M
V
A
C
A
R
T
T
w
o
i
n
g
T
r
e
e
1</p>
<p/>
</div>
<div class="page"><p/>
<p>552 20 Computationally Intensive Techniques
</p>
<p>Summary
</p>
<p>,! CART is a tree based method splitting the data sequentially into a
binary tree
</p>
<p>,! CART determined the nodes by minimising an impurity measure at
each mode
</p>
<p>,! CART is non-parametric:
When no data structure hypotheses are available, non-parametric
</p>
<p>analysis becomes the single effective data mining tool. CART is a
</p>
<p>flexible nonparametric data mining tool
</p>
<p>,! CART does not require variables to be selected in advance:
From a learning sample CART will automatically select the most
</p>
<p>significant ones
</p>
<p>,! CART is very efficient in computational terms:
Although all possible data splits are analysed, the CART architec-
</p>
<p>ture is flexible enough to do all of them quickly
</p>
<p>,! CART is robust to the effect of outliers:
Due to data-splitting nature of decision rules creation it is possible
</p>
<p>to distinguish between datasets with different characteristics and
</p>
<p>hence to neutralise outliers in separate nodes
</p>
<p>,! CART can use any combination of continuous and categorical data:
Researchers are no longer limited to a particular class of data and
</p>
<p>will be able to capture more real-life examples
</p>
<p>20.6 Boston Housing
</p>
<p>Coming back to the Boston Housing data set, we compare the results of EPP on the
</p>
<p>original data X and the transformed data OX motivated in Sect. 1.9. So we exclude
X4 (indicator of Charles River) from the present analysis.
</p>
<p>The aim of this analysis is to see from a different angle whether our proposed
</p>
<p>transformations yield more normal distributions and whether it will yield data with
</p>
<p>less outliers. Both effects will be visible in our projection pursuit analysis.
</p>
<p>We first apply the Jones and Sibson index to the non-transformed data with 50
</p>
<p>randomly chosen 13-dimensional directions. Figure 20.35 displays the results in the
</p>
<p>following form. In the lower part, we see the values of the Jones and Sibson index.
</p>
<p>It should be constant for 13-dimensional normal data. We observe that this is clearly
</p>
<p>not the case. In the upper part of Fig. 20.35 we show the standard normal density as
</p>
<p>a green curve and two densities corresponding to two extreme index values. The red,
</p>
<p>slim curve corresponds to the maximal value of the index among the 50 projections.
</p>
<p>The blue curve, which is close to the normal, corresponds to the minimal value of</p>
<p/>
</div>
<div class="page"><p/>
<p>20.6 Boston Housing 553
</p>
<p>Fig. 20.35 Projection Pursuit
with the Sibson&ndash;Jones index
with 13 original variables
MVAppsib
</p>
<p>&minus;4 &minus;3 &minus;2 &minus;1 0 1 2 3 4
&minus;0.2
</p>
<p>&minus;0.1
</p>
<p>0
</p>
<p>0.1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>Y
</p>
<p>X
</p>
<p>50 directions
</p>
<p>0 10 20 30 40 50 60
0
</p>
<p>2
</p>
<p>4
</p>
<p>Fig. 20.36 Projection Pursuit
with the Sibson&ndash;Jones index
with 13 transformed variables
MVAppsib
</p>
<p>&minus;4 &minus;3 &minus;2 &minus;1 0 1 2 3 4
&minus;0.2
</p>
<p>&minus;0.1
</p>
<p>0
</p>
<p>0.1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>Y
</p>
<p>X
</p>
<p>50 directions
</p>
<p>0 10 20 30 40 50 60
0
</p>
<p>0.5
</p>
<p>1
</p>
<p>the Jones and Sibson index. The corresponding values of the indices have the same
</p>
<p>colour in the lower part of Fig. 20.35. Below the densities, a jitter plot shows the
</p>
<p>distribution of the projected points ˛&gt;xi (i D 1; : : : ; 506). We conclude from the
outlying projection in the red distribution that several points are in conflict with the
</p>
<p>normality assumption.
</p>
<p>Figure 20.36 presents an analysis with the same design for the transformed data.
</p>
<p>We observe in the lower part of the figure values that are much lower for the Jones
</p>
<p>and Sibson index (by a factor of 10) with lower variability which suggests that the
</p>
<p>transformed data is closer to the normal. (&ldquo;Closeness&rdquo; is interpreted here in the sense
</p>
<p>of the Jones and Sibson index.) This is confirmed by looking to the upper part of
</p>
<p>Fig. 20.36 which has a significantly less outlying structure than in Fig. 20.35.</p>
<p/>
</div>
<div class="page"><p/>
<p>554 20 Computationally Intensive Techniques
</p>
<p>20.7 Exercises
</p>
<p>Exercise 20.1 Calculate the Simplicial Depth for the Swiss bank notes data set and
</p>
<p>compare the results to the univariate medians. Calculate the Simplicial Depth again
</p>
<p>for the genuine and counterfeit bank notes separately.
</p>
<p>Exercise 20.2 Construct a configuration of points in R2 such that xmed;j
from (20.2) is not in the &ldquo;centre&rdquo; of the scatterplot.
</p>
<p>Exercise 20.3 Apply the SIR technique to the US companies data with Y D
market value and X D all other variables. Which directions do you find?
Exercise 20.4 Simulate a data set with X � N4.0; I4/; Y D .X1C 3X2/2C .X3�
X4/
</p>
<p>4 C " and " � N.0; .0:1/2/. Use SIR and SIR II to find the EDR directions.
Exercise 20.5 Apply the Projection Pursuit technique on the Swiss bank notes data
</p>
<p>set and compare the results to the PC analysis and the Fisher discriminant rule.
</p>
<p>Exercise 20.6 Apply the SIR and SIR II technique on the car data set in Table 22.3
</p>
<p>with Y D price.
Exercise 20.7 Generate four regions on the two-dimensional unit square by
</p>
<p>sequentially cutting parallel to the coordinate axes. Generate 100 two-dimensional
</p>
<p>Uniform random variables and label them according to their presence in the above
</p>
<p>regions. Apply the CART algorithm to find the regions bound and to classify the
</p>
<p>observations.
</p>
<p>Exercise 20.8 Modify Exercise 20.7 by defining the regions as lying above and
</p>
<p>below the main diagonal of the unit square. Make a CART analysis and comment
</p>
<p>on the complexity of the tree.
</p>
<p>Exercise 20.9 Apply the SVM with different radial basis parameter r and different
</p>
<p>capacity parameter c in order to separate two circular datasets. This example
</p>
<p>is often called the Orange Peel exercise and involves two Normal distributions
</p>
<p>N.�;&dagger;i/, i D 1; 2, with covariance matrices &dagger;1 D 2I2 and &dagger;2 D 0:5I2.
Exercise 20.10 The noisy spiral data set consists of two intertwining spirals that
</p>
<p>need to be separated by a non-linear classification method. Apply the SVM with
</p>
<p>different radial basis parameter r and capacity parameter c in order to separate the
</p>
<p>two spiral datasets.
</p>
<p>Exercise 20.11 Apply the SVM to separate the bankrupt from the surviving (prof-
</p>
<p>itable) companies using the profitability and leverage ratios given in the Bankruptcy
</p>
<p>data set in Table 22.21.</p>
<p/>
</div>
<div class="page"><p/>
<p>Part IV
</p>
<p>Appendix</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 21
</p>
<p>Symbols and Notations
</p>
<p>Basics
</p>
<p>X; Y Random variables or vectors
</p>
<p>X1; X2; : : : ; Xp Random variables
</p>
<p>X D .X1; : : : ; Xp/&gt; Random vector
X � � X has distribution �
A;B Matrices 53
</p>
<p>&#128;;&#129; Matrices 60
</p>
<p>X ;Y Data matrices 81
</p>
<p>&dagger; Covariance matrix 80
</p>
<p>1n Vector of ones .1; : : : ; 1&bdquo; ƒ&sbquo; &hellip;
n-times
</p>
<p>/&gt; 54
</p>
<p>0n Vector of zeros .0; : : : ; 0&bdquo; ƒ&sbquo; &hellip;
n-times
</p>
<p>/&gt; 54
</p>
<p>I.:/ Indicator function, i.e. for a set M is
I D 1 onM , I D 0 otherwise
</p>
<p>i
p
�1
</p>
<p>) Implication
, Equivalence
� Approximately equal
˝ Kronecker product
iff if and only if, equivalence
</p>
<p>&copy; Springer-Verlag Berlin Heidelberg 2015
W.K. H&auml;rdle, L. Simar, Applied Multivariate Statistical Analysis,
DOI 10.1007/978-3-662-45171-7_21
</p>
<p>557</p>
<p/>
</div>
<div class="page"><p/>
<p>558 21 Symbols and Notations
</p>
<p>Mathematical Abbreviations
</p>
<p>tr.A/ Trace of matrix A
</p>
<p>hull.x1; : : : ; xk/ Convex hull of points fx1; : : : ; xkg
diag.A/ Diagonal of matrixA
</p>
<p>rank.A/ Rank of matrix A
</p>
<p>det.A/ Determinant of matrixA
</p>
<p>C.A/ Column space of matrix A
</p>
<p>Samples
</p>
<p>x; y Observations of X and Y
</p>
<p>x1; : : : ; xn D fxigniD1 Sample of n observations of X
X D fxij giD1;:::;nIjD1;:::;p (n � p) data matrix of observations of X1; : : : ; Xp
</p>
<p>or of X D .X1; : : : ; Xp/T
81
</p>
<p>x.1/; : : : ; x.n/ The order statistic of x1; : : : ; xn 5
</p>
<p>H Centering matrix, H D In � n�11n1&gt;n 90
</p>
<p>Densities and Distribution Functions
</p>
<p>f .x/ Density of X
</p>
<p>f.x; y/ Joint density of X and Y
</p>
<p>fX .x/; fY .y/ Marginal densities of X and Y
</p>
<p>fX1 .x1/; : : : ; fXp .x2/ Marginal densities of X1; : : : ; Xp
Ofh.x/ Histogram or kernel estimator of f .x/ 11
F.x/ Distribution function of X
</p>
<p>F.x; y/ Joint distribution function of X and Y
</p>
<p>FX .x/; FY .y/ Marginal distribution functions of X and Y
</p>
<p>FX1 .x1/; : : : ; fXp .xp/ Marginal distribution functions of X1; : : : ; Xp
</p>
<p>'.x/ Density of the standard normal distribution
</p>
<p>ˆ.x/ Standard normal distribution function
</p>
<p>'X .t / Characteristic function of X
</p>
<p>mk k-th moment of X
</p>
<p>�j Cumulants or semi-invariants of X</p>
<p/>
</div>
<div class="page"><p/>
<p>21 Symbols and Notations 559
</p>
<p>Moments
</p>
<p>EX;EY Mean values of random variables or vectors X and Y 80
</p>
<p>�XY D Cov.X; Y / Covariance between random variables X and Y 80
�XX D Var.X/ Variance of random variable X 80
�XY D
</p>
<p>Cov.X; Y /p
Var.X/ Var.Y /
</p>
<p>Correlation between random variables X and Y 84
</p>
<p>&dagger;XY D Cov.X; Y / Covariance between random vectors X and Y ,
i.e., Cov.X; Y / D E.X �EX/.Y � EY /&gt;
</p>
<p>&dagger;XX D Var.X/ Covariance matrix of the random vector X
</p>
<p>Empirical Moments
</p>
<p>x D 1
n
</p>
<p>nX
</p>
<p>iD1
xi Average of X sampled by fxi giD1;:::;n 7
</p>
<p>sXY D
1
</p>
<p>n
</p>
<p>nX
</p>
<p>iD1
.xi � x/.yi � y/ Empirical covariance of random variables X and Y
</p>
<p>sampled by fxigiD1;:::;n and fyi giD1;:::;n
80
</p>
<p>sXX D
1
</p>
<p>n
</p>
<p>nX
</p>
<p>iD1
.xi � x/2 Empirical variance of random variable X sampled by
</p>
<p>fxigiD1;:::;n
80
</p>
<p>rXY D
sXYp
sXX sY Y
</p>
<p>Empirical correlation of X and Y 84
</p>
<p>S D fsXiXj g D x&gt;Hx Empirical covariance matrix of X1; : : : ; Xp or of the
random vector X D .X1; : : : ; Xp/&gt;
</p>
<p>80, 90
</p>
<p>R D frXiXj g D D�1=2SD�1=2 Empirical correlation matrix of X1; : : : ; Xp or of the
random vector X D .X1; : : : ; Xp/&gt;
</p>
<p>84, 91
</p>
<p>Distributions
</p>
<p>'.x/ Density of the standard normal distribution
</p>
<p>ˆ.x/ Distribution function of the standard normal distribution
</p>
<p>N.0; 1/ Standard normal or Gaussian distribution
</p>
<p>N.�; �2/ Normal distribution with mean � and variance �2
</p>
<p>Np.�;&dagger;/ p-Dimensional normal distribution with mean � and
covariance matrix &dagger;</p>
<p/>
</div>
<div class="page"><p/>
<p>560 21 Symbols and Notations
</p>
<p>L�! Convergence in distribution 143
CLT Central Limit Theorem 143
</p>
<p>�2p �
2 distribution with p degrees of freedom
</p>
<p>�21�˛Ip 1 � ˛ quantile of the �2 distribution with p degrees of
freedom
</p>
<p>tn t -Distribution with n degrees of freedom
</p>
<p>t1�˛=2In 1� ˛=2 quantile of the t -distribution with n d.f.
Fn;m F -Distribution with n and m degrees of freedom
</p>
<p>F1�˛In;m 1�˛ quantile of theF -distribution with n andm degrees
of freedom
</p>
<p>T 2p;n Hotelling T
2-distribution with p and n degrees of
</p>
<p>freedom</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 22
</p>
<p>Data
</p>
<p>All data sets are available on the Springer webpage or at the authors&rsquo; home pages.
</p>
<p>22.1 Boston Housing Data
</p>
<p>The Boston housing data set was collected by Harrison and Rubinfeld (1978). It
</p>
<p>comprise 506 observations for each census district of the Boston metropolitan area.
</p>
<p>The data set was analysed in Belsley, Kuh, and Welsch (1980).
</p>
<p>X1: Per capita crime rate,
</p>
<p>X2: Proportion of residential land zoned for large lots,
</p>
<p>X3: Proportion of nonretail business acres,
</p>
<p>X4: Charles River (1 if tract bounds river, 0 otherwise),
</p>
<p>X5: Nitric oxides concentration,
</p>
<p>X6: Average number of rooms per dwelling,
</p>
<p>X7: Proportion of owner-occupied units built prior to 1940,
</p>
<p>X8: Weighted distances to five Boston employment centers,
</p>
<p>X9: Index of accessibility to radial highways,
</p>
<p>X10: Full-value property tax rate per $10,000,
</p>
<p>X11: Pupil/teacher ratio,
</p>
<p>X12: 1000.B � 0:63/2 I.B &lt; 0:63/ where B is the proportion of African American,
X13: % lower status of the population,
</p>
<p>X14: Median value of owner-occupied homes in $1,000.
</p>
<p>&copy; Springer-Verlag Berlin Heidelberg 2015
W.K. H&auml;rdle, L. Simar, Applied Multivariate Statistical Analysis,
DOI 10.1007/978-3-662-45171-7_22
</p>
<p>561</p>
<p/>
</div>
<div class="page"><p/>
<p>562 22 Data
</p>
<p>22.2 Swiss Bank Notes
</p>
<p>Six variables measured on 100 genuine and 100 counterfeit old Swiss 1000-franc
</p>
<p>bank notes. The data stem from Flury and Riedwyl (1988). The columns correspond
</p>
<p>to the following six variables.
</p>
<p>X1: Length of the bank note,
</p>
<p>X2: Height of the bank note, measured on the left,
</p>
<p>X3: Height of the bank note, measured on the right,
</p>
<p>X4: Distance of inner frame to the lower border,
</p>
<p>X5: Distance of inner frame to the upper border,
</p>
<p>X6: Length of the diagonal.
</p>
<p>Observations 1&ndash;100 are the genuine bank notes and the other 100 observations
</p>
<p>are the counterfeit bank notes.
</p>
<p>22.3 Car Data
</p>
<p>The car data set (Chambers, Cleveland, Kleiner &amp; Tukey, 1983) consists of 13
</p>
<p>variables measured for 74 car types. The abbreviations in this section are as follows:
</p>
<p>X1: P Price,
</p>
<p>X2: M Mileage (in miles per gallon),
</p>
<p>X3: R78 Repair record 1978 (rated on a 5-point scale; 5 best, 1 worst),
</p>
<p>X4: R77 Repair record 1977 (scale as before),
</p>
<p>X5: H Headroom (in inches),
</p>
<p>X6: R Rear seat clearance (distance from front seat back to rear seat, in inches),
</p>
<p>X7: Tr Trunk space (in cubic feet),
</p>
<p>X8: W Weight (in pound),
</p>
<p>X9: L Length (in inches),
</p>
<p>X10: T Turning diameter (clearance required to make a U-turn, in feet),
</p>
<p>X11: D Displacement (in cubic inches),
</p>
<p>X12: G Gear ratio for high gear,
</p>
<p>X13: C Company headquarter (1 for USA, 2 for Japan, 3 for Europe).</p>
<p/>
</div>
<div class="page"><p/>
<p>22.6 French Food Data 563
</p>
<p>22.4 Classic Blue Pullovers Data
</p>
<p>This is a data set consisting of ten measurements of four variables. The story:
</p>
<p>A textile shop manager is studying the sales of &ldquo;classic blue&rdquo; pullovers over ten
</p>
<p>periods. He uses three different marketing methods and hopes to understand his
</p>
<p>sales as a fit of these variables using statistics. The variables measured are
</p>
<p>X1: Numbers of sold pullovers,
</p>
<p>X2: Price (in EUR),
</p>
<p>X3: Advertisement costs in local newspapers (in EUR),
</p>
<p>X4: Presence of a sales assistant (in hours per period).
</p>
<p>22.5 US Companies Data
</p>
<p>The data set consists of measurements for 79 US companies. The abbreviations in
</p>
<p>this section are as follows:
</p>
<p>X1: A Assets (USD),
</p>
<p>X2: S Sales (USD),
</p>
<p>X3: MV Market value (USD),
</p>
<p>X4: P Profits (USD),
</p>
<p>X5: CF Cash flow (USD),
</p>
<p>X6: E Employees.
</p>
<p>22.6 French Food Data
</p>
<p>The data set consists of the average expenditures on food for several different types
</p>
<p>of families in France (manual workersDMA, employeesDEM, managersDCA)
with different numbers of children (2, 3, 4 or 5 children). The data is taken from
</p>
<p>Lebart, Morineau, and F&eacute;nelon (1982).</p>
<p/>
</div>
<div class="page"><p/>
<p>564 22 Data
</p>
<p>22.7 Car Marks
</p>
<p>The data are averaged marks for 23 car types from a sample of 40 persons. The
</p>
<p>marks range from 1 (very good) to 6 (very bad) like German school marks. The
</p>
<p>variables are:
</p>
<p>X1: A Economy,
</p>
<p>X2: B Service,
</p>
<p>X3: C Non-depreciation of value,
</p>
<p>X4: D Price, Mark 1 for very cheap cars,
</p>
<p>X5: E Design,
</p>
<p>X6: F Sporty car,
</p>
<p>X7: G Safety,
</p>
<p>X8: H Easy handling.
</p>
<p>22.8 French Baccalaur&eacute;at Frequencies
</p>
<p>The data consist of observations of 202;100 baccalaur&eacute;ats from France in 1976 and
</p>
<p>give the frequencies for different sets of modalities classified into regions. For a
</p>
<p>reference see Bouroche and Saporta (1980). The variables (modalities) are:
</p>
<p>X1: A Philosophy-Letters,
</p>
<p>X2: B Economics and Social Sciences,
</p>
<p>X3: C Mathematics and Physics,
</p>
<p>X4: D Mathematics and Natural Sciences,
</p>
<p>X5: E Mathematics and Techniques,
</p>
<p>X6: F Industrial Techniques,
</p>
<p>X7: G Economic Techniques,
</p>
<p>X8: H Computer Techniques.
</p>
<p>22.9 Journaux Data
</p>
<p>This is a data set that was created from a survey completed in the 1980s in
</p>
<p>Belgium questioning people&rsquo;s reading habits. They were asked where they live (10
</p>
<p>regions comprised of 7 provinces and 3 regions around Brussels) and what kind
</p>
<p>of newspaper they read on a regular basis. The 15 possible answers belong to 3</p>
<p/>
</div>
<div class="page"><p/>
<p>22.10 US Crime Data 565
</p>
<p>classes: Flemish newspapers (first letter v), French newspapers (first letter f) and
</p>
<p>both languages (first letter b).
</p>
<p>X1: WaBr Walloon Brabant
</p>
<p>X2: Brar Brussels area
</p>
<p>X3: Antw Antwerp
</p>
<p>X4: FlBr Flemish Brabant
</p>
<p>X5: OcFl Occidental Flanders
</p>
<p>X6: OrFl Oriental Flanders
</p>
<p>X7: Hain Hainaut
</p>
<p>X8: Li&egrave;g Li&egrave;ge
</p>
<p>X9: Limb Limburg
</p>
<p>X10: Luxe Luxembourg
</p>
<p>22.10 US Crime Data
</p>
<p>This is a data set consisting of 50 measurements of 7 variables. It states for 1 year
</p>
<p>(1985) the reported number of crimes in the 50 states of the US classified according
</p>
<p>to 7 categories (X3&ndash;X9).
</p>
<p>X1: Land area (land)
</p>
<p>X2: Population 1985 (popu 1985)
</p>
<p>X3: Murder (murd)
</p>
<p>X4: Rape
</p>
<p>X5: Robbery (robb)
</p>
<p>X6: Assault (assa)
</p>
<p>X7: Burglary (burg)
</p>
<p>X8: Larcery (larc)
</p>
<p>X9: Autothieft (auto)
</p>
<p>X10: US states region number (reg)
</p>
<p>X11: US states division number (div)</p>
<p/>
</div>
<div class="page"><p/>
<p>566 22 Data
</p>
<p>Division numbers Region numbers
</p>
<p>New England 1 Northeast 1
</p>
<p>Mid Atlantic 2 Midwest 2
</p>
<p>E N Central 3 South 3
</p>
<p>W N Central 4 West 4
</p>
<p>S Atlantic 5
</p>
<p>E S Central 6
</p>
<p>W S Central 7
</p>
<p>Mountain 8
</p>
<p>Pacific 9
</p>
<p>22.11 Plasma Data
</p>
<p>In Olkin and Veath (1980), the evolution of citrate concentration in the plasma is
</p>
<p>observed at three different times of day, X1 (8 am), X2 (11 am) and X3 (3 pm), for
</p>
<p>two groups of patients. Each group follows a different diet.
</p>
<p>X1: 8 am
</p>
<p>X2: 11 am
</p>
<p>X3: 3 pm
</p>
<p>22.12 WAIS Data
</p>
<p>Morrison (1990) compares the results of four subtests of the Wechsler Adult
</p>
<p>Intelligence Scale (WAIS) for two categories of people: in group one are n1 D 37
people who do not present a senile factor, group two are those (n2 D 12) presenting
a senile factor.
</p>
<p>WAIS subtests:
</p>
<p>X1: Information
</p>
<p>X2: Similarities
</p>
<p>X3: Arithmetic
</p>
<p>X4: Picture completion</p>
<p/>
</div>
<div class="page"><p/>
<p>22.14 Timebudget Data 567
</p>
<p>22.13 ANOVA Data
</p>
<p>The yields of wheat have been measured in 30 parcels which have been randomly
</p>
<p>attributed to 3 lots prepared by one of 3 different fertilisers A, B and C.
</p>
<p>X1: Fertiliser A
</p>
<p>X2: Fertiliser B
</p>
<p>X3: Fertiliser C
</p>
<p>22.14 Timebudget Data
</p>
<p>In Volle (1985), we can find data on 28 individuals identified according to
</p>
<p>sex, country where they live, professional activity and matrimonial status, which
</p>
<p>indicates the amount of time each person spent on ten categories of activities over
</p>
<p>100 days (100 � 24 h D 2;400 h total in each row) in the year 1976.
</p>
<p>X1: prof : Professional activity
</p>
<p>X2: tran : Transportation linked to professional activity
</p>
<p>X3: hous : Household occupation
</p>
<p>X4: kids : Occupation linked to children
</p>
<p>X5: shop : Shopping
</p>
<p>X6: pers : Time spent for personal care
</p>
<p>X7: eat : Eating
</p>
<p>X8: slee : Sleeping
</p>
<p>X9: tele : Watching television
</p>
<p>X10: leis : Other leisures
</p>
<p>maus: Active men in the USA
</p>
<p>waus: Active women in the USA
</p>
<p>wnus: Nonactive women in the USA
</p>
<p>mmus: Married men in USA
</p>
<p>wmus: Married women in USA
</p>
<p>msus: Single men in USA
</p>
<p>wsus: Single women in USA
</p>
<p>mawe: Active men from Western countries
</p>
<p>wawe: Active women from Western countries</p>
<p/>
</div>
<div class="page"><p/>
<p>568 22 Data
</p>
<p>wnwe: Nonactive women from Western countries
</p>
<p>mmwe: Married men from Western countries
</p>
<p>wmwe: Married women from Western countries
</p>
<p>mswe: Single men from Western countries
</p>
<p>wswe: Single women from Western countries
</p>
<p>mayo: Active men from Yugoslavia
</p>
<p>wayo: Active women from Yugoslavia
</p>
<p>wnyo: Nonactive women from Yugoslavia
</p>
<p>mmyo: Married men from Yugoslavia
</p>
<p>wmyo: Married women from Yugoslavia
</p>
<p>msyo: Single men from Yugoslavia
</p>
<p>wsyo: Single women from Yugoslavia
</p>
<p>maes: Active men from Eastern countries
</p>
<p>waes: Active women from Eastern countries
</p>
<p>wnes: Nonactive women from Eastern countries
</p>
<p>mmes: Married men from Eastern countries
</p>
<p>wmes: Married women from Eastern countries
</p>
<p>mses: Single men from Eastern countries
</p>
<p>wses: Single women from Eastern countries
</p>
<p>22.15 Geopol Data
</p>
<p>This data set contains a comparison of 41 countries according to 10 different
</p>
<p>political and economic parameters.
</p>
<p>X1: popu Population
</p>
<p>X2: giph Gross Internal Product per habitant
</p>
<p>X3: ripo Rate of increase of the population
</p>
<p>X4: rupo Rate of urban population
</p>
<p>X5: rlpo Rate of illiteracy in the population
</p>
<p>X6: rspo Rate of students in the population
</p>
<p>X7: eltp Expected lifetime of people
</p>
<p>X8: rnnr Rate of nutritional needs realised
</p>
<p>X9: nunh Number of newspapers and magazines per 1,000 habitants
</p>
<p>X10: nuth Number of television per 1,000 habitants</p>
<p/>
</div>
<div class="page"><p/>
<p>22.16 US Health Data 569
</p>
<p>AFS South Africa DAN Denmark MAR Marocco
</p>
<p>ALG Algeria EGY Egypt MEX Mexico
</p>
<p>BRD Germany ESP Spain NOR Norway
</p>
<p>GBR Great Britain FRA France PER Peru
</p>
<p>ARS Saudi Arabia GAB Gabun POL Poland
</p>
<p>ARG Argentine GRE Greece POR Portugal
</p>
<p>AUS Australia HOK Hong Kong SUE Sweden
</p>
<p>AUT Austria HON Hungary SUI Switzerland
</p>
<p>BEL Belgium IND India THA Tailand
</p>
<p>CAM Cameroon IDO Indonesia URS USSR
</p>
<p>CAN Canada ISR Israel USA USA
</p>
<p>CHL Chile ITA Italia VEN Venezuela
</p>
<p>CHN China JAP Japan YOU Yugoslavia
</p>
<p>CUB Cuba KEN Kenia
</p>
<p>22.16 US Health Data
</p>
<p>This is a data set consisting of 50 measurements of 13 variables. It states for 1 year
</p>
<p>(1985) the reported number of deaths in the 50 states of the US classified according
</p>
<p>to 7 categories.
</p>
<p>X1: Land area (land)
</p>
<p>X2: Population 1985 (popu)
</p>
<p>X3: Accident (acc)
</p>
<p>X4: Cardiovascular (card)
</p>
<p>X5: Cancer (canc)
</p>
<p>X6: Pulmonar (pul)
</p>
<p>X7: Pneumonia flu (pnue)
</p>
<p>X8: Diabetis (diab)
</p>
<p>X9: Liver (liv)
</p>
<p>X10: Doctors (doc)
</p>
<p>X11: Hospitals (hosp)
</p>
<p>X12: US states region number (r)
</p>
<p>X13: US states division number (d)</p>
<p/>
</div>
<div class="page"><p/>
<p>570 22 Data
</p>
<p>Division numbers Region numbers
</p>
<p>New England 1 Northeast 1
</p>
<p>Mid Atlantic 2 Midwest 2
</p>
<p>E N Central 3 South 3
</p>
<p>W N Central 4 West 4
</p>
<p>S Atlantic 5
</p>
<p>E S Central 6
</p>
<p>W S Central 7
</p>
<p>Mountain 8
</p>
<p>Pacific 9
</p>
<p>22.17 Vocabulary Data
</p>
<p>This example of the evolution of the vocabulary of children can be found in Bock
</p>
<p>(1975). Data are drawn from test results on file in the Records Office of the
</p>
<p>Laboratory School of the University of Chicago. They consist of scores, obtained
</p>
<p>from a cohort of pupils from the eighth through eleventh grade levels, on alternative
</p>
<p>forms of the vocabulary section of the Cooperative Reading Test. It provides the
</p>
<p>following scaled scores shown for the sample of 64 subjects (the origin and units
</p>
<p>are fixed arbitrarily).
</p>
<p>22.18 Athletic Records Data
</p>
<p>This data set provides data on Men&rsquo;s athletic records for 55 countries in 1984
</p>
<p>Olympic Games.
</p>
<p>22.19 Unemployment Data
</p>
<p>This data set provides unemployment rates in all federal states of Germany in
</p>
<p>November 2005.
</p>
<p>22.20 Annual Population Data
</p>
<p>The data shows yearly average population rates for Former territory of the Federal
</p>
<p>Republic of Germany incl. Berlin-West (given in 1,000 inhabitants).</p>
<p/>
</div>
<div class="page"><p/>
<p>22.22 Bankruptcy Data II 571
</p>
<p>22.21 Bankruptcy Data I
</p>
<p>The data are the profitability, leverage, and bankruptcy indicators for 84 companies.
</p>
<p>The data set contains information on 42 of the largest companies that filed for
</p>
<p>protection against creditors under Chap. 11 of the US Bankruptcy Code in 2001&ndash;
</p>
<p>2002 after the stock market crash of 2000. The bankrupt companies were matched
</p>
<p>with 42 surviving companies with the closest capitalisations and the same US
</p>
<p>industry classification codes available through the Division of Corporate Finance
</p>
<p>of the Securities and Exchange Commission (SEC, 2004).
</p>
<p>The information for each company was collected from the annual reports for
</p>
<p>1998&ndash;1999 (SEC, 2004), i.e. 3 years prior to the defaults of the bankrupt compa-
</p>
<p>nies. The following data set contains profitability and leverage ratios calculated,
</p>
<p>respectively, as the ratio of net income (NI) and total assets (TA) and the ratio of
</p>
<p>total liabilities (TL) and total assets (TA).
</p>
<p>22.22 Bankruptcy Data II
</p>
<p>Altman (1968), quoted by Morrison (1990), reports financial data on 66 banks.
</p>
<p>X1D (Working capital)/(total assets)
X2D (Retained earnings)/(total assets)
X3D (Earnings before interest and taxes)/(total assets)
X4D (Market value equity)/(book value of total liabilities)
X5D (Sales)/(total assets)
</p>
<p>The first 33 observations correspond to bankrupt banks and the last 33 for solvent
</p>
<p>banks as indicated by the last columns: values of y</p>
<p/>
</div>
<div class="page"><p/>
<p>References
</p>
<p>ALLBUS. (2006). Germany general social survey 1980&ndash;2004.
</p>
<p>Altman, E. (1968). Financial ratios, discriminant analysis and the prediction of corporate
bankruptcy. The Journal of Finance, 23, 589&ndash;609.
</p>
<p>Andrews, D. (1972). Plots of high-dimensional data. Biometrics, 28, 125&ndash;136.
Backhaus, K., Erichson, B., Plinke, W., &amp; Weiber, R. (1996). Multivariate analysemethoden.
</p>
<p>Berlin: Springer.
Bartlett, M. S. (1939). A note on tests of significance in multivariate analysis. Proceedings of the
</p>
<p>Cambridge Philosophical Society, 35, 180&ndash;185.
Bartlett, M. S. (1954). A note on multiplying factors for various chi-squared approximations.
</p>
<p>Journal of the Royal Statistical Society: Series B, 16, 296&ndash;298.
Belsley, D. A., Kuh, E., &amp; Welsch, R. E. (1980). Regression diagnostics. New York: Wiley.
Bl&aelig;sild, P., &amp; Jensen, J. (1981). Multivariate distributions of hyperbolic type. In Statistical
</p>
<p>distributions in scientific work&mdash;Proceedings of the NATO Advanced Study Institute held at
</p>
<p>the Universit&agrave; degli studi di Trieste (Vol. 4, pp. 45&ndash;66).
Bock, R. D. (1975). Multivariate statistical methods in behavioral research. New York: Mc Graw-
</p>
<p>Hill.
Bouroche, J.-M., &amp; Saporta, G. (1980). L&rsquo;analyse des donn&eacute;es. Paris: Presses Universitaires de
</p>
<p>France.
Breiman, L. (1973). Statistics: With a view towards application. Boston: Houghton Mifflin
</p>
<p>Company.
Breiman, L., Friedman, J. H., Olshen, R., &amp; Stone, C. J. (1984). Classification and regression
</p>
<p>trees. Belmont: Wadsworth.
Chambers, J. M., Cleveland, W. S., Kleiner, B., &amp; Tukey, P. A. (1983). Graphical methods for data
</p>
<p>analysis. Boston: Duxbury Press.
Chen, Y., H&auml;rdle, W., &amp; Jeong, S.-O. (2008). Nonparametric risk management with generalized
</p>
<p>hyperbolic distributions. Journal of the American Statistical Association, 103, 910&ndash;923.
Chernoff, H. (1973). Using faces to represent points in k-dimensional space graphically. Journal
</p>
<p>of the American Statistical Association, 68, 361&ndash;368.
Cook, R. D., &amp; Weisberg, S. (1991). Comment on &ldquo;sliced inverse regression for dimension
</p>
<p>reduction&rdquo;. Journal of the American Statistical Association, 86(414), 328&ndash;332.
Dillon, W. R., &amp; Goldstein, M. (1984). Multivariate analysis. New York: Wiley.
Duan, N., &amp; Li, K.-C. (1991). Slicing regression: A link-free regression method. Annals of
</p>
<p>Statistics, 19(2), 505&ndash;530.
Efron, B., Hastie, T., Johnstone, I., &amp; Tibshirani, R. (2004). Least angle regression (with
</p>
<p>discussion). Annals of Statistics, 32(2), 407&ndash;499.
</p>
<p>&copy; Springer-Verlag Berlin Heidelberg 2015
W.K. H&auml;rdle, L. Simar, Applied Multivariate Statistical Analysis,
DOI 10.1007/978-3-662-45171-7
</p>
<p>573</p>
<p/>
</div>
<div class="page"><p/>
<p>574 References
</p>
<p>Embrechts, P., McNeil, A. J., &amp; Straumann, D. (1999, May). Correlation and dependence in risk
management: Properties and pitfalls. RISK Magazine, 69&ndash;71.
</p>
<p>EUNITE. (2001). Electricity load forecast competition of the European network on intelligent
technologies for smart adaptive systems. http://neuron.tuke.sk/competition/
</p>
<p>Everitt, B., &amp; Dunn, G. (1998). Applied multivariate data analysis. London: Edward Arnold.
Fahrmeir, L., &amp; Hamerle, A. (1984). Multivariate statistische verfahren. Berlin: De Gruyter.
Fang, K. T., Kotz, S., &amp; Ng, K. W. (1990). Symmetric multivariate and related distributions.
</p>
<p>London: Chapman and Hall.
Fengler, M. R., H&auml;rdle, W., &amp; Villa, C. (2003). The dynamics of implied volatilities: A common
</p>
<p>principal components approach. Review of Derivative Research, 6, 179&ndash;202.
Flury, B. (1988). Common principle components analysis and related multivariate models. New
</p>
<p>York: Wiley.
Flury, B., &amp; Gautschi, W. (1986). An algorithm for simultaneous orthogonal transformation
</p>
<p>of several positive definite symmetric matrices to nearly diagonal form. SIAM Journal on
Scientific and Statistical Computing, 7, 169&ndash;184.
</p>
<p>Flury, B., &amp; Riedwyl, H. (1988). Multivariate statistics: A practical approach. Cambridge:
Cambridge University Press.
</p>
<p>Franke, J., H&auml;rdle, W., &amp; Hafner, C. (2011). Introduction to statistics of financial markets (3rd
ed.). Heidelberg: Springer.
</p>
<p>Friedman, J. H., Hastie, T., &amp; Tibshirani, R. (2010). Regularization paths for generalized linear
models via coordinate descent. Journal of Statistical Software, 33(1), 1&ndash;22.
</p>
<p>Friedman, J. H., &amp; Stuetzle, W. (1981). Projection pursuit classification. Unpublished manuscript.
Friedman, J. H., &amp; Tukey, J. W. (1974). A projection pursuit algorithm for exploratory data
</p>
<p>analysis. IEEE Transactions on Computers, C 23, 881&ndash;890.
Gale, D., Kuhn, H. W., &amp; Tucker, A. W. (1951). Linear programming and the theory of games. In T.
</p>
<p>C. Koopmans (Ed.), Activity analysis of production and allocation (pp. 317&ndash;329). New York:
John Wiley and Sons.
</p>
<p>Gibbins, R. (1985). Canonical analysis. A review with application in ecology. Berlin: Springer.
Giri, N. C. (1996). Multivariate statistical analysis. New York: Marcel Dekker.
Gosset, W. S. (1908). The probable error of a mean. Biometrika, 6, 1&ndash;25.
Graham, M., &amp; Kennedy, J. (2003). Using curves to enhance parallel coordinate visualisations. In
</p>
<p>Proceedings of seventh international conference on information visualization, 2003 (IV 2003)
</p>
<p>(pp. 10&ndash;16).
Hall, P. (1992). The bootstrap and edgeworth expansion. Statistical Series. New York: Springer.
Hall, P., &amp; Li, K.-C. (1993). On almost linearity of low dimensional projections from high
</p>
<p>dimensional data. Annals of Statistics, 21(2), 867&ndash;889.
H&auml;rdle, W. (1991). Smoothing techniques, with implementations in S. New York: Springer.
H&auml;rdle, W., Hautsch, N., &amp; Overbeck, L. (2009). Applied quantitative finance (2nd ed.).
</p>
<p>Heidelberg: Springer.
H&auml;rdle, W., M&uuml;ller, M., Sperlich, S., &amp; Werwatz, A. (2004). Non- and semiparametric models.
</p>
<p>Heidelberg: Springer.
H&auml;rdle, W., &amp; Scott, D. (1992). Smoothing by weighted averaging of rounded points. Computa-
</p>
<p>tional Statistics, 7, 97&ndash;128.
Harrison, D., &amp; Rubinfeld, D. L. (1978). Hedonic prices and the demand for clean air. Journal of
</p>
<p>Environment Economics &amp; Management, 5, 81&ndash;102.
Hoaglin, W., Mosteller, F., &amp; Tukey, J. (1983). Understanding robust and exploratory data
</p>
<p>analysis. New York: Wiley.
Hodges, J. L., &amp; Lehman, E. L. (1956). The efficiency of some non-parametric competitors of the
t -test. Annals of Mathematical Statistics, 27, 324&ndash;335.
</p>
<p>Hotelling, H. (1935). The most predictable criterion. Journal of Educational Psychology,
26, 139&ndash;142.
</p>
<p>Hotelling, H. (1953). New light on the correlation coefficient and its transform. Journal of the
Royal Statistical Society, Series B, 15, 193&ndash;232.
</p>
<p>Huber, P. (1985). Projection pursuit. Annals of Statistics, 13(2), 435&ndash;475.</p>
<p/>
<div class="annotation"><a href="http://neuron.tuke.sk/competition/">http://neuron.tuke.sk/competition/</a></div>
</div>
<div class="page"><p/>
<p>References 575
</p>
<p>Inselberg, A. (1985). A goodness of fit test for binary regression models based on smoothing
methods. The Visual Computer, 1, 69&ndash;91.
</p>
<p>Johnson, R. A., &amp; Wichern, D. W. (1998). Applied multivariate analysis (4th ed.). Englewood
Cliffs, NJ: Prentice Hall.
</p>
<p>Jones, M. C., &amp; Sibson, R. (1987). What is projection pursuit? (with discussion). Journal of the
Royal Statistical Society, Series A, 150(1), 1&ndash;36.
</p>
<p>Kaiser, H. F. (1985). The varimax criterion for analytic rotation in factor analysis. Psychometrika,
23, 187&ndash;200.
</p>
<p>Kendall, K., &amp; Stuart, S. (1977). Distribution theory. In The advanced theory of statistics (Vol. 1).
London: Griffin.
</p>
<p>Klinke, S., &amp; Polzehl, J. (1995). Implementation of kernel based indices in XGobi. Discussion
paper 47, SFB 373. Humboldt-University of Berlin.
</p>
<p>Kruskal, J. B. (1965). Analysis of factorial experiments by estimating a monotone transformation
of data. Journal of the Royal Statistical Society, Series B, 27, 251&ndash;263.
</p>
<p>Kruskal, J. B. (1969). Toward a practical method which helps uncover the structure of a set of
observations by finding the line tranformation which optimizes a new &ldquo;index of condensation&rdquo;.
In R. C. Milton &amp; J. A. Nelder (Eds.), Statistical computation (pp. 427&ndash;440). New York:
Academic Press.
</p>
<p>Kruskal, J. B. (1972). Linear transformation of multivariate data to reveal clustering. In R. N.
Shepard, A. K. Romney, &amp; S. B. Nerlove (Eds.), Multidimensional scaling: Theory and
applications in the behavioural sciences (Vol. 1, pp. 179&ndash;191). London: Seminar Press.
</p>
<p>Lachenbruch, P. A., &amp; Mickey, M. R. (1968). Estimation of error rates in discriminant analysis.
Technometrics, 10, 1&ndash;11.
</p>
<p>Laplace, P.-S. (1774). M&eacute;moire sur la probabilit&eacute; des causes par les &eacute;v&eacute;nements. Savants &eacute;tranges,
6, 621&ndash;656.
</p>
<p>Lawson, C., &amp; Hansen, R. (1974). Solving least square problems. Englewood Cliffs: Prentice Hall.
Lebart, L., Morineau, A. and F&eacute;nelon, J. P. (1982). Traitement des donn&eacute;es statistiques, Dunod,
</p>
<p>Paris.
Lewin-Koh, N. (2006). Hexagon binnning. Technical Report.
Li, K.-C. (1991). Sliced inverse regression for dimension reduction (with discussion). Journal of
</p>
<p>the American Statistical Association, 86(414), 316&ndash;342.
Li, K.-C. (1992). On principal Hessian directions for data visualization and dimension reduction:
</p>
<p>Another application of Stein&rsquo;s lemma. Journal of the American Statistical Association,
87, 1025&ndash;1039.
</p>
<p>Mardia, K. V., Kent, J. T., &amp; Bibby, J. M. (1979). Multivariate analysis. Duluth/London: Academic
Press.
</p>
<p>Meier, L., van de Geer, S., &amp; B&uuml;hlmann, P. (2008). The group lasso for logistic regression. Journal
of the Royal Statistical Society, Series B, 70, 53&ndash;71.
</p>
<p>Mercer, J. (1909). Functions of positive and negative type and their connection with the theory of
integral equations. Philosophical Transactions of the Royal Society of London, 209, 415&ndash;446.
</p>
<p>Morrison, D. (1990). Multivariate statistical methods (3rd ed., 495 pp.). New York: McGraw-Hill.
Muirhead, R. J. (1982). Aspects of multivariate statistics. New York: Wiley.
Nelsen, R. B. (1999). An introduction to copulas. New York: Springer.
Olkin, I., &amp; Veath, M. (1980). Maximum likelihood estimation in a two-way analysis with
</p>
<p>correlated errors in one classification. Biometrika, 68, 653&ndash;660.
Osborne, M., Presnell, B., &amp; Turlach, B. (2000). On the lasso and its dual. Journal of
</p>
<p>Computational and Graphical Statistics, 9(2), 319&ndash;337.
Parzen, E. (1962). On estimating of a probability density and mode. Annals of Mathematical
</p>
<p>Statistics, 35, 1065&ndash;1076.
Rosenblatt, M. (1956). Remarks on some nonparametric estimates of a density function. Annals of
</p>
<p>Mathematical Statistics, 27, 832&ndash;837.
Schott, J. R. (1994). Determining the dimensionality in sliced inverse regression. Journal of the
</p>
<p>American Statistical Association, 89(425), 141&ndash;148.</p>
<p/>
</div>
<div class="page"><p/>
<p>576 References
</p>
<p>Scott, D. (1985). Averaged shifted histograms: Effective nonparametric density estimation in
several dimensions. Annals of Statistics, 13, 1024&ndash;1040.
</p>
<p>SEC (2004). Securities and exchange commission: Archive of historical documents. http://www.
sec.gov/cgi-bin/srch-edgar
</p>
<p>Shevade, S. K., &amp; Keerthi, S. S. (2003). A simple and efficient algorithm for gene selection using
sparse logistic regression. Bioinformatics, 19, 2246&ndash;2253.
</p>
<p>Silverman, B. W. (1986). Density estimation for statistics and data analysis. Monographs on
statistics and applied probability (Vol. 26). London: Chapman and Hall.
</p>
<p>Simon, N., Friedman, J. H., Hastie, T., &amp; Tibshirani, R. (2013). A sparse-group lasso. Journal of
Computational and Graphical Statistics, 22(2), 231&ndash;245.
</p>
<p>Sklar, A. (1959). Fonctions de r&eacute;partition &agrave; n dimensions et leurs marges. Publications de l&rsquo;Institut
de Statistique de L&rsquo;Universit&eacute; de Paris, 8, 229&ndash;231.
</p>
<p>Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal
Statistical Society, Series B, 58, 267&ndash;288.
</p>
<p>Tufte, E. (1983). The visual display of quantitative information. Cheshire: Graphics Press.
Vapnik, V. (1995). The nature of statistical learning theory. New York: Springer.
Volle, V. (1985). Analyse des Donn&eacute;es. Paris: Economica.
Whittle, P. (1958). On the smoothing of probability density functions. Journal of the Royal
</p>
<p>Statistical Society, Series B, 55, 549&ndash;557.
Yuan, M., &amp; Lin, Y. (2006). Model selection and estimation in regression with grouped variables.
</p>
<p>Journal of the Royal Statistical Society, Series B, 68, 49&ndash;67.
Zou, H., &amp; Hastie, T. (2005). Regularization and variable selection via the elastic net. Journal of
</p>
<p>the Royal Statistical Society, Series B, 67, 301&ndash;320.</p>
<p/>
<div class="annotation"><a href="http://www.sec.gov/cgi-bin/srch-edgar">http://www.sec.gov/cgi-bin/srch-edgar</a></div>
<div class="annotation"><a href="http://www.sec.gov/cgi-bin/srch-edgar">http://www.sec.gov/cgi-bin/srch-edgar</a></div>
</div>
<div class="page"><p/>
<p>Index
</p>
<p>Actual error rate (AER), 417
Admissible, 413
Agglomerative techniques, 393
Allocation rules, 407
Analysis of variance (ANOVA), 72
Andrews&rsquo; Curves, 29
Angle between two vectors, 72
Apparent error rate (APER), 417
</p>
<p>Bayes discriminant rule, 413
Bernoulli distributions, 144
Best line, 308
Binary structure, 387
Binomial sampling, 273
Biplots, 439
Bootstrap, 176
</p>
<p>sample, 179
Bootstrap sample, 179
Boston Housing, 40, 110, 242, 262, 346, 378,
</p>
<p>400, 552
Boxplot, 4, 5
</p>
<p>construction, 7
</p>
<p>Canonical correlation, 443
analysis, 443
coefficient, 445
</p>
<p>variable, 445
vector, 445
</p>
<p>Capital asset pricing model (CAPM), 497
Cauchy distribution, 154
Centering matrix, 90
Central limit theorem (CLT), 143&ndash;145
Centroid, 396
Characteristic functions, 123, 130
</p>
<p>Classic blue pullovers, 82
Cluster algorithms, 392
Cluster analysis, 385
Cobb&ndash;Douglas production function, 254
Cochran theorem, 192
Coefficient of determination, 96, 107
</p>
<p>adjusted, 107
Column space, 74, 306
Common factors, 361
Common principal components analysis
</p>
<p>(CPCA), 342
Communality, 362
Complete linkage, 396
Computationally intensive techniques, 501
Concentration ellipsoid, 139
Conditional approximations, 188
Conditional covariance, 513
Conditional density, 119
Conditional distribution, 186
Conditional expectation, 126, 512, 513
Conditional pdf, 119
Confidence interval, 146
Confusion matrix, 417
Contingency tables, 264, 273, 425
Contrast, 227
Convex hull, 504
Copula, 120, 168
Copulae, 166
Correlation, 84
</p>
<p>multiple, 188
</p>
<p>Correspondence analysis, 425
Covariance, 80
Covariance matrix
</p>
<p>decomposition, 320
properties, 124
</p>
<p>&copy; Springer-Verlag Berlin Heidelberg 2015
W.K. H&auml;rdle, L. Simar, Applied Multivariate Statistical Analysis,
DOI 10.1007/978-3-662-45171-7
</p>
<p>577</p>
<p/>
</div>
<div class="page"><p/>
<p>578 Index
</p>
<p>Cramer&ndash;Rao, 208
Cramer&ndash;Rao lower bound, 206
Cramer&ndash;Wold, 131
Cumulant, 132
Cumulative distribution function (cdf), 118
Curse of dimensionality, 511
</p>
<p>Data depth, 504
Degrees of freedom, 102
Dendrogram, 394
Density estimates, 11
Density functions, 118
Determinant, 56
Deviance, 269
Diagonal matrix, 54
Dice, 388
Discriminant analysis, 407
Discriminant rule, 408
</p>
<p>in practice, 415
Dissimilarity of cars, 458
</p>
<p>Distance
d, 68
Euclidean, 68
iso-distance curves, 69
matrix, 460
measures, 389
</p>
<p>Distribution, 118
Draftman&rsquo;s plot, 21
Duality relations, 313
Duality theorem, 464
</p>
<p>Effective dimension reduction directions, 511,
513
</p>
<p>Effective dimension reduction space, 511
Efficient portfolio, 488
Eigenvalues, 57
Eigenvectors, 57
Elastic net, 297
Elliptical distribution, 196
Elliptically symmetric distribution, 511
Existence of a riskless asset, 492
Expected cost of misclassification, 409
Explained variation, 96
Exploratory projection pursuit, 506
Extremes, 7
</p>
<p>F-spread, 7
F-test, 104
Faces, 23
Factorial
</p>
<p>axis, 308, 309
method, 336
</p>
<p>representation, 314, 317
variable, 308, 316
</p>
<p>Factors, 306
analysis model, 359, 360
model, 367
scores, 376
</p>
<p>Farthest Neighbor, 396
Fisher information, 209
Fisher information matrix, 207, 208
Fisher&rsquo;s linear discrimination function, 418
Five-number summary, 5
Flury faces, 24
Fourths, 6
French food expenditure, 340
Full model, 103
</p>
<p>G-inverse, 57
non-uniqueness, 61
</p>
<p>General multinormal distribution, 193
Gradient, 65
Group-building algorithm, 386
</p>
<p>Heavy-tailed distributions, 149
Hessian, 65
Hexagon, 37
</p>
<p>binning algorithm, 37, 50
plot, 38
</p>
<p>Hierarchical algorithm, 393
Histograms, 11
Hotelling T 2-distribution, 193
Hyperbolic, 151
</p>
<p>Idempotent matrix, 54
Identity matrix, 54
Independence copula, 122
Independent, 85, 119
Inertia, 315, 317
Information matrix, 208
Interpretation of the factors, 363
Interpretation of the principal components, 327
Invariance of scale, 363
Inverse, 56
Inverse regression, 511, 513
</p>
<p>Jaccard, 388
Jacobian, 135
Jordan decomposition, 60, 61
</p>
<p>Kernel
densities, 15
estimator, 15
</p>
<p>Kulczynski, 388</p>
<p/>
</div>
<div class="page"><p/>
<p>Index 579
</p>
<p>Laplace distribution, 154
Lasso, 282
Likelihood function, 202
Likelihood ratio test, 214
Limit theorems, 142
Linear discriminant analysis (LDA), 412
Linear regression, 93
Linear transformation, 91
Link function, 511
Loadings, 361, 362
</p>
<p>non-uniqueness, 364
Log-likelihood function, 202
Log-linear, 264
Logit models, 272
</p>
<p>Mahalanobis distance, 412
Mahalanobis transformation, 93, 137, 138
Marginal densities, 119
Marketing strategies, 101
Maximum likelihood discriminant rule, 408
Maximum likelihood estimator, 202
MDS direction, 459
Mean-variance, 487, 488
Median, 5, 502
Metric methods, 459
Mixture model, 156
Model with interactions, 258
Moments, 123
Multidimentional scaling (MDS), 455
Multinormal, 139, 183
</p>
<p>distribution, 137
Multivariate deneralised hyperbolic
</p>
<p>distribution, 160
Multivariate laplace distribution, 163
Multivariate median, 504
Multivariate t-distribution, 163, 196
</p>
<p>Nearest neighbor, 395
Non-metric solution, 479
Nonexistence of a riskless asset, 491
Nonhomogeneous, 92
Nonmetric methods of MDS, 459
Norm of a vector, 72
Normal distribution, 203
Normal-inverse Gaussian, 151
Normalized principal components (NPCs), 335
Null space, 74
</p>
<p>Odds, 273
</p>
<p>Order statistics, 5
Orthogonal complement, 75
Orthogonal matrix, 54
</p>
<p>Orthonormed, 309
Outliers, 3
Outside bars, 7
</p>
<p>Parallel profiles, 238
Partitioned covariance matrix, 184
Partitioned matrixes, 66
PAV algorithm, 466, 484
Pearson chi-square, 269
Pearson chi-square test for independence, 269
Pool-adjacent violators algorithm, 466, 484
Portfolio
</p>
<p>analysis, 487
choice, 487
</p>
<p>Positive
definite, 62
definiteness, 65
or negative dependence, 22
</p>
<p>semidefinite, 62, 90
Principal
</p>
<p>axes, 70
components, 324
</p>
<p>method, 372
in practice, 324
technique, 324
transformation, 321, 323
</p>
<p>factors, 370
Principal components analysis (PCA), 512,
</p>
<p>515
Profile
</p>
<p>analysis, 238
method, 476
</p>
<p>Projection
matrix, 75
pursuit, 505
pursuit regression, 508
vector, 511
</p>
<p>Proximity between objects, 387
Proximity measure, 386
p-value, 269
</p>
<p>Quadratic discriminant analysis, 414
Quadratic forms, 62
Quadratic response model, 253
Quality of the representations, 339
</p>
<p>Randomized discriminant rule, 413
Rank, 55
Reduced model, 103
Rotations, 73, 374
Row space, 306
Russel and Rao (RR), 388</p>
<p/>
</div>
<div class="page"><p/>
<p>580 Index
</p>
<p>Sampling distributions, 142
Scatterplot matrix, 21
Separation line, 21
Similarity of objects, 387
Simple analysis of variance (ANOVA), 100
Simple Matching, 388, 389
Single linkage, 395
Singular normal distribution, 140
Singular value decomposition (SVD), 61, 313
Sliced inverse regression, 511, 516
</p>
<p>algorithm, 512
Sliced inverse regression II, 513, 514, 516, 517
</p>
<p>algorithm, 514
Solution
</p>
<p>nonmetric, 483
Specific factors, 361
Specific variance, 362
Spectral decompositions, 60
Spherical distribution, 195
Standardized linear combinations (SLC), 320
Statistics, 142
Stimulus, 475
Student&rsquo;s t with n, 152
</p>
<p>Student&rsquo;s t-distribution, 94
Sum of squares, 102
Summary statistics, 89
Support vector machines (SVMs), 519
Swiss bank data, 4
Symmetric matrix, 54
</p>
<p>T-test, 94
Tanimoto, 388
Three-way tables, 267
Total variation, 96
Trace, 56
Trade-off analysis, 476
Transformations, 135
Transpose, 56
Two factor method, 476
</p>
<p>Unbiased estimator, 208
Uncorrelated factors, 361
Unexplained variation, 96
Unit vector, 72
Upper triangular matrix, 54
</p>
<p>Variance explained by PCs, 333
Varimax
</p>
<p>criterion, 374
method, 374
rotation method, 374
</p>
<p>Ward clustering, 396
Wishart
</p>
<p>density, 192
distribution, 191, 192</p>
<p/>
</div>
<ul>	<li>Preface to the Fourth Edition</li>
	<li>Preface to the Third Edition</li>
	<li>Contents</li>
	<li>Part I Descriptive Techniques</li>
<ul>	<li>1 Comparison of Batches</li>
<ul>	<li>1.1 Boxplots</li>
<ul>	<li>Construction of the Boxplot</li>
</ul>
	<li>1.2 Histograms</li>
	<li>1.3 Kernel Densities</li>
	<li>1.4 Scatterplots</li>
	<li>1.5 Chernoff-Flury Faces</li>
	<li>1.6 Andrews' Curves</li>
	<li>1.7 Parallel Coordinates Plots</li>
	<li>1.8 Hexagon Plots</li>
	<li>1.9 Boston Housing</li>
<ul>	<li>Aim of the Analysis</li>
	<li>What Can Be Seen from the PCPs</li>
	<li>The Scatterplot Matrix</li>
<ul>	<li>Per-Capita Crime Rate X1</li>
	<li>Proportion of Residential Area Zoned for Large Lots X2</li>
	<li>Proportion of Non-retail Business Acres X3</li>
	<li>Charles River Dummy Variable X4</li>
	<li>Nitric Oxides Concentration X5</li>
	<li>Average Number of Rooms per Dwelling X6</li>
	<li>Proportion of Owner-Occupied Units Built Prior to 1940 X7</li>
	<li>Weighted Distance to Five Boston Employment Centers X8</li>
	<li>Index of Accessibility to Radial Highways X9</li>
	<li>Full-Value Property Tax X10</li>
	<li>Pupil/Teacher Ratio X11</li>
	<li>Proportion of African-American B, X12 = 1000 (B - 0.63)2 I(B&lt;0.63)</li>
	<li>Proportion of Lower Status of the Population X13</li>
</ul>
	<li>Transformations</li>
</ul>
	<li>1.10 Exercises</li>
</ul>
</ul>
	<li>Part II Multivariate Random Variables</li>
<ul>	<li>2 A Short Excursion into Matrix Algebra</li>
<ul>	<li>2.1 Elementary Operations</li>
<ul>	<li>Matrix Operations</li>
	<li>Properties of Matrix Operations</li>
	<li>Matrix Characteristics</li>
<ul>	<li>Rank</li>
	<li>Trace</li>
	<li>Determinant</li>
	<li>Transpose</li>
	<li>Inverse</li>
	<li>G-Inverse</li>
	<li>Eigenvalues, Eigenvectors</li>
</ul>
	<li>Properties of Matrix Characteristics</li>
</ul>
	<li>2.2 Spectral Decompositions</li>
	<li>2.3 Quadratic Forms</li>
<ul>	<li>Definiteness of Quadratic Forms and Matrices</li>
</ul>
	<li>2.4 Derivatives</li>
	<li>2.5 Partitioned Matrices</li>
	<li>2.6 Geometrical Aspects</li>
<ul>	<li>Distance</li>
	<li>Remark: Usefulness of Theorem 2.7</li>
	<li>Norm of a Vector</li>
	<li>Angle Between Two Vectors</li>
	<li>Rotations</li>
	<li>Column Space and Null Space of a Matrix</li>
	<li>Projection Matrix</li>
	<li>Projection on C(X)</li>
</ul>
	<li>2.7 Exercises</li>
</ul>
	<li>3 Moving to Higher Dimensions</li>
<ul>	<li>3.1 Covariance</li>
	<li>3.2 Correlation</li>
	<li>3.3 Summary Statistics</li>
<ul>	<li>Linear Transformation</li>
	<li>Mahalanobis Transformation</li>
</ul>
	<li>3.4 Linear Model for Two Variables</li>
	<li>3.5 Simple Analysis of Variance</li>
<ul>	<li>The F-Test in a Linear Regression Model</li>
</ul>
	<li>3.6 Multiple Linear Model</li>
<ul>	<li>Properties of </li>
	<li>The ANOVA Model in Matrix Notation</li>
</ul>
	<li>3.7 Boston Housing</li>
	<li>3.8 Exercises</li>
</ul>
	<li>4 Multivariate Distributions</li>
<ul>	<li>4.1 Distribution and Density Function</li>
	<li>4.2 Moments and Characteristic Functions</li>
<ul>	<li>Moments: Expectation and Covariance Matrix</li>
	<li>Properties of the Covariance Matrix =Var(X)</li>
	<li>Properties of Variances and Covariances</li>
	<li>Conditional Expectations</li>
	<li>Properties of Conditional Expectations</li>
	<li>Characteristic Functions</li>
	<li>Cumulant Functions</li>
</ul>
	<li>4.3 Transformations</li>
	<li>4.4 The Multinormal Distribution</li>
<ul>	<li>Geometry of the Np(μ, ) Distribution</li>
	<li>Singular Normal Distribution</li>
	<li>Gaussian Copula</li>
</ul>
	<li>4.5 Sampling Distributions and Limit Theorems</li>
<ul>	<li>Transformation of Statistics</li>
</ul>
	<li>4.6 Heavy-Tailed Distributions</li>
<ul>	<li>Generalised Hyperbolic Distribution</li>
	<li>Student's t-Distribution</li>
	<li>Laplace Distribution</li>
	<li>Cauchy Distribution</li>
	<li>Mixture Model</li>
	<li>Multivariate Generalised Hyperbolic Distribution</li>
	<li>Multivariate t-Distribution</li>
	<li>Multivariate Laplace Distribution</li>
	<li>Multivariate Mixture Model</li>
	<li>Generalised Hyperbolic Distribution</li>
</ul>
	<li>4.7 Copulae</li>
	<li>4.8 Bootstrap</li>
	<li>4.9 Exercises</li>
</ul>
	<li>5 Theory of the Multinormal</li>
<ul>	<li>5.1 Elementary Properties of the Multinormal</li>
<ul>	<li>Conditional Approximations</li>
</ul>
	<li>5.2 The Wishart Distribution</li>
	<li>5.3 Hotelling's T2-Distribution</li>
	<li>5.4 Spherical and Elliptical Distributions</li>
	<li>5.5 Exercises</li>
</ul>
	<li>6 Theory of Estimation</li>
<ul>	<li>6.1 The Likelihood Function</li>
	<li>6.2 The Cramer&ndash;Rao Lower Bound</li>
	<li>6.3 Exercises</li>
</ul>
	<li>7 Hypothesis Testing</li>
<ul>	<li>7.1 Likelihood Ratio Test</li>
<ul>	<li>Confidence Region for μ</li>
</ul>
	<li>7.2 Linear Hypothesis</li>
<ul>	<li>Repeated Measurements</li>
	<li>Comparison of Two Mean Vectors</li>
	<li>Profile Analysis</li>
<ul>	<li>Parallel Profiles</li>
	<li>Equality of Two Levels</li>
	<li>Treatment Effect</li>
</ul>
</ul>
	<li>7.3 Boston Housing</li>
<ul>	<li>Testing Linear Restrictions</li>
</ul>
	<li>7.4 Exercises</li>
</ul>
</ul>
	<li>Part III Multivariate Techniques</li>
<ul>	<li>8 Regression Models</li>
<ul>	<li>8.1 General ANOVA and ANCOVA Models</li>
<ul>	<li>8.1.1 ANOVA Models</li>
<ul>	<li>One-Factor Models</li>
	<li>Multiple-Factors Models</li>
</ul>
	<li>8.1.2 ANCOVA Models</li>
	<li>8.1.3 Boston Housing</li>
</ul>
	<li>8.2 Categorical Responses</li>
<ul>	<li>8.2.1 Multinomial Sampling and Contingency Tables</li>
	<li>8.2.2 Log-Linear Models for Contingency Tables</li>
<ul>	<li>Two-Way Tables</li>
	<li>Three-Way Tables</li>
</ul>
	<li>8.2.3 Testing Issues with Count Data</li>
	<li>8.2.4 Logit Models</li>
<ul>	<li>Logit Models for Binary Response</li>
	<li>Logit Models for Contingency Tables</li>
</ul>
</ul>
	<li>8.3 Exercises</li>
</ul>
	<li>9 Variable Selection </li>
<ul>	<li>9.1 Lasso</li>
<ul>	<li>9.1.1 Lasso in the Linear Regression Model</li>
<ul>	<li>Geometrical Aspects in R2</li>
	<li>The LAR Algorithm and Lasso Solution Paths</li>
	<li>Orthonormal Design Case</li>
	<li>General Lasso Solution</li>
</ul>
	<li>9.1.2 Lasso in High Dimensions</li>
	<li>9.1.3 Lasso in Logit Model</li>
</ul>
	<li>9.2 Elastic Net</li>
<ul>	<li>9.2.1 Elastic Net in Linear Regression Model</li>
	<li>9.2.2 Elastic Net in Logit Model</li>
</ul>
	<li>9.3 Group Lasso</li>
	<li>9.4 Exercises</li>
</ul>
	<li>10 Decomposition of Data Matrices by Factors </li>
<ul>	<li>10.1 The Geometric Point of View</li>
	<li>10.2 Fitting the p-Dimensional Point Cloud</li>
<ul>	<li>Subspaces of Dimension 1</li>
	<li>Representation of the Cloud on  F1 </li>
	<li>Subspaces of Dimension 2</li>
	<li>Subspaces of Dimension q (q&le;p)</li>
</ul>
	<li>10.3 Fitting the n-Dimensional Point Cloud</li>
<ul>	<li>Subspaces of Dimension 1</li>
	<li>Representation of the Cloud on G1</li>
	<li>Subspaces of Dimension q (q&le;n)</li>
</ul>
	<li>10.4 Relations Between Subspaces</li>
	<li>10.5 Practical Computation</li>
	<li>10.6 Exercises</li>
</ul>
	<li>11 Principal Components Analysis </li>
<ul>	<li>11.1 Standardised Linear Combination</li>
	<li>11.2 Principal Components in Practice</li>
	<li>11.3 Interpretation of the PCs</li>
	<li>11.4 Asymptotic Properties of the PCs</li>
<ul>	<li>Variance Explained by the First q PCs</li>
</ul>
	<li>11.5 Normalised Principal Components Analysis</li>
	<li>11.6 Principal Components as a Factorial Method</li>
<ul>	<li>Quality of the Representations</li>
</ul>
	<li>11.7 Common Principal Components</li>
	<li>11.8 Boston Housing</li>
	<li>11.9 More Examples</li>
	<li>11.10 Exercises</li>
</ul>
	<li>12 Factor Analysis </li>
<ul>	<li>12.1 The Orthogonal Factor Model</li>
<ul>	<li>Interpretation of the Factors</li>
	<li>Invariance of Scale</li>
	<li>Non-uniqueness of Factor Loadings</li>
</ul>
	<li>12.2 Estimation of the Factor Model</li>
<ul>	<li>The Maximum Likelihood Method</li>
<ul>	<li>Likelihood Ratio Test for the Number of Common Factors</li>
</ul>
	<li>The Method of Principal Factors</li>
	<li>The Principal Component Method</li>
	<li>Rotation</li>
</ul>
	<li>12.3 Factor Scores and Strategies</li>
<ul>	<li>Practical Suggestions</li>
	<li>Factor Analysis Versus PCA</li>
</ul>
	<li>12.4 Boston Housing</li>
	<li>12.5 Exercises</li>
</ul>
	<li>13 Cluster Analysis </li>
<ul>	<li>13.1 The Problem</li>
	<li>13.2 The Proximity Between Objects</li>
<ul>	<li>Similarity of Objects with Binary Structure</li>
	<li>Distance Measures for Continuous Variables</li>
</ul>
	<li>13.3 Cluster Algorithms</li>
<ul>	<li>Hierarchical Algorithms, Agglomerative Techniques</li>
</ul>
	<li>13.4 Boston Housing</li>
	<li>13.5 Exercises</li>
</ul>
	<li>14 Discriminant Analysis </li>
<ul>	<li>14.1 Allocation Rules for Known Distributions</li>
<ul>	<li>Maximum Likelihood Discriminant Rule</li>
	<li>Bayes Discriminant Rule</li>
	<li>Probability of Misclassification for the ML Rule (J=2)</li>
	<li>Classification with Different Covariance Matrices</li>
</ul>
	<li>14.2 Discrimination Rules in Practice</li>
<ul>	<li>Estimation of the Probabilities of Misclassifications</li>
	<li>Fisher's Linear Discrimination Function</li>
</ul>
	<li>14.3 Boston Housing</li>
	<li>14.4 Exercises</li>
</ul>
	<li>15 Correspondence Analysis </li>
<ul>	<li>15.1 Motivation</li>
	<li>15.2 Chi-Square Decomposition</li>
	<li>15.3 Correspondence Analysis in Practice</li>
<ul>	<li>Biplots</li>
</ul>
	<li>15.4 Exercises</li>
</ul>
	<li>16 Canonical Correlation Analysis</li>
<ul>	<li>16.1 Most Interesting Linear Combination</li>
	<li>16.2 Canonical Correlation in Practice</li>
<ul>	<li>Testing the Canonical Correlation Coefficients</li>
	<li>CCA with Qualitative Data</li>
</ul>
	<li>16.3 Exercises</li>
</ul>
	<li>17 Multidimensional Scaling </li>
<ul>	<li>17.1 The Problem</li>
	<li>17.2 Metric MDS</li>
<ul>	<li>The Classical Solution</li>
<ul>	<li>Recovery of Coordinates</li>
	<li>How Many Dimensions?</li>
	<li>Similarities</li>
	<li>Relation to Factorial Analysis</li>
	<li>Optimality Properties of the Classical MDS Solution</li>
</ul>
</ul>
	<li>17.3 Nonmetric MDS</li>
<ul>	<li>Shepard&ndash;Kruskal Algorithm</li>
</ul>
	<li>17.4 Exercises</li>
</ul>
	<li>18 Conjoint Measurement Analysis</li>
<ul>	<li>18.1 Introduction</li>
	<li>18.2 Design of Data Generation</li>
	<li>18.3 Estimation of Preference Orderings</li>
<ul>	<li>Metric Solution</li>
	<li>Nonmetric Solution</li>
</ul>
	<li>18.4 Exercises</li>
</ul>
	<li>19 Applications in Finance</li>
<ul>	<li>19.1 Portfolio Choice</li>
	<li>19.2 Efficient Portfolio</li>
<ul>	<li>Nonexistence of a Riskless Asset</li>
	<li>Existence of a Riskless Asset</li>
</ul>
	<li>19.3 Efficient Portfolios in Practice</li>
	<li>19.4 The Capital Asset Pricing Model</li>
	<li>19.5 Exercises</li>
</ul>
	<li>20 Computationally Intensive Techniques</li>
<ul>	<li>20.1 Simplicial Depth</li>
	<li>20.2 Projection Pursuit</li>
<ul>	<li>Exploratory Projection Pursuit</li>
	<li>Projection Pursuit Regression</li>
</ul>
	<li>20.3 Sliced Inverse Regression</li>
<ul>	<li>The SIR Algorithm</li>
	<li>SIR II</li>
	<li>The SIR II Algorithm</li>
</ul>
	<li>20.4 Support Vector Machines</li>
<ul>	<li>Classification Methodology</li>
	<li>Expected vs. Empirical Risk Minimisation</li>
	<li>The SVM in the Linearly Separable Case</li>
	<li>SVMs in the Linearly Non-separable Case</li>
	<li>Non-linear Classification</li>
	<li>SVMs for Simulated Data</li>
	<li>Solution of the SVM Classification Problem</li>
	<li>Scoring Companies</li>
</ul>
	<li>20.5 Classification and Regression Trees</li>
<ul>	<li>How Does CART Work?</li>
	<li>Impurity Measures</li>
	<li>Gini Index and Twoing Rule in Practice</li>
	<li>Optimal Size of a Decision Tree</li>
	<li>Cross-Validation for Tree Pruning</li>
	<li>Cost-Complexity Function and Cross-Validation</li>
	<li>Regression Trees</li>
	<li>Bankruptcy Analysis</li>
</ul>
	<li>20.6 Boston Housing</li>
	<li>20.7 Exercises</li>
</ul>
</ul>
	<li>Part IV Appendix</li>
<ul>	<li>21 Symbols and Notations</li>
<ul>	<li>Basics</li>
	<li>Mathematical Abbreviations</li>
	<li>Samples</li>
	<li>Densities and Distribution Functions</li>
	<li>Moments</li>
	<li>Empirical Moments</li>
	<li>Distributions</li>
</ul>
	<li>22 Data</li>
<ul>	<li>22.1 Boston Housing Data</li>
	<li>22.2 Swiss Bank Notes</li>
	<li>22.3 Car Data</li>
	<li>22.4 Classic Blue Pullovers Data</li>
	<li>22.5 US Companies Data</li>
	<li>22.6 French Food Data</li>
	<li>22.7 Car Marks</li>
	<li>22.8 French Baccalaur&eacute;at Frequencies</li>
	<li>22.9 Journaux Data</li>
	<li>22.10 US Crime Data</li>
	<li>22.11 Plasma Data</li>
	<li>22.12 WAIS Data</li>
	<li>22.13 ANOVA Data</li>
	<li>22.14 Timebudget Data</li>
	<li>22.15 Geopol Data</li>
	<li>22.16 US Health Data</li>
	<li>22.17 Vocabulary Data</li>
	<li>22.18 Athletic Records Data</li>
	<li>22.19 Unemployment Data</li>
	<li>22.20 Annual Population Data</li>
	<li>22.21 Bankruptcy Data I</li>
	<li>22.22 Bankruptcy Data II</li>
</ul>
</ul>
	<li>References</li>
	<li>Index</li>
</ul>
</body></html>