<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Untitled</title>
</head>
<body><div class="page"><p/>
<p>Linear and 
Nonlinear 
Programming 
</p>
<p>International Series in
Operations Research &amp; Management Science
</p>
<p>David G. Luenberger 
Yinyu Ye
</p>
<p>Fourth Edition</p>
<p/>
</div>
<div class="page"><p/>
<p>International Series in Operations
</p>
<p>Research &amp; Management Science
</p>
<p>Volume 228
</p>
<p>Series Editor
</p>
<p>Camille C. Price
</p>
<p>Stephen F. Austin State University, TX, USA
</p>
<p>Associate Series Editor
</p>
<p>Joe Zhu
</p>
<p>Worcester Polytechnic Institute, MA, USA
</p>
<p>Founding Series Editor
</p>
<p>Frederick S. Hillier
</p>
<p>Stanford University, CA, USA
</p>
<p>More information about this series at http://www.springer.com/series/6161</p>
<p/>
<div class="annotation"><a href="http://www.springer.com/series/6161">http://www.springer.com/series/6161</a></div>
</div>
<div class="page"><p/>
</div>
<div class="page"><p/>
<p>David G. Luenberger &bull; Yinyu Ye
</p>
<p>Linear and Nonlinear
Programming
</p>
<p>Fourth Edition
</p>
<p>123</p>
<p/>
</div>
<div class="page"><p/>
<p>David G. Luenberger
Department of Management Science
</p>
<p>and Engineering
Stanford University
Stanford, CA, USA
</p>
<p>Yinyu Ye
Department of Management Science
</p>
<p>and Engineering
Stanford University
Stanford, CA, USA
</p>
<p>ISSN 0884-8289 ISSN 2214-7934 (electronic)
International Series in Operations Research &amp; Management Science
ISBN 978-3-319-18841-6 ISBN 978-3-319-18842-3 (eBook)
DOI 10.1007/978-3-319-18842-3
</p>
<p>Library of Congress Control Number: 2015942692
</p>
<p>Springer Cham Heidelberg New York Dordrecht London
&copy; Springer International Publishing Switzerland 1973, 1984 (2003 reprint), 2008, 2016
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of
the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microfilms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology
now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a specific statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this book
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or
the editors give a warranty, express or implied, with respect to the material contained herein or for any
errors or omissions that may have been made.
</p>
<p>Printed on acid-free paper
</p>
<p>Springer International Publishing AG Switzerland is part of Springer Science+Business Media (www.
springer.com)</p>
<p/>
<div class="annotation"><a href="www.springer.com">www.springer.com</a></div>
<div class="annotation"><a href="www.springer.com">www.springer.com</a></div>
</div>
<div class="page"><p/>
<p>To Susan, Robert, Jill, and Jenna;
</p>
<p>Daisun, Fei, Tim, and Kaylee</p>
<p/>
</div>
<div class="page"><p/>
</div>
<div class="page"><p/>
<p>Preface
</p>
<p>This book is intended as a text covering the central concepts of practical optimiza-
</p>
<p>tion techniques. It is designed for either self-study by professionals or classroom
</p>
<p>work at the undergraduate or graduate level for students who have a technical back-
</p>
<p>ground in engineering, mathematics, or science. Like the field of optimization itself,
</p>
<p>which involves many classical disciplines, the book should be useful to system ana-
</p>
<p>lysts, operations researchers, numerical analysts, management scientists, and other
</p>
<p>specialists from the host of disciplines from which practical optimization appli-
</p>
<p>cations are drawn. The prerequisites for convenient use of the book are relatively
</p>
<p>modest; the prime requirement being some familiarity with introductory elements
</p>
<p>of linear algebra. Certain sections and developments do assume some knowledge
</p>
<p>of more advanced concepts of linear algebra, such as eigenvector analysis, or some
</p>
<p>background in sets of real numbers, but the text is structured so that the mainstream
</p>
<p>of the development can be faithfully pursued without reliance on this more advanced
</p>
<p>background material.
</p>
<p>Although the book covers primarily material that is now fairly standard, this edi-
</p>
<p>tion emphasizes methods that are both state-of-the-art and popular. One major in-
</p>
<p>sight is the connection between the purely analytical character of an optimization
</p>
<p>problem, expressed perhaps by properties of the necessary conditions, and the be-
</p>
<p>havior of algorithms used to solve a problem. This was a major theme of the first
</p>
<p>edition of this book and the fourth edition expands and further illustrates this rela-
</p>
<p>tionship.
</p>
<p>As in the earlier editions, the material in this fourth edition is organized into three
</p>
<p>separate parts. Part I is a self-contained introduction to linear programming, a key
</p>
<p>component of optimization theory. The presentation in this part is fairly conven-
</p>
<p>tional, covering the main elements of the underlying theory of linear programming,
</p>
<p>many of the most effective numerical algorithms, and many of its important special
</p>
<p>applications. Part II, which is independent of Part I, covers the theory of uncon-
</p>
<p>strained optimization, including both derivations of the appropriate optimality con-
</p>
<p>ditions and an introduction to basic algorithms. This part of the book explores the
</p>
<p>general properties of algorithms and defines various notions of convergence. Part III
</p>
<p>vii</p>
<p/>
</div>
<div class="page"><p/>
<p>viii Preface
</p>
<p>extends the concepts developed in the second part to constrained optimization
</p>
<p>problems. Except for a few isolated sections, this part is also independent of Part I.
</p>
<p>It is possible to go directly into Parts II and III omitting Part I, and, in fact, the
</p>
<p>book has been used in this way in many universities. Each part of the book contains
</p>
<p>enough material to form the basis of a one-quarter course. In either classroom use
</p>
<p>or for self-study, it is important not to overlook the suggested exercises at the end of
</p>
<p>each chapter. The selections generally include exercises of a computational variety
</p>
<p>designed to test one&rsquo;s understanding of a particular algorithm, a theoretical variety
</p>
<p>designed to test one&rsquo;s understanding of a given theoretical development, or of the
</p>
<p>variety that extends the presentation of the chapter to new applications or theoretical
</p>
<p>areas. One should attempt at least four or five exercises from each chapter. In pro-
</p>
<p>gressing through the book it would be unusual to read straight through from cover
</p>
<p>to cover. Generally, one will wish to skip around. In order to facilitate this mode, we
</p>
<p>have indicated sections of a specialized or digressive nature with an asterisk&lowast;.
New to this edition is a special Chap. 6 devoted to Conic Linear Programming, a
</p>
<p>powerful generalization of Linear Programming. While the constraint set in a nor-
</p>
<p>mal linear program is defined by a finite number of linear inequalities of finite-
</p>
<p>dimensional vector variables, the constraint set in conic linear programming may be
</p>
<p>defined, for example, as a linear combination of symmetric positive semi-definite
</p>
<p>matrices of a given dimension. Indeed, many conic structures are possible and use-
</p>
<p>ful in a variety of applications. It must be recognized, however, that conic linear
</p>
<p>programming is an advanced topic, requiring special study.
</p>
<p>Another important topic is an accelerated steepest descent method that exhibits
</p>
<p>superior convergence properties, and for this reason, has become quite popular. The
</p>
<p>proof of the convergence property for both standard and accelerated steepest descent
</p>
<p>methods are presented in Chap. 8.
</p>
<p>As the field of optimization advances, addressing greater complexity, treating
</p>
<p>problems with ever more variables (as in Big Data situations), ranging over diverse
</p>
<p>applications. The field responds yo these challenges, developing new algorithms,
</p>
<p>building effective software, and expanding overall theory. An example of a valu-
</p>
<p>able new development is the work on big data problems. Surprisingly, coordinate
</p>
<p>descent, with randomly selected coordinates at each step, is quite effective as ex-
</p>
<p>plained in Chap. 8. As another example some problems are formulated so that the
</p>
<p>unknowns can be split into two sub groups, there are linear constraints and the objec-
</p>
<p>tive function is separable with respect to the two groups of variables. The augmented
</p>
<p>Lagrangian can be computed and it is natural to use an alternating series method.
</p>
<p>We discuss the alternating direction method with multipliers as a dual method in
</p>
<p>Chap. 14. Interestingly, this method is convergent for when the number of partition
</p>
<p>groups is two, but not for finer partitions.
</p>
<p>We wish to thank the many students and researchers who over the years have
</p>
<p>given us comments concerning the book and those who encouraged us to carry out
</p>
<p>this revision.
</p>
<p>Stanford, CA, USA D.G. Luenberger
</p>
<p>Stanford, CA, USA Y. Ye
</p>
<p>January 2015</p>
<p/>
</div>
<div class="page"><p/>
<p>Contents
</p>
<p>1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
</p>
<p>1.1 Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
</p>
<p>1.2 Types of Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
</p>
<p>1.3 Size of Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
</p>
<p>1.4 Iterative Algorithms and Convergence . . . . . . . . . . . . . . . . . . . . . . . . 6
</p>
<p>Part I Linear Programming
</p>
<p>2 Basic Properties of Linear Programs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
</p>
<p>2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
</p>
<p>2.2 Examples of Linear Programming Problems . . . . . . . . . . . . . . . . . . . 14
</p>
<p>2.3 Basic Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
</p>
<p>2.4 The Fundamental Theorem of Linear Programming . . . . . . . . . . . . . 20
</p>
<p>2.5 Relations to Convexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
</p>
<p>2.6 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
</p>
<p>3 The Simplex Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
</p>
<p>3.1 Pivots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
</p>
<p>3.2 Adjacent Extreme Points . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
</p>
<p>3.3 Determining a Minimum Feasible Solution . . . . . . . . . . . . . . . . . . . . 42
</p>
<p>3.4 Computational Procedure: Simplex Method . . . . . . . . . . . . . . . . . . . 45
</p>
<p>3.5 Finding a Basic Feasible Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
</p>
<p>3.6 Matrix Form of the Simplex Method . . . . . . . . . . . . . . . . . . . . . . . . . 54
</p>
<p>3.7 Simplex Method for Transportation Problems . . . . . . . . . . . . . . . . . . 56
</p>
<p>3.8 Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
</p>
<p>3.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
</p>
<p>3.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
</p>
<p>4 Duality and Complementarity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
</p>
<p>4.1 Dual Linear Programs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
</p>
<p>4.2 The Duality Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86
</p>
<p>ix</p>
<p/>
</div>
<div class="page"><p/>
<p>x Contents
</p>
<p>4.3 Relations to the Simplex Procedure . . . . . . . . . . . . . . . . . . . . . . . . . . 88
</p>
<p>4.4 Sensitivity and Complementary Slackness . . . . . . . . . . . . . . . . . . . . . 92
</p>
<p>4.5 Max Flow&ndash;Min Cut Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94
</p>
<p>4.6 The Dual Simplex Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
</p>
<p>4.7 &lowast;The Primal-Dual Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
4.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
</p>
<p>4.9 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
</p>
<p>5 Interior-Point Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
</p>
<p>5.1 Elements of Complexity Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
</p>
<p>5.2 &lowast;The Simplex Method Is Not Polynomial-Time . . . . . . . . . . . . . . . . 118
5.3 &lowast;The Ellipsoid Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
5.4 The Analytic Center . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
</p>
<p>5.5 The Central Path . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
</p>
<p>5.6 Solution Strategies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
</p>
<p>5.7 Termination and Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
</p>
<p>5.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142
</p>
<p>5.9 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
</p>
<p>6 Conic Linear Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
</p>
<p>6.1 Convex Cones . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
</p>
<p>6.2 Conic Linear Programming Problem . . . . . . . . . . . . . . . . . . . . . . . . . 150
</p>
<p>6.3 Farkas&rsquo; Lemma for Conic Linear Programming . . . . . . . . . . . . . . . . 154
</p>
<p>6.4 Conic Linear Programming Duality . . . . . . . . . . . . . . . . . . . . . . . . . . 158
</p>
<p>6.5 Complementarity and Solution Rank of SDP . . . . . . . . . . . . . . . . . . 166
</p>
<p>6.6 Interior-Point Algorithms for Conic Linear Programming . . . . . . . . 170
</p>
<p>6.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
</p>
<p>6.8 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174
</p>
<p>Part II Unconstrained Problems
</p>
<p>7 Basic Properties of Solutions and Algorithms . . . . . . . . . . . . . . . . . . . . . . 179
</p>
<p>7.1 First-Order Necessary Conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180
</p>
<p>7.2 Examples of Unconstrained Problems . . . . . . . . . . . . . . . . . . . . . . . . 182
</p>
<p>7.3 Second-Order Conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185
</p>
<p>7.4 Convex and Concave Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188
</p>
<p>7.5 Minimization and Maximization of Convex Functions . . . . . . . . . . . 192
</p>
<p>7.6 &lowast;Zero-Order Conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
7.7 Global Convergence of Descent Algorithms . . . . . . . . . . . . . . . . . . . 196
</p>
<p>7.8 Speed of Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204
</p>
<p>7.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209
</p>
<p>7.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209</p>
<p/>
</div>
<div class="page"><p/>
<p>Contents xi
</p>
<p>8 Basic Descent Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213
</p>
<p>8.1 Line Search Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214
</p>
<p>8.2 The Method of Steepest Descent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229
</p>
<p>8.3 Applications of the Convergence Theory . . . . . . . . . . . . . . . . . . . . . . 239
</p>
<p>8.4 Accelerated Steepest Descent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243
</p>
<p>8.5 Newton&rsquo;s Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
</p>
<p>8.6 Coordinate Descent Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 252
</p>
<p>8.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257
</p>
<p>8.8 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258
</p>
<p>9 Conjugate Direction Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263
</p>
<p>9.1 Conjugate Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263
</p>
<p>9.2 Descent Properties of the Conjugate Direction Method . . . . . . . . . . 266
</p>
<p>9.3 The Conjugate Gradient Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268
</p>
<p>9.4 The C&ndash;G Method as an Optimal Process . . . . . . . . . . . . . . . . . . . . . . 270
</p>
<p>9.5 The Partial Conjugate Gradient Method . . . . . . . . . . . . . . . . . . . . . . . 273
</p>
<p>9.6 Extension to Nonquadratic Problems . . . . . . . . . . . . . . . . . . . . . . . . . 276
</p>
<p>9.7 &lowast;Parallel Tangents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279
9.8 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281
</p>
<p>10 Quasi-Newton Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
</p>
<p>10.1 Modified Newton Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286
</p>
<p>10.2 Construction of the Inverse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288
</p>
<p>10.3 Davidon-Fletcher-Powell Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290
</p>
<p>10.4 The Broyden Family . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293
</p>
<p>10.5 Convergence Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296
</p>
<p>10.6 Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 300
</p>
<p>10.7 Memoryless Quasi-Newton Methods . . . . . . . . . . . . . . . . . . . . . . . . . 304
</p>
<p>10.8 &lowast;Combination of Steepest Descent and Newton&rsquo;s Method . . . . . . . . 306
10.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312
</p>
<p>10.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313
</p>
<p>Part III Constrained Minimization
</p>
<p>11 Constrained Minimization Conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321
</p>
<p>11.1 Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321
</p>
<p>11.2 Tangent Plane . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323
</p>
<p>11.3 First-Order Necessary Conditions (Equality Constraints) . . . . . . . . 326
</p>
<p>11.4 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327
</p>
<p>11.5 Second-Order Conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 333
</p>
<p>11.6 Eigenvalues in Tangent Subspace . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335
</p>
<p>11.7 Sensitivity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 338
</p>
<p>11.8 Inequality Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 340
</p>
<p>11.9 Zero-Order Conditions and Lagrangian Relaxation . . . . . . . . . . . . . 344
</p>
<p>11.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351
</p>
<p>11.11 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 352</p>
<p/>
</div>
<div class="page"><p/>
<p>xii Contents
</p>
<p>12 Primal Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357
</p>
<p>12.1 Advantage of Primal Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357
</p>
<p>12.2 Feasible Direction Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 358
</p>
<p>12.3 Active Set Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 360
</p>
<p>12.4 The Gradient Projection Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 364
</p>
<p>12.5 Convergence Rate of the Gradient Projection Method . . . . . . . . . . . 370
</p>
<p>12.6 The Reduced Gradient Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 378
</p>
<p>12.7 Convergence Rate of the Reduced Gradient Method . . . . . . . . . . . . 383
</p>
<p>12.8 &lowast;Variations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 390
12.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 392
</p>
<p>12.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 392
</p>
<p>13 Penalty and Barrier Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 397
</p>
<p>13.1 Penalty Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 398
</p>
<p>13.2 Barrier Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 401
</p>
<p>13.3 Properties of Penalty and Barrier Functions . . . . . . . . . . . . . . . . . . . 403
</p>
<p>13.4 Newton&rsquo;s Method and Penalty Functions . . . . . . . . . . . . . . . . . . . . . . 412
</p>
<p>13.5 Conjugate Gradients and Penalty Methods . . . . . . . . . . . . . . . . . . . . 413
</p>
<p>13.6 Normalization of Penalty Functions . . . . . . . . . . . . . . . . . . . . . . . . . . 415
</p>
<p>13.7 Penalty Functions and Gradient Projection . . . . . . . . . . . . . . . . . . . . 417
</p>
<p>13.8 &lowast;Exact Penalty Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 421
13.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 423
</p>
<p>13.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 425
</p>
<p>14 Duality and Dual Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 429
</p>
<p>14.1 Global Duality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 430
</p>
<p>14.2 Local Duality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 435
</p>
<p>14.3 Canonical Convergence Rate of Dual Steepest Ascent . . . . . . . . . . . 440
</p>
<p>14.4 Separable Problems and Their Duals . . . . . . . . . . . . . . . . . . . . . . . . . 441
</p>
<p>14.5 Augmented Lagrangian . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 445
</p>
<p>14.6 The Method of Multipliers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 449
</p>
<p>14.7 The Alternating Direction Method of Multipliers . . . . . . . . . . . . . . . 454
</p>
<p>14.8 &lowast;Cutting Plane Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 458
14.9 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 464
</p>
<p>15 Primal-Dual Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 467
</p>
<p>15.1 The Standard Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 467
</p>
<p>15.2 A Simple Merit Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 470
</p>
<p>15.3 Basic Primal-Dual Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 471
</p>
<p>15.4 Modified Newton Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 477
</p>
<p>15.5 Descent Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 478
</p>
<p>15.6 &lowast;Rate of Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 483
15.7 Primal-Dual Interior Point Methods . . . . . . . . . . . . . . . . . . . . . . . . . . 485
</p>
<p>15.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 488
</p>
<p>15.9 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 489</p>
<p/>
</div>
<div class="page"><p/>
<p>Contents xiii
</p>
<p>A Mathematical Review . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 495
</p>
<p>A.1 Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 495
</p>
<p>A.2 Matrix Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 496
</p>
<p>A.3 Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 497
</p>
<p>A.4 Eigenvalues and Quadratic Forms . . . . . . . . . . . . . . . . . . . . . . . . . . . . 498
</p>
<p>A.5 Topological Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 499
</p>
<p>A.6 Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 500
</p>
<p>B Convex Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 505
</p>
<p>B.1 Basic Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 505
</p>
<p>B.2 Hyperplanes and Polytopes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 507
</p>
<p>B.3 Separating and Supporting Hyperplanes . . . . . . . . . . . . . . . . . . . . . . 509
</p>
<p>B.4 Extreme Points . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 511
</p>
<p>C Gaussian Elimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 513
</p>
<p>D Basic Network Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 517
</p>
<p>D.1 Flows in Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 519
</p>
<p>D.2 Tree Procedure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 519
</p>
<p>D.3 Capacitated Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 521
</p>
<p>Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 523
</p>
<p>Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 539</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 1
</p>
<p>Introduction
</p>
<p>1.1 Optimization
</p>
<p>The concept of optimization is now well rooted as a principle underlying the analysis
</p>
<p>of many complex decision or allocation problems. It offers a certain degree of philo-
</p>
<p>sophical elegance that is hard to dispute, and it often offers an indispensable degree
</p>
<p>of operational simplicity. Using this optimization philosophy, one approaches a
</p>
<p>complex decision problem, involving the selection of values for a number of in-
</p>
<p>terrelated variables, by focusing attention on a single objective designed to quantify
</p>
<p>performance and measure the quality of the decision. This one objective is maxi-
</p>
<p>mized (or minimized, depending on the formulation) subject to the constraints that
</p>
<p>may limit the selection of decision variable values. If a suitable single aspect of a
</p>
<p>problem can be isolated and characterized by an objective, be it profit or loss in
</p>
<p>a business setting, speed or distance in a physical problem, expected return in the
</p>
<p>environment of risky investments, or social welfare in the context of government
</p>
<p>planning, optimization may provide a suitable framework for analysis.
</p>
<p>It is, of course, a rare situation in which it is possible to fully represent all the
</p>
<p>complexities of variable interactions, constraints, and appropriate objectives when
</p>
<p>faced with a complex decision problem. Thus, as with all quantitative techniques
</p>
<p>of analysis, a particular optimization formulation should be regarded only as an
</p>
<p>approximation. Skill in modeling, to capture the essential elements of a problem,
</p>
<p>and good judgment in the interpretation of results are required to obtain meaningful
</p>
<p>conclusions. Optimization, then, should be regarded as a tool of conceptualization
</p>
<p>and analysis rather than as a principle yielding the philosophically correct solution.
</p>
<p>Skill and good judgment, with respect to problem formulation and interpretation
</p>
<p>of results, is enhanced through concrete practical experience and a thorough under-
</p>
<p>standing of relevant theory. Problem formulation itself always involves a tradeoff
</p>
<p>between the conflicting objectives of building a mathematical model sufficiently
</p>
<p>complex to accurately capture the problem description and building a model that is
</p>
<p>&copy; Springer International Publishing Switzerland 2016
</p>
<p>D.G. Luenberger, Y. Ye, Linear and Nonlinear Programming, International
Series in Operations Research &amp; Management Science 228,
DOI 10.1007/978-3-319-18842-3 1
</p>
<p>1</p>
<p/>
</div>
<div class="page"><p/>
<p>2 1 Introduction
</p>
<p>tractable. The expert model builder is facile with both aspects of this tradeoff. One
</p>
<p>aspiring to become such an expert must learn to identify and capture the important
</p>
<p>issues of a problem mainly through example and experience; one must learn to
</p>
<p>distinguish tractable models from nontractable ones through a study of available
</p>
<p>technique and theory and by nurturing the capability to extend existing theory to
</p>
<p>new situations.
</p>
<p>This book is centered around a certain optimization structure&mdash;that characteristic
</p>
<p>of linear and nonlinear programming. Examples of situations leading to this struc-
</p>
<p>ture are sprinkled throughout the book, and these examples should help to indicate
</p>
<p>how practical problems can be often fruitfully structured in this form. The book
</p>
<p>mainly, however, is concerned with the development, analysis, and comparison of
</p>
<p>algorithms for solving general subclasses of optimization problems. This is valuable
</p>
<p>not only for the algorithms themselves, which enable one to solve given problems,
</p>
<p>but also because identification of the collection of structures they most effectively
</p>
<p>solve can enhance one&rsquo;s ability to formulate problems.
</p>
<p>1.2 Types of Problems
</p>
<p>The content of this book is divided into three major parts: Linear Programming,
</p>
<p>Unconstrained Problems, and Constrained Problems. The last two parts together
</p>
<p>comprise the subject of nonlinear programming.
</p>
<p>Linear Programming
</p>
<p>Linear programming is without doubt the most natural mechanism for formulat-
</p>
<p>ing a vast array of problems with modest effort. A linear programming problem
</p>
<p>is characterized, as the name implies, by linear functions of the unknowns; the
</p>
<p>objective is linear in the unknowns, and the constraints are linear equalities or linear
</p>
<p>inequalities in the unknowns. One familiar with other branches of linear mathe-
</p>
<p>matics might suspect, initially, that linear programming formulations are popular
</p>
<p>because the mathematics is nicer, the theory is richer, and the computation simpler
</p>
<p>for linear problems than for nonlinear ones. But, in fact, these are not the primary
</p>
<p>reasons. In terms of mathematical and computational properties, there are much
</p>
<p>broader classes of optimization problems than linear programming problems that
</p>
<p>have elegant and potent theories and for which effective algorithms are available.
</p>
<p>It seems that the popularity of linear programming lies primarily with the formu-
</p>
<p>lation phase of analysis rather than the solution phase&mdash;and for good cause. For
</p>
<p>one thing, a great number of constraints and objectives that arise in practice are
</p>
<p>indisputably linear. Thus, for example, if one formulates a problem with a budget
</p>
<p>constraint restricting the total amount of money to be allocated among two different
</p>
<p>commodities, the budget constraint takes the form x1 + x2 &le; B, where x j, i = 1, 2,</p>
<p/>
</div>
<div class="page"><p/>
<p>1.2 Types of Problems 3
</p>
<p>is the amount allocated to activity i, and B is the budget. Similarly, if the objective
</p>
<p>is, for example, maximum weight, then it can be expressed as w1x1 + w2x2, where
</p>
<p>w j, i = 1, 2, is the unit weight of the commodity i. The overall problem would
</p>
<p>be expressed as
</p>
<p>maximize w1x1 + w2x2
</p>
<p>subject to x1 + x2 &le; B,
x1 &ge; 0, x2 &ge; 0,
</p>
<p>which is an elementary linear program. The linearity of the budget constraint is
</p>
<p>extremely natural in this case and does not represent simply an approximation to a
</p>
<p>more general functional form.
</p>
<p>Another reason that linear forms for constraints and objectives are so popular in
</p>
<p>problem formulation is that they are often the least difficult to define. Thus, even if
</p>
<p>an objective function is not purely linear by virtue of its inherent definition (as in
</p>
<p>the above example), it is often far easier to define it as being linear than to decide on
</p>
<p>some other functional form and convince others that the more complex form is the
</p>
<p>best possible choice. Linearity, therefore, by virtue of its simplicity, often is selected
</p>
<p>as the easy way out or, when seeking generality, as the only functional form that will
</p>
<p>be equally applicable (or nonapplicable) in a class of similar problems.
</p>
<p>Of course, the theoretical and computational aspects do take on a somewhat spe-
</p>
<p>cial character for linear programming problems&mdash;the most significant development
</p>
<p>being the simplex method. This algorithm is developed in Chaps. 2 and 3. More re-
</p>
<p>cent interior point methods are nonlinear in character and these are developed in
</p>
<p>Chap. 5.
</p>
<p>Unconstrained Problems
</p>
<p>It may seem that unconstrained optimization problems are so devoid of structural
</p>
<p>properties as to preclude their applicability as useful models of meaningful problems.
</p>
<p>Quite the contrary is true for two reasons. First, it can be argued, quite convincingly,
</p>
<p>that if the scope of a problem is broadened to the consideration of all relevant de-
</p>
<p>cision variables, there may then be no constraints&mdash;or put another way, constraints
</p>
<p>represent artificial delimitations of scope, and when the scope is broadened the con-
</p>
<p>straints vanish. Thus, for example, it may be argued that a budget constraint is not
</p>
<p>characteristic of a meaningful problem formulation; since by borrowing at some
</p>
<p>interest rate it is always possible to obtain additional funds, and hence rather than
</p>
<p>introducing a budget constraint, a term reflecting the cost of funds should be incor-
</p>
<p>porated into the objective. A similar argument applies to constraints describing the
</p>
<p>availability of other resources which at some cost (however great) could be supple-
</p>
<p>mented.
</p>
<p>The second reason that many important problems can be regarded as hav-
</p>
<p>ing no constraints is that constrained problems are sometimes easily converted to</p>
<p/>
</div>
<div class="page"><p/>
<p>4 1 Introduction
</p>
<p>unconstrained problems. For instance, the sole effect of equality constraints is sim-
</p>
<p>ply to limit the degrees of freedom, by essentially making some variables functions
</p>
<p>of others. These dependencies can sometimes be explicitly characterized, and a new
</p>
<p>problem having its number of variables equal to the true degree of freedom can be
</p>
<p>determined. As a simple specific example, a constraint of the form x1 + x2 = B can
</p>
<p>be eliminated by substituting x2 = B &minus; x1 everywhere else that x2 appears in the
problem.
</p>
<p>Aside from representing a significant class of practical problems, the study of un-
</p>
<p>constrained problems, of course, provides a stepping stone toward the more general
</p>
<p>case of constrained problems. Many aspects of both theory and algorithms are most
</p>
<p>naturally motivated and verified for the unconstrained case before progressing to the
</p>
<p>constrained case.
</p>
<p>Constrained Problems
</p>
<p>In spite of the arguments given above, many problems met in practice are formulated
</p>
<p>as constrained problems. This is because in most instances a complex problem such
</p>
<p>as, for example, the detailed production policy of a giant corporation, the planning
</p>
<p>of a large government agency, or even the design of a complex device cannot be
</p>
<p>directly treated in its entirety accounting for all possible choices, but instead must be
</p>
<p>decomposed into separate subproblems&mdash;each subproblem having constraints that
</p>
<p>are imposed to restrict its scope. Thus, in a planning problem, budget constraints are
</p>
<p>commonly imposed in order to decouple that one problem from a more global one.
</p>
<p>Therefore, one frequently encounters general nonlinear constrained mathematical
</p>
<p>programming problems.
</p>
<p>The general mathematical programming problem can be stated as
</p>
<p>minimize f (x)
</p>
<p>subject to h j(x) = 0, i = 1, 2, . . . , m
</p>
<p>g j(x) &le; 0, j = 1, 2, p
x &isin; S .
</p>
<p>In this formulation, x is an n-dimensional vector of unknowns, x = (x1, x2, . . . ,
</p>
<p>xn), and f , hi, i = 1, 2, . . . , m, and g j, j = 1, 2, . . . , p, are real-valued functions of
</p>
<p>the variables x1, x2, . . . , xn. The set S is a subset of n-dimensional space. The func-
</p>
<p>tion f is the objective function of the problem and the equations, inequalities, and
</p>
<p>set restrictions are constraints.
</p>
<p>Generally, in this book, additional assumptions are introduced in order to make
</p>
<p>the problem smooth in some suitable sense. For example, the functions in the prob-
</p>
<p>lem are usually required to be continuous, or perhaps to have continuous derivatives.
</p>
<p>This ensures that small changes in x lead to small changes in other values associ-
</p>
<p>ated with the problem. Also, the set S is not allowed to be arbitrary but usually is
</p>
<p>required to be a connected region of n-dimensional space, rather than, for example,</p>
<p/>
</div>
<div class="page"><p/>
<p>1.3 Size of Problems 5
</p>
<p>a set of distinct isolated points. This ensures that small changes in x can be made.
</p>
<p>Indeed, in a majority of problems treated, the set S is taken to be the entire space;
</p>
<p>there is no set restriction.
</p>
<p>In view of these smoothness assumptions, one might characterize the problems
</p>
<p>treated in this book as continuous variable programming, since we generally discuss
</p>
<p>problems where all variables and function values can be varied continuously. In fact,
</p>
<p>this assumption forms the basis of many of the algorithms discussed, which operate
</p>
<p>essentially by making a series of small movements in the unknown x vector.
</p>
<p>1.3 Size of Problems
</p>
<p>One obvious measure of the complexity of a programming problem is its size,
</p>
<p>measured in terms of the number of unknown variables or the number of constraints.
</p>
<p>As might be expected, the size of problems that can be effectively solved has been
</p>
<p>increasing with advancing computing technology and with advancing theory. Today,
</p>
<p>with present computing capabilities, however, it is reasonable to distinguish three
</p>
<p>classes of problems: small-scale problems having about five or fewer unknowns
</p>
<p>and constraints; intermediate-scale problems having from about five to a hundred
</p>
<p>or a thousand variables; and large-scale problems having perhaps thousands or even
</p>
<p>millions of variables and constraints. This classification is not entirely rigid, but
</p>
<p>it reflects at least roughly not only size but the basic differences in approach that
</p>
<p>accompany different size problems. As a rough rule, small-scale problems can be
</p>
<p>solved by hand or by a small computer. Intermediate-scale problems can be solved
</p>
<p>on a personal computer with general purpose mathematical programming codes.
</p>
<p>Large-scale problems require sophisticated codes that exploit special structure and
</p>
<p>usually require large computers.
</p>
<p>Much of the basic theory associated with optimization, particularly in non-
</p>
<p>linear programming, is directed at obtaining necessary and sufficient conditions
</p>
<p>satisfied by a solution point, rather than at questions of computation. This theory
</p>
<p>involves mainly the study of Lagrange multipliers, including the Karush-Kuhn-
</p>
<p>Tucker Theorem and its extensions. It tremendously enhances insight into the phi-
</p>
<p>losophy of constrained optimization and provides satisfactory basic foundations for
</p>
<p>other important disciplines, such as the theory of the firm, consumer economics,
</p>
<p>and optimal control theory. The interpretation of Lagrange multipliers that accom-
</p>
<p>panies this theory is valuable in virtually every optimization setting. As a basis for
</p>
<p>computing numerical solutions to optimization, however, this theory is far from ade-
</p>
<p>quate, since it does not consider the difficulties associated with solving the equations
</p>
<p>resulting from the necessary conditions.
</p>
<p>If it is acknowledged from the outset that a given problem is too large and too
</p>
<p>complex to be efficiently solved by hand (and hence it is acknowledged that a
</p>
<p>computer solution is desirable), then one&rsquo;s theory should be directed toward devel-
</p>
<p>opment of procedures that exploit the efficiencies of computers. In most cases this</p>
<p/>
</div>
<div class="page"><p/>
<p>6 1 Introduction
</p>
<p>leads to the abandonment of the idea of solving the set of necessary conditions in
</p>
<p>favor of the more direct procedure of searching through the space (in an intelligent
</p>
<p>manner) for ever-improving points.
</p>
<p>Today, search techniques can be effectively applied to more or less general non-
</p>
<p>linear programming problems. Problems of great size, large-scale programming
</p>
<p>problems, can be solved if they possess special structural characteristics, especially
</p>
<p>sparsity, that can be exploited by a solution method. Today linear programming soft-
</p>
<p>ware packages are capable of automatically identifying sparse structure within the
</p>
<p>input data and taking advantage of this sparsity in numerical computation. It is now
</p>
<p>not uncommon to solve linear programs of up to a million variables and constraints,
</p>
<p>as long as the structure is sparse. Problem-dependent methods, where the structure
</p>
<p>is not automatically identified, are largely directed to transportation and network
</p>
<p>flow problems as discussed in the book.
</p>
<p>This book focuses on the aspects of general theory that are most fruitful for
</p>
<p>computation in the widest class of problems. While necessary and sufficient con-
</p>
<p>ditions are examined and their application to small-scale problems is illustrated, our
</p>
<p>primary interest in such conditions is in their role as the core of a broader theory
</p>
<p>applicable to the solution of larger problems. At the other extreme, although some
</p>
<p>instances of structure exploitation are discussed, we focus primarily on the general
</p>
<p>continuous variable programming problem rather than on special techniques for spe-
</p>
<p>cial structures.
</p>
<p>1.4 Iterative Algorithms and Convergence
</p>
<p>The most important characteristic of a high-speed computer is its ability to per-
</p>
<p>form repetitive operations efficiently, and in order to exploit this basic character-
</p>
<p>istic, most algorithms designed to solve large optimization problems are iterative
</p>
<p>in nature. Typically, in seeking a vector that solves the programming problem, an
</p>
<p>initial vector x0 is selected and the algorithm generates an improved vector x1. The
</p>
<p>process is repeated and a still better solution x2 is found. Continuing in this fashion,
</p>
<p>a sequence of ever-improving points x0, x1, . . . , xk, . . ., is found that approaches a
</p>
<p>solution point x&lowast;. For linear programming problems solved by the simplex method,
the generated sequence is of finite length, reaching the solution point exactly after a
</p>
<p>finite (although initially unspecified) number of steps. For nonlinear programming
</p>
<p>problems or interior-point methods, the sequence generally does not ever exactly
</p>
<p>reach the solution point, but converges toward it. In operation, the process is termi-
</p>
<p>nated when a point sufficiently close to the solution point, for practical purposes, is
</p>
<p>obtained.
</p>
<p>The theory of iterative algorithms can be divided into three (somewhat overlap-
</p>
<p>ping) aspects. The first is concerned with the creation of the algorithms themselves.
</p>
<p>Algorithms are not conceived arbitrarily, but are based on a creative examination
</p>
<p>of the programming problem, its inherent structure, and the efficiencies of digital
</p>
<p>computers. The second aspect is the verification that a given algorithm will in fact</p>
<p/>
</div>
<div class="page"><p/>
<p>1.4 Iterative Algorithms and Convergence 7
</p>
<p>generate a sequence that converges to a solution point. This aspect is referred to as
</p>
<p>global convergence analysis, since it addresses the important question of whether
</p>
<p>the algorithm, when initiated far from the solution point, will eventually converge
</p>
<p>to it. The third aspect is referred to as local convergence analysis or complexity
</p>
<p>analysis and is concerned with the rate at which the generated sequence of points
</p>
<p>converges to the solution. One cannot regard a problem as solved simply because
</p>
<p>an algorithm is known which will converge to the solution, since it may require
</p>
<p>an exorbitant amount of time to reduce the error to an acceptable tolerance. It is
</p>
<p>essential when prescribing algorithms that some estimate of the time required be
</p>
<p>available. It is the convergence-rate aspect of the theory that allows some quantita-
</p>
<p>tive evaluation and comparison of different algorithms, and at least crudely, assigns
</p>
<p>a measure of tractability to a problem, as discussed in Sect. 1.1.
</p>
<p>A modern-day technical version of Confucius&rsquo; most famous saying, and one
</p>
<p>which represents an underlying philosophy of this book, might be, &ldquo;One good theory
</p>
<p>is worth a thousand computer runs.&rdquo; Thus, the convergence properties of an itera-
</p>
<p>tive algorithm can be estimated with confidence either by performing numerous
</p>
<p>computer experiments on different problems or by a simple well-directed theoreti-
</p>
<p>cal analysis. A simple theory, of course, provides invaluable insight as well as the
</p>
<p>desired estimate.
</p>
<p>For linear programming using the simplex method, solid theoretical statements
</p>
<p>on the speed of convergence were elusive, because the method actually converges to
</p>
<p>an exact solution in a finite number of steps. The question is how many steps might
</p>
<p>be required. This question was finally resolved when it was shown that it was possi-
</p>
<p>ble for the number of steps to be exponential in the size of the program. The situa-
</p>
<p>tion is different for interior point algorithms, which essentially treat the problem by
</p>
<p>introducing nonlinear terms, and which therefore do not generally obtain a solution
</p>
<p>in a finite number of steps but instead converge toward a solution.
</p>
<p>For nonlinear programs, including interior point methods applied to linear pro-
</p>
<p>grams, it is meaningful to consider the speed of convergence. There are many
</p>
<p>different classes of nonlinear programming algorithms, each with its own conver-
</p>
<p>gence characteristics. However, in many cases the convergence properties can be
</p>
<p>deduced analytically by fairly simple means, and this analysis is substantiated by
</p>
<p>computational experience. Presentation of convergence analysis, which seems to be
</p>
<p>the natural focal point of a theory directed at obtaining specific answers, is a unique
</p>
<p>feature of this book.
</p>
<p>There are in fact two aspects of convergence-rate theory. The first is generally
</p>
<p>known as complexity analysis and focuses on how fast the method converges over-
</p>
<p>all, distinguishing between polynomial-time algorithms and non-polynomial-time
</p>
<p>algorithms. The second aspect provides more detailed analysis of how fast the
</p>
<p>method converges in the final stages, and can provide comparisons between dif-
</p>
<p>ferent algorithms. Both of these are treated in this book.
</p>
<p>The convergence-rate theory presented has two somewhat surprising but definitely
</p>
<p>pleasing aspects. First, the theory is, for the most part, extremely simple in nature.
</p>
<p>Although initially one might fear that a theory aimed at predicting the speed of
</p>
<p>convergence of a complex algorithm might itself be doubly complex, in fact the</p>
<p/>
</div>
<div class="page"><p/>
<p>8 1 Introduction
</p>
<p>associated convergence analysis often turns out to be exceedingly elementary, re-
</p>
<p>quiring only a line or two of calculation. Second, a large class of seemingly distinct
</p>
<p>algorithms turns out to have a common convergence rate. Indeed, as emphasized
</p>
<p>in the later chapters of the book, there is a canonical rate associated with a given
</p>
<p>programming problem that seems to govern the speed of convergence of many algo-
</p>
<p>rithms when applied to that problem. It is this fact that underlies the potency of the
</p>
<p>theory, allowing definitive comparisons among algorithms to be made even without
</p>
<p>detailed knowledge of the problems to which they will be applied. Together these
</p>
<p>two properties, simplicity and potency, assure convergence analysis a permanent
</p>
<p>position of major importance in mathematical programming theory.</p>
<p/>
</div>
<div class="page"><p/>
<p>Part I
</p>
<p>Linear Programming</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 2
</p>
<p>Basic Properties of Linear Programs
</p>
<p>2.1 Introduction
</p>
<p>A linear program (LP) is an optimization problem in which the objective function
</p>
<p>is linear in the unknowns and the constraints consist of linear equalities and linear
</p>
<p>inequalities. The exact form of these constraints may differ from one problem to an-
</p>
<p>other, but as shown below, any linear program can be transformed into the following
</p>
<p>standard form:
</p>
<p>minimize c1x1 + c2x2 + . . . + cnxn
subject to a11x1 + a12x2 + . . . + a1nxn = b1
</p>
<p>a21x1 + a22x2 + . . . + a2nxn = b2
...
</p>
<p>...
</p>
<p>am1x1 + am2x2 + &middot; &middot; &middot; + amnxn = bm
and x1 � 0, x2 � 0, . . . , xn � 0,
</p>
<p>(2.1)
</p>
<p>where the bi&rsquo;s, ci&rsquo;s and ai j&rsquo;s are fixed real constants, and the xi&rsquo;s are real numbers to
</p>
<p>be determined. We always assume that each equation has been multiplied by minus
</p>
<p>unity, if necessary, so that each bi � 0.
</p>
<p>In more compact vector notation,1 this standard problem becomes
</p>
<p>minimize cTx
</p>
<p>subject to Ax = b and x � 0. (2.2)
</p>
<p>Here x is an n-dimensional column vector, cT is an n-dimensional row vector, A is
</p>
<p>an m &times; n matrix, and b is an m-dimensional column vector. The vector inequality
x � 0 means that each component of x is nonnegative.
</p>
<p>1 See Appendix A for a description of the vector notation used throughout this book.
</p>
<p>&copy; Springer International Publishing Switzerland 2016
</p>
<p>D.G. Luenberger, Y. Ye, Linear and Nonlinear Programming, International
Series in Operations Research &amp; Management Science 228,
DOI 10.1007/978-3-319-18842-3 2
</p>
<p>11</p>
<p/>
</div>
<div class="page"><p/>
<p>12 2 Basic Properties of Linear Programs
</p>
<p>Before giving some examples of areas in which linear programming problems
</p>
<p>arise naturally, we indicate how various other forms of linear programs can be con-
</p>
<p>verted to the standard form.
</p>
<p>Example 1 (Slack Variables). Consider the problem
</p>
<p>minimize c1x1 + c2x2 + &middot; &middot; &middot; + cnxn
subject to a11x1 + a12x2 + &middot; &middot; &middot; + a1nxn � b1
</p>
<p>a21x1 + a22x2 + &middot; &middot; &middot; + a2nxn � b2
...
</p>
<p>...
</p>
<p>am1x1 + am2x2 + &middot; &middot; &middot; + amnxn � bm
and x1 � 0, x2 � 0, . . . , xn � 0,
</p>
<p>In this case the constraint set is determined entirely by linear inequalities.
</p>
<p>The problem may be alternatively expressed as
</p>
<p>minimize c1x1 + c2x2 + &middot; &middot; &middot; + cnxn
subject to a11x1 + a12x2 + &middot; &middot; &middot; + a1nxn + y1 = b1
</p>
<p>a21x1 + a22x2 + &middot; &middot; &middot; + a2nxn + y2 = b2
...
</p>
<p>...
</p>
<p>am1x1 + am2x2 + &middot; &middot; &middot; + amnxn + ym = bm
and x1 � 0, x2 � 0, . . . , xn � 0,
</p>
<p>and y1 � 0, y2 � 0, . . . , ym � 0.
</p>
<p>The new positive variables yi introduced to convert the inequalities to equalities
</p>
<p>are called slack variables (or more loosely, slacks). By considering the problem
</p>
<p>as one having n + m unknowns x1, x2, . . . , xn, y1, y2, . . . , ym, the problem takes
</p>
<p>the standard form. The m &times; (n + m) matrix that now describes the linear equality
constraints is of the special form [A, I] (that is, its columns can be partitioned into
</p>
<p>two sets; the first n columns make up the original A matrix and the last m columns
</p>
<p>make up an m &times; m identity matrix).
</p>
<p>Example 2 (Surplus Variables). If the linear inequalities of Example 1 are reversed
</p>
<p>so that a typical inequality is
</p>
<p>ai1x1 + ai2x2 + &middot; &middot; &middot; + ainxn � bi,
</p>
<p>it is clear that this is equivalent to
</p>
<p>ai1x1 + ai2x2 + &middot; &middot; &middot; + ainxn &minus; yi = bi
</p>
<p>with yi � 0. Variables, such as yi, adjoined in this fashion to convert a &ldquo;greater than
</p>
<p>or equal to&rdquo; inequality to equality are called surplus variables.
</p>
<p>It should be clear that by suitably multiplying by minus unity, and adjoining slack
</p>
<p>and surplus variables, any set of linear inequalities can be converted to standard form
</p>
<p>if the unknown variables are restricted to be nonnegative.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.1 Introduction 13
</p>
<p>Example 3 (Free Variables&mdash;First Method). If a linear program is given in standard
</p>
<p>form except that one or more of the unknown variables is not required to be non-
</p>
<p>negative, the problem can be transformed to standard form by either of two simple
</p>
<p>techniques.
</p>
<p>To describe the first technique, suppose in (2.1), for example, that the restriction
</p>
<p>x1 � 0 is not present and hence x1 is free to take on either positive or negative
</p>
<p>values. We then write
</p>
<p>x1 = u1 &minus; v1, (2.3)
where we require u1 � 0 and v1 � 0. If we substitute u1 &minus; v1 for x1 everywhere in
(2.1), the linearity of the constraints is preserved and all variables are now required
</p>
<p>to be nonnegative. The problem is then expressed in terms of the n + 1 variables
</p>
<p>u1, v1, x2, x3, . . . , xn.
</p>
<p>There is obviously a certain degree of redundancy introduced by this technique,
</p>
<p>however, since a constant added to u1 and v1 does not change x1 (that is, the rep-
</p>
<p>resentation of a given value x1 is not unique). Nevertheless, this does not hinder
</p>
<p>the simplex method of solution.
</p>
<p>Example 4 (Free Variables&mdash;Second Method). A second approach for converting to
</p>
<p>standard form when x1 is unconstrained in sign is to eliminate x1 together with one
</p>
<p>of the constraint equations. Take any one of the m equations in (2.1) which has a
</p>
<p>nonzero coefficient for x1. Say, for example,
</p>
<p>ai1x1 + ai2x2 + &middot; &middot; &middot; + ainxn = bi, (2.4)
</p>
<p>where ai1 � 0. Then x1 can be expressed as a linear combination of the other vari-
</p>
<p>ables plus a constant. If this expression is substituted for x1 everywhere in (2.1),
</p>
<p>we are led to a new problem of exactly the same form but expressed in terms of
</p>
<p>the variables x2, x3, . . . , xn only. Furthermore, the ith equation, used to determine
</p>
<p>x1, is now identically zero and it too can be eliminated. This substitution scheme
</p>
<p>is valid since any combination of nonnegative variables x2, x3, . . . , xn leads to a
</p>
<p>feasible x1 from (2.4), since the sign of x1 is unrestricted. As a result of this sim-
</p>
<p>plification, we obtain a standard linear program having n &minus; 1 variables and m &minus; 1
constraint equations. The value of the variable x1 can be determined after solution
</p>
<p>through (2.4).
</p>
<p>Example 5 (Specific Case). As a specific instance of the above technique consider
</p>
<p>the problem
</p>
<p>minimize x1 + 3x2 + 4x3
</p>
<p>subject to x1 + 2x2 + x3 = 5
</p>
<p>2x1 + 3x2 + x3 = 6
</p>
<p>x2 � 0, x3 � 0.
</p>
<p>Since x1 is free, we solve for it from the first constraint, obtaining</p>
<p/>
</div>
<div class="page"><p/>
<p>14 2 Basic Properties of Linear Programs
</p>
<p>x1 = 5 &minus; 2x2 &minus; x3. (2.5)
</p>
<p>Substituting this into the objective and the second constraint, we obtain the equiva-
</p>
<p>lent problem (subtracting five from the objective)
</p>
<p>minimize x2 + 3x3
</p>
<p>subject to x2 + x3 = 4
</p>
<p>x2 � 0, x3 � 0,
</p>
<p>which is a problem in standard form. After the smaller problem is solved (the answer
</p>
<p>is x2 = 4, x3 = 0) the value for x1(x1 = &minus;3) can be found from (2.5).
</p>
<p>2.2 Examples of Linear Programming Problems
</p>
<p>Linear programming has long proved its merit as a significant model of numerous
</p>
<p>allocation problems and economic phenomena. The continuously expanding litera-
</p>
<p>ture of applications repeatedly demonstrates the importance of linear programming
</p>
<p>as a general framework for problem formulation. In this section we present some
</p>
<p>classic examples of situations that have natural formulations.
</p>
<p>Example 1 (The Diet Problem). How can we determine the most economical diet
</p>
<p>that satisfies the basic minimum nutritional requirements for good health? Such a
</p>
<p>problem might, for example, be faced by the dietitian of a large army. We assume
</p>
<p>that there are available at the market n different foods and that the jth food sells at a
</p>
<p>price c j per unit. In addition there are m basic nutritional ingredients and, to achieve
</p>
<p>a balanced diet, each individual must receive at least bi units of the ith nutrient per
</p>
<p>day. Finally, we assume that each unit of food j contains ai j units of the ith nutrient.
</p>
<p>If we denote by x j the number of units of food j in the diet, the problem then is
</p>
<p>to select the x j&rsquo;s to minimize the total cost
</p>
<p>c1x1 + c2x2 + &middot; &middot; &middot; + cnxn
</p>
<p>subject to the nutritional constraints
</p>
<p>ai1x1 + ai2x2 + &middot; &middot; &middot; + ainxn � bi, i = 1, . . . ,m,
</p>
<p>and the nonnegativity constraints
</p>
<p>x1 � 0, x2 � 0, . . . , xn � 0
</p>
<p>on the food quantities.
</p>
<p>This problem can be converted to standard form by subtracting a nonnegative
</p>
<p>surplus variable from the left side of each of the m linear inequalities. The diet
</p>
<p>problem is discussed further in Chap. 4.
</p>
<p>Example 2 (Manufacturing Problem). Suppose we own a facility that is capable of
</p>
<p>manufacturing n different products, each of which may require various amounts</p>
<p/>
</div>
<div class="page"><p/>
<p>2.2 Examples of Linear Programming Problems 15
</p>
<p>of m different resources. Each product can be produced at any level x j � 0,
</p>
<p>j = 1, 2, . . . , n, and each unit of the jth product can sell for p j dollars and needs
</p>
<p>ai j units of the ith resource, i = 1, 2, . . . ,m. Assuming linearity of the production
</p>
<p>facility, if we are given a set of m numbers b1, b2, . . . , bm describing the available
</p>
<p>quantities of the m resources, and we wish to manufacture products at maximum
</p>
<p>revenue, ours decision problem is a linear program to maximize
</p>
<p>p1x1 + p2x2 + &middot; &middot; &middot; + pnxn
</p>
<p>subject to the resource constraints
</p>
<p>ai1x1 + ai2x2 + &middot; &middot; &middot; + ainxn � bi, i = 1, . . . ,m
</p>
<p>and the nonnegativity constraints on all production variables.
</p>
<p>Example 3 (The Transportation Problem). Quantities a1, a2, . . . , am, respectively,
</p>
<p>of a certain product are to be shipped from each of m locations and received in
</p>
<p>amounts b1, b2, . . . , bn, respectively, at each of n destinations. Associated with the
</p>
<p>shipping of a unit of product from origin i to destination j is a shipping cost ci j. It is
</p>
<p>desired to determine the amounts xi j to be shipped between each origin&ndash;destination
</p>
<p>pair i = 1, 2, . . . , m; j = 1, 2, . . . , n; so as to satisfy the shipping requirements and
</p>
<p>minimize the total cost of transportation.
</p>
<p>To formulate this problem as a linear programming problem, we set up the array
</p>
<p>shown below:
</p>
<p>The ith row in this array defines the variables associated with the ith origin, while
</p>
<p>the jth column in this array defines the variables associated with the jth destina-
</p>
<p>tion. The problem is to place nonnegative variables xi j in this array so that the sum
</p>
<p>across the ith row is a j, the sum down the jth column is b j, and the weighted sum
&sum;n
</p>
<p>j=1
</p>
<p>&sum;m
i=1 ci jxi j, representing the transportation cost, is minimized.
</p>
<p>Thus, we have the linear programming problem:
</p>
<p>minimize
&sum;
</p>
<p>i j
</p>
<p>ci jxi j
</p>
<p>subject to
</p>
<p>n
&sum;
</p>
<p>j=1
</p>
<p>xi j = a j for i = 1, 2, . . . , m (2.6)
</p>
<p>m
&sum;
</p>
<p>i=1
</p>
<p>xi j = b j for j = 1, 2, . . . , n (2.7)</p>
<p/>
</div>
<div class="page"><p/>
<p>16 2 Basic Properties of Linear Programs
</p>
<p>xi j � 0 for i = 1, 2, . . . , m; j = 1, 2, . . . , n.
</p>
<p>In order that the constraints (2.6) and (2.7) be consistent, we must, of course,
</p>
<p>assume that
&sum;m
</p>
<p>i=1 ai =
&sum;n
</p>
<p>j=1 b j which corresponds to assuming that the total amount
</p>
<p>shipped is equal to the total amount received.
</p>
<p>The transportation problem is now clearly seen to be a linear programming prob-
</p>
<p>lem in mn variables. The equations (2.6) and (2.7) can be combined and expressed
</p>
<p>in matrix form in the usual manner and this results in an (m + n) &times; (mn) coefficient
matrix consisting of zeros and ones only.
</p>
<p>Fig. 2.1 A network with capacities
</p>
<p>Example 4 (The Maximal Flow Problem). Consider a capacitated network (see
</p>
<p>Fig. 2.1, and Appendix D) in which two special nodes, called the source and the
</p>
<p>sink, are distinguished. Say they are nodes 1 and m, respectively. All other nodes
</p>
<p>must satisfy the strict conservation requirement; that is, the net flow into these nodes
</p>
<p>must be zero. However, the source may have a net outflow and the sink a net inflow.
</p>
<p>The outflow f of the source will equal the inflow of the sink as a consequence of
</p>
<p>the conservation at all other nodes. A set of arc flows satisfying these conditions
</p>
<p>is said to be a flow in the network of value f . The maximal flow problem is that
</p>
<p>of determining the maximal flow that can be established in such a network. When
</p>
<p>written out, it takes the form
</p>
<p>minimize f
</p>
<p>subject to
</p>
<p>n
&sum;
</p>
<p>j=1
</p>
<p>x1 j &minus;
n
&sum;
</p>
<p>j=1
</p>
<p>x j1 &minus; f = 0
</p>
<p>n
&sum;
</p>
<p>j=1
</p>
<p>xi j &minus;
n
&sum;
</p>
<p>j=1
</p>
<p>x ji = 0, i � 1, m (2.8)
</p>
<p>n
&sum;
</p>
<p>j=1
</p>
<p>xm j &minus;
n
&sum;
</p>
<p>j=1
</p>
<p>x jm + f = 0
</p>
<p>0 &le; xi j &le; ki j, forall i, j,
</p>
<p>where ki j = 0 for those no-arc pairs (i, j).</p>
<p/>
</div>
<div class="page"><p/>
<p>2.2 Examples of Linear Programming Problems 17
</p>
<p>Example 5 (A Warehousing Problem). Consider the problem of operating a ware-
</p>
<p>house, by buying and selling the stock of a certain commodity, in order to maximize
</p>
<p>profit over a certain length of time. The warehouse has a fixed capacity C, and there
</p>
<p>is a cost r per unit for holding stock for one period. The price, pi, of the commod-
</p>
<p>ity is known to fluctuate over a number of time periods&mdash;say months, indexed by
</p>
<p>i. In any period the same price holds for both purchase or sale. The warehouse is
</p>
<p>originally empty and is required to be empty at the end of the last period.
</p>
<p>To formulate this problem, variables are introduced for each time period. In par-
</p>
<p>ticular, let xi denote the level of stock in the warehouse at the beginning of period i.
</p>
<p>Let ui denote the amount bought during period i, and let si denote the amount sold
</p>
<p>during period i. If there are n periods, the problem is
</p>
<p>maximize
n
&sum;
</p>
<p>i=1
(pi(si &minus; ui) &minus; rxi)
</p>
<p>subject to xi+1 = xi + ui &minus; si i = 1, 2, . . . , n &minus; 1
0 = xn + un &minus; sn
xi + zi = C i = 2, . . . , n
</p>
<p>x1 = 0, xi � 0, ui � 0, si � 0, zi � 0,
</p>
<p>where zi is a slack variable. If the constraints are written out explicitly for the case
</p>
<p>n = 3, they take the form
</p>
<p>&minus;u1 + s1 +x2 =0
&minus;x2 &minus; u2 + s2 +x3 =0
x2 + z2 =C
</p>
<p>&minus;x3 &minus; u3 + s3 =0
x3 + z3 =C
</p>
<p>Note that the coefficient matrix can be partitioned into blocks corresponding to
</p>
<p>the variables of the different time periods. The only blocks that have nonzero entries
</p>
<p>are the diagonal ones and the ones immediately above the diagonal. This structure
</p>
<p>is typical of problems involving time.
</p>
<p>Example 6 (Linear Classifier and Support Vector Machine). Suppose several
</p>
<p>d-dimensional data points are classified into two distinct classes. For example, two-
</p>
<p>dimensional data points may be grade averages in science and humanities for differ-
</p>
<p>ent students. We also know the academic major of each student, as being in science
</p>
<p>or humanities, which serves as the classification. In general we have vectors ai &isin; Ed
for i = 1, 2, . . . , n1 and vectors b j &isin; Ed for j = 1, 2, . . . , n2. We wish to find
a hyperplane that separates the ai&rsquo;s from the b j&rsquo;s. Mathematically we wish to find
</p>
<p>y &isin; Ed and a number β such that
</p>
<p>aTi y + β � 1 for all i
</p>
<p>bTj y + β � &minus;1 for all j,
</p>
<p>where {x : xTy + β = 0} is the desired hyperplane, and the separation is defined by
the +1 and &minus;l. This is a linear program. See Fig. 2.2.</p>
<p/>
</div>
<div class="page"><p/>
<p>18 2 Basic Properties of Linear Programs
</p>
<p>Example 7 (Combinatorial Auction). Suppose there are m mutually exclusive po-
</p>
<p>tential states and only one of them will be true at maturity. For example, the states
</p>
<p>may correspond to the winning horse in a race of m horses, or the value of a stock
</p>
<p>index, falling within m intervals. An auction organizer who establishes a parimutuel
</p>
<p>auction is prepared to issue contracts specifying subsets of the m possibilities that
</p>
<p>pay $1 if the final state is one of those designated by the contract, and zero oth-
</p>
<p>erwise. There are n participants who may place orders with the organizer for the
</p>
<p>purchase of such contracts. An order by the jth participant consists of an m-vector
</p>
<p>a j = (a1 j, a2 j, . . . , am j)
T where each component is either 0 or 1, a one indicating a
</p>
<p>desire to be paid if the corresponding state occurs.
</p>
<p>Fig. 2.2 Support vector for data classification
</p>
<p>Accompanying the order is a number π j which is the price limit the participant
</p>
<p>is willing to pay for one unit of the order. Finally, the participant also declares the
</p>
<p>maximum number q j of units he or she is willing to accept under these terms.
</p>
<p>The auction organizer, after receiving these various orders, must decide how
</p>
<p>many contracts to fill. Let x j be the (real) number of units awarded to the jth or-
</p>
<p>der. Then the jth participant will pay π jx j. The total amount paid by all participants
</p>
<p>is πTx, where x is the vector of x j&rsquo;s and π is the vector of prices.
</p>
<p>If the outcome is the ith state, the auction organizer must pay out a total of
&sum;n
</p>
<p>j=1 ai jx j = (Ax) j. The organizer would like to maximize profit in the worst possi-
</p>
<p>ble case, and does this by solving the problem
</p>
<p>maximize πTx &minus; maxi(Ax)i
subject to 0 � x � q.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.3 Basic Solutions 19
</p>
<p>This problem can be expressed alternatively as selecting x and scalar s to
</p>
<p>maximize πTx &minus; s
subject to Ax &minus; 1s � 0
</p>
<p>0 � x � q
</p>
<p>where 1 is the vector of all 1&rsquo;s. Notice that the profit will always be nonnegative,
</p>
<p>since x = 0 is feasible.
</p>
<p>2.3 Basic Solutions
</p>
<p>Consider the system of equalities
</p>
<p>Ax = b, (2.9)
</p>
<p>where x is an n-vector, b an m-vector, and A is an m &times; n matrix. Suppose that from
the n columns of A we select a set of m linearly independent columns (such a set
</p>
<p>exists if the rank of A is m). For notational simplicity assume that we select the first
</p>
<p>m columns of A and denote the m &times; m matrix determined by these columns by B.
The matrix B is then nonsingular and we may uniquely solve the equation.
</p>
<p>BxB = b (2.10)
</p>
<p>for the m-vector xB. By putting x = (xB, 0) (that is, setting the first m components
</p>
<p>of x equal to those of xB and the remaining components equal to zero), we obtain a
</p>
<p>solution to Ax = b. This leads to the following definition.
</p>
<p>Definition. Given the set of m simultaneous linear equations in n unknowns (2.9), let B be
any nonsingular m&times;m submatrix made up of columns of A. Then, if all n&minus;m components of
x not associated with columns of B are set equal to zero, the solution to the resulting set of
equations is said to be a basic solution to (2.9) with respect to the basis B. The components
of x associated with columns of B are called basic variables.
</p>
<p>In the above definition we refer to B as a basis, since B consists of m linearly
</p>
<p>independent columns that can be regarded as a basis for the space Em. The basic
</p>
<p>solution corresponds to an expression for the vector b as a linear combination of
</p>
<p>these basis vectors. This interpretation is discussed further in the next section.
</p>
<p>In general, of course, Eq. (2.9) may have no basic solutions. However, we may
</p>
<p>avoid trivialities and difficulties of a nonessential nature by making certain elemen-
</p>
<p>tary assumptions regarding the structure of the matrix A. First, we usually assume
</p>
<p>that n &gt; m, that is, the number of variables x j exceeds the number of equality con-
</p>
<p>straints. Second, we usually assume that the rows of A are linearly independent, cor-
</p>
<p>responding to linear independence of the m equations. A linear dependency among
</p>
<p>the rows of A would lead either to contradictory constraints and hence no solutions
</p>
<p>to (2.9), or to a redundancy that could be eliminated. Formally, we explicitly make
</p>
<p>the following assumption in our development, unless noted otherwise.</p>
<p/>
</div>
<div class="page"><p/>
<p>20 2 Basic Properties of Linear Programs
</p>
<p>Full Rank Assumption. The m &times; n matrix A has m &lt; n, and the m rows of A are linearly
independent.
</p>
<p>Under the above assumption, the system (2.9) will always have a solution and, in
</p>
<p>fact, it will always have at least one basic solution.
</p>
<p>The basic variables in a basic solution are not necessarily all nonzero. This is
</p>
<p>noted by the following definition.
</p>
<p>Definition. If one or more of the basic variables in a basic solution has value zero, that
solution is said to be a degenerate basic solution.
</p>
<p>We note that in a nondegenerate basic solution the basic variables, and hence the
</p>
<p>basis B, can be immediately identified from the positive components of the solution.
</p>
<p>There is ambiguity associated with a degenerate basic solution, however, since the
</p>
<p>zero-valued basic and some of nonbasic variables can be interchanged.
</p>
<p>So far in the discussion of basic solutions we have treated only the equality con-
</p>
<p>straint (2.9) and have made no reference to positivity constraints on the variables.
</p>
<p>Similar definitions apply when these constraints are also considered. Thus, consider
</p>
<p>now the system of constraints
</p>
<p>Ax = b, x � 0, (2.11)
</p>
<p>which represent the constraints of a linear program in standard form.
</p>
<p>Definition. A vector x satisfying (2.11) is said to be feasible for these constraints. A feasi-
ble solution to the constraints (2.11) that is also basic is said to be a basic feasible solution;
if this solution is also a degenerate basic solution, it is called a degenerate basic feasible
solution.
</p>
<p>2.4 The Fundamental Theorem of Linear Programming
</p>
<p>In this section, through the fundamental theorem of linear programming, we estab-
</p>
<p>lish the primary importance of basic feasible solutions in solving linear programs.
</p>
<p>The method of proof of the theorem is in many respects as important as the result
</p>
<p>itself, since it represents the beginning of the development of the simplex method.
</p>
<p>The theorem (due to Carathéodory) itself shows that it is necessary only to con-
</p>
<p>sider basic feasible solutions when seeking an optimal solution to a linear program
</p>
<p>because the optimal value is always achieved at such a solution.
</p>
<p>Corresponding to a linear program in standard form
</p>
<p>minimize cTx
</p>
<p>subject to Ax = b, x � 0 (2.12)
</p>
<p>a feasible solution to the constraints that achieves the minimum value of the objec-
</p>
<p>tive function subject to those constraints is said to be an optimal feasible solution.
</p>
<p>If this solution is basic, it is an optimal basic feasible solution.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.4 The Fundamental Theorem of Linear Programming 21
</p>
<p>Fundamental Theorem of Linear Programming. Given a linear program in standard form
</p>
<p>(2.12) where A is an m &times; n matrix of rank m,
</p>
<p>i) if there is a feasible solution, there is a basic feasible solution;
</p>
<p>ii) if there is an optimal feasible solution, there is an optimal basic feasible solution.
</p>
<p>Proof of (i). Denote the columns of A by a1, a2, . . . , an. Suppose x = (x1, x2, . . . ,
</p>
<p>xn) is a feasible solution. Then, in terms of the columns of A, this solution satisfies:
</p>
<p>x1a1 + x2a2 + &middot; &middot; &middot; + xnan = b.
</p>
<p>Assume that exactly p of the variables xi are greater than zero, and for convenience,
</p>
<p>that they are the first p variables. Thus
</p>
<p>x1a1 + x2a2 + &middot; &middot; &middot; + xpap = b. (2.13)
</p>
<p>There are now two cases, corresponding as to whether the set a1, a2, . . . , ap is
</p>
<p>linearly independent or linearly dependent.
</p>
<p>Case 1: Assume a1, a2, . . . , ap are linearly independent. Then clearly, p � m.
</p>
<p>If p = m, the solution is basic and the proof is complete. If p &lt; m, then, since A
</p>
<p>has rank m, m &minus; p vectors can be found from the remaining n &minus; p vectors so that
the resulting set of m vectors is linearly independent. (See Exercise 12.) Assign-
</p>
<p>ing the value zero to the corresponding m &minus; p variables yields a (degenerate) basic
feasible solution.
</p>
<p>Case 2: Assume a1, a2, . . . , ap are linearly dependent. Then there is a non-
</p>
<p>trivial linear combination of these vectors that is zero. Thus there are constants
</p>
<p>y1, y2, . . . , yp, at least one of which can be assumed to be positive, such that
</p>
<p>y1a1 + y2a2 + &middot; &middot; &middot; + ypap = 0. (2.14)
</p>
<p>Multiplying this equation by a scalar ε and subtracting it from (2.13), we obtain
</p>
<p>(x1 &minus; εy1)a1 + (x2 &minus; εy2)a2 + &middot; &middot; &middot; + (xp &minus; εyp)ap = b. (2.15)
</p>
<p>This equation holds for every ε, and for each ε the components x j&minus;εy j correspond to
a solution of the linear equalities&mdash;although they may violate xi &minus; εyi � 0. Denoting
y = (y1, y2, . . . , yp, 0, 0, . . . , 0), we see that for any ε
</p>
<p>x &minus; εy (2.16)
</p>
<p>is a solution to the equalities. For ε = 0, this reduces to the original feasible solution.
</p>
<p>As ε is increased from zero, the various components increase, decrease, or remain
</p>
<p>constant, depending upon whether the corresponding yi is negative, positive, or zero.
</p>
<p>Since we assume at least one yi is positive, at least one component will decrease as ε
</p>
<p>is increased. We increase ε to the first point where one or more components become
</p>
<p>zero. Specifically, we set
</p>
<p>ε = min{xi/yi : yi &gt; 0}.</p>
<p/>
</div>
<div class="page"><p/>
<p>22 2 Basic Properties of Linear Programs
</p>
<p>For this value of ε the solution given by (2.16) is feasible and has at most p &minus; 1
positive variables. Repeating this process if necessary, we can eliminate positive
</p>
<p>variables until we have a feasible solution with corresponding columns that are lin-
</p>
<p>early independent. At that point Case 1 applies. �
</p>
<p>Proof of (ii). Let x = (x1, x2, . . . , xn) be an optimal feasible solution and, as in
</p>
<p>the proof of (i) above, suppose there are exactly p positive variables x1, x2, . . . , xp.
</p>
<p>Again there are two cases; and Case 1, corresponding to linear independence, is
</p>
<p>exactly the same as before.
</p>
<p>Case 2 also goes exactly the same as before, but it must be shown that for any
</p>
<p>ε the solution (2.16) is optimal. To show this, note that the value of the solution
</p>
<p>x &minus; εy is
cTx &minus; εcTy. (2.17)
</p>
<p>For ε sufficiently small in magnitude, x &minus; εy is a feasible solution for positive or
negative values of ε. Thus we conclude that cTy = 0. For, if cTy � 0, an ε of small
</p>
<p>magnitude and proper sign could be determined so as to render (2.17) smaller than
</p>
<p>cTx while maintaining feasibility. This would violate the assumption of optimality
</p>
<p>of x and hence we must have cTy = 0.
</p>
<p>Having established that the new feasible solution with fewer positive components
</p>
<p>is also optimal, the remainder of the proof may be completed exactly as in part (i).
</p>
<p>�
</p>
<p>This theorem reduces the task of solving a linear program to that of searching
</p>
<p>over basic feasible solutions. Since for a problem having n variables and m con-
</p>
<p>straints there are at most
(
</p>
<p>n
</p>
<p>m
</p>
<p>)
</p>
<p>=
n!
</p>
<p>m!(n &minus; m)!
</p>
<p>basic solutions (corresponding to the number of ways of selecting m of n columns),
</p>
<p>there are only a finite number of possibilities. Thus the fundamental theorem yields
</p>
<p>an obvious, but terribly inefficient, finite search technique. By expanding upon the
</p>
<p>technique of proof as well as the statement of the fundamental theorem, the efficient
</p>
<p>simplex procedure is derived.
</p>
<p>It should be noted that the proof of the fundamental theorem given above is of
</p>
<p>a simple algebraic character. In the next section the geometric interpretation of this
</p>
<p>theorem is explored in terms of the general theory of convex sets. Although the
</p>
<p>geometric interpretation is aesthetically pleasing and theoretically important, the
</p>
<p>reader should bear in mind, lest one be diverted by the somewhat more advanced
</p>
<p>arguments employed, the underlying elementary level of the fundamental theorem.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.5 Relations to Convexity 23
</p>
<p>2.5 Relations to Convexity
</p>
<p>Our development to this point, including the above proof of the fundamental theo-
</p>
<p>rem, has been based only on elementary properties of systems of linear equations.
</p>
<p>These results, however, have interesting interpretations in terms of the theory of
</p>
<p>convex sets that can lead not only to an alternative derivation of the fundamen-
</p>
<p>tal theorem, but also to a clearer geometric understanding of the result. The main
</p>
<p>link between the algebraic and geometric theories is the formal relation between
</p>
<p>basic feasible solutions of linear inequalities in standard form and extreme points
</p>
<p>of polytopes. We establish this correspondence as follows. The reader is referred to
</p>
<p>Appendix B for a more complete summary of concepts related to convexity, but the
</p>
<p>definition of an extreme point is stated here.
</p>
<p>Definition. A point x in a convex set C is said to be an extreme point of C if there are no
two distinct points x1 and x2 in C such that x = αx1 + (1 &minus; α)x2 for some α, 0 &lt; α &lt; 1.
</p>
<p>An extreme point is thus a point that does not lie strictly within a line segment
</p>
<p>connecting two other points of the set. The extreme points of a triangle, for example,
</p>
<p>are its three vertices.
</p>
<p>Theorem (Equivalence of Extreme Points and Basic Solutions). Let A be an m&times;n matrix
of rank m and b an m-vector. Let K be the convex polytope consisting of all n-vectors x
</p>
<p>satisfying
</p>
<p>Ax = b, x � 0. (2.18)
</p>
<p>A vector x is an extreme point of K if and only if x is a basic feasible solution to (2.18).
</p>
<p>Proof. Suppose first that x = (x1, x2, . . . , xm, 0, 0, . . . , 0) is a basic feasible
</p>
<p>solution to (2.18). Then
</p>
<p>x1a1 + x2a2 + &middot; &middot; &middot; + xmam = b,
</p>
<p>where a1, a2, . . . , am, the first m columns of A, are linearly independent. Suppose
</p>
<p>that x could be expressed as a convex combination of two other points in K; say,
</p>
<p>x = αy+(1&minus;α)z, 0 &lt; α &lt; 1, y � z. Since all components of x, y, z are nonnegative
and since 0 &lt; α &lt; 1, it follows immediately that the last n&minus;m components of y and
z are zero. Thus, in particular, we have
</p>
<p>y1a1 + y2a2 + &middot; &middot; &middot; + ymam = b
</p>
<p>and
</p>
<p>z1a1 + z2a2 + &middot; &middot; &middot; + zmam = b.
Since the vectors a1, a2, . . . , am are linearly independent, however, it follows that
</p>
<p>x = y = z and hence x is an extreme point of K.
</p>
<p>Conversely, assume that x is an extreme point of K. Let us assume that the
</p>
<p>nonzero components of x are the first k components. Then
</p>
<p>x1a1 + x2a2 + &middot; &middot; &middot; + xkak = b,</p>
<p/>
</div>
<div class="page"><p/>
<p>24 2 Basic Properties of Linear Programs
</p>
<p>with xi &gt; 0, i = 1, 2, . . . , k. To show that x is a basic feasible solution it must be
</p>
<p>shown that the vectors a1, a2, . . . , ak are linearly independent. We do this by con-
</p>
<p>tradiction. Suppose a1, a2, . . . , ak are linearly dependent. Then there is a nontrivial
</p>
<p>linear combination that is zero:
</p>
<p>y1a1 + y2a2 + &middot; &middot; &middot; + ykak = 0.
</p>
<p>Define the n-vector y = (y1, y2, . . . , yk, 0, 0, . . . , 0). Since xi &gt; 0, 1 � i � k, it is
</p>
<p>possible to select ε such that
</p>
<p>x + εy � 0, x &minus; εy � 0.
</p>
<p>We then have x = 1
2
(x+εy)+ 1
</p>
<p>2
(x&minus;εy) which expresses x as a convex combination of
</p>
<p>two distinct vectors in K. This cannot occur, since x is an extreme point of K. Thus
</p>
<p>a1, a2, . . . , ak are linearly independent and x is a basic feasible solution. (Although
</p>
<p>if k &lt; m, it is a degenerate basic feasible solution.) �
</p>
<p>This correspondence between extreme points and basic feasible solutions enables
</p>
<p>us to prove certain geometric properties of the convex polytope K defining the con-
</p>
<p>straint set of a linear programming problem.
</p>
<p>Corollary 1. If the convex set K corresponding to (2.18) is nonempty, it has at least one
</p>
<p>extreme point.
</p>
<p>Proof. This follows from the first part of the Fundamental Theorem and the Equiv-
</p>
<p>alence Theorem above. �
</p>
<p>Corollary 2. If there is a finite optimal solution to a linear programming problem, there is
</p>
<p>a finite optimal solution which is an extreme point of the constraint set.
</p>
<p>Corollary 3. The constraint set K corresponding to (2.18) possesses at most a finite number
</p>
<p>of extreme points.
</p>
<p>Proof. There are obviously only a finite number of basic solutions obtained by
</p>
<p>selecting m basis vectors from the n columns of A. The extreme points of K are
</p>
<p>a subset of these basic solutions. �
</p>
<p>Finally, we come to the special case which occurs most frequently in practice and
</p>
<p>which in some sense is characteristic of well-formulated linear programs&mdash;the case
</p>
<p>where the constraint set K is nonempty and bounded. In this case we combine the
</p>
<p>results of the Equivalence Theorem and Corollary 3 above to obtain the following
</p>
<p>corollary.
</p>
<p>Corollary 4. If the convex polytope K corresponding to (2.18) is bounded, then K is a con-
</p>
<p>vex polyhedron, that is, K consists of points that are convex combinations of a finite number
</p>
<p>of points.
</p>
<p>Some of these results are illustrated by the following examples:</p>
<p/>
</div>
<div class="page"><p/>
<p>2.5 Relations to Convexity 25
</p>
<p>Example 1. Consider the constraint set in E3 defined by
</p>
<p>x1 + x2 + x3 = 1
</p>
<p>x1 � 0, x2 � 0, x3 � 0.
</p>
<p>This set is illustrated in Fig. 2.3. It has three extreme points, corresponding to the
</p>
<p>three basic solutions to x1 + x2 + x3 = 1.
</p>
<p>Example 2. Consider the constraint set in E3 defined by
</p>
<p>x1 + x2 + x3 = 1
</p>
<p>2x1 + 3x2 = 1
</p>
<p>x1 � 0, x2 � 0, x3 � 0.
</p>
<p>Fig. 2.3 Feasible set for Example 1
</p>
<p>This set is illustrated in Fig. 2.4. It has two extreme points, corresponding to the
</p>
<p>two basic feasible solutions. Note that the system of equations itself has three basic
</p>
<p>solutions, (2, &minus;1, 0), (1/2, 0, 1/2 ), (0, 1/3, 2/3), the first of which is not feasible.
</p>
<p>Example 3. Consider the constraint set in E2 defined in terms of the inequalities
</p>
<p>x1 +
8
</p>
<p>3
x2 � 4
</p>
<p>x1 + x2 � 2
</p>
<p>2x1 � 3
</p>
<p>x1 � 0, x2 � 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>26 2 Basic Properties of Linear Programs
</p>
<p>This set is illustrated in Fig. 2.5. We see by inspection that this set has five ex-
</p>
<p>treme points. In order to compare this example with our general results we must
</p>
<p>introduce slack variables to yield the equivalent set in E5:
</p>
<p>x1 +
8
</p>
<p>3
x2 + x3 = 4
</p>
<p>x1 + x2 + x4 = 2
</p>
<p>2x1 + x5 = 3
</p>
<p>x1 � 0, x2 � 0, x3 � 0, x4 � 0, x5 � 0.
</p>
<p>A basic solution for this system is obtained by setting any two variables to zero and
</p>
<p>solving for the remaining three. As indicated in Fig. 2.5, each edge of the figure
</p>
<p>corresponds to one variable being zero, and the extreme points are the points where
</p>
<p>two variables are zero.
</p>
<p>Fig. 2.4 Feasible set for Example 2
</p>
<p>The last example illustrates that even when not expressed in standard form the
</p>
<p>extreme points of the set defined by the constraints of a linear program correspond to
</p>
<p>the possible solution points. This can be illustrated more directly by including the
</p>
<p>objective function in the figure as well. Suppose, for example, that in Example 3
</p>
<p>the objective function to be minimized is &minus;2x1 &minus; x2. The set of points satisfying
&minus;2x1 &minus; x2 = z for fixed z is a line. As z varies, different parallel lines are obtained
as shown in Fig. 2.6. The optimal value of the linear program is the smallest value
</p>
<p>of z for which the corresponding line has a point in common with the feasible set.
</p>
<p>It should be reasonably clear, at least in two dimensions, that the points of solution
</p>
<p>will always include an extreme point. In the figure this occurs at the point (3/2, 1/2)
</p>
<p>with z = &minus;7/2.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.6 Exercises 27
</p>
<p>2.6 Exercises
</p>
<p>1. Convert the following problems to standard form:
</p>
<p>(a) minimize x + 2y + 3z
</p>
<p>subject to 2 � x + y � 3
</p>
<p>4 � x + z � 5
</p>
<p>x � 0, y � 0, z � 0.
</p>
<p>(b) minimize x + y + z
</p>
<p>subject to x + 2y + 3z = 10
</p>
<p>x � 1, y � 2, z � 1.
</p>
<p>Fig. 2.5 Feasible set for Example 3
</p>
<p>2. A manufacturer wishes to produce an alloy that is, by weight, 30 % metal A and
</p>
<p>70 % metal B. Five alloys are available at various prices as indicated below:
</p>
<p>Alloy 1 2 3 4 5
</p>
<p>%A 10 25 50 75 95
</p>
<p>% B 90 75 50 25 5
</p>
<p>Price/lb $ 5 $ 4 $ 3 $ 2 $ 1.50
</p>
<p>The desired alloy will be produced by combining some of the other alloys. The
</p>
<p>manufacturer wishes to find the amounts of the various alloys needed and to
</p>
<p>determine the least expensive combination. Formulate this problem as a linear
</p>
<p>program.</p>
<p/>
</div>
<div class="page"><p/>
<p>28 2 Basic Properties of Linear Programs
</p>
<p>Fig. 2.6 Illustration of extreme point solution
</p>
<p>3. An oil refinery has two sources of crude oil: a light crude that costs $35/barrel
</p>
<p>and a heavy crude that costs $30/barrel. The refinery produces gasoline, heating
</p>
<p>oil, and jet fuel from crude in the amounts per barrel indicated in the following
</p>
<p>table:
</p>
<p>Gasoline Heating oil Jet fuel
</p>
<p>Light crude 0.3 0.2 0.3
</p>
<p>Heavy crude 0.3 0.4 0.2
</p>
<p>The refinery has contracted to supply 900,000 barrels of gasoline, 800,000 bar-
</p>
<p>rels of heating oil, and 500,000 barrels of jet fuel. The refinery wishes to find
</p>
<p>the amounts of light and heavy crude to purchase so as to be able to meet its
</p>
<p>obligations at minimum cost. Formulate this problem as a linear program.
</p>
<p>4. A small firm specializes in making five types of spare automobile parts. Each
</p>
<p>part is first cast from iron in the casting shop and then sent to the finishing shop
</p>
<p>where holes are drilled, surfaces are turned, and edges are ground. The required
</p>
<p>worker-hours (per 100 units) for each of the parts of the two shops are shown
</p>
<p>below:
</p>
<p>Part 1 2 3 4 5
</p>
<p>Casting 2 1 3 3 1
</p>
<p>Finishing 3 2 2 1 1
</p>
<p>The profits from the parts are $30, $20, $40, $25, and $10 (per 100 units),
</p>
<p>respectively. The capacities of the casting and finishing shops over the next
</p>
<p>month are 700 and 1,000 worker-hours, respectively. Formulate the problem of</p>
<p/>
</div>
<div class="page"><p/>
<p>2.6 Exercises 29
</p>
<p>determining the quantities of each spare part to be made during the month so as
</p>
<p>to maximize profit.
</p>
<p>5. Convert the following problem to standard form and solve:
</p>
<p>maximize x1 + 4x2 + x3
</p>
<p>subject to 2x1 &minus; 2x2 + x3 = 4
x1 &minus; x3 = 1
x2 � 0, x3 � 0.
</p>
<p>6. A large textile firm has two manufacturing plants, two sources of raw material,
</p>
<p>and three market centers. The transportation costs between the sources and the
</p>
<p>plants and between the plants and the markets are as follows:
</p>
<p>Ten tons are available from source 1 and 15 tons from source 2. The three market
</p>
<p>centers require 8 tons, 14 tons, and 3 tons. The plants have unlimited processing
</p>
<p>capacity.
</p>
<p>(a) Formulate the problem of finding the shipping patterns from sources to
</p>
<p>plants to markets that minimizes the total transportation cost.
</p>
<p>(b) Reduce the problem to a single standard transportation problem with two
</p>
<p>sources and three destinations. (Hint: Find minimum cost paths from sources
</p>
<p>to markets.)
</p>
<p>(c) Suppose that plant A has a processing capacity of 8 tons, and plant B has
</p>
<p>a processing capacity of 7 tons. Show how to reduce the problem to two
</p>
<p>separate standard transportation problems.
</p>
<p>7. A businessman is considering an investment project. The project has a lifetime
</p>
<p>of 4 years, with cash flows of &minus;$100,000, +$50,000,+$70,000, and +$30,000
in each of the 4 years, respectively. At any time he may borrow funds at the
</p>
<p>rates of 12 %, 22 %, and 34 % (total) for 1, 2, or 3 periods, respectively. He may
</p>
<p>loan funds at 10 % per period. He calculates the present value of a project as
</p>
<p>the maximum amount of money he would pay now, to another party, for the
</p>
<p>project, assuming that he has no cash on hand and must borrow and lend to pay
</p>
<p>the other party and operate the project while maintaining a nonnegative cash</p>
<p/>
</div>
<div class="page"><p/>
<p>30 2 Basic Properties of Linear Programs
</p>
<p>balance after all debts are paid. Formulate the project valuation problem in a
</p>
<p>linear programming framework.
</p>
<p>8. Convert the following problem to a linear program in standard form:
</p>
<p>minimize |x| + |y| + |z|
subject to x + y � 1
</p>
<p>2x + z = 3.
</p>
<p>9. A class of piecewise linear functions can be represented as f (x) = Maximum
</p>
<p>(cT1 x+d1, c
T
2 x+d2, . . . , c
</p>
<p>T
px+dp). For such a function f , consider the problem
</p>
<p>minimize f (x)
</p>
<p>subject to Ax = b, x � 0.
</p>
<p>Show how to convert this problem to a linear programming problem.
</p>
<p>10. A small computer manufacturing company forecasts the demand over the next
</p>
<p>n months to be di, i = 1, 2, . . . , n. In any month it can produce r units, using
</p>
<p>regular production, at a cost of b dollars per unit. By using overtime, it can
</p>
<p>produce additional units at c dollars per unit, where c &gt; b. The firm can store
</p>
<p>units from month to month at a cost of s dollars per unit per month. Formulate
</p>
<p>the problem of determining the production schedule that minimizes cost. (Hint:
</p>
<p>See Exercise 9.)
</p>
<p>11. Discuss the situation of a linear program that has one or more columns of the A
</p>
<p>matrix equal to zero. Consider both the case where the corresponding variables
</p>
<p>are required to be nonnegative and the case where some are free.
</p>
<p>12. Suppose that the matrix A = (a1, a2, . . . , an) has rank m, and that for some
</p>
<p>p &lt; m, a1, a2, . . . , ap are linearly independent. Show that m &minus; p vectors
from the remaining n &minus; p vectors can be adjoined to form a set of m linearly
independent vectors.
</p>
<p>13. Suppose that x is a feasible solution to the linear program (2.12), with A an
</p>
<p>m&times; n matrix of rank m. Show that there is a feasible solution y having the same
value (that is, cTy = cTx) and having at most m + 1 positive components.
</p>
<p>14. What are the basic solutions of Example 3, Sect. 2.5?
</p>
<p>15. Let S be a convex set in En and S &lowast; a convex set in Em. Suppose T is an m &times; n
matrix that establishes a one-to-one correspondence between S and S &lowast;, i.e., for
every s &isin; S there is s&lowast; &isin; S &lowast; such that Ts = s&lowast;, and for every s&lowast; &isin; S &lowast; there is a
single s &isin; S such that Ts = s&lowast;. Show that there is a one-to-one correspondence
between extreme points of S and S &lowast;.
</p>
<p>16. Consider the two linear programming problems in Example 1, Sect. 2.1, one
</p>
<p>in En and the other in En+m. Show that there is a one-to-one correspondence
</p>
<p>between extreme points of these two problems.</p>
<p/>
</div>
<div class="page"><p/>
<p>References 31
</p>
<p>References
</p>
<p>2.1&ndash;2.4 The approach taken in this chapter, which is continued in the next, is the
</p>
<p>more or less standard approach to linear programming as presented in, for
</p>
<p>example, Dantzig [D6], Hadley [H1], Gass [G4], Simonnard [S6], Murty
</p>
<p>[M11], and Gale [G2]. Also see Bazaraa, Jarvis, and H. F. Sherali [B6],
</p>
<p>Bertsimas and Tsitsiklis [B13], Cottle, [C6], Dantzig and Thapa [D9, D10],
</p>
<p>Nash and Sofer [N1], Saigal [S1], and Vanderbei [V3].
</p>
<p>2.5 An excellent discussion of this type can be found in Simonnard [S6].</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 3
</p>
<p>The Simplex Method
</p>
<p>The idea of the simplex method is to proceed from one basic feasible solution (that
</p>
<p>is, one extreme point) of the constraint set of a problem in standard form to another,
</p>
<p>in such a way as to continually decrease the value of the objective function until a
</p>
<p>minimum is reached. The results of Chap. 2 assure us that it is sufficient to consider
</p>
<p>only basic feasible solutions in our search for an optimal feasible solution. This
</p>
<p>chapter demonstrates that an efficient method for moving among basic solutions to
</p>
<p>the minimum can be constructed.
</p>
<p>In the first five sections of this chapter the simplex machinery is developed from
</p>
<p>a careful examination of the system of linear equations that defines the constraints
</p>
<p>and the basic feasible solutions of the system. This approach, which focuses on
</p>
<p>individual variables and their relation to the system, is probably the simplest, but
</p>
<p>unfortunately is not easily expressed in compact form. In the last few sections of
</p>
<p>the chapter, the simplex method is viewed from a matrix theoretic approach, which
</p>
<p>focuses on all variables together. This more sophisticated viewpoint leads to a com-
</p>
<p>pact notational representation, increased insight into the simplex process, and to
</p>
<p>alternative methods for implementation.
</p>
<p>3.1 Pivots
</p>
<p>To obtain a firm grasp of the simplex procedure, it is essential that one first under-
</p>
<p>stand the process of pivoting in a set of simultaneous linear equations. There are two
</p>
<p>dual interpretations of the pivot procedure.
</p>
<p>&copy; Springer International Publishing Switzerland 2016
</p>
<p>D.G. Luenberger, Y. Ye, Linear and Nonlinear Programming, International
Series in Operations Research &amp; Management Science 228,
DOI 10.1007/978-3-319-18842-3 3
</p>
<p>33</p>
<p/>
</div>
<div class="page"><p/>
<p>34 3 The Simplex Method
</p>
<p>First Interpretation
</p>
<p>Consider the set of simultaneous linear equations
</p>
<p>a11x1 + a12x2 + . . . + a1nxn = b1
</p>
<p>a21x1 + a22x2 + . . . + a2nxn = b2
...
</p>
<p>... (3.1)
</p>
<p>am1x1 + am2x2 + . . . + amnxn = bm,
</p>
<p>where m � n. In matrix form we write this as
</p>
<p>Ax = b. (3.2)
</p>
<p>In the space En we interpret this as a collection of m linear relations that must be
</p>
<p>satisfied by a vector x. Thus denoting by a j the ith row of A we may express (3.1) as:
</p>
<p>a1x = b1
</p>
<p>a2x = b2
... (3.3)
</p>
<p>amx = bm.
</p>
<p>This corresponds to the most natural interpretation of (3.1) as a set of m equations.
</p>
<p>If m &lt; n and the equations are linearly independent, then there is not a unique
</p>
<p>solution but a whole linear variety of solutions (see Appendix B). A unique solution
</p>
<p>results, however, if n &minus; m additional independent linear equations are adjoined. For
example, we might specify n &minus; m equations of the form ekx = 0, where ek is the kth
unit vector (which is equivalent to xk = 0), in which case we obtain a basic solu-
</p>
<p>tion to (3.1). Different basic solutions are obtained by imposing different additional
</p>
<p>equations of this special form.
</p>
<p>If Eq. (3.3) are linearly independent, we may replace a given equation by any
</p>
<p>nonzero multiple of itself plus any linear combination of the other equations in the
</p>
<p>system. This leads to the well-known Gaussian reduction schemes, whereby mul-
</p>
<p>tiples of equations are systematically subtracted from one another to yield either a
</p>
<p>triangular or canonical form. It is well known, and easily proved, that if the first m
</p>
<p>columns of A are linearly independent, the system (3.1) can, by a sequence of such
</p>
<p>multiplications and subtractions, be converted to the following canonical form:
</p>
<p>x1 +ā1(m+1)xm+1 + ā1(m+2)xm+2 + &middot; &middot; &middot; + ā1nxn = ā10
x2 +ā2(m+1)xm+1 + ā2(m+2)xm+2 + &middot; &middot; &middot; + ā2nxn = ā20
</p>
<p>...
... (3.4)
</p>
<p>xm +ām(m+1)xm+1 + ām(m+2)xm+2 + &middot; &middot; &middot; + āmnxn = ām0.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.1 Pivots 35
</p>
<p>Corresponding to this canonical representation of the system, the variables x1,
</p>
<p>x2, . . . , xm are called basic and the other variables are nonbasic. The corresponding
</p>
<p>basic solution is then:
</p>
<p>x1 = ā10, x2 = ā20, . . . , xm = ām0, xm+1 = 0, . . . , xn = 0,
</p>
<p>or in vector form: x = (ā0, 0) where ā0 is m-dimensional and 0 is the (n &minus; m)-
dimensional zero vector.
</p>
<p>Actually, we relax our definition somewhat and consider a system to be in canon-
</p>
<p>ical form if, among the n variables, there are m basic ones with the property that each
</p>
<p>appears in only one equation, its coefficient in that equation is unity, and no two of
</p>
<p>these m variables appear in any one equation. This is equivalent to saying that a
</p>
<p>system is in canonical form if by some reordering of the equations and the variables
</p>
<p>it takes the form (3.4).
</p>
<p>Also it is customary, from the dictates of economy, to represent the system (3.4)
</p>
<p>by its corresponding array of coefficients or tableau:
</p>
<p>x1 x2 x3 &middot; &middot; &middot; xm xm+1 xm+2 &middot; &middot; &middot; xn
1 0 0 &middot; &middot; &middot; 0 ā1(m+1) ā1(m+2) &middot; &middot; &middot; ā1n ā10
0 1 0 &middot; &middot; &middot; 0 ā2(m+1) ā2(m+2) &middot; &middot; &middot; . ā20
0 0 1 &middot; &middot; &middot; . . . . .
. . . . . . . .
</p>
<p>. . . . . . . .
</p>
<p>. . . . . . . .
</p>
<p>0 0 0 &middot; &middot; &middot; 1 ām(m+1) ām(m+2) &middot; &middot; &middot; āmn ām0
</p>
<p>(3.5)
</p>
<p>The question solved by pivoting is this: given a system in canonical form, suppose
</p>
<p>a basic variable is to be made nonbasic and a nonbasic variable is to be made basic;
</p>
<p>what is the new canonical form corresponding to the new set of basic variables? The
</p>
<p>procedure is quite simple. Suppose in the canonical system (3.4) we wish to replace
</p>
<p>the basic variable xp, 1 � p � m, by the nonbasic variable xq. This can be done if
</p>
<p>and only if āpq is nonzero; it is accomplished by dividing row p by āpq to get a unit
</p>
<p>coefficient for xq in the pth equation, and then subtracting suitable multiples of row
</p>
<p>p from each of the other rows in order to get a zero coefficient for xq in all other
</p>
<p>equations. This transforms the qth column of the tableau so that it is zero except
</p>
<p>in its pth entry (which is unity) and does not affect the columns of the other basic
</p>
<p>variables. Denoting the coefficients of the new system in canonical form by ā&prime;
ij
, we
</p>
<p>have explicitly
⎧
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>ā&prime;
ij
= āij &minus; āiqāpq āpj, i � p
</p>
<p>ā&prime;
pj
=
</p>
<p>āpj
</p>
<p>āpq
.
</p>
<p>(3.6)
</p>
<p>Equation (3.6) are the pivot equations that arise frequently in linear programming.
</p>
<p>The element āpq in the original system is said to be the pivot element.</p>
<p/>
</div>
<div class="page"><p/>
<p>36 3 The Simplex Method
</p>
<p>Example 1. Consider the system in canonical form:
</p>
<p>x1 + x4 + x5 &minus; x6 = 5
x2 + 2x4 &minus; 3x5 + x6 = 3
</p>
<p>x3 &minus; x4 + 2x5 &minus; x6 = &minus;1.
</p>
<p>Let us find the basic solution having basic variables x4, x5, x6. We set up the coef-
</p>
<p>ficient array below:
</p>
<p>x1 x2 x3 x4 x5 x6
1 0 0 1&copy; 1 &minus;1 5
0 1 0 2 &minus;3 1 3
0 0 1 &minus;1 2 &minus;1 &minus;1
</p>
<p>The circle indicated is our first pivot element and corresponds to the replacement of
</p>
<p>x1 by x4 as a basic variable. After pivoting we obtain the array
</p>
<p>and again we have circled the next pivot element indicating our intention to replace
</p>
<p>x2 by x5. We then obtain
</p>
<p>Continuing, there results
</p>
<p>x1 x2 x3 x4 x5 x6
1 &minus;1 &minus;2 1 0 0 4
1 &minus;2 &minus;3 0 1 0 2
1 &minus;3 &minus;5 0 0 1 1
</p>
<p>From this last canonical form we obtain the new basic solution
</p>
<p>x4 = 4, x5 = 2, x6 = 1.
</p>
<p>Second Interpretation
</p>
<p>The set of simultaneous equations represented by (3.1) and (3.2) can be interpreted
</p>
<p>in Em as a vector equation. Denoting the columns of A by a1, a2, . . . , an we write
</p>
<p>(3.1) as
x1a1 + x2a2 + &middot; &middot; &middot; + xnan = b. (3.7)
</p>
<p>In this interpretation we seek to express b as a linear combination of the ai&rsquo;s.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.1 Pivots 37
</p>
<p>If m &lt; n and the vectors a j span E
m then there is not a unique solution but a
</p>
<p>whole family of solutions. The vector b has a unique representation, however, as
</p>
<p>a linear combination of a given linearly independent subset of these vectors. The
</p>
<p>corresponding solution with (n&minus;m) x j variables set equal to zero is a basic solution
to (3.1).
</p>
<p>Suppose now that we start again with a system in the canonical form correspond-
</p>
<p>ing to the tableau:
</p>
<p>a1 a2 a3 &middot; &middot; &middot; am am+1 am+2 &middot; &middot; &middot; an b
1 0 0 &middot; &middot; &middot; 0 ā1(m+1) ā1(m+2) &middot; &middot; &middot; ā1n ā10
0 1 0 &middot; &middot; &middot; 0 ā2(m+1) ā2(m+2) &middot; &middot; &middot; . ā20
0 0 1 &middot; &middot; &middot; . . . . .
. . . . . . . .
</p>
<p>. . . . . . . .
</p>
<p>. . . . . . . .
</p>
<p>0 0 0 &middot; &middot; &middot; 1 ām(m+1) ām(m+2) &middot; &middot; &middot; āmn ām0
</p>
<p>(3.8)
</p>
<p>In this case the first m vectors form a basis. Furthermore, every other vector repre-
</p>
<p>sented in the tableau can be expressed as a linear combination of these basis vectors
</p>
<p>by simply reading the coefficients down the corresponding column. Thus
</p>
<p>a j = ā1 ja1 + ā2 ja2 + &middot; &middot; &middot; + ām jam. (3.9)
</p>
<p>The tableau can be interpreted as giving the representations of the vectors a j
in terms of the basis; the jth column of the tableau is the representation for the
</p>
<p>vector a j. In particular, the expression for b in terms of the basis is given in the last
</p>
<p>column.
</p>
<p>We now consider the operation of replacing one member of the basis by another
</p>
<p>vector not already in the basis. Suppose for example we wish to replace the basis
</p>
<p>vector ap, 1 � p � m, by the vector aq. Provided that the first m vectors with ap
replaced by aq are linearly independent these vectors constitute a basis and every
</p>
<p>vector can be expressed as a linear combination of this new basis. To find the new
</p>
<p>representations of the vectors we must update the tableau. The linear independence
</p>
<p>condition holds if and only if āpq � 0.
</p>
<p>Any vector a j can be expressed in terms of the old array through (3.9). For aq we
</p>
<p>have
</p>
<p>aq =
</p>
<p>m
&sum;
</p>
<p>i=1
i�p
</p>
<p>āiqai + āpqap
</p>
<p>from which we may solve for ap,
</p>
<p>ap =
1
</p>
<p>āpq
aq &minus;
</p>
<p>m
&sum;
</p>
<p>i=1
i�p
</p>
<p>āiq
</p>
<p>āpq
ai. (3.10)</p>
<p/>
</div>
<div class="page"><p/>
<p>38 3 The Simplex Method
</p>
<p>Substituting (3.10) into (3.9) we obtain:
</p>
<p>a j =
</p>
<p>m
&sum;
</p>
<p>i=1
i�p
</p>
<p>(
</p>
<p>āij &minus;
āiq
</p>
<p>āpq
āpj
</p>
<p>)
</p>
<p>ai +
āpj
</p>
<p>āpq
aq. (3.11)
</p>
<p>Denoting the coefficients of the new tableau, which give the linear combinations,
</p>
<p>by ā&prime;
ij
</p>
<p>we obtain immediately from (3.11)
</p>
<p>⎧
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>ā&prime;
ij
= āij &minus; āiqāpq āp j, i � p
</p>
<p>ā&prime;
pj
=
</p>
<p>āpj
</p>
<p>āpq
.
</p>
<p>(3.12)
</p>
<p>These formulas are identical to (3.6).
</p>
<p>If a system of equations is not originally given in canonical form, we may put
</p>
<p>it into canonical form by adjoining the m unit vectors to the tableau and, starting
</p>
<p>with these vectors as the basis, successively replace each of them with columns of
</p>
<p>A using the pivot operation.
</p>
<p>Example 2. Suppose we wish to solve the simultaneous equations
</p>
<p>x1 + x2 &minus; x3 = 5
2x1 &minus; 3x2 + x3 = 3
&minus;x1 + 2x2 &minus; x3 = &minus;1.
</p>
<p>To obtain an original basis, we form the augmented tableau
</p>
<p>e1 e2 e3 a1 a2 a3 b
</p>
<p>1 0 0 1 1 &minus;1 5
0 1 0 2 &minus;3 1 3
0 0 1 &minus;1 2 &minus;1 &minus;1
</p>
<p>and replace e1 by a1, e2 by a2, and e3 by a3. The required operations are identical
</p>
<p>to those of Example 1.
</p>
<p>3.2 Adjacent Extreme Points
</p>
<p>In Chap. 2 it was discovered that it is only necessary to consider basic feasible solu-
</p>
<p>tions to the system
</p>
<p>Ax = b, x � 0 (3.13)
</p>
<p>when solving a linear program, and in the previous section it was demonstrated that
</p>
<p>the pivot operation can generate a new basic solution from an old one by replacing
</p>
<p>one basic variable by a nonbasic variable. It is clear, however, that although the pivot</p>
<p/>
</div>
<div class="page"><p/>
<p>3.2 Adjacent Extreme Points 39
</p>
<p>operation takes one basic solution into another, the nonnegativity of the solution will
</p>
<p>not in general be preserved. Special conditions must be satisfied in order that a pivot
</p>
<p>operation maintain feasibility. In this section we show how it is possible to select
</p>
<p>pivots so that we may transfer from one basic feasible solution to another.
</p>
<p>We show that although it is not possible to arbitrarily specify the pair of vari-
</p>
<p>ables whose roles are to be interchanged and expect to maintain the nonnegativity
</p>
<p>condition, it is possible to arbitrarily specify which nonbasic variable is to become
</p>
<p>basic and then determine which basic variable should become nonbasic. As is con-
</p>
<p>ventional, we base our derivation on the vector interpretation of the linear equations
</p>
<p>although the dual interpretation could alternatively be used.
</p>
<p>Nondegeneracy Assumption
</p>
<p>Many arguments in linear programming are substantially simplified upon the intro-
</p>
<p>duction of the following.
</p>
<p>Nondegeneracy Assumption: Every basic feasible solution of (3.13) is a nondegenerate
basic feasible solution.
</p>
<p>This assumption is invoked throughout our development of the simplex method,
</p>
<p>since when it does not hold the simplex method can break down if it is not suitably
</p>
<p>amended. The assumption, however, should be regarded as one made primarily for
</p>
<p>convenience, since all arguments can be extended to include degeneracy, and the
</p>
<p>simplex method itself can be easily modified to account for it.
</p>
<p>Determination of Vector to Leave Basis
</p>
<p>Suppose we have the basic feasible solution x = (x1, x2, . . . , xm, 0, 0, . . . , 0) or,
</p>
<p>equivalently, the representation
</p>
<p>x1a1 + x2a2 + &middot; &middot; &middot; + xmam = b. (3.14)
</p>
<p>Under the nondegeneracy assumption, x j &gt; 0, i = 1, 2, . . . , m. Suppose also that
</p>
<p>we have decided to bring into the representation the vector aq, q &gt; m. We have
</p>
<p>available a representation of aq in terms of the current basis
</p>
<p>aq = ā1qa1 + ā2qa2 + &middot; &middot; &middot; + āmqam. (3.15)
</p>
<p>Multiplying (3.15) by a variable ε � 0 and subtracting from (3.14), we have
</p>
<p>(x1 &minus; εā1q)a1 + (x2 &minus; εā2q)a2 + &middot; &middot; &middot; + (xm &minus; εāmq)am + εaq = b. (3.16)</p>
<p/>
</div>
<div class="page"><p/>
<p>40 3 The Simplex Method
</p>
<p>Thus, for any ε � 0 (3.16) gives b as a linear combination of at most m + 1 vectors.
</p>
<p>For ε = 0 we have the old basic feasible solution. As ε is increased from zero,
</p>
<p>the coefficient of aq increases, and it is clear that for small enough ε, (3.16) gives
</p>
<p>a feasible but nonbasic solution. The coefficients of the other vectors will either
</p>
<p>increase or decrease linearly as ε is increased. If any decrease, we may set ε equal
</p>
<p>to the value corresponding to the first place where one (or more) of the coefficients
</p>
<p>vanishes. That is
</p>
<p>ε = min
i
{x j/āiq : āiq &gt; 0}. (3.17)
</p>
<p>In this case we have a new basic feasible solution, with the vector aq replacing the
</p>
<p>vector ap, where p corresponds to the minimizing index in (3.17). If the minimum in
</p>
<p>(3.17) is achieved by more than a single index i, then the new solution is degenerate
</p>
<p>and any of the vectors with zero component can be regarded as the one that left the
</p>
<p>basis.
</p>
<p>If none of the āiq&rsquo;s are positive, then all coefficients in the representation (3.16)
</p>
<p>increase (or remain constant) as ε is increased, and no new basic feasible solution is
</p>
<p>obtained. We observe, however, that in this case, where none of the āiq&rsquo;s are positive,
</p>
<p>there are feasible solutions to (3.13) having arbitrarily large coefficients. This means
</p>
<p>that the set K of feasible solutions to (3.13) is unbounded, and this special case, as
</p>
<p>we shall see, is of special significance in the simplex procedure.
</p>
<p>In summary, we have deduced that, given a basic feasible solution and an arbi-
</p>
<p>trary vector aq, there is either a new basic feasible solution having aq in its basis and
</p>
<p>one of the original vectors removed, or the set of feasible solutions is unbounded.
</p>
<p>Let us consider how the calculation of this section can be displayed in our
</p>
<p>tableau. We assume that corresponding to the constraints
</p>
<p>Ax = b, x � 0,
</p>
<p>we have a tableau of the form (3.8). Note that the tableau may be the result of sev-
</p>
<p>eral pivot operations applied to the original tableau, but in any event, it represents a
</p>
<p>solution with basis a1, a2, . . . , am. We assume that ā10, ā20, . . . , ām0 are nonneg-
</p>
<p>ative, so that the corresponding basic solution x1 = ā10, x2 = ā20, . . . , xm = ām0 is
</p>
<p>feasible. We wish to bring into the basis the vector aq, q &gt; m, and maintain feasibil-
</p>
<p>ity. In order to determine which element in the qth column to use as the pivot (and
</p>
<p>hence which vector in the basis will leave), we use (3.17) and compute the ratios
</p>
<p>xi/āiq = āi0/āiq, i = 1, 2, . . . , m, select the smallest nonnegative ratio, and pivot on
</p>
<p>the corresponding āiq.
</p>
<p>Example 3. Consider the system
</p>
<p>a1 a2 a3 a4 a5 a6 b
</p>
<p>1 0 0 2 4 6 4
</p>
<p>0 1 0 1 2 3 3
</p>
<p>0 0 1 &minus;1 2 1 1</p>
<p/>
</div>
<div class="page"><p/>
<p>3.2 Adjacent Extreme Points 41
</p>
<p>which has basis a1, a2, a3 yielding a basic feasible solution x = (4, 3, 1, 0, 0, 0).
</p>
<p>Suppose we elect to bring a4 into the basis. To determine which element in the
</p>
<p>fourth column is the appropriate pivot, we compute the three ratios:
</p>
<p>4/2 = 2, 3/1 = 3, 1/ &minus; 1 = &minus;1
</p>
<p>and select the smallest nonnegative one. This gives 2 as the pivot element. The new
</p>
<p>tableau is
a1 a2 a3 a4 a5 a6 b
</p>
<p>1/2 0 0 1 2 3 2
</p>
<p>&minus;1/2 1 0 0 0 0 1
1/2 0 1 0 4 4 3
</p>
<p>with corresponding basic feasible solution x = (0, 1, 3, 2, 0, 0).
</p>
<p>Our derivation of the method for selecting the pivot in a given column that will
</p>
<p>yield a new feasible solution has been based on the vector interpretation of the equa-
</p>
<p>tion Ax = b. An alternative derivation can be constructed by considering the dual
</p>
<p>approach that is based on the rows of the tableau rather than the columns. Briefly,
</p>
<p>the argument runs like this: if we decide to pivot on āpq, then we first divide the pth
</p>
<p>row by the pivot element āpq to change it to unity. In order that the new āp0 remain
</p>
<p>positive, it is clear that we must have āpq &gt; 0. Next we subtract multiples of the
</p>
<p>pth row from each other row in order to obtain zeros in the qth column. In this pro-
</p>
<p>cess the new elements in the last column must remain nonnegative&mdash;if the pivot was
</p>
<p>properly selected. The full operation is to subtract, from the ith row, āiq/āpq times
</p>
<p>the pth row. This yields a new solution obtained directly from the last column:
</p>
<p>x&prime;i = xi &minus;
āiq
</p>
<p>āpq
xp.
</p>
<p>For this to remain nonnegative, it follows that xp/āpq � xi/āiq, and hence again we
</p>
<p>are led to the conclusion that we select p as the index i minimizing xi/āiq.
</p>
<p>Geometrical Interpretations
</p>
<p>Corresponding to the two interpretations of pivoting and extreme points developed
</p>
<p>algebraically, are two geometrical interpretations. The first is in activity space, the
</p>
<p>space where x is represented. This is perhaps the most natural space to consider, and
</p>
<p>it was used in Sect. 2.5. Here the feasible region is shown directly as a convex set,
</p>
<p>and basic feasible solutions are extreme points. Adjacent extreme points are points
</p>
<p>that lie on a common edge.
</p>
<p>The second geometrical interpretation is in requirements space, the space where
</p>
<p>the columns of A and b are represented. The fundamental relation is
</p>
<p>a1x1 + a2x2 + &middot; &middot; &middot; + anxn = b.</p>
<p/>
</div>
<div class="page"><p/>
<p>42 3 The Simplex Method
</p>
<p>Fig. 3.1 Constraint representation in requirements space
</p>
<p>An example for m = 2, n = 4 is shown in Fig. 3.1. A feasible solution defines a
</p>
<p>representation of b as a positive combination of the ai&rsquo;s. A basic feasible solution
</p>
<p>will use only m positive weights. In the figure a basic feasible solution can be con-
</p>
<p>structed with positive weights on a1 and a2 because b lies between them. A basic
</p>
<p>feasible solution cannot be constructed with positive weights on a1 and a4. Suppose
</p>
<p>we start with a1 and a2 as the initial basis. Then an adjacent basis is found by bring-
</p>
<p>ing in some other vector. If a3 is brought in, then clearly a2 must go out. On the
</p>
<p>other hand, if a4 is brought in, a1 must go out.
</p>
<p>3.3 Determining a Minimum Feasible Solution
</p>
<p>In the last section we showed how it is possible to pivot from one basic feasible
</p>
<p>solution to another (or determine that the solution set is unbounded) by arbitrarily
</p>
<p>selecting a column to pivot on and then appropriately selecting the pivot in that
</p>
<p>column. The idea of the simplex method is to select the column so that the resulting
</p>
<p>new basic feasible solution will yield a lower value to the objective function than
</p>
<p>the previous one. This then provides the final link in the simplex procedure. By an
</p>
<p>elementary calculation, which is derived below, it is possible to determine which
</p>
<p>vector should enter the basis so that the objective value is reduced, and by another
</p>
<p>simple calculation, derived in the previous section, it is possible to then determine
</p>
<p>which vector should leave in order to maintain feasibility.
</p>
<p>Suppose we have a basic feasible solution
</p>
<p>(xB, 0) = (ā10, ā20, . . . , ām0, 0, 0, . . . , 0)
</p>
<p>together with a tableau having an identity matrix appearing in the first m columns
</p>
<p>as shown in tableau (3.8). The value of the objective function corresponding to any
</p>
<p>solution x is
</p>
<p>z = c1x1 + c2x2 + &middot; &middot; &middot; + cnxn, (3.18)</p>
<p/>
</div>
<div class="page"><p/>
<p>3.3 Determining a Minimum Feasible Solution 43
</p>
<p>and hence for the basic solution, the corresponding value is
</p>
<p>z0 = c
T
BxB, (3.19)
</p>
<p>where cT
B
= [c1, c2, . . . , cm].
</p>
<p>Although it is natural to use the basic solution (xB, 0) when we have the tableau
</p>
<p>(3.8), it is clear that if arbitrary values are assigned to xm+1, xm+2, . . . , xn, we can
</p>
<p>easily solve for the remaining variables as
</p>
<p>x1 = ā10 &minus;
n
&sum;
</p>
<p>j=m+1
</p>
<p>ā1 jx j
</p>
<p>x2 = ā20 &minus;
n
&sum;
</p>
<p>j=m+1
</p>
<p>ā2 jx j
</p>
<p>... (3.20)
</p>
<p>xm = ām0 &minus;
n
&sum;
</p>
<p>j=m+1
</p>
<p>āmjx j.
</p>
<p>Using (3.20) we may eliminate x1, x2, . . . , xm from the general formula (3.18).
</p>
<p>Doing this we obtain
</p>
<p>z = cTx = z0 + (cm+1 &minus; zm+1)xm+1
+(cm+2 &minus; zm+2)xm+2 + &middot; &middot; &middot; + (cn &minus; zn)xn (3.21)
</p>
<p>where
</p>
<p>z j = ā1 jc1 + ā2 jc2 + &middot; &middot; &middot; + āmjcm, m + 1 � j � n, (3.22)
which is the fundamental relation required to determine the pivot column. The imp-
</p>
<p>ortant point is that this equation gives the values of the objective function z for
</p>
<p>any solution of Ax = b in terms of the variables xm+1, . . . , xn. From it we can
</p>
<p>determine if there is any advantage in changing the basic solution by introducing
</p>
<p>one of the nonbasic variables. For example, if c j &minus; z j is negative for some j, m+1 �
j � n, then increasing x j from zero to some positive value would decrease the total
</p>
<p>cost, and therefore would yield a better solution. The formula (3.21) and (3.22)
</p>
<p>automatically take into account the changes that would be required in the values of
</p>
<p>the basic variables x1, x2, . . . , xm to accommodate the change in x j.
</p>
<p>Let us derive these relations from a different viewpoint. Let ā j be the jth column
</p>
<p>of the tableau. Then any solution satisfies
</p>
<p>x1e1 + x2e2 + &middot; &middot; &middot; + xmem = ā0 &minus; xm+1ām+1 &minus; xm+2ām+2 &minus; &middot; &middot; &middot; &minus; xnān.
Taking the inner product of this vector equation with cT
</p>
<p>B
, we have
</p>
<p>m
&sum;
</p>
<p>i=1
</p>
<p>c jx j = c
T
Bā0 &minus;
</p>
<p>n
&sum;
</p>
<p>j=m+1
</p>
<p>z jx j,</p>
<p/>
</div>
<div class="page"><p/>
<p>44 3 The Simplex Method
</p>
<p>where z j = c
T
B
</p>
<p>ā j. Thus, adding
n
&sum;
</p>
<p>j=m+1
c jx j to both sides,
</p>
<p>cTx = z0 +
</p>
<p>n
&sum;
</p>
<p>j=m+1
</p>
<p>(c j &minus; z j)x j (3.23)
</p>
<p>as before.
</p>
<p>We now state the condition for improvement, which follows easily from the
</p>
<p>above observation, as a theorem.
</p>
<p>Theorem (Improvement of Basic Feasible Solution). Given a nondegenerate basic fea-
</p>
<p>sible solution with corresponding objective value z0, suppose that for some j there holds
</p>
<p>c j &minus; z j &lt; 0. Then there is a feasible solution with objectivevalue z &lt; z0. If the column a j can
be substituted for some vector in the originalbasis to yield a new basic feasible solution,
</p>
<p>this new solution will have z &lt; z0. If a j cannot be substituted to yield a basic feasible solu-
</p>
<p>tion, then the solutionset K is unbounded and the objective function can be made arbitrarily
</p>
<p>small (toward minus infinity).
</p>
<p>Proof. The result is an immediate consequence of the previous discussion. Let
</p>
<p>(x1, x2, . . . , xm, 0, 0, . . . , 0) be the basic feasible solution with objective value
</p>
<p>z0 and suppose cm+1 &minus; zm+1 &lt; 0. Then, in any case, new feasible solutions can be
constructed of the form (x&prime;1, x
</p>
<p>&prime;
2, . . . , x
</p>
<p>&prime;
m, x
</p>
<p>&prime;
m+1, 0, 0, . . . , 0) with x
</p>
<p>&prime;
m+1 &gt; 0. Substi-
</p>
<p>tuting this solution in (3.21) we have
</p>
<p>z &minus; z0 = (cm+1 &minus; zm+1)x&prime;m+1 &lt; 0,
and hence z &lt; z0 for any such solution. It is clear that we desire to make x
</p>
<p>&prime;
m+1
</p>
<p>as large
</p>
<p>as possible. As x&prime;
m+1 is increased, the other components increase, remain constant,
</p>
<p>or decrease. Thus x&prime;
m+1
</p>
<p>can be increased until one x&prime;
i
= 0, i � m, in which case
</p>
<p>we obtain a new basic feasible solution, or if none of the x&prime;
i
&rsquo;s decrease, x&prime;
</p>
<p>m+1 can
</p>
<p>be increased without bound indicating an unbounded solution set and an objective
</p>
<p>value without lower bound. �
</p>
<p>We see that if at any stage c j &minus; z j &lt; 0 for some j, it is possible to make x j
positive and decrease the objective function. The final question remaining is whether
</p>
<p>c j &minus; z j � 0 for all j implies optimality.
Optimality Condition Theorem. If for some basic feasible solution c j&minus; z j � 0 for all j, then
that solution is optimal.
</p>
<p>Proof. This follows immediately from (3.21), since any other feasible solution must
</p>
<p>have xi � 0 for all i, and hence the value z of the objective will satisfy z &minus; z0 � 0. �
</p>
<p>Since the constants c j &minus; z j play such a central role in the development of the
simplex method, it is convenient to introduce the somewhat abbreviated notation
</p>
<p>r j = c j &minus; z j and refer to the r j&rsquo;s as the relative cost coefficients or, alternatively, the
reduced cost coefficients (both terms occur in common usage). These coefficients
</p>
<p>measure the cost of a variable relative to a given basis. (For notational convenience
</p>
<p>we extend the definition of relative cost coefficients to basic variables as well; the
</p>
<p>relative cost coefficient of a basic variable is zero.)</p>
<p/>
</div>
<div class="page"><p/>
<p>3.4 Computational Procedure: Simplex Method 45
</p>
<p>We conclude this section by giving an economic interpretation of the relative cost
</p>
<p>coefficients. Let us agree to interpret the linear program
</p>
<p>minimize cTx
</p>
<p>subject to Ax = b, x � 0
</p>
<p>as a diet problem (see Sect. 2.2) where the nutritional requirements must be met
</p>
<p>exactly. A column of A gives the nutritional equivalent of a unit of a particular food.
</p>
<p>With a given basis consisting of, say, the first m columns of A, the corresponding
</p>
<p>simplex tableau shows how any food (or more precisely, the nutritional content of
</p>
<p>any food) can be constructed as a combination of foods in the basis. For instance,
</p>
<p>if carrots are not in the basis we can, using the description given by the tableau,
</p>
<p>construct a synthetic carrot which is nutritionally equivalent to a carrot, by an app-
</p>
<p>ropriate combination of the foods in the basis.
</p>
<p>In considering whether or not the solution represented by the current basis is
</p>
<p>optimal, we consider a certain food not in the basis&mdash;say carrots&mdash;and determine if
</p>
<p>it would be advantageous to bring it into the basis. This is very easily determined
</p>
<p>by examining the cost of carrots as compared with the cost of synthetic carrots. If
</p>
<p>carrots are food j, then the unit cost of carrots is c j. The cost of a unit of synthetic
</p>
<p>carrots is, on the other hand,
</p>
<p>z j =
</p>
<p>m
&sum;
</p>
<p>i=1
</p>
<p>c jāij.
</p>
<p>If r j = c j &minus; z j &lt; 0, it is advantageous to use real carrots in place of synthetic carrots,
and carrots should be brought into the basis.
</p>
<p>In general each z j can be thought of as the price of a unit of the column a j when
</p>
<p>constructed from the current basis. The difference between this synthetic price and
</p>
<p>the direct price of that column determines whether that column should enter the
</p>
<p>basis.
</p>
<p>3.4 Computational Procedure: Simplex Method
</p>
<p>In previous sections the theory, and indeed much of the technique, necessary for
</p>
<p>the detailed development of the simplex method has been established. It is only
</p>
<p>necessary to put it all together and illustrate it with examples.
</p>
<p>In this section we assume that we begin with a basic feasible solution and that the
</p>
<p>tableau corresponding to Ax = b is in the canonical form for this solution. Methods
</p>
<p>for obtaining this first basic feasible solution, when one is not obvious, are described
</p>
<p>in the next section.
</p>
<p>In addition to beginning with the array Ax = b expressed in canonical form
</p>
<p>corresponding to a basic feasible solution, we append a row at the bottom consisting
</p>
<p>of the relative cost coefficients and the negative of the current cost. The result is a
</p>
<p>simplex tableau.</p>
<p/>
</div>
<div class="page"><p/>
<p>46 3 The Simplex Method
</p>
<p>Thus, if we assume the basic variables are (in order) x1, x2, . . . , xm, the simplex
</p>
<p>tableau takes the initial form shown in Fig. 3.2.
</p>
<p>The basic solution corresponding to this tableau is
</p>
<p>x j =
</p>
<p>{
</p>
<p>āi0 0 � i � m
</p>
<p>0 m + 1 � i � n
</p>
<p>which we have assumed is feasible, that is, āi0 � 0, i = 1, 2, . . . , m. The corre-
</p>
<p>sponding value of the objective function is z0.
</p>
<p>Fig. 3.2 Canonical simplex tableau
</p>
<p>The relative cost coefficients r j indicate whether the value of the objective will
</p>
<p>increase or decrease if x j is pivoted into the solution. If these coefficients are all
</p>
<p>nonnegative, then the indicated solution is optimal. If some of them are negative, an
</p>
<p>improvement can be made (assuming nondegeneracy) by bringing the correspond-
</p>
<p>ing component into the solution. When more than one of the relative cost coefficients
</p>
<p>is negative, any one of them may be selected to determine in which column to pivot.
</p>
<p>Common practice is to select the most negative value. (See Exercise 13 for further
</p>
<p>discussion of this point.)
</p>
<p>Some more discussion of the relative cost coefficients and the last row of the
</p>
<p>tableau is warranted. We may regard z as an additional variable and
</p>
<p>c1x1 + c2x2 + &middot; &middot; &middot; + cnxn &minus; z = 0
</p>
<p>as another equation. A basic solution to the augmented system will have m+1 basic
</p>
<p>variables, but we can require that z be one of them. For this reason it is not neces-
</p>
<p>sary to add a column corresponding to z, since it would always be (0, 0, . . . , 0, 1).
</p>
<p>Thus, initially, a last row consisting of the c j&rsquo;s and a right-hand side of zero can be
</p>
<p>appended to the standard array to represent this additional equation. Using standard
</p>
<p>pivot operations, the elements in this row corresponding to basic variables can be
</p>
<p>reduced to zero. This is equivalent to transforming the additional equation to the
</p>
<p>form
</p>
<p>rm+1xm+1 + rm+2xm+2 + &middot; &middot; &middot; + rnxn &minus; z = &minus;z0. (3.24)</p>
<p/>
</div>
<div class="page"><p/>
<p>3.4 Computational Procedure: Simplex Method 47
</p>
<p>This must be equivalent to (3.23), and hence the r j&rsquo;s obtained are the relative cost
</p>
<p>coefficients. Thus, the last row can be treated operationally like any other row: just
</p>
<p>start with c j&rsquo;s and reduce the terms corresponding to basic variables to zero by row
</p>
<p>operations.
</p>
<p>After a column q is selected in which to pivot, the final selection of the pivot
</p>
<p>element is made by computing the ratio āi0/āiq for the positive elements āiq, i =
</p>
<p>1, 2, . . . , m, of the qth column and selecting the element p yielding the minimum
</p>
<p>ratio. Pivoting on this element will maintain feasibility as well as (assuming nonde-
</p>
<p>generacy) decrease the value of the objective function. If there are ties, any element
</p>
<p>yielding the minimum can be used. If there are no nonnegative elements in the col-
</p>
<p>umn, the problem is unbounded. After updating the entire tableau with āpq as pivot
</p>
<p>and transforming the last row in the same manner as all other rows (except row q),
</p>
<p>we obtain a new tableau in canonical form. The new value of the objective function
</p>
<p>again appears in the lower right-hand corner of the tableau.
</p>
<p>The simplex algorithm can be summarized by the following steps:
</p>
<p>Step 0. Form a tableau as in Fig. 3.2 corresponding to a basic feasible solution.
</p>
<p>The relative cost coefficients can be found by row reduction.
</p>
<p>Step 1. If each r j � 0, stop; the current basic feasible solution is optimal.
</p>
<p>Step 2. Select q such that rq &lt; 0 to determine which nonbasic variable is to be-
</p>
<p>come basic.
</p>
<p>Step 3. Calculate the ratios āi0/āiq for āiq &gt; 0, i = 1, 2, . . . , m. If no āiq &gt; 0,
</p>
<p>stop; the problem is unbounded. Otherwise, select p as the index i corresponding
</p>
<p>to the minimum ratio.
</p>
<p>Step 4. Pivot on the pqth element, updating all rows including the last. Return to
</p>
<p>Step 1.
</p>
<p>Proof that the algorithm solves the problem (again assuming nondegeneracy) is
</p>
<p>essentially established by our previous development. The process terminates only
</p>
<p>if optimality is achieved or unboundedness is discovered. If neither condition is
</p>
<p>discovered at a given basic solution, then the objective is strictly decreased. Since
</p>
<p>there are only a finite number of possible basic feasible solutions, and no basis
</p>
<p>repeats because of the strictly decreasing objective, the algorithm must reach a basis
</p>
<p>satisfying one of the two terminating conditions.
</p>
<p>Example 1. Maximize 3x1 + x2 + 3x3 subject to
</p>
<p>2x1 + x2 + x3 � 2
</p>
<p>x1 + 2x2 + 3x3 � 5
</p>
<p>2x1 + 2x2 + x3 � 6
</p>
<p>x1 � 0, x2 � 0, x3 � 0.
</p>
<p>To transform the problem into standard form so that the simplex procedure can be
</p>
<p>applied, we change the maximization to minimization by multiplying the objective
</p>
<p>function by minus one, and introduce three nonnegative slack variables x4, x5, x6.
</p>
<p>We then have the initial tableau</p>
<p/>
</div>
<div class="page"><p/>
<p>48 3 The Simplex Method
</p>
<p>a1 a2 a3 a4 a5 a6 b
</p>
<p>2&copy; 1&copy; 1 1 0 0 2
1 2 3&copy; 0 1 0 5
2 2 1 0 0 1 6
</p>
<p>rT &minus;3 &minus;1 &minus;3 0 0 0 0
First tableau
</p>
<p>The problem is already in canonical form with the three slack variables serving as
</p>
<p>the basic variables. We have at this point r j = c j&minus;z j = c j, since the costs of the slacks
are zero. Application of the criterion for selecting a column in which to pivot shows
</p>
<p>that any of the first three columns would yield an improved solution. In each of these
</p>
<p>columns the appropriate pivot element is determined by computing the ratios āi0/āij
and selecting the smallest positive one. The three allowable pivots are all circled
</p>
<p>on the tableau. It is only necessary to determine one allowable pivot, and normally
</p>
<p>we would not bother to calculate them all. For hand calculation on problems of this
</p>
<p>size, however, we may wish to examine the allowable pivots and select one that will
</p>
<p>minimize (at least in the short run) the amount of division required. Thus for this
</p>
<p>example we select the second column and result in:
</p>
<p>2 1 1 1 0 0 2
</p>
<p>&minus;3 0 1&copy; &minus;2 1 0 1
&minus;2 0 &minus;1 &minus;2 0 1 2
&minus;1 0 &minus;2 1 0 0 2
</p>
<p>Second tableau
</p>
<p>We note that the objective function&mdash;we are using the negative of the original one&mdash;
</p>
<p>has decreased from zero to minus two. We now pivot on 1&copy;.
5&copy; 1 0 3 &minus;1 0 1
&minus;3 0 1 &minus;2 1 0 1
&minus;5 0 0 &minus;4 1 1 3
&minus;7 0 0 &minus;3 2 0 4
</p>
<p>Third tableau
</p>
<p>The value of the objective function has now decreased to minus four and we may
</p>
<p>pivot in either the first or fourth column. We select 5&copy;.
</p>
<p>1 1/5 0 3/5 &minus;1/5 0 1/5
0 3/5 1 &minus;1/5 2/5 0 8/5
0 1 0 &minus;1 0 1 4
0 7/5 0 6/5 3/5 0 27/5
</p>
<p>Fourth tableau
</p>
<p>Since the last row has no negative elements, we conclude that the solution corre-
</p>
<p>sponding to the fourth tableau is optimal. Thus x1 = 1/5, x2 = 0, x3 = 8/5, x4 =
</p>
<p>0, x5 = 0, x6 = 4 is the optimal solution with a corresponding value of the (nega-
</p>
<p>tive) objective of &minus;(27/5).</p>
<p/>
</div>
<div class="page"><p/>
<p>3.5 Finding a Basic Feasible Solution 49
</p>
<p>Degeneracy
</p>
<p>It is possible that in the course of the simplex procedure, degenerate basic feasible
</p>
<p>solutions may occur. Often they can be handled as a nondegenerate basic feasible
</p>
<p>solution. However, it is possible that after a new column q is selected to enter the ba-
</p>
<p>sis, the minimum of the ratios āi0/āiq may be zero, implying that the zero-valued ba-
</p>
<p>sic variable is the one to go out. This means that the new variable xq will come in at
</p>
<p>zero value, the objective will not decrease, and the new basic feasible solution will
</p>
<p>also be degenerate. Conceivably, this process could continue for a series of steps
</p>
<p>until, finally, the original degenerate solution is again obtained. The result is a cycle
</p>
<p>that could be repeated indefinitely.
</p>
<p>Methods have been developed to avoid such cycles (see Exercises 15&ndash;17 for a
</p>
<p>full discussion of one of them, which is based on perturbing the problem slightly
</p>
<p>so that zero-valued variables are actually small positive values, and Exercise 32 for
</p>
<p>Bland&rsquo;s rule, which is simpler). In practice, however, such procedures are found to
</p>
<p>be unnecessary. When degenerate solutions are encountered, the simplex procedure
</p>
<p>generally does not enter a cycle. However, anticycling procedures are simple, and
</p>
<p>many codes incorporate such a procedure for the sake of safety.
</p>
<p>3.5 Finding a Basic Feasible Solution
</p>
<p>A basic feasible solution is sometimes immediately available for linear programs.
</p>
<p>For example, in problems with constraints of the form
</p>
<p>Ax � b, x � 0 (3.25)
</p>
<p>with b � 0, a basic feasible solution to the corresponding standard form of the
</p>
<p>problem is provided by the slack variables. This provides a means for initiating the
</p>
<p>simplex procedure. The example in the last section was of this type. An initial basic
</p>
<p>feasible solution is not always apparent for other types of linear programs, how-
</p>
<p>ever, and it is necessary to develop a means for determining one so that the simplex
</p>
<p>method can be initiated. Interestingly (and fortunately), an auxiliary linear program
</p>
<p>and corresponding application of the simplex method can be used to determine the
</p>
<p>required initial solution.
</p>
<p>By elementary straightforward operations the constraints of a linear program-
</p>
<p>ming problem can always be expressed in the form
</p>
<p>Ax = b, x � 0 (3.26)</p>
<p/>
</div>
<div class="page"><p/>
<p>50 3 The Simplex Method
</p>
<p>with b � 0. In order to find a solution to (3.26) consider the artificial minimization
</p>
<p>problem
</p>
<p>minimize
</p>
<p>m
&sum;
</p>
<p>i=1
</p>
<p>u j
</p>
<p>subject to Ax + u = b (3.27)
</p>
<p>x � 0, u � 0
</p>
<p>where u = (u1, u2, . . . , um) is a vector of artificial variables. If there is a feasible
</p>
<p>solution to (3.26), then it is clear that (3.27) has a minimum value of zero with u = 0.
</p>
<p>If (3.26) has no feasible solution, then the minimum value of (3.27) is greater than
</p>
<p>zero.
</p>
<p>Now (3.27) is itself a linear program in the variables x, u, and the system is
</p>
<p>already in canonical form with basic feasible solution u = b. If (3.27) is solved
</p>
<p>using the simplex technique, a basic feasible solution is obtained at each step. If the
</p>
<p>minimum value of (3.27) is zero, then the final basic solution will have all u j = 0,
</p>
<p>and hence barring degeneracy, the final solution will have no u j variables basic. If in
</p>
<p>the final solution some u j are both zero and basic, indicating a degenerate solution,
</p>
<p>these basic variables can be exchanged for nonbasic x j variables (again at zero level)
</p>
<p>to yield a basic feasible solution involving x variables only. (However, the situation
</p>
<p>is more complex if A is not of full rank. See Exercise 21.)
</p>
<p>Example 1. Find a basic feasible solution to
</p>
<p>2x1 + x2 + 2x3 = 4
</p>
<p>3x1 + 3x2 + x3 = 3
</p>
<p>x1 � 0, x2 � 0, x3 � 0.
</p>
<p>We introduce artificial variables x4 � 0, x5 � 0 and an objective function x4 + x5.
</p>
<p>The initial tableau is
</p>
<p>x1 x2 x3 x4 x5 b
</p>
<p>2 1 2 1 0 4
</p>
<p>3 3 1 0 1 3
</p>
<p>cT 0 0 0 1 1 0
</p>
<p>Initial tableau
</p>
<p>A basic feasible solution to the expanded system is given by the artificial variables.
</p>
<p>To initiate the simplex procedure we must update the last row so that it has zero
</p>
<p>components under the basic variables. This yields:
</p>
<p>2 1 2 1 0 4
</p>
<p>➂ 3 1 0 1 3
</p>
<p>rT &minus;5 &minus;4 &minus;3 0 0 &minus;7
First tableau</p>
<p/>
</div>
<div class="page"><p/>
<p>3.5 Finding a Basic Feasible Solution 51
</p>
<p>Pivoting in the column having the most negative bottom row component as indi-
</p>
<p>cated, we obtain:
</p>
<p>In the second tableau there is only one choice for pivot, and it leads to the final
</p>
<p>tableau shown.
0 &minus;3/4 1 3/4 &minus;1/2 3/2
1 5/4 0 &minus;1/4 1/2 1/2
0 0 0 1 1 0
</p>
<p>Final tableau
</p>
<p>Both of the artificial variables have been driven out of the basis, thus reducing the
</p>
<p>value of the objective function to zero and leading to the basic feasible solution to
</p>
<p>the original problem
</p>
<p>x1 = 1/2, x2 = 0, x3 = 3/2.
</p>
<p>Using artificial variables, we attack a general linear programming problem by
</p>
<p>use of the two-phase method. This method consists simply of a phase I in which
</p>
<p>artificial variables are introduced as above and a basic feasible solution is found
</p>
<p>(or it is determined that no feasible solutions exist); and a phase Π in which, using
</p>
<p>the basic feasible solution resulting from phase I, the original objective function
</p>
<p>is minimized. During phase II the artificial variables and the objective function of
</p>
<p>phase I are omitted. Of course, in phase I artificial variables need be introduced only
</p>
<p>in those equations that do not contain slack variables.
</p>
<p>Example 2. Consider the problem
</p>
<p>minimize 4x1 + x2 + x3
subject to 2x1 + x2 + 2x3 = 4
</p>
<p>3x1 + 3x2 + x3 = 3
</p>
<p>x1 � 0, x2 � 0, x3 � 0.
</p>
<p>There is no basic feasible solution apparent, so we use the two-phase method. The
</p>
<p>first phase was done in Example 1 for these constraints, so we shall not repeat it
</p>
<p>here. We give only the final tableau with the columns corresponding to the artificial
</p>
<p>variables deleted, since they are not used in phase II. We use the new cost function
</p>
<p>in place of the old one. Temporarily writing cT in the bottom row we have
</p>
<p>x1 x2 x3 b
</p>
<p>0 &minus;3/4 1 3/2
1 5/4 0 1/2
</p>
<p>cT 4 1 1 0
</p>
<p>Initial tableau</p>
<p/>
</div>
<div class="page"><p/>
<p>52 3 The Simplex Method
</p>
<p>Transforming the last row so that zeros appear in the basic columns, we have
</p>
<p>and hence the optimal solution is x1 = 0, x2 = 2/5, x3 = 9/5.
</p>
<p>Example 3 (A Free Variable Problem).
</p>
<p>minimize &minus;2x1 + 4x2 + 7x3 + x4 + 5x5
subject to &minus;x1 + x2 + 2x3 + x4 + 2x5 = 7
</p>
<p>&minus;x1 + 2x2 + 3x3 + x4 + x5 = 6
&minus;x1 + x2 + x3 + 2x4 + x5 = 4
x1 free, x2 � 0, x3 � 0, x4 � 0, x5 � 0.
</p>
<p>Since x1 is free, it can be eliminated, as described in Chap. 2, by solving for x1
in terms of the other variables from the first equation and substituting everywhere
</p>
<p>else. This can all be done with the simplex tableau as follows:
</p>
<p>x1 x2 x3 x4 x5 b
</p>
<p>&minus;➀ 1 2 1 2 7
&minus;1 2 3 1 1 6
&minus;1 1 1 2 1 4
</p>
<p>cT &minus;2 4 7 1 5 0
Initial tableau
</p>
<p>We select any nonzero element in the first column to pivot on&mdash;this will eliminate x1.
</p>
<p>We now save the first row for future reference, but our linear program only in-
</p>
<p>volves the sub-tableau indicated. There is no obvious basic feasible solution for this
</p>
<p>problem, so we introduce artificial variables x6 and x7.
</p>
<p>x2 x3 x4, x5 x6 x7 b
</p>
<p>&minus;1 &minus;1 0 1 1 0 1
0 1 &minus;1 1 0 1 3
</p>
<p>cT 0 0 0 0 1 1 0
</p>
<p>Initial tableau for phase I</p>
<p/>
</div>
<div class="page"><p/>
<p>3.5 Finding a Basic Feasible Solution 53
</p>
<p>Transforming the last row appropriately we obtain
</p>
<p>x2 x3 x4 x5 x6 x7 b
</p>
<p>&minus;1 &minus;1 0 ➀ 1 0 1
0 1 &minus;1 1 0 1 3
</p>
<p>rT 1 0 1 &minus;2 0 0 &minus;4
First tableau&mdash;phase I
</p>
<p>x2 x3 x4 x5 x6 x7 b
</p>
<p>&minus;1 &minus;1 0 1 1 0 1
➀ 2 &minus;1 0 &minus;1 1 2
&minus;1 &minus;2 1 0 2 0 &minus;2
</p>
<p>Second tableau&mdash;phase I
</p>
<p>0 1 &minus;1 1 0 1 3
1 2 &minus;1 0 &minus;1 1 2
0 0 0 0 1 1 0
</p>
<p>Final tableau&mdash;phase I
</p>
<p>Now we go back to the equivalent reduced problem
</p>
<p>x2 x3 x4 x5 b
</p>
<p>0 1 &minus;1 1 3
1 2 &minus;1 0 2
</p>
<p>cT 2 3 &minus;1 1 &minus;14
Initial tableau&mdash;phase II
</p>
<p>Transforming the last row appropriately we proceed with:
</p>
<p>0 1 &minus;1 1 3
0 ➁ &minus;1 0 2
0 &minus;2 2 0 &minus;21
</p>
<p>First tableau&mdash;phase II
</p>
<p>&minus;1/2 0 &minus;1/2 1 2
1/2 1 &minus;1/2 0 1
1 0 1 0 &minus;19
</p>
<p>Final tableau&mdash;phase II
</p>
<p>The solution x3 = 1, x5 = 2 can be inserted in the expression for x1 giving
</p>
<p>x1 = &minus;7 + 2 &middot; 1 + 2 &middot; 2 = &minus;1;
</p>
<p>thus the final solution is
</p>
<p>x1 = &minus;1, x2 = 0, x3 = 1, x4 = 0, x5 = 2.</p>
<p/>
</div>
<div class="page"><p/>
<p>54 3 The Simplex Method
</p>
<p>3.6 Matrix Form of the Simplex Method
</p>
<p>Although the elementary pivot transformations associated with the simplex method
</p>
<p>are in many respects most easily discernible in the tableau format, with attention
</p>
<p>focused on the individual elements, there is much insight to be gained by studying
</p>
<p>a matrix interpretation of the procedure. The vector-matrix relationships that exist
</p>
<p>between the various rows and columns of the tableau lead, however, not only to
</p>
<p>increased understanding but also, in a rather direct way, to the revised simplex pro-
</p>
<p>cedure which in many cases can result in considerable computational advantage.
</p>
<p>The matrix formulation is also a natural setting for the discussion of dual linear
</p>
<p>programs and other topics related to linear programming.
</p>
<p>A preliminary observation in the development is that the tableau at any point in
</p>
<p>the simplex procedure can be determined solely by a knowledge of which variables
</p>
<p>are basic. As before we denote by B the submatrix of the original A matrix consist-
</p>
<p>ing of the m columns of A corresponding to the basic variables. These columns are
</p>
<p>linearly independent and hence the columns of B form a basis for Em. We refer to B
</p>
<p>as the basis matrix.
</p>
<p>As usual, let us assume that B consists of the first m columns of A. Then by
</p>
<p>partitioning A, x, and cT as
</p>
<p>A = [B, D]
</p>
<p>x = (xB, xD), c
T =
</p>
<p>[
</p>
<p>cTB, c
T
D
</p>
<p>]
</p>
<p>,
</p>
<p>the standard linear program becomes
</p>
<p>minimize cTBxB + c
T
DxD
</p>
<p>subject to BxB + DxD = b (3.28)
</p>
<p>xB � 0, xD � 0.
</p>
<p>The basic solution, which we assume is also feasible, corresponding to the basis
</p>
<p>B is x = (xB, 0) where xB = B
&minus;1b. The basic solution results from setting xD = 0.
</p>
<p>However, for any value of xD the necessary value of xB can be computed from
</p>
<p>(3.28) as
xB = B
</p>
<p>&minus;1b &minus; B&minus;1DxD, (3.29)
and this general expression when substituted in the cost function yields
</p>
<p>z = cTB(B
&minus;1b &minus; B&minus;1DxD) + cTDxD
</p>
<p>= cTBB
&minus;1b + (cTD &minus; cTBB&minus;1D)xD, (3.30)
</p>
<p>which expresses the cost of any solution to (3.28) in terms of xD. Thus
</p>
<p>rTD = c
T
D &minus; cTBB&minus;1D (3.31)
</p>
<p>is the relative cost vector (for nonbasic variables). It is the components of this vector
</p>
<p>that are used to determine which vector to bring into the basis.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.6 Matrix Form of the Simplex Method 55
</p>
<p>Having derived the vector expression for the relative cost it is now possible to
</p>
<p>write the simplex tableau in matrix form. The initial tableau takes the form
⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>A � b
</p>
<p>&minus;&minus; | &minus;&minus;
cT � 0
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>=
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>B � D � b
</p>
<p>&minus;&minus; | &minus;&minus; | &minus;&minus;
cT
</p>
<p>B
� cT
</p>
<p>D
� 0
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>, (3.32)
</p>
<p>which is not in general in canonical form and does not correspond to a point in the
</p>
<p>simplex procedure. If the matrix B is used as a basis, then the corresponding tableau
</p>
<p>becomes
</p>
<p>T =
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>I � B&minus;1D � B&minus;1b
&minus;&minus; | &minus; &minus; &minus; &minus; &minus; &minus; &minus; | &minus; &minus; &minus; &minus; &minus;
0 � cT
</p>
<p>D
&minus; CTBB&minus;1D � &minus;cTBB&minus;1b
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>, (3.33)
</p>
<p>which is the matrix form we desire.
</p>
<p>&lowast;The Revised Simplex Method and LU Decomposition
</p>
<p>Extensive experience with the simplex procedure applied to problems from various
</p>
<p>fields, and having various values of n and m, has indicated that the method can be
</p>
<p>expected to converge to an optimum solution in about m, or perhaps 3m/2, pivot
</p>
<p>operations. (Except in the worst case. See Chap. 5.) Thus, particularly if m is much
</p>
<p>smaller than n, that is, if the matrix A has far fewer rows than columns, pivots will
</p>
<p>occur in only a small fraction of the columns during the course of optimization.
</p>
<p>Since the other columns are not explicitly used, it appears that the work expended
</p>
<p>in calculating the elements in these columns after each pivot is, in some sense,
</p>
<p>wasted effort. The revised simplex method is a scheme for ordering the compu-
</p>
<p>tations required of the simplex method so that unnecessary calculations are avoided.
</p>
<p>In fact, even if pivoting is eventually required in all columns, but m is small com-
</p>
<p>pared to n, the revised simplex method can frequently save computational effort.
</p>
<p>The revised form of the simplex method is this: Given the inverse B&minus;1 of a current
basis, and the current solution xB = ā0 = B
</p>
<p>&minus;1b,
</p>
<p>Step 1. Calculate the current relative cost coefficients rT
D
</p>
<p>= cT
D
&minus; cT
</p>
<p>B
B&minus;1D. This
</p>
<p>can best be done by first calculating yT = cT
B
</p>
<p>B&minus;1 and then the relative cost vector
rT
</p>
<p>D
= cT
</p>
<p>D
&minus; yTD. If rD � 0 stop; the current solution is optimal.
</p>
<p>Step 2. Determine which vector aq is to enter the basis by selecting the most
</p>
<p>negative cost coefficient; and calculate āq = B
&minus;1aq which gives the vector aq
</p>
<p>expressed in terms of the current basis.
</p>
<p>Step 3. If no āiq &gt; 0, stop; the problem is unbounded. Otherwise, calculate the
</p>
<p>ratios āi0/āiq for āiq &gt; 0 to determine which vector is to leave the basis.
</p>
<p>Step 4. Update B&minus;1 and the current solution B&minus;1b. Return to Step 1.
</p>
<p>Updating of B&minus;1 is accomplished by the usual pivot operations applied to an array
consisting of B&minus;1 and āq, where the pivot is the appropriate element in āq. Of course
B&minus;1b may be updated at the same time by adjoining it as another column.</p>
<p/>
</div>
<div class="page"><p/>
<p>56 3 The Simplex Method
</p>
<p>One may go one step further in the matrix interpretation of the simplex method
</p>
<p>and note that execution of a single simplex cycle is not explicitly dependent on
</p>
<p>having B&minus;1 but rather on the ability to solve linear systems with B as the coefficient
matrix. A decomposition of B = LU can be updated where L is a lower triangular
</p>
<p>matrix and U is an upper triangular matrix. Then each of the linear systems can be
</p>
<p>solved by solving two triangular systems.
</p>
<p>3.7 Simplex Method for Transportation Problems
</p>
<p>The transportation problem was stated briefly in Chap. 2. We restate it here. There
</p>
<p>are m origins that contain various amounts of a commodity that must be shipped to n
</p>
<p>destinations to meet demand requirements. Specifically, origin i contains an amount
</p>
<p>ai, and destination j has a requirement of amount b j. It is assumed that the system
</p>
<p>is balanced in the sense that total supply equals total demand. That is,
</p>
<p>m
&sum;
</p>
<p>i=1
</p>
<p>ai =
</p>
<p>n
&sum;
</p>
<p>j=1
</p>
<p>b j. (3.34)
</p>
<p>The numbers ai and b j, i = 1, 2, . . . , m; j = 1, 2, . . . , n, are assumed to be non-
</p>
<p>negative, and in many applications they are in fact nonnegative integers. There is a
</p>
<p>unit cost ci j associated with the shipping of the commodity from origin i to destina-
</p>
<p>tion j. The problem is to find the shipping pattern between origins and destinations
</p>
<p>that satisfies all the requirements and minimizes the total shipping cost.
</p>
<p>In mathematical terms the above problem can be expressed as finding a set of xi j&rsquo;
</p>
<p>s, i = 1, 2, . . . , m; j = 1, 2, . . . , n, to
</p>
<p>minimize
</p>
<p>m
&sum;
</p>
<p>i=1
</p>
<p>n
&sum;
</p>
<p>j=1
</p>
<p>ci jxi j
</p>
<p>subject to
</p>
<p>n
&sum;
</p>
<p>j=1
</p>
<p>xi j = ai for i = 1, 2, . . . ,m (3.35)
</p>
<p>m
&sum;
</p>
<p>i=1
</p>
<p>xi j = b j for j = 1, 2, . . . , n
</p>
<p>xi j � 0 for all i and j.
</p>
<p>This mathematical problem, together with the assumption (3.34), is the general
</p>
<p>transportation problem. In the shipping context, the variables xi j represent the
</p>
<p>amounts of the commodity shipped from origin i to destination j.
The structure of the problem can be seen more clearly by writing the constraint
</p>
<p>equations in standard form:</p>
<p/>
</div>
<div class="page"><p/>
<p>3.7 Simplex Method for Transportation Problems 57
</p>
<p>x11 + x12 + &middot; &middot; &middot; + x1n = a1
x21 + x22 + &middot; &middot; &middot; + x2n = a2
</p>
<p>...
</p>
<p>xm1 + xm2 + &middot; &middot; &middot; + xmn = am
x11 + x21 xm1 = b1
</p>
<p>x12 + x22 + xm2 = b2
</p>
<p>... (3.36)
</p>
<p>x1n + x2n + xmn = bn
</p>
<p>The structure is perhaps even more evident when the coefficient matrix A of the
</p>
<p>system of equations above is expressed in vector-matrix notation as
</p>
<p>A =
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>1T
</p>
<p>1T
</p>
<p>. . .
</p>
<p>1T
</p>
<p>I I &middot; &middot; &middot; I
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>, (3.37)
</p>
<p>where 1 = (1, 1, . . . , 1) is n-dimensional, and where each I is an n&times;n identity matrix.
In practice it is usually unnecessary to write out the constraint equations of the
</p>
<p>transportation problem in the explicit form (3.36). A specific transportation problem
</p>
<p>is generally defined by simply presenting the data in compact form, such as:
</p>
<p>a = (a1, a2, . . . , am)
</p>
<p>C =
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>c11 c12 &middot; &middot; &middot; c1n
c21 c22 &middot; &middot; &middot; c2n
cm1 cm2 &middot; &middot; &middot; cmn
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>.
</p>
<p>b = (b1, b2, . . . , bn)
</p>
<p>The solution can also be represented by an m &times; n array, and as we shall see, all
computations can be made on arrays of a similar dimension.
</p>
<p>Example 1. As an example, which will be solved completely in a later section, a
</p>
<p>specific transportation problem with four origins and five destinations is defined by
</p>
<p>a = (30, 80, 10, 60)
</p>
<p>C =
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>3 4 6 8 9
</p>
<p>2 2 4 5 5
</p>
<p>2 2 2 3 2
</p>
<p>3 3 2 4 2
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>.
</p>
<p>b = (10, 50, 20, 80, 20)
</p>
<p>Note that the balance requirement is satisfied, since the sum of the supply and the
</p>
<p>demand are both 180.</p>
<p/>
</div>
<div class="page"><p/>
<p>58 3 The Simplex Method
</p>
<p>Finding a Basic Feasible Solution
</p>
<p>A first step in the study of the structure of the transportation problem is to show
</p>
<p>that there is always a feasible solution, thus establishing that the problem is well
</p>
<p>defined. A feasible solution can be found by allocating shipments from origins to
</p>
<p>destinations in proportion to supply and demand requirements. Specifically, let S
</p>
<p>be equal to the total supply (which is also equal to the total demand). Then let
</p>
<p>xi j = aib j/S for i = 1, 2, . . . , m; j = 1, 2, . . . , n. The reader can easily verify that
</p>
<p>this is a feasible solution. We also note that the solutions are bounded, since each
</p>
<p>xi j is bounded by ai (and by b j). A bounded program with a feasible solution has an
</p>
<p>optimal solution. Thus, a transportation problem always has an optimal solution.
</p>
<p>A second step in the study of the structure of the transportation problem is based
</p>
<p>on a simple examination of the constraint equations. Clearly there are m equations
</p>
<p>corresponding to origin constraints and n equations corresponding to destination
</p>
<p>constraints&mdash;a total of n + m. However, it is easily noted that the sum of the origin
</p>
<p>equations is
</p>
<p>m
&sum;
</p>
<p>i=1
</p>
<p>n
&sum;
</p>
<p>j=1
</p>
<p>xi j =
</p>
<p>m
&sum;
</p>
<p>i=1
</p>
<p>ai, (3.38)
</p>
<p>and the sum of the destination equations is
</p>
<p>n
&sum;
</p>
<p>j=1
</p>
<p>m
&sum;
</p>
<p>i=1
</p>
<p>xi j =
</p>
<p>n
&sum;
</p>
<p>j=1
</p>
<p>b j. (3.39)
</p>
<p>The left-hand sides of these equations are equal. Since they were formed by two dis-
</p>
<p>tinct linear combinations of the original equations, it follows that the equations in the
</p>
<p>original system are not independent. The right-hand sides of (3.38) and (3.39) are
</p>
<p>equal by the assumption that the system is balanced, and therefore the two equations
</p>
<p>are, in fact, consistent. However, it is clear that the original system of equations is
</p>
<p>redundant. This means that one of the constraints can be eliminated without chang-
</p>
<p>ing the set of feasible solutions. Indeed, any one of the constraints can be chosen
</p>
<p>as the one to be eliminated, for it can be reconstructed from those remaining. It fol-
</p>
<p>lows that a basis for the transportation problem consists of m + n &minus; 1 vectors, and
a nondegenerate basic feasible solution consists of m + n &minus; 1 variables. The simple
solution found earlier in this section is clearly not a basic solution.
</p>
<p>There is a straightforward way to compute an initial basic feasible solution to
</p>
<p>a transportation problem. The method is worth studying at this stage because it
</p>
<p>introduces the computational process that is the foundation for the general solution
</p>
<p>technique based on the simplex method. It also begins to illustrate the fundamental
</p>
<p>property of the structure of transportation problems.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.7 Simplex Method for Transportation Problems 59
</p>
<p>The Northwest Corner Rule
</p>
<p>This procedure is conducted on the solution array shown below:
</p>
<p>x11 x12 x13 &middot; &middot; &middot; x1n a1
x21 x22 x23 &middot; &middot; &middot; x2n a2
...
</p>
<p>...
</p>
<p>xm1 xm2 xm3 &middot; &middot; &middot; xmn am
b1 b2 b3 &middot; &middot; &middot; bn
</p>
<p>(3.40)
</p>
<p>The individual elements of the array appear in cells and represent a solution. An
</p>
<p>empty cell denotes a value of zero.
</p>
<p>Beginning with all empty cells, the procedure is given by the following steps:
</p>
<p>Step 1. Start with the cell in the upper left-hand corner.
</p>
<p>Step 2. Allocate the maximum feasible amount consistent with row and column
</p>
<p>sum requirements involving that cell. (At least one of these requirements will
</p>
<p>then be met.)
</p>
<p>Step 3. Move one cell to the right if there is any remaining row requirement (sup-
</p>
<p>ply). Otherwise move one cell down. If all requirements are met, stop; otherwise
</p>
<p>go to Step 2.
</p>
<p>The procedure is called the Northwest Corner Rule because at each step it selects
</p>
<p>the cell in the upper left-hand corner of the subarray consisting of current nonzero
</p>
<p>row and column requirements.
</p>
<p>Example 1. A basic feasible solution constructed by the Northwest corner Rule is
</p>
<p>shown below for Example 1 of the last section.
</p>
<p>10 20 30
</p>
<p>30 20 30 80
</p>
<p>10 10
</p>
<p>40 20 60
</p>
<p>10 50 20 80 20
</p>
<p>(3.41)
</p>
<p>In the first step, at the upper left-hand corner, a maximum of 10 units could be
</p>
<p>allocated, since that is all that was required by column 1. This left 30 &minus; 10 = 20
units required in the first row. Next, moving to the second cell in the top row, the
</p>
<p>remaining 20 units were allocated. At this point the row 1 requirement is met, and
</p>
<p>it is necessary to move down to the second row. The reader should be able to follow
</p>
<p>the remaining steps easily.
</p>
<p>There is the possibility that at some point both the row and column requirements
</p>
<p>corresponding to a cell may be met. The next entry will then be a zero, indicating a
</p>
<p>degenerate basic solution. In such a case there is a choice as to where to place the
</p>
<p>zero. One can either move right or move down to enter the zero. Two examples of
</p>
<p>degenerate solutions to a problem are shown below:</p>
<p/>
</div>
<div class="page"><p/>
<p>60 3 The Simplex Method
</p>
<p>30 30
</p>
<p>20 20 40
</p>
<p>0 20 20
</p>
<p>20 40 60
</p>
<p>50 20 40 40
</p>
<p>30 30
</p>
<p>20 20 0 40
</p>
<p>20 20
</p>
<p>20 40 60
</p>
<p>50 20 40 40
</p>
<p>It should be clear that the Northwest Corner Rule can be used to obtain different
</p>
<p>basic feasible solutions by first permuting the rows and columns of the array before
</p>
<p>the procedure is applied. Or equivalently, one can do this indirectly by starting the
</p>
<p>procedure at an arbitrary cell and then considering successive rows and columns in
</p>
<p>an arbitrary order.
</p>
<p>Basis Triangularity
</p>
<p>We now establish the most important structural property of the transportation prob-
</p>
<p>lem: the triangularity of all bases. This property simplifies the process of solution
</p>
<p>of a system of equations whose coefficient matrix corresponds to a basis, and thus
</p>
<p>leads to efficient implementation of the simplex method.
</p>
<p>The concept of upper and lower triangular matrices was introduced in connection
</p>
<p>with Gaussian elimination methods, see Appendix C. It is useful at this point to
</p>
<p>generalize slightly the notion of upper and lower triangularity.
</p>
<p>Definition. A nonsingular square matrix M is said to be triangular if by a permutation of
its rows and columns it can be put in the form of a lower triangular matrix.
</p>
<p>There is a simple and useful procedure for determining whether a given matrix
</p>
<p>M is triangular:
</p>
<p>Step 1. Find a row with exactly one nonzero entry.
</p>
<p>Step 2. Form a submatrix of the matrix used in Step 1 by crossing out the row
</p>
<p>found in Step 1 and the column corresponding to the nonzero entry in that row.
</p>
<p>Return to Step 1 with this submatrix.
</p>
<p>If this procedure can be continued until all rows have been eliminated, then the
</p>
<p>matrix is triangular. It can be put in lower triangular form explicitly by arranging
</p>
<p>the rows and columns in the order that was determined by the procedure.
</p>
<p>Example 1. Shown below on the left is a matrix before the above procedure is ap-
</p>
<p>plied to it. Indicated along the edges of this matrix is the order in which the rows
</p>
<p>and columns are indexed according to the procedure. Shown at the right is the same
</p>
<p>matrix when its rows and columns are permuted according to the order found.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.7 Simplex Method for Transportation Problems 61
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>1 2 0 1 0 2
</p>
<p>4 1 0 5 0 0
</p>
<p>0 0 0 4 0 0
</p>
<p>2 1 7 2 1 3
</p>
<p>2 3 2 0 0 3
</p>
<p>0 2 0 1 0 0
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>3
</p>
<p>6
</p>
<p>2
</p>
<p>1
</p>
<p>5
</p>
<p>4
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>4 0 0 0 0 0
</p>
<p>1 2 0 0 0 0
</p>
<p>5 1 4 0 0 0
</p>
<p>1 2 1 2 0 0
</p>
<p>0 3 2 3 2 0
</p>
<p>2 1 2 3 7 1
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>4 2 1 6 3 5
</p>
<p>Triangularization
</p>
<p>We are now prepared to derive the most important structural property of the trans-
</p>
<p>portation problem.
</p>
<p>Basis Triangularity Theorem. Every basis of the transportation problem is triangular.
</p>
<p>Proof. Refer to the system of constraints (3.36). Let us change the sign of the top
</p>
<p>half of the system; then the coefficient matrix of the system consists of entries that
</p>
<p>are either +1, &minus;1, or 0. Following the result of the theorem in Sect. 3.7, delete any
one of the equations to eliminate the redundancy. From the resulting coefficient
</p>
<p>matrix, form a basis B by selecting a nonsingular subset of m + n &minus; 1 columns.
Each column of B contains at most two nonzero entries, a + 1 and a &minus; 1. Thus
</p>
<p>there are at most 2(m+n&minus;1) nonzero entries in the basis. However, if every column
contained two nonzero entries, then the sum of all rows would be zero, contradict-
</p>
<p>ing the nonsingularity of B. Thus at least one column of B must contain only one
</p>
<p>nonzero entry. This means that the total number of nonzero entries in B is less than
</p>
<p>2(m + n &minus; 1). It then follows that there must be a row with only one nonzero entry;
for if every row had two or more nonzero entries, the total number would be at least
</p>
<p>2(m+ n&minus; 1). This means that the first step of the procedure for verifying triangular-
ity is satisfied. A similar argument can be applied to the submatrix of B obtained by
</p>
<p>crossing out the row with the single nonzero entry and the column corresponding to
</p>
<p>that entry; that submatrix must also contain a row with a single nonzero entry. This
</p>
<p>argument can be continued, establishing that the basis B is triangular. �
</p>
<p>Example 2. As an illustration of the Basis Triangularity Theorem, consider the ba-
</p>
<p>sis selected by the Northwest Corner Rule in Example 1. This basis is represented
</p>
<p>below, except that only the basic variables are indicated, not their values.
</p>
<p>x11 x12 30
</p>
<p>x22 x23 x24 80
</p>
<p>x34 10
</p>
<p>x44 x45 60
</p>
<p>10 50 20 80 20
</p>
<p>A row in a basis matrix corresponds to an equation in the original system and is
</p>
<p>associated with a constraint either on a row or column sum in the solution array. In
</p>
<p>this example the equation corresponding to the first column sum contains only one</p>
<p/>
</div>
<div class="page"><p/>
<p>62 3 The Simplex Method
</p>
<p>basis variable, x11. The value of this variable can be found immediately to be 10.
</p>
<p>The next equation corresponds to the first row sum. The corresponding variable is
</p>
<p>x12, which can be found to be 20, since x11 is known. Progression in this manner
</p>
<p>through the basis variables is equivalent to back substitution.
</p>
<p>The importance of triangularity is, of course, the associated method of back
</p>
<p>substitution for the solution of a triangular system of equations, as discussed in
</p>
<p>Appendix C. Moreover, since any basis matrix is triangular and all nonzero ele-
</p>
<p>ments are equal to one (or minus one if the signs of some equations are changed), it
</p>
<p>follows that the process of back substitution will simply involve repeated additions
</p>
<p>and subtractions of the given row and column sums. No multiplication is required. It
</p>
<p>therefore follows that if the original row and column totals are integers, the values of
</p>
<p>all basic variables will be integers. This is an important result, which we summarize
</p>
<p>by a corollary to the Basis Triangularity Theorem.
</p>
<p>Corollary. If the row and column sums of a transportation problem are integers, then the
</p>
<p>basic variables in any basic solution are integers.
</p>
<p>The Transportation Simplex Method
</p>
<p>Now that the structural properties of the transportation problem have been devel-
</p>
<p>oped, it is a relatively straightforward task to work out the details of the simplex
</p>
<p>method for the transportation problem. A major objective is to exploit fully the tri-
</p>
<p>angularity property of bases in order to achieve both computational efficiency and
</p>
<p>a compact representation of the method. The method used is actually a direct adap-
</p>
<p>tation of the version of the revised simplex method presented in the first part of
</p>
<p>Sect. 3.6. The basis is never inverted; instead, its triangular form is used directly to
</p>
<p>solve for all required variables.
</p>
<p>Simplex Multipliers
</p>
<p>Simplex multipliers are associated with the constraint equations. In this case we
</p>
<p>partition the vector of multipliers as y = (u, v). Here, ui represents the multiplier
</p>
<p>associated with the ith row sum constraint, and v j represents the multiplier associ-
</p>
<p>ated with the jth column sum constraint. Since one of the constraints is redundant,
</p>
<p>an arbitrary value may be assigned to any one of the multipliers (see Exercise 4,
</p>
<p>Chap. 4). For notational simplicity we shall at this point set vn = 0.
</p>
<p>Given a basis B, the simplex multipliers are found to be the solution to the
</p>
<p>equation yTB = cT
B
</p>
<p>. To determine the explicit form of these equations, we again
</p>
<p>refer to the original system of constraints (3.36). If xi j is basic, then the correspond-
</p>
<p>ing column from A will be included in B. This column has exactly two +1 entries:
</p>
<p>one in the ith position of the top portion and one in the jth position of the bottom</p>
<p/>
</div>
<div class="page"><p/>
<p>3.7 Simplex Method for Transportation Problems 63
</p>
<p>portion. This column thus generates the simplex multiplier equation ui + v j = ci j,
</p>
<p>since ui and v j are the corresponding components of the multiplier vector. Overall,
</p>
<p>the simplex multiplier equations are
</p>
<p>ui + v j = ci j, (3.42)
</p>
<p>for all i, j for which xi j is basic. The coefficient matrix of this system is the transpose
</p>
<p>of the basis matrix and hence it is triangular. Thus, this system can be solved by back
</p>
<p>substitution. This is similar to the procedure for finding the values of basic variables
</p>
<p>and, accordingly, as another corollary of the Triangular Basis Theorem, an integer
</p>
<p>property holds for simplex multipliers.
</p>
<p>Corollary. If the unit costs ci j of a transportation problem are all integers, then (assuming
</p>
<p>one simplex multiplier is set arbitrarily equal to an integer) the simplex multipliers associ-
</p>
<p>ated with any basis are integers.
</p>
<p>Once the simplex multipliers are known, the relative cost coefficients for nonba-
</p>
<p>sic variables can be found in the usual manner as rT
D
= cT
</p>
<p>D
&minus; yTD. In this case the
</p>
<p>relative cost coefficients are
</p>
<p>ri j = ci j &minus; ui &minus; v j for i = 1, 2, . . . ,m
j = 1, 2, . . . , n. (3.43)
</p>
<p>This relation is valid for basic variables as well if we define relative cost coefficients
</p>
<p>for them&mdash;having value zero.
</p>
<p>Given a basis, computation of the simplex multipliers is quite similar to the cal-
</p>
<p>culation of the values of the basic variables. The calculation is easily carried out
</p>
<p>on an array of the form shown below, where the circled elements correspond to the
</p>
<p>positions of the basic variables in the current basis.
</p>
<p>In this case the main part of the array, with the coefficients ci j, remains fixed, and
</p>
<p>we calculate the extra column and row corresponding to u and v.
</p>
<p>The procedure for calculating the simplex multipliers is this:
</p>
<p>Step 1. Assign an arbitrary value to any one of the multipliers.
</p>
<p>Step 2. Scan the rows and columns of the array until a circled element ci j is found
</p>
<p>such that either ui or v j (but not both) has already been determined.
</p>
<p>Step 3. Compute the undetermined ui or v j from the equation ci j = ui + v j. If all
</p>
<p>multipliers are determined, stop. Otherwise, return to Step 2.
</p>
<p>The triangularity of the basis guarantees that this procedure can be carried
</p>
<p>through to determine all the simplex multipliers.</p>
<p/>
</div>
<div class="page"><p/>
<p>64 3 The Simplex Method
</p>
<p>Example 1. Consider the cost array of Example 1 of Sect. 5.1, which is shown below
</p>
<p>with the circled elements corresponding to a basic feasible solution (found by the
</p>
<p>Northwest Corner Rule). Only these numbers are used in the calculation of the
</p>
<p>multipliers.
⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>➂ ➃ 6 8 9
2 ➁ ➃ ➄ 5
2 2 2 ➂ 2
3 3 2 ➃ ➁
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>.
</p>
<p>We first arbitrarily set v5 = 0. We then scan the cells, searching for a circled element
</p>
<p>for which only one multiplier must be determined. This is the bottom right corner
</p>
<p>element, and it gives u4 = 2. Then, from the equation 4 = 2+ v4, v4 is found to be 2.
</p>
<p>Next, u3 and u2 are determined, then v3 and v2, and finally u1 and v1. The result is
</p>
<p>shown below:
</p>
<p>Cycle of Change
</p>
<p>In accordance with the general simplex procedure, if a nonbasic variable has an
</p>
<p>associated relative cost coefficient that is negative, then that variable is a candidate
</p>
<p>for entry into the basis. As the value of this variable is gradually increased, the
</p>
<p>values of the current basic variables will change continuously in order to maintain
</p>
<p>feasibility. Then, as usual, the value of the new variable is increased precisely to the
</p>
<p>point where one of the old basic variables is driven to zero.
</p>
<p>We must work out the details of how the values of the current basic variables
</p>
<p>change as a new variable is entered. If the new basic vector is d, then the change
</p>
<p>in the other variables is given by &minus;B&minus;1d, where B is the current basis. Hence, once
again we are faced with a problem of solving a system associated with the triangular
</p>
<p>basis, and once again the solution has special properties. In the next theorem recall
</p>
<p>that A is defined by (3.37).
</p>
<p>Theorem. Let B be a basis from A (ignoring one row), and let d be another column. Then
</p>
<p>the components of the vector w = B&minus;1d are either 0, +1, or &minus;1.
</p>
<p>Proof. Let w be the solution to the equation Bw = d. Then w is the representation
</p>
<p>of d in terms of the basis. This equation can be solved by Cramer&rsquo;s rule as
</p>
<p>wk =
det Bk
</p>
<p>det B
,</p>
<p/>
</div>
<div class="page"><p/>
<p>3.7 Simplex Method for Transportation Problems 65
</p>
<p>where Bk is the matrix obtained by replacing the kth column of B by d. Both B and
</p>
<p>Bk are submatrices of the original constraint matrix A. The matrix B may be put
</p>
<p>in triangular form with all diagonal elements equal to +1. Hence, accounting for
</p>
<p>the sign change that may result from the combined row and column interchanges,
</p>
<p>det B = +1 or &minus;1. Likewise, it can be shown (see Exercise 3) that det Bk = 0,+1,
or &minus;1. We conclude that each component of w is either 0, +1, or &minus;1. �
</p>
<p>The implication of the above result is that when a new variable is added to the
</p>
<p>solution at a unit level, the current basic variables will each change by +1, &minus;1, or 0.
If the new variable has a value θ, then, correspondingly, the basic variables change
</p>
<p>by +θ, &minus;θ, or 0. It is therefore only necessary to determine the signs of change for
each basic variable.
</p>
<p>The determination of these signs is again accomplished by row and column scan-
</p>
<p>ning. Operationally, one assigns a + to the cell of the entering variable to represent
</p>
<p>a change of +θ, where θ is yet to be determined. Then +&rsquo;s, &minus;&rsquo;s, and 0&rsquo;s are assigned,
one by one, to the cells of some basic variables, indicating changes of +θ, &minus;θ, or
0 to maintain a solution. As usual, after each step there will always be an equation
</p>
<p>that uniquely determines the sign to be assigned to another basic variable. The result
</p>
<p>will be a sequence of pluses and minuses assigned to cells that form a cycle leading
</p>
<p>from the cell of the entering variable back to that cell. In essence, the new change is
</p>
<p>part of a cycle of redistribution of the commodity flow in the transportation system.
</p>
<p>Once the sequence of +&rsquo;s, &minus;&rsquo;s, and 0&rsquo;s is determined, the new basic feasible
solution is found by setting the level of the change θ. This is set so as to drive one
</p>
<p>of the old basic variables to zero. One must simply examine those basic variables
</p>
<p>for which a minus sign has been assigned, for these are the ones that will decrease
</p>
<p>as the new variable is introduced. Then θ is set equal to the smallest magnitude of
</p>
<p>these variables. This value is added to all cells that have a + assigned to them and
</p>
<p>subtracted from all cells that have a &minus; assigned. The result will be the new basic
feasible solution.
</p>
<p>The procedure is illustrated by the following example.
</p>
<p>Example 2. A completed solution array is shown below:
</p>
<p>100 10
</p>
<p>20&minus; 10+ 30
20+ 100 30&minus; 60
100 10
</p>
<p>10&minus; + 400 50
40 10 30 40 40
</p>
<p>In this example x53 is the entering variable, so a plus sign is assigned there. The
</p>
<p>signs of the other cells were determined in the order x13, x23, x25, x35, x32, x31, x41,
</p>
<p>x51, x54. The smallest variable with a minus assigned to it is x51 = 10. Thus we set
</p>
<p>θ = 10.</p>
<p/>
</div>
<div class="page"><p/>
<p>66 3 The Simplex Method
</p>
<p>The Transportation Simplex Algorithm
</p>
<p>It is now possible to put together the components developed to this point in the form
</p>
<p>of a complete revised simplex procedure for the transportation problem. The steps
</p>
<p>are:
</p>
<p>Step 1. Compute an initial basic feasible solution using the Northwest Corner
</p>
<p>Rule or some other method.
</p>
<p>Step 2. Compute the simplex multipliers and the relative cost coefficients. If all
</p>
<p>relative cost coefficients are nonnegative, stop; the solution is optimal. Otherwise,
</p>
<p>go to Step 3.
</p>
<p>Step 3. Select a nonbasic variable corresponding to a negative cost coefficient to
</p>
<p>enter the basis (usually the one corresponding to the most negative cost coeffi-
</p>
<p>cient). Compute the cycle of change and set θ equal to the smallest basic variable
</p>
<p>with a minus assigned to it. Update the solution. Go to Step 2.
</p>
<p>Example 3. We can now completely solve the problem that was introduced in Exam-
</p>
<p>ple 1 of the first section. The requirements and a first basic feasible solution obtained
</p>
<p>by the Northwest Corner Rule are shown below. The plus and minus signs indicated
</p>
<p>on the array should be ignored at this point, since they cannot be computed until the
</p>
<p>next step is completed.
</p>
<p>10 20 30
</p>
<p>30 20&minus; 30+ 80
100 10
</p>
<p>+ 40&minus; 200 60
10 50 20 80 20
</p>
<p>The cost coefficients of the problem are shown in the array below, with the circled
</p>
<p>cells corresponding to the current basic variables. The simplex multipliers, com-
</p>
<p>puted by row and column scanning, are shown as well.
</p>
<p>➂ ➃ 6 8 9 5
2 ➁ ➃ ➄ 5 3
2 2 2 ➂ 2 1
3 3 2 ➃ ➁ 2
</p>
<p>&minus;2 &minus;1 1 2 0
The relative cost coefficients are found by subtracting u j + v j from ci j. In this case
</p>
<p>the only negative result is in cell 4,3; so variable x43 will be brought into the basis.
</p>
<p>Thus a + is entered into this cell in the original array, and the cycle of zeros and plus
</p>
<p>and minus signs is determined as shown in that array. (It is not necessary to continue
</p>
<p>scanning once a complete cycle is determined.)
</p>
<p>The smallest basic variable with a minus sign is 20 and, accordingly, 20 is added
</p>
<p>or subtracted from elements of the cycle as indicated by the signs. This leads to the
</p>
<p>new basic feasible solution shown in the array below:</p>
<p/>
</div>
<div class="page"><p/>
<p>3.7 Simplex Method for Transportation Problems 67
</p>
<p>10 20 30
</p>
<p>30 50 80
</p>
<p>10 10
</p>
<p>20 20 20 60
</p>
<p>10 50 20 80 20
</p>
<p>The new simplex multipliers corresponding to the new basis are computed, and
</p>
<p>the cost array is revised as shown below. In this case all relative cost coefficients are
</p>
<p>positive, indicating that the current solution is optimal.
</p>
<p>➂ ➃ 6 8 9 5
2 ➁ 4 ➄ 5 3
2 2 2 ➂ 2 1
3 3 ➁ ➃ ➁ 2
</p>
<p>&minus;2 &minus;1 0 2 0
</p>
<p>Degeneracy
</p>
<p>As in all linear programming problems, degeneracy, corresponding to a basic vari-
</p>
<p>able having the value zero, can occur in the transportation problem. If degeneracy
</p>
<p>is encountered in the simplex procedure, it can be handled quite easily by introduc-
</p>
<p>tion of the standard perturbation method (see Exercise 15, Chap. 3). In this method
</p>
<p>a zero-valued basic variable is assigned the value ε and is then treated in the usual
</p>
<p>way. If it later leaves the basis, then the ε can be dropped.
</p>
<p>Example 4. To illustrate the method of dealing with degeneracy, consider a modifi-
</p>
<p>cation of Example 3, with the fourth row sum changed from 60 to 20 and the fourth
</p>
<p>column sum changed from 80 to 40. Then the initial basic feasible solution found
</p>
<p>by the Northwest Corner Rule is degenerate. An ε is placed in the array for the
</p>
<p>zero-valued basic variable as shown below:
</p>
<p>10 20 30
</p>
<p>30 20&minus; 30+ 80
100 10
</p>
<p>+ ε&minus; 200 20
10 50 20 40 20
</p>
<p>The relative cost coefficients will be the same as in Example 3, and hence again
</p>
<p>x43 should be chosen to enter, and the cycle of change is the same as before. In
</p>
<p>this case, however, the change is only ε, and variable x44 leaves the basis. The new</p>
<p/>
</div>
<div class="page"><p/>
<p>68 3 The Simplex Method
</p>
<p>relative cost coefficients are all positive, indicating that the new solution is optimal.
</p>
<p>Now the ε can be dropped to yield the final solution (which is, itself, degenerate in
</p>
<p>this case).
</p>
<p>10 20 30
</p>
<p>30 20 30 80
</p>
<p>10 10
</p>
<p>ε 20 20
</p>
<p>10 50 20 40 20
</p>
<p>*3.8 Decomposition
</p>
<p>Large linear programming problems usually have some special structural form that
</p>
<p>can (and should) be exploited to develop efficient computational procedures. One
</p>
<p>common structure is where there are a number of separate activity areas that are
</p>
<p>linked through common resource constraints. An example is provided by a multidi-
</p>
<p>visional firm attempting to minimize the total cost of its operations. The divisions
</p>
<p>of the firm must each meet internal requirements that do not interact with the con-
</p>
<p>straints of other divisions; but in addition there are common resources that must be
</p>
<p>shared among divisions and thereby represent linking constraints.
</p>
<p>A problem of this form can be solved by the Dantzig-Wolfe decomposition
</p>
<p>method described in this section. The method is an iterative process where at each
</p>
<p>step a number of separate subproblems are solved. The subproblems are themselves
</p>
<p>linear programs within the separate areas (or within divisions in the example of
</p>
<p>the firm). The objective functions of these subproblems are varied from iteration to
</p>
<p>iteration and are determined by a separate calculation based on the results of the
</p>
<p>previous iteration. This action coordinates the individual subproblems so that, ulti-
</p>
<p>mately, the solution to the overall problem is solved. The method can be derived as
</p>
<p>a special version of the revised simplex method, where the subproblems correspond
</p>
<p>to evaluation of reduced cost coefficients for the main problem.
</p>
<p>To describe the method we consider the linear program in standard form
</p>
<p>minimize cTx
</p>
<p>subject to Ax = b, x � 0.
(3.44)
</p>
<p>Suppose, for purposes of this entire section, that the A matrix has the special &ldquo;block-
</p>
<p>angular&rdquo; structure:
</p>
<p>A =
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>L1 L2 &middot; &middot; &middot; LN
A1
</p>
<p>A2
. . .
</p>
<p>AN
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>(3.45)</p>
<p/>
</div>
<div class="page"><p/>
<p>3.8 Decomposition 69
</p>
<p>By partitioning the vectors x, cT , and b consistent with this partition of A, the
</p>
<p>problem can be rewritten as
</p>
<p>minimize
</p>
<p>N
&sum;
</p>
<p>i=1
</p>
<p>cTi xi
</p>
<p>subject to
</p>
<p>N
&sum;
</p>
<p>i=1
</p>
<p>Lixi = b0 (3.46)
</p>
<p>Aixi = bi
</p>
<p>xi � 0, i = 1, . . . , N.
</p>
<p>This may be viewed as a problem of minimizing the total cost of N different linear
</p>
<p>programs that are independent except for the first constraint, which is a linking
</p>
<p>constraint of, say, dimension m.
</p>
<p>Each of the subproblems is of the form
</p>
<p>minimize cTi xi
</p>
<p>subject to Aixi = bi, xi � 0.
</p>
<p>The constraint set for the ith subproblem is S i = {xi : Aixi = bi, xi � 0}. As
for any linear program, this constraint set S i is a polytope and can be expressed
</p>
<p>as the intersection of a finite number of closed half-spaces. There is no guarantee
</p>
<p>that each S i is bounded, even if the original linear program (3.44) has a bounded
</p>
<p>constraint set. We shall assume for simplicity, however, that each of the polytopes
</p>
<p>S i, i = 1, . . . , N is indeed bounded and hence is a polyhedron. One may guarantee
</p>
<p>that this assumption is satisfied by placing artificial (large) upper bounds on each xi.
</p>
<p>Under the boundedness assumption, each polyhedron S i consists entirely of
</p>
<p>points that are convex combinations of its extreme points. Thus, if the extreme points
</p>
<p>of S i are {xi1, xi2, . . . , xiKi }, then any point xi &isin; S i can be expressed in the form
</p>
<p>xi =
Ki
&sum;
</p>
<p>j=1
αijxij,
</p>
<p>where
Ki
&sum;
</p>
<p>j=1
αij = 1
</p>
<p>and αij � 0, j = 1, . . . , Ki.
</p>
<p>(3.47)
</p>
<p>The αij&rsquo;s are the weighting coefficients of the extreme points.
</p>
<p>We now convert the original linear program to an equivalent master problem,
</p>
<p>of which the objective is to find the optimal weighting coefficients for each poly-
</p>
<p>hedron, S i. Corresponding to each extreme point xij in S i, define pij = c
T
i
</p>
<p>xij and
</p>
<p>qij = Lixij. Clearly pij is the equivalent cost of the extreme point xij, and qij is its
</p>
<p>equivalent activity vector in the linking constraints.</p>
<p/>
</div>
<div class="page"><p/>
<p>70 3 The Simplex Method
</p>
<p>Then the original linear program (3.44) is equivalent, using (3.47), to the master
</p>
<p>problem:
</p>
<p>minimize
</p>
<p>N
&sum;
</p>
<p>i=1
</p>
<p>Ki
&sum;
</p>
<p>j=1
</p>
<p>pijαij
</p>
<p>subject to
</p>
<p>N
&sum;
</p>
<p>i=1
</p>
<p>Ki
&sum;
</p>
<p>j=1
</p>
<p>qijαij = b0 (3.48)
</p>
<p>Ki
&sum;
</p>
<p>j=1
αi j = 1
</p>
<p>αij � 0, j = 1, . . . , Ki
</p>
<p>⎫
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎬
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎭
</p>
<p>i = 1, . . . , N.
</p>
<p>This master problem has variables
</p>
<p>αT =
(
</p>
<p>α11, . . . , α1K1 , α21, . . . , α2K2 , . . . , αN1, . . . , αNKN
)
</p>
<p>and can be expressed more compactly as
</p>
<p>minimize pTα
</p>
<p>subject to Qα = g,α � 0 (3.49)
</p>
<p>where gT = [bT0 , 1, 1, . . . , 1]; the element of p associated with αij is pij; and the
</p>
<p>column of Q associated with αij is
</p>
<p>[
</p>
<p>qij
ei
</p>
<p>]
</p>
<p>,
</p>
<p>with ei denoting the ith unit vector in E
N .
</p>
<p>Suppose that at some stage of the revised simplex method for the master prob-
</p>
<p>lem we know the basis B and corresponding simplex multipliers yT = pT
B
</p>
<p>B&minus;1. The
corresponding relative cost vector is rT
</p>
<p>D
= cT
</p>
<p>D
&minus; yTD, having components
</p>
<p>rij = pij &minus; yT
[
</p>
<p>qij
ei
</p>
<p>]
</p>
<p>. (3.50)
</p>
<p>It is not necessary to calculate all the rij&rsquo;s; it is only necessary to determine the
</p>
<p>minimal rij. If the minimal value is nonnegative, the current solution is optimal and
</p>
<p>the process terminates. If, on the other hand, the minimal element is negative, the
</p>
<p>corresponding column should enter the basis.
</p>
<p>The search for the minimal element in (3.50) is normally made with respect
</p>
<p>to nonbasic columns only. The search can be formally extended to include basic
</p>
<p>columns as well, however, since for basic elements
</p>
<p>pij &minus; yT
[
</p>
<p>qij
ei
</p>
<p>]
</p>
<p>= 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.8 Decomposition 71
</p>
<p>The extra zero values do not influence the subsequent procedure, since a new column
</p>
<p>will enter only if the minimal value is less than zero.
</p>
<p>We therefore define r&lowast; as the minimum relative cost coefficient for all possible
basis vectors. That is,
</p>
<p>r&lowast; = minimum
i&isin;{1,...,N}
</p>
<p>{
</p>
<p>r&lowast;i = minimum
j&isin;{1,...,Ki}
</p>
<p>{pij &minus; yT
[
</p>
<p>qij
ei
</p>
<p>]
</p>
<p>}
}
</p>
<p>.
</p>
<p>Using the definitions of pij and qij, this becomes
</p>
<p>r&lowast;i = minimum
j&isin;{1,...,Ki}
</p>
<p>{
</p>
<p>cTi xij &minus; yT0 L jxij &minus; ym+i
}
</p>
<p>, (3.51)
</p>
<p>where y0 is the vector made up of the first m elements of y, m being the number of
</p>
<p>rows of L j [the number of linking constraints in (3.47)].
</p>
<p>The minimization problem in (3.51) is actually solved by the ith subproblem:
</p>
<p>minimize (cTi &minus; yT0 L j)x j
subject to A jx j = b j, x j � 0 (3.52)
</p>
<p>This follows from the fact that ym+i is independent of the extreme point index j
</p>
<p>(since y is fixed during the determination of the r j&rsquo;s), and that the solution of (3.52)
</p>
<p>must be that extreme point of S i, say xik, of minimum cost, using the adjusted cost
</p>
<p>coefficients cT
i
&minus; yT0 L j.
</p>
<p>Thus, an algorithm for this special version of the revised simplex method applied
</p>
<p>to the master problem is the following: Given a basis B
</p>
<p>Step 1. Calculate the current basic solution xB, and solve y
TB = cT
</p>
<p>B
for y.
</p>
<p>Step 2. For each i = 1, 2, . . . , N, determine the optimal solution x&lowast;
i
</p>
<p>of the ith
</p>
<p>subproblem (3.52) and calculate
</p>
<p>r&lowast;i = (c
T
i &minus; yT0 L j)x&lowast;i &minus; ym+i. (3.53)
</p>
<p>If all r&lowast;
i
&gt; 0, stop; the current solution is optimal.
</p>
<p>Step 3. Determine which column is to enter the basis by selecting the minimal r&lowast;
i
.
</p>
<p>Step 4. Update the basis of the master problem as usual.
</p>
<p>This algorithm has an interesting economic interpretation in the context of a
</p>
<p>multidivisional firm minimizing its total cost of operations as described earlier.
</p>
<p>Division i&rsquo;s activities are internally constrained by Axi = b j, and the common res-
</p>
<p>ources b0 impose linking constraints. At Step 1 of the algorithm, the firm&rsquo;s central
</p>
<p>management formulates its current master plan, which is perhaps suboptimal, and
</p>
<p>announces a new set of prices that each division must use to revise its recommended
</p>
<p>strategy at Step 2. In particular, &minus;y0 reflects the new prices that higher management
has placed on the common resources. The division that reports the greatest rate of
</p>
<p>potential cost improvement has its recommendations incorporated in the new mas-
</p>
<p>ter plan at Step 3, and the process is repeated. If no cost improvement is possible,
</p>
<p>central management settles on the current master plan.</p>
<p/>
</div>
<div class="page"><p/>
<p>72 3 The Simplex Method
</p>
<p>3.9 Summary
</p>
<p>The simplex method is founded on the fact that the optimal value of a linear pro-
</p>
<p>gram, if finite, is always attained at a basic feasible solution. Using this foundation
</p>
<p>there are two ways in which to visualize the simplex process. The first is to view the
</p>
<p>process as one of continuous change. One starts with a basic feasible solution and
</p>
<p>imagines that some nonbasic variable is increased slowly from zero. As the value of
</p>
<p>this variable is increased, the values of the current basic variables are continuously
</p>
<p>adjusted so that the overall vector continues to satisfy the system of linear equality
</p>
<p>constraints. The change in the objective function due to a unit change in this non-
</p>
<p>basic variable, taking into account the corresponding required changes in the values
</p>
<p>of the basic variables, is the relative cost coefficient associated with the nonbasic
</p>
<p>variable. If this coefficient is negative, then the objective value will be continuously
</p>
<p>improved as the value of this nonbasic variable is increased, and therefore one inc-
</p>
<p>reases the variable as far as possible, to the point where further increase would
</p>
<p>violate feasibility. At this point the value of one of the basic variables is zero, and
</p>
<p>that variable is declared nonbasic, while the nonbasic variable that was increased is
</p>
<p>declared basic.
</p>
<p>The other viewpoint is more discrete in nature. Realizing that only basic feasible
</p>
<p>solutions need be considered, various bases are selected and the corresponding basic
</p>
<p>solutions are calculated by solving the associated set of linear equations. The logic
</p>
<p>for the systematic selection of new bases again involves the relative cost coefficients
</p>
<p>and, of course, is derived largely from the first, continuous, viewpoint.
</p>
<p>Problems of special structure are important both for applications and for theory.
</p>
<p>The transportation problem represents an important class of linear programs with
</p>
<p>structural properties that lead to an efficient implementation of the simplex method.
</p>
<p>The most important property of the transportation problem is that any basis is trian-
</p>
<p>gular. This means that the basic variables can be found, one by one, directly by back
</p>
<p>substitution, and the basis need never be inverted. Likewise, the simplex multipli-
</p>
<p>ers can be found by back substitution, since they solve a set of equations involving
</p>
<p>the transpose of the basis. Moreover, when any basis matrix is triangular and all
</p>
<p>nonzero elements are equal to one (or minus one if the signs of some equations
</p>
<p>are changed), it follows that the process of back substitution will simply involve
</p>
<p>repeated additions and subtractions of the given row and column sums. No multi-
</p>
<p>plication or division is required. It therefore follows that if the original right-hand
</p>
<p>side are integers, the values of all basic variables will be integers. Hence, an opti-
</p>
<p>mal basic solution, where each entry is integral, always exists; that is, there is no
</p>
<p>gap between continuous linear program and integer linear program (or the integral-
</p>
<p>ity gap is zero). The transportation problem can be generalized to a minimum cost
</p>
<p>flow problem in a network. This leads to the interpretation of a simplex basis as
</p>
<p>corresponding to a spanning tree in the network; see Appendix D.
</p>
<p>Many linear programming methods have implemented a Presolver procedure to
</p>
<p>eliminate redundant or duplicate constraints and/or value fixed variables, and to
</p>
<p>check possible constraint inconsistency and unboundedness. This typically results
</p>
<p>in problem size reduction and possible infeasibility detection.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.10 Exercises 73
</p>
<p>3.10 Exercises
</p>
<p>1. Using pivoting, solve the simultaneous equations
</p>
<p>3x1 + 2x2 = 5
</p>
<p>5x1 + x2 = 9.
</p>
<p>2. Using pivoting, solve the simultaneous equations
</p>
<p>x1 + 2x2 + x3 = 7
</p>
<p>2x1 &minus; x2 + 2x3 = 6
x1 + x2 + 3x3 = 12.
</p>
<p>3. Solve the equations in Exercise 2 by Gaussian elimination as described in
</p>
<p>Appendix C.
</p>
<p>4. Suppose B is an m &times; m square nonsingular matrix, and let the tableau T be
constructed, T = [I, B] where I is the m&times;m identity matrix. Suppose that pivot
operations are performed on this tableau so that it takes the form [C, I]. Show
</p>
<p>that C = B&minus;1.
5. Show that if the vectors a1, a2, . . . , am are a basis in E
</p>
<p>m, the vectors a1,
</p>
<p>a2, . . . , ap&minus;1,
aq, ap+1, . . . , am also are a basis if and only if āpq � 0, where āpq is defined by
</p>
<p>the tableau (3.5).
</p>
<p>6. If r j &gt; 0 for every j corresponding to a variable x j that is not basic, show that
</p>
<p>the corresponding basic feasible solution is the unique optimal solution.
</p>
<p>7. Show that a degenerate basic feasible solution may be optimal without satisfy-
</p>
<p>ing r j � 0 for all j.
</p>
<p>8.
</p>
<p>(a) Using the simplex procedure, solve
</p>
<p>maximize &minus;x1 + x2
subject to x1 &minus; x2 � 2
</p>
<p>x1 + x2 � 6
</p>
<p>x1 � 0, x2 � 0.
</p>
<p>(b) Draw a graphical representation of the problem in x1, x2 space and indicate the
</p>
<p>path of the simplex steps.
</p>
<p>(c) Repeat for the problem
</p>
<p>maximize x1 + x2
subject to &minus;2x1 + x2 � 1
</p>
<p>x1 &minus; x2 � 1
x1 � 0, x2 � 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>74 3 The Simplex Method
</p>
<p>9. Using the simplex procedure, solve the spare-parts manufacturer&rsquo;s problem
</p>
<p>(Exercise 4, Chap. 2).
</p>
<p>10. Using the simplex procedure, solve
</p>
<p>minimize 2x1 + 4x2 + x3 + x4
subject to x1 + 3x2 + x4 � 4
</p>
<p>2x1 + x2 � 3
</p>
<p>x2 + 4x3 + x4 � 3
</p>
<p>x1 � 0 i = 1, 2, 3, 4.
</p>
<p>11. For the linear program of Exercise 10
</p>
<p>(a) How much can the elements of b = (4, 3, 3) be changed without changing the
</p>
<p>optimal basis?
</p>
<p>(b) How much can the elements of c = (2, 4, 1, 1) be changed without changing the
</p>
<p>optimal basis?
</p>
<p>(c) What happens to the optimal cost for small changes in b?
</p>
<p>(d) What happens to the optimal cost for small changes in c?
</p>
<p>12. Consider the problem
</p>
<p>minimize x1 &minus; 3x2 &minus; 0.4x3
subject to 3x1 &minus; x2 + 2x3 � 7
</p>
<p>&minus;2x1 + 4x2 � 12
&minus;4x1 + 3x2 + 3x3 � 14
x1 � 0, x2 � 0, x3 � 0.
</p>
<p>(a) Find an optimal solution.
</p>
<p>(b) How many optimal basic feasible solutions are there?
</p>
<p>(c) Show that if c4 +
1
3
a14 +
</p>
<p>4
5
a24 � 0, then another activity x4 can be introduced
</p>
<p>with cost coefficient c1 and activity vector (a14, a24, a34) without changing the
</p>
<p>optimal solution.
</p>
<p>13. Rather than select the variable corresponding to the most negative relative cost
</p>
<p>coefficient as the variable to enter the basis, it has been suggested that a better
</p>
<p>criterion would be to select that variable which, when pivoted in, will pro-
</p>
<p>duce the greatest improvement in the objective function. Show that this crite-
</p>
<p>rion leads to selecting the variable xk corresponding to the index k minimizing
</p>
<p>max
i,āik&gt;0
</p>
<p>rkāi0/āik.
</p>
<p>14. In the ordinary simplex method one new vector is brought into the basis and
</p>
<p>one removed at every step. Consider the possibility of bringing two new vectors
</p>
<p>into the basis and removing two at each stage. Develop a complete procedure
</p>
<p>that operates in this fashion.
</p>
<p>15. Degeneracy. If a basic feasible solution is degenerate, it is then theoretically
</p>
<p>possible that a sequence of degenerate basic feasible solutions will be generated
</p>
<p>that endlessly cycles without making progress. It is the purpose of this exercise
</p>
<p>and the next two to develop a technique that can be applied to the simplex
</p>
<p>method to avoid this cycling.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.10 Exercises 75
</p>
<p>Corresponding to the linear system Ax = b where A = [a1, a2, . . . , an] define
</p>
<p>the perturbed system Ax = b(ε) where b(ε) = b + εa1 + ε
2a2 + &middot; &middot; &middot; + εnan, ε &gt;
</p>
<p>0. Show that if there is a basic feasible solution (possibly degenerate) to the
</p>
<p>unperturbed system with basis B = [a1, a2, . . . , am], then corresponding to
</p>
<p>the same basis, there is a nondegenerate basic feasible solution to the perturbed
</p>
<p>system for some range of ε &gt; 0.
</p>
<p>16. Show that corresponding to any basic feasible solution to the perturbed system
</p>
<p>of Exercise 15, which is nondegenerate for some range of ε &gt; 0, and to a vector
</p>
<p>ak not in the basis, there is a unique vector a j in the basis which when replaced
</p>
<p>by ak leads to a basic feasible solution; and that solution is nondegenerate for a
</p>
<p>range of ε &gt; 0.
</p>
<p>17. Show that the tableau associated with a basic feasible solution of the perturbed
</p>
<p>system of Exercise 15, and which is nondegenerate for a range of ε &gt; 0, is
</p>
<p>identical with that of the unperturbed system except in the column under b(ε).
</p>
<p>Show how the proper pivot in a given column to preserve feasibility of the
</p>
<p>perturbed system can be determined from the tableau of the unperturbed system.
</p>
<p>Conclude that the simplex method will avoid cycling if whenever there is a
</p>
<p>choice in the pivot element of a column k, arising from a tie in the minimum of
</p>
<p>āi0/āik among the elements i &isin; I0, the tie is resolved by finding the minimum
of āi1/āik, i &isin; I0. If there still remainties among elements i &isin; I, the process is
repeated with āi2/āik, etc., until there is a unique element.
</p>
<p>18. Using the two-phase simplex procedure solve
</p>
<p>(a)
minimize &minus;3x1 + x2 + 3x3 &minus; x4
subject to x1 + 2x2 &minus; x3 + x4 = 0
</p>
<p>2x1 &minus; 2x2 + 3x3 + 3x4 = 9
x1 &minus; x2 + 2x3 &minus; x4 = 6
x1 � 0, i = 1, 2, 3, 4.
</p>
<p>(b)
minimize x1 + 6x2 &minus; 7x3 + x4 + 5x5
subject to 5x1 &minus; 4x2 + 13x3 &minus; 2x4 + x5 = 20
</p>
<p>x1 &minus; x2 + 5x3 &minus; x4 + x5 = 8
x1 � 0, i = 1, 2, 3.4, 5.
</p>
<p>19. Solve the oil refinery problem (Exercise 3, Chap. 2).
</p>
<p>20. Show that in the phase I procedure of a problem that has feasible solutions, if an
</p>
<p>artificial variable becomes nonbasic, it need never again be made basic. Thus,
</p>
<p>when an artificial variable becomes nonbasic its column can be eliminated from
</p>
<p>future tableaus.
</p>
<p>21. Suppose the phase I procedure is applied to the system Ax = b, x � 0, and that
</p>
<p>the resulting tableau (ignoring the cost row) has the form</p>
<p/>
</div>
<div class="page"><p/>
<p>76 3 The Simplex Method
</p>
<p>This corresponds to having m &minus; k basic artificial variables at zero level.
(a) Show that any nonzero element in R2 can be used as a pivot to eliminate a basic
</p>
<p>artificial variable, thus yielding a similar tableau but with k increased by one.
</p>
<p>(b) Suppose that the process in (a) has been repeated to the point where R2 = 0.
</p>
<p>Show that the original system is redundant, and show how phase II may proceed
</p>
<p>by eliminating the bottom rows.
</p>
<p>(c) Use the above method to solve the linear program
</p>
<p>minimize 2x1 + 6x2 + x3 + x4
subject to x1 + 2x2 + x4 = 6
</p>
<p>x1 + 2x2 + x3 + x4 = 7
</p>
<p>x1 + 3x2 &minus; x3 + 2x4 = 7
x1 + x2 + x3 = 5
</p>
<p>x1 � 0, x2 � 0, x3 � 0, x4 � 0.
</p>
<p>22. Find a basic feasible solution to
</p>
<p>x1 + 2x2 &minus; x3 + x4 = 3
2x1 + 4x2 + x3 + 2x4 = 12
</p>
<p>x1 + 4x2 + 2x3 + x4 = 9
</p>
<p>x1 � 0, i = 1, 2, 3, 4.
</p>
<p>23. Consider the system of linear inequalities Ax � b, x � 0 with b � 0. This
</p>
<p>system can be transformed to standard form by the introduction of m surplus
</p>
<p>variables so that it becomes Ax&ndash;y = b, x � 0, y � 0. Let bk = maxi bi and
</p>
<p>consider the new system in standard form obtained by adding the kth row to the
</p>
<p>negative of every other row. Show that the new system requires the addition of
</p>
<p>only a single artificial variable to obtain an initial basic feasible solution.
</p>
<p>Use this technique to find a basic feasible solution to the system.
</p>
<p>x1 + 2x2 + x3 � 4
</p>
<p>2x1 + x2 + x3 � 5
</p>
<p>2x1 + 3x2 + 2x3 � 6
</p>
<p>x j � 0, i = 1, 2, 3.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.10 Exercises 77
</p>
<p>24. It is possible to combine the two phases of the two-phase method into a single
</p>
<p>procedure by the big-M method. Given the linear program in standard form
</p>
<p>minimize cTx
</p>
<p>subject to Ax = b, x � 0,
</p>
<p>one forms the approximating problem
</p>
<p>minimize cTx +M
</p>
<p>m
&sum;
</p>
<p>i=1
</p>
<p>ui
</p>
<p>subject to Ax + u = b
</p>
<p>x � 0, u � 0.
</p>
<p>In this problem u = (u1, u2, . . . , um) is a vector of artificial variables and M is
</p>
<p>a large constant. The term M
</p>
<p>m
&sum;
</p>
<p>i=1
</p>
<p>u j serves as a penalty term for nonzero ui&rsquo;s.
</p>
<p>If this problem is solved by the simplex method, show the following:
</p>
<p>(a) If an optimal solution is found with y = 0, then the corresponding x is an
</p>
<p>optimal basic feasible solution to the original problem.
</p>
<p>(b) If for every M &gt; 0 an optimal solution is found with y � 0, then the original
</p>
<p>problem is infeasible.
</p>
<p>(c) If for every M &gt; 0 the approximating problem is unbounded, then the original
</p>
<p>problem is either unbounded or infeasible.
</p>
<p>(d) Suppose now that the original problem has a finite optimal value V(&infin;). Let
V(M) be the optimal value of the approximating problem. Show that
</p>
<p>V(M) � V(&infin;).
(e) Show that for M1 � M2 we have V(M1) � V(M2).
</p>
<p>(f) Show that there is a value M0 such that for M � M0, V(M) = V(&infin;), and hence
conclude that the big-M method will produce the right solution for large enough
</p>
<p>values of M.
</p>
<p>25. Using the revised simplex method find a basic feasible solution to
</p>
<p>x1 +2x2 &minus; x3 + x4 = 3
2x1 +4x2 + x3 + 2x4 = 12
</p>
<p>x1 +4x2 + 2x3 + x4 = 9
</p>
<p>x1 � 0, i = 1, 2, 3, 4.
</p>
<p>26. The following tableau is an intermediate stage in the solution of a minimization
</p>
<p>problem:
y1 y2 y3 y4 y5 y6 y0
1 2/3 0 0 4/3 0 4
</p>
<p>0 &minus;7/3 3 1 &minus;2/3 0 2
0 &minus;2/3 &minus;2 0 2/3 1 2
</p>
<p>rT 0 8/3 &minus;11 0 4/3 0 &minus;8</p>
<p/>
</div>
<div class="page"><p/>
<p>78 3 The Simplex Method
</p>
<p>(a) Determine the next pivot element.
</p>
<p>(b) Given that the inverse of the current basis is
</p>
<p>B&minus;1 = [a1, a4, a6]
&minus;1 =
</p>
<p>1
</p>
<p>3
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>1 1 &minus;1
1 &minus;2 2
&minus;1 2 1
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>and the corresponding cost coefficients are
</p>
<p>cTB = (c1, c4, c6) = (&minus;1, &minus;3, 1),
</p>
<p>find the original problem.
</p>
<p>27. In many applications of linear programming it may be sufficient, for practical
</p>
<p>purposes, to obtain a solution for which the value of the objective function is
</p>
<p>within a predetermined tolerance ε from the minimum value z&lowast;. Stopping the
simplex algorithm at such a solution rather than searching for the true minimum
</p>
<p>may considerably reduce the computations.
</p>
<p>(a) Consider a linear programming problem for which the sum of the variables is
</p>
<p>known to be bounded above by s. Let z0 denote the current value of the objective
</p>
<p>function at some stage of the simplex algorithm, (c j &minus; z j) the corresponding
relative cost coefficients, and
</p>
<p>M = max(z j &minus; c j) j.
</p>
<p>Show that if M � ε/s, then z0 &minus; z&lowast; &le; ε.
(b) Consider the transportation problem described in Sect. 2.2 (Example 3). Assum-
</p>
<p>ing this problem is solved by the simplex method and it is sufficient to obtain
</p>
<p>a solution within ε tolerance from the optimal value of the objective function,
</p>
<p>specify a stopping criterion for the algorithm in terms of ε and the parameters
</p>
<p>of the problem.
</p>
<p>28. A matrix A is said to be totally unimodular if the determinant of every square
</p>
<p>submatrix formed from it has value 0, +1, or &minus;1
(a) Show that the matrix A defining the equality constraints of a transportation
</p>
<p>problem is totally unimodular.
</p>
<p>(b) In the system of equations Ax = b, assume that A is totally unimodular and
</p>
<p>that all elements of A and b are integers. Show that all basic solutions have
</p>
<p>integer components.
</p>
<p>29. For the arrays below:
</p>
<p>(a) Compute the basic solutions indicated. (Note: They may be infeasible.)
</p>
<p>(b) Write the equations for the basic variables, corresponding to the indicated
</p>
<p>basic solutions, in lower triangular form.
</p>
<p>x x 10
</p>
<p>x 20
</p>
<p>x x 30
</p>
<p>20 20 20
</p>
<p>x x 10
</p>
<p>x 20
</p>
<p>x x 30
</p>
<p>20 20 20</p>
<p/>
</div>
<div class="page"><p/>
<p>3.10 Exercises 79
</p>
<p>30. For the arrays of cost coefficients below, the circled positions indicate basic
</p>
<p>variables.
</p>
<p>(a) Compute the simplex multipliers.
</p>
<p>(b) Write the equations for the simplex multipliers in upper triangular form,
</p>
<p>and compare with Part(b) of Exercise 4.
</p>
<p>3 ➅ ➆
</p>
<p>2 ➃ 3
</p>
<p>➀ 5 ➁
</p>
<p>➂ 6 ➆
</p>
<p>2 ➃ 3
</p>
<p>1 ➄ ➁
</p>
<p>31. Consider the modified transportation problem where there is more available at
</p>
<p>origins than is required at destinations (i.e.,
m
&sum;
</p>
<p>i=1
ai &gt;
</p>
<p>n
&sum;
</p>
<p>j=1
b j).
</p>
<p>minimize
</p>
<p>m
&sum;
</p>
<p>j=1
</p>
<p>n
&sum;
</p>
<p>i=1
</p>
<p>ci jxi j
</p>
<p>subject to
</p>
<p>n
&sum;
</p>
<p>j=1
</p>
<p>xi j � ai, i = 1, 2, . . . ,m
</p>
<p>n
&sum;
</p>
<p>i=1
</p>
<p>xi j = b j, j = 1, 2, . . . , n
</p>
<p>xi j � 0, for all i, j.
</p>
<p>(a) Show how to convert it to an ordinary transportation problem.
</p>
<p>(b) Suppose there is a storage cost of si per unit at origin i for goods not trans-
</p>
<p>ported to a destination. Repeat Part(a) with this assumption.
</p>
<p>32. Solve the following transportation problem, which is an original example of
</p>
<p>Hitchcock.
</p>
<p>a =
(
</p>
<p>25 25 50
)
</p>
<p>b =
(
</p>
<p>15 20 30 35
) C =
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>10 5 6 7
</p>
<p>8 2 7 6
</p>
<p>9 3 4 8
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>33. In a transportation problem, suppose that two rows or two columns of the cost
</p>
<p>coefficient array differ by a constant. Show that the problem can be reduced by
</p>
<p>combining those rows or columns.
</p>
<p>34. The transportation problem is often solved more quickly by carefully selecting
</p>
<p>the starting basic feasible solution. The matrix minimum technique for finding
</p>
<p>a starting solution is: (3.34) Find the lowest cost unallocated cell in the array,
</p>
<p>and allocate the maximum possible to it, (3.35) Reduce the corresponding row
</p>
<p>and column requirements, and drop the row or column having zero remaining
</p>
<p>requirement. Go back to Step 1 unless all remaining requirements are zero.
</p>
<p>(a) Show that this procedure yields a basic feasible solution.
</p>
<p>(b) Apply the method to Exercise 7.</p>
<p/>
</div>
<div class="page"><p/>
<p>80 3 The Simplex Method
</p>
<p>35. The caterer problem. A caterer is booked to cater a banquet each evening for the
</p>
<p>next T days. He requires rt clean napkins on the tth day for t = 1, 2, . . . , T . He
</p>
<p>may send dirty napkins to the laundry, which has two speeds of service&mdash;fast
</p>
<p>and slow. The napkins sent to the fast service will be ready for the next day&rsquo;s
</p>
<p>banquet; those sent to the slow service will be ready for the banquet 2 days later.
</p>
<p>Fast and slow service cost c1 and c2 per napkin, respectively, with c1 &gt; c2. The
</p>
<p>caterer may also purchase new napkins at any time at cost c0. He has an initial
</p>
<p>stock of s napkins and wishes to minimize the total cost of supplying fresh
</p>
<p>napkins.
</p>
<p>(a) Formulate the problem as a transportation problem. (Hint: Use T+1 sources
</p>
<p>and T destinations.)
</p>
<p>(b) Using the values T = 4, s = 200, r1 = 100, r2 = 130, r3 = 150, r4 =
</p>
<p>140, c1 = 6, c2 = 4, c0 = 12, solve the problem.
</p>
<p>36. The marriage assignment problem. A group of n men and n women live on an
</p>
<p>island. The amount of happiness that the ith man and the jth woman derive by
</p>
<p>spending a fraction xi j of their lives together is ci jxi j. What is the nature of the
</p>
<p>living arrangements that maximizes the total happiness of the islanders?
</p>
<p>37. Anticycling Rule. A remarkably simple procedure for avoiding cycling was
</p>
<p>developed by Bland, and we discuss it here.
</p>
<p>Bland&rsquo;s Rule. In the simplex method:
</p>
<p>(a) Select the column to enter the basis by j = min{ j : r j &lt; 0}; that is, select the
lowest indexed favorable column.
</p>
<p>(b) In case ties occur in the criterion for determining which column is to leave the
</p>
<p>basis, select the one with lowest index.
</p>
<p>We can prove by contradiction that the use of Bland&rsquo;s rule prohibits cycling.
</p>
<p>Suppose that cycling occurs. During the cycle a finite number of columns enter
</p>
<p>and leave the basis. Each of these columns enters at level zero, and the cost
</p>
<p>function does not change.
</p>
<p>Delete all rows and columns that do not contain pivots during a cycle, obtaining
</p>
<p>a new linear program that also cycles. Assume that this reduced linear program
</p>
<p>has m rows and n columns. Consider the solution stage where column n is about
</p>
<p>to leave the basis, being replaced by column p. The corresponding tableau is as
</p>
<p>follows (where the entries shown are explained below):
</p>
<p>a1 &middot; &middot; &middot; ap &middot; &middot; &middot; an b
�0 0 0
</p>
<p>�0 0 0
...
</p>
<p>...
...
</p>
<p>&gt; 0 1 0
</p>
<p>cT &lt; 0 0 0
</p>
<p>Without loss of generality, we assume that the current basis consists of the last
</p>
<p>m columns. In fact, we may define the reduced linear program in terms of this
</p>
<p>tableau, calling the current coefficient array A and the current relative cost vec-
</p>
<p>tor c. In this tableau we pivot on amp, so amp &gt; 0. By Part(b) of Bland&rsquo;s rule,</p>
<p/>
</div>
<div class="page"><p/>
<p>References 81
</p>
<p>an can leave the basis only if there are no ties in the ratio test, and since b = 0
</p>
<p>because all rows are in the cycle, it follows that aip � 0 for all i � m.
</p>
<p>Now consider the situation when column n is about to reenter the basis. Part(a)
</p>
<p>of Bland&rsquo;s rule ensures that rn &lt; 0 and r j � 0 for all i � n. Apply the formula
</p>
<p>ri = ci &minus; yTai to the last m columns to show that each component of y except ym
is nonpositive; and ym &gt; 0. Then use this to show that rp = cp &minus; yTap &lt; cp &lt; 0,
contradicting rp � 0.
</p>
<p>38. Use the Dantzig-Wolfe decomposition method to solve
</p>
<p>minimize &minus;4x1 &minus; x2 &minus; 3x3 &minus; 2x4
subject to 2x1 + 2x2 + x3 + 2x4 � 6
</p>
<p>x2 + 2x3 + 3x4 � 4
</p>
<p>2x1 + x2 � 5
</p>
<p>x2 � 1
</p>
<p>&minus; x3 + 2x4 � 2
x3 + 2x4 � 6
</p>
<p>x1 � 0, x2 � 0, x3 � 0, x4 � 0.
</p>
<p>References
</p>
<p>3.1&ndash;3.5 All of this is now standard material contained in most courses in linear
</p>
<p>programming. See the references cited at the end of Chap. 2. For the orig-
</p>
<p>inal work in this area, see Dantzig [D2] for development of the simplex
</p>
<p>method; Orden [O2] for the artificial basis technique; Dantzig, Orden and
</p>
<p>Wolfe [D8], Orchard-Hays [O1], and Dantzig [D4] for the revised simplex
</p>
<p>method; and Charnes and Lemke [C3] and Dantzig [D5] for upper bounds.
</p>
<p>The synthetic carrot interpretation is due to Gale [G2].
</p>
<p>3.6 The idea of using LU decomposition for the simplex method is due to Bar-
</p>
<p>tels and Golub [B2]. See also Bartels [B1]. For a nice simple introduction
</p>
<p>to Gaussian elimination, see Forsythe and Moler [F15]. For an expository
</p>
<p>treatment of modern computer implementation issues of linear program-
</p>
<p>ming, see Murtagh [M9].
</p>
<p>3.7 The transportation problem in its present form was first formulated by
</p>
<p>Hitchcock [H11]. Koopmans [K8] also contributed significantly to the early
</p>
<p>development of the problem. The simplex method for the transportation
</p>
<p>problem was developed by Dantzig [D3]. Most textbooks on linear pro-
</p>
<p>gramming include a discussion of the transportation problem. See espe-
</p>
<p>cially Simonnard [S6], Murty [M11], and Bazaraa and Jarvis [B5]. The
</p>
<p>method of changing basis is often called the stepping stone method. The as-
</p>
<p>signment problem has a long and interesting history. The important fact that
</p>
<p>the integer problem is solved by a standard linear programming problem
</p>
<p>follows from a theorem of Birkhoff [B16], which states that the extreme
</p>
<p>points of the set of feasible assignments are permutation matrices.</p>
<p/>
</div>
<div class="page"><p/>
<p>82 3 The Simplex Method
</p>
<p>3.8 For a more comprehensive description of the Dantzig and Wolfe [D11]
</p>
<p>decomposition method, see Dantzig [D6].
</p>
<p>The degeneracy technique discussed in Exercises 15&ndash;17 is due to Charnes
</p>
<p>[C2]. The anticycling method of Exercise 35 is due to Bland [B19]. For the
</p>
<p>state of the art in Simplex solvers see Bixby [B18].</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 4
</p>
<p>Duality and Complementarity
</p>
<p>Associated with every linear program, and intimately related to it, is a corresponding
</p>
<p>dual linear program. Both programs are constructed from the same underlying cost
</p>
<p>and constraint coefficients but in such a way that if one of these problems is one of
</p>
<p>minimization the other is one of maximization, and the optimal values of the corre-
</p>
<p>sponding objective functions, if finite, are equal. The variables of the dual problem
</p>
<p>can be interpreted as prices associated with the constraints of the original (primal)
</p>
<p>problem, and through this association it is possible to give an economically mean-
</p>
<p>ingful characterization to the dual whenever there is such a characterization for the
</p>
<p>primal.
</p>
<p>The variables of the dual problem are also intimately related to the calculation of
</p>
<p>the relative cost coefficients in the simplex method. Thus, a study of duality sharp-
</p>
<p>ens our understanding of the simplex procedure and motivates certain alternative
</p>
<p>solution methods. Indeed, the simultaneous consideration of a problem from both
</p>
<p>the primal and dual viewpoints often provides significant computational advantage
</p>
<p>as well as economic insight.
</p>
<p>4.1 Dual Linear Programs
</p>
<p>In this section we define the dual program that is associated with a given linear pro-
</p>
<p>gram. Initially, we depart from our usual strategy of considering programs in stan-
</p>
<p>dard form, since the duality relationship is most symmetric for programs expressed
</p>
<p>solely in terms of inequalities. Specifically then, we define duality through the pair
</p>
<p>of programs displayed below.
</p>
<p>&copy; Springer International Publishing Switzerland 2016
</p>
<p>D.G. Luenberger, Y. Ye, Linear and Nonlinear Programming, International
Series in Operations Research &amp; Management Science 228,
DOI 10.1007/978-3-319-18842-3 4
</p>
<p>83</p>
<p/>
</div>
<div class="page"><p/>
<p>84 4 Duality and Complementarity
</p>
<p>Primal Dual
</p>
<p>minimize cTx maximize yTb
</p>
<p>subject to Ax � b subject to yTA � cT
</p>
<p>x � 0 y � 0
</p>
<p>(4.1)
</p>
<p>If A is an m &times; n matrix, then x is an m-dimensional column vector, b is an
n-dimensional column vector, cT is an n-dimensional row vector, and yT is an
</p>
<p>m-dimensional row vector. The vector x is the variable of the primal program, and y
</p>
<p>is the variable of the dual program.
</p>
<p>The pair of programs (4.1) is called the symmetric form of duality and, as ex-
</p>
<p>plained below, can be used to define the dual of any linear program. It is important
</p>
<p>to note that the role of primal and dual can be reversed. Thus, studying in detail
</p>
<p>the process by which the dual is obtained from the primal: interchange of cost and
</p>
<p>constraint vectors, transposition of coefficient matrix, reversal of constraint inequal-
</p>
<p>ities, and change of minimization to maximization; we see that this same process
</p>
<p>applied to the dual yields the primal. Put another way, if the dual is transformed,
</p>
<p>by multiplying the objective and the constraints by minus unity, so that it has the
</p>
<p>structure of the primal (but is still expressed in terms of y), its corresponding dual
</p>
<p>will be equivalent to the original primal.
</p>
<p>The dual of any linear program can be found by converting the program to the
</p>
<p>form of the primal shown above. For example, given a linear program in standard
</p>
<p>form
</p>
<p>minimize cTx
</p>
<p>subject to Ax = b, x � 0,
</p>
<p>we write it in the equivalent form
</p>
<p>minimize cTx
</p>
<p>subject to Ax � b
</p>
<p>&minus;Ax � &minus;b
x � 0,
</p>
<p>which is in the form of the primal of (4.1) but with coefficient matrix
</p>
<p>[
</p>
<p>A
</p>
<p>&minus;A
</p>
<p>]
</p>
<p>. Using
</p>
<p>a dual vector partitioned as (u, v), the corresponding dual is
</p>
<p>maximize uTb &minus; vTb
subject to uTA &minus; vTA � cT
</p>
<p>u � 0, v � 0.
</p>
<p>Letting y = u &minus; v we may simplify the representation of the dual program so that
we obtain the pair of problems displayed below:
</p>
<p>Primal Dual
</p>
<p>minimize cTx maximize yTb
</p>
<p>subject to Ax = b, x � 0 subject to yTA � cT .
</p>
<p>(4.2)</p>
<p/>
</div>
<div class="page"><p/>
<p>4.1 Dual Linear Programs 85
</p>
<p>This is the asymmetric form of the duality relation. In this form the dual vector y
</p>
<p>(which is really a composite of u and v) is not restricted to be nonnegative.
</p>
<p>Similar transformations can be worked out for any linear program to first get the
</p>
<p>primal in the form (4.1), calculate the dual, and then simplify the dual to account
</p>
<p>for special structure.
</p>
<p>In general, if some of the linear inequalities in the primal (4.1) are changed to
</p>
<p>equality, the corresponding components of y in the dual become free variables.
</p>
<p>If some of the components of x in the primal are free variables, then the corre-
</p>
<p>sponding inequalities in yTA � cT are changed to equality in the dual. We mention
</p>
<p>again that these are not arbitrary rules but are direct consequences of the original
</p>
<p>definition and the equivalence of various forms of linear programs.
</p>
<p>Example 1 (Dual of the Diet Problem). The diet problem, Example 1, Sect. 2.2, was
</p>
<p>the problem faced by a dietitian trying to select a combination of foods to meet
</p>
<p>certain nutritional requirements at minimum cost. This problem has the form
</p>
<p>minimize cTx
</p>
<p>subject to Ax � b, x � 0
</p>
<p>and hence can be regarded as the primal program of the symmetric pair above. We
</p>
<p>describe an interpretation of the dual problem.
</p>
<p>Imagine a pharmaceutical company that produces in pill form each of the
</p>
<p>nutrients considered important by the dietitian. The pharmaceutical company tries
</p>
<p>to convince the dietitian to buy pills, and thereby supply the nutrients directly rather
</p>
<p>than through purchase of various foods. The problem faced by the drug company
</p>
<p>is that of determining positive unit prices λ1, λ2, . . . , λm for the nutrients so as to
</p>
<p>maximize revenue while at the same time being competitive with real food. To be
</p>
<p>competitive with real food, the cost of a unit of food i made synthetically from
</p>
<p>pure nutrients bought from the druggist must be no greater than ci, the market price
</p>
<p>of the food. Thus, denoting by ai the ith food, the company must satisfy y
Tai � ci
</p>
<p>for each i. In matrix form this is equivalent to yTA � cT . Since b j units of the jth
</p>
<p>nutrient will be purchased, the problem of the druggist is
</p>
<p>maximize yTb
</p>
<p>subject to yTA � cT , y � 0,
</p>
<p>which is the dual problem.
</p>
<p>Example 2 (Dual of the Transportation Problem). The transportation problem,
</p>
<p>Example 3, Sect. 2.2, is the problem, faced by a manufacturer, of selecting the
</p>
<p>pattern of product shipments between several fixed origins and destinations so as to
</p>
<p>minimize transportation cost while satisfying demand. Referring to (4.6) and (4.7)
</p>
<p>of Chap. 2, the problem is in standard form, and hence the asymmetric version of
</p>
<p>the duality relation applies. There is a dual variable for each constraint. In this case
</p>
<p>we denote the variables ui, i = 1, 2, . . . , m for (4.6) and v j, j = 1, 2, . . . , n for (4.7).
</p>
<p>Accordingly, the dual is</p>
<p/>
</div>
<div class="page"><p/>
<p>86 4 Duality and Complementarity
</p>
<p>maximize
m
&sum;
</p>
<p>i=1
aiui +
</p>
<p>n
&sum;
</p>
<p>j=1
b jv j
</p>
<p>subject to ui + v j � cij, i = 1, 2, . . . , m,
</p>
<p>j = 1, 2, . . . , n.
</p>
<p>To interpret the dual problem, we imagine an entrepreneur who, feeling that he can
</p>
<p>ship more efficiently, comes to the manufacturer with the offer to buy his product at
</p>
<p>the plant sites (origins) and sell it at the warehouses (destinations). The product price
</p>
<p>that is to be used in these transactions varies from point to point, and is determined
</p>
<p>by the entrepreneur in advance. He must choose these prices, of course, so that his
</p>
<p>offer will be attractive to the manufacturer.
</p>
<p>The entrepreneur, then, must select prices &minus;u1,&minus;u2, . . . ,&minus;um for the m origins
and v1, v2, . . . , vn for the n destinations. To be competitive with usual transportation
</p>
<p>modes, his prices must satisfy ui + v j � cij for all i, j, since ui + v j represents the
</p>
<p>net amount the manufacturer must pay to sell a unit of product at origin i and buy
</p>
<p>it back again at destination j. Subject to this constraint, the entrepreneur will adjust
</p>
<p>his prices to maximize his revenue. Thus, his problem is as given above.
</p>
<p>4.2 The Duality Theorem
</p>
<p>To this point the relation between the primal and dual programs has been simply a
</p>
<p>formal one based on what might appear as an arbitrary definition. In this section,
</p>
<p>however, the deeper connection between a program and its dual, as expressed by the
</p>
<p>Duality Theorem, is derived.
</p>
<p>The proof of the Duality Theorem given in this section relies on the Separating
</p>
<p>Hyperplane Theorem (Appendix B) and is therefore somewhat more advanced than
</p>
<p>previous arguments. It is given here so that the most general form of the Duality
</p>
<p>Theorem is established directly. An alternative approach is to use the theory of the
</p>
<p>simplex method to derive the duality result. A simplified version of this alternative
</p>
<p>approach is given in the next section.
</p>
<p>Throughout this section we consider the primal program in standard form
</p>
<p>minimize cTx
</p>
<p>subject to Ax = b, x � 0
(4.3)
</p>
<p>and its corresponding dual
</p>
<p>maximize yTb
</p>
<p>subject to yTA � cT .
(4.4)
</p>
<p>In this section it is not assumed that A is necessarily of full rank. The following
</p>
<p>lemma is easily established and gives us an important relation between the two
</p>
<p>problems.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.2 The Duality Theorem 87
</p>
<p>Fig. 4.1 Relation of primal and dual values
</p>
<p>Lemma 1 (Weak Duality Lemma). If x and y are feasible for (4.3) and (4.4),respectively,
</p>
<p>then cTx � yTb.
</p>
<p>Proof. We have
</p>
<p>yTb = yTAx � cTx,
</p>
<p>the last inequality being valid since x � 0 and yTA � cT . �
</p>
<p>This lemma shows that a feasible vector to either problem yields a bound on the
</p>
<p>value of the other problem. The values associated with the primal are all larger than
</p>
<p>the values associated with the dual as illustrated in Fig. 4.1. Since the primal seeks
</p>
<p>a minimum and the dual seeks a maximum, each seeks to reach the other. From this
</p>
<p>we have an important corollary.
</p>
<p>Corollary. If x0 and y0 are feasible for (4.3) and (4.4), respectively, and if c
Tx0 = y
</p>
<p>T
0
</p>
<p>b,
</p>
<p>then x0 and y0 are optimal for their respective problems.
</p>
<p>The above corollary shows that if a pair of feasible vectors can be found to the
</p>
<p>primal and dual programs with equal objective values, then these are both optimal.
</p>
<p>The Duality Theorem of linear programming states that the converse is also true,
</p>
<p>and that, in fact, the two regions in Fig. 4.1 actually have a common point; there is
</p>
<p>no &ldquo;gap.&rdquo;
</p>
<p>Duality Theorem of Linear Programming. If either of the problems (4.3) or (4.4) has a
finite optimal solution, so does the other, and the corresponding values of the objective
</p>
<p>functions are equal. If either problem has an unbounded objective, the other problem has
</p>
<p>no feasible solution.
</p>
<p>Proof. We note first that the second statement is an immediate consequence of
</p>
<p>Lemma 1. For if the primal is unbounded and y is feasible for the dual, we must
</p>
<p>have yTb � &minus;M for arbitrarily large M, which is clearly impossible.
Second we note that although the primal and dual are not stated in symmetric
</p>
<p>form it is sufficient, in proving the first statement, to assume that the primal has
</p>
<p>a finite optimal solution and then show that the dual has a solution with the same
</p>
<p>value. This follows because either problem can be converted to standard form and
</p>
<p>because the roles of primal and dual are reversible.
</p>
<p>Suppose (4.3) has a finite optimal solution with value z0. In the space E
m+1 define
</p>
<p>the convex set
</p>
<p>C = {(r, w) : r = tz0 &minus; cTx, w = tb &minus; Ax, x � 0, t � 0}.
</p>
<p>It is easily verified that C is in fact a closed convex cone. We show that the point (1,
</p>
<p>0) is not in C. If w = t0b &minus; Ax0 = 0 with t0 &gt; 0, x0 � 0, then x = x0/t0 is feasible
for (4.3) and hence r/t0 = z0 &minus; cTx � 0; which means r � 0. If w = &minus;Ax0 = 0</p>
<p/>
</div>
<div class="page"><p/>
<p>88 4 Duality and Complementarity
</p>
<p>with x0 � 0 and c
Tx0 = &minus;1, and if x is any feasible solution to (4.3), then x + αx0 is
</p>
<p>feasible for any α � 0 and gives arbitrarily small objective values as α is increased.
</p>
<p>This contradicts our assumption on the existence of a finite optimum and thus we
</p>
<p>conclude that no such x0 exists. Hence (1, 0) � C.
</p>
<p>Now since C is a closed convex set, there is by Theorem 4.4, Sect. B.3, a hyper-
</p>
<p>plane separating (1, 0) and C. Thus there is a nonzero vector [s, y] &isin; Em+1 and a
constant c such that
</p>
<p>s &lt; c = inf{sr + yTw : (r, w) &isin; C}.
</p>
<p>Now since C is a cone, it follows that c � 0. For if there were (r, w) &isin; C such that
sr + yTw &lt; 0, then α(r, w) for large α would violate the hyperplane inequality. On
</p>
<p>the other hand, since (0, 0) &isin; C we must have c � 0. Thus c = 0. As a consequence
s &lt; 0, and without loss of generality we may assume s = &minus;1.
</p>
<p>We have to this point established the existence of y &isin; Em such that
</p>
<p>&minus;r + yTw � 0
</p>
<p>for all (r, w) &isin; C. Equivalently, using the definition of C,
</p>
<p>(c &minus; yTA)x &minus; tz0 + tyTb � 0
</p>
<p>for all x � 0, t � 0. Setting t = 0 yields yTA � cT , which says y is feasible for the
</p>
<p>dual. Setting x = 0 and t = 1 yields yTb � z0, which in view of Lemma 1 and its
</p>
<p>corollary shows that y is optimal for the dual. �
</p>
<p>4.3 Relations to the Simplex Procedure
</p>
<p>In this section the Duality Theorem is proved by making explicit use of the char-
</p>
<p>acteristics of the simplex procedure. As a result of this proof it becomes clear that
</p>
<p>once the primal is solved by the simplex procedure a solution to the dual is readily
</p>
<p>obtainable.
</p>
<p>Suppose that for the linear program
</p>
<p>minimize cTx
</p>
<p>subject to Ax = b, x � 0,
(4.5)
</p>
<p>we have the optimal basic feasible solution x = (xB, 0) with corresponding basis B.
</p>
<p>We shall determine a solution of the dual program
</p>
<p>maximize yTb
</p>
<p>subject to yTA � cT
(4.6)
</p>
<p>in terms of B.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.3 Relations to the Simplex Procedure 89
</p>
<p>We partition A as A = [B, D]. Since the basic feasible solution xB = B
&minus;1b is
</p>
<p>optimal, the relative cost vector r must be nonnegative in each component. From
</p>
<p>Sect. 3.6 we have
</p>
<p>rTD = c
T
D &minus; cTBB&minus;1D,
</p>
<p>and since rD is nonnegative in each component we have c
T
B
</p>
<p>B&minus;1D � cT
D
</p>
<p>.
</p>
<p>Now define yT = cT
B
</p>
<p>B&minus;1. We show that this choice of y solves the dual problem.
We have
</p>
<p>yTA = [yTB, yTD] = [cTB, c
T
BB
</p>
<p>&minus;1D] � [cTB, c
T
D] = c
</p>
<p>T .
</p>
<p>Thus since yTA � cT , y is feasible for the dual. On the other hand,
</p>
<p>yTb = cTBB
&minus;1b = cTBxB,
</p>
<p>and thus the value of the dual objective function for this y is equal to the value of
</p>
<p>the primal problem. This, in view of Lemma 1, Sect. 4.2, establishes the optimality
</p>
<p>of y for the dual. The above discussion yields an alternative derivation of the main
</p>
<p>portion of the Duality Theorem.
</p>
<p>Theorem. Let the linear program (4.5) have an optimal basic feasible solution correspond-
</p>
<p>ing to the basis B. Then the vector y satisfying yT = cT
B
B&minus;1 is an optimal solution to the
</p>
<p>dual program (4.6). The optimal values of both problems are equal.
</p>
<p>We turn now to a discussion of how the solution of the dual can be obtained
</p>
<p>directly from the final simplex tableau of the primal. Suppose that embedded in the
</p>
<p>original matrix A is an m &times; m identity matrix. This will be the case if, for example,
m slack variables are employed to convert inequalities to equalities. Then in the
</p>
<p>final tableau the matrix B&minus;1 appears where the identity appeared in the beginning.
Furthermore, in the last row the components corresponding to this identity matrix
</p>
<p>will be cT
I
&minus; cT
</p>
<p>B
B&minus;1, where cI is the m-vector representing the cost coefficients of
</p>
<p>the variables corresponding to the columns of the original identity matrix. Thus by
</p>
<p>subtracting these cost coefficients from the corresponding elements in the last row,
</p>
<p>the negative of the solution yT = cT
B
</p>
<p>B&minus;1 to the dual is obtained. In particular, if, as
is the case with slack variables, cI = 0, then the elements in the last row under B
</p>
<p>&minus;1
</p>
<p>are equal to the negative of components of the solution to the dual.
</p>
<p>Example. Consider the primal program
</p>
<p>minimize &minus;x1 &minus; 4x2 &minus; 3x3
subject to 2x1 + 2x2 + x3 � 4
</p>
<p>x1 + 2x2 + 2x3 � 6
</p>
<p>x1 � 0, x2 � 0, x3 � 0.
</p>
<p>This can be solved by introducing slack variables and using the simplex proce-
</p>
<p>dure. The appropriate sequence of tableaus is given below without explanation.</p>
<p/>
</div>
<div class="page"><p/>
<p>90 4 Duality and Complementarity
</p>
<p>2 ➁ 1 1 0 4
</p>
<p>1 2 2 0 1 6
</p>
<p>&minus;1 &minus;4 &minus;3 0 0 0
1 1 1/2 1/2 0 2
</p>
<p>&minus;1 0 ➀ &minus;1 1 2
3 0 &minus;1 2 0 8
</p>
<p>3/2 1 0 1 &minus;1/2 1
&minus;1 0 1 &minus;1 1 2
</p>
<p>2 0 0 1 1 10
</p>
<p>The optimal solution is x1 = 0, x2 = 1, x3 = 2. The corresponding dual program is
</p>
<p>maximize 4λ1 + 6λ2
subject to 2λ1 + λ2 � &minus;1
</p>
<p>2λ1 + 2λ2 � &minus;4
λ1 + 2λ2 � &minus;3
λ1 � 0, λ2 � 0.
</p>
<p>The optimal solution to the dual is obtained directly from the last row of the sim-
</p>
<p>plex tableau under the columns where the identity appeared in the first tableau:
</p>
<p>λ1 = &minus;1, λ2 = &minus;1.
</p>
<p>Geometric Interpretation
</p>
<p>The duality relations can be viewed in terms of the dual interpretations of linear
</p>
<p>constraints emphasized in Chap. 3. Consider a linear program in standard form. For
</p>
<p>sake of concreteness we consider the problem
</p>
<p>minimize 18x1 + 12x2 + 2x3 + 6x4
subject to 3x1 + x2 &minus; 2x3 + x4 = 2
</p>
<p>x1 + 3x2 &minus; x4 = 2
x1 � 0, x2 � 0, x3 � 0, x4 � 0.
</p>
<p>The columns of the constraints are represented in requirements space in Fig. 4.2.
</p>
<p>A basic solution represents construction of b with positive weights on two of the ai&rsquo;s.
</p>
<p>The dual problem is
</p>
<p>maximize 2λ1 + 2λ2
subject to 3λ1 + λ2 � 18
</p>
<p>λ1 + 3λ2 � 12
</p>
<p>&minus;2λ1 � 2
λ1 &minus; λ2 � 6.
</p>
<p>The dual problem is shown geometrically in Fig. 4.3. Each column ai of the pri-
</p>
<p>mal defines a constraint of the dual as a half-space whose boundary is orthogonal</p>
<p/>
</div>
<div class="page"><p/>
<p>4.3 Relations to the Simplex Procedure 91
</p>
<p>Fig. 4.2 The primal requirements space
</p>
<p>to that column vector and is located at a point determined by ci. The dual objective
</p>
<p>is maximized at an extreme point of the dual feasible region. At this point exactly
</p>
<p>two dual constraints are active. These active constraints correspond to an optimal
</p>
<p>basis of the primal. In fact, the vector defining the dual objective is a positive linear
</p>
<p>combination of the vectors. In the specific example, b is a positive combination of
</p>
<p>a1 and a2. The weights in this combination are the xi&rsquo;s in the solution of the primal.
</p>
<p>Fig. 4.3 The dual in activity space</p>
<p/>
</div>
<div class="page"><p/>
<p>92 4 Duality and Complementarity
</p>
<p>Simplex Multipliers
</p>
<p>We conclude this section by giving an economic interpretation of the relation
</p>
<p>between the simplex basis and the vector y. At any point in the simplex procedure
</p>
<p>we may form the vector y satisfying yT = cT
B
</p>
<p>B&minus;1. This vector is not a solution to the
dual unless B is an optimal basis for the primal, but nevertheless, it has an economic
</p>
<p>interpretation. Furthermore, as we have seen in the development of the revised sim-
</p>
<p>plex method, this y vector can be used at every step to calculate the relative cost
</p>
<p>coefficients. For this reason yT = cT
B
</p>
<p>B&minus;1, corresponding to any basis, is often called
the vector of simplex multipliers.
</p>
<p>Let us pursue the economic interpretation of these simplex multipliers. As usual,
</p>
<p>denote the columns of A by a1, a2, . . . , an and denote by e1, e2, . . . , em the m unit
</p>
<p>vectors in Em. The components of the ai&rsquo;s and b tell how to construct these vectors
</p>
<p>from the ei&rsquo;s.
</p>
<p>Given any basis B, however, consisting of m columns of A, any other vector
</p>
<p>can be constructed (synthetically) as a linear combination of these basis vectors.
</p>
<p>If there is a unit cost ci associated with each basis vector ai, then the cost of a
</p>
<p>(synthetic) vector constructed from the basis can be calculated as the corresponding
</p>
<p>linear combination of the ci&rsquo;s associated with the basis. In particular, the cost of the
</p>
<p>jth unit vector, e j, when constructed from the basis B, is λ j, the jth component of
</p>
<p>yT = cT
B
</p>
<p>B&minus;1. Thus the λ j&rsquo;s can be interpreted as synthetic prices of the unit vectors.
Now, any vector can be expressed in terms of the basis B in two steps: (1) express
</p>
<p>the unit vectors in terms of the basis, and then (2) express the desired vector as a
</p>
<p>linear combination of unit vectors. The corresponding synthetic cost of a vector con-
</p>
<p>structed from the basis B can correspondingly be computed directly by: (1) finding
</p>
<p>the synthetic price of the unit vectors, and then (2) using these prices to evaluate the
</p>
<p>cost of the linear combination of unit vectors. Thus, the simplex multipliers can be
</p>
<p>used to quickly evaluate the synthetic cost of any vector that is expressed in terms of
</p>
<p>the unit vectors. The difference between the true cost of this vector and the synthetic
</p>
<p>cost is the relative cost. The process of calculating the synthetic cost of a vector,
</p>
<p>with respect to a given basis, by using the simplex multipliers is sometimes referred
</p>
<p>to as pricing out the vector.
</p>
<p>Optimality of the primal corresponds to the situation where every vector a1, a2,
</p>
<p>. . . , an is cheaper when constructed from the basis than when purchased directly at
</p>
<p>its own price. Thus we have yTai � ci for i = 1, 2, . . . , n or equivalently y
TA � cT .
</p>
<p>4.4 Sensitivity and Complementary Slackness
</p>
<p>The optimal values of the dual variables in a linear program can, as we have seen, be
</p>
<p>interpreted as prices. In this section this interpretation is explored in further detail.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.4 Sensitivity and Complementary Slackness 93
</p>
<p>Sensitivity
</p>
<p>Suppose in the linear program
</p>
<p>minimize cTx
</p>
<p>subject to Ax = b, x � 0,
(4.7)
</p>
<p>the optimal basis is B with corresponding solution (xB, 0), where xB = B
&minus;1b. A
</p>
<p>solution to the corresponding dual is yT = cT
B
</p>
<p>B&minus;1.
Now, assuming nondegeneracy, small changes in the vector b will not cause the
</p>
<p>optimal basis to change. Thus for b + ∆b the optimal solution is
</p>
<p>x = (xB + ∆xB, 0),
</p>
<p>where ∆xB = B
&minus;1
∆b. Thus the corresponding increment in the cost function is
</p>
<p>∆z = cTB∆xB = y
T
∆b. (4.8)
</p>
<p>This equation shows that y gives the sensitivity of the optimal cost with respect to
</p>
<p>small changes in the vector b. In other words, if a new program were solved with b
</p>
<p>changed to b + ∆b, the change in the optimal value of the objective function would
</p>
<p>be yT∆b.
</p>
<p>This interpretation of the dual vector y is intimately related to its interpretation
</p>
<p>as a vector of simplex multipliers. Since λ j is the price of the unit vector e j when
</p>
<p>constructed from the basis B, it directly measures the change in cost due to a change
</p>
<p>in the jth component of the vector b. Thus, λ j may equivalently be considered as
</p>
<p>the marginal price of the component b j, since if b j is changed to b j + ∆b j the value
</p>
<p>of the optimal solution changes by λ j∆b j.
</p>
<p>If the linear program is interpreted as a diet problem, for instance, then λ j is
</p>
<p>the maximum price per unit that the dietitian would be willing to pay for a small
</p>
<p>amount of the jth nutrient, because decreasing the amount of nutrient that must
</p>
<p>be supplied by food will reduce the food bill by λ j dollars per unit. If, as another
</p>
<p>example, the linear program is interpreted as the problem faced by a manufacturer
</p>
<p>who must select levels x1, x2, . . . , xn of n production activities in order to meet
</p>
<p>certain required levels of output b1, b2, . . . , bm while minimizing production costs,
</p>
<p>the λi&rsquo;s are the marginal prices of the outputs. They show directly how much the
</p>
<p>production cost varies if a small change is made in the output levels.
</p>
<p>Complementary Slackness
</p>
<p>The optimal solutions to primal and dual programs satisfy an additional relation
</p>
<p>that has an economic interpretation. This relation can be stated for any pair of dual
</p>
<p>linear programs, but we state it here only for the asymmetric and the symmetric
</p>
<p>pairs defined in Sect. 4.1.</p>
<p/>
</div>
<div class="page"><p/>
<p>94 4 Duality and Complementarity
</p>
<p>Theorem. (Complementary slackness&mdash;asymmetric form). Let x and y be feasible solu-
tions for the primal and dual programs, respectively, in the pair (4.2). A necessary and
</p>
<p>sufficient condition that they both be optimal solutions is that&dagger; for all i
</p>
<p>i) xi &gt; 0 &rArr; yTai = ci
ii) xi = 0 &lArr; yTa j &lt; c j.
</p>
<p>Proof. If the stated conditions hold, then clearly (yTA &minus; cT )x = 0. Thus yTb =
cTx, and by the corollary to Lemma 1, Sect. 4.2, the two solutions are optimal.
</p>
<p>Conversely, if the two solutions are optimal, it must hold, by the Duality Theo-
</p>
<p>rem, that yTb = cTx and hence that (yTA &minus; cT )x = 0. Since each component of x is
nonnegative and each component of yTA &minus; cT is nonpositive, the conditions (i) and
(ii) must hold. �
</p>
<p>Theorem. (Complementary slackness&mdash;symmetric form). Let x and y be feasible solutions
for the primal and dual programs, respectively, in the pair (4.1). A necessary and sufficient
</p>
<p>condition that they both be optimal solutions is that for all i and j
</p>
<p>i) xi &gt; 0 &rArr; yTai = ci
ii) xi = 0 &lArr; yTai &lt; ci
iii) λ j &gt; 0 &rArr; a jx = b j
iv) λ j = 0 &lArr; a jx &gt; b j,
</p>
<p>(where a j is the jth row of A).
</p>
<p>Proof. This follows by transforming the previous theorem. �
</p>
<p>The complementary slackness conditions have a rather obvious economic inter-
</p>
<p>pretation. Thinking in terms of the diet problem, for example, which is the primal
</p>
<p>part of a symmetric pair of dual problems, suppose that the optimal diet supplies
</p>
<p>more than b j units of the jth nutrient. This means that the dietitian would be unwill-
</p>
<p>ing to pay anything for small quantities of that nutrient, since availability of it would
</p>
<p>not reduce the cost of the optimal diet. This, in view of our previous interpretation
</p>
<p>of λ j as a marginal price, implies λ j = 0 which is (iv) of Theorem 4.4. The other
</p>
<p>conditions have similar interpretations which the reader can work out.
</p>
<p>4.5 Max Flow&ndash;Min Cut Theorem
</p>
<p>One of the most exemplary pairs of linear primal and dual problems is the max-flow
</p>
<p>and min-cut theorem, which we describe in this section. The maximal flow problem
</p>
<p>described in Chap. 2 can be expressed more compactly in terms of the node&ndash;arc
</p>
<p>incidence matrix (see Appendix D). Let x be the vector of arc flows xi j (ordered in
</p>
<p>any way). Let A be the corresponding node-arc incidence matrix. Finally, let e be a
</p>
<p>&dagger; The symbol &rArr; means &ldquo;implies&rdquo; and &lArr; means &ldquo;is implied by.&rdquo;</p>
<p/>
</div>
<div class="page"><p/>
<p>4.5 Max Flow&ndash;Min Cut Theorem 95
</p>
<p>vector with dimension equal to the number of nodes and having a + 1 component on
</p>
<p>node 1, a &ndash; 1 on node m, and all other components zero. The maximal flow problem
</p>
<p>is then
</p>
<p>maximize f
</p>
<p>subject to Ax &minus; f e = 0 (4.9)
x � k.
</p>
<p>The coefficient matrix of this problem is equal to the node&ndash;arc incidence matrix with
</p>
<p>an additional column for the flow variable f . Any basis of this matrix is triangular,
</p>
<p>and hence as indicated by the theory in the transportation problem in Chap. 3, the
</p>
<p>simplex method can be effectively employed to solve this problem. However, instead
</p>
<p>of the simplex method, a simple algorithm based on the tree algorithm (also see
</p>
<p>Appendix D) can be used.
</p>
<p>Max Flow Augmenting Algorithm
</p>
<p>The basic strategy of the algorithm is quite simple. First we recognize that it is
</p>
<p>possible to send nonzero flow from node 1 to node m only if node m is reachable
</p>
<p>from node 1. The tree procedure can be used to determine if m is in fact reachable;
</p>
<p>and if it is reachable, the algorithm will produce a path from 1 to m. By examining
</p>
<p>the arcs along this path, we can determine the one with minimum capacity. We may
</p>
<p>then construct a flow equal to this capacity from 1 to m by using this path. This gives
</p>
<p>us a strictly positive (and integer-valued) initial flow.
</p>
<p>Next consider the nature of the network at this point in terms of additional flows
</p>
<p>that might be assigned. If there is already flow xi j in the arc (i, j), then the effective
</p>
<p>capacity of that arc is reduced by xi j(to ki j &minus; xi j), since that is the maximal amount
of additional flow that can be assigned to that arc. On the other hand, the effective
</p>
<p>reverse capacity, on the arc ( j, i), is increased by xi j(to k ji + xi j), since a small incre-
</p>
<p>mental backward flow is actually realized as a reduction in the forward flow through
</p>
<p>that arc. Once these changes in capacities have been made, the tree procedure can
</p>
<p>again be used to find a path from node 1 to node m on which to assign additional
</p>
<p>flow. (Such a path is termed an augmenting path.) Finally, if m is not reachable
</p>
<p>from 1, no additional flow can be assigned, and the procedure is complete.
</p>
<p>It is seen that the method outlined above is based on repeated application of
</p>
<p>the tree procedure, which is implemented by labeling and scanning. By including
</p>
<p>slightly more information in the labels than in the basic tree algorithm, the minimum
</p>
<p>arc capacity of the augmenting path can be determined during the initial scanning,
</p>
<p>instead of by reexamining the arcs after the path is found. A typical label at a node
</p>
<p>i has the form (k, ci), where k denotes a precursor node and ci is the maximal flow
</p>
<p>that can be sent from the source to node i through the path created by the previous
</p>
<p>labeling and scanning. The complete procedure is this:</p>
<p/>
</div>
<div class="page"><p/>
<p>96 4 Duality and Complementarity
</p>
<p>Step 0. Set all xi j = 0 and f = 0.
</p>
<p>Step 1. Label node 1 (&minus;, &infin;). All other nodes are unlabeled.
Step 2. Select any labeled node i for scanning. Say it has label (k, ci). For all
</p>
<p>unlabeled nodes j such that (i, j) is an arc with xi j &lt; ki j, assign the label (i, c j),
</p>
<p>where c j = min {ci, ki j &minus; xi j}. For all unlabeled nodes j such that ( j, i) is an arc
with x ji &gt; 0, assign the label (i, c j), where c j = min {ci, x ji}.
Step 3. Repeat Step 2 until either node m is labeled or until no more labels can
</p>
<p>be assigned. In this latter case, the current solution is optimal.
</p>
<p>Step 4. (Augmentation.) If the node m is labeled (i, cm), then increase f and
</p>
<p>the flow on arc (i,m) by cm. Continue to work backward along the augmenting
</p>
<p>path determined by the nodes, increasing the flow on each arc of the path by cm.
</p>
<p>Return to Step 1.
</p>
<p>The validity of the algorithm should be fairly apparent, that is, the finite termi-
</p>
<p>nation of the algorithm. However, a complete proof is deferred until we consider
</p>
<p>the max flow-min cut theorem below.
</p>
<p>Example. An example of the above procedure is shown in Fig. 4.4. Node 1 is the
</p>
<p>source, and node 6 is the sink. The original network with capacities indicated on the
</p>
<p>arcs is shown in Fig. 4.4a. Also shown in that figure are the initial labels obtained by
</p>
<p>the procedure. In this case the sink node is labeled, indicating that a flow of 1 unit
</p>
<p>can be achieved. The augmenting path of this flow is shown in Fig. 4.4b. Numbers
</p>
<p>in square boxes indicate the total flow in an arc. The new labels are then found and
</p>
<p>added to that figure. Note that node 2 cannot be labeled from node 1 because there
</p>
<p>is no unused capacity in that direction. Node 2 can, however, be labeled from node
</p>
<p>4, since the existing flow provides a reverse capacity of 1 unit. Again the sink is
</p>
<p>labeled, and 1 unit more flow can be constructed. The augmenting path is shown in
</p>
<p>Fig. 4.4c. A new labeling is appended to that figure. Again the sink is labeled, and
</p>
<p>an additional 1 unit of flow can be sent from source to sink. The path of this 1 unit is
</p>
<p>shown in Fig. 4.4d. Note that it includes a flow from node 4 to node 2, even though
</p>
<p>flow was not allowed in this direction in the original network. This flow is allowable
</p>
<p>now, however, because there is already flow in the opposite direction. The total flow
</p>
<p>at this point is shown in Fig. 4.4e. The flow levels are again in square boxes. This
</p>
<p>flow is maximal, since only the source node can be labeled.
</p>
<p>Max Flow&ndash;Min Cut Theorem
</p>
<p>A great deal of insight and some further results can be obtained through the
</p>
<p>introduction of the notion of cuts in a network. Given a network with source node
</p>
<p>1 and sink node m, divide the nodes arbitrarily into two sets S and S̄ such that
</p>
<p>the source node is in S and the sink is in S̄ . The set of arcs from S to S̄ is a cut and
</p>
<p>is denoted (S , S̄ ). The capacity of the cut is the sum of the capacities of the arcs in
</p>
<p>the cut.
</p>
<p>An example of a cut is shown in Fig. 4.5. The set S consists of nodes 1 and 2,
</p>
<p>while S̄ consists of 3, 4, 5, 6. The capacity of this cut is 4.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.5 Max Flow&ndash;Min Cut Theorem 97
</p>
<p>Fig. 4.4 Illustration of algorithmic steps of the maximal flow example</p>
<p/>
</div>
<div class="page"><p/>
<p>98 4 Duality and Complementarity
</p>
<p>Fig. 4.5 A cut
</p>
<p>It should be clear that a path from node 1 to node m must include at least one arc
</p>
<p>in any cut, for the path must have an arc from the set S to the set S̄ . Furthermore, it
</p>
<p>is clear that the maximal amount of flow that can be sent through a cut is equal to
</p>
<p>its capacity. Thus each cut gives an upper bound on the value of the maximal flow
</p>
<p>problem. The max flow-min cut theorem states that equality is actually achieved for
</p>
<p>some cut. That is, the maximal flow is equal to the minimal cut capacity. It should
</p>
<p>be noted that the proof of the theorem also establishes the maximality of the flow
</p>
<p>obtained by the maximal flow algorithm.
</p>
<p>Max Flow&ndash;Min Cut Theorem. In a network the maximal flow between a source and a sink
</p>
<p>is equal to the minimal cut capacity of all cuts separating the source and sink.
</p>
<p>Proof. Since any cut capacity must be greater than or equal to the maximal flow, it is
</p>
<p>only necessary to exhibit a flow and a cut for which equality is achieved. Begin with
</p>
<p>a flow in the network that cannot be augmented by the maximal flow algorithm. For
</p>
<p>this flow find the effective arc capacities of all arcs for incremental flow changes as
</p>
<p>described earlier and apply the labeling procedure of the maximal flow algorithm.
</p>
<p>Since no augmenting path exists, the algorithm must terminate before the sink is
</p>
<p>labeled.
</p>
<p>Let S and S̄ consist of all labeled and unlabeled nodes, respectively. This defines
</p>
<p>a cut separating the source from the sink. All arcs originating in S and terminating
</p>
<p>in S̄ have zero incremental capacity, or else a node in S̄ could have been labeled.
</p>
<p>This means that each arc in the cut is saturated by the original flow; that is, the
</p>
<p>flow is equal to the capacity. Any arc originating in S̄ and terminating in S , on the
</p>
<p>other hand, must have zero flow; otherwise, this would imply a positive incremental
</p>
<p>capacity in the reverse direction, and the originating node in S̄ would be labeled.
</p>
<p>Thus, there is a total flow from S to S̄ equal to the cut capacity, and zero flow from
</p>
<p>S̄ to S . This means that the flow from source to sink is equal to the cut capacity.
</p>
<p>Thus the cut capacity must be minimal, and the flow must be maximal. �
</p>
<p>In the network of Fig. 4.4, the minimal cut corresponds to the S consisting only
</p>
<p>of the source. That cut capacity is 3. Note that in accordance with the max flow&ndash;
</p>
<p>min cut theorem, this is equal to the value of the maximal flow, and the minimal</p>
<p/>
</div>
<div class="page"><p/>
<p>4.5 Max Flow&ndash;Min Cut Theorem 99
</p>
<p>cut is determined by the final labeling in Fig. 4.4e. In Fig. 4.5 the cut shown is also
</p>
<p>minimal, and the reader should easily be able to determine the pattern of maximal
</p>
<p>flow.
</p>
<p>Relation to Duality
</p>
<p>The character of the max flow&ndash;min cut theorem suggests a connection with the
</p>
<p>Duality Theorem. We conclude this section by exploring this connection.
</p>
<p>The maximal flow problem is a linear program, which is expressed formally
</p>
<p>by (4.9). The dual problem is found to be
</p>
<p>minimize wTk
</p>
<p>subject to uTA = wT (4.10)
</p>
<p>uT e = 1
</p>
<p>w &ge; 0.
</p>
<p>When written out in detail, the dual is
</p>
<p>minimize
&sum;
</p>
<p>i j
</p>
<p>wi jki j
</p>
<p>subject to ui &minus; u j = wi j
u1 &minus; um = 1 (4.11)
wi j &ge; 0.
</p>
<p>A pair i, j is included in the above only if (i, j) is an arc of the network.
</p>
<p>A feasible solution to this dual problem can be found in terms of any cut
</p>
<p>set (S , S̄ ). In particular, it is easily seen that
</p>
<p>ui =
</p>
<p>{
</p>
<p>1 if i &isin; S
0 if i &isin; S̄ (4.12)
</p>
<p>wi j =
</p>
<p>{
</p>
<p>1 if (i, j) &isin; (S , S̄ )
0 otherwise
</p>
<p>is a feasible solution. The value of the dual problem corresponding to this solution
</p>
<p>is the cut capacity. If we take the cut set to be the one determined by the labeling
</p>
<p>procedure of the maximal flow algorithm as described in the proof of the theorem
</p>
<p>above, it can be seen to be optimal by verifying the complementary slackness con-
</p>
<p>ditions (a task we leave to the reader). The minimum value of the dual is therefore
</p>
<p>equal to the minimum cut capacity.</p>
<p/>
</div>
<div class="page"><p/>
<p>100 4 Duality and Complementarity
</p>
<p>4.6 The Dual Simplex Method
</p>
<p>Often there is available a basic solution to a linear program which is not feasible but
</p>
<p>which prices out optimally; that is, the simplex multipliers are feasible for the dual
</p>
<p>problem. In the simplex tableau this situation corresponds to having no negative ele-
</p>
<p>ments in the bottom row but an infeasible basic solution. Such a situation may arise,
</p>
<p>for example, if a solution to a certain linear programming problem is calculated and
</p>
<p>then a new problem is constructed by changing the vector b. In such situations a
</p>
<p>basic feasible solution to the dual is available and hence it is desirable to pivot in
</p>
<p>such a way as to optimize the dual.
</p>
<p>Rather than constructing a tableau for the dual problem (which, if the primal is
</p>
<p>in standard form; involves m free variables and n nonnegative slack variables), it is
</p>
<p>more efficient to work on the dual from the primal tableau. The complete technique
</p>
<p>based on this idea is the dual simplex method. In terms of the primal problem,
</p>
<p>it operates by maintaining the optimality condition of the last row while working
</p>
<p>toward feasibility. In terms of the dual problem, however, it maintains feasibility
</p>
<p>while working toward optimality.
</p>
<p>Given the linear program
</p>
<p>minimize cTx
</p>
<p>subject to Ax = b, x � 0,
(4.13)
</p>
<p>suppose a basis B is known such that y defined by yT = cT
B
</p>
<p>B&minus;1 is feasible for the
dual. In this case we say that the corresponding basic solution to the primal, xB =
</p>
<p>B&minus;1b, is dual feasible. If xB � 0 then this solution is also primal feasible and hence
optimal.
</p>
<p>The given vector y is feasible for the dual and thus satisfies yTa j � c j, for j =
</p>
<p>1, 2, . . . , n. Indeed, assuming as usual that the basis is the first m columns of A,
</p>
<p>there is equality
</p>
<p>yTa j = c j, for j = 1, 2, . . . , m, (4.14a)
</p>
<p>and (barring degeneracy in the dual) there is inequality
</p>
<p>yTa j &lt; c j, for j = m + 1, . . . , n. (4.14b)
</p>
<p>To develop one cycle of the dual simplex method, we find a new vector y such that
</p>
<p>one of the equalities becomes an inequality and one of the inequalities becomes
</p>
<p>equality, while at the same time increasing the value of the dual objective function.
</p>
<p>The m equalities in the new solution then determine a new basis.
</p>
<p>Denote the ith row of B&minus;1 by ui. Then for
</p>
<p>y
T
= yT &minus; εui, (4.15)
</p>
<p>we have yTa j = y
Ta j &minus; εu ja j. Thus, recalling that z j = yTa j and noting that uia j =
</p>
<p>yij, the ijth element of the tableau, we have</p>
<p/>
</div>
<div class="page"><p/>
<p>4.6 The Dual Simplex Method 101
</p>
<p>y
T
</p>
<p>a j = c j, j = 1, 2, . . . , m, i � j (4.16a)
</p>
<p>y
T
</p>
<p>ai = ci &minus; ε (4.16b)
y
T
</p>
<p>a j = z j &minus; εyij, j = m + 1, m + 2, . . . , n. (4.16c)
</p>
<p>Also,
</p>
<p>y
T
</p>
<p>b = yTb &minus; εxBi. (4.17)
These last equations lead directly to the algorithm:
</p>
<p>Step 1. Given a dual feasible basic solution xB, if xB � 0 the solution is optimal.
</p>
<p>If xB is not nonnegative, select an index i such that the ith component of xB, xBi &lt;
</p>
<p>0.
</p>
<p>Step 2. If all yij � 0, j = 1, 2, . . . , n, then the dual has no maximum (this follows
</p>
<p>since by (4.16) λ̄ is feasible for all ε &gt; 0). If yij &lt; 0 for some j, then let
</p>
<p>ε0 =
zk &minus; ck
yik
</p>
<p>= min
j
</p>
<p>{
</p>
<p>z j &minus; c j
yij
</p>
<p>: yij &lt; 0
</p>
<p>}
</p>
<p>. (4.18)
</p>
<p>Step 3. Form a new basis B by replacing ai by ak. Using this basis determine the
</p>
<p>corresponding basic dual feasible solution xB and return to Step 1.
</p>
<p>The proof that the algorithm converges to the optimal solution is similar in its
</p>
<p>details to the proof for the primal simplex procedure. The essential observations are:
</p>
<p>(a) from the choice of k in (4.18) and from (4.16a), (4.16b), 4.16c) the new solution
</p>
<p>will again be dual feasible; (b) by (4.17) and the choice xBi &lt; 0, the value of the dual
</p>
<p>objective will increase; (c) the procedure cannot terminate at a nonoptimum point;
</p>
<p>and (d) since there are only a finite number of bases, the optimum must be achieved
</p>
<p>in a finite number of steps.
</p>
<p>Example. A form of problem arising frequently is that of minimizing a positive
</p>
<p>combination of positive variables subject to a series of &ldquo;greater than&rdquo; type inequal-
</p>
<p>ities having positive coefficients. Such problems are natural candidates for applica-
</p>
<p>tion of the dual simplex procedure. The classical diet problem is of this type as is
</p>
<p>the simple example below.
</p>
<p>minimize 3x1 + 4x2 + 5x3
subject to xi + 2x2 + 3x3 � 5
</p>
<p>2x1 + 2x2 + x3 � 6
</p>
<p>x1 � 0, x2 � 0, x3 � 0.
</p>
<p>By introducing surplus variables and by changing the sign of the inequalities we
</p>
<p>obtain the initial tableau
</p>
<p>&minus;1 &minus;2 &minus;3 1 0 &minus;5
&minus;➁ &minus;2 &minus;1 0 1 &minus;6
</p>
<p>3 4 5 0 0 0
</p>
<p>Initial tableau</p>
<p/>
</div>
<div class="page"><p/>
<p>102 4 Duality and Complementarity
</p>
<p>The basis corresponds to a dual feasible solution since all of the c j &minus; z j&rsquo;s are
nonnegative. We select any xBi &lt; 0, say x5 = &minus;6, to remove from the set of basic
variables. To find the appropriate pivot element in the second row we compute the
</p>
<p>ratios (z j &minus; c j)/y2 j and select the minimum positive ratio. This yields the pivot indi-
cated. Continuing, the remaining tableaus are
</p>
<p>0 &minus;➀ &minus;5/2 1 &minus;1/2 &minus;2
1 1 1/2 0 &minus;1/2 3
0 1 7/2 0 3/2 9
</p>
<p>Second tableau
</p>
<p>0 1 5/2 &minus;1 1/2 2
1 0 &minus;2 1 &minus;1 1
0 0 1 1 1 11
</p>
<p>Final tableau
</p>
<p>The third tableau yields a feasible solution to the primal which must be optimal.
</p>
<p>Thus the solution is x1 = 1, x2 = 2, x3 = 0.
</p>
<p>*4.7 &lowast;The Primal-Dual Algorithm
</p>
<p>In this section a procedure is described for solving linear programming problems by
</p>
<p>working simultaneously on the primal and the dual problems. The procedure begins
</p>
<p>with a feasible solution to the dual that is improved at each step by optimizing an
</p>
<p>associated restricted primal problem. As the method progresses it can be regarded
</p>
<p>as striving to achieve the complementary slackness conditions for optimality. Orig-
</p>
<p>inally, the primal-dual method was developed for solving a special kind of linear
</p>
<p>program arising in network flow problems, and it continues to be the most efficient
</p>
<p>procedure for these problems. (For general linear programs the dual simplex method
</p>
<p>is most frequently used). In this section we describe the generalized version of the
</p>
<p>algorithm and point out an interesting economic interpretation of it. We consider the
</p>
<p>program
</p>
<p>minimize cTx
</p>
<p>subject to Ax = b, x � 0
(4.19)
</p>
<p>and the corresponding dual program
</p>
<p>maximize yTb
</p>
<p>subject to yTA � cT .
(4.20)
</p>
<p>Given a feasible solution y to the dual, define the subset P of 1, 2, . . . , n by
</p>
<p>i &isin; P if yTai = ci where ai is the ith column of A. Thus, since y is dual feasible, it
follows that i � P implies yTai &lt; ci. Now corresponding to y and P, we define the
</p>
<p>associated restricted primal problem</p>
<p/>
</div>
<div class="page"><p/>
<p>4.7 &lowast;The Primal-Dual Algorithm 103
</p>
<p>minimize 1Ty
</p>
<p>subject to Ax + y = b
</p>
<p>x � 0, xi = 0 for i � P
</p>
<p>y � 0,
</p>
<p>(4.21)
</p>
<p>where 1 denotes the m-vector (1, 1, . . ., 1).
</p>
<p>The dual of this associated restricted primal is called the associated restricted
</p>
<p>dual. It is
maximize uTb
</p>
<p>subject to uTai � 0, i � P
</p>
<p>u � 1.
</p>
<p>(4.22)
</p>
<p>The condition for optimality of the primal-dual method is expressed in the following
</p>
<p>theorem.
</p>
<p>Primal-Dual Optimality Theorem. Suppose that y is feasible for the dual and that x and
</p>
<p>y = 0 is feasible (and of course optimal) for the associated restricted primal. Then x and y
are optimal for the original primal and dual programs, respectively.
</p>
<p>Proof. Clearly x is feasible for the primal. Also we have cTx = yTAx, because yTA
</p>
<p>is identical to cT on the components corresponding to nonzero elements of x. Thus
</p>
<p>cTx = yTAx = yTb and optimality follows from Lemma 1, Sect. 4.2. �
</p>
<p>The primal&ndash;dual method starts with a feasible solution to the dual and then
</p>
<p>optimizes the associated restricted primal. If the optimal solution to this associated
</p>
<p>restricted primal is not feasible for the primal, the feasible solution to the dual is
</p>
<p>improved and a new associated restricted primal is determined. Here are the details:
</p>
<p>Step 1. Given a feasible solution y0 to the dual program (4.20), determine the
</p>
<p>associated restricted primal according to (4.21).
</p>
<p>Step 2. Optimize the associated restricted primal. If the minimal-value of this
</p>
<p>problem is zero, the corresponding solution is optimal for the original primal
</p>
<p>by the Primal-Dual Optimality Theorem.
</p>
<p>Step 3. If the minimal value of the associated restricted primal is strictly posi-
</p>
<p>tive, obtain from the final simplex tableau of the restricted primal, the solution
</p>
<p>u0 of the associated restricted dual (4.22). If there is no j for which u
T
0
</p>
<p>a j &gt; 0
</p>
<p>conclude the primal has no feasible solutions. If, on the other hand, for at least
</p>
<p>one j, uT0 a j &gt; 0, define the new dual feasible vector
</p>
<p>y = y0 + ε0u0
</p>
<p>where
</p>
<p>ε0 =
ck &minus; yT0 ak
</p>
<p>uT
0
</p>
<p>ak
= min
</p>
<p>j
</p>
<p>⎧
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>c j &minus; yT0 a j
uT
</p>
<p>0
a j
</p>
<p>: uT0 a j &gt; 0
</p>
<p>⎫
</p>
<p>⎪
</p>
<p>⎬
</p>
<p>⎪
</p>
<p>⎭
</p>
<p>.
</p>
<p>Now go back to Step 1 using this y.
</p>
<p>To prove convergence of this method a few simple observations and explanations
</p>
<p>must be made. First we verify the statement made in Step 3 that uT0 a j � 0 for all j</p>
<p/>
</div>
<div class="page"><p/>
<p>104 4 Duality and Complementarity
</p>
<p>implies that the primal has no feasible solution. The vector yε = y0 + εu0 is feasible
</p>
<p>for the dual problem for all positive ε, since uT
0
</p>
<p>A � 0. In addition, yTε b = y
T
0
</p>
<p>b+εuT
0
</p>
<p>b
</p>
<p>and, since uT
0
</p>
<p>b = 1Ty &gt; 0, we see that as ε is increased we obtain an unbounded
</p>
<p>solution to the dual. In view of the Duality Theorem, this implies that there is no
</p>
<p>feasible solution to the primal.
</p>
<p>Next suppose that in Step 3, for at least one j, uT
0
</p>
<p>a j &gt; 0. Again we define the
</p>
<p>family of vectors yε = y0 + εu0. Since u0 is a solution to (4.22) we have u
T
0 ai � 0
</p>
<p>for i &isin; P, and hence for small positive ε the vector yε is feasible for the dual. We
increase ε to the first point where one of inequalities yTε a j &lt; c j, j � P becomes
</p>
<p>an equality. This determines ε0 &gt; 0 and k. The new y vector corresponds to an in-
</p>
<p>creased value of the dual objective yTb = yT
0
</p>
<p>b+εuT
0
</p>
<p>b. In addition, the corresponding
</p>
<p>new set P now includes the index k. Any other index i that corresponded to a pos-
</p>
<p>itive value of xi in the associated restricted primal is in the new set P, because by
</p>
<p>complementary slackness uT
0
</p>
<p>ai = 0 for such an i and thus y
Tai = y
</p>
<p>T
0
</p>
<p>ai+ε0u
T
0
</p>
<p>ai = ci.
</p>
<p>This means that the old optimal solution is feasible for the new associated restricted
</p>
<p>primal and that ak can be pivoted into the basis. Since u
T
0 ak &gt; 0, pivoting in ak will
</p>
<p>decrease the value of the associated restricted primal.
</p>
<p>In summary, it has been shown that at each step either an improvement in the
</p>
<p>associated primal is made or an infeasibility condition is detected. Assuming non-
</p>
<p>degeneracy, this implies that no basis of the associated primal is repeated&mdash;and since
</p>
<p>there are only a finite number of possible bases, the solution is reached in a finite
</p>
<p>number of steps.
</p>
<p>The primal-dual algorithm can be given an interesting interpretation in terms of
</p>
<p>the manufacturing problem in Example 2, Sect. 2.2. Suppose we own a facility that
</p>
<p>is capable of engaging in n different production activities each of which produces
</p>
<p>various amounts of m commodities. Each activity i can be operated at any level
</p>
<p>xi � 0, but when operated at the unity level the ith activity costs ci dollars and yields
</p>
<p>the m commodities in the amounts specified by the m-vector ai. Assuming linearity
</p>
<p>of the production facility, if we are given a vector b describing output requirements
</p>
<p>of the m commodities, and we wish to produce these at minimum cost, ours is the
</p>
<p>primal problem.
</p>
<p>Imagine that an entrepreneur not knowing the value of our requirements vector b
</p>
<p>decides to sell us these requirements directly. He assigns a price vector y0 to these
</p>
<p>requirements such that yT0 A � c. In this way his prices are competitive with our
</p>
<p>production activities, and he can assure us that purchasing directly from him is no
</p>
<p>more costly than engaging activities. As owner of the production facilities we are
</p>
<p>reluctant to abandon our production enterprise but, on the other hand, we deem it not
</p>
<p>frugal to engage an activity whose output can be duplicated by direct purchase for
</p>
<p>lower cost. Therefore, we decide to engage only activities that cannot be duplicated
</p>
<p>cheaper, and at the same time we attempt to minimize the total business volume
</p>
<p>given the entrepreneur. Ours is the associated restricted primal problem.
</p>
<p>Upon receiving our order, the greedy entrepreneur decides to modify his prices
</p>
<p>in such a manner as to keep them competitive with our activities but increase the
</p>
<p>cost of our order. As a reasonable and simple approach he seeks new prices of the
</p>
<p>form</p>
<p/>
</div>
<div class="page"><p/>
<p>4.7 &lowast;The Primal-Dual Algorithm 105
</p>
<p>y = y0 + εu0,
</p>
<p>where he selects u0 as the solution to
</p>
<p>maximize uTy
</p>
<p>subject to uTai � 0, i &isin; P
u � 1.
</p>
<p>The first set of constraints is to maintain competitiveness of his new price vector for
</p>
<p>small ε, while the second set is an arbitrary bound imposed to keep this subproblem
</p>
<p>bounded. It is easily shown that the solution u0 to this problem is identical to the
</p>
<p>solution of the associated dual (4.22). After determining the maximum ε to maintain
</p>
<p>feasibility, he announces his new prices.
</p>
<p>At this point, rather than concede to the price adjustment, we recalculate the new
</p>
<p>minimum volume order based on the new prices. As the greedy (and shortsighted)
</p>
<p>entrepreneur continues to change his prices in an attempt to maximize profit he
</p>
<p>eventually finds he has reduced his business to zero! At that point we have, with his
</p>
<p>help, solved the original primal problem.
</p>
<p>Example. To illustrate the primal-dual method and indicate how it can be imple-
</p>
<p>mented through use of the tableau format consider the following problem:
</p>
<p>minimize 2x1 + x2 + 4x3
subject to x1 + x2 + 2x3 = 3
</p>
<p>2x1 + x2 + 3x3 = 5
</p>
<p>x1 � 0, x2 � 0, x3 � 0.
</p>
<p>Because all of the coefficients in the objective function are nonnegative, y = (0, 0)
</p>
<p>is a feasible vector for the dual. We lay out the simplex tableau shown below
</p>
<p>a1 a2 a3 &middot; &middot; b
1 1 2 1 0 3
</p>
<p>2 1 3 0 1 5
</p>
<p>&minus;3 &minus;2 &minus;5 0 0 &minus;8
ci &minus; yTai &rarr; 2 1 4 &middot; &middot; &middot;
</p>
<p>First tableau
</p>
<p>To form this tableau we have adjoined artificial variables in the usual manner.
</p>
<p>The third row gives the relative cost coefficients of the associated primal problem&mdash;
</p>
<p>the same as the row that would be used in a phase I procedure. In the fourth row
</p>
<p>are listed the ci &minus; yTai&rsquo;s for the current y. The allowable columns in the associated
restricted primal are determined by the zeros in this last row.
</p>
<p>Since there are no zeros in the last row, no progress can be made in the associated
</p>
<p>restricted primal and hence the original solution x1 = x2 = x3 = 0, y1 = 3, y2 = 5 is
</p>
<p>optimal for this y. The solution u0 to the associated restricted dual is u0 = (1, 1), and
</p>
<p>the numbers &minus;uT0 ai, i = 1, 2, 3 are equal to the first three elements in the third row.</p>
<p/>
</div>
<div class="page"><p/>
<p>106 4 Duality and Complementarity
</p>
<p>Thus, we compute the three ratios 2
3
, 1
</p>
<p>2
, 4
</p>
<p>5
from which we find ε0 =
</p>
<p>1
2
. The new
</p>
<p>values for the fourth row are now found by adding ε0 times the (first three) elements
</p>
<p>of the third row to the fourth row.
</p>
<p>a1 a2 a3 &middot; &middot; b
1 ➀ 2 1 0 3
</p>
<p>2 1 3 0 1 5
</p>
<p>&minus;3 &minus;2 &minus;5 0 0 &minus;8
1/2 0 3/2 &middot; &middot; &middot;
</p>
<p>Second tableau
</p>
<p>Minimizing the new associated restricted primal by pivoting as indicated we obtain
</p>
<p>a1 a2 a3 &middot; &middot; b
1 1 2 1 0 3
</p>
<p>1 0 1 &minus;1 1 2
&minus;1 0 &minus;1 2 0 &minus;2
&minus;1/2 0 3/2 &middot; &middot; &middot;
</p>
<p>Now we again calculate the ratios 1
2
, 3
</p>
<p>2
obtaining ε0 =
</p>
<p>1
2
, and add this multiple of
</p>
<p>the third row to the fourth row to obtain the next tableau.
</p>
<p>a1 a2 a3 &middot; &middot; b
1 1 2 1 0 3
</p>
<p>➀ 0 1 &minus;1 1 2
&minus;1 0 &minus;1 2 0 &minus;2
</p>
<p>0 0 1 &middot; &middot; &middot;
Third tableau
</p>
<p>optimizing the new restricted primal we obtain the tableau:
</p>
<p>a1 a2 a3 &middot; &middot; b
0 1 1 2 &minus;1 1
1 0 1 &minus;1 1 2
0 0 0 1 1 0
</p>
<p>0 0 1 &middot; &middot; &middot;
Final tableau
</p>
<p>Having obtained feasibility in the primal, we conclude that the solution is also
</p>
<p>optimal: x1 = 2, x2 = 1, x3 = 0.
</p>
<p>4.8 Summary
</p>
<p>There is a corresponding dual linear program associated with every (primal) linear
</p>
<p>program. Both programs share the same underlying cost and constraint coefficients.
</p>
<p>We have demonstrated rich theorems to relate the pair. The variables of the dual</p>
<p/>
</div>
<div class="page"><p/>
<p>4.9 Exercises 107
</p>
<p>problem can be interpreted as prices associated with the constraints of the original
</p>
<p>(primal) problem, and through this association it is possible to give an economically
</p>
<p>meaningful characterization to the dual whenever there is such a characterization
</p>
<p>for the primal.
</p>
<p>Mathematically, the pair also establish an optimality certificate to each other:
</p>
<p>one cannot claim an optimal objective value unless you find an solution for the
</p>
<p>dual to achieve the same value of the dual objective. This also leads to the set of
</p>
<p>optimality conditions, including the complementarity conditions, that we would see
</p>
<p>many times in the rest of the book.
</p>
<p>4.9 Exercises
</p>
<p>1. Verify in detail that the dual of a linear program is the original problem.
</p>
<p>2. Show that if a linear inequality in a linear program is changed to equality, the
</p>
<p>corresponding dual variable becomes free.
</p>
<p>3. Find the dual of
minimize cTx
</p>
<p>subject to Ax = b, x � a
</p>
<p>where a � 0.
</p>
<p>4. Show that in the transportation problem the linear equality constraints are not
</p>
<p>linearly independent, and that in an optimal solution to the dual problem the
</p>
<p>dual variables are not unique. Generalize this observation to any linear program
</p>
<p>having redundant equality constraints.
</p>
<p>5. Construct an example of a primal problem that has no feasible solutions and
</p>
<p>whose corresponding dual also has no feasible solutions.
</p>
<p>6. Let A be anm&times;n matrix and b be an n-vector. Prove that Ax � 0 implies cTx � 0
if and only if cT = yTA for some y � 0. Give a geometric interpretation of the
</p>
<p>result.
</p>
<p>7. There is in general a strong connection between the theories of optimization and
</p>
<p>free competition, which is illustrated by an idealized model of activity location.
</p>
<p>Suppose there are n economic activities (various factories, homes, stores, etc.)
</p>
<p>that are to be individually located on n distinct parcels of land. If activity i is
</p>
<p>located on parcel j that activity can yield sij units (dollars) of value.
</p>
<p>If the assignment of activities to land parcels is made by a central authority, it
</p>
<p>might be made in such a way as to maximize the total value generated. In other
</p>
<p>words, the assignment would be made so as to maximize
&sum;
</p>
<p>i
</p>
<p>&sum;
</p>
<p>j sijxij where
</p>
<p>xij =
</p>
<p>⎧
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>1 if activity i is assigned to parcel j
</p>
<p>0 otherwise</p>
<p/>
</div>
<div class="page"><p/>
<p>108 4 Duality and Complementarity
</p>
<p>More explicitly this approach leads to the optimization problem
</p>
<p>maximize
&sum;
</p>
<p>i
</p>
<p>&sum;
</p>
<p>j
</p>
<p>sijxij
</p>
<p>subject to
&sum;
</p>
<p>j
</p>
<p>xij = 1, i = 1, 2, . . . , n
</p>
<p>&sum;
</p>
<p>i
</p>
<p>xij = 1, j = 1, 2, . . . , n
</p>
<p>xij � 0, xij = 0 or 1.
</p>
<p>Actually, it can be shown that the final requirement (xij = 0 or 1 ) is automati-
</p>
<p>cally satisfied at any extreme point of the set defined by the other constraints, so
</p>
<p>that in fact the optimal assignment can be found by using the simplex method
</p>
<p>of linear programming.
</p>
<p>If one considers the problem from the viewpoint of free competition, it is
</p>
<p>assumed that, rather than a central authority determining the assignment, the
</p>
<p>individual activities bid for the land and thereby establish prices.
</p>
<p>(a) Show that there exists a set of activity prices pi, i = 1, 2, . . . , n and land
</p>
<p>prices q j, j = 1, 2, . . . , n such that
</p>
<p>pi + q j � sij, i = 1, 2, . . . , n, j = 1, 2, . . . , n
</p>
<p>with equality holding if in an optimal assignment activity i is assigned to
</p>
<p>parcel j.
</p>
<p>(b) Show that Part (a) implies that if activity i is optimally assigned to parcel j
</p>
<p>and if j&prime; is any other parcel
</p>
<p>sij &minus; q j � sij&prime; &minus; q j&prime; .
</p>
<p>Give an economic interpretation of this result and explain the relation
</p>
<p>between free competition and optimality in this context.
</p>
<p>(c) Assuming that each sij is positive, show that the prices can all be assumed
</p>
<p>to be nonnegative.
</p>
<p>8. Construct the dual of the combinatorial auction problem of Example 7 of
</p>
<p>Chap. 2, and give an economical interpretation for each type of the dual vari-
</p>
<p>ables.
</p>
<p>9. Game theory is in part related to linear programming theory. Consider the game
</p>
<p>in which player X may select any one of m moves, and player Y may select any
</p>
<p>one of n moves. If X selects i and Y selects j, then X wins an amount aij from Y.
</p>
<p>The game is repeated many times. Player X develops a mixed strategy where the
</p>
<p>various moves are played according to probabilities represented by the compo-
</p>
<p>nents of the vector x = (x1, x2, . . . , xm), where x1 � 0, i = 1, 2, . . . , m
</p>
<p>and
m
&sum;
</p>
<p>i=1
xi = 1. Likewise Y develops a mixed strategy y = (y1, y2, . . . , yn),
</p>
<p>where yi � 0, i = 1, 2, . . . , n and
n
&sum;
</p>
<p>i=1
yi = 1. The average payoff to X is then
</p>
<p>P(x, y) = xTAy.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.9 Exercises 109
</p>
<p>(a) Suppose X selects x as the solution to the linear program
</p>
<p>maximize A
</p>
<p>subject to
m
&sum;
</p>
<p>i=1
xi = 1
</p>
<p>m
&sum;
</p>
<p>i=1
xiaij � A, j = 1, 2, . . . , n
</p>
<p>xi � 0, i = 1, 2, . . . , m.
</p>
<p>Show that X is guaranteed a payoff of at least A no matter what y is chosen
</p>
<p>by Y.
</p>
<p>(b) Show that the dual of the problem above is
</p>
<p>minimize B
</p>
<p>subject to
n
&sum;
</p>
<p>j=1
y j = 1
</p>
<p>n
&sum;
</p>
<p>j=1
aijy j � B, i = 1, 2, . . . , m
</p>
<p>y j � 0, j = 1, 2, . . . , n.
</p>
<p>(c) Prove that max A = min B. (The common value is called the value of the
</p>
<p>game.)
</p>
<p>(d) Consider the &ldquo;matching&rdquo; game. Each player selects heads or tails. If the
</p>
<p>choices match, X wins $1 from Y; if they do not match, Y wins $1 from X.
</p>
<p>Find the value of this game and the optimal mixed strategies.
</p>
<p>(e) Repeat Part (d) for the game where each player selects either 1, 2, or 3.
</p>
<p>The player with the highest number wins $1 unless that number is exactly
</p>
<p>1 higher than the other player&rsquo;s number, in which case he loses $3. When
</p>
<p>the numbers are equal there is no payoff.
</p>
<p>10. Consider the primal linear program in the standard form. Suppose that this
</p>
<p>program and its dual are feasible. Let y be a known optimal solution to the
</p>
<p>dual.
</p>
<p>(a) If the kth equation of the primal is multiplied by μ � 0, determine an opti-
</p>
<p>mal solution w to the dual of this new problem.
</p>
<p>(b) Suppose that, in the original primal, we add μ times the kth equation to
</p>
<p>the rth equation. What is an optimal solution w to the corresponding dual
</p>
<p>problem?
</p>
<p>(c) Suppose, in the original primal, we add μ times the kth row of A to c. What
</p>
<p>is an optimal solution to the corresponding dual problem?
</p>
<p>11. Consider the linear program (P) of the form
</p>
<p>minimize qT z
</p>
<p>subject to Mz &ge; &minus;q, z &ge; 0
</p>
<p>in which the matrix M is skew symmetric; that is, M = &minus;MT .</p>
<p/>
</div>
<div class="page"><p/>
<p>110 4 Duality and Complementarity
</p>
<p>(a) Show that problem (P) and its dual are the same.
</p>
<p>(b) A problem of the kind in part (a) is said to be self-dual. An example of a
</p>
<p>self-dual problem has
</p>
<p>M =
</p>
<p>[
</p>
<p>0 &minus;AT
A 0
</p>
<p>]
</p>
<p>, q =
</p>
<p>[
</p>
<p>c
</p>
<p>&minus;b
</p>
<p>]
</p>
<p>, z =
</p>
<p>[
</p>
<p>x
</p>
<p>y
</p>
<p>]
</p>
<p>.
</p>
<p>Give an interpretation of the problem with this data.
</p>
<p>(c) Show that a self-dual linear program has an optimal solution if and only if
</p>
<p>it is feasible.
</p>
<p>12. A company may manufacture n different products, each of which uses various
</p>
<p>amounts of m limited resources. Each unit of product i yields a profit of ci
dollars and uses aji units of the jth resource. The available amount of the jth
</p>
<p>resource is b j. To maximize profit the company selects the quantities xi to be
</p>
<p>manufactured of each product by solving
</p>
<p>maximize cTx
</p>
<p>subject to Ax = b, x � 0.
</p>
<p>The unit profits ci already take into account the variable cost associated with
</p>
<p>manufacturing each unit. In addition to that cost, the company incurs a fixed
</p>
<p>overhead H, and for accounting purposes it wants to allocate this overhead to
</p>
<p>each of its products. In other words, it wants to adjust the unit profits so as to
</p>
<p>account for the overhead. Such an overhead allocation scheme must satisfy two
</p>
<p>conditions: (4.1) Since H is fixed regardless of the product mix, the overhead
</p>
<p>allocation scheme must not alter the optimal solution, (4.2) All the overhead
</p>
<p>must be allocated; that is, the optimal value of the objective with the modified
</p>
<p>cost coefficients must be H dollars lower than z&mdash;the original optimal value of
</p>
<p>the objective.
</p>
<p>(a) Consider the allocation scheme in which the unit profits are modified
</p>
<p>according to ĉT = cT&minus;ryT0 A, where y0 is the optimal solution to the original
dual and r = H/z0 (assume H � z0).
</p>
<p>(i) Show that the optimal x for the modified problem is the same as that
</p>
<p>for the original problem, and the new dual solution is ŷ0 = (1 &minus; r)y0.
(ii) Show that this approach fully allocates H.
</p>
<p>(b) Suppose that the overhead can be traced to each of the resource constraints.
</p>
<p>Let Hi � 0 be the amount of overhead associated with the ith resource,
</p>
<p>where
m
&sum;
</p>
<p>i=1
Hi � z0 and ri = Hi/bi � λ
</p>
<p>0
i
</p>
<p>for i = 1, . . . , m. Based on this
</p>
<p>information, an allocation scheme has been proposed where the unit profits
</p>
<p>are modified such that ĉT = cT &minus; rTA.
(i) Show that the optimal x for this modified problem is the same as that for
</p>
<p>the original problem, and the corresponding dual solution is ŷ0 = y0&minus;r.
(ii) Show that this scheme fully allocates H.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.9 Exercises 111
</p>
<p>13. Solve the linear inequalities
</p>
<p>&minus;2x1 + 2x2 � &minus;1
2x1 &minus; x2 � 2
</p>
<p>&minus; 4x2 � 3
&minus;15x1 &minus; 12x2 � &minus;2
</p>
<p>12x1 + 20x2 � &minus;1.
</p>
<p>Note that x1 and x2 are not restricted to be positive. Solve this problem by
</p>
<p>considering the problem of maximizing 0 &middot; x1+0 &middot; x2 subject to these constraints,
taking the dual and using the simplex method.
</p>
<p>14. (a) Using the simplex method solve
</p>
<p>minimize 2x1 &minus; x2
subject to 2x1 &minus; x2 &minus; x3 � 3
</p>
<p>x1 &minus; x2 + x3 � 2
xi � 0, i = 1, 2, 3.
</p>
<p>(Hint: Note that x1 = 2 gives a feasible solution.)
</p>
<p>(b) What is the dual problem and its optimal solution?
</p>
<p>15. (a) Using the simplex method solve
</p>
<p>minimize 2x1 + 3x2 + 2x3 + 2x4
subject to x1 + 2x2 + x3 + 2x4 = 3
</p>
<p>x1 + x2 + 2x3 + 4x4 = 5
</p>
<p>xi � 0, i = 1, 2, 3, 4.
</p>
<p>(b) Using the work done in Part (a) and the dual simplex method, solve the
</p>
<p>same problem but with the right-hand sides of the equations changed to 8
</p>
<p>and 7 respectively.
</p>
<p>16. For the problem
</p>
<p>minimize 5x1 + 3x2
subject to 2x1 &minus; x2 + 4x3 � 4
</p>
<p>x1 + x2 + 2x3 � 5
</p>
<p>2x1 &minus; x2 + x3 � 1
x1 � 0, x2 � 0, x3 � 0;
</p>
<p>(a) Using a single pivot operation with pivot element 1, find a feasible solution.
</p>
<p>(b) Using the simplex method, solve the problem.
</p>
<p>(c) What is the dual problem?
</p>
<p>(d) What is the solution to the dual?</p>
<p/>
</div>
<div class="page"><p/>
<p>112 4 Duality and Complementarity
</p>
<p>17. Solve the following problem by the dual simplex method:
</p>
<p>minimize &minus; 7x1 + 7x2 &minus; 2x3 &minus; x4 &minus; 6x5
subject to 3x1 &minus; x2 + x3 &minus; 2x4 = &minus;3
</p>
<p>2x1 + x2 + x4 + x5 = 4
</p>
<p>&minus;x1 + 3x2 &minus; 3x4 + x6 = 12
and xi � 0, i = 1, . . . , 6.
</p>
<p>18. Given the linear programming problem in standard form (4.3) suppose a basis B
</p>
<p>and the corresponding (not necessarily feasible) primal and dual basic solutions
</p>
<p>x and y are known. Assume that at least one relative cost coefficient ci &minus; yTai is
negative. Consider the auxiliary problem
</p>
<p>minimize cTx
</p>
<p>subject to Ax = b
&sum;
</p>
<p>i&isin;T
xi + y = M
</p>
<p>x � 0, y � 0,
</p>
<p>where T = {i : ci &minus; yTai &lt; 0}, y is a slack variable, and M is a large positive
constant. Show that if k is the index corresponding to the most negative rela-
</p>
<p>tive cost coefficient in the original solution, then (y, ck &minus; yTak) is dual feasible
for the auxiliary problem. Based on this observation, develop a big-M artificial
</p>
<p>constraint method for the dual simplex method. (Refer to Exercise 24, Chap. 3.)
</p>
<p>19. A textile firm is capable of producing three products&mdash;x1, x2, x3. Its production
</p>
<p>plan for next month must satisfy the constraints
</p>
<p>x1 + 2x2 + 2x3 � 12
</p>
<p>2x1 + 4x2 + x3 � f
</p>
<p>x1 � 0, x2 � 0, x3 � 0.
</p>
<p>The first constraint is determined by equipment availability and is fixed. The
</p>
<p>second constraint is determined by the availability of cotton. The net profits of
</p>
<p>the products are 2, 3, and 3, respectively, exclusive of the cost of cotton and
</p>
<p>fixed costs.
</p>
<p>(a) Find the shadow price λ2 of the cotton input as a function of f . (Hint: Use
</p>
<p>the dual simplex method.) Plot λ2( f ) and the net profit z( f ) exclusive of the
</p>
<p>cost for cotton.
</p>
<p>(b) The firm may purchase cotton on the open market at a price of 1/6. How-
</p>
<p>ever, it may acquire a limited amount at a price of 1/12 from a major sup-
</p>
<p>plier that it purchases from frequently. Determine the net profit of the firm
</p>
<p>π(s) as a function of s.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.9 Exercises 113
</p>
<p>20. A certain telephone company would like to determine the maximum number
</p>
<p>of long-distance calls from Westburgh to Eastville that it can handle at any one
</p>
<p>time. The company has cables linking these cities via several intermediary cities
</p>
<p>as follows:
</p>
<p>Each cable can handle a maximum number of calls simultaneously as indicated
</p>
<p>in the figure. For example, the number of calls routed from Westburgh to North-
</p>
<p>gate cannot exceed five at any one time. A call from Westburgh to Eastville can
</p>
<p>be routed through any other city, as long as there is a cable available that is not
</p>
<p>currently being used to its capacity. In addition to determining the maximum
</p>
<p>number of calls from Westburgh to Eastville, the company would, of course,
</p>
<p>like to know the optimal routing of these calls. Assume calls can be routed only
</p>
<p>in the directions indicated by the arrows.
</p>
<p>(a) Formulate the above problem as a linear programming problem with upper
</p>
<p>bounds.
</p>
<p>(Hint: Denote by xij the number of calls routed from city i to city j.)
</p>
<p>(b) Find the solution by inspection of the graph.
</p>
<p>21. Apply the maximal flow algorithm to the network below. All arcs have capacity
</p>
<p>1 unless otherwise indicated.
</p>
<p>22. Consider the problem
</p>
<p>minimize 2x1 + x2 + 4x3
subject to x1 + x2 + 2x3 = 3
</p>
<p>2x1 + x2 + 3x3 = 5
</p>
<p>xi � 0, x2 � 0, x3 � 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>114 4 Duality and Complementarity
</p>
<p>(a) What is the dual problem?
</p>
<p>(b) Note that y = (1, 0) is feasible for the dual. Starting with this y, solve the
</p>
<p>primal using the primal-dual algorithm.
</p>
<p>23. Show that in the associated restricted dual of the primal-dual method the
</p>
<p>objective yTb can be replaced by yTy.
</p>
<p>24. Consider the primal feasible region in standard form Ax = b, x � 0, where A is
</p>
<p>an m &times; n matrix, b is a constant nonzero m-vector, and x is a variable n-vector.
</p>
<p>(a)A variable xi is said to be a null variable if xi = 0 in every feasible solution.
</p>
<p>Prove that, if the feasible region is non-empty, xi is a null variable if and only
</p>
<p>if there is a nonzero vector y &isin; Em such that yTA &ge; 0, yTb = 0. and the ith
component of yTA is strictly positive.
</p>
<p>(b)Strict complementarity Let the feasible region be nonempty. Then there is a
</p>
<p>feasible x and vector y &isin; Em such that
</p>
<p>yTA &ge; 0, yTb = 0, yTb + x &gt; 0.
</p>
<p>(c)A variable xi is a nonextremal variable if xi &gt; 0 in every feasible solution.
</p>
<p>Prove that, if the feasible region is non-empty, xi is a nonextremal variable
</p>
<p>if and only if there is y &isin; Em and d &isin; En such that yTA = dT , where
di = &minus;1, d j � 0 for j � i; and such that yTb &lt; 0.
</p>
<p>References
</p>
<p>4.1&ndash;4.4 Again most of the material in this chapter is now quite standard. See the
</p>
<p>references of Chap. 2. A particularly careful discussion of duality can be
</p>
<p>found in Simonnard [S6].
</p>
<p>4.5 Koopmans [K8] was the first to discover the relationship between bases and
</p>
<p>tree structures in a network. The classic reference for network flow theory is
</p>
<p>Ford and Fulkerson [F13]. For discussion of even more efficient versions of
</p>
<p>the maximal flow algorithm, see Lawler [L2] and Papadimitriou and Stei-
</p>
<p>glitz [P2]. The Hungarian method for the assignment problem was designed
</p>
<p>by Kuhn [K10]. It is called the Hungarian method because it was based on
</p>
<p>work by the Hungarian mathematicians Egerváry and König. Ultimately,
</p>
<p>this led to the general primal&ndash;dual algorithm for linear programming.
</p>
<p>4.6 The dual simplex method is due to Lemke [L4].
</p>
<p>4.7 The general primal-dual algorithm is due to Dantzig, Ford and Fulker-
</p>
<p>son [D7]. See also Ford and Fulkerson [F13]. The economic interpretation
</p>
<p>given in this section is apparently novel.
</p>
<p>The concepts of reduction are due to Shefi [S5], who has developed a
</p>
<p>complete theory in this area. For more details along the lines presented
</p>
<p>here, see Luenberger [L15].</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 5
</p>
<p>Interior-Point Methods
</p>
<p>Linear programs can be viewed in two somewhat complementary ways. They are,
</p>
<p>in one view, a class of continuous optimization problems each with continuous vari-
</p>
<p>ables defined on a convex feasible region and with a continuous objective function.
</p>
<p>They are, therefore, a special case of the general form of problem considered in
</p>
<p>this text. However, linearity implies a certain degree of degeneracy, since for exam-
</p>
<p>ple the derivatives of all functions are constants and hence the differential methods
</p>
<p>of general optimization theory cannot be directly used. From an alternative view,
</p>
<p>linear programs can be considered as a class of combinatorial problems because it
</p>
<p>is known that solutions can be found by restricting attention to the vertices of the
</p>
<p>convex polyhedron defined by the constraints. Indeed, this view is natural when con-
</p>
<p>sidering network problems such as those of early chapters. However, the number of
</p>
<p>vertices may be large, up to n!/m!(n&minus;m) !, making direct search impossible for even
modest size problems.
</p>
<p>The simplex method embodies both of these viewpoints, for it restricts attention
</p>
<p>to vertices, but exploits the continuous nature of the variables to govern the progress
</p>
<p>from one vertex to another, defining a sequence of adjacent vertices with improving
</p>
<p>values of the objective as the process reaches an optimal point. The simplex method,
</p>
<p>with ever-evolving improvements, has for five decades provided an efficient general
</p>
<p>method for solving linear programs.
</p>
<p>Although it performs well in practice, visiting only a small fraction of the total
</p>
<p>number of vertices, a definitive theory of the simplex method&rsquo;s performance was
</p>
<p>unavailable. However, in 1972, Klee and Minty showed by examples that for certain
</p>
<p>linear programs the simplex method will examine every vertex f. These examples
</p>
<p>proved that in the worst case, the simplex method requires a number of steps that is
</p>
<p>exponential in the size of the problem.
</p>
<p>&copy; Springer International Publishing Switzerland 2016
</p>
<p>D.G. Luenberger, Y. Ye, Linear and Nonlinear Programming, International
Series in Operations Research &amp; Management Science 228,
DOI 10.1007/978-3-319-18842-3 5
</p>
<p>115</p>
<p/>
</div>
<div class="page"><p/>
<p>116 5 Interior-Point Methods
</p>
<p>In view of this result, many researchers believed that a good algorithm, differ-
</p>
<p>ent than the simplex method, might be devised whose number of steps would be
</p>
<p>polynomial rather than exponential in the program&rsquo;s size&mdash;that is, the time required
</p>
<p>to compute the solution would be bounded above by a polynomial in the size of the
</p>
<p>problem.1
</p>
<p>Indeed, in 1979, a new approach to linear programming, Khachiyan&rsquo;s ellipsoid
</p>
<p>method was announced with great acclaim. The method is quite different in struc-
</p>
<p>ture than the simplex method, for it constructs a sequence of shrinking ellipsoids
</p>
<p>each of which contains the optimal solution set and each member of the sequence
</p>
<p>is smaller in volume than its predecessor by at least a certain fixed factor. There-
</p>
<p>fore, the solution set can be found to any desired degree of approximation by con-
</p>
<p>tinuing the process. Khachiyan proved that the ellipsoid method, developed dur-
</p>
<p>ing the 1970s by other mathematicians, is a polynomial-time algorithm for linear
</p>
<p>programming.
</p>
<p>Practical experience, however, was disappointing. In almost all cases, the simplex
</p>
<p>method was much faster than the ellipsoid method. However, Khachiyan&rsquo;s ellipsoid
</p>
<p>method showed that polynomial time algorithms for linear programming do exist.
</p>
<p>It left open the question of whether one could be found that, in practice, was faster
</p>
<p>than the simplex method.
</p>
<p>It is then perhaps not surprising that the announcement by Karmarkar in 1984
</p>
<p>of a new polynomial time algorithm, an interior-point method, with the potential
</p>
<p>to improve the practical effectiveness of the simplex method made front-page news
</p>
<p>in major newspapers and magazines throughout the world. It is this interior-point
</p>
<p>approach that is the subject of this chapter and the next.
</p>
<p>This chapter begins with a brief introduction to complexity theory, which is the
</p>
<p>basis for a way to quantify the performance of iterative algorithms, distinguishing
</p>
<p>polynomial-time algorithms from others.
</p>
<p>Next the example of Klee and Minty showing that the simplex method is not
</p>
<p>a polynomial-time algorithm in the worst case is presented. Following that the
</p>
<p>ellipsoid algorithm is defined and shown to be a polynomial-time algorithm. These
</p>
<p>two sections provide a deeper understanding of how the modern theory of linear
</p>
<p>programming evolved, and help make clear how complexity theory impacts linear
</p>
<p>programming. However, the reader may wish to consider them optional and omit
</p>
<p>them at first reading.
</p>
<p>The development of the basics of interior-point theory begins with Sect. 5.4
</p>
<p>which introduces the concept of barrier functions and the analytic center. Section 5.5
</p>
<p>introduces the central path which underlies interior-point algorithms. The relations
</p>
<p>between primal and dual in this context are examined. An overview of the details
</p>
<p>of specific interior-point algorithms based on the theory are presented in Sects. 5.6
</p>
<p>and 5.7
</p>
<p>1 We will be more precise about complexity notions such as &ldquo;polynomial algorithm&rdquo; in Sect. 5.1
below.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.1 Elements of Complexity Theory 117
</p>
<p>5.1 Elements of Complexity Theory
</p>
<p>Complexity theory is arguably the foundation for analysis of computer algorithms.
</p>
<p>The goal of the theory is twofold: to develop criteria for measuring the effectiveness
</p>
<p>of various algorithms (and thus, be able to compare algorithms using these criteria),
</p>
<p>and to assess the inherent difficulty of various problems.
</p>
<p>The term complexity refers to the amount of resources required by a computa-
</p>
<p>tion. In this chapter we focus on a particular resource, namely, computing time. In
</p>
<p>complexity theory, however, one is not interested in the execution time of a pro-
</p>
<p>gram implemented in a particular programming language, running on a particular
</p>
<p>computer over a particular input. This involves too many contingent factors. In-
</p>
<p>stead, one wishes to associate to an algorithm more intrinsic measures of its time
</p>
<p>requirements.
</p>
<p>Roughly speaking, to do so one needs to define:
</p>
<p>&bull; a notion of input size,
&bull; a set of basic operations, and
&bull; a cost for each basic operation.
The last two allow one to associate a cost of a computation. If x is any input, the
</p>
<p>cost C(x) of the computation with input x is the sum of the costs of all the basic
</p>
<p>operations performed during this computation.
</p>
<p>Let A be an algorithm and Jn be the set of all its inputs having size n. The
worst-case cost function of A is the function TwA defined by
</p>
<p>TwA(n) = sup
x&isin;Jn
</p>
<p>C(x).
</p>
<p>If there is a probability structure on Jn it is possible to define the average-case cost
function T aA given by
</p>
<p>T aA(n) = En(C(x)).
</p>
<p>where En is the expectation over Jn. However, the average is usually more difficult
to find, and there is of course the issue of what probabilities to assign.
</p>
<p>We now discuss how the objects in the three items above are selected. The selec-
</p>
<p>tion of a set of basic operations is generally easy. For the algorithms we consider
</p>
<p>in this chapter, the obvious choice is the set {+, &minus;, &times;, /, &le;} of the four arithmetic
operations and the comparison. Selecting a notion of input size and a cost for the
</p>
<p>basic operations depends on the kind of data dealt with by the algorithm. Some
</p>
<p>kinds can be represented within a fixed amount of computer memory; others require
</p>
<p>a variable amount.
</p>
<p>Examples of the first are fixed-precision floating-point numbers, stored in a fixed
</p>
<p>amount of memory (usually 32 or 64 bits). For this kind of data the size of an
</p>
<p>element is usually taken to be 1 and consequently to have unit size per number.
</p>
<p>Examples of the second are integer numbers which require a number of bits
</p>
<p>approximately equal to the logarithm of their absolute value. This (base 2) logarithm
</p>
<p>is usually referred to as the bit size of the integer. Similar ideas apply for rational
</p>
<p>numbers.</p>
<p/>
</div>
<div class="page"><p/>
<p>118 5 Interior-Point Methods
</p>
<p>Let A be some kind of data and x = (x1, . . . , xn) &isin; An. If A is of the first kind
above then we define size(x) = n. Otherwise, we define size(x) =
</p>
<p>&sum;n
i=1 bit-size (xi).
</p>
<p>The cost of operating on two unit-size numbers is taken to be 1 and is called the
</p>
<p>unit cost. In the bit-size case, the cost of operating on two numbers is the product of
</p>
<p>their bit-sizes (for multiplications and divisions) or their maximum (for additions,
</p>
<p>subtractions, and comparisons).
</p>
<p>The consideration of integer or rational data with their associated bit size and
</p>
<p>bit cost for the arithmetic operations is usually referred to as the Turing model of
</p>
<p>computation. The consideration of idealized reals with unit size and unit cost is
</p>
<p>today referred as the real number arithmetic model. When comparing algorithms,
</p>
<p>one should make clear which model of computation is used to derive complexity
</p>
<p>bounds.
</p>
<p>A basic concept related to both models of computation is that of polynomial
</p>
<p>time. An algorithm A is said to be a polynomial time algorithm if TwA(n) is bounded
above by a polynomial. A problem can be solved in polynomial time if there is a
</p>
<p>polynomial time algorithm solving the problem. The notion of average polynomial
</p>
<p>time is defined similarly, replacing TwA by T
a
A.
</p>
<p>The notion of polynomial time is usually taken as the formalization of efficiency
</p>
<p>in complexity theory.
</p>
<p>*5.2 &lowast;The Simplex Method Is Not Polynomial-Time
</p>
<p>When the simplex method is used to solve a linear program in standard form with
</p>
<p>coefficient matrix A &isin; Em&times;n, b &isin; Em and c &isin; En, the number of pivot steps to solve
the problem starting from a basic feasible solution is typically a small multiple of
</p>
<p>m: usually between 2m and 3m. In fact, Dantzig observed that for problems with
</p>
<p>m &le; 50 and n &le; 200 the number of iterations is ordinarily less than 1.5m.
At one time researchers believed&mdash;and attempted to prove&mdash;that the simplex
</p>
<p>algorithm (or some variant thereof) always requires a number of iterations that is
</p>
<p>bounded by a polynomial expression in the problem size. That was until Victor Klee
</p>
<p>and George Minty exhibited a class of linear programs each of which requires an
</p>
<p>exponential number of iterations when solved by the conventional simplex method.
</p>
<p>One form of the Klee&ndash;Minty example is
</p>
<p>maximize
</p>
<p>n
&sum;
</p>
<p>j=1
</p>
<p>10n&minus; jx j
</p>
<p>subject to 2
</p>
<p>i&minus;1
&sum;
</p>
<p>j=1
</p>
<p>10i&minus; jx j + xi &le; 100i&minus;1i = 1, . . . , n (5.1)
</p>
<p>x j &ge; 0 j = 1, . . . , n
</p>
<p>The problem above is easily cast as a linear program in standard form.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.3 &lowast;The Ellipsoid Method 119
</p>
<p>A specific case is that for n = 3, giving
</p>
<p>maximize 100x1 + 10x2 + x3
</p>
<p>subject to x1 &le; 1
20x1 + x2 &le; 100
200x1 + 20x2 + x3 &le; 10, 000
x1 � 0, x2 � 0, x3 � 0.
</p>
<p>In this case, we have three constraints and three variables (along with their non-
</p>
<p>negativity constraints). After adding slack variables, the problem is in standard form.
</p>
<p>The system has m = 3 equations and n = 6 nonnegative variables. It can be verified
</p>
<p>that it takes 23 &minus; 1 = 7 pivot steps to solve the problem with the simplex method
when at each step the pivot column is chosen to be the one with the largest (because
</p>
<p>this a maximization problem) reduced cost. (See Exercise 1.)
</p>
<p>The general problem of the class (1) takes 2n &minus; 1 pivot steps and this is in fact
the number of vertices minus one (which is the starting vertex). To get an idea of
</p>
<p>how bad this can be, consider the case where n = 50. We have 250 &minus; 1 &asymp; 1015. In
a year with 365 days, there are approximately 3 &times; 107 s. If a computer ran contin-
uously, performing a million pivots of the simplex algorithm per second, it would
</p>
<p>take approximately
1015
</p>
<p>3 &times; 107 &times; 106 &asymp; 33 years
</p>
<p>to solve a problem of this class using the greedy pivot selection rule.
</p>
<p>Although it is not polynomial in the worst case, the simplex method remains
</p>
<p>one of major solvers for linear programming. In fact, the method has been recently
</p>
<p>proved to be (strongly) polynomial for solving the Markov Decision Process with
</p>
<p>any fixed discount rate.
</p>
<p>*5.3 &lowast;The Ellipsoid Method
</p>
<p>The basic ideas of the ellipsoid method stem from research done in the 1960s and
</p>
<p>1970s mainly in the Soviet Union (as it was then called) by others who preceded
</p>
<p>Khachiyan. In essence, the idea is to enclose the region of interest in ever smaller
</p>
<p>ellipsoids.
</p>
<p>The significant contribution of Khachiyan was to demonstrate in that under cer-
</p>
<p>tain assumptions, the ellipsoid method constitutes a polynomially bounded algorithm
</p>
<p>for linear programming.
</p>
<p>The version of the method discussed here is really aimed at finding a point of a
</p>
<p>polyhedral set Ω given by a system of linear inequalities.
</p>
<p>Ω = {y &isin; Em : yTa j &le; c j, j = 1, . . . n}</p>
<p/>
</div>
<div class="page"><p/>
<p>120 5 Interior-Point Methods
</p>
<p>Finding a point of Ω can be thought of as equivalent to solving a linear programming
</p>
<p>problem.
</p>
<p>Two important assumptions are made regarding this problem:
</p>
<p>(A1) There is a vector y0 &isin; Em and a scalar R &gt; 0 such that the closed ball S (y0, R)
with center y0 and radius R, that is
</p>
<p>{y &isin; Em : |y &minus; y0| &le; R},
</p>
<p>contains Ω.
</p>
<p>(A2) If Ω is nonempty, there is a known scalar r &gt; 0 such that Ω contains a ball
</p>
<p>of the form S (y&lowast;, r) with center at y&lowast; and radius r. (This assumption implies
that if Ω is nonempty, then it has a nonempty interior and its volume is at least
</p>
<p>vol(S (0, r)).)2
</p>
<p>Definition. An ellipsoid in Em is a set of the form
</p>
<p>E = {y &isin; Em : (y &minus; z)TQ(y &minus; z) &le; 1}
</p>
<p>where z &isin; Em is a given point (called the center) and Q is a positive definite matrix (see
Sect. A.4 of Appendix A) of dimension m &times; m. This ellipsoid is denoted E(z, Q).
</p>
<p>The unit sphere S (0, 1) centered at the origin 0 is a special ellipsoid with Q = I, the
</p>
<p>identity matrix.
</p>
<p>The axes of a general ellipsoid are the eigenvectors of Q and the lengths of the
</p>
<p>axes are λ&minus;1/2
1
</p>
<p>, λ
&minus;1/2
2
</p>
<p>, . . . , λ
&minus;1/2
m , where the λi&rsquo;s are the corresponding eigenvalues.
</p>
<p>It can be shown that the volume of an ellipsoid is
</p>
<p>vol(E) = vol(S (0, 1))Πmi=1λ
&minus;1/2
i
</p>
<p>= vol(S (0, 1)) det(Q&minus;1/2).
</p>
<p>Cutting Plane and New Containing Ellipsoid
</p>
<p>In the ellipsoid method, a series of ellipsoids Ek is defined, with centers yk and with
</p>
<p>the defining Q = B&minus;1
k
</p>
<p>, where Bk is symmetric and positive definite.
</p>
<p>At each iteration of the algorithm, we have Ω &sub; Ek. It is then possible to check
whether yk &isin; Ω. If so, we have found an element of Ω as required. If not, there is at
least one constraint that is violated. Suppose aT
</p>
<p>j
yk &gt; c j. Then
</p>
<p>Ω &sub; 1
2
Ek = {y &isin; Ek : aTj y &le; aTj yk}
</p>
<p>This set is half of the ellipsoid, obtained by cutting the ellipsoid in half through its
</p>
<p>center (Fig. 5.1).
</p>
<p>2 The (topological) interior of any set Ω is the set of points in Ω which are the centers of some balls
contained in Ω.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.3 &lowast;The Ellipsoid Method 121
</p>
<p>The successor ellipsoid Ek+1 is defined to be the minimal-volume ellipsoid
</p>
<p>containing (1/2)Ek. It is constructed as follows. Define
</p>
<p>τ =
1
</p>
<p>m + 1
, δ =
</p>
<p>m2
</p>
<p>m2 &minus; 1 , σ = 2τ.
</p>
<p>Fig. 5.1 A half-ellipsoid
</p>
<p>Then put
</p>
<p>yk+1 = yk &minus;
τ
</p>
<p>(aT
j
Bka j)1/2
</p>
<p>Bka j
</p>
<p>Bk+1 = δ
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>Bk &minus; σ
Bka ja
</p>
<p>T
j
Bk
</p>
<p>aT
j
Bka j
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>(5.2)
</p>
<p>Theorem 1. The ellipsoid Ek+1 = E(yk+1, B
&minus;1
k+1
</p>
<p>) defined as above is the ellipsoid of least
volume containing (1/2)Ek . Moreover,
</p>
<p>vol(Ek+1)
</p>
<p>vol(Ek)
=
</p>
<p>(
</p>
<p>m2
</p>
<p>m2 &minus; 1
</p>
<p>)(m&minus;1)/2
m
</p>
<p>m + 1
&lt; exp
</p>
<p>(
</p>
<p>&minus; 1
2(m + 1)
</p>
<p>)
</p>
<p>&lt; 1.
</p>
<p>Proof. We shall not prove the statement about the new ellipsoid being of least
</p>
<p>volume, since that is not necessary for the results that follow. To prove the remainder
</p>
<p>of the statement, we have
</p>
<p>vol(Ek+1)
</p>
<p>vol(Ek)
=
</p>
<p>det(B1/2
k+1
</p>
<p>)
</p>
<p>det(B1/2
k
</p>
<p>)
</p>
<p>For simplicity, by a change of coordinates, we may take Bk = I. Then Bk+1 has m&minus;1
eigenvalues equal to δ = m
</p>
<p>2
</p>
<p>m2&minus;1 and one eigenvalue equal to δ&minus;2δτ =
m2
</p>
<p>m2&minus;1 (1&minus;
2
</p>
<p>m+1
) =
</p>
<p>( m
m+1
</p>
<p>)2. The reduction in volume is the product of the square roots of these, giving
</p>
<p>the equality in the theorem.</p>
<p/>
</div>
<div class="page"><p/>
<p>122 5 Interior-Point Methods
</p>
<p>Then using (1 + x)p � exp, we have
</p>
<p>(
</p>
<p>m2
</p>
<p>m2 &minus; 1
</p>
<p>)(m&minus;1)/2
m
</p>
<p>m + 1
=
</p>
<p>(
</p>
<p>1 +
1
</p>
<p>m2 &minus; 1
</p>
<p>)(m&minus;1)/2 (
</p>
<p>1 &minus; 1
m + 1
</p>
<p>)
</p>
<p>&lt; exp
</p>
<p>(
</p>
<p>1
</p>
<p>2(m + 1)
&minus; 1
</p>
<p>(m + 1)
</p>
<p>)
</p>
<p>= exp
</p>
<p>(
</p>
<p>&minus; 1
2(m + 1)
</p>
<p>)
</p>
<p>. �
</p>
<p>Convergence
</p>
<p>The ellipsoid method is initiated by selecting y0 and R such that condition (A1) is
</p>
<p>satisfied. Then B0 = R
2I, and the corresponding E0 contains Ω. The updating of the
</p>
<p>Ek&rsquo;s is continued until a solution is found.
</p>
<p>Under the assumptions stated above, a single repetition of the ellipsoid method
</p>
<p>reduces the volume of an ellipsoid to one-half of its initial value in O(m) iterations.
</p>
<p>(See Appendix A for O notation.) Hence it can reduce the volume to less than that
</p>
<p>of a sphere of radius r in O(m2 log(R/r)) iterations, since its volume is bounded
</p>
<p>from below by vol(S (0, 1))rm and the initial volume is vol(S (0, 1))Rm. Generally
</p>
<p>a single iteration requires O(m2) arithmetic operations. Hence the entire process
</p>
<p>requires O(m4 log(R/r)) arithmetic operations.3
</p>
<p>Ellipsoid Method for Usual Form of LP
</p>
<p>Now consider the linear program (where A is m &times; n)
</p>
<p>(P)
maximize cTx
</p>
<p>subject to Ax &le; b, x &ge; 0
and its dual
</p>
<p>(D)
minimize yTb
</p>
<p>subject to yTA &ge; cT , y &ge; 0.
Note that both problems can be solved by finding a feasible point to inequalities
</p>
<p>&minus; cTx + bTy &le; 0
Ax &le; b
</p>
<p>&minus;ATy &le; &minus;c (5.3)
x, y &ge; 0,
</p>
<p>where both x and y are variables. Thus, the total number of arithmetic operations
</p>
<p>for solving a linear program is bounded by O((m + n)4 log(R/r)).
</p>
<p>3 Assumption (A2) is sometimes too strong. It has been shown, however, that when the data consists
of integers, it is possible to perturb the problem so that (A2) is satisfied and if the perturbed problem
has a feasible solution, so does the original Ω.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.4 The Analytic Center 123
</p>
<p>5.4 The Analytic Center
</p>
<p>The new interior-point algorithms introduced by Karmarkar move by successive
</p>
<p>steps inside the feasible region. It is the interior of the feasible set rather than the ver-
</p>
<p>tices and edges that plays a dominant role in this type of algorithm. In fact, these
</p>
<p>algorithms purposely avoid the edges of the set, only eventually converging to one
</p>
<p>as a solution.
</p>
<p>Our study of these algorithms begins in the next section, but it is useful at this
</p>
<p>point to introduce a concept that definitely focuses on the interior of a set, termed
</p>
<p>the set&rsquo;s analytic center. As the name implies, the center is away from the edge.
</p>
<p>In addition, the study of the analytic center introduces a special structure, termed
</p>
<p>a barrier or potential that is fundamental to interior-point methods.
</p>
<p>Consider a set S in a subset of X of En defined by a group of inequalities as
</p>
<p>S = {x &isin; X : g j(x) � 0, j = 1, 2, . . . , m},
</p>
<p>and assume that the functions g j are continuous. S has a nonempty interior S̊ =
{x &isin; X : g j(x) &gt; 0, all j}. Associated with this definition of the set is the potential
function
</p>
<p>ψ(x) = &minus;
m
&sum;
</p>
<p>j=1
</p>
<p>log g j(x)
</p>
<p>defined on S̊.
The analytic center of S is the vector (or set of vectors) that minimizes the
</p>
<p>potential; that is, the vector (or vectors) that solve
</p>
<p>minψ(x) = min
</p>
<p>⎧
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>&minus;
m
&sum;
</p>
<p>j=1
</p>
<p>log g j(x) : x &isin; X, g j(x) &gt; 0 for each j
</p>
<p>⎫
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎬
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎭
</p>
<p>.
</p>
<p>Example 1 (A Cube). Consider the set S defined by xi � 0, (1 &minus; xi) � 0, for i =
1, 2, . . . , n. This is S = [0, 1]n, the unit cube in En. The analytic center can be found
by differentiation to be xi = 1/2, for all i. Hence, the analytic center is identical to
</p>
<p>what one would normally call the center of the unit cube.
</p>
<p>In general, the analytic center depends on how the set is defined&mdash;on the partic-
</p>
<p>ular inequalities used in the definition. For instance, the unit cube is also defined by
</p>
<p>the inequalities xi � 0, (1 &minus; xi)d � 0 with odd d &gt; 1. In this case the solution is
xi = 1/(d+1) for all i. For large d this point is near the inner corner of the unit cube.
</p>
<p>Also, the addition of redundant inequalities can change the location of the
</p>
<p>analytic center. For example, repeating a given inequality will change the center&rsquo;s
</p>
<p>location.
</p>
<p>There are several sets associated with linear programs for which the analytic
</p>
<p>center is of particular interest. One such set is the feasible region itself. Another is
</p>
<p>the set of optimal solutions. There are also sets associated with dual and primal-dual
</p>
<p>formulations. All of these are related in important ways.</p>
<p/>
</div>
<div class="page"><p/>
<p>124 5 Interior-Point Methods
</p>
<p>Let us illustrate by considering the analytic center associated with a bounded
</p>
<p>polytope Ω in Em represented by n(&gt; m) linear inequalities; that is,
</p>
<p>Ω = {y &isin; Em : cT &minus; yTA � 0},
</p>
<p>where A &isin; Em&times;n and c &isin; En are given and A has rank m. Denote the interior of Ω by
</p>
<p>Ω̊ = {y &isin; Em : cT &minus; yTA &gt; 0}.
</p>
<p>The potential function for this set is
</p>
<p>ψΩ(y) &equiv; &minus;
n
&sum;
</p>
<p>j=1
</p>
<p>log(c j &minus; yTa j) = &minus;
n
&sum;
</p>
<p>j=1
</p>
<p>log s j, (5.4)
</p>
<p>where s &equiv; c &minus; ATy is a slack vector. Hence the potential function is the negative
sum of the logarithms of the slack variables.
</p>
<p>The analytic center of Ω is the interior point of Ω that minimizes the poten-
</p>
<p>tial function. This point is denoted by ya and has the associated sa = c &minus; ATya.
The pair (ya, sa) is uniquely defined, since the potential function is strictly convex
</p>
<p>(see Sect. 7.4) in the bounded convex set Ω.
</p>
<p>Setting to zero the derivatives of ψ(y) with respect to each yi gives
</p>
<p>n
&sum;
</p>
<p>j=1
</p>
<p>ai j
</p>
<p>c j &minus; yTa j
= 0, for all i.
</p>
<p>which can be written
n
&sum;
</p>
<p>j=1
</p>
<p>ai j
</p>
<p>s j
= 0, for all i.
</p>
<p>Now define x j = 1/s j for each j. We introduce the notation
</p>
<p>x ◦ s &equiv; (x1s1, x2s2, . . . , xnsn)T ,
</p>
<p>which is component multiplication. Then the analytic center is defined by the
</p>
<p>conditions
</p>
<p>x ◦ s = 1
Ax = 0
</p>
<p>ATy + s = c.
</p>
<p>The analytic center can be defined when the interior is empty or equalities are
</p>
<p>present, such as
</p>
<p>Ω = {y &isin; Em : cT &minus; yTA � 0, By = b}.
In this case the analytic center is chosen on the linear surface {y : By = b} to
maximize the product of the slack variables s = c &minus; ATy. Thus, in this context</p>
<p/>
</div>
<div class="page"><p/>
<p>5.5 The Central Path 125
</p>
<p>the interior of Ω refers to the interior of the positive orthant of slack variables:
</p>
<p>Rn+ &equiv; {s : s � 0}. This definition of interior depends only on the region of the slack
variables. Even if there is only a single point in Ω with s = c &minus; ATy for some y
where By = b with s &gt; 0, we still say that Ω̊ is not empty.
</p>
<p>5.5 The Central Path
</p>
<p>The concept underlying interior-point methods for linear programming is to use
</p>
<p>nonlinear programming techniques of analysis and methodology. The analysis is
</p>
<p>often based on differentiation of the functions defining the problem. Traditional
</p>
<p>linear programming does not require these techniques since the defining functions
</p>
<p>are linear. Duality in general nonlinear programs is typically manifested through
</p>
<p>Lagrange multipliers (which are called dual variables in linear programming). The
</p>
<p>analysis and algorithms of the remaining sections of the chapter use these nonlin-
</p>
<p>ear techniques. These techniques are discussed systematically in later chapters, so
</p>
<p>rather than treat them in detail at this point, these current sections provide only
</p>
<p>minimal detail in their application to linear programming. It is expected that most
</p>
<p>readers are already familiar with the basic method for minimizing a function by set-
</p>
<p>ting its derivative to zero, and for incorporating constraints by introducing Lagrange
</p>
<p>multipliers. These methods are discussed in detail in Chaps. 11&ndash;15.
</p>
<p>The computational algorithms of nonlinear programming are typically iterative
</p>
<p>in nature, often characterized as search algorithms. At any step with a given point,
</p>
<p>a direction for search is established and then a move in that direction is made to
</p>
<p>define the next point. There are many varieties of such search algorithms and they
</p>
<p>are systematically presented throughout the text. In this chapter, we use versions of
</p>
<p>Newton&rsquo;s method as the search algorithm, but we postpone a detailed study of the
</p>
<p>method until later chapters.
</p>
<p>Not only have nonlinear methods improved linear programming, but interior-
</p>
<p>point methods for linear programming have been extended to provide new ap-
</p>
<p>proaches to nonlinear programming. This chapter is intended to show how this
</p>
<p>merger of linear and nonlinear programming produces elegant and effective methods.
</p>
<p>These ideas take an especially pleasing form when applied to linear programming.
</p>
<p>Study of them here, even without all the detailed analysis, should provide good
</p>
<p>intuitive background for the more general manifestations.
</p>
<p>Consider a primal linear program in standard form
</p>
<p>(LP) minimize cTx (5.5)
</p>
<p>subject to Ax = b, x � 0.
</p>
<p>We denote the feasible region of this program by Fp. We assume that F̊p = {x :
Ax = b, x &gt; 0} is nonempty and the optimal solution set of the problem is bounded.</p>
<p/>
</div>
<div class="page"><p/>
<p>126 5 Interior-Point Methods
</p>
<p>Associated with this problem, we define for μ � 0 the barrier problem
</p>
<p>(BP) minimize cTx &minus; μ
n
&sum;
</p>
<p>j=1
</p>
<p>log x j (5.6)
</p>
<p>subject to Ax = b, x &gt; 0.
</p>
<p>It is clear that μ = 0 corresponds to the original problem (5.5). As μ &rarr; &infin;, the
solution approaches the analytic center of the feasible region (when it is bounded),
</p>
<p>since the barrier term swamps out cTx in the objective. As μ is varied continuously
</p>
<p>toward 0, there is a path x(μ) defined by the solution to (BP). This path x(μ) is
</p>
<p>termed the primal central path. As μ &rarr; 0 this path converges to the analytic center
of the optimal face {x : cTx = z&lowast;, Ax = b, x � 0}, where z&lowast; is the optimal value of
(LP).
</p>
<p>A strategy for solving (LP) is to solve (BP) for smaller and smaller values of μ
</p>
<p>and thereby approach a solution to (LP). This is indeed the basic idea of interior-
</p>
<p>point methods.
</p>
<p>At any μ &gt; 0, under the assumptions that we have made for problem (5.5), the
</p>
<p>necessary and sufficient conditions for a unique and bounded solution are obtained
</p>
<p>by introducing a Lagrange multiplier vector y for the linear equality constraints to
</p>
<p>form the Lagrangian (see Chap. 11)
</p>
<p>cTx &minus; μ
n
&sum;
</p>
<p>j=1
</p>
<p>log x j &minus; yT (Ax &minus; b).
</p>
<p>The derivatives with respect to the x j&rsquo;s are set to zero, leading to the conditions
</p>
<p>c j &minus; μ/x j &minus; yTa j = 0, for each j
</p>
<p>or equivalently
</p>
<p>μX&minus;11 + ATy = c (5.7)
</p>
<p>where as before a j is the jth column of A, 1 is the vector of 1&rsquo;s, and X is the diagonal
</p>
<p>matrix whose diagonal entries are the components of x &gt; 0. Setting s j = μ/x j the
</p>
<p>complete set of conditions can be rewritten
</p>
<p>x ◦ s = μ1
Ax = b (5.8)
</p>
<p>ATy + s = c.
</p>
<p>Note that y is a dual feasible solution and c &minus; ATy &gt; 0 (see Exercise 4).
</p>
<p>Example 2 (A Square Primal). Consider the problem of maximizing x1 within the
</p>
<p>unit square S = [0, 1]2. The problem is formulated as</p>
<p/>
</div>
<div class="page"><p/>
<p>5.5 The Central Path 127
</p>
<p>min &minus;x1
s.t. x1 + x3 = 1
</p>
<p>x2 + x4 = 1
</p>
<p>x1 � 0, x2 � 0, x3 � 0, x4 � 0.
</p>
<p>Here x3 and x4 are slack variables for the original problem to put it in standard
</p>
<p>form. The optimality conditions for x(μ) consist of the original two linear constraint
</p>
<p>equations and the four equations
</p>
<p>y1 + s1 = &minus;1, y2 + s2 = 0, y1 + s3 = 0, y2 + s4 = 0
</p>
<p>together with the relations si = μ/xi for i = 1, 2 . . . , 4. These equations are readily
</p>
<p>solved with a series of elementary variable eliminations to find
</p>
<p>x1(μ) =
1 &minus; 2μ &plusmn;
</p>
<p>&radic;
</p>
<p>1 + 4μ2
</p>
<p>2
x2(μ) = 1/2.
</p>
<p>Using the &ldquo;+&rdquo; solution, it is seen that as μ &rarr; 0 the solution goes to x &rarr; (1, 1/2).
Note that this solution is not a corner of the cube. Instead it is at the analytic center
</p>
<p>of the optimal face {x : x1 = 1, 0 � x2 � 1}. See Fig. 5.2. The limit of x(μ) as
μ &rarr; &infin; can be seen to be the point (1/2, 1/2). Hence, the central path in this case is
a straight line progressing from the analytic center of the square (at μ &rarr; &infin;) to the
analytic center of the optimal face (at μ &rarr; 0).
</p>
<p>Dual Central Path
</p>
<p>Now consider the dual problem
</p>
<p>(LD) maximize yTb
</p>
<p>subject to yTA + sT = cT , s � 0.
</p>
<p>We may apply the barrier approach to this problem by formulating the problem
</p>
<p>(BD) maximize yTb + μ
</p>
<p>n
&sum;
</p>
<p>j=1
</p>
<p>log s j
</p>
<p>subject to yTA + sT = cT , s &gt; 0.
</p>
<p>We assume that the dual feasible set Fd has an interior F̊d = {(y, s) : yTA + sT =
cT , s &gt; 0} is nonempty and the optimal solution set of (LD) is bounded. Then, as μ
is varied continuously toward 0, there is a path (y(μ), s(μ)) defined by the solution
</p>
<p>to (BD). This path is termed the dual central path.</p>
<p/>
</div>
<div class="page"><p/>
<p>128 5 Interior-Point Methods
</p>
<p>Fig. 5.2 The analytic path for the square
</p>
<p>To work out the necessary and sufficient conditions we introduce x as a Lagrange
</p>
<p>multiplier and form the Lagrangian
</p>
<p>yTb + μ
</p>
<p>n
&sum;
</p>
<p>j=1
</p>
<p>log s j &minus; (yTA + sT &minus; cT )x.
</p>
<p>Setting to zero the derivative with respect to yi leads to
</p>
<p>bi &minus; aix = 0, for all i
</p>
<p>where ai is the ith row of A. Setting to zero the derivative with respect to s j leads to
</p>
<p>μ/s j &minus; x j = 0, or 1 &minus; x js j = 0, for all j.
</p>
<p>Combining these equations and including the original constraint yields the complete
</p>
<p>set of conditions which are identical to the optimality conditions for the primal
</p>
<p>central path (5.8). Note that x is indeed a primal feasible solution and x &gt; 0.
</p>
<p>To see the geometric representation of the dual central path, consider the dual
</p>
<p>level set
</p>
<p>Ω(z) = {y : cT &minus; yTA � 0, yTb � z}
for any z &lt; z&lowast; where z&lowast; is the optimal value of (LD). Then, the analytic center
(y(z), s(z)) of Ω(z) coincides with the dual central path as z tends to the optimal
</p>
<p>value z&lowast; from below. This is illustrated in Fig. 5.3, where the feasible region of the
dual set (not the primal) is shown. The level sets Ω(z) are shown for various values
</p>
<p>of z. The analytic centers of these level sets correspond to the dual central path.
</p>
<p>Example 3 (The Square Dual). Consider the dual of Example 2. This is
</p>
<p>max y1 + y2
</p>
<p>subject to y1 � &minus;1
y2 � 0.
</p>
<p>(The values of s1 and s2 are the slack variables of the inequalities.) The solution
</p>
<p>to the dual barrier problem is easily found from the solution of the primal barrier</p>
<p/>
</div>
<div class="page"><p/>
<p>5.5 The Central Path 129
</p>
<p>problem to be
</p>
<p>y1(μ) = &minus;1 &minus; μ/x1(μ), y2 = &minus;2μ.
</p>
<p>Fig. 5.3 The central path as analytic centers in the dual feasible region
</p>
<p>As μ &rarr; 0, we have y1 &rarr; &minus;1, y2 &rarr; 0, which is the unique solution to the dual LP.
However, as μ &rarr; &infin;, the vector y is unbounded, for in this case the dual feasible set
is itself unbounded.
</p>
<p>Primal&ndash;Dual Central Path
</p>
<p>Suppose the feasible region of the primal (LP) has interior points and its optimal
</p>
<p>solution set is bounded. Then, the dual also has interior points (see Exercise 4). The
</p>
<p>primal&ndash;dual path is defined to be the set of vectors (x(μ) &gt; 0, y(μ), s(μ) &gt; 0) that
</p>
<p>satisfy the conditions
</p>
<p>x ◦ s = μ1
Ax = b
</p>
<p>ATy + s = c
</p>
<p>for 0 � μ � &infin;. Hence the central path is defined without explicit reference to
an optimization problem. It is simply defined in terms of the set of equality and
</p>
<p>inequality conditions.
</p>
<p>Since conditions (5.8) and (5.9) are identical, the primal&ndash;dual central path can be
</p>
<p>split into two components by projecting onto the relevant space, as described in the
</p>
<p>following proposition.
</p>
<p>Proposition 1. Suppose the feasible sets of the primal and dual programs contain interior
</p>
<p>points. Then the primal&ndash;dual central path (x(μ), y(μ), s(μ)) exists for all μ, 0 � μ &lt; &infin;.
Furthermore, x(μ) is the primal central path, and (y(μ), s(μ)) is the dual central path. More-</p>
<p/>
</div>
<div class="page"><p/>
<p>130 5 Interior-Point Methods
</p>
<p>over, x(μ) and (y(μ), s(μ)) converge to the analytic centers of the optimal primal solution
and dual solution faces, respectively, as μ &rarr; 0.
</p>
<p>Duality Gap
</p>
<p>Let (x(μ), y(μ), s(μ)) be on the primal-dual central path. Then from (5.9) it follows
</p>
<p>that
</p>
<p>cTx &minus; yTb = yTAx + sTx &minus; yTb = sTx = nμ.
The value cTx &minus; yTb = sTx is the difference between the primal objective value
and the dual objective value. This value is always nonnegative (see the weak duality
</p>
<p>lemma in Sect. 4.2) and is termed the duality gap. At any point on the primal&ndash;dual
</p>
<p>central path, the duality gap is equal to nμ. It is clear that as μ &rarr; 0 the duality
gap goes to zero, and hence both x(μ) and (y(μ), s(μ)) approach optimality for the
</p>
<p>primal and dual, respectively.
</p>
<p>The duality gap provides a measure of closeness to optimality. For any primal
</p>
<p>feasible x, the value cTx gives an upper bound as cTx � z&lowast; where z&lowast; is the optimal
value of the primal. Likewise, for any dual feasible pair (y, s), the value yTb gives
</p>
<p>a lower bound as yTb � z&lowast;. The difference, the duality gap g = cTx &minus; yTb, provides
a bound on z&lowast; as z&lowast; � cTx &minus; g. Hence if at a feasible point x, a dual feasible (y, s)
is available, the quality of x can be measured as cTx &minus; z&lowast; � g.
</p>
<p>5.6 Solution Strategies
</p>
<p>The various definitions of the central path directly suggest corresponding strate-
</p>
<p>gies for solution of a linear program. We outline three general approaches here:
</p>
<p>the primal barrier or path-following method, the primal-dual path-following method
</p>
<p>and the primal-dual potential-reduction method, although the details of their im-
</p>
<p>plementation and analysis must be deferred to later chapters after study of general
</p>
<p>nonlinear methods. Table 5.1 depicts these solution strategies and the simplex meth-
</p>
<p>ods described in Chaps. 3 and 4 with respect to how they meet the three optimality
</p>
<p>conditions: Primal Feasibility, Dual Feasibility, and Zero-Duality during the itera-
</p>
<p>tive process.
</p>
<p>Table 5.1 Properties of algorithms
</p>
<p>P-F D-F 0-Duality
</p>
<p>Primal simplex
&radic; &radic;
</p>
<p>Dual simplex
&radic; &radic;
</p>
<p>Primal barrier
&radic;
</p>
<p>Primal-dual path-following
&radic; &radic;
</p>
<p>Primal-dual potential-reduction
&radic; &radic;</p>
<p/>
</div>
<div class="page"><p/>
<p>5.6 Solution Strategies 131
</p>
<p>For example, the primal simplex method keeps improving a primal feasible so-
</p>
<p>lution, maintains the zero-duality gap (complementarity slackness condition) and
</p>
<p>moves toward dual feasibility; while the dual simplex method keeps improving a
</p>
<p>dual feasible solution, maintains the zero-duality gap (complementarity condition)
</p>
<p>and moves toward primal feasibility (see Sect. 4.3). The primal barrier method keeps
</p>
<p>improving a primal feasible solution and moves toward dual feasibility and comple-
</p>
<p>mentarity; and the primal-dual interior-point methods keep improving a primal and
</p>
<p>dual feasible solution pair and move toward complementarity.
</p>
<p>Primal Barrier Method
</p>
<p>A direct approach is to use the barrier construction and solve the the problem
</p>
<p>minimize cTx &minus; μ
&sum;n
</p>
<p>j=1
log x j (5.9)
</p>
<p>subject to Ax = b, x &gt; 0,
</p>
<p>for a very small value of μ. In fact, if we desire to reduce the duality gap to ε it is
</p>
<p>only necessary to solve the problem for μ = ε/n. Unfortunately, when μ is small,
</p>
<p>the problem (5.9) could be highly ill-conditioned in the sense that the necessary
</p>
<p>conditions are nearly singular. This makes it difficult to directly solve the problem
</p>
<p>for small μ.
</p>
<p>An overall strategy, therefore, is to start with a moderately large μ (say μ =
</p>
<p>100) and solve that problem approximately. The corresponding solution is a point
</p>
<p>approximately on the primal central path, but it is likely to be quite distant from the
</p>
<p>point corresponding to the limit of μ &rarr; 0. However this solution point at μ = 100
can be used as the starting point for the problem with a slightly smaller μ, for this
</p>
<p>point is likely to be close to the solution of the new problem. The value of μ might
</p>
<p>be reduced at each stage by a specific factor, giving μk+1 = γμk, where γ is a fixed
</p>
<p>positive parameter less than one and k is the stage count.
</p>
<p>If the strategy is begun with a value μ0, then at the kth stage we have μk = γ
kμ0.
</p>
<p>Hence to reduce μk/μ0 to below ε, requires
</p>
<p>k =
log ε
</p>
<p>log γ
</p>
<p>stages.
</p>
<p>Often a version of Newton&rsquo;s method for minimization is used to solve each of the
</p>
<p>problems. For the current strategy, Newton&rsquo;s method works on problem (5.9) with
</p>
<p>fixed μ by considering the central path equations (5.8)
</p>
<p>x ◦ s = μ1
Ax = b (5.10)
</p>
<p>ATy + s = c.</p>
<p/>
</div>
<div class="page"><p/>
<p>132 5 Interior-Point Methods
</p>
<p>From a given point x &isin; F̊p, Newton&rsquo;s method moves to a closer point x+ &isin; F̊p
by moving in the directions dx, dy and ds determined from the linearized version
</p>
<p>of (5.10)
</p>
<p>μX&minus;2dx + ds = μX
&minus;11 &minus; c,
</p>
<p>Adx = 0, (5.11)
</p>
<p>&minus;ATdy &minus; ds = 0.
</p>
<p>(Recall that X is the diagonal matrix whose diagonal entries are components of
</p>
<p>x &gt; 0.) The new point is then updated by taking a step in the direction of dx, as
</p>
<p>x+ = x + dx.
</p>
<p>Notice that if x ◦ s = μ1 for some s = c&minus;ATy, then d &equiv; (dx, dy, ds) = 0 because
the current point satisfies Ax = b and hence is already the central path solution
</p>
<p>for μ. If some component of x ◦ s is less than μ, then d will tend to increment the
solution so as to increase that component. The converse will occur for components
</p>
<p>of x ◦ s greater than μ.
This process may be repeated several times until a point close enough to the
</p>
<p>proper solution to the barrier problem for the given value of μ is obtained. That is,
</p>
<p>until the necessary and sufficient conditions (5.7) are (approximately) satisfied.
</p>
<p>There are several details involved in a complete implementation and analysis of
</p>
<p>Newton&rsquo;s method. These items are discussed in later chapters of the text. However,
</p>
<p>the method works well if either μ is moderately large, or if the algorithm is initi-
</p>
<p>ated at a point very close to the solution, exactly as needed for the barrier strategy
</p>
<p>discussed in this subsection.
</p>
<p>To solve (5.11), premultiplying both sides by X2 we have
</p>
<p>μdx + X
2ds = μX1 &minus; X2c.
</p>
<p>Then, premultiplying by A and using Adx = 0, we have
</p>
<p>AX2ds = μAX1 &minus; AX2c.
</p>
<p>Using ds = &minus;ATdy we have
</p>
<p>(AX2AT )dy = &minus;μAX1 + AX2c.
</p>
<p>Thus, dy can be computed by solving the above linear system of equations. Then ds
can be found from the third equation in (5.11) and finally dx can be found from the
</p>
<p>first equation in (5.11), together this amounts to O(nm2 + m3) arithmetic operations
</p>
<p>for each Newton step.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.6 Solution Strategies 133
</p>
<p>Primal-Dual Path-Following
</p>
<p>Another strategy for solving a linear program is to follow the central path from a
</p>
<p>given initial primal-dual solution pair. Consider a linear program in standard form
</p>
<p>Primal Dual
</p>
<p>minimize cTx maximize yTb
</p>
<p>subject to Ax = b, x � 0 subject to yTA � cT .
</p>
<p>Assume that the interior of both primal and dual feasible regions F̊ � &empty;; that is,
both4
</p>
<p>F̊p = {x : Ax = b, x &gt; 0} � &empty; and F̊d = {(y, s) : s = c &minus; ATy &gt; 0} � &empty;;
</p>
<p>and denote by z&lowast; the optimal objective value.
The central path can be expressed as
</p>
<p>C =
{
</p>
<p>(x, y, s) &isin; F̊ : x ◦ s = x
T s
</p>
<p>n
1
</p>
<p>}
</p>
<p>in the primal-dual form. On the path we have x ◦ s = μ1 and hence sTx = nμ.
A neighborhood of the central path C is of the form
</p>
<p>N(η) = {(x, y, s) &isin; F̊ : |s ◦ x &minus; μ1| &lt; ημ, where μ = sTx/n} (5.12)
</p>
<p>for some η &isin; (0, 1), say η = 1/4. This can be thought of as a tube whose center is
the central path.
</p>
<p>The idea of the path-following method is to move within a tubular neighborhood
</p>
<p>of the central path toward the solution point. A suitable initial point (x0, y0, s0) &isin;
N(η) can be found by solving the barrier problem for some fixed μ0 or from an ini-
tialization phase proposed later. After that, step by step moves are made, alternating
</p>
<p>between a predictor step and a corrector step. After each pair of steps, the point
</p>
<p>achieved is again in the fixed given neighborhood of the central path, but closer to
</p>
<p>the linear program&rsquo;s solution set.
</p>
<p>The predictor step is designed to move essentially parallel to the true central
</p>
<p>path. The step d &equiv; (dx, dy, ds) is determined from the linearized version of the
primal&ndash;dual central path equations of (5.9), as
</p>
<p>s ◦ dx + x ◦ ds = γμ1 &minus; x ◦ s,
Adx = 0, (5.13)
</p>
<p>&minus;ATdy &minus; ds = 0,
</p>
<p>where here one selects γ = 0. (To show the dependence of d on the current pair
</p>
<p>(x, s) and the parameter γ, we write d = d(x, s, γ).)
</p>
<p>4 The symbol &empty; denotes the empty set.</p>
<p/>
</div>
<div class="page"><p/>
<p>134 5 Interior-Point Methods
</p>
<p>The new point is then found by taking a step in the direction of d, as (x+, y+, s+) =
</p>
<p>(x, y, s) + α(dx, dy, ds), where α is the step-size. Note that d
T
x ds = &minus;dTx ATdy = 0
</p>
<p>here. Then
</p>
<p>(x+)T s+ = (x + αdx)
T (s + αds) = x
</p>
<p>T s + α(dTx s + x
Tds) = (1 &minus; α)xT s,
</p>
<p>where the last step follows by multiplying the first equation in (5.13) by 1T . Thus,
</p>
<p>the predictor step reduces the duality gap by a factor 1 &minus; α. The maximum possible
step-size α in that direction is made in that parallel direction without going outside
</p>
<p>of the neighborhood N(2η).
The corrector step essentially moves perpendicular to the central path in order to
</p>
<p>get closer to it. This step moves the solution back to within the neighborhood N(η),
and the step is determined by selecting γ = 1 in (5.13) with μ = xT s/n. Notice that
</p>
<p>if x ◦ s = μ1, then d = 0 because the current point is already a central path solution.
This corrector step is identical to one step of the barrier method. Note, however,
</p>
<p>that the predictor&ndash;corrector method requires only one sequence of steps, each con-
</p>
<p>sisting of a single predictor and corrector. This contrasts with the barrier method
</p>
<p>which requires a complete sequence for each μ to get back to the central path, and
</p>
<p>then an outer sequence to reduce the μ&rsquo;s.
</p>
<p>One can prove that for any (x, y, s) &isin; N(η) with μ = xT s/n, the step-size in the
predictor stop satisfies
</p>
<p>α �
1
</p>
<p>2
&radic;
n
.
</p>
<p>Thus, the iteration complexity of the method is O(
&radic;
n) log(1/ε)) to achieve μ/μ0 � ε
</p>
<p>where nμ0 is the initial duality gap. Moreover, one can prove that the step-size α &rarr; 1
as xT s &rarr; 0, that is, the duality reduction speed is accelerated as the gap becomes
smaller.
</p>
<p>Primal-Dual Potential Reduction Algorithm
</p>
<p>In this method a primal-dual potential function is used to measure the solution&rsquo;s
</p>
<p>progress. The potential is reduced at each iteration. There is no restriction on either
</p>
<p>neighborhood or step-size during the iterative process as long as the potential is
</p>
<p>reduced. The greater the reduction of the potential function, the faster the conver-
</p>
<p>gence of the algorithm. Thus, from a practical point of view, potential-reduction
</p>
<p>algorithms may have an advantage over path-following algorithms where iterates
</p>
<p>are confined to lie in certain neighborhoods of the central path.
</p>
<p>For x &isin; F̊p and (y, s) &isin; F̊d the primal-dual potential function is defined by
</p>
<p>ψn+ρ(x, s) &equiv; (n + ρ) log(xT s) &minus;
n
&sum;
</p>
<p>j=1
</p>
<p>log(x js j), (5.14)
</p>
<p>where ρ � 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.6 Solution Strategies 135
</p>
<p>From the arithmetic and geometric mean inequality (also see Exercise 10) we
</p>
<p>can derive that
</p>
<p>n log(xT s) &minus;
n
&sum;
</p>
<p>j=1
</p>
<p>log(x js j) � n log n.
</p>
<p>Then
</p>
<p>ψn+ρ(x, s) = ρ log(x
T s) + n log(xTs) &minus;
</p>
<p>n
&sum;
</p>
<p>j=1
</p>
<p>log(x js j) � ρ log(x
T s) + n log n. (5.15)
</p>
<p>Thus, for ρ &gt; 0, ψn+ρ(x, s) &rarr; &minus;&infin; implies that xT s &rarr; 0. More precisely, we have
from (5.15)
</p>
<p>xT s � exp
</p>
<p>(
</p>
<p>ψn+ρ(x, s) &minus; n log n
ρ
</p>
<p>)
</p>
<p>.
</p>
<p>Hence the primal&ndash;dual potential function gives an explicit bound on the magnitude
</p>
<p>of the duality gap.
</p>
<p>The objective of this method is to drive the potential function down toward minus
</p>
<p>infinity. The method of reduction is a version of Newton&rsquo;s method (5.13). In this
</p>
<p>case we select γ = n/(n + ρ) in (5.13). Notice that is a combination of a predictor
</p>
<p>and corrector choice. The predictor uses γ = 0 and the corrector uses γ = 1. The
</p>
<p>primal&ndash;dual potential method uses something in between. This seems logical, for
</p>
<p>the predictor moves parallel to the central path toward a lower duality gap, and the
</p>
<p>corrector moves perpendicular to get close to the central path. This new method
</p>
<p>does both at once. Of course, this intuitive notion must be made precise.
</p>
<p>For ρ �
&radic;
n, there is in fact a guaranteed decrease in the potential function by a
</p>
<p>fixed amount δ (see Exercises 12 and 13). Specifically,
</p>
<p>ψn+ρ(x
+, s+) &minus; ψn+ρ(x, s) � &minus;δ (5.16)
</p>
<p>for a constant δ � 0.2. This result provides a theoretical bound on the number of
</p>
<p>required iterations and the bound is competitive with other methods. However, a
</p>
<p>faster algorithm may be achieved by conducting a line search along direction d to
</p>
<p>achieve the greatest reduction in the primal-dual potential function at each iteration.
</p>
<p>We outline the algorithm here:
</p>
<p>Step 1. Start at a point (x0, y0, s0) &isin; F̊ with ψn+ρ(x0, s0) &le; ρ log((s0)Tx0) +
n log n+O(
</p>
<p>&radic;
n log n) which is determined by an initiation procedure, as discussed
</p>
<p>in Sect. 5.7. Set ρ &ge; &radic;n. Set k = 0 and γ = n/(n + ρ). Select an accuracy
parameter ε &gt; 0.
</p>
<p>Step 2. Set (x, s) = (xk, sk) and compute (dx, dy, ds) from (5.13).
</p>
<p>Step 3. Let xk+1 = xk + ᾱdx, yk+1 = yk + ᾱdy, and sk+1 = sk + ᾱds where
</p>
<p>ᾱ = arg min
α&ge;0
</p>
<p>ψn+ρ(xk + αdx, sk + αds).
</p>
<p>Step 4. Let k = k + 1. If
sT
k
</p>
<p>xk
</p>
<p>sT
0
</p>
<p>x0
&le; ε, Stop. Otherwise return to Step 2.</p>
<p/>
</div>
<div class="page"><p/>
<p>136 5 Interior-Point Methods
</p>
<p>Theorem 2. The algorithm above terminates in at most O(ρ log(n/ε)) iterations with
</p>
<p>(sk)
Txk
</p>
<p>(s0)Tx0
&le; ε.
</p>
<p>Proof. Note that after k iterations, we have from (5.16)
</p>
<p>ψn+ρ(xk, sk) &le; ψn+ρ(x0, s0) &minus; k &middot; δ &le; ρ log((s0)Tx0) + n log n + O(
&radic;
n log n) &minus; k &middot; δ.
</p>
<p>Thus, from the inequality (5.15),
</p>
<p>ρ log(sTk xk) + n log n &le; ρ log(sT0 x0) + n log n + O(
&radic;
n log n) &minus; k &middot; δ,
</p>
<p>or
</p>
<p>ρ(log(sTk xk) &minus; log(sT0 x0)) &le; &minus;k &middot; δ + O(
&radic;
n log n).
</p>
<p>Therefore, as soon as k &ge; O(ρ log(n/ε)), we must have
</p>
<p>ρ(log(sTk xk) &minus; log(sT0 x0)) &le; &minus;ρ log(1/ε),
</p>
<p>or
</p>
<p>sT
k
</p>
<p>xk
</p>
<p>sT
0
</p>
<p>x0
&le; ε.�
</p>
<p>Theorem 2 holds for any ρ &ge; &radic;n. Thus, by choosing ρ = &radic;n, the iteration
complexity bound becomes O(
</p>
<p>&radic;
n log(n/ε)).
</p>
<p>Iteration Complexity
</p>
<p>The computation of each iteration basically requires solving (5.13) for d. Note that
</p>
<p>the first equation of (5.13) can be written as
</p>
<p>Sdx + Xds = γμ1 &minus; XS1
</p>
<p>where X and S are two diagonal matrices whose diagonal entries are components of
</p>
<p>x &gt; 0 and s &gt; 0, respectively. Premultiplying both sides by S&minus;1 we have
</p>
<p>dx + S
&minus;1Xds = γμS
</p>
<p>&minus;11 &minus; x.
</p>
<p>Then, premultiplying by A and using Adx = 0, we have
</p>
<p>AS&minus;1Xds = γμAS
&minus;11 &minus; Ax = γμAS&minus;11 &minus; b.
</p>
<p>Using ds = &minus;ATdy we have
</p>
<p>(AS&minus;1XAT )dy = b &minus; γμAS&minus;11.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.7 Termination and Initialization 137
</p>
<p>Thus, the primary computational cost of each iteration of the interior-point algorithm
</p>
<p>discussed in this section is to form and invert the normal matrix AXS&minus;1AT , which
typically requires O(nm2 + m3) arithmetic operations. However, an approximation
</p>
<p>of this matrix can be updated and inverted using far fewer arithmetic operations. In
</p>
<p>fact, using a rank-one technique (see Chap. 10) to update the approximate inverse of
</p>
<p>the normal matrix during the iterative progress, one can reduce the average number
</p>
<p>of arithmetic operations per iteration to O(
&radic;
nm2). Thus, if the relative tolerance ε
</p>
<p>is viewed as a variable, we have the following total arithmetic operation complexity
</p>
<p>bound to solve a linear program:
</p>
<p>Corollary. Let ρ =
&radic;
n. Then, the algorithm above Theorem 2 terminates in at most
</p>
<p>O(nm2 log(n/ε)) arithmetic operations.
</p>
<p>5.7 Termination and Initialization
</p>
<p>There are several remaining important issues concerning interior-point algorithms
</p>
<p>for linear programs. The first issue involves termination. Unlike the simplex method
</p>
<p>which terminates with an exact solution, interior-point algorithms are continuous
</p>
<p>optimization algorithms that generate an infinite solution sequence converging to an
</p>
<p>optimal solution. If the data of a particular problem are integral or rational, an argu-
</p>
<p>ment is made that, after the worst-case time bound, an exact solution can be rounded
</p>
<p>from the latest approximate solution. Several questions arise. First, under the real
</p>
<p>number computation model (that is, the data consists of real numbers), how can
</p>
<p>we terminate at an exact solution? Second, regardless of the data&rsquo;s status, is there a
</p>
<p>practical test, which can be computed cost-effectively during the iterative process, to
</p>
<p>identify an exact solution so that the algorithm can be terminated before the worse-
</p>
<p>case time bound? Here, by exact solution we mean one that could be found using
</p>
<p>exact arithmetic, such as the solution of a system of linear equations, which can be
</p>
<p>computed in a number of arithmetic operations bounded by a polynomial in n.
</p>
<p>The second issue involves initialization. Almost all interior-point algorithms
</p>
<p>require the regularity assumption that F̊ � &empty;. What is to be done if this is not true?
A related issue is that interior-point algorithms have to start at a strictly feasible
</p>
<p>point near the central path.
</p>
<p>&lowast;Termination
</p>
<p>Complexity bounds for interior-point algorithms generally depend on an ε which
</p>
<p>must be zero in order to obtain an exact optimal solution. Sometimes it is advanta-
</p>
<p>geous to employ an early termination or rounding method while ε is still moderately
</p>
<p>large. There are five basic approaches.</p>
<p/>
</div>
<div class="page"><p/>
<p>138 5 Interior-Point Methods
</p>
<p>&bull; A &ldquo;purification&rdquo; procedure finds a feasible corner whose objective value is at
least as good as the current interior point. This can be accomplished in strongly
</p>
<p>polynomial time (that is, the complexity bound is a polynomial only in the
</p>
<p>dimensions m and n). One difficulty is that there may be many non-optimal ver-
</p>
<p>tices close to the optimal face, and the procedure might require many pivot steps
</p>
<p>for difficult problems.
</p>
<p>&bull; A second method seeks to identify an optimal basis. It has been shown that if
the linear program is nondegenerate, the unique optimal basis may be identified
</p>
<p>early. The procedure seems to work well for some problems but it has diffi-
</p>
<p>culty if the problem is degenerate. Unfortunately, most real linear programs are
</p>
<p>degenerate.
</p>
<p>&bull; The third approach is to slightly perturb the data such that the new program
is nondegenerate and its optimal basis remains one of the optimal bases of the
</p>
<p>original program. There are questions about how and when to perturb the data
</p>
<p>during the iterative process, decisions which can significantly affect the success
</p>
<p>of the effort.
</p>
<p>&bull; The fourth approach is to guess the optimal face and find a feasible solution on
that face. It consists of two phases: the first phase uses interior point algorithms to
</p>
<p>identify the complementarity partition (P&lowast;, Z&lowast;) (see Exercise 6), and the second
phase adapts the simplex method to find an optimal primal (or dual) basic solu-
</p>
<p>tion and one can use (P&lowast;, Z&lowast;) as a starting base for the second phase. This method
is often called the cross-over method. It is guaranteed to work in finite time and
</p>
<p>is implemented in several popular linear programming software packages.
</p>
<p>&bull; The fifth approach is to guess the optimal face and project the current interior
point onto the interior of the optimal face. See Fig. 5.4. The termination criterion
</p>
<p>is guaranteed to work in finite time.
</p>
<p>The fourth and fifth methods above are based on the fact that (as observed in practice
</p>
<p>and subsequently proved) many interior-point algorithms for linear programming
</p>
<p>generate solution sequences that converge to a strictly complementary solution or
</p>
<p>an interior solution on the optimal face; see Exercise 8.
</p>
<p>Initialization
</p>
<p>Most interior-point algorithms must be initiated at a strictly feasible point. The
</p>
<p>complexity of obtaining such an initial point is the same as that of solving the
</p>
<p>linear program itself. More importantly, a complete algorithm should accomplish
</p>
<p>two tasks: (1) detect the infeasibility or unboundedness status of the problem, then
</p>
<p>(2) generate an optimal solution if the problem is neither infeasible nor unbounded.
</p>
<p>Several approaches have been proposed to accomplish these goals:
</p>
<p>&bull; The primal and dual can be combined into a single linear feasibility problem,
and a feasible point found. Theoretically, this approach achieves the currently
</p>
<p>best iteration complexity bound, that is, O(
&radic;
n log(1/ε)). Practically, a significant
</p>
<p>disadvantage of this approach is the doubled dimension of the system of equations
</p>
<p>that must be solved at each iteration.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.7 Termination and Initialization 139
</p>
<p>&bull; The big-M method can be used by adding one or more artificial column(s) and/or
row(s) and a huge penalty parameter M to force solutions to become feasible
</p>
<p>during the algorithm. A major disadvantage of this approach is the numerical
</p>
<p>problems caused by the addition of coefficients of large magnitude.
</p>
<p>&bull; Phase I-then-Phase II methods are effective. A major disadvantage of this
approach is that the two (or three) related linear programs must be solved
</p>
<p>sequentially.
</p>
<p>Fig. 5.4 Illustration of the projection of an interior point onto the optimal face
</p>
<p>&bull; A modified Phase I-Phase II method approaches feasibility and optimality si-
multaneously. To our knowledge, the currently best iteration complexity bound
</p>
<p>of this approach is O(n log(1/ε)), as compared to O(
&radic;
n log(1/ε)) of the three
</p>
<p>above. Other disadvantages of the method include the assumption of non-empty
</p>
<p>interior and the need of an objective lower bound.
</p>
<p>The HSD Algorithm
</p>
<p>There is an algorithm, termed the Homogeneous Self-Dual Algorithm that over-
</p>
<p>comes the difficulties mentioned above. The algorithm achieves the theoretically
</p>
<p>best O(
&radic;
n log(1/ε)) complexity bound and is often used in linear programming
</p>
<p>software packages.
</p>
<p>The algorithm is based on the construction of a homogeneous and self-dual linear
</p>
<p>program related to (LP) and (LD) (see Sect. 5.5). We now briefly explain the two
</p>
<p>major concepts, homogeneity and self-duality, used in the construction.
</p>
<p>In general, a system of linear equations of inequalities is homogeneous if the right
</p>
<p>hand side components are all zero. Then if a solution is found, any positive mul-
</p>
<p>tiple of that solution is also a solution. In the construction used below, we allow a
</p>
<p>single inhomogeneous constraint, often called a normalizing constraint. Karmarkar&rsquo;s
</p>
<p>original canonical form is a homogeneous linear program.</p>
<p/>
</div>
<div class="page"><p/>
<p>140 5 Interior-Point Methods
</p>
<p>A linear program is termed self-dual if the dual of the problem is equivalent to
</p>
<p>the primal. The advantage of self-duality is that we can apply a primal-dual interior-
</p>
<p>point algorithm to solve the self-dual problem without doubling the dimension of
</p>
<p>the linear system solved at each iteration.
</p>
<p>The homogeneous and self-dual linear program (HSDP) is constructed from (LP)
</p>
<p>and (LD) in such a way that the point x = 1, y = 0, τ = 1, z = 1, θ = 1 is feasible.
</p>
<p>The primal program is
</p>
<p>(HSDP) minimize (n + 1)θ
</p>
<p>subject to Ax &minus;bτ +b̄θ = 0,
&minus;ATy +cτ &minus;c̄θ &ge; 0,
bTy &minus;cTx +z̄θ &ge; 0,
&minus;b̄Ty +c̄Tx &minus;z̄τ = &minus;(n + 1),
y free, x &ge; 0, τ &ge; 0, θ free;
</p>
<p>where
</p>
<p>b̄ = b &minus; Al, c̄ = c &minus; 1, z̄ = cT1 + 1. (5.17)
</p>
<p>Notice that b̄, c̄, and z̄ represent the &ldquo;infeasibility&rdquo; of the initial primal point, dual
</p>
<p>point, and primal-dual &ldquo;gap,&rdquo; respectively. They are chosen so that the system is
</p>
<p>feasible. For example, for the point x = 1, y = 0, τ = 1, θ = 1, the last equation
</p>
<p>becomes
</p>
<p>0 + cTx &minus; 1Tx &minus; (cTx + 1) = &minus;n &minus; 1.
Note also that the top two constraints in (HSDP), with τ = 1 and θ = 0, represent
</p>
<p>primal and dual feasibility (with x &ge; 0). The third equation represents reversed
weak duality (with bTy &ge; cTx) rather than the reverse. So if these three equations
are satisfied with τ = 1 and θ = 0 they define primal and dual optimal solutions.
</p>
<p>Then, to achieve primal and dual feasibility for x = 1, (y, s) = (0, 1), we add the
</p>
<p>artificial variable θ. The fourth constraint is added to achieve self-duality.
</p>
<p>The problem is self-dual because its overall coefficient matrix has the property
</p>
<p>that its transpose is equal to its negative. It is skew-symmetric.
</p>
<p>Denote by s the slack vector for the second constraint and by κ the slack scalar for
</p>
<p>the third constraint. Denote by Fh the set of all points (y, x, τ, θ, s, κ) that are feasi-
ble for (HSDP). Denote by F 0
</p>
<p>h
the set of strictly feasible points with (x, τ, s, κ) &gt; 0
</p>
<p>in Fh. By combining the constraints (Exercise 14) we can write the last (equality)
constraint as
</p>
<p>1Tx + 1T s + τ + κ &minus; (n + 1)θ = (n + 1), (5.18)
which serves as a normalizing constraint for (HSDP). This implies that for 0 &le; θ &le; 1
the variables in this equation are bounded.
</p>
<p>We state without proof the following basic result.
</p>
<p>Theorem 1. Consider problems (HSDP).
</p>
<p>(i) (HSDP) has an optimal solution and its optimal solution set is bounded.
</p>
<p>(ii) The optimal value of (HSDP) is zero, and</p>
<p/>
</div>
<div class="page"><p/>
<p>5.7 Termination and Initialization 141
</p>
<p>(y, x, τ, θ, s, κ) &isin; Fh implies that (n + 1)θ = xT s + τκ.
</p>
<p>(iii) There is an optimal solution (y&lowast;, x&lowast;, τ&lowast;, θ&lowast; = 0, s&lowast;, κ&lowast;) &isin; Fh such that
(
</p>
<p>x&lowast; + s&lowast;
</p>
<p>τ&lowast; + κ&lowast;
</p>
<p>)
</p>
<p>&gt; 0,
</p>
<p>which we call a strictly self-complementary solution.
</p>
<p>Part (ii) of the theorem shows that as θ goes to zero, the solution tends toward
</p>
<p>satisfying complementary slackness between x and s and between τ and κ. Part (iii)
</p>
<p>shows that at a solution with θ = 0, the complementary slackness is strict in the sense
</p>
<p>that at least one member of a complementary pair must be positive. For example,
</p>
<p>x1s1 = 0 is required by complementary slackness, but in this case x1 = 0, s1 = 0
</p>
<p>will not occur; exactly one of them must be positive.
</p>
<p>We now relate optimal solutions to (HSDP) to those for (LP) and (LD).
</p>
<p>Theorem 2. Let (y&lowast; , x&lowast;, τ&lowast;, θ&lowast; = 0, s&lowast;, κ&lowast;) be a strictly-self complementary solution for
(HSDP).
</p>
<p>(i) (LP) has a solution (feasible and bounded) if and only if τ&lowast; &gt; 0. In this case, x&lowast;/τ&lowast; is
an optimal solution for (LP) and y&lowast;/τ&lowast;, s&lowast;/τ&lowast; is an optimal solution for (LD).
</p>
<p>(ii) (LP) has no solution if and only if κ&lowast; &gt; 0. In this case, x&lowast;/κ&lowast; or y&lowast;/κ&lowast;or both are
certificates for proving infeasibility: if cTx&lowast; &lt; 0 then (LD) is infeasible; if &minus;bTy&lowast; &lt; 0
then (LP) is infeasible; and if both cTx&lowast; &lt; 0 and &minus;bTy&lowast; &lt; 0 then both (LP) and (LD)
are infeasible.
</p>
<p>Proof. We prove the second statement. We first assume that one of (LP) and (LD)
</p>
<p>is infeasible, say (LD) is infeasible. Then there is some certificate x̄ &ge; 0 such that
Ax̄ = 0 and cT x̄ = &minus;1. Let (ȳ = 0, s̄ = 0) and
</p>
<p>α =
n + 1
</p>
<p>1TX̄ + 1T s̄ + 1
&gt; 0.
</p>
<p>Then one can verify that
</p>
<p>ỹ&lowast; = αȳ, x̃&lowast; = αx̄, τ̃&lowast; = 0, θ̃&lowast; = 0, s̃&lowast; = αs̄, κ̃&lowast; = α
</p>
<p>is a self-complementary solution for (HSDP). Since the supporting set (the set of
</p>
<p>positive entries) of a strictly complementary solution for (HSDP) is unique (see
</p>
<p>Exercise 6), κ&lowast; &gt; 0 at any strictly complementary solution for (HSDP).
Conversely, if τ&lowast; = 0, then κ&lowast; &gt; 0, which implies that cTx&lowast; &minus; bTy&lowast; &lt; 0, i.e.,
</p>
<p>at least one of cTx&lowast; and &minus;bTy&lowast; is strictly less than zero. Let us say cTx&lowast; &lt; 0. In
addition, we have
</p>
<p>Ax&lowast; = 0, ATy&lowast; + s&lowast; = 0, (x&lowast;)T s&lowast; = 0 and x&lowast; + s&lowast; &gt; 0.
</p>
<p>From Farkas&rsquo; lemma (Exercise 5), x&lowast;/κ&lowast; is a certificate for proving dual infeasibility.
</p>
<p>The other cases hold similarly. �</p>
<p/>
</div>
<div class="page"><p/>
<p>142 5 Interior-Point Methods
</p>
<p>To solve (HSDP), we have the following theorem that resembles the the central
</p>
<p>path analyzed for (LP) and (LD).
</p>
<p>Theorem 3. Consider problem (HSDP). For any μ &gt; 0, there is a unique (y, x, τ, θ, s, κ)
in F̊h, such that
</p>
<p>(
</p>
<p>x ◦ s
τκ
</p>
<p>)
</p>
<p>= μ1.
</p>
<p>Moreover, (x, τ) = (1, 1), (y, s, κ) = (0, 0, 1) and θ = 1 is the solution with μ = 1.
</p>
<p>Theorem 3 defines an endogenous path associated with (HSDP):
</p>
<p>C =
{
</p>
<p>(y, x, τ, θ, s, κ) &isin; F 0h :
(
</p>
<p>x ◦ s
τκ
</p>
<p>)
</p>
<p>=
xT s + τκ
</p>
<p>n + 1
1
</p>
<p>}
</p>
<p>.
</p>
<p>Furthermore, the potential function for (HSDP) can be defined as
</p>
<p>ψn+1+ρ(x, τ, s, κ) = (n + 1 + ρ) log(x
T s + τκ) &minus;
</p>
<p>n
&sum;
</p>
<p>j=1
</p>
<p>log(x js j) &minus; log(τκ), (5.19)
</p>
<p>where ρ &ge; 0. One can then apply the interior-point algorithms described earlier to
solve (HSDP) from the initial point (x, τ) = (1, 1), (y, s, κ) = (0, 1, 1) and θ = 1
</p>
<p>with μ = (xT s + τκ)/(n + 1) = 1.
</p>
<p>The HSDP method outlined above enjoys the following properties:
</p>
<p>&bull; It does not require regularity assumptions concerning the existence of optimal,
feasible, or interior feasible solutions.
</p>
<p>&bull; It can be initiated at x = 1, y = 0 and s = 1, feasible or infeasible, on the
central ray of the positive orthant (cone), and it does not require a big-M penalty
</p>
<p>parameter or lower bound.
</p>
<p>&bull; Each iteration solves a system of linear equations whose dimension is almost the
same as that used in the standard (primal-dual) interior-point algorithms.
</p>
<p>&bull; If the linear program has a solution, the algorithm generates a sequence that
approaches feasibility and optimality simultaneously; if the problem is infeasible
</p>
<p>or unbounded, the algorithm produces an infeasibility certificate for at least one
</p>
<p>of the primal and dual problems; see Exercise 5.
</p>
<p>5.8 Summary
</p>
<p>The simplex method has for decades been an efficient method for solving linear pro-
</p>
<p>grams, despite the fact that there are no theoretical results to support its efficiency.
</p>
<p>Indeed, it was shown that in the worst case, the method may visit every vertex of
</p>
<p>the feasible region and this can be exponential in the number of variables and con-
</p>
<p>straints. If on practical problems the simplex method behaved according to the worst
</p>
<p>case, even modest problems would require years of computer time to solve. The
</p>
<p>ellipsoid method was the first method that was proved to converge in time propor-
</p>
<p>tional to a polynomial in the size of the program, rather than to an exponential in the</p>
<p/>
</div>
<div class="page"><p/>
<p>5.9 Exercises 143
</p>
<p>size. However, in practice, it was disappointingly less fast than the simplex method.
</p>
<p>Later, the interior-point method of Karmarkar significantly advanced the field of lin-
</p>
<p>ear programming, for it not only was proved to be a polynomial-time method, but it
</p>
<p>was found in practice to be faster than the simplex method when applied to general
</p>
<p>linear programs.
</p>
<p>The interior-point method is based on introducing a logarithmic barrier function
</p>
<p>with a weighting parameter μ; and now there is a general theoretical structure defin-
</p>
<p>ing the analytic center, the central path of solutions as μ &rarr; 0, and the duals of these
concepts. This structure is useful for specifying and analyzing various versions of
</p>
<p>interior point methods.
</p>
<p>Most methods employ a step of Newton&rsquo;s method to find a point near the central
</p>
<p>path when moving from one value of μ to another. One approach is the predictor-
</p>
<p>corrector method, which first takes a step in the direction of decreasing μ and then a
</p>
<p>corrector step to get closer to the central path. Another method employs a potential
</p>
<p>function whose value can be decreased at each step, which guarantees convergence
</p>
<p>and assures that intermediate points simultaneously make progress toward the solu-
</p>
<p>tion while remaining close to the central path.
</p>
<p>Complete algorithms based on these approaches require a number of other fea-
</p>
<p>tures and details. For example, once systematic movement toward the solution is
</p>
<p>terminated, a final phase may move to a nearby vertex or to a non-vertex point on
</p>
<p>a face of the constraint set. Also, an initial phase must be employed to obtain an
</p>
<p>feasible point that is close to the central path from which the steps of the search
</p>
<p>algorithm can be started. These features are incorporated into several commercial
</p>
<p>software packages, and generally they perform well, able to solve very large linear
</p>
<p>programs in reasonable time.
</p>
<p>5.9 Exercises
</p>
<p>1. Using the simplex method, solve the program (5.1) and count the number of
</p>
<p>pivots required.
</p>
<p>2. Prove the volume reduction rate in Theorem 1 for the ellipsoid method.
</p>
<p>3. Develop a cutting plane method, based on the ellipsoid method, to find a point
</p>
<p>satisfying convex inequalities
</p>
<p>fi(x) � 0, i = 1, . . . , m, |x|2 � E2,
</p>
<p>where fi&rsquo;s are convex functions of x in C
1.
</p>
<p>4. Consider the linear program (5.5) and assume that F̊p = {x : Ax = b, x &gt; 0}
is nonempty and its optimal solution set is bounded. Show that the dual of the
</p>
<p>problem has a nonempty interior.
</p>
<p>5. (Farkas&rsquo; lemma) Prove: Exactly one of the feasible sets {x : Ax = b, x � 0}
and {y : yTA � 0, yTb = 1} is nonempty. A vector y in the latter set is called an
infeasibility certificate for the former.</p>
<p/>
</div>
<div class="page"><p/>
<p>144 5 Interior-Point Methods
</p>
<p>6. (Strict complementarity) Consider any linear program in standard form and its
</p>
<p>dual and let both of them be feasible. Then, there always exists a strictly com-
</p>
<p>plementary solution pair, (x&lowast;, y&lowast;, s&lowast;), such that
</p>
<p>x&lowast;j s
&lowast;
j = 0 and x
</p>
<p>&lowast;
j + s
</p>
<p>&lowast;
j &gt; 0 for all j.
</p>
<p>Moreover, the supports of x&lowast; and s&lowast;, P&lowast; = { j : x&lowast;
j
&gt; 0} and Z&lowast; = { j : x&lowast;
</p>
<p>j
&gt; 0},
</p>
<p>are invariant among all strictly complementary solution pairs.
</p>
<p>7. (Central path theorem) Let (x(μ), y(μ), s(μ)) be the central path of (5.9). Then
</p>
<p>prove
</p>
<p>(a) The central path point (x(μ), y(μ), s(μ)) is bounded for 0 &lt; μ � μ0 and any
</p>
<p>given 0 &lt; μ0 &lt; &infin;.
(b) For 0 &lt; μ&prime; &lt; μ,
</p>
<p>cTx(μ&prime;) � cTx(μ) and bTy(μ&prime;) � bTy(μ).
</p>
<p>Furthermore, if x(μ&prime;) � x(μ) and y(μ&prime;) � y(μ),
</p>
<p>cTx(μ&prime;) &lt; cTx(μ) and bTy(μ&prime;) &gt; bTy(μ).
</p>
<p>(c) (x(μ), y(μ), s(μ)) converges to an optimal solution pair for (LP) and (LD).
</p>
<p>Moreover, the limit point x(0)P&lowast; is the analytic center on the primal optimal
</p>
<p>face, and the limit point s(0)Z&lowast; is the analytic center on the dual optimal
</p>
<p>face, where (P&lowast;, Z&lowast;) is the strict complementarity partition of the index set
{1, 2, . . . , n}.
</p>
<p>8. Consider a primal-dual interior point (x, y, s) &isin; N(η) where η &lt; 1. Prove that
there is a fixed quantity δ &gt; 0 such that
</p>
<p>x j � δ, for all j &isin; P&lowast;
</p>
<p>and
</p>
<p>s j � δ, for all j &isin; Z&lowast;,
where (P&lowast;, Z&lowast;) is defined in Exercise 6.
</p>
<p>9. (Potential level theorem) Define the potential level set
</p>
<p>Ψ(δ) := {(x, y, s) &isin; F̊ : ψn+ρ(x, s) � δ}.
</p>
<p>Prove
</p>
<p>(a)
</p>
<p>Ψ(δ1) &sub; Ψ(δ2) if δ1 � δ2.
(b) For every δ, Ψ(δ) is bounded and its closure Ψ(δ) has non-empty intersec-
</p>
<p>tion with the solution set.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.9 Exercises 145
</p>
<p>10. Given 0 &lt; x, 0 &lt; s &isin; En, show that
</p>
<p>n log(xTs) &minus;
n
&sum;
</p>
<p>j=1
</p>
<p>log(x js j) � n log n
</p>
<p>and
</p>
<p>xT s � exp
</p>
<p>[
</p>
<p>ψn+p(x, s) &minus; n log n
p
</p>
<p>]
</p>
<p>.
</p>
<p>11. (Logarithmic approximation) If d &isin; En such that |d|&infin; &lt; 1 then
</p>
<p>1Td �
</p>
<p>n
&sum;
</p>
<p>i=1
</p>
<p>log(1 + di) � 1
Td &minus; |d|
</p>
<p>2
</p>
<p>2(1 &minus; |d|&infin;)
.
</p>
<p>[Note:If d = (d1, d2, . . . dn) then |d|&infin; &equiv; maxi{d j}.]
12. Let the direction (dx, dy, ds) be generated by system (5.13) with γ = n/(n + ρ)
</p>
<p>and μ = xT s/n, and let the step size be
</p>
<p>α =
θ
&radic;
</p>
<p>min(Xs)
</p>
<p>|(XS)&minus;1/2( xT s
(n+ρ)
</p>
<p>1 &minus; Xs)|
, (5.20)
</p>
<p>where θ is a positive constant less than 1. Let
</p>
<p>x+ = x + αdx, y
+ = y + αdy, and s
</p>
<p>+ = s + αds.
</p>
<p>Then, using Exercise 11 and the concavity of the logarithmic function show
</p>
<p>(x+, y+, s+) &isin; F̊ and
</p>
<p>ψn+ρ(x
+, s+) &minus; ψn+ρ(x, s)
</p>
<p>� &minus;θ
&radic;
</p>
<p>min(Xs) |(Xs)&minus;1/2(1 &minus; (n + ρ)
xτS
</p>
<p>Xs)| + θ
2
</p>
<p>2(1 &minus; θ) .
</p>
<p>13. Let v = Xs in Exercise 12. Prove
</p>
<p>&radic;
</p>
<p>min(v)|V&minus;1/2(1 &minus; (n + ρ)
1TV
</p>
<p>v)| �
&radic;
</p>
<p>3/4,
</p>
<p>where V is the diagonal matrix of v. Thus, the two exercises imply
</p>
<p>ψn+ρ(x
+, s+) &minus; ψn+ρ(x, s) � &minus;θ
</p>
<p>&radic;
</p>
<p>3/4 +
θ2
</p>
<p>2(1 &minus; θ) = &minus;δ
</p>
<p>for a constant δ. One can verify that δ &gt; 0.2 when θ = 0.4.
</p>
<p>14. Prove property (5.18) for (HDSP).
</p>
<p>15. Prove Theorem 1</p>
<p/>
</div>
<div class="page"><p/>
<p>146 5 Interior-Point Methods
</p>
<p>References
</p>
<p>5.1 Computation and complexity models were developed by a number of scien-
</p>
<p>tists; see, e.g., Cook [C5], Hartmanis and Stearns [H5] and Papadimitriou
</p>
<p>and Steiglitz [P2] for the bit complexity models and Blum et al. [B21] for
</p>
<p>the real number arithmetic model. For a general discussion of complexity
</p>
<p>see Vavasis [V4]. For a comprehensive treatment which served as the basis
</p>
<p>for much of this chapter, see Ye [Y3].
</p>
<p>5.2 The Klee Minty example is presented in [K5]. Much of this material is
</p>
<p>based on a teaching note of Cottle on Linear Programming taught at Stan-
</p>
<p>ford [C6]. Practical performances of the simplex method can be seen in
</p>
<p>Bixby [B18]. The simplex method efficiency for the Markov Decision
</p>
<p>Process is due to Ye [269].
</p>
<p>5.3 The ellipsoid method was developed by Khachiyan [K4]; more develop-
</p>
<p>ments of the ellipsoid method can be found in Bland, Goldfarb and Todd
</p>
<p>[B20].
</p>
<p>5.3 The analytic center for a convex polyhedron given by linear inequalities
</p>
<p>was introduced by Huard [H12], and later by Sonnevend [S8]. The barrier
</p>
<p>function was introduced by Frisch [F19]. The central path was analyzed
</p>
<p>in McLinden [M3], Megiddo [M4], and Bayer and Lagarias [B3, B4], Gill
</p>
<p>et al. [G5].
</p>
<p>5.5 Path-following algorithms were first developed by Renegar [R1]. A primal
</p>
<p>barrier or path-following algorithm was independently analyzed by Gon-
</p>
<p>zaga [G13]. Both Gonzaga [G13] and Vaidya [V1] extended the rank-one
</p>
<p>updating technique [K2] for solving the Newton equation of each itera-
</p>
<p>tion, and proved that each iteration uses O(n2.5) arithmetic operations on
</p>
<p>average. Kojima, Mizuno and Yoshise [K6] and Monteiro and Adler [M7]
</p>
<p>developed a symmetric primal-dual path-following algorithm with the same
</p>
<p>iteration and arithmetic operation bounds.
</p>
<p>5.6&ndash;5.7 Predictor-corrector algorithms were developed by Mizuno et al. [M6].
</p>
<p>A more practical predictor-corrector algorithm was proposed by Mehrotra
</p>
<p>[M5] (also see Lustig et al. [L19] and Zhang and Zhang [Z3]). Mehrotra&rsquo;s
</p>
<p>technique has been used in almost all linear programming interior-point
</p>
<p>implementations. A primal potential reduction algorithm was initially pro-
</p>
<p>posed by Karmarkar [K2]. The primal-dual potential function was proposed
</p>
<p>by Tanabe [T2] and Todd and Ye [T5]. The primal-dual potential reduction
</p>
<p>algorithm was developed by Ye [Y1], Freund [F18], Kojima, Mizuno and
</p>
<p>Yoshise [K7], Goldfarb and Xiao [G11], Gonzaga and Todd [G14], Todd
</p>
<p>[T4], Tunςel [T10], Tutuncu [T11], and others. The homogeneous and self-
</p>
<p>dual embedding method can be found in Ye et al. [Y2], Luo et al. [L18],
</p>
<p>Andersen and Ye [A5], and many others. It is also implemented in most
</p>
<p>linear programming software packages such as SEDUMI of Sturm [S11].</p>
<p/>
</div>
<div class="page"><p/>
<p>References 147
</p>
<p>5.1&ndash;5.7 There are several comprehensive text books which cover interior-point
</p>
<p>linear programming algorithms. They include Bazaraa, Jarvis and Sherali
</p>
<p>[B6], Bertsekas [B12], Bertsimas and Tsitsiklis [B13], Cottle [C6], Cottle,
</p>
<p>Pang and Stone [C7], Dantzig and Thapa [D9, D10], Fang and Puthen-
</p>
<p>pura [F2], den Hertog [H6], Murty [M12], Nash and Sofer [N1], Nesterov
</p>
<p>[N2], Roos et al. [R4], Renegar [R2], Saigal [S1], Vanderebei [V3], and
</p>
<p>Wright [W8].</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 6
</p>
<p>Conic Linear Programming
</p>
<p>6.1 Convex Cones
</p>
<p>Conic Linear Programming, hereafter CLP, is a natural extension of Linear
</p>
<p>programming (LP). In LP, the variables form a vector which is required to be com-
</p>
<p>ponentwise nonnegative, while in CLP they are points in a pointed convex cone (see
</p>
<p>Appendix B.1) of an Euclidean space, such as vectors as well as matrices of finite
</p>
<p>dimensions. For example, Semidefinite programming (SDP) is a kind of CLP, where
</p>
<p>the variable points are symmetric matrices constrained to be positive semidefinite.
</p>
<p>Both types of problems may have linear equality constraints as well. Although CLPs
</p>
<p>have long been known to be convex optimization problems, no efficient solution
</p>
<p>algorithm was known until about two decades ago, when it was discovered that
</p>
<p>interior-point algorithms for LP discussed in Chap. 5, can be adapted to solve cer-
</p>
<p>tain CLPs with both theoretical and practical efficiency. During the same period, it
</p>
<p>was discovered that CLP, especially SDP, is representative of a wide assortment of
</p>
<p>applications, including combinatorial optimization, statistical computation, robust
</p>
<p>optimization, Euclidean distance geometry, quantum computing, optimal control,
</p>
<p>etc. CLP is now widely recognized as a powerful mathematical computation model
</p>
<p>of general importance.
</p>
<p>First, we illustrate several convex cones popularly used in conic linear optimiza-
</p>
<p>tion.
</p>
<p>Example 1. The followings are all (closed) convex cones.
</p>
<p>&bull; The n-dimensional non-negative orthant, En+ = {x &isin; En : x &ge; 0}, is a convex
cone.
</p>
<p>&bull; The set of all n-dimensional symmetric positive semidefinite matrices, denoted
by Sn+, is a convex cone, called the positive semidefinite matrix cone. When X is
positive semidefinite (positive definite), we often write the property as X � (≻)0.
</p>
<p>&copy; Springer International Publishing Switzerland 2016
</p>
<p>D.G. Luenberger, Y. Ye, Linear and Nonlinear Programming, International
Series in Operations Research &amp; Management Science 228,
DOI 10.1007/978-3-319-18842-3 6
</p>
<p>149</p>
<p/>
</div>
<div class="page"><p/>
<p>150 6 Conic Linear Programming
</p>
<p>&bull; The set {(u; x) &isin; En+1 : u &ge; |x|p} is a convex cone in En+1, called the p-order
cone where 1 &le; p &lt; &infin;. When p = 2, the cone is called second-order cone or
&ldquo;Ice-cream&rdquo; cone.
</p>
<p>Sometimes, we use the notion of conic inequalities P �K Q or Q �K P, in which
cases we simply mean P &minus; Q &isin; K.
</p>
<p>Suppose A and B are k &times; n matrices. We define the inner product
</p>
<p>A &bull; B = trace(ATB) =
&sum;
</p>
<p>i, j
</p>
<p>aijbij.
</p>
<p>When k = 1, they become n-dimensional vectors and the inner product is the stan-
</p>
<p>dard dot product of two vectors. In SDP, this definition is almost always used for the
</p>
<p>case where the matrices are both square and symmetric. The matrix norm associated
</p>
<p>with the inner product is called Frobenius norm:
</p>
<p>|X| f =
&radic;
</p>
<p>X &bull; X .
</p>
<p>For a cone K, the dual of K is the cone
</p>
<p>K&lowast; := {Y : X &bull; Y &ge; 0 for all X &isin; K}.
</p>
<p>It is not difficult to see that the dual cones of the first two cones in Example 1 are all
</p>
<p>them self, respectively; while the dual cone of the p-order cone is the q-order cone
</p>
<p>where
</p>
<p>1
</p>
<p>p
+
</p>
<p>1
</p>
<p>q
= 1.
</p>
<p>One can see that when p = 2, q = 2 as well; that is, they are both 2-order cones. For
</p>
<p>a closed convex cone K, the dual of the dual cone is itself.
</p>
<p>6.2 Conic Linear Programming Problem
</p>
<p>Now let C and Ai, i = 1, 2, . . . , m, be given matrices of E
k&times;n, b &isin; Em, and K be
</p>
<p>a closed convex cone in Ek&times;n. And let X be an unknown matrix of Ek&times;n. Then, the
standard form (primal) conic linear programming problem is
</p>
<p>(CLP) minimize C &bull; X
subject to Ai &bull; X = bi, i = 1, 2, . . . , m, X &isin; K. (6.1)
</p>
<p>Note that in CLP we minimize a linear function of the decision matrix constrained
</p>
<p>in cone K and subject to linear equality constraints.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.2 Conic Linear Programming Problem 151
</p>
<p>For convenience, we define an operator from a symmetric matrix to a vector:
</p>
<p>AX =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>A1 &bull; X
A2 &bull; X
&middot; &middot; &middot;
</p>
<p>Am &bull; X
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>. (6.2)
</p>
<p>Then, CLP can be written in a compact form:
</p>
<p>(CLP) minimize C &bull; X
subject to AX = b, X &isin; K.
</p>
<p>When cone K is the non-negative orthant En+, CLP reduces to linear programming
</p>
<p>(LP) in the standard form, where A becomes the constraint matrix A. When K is the
positive semidefinite cone Sn+, CLP is called semidefinite programming (SDP); and
when K is the p-order cone, it is called p-order cone programming. In particular,
</p>
<p>when p = 2, the model is called second-order cone programming (SOCP). Fre-
</p>
<p>quently, we write variable X in (CLP) as x if it is indeed a vector, such as when K is
</p>
<p>the nonnegative orthant or p-order cone.
</p>
<p>One can see that the problem (S DP) (that is, (6.1) with the semidefinite cone)
</p>
<p>generalizes classical linear programming in standard form:
</p>
<p>minimize cTx,
</p>
<p>subject to Ax = b, x &ge; 0.
</p>
<p>Define C = Diag[c1, c2, . . . , cn], and let Ai = Diag[ai1, ai2, . . . , ain] for i =
</p>
<p>1, 2, . . .m. The unknown is the n &times; n symmetric matrix X which is constrained by
X � 0. Since the trace of C &bull; X and Ai &bull; X depend only on the diagonal elements
of X, we may restrict the solutions X to diagonal matrices. It follows that in this
</p>
<p>case the SDP problem is equivalent to a linear program, since a diagonal matrix
</p>
<p>is positive semidefinite is and only if its all diagonal elements are nonnegative.
</p>
<p>One can further see the role of cones in the following examples.
</p>
<p>Example 1. Consider the following optimization problems with three variables.
</p>
<p>&bull; This is a linear programming problem in standard form:
</p>
<p>minimize 2x1 + x2 + x3
subject to x1 + x2 + x3 = 1,
</p>
<p>(x1; x2; x3) &ge; 0.
</p>
<p>&bull; This is a semidefinite programming problem where the dimension of the matrix
is two:
</p>
<p>minimize 2x1 + x2 + x3
subject to x1 + x2 + x3 = 1,
</p>
<p>[
</p>
<p>x1 x2
x2 x3
</p>
<p>]
</p>
<p>� 0,</p>
<p/>
</div>
<div class="page"><p/>
<p>152 6 Conic Linear Programming
</p>
<p>Let
</p>
<p>C =
</p>
<p>[
</p>
<p>2 .5
</p>
<p>.5 1
</p>
<p>]
</p>
<p>and A1 =
</p>
<p>[
</p>
<p>1 .5
</p>
<p>.5 1
</p>
<p>]
</p>
<p>.
</p>
<p>Then, the problem can be written in a standard SDP form
</p>
<p>minimize C &bull; X
subject to A1 &bull; X = 1, X &isin; S2+.
</p>
<p>&bull; This is a second-order cone programming problem:
</p>
<p>minimize 2x1 + x2 + x3
subject to x1 + x2 + x3 = 1,
</p>
<p>&radic;
</p>
<p>x2
2
+ x2
</p>
<p>3
&le; x1.
</p>
<p>We present several application examples to illustrate the flexibility of this formu-
</p>
<p>lation.
</p>
<p>Example 2 (Binary Quadratic Optimization). Consider a binary quadratic maxi-
</p>
<p>mization problem
</p>
<p>maximize xTQx + 2cTx
</p>
<p>subject to x j = {1, &minus;1}, for all j = 1, . . . , n,
</p>
<p>which is a difficult nonconvex optimization problem. The problem can be rewrit-
</p>
<p>ten as
</p>
<p>z&lowast; &equiv; maximize
[
</p>
<p>x
</p>
<p>1
</p>
<p>]T [
</p>
<p>Q c
</p>
<p>cT 0
</p>
<p>] [
</p>
<p>x
</p>
<p>1
</p>
<p>]
</p>
<p>subject to (x j)
2 = 1, for all j = 1, . . . , n,
</p>
<p>which can be also written as a homogeneous quadratic binary problem
</p>
<p>z&lowast; &equiv; maximize
[
</p>
<p>Q c
</p>
<p>cT 0
</p>
<p>]
</p>
<p>&bull;
[
</p>
<p>x
</p>
<p>xn+1
</p>
<p>] [
</p>
<p>x
</p>
<p>xn+1
</p>
<p>]T
</p>
<p>subject to I j &bull;
[
</p>
<p>x
</p>
<p>xn+1
</p>
<p>] [
</p>
<p>x
</p>
<p>xn+1
</p>
<p>]T
</p>
<p>= 1, for all j = 1, . . . , n + 1,
</p>
<p>where I j is the (n + 1) &times; (n + 1) matrix whose components are all zero except at the
jth position on the main diagonal where it is 1. Let (x&lowast;; x&lowast;
</p>
<p>n+1
) be an optimal solution
</p>
<p>for the homogeneous problem. Then, one can see that x&lowast;/x&lowast;
n+1 would be an optimal
</p>
<p>solution to the original problem.
</p>
<p>Since
</p>
<p>[
</p>
<p>x
</p>
<p>xn+1
</p>
<p>] [
</p>
<p>x
</p>
<p>xn+1
</p>
<p>]T
</p>
<p>forms a positive-semidefinite matrix (with rank equal to 1),
</p>
<p>a semidefinite relaxation of the problem is defined as</p>
<p/>
</div>
<div class="page"><p/>
<p>6.2 Conic Linear Programming Problem 153
</p>
<p>zSDP &equiv; maximize
[
</p>
<p>Q c
</p>
<p>cT 0
</p>
<p>]
</p>
<p>&bull; Y
</p>
<p>subject to I j &bull; Y = 1, for all j = 1, . . . , n + 1, (6.3)
Y &isin; Sn+1+ ,
</p>
<p>where the symmetric matrix Y has dimension n + 1. Obviously, zSDP is a upper
</p>
<p>bound of z&lowast;, since the rank-1 requirement is not enforced in the relaxation.
</p>
<p>Let&rsquo;s see how to use the relaxation. For simplicity, assuming zSDP &gt; 0, it has been
</p>
<p>shown that in many cases of this problem an optimal SDP solution either constitutes
</p>
<p>an exact solution or can be rounded to a good approximate solution of the original
</p>
<p>problem. In the former case, one can show that a rank-1 optimal solution matrix Y
</p>
<p>exists for the semidefinite relaxation and it can be found by using a rank-reduction
</p>
<p>procedure. For the latter case, one can, using a randomized rank-reduction procedure
</p>
<p>or the principle components of Y, find a rank-1 feasible solution matrix Ŷ such that
</p>
<p>[
</p>
<p>Q c
</p>
<p>cT 0
</p>
<p>]
</p>
<p>&bull; Ŷ &ge; α &middot; ZSDP &ge; α &middot; Z&lowast;
</p>
<p>for a provable factor 0 &lt; α &le; 1. Thus, one can find a feasible solution to the
original problem whose objective value is no less than a factor α of the true maximal
</p>
<p>objective cost.
</p>
<p>Example 3 (Sensor Localization). This problem is that of determining the location
</p>
<p>of sensors (for example, several cell phones scattered in a building) when measure-
</p>
<p>ments of some of their separation Euclidean distances can be determined, but their
</p>
<p>specific locations are not known. In general, suppose there are n unknown points
</p>
<p>x j &isin; Ed, j = 1, . . . , n. We consider an edge to be a path between two points,
say, i and j. There is a known subset Ne of pairs (edges) i j for which the separation
</p>
<p>distance di j is known. For example, this distance might be determined by the signal
</p>
<p>strength or delay time between the points. Typically, in the cell phone example, Ne
contains those edges whose lengths are small so that there is a strong radio signal.
</p>
<p>Then, the localization problem is to find locations x j, j = 1, . . . , n, such that
</p>
<p>|xi &minus; x j|2 = (dij)2, for all (i, j) &isin; Ne,
</p>
<p>subject to possible rotation and translation. (If the locations of some of the sensors
</p>
<p>are known, these may be sufficient to determine the rotation and translation as well.)
</p>
<p>Let X = [x1 x2 . . . xn] be the d &times; n matrix to be determined. Then
</p>
<p>|xi &minus; x j|2 = (ei &minus; e j)TXTX(ei &minus; e j),
</p>
<p>where ei &isin; En is the vector with 1 at the ith position and zero everywhere else. Let
Y = XTX. Then the semidefinite relaxation of the localization problem is to find Y
</p>
<p>such that</p>
<p/>
</div>
<div class="page"><p/>
<p>154 6 Conic Linear Programming
</p>
<p>(ei &minus; e j)(ei &minus; e j)T &bull; Y = (di j)2, for all (i, j) &isin; Ne,
Y � 0.
</p>
<p>This problem is one of finding a feasible solution; the objective function is null. But
</p>
<p>if the distance measurements have noise, one can add additional variables and an
</p>
<p>error objective to minimize. For example,
</p>
<p>minimize
&sum;
</p>
<p>(i, j)&isin;Ne |zi j|
subject to (ei &minus; e j)(ei &minus; e j)T &bull; Y + zi j = (di j)2, for all (i, j) &isin; Ne,
</p>
<p>Y � 0.
</p>
<p>This problem can be converted into a conic linear program with mixed nonnegative
</p>
<p>orthant and semidefinite cones.
</p>
<p>Under certain graph structure, an optimal SDP solution Y of the formulation would
</p>
<p>be guaranteed rank-d so that it constitutes an exact solution of the original problem.
</p>
<p>Also, in general Y can be rounded to a good approximate solution of the original
</p>
<p>problem. For example, one can, using a randomized rank-reduction procedure or the
</p>
<p>d principle components of Y, find a rank-d solution matrix Ŷ.
</p>
<p>6.3 Farkas&rsquo; Lemma for Conic Linear Programming
</p>
<p>We first introduce the notion of &ldquo;interior&rdquo; of cones.
</p>
<p>Definition 1. We call X an interior point of cone K if and only if, for any point
</p>
<p>Y &isin; K&lowast;, Y &bull; X = 0 implies Y = 0.
</p>
<p>The set of interior points of K is denoted by
◦
K.
</p>
<p>Theorem 1. The interior of the followings convex cones are given as:
</p>
<p>&bull; The interior of the non-negative orthant cone is the set of all vectors where every entry
is positive.
</p>
<p>&bull; The interior of the positive semidefinite cone is the set of all positive definite matrices.
&bull; The interior of p-order cone is the set of {(u; x) &isin; En+1 : u &gt; |x|p}.
</p>
<p>We give a sketch of the proof for the second order cone, i.e., p = 2. Let (ū; x̄) � 0
</p>
<p>be any second-order cone point but ū = |x̄|. Then, we can choose a dual cone (also
the second-order cone) point (v; y) such that
</p>
<p>v = αū, y = &minus;αx̄,
</p>
<p>for a positive α. Note that
</p>
<p>(ū; x̄) &bull; (v; y) = αv̄2 &minus; α|x̄|2 = 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.3 Farkas&rsquo; Lemma for Conic Linear Programming 155
</p>
<p>Then, one can let α &rarr; &infin; so that (v; y) cannot be bounded.
Now let (ū; x̄) be any given second-order cone point with ū &gt; |x̄|. We like to prove
</p>
<p>that, for any dual cone (also the second-order cone) point (v; y),
</p>
<p>(ū; x̄) &bull; (v; y) = 0
</p>
<p>implies that (v; y) is bounded. Note that
</p>
<p>0 = (ū; x̄) &bull; (v; y) = ūv + x̄ &bull; y
</p>
<p>or
</p>
<p>ūv &le; &minus;x̄ &bull; y &le; |x̄||y|.
</p>
<p>If v = 0, we must have y = 0; otherwise,
</p>
<p>ū &le; |x̄||y|/v &le; |x|,
</p>
<p>which contradicts ū &gt; |x̄|.
We leave the proof of the following proposition as an exercise.
</p>
<p>Proposition 1. Let X &isin;
◦
K and Y &isin; K&lowast;. Then For any nonnegative constant κ, Y &bull; X &le; κ
</p>
<p>implies that Y is bounded.
</p>
<p>Let us now consider the feasible region of (CLP) (6.1):
</p>
<p>F := {X : AX = b, X &isin; K};
</p>
<p>where the interior of the feasible region is
</p>
<p>◦
F := {X : AX = b, X &isin;
</p>
<p>◦
K}.
</p>
<p>If F is empty with K = En+, from Farkas&rsquo; lemma for linear programming, a vector
y &isin; Em, with yTA &le; 0 and yTb &gt; 0, always exists and is called an infeasibility
certificate for the system {x : Ax = b, x &ge; 0}.
</p>
<p>Does this alternative relations hold for K being a general closed convex one? Let
</p>
<p>us rigorousize the question. Let us define the reverse operator of (6.2) from a vector
</p>
<p>to a matrix:
</p>
<p>yTA =
m
&sum;
</p>
<p>i=1
</p>
<p>Aiyi. (6.4)
</p>
<p>Note that, by the definition, for any matrix X &isin; Ek&times;n
</p>
<p>yTA &bull; X = yT (AX),
</p>
<p>that is, the association property holds. Also, (yTA)T = ATy, that is, the transpose
operation applies here as well.
</p>
<p>Then, the question becomes: when F is empty, does there exist a vector y &isin;
Em such that &minus;yTA &isin; K&lowast; and yTb &gt; 0? Similarly, one can ask: when set</p>
<p/>
</div>
<div class="page"><p/>
<p>156 6 Conic Linear Programming
</p>
<p>{y : CT &minus; yTA &isin; K} is empty, does there exist a matrix X &isin; K&lowast; such that AX = 0
and C &bull; X &lt; 0? Note that the answer to the second question is also &ldquo;yes&rdquo; when
K = En+.
</p>
<p>Example 1. The answer to either question is &ldquo;not necessarily&rdquo;; see example below.
</p>
<p>&bull; For the first question, consider K = S2+ and
</p>
<p>A1 =
</p>
<p>[
</p>
<p>1 0
</p>
<p>0 0
</p>
<p>]
</p>
<p>, A2 =
</p>
<p>[
</p>
<p>0 1
</p>
<p>1 0
</p>
<p>]
</p>
<p>and
</p>
<p>b =
</p>
<p>[
</p>
<p>0
</p>
<p>2
</p>
<p>]
</p>
<p>&bull; For the second question, consider K = S2+ and
</p>
<p>C =
</p>
<p>[
</p>
<p>0 1
</p>
<p>1 0
</p>
<p>]
</p>
<p>and A1 =
</p>
<p>[
</p>
<p>1 0
</p>
<p>0 0
</p>
<p>]
</p>
<p>.
</p>
<p>However, if the data set A satisfies additional conditions, the answer would be
&ldquo;yes&rdquo;; see theorem below.
</p>
<p>Theorem 2 (Farkas&rsquo; Lemma for CLP). We have
</p>
<p>&bull; Consider set
Fp := {X : AX = b, X &isin; K}.
</p>
<p>Suppose that there exists a vector
◦
y such that &minus;
</p>
<p>◦
y
T
</p>
<p>A &isin;
◦
K&lowast;. Then,
</p>
<p>1. Set C := {AX &isin; Em : X &isin; K} is a closed convex set;
2. Fp has a (feasible) solution if and only if set {y : &minus;yTA &isin; K&lowast;, yTb &gt; 0} has no
</p>
<p>feasible solution.
</p>
<p>&bull; Consider set
Fd := {y : CT &minus; yTA &isin; K}.
</p>
<p>Suppose that there exists a vector
◦
X&isin;
</p>
<p>◦
K&lowast; such thatA
</p>
<p>◦
X= 0. Then,
</p>
<p>1. Set C := {S &minus; yTA : S &isin; K} is a closed convex set;
2. Fd has a (feasible) solution if and only if set {X : AX = 0, X &isin; K&lowast;, C &bull;X &lt; 0} has
</p>
<p>no feasible solution.
</p>
<p>Proof. We prove the first statement of the theorem. We prove the first part. It is
</p>
<p>clear that C is a convex set. To prove that C is a closed set, we need to show that
</p>
<p>if yk := AXk &isin; Em for Xk &isin; K, k = 1, . . ., converges to a vector ȳ, then ȳ &isin; C or
there is X̄ &isin; K such that ȳ := AX̄. Without loss of generality, we assume that yk is
a bounded sequence. Then, we have, for a positive constant c,
</p>
<p>c &ge; &minus;(
◦
y)Tyk = &minus;(
</p>
<p>◦
y)T (AXk) = &minus;(
</p>
<p>◦
y)TA &bull; Xk,&forall;k.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.3 Farkas&rsquo; Lemma for Conic Linear Programming 157
</p>
<p>Since &minus;(
◦
y)TA &isin;
</p>
<p>◦
K&lowast;, by definition, the sequence of Xk is also bounded. Then there is
</p>
<p>at least an accumulate point X̄ &isin; K because K is a closed cone. Thus, we must have
ȳ := AX̄.
</p>
<p>We now prove the second part. If Fp has a feasible solution X̄. Then, let y make
&minus;yTA &isin; K&lowast;
</p>
<p>&minus;yTb = &minus;yT (AX̄) = &minus;yTA &bull; X̄ &ge; 0.
Thus, it must be true yTb &le; 0, that is, {y : &minus;yTA &isin; K&lowast;, yTb &gt; 0} must be empty.
</p>
<p>On the other hand, let Fp has no feasible solution, or equivalently, b � C. We
now show that {y : &minus;yTA &isin; K&lowast;, yTb &gt; 0} must be nonempty.
</p>
<p>Since C is a closed convex set, from the separating hyperplane theorem, there
</p>
<p>must exist a ȳ &isin; Em such that
</p>
<p>ȳTb &gt; ȳTy, &forall;y &isin; C,
</p>
<p>or, from y = AX, X &isin; K, we have
</p>
<p>ȳTb &gt; ȳT (AX) = ȳTA &bull; X, &forall;X &isin; K.
</p>
<p>That is, ȳTA &bull; X is bounded above for all X &isin; K.
Immediately, we see ȳTb &gt; 0 since 0 &isin; K. Next, it must be true &minus;ȳTA &isin; K&lowast;.
</p>
<p>Otherwise, we must be able to find an X̄ &isin; K such that &minus;ȳTA &bull; X̄ &lt; 0 by the
definition of K and its dual K&lowast;. For any positive constant α we maintain αX̄ &isin; K
and let α go to &infin;. Then, ȳTA&bull; (αX̄) goes to &infin;, contradicting the fact that ȳTA&bull; X
is bounded above for all X &isin; K. Thus, ȳ is a feasible solution in {y : &minus;yTA &isin;
K&lowast;, yTb &gt; 0}. �
</p>
<p>Note that C may not be a closed set if the interior condition of Theorem 2 is not
</p>
<p>met. Consider A1, A2 and b in Example 1, and we have
</p>
<p>C =
</p>
<p>{
</p>
<p>AX =
[
</p>
<p>A1 &bull; X
A2 &bull; X
</p>
<p>]
</p>
<p>: X &isin; S2+
}
</p>
<p>.
</p>
<p>Let
</p>
<p>Xk =
</p>
<p>[
</p>
<p>1
k
</p>
<p>1
</p>
<p>1 k
</p>
<p>]
</p>
<p>&isin; S2+, &forall;k = 1, . . . .
</p>
<p>Then we see
</p>
<p>yk = AXk =
[
</p>
<p>1
k
</p>
<p>2
</p>
<p>]
</p>
<p>.
</p>
<p>As k &rarr; &infin; we see yk converges b, but b is not in C.</p>
<p/>
</div>
<div class="page"><p/>
<p>158 6 Conic Linear Programming
</p>
<p>6.4 Conic Linear Programming Duality
</p>
<p>Because conic linear programming is an extension of classical linear programming,
</p>
<p>it would seem that there is a natural dual to the primal problem, and that this dual
</p>
<p>is itself a conic linear program. This is indeed the case, and it is related to the primal
</p>
<p>in much the same way as primal and dual linear programs are related. Furthermore,
</p>
<p>the primal and dual together lead to the formation a primal-dual solution method,
</p>
<p>which is discussed later in this chapter.
</p>
<p>The dual of the (primal) CLP (6.1) is
</p>
<p>(CLD) maximize yTb
</p>
<p>subject to
&sum;m
</p>
<p>i
yiAi + S = C
</p>
<p>T , S &isin; K&lowast;. (6.5)
</p>
<p>On written in a compact form:
</p>
<p>(CLD) maximize yTb
</p>
<p>subject to yTA + S = CT , S &isin; K&lowast;.
</p>
<p>Notice that S represents a slack matrix, and hence the problem can alternatively be
</p>
<p>expressed as
</p>
<p>maximize yTb
</p>
<p>subject to
&sum;m
</p>
<p>i
yiAi �K&lowast; CT . (6.6)
</p>
<p>Recall that conic inequality Q �K P means P &minus; Q &isin; K.
Again, just like linear programming, the dual of (CLD) will be (CLP), and they
</p>
<p>form a primal and dual pair. Whichever is the primal, then the other will be the dual.
</p>
<p>We would see more primal and dual relations later.
</p>
<p>Example 1. Here are dual problems to the three instances in Example 1 where y is
</p>
<p>just a scalar.
</p>
<p>&bull; The dual to the linear programming instance:
</p>
<p>maximize y
</p>
<p>subject to y(1, 1, 1) + (s1, s2, s3) = (2, 1, 1),
</p>
<p>s = (s1, s2, s3) &isin; K&lowast; = E3+.
</p>
<p>&bull; The dual to semidefinite programming instance:
</p>
<p>maximize y
</p>
<p>subject to yA1 + S = C,
</p>
<p>S &isin; K&lowast; = S2+,</p>
<p/>
</div>
<div class="page"><p/>
<p>6.4 Conic Linear Programming Duality 159
</p>
<p>where recall
</p>
<p>C =
</p>
<p>[
</p>
<p>2 .5
</p>
<p>.5 1
</p>
<p>]
</p>
<p>and A1 =
</p>
<p>[
</p>
<p>1 .5
</p>
<p>.5 1
</p>
<p>]
</p>
<p>.
</p>
<p>&bull; The dual to the second-order cone instance:
</p>
<p>maximize y
</p>
<p>subject to y(1, 1, 1) + (s1, s2, s3) = (2, 1, 1),
&radic;
</p>
<p>s2
2
+ s2
</p>
<p>3
&le; s1, or s = (s1, s2, s3) in second-order cone.
</p>
<p>Let us consider a couple of more dual examples of the problems we posted earlier.
</p>
<p>Example 2 (The Dual of Binary Quadratic Maximization). Consider the semidefi-
</p>
<p>nite relaxation (6.3) for the binary quadratic maximization problem. It&rsquo;s dual is
</p>
<p>minimize
&sum;n+1
</p>
<p>j=1
y j
</p>
<p>subject to
&sum;n+1
</p>
<p>j=1
y jI j &minus; S =
</p>
<p>[
</p>
<p>Q c
</p>
<p>cT 0
</p>
<p>]
</p>
<p>, S � 0.
</p>
<p>Note that
n+1
&sum;
</p>
<p>j=1
</p>
<p>y jI j &minus;
[
</p>
<p>Q c
</p>
<p>cT 0
</p>
<p>]
</p>
<p>is exactly the Hessian matrix of the Lagrange function of the quadratic maximization
</p>
<p>problem; see Chap. 11. Therefore, there is a close connection between the Lagrange
</p>
<p>and conic dualities. The problems is to find a diagonal matrix Diag[(y1; . . . ; yn+1)]
</p>
<p>such that the Lagrange Hessian is positive semidefinite and its sum of diagonal
</p>
<p>elements is minimized.
</p>
<p>Example 3 (The Dual of Sensor Localization). Consider the semidefinite program-
</p>
<p>ming relaxation for the sensor localization problem (with no noises). It&rsquo;s dual is
</p>
<p>maximize
&sum;
</p>
<p>(i, j)&isin;Ne
yi j
</p>
<p>subject to
&sum;
</p>
<p>(i, j)&isin;Ne
yi j(ei &minus; e j)(ei &minus; e j)T + S = 0, S � 0.
</p>
<p>Here, yi j represents an internal force or tension on edge (i, j). Obviously, yi j = 0
</p>
<p>for all (i, j) &isin; Ne is a feasible solution for the dual. However, finding non-trivial
internal forces is a fundamental problem in network and structure design, and the
</p>
<p>maximization of the dual would help to achieve the goal.
</p>
<p>Many optimization problems can be directly cast in the CLD form.
</p>
<p>Example 4 (Euclidean Facility Location). This problem is to determine the location
</p>
<p>of a facility serving n clients placed in a Euclidean space, whose known locations
</p>
<p>are denoted by a j &isin; Ed, j = 1, . . . , n. The location of the facility would minimize</p>
<p/>
</div>
<div class="page"><p/>
<p>160 6 Conic Linear Programming
</p>
<p>the sum of the Euclidean distances from the facility to each of the clients. Let the
</p>
<p>location decision be vector f &isin; Ed. Then the problem is
</p>
<p>minimize
&sum;n
</p>
<p>j=1 |f &minus; a j| .
</p>
<p>The problem can be reformulated as
</p>
<p>minimize
&sum;n
</p>
<p>j=1 δ j
subject to s j + f = a j, &forall; j = 1, . . . , n,
</p>
<p>|s j| &le; δ j, &forall; j = 1, . . . , n.
</p>
<p>This is a conic formulation in the (CLD) form. To see it clearly, let d = 2 and n = 3
</p>
<p>in the example, and let
</p>
<p>AT =
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>1 0 0 0 0
</p>
<p>0 0 0 &minus;1 0
0 0 0 0 &minus;1
0 1 0 0 0
</p>
<p>0 0 0 &minus;1 0
0 0 0 0 &minus;1
0 0 1 0 0
</p>
<p>0 0 0 &minus;1 0
0 0 0 0 &minus;1
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>&isin; E9&times;5, b =
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>1
</p>
<p>1
</p>
<p>1
</p>
<p>0
</p>
<p>0
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>&isin; E5, c =
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>0
</p>
<p>0
</p>
<p>0
</p>
<p>a1
a2
a3
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>&isin; E9,
</p>
<p>and variable vector
</p>
<p>y = [δ1; δ2; δ3; f] &isin; E5.
Then, the facility location problem becomes
</p>
<p>minimize yTb
</p>
<p>subject to yTA + sT = cT , s &isin; K;
</p>
<p>where K is the product of three second-order cones each of which has dimension 3.
</p>
<p>More precisely, the first three elements of s &isin; E9 are in the 3-dimensional second-
order cone; and so are the second three elements and the third three elements of s.
</p>
<p>In general, the product of (possibly mixed) cones, say K1, K2 and K3, is denoted by
</p>
<p>K1 &oplus; K2 &oplus; K3, and X &isin; K1 &oplus; K2 &oplus; K3 means that X is divided into three components
such that
</p>
<p>X = (X1; X2; X3), where X1 &isin; K1, X2 &isin; K2, and X3 &isin; K3.
</p>
<p>The dual of the facility location problem would be in the (CLP) form:
</p>
<p>minimize cTx
</p>
<p>subject to Ax = b, x &isin; K&lowast;;</p>
<p/>
</div>
<div class="page"><p/>
<p>6.4 Conic Linear Programming Duality 161
</p>
<p>where
</p>
<p>K&lowast; = (K1 &oplus; K2 &oplus; K3)&lowast; = K&lowast;1 &oplus; K&lowast;2 &oplus; K&lowast;3 .
That is, in this particular problem, the first three elements of x &isin; E9 are in the
3-dimensional second-order cone; and so are the second three elements and the third
</p>
<p>three elements of x.
</p>
<p>Consider further the equality constraints, the dual can be simplified as
</p>
<p>maximize
&sum;3
</p>
<p>j=1 a
T
j
x j
</p>
<p>subject to
&sum;3
</p>
<p>j=1 x j = 0 &isin; E2,
|x j| &le; 1, &forall; j = 1, 2, 3.
</p>
<p>Example 5 (Quadratic Constraints). Quadratic constraints can be transformed to
</p>
<p>linear semidefinite form by using the concept of Schur complements. Let A be a
</p>
<p>(symmetric) m-dimension positive definite matrix, C be a symmetric n-dimension
</p>
<p>matrix, and B be an m &times; n matrix. Then, matrix
</p>
<p>S = C &minus; BTA&minus;1B
</p>
<p>is called the Schur complement of A in the matrix
</p>
<p>Z =
</p>
<p>[
</p>
<p>A B
</p>
<p>BT C
</p>
<p>]
</p>
<p>.
</p>
<p>Moreover, Z is positive semidefinite if and only if S is positive semidefinite.
</p>
<p>Now consider a general quadratic constraint of the form
</p>
<p>yTBTBy &minus; cTy &minus; d &le; 0. (6.7)
</p>
<p>This is equivalent to
[
</p>
<p>I By
</p>
<p>yTBT cTy + d
</p>
<p>]
</p>
<p>� 0 (6.8)
</p>
<p>because the Schur complement of this matrix with respect to I is the negative of the
</p>
<p>left side of the original constraint (6.7). Note that in this larger matrix, the variable
</p>
<p>y appears only affinely, not quadratically.
</p>
<p>Indeed, (6.8) can be written as
</p>
<p>P(y) = P0 + y1P1 + y2P2 + &middot; &middot; &middot; ynPn � 0, (6.9)
</p>
<p>where
</p>
<p>P0 =
</p>
<p>[
</p>
<p>I 0
</p>
<p>0 d
</p>
<p>]
</p>
<p>, Pi =
</p>
<p>[
</p>
<p>0 bi
bT
i
ci
</p>
<p>]
</p>
<p>for i = 1, 2, . . . n
</p>
<p>with bi being the ith column of B and ci being the ith component of c. The constraint
</p>
<p>(6.9) is of the form that appears in the dual form of a semidefinite program.</p>
<p/>
</div>
<div class="page"><p/>
<p>162 6 Conic Linear Programming
</p>
<p>There is a more efficient mixed semidefinite and second-order cone formulation
</p>
<p>of the inequality (6.7) to reduce the dimension of semidefinite cone. We first intro-
</p>
<p>duce slack variable s and s0 by linear constraints:
</p>
<p>By &minus; s = 0
</p>
<p>Then, we let |s| &le; s0 (or (s0; s) in the second-order cone) and
[
</p>
<p>1 s0
s0 c
</p>
<p>Ty + d
</p>
<p>]
</p>
<p>� 0.
</p>
<p>Again, the matrix constraint is of the dual form of a semidefinite cone, but its
</p>
<p>dimension is fixed at 2.
</p>
<p>Suppose the original optimization problem has a quadratic objective: mini-
</p>
<p>mize q(x). The objective can be written instead as: minimize t subject to q(x) &le; t,
and then this constraint as well as any number of other quadratic constraints can
</p>
<p>be transformed to semidefinite constraints, and hence the entire problem converted
</p>
<p>to a mixed second-order cone and semidefinite program. This approach is useful
</p>
<p>in many applications, especially in various problems of financial engineering and
</p>
<p>control theory.
</p>
<p>The duality is manifested by the relation between the optimal values of the primal
</p>
<p>and dual programs. The weak form of this relation is spelled out in the following
</p>
<p>lemma, the proof of which, like the weak form of other duality relations we have
</p>
<p>studied, is essentially an accounting issue.
</p>
<p>Weak Duality in CLP. Let X be feasible for (CLP) and (y, S) feasible for (CLD). Then,
</p>
<p>C &bull; X &ge; yTb.
</p>
<p>Proof. By direct calculation
</p>
<p>C &bull; X &minus; yTb = (
m
&sum;
</p>
<p>i=1
</p>
<p>yiAi + S) &bull; X &minus; yTb
</p>
<p>=
</p>
<p>m
&sum;
</p>
<p>i=1
</p>
<p>yi(Ai &bull; X) + S &bull; X &minus; yTb
</p>
<p>=
</p>
<p>m
&sum;
</p>
<p>i=1
</p>
<p>yibi + S &bull; X &minus; yTb
</p>
<p>= S &bull; X &ge; 0,
</p>
<p>where the last inequality comes from X &isin; K and S &isin; K&lowast;. �
</p>
<p>As in other instances of duality, the strong duality of conic linear programming
</p>
<p>is weak unless other conditions hold. For example, the duality gap may not be zero
</p>
<p>at optimality in the following SDP instance.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.4 Conic Linear Programming Duality 163
</p>
<p>Example 6. The following semidefinite program has a duality gap:
</p>
<p>C =
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>0 1 0
</p>
<p>1 0 0
</p>
<p>0 0 0
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>, A1 =
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>0 0 0
</p>
<p>0 1 0
</p>
<p>0 0 0
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>, A2 =
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>0 &minus;1 0
&minus;1 0 0
0 0 2
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>and
</p>
<p>b =
</p>
<p>[
</p>
<p>0
</p>
<p>2
</p>
<p>]
</p>
<p>.
</p>
<p>The primal minimal objective value is 0 achieved by
</p>
<p>X =
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>0 0 0
</p>
<p>0 0 0
</p>
<p>0 0 1
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>and the dual maximal objective value is &minus;2 achieved by y = [0, &minus;1]; so the duality
gap is 2.
</p>
<p>However, under certain technical conditions, there would be no duality gap. One
</p>
<p>condition is related to weather or not the primal feasible region Fp or dual feasible
region has an interior feasible solution. We say Fp has an interior (feasible solution)
if and only if
</p>
<p>◦
F p:= {X : AX = b, X &isin;
</p>
<p>◦
K}
</p>
<p>is non-empty, and Fd has an interior feasible solution if and only if
◦
F d:= {(y, S) : yTA + S = C, S &isin;
</p>
<p>◦
K
</p>
<p>&lowast;
}
</p>
<p>is non-empty. We state here a version of the strong duality theorem.
</p>
<p>Strong Duality in (CLP).
</p>
<p>i) Let (CLP) or (CLD) be infeasible, and furthermore the other be feasible and has an
interior. Then the other is unbounded.
</p>
<p>ii) Let (CLP) and (CLD) be both feasible, and furthermore one of them has an interior.
Then there is no duality gap between (CLP) and (CLD).
</p>
<p>iii) Let (CLP) and (CLD) be both feasible and have interior. Then, both have optimal
solutions with no duality gap.
</p>
<p>Proof. We let cone H = K &oplus; E1+ in the following proof.
i) Suppose Fd is empty and Fp is feasible and has an interior feasible solution.
</p>
<p>Then, we have an X̄ &isin;
◦
K and τ̄ = 1 that is an interior feasible solution to (homo-
</p>
<p>geneous) conic system:
</p>
<p>AX̄ &minus; bτ̄ = 0, (X̄, τ̄) &isin;
◦
H .</p>
<p/>
</div>
<div class="page"><p/>
<p>164 6 Conic Linear Programming
</p>
<p>Now, for any z&lowast;, we form an alternative system pair based on Farkas&rsquo; Lemma
(Theorem 2):
</p>
<p>{(X, τ) : AX &minus; bτ = 0, C &bull; X &minus; z&lowast;τ &lt; 0, (X, τ) &isin; H},
</p>
<p>and
</p>
<p>{(y; S, κ) : ATy + S = C, &minus;bTy + κ = &minus;z&lowast;, (S, κ) &isin; H&lowast;}.
But the latter is infeasible, so that the former has a feasible solution (X, τ).
</p>
<p>At such a feasible solution, if τ &gt; 0, we have C &bull; (X/τ) &lt; z&lowast; for any z&lowast;.
Otherwise, τ = 0 implies that a new solution X̄ + αX is feasible for (CLP)
</p>
<p>for any positive α; and, as α &rarr; &infin;, the objective value of the new solution goes
to &minus;&infin;. Hence, either way we have a feasible solution for (CLP) whose objective
value is unbounded from below.
</p>
<p>ii) Let Fp be feasible and have an interior feasible solution, and let z&lowast; be its objec-
tive infimum. Again, we have an alternative system pair as listed in the proof
</p>
<p>of i). But now the former is infeasible, so that we have a solution for the latter.
</p>
<p>From the Weak Duality theorem bTy &le; z&lowast;, thus we must have κ = 0, that is, we
have a solution (y, S) such that
</p>
<p>ATy + S = C, bTy = z&lowast;, S &isin; K&lowast;.
</p>
<p>iii) We only need to prove that there exist a solution X &isin; Fp such that C &bull; X = z&lowast;,
that is, the infimum of (CLP) is attainable. But this is just the other side of the
</p>
<p>proof given that Fd is feasible and has an interior feasible solution, and z&lowast; is
also the supremum of (CLD). �
</p>
<p>Again, if one of (CLP) and (CLD) has no interior feasible solution, the common
</p>
<p>objective value may not be attainable. For example,
</p>
<p>C =
</p>
<p>[
</p>
<p>1 0
</p>
<p>0 0
</p>
<p>]
</p>
<p>, A1 =
</p>
<p>[
</p>
<p>0 1
</p>
<p>1 0
</p>
<p>]
</p>
<p>, and b1 = 2.
</p>
<p>The dual is feasible but has no interior, while the primal has an interior. The common
</p>
<p>objective value equals 0, but no primal solution attaining the infimum value.
</p>
<p>Most of these examples that make the strong duality failed are superficial, and
</p>
<p>a small perturbation would overcome the failure. Thus, in real applications and in
</p>
<p>the rest of the chapter, we may assume that both (CLP) and (CLD) have interior
</p>
<p>when they are feasible. Consequently, any primal and dual optimal solution pair
</p>
<p>must satisfy the optimality conditions:
</p>
<p>C &bull; X &minus; yTb = 0
AX = b
</p>
<p>yTA + S = CT
X &isin; K, S &isin; K&lowast;
</p>
<p>; (6.10)</p>
<p/>
</div>
<div class="page"><p/>
<p>6.4 Conic Linear Programming Duality 165
</p>
<p>or
</p>
<p>X &bull; S = 0
AX = b
</p>
<p>yTA + S = CT
X &isin; K, S &isin; K&lowast;
</p>
<p>. (6.11)
</p>
<p>We now present an application of the strong duality theorem.
</p>
<p>Example 7 (Robust Portfolio Design). The Markowitz portfolio design model (also
</p>
<p>see 5) is
</p>
<p>minimize xTΣx
</p>
<p>subject to 1Tx = 1, πTx &ge; π,
</p>
<p>where Σ is the covariance matrix and π is the expect return rate vector of a set of
</p>
<p>stocks, and π is the desired return rate of the portfolio. The problem can be equiva-
</p>
<p>lently written as a mixed conic problem
</p>
<p>minimize Σ &bull; X
subject to 1Tx = 1, πTx &ge; π,
</p>
<p>X &minus; xxT � 0.
</p>
<p>Now suppose Σ is incomplete and/or uncertain, and it is expressed by
</p>
<p>Σ0 +
</p>
<p>m
&sum;
</p>
<p>i=1
</p>
<p>yiΣi(� 0),
</p>
<p>for some variables yi&rsquo;s. Then, we like to solve a robust model
</p>
<p>minimize
</p>
<p>{
</p>
<p>maxy
(
</p>
<p>Σ0 +
&sum;m
</p>
<p>i=1 yiΣi
)
</p>
<p>&bull; X
s.t. Σ0 +
</p>
<p>&sum;m
i=1 yiΣ i � 0
</p>
<p>}
</p>
<p>subject to 1Tx = 1, πTx &ge; π,
X &minus; xxT � 0.
</p>
<p>The inner problem is an SDP problem. Assuming strong duality holds, we replace
</p>
<p>it by its dual, and have
</p>
<p>minimize
</p>
<p>⎧
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>minY Σ0 &bull; (Y + X)
s.t. Σi &bull; (Y + X) = 0, &forall;i = 1, . . . ,m,
</p>
<p>Y � 0
</p>
<p>⎫
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎬
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎭
</p>
<p>subject to 1Tx = 1, πTx &ge; π,
X &minus; xxT � 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>166 6 Conic Linear Programming
</p>
<p>Then, we can integrate the two minimization problems together and form
</p>
<p>minimize Σ0 &bull; (Y + X)
subject to 1Tx = 1, πTx &ge; π,
</p>
<p>Σ i &bull; (Y + X) = 0, &forall;i = 1, . . . ,m,
Y � 0, X &minus; xxT � 0.
</p>
<p>6.5 Complementarity and Solution Rank of SDP
</p>
<p>In linear programming, since x &ge; 0 and s &ge; 0,
</p>
<p>0 = x &bull; s = xT s =
n
&sum;
</p>
<p>j=1
</p>
<p>x js j
</p>
<p>implies that x js j = 0 for all j = 1, . . . , n. This property is often called complemen-
</p>
<p>tarity. Thus, besides feasibility, and optimal linear programming solution pair must
</p>
<p>satisfy complementarity.
</p>
<p>Now consider semidefinite cone Sn+. Since X � 0 and S � 0, 0 = X &bull; S implies
XS = 0, that is, the regular matrix product of the two is a zero matrix. In other
</p>
<p>words, every column (or row) of X is orthogonal to every column (or row) of X.
</p>
<p>We also call such property complementarity. Thus, besides feasibility, an optimal
</p>
<p>semidefinite programming solution pair must satisfy complementarity.
</p>
<p>Proposition 1. Let X&lowast; and (y&lowast;, S&lowast;) be any optimal SDP solution pair with zero duality gap.
Then complementarity of X&lowast; and S&lowast; implies
</p>
<p>rank(X&lowast;) + rank(S&lowast;) &le; n.
</p>
<p>Furthermore, is there an optimal (dual) S&lowast; such that rankS&lowast; &ge; d, then the rank of any
optimal (primal) X&lowast; is bounded above by n &minus; d, where integer 0 &le; d &le; n; and the converse
is also true.
</p>
<p>In certain SDP problems, one may be interested in finding an optimal solution
</p>
<p>whose rank is minimal, while the interior-point algorithm for SDP (developed later)
</p>
<p>typically generates solution whose rank is maximal for primal and dual, respec-
</p>
<p>tively. Thus, a rank reduction method sometimes is necessary to achieve this goal.
</p>
<p>For linear programming in the standard form, it is known that if there is an optimal
</p>
<p>solution, then there is an optimal basic solution x&lowast; whose positive entries have at
most m many. Is there a similar structural fact for semidefinite programming? In
</p>
<p>deed, we have
</p>
<p>Proposition 2. If there is an optimal solution for SDP, then there is an optimal solution of
</p>
<p>SDP whose rank r satisfies
r(r+1)
</p>
<p>2
&le; m.
</p>
<p>The proposition resembles the linear programming fundamental theorem of
</p>
<p>Carathéodory in Sect. 2.4. We now give a sketch of similar constructive proof, as
</p>
<p>well as several other rank-reduction methods.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.5 Complementarity and Solution Rank of SDP 167
</p>
<p>Null-Space Rank Reduction
</p>
<p>Let X&lowast; be an optimal solution of SDP with rank r. If r(r + 1)/2 &gt; m, we orthonor-
mally factorize X&lowast;
</p>
<p>X&lowast; = (V&lowast;)TV&lowast;, V&lowast; &isin; Er&times;n.
Then we consider a related SDP problem
</p>
<p>minimize V&lowast;C(V&lowast;)T &bull; U
subject to V&lowast;Ai(V&lowast;)T &bull; U = bi, i = 1, . . . ,m
</p>
<p>U &isin; Sr+.
(6.12)
</p>
<p>Note that, for any feasible solution of (6.12) one can construct a feasible solution
</p>
<p>for original SDP using
</p>
<p>X(U) = (V&lowast;)TUV&lowast; and C &bull; X(U) = V&lowast;C(V&lowast;)T &bull; U.
</p>
<p>Thus, the minimal value of (6.12) is also z&lowast;, and in particular U = I (the identity
matrix) is an minimizer of (6.12), since
</p>
<p>V&lowast;C(V&lowast;)T &bull; I = C &bull; (V&lowast;)TV&lowast; = C &bull; X&lowast; = z&lowast;.
</p>
<p>Also, one can show that any feasible solution U of (6.12) is its minimizer, so that
</p>
<p>X(U) is a minimizer of original SDP.
</p>
<p>Consider the system of homogeneous linear equations:
</p>
<p>V&lowast;Ai(V
&lowast;)T &bull; W = 0, i = 1, . . . ,m.
</p>
<p>where W &isin; Sr (i.e., a r &times; r symmetric matrices that does not need to be semidef-
inite). This system has r(r + 1)/2 real variables and m equations. Thus, as long as
</p>
<p>r(r + 1)/2 &gt; m, we must be able to find a symmetric matrix W � 0 to satisfy all
</p>
<p>the m equations. Without loss of generality, let W be either indefinite or negative
</p>
<p>semidefinite (if it is positive semidefinite, we take &minus;W as W), that is, W have at
least one negative eigenvalue. Then we consider
</p>
<p>U(α) = I + αW.
</p>
<p>Choosing a α&lowast; sufficiently large such that U(α&lowast;) � 0 and it has at least one 0 eigen-
value (or rankU(α&lowast;) &lt; r). Note that
</p>
<p>V&lowast;Ai(V
&lowast;)T &bull; U(α&lowast;) = V&lowast;Ai(V&lowast;)T &bull; (I + α&lowast;W) = V&lowast;Ai(V&lowast;)T &bull; I = bi, i = 1, . . . ,m.
</p>
<p>That is, U(α&lowast;) is feasible and also optimal for (6.12). Thus, X(U(α&lowast;)) is a new min-
imizer for the original SDP, and its rank is strictly less than r. This process can be
</p>
<p>repeated till the system of homogeneous linear equations has only all-zero solution,
</p>
<p>which is necessary when r(r + 1)/2 &le; m. Such a solution rank reduction procedure
is called the Null-space reduction, which is deterministic.</p>
<p/>
</div>
<div class="page"><p/>
<p>168 6 Conic Linear Programming
</p>
<p>To see an application of Proposition 2, consider a general quadratic minimization
</p>
<p>with sphere constraint
</p>
<p>z&lowast; &equiv; minimize xTQx + 2cTx
subject to |x|2 = 1, x &isin; En,
</p>
<p>where Q is general. The problem has an SDP relaxation:
</p>
<p>zSDP &equiv; maximize
[
</p>
<p>Q c
</p>
<p>cT 0
</p>
<p>]
</p>
<p>&bull; Y
</p>
<p>subject to
</p>
<p>[
</p>
<p>I 0
</p>
<p>0T 0
</p>
<p>]
</p>
<p>&bull; Y = 1,
[
</p>
<p>0 0
</p>
<p>0T 1
</p>
<p>]
</p>
<p>&bull; Y = 1,
</p>
<p>Y &isin; Sn+1+ .
</p>
<p>Note that the relaxation and its dual both have interior so that the strong duality
</p>
<p>theorem holds, and it must have a rank-1 optimal SDP solution because m = 2. But
</p>
<p>a rank-1 optimal SDP solution would be optimal to the original quadratic minimiza-
</p>
<p>tion with sphere constraint. Thus, we must have z&lowast; = zSDP.
</p>
<p>Gaussian Projection Reduction
</p>
<p>There is also a randomized procedure to produce an approximate SDP solution with
</p>
<p>a desired low rank d. Again, let X&lowast; be an optimal solution of SDP with rank r &gt; d
and we factorize X&lowast; as
</p>
<p>X&lowast; = (V&lowast;)TV&lowast;, V&lowast; &isin; Er&times;n.
</p>
<p>We then generate i.i.d. Gaussian random variables ξ
j
</p>
<p>i
with mean 0 and variance 1/d,
</p>
<p>i = 1, . . . , r; j = 1, . . . , d, and form random vectors ξ j = (ξ
j
</p>
<p>1
; . . . ; ξ
</p>
<p>j
r ), j = 1, . . . , d.
</p>
<p>Finally, we let
</p>
<p>X̂ = (V&lowast;)T
⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>d
&sum;
</p>
<p>j=1
</p>
<p>ξ j(ξ j)T
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>V&lowast;.
</p>
<p>Note that the rank of X̂ is d and
</p>
<p>E(X̂) = (V&lowast;)TE
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>d
&sum;
</p>
<p>j=1
</p>
<p>ξ j(ξ j)T
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>V&lowast; = (V&lowast;)T IV&lowast; = X&lowast;.
</p>
<p>One can further show that X̂ would be a good rank-d approximate SDP solution in
</p>
<p>many cases.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.5 Complementarity and Solution Rank of SDP 169
</p>
<p>Randomized Binary Reduction
</p>
<p>As discussed in the binary QP optimization, we like to produce a vector x where
</p>
<p>each entry is either 1 or &minus;1. A procedure to achieve this is as follows. Let X&lowast; be any
optimal solution of SDP and we factorize X&lowast; as
</p>
<p>X&lowast; = (V&lowast;)TV&lowast;, V&lowast; &isin; En&times;n.
</p>
<p>Then, we generate a random n-dimensional vector ξ where each entry is a i.i.d. Gaus-
</p>
<p>sian random variable with mean 0 and variance 1. Then we let
</p>
<p>x̂ = sign((V&lowast;)Tξ)
</p>
<p>where
</p>
<p>sign(x) =
</p>
<p>{
</p>
<p>1 if x &ge; 0
&minus;1 otherwise.
</p>
<p>It was proved by Sheppard [228]:
</p>
<p>E[x̂i x̂ j] =
2
</p>
<p>π
arcsin(X&lowast;i j), i, j = 1, 2, . . . , n.
</p>
<p>Obviously, each entry of x̂ is either 1 or &minus;1.
One can further show x̂ would be a good approximate solution to the origi-
</p>
<p>nal binary QP. Let us consider the (homogeneous) binary quadratic maximization
</p>
<p>problem
</p>
<p>z&lowast; := maximize xTQx
</p>
<p>subject to x j = {1, &minus;1}, for all j = 1, . . . , n,
</p>
<p>where we assume Q is positive semidefinite. Then, the SDP relaxation would be
</p>
<p>zSDP := maximize Q &bull; X
subject to I j &bull; X = 1, for all j = 1, . . . , n,
</p>
<p>X &isin; Sn+;
</p>
<p>and let X&lowast; be any optimal solution, from which we produced a random binary vector
x̂. Let us evaluate the expected objective value
</p>
<p>E(x̂TQx̂) = E(Q &bull; x̂x̂T ) = Q &bull; E(x̂x̂T ) = Q &bull; 2
π
</p>
<p>arcsin[X&lowast;] =
2
</p>
<p>π
(Q &bull; arcsin[X&lowast;]),
</p>
<p>where arcsin[X&lowast;] &isin; Sn whose (i, j) the entry equals arcsin(X&lowast;
i j
</p>
<p>). One can further
</p>
<p>show
</p>
<p>arcsin[X&lowast;] &minus; X&lowast; � 0</p>
<p/>
</div>
<div class="page"><p/>
<p>170 6 Conic Linear Programming
</p>
<p>so that (from Q � 0)
</p>
<p>Q &bull; arcsin[X&lowast;] &ge; Q &bull; X&lowast; = zSDP &ge; z&lowast;,
</p>
<p>that is, the expected objective value of x̂ is no less than factor 2
π
</p>
<p>of the maximal
</p>
<p>value of the binary QP.
</p>
<p>The randomized binary reduction can be extended to quadratic optimization with
</p>
<p>simple bound constraints such as x2
j
&le; 1.
</p>
<p>6.6 Interior-Point Algorithms for Conic Linear Programming
</p>
<p>Since (CLP) is a convex minimization problem, many optimization algorithms are
</p>
<p>applicable for solving it. However, the most natural conic linear programming algo-
</p>
<p>rithm seems to be an extension of the interior-point linear programming algorithm
</p>
<p>described in Chap. 5. We describe what it is now.
</p>
<p>To develop efficient interior-point algorithms, the key is to find a suitable barrier
</p>
<p>or potential function. There is a general theory on selection of barrier functions for
</p>
<p>(CLP), depending on the convex cone involved. We present few for the convex cones
</p>
<p>listed in Example 1.
</p>
<p>Example 1. The following are barrier function for each of the convex cones.
</p>
<p>&bull; The n-dimensional non-negative orthant En+:
</p>
<p>B(x) = &minus;
n
&sum;
</p>
<p>j=1
</p>
<p>log(x j).
</p>
<p>&bull; The n-dimensional semidefinite cone Sn+:
</p>
<p>B(X) = &minus; log(det X).
</p>
<p>&bull; The (n + 1)-dimensional second-order cone {(u; x) : u &ge; |x|}:
</p>
<p>B(u; x) = &minus; log(u2 &minus; |x|2).
</p>
<p>In the rest of the section, we devote our discussion on solving (SDP). Similar to
</p>
<p>LP, we consider (SDP) with the barrier function added in the objective:
</p>
<p>(S DPB) minimize C &bull; X &minus; μ log det(X)
subject to X &isin;
</p>
<p>◦
F p,
</p>
<p>or (SDD) with the barrier function added in the objective:
</p>
<p>(S DDB) maximize yTb + μ log det(S)
</p>
<p>subject to (y, S) &isin;
◦
F d,</p>
<p/>
</div>
<div class="page"><p/>
<p>6.6 Interior-Point Algorithms for Conic Linear Programming 171
</p>
<p>where again μ &gt; 0 is called the barrier weight parameter. For a given μ, the mini-
</p>
<p>mizers of (SDPB) and (SDDB) satisfy conditions:
</p>
<p>XS = μI
</p>
<p>AX = b
ATy + S = C
</p>
<p>X ≻ 0, S ≻ 0
(6.13)
</p>
<p>Since
</p>
<p>μ =
trace(XS)
</p>
<p>n
=
</p>
<p>X &bull; S
n
</p>
<p>=
C &bull; X &minus; yTb
</p>
<p>n
,
</p>
<p>so that μ equals the average of complementarity or duality gap. And, these minimiz-
</p>
<p>ers, denoted by (X(μ), y(μ), S(μ)), form the central path of SDP for mu &isin; (0,&infin;). It is
known that when μ &rarr; 0, (X(μ), y(μ), S(μ)) tends to an optimal solution pair whose
rank is maximal (Exercise 11).
</p>
<p>We can also extend the primal-dual potential function from LP to SDP as a
</p>
<p>descent merit function:
</p>
<p>ψn+ρ(X, S) = (n + ρ) log(X &bull; S) &minus; log(det(X) &middot; det(S))
</p>
<p>where ρ &ge; 0. Note that if X and S are diagonal matrices, these definitions reduce to
those for linear programming.
</p>
<p>Once we have an interior feasible point (X, y, S), we can generate a new iterate
</p>
<p>(X+, y+, S+) by solving for (Dx, dy, Ds) from the primal-dual system of linear
</p>
<p>equations
</p>
<p>D&minus;1DxD
&minus;1 + Ds =
</p>
<p>n
</p>
<p>n + ρ
μX&minus;1 &minus; S,
</p>
<p>Ai &bull; Dx = 0, for all i, (6.14)
&sum;m
</p>
<p>i
(dy)iAi + Ds = 0,
</p>
<p>where D is the (scaling) matrix
</p>
<p>D = X
1
2 (X
</p>
<p>1
2 SX
</p>
<p>1
2 )&minus;
</p>
<p>1
2 X
</p>
<p>1
2
</p>
<p>and μ = X &bull; S/n. Then one assigns X+ = X+ αDx, y+ = y+ αdy, and S+ = s + αDs
for a step size α &gt; 0. Furthermore, it can be shown that there exists a step size α = ᾱ
</p>
<p>such that
</p>
<p>ψn+ρ(X
+, S+) &minus; ψn+ρ(X, S) &le; &minus;δ
</p>
<p>for a constant δ &gt; 0.2.
</p>
<p>We outline the algorithm here
</p>
<p>Step 1. Given (X0, y0, S0) &isin;
◦
F . Set ρ &ge;
</p>
<p>&radic;
n and k := 0.
</p>
<p>Step 2. Set (X, S) = (Xk, Sk) and compute (Dx, dy,Ds) from (6.14).</p>
<p/>
</div>
<div class="page"><p/>
<p>172 6 Conic Linear Programming
</p>
<p>Step 3. Let Xk+1 = Xk + ᾱDx, y
k+1 = yk + ᾱdy, and S
</p>
<p>k+1 = Sk + ᾱDs, where
</p>
<p>ᾱ = arg min
α&ge;0
</p>
<p>ψn+ρ(X
k + αDx, S
</p>
<p>k + αDs).
</p>
<p>Step 4. Let k := k + 1. If X
k&bull;Sk
</p>
<p>X0&bull;S0 &le; ǫ, Stop. Otherwise return to Step 2.
Theorem 3. Let ψn+ρ(X
</p>
<p>0 , S0) &le; ρ log(X0 &bull; S0) + n log n. Then, the algorithm terminates in
at most O(ρ log(n/ǫ) iterations.
</p>
<p>Initialization: The HSD Algorithm
</p>
<p>The linear programming Homogeneous Self-Dual Algorithm is also extendable to
</p>
<p>conic linear programming. Consider the minimization problem Homogeneous self-
</p>
<p>dual algorithm! for conic linear programming
</p>
<p>(HSDCLP) min (n + 1)θ
</p>
<p>s.t. AX &minus;bτ +b̄θ = 0,
&minus;ATy +Cτ &minus;C̄θ = S &isin; K&lowast;,
</p>
<p>bTy &minus;C &bull; X +z̄θ = κ &ge; 0,
&minus;b̄Ty +C̄ &bull; X &minus;z̄τ = &minus;(n + 1),
</p>
<p>y free, X &isin; K, τ &ge; 0, θ free,
</p>
<p>where
</p>
<p>b̄ = b &minus;AX0, C̄ = C &minus; S0, z̄ = C &bull; X0 + 1
Here X0 and S0 are any pair of interior points in the interior of K and K&lowast; such
that they form a central path point with μ = 1. Note that X0 and S0 don&rsquo;t need to
</p>
<p>satisfy other equality constraint, so that they can be easily identified. For examples,
</p>
<p>x0 = y0 = 1 for the nonnegative orthant cone; x0 = y0 = (1; 0) for the p-order cone;
</p>
<p>and X0 = X0 = I for the semidefinite cone.
</p>
<p>Let F be the set of all feasible points (y,X &isin; K, τ &ge; 0, θ, S &isin; K&lowast;, κ &ge; 0). Then
◦
F
</p>
<p>is the set of interior feasible points (y,X &isin;
◦
K, τ &gt; 0, θ, S &isin;
</p>
<p>◦
K
</p>
<p>&lowast;
, κ &gt; 0).
</p>
<p>Theorem 4. Consider the conic optimization (HSDCLP).
</p>
<p>i) (HSDCLP) is self-dual, that is, its dual has an identical form of (HSDCLP).
</p>
<p>ii) (HSDCLP) has an optimal solution and its optimal solution set is bounded.
</p>
<p>iii) (HSDCLP) has an interior feasible point
</p>
<p>y = 0, X = X0, τ = 1, θ = 1, S = S0, κ = 1.
</p>
<p>iv) For any feasible point (y,X, τ, θ, S, κ) &isin; F
</p>
<p>S0 &bull; X + X0 &bull; S + τ + κ &minus; (n + 1)θ = (n + 1),
</p>
<p>and
</p>
<p>X &bull; S + τκ = (n + 1)θ.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.7 Summary 173
</p>
<p>v) The optimal objective value of (HSDCLP) is zero, that is, any optimal solution of
</p>
<p>(HSDCLP) has
</p>
<p>X&lowast; &bull; S&lowast; + τ&lowast;κ&lowast; = (n + 1)θ&lowast; = 0.
</p>
<p>Now we are ready to apply the interior-point algorithm, starting from a available
</p>
<p>initial interior-point feasible solution, to solve (HSDCLP). The question is: how is
</p>
<p>an optimal solution of (HSDCLP) related to optimal solutions of original (CLP) and
</p>
<p>(CLD)? We present the next theorem, and leave this proof as an exercise.
</p>
<p>Theorem 5. Let (y&lowast; ,X&lowast;, τ&lowast;, θ&lowast; = 0, S&lowast;, κ&lowast;) be a (maximal rank) optimal solution of (HSD-
CLP) (as it is typically computed by interior-point algorithms).
</p>
<p>i) (CLP) and (CLD) have an optimal solution pair if and only if τ&lowast; &gt; 0. In this case,
X&lowast;/τ&lowast; is an optimal solution for (CLP) and (y&lowast;/τ&lowast;, S&lowast;/τ&lowast;) is an optimal solution for
(CLD).
</p>
<p>ii) (CLP) or (CLD) has an infeasibility certificate if and only if κ&lowast; &gt; 0. In this case, X&lowast;/κ&lowast;
</p>
<p>or S&lowast;/κ&lowast; or both are certificates for proving infeasibility; see Farkas&rsquo; lemma for CLP.
iii) For all other cases, τ&lowast; = κ&lowast; = 0.
</p>
<p>6.7 Summary
</p>
<p>A relatively new class of mathematical programming problems, Conic linear pro-
</p>
<p>gramming (hereafter CLP), is a natural extension of Linear programming that is a
</p>
<p>central decision model in Management Science and Operations Research. In CLP,
</p>
<p>the unknown is a vector or matrix in a closed convex cone while its entries are also
</p>
<p>restricted by some linear equalities and/or inequalities.
</p>
<p>One of cones is the semidefinite cone, that is, the set of all symmetric positive
</p>
<p>semidefinite matrices in a given dimension. There is a variety of interesting and
</p>
<p>important practical problems that can be naturally cast in this form. Because many
</p>
<p>problems which appear nonlinear (such as quadratic problems) become essentially
</p>
<p>linear in semidefinite form. We have described some of these applications and se-
</p>
<p>lected results in Combinatory Optimization, Robust Optimization, and Engineering
</p>
<p>Sensor Network. We have also illustrated some analyses to show why CLP is an
</p>
<p>effective model to tackle these difficult optimization problems.
</p>
<p>We present fundamental theorems underlying conic linear programming. These
</p>
<p>theorems include Farkas&rsquo; lemma, weak and strong dualities, and solution rank struc-
</p>
<p>ture. We show the common features and differences of these theorems between LP
</p>
<p>and CLP.
</p>
<p>The efficient interior-point algorithms for linear programming can be extended
</p>
<p>to solving these problems as well. We describe these extensions applied to gen-
</p>
<p>eral conic programming problems. These algorithms closely parallel those for linear
</p>
<p>programming. There is again a central path and potential functions, and Newton&rsquo;s
</p>
<p>method is a good way to follow the path or reduce the potential function. The homo-
</p>
<p>geneous and self-dual algorithm, which is popularly used for linear programming,
</p>
<p>is also extended to CLP.</p>
<p/>
</div>
<div class="page"><p/>
<p>174 6 Conic Linear Programming
</p>
<p>6.8 Exercises
</p>
<p>1. Prove that
</p>
<p>i) The dual cone of En+ is itself.
</p>
<p>ii) The dual cone of Sn+ is itself.
iii) The dual cone of p-order cone is the q-order cone where 1
</p>
<p>p
+ 1
</p>
<p>q
= 1 and
</p>
<p>1 &le; p &le; &infin;.
2. When both K1 and K2 are closed convex cones. Show
</p>
<p>i) (K&lowast;1)
&lowast; = K1.
</p>
<p>ii) K1 &sub; K2 =&rArr; K&lowast;2 &sub; K&lowast;1 .
iii) (K1 &oplus; K2)&lowast; = K&lowast;1 &oplus; K&lowast;2 .
iv) (K1 + K2)
</p>
<p>&lowast; = K&lowast;
1
&cap; K&lowast;
</p>
<p>2
.
</p>
<p>v) (K1 &cap; K2)&lowast; = K&lowast;1 + K&lowast;2 .
Note: by definition S + T = {s + t : s &isin; S , t &isin; T }.
</p>
<p>3. Prove the following:
</p>
<p>i) Theorem 1.
</p>
<p>ii) Proposition 1.
</p>
<p>iii) Let X &isin;
◦
K and Y &isin;
</p>
<p>◦
K&lowast;. Then X &bull; Y &gt; 0.
</p>
<p>4. Guess an optimal solution and the optimal objective value of each instance of
</p>
<p>Example 1.
</p>
<p>5. Prove the second statement of Theorem 2.
</p>
<p>6. Verify the weak duality theorem of the three CLP instances in Example 1 in
</p>
<p>Sect. 6.2 and Example 1 in Sect. 6.4.
</p>
<p>7. Consider the SDP relaxation of the sensor network localization problem with
</p>
<p>four sensors:
</p>
<p>(ei &minus; e j)(ei &minus; e j)T &bull; X = 1, &forall;i &lt; j = 1, 2, 3, 4,
X &isin; S4+,
</p>
<p>in which m = 6. Show that the SDP problem has the solution with rank 3, which
</p>
<p>reaches the bound of Proposition 2.
</p>
<p>8. Let A and B be two symmetric and positive semidefinite matrices. Prove that
</p>
<p>A &bull; B &ge; 0, and A &bull; B = 0 implies AB = 0.
9. Let X and S both be positive definite. Prove that
</p>
<p>n log(X &bull; S) &minus; log(det(X) &middot; det(S)) &ge; n log n.
</p>
<p>10. Consider a SDP and the potential level set
</p>
<p>Ψ(δ) = {(X, y, S) &isin;
◦
F : ψn+ρ(X, S) &le; δ}.</p>
<p/>
</div>
<div class="page"><p/>
<p>References 175
</p>
<p>Prove that
</p>
<p>Ψ(δ1) &sub; Ψ(δ2) if δ1 &le; δ2,
and for every δ, Ψ(δ) is bounded and its closure Ψ(δ) has non-empty intersec-
</p>
<p>tion with the SDP solution set.
</p>
<p>11. Let both (SDP) and (SDD) have interior feasible points. Then for any 0 &lt; μ &lt; &infin;,
the central path point (X(μ), y(μ), S(μ)) exists and is unique. Moreover,
</p>
<p>i) the central path point (X(μ), y(μ), S(μ)) is bounded where 0 &lt; μ &le; μ0 for
any given 0 &lt; μ0 &lt; &infin;.
</p>
<p>ii) For 0 &lt; μ&prime; &lt; μ,
</p>
<p>C &bull; X(μ&prime;) &lt; C &bull; X(μ) and bTy(μ&prime;) &gt; bTy(μ)
</p>
<p>if X(μ) � X(μ&prime;) and y(μ) � y(μ&prime;).
iii) (X(μ), y(μ), S(μ)) converges to an optimal solution pair for (SDP) and
</p>
<p>(SDD), and the rank of the limit of X(μ) is maximal among all optimal
</p>
<p>solutions of (SDP) and the rank of the limit S(μ) is maximal among all
</p>
<p>optimal solutions of (SDD).
</p>
<p>12. Prove the logarithmic approximation lemma for SDP. Let D &isin; Sn and |D|&infin; &lt; 1.
Then,
</p>
<p>trace(D) &ge; log det(I + D) &ge; trace(D) &minus; |D|
2
</p>
<p>2(1 &minus; |D|&infin;)
.
</p>
<p>13. Let V &isin;
◦
S
n
</p>
<p>+ and ρ &ge;
&radic;
n. Then,
</p>
<p>|V&minus;1/2 &minus; n+ρ
I&bull;V V
</p>
<p>1/2|
|V&minus;1/2|&infin;
</p>
<p>&ge;
&radic;
</p>
<p>3/4.
</p>
<p>14. Prove both Theorems 4 and 5.
</p>
<p>References
</p>
<p>6.1 Most of the materials presented can be found from convex analysis, such as
</p>
<p>Rockeafellar [219].
</p>
<p>6.2 Semidefinite relaxations have appeared in relation to relaxations discrete opti-
</p>
<p>mization problems. In Lovasz and Shrijver [159], a &ldquo;lifting&rdquo; procedure is pre-
</p>
<p>sented to obtain a problem in &real;n2 ; and then the problem is projected back to
obtain tighter inequalities; see also Balas et al. [12]. Then, there have been
</p>
<p>several remarkable results of SDP relaxations for combinatorial optimization.
</p>
<p>The binary QP, a generalized Max-Cut problem, was studied by Goemans and
</p>
<p>Williamson [G8] and Nesterov [189]. Other SDP relaxations can be found in
</p>
<p>the survey by Luo et al. [171] and references therein. More CLP applications
</p>
<p>can be found in Boyd et al [B22], Vandenberghe and Boyd [V2], and Lobo,
</p>
<p>Vandenberghe and Boyd [156], Lasserre [150], Parrilo [204], etc.</p>
<p/>
</div>
<div class="page"><p/>
<p>176 6 Conic Linear Programming
</p>
<p>The sensor localization problem described here is due to Biswas and Ye [B17].
</p>
<p>Note that we can view the Sensor Network Localization problem as a Graph
</p>
<p>Realization or Embedding problem in Euclidean spaces, see So and Ye [231]
</p>
<p>and references therein; and it is related to the Euclidean Distance Matrix Com-
</p>
<p>pletion Problems, see Alfakih et al. [3] and Laurent [151].
</p>
<p>6.3 Farkas&rsquo; lemma for conic linear constraints are closely linked to convex analysis
</p>
<p>(i.e, Rockeafellar [219]) and the CLP duality theorems commented next.
</p>
<p>6.4 The conic formulation of the Euclidean facility location problem was due to
</p>
<p>Xue and Ye [264]. For discussion of Schur complements see Boyd and Vander-
</p>
<p>berghe [B23]. Robust optimization models using SDP can be found in Ben-Tal
</p>
<p>and Nemirovski [26] and Goldfarb and Iyengar [112], and etc. The SDP duality
</p>
<p>theory was studied by Barvinok [16], Nesterov and Nemirovskii [N2], Ramana
</p>
<p>[214], Ramana e al. [215], etc. The SDP example with a duality gap was con-
</p>
<p>structed by R. Freund (private communication).
</p>
<p>6.5 Complementarity and rank. The exact rank theorem described here is due to
</p>
<p>Pataki [205], also see Barvinok [15]. A analysis of the Gaussian projection was
</p>
<p>presented by So et al. [232] which can be sees as a generalization of the John-
</p>
<p>son and Lindenstrauss theorem [137]. The expectation of the randomized binary
</p>
<p>reduction is due to Sheppard [228] in 1900, and it was extensively used in Goe-
</p>
<p>mans and Williamson [G8] and Nesterov [189], Ye [265], and Bertsimas and
</p>
<p>Ye, [31].
</p>
<p>6.6 In interior-point algorithms, the search direction (Dx, dy,Ds) can be determined
</p>
<p>by Newton&rsquo;s method with three different scalings: primal, dual and primal-dual.
</p>
<p>A primal-scaling (potential reduction) algorithm for semidefinite programming
</p>
<p>is due to Alizadeh [A4, A3] where Yinyu Ye &ldquo;suggested studying the primal-
</p>
<p>dual potential function for this problem&rdquo; and &ldquo;looking at symmetric preserving
</p>
<p>scalings of the form X&minus;1/2
0
</p>
<p>XX
&minus;1/2
0
</p>
<p>&rdquo;, and to Nesterov and Nemirovskii [N2]. A
</p>
<p>dual-scaling algorithm was developed by Benson et al. [25] which exploits the
</p>
<p>sparse structure of the dual SDP. The primal-dual SDP algorithm described here
</p>
<p>is due to Nesterov and Todd [N3] and references therein.
</p>
<p>Efficient interior-point algorithms are also developed for optimization over the
</p>
<p>second-order cone; see Nesterov and Nemirovskii [N2] and Xue and Ye [264].
</p>
<p>These algorithms have established the best approximation complexity results
</p>
<p>for certain combinatorial location problems.
</p>
<p>The homogeneous and self-dual initialization model was originally developed
</p>
<p>by Ye, Todd and Mizuno for LP [Y2], and for SDP by de Klerk et al. [72], Luo
</p>
<p>et al. [L18], and Nesterov et al. [191], and it became the foundational algorithm
</p>
<p>implemented in Sturm [S11] and Andersen [6].</p>
<p/>
</div>
<div class="page"><p/>
<p>Part II
</p>
<p>Unconstrained Problems</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 7
</p>
<p>Basic Properties of Solutions and Algorithms
</p>
<p>In this chapter we consider optimization problems of the form
</p>
<p>minimize f (x) (7.1)
</p>
<p>subject to x &isin; Ω,
</p>
<p>where f is a real-valued function and Ω, the feasible set, is a subset of En.
</p>
<p>Throughout most of the chapter attention is restricted to the case where Ω = En,
</p>
<p>corresponding to the completely unconstrained case, but sometimes we consider
</p>
<p>cases where Ω is some particularly simple subset of En.
</p>
<p>The first and third sections of the chapter characterize the first- and second-order
</p>
<p>conditions that must hold at a solution point of (7.1). These conditions are simply
</p>
<p>extensions to En of the well-known derivative conditions for a function of a single
</p>
<p>variable that hold at a maximum or a minimum point. The fourth and fifth sections
</p>
<p>of the chapter introduce the important classes of convex and concave functions that
</p>
<p>provide zeroth-order conditions as well as a natural formulation for a global theory
</p>
<p>of optimization and provide geometric interpretations of the derivative conditions
</p>
<p>derived in the first two sections.
</p>
<p>The final sections of the chapter are devoted to basic convergence characteristics
</p>
<p>of algorithms. Although this material is not exclusively applicable to optimization
</p>
<p>problems but applies to general iterative algorithms for solving other problems as
</p>
<p>well, it can be regarded as a fundamental prerequisite for a modern treatment of
</p>
<p>optimization techniques. Two essential questions are addressed concerning itera-
</p>
<p>tive algorithms. The first question, which is qualitative in nature, is whether a given
</p>
<p>algorithm in some sense yields, at least in the limit, a solution to the original prob-
</p>
<p>lem. This question is treated in Sect. 7.6, and conditions sufficient to guarantee
</p>
<p>appropriate convergence are established. The second question, the more quantita-
</p>
<p>tive one, is related to how fast the algorithm converges to a solution. This question
</p>
<p>is defined more precisely in Sect. 7.7. Several special types of convergence, which
</p>
<p>arise frequently in the development of algorithms for optimization, are explored.
</p>
<p>&copy; Springer International Publishing Switzerland 2016
</p>
<p>D.G. Luenberger, Y. Ye, Linear and Nonlinear Programming, International
Series in Operations Research &amp; Management Science 228,
DOI 10.1007/978-3-319-18842-3 7
</p>
<p>179</p>
<p/>
</div>
<div class="page"><p/>
<p>180 7 Basic Properties of Solutions and Algorithms
</p>
<p>7.1 First-Order Necessary Conditions
</p>
<p>Perhaps the first question that arises in the study of the minimization problem (7.1)
</p>
<p>is whether a solution exists. The main result that can be used to address this issue is
</p>
<p>the theorem of Weierstras, which states that if f is continuous and Ω is compact, a
</p>
<p>solution exists (see Appendix A.6). This is a valuable result that should be kept in
</p>
<p>mind throughout our development; however, our primary concern is with character-
</p>
<p>izing solution points and devising effective methods for finding them.
</p>
<p>In an investigation of the general problem (7.1) we distinguish two kinds of
</p>
<p>solution points: local minimum points, and global minimum points.
</p>
<p>Definition. A point x&lowast; &isin; Ω is said to be a relative minimum point or a local minimum point
of f over Ω if there is an ε &gt; 0 such that f (x) � f (x&lowast;) for all x &isin; Ω within a distance ε of
x&lowast; (that is, x &isin; Ω and |x &minus; x&lowast;| &lt; ε). If f (x) &gt; f (x&lowast;) for all x &isin; Ω, x � x&lowast;, within a distance ε
of x&lowast;, then x&lowast; is said to be a strict relative minimum point of f over Ω.
</p>
<p>Definition. A point x&lowast; &isin; Ω is said to be a global minimum point of f over Ω if f (x) � f (x&lowast;)
for all x &isin; Ω. If f (x) &gt; f (x&lowast;) for all x &isin; Ω, x � x&lowast;, then x&lowast; is said to be a strict global
minimum point of f over Ω.
</p>
<p>In formulating and attacking problem (7.1) we are, by definition, explicitly ask-
</p>
<p>ing for a global minimum point of f over the set Ω. Practical reality, however, both
</p>
<p>from the theoretical and computational viewpoint, dictates that we must in many
</p>
<p>circumstances be content with a relative minimum point. In deriving necessary con-
</p>
<p>ditions based on the differential calculus, for instance, or when searching for the
</p>
<p>minimum point by a convergent stepwise procedure, comparisons of the values of
</p>
<p>nearby points is all that is possible and attention focuses on relative minimum points.
</p>
<p>Global conditions and global solutions can, as a rule, only be found if the problem
</p>
<p>possesses certain convexity properties that essentially guarantee that any relative
</p>
<p>minimum is a global minimum. Thus, in formulating and attacking problem (7.1)
</p>
<p>we shall, by the dictates of practicality, usually consider, implicitly, that we are
</p>
<p>asking for a relative minimum point. If appropriate conditions hold, this will also be
</p>
<p>a global minimum point.
</p>
<p>Feasible Directions
</p>
<p>To derive necessary conditions satisfied by a relative minimum point x&lowast;, the basic
idea is to consider movement away from the point in some given direction. Along
</p>
<p>any given direction the objective function can be regarded as a function of a single
</p>
<p>variable, the parameter defining movement in this direction, and hence the ordinary
</p>
<p>calculus of a single variable is applicable. Thus given x &isin; Ω we are motivated to say
that a vector d is a feasible direction at x if there is an ᾱ &gt; 0 such that x + αd &isin; Ω
for all α, 0 � α � ᾱ. With this simple concept we can state some simple conditions
</p>
<p>satisfied by relative minimum points.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.1 First-Order Necessary Conditions 181
</p>
<p>Proposition 1 (First-Order Necessary Conditions). LetΩ be a subset of En and let f &isin; C1
be a function on Ω. If x&lowast; is a relative minimum point of f over Ω, then for any d &isin; En that
is a feasible direction at x&lowast;, we have &nabla; f (x&lowast;)d � 0.
</p>
<p>Proof. For any α, 0 � α � ᾱ, the point x(α) = x&lowast; + αd &isin; Ω. For 0 � α � ᾱ define
the function g(α) = f (x(α)). Then g has a relative minimum at α = 0. A typical g is
</p>
<p>shown in Fig. 7.1. By the ordinary calculus we have
</p>
<p>g(α) &minus; g(0) = g&prime;(0)α + o(α), (7.2)
</p>
<p>where o(α) denotes terms that go to zero faster than α (see Appendix A). If g&prime;(0) &lt; 0
then, for sufficiently small values of α &gt; 0, the right side of (7.2) will be negative,
</p>
<p>and hence g(α) &minus; g(0) &lt; 0, which contradicts the minimal nature of g(0). Thus
g&prime;(0) = &nabla; f (x&lowast;)d � 0. �
</p>
<p>A very important special case is where x&lowast; is in the interior of Ω (as would be
the case if Ω = En). In this case there are feasible directions emanating in every
</p>
<p>direction from x&lowast;, and hence &nabla; f (x&lowast;)d � 0 for all d &isin; En. This implies &nabla; f (x&lowast;) = 0.
We state this important result as a corollary.
</p>
<p>Corollary (Unconstrained Case). Let Ω be a subset of En, and let f &isin; C1 be function&rsquo; on
Ω. If x&lowast; is a relative minimum point of f over Ω and if x&lowast; is an interior point of Ω, then
&nabla; f (x&lowast;) = 0.
</p>
<p>The necessary conditions in the pure unconstrained case lead to n equations
</p>
<p>(one for each component of &nabla; f ) in n unknowns (the components of x&lowast;), which in
many cases can be solved to determine the solution. In practice, however, as demon-
</p>
<p>strated in the following chapters, an optimization problem is solved directly without
</p>
<p>explicitly attempting to solve the equations arising from the necessary conditions.
</p>
<p>Nevertheless, these conditions form a foundation for the theory.
</p>
<p>Fig. 7.1 Construction for proof</p>
<p/>
</div>
<div class="page"><p/>
<p>182 7 Basic Properties of Solutions and Algorithms
</p>
<p>Example 1. Consider the problem
</p>
<p>minimize f (x1, x2) = x
2
1 &minus; x1x2 + x22 &minus; 3x2.
</p>
<p>There are no constraints, so Ω = E2. Setting the partial derivatives of f equal to zero
</p>
<p>yields the two equations
</p>
<p>2x1 &minus; x2 = 0
&minus;x1 + 2x2 = 3.
</p>
<p>These have the unique solution x1 = 1, x2 = 2, which is a global minimum point
</p>
<p>of f .
</p>
<p>Example 2. Consider the problem
</p>
<p>minimize f (x1, x2) = x
2
1 &minus; x1 + x2 + x1x2
</p>
<p>subject to x1 � 0, x2 � 0.
</p>
<p>This problem has a global minimum at x1 =
1
2
, x2 = 0. At this point
</p>
<p>&part; f
</p>
<p>&part;x1
= 2x1 &minus; 1 + x2 = 0
</p>
<p>&part; f
</p>
<p>&part;x2
= 1 + x1 =
</p>
<p>3
</p>
<p>2
.
</p>
<p>Thus, the partial derivatives do not both vanish at the solution, but since any
</p>
<p>feasible direction must have an x2 component greater than or equal to zero, we have
</p>
<p>&nabla; f (x&lowast;)d � 0 for all d &isin; E2 such that d is a feasible direction at the point (1/2, 0).
</p>
<p>7.2 Examples of Unconstrained Problems
</p>
<p>Unconstrained optimization problems occur in a variety of contexts, but most
</p>
<p>frequently when the problem formulation is simple. More complex formulations
</p>
<p>often involve explicit functional constraints. However, many problems with con-
</p>
<p>straints are frequently converted to unconstrained problems, such as using the barrier
</p>
<p>functions, e.g., the analytic center problem for (dual) linear programs. We present a
</p>
<p>few more examples here that should begin to indicate the wide scope to which the
</p>
<p>theory applies.
</p>
<p>Example 1 (Logistic Regression). Recall the classification problem where we have
</p>
<p>vectors ai &isin; Ed for i = 1, 2, . . . , n1 in a class, and vectors b j &isin; Ed for j =
1, 2, . . . , n2 not. Then we wish to find y &isin; Ed and a number β such that
</p>
<p>exp(aT
i
</p>
<p>y + β)
</p>
<p>1 + exp(aT
i
</p>
<p>y + β)</p>
<p/>
</div>
<div class="page"><p/>
<p>7.2 Examples of Unconstrained Problems 183
</p>
<p>is close to 1 for all i, and
</p>
<p>exp(bTj y + β)
</p>
<p>1 + exp(bTj y + β)
</p>
<p>is close to 0 for all j. The problem can be cast as a unconstrained optimization
</p>
<p>problem, called the max-likelihood,
</p>
<p>maximizey, β
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>&prod;
</p>
<p>i
</p>
<p>exp(aT
i
</p>
<p>y + β)
</p>
<p>1 + exp(aT
i
</p>
<p>y + β)
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>&prod;
</p>
<p>j
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>1 &minus;
exp(bTj y + β)
</p>
<p>1 + exp(bTj y + β)
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>,
</p>
<p>which can be also equivalently, using a logarithmic transformation, written as
</p>
<p>minimizey, β
&sum;
</p>
<p>i
</p>
<p>log
(
</p>
<p>1 + exp(&minus;aTi y &minus; β)
)
</p>
<p>+
&sum;
</p>
<p>j
</p>
<p>log
(
</p>
<p>1 + exp(bTj y + β)
)
</p>
<p>.
</p>
<p>Example 2 (Utility Maximization). A common problem in economic theory is the
</p>
<p>determination of the best way to combine various inputs in order to maximize a
</p>
<p>utility function f (x1, x2, . . . , xn) (in the monetary unit) of the amounts x j of the
</p>
<p>inputs, i = 1, 2, . . . , n. The unit prices of the inputs are p1, p2, . . . , pn. The pro-
</p>
<p>ducer wishing to maximize profit must solve the problem
</p>
<p>maximize f (x1, x2, . . . , xn) &minus; p1x1 &minus; p2x2 . . . &minus; pnxn.
</p>
<p>The first-order necessary conditions are that the partial derivatives with respect
</p>
<p>to the xi&rsquo;s each vanish. This leads directly to the n equations
</p>
<p>&part; f
</p>
<p>&part;xi
(x1, x2, . . . , xn) = pi, i = 1, 2, . . . , n.
</p>
<p>These equations can be interpreted as stating that, at the solution, the marginal value
</p>
<p>due to a small increase in the ith input must be equal to the price pi.
</p>
<p>Example 3 (Parametric Estimation). A common use of optimization is for the
</p>
<p>purpose of function approximation. Suppose, for example, that through an exper-
</p>
<p>iment the value of a function g is observed at m points, x1, x2, . . . , xm. Thus, values
</p>
<p>g(x1), g(x2), . . . , g(xm) are known. We wish to approximate the function by a poly-
</p>
<p>nomial
</p>
<p>h(x) = anx
n + an&minus;1x
</p>
<p>n&minus;1 + . . . + a0
</p>
<p>of degree n (or less), where n &lt; m. Corresponding to any choice of the approximating
</p>
<p>polynomial, there will be a set of errors εk = g(xk) &minus; h(xk). We define the best
approximation as the polynomial that minimizes the sum of the squares of these
</p>
<p>errors; that is, minimizes
</p>
<p>m
&sum;
</p>
<p>k=1
</p>
<p>(εk)
2.</p>
<p/>
</div>
<div class="page"><p/>
<p>184 7 Basic Properties of Solutions and Algorithms
</p>
<p>This in turn means that we minimize
</p>
<p>f (a) =
</p>
<p>m
&sum;
</p>
<p>k=1
</p>
<p>[
</p>
<p>g(xk) &minus;
(
</p>
<p>anx
n
k + an&minus;1x
</p>
<p>n&minus;1
k + . . . + a0
</p>
<p>)]2
</p>
<p>with respect to a = (a0, a1, . . . , an) to find the best coefficients. This is a quadratic
</p>
<p>expression in the coefficients a. To find a compact representation for this objective
</p>
<p>we define qi j =
m
&sum;
</p>
<p>k=1
(xk)
</p>
<p>i+ j, b j =
m
&sum;
</p>
<p>k=1
g(xk)(xk)
</p>
<p>j and c =
m
&sum;
</p>
<p>k=1
g(xk)
</p>
<p>2. Then after a bit of
</p>
<p>algebra it can be shown that
</p>
<p>f (a) = aTQa &minus; 2bTa + c
</p>
<p>where Q = [qi j], b = (b1, b2, . . . , bn+1).
</p>
<p>The first-order necessary conditions state that the gradient of f must vanish. This
</p>
<p>leads directly to the system of n + 1 equations
</p>
<p>Qa = b.
</p>
<p>These can be solved to determine a.
</p>
<p>Example 4 (Selection Problem). It is often necessary to select an assortment of fac-
</p>
<p>tors to meet a given set of requirements. An example is the problem faced by an
</p>
<p>electric utility when selecting its power-generating facilities. The level of power
</p>
<p>that the company must supply varies by time of the day, by day of the week, and
</p>
<p>by season. Its power-generating requirements are summarized by a curve, h(x), as
</p>
<p>shown in Fig. 7.2a, which shows the total hours in a year that a power level of at
</p>
<p>least x is required for each x. For convenience the curve is normalized so that the
</p>
<p>upper limit is unity.
</p>
<p>The power company may meet these requirements by installing generating equip-
</p>
<p>ment, such as (7.1) nuclear or (7.2) coal-fired, or by purchasing power from a central
</p>
<p>energy grid. Associated with type i(i = 1, 2) of generating equipment is a yearly
</p>
<p>unit capital cost bi and a unit operating cost ci. The unit price of power purchased
</p>
<p>from the grid is c3.
</p>
<p>Nuclear plants have a high capital cost and low operating cost, so they are used to
</p>
<p>supply a base load. Coal-fired plants are used for the intermediate level, and power
</p>
<p>is purchased directly only for peak demand periods. The requirements are satisfied
</p>
<p>as shown in Fig. 7.2b, where x1 and x2 denote the capacities of the nuclear and coal-
</p>
<p>fired plants, respectively. (For example, the nuclear power plant can be visualized
</p>
<p>as consisting of x1/∆ small generators of capacity ∆, where ∆ is small. The first
</p>
<p>such generator is on for about h(∆) hours, supplying ∆h(∆) units of energy; the
</p>
<p>next supplies ∆h(2∆) units, and so forth. The total energy supplied by the nuclear
</p>
<p>plant is thus the area shown.)</p>
<p/>
</div>
<div class="page"><p/>
<p>7.3 Second-Order Conditions 185
</p>
<p>The total cost is
</p>
<p>f (x1, x2) = b1x1 + b2x2 + c1
</p>
<p>&int; x1
</p>
<p>0
</p>
<p>h(x)dx
</p>
<p>+c2
</p>
<p>&int; x1+x2
</p>
<p>x1
</p>
<p>h(x)dx + c3
</p>
<p>&int; 1
</p>
<p>x1+x2
</p>
<p>h(x)dx,
</p>
<p>Fig. 7.2 (a) Power requirement curve; (b) x1 and x2 denote the capacities of the nuclear and coal-
fired plants, respectively
</p>
<p>and the company wishes to minimize this over the set defined by
</p>
<p>x1 � 0, x2 � 0, x1 + x2 � 1.
</p>
<p>Assuming that the solution is interior to the constraints, by setting the partial
</p>
<p>derivatives equal to zero, we obtain the two equations
</p>
<p>b1 + (c1 &minus; c2)h(x1) + (c2 &minus; c3)h(x1 + x2) = 0
b2 + (c2 &minus; c3)h(x1 + x2) = 0,
</p>
<p>which represent the necessary conditions.
</p>
<p>If x1 = 0, then the general necessary condition theorem shows that the first equal-
</p>
<p>ity could relax to � 0. Likewise, if x2 = 0, then the second equality could relax to
</p>
<p>� 0. The case x1 + x2 = 1 requires a bit more analysis (see Exercise 2).
</p>
<p>7.3 Second-Order Conditions
</p>
<p>The proof of Proposition 1 in Sect. 7.1 is based on making a first-order approx-
</p>
<p>imation to the function f in the neighborhood of the relative minimum point.
</p>
<p>Additional conditions can be obtained by considering higher-order approximations.</p>
<p/>
</div>
<div class="page"><p/>
<p>186 7 Basic Properties of Solutions and Algorithms
</p>
<p>The second-order conditions, which are defined in terms of the Hessian matrix
</p>
<p>&nabla;
2 f of second partial derivatives of f (see Appendix A), are of extreme theoret-
</p>
<p>ical importance and dominate much of the analysis presented in later chapters.
</p>
<p>Proposition 1 (Second-Order Necessary Conditions). Let Ω be a subset of En and let
</p>
<p>f &isin; C2 be a function on Ω. If x&lowast; is a relative minimum point of f over Ω, then for any
d &isin; En that is a feasible direction at x&lowast; we have
</p>
<p>i) &nabla; f (x&lowast;)d � 0 (7.3)
</p>
<p>ii) if &nabla; f (x&lowast;)d = 0, then dT&nabla;2 f (x&lowast;)d � 0. (7.4)
</p>
<p>Proof. The first condition is just Proposition 1, and the second applies only if
</p>
<p>&nabla; f (x&lowast;)d = 0. In this case, introducing x(α) = x&lowast; + αd and g(α) = f (x(α)) as
before, we have, in view of g&prime;(0) = 0,
</p>
<p>g(α) &minus; g(0) = 1
2
g&prime;&prime;(0)α2 + o(α2).
</p>
<p>If g&prime;&prime;(0) &lt; 0 the right side of the above equation is negative for sufficiently small α
which contradicts the relative minimum nature of g(0). Thus
</p>
<p>g&prime;&prime;(0) = dT&nabla;2 f (x&lowast;)d � 0. �
</p>
<p>Example 1. For the same problem as Example 2 of Sect. 7.1, we have for d =
</p>
<p>(d1, d2)
</p>
<p>&nabla; f (x&lowast;)d =
3
</p>
<p>2
d2.
</p>
<p>Thus condition (ii) of Proposition 1 applies only if d2 = 0. In that case we have
</p>
<p>dT&nabla;2 f (x&lowast;)d = 2d2
1
� 0, so condition (ii) is satisfied.
</p>
<p>Again of special interest is the case where the minimizing point is an interior
</p>
<p>point of Ω, as, for example, in the case of completely unconstrained problems.
</p>
<p>We then obtain the following classical result.
</p>
<p>Proposition 2 (Second-Order Necessary Conditions&mdash;Unconstrained Case). Let x&lowast; be
an interior point of the set Ω, and suppose x&lowast; is a relative minimum point over Ω of the
function f &isin; C2. Then
</p>
<p>i) &nabla; f (x&lowast;) = 0 (7.5)
</p>
<p>ii) for all d, dT&nabla;2 f (x&lowast;)d � 0. (7.6)
</p>
<p>For notational simplicity we often denote &nabla;2 f (x), the n &times; n matrix of the second
partial derivatives of f , the Hessian of f , by the alternative notation F(x). Condi-
</p>
<p>tion (ii) is equivalent to stating that the matrix F(x&lowast;) is positive semidefinite. As
we shall see, the matrix F(x&lowast;), which arises here quite naturally in a discussion of
necessary conditions, plays a fundamental role in the analysis of iterative methods
</p>
<p>for solving unconstrained optimization problems. The structure of this matrix is the
</p>
<p>primary determinant of the rate of convergence of algorithms designed to minimize
</p>
<p>the function f .</p>
<p/>
</div>
<div class="page"><p/>
<p>7.3 Second-Order Conditions 187
</p>
<p>Example 2. Consider the problem
</p>
<p>minimize f (x1, x2) = x
3
1 &minus; x21x2 + 2x22
</p>
<p>subject to x1 � 0, x2 � 0.
</p>
<p>If we assume that the solution is in the interior of the feasible set, that is, if
</p>
<p>x1 &gt; 0, x2 &gt; 0, then the first-order necessary conditions are
</p>
<p>3x21 &minus; 2x1x2 = 0, &minus;x21 + 4x2 = 0.
</p>
<p>There is a solution to these at x1 = x2 = 0 which is a boundary point, but there is
</p>
<p>also a solution at x1 = 6, x2 = 9. We note that for x1 fixed at x1 = 6, the objective
</p>
<p>attains a relative minimum with respect to x2 at x2 = 9. Conversely, with x2 fixed
</p>
<p>at x2 = 9, the objective attains a relative minimum with respect to x1 at x1 = 6.
</p>
<p>Despite this fact, the point x1 = 6, x2 = 9 is not a relative minimum point, because
</p>
<p>the Hessian matrix is
</p>
<p>F =
</p>
<p>[
</p>
<p>6x1 &minus; 2x2 &minus;2x1
&minus;2x1 4
</p>
<p>]
</p>
<p>,
</p>
<p>which, evaluated at the proposed solution x1 = 6, x2 = 9, is
</p>
<p>F =
</p>
<p>[
</p>
<p>18 &minus;12
&minus;12 4
</p>
<p>]
</p>
<p>.
</p>
<p>This matrix is not positive semidefinite, since its determinant is negative. Thus the
</p>
<p>proposed solution is not a relative minimum point.
</p>
<p>Sufficient Conditions for a Relative Minimum
</p>
<p>By slightly strengthening the second condition of Proposition 2 above, we obtain a
</p>
<p>set of conditions that imply that the point x&lowast; is a relative minimum. We give here
the conditions that apply only to unconstrained problems, or to problems where the
</p>
<p>minimum point is interior to the feasible region, since the corresponding conditions
</p>
<p>for problems where the minimum is achieved on a boundary point of the feasible
</p>
<p>set are a good deal more difficult and of marginal practical or theoretical value.
</p>
<p>A more general result, applicable to problems with functional constraints, is given
</p>
<p>in Chap. 11.
</p>
<p>Proposition 3 (Second-Order Sufficient Conditions&mdash;Unconstrained Case). Let f &isin; C2
be function defined on a region in which the point x&lowast;is an interior point. Suppose in addition
that
</p>
<p>i) &nabla; f (x&lowast;) = 0 (7.7)
</p>
<p>ii) F(x&lowast;) is positive definite (7.8)
</p>
<p>Then x&lowast; is a strict relative minimum point of f .</p>
<p/>
</div>
<div class="page"><p/>
<p>188 7 Basic Properties of Solutions and Algorithms
</p>
<p>Proof. Since F(x&lowast;) is positive definite, there is an a &gt; 0 such that for all d, dTF(x&lowast;)
d � a|d|2. Thus by the Taylor&rsquo;s Theorem (with remainder)
</p>
<p>f (x&lowast; + d) &minus; f (x&lowast;) = 1
2
</p>
<p>dTF(x&lowast;)d + o(|d|2)
</p>
<p>� (a/2)|d|2 + o(|d|2)
</p>
<p>For small |d| the first term on the right dominates the second, implying that both
sides are positive for small d. �
</p>
<p>7.4 Convex and Concave Functions
</p>
<p>In order to develop a theory directed toward characterizing global, rather than local,
</p>
<p>minimum points, it is necessary to introduce some sort of convexity assumptions.
</p>
<p>This results not only in a more potent, although more restrictive, theory but also pro-
</p>
<p>vides an interesting geometric interpretation of the second-order sufficiency result
</p>
<p>derived above.
</p>
<p>Definition. A function f defined on a convex set Ω is said to be convex if, for every x1, x2 &isin;
Ω and every α, 0 � α � 1, there holds
</p>
<p>f (αx1 + (1 &minus; α)x2) � α f (x1) + (1 &minus; α) f (x2).
</p>
<p>If, for every α, 0 &lt; α &lt; 1, and x1 � x2, there holds
</p>
<p>f (αx1 + (1 &minus; α)x2) &lt; α f (x1) + (1 &minus; α) f (x2),
</p>
<p>then f is said to be strictly convex.
</p>
<p>Several examples of convex or nonconvex functions are shown in Fig. 7.3.
</p>
<p>Geometrically, a function is convex if the line joining two points on its graph lies
</p>
<p>nowhere below the graph, as shown in Fig. 7.3a, or, thinking of a function in two
</p>
<p>dimensions, it is convex if its graph is bowl shaped.
</p>
<p>Next we turn to the definition of a concave function.
</p>
<p>Definition. A function g defined on a convex set Ω is said to be concave if the function
f = &minus;g is convex. The function g is strictly concave if &minus;g is strictly convex.
</p>
<p>Combinations of Convex Functions
</p>
<p>We show that convex functions can be combined to yield new convex functions and
</p>
<p>that convex functions when used as constraints yield convex constraint sets.
</p>
<p>Proposition 1. Let f1 and f2 be convex functions on the convex set Ω. Then the function
</p>
<p>f1 + f2 is convex on Ω.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.4 Convex and Concave Functions 189
</p>
<p>Fig. 7.3 Convex and nonconvex functions</p>
<p/>
</div>
<div class="page"><p/>
<p>190 7 Basic Properties of Solutions and Algorithms
</p>
<p>Proof. Let x1, x2 &isin; Ω, and 0 &lt; α &lt; 1. Then
</p>
<p>f1(αx1 + (1 &minus; α)x2) + f2(αx1) + (1 &minus; α)x2)
� α[ f1(x1) + f2(x1)] + (1 &minus; α)[ f1(x2) + f2(x2)]. �
</p>
<p>Proposition 2. Let f be a convex function over the convex set Ω. Then the function af is
</p>
<p>convex for any a � 0.
</p>
<p>Proof. Immediate. �
</p>
<p>Note that through repeated application of the above two propositions it follows
</p>
<p>that a positive combination a1 f1 + a2 f2 + . . . + am fm of convex functions is again
</p>
<p>convex.
</p>
<p>Finally, we consider sets defined by convex inequality constraints.
</p>
<p>Proposition 3. Let f be a convex function on a convex set Ω. The set Γc = {x : x &isin;
Ω, f (x) � c} is convex for every real number c.
</p>
<p>Proof. Let x1, x2 &isin; Γc. Then f (x1) � c, f (x2) � c and for 0 &lt; α &lt; 1,
</p>
<p>f (αx1 + (1 &minus; α)x2) � α f (x1) + (1 &minus; α) f (x2) � c.
</p>
<p>Thus αx1 + (1 &minus; α)x2 &isin; Γc. �
</p>
<p>We note that, since the intersection of convex sets is also convex, the set of points
</p>
<p>simultaneously satisfying
</p>
<p>f1(x) � c1, f2(x) � c2, . . . , fm(x) � cm,
</p>
<p>where each fi is a convex function, defines a convex set. This is important in math-
</p>
<p>ematical programming, since the constraint set is often defined this way.
</p>
<p>Properties of Differentiable Convex Functions
</p>
<p>If a function f is differentiable, then there are alternative characterizations of con-
</p>
<p>vexity.
</p>
<p>Proposition 4. Let f &isin; C1. Then f is convex over a convex set Ω if and only if
</p>
<p>f (y) � f (x) + &nabla; f (x)(y &minus; x) (7.9)
</p>
<p>for all x, y &isin; Ω.
</p>
<p>Proof. First suppose f is convex. Then for all α, 0 � α � 1,
</p>
<p>f (αy + (1 &minus; α)x) � α f (y) + (1 &minus; α) f (x).</p>
<p/>
</div>
<div class="page"><p/>
<p>7.4 Convex and Concave Functions 191
</p>
<p>Thus for 0 &lt; α � 1
</p>
<p>f (x + α(y &minus; x)) &minus; f (x)
α
</p>
<p>� f (y) &minus; f (x).
</p>
<p>Letting α &rarr; 0 we obtain
</p>
<p>&nabla; f (x) (y &minus; x) � f (y) &minus; f (x).
</p>
<p>This proves the &ldquo;only if&rdquo; part.
</p>
<p>Now assume
</p>
<p>f (y) � f (x) + &nabla; f (x) (y &minus; x)
for all x, y &isin; Ω. Fix x1, x2 &isin; Ω and α, 0 � α � 1. Setting x = αx1 + (1 &minus; α)x2 and
alternatively y = x1 or y = x2, we have
</p>
<p>f (x1) � f (x) + &nabla; f (x)(x1 &minus; x) (7.10)
f (x2) � f (x) + &nabla; f (x)(x2 &minus; x). (7.11)
</p>
<p>Multiplying (7.10) by α and (7.11) by (1 &minus; α) and adding, we obtain
</p>
<p>α f (x1) + (1 &minus; α) f (x2) � f (x) + &nabla; f (x)[αx1 + (1 &minus; α)x2 &minus; x].
</p>
<p>But substituting x = αx1 + (1 &minus; α)x2, we obtain
</p>
<p>α f (x1) + (1 &minus; α) f (x2) � f (αx1 + (1 &minus; α)x2). �
</p>
<p>The statement of the above proposition is illustrated in Fig. 7.4. It can be regarded
</p>
<p>as a sort of dual characterization of the original definition illustrated in Fig. 7.3.
</p>
<p>The original definition essentially states that linear interpolation between two points
</p>
<p>overestimates the function, while the above proposition states that linear approxima-
</p>
<p>tion based on the local derivative underestimates the function.
</p>
<p>For twice continuously differentiable functions, there is another characterization
</p>
<p>of convexity.
</p>
<p>Fig. 7.4 Illustration of Proposition 4</p>
<p/>
</div>
<div class="page"><p/>
<p>192 7 Basic Properties of Solutions and Algorithms
</p>
<p>Proposition 5. Let f &isin; C2. Then f is convex over a convex set Ω containing an interior
point if and only if the Hessian matrix F of f is positive semidefinite throughout Ω.
</p>
<p>Proof. By Taylor&rsquo;s theorem we have
</p>
<p>f (y) = f (x) = &nabla; f (x)(y &minus; x) + 1
2
</p>
<p>(y &minus; x)TF(x + α(y &minus; x))(y &minus; x) (7.12)
</p>
<p>for some α, 0 � α � 1. Clearly, if the Hessian is everywhere positive semidefinite,
</p>
<p>we have
</p>
<p>f (y) � f (x) + &nabla; f (x)(y &minus; x), (7.13)
which in view of Proposition 4 implies that f is convex.
</p>
<p>Now suppose the Hessian is not positive semidefinite at some point x &isin; Ω.
By continuity of the Hessian it can be assumed, without loss of generality, that x
</p>
<p>is an interior point of Ω. There is a y &isin; Ω such that (y &minus; x)TF(x)(y &minus; x) &lt; 0. Again
by the continuity of the Hessian, y may be selected so that for all α, 0 � α � 1,
</p>
<p>(y &minus; x)TF(x + α(y &minus; x)) (y &minus; x) &lt; 0.
</p>
<p>This in view of (7.12) implies that (7.13) does not hold; which in view of Proposi-
</p>
<p>tion 4 implies that f is not convex. �
</p>
<p>The Hessian matrix is the generalization to En of the concept of the curvature of a
</p>
<p>function, and correspondingly, positive definiteness of the Hessian is the generaliza-
</p>
<p>tion of positive curvature. Convex functions have positive (or at least nonnegative)
</p>
<p>curvature in every direction. Motivated by these observations, we sometimes refer
</p>
<p>to a function as being locally convex if its Hessian matrix is positive semidefinite
</p>
<p>in a small region, and locally strictly convex if the Hessian is positive definite in
</p>
<p>the region. In these terms we see that the second-order sufficiency result of the last
</p>
<p>section requires that the function be locally strictly convex at the point x&lowast;. Thus,
even the local theory, derived solely in terms of the elementary calculus, is actually
</p>
<p>intimately related to convexity&mdash;at least locally. For this reason we can view the two
</p>
<p>theories, local and global, not as disjoint parallel developments but as complemen-
</p>
<p>tary and interactive. Results that are based on convexity apply even to nonconvex
</p>
<p>problems in a region near the solution, and conversely, local results apply to a global
</p>
<p>minimum point.
</p>
<p>7.5 Minimization and Maximization of Convex Functions
</p>
<p>We turn now to the three classic results concerning minimization or maximization
</p>
<p>of convex functions.
</p>
<p>Theorem 1. Let f be a convex function defined on the convex set Ω. Then the set Γ where f
</p>
<p>achieves its minimum is convex, and any relative minimum of f is a global minimum.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.5 Minimization and Maximization of Convex Functions 193
</p>
<p>Proof. If f has no relative minima the theorem is valid by default. Assume now that
</p>
<p>c0 is the minimum of f . Then clearly Γ = {x : f (x) � c0, x &isin; Ω} and this is convex
by Proposition 3 of the last section.
</p>
<p>Suppose now that x&lowast; &isin; Ω is a relative minimum point of f , but that there is
another point y &isin; Ω with f (y) &lt; f (x&lowast;). On the line αy + (1 &minus; α)x&lowast;, 0 &lt; α &lt; 1 we
have
</p>
<p>f (αy + (1 &minus; α)x&lowast;) � α f (y) + (1 &minus; α) f (x&lowast;) &lt; f (x&lowast;),
</p>
<p>contradicting the fact that x&lowast; is a relative minimum point. �
</p>
<p>We might paraphrase the above theorem as saying that for convex functions, all
</p>
<p>minimum points are located together (in a convex set) and all relative minima are
</p>
<p>global minima. The next theorem says that if f is continuously differentiable and
</p>
<p>convex, then satisfaction of the first-order necessary conditions are both necessary
</p>
<p>and sufficient for a point to be a global minimizing point.
</p>
<p>Theorem 2. Let f &isin; C1 be convex on the convex set Ω. If there is a point x&lowast; &isin; Ω such that,
for all y &isin; Ω, &nabla; f (x&lowast;)(y &minus; x&lowast;) � 0, then x&lowast; is a global minimum point of f over Ω.
</p>
<p>Proof. We note parenthetically that since y&minus;x&lowast; is a feasible direction at x&lowast;, the given
condition is equivalent to the first-order necessary condition stated in Sect. 7.1. The
</p>
<p>proof of the proposition is immediate, since by Proposition 4 of the last section
</p>
<p>f (y) � f (x&lowast;) + &nabla; f (x&lowast;)(y &minus; x&lowast;) � f (x&lowast;). �
</p>
<p>Next we turn to the question of maximizing a convex function over a convex set.
</p>
<p>There is, however, no analog of Theorem 1 for maximization; indeed, the tendency
</p>
<p>is for the occurrence of numerous nonglobal relative maximum points. Nevertheless,
</p>
<p>it is possible to prove one important result. It is not used in subsequent chapters,
</p>
<p>but it is useful for some areas of optimization.
</p>
<p>Theorem 3. Let f be a convex function defined on the bounded, closed convex set Ω. If f
</p>
<p>has a maximum over Ω it is achieved at an extreme point of Ω.
</p>
<p>Proof. Suppose f achieves a global maximum at x&lowast; &isin; Ω. We show first that this
maximum is achieved at some boundary point of Ω. If x&lowast; is itself a boundary point,
then there is nothing to prove, so assume x&lowast; is not a boundary point. Let L be any
line passing through the point x&lowast;. The intersection of this line with Ω is an interval
of the line L having end points y1, y2 which are boundary points of Ω, and we have
</p>
<p>x&lowast; = αy1 + (1 &minus; α)y2 for some α, 0 &lt; α &lt; 1. By convexity of f
</p>
<p>f (x&lowast;) � α f (y1) + (1 &minus; α) f (y2) � max{ f (y1), f (y2)}.
</p>
<p>Thus either f (y1) or f (y2) must be at least as great as f (x
&lowast;). Since x&lowast; is a maximum
</p>
<p>point, so is either y1 or y2.
</p>
<p>We have shown that the maximum, if achieved, must be achieved at a boundary
</p>
<p>point of Ω. If this boundary point, x&lowast;, is an extreme point of Ω there is nothing
more to prove. If it is not an extreme point, consider the intersection of Ω with a</p>
<p/>
</div>
<div class="page"><p/>
<p>194 7 Basic Properties of Solutions and Algorithms
</p>
<p>supporting hyperplane H at x&lowast;. This intersection, T1, is of dimension n &minus; 1 or less
and the global maximum of f over T1 is equal to f (x
</p>
<p>&lowast;) and must be achieved at a
boundary point x1 of T1. If this boundary point is an extreme point of T1, it is also an
</p>
<p>extreme point of Ω by Lemma 1, Sect. B.4, and hence the theorem is proved. If x1 is
</p>
<p>not an extreme point of T1, we form T2, the intersection of T1 with a hyperplane in
</p>
<p>En&minus;1 supporting T1 at x1. This process can continue at most a total of n times when a
set Tn of dimension zero, consisting of a single point, is obtained. This single point
</p>
<p>is an extreme point of Tn and also, by repeated application of Lemma 1, Sect. B.4,
</p>
<p>an extreme point of Ω. �
</p>
<p>*7.6 &lowast;Zero-Order Conditions
</p>
<p>We have considered the problem
</p>
<p>minimize f (x)
</p>
<p>subject to x &isin; Ω (7.14)
</p>
<p>to be unconstrained because there are no functional constraints of the form g(x) � b
</p>
<p>or h(x) = c. However, the problem is of course constrained by the set Ω. This
</p>
<p>constraint influences the first- and second-order necessary and sufficient conditions
</p>
<p>through the relation between feasible directions and derivatives of the function f .
</p>
<p>Nevertheless, there is a way to treat this constraint without reference to derivatives.
</p>
<p>The resulting conditions are then of zero order. These necessary conditions require
</p>
<p>that the problem be convex is a certain way, while the sufficient conditions require
</p>
<p>no assumptions at all. The simplest assumptions for the necessary conditions are that
</p>
<p>Ω is a convex set and that f is a convex function on all of En.
</p>
<p>Fig. 7.5 The epigraph, the tubular region, and the hyperplane</p>
<p/>
</div>
<div class="page"><p/>
<p>7.6 &lowast;Zero-Order Conditions 195
</p>
<p>To derive the necessary conditions under these assumptions consider the set Γ &sub;
En+1 = {(r, x) : r � f (x), x &isin; En}. In a figure of the graph of f , the set Γ is the
region above the graph, shown in the upper part of Fig. 7.5. This set is called the
</p>
<p>epigraph of f . It is easy to verify that the set Γ is convex if f is a convex function.
</p>
<p>Suppose that x&lowast; &isin; Ω is the minimizing point with value f &lowast; = f (x&lowast;). We construct
a tubular region with cross section Ω and extending vertically from &minus;&infin; up to f &lowast;,
shown as B in the upper part of Fig. 7.5. This is also a convex set, and it overlaps
</p>
<p>the set Γ only at the boundary point ( f &lowast;, b&lowast;) above x&lowast;(or possibly many boundary
points if f is flat near x&lowast;).
</p>
<p>According to the separating hyperplane theorem (Appendix B), there is a hyper-
</p>
<p>plane separating these two sets. This hyperplane can be represented by a nonzero
</p>
<p>vector of the form (s, λ) &isin; En+1 with s a scalar and λ &isin; En, and a separation constant
c. The separation conditions are
</p>
<p>sr + λTx � c for all x &isin; En and r � f (x) (7.15)
sr + λTx � c for all x &isin; Ω and r � f &lowast;. (7.16)
</p>
<p>It follows that s � 0; for otherwise λ � 0 and then (7.15) would be violated for some
</p>
<p>x &isin; En. It also follows that s � 0 since otherwise (7.16) would be violated by very
negative values of r. Hence, together we find s &gt; 0 and by appropriate scaling we
</p>
<p>may take s = 1.
</p>
<p>It is easy to see that the above conditions can be expressed alternatively as two
</p>
<p>optimization problems, as stated in the following proposition.
</p>
<p>Proposition 1 (Zero-Order Necessary Conditions). If x&lowast; solves (7.14) under the stated
convexity conditions, then there is a nonzero vector λ &isin; En such that x&lowast; is a solution to the
two problems:
</p>
<p>minimize f (x) + λTx
</p>
<p>subject to x &isin; En (7.17)
</p>
<p>and
</p>
<p>maximize λTx
</p>
<p>subject to x &isin; Ω. (7.18)
</p>
<p>Proof. Problem (7.17) follows from (7.15) (with s = 1) and the fact that f (x) � r
</p>
<p>for r � f (x). The value c is attained from above at ( f &lowast;, x&lowast;). Likewise (7.18) follows
</p>
<p>from (7.16) and the fact that x&lowast; and the appropriate r attain c from below. �
</p>
<p>Notice that problem (7.17) is completely unconstrained, since x may range over
</p>
<p>all of En. The second problem (7.18) is constrained by Ω but has a linear objective
</p>
<p>function. It is clear from Fig. 7.5 that the slope of the hyperplane is equal to the
</p>
<p>slope of the function f when f is continuously differentiable at the solution x&lowast;.
If the optimal solution x&lowast; is in the interior of Ω, then the second problem (7.18)
</p>
<p>implies that λ = 0, for otherwise there would be a direction of movement from x&lowast;
</p>
<p>that increases the product λTx above λTx&lowast;. The hyperplane is horizontal in that case.</p>
<p/>
</div>
<div class="page"><p/>
<p>196 7 Basic Properties of Solutions and Algorithms
</p>
<p>The zeroth-order conditions provide no new information in this situation. However,
</p>
<p>when the solution is on a boundary point of Ω the conditions give very useful infor-
</p>
<p>mation.
</p>
<p>Example 1 (Minimization Over an Interval). Consider a continuously differentiable
</p>
<p>function f of a single variable x &isin; E1 defined on the unit interval [0,1] which plays
the role of Ω here. The first problem (7.17) implies f &prime;(x&lowast;) = &minus;λ. If the solution is
at the left end of the interval (at x = 0) then the second problem (7.18) implies that
</p>
<p>λ &le; 0 which means that f &prime;(x&lowast;) &ge; 0. The reverse holds if x&lowast; is at the right end. These
together are identical to the first-order conditions of Sect. 7.1.
</p>
<p>Example 2. As a generalization of the above example, let f &isin; C1 on En, and let f
have a minimum with respect to Ω at x&lowast;. Let d &isin; En be a feasible direction at x&lowast;.
Then it follows again from (7.17) that &nabla; f (x&lowast;)d &ge; 0.
</p>
<p>Sufficient Conditions Theorem. The conditions of Proposition 1 are sufficient for x&lowast; to be
a minimum even without the convexity assumptions.
</p>
<p>Proposition 2 (Zero-Order Sufficiency Conditions). If there is a λ such that x&lowast; &isin; Ω solves
the problems (7.17) and (7.18), then x&lowast; solves (7.14).
</p>
<p>Proof. Suppose x1 is any other point in Ω. Then from (7.17)
</p>
<p>f (x1) + λ
Tx1 � f (x
</p>
<p>&lowast;) + λTx&lowast;.
</p>
<p>This can be rewritten as
</p>
<p>f (x1) &minus; f (x&lowast;) � λTx&lowast; &minus; λTx1.
</p>
<p>By problem (7.18) the right hand side of this is greater than or equal to zero. Hence
</p>
<p>f (x1) &minus; f (x&lowast;) � 0 which establishes the result. �
</p>
<p>7.7 Global Convergence of Descent Algorithms
</p>
<p>A good portion of the remainder of this book is devoted to presentation and analysis
</p>
<p>of various algorithms designed to solve nonlinear programming problems. Although
</p>
<p>these algorithms vary substantially in their motivation, application, and detailed
</p>
<p>analysis, ranging from the simple to the highly complex, they have the common
</p>
<p>heritage of all being iterative descent algorithms. By iterative, we mean, roughly,
</p>
<p>that the algorithm generates a series of points, each point being calculated on the
</p>
<p>basis of the points preceding it. By descent, we mean that as each new point is
</p>
<p>generated by the algorithm the corresponding value of some function (evaluated at
</p>
<p>the most recent point) decreases in value. Ideally, the sequence of points generated
</p>
<p>by the algorithm in this way converges in a finite or infinite number of steps to a
</p>
<p>solution of the original problem.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.7 Global Convergence of Descent Algorithms 197
</p>
<p>An iterative algorithm is initiated by specifying a starting point. If for arbitrary
</p>
<p>starting points the algorithm is guaranteed to generate a sequence of points con-
</p>
<p>verging to a solution, then the algorithm is said to be globally convergent. Quite
</p>
<p>definitely, not all algorithms have this obviously desirable property. Indeed, many of
</p>
<p>the most important algorithms for solving nonlinear programming problems are not
</p>
<p>globally convergent in their purest form and thus occasionally generate sequences
</p>
<p>that either do not converge at all or converge to points that are not solutions. It is
</p>
<p>often possible, however, to modify such algorithms, by appending special devices,
</p>
<p>so as to guarantee global convergence.
</p>
<p>Fortunately, the subject of global convergence can be treated in a unified manner
</p>
<p>through the analysis of a general theory of algorithms developed mainly by Zang-
</p>
<p>will. From this analysis, which is presented in this section, we derive the Global
</p>
<p>Convergence Theorem that is applicable to the study of any iterative descent algo-
</p>
<p>rithm. Frequent reference to this important result is made in subsequent chapters.
</p>
<p>Iterative Algorithms
</p>
<p>We think of an algorithm as a mapping. Given a point x in some space X, the output
</p>
<p>of an algorithm applied to x is a new point. Operated iteratively, an algorithm is
</p>
<p>repeatedly reapplied to the new points it generates so as to produce a whole sequence
</p>
<p>of points. Thus, as a preliminary definition, we might formally define an algorithm A
</p>
<p>as a mapping taking points in a space X into (other) points in X. Operated iteratively,
</p>
<p>the algorithm A initiated at x0 &isin; X would generate the sequence {xk} defined by
</p>
<p>xk+1 = A(xk).
</p>
<p>In practice, the mapping A might be defined explicitly by a simple mathematical
</p>
<p>expression or it might be defined implicitly by, say, a lengthy complex computer
</p>
<p>program. Given an input vector, both define a corresponding output.
</p>
<p>With this intuitive idea of an algorithm in mind, we now generalize the concept
</p>
<p>somewhat so as to provide greater flexibility in our analyses.
</p>
<p>Definition. An algorithm A is a mapping defined on a space X that assigns to every point
x &isin; X a subset of X.
</p>
<p>In this definition the term &ldquo;space&rdquo; can be interpreted loosely. Usually X is the
</p>
<p>vector space En but it may be only a subset of En or even a more general metric
</p>
<p>space. The most important aspect of the definition, however, is that the mapping A,
</p>
<p>rather than being a point-to-point mapping of X, is a point-to-set mapping of X.
</p>
<p>An algorithm A generates a sequence of points in the following way. Given
</p>
<p>xk &isin; X the algorithm yields A(xk) which is a subset of X. From this subset an ar-
bitrary element xk+1 is selected. In this way, given an initial point x0, the algorithm
</p>
<p>generates sequences through the iteration
</p>
<p>xk+1 &isin; A(xk).</p>
<p/>
</div>
<div class="page"><p/>
<p>198 7 Basic Properties of Solutions and Algorithms
</p>
<p>It is clear that, unlike the case where A is a point-to-point mapping, the sequence
</p>
<p>generated by the algorithm A cannot, in general, be predicted solely from knowledge
</p>
<p>of the initial point x0. This degree of uncertainty is designed to reflect uncertainty
</p>
<p>that we may have in practice as to specific details of an algorithm.
</p>
<p>Example 1. Suppose for x on the real line we define
</p>
<p>A(x) = [&minus;|x|/2, |x|/2]
</p>
<p>so that A(x) is an interval of the real line. Starting at x0 = 100, each of the sequences
</p>
<p>below might be generated from iterative application of this algorithm.
</p>
<p>100, 50, 25, 12, &minus;6, &minus;2, 1, 1/2, . . .
100, &minus;40, 20, &minus;5, &minus;2, 1, 1/4, 1/8, . . .
</p>
<p>100, 10, &minus;1, 1/16, 1/100, &minus;1/1000, 1/10, 100, . . .
</p>
<p>The apparent ambiguity that is built into this definition of an algorithm is not meant
</p>
<p>to imply that actual algorithms are random in character. In actual implementation
</p>
<p>algorithms are not defined ambiguously. Indeed, a particular computer program
</p>
<p>executed twice from the same starting point will generate two copies of the same
</p>
<p>sequence. In other words, in practice algorithms are point-to-point mappings. The
</p>
<p>utility of the more general definition is that it allows one to analyze, in a single step,
</p>
<p>the convergence of an infinite family of similar algorithms. Thus, two computer pro-
</p>
<p>grams, designed from the same basic idea, may differ slightly in some details, and
</p>
<p>therefore perhaps may not produce identical results when given the same starting
</p>
<p>point. Both programs may, however, be regarded as implementations of the same
</p>
<p>point-to-set mappings. In the example above, for instance, it is not necessary to
</p>
<p>know exactly how xk+1 is determined from xk so long as it is known that its absolute
</p>
<p>value is no greater than one-half xk&rsquo;s absolute value. The result will always tend to-
</p>
<p>ward zero. In this manner, the generalized concept of an algorithm sometimes leads
</p>
<p>to simpler analysis.
</p>
<p>Descent
</p>
<p>In order to describe the idea of a descent algorithm we first must agree on a subset
</p>
<p>Γ of the space X, referred to as the solution set. The basic idea of a descent function,
</p>
<p>which is defined below, is that for points outside the solution set, a single step of the
</p>
<p>algorithm yields a decrease in the value of the descent function.
</p>
<p>Definition. Let Γ &sub; X be a given solution set and let A be an algorithm on X. A continuous
real-valued function Z on X is said to be a descent function for Γ and A if it satisfies
</p>
<p>i) if x � Γ and y &isin; A(x), then Z(y) &lt; Z(x)
ii) if x &isin; Γ and y &isin; A(x), then Z(y) � Z(x).</p>
<p/>
</div>
<div class="page"><p/>
<p>7.7 Global Convergence of Descent Algorithms 199
</p>
<p>There are a number of ways a solution set, algorithm, and descent function can
</p>
<p>be defined. A natural set-up for the problem
</p>
<p>minimize f (x) (7.19)
</p>
<p>subject to x &isin; Ω
</p>
<p>is to let Γ be the set of minimizing points, and define an algorithm A on Ω in such a
</p>
<p>way that f decreases at each step and thereby serves as a descent function. Indeed,
</p>
<p>this is the procedure followed in a majority of cases. Another possibility for uncon-
</p>
<p>strained problems is to let Γ be the set of points x satisfying &nabla; f (x) = 0. In this case
</p>
<p>we might design an algorithm for which |&nabla; f (x)| serves as a descent function or for
which f (x) serves as a descent function.
</p>
<p>&lowast;Closed Mappings
</p>
<p>An important property possessed by some algorithms is that they are closed. This
</p>
<p>property, which is a generalization for point-to-set mappings of the concept of con-
</p>
<p>tinuity for point-to-point mappings, turns out to be the key to establishing a gen-
</p>
<p>eral global convergence theorem. In defining this property we allow the point-to-set
</p>
<p>mapping to map points in one space X into subsets of another space Y.
</p>
<p>Definition. A point-to-set mapping A from X to Y is said to be closed at x &isin; X if the
assumptions
</p>
<p>i) xk &rarr; x, xk &isin; X,
ii) yk &rarr; y, yk &isin; A(xk)
</p>
<p>imply
</p>
<p>iii) y &isin; A(x).
</p>
<p>Fig. 7.6 Graphs of mappings
</p>
<p>The point-to-set map A is said to be closed on X if it is closed at each point of X.</p>
<p/>
</div>
<div class="page"><p/>
<p>200 7 Basic Properties of Solutions and Algorithms
</p>
<p>Example 2. As a special case, suppose that the mapping A is a point-to-point map-
</p>
<p>ping; that is, for each x &isin; X the set A(x) consists of a single point in Y. Suppose also
that A is continuous at x &isin; X. This means that if xk &rarr; x then A(xk) &rarr; A(x), and
it follows that A is closed at x. Thus for point-to-point mappings continuity implies
</p>
<p>closedness. The converse is, however, not true in general.
</p>
<p>The definition of a closed mapping can be visualized in terms of the graph of the
</p>
<p>mapping, which is the set {(x, y) : x &isin; X, y &isin; A(x)}. If X is closed, then A is closed
throughout X if and only if this graph is a closed set. This is illustrated in Fig. 7.6.
</p>
<p>However, this equivalence is valid only when considering closedness everywhere.
</p>
<p>In general a mapping may be closed at some points and not at others.
</p>
<p>Example 3. The reader should verify that the point-to-set mapping defined in
</p>
<p>Example 1 is closed.
</p>
<p>Many complex algorithms that we analyze are most conveniently regarded as the
</p>
<p>composition of two or more simple point-to-set mappings. It is therefore natural to
</p>
<p>ask whether closedness of the individual maps implies closedness of the composite.
</p>
<p>The answer is a qualified &ldquo;yes.&rdquo; The technical details of composition are described
</p>
<p>in the remainder of this subsection. They can safely be omitted at first reading while
</p>
<p>proceeding to the Global Convergence Theorem.
</p>
<p>Definition. Let A : X &rarr; Y and B : Y &rarr; Z be point-to-set mappings. The composite
mapping C = BA is defined as the point-to-set mapping C : X &rarr; Z with
</p>
<p>C(x) =
⋃
</p>
<p>y&isin;A(x)
B(y).
</p>
<p>This definition is illustrated in Fig. 7.7.
</p>
<p>Proposition. Let A : X &rarr; Y and B : Y &rarr; Z be point-to-set mappings. Suppose A is closed
at x and B is closed on A(x). Suppose also that if xk &rarr; x and yk &isin; A(xk), there is a y such
that, for some subsequence {yki}, yki &rarr; y. Then the composite mapping C = BA is closed
at x.
</p>
<p>Proof. Let xk &rarr; x and zk &rarr; z with zk &isin; C(xk). It must be shown that z &isin; C(x).
Select yk &isin; A(xk) such that zk &isin; B(yk) and according to the hypothesis let y and
</p>
<p>{yki} be such that yki &rarr; y. Since A is closed at x it follows that y &isin; A(x).
Likewise, since yki &rarr; y, zki &rarr; z and B is closed at y, it follows that z &isin; B(y) &sub;
</p>
<p>BA(x) = C(x). �
</p>
<p>Two important corollaries follow immediately.
</p>
<p>Corollary 1. Let A : X &rarr; Y and B : Y &rarr; Z be point-to-set mappings. If A is closed at x, B
is closed on A(x) and Y is compact, then the composite map C = BA is closed at x.
</p>
<p>Corollary 2. Let A : X &rarr; Y be a point-to-point mapping and B : Y &rarr; Z a point-to-
set mapping. If A is continuous at x and B is closed at A(x), then the composite mapping
C = BA is closed at x.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.7 Global Convergence of Descent Algorithms 201
</p>
<p>Fig. 7.7 Composition of mappings
</p>
<p>Global Convergence Theorem
</p>
<p>The Global Convergence Theorem is used to establish convergence for the follow-
</p>
<p>ing general situation. There is a solution set Γ. Points are generated according to
</p>
<p>the algorithm xk+1 &isin; A(xk), and each new point always strictly decreases a descent
function Z unless the solution set Γ is reached. For example, in nonlinear program-
</p>
<p>ming, the solution set may be the set of minimum points (perhaps only one point),
</p>
<p>and the descent function may be the objective function itself. A suitable algorithm
</p>
<p>is found that generates points such that each new point strictly reduces the value of
</p>
<p>the objective. Then, under appropriate conditions, it follows that the sequence con-
</p>
<p>verges to the solution set. The Global Convergence Theorem establishes technical
</p>
<p>conditions for which convergence is guaranteed.
</p>
<p>Global Convergence Theorem. Let A be an algorithm on X, and suppose that, given x0 the
sequence {xk}&infin;k=0 is generated satisfying
</p>
<p>xk+1 &isin; A(xk).
</p>
<p>Let a solution set Γ &sub; X be given, and suppose
</p>
<p>i) all points xk are contained in a compact set S &sub; X
ii) there is a continuous function Z on X such that
</p>
<p>(a) if x � Γ, then Z(y) &lt; Z(x) for all y &isin; A(x)
(b) if x &isin; Γ, then Z(y) � Z(x) for all y &isin; A(x)
</p>
<p>iii) the mapping A is closed at points outside Γ.
</p>
<p>Then the limit of any convergent subsequence of {xk} is a solution.
</p>
<p>Proof. Suppose the convergent subsequence {xk}, k &isin; K converges to the limit x.
Since Z is continuous, it follows that for k &isin; K , Z(xk) &rarr; Z(x). This means that Z is
convergent with respect to the subsequence, and we shall show that it is convergent</p>
<p/>
</div>
<div class="page"><p/>
<p>202 7 Basic Properties of Solutions and Algorithms
</p>
<p>with respect to the entire sequence. By the monotonicity of Z on the sequence {xk}
we have Z(xk) &minus; Z(x) � 0 for all k. By the convergence of Z on the subsequence,
there is, for a given ε &gt; 0, a K &isin; K such that Z(xk) &minus; Z(x) &lt; ε for all k &gt; K, k &isin; K .
</p>
<p>Thus for all k &gt; K
</p>
<p>Z(xk) &minus; Z(x) = Z(xk) &minus; Z(xK) + Z(xK) &minus; Z(x) &lt; ε,
</p>
<p>which shows that Z(xk) &rarr; Z(x).
To complete the proof it is only necessary to show that x is a solution. Sup-
</p>
<p>pose x is not a solution. Consider the subsequence {xk+1}K . Since all members of
this sequence are contained in a compact set, there is a K̄ &sub; K such that {xk+1}K̄
converges to some limit x̄. We thus have xk &rarr; x, k &isin; K̄ , and xk+1 &isin; A(xk) with
xk+1 &rarr; x̄, k &isin; K̄ . Thus since A is closed at x it follows that x̄ &isin; A(x). But from
above, Z(x̄) = Z(x) which contradicts the fact that Z is a descent function. �
</p>
<p>Corollary. If under the conditions of the Global Convergence Theorem Γ consists of a
</p>
<p>single point x̄, then the sequence {xk} converges to x̄.
</p>
<p>Proof. Suppose to the contrary that there is a subsequence {xk}K and an ε &gt; 0 such
that |xk &minus; x̄| &gt; ε for all k &isin; K . By compactness there must be K &prime; &sub; K such that
{xk}K &prime; , converges, say to x&prime;. Clearly, |x&prime; &minus; x̄| � ε, but by the Global Convergence
Theorem x&prime; &isin; Γ, which is a contradiction. �
</p>
<p>In later chapters the Global Convergence Theorem is used to establish the con-
</p>
<p>vergence of several standard algorithms. Here we consider some simple examples
</p>
<p>designed to illustrate the roles of the various conditions of the theorem.
</p>
<p>Example 4. In many respects condition (iii) of the theorem, the closedness of A out-
</p>
<p>side the solution set, is the most important condition. The failure of many popular
</p>
<p>algorithms can be traced to nonsatisfaction of this condition. On the real line con-
</p>
<p>sider the point-to-point algorithm
</p>
<p>A(x) =
</p>
<p>⎧
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>1
2
(x &minus; 1) + 1 x &gt; 1
</p>
<p>1
2
x x � 1
</p>
<p>and the solution set Γ = {0}. It is easily verified that a descent function for this
solution set and this algorithm is Z(x) = |x|. However, starting from x &gt; 1, the
algorithm generates a sequence converging to x = 1 which is not a solution. The
</p>
<p>difficulty is that A is not closed at x = 1.
</p>
<p>Example 5. On the real line X consider the solution set to be empty, the descent
</p>
<p>function Z(x) = e&minus;x, and the algorithm A(x) = x + 1. All conditions of the conver-
gence theorem except (i) hold. The sequence generated from any starting condition
</p>
<p>diverges to infinity. This is not strictly a violation of the conclusion of the theorem
</p>
<p>but simply an example illustrating that if no compactness assumption is introduced,
</p>
<p>the generated sequence may have no convergent subsequence.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.7 Global Convergence of Descent Algorithms 203
</p>
<p>Example 6. Consider the point-to-set algorithm A defined by the graph in Fig. 7.8
</p>
<p>and given explicitly on X = [0, 1] by
</p>
<p>A(x) =
</p>
<p>{
</p>
<p>[0, x) 1 � x &gt; 0
</p>
<p>0 x = 0,
</p>
<p>where [0, x) denotes a half-open interval (see Appendix A). Letting Γ = {0}, the
function Z(x) = x serves as a descent function, because for x � 0 all points in A(x)
</p>
<p>are less than x.
</p>
<p>Fig. 7.8 Graph for Example 6
</p>
<p>The sequence defined by
</p>
<p>x0 = 1
</p>
<p>xk+1 = xk &minus;
1
</p>
<p>2k+2
</p>
<p>satisfies xk+1 &isin; A(xk) but it can easily be seen that xk &rarr; 12 � Γ. The difficulty here,
of course, is that the algorithm A is not closed outside the solution set.
</p>
<p>&lowast;Spacer Steps
</p>
<p>In some of the more complex algorithms presented in later chapters, the rule used to
</p>
<p>determine a succeeding point in an iteration may depend on several previous points
</p>
<p>rather than just the current point, or it may depend on the iteration index k. Such
</p>
<p>features are generally introduced in order to obtain a rapid rate of convergence but
</p>
<p>they can grossly complicate the analysis of global convergence.</p>
<p/>
</div>
<div class="page"><p/>
<p>204 7 Basic Properties of Solutions and Algorithms
</p>
<p>If in such a complex sequence of steps there is inserted, perhaps irregularly but
</p>
<p>infinitely often, a step of an algorithm such as steepest descent that is known to
</p>
<p>converge, then it is not difficult to insure that the entire complex process converges.
</p>
<p>The step which is repeated infinitely often and guarantees convergence is called a
</p>
<p>spacer step, since it separates disjoint portions of the complex sequence. Essentially
</p>
<p>the only requirement imposed on the other steps of the process is that they do not
</p>
<p>increase the value of the descent function.
</p>
<p>This type of situation can be analyzed easily from the following viewpoint.
</p>
<p>Suppose B is an algorithm which together with the descent function Z and solu-
</p>
<p>tion set Γ, satisfies all the requirements of the Global Convergence Theorem. Define
</p>
<p>the algorithm C by C(x) = {y : Z(y) � Z(x)}. In other words, C applied to x can
give any point so long as it does not increase the value of Z. It is easy to verify that
</p>
<p>C is closed. We imagine that B represents the spacer step and the complex process
</p>
<p>between spacer steps is just some realization of C. Thus the overall process amounts
</p>
<p>merely to repeated applications of the composite algorithm CB. With this viewpoint
</p>
<p>we may state the Spacer Step Theorem.
</p>
<p>Spacer Step Theorem. Suppose B is an algorithm on X which is closed outside the solution
</p>
<p>set Γ. Let Z be a descent function corresponding to B and Γ.
Suppose that the sequence {xk}&infin;k=0 is generated satisfying
</p>
<p>xk+1 &isin; B(xk)
</p>
<p>for k in an infinite index set K , and that
</p>
<p>Z(xk+1) � Z(xk)
</p>
<p>for all k. Suppose also that the set S = {x : Z(x) � Z(x0)} is compact. Then the limit of any
convergent subsequence of {xk}K is a solution.
</p>
<p>Proof. We first define for any x &isin; X, B̄(x) = S &cap;B(x) and then observe that A = CB̄
is closed outside the solution set by Corollary 1. The Global Convergence Theorem
</p>
<p>can then be applied to A. Since S is compact, there is a subsequence of {xk}k&isin;K
converging to a limit x. In view of the above we conclude that x &isin; Γ. �
</p>
<p>7.8 Speed of Convergence
</p>
<p>The study of speed of convergence is an important but sometimes complex subject.
</p>
<p>Nevertheless, there is a rich and yet elementary theory of convergence rates that
</p>
<p>enables one to predict with confidence the relative effectiveness of a wide class of
</p>
<p>algorithms. In this section we introduce various concepts designed to measure speed
</p>
<p>of convergence, and prepare for a study of this most important aspect of nonlinear
</p>
<p>programming.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.8 Speed of Convergence 205
</p>
<p>Order of Convergence
</p>
<p>Consider a sequence of real numbers {rk}&infin;k=0 converging to the limit r&lowast;. We define
several notions related to the speed of convergence of such a sequence.
</p>
<p>Definition. Let the sequence {rk} converge to r&lowast;. The order of convergence of {rk} is defined
as the supremum of the nonnegative numbers p satisfying
</p>
<p>0 � lim
k&rarr;&infin;
</p>
<p>|rk+1 &minus; r&lowast;|
|rk &minus; r&lowast; |p
</p>
<p>&lt; &infin;.
</p>
<p>To ensure that the definition is applicable to any sequence, it is stated in terms
</p>
<p>of limit superior rather than just limit and 0/0 (which occurs if rk = r
&lowast; for all k)
</p>
<p>is regarded as finite. But these technicalities are rarely necessary in actual analysis,
</p>
<p>since the sequences generated by algorithms are generally quite well behaved.
</p>
<p>It should be noted that the order of convergence, as with all other notions related
</p>
<p>to speed of convergence that are introduced, is determined only by the properties
</p>
<p>of the sequence that hold as k &rarr; &infin;. Somewhat loosely but picturesquely, we are
therefore led to refer to the tail of a sequence&mdash;that part of the sequence that is
</p>
<p>arbitrarily far out. In this language we might say that the order of convergence is a
</p>
<p>measure of how good the worst part of the tail is. Larger values of the order p imply,
</p>
<p>in a sense, faster convergence, since the distance from the limit r&lowast; is reduced, at least
in the tail, by the pth power in a single step. Indeed, if the sequence has order p and
</p>
<p>(as is the usual case) the limit
</p>
<p>β = lim
k&rarr;&infin;
</p>
<p>|rk+1 &minus; r&lowast;|
|rk &minus; r&lowast;|p
</p>
<p>exists, then asymptotically we have
</p>
<p>|rk+1 &minus; r&lowast;| = β|rk &minus; r&lowast;|p.
</p>
<p>Example 1. The sequence with rk = a
k where 0 &lt; a &lt; 1 converges to zero with
</p>
<p>order unity, since rk+1/rk = a.
</p>
<p>Example 2. The sequence with rk = a
(2k) for 0 &lt; a &lt; 1 converges to zero with order
</p>
<p>two, since rk+1/r
2
k
= 1.
</p>
<p>Linear Convergence
</p>
<p>Most algorithms discussed in this book have an order of convergence equal to unity.
</p>
<p>It is therefore appropriate to consider this class in greater detail and distinguish
</p>
<p>certain cases within it.
</p>
<p>Definition. If the sequence {rk} converges to r&lowast; in such a way that
</p>
<p>lim
k&rarr;&infin;
</p>
<p>|rk+1 &minus; r&lowast;|
|rk &minus; r&lowast; |
</p>
<p>= β &lt; 1,
</p>
<p>the sequence is said to converge linearly to r&lowast; with convergence ratio (or rate) β.</p>
<p/>
</div>
<div class="page"><p/>
<p>206 7 Basic Properties of Solutions and Algorithms
</p>
<p>Linear convergence is, for our purposes, without doubt the most important type
</p>
<p>of convergence behavior. A linearly convergent sequence, with convergence ratio β,
</p>
<p>can be said to have a tail that converges at least as fast as the geometric sequence
</p>
<p>cβk for some constant c. Thus linear convergence is sometimes referred to as geo-
</p>
<p>metric convergence, although in this book we reserve that phrase for the case when
</p>
<p>a sequence is exactly geometric.
</p>
<p>As a rule, when comparing the relative effectiveness of two competing algorithms
</p>
<p>both of which produce linearly convergent sequences, the comparison is based on
</p>
<p>their corresponding convergence ratios&mdash;the smaller the ratio the faster the rate.
</p>
<p>The ultimate case where β = 0 is referred to as superlinear convergence. We note
</p>
<p>immediately that convergence of any order greater than unity is superlinear, but it is
</p>
<p>also possible for superlinear convergence to correspond to unity order.
</p>
<p>Example 3. The sequence rk = (1/k)
k is of order unity, since rk+1/r
</p>
<p>p
</p>
<p>k
&rarr; &infin; for p &gt; 1.
</p>
<p>However, rk+1/rk &rarr; 0 as k &rarr; &infin; and hence this is superlinear convergence.
</p>
<p>Arithmetic Convergence
</p>
<p>Linear convergence is also called geometric convergence. There is another (slower)
</p>
<p>type of convergence:
</p>
<p>Definition. If the sequence {rk} converges to r&lowast; in such a way that
</p>
<p>|rk &minus; r&lowast;| &le; C
|r0 &minus; r&lowast;|
</p>
<p>kp
, k &ge; 1, 0 &lt; p &lt; &infin;
</p>
<p>where C is a fixed positive number, the sequence is said to converge arithmetically to r&lowast;
</p>
<p>with order p.
</p>
<p>When p = 1, it is referred as arithmetic convergence. The greater of p the faster of
</p>
<p>the convergence.
</p>
<p>Example 4. The sequence rk = 1/k converges to zero arithmetically. The conver-
</p>
<p>gence is of order one but it is not linear, since lim
k&rarr;&infin;
</p>
<p>(rk+1/rk) = 1, that is, β is not
</p>
<p>strictly less than one.
</p>
<p>&lowast;Average Rates
</p>
<p>All the definitions given above can be referred to as step-wise concepts of conver-
</p>
<p>gence, since they define bounds on the progress made by going a single step: from k
</p>
<p>to k + 1. Another approach is to define concepts related to the average progress per
</p>
<p>step over a large number of steps. We briefly illustrate how this can be done.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.8 Speed of Convergence 207
</p>
<p>Definition. Let the sequence {rk} converge to r&lowast;. The average order of convergence is the
infimum of the numbers p &gt; 1 such that
</p>
<p>lim
k&rarr;&infin;
</p>
<p>|rk &minus; r&lowast;|1/p
k
</p>
<p>= 1.
</p>
<p>The order is infinity if the equality holds for no p &gt; 1.
</p>
<p>Example 5. For the sequence rk = a
(2k), 0 &lt; a &lt; 1, given in Example 2, we have
</p>
<p>|rk |1/2
k
</p>
<p>= a,
</p>
<p>while
</p>
<p>|rk|1/p
k
</p>
<p>= a(2/p)
k &rarr; 1
</p>
<p>for p &gt; 2. Thus the average order is two.
</p>
<p>Example 6. For rk = a
k with 0 &lt; a &lt; 1 we have
</p>
<p>(rk)
1/pk = ak(1/p)
</p>
<p>k &rarr; 1
</p>
<p>for any p &gt; 1. Thus the average order is unity.
</p>
<p>As before, the most important case is that of unity order, and in this case we
</p>
<p>define the average convergence ratio as lim
k&rarr;&infin;
</p>
<p>|rk &minus; r&lowast;|1/k. Thus for the geometric
sequence rk = ca
</p>
<p>k, 0 &lt; a &lt; 1, the average convergence ratio is a. Paralleling
</p>
<p>the earlier definitions, the reader can then in a similar manner define corresponding
</p>
<p>notions of average linear and average superlinear convergence.
</p>
<p>Although the above array of definitions can be further embellished and expanded,
</p>
<p>it is quite adequate for our purposes. For the most part we work with the step-wise
</p>
<p>definitions, since in analyzing iterative algorithms it is natural to compare one step
</p>
<p>with the next. In most situations, moreover, when the sequences are well behaved
</p>
<p>and the limits exist in the definitions, then the step-wise and average concepts of
</p>
<p>convergence rates coincide.
</p>
<p>&lowast;Convergence of Vectors
</p>
<p>Suppose {xk}&infin;k=0 is a sequence of vectors in En converging to a vector x&lowast;. The con-
vergence properties of such a sequence are defined with respect to some particular
</p>
<p>function that converts the sequence of vectors into a sequence of numbers. Thus,
</p>
<p>if f is a given continuous function on En, the convergence properties of {xk} can
be defined with respect to f by analyzing the convergence of f (xk) to f (x
</p>
<p>&lowast;). The
function f used in this way to measure convergence is called the error function.
</p>
<p>In optimization theory it is common to choose the error function by which to
</p>
<p>measure convergence as the same function that defines the objective function of the
</p>
<p>original optimization problem. This means we measure convergence by how fast the</p>
<p/>
</div>
<div class="page"><p/>
<p>208 7 Basic Properties of Solutions and Algorithms
</p>
<p>objective converges to its minimum. alternatively, we sometimes use the function
</p>
<p>|x &minus; x&lowast;|2 and thereby measure convergence by how fast the (squared) distance from
the solution point decreases to zero.
</p>
<p>Generally, the order of convergence of a sequence is insensitive to the particular
</p>
<p>error function used; but for step-wise linear convergence the associated convergence
</p>
<p>ratio is not. Nevertheless, the average convergence ratio is not too sensitive, as the
</p>
<p>following proposition demonstrates, and hence the particular error function used to
</p>
<p>measure convergence is not really very important.
</p>
<p>Proposition. Let f and g be two error functions satisfying f (x&lowast;) = g(x&lowast;) = 0 and, for all x,
a relation of the form
</p>
<p>0 � a1g(x) � f (x) � a2g(x)
</p>
<p>for some fixed a1 &gt; 0, a2 &gt; 0. If the sequence {xk}&infin;k=0 converges to x&lowast; linearly with average
ratio β with respect to one of these functions, it also does so with respect to the other.
</p>
<p>Proof. The statement is easily seen to be symmetric in f and g. Thus we assume
</p>
<p>{xk} is linearly convergent with average convergence ratio β with respect to f , and
will prove that the same is true with respect to g. We have
</p>
<p>β = lim
k&rarr;&infin;
</p>
<p>f (xk)
1/k
� lim
</p>
<p>k&rarr;&infin;
a
</p>
<p>1/k
2
</p>
<p>g(xk)
1/k = lim
</p>
<p>k&rarr;&infin;
g(xk)
</p>
<p>1/k
</p>
<p>and
</p>
<p>β = lim
k&rarr;&infin;
</p>
<p>f (xk)
1/k
� lim
</p>
<p>k&rarr;&infin;
a
</p>
<p>1/k
1
</p>
<p>g(xk)
1/k = lim
</p>
<p>k&rarr;&infin;
g(xk)
</p>
<p>1/k.
</p>
<p>Thus
</p>
<p>β = lim
k&rarr;&infin;
</p>
<p>g(xk)
1/k. �
</p>
<p>As an example of an application of the above proposition, consider the case
</p>
<p>where g(x) = |x &minus; x&lowast;|2 and f (x) = (x &minus; x&lowast;)TQ(x &minus; x&lowast;), where Q is a positive defi-
nite symmetric matrix. Then a1 and a2 correspond, respectively, to the smallest and
</p>
<p>largest eigenvalues of Q. Thus average linear convergence is identical with respect
</p>
<p>to any error function constructed from a positive definite quadratic form.
</p>
<p>Complexity
</p>
<p>Complexity theory as outlined in Sect. 5.1 is an important aspect of convergence
</p>
<p>theory. This theory can be used in conjunction with the theory of local convergence.
</p>
<p>If an algorithm converges according to any order greater than zero, then for a fixed
</p>
<p>problem, the sequence generated by the algorithm will converge in a time that is a
</p>
<p>function of the convergence order (and rate, if convergence is linear). For example,
</p>
<p>if the order is one with rate 0 &lt; c &lt; 1 and the process begins with an error of R,
</p>
<p>a final error of r can be achieved by a number of steps n satisfying cnR � r. Thus
</p>
<p>it requires approximately n = log(R/r)/ log(1/c) steps. In this form the number of
</p>
<p>steps is not affected by the size of the problem. However, problem size enters in
</p>
<p>two possible ways. First, the rate c may depend on the size-say going toward 1 as</p>
<p/>
</div>
<div class="page"><p/>
<p>7.10 Exercises 209
</p>
<p>the size increases so that the speed is slower for large problems. The second way
</p>
<p>that size may enter, and this is the more important way, is that the time to exe-
</p>
<p>cute a single step almost always increases with problem size. For instance if, for a
</p>
<p>problem seeking an optimal vector of dimension m, each step requires a Gaussian
</p>
<p>elimination inversion of an m &times;m matrix, the solution time will increase by a factor
proportional to m3. Overall the algorithm is therefore a polynomial time algorithm.
</p>
<p>Essentially all algorithms in this book employ steps, such as matrix multiplications
</p>
<p>or inversion or other algebraic operations, which are polynomial-time in character.
</p>
<p>Convergence analysis, therefore, focuses on whether an algorithm is globally con-
</p>
<p>vergent, on its local convergence properties, and also on the order of the algebraic
</p>
<p>operations required to execute the steps required. The last of these is usually easily
</p>
<p>deduced by listing the number and size of the required vector and matrix operations.
</p>
<p>7.9 Summary
</p>
<p>There are two different but complementary ways to characterize the solution to
</p>
<p>unconstrained optimization problems. In the local approach, one examines the re-
</p>
<p>lation of a given point to its neighbors. This leads to the conclusion that, at an
</p>
<p>unconstrained relative minimum point of a smooth function, the gradient of the
</p>
<p>function vanishes and the Hessian is positive semidefinite; and conversely, if at a
</p>
<p>point the gradient vanishes and the Hessian is positive definite, that point is a rel-
</p>
<p>ative minimum point. This characterization has a natural extension to the global
</p>
<p>approach where convexity ensures that if the gradient vanishes at a point, that point
</p>
<p>is a global minimum point.
</p>
<p>In considering iterative algorithms for finding either local or global minimum
</p>
<p>points, there are two distinct issues: global convergence properties and local con-
</p>
<p>vergence properties. The first is concerned with whether starting at an arbitrary
</p>
<p>point the sequence generated will converge to a solution. This is ensured if the
</p>
<p>algorithm is closed, has a descent function, and generates a bounded sequence. It
</p>
<p>is also explained that global convergence is guaranteed simply by the inclusion, in
</p>
<p>a complex algorithm, of spacer steps. This result is called upon frequently in what
</p>
<p>follows. Local convergence properties are a measure of the ultimate speed of con-
</p>
<p>vergence and generally determine the relative advantage of one algorithm to another.
</p>
<p>7.10 Exercises
</p>
<p>1. To approximate a function g over the interval [0, 1] by a polynomial p of degree
</p>
<p>n (or less), we minimize the criterion
</p>
<p>f (a) =
</p>
<p>&int; 1
</p>
<p>0
</p>
<p>[g(x) &minus; p(x)]2dx,
</p>
<p>where p(x) = anx
n + an&minus;1xn&minus;1 + . . . + a0. Find the equations satisfied by the
</p>
<p>optimal coefficients a = (a0, a1, . . . , an).</p>
<p/>
</div>
<div class="page"><p/>
<p>210 7 Basic Properties of Solutions and Algorithms
</p>
<p>2. In Example 4 of Sect. 7.2 show that if the solution has x1 &gt; 0, x1 + x2 = 1, then
</p>
<p>it is necessary that
</p>
<p>b1 &minus; b2 + (c1 &minus; c2)h(x1) = 0
b2 + (c2 &minus; c3)h(x1 + x2) � 0.
</p>
<p>Hint: One way is to reformulate the problem in terms of the variables x1 and
</p>
<p>y = x1 + x2.
</p>
<p>3. (a) Using the first-order necessary conditions, find a minimum point of the
</p>
<p>function
</p>
<p>f (x, y, z) = 2x2 + xy + y2 + yz + z2 &minus; 6x &minus; 7y &minus; 8z + 9.
</p>
<p>(b) Verify that the point is a relative minimum point by verifying that the
</p>
<p>second-order sufficiency conditions hold.
</p>
<p>(c) Prove that the point is a global minimum point.
</p>
<p>4. In this exercise and the next we develop a method for determining whether a
</p>
<p>given symmetric matrix is positive definite. Given an n &times; n matrix A let Ak
denote the principal submatrix made up of the first k rows and columns. Show
</p>
<p>(by induction) that if the first n &minus; 1 principal submatrices are nonsingular, then
there is a unique lower triangular matrix L with unit diagonal and a unique
</p>
<p>upper triangular matrix U such that A = LU. (See Appendix C.)
</p>
<p>5. A symmetric matrix is positive definite if and only if the determinant of each
</p>
<p>of its principal submatrices is positive. Using this fact and the considerations of
</p>
<p>Exercise 4, show that an n&times;n symmetric matrix A is positive definite if and only
if it has an LU decomposition (without interchange of rows) and the diagonal
</p>
<p>elements of U are all positive.
</p>
<p>6. Using Exercise 5 show that an n&times; n matrix A is symmetric and positive definite
if and only if it can be written as A = GGT where G is a lower triangular matrix
</p>
<p>with positive diagonal elements. This representation is known as the Cholesky
</p>
<p>factorization of A.
</p>
<p>7. Let f j, i &isin; I be a collection of convex functions defined on a convex set Ω.
Show that the function f defined by f (x) = sup
</p>
<p>i&isin;I
fi(x) is convex on the region
</p>
<p>where it is finite.
</p>
<p>8. Let γ be a monotone nondecreasing function of a single variable (that is, γ(r) �
</p>
<p>γ(r&prime;) for r&prime; &gt; r) which is also convex; and let f be a convex function defined
on a convex set Ω. Show that the function γ( f ) defined by γ( f )(x) = γ[ f (x)] is
</p>
<p>convex on Ω.
</p>
<p>9. Let f be twice continuously differentiable on a region Ω &sub; En. Show that a
sufficient condition for a point x&lowast; in the interior of Ω to be a relative minimum
point of f is that &nabla; f (x&lowast;) = 0 and that f be locally convex at x&lowast;.</p>
<p/>
</div>
<div class="page"><p/>
<p>References 211
</p>
<p>10. Define the point-to-set mapping on En by
</p>
<p>A(x) = {y : yTx � b},
</p>
<p>where b is a fixed constant. Is A closed?
</p>
<p>11. Prove the two corollaries in Sect. 7.6 on the closedness of composite mappings.
</p>
<p>12. Show that if A is a continuous point-to-point mapping, the Global Conver-
</p>
<p>gence Theorem is valid even without assumption (i). Compare with Example 2,
</p>
<p>Sect. 7.7.
</p>
<p>13. Let {rk}&infin;k=0 and {ck}&infin;k=0 be sequences of real numbers. Suppose rk &rarr; 0 average
linearly and that there are constants c &gt; 0 and C such that c � ck � C for all k.
</p>
<p>Show that ckrk &rarr; 0 average linearly.
14. Prove a proposition, similar to the one in Sect. 7.8, showing that the order of
</p>
<p>convergence is insensitive to the error function.
</p>
<p>15. Show that if rk &rarr; r&lowast; (step-wise) linearly with convergence ratio β, then rk &rarr;
r&lowast;(average) linearly with average convergence ratio no greater than β.
</p>
<p>References
</p>
<p>7.1&ndash;7.5 For alternative discussions of the material in these sections, see Hadley
</p>
<p>[H2], Fiacco and McCormick [F4], Zangwill [Z2] and Luenberger [L8].
</p>
<p>7.6 Although the general concepts of this section are well known, the formula-
</p>
<p>tion as zero-order conditions appears to be new.
</p>
<p>7.7 The idea of using a descent function (usually the objective itself) in order
</p>
<p>to guarantee convergence of minimization algorithms is an old one that
</p>
<p>runs through most literature on optimization, and has long been used to
</p>
<p>establish global convergence. Formulation of the general Global Conver-
</p>
<p>gence Theorem, which captures the essence of many previously diverse
</p>
<p>arguments, and the idea of representing an algorithm as a point-to-set map-
</p>
<p>ping are both due to Zangwill [Z2]. A version of the Spacer Step Theorem
</p>
<p>can be found in Zangwill [Z2] as well.
</p>
<p>7.8 Most of the definitions given in this section have been standard for quite
</p>
<p>some time. A thorough discussion which contributes substantially to the
</p>
<p>unification of these concepts is contained in Ortega and Rheinboldt [O7].</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 8
</p>
<p>Basic Descent Methods
</p>
<p>We turn now to a description of the basic techniques used for iteratively solving
</p>
<p>unconstrained minimization problems. These techniques are, of course, important
</p>
<p>for practical application since they often offer the simplest, most direct alternatives
</p>
<p>for obtaining solutions; but perhaps their greatest importance is that they establish
</p>
<p>certain reference plateaus with respect to difficulty of implementation and speed
</p>
<p>of convergence. Thus in later chapters as more efficient techniques and techniques
</p>
<p>capable of handling constraints are developed, reference is continually made to the
</p>
<p>basic techniques of this chapter both for guidance and as points of comparison.
</p>
<p>There is a fundamental underlying structure for almost all the descent algorithms
</p>
<p>we discuss. One starts at an initial point; determines, according to a fixed rule, a
</p>
<p>direction of movement; and then moves in that direction to a (relative) minimum of
</p>
<p>the objective function on that line. At the new point a new direction is determined
</p>
<p>and the process is repeated. The primary differences between algorithms (steepest
</p>
<p>descent, Newton&rsquo;s method, etc.) rest with the rule by which successive directions of
</p>
<p>movement are selected. Once the selection is made, all algorithms call for movement
</p>
<p>to the minimum point on the corresponding line.
</p>
<p>The process of determining the minimum point on a given line (one variable
</p>
<p>only) is called line search. For general nonlinear functions that cannot be minimized
</p>
<p>analytically, this process actually is accomplished by searching, in an intelligent
</p>
<p>manner, along the line for the minimum point. These line search techniques, which
</p>
<p>are really procedures for solving one-dimensional minimization problems, form the
</p>
<p>backbone of nonlinear programming algorithms, since higher dimensional problems
</p>
<p>are ultimately solved by executing a sequence of successive line searches. There are
</p>
<p>a number of different approaches to this important phase of minimization and the
</p>
<p>first half of this chapter is devoted to their, discussion.
</p>
<p>The last sections of the chapter are devoted to a description and analysis of
</p>
<p>the basic descent algorithms for unconstrained problems; steepest descent, coor-
</p>
<p>dinate descent, and Newton&rsquo;s method. These algorithms serve as primary models
</p>
<p>for the development and analysis of all others discussed in the book.
</p>
<p>&copy; Springer International Publishing Switzerland 2016
</p>
<p>D.G. Luenberger, Y. Ye, Linear and Nonlinear Programming, International
Series in Operations Research &amp; Management Science 228,
DOI 10.1007/978-3-319-18842-3 8
</p>
<p>213</p>
<p/>
</div>
<div class="page"><p/>
<p>214 8 Basic Descent Methods
</p>
<p>8.1 Line Search Algorithms
</p>
<p>These algorithms are classified by the order of information of the objective functions
</p>
<p>f (x) being evaluated.
</p>
<p>0th-Order Method: Golden Section Search and Curve Fitting
</p>
<p>A very popular method for resolving the line search problem is the Fibonacci search
</p>
<p>method described in this section. The method has a certain degree of theoretical
</p>
<p>elegance, which no doubt partially accounts for its popularity, but on the whole, as
</p>
<p>we shall see, there are other procedures which in most circumstances are superior.
</p>
<p>The method determines the minimum value of a function f over a closed interval
</p>
<p>[c1, c2]. In applications, f may in fact be defined over a broader domain, but for
</p>
<p>this method a fixed interval of search must be specified. The only property that is
</p>
<p>assumed of f is that it is unimodal, that is, it has a single relative minimum (see
</p>
<p>Fig. 8.1). The minimum point of f is to be determined, at least approximately, by
</p>
<p>measuring the value of f at a certain number of points. It should be imagined, as is
</p>
<p>indeed the case in the setting of nonlinear programming, that each measurement of
</p>
<p>f is somewhat costly&mdash;of time if nothing more.
</p>
<p>To develop an appropriate search strategy, that is, a strategy for selecting mea-
</p>
<p>surement points based on the previously obtained values, we pose the following
</p>
<p>problem: Find how to successively select N measurement points so that, without
</p>
<p>explicit knowledge of f , we can determine the smallest possible region of uncer-
</p>
<p>tainty in which the minimum must lie. In this problem the region of uncertainty is
</p>
<p>determined in any particular case by the relative values of the measured points in
</p>
<p>conjunction with our assumption that f is unimodal. Thus, after values are known
</p>
<p>at N points x1, x2, . . . , xN with
</p>
<p>c1 � x1 &lt; x2 . . . &lt; xN&minus;1 &lt; xN � c2,
</p>
<p>the region of uncertainty is the interval [xk&minus;1, xk+1] where xk is the minimum point
among the N, and we define x0 = c1, xN+1 = c2 for consistency. The minimum of f
</p>
<p>must lie somewhere in this interval.
</p>
<p>The derivation of the optimal strategy for successively selecting measurement
</p>
<p>points to obtain the smallest region of uncertainty is fairly straight-forward but
</p>
<p>somewhat tedious. We simply state the result and give an example.
</p>
<p>Let
</p>
<p>d1 = c2 &minus; c1, the initial width of uncertainty
dk = width of uncertainty after k measurements</p>
<p/>
</div>
<div class="page"><p/>
<p>8.1 Line Search Algorithms 215
</p>
<p>Fig. 8.1 A unimodal function
</p>
<p>Then, if a total of N measurements are to be made, we have
</p>
<p>dk =
</p>
<p>(
</p>
<p>FN&minus;k+1
FN
</p>
<p>)
</p>
<p>d1, (8.1)
</p>
<p>where the integers Fk are members of the Fibonacci sequence generated by the
</p>
<p>recurrence relation
</p>
<p>FN = FN&minus;1 + FN&minus;2, F0 = F1 = 1. (8.2)
</p>
<p>The resulting sequence is 1, 1, 2, 3, 5, 8, 13, . . . .
</p>
<p>The procedure for reducing the width of uncertainty to dN is this: The first two
</p>
<p>measurements are made symmetrically at a distance of (FN&minus;1/FN)d1 from the ends
of the initial intervals; according to which of these is of lesser value, an uncertainty
</p>
<p>interval of width d2 = (FN&minus;1/FN)d1 is determined. The third measurement point is
placed symmetrically in this new interval of uncertainty with respect to the measure-
</p>
<p>ment already in the interval. The result of this third measurement gives an interval
</p>
<p>of uncertainty d3 = (FN&minus;2/FN)d1. In general, each successive measurement point
is placed in the current interval of uncertainty symmetrically with the point already
</p>
<p>existing in that interval.
</p>
<p>Some examples are shown in Fig. 8.2. In these examples the sequence of mea-
</p>
<p>surement points is determined in accordance with the assumption that each measure-
</p>
<p>ment is of lower value than its predecessors. Note that the procedure always calls
</p>
<p>for the last two measurements to be made at the midpoint of the semifinal interval of
</p>
<p>uncertainty. We are to imagine that these two points are actually separated a small
</p>
<p>distance so that a comparison of their respective values will reduce the interval to
</p>
<p>nearly half. This terminal anomaly of the Fibonacci search process is, of course, of
</p>
<p>no great practical consequence.</p>
<p/>
</div>
<div class="page"><p/>
<p>216 8 Basic Descent Methods
</p>
<p>Search by Golden Section
</p>
<p>If the number N of allowed measurement points in a Fibonacci search is made to
</p>
<p>approach infinity, we obtain the golden section method. It can be argued, based on
</p>
<p>the optimal property of the finite Fibonacci method, that the corresponding infinite
</p>
<p>version yields a sequence of intervals of uncertainty whose widths tend to zero faster
</p>
<p>than that which would be obtained by other methods.
</p>
<p>Fig. 8.2 Fibonacci search
</p>
<p>The solution to the Fibonacci difference equation
</p>
<p>FN = FN&minus;1 + FN&minus;2 (8.3)
</p>
<p>is of the form
</p>
<p>FN = Aτ
N
1 + Bτ
</p>
<p>N
2 , (8.4)
</p>
<p>where τ1 and τ2 are roots of the characteristic equation
</p>
<p>τ2 = τ + 1.
</p>
<p>Explicitly,
</p>
<p>τ1 =
1 +
</p>
<p>&radic;
5
</p>
<p>2
, τ2 =
</p>
<p>1 &minus;
&radic;
</p>
<p>5
</p>
<p>2
.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.1 Line Search Algorithms 217
</p>
<p>(The number τ1 ≃ 1.618 is known as the golden section ratio and was considered by
early Greeks to be the most aesthetic value for the ratio of two adjacent sides of a
</p>
<p>rectangle.) For large N the first term on the right side of (8.4) dominates the second,
</p>
<p>and hence
</p>
<p>lim
N&rarr;&infin;
</p>
<p>FN&minus;1
FN
</p>
<p>=
1
</p>
<p>τ1
≃ 0.618.
</p>
<p>It follows from (8.1) that the interval of uncertainty at any point in the process has
</p>
<p>width
</p>
<p>dk =
</p>
<p>(
</p>
<p>1
</p>
<p>τ1
</p>
<p>)k&minus;1
d1, (8.5)
</p>
<p>and from this it follows that
</p>
<p>dk+1
</p>
<p>dk
=
</p>
<p>1
</p>
<p>τ1
= 0.618. (8.6)
</p>
<p>Therefore, we conclude that, with respect to the width of the uncertainty interval, the
</p>
<p>search by golden section converges linearly (see Sect. 7.8) to the overall minimum
</p>
<p>of the function f with convergence ratio 1/τ1 = 0.618.
</p>
<p>The Fibonacci search method has a certain amount of theoretical appeal, since it
</p>
<p>assumes only that the function being searched is unimodal and with respect to this
</p>
<p>broad class of functions the method is, in some sense, optimal. In most problems,
</p>
<p>however, it can be safely assumed that the function being searched, as well as being
</p>
<p>unimodal, possesses a certain degree of smoothness, and one might, therefore, ex-
</p>
<p>pect that more efficient search techniques exploiting this smoothness can be devised;
</p>
<p>and indeed they can. Techniques of this nature are usually based on curve fitting pro-
</p>
<p>cedures where a smooth curve is passed through the previously measured points in
</p>
<p>order to determine an estimate of the minimum point. A variety of such techniques
</p>
<p>can be devised depending on whether or not derivatives of the function as well as the
</p>
<p>values can be measured, how many previous points are used to determine the fit, and
</p>
<p>the criterion used to determine the fit. In this section a number of possibilities are
</p>
<p>outlined and analyzed. All of them have orders of convergence greater than unity.
</p>
<p>Quadratic Fit
</p>
<p>The scheme that is often most useful in line searching is that of fitting a quadratic
</p>
<p>through three given points. This has the advantage of not requiring any deriva-
</p>
<p>tive information. Given x1, x2, x3 and corresponding values f (x1) = f1, f (x2) =
</p>
<p>f2, f (x3) = f3 we construct the quadratic passing through these points
</p>
<p>q(x) =
</p>
<p>3
&sum;
</p>
<p>i=1
</p>
<p>fi
</p>
<p>&prod;
</p>
<p>j�i(x &minus; x j)
&prod;
</p>
<p>j�i(xi &minus; x j)
, (8.7)</p>
<p/>
</div>
<div class="page"><p/>
<p>218 8 Basic Descent Methods
</p>
<p>and determine a new point x4 as the point where the derivative of q vanishes. Thus
</p>
<p>x4 =
1
</p>
<p>2
</p>
<p>b23 f1 + b31 f2 + b12 f3
</p>
<p>a23 f1 + a31 f2 + a12 f3
, (8.8)
</p>
<p>where ai j = xi &minus; x j, bi j = x2i &minus; x2j .
Define the errors εi = x
</p>
<p>&lowast; &minus; xi, i = 1, 2, 3, 4. The expression for ε4 must be a
polynomial in ε1, ε2, ε3. It must be second order (since it is a quadratic fit). It must
</p>
<p>go to zero if any two of the errors ε1, ε2, ε3 is zero. (The reader should check this.)
</p>
<p>Finally, it must be symmetric (since the order of points is relevant). It follows that
</p>
<p>near a minimum point x&lowast; of f , the errors are related approximately by
</p>
<p>ε4 = M(ε1ε2 + ε2ε3 + ε1ε3), (8.9)
</p>
<p>where M depends on the values of the second and third derivatives of f at x&lowast;.
If we assume that εk &rarr; 0 with an order greater than unity, then for large k the
</p>
<p>error is governed approximately by
</p>
<p>εk+2 = Mεkεk&minus;1.
</p>
<p>Letting yk = log Mεk this becomes
</p>
<p>yk+2 = yk + yk&minus;1
</p>
<p>with characteristic equation
</p>
<p>λ3 &minus; λ &minus; 1 = 0.
The largest root of this equation is λ ≃ 1.3 which thus determines the rate of growth
of yk and is the order of convergence of the quadratic fit method.
</p>
<p>1st-Order Method: Curve Fitting and Methods of False Position
</p>
<p>In this section a number fitting methods using the first derivative information are
</p>
<p>described. All of them have orders of convergence greater than unity.
</p>
<p>Quadratic Fit: Method of False Position
</p>
<p>Suppose that at two points xk and xk&minus;1 where measurements f (xk), f &prime;(xk), f &prime;(xk&minus;1)
are available, it is possible to fit the quadratic
</p>
<p>q(x) = f (xk) + f
&prime;(xk)(x &minus; xk) +
</p>
<p>f &prime;(xk&minus;1) &minus; f &prime;(xk)
xk&minus;1 &minus; xk
</p>
<p>&middot; (x &minus; xk)
2
</p>
<p>2
,</p>
<p/>
</div>
<div class="page"><p/>
<p>8.1 Line Search Algorithms 219
</p>
<p>which has the same corresponding values. An estimate xk+1 can then be determined
</p>
<p>by finding the point where the derivative of q vanishes; thus
</p>
<p>xk+1 = xk &minus; f &prime;(xk)
[
</p>
<p>xk&minus;1 &minus; xk
f &prime;(xk&minus;1) &minus; f &prime;(xk)
</p>
<p>]
</p>
<p>. (8.10)
</p>
<p>(See Fig. 8.3.) Comparing this formula with Newton&rsquo;s method, we see again that
</p>
<p>the value f (xk) does not enter; hence, our fit could have been passed through either
</p>
<p>f (xk) or f (xk&minus;1). Also the formula can be regarded as an approximation to New-
ton&rsquo;s method where the second derivative is replaced by the difference of two first
</p>
<p>derivatives.
</p>
<p>Fig. 8.3 False position for minimization
</p>
<p>Again, since this method does not depend on values of f directly, it can be
</p>
<p>regarded as a method for solving f &prime;(x) &equiv; g(x) = 0. Viewed in this way the method,
which is illustrated in Fig. 8.4, takes the form
</p>
<p>xk+1 = xk &minus; g(xk)
[
</p>
<p>xk &minus; xk&minus;1
g(xk) &minus; g(xk&minus;1)
</p>
<p>]
</p>
<p>. (8.11)
</p>
<p>We next investigate the order of convergence of the method of false position and
</p>
<p>discover that it is order τ1 ≃ 1.618, the golden mean.
Proposition. Let g have a continuous second derivative and suppose x&lowast; is such that g(x&lowast;) =
0, g&prime;(x&lowast;) � 0. Then for x0 sufficiently close to x&lowast;, the sequence {xk}&infin;k=0 generated by the
method of false position (8.11) converges to x&lowast; with order τ1 ≃ 1.618.
</p>
<p>Proof. Introducing the notation
</p>
<p>g[a, b] =
g(b) &minus; g(a)
</p>
<p>b &minus; a , (8.12)</p>
<p/>
</div>
<div class="page"><p/>
<p>220 8 Basic Descent Methods
</p>
<p>Fig. 8.4 False position for solving equations
</p>
<p>we have
</p>
<p>xk&minus;1 &minus; x&lowast; = xk &minus; x&lowast; &minus; g(xk)
[
</p>
<p>xk &minus; xk&minus;1
g(xk) &minus; g(xk&minus;1)
</p>
<p>]
</p>
<p>= (xk &minus; x&lowast;)
{
</p>
<p>g[xk&minus;1, xk] &minus; g[xk, x&lowast;]
g[xk&minus;1, xk]
</p>
<p>}
</p>
<p>. (8.13)
</p>
<p>Further, upon the introduction of the notation
</p>
<p>g[a, b, c] =
g[a, b]&minus; g[b, c]
</p>
<p>a &minus; c ,
</p>
<p>we may write (8.13) as
</p>
<p>xk+1 &minus; x&lowast; = (xk &minus; x&lowast;)(xk&minus;1 &minus; x&lowast;)
{
</p>
<p>g[xk&minus;1, xk, x&lowast;]
</p>
<p>g[xk&minus;1, xk]
</p>
<p>}
</p>
<p>.
</p>
<p>Now, by the mean value theorem with remainder, we have (see Exercise 2)
</p>
<p>g[xk&minus;1, xk] = g
&prime;(ξk) (8.14)
</p>
<p>and
</p>
<p>g[xk&minus;1, xk, x
&lowast;] =
</p>
<p>1
</p>
<p>2
g&prime;&prime;(ηk), (8.15)
</p>
<p>where ξk and ηk are convex combinations of xk, xk&minus;1 and xk, xk&minus;1, x&lowast;, respectively.
Thus
</p>
<p>xk+1 &minus; x&lowast; =
g&prime;&prime;(ηk)
</p>
<p>2g&prime;(ξk)
(xk &minus; x&lowast;)(xk&minus;1 &minus; x&lowast;). (8.16)
</p>
<p>It follows immediately that the process converges if it is started sufficiently close
</p>
<p>to x&lowast;.
To determine the order of convergence, we note that for large k Eq. (8.16) be-
</p>
<p>comes approximately</p>
<p/>
</div>
<div class="page"><p/>
<p>8.1 Line Search Algorithms 221
</p>
<p>xk+1 &minus; x&lowast; = M(xk &minus; x&lowast;)(xk&minus;1 &minus; x&lowast;),
</p>
<p>where
</p>
<p>M =
g&prime;&prime;(x&lowast;)
</p>
<p>2g&prime;(x&lowast;)
.
</p>
<p>Thus defining εk = (xk &minus; x&lowast;) we have, in the limit,
</p>
<p>εk+1 = Mεkεk&minus;1. (8.17)
</p>
<p>Taking the logarithm of this equation we have, with yk = log Mεk,
</p>
<p>yk+1 = yk + yk&minus;1, (8.18)
</p>
<p>which is the Fibonacci difference equation discussed in Sect. 7.1. A solution to this
</p>
<p>equation will satisfy
</p>
<p>yk+1 &minus; τ1yk &rarr; 0.
Thus
</p>
<p>log Mεk+1 &minus; τ1 log Mεk &rarr; 0 or log
Mεk+1
</p>
<p>(Mεk)τ1
&rarr; 0,
</p>
<p>and hence
εk+1
</p>
<p>ε
τ1
k
</p>
<p>&rarr; M(τ1&minus;1).�
</p>
<p>Having derived the error formula (8.17) by direct analysis, it is now appropriate
</p>
<p>to point out a short-cut technique, based on symmetry and other considerations,
</p>
<p>that can sometimes be used in even more complicated situations. The right side of
</p>
<p>error formula (8.17) must be a polynomial in εk and εk&minus;1, since it is derived from
approximations based on Taylor&rsquo;s theorem. Furthermore, it must be second order,
</p>
<p>since the method reduces to Newton&rsquo;s method when xk = xk&minus;1. Also, it must go
to zero if either εk or εk&minus;1 go to zero, since the method clearly yields εk+1 = 0 in
that case. Finally, it must be symmetric in εk and εk&minus;1, since the order of points is
irrelevant. The only formula satisfying these requirements is εk+1 = Mεkεk&minus;1.
</p>
<p>Cubic Fit
</p>
<p>Given the points xk&minus;1 and xk together with the values f (xk&minus;1), f &prime;(xk&minus;1), f (xk), f &prime;(xk),
it is also possible to fit a cubic equation to the points having corresponding values.
</p>
<p>The next point xk+1 can then be determined as the relative minimum point of this
</p>
<p>cubic. This leads to
</p>
<p>xk+1 = xk &minus; (xk &minus; xk&minus;1)
[
</p>
<p>f &prime;(xk) + u2 &minus; u1
f &prime;(xk) &minus; f &prime;(xk&minus;1) + 2u2
</p>
<p>]
</p>
<p>, (8.19)
</p>
<p>where
</p>
<p>u1 = f
&prime;(xk&minus;1) + f
</p>
<p>&prime;(xk) &minus; 3
f (xk&minus;1) &minus; f (xk)
</p>
<p>xk&minus;1 &minus; xk
u2 = [u
</p>
<p>2
1 &minus; f &prime;(xk&minus;1) f &prime;(xk)]1/2,</p>
<p/>
</div>
<div class="page"><p/>
<p>222 8 Basic Descent Methods
</p>
<p>which is easily implementable for computations.
</p>
<p>It can be shown (see Exercise 3) that the order of convergence of the cubic fit
</p>
<p>method is 2.0. Thus, although the method is exact for cubic functions indicating
</p>
<p>that its order might be three, its order is actually only two.
</p>
<p>2nd-Order Method: Newton&rsquo;s Method
</p>
<p>Suppose that the function f of a single variable x is to be minimized, and suppose
</p>
<p>that at a point xk where a measurement is made it is possible to evaluate the three
</p>
<p>numbers f (xk), f
&prime;(xk), f &prime;&prime;(xk). It is then possible to construct a quadratic function
</p>
<p>q which at xk agrees with f up to second derivatives, that is
</p>
<p>q(x) = f (xk) + f
&prime;(xk)(x &minus; xk) +
</p>
<p>1
</p>
<p>2
f &prime;&prime;(xk)(x &minus; xk)2. (8.20)
</p>
<p>We may then calculate an estimate xk+1 of the minimum point of f by finding the
</p>
<p>point where the derivative of q vanishes. Thus setting
</p>
<p>0 = q&prime;(xk+1) = f
&prime;(xk) + f
</p>
<p>&prime;&prime;(xk)(xk+1 &minus; xk),
</p>
<p>Fig. 8.5 Newton&rsquo;s method for minimization
</p>
<p>we find
</p>
<p>xk+1 = xk &minus;
f &prime;(xk)
</p>
<p>f &prime;&prime;(xk)
. (8.21)
</p>
<p>This process, which is illustrated in Fig. 8.5, can then be repeated at xk+1.
</p>
<p>We note immediately that the new point xk+1 resulting from Newton&rsquo;s method
</p>
<p>does not depend on the value f (xk). The method can more simply be viewed as a
</p>
<p>technique for iteratively solving equations of the form
</p>
<p>g(x) = 0,</p>
<p/>
</div>
<div class="page"><p/>
<p>8.1 Line Search Algorithms 223
</p>
<p>where, when applied to minimization, we put g(x) &equiv; f &prime;(x). In this notation Newton&rsquo;s
method takes the form
</p>
<p>xk+1 = xk &minus;
g(xk)
</p>
<p>g&prime;(xk)
. (8.22)
</p>
<p>This form is illustrated in Fig. 8.6.
</p>
<p>We now show that Newton&rsquo;s method has order two convergence:
</p>
<p>Proposition. Let the function g have a continuous second derivative, and let x&lowast; satisfy
g(x&lowast;) = 0, g&prime;(x&lowast;) � 0. Then, provided x0 is sufficiently close to x&lowast;, the sequence {xk}&infin;k=0
generated by Newton&rsquo;s method (8.22) converges to x&lowast; with an order of convergence at least
two.
</p>
<p>Proof. For points ξ in a region near x&lowast; there is a k1 such that |g&prime;&prime;(ξ)| &lt; k1 and a k2
such that |g&prime;(ξ)| &gt; k2. Then since g(x&lowast;) = 0 we can write
</p>
<p>xk+1 &minus; x&lowast; = xk &minus; x&lowast; &minus;
g(xk) &minus; g(x&lowast;)
</p>
<p>g&prime;(xk)
</p>
<p>= &minus;[g(xk) &minus; g(x&lowast;) + g&prime;(xk)(x&lowast; &minus; xk)]/g&prime;(xk).
</p>
<p>Fig. 8.6 Newton&rsquo;s method for solving equations
</p>
<p>The term in brackets is, by Taylor&rsquo;s theorem, zero to first-order. In fact, using the
</p>
<p>remainder term in a Taylor series expansion about xk, we obtain
</p>
<p>xk+1 &minus; x&lowast; =
1
</p>
<p>2
</p>
<p>g&prime;&prime;(ξ)
</p>
<p>g&prime;(xk)
(xk &minus; x&lowast;)2
</p>
<p>for some ξ between x&lowast; and xk. Thus in the region near x&lowast;,
</p>
<p>|xk+1 &minus; x&lowast;| �
k1
</p>
<p>2k2
|xk &minus; x&lowast;|2.
</p>
<p>We see that if |xk &minus; x&lowast;|k1/2k2 &lt; 1, then |xk+1 &minus; x&lowast;| &lt; |xk &minus; x&lowast;| and thus we conclude
that if started close enough to the solution, the method will converge to x&lowast; with an
</p>
<p>order of convergence at least two. �</p>
<p/>
</div>
<div class="page"><p/>
<p>224 8 Basic Descent Methods
</p>
<p>Global Convergence of Curve Fitting
</p>
<p>Above, we analyzed the convergence of various curve fitting procedures in the
</p>
<p>neighborhood of the solution point. If, however, any of these procedures were
</p>
<p>applied in pure form to search a line for a minimum, there is the danger&mdash;alas,
</p>
<p>the most likely possibility&mdash;that the process would diverge or wander about mean-
</p>
<p>inglessly. In other words, the process may never get close enough to the solution for
</p>
<p>our detailed local convergence analysis to be applicable. It is therefore important to
</p>
<p>artfully combine our knowledge of the local behavior with conditions guaranteeing
</p>
<p>global convergence to yield a workable and effective procedure.
</p>
<p>The key to guaranteeing global convergence is the Global Convergence Theorem
</p>
<p>of Chap. 7. Application of this theorem in turn hinges on the construction of a suit-
</p>
<p>able descent function and minor modifications of a pure curve fitting algorithm. We
</p>
<p>offer below a particular blend of this kind of construction and analysis, taking as
</p>
<p>departure point the quadratic fit procedure discussed in Sect. 8.1 above.
</p>
<p>Let us assume that the function f that we wish to minimize is strictly unimodal
</p>
<p>and has continuous second partial derivatives. We initiate our search procedure by
</p>
<p>searching along the line until we find three points x1, x2, x3 with x1 &lt; x2 &lt; x3 such
</p>
<p>that f (x1) � f (x2) � f (x3). In other words, the value at the middle of these three
</p>
<p>points is less than that at either end. Such a sequence of points can be determined in
</p>
<p>a number of ways&mdash;see Exercise 7.
</p>
<p>The main reason for using points having this pattern is that a quadratic fit to these
</p>
<p>points will have a minimum (rather than a maximum) and the minimum point will
</p>
<p>lie in the interval [x1, x3]. See Fig. 8.7. We modify the pure quadratic fit algorithm
</p>
<p>so that it always works with points in this basic three-point pattern.
</p>
<p>The point x4 is calculated from the quadratic fit in the standard way and f (x4)
</p>
<p>is measured. Assuming (as in the figure) that x2 &lt; x4 &lt; x3, and accounting for the
</p>
<p>unimodal nature of f , there are but two possibilities:
</p>
<p>1. f (x4) � f (x2)
</p>
<p>2. f (x2) &lt; f (x4) � f (x3).
</p>
<p>In either case a new three-point pattern, x̄1, x̄2, x̄3, involving x4 and two of the old
</p>
<p>points, can be determined: In case (8.1) it is
</p>
<p>(x̄1, x̄2, x̄3) = (x2, x4, x3),
</p>
<p>while in case (8.2) it is
</p>
<p>(x̄1, x̄2, x̄3) = (x1, x2, x4).
</p>
<p>We then use this three-point pattern to fit another quadratic and continue. The pure
</p>
<p>quadratic fit procedure determines the next point from the current point and the
</p>
<p>previous two points. In the modification above, the next point is determined from
</p>
<p>the current point and the two out of three last points that form a three-point pattern
</p>
<p>with it. This simple modification leads to global convergence.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.1 Line Search Algorithms 225
</p>
<p>To prove convergence, we note that each three-point pattern can be thought of as
</p>
<p>defining a vector x in E3. Corresponding to an x = (x1, x2, x3) such that (x1, x2, x3)
</p>
<p>form a three-point pattern with respect to f , we define A(x) = (x̄1, x̄2, x̄3) as dis-
</p>
<p>cussed above. For completeness we must consider the case where two or more
</p>
<p>of the xi, i = 1, 2, 3 are equal, since this may occur. The appropriate defini-
</p>
<p>tions are simply limiting cases of the earlier ones. For example, if x1 = x2, then
</p>
<p>(x1, x2, x3) form a three-point pattern if f (x2) � f (x3) and f
&prime;(x2) &lt; 0 (which
</p>
<p>is the limiting case of f (x2) &lt; f (x1)). A quadratic is fit in this case by using the
</p>
<p>values at the two distinct points and the derivative at the duplicated point. In case
</p>
<p>x1 = x2 = x3, (x1, x2, x3)forms a three-point pattern if f
&prime;(x2) = 0 and f &prime;&prime;(x2) � 0.
</p>
<p>Fig. 8.7 Three-point pattern
</p>
<p>With these definitions, the map A is well defined. It is also continuous, since curve
</p>
<p>fitting depends continuously on the data.
</p>
<p>We next define the solution set Γ &sub; E3 as the points x&lowast; = (x&lowast;, x&lowast;, x&lowast;) where
f &prime;(x&lowast;) = 0.
</p>
<p>Finally, we let Z(x) = f (x1) + f (x2) + f (x3). It is easy to see that Z is a descent
</p>
<p>function for A. After application of A one of the values f (x1), f (x2), f (x3) will be
</p>
<p>replaced by f (x4), and by construction, and the assumption that f is unimodal, it will
</p>
<p>replace a strictly larger value. Of course, at x&lowast; = (x&lowast;, x&lowast;, x&lowast;) we have A(x&lowast;) = x&lowast;
</p>
<p>and hence Z(A(x&lowast;)) = Z(x&lowast;).
Since all points are contained in the initial interval, we have all the requirements
</p>
<p>for the Global Convergence Theorem. Thus the process converges to the solution.
</p>
<p>The order of convergence may not be destroyed by this modification, if near the
</p>
<p>solution the three-point pattern is always formed from the previous three points. In
</p>
<p>this case we would still have convergence of order 1.3. This cannot be guaranteed,
</p>
<p>however.
</p>
<p>It has often been implicitly suggested, and accepted, that when using the quadratic
</p>
<p>fit technique one should require
</p>
<p>f (xk+1) &lt; f (xk)</p>
<p/>
</div>
<div class="page"><p/>
<p>226 8 Basic Descent Methods
</p>
<p>so as to guarantee convergence. If the inequality is not satisfied at some cycle, then a
</p>
<p>special local search is used to find a better xk+1 that does satisfy it. This philosophy
</p>
<p>amounts to taking Z(x) = f (x3) in our general framework and, unfortunately, this
</p>
<p>is not a descent function even for unimodal functions, and hence the special local
</p>
<p>search is likely to be necessary several times. It is true, of course, that a similar
</p>
<p>special local search may, occasionally, be required for the technique we suggest in
</p>
<p>regions of multiple minima, but it is never required in a unimodal region.
</p>
<p>The above construction, based on the pure quadratic fit technique, can be emu-
</p>
<p>lated to produce effective procedures based on other curve fitting techniques. For
</p>
<p>application to smooth functions these techniques seem to be the best available in
</p>
<p>terms of flexibility to accommodate as much derivative information as is available,
</p>
<p>fast convergence, and a guarantee of global convergence.
</p>
<p>&lowast;Closedness of Line Search Algorithms
</p>
<p>Since searching along a line for a minimum point is a component part of most non-
</p>
<p>linear programming algorithms, it is desirable to establish at once that this pro-
</p>
<p>cedure is closed; that is, that the end product of the iterative procedures outlined
</p>
<p>above, when viewed as a single algorithmic step finding a minimum along a line,
</p>
<p>define closed algorithms. That is the objective of this section.
</p>
<p>To initiate a line search with respect to a function f , two vectors must be spec-
</p>
<p>ified: the initial point x and the direction d in which the search is to be made. The
</p>
<p>result of the search is a new point. Thus we define the search algorithm S as a
</p>
<p>mapping from E2n to En.
</p>
<p>We assume that the search is to be made over the semi-infinite line emanating
</p>
<p>from x in the direction d. We also assume, for simplicity, that the search is not made
</p>
<p>in vain; that is, we assume that there is a minimum point along the line. This will
</p>
<p>be the case, for instance, if f is continuous and increases without bound as x tends
</p>
<p>toward infinity.
</p>
<p>Definition. The mapping S : E2n &rarr; En is defined by
</p>
<p>S(x, d) = {y : y = x + αd for some α � 0, f (y) = min
0�α�&infin;
</p>
<p>f (x + αd)}. (8.23)
</p>
<p>In some cases there may be many vectors y yielding the minimum, so S is a set-
</p>
<p>valued mapping. We must verify that S is closed.
</p>
<p>Theorem. Let f be continuous on En. Then the mapping defined by (8.23) is closed at (x, d)
if d � 0.
</p>
<p>Proof. Suppose {xk} and {dk} are sequences with xk &rarr; x, dk &rarr; d � 0. Suppose
also that yk &isin; S(xk, dk) and that yk &rarr; y. We must show that y &isin; S(x, d).
</p>
<p>For each k we have yk = xk + αkdk for some αk. From this we may write
</p>
<p>αk =
|yk &minus; xk |
|dk |
</p>
<p>.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.1 Line Search Algorithms 227
</p>
<p>Taking the limit of the right-hand side of the above, we see that
</p>
<p>αk &rarr; α &equiv;
|y &minus; x|
|d| .
</p>
<p>It then follows that y = x + αd. It still remains to be shown that y &isin; S(x, d).
For each k and each α, 0 � α &lt; &infin;,
</p>
<p>f (yk) � f (xk + αdk).
</p>
<p>Letting k &rarr; &infin; we obtain
f (y) � f (x + αd).
</p>
<p>Thus
</p>
<p>f (y) � min
0�α&lt;&infin;
</p>
<p>f (x + αd),
</p>
<p>and hence y &isin; S(x, d). �
</p>
<p>The requirement that d � 0 is natural both theoretically and practically. From
</p>
<p>a practical point of view this condition implies that, when constructing algorithms,
</p>
<p>the choice d = 0 had better occur only in the solution set; but it is clear that if d = 0,
</p>
<p>no search will be made. Theoretically, the map S can fail to be closed at d = 0, as
</p>
<p>illustrated below.
</p>
<p>Example. On E1 define f (x) = (x&minus;1)2. Then S (x, d) is not closed at x = 0, d = 0.
To see this we note that for any d &gt; 0
</p>
<p>min
0�α&lt;&infin;
</p>
<p>f (αd) = f (1),
</p>
<p>and hence
</p>
<p>S (0, d) = 1;
</p>
<p>but
</p>
<p>min
0�α&lt;&infin;
</p>
<p>f (α &middot; 0) = f (0)
</p>
<p>so that
</p>
<p>S (0, 0) = 0.
</p>
<p>Thus as d &rarr; 0, S (0, d)� S (0, 0).
</p>
<p>Inaccurate Line Search
</p>
<p>In practice, of course, it is impossible to obtain the exact minimum point called
</p>
<p>for by the ideal line search algorithm S described above. As a matter of fact, it is
</p>
<p>often desirable to sacrifice accuracy in the line search routine in order to conserve</p>
<p/>
</div>
<div class="page"><p/>
<p>228 8 Basic Descent Methods
</p>
<p>overall computation time. Because of these factors we must, to be realistic, be cer-
</p>
<p>tain, at every stage of development, that our theory does not crumble if inaccurate
</p>
<p>line searches are introduced.
</p>
<p>Inaccuracy generally is introduced in a line search algorithm by simply terminat-
</p>
<p>ing the search procedure before it has converged. The exact nature of the inaccu-
</p>
<p>racy introduced may therefore depend on the particular search technique employed
</p>
<p>and the criterion used for terminating the search. We cannot develop a theory that
</p>
<p>simultaneously covers every important version of inaccuracy without seriously de-
</p>
<p>tracting from the underlying simplicity of the algorithms discussed later. For this
</p>
<p>reason our general approach, which is admittedly more free-wheeling in spirit than
</p>
<p>necessary but thereby more transparent and less encumbered than a detailed account
</p>
<p>of inaccuracy, will be to analyze algorithms as if an accurate line search were
</p>
<p>made at every step, and then point out in side remarks and exercises the effect of
</p>
<p>inaccuracy.
</p>
<p>Armijo&rsquo;s Rule
</p>
<p>A practical and popular criterion for terminating a line search is Armijo&rsquo;s rule. The
</p>
<p>essential idea is that the rule should first guarantee that the selected α is not too
</p>
<p>large, and next it should not be too small. Let us define the function
</p>
<p>φ(α) = f (xk + αdk).
</p>
<p>Armijo&rsquo;s rule is implemented by consideration of the function φ(0) + εφ&prime;(0)α for
fixed ε, 0 &lt; ε &lt; 1. This function is shown in Fig. 8.8a as the dashed line. A value
</p>
<p>of α is considered to be not too large if the corresponding function value lies below
</p>
<p>the dashed line; that is, if
</p>
<p>φ(α) � φ(0) + εφ&prime;(0)α. (8.24)
</p>
<p>To insure that α is not too small, a value η &gt; 1 is selected, and α is then considered
</p>
<p>to be not too small if
</p>
<p>φ(ηα) &gt; φ(0) + εφ&prime;(0)ηα.
</p>
<p>This means that if α is increased by the factor η, it will fail to meet the test (8.24).
</p>
<p>The acceptable region defined by the Armijo rule is shown in Fig. 8.8a when η = 2
</p>
<p>(there are also other rules can be adapted).
</p>
<p>Sometimes in practice, the Armijo test is used to define a simplified line search
</p>
<p>technique that does not employ curve fitting methods. One begins with an arbitraryα.
</p>
<p>If it satisfies (8.24), it is repeatedly increased by η(η = 2 or η = 10 and ε = .2 are
</p>
<p>often used) until (8.24) is not satisfied, and then the penultimate α is selected. If, on
</p>
<p>the other hand, the original α does not satisfy (8.24), it is repeatedly divided by η
</p>
<p>until the resulting α does satisfy (8.24).</p>
<p/>
</div>
<div class="page"><p/>
<p>8.2 The Method of Steepest Descent 229
</p>
<p>8.2 The Method of Steepest Descent
</p>
<p>One of the oldest and most widely known methods for minimizing a function
</p>
<p>of several variables is the method of steepest descent (often referred to as the
</p>
<p>gradient method). The method is extremely important from a theoretical view-
</p>
<p>point, since it is one of the simplest for which a satisfactory analysis exists. More
</p>
<p>advanced algorithms are often motivated by an attempt to modify the basic steep-
</p>
<p>est descent technique in such a way that the new algorithm will have superior
</p>
<p>convergence properties. The method of steepest descent remains, therefore, not only
</p>
<p>the technique most often first tried on a new problem but also the standard of ref-
</p>
<p>erence against which other techniques are measured. The principles used for its
</p>
<p>analysis will be used throughout this book.
</p>
<p>The Method
</p>
<p>Let f have continuous first partial derivatives on En. We will frequently have need
</p>
<p>for the gradient vector of f and therefore we introduce some simplifying notation.
</p>
<p>The gradient&nabla; f (x) is, according to our conventions, defined as a n-dimensional row
</p>
<p>vector. For convenience we define the n-dimensional column vector g(x) = &nabla; f (x)T .
</p>
<p>When there is no chance for ambiguity, we sometimes suppress the argument x and,
</p>
<p>for example, write gk for g(xk) = &nabla; f (xk)
T .
</p>
<p>The method of steepest descent is defined by the iterative algorithm
</p>
<p>xk+1 = xk &minus; αkgk,
</p>
<p>where stepsize αk is a nonnegative scalar possibly minimizing f (xk&minus;αgk). In words,
from the point xk we search along the direction of the negative gradient &minus;gk to a
minimum point on this line; this minimum point is taken to be xk+1.
</p>
<p>In formal terms, the overall algorithm A : En &rarr; En which gives xk+1 &isin; A(xk)
can be decomposed in the form A = SG. Here G : En &rarr; E2n is defined by G(x) =
(x, &minus;g(x)), giving the initial point and direction of a line search. This is followed by
the line search S : E2n &rarr; En defined in Sect. 8.1.
</p>
<p>Global Convergence and Convergence Speed
</p>
<p>It was shown in Sect. 8.1 that S is closed if &nabla; f (x) � 0, and it is clear that G is
</p>
<p>continuous. Therefore, by Corollary 2 in Sect. 7.7 A is closed.
</p>
<p>We define the solution set to be the points x where &nabla; f (x) = 0. Then Z(x) = f (x)
</p>
<p>is a descent function for A, since for &nabla; f (x) � 0
</p>
<p>lim
0�α&lt;&infin;
</p>
<p>f (x &minus; αg(x)) &lt; f (x).</p>
<p/>
</div>
<div class="page"><p/>
<p>230 8 Basic Descent Methods
</p>
<p>Fig. 8.8 Stopping rules. (a) Armijo rule. (b) Golden test. (c) Wolfe test
</p>
<p>Thus by the Global Convergence Theorem, if the sequence {xk} is bounded, it will
have limit points and each of these is a solution. What about the convergence speed?
</p>
<p>Assume that f (x) is convex and differentiable everywhere, admits a minimizer x&lowast;,
and satisfies the (first-order) β-Lipschitz condition, that is, for any two points x and y
</p>
<p>|&nabla; f (x) &minus; &nabla; f (y)| &le; β|x &minus; y|
</p>
<p>for a positive real number β. Starting from any point x0, we consider the method of
</p>
<p>steepest descent with a fixed step size αk =
1
β
</p>
<p>for all k:</p>
<p/>
</div>
<div class="page"><p/>
<p>8.2 The Method of Steepest Descent 231
</p>
<p>xk+1 = xk &minus;
1
</p>
<p>β
gk = xk &minus;
</p>
<p>1
</p>
<p>β
&nabla; f (xk)T . (8.25)
</p>
<p>We first prove a lemma.
</p>
<p>Lemma 1. Let f (x) be differentiable everywhere and satisfy the (first-order) β-Lipschitz
condition. Then, for any two points x and y
</p>
<p>f (x) &minus; f (y) &minus; &nabla; f (y)(x &minus; y) &le; β
2
|x &minus; y|2.
</p>
<p>Then we prove
</p>
<p>Theorem 1 (Steepest Descent&mdash;Lipschitz Convex Case). Let f (x) be convex and differen-
tiable everywhere, satisfy the (first-order) β-Lipschitz condition, and admit a minimizer x&lowast;.
Then, the method of steepest descent (8.25) generates a sequence of solutions xk such that
</p>
<p>|&nabla; f (xk)| &le;
β2&radic;
</p>
<p>k(k + 1)
|x0 &minus; x&lowast;|,
</p>
<p>and
</p>
<p>f (xk) &minus; f (x&lowast;) &le;
β
</p>
<p>2(k + 1)
|x0 &minus; x&lowast;|2.
</p>
<p>Proof. Consider the function gx(y) = f (y) &minus; &nabla; f (x)y for any given x. Note that gx is
also convex and satisfies the β-Lipschitz condition. Moreover, x is the minimizer of
</p>
<p>gx(y) and &nabla;gx(y) = &nabla; f (y) &minus; &nabla; f (x).
Applying Lemma 1 to gx and noting the relations of gx and f (x), we have
</p>
<p>f (x) &minus; f (y) &minus; &nabla; f (x)(x &minus; y) = gx(x) &minus; gx(y)
&le; gx(y &minus; 1β&nabla;gx(y)) &minus; gx(y)
&le; &nabla;gx(y)(&minus; 1β&nabla;gx(y)T ) +
</p>
<p>β
</p>
<p>2
1
β2
|&nabla;gx(y)|2
</p>
<p>= &minus; 1
2β
|&nabla;gx(y)|2
</p>
<p>= &minus; 1
2β
|&nabla; f (x) &minus; &nabla; f (y)|2.
</p>
<p>(8.26)
</p>
<p>Similarly, we have
</p>
<p>f (y) &minus; f (x) &minus; &nabla; f (y)(y &minus; x) &le; &minus; 1
2β
</p>
<p>|&nabla; f (x) &minus; &nabla; f (y)|2.
</p>
<p>Adding the above two derived inequalities, we have for any x and y:
</p>
<p>(&nabla; f (x) &minus; &nabla; f (y))(x &minus; y) &ge; 1
β
|&nabla; f (x) &minus; &nabla; f (y)|2. (8.27)
</p>
<p>For simplification, in what follows let dk = xk &minus; x&lowast; and δk = [ f (xk) &minus; f (x&lowast;)] &ge; 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>232 8 Basic Descent Methods
</p>
<p>Now let x = xk+1 and y = xk in (8.27). Then
</p>
<p>&minus;1
β
</p>
<p>(gk)
T (gk+1 &minus; gk) = (xk+1 &minus; xk)T (gk+1 &minus; gk) &ge;
</p>
<p>1
</p>
<p>β
|gk+1 &minus; gk |2,
</p>
<p>which leads to
</p>
<p>|gk+1|2 &le; (gk+1)Tgk &le; |gk+1||gk |, that is |gk+1| &le; |gk |. (8.28)
</p>
<p>Inequality (8.28) implies that |gk | = |&nabla; f (xk)| is monotonically decreasing.
Applying inequality (8.26) for x = xk and y = x
</p>
<p>&lowast; and noting g&lowast; = 0 we have
</p>
<p>δk &le; (gk)Tdk &minus; 12β |gk |2
= &minus;β(xk+1 &minus; xk)dk &minus; β2 |xk+1 &minus; xk |2
= &minus; β
</p>
<p>2
(|xk+1 &minus; xk |2 + 2(xk+1 &minus; xk)Tdk)
</p>
<p>= &minus; β
2
(|dk+1 &minus; dk |2 + 2(dk+1 &minus; dk)Tdk)
</p>
<p>=
β
</p>
<p>2
(|dk |2 &minus; |dk+1|2).
</p>
<p>(8.29)
</p>
<p>Summing up (8.29) from 0 to k, we have
</p>
<p>k
&sum;
</p>
<p>l=0
</p>
<p>δl &le;
β
</p>
<p>2
(|d0|2 &minus; |dk+1|2) &le;
</p>
<p>β
</p>
<p>2
|d0|2. (8.30)
</p>
<p>Using (8.26) again for x = xk+1 and y = xk and noting (8.25) we have
</p>
<p>δk+1 &minus; δk = f (xk+1) &minus; f (xk)
&le; gT
</p>
<p>k+1(&minus; 1βgk) &minus; 12β |gk+1 &minus; gk |2
= &minus; 1
</p>
<p>2β
(|gk+1|2 + |gk |2).
</p>
<p>(8.31)
</p>
<p>Noting (8.31) holts for all k, we have
</p>
<p>&sum;k
l=0 δl =
</p>
<p>&sum;k
l=0 δl(l + 1 &minus; l)
</p>
<p>=
&sum;k
</p>
<p>l=0 δl(l + 1) &minus;
&sum;k
</p>
<p>l=0 δll
</p>
<p>=
&sum;k+1
</p>
<p>l=1 δl&minus;1l &minus;
&sum;k
</p>
<p>l=1 δll
</p>
<p>= δk(k + 1) +
&sum;k
</p>
<p>l=1(δl&minus;1 &minus; δl)l
&ge; δk(k + 1) +
</p>
<p>&sum;k
l=1
</p>
<p>l
2β
</p>
<p>(|gl|2 + |gl&minus;1|2)
&ge; δk(k + 1) + k(k+1)2β |gk |2
</p>
<p>where the last inequality comes |gk| = |&nabla; f (xk)| is monotonically decreasing.
Using (8.30) we finally have
</p>
<p>(k + 1)δk +
k(k + 1)
</p>
<p>2β
|gk |2 &le;
</p>
<p>β
</p>
<p>2
|d0|2. (8.32)
</p>
<p>Inequality (8.32), from δk = f (xk) &minus; f (x&lowast;) &ge; 0 and d0 = x0 &minus; x&lowast;, proves the desired
bounds. �
</p>
<p>Theorem 1 implies that the convergence speed of the steepest descent method is
</p>
<p>arithmetic.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.2 The Method of Steepest Descent 233
</p>
<p>The Quadratic Case
</p>
<p>When f (x) is strongly convex, the convergence speed can be increased from arith-
</p>
<p>metic to geometric or linear convergence. Since all of the important convergence
</p>
<p>characteristics of the method of steepest descent are revealed by an investigation of
</p>
<p>the method when applied to quadratic problems, we focus here on
</p>
<p>f (x) =
1
</p>
<p>2
xTQx &minus; xTb, (8.33)
</p>
<p>where Q is a positive definite symmetric n &times; n matrix. Since Q is positive definite,
all of its eigenvalues are positive. We assume that these eigenvalues are ordered: 0 &lt;
</p>
<p>a = λ1 � λ2 . . . � λn = A. With Q positive definite, it follows (from Proposition 5,
</p>
<p>Sect. 7.4) that f is strictly convex.
</p>
<p>The unique minimum point of f can be found directly, by setting the gradient to
</p>
<p>zero, as the vector x&lowast; satisfying
Qx&lowast; = b. (8.34)
</p>
<p>Moreover, introducing the function
</p>
<p>E(x) =
1
</p>
<p>2
(x &minus; x&lowast;)TQ(x &minus; x&lowast;), (8.35)
</p>
<p>we have E(x) = f (x) + (1/2)x&lowast;TQx&lowast;, which shows that the function E differs from
f only by a constant. For many purposes then, it will be convenient to consider that
</p>
<p>we are minimizing E rather than f .
</p>
<p>The gradient (of both f and E) is given explicitly by
</p>
<p>g(x) = Qx &minus; b. (8.36)
</p>
<p>Thus the method of steepest descent can be expressed as
</p>
<p>xk+1 = xk &minus; αkgk, (8.37)
</p>
<p>where gk = Qxk &minus; b and where αk minimizes f (xk &minus; αgk). We can, however, in this
special case, determine the value of αk explicitly. We have, by definition (8.33),
</p>
<p>f (xk &minus; αgk) =
1
</p>
<p>2
(xk &minus; αgk)TQ(xk &minus; αgk) &minus; (xk &minus; αgk)Tb,
</p>
<p>which (as can be found by differentiating with respect to α) is minimized at
</p>
<p>αk =
gT
k
</p>
<p>gk
</p>
<p>gT
k
</p>
<p>Qgk
. (8.38)
</p>
<p>Hence the method of steepest descent (8.37) takes the explicit form
</p>
<p>xk+1 = xk &minus;
⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>gT
k
</p>
<p>gk
</p>
<p>gT
k
</p>
<p>Qgk
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>gk, (8.39)
</p>
<p>where gk = Qxk &minus; b.</p>
<p/>
</div>
<div class="page"><p/>
<p>234 8 Basic Descent Methods
</p>
<p>The function f and the steepest descent process can be illustrated as in Fig. 8.9 by
</p>
<p>showing contours of constant values of f and a typical sequence developed by the
</p>
<p>process. The contours of f are n-dimensional ellipsoids with axes in the directions
</p>
<p>of the n-mutually orthogonal eigenvectors of Q. The axis corresponding to the ith
</p>
<p>eigenvector has length proportional to 1/λi. We now analyze this process and show
</p>
<p>that the rate of convergence depends on the ratio of the lengths of the axes of the
</p>
<p>elliptical contours of f , that is, on the eccentricity of the ellipsoids.
</p>
<p>Fig. 8.9 Steepest descent
</p>
<p>Lemma 2. The iterative process (8.39) satisfies
</p>
<p>E(xk+1) =
</p>
<p>⎧
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>1 &minus;
(gT
</p>
<p>k
gk)
</p>
<p>2
</p>
<p>(gT
k
Qgk)(g
</p>
<p>T
k
Q&minus;lgk)
</p>
<p>⎫
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎬
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎭
</p>
<p>E(xk). (8.40)
</p>
<p>Proof. The proof is by direct computation. We have, setting yk = xk &minus; x&lowast;,
</p>
<p>E(xk) &minus; E(xk+1)
E(xk)
</p>
<p>=
2αkg
</p>
<p>T
k
</p>
<p>Qyk &minus; α2kgTk Qgk
yT
k
</p>
<p>Qyk
.
</p>
<p>Using gk = Qyk we have
</p>
<p>E(xk) &minus; E(xk+1)
E(xk)
</p>
<p>=
</p>
<p>2(gT
k
</p>
<p>gk)
2
</p>
<p>(gT
k
</p>
<p>Qgk)
&minus; (g
</p>
<p>T
k
</p>
<p>gk)
2
</p>
<p>(gT
k
</p>
<p>Qgk)
</p>
<p>gT
k
</p>
<p>Q&minus;1gk
</p>
<p>=
(gT
</p>
<p>k
gk)
</p>
<p>2
</p>
<p>(gT
k
</p>
<p>Qgk)(g
T
k
</p>
<p>Q&minus;lgk)
.�
</p>
<p>In order to obtain a bound on the rate of convergence, we need a bound on the right-
</p>
<p>hand side of (8.40). The best bound is due to Kantorovich and his lemma, stated
</p>
<p>below, is a useful general tool in convergence analysis.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.2 The Method of Steepest Descent 235
</p>
<p>Kantorovich inequality: Let Q be a positive definite symmetric n&times; n matrix. For any vector
x there holds
</p>
<p>(xTx)2
</p>
<p>(xTQx)(xTQ&minus;1x)
�
</p>
<p>4aA
</p>
<p>(a + A)2
, (8.41)
</p>
<p>where a and A are, respectively, the smallest and largest eigenvalues of Q.
</p>
<p>Proof. Let the eigenvalues λ1, λ2, . . . , λn of Q satisfy
</p>
<p>0 &lt; a = λ1 � λ2 . . . � λn = A.
</p>
<p>By an appropriate change of coordinates the matrix Q becomes diagonal with diag-
</p>
<p>onal (λ1, λ2, . . . , λn). In this coordinate system we have
</p>
<p>(xTx)2
</p>
<p>(xTQx)(xTQ&minus;1x)
=
</p>
<p>(
&sum;n
</p>
<p>i=1 x
2
i
)2
</p>
<p>(
&sum;n
</p>
<p>i=1 λix
2
i
)(
&sum;n
</p>
<p>i=1(x
2
i
/λi))
</p>
<p>,
</p>
<p>which can be written as
</p>
<p>(xTx)2
</p>
<p>(xTQx)(xTQ&minus;1x)
=
</p>
<p>1/
&sum;n
</p>
<p>i=1 ξiλi
&sum;n
</p>
<p>i=1(ξi/λi)
&equiv; φ(ξ)
</p>
<p>ψ(ξ)
,
</p>
<p>where ξi = x
2
i
/
&sum;n
</p>
<p>i=1 x
2
i
. We have converted the expression to the ratio of two func-
</p>
<p>tions involving convex combinations; one a combination of λi&rsquo;s; the other a com-
</p>
<p>bination of 1/λi&rsquo;s. The situation is shown pictorially in Fig. 8.10. The curve in the
</p>
<p>figure represents the function 1/λ. Since
&sum;n
</p>
<p>i=1 ξiλi is a point between λ1 and λn, the
</p>
<p>value of φ(ξ) is a point on the curve. On the other hand, the value of ψ(ξ) is a convex
</p>
<p>combination of points on the curve and its value corresponds to a point in the shaded
</p>
<p>region. For the same vector ξ both functions are represented by points on the same
</p>
<p>vertical line. The minimum value of this ratio is achieved for some λ = ξ1λ1 + ξnλn,
</p>
<p>with ξ1 + ξn = 1. Using the relation ξ1/λ1 + ξn/λn = (λ1 + λn &minus; ξ1λ1 &minus; ξnλn)/λ1λn,
an appropriate bound is
</p>
<p>φ(ξ)
</p>
<p>ψ(ξ)
� lim
</p>
<p>λ1�λ�λn
</p>
<p>(1/λ)
</p>
<p>(λ1 + λn &minus; λ)/(λ1λn)
.
</p>
<p>The minimum is achieved at λ = (λ1 + λn)/2, yielding
</p>
<p>φ(ξ)
</p>
<p>ψ(ξ)
�
</p>
<p>4λ1λn
</p>
<p>(λ1 + λn)2
.�
</p>
<p>Combining the above two lemmas, we obtain the central result on the convergence
</p>
<p>of the method of steepest descent.
</p>
<p>Theorem 2 (Steepest Descent&mdash;Quadratic Case). For any x0 &isin; En the method of steepest
descent (8.39) converges to the unique minimum point x&lowast; of f Furthermore, with E(x) =
1
2
(x &minus; x&lowast;)TQ(x &minus; x&lowast;), there holds at every step k
</p>
<p>E(xk+1) �
(
</p>
<p>A &minus; a
A + a
</p>
<p>)2
</p>
<p>E(xk). (8.42)</p>
<p/>
</div>
<div class="page"><p/>
<p>236 8 Basic Descent Methods
</p>
<p>Proof. By Lemma 2 and the Kantorovich inequality
</p>
<p>E(xk+1) �
</p>
<p>{
</p>
<p>1 &minus; 4aA
(A + a)2
</p>
<p>}
</p>
<p>E(xk) =
(
</p>
<p>A &minus; a
A + a
</p>
<p>)2
</p>
<p>E(xk).
</p>
<p>Fig. 8.10 Kantorovich inequality
</p>
<p>It follows immediately that E(xk) &rarr; 0 and hence, since Q is positive definite, that
xk &rarr; x&lowast;. �
</p>
<p>Roughly speaking, the above theorem says that the convergence rate of steepest
</p>
<p>descent is slowed as the contours of f become more eccentric. If a = A, correspond-
</p>
<p>ing to circular contours, convergence occurs in a single step. Note, however, that
</p>
<p>even if n&minus; 1 of the n eigenvalues are equal and the remaining one is a great distance
from these, convergence will be slow, and hence a single abnormal eigenvalue can
</p>
<p>destroy the effectiveness of steepest descent.
</p>
<p>In the terminology introduced in Sect. 7.8, the above theorem states that with
</p>
<p>respect to the error function E (or equivalently f) the method of steepest descent
</p>
<p>converges linearly with a ratio no greater than [(A &minus; a)/(A + a)]2. The actual rate
depends on the initial point x0. However, for some initial points the bound is actually
</p>
<p>achieved. Furthermore, it has been shown by Akaike that, if the ratio is unfavorable,
</p>
<p>the process is very likely to converge at a rate close to the bound. Thus, somewhat
</p>
<p>loosely but with reasonable justification, we say that the convergence ratio of steep-
</p>
<p>est descent is [(A &minus; a)/(A + a)]2.
It should be noted that the convergence rate actually depends only on the ratio
</p>
<p>r = A/a of the largest to the smallest eigenvalue. Thus the convergence ratio is
</p>
<p>(
</p>
<p>A &minus; a
A + a
</p>
<p>)2
</p>
<p>=
</p>
<p>(
</p>
<p>r &minus; 1
r + 1
</p>
<p>)2
</p>
<p>,</p>
<p/>
</div>
<div class="page"><p/>
<p>8.2 The Method of Steepest Descent 237
</p>
<p>which clearly shows that convergence is slowed as r increases. The ratio r, which
</p>
<p>is the single number associated with the matrix Q that characterizes convergence, is
</p>
<p>often called the condition number of the matrix.
</p>
<p>Example. Let us take
</p>
<p>Q =
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>0.78 &minus;0.02 &minus;0.12 &minus;0.14
&minus;0.02 0.86 &minus;0.04 0.06
&minus;0.12 &minus;0.04 0.72 &minus;0.08
&minus;0.14 0.06 &minus;0.08 0.74
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>b = (0.76, 0.08, 1.12, 0.68).
</p>
<p>For this matrix it can be calculated that a = 0.52, A = 0.94 and hence r = 1.8.
</p>
<p>This is a very favorable condition number and leads to the convergence ratio [(A &minus;
a)/(A + a)]2 = 0.081. Thus each iteration will reduce the error in the objective by
</p>
<p>more than a factor of ten; or, equivalently, each iteration will add about one more
</p>
<p>digit of accuracy. Indeed, starting from the origin the sequence of values obtained
</p>
<p>by steepest descent as shown in Table 8.1 is consistent with this estimate.
</p>
<p>The Nonquadratic Case
</p>
<p>For nonquadratic functions, we expect that steepest descent will also do reason-
</p>
<p>ably well if the condition number is modest. Fortunately, we are able to establish
</p>
<p>estimates of the progress of the method when the Hessian matrix is always posi-
</p>
<p>tive definite. Specifically, we assume that the Hessian matrix is bounded above and
</p>
<p>below as aI � F(x̄) � AI. (Thus f is strongly convex.) We present three analyses:
</p>
<p>Table 8.1 Solution to Example
</p>
<p>Step k f (xk)
</p>
<p>0 0
1 &minus;2.1563625
2 &minus;2.1744062
3 &minus;2.1746440
4 &minus;2.1746585
5 &minus;2.1746595
6 &minus;2.1746595
Solution point x&lowast; = (1.534965, 0.1220097, 1.975156, 1.412954)
</p>
<p>1. Exact Line Search. Given a point xk, we have for any α
</p>
<p>f (xk &minus; αg(xk)) � f (xk)) &minus; αg(xk)Tg(xk) +
Aα2
</p>
<p>2
g(xk)
</p>
<p>Tg(xk). (8.43)</p>
<p/>
</div>
<div class="page"><p/>
<p>238 8 Basic Descent Methods
</p>
<p>Minimizing both sides separately with respect to α the inequality will hold for the
</p>
<p>two minima. The minimum of the left hand side is f (xk+1). The minimum of the
</p>
<p>right hand side occurs at α = 1/A, yielding the result
</p>
<p>f (xk+1) � f (xk) &minus;
1
</p>
<p>2A
|g(xk)|2.
</p>
<p>where |g(xk)|2 &equiv; g(xk)Tg(xk). Subtracting the optimal value f &lowast; = f (x&lowast;) from both
sides produces
</p>
<p>f (xk+1) &minus; f&lowast; � f(xk) &minus; f&lowast; &minus;
1
</p>
<p>2A
|g(xk)|2. (8.44)
</p>
<p>In a similar way, for any x there holds
</p>
<p>f (x) � f (xk) + g(xk)
T (x &minus; xk) +
</p>
<p>a
</p>
<p>2
|x &minus; xk|2.
</p>
<p>Again we can minimize both sides separately. The minimum of the left hand side is
</p>
<p>f &lowast; the optimal solution value. Minimizing the right hand side leads to the quadratic
optimization problem. The solution is x = xk &minus; g(xk)/a. Substituting this x in the
right hand side of the inequality gives
</p>
<p>f &lowast; � f (xk) &minus;
1
</p>
<p>2a
|g(xk)|2. (8.45)
</p>
<p>From (8.45) we have
</p>
<p>&minus; |g(xk)|2 � 2a[ f &lowast; &minus; f (xk)]. (8.46)
Substituting this in (8.44) gives
</p>
<p>f (xk+1) &minus; f &lowast; � (1 &minus; a/A)[ f (xk) &minus; f &lowast;]. (8.47)
</p>
<p>This shows that the method of steepest descent makes progress even when it is not
</p>
<p>close to the solution.
</p>
<p>2. Other Stopping Criteria. As an example of how other stopping criteria can
</p>
<p>be treated, we examine the rate of convergence when using Amijo&rsquo;s rule with
</p>
<p>ε &lt; 0.5 and η &gt; 1. Note first that the inequality t � t2 for 0 � t � 1 implies by a
</p>
<p>change of variable that
</p>
<p>&minus;α + α
2A
</p>
<p>2
&le; &minus;α/2
</p>
<p>for 0 � α � 1/A. Then using (8.43) we have that for α &lt; 1/A
</p>
<p>f (xk &minus; αg(xk)) &le; f (xk) &minus; α|g(xk)|2 + 0.5α2A|g(xl)|2
</p>
<p>&le; f (xk) &minus; 0.5α|g(xk)|2
</p>
<p>&lt; f (xk) &minus; εα|g(xk)|2
</p>
<p>since ε &lt; 0.5. This means that the first part of the stopping criterion is satisfied
</p>
<p>for α &lt; 1/A.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.3 Applications of the Convergence Theory 239
</p>
<p>The second part of the stopping criterion states that ηα does not satisfy the first
</p>
<p>criterion and thus the final α must satisfy α &ge; 1/(ηA). Therefore the inequality of
the first part of the criterion implies
</p>
<p>f (xk+1) &le; f (xk) &minus;
ε
</p>
<p>ηA
|g(xk)|2.
</p>
<p>Subtracting f &lowast; from both sides,
</p>
<p>f (xk+1) &minus; f &lowast; &le; f (xk) &minus; f &lowast; &minus;
ε
</p>
<p>ηA
|g(xk)|2.
</p>
<p>Finally, using (8.46) we obtain
</p>
<p>f (xk+1) &minus; f &lowast; &le; [1 &minus; (2εa/ηA)]( f (xk) &minus; f &lowast;).
</p>
<p>Clearly 2εa/ηA &lt; 1 and hence there is linear convergence. Notice if that in fact ε is
</p>
<p>chosen very close to 0.5 and η is chosen very close to 1, then the stopping condition
</p>
<p>demands that the α be restricted to a very small range, and the estimated rate of
</p>
<p>convergence is very close to the estimate obtained above for exact line search.
</p>
<p>3. Asymptotic Convergence. We expect that as the points generated by steepest
</p>
<p>descent approach the solution point, the convergence characteristics will be close
</p>
<p>to those inherent for quadratic functions. This is indeed the case.
</p>
<p>The general procedure for proving such a result, which is applicable to most
</p>
<p>methods having unity order of convergence, is to use the Hessian of the objective at
</p>
<p>the solution point as if it were the Q matrix of a quadratic problem. The particular
</p>
<p>theorem stated below is a special case of a theorem in Sect. 12.5 so we do not prove
</p>
<p>it here; but it illustrates the generalizability of an analysis of quadratic problems.
</p>
<p>Theorem. Suppose f is defined on En, has continuous second partial derivatives, and has
</p>
<p>a relative minimum at x&lowast;. Suppose further that the Hessian matrix of f , F(x&lowast;), has smallest
eigenvalue a &gt; 0 and largest eigenvalue A &gt; 0. If {xk} is a sequence generated by the
method of steepest descent that converges to x&lowast;, then the sequence of objective values { f (xk)}
converges to f (x&lowast;) linearly with a convergence ratio no greater than [(A &minus; a)/(A + a)]2 .
</p>
<p>8.3 Applications of the Convergence Theory
</p>
<p>Now that the basic convergence theory, as represented by the formula (8.42) for
</p>
<p>the rate of convergence, has been developed and demonstrated to actually charac-
</p>
<p>terize the behavior of steepest descent, it is appropriate to illustrate how the theory
</p>
<p>can be used. Generally, we do not suggest that one compute the numerical value
</p>
<p>of the formula&mdash;since it involves eigenvalues, or ratios of eigenvalues, that are not
</p>
<p>easily determined. Nevertheless, the formula itself is of immense practical impor-
</p>
<p>tance, since it allows one to theoretically compare various situations. Without such
</p>
<p>a theory, one would be forced to rely completely on experimental comparisons.</p>
<p/>
</div>
<div class="page"><p/>
<p>240 8 Basic Descent Methods
</p>
<p>Application 1 (Solution of Gradient Equation). One approach to the minimization
</p>
<p>of a function f is to consider solving the equations &nabla; f (x) = 0 that represent the
</p>
<p>necessary conditions. It has been proposed that these equations could be solved by
</p>
<p>applying steepest descent to the function h(x) = |&nabla; f (x)|2. One advantage of this
method is that the minimum value is known. We ask whether this method is likely
</p>
<p>to be faster or slower than the application of steepest descent to the original function
</p>
<p>f itself.
</p>
<p>For simplicity we consider only the case where f is quadratic. Thus let f (x) =
</p>
<p>(1/2)xTQx &minus; bTx. Then the gradient of f is g(x) = Qx &minus; b, and h(x) = |g(x)|2 =
xTQ2x &minus; 2xTQb + bTb. Thus h(x) is itself a quadratic function. The rate of conver-
gence of steepest descent applied to h will be governed by the eigenvalues of the
</p>
<p>matrix Q2. In particular the rate will be
</p>
<p>(
</p>
<p>r̄ &minus; 1
r̄ + 1
</p>
<p>)2
</p>
<p>,
</p>
<p>where r̄ is the condition number of the matrix Q2. However, the eigenvalues of Q2
</p>
<p>are the squares of those of Q itself, so r̄ = r2, where r is the condition number of Q,
</p>
<p>and it is clear that the convergence rate for the proposed method will be worse than
</p>
<p>for steepest descent applied to the original function.
</p>
<p>We can go further and actually estimate how much slower the proposed method
</p>
<p>is likely to be. If r is large, we have
</p>
<p>steepest descent rate =
</p>
<p>(
</p>
<p>r &minus; 1
r + 1
</p>
<p>)2
</p>
<p>≃ (1 &minus; 1/r)4
</p>
<p>proposed method rate =
</p>
<p>(
</p>
<p>r2 &minus; 1
r2 + 1
</p>
<p>)2
</p>
<p>≃ (1 &minus; 1/r2)4.
</p>
<p>Since (1&minus;1/r2)r ≃ 1&minus;1/r, it follows that it takes about r steps of the new method to
equal one step of ordinary steepest descent. We conclude that if the original problem
</p>
<p>is difficult to solve with steepest descent, the proposed method will be quite a bit
</p>
<p>worse.
</p>
<p>Application 2 (Penalty Methods). Let us briefly consider a problem with a single
</p>
<p>constraint:
</p>
<p>minimize f (x) (8.48)
</p>
<p>subject to h(x) = 0.
</p>
<p>One method for approaching this problem is to convert it (at least approximately) to
</p>
<p>the unconstrained problem
</p>
<p>minimize f (x) +
1
</p>
<p>2
μh(x)2, (8.49)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.3 Applications of the Convergence Theory 241
</p>
<p>where μ is a (large) penalty coefficient. Because of the penalty, the solution to (8.49)
</p>
<p>will tend to have a small h(x). Problem (8.49) can be solved as an unconstrained
</p>
<p>problem by the method of steepest descent. How will this behave?
</p>
<p>For simplicity let us consider the case where f is quadratic and h is linear. Specif-
</p>
<p>ically, we consider the problem
</p>
<p>minimize
1
</p>
<p>2
xTQx &minus; bTx (8.50)
</p>
<p>subject to cTx = 0.
</p>
<p>The objective of the associated penalty problem is (1/2){xTQx+μxTccTx}&minus;bTx. The
quadratic form associated with this objective is defined by the matrix Q+ μccT and,
</p>
<p>accordingly, the convergence rate of steepest descent will be governed by the condi-
</p>
<p>tion number of this matrix. This matrix is the original matrix Q with a large rank-one
</p>
<p>matrix added. It should be fairly clear&dagger; that this addition will cause one eigenvalue
of the matrix to be large (on the order of μ). Thus the condition number is roughly
</p>
<p>proportional to μ. Therefore, as one increases μ in order to get an accurate solution
</p>
<p>to the original constrained problem, the rate of convergence becomes extremely
</p>
<p>poor. We conclude that the penalty function method used in this simplistic way with
</p>
<p>steepest descent will not be very effective. (Penalty functions, and how to minimize
</p>
<p>them more rapidly, are considered in detail in Chap. 11.)
</p>
<p>Scaling
</p>
<p>The performance of the method of steepest descent is dependent on the particular
</p>
<p>choice of variables x used to define the problem. A new choice may substantially
</p>
<p>alter the convergence characteristics.
</p>
<p>Suppose that T is an invertible n &times; n matrix. We can then represent points in En
either by the standard vector x or by y where Ty = x. The problem of finding x to
</p>
<p>minimize f (x) is equivalent to that of finding y to minimize h(y) = f (Ty). Using y
</p>
<p>as the underlying set of variables, we then have
</p>
<p>&nabla;h = &nabla; fT, (8.51)
</p>
<p>where &nabla; f is the gradient of f with respect to x. Thus, using steepest descent, the
</p>
<p>direction of search will be
</p>
<p>&nabla;y = &minus;TT&nabla; f T , (8.52)
which in the original variables is
</p>
<p>∆x = &minus;TTT&nabla; f T . (8.53)
</p>
<p>&dagger;See the Interlocking Eigenvalues Lemma in Sect. 10.6 for a proof that only one eigenvalue
becomes large.</p>
<p/>
</div>
<div class="page"><p/>
<p>242 8 Basic Descent Methods
</p>
<p>Thus we see that the change of variables changes the direction of search.
</p>
<p>The rate of convergence of steepest descent with respect to y will be determined
</p>
<p>by the eigenvalues of the Hessian of the objective, taken with respect to y. That
</p>
<p>Hessian is
</p>
<p>&nabla;
2h(y) &equiv; H(y) = TTF(Ty)T.
</p>
<p>Thus, if x&lowast; = Ty&lowast; is the solution point, the rate of convergence is governed by the
matrix
</p>
<p>H(y&lowast;) = TTF(x&lowast;)T. (8.54)
</p>
<p>Very little can be said in comparison of the convergence ratio associated with H
</p>
<p>and that of F. If T is an orthonormal matrix, corresponding to y being defined from
</p>
<p>x by a simple rotation of coordinates, then TTT = I, and we see from (8.48) that the
</p>
<p>directions remain unchanged and the eigenvalues of H are the same as those of F.
</p>
<p>In general, before attacking a problem with steepest descent, it is desirable, if it is
</p>
<p>feasible, to introduce a change of variables that leads to a more favorable eigenvalue
</p>
<p>structure. Usually the only kind of transformation that is at all practical is one having
</p>
<p>T equal to a diagonal matrix, corresponding to the introduction of scale factors on
</p>
<p>each of the variables. One should strive, in doing this, to make the second derivatives
</p>
<p>with respect to each variable roughly the same. Although appropriate scaling can
</p>
<p>potentially lead to substantial payoff in terms of enhanced convergence rate, we
</p>
<p>largely ignore this possibility in our discussions of steepest descent. However, see
</p>
<p>the next application for a situation that frequently occurs.
</p>
<p>Application 3 (Program Design). In applied work it is extremely rare that one
</p>
<p>solves just a single optimization problem of a given type. It is far more usual that
</p>
<p>once a problem is coded for computer solution, it will be solved repeatedly for
</p>
<p>various parameter values. Thus, for example, if one is seeking to find the optimal
</p>
<p>production plan (as in Example 2 of Sect. 7.2), the problem will be solved for the
</p>
<p>different values of the input prices. Similarly, other optimization problems will be
</p>
<p>solved under various assumptions and constraint values. It is for this reason that
</p>
<p>speed of convergence and convergence analysis is so important. One wants a pro-
</p>
<p>gram that can be used efficiently. In many such situations, the effort devoted to
</p>
<p>proper scaling repays itself, not with the first execution, but in the long run.
</p>
<p>As a simple illustration consider the problem of minimizing the function
</p>
<p>f (x) = x2 &minus; 5xy + y4 &minus; ax &minus; by.
</p>
<p>It is desirable to obtain solutions quickly for different values of the parameters a
</p>
<p>and b. We begin with the values a = 25, b = 8.
</p>
<p>The result of steepest descent applied to this problem directly is shown in Ta-
</p>
<p>ble 8.2, column (a). It requires eighty iterations for convergence, which could be
</p>
<p>regarded as disappointing.
</p>
<p>The reason for this poor performance is revealed by examining the Hessian
</p>
<p>matrix
</p>
<p>F =
</p>
<p>[
</p>
<p>2 &minus;5
&minus;5 12y2
</p>
<p>]</p>
<p/>
</div>
<div class="page"><p/>
<p>8.4 Accelerated Steepest Descent 243
</p>
<p>Using the results of our first experiment, we know that y = 3. Hence the diagonal
</p>
<p>elements of the Hessian, at the solution, differ by a factor of 54. (In fact, the condi-
</p>
<p>tion number is about 61.) As a simple remedy we scale the problem by replacing the
</p>
<p>variable y by z = ty. The new lower right-corner term of the Hessian then becomes
</p>
<p>12z2/t4, which has magnitude 12 &times; t2 &times; 32/t4 = 108/t2. Thus we might put t = 7 in
order to make the two diagonal terms approximately equal. The result of applying
</p>
<p>steepest descent to the problem scaled this way is shown in Table 8.2, column (b).
</p>
<p>(This superior performance is in accordance with our general theory, since the con-
</p>
<p>dition number of the scaled problem is about two.) For other nearby values of a and
</p>
<p>b, similar speeds will be attained.
</p>
<p>Table 8.2 Solution to scaling application
</p>
<p>Value of f
Iteration (a) (b)
no. Unscaled Scaled
</p>
<p>0 0.0000 0.0000
1 &minus;230.9958 &minus;162.2000
2 &minus;256.4042 &minus;289.3124
4 &minus;293.1705 &minus;341.9802
6 &minus;313.3619 &minus;342.9865
8 &minus;324.9978 &minus;342.9998
9 &minus;329.0408 &minus;343.0000
15 &minus;339.6124
20 &minus;341.9022
25 &minus;342.6004
30 &minus;342.8372
35 &minus;342.9275
40 &minus;342.9650
45 &minus;342.9825
50 &minus;342.9909
55 &minus;342.9951
60 &minus;342.9971 Solution
65 &minus;342.9883 x = 20.0
70 &minus;342.9990 y = 3.0
75 &minus;342.9994
80 &minus;342.9997
</p>
<p>8.4 Accelerated Steepest Descent
</p>
<p>There is an accelerated steepest descent method that works as follows:
</p>
<p>λ0 = 0, λk+1 =
1+
&radic;
</p>
<p>1+4(λk)2
</p>
<p>2
, αk =
</p>
<p>1 &minus; λk
λk+1
</p>
<p>, (8.55)
</p>
<p>x̃k+1 = xk &minus; 1β&nabla; f (xk)T , xk+1 = (1 &minus; αk)x̃k+1 + αkx̃k. (8.56)</p>
<p/>
</div>
<div class="page"><p/>
<p>244 8 Basic Descent Methods
</p>
<p>Note that (λk)
2 = λk+1(λk+1 &minus; 1), λk &gt; k/2 and αk &le; 0. One can prove:
</p>
<p>Theorem (Accelerated Steepest Descent). Let f (x) be convex and differentiable every-
where, satisfies the (first-order) β-Lipschitz condition, and admits a minimizer x&lowast;. Then,
the method of accelerated steepest descent generates a sequence of solutions such that
</p>
<p>f (x̃k+1) &minus; f (x&lowast;) &le;
2β
</p>
<p>k2
|x0 &minus; x&lowast; |2, &forall;k &ge; 1.
</p>
<p>Proof. We now let dk = λkxk &minus; (λk &minus; 1)x̃k &minus; x&lowast;, and δk = f (x̃k) &minus; f (x&lowast;)(&ge; 0).
Applying Lemma 1 for x = x̃k+1 and y = x̃k, convexity of f and (8.56), we have
</p>
<p>δk+1 &minus; δk = f (x̃k+1) &minus; f (xk) + f (xk) &minus; f (x̃k)
&le; &minus; β
</p>
<p>2
|x̃k+1 &minus; xk|2 + f (xk) &minus; f (x̃k)
</p>
<p>&le; &minus; β
2
|x̃k+1 &minus; xk|2 + (gk)T (xk &minus; x̃k)
</p>
<p>= &minus; β
2
|x̃k+1 &minus; xk|2 &minus; β(x̃k+1 &minus; xk)T (xk &minus; x̃k).
</p>
<p>(8.57)
</p>
<p>Applying Lemma 1 for x = x̃k+1 and y = x
&lowast;, convexity of f and (8.56), we have
</p>
<p>δk+1 = f (x̃k+1) &minus; f (xk) + f (xk) &minus; f (x&lowast;)
&le; &minus; β
</p>
<p>2
|x̃k+1 &minus; xk |2 + f (xk) &minus; f (x&lowast;)
</p>
<p>&le; &minus; β
2
|x̃k+1 &minus; xk |2 + (gk)T (xk &minus; x&lowast;)
</p>
<p>= &minus; β
2
|x̃k+1 &minus; xk |2 &minus; β(x̃k+1 &minus; xk)T (xk &minus; x&lowast;).
</p>
<p>(8.58)
</p>
<p>Multiplying (8.57) by λk(λk &minus; 1) and (8.58) by λk respectively, and summing the
two, we have
</p>
<p>(λk)
2δk+1 &minus; (λk&minus;1)2δk
</p>
<p>&le; &minus;(λk)2 β2 |x̃k+1 &minus; xk |2 &minus; λkβ(x̃k+1 &minus; xk)Tdk
= &minus; β
</p>
<p>2
((λk)
</p>
<p>2|x̃k+1 &minus; xk |2 + 2λk(x̃k+1 &minus; xk)Tdk)
= &minus; β
</p>
<p>2
(|λkx̃k+1 &minus; (λk &minus; 1)x̃k &minus; x&lowast;|2 &minus; |dk |2)
</p>
<p>=
β
</p>
<p>2
(|dk |2 &minus; |λkx̃k+1 &minus; (λk &minus; 1)x̃k &minus; x&lowast;|2).
</p>
<p>Using (8.55) and (8.56) we derive
</p>
<p>λkx̃k+1 &minus; (λk &minus; 1)x̃k = λk+1xk+1 &minus; (λk+1 &minus; 1)x̃k+1.
</p>
<p>Thus,
</p>
<p>(λk)
2δk+1 &minus; (λk&minus;1)2δk &le;
</p>
<p>β
</p>
<p>2
(|dk |2 &minus; |dk+1|2.) (8.59)
</p>
<p>Summing up (8.59) from 1 to k we have
</p>
<p>δk+1 &le;
β
</p>
<p>2(λk)2
|d1|2 &le;
</p>
<p>2β
</p>
<p>k2
|d0|2
</p>
<p>where we used facts λk &ge; k/2 and |d1| &le; |d0|. �</p>
<p/>
</div>
<div class="page"><p/>
<p>8.5 Newton&rsquo;s Method 245
</p>
<p>The Method of False Position
</p>
<p>Yet there is another steepest descent method, commonly called the BB method, that
</p>
<p>works as follows:
</p>
<p>∆xk = xk &minus; xk&minus;1 and ∆
g
</p>
<p>k
= &nabla; f (xk) &minus; &nabla; f (xk&minus;1), (8.60)
</p>
<p>αk =
(∆xk)
</p>
<p>T∆
g
</p>
<p>k
</p>
<p>(∆
g
</p>
<p>k
)T∆
</p>
<p>g
</p>
<p>k
</p>
<p>or αk =
(∆x
</p>
<p>k
)T∆x
</p>
<p>k
</p>
<p>(∆x
k
)T∆
</p>
<p>g
</p>
<p>k
</p>
<p>,
</p>
<p>Then
</p>
<p>xk+1 = xk &minus; αkgk = xk &minus; αk&nabla; f (xk)T . (8.61)
The step size of the BB method resembles the one used in quadratic curve fitting
</p>
<p>discussed for line search. There, the step size of (8.10) is given as xk&minus;1&minus;xk
f &prime;(xk&minus;1)&minus; f &prime;(xk) . If
</p>
<p>we let δx
k
= xk &minus; xk&minus;1 and δgk = f &prime;(xk) &minus; f &prime;(xk&minus;1), this quantity can be written as
</p>
<p>δx
k
δ
g
</p>
<p>k
</p>
<p>(δ
g
</p>
<p>k
)2
</p>
<p>or
(δx
</p>
<p>k
)2
</p>
<p>δx
k
δ
g
</p>
<p>k
</p>
<p>. In the vector case, multiplication is replaced by inner product.
</p>
<p>There was another explanation on the step size of the BB method. Consider con-
</p>
<p>vex quadratic minimization, and let the distinct positive eigenvalues of the Hessian
</p>
<p>Q be λ1, λ2, . . . λK . Then, if we let the step size in the method of steepest descent be
</p>
<p>αk =
1
λk
</p>
<p>, k = 1, . . . ,K, the method terminates in K iterations (which we leave as an
</p>
<p>exercise). In the BB method, αk minimizes
</p>
<p>|∆xk &minus; α∆
g
</p>
<p>k
| = |∆xk &minus; αQ∆xk |.
</p>
<p>If the error becomes 0 plus |∆xk | � 0, 1αk will be a positive eigenvalue of Q. Notice that
the objective values of the iterates generated by the BB method is not monotonically
</p>
<p>decreasing; the method may overshoot in order to have a better position in the long
</p>
<p>run.
</p>
<p>8.5 Newton&rsquo;s Method
</p>
<p>The idea behind Newton&rsquo;s method is that the function f being minimized is approx-
</p>
<p>imated locally by a quadratic function, and this approximate function is minimized
</p>
<p>exactly. Thus near xk we can approximate f by the truncated Taylor series
</p>
<p>f (x) ≃ f (xk) + &nabla; f (xk)(x &minus; xk) +
1
</p>
<p>2
(x &minus; xk)TF(xk)(x &minus; xk).
</p>
<p>The right-hand side is minimized at
</p>
<p>xk+1 = xk &minus; [F(xk)]&minus;1&nabla; f (xk)T , (8.62)</p>
<p/>
</div>
<div class="page"><p/>
<p>246 8 Basic Descent Methods
</p>
<p>and this equation is the pure form of Newton&rsquo;s method.
</p>
<p>In view of the second-order sufficiency conditions for a minimum point, we as-
</p>
<p>sume that at a relative minimum point, x&lowast;, the Hessian matrix, F(x&lowast;), is positive
definite. We can then argue that if f has continuous second partial derivatives, F(x)
</p>
<p>is positive definite near x&lowast; and hence the method is well defined near the solution.
</p>
<p>Order Two Convergence
</p>
<p>Newton&rsquo;s method has very desirable properties if started sufficiently close to the
</p>
<p>solution point. Its order of convergence is two.
</p>
<p>Theorem (Newton&rsquo;s Method). Let f &isin; C3 on En, and assume that at the local minimum
point x&lowast;, the Hessian F(x&lowast;) is positive definite. Then if started sufficiently close to x&lowast;, the
points generated by Newton&rsquo;s method converge to x&lowast;. The order of convergence is at least
two.
</p>
<p>Proof. There are ρ &gt; 0, β1 &gt; 0, β2 &gt; 0 such that for all x with |x &minus; x&lowast;| &lt; ρ, there
holds |F(x)&minus;1| &lt; β1 (see Appendix A for the definition of the norm of a matrix) and
|&nabla; f (x&lowast;)T &minus; &nabla; f (x)T &minus; F(x)(x&lowast; &minus; x)| � β2|x &minus; x&lowast;|2. Now suppose xk is selected with
β1β2|xk &minus; x&lowast;| &lt; 1 and |xk &minus; x&lowast;| &lt; ρ. Then
</p>
<p>|xk+1 &minus; x&lowast;| = |xk &minus; x&lowast; &minus; F(xk)&minus;1&nabla; f (xk)T |
= |F(xk)&minus;1[&nabla; f (x&lowast;)T &minus; &nabla; f (xk)T &minus; F(xk)(x&lowast; &minus; xk)]|
� |F(xk)&minus;1|β2|xk &minus; x&lowast;|2
</p>
<p>� β1β2|xk &minus; x&lowast;|2 &lt; |xk &minus; x&lowast;|.
</p>
<p>The final inequality shows that the new point is closer to x&lowast; than the old point, and
hence all conditions apply again to xk+1. The previous inequality establishes that
</p>
<p>convergence is second order. �
</p>
<p>Modifications
</p>
<p>Although Newton&rsquo;s method is very attractive in terms of its convergence properties
</p>
<p>near the solution, it requires modification before it can be used at points that are
</p>
<p>remote from the solution. The general nature of these modifications is discussed in
</p>
<p>the remainder of this section.
</p>
<p>1. Damping. The first modification is that usually a search parameter α is intro-
</p>
<p>duced so that the method takes the form
</p>
<p>xk+1 = xk &minus; αk[F(xk)]&minus;1&nabla; f (xk)T ,</p>
<p/>
</div>
<div class="page"><p/>
<p>8.5 Newton&rsquo;s Method 247
</p>
<p>where αk is selected to minimize f . Near the solution we expect, on the basis of
</p>
<p>how Newton&rsquo;s method was derived, that αk ≃ 1. Introducing the parameter for
general points, however, guards against the possibility that the objective might
</p>
<p>increase with αk = 1, due to nonquadratic terms in the objective function.
</p>
<p>2. Positive Definiteness. A basic consideration for Newton&rsquo;s method can be seen
</p>
<p>most clearly by a brief examination of the general class of algorithms
</p>
<p>xk+1 = xk &minus; αMkgk, (8.63)
</p>
<p>where Mk is an n&times; n matrix, α is a positive search parameter, and gk = &nabla; f (xk)T .
We note that both steepest descent (Mk = I) and Newton&rsquo;s method (Mk =
</p>
<p>[F(xk)]
&minus;1) belong to this class. The direction vector dk = &minus;Mkgk obtained in
</p>
<p>this way is a direction of descent if for small α the value of f decreases as α
</p>
<p>increases from zero. For small α we can say
</p>
<p>f (xk+1) = f (xk) + &nabla; f (xk)(xk+1 &minus; xk) + O(|xk+1 &minus; xk |2).
</p>
<p>Employing (8.51) this can be written as
</p>
<p>f (xk+1) = f (xk) &minus; αgTk Mkgk + O(α2).
</p>
<p>As α &rarr; 0, the second term on the right dominates the third. Hence if one is to
guarantee a decrease in f for small α, we must have gT
</p>
<p>k
Mkgk &gt; 0. The simplest
</p>
<p>way to insure this is to require that Mk be positive definite.
</p>
<p>The best circumstance is that where F(x) is itself positive definite throughout
</p>
<p>the search region. The objective function of many important optimization problems
</p>
<p>have this property, including for example interior-point approaches to linear pro-
</p>
<p>gramming using the logarithm as a barrier function. Indeed, it can be argued that
</p>
<p>convexity is an inherent property of the majority of well-formulated optimization
</p>
<p>problems.
</p>
<p>Therefore, assume that the Hessian matrix F(x) is positive definite throughout
</p>
<p>the search region and that f has continuous third derivatives. At a given xk define
</p>
<p>the symmetric matrix T = F(xk)
&minus;1/2. As in Sect. 8.3 introduce the change of variable
</p>
<p>Ty = x. Then according to (8.48) a steepest descent direction with respect to y is
</p>
<p>equivalent to a direction with respect to x of d = &minus;TTTg(xk), where g(xk) is the
gradient of f with respect to x at xk. Thus, d = F
</p>
<p>&minus;1g(xk). In other words, a steepest
descent direction in y is equivalent to a Newton direction in x.
</p>
<p>We can turn this relation around to analyze Newton steps in x as equivalent to
</p>
<p>gradient steps in y. We know that convergence properties in y depend on the bounds
</p>
<p>on the Hessian matrix given by (8.49) as
</p>
<p>H(y) = TTF(x)T = F&minus;1/2F(x)F&minus;1/2. (8.64)
</p>
<p>Recall that F = F(xk) which is fixed, whereas F(x) denotes the general Hessian
</p>
<p>matrix with respect to x near xk. The product (8.64) is the identity matrix at yk</p>
<p/>
</div>
<div class="page"><p/>
<p>248 8 Basic Descent Methods
</p>
<p>but the rate of convergence of steepest descent in y depends on the bounds of the
</p>
<p>smallest and largest eigenvalues of H(y) in a region near yk.
</p>
<p>These observations tell us that the damped method of Newton&rsquo;s method will con-
</p>
<p>verge at a linear rate at least as fast as c = (1 &minus; a/A) where a and A are lower
and upper bounds on the eigenvalues of F(x0)
</p>
<p>&minus;1/2F(x0)F(x0)&minus;1/2, where x0 and x0
</p>
<p>are arbitrary points in the local search region. These bounds depend, in turn, on
</p>
<p>the bounds of the third-order derivatives of f . It is clear, however, by continuity of
</p>
<p>F(x) and its derivatives, that the rate becomes very fast near the solution, becoming
</p>
<p>superlinear, and in fact, as we know, quadratic.
</p>
<p>3. Backtracking. The backtracking method of line search, using α = 1 as the
</p>
<p>initial guess, is an attractive procedure for use with Newton&rsquo;s method. Using
</p>
<p>this method the overall progress of Newton&rsquo;s method divides naturally into two
</p>
<p>phases: first a damping phase where backtracking may require α &lt; 1, and sec-
</p>
<p>ond a quadratic phase where α = 1 satisfies the backtracking criterion at every
</p>
<p>step. The damping phase was discussed above.
</p>
<p>Let us now examine the situation when close to the solution. We assume that all
</p>
<p>derivatives of f through the third are continuous and uniformly bounded. We also
</p>
<p>assume that in the region close to the solution, F(x) is positive definite with a &gt; 0
</p>
<p>and A &gt; 0 being, respectively, uniform lower and upper bounds on the eigenvalues
</p>
<p>of F(x). Using α = 1 and ε &lt; 0.5 we have for dk = &minus;F(xk)&minus;1g(xk)
</p>
<p>f (xk + dk) = f (xk) &minus; g(xk)TF(xk)&minus;1g(xk) +
1
</p>
<p>2
g(xk)
</p>
<p>TF(xk)
&minus;1g(xk) + o(|g(xk)|2)
</p>
<p>= f (xk) &minus;
1
</p>
<p>2
g(xk)
</p>
<p>TF(xk)
&minus;1g(xk) + o(|g(xk)|2)
</p>
<p>&lt; f (xk) &minus; εg(xk)TF(xk)&minus;1g(xk) + o(|g(xk)|2),
</p>
<p>where the o bound is uniform for all xk. Since |g(xk)| &rarr; 0 (uniformly) as xk &rarr; x&lowast;, it
follows that once xk is sufficiently close to x
</p>
<p>&lowast;, then f (xk+dk) &lt; f (xk)&minus;εg(xk)Tdk and
hence the backtracking test (the first part of Amijo&rsquo;s rule) is satisfied. This means
</p>
<p>that α = 1 will be used throughout the final phase.
</p>
<p>4. General Problems. In practice, Newton&rsquo;s method must be modified to ac-
</p>
<p>commodate the possible nonpositive definiteness at regions remote from the
</p>
<p>solution.
</p>
<p>A common approach is to take Mk = [εkI+F(xk)]
&minus;1 for some non-negative value
</p>
<p>of εk. This can be regarded as a kind of compromise between steepest descent (εk
very large) and Newton&rsquo;s method (εk = 0). There is always an εk that makes Mk
positive definite. We shall present one modification of this type.
</p>
<p>Let Fk &equiv; F(xk). Fix a constant δ &gt; 0. Given xk, calculate the eigenvalues of Fk
and let εk be the smallest nonnegative constant for which the matrix εkI + Fk has
</p>
<p>eigenvalues greater than or equal to δ. Then define
</p>
<p>dk = &minus;(εkI + Fk)&minus;1gk (8.65)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.5 Newton&rsquo;s Method 249
</p>
<p>and iterate according to
</p>
<p>xk+1 = xk + αkdk, (8.66)
</p>
<p>where αk minimizes f (xk + αdk), α � 0.
</p>
<p>This algorithm has the desired global and local properties. First, since the eigen-
</p>
<p>values of a matrix depend continuously on its elements, εk is a continuous function
</p>
<p>of xk and hence the mapping D : E
n &rarr; E2n defined by D(xk) = (xk, dk) is con-
</p>
<p>tinuous. Thus the algorithm A =SD is closed at points outside the solution set
</p>
<p>Ω = {x : &nabla; f (x) = 0}. Second, since εkI+Fk is positive definite, dk is a descent direc-
tion and thus Z(x) &equiv; f (x) is a continuous descent function for A. Therefore, assum-
ing the generated sequence is bounded, the Global Convergence Theorem applies.
</p>
<p>Furthermore, if δ &gt; 0 is smaller than the smallest eigenvalue of F(x&lowast;), then for xk
sufficiently close to x&lowast; we will have εk = 0, and the method reduces to Newton&rsquo;s
method. Thus this revised method also has order of convergence equal to two.
</p>
<p>The selection of an appropriate δ is somewhat of an art. A small δ means that
</p>
<p>nearly singular matrices must be inverted, while a large δ means that the order
</p>
<p>two convergence may be lost. Experimentation and familiarity with a given class
</p>
<p>of problems are often required to find the best δ.
</p>
<p>The utility of the above algorithm is hampered by the necessity to calculate the
</p>
<p>eigenvalues of F(xk), and in practice an alternate procedure is used. In one class
</p>
<p>of methods (Levenberg&ndash;Marquardt type methods), for a given value of εk, Cholesky
</p>
<p>factorization of the form εkI+F(xk) = GG
T (see Exercise 6 of Chap. 7) is employed
</p>
<p>to check for positive definiteness. If the factorization breaks down, εk is increased.
</p>
<p>The factorization then also provides the direction vector through solution of the
</p>
<p>equations GGTdk = gk, which are easily solved, since G is triangular. Then the
</p>
<p>value f (xk + dk) is examined. If it is sufficiently below f (xk), then xk+1 is accepted
</p>
<p>and a new εk+1 is determined. Essentially, ε serves as a search parameter in these
</p>
<p>methods. It should be clear from this discussion that the simplicity that Newton&rsquo;s
</p>
<p>method first seemed to promise is not fully realized in practice.
</p>
<p>Newton&rsquo;s Method and Logarithms
</p>
<p>Interior point methods of linear and nonlinear programming use barrier functions,
</p>
<p>which usually are based on the logarithm. For linear programming especially, this
</p>
<p>means that the only nonlinear terms are logarithms. Newton&rsquo;s method enjoys some
</p>
<p>special properties in this case,
</p>
<p>To illustrate, let us apply Newton&rsquo;s method to the one-dimensional problem
</p>
<p>min
x
</p>
<p>[tx &minus; ln x] (8.67)
</p>
<p>where t is a positive parameter. The derivative at x is
</p>
<p>f &prime;(x) = t &minus; 1
x
,</p>
<p/>
</div>
<div class="page"><p/>
<p>250 8 Basic Descent Methods
</p>
<p>and of course the solution is x&lowast; = 1/t, or equivalently 1 &minus; tx&lowast; = 0. The second
derivative is f &prime;&prime;(x) = 1/x2. Denoting by x+ the result of one step of a pure Newton&rsquo;s
method (with step length equal to 1) applied to the point x, we find
</p>
<p>x+ = x &minus; [ f &prime;&prime;(x)]&minus;1 f &prime;(x) = x &minus; x2
(
</p>
<p>t &minus; 1
x
</p>
<p>)
</p>
<p>= x &minus; tx2 + x
</p>
<p>= 2x &minus; tx2.
</p>
<p>Thus
</p>
<p>1 &minus; tx+ = 1 &minus; 2tx + x2t2 = (1 &minus; tx)2 (8.68)
Therefore, rather surprisingly, the quadratic nature of convergence of (1 &minus; tx) &rarr; 0
is directly evident and exact. Expression (8.68) represents a reduction in the error
</p>
<p>magnitude only if |(1 &minus; tx)| &lt; 1, or equivalently, 0 &lt; x &lt; 2/t. If x is too large,
then Newton&rsquo;s method must be used with damping until the region 0 &lt; x &lt; 2/t is
</p>
<p>reached. From then on, a step size of 1 will exhibit pure quadratic error reduction.
</p>
<p>The situation is shown in Fig. 8.11. The graph is that of f &prime;(x) = t&minus;1/x. The root-
finding form of Newton&rsquo;s method (Sect. 8.1) is then applied to this function. At each
</p>
<p>point, the tangent line is followed to the x axis to find the new point. The starting
</p>
<p>value marked x1 is far from the solution 1/t and hence following the tangent would
</p>
<p>lead to a new point that was negative. Damping must be applied at that starting point.
</p>
<p>Once a point x is reached with 0 &lt; x &lt; 1/t, all further points will remain to the left
</p>
<p>of 1/t and move toward it quadratically.
</p>
<p>Fig. 8.11 Newton&rsquo;s method applied to minimization of tx &minus; ln x
</p>
<p>In interior point methods for linear programming, a logarithmic barrier function
</p>
<p>is applied separately to the variables that must remain positive. The convergence
</p>
<p>analysis in these situations is an extension of that for the simple case given here,
</p>
<p>allowing for estimates of the rate of convergence that do not require knowledge of
</p>
<p>bounds of third-order derivatives.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.5 Newton&rsquo;s Method 251
</p>
<p>Self-concordant Functions
</p>
<p>The special properties exhibited above for the logarithm have been extended to the
</p>
<p>general class of self-concordant functions of which the logarithm is the primary
</p>
<p>example. A function f defined on the real line is self-concordant if it satisfies
</p>
<p>| f &prime;&prime;&prime;(x)| &le; 2 f &prime;&prime;(x)3/2, (8.69)
</p>
<p>throughout its domain. It is easily verified that f (x) = &minus; ln x satisfies this inequality
with equality for x &gt; 0.
</p>
<p>Self-concordancy is preserved by the addition of an affine term since such a term
</p>
<p>does not affect the second or third derivatives.
</p>
<p>A function defined on En is said to be self-concordant if it is self-concordant in
</p>
<p>every direction: that is if f (x + αd) is self-concordant with respect to α for every d
</p>
<p>throughout the domain of f .
</p>
<p>Self-concordant functions can be combined by addition and even by composition
</p>
<p>with affine functions to yield other self-concordant functions. (See Exercise 29.) For
</p>
<p>example the function
</p>
<p>f (x) = &minus;
m
&sum;
</p>
<p>i=1
</p>
<p>ln(b j &minus; aTi x),
</p>
<p>often used in interior point methods for linear programming, is self-concordant.
</p>
<p>When a self-concordant function is subjected to Newton&rsquo;s method, the quadratic
</p>
<p>convergence of final phase can be measured in terms of the function
</p>
<p>λ(x) = [&nabla; f (x)F(x)&minus;1&nabla; f (x)T ]1/2,
</p>
<p>where as usual F(x) is the Hessian matrix of f at x. Then it can be shown that close
</p>
<p>to the solution
</p>
<p>2λ(xk+1) &le; [2λ(xk)]2. (8.70)
Furthermore, in a backtracking procedure, estimates of both the stepwise progress
</p>
<p>in the damping phase and the point at which the quadratic phase begins can be
</p>
<p>expressed in terms of parameters that depend only on the backtracking parameters.
</p>
<p>Although, this knowledge does not generally influence practice, it is theoretically
</p>
<p>quite interesting.
</p>
<p>Example 1 (The Logarithmic Case). Consider the earlier example of f (x) = tx&minus;ln x.
There
</p>
<p>λ(x) = [ f &prime;(x)2/ f &prime;&prime;(x)]
1
2 = |(t &minus; 1/x)x| = |1 &minus; tx|.
</p>
<p>Then (8.70) gives
</p>
<p>(1 &minus; tx+) &le; 2(1 &minus; tx)2.
Actually, for this example, as we found in (8.68), the factor of 2 is not required.</p>
<p/>
</div>
<div class="page"><p/>
<p>252 8 Basic Descent Methods
</p>
<p>There is a relation between the analysis of self-concordant functions and our
</p>
<p>earlier convergence analysis.
</p>
<p>Recall that one way to analyze Newton&rsquo;s method is to change variables from x to
</p>
<p>y according to ỹ = [F(x)]&minus;(1/2)x̃, where here x is a reference point and x̃ is variable.
The gradient with respect to y at ỹ is then F(x)&minus;(1/2)&nabla; f (x̃), and hence the norm of the
gradient at y is [&nabla; f (x)F(x)&minus;1&nabla; f (x)T ](1/2) &equiv; λ(x). Hence it is perhaps not surprising
that λ(x) plays a role analogous to the role played by the norm of the gradient in the
</p>
<p>analysis of steepest descent.
</p>
<p>8.6 Coordinate Descent Methods
</p>
<p>The algorithms discussed in this section are sometimes attractive because of their
</p>
<p>easy implementation. Generally, however, their convergence properties are poorer
</p>
<p>than steepest descent.
</p>
<p>Let f be a function on En having continuous first partial derivatives. Given a
</p>
<p>point x = (x1, x2, . . . , xn), descent with respect to the coordinate xi ( i fixed) means
</p>
<p>that one solves
</p>
<p>minimize
xi
</p>
<p>f (x1, x2, . . . , xn).
</p>
<p>Thus only changes in the single component xi are allowed in seeking a new and
</p>
<p>better vector x (one can also consider xi the ith block of variables, called the block-
</p>
<p>coordinate method). In our general terminology, each such descent can be regarded
</p>
<p>as a descent in the direction ei(or &minus;ei) where ei is the ith unit vector. By sequentially
minimizing with respect to different components, a relative minimum of f might
</p>
<p>ultimately be determined.
</p>
<p>There are a number of ways that this concept can be developed into a full algo-
</p>
<p>rithm. The cyclic coordinate descent algorithm minimizes f cyclically with respect
</p>
<p>to the coordinate variables. Thus x1 is changed first, then x2 and so forth through xn.
</p>
<p>The process is then repeated starting with x1 again. A variation of this is the Aitken
</p>
<p>double sweep method. In this procedure one searches over x1, x2, . . . , xn, in that
</p>
<p>order, and then comes back in the order xn&minus;1, xn&minus;2, . . . , x1. These cyclic meth-
ods have the advantage of not requiring any information about &nabla; f to determine the
</p>
<p>descent directions.
</p>
<p>If the gradient of f is available, then it is possible to select the order of descent co-
</p>
<p>ordinates on the basis of the gradient. A popular technique is the Gauss&minus;Southwell
Method where at each stage the coordinate corresponding to the largest (in absolute
</p>
<p>value) component of the gradient vector is selected for descent. A randomized strat-
</p>
<p>egy can be also adapted in which one randomly chooses a coordinate to optimize in
</p>
<p>each step; see more discussions later.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.6 Coordinate Descent Methods 253
</p>
<p>Global Convergence
</p>
<p>It is simple to prove global convergence for cyclic coordinate descent. The algorith-
</p>
<p>mic map A is the composition of 2n maps
</p>
<p>A = SCnSCn&minus;1 . . .SC1,
</p>
<p>where Ci(x) = (x, ei) with ei equal to the ith unit vector, and S is the usual line search
</p>
<p>algorithm but over the doubly infinite line rather than the semi-infinite line. The map
</p>
<p>Ci is obviously continuous and S is closed. If we assume that points are restricted
</p>
<p>to a compact set, then A is closed by Corollary 1, Sect. 7.7. We define the solution
</p>
<p>set Γ = {x : &nabla; f (x) = 0}. If we impose the mild assumption on f that a search
along any coordinate direction yields a unique minimum point, then the function
</p>
<p>Z(x) &equiv; f (x) serves as a continuous descent function for A with respect to Γ. This is
because a search along any coordinate direction either must yield a decrease or, by
</p>
<p>the uniqueness assumption, it cannot change position. Therefore, if at a point x we
</p>
<p>have &nabla; f (x) � 0, then at least one component of &nabla; f (x) does not vanish and a search
</p>
<p>along the corresponding coordinate direction must yield a decrease.
</p>
<p>Local Convergence Rate
</p>
<p>It is difficult to compare the rates of convergence of these algorithms with the rates
</p>
<p>of others that we analyze. This is partly because coordinate descent algorithms are
</p>
<p>from an entirely different general class of algorithms than, for example, steepest
</p>
<p>descent and Newton&rsquo;s method, since coordinate descent algorithms are unaffected
</p>
<p>by (diagonal) scale factor changes but are affected by rotation of coordinates&mdash;the
</p>
<p>opposite being true for steepest descent. Nevertheless, some comparison is possible.
</p>
<p>It can be shown (see Exercise 20) that for the same quadratic problem as treated
</p>
<p>in Sect. 8.2, there holds for the Gauss&minus;Southwell method
</p>
<p>E(xk+1) �
</p>
<p>(
</p>
<p>1 &minus; a
A(n &minus; 1)
</p>
<p>)
</p>
<p>E(xk), (8.71)
</p>
<p>where a, A are as in Sect. 8.2 and n is the dimension of the problem. Since
</p>
<p>(
</p>
<p>A &minus; a
A + a
</p>
<p>)2
</p>
<p>�
</p>
<p>(
</p>
<p>1 &minus; a
A
</p>
<p>)
</p>
<p>�
</p>
<p>(
</p>
<p>1 &minus; a
A(n &minus; 1)
</p>
<p>)n&minus;1
, (8.72)
</p>
<p>we see that the bound we have for steepest descent is better than the bound we have
</p>
<p>for n &minus; 1 applications of the Gauss&minus;Southwell scheme. Hence we might argue that
it takes essentially n &minus; 1 coordinate searches to be as effective as a single gradient
search. This is admittedly a crude guess, since (8.54) is generally not a tight bound,
</p>
<p>but the overall conclusion is consistent with the results of many experiments. In-
</p>
<p>deed, unless the variables of a problem are essentially uncoupled from each other</p>
<p/>
</div>
<div class="page"><p/>
<p>254 8 Basic Descent Methods
</p>
<p>(corresponding to a nearly diagonal Hessian matrix) coordinate descent methods
</p>
<p>seem to require about n line searches to equal the effect of one step of steepest
</p>
<p>descent.
</p>
<p>The above discussion again illustrates the general objective that we seek in con-
</p>
<p>vergence analysis. By comparing the formula giving the rate of convergence for
</p>
<p>steepest descent with a bound for coordinate descent, we are able to draw some
</p>
<p>general conclusions on the relative performance of the two methods that are not
</p>
<p>dependent on specific values of a and A. Our analyses of local convergence proper-
</p>
<p>ties, which usually involve specific formulae, are always guided by this objective of
</p>
<p>obtaining general qualitative comparisons.
</p>
<p>Example. The quadratic problem considered in Sect. 8.2 with
</p>
<p>Q =
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>0.78 &minus;0.02 &minus;0.12 &minus;0.14
&minus;0.02 0.86 &minus;0.04 0.06
&minus;0.12 &minus;0.04 0.72 &minus;0.08
&minus;0.14 0.06 &minus;0.08 0.74
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>b = (0.76, 0.08, 1.12, 0.68)
</p>
<p>was solved by the various coordinate search methods. The corresponding values of
</p>
<p>the objective function are shown in Table 8.3. Observe that the convergence rates
</p>
<p>of the three coordinate search methods are approximately equal but that they all
</p>
<p>converge about three times slower than steepest descent. This is in accord with the
</p>
<p>estimate given above for the Gauss&minus;Southwell method, since in this case n &minus; 1 = 3.
</p>
<p>Convergence Speed of a Randomized Coordinate Descent Method
</p>
<p>We now describe a randomized strategy in selecting xi in each step of the coordinate
</p>
<p>descent method for f that is differentiable and Lipschitz continuous; that is, there
</p>
<p>exist some constants βi &gt; 0, i = 1, . . . , n, such that
</p>
<p>|&nabla;i f (x + hei) &minus; &nabla;i f (x)| &le; βi|h|, &forall; h &isin; E, x &isin; En, (8.73)
</p>
<p>where &nabla;i f (x) denotes the ith partial derivative of f at x, and ei is the ith unit vector
with the ith entry equal 1 and everywhere else equal 0.
</p>
<p>Randomized coordinate decent method. Given an initial point x0; repeat for k = 0, 1, 2, . . .
</p>
<p>1. Choose ik &isin; {1, . . . , n} randomly with a uniform distribution.
2. Update xk+1 = xk &minus; 1βik &nabla;ik f (xk)eik .
</p>
<p>Note that after k iterations, the randomized coordinate descent method generates a
</p>
<p>random sequence of xk, which depends on the observed realization of the random
</p>
<p>variable
</p>
<p>ξk&minus;1 = {i0, i1, . . . , ik&minus;1}.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.6 Coordinate Descent Methods 255
</p>
<p>Table 8.3 Solutions to Example
</p>
<p>Value of f for various methods
Iteration no. Gauss-Southwell Cyclic Double sweep
</p>
<p>0 0.0 0.0 0.0
1 &minus;0.871111 &minus;0.370256 &minus;0.370256
2 &minus;1.445584 &minus;0.376011 &minus;0.376011
3 &minus;2.087054 &minus;1.446460 &minus;1.446460
4 &minus;2.130796 &minus;2.052949 &minus;2.052949
5 &minus;2.163586 &minus;2.149690 &minus;2.060234
6 &minus;2.170272 &minus;2.149693 &minus;2.060237
7 &minus;2.172786 &minus;2.167983 &minus;2.165641
8 &minus;2.174279 &minus;2.173169 &minus;2.165704
9 &minus;2.174583 &minus;2.174392 &minus;2.168440
</p>
<p>10 &minus;2.174638 &minus;2.174397 &minus;2.173981
11 &minus;2.174651 &minus;2.174582 &minus;2.174048
12 &minus;2.174655 &minus;2.174643 &minus;2.174054
13 &minus;2.174658 &minus;2.174656 &minus;2.174608
14 &minus;2.174659 &minus;2.174656 &minus;2.174608
15 &minus;2.174659 &minus;2.174658 &minus;2.174622
16 &minus;2.174659 &minus;2.174655
17 &minus;2.174659 &minus;2.174656
18 &minus;2.174656
19 &minus;2.174659
20 &minus;2.174659
</p>
<p>Theorem 3 (Randomized Coordinate Descent&mdash;Lipschitz Convex Case). Let f (x) be
convex and differentiable everywhere, satisfy the Lipschitz condition (8.73), and admit a
</p>
<p>minimizer x&lowast;. Then, the randomized coordinate decent method generates a sequence of
solutions xk such that for any k &ge; 1, the iterate xk satisfies
</p>
<p>Eξk&minus;1 [ f (xk)] &minus; f (x&lowast;) &le;
n
</p>
<p>n + k
</p>
<p>(
</p>
<p>1
</p>
<p>2
|x0 &minus; x&lowast; |2β + f (x0) &minus; f (x&lowast;)
</p>
<p>)
</p>
<p>,
</p>
<p>where |x|β =
(
</p>
<p>&sum;
</p>
<p>i
</p>
<p>βix
2
i
</p>
<p>)1/2
</p>
<p>for all x &isin; En.
</p>
<p>Proof. Let r2
k
= |xk &minus; x&lowast;|2β =
</p>
<p>&sum;n
i=1 βi((xk)i &minus; x&lowast;i )2 for any k &ge; 0. Since xk+1 =
</p>
<p>xk &minus; 1βik &nabla;ik f (xk)eik , we have
</p>
<p>r2k+1 = r
2
k &minus; 2&nabla;ik f (xk)((xk)ik &minus; x&lowast;ik ) +
</p>
<p>1
</p>
<p>βik
(&nabla;ik f (xk))2.
</p>
<p>It follows from (8.73), Lemma 1, and xk+1 = xk &minus; 1βik &nabla;ik f (xk)eik that
</p>
<p>f (xk+1) &le; f (xk) + &nabla;ik f (xk)((xk+1)ik &minus; (xk)ik ) +
βik
</p>
<p>2
((xk+1)ik &minus; (xk)ik )2
</p>
<p>= f (xk) &minus;
1
</p>
<p>2βik
(&nabla;ik f (xk))2. (8.74)</p>
<p/>
</div>
<div class="page"><p/>
<p>256 8 Basic Descent Methods
</p>
<p>Combining the above two relations, one has
</p>
<p>r2k+1 &le; r2k &minus; 2&nabla;ik f (xk)((xk)ik &minus; x&lowast;ik ) + 2( f (xk) &minus; f (xk+1)).
</p>
<p>Multiplying both sides by 1/2 and taking expectation with respect to ik yields
</p>
<p>Eik
</p>
<p>[
</p>
<p>1
</p>
<p>2
r2k+1
</p>
<p>]
</p>
<p>&le; 1
2
r2k &minus;
</p>
<p>1
</p>
<p>n
&nabla; f (xk)(xk &minus; x&lowast;) + f (xk) &minus; Eik
</p>
<p>[
</p>
<p>f (xk+1)
]
</p>
<p>,
</p>
<p>which together with the fact that &nabla; f (xk)(x&lowast; &minus; xk) &le; f (x&lowast;) &minus; f (xk) yields
</p>
<p>Eik
</p>
<p>[
</p>
<p>1
</p>
<p>2
r2k+1
</p>
<p>]
</p>
<p>&le; 1
2
r2k +
</p>
<p>1
</p>
<p>n
f (x&lowast;) +
</p>
<p>n &minus; 1
n
</p>
<p>f (xk) &minus; Eik
[
</p>
<p>f (xk+1)
]
</p>
<p>.
</p>
<p>By rearranging terms, we obtain that for each k &ge; 0,
</p>
<p>Eik
</p>
<p>[
</p>
<p>1
</p>
<p>2
r2k+1 + f (xk+1) &minus; f (x&lowast;)
</p>
<p>]
</p>
<p>&le;
(
</p>
<p>1
</p>
<p>2
r2k + f (xk) &minus; f (x&lowast;)
</p>
<p>)
</p>
<p>&minus; 1
n
</p>
<p>( f (xk) &minus; f (x&lowast;)) .
</p>
<p>Let f &lowast; = f (x&lowast;). Then, taking expectation with respect to ξk&minus;1 on both sides of the
above relation, we have
</p>
<p>Eξk
</p>
<p>[
</p>
<p>1
</p>
<p>2
r2k+1 + f (xk+1) &minus; f &lowast;
</p>
<p>]
</p>
<p>&le; Eξk&minus;1
[
</p>
<p>1
</p>
<p>2
r2k + f (xk) &minus; f &lowast;
</p>
<p>]
</p>
<p>&minus;
Eξk&minus;1
</p>
<p>[
</p>
<p>f (xk) &minus; f &lowast;
]
</p>
<p>n
. (8.75)
</p>
<p>In addition, it follows from (8.74) that Eξ j [ f (x j+1)] &le; Eξ j&minus;1 [ f (x j)] for all j &ge; 0.
Using this relation and applying the inequality (8.75) recursively, we further obtain
</p>
<p>that
</p>
<p>Eξk
[
</p>
<p>f (xk+1)
] &minus; f &lowast; &le; Eξk
</p>
<p>[
</p>
<p>1
</p>
<p>2
r2k+1 + f (xk+1) &minus; f &lowast;
</p>
<p>]
</p>
<p>&le; 1
2
r20 + f (x0) &minus; f &lowast; &minus;
</p>
<p>1
</p>
<p>n
</p>
<p>k
&sum;
</p>
<p>j=0
</p>
<p>(
</p>
<p>Eξ j&minus;1
[
</p>
<p>f (x j)
]
</p>
<p>&minus; f &lowast;
)
</p>
<p>&le; 1
2
r20 + f (x0) &minus; f &lowast; &minus;
</p>
<p>k + 1
</p>
<p>n
</p>
<p>(
</p>
<p>Eξk
[
</p>
<p>f (xk+1)
] &minus; f &lowast;
</p>
<p>)
</p>
<p>.
</p>
<p>This leads to the desired result by moving the last term on the right to the left side.
</p>
<p>�
</p>
<p>If f is a strongly convex quadratic function, the randomized coordinate decent
</p>
<p>method would have an expected average convergence rate (1 &minus; a
An
</p>
<p>). However, each
</p>
<p>step of the method does 1
n
</p>
<p>amount of work of the full steepest descent update; see
</p>
<p>an exercise.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.7 Summary 257
</p>
<p>8.7 Summary
</p>
<p>Most iterative algorithms for minimization require a line search at every stage of
</p>
<p>the process. By employing any one of a variety of curve fitting techniques, however,
</p>
<p>the order of convergence of the line search process can be made greater than unity,
</p>
<p>which means that as compared to the linear convergence that accompanies most full
</p>
<p>descent algorithms (such as steepest descent) the individual line searches are rapid.
</p>
<p>Indeed, in common practice, only about three search points are required in any one
</p>
<p>line search. If the first derivatives are available, then two search points are required
</p>
<p>(method of false position); and if both first and second derivatives are available, then
</p>
<p>one search point is required (Newton&rsquo;s method).
</p>
<p>It was also shown in Sect. 8.1 and the exercises that line search algorithms of
</p>
<p>varying degrees of accuracy are all closed. Thus line searching is not only rapid
</p>
<p>enough to be practical but also behaves in such a way as to make analysis of global
</p>
<p>convergence simple.
</p>
<p>The most important results of this chapter are the arithmetic convergence of the
</p>
<p>method of steepest descent for solving convex minimization, the improved arith-
</p>
<p>metic convergence of the accelerated steepest descent method, and the geometric
</p>
<p>convergence of the method for solving strongly convex minimization. The fact that
</p>
<p>the method of steepest descent converges linearly with a convergence ratio equal to
</p>
<p>[(A &minus; a)/(A + a)]2, where a and A are, respectively, the smallest and largest eigen-
values of the Hessian of the objective function evaluated at the solution point. This
</p>
<p>formula, which arises frequently throughout the remainder of the book, serves as a
</p>
<p>fundamental reference point for other algorithms. It is, however, important to under-
</p>
<p>stand that it is the formula and not its value that serves as the reference. We rarely
</p>
<p>advocate that the formula be evaluated since it involves quantities (namely eigenval-
</p>
<p>ues) that are generally not computable until after the optimal solution is known. The
</p>
<p>formula itself, however, even though its value is unknown, can be used to make sig-
</p>
<p>nificant comparisons of the effectiveness of steepest descent versus other algorithms.
</p>
<p>Newton&rsquo;s method has order two convergence. However, it must be modified to
</p>
<p>insure global convergence, and evaluation of the Hessian at every point can be
</p>
<p>costly. Nevertheless, Newton&rsquo;s method provides another valuable reference point
</p>
<p>in the study of algorithms, and is frequently employed in interior point methods
</p>
<p>using a logarithmic barrier function.
</p>
<p>As optimization problem sizes become bigger and bigger, various coordinate
</p>
<p>descent algorithms are extremely popular. They are valuable especially in situations
</p>
<p>where the variables are essentially uncoupled or there is special structure that makes
</p>
<p>searching in the coordinate directions particularly easy. Typically, steepest descent
</p>
<p>can be expected to be faster. Even if the gradient is not directly available, it would
</p>
<p>probably be better to evaluate a finite-difference approximation to the gradient, by
</p>
<p>taking a single step in each coordinate direction, and use this approximation in a
</p>
<p>steepest descent algorithm, rather than executing a full line search in each coordi-
</p>
<p>nate direction.</p>
<p/>
</div>
<div class="page"><p/>
<p>258 8 Basic Descent Methods
</p>
<p>8.8 Exercises
</p>
<p>1. Show that g[a, b, c] defined by (8.14) is symmetric, that is, interchange of the
</p>
<p>arguments does not affect its value.
</p>
<p>2. Prove (8.14) and (8.15).
</p>
<p>Hint: To prove (8.15) expand it, and subtract and add g&prime;(xk) to the numerator.
3. Argue using symmetry that the error in the cubic fit method approximately sat-
</p>
<p>isfies an equation of the form
</p>
<p>εk+1 = M(ε
2
kεk&minus;1 + εkε
</p>
<p>2
k&minus;1)
</p>
<p>and then find the order of convergence.
</p>
<p>4. What conditions on the values and derivatives at two points guarantee that a
</p>
<p>cubic polynomial fit to this data will have a minimum between the two points?
</p>
<p>Use your answer to develop a search scheme, based on cubic fit, that is globally
</p>
<p>convergent for unimodal functions.
</p>
<p>5. Using a symmetry argument, find the order of convergence for a line search
</p>
<p>method that fits a cubic to xk&minus;3, xk&minus;2, xk&minus;1, xk in order to find xk+1.
6. Consider the iterative process
</p>
<p>xk+1 =
1
</p>
<p>2
</p>
<p>(
</p>
<p>xk +
a
</p>
<p>xk
</p>
<p>)
</p>
<p>,
</p>
<p>where a &gt; 0. Assuming the process converges, to what does it converge? What
</p>
<p>is the order of convergence?
</p>
<p>7. Suppose the continuous real-valued function f of a single variable satisfies
</p>
<p>min
x�0
</p>
<p>f (x) &lt; f (0).
</p>
<p>Starting at any x &gt; 0 show that, through a series of halvings and doublings
</p>
<p>of x and evaluation of the corresponding f (x)&rsquo;s, a three-point pattern can be
</p>
<p>determined.
</p>
<p>8. For δ &gt; 0 define the map Sδ by
</p>
<p>Sδ(x, d) = {y : y = x + αd, 0 � α � δ; f (y) = min
0�β�δ
</p>
<p>f (x + βd)}.
</p>
<p>Thus Sδ searches the interval [0, δ] for a minimum of f (x + αd), representing
</p>
<p>a &ldquo;limited range&rdquo; line search. Show that if f is continuous, Sδ is closed at all
</p>
<p>(x, d).
</p>
<p>9. For ε &gt; 0 define the map εS by
</p>
<p>εS(x, d) = {y : y = x + αd, α � 0, f (y) � min
0�β
</p>
<p>f (x + βd) + ε}.
</p>
<p>Show that if f is continuous, εS is closed at (x, d) if d � 0. This map corre-
</p>
<p>sponds to an &ldquo;inaccurate&rdquo; line search.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.8 Exercises 259
</p>
<p>10. Referring to the previous two exercises, define and prove a result for εSδ.
</p>
<p>11. Define S̄ as the line search algorithm that finds the first relative minimum of
</p>
<p>f (x + αd) for α � 0. If f is continuous and d � 0, is S̄ closed?
</p>
<p>12. Consider the problem
</p>
<p>minimize 5x2 + 5y2 &minus; xy &minus; 11x + 11y + 11.
</p>
<p>(a) Find a point satisfying the first-order necessary conditions for a solution.
</p>
<p>(b) Show that this point is a global minimum.
</p>
<p>(c) What would be the rate of convergence of steepest descent for this problem?
</p>
<p>(d) Starting at x = y = 0, how many steepest descent iterations would it take
</p>
<p>(at most) to reduce the function value to 10&minus;11?
</p>
<p>13. Define the search mapping F that determines the parameter α to within a given
</p>
<p>fraction c, 0 � c � 1, by
</p>
<p>F(x, d) = {y : y = x + αd, 0 � α &lt; &infin;, |α| � cα, where d
dα
</p>
<p>f (x + αd) = 0}.
</p>
<p>Show that if d � 0 and (d/dα) f (x+αd) is continuous, then F is closed at (x, d).
</p>
<p>14. Let e1, e2, . . . , en denote the eigenvectors of the symmetric positive definite
</p>
<p>n &times; n matrix Q. For the quadratic problem considered in Sect. 8.2, suppose x0
is chosen so that g0 belongs to a subspace M spanned by a subset of the ei&rsquo;s.
</p>
<p>Show that for the method of steepest descent gk &isin; M for all k. Find the rate of
convergence in this case.
</p>
<p>15. Suppose we use the method of steepest descent to minimize the quadratic func-
</p>
<p>tion f (x) = 1
2
(x &minus; x&lowast;)TQ(x &minus; x&lowast;) but we allow a tolerance &plusmn;δαk (δ � 0) in the
</p>
<p>line search, that is xk+1 = xk &minus; αkgk, where
</p>
<p>(1 &minus; δ)αk � αk � (1 + δ)αk
</p>
<p>and αk minimizes f (xk &minus; αgk) over α.
(a) Find the convergence rate of the algorithm in terms of a and A, the smallest
</p>
<p>and largest eigenvalues of Q, and the tolerance δ.
</p>
<p>Hint: Assume the extreme case αk = (1 + δ)αk.
</p>
<p>(b) What is the largest δ that guarantees convergence of the algorithm? Explain
</p>
<p>this result geometrically.
</p>
<p>(c) Does the sign of δ make any difference?
</p>
<p>16. Show that for a quadratic objective function the percentage test and the Gold-
</p>
<p>stein test are equivalent.
</p>
<p>17. Suppose in the method of steepest descent for the quadratic problem, the value
</p>
<p>of αk is not determined to minimize E(xk+1) exactly but instead only satisfies
</p>
<p>E(xk) &minus; E(xk+1)
E(xk)
</p>
<p>� β
E(xk) &minus; E
E(xk)</p>
<p/>
</div>
<div class="page"><p/>
<p>260 8 Basic Descent Methods
</p>
<p>for some β, 0 &lt; β &lt; 1, where E is the value that corresponds to the best αk.
</p>
<p>Find the best estimate for the rate of convergence in this case.
</p>
<p>18. Suppose an iterative algorithm of the form xk+1 = xk + αkdk is applied to the
</p>
<p>quadratic problem with matrix Q, where αk as usual is chosen as the mini-
</p>
<p>mum point of the line search and where dk is a vector satisfying d
T
k
</p>
<p>gk &lt; 0 and
</p>
<p>(dTk gk)
2
� β(dTk Qdk)(g
</p>
<p>T
k
</p>
<p>Q&minus;1gk), where 0 &lt; β � 1. This corresponds to a steep-
est descent algorithm with &ldquo;sloppy&rdquo; choice of direction. Estimate the rate of
</p>
<p>convergence of this algorithm.
</p>
<p>19. Repeat Exercise 18 with the condition on (dTk gk)
2 replaced by
</p>
<p>(dTk gk)
2
� β(dTk dk)(g
</p>
<p>T
k gk), 0 &lt; β � 1.
</p>
<p>20. Use the result of Exercise 19 to derive (8.71) for the Gauss-Southwell method.
</p>
<p>21. Let f (x, y) = s2 + y2 + xy &minus; 3x.
(a) Find an unconstrained local minimum point of f .
</p>
<p>(b) Why is the solution to (a) actually a global minimum point?
</p>
<p>(c) Find the minimum point of f subject to x � 0, y � 0.
</p>
<p>(d) If the method of steepest descent were applied to (a), what would be the
</p>
<p>rate of convergence of the objective function?
</p>
<p>22. Find an estimate for the rate of convergence for the modified Newton method
</p>
<p>xk+1 = xk &minus; αk(εkI + Fk)&minus;1gk
</p>
<p>given by (8.65) and (8.66) when δ is larger than the smallest eigenvalue of F(x&lowast;).
23. Prove global convergence of the Gauss-Southwell method.
</p>
<p>24. Consider a problem of the form
</p>
<p>minimize f (x)
</p>
<p>subject to x � 0,
</p>
<p>where x &isin; En. A gradient-type procedure has been suggested for this kind of
problem that accounts for the constraint. At a given point x = (x1, x2, . . . , xn),
</p>
<p>the direction d = (d1, d2, . . . , dn) is determined from the gradient &nabla; f (x)
T =
</p>
<p>g = (g1, g2, . . . , gn) by
</p>
<p>di =
</p>
<p>{
</p>
<p>&minus;gi if xi &gt; 0 or gi &lt; 0
0 if xi = 0 and gi � 0.
</p>
<p>This direction is then used as a direction of search in the usual manner.
</p>
<p>(a) What are the first-order necessary conditions for a minimum point of this
</p>
<p>problem?
</p>
<p>(b) Show that d, as determined by the algorithm, is zero only at a point satisfy-
</p>
<p>ing the first-order conditions.
</p>
<p>(c) Show that if d � 0, it is possible to decrease the value of f by movement
</p>
<p>along d.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.8 Exercises 261
</p>
<p>(d) If restricted to a compact region, does the Global Convergence Theorem
</p>
<p>apply? Why?
</p>
<p>25. Consider the quadratic problem and suppose Q has unity diagonal. Consider a
</p>
<p>coordinate descent procedure in which the coordinate to be searched is at every
</p>
<p>stage selected randomly, each coordinate being equally likely. Let εk = xk &minus; x&lowast;.
Assuming εk is known, show that ε
</p>
<p>T
k+1
</p>
<p>Qεk+1, the expected value of ε
T
k+1
</p>
<p>Qεk+1,
</p>
<p>satisfies
</p>
<p>εT
k+1
</p>
<p>Qεk+1 =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>1 &minus;
εT
k
</p>
<p>Q2εk
</p>
<p>nεT
k
</p>
<p>Qεk
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>εTk Qεk �
</p>
<p>(
</p>
<p>1 &minus; a
2
</p>
<p>nA
</p>
<p>)
</p>
<p>εTk Qεk.
</p>
<p>26. If the matrix Q has a condition number of 10, how many iterations of steepest
</p>
<p>descent would be required to get six place accuracy in the minimum value of
</p>
<p>the objective function of the corresponding quadratic problem?
</p>
<p>27. Stopping criterion. A question that arises in using an algorithm such as steep-
</p>
<p>est descent to minimize an objective function f is when to stop the iterative
</p>
<p>process, or, in other words, how can one tell when the current point is close to
</p>
<p>a solution. If, as with steepest descent, it is known that convergence is linear,
</p>
<p>this knowledge can be used to develop a stopping criterion. Let { fk}&infin;k=0 be the
sequence of values obtained by the algorithm. We assume that fk &rarr; f &lowast; linearly,
but both f &lowast; and the convergence ratio β are unknown. However we know that,
at least approximately,
</p>
<p>fk+1 &minus; f &lowast; = β( fk &minus; f &lowast;)
and
</p>
<p>fk &minus; f &lowast; = β( fk&minus;1 &minus; f &lowast;).
These two equations can be solved for β and f &lowast;.
</p>
<p>(a) Show that
</p>
<p>f &lowast; =
f 2
k
&minus; fk&minus;1 fk+1
</p>
<p>2 fk &minus; fk&minus;1 &minus; fk+1
, β =
</p>
<p>fk+1 &minus; fk
fk &minus; fk&minus;1
</p>
<p>.
</p>
<p>(b) Motivated by the above we form the sequence { f &lowast;
k
} defined by
</p>
<p>f &lowast;k =
f 2
k
&minus; fk&minus;1 fk+1
</p>
<p>2 fk &minus; fk&minus;1 &minus; fk+1
as the original sequence is generated. (This procedure of generating { f &lowast;
</p>
<p>k
}
</p>
<p>from { fk} is called the Aitken δ2-process.) If | fk &minus; f &lowast;| = βk + o(βk) show
that | f &lowast;
</p>
<p>k
&minus; f &lowast; | = o(βk) which means that { f &lowast;
</p>
<p>k
} converges to f &lowast; faster than
</p>
<p>{ fk} does. The iterative search for the minimum of f can then be terminated
when fk &minus; f &lowast;k is smaller than some prescribed tolerance.
</p>
<p>28. Show that the concordant requirement (8.69) can be expressed as
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>d
</p>
<p>dx
f &prime;&prime;(x)&minus;
</p>
<p>1
2
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>&le; 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>262 8 Basic Descent Methods
</p>
<p>29. Assume f (x) and g(x) are self-concordant. Show that the following functions
</p>
<p>are also self-concordant.
</p>
<p>(a) a f (x) for a &gt; 1
</p>
<p>(b) ax + b + f (x)
</p>
<p>(c) f (ax + b)
</p>
<p>(d) f (x) + g(x)
</p>
<p>1. Prove Lemma 1
</p>
<p>2. Consider convex quadratic minimization with matrix Q, and let its distinct pos-
</p>
<p>itive eigenvalues be λ1, λ2, . . . λK . Then, if we let the step size in the method of
</p>
<p>steepest descent be αk =
1
λk
</p>
<p>, k = 1, . . . ,K, the method terminates in K iterations.
</p>
<p>3. Show that the randomized coordinate descent method has the expected average
</p>
<p>convergence rate (1&minus; a
An
</p>
<p>) for solving strongly convex quadratic programs where
</p>
<p>a and A are smallest and largest eigenvalues of the Hessian matrix.
</p>
<p>References
</p>
<p>8.1 For a detailed exposition of Fibonacci search techniques, see Wilde and
</p>
<p>Beightler [W1]. For an introductory discussion of difference equations, see
</p>
<p>Lanczos [L1]. Many of these techniques are standard among numerical analysts.
</p>
<p>See, for example, Kowalik and Osborne [K9], or Traub [T9]. Also see Tamir
</p>
<p>[T1] for an analysis of high-order fit methods. The use of symmetry arguments
</p>
<p>to shortcut the analysis is new. The closedness of line search algorithms was
</p>
<p>established by Zangwill [Z2]. For the line search stopping criteria, see Armijo
</p>
<p>[A8], Goldstein [G12], and Wolfe [W6].
</p>
<p>8.2 For an alternate exposition of this well-known method, see Antosiewicz and
</p>
<p>Rheinboldt [A7] or Luenberger [L8]. For a proof that the estimate (8.42) is
</p>
<p>essentially exact, see Akaike [A2]. For early work on the nonquadratic case, see
</p>
<p>Curry [C10]. For recent work reports in this section see Boyd and Vandenberghe
</p>
<p>[B23]. The numerical problem considered in the example is a standard one. See
</p>
<p>Faddeev and Faddeeva [F1].
</p>
<p>8.4 The accelerated method of steepest descent is due to Nesterov [190], also see
</p>
<p>Beck and Teboulle [23]. The BB method is due to Barzilai and Borwein [17],
</p>
<p>also see Dai and Fletcher [58].
</p>
<p>8.5 For good reviews of modern Newton methods, see Fletcher [F9] and Gill, Mur-
</p>
<p>ray, and Wright [G7]. The theory of self-concordant functions was developed
</p>
<p>by Nesterov and Nemirovskri, see [N2], [N4], there is a nice reformulation by
</p>
<p>Renegar [R2] and an introduction in Boyd and Vandenberghe [B23].
</p>
<p>8.6 A detailed analysis of coordinate algorithms can be found in Fox [F17] and
</p>
<p>Isaacson and Keller [I1]. For a discussion of the Gauss-Southwell method, see
</p>
<p>Forsythe and Wasow [F16]. The proof of convergence speed of the random-
</p>
<p>ized coordinate descent method is essentially due to Nesterov [188] and Lu and
</p>
<p>Lin [160].</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 9
</p>
<p>Conjugate Direction Methods
</p>
<p>Conjugate direction methods can be regarded as being somewhat intermediate
</p>
<p>between the method of steepest descent and Newton&rsquo;s method. They are motivated
</p>
<p>by the desire to accelerate the typically slow convergence associated with steepest
</p>
<p>descent while avoiding the information requirements associated with the evaluation,
</p>
<p>storage, and inversion of the Hessian (or at least solution of a corresponding system
</p>
<p>of equations) as required by Newton&rsquo;s method.
</p>
<p>Conjugate direction methods invariably are invented and analyzed for the purely
</p>
<p>quadratic problem
</p>
<p>minimize
1
</p>
<p>2
xT Qx &minus; bTx,
</p>
<p>where Q is an n&times;n symmetric positive definite matrix. The techniques once worked
out for this problem are then extended, by approximation, to more general problems;
</p>
<p>it being argued that, since near the solution point every problem is approximately
</p>
<p>quadratic, convergence behavior is similar to that for the pure quadratic situation.
</p>
<p>The area of conjugate direction algorithms has been one of great creativity
</p>
<p>in the nonlinear programming field, illustrating that detailed analysis of the pure
</p>
<p>quadratic problem can lead to significant practical advances. Indeed, conjugate di-
</p>
<p>rection methods, especially the method of conjugate gradients, have proved to be
</p>
<p>extremely effective in dealing with general objective functions and are considered
</p>
<p>among the best general purpose methods.
</p>
<p>9.1 Conjugate Directions
</p>
<p>Definition. Given a symmetric matrix Q, two vectors d1 and d2 are said to be Q-orthogonal,
or conjugate with respect to Q, if dT1 Qd2 = 0.
</p>
<p>&copy; Springer International Publishing Switzerland 2016
</p>
<p>D.G. Luenberger, Y. Ye, Linear and Nonlinear Programming, International
Series in Operations Research &amp; Management Science 228,
DOI 10.1007/978-3-319-18842-3 9
</p>
<p>263</p>
<p/>
</div>
<div class="page"><p/>
<p>264 9 Conjugate Direction Methods
</p>
<p>In the applications that we consider, the matrix Q will be positive definite but this
</p>
<p>is not inherent in the basic definition. Thus if Q = 0, any two vectors are conjugate,
</p>
<p>while if Q = I, conjugacy is equivalent to the usual notion of orthogonality. A finite
</p>
<p>set of vectors d0, d1, . . . , dk is said to be a Q-orthogonal set if d
T
i
</p>
<p>Qd j = 0 for all
</p>
<p>i � j.
</p>
<p>Proposition. If Q is positive definite and the set of nonzero vectors d0, d1, d2 , . . . , dk are
</p>
<p>Q-orthogonal, then these vectors are linearly independent.
</p>
<p>Proof. Suppose there are constants αi, i = 0, 1, 2, . . . , k such that
</p>
<p>α0d0 + &middot; &middot; &middot; + αkdk = 0.
</p>
<p>Multiplying by Q and taking the scalar product with di yields
</p>
<p>αid
T
i Qdi = 0.
</p>
<p>Or, since dT
i
</p>
<p>Qdi &gt; 0 in view of the positive definiteness of Q, we have αi = 0. �
</p>
<p>Before discussing the general conjugate direction algorithm, let us investigate
</p>
<p>just why the notion of Q-orthogonality is useful in the solution of the quadratic
</p>
<p>problem
</p>
<p>minimize
1
</p>
<p>2
xTQx &minus; bTx, (9.1)
</p>
<p>when Q is positive definite. Recall that the unique solution to this problem is also
</p>
<p>the unique solution to the linear equation
</p>
<p>Qx = b, (9.2)
</p>
<p>and hence that the quadratic minimization problem is equivalent to a linear equation
</p>
<p>problem.
</p>
<p>Corresponding to the n &times; n positive definite matrix Q let d0, d1, . . . , dn&minus;1 be n
nonzero Q-orthogonal vectors. By the above proposition they are linearly indepen-
</p>
<p>dent, which implies that the solution x&lowast; of (9.1) or (9.2) can be expanded in terms
of them as
</p>
<p>x&lowast; = α0d0 + &middot; &middot; &middot; + αn&minus;1dn&minus;1 (9.3)
for some set of αi&rsquo;s. In fact, multiplying by Q and then taking the scalar product
</p>
<p>with di yields directly
</p>
<p>αi =
dT
i
</p>
<p>Qx&lowast;
</p>
<p>dT
i
</p>
<p>Qdi
=
</p>
<p>dT
i
</p>
<p>b
</p>
<p>dT
i
</p>
<p>Qdi
. (9.4)
</p>
<p>This shows that the αi&rsquo;s and consequently the solution x
&lowast; can be found by evaluation
</p>
<p>of simple scalar products. The end result is
</p>
<p>x&lowast; =
n&minus;1
&sum;
</p>
<p>i=0
</p>
<p>dT
i
</p>
<p>b
</p>
<p>dT
i
</p>
<p>Qdi
di. (9.5)</p>
<p/>
</div>
<div class="page"><p/>
<p>9.1 Conjugate Directions 265
</p>
<p>There are two basic ideas imbedded in (9.5). The first is the idea of selecting
</p>
<p>an orthogonal set of di&rsquo;s so that by taking an appropriate scalar product, all terms
</p>
<p>on the right side of (9.3), except the ith, vanish. This could, of course, have been
</p>
<p>accomplished by making the di&rsquo;s orthogonal in the ordinary sense instead of mak-
</p>
<p>ing them Q-orthogonal. The second basic observation, however, is that by using
</p>
<p>Q-orthogonality the resulting equation for αi can be expressed in terms of the known
</p>
<p>vector b rather than the unknown vector x&lowast;; hence the coefficients can be evaluated
without knowing x&lowast;.
</p>
<p>The expansion for x&lowast; can be considered to be the result of an iterative process
of n steps where at the ith step αidi is added. Viewing the procedure this way, and
</p>
<p>allowing for an arbitrary initial point for the iteration, the basic conjugate direction
</p>
<p>method is obtained.
</p>
<p>Conjugate Direction Theorem. Let {di}n&minus;1i=0 be a set of nonzero Q-orthogonal vectors. For
any x0 &isin; En the sequence {xk} generated according to
</p>
<p>xk+1 = xk + αkdk, k � 0 (9.6)
</p>
<p>with
</p>
<p>αk = &minus;
gT
k
</p>
<p>dk
</p>
<p>dT
k
</p>
<p>Qdk
(9.7)
</p>
<p>and
</p>
<p>gk = Qxk &minus; b,
converges to the unique solution, x&lowast;, of Qx = b after n steps, that is, xn = x&lowast;.
</p>
<p>Proof. Since the dk&rsquo;s are linearly independent, we can write
</p>
<p>x&lowast; &minus; x0 = α0d0 + α1d1 + &middot; &middot; &middot; + αn&minus;1dn&minus;1
</p>
<p>for some set of αk&rsquo;s. As we did to get (9.4), we multiply by Q and take the scalar
</p>
<p>product with dk to find
</p>
<p>αk =
dT
k
</p>
<p>Q(x&lowast; &minus; x0)
dT
k
</p>
<p>Qdk
. (9.8)
</p>
<p>Now following the iterative process (9.6) from x0 up to xk gives
</p>
<p>xk &minus; x0 = α0d0 + α1d1 + &middot; &middot; &middot; + αk&minus;1dk&minus;1, (9.9)
</p>
<p>and hence by the Q-orthogonality of the dk&rsquo;s it follows that
</p>
<p>dTk Q(xk &minus; x0) = 0. (9.10)
</p>
<p>Substituting (9.10) into (9.8) produces
</p>
<p>αk =
dT
k
</p>
<p>Q(x&lowast; &minus; xk)
dT
k
</p>
<p>Qdk
= &minus;
</p>
<p>gT
k
</p>
<p>dk
</p>
<p>dT
k
</p>
<p>Qdk
,
</p>
<p>which is identical with (9.7). �</p>
<p/>
</div>
<div class="page"><p/>
<p>266 9 Conjugate Direction Methods
</p>
<p>To this point the conjugate direction method has been derived essentially through
</p>
<p>the observation that solving (9.1) is equivalent to solving (9.2). The conjugate di-
</p>
<p>rection method has been viewed simply as a somewhat special, but nevertheless
</p>
<p>straightforward, orthogonal expansion for the solution to (9.2). This viewpoint, al-
</p>
<p>though important because of its underlying simplicity, ignores some of the most
</p>
<p>important aspects of the algorithm; especially those aspects that are important when
</p>
<p>extending the method to nonquadratic problems. These additional properties are dis-
</p>
<p>cussed in the next section.
</p>
<p>Also, methods for selecting or generating sequences of conjugate directions have
</p>
<p>not yet been presented. Some methods for doing this are discussed in the exer-
</p>
<p>cises; while the most important method, that of conjugate gradients, is discussed
</p>
<p>in Sect. 9.3.
</p>
<p>9.2 Descent Properties of the Conjugate Direction Method
</p>
<p>We define Bk as the subspace of En spanned by {d0, d1, . . . , dk&minus;1}. We shall show
that as the method of conjugate directions progresses each xk minimizes the objec-
</p>
<p>tive over the k-dimensional linear variety x0 + Bk.
Expanding Subspace Theorem. Let {di}n&minus;1i=0 be a sequence of nonzero Q-orthogonal vectors
in En. Then for any x0 &isin; En the sequence {xk} generated according to
</p>
<p>xk+1 = xk + αkdk (9.11)
</p>
<p>αk = &minus;
gT
k
</p>
<p>dk
</p>
<p>dT
k
</p>
<p>Qdk
(9.12)
</p>
<p>has the property that xk minimizes f (x) =
1
2
xTQx&minus;bTx on the line x = xk&minus;1 +αdk&minus;1,&minus;&infin; &lt;
</p>
<p>α &lt; &infin;, as well as on the linear variety x0 +Bk.
</p>
<p>Proof. It need only be shown that xk minimizes f on the linear variety x0 + Bk,
since it contains the line x = xk&minus;1 + αdk&minus;1. Since f is a strictly convex function,
the conclusion will hold if it can be shown that gk is orthogonal to Bk (that is, the
gradient of f at xk is orthogonal to the subspace Bk). The situation is illustrated in
Fig. 9.1. (Compare Theorem 2, Sect. 7.5.)
</p>
<p>We prove gk &perp; Bk by induction. Since B0 is empty that hypothesis is true for
k = 0. Assuming that it is true for k, that is, assuming gk &perp; Bk, we show that
gk+1 &perp; Bk+1. We have
</p>
<p>gk+1 = gk + αkQdk, (9.13)
</p>
<p>and hence
</p>
<p>dTk gk+1 = d
T
k gk + αkd
</p>
<p>T
k Qdk = 0 (9.14)
</p>
<p>by definition of αk. Also for i &lt; k
</p>
<p>dTi gk+1 = d
T
i gk + αkd
</p>
<p>T
i Qdk. (9.15)</p>
<p/>
</div>
<div class="page"><p/>
<p>9.2 Descent Properties of the Conjugate Direction Method 267
</p>
<p>The first term on the right-hand side of (9.15) vanishes because of the induction
</p>
<p>hypothesis, while the second vanishes by the Q-orthogonality of the di&rsquo;s. Thus
</p>
<p>gk+1 &perp; Bk+1. �
</p>
<p>Fig. 9.1 Conjugate direction method
</p>
<p>Corollary. In the method of conjugate directions the gradients gk, k = 0, 1, . . . , n satisfy
</p>
<p>gTk di = 0 for i &lt; k.
</p>
<p>The above theorem is referred to as the Expanding Subspace Theorem, since the
</p>
<p>Bk&rsquo;s form a sequence of subspaces with Bk+1 &sup; Bk. Since xk minimizes f over
x0 + Bk, it is clear that xn must be the overall minimum of f .
</p>
<p>Fig. 9.2 Interpretation of expanding subspace theorem</p>
<p/>
</div>
<div class="page"><p/>
<p>268 9 Conjugate Direction Methods
</p>
<p>To obtain another interpretation of this result we again introduce the function
</p>
<p>E(x) =
1
</p>
<p>2
(x &minus; x&lowast;)TQ(x &minus; x&lowast;) (9.16)
</p>
<p>as a measure of how close the vector x is to the solution x&lowast;. Since E(x) =
f (x) + (1/2)x&lowast;TQx&lowast; the function E can be regarded as the objective that we seek
to minimize.
</p>
<p>By considering the minimization of E we can regard the original problem as one
</p>
<p>of minimizing a generalized distance from the point x&lowast;. Indeed, if we had Q = I,
the generalized notion of distance would correspond (within a factor of two) to the
</p>
<p>usual Euclidean distance. For an arbitrary positive-definite Q we say E is a general-
</p>
<p>ized Euclidean metric or distance function. Vectors di, i = 0, 1, . . . , n &minus; 1 that are
Q-orthogonal may be regarded as orthogonal in this generalized Euclidean space
</p>
<p>and this leads to the simple interpretation of the Expanding Subspace Theorem il-
</p>
<p>lustrated in Fig. 9.2. For simplicity we assume x0 = 0. In the figure dk is shown as
</p>
<p>being orthogonal to Bk with respect to the generalized metric. The point xk mini-
mizes E over Bk while xk+1 minimizes E over Bk+1. The basic property is that, since
dk is orthogonal to Bk, the point xk+1 can be found by minimizing E along dk and
adding the result to xk.
</p>
<p>9.3 The Conjugate Gradient Method
</p>
<p>The conjugate gradient method is the conjugate direction method that is obtained by
</p>
<p>selecting the successive direction vectors as a conjugate version of the successive
</p>
<p>gradients obtained as the method progresses. Thus, the directions are not specified
</p>
<p>beforehand, but rather are determined sequentially at each step of the iteration. At
</p>
<p>step k one evaluates the current negative gradient vector and adds to it a linear com-
</p>
<p>bination of the previous direction vectors to obtain a new conjugate direction vector
</p>
<p>along which to move.
</p>
<p>There are three primary advantages to this method of direction selection. First,
</p>
<p>unless the solution is attained in less than n steps, the gradient is always nonzero
</p>
<p>and linearly independent of all previous direction vectors. Indeed, the gradient gk
is orthogonal to the subspace Bk generated by d0, d1, . . . , dk&minus;1. If the solution is
reached before n steps are taken, the gradient vanishes and the process terminates&mdash;
</p>
<p>it being unnecessary, in this case, to find additional directions.
</p>
<p>Second, a more important advantage of the conjugate gradient method is the
</p>
<p>especially simple formula that is used to determine the new direction vector. This
</p>
<p>simplicity makes the method only slightly more complicated than steepest descent.
</p>
<p>Third, because the directions are based on the gradients, the process makes good
</p>
<p>uniform progress toward the solution at every step. This is in contrast to the situation
</p>
<p>for arbitrary sequences of conjugate directions in which progress may be slight until
</p>
<p>the final few steps. Although for the pure quadratic problem uniform progress is of
</p>
<p>no great importance, it is important for generalizations to nonquadratic problems.</p>
<p/>
</div>
<div class="page"><p/>
<p>9.3 The Conjugate Gradient Method 269
</p>
<p>Conjugate Gradient Algorithm
</p>
<p>Starting at any x0 &isin; En define d0 = &minus;g0 = b &minus; Qx0 and
</p>
<p>xk+1 = xk + αkdk (9.17)
</p>
<p>αk = &minus;
gT
k
</p>
<p>dk
</p>
<p>dT
k
</p>
<p>Qdk
(9.18)
</p>
<p>dk+1 = &minus;gk+1 + βkdk (9.19)
</p>
<p>βk =
gT
k+1
</p>
<p>Qdk
</p>
<p>dT
k
</p>
<p>Qdk
, (9.20)
</p>
<p>where gk = Qxk &minus; b.
In the algorithm the first step is identical to a steepest descent step; each succeed-
</p>
<p>ing step moves in a direction that is a linear combination of the current gradient and
</p>
<p>the preceding direction vector. The attractive feature of the algorithm is the simple
</p>
<p>formulae, (9.19) and (9.20), for updating the direction vector. The method is only
</p>
<p>slightly more complicated to implement than the method of steepest descent but
</p>
<p>converges in a finite number of steps.
</p>
<p>Verification of the Algorithm
</p>
<p>To verify that the algorithm is a conjugate direction algorithm, it is necessary to
</p>
<p>verify that the vectors {dk} are Q-orthogonal. It is easiest to prove this by simulta-
neously proving a number of other properties of the algorithm. This is done in the
</p>
<p>theorem below where the notation [d0, d1, . . . , dk] is used to denote the subspace
</p>
<p>spanned by the vectors d0, d1, . . . , dk.
</p>
<p>Conjugate Gradient Theorem. The conjugate gradient algorithm (9.17)&ndash;(9.20) is a conju-
gate direction method. If it does not terminate at xk, then
</p>
<p>a) [g0, g1, . . . , gk] = [g0 , Qg0, . . . , Q
kg0]
</p>
<p>b) [d0, d1, . . . , dk] = [g0 , Qg0, . . . , Q
kg0]
</p>
<p>c) dT
k
</p>
<p>Qdi = 0 for i � k &minus; 1
d) αk = g
</p>
<p>T
k
</p>
<p>gk/d
T
k
</p>
<p>Qdk
</p>
<p>e) βk = g
T
k+1gk+1/g
</p>
<p>T
k
</p>
<p>gk.
</p>
<p>Proof. We first prove (a), (b) and (c) simultaneously by induction. Clearly, they are
</p>
<p>true for k = 0. Now suppose they are true for k, we show that they are true for k + 1.
</p>
<p>We have
</p>
<p>gk+1 = gk + αkQdk.
</p>
<p>By the induction hypothesis both gk and Qdk belong to [g0, Qg0, . . . , Q
k+1g0], the
</p>
<p>first by (a) and the second by (b). Thus gk+1 &isin; [g0, Qg0, . . . , Qk+1g0]. Further-
more gk+1 � [g0, Qg0, . . . , Q
</p>
<p>kg0] = [d0, d1, . . . , dk] since otherwise gk+1 = 0,</p>
<p/>
</div>
<div class="page"><p/>
<p>270 9 Conjugate Direction Methods
</p>
<p>because for any conjugate direction method gk+1 is orthogonal to [d0, d1, . . . , dk].
</p>
<p>(The induction hypothesis on (c) guarantees that the method is a conjugate direction
</p>
<p>method up to xk+1.) Thus, finally we conclude that
</p>
<p>[g0, g1, . . . , gk+1] = [g0, Qg0, . . . , Q
k+1g0],
</p>
<p>which proves (a).
</p>
<p>To prove (b) we write
dk+1 = &minus;gk+1 + βkdk,
</p>
<p>and (b) immediately follows from (a) and the induction hypothesis on (b).
</p>
<p>Next, to prove (c) we have
</p>
<p>dTk+1Qdi = &minus;gTk+1Qdi + βkdTk Qdi.
</p>
<p>For i = k the right side is zero by definition of βk. For i &lt; k both terms vanish.
</p>
<p>The first term vanishes since Qdi &isin; [d1, d2, . . . , di+1], the induction hypothe-
sis which guarantees the method is a conjugate direction method up to xk+1, and
</p>
<p>by the Expanding Subspace Theorem that guarantees that gk+1 is orthogonal to
</p>
<p>[d0, d1, . . . , di+1]. The second term vanishes by the induction hypothesis on (c).
</p>
<p>This proves (c), which also proves that the method is a conjugate direction method.
</p>
<p>To prove (d) we have
</p>
<p>&minus;gTk dk = gTk gk &minus; βk&minus;1gTk dk&minus;1,
</p>
<p>and the second term is zero by the Expanding Subspace Theorem.
</p>
<p>Finally, to prove (e) we note that gT
k+1
</p>
<p>gk = 0, because gk &isin; [d0, . . . , dk] and gk+1
is orthogonal to [d0, . . . , dk]. Thus since
</p>
<p>Qdk =
1
</p>
<p>αk
(gk+1 &minus; gk),
</p>
<p>we have
</p>
<p>gTk+1Qdk =
1
</p>
<p>αk
gTk+1gk+1. �
</p>
<p>Parts (a) and (b) of this theorem are a formal statement of the interrelation between
</p>
<p>the direction vectors and the gradient vectors. Part (c) is the equation that verifies
</p>
<p>that the method is a conjugate direction method. Parts (d) and (e) are identities
</p>
<p>yielding alternative formulae for αk and βk that are often more convenient than the
</p>
<p>original ones.
</p>
<p>9.4 The C&ndash;G Method as an Optimal Process
</p>
<p>We turn now to the description of a special viewpoint that leads quickly to some
</p>
<p>very profound convergence results for the method of conjugate gradients. The basis
</p>
<p>of the viewpoint is part (b) of the Conjugate Gradient Theorem. This result tells us</p>
<p/>
</div>
<div class="page"><p/>
<p>9.4 The C&ndash;G Method as an Optimal Process 271
</p>
<p>the spaces Bk over which we successively minimize are determined by the original
gradient g0 and multiplications of it by Q. Each step of the method brings into
</p>
<p>consideration an additional power of Q times g0. It is this observation we exploit.
</p>
<p>Let us consider a new general approach for solving the quadratic minimization
</p>
<p>problem. Given an arbitrary starting point x0, let
</p>
<p>xk+1 = x0 + Pk(Q)g0, (9.21)
</p>
<p>where Pk is a polynomial of degree k. Selection of a set of coefficients for each of
</p>
<p>the polynomials Pk determines a sequence of xk&rsquo;s. We have
</p>
<p>xk+1 &minus; x&lowast; = x0 &minus; x&lowast; + Pk(Q)Q(x0 &minus; x&lowast;)
= [I + QPk(Q)](x0 &minus; x&lowast;), (9.22)
</p>
<p>and hence
</p>
<p>E(xk+1) =
1
</p>
<p>2
(xk+1 &minus; x&lowast;)TQ(xk+1 &minus; x&lowast;)
</p>
<p>=
1
</p>
<p>2
(x0 &minus; x&lowast;)TQ[I + QPk(Q)]2(x0 &minus; x&lowast;). (9.23)
</p>
<p>We may now pose the problem of selecting the polynomial Pk in such a way as to
</p>
<p>minimize E(xk+1) with respect to all possible polynomials of degree k. Expanding
</p>
<p>(9.21), however, we obtain
</p>
<p>xk+1 = x0 + γ0g0 + γ1Qg0 + &middot; &middot; &middot; + γkQkg0, (9.24)
</p>
<p>where the γi&rsquo;s are the coefficients of Pk. In view of
</p>
<p>Bk+1 = [d0, d1, . . . , dk] = [g0, Qg0, . . . , Qkg0],
</p>
<p>the vector xk+1 = x0+α0d0+α1d1+ . . .+αkdk generated by the method of conjugate
</p>
<p>gradients has precisely this form; moreover, according to the Expanding Subspace
</p>
<p>Theorem, the coefficients γi determined by the conjugate gradient process are such
</p>
<p>as to minimize E(xk+1). Therefore, the problem posed of selecting the optimal Pk is
</p>
<p>solved by the conjugate gradient procedure.
</p>
<p>The explicit relation between the optimal coefficients γi of Pk and the constants
</p>
<p>αi, βi associated with the conjugate gradient method is, of course, somewhat com-
</p>
<p>plicated, as is the relation between the coefficients of Pk and those of Pk+1. The
</p>
<p>power of the conjugate gradient method is that as it progresses it successively solves
</p>
<p>each of the optimal polynomial problems while updating only a small amount of
</p>
<p>information.
</p>
<p>We summarize the above development by the following very useful theorem.</p>
<p/>
</div>
<div class="page"><p/>
<p>272 9 Conjugate Direction Methods
</p>
<p>Theorem 1. The point xk+1 generated by the conjugate gradient method satisfies
</p>
<p>E(xk+1) = min
Pk
</p>
<p>1
</p>
<p>2
(x0 &minus; x&lowast;)TQ[I + QPk(Q)]2(x0 &minus; x&lowast;), (9.25)
</p>
<p>where the minimum is taken with respect to all polynomials Pk of degree k.
</p>
<p>Bounds on Convergence
</p>
<p>To use Theorem 1 most effectively it is convenient to recast it in terms of eigenvec-
</p>
<p>tors and eigenvalues of the matrix Q. Suppose that the vector x0 &minus; x&lowast; is written in
the eigenvector expansion
</p>
<p>x0 &minus; x&lowast; = ξ1e1 + ξ2e2 + &middot; &middot; &middot; + ξnen,
</p>
<p>where the ei&rsquo;s are normalized eigenvectors of Q. Then since Q(x0 &minus; x&lowast;) = λ1ξ1e1 +
λ2ξ2e2 + . . . + λnξnen and since the eigenvectors are mutually orthogonal, we have
</p>
<p>E(x0) =
1
</p>
<p>2
(x0 &minus; x&lowast;)TQ(x0 &minus; x&lowast;) =
</p>
<p>1
</p>
<p>2
</p>
<p>n
&sum;
</p>
<p>i=1
</p>
<p>λiξ
2
i , (9.26)
</p>
<p>where the λi&rsquo;s are the corresponding eigenvalues of Q. Applying the same manipu-
</p>
<p>lations to (9.25), we find that for any polynomial Pk of degree k there holds
</p>
<p>E(xk+1) �
1
</p>
<p>2
</p>
<p>n
&sum;
</p>
<p>i=1
</p>
<p>[1 + λiPk(λi)]
2λiξ
</p>
<p>2
i .
</p>
<p>It then follows that
</p>
<p>E(xk+1) � max
λi
</p>
<p>[1 + λiPk(λi)]
2 1
</p>
<p>2
</p>
<p>n
&sum;
</p>
<p>i=1&prime;
</p>
<p>λiξ
2
i ,
</p>
<p>and hence finally
</p>
<p>E(xk+1) � max
λi
</p>
<p>[1 + λiPk(λi)]
2E(x0).
</p>
<p>We summarize this result by the following theorem.
</p>
<p>Theorem 2. In the method of conjugate gradients we have
</p>
<p>E(xk+1) � max
λi
</p>
<p>[1 + λiPk(λi)]
2E(x0) (9.27)
</p>
<p>for any polynomial Pk of degree k, where the maximum is taken over all eigenvalues λi
of Q.
</p>
<p>This way of viewing the conjugate gradient method as an optimal process is ex-
</p>
<p>ploited in the next section. We note here that it implies the far from obvious fact that
</p>
<p>every step of the conjugate gradient method is at least as good as a steepest descent</p>
<p/>
</div>
<div class="page"><p/>
<p>9.5 The Partial Conjugate Gradient Method 273
</p>
<p>step would be from the same point. To see this, suppose xk has been computed by
</p>
<p>the conjugate gradient method. From (9.24) we know xk has the form
</p>
<p>xk = x0 + γ̄0g0 + γ̄1Qg0 + &middot; &middot; &middot; + γ̄k&minus;1Qk&minus;1g0.
</p>
<p>Now if xk+1 is computed from xk by steepest descent, then xk+1 = xk&minus;αkgk for some
αk. In view of part (a) of the Conjugate Gradient Theorem xk+1 will have the form
</p>
<p>(9.24). Since for the conjugate direction method E(xk+1) is lower than any other xk+1
of the form (9.24), we obtain the desired conclusion.
</p>
<p>Typically when some information about the eigenvalue structure of Q is known,
</p>
<p>that information can be exploited by construction of a suitable polynomial Pk to use
</p>
<p>in (9.27). Suppose, for example, it were known that Q had onlym &lt; n distinct eigen-
</p>
<p>values. Then it is clear that by suitable choice of Pm&minus;1 it would be possible to make
the mth degree polynomial 1+λPm&minus;1(λ) have its m zeros at the m eigenvalues. Using
that particular polynomial in (9.27) shows that E(xm) = 0. Thus the optimal solution
</p>
<p>will be obtained in at most m, rather than n, steps. More sophisticated examples of
</p>
<p>this type of reasoning are contained in the next section and in the exercises at the
</p>
<p>end of the chapter.
</p>
<p>9.5 The Partial Conjugate Gradient Method
</p>
<p>A collection of procedures that are natural to consider at this point are those in
</p>
<p>which the conjugate gradient procedure is carried out for m + 1 &lt; n steps and then,
</p>
<p>rather than continuing, the process is restarted from the current point and m + 1
</p>
<p>more conjugate gradient steps are taken. The special case of m = 0 corresponds
</p>
<p>to the standard method of steepest descent, while m = n &minus; 1 corresponds to the
full conjugate gradient method. These partial conjugate gradient methods are of
</p>
<p>extreme theoretical and practical importance, and their analysis yields additional
</p>
<p>insight into the method of conjugate gradients. The development of the last section
</p>
<p>forms the basis of our analysis.
</p>
<p>As before, given the problem
</p>
<p>minimize
1
</p>
<p>2
xTQx &minus; bTx, (9.28)
</p>
<p>we define for any point xk the gradient gk = Qxk&minus;b. We consider an iteration scheme
of the form
</p>
<p>xk+1 = xk + P
k(Q)gk, (9.29)
</p>
<p>where Pk is a polynomial of degree m. We select the coefficients of the polynomial
</p>
<p>Pk so as to minimize
</p>
<p>E(xk+1) =
1
</p>
<p>2
(xk+1 &minus; x&lowast;)TQ(xk+1 &minus; x&lowast;), (9.30)</p>
<p/>
</div>
<div class="page"><p/>
<p>274 9 Conjugate Direction Methods
</p>
<p>where x&lowast; is the solution to (9.28). In view of the development of the last section, it
is clear that xk+1 can be found by taking m + 1 conjugate gradient steps rather than
</p>
<p>explicitly determining the appropriate polynomial directly. (The sequence indexing
</p>
<p>is slightly different here than in the previous section, since now we do not give
</p>
<p>separate indices to the intermediate steps of this process. Going from xk to xk+1 by
</p>
<p>the partial conjugate gradient method involves m other points.)
</p>
<p>The results of the previous section provide a tool for convergence analysis of
</p>
<p>this method. In this case, however, we develop a result that is of particular interest
</p>
<p>for Q&rsquo;s having a special eigenvalue structure that occurs frequently in optimization
</p>
<p>problems, especially, as shown below and in Chap. 12, in the context of penalty
</p>
<p>function methods for solving problems with constraints. We imagine that the eigen-
</p>
<p>values of Q are of two kinds: there are m large eigenvalues that may or may not be
</p>
<p>located near each other, and n &minus; m smaller eigenvalues located within an interval
[a, b]. Such a distribution of eigenvalues is shown in Fig. 9.3.
</p>
<p>As an example, consider as in Sect. 8.3 the problem on En
</p>
<p>minimize
1
</p>
<p>2
xTQx &minus; bTx
</p>
<p>subject to cTx = 0,
</p>
<p>Fig. 9.3 Eigenvalue distribution
</p>
<p>where Q is a symmetric positive definite matrix with eigenvalues in the interval
</p>
<p>[a, A] and b and c are vectors in En. This is a constrained problem but it can be
</p>
<p>approximated by the unconstrained problem
</p>
<p>minimize
1
</p>
<p>2
xTQx &minus; bTx + 1
</p>
<p>2
μ(cTx)2,
</p>
<p>where μ is a large positive constant. The last term in the objective function is called
</p>
<p>a penalty term; for large μ minimization with respect to x will tend to make cTx
</p>
<p>small.
</p>
<p>The total quadratic term in the objective is 1
2
xT (Q + μccT )x, and thus it is appro-
</p>
<p>priate to consider the eigenvalues of the matrix Q + μccT . As μ tends to infinity it
</p>
<p>can be shown (see Chap. 13) that one eigenvalue of this matrix tends to infinity and
</p>
<p>the other n &minus; 1 eigenvalues remain bounded within the original interval [a, A].
As noted before, if steepest descent were applied to a problem with such a struc-
</p>
<p>ture, convergence would be governed by the ratio of the smallest to largest eigen-
</p>
<p>value, which in this case would be quite unfavorable. In the theorem below it is
</p>
<p>stated that by successively repeating m+1 conjugate gradient steps the effects of the</p>
<p/>
</div>
<div class="page"><p/>
<p>9.5 The Partial Conjugate Gradient Method 275
</p>
<p>m largest eigenvalues are eliminated and the rate of convergence is determined as if
</p>
<p>they were not present. A computational example of this phenomenon is presented in
</p>
<p>Sect. 13.5. The reader may find it interesting to read that section right after this one.
</p>
<p>Theorem (Partial Conjugate Gradient Method). Suppose the symmetric positive definite
</p>
<p>matrix Q has n&minus;m eigenvalues in the interval [a, b], a &gt; 0 and the remaining m eigenvalues
are greater than b. Then the method of partial conjugate gradients, restarted every m + 1
steps, satisfies
</p>
<p>E(xk+1) �
</p>
<p>(
</p>
<p>b &minus; a
b + a
</p>
<p>)2
</p>
<p>E(xk). (9.31)
</p>
<p>(The point xk+1 is found from xk by taking m + 1 conjugate gradient steps so that each
increment in k is a composite of several simple steps.)
</p>
<p>Proof. Application of (9.27) yields
</p>
<p>E(xk+1 � max
λi
</p>
<p>[1 + λiP(λi)]
2E(xk) (9.32)
</p>
<p>for any mth-order polynomial P, where the λi&rsquo;s are the eigenvalues of Q. Let us
</p>
<p>select P so that the (m + 1)th-degree polynomial q(λ) = 1 + λP(λ) vanishes at
</p>
<p>(a + b)/2 and at the m large eigenvalues of Q. This is illustrated in Fig. 9.4. For this
</p>
<p>choice of P we may write (9.32) as
</p>
<p>E(xk+1) � max
a�λi�b
</p>
<p>[1 + λiP(λi)]
2E(xk).
</p>
<p>Since the polynomial q(λ) = 1 + λP(λ) has m + 1 real roots, q&prime;(λ) will have m real
roots which alternate between the roots of q(λ) on the real axis. Likewise, q&prime;&prime;(λ)
will have m &minus; 1 real roots which alternate between the roots of q&prime;(λ). Thus, since
q(λ) has no root in the interval (&minus;&infin;, (a + b)/2), we see that q&prime;&prime;(λ) does not change
sign in that interval; and since it is easily verified that q&prime;&prime;(0) &gt; 0 it follows that q(λ)
is convex for λ &lt; (a + b)/2. Therefore, on [0, (a + b)/2], q(λ) lies below the line
</p>
<p>1 &minus; [2λ/(a + b)]. Thus we conclude that
</p>
<p>q(λ) � 1 &minus; 2λ
a + b
</p>
<p>on [0, (a + b)/2] and that
</p>
<p>q&prime;
(
</p>
<p>a + b
</p>
<p>2
</p>
<p>)
</p>
<p>� &minus; 2
a + b
</p>
<p>.
</p>
<p>We can see that on [(a + b)/2, b]
</p>
<p>q(λ) � 1 &minus; 2λ
a + b
</p>
<p>,
</p>
<p>since for q(λ) to cross first the line 1&minus; [2λ/(a+b)] and then the λ-axis would require
at least two changes in sign of q&prime;&prime;(λ), whereas, at most one root of q&prime;&prime;(λ) exists to
the left of the second root of q(λ). We see then that the inequality</p>
<p/>
</div>
<div class="page"><p/>
<p>276 9 Conjugate Direction Methods
</p>
<p>|1 + λP(λ)| � |1 &minus; 2λ
a + b
</p>
<p>|
</p>
<p>is valid on the interval [a, b]. The final result (9.31) follows immediately. �
</p>
<p>Fig. 9.4 Construction for proof
</p>
<p>In view of this theorem, the method of partial conjugate gradients can be regarded
</p>
<p>as a generalization of steepest descent, not only in its philosophy and implementa-
</p>
<p>tion, but also in its behavior. Its rate of convergence is bounded by exactly the same
</p>
<p>formula as that of steepest descent but with the largest eigenvalues removed from
</p>
<p>consideration. (It is worth noting that for m = 0 the above proof provides a simple
</p>
<p>derivation of the Steepest Descent Theorem.)
</p>
<p>9.6 Extension to Nonquadratic Problems
</p>
<p>The general unconstrained minimization problem on En
</p>
<p>minimize f (x)
</p>
<p>can be attacked by making suitable approximations to the conjugate gradient al-
</p>
<p>gorithm. There are a number of ways that this might be accomplished; the choice
</p>
<p>depends partially on what properties of f are easily computable. We look at three
</p>
<p>methods in this section and another in the following section.
</p>
<p>Quadratic Approximation
</p>
<p>In the quadratic approximation method we make the following associations at xk:
</p>
<p>gk &harr; &nabla; f (xk)T , Q &harr; F(xk),</p>
<p/>
</div>
<div class="page"><p/>
<p>9.6 Extension to Nonquadratic Problems 277
</p>
<p>and using these associations, reevaluated at each step, all quantities necessary to
</p>
<p>implement the basic conjugate gradient algorithm can be evaluated. If f is quadratic,
</p>
<p>these associations are identities, so that the general algorithm obtained by using
</p>
<p>them is a generalization of the conjugate gradient scheme. This is similar to the
</p>
<p>philosophy underlying Newton&rsquo;s method where at each step the solution of a general
</p>
<p>problem is approximated by the solution of a purely quadratic problem through these
</p>
<p>same associations.
</p>
<p>When applied to nonquadratic problems, conjugate gradient methods will not
</p>
<p>usually terminate within n steps. It is possible therefore simply to continue finding
</p>
<p>new directions according to the algorithm and terminate only when some termina-
</p>
<p>tion criterion is met. Alternatively, the conjugate gradient process can be interrupted
</p>
<p>after n or n + 1 steps and restarted with a pure gradient step. Since Q-conjugacy of
</p>
<p>the direction vectors in the pure conjugate gradient algorithm is dependent on the
</p>
<p>initial direction being the negative gradient, the restarting procedure seems to be
</p>
<p>preferred. We always include this restarting procedure. The general conjugate gra-
</p>
<p>dient algorithm is then defined as below.
</p>
<p>Step 1. Starting at x0 compute g0 = &nabla; f (x0)
T and set d0 = &minus;g0.
</p>
<p>Step 2. For k = 0, 1, . . . , n &minus; 1:
</p>
<p>(a) Set xk+1 = xk + αkdk where αk =
&minus;gT
</p>
<p>k
dk
</p>
<p>dT
k
</p>
<p>F(xk)dk
.
</p>
<p>(b) Compute gk+1 = &nabla; f (xk+1)
T .
</p>
<p>(c) Unless k = n &minus; 1, set dk+1 = &minus;gk+1 + βkdk where
</p>
<p>βk =
gT
k+1
</p>
<p>F(xk)dk
</p>
<p>dT
k
</p>
<p>F(xk)dk
</p>
<p>and repeat (a).
</p>
<p>Step 3. Replace x0 by xn and go back to Step 1.
</p>
<p>An attractive feature of the algorithm is that, just as in the pure form of Newton&rsquo;s
</p>
<p>method, no line searching is required at any stage. Also, the algorithm converges
</p>
<p>in a finite number of steps for a quadratic problem. The undesirable features are
</p>
<p>that F(xk) must be evaluated at each point, which is often impractical, and that the
</p>
<p>algorithm is not, in this form, globally convergent.
</p>
<p>Line Search Methods
</p>
<p>It is possible to avoid the direct use of the association Q &harr; F(xk). First, instead
of using the formula for αk in Step 2(a) above, αk is found by a line search that
</p>
<p>minimizes the objective. This agrees with the formula in the quadratic case. Second,
</p>
<p>the formula for βk in Step 2(c) is replaced by a different formula, which is, however,
</p>
<p>equivalent to the one in 2(c) in the quadratic case.</p>
<p/>
</div>
<div class="page"><p/>
<p>278 9 Conjugate Direction Methods
</p>
<p>The first such method proposed was the Fletcher&ndash;Reeves method, in which Part
</p>
<p>(e) of the Conjugate Gradient Theorem is employed; that is,
</p>
<p>βk =
gT
k+1
</p>
<p>gk+1
</p>
<p>gT
k
</p>
<p>gk
.
</p>
<p>The complete algorithm (using restarts) is:
</p>
<p>Step 1. Given x0 compute g0 = &nabla; f (x0)
T and set d0 = &minus;g0.
</p>
<p>Step 2. For k = 0, 1, . . . , n &minus; 1:
(a) Set xk+1 = xk + αkdk where αk minimizes f (xk + αdk).
</p>
<p>(b) Compute gk+1 = &nabla; f (xk+1)
T .
</p>
<p>(c) Unless k = n &minus; 1, set dk+1 = &minus;gk+1 + βkdk where
</p>
<p>βk =
gT
k+1
</p>
<p>gk+1
</p>
<p>gT
k
</p>
<p>gk
.
</p>
<p>Step 3. Replace x0 by xn and go back to Step 1.
</p>
<p>Another important method of this type is the Polak&ndash;Ribiere method, where
</p>
<p>βk =
(gk+1 &minus; gk)Tgk+1
</p>
<p>gT
k
</p>
<p>gk
</p>
<p>is used to determine βk. Again this leads to a value identical to the standard for-
</p>
<p>mula in the quadratic case. Experimental evidence seems to favor the Polak&ndash;Ribiere
</p>
<p>method over other methods of this general type.
</p>
<p>Convergence
</p>
<p>Global convergence of the line search methods is established by noting that a pure
</p>
<p>steepest descent step is taken every n steps and serves as a spacer step. Since the
</p>
<p>other steps do not increase the objective, and in fact hopefully they decrease it,
</p>
<p>global convergence is assured. Thus the restarting aspect of the algorithm is impor-
</p>
<p>tant for global convergence analysis, since in general one cannot guarantee that the
</p>
<p>directions dk generated by the method are descent directions.
</p>
<p>The local convergence properties of both of the above, and most other, non-
</p>
<p>quadratic extensions of the conjugate gradient method can be inferred from the
</p>
<p>quadratic analysis. Assuming that at the solution, x&lowast;, the matrix F(x&lowast;) is positive
definite, we expect the asymptotic convergence rate per step to be at least as good
</p>
<p>as steepest descent, since this is true in the quadratic case. In addition to this bound
</p>
<p>on the single step rate we expect that the method is of order two with respect to
</p>
<p>each complete cycle of n steps. In other words, since one complete cycle solves a
</p>
<p>quadratic problem exactly just as Newton&rsquo;s method does in one step, we expect that</p>
<p/>
</div>
<div class="page"><p/>
<p>9.7 &lowast;Parallel Tangents 279
</p>
<p>for general nonquadratic problems there will hold |xk+n &minus; x&lowast;| � c|xk &minus; x&lowast;|2 for some
c and k = 0, n, 2n, 3n, . . .. This can indeed be proved, and of course underlies the
</p>
<p>original motivation for the method. For problems with large n, however, a result of
</p>
<p>this type is in itself of little comfort, since we probably hope to terminate in fewer
</p>
<p>than n steps. Further discussion on this general topic is contained in Sect. 10.4.
</p>
<p>Scaling and Partial Methods
</p>
<p>Convergence of the partial conjugate gradient method, restarted every m + 1 steps,
</p>
<p>will in general be linear. The rate will be determined by the eigenvalue structure
</p>
<p>of the Hessian matrix F(x&lowast;), and it may be possible to obtain fast convergence
by changing the eigenvalue structure through scaling procedures. If, for example,
</p>
<p>the eigenvalues can be arranged to occur in m + 1 bunches, the rate of the partial
</p>
<p>method will be relatively fast. Other structures can be analyzed by use of Theorem 2,
</p>
<p>Sect. 9.4, by using F(x&lowast;) rather than Q.
</p>
<p>*9.7 &lowast;Parallel Tangents
</p>
<p>In early experiments with the method of steepest descent the path of descent was
</p>
<p>noticed to be highly zig-zag in character, making slow indirect progress toward the
</p>
<p>solution. (This phenomenon is now quite well understood and is predicted by the
</p>
<p>convergence analysis of Sect. 8.2.) It was also noticed that in two dimensions the so-
</p>
<p>lution point often lies close to the line that connects the zig-zag points, as illustrated
</p>
<p>in Fig. 9.5. This observation motivated the accelerated gradient method in which
</p>
<p>a complete cycle consists of taking two steepest descent steps and then searching
</p>
<p>along the line connecting the initial point and the point obtained after the two gra-
</p>
<p>dient steps. The method of parallel tangents (PARTAN) was developed through an
</p>
<p>Fig. 9.5 Path of gradient method</p>
<p/>
</div>
<div class="page"><p/>
<p>280 9 Conjugate Direction Methods
</p>
<p>attempt to extend this idea to an acceleration scheme involving all previous steps.
</p>
<p>The original development was based largely on a special geometric property of the
</p>
<p>tangents to the contours of a quadratic function, but the method is now recognized
</p>
<p>as a particular implementation of the method of conjugate gradients, and this is the
</p>
<p>context in which it is treated here.
</p>
<p>The algorithm is defined by reference to Fig. 9.6. Starting at an arbitrary point x0
the point x1 is found by a standard steepest descent step. After that, from a point xk
the corresponding yk is first found by a standard steepest descent step from xk, and
</p>
<p>then xk+1 is taken to be the minimum point on the line connecting xk&minus;1 and yk. The
process is continued for n steps and then restarted with a standard steepest descent
</p>
<p>step.
</p>
<p>Notice that except for the first step, xk+1 is determined from xk, not by searching
</p>
<p>along a single line, but by searching along two lines. The direction dk connecting
</p>
<p>two successive points (indicated as dotted lines in the figure) is thus determined
</p>
<p>only indirectly. We shall see, however, that, in the case where the objective function
</p>
<p>is quadratic, the dk&rsquo;s are the same directions, and the xk&rsquo;s are the same points, as
</p>
<p>would be generated by the method of conjugate gradients.
</p>
<p>PARTAN Theorem. For a quadratic function, PARTAN is equivalent to the method of con-
</p>
<p>jugate gradients.
</p>
<p>Fig. 9.6 PARTAN
</p>
<p>Fig. 9.7 One step of PARTAN
</p>
<p>Proof. The proof is by induction. It is certainly true of the first step, since it is a
</p>
<p>steepest descent step. Suppose that x0, x1, . . . , xk have been generated by the con-
</p>
<p>jugate gradient method and xk+1 is determined according to PARTAN. This single</p>
<p/>
</div>
<div class="page"><p/>
<p>9.8 Exercises 281
</p>
<p>step is shown in Fig. 9.7. We want to show that xk+1 is the same point as would
</p>
<p>be generated by another step of the conjugate gradient method. For this to be true
</p>
<p>xk+1 must be that point which minimizes f over the plane defined by dk&minus;1 and
gk = &nabla; f (xk)
</p>
<p>T . From the theory of conjugate gradients, this point will also minimize
</p>
<p>f over the subspace determined by gk and all previous di&rsquo; s. Equivalently, we must
</p>
<p>find the point x where &nabla; f (x) is orthogonal to both gk and dk&minus;1. Since yk minimizes
f along gk, we see that &nabla; f (yk) is orthogonal to gk. Since &nabla; f (xk&minus;1) is contained in
the subspace [d0, d1, . . . , dk&minus;1] and because gk is orthogonal to this subspace by
the Expanding Subspace Theorem, we see that &nabla; f (xk&minus;1) is also orthogonal to gk.
Since &nabla; f (x) is linear in x, it follows that at every point x on the line through xk&minus;1
and yk we have &nabla; f (x) orthogonal to gk. By minimizing f along this line, a point
</p>
<p>xk+1 is obtained where in addition &nabla; f (xk+1) is orthogonal to the line. Thus &nabla; f (xk+1)
</p>
<p>is orthogonal to both gk and the line joining xk&minus;1 and yk. It follows that &nabla; f (xk+1) is
</p>
<p>orthogonal to the plane. �
</p>
<p>There are advantages and disadvantages of PARTAN relative to other methods
</p>
<p>when applied to nonquadratic problems. One attractive feature of the algorithm is
</p>
<p>its simplicity and ease of implementation. Probably its most desirable property, how-
</p>
<p>ever, is its strong global convergence characteristics. Each step of the process is at
</p>
<p>least as good as steepest descent; since going from xk to yk is exactly steepest de-
</p>
<p>scent, and the additional move to xk+1 provides further decrease of the objective
</p>
<p>function. Thus global convergence is not tied to the fact that the process is restarted
</p>
<p>every n steps. It is suggested, however, that PARTAN should be restarted every n
</p>
<p>steps (or n + 1 steps) so that it will behave like the conjugate gradient method near
</p>
<p>the solution.
</p>
<p>An undesirable feature of the algorithm is that two line searches are required at
</p>
<p>each step, except the first, rather than one as is required by, say, the Fletcher&ndash;Reeves
</p>
<p>method. This is at least partially compensated by the fact that searches need not
</p>
<p>be as accurate for PARTAN, for while inaccurate searches in the Fletcher&ndash;Reeves
</p>
<p>method may yield nonsensical successive search directions, PARTAN will at least
</p>
<p>do as well as steepest descent.
</p>
<p>9.8 Exercises
</p>
<p>1. Let Q be a positive definite symmetric matrix and suppose p0, p1, . . . , pn&minus;1
are linearly independent vectors in En. Show that a Gram&ndash;Schmidt procedure
</p>
<p>can be used to generate a sequence of Q-conjugate directions from the pi&rsquo;s.
</p>
<p>Specifically, show that d0, d1, . . . , dn&minus;1 defined recursively by
</p>
<p>d0 = p0
</p>
<p>dk+1 = pk+1 &minus;
k
</p>
<p>&sum;
</p>
<p>i=0
</p>
<p>pT
k+1Qdi
</p>
<p>dT
i
</p>
<p>Qdi
di
</p>
<p>form&rsquo;s a Q-conjugate set.</p>
<p/>
</div>
<div class="page"><p/>
<p>282 9 Conjugate Direction Methods
</p>
<p>2. Suppose the pi&rsquo;s in Exercise 1 are generated as moments of Q, that is, suppose
</p>
<p>pk = Q
kp0, k = 1, 2, . . . , n &minus; 1. Show that the corresponding dk&rsquo;s can then
</p>
<p>be generated by a (three-term) recursion formula where dk+1 is defined only in
</p>
<p>terms of Qdk, dk and dk&minus;1.
3. Suppose the pk&rsquo;s in Exercise 1 are taken as pk = ek where ek is the kth unit
</p>
<p>coordinate vector and the dk&rsquo;s are constructed accordingly. Show that using dk&rsquo;s
</p>
<p>in a conjugate direction method to minimize (1/2)xTQx &minus; bTx is equivalent to
the application of Gaussian elimination to solve Qx = b.
</p>
<p>4. Let f (x) = (1/2)xTQx &minus; bTx be defined on En with Q positive definite. Let
x1 be a minimum point of f over a subspace of E
</p>
<p>n containing the vector d
</p>
<p>and let x2 be the minimum of f over another subspace containing d. Suppose
</p>
<p>f (x1) &lt; f (x2). Show that x1 &minus; x2 is Q-conjugate to d.
5. Let Q be a symmetric matrix. Show that any two eigenvectors of Q, correspond-
</p>
<p>ing to distinct eigenvalues, are Q-conjugate.
</p>
<p>6. Let Q be an n &times; n symmetric matrix and let d0, d1, . . . , dn&minus;1 be Q-conjugate.
Show how to find an E such that ETQE is diagonal.
</p>
<p>7. Show that in the conjugate gradient method Qdk&minus;1 &isin; Bk+1.
8. Derive the rate of convergence of the method of steepest descent by viewing it
</p>
<p>as a one-step optimal process.
</p>
<p>9. Let Pk(Q) = c0 + c1Q + c2Q
2 + &middot; &middot; &middot;+ cmQm be the optimal polynomial in (9.29)
</p>
<p>minimizing (9.30). Show that the ci&rsquo;s can be found explicitly by solving the
</p>
<p>vector equation
</p>
<p>&minus;
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>gT
k
</p>
<p>Qgk g
T
k
</p>
<p>Q2gk &middot; &middot; &middot; gTk Qm+1gk
gT
k
</p>
<p>Q2gk g
T
k
</p>
<p>Q3gk &middot; &middot; &middot; gTk Qm+2gk
...
</p>
<p>gT
k
</p>
<p>Qm+1gk &middot; &middot; &middot; gTk Q2m+1gk
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>c0
c1
...
</p>
<p>cm
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>=
</p>
<p>gT
k
</p>
<p>gk
gT
k
</p>
<p>Qgk
...
</p>
<p>gT
k
</p>
<p>Qmgk
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>Show that this reduces to steepest descent when m = 0.
</p>
<p>10. Show that for the method of conjugate directions there holds
</p>
<p>E(xk) � 4
</p>
<p>(
</p>
<p>1 &minus; &radic;γ
1 +
</p>
<p>&radic;
γ
</p>
<p>)2k
</p>
<p>E(x0),
</p>
<p>where γ = a/A and a and A are the smallest and largest eigenvalues of Q. Hint:
</p>
<p>In (9.27) select Pk&minus;1(λ) so that
</p>
<p>1 + λPk&minus;1(λ) =
Tk
</p>
<p>(
</p>
<p>A+a&minus;2λ
A&minus;a
</p>
<p>)
</p>
<p>Tk
(
</p>
<p>A+a
A&minus;a
</p>
<p>) ,</p>
<p/>
</div>
<div class="page"><p/>
<p>References 283
</p>
<p>where Tk(λ) = cos(k arc cos λ) is the kth Chebyshev polynomial. This choice
</p>
<p>gives the minimum maximum magnitude on [a, A]. Verify and use the inequality
</p>
<p>(1 &minus; γ)k
(1 +
</p>
<p>&radic;
γ)2k + (1 &minus; &radic;γ)2k �
</p>
<p>(
</p>
<p>1 &minus; &radic;γ
1 +
</p>
<p>&radic;
γ
</p>
<p>)k
</p>
<p>.
</p>
<p>11. Suppose it is known that each eigenvalue of Q lies either in the interval [a, A]
</p>
<p>or in the interval [a+∆, A+∆] where a, A, and ∆ are all positive. Show that the
</p>
<p>partial conjugate gradient method restarted every two steps will converge with
</p>
<p>a ratio no greater than [(A &minus; a)/(A + a)]2 no matter how large ∆ is.
12. Modify the first method given in Sect. 9.6 so that it is globally convergent.
</p>
<p>13. Show that in the purely quadratic form of the conjugate gradient method
</p>
<p>dT
k
</p>
<p>Qdk = &minus;dTk Qgk. Using this show that to obtain xk+1 from xk it is necessary
to use Q only to evaluate gk and Qgk.
</p>
<p>14. Show that in the quadratic problem Qgk can be evaluated by taking a unit step
</p>
<p>from xk in the direction of the negative gradient and evaluating the gradient
</p>
<p>there. Specifically, if yk = xk &minus; gk and pk = &nabla; f (yk)T , then Qgk = gk &minus; pk.
15. Combine the results of Exercises 13 and 14 to derive a conjugate gradient
</p>
<p>method for general problems much in the spirit of the first method of Sect. 9.6
</p>
<p>but which does not require knowledge of F(xk) or a line search.
</p>
<p>References
</p>
<p>9.1&ndash;9.3 For the original development of conjugate direction methods, see Hestenes
</p>
<p>and Stiefel [H10] and Hestenes [H7], [H9]. For another introductory treat-
</p>
<p>ment see Beckman [B8]. The method was extended to the case where Q is
</p>
<p>not positive definite, which arises in constrained problems, by Luenberger
</p>
<p>[L9], [L11].
</p>
<p>9.4 The idea of viewing the conjugate gradient method as an optimal process
</p>
<p>was originated by Stiefel [S10]. Also see Daniel [D1] and Faddeev and
</p>
<p>Faddeeva [F1].
</p>
<p>9.5 The partial conjugate gradient method presented here is identical to the
</p>
<p>so-called s-step gradient method. See Faddeev and Faddeeva [F1] and
</p>
<p>Forsythe [F14]. The bound on the rate of convergence given in this sec-
</p>
<p>tion in terms of the interval containing the n &minus; m smallest eigenvalues was
first given in Luenberger [L13]. Although this bound cannot be expected to
</p>
<p>be tight, it is a reasonable conjecture that it becomes tight as the m largest
</p>
<p>eigenvalues tend to infinity with arbitrarily large separation.
</p>
<p>9.6 For the first approximate method, see Daniel [D1]. For the line search meth-
</p>
<p>ods, see Fletcher and Reeves [F12], Polak and Ribiere [P5], and Polak [P4].
</p>
<p>For proof of the n-step, order two convergence, see Cohen [C4]. For a sur-
</p>
<p>vey of computational experience of these methods, see Fletcher [F9].</p>
<p/>
</div>
<div class="page"><p/>
<p>284 9 Conjugate Direction Methods
</p>
<p>9.7 PARTAN is due to Shah, Buehler, and Kempthome [S2]. Also see
</p>
<p>Wolfe [W5].
</p>
<p>9.8 The approach indicated in Exercises 1 and 2 can be used as a foundation
</p>
<p>for the development of conjugate gradients; see Antosiewicz and Rhein-
</p>
<p>boldt [A7], Vorobyev [V6], Faddeev and Faddeeva [F1], and Luenberger
</p>
<p>[L8]. The result stated in Exercise 3 is due to Hestenes and Stiefel [H10].
</p>
<p>Exercise 4 is due to Powell [P6]. For the solution to Exercise 10, see Fad-
</p>
<p>deev and Faddeeva [F1] or Daniel [D1].</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 10
</p>
<p>Quasi-Newton Methods
</p>
<p>In this chapter we take another approach toward the development of methods lying
</p>
<p>somewhere intermediate to steepest descent and Newton&rsquo;s method. Again working
</p>
<p>under the assumption that evaluation and use of the Hessian matrix is impractical or
</p>
<p>costly, the idea underlying quasi-Newton methods is to use an approximation to the
</p>
<p>inverse Hessian in place of the true inverse that is required in Newton&rsquo;s method. The
</p>
<p>form of the approximation varies among different methods&mdash;ranging from the sim-
</p>
<p>plest where it remains fixed throughout the iterative process, to the more advanced
</p>
<p>where improved approximations are built up on the basis of information gathered
</p>
<p>during the descent process.
</p>
<p>The quasi-Newton methods that build up an approximation to the inverse Hessian
</p>
<p>are analytically the most sophisticated methods discussed in this book for solving
</p>
<p>unconstrained problems and represent the culmination of the development of algo-
</p>
<p>rithms through detailed analysis of the quadratic problem. As might be expected,
</p>
<p>the convergence properties of these methods are somewhat more difficult to dis-
</p>
<p>cover than those of simpler methods. Nevertheless, we are able, by continuing with
</p>
<p>the same basic techniques as before, to illuminate their most important features.
</p>
<p>In the course of our analysis we develop two important generalizations of the
</p>
<p>method of steepest descent and its corresponding convergence rate theorem. The
</p>
<p>first, discussed in Sect. 10.1, modifies steepest descent by taking as the direction
</p>
<p>vector a positive definite transformation of the negative gradient. The second, dis-
</p>
<p>cussed in Sect. 10.8, is a combination of steepest descent and Newton&rsquo;s method.
</p>
<p>Both of these fundamental methods have convergence properties analogous to those
</p>
<p>of steepest descent.
</p>
<p>&copy; Springer International Publishing Switzerland 2016
</p>
<p>D.G. Luenberger, Y. Ye, Linear and Nonlinear Programming, International
Series in Operations Research &amp; Management Science 228,
DOI 10.1007/978-3-319-18842-3 10
</p>
<p>285</p>
<p/>
</div>
<div class="page"><p/>
<p>286 10 Quasi-Newton Methods
</p>
<p>10.1 Modified Newton Method
</p>
<p>A very basic iterative process for solving the problem
</p>
<p>minimize f (x)
</p>
<p>which includes as special cases most of our earlier ones is
</p>
<p>xk+1 = xk &minus; αkSk&nabla; f (xk)T (10.1)
</p>
<p>where Sk is a symmetric n &times; n matrix and where, as usual, αk is chosen to minimize
f (xk+1). If Sk is the inverse of the Hessian of f , we obtain Newton&rsquo;s method, while
</p>
<p>if Sk = I we have steepest descent. It would seem to be a good idea, in general,
</p>
<p>to select Sk as an approximation to the inverse of the Hessian. We examine that
</p>
<p>philosophy in this section.
</p>
<p>First, we note, as in Sect. 8.5, that in order that the process (10.1) be guaranteed
</p>
<p>to be a descent method for small values of α, it is necessary in general to require
</p>
<p>that Sk be positive definite. We shall therefore always impose this as a requirement.
</p>
<p>Because of the similarity of the algorithm (10.1) with steepest descent&dagger; it should
not be surprising that its convergence properties are similar in character to our ear-
</p>
<p>lier results. We derive the actual rate of convergence by considering, as usual, the
</p>
<p>standard quadratic problem with
</p>
<p>f (x) =
1
</p>
<p>2
xTQx &minus; bTx, (10.2)
</p>
<p>where Q is symmetric and positive definite. For this case we can find an explicit
</p>
<p>expression for αk in (10.1). The algorithm becomes
</p>
<p>xk+1 = xk &minus; αkSkgk, (10.3a)
</p>
<p>where
</p>
<p>gk = Qxk &minus; b (10.3b)
</p>
<p>αk =
gT
k
</p>
<p>Skgk
</p>
<p>gT
k
</p>
<p>SkQSkgk
. (10.3c)
</p>
<p>We may then derive the convergence rate of this algorithm by slightly extending the
</p>
<p>analysis carried out for the method of steepest descent.
</p>
<p>Modified Newton Method Theorem (Quadratic Case). Let x&lowast; be the uniqueminimum point
of f , and define E(x) = 1
</p>
<p>2
(x &minus; x&lowast;)TQ(x &minus; x&lowast;).
</p>
<p>&dagger; The algorithm (10.1) is sometimes referred to as the method of deflected gradients, since the
direction vector can be thought of as being determined by deflecting the gradient through multipli-
cation by Sk.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.1 Modified Newton Method 287
</p>
<p>Then for the algorithm (10.3) there holds at every step k
</p>
<p>E(xk+1) �
</p>
<p>(
</p>
<p>Bk &minus; bk
Bk + bk
</p>
<p>)2
</p>
<p>E(xk), (10.4)
</p>
<p>where bk and Bk are, respectively, the smallest and largest eigenvalues of thematrix SkQ.
</p>
<p>Proof. We have by direct substitution
</p>
<p>E(xk) &minus; E(xk+1)
E(xk)
</p>
<p>=
(gT
</p>
<p>k
Skgk)
</p>
<p>2
</p>
<p>(gT
k
</p>
<p>SkQSkgk)(g
T
k
</p>
<p>Q&minus;lgk)
.
</p>
<p>Letting Tk = S
1/2
k
</p>
<p>QS
1/2
k
</p>
<p>and pk = S
1/2
k
</p>
<p>gk we obtain
</p>
<p>E(xk) &minus; E(xk+1)
E(xk)
</p>
<p>=
(pT
</p>
<p>k
Pk)
</p>
<p>2
</p>
<p>(pT
k
</p>
<p>Tkpk)(p
T
k
</p>
<p>T&minus;1k pk)
.
</p>
<p>From the Kantorovich inequality we obtain easily
</p>
<p>E(xk+1) �
</p>
<p>(
</p>
<p>Bk &minus; bk
Bk + bk
</p>
<p>)2
</p>
<p>E(xk),
</p>
<p>where bk and Bk are the smallest and largest eigenvalues of Tk. Since S
1/2
k
</p>
<p>TkS
&minus;1/2
k
</p>
<p>=
</p>
<p>SkQ, we see that SkQ is similar to Tk and therefore has the same eigenvalues. �
</p>
<p>This theorem supports the intuitive notion that for the quadratic problem one
</p>
<p>should strive to make Sk close to Q
&minus;1 since then both bk and Bk would be close
</p>
<p>to unity and convergence would be rapid. For a nonquadratic objective function f
</p>
<p>the analog to Q is the Hessian F(x), and hence one should try to make Sk close
</p>
<p>to F(xk)
&minus;1.
</p>
<p>Two remarks may help to put the above result in proper perspective. The first re-
</p>
<p>mark is that both the algorithm (10.1) and the theorem stated above are only simple,
</p>
<p>minor, and natural extensions of the work presented in Chap. 8 on steepest descent.
</p>
<p>As such the result of this section can be regarded, correspondingly, not as a new idea
</p>
<p>but as an extension of the basic result on steepest descent. The second remark is that
</p>
<p>this one simple result when properly applied can quickly characterize the conver-
</p>
<p>gence properties of some fairly complex algorithms. Thus, rather than an isolated
</p>
<p>result concerned with a specific form of algorithm, the theorem above should be
</p>
<p>regarded as a general tool for convergence analysis. It provides significant insight
</p>
<p>into various quasi-Newton methods discussed in this chapter.
</p>
<p>A Classical Method
</p>
<p>We conclude this section by mentioning the classical modified Newton&rsquo;s method,
</p>
<p>a standard method for approximating Newton&rsquo;s method without evaluating F(xk)
&minus;1
</p>
<p>for each k. We set</p>
<p/>
</div>
<div class="page"><p/>
<p>288 10 Quasi-Newton Methods
</p>
<p>xk+1 = xk &minus; αk[F(x0)]&minus;1&nabla; f (xk)T . (10.5)
In this method the Hessian at the initial point x0 is used throughout the process.
</p>
<p>The effectiveness of this procedure is governed largely by how fast the Hessian is
</p>
<p>changing&mdash;in other words, by the magnitude of the third derivatives of f .
</p>
<p>10.2 Construction of the Inverse
</p>
<p>The fundamental idea behind most quasi-Newton methods is to try to construct the
</p>
<p>inverse Hessian, or an approximation of it, using information gathered as the descent
</p>
<p>process progresses. The current approximation Hk is then used at each stage to de-
</p>
<p>fine the next descent direction by setting Sk = Hk in the modified Newton method.
</p>
<p>Ideally, the approximations converge to the inverse of the Hessian at the solution
</p>
<p>point and the overall method behaves somewhat like Newton&rsquo;s method. In this sec-
</p>
<p>tion we show how the inverse Hessian can be built up from gradient information
</p>
<p>obtained at various points.
</p>
<p>Let f be a function on En that has continuous second partial derivatives. If for
</p>
<p>two points xk+1, xk we define gk+1 = &nabla; f (xk+1)
T , gk = &nabla; f (xk)
</p>
<p>T and pk = xk+1 &minus; xk,
then
</p>
<p>gk+1 &minus; gk � F(xk)pk. (10.6)
If the Hessian, F, is constant, then we have
</p>
<p>qk &equiv; gk+1 &minus; gk = Fpk, (10.7)
</p>
<p>and we see that evaluation of the gradient at two points gives information about F. If
</p>
<p>n linearly independent directions p0, p1, p2, . . . , pn&minus;1 and the corresponding qk&rsquo;s are
known, then F is uniquely determined. Indeed, letting P and Q be the n&times; n matrices
with columns pk and qk respectively, we have F = QP
</p>
<p>&minus;1.
It is natural to attempt to construct successive approximations Hk to F
</p>
<p>&minus;1 based
on data obtained from the first k steps of a descent process in such a way that if
</p>
<p>F were constant the approximation would be consistent with (10.7) for these steps.
</p>
<p>Specifically, if F were constant Hk+1 would satisfy
</p>
<p>Hk+1qi = pi, 0 � i � k. (10.8)
</p>
<p>After n linearly independent steps we would then have Hn = F
&minus;1.
</p>
<p>For any k &lt; n the problem of constructing a suitable Hk, with in general serves as
</p>
<p>an approximation to the inverse Hessian and which in the case of constant F satis-
</p>
<p>fies (10.8), admits an infinity of solutions, since there are more degrees of freedom
</p>
<p>than there are constraints. Thus a particular method can take into account addi-
</p>
<p>tional considerations. We discuss below one of the simplest schemes that has been
</p>
<p>proposed.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.2 Construction of the Inverse 289
</p>
<p>Rank One Correction
</p>
<p>Since F and F&minus;1 are symmetric, it is natural to require that Hk, the approximation to
F&minus;1, be symmetric. We investigate the possibility of defining a recursion of the form
</p>
<p>Hk+1 = Hk + akzkz
T
k , (10.9)
</p>
<p>which preserves symmetry. The vector zk and the constant ak define a matrix of (at
</p>
<p>most) rank one, by which the approximation to the inverse is updated. We select
</p>
<p>them so that (10.8) is satisfied. Setting i equal to k in (10.8) and substituting (10.9)
</p>
<p>we obtain
</p>
<p>pk = Hk+1qk = Hkqk + akzkz
T
k qk. (10.10)
</p>
<p>Taking the inner product with qk we have
</p>
<p>qTk pk &minus; qTk Hkqk = ak
(
</p>
<p>zTk qk
</p>
<p>)2
(10.11)
</p>
<p>On the other hand, using (10.10) we may write (10.9) as
</p>
<p>Hk+1 = Hk +
(pk &minus; Hkqk)(pk &minus; Hkqk)T
</p>
<p>ak
(
</p>
<p>zT
k
</p>
<p>qk
</p>
<p>)2
,
</p>
<p>which in view of (10.11) leads finally to
</p>
<p>Hk+1 = Hk +
(pk &minus; Hkqk)(pk &minus; Hkqk)T
</p>
<p>qT
k
</p>
<p>(pk &minus; Hkqk)
. (10.12)
</p>
<p>We have determined what a rank one correction must be if it is to satisfy (10.8)
</p>
<p>for i = k. It remains to be shown that, for the case where F is constant, (10.8) is also
</p>
<p>satisfied for i &lt; k. This in turn will imply that the rank one recursion converges to
</p>
<p>F&minus;1 after at most n steps.
</p>
<p>Theorem. Let F be a fixed symmetric matrix and suppose that p0, p1, p2, . . . , pk are given
</p>
<p>vectors. Define the vectors qi = Fpi, i = 0, 1, 2, . . . , k.
</p>
<p>Starting with any initial symmetric matrix H0 let
</p>
<p>Hi+1 = Hi +
(pi &minus;Hiqi)(pi &minus;Hiqi)T
</p>
<p>qT
i
</p>
<p>(pi &minus;Hiqi)
. (10.13)
</p>
<p>Then
</p>
<p>pi = Hk+1qi f or i � k. (10.14)
</p>
<p>Proof. The proof is by induction. Suppose it is true for Hk, and i � k &minus; 1. The
relation was shown above to be true for Hk+1 and i = k. For i &lt; k
</p>
<p>Hk+1qi = Hkqi + yk(p
T
k qi &minus; qTk Hkqi), (10.15)</p>
<p/>
</div>
<div class="page"><p/>
<p>290 10 Quasi-Newton Methods
</p>
<p>where
</p>
<p>yk =
(pk &minus; Hkqk)
</p>
<p>qT
k
</p>
<p>(pk &minus; Hkqk)
.
</p>
<p>By the induction hypothesis, (10.15) becomes
</p>
<p>Hk+1qi = pi + yk
</p>
<p>(
</p>
<p>pTk qi &minus; qTk pi
)
</p>
<p>.
</p>
<p>From the calculation
</p>
<p>qTk pi = p
T
k Fpi = p
</p>
<p>T
k qi,
</p>
<p>it follows that the second term vanishes. �
</p>
<p>To incorporate the approximate inverse Hessian in a descent procedure while
</p>
<p>simultaneously improving it, we calculate the direction dk From
</p>
<p>dk = &minus;Hkgk
</p>
<p>and then minimize f (xk + αdk) with respect to α � 0. This determines xk+1 =
</p>
<p>xk + αkdk, pk = αkdk, and gk+1. Then Hk+1 can be calculated according to (10.12).
</p>
<p>There are some difficulties with this simple rank one procedure. First, the up-
</p>
<p>dating formula (10.12) preserves positive definiteness only if qT
k
</p>
<p>(pk &minus; Hkqk) &gt; 0,
which cannot be guaranteed (see Exercise 6). Also, even if qT
</p>
<p>k
(pk &minus; Hkqk) is pos-
</p>
<p>itive, it may be small, which can lead to numerical difficulties. Thus, although an
</p>
<p>excellent simple example of how information gathered during the descent process
</p>
<p>can in principle be used to update an approximation to the inverse Hessian, the rank
</p>
<p>one method possesses some limitations.
</p>
<p>10.3 Davidon-Fletcher-Powell Method
</p>
<p>The earliest, and certainly one of the most clever schemes for constructing the in-
</p>
<p>verse Hessian, was originally proposed by Davidon and later developed by Fletcher
</p>
<p>and Powell. It has the fascinating and desirable property that, for a quadratic ob-
</p>
<p>jective, it simultaneously generates the directions of the conjugate gradient method
</p>
<p>while constructing the inverse Hessian. At each step the inverse Hessian is updated
</p>
<p>by the sum of two symmetric rank one matrices, and this scheme is therefore often
</p>
<p>referred to as a rank two correction procedure. The method is also often referred to
</p>
<p>as the variable metric method, the name originally suggested by Davidon.
</p>
<p>The procedure is this: Starting with any symmetric positive definite matrix H0,
</p>
<p>any point x0, and with k = 0,
</p>
<p>Step 1. Set dk = &minus;Hkgk.
Step 2. Minimize f (xk + αdk) with respect to α � 0 to obtain xk+1, pk = αkdk,
</p>
<p>and gk+1.
</p>
<p>Step 3. Set qk = gk+1 &minus; gk and</p>
<p/>
</div>
<div class="page"><p/>
<p>10.3 Davidon-Fletcher-Powell Method 291
</p>
<p>Hk+1 = Hk +
pkp
</p>
<p>T
k
</p>
<p>pT
k
</p>
<p>qk
&minus;
</p>
<p>Hkqkq
T
k
</p>
<p>Hk
</p>
<p>qT
k
</p>
<p>Hkqk
. (10.16)
</p>
<p>Update k and return to Step 1.
</p>
<p>Positive Definiteness
</p>
<p>We first demonstrate that if Hk is positive definite, then so is Hk+1. For any x &isin; En
we have
</p>
<p>xTHk+1x = x
THkx +
</p>
<p>(xTpk)
2
</p>
<p>pT
k
</p>
<p>qk
&minus; (x
</p>
<p>THkqk)
2
</p>
<p>qT
k
</p>
<p>Hkqk
. (10.17)
</p>
<p>Defining a = H1/2
k
</p>
<p>x, b = H
1/2
k
</p>
<p>qk we may rewrite (10.17) as
</p>
<p>xTHk+1x =
(aTa)(bTb) &minus; (aTb)2
</p>
<p>(bTb)
+
</p>
<p>(xTpk)
2
</p>
<p>pT
k
</p>
<p>qk
.
</p>
<p>We also have
</p>
<p>pTk qk = p
T
k gk+1 &minus; pTk gk = &minus;pTk gk, (10.18)
</p>
<p>since
</p>
<p>pTk gk+1 = 0, (10.19)
</p>
<p>because xk+1 is the minimum point of f along pk. Thus by definition of pk
</p>
<p>pTk qk = αkg
T
k Hkgk, (10.20)
</p>
<p>and hence
</p>
<p>xTHk+1x =
(aTa)(bTb) &minus; (aTb)2
</p>
<p>(bTb)
+
</p>
<p>(xTpk)
2
</p>
<p>αkg
T
k
</p>
<p>Hkgk
. (10.21)
</p>
<p>Both terms on the right of (10.21) are nonnegative&mdash;the first by the Cauchy&ndash;
</p>
<p>Schwarz inequality. We must only show they do not both vanish simultaneously.
</p>
<p>The first term vanishes only if a and b are proportional. This in turn implies that x
</p>
<p>and qk are proportional, say x = βqk. In that case, however,
</p>
<p>pTk x = βp
T
k qk = βαkg
</p>
<p>T
k Hkgk � 0
</p>
<p>from (10.20). Thus xTHk+1x &gt; 0 for all nonzero x.
</p>
<p>It is of interest to note that in the proof above the fact that αk is chosen as the
</p>
<p>minimum point of the line search was used in (10.19), which led to the important
</p>
<p>conclusion pT
k
</p>
<p>qk &gt; 0. Actually any αk, whether the minimum point or not, that
</p>
<p>gives pT
k
</p>
<p>qk &gt; 0 can be used in the algorithm, and Hk+1 will be positive definite (see
</p>
<p>Exercises 8 and 9).</p>
<p/>
</div>
<div class="page"><p/>
<p>292 10 Quasi-Newton Methods
</p>
<p>Finite Step Convergence
</p>
<p>We assume now that f is quadratic with (constant) Hessian F. We show in this
</p>
<p>case that the Davidon-Fletcher-Powell method produces direction vectors pk that are
</p>
<p>F-orthogonal and that if the method is carried n steps then Hn = F
&minus;1.
</p>
<p>Theorem. If f is quadratic with positive definite Hessian F, then for the Davidon-Fletcher-
</p>
<p>Powell method
</p>
<p>pTi Fp j = 0, 0 � i &lt; j � k (10.22)
</p>
<p>Hk+1Fpi = pi f or 0 � i � k. (10.23)
</p>
<p>Proof. We note that for the quadratic case
</p>
<p>qk = gk+1 &minus; gk = Fxk+1 &minus; Fxk = Fpk. (10.24)
</p>
<p>Also
</p>
<p>Hk+1Fpk = Hk+1qk = pk (10.25)
</p>
<p>from (10.16).
</p>
<p>We now prove (10.22) and (10.23) by induction. From (10.25) we see that they
</p>
<p>are true for k = 0. Assuming they are true for k&minus; 1, we prove they are true for k. We
have
</p>
<p>gk = gi+1 + F(pi+1 + &middot; &middot; &middot; + pk&minus;1).
Therefore from (10.22) and (10.19)
</p>
<p>pTi gk = p
T
i gi+1 = 0 for 0 � i &lt; k. (10.26)
</p>
<p>Hence from (10.23)
</p>
<p>pTi FHkgk = 0. (10.27)
</p>
<p>Thus since pk = &minus;αkHkgk and since αk � 0, we obtain
</p>
<p>pTi Fpk = 0 for i &lt; k, (10.28)
</p>
<p>which proves (10.22) for k.
</p>
<p>Now since from (10.23) for k &minus; 1, (10.24) and (10.28)
</p>
<p>qTk HkFpi = q
T
k pi = p
</p>
<p>T
k Fpi = 0, 0 � i &lt; k
</p>
<p>we have
</p>
<p>Hk+1Fpi = HkFpi = pi, 0 � i &lt; k.
</p>
<p>This together with (10.25) proves (10.23) for k. �
</p>
<p>Since the pk&rsquo;s are F-orthogonal and since we minimize f successively in these
</p>
<p>directions, we see that the method is a conjugate direction method. Furthermore,</p>
<p/>
</div>
<div class="page"><p/>
<p>10.4 The Broyden Family 293
</p>
<p>if the initial approximation H0 is taken equal to the identity matrix, the method
</p>
<p>becomes the conjugate gradient method. In any case the process obtains the overall
</p>
<p>minimum point within n steps.
</p>
<p>Finally, (10.23) shows that p0, p1, p2, . . . , pk are eigenvectors corresponding to
</p>
<p>unity eigenvalue for the matrix Hk+1F. These eigenvectors are linearly independent,
</p>
<p>since they are F-orthogonal, and therefore Hn = F
&minus;1.
</p>
<p>10.4 The Broyden Family
</p>
<p>The updating formulae for the inverse Hessian considered in the previous two sec-
</p>
<p>tions are based on satisfying
</p>
<p>Hk+1qi = pi, 0 � i � k, (10.29)
</p>
<p>which is derived from the relation
</p>
<p>qi = Fpi, 0 � i � k, (10.30)
</p>
<p>which would hold in the purely quadratic case. It is also possible to update ap-
</p>
<p>proximations to the Hessian F itself, rather than its inverse. Thus, denoting the kth
</p>
<p>approximation of F by Bk, we would, analogously, seek to satisfy
</p>
<p>qi = Bk+1pi, 0 � i � k. (10.31)
</p>
<p>Equation (10.31) has exactly the same form as (10.29) except that qi and pi are
</p>
<p>interchanged and H is replaced by B. It should be clear that this implies that any
</p>
<p>update formula for H derived to satisfy (10.29) can be transformed into a corre-
</p>
<p>sponding update formula for B. Specifically, given any update formula for H, the
</p>
<p>complementary formula is found by interchanging the roles of B and H and of q
</p>
<p>and p. Likewise, any updating formula for B that satisfies (10.31) can be converted
</p>
<p>by the same process to a complementary formula for updating H. It is easily seen
</p>
<p>that taking the complement of a complement restores the original formula.
</p>
<p>To illustrate complementary formulae, consider the rank one update of Sect. 10.2,
</p>
<p>which is
</p>
<p>Hk+1 = Hk +
(pk &minus; Hkqk)(pk &minus; Hkqk)T
</p>
<p>qT
k
</p>
<p>(pk &minus; Hkqk)
. (10.32)
</p>
<p>The corresponding complementary formula is
</p>
<p>Bk+1 = Bk +
(qk &minus; Bkpk)(qk &minus; Bkpk)T
</p>
<p>pT
k
</p>
<p>(qk &minus; Bkpk)
. (10.33)</p>
<p/>
</div>
<div class="page"><p/>
<p>294 10 Quasi-Newton Methods
</p>
<p>Likewise, the Davidon-Fletcher-Powell (or simply DFP) formula is
</p>
<p>HDFPk+1 = Hk +
pkp
</p>
<p>T
k
</p>
<p>pT
k
</p>
<p>qk
&minus;
</p>
<p>Hkqkq
T
k
</p>
<p>Hk
</p>
<p>qT
k
</p>
<p>Hkqk
, (10.34)
</p>
<p>and its complement is
</p>
<p>Bk+1 = Bk +
qkq
</p>
<p>T
k
</p>
<p>qT
k
</p>
<p>pk
&minus;
</p>
<p>Bkpkp
T
k
</p>
<p>Bk
</p>
<p>pT
k
</p>
<p>Bkpk
. (10.35)
</p>
<p>This last update is known as the Broyden-Fletcher-Goldfarb-Shanno update of Bk,
</p>
<p>and it plays an important role in what follows.
</p>
<p>Another way to convert an updating formula for H to one for B or vice versa is
</p>
<p>to take the inverse. Clearly, if
</p>
<p>Hk+1qi = pi, 0 � i � k, (10.36)
</p>
<p>then
</p>
<p>qi = H
&minus;1
k+1pi, 0 � i � k, (10.37)
</p>
<p>which implies that H&minus;1
k+1
</p>
<p>satisfies (10.31), the criterion for an update of B. Also, most
</p>
<p>importantly, the inverse of a rank two formula is itself a rank two formula.
</p>
<p>The new formula can be found explicitly by two applications of the general in-
</p>
<p>version identity (often referred to as the Sherman-Morrison formula)
</p>
<p>[
</p>
<p>A + abT
]&minus;1
</p>
<p>= A&minus;1 &minus; A
&minus;1abTA&minus;1
</p>
<p>1 + bTA&minus;1a
, (10.38)
</p>
<p>where A is an n &times; n matrix, and a and b are n-vectors, which is valid provided the
inverses exist. (This is easily verified by multiplying through by A + abT .)
</p>
<p>The Broyden-Fletcher-Goldfard-Shanno update for B produces, by taking the
</p>
<p>inverse, a corresponding update for H of the form
</p>
<p>HBFGSk+1 = Hk +
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>1 + qT
k
</p>
<p>Hkqk
</p>
<p>qT
k
</p>
<p>qk
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>pkp
T
k
</p>
<p>pT
k
</p>
<p>qk
&minus;
</p>
<p>pkq
T
k
</p>
<p>Hk + Hkqkq
T
k
</p>
<p>qT
k
</p>
<p>pk
. (10.39)
</p>
<p>This is an important update formula that can be used exactly like the DFP formula.
</p>
<p>Numerical experiments have repeatedly indicated that its performance is superior to
</p>
<p>that of the DFP formula, and for this reason it is now generally preferred.
</p>
<p>It can be noted that both the DFP and the BFGS updates have symmetric rank
</p>
<p>two corrections that are constructed from the vectors pk and Hkqk. Weighted combi-
</p>
<p>nations of these formulae will therefore also be of this same type (symmetric, rank
</p>
<p>two, and constructed from pk and Hkqk). This observation naturally leads to consid-
</p>
<p>eration of a whole collection of updates, known as the Broyden family, defined by
</p>
<p>Hφ = (1 &minus; φ)HDFP + φHBFGS, (10.40)</p>
<p/>
</div>
<div class="page"><p/>
<p>10.4 The Broyden Family 295
</p>
<p>where φ is a parameter that may take any real value. Clearly φ = 0 and φ = 1 yield
</p>
<p>the DFP and BFGS updates, respectively. The Broyden family also includes the rank
</p>
<p>one update (see Exercise 12).
</p>
<p>An explicit representation of the Broyden family can be found, after a fair amount
</p>
<p>of algebra, to be
</p>
<p>H
φ
</p>
<p>k+1
= Hk +
</p>
<p>pkp
T
k
</p>
<p>pT
k
</p>
<p>qk
&minus;
</p>
<p>Hkqkq
T
k
</p>
<p>Hk
</p>
<p>qT
k
</p>
<p>Hkqk
+ φvkv
</p>
<p>T
k = H
</p>
<p>DFP
k+1 + φvkv
</p>
<p>T
k , (10.41)
</p>
<p>where
</p>
<p>vk = (q
T
k Hkqk)
</p>
<p>1/2
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>pk
</p>
<p>pT
k
</p>
<p>qk
&minus; Hkqk
</p>
<p>qT
k
</p>
<p>Hkqk
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>.
</p>
<p>This form will be useful in some later developments.
</p>
<p>A Broyden method is defined as a quasi-Newton method in which at each iteration
</p>
<p>a member of the Broyden family is used as the updating formula. The parameter φ
</p>
<p>is, in general, allowed to vary from one iteration to another, so a particular Broyden
</p>
<p>method is defined by a sequence φ1, φ2, . . ., of parameter values. A pure Broyden
</p>
<p>method is one that uses a constant φ.
</p>
<p>Since both HDFP and HBFGS satisfy the fundamental relation (10.29) for updates,
</p>
<p>this relation is also satisfied by all members of the Broyden family. Thus it can be
</p>
<p>expected that many properties that were found to hold for the DFP method will
</p>
<p>also hold for any Broyden method, and indeed this is so. The following is a direct
</p>
<p>extension of the theorem of Sect. 10.3.
</p>
<p>Theorem. If f is quadratic with positive definite Hessian F, then for a Broydenmethod
</p>
<p>pTi Fp j = 0, 0 � i &lt; j � k
</p>
<p>Hk+1Fpi = pi f or 0 � i � k.
</p>
<p>Proof. The proof parallels that of Sect. 10.3, since the results depend only on the
</p>
<p>basic relation (10.29) and the orthogonality (10.19) because of exact line search. �
</p>
<p>The Broyden family does not necessarily preserve positive definiteness of Hφ
</p>
<p>for all values of φ. However, we know that the DFP method does preserve positive
</p>
<p>definiteness. Hence from (10.41) it follows that positive definiteness is preserved
</p>
<p>for any φ � 0, since the sum of a positive definite matrix and a positive semidefinite
</p>
<p>matrix is positive definite. For φ &lt; 0 there is the possibility that Hφ may become
</p>
<p>singular, and thus special precautions should be introduced. In practice φ � 0 is
</p>
<p>usually imposed to avoid difficulties.
</p>
<p>There has been considerable experimentation with Broyden methods to deter-
</p>
<p>mine superior strategies for selecting the sequence of parameters φk.
</p>
<p>The above theorem shows that the choice is irrelevant in the case of a quadratic
</p>
<p>objective and accurate line search. More surprisingly, it has been shown that even for
</p>
<p>the case of nonquadratic functions and accurate line searches, the points generated</p>
<p/>
</div>
<div class="page"><p/>
<p>296 10 Quasi-Newton Methods
</p>
<p>by all Broyden methods will coincide (provided singularities are avoided and
</p>
<p>multiple minima are resolved consistently). This means that differences in methods
</p>
<p>are important only with inaccurate line search.
</p>
<p>For general nonquadratic functions of modest dimension, Broyden methods seem
</p>
<p>to offer a combination of advantages as attractive general procedures. First, they re-
</p>
<p>quire only that first-order (that is, gradient) information be available. Second, the
</p>
<p>directions generated can always be guaranteed to be directions of descent by arrang-
</p>
<p>ing for Hk to be positive definite throughout the process. Third, since for a quadratic
</p>
<p>problem the matrices Hk converge to the inverse Hessian in at most n steps, it might
</p>
<p>be argued that in the general case Hk will converge to the inverse Hessian at the
</p>
<p>solution, and hence convergence will be superlinear. Unfortunately, while the meth-
</p>
<p>ods are certainly excellent, their convergence characteristics require more careful
</p>
<p>analysis, and this will lead us to an important additional modification.
</p>
<p>Partial Quasi-Newton Methods
</p>
<p>There is, of course, the option of restarting a Broyden method every m + 1 steps,
</p>
<p>where m + 1 &lt; n. This would yield a partial quasi-Newton method that, for small
</p>
<p>values of m, would have modest storage requirements, since the approximate inverse
</p>
<p>Hessian could be stored implicitly by storing only the vectors pi and qi, i � m+1. In
</p>
<p>the quadratic case this method exactly corresponds to the partial conjugate gradient
</p>
<p>method and hence it has similar convergence properties.
</p>
<p>10.5 Convergence Properties
</p>
<p>The various schemes for simultaneously generating and using an approximation to
</p>
<p>the inverse Hessian are difficult to analyze definitively. One must therefore, to some
</p>
<p>extent, resort to the use of analogy and approximate analyses to determine their
</p>
<p>effectiveness. Nevertheless, the machinery we developed earlier provides a basis for
</p>
<p>at least a preliminary analysis.
</p>
<p>Global Convergence
</p>
<p>In practice, quasi-Newton methods are usually executed in a continuing fashion,
</p>
<p>starting with an initial approximation and successively improving it throughout the
</p>
<p>iterative process. Under various and somewhat stringent conditions, it can be proved
</p>
<p>that this procedure is globally convergent. If, on the other hand, the quasi-Newton
</p>
<p>methods are restarted every n or n + 1 steps by resetting the approximate inverse
</p>
<p>Hessian to its initial value, then global convergence is guaranteed by the presence
</p>
<p>of the first descent step of each cycle (which acts as a spacer step).</p>
<p/>
</div>
<div class="page"><p/>
<p>10.5 Convergence Properties 297
</p>
<p>Local Convergence
</p>
<p>The local convergence properties of quasi-Newton methods in the pure form dis-
</p>
<p>cussed so far are not as good as might first be thought. Let us focus on the local
</p>
<p>convergence properties of these methods when executed with the restarting feature.
</p>
<p>Specifically, consider a Broyden method and for simplicity assume that at the begin-
</p>
<p>ning of each cycle the approximate inverse Hessian is reset to the identity matrix.
</p>
<p>Each cycle, if at least n steps in duration, will then contain one complete cycle of an
</p>
<p>approximation to the conjugate gradient method. Asymptotically, in the tail of the
</p>
<p>generated sequence, this approximation becomes arbitrarily accurate, and hence we
</p>
<p>may conclude, as for any method that asymptotically approaches the conjugate gra-
</p>
<p>dient method, that the method converges superlinearly (at least if viewed at the end
</p>
<p>of each cycle). Although superlinear convergence is attractive, the fact that in this
</p>
<p>case it hinges on repeated cycles of n steps in duration can seriously detract from its
</p>
<p>practical significance for problems with large n, since we might hope to terminate
</p>
<p>the procedure before completing even a single full cycle of n steps.
</p>
<p>To obtain insight into the defects of the method, let us consider a special situation.
</p>
<p>Suppose that f is quadratic and that the eigenvalues of the Hessian, F, of f are close
</p>
<p>together but all very large. If, starting with the identity matrix, an approximation
</p>
<p>to the inverse Hessian is updated m times, the matrix HmF will have m eigenvalues
</p>
<p>equal to unity and the rest will still be large. Thus, the ratio of smallest to largest
</p>
<p>eigenvalue of HmF, the condition number, will be worse than for F itself. Therefore,
</p>
<p>if the updating were discontinued and Hm were used as the approximation to F
&minus;1 in
</p>
<p>future iterations according to the procedure of Sect. 10.1, we see that convergence
</p>
<p>would be poorer than it would be for ordinary steepest descent. In other words, the
</p>
<p>approximations to F&minus;1 generated by the updating formulas, although accurate over
the subspace traveled, do not necessarily improve and, indeed, are likely to worsen
</p>
<p>the eigenvalue structure of the iteration process.
</p>
<p>In practice a poor eigenvalue structure arising in this manner will play a domi-
</p>
<p>nating role whenever there are factors that tend to weaken its approximation to the
</p>
<p>conjugate gradient method. Common factors of this type are round-off errors, in-
</p>
<p>accurate line searches, and nonquadratic terms in the objective function. Indeed, it
</p>
<p>has been frequently observed, empirically, that performance of the DFP method is
</p>
<p>highly sensitive to the accuracy of the line search algorithm&mdash;to the point where
</p>
<p>superior step-wise convergence properties can only be obtained through excessive
</p>
<p>time expenditure in the line search phase.
</p>
<p>Example. To illustrate some of these conclusions we consider the six-dimensional
</p>
<p>problem defined by
</p>
<p>f (x) =
1
</p>
<p>2
xTQx,</p>
<p/>
</div>
<div class="page"><p/>
<p>298 10 Quasi-Newton Methods
</p>
<p>where
</p>
<p>Q =
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>40 0 0 0 0 0
</p>
<p>0 38 0 0 0 0
</p>
<p>0 0 36 0 0 0
</p>
<p>0 0 0 34 0 0
</p>
<p>0 0 0 0 32 0
</p>
<p>0 0 0 0 0 30
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>.
</p>
<p>This function was minimized iteratively (the solution is obviously x&lowast; = 0) starting
at x0 =(10, 10, 10, 10, 10, 10), with f (x0) = 10,500, by using, alternatively, the
</p>
<p>method of steepest descent, the DFP method, the DFP method restarted every six
</p>
<p>steps, and the self-scaling method described in the next section. For this quadratic
</p>
<p>problem the appropriate step size to take at any stage can be calculated by a simple
</p>
<p>formula. On different computer runs of a given method, different levels of error were
</p>
<p>deliberately introduced into the step size in order to observe the effect of line search
</p>
<p>accuracy. This error took the form of a fixed percentage increase over the optimal
</p>
<p>value. The results are presented below:
</p>
<p>Case 1. No error in step size α
</p>
<p>Function value
</p>
<p>Iteration Steepest descent DFP DFP (with restart) Self-scaling
</p>
<p>1 96.29630 96.29630 96.29630 96.29630
</p>
<p>2 1.560669 6.900839&times; 10&minus;1 6.900839&times; 10&minus;1 6.900839&times; 10&minus;1
3 2.932559&times; 10&minus;2 3.988497&times; 10&minus;3 3.988497&times; 10&minus;3 3.988497&times; 10&minus;3
4 5.787315&times; 10&minus;4 1.683310&times; 10&minus;5 1.683310&times; 10&minus;5 1.683310&times; 10&minus;5
5 1.164595&times; 10&minus;5 3.878639&times; 10&minus;8 3.878639&times; 10&minus;8 3.878639&times; 10&minus;8
6 2.359563&times; 10&minus;7
</p>
<p>Case 2. 0.1 % error in step size α
</p>
<p>Function value
</p>
<p>Iteration Steepest descent DFP DFP (with restart) Self-scaling
</p>
<p>1 96.30669 96.30669 96.30669 96.30669
</p>
<p>2 1.564971 6.994023&times; 10&minus;1 6.994023&times; 10&minus;1 6.902072&times; 10&minus;1
3 2.939804&times; 10&minus;2 1.225501&times; 10&minus;2 1.225501&times; 10&minus;2 3.989507&times; 10&minus;3
4 5.810123&times; 10&minus;4 7.301088&times; 10&minus;3 7.301088&times; 10&minus;3 1.684263&times; 10&minus;5
5 1.169205&times; 10&minus;5 2.636716&times; 10&minus;3 2.636716&times; 10&minus;3 3.881674&times; 10&minus;8
6 2.372385&times; 10&minus;7 1.031086&times; 10&minus;5 1.031086&times; 10&minus;5
7 3.633330&times; 10&minus;9 2.399278&times; 10&minus;8</p>
<p/>
</div>
<div class="page"><p/>
<p>10.5 Convergence Properties 299
</p>
<p>Case 3. 1 % error in step size α
</p>
<p>Function value
</p>
<p>Iteration Steepest descent DFP DFP (with restart) Self-scaling
</p>
<p>1 97.33665 97.33665 97.33665 97.33665
</p>
<p>2 1.586251 1.621908 1.621908 0.7024872
</p>
<p>3 2.989875&times; 10&minus;2 8.268893&times; 10&minus;1 8.268893&times; 10&minus;1 4.090350&times; 10&minus;3
4 5.908101&times; 10&minus;4 4.302943&times; 10&minus;1 4.302943&times; 10&minus;1 1.779424&times; 10&minus;5
5 1.194144&times; 10&minus;5 4.449852&times; 10&minus;3 4.449852&times; 10&minus;3 4.195668&times; 10&minus;8
6 2.422985&times; 10&minus;7 5.337835&times; 10&minus;5 5.337835&times; 10&minus;5
7 3.767830&times; 10&minus;5 4.493397&times; 10&minus;7
8 3.768097&times; 10&minus;9
</p>
<p>Case 4. 10 % error in step size α
</p>
<p>Function value
</p>
<p>Iteration Steepest descent DFP DFP (with restart) Self-scaling
</p>
<p>1 200.333 200.333 200.333 200.333
</p>
<p>2 2.732789 93.65457 93.65457 2.811061
</p>
<p>3 3.836899&times; 10&minus;2 56.92999 56.92999 3.562769&times; 10&minus;2
4 6.376461&times; 10&minus;4 1.620688 1.620688 4.200600&times; 10&minus;4
5 1.219515&times; 10&minus;5 5.251115&times; 10&minus;1 5.251115&times; 10&minus;1 4.726918&times; 10&minus;6
6 2.457944&times; 10&minus;7 3.323745&times; 10&minus;1 3.323745&times; 10&minus;1
7 6.150890&times; 10&minus;3 8.102700&times; 10&minus;3
8 3.025393&times; 10&minus;3 2.973021&times; 10&minus;3
9 3.025476&times; 10&minus;5 1.950152&times; 10&minus;3
10 3.025476&times; 10&minus;7 2.769299&times; 10&minus;5
11 1.760320&times; 10&minus;5
12 1.123844&times; 10&minus;6
</p>
<p>We note first that the error introduced is reported as a percentage of the step
</p>
<p>size itself. In terms of the change in function value, the quantity that is most often
</p>
<p>monitored to determine when to terminate a line search, the fractional error is the
</p>
<p>square of that in the step size. Thus, a one percent error in step size is equivalent to
</p>
<p>a 0.01 % error in the change in function value.
</p>
<p>Next we note that the method of steepest descent is not radically affected by an
</p>
<p>inaccurate line search while the DFP methods are. Thus for this example while DFP
</p>
<p>is superior to steepest descent in the case of perfect accuracy, it becomes inferior at
</p>
<p>an error of only 0.1 % in step size.</p>
<p/>
</div>
<div class="page"><p/>
<p>300 10 Quasi-Newton Methods
</p>
<p>10.6 Scaling
</p>
<p>There is a general viewpoint about what makes up a desirable descent method that
</p>
<p>underlies much of our earlier discussions and which we now summarize briefly in
</p>
<p>order to motivate the presentation of scaling. A method that converges to the exact
</p>
<p>solution after n steps when applied to a quadratic function on En has obvious appeal
</p>
<p>especially if, as is usually the case, it can be inferred that for nonquadratic problems
</p>
<p>repeated cycles of length n of the method will yield superlinear convergence. For
</p>
<p>problems having large n, however, a more sophisticated criterion of performance
</p>
<p>needs to be established, since for such problems one usually hopes to be able to
</p>
<p>terminate the descent process before completing even a single full cycle of length
</p>
<p>n. Thus, with these sorts of problems in mind, the finite-step convergence property
</p>
<p>serves at best only as a sign post indicating that the algorithm might, make rapid
</p>
<p>progress in its early stages. It is essential to insure that in fact it will make rapid
</p>
<p>progress at every stage. Furthermore, the rapid convergence at each step must not
</p>
<p>be tied to an assumption on conjugate directions, a property easily destroyed by
</p>
<p>inaccurate line search and nonquadratic objective functions. With this viewpoint
</p>
<p>it is natural to look for quasi-Newton methods that simultaneously possess favor-
</p>
<p>able eigenvalue structure at each step (in the sense of Sect. 10.1) and reduce to the
</p>
<p>conjugate gradient method if the objective function happens to be quadratic. Such
</p>
<p>methods are developed in this section.
</p>
<p>Improvement of Eigenvalue Ratio
</p>
<p>Referring to the example presented in the last section where the Davidon-Fletcher-
</p>
<p>Powell method performed poorly, we can trace the difficulty to the simple observa-
</p>
<p>tion that the eigenvalues of H0Q are all much larger than unity. The DFP algorithm,
</p>
<p>or any Broyden method, essentially moves these eigenvalues, one at a time, to unity
</p>
<p>thereby producing an unfavorable eigenvalue ratio in each HkQ for 1 � k &lt; n. This
</p>
<p>phenomenon can be attributed to the fact that the methods are sensitive to simple
</p>
<p>scale factors. In particular if H0 were multiplied by a constant, the whole process
</p>
<p>would be different. In the example of the last section, if H0 were scaled by, for in-
</p>
<p>stance, multiplying it by 1/35, the eigenvalues of H0Q would be spread above and
</p>
<p>below unity, and in that case one might suspect that the poor performance would not
</p>
<p>show up.
</p>
<p>Motivated by the above considerations, we shall establish conditions under which
</p>
<p>the eigenvalue ratio of Hk+1F is at least as favorable as that of HkF in a Broyden
</p>
<p>method. These conditions will then be used as a basis for introducing appropriate
</p>
<p>scale factors.
</p>
<p>We use (but do not prove) the following matrix theoretic result due to Loewner.
</p>
<p>Interlocking Eigenvalues Lemma. Let the symmetric n&times; n matrix A have eigen-
values λ1 � λ2 � . . . � λn. Let a be any vector in E
</p>
<p>n and denote the eigenvalues of
</p>
<p>the matrix A + aaT by μ1 � μ2 . . . � μn. Then λ1 � μ1 � λ2 � μ2 . . . � λn � μn.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.6 Scaling 301
</p>
<p>For convenience we introduce the following definitions:
</p>
<p>Rk = F
1/2
k
</p>
<p>HkF
1/2
k
</p>
<p>rk = F
1/2
k
</p>
<p>pk.
</p>
<p>Then using qk = F
1/2
k
</p>
<p>rk, it can be readily verified that (10.41) is equivalent to
</p>
<p>R
φ
</p>
<p>k+1
= Rk &minus;
</p>
<p>Rkrkr
T
k
</p>
<p>Rk
</p>
<p>rT
k
</p>
<p>Rkrk
+
</p>
<p>rkr
T
k
</p>
<p>rT
k
</p>
<p>rk
+ φzkz
</p>
<p>T
k , (10.42)
</p>
<p>where
</p>
<p>zk = F
1/2Vk =
</p>
<p>&radic;
</p>
<p>rT
k
</p>
<p>Rkrk
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>rk
</p>
<p>rT
k
</p>
<p>rk
&minus; Rkrk
</p>
<p>rT
k
</p>
<p>Rkrk
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>.
</p>
<p>Since Rk is similar to HkF (because HkF = F
1/2RkF
</p>
<p>1/2), both have the same eigen-
</p>
<p>values. It is most convenient, however, in view of (10.42) to study Rk, obtaining
</p>
<p>conclusions about HkF indirectly.
</p>
<p>Before proving the general theorem we shall consider the case φ = 0 correspond-
</p>
<p>ing to the DFP formula. Suppose the eigenvalues of Rk are λ1, λ2, . . . , λn with
</p>
<p>0 &lt; λ1 � λ2 � . . . � λn. Suppose also that 1 &isin; [λ1, λn]. We will show that the eigen-
values of Rk+1 are all contained in the interval [λ1, λn], which of course implies that
</p>
<p>Rk+1 is no worse than Rk in terms of its condition number. Let us first consider the
</p>
<p>matrix
</p>
<p>P = Rk &minus;
Rkrkr
</p>
<p>T
k
</p>
<p>Rk
</p>
<p>rT
k
</p>
<p>Rkrk
.
</p>
<p>We see that Prk = 0 so one eigenvalue of P is zero. If we denote the eigenvalues of
</p>
<p>P by μ1 � μ2 � . . . � μn, we have from the above observation and the lemma on
</p>
<p>interlocking eigenvalues that
</p>
<p>0 = μ1 � λ1 � μ2 � . . . � μn � λn.
</p>
<p>Next we consider
</p>
<p>Rk+1 = Rk &minus;
Rkrkr
</p>
<p>T
k
</p>
<p>Rk
</p>
<p>rT
k
</p>
<p>Rkrk
+
</p>
<p>rkr
T
k
</p>
<p>rT
k
</p>
<p>rk
= P +
</p>
<p>rkr
T
k
</p>
<p>rT
k
</p>
<p>rk
. (10.43)
</p>
<p>Since rk is an eigenvector of P and since, by symmetry, all other eigenvectors of P
</p>
<p>are therefore orthogonal to rk, it follows that the only eigenvalue different in Rk+1
from in P is the one corresponding to rk&mdash;it now being unity. Thus Rk+1 has eigen-
</p>
<p>values μ2, μ3, . . . , μn and unity. These are all contained in the interval [λ1, λn].
</p>
<p>Thus updating does not worsen the eigenvalue ratio. It should be noted that this
</p>
<p>result in no way depends on αk being selected to minimize f .</p>
<p/>
</div>
<div class="page"><p/>
<p>302 10 Quasi-Newton Methods
</p>
<p>We now extend the above to the Broyden class with 0 � φ � 1.
</p>
<p>Theorem. Let the n eigenvalues of HkF be λ1, λ2 , . . . , λn with 0 &lt; λ1 � λ2 � . . . � λn.
Suppose that 1 &isin; [λ1 , λn]. Then for any φ, 0 � φ � 1, the eigenvalues of Hφk+1F, where
H
</p>
<p>φ
</p>
<p>k+1
is defined by (10.41), are all contained in [λ1 , λn].
</p>
<p>Proof. The result shown above corresponds to φ = 0. Let us now consider φ = 1,
</p>
<p>corresponding to the BFGS formula. By our original definition of the BFGS update,
</p>
<p>H&minus;1 is defined by the formula that is complementary to the DFP formula. Thus
</p>
<p>H&minus;1k+1 = H
&minus;1
k +
</p>
<p>qkq
T
k
</p>
<p>qT
k
</p>
<p>pk
&minus;
</p>
<p>H&minus;1k+1pkp
T
k
</p>
<p>H&minus;1k
</p>
<p>pT
k
</p>
<p>H&minus;1k pk
.
</p>
<p>This is equivalent to
</p>
<p>R&minus;1k+1 = R
&minus;1
k &minus;
</p>
<p>R&minus;1k rkr
T
k
</p>
<p>R&minus;1k
</p>
<p>rT
k
</p>
<p>R&minus;1k rk
+
</p>
<p>rkr
T
k
</p>
<p>rT
k
</p>
<p>rk
, (10.44)
</p>
<p>which is identical to (10.43) except that Rk is replaced by R
&minus;1
k
</p>
<p>.
</p>
<p>The eigenvalues of R&minus;1
k
</p>
<p>are 1/λn � 1/λn&minus;1 � . . . � 1/λ1. Clearly, 1 &isin;
[1/λn, 1/λ1]. Thus by the preliminary result, if the eigenvalues of R
</p>
<p>&minus;1
k+1 are de-
</p>
<p>noted 1/μn &lt; 1/μn&minus;1 &lt; . . . &lt; 1/μ1, it follows that they are contained in the interval
[1/λn, 1/λ1]. Thus 1/λn &lt; 1/μn and 1/λ1 &gt; 1/μ1. When inverted this yields μ1 &gt; λ1
and μn &lt; λn, which shows that the eigenvalues of Rk+1 are contained in [λ1, λn].
</p>
<p>This establishes the result for φ = 1.
</p>
<p>For general φ the matrix R
φ
</p>
<p>k+1
defined by (10.42) has eigenvalues that are all
</p>
<p>monotonically increasing with φ (as can be seen from the interlocking eigenvalues
</p>
<p>lemma). However, from above it is known that these eigenvalues are contained in
</p>
<p>[λ1, λn] for φ = 0 and φ = 1. Hence, they must be contained in [λ1, λn] for all
</p>
<p>φ, 0 � φ � 1. �
</p>
<p>Scale Factors
</p>
<p>In view of the result derived above, it is clearly advantageous to scale the matrix Hk
so that the eigenvalues of HkF are spread both below and above unity. Of course
</p>
<p>in the ideal case of a quadratic problem with perfect line search this is strictly only
</p>
<p>necessary for H0, since unity is an eigenvalue of HkF for k &gt; 0. But because of
</p>
<p>the inescapable deviations from the ideal, it is useful to consider the possibility of
</p>
<p>scaling every Hk.
</p>
<p>A scale factor can be incorporated directly into the updating formula. We first
</p>
<p>multiply Hk by the scale factor γk and then apply the usual updating formula. This
</p>
<p>is equivalent to replacing Hk by γkHk in (10.42) and leads to</p>
<p/>
</div>
<div class="page"><p/>
<p>10.6 Scaling 303
</p>
<p>Hk+1 =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>Hk &minus;
Hkqkq
</p>
<p>T
k
</p>
<p>Hk
</p>
<p>qT
k
</p>
<p>Hkqk
+ φkvkv
</p>
<p>T
k
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>γk +
pkp
</p>
<p>T
k
</p>
<p>pT
k
</p>
<p>qk
. (10.45)
</p>
<p>This defines a two-parameter family of updates that reduces to the Broyden family
</p>
<p>for γk = 1.
</p>
<p>Using γ0, γ1, . . . as arbitrary positive scale factors, we consider the algorithm:
</p>
<p>Start with any symmetric positive definite matrix H0 and any point x0, then starting
</p>
<p>with k = 0,
</p>
<p>Step 1. Set dk = &minus;Hkgk.
Step 2. Minimize f (xk + αdk) with respect to α � 0 to obtain xk+1, Pk = αkdk,
</p>
<p>and gk+1.
</p>
<p>Step 3. Set qk = gk+1 &minus; gk and
</p>
<p>Hk+1 =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>Hk &minus;
Hkqkq
</p>
<p>T
k
</p>
<p>Hk
</p>
<p>qT
k
</p>
<p>Hkqk
+ φkvkv
</p>
<p>T
k
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>γk +
pkp
</p>
<p>T
k
</p>
<p>pT
k
</p>
<p>qk
</p>
<p>vk = (q
T
k Hqk)
</p>
<p>1/2
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>pk
</p>
<p>pT
k
</p>
<p>qk
&minus; Hkqk
</p>
<p>qT
k
</p>
<p>Hkqk
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>. (10.46)
</p>
<p>The use of scale factors does destroy the property Hn = F
&minus;1 in the quadratic case,
</p>
<p>but it does not destroy the conjugate direction property. The following properties of
</p>
<p>this method can be proved as simple extensions of the results given in Sect. 10.3.
</p>
<p>1. If Hk is positive definite and p
T
k
</p>
<p>qk &gt; 0, (10.46) yields an Hk+1 that is positive
</p>
<p>definite.
</p>
<p>2. If f is quadratic with Hessian F, then the vectors P0, p1, . . . , pn&minus;1 are mutually
F-orthogonal, and, for each k, the vectors P0, p1, . . . , pk are eigenvectors of
</p>
<p>Hk+1F.
</p>
<p>We can conclude that scale factors do not destroy the underlying conjugate be-
</p>
<p>havior of the algorithm. Hence we can use scaling to ensure good single-step con-
</p>
<p>vergence properties.
</p>
<p>A Self-scaling Quasi-Newton Algorithm
</p>
<p>The question that arises next is how to select appropriate scale factors. If λ1 �
</p>
<p>λ2 � . . . � λn are the eigenvalues of HkF, we want to multiply Hk by γk where
</p>
<p>λ1 � 1/γk � λn. This will ensure that the new eigenvalues contain unity in the
</p>
<p>interval they span.
</p>
<p>Note that in terms of our earlier notation
</p>
<p>qT
k
</p>
<p>Hkqk
</p>
<p>pT
k
</p>
<p>qk
=
</p>
<p>rT
k
</p>
<p>Rkrk
</p>
<p>rT
k
</p>
<p>rk
.</p>
<p/>
</div>
<div class="page"><p/>
<p>304 10 Quasi-Newton Methods
</p>
<p>Recalling that Rk has the same eigenvalues as HkF and noting that for any rk
</p>
<p>λ1 �
rT
k
</p>
<p>Rkrk
</p>
<p>rT
k
</p>
<p>rk
� λn,
</p>
<p>we see that
</p>
<p>γk =
pT
k
</p>
<p>qk
</p>
<p>qT
k
</p>
<p>Hkqk
(10.47)
</p>
<p>serves as a suitable scale factor.
</p>
<p>We now state a complete self-scaling, restarting, quasi-Newton method based on
</p>
<p>the ideas above. For simplicity we take φ = 0 and thus obtain a modification of the
</p>
<p>DFP method. Start at any point x0, k = 0.
</p>
<p>Step 1. Set Hk = I.
</p>
<p>Step 2. Set dk = &minus;Hkgk.
Step 3. Minimize f (xk + αdk) with respect to α � 0 to obtain αk, xk+1, pk =
</p>
<p>αkdk, gk+1 and qk = gk+1 &minus; gk. (Select αk accurately enough to ensure pTk qk &gt; 0.)
Step 4. If k is not an integer multiple of n, set
</p>
<p>Hk+1 =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>Hk &minus;
Hkqkq
</p>
<p>T
k
</p>
<p>Hk
</p>
<p>qT
k
</p>
<p>Hkqk
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>pT
k
</p>
<p>qk
</p>
<p>qT
k
</p>
<p>Hkqk
+
</p>
<p>pkp
T
k
</p>
<p>pT
k
</p>
<p>qk
. (10.48)
</p>
<p>Add one to k and return to Step 2. If k is an integer multiple of n, return to Step 1.
</p>
<p>This algorithm was run, with various amounts of inaccuracy introduced in the line
</p>
<p>search, on the quadratic problem presented in Sect. 10.4. The results are presented
</p>
<p>in that section.
</p>
<p>10.7 Memoryless Quasi-Newton Methods
</p>
<p>The preceding development of quasi-Newton methods can be used as a basis for
</p>
<p>reconsideration of conjugate gradient methods. The result is an attractive class of
</p>
<p>new procedures.
</p>
<p>Consider a simplification of the BFGS quasi-Newton method where Hk+1 is de-
</p>
<p>fined by a BFGS update applied to H = I, rather than to Hk. Thus Hk+1 is determined
</p>
<p>without reference to the previous Hk, and hence the update procedure is memoryless.
</p>
<p>This update procedure leads to the following algorithm: Start at any point x0, k = 0.
</p>
<p>Step 1.
</p>
<p>Set Hk = I. (10.49)
</p>
<p>Step 2.
</p>
<p>Set dk = &minus;Hkgk. (10.50)</p>
<p/>
</div>
<div class="page"><p/>
<p>10.7 Memoryless Quasi-Newton Methods 305
</p>
<p>Step 3. Minimize f (xk + αdk) with respect to α � 0 to obtain αk, xk+1, pk =
</p>
<p>αkdk, gk+1, and qk = gk+1&minus;gk. (Select αk accurately enough to ensure pTk qk &gt; 0.)
Step 4. If k is not an integer multiple of n, set
</p>
<p>Hk+1 = I &minus;
qkp
</p>
<p>T
k
+ pkq
</p>
<p>T
k
</p>
<p>pT
k
</p>
<p>qk
+
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>1 +
qT
k
</p>
<p>qk
</p>
<p>pT
k
</p>
<p>qk
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>pkp
T
k
</p>
<p>pT
k
</p>
<p>qk
. (10.51)
</p>
<p>Add 1 to k and return to Step 2. If k is an integer multiple of n, return to Step 1.
</p>
<p>Combining (10.50) and (10.51), it is easily seen that
</p>
<p>dk+1 = &minus;gk+1 +
qkp
</p>
<p>T
k
</p>
<p>gk+1 + pkq
T
k
</p>
<p>gk+1
</p>
<p>pT
k
</p>
<p>qk
&minus;
⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>1 +
qT
k
</p>
<p>qk
</p>
<p>pT
k
</p>
<p>qk
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>pkp
T
k
</p>
<p>gk&minus;1
</p>
<p>pT
k
</p>
<p>qk
. (10.52)
</p>
<p>If the line search is exact, then pT
k
</p>
<p>gk+1 = 0 and hence p
T
k
</p>
<p>qk = &minus;pTk gk. In this case
(10.52) is equivalent to
</p>
<p>dk+1 = &minus;gk+1 +
qT
k
</p>
<p>gk+1
</p>
<p>pT
k
</p>
<p>qk
pk = &minus;gk+1 + βkdk, (10.53)
</p>
<p>where
</p>
<p>βk =
qkq
</p>
<p>T
k+1
</p>
<p>gT
k
</p>
<p>qk
.
</p>
<p>This coincides exactly with the Polak-Ribiere form of the conjugate gradient method.
</p>
<p>Thus use of the BFGS update in this way yields an algorithm that is of the modified
</p>
<p>Newton type with positive definite coefficient matrix and which is equivalent to a
</p>
<p>standard implementation of the conjugate gradient method when the line search is
</p>
<p>exact.
</p>
<p>The algorithm can be used without exact line search in a form that is similar
</p>
<p>to that of the conjugate gradient method by using (10.52). This requires storage of
</p>
<p>only the same vectors that are required of the conjugate gradient method. In light
</p>
<p>of the theory of quasi-Newton methods, however, the new form can be expected
</p>
<p>to be superior when inexact line searches are employed, and indeed experiments
</p>
<p>confirm this.
</p>
<p>The above idea can be easily extended to produce a memoryless quasi-Newton
</p>
<p>method corresponding to any member of the Broyden family. The update formula
</p>
<p>(10.51) would simply use the general Broyden update (10.41) with Hk set equal to
</p>
<p>I. In the case of exact line search (with pT
k
</p>
<p>gk+1 = 0), the resulting formula for dk+1
reduces to
</p>
<p>dk+1 = &minus;gk+1 + (1 &minus; φ)
qT
k
</p>
<p>gk+1
</p>
<p>qT
k
</p>
<p>qk
qk + φ
</p>
<p>qT
k
</p>
<p>gk+1
</p>
<p>pT
k
</p>
<p>qk
pk. (10.54)
</p>
<p>We note that (10.54) is equivalent to the conjugate gradient direction (10.53) only
</p>
<p>for φ = 1, corresponding to the BFGS update. For this reason the choice φ = 1 is
</p>
<p>generally preferred for this type of method.</p>
<p/>
</div>
<div class="page"><p/>
<p>306 10 Quasi-Newton Methods
</p>
<p>Scaling and Preconditioning
</p>
<p>Since the conjugate gradient method implemented as a memoryless quasi-Newton
</p>
<p>method is a modified Newton method, the fundamental convergence theory based
</p>
<p>on condition number emphasized throughout this part of the book is applicable, as
</p>
<p>are the procedures for improving convergence. It is clear that the function scaling
</p>
<p>procedures discussed in the previous section can be incorporated.
</p>
<p>According to the general theory of modified Newton methods, it is the eigenval-
</p>
<p>ues of HkF(xk) that influence the convergence properties of these algorithms. From
</p>
<p>the analysis of the last section, the memoryless BFGS update procedure will, in the
</p>
<p>pure quadratic case, yield a matrix HkF that has a more favorable eigenvalue ratio
</p>
<p>than F itself only if the function f is scaled so that unity is contained in the interval
</p>
<p>spanned by the eigenvalues of F. Experimental evidence verifies that at least an ini-
</p>
<p>tial scaling of the function in this way can lead to significant improvement. Scaling
</p>
<p>can be introduced at every step as well, and complete self-scaling can be effective
</p>
<p>in some situations.
</p>
<p>It is possible to extend the scaling procedure to a more general precondition-
</p>
<p>ing procedure. In this procedure the matrix governing convergence is changed from
</p>
<p>F(xk) to HF(xk) for some H. If HF(xk) has its eigenvalues all close to unity, then
</p>
<p>the memoryless quasi-Newton method can be expected to perform exceedingly
</p>
<p>well, since it possesses simultaneously the advantages of being a conjugate gradient
</p>
<p>method and being a well-conditioned modified Newton method.
</p>
<p>Preconditioning can be conveniently expressed in the basic algorithm by simply
</p>
<p>replacing Hk in the BFGS update formula by H instead of I and replacing I by H in
</p>
<p>Step 1. Thus (10.51) becomes
</p>
<p>Hk+1 = H &minus;
Hqkp
</p>
<p>T
k
+ pkq
</p>
<p>T
k
</p>
<p>H
</p>
<p>qT
k
</p>
<p>qk
+
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>1 +
qT
k
</p>
<p>Hqk
</p>
<p>pT
k
</p>
<p>qk
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>pkp
T
k
</p>
<p>pT
k
</p>
<p>pk
, (10.55)
</p>
<p>and the explicit conjugate gradient version (10.52) is also modified accordingly.
</p>
<p>Preconditioning can also be used in conjunction with an (m + 1)-cycle partial
</p>
<p>conjugate gradient version of the memoryless quasi-Newton method. This is highly
</p>
<p>effective if a simple H can be found (as it sometimes can in problems with structure)
</p>
<p>so that the eigenvalues of HF(xk) are such that either all but m are equal to unity or
</p>
<p>they are in m bunches. For large-scale problems, methods of this type seem to be
</p>
<p>quite promising.
</p>
<p>*10.8 &lowast;Combination of Steepest Descent and Newton&rsquo;s Method
</p>
<p>In this section we digress from the study of quasi-Newton methods, and again
</p>
<p>expand our collection of basic principles. We present a combination of steepest de-
</p>
<p>scent and Newton&rsquo;s method which includes them both as special cases. The resulting</p>
<p/>
</div>
<div class="page"><p/>
<p>10.8 &lowast;Combination of Steepest Descent and Newton&rsquo;s Method 307
</p>
<p>combined method can be used to develop algorithms for problems having special
</p>
<p>structure, as illustrated in Chap. 13. This method and its analysis comprises a fun-
</p>
<p>damental element of the modern theory of algorithms.
</p>
<p>The method itself is quite simple. Suppose there is a subspace N of En on which
</p>
<p>the inverse Hessian of the objective function f is known (we shall make this state-
</p>
<p>ment more precise later). Then, in the quadratic case, the minimum of f over any
</p>
<p>linear variety parallel to N (that is, any translation of N) can be found in a single
</p>
<p>step. To minimize f over the whole space starting at any point xk, we could mini-
</p>
<p>mize f over the linear variety parallel to N and containing xk to obtain zk; and then
</p>
<p>take a steepest descent step from there. This procedure is illustrated in Fig. 10.1.
</p>
<p>Since zk is the minimum point of f over a linear variety parallel to N, the gradient
</p>
<p>at zk will be orthogonal to N, and hence the gradient step is orthogonal to N. If f
</p>
<p>is not quadratic we can, knowing the Hessian of f on N, approximate the minimum
</p>
<p>point of f over a linear variety parallel to N by one step of Newton&rsquo;s method. To
</p>
<p>implement this scheme, that we described in a geometric sense, it is necessary to
</p>
<p>agree on a method for defining the subspace N and to determine what information
</p>
<p>about the inverse Hessian is required so as to implement a Newton step over N. We
</p>
<p>now turn to these questions.
</p>
<p>Often, the most convenient way to describe a subspace, and the one we follow
</p>
<p>in this development, is in terms of a set of vectors that generate it. Thus, if B is an
</p>
<p>n&times;m matrix consisting of m column vectors that generate N, we may write N as the
set of all vectors of the form Bu where u &isin; Em. For simplicity we always assume
that the columns of B are linearly independent.
</p>
<p>To see what information about the inverse Hessian is required, imagine that we
</p>
<p>are at a point xk and wish to find the approximate minimum point zk of f with
</p>
<p>respect to movement in N. Thus, we seek uk so that
</p>
<p>zk = xk + Buk
</p>
<p>approximately minimizes f . By &ldquo;approximately minimizes&rdquo; we mean that zk should
</p>
<p>be the Newton approximation to the minimum over this subspace. We write
</p>
<p>f (zk) � f (xk) + &nabla; f (xk)Buk +
1
</p>
<p>2
uTk B
</p>
<p>TF(xk)Buk
</p>
<p>and solve for uk to obtain the Newton approximation. We find
</p>
<p>uk = &minus;(BTF(xk)B)&minus;1BT&nabla; f (xk)T
</p>
<p>zk = xk &minus; B(BTF(xk)B)&minus;1BT&nabla; f (xk)T .
</p>
<p>We see by analogy with the formula for Newton&rsquo;s method that the expression
</p>
<p>B(BTF(xk)B)
&minus;1BT can be interpreted as the inverse of F(xk) restricted to the sub-
</p>
<p>space N.</p>
<p/>
</div>
<div class="page"><p/>
<p>308 10 Quasi-Newton Methods
</p>
<p>Fig. 10.1 Combined method
</p>
<p>Example. Suppose
</p>
<p>B =
</p>
<p>[
</p>
<p>I
</p>
<p>0
</p>
<p>]
</p>
<p>,
</p>
<p>where I is an m &times; m identity matrix. This corresponds to the case where N is the
subspace generated by the first m unit basis elements of En. Let us partition F =
</p>
<p>&nabla;
2 f (xk) as
</p>
<p>F =
</p>
<p>[
</p>
<p>F11 F12
F21 F22
</p>
<p>]
</p>
<p>,
</p>
<p>where F11 is m &times; m. Then, in this case
</p>
<p>(BTFB)&minus;1 = F&minus;111 ,
</p>
<p>and
</p>
<p>B(BTFB)&minus;1BT =
</p>
<p>[
</p>
<p>F&minus;111 0
0 0
</p>
<p>]
</p>
<p>,
</p>
<p>which shows explicitly that it is the inverse of F on N that is required. The general
</p>
<p>case can be regarded as being obtained through partitioning in some skew coordinate
</p>
<p>system.
</p>
<p>Now that the Newton approximation over N has been derived, it is possible to
</p>
<p>formalize the details of the algorithm suggested by Fig. 10.1. At a given point xk,
</p>
<p>the point xk+1 is determined through
</p>
<p>a) Setdk = &minus;B(BTF(xk)B)&minus;1BT&nabla;f(xk)T .
b) zk = xk + βkdk, where βk minimizes f(xk + βdk). (10.56)
</p>
<p>c) Set pk = &minus;&nabla;f(zk)T .
d) xk+1 = zk + αkpk, where αk minimizes f(zk + αpk).
</p>
<p>The scalar search parameter βk is introduced in the Newton part of the algorithm
</p>
<p>simply to assure that the descent conditions required for global convergence are
</p>
<p>met. Normally βk will be approximately equal to unity. (See Sect. 8.5.)</p>
<p/>
</div>
<div class="page"><p/>
<p>10.8 &lowast;Combination of Steepest Descent and Newton&rsquo;s Method 309
</p>
<p>Analysis of Quadratic Case
</p>
<p>Since the method is not a full Newton method, we can conclude that it possesses
</p>
<p>only linear convergence and that the dominating aspects of convergence will be re-
</p>
<p>vealed by an analysis of the method as applied to a quadratic function. Furthermore,
</p>
<p>as might be intuitively anticipated, the associated rate of convergence is governed
</p>
<p>by the steepest descent part of algorithm (10.56), and that rate is governed by a
</p>
<p>Kantorovich-like ratio defined over the subspace orthogonal to N.
</p>
<p>Theorem (Combined Method). Let Q be an n &times; n symmetric positive definite matrix, and
let x&lowast; &isin; En. Define the function
</p>
<p>E(x) =
1
</p>
<p>2
(x &minus; x&lowast;)TQ(x &minus; x&lowast;)
</p>
<p>and let b = Qx&lowast;. Let B be an n &times; m matrix of rank m. Starting at an arbitrary point x0,
define the iterative process
</p>
<p>a) uk = &minus;(BTQB)&minus;1BTgk, where gk = Qxk &minus; b.
b) zk = xk + Buk.
</p>
<p>c) pk = b &minus;Qzk.
</p>
<p>d) xk+1 = zk + αkpk, where αk =
pT
k
pk
</p>
<p>pT
k
Qpk
</p>
<p>.
</p>
<p>This process converges to x&lowast;, and satisfies
</p>
<p>E(xk+1) � (1 &minus; δ)E(xk) (10.57)
</p>
<p>where δ, 0 � 8 � 1, is the minimum of
</p>
<p>(pTp)2
</p>
<p>(pTQp)(pTQ&minus;1p)
</p>
<p>over all vectors p in the nullspace of BT .
</p>
<p>Proof. The algorithm given in the theorem statement is exactly the general com-
</p>
<p>bined algorithm specialized to the quadratic situation. Next we note that
</p>
<p>BTpk = B
TQ(x&lowast; &minus; zk) = BTQ(x&lowast; &minus; xk) &minus; BTQBuk (10.58)
</p>
<p>= &minus;BTgk + BQBT (BTQB)&minus;1BTgk = 0,
</p>
<p>which merely proves that the gradient at zk is orthogonal to N. Next we calculate
</p>
<p>2{E(xk) &minus; E(zk)} = (xk &minus; x&lowast;)TQ(xk &minus; x&lowast;) &minus; (zk &minus; x&lowast;)TQ(zk &minus; x&lowast;)
= &minus;2uTk BTQ(xk &minus; x&lowast;) &minus; uTk BTQBuk
= &minus;2uTk BTgk + uTk BTQB(BTQB)&minus;1BTgk (10.59)
= &minus;uTk BTgk = gTk B(BTQB)&minus;1BTgk.</p>
<p/>
</div>
<div class="page"><p/>
<p>310 10 Quasi-Newton Methods
</p>
<p>Then we compute
</p>
<p>2{E(zk) &minus; E(xk+1)} = (zk &minus; x&lowast;)TQ(zk &minus; x&lowast;) &minus; (xk+1 &minus; x&lowast;)TQ(xk+1 &minus; x&lowast;)
= &minus;2αkpTk Q(zk &minus; x&lowast;) &minus; α2kpTk Qpk
= 2αkp
</p>
<p>T
k pk &minus; α2kpTk Qpk (10.60)
</p>
<p>= αkp
T
k pk =
</p>
<p>(pT
k
</p>
<p>pk)
2
</p>
<p>pT
k
</p>
<p>Qpk
.
</p>
<p>Now using (10.58) and pk = &minus;gk &minus; QBuk we have
</p>
<p>2E(xk) = (xk &minus; x&lowast;)TQ(xk &minus; x&lowast;) = gTk Q&minus;1gk
= (pTk + u
</p>
<p>T
k B
</p>
<p>TQ)Q&minus;1(pk + QBuk) (10.61)
</p>
<p>= pTk Q
&minus;1pk + u
</p>
<p>T
k B
</p>
<p>TQBuk
</p>
<p>= pTk Q
&minus;1pk + g
</p>
<p>T
k B(B
</p>
<p>TQB)&minus;1BTgk.
</p>
<p>Adding (10.59) and (10.60) and dividing by (10.61) there results
</p>
<p>E(xk) &minus; E(xk+1)
E(xk)
</p>
<p>=
gT
k
</p>
<p>B(BTQB)&minus;1BTgk + (p
T
k
</p>
<p>pk)
2/pT
</p>
<p>k
Qpk
</p>
<p>pT
k
</p>
<p>Q&minus;1pk + g
T
k
</p>
<p>B(BTQB)&minus;1BTgk
</p>
<p>=
q + (pT
</p>
<p>k
pk)/(p
</p>
<p>T
k
</p>
<p>Qpk)
</p>
<p>q + (pT
k
</p>
<p>Q&minus;1pk)/(p
T
k
</p>
<p>pk)
,
</p>
<p>where q � 0. This has the form (q + a)/(q + b) with
</p>
<p>a =
pT
k
</p>
<p>pk
</p>
<p>pT
k
</p>
<p>Qpk
, b =
</p>
<p>pT
k
</p>
<p>Q&minus;1pk
</p>
<p>pT
k
</p>
<p>pk
.
</p>
<p>But for any pk, it follows that a � b. Hence
</p>
<p>q + a
</p>
<p>q + b
�
</p>
<p>a
</p>
<p>b
,
</p>
<p>and thus
E(xk) &minus; E(xk+1)
</p>
<p>E(xk)
�
</p>
<p>(pT
k
</p>
<p>pk)
2
</p>
<p>(pT
k
</p>
<p>Qpk)(p
T
k
</p>
<p>Q&minus;1pk)
.
</p>
<p>Finally,
</p>
<p>E(xk+1) � E(xk)
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>1 &minus;
(pT
</p>
<p>k
pk)
</p>
<p>2
</p>
<p>(pT
k
</p>
<p>Qpk)(p
T
k
</p>
<p>Q&minus;1pk)
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>� (1 &minus; δ)E(xk),
</p>
<p>since BTpk = 0. �</p>
<p/>
</div>
<div class="page"><p/>
<p>10.8 &lowast;Combination of Steepest Descent and Newton&rsquo;s Method 311
</p>
<p>The value δ associated with the above theorem is related to the eigenvalue struc-
</p>
<p>ture of Q. If p were allowed to vary over the whole space, then the Kantorovich
</p>
<p>inequality
(pTp)2
</p>
<p>(pTQp)(pTQ&minus;1p)
�
</p>
<p>4aA
</p>
<p>(a + A)2
, (10.62)
</p>
<p>where a and A are, respectively, the smallest and largest eigenvalues of Q, gives
</p>
<p>explicitly
</p>
<p>δ =
4aA
</p>
<p>(a + A)2
.
</p>
<p>When p is restricted to the nullspace of BT , the corresponding value of δ is larger.
</p>
<p>In some special cases it is possible to obtain a fairly explicit estimate of δ. Suppose,
</p>
<p>for example, that the subspace N were the subspace spanned by m eigenvectors of
</p>
<p>Q. Then the subspace in which p is allowed to vary is the space orthogonal to N and
</p>
<p>is thus, in this case, the space generated by the other n &minus; m eigenvectors of Q. In
this case since for p in N&perp; (the space orthogonal to N), both Qp and Q&minus;1p are also
in N&perp;, the ratio δ satisfies
</p>
<p>δ =
(pTp)2
</p>
<p>(pTQp)(pTQ&minus;1p)
�
</p>
<p>4aA
</p>
<p>(a + A)2
,
</p>
<p>where now a and A are, respectively, the smallest and largest of the n &minus; m eigen-
values of Q corresponding to N&perp;. Thus the convergence ratio (10.57) reduces to the
familiar form
</p>
<p>E(xk+1) �
(
</p>
<p>A &minus; a
A + a
</p>
<p>)2
</p>
<p>E(xk),
</p>
<p>where a and A are these special eigenvalues. Thus, if B, or equivalently N, is chosen
</p>
<p>to include the eigenvectors corresponding to the most undesirable eigenvalues of Q,
</p>
<p>the convergence rate of the combined method will be quite attractive.
</p>
<p>Applications
</p>
<p>The combination of steepest descent and Newton&rsquo;s method can be applied usefully
</p>
<p>in a number of important situations. Suppose, for example, we are faced with a
</p>
<p>problem of the form
</p>
<p>minimize f (x, y),
</p>
<p>where x &isin; En, y &isin; Em, and where the second partial derivatives with respect to x are
easily computable but those with respect to y are not. We may then employ Newton
</p>
<p>steps with respect to x and steepest descent with respect to y.
</p>
<p>Another instance where this idea can be greatly effective is when there are a few
</p>
<p>vital variables in a problem which, being assigned high costs, tend to dominate the
</p>
<p>value of the objective function; in other words, the partial second derivatives with
</p>
<p>respect to these variables are large. The poor conditioning induced by these variables</p>
<p/>
</div>
<div class="page"><p/>
<p>312 10 Quasi-Newton Methods
</p>
<p>can to some extent be reduced by proper scaling of variables, but more effectively,
</p>
<p>by carrying out Newton&rsquo;s method with respect to them and steepest descent with
</p>
<p>respect to the others.
</p>
<p>10.9 Summary
</p>
<p>The basic motivation behind quasi-Newton methods is to try to obtain, at least on
</p>
<p>the average, the rapid convergence associated with Newton&rsquo;s method without explic-
</p>
<p>itly evaluating the Hessian at every step. This can be accomplished by constructing
</p>
<p>approximations to the inverse Hessian based on information gathered during the de-
</p>
<p>scent process, and results in methods which viewed in blocks of n steps (where n is
</p>
<p>the dimension of the problem) generally possess superlinear convergence.
</p>
<p>Good, or even superlinear, convergence measured in terms of large blocks, how-
</p>
<p>ever, is not always indicative of rapid convergence measured in terms of individual
</p>
<p>steps. It is important, therefore, to design quasi-Newton methods so that their single
</p>
<p>step convergence is rapid and relatively insensitive to line search inaccuracies. We
</p>
<p>discussed two general principles for examining these aspects of descent algorithms.
</p>
<p>The first of these is the modified Newton method in which the direction of descent
</p>
<p>is taken as the result of multiplication of the negative gradient by a positive def-
</p>
<p>inite matrix S. The single step convergence ratio of this method is determined by
</p>
<p>the usual steepest descent formula, but with the condition number of SF rather than
</p>
<p>just F used. This result was used to analyze some popular quasi-Newton methods,
</p>
<p>to develop the self-scaling method having good single step convergence properties,
</p>
<p>and to reexamine conjugate gradient methods.
</p>
<p>The second principle method is the combined method in which Newton&rsquo;s method
</p>
<p>is executed over a subspace where the Hessian is known and steepest descent is
</p>
<p>executed elsewhere. This method converges at least as fast as steepest descent, and
</p>
<p>by incorporating the information gathered as the method progresses, the Newton
</p>
<p>portion can be executed over larger and larger subspaces.
</p>
<p>At this point, it is perhaps valuable to summarize some of the main themes that
</p>
<p>have been developed throughout the four chapters comprising Part II. These chap-
</p>
<p>ters contain several important and popular algorithms that illustrate the range of
</p>
<p>possibilities available for minimizing a general nonlinear function. From a broad
</p>
<p>perspective, however, these individual algorithms can be considered simply as spe-
</p>
<p>cific patterns on the analytical fabric that is woven through the chapters&mdash;the fabric
</p>
<p>that will support new algorithms and future developments.
</p>
<p>One unifying element, that has reproved its value several times, is the Global
</p>
<p>Convergence Theorem. This result helped mold the final form of every algorithm
</p>
<p>presented in Part II and has effectively resolved the major questions concerning
</p>
<p>global convergence.
</p>
<p>Another unifying element is the speed of convergence of an algorithm, which
</p>
<p>we have defined in terms of the asymptotic properties of the sequences an algo-
</p>
<p>rithm generates. Initially, it might have been argued that such measures, based on</p>
<p/>
</div>
<div class="page"><p/>
<p>10.10 Exercises 313
</p>
<p>properties of the tail of the sequence, are perhaps not truly indicative of the ac-
</p>
<p>tual time required to solve a problem&mdash;after all, a sequence generated in practice
</p>
<p>is a truncated version of the potentially infinite sequence, and asymptotic proper-
</p>
<p>ties may not be representative of the finite version&mdash;a more complex measure of the
</p>
<p>speed of convergence may be required. It is fair to demand that the validity of the
</p>
<p>asymptotic measures we have proposed be judged in terms of how well they pre-
</p>
<p>dict the performance of algorithms applied to specific examples. On this basis, as
</p>
<p>illustrated by the numerical examples presented in these chapters, and on others, the
</p>
<p>asymptotic rates are extremely reliable predictors of performance&mdash;provided that
</p>
<p>one carefully tempers one&rsquo;s analysis with common sense (by, for example, not con-
</p>
<p>cluding that superlinear convergence is necessarily superior to linear convergence
</p>
<p>when the superlinear convergence is based on repeated cycles of length n). A ma-
</p>
<p>jor conclusion, therefore, of the previous chapters is the essential validity of the
</p>
<p>asymptotic approach to convergence analysis. This conclusion is a major strand in
</p>
<p>the analytical fabric of nonlinear programming.
</p>
<p>10.10 Exercises
</p>
<p>1. Prove (10.4) directly for the modified Newton method by showing that each
</p>
<p>step of the modified Newton method is simply the ordinary method of steepest
</p>
<p>descent applied to a scaled version of the original problem.
</p>
<p>2. Find the rate of convergence of the version of Newton&rsquo;s method defined by
</p>
<p>(10.50), (10.51) of Chap. 8. Show that convergence is only linear if δ is larger
</p>
<p>than the smallest eigenvalue of F(x&lowast;).
3. Consider the problem of minimizing a quadratic function
</p>
<p>f (x) =
1
</p>
<p>2
xTQx &minus; xTb,
</p>
<p>where Q is symmetric and sparse (that is, there are relatively few nonzero en-
</p>
<p>tries in Q). The matrix Q has the form
</p>
<p>Q = I + V,
</p>
<p>where I is the identity and V is a matrix with eigenvalues bounded by e &lt; 1 in
</p>
<p>magnitude.
</p>
<p>(a) With the given information, what is the best bound you can give for the rate of
</p>
<p>convergence of steepest descent applied to this problem?
</p>
<p>(b) In general it is difficult to invert Q but the inverse can be approximated by I&ndash;V,
</p>
<p>which is easy to calculate. (The approximation is very good for small e.) We are
</p>
<p>thus led to consider the iterative process
</p>
<p>xk&minus;l = xk &minus; αk[I &minus; V]gk,</p>
<p/>
</div>
<div class="page"><p/>
<p>314 10 Quasi-Newton Methods
</p>
<p>where gk = Qxk &minus; b and αk is chosen to minimize f in the usual way. With
the information given, what is the best bound on the rate of convergence of this
</p>
<p>method?
</p>
<p>(c) Show that for e &lt; (
&radic;
</p>
<p>5 &minus; 1)/2 the method in part (b) is always superior to
steepest descent.
</p>
<p>4. This problem shows that the modified Newton&rsquo;s method is globally convergent
</p>
<p>under very weak assumptions.
</p>
<p>Let a &gt; 0 and b � a be given constants. Consider the collection P of all n &times; n
symmetric positive definite matrices P having all eigenvalues greater than or
</p>
<p>equal to a and all elements bounded in absolute value by b. Define the point-
</p>
<p>to-set mapping B : En &rarr; En+n2 by B(x) = {(x, P) : P &isin; P}. Show that B is a
closed mapping.
</p>
<p>Now given an objective function f &isin; C1, consider the iterative algorithm
</p>
<p>xk+1 = xk &minus; αkPkgk,
</p>
<p>where gk = g(xk) is the gradient of f at xk, Pk is any matrix from P and αk
is chosen to minimize f (xk+1). This algorithm can be represented by A which
</p>
<p>can be decomposed as A = SCB where B is defined above, C is defined by
</p>
<p>C(x, P) = (x, &minus;Pg(x)), and S is the standard line search mapping. Show that if
restricted to a compact set in En, the mapping A is closed.
</p>
<p>Assuming that a sequence {xk} generated by this algorithm is bounded, show
that the limit x&lowast; of any convergent subsequence satisfies g(x&lowast;) = 0.
</p>
<p>5. The following algorithm has been proposed for minimizing unconstrained func-
</p>
<p>tions f (x), x &isin; En, without using gradients: Starting with some arbitrary point
x0, obtain a direction of search dk such that for each component of dk
</p>
<p>f (xk = (dk)iei) = min
di
</p>
<p>f (xk + diei),
</p>
<p>where e j denotes the ith column of the identity matrix. In other words, the ith
</p>
<p>component of dk is determined through a line search minimizing f (x) along the
</p>
<p>ith component.
</p>
<p>The next point xk+1 is then determined in the usual way through a line search
</p>
<p>along dk; that is,
</p>
<p>xk+1 = xk + αkdk,
</p>
<p>where dk minimizes f (xk+1).
</p>
<p>(a) Obtain an explicit representation for the algorithm for the quadratic case where
</p>
<p>f (x) =
1
</p>
<p>2
(x &minus; x&lowast;)TQ(x &minus; x&lowast;) + f (x&lowast;).
</p>
<p>(b) What condition on f (x) or its derivatives will guarantee descent of this algo-
</p>
<p>rithm for general f (x)?
</p>
<p>(c) Derive the convergence rate of this algorithm (assuming a quadratic objective).
</p>
<p>Express your answer in terms of the condition number of some matrix.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.10 Exercises 315
</p>
<p>6. Suppose that the rank one correction method of Sect. 10.2 is applied to the
</p>
<p>quadratic problem (10.2) and suppose that the matrix R0 = F
1/2H0F
</p>
<p>1/2 has
</p>
<p>m &lt; n eigenvalues less than unity and n &minus; m eigenvalues greater than unity.
Show that the condition qT
</p>
<p>k
(pk &minus; Hkqk) &gt; 0 will be satisfied at most m times
</p>
<p>during the course of the method and hence, if updating is performed only when
</p>
<p>this condition holds, the sequence {Hk} will not converge to F&minus;1. Infer from this
that, in using the rank one correction method, H0 should be taken very small;
</p>
<p>but that, despite such a precaution, on nonquadratic problems the method is
</p>
<p>subject to difficulty.
</p>
<p>7. Show that if H0 = I the Davidon-Fletcher-Powell method is the conjugate gra-
</p>
<p>dient method. What similar statement can be made when H0 is an arbitrary
</p>
<p>symmetric positive definite matrix?
</p>
<p>8. In the text it is shown that for the Davidon-Fletcher-Powell method Hk+1 is pos-
</p>
<p>itive definite if Hk is. The proof assumed that αk is chosen to exactly minimize
</p>
<p>f (xk + αdk). Show that any αk &gt; 0 which leads to p
T
k
</p>
<p>qk &gt; 0 will guarantee
</p>
<p>the positive definiteness of Hk+1. Show that for a quadratic problem any αk � 0
</p>
<p>leads to a positive definite Hk+1.
</p>
<p>9. Suppose along the line xk+αdk, α &gt; 0, the function f (xk+αdk) is unimodal and
</p>
<p>differentiable. Let αk be the minimizing value of α. Show that if any αk &gt; αk is
</p>
<p>selected to define xk+1 = xk + αkdk, then p
T
k
</p>
<p>qk &gt; 0. (Refer to Sect. 10.3.)
</p>
<p>10. Let {Hk}, k = 0, 1, 2 . . . be the sequence of matrices generated by the Davidon-
Fletcher-Powell method applied, without restarting, to a function f having con-
</p>
<p>tinuous second partial derivatives. Assuming that there is a &gt; 0, A &gt; 0 such
</p>
<p>that for all k we have Hk&minus;aI and AI&minus;Hk positive definite and the corresponding
sequence of xk&rsquo;s is bounded, show that the method is globally convergent.
</p>
<p>11. Verify Eq. (10.41).
</p>
<p>12.
</p>
<p>(a) Show that starting with the rank one update formula for H, forming the com-
</p>
<p>plementary formula, and then taking the inverse restores the original formula.
</p>
<p>(b) What value of φ in the Broyden class corresponds to the rank one formula?
</p>
<p>13. Explain how the partial Davidon method can be implemented for m &lt; n/2, with
</p>
<p>less storage than required by the full method.
</p>
<p>14. Prove statements (10.1) and (10.2) below Eq. (10.46) in Sect. 10.6.
</p>
<p>15. Consider using
</p>
<p>γk =
pT
k
</p>
<p>H&minus;1k pk
</p>
<p>pT
k
</p>
<p>qk
</p>
<p>instead of (10.47).
</p>
<p>(a) Show that this also serves as a suitable scale factor for a self-scaling quasi-
</p>
<p>Newton method.
</p>
<p>(b) Extend part (a) to
</p>
<p>γk = (1 &minus; φ)
pT
k
</p>
<p>qk
</p>
<p>qT
k
</p>
<p>Hkqk
+ φ
</p>
<p>pT
k
</p>
<p>H&minus;1k pk
</p>
<p>pT
k
</p>
<p>qk
</p>
<p>for 0 � φ � 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>316 10 Quasi-Newton Methods
</p>
<p>16. Prove global convergence of the combination of steepest descent and Newton&rsquo;s
</p>
<p>method.
</p>
<p>17. Formulate a rate of convergence theorem for the application of the combination
</p>
<p>of steepest and Newton&rsquo;s method to nonquadratic problems.
</p>
<p>18. Prove that if Q is positive definite
</p>
<p>(pTp)
</p>
<p>pTQp
�
</p>
<p>pTQ&minus;1p
</p>
<p>pTp
</p>
<p>for any vector p.
</p>
<p>19. It is possible to combine Newton&rsquo;s method and the partial conjugate gradient
</p>
<p>method. Given a subspace N &sub; En, xk+1 is generated from xk by first finding
zk by taking a Newton step in the linear variety through xk parallel to N, and
</p>
<p>then taking m conjugate gradient steps from zk. What is a bound on the rate of
</p>
<p>convergence of this method?
</p>
<p>20. In this exercise we explore how the combined method of Sect. 10.7 can be
</p>
<p>updated as more information becomes available. Begin with N0 = {0}. If Nk is
represented by the corresponding matrix Bk, define Nk+1 by the corresponding
</p>
<p>Bk+1 = [Bk, pk], where pk = xk+1 &minus; zk.
(a) If Dk = Bk(B
</p>
<p>T
k FBk)
</p>
<p>&minus;1BTk is known, show that
</p>
<p>Dk+1 = Dk =
(pk &minus; Dkqk)(pk &minus; Dkqk)T
</p>
<p>(pk &minus; Dkqk)Tqk
,
</p>
<p>where qk = gk+1 &minus; gk. (This is the rank one correction of Sect. 10.2.)
(b) Develop an algorithm that uses (a) in conjunction with the combined method of
</p>
<p>Sect. 10.8 and discuss its convergence properties.
</p>
<p>References
</p>
<p>10.1 An early analysis of this method was given by Crockett and Chenoff
</p>
<p>[C9].
</p>
<p>10.2&ndash;10.3 The variable metric method was originally developed by Davidon [D12],
</p>
<p>and its relation to the conjugate gradient method was discovered by
</p>
<p>Fletcher and Powell [F11]. The rank one method was later developed
</p>
<p>by Davidon [D13] and Broyden [B24]. For an early general discussion
</p>
<p>of these methods, see Murtagh and Sargent [M10], and for an excellent
</p>
<p>recent review, see Dennis and Moré [D15].
</p>
<p>10.4 The Broyden family was introduced in Broyden [B24]. The BFGS
</p>
<p>method was suggested independently by Broyden [B25], Fletcher [F6],
</p>
<p>Goldfarb [G9], and Shanno [S3]. The beautiful concept of complemen-
</p>
<p>tarity, which leads easily to the BFGS update and definition of the Broy-
</p>
<p>den class as presented in the text, is due to Fletcher. Another larger class
</p>
<p>was defined by Huang [H13]. A variational approach to deriving variable</p>
<p/>
</div>
<div class="page"><p/>
<p>10.10 Exercises 317
</p>
<p>metric methods was introduced by Greenstadt [G15]. Also see Dennis
</p>
<p>and Schnabel [D16]. Originally there was considerable effort devoted to
</p>
<p>searching for a best sequence of φk&rsquo;s in a Broyden method, but Dixon
</p>
<p>[D17] showed that all methods are identical in the case of exact linear
</p>
<p>search. There are a number of numerical analysis and implementation is-
</p>
<p>sues that arise in connection with quasi-Newton updating methods. From
</p>
<p>this viewpoint Gill and Murray [G6] have suggested working directly
</p>
<p>with Bk, an approximation to the Hessian itself, and updating a triangu-
</p>
<p>lar factorization at each step.
</p>
<p>10.5 Under various assumptions on the criterion function, it has been shown
</p>
<p>that quasi- Newton methods converge globally and superlinearly, pro-
</p>
<p>vided that accurate exact line search is used. See Powell [P8] and Dennis
</p>
<p>and Moré [D15]. With inexact line search, restarting is generally required
</p>
<p>to establish global convergence.
</p>
<p>10.6 The lemma on interlocking eigenvalues is due to Loewner [L6]. An anal-
</p>
<p>ysis of the one-by-one shift of the eigenvalues to unity is contained
</p>
<p>in Fletcher [F6]. The scaling concept, including the self-scaling algo-
</p>
<p>rithm, is due to Oren and Luenberger [O5]. Also see Oren [O4]. The
</p>
<p>two-parameter class of updates defined by the scaling procedure can be
</p>
<p>shown to be equivalent to the symmetric Huang class. Oren and Spedi-
</p>
<p>cato [O6] developed a procedure for selecting the scaling parameter so
</p>
<p>as to optimize the condition number of the update.
</p>
<p>10.7 The idea of expressing conjugate gradient methods as update formulae is
</p>
<p>due to Perry [P3]. The development of the form presented here is due to
</p>
<p>Shanno [S4]. Preconditioning for conjugate gradient methods was sug-
</p>
<p>gested by Bertsekas [B9].
</p>
<p>10.8 The combined method appears in Luenberger [L10].</p>
<p/>
</div>
<div class="page"><p/>
<p>Part III
</p>
<p>Constrained Minimization</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 11
</p>
<p>Constrained Minimization Conditions
</p>
<p>We turn now, in this final part of the book, to the study of minimization problems
</p>
<p>having constraints. We begin by studying in this chapter the necessary and sufficient
</p>
<p>conditions satisfied at solution points. These conditions, aside from their intrinsic
</p>
<p>value in characterizing solutions, define Lagrange multipliers and a certain Hessian
</p>
<p>matrix which, taken together, form the foundation for both the development and
</p>
<p>analysis of algorithms presented in subsequent chapters.
</p>
<p>The general method used in this chapter to derive necessary and sufficient condi-
</p>
<p>tions is a straightforward extension of that used in Chap. 7 for unconstrained prob-
</p>
<p>lems. In the case of equality constraints, the feasible region is a curved surface
</p>
<p>embedded in En. Differential conditions satisfied at an optimal point are derived by
</p>
<p>considering the value of the objective function along curves on this surface passing
</p>
<p>through the optimal point. Thus the arguments run almost identically to those for the
</p>
<p>unconstrained case; families of curves on the constraint surface replacing the ear-
</p>
<p>lier artifice of considering feasible directions. There is also a theory of zero-order
</p>
<p>conditions that is presented in the final section of the chapter.
</p>
<p>11.1 Constraints
</p>
<p>We deal with general nonlinear programming problems of the form
</p>
<p>minimize f (x)
</p>
<p>subject to h1(x) = 0, g1(x) � 0
</p>
<p>h2(x) = 0, g2(x) � 0
...
</p>
<p>...
</p>
<p>hm(x) = 0, gp(x) � 0
</p>
<p>x &isin; Ω &sub; En,
</p>
<p>(11.1)
</p>
<p>&copy; Springer International Publishing Switzerland 2016
</p>
<p>D.G. Luenberger, Y. Ye, Linear and Nonlinear Programming, International
Series in Operations Research &amp; Management Science 228,
DOI 10.1007/978-3-319-18842-3 11
</p>
<p>321</p>
<p/>
</div>
<div class="page"><p/>
<p>322 11 Constrained Minimization Conditions
</p>
<p>where m � n and the functions f , hi, i = 1, 2, . . . ,m and g j, j = 1, 2, . . . , p are con-
</p>
<p>tinuous, and usually assumed to possess continuous second partial derivatives. For
</p>
<p>notational simplicity, we introduce the vector-valued functions h = (h1, h2, . . . , hm)
</p>
<p>and g = (g1, g2, . . . , gP) and rewrite (11.1) as
</p>
<p>minimize f (x)
</p>
<p>subject to h(x) = 0, g(x) � 0
</p>
<p>x &isin; Ω.
(11.2)
</p>
<p>The constraints h(x) = 0, g(x) � 0 are referred to as functional constraints, while
</p>
<p>the constraint x &isin; Ω is a set constraint. As before we continue to de-emphasize the
set constraint, assuming in most cases that either Ω is the whole space En or that the
</p>
<p>solution to (11.2) is in the interior of Ω. A point x &isin; Ω that satisfies all the functional
constraints is said to be feasible.
</p>
<p>A fundamental concept that provides a great deal of insight as well as simplifying
</p>
<p>the required theoretical development is that of an active constraint. An inequality
</p>
<p>constraint gi(x) � 0 is said to be active at a feasible point x if gi(x) = 0 and inactive
</p>
<p>at x if gi(x) &lt; 0. By convention we refer to any equality constraint hi(x) = 0 as active
</p>
<p>at any feasible point. The constraints active at a feasible point x restrict the domain
</p>
<p>of feasibility in neighborhoods of x, while the other, inactive constraints, have no
</p>
<p>influence in neighborhoods of x. Therefore, in studying the properties of a local
</p>
<p>minimum point, it is clear that attention can be restricted to the active constraints.
</p>
<p>This is illustrated in Fig. 11.1 where local properties satisfied by the solution x&lowast;
</p>
<p>obviously do not depend on the inactive constraints g2 and g3.
</p>
<p>It is clear that, if it were known a priori which constraints were active at the solu-
</p>
<p>tion to (11.1), the solution would be a local minimum point of the problem defined
</p>
<p>by ignoring the inactive constraints and treating all active constraints as equality
</p>
<p>constraints. Hence, with respect to local (or relative) solutions, the problem could
</p>
<p>be regarded as having equality constraints only. This observation suggests that the
</p>
<p>Fig. 11.1 Example of inactive constraints</p>
<p/>
</div>
<div class="page"><p/>
<p>11.2 Tangent Plane 323
</p>
<p>majority of insight and theory applicable to (11.1) can be derived by consideration of
</p>
<p>equality constraints alone, later making additions to account for the selection of the
</p>
<p>active constraints. This is indeed so. Therefore, in the early portion of this chapter
</p>
<p>we consider problems having only equality constraints, thereby both economizing
</p>
<p>on notation and isolating the primary ideas associated with constrained problems.
</p>
<p>We then extend these results to the more general situation.
</p>
<p>11.2 Tangent Plane
</p>
<p>A set of equality constraints on En
</p>
<p>h1(x) = 0
</p>
<p>h2(x) = 0
...
</p>
<p>hm(x) = 0
</p>
<p>(11.3)
</p>
<p>defines a subset of En which is best viewed as a hypersurface. If the constraints
</p>
<p>are everywhere regular, in a sense to be described below, this hypersurface is of
</p>
<p>dimension n &minus; m. If, as we assume in this section, the functions hi, i = 1, 2, . . . , m
belong to C1, the surface defined by them is said to be smooth.
</p>
<p>Associated with a point on a smooth surface is the tangent plane at that point,
</p>
<p>a term which in two or three dimensions has an obvious meaning. To formalize
</p>
<p>the general notion, we begin by defining curves on a surface. A curve on a surface
</p>
<p>S is a family of points x(t) &isin; S continuously parameterized by t for a � t � b.
The curve is differentiable if ẋ &equiv; (d/dt)x(t) exists, and is twice differentiable if
ẍ(t) exists. A curve x(t) is said to pass through the point x&lowast; if x&lowast; = x(t&lowast;) for some
t&lowast;, a � t&lowast; � b. The derivative of the curve at x&lowast; is, of course, defined as ẋ(t&lowast;). It is
itself a vector in En.
</p>
<p>Now consider all differentiable curves on S passing through a point x&lowast;. The tan-
gent plane at x&lowast; is defined as the collection of the derivatives at x&lowast; of all these
differentiable curves. The tangent plane is a subspace of En.
</p>
<p>For surfaces defined through a set of constraint relations such as (11.3), the prob-
</p>
<p>lem of obtaining an explicit representation for the tangent plane is a fundamental
</p>
<p>problem that we now address. Ideally, we would like to express this tangent plane
</p>
<p>in terms of derivatives of functions hi that define the surface. We introduce the sub-
</p>
<p>space
</p>
<p>M = {y : &nabla;h(x&lowast;)y = 0}
and investigate under what conditions M is equal to the tangent plane at x&lowast;. The key
concept for this purpose is that of a regular point. Figure 11.2 shows some examples
</p>
<p>where for visual clarity the tangent planes (which are sub-spaces) are translated to
</p>
<p>the point x&lowast;.</p>
<p/>
</div>
<div class="page"><p/>
<p>324 11 Constrained Minimization Conditions
</p>
<p>Fig. 11.2 Three examples of tangent planes (translated to x&lowast;)</p>
<p/>
</div>
<div class="page"><p/>
<p>11.2 Tangent Plane 325
</p>
<p>Definition. A point x&lowast; satisfying the constraint h(x&lowast;) = 0 is said to be a regular point of the
constraint if the gradient vectors &nabla;h1(x
</p>
<p>&lowast;), &nabla;h2(x
&lowast;), . . . , &nabla;hm(x
</p>
<p>&lowast;) are linearly independent.
</p>
<p>Note that if h is affine, h(x) = Ax+b, regularity is equivalent to A having rank equal
</p>
<p>to m, and this condition is independent of x.
</p>
<p>In general, at regular points it is possible to characterize the tangent plane in
</p>
<p>terms of the gradients of the constraint functions.
</p>
<p>Theorem. At a regular point x&lowast; of the surface S defined by h(x) = 0 the tangent plane is
equal to
</p>
<p>M = {y : &nabla;h(x&lowast;)y = 0}.
</p>
<p>Proof. Let T be the tangent plane at x&lowast;. It is clear that T &sub; M whether x&lowast; is regular
or not, for any curve x(t) passing through x&lowast; at t = t&lowast; having derivative ẋ(t&lowast;) such
that &nabla;h(x&lowast;)ẋ(t&lowast;) � 0 would not lie on S .
</p>
<p>To prove that M &sub; T we must show that if y &isin; M then there is a curve on
S passing through x&lowast; with derivative y. To construct such a curve we consider the
equations
</p>
<p>h(x&lowast; + ty + &nabla;h(x&lowast;)Tu(t)) = 0, (11.4)
</p>
<p>where for fixed t we consider u(t) &isin; Em to be the unknown. This is a nonlinear
system of m equations and m unknowns, parameterized continuously, by t. At t = 0
</p>
<p>there is a solution u(0) = 0. The Jacobian matrix of the system with respect to u at
</p>
<p>t = 0 is the m &times; m matrix
&nabla;h(x&lowast;)&nabla;h(x&lowast;)T ,
</p>
<p>which is nonsingular, since &nabla;h(x&lowast;) is of full rank if x&lowast; is a regular point. Thus, by the
Implicit Function Theorem (see Appendix A) there is a continuously differentiable
</p>
<p>solution u(t) in some region &minus;a � t � a.
The curve x(t) = x&lowast; + ty+&nabla;h(x&lowast;)Tu(t) is thus, by construction, a curve on S . By
</p>
<p>differentiating the system (11.4) with respect to t at t = 0 we obtain
</p>
<p>0 =
d
</p>
<p>dt
h(x(t))
</p>
<p>]
</p>
<p>t=0
</p>
<p>= &nabla;h(x&lowast;)y + &nabla;h(x&lowast;)&nabla;h(x&lowast;)T u̇(0).
</p>
<p>By definition of y we have &nabla;h(x&lowast;)y = 0 and thus, again since &nabla;h(x&lowast;)&nabla;h(x&lowast;)T is
nonsingular, we conclude that ẋ(0) = 0. Therefore
</p>
<p>ẋ(0) = y + &nabla;h(x&lowast;)T ẋ(0) = y,
</p>
<p>and the constructed curve has derivative y at x&lowast;. �
</p>
<p>It is important to recognize that the condition of being a regular point is not a
</p>
<p>condition on the constraint surface itself but on its representation in terms of an h.
</p>
<p>The tangent plane is defined independently of the representation, while M is not.
</p>
<p>Example. In E2 let h(x1, x2) = x1. Then h(x) = 0 yields the x2 axis, and every point
</p>
<p>on that axis is regular. If instead we put h(x1, x2) = x
2
1
, again S is the x2 axis but
</p>
<p>now no point on the axis is regular. Indeed in this case M = E2, while the tangent
</p>
<p>plane is the x2 axis.</p>
<p/>
</div>
<div class="page"><p/>
<p>326 11 Constrained Minimization Conditions
</p>
<p>11.3 First-Order Necessary Conditions (Equality Constraints)
</p>
<p>The derivation of necessary and sufficient conditions for a point to be a local min-
</p>
<p>imum point subject to equality constraints is fairly simple now that the representa-
</p>
<p>tion of the tangent plane is known. We begin by deriving the first-order necessary
</p>
<p>conditions.
</p>
<p>Lemma. Let x&lowast; be a regular point of the constraints h(x) = 0 and a local extremum
point (a minimum or maximum) of f subject to these constraints.
</p>
<p>Then all y &isin; En satisfying
&nabla;h(x&lowast;)y = 0 (11.5)
</p>
<p>must also satisfy
</p>
<p>&nabla; f (x&lowast;)y = 0. (11.6)
</p>
<p>Proof. Let y be any vector in the tangent plane at x&lowast; and let x(t) be any smooth
curve on the constraint surface passing through x&lowast; with derivative y at x&lowast;; that is,
x(0) = x&lowast;, ẋ(0) = y, and h(x(t)) = 0 for &minus;a � t � a for some a &gt; 0.
</p>
<p>Since x&lowast; is a regular point, the tangent plane is identical with the set of y&rsquo;s sat-
isfying &nabla;h(x&lowast;)y = 0. Then, since x&lowast; is a constrained local extremum point of f , we
have
</p>
<p>d
</p>
<p>dt
f (x(t))
</p>
<p>]
</p>
<p>t=0
</p>
<p>= 0,
</p>
<p>or equivalently,
</p>
<p>&nabla; f (x&lowast;)y = 0. �
</p>
<p>The above Lemma says that &nabla; f (x&lowast;) is orthogonal to the tangent plane. Next we
conclude that this implies that &nabla; f (x&lowast;) is a linear combination of the gradients of h
at x&lowast;, a relation that leads to the introduction of Lagrange multipliers. As in much
of nonlinear programming, the Lagrange multiplier vector is often labeled λ rather
</p>
<p>than y in linear programming, and this convention is followed here.
</p>
<p>Theorem. Let x&lowast; be a local extremum point of f subject to the constraints h(x) = 0. Assume
further that x&lowast; is a regular point of these constraints. Then there is a λ &isin; Em such that
</p>
<p>&nabla; f (x&lowast;) + λT&nabla;h(x&lowast;) = 0. (11.7)
</p>
<p>Proof. From the Lemma we may conclude that the value of the linear program
</p>
<p>maximize &nabla; f (x&lowast;)y
</p>
<p>subject to &nabla;h(x&lowast;)y = 0
</p>
<p>is zero. Thus, by the Duality Theorem of linear programming (Sect. 4.2) the dual
</p>
<p>problem is feasible. Specifically, there is λ &isin; Em such that &nabla; f (x&lowast;) + λT&nabla;h(x&lowast;)
= 0. �</p>
<p/>
</div>
<div class="page"><p/>
<p>11.4 Examples 327
</p>
<p>It should be noted that the first-order necessary conditions
</p>
<p>&nabla; f (x&lowast;) + λT&nabla;h(x&lowast;) = 0
</p>
<p>together with the constraints
</p>
<p>h(x&lowast;) = 0
</p>
<p>give a total of n+m (generally nonlinear) equations in the n+m variables comprising
</p>
<p>x&lowast;, λ. Thus the necessary conditions are a complete set since, at least locally, they
determine a unique solution.
</p>
<p>It is convenient to introduce the Lagrangian associated with the constrained prob-
</p>
<p>lem, defined as
</p>
<p>l(x, λ) = f (x) + λTh(x). (11.8)
</p>
<p>The necessary conditions can then be expressed in the form
</p>
<p>&nabla;xl(x, λ) = 0 (11.9)
</p>
<p>&nabla;λl(x, λ) = 0, (11.10)
</p>
<p>the second of these being simply a restatement of the constraints.
</p>
<p>11.4 Examples
</p>
<p>We digress briefly from our mathematical development to consider some examples
</p>
<p>of constrained optimization problems. We present five simple examples that can
</p>
<p>be treated explicitly in a short space and then briefly discuss a broader range of
</p>
<p>applications.
</p>
<p>Example 1. Consider the problem
</p>
<p>minimize x1x2 + x2x3 + x1x3
</p>
<p>subject to x1 + x2 + x3 = 3.
</p>
<p>The necessary conditions become
</p>
<p>x2 + x3 + λ = 0
</p>
<p>x1 + x3 + λ = 0
</p>
<p>x1 + x2 + λ = 0.
</p>
<p>These three equations together with the one constraint equation give four equations
</p>
<p>that can be solved for the four unknowns x1, x2, x3, λ. Solution yields x1 = x2 =
</p>
<p>x3 = 1, λ = &minus;2.
</p>
<p>Example 2 (Maximum Volume). Let us consider an example of the type that is now
</p>
<p>standard in textbooks and which has a structure similar to that of the example above.</p>
<p/>
</div>
<div class="page"><p/>
<p>328 11 Constrained Minimization Conditions
</p>
<p>We seek to construct a cardboard box of maximum volume, given a fixed area of
</p>
<p>cardboard.
</p>
<p>Denoting the dimensions of the box by x, y, z, the problem can be expressed as
</p>
<p>maximize xyz
</p>
<p>subject to (xy + yz + xz) =
c
</p>
<p>2
, (11.11)
</p>
<p>where c &gt; 0 is the given area of cardboard. Introducing a Lagrange multiplier, the
</p>
<p>first-order necessary conditions are easily found to be
</p>
<p>yz + λ(y + z) = 0
</p>
<p>xz + λ(x + z) = 0 (11.12)
</p>
<p>xy + λ(x + y) = 0
</p>
<p>together with the constraint. Before solving these, let us note that the sum of these
</p>
<p>equations is (xy + yz + xz) + 2λ(x + y + z) = 0. Using the constraint this becomes
</p>
<p>c/2 + 2λ(x + y + z) = 0. From this it is clear that λ � 0. Now we can show that
</p>
<p>x, y, and z are nonzero. This follows because x = 0 implies z = 0 from the second
</p>
<p>equation and y = 0 from the third equation. In a similar way, it is seen that if either
</p>
<p>x, y, or z are zero, all must be zero, which is impossible.
</p>
<p>To solve the equations, multiply the first by x and the second by y, and then
</p>
<p>subtract the two to obtain
</p>
<p>λ(x &minus; y)z = 0.
Operate similarly on the second and third to obtain
</p>
<p>λ(y &minus; z)x = 0.
</p>
<p>Since no variables can be zero, it follows that x = y = z =
&radic;
c/6 is the unique
</p>
<p>solution to the necessary conditions. The box must be a cube.
</p>
<p>Example 3 (Entropy). optimization problems often describe natural phenomena. An
</p>
<p>example is the characterization of naturally occurring probability distributions as
</p>
<p>maximum entropy distributions.
</p>
<p>As a specific example consider a discrete probability density corresponding to a
</p>
<p>measured value taking one of n values x1, x2, . . . , xn. The probability associated
</p>
<p>with xi is pi. The pi&rsquo;s satisfy pi � 0 and
n
&sum;
</p>
<p>i=1
pi = 1.
</p>
<p>The entropy of such a density is
</p>
<p>ε = &minus;
n
&sum;
</p>
<p>i=1
</p>
<p>pi log(pi).
</p>
<p>The mean value of the density is
n
&sum;
</p>
<p>i=1
xipi.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.4 Examples 329
</p>
<p>If the value of mean is known to be m (by the physical situation), the maximum
</p>
<p>entropy argument suggests that the density should be taken as that which solves the
</p>
<p>following problem:
</p>
<p>maximize &minus;
n
&sum;
</p>
<p>i=1
</p>
<p>pi log(pi)
</p>
<p>subject to
</p>
<p>n
&sum;
</p>
<p>i=1
</p>
<p>pi = 1 (11.13)
</p>
<p>n
&sum;
</p>
<p>i=1
</p>
<p>xipi = m
</p>
<p>pi &ge; 0, i = 1, 2, . . . , n.
</p>
<p>We begin by ignoring the nonnegativity constraints, believing that they may be
</p>
<p>inactive. Introducing two Lagrange multipliers, λ and μ, the Lagrangian is
</p>
<p>l =
</p>
<p>n
&sum;
</p>
<p>i=1
</p>
<p>{&minus;pi log pi + λpi + μxipi} &minus; λ &minus; μm.
</p>
<p>The necessary conditions are immediately found to be
</p>
<p>&minus; log pi &minus; 1 + λ + μxi = 0, i = 1, 2, . . . , n.
</p>
<p>This leads to
</p>
<p>pi = exp{(λ &minus; 1) + μxi}, i = 1, 2, . . . , n. (11.14)
We note that pi &gt; 0, so the nonnegativity constraints are indeed inactive. The re-
</p>
<p>sult (11.14) is known as an exponential density. The Lagrange multipliers λ and μ
</p>
<p>are parameters that must be selected so that the two equality constraints are satisfied.
</p>
<p>Example 4 (Hanging Chain). A chain is suspended from two thin hooks that are
</p>
<p>16 ft apart on a horizontal line as shown in Fig. 11.3. The chain itself consists of
</p>
<p>20 links of stiff steel. Each link is one foot in length (measured inside). We wish to
</p>
<p>formulate the problem to determine the equilibrium shape of the chain.
</p>
<p>The solution can be found by minimizing the potential energy of the chain. Let
</p>
<p>us number the links consecutively from 1 to 20 starting with the left end. We let link
</p>
<p>i span an x distance of xi and a y distance of yi. Then x
2
i
+ y2
</p>
<p>i
= 1. The potential
</p>
<p>energy of a link is its weight times its vertical height (from some reference). The
</p>
<p>potential energy of the chain is the sum of the potential energies of each link. We
</p>
<p>may take the top of the chain as reference and assume that the mass of each link is
</p>
<p>concentrated at its center. Assuming unit weight, the potential energy is then</p>
<p/>
</div>
<div class="page"><p/>
<p>330 11 Constrained Minimization Conditions
</p>
<p>Fig. 11.3 A hanging chain
</p>
<p>1
</p>
<p>2
y1 +
</p>
<p>(
</p>
<p>y1 +
1
</p>
<p>2
y2
</p>
<p>)
</p>
<p>+
</p>
<p>(
</p>
<p>y1 + y2 +
1
</p>
<p>2
y3
</p>
<p>)
</p>
<p>+ &middot; &middot; &middot;
</p>
<p>+
</p>
<p>(
</p>
<p>y1 + y2 + &middot; &middot; &middot; + yn&minus;1 +
1
</p>
<p>2
yn
</p>
<p>)
</p>
<p>=
</p>
<p>n
&sum;
</p>
<p>i=1
</p>
<p>(
</p>
<p>n &minus; i + 1
2
</p>
<p>)
</p>
<p>yi,
</p>
<p>where n = 20 in our example.
</p>
<p>The chain is subject to two constraints: The total y displacement is zero, and the
</p>
<p>total x displacement is 16. Thus the equilibrium shape is the solution of
</p>
<p>minimize
</p>
<p>n
&sum;
</p>
<p>i=1
</p>
<p>(
</p>
<p>n &minus; i + 1
2
</p>
<p>)
</p>
<p>yi
</p>
<p>subject to
</p>
<p>n
&sum;
</p>
<p>i=1
</p>
<p>yi = 0 (11.15)
</p>
<p>n
&sum;
</p>
<p>i=1
</p>
<p>&radic;
</p>
<p>1 &minus; y2
i
= 16.
</p>
<p>The first-order necessary conditions are
</p>
<p>(
</p>
<p>n &minus; i + 1
2
</p>
<p>)
</p>
<p>+ λ &minus; μyi&radic;
1 &minus; y2
</p>
<p>i
</p>
<p>= 0 (11.16)
</p>
<p>for i = 1, 2, . . . , n. This leads directly to
</p>
<p>yi = &minus;
n &minus; i + 1
</p>
<p>2
+ λ
</p>
<p>&radic;
</p>
<p>μ2 +
(
</p>
<p>n &minus; i + 1
2
+ λ
</p>
<p>)2
. (11.17)
</p>
<p>As in Example 2 the solution is determined once the Lagrange multipliers are
</p>
<p>known. They must be selected so that the solution satisfies the two constraints.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.4 Examples 331
</p>
<p>It is useful to point out that problems of this type may have local minimum points.
</p>
<p>The reader can examine this by considering a short chain of, say, four links and v
</p>
<p>and w configurations.
</p>
<p>Example 5 (Portfolio Design). Suppose there are n securities indexed by i = 1, 2,
</p>
<p>. . . , n. Each security i is characterized by its random rate of return ri which has
</p>
<p>mean value r̄i. Its covariances with the rates of return of other securities are σij, for
</p>
<p>j = 1, 2, . . . , n. The portfolio problem is to allocate total available wealth among
</p>
<p>these n securities, allocating a fraction wi of wealth to the security i.
</p>
<p>The overall rate of return of a portfolio is r =
&sum;n
</p>
<p>i=1 wir̄i and variance σ
2 =
</p>
<p>&sum;n
i, j=1
</p>
<p>wiσijw j.
</p>
<p>Markowitz introduced the concept of devising efficient portfolios which for a
</p>
<p>given expected rate of return r̄ have minimum possible variance. Such a portfolio is
</p>
<p>the solution to the problem
</p>
<p>min
wi,w2,...wn
</p>
<p>&sum;n
</p>
<p>i, j=1
wiσijw j
</p>
<p>subject to
&sum;n
</p>
<p>i=1
wir̄i = r̄
</p>
<p>&sum;n
</p>
<p>i=1
wi = 1.
</p>
<p>The second constraint forces the sum of the weights to equal one. There may be the
</p>
<p>further restriction that each wi &ge; 0 which would imply that the securities must not
be shorted (that is, sold short).
</p>
<p>Introducing Lagrange multipliers λ and μ for the two constraints leads easily to
</p>
<p>the n + 2 linear equations
</p>
<p>n
&sum;
</p>
<p>j=1
</p>
<p>σijw j + λr̄i + μ = 0 for i = 1, 2, . . . , n
</p>
<p>n
&sum;
</p>
<p>i=1
</p>
<p>wir̄i = r̄
</p>
<p>n
&sum;
</p>
<p>i=1
</p>
<p>wi = 1
</p>
<p>in the n + 2 unknowns (the wi&rsquo;s, λ and μ).
</p>
<p>Large-Scale Applications
</p>
<p>The problems that serve as the primary motivation for the methods described in
</p>
<p>this part of the book are actually somewhat different in character than the prob-
</p>
<p>lems represented by the above examples, which by necessity are quite simple.
</p>
<p>Larger, more complex, nonlinear programming problems arise frequently in modern</p>
<p/>
</div>
<div class="page"><p/>
<p>332 11 Constrained Minimization Conditions
</p>
<p>applied analysis in a wide variety of disciplines. Indeed, within the past few decades
</p>
<p>nonlinear programming has advanced from a relatively young and primarily analytic
</p>
<p>subject to a substantial general tool for problem solving.
</p>
<p>Large nonlinear programming problems arise in problems of mechanical struc-
</p>
<p>tures, such as determining optimal configurations for bridges, trusses, and so forth.
</p>
<p>Some mechanical designs and configurations that in the past were found by solving
</p>
<p>differential equations are now often found by solving suitable optimization prob-
</p>
<p>lems. An example that is somewhat similar to the hanging chain problem is the
</p>
<p>determination of the shape of a stiff cable suspended between two points and sup-
</p>
<p>porting a load.
</p>
<p>A wide assortment, of large-scale optimization problems arise in a similar way as
</p>
<p>methods for solving partial differential equations. In situations where the underlying
</p>
<p>continuous variables are defined over a two- or three-dimensional region, the con-
</p>
<p>tinuous region is replaced by a grid consisting of perhaps several thousand discrete
</p>
<p>points. The corresponding discrete approximation to the partial differential equation
</p>
<p>is then solved indirectly by formulating an equivalent optimization problem. This
</p>
<p>approach is used in studies of plasticity, in heat equations, in the flow of fluids, in
</p>
<p>atomic physics, and indeed in almost all branches of physical science.
</p>
<p>Problems of optimal control lead to large-scale nonlinear programming prob-
</p>
<p>lems. In these problems a dynamic system, often described by an ordinary differ-
</p>
<p>ential equation, relates control variables to a trajectory of the system state. This
</p>
<p>differential equation, or a discretized version of it, defines one set of constraints.
</p>
<p>The problem is to select the control variables so that the resulting trajectory satisfies
</p>
<p>various additional constraints and minimizes some criterion. An early example of
</p>
<p>such a problem that was solved numerically was the determination of the trajectory
</p>
<p>of a rocket to the moon that required the minimum fuel consumption.
</p>
<p>There are many examples of nonlinear programming in industrial operations and
</p>
<p>business decision making. Many of these are nonlinear versions of the kinds of ex-
</p>
<p>amples that were discussed in the linear programming part of the book. Nonlinear-
</p>
<p>ities can arise in production functions, cost curves, and, in fact, in almost all facets
</p>
<p>of problem formulation.
</p>
<p>Portfolio analysis, in the context of both stock market investment and evaluation
</p>
<p>of a complex project within a firm, is an area where nonlinear programming is be-
</p>
<p>coming increasingly useful. These problems can easily have thousands of variables.
</p>
<p>In many areas of model building and analysis, optimization formulations are
</p>
<p>increasingly replacing the direct formulation of systems of equations. Thus large
</p>
<p>economic forecasting models often determine equilibrium prices by minimizing an
</p>
<p>objective termed consumer surplus. Physical models are often formulated as mini-
</p>
<p>mization of energy. Decision problems are formulated as maximizing expected util-
</p>
<p>ity. Data analysis procedures are based on minimizing an average error or maxi-
</p>
<p>mizing a probability. As the methodology for solution of nonlinear programming
</p>
<p>improves, one can expect that this trend will continue.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.5 Second-Order Conditions 333
</p>
<p>11.5 Second-Order Conditions
</p>
<p>By an argument analogous to that used for the unconstrained case, we can also derive
</p>
<p>the corresponding second-order conditions for constrained problems. Throughout
</p>
<p>this section it is assumed that f , h &isin; C2.
Second-Order Necessary Conditions. Suppose that x&lowast; is a local minimum of f subject to
h(x) = 0 and that x&lowast; is a regular point of these constraints. Then there is a λ &isin; Em such that
</p>
<p>&nabla; f (x&lowast;) + λT&nabla;h(x&lowast;) = 0. (11.18)
</p>
<p>If we denote by M the tangent plane M = {y : &nabla;h(x&lowast;)y = 0}, then the matrix
</p>
<p>L(x&lowast;) = F(x&lowast;) + λTH(x&lowast;) (11.19)
</p>
<p>is positive semidefinite on M, that is, yTL(x&lowast;)y � 0 for all y &isin; M.
</p>
<p>Proof. From elementary calculus it is clear that for every twice differentiable curve
</p>
<p>on the constraint surface S through x&lowast; (with x(0) = x&lowast;) we have
</p>
<p>d2
</p>
<p>dt2
f (x(t))
</p>
<p>]
</p>
<p>t=0
</p>
<p>� 0. (11.20)
</p>
<p>By definition
</p>
<p>d2
</p>
<p>dt2
f (x(t))
</p>
<p>]
</p>
<p>t=0
</p>
<p>= ẋ(0)TF(x&lowast;)ẋ(0) + &nabla; f (x&lowast;)ẍ(0). (11.21)
</p>
<p>Furthermore, differentiating the relation λTh(x(t)) = 0 twice, we obtain
</p>
<p>ẋ(0)TλTH(x&lowast;)ẋ(0) + λT&nabla;h(x&lowast;)ẍ(0) = 0. (11.22)
</p>
<p>Adding (11.22) to (11.21), while taking account of (11.20), yields the result
</p>
<p>d2
</p>
<p>dt2
f (x(t))
</p>
<p>]
</p>
<p>t=0
</p>
<p>= ẋ(0)TL(x&lowast;)ẋ(0) � 0.
</p>
<p>Since ẋ(0) is arbitrary in M, we immediately have the stated conclusion. �
</p>
<p>The above theorem is our first encounter with the matrix L = F + λTH which
</p>
<p>is the matrix of second partial derivatives, with respect to x, of the Lagrangian l.
</p>
<p>(See Appendix A, Sect. A.6, for a discussion of the notation λTH used here.) This
</p>
<p>matrix is the backbone of the theory of algorithms for constrained problems, and it
</p>
<p>is encountered often in subsequent chapters.
</p>
<p>We next state the corresponding set of sufficient conditions.
</p>
<p>Second-Order Sufficiency Conditions. Suppose there is a point x&lowast; satisfying h(x&lowast;) = 0, and
a λ &isin; Em such that
</p>
<p>&nabla; f (x&lowast;) + λT&nabla;h(x&lowast;) = 0. (11.23)</p>
<p/>
</div>
<div class="page"><p/>
<p>334 11 Constrained Minimization Conditions
</p>
<p>Suppose also that the matrix L(x&lowast;) = F(x&lowast;) + λTH(x&lowast;) is positive definite on M = {y :
&nabla;h(x&lowast;)y = 0}, that is, for y &isin; M, y � 0 there holds yTL(x&lowast;)y &gt; 0. Then x&lowast; is a strict local
minimum of f subject to h(x) = 0.
</p>
<p>Proof. If x&lowast; is not a strict relative minimum point, there exists a sequence of feasible
points {yk} converging to x&lowast; such that for each k, f (yk) � f (x&lowast;). Write each yk in
the form yk = x
</p>
<p>&lowast; + δksk where sk &isin; En, |sk | = 1, and δk &gt; 0 for each k. Clearly,
δk &rarr; 0 and the sequence {sk}, being bounded, must have a convergent subsequence
converging to some s&lowast;. For convenience of notation, we assume that the sequence
{sk} is itself convergent to s&lowast;. We also have h(yk)&minus;h(x&lowast;) = 0, and dividing by δk and
letting k &rarr; &infin; we see that &nabla;h(x&lowast;)s&lowast; = 0.
</p>
<p>Now by Taylor&rsquo;s theorem, we have for each j
</p>
<p>0 = h j(yk) = h j(x
&lowast;) + δk&nabla;h j(x
</p>
<p>&lowast;)sk +
δ2
k
</p>
<p>2
sTk&nabla;
</p>
<p>2h j(η j)sk (11.24)
</p>
<p>and
</p>
<p>0 � f (yk) &minus; f (x&lowast;) = δk&nabla; f (x&lowast;)sk +
δ2
k
</p>
<p>2
sTk&nabla;
</p>
<p>2 f (η0)sk, (11.25)
</p>
<p>where each η j is a point on the line segment joining x
&lowast; and yk. Multiplying (11.24)
</p>
<p>by λ j and adding these to (11.25) we obtain, on accounting for (11.23),
</p>
<p>0 �
δ2
k
</p>
<p>2
sTk
</p>
<p>⎧
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>&nabla;
2 f (η0) +
</p>
<p>m
&sum;
</p>
<p>i=1
</p>
<p>λi&nabla;
2hi(ηi)
</p>
<p>⎫
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎬
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎭
</p>
<p>sk,
</p>
<p>which yields a contradiction as k &rarr; &infin;. �
</p>
<p>Example 1. Consider the problem
</p>
<p>maximize x1x2 + x2x3 + x1x3
</p>
<p>subject to x1 + x2 + x3 = 3.
</p>
<p>In Example 1 of Sect. 11.4 it was found that x1 = x2 = x3 = 1, λ = &minus;2 satisfy the
first-order conditions. The matrix F + λTH becomes in this case
</p>
<p>L =
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>0 1 1
</p>
<p>1 0 1
</p>
<p>1 1 0
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>,
</p>
<p>which itself is neither positive nor negative definite. On the subspace M = {y: y1 +
y2 + y3 = 0}, however, we note that
</p>
<p>yTLy = y1(y2 + y3) + y2(y1 + y3) + y3(y1 + y2)
</p>
<p>= &minus;(y21 + y22 + y23),
</p>
<p>and thus L is negative definite on M. Therefore, the solution we found is at least a
</p>
<p>local maximum.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.6 Eigenvalues in Tangent Subspace 335
</p>
<p>11.6 Eigenvalues in Tangent Subspace
</p>
<p>In the last section it was shown that the matrix L restricted to the subspace M that
</p>
<p>is tangent to the constraint surface plays a role in second-order conditions entirely
</p>
<p>analogous to that of the Hessian of the objective function in the unconstrained case.
</p>
<p>It is perhaps not surprising, in view of this, that the structure of L restricted to M
</p>
<p>also determines rates of convergence of algorithms designed for constrained prob-
</p>
<p>lems in the same way that the structure of the Hessian of the objective function
</p>
<p>does for unconstrained algorithms. Indeed, we shall see that the eigenvalues of L
</p>
<p>restricted to M determine the natural rates of convergence for algorithms designed
</p>
<p>for constrained problems. It is important, therefore, to understand what these re-
</p>
<p>stricted eigenvalues represent. We first determine geometrically what we mean by
</p>
<p>the restriction of L to M which we denote by LM . Next we define the eigenvalues of
</p>
<p>the operator LM . Finally we indicate how these various quantities can be computed.
</p>
<p>Given any vector y &isin; M, the vector Ly is in En but not necessarily in M. We
project Ly orthogonally back onto M, as shown in Fig. 11.4, and the result is said to
</p>
<p>be the restriction of L to M operating on y. In this way we obtain a linear transforma-
</p>
<p>tion from M to M. The transformation is determined somewhat implicitly, however,
</p>
<p>since we do not have an explicit matrix representation.
</p>
<p>Fig. 11.4 Definition of LM
</p>
<p>A vector y &isin; M is an eigenvector of LM if there is a real number λ such that
LMy = λy; the corresponding λ is an eigenvalue of LM . This coincides with the
</p>
<p>standard definition. In terms of L we see that y is an eigenvector of LM if Ly can be
</p>
<p>written as the sum of λy and a vector orthogonal to M. See Fig. 11.5.
</p>
<p>To obtain a matrix representation for LM it is necessary to introduce a basis in
</p>
<p>the subspace M. For simplicity it is best to introduce an orthonormal basis, say
</p>
<p>e1, e2, . . . , en&minus;m. Define the matrix E to be the n &times; (n &minus; m) matrix whose columns
consist of the vectors ei. Then any vector y in M can be written as y = Ez for some
</p>
<p>z &isin; En&minus;m and, of course, LEz represents the action of L on such a vector. To project
this result back into M and express the result in terms of the basis e1, e2, . . . , en&minus;m,</p>
<p/>
</div>
<div class="page"><p/>
<p>336 11 Constrained Minimization Conditions
</p>
<p>Fig. 11.5 Eigenvector of LM
</p>
<p>we merely multiply by ET . Thus ETLEz is the vector whose components give the
</p>
<p>representation in terms of the basis; and, correspondingly, the (n &minus; m) &times; (n &minus; m)
matrix ETLE is the matrix representation of L restricted to M.
</p>
<p>The eigenvalues of L restricted to M can be found by determining the eigenvalues
</p>
<p>of ETLE. These eigenvalues are independent of the particular orthonormal basis E.
</p>
<p>Example 1. In the last section we considered
</p>
<p>L =
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>0 1 1
</p>
<p>1 0 1
</p>
<p>1 1 0
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>restricted to M = {y : y1 + y2 + y3 = 0}. To obtain an explicit matrix representation
on M let us introduce the orthonormal basis:
</p>
<p>e1 =
1&radic;
2
</p>
<p>(1, 0, &minus;1)
</p>
<p>e2 =
1&radic;
6
</p>
<p>(1, &minus;2, 1).
</p>
<p>This gives, upon expansion,
</p>
<p>ETLE =
</p>
<p>[
</p>
<p>&minus;1 0
0 &minus;1
</p>
<p>]
</p>
<p>,
</p>
<p>and hence L restricted to M acts like the negative of the identity.
</p>
<p>Example 2. Let us consider the problem
</p>
<p>extremize x1 + x
2
2 + x2x3 + 2x
</p>
<p>2
3
</p>
<p>subject to
1
</p>
<p>2
(x21 + x
</p>
<p>2
2 + x
</p>
<p>2
3) = 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.6 Eigenvalues in Tangent Subspace 337
</p>
<p>The first-order necessary conditions are
</p>
<p>1 + λx1 = 0
</p>
<p>2x2 + x3 + λx2 = 0
</p>
<p>x2 + 4x3 + λx3 = 0.
</p>
<p>One solution to this set is easily seen to be x1 = 1, x2 = 0, x3 = 0, λ = &minus;1. Let us
examine the second-order conditions at this solution point. The Lagrangian matrix
</p>
<p>there is
</p>
<p>L =
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>&minus;1 0 0
0 1 1
</p>
<p>0 1 3
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>,
</p>
<p>and the corresponding subspace M is
</p>
<p>M = {y : y1 = 0}.
</p>
<p>In this case M is the subspace spanned by the second two basis vectors in E3 and
</p>
<p>hence the restriction of L to M can be found by taking the corresponding submatrix
</p>
<p>of L. Thus, in this case,
</p>
<p>ETLE =
</p>
<p>[
</p>
<p>1 1
</p>
<p>1 3
</p>
<p>]
</p>
<p>.
</p>
<p>The characteristic polynomial of this matrix is
</p>
<p>det
</p>
<p>[
</p>
<p>1 &minus; λ 1
1 3 &minus; λ
</p>
<p>]
</p>
<p>= (1 &minus; λ)(3 &minus; λ) &minus; 1 = λ2 &minus; 4λ + 2.
</p>
<p>The eigenvalues of LM are thus λ = 2 &plusmn;
&radic;
</p>
<p>2, and LM is positive definite.
</p>
<p>Since the LM matrix is positive definite, we conclude that the point found is a
</p>
<p>relative minimum point. This example illustrates that, in general, the restriction of
</p>
<p>L to M can be thought of as a submatrix of L, although it can be read directly from
</p>
<p>the original matrix only if the subspace M is spanned by a subset of the original
</p>
<p>basis vectors.
</p>
<p>Projected Hessians
</p>
<p>The above approach for determining the eigenvalues of L projected onto M is quite
</p>
<p>direct and relatively simple. There is another approach, however, that is useful in
</p>
<p>some theoretical arguments and convenient for simple applications. It is based on
</p>
<p>constructing matrices and determinants of order n rather than n &minus; m, but there is
no need to find the orthonormal basis E. For simplicity, let A = &nabla;h which has full
</p>
<p>row rank.</p>
<p/>
</div>
<div class="page"><p/>
<p>338 11 Constrained Minimization Conditions
</p>
<p>Any x satisfying Ax = 0 can be expressed as
</p>
<p>x = (I &minus; AT (AAT )&minus;1A)z
</p>
<p>for some z (and the converse is also true), where PA = (I &minus; AT (AAT )&minus;1A) is the so
called projection matrix to the null space of A (that is, M). If xTLx &ge; 0 for all x in
this null space, then zTPALPAz &ge; 0 for all z &isin; En, or the n-dimensional symmetric
matrix PALPA is positive semidefinite. Furthermore, if PALPA has rank n &minus; m, then
LM is positive definite, which results the following test.
</p>
<p>Projected Hessian Test. The matrix L is positive definite on the subspace M = {x : &nabla;hx = 0}
if and only if the projected Hessian matrix to the null space of &nabla;h is positive semidefinite
</p>
<p>and has rank n &minus;m.
</p>
<p>Example 3. Approaching Example 2 in this way and noting A = &nabla;h = (1, 0, 0) we
</p>
<p>have
</p>
<p>PA = I &minus;
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>1
</p>
<p>0
</p>
<p>0
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>1
</p>
<p>0
</p>
<p>0
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>T
</p>
<p>=
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>0 0 0
</p>
<p>0 1 0
</p>
<p>0 0 1
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>.
</p>
<p>Then
</p>
<p>PALPA =
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>0 0 0
</p>
<p>0 1 1
</p>
<p>0 1 3
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>which is clearly positive semidefinite and has rank 2.
</p>
<p>11.7 Sensitivity
</p>
<p>The Lagrange multipliers associated with a constrained minimization problem have
</p>
<p>an interpretation as prices, similar to the prices associated with constraints in linear
</p>
<p>programming. In the nonlinear case the multipliers are associated with the particu-
</p>
<p>lar solution point and correspond to incremental or marginal prices, that is, prices
</p>
<p>associated with small variations in the constraint requirements.
</p>
<p>Suppose the problem
</p>
<p>minimize f (x)
</p>
<p>subject to h(x) = 0 (11.26)
</p>
<p>has a solution at the point x&lowast; which is a regular point of the constraints. Let λ be the
corresponding Lagrange multiplier vector. Now consider the family of problems
</p>
<p>minimize f (x)
</p>
<p>subject to h(x) = c, (11.27)
</p>
<p>where c &isin; Em. For a sufficiently small range of c near the zero vector, the problem
will have a solution point x(c) near x(0) &equiv; x&lowast;. For each of these solutions there is a</p>
<p/>
</div>
<div class="page"><p/>
<p>11.7 Sensitivity 339
</p>
<p>corresponding value f (x(c)), and this value can be regarded as a function of c, the
</p>
<p>right-hand side of the constraints. The components of the gradient of this function
</p>
<p>can be interpreted as the incremental rate of change in value per unit change in
</p>
<p>the constraint requirements. Thus, they are the incremental prices of the constraint
</p>
<p>requirements measured in units of the objective. We show below how these prices
</p>
<p>are related to the Lagrange multipliers of the problem having c = 0.
</p>
<p>Sensitivity Theorem. Let f , h &isin; C2 and consider the family of problems
</p>
<p>minimize f (x)
</p>
<p>subject to h(x) = c. (11.29)
</p>
<p>Suppose for c = 0 there is a local solution x&lowast; that is a regular point and that, together with
its associated Lagrange multiplier vector λ, satisfies the second-order sufficiency conditions
for a strict local minimum. Then for every c &isin; Emin a region containing 0 there is an x(c),
depending continuously on c, such that x(0) = x&lowast; and such that x(c) is a local minimum
of (11.27). Furthermore,
</p>
<p>&nabla;c f (x(c))
]
</p>
<p>c=0 = &minus;λT .
</p>
<p>Proof. Consider the system of equations
</p>
<p>&nabla; f (x) + λT&nabla;h(x) = 0 (11.30)
</p>
<p>h(x) = c. (11.31)
</p>
<p>By hypothesis, there is a solution x&lowast;, λ to this system when c = 0. The Jacobian
matrix of the system at this solution is
</p>
<p>[
</p>
<p>L(x&lowast;) &nabla;h(x&lowast;)T
</p>
<p>&nabla;h(x&lowast;) 0
</p>
<p>]
</p>
<p>.
</p>
<p>Because by assumption x&lowast; is a regular point and L(x&lowast;) is positive definite on M,
it follows that this matrix is nonsingular (see Exercise 11). Thus, by the Implicit
</p>
<p>Function Theorem, there is a solution x(c), λ(c) to the system which is in fact con-
</p>
<p>tinuously differentiable.
</p>
<p>By the chain rule we have
</p>
<p>&nabla;c f (x(c))
]
</p>
<p>c=0 = &nabla;x f (x
&lowast;)&nabla;cx(0).
</p>
<p>and
</p>
<p>&nabla;ch(x(c))]c=0 = &nabla;xh(x
&lowast;)&nabla;cx(0).
</p>
<p>In view of (11.31), the second of these is equal to the identity I on Em, while this, in
</p>
<p>view of (11.30), implies that the first can be written
</p>
<p>&nabla;c f (x(c))
]
</p>
<p>c=0 = &minus;λT . �</p>
<p/>
</div>
<div class="page"><p/>
<p>340 11 Constrained Minimization Conditions
</p>
<p>11.8 Inequality Constraints
</p>
<p>We consider now problems of the form
</p>
<p>minimize f (x)
</p>
<p>subject to h(x) = 0, g(x) � 0. (11.32)
</p>
<p>We assume that f and h are as before and that g is a p-dimensional function. Initially,
</p>
<p>we assume f , h, g &isin; C1.
There are a number of distinct theories concerning this problem, based on various
</p>
<p>regularity conditions or constraint qualifications, which are directed toward obtain-
</p>
<p>ing definitive general statements of necessary and sufficient conditions. One can by
</p>
<p>no means pretend that all such results can be obtained as minor extensions of the
</p>
<p>theory for problems having equality constraints only. To date, however, these al-
</p>
<p>ternative results concerning necessary conditions have been of isolated theoretical
</p>
<p>interest only&mdash;for they have not had an influence on the development of algorithms,
</p>
<p>and have not contributed to the theory of algorithms. Their use has been limited to
</p>
<p>small-scale programming problems of two or three variables. We therefore choose to
</p>
<p>emphasize the simplicity of incorporating inequalities rather than the possible com-
</p>
<p>plexities, not only for ease of presentation and insight, but also because it is
</p>
<p>this viewpoint that forms the basis for work beyond that of obtaining necessary
</p>
<p>conditions.
</p>
<p>First-Order Necessary Conditions
</p>
<p>With the following generalization of our previous definition it is possible to parallel
</p>
<p>the development of necessary conditions for equality constraints.
</p>
<p>Definition. Let x&lowast; be a point satisfying the constraints
</p>
<p>h(x&lowast;) = 0, g(x&lowast;) � 0, (11.33)
</p>
<p>and let J be the set of indices j for which g j(x
&lowast;) = 0. Then x&lowast; is said to be a regular point
</p>
<p>of the constraints (11.33) if the gradient vectors &nabla;hi(x
&lowast;), &nabla;gi(x
</p>
<p>&lowast;), 1 � i � m, j &isin; J are
linearly independent.
</p>
<p>We note that, following the definition of active constraints given in Sect. 11.1, a
</p>
<p>point x&lowast; is a regular point if the gradients of the active constraints are linearly inde-
pendent. Or, equivalently, x&lowast; is regular for the constraints if it is regular in the sense
of the earlier definition for equality constraints applied to the active constraints.
</p>
<p>Karush-Kuhn-Tucker Conditions. Let x&lowast; be a relative minimum point for the problem
</p>
<p>minimize f (x)
</p>
<p>subject to h(x) = 0, g(x) � 0, (11.34)</p>
<p/>
</div>
<div class="page"><p/>
<p>11.8 Inequality Constraints 341
</p>
<p>and suppose x&lowast; is a regular point for the constraints. Then there is a vector λ &isin; Em and a
vector &micro; &isin; E p with &micro; � 0 such that
</p>
<p>&nabla; f (x&lowast;) + λT&nabla;h(x&lowast;) + &micro;T&nabla;g(x&lowast;) = 0 (11.35)
&micro;Tg(x&lowast;) = 0. (11.36)
</p>
<p>Proof. We note first, since &micro; � 0 and g(x&lowast;) � 0, (11.36) is equivalent to the state-
ment that a component of &micro; may be nonzero only if the corresponding constraint
</p>
<p>is active. This a complementary slackness condition, stating that g(x&lowast;)i &lt; 0 implies
μi = 0 and μi &gt; 0 implies g(x
</p>
<p>&lowast;)i = 0.
Since x&lowast; is a relative minimum point over the constraint set, it is also a relative
</p>
<p>minimum over the subset of that set defined by setting the active constraints to zero.
</p>
<p>Thus, for the resulting equality constrained problem defined in a neighborhood of
</p>
<p>x&lowast;, there are Lagrange multipliers. Therefore, we conclude that (11.35) holds with
μ j = 0 if g j(x
</p>
<p>&lowast;) � 0 (and hence (11.36) also holds).
It remains to be shown that &micro; &ge; 0. Suppose μk &lt; 0 for some k &isin; J. Let S and M
</p>
<p>be the surface and tangent plane, respectively, defined by all other active constraints
</p>
<p>at x&lowast;. By the regularity assumption, there is a y such that y &isin; M and &nabla;gk(x&lowast;)y &lt; 0.
Let x(t) be a curve on S passing through x&lowast;(at t = 0) with ẋ(0) = y. Then for small
t &ge; 0, x(t) is feasible, and
</p>
<p>df
</p>
<p>dt
(x(t))
</p>
<p>]
</p>
<p>t=0
</p>
<p>= &nabla; f (x&lowast;)y &lt; 0
</p>
<p>by (11.35), which contradicts the minimality of x&lowast;. �
</p>
<p>Example. Consider the problem
</p>
<p>minimize 2x21 + 2x1x2 + x
2
2 &minus; 10x1 &minus; 10x2
</p>
<p>subject to x21 + x
2
2 � 5
</p>
<p>3x1 + x2 � 6.
</p>
<p>The first-order necessary conditions, in addition to the constraints, are
</p>
<p>4x1 + 2x2 &minus; 10 + 2μ1x1 + 3μ2 = 0
2x1 + 2x2 &minus; 10 + 2μ1x2 + μ2 = 0
</p>
<p>μ1 &ge; 0, μ2 &ge; 0
μ1(x
</p>
<p>2
1 + x
</p>
<p>2
2 &minus; 5) = 0
</p>
<p>μ2(3x1 + x2 &minus; 6) = 0.
</p>
<p>To find a solution we define various combinations of active constraints and check
</p>
<p>the signs of the resulting Lagrange multipliers. In this problem we can try setting
</p>
<p>none, one, or two constraints active. Assuming the first constraint is active and the
</p>
<p>second is inactive yields the equations</p>
<p/>
</div>
<div class="page"><p/>
<p>342 11 Constrained Minimization Conditions
</p>
<p>4x1 + 2x2 &minus; 10 + 2μ1x1 = 0
2x1 + 2x2 &minus; 10 + 2μ1x2 = 0
</p>
<p>x21 + x
2
2 = 5,
</p>
<p>which has the solution
</p>
<p>x1 = 1, x2 = 2, μ1 = 1.
</p>
<p>This yields 3x1 + x2 = 5 and hence the second constraint is satisfied. Thus, since
</p>
<p>μ1 &gt; 0, we conclude that this solution satisfies the first-order necessary conditions.
</p>
<p>Second-Order Conditions
</p>
<p>The second-order conditions, both necessary and sufficient, for problems with in-
</p>
<p>equality constraints, are derived essentially by consideration only of the equality
</p>
<p>constrained problem that is implied by the active constraints. The appropriate tan-
</p>
<p>gent plane for these problems is the plane tangent to the active constraints.
</p>
<p>Second-Order Necessary Conditions. Suppose the functions f , g, h &isin; C2 and that x&lowast; is a
regular point of the constraints (11.33). If x&lowast; is a relative minimumpoint for problem (11.32),
then there is a λ &isin; Em, &micro; &isin; E p, &micro; &ge; 0 such that (11.35) and (36) hold and such that
</p>
<p>L(x&lowast;) = F(x&lowast;) + λTH(x&lowast;) + &micro;TG(x&lowast;) (11.37)
</p>
<p>is positive semidefinite on the tangent subspace of the active constraints at x&lowast;.
</p>
<p>Proof. If x&lowast; is a relative minimum point over the constraints (11.33), it is also a
relative minimum point for the problem with the active constraints taken as equality
</p>
<p>constraints. �
</p>
<p>Just as in the theory of unconstrained minimization, it is possible to formulate a
</p>
<p>converse to the Second-Order Necessary Condition Theorem and thereby obtain a
</p>
<p>Second-Order Sufficiency Condition Theorem. By analogy with the unconstrained
</p>
<p>situation, one can guess that the required hypothesis is that L(x&lowast;) be positive definite
on the tangent plane M. This is indeed sufficient in most situations. However, if there
</p>
<p>are degenerate inequality constraints (that is, active inequality constraints having
</p>
<p>zero as associated Lagrange multiplier), we must require L(x&lowast;) to be positive definite
on a subspace that is larger than M.
</p>
<p>Second-Order Sufficiency Conditions. Let f , g, h &isin; C2. Sufficient conditions that a point
x&lowast; satisfying (33) be a strict relative minimum point of problem (11.32) is that there exist
λ &isin; Em, &micro; &isin; E p, such that
</p>
<p>&micro; &ge; 0 (11.38)
&micro;Tg(x&lowast;) = 0 (11.39)
</p>
<p>&nabla; f (x&lowast;) + λT&nabla;h(x&lowast;) + &micro;T1&nabla;g(x&lowast;) = 0, (11.40)</p>
<p/>
</div>
<div class="page"><p/>
<p>11.8 Inequality Constraints 343
</p>
<p>and the Hessian matrix
</p>
<p>L(x&lowast;) = F(x&lowast;) + λTH(x&lowast;) + μTG(x&lowast;) (11.41)
</p>
<p>is positive definite on the subspace
</p>
<p>M&prime; =
{
</p>
<p>y : &nabla;h(x&lowast;)y = 0, &nabla;g j(x
&lowast;)y = 0 f or all j &isin; J
</p>
<p>}
</p>
<p>,
</p>
<p>where J =
{
</p>
<p>j : g j(x
&lowast;) = 0, μ j &gt; 0
</p>
<p>}
</p>
<p>.
</p>
<p>Proof. As in the proof of the corresponding theorem for equality constraints in
</p>
<p>Sect. 11.5, assume that x&lowast; is not a strict relative minimum point; let {yk} be a se-
quence of feasible points converging to x&lowast; such that f (yk) � f (x
</p>
<p>&lowast;), and write each
yk in the form yk = x
</p>
<p>&lowast; + δksk with |sk | = 1, δk &gt; 0. We may assume that δk &rarr; 0 and
sk &rarr; s&lowast;. We have 0 &ge; &nabla; f (x&lowast;)s&lowast;, and for each i = 1, . . . , m we have
</p>
<p>&nabla;hi(x
&lowast;)s&lowast; = 0.
</p>
<p>Also for each active constraint g j we have g j(yk) &minus; g j(x&lowast;) � 0, and hence
</p>
<p>&nabla;g j(x
&lowast;)s&lowast; � 0.
</p>
<p>If &nabla;g j(x
&lowast;)s&lowast; = 0 for all j &isin; J, then the proof goes through just as in Sect. 11.5. If
</p>
<p>&nabla;g j(x&lowast;)s&lowast; &lt; 0 for at least one j &isin; J, then
</p>
<p>0 � &nabla; f (x&lowast;)s&lowast; = &minus;λT&nabla;h(x&lowast;)s&lowast; &minus; &micro;T&nabla;g(x&lowast;)s&lowast; &gt; 0,
</p>
<p>which is a contradiction. �
</p>
<p>We note in particular that if all active inequality constraints have strictly positive
</p>
<p>corresponding Lagrange multipliers (no degenerate inequalities), then the set J in-
</p>
<p>cludes all of the active inequalities. In this case the sufficient condition is that the
</p>
<p>Lagrangian be positive definite on M, the tangent plane of active constraints.
</p>
<p>Sensitivity
</p>
<p>The sensitivity result for problems with inequalities is a simple restatement of the
</p>
<p>result for equalities. In this case, a nondegeneracy assumption is introduced so that
</p>
<p>the small variations produced in Lagrange multipliers when the constraints are var-
</p>
<p>ied will not violate the positivity requirement.
</p>
<p>Sensitivity Theorem. Let f , g, h &isin; C2 and consider the family of problems
</p>
<p>minimize f (x)
</p>
<p>subject to h(x) = c, g(x) � d. (11.42)
</p>
<p>Suppose that for c = 0, d = 0, there is a local solution x&lowast; that is a regularpoint and
that, together with the associated Lagrange multipliers, λ, &micro; &ge; 0,satisfies the second-order</p>
<p/>
</div>
<div class="page"><p/>
<p>344 11 Constrained Minimization Conditions
</p>
<p>sufficiency conditions for a strict local minimum.Assume further that no active inequality
constraint is degenerate. Then for every (c, d) &isin; Em+p in a region containing (0, 0) there is
a solution x(c, d),depending continuously on (c, d), such that x(0, 0) = x&lowast;, and such that
x(c, d)is a relative minimum point of (11.42). Furthermore,
</p>
<p>&nabla;c f (x(c, d))
]
</p>
<p>0,0 = &minus;λT (11.43)
&nabla;d f (x(c, d))
</p>
<p>]
</p>
<p>0,0 = &minus;&micro;T. (11.44)
</p>
<p>11.9 Zero-Order Conditions and Lagrangian Relaxation
</p>
<p>Zero-order conditions for functionally constrained problems express conditions in
</p>
<p>terms of Lagrange multipliers without the use of derivatives. This theory is not only
</p>
<p>of great practical value, but it also gives new insight into the meaning of Lagrange
</p>
<p>multipliers. Rather than regarding the Lagrange multipliers as separate scalars, they
</p>
<p>are identified as components of a single vector that has a strong geometric interpre-
</p>
<p>tation. As before, the basic constrained problem is
</p>
<p>minimize f (x)
</p>
<p>subject to h(x) = 0, g(x) &le; 0
x &isin; Ω,
</p>
<p>(11.45)
</p>
<p>where x is a vector in En, and h and g are m-dimensional and p-dimensional func-
</p>
<p>tions, respectively.
</p>
<p>In purest form, zero-order conditions require that the functions that define the
</p>
<p>objective and the constraints are convex functions and sets. (See Appendix B.)
</p>
<p>The vector-valued function g consisting of p individual component functions
</p>
<p>g1, g2, . . . , gp is said to be convex if each of the component functions is convex.
</p>
<p>The programming problem (11.45) above is termed a convex programming prob-
</p>
<p>lem if the functions f and g are convex, the function h is affine (that is, linear plus a
</p>
<p>constant and can be written as Ax &minus; b), and the set Ω &sub; En is convex.
Notice that according to Proposition 3, Sect. 7.4, the set defined by each of the
</p>
<p>inequalities g j(x) � 0 is convex. This is true also of a set defined by hi(x) = 0.
</p>
<p>Since the overall constraint set is the intersection of these and Ω it follows from
</p>
<p>Proposition 1 of Appendix B that this overall constraint set is itself convex. Hence
</p>
<p>the problem can be regarded as minimize f (x), x &isin; Ω1 where Ω1 is a convex sub-
set of Ω.
</p>
<p>With this view, one could apply the zero-order conditions of Sect. 7.6 to the prob-
</p>
<p>lem with constraint set Ω1. However, in the case of functional constraints it is com-
</p>
<p>mon to keep the structure of the constraints explicit instead of folding them into an
</p>
<p>amorphous set.
</p>
<p>Although it is possible to derive the zero-order conditions for (11.45) all at once,
</p>
<p>treating both equality and inequality constraints together, it is notationally cumber-
</p>
<p>some to do so and it may obscure the basic simplicity of the arguments. For this
</p>
<p>reason, we treat equality constraints first, then inequality constraints, and finally the
</p>
<p>combination of the two.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.9 Zero-Order Conditions and Lagrangian Relaxation 345
</p>
<p>The equality problem is
</p>
<p>minimize f (x)
</p>
<p>subject to h(x) = 0
</p>
<p>x &isin; Ω.
(11.46)
</p>
<p>Letting Y = Em, we have h(x) &isin; Y for all x. For this problem we require a regularity
condition.
</p>
<p>Definition. An affine function h is regular with respect to Ω if the set C in Y defined by
C = {y : h(x) = y for some x &isin; Ω} contains an open sphere around 0; that is, C contains a
set of the form {y : |y| &lt; ε} for some ε &gt; 0.
</p>
<p>This condition means that h(x) can attain 0 and can vary in arbitrary directions
</p>
<p>from 0. Notice that this condition is similar to the definition of a regular point in
</p>
<p>the context of first-order conditions. If h has continuous derivatives at a point x&lowast;
</p>
<p>the earlier regularity condition implies that &nabla;h(x&lowast;) is of full rank and the Implicit
Function Theorem (of Appendix A) then guarantees that there is an ε &gt; 0 such that
</p>
<p>for any y with |y &minus; h(x&lowast;)| &lt; ε there is an x such that h(x) = y. In other words, there
is an open sphere around y&lowast; = h(x&lowast;) that is attainable. In the present situation we
assume this attainability directly, at the point 0 &isin; Y.
</p>
<p>Next we introduce the following important construction.
</p>
<p>Definition. The primal function associated with problem (11.46) is
</p>
<p>w(y) = inf{ f (x) : h(x) = y, x &isin; Ω},
</p>
<p>defined for all y &isin; C.
</p>
<p>Notice that the primal function is defined by varying the right hand side of the
</p>
<p>constraint. The original problem (11.46) corresponds to ω(0). The primal function
</p>
<p>is illustrated in Fig. 11.6.
</p>
<p>Proposition 1. Suppose Ω is convex, the function f is convex, and h is affine. Then the
</p>
<p>primal function ω is convex.
</p>
<p>Fig. 11.6 The primal function</p>
<p/>
</div>
<div class="page"><p/>
<p>346 11 Constrained Minimization Conditions
</p>
<p>Proof. For simplicity of notation we assume that Ω is the entire space X. Then we
</p>
<p>observe
</p>
<p>Ω(αy1 + (1 &minus; α)y2) = inf{ f (x) : h(x) = αy1 + (1 &minus; α)y2}
&le; inf{ f (x) : x = αx1 + (1 &minus; α)x2, h(x1) = y1, h(x2) = y2}
&le; α inf{ f (x1) : h(x1) = y1} + (1 &minus; α) inf{ fx2) : h(x2) = y2}
= α ω(y1) + (1 &minus; α)ω(y2). �
</p>
<p>We now turn to the derivation of the Lagrange multiplier result for (11.46).
</p>
<p>Proposition 2. Assume that Ω &sub; En is convex, f is a convex function on Ω and h is an
m-dimensional affine function on Ω. Assume that h is regular with respect to Ω. If x&lowast;
</p>
<p>solves (11.46), then there is λ &isin; Em such that x&lowast; solves the Lagrangian relaxation prob-
lem
</p>
<p>minimize f (x) + λTh(x)
</p>
<p>sub ject to x &isin; Ω.
</p>
<p>Proof. Let f &lowast; = f (x&lowast;). Define the sets A and B in Em+1 as
</p>
<p>A = {(r, y) : r &ge; ω(y), y &isin; C} and B = {(r, y) : r � f &lowast;, y = 0}.
</p>
<p>A is the epigraph of ω (see Sect. 7.6) and B is the vertical line extending below f &lowast;
</p>
<p>and aligned with the origin. Both A and B are convex sets. Their only common point
</p>
<p>is at ( f &lowast;, 0). See Fig. 11.7.
</p>
<p>Fig. 11.7 The sets A and B and the separating hyperplane
</p>
<p>According to the separating hyperplane theorem (Appendix B), there is a hyper-
</p>
<p>plane separating A and B. This hyperplane can be represented by a nonzero vector
</p>
<p>in Em+1 of the form (s, λ), with λ &isin; Em, and a separation constant c. The separation
conditions are
</p>
<p>sr + λTy &ge; c for all (r, y) &isin; A and sr + λTy � c for all (r, y) &isin; B.
</p>
<p>It follows immediately that s &ge; 0 for otherwise points (r, 0) &isin; B with r very negative
would violate the second inequality.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.9 Zero-Order Conditions and Lagrangian Relaxation 347
</p>
<p>Geometrically, if s = 0 the hyperplane would be vertical. We wish to show that
</p>
<p>s � 0, and it is for this purpose that we make use of the regularity condition. Suppose
</p>
<p>s = 0. Then λ � 0 since both s and λ cannot be zero. It follows from the second
</p>
<p>separation inequality that c = 0 because the hyperplane must include the point
</p>
<p>( f &lowast;, 0). Now, as y ranges over a sphere centered at 0 &isin; C, the left hand side of the
first separation inequality ranges correspondingly over λTy which is negative for
</p>
<p>some y&rsquo;s. This contradicts the first separation inequality. Thus s � 0 and thus in fact
</p>
<p>s &gt; 0. Without loss of generality we may, by rescaling if necessary, assume that
</p>
<p>s = 1.
</p>
<p>Finally, suppose x &isin; Ω. Then ( f (x), h(x)) &isin; A and ( f (x&lowast;), 0) &isin; B. Thus, from
the separation inequality (with s = 1) we have
</p>
<p>f (x) + λTh(x) &ge; f (x&lowast;) = f (x&lowast;) + λTh(x&lowast;).
</p>
<p>Hence x&lowast; solves the stated minimization problem. �
</p>
<p>Example 1 (Best Rectangle). Consider the classic problem of finding the rectangle
</p>
<p>of maximum area while limiting the perimeter to a length of 4. This can be formu-
</p>
<p>lated as
</p>
<p>minimize &minus; x1x2
subject to x1 + x2 &minus; 2 = 0
</p>
<p>x1 &ge; 0, x2 &ge; 0.
</p>
<p>The regularity condition is met because it is possible to make the right hand side of
</p>
<p>the functional constraint slightly positive or slightly negative with nonnegative x1
and x2. We know the answer to the problem is x1 = x2 = 1. The Lagrange multiplier
</p>
<p>is λ = 1. The Lagrangian problem of Proposition 2 is
</p>
<p>minimize &minus; x1x2 + 1 &middot; (x1 + x2 &minus; 2)
subject to x1 &ge; 0, x2 &ge; 0.
</p>
<p>This can be solved by differentiation to obtain x1 = x2 = 1.
</p>
<p>However the conclusion of the proposition is not satisfied! The value of the La-
</p>
<p>grangian at the solution is V = &minus;1 + 1 + 1 &minus; 2 = &minus;1. However, at x1 = x2 = 0
the value of the Lagrangian is V &prime; = &minus;2 which is less than V . The Lagrangian is
not minimized at the solution. The proposition breaks down because the objective
</p>
<p>function f (x1, x2) = &minus;x1x2 is not convex.
</p>
<p>Example 2 (Best Diagonal). As an alternative problem, consider minimizing the
</p>
<p>length of the diagonal of a rectangle subject to the perimeter being of length 4.
</p>
<p>This problem can be formulated as
</p>
<p>minimize 1
2
(x2
</p>
<p>1
+ x2
</p>
<p>2
)
</p>
<p>subject to x1 + x2 &minus; 2 = 0
x1 &ge; 0, x2 &ge; 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>348 11 Constrained Minimization Conditions
</p>
<p>In this case the objective function is convex. The solution is x1 = x2 = 1 and the
</p>
<p>Lagrange multiplier is λ = &minus;1. The Lagrangian problem is
</p>
<p>minimize
1
</p>
<p>2
(x21 + x
</p>
<p>2
2) &minus; 1 &middot; (x1 + x2 &minus; 2)
</p>
<p>subject to x1 &ge; 0, x2 &ge; 0.
</p>
<p>The value of the Lagrangian at the solution is V = 1 which in this case is a minimum
</p>
<p>as guaranteed by the proposition. (The value at x1 = x2 = 0 is V
&prime; = 2.)
</p>
<p>Inequality Constraints
</p>
<p>We outline the parallel results for the inequality constrained problem
</p>
<p>minimize f (x)
</p>
<p>subject to g(x) &le; 0 (11.47)
x &isin; Ω,
</p>
<p>where g is a p-dimensional function.
</p>
<p>We let Z = Ep and define D &sub; Z as D = {z &isin; Z : g(x) � z for some x &isin; Ω}.
The regularity condition (called the Slater condition) is that there is a z1 &isin; D with
z1 &lt; 0.
</p>
<p>As before we introduce the primal function.
</p>
<p>Definition. The primal function associated with problem (11.47) is
</p>
<p>w(z) = inf{ f (x) : g(x) &le; z, x &isin; Ω}.
</p>
<p>The primal function is again defined by varying the right hand side of the constraint
</p>
<p>function, using the variable z. Now the primal function in monotonically decreasing
</p>
<p>with z, since an increase in z enlarges the constraint region.
</p>
<p>Proposition 3. Suppose Ω &sub; En is convex and f and g are convex functions. Then the
primal function ω is also convex.
</p>
<p>Proof. The proof parallels that of Proposition 1. One simply substitutes g(x) &le; 0 for
h(x) = y throughout the series of inequalities. �
</p>
<p>The zero-order necessary Lagrangian conditions are then given by the proposi-
</p>
<p>tion below.
</p>
<p>Proposition 4. Assume Ω is a convex subset of En and that f and g are convex functions.
</p>
<p>Assume also that there is a point x1 &isin; Ω such that g(x1) &lt; 0. Then, if x&lowast; solves (11.47),
there is a vector &micro; &isin; E p with &micro; &ge; 0 such that x&lowast; solves the Lagrangian relaxation problem
</p>
<p>minimize f (x&lowast;) + &micro;Tg(x) (11.48)
</p>
<p>subject to x &isin; Ω.
</p>
<p>Furthermore, &micro;Tg(x&lowast;) = 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.9 Zero-Order Conditions and Lagrangian Relaxation 349
</p>
<p>Proof. Here is the proof outline. Let f &lowast; = f (x&lowast;). In this case define in Ep+1 the
two sets
</p>
<p>A = {(r, 0) : r &ge; f (x), 0 &ge; g(x), for some x &isin; Ω} and B = {(r, 0) : r &le; f &lowast;, 0 &le; 0}.
</p>
<p>A is the epigraph of the primal function ω. The set B is the rectangular region at or
</p>
<p>to the left of the vertical axis and at or lower than f &lowast;. Both A and B are convex. See
Fig. 11.8.
</p>
<p>Fig. 11.8 The sets A and B and the separating hyperplane for inequalities
</p>
<p>The proof is made by constructing a hyperplane separating A and B. The regu-
</p>
<p>larity condition guarantees that this hyperplane is not vertical. �
</p>
<p>The condition &micro;Tg(x&lowast;) = 0 is the complementary slackness condition that is
characteristic of necessary conditions for problems with inequality constraints.
</p>
<p>Example 4 (Quadratic Program). Consider the quadratic program
</p>
<p>minimize xTQx + cTx
</p>
<p>subject to aTx &le; b
x &ge; 0.
</p>
<p>Let Ω = {x : x &ge; 0} and g(x) = aTx &minus; b. Assume that the n &times; n matrix Q is positive
definite, in which case the objective function is convex. Assuming that b &gt; 0, the
</p>
<p>Slater regularity condition is satisfied. Hence there is a Lagrange multiplier μ &ge; 0
(a scalar in this case) such that the solution x&lowast; to the quadratic program is also a
solution to
</p>
<p>minimize xTQx + cTx + μ(aTx &minus; b)
subject to x &ge; 0 and μ(aTx&lowast; &minus; b) = 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>350 11 Constrained Minimization Conditions
</p>
<p>Mixed Constraints
</p>
<p>The two previous results can be combined to obtain zero-order conditions for the
</p>
<p>problem
minimize f (x)
</p>
<p>subject to h(x) = 0, g(x) &le; 0
x &isin; Ω.
</p>
<p>(11.49)
</p>
<p>Zero-order Lagrange Theorem. Assume that Ω &sub; En is a convex set, f and g are convex
functions of dimension 1 and p, respectively, and h is affine ofdimension m. Assume also
that h satisfies the regularity condition with respect to Ω and that there is an x1 &isin; Ω with
h(x1) = 0 and g(x1) &lt; 0. Suppose x
</p>
<p>&lowast; solves (11.49). Then there are vectors λ &isin; Em and
&micro; &isin; E p with &micro; &ge; 0 such that x&lowast; solves the Lagrangian relaxation problem
</p>
<p>minimize f (x) + λTh(x) + &micro;Tg(x) (11.50)
</p>
<p>subject to x &isin; Ω.
</p>
<p>Furthermore, &micro;Tg(x&lowast;) = 0.
</p>
<p>The convexity requirements of this result are satisfied in many practical problems.
</p>
<p>Indeed convex programming problems are both pervasive and relatively well treated
</p>
<p>by theory and numerical methods. The corresponding theory also motivates many
</p>
<p>approaches to general nonlinear programming problems. In fact, it will be apparent
</p>
<p>that many methods attempt to &ldquo;convexify&rdquo; a general nonlinear problem either by
</p>
<p>changing the formulation of the underlying application or by introducing devices
</p>
<p>that temporarily relax as the method progresses.
</p>
<p>Zero-Order Sufficient Conditions
</p>
<p>The sufficiency conditions are very strong and do not require convexity.
</p>
<p>Proposition 5 (Sufficiency Conditions). Suppose f is a real-valued function on a set Ω &sub;
En. Suppose also that h and g are, respectively, m-dimensionaland p-dimensional functions
</p>
<p>on Ω. Finally, suppose there are vectors x&lowast; &isin; Ω, λ &isin; Em, and &micro; &isin; E p with &micro; &ge; 0 such that
</p>
<p>f (x&lowast;) + λTh(x&lowast;) + &micro;Tg(x&lowast;) &le; f (x) + λTh(x) + &micro;Tg(x)
</p>
<p>for all x &isin; Ω. Then x&lowast; solves
</p>
<p>minimize f (x)
subject to h(x) = h(x&lowast;), g(x) &le; g(x&lowast;)
</p>
<p>x &isin; Ω.
</p>
<p>Proof. Suppose there is x1 &isin; Ω with f (x1) &lt; f (x&lowast;), h(x1) = h(x&lowast;), and g(x1) &le;
g(x&lowast;). From&micro; &ge; 0 it is clear that &micro;Tg(x1) &le; &micro;Tg(x&lowast;). It follows that f (x1)+λTh(x1)+
&micro;Tg(x1) &lt; f (x
</p>
<p>&lowast;) + λTh(x&lowast;) + &micro;Tg(x&lowast;), which is a contradiction. �
</p>
<p>Notice that the constraint of the Lagrangian relaxation problem is significantly
</p>
<p>simpler, and typically much easier to solve for given λ and &micro;. The result suggests</p>
<p/>
</div>
<div class="page"><p/>
<p>11.10 Summary 351
</p>
<p>that Lagrange multiplier values might be guessed and used to define an initial La-
</p>
<p>grangian relaxation problem which is subsequently minimized. This will produce
</p>
<p>a solution of x and its constraint values. If these values meet the given right-hand
</p>
<p>side requirement, then x is optimal. Otherwise, one may adjust Lagrange multiplier
</p>
<p>values accordingly. Indeed, this approach, the Lagrangian relaxation method, will
</p>
<p>be characteristic of a duality method treated in Chap. 14.
</p>
<p>The theory of this section has an inherent geometric simplicity captured clearly
</p>
<p>by Figs. 11.7 and 11.8. It raises ones&rsquo; s level of understanding of Lagrange multi-
</p>
<p>pliers and sets the stage for the theory of convex duality presented in Chap. 14. It is
</p>
<p>certainly possible to jump ahead and read that now.
</p>
<p>11.10 Summary
</p>
<p>Given a minimization problem subject to equality constraints in which all functions
</p>
<p>are smooth, a necessary condition satisfied at a minimum point is that the gradient
</p>
<p>of the objective function is orthogonal to the tangent plane of the constraint surface.
</p>
<p>If the point is regular, then the tangent plane has a simple representation in terms of
</p>
<p>the gradients of the constraint functions, and the above condition can be expressed
</p>
<p>in terms of Lagrange multipliers.
</p>
<p>If the functions have continuous second partial derivatives and Lagrange multi-
</p>
<p>pliers exist, then the Hessian of the Lagrangian restricted to the tangent plane plays
</p>
<p>a role in second-order conditions analogous to that played by the Hessian of the
</p>
<p>objective function in unconstrained problems. Specifically, the restricted Hessian
</p>
<p>must be positive semidefinite at a relative minimum point and, conversely, if it is
</p>
<p>positive definite at a point satisfying the first-order conditions, that point is a strict
</p>
<p>local minimum point.
</p>
<p>Inequalities are treated by determining which of them are active at a solution.
</p>
<p>An active inequality then acts just like an equality, except that its associated La-
</p>
<p>grange multiplier can never be negative because of the sensitivity interpretation of
</p>
<p>the multipliers.
</p>
<p>The necessary conditions for convex problems can be expressed without deriva-
</p>
<p>tives, and these are according termed zero-order conditions. These conditions are
</p>
<p>highly geometric in character and explicitly treat the Lagrange multiplier as a vector
</p>
<p>in a space having dimension equal to that of the right-hand-side of the constraints.
</p>
<p>This Lagrange multiplier vector defines a hyperplane that separates the epigraph of
</p>
<p>the primal function from a set of unattainable objective and constraint value combi-
</p>
<p>nations.
</p>
<p>The &ldquo;zero-order&rdquo; optimality condition developed in this chapter establishes a
</p>
<p>theoretical base of the Lagrangian relaxation method, which would be introduced
</p>
<p>later and is extremely popular for large-scale optimization.</p>
<p/>
</div>
<div class="page"><p/>
<p>352 11 Constrained Minimization Conditions
</p>
<p>11.11 Exercises
</p>
<p>1. In E2 consider the constraints
</p>
<p>x1 � 0, x2 � 0
</p>
<p>x2 &minus; (x1 &minus; 1)2 � 0.
</p>
<p>Show that the point x1 = 1, x2 = 0 is feasible but is not a regular point.
</p>
<p>2. Find the rectangle of given perimeter that has greatest area by solving the first-
</p>
<p>order necessary conditions. Verify that the second-order sufficiency conditions
</p>
<p>are satisfied.
</p>
<p>3. Verify the second-order conditions for the entropy example of Sect. 11.4.
</p>
<p>4. A cardboard box for packing quantities of small foam balls is to be manufac-
</p>
<p>tured as shown in Fig. 11.9. The top, bottom, and front faces must be of double
</p>
<p>weight (i.e., two pieces of cardboard). A problem posed is to find the dimen-
</p>
<p>sions of such a box that maximize the volume for a given amount of cardboard,
</p>
<p>equal to 72 sq. ft.
</p>
<p>(a) What are the first-order necessary conditions?
</p>
<p>(b) Find x, y, z.
</p>
<p>(c) Verify the second-order conditions.
</p>
<p>Fig. 11.9 Packing box
</p>
<p>5. Define
</p>
<p>L =
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>4 3 2
</p>
<p>3 1 1
</p>
<p>2 1 1
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>, h = (1, 1, 0),
</p>
<p>and let M be the subspace consisting of those points x = (x1, x2, x3) satisfying
</p>
<p>hTx = 0.
</p>
<p>(a) Find LM .
</p>
<p>(b) Find the eigenvalues of LM .</p>
<p/>
</div>
<div class="page"><p/>
<p>11.11 Exercises 353
</p>
<p>(c) Find
</p>
<p>p(λ) = det
</p>
<p>[
</p>
<p>0 hT
</p>
<p>&minus;h L &minus; Iλ
</p>
<p>]
</p>
<p>.
</p>
<p>(d) Apply the projected Hessian test.
</p>
<p>6. Show that zTx = 0 for all x satisfying Ax = 0 if and only if z = ATw for
</p>
<p>some w. (Hint: Use the Duality Theorem of Linear Programming.)
</p>
<p>7. After a heavy military campaign a certain army requires many new shoes. The
</p>
<p>quartermaster can order three sizes of shoes. Although he does not know pre-
</p>
<p>cisely how many of each size are required, he feels that the demand for the three
</p>
<p>sizes are independent and the demand for each size is uniformly distributed be-
</p>
<p>tween zero and three thousand pairs. He wishes to allocate his shoe budget of
</p>
<p>$4,000 among the three sizes so as to maximize the expected number of men
</p>
<p>properly shod. Small shoes cost $1 per pair, medium shoes cost $2 per pair, and
</p>
<p>large shoes cost $4 per pair. How many pairs of each size should he order?
</p>
<p>8. Optimal control. A one-dimensional dynamic process is governed by a differ-
</p>
<p>ence equation
</p>
<p>x(k + 1) = φ(x(k), u(k), k)
</p>
<p>with initial condition x(0) = x0. In this equation the value x(k) is called the state
</p>
<p>at step k and u(k) is the control at step k. Associated with this system there is an
</p>
<p>objective function of the form
</p>
<p>J =
</p>
<p>N
&sum;
</p>
<p>k=0
</p>
<p>ψ(x(k), u(k), k).
</p>
<p>In addition, there is a terminal constraint of the form
</p>
<p>g(x(N + 1)) = 0.
</p>
<p>The problem is to find the sequence of controls u(0), u(11.1), u(11.2), . . . , u(N)
</p>
<p>and corresponding state values to minimize the objective function while satisfy-
</p>
<p>ing the terminal constraint. Assuming all functions have continuous first partial
</p>
<p>derivatives and that the regularity condition is satisfied, show that associated
</p>
<p>with an optimal solution there is a sequence λ(k), k = 0, 1, . . . , N and a μ such
</p>
<p>that
</p>
<p>λ(k &minus; 1) = λ(k)φx(x(k), u(k), k) + ψx(x(k), u(k), k), k = 1, 2, . . . , N
λ(N) = μgx(x(N + 1))
</p>
<p>ψu(x(k), u(k), k) + λ(k)φu(x(k), u(k), k) = 0, k = 0, 1, 2, . . . , N.
</p>
<p>9. Generalize Exercise 9 to include the case where the state x(k) is an n-dimensional
</p>
<p>vector and the control u(k) is an m-dimensional vector at each stage k.
</p>
<p>10. An egocentric young man has just inherited a fortune F and is now planning
</p>
<p>how to spend it so as to maximize his total lifetime enjoyment. He deduces</p>
<p/>
</div>
<div class="page"><p/>
<p>354 11 Constrained Minimization Conditions
</p>
<p>that if x(k) denotes his capital at the beginning of year k, his holdings will be
</p>
<p>approximately governed by the difference equation
</p>
<p>x(k + 1) = αx(k) &minus; u(k), x(0) = F,
</p>
<p>where α � 1 (with α&minus; 1 as the interest rate of investment) and where u(k) is the
amount spent in year k. He decides that the enjoyment achieved in year k can
</p>
<p>be expressed as ψ(u(k)) where ψ, his utility function, is a smooth function, and
</p>
<p>that his total lifetime enjoyment is
</p>
<p>J =
</p>
<p>N
&sum;
</p>
<p>k=0
</p>
<p>ψ(u(k))βk,
</p>
<p>where the term βk(0 &lt; β &lt; 1) reflects the notion that future enjoyment is
</p>
<p>counted less today. The young man wishes to determine the sequence of ex-
</p>
<p>penditures that will maximize his total enjoyment subject to the condition
</p>
<p>x(N + 1) = 0.
</p>
<p>(a) Find the general optimality relationship for this problem.
</p>
<p>(b) Find the solution for the special case ψ(u) = u1/2.
</p>
<p>11. Let A be an m&times;n matrix of rank m and let L be an n&times;n matrix that is symmetric
and positive definite on the subspace M = {x : Ax = 0}. Show that the (n+m)&times;
(n + m) matrix
</p>
<p>[
</p>
<p>L AT
</p>
<p>A 0
</p>
<p>]
</p>
<p>is nonsingular.
</p>
<p>12. Consider the quadratic program
</p>
<p>minimize
1
</p>
<p>2
xTQx &minus; bTx
</p>
<p>subject to Ax = c.
</p>
<p>Prove that x&lowast; is a local minimum point if and only if it is a global minimum
point. (No convexity is assumed.)
</p>
<p>13. Maximize 14x &minus; x2 + 6y &minus; y2 + 7 subject to x + y � 2, x + 2y � 3.
14. In the quadratic program example of Sect. 11.9, what are more general condi-
</p>
<p>tions on a and b that satisfy the Slater condition?
</p>
<p>15. What are the general zero-order Lagrangian conditions for the problem (11.46)
</p>
<p>without the regularity condition? [The coefficient of f will be zero, so there is
</p>
<p>no real condition.]
</p>
<p>16. Show that the problem of finding the rectangle of maximum area with a diago-
</p>
<p>nal of unit length can be formulated as an unconstrained convex programming
</p>
<p>problem using trigonometric functions. [Hint: use variable θ over the range
</p>
<p>0 &le; θ &le; 45◦.]</p>
<p/>
</div>
<div class="page"><p/>
<p>References 355
</p>
<p>References
</p>
<p>11.1&ndash;11.5 For a classic treatment of Lagrange multipliers see Hancock [H4]. Also
</p>
<p>see Fiacco and McCormick [F4], Luenberger [L8], or McCormick [M2].
</p>
<p>11.6 The simple formula for the characteristic polynomial of LM as an (n+m)
</p>
<p>th-order determinant is apparently due to Luenberger [L17].
</p>
<p>11.8 The systematic treatment of inequality constraints was published by
</p>
<p>Kuhn and Tucker [K11]. Later it was found that the essential elements
</p>
<p>of the theory were contained in the 1939 unpublished M.Sci Disertation
</p>
<p>of W. Karush in the Department of Mathematics, University of Chicago.
</p>
<p>It is common to recognize this contribution by including his name to the
</p>
<p>conditions for optimality.
</p>
<p>11.9 The theory of convex problems and the corresponding Lagrange multi-
</p>
<p>plier theory was developed by Slater [S7]. For presentations similar to
</p>
<p>this section, see Hurwicz [H14] and Luenberger [L8].</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 12
</p>
<p>Primal Methods
</p>
<p>In this chapter we initiate the presentation, analysis, and comparison of algorithms
</p>
<p>designed to solve constrained minimization problems. The four chapters that con-
</p>
<p>sider such problems roughly correspond to the following classification scheme.
</p>
<p>Consider a constrained minimization problem having n variables and m constraints.
</p>
<p>Methods can be devised for solving this problem that work in spaces of dimension
</p>
<p>n &minus; m, n, m, or n + m. Each of the following chapters corresponds to methods in
one of these spaces. Thus, the methods in the different chapters represent quite dif-
</p>
<p>ferent approaches and are founded on different aspects of the theory. However, there
</p>
<p>are also strong interconnections between the methods of the various chapters, both
</p>
<p>in the final form of implementation and in their performance. Indeed, there soon
</p>
<p>emerges the theme that the rates of convergence of most practical algorithms are
</p>
<p>determined by the structure of the Hessian of the Lagrangian much like the struc-
</p>
<p>ture of the Hessian of the objective function determines the rates of convergence
</p>
<p>for a wide assortment of methods for unconstrained problems. Thus, although the
</p>
<p>various algorithms of these chapters differ substantially in their motivation, they are
</p>
<p>ultimately found to be governed by a common set of principles.
</p>
<p>12.1 Advantage of Primal Methods
</p>
<p>We consider the question of solving the general nonlinear programming problem
</p>
<p>minimize f (x)
</p>
<p>subject to h(x) = 0, g(x) � 0
(12.1)
</p>
<p>where x is of dimension n, while f , g, and h have dimensions 1, p, and m, respec-
</p>
<p>tively. It is assumed throughout the chapter that all of the functions have continuous
</p>
<p>partial derivatives of order three. Geometrically, we regard the problem as that of
</p>
<p>minimizing f over the region in En defined by the constraints.
</p>
<p>&copy; Springer International Publishing Switzerland 2016
</p>
<p>D.G. Luenberger, Y. Ye, Linear and Nonlinear Programming, International
Series in Operations Research &amp; Management Science 228,
DOI 10.1007/978-3-319-18842-3 12
</p>
<p>357</p>
<p/>
</div>
<div class="page"><p/>
<p>358 12 Primal Methods
</p>
<p>By a primal method of solution we mean a search method that works on the
</p>
<p>original problem directly by searching through the feasible region for the optimal
</p>
<p>solution. Each point in the process is feasible and the value of the objective func-
</p>
<p>tion constantly decreases. For a problem with n variables and having m equality
</p>
<p>constraints only, primal methods work in the feasible space, which has dimension
</p>
<p>n &minus; m.
Primal methods possess three significant advantages that recommend their use
</p>
<p>as general procedures applicable to almost all nonlinear programming problems.
</p>
<p>First, since each point generated in the search procedure is feasible, if the process
</p>
<p>is terminated before reaching the solution (as practicality almost always dictates
</p>
<p>for nonlinear problems), the terminating point is feasible. Thus this final point is a
</p>
<p>feasible and probably nearly optimal solution to the original problem and therefore
</p>
<p>may represent an acceptable solution to the practical problem that motivated the
</p>
<p>nonlinear program. A second attractive feature of primal methods is that, often, it
</p>
<p>can be guaranteed that if they generate a convergent sequence, the limit point of that
</p>
<p>sequence must be at least a local constrained minimum. Finally, a major advantage
</p>
<p>is that most primal methods do not rely on special problem structure, such as con-
</p>
<p>vexity, and hence these methods are applicable to general nonlinear programming
</p>
<p>problems.
</p>
<p>Primal methods are not, however, without major disadvantages. They require a
</p>
<p>phase I procedure (see Sect. 3.5) to obtain an initial feasible point, and they are all
</p>
<p>plagued, particularly for problems with nonlinear constraints, with computational
</p>
<p>difficulties arising from the necessity to remain within the feasible region as the
</p>
<p>method progresses. Some methods can fail to converge for problems with inequality
</p>
<p>constraints unless elaborate precautions are taken.
</p>
<p>The convergence rates of primal methods are competitive with those of other
</p>
<p>methods, and particularly for linear constraints, they are often among the most effi-
</p>
<p>cient. On balance their general applicability and simplicity place these methods in a
</p>
<p>role of central importance among nonlinear programming algorithms.
</p>
<p>12.2 Feasible Direction Methods
</p>
<p>The idea of feasible direction methods is to take steps through the feasible region of
</p>
<p>the form
</p>
<p>xk+1 = xk + αkdk, (12.2)
</p>
<p>where dk is a direction vector and αk is a nonnegative scalar. The scalar is chosen
</p>
<p>to minimize the objective function f with the restriction that the point xk+1 and
</p>
<p>the line segment joining xk and xk+1 be feasible. Thus, in order that the process
</p>
<p>of minimizing with respect to α be nontrivial, an initial segment of the ray xk +
</p>
<p>αdk, α &gt; 0 must be contained in the feasible region. This motivates the use of
</p>
<p>feasible directions for the directions of search. We recall from Sect. 7.1 that a vector
</p>
<p>dk is a feasible direction (at xk) if there is an ᾱ &gt; 0 such that xk + αdk is feasible</p>
<p/>
</div>
<div class="page"><p/>
<p>12.2 Feasible Direction Methods 359
</p>
<p>for all α, 0 � α � ᾱ. A feasible direction method can be considered as a natural
</p>
<p>extension of our unconstrained descent methods. Each step is the composition of
</p>
<p>selecting a feasible direction and a constrained line search.
</p>
<p>Let us consider the problem with linear inequality constraints
</p>
<p>minimize f (x)
</p>
<p>subject to aT
i
</p>
<p>x � bi, i = 1, . . .,m.
(12.3)
</p>
<p>Example 1 (Frank-Wolfe Method). One of the earliest proposals for a feasible direc-
</p>
<p>tion method uses a sequential linear programming subproblem approach. Given a
</p>
<p>feasible point xk, the direction vector
</p>
<p>dk = x
&lowast;
k &minus; xk
</p>
<p>where x&lowast;
k
</p>
<p>is a solution to the linear program
</p>
<p>minimize &nabla; f (xk)x
</p>
<p>subject to aT
i
</p>
<p>x � bi, i = 1, . . .,m.
</p>
<p>Example 2 (Simplified Zoutendijk Method). Another proposal solves a sequence of
</p>
<p>linear subprograms as follows. Given a feasible point, xk, let I be the set of indices
</p>
<p>representing active constraints, that is, aT
i
</p>
<p>xk = bi for i &isin; I. The direction vector dk
is then chosen as a solution to the linear program
</p>
<p>minimize &nabla; f (xk)d
</p>
<p>subject to aT
i
</p>
<p>d � 0, i &isin; I
n
&sum;
</p>
<p>i=1
|di| = 1,
</p>
<p>(12.4)
</p>
<p>where d = (d1, d2, . . . , dn). The last equation is a normalizing equation that en-
</p>
<p>sures a bounded solution. (Even though it is written in terms of absolute values, the
</p>
<p>problem can be converted to a linear program; see Exercise 1.) The other constraints
</p>
<p>assure that vectors of the form xk + αdk will be feasible for sufficiently small α &gt; 0,
</p>
<p>and subject to these conditions, d is chosen to line up as closely as possible with
</p>
<p>the negative gradient of f . In some sense this will result in the locally best direc-
</p>
<p>tion in which to proceed. The overall procedure progresses by generating feasible
</p>
<p>directions in this manner, and moving along them to decrease the objective.
</p>
<p>There are two major shortcomings of feasible direction methods that require that
</p>
<p>they be modified in most cases. The first shortcoming is that for general problems
</p>
<p>there may not exist any feasible directions. If, for example, a problem had nonlinear
</p>
<p>equality constraints, we might find ourselves in the situation depicted by Fig. 12.1
</p>
<p>where no straight line from xk has a feasible segment. For such problems it is nec-
</p>
<p>essary either to relax our requirement of feasibility by allowing points to deviate
</p>
<p>slightly from the constraint surface or to introduce the concept of moving along
</p>
<p>curves rather than straight lines.</p>
<p/>
</div>
<div class="page"><p/>
<p>360 12 Primal Methods
</p>
<p>Fig. 12.1 No feasible direction
</p>
<p>A second shortcoming is that in simplest form most feasible direction methods,
</p>
<p>such as the simplified Zoutendijk method, are not globally convergent. They are sub-
</p>
<p>ject to jamming (sometimes referred to as zigzagging) where the sequence of points
</p>
<p>generated by the process converges to a point that is not even a constrained local
</p>
<p>minimum point. This phenomenon can be explained by the fact that the algorithmic
</p>
<p>map is not closed.
</p>
<p>It is possible to develop feasible direction algorithms that are closed and hence
</p>
<p>not subject to jamming. Some procedures for doing so are discussed in Exercises
</p>
<p>4&ndash;7. However, such methods can become somewhat complicated. A simpler ap-
</p>
<p>proach for treating inequality constraints is to use an active set method, as discussed
</p>
<p>in the next section.
</p>
<p>12.3 Active Set Methods
</p>
<p>The idea underlying active set methods is to partition inequality constraints into
</p>
<p>two groups: those that are to be treated as active and those that are to be treated as
</p>
<p>inactive. The constraints treated as inactive are essentially ignored.
</p>
<p>Consider the constrained problem
</p>
<p>minimize f (x)
</p>
<p>subject to g(x) � 0,
(12.5)
</p>
<p>which for simplicity of the current discussion is taken to have inequality constraints
</p>
<p>only. The inclusion of equality constraints is straightforward, as will become clear.
</p>
<p>The necessary conditions for this problem are
</p>
<p>&nabla; f (x) + λT&nabla;g(x) = 0
</p>
<p>g(x) � 0
</p>
<p>λTg(x) = 0 (12.6)
</p>
<p>λ � 0.
</p>
<p>(See Sect. 11.8.) These conditions can be expressed in a somewhat simpler form in
</p>
<p>terms of the set of active constraints. Let A denote the index set of active constraints;
</p>
<p>that is, A is the set of i such that gi(x
&lowast;) = 0. Then the necessary conditions (12.6)
</p>
<p>become</p>
<p/>
</div>
<div class="page"><p/>
<p>12.3 Active Set Methods 361
</p>
<p>&nabla; f (x) +
&sum;
</p>
<p>i&isin;A
λi&nabla;gi(x) = 0
</p>
<p>gi(x) = 0, i &isin; A
gi(x) &lt; 0, i � A (12.7)
</p>
<p>λi � 0, i &isin; A
λi = 0, i � A
</p>
<p>The first two lines of these conditions correspond identically to the necessary condi-
</p>
<p>tions of the equality constrained problem obtained by requiring the active constraints
</p>
<p>to be zero. The next line guarantees that the inactive constraints are satisfied, and the
</p>
<p>sign requirement of the Lagrange multipliers guarantees that every constraint that is
</p>
<p>active should be active.
</p>
<p>It is clear that if the active set were known, the original problem could be replaced
</p>
<p>by the corresponding problem having equality constraints only. Alternatively, sup-
</p>
<p>pose an active set was guessed and the corresponding equality constrained problem
</p>
<p>solved. Then if the other constraints were satisfied and the Lagrange multipliers
</p>
<p>turned out to be nonnegative, that solution would be correct.
</p>
<p>The idea of active set methods is to define at each step, or at each phase, of
</p>
<p>an algorithm a set of constraints, termed the working set, that is to be treated as
</p>
<p>the active set. The working set is chosen to be a subset of the constraints that are
</p>
<p>actually active at the current point, and hence the current point is feasible for the
</p>
<p>working set. The algorithm then proceeds to move on the surface defined by the
</p>
<p>working set of constraints to an improved point. At this new point the working
</p>
<p>set may be changed. Overall, then, an active set method consists of the following
</p>
<p>components: (1) determination of a current working set that is a subset of the current
</p>
<p>active constraints, and (2) movement on the surface defined by the working set to an
</p>
<p>improved point.
</p>
<p>There are several methods for determining the movement on the surface defined
</p>
<p>by the working set. (This surface will be called the working surface.) The most im-
</p>
<p>portant of these methods are discussed in the following sections. The direction of
</p>
<p>movement is generally determined by first-order or second-order approximations
</p>
<p>of the functions at the current point in a manner similar to that for unconstrained
</p>
<p>problems. The asymptotic convergence properties of active set methods depend en-
</p>
<p>tirely on the procedure for moving on the working surface, since near the solution
</p>
<p>the working set is generally equal to the correct active set, and the process simply
</p>
<p>moves successively on the surface determined by those constraints.
</p>
<p>Changes in Working Set
</p>
<p>Suppose that for a given working set W the problem with equality constraints
</p>
<p>minimize f (x)
</p>
<p>subject to gi(x) = 0, i &isin; W</p>
<p/>
</div>
<div class="page"><p/>
<p>362 12 Primal Methods
</p>
<p>is solved yielding the point xW that satisfies gi(xW ) &lt; 0, i � W. This point satisfies
</p>
<p>the necessary conditions
</p>
<p>&nabla; f (xW ) +
&sum;
</p>
<p>i&isin;W
λ j&nabla;gi(xW ) = 0. (12.8)
</p>
<p>If λi � 0 for all i &isin; W, then the point xW is a local solution to the original prob-
lem. If, on the other hand, there is an i &isin; W such that λi &lt; 0, then the objective
can be decreased by relaxing constraint i. This follows directly from the sensitiv-
</p>
<p>ity interpretation of Lagrange multipliers, since a small decrease in the constraint
</p>
<p>value from 0 to &minus;c would lead to a change in the objective function of λic, which
is negative. Thus, by dropping the constraint i from the working set, an improved
</p>
<p>solution can be obtained. The Lagrange multiplier of a problem thereby serves as
</p>
<p>an indication of which constraints should be dropped from the working set. This is
</p>
<p>illustrated in Fig. 12.2. In the figure, x is the minimum point of f on the surface (a
</p>
<p>curve in this case) defined by g1(x) = 0. However, it is clear that the corresponding
</p>
<p>Lagrange multiplier λ1 is negative, implying that g1 should be dropped. Since &nabla; f
</p>
<p>points outside, it is clear that a movement toward the interior of the feasible region
</p>
<p>will indeed decrease f .
</p>
<p>During the course of minimizing f (x) over the working surface, it is necessary
</p>
<p>to monitor the values of the other constraints to be sure that they are not violated,
</p>
<p>since all points defined by the algorithm must be feasible. It often happens that
</p>
<p>while moving on the working surface a new constraint boundary is encountered. It
</p>
<p>is then convenient to add this constraint to the working set, proceeding on a surface
</p>
<p>of one lower dimension than before. This is illustrated in Fig. 12.3. In the figure the
</p>
<p>working constraint is just g1 = 0 for x1, x2, x3. A boundary is encountered at the
</p>
<p>next step, and therefore g2 = 0 is adjoined to the set of working constraints.
</p>
<p>Fig. 12.2 Constraint to be dropped
</p>
<p>A complete active set strategy for systematically dropping and adding constraints
</p>
<p>can be developed by combining the above two ideas. One starts with a given working
</p>
<p>set and begins minimizing over the corresponding working surface. If new constraint
</p>
<p>boundaries are encountered, they may be added to the working set, but no constraints
</p>
<p>are dropped from the working set. Finally, a point is obtained that minimizes f</p>
<p/>
</div>
<div class="page"><p/>
<p>12.3 Active Set Methods 363
</p>
<p>Fig. 12.3 Constraint added to working set
</p>
<p>with respect to the current working set of constraints. The corresponding Lagrange
</p>
<p>multipliers are determined, and if they are all nonnegative the solution is optimal.
</p>
<p>Otherwise, one or more constraints with negative Lagrange multipliers are dropped
</p>
<p>from the working set. The procedure is reinitiated with this new working set, and f
</p>
<p>will strictly decrease on the next step.
</p>
<p>An active set method built upon this basic active set strategy requires that a pro-
</p>
<p>cedure be defined for minimization on a working surface that allows constraints to
</p>
<p>be added to the working set when they are encountered, and that, after dropping a
</p>
<p>constraint, insures that the objective is strictly decreased. Such a method is guaran-
</p>
<p>teed to converge to the optimal solution, as shown below.
</p>
<p>Active Set Theorem. Suppose that for every subset W of the constraint indices, the con-
</p>
<p>strained problem
</p>
<p>minimize f (x)
subject to gi(x) = 0, i &isin; W (12.9)
</p>
<p>is well-defined with a unique nondegenerate solution (that is, for all i &isin; W, λi � 0). Then
the sequence of points generated by the basic active set strategy converges to the solution
</p>
<p>of the inequality constrained problem (12.6).
</p>
<p>Proof. After the solution corresponding to one working set is found, a decrease in
</p>
<p>the objective is made, and hence it is not possible to return to that working set. Since
</p>
<p>there are only a finite number of working sets, the process must terminate. �
</p>
<p>The difficulty with the above procedure is that several problems with incorrect
</p>
<p>active sets must be solved. Furthermore, the solutions to these intermediate prob-
</p>
<p>lems must, in general, be exact global minimum points in order to determine the
</p>
<p>correct sign of the Lagrange multipliers and to assure that during the subsequent
</p>
<p>descent process the current working surface is not encountered again.
</p>
<p>In practice one deviates from the ideal basic method outlined above by dropping
</p>
<p>constraints using various criteria before an exact minimum on the working surface
</p>
<p>is found. Convergence cannot be guaranteed for many of these methods, and in-
</p>
<p>deed they are subject to zigzagging (or jamming) where the working set changes</p>
<p/>
</div>
<div class="page"><p/>
<p>364 12 Primal Methods
</p>
<p>an infinite number of times. However, experience has shown that zigzagging is very
</p>
<p>rare for many algorithms, and in practice the active set strategy with various refine-
</p>
<p>ment is often very effective.
</p>
<p>It is clear that a fundamental component of an active set method is the algorithm
</p>
<p>for solving a problem with equality constraints only, that is, for minimizing on the
</p>
<p>working surface. Such methods and their analyses are presented in the following
</p>
<p>sections.
</p>
<p>12.4 The Gradient Projection Method
</p>
<p>The gradient projection method is motivated by the ordinary method of steepest
</p>
<p>descent for unconstrained problems. The negative gradient is projected onto the
</p>
<p>working surface in order to define the direction of movement. We present it here in
</p>
<p>a simplified form that is based on a pure active set strategy.
</p>
<p>Linear Constraints
</p>
<p>Consider first problems of the form
</p>
<p>minimize f (x)
</p>
<p>subject to aT
i
</p>
<p>x � bi, i &isin; I1
aT
i
</p>
<p>x = bi, i &isin; I2
(12.10)
</p>
<p>having linear equalities and inequalities.
</p>
<p>A feasible solution to the constraints, if one exists, can be found by application
</p>
<p>of the phase I procedure of linear programming; so we shall always assume that
</p>
<p>our descent process is initiated at such a feasible point. At a given feasible point x
</p>
<p>there will be a certain number q of active constraints satisfying aT
i
</p>
<p>x = bi and some
</p>
<p>inactive constraints aT
i
</p>
<p>x &lt; bi. We initially take the working set W(x) to be the set of
</p>
<p>active constraints.
</p>
<p>At the feasible point x we seek a feasible direction vector d satisfying &nabla; f (x)d &lt;
</p>
<p>0, so that movement in the direction d will cause a decrease in the function f . Ini-
</p>
<p>tially, we consider directions satisfying aT
i
</p>
<p>d = 0, i &isin; W(x) so that all working
constraints remain active. This requirement amounts to requiring that the direction
</p>
<p>vector d lie in the tangent subspace M defined by the working set of constraints. The
</p>
<p>particular direction vector that we shall use is the projection of the negative gradient
</p>
<p>onto this subspace.
</p>
<p>To compute this projection let Aq be defined as composed of the rows of working
</p>
<p>constraints. Assuming regularity of the constraints, as we shall always assume, Aq
will be a q &times; n matrix of rank q &lt; n. The tangent subspace M in which d must
lie is the subspace of vectors satisfying Aqd = 0. This means that the subspace N</p>
<p/>
</div>
<div class="page"><p/>
<p>12.4 The Gradient Projection Method 365
</p>
<p>consisting of the vectors making up the rows of Aq (that is, all vectors of the form
</p>
<p>ATqλ for λ &isin; Eq) is orthogonal to M. Indeed, any vector can be written as the sum of
vectors from each of these two complementary subspaces. In particular, the negative
</p>
<p>gradient vector &minus;gk can be written
</p>
<p>&minus; gk = dk + ATqλk (12.11)
</p>
<p>where dk &isin; M and λk &isin; Eq. We may solve for λk through the requirement that
Aqdk = 0. Thus
</p>
<p>Aqdk = &minus;Aqgk &minus; (AqATq )λk = 0, (12.12)
which leads to
</p>
<p>λk = &minus;(AqATq )&minus;1Aqgk (12.13)
and
</p>
<p>dk = &minus;[I &minus; ATq (AqATq )&minus;1Aq]gk = &minus;Pkgk. (12.14)
The matrix
</p>
<p>Pk = [I &minus; ATq (AqATq )&minus;1Aq] (12.15)
is called the projection matrix corresponding to the subspace M. Action by it on any
</p>
<p>vector yields the projection of that vector onto M. See Exercises 8 and 9 for other
</p>
<p>derivations of this result.
</p>
<p>We easily check that if dk � 0, then it is a direction of descent. Since gk + dk is
</p>
<p>orthogonal to dk, we have
</p>
<p>gTk dk = (g
T
k + d
</p>
<p>T
k &minus; dTk )dk = &minus;|dk |2.
</p>
<p>Thus if dk as computed from (12.14) turns out to be nonzero, it is a feasible direction
</p>
<p>of descent on the working surface.
</p>
<p>We next consider selection of the step size. As α is increased from zero, the
</p>
<p>point x + αd will initially remain feasible and the corresponding value of f will
</p>
<p>decrease. We find the length of the feasible segment of the line emanating from x
</p>
<p>and then minimize f over this segment. If the minimum occurs at the endpoint, a
</p>
<p>new constraint will become active and will be added to the working set.
</p>
<p>Next, consider the possibility that the projected negative gradient is zero. We
</p>
<p>have in that case
</p>
<p>&nabla; f (xk) + λ
T
k Aq = 0, (12.16)
</p>
<p>and the point xk satisfies the necessary conditions for a minimum on the working
</p>
<p>surface. If the components of λk corresponding to the active inequalities are all non-
</p>
<p>negative, then this fact together with (12.16) implies that the Karush-Kuhn-Tucker
</p>
<p>conditions for the original problem are satisfied at xk and the process terminates. In
</p>
<p>this case the λk found by projecting the negative gradient is essentially the Lagrange
</p>
<p>multiplier vector for the original problem (except that zero-valued multipliers must
</p>
<p>be appended for the inactive constraints).
</p>
<p>If, however, at least one of those components of λk is negative, it is possible, by
</p>
<p>relaxing the corresponding inequality, to move in a new direction to an improved</p>
<p/>
</div>
<div class="page"><p/>
<p>366 12 Primal Methods
</p>
<p>point. Suppose that λjk, the jth component of λk, is negative and the indexing is ar-
</p>
<p>ranged so that the corresponding constraint is the inequality aT
j
x � b j. We determine
</p>
<p>the new direction vector by relaxing the jth constraint and projecting the negative
</p>
<p>gradient onto the subspace determined by the remaining q&minus;1 active constraints. Let
Aq̄ denote the matrix Aq with row a j deleted. We have for some λ̄k
</p>
<p>&minus; gk = ATq λk (12.17)
&minus;gk = dk + ATq λk, (12.18)
</p>
<p>where dk is the projection of &minus;gk using Aq̄. It is immediately clear that dk � 0,
since otherwise (12.18) would be a special case of (12.17) with λjk = 0 which
</p>
<p>is impossible, since the rows of Aq are linearly independent. From our previous
</p>
<p>work we know that gT
k
</p>
<p>d̄k &lt; 0. Multiplying the transpose of (12.17) by dk and using
</p>
<p>Aq̄dk = 0 we obtain
</p>
<p>0 &gt; gTk dk = &minus;λjkaTj dk. (12.19)
</p>
<p>Since λjk &lt; 0 we conclude that a
T
j
dk &lt; 0. Thus the vector dk is not only a direction
</p>
<p>of descent, but it is a feasible direction, since aT
j
dk = 0, i &isin; W(xk), i � j, and
</p>
<p>aT
j
dk &lt; 0. Hence j can be dropped from W(xk).
</p>
<p>In summary, one step of the algorithm is as follows: Given a feasible point x
</p>
<p>1. Find the subspace of active constraints M, and form Aq, W(x).
</p>
<p>2. Calculate P = I &minus; ATq (AqATq )&minus;1Aq and d = &minus;P&nabla; f (x)T .
3. If d � 0, find α1 and α2 achieving, respectively,
</p>
<p>max{α : x + αd is feasible}
min{ f (x + αd) : 0 � α � α1}.
</p>
<p>Set x to x + α2d and return to (12.1).
</p>
<p>4. If d = 0, find λ = &minus;(AqATq )&minus;1Aq&nabla; f (x)T .
(a) If λ j � 0, for all j corresponding to active inequalities, stop; x satisfies the
</p>
<p>Karush-Kuhn-Tucker conditions.
</p>
<p>(b) Otherwise, delete the row from Aq corresponding to the inequality with the
</p>
<p>most negative component of λ (and drop the corresponding constraint from
</p>
<p>W(x)) and return to (12.2).
</p>
<p>The projection matrix need not be recomputed in its entirety at each new point.
</p>
<p>Since the set of active constraints in the working set changes by at most one con-
</p>
<p>straint at a time, it is possible to calculate one required projection matrix from the
</p>
<p>previous one by an updating procedure. (See Exercise 11.) This is an important fea-
</p>
<p>ture of the gradient projection method and greatly reduces the computation required
</p>
<p>at each step.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.4 The Gradient Projection Method 367
</p>
<p>Example. Consider the problem
</p>
<p>minimize x2
1
+ x2
</p>
<p>2
+ x2
</p>
<p>3
+ x2
</p>
<p>4
&minus; 2x1 &minus; 3x4
</p>
<p>subject to 2x1 + x2 + x3 + 4x4 = 7
</p>
<p>x1 + x2 + 2x3 + x4 = 6
</p>
<p>xi � 0, i = 1, 2, 3, 4.
</p>
<p>(12.20)
</p>
<p>Suppose that given the feasible point x = (2, 2, 1, 0) we wish to find the direction of
</p>
<p>the projected negative gradient. The active constraints are the two equalities and the
</p>
<p>inequality x4 � 0. Thus
</p>
<p>Aq =
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>2 1 1 4
</p>
<p>1 1 2 1
</p>
<p>0 0 0 1
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>, (12.21)
</p>
<p>and hence
</p>
<p>AqA
T
q =
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>22 9 4
</p>
<p>9 7 1
</p>
<p>4 1 1
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>.
</p>
<p>After considerable calculation we then find
</p>
<p>(AqA
T
q )
</p>
<p>&minus;1 =
1
</p>
<p>11
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>6 &minus;5 &minus;19
&minus;5 6 14
&minus;19 14 73
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>and finally
</p>
<p>P =
1
</p>
<p>11
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>1 &minus;3 1 0
&minus;3 9 &minus;3 0
1 &minus;3 1 0
0 0 0 0
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>. (12.22)
</p>
<p>The gradient at the point (2, 2, 1, 0) is g = (2, 4, 2,&minus;3) and hence we find
</p>
<p>d = &minus;Pg = 1
11
</p>
<p>(&minus;8, 24,&minus;8, 0),
</p>
<p>or normalizing by 8/11
</p>
<p>d = (&minus;1, 3,&minus;1, 0). (12.23)
It can be easily verified that movement in this direction does not violate the
</p>
<p>constraints.
</p>
<p>Nonlinear Constraints
</p>
<p>In extending the gradient projection method to problems of the form
</p>
<p>minimize f (x)
</p>
<p>subject to h(x) = 0, g(x) � 0,
(12.24)</p>
<p/>
</div>
<div class="page"><p/>
<p>368 12 Primal Methods
</p>
<p>the basic idea is that at a feasible point xk one determines the active constraints and
</p>
<p>projects the negative gradient onto the subspace tangent to the surface determined
</p>
<p>by these constraints. This vector, if it is nonzero, determines the direction for the
</p>
<p>next step. The vector itself, however, is not in general a feasible direction, since the
</p>
<p>surface may be curved as illustrated in Fig. 12.4. It is therefore not always possible
</p>
<p>to move along this projected negative gradient to obtain the next point.
</p>
<p>What is typically done in the face of this difficulty is essentially to search along
</p>
<p>a curve on the constraint surface, the direction of the curve being defined by the
</p>
<p>projected negative gradient. A new point is found in the following way: First, a
</p>
<p>move is made along the projected negative gradient to a point y. Then a move is
</p>
<p>made in the direction perpendicular to the tangent plane at the original point to a
</p>
<p>nearby feasible point on the working surface, as illustrated in Fig. 12.4. Once this
</p>
<p>point is found the value of the objective is determined. This is repeated with various
</p>
<p>y&rsquo;s until a feasible point is found that satisfies one of the standard descent criteria
</p>
<p>for improvement relative to the original point.
</p>
<p>Fig. 12.4 Gradient projection method
</p>
<p>This procedure of tentatively moving away from the feasible region and then
</p>
<p>coming back introduces a number of additional difficulties that require a series of
</p>
<p>interpolations and nonlinear equation solutions for their resolution. A satisfactory
</p>
<p>general routine implementing the gradient projection philosophy is therefore of ne-
</p>
<p>cessity quite complex. It is not our purpose here to elaborate on these details but
</p>
<p>simply to point out the general nature of the difficulties and the basic devices for
</p>
<p>surmounting them.
</p>
<p>One difficulty is illustrated in Fig. 12.5. If, after moving along the projected neg-
</p>
<p>ative gradient to a point y, one attempts to return to a point that satisfies the old
</p>
<p>active constraints, some inequalities that were originally satisfied may then be vio-
</p>
<p>lated. One must in this circumstance use an interpolation scheme to find a new point
</p>
<p>y along the negative gradient so that when returning to the active constraints no orig-
</p>
<p>inally nonactive constraint is violated. Finding an appropriate y is to some extent a
</p>
<p>trial and error process. Finally, the job of returning to the active constraints is itself</p>
<p/>
</div>
<div class="page"><p/>
<p>12.4 The Gradient Projection Method 369
</p>
<p>a nonlinear problem which must be solved with an iterative technique. Such a tech-
</p>
<p>nique is described below, but within a finite number of iterations, it cannot exactly
</p>
<p>reach the surface. Thus typically an error tolerance δ is introduced, and throughout
</p>
<p>the procedure the constraints are satisfied only to within δ.
</p>
<p>Computation of the projections is also more difficult in the nonlinear case. Lump-
</p>
<p>ing, for notational convenience, the active inequalities together with the equalities
</p>
<p>into h(xk), the projection matrix at xk is
</p>
<p>Pk = I &minus; &nabla;h(xk)T [&nabla;h(xk)&nabla;h(xk)T ]&minus;1&nabla;h(xk). (12.25)
</p>
<p>At the point xk this matrix can be updated to account for one more or one less
</p>
<p>constraint, just as in the linear case. When moving from xk to xk+1, however, &nabla;h
</p>
<p>will change and the new projection matrix cannot be found from the old, and hence
</p>
<p>this matrix must be recomputed at each step.
</p>
<p>Fig. 12.5 Interpolation to obtain feasible point
</p>
<p>The most important new feature of the method is the problem of returning to the
</p>
<p>feasible region from points outside this region. The type of iterative technique em-
</p>
<p>ployed is a common one in nonlinear programming, including interior-point meth-
</p>
<p>ods of linear programming, and we describe it here. The idea is, from any point near
</p>
<p>xk, to move back to the constraint surface in a direction orthogonal to the tangent
</p>
<p>plane at xk. Thus from a point y we seek a point of the form y+&nabla;h(xk)
Tα = y&lowast; such
</p>
<p>that h(y&lowast;) = 0. As shown in Fig. 12.6 such a solution may not always exist, but it
does for y sufficiently close to xk.
</p>
<p>To find a suitable first approximation to α, and hence to y&lowast;, we linearize the
equation at xk obtaining
</p>
<p>h(y + &nabla;h(xk)
Tα) ≃ h(y) + &nabla;h(xk)&nabla;h(xk)Tα, (12.26)
</p>
<p>the approximation being accurate for |α| and |y &minus; x| small. This motivates the first
approximation</p>
<p/>
</div>
<div class="page"><p/>
<p>370 12 Primal Methods
</p>
<p>α1 = &minus;[&nabla;h(xk)&nabla;h(xk)T ]&minus;1h(y) (12.27)
y1 = y &minus; &nabla;h(xk)T [&nabla;h(xk)&nabla;h(xk)T ]&minus;1h(y). (12.28)
</p>
<p>Substituting y1 for y and successively repeating the process yields the sequence {y j}
generated by
</p>
<p>y j+1 = y j &minus; &nabla;h(xk)T [&nabla;h(xk)&nabla;h(xk)T ]&minus;1h(y j), (12.29)
which, started close enough to xk and the constraint surface, will converge to a
</p>
<p>solution y&lowast;. We note that this process requires the same matrices as the projection
operation.
</p>
<p>The gradient projection method has been successfully implemented and has been
</p>
<p>found to be effective in solving general nonlinear programming problems. Success-
</p>
<p>ful implementation resolving the several difficulties introduced by the requirement
</p>
<p>of staying in the feasible region requires, as one would expect, some degree of skill.
</p>
<p>The true value of the method, however, can be determined only through an analysis
</p>
<p>of its rate of convergence.
</p>
<p>Fig. 12.6 Case in which it is impossible to return to surface
</p>
<p>12.5 Convergence Rate of the Gradient Projection Method
</p>
<p>An analysis that directly attacked the nonlinear version of the gradient projection
</p>
<p>method, with all of its iterative and interpolative devices, would quickly become
</p>
<p>monstrous. To obtain the asymptotic rate of convergence, however, it is not neces-
</p>
<p>sary to analyze this complex algorithm directly&mdash;instead it is sufficient to analyze an
</p>
<p>alternate simplified algorithm that asymptotically duplicates the gradient projection
</p>
<p>method near the solution. Through the introduction of this idealized algorithm we
</p>
<p>show that the rate of convergence of the gradient projection method is governed by
</p>
<p>the eigenvalue structure of the Hessian of the Lagrangian restricted to the constraint
</p>
<p>tangent subspace.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.5 Convergence Rate of the Gradient Projection Method 371
</p>
<p>Geodesic Descent
</p>
<p>For simplicity we consider first the problem having only equality constraints
</p>
<p>minimize f (x)
</p>
<p>subject to h(x) = 0.
(12.30)
</p>
<p>The constraints define a continuous surface Ω in En.
</p>
<p>In considering our own difficulties with this problem, owing to the fact that the
</p>
<p>surface is nonlinear thereby making directions of descent difficult to define, it is
</p>
<p>well to also consider the problem as it would be viewed by a small bug confined to
</p>
<p>the constraint surface who imagines it to be his total universe. To him the problem
</p>
<p>seems to be a simple one. It is unconstrained, with respect to his universe, and is
</p>
<p>only (n &minus; m)-dimensional. He would characterize a solution point as a point where
the gradient of f (as measured on the surface) vanishes and where the appropriate
</p>
<p>(n &minus; m)-dimensional Hessian of f is positive semidefinite. If asked to develop a
computational procedure for this problem, he would undoubtedly suggest, since he
</p>
<p>views the problem as unconstrained, the method of steepest descent. He would com-
</p>
<p>pute the gradient, as measured on his surface, and would move along what would
</p>
<p>appear to him to be straight lines.
</p>
<p>Exactly what the bug would compute as the gradient and exactly what he would
</p>
<p>consider as straight lines would depend basically on how distance between two
</p>
<p>points on his surface were measured. If, as is most natural, we assume that he in-
</p>
<p>herits his notion of distance from the one which we are using in En, then the path
</p>
<p>x(t) between two points x1 and x2 on his surface that minimizes
&int; x2
</p>
<p>x1
|ẋ(t)|dt would be
</p>
<p>considered a straight line by him. Such a curve, having minimum arc length between
</p>
<p>two given points, is called a geodesic.
</p>
<p>Returning to our own view of the problem, we note, as we have previously, that if
</p>
<p>we project the negative gradient onto the tangent plane of the constraint surface at a
</p>
<p>point xk, we cannot move along this projection itself and remain feasible. We might,
</p>
<p>however, consider moving along a curve which had the same initial heading as the
</p>
<p>projected negative gradient but which remained on the surface. Exactly which such
</p>
<p>curve to move along is somewhat arbitrary, but a natural choice, inspired perhaps
</p>
<p>by the considerations of the bug, is a geodesic. Specifically, at a given point on the
</p>
<p>surface, we would determine the geodesic curve passing through that point that had
</p>
<p>an initial heading identical to that of the projected negative gradient. We would then
</p>
<p>move along this geodesic to a new point on the surface having a lesser value of f .
</p>
<p>The idealized procedure then, which the bug would use without a second thought,
</p>
<p>and which we would use if it were computationally feasible (which it definitely is
</p>
<p>not), would at a given feasible point xk (see Fig. 12.7):
</p>
<p>1. Calculate the projection p of &minus;&nabla; f (xk)T onto the tangent plane at xk.
2. Find the geodesic, x(t), t � 0, of the constraint surface having x(0) = xk,
</p>
<p>ẋ(0) = p.
</p>
<p>3. Minimize f (x(t)) with respect to t � 0, obtaining tk and xk+1 = x(tk).</p>
<p/>
</div>
<div class="page"><p/>
<p>372 12 Primal Methods
</p>
<p>At this point we emphasize that this technique (which we refer to as geodesic de-
</p>
<p>scent) is proposed essentially for theoretical purposes only. It does, however, capture
</p>
<p>the main philosophy of the gradient projection method. Furthermore, as the step size
</p>
<p>of the methods go to zero, as it does near the solution point, the distance between
</p>
<p>the point that would be determined by the gradient projection method and the point
</p>
<p>found by the idealized method goes to zero even faster. Thus the asymptotic rates of
</p>
<p>convergence for the two methods will be equal, and it is, therefore, appropriate to
</p>
<p>concentrate on the idealized method only.
</p>
<p>Our bug confined to the surface would have no hesitation in estimating the rate
</p>
<p>of convergence of this method. He would simply express it in terms of the smallest
</p>
<p>and largest eigenvalues of the Hessian of f as measured on his surface. It should not
</p>
<p>be surprising, then, that we show that the asymptotic convergence ratio is
</p>
<p>(
</p>
<p>A &minus; a
A + a
</p>
<p>)2
</p>
<p>, (12.31)
</p>
<p>Fig. 12.7 Geodesic descent
</p>
<p>where a and A are, respectively, the smallest and largest eigenvalues of L, the Hes-
</p>
<p>sian of the Lagrangian, restricted to the tangent subspace M. This result parallels
</p>
<p>the convergence rate of the method of steepest descent, but with the eigenvalues
</p>
<p>determined from the same restricted Hessian matrix that is important in the general
</p>
<p>theory of necessary and sufficient conditions for constrained problems. This rate,
</p>
<p>which almost invariably arises when studying algorithms designed for constrained
</p>
<p>problems, will be referred to as the canonical rate.
</p>
<p>We emphasize again that, since this convergence ratio governs the convergence
</p>
<p>of a large family of algorithms, it is the formula itself rather than its numerical
</p>
<p>value that is important. For any given problem we do not suggest that this ratio be
</p>
<p>evaluated, since this would be extremely difficult. Instead, the potency of the result
</p>
<p>derives from the fact that fairly comprehensive comparisons among algorithms can
</p>
<p>be made, on the basis of this formula, that apply to general classes of problems
</p>
<p>rather than simply to particular problems.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.5 Convergence Rate of the Gradient Projection Method 373
</p>
<p>The remainder of this section is devoted to the analysis that is required to estab-
</p>
<p>lish the convergence rate. Since this analysis is somewhat involved and not crucial
</p>
<p>for an understanding of remaining material, some readers may wish to simply read
</p>
<p>the theorem statement and proceed to the next section.
</p>
<p>Geodesics
</p>
<p>Given the surface Ω = {x : h(x) = 0} &sub; En, a smooth curve, x(t) &isin; Ω, 0 � t � T
starting at x(0) and terminating at x(T ) that minimizes the total arc length
</p>
<p>&int; T
</p>
<p>0
</p>
<p>|ẋ(t)|dt
</p>
<p>with respect to all other such curves on Ω is said to be a geodesic connecting x(0)
</p>
<p>and x(T ).
</p>
<p>It is common to parameterize a geodesic x(t), 0 � t � T so that |ẋ(t)| = 1. The
parameter t is then itself the arc length. If the parameter t is also regarded as time,
</p>
<p>then this parameterization corresponds to moving along the geodesic curve with unit
</p>
<p>velocity. Parameterized in this way, the geodesic is said to be normalized. On any
</p>
<p>linear subspace of En geodesics are straight lines. On a three-dimensional sphere,
</p>
<p>the geodesics are arcs of great circles.
</p>
<p>It can be shown, using the calculus of variations, that any normalized geodesic
</p>
<p>on Ω satisfies the condition
</p>
<p>ẍ(t) = &nabla;hT (x(t))ω(t) (12.32)
</p>
<p>for some function ω taking values in Em. Geometrically, this condition says that
</p>
<p>if one moves along the geodesic curve with unit velocity, the acceleration at every
</p>
<p>point will be orthogonal to the surface. Indeed, this property can be regarded as
</p>
<p>the fundamental defining characteristic of a geodesic. To stay on the surface Ω, the
</p>
<p>geodesic must also satisfy the equation
</p>
<p>&nabla;h(x(t))ẋ(t) = 0, (12.33)
</p>
<p>since the velocity vector at every point is tangent to Ω. At a regular point x0 these
</p>
<p>two differential equations, together with the initial conditions x(0) = x0, ẋ(0) spec-
</p>
<p>ified, and |ẋ(0)| = 1, uniquely specify a curve x(t), t � 0 that can be continued as
long as points on the curve are regular. Furthermore, |ẋ(t)| = 1 for t � 0. Hence
geodesic curves emanate in every direction from a regular point. Thus, for example,
</p>
<p>at any point on a sphere there is a unique great circle passing through the point in a
</p>
<p>given direction.</p>
<p/>
</div>
<div class="page"><p/>
<p>374 12 Primal Methods
</p>
<p>Lagrangian and Geodesics
</p>
<p>Corresponding to any regular point x &isin; Ω we may define a corresponding Lagrange
multiplier λ(x) by calculating the projection of the gradient of f onto the tangent
</p>
<p>subspace at x, denoted M(x). The matrix that, when operating on a vector, projects
</p>
<p>it onto M(x) is
</p>
<p>P(x) = I &minus; &nabla;h(x)T [&nabla;h(x)&nabla;h(x)T ]&minus;1&nabla;h(x),
and it follows immediately that the projection of &nabla; f (x)T onto M(x) has the form
</p>
<p>y(x) = [&nabla; f (x) + λ(x)T&nabla;h(x)]T , (12.34)
</p>
<p>where λ(x) is given explicitly as
</p>
<p>λ(x)T = &minus;&nabla; f (x)&nabla;h(x)T [&nabla;h(x)&nabla;h(x)T ]&minus;1. (12.35)
</p>
<p>Thus, in terms of the Lagrangian function l(x, λ) = f (x) + λTh(x), the projected
</p>
<p>gradient is
</p>
<p>y(x) = lx(x, λ(x))
T . (12.36)
</p>
<p>If a local solution to the original problem occurs at a regular point x&lowast; &isin; Ω, then as
we know
</p>
<p>lx(x
&lowast;, λ(x&lowast;)) = 0, (12.37)
</p>
<p>which states that the projected gradient must vanish at x&lowast;. Defining L(x) = lxx(x, λ(x)) =
F(x) + λ(x)TH(x) we also know that at x&lowast; we have the second-order necessary con-
dition that L(x&lowast;) is positive semidefinite on M(x&lowast;); that is, zTL(x&lowast;)z � 0 for all
z &isin; M(x&lowast;). Equivalently, letting
</p>
<p>L(x) = P(x)L(x)P(x), (12.38)
</p>
<p>it follows that L(x&lowast;) is positive semidefinite.
We then have the following fundamental and simple result, valid along a geodesic.
</p>
<p>Proposition 1. Let x(t), 0 � t � T, be a geodesic on Ω. Then
</p>
<p>d
</p>
<p>dt
f (x(t)) = lx(x, λ(x))ẋ(t) (12.39)
</p>
<p>d2
</p>
<p>dt2
f (x(t)) = ẋ(t)TL(x(t))ẋ(t). (12.40)
</p>
<p>Proof. We have
</p>
<p>d
</p>
<p>dt
f (x(t)) = &nabla; f (x(t))ẋ(t) = lx(x, λ(x))ẋ(t),
</p>
<p>the second equality following from the fact that ẋ(t) &isin; M(x). Next,
</p>
<p>d2
</p>
<p>dt2
f (x(t)) = ẋ(t)TF(x(t))ẋ(t) + &nabla; f (x(t))ẍ(t). (12.41)</p>
<p/>
</div>
<div class="page"><p/>
<p>12.5 Convergence Rate of the Gradient Projection Method 375
</p>
<p>But differentiating the relation λTh(x(t)) = 0 twice, for fixed λ, yields
</p>
<p>ẋ(t)TλTH(x(t))ẋ(t) + λT&nabla;h(x(t))ẍ(t) = 0.
</p>
<p>Adding this to (12.41), we have
</p>
<p>d2
</p>
<p>dt2
f (x(t)) = ẋ(t)T (F + λTH)ẋ(t) + (&nabla; f (x) + λT&nabla;h(x))ẍ(t),
</p>
<p>which is true for any fixed λ. Setting λ = λ(x) determined as above, (&nabla;f + λT&nabla;h)T
</p>
<p>is in M(x) and hence orthogonal to ẍ(t), since x(t) is a normalized geodesic. This
</p>
<p>gives (12.40). �
</p>
<p>It should be noted that we proved a simplified version of this result in Chap. 11.
</p>
<p>There the result was given only for the optimal point x&lowast;, although it was valid for
any curve. Here we have shown that essentially the same result is valid at any point
</p>
<p>provided that we move along a geodesic.
</p>
<p>Rate of Convergence
</p>
<p>We now prove the main theorem regarding the rate of convergence. We assume
</p>
<p>that all functions are three times continuously differentiable and that every point
</p>
<p>in a region near the solution x&lowast; is regular. This theorem only establishes the rate
of convergence and not convergence itself so for that reason the stated hypotheses
</p>
<p>assume that the method of geodesic descent generates a sequence {xk} converging
to x&lowast;.
</p>
<p>Theorem. Theorem. Let x&lowast; be a local solution to the problem (12.30) and suppose that
A and a &gt; 0 are, respectively, the largest and smallest eigenvalues of L(x&lowast;) restricted to
the tangent subspace M(x&lowast;). If {xk} is a sequence generated by the method of geodesic
descent that converges to x&lowast;, then the sequence of objective values { f (xk)} converges to
f (x&lowast;) linearly with a ratio no greater than [(A &minus; a)/(A + a)]2 .
</p>
<p>Proof. Without loss of generality we may assume f (x&lowast;) = 0. Given a point xk it
will be convenient to define its distance from the solution point x&lowast; as the arc length
of the geodesic connecting x&lowast; and xk. Thus if x(t) is a parameterized version of the
geodesic with x(0) = x&lowast;, |ẋ(t)| = 1, x(T ) = xk, then T is the distance of xk from
x&lowast;. Associated with such a geodesic we also have the family y(t), 0 � t � T , of
corresponding projected gradients y(t) = lx(x, λ(x))
</p>
<p>T , and Hessians L(t) = L(x(t)).
</p>
<p>We write yk = y(xk), Lk = L(xk).
</p>
<p>We now derive an estimate for f (xk). Using the geodesic discussed above we can
</p>
<p>write (setting ẋk = ẋ(T ))
</p>
<p>f (x&lowast;) &minus; f (xk) = &minus; f (xk) = &minus;yTk ẋkT +
1
</p>
<p>2
T 2ẋTk Lkẋk + o(T
</p>
<p>2), (12.42)</p>
<p/>
</div>
<div class="page"><p/>
<p>376 12 Primal Methods
</p>
<p>which follows from Proposition 1. We also have
</p>
<p>yk = &minus;y(x&lowast;) + y(xk) = ẏkT + o(T ). (12.43)
</p>
<p>But differentiating (12.34) we obtain
</p>
<p>ẏk = Lkẋk + &nabla;h(xk)
T λ̇
</p>
<p>T
</p>
<p>k , (12.44)
</p>
<p>and hence if Pk is the projection matrix onto M(xk) = Mk, we have
</p>
<p>Pkẏk = PkLkẋk. (12.45)
</p>
<p>Multiplying (12.43) by Pk and accounting for Pkyk = yk we have
</p>
<p>PkẏkT = yk + o(T ). (12.46)
</p>
<p>Substituting (12.45) into this we obtain
</p>
<p>PkLkẋkT = yk + o(T ).
</p>
<p>Since Pkẋk = ẋk we have, defining Lk = PkLkPk,
</p>
<p>LkẋkT = yk + o(T ). (12.47)
</p>
<p>The matrix Lk is related to LMk , the restriction of Lk to Mk, the only difference
</p>
<p>being that while LMk is defined only on Mk, the matrix Lk is defined on all of E
n
</p>
<p>but in such a way that it agrees with LMk on Mk and is zero on M
&perp;
k
</p>
<p>. The matrix Lk
is not invertible, but for yk &isin; Mk there is a unique solution z &isin; Mk to the equation
Lkz = yk which we denote
</p>
<p>&dagger; Lk
&minus;1
</p>
<p>yk. With this notation we obtain from (12.47)
</p>
<p>ẋkT = Lk
&minus;1
</p>
<p>yk + o(T ). (12.48)
</p>
<p>Substituting this last result into (12.42) and accounting for |yk | = O(T ) (see (12.43))
we have
</p>
<p>f (xk) =
1
</p>
<p>2
yTk L
</p>
<p>&minus;1
k yk + o(T
</p>
<p>2), (12.49)
</p>
<p>which expresses the objective value at xk in terms of the projected gradient.
</p>
<p>Since |ẋk | = 1 and since Lk &rarr; L
&lowast;
</p>
<p>as xk &rarr; x&lowast;, we see from (12.47) that
</p>
<p>o(T ) + aT � |yk| � AT + o(T ), (12.50)
</p>
<p>which means that not only do we have |yk | = O(T ), which was known before, but
also |yk | � o(T ). We may therefore write our estimate (12.49) in the alternate form
</p>
<p>&dagger; Actually a more standard procedure is to define the pseudoinverse L
&dagger;
k , and then z = L
</p>
<p>&dagger;
kyk.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.5 Convergence Rate of the Gradient Projection Method 377
</p>
<p>f (xk) =
1
</p>
<p>2
yTk Lk
</p>
<p>&minus;1
yk
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>1 +
o(T 2)
</p>
<p>yT
k
</p>
<p>L
&minus;1
k yk
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>, (12.51)
</p>
<p>and since o(T 2) � yT
k
</p>
<p>L
&minus;1
k yk = O(T
</p>
<p>2), we have
</p>
<p>f (xk) =
1
</p>
<p>2
yTk L
</p>
<p>&minus;1
k yk(1 + O(T )), (12.52)
</p>
<p>which is the desired estimate.
</p>
<p>Next, we estimate f (xk+1) in terms of f (xk). Given xk now let x(t), t � 0, be
</p>
<p>the normalized geodesic emanating from xk &equiv; x(0) in the direction of the negative
projected gradient, that is,
</p>
<p>ẋ(0) &equiv; ẋk = &minus;yk/|yk |.
Then
</p>
<p>f (x(t)) = f (xk) + ty
T
k ẋk +
</p>
<p>t2
</p>
<p>2
ẋTk Lkẋk + o(t
</p>
<p>2). (12.53)
</p>
<p>This is minimized at
</p>
<p>tk = &minus;
yT
k
</p>
<p>ẋk
</p>
<p>ẋT
k
</p>
<p>Lkẋk
+ o(tk). (12.54)
</p>
<p>In view of (12.50) this implies that tk = O(T ), tk � o(T ). Thus tk goes to zero at
</p>
<p>essentially the same rate as T . Thus we have
</p>
<p>f (xk+1) = f (xk) &minus;
1
</p>
<p>2
</p>
<p>(yT
k
</p>
<p>ẋk)
2
</p>
<p>ẋT
k
</p>
<p>Lkẋk
+ o(T 2). (12.55)
</p>
<p>Using the same argument as before we can express this as
</p>
<p>f (xk) &minus; f (xk+1) =
1
</p>
<p>2
</p>
<p>(yT
k
</p>
<p>yk)
2
</p>
<p>yT
k
</p>
<p>Lkyk
(1 + O(T )), (12.56)
</p>
<p>which is the other required estimate.
</p>
<p>Finally, dividing (12.56) by (12.52) we find
</p>
<p>f (xk) &minus; f (xk+1)
f (xk)
</p>
<p>=
(yT
</p>
<p>k
yk)
</p>
<p>2(1 + O(T ))
</p>
<p>(yT
k
</p>
<p>Lkyk)(y
T
k
</p>
<p>Lk
&minus;1
</p>
<p>yk)
, (12.57)
</p>
<p>and thus
</p>
<p>f (xk+1) =
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>1 &minus;
(yT
</p>
<p>k
yk)
</p>
<p>2(1 + O(T ))
</p>
<p>(yT
k
</p>
<p>Lkyk)(y
T
k
</p>
<p>Lk
&minus;1
</p>
<p>yk)
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>f (xk). (12.58)
</p>
<p>Using the fact that Lk &rarr; L&lowast; and applying the Kantorovich inequality leads to
</p>
<p>f (xk+1) �
</p>
<p>[
</p>
<p>(
</p>
<p>A &minus; a
A + a
</p>
<p>)2
</p>
<p>+ O(T )
</p>
<p>]
</p>
<p>f (xk). � (12.59)</p>
<p/>
</div>
<div class="page"><p/>
<p>378 12 Primal Methods
</p>
<p>Problems with Inequalities
</p>
<p>The idealized version of gradient projection could easily be extended to prob-
</p>
<p>lems having nonlinear inequalities as well as equalities by following the pattern of
</p>
<p>Sect. 12.4. Such an extension, however, has no real value, since the idealized scheme
</p>
<p>cannot be implemented. The idealized procedure was devised only as a technique
</p>
<p>for analyzing the asymptotic rate of convergence of the analytically more complex,
</p>
<p>but more practical, gradient projection method.
</p>
<p>The analysis of the idealized version of gradient projection given above, never-
</p>
<p>theless, does apply to problems having inequality as well as equality constraints. If
</p>
<p>a computationally feasible procedure is employed that avoids jamming and does not
</p>
<p>bounce on and off constraint boundaries an infinite number of times, then near the
</p>
<p>solution the active constraints will remain fixed. This means that near the solution
</p>
<p>the method acts just as if it were solving a problem having the active constraints as
</p>
<p>equality constraints. Thus the asymptotic rate of convergence of the gradient projec-
</p>
<p>tion method applied to a problem with inequalities is also given by (12.59) but with
</p>
<p>L(x&lowast;) and M(x&lowast;) (and hence a and A) determined by the active constraints at the so-
lution point x&lowast;. In every case, therefore, the rate of convergence is determined by the
eigenvalues of the same restricted Hessian that arises in the necessary conditions.
</p>
<p>12.6 The Reduced Gradient Method
</p>
<p>From a computational viewpoint, the reduced gradient method, discussed in this
</p>
<p>section and the next, is closely related to the simplex method of linear programming
</p>
<p>in that the problem variables are partitioned into basic and nonbasic groups. From
</p>
<p>a theoretical viewpoint, the method can be shown to behave very much like the
</p>
<p>gradient projection method.
</p>
<p>Linear Constraints
</p>
<p>Consider the problem
minimize f (x)
</p>
<p>subject to Ax = b, x � 0,
(12.60)
</p>
<p>where x &isin; En, b &isin; Em, A is m &times; n, and f is a function in C2. The constraints are
expressed in the format of the standard form of linear programming. For simplic-
</p>
<p>ity of notation it is assumed that each variable is required to be non-negative&mdash;if
</p>
<p>some variables were free, the procedure (but not the notation) would be somewhat
</p>
<p>simplified.
</p>
<p>We invoke the nondegeneracy assumptions that every collection of m columns
</p>
<p>from A is linearly independent and every basic solution to the constraints has m</p>
<p/>
</div>
<div class="page"><p/>
<p>12.6 The Reduced Gradient Method 379
</p>
<p>strictly positive variables. With these assumptions any feasible solution will have at
</p>
<p>most n&minus;m variables taking the value zero. Given a vector x satisfying the constraints,
we partition the variables into two groups: x = (y, z) where y has dimension m and
</p>
<p>z has dimension n &minus; m. This partition is formed in such a way that all variables in
y are strictly positive (for simplicity of notation we indicate the basic variables as
</p>
<p>being the first m components of x but, of course, in general this will not be so). With
</p>
<p>respect to the partition, the original problem can be expressed as
</p>
<p>minimize f (y, z) (12.61a)
</p>
<p>subject to By + Cz = b (12.61b)
</p>
<p>y � 0, z � 0, (12.61c)
</p>
<p>where, of course, A = [B, C]. We can regard z as consisting of the indepen-
</p>
<p>dent variables and y the dependent variables, since if z is specified, (12.61b) can
</p>
<p>be uniquely solved for y. Furthermore, a small change ∆z from the original value
</p>
<p>that leaves z+∆z nonnegative will, upon solution of (12.61b), yield another feasible
</p>
<p>solution, since y was originally taken to be strictly positive and thus y + ∆y will
</p>
<p>also be positive for small ∆y. We may therefore move from one feasible solution to
</p>
<p>another by selecting a ∆z and moving z on the line z + α∆z, α � 0. Accordingly,
</p>
<p>y will move along a corresponding line y + α∆y. If in moving this way some vari-
</p>
<p>able becomes zero, a new inequality constraint becomes active. If some independent
</p>
<p>variable becomes zero, a new direction ∆z must be chosen. If a dependent (basic)
</p>
<p>variable becomes zero, the partition must be modified. The zero-valued basic vari-
</p>
<p>able is declared independent and one of the strictly positive independent variables
</p>
<p>is made dependent. Operationally, this interchange will be associated with a pivot
</p>
<p>operation.
</p>
<p>The idea of the reduced gradient method is to consider, at each stage, the problem
</p>
<p>only in terms of the independent variables. Since the vector of dependent variables
</p>
<p>y is determined through the constraints (12.61b) from the vector of independent
</p>
<p>variables z, the objective function can be considered to be a function of z only.
</p>
<p>Hence a simple modification of steepest descent, accounting for the constraints, can
</p>
<p>be executed. The gradient with respect to the independent variables z (the reduced
</p>
<p>gradient) is found by evaluating the gradient of f (B&minus;1b &minus; B&minus;1 Cz z). It is equal to
</p>
<p>rT = &nabla;z f (y, z) &minus; &nabla;y f (y, z)B&minus;1C. (12.62)
</p>
<p>It is easy to see that a point (y, z) satisfies the first-order necessary conditions for
</p>
<p>optimality if and only if
</p>
<p>ri = 0 for all zi &gt; 0
</p>
<p>ri � 0 for all zi = 0.
</p>
<p>In the active set form of the reduced gradient method the vector z is moved in
</p>
<p>the direction of the reduced gradient on the working surface. Thus at each step, a
</p>
<p>direction of the form</p>
<p/>
</div>
<div class="page"><p/>
<p>380 12 Primal Methods
</p>
<p>∆zi =
</p>
<p>{
</p>
<p>&minus;ri, i � W(z)
0, i &isin; W(z)
</p>
<p>is determined and a descent is made in this direction. The working set is augmented
</p>
<p>whenever a new variable reaches zero; if it is a basic variable, a new partition is also
</p>
<p>formed. If a point is found where ri = 0 for all i � W(z) (representing a vanishing
</p>
<p>reduced gradient on the working surface) but r j &lt; 0 for some j &isin; W(z), then j is
deleted from W(z) as in the standard active set strategy.
</p>
<p>It is possible to avoid the pure active set strategy by moving away from our active
</p>
<p>constraint whenever that would lead to an improvement, rather than waiting until an
</p>
<p>exact minimum on the working surface is found. Indeed, this type of procedure is
</p>
<p>often used in practice. One version progresses by moving the vector z in the direc-
</p>
<p>tion of the overall negative reduced gradient, except that zero-valued components of
</p>
<p>z that would thereby become negative are held at zero. One step of the procedure is
</p>
<p>as follows:
</p>
<p>1. Let ∆zi =
</p>
<p>{
</p>
<p>&minus;ri if ri &lt; 0 or zi &gt; 0
0 otherwise.
</p>
<p>2. If ∆z is zero, stop; the current point is a solution. Otherwise, find ∆y =
</p>
<p>&minus;B&minus;1C∆z.
3. Find α1, α2, α3 achieving, respectively,
</p>
<p>max{α : y + α∆y � 0}
max{α : z + α∆z � 0}
min{ f (x + α∆x) : 0 � α � α1, 0 � α � α2}
</p>
<p>Let x = x + α3∆x.
</p>
<p>4. If α3 &lt; α1, return to (12.1). Otherwise, declare the vanishing variable in the
</p>
<p>dependent set independent and declare a strictly positive variable in the inde-
</p>
<p>pendent set dependent. Update B and C.
</p>
<p>Example. We consider the example presented in Sect. 12.4 where the projected
</p>
<p>negative gradient was computed:
</p>
<p>minimize x21 + x
2
2 + x
</p>
<p>2
3 + x
</p>
<p>2
4 &minus; 2x1 &minus; 3x4
</p>
<p>subject to 2x1 + x2 + x3 + 4x4 = 7
</p>
<p>x1 + x2 + 2x3 + x4 = 6
</p>
<p>xi � 0, i = 1, 2, 3, 4.
</p>
<p>We are given the feasible point x = (2, 2, 1, 0). We may select any two of the strictly
</p>
<p>positive variables to be the basic variables. Suppose y = (x1, x2) is selected. In
</p>
<p>standard form the constraints are then
</p>
<p>x1 + 0 &minus; x3 + 3x4 = 1
0 + x2 + 3x3 &minus; 2x4 = 5
xi � 0, i = 1, 2, 3, 4.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.6 The Reduced Gradient Method 381
</p>
<p>The gradient at the current point is g = (2, 4, 2,&minus;3). The corresponding reduced
gradient (with respect to z = (x3, x4)) is then found by pricing-out in the usual
</p>
<p>manner. The situation at the current point can then be summarized by the tableau
</p>
<p>In this solution x3 and x4 would be increased together in a ratio of eight to one.
</p>
<p>As they increase, x1 and x2 would follow in such a way as to keep the constraints
</p>
<p>satisfied. Overall, in E4, the implied direction of movement is thus
</p>
<p>d = (5,&minus;22, 8, 1).
</p>
<p>If the reader carefully supplies the computational details not shown in the presenta-
</p>
<p>tion of the example as worked here and in Sect. 12.4, he will undoubtedly develop a
</p>
<p>considerable appreciation for the relative simplicity of the reduced gradient method.
</p>
<p>It should be clear that the reduced gradient method can, as illustrated in the ex-
</p>
<p>ample above, be executed with the aid of a tableau. At each step the tableau of
</p>
<p>constraints is arranged so that an identity matrix appears over the m dependent vari-
</p>
<p>ables, and thus the dependent variables can be easily calculated from the values of
</p>
<p>the independent variables. The reduced gradient at any step is calculated by evaluat-
</p>
<p>ing the n-dimensional gradient and &ldquo;pricing out&rdquo; the dependent variables just as the
</p>
<p>reduced cost vector is calculated in linear programming. And when the partition of
</p>
<p>basic and non-basic variables must be changed, a simple pivot operation is all that
</p>
<p>is required.
</p>
<p>Global Convergence
</p>
<p>The perceptive reader will note the direction finding algorithm that results from the
</p>
<p>second form of the reduced gradient method is not closed, since slight movement
</p>
<p>away from the boundary of an inequality constraint can cause a sudden change in
</p>
<p>the direction of search. Thus one might suspect, and correctly so, that this method
</p>
<p>is subject to jamming. However, a trivial modification will yield a closed mapping;
</p>
<p>and hence global convergence. This is discussed in Exercise 19.</p>
<p/>
</div>
<div class="page"><p/>
<p>382 12 Primal Methods
</p>
<p>Nonlinear Constraints
</p>
<p>The generalized reduced gradient method solves nonlinear programming problems
</p>
<p>in the standard form
</p>
<p>minimize f (x)
</p>
<p>subject to h(x) = 0, a � x � b,
</p>
<p>where h(x) is of dimension m. A general nonlinear programming problem can al-
</p>
<p>ways be expressed in this form by the introduction of slack variables, if required,
</p>
<p>and by allowing some components of a and b to take on the values +&infin; or &minus;&infin;, if
necessary.
</p>
<p>In a manner quite analogous to that of the case of linear constraints, we introduce
</p>
<p>a nondegeneracy assumption that, at each point x, hypothesizes the existence of a
</p>
<p>partition of x into x = (y, z) having the following properties:
</p>
<p>(i) y is of dimension m, and z is of dimension n &minus; m.
(ii) If a = (ay, az) and b = (by, bz) are the corresponding partitions of a, b, then
</p>
<p>ay &lt; y &lt; by.
</p>
<p>(iii) The m &times; m matrix &nabla;yh(y, z) is nonsingular at x = (y, z).
Again y and z are referred to as the vectors of dependent and independent vari-
</p>
<p>ables, respectively.
</p>
<p>The reduced gradient (with respect to z) is in this case:
</p>
<p>rT = &nabla;z f (y, z) + λ
T
&nabla;zh(y, z),
</p>
<p>where λ satisfies
</p>
<p>&nabla;y f (y, z) + λ
T
&nabla;yh(y, z) = 0.
</p>
<p>Equivalently, we have
</p>
<p>rT = &nabla;z f (y, z) &minus; &nabla;y f (y, z)[&nabla;yh(y, z)]&minus;1&nabla;zh(y, z). (12.63)
</p>
<p>The actual procedure is roughly the same as for linear constraints in that moves
</p>
<p>are taken by changing z in the direction of the negative reduced gradient (with
</p>
<p>components of z on their boundary held fixed if the movement would violate the
</p>
<p>bound). The difference here is that although z moves along a straight line as before,
</p>
<p>the vector of dependent variables y must move nonlinearly to continuously satisfy
</p>
<p>the equality constraints. Computationally, this is accomplished by first moving lin-
</p>
<p>early along the tangent to the surface defined by z &rarr; z + ∆z, y &rarr; y + ∆y with
∆y = &minus;[&nabla;yh]&minus;1&nabla;zh∆z. Then a correction procedure, much like that employed in the
gradient projection method, is used to return to the constraint surface and the mag-
</p>
<p>nitude bounds on the dependent variables are checked for feasibility. As with the
</p>
<p>gradient projection method, a feasibility tolerance must be introduced to acknowl-
</p>
<p>edge the impossibility of returning exactly to the constraint surface. An example
</p>
<p>corresponding to n = 3, m = 1, a = 0, b = +&infin; is shown in Fig. 12.8.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.7 Convergence Rate of the Reduced Gradient Method 383
</p>
<p>To return to the surface once a tentative move along the tangent is made, an
</p>
<p>iterative scheme is employed. If the point xk was the point at the previous step, then
</p>
<p>from any point x = (v, w) near xk one gets back to the constraint surface by solving
</p>
<p>the nonlinear equation
</p>
<p>h(y,w) = 0 (12.64)
</p>
<p>for y (with w fixed). This is accomplished through the iterative process
</p>
<p>y j+1 = y j &minus; [&nabla;yh(xk)]&minus;1h(y j, w), (12.65)
</p>
<p>which, if started close enough to xk, will produce {y j} with y j &rarr; y, solving (12.64).
The reduced gradient method suffers from the same basic difficulties as the gradi-
</p>
<p>ent projection method, but as with the latter method, these difficulties can all be more
</p>
<p>or less successfully resolved. Computation is somewhat less complex in the case of
</p>
<p>the reduced gradient method, because rather than compute with [&nabla;h(x)&nabla;h(x)T ]&minus;1 at
each step, the matrix [&nabla;yh(y, z)]
</p>
<p>&minus;1 is used.
</p>
<p>Fig. 12.8 Reduced gradient method
</p>
<p>12.7 Convergence Rate of the Reduced Gradient Method
</p>
<p>As argued before, for purposes of analyzing the rate of convergence, it is sufficient
</p>
<p>to consider the problem having only equality constraints
</p>
<p>minimize f (x)
</p>
<p>subject to h(x) = 0.
(12.66)
</p>
<p>We then regard the problem as being defined over a surface Ω of dimension
</p>
<p>n &minus; m. At this point it is again timely to consider the view of our bug, who lives on
this constraint surface. Invariably, he continues to regard the problem as extremely
</p>
<p>elementary, and indeed would have little appreciation for the complexity that seems</p>
<p/>
</div>
<div class="page"><p/>
<p>384 12 Primal Methods
</p>
<p>to face us. To him the problem is an unconstrained problem in n&minus;m dimensions not,
as we see it, a constrained problem in n dimensions. The bug will tenaciously hold
</p>
<p>to the method of steepest descent. We can emulate him provided that we know how
</p>
<p>he measures distance on his surface and thus how he calculates gradients and what
</p>
<p>he considers to be straight lines.
</p>
<p>Rather than imagine that the measure of distance on his surface is the one that
</p>
<p>would be inherited from us in n dimensions, as we did when studying the gradient
</p>
<p>projection method, we, in this instance, follow the construction shown in Fig. 12.9.
</p>
<p>In our n-dimensional space, n &minus; m coordinates are selected as independent vari-
ables in such a way that, given their values, the values of the remaining (dependent)
</p>
<p>variables are determined by the surface. There is already a coordinate system in the
</p>
<p>space of independent variables, and it can be used on the surface by projecting it par-
</p>
<p>allel to the space of the remaining dependent variables. Thus, an arc on the surface is
</p>
<p>considered to be straight if its projection onto the space of independent variables is a
</p>
<p>segment of a straight line. With this method for inducing a geometry on the surface,
</p>
<p>the bug&rsquo;s notion of steepest descent exactly coincides with an idealized version of
</p>
<p>the reduced gradient method.
</p>
<p>Fig. 12.9 Induced coordinate system
</p>
<p>In the idealized version of the reduced gradient method for solving (12.66), the
</p>
<p>vector x is partitioned as x = (y, z) where y &isin; Em, z &isin; En&minus;m. It is assumed that the
m &times; m matrix &nabla;yh(y, z) is nonsingular throughout a given region of interest. (With
respect to the more general problem, this region is a small neighborhood around the
</p>
<p>solution where it is not necessary to change the partition.) The vector y is regarded
</p>
<p>as an implicit function of z through the equation
</p>
<p>h(y(z), z) = 0. (12.67)
</p>
<p>The ordinary method of steepest descent is then applied to the function q(z) =
</p>
<p>f (y(z), z). We note that the gradient rT of this function is given by (12.63).
</p>
<p>Since the method is really just the ordinary method of steepest descent with re-
</p>
<p>spect to z, the rate of convergence is determined by the eigenvalues of the Hessian
</p>
<p>of the function q at the solution. We therefore turn to the question of evaluating this
</p>
<p>Hessian.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.7 Convergence Rate of the Reduced Gradient Method 385
</p>
<p>Denote by Y(z) the first derivatives of the implicit function y(z), that is, Y(z) &equiv;
&nabla;zy(z). Explicitly,
</p>
<p>Y(z) = &minus;[&nabla;yh(y, z)]&minus;1&nabla;zh(y, z). (12.68)
For any λ &isin; Em we have
</p>
<p>q(z) = f (y(z), z) = f (y(z), z) + λTh(y(z), z). (12.69)
</p>
<p>Thus
</p>
<p>&nabla;q(z) = [&nabla;y f (y, z) + λ
T
&nabla;yh(y, z)]Y(z) + &nabla;z f (y, z) + λ
</p>
<p>T
&nabla;zh(y, z). (12.70)
</p>
<p>Now if at a given point x&lowast; = (y&lowast;, z&lowast;) = (y(z&lowast;), z&lowast;), we let λ satisfy
</p>
<p>&nabla;y f (y
&lowast;, z&lowast;) + λT&nabla;yh(y
</p>
<p>&lowast;, z&lowast;) = 0; (12.71)
</p>
<p>then introducing the Lagrangian l(y, z, λ) = f (y, z) + λTh(y, z), we obtain by
</p>
<p>differentiating (12.70)
</p>
<p>&nabla;
2q(z&lowast;) = Y(z&lowast;)T&nabla;2yyl(y
</p>
<p>&lowast;, z&lowast;)Y(z&lowast;) + &nabla;2zyl(y
&lowast;, z&lowast;)Y(z&lowast;)
</p>
<p>+Y(z&lowast;)T&nabla;2yzl(y
&lowast;, z&lowast;) + &nabla;2zzl(y
</p>
<p>&lowast;, z&lowast;). (12.72)
</p>
<p>Or defining the n &times; (n &minus; m) matrix
</p>
<p>C =
</p>
<p>[
</p>
<p>Y(z&lowast;)
</p>
<p>I
</p>
<p>]
</p>
<p>, (12.73)
</p>
<p>where I is the (n &minus; m) &times; (n &minus; m) identity, we have
</p>
<p>Q &equiv; &nabla;2q(z&lowast;) = CTL(x&lowast;)C. (12.74)
</p>
<p>The matrix L(x&lowast;) is the n &times; n Hessian of the Lagrangian at x&lowast;, and &nabla;2q(z&lowast;) is an
(n &minus; m) &times; (n &minus; m) matrix that is a restriction of L(x&lowast;) to the tangent subspace M,
but it is not the usual restriction. We summarize our conclusion with the following
</p>
<p>theorem.
</p>
<p>Theorem. Let x&lowast; be a local solution of problem (12.66). Suppose that the idealized reduced
gradient method produces a sequence {xk} converging to x&lowast; and that the partition x = (y, z)
is used throughout the tail of the sequence. Let L be the Hessian of the Lagrangian at x&lowast; and
define the matrix C by (12.73) and (12.68). Then the sequence of objective values { f (xk)}
converges to f (x&lowast;) linearly with a ratio no greater than [(B &minus; b)/(B + b)]2 where b and B
are, respectively, the smallest and largest eigenvalues of the matrix Q = CTLC.
</p>
<p>To compare the matrix CTLC with the usual restriction of L to M that determines
</p>
<p>the convergence rate of most methods, we note that the n &times; (n &minus; m) matrix C maps
∆z &isin; En&minus;m into (∆y, ∆z) &isin; En lying in the tangent subspace M; that is, &nabla;yh∆y +
&nabla;zh∆z = 0. Thus the columns of C form a basis for the subspace M. Next note that
</p>
<p>the columns of the matrix
</p>
<p>E = C(CTC)&minus;1/2 (12.75)</p>
<p/>
</div>
<div class="page"><p/>
<p>386 12 Primal Methods
</p>
<p>form an orthonormal basis for M, since each column of E is just a linear combina-
</p>
<p>tion of columns of C and by direct calculation we see that ETE = I. Thus by the
</p>
<p>procedure described in Sect. 11.6 we see that a representation for the usual restric-
</p>
<p>tion of L to M is
</p>
<p>LM = (C
TC)&minus;1/2CTLC(CTC)&minus;1/2. (12.76)
</p>
<p>Comparing (12.76) with (12.74) we deduce that
</p>
<p>Q = (CTC)1/2LM(C
TC)1/2. (12.77)
</p>
<p>This means that the Hessian matrix for the reduced gradient method is the restriction
</p>
<p>of L to M but pre- and post-multiplied by a positive definite symmetric matrix.
</p>
<p>The eigenvalues of Q depend on the exact nature of C as well as LM . Thus, the
</p>
<p>rate of convergence of the reduced gradient method is not coordinate independent
</p>
<p>but depends strongly on just which variables are declared as independent at the final
</p>
<p>stage of the process. The convergence rate can be either faster or slower than that
</p>
<p>of the gradient projection method. In general, however, if C is well-behaved (that
</p>
<p>is, well-conditioned), the ratio of eigenvalues for the reduced gradient method can
</p>
<p>be expected to be the same order of magnitude as that of the gradient projection
</p>
<p>method. If, however, C should be ill-conditioned, as would arise in the case where
</p>
<p>the implicit equation h(y, z) = 0 is itself ill-conditioned, then it can be shown that
</p>
<p>the eigenvalue ratio for the reduced gradient method will most likely be considerably
</p>
<p>worsened. This suggests that care should be taken to select a set of basic variables y
</p>
<p>that leads to a well-behaved C matrix.
</p>
<p>Example (The Hanging Chain Problem). Consider again the hanging chain prob-
</p>
<p>lem discussed in Sect. 11.4. This problem can be used to illustrate a wide assortment
</p>
<p>of theoretical principles and practical techniques. Indeed, a study of this example
</p>
<p>clearly reveals the predictive power that can be derived from an interplay of theory
</p>
<p>and physical intuition.
</p>
<p>The problem is
</p>
<p>minimize
n
&sum;
</p>
<p>i=1
(n &minus; i + 0.5)yi
</p>
<p>subject to
n
&sum;
</p>
<p>i=1
yi = 0
</p>
<p>n
&sum;
</p>
<p>i=1
</p>
<p>&radic;
</p>
<p>1 &minus; y2
i
= 16,
</p>
<p>where in the original formulation n = 20.
</p>
<p>This problem has been solved numerically by the reduced gradient method.&lowast; An
initial feasible solution was the triangular shape shown in Fig. 12.10a with
</p>
<p>&lowast; The exact solution is obviously symmetric about the center of the chain, and hence the problem
could be reduced to having ten links and only one constraint. However, this symmetry disappears
if the first constraint value is specified as nonzero. Therefore for generality we solve the full chain
problem.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.7 Convergence Rate of the Reduced Gradient Method 387
</p>
<p>yi =
</p>
<p>{
</p>
<p>&minus;0.6, 1 � i � 10
0.6, 11 � i � 20.
</p>
<p>Table 12.1 Results of original chain problem
</p>
<p>Iteration Value Solution (1/2 of chain)
</p>
<p>0 &minus;60.00000 y1 = &minus;0.8148260
10 &minus;66.47610 y2 = &minus;0.7826505
20 &minus;66.52180 y3 = &minus;0.7429208
30 &minus;66.53595 y4 = &minus;0.6930959
40 &minus;66.54154 y5 = &minus;0.6310976
50 &minus;66.54537 y6 = &minus;0.5541078
60 &minus;66.54628 y7 = &minus;0.4597160
69 &minus;66.54659 y8 = &minus;0.3468334
70 &minus;66.54659 y9 = &minus;0.2169879
</p>
<p>y10 = &minus;0.07492541
Lagrange multipliers &minus;9.993817, &minus;6.763148
</p>
<p>The results obtained from a reduced gradient package are shown in Table 12.1.
</p>
<p>Note that convergence is obtained in approximately 70 iterations.
</p>
<p>The Lagrange multipliers of the constraints are a by-product of the solution.
</p>
<p>These can be used to estimate the change in solution value if the constraint values
</p>
<p>are changed slightly. For example, suppose we wish to estimate, without resolving
</p>
<p>the problem, the change in potential energy (the objective function) that would re-
</p>
<p>sult if the separation between the two supports were increased by, say, one inch. The
</p>
<p>change can be estimated by the formula ∆u = &minus;λ2/12 = 0.0833 &times; (6.76) = 0.563.
(When solved again numerically the change is found to be 0.568.)
</p>
<p>Let us now pose some more challenging questions. Consider two variations of the
</p>
<p>original problem. In the first variation the chain is replaced by one having twice as
</p>
<p>many links, but each link is now half the size of the original links. The overall chain
</p>
<p>length is therefore the same as before. In the second variation the original chain is
</p>
<p>replaced by one having twice as many links, but each link is the same size as the
</p>
<p>original links. The chain length doubles in this case. If these problems are solved by
</p>
<p>the same method as the original problem, approximately how many iterations will
</p>
<p>be required&mdash;about the same number, many more, or substantially less?
</p>
<p>These questions can be easily answered by using the theory of convergence rates
</p>
<p>developed in this chapter. The Hessian of the Lagrangian is
</p>
<p>L = F + λ1H1 + λ2H2.
</p>
<p>However, since the objective function and the first constraint are both linear, the
</p>
<p>only nonzero term in the above equation is λ2H2. Furthermore, since convergence
</p>
<p>rates depend only on eigenvalue ratios, the λ2 can be ignored. Thus the eigenvalues
</p>
<p>of H2 determine the canonical convergence rate.</p>
<p/>
</div>
<div class="page"><p/>
<p>388 12 Primal Methods
</p>
<p>Fig. 12.10 The chain example. (a) Original configuration of chain. (b) Final configuration.
(c) Long chain</p>
<p/>
</div>
<div class="page"><p/>
<p>12.7 Convergence Rate of the Reduced Gradient Method 389
</p>
<p>It is easily seen that H2 is diagonal with ith diagonal term,
</p>
<p>(H2)ii = &minus;(1 &minus; y2i )&minus;3/2,
</p>
<p>and these values are the eigenvalues of H2. The canonical convergence rate is de-
</p>
<p>fined by the eigenvalues of H22 in the (n &minus; 2)-dimensional tangent subspace M. We
cannot exactly determine these eigenvalues without a lot of work, but we can assume
</p>
<p>that they are close to the eigenvalues of H22. (Indeed, a version of the Interlocking
</p>
<p>Eigenvalues Lemma states that the n&minus; 2 eigenvalues are interlocked with the eigen-
values of H22.) Then the convergence rate of the gradient projection method will
</p>
<p>be governed by these eigenvalues. The reduced gradient method will most likely be
</p>
<p>somewhat slower.
</p>
<p>The eigenvalue of smallest absolute value corresponds to the center links, where
</p>
<p>yi ≃ 0. Conversely, the eigenvalue of largest absolute value corresponds to the first
or last link, where yi is largest in absolute value. Thus the relevant eigenvalue ratio
</p>
<p>is approximately
</p>
<p>r =
1
</p>
<p>(1 &minus; y2
1
)3/2
</p>
<p>=
1
</p>
<p>(sin θ)3/2
,
</p>
<p>where θ is the angle shown in Fig. 12.10b.
</p>
<p>For very little effort we have obtained a powerful understanding of the chain
</p>
<p>problem and its convergence properties. We can use this to answer the questions
</p>
<p>posed earlier. For the first variation, with twice as many links but each of half size,
</p>
<p>the angle θ will be about the same (perhaps a little smaller because of increased flex-
</p>
<p>ibility of the chain). Thus the number of iterations should be slightly larger because
</p>
<p>of the increase in θ and somewhat larger again because there are more variables
</p>
<p>(which tends to increase the condition number of CTC). Note in Table 12.2 that
</p>
<p>about 122 iterations were required, which is consistent with this estimate.
</p>
<p>For the second variation the chain will hang more vertically; hence y1 will be
</p>
<p>larger, and therefore convergence will be fundamentally slower. To be more specific
</p>
<p>it is necessary to substitute a few numbers in our simple formula. For the original
</p>
<p>case we have y1 ≃ &minus;.81. This yields
</p>
<p>r = (1 &minus; .812)&minus;3/2 = 4.9
</p>
<p>and a convergence factor of
</p>
<p>R =
</p>
<p>(
</p>
<p>r &minus; 1
r + 1
</p>
<p>)2
</p>
<p>≃ .44.
</p>
<p>This is a modest value and quite consistent with the observed result of 70 iterations
</p>
<p>for a reduced gradient method. For the long chain we can estimate that y1 ≃ 98. This
yields</p>
<p/>
</div>
<div class="page"><p/>
<p>390 12 Primal Methods
</p>
<p>Table 12.2 Results of modified chain problems
</p>
<p>Short links Long chain
</p>
<p>Iteration Value Iteration Value
</p>
<p>0 &minus;60.00000 0 &minus;366.6061
10 &minus;66.45499 10 &minus;375.6423
20 &minus;66.56377 20 &minus;375.9123
40 &minus;66.58443 50 &minus;376.5128
60 &minus;66.59191 100 &minus;377.1625
80 &minus;66.59514 200 &minus;377.8983
</p>
<p>100 &minus;66.59656 500 &minus;378.7989
120 &minus;66.59825 1000 &minus;379.3012
121 &minus;66.59827 1500 &minus;379.4994
122 &minus;66.59827 2000 &minus;379.5965
</p>
<p>2500 &minus;379.6489
y1 = 0.4109519 y1 = 0.9886223
</p>
<p>r = (1 &minus; .982)&minus;3/2 ≃ 127
</p>
<p>R =
</p>
<p>(
</p>
<p>r &minus; 1
r + 1
</p>
<p>)2
</p>
<p>≃ .969.
</p>
<p>This last number represents extremely slow convergence. Indeed, since (0.969)25 ≃
0.44, we expect that it may easily take 25 times as many iterations for the long chain
</p>
<p>problem to converge as the original problem (although quantitative estimates of this
</p>
<p>type are rough at best). This again is verified by the results shown in Table 12.2,
</p>
<p>where it is indicated that over 2,500 iterations were required by a version of the
</p>
<p>reduced gradient method.
</p>
<p>*12.8 &lowast;Variations
</p>
<p>It is possible to modify either the gradient projection method or the reduced gradient
</p>
<p>method so as to move in directions that are determined through additional consid-
</p>
<p>erations. For example, analogs of the conjugate gradient method, PARTAN, or any
</p>
<p>of the quasi-Newton methods can be applied to constrained problems by handling
</p>
<p>constraints through projection or reduction. The corresponding asymptotic rates of
</p>
<p>convergence for such methods are easily determined by applying the results for un-
</p>
<p>constrained problems on the (n&minus;m)-dimensional surface of constraints, as illustrated
in this chapter.
</p>
<p>Although such generalizations can sometimes lead to substantial improvement
</p>
<p>in convergence rates, one must recognize that the detailed logic for a complicated
</p>
<p>generalization can become lengthy. If the method relies on the use of an approxi-
</p>
<p>mate inverse Hessian restricted to the constraint surface, there must be an effective</p>
<p/>
</div>
<div class="page"><p/>
<p>12.8 &lowast;Variations 391
</p>
<p>procedure for updating the approximation when the iterative process progresses
</p>
<p>from one set of active constraints to another. One would also like to insure that the
</p>
<p>poor eigenvalue structure sometimes associated with quasi-Newton methods does
</p>
<p>not dominate the short-term convergence characteristics of the extended method
</p>
<p>when the active constraint set changes. In other words, one would like to be able to
</p>
<p>achieve simultaneously both superlinear convergence and a guarantee of fast single
</p>
<p>step progress. There has been some work in this general area and it appears to be
</p>
<p>one of potential promise.
</p>
<p>&lowast;Convex Simplex Method
</p>
<p>A popular modification of the reduced gradient method, termed the convex simplex
</p>
<p>method, most closely parallels the highly effective simplex method for solving lin-
</p>
<p>ear programs. The major difference between this method and the reduced gradient
</p>
<p>method is that instead of moving all (or several) of the independent variables in the
</p>
<p>direction of the negative reduced gradient, only one independent variable is changed
</p>
<p>at a time. The selection of the one independent variable to change is made much as
</p>
<p>in the ordinary simplex method.
</p>
<p>At a given feasible point, let x = (y, z) be the partition of x into dependent and
</p>
<p>independent parts, and assume for simplicity that the bounds on x are x � 0. Given
</p>
<p>the reduced gradient rT at the current point, the component zi to be changed is found
</p>
<p>from:
</p>
<p>1. Let ri1 = min
i
{ri}.
</p>
<p>2. Let ri2zi2 = max
i
{rizi}
</p>
<p>If ri1 = ri2zi2 = 0, terminate. Otherwise
</p>
<p>If ri1 � &minus;|ri2zi2|, increase zi1
If ri1 � &minus;|ri2zi2|, decrease zi2.
</p>
<p>The rule in Step 2 amounts to selecting the variable that yields the best potential
</p>
<p>decrease in the cost function. The rule accounts for the non-negativity constraint
</p>
<p>on the independent variables by weighting the cost coefficients of those variables
</p>
<p>that are candidates to be decreased by their distance from zero. This feature ensures
</p>
<p>global convergence of the method.
</p>
<p>The remaining details of the method are identical to those of the reduced gradient
</p>
<p>method. Once a particular component of z is selected for change, according to the
</p>
<p>above criterion, the corresponding y vector is computed as a function of the change
</p>
<p>in that component so as to continuously satisfy the constraints. The component of z
</p>
<p>is continuously changed until either a local minimum with respect to that component
</p>
<p>is attained or the boundary of one nonnegativity constraint is reached.
</p>
<p>Just as in the discussion of the reduced gradient method, it is convenient, for pur-
</p>
<p>poses of convergence analysis, to view the problem as unconstrained with respect
</p>
<p>to the independent variables. The convex simplex method is then seen to be a co-
</p>
<p>ordinate descent procedure in the space of these n &minus; m variables. Indeed, since the
component selected is based on the magnitude of the components of the reduced</p>
<p/>
</div>
<div class="page"><p/>
<p>392 12 Primal Methods
</p>
<p>gradient, the method is merely an adaptation of the Gauss-Southwell scheme dis-
</p>
<p>cussed in Sect. 8.6 to the constrained situation. Hence, although it is difficult to pin
</p>
<p>down precisely, we expect that it would take approximately n &minus; m steps of this co-
ordinate descent method to make the progress of a single reduced gradient step. To
</p>
<p>be competitive with the reduced gradient method; therefore, the difficulties associ-
</p>
<p>ated with a single step&mdash;line searching and constraint evaluation&mdash;must be approx-
</p>
<p>imately n &minus; m times simpler when only a single component is varied than when all
n &minus; m are varied simultaneously. This is indeed the case for linear programs and
for some quadratic programs but not for nonlinear problems that require the full
</p>
<p>line search machinery. Hence, in general, the convex simplex method may not be a
</p>
<p>bargain.
</p>
<p>12.9 Summary
</p>
<p>The concept of feasible direction methods is a straightforward and logical extension
</p>
<p>of the methods used for unconstrained problems but leads to some subtle difficulties.
</p>
<p>These methods are susceptible to jamming (lack of global convergence) because
</p>
<p>many simple direction finding mappings and the usual line search mapping are not
</p>
<p>closed.
</p>
<p>Problems with inequality constraints can be approached with an active set strat-
</p>
<p>egy. In this approach certain constraints are treated as active and the others are
</p>
<p>treated as inactive. By systematically adding and dropping constraints from the
</p>
<p>working set, the correct set of active constraints is determined during the search pro-
</p>
<p>cess. In general, however, an active set method may require that several constrained
</p>
<p>problems be solved exactly.
</p>
<p>The most practical primal methods are the gradient projection methods and
</p>
<p>the reduced gradient method. Both of these basic methods can be regarded as the
</p>
<p>method of steepest descent applied on the surface defined by the active constraints.
</p>
<p>The rate of convergence for the two methods can be expected to be approximately
</p>
<p>equal and is determined by the eigenvalues of the Hessian of the Lagrangian re-
</p>
<p>stricted to the subspace tangent to the active constraints. Of the two methods, the re-
</p>
<p>duced gradient method seems to be best. It can be easily modified to ensure against
</p>
<p>jamming and it requires fewer computations per iterative step and therefore, for most
</p>
<p>problems, will probably converge in less time than the gradient projection method.
</p>
<p>12.10 Exercises
</p>
<p>1. Show that the Frank-Wolfe method is globally convergent if the intersection of
</p>
<p>the feasible region and the objective level set {x : f (x) &le; f (x0)} is bounded.
2. Sometimes a different normalizing term is used in (12.4). Show that the problem
</p>
<p>of finding d = (d1, d2, . . . , dn) to</p>
<p/>
</div>
<div class="page"><p/>
<p>12.10 Exercises 393
</p>
<p>minimize cTd
</p>
<p>subject to Ad � 0, (
&sum;
</p>
<p>i
</p>
<p>|di|p)1/p = 1
</p>
<p>for p = 1 or p = &infin; can be converted to a linear program.
3. Perhaps the most natural normalizing term to use in (12.4) is one based on the
</p>
<p>Euclidean norm. This leads to the problem of finding d = (d1, d2, . . . , dn) to
</p>
<p>minimize cTd
</p>
<p>subject to Ad � 0,
n
&sum;
</p>
<p>i=1
d2
i
= 1.
</p>
<p>Find the Karush-Kuhn&ndash;Tucker necessary conditions for this problem and show
</p>
<p>how they can be solved by a modification of the simplex procedure.
</p>
<p>4. Let Ω &sub; En be a given feasible region. A set Γ &sub; E2n consisting of pairs (x, d),
with x &isin; Ω and d a feasible direction at x, is said to be a set of uniformly feasible
direction vectors if there is a δ &gt; 0 such that (x, d) &isin; Γ implies that x + αd is
feasible for all α, 0 � α � δ. The number δ is referred to as the feasibility
</p>
<p>constant of the set Γ.
</p>
<p>Let Γ &sub; E2n be a set of uniformly feasible direction vectors for Ω, with feasi-
bility constant δ. Define the mapping
</p>
<p>Mδ(x, d) = {y : f (y) � f (x + τd) for all τ, 0 � τ � δ; y = x + αd,
for some α, 0 � α � &infin;, y &isin; Ω}.
</p>
<p>Show that if d � 0, the map Mδ is closed at (x, d).
</p>
<p>5. Let Γ &sub; E2n be a set of uniformly feasible direction vectors forΩ with feasibility
constant δ. For ε &gt; 0 define the map εMδ or Γ by
</p>
<p>εMδ(x, d) = {y : f (y) � f (x + τd) + ε for all τ, 0 � τ � δ; y = x + αd,
for some α, 0 � α � &infin;, y &isin; Ω}.
</p>
<p>The map εMδ corresponds to an &ldquo;inaccurate&rdquo; constrained line search. Show that
</p>
<p>this map is closed if d � 0.
</p>
<p>6. For the problem
</p>
<p>minimize f (x)
</p>
<p>subject to aT
i
</p>
<p>x � bi, i = 1, 2, . . . , m
</p>
<p>consider selecting d = (d1, d2, . . . , dn) at a feasible point x by solving the
</p>
<p>problem
minimize &nabla; f (x)d
</p>
<p>subject to aT
i
</p>
<p>d � (bi &minus; aTi x)M, i = 1, 2, . . . , m
n
&sum;
</p>
<p>i=1
|di| = 1,
</p>
<p>where M is some given positive constant. For large M the ith inequality of this
</p>
<p>subsidiary problem will be active only if the corresponding inequality in the</p>
<p/>
</div>
<div class="page"><p/>
<p>394 12 Primal Methods
</p>
<p>original problem is nearly active at x (indeed, note that M &rarr; &infin; corresponds to
Zoutendijk&rsquo;s method). Show that this direction finding mapping is closed and
</p>
<p>generates uniformly feasible directions with feasibility constant 1/M.
</p>
<p>7. Generalize the method of Exercise 6 so that it is applicable to nonlinear inequal-
</p>
<p>ities.
</p>
<p>8. An alternate, but equivalent, definition of the projected gradient p is that it is
</p>
<p>the vector solving
minimize |g &minus; p|2
subject to Aqp = 0.
</p>
<p>Using the Karush-Kuhn&ndash;Tucker necessary conditions, solve this problem and
</p>
<p>thereby derive the formula for the projected gradient.
</p>
<p>9. Show that finding the d that solves
</p>
<p>minimize gTd
</p>
<p>subject to Aqd = 0, |d|2 = 1
</p>
<p>gives a vector d that has the same direction as the negative projected gradient.
</p>
<p>10. Let P be a projection matrix. Show that PT = P, P2 = P.
</p>
<p>11. Suppose Aq = [a
T , Aq] so that Aq is the matrix Aq with the row a
</p>
<p>T adjoined.
</p>
<p>Show that (AqA
T
q )
</p>
<p>&minus;1 can be found from (AqA
T
q
</p>
<p>)&minus;1 from the formula
</p>
<p>(AqA
T
q )
</p>
<p>&minus;1 =
</p>
<p>[
</p>
<p>ε &minus;εaTAT
q
</p>
<p>(AqA
T
q
</p>
<p>)&minus;1
</p>
<p>&minus;ε(AqATq )&minus;1Aqa (AqATq )&minus;1[I + AqaaTATq (AqATq )&minus;1]
</p>
<p>]
</p>
<p>,
</p>
<p>where
</p>
<p>ε =
1
</p>
<p>aTa &minus; aTAT
q
</p>
<p>(AqA
T
q
</p>
<p>)&minus;1Aqa
.
</p>
<p>Develop a similar formula for (AqAq)
&minus;1 in terms of (AqAq)&minus;1.
</p>
<p>12. Show that the gradient projection method will solve a linear program in a finite
</p>
<p>number of steps.
</p>
<p>13. Suppose that the projected negative gradient d is calculated satisfying
</p>
<p>&minus;g = d + ATq λ
</p>
<p>and that some component λi of λ, corresponding to an inequality, is negative.
</p>
<p>Show that if the ith inequality is dropped, the projection di of the negative gra-
</p>
<p>dient onto the remaining constraints is a feasible direction of descent.
</p>
<p>14. Using the result of Exercise 13, it is possible to avoid the discontinuity at d = 0
</p>
<p>in the direction finding mapping of the simple gradient projection method. At
</p>
<p>a given point let γ = &minus;min{0, λi}, with the minimum taken with respect to the
indices i corresponding the active inequalities. The direction to be taken at this
</p>
<p>point is d = &minus;Pg if |Pg| � γ, or d, defined by dropping the inequality i for
which λi = &minus;γ, if |Pg| � γ. (In case of equality either direction is selected.)
Show that this direction finding map is closed over a region where the set of
</p>
<p>active inequalities does not change.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.10 Exercises 395
</p>
<p>15. Consider the problem of maximizing entropy discussed in Example 3, Sect. 14.4.
</p>
<p>Suppose this problem were solved numerically with two constraints by the gra-
</p>
<p>dient projection method. Derive an estimate for the rate of convergence in terms
</p>
<p>of the optimal pi&rsquo;s.
</p>
<p>16. Find the geodesics of
</p>
<p>(a) a two-dimensional plane
</p>
<p>(b) a sphere.
</p>
<p>17. Suppose that the problem
</p>
<p>minimize f (x)
</p>
<p>subject to h(x) = 0
</p>
<p>is such that every point is a regular point. And suppose that the sequence of
</p>
<p>points {xk}&infin;k=0 generated by geodesic descent is bounded. Prove that every limit
point of the sequence satisfies the first-order necessary conditions for a con-
</p>
<p>strained minimum.
</p>
<p>18. Show that, for linear constraints, if at some point in the reduced gradient method
</p>
<p>∆z is zero, that point satisfies the Karush-Kuhn&ndash;Tucker first-order necessary
</p>
<p>conditions for a constrained minimum.
</p>
<p>19. Consider the problem
</p>
<p>minimize f (x)
</p>
<p>subject to Ax = b, x � 0,
</p>
<p>where A is m &times; n. Assume f &isin; C1, that the feasible set is bounded, and that
the nondegeneracy assumption holds. Suppose a &ldquo;modified&rdquo; reduced gradient
</p>
<p>algorithm is defined following the procedure in Sect. 12.6 but with two modifi-
</p>
<p>cations: (1) the basic variables are, at the beginning of an iteration, always taken
</p>
<p>as the m largest variables (ties are broken arbitrarily); (2) the formula for ∆z is
</p>
<p>replaced by
</p>
<p>∆zi =
</p>
<p>{
</p>
<p>&minus;ri if ri � 0
&minus;xiri if ri &gt; 0
</p>
<p>Establish the global convergence of this algorithm.
</p>
<p>20. Find the exact solution to the example presented in Sect. 12.4.
</p>
<p>21. Find the direction of movement that would be taken by the gradient projection
</p>
<p>method if in the example of Sect. 12.4 the constraint x4 = 0 were relaxed. Show
</p>
<p>that if the term &minus;3x4 in the objective function were replaced by &minus;x4, then both
the gradient projection method and the reduced gradient method would move in
</p>
<p>identical directions.
</p>
<p>22. Show that in terms of convergence characteristics, the reduced gradient method
</p>
<p>behaves like the gradient projection method applied to a scaled version of the
</p>
<p>problem.</p>
<p/>
</div>
<div class="page"><p/>
<p>396 12 Primal Methods
</p>
<p>23. Let r be the condition number of LM and s the condition number of C
TC. Show
</p>
<p>that the rate of convergence of the reduced gradient method is no worse than
</p>
<p>[(sr &minus; 1)/(sr + 1)]2.
24. Formulate the symmetric version of the hanging chain problem using a single
</p>
<p>constraint. Find an explicit expression for the condition number of the corre-
</p>
<p>sponding CTC matrix (assuming y1 is basic). Use Exercise 23 to obtain an es-
</p>
<p>timate of the convergence rate of the reduced gradient method applied to this
</p>
<p>problem, and compare it with the rate obtained in Table 12.1, Sect. 12.7. Repeat
</p>
<p>for the two-constraint formulation (assuming y1 and yn are basic).
</p>
<p>25. Referring to Exercise 19 establish a global convergence result for the convex
</p>
<p>simplex method.
</p>
<p>References
</p>
<p>12.2 Feasible direction methods of various types were originally suggested
</p>
<p>and developed by Zoutendijk [Z4]. The systematic study of the global
</p>
<p>convergence properties of feasible direction methods was begun by Top-
</p>
<p>kis and Veinott [T8] and by Zangwill [Z2]. The Frank-Wolfe method was
</p>
<p>initially proposed in [98].
</p>
<p>12.3&ndash;12.4 The gradient projection method was proposed and developed (more com-
</p>
<p>pletely than discussed here) by Rosen [R5, R6], who also introduced the
</p>
<p>notion of an active set strategy. See Gill, Murray, and Wright [G7] for a
</p>
<p>discussion of working sets and active set strategies.
</p>
<p>12.5 This material is taken from Luenberger [L14].
</p>
<p>12.6&ndash;12.7 The reduced gradient method was originally proposed by Wolfe [W5] for
</p>
<p>problems with linear constraints and generalized to nonlinear constraints
</p>
<p>by Abadie and Carpentier [A1]. Wolfe [W4] presents an example of jam-
</p>
<p>ming in the reduced gradient method. The convergence analysis given in
</p>
<p>this section is new.
</p>
<p>12.8 The convex simplex method, for problems with linear constraints, to-
</p>
<p>gether with a proof of its global convergence is due to Zangwill [Z2].</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 13
</p>
<p>Penalty and Barrier Methods
</p>
<p>Penalty and barrier methods are procedures for approximating constrained optimiza-
</p>
<p>tion problems by unconstrained problems. The approximation is accomplished in
</p>
<p>the case of penalty methods by adding to the objective function a term that pre-
</p>
<p>scribes a high cost for violation of the constraints, and in the case of barrier meth-
</p>
<p>ods by adding a term that favors points interior to the feasible region over those near
</p>
<p>the boundary. Associated with these methods is a parameter c or μ that determines
</p>
<p>the severity of the penalty or barrier and consequently the degree to which the unc-
</p>
<p>onstrained problem approximates the original constrained problem. For a problem
</p>
<p>with n variables and m constraints, penalty and barrier methods work directly in
</p>
<p>the n-dimensional space of variables, as compared to primal methods that work in
</p>
<p>(n &minus; m)-dimensional space.
There are two fundamental issues associated with the methods of this chapter.
</p>
<p>The first has to do with how well the unconstrained problem approximates the con-
</p>
<p>strained one. This is essential in examining whether, as the parameter c is increased
</p>
<p>toward infinity, the solution of the unconstrained problem converges to a solution
</p>
<p>of the constrained problem. The other issue, most important from a practical view-
</p>
<p>point, is the question of how to solve a given unconstrained problem when its obj-
</p>
<p>ective function contains a penalty or barrier term. It turns out that as c is increased
</p>
<p>to yield a good approximating problem, the corresponding structure of the resulting
</p>
<p>unconstrained problem becomes increasingly unfavorable thereby slowing the con-
</p>
<p>vergence rate of many algorithms that might be applied. (Exact penalty functions
</p>
<p>also have a very unfavorable structure.) It is necessary, then, to devise acceleration
</p>
<p>procedures that circumvent this slow convergence phenomenon.
</p>
<p>Penalty and barrier methods are of great interest to both the practitioner and
</p>
<p>the theorist. To the practitioner they offer a simple straightforward method for han-
</p>
<p>dling constrained problems that can be implemented without sophisticated com-
</p>
<p>puter programming and that possess much the same degree of generality as primal
</p>
<p>methods. The theorist, striving to make this approach practical by overcoming its
</p>
<p>inherently slow convergence, finds it appropriate to bring into play nearly all aspects
</p>
<p>&copy; Springer International Publishing Switzerland 2016
</p>
<p>D.G. Luenberger, Y. Ye, Linear and Nonlinear Programming, International
Series in Operations Research &amp; Management Science 228,
DOI 10.1007/978-3-319-18842-3 13
</p>
<p>397</p>
<p/>
</div>
<div class="page"><p/>
<p>398 13 Penalty and Barrier Methods
</p>
<p>of optimization theory; including Lagrange multipliers, necessary conditions, and
</p>
<p>many of the algorithms discussed earlier in this book. The canonical rate of conver-
</p>
<p>gence associated with the original constrained problem again asserts its fundamental
</p>
<p>role by essentially determining the natural accelerated rate of convergence for unc-
</p>
<p>onstrained penalty or barrier problems.
</p>
<p>13.1 Penalty Methods
</p>
<p>Consider the problem
</p>
<p>minimize f (x)
</p>
<p>subject to x &isin; S , (13.1)
</p>
<p>where f is a continuous function on En and S is a constraint set in En. In most
</p>
<p>applications S is defined implicitly by a number of functional constraints, but in
</p>
<p>this section the more general description in (13.1) can be handled. The idea of a
</p>
<p>penalty function method is to replace problem (13.1) by an unconstrained problem
</p>
<p>of the form
minimize f (x) + cP(x), (13.2)
</p>
<p>where c is a positive constant and P is a function on En satisfying: (i) P is continu-
</p>
<p>ous, (ii) P(x) � 0 for all x &isin; En, and (iii) P(x) = 0 if and only if x &isin; S .
</p>
<p>Example 1. Suppose S is defined by a number of inequality constraints:
</p>
<p>S = {x : gi(x) � 0, i = 1, 2, . . . , p}.
A very useful penalty function in this case is
</p>
<p>P(x) =
1
</p>
<p>2
</p>
<p>P
&sum;
</p>
<p>i=1
</p>
<p>(max[0, gi(x)])
2.
</p>
<p>The function cP(x) is illustrated in Fig. 13.1 for the one-dimensional case with
</p>
<p>g1(x) = x &minus; b, g2(x) = a &minus; x.
</p>
<p>For large c it is clear that the minimum point of problem (13.2) will be in a region
</p>
<p>where P is small. Thus, for increasing c it is expected that the corresponding so-
</p>
<p>lution points will approach the feasible region S and, subject to being close, will
</p>
<p>minimize f . Ideally then, as c &rarr; &infin; the solution point of the penalty problem will
converge to a solution of the constrained problem.
</p>
<p>The Method
</p>
<p>The procedure for solving problem (13.1) by the penalty function method is this:
</p>
<p>Let {ck}, k = 1, 2, . . ., be a sequence tending to infinity such that for each
k, ck � 0, ck+1 &gt; ck. Define the function
</p>
<p>q(c, x) = f (x) + cP(x). (13.3)</p>
<p/>
</div>
<div class="page"><p/>
<p>13.1 Penalty Methods 399
</p>
<p>Fig. 13.1 Plot of cP(x)
</p>
<p>For each k solve the problem
</p>
<p>minimize q(ck, x), (13.4)
</p>
<p>obtaining a solution point xk.
</p>
<p>We assume here that, for each k, problem (13.4) has a solution. This will be true,
</p>
<p>for example, if q(c, x) increases unboundedly as |x| &rarr; &infin;. (Also see Exercise 2 to
see that it is not necessary to obtain the minimum precisely.)
</p>
<p>Convergence
</p>
<p>The following lemma gives a set of inequalities that follow directly from the defini-
</p>
<p>tion of xk and the inequality ck+1 &gt; ck.
</p>
<p>Lemma 1.
</p>
<p>q(ck , xk) � q(ck+1 , xk+1) (13.5)
</p>
<p>P(xk) � P(xk+1) (13.6)
</p>
<p>f (xk) � f (xk+1). (13.7)
</p>
<p>Proof.
</p>
<p>q(ck+1, xk+1) = f (xk+1) + ck+1P(xk+1) � f (xk+1) + ckP(xk+1)
</p>
<p>� f (xk) + ckP(xk) = q(ck, xk),
</p>
<p>which proves (13.5).
</p>
<p>We also have
</p>
<p>f (xk) + ckP(xk) � f (xk+1) + ckP(xk+1) (13.8)
</p>
<p>f (xk+1) + ck+1P(xk+1) � f (xk) + ck+1P(xk). (13.9)</p>
<p/>
</div>
<div class="page"><p/>
<p>400 13 Penalty and Barrier Methods
</p>
<p>Adding (13.8) and (13.9) yields
</p>
<p>(ck+1 &minus; ck)P(xk+1) � (ck+1 &minus; ck)P(xk),
</p>
<p>which proves (13.6).
</p>
<p>Also
</p>
<p>f (xk+1) + ckP(xk+1) � f (xk) + ckP(xk),
</p>
<p>and hence using (13.6) we obtain (13.7). �
</p>
<p>Lemma 2. Let x&lowast; be a solution to problem (13.1). Then for each k
</p>
<p>f (x&lowast;) � q(ck , xk) � f (xk).
</p>
<p>Proof.
</p>
<p>f (x&lowast;) = f (x&lowast;) + ckP(x
&lowast;) � f (xk) + ckP(xk) � f (xk). �
</p>
<p>Global convergence of the penalty method, or more precisely verification that any
</p>
<p>limit point of the sequence is a solution, follows easily from the two lemmas above.
</p>
<p>Theorem. Let {xk} be a sequence generated by the penalty method. Then, any limit point of
the sequence is a solution to (13.1).
</p>
<p>Proof. Suppose the subsequence {xk}, k &isin; K is a convergent subsequence of {xk}
having limit x. Then by the continuity of f , we have
</p>
<p>limit
k&isin;K
</p>
<p>f (xk) = f (x). (13.10)
</p>
<p>Let f &lowast; be the optimal value associated with problem (13.1). Then according to
Lemmas 1 and 2, the sequence of values q(ck, xk) is nondecreasing and bounded
</p>
<p>above by f &lowast;. Thus
limit
k&isin;K
</p>
<p>q(ck, xk) = q
&lowast;
� f &lowast;. (13.11)
</p>
<p>Subtracting (13.10) from (13.11) yields
</p>
<p>limit
k&isin;K
</p>
<p>ckP(xk) = q
&lowast; &minus; f (x). (13.12)
</p>
<p>Since P(xk) � 0 and ck &rarr; &infin;, (13.12) implies
</p>
<p>limit
k&isin;K
</p>
<p>P(xk) = 0.
</p>
<p>Using the continuity of P, this implies P(x) = 0. We therefore have shown that the
</p>
<p>limit point x is feasible for (13.1).
</p>
<p>To show that x is optimal we note that from Lemma 2, f (xk) � f
&lowast; and hence
</p>
<p>f (x) = limitk&isin;K f (xk) � f
&lowast;.�</p>
<p/>
</div>
<div class="page"><p/>
<p>13.2 Barrier Methods 401
</p>
<p>13.2 Barrier Methods
</p>
<p>Barrier methods are applicable to problems of the form
</p>
<p>minimize f (x)
</p>
<p>subject to x &isin; S , (13.13)
</p>
<p>where the constraint set S has a nonempty interior that is arbitrarily close to any
</p>
<p>point of S . Intuitively, what this means is that the set has an interior and it is possible
</p>
<p>to get to any boundary point by approaching it from the interior. We shall refer to
</p>
<p>such a set as robust. Some examples of robust and nonrobust sets are shown in
</p>
<p>Fig. 13.2. This kind of set often arises in conjunction with inequality constraints,
</p>
<p>where S takes the form
</p>
<p>S = {x : gi(x) � 0, i = 1, 2, . . . , p}
</p>
<p>Barrier methods are also termed interior methods. They work by establishing a bar-
</p>
<p>rier on the boundary of the feasible region that prevents a search procedure from
</p>
<p>leaving the region. A barrier function is a function B defined on the interior of S
</p>
<p>such that: (i) B is continuous, (ii) B(x) � 0, (iii) B(x) &rarr; &infin; as x approaches the
boundary of S .
</p>
<p>Example 1. Let gi, i = 1, 2, . . . , p be continuous functions on E
n. Suppose
</p>
<p>S = {x : gi(x) � 0, i = 1, 2, . . . , p}.
</p>
<p>is robust, and suppose the interior of S is the set of x&rsquo;s where gi(x) &lt; 0, i =
</p>
<p>1, 2, . . . , p. Then the function
</p>
<p>B(x) = &minus;
p
</p>
<p>&sum;
</p>
<p>i=1
</p>
<p>1
</p>
<p>gi(x)
,
</p>
<p>defined on the interior of S , is a barrier function. It is illustrated in one dimension
</p>
<p>for g1 = x &minus; a, g2 = x &minus; b in Fig. 13.3.
</p>
<p>Fig. 13.2 Examples</p>
<p/>
</div>
<div class="page"><p/>
<p>402 13 Penalty and Barrier Methods
</p>
<p>Example 2. For the same situation as Example 1, we may use the logarithmic utility
</p>
<p>function
</p>
<p>B(x) = &minus;
p
</p>
<p>&sum;
</p>
<p>i=1
</p>
<p>log[&minus;gi(x)].
</p>
<p>This is the barrier function commonly used in linear programming interior point
</p>
<p>methods, and it is frequently used more generally as well.
</p>
<p>Corresponding to the problem (13.13), consider the approximate problem
</p>
<p>minimize f (x) + 1
c
B(x)
</p>
<p>subject to x &isin; interior of S , (13.14)
</p>
<p>where c is a positive constant.
</p>
<p>Alternatively, it is common to formulate the barrier method as
</p>
<p>minimize f (x) + μB(x)
</p>
<p>subject to x &isin; interior of S . (13.15)
</p>
<p>Fig. 13.3 Barrier function
</p>
<p>When formulated with c we take c large (going to infinity); while when for-
</p>
<p>mulated with μ we take μ small (going to zero). Either way the result is a con-
</p>
<p>strained problem, and indeed the constraint is somewhat more complicated than in
</p>
<p>the original problem (13.13). The advantage of this problem, however, is that it can
</p>
<p>be solved by using an unconstrained search technique. To find the solution one starts
</p>
<p>at an initial interior point and then searches from that point using steepest descent
</p>
<p>or some other iterative descent method applicable to unconstrained problems. Since
</p>
<p>the value of the objective function approaches infinity near the boundary of S , the
</p>
<p>search technique (if carefully implemented) will automatically remain within the</p>
<p/>
</div>
<div class="page"><p/>
<p>13.3 Properties of Penalty and Barrier Functions 403
</p>
<p>interior of S , and the constraint need not be accounted for explicitly. Thus, although
</p>
<p>problem (13.14) or (13.15) is from a formal viewpoint a constrained problem, from
</p>
<p>a computational viewpoint it is unconstrained.
</p>
<p>The Method
</p>
<p>The barrier method is quite analogous to the penalty method. Let {ck} be a sequence
tending to infinity such that for each k, k = 1, 2, . . . , ck � 0, ck+1 &gt; ck. Define the
</p>
<p>function
</p>
<p>r(c, x) = f (x) +
1
</p>
<p>c
B(x).
</p>
<p>For each k solve the problem
</p>
<p>minimize r(ck, x)
</p>
<p>subject to x &isin; interior ofS ,
</p>
<p>obtaining the point xk.
</p>
<p>Convergence
</p>
<p>Virtually the same convergence properties hold for the barrier method as for the
</p>
<p>penalty method. We leave to the reader the proof of the following result.
</p>
<p>Theorem. Any limit point of a sequence {xk} generated by the barrier method is a solution
to problem (13.13).
</p>
<p>13.3 Properties of Penalty and Barrier Functions
</p>
<p>Penalty and barrier methods are applicable to nonlinear programming problems
</p>
<p>having a very general form of constraint set S . In most situations, however, this
</p>
<p>set is not given explicitly but is defined implicitly by a number of functional con-
</p>
<p>straints. In these situations, the penalty or barrier function is invariably defined in
</p>
<p>terms of the constraint functions themselves; and although there are an unlimited
</p>
<p>number of ways in which this can be done, some important general implications
</p>
<p>follow from this kind of construction.
</p>
<p>For economy of notation we consider problems of the form
</p>
<p>minimize f (x)
</p>
<p>subject to gi (x) � 0, i = 1, 2, . . . , p.
(13.16)</p>
<p/>
</div>
<div class="page"><p/>
<p>404 13 Penalty and Barrier Methods
</p>
<p>For our present purposes, equality constraints are suppressed, at least notationally,
</p>
<p>by writing each of them as two inequalities. If the problem is to be attacked with
</p>
<p>a barrier method, then, of course, equality constraints are not present even in an
</p>
<p>unsuppressed version.
</p>
<p>Penalty Functions
</p>
<p>A penalty function for a problem expressed in the form (13.16) will most naturally
</p>
<p>be expressed in terms of the auxiliary constraint functions
</p>
<p>g+i (x) &equiv; max[0, gi(x)], i = 1, 2, . . . , p. (13.17)
</p>
<p>This is because in the interior of the constraint region P(x) &equiv; 0 and hence P should
be a function only of violated constraints. Denoting by g+(x) the p-dimensional
</p>
<p>vector made up of the g+
i
(x)&rsquo;s, we consider the general class of penalty functions
</p>
<p>P(x) = γ(g+(x)), (13.18)
</p>
<p>where γ is a continuous function from Ep to the real numbers, defined in such a way
</p>
<p>that P satisfies the requirements demanded of a penalty function.
</p>
<p>Example 1. Set
</p>
<p>P(x) =
1
</p>
<p>2
</p>
<p>p
&sum;
</p>
<p>i=1
</p>
<p>g+i (x)
2 =
</p>
<p>1
</p>
<p>2
|g+(x)|2,
</p>
<p>which is without doubt the most popular penalty function. In this case γ is one-half
</p>
<p>times the identity quadratic form on Ep, that is, γ(y) = 1
2
|y|2.
</p>
<p>Example 2. By letting
</p>
<p>γ(y) = yTΓy,
</p>
<p>where Γ is a symmetric positive definite p&times; p matrix, we obtain the penalty function
</p>
<p>P(x) = g+(x)TΓg+(x).
</p>
<p>Example 3. A general class of penalty functions is
</p>
<p>P(x) =
</p>
<p>p
&sum;
</p>
<p>i=1
</p>
<p>(g+i (x))
ε
</p>
<p>for some ε &gt; 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.3 Properties of Penalty and Barrier Functions 405
</p>
<p>Lagrange Multipliers
</p>
<p>In the penalty method we solve, for various ck, the unconstrained problem
</p>
<p>minimize f (x) + ckP(x). (13.19)
</p>
<p>Most algorithms require that the objective function has continuous first partial
</p>
<p>derivatives. Since we shall, as usual, assume that both f and g &isin; C1, it is natural
to require, then, that the penalty function P &isin; C1. We define
</p>
<p>&nabla;g+i (x) =
</p>
<p>⎧
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>&nabla;gi(x) if gi(x) � 0
</p>
<p>0 if gi(x) &lt; 0
, (13.20)
</p>
<p>and, of course, &nabla;g+(x) is the m&times; n matrix whose rows are the &nabla;g+
i
&rsquo;s. Unfortunately,
</p>
<p>&nabla;g+ is usually discontinuous at points where g+
i
(x) = 0 for some i = 1, 2, . . . , p,
</p>
<p>and thus some restrictions must be placed on γ in order to guarantee P &isin; C1. We
assume that γ &isin; C1 and that if y = (y1, y2, . . . , yn), &nabla;γ(y) = (&nabla;γ1, &nabla;γ2, . . . , &nabla;γn),
then
</p>
<p>yi = 0 implies &nabla;γi = 0. (13.21)
(In Example 3 above, for instance, this condition is satisfied only for ε &gt; 1.) With
</p>
<p>this assumption, the derivative of γ(g+(x)) with respect to x is continuous and can be
</p>
<p>written as &nabla;γ(g+(x))&nabla;g(x). In this result &nabla;g(x) legitimately replaces the discontin-
</p>
<p>uous &nabla;g+(x), because it is premultiplied by &nabla;γ(g+(x)). Of course, these considera-
</p>
<p>tions are necessary only for inequality constraints. If equality constraints are treated
</p>
<p>directly, the situation is far simpler.
</p>
<p>In view of this assumption, problem (13.19) will have its solution at a point xk
satisfying
</p>
<p>&nabla; f (xk) + ck&nabla;γ(g
+(xk))&nabla;g(xk) = 0,
</p>
<p>which can be written as
</p>
<p>&nabla; f (xk) + λ
T
k&nabla;g(xk) = 0, (13.22)
</p>
<p>where
</p>
<p>λTk &equiv; ck&nabla;γ(g+(xk)). (13.23)
Thus, associated with every c is a Lagrange multiplier vector that is determined after
</p>
<p>the unconstrained minimization is performed.
</p>
<p>If a solution x&lowast; to the original problem (13.16) is a regular point of the constraints,
then there is a unique Lagrange multiplier vector λ&lowast; associated with the solution. The
result stated below says that λk &rarr; λ&lowast;.
</p>
<p>Proposition. Suppose that the penalty function method is applied to problem (13.16) using
</p>
<p>a penalty function of the form (13.18) with γ &isin; C1 and satisfying (13.21). Corresponding
to the sequence {xk} generated by this method, define λTk = ck&nabla;γ(g+(xk)). If xk &rarr; x&lowast;,
a solution to (13.16), and this solution is a regular point, then λk &rarr; λ&lowast;, the Lagrange
multiplier associated with problem (13.16).
</p>
<p>Proof left to the reader.</p>
<p/>
</div>
<div class="page"><p/>
<p>406 13 Penalty and Barrier Methods
</p>
<p>Example 4. For P(x) = 1
2
|g+(x)|2 we have λk = ckg+(xk).
</p>
<p>As a final observation we note that in general if xk &rarr; x&lowast;, then since λk =
ck&nabla;γ(g
</p>
<p>+(xk))
T &rarr; λ&lowast;, the sequence xk approaches x&lowast; from outside the constraint
</p>
<p>region. Indeed, as xk approaches x
&lowast; all constraints that are active at x&lowast; and have
</p>
<p>positive Lagrange multipliers will be violated at xk because the corresponding com-
</p>
<p>ponents of &nabla;γ(g+(xk)) are positive. Thus, if we assume that the active constraints are
</p>
<p>nondegenerate (all Lagrange multipliers are strictly positive), every active constraint
</p>
<p>will be approached from the outside.
</p>
<p>The Hessian Matrix
</p>
<p>Since the penalty function method must, for various (large) values of c, solve the
</p>
<p>unconstrained problem
</p>
<p>minimize f (x) + cP(x), (13.24)
</p>
<p>it is important, in order to evaluate the difficulty of such a problem, to determine
</p>
<p>the eigenvalue structure of the Hessian of this modified objective function. We show
</p>
<p>here that the structure becomes increasingly unfavorable as c increases.
</p>
<p>Although in this section we require that the function P &isin; C1, we do not require
that P &isin; C2. In particular, the most popular penalty function P(x) = 1
</p>
<p>2
|g+(x)|2,
</p>
<p>illustrated in Fig. 13.1 for one component, has a discontinuity in its second derivative
</p>
<p>at any point where a component of g is zero. At first this might appear to be a
</p>
<p>serious drawback, since it means the Hessian is discontinuous at the boundary of the
</p>
<p>constraint region&mdash;right where, in general, the solution is expected to lie. However,
</p>
<p>as pointed out above, the penalty method generates points that approach a boundary
</p>
<p>solution from outside the constraint region. Thus, except for some possible chance
</p>
<p>occurrences, the sequence will, as xk &rarr; x&lowast;, be at points where the Hessian is well-
defined. Furthermore, in iteratively solving the unconstrained problem (13.24) with
</p>
<p>a fixed ck, a sequence will be generated that converges to xk which is (for most
</p>
<p>values of k) a point where the Hessian is well-defined, and hence the standard type
</p>
<p>of analysis will be applicable to the tail of such a sequence.
</p>
<p>Defining q(c, x) = f (x)+cγ(g+(x)) we have for the Hessian, Q, of q (with respect
</p>
<p>to x)
</p>
<p>Q(c, x) = F(x) + c&nabla;γ(g+(x))G(x) + c&nabla;g+(x)TΓ(g+(x))&nabla;g+(x),
</p>
<p>where F, G, and Γ are, respectively, the Hessians of f , g, and γ. For a fixed ck we
</p>
<p>use the definition of λk given by (13.23) and introduce the rather natural definition
</p>
<p>Lk(xk) = F(xk) + λ
T
k G(xk), (13.25)
</p>
<p>which is the Hessian of the corresponding Lagrangian. Then we have
</p>
<p>Q(ck, xk) = Lk(xk) + ck&nabla;g
+(xk)
</p>
<p>T
Γ(g+(xk))&nabla;g
</p>
<p>+(xk), (13.26)
</p>
<p>which is the desired expression.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.3 Properties of Penalty and Barrier Functions 407
</p>
<p>The first term on the right side of (13.26) converges to the Hessian of the La-
</p>
<p>grangian of the original constrained problem as xk &rarr; x&lowast;, and hence has a limit that
is independent of ck. The second term is a matrix having rank equal to the rank of
</p>
<p>the active constraints and having a magnitude tending to infinity. (See Exercise 7.)
</p>
<p>Example 5. For P(x) = 1
2
|g+(x)|2 we have
</p>
<p>Γ(g+(xk)) =
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>e1 0 &middot; &middot; &middot; 0
0 e2 0
</p>
<p>0 &middot; &middot;
&middot; &middot; &middot;
&middot; &middot; &middot;
0 &middot; &middot; &middot; 0 ep
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>,
</p>
<p>where
</p>
<p>ei =
</p>
<p>⎧
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>1 if gi(xk) &gt; 0
</p>
<p>0 if gi(xk) &lt; 0
</p>
<p>undefined if gi(xk) = 0
</p>
<p>Thus
</p>
<p>ck&nabla;g
+(xk)
</p>
<p>T (g+(xk))&nabla;g
+(xk) = ck&nabla;g
</p>
<p>+(xk)
T
&nabla;g+(xk),
</p>
<p>which is ck times a matrix that approaches &nabla;g
+(x&lowast;)T&nabla;g+(x&lowast;). This matrix has rank
</p>
<p>equal to the rank of the active constraints at x&lowast; [refer to (13.20)].
</p>
<p>Assuming that there are r active constraints at the solution x&lowast;, then for well-
behaved γ, the Hessian matrix Q(ck, xk) has r eigenvalues that tend to infinity as
</p>
<p>ck &rarr; &infin;, arising from the second term on the right side of (13.26). There will be
n &minus; r other eigenvalues that, although varying with ck, tend to finite limits. These
limits turn out to be, as is perhaps not too surprising at this point, the eigenvalues
</p>
<p>of L(x&lowast;) restricted to the tangent subspace M of the active constraints. The proof of
this requires some further analysis.
</p>
<p>Lemma 1. Let A(c) be a symmetric matrix written in partitioned form
</p>
<p>A(c) =
</p>
<p>[
</p>
<p>A1(c) A2(c)
</p>
<p>AT2 (c) A3(c)
</p>
<p>]
</p>
<p>, (13.27)
</p>
<p>where A1(c) tends to a positive definite matrix A1, A2(c) tends to a finite matrix, and A3(c)
is a positive definite matrix tending to infinity with c (that is, for any s &gt; 0, A3(c) ⇁ sI is
positive definite for sufficiently large c). Then
</p>
<p>A&minus;1(c) &rarr;
[
</p>
<p>A&minus;11 0
0 0
</p>
<p>]
</p>
<p>(13.28)
</p>
<p>as c &rarr; &infin;.
</p>
<p>Proof. We have the identity
</p>
<p>[
</p>
<p>A1 A2
AT2 A3
</p>
<p>]&minus;1
=
</p>
<p>[
</p>
<p>(A1 &minus; A2A&minus;13 AT2 )&minus;1 &minus;(A1 &minus; A2A&minus;13 AT2 )A2A&minus;13
&minus;A&minus;13 AT2 (A1 &minus; A2A&minus;13 AT2 )&minus;1 (A3 &minus; AT2 A&minus;11 A2)&minus;1
</p>
<p>]
</p>
<p>. (13.29)
</p>
<p>Using the fact that A&minus;13 (c) &rarr; 0 gives the result. �</p>
<p/>
</div>
<div class="page"><p/>
<p>408 13 Penalty and Barrier Methods
</p>
<p>To apply this result to the Hessian matrix (13.26) we associate A with Q(ck, xk)
</p>
<p>and let the partition of A correspond to the partition of the space En into the subspace
</p>
<p>M and the subspace N that is orthogonal to M; that is, N is the subspace spanned by
</p>
<p>the gradients of the active constraints. In this partition, LM , the restriction of L to
</p>
<p>M, corresponds to the matrix A1.
</p>
<p>We leave the details of the required continuity arguments to the reader. The
</p>
<p>important conclusion is that if x&lowast; is a solution to (13.16), is a regular point, and
has exactly r active constraints none of which are degenerate, then the Hessian ma-
</p>
<p>trices Q(ck, xk) of a penalty function of form (13.18) have r eigenvalues tending to
</p>
<p>infinity as ck &rarr; &infin;, and n &minus; r eigenvalues tending to the eigenvalues of LM .
This explicit characterization of the structure of penalty function Hessians is of
</p>
<p>great importance in the remainder of the chapter. The fundamental point is that
</p>
<p>virtually any choice of penalty function (within the class considered) leads both to
</p>
<p>an ill-conditioned Hessian and to consideration of the ubiquitous Hessian of the
</p>
<p>Lagrangian restricted to M.
</p>
<p>Barrier Functions
</p>
<p>Essentially the same story holds for barrier function. If we consider for Prob-
</p>
<p>lem (13.16) barrier functions of the form
</p>
<p>B(x) = η(g(x)), (13.30)
</p>
<p>then Lagrange multipliers and ill-conditioned Hessians are again inevitable. Rather
</p>
<p>than parallel the earlier analysis of penalty functions, we illustrate the conclusions
</p>
<p>with two examples.
</p>
<p>Example 1. Define
</p>
<p>B(x) =
</p>
<p>p
&sum;
</p>
<p>i=1
</p>
<p>&minus; 1
gi(x)
</p>
<p>. (13.31)
</p>
<p>The barrier objective
</p>
<p>r(ck, x) = f (x) &minus;
1
</p>
<p>ck
</p>
<p>p
&sum;
</p>
<p>i=1
</p>
<p>1
</p>
<p>gi(x)
</p>
<p>has its minimum at a point xk satisfying
</p>
<p>&nabla; f (xk) +
1
</p>
<p>ck
</p>
<p>p
&sum;
</p>
<p>i=1
</p>
<p>1
</p>
<p>gi(xk)2
&nabla;gi(xk) = 0. (13.32)
</p>
<p>Thus, we define λk to be the vector having ith component
1
ck
. 1
gi(xk)2
</p>
<p>. Then (13.32)
</p>
<p>can be written as
</p>
<p>&nabla; f (xk) + λ
T
k&nabla;g(xk) = 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.3 Properties of Penalty and Barrier Functions 409
</p>
<p>Again, assuming xk &rarr; x&lowast;, the solution of (13.16), we can show that λk &rarr; λ&lowast;, the
Lagrange multiplier vector associated with the solution. This implies that if gi is an
</p>
<p>active constraint,
</p>
<p>1
</p>
<p>ckgi(xk)2
&rarr; λ&lowast;i &lt; &infin;. (13.33)
</p>
<p>Next, evaluating the Hessian R(ck, xk) of r(ck, xk), we have
</p>
<p>R(ck, xk) = F(xk) +
1
</p>
<p>ck
</p>
<p>p
&sum;
</p>
<p>i=1
</p>
<p>1
</p>
<p>gi(xk)2
Gi(xk) &minus;
</p>
<p>1
</p>
<p>ck
</p>
<p>p
&sum;
</p>
<p>i=1
</p>
<p>2
</p>
<p>gi(xk)3
&nabla;gi(xk)
</p>
<p>T
&nabla;gi(xk)
</p>
<p>= L(xk) &minus;
1
</p>
<p>ck
</p>
<p>p
&sum;
</p>
<p>i=1
</p>
<p>2
</p>
<p>gi(xk)3
&nabla;gi(xk)
</p>
<p>T
&nabla;gi(xk).
</p>
<p>As ck &rarr; &infin; we have
</p>
<p>&minus;1
ckgi(xk)3
</p>
<p>&rarr;
{
</p>
<p>&infin; if gi is active at x&lowast;
0 if gi is inactive at x
</p>
<p>&lowast;
</p>
<p>so that we may write, from (13.33),
</p>
<p>R(ck, xk) &rarr; L(xk) +
&sum;
</p>
<p>i&isin;1
&minus; λi,k
gi(xk)
</p>
<p>&nabla;gi(xk)
T
&nabla;gi(xk),
</p>
<p>where I is the set of indices corresponding to active constraints. Thus the Hessian
</p>
<p>of the barrier objective function has exactly the same structure as that of penalty
</p>
<p>objective functions.
</p>
<p>Example 2. Let us use the logarithmic barrier function
</p>
<p>B(x) = &minus;
p
</p>
<p>&sum;
</p>
<p>i=1
</p>
<p>log[&minus;gi(x)]
</p>
<p>In this case we will define the barrier objective in terms of μ as
</p>
<p>r(μ, x) = f (x) &minus; μ
p
</p>
<p>&sum;
</p>
<p>i=1
</p>
<p>log[&minus;gi(x)]
</p>
<p>The minimum point xμ satisfies
</p>
<p>0 = &nabla; f (xμ) + μ
</p>
<p>p
&sum;
</p>
<p>i=1
</p>
<p>&minus;1
gi(xμ)
</p>
<p>&nabla;gi(xμ). (13.34)
</p>
<p>Defining
</p>
<p>λμ,i = μ
&minus;1
</p>
<p>gi(xμ)</p>
<p/>
</div>
<div class="page"><p/>
<p>410 13 Penalty and Barrier Methods
</p>
<p>(13.34) can be written as
</p>
<p>&nabla; f (xμ) + λ
T
μ&nabla;g(xμ) = 0.
</p>
<p>Further we expect that λμ &rarr; λ&lowast; as μ &rarr; 0.
The Hessian of r(μ, x) is
</p>
<p>R(μ, xμ) = F(xμ) +
</p>
<p>p
&sum;
</p>
<p>i=1
</p>
<p>λi,μGi(xμ) +
</p>
<p>p
&sum;
</p>
<p>i=1
</p>
<p>&minus;
λi,μ
</p>
<p>gI(xμ)
&nabla;gi(xμ)
</p>
<p>T
&nabla;gi(xμ).
</p>
<p>Hence, for small μ it has the same structure as that found in Example 1.
</p>
<p>The Central Path
</p>
<p>The definition of the central path associated with linear programs is easily extended
</p>
<p>to general nonlinear programs. For example, consider the problem
</p>
<p>minimize f (x)
</p>
<p>subject to h(x) = 0, g(x) &le; 0.
</p>
<p>We assume that F̊ = {x : h(x) = 0, g(x) &lt; 0} � φ. Then we use the logarithmic
barrier function to define the problems
</p>
<p>minimize f (x) &minus; μ&sum;p
i=1
</p>
<p>log[&minus;gi(x)]
subject to h(x) = 0.
</p>
<p>The solution xμ parameterized by μ &rarr; 0 is called the central path; see Chap. 5.
The necessary conditions for the problem can be written as
</p>
<p>&nabla; f (xμ) + s
T
&nabla;g(xμ) + y
</p>
<p>T
&nabla;h(xμ) = 0
</p>
<p>h(xμ) = 0.
</p>
<p>sigi(xμ) = &minus;μ; i = 1, 2, . . . , p
</p>
<p>where y is the Lagrange multiplier vector for the constraint h(xμ) = 0. Then, the
</p>
<p>Newton method can be applied to solving the condition system as μ is gradually
</p>
<p>reduced to 0, that is, following the path.
</p>
<p>Geometric Interpretation: The Primal Function
</p>
<p>There is a geometric construction that provides a simple interpretation of penalty
</p>
<p>functions. The basis of the construction itself is also useful in other areas of opti-
</p>
<p>mization, especially duality theory, as explained in the next chapter.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.3 Properties of Penalty and Barrier Functions 411
</p>
<p>Let us again consider the problem
</p>
<p>minimize f (x)
</p>
<p>subject to h(x) = 0,
(13.35)
</p>
<p>where h(x) &isin; Em. We assume that the solution point x&lowast; of (13.35) is a regular point
and that the second-order sufficiency conditions are satisfied. Corresponding to this
</p>
<p>problem we introduce the following definition:
</p>
<p>Definition. Corresponding to the constrained minimization problem (13.35), the primal
</p>
<p>function ω is defined on Em in a neighborhood of 0 to be
</p>
<p>ω(y) = min{ f (x) : h(x) = y}. (13.36)
</p>
<p>The primal function gives the optimal value of the objective for various values of
</p>
<p>the right-hand side. In particular ω(0) gives the value of the original problem.
</p>
<p>Strictly speaking the minimum in the definition (13.36) must be specified as a
</p>
<p>local minimum, in a neighborhood of x&lowast;. The existence of ω(y) then follows di-
rectly from the Sensitivity Theorem in Sect. 11.7. Furthermore, from that theorem it
</p>
<p>follows that &nabla;ω(0) = &minus;λ&lowast;T .
Now consider the penalty problem and note the following relations:
</p>
<p>min{ f (x) + 1
2
c|h(x)|2} = minx,y{ f (x) +
</p>
<p>1
</p>
<p>2
c|y|2 : h(x) = y}
</p>
<p>= miny{ω(y) +
1
</p>
<p>2
c|y|2}. (13.37)
</p>
<p>Fig. 13.4 The primal function
</p>
<p>This is illustrated in Fig. 13.4 for the case where y is one-dimensional. The primal
</p>
<p>function is the lowest curve in the figure. Its value at y = 0 is the value of the</p>
<p/>
</div>
<div class="page"><p/>
<p>412 13 Penalty and Barrier Methods
</p>
<p>original constrained problem. Above the primal function are the curves ω(y) + 1
2
cy2
</p>
<p>for various values of c. The value of the penalty problem is shown by (13.37) to be
</p>
<p>the minimum point of this curve. For large values of c this curve becomes convex
</p>
<p>near 0 even if ω(y) is not convex. Viewed in this way, the penalty functions can be
</p>
<p>thought of as convexifying the primal.
</p>
<p>Also, as c increases, the associated minimum point moves toward 0. However, it
</p>
<p>is never zero for finite c. Furthermore, in general, the criterion for u to be optimal
</p>
<p>for the penalty problem is that the gradient of ω(y) + 1
2
cy2 equals zero. This yields
</p>
<p>&nabla;ω(y) + cyT = 0. Using &nabla;ω(y) = &minus;λT and y = h(xc), where now xc denotes
the minimum point of the penalty problem, gives λ = ch(xc), which is the same
</p>
<p>as (13.23).
</p>
<p>13.4 Newton&rsquo;s Method and Penalty Functions
</p>
<p>In the next few sections we address the problem of efficiently solving the uncon-
</p>
<p>strained problems associated with a penalty or barrier method. The main difficulty
</p>
<p>is the extremely unfavorable eigenvalue structure that, as explained in Sect. 13.3,
</p>
<p>always accompanies unconstrained problems derived in this way. Certainly straight-
</p>
<p>forward application of the method of steepest descent is out of the question!
</p>
<p>One method for avoiding slow convergence for these problems is to apply New-
</p>
<p>ton&rsquo;s method (or one of its variations), since the order two convergence of Newton&rsquo;s
</p>
<p>method is unaffected by the poor eigenvalue structure. In applying the method, how-
</p>
<p>ever, special care must be devoted to the manner by which the Hessian is inverted,
</p>
<p>since it is ill-conditioned. Nevertheless, if second-order information is easily avail-
</p>
<p>able, Newton&rsquo;s method offers an extremely attractive and effective method for solv-
</p>
<p>ing modest size penalty or barrier optimization problems. When such information
</p>
<p>is not readily available, or if data handling and storage requirements of Newton&rsquo;s
</p>
<p>method are excessive, attention naturally focuses on first-order methods.
</p>
<p>A simple modified Newton&rsquo;s method can often be quite effective for some penalty
</p>
<p>problems. For example, consider the problem having only equality constraints
</p>
<p>minimize f (x)
</p>
<p>subject to h(x) = 0
(13.38)
</p>
<p>with x &isin; En, h(x) &isin; Em, m &lt; n. Applying the standard quadratic penalty method
we solve instead the unconstrained problem
</p>
<p>minimize f (x) + 1
2
c|h(x)|2 (13.39)
</p>
<p>for some large c. Calling the penalty objective function q(x) we consider the iterative
</p>
<p>process
</p>
<p>xk+1 = xk &minus; αk[I + c&nabla;h(xk)T&nabla;h(xk)]&minus;1&nabla;q(xk)T , (13.40)</p>
<p/>
</div>
<div class="page"><p/>
<p>13.5 Conjugate Gradients and Penalty Methods 413
</p>
<p>where αk is chosen to minimize q(xk+1). The matrix I + c&nabla;h(xk)
T
&nabla;h(xk) is positive
</p>
<p>definite and although quite ill-conditioned it can be inverted efficiently (see
</p>
<p>Exercise 11).
</p>
<p>According to the Modified Newton Method Theorem (Sect. 10.1) the rate of con-
</p>
<p>vergence of this method is determined by the eigenvalues of the matrix
</p>
<p>[I + c&nabla;h(xk)
T
&nabla;h(xk)]
</p>
<p>&minus;1Q(xk), (13.41)
</p>
<p>where Q(xk) is the Hessian of q at xk. In view of (13.26), as c &rarr; &infin; the ma-
trix (13.41) will have m eigenvalues that approach unity, while the remaining n &minus; m
eigenvalues approach the eigenvalues of LM evaluated at the solution x
</p>
<p>&lowast; of (13.38).
Thus, if the smallest and largest eigenvalues of LM , a and A, are located such that
</p>
<p>the interval [a, A] contains unity, the convergence ratio of this modified Newton&rsquo;s
</p>
<p>method will be equal (in the limit of c &rarr; &infin;) to the canonical ratio [(A&minus;a)/(A+a)]2
for problem (13.38).
</p>
<p>If the eigenvalues of LM are not spread below and above unity, the convergence
</p>
<p>rate will be slowed. If a point in the interval containing the eigenvalues of LM is
</p>
<p>known, a scalar factor can be introduced so that the canonical rate is achieved, but
</p>
<p>such information is often not easily available.
</p>
<p>Inequalities
</p>
<p>If there are inequality as well as equality constraints in the problem, the analogous
</p>
<p>procedure can be applied to the associated penalty objective function. The unusual
</p>
<p>feature of this case is that corresponding to an inequality constraint gi(x) � 0,
</p>
<p>the term &nabla;g+
i
(x)T&nabla;g+
</p>
<p>i
(x) used in the iteration matrix will suddenly appear if the
</p>
<p>constraint is violated. Thus the iteration matrix is discontinuous with respect to x,
</p>
<p>and as the method progresses its nature changes according to which constraints are
</p>
<p>violated. This discontinuity does not, however, imply that the method is subject to
</p>
<p>jamming, since the result of Exercise 4, Chap. 10 is applicable to this method.
</p>
<p>13.5 Conjugate Gradients and Penalty Methods
</p>
<p>The partial conjugate gradient method proposed and analyzed in Sect. 9.5 is ideally
</p>
<p>suited to penalty or barrier problems having only a few active constraints. If there are
</p>
<p>m active constraints, then taking cycles of m + 1 conjugate gradient steps will yield
</p>
<p>a rate of convergence that is independent of the penalty constant c. For example,
</p>
<p>consider the problem having only equality constraints:
</p>
<p>minimize f (x)
</p>
<p>subject to h(x) = 0,
(13.42)</p>
<p/>
</div>
<div class="page"><p/>
<p>414 13 Penalty and Barrier Methods
</p>
<p>where x &isin; En, h(x) &isin; Em, m &lt; n. Applying the standard quadratic penalty method,
we solve instead the unconstrained problem
</p>
<p>minimize f (x) + 1
2
c|h(x)|2 (13.43)
</p>
<p>for some large c. The objective function of this problem has a Hessian matrix that
</p>
<p>has m eigenvalues that are of the order c in magnitude, while the remaining n &minus; m
eigenvalues are close to the eigenvalues of the matrix LM , corresponding to problem
</p>
<p>(13.42). Thus, letting xk+1 be determined from xk by taking m + 1 steps of a (non-
</p>
<p>quadratic) conjugate gradient method, and assuming xk &rarr; x, a solution to (13.43),
the sequence { f (xk)} converges linearly to f (x) with a convergence ratio equal to
approximately
</p>
<p>(
</p>
<p>A &minus; a
A + a
</p>
<p>)2
</p>
<p>(13.44)
</p>
<p>where a and A are, respectively, the smallest and largest eigenvalues of LM(x).
</p>
<p>This is an extremely effective technique when m is relatively small. The pro-
</p>
<p>gramming logic required is only slightly greater than that of steepest descent, and
</p>
<p>the time per iteration is only about m + 1 times as great as for steepest descent.
</p>
<p>The method can be used for problems having inequality constraints as well but it is
</p>
<p>advisable to change the cycle length, depending on the number of constraints active
</p>
<p>at the end of the previous cycle.
</p>
<p>Example 3.
</p>
<p>minimize f (x1, x2, . . . , x10) =
10
&sum;
</p>
<p>k=1
kx2k
</p>
<p>subject to 1.5x1 + x2 + x3 + 0.5x4 + 0.5x5 = 5.5
</p>
<p>2.0x6 &minus; 0.5x7 &minus; 0.5x8 + x9 &minus; x10 = 2.0
x1 + x3 + x5 + x7 + x9 = 10
</p>
<p>x2 + x4 + x6 + x8 + x10 = 15.
</p>
<p>This problem was treated by the penalty function approach, and the resulting com-
</p>
<p>posite function was then solved for various values of c by using various cycle lengths
</p>
<p>of a conjugate gradient algorithm. In Table 13.1 p is the number of conjugate gra-
</p>
<p>dient steps in a cycle. Thus, p = 1 corresponds to ordinary steepest descent; p = 5
</p>
<p>corresponds, by the theory of Sect. 9.5, to the smallest value of p for which the rate
</p>
<p>of convergence is independent of c; and p = 10 is the standard conjugate gradient
</p>
<p>method. Note that for p &lt; 5 the convergence rate does indeed depend on c, while it
</p>
<p>is more or less constant for p � 5. The value of c&rsquo;s selected are not artificially large,
</p>
<p>since for c = 200 the constraints are satisfied only to within 0.5 % of their right-
</p>
<p>hand sides. For problems with nonlinear constraints the results will most likely be
</p>
<p>somewhat less favorable, since the predicted convergence rate would apply only to
</p>
<p>the tail of the sequence.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.6 Normalization of Penalty Functions 415
</p>
<p>Table 13.1 Results for example 3
</p>
<p>p (steps
per cycle)
</p>
<p>Number
of cycles
to conver-
gence
</p>
<p>No. of steps Value of modified objective
</p>
<p>c = 20
⎧
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>1
</p>
<p>3
</p>
<p>5
</p>
<p>7
</p>
<p>90 90 388.565
8 24 388.563
3 15 388.563
3 21 388.563
</p>
<p>c = 200
⎧
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>1
</p>
<p>3
</p>
<p>5
</p>
<p>7
</p>
<p>230a 230 488.607
21 63 487.446
4 20 487.438
2 14 487.433
</p>
<p>c = 2000
⎧
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>1
</p>
<p>3
</p>
<p>5
</p>
<p>7
</p>
<p>260a 260 525.238
45a 135 503.550
3 15 500.910
3 21 500.882
</p>
<p>a Program not run to convergence due to excessive time
</p>
<p>13.6 Normalization of Penalty Functions
</p>
<p>There is a good deal of freedom in the selection of penalty or barrier functions that
</p>
<p>can be exploited to accelerate convergence. We propose here a simple normaliza-
</p>
<p>tion procedure that together with a two-step cycle of conjugate gradients yields the
</p>
<p>canonical rate of convergence. Again for simplicity we illustrate the technique for
</p>
<p>the penalty method applied to the problem
</p>
<p>minimize f (x)
</p>
<p>subject to h(x) = 0
(13.45)
</p>
<p>as in Sects. 13.4 and 13.5, but the idea is easily extended to other penalty or barrier
</p>
<p>situations.
</p>
<p>Corresponding to (13.45) we consider the family of quadratic penalty functions
</p>
<p>P(x) =
1
</p>
<p>2
h(x)TΓh(x), (13.46)
</p>
<p>where Γ is a symmetric positive definite m &times;m matrix. We ask what the best choice
of Γ might be.
</p>
<p>Letting
</p>
<p>q(c, x) = f (x) + cP(x), (13.47)
</p>
<p>the Hessian of q turns out to be, using (13.26),
</p>
<p>Q(c, xk) = L(xk) + c&nabla;h(xk)
T
Γ&nabla;h(xk). (13.48)</p>
<p/>
</div>
<div class="page"><p/>
<p>416 13 Penalty and Barrier Methods
</p>
<p>The m large eigenvalues are due to the second term on the right. The observation
</p>
<p>we make is that although the m large eigenvalues are all proportional to c, they
</p>
<p>are not necessarily all equal. Indeed, for very large c these eigenvalues are deter-
</p>
<p>mined almost exclusively by the second term, and are therefore c times the nonzero
</p>
<p>eigenvalues of the matrix &nabla;h(xk)
T
Γ&nabla;h(xk). We would like to select Γ so that these
</p>
<p>eigenvalues are not spread out but are nearly equal to one another. An ideal choice
</p>
<p>for the kth iteration would be
</p>
<p>Γ = [&nabla;h(xk)&nabla;h(xk)
T ]&minus;1, (13.49)
</p>
<p>since then all nonzero eigenvalues would be exactly equal. However, we do not
</p>
<p>allow to change at each step, and therefore compromise by setting
</p>
<p>Γ = [&nabla;h(x0)&nabla;h(x0)
T ]&minus;1, (13.50)
</p>
<p>where x0 is the initial point of the iteration.
</p>
<p>Using this penalty function, the corresponding eigenvalue structure will at any
</p>
<p>point look approximately like that shown in Fig. 13.5. The eigenvalues are bunched
</p>
<p>into two separate groups. As c is increased the smaller eigenvalues move into the
</p>
<p>interval [a, A] where a and A are, as usual, the smallest and largest eigenvalues of
</p>
<p>LM at the solution to (13.45). The larger eigenvalues move forward to the right and
</p>
<p>spread further apart.
</p>
<p>Fig. 13.5 Eigenvalue distributions
</p>
<p>Using the result of Exercise 11, Chap. 9, we see that if xk+1 is determined from
</p>
<p>xk by two conjugate gradient steps, the rate of convergence will be linear at a ratio
</p>
<p>determined by the widest of the two eigenvalue groups. If our normalization is suf-
</p>
<p>ficiently accurate, the large-valued group will have the lesser width. In that case
</p>
<p>convergence of this scheme is approximately that of the canonical rate for the origi-
</p>
<p>nal problem. Thus, by proper normalization it is possible to obtain the canonical rate
</p>
<p>of convergence for only about twice the time per iteration as required by steepest
</p>
<p>descent.
</p>
<p>There are, of course, numerous variations of this method that can be used in
</p>
<p>practice.Γ can, for example, be allowed to vary at each step, or it can be occasionally
</p>
<p>updated.
</p>
<p>Example. The example problem presented in the previous section was also solved
</p>
<p>by the normalization method presented above. The results for various values of c
</p>
<p>and for cycle lengths of one, two, and three are presented in Table 13.2. (All runs
</p>
<p>were initiated from the zero vector.)</p>
<p/>
</div>
<div class="page"><p/>
<p>13.7 Penalty Functions and Gradient Projection 417
</p>
<p>Table 13.2 Results for example 3
</p>
<p>p (steps
per cycle)
</p>
<p>Number
of cycles
to conver-
gence
</p>
<p>No. of steps Value of modified objective
</p>
<p>c = 10
⎧
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>28 28 251.2657
9 18 251.2657
5 15 251.2657
</p>
<p>c = 100
⎧
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>153 153 379.5955
13 26 379.5955
11 33 379.5955
</p>
<p>c = 1000
⎧
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>261a 261 402.0903
14 28 400.1687
13 39 400.1687
</p>
<p>a Program not run to convergence due to excessive time
</p>
<p>13.7 Penalty Functions and Gradient Projection
</p>
<p>The penalty function method can be combined with the idea of the gradient projec-
</p>
<p>tion method to yield an attractive general purpose procedure for solving constrained
</p>
<p>optimization problems. The proposed combination method can be viewed either as a
</p>
<p>way of accelerating the rate of convergence of the penalty function method by elim-
</p>
<p>inating the effect of the large eigenvalues, or as a technique for efficiently handling
</p>
<p>the delicate and usually cumbersome requirement in the gradient projection method
</p>
<p>that each point be feasible. The combined method converges at the canonical rate
</p>
<p>(the same as does the gradient projection method), is globally convergent (unlike
</p>
<p>the gradient projection method), and avoids much of the computational difficulty
</p>
<p>associated with staying feasible.
</p>
<p>Underlying Concept
</p>
<p>The basic theoretical result that motivates the development of this algorithm is the
</p>
<p>Combined Steepest Descent and Newton&rsquo;s Method Theorem of Sect. 10.7. The idea
</p>
<p>is to apply this combined method to a penalty problem. For simplicity we first con-
</p>
<p>sider the equality constrained problem
</p>
<p>minimize f (x)
</p>
<p>subject to h(x) = 0,
(13.51)
</p>
<p>where x &isin; En, h(x) &isin; Em. The associated unconstrained penalty problem that we
consider is
</p>
<p>minimize q(x), (13.52)</p>
<p/>
</div>
<div class="page"><p/>
<p>418 13 Penalty and Barrier Methods
</p>
<p>where
</p>
<p>q(x) = f (x) +
1
</p>
<p>2
c|h(x)|2.
</p>
<p>At any point xk let M(xk) be the subspace tangent to the surface S k = {x : h(x) =
h(xk)}. This is a slight extension of the tangent subspaces that we have considered
before, since M(xk) is defined even for points that are not feasible. If the sequence
</p>
<p>{xk} converges to a solution xc of problem (13.52), then we expect that M(xk) will
in some sense converge to M(xc). The orthogonal complement of M(xk) is the space
</p>
<p>generated by the gradients of the constraint functions evaluated at xk. Let us denote
</p>
<p>this space by N(xk). The idea of the algorithm is to take N as the subspace over
</p>
<p>which Newton&rsquo;s method is applied, and M as the space over which the gradient
</p>
<p>method is applied. A cycle of the algorithm would be as follows:
</p>
<p>1. Given xk, apply one step of Newton&rsquo;s method over, the subspace N(xk) to obtain
</p>
<p>a point wk of the form
</p>
<p>wk = xk + &nabla;h(xk)
Tuk
</p>
<p>uk &isin; Em.
</p>
<p>2. From wk, take an ordinary steepest descent step to obtain xk+1.
</p>
<p>Of course, we must show how Step 1 can be easily executed, and this is done below,
</p>
<p>but first, without drawing out the details, let us examine the general structure of this
</p>
<p>algorithm.
</p>
<p>The process is illustrated in Fig. 13.6. The first step is analogous to the step
</p>
<p>in the gradient projection method that returns to the feasible surface; except that
</p>
<p>here the criterion is reduction of the objective function rather than satisfaction of
</p>
<p>constraints. To interpret the second step, suppose for the moment that the origi-
</p>
<p>nal problem (13.51) has a quadratic objective and linear constraints; so that, con-
</p>
<p>sequently, the penalty problem (13.52) has a quadratic objective and N(x), M(x)
</p>
<p>and &nabla;h(x) are independent of x. In that case the first (Newton) step would exactly
</p>
<p>minimize q with respect to N, so that the gradient of q at wk would be orthog-
</p>
<p>onal to N; that is, the gradient would lie in the subspace M. Furthermore, since
</p>
<p>&nabla;q(wk) = &nabla; f (wk) + ch(wk)&nabla;h(wk), we see that &nabla;q(wk) would in that case be equal
</p>
<p>to the projection of the gradient of f onto M. Hence, the second step is, in the
</p>
<p>quadratic case exactly, and in the general case approximately, a move in the direc-
</p>
<p>tion of the projected negative gradient of the original objective function.
</p>
<p>The convergence properties of such a scheme are easily predicted from the
</p>
<p>theorem on the Combined Steepest Descent and Newton&rsquo;s Method, in Sect. 10.7,
</p>
<p>and our analysis of the structure of the Hessian of the penalty objective function
</p>
<p>given by (13.26). As xk &rarr; xc the rate will be determined by the ratio of largest to
smallest eigenvalues of the Hessian restricted to M(xc).
</p>
<p>This leads, however, by what was shown in Sect. 12.3, to approximately the
</p>
<p>canonical rate for problem (13.51). Thus this combined method will yield again
</p>
<p>the canonical rate as c &rarr; &infin;.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.7 Penalty Functions and Gradient Projection 419
</p>
<p>Fig. 13.6 Illustration of the method
</p>
<p>Implementing the First Step
</p>
<p>To implement the first step of the algorithm suggested above it is necessary to show
</p>
<p>how a Newton step can be taken in the subspace N(xk). We show that, again for
</p>
<p>large values of c, this can be accomplished easily.
</p>
<p>At the point xk the function b, defined by
</p>
<p>b(u) = q(xk + &nabla;h(xk)
Tu) (13.53)
</p>
<p>for u &isin; Em, measures the variations in q with respect to displacements in N(xk).
We shall, for simplicity, assume that at each point, xk, &nabla;h(xk) has rank m. We can
</p>
<p>immediately calculate the gradient with respect to u,
</p>
<p>&nabla;b(u) = &nabla;q(xk + &nabla;h(xk)
Tu)&nabla;h(xk)
</p>
<p>T , (13.54)
</p>
<p>and the m &times; n Hessian with respect to u at u = 0,
B = &nabla;h(xk)Q(xk)&nabla;h(xk)
</p>
<p>T . (13.55)
</p>
<p>where Q is the n&times; n Hessian of q with respect to x. From (13.26) we have that at xk
Q(xk) = Lk(xk) + c&nabla;h(xk)
</p>
<p>T
&nabla;h(xk). (13.56)
</p>
<p>And given B, the direction for the Newton step in N would be
</p>
<p>dk = &minus;&nabla;h(xk)TB&minus;1&nabla;c(0)T
</p>
<p>= &minus;&nabla;h(xk)TB&minus;1&nabla;h(xk)&nabla;q(xk)T . (13.57)
</p>
<p>It is clear from (13.55) and (13.56) that exact evaluation of the Newton step
</p>
<p>requires knowledge of L(xk) which usually is costly to obtain. For large values of c,
</p>
<p>however, B can be approximated by
</p>
<p>B ≃ c[&nabla;h(xk)&nabla;h(xk)T ]2, (13.58)</p>
<p/>
</div>
<div class="page"><p/>
<p>420 13 Penalty and Barrier Methods
</p>
<p>and hence a good approximation to the Newton direction is
</p>
<p>dk = &minus;
1
</p>
<p>c
&nabla;h(xk)
</p>
<p>T [&nabla;h(xk)&nabla;h(xk)
T ]&minus;2&nabla;h(xk)&nabla;q(xk)
</p>
<p>T . (13.59)
</p>
<p>Thus a suitable implementation of one cycle of the algorithm is:
</p>
<p>1. Calculate
</p>
<p>dk = &minus;
1
</p>
<p>c
&nabla;h(xk)
</p>
<p>T [&nabla;h(xk)&nabla;h(xk)
T ]&minus;2&nabla;h(xk)&nabla;q(xk)
</p>
<p>T .
</p>
<p>2. Find βk to minimize q(xk +βdk) (using βk = 1 as an initial search point), and set
</p>
<p>wk = xk + βkdk.
</p>
<p>3. Calculate pk = &minus;&nabla;q(wk)T .
4. Find αk to minimize q(wk + αpk), and set xk+1 = wk + αkpk.
</p>
<p>It is interesting to compare the Newton step of this version of the algorithm with
</p>
<p>the step for returning to the feasible region used in the ordinary gradient projection
</p>
<p>method. We have
</p>
<p>&nabla;q(xk)
T = &nabla; f (xk)
</p>
<p>T + c&nabla;h(xk)
Th(xk). (13.60)
</p>
<p>If we neglect &nabla; f (xk)
T on the right (as would be valid if we are a long distance from
</p>
<p>the constraint boundary) then the vector dk reduces to
</p>
<p>dk = &minus;&nabla;h(xk)T [&nabla;h(xk)&nabla;h(xk)T ]&minus;1h(xk),
which is precisely the first estimate used to return to the boundary in the gradient
</p>
<p>projection method. The scheme developed in this section can therefore be regarded
</p>
<p>as one which corrects this estimate by accounting for the variation in f .
</p>
<p>An important advantage of the present method is that it is not necessary to carry
</p>
<p>out the search in detail. If β = 1 yields an improved value for the penalty objective,
</p>
<p>no further search is required. If not, one need search only until some improvement
</p>
<p>is obtained. At worst, if this search is poorly performed, the method degenerates
</p>
<p>to steepest descent. When one finally gets close to the solution, however, β = 1 is
</p>
<p>bound to yield an improvement and terminal convergence will progress at nearly the
</p>
<p>canonical rate.
</p>
<p>Inequality Constraints
</p>
<p>The procedure is conceptually the same for problems with inequality constraints.
</p>
<p>The only difference is that at the beginning of each cycle the subspace M(xk) is
</p>
<p>calculated on the basis of those constraints that are either active or violated at xk,
</p>
<p>the others being ignored. The resulting technique is a descent algorithm in that
</p>
<p>the penalty objective function decreases at each cycle; it is globally convergent
</p>
<p>because of the pure gradient step taken at the end of each cycle; its rate of conver-
</p>
<p>gence approaches the canonical rate for the original constrained problem as c &rarr; &infin;;
and there are no feasibility tolerances or subroutine iterations required.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.8 &lowast;Exact Penalty Functions 421
</p>
<p>*13.8 &lowast;Exact Penalty Functions
</p>
<p>It is possible to construct penalty functions that are exact in the sense that the
</p>
<p>solution of the penalty problem yields the exact solution to the original problem
</p>
<p>for a finite value of the penalty parameter. With these functions it is not necessary
</p>
<p>to solve an infinite sequence of penalty problems to obtain the correct solution.
</p>
<p>However, a new difficulty introduced by these penalty functions is that they are non-
</p>
<p>differentiable.
</p>
<p>For the general constrained problem
</p>
<p>minimize f (x)
</p>
<p>subject to h(x) = 0, g(x) � 0,
(13.61)
</p>
<p>consider the absolute-value penalty function
</p>
<p>P(x) =
</p>
<p>m
&sum;
</p>
<p>i=1
</p>
<p>|hi(x)| +
p
</p>
<p>&sum;
</p>
<p>j=1
</p>
<p>max(0, g j(x)). (13.62)
</p>
<p>The penalty problem is then, as usual,
</p>
<p>minimize f (x) + cP(x) (13.63)
</p>
<p>for some positive constant c. We investigate the properties of the absolute-value
</p>
<p>penalty function through an example and then generalize the results.
</p>
<p>Example 1. Consider the simple quadratic problem
</p>
<p>minimize 2x2 + 2xy + y2 &minus; 2y
subject to x = 0. (13.64)
</p>
<p>It is easy to solve this problem directly by substituting x = 0 into the objective. This
</p>
<p>leads immediately to x = 0, y = 1.
</p>
<p>If a standard quadratic penalty function is used, we minimize the objective
</p>
<p>2x2 + 2xy + y2 &minus; 2y + 1
2
cx2 (13.65)
</p>
<p>for c &gt; 0. The solution again can be easily found and is x = &minus;2/(2 + c), y =
1 &minus; 2/(2 + c). This solution approaches the true solution as c &rarr; &infin;, as predicted by
the general theory. However, for any finite c the solution is inexact.
</p>
<p>Now let us use the absolute-value penalty function. We minimize the function
</p>
<p>2x2 + 2xy + y2 &minus; 2y + c|x|. (13.66)</p>
<p/>
</div>
<div class="page"><p/>
<p>422 13 Penalty and Barrier Methods
</p>
<p>We rewrite (13.66) as
</p>
<p>2x2 + 2xy + y2 &minus; 2y + c|x|
= 2x2 + 2xy + c|x| + (y &minus; 1)2 &minus; 1
= 2x2 + 2x + c|x| + (y &minus; 1)2 + 2x(y &minus; 1) &minus; 1 (13.67)
= x2 + (2x + c|x|) + (y &minus; 1 + x)2 &minus; 1.
</p>
<p>All terms (except the &minus;l) are nonnegative if c &gt; 2. Therefore, the minimum value
of this expression is &minus;1, which is achieved (uniquely) by x = 0, y = 1. Therefore,
for c &gt; 2 the minimum point of the penalty problem is the correct solution to the
</p>
<p>original problem (13.64).
</p>
<p>We let the reader verify that λ = &minus;2 for this example. The fact that c &gt; |λ| is
required for the solution to be exact is an illustration of a general result given by the
</p>
<p>following theorem.
</p>
<p>Exact Penalty Theorem. Suppose that the point x&lowast; satisfies the second-order sufficiency
conditions for a local minimum of the constrained problem (13.61). Let λ and &micro; be the
corresponding Lagrange multipliers. Then for c &gt; max{|λi |, μ j : i = 1, 2, . . . , m, j =
1, 2, . . . , p}, x&lowast; is also a local minimum of the absolute-value penalty objective (13.62).
</p>
<p>Proof. For simplicity we assume that there are equality constraints only. Define the
</p>
<p>primal function
</p>
<p>ω(z) = min
x
{ f (x) : hi(x) = zi for i = 1, 2, . . . , m}. (13.68)
</p>
<p>The primal function was introduced in Sect. 12.3. Under our assumption the function
</p>
<p>exists in a neighborhood of x&lowast; and is continuously differentiable, with &nabla;ω(0) = &minus;λT .
Now define
</p>
<p>ωc(z) = ω(z) + c
</p>
<p>m
&sum;
</p>
<p>i=1
</p>
<p>|zi|.
</p>
<p>Then we have
</p>
<p>min
x
{ f (x) + c
</p>
<p>m
&sum;
</p>
<p>i=1
</p>
<p>|hi(x)|} = min
x,u
</p>
<p>{ f (x) + c
m
&sum;
</p>
<p>i=1
</p>
<p>|zi| : h(x) = z}
</p>
<p>= min
u
{p(z) + c
</p>
<p>m
&sum;
</p>
<p>i=1
</p>
<p>|zi|}
</p>
<p>= min
u
</p>
<p>pc(z).
</p>
<p>By the Mean Value Theorem,
</p>
<p>ω(z) = ω(0) + &nabla;ω(αz)z
</p>
<p>for some α, 0 � α � 1. Therefore,
</p>
<p>ωc(z) = ω(0) + &nabla;ω(αz)z + c
</p>
<p>m
&sum;
</p>
<p>i=1
</p>
<p>|zi|. (13.69)</p>
<p/>
</div>
<div class="page"><p/>
<p>13.9 Summary 423
</p>
<p>We know that &nabla;ω(z) is continuous at 0, and thus given ε &gt; 0 there is a neighborhood
</p>
<p>of 0 such that |&nabla;ω(z)i| &lt; |λi| + ε. Thus
</p>
<p>&nabla;ω(αz)z =
</p>
<p>m
&sum;
</p>
<p>i=1
</p>
<p>&nabla;ω(αz)izi � &minus;{max
i
</p>
<p>|&nabla;ω(αz)i|}
m
&sum;
</p>
<p>i=1
</p>
<p>|zi|
</p>
<p>� &minus;{max
i
</p>
<p>(|λi| + ε)}
m
&sum;
</p>
<p>i=1
</p>
<p>|zi|.
</p>
<p>Using this in (13.69), we obtain
</p>
<p>ωc(z) � p(0) + (c &minus; ε &minus; max |λi|)
m
&sum;
</p>
<p>i=1
</p>
<p>|zi|.
</p>
<p>For c &gt; ε+max |λi| it follows that ωc(z) is minimized at z = 0. Since ε was arbitrary,
the result holds for c &gt; max |λi|.
</p>
<p>This result is easily extended to include inequality constraints. (See Exercise 16.)
</p>
<p>�
</p>
<p>It is possible to develop a geometric interpretation of the absolute-value penalty
</p>
<p>function analogous to the interpretation for ordinary penalty functions given in
</p>
<p>Fig. 13.4. Figure 13.7 corresponds to a problem for a single constraint. The smooth
</p>
<p>curve represents the primal function of the problem. Its value at 0 is the value of the
</p>
<p>original problem, and its slope at 0 is &minus;λ. The function ωc(z) is obtained by adding
c|z| to the primal function, and this function has a discontinuous derivative at z = 0.
It is clear that for c &gt; |λ|, this composite function has a minimum at exactly z = 0,
corresponding to the correct solution.
</p>
<p>There are other exact penalty functions but, like the absolute-value penalty
</p>
<p>function, most are nondifferentiable at the solution. Such penalty functions are for
</p>
<p>this reason difficult to use directly; special descent algorithms for nondifferentiable
</p>
<p>objective functions have been developed, but they can be cumbersome. Furthermore,
</p>
<p>although these penalty functions are exact for a large enough c, it is not known at the
</p>
<p>outset what magnitude is sufficient. In practice a progression of c&rsquo;s must often be
</p>
<p>used. Because of these difficulties, the major use of exact penalty functions in non-
</p>
<p>linear programming is as merit functions&ndash;measuring the progress of descent but not
</p>
<p>entering into the determination of the direction of movement. This idea is discussed
</p>
<p>in Chap. 15.
</p>
<p>13.9 Summary
</p>
<p>Penalty methods approximate a constrained problem by an unconstrained prob-
</p>
<p>lem that assigns high cost to points that are far from the feasible region. As the
</p>
<p>approximation is made more exact (by letting the parameter c tend to infinity)
</p>
<p>the solution of the unconstrained penalty problem approaches the solution to the</p>
<p/>
</div>
<div class="page"><p/>
<p>424 13 Penalty and Barrier Methods
</p>
<p>Fig. 13.7 Geometric interpretation of absolute-value penalty function
</p>
<p>original constrained problem from outside the active constraints. Barrier methods,
</p>
<p>on the other hand, approximate a constrained problem by an (essentially) uncon-
</p>
<p>strained problem that assigns high cost to being near the boundary of the feasible
</p>
<p>region, but unlike penalty methods, these methods are applicable only to problems
</p>
<p>having a robust feasible region. As the approximation is made more exact, the
</p>
<p>solution of the unconstrained barrier problem approaches the solution to the original
</p>
<p>constrained problem from inside the feasible region.
</p>
<p>The objective functions of all penalty and barrier methods of the form P(x) =
</p>
<p>γ(h(x)), B(x) = η(g(x)) are ill-conditioned. If they are differentiable, then as c &rarr; &infin;
the Hessian (at the solution) is equal to the sum of L, the Hessian of the Lagrangian
</p>
<p>associated with the original constrained problem, and a matrix of rank r that tends to
</p>
<p>infinity (where r is the number of active constraints). This is a fundamental property
</p>
<p>of these methods.
</p>
<p>Effective exploitation of differentiable penalty and barrier functions requires that
</p>
<p>schemes be devised that eliminate the effect of the associated large eigenvalues. For
</p>
<p>this purpose the three general principles developed in earlier chapters, The Partial
</p>
<p>Conjugate Gradient Method, The Modified Newton Method, and The Combination
</p>
<p>of Steepest Descent and Newton&rsquo;s Method, when creatively applied, all yield meth-
</p>
<p>ods that converge at approximately the canonical rate associated with the original
</p>
<p>constrained problem.
</p>
<p>It is necessary to add a point of qualification with respect to some of the algo-
</p>
<p>rithms introduced in this chapter, lest it be inferred that they are offered as panaceas
</p>
<p>for the general programming problem. As has been repeatedly emphasized, the ideal
</p>
<p>study of convergence is a careful blend of analysis, good sense, and experimentation.
</p>
<p>The rate of convergence does not always tell the whole story, although it is often a
</p>
<p>major component of it. Although some of the algorithms presented in this chapter
</p>
<p>asymptotically achieve the canonical rate of convergence (at least approximately),</p>
<p/>
</div>
<div class="page"><p/>
<p>13.10 Exercises 425
</p>
<p>for large c the points may have to be quite close to the solution before this rate
</p>
<p>characterizes the process. In other words, for large c the process may converge
</p>
<p>slowly in its initial phase, and, to obtain a truly representative analysis, one must
</p>
<p>look beyond the first-order convergence properties of these methods. For this reason
</p>
<p>many people find Newton&rsquo;s method attractive, although the work at each step can
</p>
<p>be substantial.
</p>
<p>13.10 Exercises
</p>
<p>1. Show that if q(c, x) is continuous (with respect to x) and q(c, x) &rarr; &infin; as
|x| &rarr; &infin;, then q(c, x) has a minimum.
</p>
<p>2. Suppose problem (13.1), with f continuous, is approximated by the penalty
</p>
<p>problem (13.2), and let {ck} be an increasing sequence of positive constants
tending to infinity. Define q(c, x) = f (x) + cP(x), and fix ε &gt; 0. For each k let
</p>
<p>xk be determined satisfying
</p>
<p>q(ck, xk) � [min
x
</p>
<p>q(ck, x)] + ε.
</p>
<p>Show that if x&lowast; is a solution to (13.1), any limit point, x, of the sequence {xk} is
feasible and satisfies f (x) � f (x&lowast;) + ε.
</p>
<p>3. Construct an example problem and a penalty function such that, as c &rarr; &infin;, the
solution to the penalty problem diverges to infinity.
</p>
<p>4. Combined penalty and barrier method. Consider a problem of the form
</p>
<p>minimize f (x)
</p>
<p>subject to x &isin; S &cap; T
</p>
<p>and suppose P is a penalty function for S and B is a barrier function for T .
</p>
<p>Define
</p>
<p>d(c, x) = f (x) + cP(x) +
1
</p>
<p>c
B(x).
</p>
<p>Let {ck} be a sequence ck &rarr; &infin;, and for k = 1, 2, . . . let xk be a solution to
</p>
<p>minimize d(ck, x)
</p>
<p>subject to x &isin; interior of T . Assume all functions are continuous, T is compact
(and robust), the original problem has a solution x&lowast;, and that S&cap; [interior of T ]
is not empty. Show that
</p>
<p>(a) limit
k&isin;&infin;
</p>
<p>d(ck, xk) = f (x
&lowast;).
</p>
<p>(b) limit
k&isin;&infin;
</p>
<p>ckP(xk) = 0.
</p>
<p>(c) limit
k&isin;&infin;
</p>
<p>1
ck
B(xk) = 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>426 13 Penalty and Barrier Methods
</p>
<p>5. Prove the Theorem at the end of Sect. 13.2.
</p>
<p>6. Find the central path for the problem of minimizing x2 subject to x � 0.
</p>
<p>7. Consider a penalty function for the equality constraints
</p>
<p>h(x) = 0, h(x) &isin; Em,
</p>
<p>having the form
</p>
<p>P(x) = γ(h(x)) =
</p>
<p>m
&sum;
</p>
<p>i=1
</p>
<p>w(hi(x)),
</p>
<p>where w is a function whose derivative w&prime; is analytic and has a zero of order
s � 1 at zero.
</p>
<p>(a) Show that corresponding to (13.26) we have
</p>
<p>Q(ck, xk) = Lk(xk) + ck
</p>
<p>m
&sum;
</p>
<p>i=1
</p>
<p>{w&prime;&prime;(hi(xk))}&nabla;hi(xk)T&nabla;hi(xk).
</p>
<p>(b) Show that as ck &rarr; &infin;, m eigenvalues of Q(ck, xk) have magnitude on the
order of (ck)
</p>
<p>1/s.
</p>
<p>8. Corresponding to the problem
</p>
<p>minimize f (x)
</p>
<p>subject to g(x) � 0,
</p>
<p>consider the sequence of unconstrained problems
</p>
<p>minimize f (x) + [g+(x) + 1]k &minus; 1,
</p>
<p>and suppose xk is the solution to the kth problem.
</p>
<p>(a) Find an appropriate definition of a Lagrange multiplier λk to associate
</p>
<p>with xk.
</p>
<p>(b) Find the limiting form of the Hessian of the associated objective function,
</p>
<p>and determine how fast the largest eigenvalues tend to infinity.
</p>
<p>9. Repeat Exercise 8 for the sequence of unconstrained problems
</p>
<p>minimize f (x) + [(g(x) + 1)+]k.
</p>
<p>10. Morrison &rsquo;s method. Suppose the problem
</p>
<p>minimize f (x)
</p>
<p>subject to h(x) = 0
(13.70)
</p>
<p>has solution x&lowast;. Let M be an optimistic estimate of f (x&lowast;), that is, M � f (x&lowast;).
Define v(M, x) = [ f (x) &minus; M]2 + |h(x)|2 and define the unconstrained problem
</p>
<p>minimize v(M, x). (13.71)</p>
<p/>
</div>
<div class="page"><p/>
<p>13.10 Exercises 427
</p>
<p>Given Mk � f (x
&lowast;), a solution xMk to the corresponding problem (13.71) is
</p>
<p>found, then Mk is updated through
</p>
<p>Mk+1 = Mk + [v(Mk, xMk )]
1/2 (13.72)
</p>
<p>and the process repeated.
</p>
<p>(a) Show that if M = f (x&lowast;), a solution to (13.71) is a solution to (13.70).
(b) Show that if xM is a solution to (13.71), then f (xM) � f (x
</p>
<p>&lowast;).
(c) Show that if Mk � f (x
</p>
<p>&lowast;) then Mk+1 determined by (13.72) satisfies
Mk+1 � f (x
</p>
<p>&lowast;).
(d) Show that Mk &rarr; f (x&lowast;).
(e) Find the Hessian of v(M, x) (with respect to x&lowast;). Show that, to within a scale
</p>
<p>factor, it is identical to that associated with the standard penalty function
</p>
<p>method.
</p>
<p>11. Let A be an m &times; n matrix of rank m. Prove the matrix identity
</p>
<p>[I + ATA]&minus;1 = I &minus; AT [I + AAT ]&minus;1A
</p>
<p>and discuss how it can be used in conjunction with the method of Sect. 13.4.
</p>
<p>12. Show that in the limit of large c, a single cycle of the normalization method of
</p>
<p>Sect. 13.6 is exactly the same as a single cycle of the combined penalty function
</p>
<p>and gradient projection method of Sect. 13.7.
</p>
<p>13. Suppose that at some step k of the combined penalty function and gradient pro-
</p>
<p>jection method, the m&times;n matrix &nabla;h(xk) is not of rank m. Show how the method
can be continued by temporarily executing the Newton step over a subspace of
</p>
<p>dimension less than m.
</p>
<p>14. For a problem with equality constraints, show that in the combined penalty
</p>
<p>function and gradient projection method the second step (the steepest descent
</p>
<p>step) can be replaced by a step in the direction of the negative projected gradient
</p>
<p>(projected onto Mk) without destroying the global convergence property and
</p>
<p>without changing the rate of convergence.
</p>
<p>15. Develop a method that is analogous to that of Sect. 13.7, but which is a combi-
</p>
<p>nation of penalty functions and the reduced gradient method. Establish that the
</p>
<p>rate of convergence of the method is identical to that of the reduced gradient
</p>
<p>method.
</p>
<p>16. Extend the result of the Exact Penalty Theorem of Sect. 13.8 to inequalities.
</p>
<p>Write g j(x) � 0 in the form of an equality as g j(x) + y
2
j
= 0 and show that the
</p>
<p>original theorem applies.
</p>
<p>17. Develop a result analogous to that of the Exact Penalty Theorem of Sect. 13.8
</p>
<p>for the penalty function
</p>
<p>P(x) = max{0, gi(x), g2(x), . . . , gp(x), |hi(x)|, |h2(x)|, . . . , |hm(x)|}.</p>
<p/>
</div>
<div class="page"><p/>
<p>428 13 Penalty and Barrier Methods
</p>
<p>18. Solve the problem
</p>
<p>minimize x2 + xy + y2 &minus; 2y
subject to x + y = 2
</p>
<p>three ways analytically
</p>
<p>(a) with the necessary conditions.
</p>
<p>(b) with a quadratic penalty function.
</p>
<p>(c) with an exact penalty function.
</p>
<p>References
</p>
<p>13.1 The penalty approach to constrained optimization is generally attributed to
</p>
<p>Courant [C8]. For more details than presented here, see Butler and Martin
</p>
<p>[B26] or Zangwill [Z1].
</p>
<p>13.2 The barrier method is due to Carroll [C1], but was developed and popularized
</p>
<p>by Fiacco and McCormick [F4] who proved the general effectiveness of the
</p>
<p>method. Also see Frisch [F19].
</p>
<p>13.3 It has long been known that penalty problems are solved slowly by steep-
</p>
<p>est descent, and the difficulty has been traced to the ill-conditioning of the
</p>
<p>Hessian. The explicit characterization given here is a generalization of that
</p>
<p>in Luenberger [L10]. For the geometric interpretation, see Luenberger [L8].
</p>
<p>The central path for nonlinear programming was analyzed by Nesterov and
</p>
<p>Nemirovskii [N2], Jarre [J2] and den Hertog [H6].
</p>
<p>13.5 Most previous successful implementations of penalty or barrier methods have
</p>
<p>employed Newton&rsquo;s method to solve the unconstrained problems and thereby
</p>
<p>have largely avoided the effects of the ill-conditioned Hessian. See Fiacco
</p>
<p>and McCormick [F4] for some suggestions. The technique at the end of the
</p>
<p>section is new.
</p>
<p>13.6 This method was first presented in Luenberger [L13].
</p>
<p>13.8 See Luenberger [L10], for further analysis of this method.
</p>
<p>13.9 The fact that the absolute-value penalty function is exact was discovered by
</p>
<p>Zangwill [Z1]. The fact that c &gt; |λ| is sufficient for exactness was pointed
out by Luenberger [L12]. Line search methods have been developed for non-
</p>
<p>smooth functions. See Lemarechal and Mifflin [L3].
</p>
<p>13.10 For analysis along the lines of Exercise 7, see Lootsma [L7]. For the functions
</p>
<p>suggested in Exercises 8 and 9, see Levitin and Polyak [L5]. For the method
</p>
<p>of Exercise 10, see Morrison [M8].</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 14
</p>
<p>Duality and Dual Methods
</p>
<p>We first derive the duality theory of for constrained optimization, which is based
</p>
<p>on our earlier zero-order optimality conditions and the Lagrangian relaxations. The
</p>
<p>variables of the dual are typically the Lagrange multipliers associated with the con-
</p>
<p>straints in the primal problem&mdash;the original constrained optimization problem.
</p>
<p>Thus, dual methods are based on the viewpoint that it is the Lagrange multipliers
</p>
<p>which are the fundamental unknowns associated with a constrained problem; once
</p>
<p>these multipliers are known determination of the solution point is simple (at least
</p>
<p>in some situations). Dual methods, therefore, do not attack the original constrained
</p>
<p>problem directly but instead attack an alternate problem, the dual problem, whose
</p>
<p>unknowns are the Lagrange multipliers of the first problem. For a problem with n
</p>
<p>variables and m equality constraints, dual methods thus work in the m-dimensional
</p>
<p>space of Lagrange multipliers. Because Lagrange multipliers measure sensitivities
</p>
<p>and hence often have meaningful intuitive interpretations as prices associated with
</p>
<p>constraint resources, searching for these multipliers, is often, in the context of a
</p>
<p>given practical problem, as appealing as searching for the values of the original
</p>
<p>problem variables.
</p>
<p>The study of dual methods, and more particularly the introduction of the dual
</p>
<p>problem, precipitates some extensions of earlier concepts. One interesting feature of
</p>
<p>this chapter is the calculation of the Hessian of the dual problem and the discovery
</p>
<p>of a dual canonical convergence ratio associated with a constrained problem that
</p>
<p>governs the convergence of steepest ascent applied to the dual.
</p>
<p>The convergence ratio theory lead to a popular method, the method of multipli-
</p>
<p>ers based on the augmented Lagrangian, in which the Hessian condition would be
</p>
<p>significantly improved to facilitate faster convergence.
</p>
<p>The alternate direction method of multipliers is based on an idea resembling
</p>
<p>that in the coordinate descent method. Here, the gradient of the dual is calculated
</p>
<p>approximately in a block coordinate fashion using primal variables. This method is
</p>
<p>particularly effective for large-scale optimization.
</p>
<p>&copy; Springer International Publishing Switzerland 2016
</p>
<p>D.G. Luenberger, Y. Ye, Linear and Nonlinear Programming, International
Series in Operations Research &amp; Management Science 228,
DOI 10.1007/978-3-319-18842-3 14
</p>
<p>429</p>
<p/>
</div>
<div class="page"><p/>
<p>430 14 Duality and Dual Methods
</p>
<p>Cutting plane algorithms, exceedingly elementary in principle, develop a series
</p>
<p>of ever-improving approximating linear programs, whose solutions converge to the
</p>
<p>solution of the original problem. The methods differ only in the manner by which
</p>
<p>an improved approximating problem is constructed once a solution to the old app-
</p>
<p>roximation is known. The theory associated with these algorithms is, unfortunately,
</p>
<p>scant and their convergence properties are not particularly attractive. They are,
</p>
<p>however, often very easy to implement.
</p>
<p>14.1 Global Duality
</p>
<p>Duality in nonlinear programming takes its most elegant form when it is formulated
</p>
<p>globally in terms of sets and hyperplanes that touch those sets. This theory makes
</p>
<p>clear the role of Lagrange multipliers as defining hyperplanes which can be consid-
</p>
<p>ered as dual to points in a vector space. The theory provides a symmetry between
</p>
<p>primal and dual problems and this symmetry can be considered as perfect for con-
</p>
<p>vex problems. For non-convex problems the &ldquo;imperfection&rdquo; is made clear by the
</p>
<p>duality gap which has a simple geometric interpretation. The global theory, which
</p>
<p>is presented in this section, serves as useful background when later we specialize to
</p>
<p>a local duality theory that can be used even without convexity and which is central
</p>
<p>to the understanding of the convergence of dual algorithms.
</p>
<p>As a counterpoint to Sect. 11.9 where equality constraints were considered before
</p>
<p>inequality constraints, here we shall first consider a problem with inequality con-
</p>
<p>straints. In particular, consider the problem
</p>
<p>minimize f (x) (14.1)
</p>
<p>subject to g(x) &le; 0
x &isin; Ω.
</p>
<p>Ω &sub; En is a convex set, and the functions f and g are defined on Ω. The function g
is p-dimensional. The problem is not necessarily convex, but we assume that there
</p>
<p>is a feasible point. Recall that the primal function associated with (14.1) is defined
</p>
<p>for z &isin; Ep as
ω(z) = inf{ f (x) : g(x) &le; z, x &isin; Ω}, (14.2)
</p>
<p>defined by letting the right hand side of inequality constraint take on arbitrary
</p>
<p>values. It is understood that (14.2) is defined on the set D = {z : g(x) &le; z, for
some x &isin; Ω}.
</p>
<p>If problem (14.1) has a solution x&lowast; with value f &lowast; = f (x&lowast;), then f &lowast; is the point on
the vertical axis in Ep+1 where the primal function passes through the axis. If (14.1)
</p>
<p>does not have a solution, then f &lowast; = inf{ f (x) : g(x) &le; 0, x &isin; Ω} is the intersection
point.</p>
<p/>
</div>
<div class="page"><p/>
<p>14.1 Global Duality 431
</p>
<p>The duality principle is derived from consideration of all hyperplanes that lie
</p>
<p>below the primal function. As illustrated in Fig. 14.1 the intercept with the vertical
</p>
<p>axis of such a hyperplanes lies below (or at) the value f &lowast;.
To express this property we define the dual function defined on the positive cone
</p>
<p>in Ep as
</p>
<p>φ(&micro;) = inf{ f (x) + &micro;Tg(x) : x &isin; Ω}. (14.3)
</p>
<p>Fig. 14.1 Hyperplane below ω(z)
</p>
<p>In general, φ may not be finite throughout the positive orthant E
p
+ but the region
</p>
<p>where it is finite is convex.
</p>
<p>Proposition 1. The dual function is concave on the region where it is finite.
</p>
<p>Proof. Suppose &micro;1, &micro;2 are in the finite region, and let 0 &le; α &le; 1. Then
</p>
<p>φ(α&micro;1 + (1 &minus; α&micro;2)) = inf{ f (x) + (α&micro;1 + (1 &minus; α)&micro;2)Tg(x) : x &isin; Ω}
&ge; inf{α f (x1) + α&micro;Tl g(x1) : x1 &isin; Ω}
+ inf{(1 &minus; α) f (x2) + (1 &minus; α)&micro;T2 g(x2) : x2 &isin; Ω}
= αφ(&micro;1) + (1 &minus; α)φ(&micro;2).�
</p>
<p>We define φ&lowast; = sup{φ(&micro;) : &micro; &ge; 0} where it is understood that the supremum is
taken over the region where φ is finite. We can now state the weak form of global
</p>
<p>duality.
</p>
<p>Weak Duality Proposition. φ&lowast; &le; f &lowast;.
</p>
<p>Proof. For every &micro; &ge; 0 we have
</p>
<p>φ(&micro;) = inf{ f (x) + &micro;Tg(x) : x &isin; Ω}
&le; inf{ f (x) + &micro;Tg(x) : g(x) &le; 0, x &isin; Ω}
&le; inf{ f (x) : g(x) &le; 0, x &isin; Ω} = f &lowast;.
</p>
<p>Taking the supremum over the left hand side gives φ&lowast; &le; f &lowast;. �</p>
<p/>
</div>
<div class="page"><p/>
<p>432 14 Duality and Dual Methods
</p>
<p>Hence the dual function gives lower bounds on the optimal value f &lowast;.
This dual function has a strong geometric interpretation. Consider a p + 1-
</p>
<p>dimensional vector (1, &micro;) &isin; Ep+1 with &micro; &ge; 0 and a constant c. The set of vectors
(r, z) such that the inner product (1, &micro;)T (r, z) &equiv; r + &micro;Tz = c defines a hyperplane
in Ep+1. Different values of c give different hyperplanes, all of which are parallel.
</p>
<p>For a given (1, &micro;) we consider the lowest possible hyperplane of this form
</p>
<p>that just barely touches (supports) the region above the primal function of prob-
</p>
<p>lem (14.1). Suppose x1 defines the touching point with values r = f (x1) and
</p>
<p>z = g(x1). Then c = f (x1) + &micro;
Tg(x1) = φ(&micro;).
</p>
<p>The hyperplane intersects the vertical axis at a point of the form (r0, 0). This
</p>
<p>point also must satisfy (1, &micro;)T (r0, 0) = c = φ(&micro;). This gives c = r0. Thus the
</p>
<p>intercept gives φ(&micro;) directly. Thus the dual function at &micro; is equal to the intercept of
</p>
<p>the hyperplane defined by &micro; that just touches the epigraph of the primal function.
</p>
<p>Fig. 14.2 The highest hyperplane
</p>
<p>Furthermore, this intercept (and dual function value) is maximized by the
</p>
<p>Lagrange multiplier which corresponds to the largest possible intercept, at a point
</p>
<p>no higher than the optimal value f &lowast;. See Fig. 14.2.
By introducing convexity assumptions, the foregoing analysis can be strength-
</p>
<p>ened to give the strong duality theorem, with no duality gap when the intercept is
</p>
<p>at f &lowast;. See Fig. 14.3.
We shall state the result for the more general problem that includes equality con-
</p>
<p>straints of the form h(x) = 0, as in Sect. 11.9. Specifically, we consider the problem
</p>
<p>maximize f (x) (14.4)
</p>
<p>subject to h(x) = 0, g(x) &le; 0
x &isin; Ω
</p>
<p>where h is affine of dimension m, g is convex of dimension p, andΩ is a convex set.
</p>
<p>In this case the dual function is
</p>
<p>φ(λ, &micro;) = inf{ f (x) + λTh(x) + &micro;Tg(x) : x &isin; Ω}.</p>
<p/>
</div>
<div class="page"><p/>
<p>14.1 Global Duality 433
</p>
<p>And let
</p>
<p>φ&lowast; = sup{φ(λ, &micro;) : λ &isin; Em, &micro; &isin; Ep, &micro; &ge; 0}.
</p>
<p>Fig. 14.3 The strong duality theorem. There is no duality gap
</p>
<p>Strong Duality Theorem. Suppose in the problem (14.4), h is affine and regular with respect
to Ω and there is a point x1 &isin; Ω with that h(x) = 0 and g(x) &lt; 0.
Suppose the problem has solution x&lowast; with value f (x&lowast;) = f &lowast;. Then for every λ and &micro; &ge; 0
there holds
</p>
<p>φ&lowast; &le; f &lowast;.
Furthermore, there are λ, &micro; &ge; 0 such that
</p>
<p>φ(λ, &micro;) = f &lowast;
</p>
<p>and hence φ&lowast; = f &lowast;. Moreover, the λ and &micro; above are Lagrange multipliers for the original
problem.
</p>
<p>Proof. The proof follows almost immediately from the zero-order Lagrange theorem
</p>
<p>of Sect. 11.9. The Lagrange multipliers of that theorem give
</p>
<p>f &lowast; = max{ f (x) + λTh(x) + &micro;Tg(x) : x &isin; Ω}
= φ(λ, &micro;) &le; φ&lowast; &le; f &lowast;.
</p>
<p>Equality must hold across the inequalities, which establishes the results. �
</p>
<p>As a nice summary we can place the primal and dual problems together for the
</p>
<p>problem with inequality constraints.
</p>
<p>Primal Dual
</p>
<p>f &lowast; = minimize ω(z) φ&lowast; = maximize φ(&micro;)
subject to z &le; 0 subject to &micro; &ge; 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>434 14 Duality and Dual Methods
</p>
<p>Example 1 (Quadratic Program). Consider the problem
</p>
<p>minimize
1
</p>
<p>2
xT Qx (14.5)
</p>
<p>subject to Bx &minus; b &le; 0.
</p>
<p>The dual function is
</p>
<p>φ(&micro;) = min
x
</p>
<p>1
</p>
<p>2
xTQx + &micro;T (Bx &minus; b).
</p>
<p>This gives the necessary conditions
</p>
<p>Qx + BT&micro; = 0
</p>
<p>and hence x = &minus;Q&minus;1BT&micro;. Substituting this into φ(&micro;) gives
</p>
<p>φ(&micro;) = &minus;1
2
&micro;TBQ&minus;1BT&micro; &minus; &micro;Tb.
</p>
<p>Hence the dual problem is
</p>
<p>maximize &minus; 1
2
&micro;TBQ&minus;1BT&micro; &minus; &micro;Tb (14.6)
</p>
<p>subject to &micro; &ge; 0,
</p>
<p>which is also a quadratic programming problem. If this problem is solved for &micro;, that
</p>
<p>&micro; will be the Lagrange multiplier for the primal problem (14.5).
</p>
<p>Note that the first-order conditions for the dual problem (14.6) imply
</p>
<p>&micro;T [&minus;BQ&minus;1BT&micro; &minus; b] = 0,
</p>
<p>which by substituting the formula for x is equivalent to
</p>
<p>&micro;T [Bx &minus; b] = 0.
</p>
<p>This is the complementary slackness condition for the original (primal) prob-
</p>
<p>lem (14.5).
</p>
<p>Example 2 (Integer Solutions). Duality gaps may arise if the object function or the
</p>
<p>constraint functions are not convex. A gap may also arise if the underlying set is not
</p>
<p>convex. This is characteristic, for example, of problems in which the components of
</p>
<p>the solution vector are constrained to be integers. For instance, consider the problem
</p>
<p>minimize x21 + 2x
2
2
</p>
<p>subject to x1 + x2 &ge; 1/2
x1, x2 nonnegative integers</p>
<p/>
</div>
<div class="page"><p/>
<p>14.2 Local Duality 435
</p>
<p>It is clear that the solution is x1 = 1, x2 = 0, with objective value f
&lowast; = 1. To put this
</p>
<p>problem in the standard form we have discussed, we write the constraint as
</p>
<p>&minus;x1 &minus; x2 + 1/2 &le; z, where z = 0.
The primal function ω(z) is equal to 0 for z &ge; 1/2 since then x1 = x2 = 0 is feasible.
The entire primal function has steps as z steps negatively integer by integer, as shown
</p>
<p>in Fig. 14.4.
</p>
<p>Fig. 14.4 Duality for an integer problem
</p>
<p>The dual function is
</p>
<p>φ(μ) = max{x21 + x22 &minus; λ(x1 + x2 &minus; 1/2)}
where the maximum is taken with respect to the integer constraint. Analytically, the
</p>
<p>solution for small values of &micro; is
</p>
<p>φ(μ) = μ/2 for 0 &le; μ &le; 1,
= 1 &minus; μ/2 for 1 &le; μ &le; 2,
... and more
</p>
<p>The maximum value of φ(μ) is the maximum intercept of the corresponding hy-
</p>
<p>perplanes (lines, in this case) with the vertical axis. This occurs for μ = 1 with
</p>
<p>a corresponding value of φ&lowast; = φ(1) = 1/2. We have φ&lowast; &lt; f &lowast; and the difference
f &lowast; &minus; φ&lowast; = 1/2 is the duality gap.
</p>
<p>14.2 Local Duality
</p>
<p>In practice the mechanics of duality are frequently carried out locally, by setting
</p>
<p>derivatives to zero, or moving in the direction of a gradient. For these operations
</p>
<p>the beautiful global theory can in large measure be replaced by a weaker but often</p>
<p/>
</div>
<div class="page"><p/>
<p>436 14 Duality and Dual Methods
</p>
<p>more useful local theory. This theory requires a minimum of convexity assumptions
</p>
<p>defined locally. We present such a theory in this section, since it is in keeping with
</p>
<p>the spirit of the earlier chapters and is perhaps the simplest way to develop compu-
</p>
<p>tationally useful duality results.
</p>
<p>As often done before for convenience, we again consider nonlinear programming
</p>
<p>problems of the form
</p>
<p>minimize f (x) (14.7)
</p>
<p>subject to h(x) = 0,
</p>
<p>where x &isin; En, h(x) &isin; En and f , h &isin; C2. Global convexity is not assumed here.
Everything we do can be easily extended to problems having inequality as well as
</p>
<p>equality constraints, for the price of a somewhat more involved notation.
</p>
<p>We focus attention on a local solution x&lowast; of (14.7). Assuming that x&lowast; is a regular
point of the constraints, then, as we know, there will be a corresponding Lagrange
</p>
<p>multiplier (row) vector λ&lowast; such that
</p>
<p>&nabla; f (x&lowast;) + (λ&lowast;)T&nabla;h(x&lowast;) = 0, (14.8)
</p>
<p>and the Hessian of the Lagrangian
</p>
<p>L(x&lowast;) = F(x&lowast;) + (λ&lowast;)TH(x&lowast;) (14.9)
</p>
<p>must be positive semidefinite on the tangent subspace
</p>
<p>M = {x : &nabla;h(x&lowast;)x = 0}.
</p>
<p>At this point we introduce the special local convexity assumption necessary for
</p>
<p>the development of the local duality theory. Specifically, we assume that the Hessian
</p>
<p>L(x&lowast;) is positive definite. Of course, it should be emphasized that by this we mean
L(x&lowast;) is positive definite on the whole space En, not just on the subspace M. The
assumption guarantees that the Lagrangian l(x) = f (x)+ (λ&lowast;)Th(x) is locally convex
at x&lowast;.
</p>
<p>With this assumption, the point x&lowast; is not only a local solution to the constrained
problem (14.7); it is also a local solution to the unconstrained problem
</p>
<p>minimize f (x) + (λ&lowast;)Th(x), (14.10)
</p>
<p>since it satisfies the first- and second-order sufficiency conditions for a local mini-
</p>
<p>mum point. Furthermore, for any λ sufficiently close to λ&lowast; the function f (x)+λTh(x)
will have a local minimum point at a point x near x&lowast;. This follows by noting that, by
the Implicit Function Theorem, the equation
</p>
<p>&nabla; f (x) + λT&nabla;h(x) = 0 (14.11)</p>
<p/>
</div>
<div class="page"><p/>
<p>14.2 Local Duality 437
</p>
<p>has a solution x near x&lowast; when λ is near λ&lowast;, because L&lowast; is nonsingular; and by the fact
that, at this solution x, the Hessian F(x) + λTH(x) is positive definite. Thus locally
</p>
<p>there is a unique correspondence between λ and x through solution of the uncon-
</p>
<p>strained problem
</p>
<p>minimize f (x) + λTh(x). (14.12)
</p>
<p>Furthermore, this correspondence is continuously differentiable.
</p>
<p>Near λ&lowast; we define the dual function φ by the equation
</p>
<p>φ(λ) = minimum [ f (x) + λTh(x)], (14.13)
</p>
<p>where here it is understood that the minimum is taken locally with respect to x
</p>
<p>near x&lowast;. We are then able to show (and will do so below) that locally the original
constrained problem (14.7) is equivalent to unconstrained local maximization of the
</p>
<p>dual function φ with respect to λ. Hence we establish an equivalence between a
</p>
<p>constrained problem in x and an unconstrained problem in λ.
</p>
<p>To establish the duality relation we must prove two important lemmas. In the
</p>
<p>statements below we denote by x(λ) the unique solution to (14.12) in the neighbor-
</p>
<p>hood of x&lowast;.
</p>
<p>Lemma 1. The dual function φ has gradient
</p>
<p>&nabla;φ(λ) = h(x(λ))T (14.14)
</p>
<p>Proof. We have explicitly, from (14.13),
</p>
<p>φ(λ) = f (x(λ)) + λTh(x(λ)).
</p>
<p>Thus
</p>
<p>&nabla;φ(λ) = [&nabla; f (x(λ)) + λT&nabla;h(x(λ))]&nabla;x(λ) + h(x(λ))T .
</p>
<p>Since the first term on the right vanishes by definition of x(λ), we obtain (14.14). �
</p>
<p>Lemma 1 is of extreme practical importance, since it shows that the gradient of
</p>
<p>the dual function is simple to calculate. Once the dual function itself is evaluated,
</p>
<p>by minimization with respect to x, the corresponding h(x)T , which is the gradient,
</p>
<p>can be evaluated without further calculation.
</p>
<p>The Hessian of the dual function can be expressed in terms of the Hessian of the
</p>
<p>Lagrangian. We use the notation L(x, λ) = F(x) + λTH(x), explicitly indicating the
</p>
<p>dependence on λ. (We continue to use L(x&lowast;) when λ = λ&lowast; is understood.) We then
have the following lemma.
</p>
<p>Lemma 2. The Hessian of the dual function is
</p>
<p>Φ(λ) = &minus;&nabla;h(x(λ))L&minus;1(x(λ), λ)&nabla;h(x(λ))T . (14.15)
</p>
<p>Proof. The Hessian is the derivative of the gradient. Thus, by Lemma 1,
</p>
<p>Φ(λ) = &nabla;h(x(λ))&nabla;x(λ). (14.16)</p>
<p/>
</div>
<div class="page"><p/>
<p>438 14 Duality and Dual Methods
</p>
<p>By definition we have
</p>
<p>&nabla; f (x(λ)) + λT&nabla;h(x(λ)) = 0,
</p>
<p>and differentiating this with respect to λ we obtain
</p>
<p>L(x(λ), λ)&nabla;x(λ) + &nabla;h(x(λ))T = 0.
</p>
<p>Solving for &nabla;x(λ) and substituting in (14.16) we obtain (14.15). �
</p>
<p>Since L&minus;1(x(λ)) is positive definite, and since &nabla;h(x(λ)) is of full rank near x&lowast;,
we have as an immediate consequence of Lemma 2 that the m &times; m Hessian of φ is
negative definite. As might be expected, this Hessian plays a dominant role in the
</p>
<p>analysis of dual methods.
</p>
<p>Local Duality Theorem. Suppose that the problem
</p>
<p>minimize f (x) (14.17)
</p>
<p>subject to h(x) = 0
</p>
<p>has a local solution at x&lowast; with corresponding value r&lowast; and Lagrange multiplier λ&lowast;. Suppose
also that x&lowast; is a regular point of the constraints and that the corresponding Hessian of the
Lagrangian L&lowast; = L(x&lowast;) is positive definite. Then the dual problem
</p>
<p>maximize φ(λ) (14.18)
</p>
<p>has a local solution at λ&lowast; with corresponding value r&lowast; and x&lowast; as the point corresponding to
λ&lowast; in the definition of φ.
</p>
<p>Proof. It is clear that x&lowast; corresponds to λ&lowast; in the definition of φ. Now at λ&lowast; we have
by Lemma 1
</p>
<p>&nabla;φ(λ&lowast;) = h(x&lowast;)T = 0,
</p>
<p>and by Lemma 2 the Hessian of φ is negative definite. Thus λ&lowast; satisfies the first- and
second-order sufficiency conditions for an unconstrained maximum point of φ. The
</p>
<p>corresponding value φ(λ&lowast;) is found from the definition of φ to be r&lowast;. �
</p>
<p>Example 1. Consider the problem in two variables
</p>
<p>minimize &minus; xy
subject to (x &minus; 3)2 + y2 = 5.
</p>
<p>The first-order necessary conditions are
</p>
<p>&minus;y + (2x &minus; 6)λ = 0
&minus;x + 2yλ = 0
</p>
<p>together with the constraint. These equations have a solution at
</p>
<p>x = 4, y = 2, λ = 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>14.2 Local Duality 439
</p>
<p>The Hessian of the corresponding Lagrangian is
</p>
<p>L =
</p>
<p>[
</p>
<p>2 &minus;1
&minus;1 2
</p>
<p>]
</p>
<p>.
</p>
<p>Since this is positive definite, we conclude that the solution obtained is a local
</p>
<p>minimum. (It can be shown, in fact, that it is the global solution.)
</p>
<p>Since L is positive definite, we can apply the local duality theory near this
</p>
<p>solution. We define
</p>
<p>φ(λ) = min{&minus;xy + λ[(x &minus; 3)2 + y2 &minus; 5]},
</p>
<p>which leads to
</p>
<p>φ(λ) =
4λ + 4λ3 &minus; 80λ5
</p>
<p>(4λ2 &minus; 1)2
valid for λ &gt; 1
</p>
<p>2
. It can be verified that φ has a local maximum at λ = 1.
</p>
<p>Inequality Constraints
</p>
<p>For problems having inequality constraints as well as equality constraints the above
</p>
<p>development requires only minor modification. Consider the problem
</p>
<p>minimize f (x)
</p>
<p>subject to h(x) = 0 (14.19)
</p>
<p>g(x) &le; 0,
</p>
<p>where g(x) &isin; Ep, g &isin; C2 and everything else is as before. Suppose x&lowast; is a local
solution of (14.19) and is a regular point of the constraints. Then, as we know, there
</p>
<p>are Lagrange multipliers λ&lowast; and &micro;&lowast; &ge; 0 such that
</p>
<p>&nabla; f (x&lowast;) + (λ&lowast;)T&nabla;h(x&lowast;) + (&micro;&lowast;)T&nabla;g(x&lowast;) = 0 (14.20)
</p>
<p>(&micro;&lowast;)Tg(x&lowast;) = 0. (14.21)
</p>
<p>We impose the local convexity assumptions that the Hessian of the Lagrangian
</p>
<p>L(x&lowast;) = F(x&lowast;) + (λ&lowast;)TH(x&lowast;) + (&micro;&lowast;)TG(x&lowast;) (14.22)
</p>
<p>is positive definite (on the whole space).
</p>
<p>For λ and &micro; &ge; 0 near λ&lowast; and &micro;&lowast; we define the dual function
</p>
<p>φ(λ, &micro;) = min[ f (x) + λTh(x) + &micro;Tg(x)], (14.23)
</p>
<p>where the minimum is taken locally near x&lowast;. Then, it is easy to show, paralleling the
development above for equality constraints, that φ achieves a local maximum with
</p>
<p>respect to λ, &micro; &ge; 0 at λ&lowast;, &micro;&lowast;.</p>
<p/>
</div>
<div class="page"><p/>
<p>440 14 Duality and Dual Methods
</p>
<p>Partial Duality
</p>
<p>It is not necessary to include the Lagrange multipliers of all the constraints of a
</p>
<p>problem in the definition of the dual function. In general, if the local convexity ass-
</p>
<p>umption holds, local duality can be defined with respect to any subset of functional
</p>
<p>constraints. Thus, for example, in the problem
</p>
<p>minimize f (x)
</p>
<p>subject to h(x) = 0 (14.24)
</p>
<p>g(x) &le; 0,
</p>
<p>we might define the dual function with respect to only the equality constraints. In
</p>
<p>this case we would define
</p>
<p>φ(λ) = min
g(x)&le;0
</p>
<p>{ f (x) + λTh(x)}, (14.25)
</p>
<p>where the minimum is taken locally near the solution x&lowast; but constrained by the
remaining constraints g(x) &le; 0. Again, the dual function defined in this way will
achieve a local maximum at the optimal Lagrange multiplier λ&lowast;. The partial dual is
especially useful when constraints g(x) &le; 0 are simple such as x &le; 0 or in a box.
</p>
<p>14.3 Canonical Convergence Rate of Dual Steepest Ascent
</p>
<p>Constrained problems satisfying the local convexity assumption can be solved by
</p>
<p>solving the associated unconstrained dual problem, and any of the standard algo-
</p>
<p>rithms discussed in Chaps. 7 through 10 can be used for this purpose. Of course,
</p>
<p>the method that suggests itself immediately is the method of steepest ascent. It can
</p>
<p>be implemented by noting that, according to Lemma 1. Section 14.2, the gradient
</p>
<p>of φ is available almost without cost once φ itself is evaluated. Without some spe-
</p>
<p>cial properties, however, the method as a whole can be extremely costly to execute,
</p>
<p>since every evaluation of φ requires the solution of an unconstrained problem in the
</p>
<p>unknown x. Nevertheless, as shown in the next section, many important problems
</p>
<p>do have a structure which is suited to this approach.
</p>
<p>The method of steepest ascent, and other gradient-based algorithms, when applied
</p>
<p>to the dual problem will have a convergence rate governed by the eigenvalue struc-
</p>
<p>ture of the Hessian of the dual function φ. At the Lagrange multiplier λ&lowast; correspond-
ing to a solution x&lowast; this Hessian is (according to Lemma 2, Sect. 13.1)
</p>
<p>Φ = &minus;&nabla;h(x&lowast;)(L&lowast;)&minus;1&nabla;h(x&lowast;)T .
</p>
<p>This expression shows that Φ is in some sense a restriction of the matrix (L&lowast;)&minus;1 to the
subspace spanned by the gradients of the constraint functions, which is the orthog-
</p>
<p>onal complement of the tangent subspace M. This restriction is not the orthogonal</p>
<p/>
</div>
<div class="page"><p/>
<p>14.4 Separable Problems and Their Duals 441
</p>
<p>restriction of (L&lowast;)&minus;1 onto the complement of M since the particular representation of
the constraints affects the structure of the Hessian. We see, however, that while the
</p>
<p>convergence of primal methods is governed by the restriction of L&lowast; to M, the con-
vergence of dual methods is governed by a restriction of (L&lowast;)&minus;1 to the orthogonal
complement of M.
</p>
<p>The dual canonical convergence rate associated with the original constrained
</p>
<p>problem, which is the rate of convergence of steepest ascent applied to the dual,
</p>
<p>is (B &minus; b)2/(B + b)2 where b and B are, respectively, the smallest and largest
eigenvalues of
</p>
<p>&minus;Φ = &nabla;h(x&lowast;)(L&lowast;)&minus;1&nabla;h(x&lowast;)T .
For locally convex programming problems, this rate is as important as the primal
</p>
<p>canonical rate.
</p>
<p>Scaling
</p>
<p>We conclude this section by pointing out a kind of complementarity that exists
</p>
<p>between the primal and dual rates. Suppose one calculates the primal and dual
</p>
<p>canonical rates associated with the locally convex constrained problem
</p>
<p>minimize f (x)
</p>
<p>subject to h(x) = 0.
</p>
<p>If a change of primal variables x is introduced, the primal rate will in general change
</p>
<p>but the dual rate will not. On the other hand, if the constraints are transformed (by
</p>
<p>replacing them by Th(x) = 0 where T is a nonsingular m &times; m matrix), the dual rate
will change but the primal rate will not.
</p>
<p>14.4 Separable Problems and Their Duals
</p>
<p>A structure that arises frequently in mathematical programming applications is that
</p>
<p>of the separable problem:
</p>
<p>minimize
</p>
<p>q
&sum;
</p>
<p>i=1
</p>
<p>fi(xi) (14.26)
</p>
<p>subject to
</p>
<p>q
&sum;
</p>
<p>i=1
</p>
<p>hi(xi) = 0 (14.27)
</p>
<p>q
&sum;
</p>
<p>i=1
</p>
<p>gi(xi) &le; 0. (14.28)</p>
<p/>
</div>
<div class="page"><p/>
<p>442 14 Duality and Dual Methods
</p>
<p>In this formulation the components of the n-vector x are partitioned into q disjoint
</p>
<p>groups, x = (x1, x2, . . . , xq) where the groups may or may not have the same
</p>
<p>number of components. Both the objective function and the constraints separate into
</p>
<p>sums of functions of the individual groups. For each i, the functions fi, hi, and gi are
</p>
<p>twice continuously differentiable functions of dimensions 1, m, and p, respectively.
</p>
<p>Example 1. Suppose that we have a fixed budget of, say, A dollars that may be
</p>
<p>allocated among n activities. If xi dollars is allocated to the ith activity, then there
</p>
<p>will be a benefit (measured in some units) of fi(xi). To obtain the maximum benefit
</p>
<p>within our budget, we solve the separable problem
</p>
<p>maximize
</p>
<p>n
&sum;
</p>
<p>i=1
</p>
<p>fi(xi)
</p>
<p>subject to
</p>
<p>n
&sum;
</p>
<p>i=1
</p>
<p>xi � A (14.29)
</p>
<p>xi � 0.
</p>
<p>In the example x is partitioned into its individual components.
</p>
<p>Example 2. Problems involving a series of decisions made at distinct times are often
</p>
<p>separable. For illustration, consider the problem of scheduling water release through
</p>
<p>a dam to produce as much electric power as possible over a given time interval
</p>
<p>while satisfying constraints on acceptable water levels. A discrete-time model of
</p>
<p>this problem is to
</p>
<p>maximize
</p>
<p>N
&sum;
</p>
<p>k=1
</p>
<p>f (y(k), u(k))
</p>
<p>subject to y(k) = y(k &minus; 1) &minus; u(k) + s(k), k = 1, . . . , N
c � y(k) � d, k = 1, . . . , N
</p>
<p>0 � u(k), k = 1, . . . , N.
</p>
<p>Here y(k) represents the water volume behind the dam at the end of period k, u(k)
</p>
<p>represents the volume flow through the dam during period k, and s(k) is the volume
</p>
<p>flowing into the lake behind the dam during period k from upper streams. The func-
</p>
<p>tion f gives the power generation, and c and d are bounds on lake volume. The
</p>
<p>initial volume y(0) is given.
</p>
<p>In this example we consider x as the 2N-dimensional vector of unknowns
</p>
<p>y(k), u(k), k = 1, 2, . . . , N. This vector is partitioned into the pairs xk = (y(k), u(k)).
</p>
<p>The objective function is then clearly in separable form. The constraints can be
</p>
<p>viewed as being in the form (14.27) with hk(xk) having dimension N and such that
</p>
<p>hk(xk) is identically zero except in the k and k + 1 components.</p>
<p/>
</div>
<div class="page"><p/>
<p>14.4 Separable Problems and Their Duals 443
</p>
<p>Decomposition
</p>
<p>Separable problems are ideally suited to dual methods, because the required unconst-
</p>
<p>rained minimization decomposes into small subproblems. To see this we recall
</p>
<p>that the generally most difficult aspect of a dual method is evaluation of the
</p>
<p>dual function. For a separable problem, if we associate λ with the equality con-
</p>
<p>straints (14.27) and &micro; � 0 with the inequality constraints (14.28), the required dual
</p>
<p>function is
</p>
<p>φ(λ, &micro;) = min
</p>
<p>q
&sum;
</p>
<p>i=1
</p>
<p>(
</p>
<p>fi(xi) + λ
Thi(xi) + &micro;
</p>
<p>Tgi(xi)
)
</p>
<p>.
</p>
<p>This minimization problem decomposes into the q separate problems
</p>
<p>min
xi
</p>
<p>fi(xi) + λ
Thi(xi) + &micro;
</p>
<p>Tgi(xi).
</p>
<p>The solution of these subproblems can usually be accomplished relatively effi-
</p>
<p>ciently, since they are of smaller dimension than the original problem.
</p>
<p>Example 3. In Example 1 using duality with respect to the budget constraint, the ith
</p>
<p>subproblem becomes, for μ &gt; 0
</p>
<p>max
xi�0
</p>
<p>fi(xi) &minus; μxi,
</p>
<p>which is only a one-dimensional problem. It can be interpreted as setting a benefit
</p>
<p>value μ for dollars and then maximizing total benefit from activity i, accounting for
</p>
<p>the dollar expenditure.
</p>
<p>Example 4. In Example 1 using duality with respect to the equality constraints we
</p>
<p>denote the dual variables by λ(k), k = 1, 2, . . . , N. The kth subproblem becomes
</p>
<p>max
c�y(k)�d
</p>
<p>0�u(k)
</p>
<p>{ f (y(k), u(k)) + [λ(k + 1) &minus; λ(k)]y(k) &minus; λ(k)[u(k) &minus; s(k)]}
</p>
<p>which is a two-dimensional optimization problem. Selection of λ &isin; EN decom-
poses the problem into separate problems for each time period. The variable λ(k)
</p>
<p>can be regarded as a value, measured in units of power, for water at the beginning
</p>
<p>of period k. The kth subproblem can then be interpreted as that faced by an ent-
</p>
<p>repreneur who leased the dam for one period. He can buy water for the dam at the
</p>
<p>beginning of the period at price λ(k) and sell what he has left at the end of the period
</p>
<p>at price λ(k+1). His problem is to determine y(k) and u(k) so that his net profit, acc-
</p>
<p>ruing from sale of generated power and purchase and sale of water, is maximized.
</p>
<p>Example 5 (The Hanging Chain). Consider again the problem of finding the equi-
</p>
<p>librium position of the hanging chain considered in Example 4, Sect. 11.3, and
</p>
<p>Example 1, Sect. 12.7. The problem is</p>
<p/>
</div>
<div class="page"><p/>
<p>444 14 Duality and Dual Methods
</p>
<p>minimize
</p>
<p>n
&sum;
</p>
<p>i=1
</p>
<p>ciyi
</p>
<p>subject to
</p>
<p>n
&sum;
</p>
<p>i=1
</p>
<p>yi = 0
</p>
<p>n
&sum;
</p>
<p>i=1
</p>
<p>&radic;
</p>
<p>1 &minus; y2
i
= L,
</p>
<p>where ci = n &minus; i + 12 , L = 16. This problem is locally convex, since as shown in
Sect. 12.7 the Hessian of the Lagrangian is positive definite. The dual function is
</p>
<p>accordingly
</p>
<p>φ(λ, μ) = min
</p>
<p>n
&sum;
</p>
<p>i=1
</p>
<p>{
</p>
<p>ciyi + λyi + μ
</p>
<p>&radic;
</p>
<p>1 &minus; y2
i
</p>
<p>}
</p>
<p>&minus; Lμ.
</p>
<p>Since the problem is separable, the minimization divides into a separate minimiza-
</p>
<p>tion for each yi, yielding the equations
</p>
<p>ci + λ &minus;
μyi
</p>
<p>&radic;
</p>
<p>1 &minus; y2
i
</p>
<p>= 0
</p>
<p>or
</p>
<p>(ci + λ)
2(1 &minus; y2i ) = &micro;2y2i .
</p>
<p>This yields
</p>
<p>yi =
&minus;(ci + λ)
</p>
<p>[(ci + λ)2 + μ2]1/2
. (14.30)
</p>
<p>The above represents a local minimum point provided μ &lt; 0; and the minus sign
</p>
<p>must be taken for consistency.
</p>
<p>The dual function is then
</p>
<p>φ(λ, μ) =
</p>
<p>n
&sum;
</p>
<p>i=1
</p>
<p>⎧
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>&minus;(ci + λ)2
[(ci + λ)2 + μ2]1/2
</p>
<p>+ μ
</p>
<p>[
</p>
<p>μ2
</p>
<p>[(ci + λ)2 + μ2]
</p>
<p>]1/2
⎫
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎬
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎭
</p>
<p>&minus; Lμ
</p>
<p>or finally, using
&radic;
</p>
<p>μ2 = &minus;μ for μ &lt; 0,
</p>
<p>φ(λ, μ) = &minus;Lμ &minus;
n
&sum;
</p>
<p>i=1
</p>
<p>&radic;
</p>
<p>(ci + λ)2 + μ2.
</p>
<p>The correct values of λ and μ can be found by maximizing φ(λ, μ). One way to do
</p>
<p>this is to use steepest ascent. The results of this calculation, starting at λ = μ = 0,
</p>
<p>are shown in Table 14.1. The values of yi can then be found from (14.30).</p>
<p/>
</div>
<div class="page"><p/>
<p>14.5 Augmented Lagrangian 445
</p>
<p>Table 14.1 Results of dual of chain problem
</p>
<p>Final solution
λ = &minus;10.00048
</p>
<p>Iteration Value μ = &minus;6.761136
0 &minus;200.00000 y1 = &minus;0.8147154
1 &minus;66.94638 y2 = &minus;0.7825940
2 &minus;66.61959 y3 = &minus;0.7427243
3 &minus;66.55867 y4 = &minus;0.6930215
4 &minus;66.54845 y5 = &minus;0.6310140
5 &minus;66.54683 y6 = &minus;0.5540263
6 &minus;66.54658 y7 = &minus;0.4596696
7 &minus;66.54654 y8 = &minus;0.3467526
8 &minus;66.54653 y9 = &minus;0.2165239
9 &minus;66.54653 y10 = &minus;0.0736802
</p>
<p>14.5 Augmented Lagrangian
</p>
<p>One of the most effective general classes of nonlinear programming methods is
</p>
<p>the augmented Lagrangian methods, alternatively referred to as methods of mul-
</p>
<p>tiplier. These methods can be viewed as a combination of penalty functions and
</p>
<p>local duality methods; the two concepts work together to eliminate many of the dis-
</p>
<p>advantages associated with either method alone. The augmented Lagrangian for the
</p>
<p>equality constrained problem
</p>
<p>minimize f (x)
</p>
<p>subject to h(x) = 0, x &isin; Ω (14.31)
</p>
<p>is the function
lc(x, λ) = f (x) + λ
</p>
<p>Th(x) +
1
</p>
<p>2
c|h(x)|2
</p>
<p>for some positive constant c. We shall briefly indicate how the augmented Lag-
</p>
<p>rangian can be viewed as either a special penalty function or as the basis for a dual
</p>
<p>problem. These two viewpoints are then explored further in this and the next section.
</p>
<p>From a penalty function viewpoint the augmented Lagrangian, for a fixed value
</p>
<p>of the vector λ, is simply the standard quadratic penalty function for the problem
</p>
<p>minimize f (x) + λTh(x)
</p>
<p>subject to h(x) = 0, x &isin; Ω (14.32)
</p>
<p>This problem is clearly equivalent to the original problem (14.31), since combina-
</p>
<p>tions of the constraints adjoined to f (x) do not affect the minimum point or the
</p>
<p>minimum value.
</p>
<p>A typical step of an augmented Lagrangian method starts with a vector λk. Then
</p>
<p>x(λk) is found as the minimum point of
</p>
<p>minimize f (x) + λTk h(x) +
1
</p>
<p>2
c|h(x)|2 subject to x &isin; Ω. (14.33)</p>
<p/>
</div>
<div class="page"><p/>
<p>446 14 Duality and Dual Methods
</p>
<p>Next λk is updated to λk+1. A standard method for the update is
</p>
<p>λk+1 = λk + ch(x(λk)).
</p>
<p>To motivate the adjustment procedure, consider Ω = En and the constrained prob-
</p>
<p>lem (14.32) with λ = λk. The Lagrange multiplier corresponding to this prob-
</p>
<p>lem is λ&lowast; &minus; λk, where λ&lowast; is the Lagrange multiplier of (14.31). On the other hand
since (14.33) is the penalty function corresponding to (14.32), it follows from the
</p>
<p>results of Sect. 13.3 that ch(x(λk)) is approximately equal to the Lagrange multiplier
</p>
<p>of (14.32). Combining these two facts, we obtain ch(x(λk)) ≃ λ&lowast; &minus; λk. Therefore, a
good approximation to the unknown λ&lowast; is λk+1 = λk + ch(x(λk)).
</p>
<p>Although the main iteration in augmented Lagrangian methods is with respect to
</p>
<p>λ, the penalty parameter c may also be adjusted during the process. As in ordinary
</p>
<p>penalty function methods, the sequence of c&rsquo;s is usually preselected; c is either held
</p>
<p>fixed, is increased toward a finite value, or tends (slowly) toward infinity. Since in
</p>
<p>this method it is not necessary for c to go to infinity, and in fact it may remain
</p>
<p>of relatively modest value, the ill-conditioning usually associated with the penalty
</p>
<p>function approach is mediated.
</p>
<p>From the viewpoint of duality theory, the augmented Lagrangian is simply the
</p>
<p>standard Lagrangian for the problem
</p>
<p>minimize f (x) +
1
</p>
<p>2
c|h(x)|2
</p>
<p>subject to h(x) = 0, x &isin; Ω. (14.34)
</p>
<p>This problem is equivalent to the original problem (14.31), since the addition of
</p>
<p>the term 1
2
c|h(x)|2 to the objective does not change the optimal value, the opti-
</p>
<p>mum solution point, nor the Lagrange multiplier. However, whereas the original
</p>
<p>Lagrangian may not be convex near the solution, and hence the standard duality
</p>
<p>method cannot be applied, the term 1
2
c|h(x)|2 tends to &ldquo;convexify&rdquo; the Lagrangian.
</p>
<p>For sufficiently large c, the Lagrangian will indeed be locally convex. Thus the
</p>
<p>duality method can be employed, and the corresponding dual problem can be solved
</p>
<p>by an iterative process in λ. This viewpoint leads to the development of additional
</p>
<p>multiplier adjustment processes.
</p>
<p>The Penalty Viewpoint
</p>
<p>We begin our more detailed analysis of augmented Lagrangian methods by showing
</p>
<p>that if the penalty parameter c is sufficiently large, the augmented Lagrangian has a
</p>
<p>local minimum point near the true optimal point. This follows from the following
</p>
<p>simple lemma. (Again, we consider Ω = En for simplicity.)
</p>
<p>Lemma. Let A and B be n&times; n symmetric matrices. Suppose that B is positive semi-definite
and that A is positive definite on the subspace Bx = 0. Then there is a c&lowast; such that for all
c &ge; c&lowast;the matrix A + cB is positive definite.</p>
<p/>
</div>
<div class="page"><p/>
<p>14.5 Augmented Lagrangian 447
</p>
<p>Proof. Suppose to the contrary that for every k there were an xk with |xk | = 1 such
that xT
</p>
<p>k
(A + kB)xk &le; 0. The sequence {xk} must have a convergent subsequence
</p>
<p>converging to a limit x. Now since xT
k
</p>
<p>Bxk &ge; 0, it follows that xTBx = 0. It also
follows that x
</p>
<p>T
Ax &le; 0. However, this contradicts the hypothesis of the lemma. �
</p>
<p>This lemma applies directly to the Hessian of the augmented Lagrangian evalu-
</p>
<p>ated at the optimal solution pair x&lowast;, λ&lowast;. We assume as usual that the second-order
sufficiency conditions for a constrained minimum hold at x&lowast;, λ&lowast;. The Hessian of the
augmented Lagrangian evaluated at the optimal pair x&lowast;, λ&lowast; is
</p>
<p>Lc(x
&lowast;, λ&lowast;) = F(x&lowast;) + (λ&lowast;)TH(x&lowast;) + c&nabla;h(x&lowast;)T&nabla;h(x&lowast;)
</p>
<p>= L(x&lowast;) + c&nabla;h(x&lowast;)T&nabla;h(x&lowast;).
</p>
<p>The first term, the Hessian of the normal Lagrangian, is positive definite on the sub-
</p>
<p>space &nabla;h(x&lowast;)x = 0. This corresponds to the matrix A in the lemma. The matrix
&nabla;h(x&lowast;)T&nabla;h(x&lowast;) is positive semi-definite and corresponds to B in the lemma. It
follows that there is a c&lowast; such that for all c &gt; c&lowast;, Lc(x&lowast;, λ
</p>
<p>&lowast;) is positive definite.
This leads directly to the first basic result concerning augmented Lagrangian.
</p>
<p>Proposition 1. Assume that the second-order sufficiency conditions for a local minimum
</p>
<p>are satisfied at x&lowast;, λ&lowast;. Then there is a c&lowast; such that for all c &ge; c&lowast;, the augmented Lagrangian
lc(x, λ
</p>
<p>&lowast;) has a local minimum point at x&lowast;.
</p>
<p>By a continuity argument the result of the above proposition can be extended to
</p>
<p>a neighborhood around x&lowast;, λ&lowast;. That is, for any λ near λ&lowast;, the augmented Lagrangian
has a unique local minimum point near x&lowast;. This correspondence defines a continuous
function. If a value of λ can be found such that h(x(λ)) = 0, then that λ must in
</p>
<p>fact be λ&lowast;, since x(λ) satisfies the necessary conditions of the original problem.
Therefore, the problem of determining the proper value of λ can be viewed as one
</p>
<p>of solving the equation h(x(λ)) = 0. For this purpose the iterative process
</p>
<p>λk+1 = λk + ch(x(λk)),
</p>
<p>is a method of successive approximation. This process will converge linearly in a
</p>
<p>neighborhood around λ&lowast;, although a rigorous proof is somewhat complex. We shall
give more definite convergence results when we consider the duality viewpoint.
</p>
<p>Example 1. Consider the simple quadratic problem studied in Sect. 13.8
</p>
<p>minimize 2x2 + 2xy + y2 &minus; 2y
subject to x = 0.
</p>
<p>The augmented Lagrangian for this problem is
</p>
<p>lc(x, y, λ) = 2x
2 + 2xy + y2 &minus; 2y + λx + 1
</p>
<p>2
cx2.</p>
<p/>
</div>
<div class="page"><p/>
<p>448 14 Duality and Dual Methods
</p>
<p>The minimum of this can be found analytically to be x = &minus;(2 + λ)/(2 + c), y =
(4 + c + λ)/(2 + c). Since h(x, y) = x in this example, it follows that the iterative
</p>
<p>process for λk is
</p>
<p>λk+1 = λk &minus;
c(2 + λk)
</p>
<p>2 + c
or
</p>
<p>λk+1 =
</p>
<p>(
</p>
<p>2
</p>
<p>2 + c
</p>
<p>)
</p>
<p>λk &minus;
2c
</p>
<p>2 + c
.
</p>
<p>This converges to λ = &minus;2 for any c &gt; 0. The coefficient 2/(2 + c) governs the rate
of convergence, and clearly, as c is increased the rate improves.
</p>
<p>Geometric Interpretation
</p>
<p>The augmented Lagrangian method can be interpreted geometrically in terms of the
</p>
<p>primal function in a manner analogous to that in Sects. 13.3 and 13.8 for the ordinary
</p>
<p>quadratic penalty function and the absolute-value penalty function. Consider again
</p>
<p>the primal function ω(y) defined as
</p>
<p>ω(y) = min{ f (x) : h(x) = y},
</p>
<p>where the minimum is understood to be taken locally near x&lowast;. We remind the
reader that ω(0) = f (x&lowast;) and that &nabla;ω(0)T = &minus;λ&lowast;. The minimum of the augmented
Lagrangian at step k can be expressed in terms of the primal function as follows:
</p>
<p>min lc(x, λk) = min
x
{ f (x) + λTk h(x) +
</p>
<p>1
</p>
<p>2
c|h(x)|2}
</p>
<p>= min
x,u
</p>
<p>{ f (x) + λTk y +
1
</p>
<p>2
c|y|2 : h(x) = y} (14.35)
</p>
<p>= min
u
{ω(y) + λTk y +
</p>
<p>1
</p>
<p>2
c|y|2},
</p>
<p>where the minimization with respect to y is to be taken locally near y = 0. This min-
</p>
<p>imization is illustrated geometrically for the case of a single constraint in Fig. 14.5.
</p>
<p>The lower curve represents ω(y), and the upper curve represents ω(y) + 1
2
c|y|2. The
</p>
<p>minimum point yk of (14.30) occurs at the point where this upper curve has slope
</p>
<p>equal to &minus;λk. It is seen that for c sufficiently large this curve will be convex at y = 0.
If λk is close to λ
</p>
<p>&lowast;, it is clear that this minimum point will be close to 0; it will be
exact if λk = λ
</p>
<p>&lowast;.
The process for updating λk is also illustrated in Fig. 14.5. Note that in general, if
</p>
<p>x(λk) minimizes lc(x, λk), then yk = h(x(λk)) is the minimum point of ω(y) + λ
T
k y +
</p>
<p>1
2
c|y|2. At that point we have as before
</p>
<p>&nabla;ω(yk)
T + cyk = &minus;λk</p>
<p/>
</div>
<div class="page"><p/>
<p>14.6 The Method of Multipliers 449
</p>
<p>Fig. 14.5 Primal function and augmented Lagrangian
</p>
<p>or equivalently,
</p>
<p>&nabla;ω(yk)
T = &minus;(λk + cyk) = &minus;(λk + ch(x(λk))).
</p>
<p>It follows that for the next multiplier we have
</p>
<p>λk+1 = λk + ch(x(λk)) = &minus;&nabla;ω(yk)T ,
</p>
<p>as shown in Fig. 14.5 for the one-dimensional case. In the figure the next point yk+1
is the point where ω(y)+ 1
</p>
<p>2
c|y|2 has slope &minus;λk+1, which will yield a positive value of
</p>
<p>yk+1 in this case. It can be seen that if λk is sufficiently close to λ
&lowast;, then λk+1 will be
</p>
<p>even closer, and the iterative process will converge.
</p>
<p>14.6 The Method of Multipliers
</p>
<p>In the augmented Lagrangian method (the method of multipliers), the primary
</p>
<p>iteration is with respect to λ, and therefore it is most natural to consider the method
</p>
<p>from the dual viewpoint. This is in fact the more powerful viewpoint and leads to
</p>
<p>improvements in the algorithm.</p>
<p/>
</div>
<div class="page"><p/>
<p>450 14 Duality and Dual Methods
</p>
<p>As we observed earlier, the constrained problem
</p>
<p>minimize f (x)
</p>
<p>subject to h(x) = 0, x &isin; Ω (14.36)
</p>
<p>is equivalent to the problem
</p>
<p>minimize f (x) +
1
</p>
<p>2
c|h(x)|2
</p>
<p>subject to h(x) = 0, x &isin; Ω (14.37)
</p>
<p>in the sense that the solution points, the optimal values, and the Lagrange multipliers
</p>
<p>are the same for both problems. However, as spelled out by Proposition 1 of the pre-
</p>
<p>vious section, whereas problem (14.36) may not be locally convex, problem (14.37)
</p>
<p>is locally convex for sufficiently large c; specifically, the Hessian of the Lagrangian
</p>
<p>is positive definite at the solution pair x&lowast;, λ&lowast;. Thus local duality theory is applicable
to problem (14.37) for sufficiently large c.
</p>
<p>To apply the dual method to (14.37), we define the dual function
</p>
<p>φ(λ) = min{ f (x) + λTh(x) + 1
2
c|h(x)|2} (14.38)
</p>
<p>in a region near x&lowast;, λ&lowast;. If x(λ) is the vector minimizing the right-hand side
of (14.38), then as we have seen in Sect. 14.2, h(x(λ)) is the gradient of φ. Thus
</p>
<p>the iterative process
</p>
<p>λk+1 = λk + ch(x(λk))
</p>
<p>used in the basic augmented Lagrangian method is seen to be a steepest ascent
</p>
<p>iteration for maximizing the dual function φ. It is a simple form of steepest ascent,
</p>
<p>using a constant stepsize c.
</p>
<p>Although the stepsize c is a good choice (as will become even more evident
</p>
<p>later), it is clearly advantageous to apply the algorithmic principles of optimization
</p>
<p>developed previously by selecting the stepsize so that the new value of the dual
</p>
<p>function satisfies an ascent criterion. This can extend the range of convergence of
</p>
<p>the algorithm.
</p>
<p>The rate of convergence of the optimal steepest ascent method (where the stepsize
</p>
<p>is selected to maximize φ in the gradient direction) is determined by the eigenvalues
</p>
<p>of the Hessian of φ. The Hessian of φ is found from (14.15) to be
</p>
<p>&nabla;h(x(λ))[L(x(λ), λ) + c&nabla;h(x(λ))T&nabla;h(x(λ))]&minus;1&nabla;h(x)T . (14.39)
</p>
<p>The eigenvalues of this matrix at the solution point x&lowast;, λ&lowast; determine the convergence
rate of the method of steepest ascent.
</p>
<p>To analyze the eigenvalues we make use of the matrix identity
</p>
<p>cB(A + cBTB)&minus;1BT = I &minus; (I + cBA&minus;1BT )&minus;1,</p>
<p/>
</div>
<div class="page"><p/>
<p>14.6 The Method of Multipliers 451
</p>
<p>which is a generalization of the Sherman-Morrison formula. (See Sect. 10.4.) It
</p>
<p>is easily seen from the above identity that the matrices B(A + cBTB)&minus;1BT and
(BA&minus;1BT ) have identical eigenvectors. One way to see this is to multiply both sides
of the identity by (I + cBA&minus;1BT ) on the right to obtain
</p>
<p>cB(A + cBTB)&minus;1BT (I + cBA&minus;1BT ) = cBA&minus;1BT .
</p>
<p>Suppose both sides are applied to an eigenvector e of BA&minus;1BT having eigenvalue w.
Then we obtain
</p>
<p>cB(A + cBTB)&minus;1BT (1 + cw)e = cwe.
</p>
<p>It follows that e is also an eigenvector of B(A + cBTB)&minus;1BT , and if v is the corre-
sponding eigenvalue, the relation
</p>
<p>cu(1 + cw) = cw
</p>
<p>must hold. Therefore, the eigenvalues are related by
</p>
<p>u =
w
</p>
<p>1 + cw
. (14.40)
</p>
<p>The above relations apply directly to the Hessian (14.39) through the associations
</p>
<p>A = L(x&lowast;, λ&lowast;) and B = &nabla;h(x&lowast;). Note that the matrix &nabla;h(x&lowast;)L(x&lowast;, λ&lowast;)&minus;1&nabla;h(x&lowast;)T ,
corresponding to BA&minus;1BT above, is the Hessian of the dual function of the original
problem (14.36). As shown in Sect. 14.3 the eigenvalues of this matrix determine the
</p>
<p>rate of convergence for the ordinary dual method. Let w and W be the smallest and
</p>
<p>largest eigenvalues of this matrix. From (14.40) it follows that the ratio of smallest
</p>
<p>to largest eigenvalues of the Hessian of the dual for the augmented problem is
</p>
<p>1
w
+ c
</p>
<p>1
w
+ c
</p>
<p>.
</p>
<p>This shows explicitly how the rate of convergence of the multiplier method depends
</p>
<p>on c. As c goes to infinity, the ratio of eigenvalues goes to unity, implying arbitrarily
</p>
<p>fast convergence.
</p>
<p>Other unconstrained optimization techniques may be applied to the maximiza-
</p>
<p>tion of the dual function defined by the augmented Lagrangian; conjugate gradient
</p>
<p>methods, Newton&rsquo;s method, and quasi-Newton methods can all be used. The use
</p>
<p>of Newton&rsquo;s method requires evaluation of the Hessian matrix (14.39). For some
</p>
<p>problems this may be feasible, but for others some sort of approximation is desir-
</p>
<p>able. One approximation is obtained by noting that for large values of c, the Hes-
</p>
<p>sian (14.39) is approximately equal to (1/c)I. Using this value for the Hessian and
</p>
<p>h(x(λ)) for the gradient, we are led to the iterative scheme
</p>
<p>λk+1 = λk + ch(x(λk)),
</p>
<p>which is exactly the simple method of multipliers originally proposed.</p>
<p/>
</div>
<div class="page"><p/>
<p>452 14 Duality and Dual Methods
</p>
<p>We might summarize the above observations by the following statement relating
</p>
<p>primal and dual convergence rates. If a penalty term is incorporated into a problem,
</p>
<p>the condition number of the primal problem becomes increasingly poor as c &rarr; &infin;
but the condition number of the dual becomes increasingly good. To apply the dual
</p>
<p>method, however, an unconstrained penalty problem of poor condition number must
</p>
<p>be solved at each step.
</p>
<p>Inequality Constraints
</p>
<p>The advantage of augmented Lagrangian methods is mostly in dealing with equal-
</p>
<p>ities. But certain inequality constraints can be easily incorporated. Let us consider
</p>
<p>the problem with p inequality constraints:
</p>
<p>minimize f (x)
</p>
<p>subject to g(x) &le; 0. (14.41)
</p>
<p>We assume that this problem has a well-defined solution x&lowast;, which is a regular
point of the constraints and which satisfies the second-order sufficiency conditions
</p>
<p>for a local minimum as specified in Sect. 11.8. This problem can be written as an
</p>
<p>equivalent problem with equality constraints:
</p>
<p>minimize f (x)
</p>
<p>subject to g(x) + u = 0, u &ge; 0. (14.42)
</p>
<p>Through this conversion we can hope to simply apply the theory for equality con-
</p>
<p>straints to problems with inequalities.
</p>
<p>In order to do so we must insure that (14.42) satisfies the second-order suffi-
</p>
<p>ciency conditions of Sect. 11.5. These conditions will not hold unless we impose a
</p>
<p>strict complementarity assumption that g j(x
&lowast;) = 0 implies μ&lowast;
</p>
<p>j
&gt; 0 as well as the
</p>
<p>usual second-order sufficiency conditions for the original problem (14.41). (See Ex-
</p>
<p>ercise 10.)
</p>
<p>With these assumptions we define the (partial) dual function corresponding to the
</p>
<p>augmented Lagrangian method as
</p>
<p>φ(&micro;) = min
u&ge;0, x
</p>
<p>f (x) + &micro;T [g(x) + u] +
1
</p>
<p>2
c|g(x) + u|2. (14.43)
</p>
<p>The minimization with respect to u in (14.43) can be carried out analytically, and
</p>
<p>this will lead to a definition of the dual function that only involves minimization with
</p>
<p>respect to x. The variable u j enters the objective of the dual function only through
</p>
<p>the univariate quadratic expression
</p>
<p>P j = μ j[g j(x) + u j] +
1
</p>
<p>2
c[g j(x) + u j]
</p>
<p>2. (14.44)</p>
<p/>
</div>
<div class="page"><p/>
<p>14.6 The Method of Multipliers 453
</p>
<p>It is this expression that we must minimize with respect to u j &ge; 0. This is easily
accomplished by differentiation: If u j &gt; 0, the derivative must vanish; if u j = 0, the
</p>
<p>derivative must be nonnegative. The derivative is zero at z j = &minus;g j(x) &minus; &micro; j/c. Thus
we obtain the solution
</p>
<p>u j =
</p>
<p>{
</p>
<p>&minus;g j(x) &minus;
&micro; j
</p>
<p>c
, if &minus; g j(x) &minus; μ jc &ge; 0
</p>
<p>0, otherwise
</p>
<p>or equivalently,
</p>
<p>u j = max
{
</p>
<p>0, &minus;g j(x) &minus;
μ j
</p>
<p>c
</p>
<p>}
</p>
<p>. (14.45)
</p>
<p>We now substitute this into (14.44) in order to obtain an explicit expression for the
</p>
<p>minimum of P j.
</p>
<p>For u j = 0, we have
</p>
<p>P j =
1
</p>
<p>2c
</p>
<p>(
</p>
<p>2μ jcg j(x) + c
2g j(x)
</p>
<p>2
)
</p>
<p>=
1
</p>
<p>2c
</p>
<p>(
</p>
<p>[μ j + cg j(x)]
2 &minus; μ2j
</p>
<p>)
</p>
<p>.
</p>
<p>For u j = &minus;g j(x) &minus; μ j/c we have
</p>
<p>P j = &minus;μ2j/2c.
</p>
<p>These can be combined into the formula
</p>
<p>P j =
1
</p>
<p>2c
</p>
<p>(
</p>
<p>[max{0, μ j + cg j(x)}]2 &minus; μ2j
)
</p>
<p>.
</p>
<p>In view of the above, let us define the function of two scalar arguments t and &micro;:
</p>
<p>Pc(t, μ) =
1
</p>
<p>2c
</p>
<p>(
</p>
<p>[max{0, &micro; + ct}]2 &minus; μ2
)
</p>
<p>. (14.46)
</p>
<p>For a fixed μ &gt; 0, this function is shown in Fig. 14.6. Note that it is a smooth
</p>
<p>function with derivative with respect to t equal to μ at t = 0.
</p>
<p>The dual function for the inequality problem can now be written as
</p>
<p>φ(μ) = min
x
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>f (x) +
</p>
<p>p
&sum;
</p>
<p>j=1
</p>
<p>Pc(g j(x), &micro; j)
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>. (14.47)
</p>
<p>Thus inequality problems can be treated by adjoining to f (x) a special penalty func-
</p>
<p>tion (that depends on &micro;). The Lagrange multiplier &micro; can then be adjusted to maxi-
</p>
<p>mize φ, just as in the case of equality constraints.</p>
<p/>
</div>
<div class="page"><p/>
<p>454 14 Duality and Dual Methods
</p>
<p>Fig. 14.6 Penalty function for inequality problem
</p>
<p>14.7 The Alternating Direction Method of Multipliers
</p>
<p>Consider the convex minimization model with linear constraints and an objective
</p>
<p>function which is the sum of two separable functions:
</p>
<p>minimize f1(x
1) + f2(x
</p>
<p>2)
</p>
<p>subject to A1x
1 + A2x
</p>
<p>2 = b,
</p>
<p>x1 &isin; Ω1, x2 &isin; Ω2,
(14.48)
</p>
<p>where Ai &isin; Em&times;ni (i = 1, 2), b &isin; Em, Ωi &sub; Eni (i = 1, 2) are closed convex sets;
and fi : E
</p>
<p>ni &rarr; E (i = 1, 2) are convex functions on Ωi, respectively. Then, the
augmented Lagrangian function for (14.48) would be
</p>
<p>lc(x
1, x2, λ) = f1(x
</p>
<p>1) + f2(x
2) + λT (A1x
</p>
<p>1 + A2x
2 &minus; b) + c
</p>
<p>2
|A1x1 + A2x2 &minus; b|2.
</p>
<p>Throughout this section, we assume problem (14.48) has at least one optimal solu-
</p>
<p>tion.
</p>
<p>In contrast to the method of multipliers in the last section, the alternating direc-
</p>
<p>tion method of multipliers (ADMM) is to (approximately) minimize lc(x
1, x2, λ) in
</p>
<p>an alternative order:
</p>
<p>x1
k+1
</p>
<p>: = arg minx1&isin;Ω1 lc(x
1 , x2
</p>
<p>k
, λk),
</p>
<p>x2
k+1
</p>
<p>: = arg minx2&isin;Ω2 lc(x
1
k+1
</p>
<p>, x2, λk),
</p>
<p>λk+1 : = λk + c(A1x
1
k+1
</p>
<p>+ A2x
2
k+1
</p>
<p>&minus; b).
(14.49)
</p>
<p>The idea is that each of the smaller minimization problems can be solved more
</p>
<p>efficiently or even in close forms for certain cases.</p>
<p/>
</div>
<div class="page"><p/>
<p>14.7 The Alternating Direction Method of Multipliers 455
</p>
<p>Convergence Speed Analysis
</p>
<p>We present a convergence speed analysis of the ADMM. For simplicity, we shall let
</p>
<p>Ωi be E
ni and fi be differentiable convex functions [the result is also valid for the
</p>
<p>ADMM applied to the aforementioned more general problem (14.48)]. Then, any
</p>
<p>optimal solution and multiplier (x1&lowast;, x
2
&lowast;, λ&lowast;) satisfy
</p>
<p>&nabla; f1(x1&lowast;)T + AT1 λ&lowast; = 0, &nabla; f2(x2&lowast;)T + AT2 λ&lowast; = 0, A1x1&lowast; + A2x2&lowast; &minus; b = 0, (14.50)
</p>
<p>and these conditions are also sufficient.
</p>
<p>We first establish a key lemma.
</p>
<p>Lemma 1. Let di
k
= Ai(x
</p>
<p>i
k
&minus; xi&lowast;), i = 1, 2, and dλk = λk &minus; λ&lowast;; and {x1k , x2k , λk} be the sequence
</p>
<p>generated by ADMM (14.49). Then, it holds that
</p>
<p>c
∣
</p>
<p>∣
</p>
<p>∣A2(x
2
k+1 &minus; x2k )
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>2
+
</p>
<p>1
</p>
<p>c
|λk+1 &minus; λk |2 &le;
</p>
<p>(
</p>
<p>c
∣
</p>
<p>∣
</p>
<p>∣A2d
2
k
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>2
+
</p>
<p>1
</p>
<p>c
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣dλk
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>2
)
</p>
<p>&minus;
(
</p>
<p>c
∣
</p>
<p>∣
</p>
<p>∣A2d
2
k+1
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>2
+
</p>
<p>1
</p>
<p>c
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣dλk+1
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>2
)
</p>
<p>.
</p>
<p>Proof. From the first-order optimality conditions of (14.49), we have
</p>
<p>⎧
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>&nabla; f1(x1k+1)T + AT1 [λk + c(A1x1k+1 + A2x2k &minus; b)] = 0,
&nabla; f2(x2k+1)T + AT2 [λk + c(A1x1k+1 + A2x2k+1 &minus; b)] = 0,
λk+1 = λk + c(A1x
</p>
<p>1
k+1 + A2x
</p>
<p>2
k+1 &minus; b).
</p>
<p>(14.51)
</p>
<p>Substituting the last equation into other equations in (14.51), we obtain
</p>
<p>⎧
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>&nabla; f1(x1k+1)T + AT1 λk+1 = &minus;cAT1 A2(x2k &minus; x2k+1),
&nabla; f2(x2k+1)T + AT2 λk+1 = 0,
A1x
</p>
<p>1
k+1
</p>
<p>+ A2x
2
k+1
</p>
<p>&minus; b = 1
c
(λk+1 &minus; λk).
</p>
<p>(14.52)
</p>
<p>Moreover, the convexity of fi, i = 1, 2, implies
</p>
<p>(&nabla; f1(x1k+1) &minus; &nabla; f1(x1&lowast;))(x1k+1 &minus; x1&lowast;) &ge; 0 and (&nabla; f2(x2k+1) &minus; &nabla; f2(x2&lowast;))(x2k+1 &minus; x2&lowast;) &ge; 0.
</p>
<p>On the other hand, from (14.50) and (14.52),
</p>
<p>&nabla; f1(x1k+1)T &minus; &nabla; f1(x1&lowast;)T = &nabla; f1(x1k+1) + AT1 λ&lowast; = &minus;AT1 dλk+1 &minus; cAT1 A2(x2k &minus; x2k+1)
</p>
<p>&nabla; f2(x2k+1)T &minus; &nabla; f2(x2&lowast;)T = &nabla; f2(x2k+1) + AT2 λ&lowast; = &minus;AT2 dλk+1
and
</p>
<p>0 = A1x
1
k+1 + A2x
</p>
<p>2
k+1 &minus; b &minus;
</p>
<p>1
</p>
<p>c
(λk+1 &minus; λk) = A1d1k+1 + A2dk+12 +
</p>
<p>1
</p>
<p>c
(λk &minus; λk+1).</p>
<p/>
</div>
<div class="page"><p/>
<p>456 14 Duality and Dual Methods
</p>
<p>Thus,
</p>
<p>0 &le;
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>d1
k+1
</p>
<p>dk+1
2
</p>
<p>dλ
k+1
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>T ⎛
⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>&nabla; f1(x1k+1)T &minus; &nabla; f1(x1&lowast;)T
&nabla; f2(x2k+1)T &minus; &nabla; f2(x2&lowast;)T
</p>
<p>0
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>=
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>d1
k+1
</p>
<p>dk+1
2
</p>
<p>dλ
k+1
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>T ⎛
⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>&minus;AT1 dλk+1 &minus; cAT1 A2(x2k &minus; x2k+1)
&minus;AT
</p>
<p>2
dλ
k+1
</p>
<p>A1d
1
k+1 + A2d
</p>
<p>k+1
2
</p>
<p>+ 1
c
(λk &minus; λk+1)
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>=
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>d1
k+1
</p>
<p>dk+12
dλ
k+1
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>T ⎛
⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>&minus;AT1 dλk+1
&minus;AT2 dλk+1
</p>
<p>A1d
1
k+1
</p>
<p>+ A2d
k+1
2
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>+
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>&minus;cAT1 A2(x2k &minus; x2k+1)
0
</p>
<p>1
c
(λk &minus; λk+1)
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>=
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>d1
k+1
</p>
<p>dk+1
2
</p>
<p>dλ
k+1
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>T ⎛
⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>&minus;cAT
1
A2(x
</p>
<p>2
k
&minus; x2
</p>
<p>k+1
)
</p>
<p>0
1
c
(λk &minus; λk+1)
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>=
</p>
<p>(
</p>
<p>&minus;A1d1k+1
dλ
k+1
</p>
<p>)T (
</p>
<p>cA2(x
2
k
&minus; x2
</p>
<p>k+1
)
</p>
<p>1
c
(λk &minus; λk+1)
</p>
<p>)
</p>
<p>(14.53)
</p>
<p>Again from &minus;A1d1k+1 = 1c (λk+1 &minus; λk) + A2d2k+1, inequality (14.53) implies
</p>
<p>0 &le;
(
</p>
<p>1
c
(λk+1 &minus; λk) + A2d2k+1
</p>
<p>dλ
k+1
</p>
<p>)T (
</p>
<p>cA2(x
2
k
&minus; x2
</p>
<p>k+1)
1
c
(λk &minus; λk+1)
</p>
<p>)
</p>
<p>=
</p>
<p>(
</p>
<p>A2d
2
k+1
</p>
<p>dλ
k+1
</p>
<p>)T (
</p>
<p>cA2(x
2
k
&minus; x2
</p>
<p>k+1)
1
c
(λk &minus; λk+1)
</p>
<p>)
</p>
<p>+ (λk &minus; λk+1)TA2(x2k &minus; x2k+1)
</p>
<p>Since &nabla; f2(x2k) = &minus;λ
T
k A2 holds for every k &ge; 0, it follows from the convexity of f2
</p>
<p>that
</p>
<p>(λk &minus; λk+1)TA2(x2k &minus; x2k+1) = &minus;(&nabla; f2(x2k) &minus; &nabla; f2(x2k+1))(x2k &minus; x2k+1) &le; 0.
Thus,
(
</p>
<p>A2d
2
k+1
</p>
<p>dλ
k+1
</p>
<p>)T (
</p>
<p>cA2(x
2
k
&minus; x2
</p>
<p>k+1
)
</p>
<p>1
c
(λk &minus; λk+1)
</p>
<p>)
</p>
<p>&ge; 0 or
( &radic;
</p>
<p>cA2d
2
k+1
</p>
<p>1&radic;
c
dλ
k+1
</p>
<p>)T ( &radic;
cA2(x
</p>
<p>2
k+1
</p>
<p>&minus; x2
k
)
</p>
<p>1&radic;
c
(λk+1 &minus; λk)
</p>
<p>)
</p>
<p>&le; 0.
</p>
<p>Representing the left vector by u and the right one by v in the last inequality, we
</p>
<p>have
</p>
<p>0 &ge; uTv = 1
2
</p>
<p>(|u|2 + |v|2 &minus; |u &minus; v|2).
</p>
<p>Noting
</p>
<p>u &minus; v =
( &radic;
</p>
<p>cA2d
2
k+1
</p>
<p>1&radic;
c
dλ
k+1
</p>
<p>)
</p>
<p>&minus;
( &radic;
</p>
<p>cA2(x
2
k+1
</p>
<p>&minus; x2
k
)
</p>
<p>1&radic;
c
(λk+1 &minus; λk)
</p>
<p>)
</p>
<p>=
</p>
<p>( &radic;
cA2d
</p>
<p>2
k
</p>
<p>1&radic;
c
dλ
k
</p>
<p>)
</p>
<p>,
</p>
<p>we obtain the desired result in Lemma 1. �
</p>
<p>For simplicity, let c = 1 in the following. Taking the sum from iterate 0 to iterate
</p>
<p>k for the inequality in Lemma 1, we obtain
</p>
<p>k
&sum;
</p>
<p>t=0
</p>
<p>(
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣A2(x
2
t+1 &minus; x2t )
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>2
+ |λt+1 &minus; λt |2
</p>
<p>)
</p>
<p>&le;
∣
</p>
<p>∣
</p>
<p>∣A2x
2
0 &minus; A2x2&lowast;
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>2
+ |λ0 &minus; λ&lowast;|2 .</p>
<p/>
</div>
<div class="page"><p/>
<p>14.7 The Alternating Direction Method of Multipliers 457
</p>
<p>Thus, we have
</p>
<p>min
0&le;t&le;k
</p>
<p>{
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣A2(x
2
t+1 &minus; x2t )
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>2
+ |λt+1 &minus; λt|2
</p>
<p>}
</p>
<p>&le; 1
k
</p>
<p>(
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣A2(x
2
0 &minus; x2&lowast;)
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>2
+ |λ0 &minus; λ&lowast;|2
</p>
<p>)
</p>
<p>.
</p>
<p>Therefore, from (14.52) we have
</p>
<p>Theorem 1. After k iterations of the ADMM method, there must be at least one iterate
</p>
<p>0 &le; k̄ &le; k such that
∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>&nabla; f1(x1k̄+1)
T + AT
</p>
<p>1
λk̄+1
</p>
<p>&nabla; f2(x2k̄+1)
T + AT
</p>
<p>2
λk̄+1
</p>
<p>A1x
1
k̄+1
</p>
<p>+ A2x
2
k̄+1
</p>
<p>&minus; b
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>2
</p>
<p>&le; 1 + |A1 |
k
</p>
<p>(
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣A2(x
2
0 &minus; x2&lowast;)|2 + |λ0 &minus; λ&lowast;
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>2
)
</p>
<p>,
</p>
<p>that is, (x1
k̄+1
</p>
<p>, x2
k̄+1
</p>
<p>, λk̄+1) has its optimality condition error square bounded by the quantity
on the right-hand side that converges 0 arithmetically as k &rarr; &infin;.
</p>
<p>The Three Block Extension
</p>
<p>It is natural to consider the ADMM method for solving problems with more than
</p>
<p>two blocks:
</p>
<p>minimize f1(x
1) + f2(x
</p>
<p>2) + f3(x
3)
</p>
<p>subject to A1x
1 + A2x
</p>
<p>2 + A3x
3 = b,
</p>
<p>x1 &isin; Ω1, x2 &isin; Ω2, x3 &isin; Ω3,
(14.54)
</p>
<p>where Ai &isin; Em&times;ni (i = 1, 2, 3), b &isin; Em, Ωi &sub; Eni (i = 1, 2, 3) are closed convex sets;
and fi : E
</p>
<p>ni &rarr; E (i = 1, 2, 3) are convex functions on Ωi, respectively. With the
same philosophy as the ADMM to take advantage of the separable structure, one
</p>
<p>could consider the procedure
</p>
<p>x1k+1 : = arg min
x1&isin;Ω1
</p>
<p>lc(x
1 , x2k , x
</p>
<p>3
k , λk),
</p>
<p>x2k+1 : = arg min
x2&isin;Ω2
</p>
<p>lc(x
1
k+1 , x
</p>
<p>2, x3k , λk), (14.55)
</p>
<p>x3k+1 : = arg min
x3&isin;Ω3
</p>
<p>lc(x
1
k+1 , x
</p>
<p>2
k+1, x
</p>
<p>3, λk),
</p>
<p>λk+1 : = λk + c(A1x
1
k+1 + A2x
</p>
<p>2
k+1 + A3x
</p>
<p>3
k+1 &minus; b),
</p>
<p>where the augmented Lagrangian function
</p>
<p>lc(x
1 , x2, x3, λ) =
</p>
<p>3
&sum;
</p>
<p>i=1
</p>
<p>fi(x
i) + λT
</p>
<p>(
</p>
<p>3
&sum;
</p>
<p>i=1
</p>
<p>Aix
i &minus; b) + c
</p>
<p>2
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>3
&sum;
</p>
<p>i=1
</p>
<p>Aix
i &minus; b
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>2
.
</p>
<p>Unfortunately, unlike the convergence property for solving two-block problems,
</p>
<p>such a direct extension of ADMM not converge for problems with three blocks.
</p>
<p>Indeed, consider the following linear homogeneous equation with three variables</p>
<p/>
</div>
<div class="page"><p/>
<p>458 14 Duality and Dual Methods
</p>
<p>(A1, A2, A3)
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>x1
</p>
<p>x2
</p>
<p>x3
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>=
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>1 1 1
</p>
<p>1 1 2
</p>
<p>1 2 2
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>x1
</p>
<p>x2
</p>
<p>x3
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>= 0.
</p>
<p>Let c = 1 and each block contain one variable. Then, simple calculation will show
</p>
<p>that the direct extension of ADMM (14.55) is divergent from any point in a subspace
</p>
<p>of E3. Note that the convergence of ADMM (14.55) applied to solving the linear
</p>
<p>equations with a null objective is independent of the selection of the penalty par-
</p>
<p>ameter c. We conclude:
</p>
<p>Theorem 2. For the three-block convex minimization problem (14.54), the direct extension
</p>
<p>of ADMM (14.55) may not converge for any penalty parameter c &gt; 0 starting from any
point in a subspace.
</p>
<p>*14.8 &lowast;Cutting Plane Methods
</p>
<p>Cutting plane methods are applied to problems having the general form
</p>
<p>minimize cTx
</p>
<p>subject to x &isin; S , (14.56)
</p>
<p>where S &sub; En is a closed convex set. Problems that involve minimization of a
convex function over a convex set, such as the problem
</p>
<p>minimize f (y)
</p>
<p>subject to y &isin; R, (14.57)
</p>
<p>where R &sub; En&minus;1 is a convex set and f is a convex function, can be easily converted
to the form (14.56) by writing (14.57) equivalently as
</p>
<p>minimize r
</p>
<p>subject to f (y) &minus; r � 0, y &isin; R (14.58)
</p>
<p>which, with x = (r, y) &isin; En, is a special case of (14.56).
</p>
<p>General Form of Algorithm
</p>
<p>The general form of a cutting-plane algorithm for problem (14.56) is as follows:
</p>
<p>Given a polytope Pk &sup; S
Step 1. Minimize cTx over Pk obtaining a point xk in Pk. If xk &isin; S , stop; xk is
</p>
<p>optimal. Otherwise,</p>
<p/>
</div>
<div class="page"><p/>
<p>14.8 &lowast;Cutting Plane Methods 459
</p>
<p>Step 2. Find a hyperplane Hk separating the point xk from S , that is, find ak &isin;
En, bk &isin; E1 such that S &sub; {x : aTk x � bk}, xk &isin; {x : aTk x &gt; bk}. Update Pk to
obtain Pk+1 including as a constraint a
</p>
<p>T
k
</p>
<p>x � bk.
</p>
<p>The process is illustrated in Fig. 14.7.
</p>
<p>Specific algorithms differ mainly in the manner in which the hyperplane that
</p>
<p>separates the current point xk from the constraint set S is selected. This selection is,
</p>
<p>of course, the most important aspect of the algorithm, since it is the deepness of the
</p>
<p>cut associated with the separating hyperplane, the distance of the hyperplane from
</p>
<p>the current point, that governs how much improvement there is in the approximation
</p>
<p>to the constraint set, and hence how fast the method converges.
</p>
<p>Fig. 14.7 Cutting plane method
</p>
<p>Specific algorithms also differ somewhat with respect to the manner by which
</p>
<p>the polytope is updated once the new hyperplane is determined. The most straight-
</p>
<p>forward procedure is to simply adjoin the linear inequality associated with that hyp-
</p>
<p>erplane to the ones determined previously. This yields the best possible updated
</p>
<p>approximation to the constraint set but tends to produce, after a large number of
</p>
<p>iterations, an unwieldy number of inequalities expressing the approximation. Thus,
</p>
<p>in some algorithms, older inequalities that are not binding at the current point are
</p>
<p>discarded from further consideration.
</p>
<p>The general cutting plane algorithm can be regarded as an extended application
</p>
<p>of duality in linear programming, and although this viewpoint does not particularly
</p>
<p>aid in the analysis of the method, it reveals the basic interconnection between cutting
</p>
<p>plane and dual methods. The foundation of this viewpoint is the fact that S can be
</p>
<p>written as the intersection of all the half-spaces that contain it; thus
</p>
<p>S = {x : aTi x &le; bi, i &isin; I},</p>
<p/>
</div>
<div class="page"><p/>
<p>460 14 Duality and Dual Methods
</p>
<p>where I is an (infinite) index set corresponding to all half-spaces containing S . With
</p>
<p>S viewed in this way problem (14.56) can be thought of as an (infinite) linear pro-
</p>
<p>gramming problem.
</p>
<p>Corresponding to this linear program there is (at least formally) the dual problem
</p>
<p>maximize
&sum;
</p>
<p>i&isin;I
λibi
</p>
<p>subject to
&sum;
</p>
<p>i&isin;l
λiai = c (14.59)
</p>
<p>λi � 0, i &isin; I.
</p>
<p>Selecting a finite subset of I, say I, and forming
</p>
<p>P = {x : aTi x � bi, i &isin; I}
gives a polytope that contains S . Minimizing cTx over this polytope yields a point
</p>
<p>and a corresponding subset of active constraints IA. The dual problem with the addi-
</p>
<p>tional restriction λi = 0 for i � IA will then have a feasible solution, but this solution
</p>
<p>will in general not be optimal. Thus, a solution to a polytope problem corresponds
</p>
<p>to a feasible but non-optimal solution to the dual. For this reason the cutting plane
</p>
<p>method can be regarded as working toward optimality of the (infinite dimensional)
</p>
<p>dual.
</p>
<p>Kelley&rsquo;s Convex Cutting Plane Algorithm
</p>
<p>The convex cutting plane method was developed to solve convex programming
</p>
<p>problems of the form
</p>
<p>minimize f (x) (14.60)
</p>
<p>subject to gi(x) � 0, i = 1, 2, . . . , p,
</p>
<p>where x &isin; En and f and the gi&rsquo;s are differentiable convex functions. As indicated in
the last section, it is sufficient to consider the case where the objective function is
</p>
<p>linear; thus, we consider the problem
</p>
<p>minimize cTx (14.61)
</p>
<p>subject to g(x) � 0
</p>
<p>where x &isin; En and g(x) &isin; Ep is convex and differentiable.
For g convex and differentiable we have the fundamental inequality
</p>
<p>g(x) � g(w) + &nabla;g(w)(x &minus; w) (14.62)
</p>
<p>for any x, w. We use this equation to determine the separating hyperplane. Specifi-
</p>
<p>cally, the algorithm is as follows:</p>
<p/>
</div>
<div class="page"><p/>
<p>14.8 &lowast;Cutting Plane Methods 461
</p>
<p>Let S = {x : g(x) � 0} and let P be an initial polytope containing S and such that
cTx is bounded on P. Then
</p>
<p>Step 1. Minimize cTx over P obtaining the point x = w. If g(w) � 0, stop; w is an
</p>
<p>optimal solution. Otherwise,
</p>
<p>Step 2. Let i be an index maximizing gi(w). Clearly gi(w) &gt; 0. Define the new
</p>
<p>approximating polytope to be the old one intersected with the half-space
</p>
<p>{x : gi(w) + &nabla;gi(w)(x &minus; w) � 0}. (14.63)
</p>
<p>Return to Step 1.
</p>
<p>The set defined by (14.63) is actually a half-space if &nabla;gi(w) � 0. However,
</p>
<p>&nabla;gi(w) = 0 would imply that w minimizes gi which is impossible if S is nonempty.
</p>
<p>Furthermore, the half-space given by (14.63) contains S , since if g(x) � 0 then
</p>
<p>by (14.62) gi(w) + &nabla;gi(w)(x &minus; w) � gi(x) � 0. The half-space does not contain
the point w since gi(w) &gt; 0. This method for selecting the separating hyperplane is
</p>
<p>illustrated in Fig. 14.8 for the one-dimensional case. Note that in one dimension, the
</p>
<p>procedure reduces to Newton&rsquo;s method.
</p>
<p>Fig. 14.8 Convex cutting plane
</p>
<p>Calculation of the separating hyperplane is exceedingly simple in this algorithm,
</p>
<p>and hence the method really amounts to the solution of a series of linear program-
</p>
<p>ming problems. It should be noted that this algorithm, valid for any convex program-
</p>
<p>ming problem, does not involve any line searches. In that respect it is also similar to
</p>
<p>Newton&rsquo;s method applied to a convex function.
</p>
<p>Convergence
</p>
<p>Under fairly mild assumptions on the convex function, the convex cutting plane
</p>
<p>method is globally convergent. It is possible to apply the general convergence
</p>
<p>theorem to prove this, but somewhat easier, in this case, to prove it directly.</p>
<p/>
</div>
<div class="page"><p/>
<p>462 14 Duality and Dual Methods
</p>
<p>Theorem. Let the convex functions gi, i = 1, 2, . . . , p be continuously differentiable, and
suppose the convex cutting plane algorithm generates the sequence of points {wk}. Any limit
point of this sequence is a solution to problem (14.61).
</p>
<p>Proof. Suppose {wk}, k &isin; K is a subsequence of {wk} converging to w. By taking
a further subsequence of this, if necessary, we may assume that the index i corre-
</p>
<p>sponding to Step 2 of the algorithm is fixed throughout the subsequence. Now if
</p>
<p>k &isin; K , k&prime; &isin; K and k&prime; &gt; k, then we must have
</p>
<p>gi(wk) + &nabla;gi(wk)(wk&prime; &minus; wk) � 0,
</p>
<p>which implies that
</p>
<p>gi(wk) &le; |&nabla;gi(wk)||wk&prime; &minus; wk |. (14.64)
Since |&nabla;gi(wk)| is bounded with respect to k &isin; K , the right-hand side of (14.64) goes
to zero as k and k&prime; go to infinity. The left-hand side goes to gi(w). Thus gi(w) � 0
and we see that w is feasible for problem (14.61).
</p>
<p>If f &lowast; is the optimal value of problem (14.61), we have cTwk � f &lowast; for each k since
wk is obtained by minimizing over a set containing S . Thus, by continuity, c
</p>
<p>Tw � f &lowast;
</p>
<p>and hence w is an optimal solution. �
</p>
<p>As with most algorithms based on linear programming concepts, the rate of con-
</p>
<p>vergence of cutting plane algorithms has not yet been satisfactorily analyzed. Pre-
</p>
<p>liminary research shows that these algorithms converge arithmetically, that is, if x&lowast;
</p>
<p>is optimal, then |xk &minus; x&lowast;|2 � c/k for some constant c. This is an exceedingly poor
type of convergence. This estimate, however, may not be the best possible and in-
</p>
<p>deed there are indications that the convergence is actually geometric but with a ratio
</p>
<p>that goes to unity as the dimension of the problem increases.
</p>
<p>Modifications
</p>
<p>We now describe the supporting hyperplane algorithm (an alternative method for
</p>
<p>determining a cutting plane) and examine the possibility of dropping from consid-
</p>
<p>eration some old hyperplanes so that the linear programs do not grow too large.
</p>
<p>The convexity requirements are less severe for this algorithm. It is applicable to
</p>
<p>problems of the form
</p>
<p>minimize cTx
</p>
<p>subject to g(x) � 0,
</p>
<p>where x &isin; En, g(x) &isin; Ep, the gi&rsquo;s are continuously differentiable, and the constraint
region S defined by the inequalities is convex. Note that convexity of the functions
</p>
<p>themselves is not required. We also assume the existence of a point interior to the
</p>
<p>constraint region, that is, we assume the existence of a point y such that g(y) &lt; 0,
</p>
<p>and we assume that on the constraint boundary gi(x) = 0 implies &nabla;gi(x) � 0. The
</p>
<p>algorithm is as follows:</p>
<p/>
</div>
<div class="page"><p/>
<p>14.8 &lowast;Cutting Plane Methods 463
</p>
<p>Start with an initial polytope P containing S and such that cTx is bounded below
</p>
<p>on S . Then
</p>
<p>Step 1. Determine w = x to minimize cTx over P. If w &isin; S , stop. Otherwise,
Step 2. Find the point u on the line joining y and w that lies on the boundary
</p>
<p>of S . Let i be an index for which gi(u) = 0 and define the half-space H = {x:
&nabla;gi(u)(x &minus; u) � 0}. Update P by intersecting with H. Return to Step 1.
The algorithm is illustrated in Fig. 14.9.
</p>
<p>The price paid for the generality of this method over the convex cutting plane
</p>
<p>method is that an interpolation along the line joining y and w must be executed to
</p>
<p>find the point u. This is analogous to the line search for a minimum point required
</p>
<p>by most programming algorithms.
</p>
<p>Fig. 14.9 Supporting hyperplane algorithm
</p>
<p>Dropping Nonbinding Constraints
</p>
<p>In all cutting plane algorithms nonbinding constraints can be dropped from the app-
</p>
<p>roximating set of linear inequalities so as to keep the complexity of the approx-
</p>
<p>imation manageable. Indeed, since n linearly independent hyperplanes determine
</p>
<p>a single point in En, the algorithm can be arranged, by discarding the nonbinding
</p>
<p>constraints at the end of each step, so that the polytope consists of exactly n linear
</p>
<p>inequalities at every stage.
</p>
<p>Global convergence is not destroyed by this process, since the sequence of obj-
</p>
<p>ective values will still be monotonically increasing. It is not known, however, what
</p>
<p>effect this has on the speed of convergence.</p>
<p/>
</div>
<div class="page"><p/>
<p>464 14 Duality and Dual Methods
</p>
<p>14.9 Exercises
</p>
<p>1. (Linear programming) Use the global duality theorem to find the dual of the
</p>
<p>linear program
</p>
<p>minimize cTx
</p>
<p>subject to Ax = b, x &ge; 0.
Note that some of the regularity conditions may not be necessary for the linear
</p>
<p>case.
</p>
<p>2. (Double dual) Show that the for a convex programming problem with a solution,
</p>
<p>the dual of the dual is in some sense the original problem.
</p>
<p>3. (Non-convex?) Consider the problem
</p>
<p>minimize xy
</p>
<p>subject to x + y &minus; 4 &ge; 0
1 &le; x &le; 5, 1 &le; y &le; 5.
</p>
<p>Show that although the objective function is not convex, the primal function is
</p>
<p>convex. Find the optimal value and the Lagrange multiplier.
</p>
<p>4. Find the global maximum of the dual function of Example 1, Sect. 14.2.
</p>
<p>5. Show that the function φ defined for λ, &micro;, (&micro; � 0), by φ(λ, &micro;) = minx[ f (x) +
</p>
<p>λTh(x) + &micro;Tg(x)] is concave over any convex region where it is finite.
</p>
<p>6. Prove that the dual canonical rate of convergence is not affected by a change of
</p>
<p>variables in x.
</p>
<p>7. Corresponding to the dual function (14.23):
</p>
<p>(a) Find its gradient.
</p>
<p>(b) Find its Hessian.
</p>
<p>(c) Verify that it has a local maximum at λ&lowast;, &micro;&lowast;.
</p>
<p>8. Find the Hessian of the dual function for a separable problem.
</p>
<p>9. Find an explicit formula for the dual function for the entropy problem (Exam-
</p>
<p>ple 3, Sect. 11.4).
</p>
<p>10. Consider the problems
</p>
<p>minimize f (x) (14.65)
</p>
<p>subject to gi(x) � 0, j = 1, 2, . . . , p
</p>
<p>and
</p>
<p>minimize f (x) (14.66)
</p>
<p>subject to gi(x) + z
2
j = 0, j = 1, 2, . . . , p.
</p>
<p>(a) Let x&lowast;, μ&lowast;1, μ
&lowast;
2, . . . , μ
</p>
<p>&lowast;
p be a point and set of Lagrange multipliers that
</p>
<p>satisfy the first-order necessary conditions for (14.65). For x&lowast;, &micro;&lowast;, write the
second-order sufficiency conditions for (14.66).</p>
<p/>
</div>
<div class="page"><p/>
<p>References 465
</p>
<p>(b) Show that in general they are not satisfied unless, in addition to satisfying
</p>
<p>the sufficiency conditions of Sect. 11.8, g j(x
&lowast;) implies μ&lowast;
</p>
<p>j
&gt; 0.
</p>
<p>11. Establish global convergence for the supporting hyperplane algorithm.
</p>
<p>12. Establish global convergence for an imperfect version of the supporting hyper-
</p>
<p>plane algorithm that in interpolating to find the boundary point u actually finds
</p>
<p>a point somewhere on the segment joining u and 1
2
u + 1
</p>
<p>2
w and establishes a
</p>
<p>hyperplane there.
</p>
<p>13. Prove that the convex cutting plane method is still globally convergent if it is
</p>
<p>modified by discarding from the definition of the polytope at each stage hyper-
</p>
<p>planes corresponding to inactive linear inequalities.
</p>
<p>References
</p>
<p>14.1 Global duality was developed in conjunction with the theory of Sect. 11.9,
</p>
<p>by Hurwicz [H14] and Slater [S7]. The theory was presented in this form
</p>
<p>in Luenberger [L8].
</p>
<p>14.2&ndash;14.3 An important early differential form of duality was developed by Wolfe
</p>
<p>[W3]. The convex theory can be traced to the Legendre transformation
</p>
<p>used in the calculus of variations but it owes its main heritage to Fenchel
</p>
<p>[F3]. This line was further developed by Karlin [K1] and Hurwicz [H14].
</p>
<p>Also see Luenberger [L8].
</p>
<p>14.4 The solution of separable problems by dual methods in this manner was
</p>
<p>pioneered by Everett [E2].
</p>
<p>14.5&ndash;14.6 The method of multipliers was originally suggested by Hestenes [H8]
</p>
<p>and from a different viewpoint by Powell [P7]. The relation to duality
</p>
<p>was presented briefly in Luenberger [L15]. The method for treating ine-
</p>
<p>quality constraints was devised by Rockafellar [R3]. For an excellent
</p>
<p>survey of multiplier methods see Bertsekas [B12].
</p>
<p>14.7 The alternating direction method of multipliers was due to Gabay and
</p>
<p>Mercier[109] and Glowinski and Marrocco [102]; also see Fortin and
</p>
<p>Glowinski[96], Eckstein and Bertsekas [78] and Boyd et al. [41]. The
</p>
<p>convergence speed analysis was initially done by He and Yuan [124]
</p>
<p>and Monteiro and Svaiter [180]. The non-convergence examples of three
</p>
<p>blocks were constructed by Chen et al. [50].
</p>
<p>14.8 Cutting plane methods were first introduced by Kelley [K3] who dev-
</p>
<p>eloped the convex cutting plane method. The supporting hyperplane
</p>
<p>algorithm was suggested by Veinott [V5]. To see how global conver-
</p>
<p>gence of cutting plane algorithms can be established from the general
</p>
<p>convergence theorem see Zangwill [Z2]. For some results on the conver-
</p>
<p>gence rates of cutting plane algorithms consult Topkis [T7], Eaves and
</p>
<p>Zangwill [E1], and Wolfe [W7].</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 15
</p>
<p>Primal-Dual Methods
</p>
<p>This chapter discusses methods that work simultaneously with primal and dual
</p>
<p>variables, in essence seeking to satisfy the first-order necessary conditions for opti-
</p>
<p>mality. The methods employ many of the concepts used in earlier chapters, including
</p>
<p>those related to active set methods, various first and second order methods, penalty
</p>
<p>methods, and barrier methods. Indeed, a study of this chapter is in a sense a review
</p>
<p>and extension of what has been presented earlier.
</p>
<p>The first several sections of the chapter discuss methods for solving the standard
</p>
<p>nonlinear programming structure that has been treated in the Parts II and III of the
</p>
<p>text. These sections provide alternatives to the methods discussed earlier.
</p>
<p>15.1 The Standard Problem
</p>
<p>Consider again the standard nonlinear program
</p>
<p>minimize f (x) (15.1)
</p>
<p>subject to h(x) = 0, g(x) � 0.
</p>
<p>Together with the feasibility, the first-order necessary conditions for optimality are,
</p>
<p>as we know,
</p>
<p>&nabla; f (x) + λT&nabla;h(x) + &micro;T&nabla;g(x) = 0 (15.2)
</p>
<p>&micro; � 0
</p>
<p>&micro;Tg(x) = 0
</p>
<p>The last requirement is the complementary slackness condition. If it is known which
</p>
<p>of the inequality constraints is active at the solution, these active constraints can be
</p>
<p>&copy; Springer International Publishing Switzerland 2016
</p>
<p>D.G. Luenberger, Y. Ye, Linear and Nonlinear Programming, International
Series in Operations Research &amp; Management Science 228,
DOI 10.1007/978-3-319-18842-3 15
</p>
<p>467</p>
<p/>
</div>
<div class="page"><p/>
<p>468 15 Primal-Dual Methods
</p>
<p>rolled into the equality constraints h(x) = 0, and the inactive inequalities along with
</p>
<p>the complementary slackness condition dropped, to obtain a problem with equality
</p>
<p>constraints only. This indeed is the structure of the problem near the solution.
</p>
<p>If in this structure the vector x is n-dimensional and h is m-dimensional, then λ
</p>
<p>will also be m-dimensional. The system (15.1) will, in this reduced form, consist of
</p>
<p>n + m equations and n + m unknowns, which is an indication that the system may
</p>
<p>be well defined, and hence that there is a solution for the pair (x, λ). In essence,
</p>
<p>primal-dual methods amount to solving this system of equations, and use additional
</p>
<p>strategies to account for inequality constraints.
</p>
<p>In view of the above observation it is natural to consider whether in fact the
</p>
<p>system of necessary conditions is in fact well conditioned, possessing a unique
</p>
<p>solution (x, λ). We investigate this question by considering a linearized version
</p>
<p>of the conditions.
</p>
<p>A useful and somewhat more generally useful approach is to consider the
</p>
<p>quadratic program
</p>
<p>minimize
1
</p>
<p>2
xTQx + cTx (15.3)
</p>
<p>subject to Ax = b,
</p>
<p>where x is n-dimensional and b is m-dimensional.
</p>
<p>The first-order conditions for this problem are
</p>
<p>Qx + ATλ + c = 0 (15.4)
</p>
<p>Ax &minus; b = 0.
</p>
<p>These correspond to the necessary conditions (15.2) for equality constraints only.
</p>
<p>The following proposition gives conditions under which the system is nonsingular.
</p>
<p>Proposition. Let Q and A be n &times; n and m &times; n matrices, respectively. Suppose that A has
rank m and that Q is positive definite on the subspace M = {x : Ax = 0}. Then the matrix
</p>
<p>[
</p>
<p>Q AT
</p>
<p>A 0
</p>
<p>]
</p>
<p>(15.5)
</p>
<p>is nonsingular.
</p>
<p>Proof. Suppose (x, y) &isin; En+m is such that
</p>
<p>Qx + ATy = 0
</p>
<p>Ax = 0. (15.6)
</p>
<p>Multiplication of the first equation by xT yields
</p>
<p>xTQx + xTATy = 0,
</p>
<p>and substitution of Ax = 0 yields xTQx = 0. However, clearly x &isin; M, and thus the
hypothesis on Q together with xTQx = 0 implies that x = 0. It then follows from the
</p>
<p>first equation that ATy = 0. The full-rank condition on A then implies that y = 0.
</p>
<p>Thus the only solution to (15.6) is x = 0, y = 0. �</p>
<p/>
</div>
<div class="page"><p/>
<p>15.1 The Standard Problem 469
</p>
<p>If, as is often the case, the matrix Q is actually positive definite (over the whole
</p>
<p>space), then an explicit formula for the solution of the system can be easily derived
</p>
<p>as follows: From the first equation in (15.4) we have
</p>
<p>x = &minus;Q&minus;1ATλ &minus; Q&minus;1c.
</p>
<p>Substitution of this into the second equation then yields
</p>
<p>&minus;AQ&minus;1ATλ &minus; AQ&minus;1c &minus; b = 0,
</p>
<p>from which we immediately obtain
</p>
<p>λ = &minus;(AQ&minus;1AT )&minus;1[AQ&minus;1c + b] (15.7)
</p>
<p>and
</p>
<p>x = Q&minus;1AT (AQ&minus;1AT )&minus;1[AQ&minus;1c + b] &minus; Q&minus;1c
= &minus;Q&minus;1[I &minus; AT (AQ&minus;1AT )&minus;1AQ&minus;1]c (15.8)
+ Q&minus;1AT (AQ&minus;1AT )&minus;1b.
</p>
<p>Strategies
</p>
<p>There are some general strategies that guide the development of the primal-dual
</p>
<p>methods of this chapter.
</p>
<p>1. Descent Measures. A fundamental concept that we have frequently used is
</p>
<p>that of assuring that progress is made at each step of an iterative algorithm.
</p>
<p>It is this that is used to guarantee global convergence. In primal methods this
</p>
<p>measure of descent is the objective function. Even the simplex method of linear
</p>
<p>programming is founded on this idea of making progress with respect to the
</p>
<p>objective function. For primal minimization methods, one typically arranges
</p>
<p>that the objective function decreases at each step.
</p>
<p>The objective function is not the only possible way to measure progress. We
</p>
<p>have, for example, when minimizing a function f , considered the quantity
</p>
<p>(1/2)|&nabla; f (x)|2, seeking to monotonically reduce it to zero.
In general, a function used to measure progress is termed a merit function.
</p>
<p>Typically, it is defined so as to decrease as progress is made toward the solution
</p>
<p>of a minimization problem, but the sign may be reversed in some definitions.
</p>
<p>For primal-dual methods, the merit function may depend on both x and λ. One
</p>
<p>especially useful merit function for equality constrained problems is
</p>
<p>m(x, λ) =
1
</p>
<p>2
|&nabla; f (x) + λT&nabla;h(x)|2 + 1
</p>
<p>2
|h(x))|2.
</p>
<p>It is examined in the next section.</p>
<p/>
</div>
<div class="page"><p/>
<p>470 15 Primal-Dual Methods
</p>
<p>We shall examine other merit functions later in the chapter. With interior point
</p>
<p>methods or semidefinite programming, we shall use a potential function that
</p>
<p>serves as a merit function.
</p>
<p>2. Active Set Methods. Inequality constraints can be treated using active set
</p>
<p>methods that treat the active constraints as equality constraints, at least for the
</p>
<p>current iteration. However, in primal-dual methods, both x and λ are changed.
</p>
<p>We shall consider variations of steepest descent, conjugate directions, and
</p>
<p>Newton&rsquo;s method where movement is made in the (x,λ) space.
</p>
<p>3. Penalty Functions. In some primal-dual methods, a penalty function can serve
</p>
<p>as a merit function, even though the penalty function depends only on x. This
</p>
<p>is particularly attractive for recursive quadratic programming methods where a
</p>
<p>quadratic program is solved at each stage to determine the direction of change
</p>
<p>in the pair (x,λ).
</p>
<p>4. Interior (Barrier) Methods. Barrier methods lead to methods that move within
</p>
<p>the relative interior of the inequality constraints. This approach leads to the
</p>
<p>concept of the primal-dual central path. These methods are used for semidefinite
</p>
<p>programming since these problems are characterized as possessing a special
</p>
<p>form of inequality constraint.
</p>
<p>15.2 A Simple Merit Function
</p>
<p>It is very natural, when considering the system of necessary conditions (15.2), to
</p>
<p>form the function
</p>
<p>m(x, λ) =
1
</p>
<p>2
|&nabla; f (x) + λT&nabla;h(x)|2 + 1
</p>
<p>2
|h(x)|2, (15.9)
</p>
<p>and use it as a measure of how close a point (x, λ) is to a solution.
</p>
<p>It must be noted, however, that the function m(x, λ) is not always well-behaved;
</p>
<p>it may have local minima, and these are of no value in a search for a solution. The
</p>
<p>following theorem gives the conditions under which the function m(x, λ) can serve
</p>
<p>as a well-behaved merit function. Basically, the main requirement is that the Hessian
</p>
<p>of the Lagrangian be positive definite. As usual, we define l(x, λ) = f (x) + λTh(x).
</p>
<p>Theorem. Let f and h be twice continuously differentiable functions on En of dimension 1
</p>
<p>and m, respectively. Suppose that x&lowast; and λ&lowast; satisfy the first-order necessary conditions for a
local minimum of m(x, λ) = 1
</p>
<p>2
|&nabla; f (x)+λT&nabla;h(x)|2+ 1
</p>
<p>2
|h(x)|2 with respect to x and λ. Suppose
</p>
<p>also that at x&lowast;, λ&lowast;, (i) the rank of &nabla;h(x&lowast;) is m and (ii) the Hessian matrix L(x&lowast;, λ&lowast;) =
F(x&lowast;)+λ&lowast;TH(x&lowast;) is positive definite. Then, x&lowast;, λ&lowast; is a (possibly nonunique) global minimum
point of m(x, λ), with value m(x&lowast;, λ&lowast;) = 0.
</p>
<p>Proof. Since x&lowast;, λ&lowast; satisfies the first-order conditions for a local minimum point of
m(x, λ), we have
</p>
<p>[&nabla; f (x&lowast;) + λ&lowast;T&nabla;h(x&lowast;)]L(x&lowast;, λ&lowast;) + h(x&lowast;)T&nabla;h(x&lowast;) = 0 (15.10)
</p>
<p>[&nabla; f (x&lowast;) + λ&lowast;T&nabla;h(x&lowast;)]&nabla;h(x&lowast;)T = 0. (15.11)</p>
<p/>
</div>
<div class="page"><p/>
<p>15.3 Basic Primal-Dual Methods 471
</p>
<p>Multiplying (15.10) on the right by [&nabla; f (x&lowast;) + λ&lowast;T&nabla;h(x&lowast;)]T and using (15.11) we
obtain&dagger;
</p>
<p>&nabla;l(x&lowast;, λ&lowast;)L(x&lowast;, λ&lowast;)&nabla;l(x&lowast;, λ&lowast;)T = 0.
</p>
<p>Since L(x&lowast;, λ&lowast;) is positive definite, this implies that &nabla;l(x&lowast;, λ&lowast;) = 0. Using this
in (15.10), we find that h(x&lowast;)T&nabla;h(x&lowast;) = 0, which, since &nabla;h(x&lowast;) is of rank m, implies
</p>
<p>that h(x&lowast;) = 0. �
</p>
<p>The requirement that the Hessian of the Lagrangian L(x&lowast;, λ&lowast;) be positive defi-
nite at a stationary point of the merit function m is actually not too restrictive. This
</p>
<p>condition will be satisfied in the case of a convex programming problem where f is
</p>
<p>strictly convex and h is linear. Furthermore, even in nonconvex problems one can
</p>
<p>often arrange for this condition to hold, at least near a solution to the original con-
</p>
<p>strained minimization problem. If it is assumed that the second-order sufficiency
</p>
<p>conditions for a constrained minimum hold at x&lowast;, λ&lowast;, then L(x&lowast;, λ&lowast;) is positive
definite on the subspace that defines the tangent to the constraints; that is, on the
</p>
<p>subspace defined by &nabla;h(x&lowast;)x = 0. Now if the original problem is modified with a
penalty term to the problem
</p>
<p>minimize f (x) +
1
</p>
<p>2
c|h(x)|2 (15.12)
</p>
<p>subject to h(x) = 0,
</p>
<p>the solution point x&lowast; will be unchanged. However, as discussed in Chap. 14, the
Hessian of the Lagrangian of this new problem (15.12) at the solution point is
</p>
<p>L(x&lowast;, λ&lowast;) + c&nabla;h(x&lowast;)T&nabla;h(x&lowast;). For sufficiently large c, this matrix will be positive
definite. Thus a problem can be &ldquo;convexified&rdquo; (at least locally) before the merit
</p>
<p>function method is employed.
</p>
<p>An extension to problems with inequality constraints can be defined by partition-
</p>
<p>ing the constraints into the two groups active and inactive. However, at this point
</p>
<p>the simple merit function for problems with equality constraints is adequate for the
</p>
<p>purpose of illustrating the general idea.
</p>
<p>15.3 Basic Primal-Dual Methods
</p>
<p>Many primal-dual methods are patterned after some of the methods used in ear-
</p>
<p>lier chapters, except of course that the emphasis is on equation solving rather than
</p>
<p>explicit optimization.
</p>
<p>&dagger; Unless explicitly indicated to the contrary, the notation &nabla;l(x, λ) refers to the gradient of l with
respect to x, that is, &nabla;xl(x, λ).</p>
<p/>
</div>
<div class="page"><p/>
<p>472 15 Primal-Dual Methods
</p>
<p>First-Order Method
</p>
<p>We consider first a simple straightforward approach, which in a sense parallels the
</p>
<p>idea of steepest descent in that it uses only a first-order approximation to the primal&ndash;
</p>
<p>dual equations. It is defined by
</p>
<p>xk+1 = xk &minus; αk&nabla;l(xk, λk)T (15.13)
λk+1 = λk + αkh(xk),
</p>
<p>where αk is not yet determined. This is based on the error in satisfying (15.2). As-
</p>
<p>sume that the Hessian of the Lagrangian L(x, λ) is positive definite in some compact
</p>
<p>region of interest, and consider the simple merit function
</p>
<p>m(x, λ) =
1
</p>
<p>2
|&nabla;l(x, λ)|2 + 1
</p>
<p>2
|h(x)|2 (15.14)
</p>
<p>discussed above. We would like to determine whether the direction of change
</p>
<p>in (15.13) is a descent direction with respect to this merit function. The gradient
</p>
<p>of the merit function has components corresponding to x and λ of
</p>
<p>&nabla;l(x, λ)L(x, λ) + h(x)T&nabla;h(x) (15.15)
</p>
<p>&nabla;l(x, λ)&nabla;h(x)T .
</p>
<p>Thus the inner product of this gradient with the direction vector having components
</p>
<p>&minus;&nabla;l(x, λ)T , h(x) is
</p>
<p>&minus;&nabla;l(x, λ)L(x, λ)&nabla;l(x, λ)T &minus; h(x)T&nabla;h(x)&nabla;l(x, λ)T + &nabla;l(x, λ)&nabla;h(x)Th(x)
= &minus;&nabla;l(x, λ)L(x, λ)&nabla;l(x, λ)T � 0.
</p>
<p>This shows that the search direction is in fact a descent direction for the merit
</p>
<p>function, unless &nabla;l(x, λ) = 0. Thus by selecting αk to minimize the merit func-
</p>
<p>tion in the search direction at each step, the process will converge to a point where
</p>
<p>&nabla;l(x, λ) = 0. However, there is no guarantee that h(x) = 0 at that point.
</p>
<p>We can try to improve the method either by changing the way in which the direc-
</p>
<p>tion is selected or by changing the merit function. In this case a slight modification
</p>
<p>of the merit function will work. Let
</p>
<p>w(x, λ, γ) = m(x, λ) &minus; γ[ f (x) + λTh(x)]
</p>
<p>for some γ &gt; 0. We then calculate that the gradient of w has the two components
</p>
<p>corresponding to x and λ
</p>
<p>&nabla;l(x, λ)L(x, λ) + h(x)T&nabla;h(x) &minus; γ&nabla;l(x, λ)
&nabla;l(x, λ)&nabla;h(x)T &minus; γh(x)T ,
</p>
<p>and hence the inner product of the gradient with the direction &minus;&nabla;l(x, λ)T , h(x) is
</p>
<p>&minus;&nabla;l(x, λ)[L(x, λ) &minus; γI]&nabla;l(x, λ)T &minus; γ|h(x)|2.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.3 Basic Primal-Dual Methods 473
</p>
<p>Now since we are assuming that L(x, λ) is positive definite in a compact region of
</p>
<p>interest, there is a γ &gt; 0 such that L(x, λ) &minus; γI is positive definite in this region.
Then according to the above calculation, the direction &minus;&nabla;l(x, λ)T , h(x) is a descent
direction, and the standard descent method will converge to a solution. This method
</p>
<p>will not converge very rapidly however. (See Exercise 2 for further analysis of this
</p>
<p>method.)
</p>
<p>Conjugate Directions
</p>
<p>One may also use the conjugate direction. Let us consider the quadratic program
</p>
<p>minimize
1
</p>
<p>2
xTQx &minus; bTx (15.16)
</p>
<p>subject to Ax = c.
</p>
<p>The first-order necessary conditions for this problem are
</p>
<p>Qx + ATλ = b (15.17)
</p>
<p>Ax = c.
</p>
<p>As discussed in the previous section, this problem is equivalent to solving a system
</p>
<p>of linear equations whose coefficient matrix is
</p>
<p>M =
</p>
<p>[
</p>
<p>Q AT
</p>
<p>A 0
</p>
<p>]
</p>
<p>. (15.18)
</p>
<p>This matrix is symmetric, but it is not positive definite (nor even semidefinite). How-
</p>
<p>ever, it is possible to formally generalize the conjugate gradient method to systems
</p>
<p>of this type by just applying the conjugate-gradient formulae (15.17)&ndash;(15.20) of
</p>
<p>Sect. 9.3 with Q replaced by M. A difficulty is that singular directions (defined as
</p>
<p>directions p such that pTMp = 0) may occur and cause the process to break down.
</p>
<p>Procedures for overcoming this difficulty have been developed, however. Also, as
</p>
<p>in the ordinary conjugate gradient method, the approach can be generalized to treat
</p>
<p>nonquadratic problems as well. Overall, however, the application of conjugate di-
</p>
<p>rection methods to the Lagrange system of equations, although very promising, is
</p>
<p>not currently considered practical.
</p>
<p>Second-Order Method: Newton&rsquo;s Method
</p>
<p>Newton&rsquo;s method for solving systems of equations can be easily applied to the
</p>
<p>Lagrange equations. In its most straightforward form, the method solves the system
</p>
<p>&nabla;l(x, λ) = 0 (15.19)
</p>
<p>h(x) = 0</p>
<p/>
</div>
<div class="page"><p/>
<p>474 15 Primal-Dual Methods
</p>
<p>by solving the linearized version recursively. That is, given xk, λk the new point
</p>
<p>xk+1, λk+1 is determined from the equations
</p>
<p>&nabla;l(xk, λk)
T + L(xk, λk)dk + &nabla;h(xk)
</p>
<p>Tyk = 0 (15.20)
</p>
<p>h(xk) + &nabla;h(xk)dk = 0
</p>
<p>by setting xk+1 = xk + dk, λk+1 = λk + yk. In matrix form the above Newton equa-
</p>
<p>tions are
[
</p>
<p>L(xk,λk) &nabla;h(xk)
T
</p>
<p>&nabla;h(xk) 0
</p>
<p>] [
</p>
<p>dk
yk
</p>
<p>]
</p>
<p>=
</p>
<p>[
</p>
<p>&minus;&nabla;l(xk,λk)T
&minus;h(xk)
</p>
<p>]
</p>
<p>. (15.21)
</p>
<p>The Newton equations have some important structural properties. First, we observe
</p>
<p>that by adding &nabla;h(xk)
Tλk to the top equation, the system can be transformed to the
</p>
<p>form
[
</p>
<p>L(xk,λk) &nabla;h(xk)
T
</p>
<p>&nabla;h(xk) 0
</p>
<p>] [
</p>
<p>dk
λk+1
</p>
<p>]
</p>
<p>=
</p>
<p>[
</p>
<p>&minus;&nabla; f (xk)T
&minus;h(xk)
</p>
<p>]
</p>
<p>, (15.22)
</p>
<p>where again λk+1 = λk + yk. In this form λk appears only in the matrix L(xk, λk).
</p>
<p>This conversion between (15.21) and (15.22) will be useful later.
</p>
<p>Next we note that the structure of the coefficient matrix of (15.21) or (15.22) is
</p>
<p>identical to that of the Proposition of Sect. 15.1. The standard second-order suffi-
</p>
<p>ciency conditions imply that &nabla;h(x&lowast;) is of full rank and that L(x&lowast;, λ&lowast;) is positive
definite on M = {x : &nabla;h(x&lowast;)x = 0} at the solution. By continuity these conditions
can be assumed to hold in a region near the solution as well. Under these assump-
</p>
<p>tions it follows from Proposition 1 that the Newton equation (15.21) has a unique
</p>
<p>solution.
</p>
<p>It is again worthwhile to point out that, although the Hessian of the Lagrangian
</p>
<p>need be positive definite only on the tangent subspace in order for the system (15.21)
</p>
<p>to be nonsingular, it is possible to alter the original problem by incorporation of
</p>
<p>a quadratic penalty term so that the new Hessian of the Lagrangian is L(x, λ) +
</p>
<p>c&nabla;h(x)T&nabla;h(x). For sufficiently large c, this new Hessian will be positive definite
</p>
<p>over the entire space.
</p>
<p>If L(x, λ) is positive definite (either originally or through the incorporation of
</p>
<p>a penalty term), it is possible to write an explicit expression for the solution of the
</p>
<p>system (15.21). Let us define Lk = L(xk, λk), Ak = &nabla;h(xk), Ik = &nabla;l(xk, λk)
T , hk =
</p>
<p>h(xk). The system then takes the form
</p>
<p>Lkdk + A
T
k yk = &minus;1k (15.23)
</p>
<p>Akdk = &minus;hk.
</p>
<p>The solution is readily found, as in (15.7) and (15.8) for quadratic programming,
</p>
<p>to be
</p>
<p>yk = (AkL
&minus;1
k A
</p>
<p>T
k )
</p>
<p>&minus;1[hk &minus; AkL&minus;1k Ik] (15.24)
dk = &minus;L&minus;1k [I &minus; ATk (AkL&minus;1k ATk )&minus;1AkL&minus;1k ]Ik &minus; L&minus;1k ATk (AkL&minus;1k ATk )&minus;1hk. (15.25)
</p>
<p>There are standard results concerning Newton&rsquo;s method applied to a system of non-
</p>
<p>linear equations that are applicable to the system (15.19). These results state that if</p>
<p/>
</div>
<div class="page"><p/>
<p>15.3 Basic Primal-Dual Methods 475
</p>
<p>the linearized system is nonsingular at the solution (as is implied by our assump-
</p>
<p>tions) and if the initial point is sufficiently close to the solution, the method will in
</p>
<p>fact converge to the solution and the convergence will be of order at least two. To
</p>
<p>guarantee convergence from remote initial points and hence be more broadly ap-
</p>
<p>plicable, it is desirable to use the method as a descent process. Fortunately, we can
</p>
<p>show that the direction generated by Newton&rsquo;s method is a descent direction for the
</p>
<p>simple merit function
</p>
<p>m(x, λ) =
1
</p>
<p>2
|&nabla;l(x, λ)|2 + 1
</p>
<p>2
|h(x)|2.
</p>
<p>Given dk, yk satisfying (15.23), the inner product of this direction with the gradient
</p>
<p>of m at xk, λk is, referring to (15.15),
</p>
<p>[LkIk + A
T
k hk, AkIk]
</p>
<p>T [dk, yk] = I
T
k Lkdk + h
</p>
<p>T
k Akdk + I
</p>
<p>T
k A
</p>
<p>T
k yk
</p>
<p>= &minus;|Ik |2 &minus; |hk|2.
</p>
<p>This is strictly negative unless both Ik = 0 and hk = 0. Thus Newton&rsquo;s method has
</p>
<p>desirable global convergence properties when executed as a descent method with
</p>
<p>variable step size.
</p>
<p>Note that the calculation above does not employ the explicit formulae (15.24)
</p>
<p>and (15.25), and hence it is not necessary that L(x, λ) be positive definite, as long
</p>
<p>as the system (15.21) is invertible. We summarize the above discussion by the fol-
</p>
<p>lowing theorem.
</p>
<p>Theorem. Define the Newton process by
</p>
<p>xk+1 = xk + αkdk
</p>
<p>λk+1 = λk + αkyk,
</p>
<p>where dk, yk are solutions to (15.24) and where αk is selected to minimize the merit function
</p>
<p>m(x, λ) =
1
</p>
<p>2
|&nabla;l(x, λ)|2 + 1
</p>
<p>2
|h(x)|2 .
</p>
<p>Assume that dk, yk exist and that the points generated lie in a compact set. Then any limit
</p>
<p>point of these points satisfies the first-order necessary conditions for a solution to the con-
</p>
<p>strained minimization problem (15.1).
</p>
<p>Proof. Most of this follows from the above observations and the Global Conver-
</p>
<p>gence Theorem. The one-dimensional search process is well-defined, since the merit
</p>
<p>function m is bounded below. �
</p>
<p>In view of this result, it is worth pursuing Newton&rsquo;s method further. We would
</p>
<p>like to extend it to problems with inequality constraints. We would also like to avoid
</p>
<p>the necessity of evaluating L(xk, λk) at each step and to consider alternative merit
</p>
<p>functions&mdash;perhaps those that might distinguish a local maximum from a local min-
</p>
<p>imum, which the simple merit function does not do. These considerations guide the
</p>
<p>developments of the next several sections.</p>
<p/>
</div>
<div class="page"><p/>
<p>476 15 Primal-Dual Methods
</p>
<p>Relation to Sequential Quadratic Programming
</p>
<p>It is clear from the development of the preceding discussion that Newton&rsquo;s method
</p>
<p>is closely related to quadratic programming with equality constraints. We explore
</p>
<p>this relationship more fully here, which will lead to a generalization of Newton&rsquo;s
</p>
<p>method to problems with inequality constraints.
</p>
<p>Consider the problem
</p>
<p>minimize ITk dk +
1
</p>
<p>2
dTk Lkdk (15.26)
</p>
<p>subject to Akdk + hk = 0.
</p>
<p>The first-order necessary conditions of this problem are exactly (15.21), or equiv-
</p>
<p>alently (15.23), where yk corresponds to the Lagrange multiplier of (15.26). Thus,
</p>
<p>the solution of (15.26) produces a Newton step.
</p>
<p>Alternatively, we may consider the quadratic program
</p>
<p>minimize &nabla; f (xk)dk +
1
</p>
<p>2
dTk Lkdk (15.27)
</p>
<p>subject to Akdk + hk = 0.
</p>
<p>The necessary conditions of this problem are exactly (15.22), where λk+1 now cor-
</p>
<p>responds to the Lagrange multiplier of (15.27). The program (15.27) is obtained
</p>
<p>from (15.26) by merely subtracting λTk Akdk from the objective function; and this
</p>
<p>change has no influence on dk, since Akdk is fixed.
</p>
<p>The connection with quadratic programming suggests a procedure for extending
</p>
<p>Newton&rsquo;s method to minimization problems with inequality constraints. Consider
</p>
<p>the problem
minimize f (x)
</p>
<p>subject to h(x) = 0
</p>
<p>g(x) � 0.
</p>
<p>Given an estimated solution point xk and estimated Lagrange multipliers λk, &micro;k,
</p>
<p>one solves the quadratic program
</p>
<p>minimize &nabla; f (xk)dk +
1
2
dT
k
</p>
<p>Lkdk
subject to &nabla;h(xk)dk + hk = 0
</p>
<p>&nabla;g(xk)dk + gk � 0,
</p>
<p>(15.28)
</p>
<p>where Lk = F(xk) + λ
T
k H(xk) + &micro;
</p>
<p>T
k
</p>
<p>G(xk), hk = h(xk), gk = g(xk). The new point is
</p>
<p>determined by xk+1 = xk + dk, and the new Lagrange multipliers are the Lagrange
</p>
<p>multipliers of the quadratic program (15.28). This is the essence of an early method
</p>
<p>for nonlinear programming termed SOLVER. It is a very attractive procedure, since
</p>
<p>it applies directly to problems with inequality as well as equality constraints without
</p>
<p>the use of an active set strategy (although such a strategy might be used to solve
</p>
<p>the required quadratic program). Methods of this general type, where a quadratic
</p>
<p>program is solved at each step, are referred to as recursive quadratic programming
</p>
<p>methods, and several variations are considered in this chapter.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.4 Modified Newton Methods 477
</p>
<p>As presented here the recursive quadratic programming method extends Newton&rsquo;s
</p>
<p>method to problems with inequality constraints, but the method has limitations. The
</p>
<p>quadratic program may not always be well-defined, the method requires second-
</p>
<p>order derivative information, and the simple merit function is not a descent function
</p>
<p>for the case of inequalities. Of these, the most serious is the requirement of second-
</p>
<p>order information, and this is addressed in the next section.
</p>
<p>15.4 Modified Newton Methods
</p>
<p>A modified Newton method is based on replacing the actual linearized system by an
</p>
<p>approximation.
</p>
<p>First, we concentrate on the equality constrained optimization problem
</p>
<p>minimize f (x)
</p>
<p>subject to h(x) = 0
(15.29)
</p>
<p>in order to most clearly describe the relationships between the various approaches.
</p>
<p>Problems with inequality constraints can be treated within the equality constraint
</p>
<p>framework by an active set strategy or, in some cases, by recursive quadratic pro-
</p>
<p>gramming.
</p>
<p>The basic equations for Newton&rsquo;s method can be written
</p>
<p>[
</p>
<p>xk+1
λk+1
</p>
<p>]
</p>
<p>=
</p>
<p>[
</p>
<p>xk
λk
</p>
<p>]
</p>
<p>&minus; αk
[
</p>
<p>Lk A
T
k
</p>
<p>Ak 0
</p>
<p>]&minus;1 [
lk
hk
</p>
<p>]
</p>
<p>,
</p>
<p>where as before Lk is the Hessian of the Lagrangian, Ak = &nabla;h(xk), Ik = [&nabla; f (xk) +
</p>
<p>λTk &nabla;h(xk)]
T , hk = h(xk). A structured modified Newton method is a method of the
</p>
<p>form
[
</p>
<p>xk+1
λk+1
</p>
<p>]
</p>
<p>=
</p>
<p>[
</p>
<p>xk
λk
</p>
<p>]
</p>
<p>&minus; αk
[
</p>
<p>Bk A
T
k
</p>
<p>Ak 0
</p>
<p>]&minus;1 [
lk
hk
</p>
<p>]
</p>
<p>, (15.30)
</p>
<p>where Bk is an approximation to Lk. The term &ldquo;structured&rdquo; derives from the fact that
</p>
<p>only second-order information in the original system of equations is approximated;
</p>
<p>the first-order information is kept intact.
</p>
<p>Of course the method is implemented by solving the system
</p>
<p>Bkdk + A
T
k yk = &minus;Ik (15.31)
</p>
<p>Akdk = &minus;hk
</p>
<p>for dk and yk and then setting xk+1 = xk + αkdk, λk+1 = λk + αkyk for some value
</p>
<p>of αk. In this section we will not consider the procedure for selection of αk, and thus
</p>
<p>for simplicity we take αk = 1. The simple transformation used earlier can be applied
</p>
<p>to write (15.31) in the form</p>
<p/>
</div>
<div class="page"><p/>
<p>478 15 Primal-Dual Methods
</p>
<p>Bkdk + A
T
k λk+1 = &minus;&nabla; f (xk)T (15.32)
</p>
<p>Akdk = &minus;hk.
</p>
<p>Then xk+1 = xk + dk, and λk+1 is found directly as a solution to system (15.32).
</p>
<p>There are, of course, various ways to choose the approximation Bk. One is to use
</p>
<p>a fixed, constant matrix throughout the iterative process. A second is to base Bk on
</p>
<p>some readily accessible information in L(xk, λk), such as setting Bk equal to the
</p>
<p>diagonal of L(xk, λk). Finally, a third possibility is to update Bk using one of the
</p>
<p>various quasi-Newton formulae.
</p>
<p>One important advantage of the structured method is that Bk can be taken to be
</p>
<p>positive definite even though Lk is not. If this is done, we can write the explicit
</p>
<p>solution
</p>
<p>yk = (AkB
&minus;1
k A
</p>
<p>T
k )
</p>
<p>&minus;1[hk &minus; AkB&minus;1k Ik] (15.33)
dk = &minus;B&minus;1k [I &minus; ATk (AkB&minus;1k ATk )&minus;1AkB&minus;1k ]Ik &minus; B&minus;1k ATk (AkB&minus;1k ATk )&minus;1hk. (15.34)
</p>
<p>Consider the quadratic program
</p>
<p>minimize &nabla; f (xk)dk +
1
</p>
<p>2
dTk Bkdk (15.35)
</p>
<p>subject to Akdk + h(xk) = 0.
</p>
<p>The first-order necessary conditions for this problem are
</p>
<p>Bkdk + A
T
k λk+1 = &minus;&nabla; f (xk)T (15.36)
</p>
<p>Akdk = &minus;h(xk),
</p>
<p>which are again identical to the system of equations of the structured modified
</p>
<p>Newton method&mdash;in this case in the form (15.33). The Lagrange multiplier of the
</p>
<p>quadratic program is λk+1. The equivalence of (15.35) and (15.36) leads to a recur-
</p>
<p>sive quadratic programming method, where at each xk the quadratic program (15.35)
</p>
<p>is solved to determine the direction dk. In this case an arbitrary symmetric matrix
</p>
<p>Bk is used in place of the Hessian of the Lagrangian. Note that the problem (15.35)
</p>
<p>does not explicitly depend on λk; but Bk, often being chosen to approximate the
</p>
<p>Hessian of the Lagrangian, may depend on λk.
</p>
<p>As before, a principal advantage of the quadratic programming formulation is
</p>
<p>that there is an obvious extension to problems with inequality constraints: One
</p>
<p>simply employs a linearized version of the inequalities.
</p>
<p>15.5 Descent Properties
</p>
<p>In order to ensure convergence of the structured modified Newton methods of the
</p>
<p>previous section, it is necessary to find a suitable merit function&mdash;a merit function
</p>
<p>that is compatible with the direction-finding algorithm in the sense that it decreases</p>
<p/>
</div>
<div class="page"><p/>
<p>15.5 Descent Properties 479
</p>
<p>along the direction generated. We must abandon the simple merit function at this
</p>
<p>point, since it is not compatible with these methods when Bk � Lk. However, two
</p>
<p>other penalty functions considered earlier, the absolute-value exact penalty func-
</p>
<p>tion and the quadratic penalty function, are compatible with the modified Newton
</p>
<p>approach.
</p>
<p>Absolute-Value Penalty Function
</p>
<p>Let us consider the constrained minimization problem
</p>
<p>minimize f (x) (15.37)
</p>
<p>subject to g(x) � 0,
</p>
<p>where g(x) is r-dimensional. For notational simplicity we consider the case of ine-
</p>
<p>quality constraints only, since it is, in fact, the most difficult case. The extension
</p>
<p>to equality constraints is straightforward. In accordance with the recursive quadratic
</p>
<p>programming approach, given a current point x, we select the direction of movement
</p>
<p>d by solving the quadratic programming problem
</p>
<p>minimize
1
</p>
<p>2
dTBd + &nabla; f (x)d (15.38)
</p>
<p>subject to &nabla;g(x)d + g(x) � 0,
</p>
<p>where B is positive definite.
</p>
<p>The first-order necessary conditions for a solution to this quadratic program are
</p>
<p>Bd + &nabla; f (x)T + &nabla;g(x)T&micro; = 0 (15.39a)
</p>
<p>&nabla;g(x)d + g(x) � 0 (15.39b)
</p>
<p>&micro;T [&nabla;g(x)d + g(x)] = 0 (15.39c)
</p>
<p>&micro; � 0. (15.39d)
</p>
<p>Note that if the solution to the quadratic program has d = 0, then the point x,
</p>
<p>together with &micro; from (15.39), satisfies the first-order necessary conditions for the
</p>
<p>original minimization problem (15.37). The following proposition is the fundamen-
</p>
<p>tal result concerning the compatibility of the absolute-value penalty function and
</p>
<p>the quadratic programming method for determining the direction of movement.
</p>
<p>Proposition 1. Let d, &micro; (with d � 0) be a solution of the quadratic program (15.38). Then
</p>
<p>if c � max
j
</p>
<p>(μ j), the vector d is a descent direction for the penalty function
</p>
<p>P(x) = f (x) + c
</p>
<p>r
&sum;
</p>
<p>j=1
</p>
<p>g j(x)
+.</p>
<p/>
</div>
<div class="page"><p/>
<p>480 15 Primal-Dual Methods
</p>
<p>Proof. Let J(x) = { j : g j(x) &gt; 0}. Now for α &gt; 0,
</p>
<p>P(x + αd) = f (x + αd) + c
</p>
<p>r
&sum;
</p>
<p>j=1
</p>
<p>g j(x + αd)
+
</p>
<p>= f (x) + α&nabla; f (x)d + c
</p>
<p>r
&sum;
</p>
<p>j=1
</p>
<p>[g j(x) + α&nabla;g j(x)d]
+ + o(α)
</p>
<p>= f (x) + α&nabla; f (x)d + c
</p>
<p>r
&sum;
</p>
<p>j=1
</p>
<p>g j(x)
+ + αc
</p>
<p>&sum;
</p>
<p>j&isin;J(x)
&nabla;g j(x)d + o(α)
</p>
<p>= P(x) + α&nabla; f (x)d + αc
&sum;
</p>
<p>j&isin;J(x)
&nabla;g j(x)d + o(α). (15.40)
</p>
<p>Where (15.39b) was used in the third line to infer that &nabla;g j(x) &le; 0 if g j(x) = 0.
Again using (15.39b) we have
</p>
<p>c
&sum;
</p>
<p>j&isin;J(x)
&nabla;g j(x)d � c
</p>
<p>&sum;
</p>
<p>j&isin;J(x)
&minus;g j(x) = &minus;c
</p>
<p>r
&sum;
</p>
<p>j=1
</p>
<p>g j(x)
+. (15.41)
</p>
<p>Using (15.39a) we have
</p>
<p>&nabla;f(x)d = &minus;dT Bd &minus;
r
</p>
<p>&sum;
</p>
<p>j=1
</p>
<p>μ j&nabla;g j(x)d,
</p>
<p>which by using the complementary slackness condition (15.39c) leads to
</p>
<p>&nabla; f (x)d = &minus;dTBd +
r
</p>
<p>&sum;
</p>
<p>j=1
</p>
<p>μ jg j(x) � &minus;dTBd +
r
</p>
<p>&sum;
</p>
<p>j=1
</p>
<p>μ jg j(x)
+ (15.42)
</p>
<p>&le; &minus;dT Bd + max (μ j)
r
</p>
<p>&sum;
</p>
<p>j=1
</p>
<p>g j(x)
+.
</p>
<p>Finally, substituting (15.41) and (15.42) in (15.40), we find
</p>
<p>P(x + αd) � P(x) + α{&minus;dTBd &minus; [c &minus; max(μ j)]
r
</p>
<p>&sum;
</p>
<p>j=1
</p>
<p>g j(x)
+} + o(α),
</p>
<p>Since B is positive definite and c &ge; max(μ j), it follows that for α sufficiently small,
P(x + αd) &lt; P(x). �
</p>
<p>The above proposition is exceedingly important, for it provides a basis for est-
</p>
<p>ablishing the global convergence of modified Newton methods, including recursive
</p>
<p>quadratic programming. The following is a simple global convergence result based
</p>
<p>on the descent property.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.5 Descent Properties 481
</p>
<p>Theorem. Let B be positive definite and assume that throughout some compact region
</p>
<p>&sub; En, the quadratic program (15.38) has a unique solution d, &micro; such that at each point
the Lagrange multipliers satisfy max
</p>
<p>j
(μ j) � c. Let the sequence {xk} be generated by
</p>
<p>xk+1 = xk + αkdk,
</p>
<p>where dk is the solution to (15.38) at xk and where αk minimizes P(xk+1). Assume that each
xk &isin; Ω. Then every limit point x of {xk} satisfies the first-order necessary conditions for the
constrained minimization problem (15.37).
</p>
<p>Proof. The solution to a quadratic program depends continuously on the data, and
</p>
<p>hence the direction determined by the quadratic program (15.38) is a continuous
</p>
<p>function of x. The function P(x) is also continuous, and by Proposition 1, it fol-
</p>
<p>lows that P is a descent function at every point that does not satisfy the first-order
</p>
<p>conditions. The result thus follows from the Global Convergence Theorem. �
</p>
<p>In view of the above result, recursive quadratic programming in conjunction with
</p>
<p>the absolute-value penalty function is an attractive technique. There are, however,
</p>
<p>some difficulties to be kept in mind. First, the selection of the parameter αk requires
</p>
<p>a one-dimensional search with respect to a nondifferentiable function. Thus the eff-
</p>
<p>icient curve-fitting search methods of Chap. 8 cannot be used without significant
</p>
<p>modification. Second, use of the absolute-value function requires an estimate of an
</p>
<p>upper bound for μ j&rsquo;s, so that c can be selected properly. In some applications a
</p>
<p>suitable bound can be obtained from previous experience, but in general one must
</p>
<p>develop a method for revising the estimate upward when necessary.
</p>
<p>Another potential difficulty with the quadratic programming approach above is
</p>
<p>that the quadratic program (15.38) may be infeasible at some point xk, even though
</p>
<p>the original problem (15.37) is feasible. If this happens, the method breaks down.
</p>
<p>However, see Exercise 8 for a method that avoids this problem.
</p>
<p>The Quadratic Penalty Function
</p>
<p>Another penalty function that is compatible with the modified Newton method
</p>
<p>approach is the standard quadratic penalty function. It has the added technical adv-
</p>
<p>antage that, since this penalty function is differentiable, it is possible to apply our
</p>
<p>earlier analytical principles to study the rate of convergence of the method. This
</p>
<p>leads to an analytical comparison of primal-dual methods with the methods of other
</p>
<p>chapters.
</p>
<p>We shall restrict attention to the problem with equality constraints, since that is
</p>
<p>all that is required for a rate of convergence analysis. The method can be extended
</p>
<p>to problems with inequality constraints either directly or by an active set method.
</p>
<p>Thus we consider the problem
</p>
<p>minimize f (x) (15.43)
</p>
<p>subject to h(x) = 0</p>
<p/>
</div>
<div class="page"><p/>
<p>482 15 Primal-Dual Methods
</p>
<p>and the standard quadratic penalty objective
</p>
<p>P(x) = f (x) +
1
</p>
<p>2
c|h(x)|2. (15.44)
</p>
<p>From the theory in Chap. 13, we know that minimization of the objective with
</p>
<p>a quadratic penalty function will not yield an exact solution to (15.43). In fact,
</p>
<p>the minimum of the penalty function (15.44) will have ch(x) ≃ λ, where λ is the
Lagrange multiplier of (15.43). Therefore, it seems appropriate in this case to con-
</p>
<p>sider the quadratic programming problem
</p>
<p>minimize
1
</p>
<p>2
dTBd + &nabla; f (x)d (15.45)
</p>
<p>subject to &nabla;h(x)d + h(x) = λ̂/c,
</p>
<p>where λ̂ is an estimate of the Lagrange multiplier of the original problem. A partic-
</p>
<p>ularly good choice is
</p>
<p>λ̂ = [(1/c)I + Q]&minus;1[h(x) &minus; AB&minus;1&nabla; f (x)T ], (15.46)
</p>
<p>where A = &nabla;h(x), Q = AB &minus; 1AT which is the Lagrange multiplier that would be
obtained by the quadratic program with the penalty method. The proposed method
</p>
<p>requires that λ̂ be first estimated from (15.46) and then used in the quadratic pro-
</p>
<p>gramming problem (15.45).
</p>
<p>The following proposition shows that this procedure produces a descent direction
</p>
<p>for the quadratic penalty objective.
</p>
<p>Proposition 2. For any c &gt; 0, let d, λ (with d � 0) be a solution to the quadratic pro-
gram (15.45). Then d is a descent direction of the function P(x) = f (x) + (1/2)c|h(x)|2 .
</p>
<p>Proof. We have from the constraint equation
</p>
<p>Ad = (1/c)λ̂ &minus; h(x),
which yields
</p>
<p>cATAd = AT λ̂ &minus; cATh(x).
Solving the necessary conditions for (15.45) yields (see the top part of (15.9) for a
</p>
<p>similar expression with Q = B there)
</p>
<p>Bd = ATQ&minus;1[AB&minus;1&nabla;f(x)T + (1/c)λ̂ &minus; h(x)] &minus; &nabla; f (x)T .
</p>
<p>Therefore,
</p>
<p>(B + cATA)d = ATQ&minus;1[AB&minus;1&nabla; f (x)T &minus; h(x)]
+ AT [(1/c)Q&minus;1 + I]λ̂ &minus; &nabla; f (x)T &minus; cATh(x)
= ATQ&minus;1{AB&minus;1&nabla; f (x)τ &minus; h(x) + ((1/c)I + Q)λ̂}
&minus; &nabla; f (x)T &minus; cATh(x)
= &minus;&nabla; f (x)T &minus; cATh(x) = &minus;&nabla;P(x)T .
</p>
<p>The matrix (B+cATA) is positive definite for any c � 0. It follows that &nabla;P(x)d &lt; 0.
</p>
<p>�</p>
<p/>
</div>
<div class="page"><p/>
<p>15.6 &lowast;Rate of Convergence 483
</p>
<p>*15.6 &lowast;Rate of Convergence
</p>
<p>It is now appropriate to apply the principles of convergence analysis that have been
</p>
<p>repeatedly emphasized in previous chapters to the recursive quadratic programming
</p>
<p>approach. We expect that, if this new approach is well founded, then the rate of
</p>
<p>convergence of the algorithm should be related to the familiar canonical rate, which
</p>
<p>we have learned is a fundamental measure of the complexity of the problem. If it is
</p>
<p>not so related, then some modification of the algorithm is probably required. Indeed,
</p>
<p>we shall find that a small but important modification is required.
</p>
<p>From the proof of Proposition 2 of Sect. 15.5, we have the formula
</p>
<p>(B + cATA)d = &minus;&nabla;P(x)T ,
</p>
<p>which can be written as
</p>
<p>d = &minus;(B + cATA)&minus;1&nabla;P(x)T .
</p>
<p>This shows that the method is a modified Newton method applied to the uncon-
</p>
<p>strained minimization of P(x). From the Modified Newton Method Theorem of
</p>
<p>Sect. 10.1, we see immediately that the rate of convergence is determined by the
</p>
<p>eigenvalues of the matrix that is the product of the coefficient matrix (B + cATA)&minus;1
</p>
<p>and the Hessian of the function P at the solution point. The Hessian of P is
</p>
<p>(L + cATA), where L = F(x) + ch(x)TH(x). We know that the vector ch(x) at
</p>
<p>the solution of the penalty problem is equal to λc, where &nabla; f (x) + λ
T
c&nabla;h(x) = 0.
</p>
<p>Therefore, the rate of convergence is determined by the eigenvalues of
</p>
<p>(B + cATA)&minus;1(L + cATA), (15.47)
</p>
<p>where all quantities are evaluated at the solution to the penalty problem and L =
</p>
<p>F + λTc H. For large values of c, all quantities are approximately equal to the values
</p>
<p>at the optimal solution to the constrained problem.
</p>
<p>Now what we wish to show is that as c &rarr; &infin;, the matrix (15.47) looks like B&minus;1
M
</p>
<p>LM
on the subspace, M, and like the identity matrix on M&perp;, the subspace orthogonal to
M. To do this in detail, let C be an n &times; (n &minus; m) matrix whose columns form an
orthonormal basis for M, the tangent subspace {x : Ax = 0}. Let D = AT (AAT )&minus;1.
Then AC = 0, AD = I, CTC = I, CTD = 0.
</p>
<p>The eigenvalues of (B + cATA)&minus;1(L + cATA) are equal to those of
</p>
<p>[C, D]&minus;1(B + cATA)&minus;1{[C, D]T }&minus;1[C, D]T (L + cATA)[C, D]
</p>
<p>=
</p>
<p>[
</p>
<p>CTBC CTBD
</p>
<p>DTBC DTBC + cI
</p>
<p>]&minus;1 [
CTLC CTLD
</p>
<p>DTLC DTLD + cI
</p>
<p>]
</p>
<p>.
</p>
<p>Now as c &rarr; &infin;, the matrix above approaches
[
</p>
<p>B&minus;1
M
</p>
<p>LM BMC
T (L &minus; B)D
</p>
<p>0 I
</p>
<p>]
</p>
<p>,</p>
<p/>
</div>
<div class="page"><p/>
<p>484 15 Primal-Dual Methods
</p>
<p>where BM = C
TBC, LM = C
</p>
<p>TLC (see Exercise 6). The eigenvalues of this matrix
</p>
<p>are those of B&minus;1
M
</p>
<p>LM together with those of I. This analysis leads directly to the
</p>
<p>following conclusion:
</p>
<p>Theorem. Let a, A be the smallest and largest eigenvalues, respectively, of B&minus;1M LM and
assume that a � 1 � A. Then the structured modified Newton method with quadratic penalty
function has a rate of convergence no greater than [(A &minus; a)/(A + a)]2 as c &rarr; &infin;.
</p>
<p>In the special case of B = I, the rate in the above proposition is precisely the
</p>
<p>canonical rate, defined by the eigenvalues of L restricted to the tangent plane. It is
</p>
<p>important to note, however, that in order for the rate of the theorem to be achieved,
</p>
<p>the eigenvalues of B&minus;1
M
</p>
<p>LM must be spread around unity; if not, the rate will be poorer.
</p>
<p>Thus, even if LM is well-conditioned, but the eigenvalues differ greatly from unity,
</p>
<p>the choice B = I may be poor. This is an instance where proper scaling is vital.
</p>
<p>(We also point out that the above analysis is closely related to that of Sect. 13.4,
</p>
<p>where a similar conclusion is obtained.)
</p>
<p>There is a geometric explanation for the scaling property. Take B = I for sim-
</p>
<p>plicity. Then the direction of movement d is d = &minus;&nabla; f (x)T +ATλ for some λ. Using
the fact that the projected gradient is p = &nabla; f (x)T + AT&micro; for some &micro;, we see that
</p>
<p>d = &minus;p + AT (λ + &micro;). Thus d can be decomposed into two components: one in the
direction of the projected negative gradient, the other in a direction orthogonal to
</p>
<p>the tangent plane (see Fig. 15.1). Ideally, these two components should be in proper
</p>
<p>proportions so that the constraint surface is reached at the same point as would be
</p>
<p>reached by minimization in the direction of the projected negative gradient. If they
</p>
<p>are not, convergence will be poor.
</p>
<p>Fig. 15.1 Decomposition of the direction d</p>
<p/>
</div>
<div class="page"><p/>
<p>15.7 Primal-Dual Interior Point Methods 485
</p>
<p>15.7 Primal-Dual Interior Point Methods
</p>
<p>The primal-dual interior-point methods discussed for linear programming in Chap. 5
</p>
<p>are, as mentioned there, closely related to the barrier methods presented in Chap. 13
</p>
<p>and the primal-dual methods of the current chapter. They can be naturally extended
</p>
<p>to solve nonlinear programming problems while maintaining both theoretical and
</p>
<p>practical efficiency.
</p>
<p>Consider the inequality constrained problem
</p>
<p>minimize f (x)
</p>
<p>subject to Ax = b, (15.48)
</p>
<p>g(x) &le; 0,
</p>
<p>In general, a weakness of the active constraint method for such a problem is the
</p>
<p>combinatorial nature of determining which constraints should be active.
</p>
<p>Logarithmic Barrier Function
</p>
<p>A method that avoids the necessity to explicitly select a set of active constraints
</p>
<p>is based on the logarithmic barrier method, which solves a sequence of equality
</p>
<p>constrained minimization problems. Specifically,
</p>
<p>minimize f (x) &minus; &micro;
p
</p>
<p>&sum;
</p>
<p>i=1
</p>
<p>log(&minus;g j(x)) (15.49)
</p>
<p>subject to Ax = b,
</p>
<p>where μ = μk &gt; 0, k = 1, . . .,, μk &gt; μk+1, μk &rarr; 0. The μks can be pre-determined.
Typically, we have μk+1 = γμk for some constant 0 &lt; γ &lt; 1. Here, we also assume
</p>
<p>that the original problem has a feasible interior-point x0; that is,
</p>
<p>Ax0 = b and g(x0) &lt; 0,
</p>
<p>and A has full row rank.
</p>
<p>For fixed μ, and using si = μ/gi, the first-order optimality conditions of the
</p>
<p>barrier problem (15.49) are:
</p>
<p>&minus; Sg(x) = μ1
Ax = b (15.50)
</p>
<p>&minus;ATy + &nabla; f (x)T + &nabla;g(x)Ts = 0,
</p>
<p>where S = diag(s); that is, a diagonal matrix whose diagonal entries are s, and &nabla;g(x)
</p>
<p>is the Jacobian matrix of g(x).</p>
<p/>
</div>
<div class="page"><p/>
<p>486 15 Primal-Dual Methods
</p>
<p>If f (x) and gi(x) are convex functions for all i, f (x) &minus; μ
&sum;
</p>
<p>i log(&minus;gi(x)) is strictly
convex in the interior of the feasible region, and the objective level set is bounded,
</p>
<p>then there is a unique minimizer for the barrier problem. Let (x(μ) &gt; 0, y(μ),
</p>
<p>s(μ) &gt; 0) be the (unique) solution of (15.50). Then, these values form the primal-
</p>
<p>dual central path of (15.48):
</p>
<p>C = {(x(μ), y(μ), s(μ) &gt; 0) : 0 &lt; μ &lt; &infin;}.
This can be summarized in the following theorem.
</p>
<p>Theorem 1. Let (x(μ), y(μ), s(μ)) be on the central path.
</p>
<p>i) If f (x) and gi(x) are convex functions for all i, then s(μ) is unique.
</p>
<p>ii) Furthermore, if f (x)&minus; μ&sum;i log(&minus;gi(x)) is strictly convex,(x(μ), y(μ), s(μ)) are unique,
and they are bounded for 0 &lt; μ � μ0 for any given μ0 &gt; 0.
</p>
<p>iii) For 0 &lt; μ&prime; &lt; μ, f (x(μ&prime;)) &lt; f (x(μ)) if x(μ&prime;) � x(μ).
</p>
<p>iv) (x(μ), y(μ), s(μ)) converges to a point satisfying the first-order necessary conditions
for a solution of (15.48) as μ &rarr; 0.
</p>
<p>Once we have an approximate solution point (x, y, s) = (xk, yk, sk) for (15.50)
</p>
<p>for μ = μk &gt; 0, we can again use the primal-dual methods described for linear
</p>
<p>programming to generate a new approximate solution to (15.50) for μ = μk+1 &lt;
</p>
<p>μk. The Newton direction vectors (dx, dy, ds) is found from the system of linear
</p>
<p>equations:
</p>
<p>&minus;S&nabla;g(x)dx &minus; G(x)ds = μ1 + Sg(x), (15.51)
Adx = b &minus; Ax,
</p>
<p>&minus;ATdy +
⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>&nabla;
2 f (x) +
</p>
<p>&sum;
</p>
<p>i
</p>
<p>si&nabla;
2gi(x)
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>dx
</p>
<p>+&nabla;g(x)Tds = A
Ty &minus; &nabla; f (x)T &minus; &nabla;g(x)T s,
</p>
<p>where G(x) = diag(g(x)). Then, the new iterate is update to:
</p>
<p>(xk+1, yk+1, sk+1) = (xk, yk, sk) + αk(dx, dy, ds)
</p>
<p>for a stepsize αk. Recently, this approach has also been used to find points satisfying
</p>
<p>the first-order conditions for problems when f (x) and gi(x) are not generally convex
</p>
<p>functions.
</p>
<p>Interior Point Method for Convex Quadratic Programming
</p>
<p>Let f (x) = (1/2)xTQx + cTx and gi(x) = &minus;xi for i = 1, . . . , n, and consider the
quadratic program
</p>
<p>minimize
1
</p>
<p>2
xTQx + cTx
</p>
<p>subject to Ax = b, (15.52)
</p>
<p>x � 0,</p>
<p/>
</div>
<div class="page"><p/>
<p>15.7 Primal-Dual Interior Point Methods 487
</p>
<p>where the given matrix Q &isin; En&times;n is positive semidefinite (that is, the objective is a
convex function), A &isin; En&times;m, c &isin; En and b &isin; Em. The problem reduces to finding
x &isin; En, y &isin; Em and s &isin; En satisfying the following optimality conditions:
</p>
<p>Sx = 0
</p>
<p>Ax = b (15.53)
</p>
<p>&minus;ATy + Qx &minus; s = &minus;c
(x, s) &ge; 0.
</p>
<p>The optimality conditions with the logarithmic barrier function with parameter μ
</p>
<p>are be:
</p>
<p>Sx = μ1
</p>
<p>Ax = b (15.54)
</p>
<p>&minus;ATy + Qx &minus; s = &minus;c.
Note that the bottom two sets of constraints are linear equalities.
</p>
<p>Thus, once we have an interior feasible point (x, y, s) for (15.54), with &micro; =
</p>
<p>xT s/n, we can apply Newton&rsquo;s method to compute a new (approximate) iterate
</p>
<p>(x+, y+, s+) by solving for (dx, dy, ds) from the system of linear equations:
</p>
<p>Sdx + Xds = γμ1 &minus; Xs,
Adx = 0, (15.55)
</p>
<p>&minus;ATdy + Qdx &minus; ds = 0,
</p>
<p>where X and S are two diagonal matrices whose diagonal entries are x &gt; 0 and
</p>
<p>s &gt; 0, respectively. Here, γ is a fixed positive constant less than 1, which implies
</p>
<p>that our targeted μ is reduced by the factor γ at each step.
</p>
<p>Potential Function as a Merit Function
</p>
<p>For any interior feasible point (x, y, s) of (15.52) and its dual, a suitable merit
</p>
<p>function is the potential function introduced in Chap. 5 for linear programming:
</p>
<p>ψn+ρ(x, s) = (n + ρ) log(x
T s) &minus;
</p>
<p>n
&sum;
</p>
<p>j=1
</p>
<p>log(x js j).
</p>
<p>The main result for this is stated in the following theorem.
</p>
<p>Theorem 2. In solving (15.55) for (dx, dy, ds), let γ = n/(n + ρ) &lt; 1 for fixed ρ �
&radic;
n and
</p>
<p>assign x+ = x + αdx, y
+ = y + αdy, and s
</p>
<p>+ = s + αds where
</p>
<p>α =
α
&radic;
</p>
<p>min(Xs)
</p>
<p>|(XS)&minus;1/2( xT s
n+ρ
</p>
<p>1 &minus; Xs)|
,</p>
<p/>
</div>
<div class="page"><p/>
<p>488 15 Primal-Dual Methods
</p>
<p>where α is any positive constant less than 1. (Again X and S are matrices with components
</p>
<p>on the diagonal being those of x and s, respectively.) Then,
</p>
<p>ψn+ρ(x
+, s+) &minus; ψn+ρ(x, s) � &minus;α
</p>
<p>&radic;
</p>
<p>3/4 +
α
</p>
<p>2
</p>
<p>2(1 &minus; α) .
</p>
<p>The proof of the theorem is also similar to that for linear programming; see
</p>
<p>Exercise 12. Notice that, since Q is positive semidefinite, we have
</p>
<p>dx
Tds = (dx, dy)
</p>
<p>T (ds, 0) = d
T
x Qdx � 0
</p>
<p>while dTx ds = 0 in the linear programming case.
</p>
<p>We outline the algorithm here:
</p>
<p>Given any interior feasible (x0, y0, s0) of (15.52) and its dual. Set ρ �
&radic;
n and
</p>
<p>k = 0.
</p>
<p>1. Set (x, s) = (xk, sk) and γ = n/(n + ρ) and compute (dx, dy, ds) from (15.55).
</p>
<p>2. Let xk+1 = xk + αdx, yk+1 = yk + αdy, and sk+1 = sk + αds where
</p>
<p>α = arg min
α�0
</p>
<p>ψn+ρ(xk + αdx, sk + αds).
</p>
<p>3. Let k = k + 1. If sT
k
</p>
<p>xk/s
T
0
</p>
<p>x0 &le; ε, stop. Otherwise, return to Step 1.
This algorithm exhibits an iteration complexity bound that is identical to that of
</p>
<p>linear programming expressed in Theorem 1, Sect. 5.6.
</p>
<p>15.8 Summary
</p>
<p>A constrained optimization problem can be solved by directly solving the equations
</p>
<p>that represent the first-order necessary conditions for a solution. For a quadratic pro-
</p>
<p>gramming problem with linear constraints, these equations are linear and thus can
</p>
<p>be solved by standard linear procedures. Quadratic programs with inequality con-
</p>
<p>straints can be solved by an active set method in which the direction of movement is
</p>
<p>toward the solution of the corresponding equality constrained problem. This method
</p>
<p>will solve a quadratic program in a finite number of steps.
</p>
<p>For general nonlinear programming problems, many of the standard methods
</p>
<p>for solving systems of equations can be adapted to the corresponding necessary
</p>
<p>equations. One class consists of first-order methods that move in a direction related
</p>
<p>to the residual (that is, the error) in the equations. Another class of methods is based
</p>
<p>on extending the method of conjugate directions to nonpositive-definite systems.
</p>
<p>Finally, a third class is based on Newton&rsquo;s method for solving systems of nonlinear
</p>
<p>equations, and solving a linearized version of the system at each iteration. Under
</p>
<p>appropriate assumptions, Newton&rsquo;s method has excellent global as well as local con-
</p>
<p>vergence properties, since the simple merit function, 1
2
|&nabla; f (x)+λT&nabla;h(x)|2+ 1
</p>
<p>2
|h(x)|2,
</p>
<p>decreases in the Newton direction. An individual step of Newton&rsquo;s method is</p>
<p/>
</div>
<div class="page"><p/>
<p>15.9 Exercises 489
</p>
<p>equivalent to solving a quadratic programming problem, and thus Newton&rsquo;s method
</p>
<p>can be extended to problems with inequality constraints through recursive quadratic
</p>
<p>programming.
</p>
<p>More effective methods are developed by accounting for the special structure
</p>
<p>of the linearized version of the necessary conditions and by introducing approxi-
</p>
<p>mations to the second-order information. In order to assure global convergence of
</p>
<p>these methods, a penalty (or merit) function must be specified that is compatible
</p>
<p>with the method of direction selection, in the sense that the direction is a direction
</p>
<p>of descent for the merit function. The absolute-value penalty function and the stan-
</p>
<p>dard quadratic penalty function are both compatible with some versions of recursive
</p>
<p>quadratic programming.
</p>
<p>The best of the primal-dual methods take full account of special structure, and are
</p>
<p>based on direction-finding procedures that are closely related to methods described
</p>
<p>in earlier chapters. It is not surprising therefore that the convergence properties of
</p>
<p>these methods are also closely related to those of other chapters. Again we find that
</p>
<p>the canonical rate is fundamental for properly designed first-order methods.
</p>
<p>Interior point methods in the primal-dual model are very effective for treating
</p>
<p>problems with inequality constraints, for they avoid (or at least minimize) the diffi-
</p>
<p>culties associated with determining which constraints will be active at the solution.
</p>
<p>Applied to general nonlinear programming problems, these methods closely parallel
</p>
<p>the interior point methods for linear programming. There is again a central path, and
</p>
<p>Newton&rsquo;s method is a good way to follow the path.
</p>
<p>15.9 Exercises
</p>
<p>1. Solve the quadratic program
</p>
<p>minimize x2 &minus; xy + y2 &minus; 3x
subject to x � 0
</p>
<p>y � 0
</p>
<p>x + y � 4
</p>
<p>by use of the active set method starting at x = y = 0.
</p>
<p>2. Suppose x&lowast;, λ&lowast; satisfy
</p>
<p>&nabla; f (x&lowast;) + λ&lowast;T&nabla;h(x&lowast;) = 0
</p>
<p>h(x&lowast;) = 0.
</p>
<p>Let
</p>
<p>C =
</p>
<p>[
</p>
<p>L(x&lowast;,λ&lowast;) &nabla;h(x&lowast;)T
</p>
<p>&nabla;h(x&lowast;) 0
</p>
<p>]
</p>
<p>.
</p>
<p>Assume that L(x&lowast;, λ&lowast;) is positive definite and that &nabla;h(x&lowast;) is of full rank.</p>
<p/>
</div>
<div class="page"><p/>
<p>490 15 Primal-Dual Methods
</p>
<p>(a) Show that the real part of each eigenvalue of C is positive.
</p>
<p>(b) Using the result of Part (a), show that for some α &gt; 0 the iterative process
</p>
<p>xk+1 = xk &minus; α&nabla;l(xk, λk)T and λk+1 = λk + αh(xk)
</p>
<p>converges locally to x&lowast;, λ&lowast;. (That is, if started sufficiently close to x&lowast;, λ&lowast;,
the process converges to x&lowast;, λ&lowast;.) Hint: Use Ostroski&rsquo;s Theorem: Let A(z) be
a continuously differentiable mapping from Ep to Ep, assume A(z&lowast;) = 0,
and let &nabla;A(z&lowast;) have all eigenvalues strictly inside the unit circle of the
complex plane. Then zk+1 = zk + A(zk) converges locally to z
</p>
<p>&lowast;.
</p>
<p>3. Let A be a real symmetric matrix. A vector x is singular if xT Ax = 0. A pair
</p>
<p>of vectors x, y is a hyperbolic pair if both x and y are singular and xTAy � 0.
</p>
<p>Hyperbolic pairs can be used to generalize the conjugate gradient method to the
</p>
<p>nonpositive definite case.
</p>
<p>(a) If pk is singular, show that if pk+1 is defined as
</p>
<p>pk+1 = Apk &minus;
(Apk)
</p>
<p>TA2pk
</p>
<p>2|Apk|2
pk,
</p>
<p>then pk, pk+1 is a hyperbolic pair.
</p>
<p>(b) Consider a modification of the conjugate gradient process of Sect. 8.1,
</p>
<p>where if pk is singular, pk+1 is generated as above, and then
</p>
<p>xk+1 = xk + αkpk
</p>
<p>xk+2 = xk+1 + αk+1pk+1
</p>
<p>αk =
rT
k
</p>
<p>pk+1
</p>
<p>pT
k
</p>
<p>Apk+1
, αk+1 =
</p>
<p>rT
k
</p>
<p>pk
</p>
<p>pT
k
</p>
<p>Apk+1
</p>
<p>pk+2 = rk+2 &minus;
rT
k+2
</p>
<p>Apk+1
</p>
<p>pkApk+1
pk.
</p>
<p>Show that if pk+1 is the second member of a hyperbolic pair and rk � 0,
</p>
<p>then xk+2 � xk+1, which means the process does not get &ldquo;stuck.&rdquo;
</p>
<p>4. Another method for solving a system Ax = b when A is nonsingular and sym-
</p>
<p>metric is the conjugate residual method. In this method the direction vectors are
</p>
<p>constructed to be an A2-orthogonalized version of the residuals rk = b &minus; Axk.
The error function E(x) = |Ax &minus; b|2 decreases monotonically in this process.
Since the directions are based on rk rather than the gradient of E, which is
</p>
<p>2Ark, the method extends the simplicity of the conjugate gradient method by
</p>
<p>implicit use of the fact that A2 is positive definite. The method is this: Set
</p>
<p>p1 = r1 = b&minus;Ax1 and repeat the following steps, omitting (a, b) on the first step.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.9 Exercises 491
</p>
<p>If αk&minus;1 � 0,
</p>
<p>pk = rk &minus; βkpk&minus;1, βk =
rT
k
</p>
<p>A2pk&minus;1
</p>
<p>pT
k&minus;1A
</p>
<p>2pk&minus;1
. (15.56a)
</p>
<p>If αk&minus;1 = 0,
</p>
<p>pk = Ark &minus; γkpk&minus;1 &minus; δkpk&minus;2
</p>
<p>γk =
rT
k
</p>
<p>A3pk&minus;1
</p>
<p>pT
k&minus;1A
</p>
<p>2pk&minus;1
, δk =
</p>
<p>rT
k
</p>
<p>A3pk&minus;2
</p>
<p>pT
k&minus;2A
</p>
<p>3pk&minus;2
(15.56b)
</p>
<p>xk+1 = xk + αkpk, αk =
rT
k
</p>
<p>Apk
</p>
<p>pT
k
</p>
<p>A2pk
(15.56c)
</p>
<p>rk+1 = b &minus; Axk+1. (15.56d)
</p>
<p>Show that the directions pk are A
2-orthogonal.
</p>
<p>5. Consider the (n + m)-dimensional system of equations
</p>
<p>[
</p>
<p>L AT
</p>
<p>A 0
</p>
<p>] [
</p>
<p>x
</p>
<p>λ
</p>
<p>]
</p>
<p>=
</p>
<p>[
</p>
<p>a
</p>
<p>b
</p>
<p>]
</p>
<p>.
</p>
<p>Suppose that A = [B, C], where B is m &times; m and invertible. Let x = (xB, xc),
where xB is the first m components of x. The system can then be written
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>LBB LBC B
T
</p>
<p>LCB Lcc C
T
</p>
<p>B C 0
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>xB
xC
λ
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>=
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>aB
aC
b
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>(a) Assume that L is positive definite on the tangent space {x : Ax = 0}. Derive
an explicit statement equivalent to this assumption in terms of the positive
</p>
<p>definiteness of some (n &minus; m) &times; (n &minus; m) matrix.
(b) Solve the system in terms of the submatrices of the partitioned form.
</p>
<p>6. Consider the partitioned square matrix M of the form
</p>
<p>M =
</p>
<p>[
</p>
<p>A B
</p>
<p>C D
</p>
<p>]
</p>
<p>.
</p>
<p>Show that
</p>
<p>M&minus;1 =
</p>
<p>[
</p>
<p>Q &minus;QBD&minus;1
&minus;D&minus;1CQ D&minus;1 + D&minus;1CQBD&minus;1
</p>
<p>]
</p>
<p>,
</p>
<p>where Q = (A &minus; BD&minus;1C)&minus;1, provided that all indicated inverses exist. Use this
result to verify the rate of convergence result in Sect. 15.6.
</p>
<p>7. For the problem
</p>
<p>minimize f (x)
</p>
<p>subject to g(x) � 0,</p>
<p/>
</div>
<div class="page"><p/>
<p>492 15 Primal-Dual Methods
</p>
<p>where g(x) is r-dimensional, define the penalty function
</p>
<p>p(x) = f (x) + c max{0, g1(x), g2(x), . . . , gr(x)}.
</p>
<p>Let d, (d � 0) be a solution to the quadratic program
</p>
<p>minimize
1
</p>
<p>2
dTBd + &nabla; f (x)d
</p>
<p>subject to g(x) + &nabla;g(x)d � 0,
</p>
<p>where B is positive definite. Show that d is a descent direction for p for suffi-
</p>
<p>ciently large c.
</p>
<p>8. Suppose the quadratic program of Exercise 7 is not feasible. In that case one
</p>
<p>may solve
</p>
<p>minimize
1
</p>
<p>2
dTBd + &nabla; f (x)d + cξ
</p>
<p>subject to g(x) + &nabla;g(x)d � ξ1
</p>
<p>ξ � 0.
</p>
<p>(a) Show that if d � 0 is a solution, then d is a descent direction for p.
</p>
<p>(b) If d = 0 is a solution, show that x is a critical point of p in the sense that for
</p>
<p>any d � 0, p(x + αd) &gt; p(x) + o(α).
</p>
<p>9. For the equality constrained problem, consider the function
</p>
<p>φ(x) = f (x) + λ(x)Th(x) + ch(x)TC(x)C(x)Th(x),
</p>
<p>where
</p>
<p>C(x) = [&nabla;h(x)&nabla;h(x)T ]&minus;1&nabla;h(x) and λ(x) = C(x)&nabla; f (x)T .
</p>
<p>(a) Under standard assumptions on the original problem, show that for suffi-
</p>
<p>ciently large c, φ is (locally) an exact penalty function.
</p>
<p>(b) Show that φ(x) can be expressed as
</p>
<p>φ(x) = f(x) + π(x)Th(x),
</p>
<p>where π(x) is the Lagrange multiplier of the problem
</p>
<p>minimize
1
</p>
<p>2
cdTd + &nabla; f (x)d
</p>
<p>subject to &nabla;h(x)d + h(x) = 0.
</p>
<p>(c) Indicate how φ can be defined for problems with inequality constraints.
</p>
<p>10. Let {Bk} be a sequence of positive definite symmetric matrices, and assume
that there are constants a &gt; 0, b &gt; 0 such that a|x|2 � xTBkx � b|x|2 for
all x. Suppose that B is replaced by Bk in the kth step of the recursive quadratic</p>
<p/>
</div>
<div class="page"><p/>
<p>References 493
</p>
<p>programming procedure of the theorem in Sect. 15.4. Show that the conclusions
</p>
<p>of that theorem are still valid. Hint: Note that the set of allowable Bk&rsquo;s is closed.
</p>
<p>11. (Central path theorem) Prove the central path theorem, Theorem 1 of Sect. 15.7,
</p>
<p>for convex optimization.
</p>
<p>12. Prove the potential reduction theorem, Theorem 2 of Sect. 15.7, for convex
</p>
<p>quadratic programming. This theorem can be generalized to non-quadratic con-
</p>
<p>vex objective functions f (x) satisfying the following condition: let
</p>
<p>u : (0, 1) &rarr; (1, &infin;)
</p>
<p>be a monotone increasing function; then
</p>
<p>|X(&nabla; f (x + dx) &minus; &nabla; f (x) &minus; &nabla;2 f (x)dx)|1 &le; u(α)dTx&nabla; f (x)dx
</p>
<p>whenever
</p>
<p>x &gt; 0, |X&minus;1dx|&infin; &le; α &lt; 1.
Such condition is called the scaled Lipschitz condition in {x : x &gt; 0}.
</p>
<p>References
</p>
<p>15.1 An early method for solving quadratic programming problems is the
</p>
<p>principal pivoting method of Dantzig and Wolfe; see Dantzig [D6]. For
</p>
<p>a discussion of factorization methods applied to quadratic programming,
</p>
<p>see Gill, Murray, and Wright [G7].
</p>
<p>15.2&ndash;15.4 Arrow and Hurwicz [A9] proposed a continuous process (represented
</p>
<p>as a system of differential equations) for solving the Lagrange equa-
</p>
<p>tions. This early paper showed the value of the simple merit function
</p>
<p>in attacking the equations. A formal discussion of the properties of the
</p>
<p>simple merit function may be found in Luenberger [L17]. The first-order
</p>
<p>method was examined in detail by Polak [P4]. Also see Zangwill [Z2]
</p>
<p>for an early analysis of a method for inequality constraints. The conju-
</p>
<p>gate direction method was first extended to nonpositive definite cases by
</p>
<p>the use of hyperbolic pairs and then by employing conjugate residuals.
</p>
<p>(See Exercises 3 and 4, and Luenberger [L9, L11].) Additional meth-
</p>
<p>ods with somewhat better numerical properties were later developed by
</p>
<p>Paige and Saunders [P1] and by Fletcher [F8]. It is perhaps surprising
</p>
<p>that Newton&rsquo;s method was analyzed in this form only recently, well after
</p>
<p>the development of the SOLVER method discussed in Sect. 15.2. For
</p>
<p>a comprehensive account of Newton methods, see Bertsekas, Chap. 4
</p>
<p>[B11]. The SOLVER method was proposed by Wilson [W2] for convex
</p>
<p>programming problems and was later interpreted by Beale [B7]. Garcia-
</p>
<p>Palomares and Mangasarian [G3] proposed a quadratic programming
</p>
<p>approach to the solution of the first-order equations. See Fletcher [F10]
</p>
<p>for a good overview discussion.</p>
<p/>
</div>
<div class="page"><p/>
<p>494 15 Primal-Dual Methods
</p>
<p>15.5&ndash;15.6 The discovery that the absolute-value penalty function is compatible
</p>
<p>with recursive quadratic programming was made by Pshenichny (see
</p>
<p>Pshenichny and Danilin [P10]) and later by Han [H3], who also suggested
</p>
<p>that the method be combined with a quasi-Newton update procedure.
</p>
<p>The development of recursive quadratic programming for the standard
</p>
<p>quadratic penalty function is due to Biggs [B14, B15]. The convergence
</p>
<p>rate analysis of Sect. 15.6 first appeared in the second edition of this text.
</p>
<p>15.7 Many researchers have applied interior-point algorithms to convex
</p>
<p>quadratic problems. These algorithms can be divided into three groups:
</p>
<p>the primal algorithm, the dual algorithm, and the primal-dual algorithm.
</p>
<p>Relations among these algorithms can be seen in den Hertog [H6],
</p>
<p>Anstreicher et al [A6], Sun and Qi [S12], Tseng [T12], and Ye [Y3].
</p>
<p>For results similar to those of Exercises 2,7, and 8, see Bertsekas [B11].
</p>
<p>For discussion of Exercise 9, see Fletcher [F10].</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix A
</p>
<p>Mathematical Review
</p>
<p>The purpose of this appendix is to set down for reference and review some basic
</p>
<p>definitions, notation, and relations that are used frequently in the text.
</p>
<p>A.1 Sets
</p>
<p>If x is a member of the set S , we write x &isin; S . We write y � S if y is not a member
of S .
</p>
<p>A set S may be specified by listing its elements between braces; such as, for
</p>
<p>example, S = {1, 2, 3, 4}. Alternatively, a set can be specified in the form S = {x :
P(x)} as the set of elements satisfying property P; such as S = {x : 1 � x �
4, x integer}
</p>
<p>The union of two sets S and T is denoted S &cup; T and is the set consisting of
the elements that belong to either S or T . The intersection of two sets S and T is
</p>
<p>denoted S &cap; T and is the set consisting of the elements that belong to both S and T .
If S is a subset of T , that is, if every member of S is also a member of T , we write
</p>
<p>S &sub; T or T &sup; S .
The empty set is denoted φ or &empty;. There are two ways that operations such as
</p>
<p>minimization over a set are represented. Specifically we write either
</p>
<p>min
x&isin;S
</p>
<p>f (x) or min{ f (x) : x &isin; S }
</p>
<p>to denote the minimum value of f over the set S . The set of x&rsquo;s in S that achieve the
</p>
<p>minimum is denoted argmin { f (x) : x &isin; S }.
</p>
<p>&copy; Springer International Publishing Switzerland 2016
</p>
<p>D.G. Luenberger, Y. Ye, Linear and Nonlinear Programming, International
Series in Operations Research &amp; Management Science 228,
DOI 10.1007/978-3-319-18842-3
</p>
<p>495</p>
<p/>
</div>
<div class="page"><p/>
<p>496 A Mathematical Review
</p>
<p>Sets of Real Numbers
</p>
<p>If a and b are real numbers, [a, b] denotes the set of real numbers x satisfying
</p>
<p>a � x � b. A rounded, instead of square, bracket denotes strict inequality in the
</p>
<p>definition. Thus (a, b] denotes all x satisfying a &lt; x � b.
</p>
<p>If S is a set of real numbers bounded above, then there is a smallest real number
</p>
<p>y such that x � y for all x &isin; S . The number y is called the least upper bound or
supremum of S and is denoted
</p>
<p>sup
x&isin;S
</p>
<p>(x) or sup{x : x &isin; S }.
</p>
<p>Similarly, the greatest lower bound or infimum of a set S is denoted
</p>
<p>inf
x&isin;S
</p>
<p>(x) or inf{x : x &isin; S }.
</p>
<p>A.2 Matrix Notation
</p>
<p>A matrix is a rectangular array of numbers, called elements. The matrix itself is
</p>
<p>denoted by a boldface letter. When specific numbers are not used, the elements are
</p>
<p>denoted by italicized lower-case letters, having a double subscript. Thus we write
</p>
<p>A =
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>a11 a12 &middot; &middot; &middot; a1n
a21 a22 &middot; &middot; &middot; a2n
...
</p>
<p>am1 am2 &middot; &middot; &middot; amn
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>for a matrix A having m rows and n columns. Such a matrix is referred to as an
</p>
<p>m &times; n matrix. If we wish to specify a matrix by defining a general element, we use
the notation A = [ai j].
</p>
<p>An m &times; n matrix all of whose elements are zero is called a zero matrix and
denoted 0. A square matrix (a matrix with m = n) whose elements are ai j = 0 for
</p>
<p>i � j, and aii = 1 for i = 1, 2, . . . , n is said to be an identity matrix and denoted I.
</p>
<p>The sum of two m&times; n matrices A and B is written A+B and is the matrix whose
elements are the sum of the corresponding elements in A and B. The product of a
</p>
<p>matrix A and a scalar λ, written λA or Aλ, is obtained by multiplying each element
</p>
<p>of A by λ. The product AB of an m &times; n matrix A and an n&times; p matrix B is the m &times; p
matrix C with elements ci j =
</p>
<p>&sum;n
k=1 aikbk j.
</p>
<p>The transpose of an m&times;n matrix A is the n&times;m matrix AT with elements aT
i j
= a ji.
</p>
<p>A (square) matrix A is symmetric if AT = A. A square matrix A is nonsingular if
</p>
<p>there is a matrix A&minus;1, called the inverse of A, such that A&minus;1A = I = AA&minus;1. The
determinant of a square matrix A is denoted by det (A). The determinant is nonzero
</p>
<p>if and only if the matrix is nonsingular. Two square n &times; n matrices A and B are
similar if there is a nonsingular matrix S such that B = S&minus;1AS.</p>
<p/>
</div>
<div class="page"><p/>
<p>A.3 Spaces 497
</p>
<p>Matrices having a single row are referred to as row vectors; matrices having a
</p>
<p>single column are referred to as column vectors. Vectors of either type are usually
</p>
<p>denoted by lower-case boldface letters. To economize page space, row vectors are
</p>
<p>written a = [a1, a2, . . . , an] and column vectors are written a = (a1, a2, . . . , an).
</p>
<p>Since column vectors are used frequently, this notation avoids the necessity to dis-
</p>
<p>play numerous columns. To further distinguish rows from columns, we write a &isin; En
if a is a column vector with n components, and we write b &isin; En if b is a row vector
with n components.
</p>
<p>It is often convenient to partition a matrix into submatrices. This is indicated by
</p>
<p>drawing partitioning lines through the matrix, as for example,
</p>
<p>A =
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>a11 a12 a13 a14
a21 a22 a23 a24
a31 a32 a33 a34
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>=
</p>
<p>[
</p>
<p>A11 A12
A21 A22
</p>
<p>]
</p>
<p>.
</p>
<p>The resulting submatrices are usually denoted Ai j, as illustrated.
</p>
<p>A matrix can be partitioned into either column or row vectors, in which case
</p>
<p>a special notation is convenient. Denoting the columns of an m &times; n matrix A by
a j, j = 1, 2, . . . , n, we write A = [a1, a2, . . . , an]. Similarly, denoting the rows
</p>
<p>of A by ai, i = 1, 2, . . . , m, we write A = (a1, a2, . . . , am). Following the same
</p>
<p>pattern, we often write A = [B, C] for the partitioned matrix A = [B|C].
</p>
<p>A.3 Spaces
</p>
<p>We consider the n-component vectors x = (x1, x2, . . . , xn) as elements of a vector
</p>
<p>space. The space itself, n-dimensional Euclidean space, is denoted En. Vectors in the
</p>
<p>space can be added or multiplied by a scalar, by performing the corresponding op-
</p>
<p>erations on the components. We write x � 0 if each component of x is nonnegative.
</p>
<p>The line segment connecting two vectors x and y is denoted [x, y] and consists of
</p>
<p>all vectors of the form αx + (1 &minus; α)y with 0 � α � 1.
The scalar product of two vectors x = (x1, x2, . . . , xn) and y = (y1, y2, . . . , yn)
</p>
<p>is defined as xTy = yTx =
&sum;n
</p>
<p>i=1 xiyi. The vectors x and y are said to be orthogonal if
</p>
<p>xTy = 0. The magnitude or norm of a vector x is |x| = (xTx)1/2. For any two vectors
x and y in En, the Cauchy-Schwarz Inequality holds: |xTy| � |x| &middot; |y|.
</p>
<p>A set of vectors a1, a2, . . . , ak is said to be linearly dependent if there are
</p>
<p>scalars λ1, λ2, . . . , λk, not all zero, such that
&sum;k
</p>
<p>i=1 λiai = 0. If no such set of scalars
</p>
<p>exists, the vectors are said to be linearly independent. A linear combination of the
</p>
<p>vectors a1, a2, . . . , ak is a vector of the form
&sum;k
</p>
<p>i=1 λ jai. The set of vectors that are
</p>
<p>linear combinations of a1, a2, . . . , ak is the set spanned by the vectors. A linearly
</p>
<p>independent set of vectors that span En is said to be a basis for En. Every basis for
</p>
<p>En contains exactly n vectors.</p>
<p/>
</div>
<div class="page"><p/>
<p>498 A Mathematical Review
</p>
<p>The rank of a matrix A is equal to the maximum number of linearly independent
</p>
<p>columns in A. This number is also equal to the maximum number of linearly inde-
</p>
<p>pendent rows in A. The m &times; n matrix A is said to be of full rank if the rank of A is
equal to the minimum of m and n.
</p>
<p>A subspace M of En is a subset that is closed under the operations of vector
</p>
<p>addition and scalar multiplication; that is, if a and b are vectors in M, then λa + μb
</p>
<p>is also in M for every pair of scalars λ, μ. The dimension of a subspace M is equal
</p>
<p>to the maximum number of linearly independent vectors in M. If M is a subspace of
</p>
<p>En, the orthogonal complement of M, denoted M&perp;, consists of all vectors that are
orthogonal to every vector in M. The orthogonal complement of M is easily seen to
</p>
<p>be a subspace, and together M and M&perp; span En in the sense that every vector x &isin; En
can be written uniquely in the form x = a + b with a &isin; M, b &isin; M&perp;. In this case a
and b are said to be the orthogonal projections of x onto the subspaces M and M&perp;,
respectively.
</p>
<p>A correspondence A that associates with each point in a space X a point in a space
</p>
<p>Y is said to be a mapping from X to Y. For convenience this situation is symbolized
</p>
<p>by A : X &rarr; Y. The mapping A may be either linear or nonlinear. The norm of linear
mapping A is defined as |A| = max
</p>
<p>|x|&le;1
|Ax|. It follows that for any x, |Ax| &le; |A| &middot; |x|.
</p>
<p>A.4 Eigenvalues and Quadratic Forms
</p>
<p>Corresponding to an n&times;n square matrix A, a scalar λ and a nonzero vector x satisfy-
ing the equation Ax = λx are said to be, respectively, an eigenvalue and eigenvector
</p>
<p>of A. In order that λ be an eigenvalue it is clear that it is necessary and sufficient for
</p>
<p>A &minus; λI to be singular, and hence det(A &minus; λI) = 0. This last result, when expanded,
yields an nth-order polynomial equation which can be solved for n (possibly nondis-
</p>
<p>tinct) complex roots λ which are the eigenvalues of A.
</p>
<p>Now, for the remainder of this section, assume that A is symmetric. Then the
</p>
<p>following properties hold:
</p>
<p>(i) The eigenvalues of A are real.
</p>
<p>(ii) Eigenvectors associated with distinct eigenvalues are orthogonal.
</p>
<p>(iii) There is an orthogonal basis for En, each element of which is an eigenvector
</p>
<p>of A.
</p>
<p>If the basis u1, u2, . . . , un in (iii) is normalized so that each element has magnitude
</p>
<p>unity, then defining the matrix Q = [u1, u2, . . . , un] we note that Q
TQ = I and
</p>
<p>hence QT = Q&minus;1. A matrix with this property is said to be an orthogonal matrix.
Also, we observe, in this case, that
</p>
<p>Q&minus;1AQ = QTAQ = QT [Au1, Au2, . . . , Aun]
</p>
<p>= QT [λ1u1, λ2u2, . . . , λnun].</p>
<p/>
</div>
<div class="page"><p/>
<p>A.5 Topological Concepts 499
</p>
<p>Thus
</p>
<p>Q&minus;1AQ =
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>λ1
λ2
</p>
<p>. . .
</p>
<p>λn
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>,
</p>
<p>and therefore A is similar to a diagonal matrix.
</p>
<p>A symmetric matrix A is said to be positive definite if the quadratic form xTAx is
</p>
<p>positive for all nonzero vectors x. Similarly, we define A to be positive semidefinite,
</p>
<p>negative definite, or negative semidefinite if xTAx � 0, &lt; 0, or � 0 for all x. The
</p>
<p>matrix A is indefinite if xTAx is positive for some x and negative for others.
</p>
<p>It is easy to obtain a connection between definiteness and the eigenvalues of A.
</p>
<p>For any x let y = Q&minus;1x where Q is defined as above. Then xTAx = yTQTAQy =
&sum;n
</p>
<p>i=1 λiy
2
i
. Since the yi&rsquo;s are arbitrary (since x is), it is clear that A is positive def-
</p>
<p>inite (or positive semidefinite) if and only if all eigenvalues of A are positive (or
</p>
<p>nonnegative).
</p>
<p>Through diagonalization we can also easily show that a positive semidefinite
</p>
<p>matrix A has a positive semidefinite (symmetric) square root A1/2 satisfying A1/2 &middot;
A1/2 = A. For this we use Q as above and define
</p>
<p>A1/2 = Q
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>λ
1/2
1
</p>
<p>λ
1/2
2
</p>
<p>. . .
</p>
<p>λ
1/2
n
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>QT ,
</p>
<p>which is easily verified to have the desired properties.
</p>
<p>A.5 Topological Concepts
</p>
<p>A sequence of vectors x0, x1, . . . , xk, . . ., denoted by {xk=0}&infin;k , or if the index set
is understood, by simply {xk}, is said to converge to the limit x if |xk &minus; x| &rarr; 0 as
k &rarr; &infin; (that is, if given ε &gt; 0, there is a N such that k � N implies |xk &minus; x| &lt; ε). If
{xk} converges to x, we write xk &rarr; x or lim xk = x.
</p>
<p>A point x is a limit point of the sequence {xk} if there is a subsequence of {xk}
convergent to x. Thus x is a limit point of {xk} if there is a subset K of the positive
integers such that {xk}k&isin;K converges to x.
</p>
<p>A ball (sphere) around x is a set of the form {y : |y &minus; x| &lt; (=) ε} for some ε &gt; 0.
Such a ball is also referred to as the neighborhood of x of radius ε.
</p>
<p>A subset S of En is open if around every point in S there is a sphere that is
</p>
<p>contained in S . Equivalently, S is open if given x &isin; S there is an ε &gt; 0 such that
|y &minus; x| &lt; ε implies y &isin; S . Thus the sphere {x : |x| &lt; 1} is open. In general, open sets
can be characterized as sets having no sharp boundaries. The interior of any set S
</p>
<p>in En is the set of points x &isin; S which are the center of some sphere contained in S .</p>
<p/>
</div>
<div class="page"><p/>
<p>500 A Mathematical Review
</p>
<p>It is denoted
◦
S . The interior of a set is always open; indeed it is the largest open set
</p>
<p>contained in S . The interior of the set {x : |x| � 1} is the sphere {x : |x| &lt; 1}.
A set P is closed if every point that is arbitrarily close to the set P is a member
</p>
<p>of P. Equivalently, P is closed if xk &rarr; x with xk &isin; P implies x &isin; P. Thus the set
{x : |x| � 1} is closed. The closure of any set P in En is the smallest closed set
containing P. It is denoted S . The boundary of a set is that part of the closure that is
</p>
<p>not in the interior.
</p>
<p>A set is compact if it is both closed and bounded (that is, if it is closed and is con-
</p>
<p>tained within some sphere of finite radius). An important result, due to Weierstrass,
</p>
<p>is that if S is a compact set and {xk} is a sequence each member of which belongs
to S , then {xk} has a limit point in S (that is, there is subsequence converging to a
point in S ).
</p>
<p>Corresponding to a bounded sequence {rk}&infin;k=0 of real numbers, if we let sk =
sup{ri : i � k} then {sk} converges to some real number so. This number is called the
limit superior of {rk} and is denoted lim
</p>
<p>k&rarr;&infin;
(rk).
</p>
<p>A.6 Functions
</p>
<p>A real-valued function f defined on a subset of En is said to be continuous at x if
</p>
<p>xk &rarr; x implies f (xk) &rarr; f (x). Equivalently, f is continuous at x if given ε &gt; 0 there
is a δ &gt; 0 such that |y&minus;x| &lt; δ implies | f (y)&minus; f (x)| &lt; ε. An important result connected
with continuous functions is a theorem of Weierstrass: A continuous function f
</p>
<p>defined on a compact set S has a minimum point in S ; that is, there is an x&lowast; &isin; S
such that for all x &isin; S , f (x) � f (x&lowast;).
</p>
<p>A set of real-valued functions f1, f2, . . . , fm on E
n can be regarded as a sin-
</p>
<p>gle vector function f = ( f1, f2, . . . , fm). This function assigns a vector f(x) =
</p>
<p>( f1(x), f2(x), . . . , fm(x)) in E
m to every vector x &isin; En. Such a vector-valued func-
</p>
<p>tion is said to be continuous if each of its component functions is continuous.
</p>
<p>If each component of f = ( f1, f2, . . . , fm) is continuous on some open set of
</p>
<p>En, then we write f &isin; C. If in addition, each component function has first partial
derivatives which are continuous on this set, we write f &isin; C1. In general, if the
component functions have continuous partial derivatives of order p, we write f &isin; Cp.
</p>
<p>If f &isin; C1 is a real-valued function on En, f (x) = f (x1, x2, . . . , xn), we define
the gradient of f to be the vector
</p>
<p>&nabla; f (x) =
</p>
<p>[
</p>
<p>&part; f (x)
</p>
<p>&part;x1
,
&part; f (x)
</p>
<p>&part;x2
, &middot; &middot; &middot; , &part; f (x)
</p>
<p>&part;xn
</p>
<p>]
</p>
<p>.
</p>
<p>We sometimes use the alternative notation fx(x) for &nabla; f (x). In matrix calculations
</p>
<p>the gradient is considered to be a row vector.</p>
<p/>
</div>
<div class="page"><p/>
<p>A.6 Functions 501
</p>
<p>If f &isin; C2 then we define the Hessian of f at x to be the n &times; n matrix denoted
&nabla;
</p>
<p>2 f (x) or F(x) as
</p>
<p>F(x) =
</p>
<p>[
</p>
<p>&part;2 f (x)
</p>
<p>&part;xi&part;x j
</p>
<p>]
</p>
<p>.
</p>
<p>Since
</p>
<p>&part;2 f
</p>
<p>&part;xi&part;x j
=
</p>
<p>&part;2 f
</p>
<p>&part;x j&part;xi
,
</p>
<p>it is easily seen that the Hessian is symmetric.
</p>
<p>For a vector-valued function f = ( f1, f2, . . . , fm) the situation is similar. If
</p>
<p>f &isin; C1, the first derivative is defined as the m &times; n matrix
</p>
<p>&nabla;f(x) =
</p>
<p>[
</p>
<p>&part; fi(x)
</p>
<p>&part;x j
</p>
<p>]
</p>
<p>.
</p>
<p>If f &isin; C2 it is possible to define the m Hessians F1(x), F2(x), . . . , Fm(x) corre-
sponding to the m component functions. The second derivative itself, for a vector
</p>
<p>function, is a third-order tensor but we do not require its use explicitly. Given any
</p>
<p>λT = [λ1, λ2, . . . , λm] &isin; Em, we note, however, that the real-valued function λT f
has gradient equal to λT&nabla;f(x) and Hessian, denoted λTF(x), equal to
</p>
<p>λTF(x) =
</p>
<p>m
&sum;
</p>
<p>i=1
</p>
<p>λiFi(x).
</p>
<p>Also see Sect. 7.4 for a discussion of convex functions.
</p>
<p>Taylor&rsquo;s Theorem
</p>
<p>A group of results that are used frequently in analysis are referred to under the
</p>
<p>general heading of Taylor&rsquo;s Theorem or Mean Value Theorems. If f &isin; C1 in a
region containing the line segment [x1, x2], then there is a θ, 0 � θ � 1 such that
</p>
<p>f (x2) = f (x1) + &nabla; f (θx1 + (1 &minus; θ)x2)(x2 &minus; x1).
</p>
<p>Furthermore, if f &isin; C2 then there is a θ, 0 � θ � 1 such that
</p>
<p>f (x2) = f (x1) + &nabla; f (x1)(x2 &minus; x1)
</p>
<p>+
1
</p>
<p>2
(x2 &minus; x1)TF(θx1 + (1 &minus; θ)x2)(x2 &minus; x1),
</p>
<p>where F denotes the Hessian of f .</p>
<p/>
</div>
<div class="page"><p/>
<p>502 A Mathematical Review
</p>
<p>Implicit Function Theorem
</p>
<p>Suppose we have a set of m equations in n variables
</p>
<p>hi(x) = 0, i = 1, 2, . . . , m.
</p>
<p>The implicit function theorem addresses the question as to whether if n &minus; m of the
variables are fixed, the equations can be solved for the remaining m variables. Thus
</p>
<p>selecting m variables, say x1, x2, . . . , xm, we wish to determine if these may be
</p>
<p>expressed in terms of the remaining variables in the form
</p>
<p>xi = φi(xm+1, xm+2, . . . , xn), i = 1, 2, . . . , m.
</p>
<p>The functions φi, if they exist, are called implicit functions.
</p>
<p>Theorem. Let x0 = (x0
1
, x0
</p>
<p>2
, . . . , x0n) be a point in E
</p>
<p>n satisfying the properties:
</p>
<p>i) The functions hi &isin; C p, i = 1, 2, . . . , m in some neighborhood of x0, for some p � 1.
ii) hi(x
</p>
<p>0) = 0, i = 1, 2, . . . , m.
</p>
<p>iii) The m &times;m Jacobian matrix
</p>
<p>J =
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>&part;h1(x
0)
</p>
<p>&part;x1
&middot; &middot; &middot; &part;h1(x0)
</p>
<p>&part;xm
</p>
<p>.
</p>
<p>.
</p>
<p>.
.
.
.
</p>
<p>&part;hm(x
0)
</p>
<p>&part;x1
&middot; &middot; &middot; &part;hm(x0)
</p>
<p>&part;xm
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>.
</p>
<p>is nonsingular.
</p>
<p>Then there is a neighborhood of x̂0 = (x0
m+1
</p>
<p>, x0
m+2
</p>
<p>, . . . , x0n) &isin; En&minus;m such that for x̂ =
(xm+1 , xm+2, . . . , xn) in this neighborhood there are functions φi(x̂), i = 1, 2, . . . , m such
that
</p>
<p>i) φi &isin; C p.
ii) x0
</p>
<p>i
= φi(x̂
</p>
<p>0), i = 1, 2, . . . , m.
</p>
<p>iii) hi(φ1(x̂), φ2(x̂), . . . , φm(x̂), x̂) = 0, i = 1, 2, . . . , m.
</p>
<p>Example 1. Consider the equation x2
1
+ x2 = 0. A solution is x1 = 0, x2 = 0.
</p>
<p>However, in a neighborhood of this solution there is no function φ such that x1 =
</p>
<p>φ(x2). At this solution condition (iii) of the implicit function theorem is violated. At
</p>
<p>any other solution, however, such a φ exists.
</p>
<p>Example 2. Let A be an m &times; n matrix (m &lt; n) and consider the system of linear
equations Ax = b. If A is partitioned as A = [B, C] where B is m&times;m then condition
(iii) is satisfied if and only if B is nonsingular. This condition corresponds, of course,
</p>
<p>exactly with what the theory of linear equations tells us. In view of this example, the
</p>
<p>implicit function can be regarded as a nonlinear generalization of the linear theory.</p>
<p/>
</div>
<div class="page"><p/>
<p>A.6 Functions 503
</p>
<p>o, O Notation
</p>
<p>If g is a real-valued function of a real variable, the notation g(x) = O(x) means that
</p>
<p>g(x) goes to zero at least as fast as x does. More precisely, it means that there is a
</p>
<p>K � 0 such that
∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>g(x)
</p>
<p>x
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>� K as x &rarr; 0.
</p>
<p>The notation g(x) = o(x) means that g(x) goes to zero faster than x does; or equiva-
</p>
<p>lently, that K above is zero.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix B
</p>
<p>Convex Sets
</p>
<p>B.1 Basic Definitions
</p>
<p>Concepts related to convex sets so dominate the theory of optimization that it is
</p>
<p>essential for a student of optimization to have knowledge of their most fundamental
</p>
<p>properties. In this appendix is compiled a brief summary of the most important of
</p>
<p>these properties.
</p>
<p>Definition. A set C in En is said to be convex if for every x1, x2 &isin; C and every real number
α, 0 &lt; α &lt; 1, the point αx1 + (1 &minus; α)x2 &isin; C.
</p>
<p>This definition can be interpreted geometrically as stating that a set is convex if,
</p>
<p>given two points in the set, every point on the line segment joining these two points
</p>
<p>is also a member of the set. This is illustrated in Fig. B.1.
</p>
<p>The following proposition shows that certain familiar set operations preserve
</p>
<p>convexity.
</p>
<p>Proposition 1. Convex sets in En satisfy the following relations:
</p>
<p>i) If C is a convex set and β is a real number, the set
</p>
<p>βC = {x : x = βc, c &isin; C}
</p>
<p>is convex.
</p>
<p>ii) If C and D are convex sets, then the set
</p>
<p>C + D = {x : x = c + d, c &isin; C, d &isin; D}
</p>
<p>is convex.
</p>
<p>iii) The intersection of any collection of convex sets is convex.
</p>
<p>The proofs of these three properties follow directly from the definition of a con-
</p>
<p>vex set and are left to the reader. The properties themselves are illustrated in Fig. B.2.
</p>
<p>&copy; Springer International Publishing Switzerland 2016
</p>
<p>D.G. Luenberger, Y. Ye, Linear and Nonlinear Programming, International
Series in Operations Research &amp; Management Science 228,
DOI 10.1007/978-3-319-18842-3
</p>
<p>505</p>
<p/>
</div>
<div class="page"><p/>
<p>506 B Convex Sets
</p>
<p>Another important concept is that of forming the smallest convex set containing
</p>
<p>a given set.
</p>
<p>Fig. B.1 Convexity
</p>
<p>Fig. B.2 Properties of convex sets
</p>
<p>Definition. Let S be a subset of En. The convex hull of S , denoted co(S ), is the set which
is the intersection of all convex sets containing S . The closed convex hull of S is defined as
the closure of co(S ).
</p>
<p>Finally, we conclude this section by defining a cone and a convex cone. A convex
</p>
<p>cone is a special kind of convex set that arises quite frequently.
</p>
<p>Definition. A set C is a cone if x &isin; C implies αx &isin; C for all α &gt; 0. A cone that is also
convex is a convex cone.
</p>
<p>Some cones are shown in Fig. B.3. Their basic property is that if a point x belongs
</p>
<p>to a cone, then the entire half line from the origin through the point (but not the
</p>
<p>origin itself) also must belong to the cone.</p>
<p/>
</div>
<div class="page"><p/>
<p>B.2 Hyperplanes and Polytopes 507
</p>
<p>Fig. B.3 Cones
</p>
<p>B.2 Hyperplanes and Polytopes
</p>
<p>The most important type of convex set (aside from single points) is the hyperplane.
</p>
<p>Hyperplanes dominate the entire theory of optimization, appearing under the guise
</p>
<p>of Lagrange multipliers, duality theory, or gradient calculations.
</p>
<p>The most natural definition of a hyperplane is the logical generalization of the
</p>
<p>geometric properties of a plane in three dimensions. We start by giving this geo-
</p>
<p>metric definition. For computations and for a concrete description of hyperplanes,
</p>
<p>however, there is an equivalent algebraic definition that is more useful. A major
</p>
<p>portion of this section is devoted to establishing this equivalence.
</p>
<p>Definition. A set V in En is said to be a linear variety, if, given any x1, x2 &isin; V, we have
λx1 + (1 &minus; λ)x2 &isin; V for all real numbers λ.
</p>
<p>Note that the only difference between the definition of a linear variety and a
</p>
<p>convex set is that in a linear variety the entire line passing through any two points,
</p>
<p>rather than simply the line segment between them, must lie in the set. Thus in three
</p>
<p>dimensions the nonempty linear varieties are points, lines, two-dimensional planes,
</p>
<p>and the whole space. In general, it is clear that we may speak of the dimension of a
</p>
<p>linear variety. Thus, for example, a point is a linear variety of dimension zero and
</p>
<p>a line is a linear variety of dimension one. In the general case, the dimension of
</p>
<p>a linear variety in En can be found by translating it (moving it) so that it contains
</p>
<p>the origin and then determining the dimension of the resulting set, which is then a
</p>
<p>subspace of En.
</p>
<p>Definition. A hyperplane in En is an (n &minus; 1)-dimensional linear variety.
</p>
<p>We see that hyperplanes generalize the concept of a two-dimensional plane in
</p>
<p>three-dimensional space. They can be regarded as the largest linear varieties in a
</p>
<p>space, other than the entire space itself.
</p>
<p>We now relate this abstract geometric definition to an algebraic one.</p>
<p/>
</div>
<div class="page"><p/>
<p>508 B Convex Sets
</p>
<p>Proposition 2. Let a be a nonzero n-dimensional column vector, and let c be a real number.
</p>
<p>The set
</p>
<p>H = {x &isin; En : aTx = c}
is a hyperplane in En.
</p>
<p>Proof. It follows directly from the linearity of the equation aTx = c that H is a linear
</p>
<p>variety. Let x1 be any vector in H. Translating by &minus;x1 we obtain the set M = H &minus; x1
which is a linear subspace of En. This subspace consists of all vectors x satisfying
</p>
<p>aTx = 0; in other words, all vectors orthogonal to a. This is clearly an (n &minus; 1)-
dimensional subspace. �
</p>
<p>Proposition 3. Let H be a hyperplane in En. Then there is a nonzero n- dimensional vector
</p>
<p>and a constant c such that
</p>
<p>H = {x &isin; En : aTx = c}.
</p>
<p>Proof. Let x1 &isin; H and translate by &minus;x1 obtaining the set M = H &minus; x1. Since H is a
hyperplane, M is an (n&minus;1)-dimensional subspace. Let a be any nonzero vector that is
orthogonal to this subspace, that is, a belongs to the one-dimensional subspace M&perp;.
Clearly M = {x : aTx = 0}. Letting c = aTx1 we see that if x2 &isin; H we have x2&minus;x1 &isin;
M and thus aTx2 &minus;aTx1 = 0 which implies aTx2 = c. Thus H &sub; {x : aTx = c}. Since
H is, by definition, of dimension n &minus; 1 and {x : aTx = c} is of dimension n &minus; 1 by
Proposition 2, these two sets must be equal. �
</p>
<p>Combining Propositions 2 and 3, we see that a hyperplane is the set of solutions
</p>
<p>to a single linear equation. This is illustrated in Fig. B.4. We now use hyperplanes
</p>
<p>to build up other important classes of convex sets.
</p>
<p>Definition. Let a be a nonzero vector in En and let c be a real number. Corresponding to
the hyperplane H = {x : aTx = c} are the positive and negative closed half spaces
</p>
<p>H+ = {x : aTx � c}
H&minus; = {x : aTx � c}
</p>
<p>and the positive and negative open half spaces
</p>
<p>H̊+ = {x : aTx &gt; c}
H̊&minus; = {x : aTx &lt; c}.
</p>
<p>It is easy to see that half spaces are convex sets and that the union of H+ and H&minus;
is the whole space.
</p>
<p>Definition. A set which can be expressed as the intersection of a finite number of closed
half spaces is said to be a convex polytope.
</p>
<p>We see that convex polytopes are the sets obtained as the family of solutions to a
</p>
<p>set of linear inequalities of the form</p>
<p/>
</div>
<div class="page"><p/>
<p>B.3 Separating and Supporting Hyperplanes 509
</p>
<p>Fig. B.4 Hyperplane
</p>
<p>Fig. B.5 Polytopes
</p>
<p>aT1 x � b1
</p>
<p>aT2 x � b2
</p>
<p>...
...
</p>
<p>aTmx � bm,
</p>
<p>since each individual inequality defines a half space and the solution family is the
</p>
<p>intersection of these half spaces. (If some ai = 0, the resulting set can still, as the
</p>
<p>reader may verify, be expressed as the intersection of a finite number of half spaces.)
</p>
<p>Several polytopes are illustrated in Fig. B.5. We note that a polytope may be
</p>
<p>empty, bounded, or unbounded. The case of a nonempty bounded polytope is of
</p>
<p>special interest and we distinguish this case by the following.
</p>
<p>Definition. A nonempty bounded polytope is called a polyhedron.
</p>
<p>B.3 Separating and Supporting Hyperplanes
</p>
<p>The two theorems in this section are perhaps the most important results related to
</p>
<p>convexity. Geometrically, the first states that given a point outside a convex set, a
</p>
<p>hyperplane can be passed through the point that does not touch the convex set. The
</p>
<p>second, which is a limiting case of the first, states that given a boundary point of a
</p>
<p>convex set, there is a hyperplane that contains the boundary point and contains the
</p>
<p>convex set on one side of it.</p>
<p/>
</div>
<div class="page"><p/>
<p>510 B Convex Sets
</p>
<p>Theorem 1. Let C be a convex set and let y be a point exterior to the closure of C. Then
</p>
<p>there is a vector a such that aTy &lt; inf
x&isin;C
</p>
<p>aTx.
</p>
<p>Proof. Let
</p>
<p>δ = inf
x&isin;C
</p>
<p>|x &minus; y| &gt; 0.
</p>
<p>There is an x0 on the boundary of C such that |x0 &minus; y| = δ. This follows because
the continuous function f (x) = |x &minus; y| achieves its minimum over any closed and
bounded set and it is clearly only necessary to consider x in the intersection of the
</p>
<p>closure of C and the sphere of radius 2δ centered at y.
</p>
<p>We shall show that setting a = x0 &minus; y satisfies the conditions of the theorem. Let
x &isin; C. For any α, 0 � α � 1, the point x0 + α(x &minus; x0) &isin; C and thus
</p>
<p>|x0 + α(x &minus; x0) &minus; y|2 � |x0 &minus; y|2.
</p>
<p>Expanding,
</p>
<p>2α(x0 &minus; y)T (x &minus; x0) + α2|x &minus; x0|2 � 0.
Thus, considering this as α &rarr; 0+, we obtain
</p>
<p>(x0 &minus; y)T (x &minus; x0) � 0
</p>
<p>or,
</p>
<p>(x0 &minus; y)Tx � (x0 &minus; y)Tx0 = (x0 &minus; y)Ty + (x0 &minus; y)T (x0 &minus; y)
= (x0 &minus; y)Ty + δ2.
</p>
<p>Setting a = x0 &minus; y proves the theorem. �
</p>
<p>The geometrical interpretation of Theorem 1 is that, given a convex set C and a
</p>
<p>point y exterior to the closure of C, there is a hyperplane containing y that contains
</p>
<p>C in one of its open half spaces. We can easily extend this theorem to include the
</p>
<p>case where y is a boundary point of C.
</p>
<p>Theorem 2. Let C be a convex set and let y be a boundary point of C. Then there is a
</p>
<p>hyperplane containing y and containing C in one of its closed half spaces.
</p>
<p>Proof. Let {yk} be a sequence of vectors, exterior to the closure of C, converging
to y. Let {ak} be the sequence of corresponding vectors constructed according to
Theorem 1, normalized so that |ak| = 1, such that
</p>
<p>aTk yk &lt; inf
x&isin;C
</p>
<p>aτkx.
</p>
<p>Since {ak} is a bounded sequence, it has a convergent subsequence {ak}, k &isin; K with
limit a. For this vector we have for any x &isin; C.
</p>
<p>aTy = lim
k&isin;K
</p>
<p>aTk yk � lim
k&isin;K
</p>
<p>aTk x = ax. �</p>
<p/>
</div>
<div class="page"><p/>
<p>B.4 Extreme Points 511
</p>
<p>Definition. A hyperplane containing a convex set C in one of its closed half spaces and
containing a boundary point of C is said to be a supporting hyperplane of C.
</p>
<p>In terms of this definition, Theorem 2 says that, given a convex set C and a
</p>
<p>boundary point y of C, there is a hyperplane supportingC at y.
</p>
<p>It is useful in the study of convex sets to consider the relative interior of a convex
</p>
<p>set C defined as the largest subset of C that contains no boundary points of C.
</p>
<p>Another variation of the theorems of this section is the one that follows, which is
</p>
<p>commonly known as the Separating Hyperplane Theorem.
</p>
<p>Theorem 3. Let B and C be convex sets with no common relative interior points. (That is
</p>
<p>the only common points are boundary points.) Then there is a hyperplane separating B and
</p>
<p>D. In particular, there is a nonzero vector a such that supb&isin;B a
Tb &le; infc&isin;C aT c.
</p>
<p>Proof. Consider the set G = C &minus; B. It is easily shown that G is convex and that 0 is
not a relative interior point of G. Hence, Theorem 1 or Theorem 2 applies and gives
</p>
<p>the appropriate hyperplane. �
</p>
<p>B.4 Extreme Points
</p>
<p>Definition. A point x in a convex set C is said to be an extreme point of C if there are no
two distinct points x1 and x2 in C such that x = αx1 + (1 &minus; α)x2 for some α, 0 &lt; α &lt; 1.
</p>
<p>For example, in E2 the extreme points of a square are its four corners; the extreme
</p>
<p>points of a circular disk are all points on the boundary. Note that a linear variety
</p>
<p>consisting of more than one point has no extreme points.
</p>
<p>Lemma 1. Let C be a convex set, H a supporting hyperplane of C, and T the intersection
</p>
<p>of H and C. Every extreme point of T is an extreme point of C.
</p>
<p>Proof. Suppose x0 &isin; T is not an extreme point of C. Then x0 = αx1 + (1 &minus; α)x2 for
some x1, x2 &isin; C, x1 � x2, 0 &lt; α &lt; 1. Let H be described as H = {x : aTx = c} with
C contained in its closed positive half space. Then
</p>
<p>aTx1 � c, a
Tx2 � c.
</p>
<p>But, since x0 &isin; H,
c = aTx0 = αa
</p>
<p>Tx1 + (1 &minus; α)aTx2,
and thus x1 and x2 &isin; H. Hence x1, x2 &isin; T and x0 is not an extreme point of T . �
</p>
<p>Theorem 4. A closed bounded convex set in En is equal to the closed convex hull of its
</p>
<p>extreme points.
</p>
<p>Proof. The proof is by induction on the dimension of the space En. The statement
</p>
<p>is easily seen to be true for n = 1. Suppose that it is true for n&minus; 1. Let C be a closed
bounded convex set in En, and let K be the closed convex hull of the extreme points
</p>
<p>of C. We wish to show that K = C.</p>
<p/>
</div>
<div class="page"><p/>
<p>512 B Convex Sets
</p>
<p>Assume there is y &isin; C y � K. Then by Theorem 1, Sect. B.3, there is a hyperplane
separating y and K; that is, there is a � 0, such that aTy &lt; infx&isin;K aTx. Let c0 =
inf
x&isin;C
</p>
<p>(aTx). The number c0 is finite and there is an x0 &isin; C for which aTx0 = c0, because
by Weierstrass&rsquo; Theorem, the continuous function aTx achieves its minimum over
</p>
<p>any closed bounded set. Thus the hyperplane H = {x : aTx = c0} is a supporting
hyperplane to C. It is disjoint from K since c0 &lt; inf
</p>
<p>x&isin;K
(aTx).
</p>
<p>Let T = H &cap; C. Then T is a bounded closed convex subset of H which can be
regarded as a space of dimension n &minus; 1. T is nonempty, since it contains x0. Thus,
by the induction hypothesis, T contains extreme points; and by Lemma 1 these are
</p>
<p>also extreme points of C. Thus we have found extreme points of C not in K, which
</p>
<p>is a contradiction. �
</p>
<p>Let us investigate the implications of this theorem for convex polyhedra. We
</p>
<p>recall that a convex polyhedron is a bounded polytope. Being the intersection of
</p>
<p>closed half spaces, a convex polyhedron is also closed. Thus any convex polyhedron
</p>
<p>is the closed convex hull of its extreme points. It can be shown (see Sect. 2.5) that
</p>
<p>any polytope has at most a finite number of extreme points and hence a convex
</p>
<p>polyhedron is equal to the convex hull of a finite number of points. The converse
</p>
<p>can also be established, yielding the following two equivalent characterizations.
</p>
<p>Theorem 5. A convex polyhedron can be described either as a bounded intersection of a
</p>
<p>finite number of closed half spaces, or as the convex hull of a finite number of points.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix C
</p>
<p>Gaussian Elimination
</p>
<p>This appendix describes the method for solving systems of linear equations that has
</p>
<p>proved to be, not only the most popular, but also the fastest and least susceptible
</p>
<p>to round-off error accumulation&mdash;the method of Gaussian elimination. Attention is
</p>
<p>directed toward explaining this classical elimination technique itself and its relation
</p>
<p>to the theory of LU decomposition of a non-singular square matrix.
</p>
<p>We first note how easily triangular systems of equations can be solved. Thus the
</p>
<p>system
</p>
<p>a11x1 = b1
a21x1 + a22x2 = b2
...
</p>
<p>...
</p>
<p>an1x1 + an2x2 + &middot; &middot; &middot; + annxn = bn
can be solved recursively as follows:
</p>
<p>x1 = b1/a11
</p>
<p>x2 = (b2 &minus; a21x1)/a22
...
</p>
<p>xn = (bn &minus; an1x1 &minus; an2x2 . . . &minus; ann&minus;1xn&minus;1)/ann,
</p>
<p>provided that each of the diagonal terms aii, i = 1, 2, . . . , n is nonzero (as they
</p>
<p>must be if the system is nonsingular). This observation motivates us to attempt to
</p>
<p>reduce an arbitrary system of equations to a triangular one.
</p>
<p>Definition. A square matrix C = [cij] is said to be lower triangular if cij = 0 for
</p>
<p>i &lt; j. Similarly, C is said to be upper triangular if cij = 0 for i &gt; j.
</p>
<p>In matrix notation, the idea of Gaussian elimination is to somehow find a decom-
</p>
<p>position of a given n&times; n matrix A in the form A = LU where L is a lower triangular
and U an upper triangular matrix. The system
</p>
<p>&copy; Springer International Publishing Switzerland 2016
</p>
<p>D.G. Luenberger, Y. Ye, Linear and Nonlinear Programming, International
Series in Operations Research &amp; Management Science 228,
DOI 10.1007/978-3-319-18842-3
</p>
<p>513</p>
<p/>
</div>
<div class="page"><p/>
<p>514 C Gaussian Elimination
</p>
<p>Ax = b (C.1)
</p>
<p>can then be solved by solving the two triangular systems
</p>
<p>Ly = b, Ux = y. (C.2)
</p>
<p>The calculation of L and U together with solution of the first of these systems is
</p>
<p>usually referred to as forward elimination, while solution of the second triangular
</p>
<p>system is called back substitution.
</p>
<p>Every nonsingular square matrix A has an LU decomposition, provided that int-
</p>
<p>erchanges of rows of A are introduced if necessary. This interchange of rows cor-
</p>
<p>responds to a simple reordering of the system of equations, and hence amounts to
</p>
<p>no loss of generality in the method. For simplicity of notation, however, we assume
</p>
<p>that no such interchanges are required.
</p>
<p>We turn now to the problem of explicitly determining L and U, by elimination,
</p>
<p>for a nonsingular matrix A. Given the system, we attempt to transform it so that
</p>
<p>zeros appear below the main diagonal. Assuming that a11 � 0 we subtract multiples
</p>
<p>of the first equation from each of the others in order to get zeros in the first column
</p>
<p>below a11. If we define mk1 = ak1/a11 and let
</p>
<p>M1 =
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>1
</p>
<p>&minus;m21 1
&minus;m31 1
&bull;
&bull;
&bull;
&minus;mn1 1
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>,
</p>
<p>the resulting new system of equations can be expressed as
</p>
<p>A(2)x = b(2)
</p>
<p>with
</p>
<p>A(2) = M1A, b
(2) = M1b.
</p>
<p>The matrix A(2) = [a(2)
ij
</p>
<p>] has a(2)
k1
</p>
<p>= 0, k &gt; 1.
</p>
<p>Next, assuming a(2)
22
� 0, multiples of the second equation of the new system
</p>
<p>are subtracted from equations 3 through n to yield zeros below a(2)
22
</p>
<p>in the second
</p>
<p>column. This is equivalent in premultiplying A(2) and b(2) by
</p>
<p>M2 =
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>1 0
</p>
<p>0 1
</p>
<p>&bull; &minus;m32 1
&bull; &minus;m42
&bull; &bull;
</p>
<p>&bull;
&bull;
&minus;mn2 1
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>,
</p>
<p>where mk2 = a
(2)
</p>
<p>k2
/a
</p>
<p>(2)
22
</p>
<p>. This yields A(3) = M2A
(2) and b(3) = M2A
</p>
<p>(2).</p>
<p/>
</div>
<div class="page"><p/>
<p>C Gaussian Elimination 515
</p>
<p>Proceeding in this way we obtain A(n) = Mn&minus;1Mn&minus;2 . . .M1A, an upper triangular
matrix which we denote by U. The matrix M = Mn&minus;1Mn&minus;2 . . .M1 is a lower trian-
gular matrix, and since MA = U we have A = M&minus;1U. The matrix L = M&minus;1 is also
lower triangular and becomes the L of the desired LU decomposition for A.
</p>
<p>The representation for L can be made more explicit by noting that M&minus;1
k
</p>
<p>is the
</p>
<p>same as Mk except that the off-diagonal terms have the opposite sign. Furthermore,
</p>
<p>we have L = M&minus;1 = M&minus;11 M
&minus;1
2 . . .M
</p>
<p>&minus;1
n&minus;1 which is easily verified to be
</p>
<p>L =
</p>
<p>⎡
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎢
</p>
<p>⎣
</p>
<p>1 0
</p>
<p>m21 1
</p>
<p>m31 m32 1
</p>
<p>&bull; &bull; &bull;
&bull; &bull; &bull;
&bull; &bull; &bull;
mn1 mn2 &bull; &bull; &bull; 1
</p>
<p>⎤
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎥
</p>
<p>⎦
</p>
<p>&bull;
</p>
<p>Hence L can be evaluated directly in terms of the calculations required by the elim-
</p>
<p>ination process. Of course, an explicit representation for M = L&minus;1 would actually
be more useful but a simple representation for M does not exist. Thus we content
</p>
<p>ourselves with the explicit representation for L and use it in (C.2).
</p>
<p>If the original system (C.1) is to be solved for a single b vector, the vector y
</p>
<p>satisfying Ly = b is usually calculated simultaneously with L in the form y =
</p>
<p>b(n) = Mb. The final solution x is then found by a single back substitution, from
</p>
<p>Ux = y. Once the LU decomposition of A has been obtained, however, the solution
</p>
<p>corresponding to any right-hand side can be found by solving the two systems (C.2).
</p>
<p>In practice, the diagonal element a
(k)
</p>
<p>kk
of A(k) may become zero or very close
</p>
<p>to zero. In this case it is important that the kth row be interchanged with a row
</p>
<p>that is below it. Indeed, for considerations of numerical accuracy, it is desirable to
</p>
<p>continuously introduce row interchanges of this type in such a way to insure |mij| � 1
for all i, j. If this is done, the Gaussian elimination procedure has exceptionally
</p>
<p>good stability properties.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix D
</p>
<p>Basic Network Concepts
</p>
<p>This appendix describes some of the basic graph and network terminology and
</p>
<p>concepts necessary for the development of this alternative approach.
</p>
<p>A graph consists of a finite collection of elements called nodes together with a
</p>
<p>subset of unordered pairs of the nodes called arcs. The nodes of a graph are usually
</p>
<p>numbered, say, 1, 2, 3, . . . , n. An arc between nodes i and j is then represented by
</p>
<p>the unordered pair (i, j). A graph is typically represented as shown in Fig. D.1. The
</p>
<p>nodes are designated by circles, with the number inside each circle denoting the
</p>
<p>index of that node. The arcs are represented by the lines between the nodes.
</p>
<p>Fig. D.1 A graph
</p>
<p>There are a number of other elementary definitions associated with graphs that
</p>
<p>are useful in describing their structure. A chain between nodes i and j is a sequence
</p>
<p>of arcs connecting them. The sequence must have the form (i, k1), (k1, k2), (k2, k3),
</p>
<p>. . . , (km, j). In Fig. D.1, (1, 2), (2, 4), (4, 3) is a chain between nodes 1 and 3. If a
</p>
<p>direction of movement along a chain is specified&mdash;say from node i to node j&mdash;it is
</p>
<p>then called a path from i to j. A cycle is a chain leading from node i back to node i.
</p>
<p>The chain (1, 2), (2, 4), (4, 3), (3, 1) is a cycle for the graph in Fig. D.1.
</p>
<p>&copy; Springer International Publishing Switzerland 2016
</p>
<p>D.G. Luenberger, Y. Ye, Linear and Nonlinear Programming, International
Series in Operations Research &amp; Management Science 228,
DOI 10.1007/978-3-319-18842-3
</p>
<p>517</p>
<p/>
</div>
<div class="page"><p/>
<p>518 D Basic Network Concepts
</p>
<p>A graph is connected if there is a chain between any two nodes. Thus, the graph
</p>
<p>of Fig. D.1 is connected. A graph is a tree if it is connected and has no cycles.
</p>
<p>Removal of any one of the arcs (1, 2), (1, 3), (2, 4), (3, 4) would transform the graph
</p>
<p>of Fig. D.1 into a tree. Sometimes we consider a tree within a graphG, which is just
</p>
<p>a tree made up of a subset of arcs from G. Such a tree is a spanning tree if it touches
</p>
<p>all nodes of G. It is easy to see that a graph is connected if and only if it contains a
</p>
<p>spanning tree.
</p>
<p>In directed graphs a sense of orientation is given to each arc. In this case an
</p>
<p>arc is considered to be an ordered pair of nodes (i, j), and we say that the arc is
</p>
<p>from node i to node j. This is indicated on the graph by having an arrow on the arc
</p>
<p>pointing from i to j as shown in Fig. D.2. When working with directed graphs, some
</p>
<p>node pairs may have an arc in both directions between them. Rather than explicitly
</p>
<p>indicating both arcs in such a case, it is customary to indicate a single undirected
</p>
<p>arc. The notions of paths and cycles can be directly applied to directed graphs. In
</p>
<p>addition we say that node j is reachable from i if there is a path from node i to j.
</p>
<p>In addition to the visual representation of a directed graph characterized by
</p>
<p>Fig. D.2, another common method of representation is in terms of a graph&rsquo;s node-
</p>
<p>arc incidence matrix. This is constructed by listing the nodes vertically and the arcs
</p>
<p>horizontally. Then in the column under arc (i, j), a +1 is placed in the position cor-
</p>
<p>responding to node i and a &minus;1 is placed in the position corresponding to node j. The
incidence matrix for the graph of Fig. D.2 is shown in Table D.1.
</p>
<p>Fig. D.2 A directed graph
</p>
<p>(1,2) (1,4) (2,3) (2,4) (4,2)
1 1 1
2 &ndash;1 1 1 &ndash;1
3 &ndash;1
4 &ndash;1 &ndash;1 1
</p>
<p>Table D.1 Incidence matrix for example</p>
<p/>
</div>
<div class="page"><p/>
<p>D.2 Tree Procedure 519
</p>
<p>Clearly, all information about the structure of the graph is contained in the node-
</p>
<p>arc incidence matrix. This representation is often very useful for computational pur-
</p>
<p>poses, since it is easily stored in a computer.
</p>
<p>D.1 Flows in Networks
</p>
<p>A graph is an effective way to represent the communication structure between nodes.
</p>
<p>When there is the possibility of flow along the arcs, we refer to the directed graph as
</p>
<p>a network. In applications the network might represent a transportation system or a
</p>
<p>communication network, or it may simply be a representation used for mathematical
</p>
<p>purposes (such as in the assignment problem).
</p>
<p>A flow in a given directed arc (i, j) is a number xi j � 0. Flows in the arcs of
</p>
<p>the network must jointly satisfy a conservation criterion at each node. Specifically,
</p>
<p>unless the node is a source or sink as discussed below, flow cannot be created or lost
</p>
<p>at a node; the total flow into a node must equal the total flow out of the node. Thus
</p>
<p>at each such node i
n
&sum;
</p>
<p>j=1
</p>
<p>xi j &minus;
n
&sum;
</p>
<p>k=1
</p>
<p>xki = 0.
</p>
<p>The first sum is the total flow from i, and the second sum is the total flow to i.
</p>
<p>(Of course xi j does not exist if there is no arc from i to j.) It should be clear that
</p>
<p>for nonzero flows to exist in a network without sources or sinks, the network must
</p>
<p>contain a cycle.
</p>
<p>In many applications, some nodes are in fact designated as sources or sinks (or,
</p>
<p>alternatively, supply nodes or demand nodes). The net flow out of a source may be
</p>
<p>positive, and the level of this net flow may either be fixed or variable, depending on
</p>
<p>the application. Similarly, the net flow into a sink may be positive.
</p>
<p>D.2 Tree Procedure
</p>
<p>Recall that node j is reachable from node i in a directed graph if there is a path
</p>
<p>from node i to node j. For simple graphs, determination of reachability can be ac-
</p>
<p>complished by inspection, but for large graphs it generally cannot. The problem can
</p>
<p>be solved systematically by a process of repeatedly labeling and scanning various
</p>
<p>nodes in the graph. This procedure is the backbone of a number of methods for solv-
</p>
<p>ing more complex graph and network problems, as illustrated later. It can also be
</p>
<p>used to establish quickly some important theoretical results.
</p>
<p>Assume that we wish to determine whether a path from node 1 to node m exists.
</p>
<p>At each step of the algorithm, each node is either unlabeled, labeled but unscanned,
</p>
<p>or labeled and scanned. The procedure consists of these steps:</p>
<p/>
</div>
<div class="page"><p/>
<p>520 D Basic Network Concepts
</p>
<p>Step 1. Label node 1 with any mark. All other nodes are unlabeled.
</p>
<p>Step 2. For any labeled but unscanned node i, scan the node by finding all unla-
</p>
<p>beled nodes reachable from i by a single arc. Label these nodes with an i.
</p>
<p>Step 3. If node m is labeled, stop; a breakthrough has been achieved&mdash;a path
</p>
<p>exists. If no unlabeled nodes can be labeled, stop; no connecting path exists.
</p>
<p>Otherwise, go to Step 2.
</p>
<p>The process is illustrated in Fig. D.3, where a path between nodes 1 and 10 is
</p>
<p>sought. The nodes have been labeled and scanned in the order 1, 2, 3, 5, 6, 8, 4, 7,
</p>
<p>9, 10. The labels are indicated close to the nodes. The arcs that were used in the
</p>
<p>scanning processes are indicated by heavy lines. Note that the collection of nodes
</p>
<p>and arcs selected by the process, regarded as an undirected graph, form a tree&mdash;
</p>
<p>a graph without cycles. This, of course, accounts for the name of the process, the
</p>
<p>tree procedure. If one is interested only in determining whether a connecting path
</p>
<p>exists and does not need to find the path itself, then the labels need only be simple
</p>
<p>check marks rather than node indices. However, if node indices are used as labels,
</p>
<p>then after successful completion of the algorithm, the actual connecting path can be
</p>
<p>found by tracing backward from node m by following the labels. In the example,
</p>
<p>one begins at 10 and moves to node 7 as indicated; then to 6, 3, and 1. The path
</p>
<p>follows the reverse of this sequence.
</p>
<p>It is easy to prove that the algorithm does indeed resolve the issue of the existence
</p>
<p>of a connecting path. At each stage of the process, either a new node is labeled,
</p>
<p>it is impossible to continue, or node m is labeled and the process is successfully
</p>
<p>terminated. Clearly, the process can continue for at most n&minus; 1 stages, where n is the
number of nodes in the graph. Suppose at some stage it is impossible to continue.
</p>
<p>Let S be the set of labeled nodes at that stage and let S̄ be the set of unlabeled nodes.
</p>
<p>Clearly, node 1 is contained in S , and node m is contained in S̄ . If there were a path
</p>
<p>connecting node 1 with node m, then there must be an arc in that path from a node k
</p>
<p>in S to a node in S̄ . However, this would imply that node k was not scanned, which
</p>
<p>is a contradiction. Conversely, if the algorithm does continue until reaching node
</p>
<p>m, then it is clear that a connecting path can be constructed backward as outlined
</p>
<p>above.
</p>
<p>Fig. D.3 The scanning procedure</p>
<p/>
</div>
<div class="page"><p/>
<p>D.3 Capacitated Networks 521
</p>
<p>D.3 Capacitated Networks
</p>
<p>In some network applications it is useful to assume that there are upper bounds
</p>
<p>on the allowable flow in various arcs. This motivates the concept of a capacitated
</p>
<p>network. A capacitated network is a network in which some arcs are assigned non-
</p>
<p>negative capacities, which define the maximum allowable flow in those arcs. The
</p>
<p>capacity of an arc (i, j) is denoted ki j, and this capacity is indicated on the graph by
</p>
<p>placing the number ki j adjacent to the arc. Figure 2.1 shows an example of a network
</p>
<p>with the capacities indicated. Thus the capacity from node 1 to node 2 is 12, while
</p>
<p>that from node 2 to node 1 is 6.</p>
<p/>
</div>
<div class="page"><p/>
<p>Bibliography
</p>
<p>[A1] J. Abadie, J. Carpentier, Generalization of the Wolfe reduced gradient
</p>
<p>method to the case of nonlinear constraints, in Optimization, ed. by R.
</p>
<p>Fletcher (Academic, London, 1969), pp. 37&ndash;47
</p>
<p>[A2] H. Akaike, On a successive transformation of probability distribution and
</p>
<p>its application to the analysis of the optimum gradient method. Ann. Inst.
</p>
<p>Stat. Math. 11, 1&ndash;17 (1959)
</p>
<p>[3] A.Y. Alfakih, A. Khandani, H. Wolkowicz, Solving euclidean distance
</p>
<p>matrix completion problems via semidefinite programming. Comput. Opt.
</p>
<p>Appl. 12, 13&ndash;30 (1999)
</p>
<p>[A3] F. Alizadeh, Combinatorial optimization with interior point methods and
</p>
<p>semi-definite matrices, Ph.D. thesis, University of Minnesota, Minneapolis,
</p>
<p>1991
</p>
<p>[A4] F. Alizadeh, Optimization over the positive semi-definite cone: interior-
</p>
<p>point methods and combinatorial applications, in Advances in Optimization
</p>
<p>and Parallel Computing, ed. by P.M. Pardalos (North Holland, Amsterdam,
</p>
<p>1992), pp. 1&ndash;25
</p>
<p>[6] E.D. Andersen, MOSEK: high performance software for large-scale LP, QP,
</p>
<p>SOCP, SDP and MIP, http://www.mosek.com/ (1997)
</p>
<p>[A5] E.D. Andersen, Y. Ye, On a homogeneous algorithm for the monotone com-
</p>
<p>plementarity problem. Math. Prog. 84, 375&ndash;400 (1999)
</p>
<p>[A6] K.M. Anstreicher, D. den Hertog, C. Roos, T. Terlaky, A long step bar-
</p>
<p>rier method for convex quadratic programming. Algorithmica 10, 365&ndash;382
</p>
<p>(1993)
</p>
<p>[A7] H.A. Antosiewicz, W.C. Rheinboldt, Numerical analysis and functional
</p>
<p>analysis, in Survey of Numerical Analysis, ed. by J. Todd, Chap. 14
</p>
<p>(McGraw-Hill, New York, 1962)
</p>
<p>[A8] L. Armijo, Minimization of functions having lipschitz continuous first-
</p>
<p>partial derivatives. Pac. J. Math. 16(1), 1&ndash;3 (1966)
</p>
<p>&copy; Springer International Publishing Switzerland 2016
</p>
<p>D.G. Luenberger, Y. Ye, Linear and Nonlinear Programming, International
Series in Operations Research &amp; Management Science 228,
DOI 10.1007/978-3-319-18842-3
</p>
<p>523</p>
<p/>
<div class="annotation"><a href="http://www.mosek.com/">http://www.mosek.com/</a></div>
</div>
<div class="page"><p/>
<p>524 Bibliography
</p>
<p>[A9] K.J. Arrow, L. Hurwicz, Gradient method for concave programming, I.: lo-
</p>
<p>cal results, in Studies in Linear and Nonlinear Programming, ed. by K.J.
</p>
<p>Arrow, L. Hurwicz, H. Uzawa (Stanford University Press, Stanford, CA,
</p>
<p>1958)
</p>
<p>[12] E. Balas, S. Ceria, G. Cornuejols, A lift-and-project cutting plane algorithm
</p>
<p>for mixed 0-1 programs. Math. Program. 58, 295&ndash;324 (1993)
</p>
<p>[B1] R.H. Bartels, A numerical investigation of the simplex method. Technical
</p>
<p>Report No. CS 104, Computer Science Department, Stanford University,
</p>
<p>Stanford, CA (31 July 1968)
</p>
<p>[B2] R.H. Bartels, G.H Golub, The simplex method of linear programming using
</p>
<p>LU decomposition. Commun. ACM 12(5), 266&ndash;268 (1969)
</p>
<p>[15] A. Barvinok, A remark on the rank of positive semidefinite matrices subject
</p>
<p>to affine constraints. Discrete Comput. Geom. 25, 23&ndash;31 (2001)
</p>
<p>[16] A. Barvinok, A Course in Convexity. Graduate Studies in Mathematics, vol.
</p>
<p>54 (American Mathematical Society, Providence, RI, 2002)
</p>
<p>[17] J. Barzilai, J.M. Borwein, Two-point step size gradient methods. IMA
</p>
<p>J. Numer. Anal. 8, 141&ndash;148 (2008)
</p>
<p>[B3] D.A., Bayer, J.C. Lagarias, The nonlinear geometry of linear program-
</p>
<p>ming, part i: affine and projective scaling trajectories. Trans. Am. Math. Soc
</p>
<p>314(2), 499&ndash;526 (1989)
</p>
<p>[B4] D.A. Bayer, J.C. Lagarias, The nonlinear geometry of linear programming,
</p>
<p>part ii: legendre transform coordinates. Trans. Am. Math. Soc. 314(2),
</p>
<p>527&ndash;581 (1989)
</p>
<p>[B5] M.S. Bazaraa, J.J. Jarvis, Linear Programming and Network Flows (Wiley,
</p>
<p>New York, 1977)
</p>
<p>[B6] M.S. Bazaraa, J.J. Jarvis, H.F. Sherali, Karmarkar&rsquo;s projective algorithm
</p>
<p>(Chap. 8.4), pp. 380&ndash;394; Analysis of Karmarkar&rsquo;s algorithm (Chap. 8.5),
</p>
<p>pp. 394&ndash;418, in Linear Programming and Network Flows, 2nd edn. (Wiley
</p>
<p>New York, 1990)
</p>
<p>[B7] E.M.L. Beale, in Numerical Methods, Nonlinear Programming, ed. by J.
</p>
<p>Abadie (North-Holland, Amsterdam, 1967)
</p>
<p>[23] A. Beck, M. Teboulle, A fast iterative shrinkage-thresholding algorithm for
</p>
<p>linear inverse problems. SIAM J. Imaging Sci. 2, 183&ndash;202 (2009)
</p>
<p>[B8] F.S. Beckman, The solution of linear equations by the conjugate gradient
</p>
<p>method, in Mathematical Methods for Digital Computers, ed. by A. Ralston,
</p>
<p>H.S. Wilf, vol. 1 (Wiley, New York, 1960)
</p>
<p>[25] S.J. Benson, Y. Ye, X. Zhang, Solving large-scale sparse semidefinite pro-
</p>
<p>grams for combinatorial optimization. SIAM J. Optim. 10, 443&ndash;461 (2000)
</p>
<p>[26] A. Ben-Tal, A. Nemirovski, Structural design via semidefinite program-
</p>
<p>ming, in Handbook on Semidefinite Programming (Kluwer, Boston, 2000),
</p>
<p>pp. 443&ndash;467
</p>
<p>[B9] D.P. Bertsekas, Partial conjugate gradient methods for a class of optimal
</p>
<p>control problems. IEEE Trans. Autom. Control 19, 209&ndash;217 (1973)
</p>
<p>[B10] D.P. Bertsekas, Multiplier methods: a survey. Automatica 12(2), 133&ndash;145
</p>
<p>(1976)</p>
<p/>
</div>
<div class="page"><p/>
<p>Bibliography 525
</p>
<p>[B11] D.P. Bertsekas, Constrained Optimization and Lagrange Multiplier Meth-
</p>
<p>ods (Academic, New York, 1982)
</p>
<p>[B12] D.P. Bertsekas, Nonlinear Programming (Athena Scientific, Belmont, 1995)
</p>
<p>[31] D. Bertsimas, Y. Ye, Semidefinite relaxations, multivariate normal distri-
</p>
<p>butions, and order statistics, in Handbook of Combinatorial Optimization
</p>
<p>(Springer, New York, 1999), pp. 1473&ndash;1491
</p>
<p>[B13] D.M. Bertsimas, J.N. Tsitsiklis, Linear Optimization (Athena Scientific,
</p>
<p>Belmont, 1997)
</p>
<p>[B14] M.C. Biggs, Constrained minimization using recursive quadratic program-
</p>
<p>ming: some alternative sub-problem formulations, in Towards Global Op-
</p>
<p>timization, ed. by L.C.W. Dixon, G.P. Szego (North-Holland, Amsterdam,
</p>
<p>1975)
</p>
<p>[B15] M.C. Biggs, On the convergence of some constrained minimization algo-
</p>
<p>rithms based on recursive quadratic programming. J. Inst. Math. Appl. 21,
</p>
<p>67&ndash;81 (1978)
</p>
<p>[B16] G. Birkhoff, Three observations on linear algebra. Rev. Univ. Nac. Tu-
</p>
<p>cumán, Ser. A. 5, 147&ndash;151 (1946)
</p>
<p>[B17] P. Biswas, Y. Ye, Semidefinite programming for ad hoc wireless sensor net-
</p>
<p>work localization, in Proceedings of the 3rd IPSN, 2004, pp. 46&ndash;54
</p>
<p>[B18] R.E. Bixby, Progress in linear programming. ORSA J. Comput. 6(1), 15&ndash;22
</p>
<p>(1994)
</p>
<p>[B19] R.G. Bland, New finite pivoting rules for the simplex method. Math. Oper.
</p>
<p>Res. 2(2), 103&ndash;107 (1977)
</p>
<p>[B20] R.G. Bland, D. Goldfarb, M.J. Todd, The ellipsoidal method: a survey. Oper.
</p>
<p>Res. 29, 1039&ndash;1091 (1981)
</p>
<p>[B21] L. Blum, F. Cucker, M. Shub, S. Smale, Complexity and Real Computation
</p>
<p>(Springer, New York, 1996)
</p>
<p>[41] S. Boyd, N. Parikh, E. Chu, B. Peleato, J. Eckstein, Distributed optimization
</p>
<p>and statistical learning via the alternating direction method of multipliers.
</p>
<p>Found. Trends Mach. Learn. 3, 1&ndash;122 (2010)
</p>
<p>[B22] S. Boyd, L.E. Ghaoui, E. Feron, V. Balakrishnan, LinearMatrix Inequalities
</p>
<p>in System and Control Science (SIAM, Philadelphia, 1994)
</p>
<p>[B23] S. Boyd, L. Vandenberghe, Convex Optimization (Cambridge University
</p>
<p>Press, Cambridge, 2004)
</p>
<p>[B24] C.G. Broyden, Quasi-Newton methods and their application to function
</p>
<p>minimization. Math. Comput. 21, 368&ndash;381 (1967)
</p>
<p>[B25] C.G. Broyden, The convergence of a class of double rank minimization
</p>
<p>algorithms: parts I and II. J. Inst. Math. Appl. 6, 76&ndash;90, 222&ndash;231 (1970)
</p>
<p>[B26] T. Butler, A.V. Martin, On a method of courant for minimizing functionals.
</p>
<p>J. Math. Phys. 41, 291&ndash;299 (1962)
</p>
<p>[C1] C.W. Carroll, The created response surface technique for optimizing non-
</p>
<p>linear restrained systems. Oper. Res. 9(12), 169&ndash;184 (1961)
</p>
<p>[C2] A. Charnes, Optimality and degeneracy in linear programming. Economet-
</p>
<p>rica 20, 160&ndash;170 (1952)</p>
<p/>
</div>
<div class="page"><p/>
<p>526 Bibliography
</p>
<p>[C3] A. Charnes, C.E. Lemke, The bounded variables problem. ONR Research
</p>
<p>Memorandum 10, Graduate School of Industrial Administration, Carnegie
</p>
<p>Institute of Technology, Pittsburgh (1954)
</p>
<p>[50] C.H. Chen, B.S. He, Y.Y. Ye, X.M. Yuan, The direct extension of ADMM
</p>
<p>for multi-block convex minimization problems is not necessarily conver-
</p>
<p>gent. Math. Program. (2014). doi:10.1007/s10107-014-0826-5
</p>
<p>[C4] A. Cohen, Rate of convergence for root finding and optimization algorithms.
</p>
<p>Ph.D. dissertation, University of California, Berkeley, 1970
</p>
<p>[C5] S.A. Cook, The complexity of theorem-proving procedures, in Proceedings
</p>
<p>of 3rd ACM Symposium on the Theory of Computing, 1971, pp. 151&ndash;158
</p>
<p>[C6] R.W. Cottle, Linear Programming. Lecture Notes for MS&amp; E 310 (Stanford
</p>
<p>University, Stanford, 2002)
</p>
<p>[C7] R. Cottle, J.S. Pang, R.E. Stone, Interior-Point Methods (Chap. 5.9), in The
</p>
<p>Linear Complementarity Problem (Academic, Boston, 1992), pp. 461&ndash;475
</p>
<p>[C8] R. Courant, Calculus of variations and supplementary notes and exercises
</p>
<p>(mimeographed notes), supplementary notes by M. Kruskal and H. Rubin,
</p>
<p>revised and amended by J. Moser, New York University (1962)
</p>
<p>[C9] J.B. Crockett, H. Chernoff, Gradient methods of maximization. Pac. J.
</p>
<p>Math. 5, 33&ndash;50 (1955)
</p>
<p>[C10] H. Curry, The method of steepest descent for nonlinear minimization
</p>
<p>problems. Q. Appl. Math. 2, 258&ndash;261 (1944)
</p>
<p>[58] Y.-H. Dai, R. Fletcher, Projected Barzilai-Borwein methods for large-scale
</p>
<p>box-constrained quadratic programming. Numer. Math. 100, 21&ndash;47 (2005)
</p>
<p>[D1] J.W. Daniel, The conjugate gradient method for linear and nonlinear opera-
</p>
<p>tor equations. SIAM J. Numer. Anal. 4(1), 10&ndash;26 (1967)
</p>
<p>[D2] G.B. Dantzig, Maximization of a linear function of variables subject to lin-
</p>
<p>ear inequalities (Chap. XXI), in Activity Analysis of Production and Allo-
</p>
<p>cation, ed. by T.C. Koopmans. Cowles Commission Monograph, vol. 13
</p>
<p>(Wiley, New York, 1951)
</p>
<p>[D3] G.B., Dantzig, Application of the simplex method to a transportation prob-
</p>
<p>lem, in Activity Analysis of Production and Allocation, ed. by T.C. Koop-
</p>
<p>mans (Wiley, New York, 1951), pp. 359&ndash;373
</p>
<p>[D4] G.B. Dantzig, Computational algorithm of the revised simplex method.
</p>
<p>RAND Report RM-1266, The RAND Corporation, Santa Monica, CA
</p>
<p>(1953)
</p>
<p>[D5] G.B. Dantzig, Variables with upper bounds in linear programming. RAND
</p>
<p>Report RM-1271, The RAND Corporation, Santa Monica, CA (1954)
</p>
<p>[D6] G.B. Dantzig, Linear Programming and Extensions (Princeton University
</p>
<p>Press, Princeton, 1963)
</p>
<p>[D7] G.B. Dantzig, L.R. Ford Jr., D.R. Fulkerson, A primal-dual algorithm, in
</p>
<p>Linear Inequalities and Related Systems. Annals of Mathematics Study, vol.
</p>
<p>38 (Princeton University Press, Princeton, NJ, 1956), pp. 171&ndash;181
</p>
<p>[D8] G.B. Dantzig, A. Orden, P. Wolfe, Generalized simplex method for minimiz-
</p>
<p>ing a linear form under linear inequality restraints. RAND Report RM-1264,
</p>
<p>The RAND Corporation, Santa Monica, CA (1954)</p>
<p/>
</div>
<div class="page"><p/>
<p>Bibliography 527
</p>
<p>[D9] G.B. Dantzig, M.N. Thapa, Linear Programming 1: Introduction (Springer,
</p>
<p>New York, 1997)
</p>
<p>[D10] G.B. Dantzig, M.N. Thapa, Linear Programming 2: Theory and Extensions
</p>
<p>(Springer, New York, 2003)
</p>
<p>[D11] G.B. Dantzig, P. Wolfe, Decomposition principle for linear programs. Oper.
</p>
<p>Res. 8, 101&ndash;111 (1960)
</p>
<p>[D12] W.C. Davidon, Variable metric method for minimization. Research and
</p>
<p>Development Report ANL-5990 (Ref.) U.S. Atomic Energy Commission,
</p>
<p>Argonne National Laboratories (1959)
</p>
<p>[D13] W.C. Davidon, Variance algorithm for minimization. Comput. J. 10,
</p>
<p>406&ndash;410 (1968)
</p>
<p>[72] E. de Klerk, C. Roos, T. Terlaky, Initialization in semidefinite programming
</p>
<p>via a self&ndash;dual skew&ndash;symmetric embedding. Oper. Res. Lett. 20, 213&ndash;221
</p>
<p>(1997)
</p>
<p>[D14] R.S. Dembo, S.C. Eisenstat, T. Steinhaug, Inexact Newton methods. SIAM
</p>
<p>J. Numer. Anal. 19(2), 400&ndash;408 (1982)
</p>
<p>[D15] J.E. Dennis Jr., J.J. Moré, Quasi-Newton methods, motivation and theory.
</p>
<p>SIAM Rev. 19, 46&ndash;89 (1977)
</p>
<p>[D16] J.E. Dennis Jr., R.E. Schnabel, Least change secant updates for quasi-
</p>
<p>Newton methods. SIAM Rev. 21, 443&ndash;469 (1979)
</p>
<p>[D17] L.C.W. Dixon, Quasi-Newton algorithms generate identical points. Math.
</p>
<p>Prog. 2, 383&ndash;387 (1972)
</p>
<p>[E1] B.C. Eaves, W.I. Zangwill, Generalized cutting plane algorithms. Working
</p>
<p>Paper No. 274, Center for Research in Management Science, University of
</p>
<p>California, Berkeley (July 1969)
</p>
<p>[78] J. Eckstein, D.P. Bertsekas, On the Douglas-Rachford splitting method and
</p>
<p>the proximal point algorithm for maximal monotone operators. Math. Pro-
</p>
<p>gram. 55, 293&ndash;318 (1992)
</p>
<p>[E2] H. Everett III, Generalized lagrange multiplier method for solving problems
</p>
<p>of optimum allocation of resources. Oper. Res. 11, 399&ndash;417 (1963)
</p>
<p>[F1] D.K. Faddeev, V.N. Faddeeva, Computational Methods of Linear Algebra
</p>
<p>(W. H. Freeman, San Francisco, CA, 1963)
</p>
<p>[F2] S.C. Fang, S. Puthenpura, Linear Optimization and Extensions (Prentice-
</p>
<p>Hall, Englewood Cliffs, NJ, 1994)
</p>
<p>[F3] W. Fenchel, Convex Cones, Sets, and Functions.Lecture Notes (Department
</p>
<p>of Mathematics, Princeton University, Princeton, NJ, 1953)
</p>
<p>[F4] A.V. Fiacco, G.P. McCormick, Nonlinear Programming: Sequential Uncon-
</p>
<p>strained Minimization Techniques (Wiley, New York, 1968)
</p>
<p>[F5] A.V. Fiacco, G.P. McCormick, Nonlinear Programming: Sequential Uncon-
</p>
<p>strained Minimization Techniques (Wiley, New York, 1968). Reprint: Vol-
</p>
<p>ume 4 of SIAM Classics in Applied Mathematics (SIAM Publications,
</p>
<p>Philadelphia, PA, 1990)
</p>
<p>[F6] R. Fletcher, A new approach to variable metric algorithms. Comput. J.
</p>
<p>13(13), 317&ndash;322 (1970)</p>
<p/>
</div>
<div class="page"><p/>
<p>528 Bibliography
</p>
<p>[F7] R. Fletcher, An exact penalty function for nonlinear programming with ine-
</p>
<p>qualities. Math. Program. 5, 129&ndash;150 (1973)
</p>
<p>[F8] R. Fletcher, Conjugate gradient methods for indefinite systems. Numerical
</p>
<p>Analysis Report, 11. Department of Mathematics, University of Dundee,
</p>
<p>Scotland (September 1975)
</p>
<p>[F9] R. Fletcher, Practical Methods of Optimization 1: Unconstrained Optimiza-
</p>
<p>tion (Wiley, Chichester, 1980)
</p>
<p>[F10] R. Fletcher,Practical Methods of Optimization 2: Constrained Optimization
</p>
<p>(Wiley, Chichester, 1981)
</p>
<p>[F11] R. Fletcher, M.J.D. Powell, A rapidly convergent descent method for mini-
</p>
<p>mization. Comput. J. 6, 163&ndash;168 (1963)
</p>
<p>[F12] R. Fletcher, C.M. Reeves, Function minimization by conjugate gradients.
</p>
<p>Comput. J. 7, 149&ndash;154 (1964)
</p>
<p>[F13] L.K. Ford Jr., D.K. Fulkerson, Flows in Networks (Princeton University
</p>
<p>Press, Princeton, NJ, 1962)
</p>
<p>[F14] G.E. Forsythe, On the asymptotic directions of the s-dimensional optimum
</p>
<p>gradient method. Numer. Math. 11, 57&ndash;76 (1968)
</p>
<p>[F15] G.E. Forsythe, C.B. Moler, Computer Solution of Linear Algebraic Systems
</p>
<p>(Prentice-Hall, Englewood Cliffs, NJ, 1967)
</p>
<p>[F16] G.E. Forsythe, W.R. Wasow, Finite-Difference Methods for Partial Differ-
</p>
<p>ential Equations (Wiley, New York, 1960)
</p>
<p>[96] M. Fortin, R. Glowinski, On decomposition-coordination methods using an
</p>
<p>augmented Lagrangian, in Augmented LagrangianMethods: Applications to
</p>
<p>the Solution of Boundary Problems, ed. by M. Fortin, R. Glowinski (North-
</p>
<p>Holland, Amsterdam, 1983)
</p>
<p>[F17] K. Fox, An Introduction to Numerical Linear Algebra (Clarendon Press,
</p>
<p>Oxford, 1964)
</p>
<p>[98] M. Frank, P. Wolfe, An algorithm for quadratic programming. Naval Res.
</p>
<p>Logist. Q. 3, 95&ndash;110 (1956)
</p>
<p>[F18] R.M. Freund, Polynomial-time algorithms for linear programming based
</p>
<p>only on primal scaling and projected gradients of a potential function. Math.
</p>
<p>Program. 51, 203&ndash;222 (1991)
</p>
<p>[F19] K.R. Frisch, The logarithmic potential method for convex programming.
</p>
<p>Unpublished Manuscript, Institute of Economics, University of Oslo, Oslo
</p>
<p>(1955)
</p>
<p>[G1] D. Gabay, Reduced quasi-Newton methods with feasibility improvement for
</p>
<p>nonlinear constrained optimization, in Mathematical Programming Studies,
</p>
<p>vol. 16 (North-Holland, Amsterdam, 1982), pp. 18&ndash;44
</p>
<p>[102] D. Gabay, B. Mercier, A dual algorithm for the solution of nonlinear varia-
</p>
<p>tional problems via finite element approximations. Comput. Math. Appl. 2,
</p>
<p>17&ndash;40 (1976)
</p>
<p>[G2] D. Gale, The Theory of Linear Economic Models (McGraw-Hill, New York,
</p>
<p>1960)</p>
<p/>
</div>
<div class="page"><p/>
<p>Bibliography 529
</p>
<p>[G3] U.M. Garcia-Palomares, O.L. Mangasarian, Superlinearly convergent quasi-
</p>
<p>Newton algorithms for nonlinearly constrained optimization problems.
</p>
<p>Math. Program. 11, 1&ndash;13 (1976)
</p>
<p>[G4] S.I. Gass, Linear Programming, 3rd edn. (McGraw-Hill, New York, 1969)
</p>
<p>[G5] P.E. Gill, W. Murray, M.A. Saunders, J.A. Tomlin, M.H. Wright, On pro-
</p>
<p>jected Newton barrier methods for linear programming and an equivalence
</p>
<p>to Karmarkar&rsquo;s projective method. Math. Program. 36, 183&ndash;209 (1986)
</p>
<p>[G6] P.E., Gill, W. Murray, Quasi-Newton methods for unconstrained optimiza-
</p>
<p>tion. J. Inst. Math. Appl. 9, 91&ndash;108 (1972)
</p>
<p>[G7] P.E. Gill, W. Murray, M.H. Wright, Practical Optimization (Academic,
</p>
<p>London, 1981)
</p>
<p>[109] R. Glowinski, A. Marrocco, Approximation par éléments finis d&rsquo;ordre un et
</p>
<p>résolution par pénalisation-dualité d&rsquo;une classe de problèmes non linéaires.
</p>
<p>R.A.I.R.O. R2 2, 41&ndash;76 (1975)
</p>
<p>[G8] M.X. Goemans, D.P. Williamson, Improved approximation algorithms for
</p>
<p>maximum cut and satisfiability problems using semidefinite programming.
</p>
<p>J. Assoc. Comput. Mach. 42, 1115&ndash;1145 (1995)
</p>
<p>[G9] D. Goldfarb, A family of variable metric methods derived by variational
</p>
<p>means. Math. Comput. 24, 23&ndash;26 (1970)
</p>
<p>[112] D. Goldfarb, G. Iyengar, Robust portfolio selection problems. Math. Oper.
</p>
<p>Res. 28, 1&ndash;38 (2002)
</p>
<p>[G10] D. Goldfarb, M.J. Todd, Linear programming, in Optimization, ed. by
</p>
<p>G.L. Nemhauser, A.H.G. Rinnooy Kan, M.J. Todd. Handbooks in Opera-
</p>
<p>tions Research and Management Science, vol. 1 (North Holland, Amster-
</p>
<p>dam, 1989), pp. 141&ndash;170
</p>
<p>[G11] D. Goldfarb, D. Xiao, A primal projective interior point method for linear
</p>
<p>programming. Math. Program. 51, 17&ndash;43 (1991)
</p>
<p>[G12] A.A. Goldstein, On steepest descent. SIAM J. Control 3, 147&ndash;151 (1965)
</p>
<p>[G13] C.C. Gonzaga, An algorithm for solving linear programming problems in
</p>
<p>O(n3L) operations, in Progress in Mathematical Programming: Interior
</p>
<p>Point and Related Methods, ed. by N. Megiddo (Springer, New York, 1989),
</p>
<p>pp. 1&ndash;28
</p>
<p>[G14] C.C. Gonzaga, M.J. Todd, An O(
&radic;
nL)-iteration large-step primal-dual
</p>
<p>affine algorithm for linear programming. SIAM J. Optim. 2, 349&ndash;359 (1992)
</p>
<p>[G15] J. Greenstadt, Variations on variable metric methods. Math. Comput. 24,
</p>
<p>1&ndash;22 (1970)
</p>
<p>[H1] G. Hadley, Linear Programming (Addison-Wesley, Reading, MA, 1962)
</p>
<p>[H2] G. Hadley, Nonlinear and Dynamic Programming (Addison-Wesley, Read-
</p>
<p>ing, MA, 1964)
</p>
<p>[H3] S.P. Han, A globally convergent method for nonlinear programming. J. Op-
</p>
<p>tim. Theory Appl. 22(3), 297&ndash;309 (1977)
</p>
<p>[H4] H. Hancock, Theory of Maxima and Minima (Ginn, Boston, 1917)
</p>
<p>[H5] J. Hartmanis, R.E. Stearns, On the computational complexity of algorithms.
</p>
<p>Trans. Am. Math. Soc. 117, 285&ndash;306 (1965)</p>
<p/>
</div>
<div class="page"><p/>
<p>530 Bibliography
</p>
<p>[124] B.S. He, X.M. Yuan, On the O(1/n) convergence rate of the Douglas-
</p>
<p>Rachford alternating direction method. SIAM J. Numer. Anal. 50, 700&ndash;709
</p>
<p>(2012)
</p>
<p>[H6] D. den Hertog, Interior point approach to linear, quadratic and convex pro-
</p>
<p>gramming, algorithms and complexity, Ph.D. thesis, Faculty of Mathematics
</p>
<p>and Informatics, TU Delft, BL Delft, 1992
</p>
<p>[H7] M.R. Hestenes, The conjugate gradient method for solving linear systems,
</p>
<p>in Proceeding of Symposium in Applied Mathematics, vol. VI, Numerical
</p>
<p>Analysis (McGraw-Hill, New York 1956), pp. 83&ndash;102
</p>
<p>[H8] M.R. Hestenes, Multiplier and gradient methods. J. Opt. Theory Appl. 4(5),
</p>
<p>303&ndash;320 (1969)
</p>
<p>[H9] M.R. Hestenes, Conjugate-Direction Methods in Optimization (Springer,
</p>
<p>Berlin, 1980)
</p>
<p>[H10] M.R. Hestenes, E.L. Stiefel, Methods of conjugate gradients for solving lin-
</p>
<p>ear systems. J. Res. Natl. Bur. Stand. Sect. B 49, 409&ndash;436 (1952)
</p>
<p>[H11] F.L. Hitchcock, The distribution of a product from several sources to numer-
</p>
<p>ous localities. J. Math. Phys. 20, 224&ndash;230 (1941)
</p>
<p>[H12] P. Huard, Resolution of mathematical programming with nonlinear con-
</p>
<p>straints by the method of centers, in Nonlinear Programming, ed. by J.
</p>
<p>Abadie (North Holland, Amsterdam, 1967), pp. 207&ndash;219
</p>
<p>[H13] H.Y. Huang, Unified approach to quadratically convergent algorithms for
</p>
<p>function minimization. J. Optim. Theory Appl. 5, 405&ndash;423 (1970)
</p>
<p>[H14] L. Hurwicz, Programming in linear spaces, in Studies in Linear and Non-
</p>
<p>linear Programming, ed. by K.J. Arrow, L. Hurwicz, H. Uzawa (Stanford
</p>
<p>University Press, Stanford, CA, 1958)
</p>
<p>[I1] E. Isaacson, H.B. Keller, Analysis of Numerical Methods (Wiley, New York,
</p>
<p>1966)
</p>
<p>[J1] W. Jacobs, The caterer problem. Naval Res. Logist. Q. 1, 154&ndash;165 (1954)
</p>
<p>[J2] F. Jarre, Interior-point methods for convex programming. Appl. Math. Op-
</p>
<p>tim. 26, 287&ndash;311 (1992)
</p>
<p>[137] W.B. Johnson, J. Lindenstrauss, Extensions of lipshitz mapping into Hilbert
</p>
<p>space. Comtemp. Math. 26, 189&ndash;206 (1984)
</p>
<p>[K1] S. Karlin, Mathematical Methods and Theory in Games, Programming, and
</p>
<p>Economics, vol. I (Addison-Wesley, Reading, MA, 1959)
</p>
<p>[K2] N.K. Karmarkar, A new polynomial-time algorithm for linear programming.
</p>
<p>Combinatorica 4, 373&ndash;395 (1984)
</p>
<p>[K3] J.E. Kelley, The cutting-plane method for solving convex programs. J. Soc.
</p>
<p>Ind. Appl. Math. VIII(4), 703&ndash;712 (1960)
</p>
<p>[K4] L.G. Khachiyan, A polynomial algorithm for linear programming. Doklady
</p>
<p>Akad. Nauk USSR 244, 1093&ndash;1096 (1979). Translated in Soviet Math. Dok-
</p>
<p>lady 20, 191&ndash;194 (1979)
</p>
<p>[K5] V. Klee, G.J. Minty, How good is the simplex method, in Inequalities III,
</p>
<p>ed. by O. Shisha (Academic, New York, 1972)
</p>
<p>[K6] M. Kojima, S. Mizuno, A. Yoshise, A polynomial-time algorithm for a class
</p>
<p>of linear complementarity problems. Math. Program. 44, 1&ndash;26 (1989)</p>
<p/>
</div>
<div class="page"><p/>
<p>Bibliography 531
</p>
<p>[K7] M. Kojima, S. Mizuno, A. Yoshise, An O(
&radic;
nL) iteration potential reduction
</p>
<p>algorithm for linear complementarity problems. Math. Program. 50, 331&ndash;
</p>
<p>342 (1991)
</p>
<p>[K8] T.C. Koopmans, Optimum utilization of the transportation system, in Pro-
</p>
<p>ceedings of the International Statistical Conference, Washington, DC, 1947
</p>
<p>[K9] J. Kowalik, M.R. Osborne, Methods for Unconstrained Optimization Prob-
</p>
<p>lems (Elsevier, New York, 1968)
</p>
<p>[K10] H.W. Kuhn, The Hungarian method for the assignment problem. Naval Res.
</p>
<p>Logist. Q. 2, 83&ndash;97 (1955)
</p>
<p>[K11] H.W. Kuhn, A.W. Tucker, Nonlinear programming, in Proceedings of the
</p>
<p>Second Berkeley Symposium on Mathematical Statistics and Probability,
</p>
<p>ed. by J. Neyman (University of California Press, Berkeley/Los Angeles,
</p>
<p>CA, 1961), pp. 481&ndash;492
</p>
<p>[L1] C. Lanczos, Applied Analysis (Prentice-Hall, Englewood Cliffs, NJ, 1956)
</p>
<p>[150] J.B. Lasserre, Global optimization with polynomials and the problem of
</p>
<p>moments related. SIAM J. Optim. 11, 796&ndash;817 (2001)
</p>
<p>[151] M. Laurent, Matrix completion problems. Encycl. Optim. 3, 221&ndash;229
</p>
<p>(2001)
</p>
<p>[L2] E. Lawler, Combinatorial Optimization: Networks and Matroids (Holt,
</p>
<p>Rinehart, and Winston, New York, 1976)
</p>
<p>[L3] C. Lemarechal, R. Mifflin, Nonsmooth optimization, in IIASA Proceedings
</p>
<p>III (Pergamon Press, Oxford, 1978)
</p>
<p>[L4] C.E. Lemke, The dual method of solving the linear programming problem.
</p>
<p>Naval Res. Logist. Q. 1(1), 36&ndash;47 (1954)
</p>
<p>[L5] E.S. Levitin, B.T. Polyak, Constrained minimization methods. Zh. vychisl.
</p>
<p>Math. Math. Fiz 6(5), 787&ndash;823 (1966)
</p>
<p>[156] M.S. Lobo, L. Vandenberghe, S. Boyd, Applications of second-order cone
</p>
<p>programming. Linear Algebra Appl. 284, 193&ndash;228 (1998)
</p>
<p>[L6] C. Loewner, Über monotone Matrixfunktionen. Math. Zeir. 38, 177&ndash;216
</p>
<p>(1934). Also see C. Loewner, Advanced matrix theory, mimeo notes, Stan-
</p>
<p>ford University, 1957
</p>
<p>[L7] F.A. Lootsma, Boundary properties of penalty functions for constrained
</p>
<p>minimization, Doctoral dissertation, Technical University, Eindhoven, May
</p>
<p>1970
</p>
<p>[159] L. Lovász, A. Shrijver, Cones of matrices and setfunctions, and 0 &minus; 1 opti-
mization. SIAM J. Optim. 1, 166&ndash;190 (1990)
</p>
<p>[160] Z. Lu, L. Xiao, On the complexity analysis of randomized block-coordinate
</p>
<p>descent methods. Math. Program. (2013). doi: 10.1007/s10107-014-0800-2
</p>
<p>[L8] D.G. Luenberger,Optimization by Vector SpaceMethods (Wiley, New York,
</p>
<p>1969)
</p>
<p>[L9] D.G. Luenberger, Hyperbolic pairs in the method of conjugate gradients.
</p>
<p>SIAM J. Appl. Math. 17(6), 1263&ndash;1267 (1969)
</p>
<p>[L10] D.G. Luenberger, A combined penalty function and gradient projec-
</p>
<p>tion method for nonlinear programming, Internal Memo, Department of
</p>
<p>Engineering-Economic Systems, Stanford University (June 1970)</p>
<p/>
</div>
<div class="page"><p/>
<p>532 Bibliography
</p>
<p>[L11] D. G. Luenberger, The conjugate residual method for constrained minimiza-
</p>
<p>tion problems. SIAM J. Numer. Anal. 7(3), 390&ndash;398 (1970)
</p>
<p>[L12] D.G. Luenberger, Control problems with kinks. IEEE Trans. Autom. Con-
</p>
<p>trol AC-15(5), 570&ndash;575 (1970)
</p>
<p>[L13] D.G. Luenberger, Convergence rate of a penalty-function scheme. J. Optim.
</p>
<p>Theory Appl. 7(1), 39&ndash;51 (1971)
</p>
<p>[L14] D.G. Luenberger, The gradient projection method along geodesics. Manag.
</p>
<p>Sci. 18(11), 620&ndash;631 (1972)
</p>
<p>[L15] D.G. Luenberger, Introduction to Linear and Nonlinear Programming, 1st
</p>
<p>edn. (Addison-Wesley, Reading, MA, 1973)
</p>
<p>[L16] D.G. Luenberger, Linear and Nonlinear Programming, 2nd edn. (Addison-
</p>
<p>Wesley, Reading, MA, 1984)
</p>
<p>[L17] D.G. Luenberger, An approach to nonlinear programming. J. Optim. Theory
</p>
<p>Appl. 11(3), 219&ndash;227 (1973)
</p>
<p>[171] Z.Q. Luo, W. Ma, A.M. So, Y. Ye, S. Zhang, Semidefinite relaxation of
</p>
<p>quadratic optimization problems. IEEE Signal Process. Mag. 27, 20&ndash;34
</p>
<p>(2010)
</p>
<p>[L18] Z.Q. Luo, J. Sturm, S. Zhang, Conic convex programming and self-dual
</p>
<p>embedding. Optim. Methods Softw. 14, 169&ndash;218 (2000)
</p>
<p>[L19] I.J. Lustig, R.E. Marsten, D.F. Shanno, On implementing mehrotra&rsquo;s
</p>
<p>predictor-corrector interior point method for linear programming. SIAM J.
</p>
<p>Optim. 2, 435&ndash;449 (1992)
</p>
<p>[M1] N. Maratos, Exact penalty function algorithms for finite dimensional and
</p>
<p>control optimization problems, Ph.D. thesis, Imperial College Sci. Tech.,
</p>
<p>University of London, 1978
</p>
<p>[M2] G.P. McCormick, Optimality criteria in nonlinear programming, in Nonlin-
</p>
<p>ear Programming, SIAM-AMS Proceedings, vol. IX, 1976, pp. 27&ndash;38
</p>
<p>[M3] L. McLinden, The analogue of Moreau&rsquo;s proximation theorem, with appli-
</p>
<p>cations to the nonlinear complementarity problem. Pac. J. Math. 88, 101&ndash;
</p>
<p>161 (1980)
</p>
<p>[M4] N. Megiddo, Pathways to the optimal set in linear programming, in Progress
</p>
<p>in Mathematical Programming: Interior Point and Related Methods, ed. by
</p>
<p>N. Megiddo (Springer, New York, 1989), pp. 131&ndash;158
</p>
<p>[M5] S. Mehrotra, On the implementation of a primal-dual interior point method.
</p>
<p>SIAM J. Optim. 2(4), 575&ndash;601 (1992)
</p>
<p>[M6] S. Mizuno, M.J. Todd, Y. Ye, On adaptive step primal-dual interior point
</p>
<p>algorithms for linear programming. Math. Oper. Res. 18, 964&ndash;981 (1993)
</p>
<p>[180] R.D.C. Monteiro, B.F. Svaiter, Iteration-complexity of block-decomposition
</p>
<p>algorithms and the alternating direction method of multipliers. SIAM J. Op-
</p>
<p>tim. 23, 475&ndash;507 (2013)
</p>
<p>[M7] R.D.C. Monteiro, I. Adler, Interior path following primal-dual algorithms:
</p>
<p>part i: linear programming. Math. Program. 44, 27&ndash;41 (1989)
</p>
<p>[M8] D.D. Morrison, Optimization by least squares. SIAM J. Numer. Anal. 5,
</p>
<p>83&ndash;88 (1968)</p>
<p/>
</div>
<div class="page"><p/>
<p>Bibliography 533
</p>
<p>[M9] B.A. Murtagh, Advanced Linear Programming (McGraw-Hill, New York,
</p>
<p>1981)
</p>
<p>[M10] B.A. Murtagh, R.W.H. Sargent, A constrained minimization method with
</p>
<p>quadratic convergence (Chap. 14), in Optimization, ed. by R. Fletcher (Aca-
</p>
<p>demic, London, 1969)
</p>
<p>[M11] K.G. Murty, Linear and Combinatorial Programming (Wiley, New York,
</p>
<p>1976)
</p>
<p>[M12] K.G. Murty, The Karmarkar&rsquo;s algorithm for linear programming
</p>
<p>(Chap. 11.4.1), in Linear Complementarity, Linear and Nonlinear Program-
</p>
<p>ming. Sigma Series in Applied Mathematics, vol. 3 (Heldermann Verlag,
</p>
<p>Berlin, 1988), pp. 469&ndash;494
</p>
<p>[N1] S.G., Nash, A. Sofer Linear and Nonlinear Programming (McGraw-Hill
</p>
<p>Companies, New York, 1996)
</p>
<p>[188] Y. Nesterov, Efficiency of coordinate descent methods on huge-scale opti-
</p>
<p>mization problems. SIAM J. Optim. 22, 341&ndash;362 (2012)
</p>
<p>[189] Y.E. Nesterov, Semidefinite relaxation and nonconvex quadratic optimiza-
</p>
<p>tion. Optim. Methods Softw. 9, 141&ndash;160 (1998)
</p>
<p>[190] Y. Nesterov, A method of solving a convex programming problem with con-
</p>
<p>vergence rate O((1/k2)). Soviet Math. Doklady 27(2), 372&ndash;376 (1983)
</p>
<p>[191] Y. Nesterov, M.J. Todd, Y. Ye, Infeasible-start primal-dual methods and inf-
</p>
<p>easibility detectors for nonlinear programming problems. Math. Program.
</p>
<p>84, 227&ndash;267 (1999)
</p>
<p>[N2] Y. Nesterov, A. Nemirovskii, Interior Point Polynomial Methods in Convex
</p>
<p>Programming: Theory and Algorithms (SIAM Publications, Philadelphia,
</p>
<p>1994)
</p>
<p>[N3] Y. Nesterov, M.J. Todd, Self-scaled barriers and interior-point methods for
</p>
<p>convex programming. Math. Oper. Res. 22(1) 1&ndash;42 (1997)
</p>
<p>[N4] Y. Nesterov, Introductory Lectures on Convex Optimization: A Basic Course
</p>
<p>(Kluwer, Boston, 2004)
</p>
<p>[O1] W. Orchard-Hays, Background development and extensions of the revised
</p>
<p>simplex method. RAND Report RM-1433, The RAND Corporation, Santa
</p>
<p>Monica, CA (1954)
</p>
<p>[O2] A. Orden, Application of the simplex method to a variety of matrix
</p>
<p>problems, in Directorate of Management Analysis: &ldquo;Symposium on Lin-
</p>
<p>ear Inequalities and Programming&rdquo;, ed. by A. Orden, L. Goldstein
</p>
<p>(DCS/Comptroller, Headquarters, U.S. Air Force, Washington, DC, 1952),
</p>
<p>pp. 28&ndash;50
</p>
<p>[O3] A. Orden, The transshipment problem. Manag. Sci. 2(3), 276&ndash;285 (1956)
</p>
<p>[O4] S.S. Oren, Self-scaling variable metric (ssvm) algorithms ii: implementation
</p>
<p>and experiments. Manag. Sci. 20, 863&ndash;874 (1974)
</p>
<p>[O5] S.S. Oren, D.G. Luenberger, Self-scaling variable metric (ssvm) algorithms
</p>
<p>i: criteria and sufficient conditions for scaling a class of algorithms. Manag.
</p>
<p>Sci. 20, 845&ndash;862 (1974)
</p>
<p>[O6] S.S. Oren, E. Spedicato, Optimal conditioning of self-scaling variable met-
</p>
<p>ric algorithms. Math. Program. 10, 70&ndash;90 (1976)</p>
<p/>
</div>
<div class="page"><p/>
<p>534 Bibliography
</p>
<p>[O7] J.M. Ortega, W.C. Rheinboldt, Iterative Solution of Nonlinear Equations in
</p>
<p>Several Variables (Academic, New York, 1970)
</p>
<p>[P1] C.C. Paige, M.A. Saunders, Solution of sparse indefinite systems of linear
</p>
<p>equations. SIAM J. Numer. Anal. 12(4), 617&ndash;629 (1975)
</p>
<p>[P2] C. Papadimitriou, K. Steiglitz, Combinatorial Optimization Algorithms and
</p>
<p>Complexity (Prentice-Hall, Englewood Cliffs, NJ, 1982)
</p>
<p>[204] P. Parrilo, Semidefinite programming relaxations for semialgebraic prob-
</p>
<p>lems. Math. Program. 96, 293&ndash;320 (2003)
</p>
<p>[205] G. Pataki, On the rank of extreme matrices in semidefinite programs and the
</p>
<p>multiplicity of optimal eigenvalues. Math. Oper. Res. 23, 339&ndash;358 (1998)
</p>
<p>[P3] A. Perry, A modified conjugate gradient algorithm, Discussion Paper No.
</p>
<p>229, Center for Mathematical Studies in Economics and Management Sci-
</p>
<p>ence, North-Western University, Evanston, IL (1976)
</p>
<p>[P4] E. Polak, Computational Methods in Optimization: A Unified Approach
</p>
<p>(Academic, New York, 1971)
</p>
<p>[P5] E. Polak, G. Ribiere, Note sur la Convergence de Methods de Directions
</p>
<p>Conjugres. Revue Francaise Informat. Recherche Operationnelle 16, 35&ndash;
</p>
<p>43 (1969)
</p>
<p>[P6] M.J.D. Powell, An efficient method for finding the minimum of a function
</p>
<p>of several variables without calculating derivatives. Comput. J. 7, 155&ndash;162
</p>
<p>(1964)
</p>
<p>[P7] M.J.D. Powell, A method for nonlinear constraints in minimization prob-
</p>
<p>lems, in Optimization, ed. by R. Fletcher Powell (Academic, London, 1969),
</p>
<p>pp. 283&ndash;298
</p>
<p>[P8] M.J.D. Powell, On the convergence of the variable metric algorithm. Math-
</p>
<p>ematics Branch, Atomic Energy Research Establishment, Harwell, Berk-
</p>
<p>shire, England, (October 1969)
</p>
<p>[P9] M.J.D. Powell, Algorithms for nonlinear constraints that use lagrangian
</p>
<p>functions. Math. Program. 14, 224&ndash;248 (1978)
</p>
<p>[P10] B.N. Pshenichny, Y.M. Danilin, Numerical Methods in Extremal Problems
</p>
<p>(translated from Russian by V. Zhitomirsky) (MIR Publishers, Moscow,
</p>
<p>1978)
</p>
<p>[214] M. Ramana, An exact duality theory for semidefinite programming and its
</p>
<p>complexity implications. Math. Program. 77, 129&ndash;162 (1997)
</p>
<p>[215] M. Ramana, L. Tuncel H. Wolkowicz, Strong duality for semidefinite pro-
</p>
<p>gramming. SIAM J. Optim. 7, 641&ndash;662 (1997)
</p>
<p>[R1] J. Renegar, A polynomial-time algorithm, based on Newton&rsquo;s method, for
</p>
<p>linear programming. Math. Program. 40, 59&ndash;93 (1988)
</p>
<p>[R2] J. Renegar, A Mathematical View of Interior-Point Methods in Convex Op-
</p>
<p>timization (Society for Industrial and Applied Mathematics, Philadelphia,
</p>
<p>2001)
</p>
<p>[R3] R.T. Rockafellar, The multiplier method of hestenes and powell applied to
</p>
<p>convex programming. J. Optim. Theory Appl. 12, 555&ndash;562 (1973)
</p>
<p>[219] R.T. Rockafellar, Convex Analysis (Princeton University Press, Princeton,
</p>
<p>NJ, 1970)</p>
<p/>
</div>
<div class="page"><p/>
<p>Bibliography 535
</p>
<p>[R4] C. Roos, T. Terlaky, J.-Ph. Vial, Theory and Algorithms for Linear Opti-
</p>
<p>mization: An Interior Point Approach (Wiley, Chichester, 1997)
</p>
<p>[R5] J. Rosen, The gradient projection method for nonlinear programming, I. Lin-
</p>
<p>ear contraints. J. Soc. Ind. Appl. Math. 8, 181&ndash;217 (1960)
</p>
<p>[R6] J. Rosen, The gradient projection method for nonlinear programming, II.
</p>
<p>Non-linear constraints. J. Soc. Ind. Appl. Math. 9, 514&ndash;532 (1961)
</p>
<p>[S1] R. Saigal, Linear Programming: Modern Integrated Analysis (Kluwer Aca-
</p>
<p>demic Publisher, Boston, 1995)
</p>
<p>[S2] B. Shah, R. Buehler, O. Kempthorne, Some algorithms for minimizing a
</p>
<p>function of several variables. J. Soc. Ind. Appl. Math. 12, 74&ndash;92 (1964)
</p>
<p>[S3] D.F. Shanno, Conditioning of quasi-Newton methods for function mini-
</p>
<p>mization. Math. Comput. 24, 647&ndash;656 (1970)
</p>
<p>[S4] D.F. Shanno, Conjugate gradient methods with inexact line searches. Math.
</p>
<p>Oper. Res. 3(3) 244&ndash;2560 (1978)
</p>
<p>[S5] A. Shefi, Reduction of linear inequality constraints and determination of
</p>
<p>all feasible extreme points, Ph.D. dissertation, Department of Engineering-
</p>
<p>Economic Systems, Stanford University, Stanford, CA, October 1969
</p>
<p>[228] W.F. Sheppard, On the calculation of the double integral expressing normal
</p>
<p>correlation. Trans. Camb. Philos. Soc. 19, 23&ndash;66 (1900)
</p>
<p>[S6] M. Simonnard, Linear Programming, translated by William S. Jewell
</p>
<p>(Prentice-Hall, Englewood Cliffs, NJ, 1966)
</p>
<p>[S7] M. Slater, Lagrange multipliers revisited: a contribution to non-linear pro-
</p>
<p>gramming. Cowles Commission Discussion Paper, Math 403 (November
</p>
<p>1950)
</p>
<p>[231] A.M. So, Y. Ye, Theory of semidefinite programming for sensor network
</p>
<p>localization. Math. Program. 109, 367&ndash;384 (2007)
</p>
<p>[232] A.M. So, Y. Ye, J. Zhang, A unified theorem on SDP rank reduction. Math.
</p>
<p>Oper. Res. 33, 910&ndash;920 (2008)
</p>
<p>[S8] G. Sonnevend, An &lsquo;analytic center&rsquo; for polyhedrons and new classes of
</p>
<p>global algorithms for linear (smooth, convex) programming, in SystemMod-
</p>
<p>elling and Optimization: Proceedings of the 12th IFIP-Conference held
</p>
<p>in Budapest, Hungary, September 1985, ed. by A. Prekopa, J. Szelezsan,
</p>
<p>B. Strazicky. Lecture Notes in Control and Information Sciences, vol. 84
</p>
<p>(Springer, Berlin, 1986), pp. 866&ndash;876
</p>
<p>[S9] G.W. Stewart, A modification of Davidon&rsquo;s minimization method to accept
</p>
<p>difference approximations of derivatives. J. ACM 14, 72&ndash;83 (1967)
</p>
<p>[S10] E.L. Stiefel, Kernel polynomials in linear algebra and their numerical appli-
</p>
<p>cations. Nat. Bur. Stand. Appl. Math. Ser. 49, 1&ndash;22 (1958)
</p>
<p>[S11] J.F. Sturm, Using SeDuMi 1.02, a MATLAB toolbox for optimization over
</p>
<p>symmetric cones. Optim. Methods Softw. 11&amp;12, 625&ndash;633 (1999)
</p>
<p>[S12] J. Sun, L. Qi, An interior point algorithm of O(
&radic;
n| ln(ǫ)|) iterations for C1-
</p>
<p>convex programming. Math. Program. 57, 239&ndash;257 (1992)
</p>
<p>[T1] A. Tamir, Line search techniques based on interpolating polynomials using
</p>
<p>function values only. Manag. Sci. 22(5), 576&ndash;586 (1976)</p>
<p/>
</div>
<div class="page"><p/>
<p>536 Bibliography
</p>
<p>[T2] K. Tanabe, Complementarity-enforced centered Newton method for mathe-
</p>
<p>matical programming, in New Methods for Linear Programming, ed. by K.
</p>
<p>Tone (The Institute of Statistical Mathematics, Tokyo, 1987), pp. 118&ndash;144
</p>
<p>[T3] R.A. Tapia, Quasi-Newton methods for equality constrained optimization:
</p>
<p>equivalents of existing methods and new implementation, in Symposium on
</p>
<p>Nonlinear Programming III, ed. by O. Mangasarian, R. Meyer, S. Robinson
</p>
<p>(Academic, New York, 1978), pp. 125&ndash;164
</p>
<p>[T4] M.J. Todd, A low complexity interior point algorithm for linear program-
</p>
<p>ming. SIAM J. Optim. 2, 198&ndash;209 (1992)
</p>
<p>[T5] M.J. Todd, Y. Ye, A centered projective algorithm for linear programming.
</p>
<p>Math. Oper. Res. 15, 508&ndash;529 (1990)
</p>
<p>[T6] K. Tone, Revisions of constraint approximations in the successive qp
</p>
<p>method for nonlinear programming problems. Math. Program. 26(2), 144&ndash;
</p>
<p>152 (1983)
</p>
<p>[T7] D.M. Topkis, A note on cutting-plane methods without nested constraint
</p>
<p>sets. ORC 69-36, Operations Research Center, College of Engineering,
</p>
<p>Berkeley, CA (December 1969)
</p>
<p>[T8] D.M. Topkis, A.F. Veinott Jr., On the convergence of some feasible direction
</p>
<p>algorithms for nonlinear programming. J. SIAM Control 5(2), 268&ndash;279
</p>
<p>(1967)
</p>
<p>[T9] J.F. Traub, Iterative Methods for the Solution of Equations (Prentice-Hall,
</p>
<p>Englewood Cliffs, NJ, 1964)
</p>
<p>[T10] L. Tunçel, Constant potential primal-dual algorithms: a framework. Math.
</p>
<p>Prog. 66, 145&ndash;159 (1994)
</p>
<p>[T11] R. Tutuncu, An infeasible-interior-point potential-reduction algorithm for
</p>
<p>linear programming, Ph.D. thesis, School of Operations Research and In-
</p>
<p>dustrial Engineering, Cornell University, Ithaca, NY, 1995
</p>
<p>[T12] P. Tseng, Complexity analysis of a linear complementarity algorithm based
</p>
<p>on a Lyapunov function. Math. Program. 53, 297&ndash;306 (1992)
</p>
<p>[V1] P.M. Vaidya, An algorithm for linear programming which requires O((m +
</p>
<p>n)n2 + (m+ n)1.5nL) arithmetic operations. Math. Prog. 47, 175&ndash;201 (1990).
</p>
<p>Condensed version in: Proceedings of the 19th Annual ACM Symposium
</p>
<p>on Theory of Computing, 1987, pp. 29&ndash;38
</p>
<p>[V2] L. Vandenberghe, S. Boyd, Semidefinite programming. SIAM Rev. 38(1)
</p>
<p>49&ndash;95 (1996)
</p>
<p>[V3] R.J. Vanderbei, Linear Programming: Foundations and Extensions (Kluwer
</p>
<p>Academic Publishers, Boston, 1997)
</p>
<p>[V4] S.A. Vavasis, Nonlinear Optimization: Complexity Issues (Oxford Science,
</p>
<p>New York, NY, 1991)
</p>
<p>[V5] A.F. Veinott Jr., The supporting hyperplane method for unimodal program-
</p>
<p>ming. Oper. Res. XV 1, 147&ndash;152 (1967)
</p>
<p>[V6] Y.V. Vorobyev, Methods of Moments in Applied Mathematics (Gordon and
</p>
<p>Breach, New York, 1965)
</p>
<p>[W1] D.J. Wilde, C.S. Beightler, Foundations of Optimization (Prentice-Hall, En-
</p>
<p>glewood Cliffs, NJ, 1967)</p>
<p/>
</div>
<div class="page"><p/>
<p>Bibliography 537
</p>
<p>[W2] R.B. Wilson, A simplicial algorithm for concave programming, Ph.D. dis-
</p>
<p>sertation, Harvard University Graduate School of Business Administration,
</p>
<p>1963
</p>
<p>[W3] P. Wolfe, A duality theorem for nonlinear programming. Q. Appl. Math. 19,
</p>
<p>239&ndash;244 (1961)
</p>
<p>[W4] P. Wolfe, On the convergence of gradient methods under constraints. IBM
</p>
<p>Research Report RZ 204, Zurich (1966)
</p>
<p>[W5] P. Wolfe, Methods of nonlinear programming (Chap. 6), in Nonlinear
</p>
<p>Programming, ed. by J. Abadie. Interscience (Wiley, New York, 1967),
</p>
<p>pp. 97&ndash;131
</p>
<p>[W6] P. Wolfe, Convergence conditions for ascent methods. SIAM Rev. 11,
</p>
<p>226&ndash;235 (1969)
</p>
<p>[W7] P. Wolfe, Convergence theory in nonlinear programming (Chap. 1), in Inte-
</p>
<p>ger and Nonlinear Programming, ed. by J. Abadie (North-Holland Publish-
</p>
<p>ing Company, Amsterdam, 1970)
</p>
<p>[W8] S.J. Wright, Primal-Dual Interior-Point Methods (SIAM, Philadelphia,
</p>
<p>1996)
</p>
<p>[264] G. Xue, Y. Ye, Efficient algorithms for minimizing a sum of Euclidean
</p>
<p>norms with applications. SIAM J. Optim. 7, 1017&ndash;1036 (1997)
</p>
<p>[265] Y. Ye, Approximating quadratic programming with bound and quadratic
</p>
<p>constraints. Math. Program. 84, 219&ndash;226 (1999)
</p>
<p>[Y1] Y. Ye, An O(n3L) potential reduction algorithm for linear programming.
</p>
<p>Math. Program. 50, 239&ndash;258 (1991)
</p>
<p>[Y2] Y. Ye, M.J. Todd, S. Mizuno, An O(
&radic;
nL)-iteration homogeneous and self-
</p>
<p>dual linear programming algorithm. Math. Oper. Res. 19, 53&ndash;67 (1994)
</p>
<p>[Y3] Y. Ye, Interior Point Algorithms (Wiley, New York, 1997)
</p>
<p>[269] Y. Ye, A new complexity result on solving the markov decision problem.
</p>
<p>Math. Oper. Res. 30, 733&ndash;749 (2005)
</p>
<p>[Z1] W.I. Zangwill, Nonlinear programming via penalty functions. Manag. Sci.
</p>
<p>13(5), 344&ndash;358 (1967)
</p>
<p>[Z2] W.I. Zangwill, Nonlinear Programming: A Unified Approach (Prentice-
</p>
<p>Hall, Englewood Cliffs, NJ, 1969)
</p>
<p>[Z3] Y. Zhang, D. Zhang, On polynomiality of the mehrotra-type predictor-
</p>
<p>corrector interior-point algorithms. Math. Program. 68, 303&ndash;317 (1995)
</p>
<p>[Z4] G. Zoutendijk,Methods of Feasible Directions (Elsevier, Amsterdam, 1960)</p>
<p/>
</div>
<div class="page"><p/>
<p>Index
</p>
<p>A
</p>
<p>Absolute-value penalty function, 421, 423,
424, 444, 479&ndash;481, 489
</p>
<p>Active constraints, 91, 322, 323, 340&ndash;343,
359&ndash;361, 364&ndash;368, 378, 391, 392,
406&ndash;409, 413, 424, 460, 467, 470,
485
</p>
<p>Active set methods, 360&ndash;364, 392, 467, 470,
481, 488, 489
</p>
<p>convergence properties of, 361
Active set theorem, 363&ndash;364
Activity space, 41, 92
Adjacent extreme points, 38&ndash;42
</p>
<p>Aitken δ2 method, 261
Aitken double sweep method, 252
Algorithms
</p>
<p>accelerated steepest descent, 243&ndash;244
arithmetic convergence, 206, 257
BB method, 245
coordinate descent, 252, 253
ellipsoid, 116
Frank&ndash;Wolfe, 359
geometric convergence, 206
interior, 123
interior-point, 116, 123, 137, 138, 142,
</p>
<p>149, 166, 170&ndash;173
iterative, 6&ndash;8, 116, 179, 186, 192, 193,
</p>
<p>197&ndash;198, 207, 209, 226, 229, 257,
260, 261, 314, 469
</p>
<p>line search, 135, 214&ndash;229, 257, 259, 277,
281, 297, 305, 463
</p>
<p>maximal flow, 98, 99, 113
2nd-order method, 222&ndash;223
Newton&rsquo;s method, 125, 213, 247, 249, 253,
</p>
<p>257, 307&ndash;309, 417
</p>
<p>path-following, 130, 131, 134
polynomial time, 7, 116, 118, 209
potential reduction, 130, 134&ndash;137, 493
randomized coordinate descent, 254&ndash;256,
</p>
<p>262
simplex method, 33&ndash;81, 115&ndash;119, 130,
</p>
<p>131, 137, 138, 142, 143, 378, 391,
392, 469
</p>
<p>steepest descent, 204, 213, 229, 257, 280,
282, 287, 417
</p>
<p>0th-order method, 214&ndash;218
1th-order method, 218&ndash;222
transportation, 66&ndash;67
</p>
<p>Alternating direction method of multipliers,
454&ndash;458
</p>
<p>Analytic center, 116, 123&ndash;130, 143, 144,
182
</p>
<p>Arcs, 95, 96, 98, 113, 169, 170, 373. See also
Nodes
</p>
<p>artificial, 170
basic, 6, 169
</p>
<p>Armijo rule, 228, 230
Artificial variables, 50&ndash;52, 75&ndash;77, 105,
</p>
<p>140
Assignment problem, 80, 107, 108
Associated restricted dual, 103, 105, 114
Associated restricted primal, 102&ndash;104,
</p>
<p>106
Asymptotic convergence, 239, 278, 361,
</p>
<p>372
Augmented Lagrangian methods, 429,
</p>
<p>445&ndash;452, 454, 457
Augmenting path, 95, 96, 98
Average convergence ratio, 207, 208, 211
Average rates, 206&ndash;207
</p>
<p>&copy; Springer International Publishing Switzerland 2016
</p>
<p>D.G. Luenberger, Y. Ye, Linear and Nonlinear Programming, International
Series in Operations Research &amp; Management Science 228,
DOI 10.1007/978-3-319-18842-3
</p>
<p>539</p>
<p/>
</div>
<div class="page"><p/>
<p>540 Index
</p>
<p>B
</p>
<p>Back substitution, 62, 72
</p>
<p>Backtracking, 248, 251
Barrier methods, 116, 131&ndash;132, 134, 143, 170,
</p>
<p>247, 249, 250, 257, 397&ndash;428, 467,
470, 485. See also Penalty methods
</p>
<p>Barrier problem, 126, 128, 132, 133, 398, 413,
424, 485, 486
</p>
<p>Basic feasible solution, 20&ndash;25, 33, 38&ndash;42, 44,
45, 47, 49&ndash;53, 58&ndash;60, 64&ndash;67,
72&ndash;77, 79, 88, 89, 100, 118
</p>
<p>Basic variables, 19, 20, 35, 36, 38, 39, 43, 44,
46&ndash;48, 50, 54, 61&ndash;67, 72, 78,
379&ndash;381, 386, 395
</p>
<p>Basis Triangularity theorem, 60&ndash;62
Big-M method, 77, 112, 139
</p>
<p>Bland&rsquo;s rule, 49, 80, 81
Block-angular structure, 68
</p>
<p>Bordered Hessian test, 338, 353
Broyden family, 293&ndash;296, 303, 305
</p>
<p>Broyden&ndash;Fletcher&ndash;Goldfarb&ndash;Shanno
update, 294
</p>
<p>Broyden method, 295&ndash;297, 300
Bug, 371, 372, 383, 384
</p>
<p>C
</p>
<p>Canonical form, 34&ndash;38, 45, 47, 48, 50, 55, 139
Canonical rate, 8, 372, 398, 413, 415&ndash;418,
</p>
<p>420, 424, 441, 464, 483, 484, 489
</p>
<p>Capacitated networks, 16
</p>
<p>Carathéodory theorem, 20, 166
Caterer problem, 78
</p>
<p>Cauchy&ndash;Schwarz inequality, 291
Central path, 116, 125&ndash;135, 137, 143, 144,
</p>
<p>146, 171&ndash;173, 175, 410, 426, 486,
489, 493
</p>
<p>dual, 125&ndash;130, 143
primal-dual, 129&ndash;130, 133&ndash;134, 470, 486
</p>
<p>Chain, 329&ndash;331, 339, 387&ndash;390, 445
hanging, 329&ndash;332, 386, 396, 443
</p>
<p>Cholesky factorization, 210, 249
Closed mappings, 199&ndash;201, 314, 381
</p>
<p>Closed set, 156, 157, 200
Combinatorial auction, 18, 108
</p>
<p>Compact set, 201, 202, 253, 314, 475
Complementary formula, 293
</p>
<p>Complementary slackness, 92&ndash;94, 99, 102,
141, 341, 349, 434, 467, 468, 480
</p>
<p>Complexity theory, 116&ndash;118, 208
Concave functions, 179, 188&ndash;192
</p>
<p>Condition number, 237, 240, 241, 261, 297,
301, 306, 312, 314, 389, 396, 452
</p>
<p>Cone
</p>
<p>dual, 150, 155, 174
</p>
<p>interior, 154
</p>
<p>self-dual, 172
</p>
<p>Conic linear programming (CLP), 149&ndash;175
</p>
<p>compact form, 151, 158
</p>
<p>duality, 158&ndash;166, 168, 171, 173, 174
</p>
<p>duality gap, 162, 163, 166, 171
</p>
<p>dual problem, 158
</p>
<p>facility location, 159, 160
</p>
<p>Farkas&rsquo; lemma, 54&ndash;157, 164, 173
</p>
<p>infeasibility certificate, 155, 173
</p>
<p>interior-point algorithm, 149, 166,
170&ndash;173
</p>
<p>linear programming, 149&ndash;175
</p>
<p>matrix to vector operator, 151, 155
</p>
<p>optimality conditions, 164
</p>
<p>p-order cone programming, 150, 151, 154,
172, 174
</p>
<p>potential reduction algorithm, 134
</p>
<p>SDP, 149&ndash;154, 158, 159, 161&ndash;163,
165&ndash;171, 174, 175
</p>
<p>second-order cone programming, 150&ndash;152,
154, 155, 159&ndash;162, 170
</p>
<p>strong duality, 162&ndash;165, 168, 173
</p>
<p>vector to matrix operator, 151, 155
</p>
<p>weak duality, 162, 164, 174
</p>
<p>Conjugate direction method, 263&ndash;283, 292,
473, 488
</p>
<p>algorithm, 263, 264, 266, 269&ndash;270, 277,
278, 280, 281
</p>
<p>descent properties of, 266&ndash;268
</p>
<p>theorem, 265&ndash;276, 278&ndash;281
</p>
<p>Conjugate gradient method, 263, 266,
268&ndash;283, 290, 293, 296, 297, 300,
304&ndash;306, 312, 316, 390, 413&ndash;415,
424, 451, 473, 490
</p>
<p>algorithm, 269, 277, 414
</p>
<p>non-quadratic, 268, 276&ndash;279
</p>
<p>PARTAN, 280, 281, 390
</p>
<p>partial, 273&ndash;276, 279, 283, 296, 306, 316,
413, 424
</p>
<p>theorem, 269, 270, 273, 278
</p>
<p>Conjugate residual method, 490
</p>
<p>Constrained problems, 2&ndash;5, 241, 274, 323,
333, 335, 341, 342, 344, 348, 360,
361, 363, 372, 384, 390, 397, 398,
403, 407, 411&ndash;412, 417&ndash;424, 429,
437, 440, 441, 445, 450, 469, 483,
485, 488, 492
</p>
<p>Constraints
</p>
<p>active, 91, 322, 323, 340&ndash;343, 359&ndash;361,
364&ndash;368, 378, 391, 392, 406&ndash;409,
413, 424, 460, 467&ndash;468, 470, 485
</p>
<p>inactive, 322, 361, 364, 365</p>
<p/>
</div>
<div class="page"><p/>
<p>Index 541
</p>
<p>inequality, 190, 340&ndash;344, 348&ndash;349, 359,
360, 379, 381, 392, 398, 401, 405,
413, 414, 420, 423, 430, 433, 439,
443, 452&ndash;454, 467, 468, 470, 471,
475&ndash;478, 481, 489, 492
</p>
<p>nonnegativity, 14, 15, 329, 347, 391
</p>
<p>quadratic, 161, 162
</p>
<p>redundant, 62, 72, 107
</p>
<p>Consumer surplus, 332
</p>
<p>Control example, 149, 332
</p>
<p>Convergence
</p>
<p>analysis, 7&ndash;8, 209, 224, 234, 242, 252, 274,
278, 279, 287, 313, 391, 481, 483
</p>
<p>average order of, 207
</p>
<p>canonical rate of, 8, 398, 413, 415, 416,
420, 424, 483
</p>
<p>of descent algorithms, 196&ndash;204
</p>
<p>dual canonical rate of, 429, 441, 464
</p>
<p>of ellipsoid method, 122, 142
</p>
<p>geometric, 206
</p>
<p>global theorem, 430
</p>
<p>linear, 205&ndash;206, 208, 233, 239, 257, 309,
313
</p>
<p>of Newton&rsquo;s method, 223, 246&ndash;253, 257,
260, 278, 285&ndash;287, 308, 309, 311,
313, 314, 412, 413, 475, 478, 480,
484, 488
</p>
<p>order, 208, 225
</p>
<p>of partial C&ndash;G, 224, 279
</p>
<p>of penalty and barrier methods, 397&ndash;400,
403, 412&ndash;418, 420, 424, 425, 427
</p>
<p>of quasi-Newton, 285&ndash;287, 292&ndash;293,
296&ndash;299, 306, 308, 309, 311&ndash;316
</p>
<p>rate, 7, 8, 204, 207, 236, 240&ndash;242,
253&ndash;254, 256, 259, 262, 278, 285,
286, 311, 314, 358, 370&ndash;378,
383&ndash;390, 396, 414, 440&ndash;441, 452
</p>
<p>ratio, 31, 205&ndash;208, 211, 217, 236, 237,
239, 242, 257, 261, 312, 372, 413,
414, 429
</p>
<p>speed of, 229&ndash;233, 254&ndash;256, 455.&ndash;457
</p>
<p>arithmetic convergence, 206, 232, 257,
457
</p>
<p>linear convergence, 233
</p>
<p>order of convergence, 205, 207, 208,
</p>
<p>218&ndash;220, 222, 223, 225, 239, 246,
249, 257, 258
</p>
<p>superlinear convergence, 206, 207, 297,
300, 312, 313, 391
</p>
<p>of steepest descent, 229&ndash;242, 247&ndash;248,
252&ndash;254, 256, 257, 259&ndash;261, 274,
276, 278, 282, 285, 286, 308, 309,
311&ndash;313, 414
</p>
<p>superlinear, 206, 207, 296, 297, 300, 312,
313, 391
</p>
<p>theory of, 239&ndash;243, 306
</p>
<p>of vectors, 6, 207&ndash;208
Convex cones, 87, 149&ndash;150, 154, 170, 173,
</p>
<p>174
barrier function, 170
</p>
<p>conic-inequality, 150
interior of cone, 154
</p>
<p>nonnegative orthant, 149, 151
p-order cone, 150, 154
</p>
<p>product of cones, 150
second-order cone, 150, 170
</p>
<p>semidefinite matrix, 149, 150
Convex duality, 351
</p>
<p>Convex functions, 143, 188&ndash;195, 210, 266,
344, 346, 348, 454, 455, 457, 458,
460&ndash;462, 486, 487
</p>
<p>Convex polyhedron, 115
</p>
<p>Convex polytope, 23, 24
</p>
<p>Convex programing problem, 344, 350, 354,
441, 460, 461, 464, 471
</p>
<p>Convex sets, 22&ndash;24, 30, 41, 87, 88, 124, 156,
157, 188, 190, 192&ndash;195, 210, 346,
350, 430, 432, 454, 457, 458
</p>
<p>theory of, 22, 23
Convex simplex method, 391&ndash;392
</p>
<p>Coordinate descent, 252&ndash;256, 261, 262,
429
</p>
<p>Cubic fit, 221&ndash;222, 258
</p>
<p>Curve fitting, 214&ndash;215, 217&ndash;222, 224&ndash;226,
228, 245, 257, 481
</p>
<p>Cutting plane methods, 143, 458&ndash;463, 465
</p>
<p>Cycle, in linear programming, 80
Cyclic coordinate descent, 252, 253
</p>
<p>D
</p>
<p>Damping, 246&ndash;248, 250, 251
</p>
<p>Dantzig&ndash;Wolfe decomposition method, 68, 81
Data analysis procedures, 332
</p>
<p>Davidon&ndash;Fletcher&ndash;Powell method (DFP),
290&ndash;294, 300, 315
</p>
<p>Decision problems, 1, 15, 332
</p>
<p>Decomposition, 68&ndash;71, 443&ndash;445, 484
LP, 68&ndash;71
</p>
<p>LU, 55&ndash;56, 210
Deflected gradients, 286
</p>
<p>Degeneracy, 39, 49, 50, 67&ndash;68, 74, 100, 104,
115
</p>
<p>Descent
algorithms, 196&ndash;204, 213, 252, 253, 257,
</p>
<p>260, 312, 420, 423
</p>
<p>function, 198, 199, 201&ndash;204, 209, 224,
226, 229, 249, 253, 477, 481</p>
<p/>
</div>
<div class="page"><p/>
<p>542 Index
</p>
<p>Diet problem, 14, 45, 85, 93, 94, 101
dual of, 85
</p>
<p>Differentiable convex functions, properties of,
190&ndash;192
</p>
<p>Directed graphs, 250
Duality, 83&ndash;114, 125, 130, 140, 158&ndash;166, 168,
</p>
<p>351, 429&ndash;465
asymmetric form of, 85
canonical convergence rate, 429, 441, 464
central path, 127&ndash;129
feasible, 91, 100&ndash;103, 112, 126, 127,
</p>
<p>129&ndash;131, 133, 140, 163
function defined, 431, 437, 439, 440,
</p>
<p>450&ndash;452
gap, 130, 131, 134, 135, 162, 163, 166,
</p>
<p>171, 430, 432&ndash;435
linear program, 83&ndash;86, 106, 158, 182
local, 430, 435&ndash;440, 445, 450
simplex method, 100&ndash;102, 111, 112, 131
theorem, 86&ndash;89, 94, 99, 104, 163&ndash;165,
</p>
<p>174, 326, 353, 410, 429, 430, 432,
433, 436, 438, 439, 446, 450, 464
</p>
<p>E
</p>
<p>Economic interpretation
of Dantzig&ndash;Wolfe decomposition, 68, 81
of decomposition, 71
of primal-dual algorithm, 102
of relative cost coefficients, 83
of simplex multipliers, 92
</p>
<p>Eigenvalues, 120, 167, 208, 233, 272, 287,
335, 370, 406, 440, 483
</p>
<p>interlocking, 241, 300&ndash;302, 389
steepest descent rate, 240
in tangent space, 491
</p>
<p>Eigenvector, 120, 234, 259, 272, 282, 293,
301, 303, 311, 335, 336, 451
</p>
<p>Ellipsoid method, 116, 119&ndash;122, 142, 143
Entropy, 328, 329, 352, 395, 464
Epigraph, 194, 195, 346, 349, 351, 432
Error
</p>
<p>function, 207, 208, 211, 236, 490
tolerance, 369
</p>
<p>Exact penalty theorem, 422, 427
Expanding subspace theorem, 266&ndash;268, 270,
</p>
<p>271, 281
Exponential density, 329
</p>
<p>Extreme point, 23&ndash;26, 28, 30, 33, 38&ndash;42, 69,
71, 91, 108, 193, 194
</p>
<p>F
</p>
<p>False position method, 218&ndash;222, 245, 257
Feasible direction methods, 358&ndash;360, 392
Feasible solution, 20, 33, 87, 118, 153, 364,
</p>
<p>398, 460
Fibonacci search method, 214&ndash;217
First-order necessary conditions, 180&ndash;182,
</p>
<p>186&ndash;187, 193, 194, 210, 259, 260,
326&ndash;328, 330, 333, 337, 340&ndash;342,
352, 379, 395, 438, 464, 467, 470,
473, 475, 476, 478, 479, 481, 486,
488
</p>
<p>Fletcher&ndash;Reeves method, 278, 281
Free variables, 13, 52&ndash;53, 85, 100
Full rank assumption, 20
</p>
<p>G
</p>
<p>Game theory, 108&ndash;109
Gaussian elimination, 34, 60, 73, 168, 208, 282
Gauss&ndash;Southwell method, 252&ndash;254, 260
Generalized reduced gradient method, 382
Geodesics, 371&ndash;375, 377, 395
Geometric convergence, 206, 233, 257
Global convergence, 7, 196&ndash;204, 209,
</p>
<p>224&ndash;226, 229&ndash;239, 253, 257, 260,
261, 278, 281, 296, 308, 312, 316,
381, 391, 392, 395, 396, 400, 427,
463, 465, 469, 475, 480, 489
</p>
<p>theorem, 224, 225, 481
Global duality, 430&ndash;435, 464
Global minimum points, 180, 182, 193, 209,
</p>
<p>210, 259, 260, 354, 363, 470
Golden section ratio, 217
Goldstein test, 259
Gradient projection method, 364&ndash;378, 382,
</p>
<p>386, 389, 390, 392, 394, 395,
417&ndash;420
</p>
<p>convergence rate of the, 370&ndash;378, 383&ndash;390
Graph, 73, 113, 154, 188, 195, 199, 200, 203,
</p>
<p>250
</p>
<p>H
</p>
<p>Half space, 69, 90, 459&ndash;461, 463
Hanging chain, 329&ndash;332, 386&ndash;390, 396,
</p>
<p>443&ndash;445
Hessian matrix, 159, 186, 187, 192, 237, 239,
</p>
<p>246, 247, 251, 254, 262, 279, 285,
338, 342&ndash;343, 372, 386, 406&ndash;408,
414, 451, 470</p>
<p/>
</div>
<div class="page"><p/>
<p>Index 543
</p>
<p>Hessian of dual, 425, 437&ndash;438, 440, 444, 451,
464
</p>
<p>Hessian of the Lagrangian, 351, 357, 370, 385,
387, 392, 406&ndash;408, 424, 436&ndash;439,
444, 447, 450, 471, 472, 474, 477,
478
</p>
<p>Homogeneous self-dual algorithm (HSD),
139&ndash;142, 172, 173
</p>
<p>infeasibility certificate CLP, 173
infeasibility certificate LP, 142
</p>
<p>optimal solution CLP, 172, 173
optimal solution LP, 141
</p>
<p>Hyperplanes, 17, 86, 88, 157, 194, 195, 346,
347, 349, 351, 430&ndash;432, 459&ndash;463,
465
</p>
<p>I
</p>
<p>Implicit function theorem, 325, 339, 345, 436
Inaccurate line search, 227&ndash;228, 258, 296,
</p>
<p>299, 300, 393
</p>
<p>Incidence matrix, 94, 95
Initialization, 137&ndash;142, 172&ndash;173
Integrality gap, 72
</p>
<p>Interior-point methods, 3, 6, 7, 115&ndash;145,
249&ndash;251, 257, 369, 402, 485&ndash;489
</p>
<p>Interlocking eigenvalues lemma, 241, 300, 389
Iterative algorithm, 6&ndash;8, 116, 179, 197&ndash;198,
</p>
<p>207, 209, 229, 257, 260, 314, 469
</p>
<p>J
</p>
<p>Jacobian matrix, 325, 339, 485
Jamming, 360, 363, 378, 381, 392, 413
</p>
<p>K
</p>
<p>Kantorovich inequality, 235, 236, 287, 311,
377
</p>
<p>Karush&ndash;Kuhn&ndash;Tucker conditions, 5, 340&ndash;341,
365, 366, 393&ndash;395
</p>
<p>Kelley&rsquo;s convex cutting plane algorithm,
460&ndash;463, 465
</p>
<p>Khachiyan&rsquo;s ellipsoid method, 116
</p>
<p>L
</p>
<p>Lagrange multiplier, 5, 125, 126, 128, 321,
326, 328&ndash;331, 338, 339, 341&ndash;344,
346&ndash;351, 361&ndash;363, 365, 374, 387,
398, 405&ndash;406, 408&ndash;410, 422, 426,
429, 430, 432&ndash;434, 436, 438&ndash;440,
446, 450, 453, 464, 476, 478, 481,
482, 492
</p>
<p>Levenberg&ndash;Marquardt type methods, 249
Limit superior, 205
</p>
<p>Linear convergence, 205&ndash;208, 233, 239, 257,
309, 313
</p>
<p>Linear programing, 2, 11, 35, 83, 115, 149,
182, 226, 326, 359, 402, 430, 469
</p>
<p>analytic center, 123, 182
</p>
<p>central path, 125, 130, 133, 172, 173, 410
</p>
<p>complementarity, 83&ndash;86, 88&ndash;90, 92, 93, 99,
100, 102, 106&ndash;110, 112, 113, 138,
166&ndash;171
</p>
<p>duality, 83&ndash;86, 88&ndash;90, 92, 93, 99, 100, 102,
106&ndash;110, 112, 113, 125, 158&ndash;166,
430, 459&ndash;464
</p>
<p>examples of, 2, 12, 14&ndash;19, 332
</p>
<p>fundamental theorem of, 20&ndash;22, 24, 166,
173
</p>
<p>potential function, 170, 173, 487, 488
</p>
<p>presolver, 72
Linear variety, 34, 266, 307, 316
</p>
<p>Line search, 135, 213&ndash;229, 237&ndash;239, 245, 248,
253, 254, 257&ndash;260, 277&ndash;278, 281,
283, 291, 295&ndash;300, 302, 305, 312,
314, 359, 392, 393, 461
</p>
<p>Lipschitz condition, 255, 493
Local convergence, 7, 208, 209, 224, 253&ndash;254,
</p>
<p>278, 297&ndash;299
</p>
<p>Local duality, 435&ndash;440, 445
Local minimum point, 180, 246, 260, 322, 331,
</p>
<p>351, 354, 436, 444, 446, 447, 470
</p>
<p>Logarithmic barrier method, 143, 250, 257,
409, 410, 485&ndash;487
</p>
<p>LU decomposition, 55&ndash;56, 210
</p>
<p>M
</p>
<p>Manufacturing problem, 14, 104
Mappings, 197&ndash;201, 226, 249, 259, 314, 381,
</p>
<p>392&ndash;394, 490
</p>
<p>Marginal price, 93, 94, 338
Markowitz portfolio model, 165
</p>
<p>Marriage problem, 80
</p>
<p>Master problem, 69&ndash;71
Matrix
</p>
<p>Frobenius norm, 150
inner product, 150, 289
</p>
<p>notation, 57
</p>
<p>positive definite, 120, 149, 154, 161, 192,
208, 210, 233, 235, 246&ndash;249, 259,
263, 274, 275, 281, 290, 295, 303,
305, 309, 314, 315, 337, 339, 354,
386, 404, 407, 415, 446, 447, 468,
470, 471, 473, 474, 482, 492
</p>
<p>projection matrix, 338, 365, 366, 369, 376,
394
</p>
<p>Max flow-min cut theorem, 94&ndash;99
Maximal flow, 16, 94, 95, 97&ndash;99, 113
</p>
<p>Mean value theorem, 220, 422
</p>
<p>Memoryless quasi-Newton method, 304&ndash;306</p>
<p/>
</div>
<div class="page"><p/>
<p>544 Index
</p>
<p>Minimum point, 179&ndash;182, 185&ndash;188, 192, 193,
201, 209, 210, 213, 214, 217, 218,
221, 222, 224, 226, 227, 229, 233,
235, 246, 253, 260, 280, 282, 286,
291, 293, 307, 322, 331, 337,
340&ndash;344, 351, 354, 360, 362, 363,
398, 409, 412, 422, 436, 444&ndash;448,
463, 470
</p>
<p>Morrison&rsquo;s method, 426&ndash;427
Multiplier methods, 429, 449&ndash;454
</p>
<p>N
</p>
<p>Newton&rsquo;s method, 125, 173, 213, 263, 285,
390, 410, 451, 470
</p>
<p>modified, 248, 257, 260, 286&ndash;288, 305,
306, 312&ndash;314, 412, 413, 424,
477&ndash;481, 483, 484
</p>
<p>Node-arc incidence matrix, 94, 95
Nodes, 16, 94&ndash;96, 98
Nondegeneracy, 20, 39, 44, 46, 47, 49, 58, 75,
</p>
<p>93, 138, 343, 363, 378, 382, 395,
406
</p>
<p>Nonextremal variable, 114
Normalizing constraint, 139, 140
Northwest corner rule, 59&ndash;61, 64, 66, 67
Null variables, 114
</p>
<p>O
</p>
<p>Oil refinery problem, 75
Optimal control, 5, 149, 332, 353
Optimal feasible solution, 20&ndash;22, 33
Order of convergence, 205&ndash;208, 211,
</p>
<p>218&ndash;220, 222, 223, 225, 239, 246,
249, 257, 258
</p>
<p>Orthogonal complement, 418, 440, 441
Orthogonal matrix, 51, 166, 264
</p>
<p>P
</p>
<p>Parallel tangents method. See PARTAN
Parimutuel auction, 18
PARTAN, 275&ndash;277, 376
</p>
<p>advantages and disadvantages of, 277
theorem, 276&ndash;277
</p>
<p>Partial conjugate gradient method, 273&ndash;276,
279, 296, 316, 413
</p>
<p>Partial duality, 440
Partial quasi-Newton method, 296
Path-following, 126, 127, 129&ndash;131, 374, 472,
</p>
<p>499
Penalty methods, 241, 397, 398, 404&ndash;406,
</p>
<p>408, 412&ndash;428, 445, 446, 448, 453,
454, 470, 479&ndash;482, 489, 492
</p>
<p>interpretation of, 423
normalization of, 415&ndash;417
</p>
<p>Percentage test, 259
Pivoting, 33, 35, 36, 41, 47, 51, 55, 73, 104,
</p>
<p>106
Pivot transformations, 54
Point-to-set mappings, 197&ndash;200, 211
Polak&ndash;Ribiere method, 278, 305
Polyhedron, 24, 69, 115, 119
Polynomial time, 7, 116, 118&ndash;119, 138, 143,
</p>
<p>209
Polytopes, 23, 24, 69, 124, 458&ndash;461, 463, 465
Portfolio analysis, 332
Positive definite matrix, 120, 154, 161, 264,
</p>
<p>295, 312, 407
Potential function, 123, 124, 134, 135, 142,
</p>
<p>143, 170&ndash;173, 470, 487&ndash;488
conic linear programming, 170
convex quadratic programming, 486&ndash;487
linear programming, 142
</p>
<p>Power generating example, 184&ndash;185
Preconditioning, 306
Predictor&ndash;corrector method, 134, 143
Primal central path, 126, 128, 131
Primal-dual
</p>
<p>algorithm for LP, 102&ndash;106
central path, 129&ndash;130, 133, 470, 486
methods, 102, 103, 105&ndash;106, 467&ndash;493
optimality theorem, 103
path, 128&ndash;130, 133&ndash;134
</p>
<p>potential function, 134&ndash;135, 171
Primal method, 345, 348, 349, 351, 357&ndash;397,
</p>
<p>410&ndash;412, 422, 423, 430, 432, 435,
441, 448, 449, 464, 469
</p>
<p>advantage of, 357&ndash;358
Projected Hessian test, 338, 353
Projection matrix, 338, 365, 366, 369, 376,
</p>
<p>394
Purification procedure, 138
</p>
<p>Q
</p>
<p>Quadratic
approximation, 276&ndash;277
binary optimization, 152&ndash;153
fit method, 218&ndash;221
minimization problem, 264, 271
penalty function, 412, 414, 415, 421, 428,
</p>
<p>448, 479, 481, 482, 489
program, 262, 349, 354, 396, 434, 468,
</p>
<p>470, 473, 474, 476&ndash;483, 486&ndash;489,
492, 493
</p>
<p>Schur complements, 161
second-order cone program, 151, 152</p>
<p/>
</div>
<div class="page"><p/>
<p>Index 545
</p>
<p>semidefinite program, 158, 159, 161
semidefinite relaxation, 152, 153, 159, 168
</p>
<p>Quasi-Newton methods, 285&ndash;316, 390, 391,
451
</p>
<p>memoryless, 304&ndash;306
</p>
<p>R
</p>
<p>Rank, 19&ndash;21, 23, 30, 50, 86, 124, 137,
152&ndash;154, 166&ndash;171, 173&ndash;175, 241,
289&ndash;290, 293&ndash;295, 309, 315, 316,
325, 337, 338, 345, 354, 364, 407,
419, 424, 427, 438, 468, 470, 471,
474, 485, 489
</p>
<p>Rank-one correction, 289&ndash;290, 315
Rank-reduction procedure, 153, 154, 167
Rank-two correction, 290, 294
Rate of convergence, 186, 203, 234, 238&ndash;242,
</p>
<p>247&ndash;248, 250, 253, 254, 259, 260,
274&ndash;276, 282, 286, 309, 313, 314,
316, 335, 357, 370, 372, 375&ndash;378,
383, 384, 386, 392, 395, 396, 398,
413&ndash;417, 420, 424&ndash;425, 427, 441,
448, 450, 451, 464, 481, 483&ndash;484,
491
</p>
<p>Real number
arithmetic model, 118
sets of, 350, 404
</p>
<p>Recursive quadratic programing, 470,
476&ndash;481, 483, 489, 492&ndash;493
</p>
<p>Reduced cost coefficients, 44, 68
</p>
<p>Reduced gradient method, 378&ndash;392, 395, 396
convergence rate of the, 383&ndash;390
</p>
<p>Redundant equations, 58, 61
Relative cost coefficients, 44&ndash;47, 55, 63, 64,
</p>
<p>66, 68, 71, 72, 74, 78, 79, 83, 92,
105, 112
</p>
<p>Requirements space, 41&ndash;42, 90, 91
Revised simplex method, 55&ndash;56, 62, 68, 70,
</p>
<p>71, 77, 92
Robust set, 401
</p>
<p>S
</p>
<p>Scaling, 171, 195, 241&ndash;243, 253, 279, 298,
300&ndash;304, 306, 311&ndash;313, 315,
331&ndash;332, 340, 347, 395, 441, 484,
493
</p>
<p>SDP relaxation
approximation ratio, 168&ndash;169, 175
quadratic optimization, 152&ndash;153
rank-d solution, 154
rank-1 solution, 153, 168
</p>
<p>Search by golden section, 214&ndash;218
Second-order conditions, 179, 185&ndash;188,
</p>
<p>333&ndash;335, 337, 342&ndash;343, 351, 352
Self-concordant function, 251&ndash;252, 262
Self-dual linear program, 110, 139, 140
Semidefinite programing (SDP), 149&ndash;154,
</p>
<p>158&ndash;159, 161&ndash;163, 165&ndash;171, 174,
175, 470
</p>
<p>central path, 171, 175
complementarity conditions, 166&ndash;170
exact rank reduction, 153, 154
primal-dual potential function, 171
randomized binary reduction, 169&ndash;170
randomized rank reduction, 153, 154
solution rank, 166&ndash;170
</p>
<p>Sensitivity, 92&ndash;94, 208, 297, 300, 338&ndash;339,
343&ndash;344, 351, 362, 411, 429
</p>
<p>Sensor network localization, 153&ndash;154, 159,
173, 174
</p>
<p>Separable problem, 441&ndash;445, 464
Separating hyperplane theorem, 86, 157, 195,
</p>
<p>346
Sets, 12, 22, 23, 96, 123, 128, 129, 143, 188,
</p>
<p>190, 195, 344, 346, 349, 351, 363,
401, 430, 454, 457, 487
</p>
<p>Sherman&ndash;Morrison formula, 294, 451
Simple merit function, 470&ndash;472, 475, 477,
</p>
<p>479, 488
Simplex method, 3, 6, 7, 13, 20, 33&ndash;81, 83, 86,
</p>
<p>95, 108, 111, 112, 115, 116, 118,
</p>
<p>119, 130, 131, 137, 138, 142, 143,
378, 469
</p>
<p>for dual, 33, 39, 41, 54, 83, 86, 95, 98&ndash;102,
108, 111, 112, 130, 131, 138
</p>
<p>and dual problem, 83&ndash;90, 93, 94, 99, 100,
102, 104, 107, 109, 111, 114
</p>
<p>and LU decomposition, 55&ndash;56
matrix form of, 54&ndash;55
for minimum cost flow, 71, 72
revised, 55&ndash;56, 62, 68, 70, 71, 77
for transportation problems, 56&ndash;68
</p>
<p>Simplex multipliers, 62&ndash;64, 66, 67, 70, 79, 92,
93, 100
</p>
<p>Simplex tableau, 45, 46, 52, 55, 89, 100, 103,
105
</p>
<p>Slack variables, 12, 17, 26, 47&ndash;49, 51, 89, 100,
112, 119, 124, 125, 127, 128, 162,
382
</p>
<p>Slack vector, 124, 140
Slater condition, 348, 349, 354
Spacer step, 203&ndash;204, 209, 278, 296</p>
<p/>
</div>
<div class="page"><p/>
<p>546 Index
</p>
<p>Steepest descent, 204, 213, 229&ndash;245, 247&ndash;248,
252&ndash;254, 256, 257, 259&ndash;263, 268,
269, 272&ndash;274, 276, 278&ndash;282,
285&ndash;287, 297&ndash;299, 306&ndash;314, 316,
364, 371, 372, 379, 384, 392, 402,
412, 414, 416&ndash;418, 420, 424, 427
</p>
<p>applications, 240&ndash;243
Stopping criterion, 78, 238&ndash;239, 261. See also
</p>
<p>Termination
Strong duality theorem, 163, 165, 432&ndash;434
Superlinear convergence, 206, 207, 297, 300,
</p>
<p>312, 313, 391
Supporting hyperplane, 193&ndash;194, 462, 463,
</p>
<p>465
Support vector machines, 17
Surplus variables, 12&ndash;13, 76, 101
Synthetic carrot, 45
</p>
<p>T
</p>
<p>Tableau, 35, 37, 38, 40&ndash;43, 45&ndash;48, 50&ndash;55, 73,
75&ndash;77, 80, 89, 90, 100&ndash;103, 105,
106, 381
</p>
<p>Tangent plane, 323&ndash;326, 333, 341&ndash;343, 351,
368, 369, 371, 484
</p>
<p>Taylor&rsquo;s Theorem, 188, 192, 221, 223, 334
Termination, 47, 70, 98, 136&ndash;143, 172, 228,
</p>
<p>245, 261, 262, 268, 277, 279, 297,
299, 300, 358, 363, 365, 373, 391
</p>
<p>Transportation problem, 6, 15&ndash;16, 29, 56&ndash;68,
72, 78&ndash;81, 85&ndash;86, 95, 107
</p>
<p>dual of, 85&ndash;86
northwest corner rule, 59&ndash;61, 64, 66, 67
simplex method for, 56&ndash;68
</p>
<p>Transshipment problem, 15&ndash;16, 56, 58
Tree algorithm, 95
Triangularity, 23, 34, 60&ndash;63, 65, 72, 78, 79,
</p>
<p>95, 249, 317, 386
bases, 60&ndash;62, 64
matrices, 56, 60, 210
</p>
<p>Triangularization procedure, 63
Turing model of computation, 118
</p>
<p>U
</p>
<p>Unimodal, 214, 215, 217, 224&ndash;226, 258, 315
Unimodular, 78
Upper triangular, 56, 79, 210
</p>
<p>V
</p>
<p>Variable metric method, 290
</p>
<p>W
</p>
<p>Warehousing problem, 16
Weak duality
</p>
<p>lemma, 87, 130
proposition, 431
</p>
<p>Wolfe test, 230
Working set, 361&ndash;366, 380, 392
Working surface, 361&ndash;365, 368, 379, 380
</p>
<p>Z
</p>
<p>Zero-duality gap, 131, 166
Zero-order
</p>
<p>conditions, 194&ndash;196, 344&ndash;351
</p>
<p>Lagrange theorem, 350, 433
Zigzagging, 360, 363, 364
Zoutendijk method, 359, 360</p>
<p/>
</div>
<ul>	<li>Preface</li>
	<li>Contents</li>
	<li>1 Introduction</li>
<ul>	<li>1.1 Optimization</li>
	<li>1.2 Types of Problems</li>
	<li>1.3 Size of Problems</li>
	<li>1.4 Iterative Algorithms and Convergence</li>
</ul>
	<li>Part I Linear Programming</li>
<ul>	<li>2 Basic Properties of Linear Programs</li>
<ul>	<li>2.1 Introduction</li>
	<li>2.2 Examples of Linear Programming Problems</li>
	<li>2.3 Basic Solutions</li>
	<li>2.4 The Fundamental Theorem of Linear Programming</li>
	<li>2.5 Relations to Convexity</li>
	<li>2.6 Exercises</li>
</ul>
	<li>3 The Simplex Method</li>
<ul>	<li>3.1 Pivots</li>
	<li>3.2 Adjacent Extreme Points</li>
	<li>3.3 Determining a Minimum Feasible Solution</li>
	<li>3.4 Computational Procedure: Simplex Method</li>
	<li>3.5 Finding a Basic Feasible Solution</li>
	<li>3.6 Matrix Form of the Simplex Method</li>
	<li>3.7 Simplex Method for Transportation Problems</li>
	<li>3.8 Decomposition</li>
	<li>3.9 Summary</li>
	<li>3.10 Exercises</li>
</ul>
	<li>4 Duality and Complementarity</li>
<ul>	<li>4.1 Dual Linear Programs</li>
	<li>4.2 The Duality Theorem</li>
	<li>4.3 Relations to the Simplex Procedure</li>
	<li>4.4 Sensitivity and Complementary Slackness</li>
	<li>4.5 Max Flow&ndash;Min Cut Theorem</li>
	<li>4.6 The Dual Simplex Method</li>
	<li>4.7 The Primal-Dual Algorithm</li>
	<li>4.8 Summary</li>
	<li>4.9 Exercises</li>
</ul>
	<li>5 Interior-Point Methods</li>
<ul>	<li>5.1 Elements of Complexity Theory</li>
	<li>5.2 The Simplex Method Is Not Polynomial-Time</li>
	<li>5.3 The Ellipsoid Method</li>
	<li>5.4 The Analytic Center</li>
	<li>5.5 The Central Path</li>
	<li>5.6 Solution Strategies</li>
	<li>5.7 Termination and Initialization</li>
	<li>5.8 Summary</li>
	<li>5.9 Exercises</li>
</ul>
	<li>6 Conic Linear Programming</li>
<ul>	<li>6.1 Convex Cones</li>
	<li>6.2 Conic Linear Programming Problem</li>
	<li>6.3 Farkas' Lemma for Conic Linear Programming</li>
	<li>6.4 Conic Linear Programming Duality</li>
	<li>6.5 Complementarity and Solution Rank of SDP</li>
	<li>6.6 Interior-Point Algorithms for Conic Linear Programming</li>
	<li>6.7 Summary</li>
	<li>6.8 Exercises</li>
</ul>
</ul>
	<li>Part II Unconstrained Problems</li>
<ul>	<li>7 Basic Properties of Solutions and Algorithms</li>
<ul>	<li>7.1 First-Order Necessary Conditions</li>
	<li>7.2 Examples of Unconstrained Problems</li>
	<li>7.3 Second-Order Conditions</li>
	<li>7.4 Convex and Concave Functions</li>
	<li>7.5 Minimization and Maximization of Convex Functions</li>
	<li>7.6 Zero-Order Conditions</li>
	<li>7.7 Global Convergence of Descent Algorithms</li>
	<li>7.8 Speed of Convergence</li>
	<li>7.9 Summary</li>
	<li>7.10 Exercises</li>
</ul>
	<li>8 Basic Descent Methods</li>
<ul>	<li>8.1 Line Search Algorithms</li>
	<li>8.2 The Method of Steepest Descent</li>
	<li>8.3 Applications of the Convergence Theory</li>
	<li>8.4 Accelerated Steepest Descent</li>
	<li>8.5 Newton's Method</li>
	<li>8.6 Coordinate Descent Methods</li>
	<li>8.7 Summary</li>
	<li>8.8 Exercises</li>
</ul>
	<li>9 Conjugate Direction Methods</li>
<ul>	<li>9.1 Conjugate Directions</li>
	<li>9.2 Descent Properties of the Conjugate Direction Method</li>
	<li>9.3 The Conjugate Gradient Method</li>
	<li>9.4 The C&ndash;G Method as an Optimal Process</li>
	<li>9.5 The Partial Conjugate Gradient Method</li>
	<li>9.6 Extension to Nonquadratic Problems</li>
	<li>9.7 Parallel Tangents</li>
	<li>9.8 Exercises</li>
</ul>
	<li>10 Quasi-Newton Methods</li>
<ul>	<li>10.1 Modified Newton Method</li>
	<li>10.2 Construction of the Inverse</li>
	<li>10.3 Davidon-Fletcher-Powell Method</li>
	<li>10.4 The Broyden Family</li>
	<li>10.5 Convergence Properties</li>
	<li>10.6 Scaling</li>
	<li>10.7 Memoryless Quasi-Newton Methods</li>
	<li>10.8 Combination of Steepest Descent and Newton's Method</li>
	<li>10.9 Summary</li>
	<li>10.10 Exercises</li>
</ul>
</ul>
	<li>Part III Constrained Minimization</li>
<ul>	<li>11 Constrained Minimization Conditions</li>
<ul>	<li>11.1 Constraints</li>
	<li>11.2 Tangent Plane</li>
	<li>11.3 First-Order Necessary Conditions (Equality Constraints)</li>
	<li>11.4 Examples</li>
	<li>11.5 Second-Order Conditions</li>
	<li>11.6 Eigenvalues in Tangent Subspace</li>
	<li>11.7 Sensitivity</li>
	<li>11.8 Inequality Constraints</li>
	<li>11.9 Zero-Order Conditions and Lagrangian Relaxation</li>
	<li>11.10 Summary</li>
	<li>11.11 Exercises</li>
</ul>
	<li>12 Primal Methods</li>
<ul>	<li>12.1 Advantage of Primal Methods</li>
	<li>12.2 Feasible Direction Methods</li>
	<li>12.3 Active Set Methods</li>
	<li>12.4 The Gradient Projection Method</li>
	<li>12.5 Convergence Rate of the Gradient Projection Method</li>
	<li>12.6 The Reduced Gradient Method</li>
	<li>12.7 Convergence Rate of the Reduced Gradient Method</li>
	<li>12.8 Variations</li>
	<li>12.9 Summary</li>
	<li>12.10 Exercises</li>
</ul>
	<li>13 Penalty and Barrier Methods</li>
<ul>	<li>13.1 Penalty Methods</li>
	<li>13.2 Barrier Methods</li>
	<li>13.3 Properties of Penalty and Barrier Functions</li>
	<li>13.4 Newton's Method and Penalty Functions</li>
	<li>13.5 Conjugate Gradients and Penalty Methods</li>
	<li>13.6 Normalization of Penalty Functions</li>
	<li>13.7 Penalty Functions and Gradient Projection</li>
	<li>13.8 Exact Penalty Functions</li>
	<li>13.9 Summary</li>
	<li>13.10 Exercises</li>
</ul>
	<li>14 Duality and Dual Methods</li>
<ul>	<li>14.1 Global Duality</li>
	<li>14.2 Local Duality</li>
	<li>14.3 Canonical Convergence Rate of Dual Steepest Ascent</li>
	<li>14.4 Separable Problems and Their Duals</li>
	<li>14.5 Augmented Lagrangian</li>
	<li>14.6 The Method of Multipliers</li>
	<li>14.7 The Alternating Direction Method of Multipliers</li>
	<li>14.8 Cutting Plane Methods</li>
	<li>14.9 Exercises</li>
</ul>
	<li>15 Primal-Dual Methods</li>
<ul>	<li>15.1 The Standard Problem</li>
	<li>15.2 A Simple Merit Function</li>
	<li>15.3 Basic Primal-Dual Methods</li>
	<li>15.4 Modified Newton Methods</li>
	<li>15.5 Descent Properties</li>
	<li>15.6 Rate of Convergence</li>
	<li>15.7 Primal-Dual Interior Point Methods</li>
	<li>15.8 Summary</li>
	<li>15.9 Exercises</li>
</ul>
</ul>
	<li>A Mathematical Review</li>
<ul>	<li>A.1 Sets</li>
	<li>A.2 Matrix Notation</li>
	<li>A.3 Spaces</li>
	<li>A.4 Eigenvalues and Quadratic Forms</li>
	<li>A.5 Topological Concepts</li>
	<li>A.6 Functions</li>
</ul>
	<li>B Convex Sets</li>
<ul>	<li>B.1 Basic Definitions</li>
	<li>B.2 Hyperplanes and Polytopes</li>
	<li>B.3 Separating and Supporting Hyperplanes</li>
	<li>B.4 Extreme Points</li>
</ul>
	<li>C Gaussian Elimination</li>
	<li>D Basic Network Concepts</li>
<ul>	<li>D.1 Flows in Networks</li>
	<li>D.2 Tree Procedure</li>
	<li>D.3 Capacitated Networks</li>
</ul>
	<li>Bibliography</li>
	<li>Index</li>
</ul>
</body></html>