<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title></title>
</head>
<body><div class="page"><p/>
<p>Graduate Texts in Physics
</p>
<p>Computational 
Physics 
</p>
<p>Philipp O.J. Scherer 
</p>
<p>Simulation of Classical and Quantum 
Systems
</p>
<p>Second Edition</p>
<p/>
</div>
<div class="page"><p/>
<p>Graduate Texts in Physics
</p>
<p>For further volumes:
www.springer.com/series/8431</p>
<p/>
<div class="annotation"><a href="http://www.springer.com/series/8431">http://www.springer.com/series/8431</a></div>
</div>
<div class="page"><p/>
<p>Graduate Texts in Physics
</p>
<p>Graduate Texts in Physics publishes core learning/teaching material for graduate- and ad-
vanced-level undergraduate courses on topics of current and emerging fields within physics,
both pure and applied. These textbooks serve students at the MS- or PhD-level and their
instructors as comprehensive sources of principles, definitions, derivations, experiments and
applications (as relevant) for their mastery and teaching, respectively. International in scope
and relevance, the textbooks correspond to course syllabi sufficiently to serve as required
reading. Their didactic style, comprehensiveness and coverage of fundamental material also
make them suitable as introductions or references for scientists entering, or requiring timely
knowledge of, a research field.
</p>
<p>Series Editors
</p>
<p>Professor Richard Needs
Cavendish Laboratory
JJ Thomson Avenue
Cambridge CB3 0HE, UK
rn11@cam.ac.uk
</p>
<p>Professor William T. Rhodes
Department of Computer and Electrical Engineering and Computer Science
Imaging Science and Technology Center
Florida Atlantic University
777 Glades Road SE, Room 456
Boca Raton, FL 33431, USA
wrhodes@fau.edu
</p>
<p>Professor Susan Scott
Department of Quantum Science
Australian National University
Science Road
Acton 0200, Australia
susan.scott@anu.edu.au
</p>
<p>Professor H. Eugene Stanley
Center for Polymer Studies Department of Physics
Boston University
590 Commonwealth Avenue, Room 204B
Boston, MA 02215, USA
hes@bu.edu
</p>
<p>Professor Martin Stutzmann
Walter Schottky Institut
TU M&uuml;nchen
85748 Garching, Germany
stutz@wsi.tu-muenchen.de</p>
<p/>
<div class="annotation"><a href="mailto:rn11@cam.ac.uk">mailto:rn11@cam.ac.uk</a></div>
<div class="annotation"><a href="mailto:wrhodes@fau.edu">mailto:wrhodes@fau.edu</a></div>
<div class="annotation"><a href="mailto:susan.scott@anu.edu.au">mailto:susan.scott@anu.edu.au</a></div>
<div class="annotation"><a href="mailto:hes@bu.edu">mailto:hes@bu.edu</a></div>
<div class="annotation"><a href="mailto:stutz@wsi.tu-muenchen.de">mailto:stutz@wsi.tu-muenchen.de</a></div>
</div>
<div class="page"><p/>
<p>Philipp O.J. Scherer
</p>
<p>Computational
Physics
</p>
<p>Simulation of Classical and Quantum
Systems
</p>
<p>Second Edition</p>
<p/>
</div>
<div class="page"><p/>
<p>Philipp O.J. Scherer
Physikdepartment T38
Technische Universit&auml;t M&uuml;nchen
Garching, Germany
</p>
<p>Additional material to this book can be downloaded from http://extras.springer.com.
</p>
<p>ISSN 1868-4513 ISSN 1868-4521 (electronic)
Graduate Texts in Physics
ISBN 978-3-319-00400-6 ISBN 978-3-319-00401-3 (eBook)
DOI 10.1007/978-3-319-00401-3
Springer Cham Heidelberg New York Dordrecht London
</p>
<p>Library of Congress Control Number: 2013944508
</p>
<p>&copy; Springer International Publishing Switzerland 2010, 2013
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of
the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microfilms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology
now known or hereafter developed. Exempted from this legal reservation are brief excerpts in connection
with reviews or scholarly analysis or material supplied specifically for the purpose of being entered
and executed on a computer system, for exclusive use by the purchaser of the work. Duplication of
this publication or parts thereof is permitted only under the provisions of the Copyright Law of the
Publisher&rsquo;s location, in its current version, and permission for use must always be obtained from Springer.
Permissions for use may be obtained through RightsLink at the Copyright Clearance Center. Violations
are liable to prosecution under the respective Copyright Law.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a specific statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
While the advice and information in this book are believed to be true and accurate at the date of pub-
lication, neither the authors nor the editors nor the publisher can accept any legal responsibility for any
errors or omissions that may be made. The publisher makes no warranty, express or implied, with respect
to the material contained herein.
</p>
<p>Printed on acid-free paper
</p>
<p>Springer is part of Springer Science+Business Media (www.springer.com)</p>
<p/>
<div class="annotation"><a href="http://extras.springer.com">http://extras.springer.com</a></div>
<div class="annotation"><a href="http://www.springer.com">http://www.springer.com</a></div>
<div class="annotation"><a href="http://www.springer.com/mycopy">http://www.springer.com/mycopy</a></div>
</div>
<div class="page"><p/>
<p>To Christine</p>
<p/>
</div>
<div class="page"><p/>
<p>Preface to the Second Edition
</p>
<p>This textbook introduces the main principles of computational physics, which in-
clude numerical methods and their application to the simulation of physical sys-
tems. The first edition was based on a one-year course in computational physics
where I presented a selection of only the most important methods and applications.
Approximately one-third of this edition is new. I tried to give a larger overview of
the numerical methods, traditional ones as well as more recent developments. In
many cases it is not possible to pin down the &ldquo;best&rdquo; algorithm, since this may de-
pend on subtle features of a certain application, the general opinion changes from
time to time with new methods appearing and computer architectures evolving, and
each author is convinced that his method is the best one. Therefore I concentrated
on a discussion of the prevalent methods and a comparison for selected examples.
For a comprehensive description I would like to refer the reader to specialized text-
books like &ldquo;Numerical Recipes&rdquo; or elementary books in the field of the engineering
sciences.
</p>
<p>The major changes are as follows.
A new chapter is dedicated to the discretization of differential equations and the
</p>
<p>general treatment of boundary value problems. While finite differences are a natural
way to discretize differential operators, finite volume methods are more flexible if
material properties like the dielectric constant are discontinuous. Both can be seen as
special cases of the finite element methods which are omnipresent in the engineering
sciences. The method of weighted residuals is a very general way to find the &ldquo;best&rdquo;
approximation to the solution within a limited space of trial functions. It is relevant
for finite element and finite volume methods but also for spectral methods which
use global trial functions like polynomials or Fourier series.
</p>
<p>Traditionally, polynomials and splines are very often used for interpolation. I in-
cluded a section on rational interpolation which is useful to interpolate functions
with poles but can also be an alternative to spline interpolation due to the recent
development of barycentric rational interpolants without poles.
</p>
<p>The chapter on numerical integration now discusses Clenshaw-Curtis and Gaus-
sian methods in much more detail, which are important for practical applications
due to their high accuracy.
</p>
<p>vii</p>
<p/>
</div>
<div class="page"><p/>
<p>viii Preface to the Second Edition
</p>
<p>Besides the elementary root finding methods like bisection and Newton-Raphson,
also the combined methods by Dekker and Brent and a recent extension by Chandru-
patla are discussed in detail. These methods are recommended in most text books.
Function minimization is now discussed also with derivative free methods, includ-
ing Brent&rsquo;s golden section search method. Quasi-Newton methods for root finding
and function minimizing are thoroughly explained.
</p>
<p>Eigenvalue problems are ubiquitous in physics. The QL-method, which is very
popular for not too large matrices is included as well as analytic expressions for
several differentiation matrices.
</p>
<p>The discussion of the singular value decomposition was extended and its appli-
cation to low rank matrix approximation and linear fitting is discussed.
</p>
<p>For the integration of equations of motion (i.e. of initial value problems) many
methods are available, often specialized for certain applications. For completeness,
I included the predictor-corrector methods by Nordsieck and Gear which have been
often used for molecular dynamics and the backward differentiation methods for
stiff problems.
</p>
<p>A new chapter is devoted to molecular mechanics, since this is a very important
branch of current computational physics. Typical force field terms are discussed as
well as the calculation of gradients which are necessary for molecular dynamics
simulations.
</p>
<p>The simulation of waves now includes three additional two-variable methods
which are often used in the literature and are based on generally applicable schemes
(leapfrog, Lax-Wendroff, Crank-Nicolson).
</p>
<p>The chapter on simple quantum systems was rewritten. Wave packet simulation
has become very important in theoretical physics and theoretical chemistry. Several
methods are compared for spatial discretization and time integration of the one-
dimensional Schr&ouml;dinger equation. The dissipative two-level system is used to dis-
cuss elementary operations on a qubit.
</p>
<p>The book is accompanied by many computer experiments. For those readers who
are unable to try them out, the essential results are shown by numerous figures.
</p>
<p>This book is intended to give the reader a good overview over the fundamental
numerical methods and their application to a wide range of physical phenomena.
Each chapter now starts with a small abstract, sometimes followed by necessary
physical background information. Many references, original work as well as spe-
cialized text books, are helpful for more deepened studies.
</p>
<p>Philipp O.J. SchererGarching, Germany
February 2013</p>
<p/>
</div>
<div class="page"><p/>
<p>Preface to the First Edition
</p>
<p>Computers have become an integral part of modern physics. They help to acquire,
store and process enormous amounts of experimental data. Algebra programs have
become very powerful and give the physician the knowledge of many mathemati-
cians at hand. Traditionally physics has been divided into experimental physics
which observes phenomena occurring in the real world and theoretical physics
which uses mathematical methods and simplified models to explain the experimen-
tal findings and to make predictions for future experiments. But there is also a new
part of physics which has an ever growing importance. Computational physics com-
bines the methods of the experimentalist and the theoretician. Computer simulation
of physical systems helps to develop models and to investigate their properties.
</p>
<p>This book is a compilation of the contents of a two-part course on computational
physics which I have given at the TUM (Technische Universit&auml;t M&uuml;nchen) for sev-
eral years on a regular basis. It attempts to give the undergraduate physics students
a profound background in numerical methods and in computer simulation methods
but is also very welcome by students of mathematics and computational science
</p>
<p>ix</p>
<p/>
</div>
<div class="page"><p/>
<p>x Preface to the First Edition
</p>
<p>who want to learn about applications of numerical methods in physics. This book
may also support lecturers of computational physics and bio-computing. It tries to
bridge between simple examples which can be solved analytically and more compli-
cated but instructive applications which provide insight into the underlying physics
by doing computer experiments.
</p>
<p>The first part gives an introduction into the essential methods of numerical math-
ematics which are needed for applications in physics. Basic algorithms are explained
in detail together with limitations due to numerical inaccuracies. Mathematical ex-
planations are supplemented by numerous numerical experiments.
</p>
<p>The second part of the book shows the application of computer simulation meth-
ods for a variety of physical systems with a certain focus on molecular biophysics.
The main object is the time evolution of a physical system. Starting from a simple
rigid rotor or a mass point in a central field, important concepts of classical molecu-
lar dynamics are discussed. Further chapters deal with partial differential equations,
especially the Poisson-Boltzmann equation, the diffusion equation, nonlinear dy-
namic systems and the simulation of waves on a 1-dimensional string. In the last
chapters simple quantum systems are studied to understand e.g. exponential decay
processes or electronic transitions during an atomic collision. A two-state quantum
system is studied in large detail, including relaxation processes and excitation by an
external field. Elementary operations on a quantum bit (qubit) are simulated.
</p>
<p>Basic equations are derived in detail and efficient implications are discussed to-
gether with numerical accuracy and stability of the algorithms. Analytical results
are given for simple test cases which serve as a benchmark for the numerical meth-
ods. Many computer experiments are provided realized as Java applets which can
be run in the web browser. For a deeper insight the source code can be studied and
modified with the free &ldquo;netbeans&rdquo;1 environment.
</p>
<p>Philipp O.J. SchererGarching, Germany
April 2010
</p>
<p>1www.netbeans.org.</p>
<p/>
<div class="annotation"><a href="http://www.netbeans.org">http://www.netbeans.org</a></div>
</div>
<div class="page"><p/>
<p>Contents
</p>
<p>Part I Numerical Methods
</p>
<p>1 Error Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.1 Machine Numbers and Rounding Errors . . . . . . . . . . . . . . 3
1.2 Numerical Errors of Elementary Floating Point Operations . . . . . 6
</p>
<p>1.2.1 Numerical Extinction . . . . . . . . . . . . . . . . . . . . 7
1.2.2 Addition . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
1.2.3 Multiplication . . . . . . . . . . . . . . . . . . . . . . . . 9
</p>
<p>1.3 Error Propagation . . . . . . . . . . . . . . . . . . . . . . . . . . 9
1.4 Stability of Iterative Algorithms . . . . . . . . . . . . . . . . . . . 11
1.5 Example: Rotation . . . . . . . . . . . . . . . . . . . . . . . . . . 12
1.6 Truncation Error . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
1.7 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
</p>
<p>2 Interpolation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
2.1 Interpolating Functions . . . . . . . . . . . . . . . . . . . . . . . 15
2.2 Polynomial Interpolation . . . . . . . . . . . . . . . . . . . . . . 16
</p>
<p>2.2.1 Lagrange Polynomials . . . . . . . . . . . . . . . . . . . . 17
2.2.2 Barycentric Lagrange Interpolation . . . . . . . . . . . . . 17
2.2.3 Newton&rsquo;s Divided Differences . . . . . . . . . . . . . . . . 18
2.2.4 Neville Method . . . . . . . . . . . . . . . . . . . . . . . 20
2.2.5 Error of Polynomial Interpolation . . . . . . . . . . . . . . 21
</p>
<p>2.3 Spline Interpolation . . . . . . . . . . . . . . . . . . . . . . . . . 22
2.4 Rational Interpolation . . . . . . . . . . . . . . . . . . . . . . . . 25
</p>
<p>2.4.1 Pad&eacute; Approximant . . . . . . . . . . . . . . . . . . . . . . 25
2.4.2 Barycentric Rational Interpolation . . . . . . . . . . . . . 27
</p>
<p>2.5 Multivariate Interpolation . . . . . . . . . . . . . . . . . . . . . . 32
2.6 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
</p>
<p>3 Numerical Differentiation . . . . . . . . . . . . . . . . . . . . . . . . 37
3.1 One-Sided Difference Quotient . . . . . . . . . . . . . . . . . . . 37
3.2 Central Difference Quotient . . . . . . . . . . . . . . . . . . . . . 38
</p>
<p>xi</p>
<p/>
</div>
<div class="page"><p/>
<p>xii Contents
</p>
<p>3.3 Extrapolation Methods . . . . . . . . . . . . . . . . . . . . . . . . 39
3.4 Higher Derivatives . . . . . . . . . . . . . . . . . . . . . . . . . . 41
3.5 Partial Derivatives of Multivariate Functions . . . . . . . . . . . . 42
3.6 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
</p>
<p>4 Numerical Integration . . . . . . . . . . . . . . . . . . . . . . . . . . 45
4.1 Equidistant Sample Points . . . . . . . . . . . . . . . . . . . . . . 46
</p>
<p>4.1.1 Closed Newton-Cotes Formulae . . . . . . . . . . . . . . . 46
4.1.2 Open Newton-Cotes Formulae . . . . . . . . . . . . . . . 48
4.1.3 Composite Newton-Cotes Rules . . . . . . . . . . . . . . . 48
4.1.4 Extrapolation Method (Romberg Integration) . . . . . . . . 49
</p>
<p>4.2 Optimized Sample Points . . . . . . . . . . . . . . . . . . . . . . 50
4.2.1 Clenshaw-Curtis Expressions . . . . . . . . . . . . . . . . 50
4.2.2 Gaussian Integration . . . . . . . . . . . . . . . . . . . . . 52
</p>
<p>4.3 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
</p>
<p>5 Systems of Inhomogeneous Linear Equations . . . . . . . . . . . . . 59
5.1 Gaussian Elimination Method . . . . . . . . . . . . . . . . . . . . 60
</p>
<p>5.1.1 Pivoting . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
5.1.2 Direct LU Decomposition . . . . . . . . . . . . . . . . . . 63
</p>
<p>5.2 QR Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . 64
5.2.1 QR Decomposition by Orthogonalization . . . . . . . . . . 64
5.2.2 QR Decomposition by Householder Reflections . . . . . . 66
</p>
<p>5.3 Linear Equations with Tridiagonal Matrix . . . . . . . . . . . . . . 69
5.4 Cyclic Tridiagonal Systems . . . . . . . . . . . . . . . . . . . . . 71
5.5 Iterative Solution of Inhomogeneous Linear Equations . . . . . . . 73
</p>
<p>5.5.1 General Relaxation Method . . . . . . . . . . . . . . . . . 73
5.5.2 Jacobi Method . . . . . . . . . . . . . . . . . . . . . . . . 73
5.5.3 Gauss-Seidel Method . . . . . . . . . . . . . . . . . . . . 74
5.5.4 Damping and Successive Over-Relaxation . . . . . . . . . 75
</p>
<p>5.6 Conjugate Gradients . . . . . . . . . . . . . . . . . . . . . . . . . 76
5.7 Matrix Inversion . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
5.8 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
</p>
<p>6 Roots and Extremal Points . . . . . . . . . . . . . . . . . . . . . . . . 83
6.1 Root Finding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
</p>
<p>6.1.1 Bisection . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
6.1.2 Regula Falsi (False Position) Method . . . . . . . . . . . . 85
6.1.3 Newton-Raphson Method . . . . . . . . . . . . . . . . . . 85
6.1.4 Secant Method . . . . . . . . . . . . . . . . . . . . . . . . 86
6.1.5 Interpolation . . . . . . . . . . . . . . . . . . . . . . . . . 87
6.1.6 Inverse Interpolation . . . . . . . . . . . . . . . . . . . . . 88
6.1.7 Combined Methods . . . . . . . . . . . . . . . . . . . . . 91
6.1.8 Multidimensional Root Finding . . . . . . . . . . . . . . . 97
6.1.9 Quasi-Newton Methods . . . . . . . . . . . . . . . . . . . 98</p>
<p/>
</div>
<div class="page"><p/>
<p>Contents xiii
</p>
<p>6.2 Function Minimization . . . . . . . . . . . . . . . . . . . . . . . 99
6.2.1 The Ternary Search Method . . . . . . . . . . . . . . . . . 99
6.2.2 The Golden Section Search Method (Brent&rsquo;s Method) . . . 101
6.2.3 Minimization in Multidimensions . . . . . . . . . . . . . . 106
6.2.4 Steepest Descent Method . . . . . . . . . . . . . . . . . . 106
6.2.5 Conjugate Gradient Method . . . . . . . . . . . . . . . . . 107
6.2.6 Newton-Raphson Method . . . . . . . . . . . . . . . . . . 107
6.2.7 Quasi-Newton Methods . . . . . . . . . . . . . . . . . . . 108
</p>
<p>6.3 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
</p>
<p>7 Fourier Transformation . . . . . . . . . . . . . . . . . . . . . . . . . 113
7.1 Fourier Integral and Fourier Series . . . . . . . . . . . . . . . . . 113
7.2 Discrete Fourier Transformation . . . . . . . . . . . . . . . . . . . 114
</p>
<p>7.2.1 Trigonometric Interpolation . . . . . . . . . . . . . . . . . 116
7.2.2 Real Valued Functions . . . . . . . . . . . . . . . . . . . . 118
7.2.3 Approximate Continuous Fourier Transformation . . . . . 119
</p>
<p>7.3 Fourier Transform Algorithms . . . . . . . . . . . . . . . . . . . . 120
7.3.1 Goertzel&rsquo;s Algorithm . . . . . . . . . . . . . . . . . . . . 120
7.3.2 Fast Fourier Transformation . . . . . . . . . . . . . . . . . 121
</p>
<p>7.4 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
</p>
<p>8 Random Numbers and Monte Carlo Methods . . . . . . . . . . . . . 127
8.1 Some Basic Statistics . . . . . . . . . . . . . . . . . . . . . . . . 127
</p>
<p>8.1.1 Probability Density and Cumulative Probability Distribution 127
8.1.2 Histogram . . . . . . . . . . . . . . . . . . . . . . . . . . 128
8.1.3 Expectation Values and Moments . . . . . . . . . . . . . . 129
8.1.4 Example: Fair Die . . . . . . . . . . . . . . . . . . . . . . 130
8.1.5 Normal Distribution . . . . . . . . . . . . . . . . . . . . . 131
8.1.6 Multivariate Distributions . . . . . . . . . . . . . . . . . . 132
8.1.7 Central Limit Theorem . . . . . . . . . . . . . . . . . . . 133
8.1.8 Example: Binomial Distribution . . . . . . . . . . . . . . . 133
8.1.9 Average of Repeated Measurements . . . . . . . . . . . . . 134
</p>
<p>8.2 Random Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . 135
8.2.1 Linear Congruent Mapping . . . . . . . . . . . . . . . . . 135
8.2.2 Marsaglia-Zamann Method . . . . . . . . . . . . . . . . . 135
8.2.3 Random Numbers with Given Distribution . . . . . . . . . 136
8.2.4 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . 136
</p>
<p>8.3 Monte Carlo Integration . . . . . . . . . . . . . . . . . . . . . . . 138
8.3.1 Numerical Calculation of π . . . . . . . . . . . . . . . . . 138
8.3.2 Calculation of an Integral . . . . . . . . . . . . . . . . . . 139
8.3.3 More General Random Numbers . . . . . . . . . . . . . . 140
</p>
<p>8.4 Monte Carlo Method for Thermodynamic Averages . . . . . . . . 141
8.4.1 Simple Sampling . . . . . . . . . . . . . . . . . . . . . . . 141
8.4.2 Importance Sampling . . . . . . . . . . . . . . . . . . . . 142
8.4.3 Metropolis Algorithm . . . . . . . . . . . . . . . . . . . . 142
</p>
<p>8.5 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144</p>
<p/>
</div>
<div class="page"><p/>
<p>xiv Contents
</p>
<p>9 Eigenvalue Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
9.1 Direct Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
9.2 Jacobi Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
9.3 Tridiagonal Matrices . . . . . . . . . . . . . . . . . . . . . . . . . 150
</p>
<p>9.3.1 Characteristic Polynomial of a Tridiagonal Matrix . . . . . 151
9.3.2 Special Tridiagonal Matrices . . . . . . . . . . . . . . . . 151
9.3.3 The QL Algorithm . . . . . . . . . . . . . . . . . . . . . 156
</p>
<p>9.4 Reduction to a Tridiagonal Matrix . . . . . . . . . . . . . . . . . . 157
9.5 Large Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159
9.6 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160
</p>
<p>10 Data Fitting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
10.1 Least Square Fit . . . . . . . . . . . . . . . . . . . . . . . . . . . 162
</p>
<p>10.1.1 Linear Least Square Fit . . . . . . . . . . . . . . . . . . . 163
10.1.2 Linear Least Square Fit with Orthogonalization . . . . . . 165
</p>
<p>10.2 Singular Value Decomposition . . . . . . . . . . . . . . . . . . . 167
10.2.1 Full Singular Value Decomposition . . . . . . . . . . . . . 168
10.2.2 Reduced Singular Value Decomposition . . . . . . . . . . 168
10.2.3 Low Rank Matrix Approximation . . . . . . . . . . . . . . 170
10.2.4 Linear Least Square Fit with Singular Value Decomposition 172
</p>
<p>10.3 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175
</p>
<p>11 Discretization of Differential Equations . . . . . . . . . . . . . . . . 177
11.1 Classification of Differential Equations . . . . . . . . . . . . . . . 178
</p>
<p>11.1.1 Linear Second Order PDE . . . . . . . . . . . . . . . . . . 178
11.1.2 Conservation Laws . . . . . . . . . . . . . . . . . . . . . 179
</p>
<p>11.2 Finite Differences . . . . . . . . . . . . . . . . . . . . . . . . . . 180
11.2.1 Finite Differences in Time . . . . . . . . . . . . . . . . . . 181
11.2.2 Stability Analysis . . . . . . . . . . . . . . . . . . . . . . 182
11.2.3 Method of Lines . . . . . . . . . . . . . . . . . . . . . . . 183
11.2.4 Eigenvector Expansion . . . . . . . . . . . . . . . . . . . 183
</p>
<p>11.3 Finite Volumes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185
11.3.1 Discretization of fluxes . . . . . . . . . . . . . . . . . . . 188
</p>
<p>11.4 Weighted Residual Based Methods . . . . . . . . . . . . . . . . . 190
11.4.1 Point Collocation Method . . . . . . . . . . . . . . . . . . 191
11.4.2 Sub-domain Method . . . . . . . . . . . . . . . . . . . . . 191
11.4.3 Least Squares Method . . . . . . . . . . . . . . . . . . . . 192
11.4.4 Galerkin Method . . . . . . . . . . . . . . . . . . . . . . . 192
</p>
<p>11.5 Spectral and Pseudo-spectral Methods . . . . . . . . . . . . . . . 193
11.5.1 Fourier Pseudo-spectral Methods . . . . . . . . . . . . . . 193
11.5.2 Example: Polynomial Approximation . . . . . . . . . . . . 194
</p>
<p>11.6 Finite Elements . . . . . . . . . . . . . . . . . . . . . . . . . . . 196
11.6.1 One-Dimensional Elements . . . . . . . . . . . . . . . . . 196
11.6.2 Two- and Three-Dimensional Elements . . . . . . . . . . . 197
11.6.3 One-Dimensional Galerkin FEM . . . . . . . . . . . . . . 201
</p>
<p>11.7 Boundary Element Method . . . . . . . . . . . . . . . . . . . . . 204</p>
<p/>
</div>
<div class="page"><p/>
<p>Contents xv
</p>
<p>12 Equations of Motion . . . . . . . . . . . . . . . . . . . . . . . . . . . 207
12.1 The State Vector . . . . . . . . . . . . . . . . . . . . . . . . . . 208
12.2 Time Evolution of the State Vector . . . . . . . . . . . . . . . . 209
12.3 Explicit Forward Euler Method . . . . . . . . . . . . . . . . . . 210
12.4 Implicit Backward Euler Method . . . . . . . . . . . . . . . . . 212
12.5 Improved Euler Methods . . . . . . . . . . . . . . . . . . . . . . 213
12.6 Taylor Series Methods . . . . . . . . . . . . . . . . . . . . . . . 215
</p>
<p>12.6.1 Nordsieck Predictor-Corrector Method . . . . . . . . . . 215
12.6.2 Gear Predictor-Corrector Methods . . . . . . . . . . . . 217
</p>
<p>12.7 Runge-Kutta Methods . . . . . . . . . . . . . . . . . . . . . . . 217
12.7.1 Second Order Runge-Kutta Method . . . . . . . . . . . 218
12.7.2 Third Order Runge-Kutta Method . . . . . . . . . . . . 218
12.7.3 Fourth Order Runge-Kutta Method . . . . . . . . . . . . 219
</p>
<p>12.8 Quality Control and Adaptive Step Size Control . . . . . . . . . 220
12.9 Extrapolation Methods . . . . . . . . . . . . . . . . . . . . . . . 221
12.10 Linear Multistep Methods . . . . . . . . . . . . . . . . . . . . . 222
</p>
<p>12.10.1 Adams-Bashforth Methods . . . . . . . . . . . . . . . . 222
12.10.2 Adams-Moulton Methods . . . . . . . . . . . . . . . . . 223
12.10.3 Backward Differentiation (Gear) Methods . . . . . . . . 223
12.10.4 Predictor-Corrector Methods . . . . . . . . . . . . . . . 224
</p>
<p>12.11 Verlet Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . 225
12.11.1 Liouville Equation . . . . . . . . . . . . . . . . . . . . 225
12.11.2 Split-Operator Approximation . . . . . . . . . . . . . . 226
12.11.3 Position Verlet Method . . . . . . . . . . . . . . . . . . 227
12.11.4 Velocity Verlet Method . . . . . . . . . . . . . . . . . . 227
12.11.5 St&ouml;rmer-Verlet Method . . . . . . . . . . . . . . . . . . 228
12.11.6 Error Accumulation for the St&ouml;rmer-Verlet Method . . . 229
12.11.7 Beeman&rsquo;s Method . . . . . . . . . . . . . . . . . . . . . 230
12.11.8 The Leapfrog Method . . . . . . . . . . . . . . . . . . . 231
</p>
<p>12.12 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232
</p>
<p>Part II Simulation of Classical and Quantum Systems
</p>
<p>13 Rotational Motion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239
13.1 Transformation to a Body Fixed Coordinate System . . . . . . . 239
13.2 Properties of the Rotation Matrix . . . . . . . . . . . . . . . . . 240
13.3 Properties of W , Connection with the Vector of Angular Velocity 242
13.4 Transformation Properties of the Angular Velocity . . . . . . . . 244
13.5 Momentum and Angular Momentum . . . . . . . . . . . . . . . 246
13.6 Equations of Motion of a Rigid Body . . . . . . . . . . . . . . . 246
13.7 Moments of Inertia . . . . . . . . . . . . . . . . . . . . . . . . . 247
13.8 Equations of Motion for a Rotor . . . . . . . . . . . . . . . . . . 248
13.9 Explicit Methods . . . . . . . . . . . . . . . . . . . . . . . . . . 248
13.10 Loss of Orthogonality . . . . . . . . . . . . . . . . . . . . . . . 250
13.11 Implicit Method . . . . . . . . . . . . . . . . . . . . . . . . . . 251
13.12 Kinetic Energy of a Rotor . . . . . . . . . . . . . . . . . . . . . 255</p>
<p/>
</div>
<div class="page"><p/>
<p>xvi Contents
</p>
<p>13.13 Parametrization by Euler Angles . . . . . . . . . . . . . . . . . . 255
13.14 Cayley-Klein Parameters, Quaternions, Euler Parameters . . . . . 256
13.15 Solving the Equations of Motion with Quaternions . . . . . . . . 259
13.16 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 260
</p>
<p>14 Molecular Mechanics . . . . . . . . . . . . . . . . . . . . . . . . . . . 263
14.1 Atomic Coordinates . . . . . . . . . . . . . . . . . . . . . . . . 264
14.2 Force Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266
</p>
<p>14.2.1 Intramolecular Forces . . . . . . . . . . . . . . . . . . . 267
14.2.2 Intermolecular Interactions . . . . . . . . . . . . . . . . 269
</p>
<p>14.3 Gradients . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270
14.4 Normal Mode Analysis . . . . . . . . . . . . . . . . . . . . . . . 274
</p>
<p>14.4.1 Harmonic Approximation . . . . . . . . . . . . . . . . . 274
14.5 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 276
</p>
<p>15 Thermodynamic Systems . . . . . . . . . . . . . . . . . . . . . . . . 279
15.1 Simulation of a Lennard-Jones Fluid . . . . . . . . . . . . . . . 279
</p>
<p>15.1.1 Integration of the Equations of Motion . . . . . . . . . . 280
15.1.2 Boundary Conditions and Average Pressure . . . . . . . 281
15.1.3 Initial Conditions and Average Temperature . . . . . . . 281
15.1.4 Analysis of the Results . . . . . . . . . . . . . . . . . . 282
</p>
<p>15.2 Monte Carlo Simulation . . . . . . . . . . . . . . . . . . . . . . 287
15.2.1 One-Dimensional Ising Model . . . . . . . . . . . . . . 287
15.2.2 Two-Dimensional Ising Model . . . . . . . . . . . . . . 289
</p>
<p>15.3 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290
</p>
<p>16 RandomWalk and Brownian Motion . . . . . . . . . . . . . . . . . . 293
16.1 Markovian Discrete Time Models . . . . . . . . . . . . . . . . . 293
16.2 Random Walk in One Dimension . . . . . . . . . . . . . . . . . 294
</p>
<p>16.2.1 Random Walk with Constant Step Size . . . . . . . . . . 295
16.3 The Freely Jointed Chain . . . . . . . . . . . . . . . . . . . . . . 296
</p>
<p>16.3.1 Basic Statistic Properties . . . . . . . . . . . . . . . . . 297
16.3.2 Gyration Tensor . . . . . . . . . . . . . . . . . . . . . . 299
16.3.3 Hookean Spring Model . . . . . . . . . . . . . . . . . . 300
</p>
<p>16.4 Langevin Dynamics . . . . . . . . . . . . . . . . . . . . . . . . 301
16.5 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303
</p>
<p>17 Electrostatics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 305
17.1 Poisson Equation . . . . . . . . . . . . . . . . . . . . . . . . . . 305
</p>
<p>17.1.1 Homogeneous Dielectric Medium . . . . . . . . . . . . 306
17.1.2 Numerical Methods for the Poisson Equation . . . . . . 307
17.1.3 Charged Sphere . . . . . . . . . . . . . . . . . . . . . . 309
17.1.4 Variable ε . . . . . . . . . . . . . . . . . . . . . . . . . 311
17.1.5 Discontinuous ε . . . . . . . . . . . . . . . . . . . . . . 313
17.1.6 Solvation Energy of a Charged Sphere . . . . . . . . . . 314
17.1.7 The Shifted Grid Method . . . . . . . . . . . . . . . . . 314</p>
<p/>
</div>
<div class="page"><p/>
<p>Contents xvii
</p>
<p>17.2 Poisson-Boltzmann Equation . . . . . . . . . . . . . . . . . . . . 315
17.2.1 Linearization of the Poisson-Boltzmann Equation . . . . . 317
17.2.2 Discretization of the Linearized Poisson-Boltzmann
</p>
<p>Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . 318
17.3 Boundary Element Method for the Poisson Equation . . . . . . . . 318
</p>
<p>17.3.1 Integral Equations for the Potential . . . . . . . . . . . . . 318
17.3.2 Calculation of the Boundary Potential . . . . . . . . . . . . 321
</p>
<p>17.4 Boundary Element Method for the Linearized Poisson-Boltzmann
Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 324
</p>
<p>17.5 Electrostatic Interaction Energy (Onsager Model) . . . . . . . . . 325
17.5.1 Example: Point Charge in a Spherical Cavity . . . . . . . . 326
</p>
<p>17.6 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327
</p>
<p>18 Waves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 329
18.1 Classical Waves . . . . . . . . . . . . . . . . . . . . . . . . . . . 329
18.2 Spatial Discretization in One Dimension . . . . . . . . . . . . . . 332
18.3 Solution by an Eigenvector Expansion . . . . . . . . . . . . . . . 334
18.4 Discretization of Space and Time . . . . . . . . . . . . . . . . . . 337
18.5 Numerical Integration with a Two-Step Method . . . . . . . . . . 338
18.6 Reduction to a First Order Differential Equation . . . . . . . . . . 340
18.7 Two-Variable Method . . . . . . . . . . . . . . . . . . . . . . . . 343
</p>
<p>18.7.1 Leapfrog Scheme . . . . . . . . . . . . . . . . . . . . . . 343
18.7.2 Lax-Wendroff Scheme . . . . . . . . . . . . . . . . . . . . 345
18.7.3 Crank-Nicolson Scheme . . . . . . . . . . . . . . . . . . . 347
</p>
<p>18.8 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349
</p>
<p>19 Diffusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351
19.1 Particle Flux and Concentration Changes . . . . . . . . . . . . . . 351
19.2 Diffusion in One Dimension . . . . . . . . . . . . . . . . . . . . . 353
</p>
<p>19.2.1 Explicit Euler (Forward Time Centered Space) Scheme . . 353
19.2.2 Implicit Euler (Backward Time Centered Space) Scheme . 355
19.2.3 Crank-Nicolson Method . . . . . . . . . . . . . . . . . . . 357
19.2.4 Error Order Analysis . . . . . . . . . . . . . . . . . . . . . 358
19.2.5 Finite Element Discretization . . . . . . . . . . . . . . . . 360
</p>
<p>19.3 Split-Operator Method for Multidimensions . . . . . . . . . . . . 360
19.4 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 362
</p>
<p>20 Nonlinear Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . 363
20.1 Iterated Functions . . . . . . . . . . . . . . . . . . . . . . . . . . 364
</p>
<p>20.1.1 Fixed Points and Stability . . . . . . . . . . . . . . . . . . 364
20.1.2 The Lyapunov Exponent . . . . . . . . . . . . . . . . . . . 366
20.1.3 The Logistic Map . . . . . . . . . . . . . . . . . . . . . . 367
20.1.4 Fixed Points of the Logistic Map . . . . . . . . . . . . . . 367
20.1.5 Bifurcation Diagram . . . . . . . . . . . . . . . . . . . . . 369
</p>
<p>20.2 Population Dynamics . . . . . . . . . . . . . . . . . . . . . . . . 370
20.2.1 Equilibria and Stability . . . . . . . . . . . . . . . . . . . 370
20.2.2 The Continuous Logistic Model . . . . . . . . . . . . . . . 371</p>
<p/>
</div>
<div class="page"><p/>
<p>xviii Contents
</p>
<p>20.3 Lotka-Volterra Model . . . . . . . . . . . . . . . . . . . . . . . . 372
20.3.1 Stability Analysis . . . . . . . . . . . . . . . . . . . . . . 372
</p>
<p>20.4 Functional Response . . . . . . . . . . . . . . . . . . . . . . . . . 373
20.4.1 Holling-Tanner Model . . . . . . . . . . . . . . . . . . . . 375
</p>
<p>20.5 Reaction-Diffusion Systems . . . . . . . . . . . . . . . . . . . . . 378
20.5.1 General Properties of Reaction-Diffusion Systems . . . . . 378
20.5.2 Chemical Reactions . . . . . . . . . . . . . . . . . . . . . 378
20.5.3 Diffusive Population Dynamics . . . . . . . . . . . . . . . 379
20.5.4 Stability Analysis . . . . . . . . . . . . . . . . . . . . . . 379
20.5.5 Lotka-Volterra Model with Diffusion . . . . . . . . . . . . 380
</p>
<p>20.6 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382
</p>
<p>21 Simple Quantum Systems . . . . . . . . . . . . . . . . . . . . . . . . 385
21.1 Pure and Mixed Quantum States . . . . . . . . . . . . . . . . . . . 386
</p>
<p>21.1.1 Wavefunctions . . . . . . . . . . . . . . . . . . . . . . . . 387
21.1.2 Density Matrix for an Ensemble of Systems . . . . . . . . 387
21.1.3 Time Evolution of the Density Matrix . . . . . . . . . . . . 388
</p>
<p>21.2 Wave Packet Motion in One Dimension . . . . . . . . . . . . . . . 389
21.2.1 Discretization of the Kinetic Energy . . . . . . . . . . . . 390
21.2.2 Time Evolution . . . . . . . . . . . . . . . . . . . . . . . 392
21.2.3 Example: Free Wave Packet Motion . . . . . . . . . . . . . 402
</p>
<p>21.3 Few-State Systems . . . . . . . . . . . . . . . . . . . . . . . . . . 403
21.3.1 Two-State System . . . . . . . . . . . . . . . . . . . . . . 405
21.3.2 Two-State System with Time Dependent Perturbation . . . 408
21.3.3 Superexchange Model . . . . . . . . . . . . . . . . . . . . 410
21.3.4 Ladder Model for Exponential Decay . . . . . . . . . . . . 412
21.3.5 Landau-Zener Model . . . . . . . . . . . . . . . . . . . . 414
</p>
<p>21.4 The Dissipative Two-State System . . . . . . . . . . . . . . . . . 416
21.4.1 Equations of Motion for a Two-State System . . . . . . . . 416
21.4.2 The Vector Model . . . . . . . . . . . . . . . . . . . . . . 417
21.4.3 The Spin- 12 System . . . . . . . . . . . . . . . . . . . . . 418
21.4.4 Relaxation Processes&mdash;The Bloch Equations . . . . . . . . 420
21.4.5 The Driven Two-State System . . . . . . . . . . . . . . . . 421
21.4.6 Elementary Qubit Manipulation . . . . . . . . . . . . . . . 428
</p>
<p>21.5 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 430
</p>
<p>Appendix I Performing the Computer Experiments . . . . . . . . . . . 433
</p>
<p>Appendix II Methods and Algorithms . . . . . . . . . . . . . . . . . . . 435
</p>
<p>References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 441
</p>
<p>Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 449</p>
<p/>
</div>
<div class="page"><p/>
<p>Part I
</p>
<p>Numerical Methods</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 1
</p>
<p>Error Analysis
</p>
<p>Several sources of errors are important for numerical data processing:
</p>
<p>Experimental uncertainty: Input data from an experiment have a limited precision.
Instead of the vector of exact values x the calculation uses x +�x, with an un-
certainty �x. This can lead to large uncertainties of the calculated results if an
unstable algorithm is used or if the unavoidable error inherent to the problem is
large.
</p>
<p>Rounding errors: The arithmetic unit of a computer uses only a subset of the
real numbers, the so called machine numbers A &sub; &real;. The input data as well as
the results of elementary operations have to be represented by machine numbers
whereby rounding errors can be generated. This kind of numerical error can be
avoided in principle by using arbitrary precision arithmetics1 or symbolic algebra
programs. But this is unpractical in many cases due to the increase in computing
time and memory requirements.
</p>
<p>Truncation errors: Results from more complex operations like square roots or
trigonometric functions can have even larger errors since series expansions have
to be truncated and iterations can accumulate the errors of the individual steps.
</p>
<p>1.1 Machine Numbers and Rounding Errors
</p>
<p>Floating point numbers are internally stored as the product of sign, mantissa and a
power of 2. According to the IEEE754 standard [130] single, double and quadruple
precision numbers are stored as 32, 64 or 128 bits (Table 1.1):
</p>
<p>1For instance the open source GNU MP bignum library.
</p>
<p>P.O.J. Scherer, Computational Physics, Graduate Texts in Physics,
DOI 10.1007/978-3-319-00401-3_1,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>3</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-00401-3_1">http://dx.doi.org/10.1007/978-3-319-00401-3_1</a></div>
</div>
<div class="page"><p/>
<p>4 1 Error Analysis
</p>
<p>Table 1.1 Binary floating point formats
</p>
<p>Format Sign Exponent Hidden bit Fraction Precision εM
</p>
<p>Float s b0 &middot; &middot; &middot;b7 1 a0 &middot; &middot; &middot;a22 2&minus;24 = 5.96E&minus;8
</p>
<p>Double s b0 &middot; &middot; &middot;b10 1 a0 &middot; &middot; &middot;a51 2&minus;53 = 1.11E&minus;16
</p>
<p>Quadruple s b0 &middot; &middot; &middot;b14 1 a0 &middot; &middot; &middot;a111 2&minus;113 = 9.63E&minus;35
</p>
<p>Table 1.2 Exponent bias E
</p>
<p>Decimal value Binary value Hexadecimal value Data type
</p>
<p>12710 11111112 $3F Single
</p>
<p>102310 11111111112 $3FF Double
</p>
<p>1638310 111111111111112 $3FFF Quadruple
</p>
<p>The sign bit s is 0 for positive and 1 for negative numbers. The exponent b is
biased by adding E which is half of its maximum possible value (Table 1.2).2 The
value of a number is given by
</p>
<p>x = (&minus;)s &times; a &times; 2b&minus;E. (1.1)
</p>
<p>The mantissa a is normalized such that its first bit is 1 and its value is between 1
and 2
</p>
<p>1.0002 &middot; &middot; &middot;0 &le; a &le; 1.111 &middot; &middot; &middot;12 &lt; 10.02 = 210. (1.2)
</p>
<p>Since the first bit of a normalized floating point number always is 1, it is not nec-
essary to store it explicitly (hidden bit or J-bit). However, since not all numbers can
be normalized, only the range of exponents from $001 &middot; &middot; &middot;$7FE is used for normal-
ized numbers. An exponent of $000 signals that the number is not normalized (zero
is an important example, there exist even two zero numbers with different sign)
whereas the exponent $7FF is reserved for infinite or undefined results (Table 1.3).
</p>
<p>The range of normalized double precision numbers is between
</p>
<p>Min_Normal = 2.2250738585072014&times; 10&minus;308
</p>
<p>and
</p>
<p>Max_Normal = 1.7976931348623157E&times; 10308.
</p>
<p>Example Consider the following bit pattern which represents a double precision
number:
</p>
<p>$4059000000000000.
</p>
<p>2In the following the usual hexadecimal notation is used which represents a group of 4 bits by one
of the digits 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, A, B, C, D, E, F.</p>
<p/>
</div>
<div class="page"><p/>
<p>1.1 Machine Numbers and Rounding Errors 5
</p>
<p>Table 1.3 Special double precision numbers
</p>
<p>Hexadecimal value Symbolic value
</p>
<p>$000 0000000000000 +0
$080 00000000000000 &minus;0
$7FF 0000000000000 +inf
$FFF 0000000000000 &minus;inf
$7FF 0000000000001 &middot; &middot; &middot;$7FF FFFFFFFFFFFFF NAN
$001 0000000000000 Min_Normal
</p>
<p>$7FE FFFFFFFFFFFFF Max_Normal
</p>
<p>$000 0000000000001 Min_Subnormal
</p>
<p>$000 FFFFFFFFFFFFF Max_Subnormal
</p>
<p>The exponent is 100 0000 01012 &minus; 011 1111 11112 = 1102 and the mantissa includ-
ing the J-bit is 1 1001 0000 0000 &middot; &middot; &middot;2. Hence the decimal value is
</p>
<p>1.5625 &times; 26 = 10010.
</p>
<p>Input numbers which are not machine numbers have to be rounded to the nearest
machine number. This is formally described by a mapping &real;&rarr;A
</p>
<p>x &rarr; rd(x)
</p>
<p>with the property3
∣∣x &minus; rd(x)
</p>
<p>∣∣&le; |x &minus; g| for all g &isin;A. (1.3)
For the special case that x is exactly in the middle between two successive ma-
chine numbers, a tie-breaking rule is necessary. The simplest rules are to round up
always (round-half-up) or always down (round-half-down). However, these are not
symmetric and produce a bias in the average round-off error. The IEEE754 standard
[130] recommends the round-to-nearest-even method, i.e. the least significant bit of
the rounded number should always be zero. Alternatives are round-to-nearest-odd,
stochastic rounding and alternating rounding.
</p>
<p>The cases of exponent overflow and exponent underflow need special attention:
Whenever the exponent b has the maximum possible value b = bmax and a =
</p>
<p>1.11 &middot; &middot; &middot;11 has to be rounded to a&prime; = 10.00 &middot; &middot; &middot;0, the rounded number is not a ma-
chine number and the result is &plusmn; inf.
</p>
<p>Numbers in the range 2bmin &gt; |x| &ge; 2bmin&minus;t have to be represented with loss of
accuracy by denormalized machine numbers. Their mantissa cannot be normalized
since it is a &lt; 1 and the exponent has the smallest possible value b = bmin. Even
smaller numbers with |x|&lt; 2&minus;t+bmin have to be rounded to &plusmn;0.
</p>
<p>3Sometimes rounding is replaced by a simpler truncation operation which, however leads to sig-
nificantly larger rounding errors.</p>
<p/>
</div>
<div class="page"><p/>
<p>6 1 Error Analysis
</p>
<p>Fig. 1.1 (Round to nearest) Normalized machine numbers with t = 3 binary digits are shown.
Rounding to the nearest machine number produces a round-off error which is bounded by half the
spacing of the machine numbers
</p>
<p>The maximum rounding error for normalized numbers with t binary digits
</p>
<p>a&prime; = s &times; 2b&minus;E &times; 1.a1a2 &middot; &middot; &middot;at&minus;1 (1.4)
</p>
<p>is given by (Fig. 1.1)
∣∣a &minus; a&prime;
</p>
<p>∣∣&le; 2b&minus;E &times; 2&minus;t (1.5)
and the relative error is bounded by
</p>
<p>∣∣∣∣
rd(x)&minus; x
</p>
<p>x
</p>
<p>∣∣∣∣&le;
2&minus;t &times; 2b
|a| &times; 2b &le; 2
</p>
<p>&minus;t . (1.6)
</p>
<p>The error bound determines the relative machine precision4
</p>
<p>εM = 2&minus;t (1.7)
</p>
<p>and the rounding operation can be described by
</p>
<p>rd(x)= x(1 + ε) with |ε| &le; εM . (1.8)
</p>
<p>The round-off error takes its maximum value if the mantissa is close to 1. Con-
sider a number
</p>
<p>x = 1 + ε.
</p>
<p>If ε &lt; εM then rd(x) = 1 whereas for ε &gt; εM rounding gives rd(x) = 1 + 21&minus;t
(Fig. 1.2). Hence εM is given by the largest number ε for which rd(1.0 + ε)= 1.0
and is therefore also called unit round off.
</p>
<p>1.2 Numerical Errors of Elementary Floating Point Operations
</p>
<p>Even for two machine numbers x, y &isin; A the results of addition, subtraction, multi-
plication or division are not necessarily machine numbers. We have to expect some
additional round-off errors from all these elementary operations [244]. We assume
that the results of elementary operations are approximated by machine numbers as
precisely as possible. The IEEE754 standard [130] requires that the exact operations
</p>
<p>4Also known as machine epsilon.</p>
<p/>
</div>
<div class="page"><p/>
<p>1.2 Numerical Errors of Elementary Floating Point Operations 7
</p>
<p>Fig. 1.2 (Unit round off)
</p>
<p>x+y, x&minus;y, x&times;y, x&divide;y are approximated by floating point operations A&rarr;A with
the property:
</p>
<p>f l+(x, y)= rd(x + y)
f l&minus;(x, y)= rd(x &minus; y)
f l&lowast;(x, y)= rd(x &times; y)
f l&divide;(x, y)= rd(x &divide; y).
</p>
<p>(1.9)
</p>
<p>1.2.1 Numerical Extinction
</p>
<p>For an addition or subtraction one summand has to be denormalized to line up the
exponents (for simplicity we consider only the case x &gt; 0, y &gt; 0)
</p>
<p>x + y = ax2bx&minus;E + ay2by&minus;E =
(
ax + ay2by&minus;bx
</p>
<p>)
2bx&minus;E. (1.10)
</p>
<p>If the two numbers differ much in their magnitude, numerical extinction can happen.
Consider the following case:
</p>
<p>y &lt; 2bx&minus;E &times; 2&minus;t
</p>
<p>ay2
by&minus;bx &lt; 2&minus;t .
</p>
<p>(1.11)
</p>
<p>The mantissa of the exact sum is
</p>
<p>ax + ay2by&minus;bx = 1.α2 &middot; &middot; &middot;αt&minus;101β2 &middot; &middot; &middot;βt&minus;1. (1.12)
Rounding to the nearest machine number gives
</p>
<p>rd(x + y)= 2bx &times; (1.α2 &middot; &middot; &middot;αt&minus;1)= x (1.13)
since
</p>
<p>|0.01β2 &middot; &middot; &middot;βt&minus;1 &minus; 0| &le; |0.011 &middot; &middot; &middot;1| = 0.1 &minus; 0.00 &middot; &middot; &middot;01
|0.01β2 &middot; &middot; &middot;βt&minus;1 &minus; 1| &ge; |0.01 &minus; 1| = 0.11.
</p>
<p>(1.14)
</p>
<p>Consider now the case
</p>
<p>y &lt; x &times; 2&minus;t&minus;1 = ax &times; 2bx&minus;E&minus;t&minus;1 &lt; 2bx&minus;E&minus;t . (1.15)</p>
<p/>
</div>
<div class="page"><p/>
<p>8 1 Error Analysis
</p>
<p>For normalized numbers the mantissa is in the interval
</p>
<p>1 &le; |ax |&lt; 2 (1.16)
</p>
<p>hence we have
</p>
<p>rd(x + y)= x if y
x
&lt; 2&minus;t&minus;1 = εM
</p>
<p>2
. (1.17)
</p>
<p>Especially for x = 1 we have
</p>
<p>rd(1 + y)= 1 if y &lt; 2&minus;t = 0.00 &middot; &middot; &middot;0t&minus;11t000 &middot; &middot; &middot; (1.18)
</p>
<p>2&minus;t could be rounded to 0 or to 21&minus;t since the distance is the same |2&minus;t &minus; 0| =
|2&minus;t &minus; 21&minus;t | = 2&minus;t .
</p>
<p>The smallest machine number with f l+(1, ε) &gt; 1 is either ε = 0.00 &middot; &middot; &middot;1t0 &middot; &middot; &middot; =
2&minus;t or ε = 0.00 &middot; &middot; &middot;1t0 &middot; &middot; &middot;012t&minus;1 = 2&minus;t (1 + 21&minus;t ). Hence the machine precision
εM can be determined by looking for the smallest (positive) machine number ε for
which f l+(1, ε) &gt; 1.
</p>
<p>1.2.2 Addition
</p>
<p>Consider the sum of two floating point numbers
</p>
<p>y = x1 + x2. (1.19)
</p>
<p>First the input data have to be approximated by machine numbers:
</p>
<p>x1 &rarr; rd(x1)= x1(1 + ε1)
x2 &rarr; rd(x2)= x2(1 + ε2).
</p>
<p>(1.20)
</p>
<p>The addition of the two summands may produce another error α since the result has
to be rounded. The numerical result is
</p>
<p>ỹ = f l+
(
rd(x1), rd(x2)
</p>
<p>)
=
(
x1(1 + ε1)+ x2(1 + ε2)
</p>
<p>)
(1 + α). (1.21)
</p>
<p>Neglecting higher orders of the error terms we have in first order
</p>
<p>ỹ = x1 + x2 + x1ε1 + x2ε2 + (x1 + x2)α (1.22)
</p>
<p>and the relative error of the numerical sum is
</p>
<p>ỹ &minus; y
y
</p>
<p>= x1
x1 + x2
</p>
<p>ε1 +
x2
</p>
<p>x1 + x2
ε2 + α. (1.23)
</p>
<p>If x1 &asymp;&minus;x2 then numerical extinction can produce large relative errors and uncer-
tainties of the input data can be strongly enhanced.</p>
<p/>
</div>
<div class="page"><p/>
<p>1.3 Error Propagation 9
</p>
<p>1.2.3 Multiplication
</p>
<p>Consider the multiplication of two floating point numbers
</p>
<p>y = x1 &times; x2. (1.24)
</p>
<p>The numerical result is
</p>
<p>ỹ = f l&lowast;
(
rd(x1), rd(x2)
</p>
<p>)
= x1(1 + ε1)x2(1 + ε2)(1 +μ)&asymp; x1x2(1 + ε1 + ε2 +μ)
</p>
<p>(1.25)
</p>
<p>with the relative error
</p>
<p>ỹ &minus; y
y
</p>
<p>= 1 + ε1 + ε2 +μ. (1.26)
</p>
<p>The relative errors of the input data and of the multiplication just add up to the total
relative error. There is no enhancement. Similarly for a division
</p>
<p>y = x1
x2
</p>
<p>(1.27)
</p>
<p>the relative error is
</p>
<p>ỹ &minus; y
y
</p>
<p>= 1 + ε1 &minus; ε2 +μ. (1.28)
</p>
<p>1.3 Error Propagation
</p>
<p>Consider an algorithm consisting of a sequence of elementary operations. From the
set of input data which is denoted by the vector
</p>
<p>x= (x1 &middot; &middot; &middot;xn) (1.29)
</p>
<p>a set of output data is calculated
</p>
<p>y= (y1 &middot; &middot; &middot;ym). (1.30)
</p>
<p>Formally this can be denoted by a vector function
</p>
<p>y= ϕ(x) (1.31)
</p>
<p>which can be written as a product of r simpler functions representing the elementary
operations
</p>
<p>ϕ = ϕ(r) &times; ϕ(r&minus;1) &middot; &middot; &middot;ϕ(1). (1.32)
</p>
<p>Starting with x intermediate results xi = (xi1, . . . xini ) are calculated until the output
data y result from the last step:</p>
<p/>
</div>
<div class="page"><p/>
<p>10 1 Error Analysis
</p>
<p>x1 = ϕ(1)(x)
x2 = ϕ(2)(x1)
...
</p>
<p>xr&minus;1 = ϕ(r&minus;1)(xr&minus;2)
y= ϕ(r)(xr&minus;1).
</p>
<p>(1.33)
</p>
<p>In the following we analyze the influence of numerical errors onto the final re-
sults. We treat all errors as small quantities and neglect higher orders. Due to round-
off errors and possible experimental uncertainties the input data are not exactly given
by x but by
</p>
<p>x+�x. (1.34)
The first step of the algorithm produces the result
</p>
<p>x̃1 = rd
(
ϕ(1)(x+�x)
</p>
<p>)
. (1.35)
</p>
<p>Taylor series expansion gives in first order
</p>
<p>x̃1 =
(
ϕ(1)(x)+Dϕ(1)�x
</p>
<p>)
(1 +E1)+ &middot; &middot; &middot; (1.36)
</p>
<p>with the partial derivatives
</p>
<p>Dϕ(1) =
(
&part;x1i
</p>
<p>&part;xj
</p>
<p>)
=
</p>
<p>⎛
⎜⎜⎝
</p>
<p>&part;x11
&part;x1
</p>
<p>&middot; &middot; &middot; &part;x11
&part;xn
</p>
<p>...
. . .
</p>
<p>...
&part;x1n1
&part;x1
</p>
<p>&middot; &middot; &middot; &part;x1n1
&part;xn
</p>
<p>⎞
⎟⎟⎠ (1.37)
</p>
<p>and the round-off errors of the first step
</p>
<p>E1 =
</p>
<p>⎛
⎜⎝
ε
(1)
1
</p>
<p>. . .
</p>
<p>ε
(1)
n1
</p>
<p>⎞
⎟⎠ . (1.38)
</p>
<p>The error of the first intermediate result is
</p>
<p>�x1 = x̃1 &minus; x1 =Dϕ(1)�x+ ϕ(1)(x)E1. (1.39)
The second intermediate result is
</p>
<p>x̃2 =
(
ϕ(2)(x̃1)
</p>
<p>)
(1 +E2)= ϕ(2)(x1 +�x1)(1 +E2)
</p>
<p>= x2(1 +E2)+Dϕ(2)Dϕ(1)�x+Dϕ(2)x1E1 (1.40)
with the error
</p>
<p>�x2 = x2E2 +Dϕ(2)Dϕ(1)�x+Dϕ(2)x1E1. (1.41)
Finally the error of the result is
</p>
<p>�y= yEr +Dϕ(r) &middot; &middot; &middot;Dϕ(1)�x+Dϕ(r) &middot; &middot; &middot;Dϕ(2)x1E1 + &middot; &middot; &middot; +Dϕ(r)xr&minus;1Er&minus;1.
(1.42)</p>
<p/>
</div>
<div class="page"><p/>
<p>1.4 Stability of Iterative Algorithms 11
</p>
<p>The product of the matrices Dϕ(r) &middot; &middot; &middot;Dϕ(1) is the matrix which contains the deriva-
tives of the output data with respect to the input data (chain rule)
</p>
<p>Dϕ =Dϕ(r) &middot; &middot; &middot;Dϕ(1) =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>&part;y1
&part;x1
</p>
<p>&middot; &middot; &middot; &part;y1
&part;xn
</p>
<p>...
. . .
</p>
<p>...
&part;ym
&part;x1
</p>
<p>&middot; &middot; &middot; &part;ym
&part;xn
</p>
<p>⎞
⎟⎟⎠ . (1.43)
</p>
<p>The first two contributions to the total error do not depend on the way in which the
algorithm is divided into elementary steps in contrary to the remaining summands.
Hence the inevitable error which is inherent to the problem can be estimated as
[244]
</p>
<p>�(in)yi = εM |yi | +
n&sum;
</p>
<p>j=1
</p>
<p>∣∣∣∣
&part;yi
</p>
<p>&part;xj
</p>
<p>∣∣∣∣|�xj | (1.44)
</p>
<p>or in case the error of the input data is dominated by the round-off errors |�xj | &le;
εM |xj |
</p>
<p>�(in)yi = εM |yi | + εM
n&sum;
</p>
<p>j=1
</p>
<p>∣∣∣∣
&part;yi
</p>
<p>&part;xj
</p>
<p>∣∣∣∣|xj |. (1.45)
</p>
<p>Additional errors which are smaller than this inevitable error can be regarded as
harmless. If all errors are harmless, the algorithm can be considered well behaved.
</p>
<p>1.4 Stability of Iterative Algorithms
</p>
<p>Often iterative algorithms are used which generate successive values starting from
an initial value x0 according to an iteration method
</p>
<p>xj+1 = f (xj ), (1.46)
</p>
<p>for instance to solve a large system of equations or to approximate a time evolu-
tion xj &asymp; x(j�t). Consider first a linear iteration equation which can be written in
matrix form as
</p>
<p>xj+1 =Axj . (1.47)
</p>
<p>If the matrix A is the same for all steps we have simply
</p>
<p>xj =Ajx0. (1.48)
</p>
<p>Consider the unavoidable error originating from errors �x of the start values:
</p>
<p>xj =Aj (x0 +�x)=Ajx0 +Aj�x. (1.49)</p>
<p/>
</div>
<div class="page"><p/>
<p>12 1 Error Analysis
</p>
<p>The initial errors �x can be enhanced exponentially if A has at least one eigenvalue5
</p>
<p>λ with |λ| &gt; 1. On the other hand the algorithm is conditionally stable if for all
eigenvalues |λ| &le; 1 holds. For a more general nonlinear iteration
</p>
<p>xj+1 = ϕ(xj ) (1.50)
</p>
<p>the error propagates according to
</p>
<p>x1 = ϕ(x0)+Dϕ�x
x2 = ϕ(x1)= ϕ
</p>
<p>(
ϕ(x0)
</p>
<p>)
+ (Dϕ)2�x
</p>
<p>...
</p>
<p>xj = ϕ
(
ϕ &middot; &middot; &middot;ϕ(x0)
</p>
<p>)
+ (Dϕ)j�x.
</p>
<p>(1.51)
</p>
<p>The algorithm is conditionally stable if all eigenvalues of the derivative matrix Dϕ
have absolute values |λ| &le; 1.
</p>
<p>1.5 Example: Rotation
</p>
<p>Consider a simple rotation in the complex plane. The equation of motion
</p>
<p>ż= iωz (1.52)
</p>
<p>obviously has the exact solution
</p>
<p>z(t)= z0eiωt . (1.53)
</p>
<p>As a simple algorithm for numerical integration we use a time grid
</p>
<p>tj = j�t j = 0,1,2 . . . (1.54)
zj = z(tj ) (1.55)
</p>
<p>and iterate the function values
</p>
<p>zj+1 = zj + ż(tj )= (1 + iω�t)zj . (1.56)
</p>
<p>Since
</p>
<p>|1 + iω�t | =
&radic;
</p>
<p>1 +ω2�t2 &gt; 1 (1.57)
</p>
<p>uncertainties in the initial condition will grow exponentially and the algorithm is
not stable. A stable method is obtained by taking the derivative in the middle of the
time interval (page 213)
</p>
<p>ż
</p>
<p>(
t + �t
</p>
<p>2
</p>
<p>)
= iωz
</p>
<p>(
t + �t
</p>
<p>2
</p>
<p>)
</p>
<p>5The eigenvalues of A are solutions of the eigenvalue equation Ax= λx (Chap. 9).</p>
<p/>
</div>
<div class="page"><p/>
<p>1.6 Truncation Error 13
</p>
<p>and making the approximation (page 214)
</p>
<p>z
</p>
<p>(
t + �t
</p>
<p>2
</p>
<p>)
&asymp; z(t)+ z(t +�t)
</p>
<p>2
.
</p>
<p>This gives the implicit equation
</p>
<p>zj+1 = zj + iω�t
zj+1 + zj
</p>
<p>2
(1.58)
</p>
<p>which can be solved by
</p>
<p>zj+1 =
1 + iω�t2
1 &minus; iω�t2
</p>
<p>zj . (1.59)
</p>
<p>Now we have
</p>
<p>∣∣∣∣
1 + iω�t2
1 &minus; iω�t2
</p>
<p>∣∣∣∣=
</p>
<p>&radic;
1 + ω2�t24&radic;
1 + ω2�t24
</p>
<p>= 1 (1.60)
</p>
<p>and the calculated orbit is stable.
</p>
<p>1.6 Truncation Error
</p>
<p>The algorithm in the last example is stable but of course not perfect. Each step
produces an error due to the finite time step. The exact solution
</p>
<p>z(t +�t)= z(t)eiω�t = z(t)
(
</p>
<p>1 + iω�t &minus; ω
2�t2
</p>
<p>2
+ &minus;iω
</p>
<p>3�t3
</p>
<p>6
&middot; &middot; &middot;
</p>
<p>)
(1.61)
</p>
<p>is approximated by
</p>
<p>z(t +�t)&asymp; z(t)
1 + iω�t2
1 &minus; iω�t2
</p>
<p>= z(t)
(
</p>
<p>1 + iω�t
2
</p>
<p>)(
1 + iω�t
</p>
<p>2
&minus; ω
</p>
<p>2�t2
</p>
<p>4
&minus; iω
</p>
<p>3�t3
</p>
<p>8
+ &middot; &middot; &middot;
</p>
<p>)
(1.62)
</p>
<p>= z(t)
(
</p>
<p>1 + iω�t &minus; ω
2�t2
</p>
<p>2
+ &minus;iω
</p>
<p>3�t3
</p>
<p>4
&middot; &middot; &middot;
</p>
<p>)
(1.63)
</p>
<p>which deviates from the exact solution by a term of the order O(�t3), hence the
local error order of this algorithm is O(�t3) which is indicated by writing
</p>
<p>z(t +�t)= z(t)
1 + iω�t2
1 &minus; iω�t2
</p>
<p>+O
(
�t3
</p>
<p>)
. (1.64)
</p>
<p>Integration up to a total time T = N�t accumulates a global error of the order
N�t3 = T�t2.</p>
<p/>
</div>
<div class="page"><p/>
<p>14 1 Error Analysis
</p>
<p>Table 1.4 Maximum and minimum integers
</p>
<p>Java format Bit length Minimum Maximum
</p>
<p>Byte 8 &minus;128 127
Short 16 &minus;32768 32767
Integer 32 &minus;2147483647 2147483648
Long 64 &minus;9223372036854775808 9223372036854775807
Char 16 0 65535
</p>
<p>1.7 Problems
</p>
<p>Problem 1.1 (Machine precision) In this computer experiment we determine the
machine precision εM . Starting with a value of 1.0, x is divided repeatedly by 2 until
numerical addition of 1 and x = 2&minus;M gives 1. Compare single and double precision
calculations.
</p>
<p>Problem 1.2 (Maximum and minimum integers) Integers are used as counters or to
encode elements of a finite set like characters or colors. There are different integer
formats available which store signed or unsigned integers of different length (Ta-
ble 1.4). There is no infinite integer and addition of 1 to the maximum integer gives
the minimum integer.
</p>
<p>In this computer experiment we determine the smallest and largest integer num-
bers. Beginning with I = 1 we add repeatedly 1 until the condition I + 1 &gt; I be-
comes invalid or subtract repeatedly 1 until I &minus; 1 &lt; I becomes invalid. For the 64
bit long integer format this takes to long. Here we multiply alternatively I by 2 un-
til I &minus; 1 &lt; I becomes invalid. For the character format the corresponding ordinal
number is shown which is obtained by casting the character to an integer.
</p>
<p>Problem 1.3 (Truncation error) This computer experiment approximates the cosine
function by a truncated Taylor series
</p>
<p>cos(x)&asymp; mycos(x,nmax)=
nmax&sum;
</p>
<p>n=0
(&minus;)n x
</p>
<p>2n
</p>
<p>(2n)! = 1 &minus;
x2
</p>
<p>2
+ x
</p>
<p>4
</p>
<p>24
&minus; x
</p>
<p>6
</p>
<p>720
+ &middot; &middot; &middot; (1.65)
</p>
<p>in the interval &minus;π/2 &lt; x &lt; π/2. The function mycos(x,nmax) is numerically com-
pared to the intrinsic cosine function.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 2
</p>
<p>Interpolation
</p>
<p>Experiments usually produce a discrete set of data points (xi, fi) which represent
the value of a function f (x) for a finite set of arguments {x0 &middot; &middot; &middot;xn}. If additional
data points are needed, for instance to draw a continuous curve, interpolation is nec-
essary. Interpolation also can be helpful to represent a complicated function by a
simpler one or to develop more sophisticated numerical methods for the calculation
of numerical derivatives and integrals. In the following we concentrate on the most
important interpolating functions which are polynomials, splines and rational func-
tions. Trigonometric interpolation is discussed in Chap. 7. An interpolating function
reproduces the given function values at the interpolation points exactly (Fig. 2.1).
The more general procedure of curve fitting, where this requirement is relaxed, is
discussed in Chap. 10.
</p>
<p>The interpolating polynomial can be explicitly constructed with the Lagrange
method. Newton&rsquo;s method is numerically efficient if the polynomial has to be eval-
uated at many interpolating points and Neville&rsquo;s method has advantages if the poly-
nomial is not needed explicitly and has to be evaluated only at one interpolation
point.
</p>
<p>Polynomials are not well suited for interpolation over a larger range. Spline func-
tions can be superior which are piecewise defined polynomials. Especially cubic
splines are often used to draw smooth curves. Curves with poles can be represented
by rational interpolating functions whereas a special class of rational interpolants
without poles provides a rather new alternative to spline interpolation.
</p>
<p>2.1 Interpolating Functions
</p>
<p>Consider the following problem: Given are n+ 1 sample points (xi, fi), i = 0 &middot; &middot; &middot;n
and a function of x which depends on n+ 1 parameters ai :
</p>
<p>Φ(x;a0 &middot; &middot; &middot;an). (2.1)
</p>
<p>P.O.J. Scherer, Computational Physics, Graduate Texts in Physics,
DOI 10.1007/978-3-319-00401-3_2,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>15</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-00401-3_2">http://dx.doi.org/10.1007/978-3-319-00401-3_2</a></div>
</div>
<div class="page"><p/>
<p>16 2 Interpolation
</p>
<p>Fig. 2.1 (Interpolating
function) The interpolating
function Φ(x) reproduces a
given data set Φ(xi)= fi and
provides an estimate of the
function f (x) between the
data points
</p>
<p>The parameters are to be determined such that the interpolating function has the
proper values at all sample points (Fig. 2.1)
</p>
<p>Φ(xi;a0 &middot; &middot; &middot;an)= fi i = 0 &middot; &middot; &middot;n. (2.2)
</p>
<p>An interpolation problem is called linear if the interpolating function is a linear
combination of functions
</p>
<p>Φ(x;a0 &middot; &middot; &middot;an)= a0Φ0(x)+ a1Φ1(x)+ &middot; &middot; &middot; + anΦn(x). (2.3)
</p>
<p>Important examples are
</p>
<p>&bull; polynomials
a0 + a1x + &middot; &middot; &middot; + anxn (2.4)
</p>
<p>&bull; trigonometric functions
a0 + a1eix + a2e2ix + &middot; &middot; &middot; + anenix (2.5)
</p>
<p>&bull; spline functions which are piecewise polynomials, for instance the cubic spline
</p>
<p>s(x)= αi + βi(x &minus; xi)+ γi(x &minus; xi)2 + δi(x &minus; xi)3 xi &le; x &le; xi+1. (2.6)
</p>
<p>Important examples for nonlinear interpolating functions are
</p>
<p>&bull; rational functions
p0 + p1x + &middot; &middot; &middot; + pMxM
q0 + q1x + &middot; &middot; &middot; + qNxN
</p>
<p>(2.7)
</p>
<p>&bull; exponential functions
a0e
</p>
<p>λ0x + a1eλ1x + &middot; &middot; &middot; . (2.8)
where amplitudes ai as well as exponents λi have to be optimized.
</p>
<p>2.2 Polynomial Interpolation
</p>
<p>For n+ 1 sample points (xi, fi), i = 0 &middot; &middot; &middot;n, xi �= xj there exists exactly one inter-
polating polynomial of degree n with
</p>
<p>p(xi)= fi, i = 0 &middot; &middot; &middot;n. (2.9)</p>
<p/>
</div>
<div class="page"><p/>
<p>2.2 Polynomial Interpolation 17
</p>
<p>2.2.1 Lagrange Polynomials
</p>
<p>Lagrange polynomials [137] are defined as
</p>
<p>Li(x)=
(x &minus; x0) &middot; &middot; &middot; (x &minus; xi&minus;1)(x &minus; xi+1) &middot; &middot; &middot; (x &minus; xn)
</p>
<p>(xi &minus; x0) &middot; &middot; &middot; (xi &minus; xi&minus;1)(xi &minus; xi+1) &middot; &middot; &middot; (xi &minus; xn)
. (2.10)
</p>
<p>They are of degree n and have the property
</p>
<p>Li(xk)= δi,k. (2.11)
</p>
<p>The interpolating polynomial is given in terms of Lagrange polynomials by
</p>
<p>p(x)=
n&sum;
</p>
<p>i=0
fiLi(x)=
</p>
<p>n&sum;
</p>
<p>i=0
fi
</p>
<p>n&prod;
</p>
<p>k=0,k �=i
</p>
<p>x &minus; xk
xi &minus; xk
</p>
<p>. (2.12)
</p>
<p>2.2.2 Barycentric Lagrange Interpolation
</p>
<p>With the polynomial
</p>
<p>ω(x)=
n&prod;
</p>
<p>i=0
(x &minus; xi) (2.13)
</p>
<p>the Lagrange polynomial can be written as
</p>
<p>Li(x)=
ω(x)
</p>
<p>x &minus; xi
1&prod;n
</p>
<p>k=0,k �=i(xi &minus; xk)
(2.14)
</p>
<p>which, introducing the Barycentric weights [24]
</p>
<p>ui =
1&prod;n
</p>
<p>k=0,k �=i(xi &minus; xk)
(2.15)
</p>
<p>becomes the first form of the barycentric interpolation formula
</p>
<p>Li(x)= ω(x)
ui
</p>
<p>x &minus; xi
. (2.16)
</p>
<p>The interpolating polynomial can now be evaluated according to
</p>
<p>p(x)=
n&sum;
</p>
<p>i=0
fiLi(x)= ω(x)
</p>
<p>n&sum;
</p>
<p>i=0
fi
</p>
<p>ui
</p>
<p>x &minus; xi
. (2.17)</p>
<p/>
</div>
<div class="page"><p/>
<p>18 2 Interpolation
</p>
<p>Having computed the weights ui , evaluation of the polynomial only requires O(n)
operations whereas calculation of all the Lagrange polynomials requires O(n2) op-
erations. Calculation of ω(x) can be avoided considering that
</p>
<p>p1(x)=
n&sum;
</p>
<p>i=0
Li(x)= ω(x)
</p>
<p>n&sum;
</p>
<p>i=0
</p>
<p>ui
</p>
<p>x &minus; xi
(2.18)
</p>
<p>is a polynomial of degree n with
</p>
<p>p1(xi)= 1 i = 0 &middot; &middot; &middot;n. (2.19)
</p>
<p>But this is only possible if
</p>
<p>p1(x)= 1. (2.20)
Therefore
</p>
<p>p(x)= p(x)
p1(x)
</p>
<p>=
&sum;n
</p>
<p>i=0 fi
ui
</p>
<p>x&minus;xi&sum;n
i=0
</p>
<p>ui
x&minus;xi
</p>
<p>(2.21)
</p>
<p>which is known as the second form of the barycentric interpolation formula.
</p>
<p>2.2.3 Newton&rsquo;s Divided Differences
</p>
<p>Newton&rsquo;s method of divided differences [138] is an alternative for efficient numeri-
cal calculations [271]. Rewrite
</p>
<p>f (x)= f (x0)+
f (x)&minus; f (x0)
</p>
<p>x &minus; x0
(x &minus; x0). (2.22)
</p>
<p>With the first order divided difference
</p>
<p>f [x, x0] =
f (x)&minus; f (x0)
</p>
<p>x &minus; x0
(2.23)
</p>
<p>this becomes
</p>
<p>f [x, x0] = f [x1, x0] +
f [x, x0] &minus; f [x1, x0]
</p>
<p>x &minus; x1
(x &minus; x1) (2.24)
</p>
<p>and with the second order divided difference
</p>
<p>f [x, x0, x1] =
f [x, x0] &minus; f [x1, x0]
</p>
<p>x &minus; x1
= f (x)&minus; f (x0)
</p>
<p>(x &minus; x0)(x &minus; x1)
&minus; f (x1)&minus; f (x0)
</p>
<p>(x1 &minus; x0)(x &minus; x1)
</p>
<p>= f (x)
(x &minus; x0)(x &minus; x1)
</p>
<p>+ f (x1)
(x1 &minus; x0)(x1 &minus; x)
</p>
<p>+ f (x0)
(x0 &minus; x1)(x0 &minus; x)
</p>
<p>(2.25)</p>
<p/>
</div>
<div class="page"><p/>
<p>2.2 Polynomial Interpolation 19
</p>
<p>we have
</p>
<p>f (x)= f (x0)+ (x &minus; x0)f [x1, x0] + (x &minus; x0)(x &minus; x1)f [x, x0, x1]. (2.26)
</p>
<p>Higher order divided differences are defined recursively by
</p>
<p>f [x1x2 &middot; &middot; &middot;xr&minus;1xr ] =
f [x1x2 &middot; &middot; &middot;xr&minus;1] &minus; f [x2 &middot; &middot; &middot;xr&minus;1xr ]
</p>
<p>x1 &minus; xr
. (2.27)
</p>
<p>They are invariant against permutation of the arguments which can be seen from the
explicit formula
</p>
<p>f [x1x2 &middot; &middot; &middot;xr ] =
r&sum;
</p>
<p>k=1
</p>
<p>f (xk)&prod;
i �=k(xk &minus; xi)
</p>
<p>. (2.28)
</p>
<p>Finally we have
</p>
<p>f (x)= p(x)+ q(x) (2.29)
with a polynomial of degree n
</p>
<p>p(x) = f (x0)+ f [x1, x0](x &minus; x0)+ f [x2x1x0](x &minus; x0)(x &minus; x1)+ &middot; &middot; &middot;
+ f [xnxn&minus;1 &middot; &middot; &middot;x0](x &minus; x0)(x &minus; x1) &middot; &middot; &middot; (x &minus; xn&minus;1) (2.30)
</p>
<p>and the function
</p>
<p>q(x)= f [xxn &middot; &middot; &middot;x0](x &minus; x0) &middot; &middot; &middot; (x &minus; xn). (2.31)
</p>
<p>Obviously q(xi)= 0, i = 0 &middot; &middot; &middot;n, hence p(x) is the interpolating polynomial.
</p>
<p>Algorithm The divided differences are arranged in the following way:
</p>
<p>f0
</p>
<p>f1 f [x0x1]
...
</p>
<p>...
. . .
</p>
<p>fn&minus;1 f [xn&minus;2xn&minus;1] f [xn&minus;3xn&minus;2xn&minus;1] &middot; &middot; &middot; f [x0 &middot; &middot; &middot;xn&minus;1]
fn f [xn&minus;1xn] f [xn&minus;2xn&minus;1xn] &middot; &middot; &middot; f [x1 &middot; &middot; &middot;xn] f [x0 &middot; &middot; &middot;xn]
</p>
<p>(2.32)
</p>
<p>Since only the diagonal elements are needed, a one-dimensional data array
t[0] &middot; &middot; &middot; t[n] is sufficient for the calculation of the polynomial coefficients:
</p>
<p>for i := 0 to n do begin
t[i] := f [i];
for k := i &minus; 1 downto 0 do
</p>
<p>t[k] := (t[k + 1] &minus; t[k])/(x[i] &minus; x[k]);
a[i] := t[0];
</p>
<p>end;</p>
<p/>
</div>
<div class="page"><p/>
<p>20 2 Interpolation
</p>
<p>The value of the polynomial is then evaluated by
</p>
<p>p := a[n];
for i := n&minus; 1 downto 0 do
</p>
<p>p := p&lowast;(x &minus; x[i])+ a[i];
</p>
<p>2.2.4 Neville Method
</p>
<p>The Neville method [180] is advantageous if the polynomial is not needed explicitly
and has to be evaluated only at one point. Consider the interpolating polynomial for
the points x0 &middot; &middot; &middot;xk , which will be denoted as P0,1...k(x). Obviously
</p>
<p>P0,1...k(x)=
(x &minus; x0)P1&middot;&middot;&middot;k(x)&minus; (x &minus; xk)P0&middot;&middot;&middot;k&minus;1(x)
</p>
<p>xk &minus; x0
(2.33)
</p>
<p>since for x = x1 &middot; &middot; &middot;xk&minus;1 the right hand side is
(x &minus; x0)f (x)&minus; (x &minus; xk)f (x)
</p>
<p>xk &minus; x0
= f (x). (2.34)
</p>
<p>For x = x0 we have
&minus;(x0 &minus; xk)f (x)
</p>
<p>xk &minus; x0
= f (x) (2.35)
</p>
<p>and finally for x = xk
(xk &minus; x0)f (x)
</p>
<p>xk &minus; x0
= f (x). (2.36)
</p>
<p>Algorithm We use the following scheme to calculate P0,1&middot;&middot;&middot;n(x) recursively:
</p>
<p>P0
</p>
<p>P1 P01
</p>
<p>P2 P12 P012
</p>
<p>...
...
</p>
<p>...
. . .
</p>
<p>Pn Pn&minus;1,n Pn&minus;2,n&minus;1,n &middot; &middot; &middot; P01&middot;&middot;&middot;n
</p>
<p>(2.37)
</p>
<p>The first column contains the function values Pi(x) = fi . The value P01&middot;&middot;&middot;n can be
calculated using a 1-dimensional data array p[0] &middot; &middot; &middot;p[n]:
</p>
<p>for i := 0 to n do begin
p[i] := f [i];
for k := i &minus; 1 downto 0 do
</p>
<p>p[k] := (p[k+ 1]&lowast;(x &minus; x[k])&minus; p[k]&lowast;(x &minus; x[i]))/(x[k] &minus; x[i]);
end;
f := p[0];</p>
<p/>
</div>
<div class="page"><p/>
<p>2.2 Polynomial Interpolation 21
</p>
<p>Fig. 2.2 (Interpolating
polynomial) The interpolated
function (solid curve) and the
interpolating polynomial
(broken curve) for the
example (2.40) are compared
</p>
<p>2.2.5 Error of Polynomial Interpolation
</p>
<p>The error of polynomial interpolation [12] can be estimated with the help of the
following theorem:
</p>
<p>If f (x) is n + 1 times differentiable then for each x there exists ξ within the
smallest interval containing x as well as all the xi with
</p>
<p>q(x)=
n&prod;
</p>
<p>i=0
(x &minus; xi)
</p>
<p>f (n+1)(ξ)
</p>
<p>(n+ 1)! . (2.38)
</p>
<p>From a discussion of the function
</p>
<p>ω(x)=
n&prod;
</p>
<p>i=0
(x &minus; xi) (2.39)
</p>
<p>it can be seen that the error increases rapidly outside the region of the sample points
(extrapolation is dangerous!). As an example consider the sample points (Fig. 2.2)
</p>
<p>f (x)= sin(x) xi = 0,
π
</p>
<p>2
,π,
</p>
<p>3π
</p>
<p>2
,2π. (2.40)
</p>
<p>The maximum interpolation error is estimated by (|f (n+1)| &le; 1)
</p>
<p>∣∣f (x)&minus; p(x)
∣∣&le;
</p>
<p>∣∣ω(x)
∣∣ 1
120
</p>
<p>&le; 35
120
</p>
<p>&asymp; 0.3 (2.41)
</p>
<p>whereas the error increases rapidly outside the interval 0 &lt; x &lt; 2π (Fig. 2.3).</p>
<p/>
</div>
<div class="page"><p/>
<p>22 2 Interpolation
</p>
<p>Fig. 2.3 (Interpolation error)
The polynomial ω(x) is
shown for the example (2.40).
Its roots xi are given by the x
values of the sample points
(circles). Inside the interval
x0 &middot; &middot; &middot;x4 the absolute value of
ω is bounded by |ω(x)| &le; 35
whereas outside the interval it
increases very rapidly
</p>
<p>2.3 Spline Interpolation
</p>
<p>Polynomials are not well suited for interpolation over a larger range. Often spline
functions are superior which are piecewise defined polynomials [186, 228]. The
simplest case is a linear spline which just connects the sampling points by straight
lines:
</p>
<p>pi(x)= yi +
yi+1 &minus; yi
xi+1 &minus; xi
</p>
<p>(x &minus; xi) (2.42)
</p>
<p>s(x)= pi(x) where xi &le; x &lt; xi+1. (2.43)
The most important case is the cubic spline which is given in the interval xi &le; x &lt;
xi+1 by
</p>
<p>pi(x)= αi + βi(x &minus; xi)+ γi(x &minus; xi)2 + δi(x &minus; xi)3. (2.44)
We want to have a smooth interpolation and assume that the interpolating function
and their first two derivatives are continuous. Hence we have for the inner bound-
aries:
</p>
<p>i = 0 &middot; &middot; &middot;n&minus; 1
</p>
<p>pi(xi+1)= pi+1(xi+1) (2.45)
</p>
<p>p&prime;i(xi+1)= p&prime;i+1(xi+1) (2.46)
</p>
<p>p&prime;&prime;i (xi+1)= p&prime;&prime;i+1(xi+1). (2.47)
We have to specify boundary conditions at x0 and xn. The most common choice are
natural boundary conditions s&prime;&prime;(x0) = s&prime;&prime;(xn) = 0, but also periodic boundary con-
ditions s&prime;&prime;(x0) = s&prime;&prime;(xn), s&prime;(x0) = s&prime;(xn), s(x0) = s(xn) or given derivative values
s&prime;(x0) and s&prime;(xn) are often used. The second derivative is a linear function [244]
</p>
<p>p&prime;&prime;i (x)= 2γi + 6δi(x &minus; xi) (2.48)</p>
<p/>
</div>
<div class="page"><p/>
<p>2.3 Spline Interpolation 23
</p>
<p>which can be written using hi+1 = xi+1 &minus; xi and Mi = s&prime;&prime;(xi) as
</p>
<p>p&prime;&prime;i (x)=Mi+1
(x &minus; xi)
hi+1
</p>
<p>+Mi
(xi+1 &minus; x)
</p>
<p>hi+1
i = 0 &middot; &middot; &middot;n&minus; 1 (2.49)
</p>
<p>since
</p>
<p>p&prime;&prime;i (xi)=Mi
xi+1 &minus; xi
hi+1
</p>
<p>= s&prime;&prime;(xi) (2.50)
</p>
<p>p&prime;&prime;i (xi+1)=Mi+1
(xi+1 &minus; xi)
</p>
<p>hi+1
= s&prime;&prime;(xi+1). (2.51)
</p>
<p>Integration gives with the two constants Ai and Bi
</p>
<p>p&prime;i(x)=Mi+1
(x &minus; xi)2
</p>
<p>2hi+1
&minus;Mi
</p>
<p>(xi+1 &minus; x)2
2hi+1
</p>
<p>+Ai (2.52)
</p>
<p>pi(x)=Mi+1
(x &minus; xi)3
</p>
<p>6hi+1
+Mi
</p>
<p>(xi+1 &minus; x)3
6hi+1
</p>
<p>+Ai(x &minus; xi)+Bi . (2.53)
</p>
<p>From s(xi)= yi and s(xi+1)= yi+1 we have
</p>
<p>Mi
h2i+1
</p>
<p>6
+Bi = yi (2.54)
</p>
<p>Mi+1
h2i+1
</p>
<p>6
+Aihi+1 +Bi = yi+1 (2.55)
</p>
<p>and hence
</p>
<p>Bi = yi &minus;Mi
h2i+1
</p>
<p>6
(2.56)
</p>
<p>Ai =
yi+1 &minus; yi
hi+1
</p>
<p>&minus; hi+1
6
</p>
<p>(Mi+1 &minus;Mi). (2.57)
</p>
<p>Now the polynomial is
</p>
<p>pi(x) =
Mi+1
6hi+1
</p>
<p>(x &minus; xi)3 &minus;
Mi
</p>
<p>6hi+1
(x &minus; xi &minus; hi+1)3 +Ai(x &minus; xi)+Bi
</p>
<p>= (x &minus; xi)3
(
Mi+1
6hi+1
</p>
<p>&minus; Mi
6hi+1
</p>
<p>)
+ Mi
</p>
<p>6hi+1
3hi+1(x &minus; xi)2
</p>
<p>+ (x &minus; xi)
(
Ai &minus;
</p>
<p>Mi
</p>
<p>6hi+1
3h2i+1
</p>
<p>)
+Bi +
</p>
<p>Mi
</p>
<p>6hi+1
h3i+1. (2.58)
</p>
<p>Comparison with
</p>
<p>pi(x)= αi + βi(x &minus; xi)+ γi(x &minus; xi)2 + δi(x &minus; xi)3 (2.59)</p>
<p/>
</div>
<div class="page"><p/>
<p>24 2 Interpolation
</p>
<p>gives
</p>
<p>αi = Bi +
Mi
</p>
<p>6
h2i+1 = yi (2.60)
</p>
<p>βi =Ai &minus;
hi+1Mi
</p>
<p>2
= yi+1 &minus; yi
</p>
<p>hi+1
&minus; hi+1
</p>
<p>Mi+1 + 2Mi
6
</p>
<p>(2.61)
</p>
<p>γi =
Mi
</p>
<p>2
(2.62)
</p>
<p>δi =
Mi+1 &minus;Mi
</p>
<p>6hi+1
. (2.63)
</p>
<p>Finally we calculate Mi from the continuity of s&prime;(x). Substituting for Ai in p&prime;i(x)
we have
</p>
<p>p&prime;i(x)=Mi+1
(x &minus; xi)2
</p>
<p>2hi+1
&minus;Mi
</p>
<p>(xi+1 &minus; x)2
2hi+1
</p>
<p>+ yi+1 &minus; yi
hi+1
</p>
<p>&minus; hi+1
6
</p>
<p>(Mi+1 &minus;Mi)
(2.64)
</p>
<p>and from p&prime;i&minus;1(xi)= p&prime;i(xi) it follows
</p>
<p>Mi
hi
</p>
<p>2
+ yi &minus; yi&minus;1
</p>
<p>hi
&minus; hi
</p>
<p>6
(Mi &minus;Mi&minus;1)
</p>
<p>=&minus;Mi
hi+1
</p>
<p>2
+ yi+1 &minus; yi
</p>
<p>hi+1
&minus; hi+1
</p>
<p>6
(Mi+1 &minus;Mi) (2.65)
</p>
<p>Mi
hi
</p>
<p>3
+Mi&minus;1
</p>
<p>hi
</p>
<p>6
+Mi
</p>
<p>hi+1
3
</p>
<p>+Mi+1
hi+1
</p>
<p>6
= yi+1 &minus; yi
</p>
<p>hi+1
&minus; yi &minus; yi&minus;1
</p>
<p>hi
(2.66)
</p>
<p>which is a system of linear equations for the Mi . Using the abbreviations
</p>
<p>λi =
hi+1
</p>
<p>hi + hi+1
(2.67)
</p>
<p>μi = 1 &minus; λi =
hi
</p>
<p>hi + hi+1
(2.68)
</p>
<p>di =
6
</p>
<p>hi + hi+1
</p>
<p>(
yi+1 &minus; yi
hi+1
</p>
<p>&minus; yi &minus; yi&minus;1
hi
</p>
<p>)
(2.69)
</p>
<p>we have
</p>
<p>μiMi&minus;1 + 2Mi + λiMi+1 = di i = 1 &middot; &middot; &middot;n&minus; 1. (2.70)
</p>
<p>We define for natural boundary conditions
</p>
<p>λ0 = 0 μn = 0 d0 = 0 dn = 0 (2.71)
</p>
<p>and in case of given derivative values
</p>
<p>λ0 = 1 μn = 1 d0 =
6
</p>
<p>h1
</p>
<p>(
y1 &minus; y0
</p>
<p>h1
&minus;y&prime;0
</p>
<p>)
dn =
</p>
<p>6
</p>
<p>hn
</p>
<p>(
y&prime;n&minus;
</p>
<p>yn &minus; yn&minus;1
hn
</p>
<p>)
. (2.72)</p>
<p/>
</div>
<div class="page"><p/>
<p>2.4 Rational Interpolation 25
</p>
<p>The system of equation has the form
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
</p>
<p>2 λ0
μ1 2 λ1
</p>
<p>μ2 2 λ2
. . .
</p>
<p>. . .
. . .
</p>
<p>μn&minus;1 2 λn&minus;1
μn 2
</p>
<p>⎤
⎥⎥⎥⎥⎥⎥⎥⎦
</p>
<p>⎡
⎢⎢⎢⎢⎢⎢⎢⎣
</p>
<p>M0
M1
M2
...
</p>
<p>Mn&minus;1
Mn
</p>
<p>⎤
⎥⎥⎥⎥⎥⎥⎥⎦
=
</p>
<p>⎡
⎢⎢⎢⎢⎢⎢⎢⎣
</p>
<p>d0
d1
d2
...
</p>
<p>dn&minus;1
dn
</p>
<p>⎤
⎥⎥⎥⎥⎥⎥⎥⎦
. (2.73)
</p>
<p>For periodic boundary conditions we define
</p>
<p>λn =
h1
</p>
<p>h1 + hn
μn = 1 &minus; λn dn =
</p>
<p>6
</p>
<p>h1 + hn
</p>
<p>(
y1 &minus; yn
</p>
<p>h1
&minus; yn &minus; yn&minus;1
</p>
<p>hn
</p>
<p>)
(2.74)
</p>
<p>and the system of equations is (with Mn =M0)
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
</p>
<p>2 λ1 μ1
μ2 2 λ2
</p>
<p>μ3 2 λ3
. . .
</p>
<p>. . .
. . .
</p>
<p>μn&minus;1 2 λn&minus;1
λn μn 2
</p>
<p>⎤
⎥⎥⎥⎥⎥⎥⎥⎦
</p>
<p>⎡
⎢⎢⎢⎢⎢⎢⎢⎣
</p>
<p>M1
M2
M3
...
</p>
<p>Mn&minus;1
Mn
</p>
<p>⎤
⎥⎥⎥⎥⎥⎥⎥⎦
=
</p>
<p>⎡
⎢⎢⎢⎢⎢⎢⎢⎣
</p>
<p>d1
d2
d3
...
</p>
<p>dn&minus;1
dn
</p>
<p>⎤
⎥⎥⎥⎥⎥⎥⎥⎦
. (2.75)
</p>
<p>All this tridiagonal systems can be easily solved with a special Gaussian elimination
method (Sects. 5.3, 5.4).
</p>
<p>2.4 Rational Interpolation
</p>
<p>The use of rational approximants allows to interpolate functions with poles, where
polynomial interpolation can give poor results [244]. Rational approximants with-
out poles [90] are also well suited for the case of equidistant xi , where higher order
polynomials tend to become unstable. The main disadvantages are additional poles
which are difficult to control and the appearance of unattainable points. Recent de-
velopments using the barycentric form of the interpolating function [25, 90, 227]
helped to overcome these difficulties.
</p>
<p>2.4.1 Pad&eacute; Approximant
</p>
<p>The Pad&eacute; approximant [13] of order [M/N ] to a function f (x) is the rational func-
tion
</p>
<p>RM/N (x)=
PM(x)
</p>
<p>QN (x)
= p0 + p1x + &middot; &middot; &middot; + pMx
</p>
<p>M
</p>
<p>q0 + q1x + &middot; &middot; &middot; + qNxN
(2.76)</p>
<p/>
</div>
<div class="page"><p/>
<p>26 2 Interpolation
</p>
<p>which reproduces the McLaurin series (the Taylor series at x = 0) of
</p>
<p>f (x)= a0 + a1x + a2x2 + &middot; &middot; &middot; (2.77)
</p>
<p>up to order M +N , i.e.
</p>
<p>f (0)=R(0)
d
</p>
<p>dx
f (0)= d
</p>
<p>dx
R(0)
</p>
<p>(2.78)
...
</p>
<p>d(M+N)
</p>
<p>dx(M+N)
f (0)= d
</p>
<p>(M+N)
</p>
<p>dx(M+N)
R(0).
</p>
<p>Multiplication gives
</p>
<p>p0 + p1x + &middot; &middot; &middot; + pMxM =
(
q0 + q1x + &middot; &middot; &middot; + qNxN
</p>
<p>)
(a0 + a1x + &middot; &middot; &middot; ) (2.79)
</p>
<p>and collecting powers of x we find the system of equations
</p>
<p>p0 = q0a0
</p>
<p>p1 = q0a1 + q1a0
</p>
<p>p2 = q0a2 + a1q1 + a0q2
...
</p>
<p>pM = q0aM + aM&minus;1q1 + &middot; &middot; &middot; + a0qM
</p>
<p>0 = q0aM+1 + q1aM + &middot; &middot; &middot; + qNaM&minus;N+1
...
</p>
<p>0 = q0aM+N + q1aM+N&minus;1 + &middot; &middot; &middot; + qNaM
</p>
<p>(2.80)
</p>
<p>where
</p>
<p>an = 0 for n &lt; 0 (2.81)
qj = 0 for j &gt; N. (2.82)
</p>
<p>Example (Calculate the [3,3] approximant to tan(x)) The Laurent series of the tan-
gent is
</p>
<p>tan(x)= x + 1
3
x3 + 2
</p>
<p>15
x5 + &middot; &middot; &middot; . (2.83)</p>
<p/>
</div>
<div class="page"><p/>
<p>2.4 Rational Interpolation 27
</p>
<p>We set q0 = 1. Comparison of the coefficients of the polynomial
</p>
<p>p0 + p1x + p2x2 + p3x3 =
(
1 + q1x + q2x2 + q3x3
</p>
<p>)(
x + 1
</p>
<p>3
x3 + 2
</p>
<p>15
x5
)
</p>
<p>(2.84)
</p>
<p>gives the equations
</p>
<p>x0 : p0 = 0
x1 : p1 = 1
x2 : p2 = q1
</p>
<p>x3 : p3 = q2 +
1
</p>
<p>3
(2.85)
</p>
<p>x4 : 0 = q3 +
1
</p>
<p>3
q1
</p>
<p>x5 : 0 = 2
15
</p>
<p>+ 1
3
q2
</p>
<p>x6 : 0 = 2
15
</p>
<p>q1 +
1
</p>
<p>3
q3.
</p>
<p>We easily find
</p>
<p>p2 = q1 = q3 = 0 q2 =&minus;
2
</p>
<p>5
p3 =&minus;
</p>
<p>1
</p>
<p>15
(2.86)
</p>
<p>and the approximant of order [3,3] is
</p>
<p>R3,3 =
x &minus; 115x3
</p>
<p>1 &minus; 25x2
. (2.87)
</p>
<p>This expression reproduces the tangent quite well (Fig. 2.4). Its pole at
&radic;
</p>
<p>10/2 &asymp;
1.581 is close to the pole of the tangent function at π/2 &asymp; 1.571.
</p>
<p>2.4.2 Barycentric Rational Interpolation
</p>
<p>If the weights of the barycentric form of the interpolating polynomial (2.21) are
taken as general parameters ui �= 0 it becomes a rational function
</p>
<p>R(x)=
&sum;n
</p>
<p>i=0 fi
ui
</p>
<p>x&minus;xi&sum;n
i=0
</p>
<p>ui
x&minus;xi
</p>
<p>(2.88)
</p>
<p>which obviously interpolates the data points since
</p>
<p>lim
x&rarr;xi
</p>
<p>R(x)= fi . (2.89)</p>
<p/>
</div>
<div class="page"><p/>
<p>28 2 Interpolation
</p>
<p>Fig. 2.4 (Pad&eacute;
approximation to tan(x)) The
Pad&eacute; approximant ((2.87),
dash-dotted curve)
reproduces the tangent (full
curve) quite well
</p>
<p>With the polynomials1
</p>
<p>P(x)=
n&sum;
</p>
<p>i=0
uifi
</p>
<p>n&prod;
</p>
<p>j=0;j �=i
(x &minus; xj )=
</p>
<p>n&sum;
</p>
<p>i=0
uifi
</p>
<p>ω(x)
</p>
<p>x &minus; xi
</p>
<p>Q(x)=
n&sum;
</p>
<p>i=0
ui
</p>
<p>n&prod;
</p>
<p>j=0;j �=i
(x &minus; xj )=
</p>
<p>n&sum;
</p>
<p>i=0
ui
</p>
<p>ω(x)
</p>
<p>x &minus; xi
</p>
<p>a rational interpolating function is given by2
</p>
<p>R(x)= P(x)
Q(x)
</p>
<p>.
</p>
<p>Obviously there are infinitely different rational interpolating functions which differ
by the weights u = (u0, u1 &middot; &middot; &middot;un) (an example is shown in Fig. 2.5). To fix the
parameters ui , additional conditions have to be imposed.
</p>
<p>2.4.2.1 Rational Interpolation of Order [M,N]
</p>
<p>One possibility is to assume that P(x) and Q(x) are of order &le;M and &le;N , respec-
tively with M +N = n. This gives n additional equations for the 2(n+ 1) polyno-
mial coefficients. The number of unknown equals n+ 1 and the rational interpolant
is uniquely determined up to a common factor in numerator and denominator.
</p>
<p>1ω(x)=
&prod;n
</p>
<p>i=0(x &minus; xi) as in (2.39).
2It can be shown that any rational interpolant can be written in this form.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.4 Rational Interpolation 29
</p>
<p>Fig. 2.5 (Rational interpolation) The data points (1, 12 ), (2,
1
5 ), (3,
</p>
<p>1
10 ) are interpolated by several
</p>
<p>rational functions. The [1,1] approximant (2.95) corresponding to u = (5,&minus;20,15) is shown by
the solid curve, the dashed curve shows the function R(x)= 8x2&minus;36x+38
</p>
<p>10(3x2&minus;12x+11) which is obtained for
</p>
<p>u= (1,1,1) and the dash-dotted curve shows the function R(x)= 4x2&minus;20x+26
10(5&minus;4x+x2) which follows for
</p>
<p>u= (1,&minus;1,1) and has no real poles
</p>
<p>Example (Consider the data points f (1)= 12 , f (2)=
1
5 , f (3)=
</p>
<p>1
10 ) The polynomi-
</p>
<p>als are
</p>
<p>P(x) = 1
2
u0(x &minus; 2)(x &minus; 3)+
</p>
<p>1
</p>
<p>5
u1(x &minus; 1)(x &minus; 3)+
</p>
<p>1
</p>
<p>10
u2(x &minus; 1)(x &minus; 2)
</p>
<p>= 3u0 +
3
</p>
<p>5
u1 +
</p>
<p>1
</p>
<p>5
u2 +
</p>
<p>[
&minus;5
</p>
<p>2
u0 &minus;
</p>
<p>4
</p>
<p>5
u1 &minus;
</p>
<p>3
</p>
<p>10
u2
</p>
<p>]
x
</p>
<p>+
[
</p>
<p>1
</p>
<p>2
u0 +
</p>
<p>1
</p>
<p>5
u1 +
</p>
<p>1
</p>
<p>10
u2
</p>
<p>]
x2 (2.90)
</p>
<p>Q(x) = u0(x &minus; 2)(x &minus; 3)+ u1(x &minus; 1)(x &minus; 3)+ u2(x &minus; 1)(x &minus; 2)
= 6u0 + 3u1 + 2u2 + [&minus;5u0 &minus; 4u1 &minus; 3u2]x + [u0 + u1 + u2]x2.
</p>
<p>(2.91)
</p>
<p>To obtain a [1,1] approximant we have to solve the equations
</p>
<p>1
</p>
<p>2
u0 +
</p>
<p>1
</p>
<p>5
u1 +
</p>
<p>1
</p>
<p>10
u2 = 0 (2.92)
</p>
<p>u0 + u1 + u2 = 0 (2.93)
</p>
<p>which gives
</p>
<p>u2 = 3u0 u1 =&minus;4u0 (2.94)</p>
<p/>
</div>
<div class="page"><p/>
<p>30 2 Interpolation
</p>
<p>and thus
</p>
<p>R(x)=
6
5u0 &minus;
</p>
<p>1
5u0x
</p>
<p>2u0x
= 6 &minus; x
</p>
<p>10x
. (2.95)
</p>
<p>General methods to obtain the coefficients ui for a given data set are described in
[25, 227]. They also allow to determine unattainable points corresponding to ui = 0
and to locate the poles. Without loss of generality it can be assumed [227] that
M &ge;N .3
</p>
<p>Let P(x) be the unique polynomial which interpolates the product f (x)Q(x)
</p>
<p>P (xi)= f (xi)Q(xi) i = 0 &middot; &middot; &middot;M. (2.96)
</p>
<p>Then from (2.31) we have
</p>
<p>f (x)Q(x)&minus; P(x)= (fQ)[x0 &middot; &middot; &middot;xM , x](x &minus; x0) &middot; &middot; &middot; (x &minus; xM). (2.97)
</p>
<p>Setting
</p>
<p>x = xi i =M + 1 &middot; &middot; &middot;n (2.98)
</p>
<p>we have
</p>
<p>f (xi)Q(xi)&minus; P(xi)= (fQ)[x0 &middot; &middot; &middot;xM , xi](xi &minus; x0) &middot; &middot; &middot; (x &minus; xM) (2.99)
</p>
<p>which is zero if P(xi)/Q(xi)= fi for i = 0 &middot; &middot; &middot;n. But then
</p>
<p>(fQ)[x0 &middot; &middot; &middot;xM , xi] = 0 i =M + 1 &middot; &middot; &middot;n. (2.100)
</p>
<p>The polynomial Q(x) can be written in Newtonian form (2.30)
</p>
<p>Q(x)=
N&sum;
</p>
<p>i=0
νi
</p>
<p>i&minus;1&prod;
</p>
<p>j=0
(x &minus; xj )= ν0 + ν1(x &minus; x0)+ &middot; &middot; &middot; + νN (x &minus; x0) &middot; &middot; &middot; (x &minus; xN&minus;1).
</p>
<p>(2.101)
With the abbreviation
</p>
<p>gj (x)= x &minus; xj j = 0 &middot; &middot; &middot;N (2.102)
</p>
<p>we find
</p>
<p>(fgj )[x0 &middot; &middot; &middot;xM , xi] =
&sum;
</p>
<p>k=0&middot;&middot;&middot;M,i
</p>
<p>f (xk)g(xk)&prod;
r �=k(xk &minus; xr)
</p>
<p>=
&sum;
</p>
<p>k=0&middot;&middot;&middot;M,i,k �=j
</p>
<p>f (xk)&prod;
r �=k,r �=j (xk &minus; xr)
</p>
<p>= f [x0 &middot; &middot; &middot;xj&minus;1, xj+1 &middot; &middot; &middot;xM , xi] (2.103)
</p>
<p>3The opposite case can be treated by considering the reciprocal function values 1/f (xi).</p>
<p/>
</div>
<div class="page"><p/>
<p>2.4 Rational Interpolation 31
</p>
<p>Fig. 2.6 (Interpolation of a
step function) A step function
with uniform x-values
(circles) is interpolated by a
polynomial (full curve), a
cubic spline (dashed curve)
and with the rational
Floater-Hormann d = 1
function ((2.105), dash-dotted
curve). The rational function
behaves similar to the spline
function but provides in
addition an analytical
function with continuous
derivatives
</p>
<p>which we apply repeatedly to (2.100) to get the system of n&minus;M =N equations for
N + 1 unknowns
</p>
<p>N&sum;
</p>
<p>j=0
νjf [xj , xj+1 &middot; &middot; &middot;xM , xi] = 0 i =M + 1 &middot; &middot; &middot;n (2.104)
</p>
<p>from which the coefficients νj can be found by Gaussian elimination up to a scaling
factor. The Newtonian form of Q(x) can then be converted to the barycentric form
as described in [271].
</p>
<p>2.4.2.2 Rational Interpolation Without Poles
</p>
<p>Polynomial interpolation of larger data sets can be ill behaved, especially for the
case of equidistant x-values. Rational interpolation without poles can be a much
better choice here (Fig. 2.6).
</p>
<p>Berrut [23] suggested to choose the following weights
</p>
<p>uk = (&minus;1)k.
</p>
<p>With this choice Q(x) has no real roots. Floater and Hormann [90] used the different
choice
</p>
<p>uk = (&minus;1)k&minus;1
(
</p>
<p>1
</p>
<p>xk+1 &minus; xk
+ 1
</p>
<p>xk &minus; xk&minus;1
</p>
<p>)
k = 1 &middot; &middot; &middot;n&minus; 1
</p>
<p>(2.105)
</p>
<p>u0 = &minus;
1
</p>
<p>x1 &minus; x0
un = (&minus;1)n&minus;1
</p>
<p>1
</p>
<p>xn &minus; xn&minus;1
</p>
<p>which becomes very similar for equidistant x-values.</p>
<p/>
</div>
<div class="page"><p/>
<p>32 2 Interpolation
</p>
<p>Table 2.1 Floater-Hormann
weights for uniform data |uk | d
</p>
<p>1,1,1, . . . ,1,1,1 0
</p>
<p>1,2,2,2, . . . ,2,2,2,1 1
</p>
<p>1,3,4,4,4, . . . ,4,4,4,3,1 2
</p>
<p>1,4,7,8,8,8, . . . ,8,8,8,7,4,1 3
</p>
<p>1,5,11,15,16,16,16, . . . ,16,16,16,15,11,5,1 4
</p>
<p>Floater and Hormann generalized this expression and found a class of rational
interpolants without poles given by the weights
</p>
<p>uk = (&minus;1)k&minus;d
min(k,n&minus;d)&sum;
</p>
<p>i=max(k&minus;d,0)
</p>
<p>i+d&prod;
</p>
<p>j=i,j �=k
</p>
<p>1
</p>
<p>|xk &minus; xj |
(2.106)
</p>
<p>where 0 &le; d &le; n and the approximation order increases with d . In the uniform case
this simplifies to (Table 2.1)
</p>
<p>uk = (&minus;1)k&minus;d
max(k,n&minus;d)&sum;
</p>
<p>i=min(k&minus;d,0)
</p>
<p>(
d
</p>
<p>k&minus; i
</p>
<p>)
. (2.107)
</p>
<p>2.5 Multivariate Interpolation
</p>
<p>The simplest 2-dimensional interpolation method is bilinear interpolation.4 It uses
linear interpolation for both coordinates within the rectangle xi &le; x &le; xi+1, yi &le;
yi &le; yi+1:
</p>
<p>p(xi + hx, yi + hy)
</p>
<p>= p(xi + hx, yi)+ hy
p(xi + hx, yi+1)&minus; p(xi + hx, yi)
</p>
<p>yi+1 &minus; yi
</p>
<p>= f (xi, yi)+ hx
f (xi+1, yi)&minus; f (xi, yi)
</p>
<p>xi+1 &minus; xi
</p>
<p>+ hy
f (xi , yi+1)+ hx f (xi+1,yi+1)&minus;f (xi ,yi+1)xi+1&minus;xi &minus; f (xi , yi)&minus; hx
</p>
<p>f (xi+1,yi )&minus;f (xi ,yi )
xi+1&minus;xi
</p>
<p>yi+1 &minus; yi
(2.108)
</p>
<p>4Bilinear means linear interpolation in two dimensions. Accordingly linear interpolation in three
dimensions is called trilinear.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.6 Problems 33
</p>
<p>Fig. 2.7 Bispline
interpolation
</p>
<p>which can be written as a two dimensional polynomial
</p>
<p>p(xi + hx, yi + hy)= a00 + a10hx + a01hy + a11hxhy (2.109)
</p>
<p>with
</p>
<p>a00 = f (xi, yi)
</p>
<p>a10 =
f (xi+1, yi)&minus; f (xi, yi)
</p>
<p>xi+1 &minus; xi
(2.110)
</p>
<p>a01 =
f (xi, yi+1)&minus; f (xi, yi)
</p>
<p>yi+1 &minus; yi
</p>
<p>a11 =
f (xi+1, yi+1)&minus; f (xi, yi+1)&minus; f (xi+1, yi)+ f (xi, yi)
</p>
<p>(xi+1 &minus; xi)(yi+1 &minus; yi)
.
</p>
<p>Application of higher order polynomials is straightforward. For image processing
purposes bicubic interpolation is often used.
</p>
<p>If high quality is needed more sophisticated interpolation methods can be ap-
plied. Consider for instance two-dimensional spline interpolation on a rectangular
mesh of data to create a new data set with finer resolution5
</p>
<p>fi,j = f (ihx, jhy) with 0 &le; i &lt; Nx 0 &le; j &lt; Ny . (2.111)
</p>
<p>First perform spline interpolation in x-direction for each data row j to calculate new
data sets
</p>
<p>fi&prime;,j = s(xi&prime; , fij ,0 &le; i &lt; Nx) 0 &le; j &le;Ny 0 &le; i&prime; &lt;N &prime;x (2.112)
</p>
<p>and then interpolate in y direction to obtain the final high resolution data (Fig. 2.7)
</p>
<p>fi&prime;,j &prime; = s(yj &prime; , fi&prime;j ,0 &le; j &lt; Ny) 0 &le; i&prime; &lt;N &prime;x 0 &le; j &prime; &lt;N &prime;y . (2.113)
</p>
<p>2.6 Problems
</p>
<p>Problem 2.1 (Polynomial interpolation) This computer experiment interpolates a
given set of n data points by
</p>
<p>5A typical task of image processing.</p>
<p/>
</div>
<div class="page"><p/>
<p>34 2 Interpolation
</p>
<p>Table 2.2 Zener diode
voltage/current data Voltage &minus;1.5 &minus;1.0 &minus;0.5 0.0
</p>
<p>Current &minus;3.375 &minus;1.0 &minus;0.125 0.0
</p>
<p>Table 2.3 Additional
voltage/current data Voltage 1.0 2.0 3.0 4.0 4.1 4.2 4.5
</p>
<p>Current 0.0 0.0 0.0 0.0 1.0 3.0 10.0
</p>
<p>&bull; a polynomial
</p>
<p>p(x)=
n&sum;
</p>
<p>i=0
fi
</p>
<p>n&prod;
</p>
<p>k=0,k �=i
</p>
<p>x &minus; xk
xi &minus; xk
</p>
<p>, (2.114)
</p>
<p>&bull; a linear spline which connects successive points by straight lines
</p>
<p>si(x)= ai + bi(x &minus; xi) for xi &le; x &le; xi+1 (2.115)
</p>
<p>&bull; a cubic spline with natural boundary conditions
</p>
<p>s(x)= pi(x)= αi + βi(x &minus; xi)+ γi(x &minus; xi)2 + δi(x &minus; xi)3 xi &le; x &le; xi+1
(2.116)
</p>
<p>s&prime;&prime;(xn)= s&prime;&prime;(x0)= 0 (2.117)
</p>
<p>&bull; a rational function without poles
</p>
<p>R(x)=
&sum;n
</p>
<p>i=0 fi
ui
</p>
<p>x&minus;xi&sum;n
i=0
</p>
<p>ui
x&minus;xi
</p>
<p>(2.118)
</p>
<p>with weights according to Berrut
</p>
<p>uk = (&minus;1)k (2.119)
</p>
<p>or Floater-Hormann
</p>
<p>uk = (&minus;1)k&minus;1
(
</p>
<p>1
</p>
<p>xk+1 &minus; xk
+ 1
</p>
<p>xk &minus; xk&minus;1
</p>
<p>)
k = 1 &middot; &middot; &middot;n&minus; 1 (2.120)
</p>
<p>u0 =&minus;
1
</p>
<p>x1 &minus; x0
un = (&minus;1)n&minus;1
</p>
<p>1
</p>
<p>xn &minus; xn&minus;1
. (2.121)
</p>
<p>&bull; Interpolate the data (Table 2.2) in the range &minus;1.5 &lt; x &lt; 0.
&bull; Now add some more sample points (Table 2.3) for &minus;1.5 &lt; x &lt; 4.5.
&bull; Interpolate the function f (x) = sin(x) at the points x = 0, π2 ,π,
</p>
<p>3π
2 ,2π . Take
</p>
<p>more sample points and check if the quality of the fit is improved.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.6 Problems 35
</p>
<p>Table 2.4 Pulse and step
function data x &minus;3 &minus;2 &minus;1 0 1 2 3
</p>
<p>ypulse 0 0 0 1 0 0 0
</p>
<p>ystep 0 0 0 1 1 1 1
</p>
<p>Table 2.5 Data set for
two-dimensional
interpolation
</p>
<p>x 0 1 2 0 1 2 0 1 2
</p>
<p>y 0 0 0 1 1 1 2 2 2
</p>
<p>f 1 0 &minus;1 0 0 0 &minus;1 0 1
</p>
<p>&bull; Investigate the oscillatory behavior for a discontinuous pulse or step function as
given by the data (Table 2.4).
</p>
<p>Problem 2.2 (Two-dimensional interpolation) This computer experiment uses bi-
linear interpolation or bicubic spline interpolation to interpolate the data (Table 2.5)
on a finer grid �x =�y = 0.1.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 3
</p>
<p>Numerical Differentiation
</p>
<p>For more complex problems analytical derivatives are not always available and have
to be approximated by numerical methods. Numerical differentiation is also very
important for the discretization of differential equations (Sect. 11.2). The simplest
approximation uses a forward difference quotient (Fig. 3.1) and is not very accu-
rate. A symmetric difference quotient improves the quality. Even higher precision is
obtained with the extrapolation method. Approximations to higher order derivatives
can be obtained systematically with the help of polynomial interpolation.
</p>
<p>3.1 One-Sided Difference Quotient
</p>
<p>The simplest approximation of a derivative is the ordinary difference quotient which
can be taken forward
</p>
<p>df
</p>
<p>dx
(x)&asymp; �f
</p>
<p>�x
= f (x + h)&minus; f (x)
</p>
<p>h
(3.1)
</p>
<p>or backward
df
</p>
<p>dx
(x)&asymp; �f
</p>
<p>�x
= f (x)&minus; f (x &minus; h)
</p>
<p>h
. (3.2)
</p>
<p>Its truncation error can be estimated from the Taylor series expansion
</p>
<p>f (x + h)&minus; f (x)
h
</p>
<p>=
f (x)+ hf &prime;(x)+ h22 f &prime;&prime;(x)+ &middot; &middot; &middot; &minus; f (x)
</p>
<p>h
</p>
<p>= f &prime;(x)+ h
2
f &prime;&prime;(x)+ &middot; &middot; &middot; . (3.3)
</p>
<p>The error order is O(h). The step width should not be too small to avoid rounding
errors. Error analysis gives
</p>
<p>�̃f = f l&minus;
(
f (x + h)(1 + ε1), f (x)(1 + ε2)
</p>
<p>)
</p>
<p>=
(
�f + f (x + h)ε1 &minus; f (x)ε2
</p>
<p>)
(1 + ε3)
</p>
<p>=�f +�f ε3 + f (x + h)ε1 &minus; f (x)ε2 + &middot; &middot; &middot; (3.4)
</p>
<p>P.O.J. Scherer, Computational Physics, Graduate Texts in Physics,
DOI 10.1007/978-3-319-00401-3_3,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>37</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-00401-3_3">http://dx.doi.org/10.1007/978-3-319-00401-3_3</a></div>
</div>
<div class="page"><p/>
<p>38 3 Numerical Differentiation
</p>
<p>Fig. 3.1 (Numerical
differentiation) Numerical
differentiation approximates
the differential quotient by a
difference quotient dfdx &asymp;
</p>
<p>�f
�x
</p>
<p>.
However, approximation by a
simple forward difference
df
dx (x0)&asymp;
</p>
<p>f (x0+h)&minus;f (x0)
h
</p>
<p>, is
not very accurate
</p>
<p>f l&divide;
(
�̃f ,h(1 + ε4)
</p>
<p>)
= �f +�f ε3 + f (x + h)ε1 &minus; f (x)ε2
</p>
<p>h(1 + ε4)
(1 + ε5)
</p>
<p>= �f
h
</p>
<p>(1 + ε5 &minus; ε4 + ε3)+
f (x + h)
</p>
<p>h
ε1
</p>
<p>&minus; f (x)
h
</p>
<p>ε2. (3.5)
</p>
<p>The errors are uncorrelated and the relative error of the result can be estimated by
</p>
<p>| �̃f
�x
</p>
<p>&minus; �f
�x
</p>
<p>|
�f
�x
</p>
<p>&le; 3εM +
∣∣∣∣
f (x)
</p>
<p>�f
�x
</p>
<p>∣∣∣∣2
εM
</p>
<p>h
. (3.6)
</p>
<p>Numerical extinction produces large relative errors for small step width h. The op-
timal value of h gives comparable errors from rounding and truncation. It can be
found from
</p>
<p>h
</p>
<p>2
</p>
<p>∣∣f &prime;&prime;(x)
∣∣=
</p>
<p>∣∣f (x)
∣∣2εM
</p>
<p>h
. (3.7)
</p>
<p>Assuming that the magnitude of the function and the derivative are comparable, we
have the rule of thumb
</p>
<p>ho =
&radic;
εM &asymp; 10&minus;8
</p>
<p>(double precision). The corresponding relative error is of the same order.
</p>
<p>3.2 Central Difference Quotient
</p>
<p>Accuracy is much higher if a symmetric central difference quotient is used
(Fig. 3.2):
</p>
<p>�f
</p>
<p>�x
=
</p>
<p>f (x + h2 )&minus; f (x &minus;
h
2 )
</p>
<p>h
</p>
<p>=
f (x)+ h2f &prime;(x)+
</p>
<p>h2
</p>
<p>8 f
&prime;&prime;(x)+ &middot; &middot; &middot; &minus; (f (x)&minus; h2f &prime;(x)+
</p>
<p>h2
</p>
<p>8 f
&prime;&prime;(x)+ &middot; &middot; &middot;)
</p>
<p>h
</p>
<p>= f &prime;(x)+ h
2
</p>
<p>24
f &prime;&prime;&prime;(x)+ &middot; &middot; &middot; . (3.8)</p>
<p/>
</div>
<div class="page"><p/>
<p>3.3 Extrapolation Methods 39
</p>
<p>Fig. 3.2 (Difference
quotient) The central
difference quotient (right
side) approximates the
derivative (dotted) much
more accurately than the
one-sided difference quotient
(left side)
</p>
<p>The error order is O(h2). The optimal step width is estimated from
</p>
<p>h2
</p>
<p>24
</p>
<p>∣∣f &prime;&prime;&prime;(x)
∣∣=
</p>
<p>∣∣f (x)
∣∣2εM
</p>
<p>h
(3.9)
</p>
<p>again with the assumption that function and derivatives are of similar magnitude as
</p>
<p>h0 = 3
&radic;
</p>
<p>48εM &asymp; 10&minus;5. (3.10)
</p>
<p>The relative error has to be expected in the order of
h20
24 &asymp; 10&minus;11.
</p>
<p>3.3 Extrapolation Methods
</p>
<p>The Taylor series of the symmetric difference quotient contains only even powers
of h:
</p>
<p>D(h)= f (x + h)&minus; f (x &minus; h)
2h
</p>
<p>= f &prime;(x)+ h
2
</p>
<p>3! f
&prime;&prime;&prime;(x)+ h
</p>
<p>4
</p>
<p>5! f
(5)(x)+ &middot; &middot; &middot; . (3.11)
</p>
<p>The Extrapolation method [217] uses a series of step widths, e.g.
</p>
<p>hi+1 =
hi
</p>
<p>2
(3.12)
</p>
<p>and calculates an estimate of D(0) by polynomial interpolation (Fig. 3.3). Consider
D0 = D(h0) and D1 = D(h02 ). The polynomial of degree 1 (with respect to h2)
p(h)= a + bh2 can be found by the Lagrange method
</p>
<p>p(h)=D0
h2 &minus; h
</p>
<p>2
0
</p>
<p>4
</p>
<p>h20 &minus;
h20
4
</p>
<p>+D1
h2 &minus; h20
h20
4 &minus; h20
</p>
<p>. (3.13)
</p>
<p>Extrapolation for h= 0 gives
</p>
<p>p(0)=&minus;1
3
D0 +
</p>
<p>4
</p>
<p>3
D1. (3.14)
</p>
<p>Taylor series expansion shows
</p>
<p>p(0)=&minus;1
3
</p>
<p>(
f &prime;(x)+
</p>
<p>h20
</p>
<p>3! f
&prime;&prime;&prime;(x)+
</p>
<p>h40
</p>
<p>5! f
(5)(x)+ &middot; &middot; &middot;
</p>
<p>)</p>
<p/>
</div>
<div class="page"><p/>
<p>40 3 Numerical Differentiation
</p>
<p>Fig. 3.3 (Numerical
differentiation) The derivative
d
</p>
<p>dx sin(x) is calculated
numerically using algorithms
with increasing error order
(3.1, 3.8, 3.14, 3.18). For very
small step sizes the error
increases as h&minus;1 due to
rounding errors
</p>
<p>+ 4
3
</p>
<p>(
f &prime;(x)+
</p>
<p>h20
</p>
<p>4 &middot; 3!f
&prime;&prime;&prime;(x)+
</p>
<p>h40
</p>
<p>16 &middot; 5!f
(5)(x)+ &middot; &middot; &middot;
</p>
<p>)
(3.15)
</p>
<p>= f &prime;(x)&minus; 1
4
</p>
<p>h40
</p>
<p>5! f
(5)(x)+ &middot; &middot; &middot; (3.16)
</p>
<p>that the error order is O(h40). For 3 step widths h0 = 2h1 = 4h2 we obtain the poly-
nomial of second order (in h2)
</p>
<p>p(h)=D0
(h2 &minus; h
</p>
<p>2
0
</p>
<p>4 )(h
2 &minus; h
</p>
<p>2
0
</p>
<p>16 )
</p>
<p>(h20 &minus;
h20
4 )(h
</p>
<p>2
0 &minus;
</p>
<p>h20
16 )
</p>
<p>+D1
(h2 &minus; h20)(h2 &minus;
</p>
<p>h20
16 )
</p>
<p>(
h20
4 &minus;h20)(
</p>
<p>h20
4 &minus;
</p>
<p>h20
16 )
</p>
<p>+D2
(h2 &minus; h20)(h2 &minus;
</p>
<p>h20
4 )
</p>
<p>(
h20
16 &minus; h20)(
</p>
<p>h20
16 &minus;
</p>
<p>h20
4 )
</p>
<p>(3.17)
</p>
<p>and the improved expression
</p>
<p>p(0)=D0
1
64
</p>
<p>3
4 &middot;
</p>
<p>15
16
</p>
<p>+D1
1
16
</p>
<p>&minus;3
4 &middot;
</p>
<p>3
16
</p>
<p>+D2
1
4
</p>
<p>&minus;15
16 &middot;
</p>
<p>&minus;3
16
</p>
<p>= 1
45
</p>
<p>D0 &minus;
4
</p>
<p>9
D1 +
</p>
<p>64
</p>
<p>45
D2 = f &prime;(x)+O
</p>
<p>(
h60
)
. (3.18)
</p>
<p>Often used is the following series of step widths:
</p>
<p>h2i =
h20
</p>
<p>2i
. (3.19)
</p>
<p>The Neville method
</p>
<p>Pi&middot;&middot;&middot;k
(
h2
)
=
</p>
<p>(h2 &minus; h
2
0
</p>
<p>2i
)Pi+1&middot;&middot;&middot;k(h2)&minus; (h2 &minus;
</p>
<p>h20
2k
)Pi&middot;&middot;&middot;k&minus;1(h2)
</p>
<p>h20
2k
</p>
<p>&minus; h
2
0
</p>
<p>2i
</p>
<p>(3.20)</p>
<p/>
</div>
<div class="page"><p/>
<p>3.4 Higher Derivatives 41
</p>
<p>gives for h= 0
</p>
<p>Pi&middot;&middot;&middot;k =
Pi&middot;&middot;&middot;k&minus;1 &minus; 2k&minus;iPi+1&middot;&middot;&middot;k
</p>
<p>1 &minus; 2k&minus;i (3.21)
</p>
<p>which can be written as
</p>
<p>Pi&middot;&middot;&middot;k = Pi+1&middot;&middot;&middot;k +
Pi&middot;&middot;&middot;k&minus;1 &minus; Pi+1&middot;&middot;&middot;k
</p>
<p>1 &minus; 2k&minus;i (3.22)
</p>
<p>and can be calculated according to the following scheme:
</p>
<p>P0 =D
(
h2
)
</p>
<p>P01 P012 P0123
</p>
<p>P1 =D
(
h2
</p>
<p>2
</p>
<p>)
P12 P123
</p>
<p>P2 =D
(
h2
</p>
<p>4
</p>
<p>)
P23
</p>
<p>...
...
</p>
<p>...
. . .
</p>
<p>(3.23)
</p>
<p>Here the values of the polynomials are arranged in matrix form
</p>
<p>Pi&middot;&middot;&middot;k = Ti,k&minus;i = Ti,j (3.24)
with the recursion formula
</p>
<p>Ti,j = Ti+1,j&minus;1 +
Ti,j&minus;1 &minus; Ti+1,j
</p>
<p>1 &minus; 2j . (3.25)
</p>
<p>3.4 Higher Derivatives
</p>
<p>Difference quotients for higher derivatives can be obtained systematically using
polynomial interpolation. Consider equidistant points
</p>
<p>xn = x0 + nh= &middot; &middot; &middot; = x0 &minus; 2h,x0 &minus; h,x0, x0 + h,x0 + 2h, . . . . (3.26)
From the second order polynomial
</p>
<p>p(x)= y&minus;1
(x &minus; x0)(x &minus; x1)
</p>
<p>(x&minus;1 &minus; x0)(x&minus;1 &minus; x1)
+ y0
</p>
<p>(x &minus; x&minus;1)(x &minus; x1)
(x0 &minus; x&minus;1)(x0 &minus; x1)
</p>
<p>+ y1
(x &minus; x&minus;1)(x &minus; x0)
(x1 &minus; x&minus;1)(x1 &minus; x0)
</p>
<p>= y&minus;1
(x &minus; x0)(x &minus; x1)
</p>
<p>2h2
+ y0
</p>
<p>(x &minus; x&minus;1)(x &minus; x1)
&minus;h2
</p>
<p>+ y1
(x &minus; x&minus;1)(x &minus; x0)
</p>
<p>2h2
(3.27)
</p>
<p>we calculate the derivatives
</p>
<p>p&prime;(x)= y&minus;1
2x &minus; x0 &minus; x1
</p>
<p>2h2
+ y0
</p>
<p>2x &minus; x&minus;1 &minus; x1
&minus;h2 + y1
</p>
<p>2x &minus; x&minus;1 &minus; x0
2h2
</p>
<p>(3.28)</p>
<p/>
</div>
<div class="page"><p/>
<p>42 3 Numerical Differentiation
</p>
<p>p&prime;&prime;(x)= y&minus;1
h2
</p>
<p>&minus; 2y0
h2
</p>
<p>+ y1
h2
</p>
<p>(3.29)
</p>
<p>which are evaluated at x0:
</p>
<p>f &prime;(x0)&asymp; p&prime;(x0)=&minus;
1
</p>
<p>2h
y&minus;1 +
</p>
<p>1
</p>
<p>2h
y1 =
</p>
<p>f (x0 + h)&minus; f (x0 &minus; h)
2h
</p>
<p>(3.30)
</p>
<p>f &prime;&prime;(x0)&asymp; p&prime;&prime;(x0)=
f (x0 &minus; h)&minus; 2f (x0)+ f (x0 + h)
</p>
<p>h2
. (3.31)
</p>
<p>Higher order polynomials can be evaluated with an algebra program. For five sample
points
</p>
<p>x0 &minus; 2h,x0 &minus; h,x0, x0 + h,x0 + 2h
we find
</p>
<p>f &prime;(x0)&asymp;
f (x0 &minus; 2h)&minus; 8f (x0 &minus; h)+ 8f (x0 + h)&minus; f (x0 + 2h)
</p>
<p>12h
(3.32)
</p>
<p>f &prime;&prime;(x0)&asymp;
&minus;f (x0 &minus; 2h)+ 16f (x0 &minus; h)&minus; 30f (x0)+ 16f (x0 + h)&minus; f (x0 + 2h)
</p>
<p>12h2
(3.33)
</p>
<p>f &prime;&prime;&prime;(x0)&asymp;
&minus;f (x0 &minus; 2h)+ 2f (x0 &minus; h)&minus; 2f (x0 + h)+ f (x0 + 2h)
</p>
<p>2h3
(3.34)
</p>
<p>f (4)(x0)&asymp;
f (x0 &minus; 2h)&minus; 4f (x0 &minus; h)+ 6f (x0 + h)&minus; 4f (x0 + h)+ f (x0 + 2h)
</p>
<p>h4
.
</p>
<p>3.5 Partial Derivatives of Multivariate Functions
</p>
<p>Consider polynomials of more than one variable. In two dimensions we use the
Lagrange polynomials
</p>
<p>Li,j (x, y)=
&prod;
</p>
<p>k �=i
</p>
<p>(x &minus; xk)
(xi &minus; xk)
</p>
<p>&prod;
</p>
<p>j �=l
</p>
<p>(y &minus; yl)
(yj &minus; yl)
</p>
<p>. (3.35)
</p>
<p>The interpolating polynomial is
</p>
<p>p(x, y)=
&sum;
</p>
<p>i,j
</p>
<p>fi,jLi,j (x, y). (3.36)
</p>
<p>For the nine sample points
</p>
<p>(x&minus;1, y1) (x0, y1) (x1, y1)
</p>
<p>(x&minus;1, y0) (x0, y0) (x1, y0)
</p>
<p>(x&minus;1, y&minus;1) (x0, y&minus;1) (x1, y&minus;1)
</p>
<p>(3.37)
</p>
<p>we obtain the polynomial
</p>
<p>p(x, y)= f&minus;1,&minus;1
(x &minus; x0)(x &minus; x1)(y &minus; y0)(y &minus; y1)
</p>
<p>(x&minus;1 &minus; x0)(x&minus;1 &minus; x1)(y&minus;1 &minus; y0)(y&minus;1 &minus; y1)
+ &middot; &middot; &middot; (3.38)</p>
<p/>
</div>
<div class="page"><p/>
<p>3.6 Problems 43
</p>
<p>which gives an approximation to the gradient
</p>
<p>gradf (x0y0)&asymp; gradp(x0y0)=
(
</p>
<p>f (x0+h,y0)&minus;f (x0&minus;h,y0)
2h
</p>
<p>f (x0,y0+h)&minus;f (x0,y0&minus;h)
2h
</p>
<p>)
, (3.39)
</p>
<p>the Laplace operator
(
</p>
<p>&part;2
</p>
<p>&part;x2
+ &part;
</p>
<p>2
</p>
<p>&part;y2
</p>
<p>)
f (x0, y0)&asymp;
</p>
<p>(
&part;2
</p>
<p>&part;x2
+ &part;
</p>
<p>2
</p>
<p>&part;y2
</p>
<p>)
p(x0, y0)
</p>
<p>= 1
h2
</p>
<p>(
f (x0, y0 + h)+ f (x0, y0 &minus; h)
</p>
<p>+ f (x0, y0 + h)+ f (x0, y0 &minus; h)&minus; 4f (x0, y0)
)
</p>
<p>(3.40)
</p>
<p>and the mixed second derivative
</p>
<p>&part;2
</p>
<p>&part;x&part;y
f (x0, y0)&asymp;
</p>
<p>&part;2
</p>
<p>&part;x&part;y
p(x0, y0)
</p>
<p>= 1
4h2
</p>
<p>(
f (x0 + h,y0 + h)+ f (x0 &minus; h,y0 &minus; h)
</p>
<p>&minus; f (x0 &minus; h,y0 + h)&minus; f (x0 + h,y0 &minus; h)
)
. (3.41)
</p>
<p>3.6 Problems
</p>
<p>Problem 3.1 (Numerical differentiation) In this computer experiment we calculate
the derivative of f (x)= sin(x) numerically with
&bull; the single sided difference quotient
</p>
<p>df
</p>
<p>dx
&asymp; f (x + h)&minus; f (x)
</p>
<p>h
, (3.42)
</p>
<p>&bull; the symmetrical difference quotient
df
</p>
<p>dx
&asymp;Dhf (x)=
</p>
<p>f (x + h)&minus; f (x &minus; h)
2h
</p>
<p>, (3.43)
</p>
<p>&bull; higher order approximations which can be derived using the extrapolation method
</p>
<p>&minus;1
3
Dhf (x)+
</p>
<p>4
</p>
<p>3
Dh/2f (x) (3.44)
</p>
<p>1
</p>
<p>45
Dhf (x)&minus;
</p>
<p>4
</p>
<p>9
Dh/2f (x)+
</p>
<p>64
</p>
<p>45
Dh/4f (x). (3.45)
</p>
<p>The error of the numerical approximation is shown on a log-log plot as a function
of the step width h.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 4
</p>
<p>Numerical Integration
</p>
<p>Physical simulations often involve the calculation of definite integrals over compli-
cated functions, for instance the Coulomb interaction between two electrons. Inte-
gration is also the elementary step in solving equations of motion.
</p>
<p>An integral over a finite interval [a, b] can always be transformed into an integral
over [0,1] or [&minus;1,1]
</p>
<p>&int; b
</p>
<p>a
</p>
<p>f (x)dx =
&int; 1
</p>
<p>0
f
(
a + (b&minus; a)t
</p>
<p>)
(b&minus; a)dt
</p>
<p>=
&int; 1
</p>
<p>&minus;1
f
</p>
<p>(
a + b
</p>
<p>2
+ b&minus; a
</p>
<p>2
t
</p>
<p>)
b&minus; a
</p>
<p>2
dt. (4.1)
</p>
<p>An Integral over an infinite interval may have to be transformed into an integral
over a finite interval by substitution of the integration variable, for example
</p>
<p>&int; &infin;
</p>
<p>0
f (x)dx =
</p>
<p>&int; 1
</p>
<p>0
f
</p>
<p>(
t
</p>
<p>1 &minus; t
</p>
<p>)
dt
</p>
<p>(1 &minus; t)2 (4.2)
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
f (x)dx =
</p>
<p>&int; 1
</p>
<p>&minus;1
f
</p>
<p>(
t
</p>
<p>1 &minus; t2
)
</p>
<p>t2 + 1
(t2 &minus; 1)2 dt. (4.3)
</p>
<p>In general a definite integral can be approximated numerically as the weighted
average over a finite number of function values
</p>
<p>&int; b
</p>
<p>a
</p>
<p>f (x)dx &asymp;
&sum;
</p>
<p>xi
</p>
<p>wif (xi). (4.4)
</p>
<p>Specific sets of quadrature points xi and quadrature weights wi are known as &ldquo;inte-
gral rules&rdquo;. Newton-Cotes rules like the trapezoidal rule, the midpoint rule or Simp-
son&rsquo;s rule, use equidistant points xi and are easy to apply. Accuracy can be improved
by dividing the integration range into sub-intervals and applying composite Newton-
Cotes rules. Extrapolation methods reduce the error almost to machine precision but
need many function evaluations. Equidistant sample points are convenient but not
the best choice. Clenshaw-Curtis expressions use non uniform sample points and
</p>
<p>P.O.J. Scherer, Computational Physics, Graduate Texts in Physics,
DOI 10.1007/978-3-319-00401-3_4,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>45</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-00401-3_4">http://dx.doi.org/10.1007/978-3-319-00401-3_4</a></div>
</div>
<div class="page"><p/>
<p>46 4 Numerical Integration
</p>
<p>a rapidly converging Chebyshev expansion. Gaussian integration fully optimizes
the sample points with the help of orthogonal polynomials.
</p>
<p>4.1 Equidistant Sample Points
</p>
<p>For equidistant points
</p>
<p>xi = a + ih i = 0 &middot; &middot; &middot;N h=
b&minus; a
N
</p>
<p>(4.5)
</p>
<p>the interpolating polynomial of order N with p(xi) = f (xi) is given by the La-
grange method
</p>
<p>p(x)=
N&sum;
</p>
<p>i=0
fi
</p>
<p>N&prod;
</p>
<p>k=0,k �=i
</p>
<p>x &minus; xk
xi &minus; xk
</p>
<p>. (4.6)
</p>
<p>Integration of the polynomial gives
&int; b
</p>
<p>a
</p>
<p>p(x)dx =
N&sum;
</p>
<p>i=0
fi
</p>
<p>&int; b
</p>
<p>a
</p>
<p>N&prod;
</p>
<p>k=0,k �=i
</p>
<p>x &minus; xk
xi &minus; xk
</p>
<p>dx. (4.7)
</p>
<p>After substituting
</p>
<p>x = a + hs
x &minus; xk = h(s &minus; k) (4.8)
xi &minus; xk = (i &minus; k)h
</p>
<p>we have
&int; b
</p>
<p>a
</p>
<p>N&prod;
</p>
<p>k=0,k �=i
</p>
<p>x &minus; xk
xi &minus; xk
</p>
<p>dx =
&int; N
</p>
<p>0
</p>
<p>N&prod;
</p>
<p>k=0,k �=i
</p>
<p>s &minus; k
i &minus; k hds = hαi (4.9)
</p>
<p>and hence
&int; b
</p>
<p>a
</p>
<p>p(x)dx = (b&minus; a)
N&sum;
</p>
<p>i=0
fiαi . (4.10)
</p>
<p>The weight factors are given by
</p>
<p>wi = (b&minus; a)αi =Nhαi . (4.11)
</p>
<p>4.1.1 Closed Newton-Cotes Formulae
</p>
<p>For N = 1 the polynomial is
</p>
<p>p(x)= f0
x &minus; x1
x0 &minus; x1
</p>
<p>+ f1
x &minus; x0
x1 &minus; x0
</p>
<p>(4.12)</p>
<p/>
</div>
<div class="page"><p/>
<p>4.1 Equidistant Sample Points 47
</p>
<p>Fig. 4.1 (Trapezoidal rule and midpoint rule) The trapezoidal rule (left) approximates the integral
by the average of the function values at the boundaries. The midpoint rule (right) evaluates the
function in the center of the interval and has the same error order
</p>
<p>and the integral is
</p>
<p>&int; b
</p>
<p>a
</p>
<p>p(x)dx = f0
&int; 1
</p>
<p>0
</p>
<p>s &minus; 1
0 &minus; 1hds + f1
</p>
<p>&int; 1
</p>
<p>0
</p>
<p>s &minus; 0
1 &minus; 0hds
</p>
<p>=&minus;f0h
(
(1 &minus; 1)2
</p>
<p>2
&minus; (0 &minus; 1)
</p>
<p>2
</p>
<p>2
</p>
<p>)
+ f1h
</p>
<p>(
12
</p>
<p>2
&minus; 0
</p>
<p>2
</p>
<p>2
</p>
<p>)
</p>
<p>= hf0 + f1
2
</p>
<p>(4.13)
</p>
<p>which is known as the trapezoidal rule (Fig. 4.1). N = 2 gives Simpson&rsquo;s rule
</p>
<p>2h
f0 + 4f1 + f2
</p>
<p>6
. (4.14)
</p>
<p>Larger N give further integration rules
</p>
<p>3h
f0 + 3f1 + 3f2 + f3
</p>
<p>8
3/8-rule
</p>
<p>4h
7f0 + 32f1 + 12f2 + 32f3 + 7f4
</p>
<p>90
Milne-rule
</p>
<p>5h
19f0 + 75f1 + 50f2 + 50f3 + 75f4 + 19f5
</p>
<p>288
</p>
<p>6h
41f0 + 216f1 + 27f2 + 272f3 + 27f4 + 216f5 + 41f6
</p>
<p>840
Weddle-rule.
</p>
<p>(4.15)
</p>
<p>For even larger N negative weight factors appear and the formulas are not numeri-
cally stable.</p>
<p/>
</div>
<div class="page"><p/>
<p>48 4 Numerical Integration
</p>
<p>4.1.2 Open Newton-Cotes Formulae
</p>
<p>Alternatively, the integral can be computed from only interior points
</p>
<p>xi = a + ih i = 1,2 &middot; &middot; &middot;N h=
b&minus; a
N + 1 . (4.16)
</p>
<p>The simplest case is the midpoint rule (Fig. 4.1)
&int; b
</p>
<p>a
</p>
<p>f (x)dx &asymp; 2hf1 = (b&minus; a)f
(
a + b
</p>
<p>2
</p>
<p>)
. (4.17)
</p>
<p>The next two are
3h
</p>
<p>2
(f1 + f2) (4.18)
</p>
<p>4h
</p>
<p>3
(2f1 &minus; f2 + 2f3). (4.19)
</p>
<p>4.1.3 Composite Newton-Cotes Rules
</p>
<p>Newton-Cotes formulae are only accurate, if the step width is small. Usually the
integration range is divided into small sub-intervals
</p>
<p>[xi, xi+1] xi = a + ih i = 0 &middot; &middot; &middot;N (4.20)
for which a simple quadrature formula can be used. Application of the trapezoidal
rule for each interval
</p>
<p>Ii =
h
</p>
<p>2
</p>
<p>(
f (xi)+ f (xi+1)
</p>
<p>)
(4.21)
</p>
<p>gives the composite trapezoidal rule
</p>
<p>T = h
(
f (a)
</p>
<p>2
+ f (a + h)+ &middot; &middot; &middot; + f (b&minus; h)+ f (b)
</p>
<p>2
</p>
<p>)
(4.22)
</p>
<p>with error order O(h2). Repeated application of Simpson&rsquo;s rule for [a, a+2h], [a+
2h,a + 4h], &middot; &middot; &middot; gives the composite Simpson&rsquo;s rule
</p>
<p>S = h
3
</p>
<p>(
f (a)+ 4f (a + h)+ 2f (a + 2h)+ 4f (a + 3h)+ &middot; &middot; &middot;
</p>
<p>+ 2f (b&minus; 2h)+ 4f (b&minus; h)+ f (b)
)
</p>
<p>(4.23)
</p>
<p>with error order O(h4).1
</p>
<p>Repeated application of the midpoint rule gives the composite midpoint rule
</p>
<p>M = 2h
(
f (a + h)+ f (a + 3h)+ &middot; &middot; &middot; + f (b&minus; h)
</p>
<p>)
(4.24)
</p>
<p>with error order O(h2).
</p>
<p>1The number of sample points must be even.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.1 Equidistant Sample Points 49
</p>
<p>4.1.4 Extrapolation Method (Romberg Integration)
</p>
<p>For the trapezoidal rule the Euler-McLaurin expansion exists which for a 2m times
differentiable function has the form&int; xN
</p>
<p>x0
</p>
<p>f (x)dx &minus; T = α2h2 + α4h4 + &middot; &middot; &middot; + α2m&minus;2h2m&minus;2 +O
(
h2m
</p>
<p>)
. (4.25)
</p>
<p>Therefore extrapolation methods are applicable. From the composite trapezoidal
rule for h and h/2 an approximation of error order O(h4) results:
</p>
<p>&int; xN
x0
</p>
<p>f (x)dx &minus; T (h)= α2h2 + α4h4+ &middot; &middot; &middot; (4.26)
&int; xN
x0
</p>
<p>f (x)dx &minus; T (h/2)= α2
h2
</p>
<p>4
+ α4
</p>
<p>h4
</p>
<p>16
+ &middot; &middot; &middot; (4.27)
</p>
<p>&int; xN
x0
</p>
<p>f (x)dx &minus; 4T (h/2)&minus; T (h)
3
</p>
<p>=&minus;α4
h4
</p>
<p>4
+ &middot; &middot; &middot; . (4.28)
</p>
<p>More generally, for the series of step widths
</p>
<p>hk =
h0
</p>
<p>2k
(4.29)
</p>
<p>the Neville method gives the recursion for the interpolating polynomial
</p>
<p>Pi&middot;&middot;&middot;k
(
h2
)
=
</p>
<p>(h2 &minus; h
2
0
</p>
<p>22i
)Pi+1&middot;&middot;&middot;k(h2)&minus; (h2 &minus;
</p>
<p>h20
22k
</p>
<p>)Pi&middot;&middot;&middot;k&minus;1(h2)
</p>
<p>h20
22k
</p>
<p>&minus; h
2
0
</p>
<p>22i
</p>
<p>(4.30)
</p>
<p>which for h= 0 becomes the higher order approximation to the integral (Fig. 4.2)
</p>
<p>Pi&middot;&middot;&middot;k =
2&minus;2kPi&middot;&middot;&middot;k&minus;1 &minus; 2&minus;2iPi+1&middot;&middot;&middot;k
</p>
<p>2&minus;2k &minus; 2&minus;2i =
Pi&middot;&middot;&middot;k&minus;1 &minus; 22k&minus;2iPi+1&middot;&middot;&middot;k
</p>
<p>1 &minus; 22k&minus;2i
</p>
<p>= Pi+1&middot;&middot;&middot;k +
Pi&middot;&middot;&middot;k&minus;1 &minus; Pi+1&middot;&middot;&middot;k
</p>
<p>1 &minus; 22k&minus;2i . (4.31)
</p>
<p>The polynomial values can again be arranged in matrix form
</p>
<p>P0 P01 P012 &middot; &middot; &middot;
P1 P12
P2
...
</p>
<p>(4.32)
</p>
<p>with
</p>
<p>Ti,j = Pi&middot;&middot;&middot;i+j (4.33)
and the recursion formula
</p>
<p>Ti,0 = Pi = Ts
(
h0
</p>
<p>2i
</p>
<p>)
(4.34)
</p>
<p>Ti,j = Ti+1,j&minus;1 +
Ti,j&minus;1 &minus; Ti+1,j&minus;1
</p>
<p>1 &minus; 22j . (4.35)</p>
<p/>
</div>
<div class="page"><p/>
<p>50 4 Numerical Integration
</p>
<p>Fig. 4.2 (Romberg integration) The integral
&int; π2
</p>
<p>0 sin(x
2)dx is calculated numerically. Circles
</p>
<p>show the absolute error of the composite trapezoidal rule (4.22) for the step size sequence
hi+1 = hi/2. Diamonds show the absolute error of the extrapolated value (4.31). The error or-
der of the trapezoidal rule is O(h2) whereas the error order of the Romberg method increases by
factors of h2. For very small step sizes the rounding errors dominate which increase as h&minus;1
</p>
<p>4.2 Optimized Sample Points
</p>
<p>The Newton-Cotes method integrates polynomials of order up to N &minus; 1 exactly,
using N equidistant sample points. Unfortunately the polynomial approximation
converges slowly, at least for not so well behaved integrands. The accuracy of the
integration can be improved by optimizing the sample point positions. Gaussian
quadrature determines the N positions and N weights such, that a polynomial of or-
der 2N&minus;1 is integrated exactly. The Clenshaw-Curtis and the related Fejer methods
use the roots or the extrema of the Chebyshev polynomials as nodes and determine
the weights to integrate polynomials of order N . However, since the approxima-
tion by Chebyshev polynomials usually converges very fast, the accuracy is in many
cases comparable to the Gaussian method [185, 253]. In the following we restrict
the integration interval to [&minus;1,1]. The general case [a, b] is then given by a simple
change of variables.
</p>
<p>4.2.1 Clenshaw-Curtis Expressions
</p>
<p>Clenshaw and Curtis [60] make the variable substitution
</p>
<p>x = cos θ dx =&minus; sin θdθ (4.36)
for the integral
</p>
<p>&int; 1
</p>
<p>&minus;1
f (x)dx =
</p>
<p>&int; π
</p>
<p>0
f (cos t) sin tdt (4.37)</p>
<p/>
</div>
<div class="page"><p/>
<p>4.2 Optimized Sample Points 51
</p>
<p>and approximate the function by the trigonometric polynomial ((7.19) with N =
2M , T = 2π )
</p>
<p>f (cos t)= 1
2M
</p>
<p>c0 +
1
</p>
<p>M
</p>
<p>M&minus;1&sum;
</p>
<p>j=1
cj cos(j t)+
</p>
<p>1
</p>
<p>2M
cM cos(Mt) (4.38)
</p>
<p>which interpolates (Sect. 7.2.1) f (cos t) at the sample points
</p>
<p>tn = n�t = n
π
</p>
<p>M
with n= 0,1 &middot; &middot; &middot;M (4.39)
</p>
<p>xn = cos tn = cos
(
n
π
</p>
<p>M
</p>
<p>)
(4.40)
</p>
<p>and where the Fourier coefficients are given by (7.17)
</p>
<p>cj = f0 + 2
M&minus;1&sum;
</p>
<p>n=1
f
(
cos(tn)
</p>
<p>)
cos
</p>
<p>(
π
</p>
<p>M
jn
</p>
<p>)
+ fM cos(jπ). (4.41)
</p>
<p>The function cos(j t) is related to the Chebyshev polynomials of the first kind
which for &minus;1 &le; x &le; 1 are given by the trigonometric definition
</p>
<p>Tj (x)= cos
(
j arccos(x)
</p>
<p>)
(4.42)
</p>
<p>and can be calculated recursively
</p>
<p>T0(x)= 1 (4.43)
</p>
<p>T1(x)= x (4.44)
</p>
<p>Tj+1(x)= 2xTj (x)&minus; Tj&minus;1(x). (4.45)
</p>
<p>Substituting x = cos t we find
</p>
<p>Tj (cos t)= cos(j t). (4.46)
</p>
<p>Hence the Fourier series (4.38) corresponds to a Chebyshev approximation
</p>
<p>f (x)=
M&sum;
</p>
<p>j=0
ajTj (x)=
</p>
<p>c0
</p>
<p>2M
T0(x)+
</p>
<p>M&minus;1&sum;
</p>
<p>j=1
</p>
<p>cj
</p>
<p>M
Tj (x)+
</p>
<p>cM
</p>
<p>2M
TM(x) (4.47)
</p>
<p>and can be used to approximate the integral
</p>
<p>&int; 1
</p>
<p>&minus;1
f (x)dx &asymp;
</p>
<p>&int; π
</p>
<p>0
</p>
<p>{
1
</p>
<p>2M
c0 +
</p>
<p>1
</p>
<p>M
</p>
<p>M&minus;1&sum;
</p>
<p>j=1
cj cos(j t)+
</p>
<p>1
</p>
<p>2M
cM cos(Mt)
</p>
<p>}
sin θdθ
</p>
<p>(4.48)
</p>
<p>= 1
M
</p>
<p>c0 +
1
</p>
<p>M
</p>
<p>M&minus;1&sum;
</p>
<p>j=1
cj
</p>
<p>cos(jπ)+ 1
1 &minus; j2 +
</p>
<p>1
</p>
<p>2M
cM
</p>
<p>cos(Mπ)+ 1
1 &minus;M2 (4.49)
</p>
<p>where, in fact, only the even j contribute.</p>
<p/>
</div>
<div class="page"><p/>
<p>52 4 Numerical Integration
</p>
<p>Example (Clenshaw Curtis quadrature for M = 5) The function has to be evaluated
at the sample points xk = cos(π5 k) = (1,0.80902,0.30902,&minus;0.30902,&minus;0.80902,
&minus;1). The Fourier coefficients are given by
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎝
</p>
<p>c0
c1
c2
c3
c4
c5
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎠
</p>
<p>=
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎝
</p>
<p>1 2 2 2 2 1
1 1.618 0.618 &minus;0.618 &minus;1.618 &minus;1
1 0.618 &minus;1.618 &minus;1.618 0.618 1
1 &minus;0.618 &minus;1.618 1.618 0.618 &minus;1
1 &minus;1.618 0.618 0.618 &minus;1.618 1
1 &minus;2 2 &minus;2 2 &minus;1
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎠
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎝
</p>
<p>f0
f1
f2
f3
f4
f5
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎠
</p>
<p>(4.50)
</p>
<p>and the integral is approximately
</p>
<p>&int; 1
</p>
<p>&minus;1
f (x)dx &asymp;
</p>
<p>(
1
</p>
<p>5
0 &minus; 2
</p>
<p>15
0 &minus; 2
</p>
<p>75
0
</p>
<p>)
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎝
</p>
<p>c0
c1
c2
c3
c4
c5
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎠
</p>
<p>= 0.0400f0 + 0.3607f1 + 0.5993f2 + 0.5993f3
+ 0.3607f4 + 0.0400f5. (4.51)
</p>
<p>Clenshaw Curtis weights of very high order can be calculated efficiently [240, 267]
using the FFT algorithm (fast Fourier transformation, Sect. 7.3.2)
</p>
<p>4.2.2 Gaussian Integration
</p>
<p>Now we will optimize the positions of the N quadrature points xi to obtain the
maximum possible accuracy. We approximate the integral by a sum
</p>
<p>&int; b
</p>
<p>a
</p>
<p>f (x)dx &asymp;
N&sum;
</p>
<p>i=1
f (xi)wi (4.52)
</p>
<p>and determine the 2N parameters xi and wi such that a polynomial of order 2N &minus; 1
is integrated exactly. This can be achieved with the help of a set of polynomials
which are orthogonal with respect to the scalar product
</p>
<p>〈fg〉 =
&int; b
</p>
<p>a
</p>
<p>f (x)g(x)w(x)dx (4.53)
</p>
<p>where the weight function w(x) and the interval [a, b] determine a particular set of
orthogonal polynomials.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.2 Optimized Sample Points 53
</p>
<p>4.2.2.1 Gauss-Legendre Integration
</p>
<p>Again we restrict the integration interval to [&minus;1,1] in the following. For integrals
with one or two infinite boundaries see Sect. 4.2.2.2. The simplest choice for the
weight function is
</p>
<p>w(x)= 1. (4.54)
An orthogonal system of polynomials on the interval [&minus;1,1] can be found using the
Gram-Schmidt method:
</p>
<p>P0 = 1 (4.55)
</p>
<p>P1 = x &minus;
P0
</p>
<p>〈P0P0〉
</p>
<p>&int; 1
</p>
<p>&minus;1
xP0(x)dx = x (4.56)
</p>
<p>P2 = x2 &minus;
P1
</p>
<p>〈P1P1〉
</p>
<p>&int; 1
</p>
<p>&minus;1
x2P1(x)dx &minus;
</p>
<p>P0
</p>
<p>〈P0P0〉
</p>
<p>&int; 1
</p>
<p>&minus;1
x2P0(x)dx
</p>
<p>= x2 &minus; 1
3
</p>
<p>(4.57)
</p>
<p>Pn = xn &minus;
Pn&minus;1
</p>
<p>〈Pn&minus;1Pn&minus;1〉
</p>
<p>&int; 1
</p>
<p>&minus;1
xnPn&minus;1(x)dx
</p>
<p>&minus; Pn&minus;2〈Pn&minus;2Pn&minus;2〉
</p>
<p>&int; 1
</p>
<p>&minus;1
xnPn&minus;2(x)dx &minus; &middot; &middot; &middot; . (4.58)
</p>
<p>The Pn are known as Legendre-polynomials. Consider now a polynomial p(x) of
order 2N&minus;1. It can be interpolated at the N quadrature points xi using the Lagrange
method by a polynomial p̃(x) of order N &minus; 1:
</p>
<p>p̃(x)=
N&sum;
</p>
<p>j=1
Lj (x)p(xj ). (4.59)
</p>
<p>Then p(x) can be written as
</p>
<p>p(x)= p̃(x)+ (x &minus; x1)(x &minus; x2) &middot; &middot; &middot; (x &minus; xN )q(x). (4.60)
Obviously q(x) is a polynomial of order (2N &minus; 1)&minus;N =N &minus; 1. Now choose the
positions xi as the roots of the Legendre polynomial of order N
</p>
<p>(x &minus; x1)(x &minus; x2) &middot; &middot; &middot; (x &minus; xN )= PN (x). (4.61)
Then we have
</p>
<p>&int; 1
</p>
<p>&minus;1
(x &minus; x1)(x &minus; x2) &middot; &middot; &middot; (x &minus; xN )q(x)dx = 0 (4.62)
</p>
<p>since PN is orthogonal to the polynomial of lower order. But now
&int; 1
</p>
<p>&minus;1
p(x)dx =
</p>
<p>&int; 1
</p>
<p>&minus;1
p̃(x)dx =
</p>
<p>&int; 1
</p>
<p>&minus;1
</p>
<p>N&sum;
</p>
<p>j=1
p(xj )Lj (x)dx =
</p>
<p>N&sum;
</p>
<p>j=1
wjp(xj ) (4.63)</p>
<p/>
</div>
<div class="page"><p/>
<p>54 4 Numerical Integration
</p>
<p>with the weight factors
</p>
<p>wj =
&int; 1
</p>
<p>&minus;1
Lj (x)dx. (4.64)
</p>
<p>Example (Gauss-Legendre integration with 2 quadrature points) The second order
Legendre polynomial
</p>
<p>P2(x)= x2 &minus;
1
</p>
<p>3
(4.65)
</p>
<p>has two roots
</p>
<p>x1,2 =&plusmn;
&radic;
</p>
<p>1
</p>
<p>3
. (4.66)
</p>
<p>The Lagrange polynomials are
</p>
<p>L1 =
x &minus;
</p>
<p>&radic;
1
3
</p>
<p>&minus;
&radic;
</p>
<p>1
3 &minus;
</p>
<p>&radic;
1
3
</p>
<p>L2 =
x +
</p>
<p>&radic;
1
3&radic;
</p>
<p>1
3 +
</p>
<p>&radic;
1
3
</p>
<p>(4.67)
</p>
<p>and the weights
</p>
<p>w1 =
&int; 1
</p>
<p>&minus;1
L1dx =&minus;
</p>
<p>&radic;
3
</p>
<p>2
</p>
<p>(
x2
</p>
<p>2
&minus;
&radic;
</p>
<p>1
</p>
<p>3
x
</p>
<p>)1
</p>
<p>&minus;1
= 1 (4.68)
</p>
<p>w2 =
&int; 1
</p>
<p>&minus;1
L2dx =
</p>
<p>&radic;
3
</p>
<p>2
</p>
<p>(
x2
</p>
<p>2
+
&radic;
</p>
<p>1
</p>
<p>3
x
</p>
<p>)1
</p>
<p>&minus;1
= 1. (4.69)
</p>
<p>This gives the integral rule
&int; 1
</p>
<p>&minus;1
f (x)dx &asymp; f
</p>
<p>(
&minus;
&radic;
</p>
<p>1
</p>
<p>3
</p>
<p>)
+ f
</p>
<p>(&radic;
1
</p>
<p>3
</p>
<p>)
. (4.70)
</p>
<p>For a general integration interval we substitute
</p>
<p>x = a + b
2
</p>
<p>+ b&minus; a
2
</p>
<p>u (4.71)
</p>
<p>and find the approximation
&int; b
</p>
<p>a
</p>
<p>f (x)dx =
&int; 1
</p>
<p>&minus;1
f
</p>
<p>(
a + b
</p>
<p>2
+ b&minus; a
</p>
<p>2
u
</p>
<p>)
b&minus; a
</p>
<p>2
du
</p>
<p>&asymp; b&minus; a
2
</p>
<p>(
f
</p>
<p>(
a + b
</p>
<p>2
&minus; b&minus; a
</p>
<p>2
</p>
<p>&radic;
1
</p>
<p>3
</p>
<p>)
</p>
<p>+ f
(
a + b
</p>
<p>2
+ b&minus; a
</p>
<p>2
</p>
<p>&radic;
1
</p>
<p>3
</p>
<p>))
. (4.72)
</p>
<p>The next higher order Gaussian rule is given by
</p>
<p>n= 3: w1 =w3 = 5/9, w2 = 8/9, x3 =&minus;x1 = 0.77459 &middot; &middot; &middot; , x2 = 0.
(4.73)</p>
<p/>
</div>
<div class="page"><p/>
<p>4.2 Optimized Sample Points 55
</p>
<p>4.2.2.2 Other Types of Gaussian Integration
</p>
<p>Further integral rules can be obtained by using other sets of orthogonal polynomials,
for instance
</p>
<p>Chebyshev polynomials
</p>
<p>w(x)= 1&radic;
1 &minus; x2
</p>
<p>(4.74)
</p>
<p>&int; 1
</p>
<p>&minus;1
f (x)dx =
</p>
<p>&int; 1
</p>
<p>&minus;1
f (x)
</p>
<p>&radic;
1 &minus; x2w(x)dx (4.75)
</p>
<p>Tn+1(x)= 2xTn(x)&minus; Tn&minus;1(x) (4.76)
</p>
<p>xk = cos
(
</p>
<p>2k &minus; 1
2N
</p>
<p>π
</p>
<p>)
wk =
</p>
<p>π
</p>
<p>N
. (4.77)
</p>
<p>Hermite polynomials
</p>
<p>w(x)= e&minus;x2 (4.78)
&int; &infin;
</p>
<p>&minus;&infin;
f (x)dx =
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
f (x)ex
</p>
<p>2
w(x)dx (4.79)
</p>
<p>H0(x)= 1, H1(x)= 2x, Hn+1(x)= 2xHn(x)&minus; 2nHn&minus;1(x).
</p>
<p>Laguerre polynomials
</p>
<p>w(x)= e&minus;x (4.80)
&int; &infin;
</p>
<p>0
f (x)dx =
</p>
<p>&int; &infin;
</p>
<p>0
f (x)exw(x)dx (4.81)
</p>
<p>L0(x)= 1, L1(x)= 1 &minus; x,
</p>
<p>Ln+1(x)=
1
</p>
<p>n+ 1
(
(2n+ 1 &minus; x)Ln(x)&minus; nLn&minus;1(x)
</p>
<p>)
.
</p>
<p>(4.82)
</p>
<p>4.2.2.3 Connection with an Eigenvalue Problem
</p>
<p>The determination of quadrature points and weights can be formulated as an eigen-
value problem [109, 133]. Any set of orthogonal polynomials satisfies a three term
recurrence relation
</p>
<p>Pn+1(x)= (an+1x + bn+1)Pn(x)&minus; cn+1Pn&minus;1(x) (4.83)
</p>
<p>with an &gt; 0, cn &gt; 0 which can be written in matrix form [272]</p>
<p/>
</div>
<div class="page"><p/>
<p>56 4 Numerical Integration
</p>
<p>x
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎝
</p>
<p>P0(x)
</p>
<p>P1(x)
</p>
<p>PN&minus;1(x)
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎠
</p>
<p>=
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>&minus; b1
a1
</p>
<p>1
a1
</p>
<p>c2
a2
</p>
<p>&minus; b2
a2
</p>
<p>1
a2
</p>
<p>. . .
. . .
</p>
<p>. . .
</p>
<p>. . .
. . .
</p>
<p>. . .
cN&minus;1
aN&minus;1
</p>
<p>&minus; bN&minus;1
aN&minus;1
</p>
<p>1
aN&minus;1
</p>
<p>cN
aN
</p>
<p>&minus; bN
aN
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎝
</p>
<p>P0(x)
</p>
<p>P1(x)
</p>
<p>PN&minus;1(x)
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎠
</p>
<p>+
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>0
0
...
...
</p>
<p>0
1
aN
</p>
<p>PN (x)
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>(4.84)
</p>
<p>or shorter
</p>
<p>xP(x)= T P(x)+ 1
aN
</p>
<p>PN (x)eN&minus;1 (4.85)
</p>
<p>with a tridiagonal matrix T . Obviously PN (x)= 0 if and only if
xjP(xj )= T P(xj ), (4.86)
</p>
<p>hence the roots of PN (x) are given by the eigenvalues of T . The matrix T is sym-
metric if the polynomials are orthonormal, otherwise it can be transformed into a
symmetric tridiagonal matrix by an orthogonal transformation [272]. Finally the
quadrature weight corresponding to the eigenvalue xj can be calculated from the
first component of the corresponding eigenvector [109].
</p>
<p>4.3 Problems
</p>
<p>Problem 4.1 (Romberg integration) Use the trapezoidal rule
</p>
<p>T (h)= h
(
</p>
<p>1
</p>
<p>2
f (a)+ f (a + h)+ &middot; &middot; &middot; + f (b&minus; h)+ 1
</p>
<p>2
f (b)
</p>
<p>)
=
&int; b
</p>
<p>a
</p>
<p>f (x)dx + &middot; &middot; &middot;
(4.87)
</p>
<p>with the step sequence
</p>
<p>hi =
h0
</p>
<p>2i
(4.88)
</p>
<p>and calculate the elements of the triangular matrix
</p>
<p>T (i,0)= T (hi) (4.89)
</p>
<p>T (i, k)= T (i + 1, k &minus; 1)+ T (i, k &minus; 1)&minus; T (i + 1, k&minus; 1)
</p>
<p>1 &minus; h
2
i
</p>
<p>h2i+k
</p>
<p>(4.90)</p>
<p/>
</div>
<div class="page"><p/>
<p>4.3 Problems 57
</p>
<p>to obtain the approximations
</p>
<p>T01 = P01, T02 = P012, T03 = P0123, &middot; &middot; &middot; (4.91)
&bull; calculate
</p>
<p>&int; π2
</p>
<p>0
sin
</p>
<p>(
x2
)
dx = 0.6773089370468890331 &middot; &middot; &middot; (4.92)
</p>
<p>and compare the absolute error of the trapezoidal sums T (hi)= Ti,0 and the ex-
trapolated values T0,i .
</p>
<p>&bull; calculate
&int; 1
</p>
<p>ε
</p>
<p>dx&radic;
x
</p>
<p>(4.93)
</p>
<p>for ε = 10&minus;3. Compare with the composite midpoint rule
</p>
<p>T (h)= h
(
f
</p>
<p>(
a + h
</p>
<p>2
</p>
<p>)
+ f
</p>
<p>(
a + 3h
</p>
<p>2
</p>
<p>)
+ &middot; &middot; &middot; + f
</p>
<p>(
b&minus; 3h
</p>
<p>2
</p>
<p>)
+ f
</p>
<p>(
b&minus; h
</p>
<p>2
</p>
<p>))
.
</p>
<p>(4.94)</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 5
</p>
<p>Systems of Inhomogeneous Linear Equations
</p>
<p>Many problems in physics and especially computational physics involve systems of
linear equations
</p>
<p>a11x1 + &middot; &middot; &middot; + a1nxn = b1
...
</p>
<p>...
...
</p>
<p>an1x1 + &middot; &middot; &middot; + annxn = bn
</p>
<p>(5.1)
</p>
<p>or shortly in matrix form
</p>
<p>Ax= b (5.2)
which arise e.g. from linearization of a general nonlinear problem like (Sect. 20.2)
</p>
<p>0 =
</p>
<p>⎛
⎜⎝
F1(x1 &middot; &middot; &middot;xn)
</p>
<p>...
</p>
<p>Fn(x1 &middot; &middot; &middot;xn)
</p>
<p>⎞
⎟⎠=
</p>
<p>⎛
⎜⎝
F1(x
</p>
<p>(0)
1 &middot; &middot; &middot;x
</p>
<p>(0)
n )
</p>
<p>...
</p>
<p>Fn(x
(0)
1 &middot; &middot; &middot;x
</p>
<p>(0)
n )
</p>
<p>⎞
⎟⎠+
</p>
<p>⎛
⎜⎜⎝
</p>
<p>&part;F1
&part;x1
</p>
<p>&middot; &middot; &middot; &part;F1
&part;xn
</p>
<p>...
. . .
</p>
<p>...
&part;Fn
&part;x1
</p>
<p>&middot; &middot; &middot; &part;Fn
&part;xn
</p>
<p>⎞
⎟⎟⎠
</p>
<p>⎛
⎜⎝
x1 &minus; x(0)1
</p>
<p>...
</p>
<p>xn &minus; x(0)n
</p>
<p>⎞
⎟⎠+&middot; &middot; &middot;
</p>
<p>(5.3)
or from discretization of differential equations like
</p>
<p>0 = &part;f
&part;x
</p>
<p>&minus; g(x)&rarr;
</p>
<p>⎛
⎜⎜⎝
</p>
<p>...
f ((j+1)�x)&minus;f (j�x)
</p>
<p>�x
&minus; g(j�x)
</p>
<p>...
</p>
<p>⎞
⎟⎟⎠
</p>
<p>=
</p>
<p>⎛
⎜⎜⎜⎜⎝
</p>
<p>. . .
</p>
<p>&minus; 1
�x
</p>
<p>1
�x
</p>
<p>&minus; 1
�x
</p>
<p>1
�x
</p>
<p>. . .
</p>
<p>⎞
⎟⎟⎟⎟⎠
</p>
<p>⎛
⎜⎜⎜⎜⎝
</p>
<p>...
</p>
<p>fj
fj+1
...
</p>
<p>⎞
⎟⎟⎟⎟⎠
&minus;
</p>
<p>⎛
⎜⎜⎜⎜⎝
</p>
<p>...
</p>
<p>gj
gj+1
...
</p>
<p>⎞
⎟⎟⎟⎟⎠
. (5.4)
</p>
<p>If the dimension of the system is not too large standard methods like Gaussian
elimination or QR decomposition are sufficient. Systems with a tridiagonal matrix
</p>
<p>P.O.J. Scherer, Computational Physics, Graduate Texts in Physics,
DOI 10.1007/978-3-319-00401-3_5,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>59</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-00401-3_5">http://dx.doi.org/10.1007/978-3-319-00401-3_5</a></div>
</div>
<div class="page"><p/>
<p>60 5 Systems of Inhomogeneous Linear Equations
</p>
<p>are important for cubic spline interpolation and numerical second derivatives. They
can be solved very efficiently with a specialized Gaussian elimination method. Prac-
tical applications often involve very large dimensions and require iterative methods.
Convergence of Jacobi and Gauss-Seidel methods is slow and can be improved by
relaxation or over-relaxation. An alternative for large systems is the method of con-
jugate gradients.
</p>
<p>5.1 Gaussian Elimination Method
</p>
<p>A series of linear combinations of the equations transforms the matrix A into an
upper triangular matrix. Start with subtracting ai1/a11 times the first row from rows
2 &middot; &middot; &middot;n
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>aT1
</p>
<p>aT2
...
</p>
<p>aTn
</p>
<p>⎞
⎟⎟⎟⎠&rarr;
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>aT1
aT2 &minus; l21aT1
</p>
<p>...
</p>
<p>aTn &minus; ln1aT1
</p>
<p>⎞
⎟⎟⎟⎠ (5.5)
</p>
<p>which can be written as a multiplication
</p>
<p>A(1) = L1A (5.6)
</p>
<p>with the Frobenius matrix
</p>
<p>L1 =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>1
&minus;l21 1
&minus;l31 1
...
</p>
<p>. . .
</p>
<p>&minus;ln1 1
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠
</p>
<p>li1 =
ai1
</p>
<p>a11
. (5.7)
</p>
<p>The result has the form
</p>
<p>A(1) =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>a11 a12 &middot; &middot; &middot; a1n&minus;1 a1n
0 a(1)22 &middot; &middot; &middot; a
</p>
<p>(1)
2n&minus;1 a
</p>
<p>(1)
2n
</p>
<p>0 a(1)32 &middot; &middot; &middot; &middot; &middot; &middot; a
(1)
3n
</p>
<p>0
...
</p>
<p>...
</p>
<p>0 a(1)n2 &middot; &middot; &middot; &middot; &middot; &middot; a
(1)
nn
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎠
. (5.8)
</p>
<p>Now subtract ai2
a22
</p>
<p>times the second row from rows 3 &middot; &middot; &middot;n. This can be formulated as
</p>
<p>A(2) = L2A(1) = L2L1A (5.9)</p>
<p/>
</div>
<div class="page"><p/>
<p>5.1 Gaussian Elimination Method 61
</p>
<p>with
</p>
<p>L2 =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>1
0 1
0 &minus;l32 1
...
</p>
<p>...
. . .
</p>
<p>0 &minus;ln2 1
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠
</p>
<p>li2 =
a
(1)
i2
</p>
<p>a
(1)
22
</p>
<p>. (5.10)
</p>
<p>The result is
</p>
<p>A(2) =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>a
(2)
11 a
</p>
<p>(2)
12 a
</p>
<p>(2)
13 &middot; &middot; &middot; a
</p>
<p>(2)
1n
</p>
<p>0 a(2)22 a
(2)
23 &middot; &middot; &middot; a
</p>
<p>(2)
2n
</p>
<p>0 0 a(2)33 &middot; &middot; &middot; a
(2)
3n
</p>
<p>...
...
</p>
<p>...
...
</p>
<p>0 0 a(2)n3 &middot; &middot; &middot; a
(2)
nn
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎠
. (5.11)
</p>
<p>Continue until an upper triangular matrix results after n&minus; 1 steps:
</p>
<p>A(n&minus;1) = Ln&minus;1A(n&minus;2) (5.12)
</p>
<p>Ln&minus;1 =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>1
1
</p>
<p>. . .
</p>
<p>1
&minus;ln,n&minus;1 1
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠
</p>
<p>ln,n&minus;1 =
a
(n&minus;2)
n,n&minus;1
</p>
<p>a
(n&minus;2)
n&minus;1,n&minus;1
</p>
<p>(5.13)
</p>
<p>A(n&minus;1) = Ln&minus;1Ln&minus;2 &middot; &middot; &middot;L2L1A=U (5.14)
</p>
<p>U =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>u11 u12 u13 &middot; &middot; &middot; u1n
u22 u23 &middot; &middot; &middot; u2n
</p>
<p>u33 &middot; &middot; &middot; u3n
. . .
</p>
<p>...
</p>
<p>unn
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠
. (5.15)
</p>
<p>The transformed system of equations
</p>
<p>Ux= y y= Ln&minus;1Ln&minus;1 &middot; &middot; &middot;L2L1b (5.16)
</p>
<p>can be solved easily by backward substitution:
</p>
<p>xn =
1
</p>
<p>unn
yn (5.17)</p>
<p/>
</div>
<div class="page"><p/>
<p>62 5 Systems of Inhomogeneous Linear Equations
</p>
<p>xn&minus;1 =
yn&minus;1 &minus; xnun&minus;1,n
</p>
<p>un&minus;1,n&minus;1
(5.18)
</p>
<p>... (5.19)
</p>
<p>Alternatively the matrices Li can be inverted:
</p>
<p>L&minus;11 =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>1
l21 1
l31 1
...
</p>
<p>. . .
</p>
<p>ln1 1
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠
</p>
<p>&middot; &middot; &middot; L&minus;1n&minus;1 =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>1
1
</p>
<p>. . .
</p>
<p>1
ln,n&minus;1 1
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠
.
</p>
<p>(5.20)
This gives
</p>
<p>A= L&minus;11 L
&minus;1
2 &middot; &middot; &middot;L
</p>
<p>&minus;1
n&minus;1U. (5.21)
</p>
<p>The product of the inverted matrices is a lower triangular matrix:
</p>
<p>L&minus;11 L
&minus;1
2 =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>1
l21 1
l31 l32 1
...
</p>
<p>...
. . .
</p>
<p>ln1 ln2 1
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠
</p>
<p>... (5.22)
</p>
<p>L= L&minus;11 L
&minus;1
2 &middot; &middot; &middot;L
</p>
<p>&minus;1
n&minus;1 =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>1
l21 1
...
</p>
<p>...
. . .
</p>
<p>ln&minus;1,1 ln&minus;1,2 &middot; &middot; &middot; 1
ln1 ln2 &middot; &middot; &middot; ln,n&minus;1 1
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠
.
</p>
<p>Hence the matrix A becomes decomposed into a product of a lower and an upper
triangular matrix
</p>
<p>A= LU (5.23)
which can be used to solve the system of equations (5.2).
</p>
<p>Ax= LUx= b (5.24)
</p>
<p>in two steps:
</p>
<p>Ly= b (5.25)
which can be solved from the top
</p>
<p>y1 = b1 (5.26)</p>
<p/>
</div>
<div class="page"><p/>
<p>5.1 Gaussian Elimination Method 63
</p>
<p>y2 = b2 &minus; l21y1 (5.27)
... (5.28)
</p>
<p>and
</p>
<p>Ux= y (5.29)
which can be solved from the bottom
</p>
<p>xn =
1
</p>
<p>unn
yn (5.30)
</p>
<p>xn&minus;1 =
yn&minus;1 &minus; xnun&minus;1,n
</p>
<p>un&minus;1,n&minus;1
. (5.31)
</p>
<p>... (5.32)
</p>
<p>5.1.1 Pivoting
</p>
<p>To improve numerical stability and to avoid division by zero pivoting is used. Most
common is partial pivoting. In every step the order of the equations is changed in
order to maximize the pivoting element ak,k in the denominator. This gives LU
decomposition of the matrix PA where P is a permutation matrix. P is not needed
explicitly. Instead an index vector is used which stores the new order of the equations
</p>
<p>P
</p>
<p>⎛
⎜⎝
</p>
<p>1
...
</p>
<p>N
</p>
<p>⎞
⎟⎠=
</p>
<p>⎛
⎜⎝
</p>
<p>i1
...
</p>
<p>iN
</p>
<p>⎞
⎟⎠ . (5.33)
</p>
<p>Total pivoting exchanges rows and columns of A. This can be time consuming for
larger matrices.
</p>
<p>If the elements of the matrix are of different orders of magnitude it can be nec-
essary to balance the matrix, for instance by normalizing all rows of A. This can be
also achieved by selecting the maximum of
</p>
<p>aik&sum;
j |aij |
</p>
<p>(5.34)
</p>
<p>as the pivoting element.
</p>
<p>5.1.2 Direct LU Decomposition
</p>
<p>LU decomposition can be also performed in a different order [203]. For symmet-
ric positive definite matrices there exists the simpler and more efficient Cholesky</p>
<p/>
</div>
<div class="page"><p/>
<p>64 5 Systems of Inhomogeneous Linear Equations
</p>
<p>method decomposes the matrix into the product LLT of a lower triangular matrix
and its transpose [204].
</p>
<p>5.2 QR Decomposition
</p>
<p>The Gaussian elimination method can become numerically unstable [254]. An al-
ternative method to solve a system of linear equations uses the decomposition [108]
</p>
<p>A=QR (5.35)
</p>
<p>with a unitary matrix Q&dagger;Q= 1 (an orthogonal matrix QTQ= 1 if A is real) and an
upper right triangular matrix R. The system of linear equations (5.2) is simplified
by multiplication with Q&dagger; =Q&minus;1
</p>
<p>QRx=Ax= b (5.36)
</p>
<p>Rx=Q&dagger;b. (5.37)
</p>
<p>Such a system with upper triangular matrix is easily solved (see (5.29)).
</p>
<p>5.2.1 QR Decomposition by Orthogonalization
</p>
<p>Gram-Schmidt orthogonalization [108, 244] provides a simple way to perform a QR
decomposition. It is used for symbolic calculations and also for least square fitting
(10.1.2) but can become numerically unstable.
</p>
<p>From the decomposition A=QR we have
</p>
<p>aik =
k&sum;
</p>
<p>j=1
qij rjk (5.38)
</p>
<p>ak =
k&sum;
</p>
<p>j=1
rjkqj (5.39)
</p>
<p>which gives the k-th column vector ak of A as a linear combination of the orthonor-
mal vectors q1 &middot; &middot; &middot;qk . Similarly qk is a linear combination of the first k columns
of A. With the help of the Gram-Schmidt method rjk and qj are calculated as fol-
lows:
</p>
<p>r11 := |a1| (5.40)
</p>
<p>q1 :=
a1
</p>
<p>r11
. (5.41)</p>
<p/>
</div>
<div class="page"><p/>
<p>5.2 QR Decomposition 65
</p>
<p>For k = 2 &middot; &middot; &middot;n:
</p>
<p>rik := qiak i = 1 &middot; &middot; &middot;k&minus; 1 (5.42)
</p>
<p>bk := ak &minus; r1kq1 &minus; &middot; &middot; &middot; &minus; rk&minus;1,kqk&minus;1 (5.43)
</p>
<p>rkk := |bk| (5.44)
</p>
<p>qk :=
bk
</p>
<p>rkk
. (5.45)
</p>
<p>Obviously now
</p>
<p>ak = rkkqk + rk&minus;1,kqk&minus;1 + &middot; &middot; &middot; + r1kq1 (5.46)
</p>
<p>since per definition
</p>
<p>qiak = rik i = 1 &middot; &middot; &middot;k (5.47)
</p>
<p>and
</p>
<p>r2kk = |bk|2 = |ak|2 + r21k + &middot; &middot; &middot; + r2k&minus;1,k &minus; 2r21k &minus; &middot; &middot; &middot; &minus; 2r2k&minus;1,k. (5.48)
</p>
<p>Hence
</p>
<p>qkak =
1
</p>
<p>rkk
(ak &minus; r1kq1 &middot; &middot; &middot; rk&minus;1,kqk&minus;1)ak =
</p>
<p>1
</p>
<p>rkk
</p>
<p>(
|ak|2 &minus; r21k &minus; &middot; &middot; &middot; &minus; r2k&minus;1,k
</p>
<p>)
= rkk.
(5.49)
</p>
<p>Orthogonality gives
</p>
<p>qiak = 0 i = k + 1 &middot; &middot; &middot;n. (5.50)
</p>
<p>In matrix notation we have finally
</p>
<p>A= (a1 &middot; &middot; &middot;an)= (q1 &middot; &middot; &middot;qn)
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>r11 r12 &middot; &middot; &middot; r1n
r22 &middot; &middot; &middot; r2n
</p>
<p>. . .
...
</p>
<p>rnn
</p>
<p>⎞
⎟⎟⎟⎠ . (5.51)
</p>
<p>If the columns of A are almost linearly dependent, numerical stability can be im-
proved by an additional orthogonalization step
</p>
<p>bk &rarr; bk &minus; (q1bk)q1 &minus; &middot; &middot; &middot; &minus; (qk&minus;1bk)qk&minus;1 (5.52)
</p>
<p>after (5.43) which can be iterated several times to improve the results [66, 244].</p>
<p/>
</div>
<div class="page"><p/>
<p>66 5 Systems of Inhomogeneous Linear Equations
</p>
<p>Fig. 5.1 (Householder
transformation)
Geometrically the
Householder transformation
(5.53) is a mirror operation
with respect to a plane with
normal vector u
</p>
<p>5.2.2 QR Decomposition by Householder Reflections
</p>
<p>Numerically stable algorithms use a series of transformations with unitary matrices,
mostly Householder reflections (Fig. 5.1) [244]1 which have the form
</p>
<p>P = P T = 1 &minus; 2uuT (5.53)
</p>
<p>with a unit vector
</p>
<p>|u| = 1. (5.54)
Obviously P is an orthogonal matrix since
</p>
<p>P T P =
(
1 &minus; 2uuT
</p>
<p>)(
1 &minus; 2uuT
</p>
<p>)
= 1 &minus; 4uuT + 4uuT uuT = 1. (5.55)
</p>
<p>In the first step we try to find a vector u such that the first column vector of A
</p>
<p>a1 =
</p>
<p>⎛
⎜⎝
a11
...
</p>
<p>an1
</p>
<p>⎞
⎟⎠ (5.56)
</p>
<p>is transformed into a vector along the 1-axis
</p>
<p>Pa1 =
(
1 &minus; 2uuT
</p>
<p>)
a1 = ke1 =
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>k
</p>
<p>0
...
</p>
<p>0
</p>
<p>⎞
⎟⎟⎟⎠ . (5.57)
</p>
<p>1Alternatively Givens rotations [108] can be employed which need slightly more floating point
operations.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.2 QR Decomposition 67
</p>
<p>Multiplication with the transpose vector gives
</p>
<p>k2 = (Pa1)T Pa1 = aT1 P T Pa1 = |a1|2 (5.58)
</p>
<p>and
</p>
<p>k =&plusmn;|a1| (5.59)
can have both signs. From (5.57) we have
</p>
<p>a1 &minus; 2u(ua1)= ke1. (5.60)
</p>
<p>Multiplication with aT1 gives
</p>
<p>2(ua1)
2 = |a1|2 &minus; k(a1e1) (5.61)
</p>
<p>and since
</p>
<p>|a1 &minus; ke1|2 = |a1|2 + k2 &minus; 2k(a1e1)= 2|a1|2 &minus; 2k(a1e1) (5.62)
</p>
<p>we have
</p>
<p>2(ua1)
2 = 1
</p>
<p>2
|a1 &minus; ke1|2 (5.63)
</p>
<p>and from (5.60) we find
</p>
<p>u= a1 &minus; ke1
2ua1
</p>
<p>= a1 &minus; ke1|a1 &minus; ke1|
. (5.64)
</p>
<p>To avoid numerical extinction the sign of k is chosen such that
</p>
<p>σ = sign(k)=&minus; sign(a11). (5.65)
</p>
<p>Then,
</p>
<p>u = 1&radic;
2(a211 + &middot; &middot; &middot; + a2n1)+ 2|a11|
</p>
<p>&radic;
a211 + &middot; &middot; &middot; + a2n1
</p>
<p>&times;
</p>
<p>⎛
⎜⎜⎜⎜⎝
</p>
<p>sign(a11)(|a11| +
&radic;
a211 + a221 + &middot; &middot; &middot; + a2n1)
a21
...
</p>
<p>an1
</p>
<p>⎞
⎟⎟⎟⎟⎠
</p>
<p>(5.66)
</p>
<p>2uuT a1 =
</p>
<p>⎛
⎜⎜⎜⎜⎝
</p>
<p>sign(a11)(|a11| +
&radic;
a211 + a221 + &middot; &middot; &middot; + a2n1)
a21
...
</p>
<p>an1
</p>
<p>⎞
⎟⎟⎟⎟⎠</p>
<p/>
</div>
<div class="page"><p/>
<p>68 5 Systems of Inhomogeneous Linear Equations
</p>
<p>&times; 1
</p>
<p>(a211 + &middot; &middot; &middot; + a2n1)+ |a11|
&radic;
a211 + &middot; &middot; &middot; + a2n1
</p>
<p>&times;
(
a211 + |a11|
</p>
<p>&radic;
a211 + &middot; &middot; &middot; + a2n1 + a
</p>
<p>2
21 + &middot; &middot; &middot; + a2n1
</p>
<p>)
(5.67)
</p>
<p>and the Householder transformation of the first column vector of A gives
</p>
<p>(
1 &minus; 2uuT
</p>
<p>)
a1 =
</p>
<p>⎛
⎜⎜⎜⎜⎝
</p>
<p>&minus;sign(a11)
&radic;
a211 &middot; &middot; &middot;a2n1
</p>
<p>0
...
</p>
<p>0
</p>
<p>⎞
⎟⎟⎟⎟⎠
. (5.68)
</p>
<p>Thus after the first step a matrix results of the form
</p>
<p>A(1) = P1A=
</p>
<p>⎛
⎜⎜⎜⎜⎝
</p>
<p>a
(1)
11 a
</p>
<p>(1)
12 &middot; &middot; &middot; a
</p>
<p>(1)
1n
</p>
<p>0 a(1)22 a
(1)
2n
</p>
<p>...
...
</p>
<p>...
</p>
<p>0 a(1)n2 &middot; &middot; &middot; a
(1)
nn
</p>
<p>⎞
⎟⎟⎟⎟⎠
.
</p>
<p>In the following (n &minus; 2) steps further Householder reflections are applied in the
subspace k &le; i, j &le; n to eliminate the elements
</p>
<p>ak+1,k &middot; &middot; &middot;an,k
</p>
<p>of the k-th row vector below the diagonal of the matrix
</p>
<p>A(k&minus;1) = Pk&minus;1 &middot; &middot; &middot;P1A =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>a
(1)
11 &middot; &middot; &middot; a
</p>
<p>(1)
1,k&minus;1 a
</p>
<p>(1)
1,k &middot; &middot; &middot; a
</p>
<p>(1)
1,n
</p>
<p>0
. . .
</p>
<p>...
...
</p>
<p>...
...
</p>
<p>. . . a
(k&minus;1)
k&minus;1,k&minus;1 a
</p>
<p>(k&minus;1)
k&minus;1,k a
</p>
<p>(k&minus;1)
k&minus;1,n
</p>
<p>...
... 0 a(k&minus;1)k,k a
</p>
<p>(k&minus;1)
k,n
</p>
<p>...
...
</p>
<p>... a
(k&minus;1)
k+1,k a
</p>
<p>(k&minus;1)
k+1,n
</p>
<p>...
...
</p>
<p>...
...
</p>
<p>...
</p>
<p>0 &middot; &middot; &middot; 0 a(k&minus;1)n,k &middot; &middot; &middot; a
(k&minus;1)
n,n
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>(5.69)
</p>
<p>Pk =
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>11
. . .
</p>
<p>1k&minus;1
1 &minus; 2uuT
</p>
<p>⎞
⎟⎟⎟⎠ .</p>
<p/>
</div>
<div class="page"><p/>
<p>5.3 Linear Equations with Tridiagonal Matrix 69
</p>
<p>Finally an upper triangular matrix results
</p>
<p>A(n&minus;1) = (Pn&minus;1 &middot; &middot; &middot;P1)A=R =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>a
(1)
11 a
</p>
<p>(1)
12 &middot; &middot; &middot; a
</p>
<p>(1)
1,n&minus;1 a
</p>
<p>(1)
1,n
</p>
<p>0 a(2)22 &middot; &middot; &middot; a
(2)
2,n&minus;1 a
</p>
<p>(2)
2,n
</p>
<p>... 0
. . .
</p>
<p>...
...
</p>
<p>...
...
</p>
<p>... a
(n&minus;1)
n&minus;1,n&minus;1 a
</p>
<p>(n&minus;1)
n&minus;1,n
</p>
<p>0 0 &middot; &middot; &middot; 0 a(n&minus;1)n,n
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>. (5.70)
</p>
<p>If the orthogonal matrix Q is needed explicitly additional numerical operations are
necessary to form the product
</p>
<p>Q= (Pn&minus;1 &middot; &middot; &middot;P1)T . (5.71)
</p>
<p>5.3 Linear Equations with Tridiagonal Matrix
</p>
<p>Linear equations with the form
</p>
<p>b1x1 + c1x2 = r1 (5.72)
aixi&minus;1 + bixi + cixi+1 = ri i = 2 &middot; &middot; &middot; (n&minus; 1) (5.73)
</p>
<p>anxn&minus;1 + bnxn = rn (5.74)
</p>
<p>can be solved very efficiently with a specialized Gaussian elimination method.2
</p>
<p>They are important for cubic spline interpolation or second derivatives. We begin
by eliminating a2. To that end we multiply the first line with a2/b1 and subtract it
from the first line. The result is the equation
</p>
<p>β2x2 + c2x3 = ρ2 (5.75)
</p>
<p>with the abbreviations
</p>
<p>β2 = b2 &minus;
c1a2
</p>
<p>b1
ρ2 = r2 &minus;
</p>
<p>r1a2
</p>
<p>b1
. (5.76)
</p>
<p>We iterate this procedure
</p>
<p>βixi + cixi+1 = ρi (5.77)
</p>
<p>βi = bi &minus;
ci&minus;1ai
βi&minus;1
</p>
<p>ρi = ri &minus;
ρi&minus;1ai
βi&minus;1
</p>
<p>(5.78)
</p>
<p>2This algorithm is only well behaved if the matrix is diagonal dominant |bi |&gt; |ai | + |ci |.</p>
<p/>
</div>
<div class="page"><p/>
<p>70 5 Systems of Inhomogeneous Linear Equations
</p>
<p>until we reach the n-th equation, which becomes simply
</p>
<p>βnxn = ρn (5.79)
</p>
<p>βn = bn &minus;
cn&minus;1an
βn&minus;1
</p>
<p>ρn = rn &minus;
ρn&minus;1an
βn&minus;1
</p>
<p>. (5.80)
</p>
<p>Now we immediately have
</p>
<p>xn =
ρn
</p>
<p>βn
(5.81)
</p>
<p>and backward substitution gives
</p>
<p>xi&minus;1 =
ρi&minus;1 &minus; ci&minus;1xi
</p>
<p>βi&minus;1
(5.82)
</p>
<p>and finally
</p>
<p>x1 =
r1 &minus; c1x2
</p>
<p>β2
. (5.83)
</p>
<p>This algorithm can be formulated as LU decomposition: Multiplication of the ma-
trices
</p>
<p>L=
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>1
l2 1
</p>
<p>l3 1
. . .
</p>
<p>. . .
</p>
<p>ln 1
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠
</p>
<p>U =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>β1 c1
β2 c2
</p>
<p>β3 c3
. . .
</p>
<p>βn
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠
</p>
<p>(5.84)
</p>
<p>gives
</p>
<p>LU =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>β1 c1
. . .
</p>
<p>. . .
</p>
<p>. . .
. . .
</p>
<p>liβi&minus;1 (lici&minus;1 + βi) ci
. . .
</p>
<p>. . .
. . .
</p>
<p>lnβn&minus;1 (lncn&minus;1 + βn)
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>(5.85)</p>
<p/>
</div>
<div class="page"><p/>
<p>5.4 Cyclic Tridiagonal Systems 71
</p>
<p>which coincides with the matrix
</p>
<p>A=
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>b1 c1
</p>
<p>a2
. . .
</p>
<p>. . .
</p>
<p>. . .
. . .
</p>
<p>. . .
</p>
<p>ai bi ci
. . .
</p>
<p>. . .
. . .
</p>
<p>an&minus;1 bn&minus;1 cn&minus;1
an bn
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>(5.86)
</p>
<p>if we choose
</p>
<p>li =
ai
</p>
<p>βi&minus;1
(5.87)
</p>
<p>since then from (5.78)
</p>
<p>bi = βi + lici&minus;1 (5.88)
</p>
<p>and
</p>
<p>liβi&minus;1 = ai . (5.89)
</p>
<p>5.4 Cyclic Tridiagonal Systems
</p>
<p>Periodic boundary conditions lead to a small perturbation of the tridiagonal matrix
</p>
<p>A=
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>b1 c1 a1
</p>
<p>a2
. . .
</p>
<p>. . .
</p>
<p>. . .
. . .
</p>
<p>. . .
</p>
<p>ai bi ci
. . .
</p>
<p>. . .
. . .
</p>
<p>an&minus;1 bn&minus;1 cn&minus;1
cn an bn
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>. (5.90)
</p>
<p>The system of equations
</p>
<p>Ax= r (5.91)
</p>
<p>can be reduced to a tridiagonal system [205] with the help of the Sherman-Morrison
formula [236], which states that if A0 is an invertible matrix and u,v are vectors
and
</p>
<p>1 + vTA&minus;10 u �= 0 (5.92)</p>
<p/>
</div>
<div class="page"><p/>
<p>72 5 Systems of Inhomogeneous Linear Equations
</p>
<p>then the inverse of the matrix3
</p>
<p>A=A0 + uvT (5.93)
</p>
<p>is given by
</p>
<p>A&minus;1 =A&minus;10 &minus;
A&minus;10 uv
</p>
<p>TA&minus;10
1 + vTA&minus;10 u
</p>
<p>. (5.94)
</p>
<p>We choose
</p>
<p>uvT =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>α
</p>
<p>0
...
</p>
<p>0
cn
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠
</p>
<p>(
1 0 &middot; &middot; &middot; 0 a1
</p>
<p>α
</p>
<p>)
=
</p>
<p>⎛
⎜⎜⎜⎜⎝
</p>
<p>α a1
</p>
<p>cn
a1cn
α
</p>
<p>⎞
⎟⎟⎟⎟⎠
. (5.95)
</p>
<p>Then
</p>
<p>A0 =A&minus; uvT =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>(b1 &minus; α) c1 0
</p>
<p>a2
. . .
</p>
<p>. . .
</p>
<p>. . .
. . .
</p>
<p>. . .
</p>
<p>ai bi ci
. . .
</p>
<p>. . .
. . .
</p>
<p>an&minus;1 bn&minus;1 cn&minus;1
0 an (bn &minus; a1cnα )
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>(5.96)
is tridiagonal. The free parameter α has to be chosen such that the diagonal elements
do not become too small. We solve the system (5.91) by solving the two tridiagonal
systems
</p>
<p>A0x0 = r
</p>
<p>A0q = u
(5.97)
</p>
<p>and compute x from
</p>
<p>x=A&minus;1r=A&minus;10 r&minus;
(A&minus;10 u)v
</p>
<p>T (A&minus;10 r)
</p>
<p>1 + vT (A&minus;10 u)
= x0 &minus; q
</p>
<p>vT x0
</p>
<p>1 + vTq . (5.98)
</p>
<p>3Here uvT is the outer or matrix product of the two vectors.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.5 Iterative Solution of Inhomogeneous Linear Equations 73
</p>
<p>5.5 Iterative Solution of Inhomogeneous Linear Equations
</p>
<p>Discretized differential equations often lead to systems of equations with large
sparse matrices, which have to be solved by iterative methods.
</p>
<p>5.5.1 General Relaxation Method
</p>
<p>We want to solve
</p>
<p>Ax= b (5.99)
</p>
<p>iteratively. To that end we divide the matrix A into two (non singular) parts [244]
</p>
<p>A=A1 +A2 (5.100)
</p>
<p>and rewrite (5.99) as
</p>
<p>A1x= b&minus;A2x (5.101)
</p>
<p>which we use to define the iteration
</p>
<p>x(n+1) =Φ
(
x(n)
</p>
<p>)
(5.102)
</p>
<p>Φ(x)=&minus;A&minus;11 A2x+A
&minus;1
1 b. (5.103)
</p>
<p>A fixed point x of this equation fulfills
</p>
<p>xfp =Φ(xfp)=&minus;A&minus;11 A2xfp +A
&minus;1
1 b (5.104)
</p>
<p>and is obviously a solution of (5.99). The iteration can be written as
</p>
<p>x(n+1) = &minus;A&minus;11 (A&minus;A1)x
(n) +A&minus;11 b
</p>
<p>=
(
E &minus;A&minus;11 A
</p>
<p>)
x(n) +A&minus;11 b= x
</p>
<p>(n) &minus;A&minus;11
(
Ax(n) &minus; b
</p>
<p>)
(5.105)
</p>
<p>or
</p>
<p>A1
(
x(n+1) &minus; x(n)
</p>
<p>)
=&minus;
</p>
<p>(
Ax(n) &minus; b
</p>
<p>)
. (5.106)
</p>
<p>5.5.2 Jacobi Method
</p>
<p>Jacobi divides the matrix A into its diagonal and two triangular matrices [38]:
</p>
<p>A= L+U +D. (5.107)</p>
<p/>
</div>
<div class="page"><p/>
<p>74 5 Systems of Inhomogeneous Linear Equations
</p>
<p>For A1 the diagonal part is chosen
</p>
<p>A1 =D (5.108)
</p>
<p>giving
</p>
<p>x(n+1) =&minus;D&minus;1(A&minus;D)x(n) +D&minus;1b (5.109)
</p>
<p>which reads explicitly
</p>
<p>x
(n+1)
i =&minus;
</p>
<p>1
</p>
<p>aii
</p>
<p>&sum;
</p>
<p>j �=i
aijx
</p>
<p>(n)
j +
</p>
<p>1
</p>
<p>aii
bi . (5.110)
</p>
<p>This method is stable but converges rather slowly. Reduction of the error by a factor
of 10&minus;p needs about pN2 iterations. N grid points have to be evaluated in each
iteration and the method scales with O(N2) [206].
</p>
<p>5.5.3 Gauss-Seidel Method
</p>
<p>With
</p>
<p>A1 =D+L (5.111)
</p>
<p>the iteration becomes
</p>
<p>(D +L)x(n+1) =&minus;Ux(n) + b (5.112)
</p>
<p>which has the form of a system of equations with triangular matrix [139]. It reads
explicitly
</p>
<p>&sum;
</p>
<p>j&le;i
aijx
</p>
<p>(n+1)
j =&minus;
</p>
<p>&sum;
</p>
<p>j&gt;i
</p>
<p>aijx
(n)
j + bi . (5.113)
</p>
<p>Forward substitution starting from x1 gives
</p>
<p>i = 1: x(n+1)1 =
1
</p>
<p>a11
</p>
<p>(
&minus;
&sum;
</p>
<p>j&ge;2
aijx
</p>
<p>(n)
j + b1
</p>
<p>)
</p>
<p>i = 2: x(n+1)2 =
1
</p>
<p>a22
</p>
<p>(
&minus;a21x(n+1)1 &minus;
</p>
<p>&sum;
</p>
<p>j&ge;3
aijx
</p>
<p>(n)
j + b2
</p>
<p>)
</p>
<p>i = 3: x(n+1)3 =
1
</p>
<p>a33
</p>
<p>(
&minus;a31x(n+1)1 &minus; a32x
</p>
<p>(n+1)
2 &minus;
</p>
<p>&sum;
</p>
<p>j&ge;4
aijx
</p>
<p>(n)
j + b3
</p>
<p>)
</p>
<p>...</p>
<p/>
</div>
<div class="page"><p/>
<p>5.5 Iterative Solution of Inhomogeneous Linear Equations 75
</p>
<p>x
(n+1)
i =
</p>
<p>1
</p>
<p>aii
</p>
<p>(
&minus;
&sum;
</p>
<p>j&lt;i
</p>
<p>aijx
(n+1)
j &minus;
</p>
<p>&sum;
</p>
<p>j&gt;i
</p>
<p>aijx
(n)
j + bi
</p>
<p>)
. (5.114)
</p>
<p>This looks very similar to the Jacobi method. But here all changes are made im-
mediately. Convergence is slightly better (roughly a factor of 2) and the numerical
effort is reduced [206].
</p>
<p>5.5.4 Damping and Successive Over-Relaxation
</p>
<p>Convergence can be improved [206] by combining old and new values. Starting
from the iteration
</p>
<p>A1x
(n+1) = (A1 &minus;A)x(n) + b (5.115)
</p>
<p>we form a linear combination with
</p>
<p>Dx(n+1) =Dx(n) (5.116)
</p>
<p>giving the new iteration equation
</p>
<p>(
(1 &minus;ω)D +ωA1
</p>
<p>)
x(n+1) =
</p>
<p>(
(1 &minus;ω)D +ωA1 &minus;ωA
</p>
<p>)
x(n) +ωb. (5.117)
</p>
<p>In case of the Jacobi method with D =A1 we have
</p>
<p>Dx(n+1) = (D &minus;ωA)x(n) +ωb (5.118)
</p>
<p>or explicitly
</p>
<p>x
(n+1)
i = (1 &minus;ω)x
</p>
<p>(n)
i +
</p>
<p>ω
</p>
<p>aii
</p>
<p>(
&minus;
&sum;
</p>
<p>j �=i
aijx
</p>
<p>(n)
j + bi
</p>
<p>)
. (5.119)
</p>
<p>The changes are damped (0 &lt;ω &lt; 1) or exaggerated4 (1 &lt;ω &lt; 2).
In case of the Gauss-Seidel method with A1 =D + L the new iteration (5.117)
</p>
<p>is
</p>
<p>(D +ωL)x(n+1) = (D +ωL&minus;ωA)x(n) +ωb= (1 &minus;ω)Dx(n) &minus;ωUx(n) +ωb
(5.120)
</p>
<p>or explicitly
</p>
<p>x
(n+1)
i = (1 &minus;ω)x
</p>
<p>(n)
i +
</p>
<p>ω
</p>
<p>aii
</p>
<p>(
&minus;
&sum;
</p>
<p>j&lt;i
</p>
<p>aijx
(n+1)
j &minus;
</p>
<p>&sum;
</p>
<p>j&gt;i
</p>
<p>aijx
(n)
j + b
</p>
<p>)
. (5.121)
</p>
<p>4This is also known as the method of successive over-relaxation (SOR).</p>
<p/>
</div>
<div class="page"><p/>
<p>76 5 Systems of Inhomogeneous Linear Equations
</p>
<p>It can be shown that the successive over-relaxation method converges only for 0 &lt;
ω &lt; 2.
</p>
<p>For optimal choice of ω about 13p
&radic;
N iterations are needed to reduce the error
</p>
<p>by a factor of 10&minus;p . The order of the method is O(N
3
2 ) which is comparable to the
</p>
<p>most efficient matrix inversion methods [206].
</p>
<p>5.6 Conjugate Gradients
</p>
<p>At the minimum of the quadratic function
</p>
<p>h(x)= h0 + bT x+
1
</p>
<p>2
xTAx (5.122)
</p>
<p>the gradient
</p>
<p>g=&nabla;h(x)=Ax+ b (5.123)
is zero and therefore the minimum of h is also a solution of the linear system of
equations
</p>
<p>Ax=&minus;b. (5.124)
The stationary point can be found especially efficient with the method of conjugate
gradients (page 107). The function h is minimized along the search direction
</p>
<p>sr+1 =&minus;gr+1 + βr+1sr
</p>
<p>by solving
</p>
<p>0 = &part;
&part;λ
</p>
<p>(
bT (xr + λsr)+
</p>
<p>1
</p>
<p>2
</p>
<p>(
xTr + λsTr
</p>
<p>)
A(xr + λsr)
</p>
<p>)
</p>
<p>= bT sr + xTr Asr + λsTr Asr (5.125)
</p>
<p>λr =&minus;
bT sr + xTAsr
</p>
<p>sTr Asr
=&minus; grsr
</p>
<p>sTr Asr
. (5.126)
</p>
<p>The parameter β is chosen as
</p>
<p>βr+1 =
g2r+1
g2r
</p>
<p>. (5.127)
</p>
<p>The gradient of h is the residual vector and is iterated according to
</p>
<p>gr+1 =A(xr + λrsr)+ b= gr + λrAsr . (5.128)
</p>
<p>This method [121] solves a linear system without storing the matrix A itself. Only
the product As is needed. In principle the solution is reached after N = dim(A)
steps, but due to rounding errors more steps can be necessary and the method has to
be considered as an iterative one.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.7 Matrix Inversion 77
</p>
<p>5.7 Matrix Inversion
</p>
<p>LU and QR decomposition can be also used to calculate the inverse of a nonsingular
matrix
</p>
<p>AA&minus;1 = 1. (5.129)
</p>
<p>The decomposition is performed once and then the column vectors of A&minus;1 are cal-
culated similar to (5.24)
</p>
<p>L
(
UA&minus;1
</p>
<p>)
= 1 (5.130)
</p>
<p>or (5.37)
</p>
<p>RA&minus;1 =Q&dagger;. (5.131)
</p>
<p>Consider now a small variation of the right hand side of (5.2)
</p>
<p>b+�b. (5.132)
</p>
<p>Instead of
</p>
<p>A&minus;1b= x (5.133)
</p>
<p>the resulting vector is
</p>
<p>A&minus;1(b+�b)= x+�x (5.134)
</p>
<p>and the deviation can be measured by5
</p>
<p>‖�x‖ =
∥∥A&minus;1
</p>
<p>∥∥‖�b‖ (5.135)
</p>
<p>and since
</p>
<p>‖A‖‖x‖ = ‖b‖ (5.136)
</p>
<p>the relative error becomes
</p>
<p>‖�x‖
‖x‖ = ‖A‖
</p>
<p>∥∥A&minus;1
∥∥‖�b‖
</p>
<p>‖b‖ . (5.137)
</p>
<p>The relative error of b is multiplied by the condition number for inversion
</p>
<p>cond(A)= ‖A‖
∥∥A&minus;1
</p>
<p>∥∥. (5.138)
</p>
<p>5The vector norm used here is not necessarily the Euclidian norm.</p>
<p/>
</div>
<div class="page"><p/>
<p>78 5 Systems of Inhomogeneous Linear Equations
</p>
<p>5.8 Problems
</p>
<p>Problem 5.1 (Comparison of different direct Solvers, Fig. 5.2) In this computer
experiment we solve the system of equations
</p>
<p>Ax= b (5.139)
</p>
<p>with several methods:
</p>
<p>&bull; Gaussian elimination without pivoting (Sect. 5.1),
&bull; Gaussian elimination with partial pivoting (Sect. 5.1.1),
&bull; QR decomposition with Householder reflections (Sect. 5.2.2),
&bull; QR decomposition with Gram-Schmidt orthogonalization (Sect. 5.2.1),
&bull; QR decomposition with Gram-Schmidt orthogonalization with extra orthogonal-
</p>
<p>ization step (5.52).
</p>
<p>The right hand side is chosen as
</p>
<p>b=A
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>1
2
...
</p>
<p>n
</p>
<p>⎞
⎟⎟⎟⎠ (5.140)
</p>
<p>hence the exact solution is
</p>
<p>x=
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>1
2
...
</p>
<p>n
</p>
<p>⎞
⎟⎟⎟⎠ . (5.141)
</p>
<p>Several test matrices can be chosen:
</p>
<p>&bull; Gaussian elimination is theoretically unstable but is stable in many practical
cases. The instability can be demonstrated with the example [254]
</p>
<p>A=
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>1 1
&minus;1 1 1
&minus;1 &minus;1 1 1
...
</p>
<p>...
...
</p>
<p>. . .
...
</p>
<p>&minus;1 &minus;1 &minus;1 &minus;1 1
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠
. (5.142)</p>
<p/>
</div>
<div class="page"><p/>
<p>5.8 Problems 79
</p>
<p>Fig. 5.2 (Comparison of different direct solvers) Gaussian elimination without (circles) and with
(x) pivoting, QR decomposition with Householder reflections (squares), with Gram-Schmidt or-
thogonalization (diamonds) and including extra orthogonalization (+) are compared. The maxi-
mum difference maxi=1&middot;&middot;&middot;n(|xi &minus; xexacti |) increases only slightly with the dimension n for the well
behaved matrix ((5.146), a) but quite dramatically for the ill conditioned Hilbert matrix ((5.148), b)
</p>
<p>No pivoting takes place in the LU decomposition of this matrix and the entries in
the last column double in each step:
</p>
<p>A(1) =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>1 1
1 2
&minus;1 1 2
...
</p>
<p>...
. . .
</p>
<p>...
</p>
<p>&minus;1 &minus;1 &minus;1 2
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠
</p>
<p>A(2) =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>1 1
1 2
</p>
<p>1 4
...
</p>
<p>. . .
...
</p>
<p>&minus;1 &minus;1 4
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠
</p>
<p>&middot; &middot; &middot;
</p>
<p>A(n&minus;1) =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎝
</p>
<p>1 1
1 2
</p>
<p>. . . 4
. . .
</p>
<p>...
</p>
<p>2n&minus;1
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎠
.
</p>
<p>(5.143)
</p>
<p>Since the machine precision is ǫM = 2&minus;53 for double precision calculations
we have to expect numerical inaccuracy for dimension n &gt; 53.
</p>
<p>&bull; Especially well conditioned are matrices [273] which are symmetric
</p>
<p>Aij =Aji (5.144)
</p>
<p>and also diagonal dominant
</p>
<p>&sum;
</p>
<p>j �=i
|Aij |&lt; |Aii |. (5.145)</p>
<p/>
</div>
<div class="page"><p/>
<p>80 5 Systems of Inhomogeneous Linear Equations
</p>
<p>Fig. 5.3 (Condition
numbers) The condition
number cond(A) increases
only linearly with the
dimension n for the well
behaved matrix ((5.146), full
circles) but exponentially for
the ill conditioned Hilbert
matrix ((5.148), open circles)
</p>
<p>We use the matrix
</p>
<p>A=
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>n 1 &middot; &middot; &middot; 1 1
1 n &middot; &middot; &middot; 1 1
...
</p>
<p>. . .
...
</p>
<p>1 1 1 n 1
1 1 1 1 n
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠
</p>
<p>(5.146)
</p>
<p>which can be inverted explicitly by
</p>
<p>A&minus;1 =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>a b &middot; &middot; &middot; b b
b a b b
...
</p>
<p>. . .
</p>
<p>b b b a b
</p>
<p>b b b b a
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠
</p>
<p>a = 1
n&minus; 12
</p>
<p>, b=&minus; 1
2n2 &minus; 3n+ 1 (5.147)
</p>
<p>and has a condition number6 which is proportional to the dimension n (Fig. 5.3).
&bull; The Hilbert matrix [22, 122]
</p>
<p>A=
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎝
</p>
<p>1 12
1
3 &middot; &middot; &middot;
</p>
<p>1
n
</p>
<p>1
2
</p>
<p>1
3
</p>
<p>1
4
</p>
<p>1
n+1
</p>
<p>1
3
</p>
<p>1
4
</p>
<p>1
5
</p>
<p>1
n+2
</p>
<p>...
. . .
</p>
<p>...
1
n
</p>
<p>1
n+1
</p>
<p>1
n+2 &middot; &middot; &middot;
</p>
<p>1
2n&minus;1
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎠
</p>
<p>(5.148)
</p>
<p>is especially ill conditioned [16] even for moderate dimension. It is positive def-
inite and therefore the inverse matrix exists and even can be written down ex-
plicitly [222]. Its column vectors are very close to linearly dependent and the
</p>
<p>6Using the Frobenius norm ‖A‖ =
&radic;&sum;
</p>
<p>ij A
2
ij .</p>
<p/>
</div>
<div class="page"><p/>
<p>5.8 Problems 81
</p>
<p>condition number grows exponentially with its dimension (Fig. 5.3). Numerical
errors are large for all methods compared (Fig. 5.2).
</p>
<p>&bull; Random matrices
Aij = ξ &isin; [&minus;1,1]. (5.149)</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 6
</p>
<p>Roots and Extremal Points
</p>
<p>In computational physics very often roots of a function, i.e. solutions of an equation
like
</p>
<p>f (x1 &middot; &middot; &middot;xN )= 0 (6.1)
have to be determined. A related problem is the search for local extrema (Fig. 6.1)
</p>
<p>maxf (x1 &middot; &middot; &middot;xN ) minf (x1 &middot; &middot; &middot;xN ) (6.2)
which for a smooth function are solutions of the equations
</p>
<p>&part;f (x1 &middot; &middot; &middot;xN )
&part;xi
</p>
<p>= 0, i = 1 . . .N. (6.3)
</p>
<p>In one dimension bisection is a very robust but rather inefficient root finding method.
If a good starting point close to the root is available and the function smooth enough,
the Newton-Raphson method converges much faster. Special strategies are neces-
sary to find roots of not so well behaved functions or higher order roots. The combi-
nation of bisection and interpolation like in Dekker&rsquo;s and Brent&rsquo;s methods provides
generally applicable algorithms. In multidimensions calculation of the Jacobian ma-
trix is not always possible and Quasi-Newton methods are a good choice. Whereas
local extrema can be found as the roots of the gradient, at least in principle, direct
optimization can be more efficient. In one dimension the ternary search method or
Brent&rsquo;s more efficient golden section search method can be used. In multidimen-
sions the class of direction set search methods is very popular which includes the
methods of steepest descent and conjugate gradients, the Newton-Raphson method
and, if calculation of the full Hessian matrix is too expensive, the Quasi-Newton
methods.
</p>
<p>6.1 Root Finding
</p>
<p>If there is exactly one root in the interval a0 &lt; x &lt; b0 then one of the following
methods can be used to locate the position with sufficient accuracy. If there are
</p>
<p>P.O.J. Scherer, Computational Physics, Graduate Texts in Physics,
DOI 10.1007/978-3-319-00401-3_6,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>83</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-00401-3_6">http://dx.doi.org/10.1007/978-3-319-00401-3_6</a></div>
</div>
<div class="page"><p/>
<p>84 6 Roots and Extremal Points
</p>
<p>Fig. 6.1 Roots and local
extrema of a function
</p>
<p>Fig. 6.2 Root finding by
bisection
</p>
<p>multiple roots, these methods will find one of them and special care has to be taken
to locate the other roots.
</p>
<p>6.1.1 Bisection
</p>
<p>The simplest method [45] to solve
</p>
<p>f (x)= 0 (6.4)
uses the following algorithm (Fig. 6.2):
</p>
<p>(1) Determine an interval [a0, b0], which contains a sign change of f (x). If no such
interval can be found then f (x) does not have any zero crossings.
</p>
<p>(2) Divide the interval into [a0, a0+ b0&minus;a02 ] [a0+
b0&minus;a0
</p>
<p>2 , b0] and choose that interval
[a1, b1], where f (x) changes its sign.
</p>
<p>(3) repeat until the width bn &minus; an &lt; ε is small enough.1
</p>
<p>The bisection method needs two starting points which bracket a sign change of
the function. It converges but only slowly since each step reduces the uncertainty by
a factor of 2.
</p>
<p>1Usually a combination like ε = 2εM + |bn|εr of an absolute and a relative tolerance is taken.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.1 Root Finding 85
</p>
<p>Fig. 6.3 Regula falsi method
</p>
<p>6.1.2 Regula Falsi (False Position) Method
</p>
<p>The regula falsi [92] method (Fig. 6.3) is similar to the bisection method [45].
However, polynomial interpolation is used to divide the interval [xr , ar ] with
f (xr)f (ar) &lt; 0. The root of the linear polynomial
</p>
<p>p(x)= f (xr)+ (x &minus; xr)
f (ar)&minus; f (xr)
</p>
<p>ar &minus; xr
(6.5)
</p>
<p>is given by
</p>
<p>ξr = xr &minus; f (xr)
ar &minus; xr
</p>
<p>f (ar )&minus; f (xr)
= arf (xr)&minus; xrf (ar)
</p>
<p>f (xr)&minus; f (ar)
(6.6)
</p>
<p>which is inside the interval [xr , ar ]. Choose the sub-interval which contains the sign
change:
</p>
<p>f (xr)f (ξr) &lt; 0 &rarr;[xr+1, ar+1] = [xr , ξr ]
f (xr)f (ξr) &gt; 0 &rarr;[xr+1, ar+1] = [ξr , ar ].
</p>
<p>(6.7)
</p>
<p>Then ξr provides a series of approximations with increasing precision to the root of
f (x)= 0.
</p>
<p>6.1.3 Newton-Raphson Method
</p>
<p>Consider a function which is differentiable at least two times around the root ξ .
Taylor series expansion around a point x0 in the vicinity
</p>
<p>f (x)= f (x0)+ (x &minus; x0)f &prime;(x0)+
1
</p>
<p>2
(x &minus; x0)2f &prime;&prime;(x0)+ &middot; &middot; &middot; (6.8)
</p>
<p>gives for x = ξ
</p>
<p>0 = f (x0)+ (ξ &minus; x0)f &prime;(x0)+
1
</p>
<p>2
(ξ &minus; x0)2f &prime;&prime;(x0)+ &middot; &middot; &middot; . (6.9)</p>
<p/>
</div>
<div class="page"><p/>
<p>86 6 Roots and Extremal Points
</p>
<p>Fig. 6.4 Newton-Raphson
method
</p>
<p>Truncation of the series and solving for ξ gives the first order Newton-Raphson
[45, 252] method (Fig. 6.4)
</p>
<p>xr+1 = xr &minus;
f (xr)
</p>
<p>f &prime;(xr)
(6.10)
</p>
<p>and the second order Newton-Raphson method (Fig. 6.4)
</p>
<p>xr+1 = xr &minus;
f &prime;(xr )&plusmn;
</p>
<p>&radic;
f &prime;(xr)2 &minus; 2f (xr)f &prime;&prime;(xr)
</p>
<p>f &prime;&prime;(xr)
. (6.11)
</p>
<p>The Newton-Raphson method converges fast if the starting point is close enough
to the root. Analytic derivatives are needed. It may fail if two or more roots are close
by.
</p>
<p>6.1.4 Secant Method
</p>
<p>Replacing the derivative in the first order Newton Raphson method by a finite dif-
ference quotient gives the secant method [45] (Fig. 6.5) which has been known for
thousands of years before [196]
</p>
<p>xr+1 = xr &minus; f (xr)
xr &minus; xr&minus;1
</p>
<p>f (xr)&minus; f (xr&minus;1)
. (6.12)
</p>
<p>Round-off errors can become important as |f (xr) &minus; f (xr&minus;1)| gets small. At the
beginning choose a starting point x0 and determine
</p>
<p>x1 = x0 &minus; f (x0)
2h
</p>
<p>f (x0 + h)&minus; f (x0 &minus; h)
(6.13)
</p>
<p>using a symmetrical difference quotient.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.1 Root Finding 87
</p>
<p>Fig. 6.5 Secant method
</p>
<p>6.1.5 Interpolation
</p>
<p>The secant method is also obtained by linear interpolation
</p>
<p>p(x)= x &minus; xr
xr&minus;1 &minus; xr
</p>
<p>fr&minus;1 +
x &minus; xr&minus;1
xr &minus; xr&minus;1
</p>
<p>fr . (6.14)
</p>
<p>The root of the polynomial p(xr+1)= 0 determines the next iterate xr+1
</p>
<p>xr+1 =
1
</p>
<p>fr&minus;1 &minus; fr
(xrfr&minus;1 &minus; xr&minus;1fr)= xr &minus; fr
</p>
<p>xr &minus; xr&minus;1
fr &minus; fr&minus;1
</p>
<p>. (6.15)
</p>
<p>Quadratic interpolation of three function values is known as Muller&rsquo;s method [177].
Newton&rsquo;s form of the interpolating polynomial is
</p>
<p>p(x)= fr + (x &minus; xr)f [xr , xr&minus;1] + (x &minus; xr)(x &minus; xr&minus;1)f [xr , xr&minus;1, xr&minus;2]
(6.16)
</p>
<p>which can be rewritten
</p>
<p>p(x)= fr + (x &minus; xr)f [xr , xr&minus;1] + (x &minus; xr)2f [xr , xr&minus;1, xr&minus;2]
+ (xr &minus; xr&minus;1)(x &minus; xr)f [xr , xr&minus;1, xr&minus;2]
</p>
<p>= fr + (x &minus; xr)2f [xr , xr&minus;1, xr&minus;2]
+ (x &minus; xr)
</p>
<p>(
f [xr , xr&minus;1] + f [xr , xr&minus;2] &minus; f [xr&minus;1, xr&minus;2]
</p>
<p>)
</p>
<p>= fr +A(x &minus; xr)+B(x &minus; xr)2 (6.17)
</p>
<p>and has the roots
</p>
<p>xr+1 = xr &minus;
A
</p>
<p>2B
&plusmn;
</p>
<p>&radic;
A2
</p>
<p>4B2
&minus; fr
</p>
<p>B
. (6.18)
</p>
<p>To avoid numerical cancellation, this is rewritten</p>
<p/>
</div>
<div class="page"><p/>
<p>88 6 Roots and Extremal Points
</p>
<p>Fig. 6.6 Root finding by
interpolation of the inverse
function
</p>
<p>xr+1 = xr +
1
</p>
<p>2B
</p>
<p>(
&minus;A&plusmn;
</p>
<p>&radic;
A2 &minus; 4Bfr
</p>
<p>)
</p>
<p>= xr +
&minus;2fr
</p>
<p>A2 &minus; (A2 &minus; 4Bfr)
(
A∓
</p>
<p>&radic;
A2 &minus; 4Bfr
</p>
<p>)
</p>
<p>= xr +
&minus;2fr
</p>
<p>A&plusmn;
&radic;
A2 &minus; 4Bfr
</p>
<p>. (6.19)
</p>
<p>The sign in the denominator is chosen such that xr+1 is the root closer to xr . The
roots of the polynomial can become complex valued and therefore this method is
useful to find complex roots.
</p>
<p>6.1.6 Inverse Interpolation
</p>
<p>Complex values of xr can be avoided by interpolation of the inverse function instead
</p>
<p>x = f&minus;1(y). (6.20)
</p>
<p>Using the two points xr , xr&minus;1 the Lagrange method gives
</p>
<p>p(y)= xr&minus;1
y &minus; fr
</p>
<p>fr&minus;1 &minus; fr
+ xr
</p>
<p>y &minus; fr&minus;1
fr &minus; fr&minus;1
</p>
<p>(6.21)
</p>
<p>and the next approximation of the root corresponds again to the secant method (6.12)
</p>
<p>xr+1 = p(0)=
xr&minus;1fr &minus; xrfr&minus;1
</p>
<p>fr &minus; fr&minus;1
= xr +
</p>
<p>(xr&minus;1 &minus; xr)
fr &minus; fr&minus;1
</p>
<p>fr . (6.22)
</p>
<p>Inverse quadratic interpolation needs three starting points xr , xr&minus;1, xr&minus;2 together
with the function values fr , fr&minus;1, fr&minus;2 (Fig. 6.6). The inverse function x = f&minus;1(y)
is interpolated with the Lagrange method</p>
<p/>
</div>
<div class="page"><p/>
<p>6.1 Root Finding 89
</p>
<p>Fig. 6.7 (Validity of inverse quadratic interpolation) Inverse quadratic interpolation is only appli-
cable if the interpolating polynomial p(y) is monotonous in the range of the interpolated function
values f1 . . . f3. (a) and (b) show the limiting cases where the polynomial has a horizontal tangent
at f1 or f3. (c) shows the case where the extremum of the parabola is inside the interpolation range
and interpolation is not feasible
</p>
<p>p(y)= (y &minus; fr&minus;1)(y &minus; fr)
(fr&minus;2 &minus; fr&minus;1)(fr&minus;2 &minus; fr)
</p>
<p>xr&minus;2 +
(y &minus; fr&minus;2)(y &minus; fr)
</p>
<p>(fr&minus;1 &minus; fr&minus;2)(fr&minus;1 &minus; fr)
xr&minus;1
</p>
<p>+ (y &minus; fr&minus;1)(y &minus; fr&minus;2)
(fr &minus; fr&minus;1)(fr &minus; fr&minus;2)
</p>
<p>xr . (6.23)
</p>
<p>For y = 0 we find the next iterate
</p>
<p>xr+1 = p(0)=
fr&minus;1fr
</p>
<p>(fr&minus;2 &minus; fr&minus;1)(fr&minus;2 &minus; fr)
xr&minus;2 +
</p>
<p>fr&minus;2fr
(fr&minus;1 &minus; fr&minus;2)(fr&minus;1 &minus; fr)
</p>
<p>xr&minus;1
</p>
<p>+ fr&minus;1fr&minus;2
(fr &minus; fr&minus;1)(fr &minus; fr&minus;2)
</p>
<p>xr . (6.24)
</p>
<p>Inverse quadratic interpolation is only a good approximation if the interpolating
parabola is single valued and hence if it is a monotonous function in the range of
fr , fr&minus;1, fr&minus;2. For the following discussion we assume that the three values of x
are renamed such that x1 &lt; x2 &lt; x3.
</p>
<p>Consider the limiting case (a) in Fig. 6.7 where the polynomial has a horizontal
tangent at y = f3 and can be written as
</p>
<p>p(y)= x3 + (x1 &minus; x3)
(y &minus; f3)2
(f1 &minus; f3)2
</p>
<p>. (6.25)
</p>
<p>Its value at y = 0 is
</p>
<p>p(0)= x3 + (x1 &minus; x3)
f 23
</p>
<p>(f1 &minus; f3)2
</p>
<p>= x1 + (x3 &minus; x1)
(
</p>
<p>1 &minus;
f 23
</p>
<p>(f1 &minus; f3)2
)
. (6.26)
</p>
<p>If f1 and f3 have different sign and |f1|&lt; |f3| (Sect. 6.1.7.2) we find</p>
<p/>
</div>
<div class="page"><p/>
<p>90 6 Roots and Extremal Points
</p>
<p>1 &minus;
f 23
</p>
<p>(f1 &minus; f3)2
&lt;
</p>
<p>3
</p>
<p>4
. (6.27)
</p>
<p>Brent [36] used this as a criterion for the applicability of the inverse quadratic
interpolation. However, this does not include all possible cases where interpolation
is applicable. Chandrupatla [54] gave a more general discussion. The limiting con-
dition is that the polynomial p(y) has a horizontal tangent at one of the boundaries
x1,3. The derivative values are
</p>
<p>dp
</p>
<p>dy
(y = f1)=
</p>
<p>x2(f1 &minus; f3)
(f2 &minus; f1)(f2 &minus; f3)
</p>
<p>+ x3(f1 &minus; f2)
(f3 &minus; f1)(f3 &minus; f2)
</p>
<p>+ x1
f1 &minus; f2
</p>
<p>+ x1
f1 &minus; f3
</p>
<p>= (f2 &minus; f1)
(f3 &minus; f1)(f3 &minus; f2)
</p>
<p>[
x2(f3 &minus; f1)2
(f2 &minus; f1)2
</p>
<p>&minus; x3
</p>
<p>&minus; x1(f3 &minus; f1)
2 &minus; x1(f2 &minus; f1)2
</p>
<p>(f2 &minus; f1)2
]
</p>
<p>= (f2 &minus; f1)(x2 &minus; x1)
(f3 &minus; f1)(f3 &minus; f2)
</p>
<p>[
Φ&minus;2 &minus; ξ&minus;1
</p>
<p>]
(6.28)
</p>
<p>dp
</p>
<p>dy
(y = f3)=
</p>
<p>x2(f3 &minus; f1)
(f2 &minus; f1)(f2 &minus; f3)
</p>
<p>+ x1(f3 &minus; f2)
(f1 &minus; f2)(f1 &minus; f3)
</p>
<p>+ x3
f3 &minus; f2
</p>
<p>+ x3
f3 &minus; f1
</p>
<p>= (f3 &minus; f2)
(f2 &minus; f1)(f3 &minus; f1)
</p>
<p>[
&minus;x2(f3 &minus; f1)
</p>
<p>2
</p>
<p>(f3 &minus; f2)2
+ x3
</p>
<p>(f3 &minus; f1)2
(f3 &minus; f2)2
</p>
<p>&minus; x3
(f3 &minus; f2)2
(f3 &minus; f2)2
</p>
<p>+ x1
]
</p>
<p>= (f3 &minus; f2)(x3 &minus; x2)
(f2 &minus; f1)(f3 &minus; f1)
</p>
<p>[(
1
</p>
<p>Φ &minus; 1
</p>
<p>)2
&minus; 1
</p>
<p>1 &minus; ξ
</p>
<p>]
(6.29)
</p>
<p>with [54]
</p>
<p>ξ = x2 &minus; x1
x3 &minus; x1
</p>
<p>Φ = f2 &minus; f1
f3 &minus; f1
</p>
<p>(6.30)
</p>
<p>ξ &minus; 1 = x2 &minus; x3
x3 &minus; x1
</p>
<p>Φ &minus; 1 = f2 &minus; f3
f3 &minus; f1
</p>
<p>. (6.31)
</p>
<p>Since for a parabola either f1 &lt; f2 &lt; f3 or f1 &gt; f2 &gt; f3 the conditions for appli-
cability of inverse interpolation finally become
</p>
<p>Φ2 &lt; ξ (6.32)
</p>
<p>1 &minus; ξ &gt; (1 &minus;Φ)2 (6.33)
</p>
<p>which can be combined into
</p>
<p>1 &minus;
&radic;
</p>
<p>1 &minus; ξ &lt; |Φ|&lt;
&radic;
ξ . (6.34)
</p>
<p>This method is usually used in combination with other methods (Sect. 6.1.7.2).</p>
<p/>
</div>
<div class="page"><p/>
<p>6.1 Root Finding 91
</p>
<p>6.1.7 Combined Methods
</p>
<p>Bisection converges slowly. The interpolation methods converge faster but are less
reliable. The combination of both gives methods which are reliable and converge
faster than pure bisection.
</p>
<p>6.1.7.1 Dekker&rsquo;s Method
</p>
<p>Dekker&rsquo;s method [47, 70] combines bisection and secant method. The root is brack-
eted by intervals [cr , br ] with decreasing width where br is the best approxima-
tion to the root found and cr is an earlier guess for which f (cr) and f (br) have
different sign. First an attempt is made to use linear interpolation between the
points (br , f (br)) and (ar , f (ar)) where ar is usually the preceding approximation
ar = br&minus;1 and is set to the second interval boundary ar = cr&minus;1 if the last iteration
did not lower the function value (Fig. 6.8).
</p>
<p>Starting from an initial interval [x0, x1] with sign(f (x0)) �= sign(f (x1)) the
method proceeds as follows [47]:
</p>
<p>&bull; initialization
</p>
<p>f1 = f (x1) f0 = f (x0)
if |f1|&lt; |f0| then {
b= x1 c= a = x0
fb = f1 fc = fa = f0}
else {
b= x0 c= a = x1
fb = f0 fc = fa = f1}
</p>
<p>&bull; iteration
</p>
<p>xs = b&minus; fb
b&minus; a
fb &minus; fa
</p>
<p>xm =
c+ b
</p>
<p>2
.
</p>
<p>If xs is very close to the last b then increase the distance to avoid too small steps
else choose xs if it is between b and xm, otherwise choose xm (thus choosing the
smaller interval)
</p>
<p>xr =
</p>
<p>⎧
⎨
⎩
b+ δ sign(c&minus; b) if abs(xs &minus; b) &lt; δ
xs if b+ δ &lt; xs &lt; xm or b&minus; δ &gt; xs &gt; xm
xm else.</p>
<p/>
</div>
<div class="page"><p/>
<p>92 6 Roots and Extremal Points
</p>
<p>Fig. 6.8 Dekker&rsquo;s method
</p>
<p>Determine xk as the latest of the previous iterates x0 . . . xr&minus;1 for which
sign(f (xk)) �= sign(f (xr)).
</p>
<p>If the new function value is lower update the approximation to the root
</p>
<p>fr = f (xr)
if |fr |&lt; |fk| then {
a = b b= xr c= xk
fa = fb fb = fr fc = fk}
</p>
<p>otherwise keep the old approximation and update the second interval boundary
</p>
<p>if |fr | &ge; |fk| then {
b= xk a = c= xr
fb = fk fa = fc = fr}
</p>
<p>repeat until |c&minus; b|&lt; ε or fr = 0.
</p>
<p>6.1.7.2 Brent&rsquo;s Method
</p>
<p>In certain cases Dekker&rsquo;s method converges very slowly making many small steps of
the order ε. Brent [36, 37, 47] introduced some modifications to reduce such prob-
lems and tried to speed up convergence by the use of inverse quadratic interpolation
(Sect. 6.1.6). To avoid numerical problems the iterate (6.24) is written with the help
of a quotient
</p>
<p>xr+1 =
fbfc
</p>
<p>(fa &minus; fb)(fa &minus; fc)
a + fafc
</p>
<p>(fb &minus; fa)(fb &minus; fc)
b
</p>
<p>+ fbfa
(fc &minus; fb)(fc &minus; fa)
</p>
<p>c
</p>
<p>= b+ p
q
</p>
<p>(6.35)
</p>
<p>with</p>
<p/>
</div>
<div class="page"><p/>
<p>6.1 Root Finding 93
</p>
<p>p = fb
fa
</p>
<p>(
(c&minus; b)fa
</p>
<p>fc
</p>
<p>(
fa
</p>
<p>fc
&minus; fb
</p>
<p>fc
</p>
<p>)
&minus; (b&minus; a)
</p>
<p>(
fb
</p>
<p>fc
&minus; 1
</p>
<p>))
</p>
<p>= (c&minus; b)fb(fa &minus; fb)
f 2c
</p>
<p>&minus; (b&minus; a)fb(fb &minus; fc)
fafc
</p>
<p>= afbfc(fb &minus; fc)+ b[fafb(fb &minus; fa)+ fbfc(fc &minus; fb)] + cfafb(fa &minus; fb)
faf 2c (6.36)
</p>
<p>q =&minus;
(
fa
</p>
<p>fc
&minus; 1
</p>
<p>)(
fb
</p>
<p>fc
&minus; 1
</p>
<p>)(
fb
</p>
<p>fa
&minus; 1
</p>
<p>)
=&minus; (fa &minus; fc)(fb &minus; fc)(fb &minus; fa)
</p>
<p>faf 2c
.
</p>
<p>(6.37)
</p>
<p>If only two points are available linear interpolation is used. The iterate (6.22)
then is written as
</p>
<p>xr+1 = b+
(a &minus; b)
fb &minus; fa
</p>
<p>fb = b+
p
</p>
<p>q
(6.38)
</p>
<p>with
</p>
<p>p = (a &minus; b)fb
fa
</p>
<p>q =
(
fb
</p>
<p>fa
&minus; 1
</p>
<p>)
. (6.39)
</p>
<p>The division is only performed if interpolation is appropriate and division by zero
cannot happen. Brent&rsquo;s method is fast and robust at the same time. It is often rec-
ommended by text books. The algorithm is summarized in the following [209].
</p>
<p>Start with an initial interval [x0, x1] with f (x0)f (x1)&le; 0
&bull; initialization
</p>
<p>a = x0 b= x1 c= a
fa = f (a) fb = f (b) fc = fa
e= d = b&minus; a
</p>
<p>&bull; iteration
If c is a better approximation than b exchange values
</p>
<p>if |fc|&lt; |fb| then {
a = b b= c c= a
fa = fb fb = fc fc = fa}
</p>
<p>calculate midpoint relative to b
</p>
<p>xm = 0.5(c&minus; b)
stop if accuracy is sufficient
</p>
<p>if |xm|&lt; ε or fb = 0 then exit
use bisection if the previous step width e was too small or the last step did not
improve</p>
<p/>
</div>
<div class="page"><p/>
<p>94 6 Roots and Extremal Points
</p>
<p>if |e|&lt; ε or |fa| &le; |fb| then {
e= d = xm}
</p>
<p>otherwise try interpolation
</p>
<p>else {
</p>
<p>if a = c then
{
</p>
<p>p = 2xm
fb
</p>
<p>fa
q = fb &minus; fa
</p>
<p>fa
</p>
<p>}
</p>
<p>else
</p>
<p>{
</p>
<p>p = 2xm
fb(fa &minus; fb)
</p>
<p>f 2c
&minus; (b&minus; a)fb(fb &minus; fc)
</p>
<p>fafc
</p>
<p>q =
(
fa
</p>
<p>fc
&minus; 1
</p>
<p>)(
fb
</p>
<p>fc
&minus; 1
</p>
<p>)(
fb
</p>
<p>fa
&minus; 1
</p>
<p>)}
</p>
<p>make p a positive quantity
</p>
<p>if p &gt; 0 then {q =&minus;q} else {p =&minus;p}
update previous step width
</p>
<p>s = e e= d
use interpolation if applicable, otherwise use bisection
</p>
<p>if 2p &lt; 3xmq &minus; |εq| and p &lt; |0.5 sq| then
{
</p>
<p>d = p
q
</p>
<p>}
</p>
<p>else {e= d = xm}
a = b fa = fb
if |d|&gt; ε then {
b= b+ d}
else
</p>
<p>{
b= b+ ε sign(xm)
</p>
<p>}
</p>
<p>calculate new function value
</p>
<p>fb = f (b)
be sure to bracket the root
</p>
<p>if sign(fb)= sign(fc) then {
c= a fc = fa
e= d = b&minus; a}</p>
<p/>
</div>
<div class="page"><p/>
<p>6.1 Root Finding 95
</p>
<p>Fig. 6.9 Chandrupatla&rsquo;s
method
</p>
<p>6.1.7.3 Chandrupatla&rsquo;s method
</p>
<p>In 1997 Chandrupatla [54] published a method which tries to use inverse quadratic
interpolation whenever possible according to (6.34). He calculates the relative posi-
tion of the new iterate as (Fig. 6.9)
</p>
<p>t = x &minus; c
b&minus; c
</p>
<p>= 1
b&minus; c
</p>
<p>[
fcfb
</p>
<p>(fa &minus; fb)(fa &minus; fc)
a + fafc
</p>
<p>(fb &minus; fa)(fb &minus; fc)
b
</p>
<p>+ fbfa
(fc &minus; fb)(fc &minus; fa)
</p>
<p>c&minus; c
]
</p>
<p>= a &minus; c
b&minus; c
</p>
<p>fc
</p>
<p>fc &minus; fa
fb
</p>
<p>fb &minus; fa
+ fafc
</p>
<p>(fb &minus; fa)(fb &minus; fc)
. (6.40)
</p>
<p>The algorithm proceeds as follows:
</p>
<p>Start with an initial interval [x0, x1] with f (x0)f (x1)&le; 0
</p>
<p>&bull; initialization
</p>
<p>b= x0 a = c= x1
fb = f (b) fa = fc = f (c)
t = 0.5
</p>
<p>&bull; iteration
</p>
<p>xt = a + t (b&minus; a)
ft = f (xt )</p>
<p/>
</div>
<div class="page"><p/>
<p>96 6 Roots and Extremal Points
</p>
<p>Fig. 6.10 (Comparison of different solvers) The root of the equation f (x)= x2 &minus;2 is determined
with different methods: Newton-Raphson (a, squares), Chandrupatla (b, circles), Brent (c, trian-
gle up), Dekker (d, diamonds), regula falsi (e, stars), pure bisection (f, dots). Starting values are
x1 =&minus;1, x2 = 2. The absolute error is shown as function of the number of iterations. For x1 =&minus;1,
the Newton-Raphson method converges against &minus;
</p>
<p>&radic;
2
</p>
<p>Fig. 6.11 (Comparison of
different solvers for a third
order root) The root of the
equation f (x)= (x &minus; 1)3 is
determined with different
methods: Newton-Raphson
(a), Chandrupatla (b),
Brent (c), Dekker (d), regula
falsi (e), pure bisection (f ).
Starting values are x1 = 0,
x2 = 1.8. The absolute error
is shown as function of the
number of iterations
</p>
<p>Fig. 6.12 (Comparison of
different solvers for a high
order root) The root of the
equation f (x)= x25 is
determined with different
methods: Newton-Raphson
(a), Chandrupatla (b, circles),
Brent (c), Dekker (d), regula
falsi (e), pure bisection (f,
dots). Starting values are
x1 =&minus;1, x2 = 2. The
absolute error is shown as
function of the number of
iterations</p>
<p/>
</div>
<div class="page"><p/>
<p>6.1 Root Finding 97
</p>
<p>if sign(ft )= sign(fa) {
c= a fc = fa
a = xt fa = Ft }
else {
c= b b= a a = xt
fc = fb fb = fa fa = ft }
xm = a fm = fa
if abs(fb) &lt; abs(fa) {
xm = b fm = fb}
tol = 2εM |xm| + εa
</p>
<p>tl =
tol
</p>
<p>|b&minus; c|
if tl &gt; 0.5 or fm = 0 exit
</p>
<p>ξ = a &minus; b
c&minus; b Φ =
</p>
<p>fa &minus; fb
fc &minus; fb
</p>
<p>if 1 &minus;
&radic;
</p>
<p>1 &minus; ξ &lt; Φ &lt;
&radic;
ξ
</p>
<p>{
</p>
<p>t = fa
fb &minus; fa
</p>
<p>fc
</p>
<p>fb &minus; fc
+ c&minus; a
</p>
<p>b&minus; a
fa
</p>
<p>fc &minus; fa
fb
</p>
<p>fc &minus; fb
</p>
<p>}
</p>
<p>else {t = 0.5}
if t &lt; tl {t = tl}
if t &gt; (1 &minus; tl) {t = 1 &minus; tl}
</p>
<p>Chandrupatlas&rsquo;s method is more efficient than Dekker&rsquo;s and Brent&rsquo;s, especially for
higher order roots (Figs. 6.10, 6.11, 6.12).
</p>
<p>6.1.8 Multidimensional Root Finding
</p>
<p>The Newton-Raphson method can be easily generalized for functions of more than
one variable. We search for the solution of a system of n nonlinear equations in n
variables xi
</p>
<p>f(x)=
</p>
<p>⎛
⎜⎝
f1(x1 &middot; &middot; &middot;xn)
</p>
<p>...
</p>
<p>fn(x1 &middot; &middot; &middot;xn)
</p>
<p>⎞
⎟⎠= 0. (6.41)
</p>
<p>The first order Newton-Raphson method results from linearization of
</p>
<p>0 = f(x)= f
(
x0
)
+ J
</p>
<p>(
x0
)(
x&minus; x0
</p>
<p>)
+ &middot; &middot; &middot; (6.42)
</p>
<p>with the Jacobian matrix</p>
<p/>
</div>
<div class="page"><p/>
<p>98 6 Roots and Extremal Points
</p>
<p>J =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>&part;f1
&part;x1
</p>
<p>&middot; &middot; &middot; &part;f1
&part;xn
</p>
<p>...
. . .
</p>
<p>...
&part;fn
&part;x1
</p>
<p>&middot; &middot; &middot; &part;fn
&part;xn
</p>
<p>⎞
⎟⎟⎠ . (6.43)
</p>
<p>If the Jacobian matrix is not singular the equation
</p>
<p>0 = f
(
x0
)
+ J
</p>
<p>(
x0
)(
x&minus; x0
</p>
<p>)
(6.44)
</p>
<p>can be solved by
</p>
<p>x= x0 &minus;
(
J
(
x0
))&minus;1
</p>
<p>f
(
x0
)
. (6.45)
</p>
<p>This can be repeated iteratively
</p>
<p>x(r+1) = x(r) &minus;
(
J
(
x(r)
</p>
<p>))&minus;1
f
(
x(r)
</p>
<p>)
. (6.46)
</p>
<p>6.1.9 Quasi-Newton Methods
</p>
<p>Calculation of the Jacobian matrix can be very time consuming. Quasi-Newton
methods use instead an approximation to the Jacobian which is updated during each
iteration. Defining the differences
</p>
<p>d(r) = x(r+1) &minus; x(r) (6.47)
y(r) = f
</p>
<p>(
x(r+1)
</p>
<p>)
&minus; f
</p>
<p>(
x(r)
</p>
<p>)
(6.48)
</p>
<p>we obtain from the truncated Taylor series
</p>
<p>f
(
x(r+1)
</p>
<p>)
= f
</p>
<p>(
x(r)
</p>
<p>)
+ J
</p>
<p>(
x(r)
</p>
<p>)(
x(r+1) &minus; x(r)
</p>
<p>)
(6.49)
</p>
<p>the so called Quasi-Newton or secant condition
</p>
<p>y(r) = J
(
x(r)
</p>
<p>)
d(r). (6.50)
</p>
<p>We attempt to construct a family of successive approximation matrices Jr so that, if
J were a constant, the procedure would become consistent with the Quasi-Newton
condition. Then for the new update Jr+1 we have
</p>
<p>Jr+1d
(r) = y(r). (6.51)
</p>
<p>Since d(r),y(r) are already known, these are only n equations for the n2 elements of
Jr+1. To specify Jr+1 uniquely, additional conditions are required. For instance, it
is reasonable to assume, that
</p>
<p>Jr+1u= Jru for all u&perp; d(r). (6.52)
Then Jr+1 differs from Jr only by a rank one updating matrix
</p>
<p>Jr+1 = Jr + ud(r)T . (6.53)
From the secant condition we obtain</p>
<p/>
</div>
<div class="page"><p/>
<p>6.2 Function Minimization 99
</p>
<p>Jr+1d
(r) = Jrd(r) + u
</p>
<p>(
d(r)d(r)T
</p>
<p>)
= y(r) (6.54)
</p>
<p>hence
</p>
<p>u= 1|d(r)|2
(
y(r) &minus; Jrd(r)
</p>
<p>)
. (6.55)
</p>
<p>This gives Broyden&rsquo;s update formula [42]
</p>
<p>Jr+1 = Jr +
1
</p>
<p>|d(r)|2
(
y(r) &minus; Jrd(r)
</p>
<p>)
d(r)T . (6.56)
</p>
<p>To update the inverse Jacobian matrix, the Sherman-Morrison formula [236]
</p>
<p>(
A+ uvT
</p>
<p>)&minus;1 =A&minus;1 &minus; A
&minus;1uvTA&minus;1
</p>
<p>1 + vTA&minus;1u (6.57)
</p>
<p>can be applied to have
</p>
<p>J&minus;1r+1 = J
&minus;1
r &minus;
</p>
<p>J&minus;1r
1
</p>
<p>|d(r)|2 (y
(r) &minus; Jrd(r))d(r)T J&minus;1r
</p>
<p>1 + 1|d(r)|2 d
(r)T J&minus;1r (y(r) &minus; Jrd(r))
</p>
<p>= J&minus;1r &minus;
(J&minus;1r y
</p>
<p>(r) &minus; d(r))d(r)T J&minus;1r
d(r)T J&minus;1r y(r)
</p>
<p>. (6.58)
</p>
<p>6.2 Function Minimization
</p>
<p>Minimization or maximization of a function2 is a fundamental task in numerical
mathematics and closely related to root finding. If the function f (x) is continuously
differentiable then at the extremal points the derivative is zero
</p>
<p>df
</p>
<p>dx
= 0. (6.59)
</p>
<p>Hence, in principle root finding methods can be applied to locate local extrema of
a function. However, in some cases the derivative cannot be easily calculated or the
function even is not differentiable. Then derivative free methods similar to bisection
for root finding have to be used.
</p>
<p>6.2.1 The Ternary Search Method
</p>
<p>Ternary search is a simple method to determine the minimum of a unimodal func-
tion f (x). Initially we have to find an interval [a0, b0] which is certain to con-
tain the minimum. Then the interval is divided into three equal parts [a0, c0],
</p>
<p>2In the following we consider only a minimum since a maximum could be found as the minimum
of &minus;f (x).</p>
<p/>
</div>
<div class="page"><p/>
<p>100 6 Roots and Extremal Points
</p>
<p>Fig. 6.13 Ternary search
method
</p>
<p>[c0, d0], [d0, b0] and either the first or the last of the three intervals is excluded
(Fig. 6.13). The procedure is repeated with the remaining interval [a1, b1] = [a0, d0]
or [a1, b1] = [c0, b0].
</p>
<p>Each step needs two function evaluations and reduces the interval width by a fac-
tor of 2/3 until the maximum possible precision is obtained. It can be determined by
considering a differentiable function which can be expanded around the minimum
x0 as
</p>
<p>f (x)= f (x0)+
(x &minus; x0)2
</p>
<p>2
f &prime;&prime;(x0)+ &middot; &middot; &middot; . (6.60)
</p>
<p>Numerically calculated function values f (x) and f (x0) only differ, if
</p>
<p>(x &minus; x0)2
2
</p>
<p>f &prime;&prime;(x0) &gt; εMf (x0) (6.61)
</p>
<p>which limits the possible numerical accuracy to
</p>
<p>ε(x0)= min|x &minus; x0| =
&radic;
</p>
<p>2f (x0)
</p>
<p>f &prime;&prime;(x0)
εM (6.62)
</p>
<p>and for reasonably well behaved functions (Fig. 6.14) we have the rule of thumb
[207]
</p>
<p>ε(x0)&asymp;
&radic;
εM . (6.63)
</p>
<p>However, it may be impossible to reach even this precision, if the quadratic term
of the Taylor series vanishes (Fig. 6.15).
</p>
<p>The algorithm can be formulated as follows:
</p>
<p>&bull; iteration
</p>
<p>if (b&minus; a) &lt; δ then exit
</p>
<p>c= a + 1
3
(b&minus; a) d = a + 2
</p>
<p>3
(b&minus; a)
</p>
<p>fc = f (c) fd = f (d)
if fc &lt; fd then b= d else a = c</p>
<p/>
</div>
<div class="page"><p/>
<p>6.2 Function Minimization 101
</p>
<p>Fig. 6.14 (Ternary search method) The minimum of the function f (x)= 1 + 0.01x2 + 0.1x4 is
determined with the ternary search method. Each iteration needs two function evaluations. After
50 iterations the function minimum fmin = 1 is reached to machine precision εM &asymp; 10&minus;16. The
position of the minimum xmin cannot be determined with higher precision than
</p>
<p>&radic;
εM &asymp; 10&minus;8 (6.63)
</p>
<p>Fig. 6.15 (Ternary search method for a higher order minimum) The minimum of the function
f (x)= 1+ 0.1x4 is determined with the ternary search method. Each iteration needs two function
evaluations. After 30 iterations the function minimum fmin = 1 is reached to machine precision
εM &asymp; 10&minus;16. The position of the minimum xmin cannot be determined with higher precision than
4
&radic;
εM &asymp; 10&minus;4
</p>
<p>6.2.2 The Golden Section Search Method (Brent&rsquo;s Method)
</p>
<p>To bracket a local minimum of a unimodal function f (x) three points a, b, c are
necessary (Fig. 6.16) with
</p>
<p>f (a) &gt; f (b) f (c) &gt; f (b). (6.64)</p>
<p/>
</div>
<div class="page"><p/>
<p>102 6 Roots and Extremal Points
</p>
<p>Fig. 6.16 (Golden section search method) A local minimum of the function f (x) is bracketed by
three points a, b, c. To reduce the uncertainty of the minimum position a new point ξ is chosen in
the interval a &lt; ξ &lt; c and either a or c is dropped according to the relation of the function values.
For the example shown a has to be replaced by ξ
</p>
<p>Fig. 6.17 Golden section search method
</p>
<p>The position of the minimum can be determined iteratively by choosing a new
value ξ in the interval a &lt; ξ &lt; c and dropping either a or c, depending on the ratio
of the function values. A reasonable choice for ξ can be found as follows (Fig. 6.17)
[147, 207]. Let us denote the relative positions of the middle point and the trial point
as
</p>
<p>b&minus; a
c&minus; a = β
</p>
<p>c&minus; b
c&minus; a = 1 &minus; β
</p>
<p>b&minus; a
c&minus; b =
</p>
<p>β
</p>
<p>1 &minus; β
ξ &minus; b
c&minus; a = t
</p>
<p>ξ &minus; a
c&minus; a =
</p>
<p>ξ &minus; b+ b&minus; a
c&minus; a = t + β.
</p>
<p>(6.65)
</p>
<p>The relative width of the new interval will be
</p>
<p>c&minus; ξ
c&minus; a = (1 &minus; β &minus; t) or
</p>
<p>b&minus; a
c&minus; a = β if a &lt; ξ &lt; b (6.66)
</p>
<p>ξ &minus; a
c&minus; a = (t + β) or
</p>
<p>c&minus; b
c&minus; a = (1 &minus; β) if b &lt; ξ &lt; c. (6.67)</p>
<p/>
</div>
<div class="page"><p/>
<p>6.2 Function Minimization 103
</p>
<p>The golden search method requires that
</p>
<p>t = 1 &minus; 2β = c+ a &minus; 2b
c&minus; a =
</p>
<p>(c&minus; b)&minus; (b&minus; a)
c&minus; a . (6.68)
</p>
<p>Otherwise it would be possible that the larger interval width is selected many times
slowing down the convergence. The value of t is positive if c &minus; b &gt; b &minus; a and
negative if c &minus; b &lt; b &minus; a, hence the trial point always is in the larger of the two
intervals. In addition the golden search method requires that the ratio of the spacing
remains constant. Therefore we set
</p>
<p>β
</p>
<p>1 &minus; β =&minus;
t + β
t
</p>
<p>=&minus;1 &minus; β
t
</p>
<p>if a &lt; ξ &lt; b (6.69)
</p>
<p>β
</p>
<p>1 &minus; β =
t
</p>
<p>1 &minus; β &minus; t =
t
</p>
<p>β
if b &lt; ξ &lt; c. (6.70)
</p>
<p>Eliminating t we obtain for a &lt; ξ &lt; b the equation
</p>
<p>(β &minus; 1)
β
</p>
<p>(
β2 + β &minus; 1
</p>
<p>)
= 0. (6.71)
</p>
<p>Besides the trivial solution β = 1 there is only one positive solution
</p>
<p>β =
&radic;
</p>
<p>5 &minus; 1
2
</p>
<p>&asymp; 0.618. (6.72)
</p>
<p>For b &lt; ξ &lt; c we end up with
</p>
<p>β
</p>
<p>β &minus; 1
(
β2 &minus; 3β + 1
</p>
<p>)
= 0 (6.73)
</p>
<p>which has the nontrivial positive solution
</p>
<p>β = 3 &minus;
&radic;
</p>
<p>5
</p>
<p>2
&asymp; 0.382. (6.74)
</p>
<p>Hence the lengths of the two intervals [a, b], [b, c] have to be in the golden ratio ϕ =
1+
</p>
<p>&radic;
5
</p>
<p>2 &asymp; 1.618 which gives the method its name. Using the golden ratio the width of
the interval bracketing the minimum reduces by a factor of 0.618 (Figs. 6.18, 6.19).
</p>
<p>The algorithm can be formulated as follows:</p>
<p/>
</div>
<div class="page"><p/>
<p>104 6 Roots and Extremal Points
</p>
<p>Fig. 6.18 (Golden section search method) The minimum of the function f (x) = 1 +
0.01x2 + 0.1x4 is determined with the golden section search method. Each iteration needs only
one function evaluation. After 40 iterations the function minimum fmin = 1 is reached to machine
precision εM &asymp; 10&minus;16. The position of the minimum xmin cannot be determined to higher precision
than
</p>
<p>&radic;
εM &asymp; 10&minus;8 (6.63)
</p>
<p>if c&minus; a &lt; δ then exit
if (b&minus; a)&ge; (c&minus; b) then {
x = 0.618b+ 0.382a
fx = f (x)
if fx &lt; fb then {c= b b= x fc = fb fb = fx}
else a = x fa = fx}
if (b&minus; a) &lt; (c&minus; b) then {
x = 0.618b+ 0.382c
fx = f (x)
if fx &lt; fb then {a = b b= x fa = fb fb = fx}
else c= x fc = fx}
</p>
<p>To start the method we need three initial points which can be found by Brent&rsquo;s
exponential search method (Fig. 6.20). Begin with three points
</p>
<p>a0, b0 = a0 + h, c0 + 1.618h (6.75)
</p>
<p>where h0 is a suitable initial step width which depends on the problem. If the mini-
mum is not already bracketed then if necessary exchange a0 and b0 to have
</p>
<p>f (a0) &gt; f (b0) &gt; f (c0). (6.76)
</p>
<p>Then replace the three points by</p>
<p/>
</div>
<div class="page"><p/>
<p>6.2 Function Minimization 105
</p>
<p>Fig. 6.19 (Golden section search for a higher order minimum) The minimum of the function
f (x)= 1 + 0.1x4 is determined with the golden section search method. Each iteration needs only
one function evaluation. After 28 iterations the function minimum fmin = 1 is reached to machine
precision εM &asymp; 10&minus;16. The position of the minimum xmin cannot be determined to higher precision
than 4
</p>
<p>&radic;
εM &asymp; 10&minus;4
</p>
<p>Fig. 6.20 Brent&rsquo;s
exponential search
</p>
<p>a1 = b0 b1 = c0 c1 = c0 + 1.618(c0 &minus; b0) (6.77)
</p>
<p>and repeat this step until
</p>
<p>f (bn) &lt; f (cn) (6.78)
</p>
<p>or n exceeds a given maximum number. In this case no minimum can be found and
we should check if the initial step width was too large.
</p>
<p>Brent&rsquo;s method can be improved by making use of derivatives and by combining
the golden section search with parabolic interpolation [207].</p>
<p/>
</div>
<div class="page"><p/>
<p>106 6 Roots and Extremal Points
</p>
<p>Fig. 6.21 (Direction set
minimization) Starting from
an initial guess x0 a local
minimum is approached by
making steps along a set of
direction vectors sr
</p>
<p>6.2.3 Minimization in Multidimensions
</p>
<p>We search for local minima (or maxima) of a function
</p>
<p>h(x)
</p>
<p>which is at least two times differentiable. In the following we denote the gradient
vector by
</p>
<p>gT (x)=
(
&part;h
</p>
<p>&part;x1
, . . . ,
</p>
<p>&part;h
</p>
<p>&part;xn
</p>
<p>)
(6.79)
</p>
<p>and the matrix of second derivatives (Hessian) by
</p>
<p>H =
(
</p>
<p>&part;2
</p>
<p>&part;xi&part;xj
h
</p>
<p>)
. (6.80)
</p>
<p>The very popular class of direction set methods proceeds as follows (Fig. 6.21).
Starting from an initial guess x0 a set of direction vectors sr and step lengths λr is
determined such that the series of vectors
</p>
<p>xr+1 = xr + λrsr (6.81)
approaches the minimum of h(x). The method stops if the norm of the gradient
becomes sufficiently small or if no lower function value can be found.
</p>
<p>6.2.4 Steepest Descent Method
</p>
<p>The simplest choice, which is known as the method of gradient descent or steepest
descent3 is to go in the direction of the negative gradient
</p>
<p>3Which should not be confused with the method of steepest descent for the approximate calculation
of integrals.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.2 Function Minimization 107
</p>
<p>sr =&minus;gr (6.82)
</p>
<p>and to determine the step length by minimizing h along this direction
</p>
<p>h(λ)= h(xr &minus; λgr)= min. (6.83)
</p>
<p>Obviously two consecutive steps are orthogonal to each other since
</p>
<p>0 = &part;
&part;λ
</p>
<p>h(xr+1 &minus; λgr)|λ=0 =&minus;gTr+1gr . (6.84)
</p>
<p>This can lead to a zig-zagging behavior and a very slow convergence of this method
(Figs. 6.22, 6.23).
</p>
<p>6.2.5 Conjugate Gradient Method
</p>
<p>This method is similar to the steepest descent method but the search direction is
iterated according to
</p>
<p>s0 =&minus;g0 (6.85)
xr+1 = xr + λrsr (6.86)
</p>
<p>sr+1 =&minus;gr+1 + βr+1sr (6.87)
</p>
<p>where λr is chosen to minimize h(xr+1) and the simplest choice for β is made by
Fletcher and Rieves [89]
</p>
<p>βr+1 =
g2r+1
g2r
</p>
<p>. (6.88)
</p>
<p>6.2.6 Newton-Raphson Method
</p>
<p>The first order Newton-Raphson method uses the iteration
</p>
<p>xr+1 = xr &minus;H(xr)&minus;1g(xr). (6.89)
</p>
<p>The search direction is
</p>
<p>s=H&minus;1g (6.90)
</p>
<p>and the step length is λ= 1. This method converges fast if the starting point is close
to the minimum. However, calculation of the Hessian can be very time consuming
(Fig. 6.22).</p>
<p/>
</div>
<div class="page"><p/>
<p>108 6 Roots and Extremal Points
</p>
<p>Fig. 6.22 (Function
minimization) The minimum
of the Rosenbrock
function h(x, y)=
100(y &minus; x2)2 + (1 &minus; x)2 is
determined with different
methods. Conjugate gradients
converge much faster than
steepest descent. Starting at
(x, y)= (0,2),
Newton-Raphson reaches the
minimum at x = y = 1 within
only 5 iterations to machine
precision
</p>
<p>6.2.7 Quasi-Newton Methods
</p>
<p>Calculation of the full Hessian matrix as needed for the Newton-Raphson method
can be very time consuming. Quasi-Newton methods use instead an approximation
to the Hessian which is updated during each iteration. From the Taylor series</p>
<p/>
</div>
<div class="page"><p/>
<p>6.2 Function Minimization 109
</p>
<p>Fig. 6.23 (Minimization of the Rosenbrock function) Left: Newton-Raphson finds the minimum
after 5 steps within machine precision. Middle: conjugate gradients reduce the gradient to 4&times;10&minus;14
after 265 steps. Right: The steepest descent method needs 20000 steps to reduce the gradient to
5 &times; 10&minus;14. Red lines show the minimization pathway. Colored areas indicate the function value
(light blue: &lt; 0.1, grey: 0.1. . . 0.5, green: 5. . . 50, pink: &gt; 100). Screenshots taken from Prob-
lem 6.2
</p>
<p>h(x)= h0 + bT x+
1
</p>
<p>2
xTHx+ &middot; &middot; &middot; (6.91)
</p>
<p>we obtain the gradient
</p>
<p>g(xr)= b+Hxr + &middot; &middot; &middot; = g(xr&minus;1)+H(xr &minus; xr&minus;1)+ &middot; &middot; &middot; . (6.92)
Defining the differences
</p>
<p>dr = xr+1 &minus; xr (6.93)
yr = gr+1 &minus; gr (6.94)
</p>
<p>and neglecting higher order terms we obtain the Quasi-Newton or secant condition
</p>
<p>Hdr = yr . (6.95)
We want to approximate the true Hessian by a series of matrices Hr which are
updated during each iteration to sum up all the information gathered so far. Since
the Hessian is symmetric and positive definite, this also has to be demanded for
the Hr .4 This cannot be achieved by a rank one update matrix. Popular methods use
a symmetric rank two update of the form
</p>
<p>Hr+1 =Hr + αuuT + βvvT . (6.96)
The Quasi-Newton condition then gives
</p>
<p>Hr+1dr =Hrdr + α
(
uT dr
</p>
<p>)
u+ β
</p>
<p>(
vT dr
</p>
<p>)
v= yr (6.97)
</p>
<p>hence Hrdr &minus;yr must be a linear combination of u and v. Making the simple choice
</p>
<p>u= yr v=Hrdr (6.98)
</p>
<p>4This is a major difference to the Quasi-Newton methods for root finding (6.1.9).</p>
<p/>
</div>
<div class="page"><p/>
<p>110 6 Roots and Extremal Points
</p>
<p>and assuming that these two vectors are linearly independent, we find
</p>
<p>β =&minus; 1
(vT dr)
</p>
<p>=&minus; 1
(dTr Hrdr)
</p>
<p>(6.99)
</p>
<p>α = 1
(uT dr)
</p>
<p>= 1
(yTr dr)
</p>
<p>(6.100)
</p>
<p>which together defines the very popular BFGS (Broyden, Fletcher, Goldfarb,
Shanno) method [43, 86, 105, 235]
</p>
<p>Hr+1 =Hr +
yry
</p>
<p>T
r
</p>
<p>yTr dr
&minus; (Hrdr)(Hrdr)
</p>
<p>T
</p>
<p>dTr Hrdr
. (6.101)
</p>
<p>Alternatively the DFP method by Davidon, Fletcher and Powell, directly updates
the inverse Hessian matrix B =H&minus;1 according to
</p>
<p>Br+1 = Br +
drd
</p>
<p>T
r
</p>
<p>yTr dr
&minus; (Bryr)(Bryr)
</p>
<p>T
</p>
<p>yTr Bryr
. (6.102)
</p>
<p>Both of these methods can be inverted with the help of the Sherman-Morrison
formula to give
</p>
<p>Br+1 = Br +
(dr &minus;Bryr)dTr + dr(dr &minus;Byr)T
</p>
<p>yTr dr
&minus; (dr &minus;Bryr)
</p>
<p>T y
</p>
<p>(yTr d)
2
</p>
<p>ddT
</p>
<p>(6.103)
</p>
<p>Hr+1 =Hr +
(yr &minus;Hrdr)yTr + yr(yr &minus;Hrdr)T
</p>
<p>yTr dr
&minus; (yr &minus;Hrdr)dr
</p>
<p>(yTr dr)
2
</p>
<p>yry
T
r .
</p>
<p>(6.104)
</p>
<p>6.3 Problems
</p>
<p>Problem 6.1 (Root finding methods) This computer experiment searches roots of
several test functions:
</p>
<p>f (x)= xn &minus; 2 n= 1,2,3,4 (Fig. 6.10)
f (x)= 5 sin(5x)
f (x)=
</p>
<p>(
cos(2x)
</p>
<p>)2 &minus; x2
</p>
<p>f (x)= 5
(&radic;
</p>
<p>|x + 2| &minus; 1
)
</p>
<p>f (x)= e&minus;x lnx
f (x)= (x &minus; 1)3 (Fig. 6.11)
f (x)= x25 (Fig. 6.12)
</p>
<p>You can vary the initial interval or starting value and compare the behavior of
different methods:</p>
<p/>
</div>
<div class="page"><p/>
<p>6.3 Problems 111
</p>
<p>&bull; bisection
&bull; regula falsi
&bull; Dekker&rsquo;s method
&bull; Brent&rsquo;s method
&bull; Chandrupatla&rsquo;s method
&bull; Newton-Raphson method
</p>
<p>Problem 6.2 (Stationary points) This computer experiment searches a local mini-
mum of the Rosenbrock function5
</p>
<p>h(x, y)= 100
(
y &minus; x2
</p>
<p>)2 + (1 &minus; x)2. (6.105)
&bull; The method of steepest descent minimizes h(x, y) along the search direction
</p>
<p>s(n)x =&minus;g(n)x =&minus;400x
(
x2n &minus; yn
</p>
<p>)
&minus; 2(xn &minus; 1) (6.106)
</p>
<p>s(n)y =&minus;g(n)y =&minus;200
(
yn &minus; x2n
</p>
<p>)
. (6.107)
</p>
<p>&bull; Conjugate gradients make use of the search direction
</p>
<p>s(n)x =&minus;g(n)x + βns(n&minus;1)x (6.108)
</p>
<p>s(n)y =&minus;g(n)y + βns(n&minus;1)y . (6.109)
&bull; The Newton-Raphson method needs the inverse Hessian
</p>
<p>H&minus;1 = 1
det(H)
</p>
<p>(
hyy &minus;hxy
&minus;hxy hxx
</p>
<p>)
(6.110)
</p>
<p>det(H)= hxxhyy &minus; h2xy (6.111)
hxx = 1200x2 &minus; 400y + 2 hyy = 200 hxy =&minus;400x (6.112)
</p>
<p>and iterates according to
(
xn+1
yn+1
</p>
<p>)
=
(
xn
yn
</p>
<p>)
&minus;H&minus;1
</p>
<p>(
gnx
qny
</p>
<p>)
. (6.113)
</p>
<p>You can choose an initial point (x0, y0). The iteration stops if the gradient norm
falls below 10&minus;14 or if the line search fails to find a lower function value.
</p>
<p>5A well known test function for minimization algorithms.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 7
</p>
<p>Fourier Transformation
</p>
<p>Fourier transformation is a very important tool for signal analysis but also helpful
to simplify the solution of differential equations or the calculation of convolution
integrals. In this chapter we discuss the discrete Fourier transformation as a numer-
ical approximation to the continuous Fourier integral. It can be realized efficiently
by Goertzel&rsquo;s algorithm or the family of fast Fourier transformation methods. Com-
puter experiments demonstrate trigonometric interpolation and nonlinear filtering as
applications.
</p>
<p>7.1 Fourier Integral and Fourier Series
</p>
<p>We use the symmetric definition of the Fourier transformation:
</p>
<p>f̃ (ω)=F[f ](ω)= 1&radic;
2π
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
f (t)e&minus;iωtdt. (7.1)
</p>
<p>The inverse Fourier transformation
</p>
<p>f (t)=F&minus;1[f̃ ](t)= 1&radic;
2π
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
f̃ (ω)eiωtdω (7.2)
</p>
<p>decomposes f (t) into a superposition of oscillations. The Fourier transform of a
convolution integral
</p>
<p>g(t)= f (t)&otimes; h(t)=
&int; &infin;
</p>
<p>&minus;&infin;
f
(
t &prime;
)
h
(
t &minus; t &prime;
</p>
<p>)
dt &prime; (7.3)
</p>
<p>becomes a product of Fourier transforms:
</p>
<p>g̃(ω) = 1&radic;
2π
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
dt &prime;f
</p>
<p>(
t &prime;
)
e&minus;iωt
</p>
<p>&prime;
&int; &infin;
</p>
<p>&minus;&infin;
h
(
t &minus; t &prime;
</p>
<p>)
e&minus;iω(t&minus;t
</p>
<p>&prime;)d
(
t &minus; t &prime;
</p>
<p>)
</p>
<p>=
&radic;
</p>
<p>2πf̃ (ω)̃h(ω). (7.4)
</p>
<p>P.O.J. Scherer, Computational Physics, Graduate Texts in Physics,
DOI 10.1007/978-3-319-00401-3_7,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>113</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-00401-3_7">http://dx.doi.org/10.1007/978-3-319-00401-3_7</a></div>
</div>
<div class="page"><p/>
<p>114 7 Fourier Transformation
</p>
<p>A periodic function with f (t + T )= f (t)1 is transformed into a Fourier series
</p>
<p>f (t)=
&infin;&sum;
</p>
<p>n=&minus;&infin;
eiωnt f̂ (ωn) with ωn = n
</p>
<p>2π
</p>
<p>T
, f̂ (ωn)=
</p>
<p>1
</p>
<p>T
</p>
<p>&int; T
</p>
<p>0
f (t)e&minus;iωntdt. (7.5)
</p>
<p>For a periodic function which in addition is real valued f (t) = f (t)&lowast; and even
f (t)= f (&minus;t), the Fourier series becomes a cosine series
</p>
<p>f (t)= f̂ (ω0)+ 2
&infin;&sum;
</p>
<p>n=1
f̂ (ωn) cosωnt (7.6)
</p>
<p>with real valued coefficients
</p>
<p>f̂ (ωn)=
1
</p>
<p>T
</p>
<p>&int; T
</p>
<p>0
f (t) cosωnt dt. (7.7)
</p>
<p>7.2 Discrete Fourier Transformation
</p>
<p>We divide the time interval 0 &le; t &lt; T by introducing a grid of N equidistant points
</p>
<p>tn = n�t = n
T
</p>
<p>N
with n= 0,1 &middot; &middot; &middot;N &minus; 1. (7.8)
</p>
<p>The function values (samples)
</p>
<p>fn = f (tn) (7.9)
are arranged as components of a vector
</p>
<p>f=
</p>
<p>⎛
⎜⎝
</p>
<p>f0
...
</p>
<p>fN&minus;1
</p>
<p>⎞
⎟⎠ .
</p>
<p>With respect to the orthonormal basis
</p>
<p>en =
</p>
<p>⎛
⎜⎝
</p>
<p>δ0,n
...
</p>
<p>δN&minus;1,n
</p>
<p>⎞
⎟⎠ , n= 0,1 &middot; &middot; &middot;N &minus; 1 (7.10)
</p>
<p>f is expressed as a linear combination
</p>
<p>f=
N&minus;1&sum;
</p>
<p>n=0
fnen. (7.11)
</p>
<p>1This could also be the periodic continuation of a function which is only defined for 0 &lt; t &lt; T .</p>
<p/>
</div>
<div class="page"><p/>
<p>7.2 Discrete Fourier Transformation 115
</p>
<p>The discrete Fourier transformation is the transformation to an orthogonal base in
frequency space
</p>
<p>eωj =
N&minus;1&sum;
</p>
<p>n=0
eiωj tnen =
</p>
<p>⎛
⎜⎜⎜⎜⎝
</p>
<p>1
</p>
<p>ei
2π
N
j
</p>
<p>...
</p>
<p>ei
2π
N
j (N&minus;1)
</p>
<p>⎞
⎟⎟⎟⎟⎠
</p>
<p>(7.12)
</p>
<p>with
</p>
<p>ωj =
2π
</p>
<p>T
j. (7.13)
</p>
<p>These vectors are orthogonal
</p>
<p>eωj e
&lowast;
ωj &prime;
</p>
<p>=
N&minus;1&sum;
</p>
<p>n=0
ei(j&minus;j
</p>
<p>&prime;) 2π
N
n =
</p>
<p>{
1&minus;ei(j&minus;j &prime;)2π
</p>
<p>1&minus;ei(j&minus;j &prime;)2π/N = 0 for j &minus; j
&prime; �= 0
</p>
<p>N for j &minus; j &prime; = 0
(7.14)
</p>
<p>eωj e
&lowast;
ωj &prime;
</p>
<p>=Nδj,j &prime; . (7.15)
</p>
<p>Alternatively a real valued basis can be defined:
</p>
<p>cos
</p>
<p>(
2π
</p>
<p>N
jn
</p>
<p>)
j = 0,1 &middot; &middot; &middot; jmax
</p>
<p>sin
</p>
<p>(
2π
</p>
<p>N
jn
</p>
<p>)
j = 1,2 &middot; &middot; &middot; jmax
</p>
<p>jmax =
N
</p>
<p>2
(even N) jmax =
</p>
<p>N &minus; 1
2
</p>
<p>(odd N).
</p>
<p>(7.16)
</p>
<p>The components of f in frequency space are given by the scalar product
</p>
<p>f̃ωj = feωj =
N&minus;1&sum;
</p>
<p>n=0
fne
</p>
<p>&minus;iωj tn =
N&minus;1&sum;
</p>
<p>n=0
fne
</p>
<p>&minus;ij 2π
T
n T
N =
</p>
<p>N&minus;1&sum;
</p>
<p>n=0
fne
</p>
<p>&minus;i 2π
N
jn. (7.17)
</p>
<p>From
</p>
<p>N&minus;1&sum;
</p>
<p>j=0
f̃ωj e
</p>
<p>iωj tn =
&sum;
</p>
<p>n&prime;
</p>
<p>&sum;
</p>
<p>ωj
</p>
<p>fn&prime;e
&minus;iωj tn&prime; eiωj tn =Nfn (7.18)
</p>
<p>we find the inverse transformation
</p>
<p>fn =
1
</p>
<p>N
</p>
<p>N&minus;1&sum;
</p>
<p>j=0
f̃ωj e
</p>
<p>iωj tn = 1
N
</p>
<p>N&minus;1&sum;
</p>
<p>j=0
f̃ωj e
</p>
<p>i 2π
N
nj . (7.19)</p>
<p/>
</div>
<div class="page"><p/>
<p>116 7 Fourier Transformation
</p>
<p>Fig. 7.1 (Equivalence of ω1
and ωN&minus;1) The two functions
cosωt and cos(N &minus; 1)ωt
have the same values at the
sample points tn but are very
different in between
</p>
<p>7.2.1 Trigonometric Interpolation
</p>
<p>The last equation can be interpreted as an interpolation of the function f (t) at the
sampling points tn by a linear combination of trigonometric functions
</p>
<p>f (t)= 1
N
</p>
<p>N&minus;1&sum;
</p>
<p>j=0
f̃ωj
</p>
<p>(
ei
</p>
<p>2π
T
t
)j (7.20)
</p>
<p>which is a polynomial of
</p>
<p>q = ei 2πT t . (7.21)
</p>
<p>Since
</p>
<p>e&minus;iωj tn = e&minus;i 2πN jn = ei 2πN (N&minus;j)n = eiωN&minus;j tn (7.22)
</p>
<p>the frequencies ωj and ωN&minus;j are equivalent (Fig. 7.1)
</p>
<p>f̃ωN&minus;j =
N&minus;1&sum;
</p>
<p>n=0
fne
</p>
<p>&minus;i 2π
N
(N&minus;j)n =
</p>
<p>N&minus;1&sum;
</p>
<p>n=0
fne
</p>
<p>i 2π
N
jn = f̃ω&minus;j . (7.23)
</p>
<p>If we use trigonometric interpolation to approximate f (t) between the grid
points, the two frequencies are no longer equivalent and we have to restrict the
frequency range to avoid unphysical high frequency components (Fig. 7.2):
</p>
<p>&minus;2π
T
</p>
<p>N &minus; 1
2
</p>
<p>&le; ωj &le;
2π
</p>
<p>T
</p>
<p>N &minus; 1
2
</p>
<p>N odd
</p>
<p>(7.24)
</p>
<p>&minus;2π
T
</p>
<p>N
</p>
<p>2
&le; ωj &le;
</p>
<p>2π
</p>
<p>T
</p>
<p>(
N
</p>
<p>2
&minus; 1
</p>
<p>)
N even.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.2 Discrete Fourier Transformation 117
</p>
<p>Fig. 7.2 (Trigonometric interpolation) For trigonometric interpolation the high frequencies have
to be replaced by the corresponding negative frequencies to provide meaningful results between the
sampling points. The circles show sampling points which are fitted using only positive frequencies
(full curve) or replacing the unphysical high frequency by its negative counterpart (broken curve).
The squares show the calculated Fourier spectrum
</p>
<p>The interpolating function (N even) is
</p>
<p>f (t)= 1
N
</p>
<p>N
2 &minus;1&sum;
</p>
<p>j=&minus;N2
</p>
<p>f̃ωj e
iωj t even N (7.25)
</p>
<p>f (t)= 1
N
</p>
<p>N&minus;1
2&sum;
</p>
<p>j=&minus;N&minus;12
</p>
<p>f̃ωj e
iωj t odd N. (7.26)
</p>
<p>The maximum frequency is
</p>
<p>ωmax =
2π
</p>
<p>T
</p>
<p>N
</p>
<p>2
(7.27)
</p>
<p>and hence
</p>
<p>fmax =
1
</p>
<p>2π
ωmax =
</p>
<p>N
</p>
<p>2T
= fs
</p>
<p>2
. (7.28)
</p>
<p>This is known as the sampling theorem which states that the sampling frequency fs
must be larger than twice the maximum frequency present in the signal.</p>
<p/>
</div>
<div class="page"><p/>
<p>118 7 Fourier Transformation
</p>
<p>7.2.2 Real Valued Functions
</p>
<p>For a real valued function
</p>
<p>fn = f &lowast;n (7.29)
</p>
<p>and hence
</p>
<p>f̃ &lowast;ωj =
(
N&minus;1&sum;
</p>
<p>n=0
fne
</p>
<p>&minus;iωj tn
</p>
<p>)&lowast;
=
</p>
<p>N&minus;1&sum;
</p>
<p>n=0
fne
</p>
<p>iωj tn = f̃ω&minus;j . (7.30)
</p>
<p>Here it is sufficient to calculate the sums for j = 0 &middot; &middot; &middot;N/2. If the function is real
valued and also even
</p>
<p>f&minus;n = fn (7.31)
</p>
<p>f̃ωj =
N&minus;1&sum;
</p>
<p>n=0
f&minus;ne
</p>
<p>&minus;iωj tn =
N&minus;1&sum;
</p>
<p>n=0
fne
</p>
<p>&minus;i(&minus;ωj )tn = f̃ω&minus;j (7.32)
</p>
<p>and the Fourier sum (7.19) turns into a cosine sum
</p>
<p>fn =
1
</p>
<p>2M &minus; 1 f̃ω0 +
2
</p>
<p>2M &minus; 1
</p>
<p>M&minus;1&sum;
</p>
<p>j=1
f̃ωj cos
</p>
<p>(
2π
</p>
<p>2M &minus; 1jn
)
</p>
<p>odd N = 2M&minus;1 (7.33)
</p>
<p>fn =
1
</p>
<p>2M
f̃ω0 +
</p>
<p>1
</p>
<p>M
</p>
<p>M&minus;1&sum;
</p>
<p>j=1
f̃ωj cos
</p>
<p>(
π
</p>
<p>M
jn
</p>
<p>)
+ 1
</p>
<p>2M
f̃ωM cos(nπ) even N = 2M
</p>
<p>(7.34)
which correspond to two out of eight different versions [268] of the discrete cosine
transformation [2, 211].
</p>
<p>Equation (7.34) can be used to define the interpolating function
</p>
<p>f (t)= 1
2M
</p>
<p>f̃ω0 +
1
</p>
<p>M
</p>
<p>M&minus;1&sum;
</p>
<p>j=1
f̃ωj cos(ωj t)+
</p>
<p>1
</p>
<p>2M
f̃ωM cos
</p>
<p>(
2πM
</p>
<p>T
t
</p>
<p>)
. (7.35)
</p>
<p>The real valued Fourier coefficients are given by
</p>
<p>f̃ωj = f0 + 2
M&minus;1&sum;
</p>
<p>n=1
fn cos(ωj tn) odd N = 2M &minus; 1 (7.36)
</p>
<p>f̃ωj = f0 + 2
M&minus;1&sum;
</p>
<p>n=1
fn cos(ωj tn)+ fM cos(jπ) even N = 2M. (7.37)</p>
<p/>
</div>
<div class="page"><p/>
<p>7.2 Discrete Fourier Transformation 119
</p>
<p>7.2.3 Approximate Continuous Fourier Transformation
</p>
<p>We continue the function f (t) periodically by setting
</p>
<p>fN = f0 (7.38)
</p>
<p>and write
</p>
<p>f̃ωj =
N&minus;1&sum;
</p>
<p>n=0
fne
</p>
<p>&minus;iωjn = 1
2
f0 + e&minus;iωj f1 + &middot; &middot; &middot; + e&minus;iωj (N&minus;1)fN&minus;1 +
</p>
<p>1
</p>
<p>2
fN . (7.39)
</p>
<p>Comparing with the trapezoidal rule (4.13) for the integral
</p>
<p>&int; T
</p>
<p>0
e&minus;iωj tf (t)dt &asymp; T
</p>
<p>N
</p>
<p>(
1
</p>
<p>2
e&minus;iωj 0f (0)+ e&minus;iωj TN f
</p>
<p>(
T
</p>
<p>N
</p>
<p>)
+ &middot; &middot; &middot;
</p>
<p>+ e&minus;iωj TN (N&minus;1)f
(
T
</p>
<p>N
(N &minus; 1)
</p>
<p>)
+ 1
</p>
<p>2
f (T )
</p>
<p>)
(7.40)
</p>
<p>we find
</p>
<p>f̂ (ωj )=
1
</p>
<p>T
</p>
<p>&int; T
</p>
<p>0
e&minus;iωj tf (t)dt &asymp; 1
</p>
<p>N
f̃ωj (7.41)
</p>
<p>which shows that the discrete Fourier transformation is an approximation to the
Fourier series of a periodic function with period T which coincides with f (t) in the
interval 0 &lt; t &lt; T . The range of the integral can be formally extended to &plusmn;&infin; by
introducing a windowing function
</p>
<p>W(t)=
{
</p>
<p>1 for 0 &lt; t &lt; T
</p>
<p>0 else.
(7.42)
</p>
<p>The discrete Fourier transformation approximates the continuous Fourier transfor-
mation but windowing leads to a broadening of the spectrum. For practical purposes
smoother windowing functions are used like a triangular window or one of the fol-
lowing [119]:
</p>
<p>W(tn) = e&minus;
1
2 (
</p>
<p>n&minus;(N&minus;1)/2
σ(N&minus;1)/2 )
</p>
<p>2
σ &le; 0.5 Gaussian window
</p>
<p>W(tn) = 0.53836 &minus; 0.46164 cos
(
</p>
<p>2πn
</p>
<p>N &minus; 1
</p>
<p>)
Hamming window
</p>
<p>W(tn) = 0.5
(
</p>
<p>1 &minus; cos
(
</p>
<p>2πn
</p>
<p>N &minus; 1
</p>
<p>))
Hann(ing) window.</p>
<p/>
</div>
<div class="page"><p/>
<p>120 7 Fourier Transformation
</p>
<p>7.3 Fourier Transform Algorithms
</p>
<p>Straightforward evaluation of the sum
</p>
<p>f̃ωj =
N&minus;1&sum;
</p>
<p>n=0
cos
</p>
<p>(
2π
</p>
<p>N
jn
</p>
<p>)
fn + i sin
</p>
<p>(
2π
</p>
<p>N
jn
</p>
<p>)
fn (7.43)
</p>
<p>needs O(N2) additions, multiplications and trigonometric functions.
</p>
<p>7.3.1 Goertzel&rsquo;s Algorithm
</p>
<p>Goertzel&rsquo;s method [103] is very useful if not the whole Fourier spectrum is needed
but only some of the Fourier components, for instance to demodulate a frequency
shift key signal or the dial tones which are used in telephony.
</p>
<p>The Fourier transform can be written as
</p>
<p>N&minus;1&sum;
</p>
<p>n=0
fne
</p>
<p>&minus;i 2π
N
jn = f0 + e&minus;
</p>
<p>2π i
N
</p>
<p>j
(
f1 + e&minus;
</p>
<p>2π i
N
</p>
<p>jf2 &middot; &middot; &middot;
(
fN&minus;2 + e&minus;
</p>
<p>2π i
N
</p>
<p>jfN&minus;1
)
&middot; &middot; &middot;
</p>
<p>)
(7.44)
</p>
<p>which can be evaluated recursively
</p>
<p>yN&minus;1 = fN&minus;1
</p>
<p>yn = fn + e&minus;
2π i
N
</p>
<p>jyn+1 n=N &minus; 2 &middot; &middot; &middot;0
(7.45)
</p>
<p>to give the result
</p>
<p>f̂ωj = y0. (7.46)
Equation (7.45) is a simple discrete filter function. Its transmission function is ob-
tained by application of the z-transform [144]
</p>
<p>u(z)=
&infin;&sum;
</p>
<p>n=0
unz
</p>
<p>&minus;n (7.47)
</p>
<p>(the discrete version of the Laplace transform) which yields
</p>
<p>y(z)= f (z)
1 &minus; ze&minus; 2π iN j
</p>
<p>. (7.48)
</p>
<p>One disadvantage of this method is that it uses complex numbers. This can be
avoided by the following more complicated recursion
</p>
<p>uN+1 = uN = 0
un = fn + 2un+1 cos
</p>
<p>2π
</p>
<p>N
k &minus; un+2 for n=N &minus; 1 &middot; &middot; &middot;0
</p>
<p>(7.49)</p>
<p/>
</div>
<div class="page"><p/>
<p>7.3 Fourier Transform Algorithms 121
</p>
<p>with the transmission function
</p>
<p>u(z)
</p>
<p>f (z)
= 1
</p>
<p>1 &minus; z(e 2π iN j + e&minus; 2π iN j )+ z2
</p>
<p>= 1
(1 &minus; ze&minus; 2π iN j )(1 &minus; ze 2π iN j )
</p>
<p>. (7.50)
</p>
<p>A second filter removes one factor in the denominator
</p>
<p>y(z)
</p>
<p>u(z)
=
(
1 &minus; ze 2π iN j
</p>
<p>)
(7.51)
</p>
<p>which in the time domain corresponds to the simple expression
</p>
<p>yn = un &minus; e
2π i
N
</p>
<p>jun+1.
</p>
<p>The overall filter function finally again is (7.48).
</p>
<p>y(z)
</p>
<p>f (z)
= 1
</p>
<p>1 &minus; ze&minus; 2π iN j
. (7.52)
</p>
<p>Hence the Fourier component of f is given by
</p>
<p>f̂ωj = y0 = u0 &minus; e
2π i
N
</p>
<p>ju1. (7.53)
</p>
<p>The order of the iteration (7.44) can be reversed by writing
</p>
<p>f̂ωj = f0 &middot; &middot; &middot; e
2π i
N
</p>
<p>(N&minus;1)fN&minus;1 = e&minus;
2π i
N
</p>
<p>j (N&minus;1)(f0e
2π i
N
</p>
<p>j (N&minus;1) &middot; &middot; &middot;fN&minus;1
)
</p>
<p>(7.54)
</p>
<p>which is very useful for real time filter applications.
</p>
<p>7.3.2 Fast Fourier Transformation
</p>
<p>If the number of samples is N = 2p , the Fourier transformation can be performed
very efficiently by this method.2 The phase factor
</p>
<p>e&minus;i
2π
N
jm =W jmN (7.55)
</p>
<p>can take only N different values. The number of trigonometric functions can be
reduced by reordering the sum. Starting from a sum with N samples
</p>
<p>FN (f0 &middot; &middot; &middot;fN&minus;1)=
N&minus;1&sum;
</p>
<p>n=0
fnW
</p>
<p>jn
N (7.56)
</p>
<p>2There exist several Fast Fourier Transformation algorithms [74, 187]. We consider only the sim-
plest one here [62].</p>
<p/>
</div>
<div class="page"><p/>
<p>122 7 Fourier Transformation
</p>
<p>we separate even and odd powers of the unit root
</p>
<p>FN (f0 &middot; &middot; &middot;fN&minus;1) =
N
2 &minus;1&sum;
</p>
<p>m=0
f2mW
</p>
<p>j2m
N +
</p>
<p>N
2 &minus;1&sum;
</p>
<p>m=0
f2m+1W
</p>
<p>j (2m+1)
N
</p>
<p>=
N
2 &minus;1&sum;
</p>
<p>m=0
f2me
</p>
<p>&minus;i 2π
N/2 jm +W jN
</p>
<p>N
2 &minus;1&sum;
</p>
<p>m=0
f2m+1e
</p>
<p>&minus;i 2π
N/2 jm
</p>
<p>= FN/2(f0, f2 &middot; &middot; &middot;fN&minus;2)+W jNFN/2(f1, f3 &middot; &middot; &middot;fN&minus;1).
(7.57)
</p>
<p>This division is repeated until only sums with one summand remain
</p>
<p>F1(fn)= fn. (7.58)
</p>
<p>For example, consider the case N = 8:
</p>
<p>F8(f0 &middot; &middot; &middot;f7)= F4(f0f2f4f6)+W j8 F4(f1f3f5f7)
&minus;&minus;&minus;
</p>
<p>F4(f0f2f4f6)= F2(f0f4)+W j4 F2(f2f6)
F4(f1f3f5f7)= F2(f1f5)+W j4 F2(f3f7)
</p>
<p>&minus;&minus;&minus;
F2(f0f4)= f0 +W j2 f4
F2(f2f6)= f2 +W j2 f6
F2(f1f5)= f1 +W j2 f5
F2(f3f7)= f3 +W j2 f7.
</p>
<p>Expansion gives
</p>
<p>F8 = f0 +W j2 f4 +W
j
</p>
<p>4 f2 +W
j
</p>
<p>4 W
j
</p>
<p>2 f6
</p>
<p>+W j8 f1 +W
j
</p>
<p>8 W
j
</p>
<p>2 f5 +W
j
</p>
<p>8 W
j
</p>
<p>4 f3 +W
j
</p>
<p>8 W
j
</p>
<p>4 W
j
</p>
<p>2 f7. (7.59)
</p>
<p>Generally a summand of the Fourier sum can be written using the binary represen-
tation of n
</p>
<p>n=
&sum;
</p>
<p>li li = 1,2,4,8 &middot; &middot; &middot; (7.60)
</p>
<p>in the following way:
</p>
<p>fne
&minus;i 2π
</p>
<p>N
jn = fne&minus;i
</p>
<p>2π
N
(l1+l2+&middot;&middot;&middot; )j = fnW jN/l1W
</p>
<p>j
N/l2
</p>
<p>&middot; &middot; &middot; . (7.61)</p>
<p/>
</div>
<div class="page"><p/>
<p>7.3 Fourier Transform Algorithms 123
</p>
<p>The function values are reordered according to the following algorithm
</p>
<p>(i) count from 0 to N &minus; 1 using binary numbers m= 000,001,010, &middot; &middot; &middot;
(ii) bit reversal gives the binary numbers n= 000,100,010, &middot; &middot; &middot;
</p>
<p>(iii) store fn at the position m. This will be denoted as sm = fn
</p>
<p>As an example for N = 8 the function values are in the order
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>s0
s1
s2
s3
s4
s5
s6
s7
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>=
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>f0
f4
f2
f6
f1
f5
f3
f7
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>. (7.62)
</p>
<p>Now calculate sums with two summands. Since W j2 can take only two different
values
</p>
<p>W
j
</p>
<p>2 =
{
</p>
<p>1 for j = 0,2,4,6
&minus;1 for j = 1,3,5,7 (7.63)
</p>
<p>a total of 8 sums have to be calculated which can be stored again in the same
workspace:
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>f0 + f4
f0 &minus; f4
f2 + f6
f2 &minus; f6
f1 + f5
f1 &minus; f5
f3 + f7
f3 &minus; f7
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>=
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>s0 +W 02 s1
s0 +W 12 s1
s2 +W 22 s3
s2 +W 32 s3
s4 +W 42 s5
s4 +W 52 s5
s6 +W 62 s7
s6 +W 72 s7
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>. (7.64)
</p>
<p>Next calculate sums with four summands. W j4 can take one of four values
</p>
<p>W
j
</p>
<p>4 =
</p>
<p>⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
</p>
<p>1 for j = 0,4
&minus;1 for j = 2,6
W4 for j = 1,5
&minus;W4 for j = 3,7.
</p>
<p>(7.65)</p>
<p/>
</div>
<div class="page"><p/>
<p>124 7 Fourier Transformation
</p>
<p>The following combinations are needed:
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>f0 + f4 + (f2 + f6)
f0 + f4 &minus; (f2 + f6)
</p>
<p>(f0 &minus; f4)+W4(f2 &minus; f6)
(f0 &minus; f4)&minus;W4(f2 &minus; f6)
</p>
<p>f1 + f5 + (f3 + f7)
f1 + f5 &minus; (f3 + f7)
</p>
<p>(f1 &minus; f5)&plusmn;W4(f3 &minus; f7)
(f1 &minus; f5)&plusmn;W4(f3 &minus; f7)
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>=
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>s0 +W 04 s2
s1 +W 14 s3
s0 +W 24 s2
s1 +W 34 s3
s4 +W 44 s6
s5 +W 54 s7
s4 +W 64 s6
s5 +W 74 s7
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>. (7.66)
</p>
<p>The next step gives the sums with eight summands. With
</p>
<p>W
j
</p>
<p>8 =
</p>
<p>⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
</p>
<p>1 j = 0
W8 j = 1
W 28 j = 2
W 38 j = 3
&minus;1 j = 4
&minus;W8 j = 5
&minus;W 28 j = 6
&minus;W 38 j = 7
</p>
<p>(7.67)
</p>
<p>we calculate
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>f0 + f4 + (f2 + f6)+ (f1 + f5 + (f3 + f7))
f0 + f4 &minus; (f2 + f6)+W8(f1 + f5 &minus; (f3 + f7))
</p>
<p>(f0 &minus; f4)+W4(f2 &minus; f6)+W 28 (f1 &minus; f5)&plusmn;W4(f3 &minus; f7)
(f0 &minus; f4)&minus;W4(f2 &minus; f6)+W 38 ((f1 &minus; f5)&plusmn;W4(f3 &minus; f7))
</p>
<p>f0 + f4 + (f2 + f6)&minus; (f1 + f5 + (f3 + f7))
f0 + f4 &minus; (f2 + f6)&minus;W8(f1 + f5 &minus; (f3 + f7))
</p>
<p>(f0 &minus; f4)+W4(f2 &minus; f6)&minus;W 28 ((f1 &minus; f5)&plusmn;W4(f3 &minus; f7))
(f0 &minus; f4)&minus;W4(f2 &minus; f6)&minus;W 38 ((f1 &minus; f5)&plusmn;W4(f3 &minus; f7))
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>=
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>s0 +W 08 s4
s1 +W 18 s5
s2 +W 28 s6
s3 +W 38 s7
s0 +W 48 s4
s1 +W 58 s5
s2 +W 68 s6
s3 +W 78 s7
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>(7.68)
which is the final result.
</p>
<p>The following shows a simple Fast Fourier Transformation algorithm. The num-
ber of trigonometric function evaluations can be reduced but this reduces the read-
ability. At the beginning Data[k] are the input data in bit reversed order.
</p>
<p>size := 2
first := 0
While first &lt; Number_of_Samples do begin</p>
<p/>
</div>
<div class="page"><p/>
<p>7.4 Problems 125
</p>
<p>for n := 0 to size/2 &minus; 1 do begin
j := first + n
k := j + size/2 &minus; 1
T := exp(&minus;2&lowast;P i&lowast;i&lowast;n/Number_of_Samples)&lowast;Data[k]
Data[j ] := Data[j ] + T
Data[k] := Data[k] &minus; T
</p>
<p>end;
first := first&lowast;2
size := size&lowast;2
</p>
<p>end;
</p>
<p>7.4 Problems
</p>
<p>Problem 7.1 (Discrete Fourier transformation) In this computer experiment for a
given set of input samples
</p>
<p>fn = f
(
n
T
</p>
<p>N
</p>
<p>)
n= 0 &middot; &middot; &middot;N &minus; 1 (7.69)
</p>
<p>&bull; the Fourier coefficients
</p>
<p>f̃ωj =
N&minus;1&sum;
</p>
<p>n=0
fne
</p>
<p>&minus;iωj tn ωj =
2π
</p>
<p>T
j, j = 0 &middot; &middot; &middot;N &minus; 1 (7.70)
</p>
<p>are calculated with Goertzel&rsquo;s method (7.3.1).
&bull; The results from the inverse transformation
</p>
<p>fn =
1
</p>
<p>N
</p>
<p>N&minus;1&sum;
</p>
<p>j=0
f̃ωj e
</p>
<p>i 2π
N
nj (7.71)
</p>
<p>are compared with the original function values f (tn).
&bull; The Fourier sum is used for trigonometric interpolation with only positive fre-
</p>
<p>quencies
</p>
<p>f (t)= 1
N
</p>
<p>N&minus;1&sum;
</p>
<p>j=0
f̃ωj
</p>
<p>(
ei
</p>
<p>2π
T
t
)j
. (7.72)
</p>
<p>&bull; Finally the unphysical high frequencies are replaced by negative frequencies
(7.24). The results can be studied for several kinds of input data.
</p>
<p>Problem 7.2 (Noise filter) This computer experiment demonstrates a nonlinear fil-
ter.
</p>
<p>First a noisy input signal is generated.</p>
<p/>
</div>
<div class="page"><p/>
<p>126 7 Fourier Transformation
</p>
<p>The signal can be chosen as
</p>
<p>&bull; monochromatic sin(ωt)
&bull; the sum of two monochromatic signals a1 sinω1t + a2 sinω2t
&bull; a rectangular signal with many harmonic frequencies sign(sinωt)
Different kinds of white noise can be added
</p>
<p>&bull; dichotomous &plusmn;1
&bull; constant probability density in the range [&minus;1,1]
&bull; Gaussian probability density
The amplitudes of signal and noise can be varied. All Fourier components are re-
moved which are below a threshold value and the filtered signal is calculated by
inverse Fourier transformation.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 8
</p>
<p>Random Numbers and Monte Carlo Methods
</p>
<p>Many-body problems often involve the calculation of integrals of very high dimen-
sion which cannot be treated by standard methods. For the calculation of thermo-
dynamic averages Monte Carlo methods [49, 85, 174, 220] are very useful which
sample the integration volume at randomly chosen points. In this chapter we dis-
cuss algorithms for the generation of pseudo-random numbers with given probabil-
ity distribution which are essential for all Monte Carlo methods. We show how the
efficiency of Monte Carlo integration can be improved by sampling preferentially
the important configurations. Finally the famous Metropolis algorithm is applied to
classical many-particle systems and nonlinear optimization problems.
</p>
<p>8.1 Some Basic Statistics
</p>
<p>In the following we discuss some important concepts which are used to analyze
experimental data sets [218]. Repeated measurements of some observable usually
give slightly different results due to fluctuations of the observable in time and errors
of the measurement process. The distribution of the measured data is described by a
probability distribution, which in many cases approximates a simple mathematical
form like the Gaussian normal distribution. The moments of the probability density
give important information about the statistical properties, especially the mean and
the standard deviation of the distribution. If the errors of different measurements
are uncorrelated, the average value of a larger number of measurements is a good
approximation to the &ldquo;exact&rdquo; value.
</p>
<p>8.1.1 Probability Density and Cumulative Probability Distribution
</p>
<p>Consider an observable ξ , which is measured in a real or a computer experiment.
Repeated measurements give a statistical distribution of values.
</p>
<p>P.O.J. Scherer, Computational Physics, Graduate Texts in Physics,
DOI 10.1007/978-3-319-00401-3_8,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>127</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-00401-3_8">http://dx.doi.org/10.1007/978-3-319-00401-3_8</a></div>
</div>
<div class="page"><p/>
<p>128 8 Random Numbers and Monte Carlo Methods
</p>
<p>Fig. 8.1 (Cumulative
probability distribution of
transition energies) The
figure shows schematically
the distribution of transition
energies for an atom which
has a discrete and a
continuous part
</p>
<p>The cumulative probability distribution (Fig. 8.1) is given by the function
</p>
<p>F(x)= P {ξ &le; x} (8.1)
and has the following properties:
</p>
<p>&bull; F(x) is monotonously increasing
&bull; F(&minus;&infin;)= 0, F(&infin;)= 1
&bull; F(x) can be discontinuous (if there are discrete values of ξ )
The probability to measure a value in the interval x1 &lt; ξ &le; x2 is
</p>
<p>P(x1 &lt; ξ &le; x2)= F(x2)&minus; F(x1). (8.2)
The height of a jump gives the probability of a discrete value
</p>
<p>P(ξ = x0)= F(x0 + 0)&minus; F(x0 &minus; 0). (8.3)
In regions where F(x) is continuous, the probability density can be defined as
</p>
<p>f (x0)= F &prime;(x0)= lim
�x&rarr;0
</p>
<p>1
</p>
<p>�x
P(x0 &lt; ξ &le; x0 +�x). (8.4)
</p>
<p>8.1.2 Histogram
</p>
<p>From an experiment F(x) cannot be determined directly. Instead a finite number N
of values xi are measured. By
</p>
<p>ZN (x)
</p>
<p>we denote the number of measurements with xi &le; x. The cumulative probability
distribution is the limit
</p>
<p>F(x)= lim
N&rarr;&infin;
</p>
<p>1
</p>
<p>N
ZN (x). (8.5)
</p>
<p>A histogram (Fig. 8.2) counts the number of measured values which are in the in-
terval xi &lt; x &le; xi+1:</p>
<p/>
</div>
<div class="page"><p/>
<p>8.1 Some Basic Statistics 129
</p>
<p>Fig. 8.2 (Histogram) The
cumulative distribution of 100
Gaussian random numbers is
shown together with a
histogram with bin width
�x = 0.6
</p>
<p>1
</p>
<p>N
</p>
<p>(
ZN (xi+1)&minus;ZN (xi)
</p>
<p>)
&asymp; F(xi+1)&minus; F(xi)= P(xi &lt; ξ &le; xi+1). (8.6)
</p>
<p>Contrary to ZN (x) itself, the histogram depends on the choice of the intervals.
</p>
<p>8.1.3 Expectation Values and Moments
</p>
<p>The expectation value of the random variable ξ is defined by
</p>
<p>E[ξ ] =
&int; &infin;
</p>
<p>&minus;&infin;
x dF(x)= lim
</p>
<p>a&rarr;&minus;&infin;,b&rarr;&infin;
</p>
<p>&int; b
</p>
<p>a
</p>
<p>x dF(x) (8.7)
</p>
<p>with the Riemann-Stieltjes integral [202]
</p>
<p>&int; b
</p>
<p>a
</p>
<p>x dF(x)= lim
N&rarr;&infin;
</p>
<p>N&sum;
</p>
<p>i=1
xi
(
F(xi)&minus; F(xi&minus;1)
</p>
<p>)∣∣
xi=a+ b&minus;aN i
</p>
<p>. (8.8)
</p>
<p>Higher moments are defined as
</p>
<p>E
[
ξ k
]
=
&int; &infin;
</p>
<p>&minus;&infin;
xk dF(x) (8.9)
</p>
<p>if these integrals exist. Most important are the expectation value
</p>
<p>x =E[ξ ] (8.10)
and the variance, which results from the first two moments
</p>
<p>σ 2 =
&int; &infin;
</p>
<p>&minus;&infin;
(x &minus; x)2 dF =
</p>
<p>&int;
x2 dF +
</p>
<p>&int;
x2 dF &minus; 2x
</p>
<p>&int;
x dF
</p>
<p>=E
[
ξ2
]
&minus;
(
E[ξ ]
</p>
<p>)2
. (8.11)</p>
<p/>
</div>
<div class="page"><p/>
<p>130 8 Random Numbers and Monte Carlo Methods
</p>
<p>Fig. 8.3 Cumulative
probability distribution of a
fair die
</p>
<p>The standard deviation σ is a measure of the width of the distribution. The expecta-
tion value of a function ϕ(x) is defined by
</p>
<p>E
[
ϕ(x)
</p>
<p>]
=
&int; &infin;
</p>
<p>&minus;&infin;
ϕ(x)dF (x). (8.12)
</p>
<p>For continuous F(x) we have with dF(x)= f (x)dx the ordinary integral
</p>
<p>E
[
ξ k
]
=
&int; &infin;
</p>
<p>&minus;&infin;
xkf (x)dx (8.13)
</p>
<p>E
[
ϕ(x)
</p>
<p>]
=
&int; &infin;
</p>
<p>&minus;&infin;
ϕ(x)f (x) dx (8.14)
</p>
<p>whereas for a pure step function F(x) (only discrete values xi are observed with
probabilities p(xi)= F(xi + 0)&minus; F(xi &minus; 0))
</p>
<p>E
[
ξ k
]
=
&sum;
</p>
<p>xki p(xi) (8.15)
</p>
<p>E
[
ϕ(x)
</p>
<p>]
=
&sum;
</p>
<p>ϕ(xi)p(xi). (8.16)
</p>
<p>8.1.4 Example: Fair Die
</p>
<p>When a six-sided fair die is rolled, each of its sides shows up with the same prob-
ability of 1/6. The cumulative probability distribution F(x) is a pure step function
(Fig. 8.3) and
</p>
<p>x =
&int; &infin;
</p>
<p>&minus;&infin;
x dF =
</p>
<p>6&sum;
</p>
<p>i=1
xi
(
F(xi + 0)&minus; F(xi &minus; 0)
</p>
<p>)
= 1
</p>
<p>6
</p>
<p>6&sum;
</p>
<p>i=1
xi =
</p>
<p>21
</p>
<p>6
= 3.5
</p>
<p>(8.17)
</p>
<p>x2 =
6&sum;
</p>
<p>i=1
x2i
(
F(xi + 0)&minus; F(xi &minus; 0)
</p>
<p>)
= 1
</p>
<p>6
</p>
<p>6&sum;
</p>
<p>i=1
x2i =
</p>
<p>91
</p>
<p>6
= 15.1666 &middot; &middot; &middot; (8.18)
</p>
<p>σ =
&radic;
x2 &minus; x2 = 2.9. (8.19)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.1 Some Basic Statistics 131
</p>
<p>8.1.5 Normal Distribution
</p>
<p>The Gaussian normal distribution is defined by the cumulative probability distribu-
tion
</p>
<p>Φ(x)= 1&radic;
2π
</p>
<p>&int; x
</p>
<p>&minus;&infin;
e&minus;t
</p>
<p>2/2 dt (8.20)
</p>
<p>and the probability density
</p>
<p>ϕ(x)= 1&radic;
2π
</p>
<p>e&minus;x
2/2 (8.21)
</p>
<p>with the properties
&int; &infin;
</p>
<p>&minus;&infin;
ϕ(x)dx =Φ(&infin;)= 1 (8.22)
</p>
<p>x =
&int; &infin;
</p>
<p>&minus;&infin;
xϕ(x)dx = 0 (8.23)
</p>
<p>σ 2 = x2 =
&int; &infin;
</p>
<p>&minus;&infin;
x2ϕ(x)dx = 1. (8.24)
</p>
<p>Since Φ(0)= 12 and with the definition
</p>
<p>Φ0(x)=
1&radic;
2π
</p>
<p>&int; x
</p>
<p>0
e&minus;t
</p>
<p>2/2 dt (8.25)
</p>
<p>we have
</p>
<p>Φ(x)= 1
2
+Φ0(x) (8.26)
</p>
<p>which can be expressed in terms of the error function1
</p>
<p>erf(x)= 2&radic;
π
</p>
<p>&int; x
</p>
<p>0
e&minus;t
</p>
<p>2dt dt = 2Φ0(
&radic;
</p>
<p>2x) (8.27)
</p>
<p>as
</p>
<p>Φ0(x)=
1
</p>
<p>2
erf
</p>
<p>(
x&radic;
2
</p>
<p>)
. (8.28)
</p>
<p>A general Gaussian distribution with mean value x and standard deviation σ has the
probability distribution
</p>
<p>ϕx,σ =
1
</p>
<p>σ
&radic;
</p>
<p>2π
exp
</p>
<p>(
&minus; (x
</p>
<p>&prime; &minus; x)2
2σ 2
</p>
<p>)
(8.29)
</p>
<p>and the cumulative distribution
</p>
<p>1erf(x) is an intrinsic function in FORTRAN or C.</p>
<p/>
</div>
<div class="page"><p/>
<p>132 8 Random Numbers and Monte Carlo Methods
</p>
<p>Φx,σ (x)=Φ
(
x &minus; x
σ
</p>
<p>)
=
&int; x
</p>
<p>&minus;&infin;
dx&prime;
</p>
<p>1
</p>
<p>σ
&radic;
</p>
<p>2π
exp
</p>
<p>(
&minus; (x
</p>
<p>&prime; &minus; x)2
2σ 2
</p>
<p>)
(8.30)
</p>
<p>= 1
2
</p>
<p>(
1 + erf
</p>
<p>(
x &minus; x
σ
&radic;
</p>
<p>2
</p>
<p>))
. (8.31)
</p>
<p>8.1.6 Multivariate Distributions
</p>
<p>Consider now two quantities which are measured simultaneously. ξ and η are the
corresponding random variables. The cumulative distribution function is
</p>
<p>F(x, y)= P(ξ &le; x and η &le; y). (8.32)
</p>
<p>Expectation values are defined as
</p>
<p>E
[
ϕ(x, y)
</p>
<p>]
=
&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
ϕ(x, y) d2F(x, y). (8.33)
</p>
<p>For continuous F(x, y) the probability density is
</p>
<p>f (x, y)= &part;
2F
</p>
<p>&part;x&part;y
(8.34)
</p>
<p>and the expectation value is simply
</p>
<p>E
[
ϕ(x, y)
</p>
<p>] &int; &infin;
</p>
<p>&minus;&infin;
dx
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
dy ϕ(x, y)f (x, y). (8.35)
</p>
<p>The moments of the distribution are the expectation values
</p>
<p>Mk,l =E
[
ξ kηl
</p>
<p>]
. (8.36)
</p>
<p>Most important are the averages
</p>
<p>x =E[ξ ] y =E[η] (8.37)
</p>
<p>and the covariance matrix
(
</p>
<p>E[(ξ &minus; x)2] E[(ξ &minus; x)(η&minus; y)]
E[(ξ &minus; x)(η&minus; y)] E[(η&minus; y)2]
</p>
<p>)
=
(
</p>
<p>x2 &minus; x2 xy &minus; x y
xy &minus; x y y2 &minus; y2
</p>
<p>)
.
</p>
<p>(8.38)
</p>
<p>The correlation coefficient is defined as
</p>
<p>ρ = xy &minus; x y&radic;
(x2 &minus; x2)(y2 &minus; y2)
</p>
<p>. (8.39)
</p>
<p>If there is no correlation then ρ = 0 and F(x, y)= F1(x)F2(y).</p>
<p/>
</div>
<div class="page"><p/>
<p>8.1 Some Basic Statistics 133
</p>
<p>Fig. 8.4 (Central limit
theorem) The cumulative
distribution function of η
(8.42) is shown for N = 4 and
compared to the normal
distribution (8.20)
</p>
<p>8.1.7 Central Limit Theorem
</p>
<p>Consider N independent random variables ξi with the same cumulative distribution
function F(x), for which E[ξ ] = 0 and E[ξ2] = 1. Define a new random variable
</p>
<p>ηN =
ξ1 + ξ2 + &middot; &middot; &middot; + ξN&radic;
</p>
<p>N
(8.40)
</p>
<p>with the cumulative distribution function FN (x). In the limit N &rarr;&infin; this distribu-
tion approaches (Fig. 8.4) a cumulative normal distribution [215]
</p>
<p>lim
N&rarr;&infin;
</p>
<p>FN (x)=Φ(x)=
1&radic;
2π
</p>
<p>&int; x
</p>
<p>&minus;&infin;
e&minus;t
</p>
<p>2/2 dt. (8.41)
</p>
<p>8.1.8 Example: Binomial Distribution
</p>
<p>Toss a coin N times giving ξi = 1 (heads) or ξi =&minus;1 (tails) with equal probability
P = 12 . Then E[ξi] = 0 and E[ξ2i ] = 1. The distribution of
</p>
<p>η= 1&radic;
N
</p>
<p>N&sum;
</p>
<p>i=1
ξi (8.42)
</p>
<p>can be derived from the binomial distribution
</p>
<p>1 =
[
</p>
<p>1
</p>
<p>2
+
(
&minus;1
</p>
<p>2
</p>
<p>)]N
= 2&minus;N
</p>
<p>N&sum;
</p>
<p>p=0
(&minus;1)N&minus;p
</p>
<p>(
N
</p>
<p>N &minus; p
</p>
<p>)
(8.43)
</p>
<p>where p counts the number of tosses with ξ =+1. Since
</p>
<p>n= p &middot; 1 + (N &minus; p) &middot; (&minus;1)= 2p&minus;N &isin; [&minus;N,N ] (8.44)</p>
<p/>
</div>
<div class="page"><p/>
<p>134 8 Random Numbers and Monte Carlo Methods
</p>
<p>the probability of finding η= n&radic;
N
</p>
<p>is given by the binomial coefficient
</p>
<p>P
</p>
<p>(
η= 2p&minus;N&radic;
</p>
<p>N
</p>
<p>)
= 2&minus;N
</p>
<p>(
N
</p>
<p>N &minus; p
</p>
<p>)
(8.45)
</p>
<p>or
</p>
<p>P
</p>
<p>(
η= n&radic;
</p>
<p>N
</p>
<p>)
= 2&minus;N
</p>
<p>(
N
</p>
<p>N&minus;n
2
</p>
<p>)
. (8.46)
</p>
<p>8.1.9 Average of Repeated Measurements
</p>
<p>A quantity X is measured N times. The results X1 &middot; &middot; &middot;XN are independent random
numbers with the same distribution function f (Xi). Their expectation value is the
exact value E[Xi] =
</p>
<p>&int;
dXi Xif (Xi) = X and the standard deviation due to mea-
</p>
<p>surement uncertainties is σX =
&radic;
E[X2i ] &minus;X2. The new random variables
</p>
<p>ξi =
Xi &minus;X
σX
</p>
<p>(8.47)
</p>
<p>have zero mean
</p>
<p>E[ξi] =
E[Xi] &minus;X
</p>
<p>σX
= 0 (8.48)
</p>
<p>and unit standard deviation
</p>
<p>σ 2ξ =E
[
ξ2i
]
&minus;E[ξi]2 =E
</p>
<p>[
X2i +X2 &minus; 2XXi
</p>
<p>σ 2X
</p>
<p>]
=
</p>
<p>E[X2i ] &minus;X2
</p>
<p>σ 2X
</p>
<p>= 1. (8.49)
</p>
<p>Hence the quantity
</p>
<p>η=
&sum;N
</p>
<p>1 ξi&radic;
N
</p>
<p>=
&sum;N
</p>
<p>1 Xi &minus;NX&radic;
NσX
</p>
<p>=
&radic;
N
</p>
<p>σX
(X&minus;X) (8.50)
</p>
<p>obeys a normal distribution
</p>
<p>f (η)= 1&radic;
2π
</p>
<p>e&minus;η
2/2. (8.51)
</p>
<p>From
</p>
<p>f (X)dX = f (η)dη= f
(
η(X)
</p>
<p>)&radic;N
σX
</p>
<p>dX (8.52)
</p>
<p>we obtain
</p>
<p>f (X)=
&radic;
N&radic;
</p>
<p>2πσX
exp
</p>
<p>{
&minus; N
</p>
<p>2σ 2X
(X&minus;X)2
</p>
<p>}
. (8.53)
</p>
<p>The average of N measurements obeys a Gaussian distribution around the exact
value X with a reduced standard deviation of
</p>
<p>σX =
σX&radic;
N
. (8.54)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.2 Random Numbers 135
</p>
<p>8.2 Random Numbers
</p>
<p>True random numbers of high quality can be generated using physical effects like
thermal noise in a diode or from a light source. Special algorithms are available
to generate pseudo random numbers which have comparable statistical properties
but are not unpredictable since they depend on some initial seed values. Often an
iteration
</p>
<p>ri+1 = f (ri) (8.55)
is used to calculate a series of pseudo random numbers. Using 32 Bit integers there
are 232 different numbers, hence the period cannot exceed 232.
</p>
<p>8.2.1 Linear Congruent Mapping
</p>
<p>A simple algorithm is the linear congruent mapping
</p>
<p>ri+1 = (ari + c) mod m (8.56)
with maximum period m. Larger periods can be achieved if the random number
depends on several predecessors. A function of the type
</p>
<p>ri = f (ri&minus;1, ri&minus;2 &middot; &middot; &middot; ri&minus;t ) (8.57)
using 32 Bit integers has a maximum period of 232t .
</p>
<p>Example For t = 2 and generating 106 numbers per second the period is
584942 years.
</p>
<p>8.2.2 Marsaglia-Zamann Method
</p>
<p>A high quality random number generator can be obtained from the combination of
two generators [168]. The first one
</p>
<p>ri = (ri&minus;2 &minus; ri&minus;3 &minus; c) mod
(
232 &minus; 18
</p>
<p>)
(8.58)
</p>
<p>with
</p>
<p>c=
{
</p>
<p>1 for rn&minus;2 &minus; rn&minus;3 &lt; 0
0 else
</p>
<p>(8.59)
</p>
<p>has a period of 295. The second one
</p>
<p>ri = (69069ri&minus;1 + 1013904243) mod 232 (8.60)
has a period of 232. The period of the combination is 2127. Here is a short subroutine
in C:</p>
<p/>
</div>
<div class="page"><p/>
<p>136 8 Random Numbers and Monte Carlo Methods
</p>
<p>#define N 100000
typedef unsigned long int unlong /* 32 Bit */
unlong x=521288629, y=362436069, z=16163801, c=1, n=1131199209;
unlong mzran()
{ unlong s;
</p>
<p>if (y&gt;x+c) {s=y-(x+c)-18; c=0;}
else {s=y-(x+c)-18;c=1;}
x=y; y=z; z=s; n=69069*n+1013904243;
return(z+n);
</p>
<p>}
</p>
<p>8.2.3 Random Numbers with Given Distribution
</p>
<p>Assume we have a program that generates random numbers in the interval [0,1] like
in C:
</p>
<p>rand()/(double)RAND_MAX.
</p>
<p>The corresponding cumulative distribution function is
</p>
<p>F0(x)=
</p>
<p>⎧
⎨
⎩
</p>
<p>0 for x &lt; 0
x for 0 &le; x &le; 1
1 for x &gt; 1.
</p>
<p>(8.61)
</p>
<p>Random numbers with cumulative distribution F(x) can be obtained as follows:
</p>
<p>choose an RN r &isin; [0,1] with P(r &le; x)= F0(x)
let ξ = F&minus;1(r)
</p>
<p>F (x) increases monotonously and therefore
</p>
<p>P(ξ &le; x)= P
(
F(ξ)&le; F(x)
</p>
<p>)
= P
</p>
<p>(
r &le; F(x)
</p>
<p>)
= F0
</p>
<p>(
F(x)
</p>
<p>)
(8.62)
</p>
<p>but since 0 &le; F(x)&le; 1 we have
</p>
<p>P(ξ &le; x)= F(x). (8.63)
</p>
<p>This method of course is applicable only if F&minus;1 can be expressed analytically.
</p>
<p>8.2.4 Examples
</p>
<p>8.2.4.1 Fair Die
</p>
<p>A six-sided fair die can be simulated as follows:</p>
<p/>
</div>
<div class="page"><p/>
<p>8.2 Random Numbers 137
</p>
<p>choose a random number r &isin; [0,1]
</p>
<p>Let ξ = F&minus;1(r)=
</p>
<p>⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎩
</p>
<p>1 for 0 &le; r &lt; 16
2 for 16 &le; r &lt;
</p>
<p>2
6
</p>
<p>3 for 26 &le; r &lt;
3
6
</p>
<p>4 for 36 &le; r &lt;
4
6
</p>
<p>5 for 46 &le; r &lt;
5
6
</p>
<p>6 for 56 &le; r &lt; 1.
</p>
<p>8.2.4.2 Exponential Distribution
</p>
<p>The cumulative distribution function
</p>
<p>F(x)= 1 &minus; e&minus;x/λ (8.64)
which corresponds to the exponential probability density
</p>
<p>f (x)= 1
λ
</p>
<p>e&minus;x/λ (8.65)
</p>
<p>can be inverted by solving
</p>
<p>r = 1 &minus; e&minus;x/λ (8.66)
for x:
</p>
<p>choose a random number r &isin; [0,1]
let x = F&minus;1(r)=&minus;λ ln(1 &minus; r).
</p>
<p>8.2.4.3 Random Points on the Unit Sphere
</p>
<p>We consider the surface element
1
</p>
<p>4π
R2 dϕ sin θ dθ. (8.67)
</p>
<p>Our aim is to generate points on the unit sphere (θ,ϕ) with the probability density
</p>
<p>f (θ,ϕ)dϕ dθ = 1
4π
</p>
<p>dϕ sin θ dθ =&minus; 1
4π
</p>
<p>dϕd cos θ. (8.68)
</p>
<p>The corresponding cumulative distribution is
</p>
<p>F(θ,ϕ)=&minus; 1
4π
</p>
<p>&int; cos θ
</p>
<p>1
d cos θ
</p>
<p>&int; ϕ
</p>
<p>0
dϕ = ϕ
</p>
<p>2π
</p>
<p>1 &minus; cos θ
2
</p>
<p>= FϕFθ . (8.69)
</p>
<p>Since this factorizes, the two angles can be determined independently:
</p>
<p>choose a first random number r1 &isin; [0,1]
let ϕ = F&minus;1ϕ (r1)= 2πr1
choose a second random number r2 &isin; [0,1]
let θ = F&minus;1θ (r2)= arccos(1 &minus; 2r2).</p>
<p/>
</div>
<div class="page"><p/>
<p>138 8 Random Numbers and Monte Carlo Methods
</p>
<p>8.2.4.4 Gaussian Distribution (Box Muller)
</p>
<p>For a Gaussian distribution the inverse F&minus;1 has no simple analytical form. The
famous Box Muller method [34] is based on a 2-dimensional normal distribution
with probability density
</p>
<p>f (x, y)= 1
2π
</p>
<p>exp
</p>
<p>{
&minus;x
</p>
<p>2 + y2
2
</p>
<p>}
(8.70)
</p>
<p>which reads in polar coordinates
</p>
<p>f (x, y)dxdy = fp(ρ,ϕ)dρ dϕ
1
</p>
<p>2π
e&minus;ρ
</p>
<p>2/2ρ dρ dϕ. (8.71)
</p>
<p>Hence
</p>
<p>fp(ρ,ϕ)=
1
</p>
<p>2π
ρe&minus;ρ
</p>
<p>2/2 (8.72)
</p>
<p>and the cumulative distribution factorizes:
</p>
<p>Fp(ρ,ϕ)=
1
</p>
<p>2π
ϕ &middot;
</p>
<p>&int; ρ
</p>
<p>0
ρ&prime;e&minus;ρ
</p>
<p>&prime;2/2dρ&prime; = ϕ
2π
</p>
<p>(
1 &minus; e&minus;ρ2
</p>
<p>)
= Fϕ(ϕ)Fρ(ρ). (8.73)
</p>
<p>The inverse of Fρ is
</p>
<p>ρ =
&radic;
&minus; ln(1 &minus; r) (8.74)
</p>
<p>and the following algorithm generates Gaussian random numbers:
</p>
<p>r1 = RN &isin; [0,1]
r2 = RN &isin; [0,1]
ρ =
</p>
<p>&radic;
&minus; ln(1 &minus; r1)
</p>
<p>ϕ = 2πr2
x = ρ cosϕ.
</p>
<p>8.3 Monte Carlo Integration
</p>
<p>Physical problems often involve high dimensional integrals (for instance path in-
tegrals, thermodynamic averages) which cannot be evaluated by standard methods.
Here Monte Carlo methods can be very useful. Let us start with a very basic exam-
ple.
</p>
<p>8.3.1 Numerical Calculation of π
</p>
<p>The area of a unit circle (r = 1) is given by r2π = π . Hence π can be calculated by
numerical integration. We use the following algorithm:</p>
<p/>
</div>
<div class="page"><p/>
<p>8.3 Monte Carlo Integration 139
</p>
<p>Fig. 8.5 Convergence of the
numerical integration
</p>
<p>Fig. 8.6 Error of the
numerical integration
</p>
<p>choose N points randomly in the first quadrant, for instance N indepen-
dent pairs x, y &isin; [0,1]
calculate r2 = x2 + y2
count the number of points within the circle, i.e. the number of points
Z(r2 &le; 1).
π
4 is approximately given by
</p>
<p>Z(r2&le;1)
N
</p>
<p>.
</p>
<p>The result converges rather slowly (Figs. 8.5, 8.6)
</p>
<p>8.3.2 Calculation of an Integral
</p>
<p>Let ξ be a random variable in the interval [a, b] with the distribution
</p>
<p>P(x &lt; ξ &le; x + dx)= f (x)dx =
{ 1
</p>
<p>b&minus;a for x &isin; [a, b]
0 else.
</p>
<p>(8.75)</p>
<p/>
</div>
<div class="page"><p/>
<p>140 8 Random Numbers and Monte Carlo Methods
</p>
<p>The expectation value of a function g(x) is
</p>
<p>E
[
g(x)
</p>
<p>]
=
&int; &infin;
</p>
<p>&minus;&infin;
g(x)f (x) dx =
</p>
<p>&int; b
</p>
<p>a
</p>
<p>g(x)dx (8.76)
</p>
<p>hence the average of N randomly taken function values approximates the integral
&int; b
</p>
<p>a
</p>
<p>g(x)dx &asymp; 1
N
</p>
<p>N&sum;
</p>
<p>i=1
g(ξi)= g(ξ). (8.77)
</p>
<p>To estimate the error we consider the new random variable
</p>
<p>γ = 1
N
</p>
<p>N&sum;
</p>
<p>i=1
g(ξ). (8.78)
</p>
<p>Its average is
</p>
<p>γ =E[γ ] = 1
N
</p>
<p>N&sum;
</p>
<p>i=1
E
[
g(x)
</p>
<p>]
=E
</p>
<p>[
g(x)
</p>
<p>]
=
&int; b
</p>
<p>a
</p>
<p>g(x)dx (8.79)
</p>
<p>and the variance follows from
</p>
<p>σ 2γ =E
[
(γ &minus; γ )2
</p>
<p>]
=E
</p>
<p>[(
1
</p>
<p>N
</p>
<p>&sum;
g(ξi)&minus; γ
</p>
<p>)2]
=E
</p>
<p>[(
1
</p>
<p>N
</p>
<p>&sum;(
g(ξi)&minus; γ
</p>
<p>))2]
</p>
<p>(8.80)
</p>
<p>= 1
N2
</p>
<p>E
[&sum;(
</p>
<p>g(ξi)&minus; γ
)2]= 1
</p>
<p>N
</p>
<p>(
g(ξ)2 &minus; g(ξ)2
</p>
<p>)
= 1
</p>
<p>N
σ 2g(ξ). (8.81)
</p>
<p>The width of the distribution and hence the uncertainty falls off as 1/
&radic;
N .
</p>
<p>8.3.3 More General Random Numbers
</p>
<p>Consider now random numbers ξ &isin; [a, b] with arbitrary (but within [a, b] not van-
ishing) probability density f (x). The integral is approximated by
</p>
<p>1
</p>
<p>N
</p>
<p>N&sum;
</p>
<p>i=1
</p>
<p>g(ξi)
</p>
<p>f (ξi)
=E
</p>
<p>[
g(x)
</p>
<p>f (x)
</p>
<p>]
=
&int; b
</p>
<p>a
</p>
<p>g(x)
</p>
<p>f (x)
f (x) dx =
</p>
<p>&int; b
</p>
<p>a
</p>
<p>g(x)dx. (8.82)
</p>
<p>The new random variable
</p>
<p>τ = 1
N
</p>
<p>N&sum;
</p>
<p>i=1
</p>
<p>g(ξi)
</p>
<p>f (ξi)
(8.83)
</p>
<p>according to (8.81) has a standard deviation given by
</p>
<p>στ =
1&radic;
N
σ
</p>
<p>(
g(ξ)
</p>
<p>f (ξ)
</p>
<p>)
(8.84)
</p>
<p>which can be reduced by choosing f similar to g. Then preferentially ξ are gener-
ated in regions where the integrand is large (importance sampling).</p>
<p/>
</div>
<div class="page"><p/>
<p>8.4 Monte Carlo Method for Thermodynamic Averages 141
</p>
<p>8.4 Monte Carlo Method for Thermodynamic Averages
</p>
<p>Consider the partition function of a classical N particle system
</p>
<p>ZNVT =
1
</p>
<p>N !
1
</p>
<p>h3N
</p>
<p>&int;
dp3N
</p>
<p>&int;
dq3Ne&minus;βH(p1&middot;&middot;&middot;pN ,q1&middot;&middot;&middot;qN ) (8.85)
</p>
<p>with an energy function
</p>
<p>H =
N&sum;
</p>
<p>i=1
</p>
<p>p2i
</p>
<p>2mi
+ V (q1 &middot; &middot; &middot;qN ). (8.86)
</p>
<p>If the potential energy does not depend on the momenta and for equal masses the
partition function simplifies to
</p>
<p>ZNVT =
1
</p>
<p>N !
1
</p>
<p>h3N
</p>
<p>&int;
dp3Ne&minus;β
</p>
<p>p2
i
</p>
<p>2m
</p>
<p>&int;
dq3Ne&minus;βV (q)
</p>
<p>= 1
N !
</p>
<p>(
2πmkT
</p>
<p>h2
</p>
<p>)3N/2 &int;
dq3Ne&minus;βV (q) (8.87)
</p>
<p>and it remains the calculation of the configuration integral
</p>
<p>Z
conf
NVT =
</p>
<p>&int;
dq3Ne&minus;βV (q). (8.88)
</p>
<p>In the following we do not need the partition function itself but only averages of
some quantity A(q) given by
</p>
<p>〈A〉 =
&int;
dq3NA(q)e&minus;βV (q)&int;
</p>
<p>dq3Ne&minus;βV (q)
. (8.89)
</p>
<p>8.4.1 Simple Sampling
</p>
<p>Let ξ be a random variable with probability distribution
</p>
<p>P
(
ξ &isin; [q, q + dq]
</p>
<p>)
= f (q)dq (8.90)
</p>
<p>&int;
f (q)dq = 1. (8.91)
</p>
<p>We choose M random numbers ξ and calculate the expectation value of A(ξ) from
</p>
<p>E
[
A(ξ)
</p>
<p>]
= lim
</p>
<p>M&rarr;&infin;
1
</p>
<p>M
</p>
<p>M&sum;
</p>
<p>m=1
A
(
ξ (m)
</p>
<p>)
=
&int;
</p>
<p>A(q)f (q) dq. (8.92)
</p>
<p>Consider now the case of random numbers ξ equally distributed over the range
qmin &middot; &middot; &middot;qmax:</p>
<p/>
</div>
<div class="page"><p/>
<p>142 8 Random Numbers and Monte Carlo Methods
</p>
<p>f (q)=
{ 1
</p>
<p>qmax&minus;qmin q &isin; [qmin, qmax]
0 else.
</p>
<p>(8.93)
</p>
<p>Define a sample by choosing one random value ξ for each of the 3N coordinates.
The average over a large number M of samples gives the expectation value
</p>
<p>E
(
A(ξ1 &middot; &middot; &middot; ξ3N )e
</p>
<p>&minus;βV (ξ1 &middot;&middot;&middot;ξ3N ))= lim
M&rarr;&infin;
</p>
<p>1
</p>
<p>M
</p>
<p>M&sum;
</p>
<p>m=1
A
(
ξ
(m)
i
</p>
<p>)
e&minus;βV (ξ
</p>
<p>(m)
i ) (8.94)
</p>
<p>as &int;
A(qi)e
</p>
<p>&minus;βV (qi )f (q1) &middot; &middot; &middot;f (q3N )dq1 &middot; &middot; &middot;dq3N
</p>
<p>= 1
(qmax &minus; qmin)3N
</p>
<p>&int; qmax
qmin
</p>
<p>&middot; &middot; &middot;
&int; qmax
qmin
</p>
<p>A(qi)e
&minus;βV (qi ) dq3N . (8.95)
</p>
<p>Hence
</p>
<p>E(A(ξi)e&minus;βV (ξi ))
</p>
<p>E(e&minus;βV (ξi ))
=
</p>
<p>&int; qmax
qmin
</p>
<p>A(qi)e&minus;βV (qi ) dq3N&int; qmax
qmin
</p>
<p>e&minus;βV (qi ) dq3N
&asymp; 〈A〉 (8.96)
</p>
<p>is an approximation to the average of A(qi), if the range of the qi is sufficiently
large. However, many of the samples will have small weight and contribute only
little.
</p>
<p>8.4.2 Importance Sampling
</p>
<p>Let us try to sample preferentially the most important configurations. Choose the
distribution function as
</p>
<p>f (q1 &middot; &middot; &middot;q3N )=
e&minus;βV (q1&middot;&middot;&middot;q3N )&int;
e&minus;βV (q1&middot;&middot;&middot;q3N )
</p>
<p>. (8.97)
</p>
<p>The expectation value of A(q) approximates the thermal average
</p>
<p>E
(
A(ξ1 &middot; &middot; &middot; ξ3N )
</p>
<p>)
= lim
</p>
<p>M&rarr;&infin;
1
</p>
<p>M
</p>
<p>M&sum;
</p>
<p>m=1
A
(
ξ
(m)
i
</p>
<p>)
=
</p>
<p>&int;
A(qi)e&minus;βV (qi ) dq3N&int;
</p>
<p>e&minus;βV (qi ) dq3N
= 〈A〉
</p>
<p>(8.98)
</p>
<p>and the partition function is not needed explicitly for the calculation.
</p>
<p>8.4.3 Metropolis Algorithm
</p>
<p>The algorithm by Metropolis [175] can be used to select the necessary configura-
tions. Starting from an initial configuration q0 = (q(0)1 &middot; &middot; &middot;q
</p>
<p>(0)
3N ) a chain of configu-
</p>
<p>rations is generated. Each configuration depends only on its predecessor, hence the
configurations form a Markov chain.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.4 Monte Carlo Method for Thermodynamic Averages 143
</p>
<p>Fig. 8.7 Principle of detailed
balance
</p>
<p>The transition probabilities
</p>
<p>Wi&rarr;j = P(qi &rarr; qj ) (8.99)
</p>
<p>are chosen to fulfill the condition of detailed balance (Fig. 8.7)
</p>
<p>Wi&rarr;j
Wj&rarr;i
</p>
<p>= e&minus;β(V (qj )&minus;V (qi)). (8.100)
</p>
<p>This is a sufficient condition that the configurations are generated with probabilities
given by their Boltzmann factors. This can be seen from consideration of an ensem-
ble of such Markov chains: Let Nn(qi) denote the number of chains which are in
the configuration qi after n steps. The changes during the following step are
</p>
<p>�N(qi)=Nn+1(qi)&minus;Nn(qi)=
&sum;
</p>
<p>qj&isin;conf .
Nn(qj )Wj&rarr;i &minus;Nn(qi)Wi&rarr;j .
</p>
<p>(8.101)
</p>
<p>In thermal equilibrium
</p>
<p>Neq(qi)=N0e&minus;βV (qi)
</p>
<p>and the changes (8.101) vanish:
</p>
<p>�N(qi)=N0
&sum;
</p>
<p>qj
</p>
<p>e&minus;βV (qj )Wj&rarr;i &minus; e&minus;βV (qi)Wi&rarr;j
</p>
<p>=N0
&sum;
</p>
<p>qj
</p>
<p>e&minus;βV (qj )Wj&rarr;i &minus; e&minus;βV (qi)Wj&rarr;ie&minus;β(V (qj )&minus;V (qi ))
</p>
<p>= 0. (8.102)
</p>
<p>A solution of
</p>
<p>�N(qi)=
&sum;
</p>
<p>qj&isin;conf .
Nn(qj )Wj&rarr;i &minus;Nn(qi)Wi&rarr;j = 0 (8.103)
</p>
<p>corresponds to a zero eigenvalue of the system of equations</p>
<p/>
</div>
<div class="page"><p/>
<p>144 8 Random Numbers and Monte Carlo Methods
</p>
<p>&sum;
</p>
<p>qj
</p>
<p>N(qj )Wj&rarr;i &minus;N(qi)
&sum;
</p>
<p>qj
</p>
<p>Wi&rarr;j = λN(qi). (8.104)
</p>
<p>One solution of this eigenvalue equation is given by
</p>
<p>Neq(qj )
</p>
<p>Neq(qi)
= e&minus;β(V (qj )&minus;V (qi )). (8.105)
</p>
<p>However, there may be other solutions. For instance if not all configurations are
connected by possible transitions and some isolated configurations are occupied
initially.
</p>
<p>Metropolis Algorithm This famous algorithm consists of the following steps:
</p>
<p>(a) choose a new configuration randomly (trial step) with probability
</p>
<p>T (qi &rarr; qtrial)= T (qtrial &rarr; qi)
</p>
<p>(b) calculate
</p>
<p>R = e
&minus;βV (qtrial)
</p>
<p>e&minus;βV (qi )
</p>
<p>(c) if R &ge; 1 the trial step is accepted qi+1 = qtrial
(d) if R &lt; 1 the trial step is accepted only with probability R. Choose a random
</p>
<p>number ξ &isin; [0,1] and the next configuration according to
</p>
<p>qi+1 =
{
qtrial if ξ &lt; R
qi if ξ &ge;R.
</p>
<p>The transition probability is the product
</p>
<p>Wi&rarr;j = Ti&rarr;jAi&rarr;j (8.106)
</p>
<p>of the probability Ti&rarr;j to select i &rarr; j as a trial step and the probability Ai&rarr;j to
accept the trial step. Now we have
</p>
<p>for R &ge; 1 &rarr;Ai&rarr;j = 1, Aj&rarr;i =R&minus;1
</p>
<p>for R &lt; 1 &rarr;Ai&rarr;j =R, Aj&rarr;i = 1.
(8.107)
</p>
<p>Since Ti&rarr;j = Tj&rarr;i , in both cases
</p>
<p>Neq(qj )
</p>
<p>Neq(qi)
= Wi&rarr;j
</p>
<p>Wj&rarr;i
= Ai&rarr;j
</p>
<p>Aj&rarr;i
=R = e&minus;β(V (qj )&minus;V (qi ). (8.108)
</p>
<p>8.5 Problems
</p>
<p>Problem 8.1 (Central limit theorem) This computer experiment draws a histogram
for the random variable τ , which is calculated from N random numbers ξ1 &middot; &middot; &middot; ξN :</p>
<p/>
</div>
<div class="page"><p/>
<p>8.5 Problems 145
</p>
<p>τ =
&sum;N
</p>
<p>i=1 ξi&radic;
N
</p>
<p>. (8.109)
</p>
<p>The ξi are random numbers with zero mean and unit variance and can be chosen as
</p>
<p>&bull; ξi =&plusmn;1 (coin tossing)
&bull; Gaussian random numbers
Investigate how a Gaussian distribution is approached for large N .
</p>
<p>Problem 8.2 (Nonlinear optimization) MC methods can be used for nonlinear op-
timization (Traveling salesman problem, structure optimization etc.) [31]. Consider
an energy function depending on many coordinates
</p>
<p>E(q1, q2 &middot; &middot; &middot;qN ). (8.110)
Introduce a fictitious temperature T and generate configurations with probabilities
</p>
<p>P(q1 &middot; &middot; &middot;qN )=
1
</p>
<p>Z
e&minus;E(q1&middot;&middot;&middot;qN )/T . (8.111)
</p>
<p>Slow cooling drives the system into a local minimum. By repeated heating and cool-
ing other local minima can be reached (simulated annealing).
</p>
<p>In this computer experiment we try to find the shortest path which visits each
of up to N = 50 given points. The fictitious Boltzmann factor for a path with total
length L is
</p>
<p>P(L)= e&minus;L/T (8.112)
Starting from an initial path S = (i1, i2 &middot; &middot; &middot; iN ) n &lt; 5 and p are chosen randomly and
a new path S&prime; = (i1 &middot; &middot; &middot; ip&minus;1, ip+n &middot; &middot; &middot; ip, ip+n+1 &middot; &middot; &middot; iN ) is generated by reverting the
sub-path
</p>
<p>ip &middot; &middot; &middot; ip+n &rarr; ip+n &middot; &middot; &middot; ip.
Start at high temperature T &gt; L and cool down slowly.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 9
</p>
<p>Eigenvalue Problems
</p>
<p>Eigenvalue problems are omnipresent in physics. Important examples are the time
independent Schr&ouml;dinger equation in a finite orthogonal basis (Chap. 9)
</p>
<p>M&sum;
</p>
<p>j=1
〈φj &prime; |H |φj 〉Cj =ECj &prime; (9.1)
</p>
<p>or the harmonic motion of a molecule around its equilibrium structure (Sect. 14.4.1)
</p>
<p>ω2mi
(
ξi &minus; ξ eqi
</p>
<p>)
=
&sum;
</p>
<p>j
</p>
<p>&part;2U
</p>
<p>&part;ξi&part;ξj
</p>
<p>(
ξj &minus; ξ eqj
</p>
<p>)
. (9.2)
</p>
<p>Most important are ordinary eigenvalue problems,1 which involve the solution of
a homogeneous system of linear equations
</p>
<p>N&sum;
</p>
<p>j=1
aijxj = λxi (9.3)
</p>
<p>with a Hermitian (or symmetric, if real) matrix [198]
</p>
<p>aji = a&lowast;ij . (9.4)
</p>
<p>Matrices of small dimension can be diagonalized directly by determining the
roots of the characteristic polynomial and solving a homogeneous system of linear
equations. The Jacobi method uses successive rotations to diagonalize a matrix with
a unitary transformation. A very popular method for not too large symmetric ma-
trices reduces the matrix to tridiagonal form which can be diagonalized efficiently
with the QL algorithm. Some special tridiagonal matrices can be diagonalized ana-
lytically. Special algorithms are available for matrices of very large dimension, for
instance the famous Lanczos method.
</p>
<p>1We do not consider general eigenvalue problems here.
</p>
<p>P.O.J. Scherer, Computational Physics, Graduate Texts in Physics,
DOI 10.1007/978-3-319-00401-3_9,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>147</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-00401-3_9">http://dx.doi.org/10.1007/978-3-319-00401-3_9</a></div>
</div>
<div class="page"><p/>
<p>148 9 Eigenvalue Problems
</p>
<p>9.1 Direct Solution
</p>
<p>For matrices of very small dimension (2,3) the determinant
</p>
<p>det |aij &minus; λδij | = 0 (9.5)
</p>
<p>can be written explicitly as a polynomial of λ. The roots of this polynomial are the
eigenvalues. The eigenvectors are given by the system of equations
</p>
<p>&sum;
</p>
<p>j
</p>
<p>(aij &minus; λδij )uj = 0. (9.6)
</p>
<p>9.2 Jacobi Method
</p>
<p>Any symmetric 2 &times; 2 matrix
</p>
<p>A=
(
a11 a12
a12 a22
</p>
<p>)
(9.7)
</p>
<p>can be diagonalized by a rotation of the coordinate system. Rotation by the angle ϕ
corresponds to an orthogonal transformation with the rotation matrix
</p>
<p>Rϕ =
(
</p>
<p>cosϕ &minus; sinϕ
sinϕ cosϕ
</p>
<p>)
. (9.8)
</p>
<p>In the following we use the abbreviations
</p>
<p>c= cosϕ, s = sinϕ, t = tanϕ. (9.9)
</p>
<p>The transformed matrix is
</p>
<p>RAR&minus;1 =
(
c &minus;s
s c
</p>
<p>)(
a11 a12
a12 a22
</p>
<p>)(
c s
</p>
<p>&minus;s c
</p>
<p>)
</p>
<p>=
(
</p>
<p>c2a11 + s2a22 &minus; 2csa12 cs(a11 &minus; a22)+ (c2 &minus; s2)a12
cs(a11 &minus; a22)+ (c2 &minus; s2)a12 s2a11 + c2a22 + 2csa12
</p>
<p>)
.
</p>
<p>(9.10)
</p>
<p>It is diagonal if
</p>
<p>0 = cs(a11 &minus; a22)+
(
c2 &minus; s2
</p>
<p>)
a12 =
</p>
<p>a11 &minus; a22
2
</p>
<p>sin(2ϕ)+ a12 cos(2ϕ) (9.11)
</p>
<p>or
</p>
<p>tan(2ϕ)= 2a12
a22 &minus; a11
</p>
<p>. (9.12)</p>
<p/>
</div>
<div class="page"><p/>
<p>9.2 Jacobi Method 149
</p>
<p>Calculation of ϕ is not necessary since only its cosine and sine appear in (9.10).
From [198]
</p>
<p>1 &minus; t2
t
</p>
<p>= c
2 &minus; s2
2cs
</p>
<p>= cot(2ϕ)= a22 &minus; a11
2a12
</p>
<p>(9.13)
</p>
<p>we see that t is a root of
</p>
<p>t2 + a22 &minus; a11
a12
</p>
<p>t &minus; 1 = 0 (9.14)
</p>
<p>hence
</p>
<p>t =&minus;a22 &minus; a11
2a12
</p>
<p>&plusmn;
</p>
<p>&radic;
</p>
<p>1 +
(
a22 &minus; a11
</p>
<p>2a12
</p>
<p>)2
= 1
</p>
<p>a22&minus;a11
2a12
</p>
<p>&plusmn;
&radic;
</p>
<p>1 + ( a22&minus;a112a12 )
2
. (9.15)
</p>
<p>For reasons of convergence [198] the solution with smaller magnitude is chosen
which can be written as
</p>
<p>t =
sign( a22&minus;a112a12 )
</p>
<p>| a22&minus;a112a12 | +
&radic;
</p>
<p>1 + ( a22&minus;a112a12 )
2
</p>
<p>(9.16)
</p>
<p>again for reasons of convergence the smaller solution ϕ is preferred and therefore
we take
</p>
<p>c= 1&radic;
1 + t2
</p>
<p>s = t&radic;
1 + t2
</p>
<p>. (9.17)
</p>
<p>The diagonal elements of the transformed matrix are
</p>
<p>ã11 = c2a11 + s2a22 &minus; 2csa12 (9.18)
ã22 = s2a11 + c2a22 + 2csa12. (9.19)
</p>
<p>The trace of the matrix is invariant
</p>
<p>ã11 + ã22 = a11 + a22 (9.20)
</p>
<p>whereas the difference of the diagonal elements is
</p>
<p>ã11 &minus; ã22 =
(
c2 &minus; s2
</p>
<p>)
(a11 &minus; a22)&minus; 4csa12
</p>
<p>= 1 &minus; t
2
</p>
<p>1 + t2 (a11 &minus; a22)&minus; 4
a12t
</p>
<p>1 + t2
</p>
<p>= (a11 &minus; a22)+
(
&minus;a12
</p>
<p>1 &minus; t2
t
</p>
<p>) &minus;2t2
1 + t2 &minus; 4
</p>
<p>a12t
</p>
<p>1 + t2
</p>
<p>= (a11 &minus; a22)&minus; 2ta12 (9.21)</p>
<p/>
</div>
<div class="page"><p/>
<p>150 9 Eigenvalue Problems
</p>
<p>and the transformed matrix has the simple form
</p>
<p>(
a11 &minus; a12t
</p>
<p>a22 + a12t
</p>
<p>)
. (9.22)
</p>
<p>For larger dimension N &gt; 2 the Jacobi method uses the following algorithm:
</p>
<p>(1) look for the dominant non-diagonal element max i �=j |aij |
(2) perform a rotation in the (ij )-plane to cancel the element ãij of the transformed
</p>
<p>matrix Ã=R(ij) &middot;A &middot;R(ij)&minus;1. The corresponding rotation matrix has the form
</p>
<p>R(ij) =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>1
. . .
</p>
<p>c s
</p>
<p>. . .
</p>
<p>&minus;s c
. . .
</p>
<p>1
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>(9.23)
</p>
<p>(3) repeat (1&ndash;2) until convergence (if possible).
</p>
<p>The sequence of Jacobi rotations gives the over all transformation
</p>
<p>RAR&minus;1 = &middot; &middot; &middot;R2R1AR&minus;11 R
&minus;1
2 &middot; &middot; &middot; =
</p>
<p>⎛
⎜⎝
λ1
</p>
<p>. . .
</p>
<p>λN
</p>
<p>⎞
⎟⎠ . (9.24)
</p>
<p>Hence
</p>
<p>AR&minus;1 =R&minus;1
</p>
<p>⎛
⎜⎝
λ1
</p>
<p>. . .
</p>
<p>λN
</p>
<p>⎞
⎟⎠ (9.25)
</p>
<p>and the column vectors of R&minus;1 = (v1,v2 &middot; &middot; &middot;vN ) are the eigenvectors of A:
</p>
<p>A(v1,v2 &middot; &middot; &middot;vN )= (λ1v1, λ2v2 &middot; &middot; &middot;λNvN ). (9.26)
</p>
<p>9.3 Tridiagonal Matrices
</p>
<p>A tridiagonal matrix has nonzero elements only in the main diagonal and the first
diagonal above and below. Many algorithms simplify significantly when applied to
tridiagonal matrices.</p>
<p/>
</div>
<div class="page"><p/>
<p>9.3 Tridiagonal Matrices 151
</p>
<p>9.3.1 Characteristic Polynomial of a Tridiagonal Matrix
</p>
<p>The characteristic polynomial of a tridiagonal matrix
</p>
<p>PA(λ)= det
</p>
<p>∣∣∣∣∣∣∣∣∣
</p>
<p>a11 &minus; λ a12
a21 a22 &minus; λ
</p>
<p>. . . aN&minus;1N
aNN&minus;1 aNN &minus; λ
</p>
<p>∣∣∣∣∣∣∣∣∣
(9.27)
</p>
<p>can be calculated recursively:
</p>
<p>P0 = 1
P1(λ)= a11 &minus; λ
P2(λ)= (a22 &minus; λ)P1(λ)&minus; a12a21
...
</p>
<p>PN (λ)= (aNN &minus; λ)PN&minus;1(λ)&minus; aN,N&minus;1aN&minus;1,NPN&minus;2(λ).
</p>
<p>(9.28)
</p>
<p>9.3.2 Special Tridiagonal Matrices
</p>
<p>Certain classes of tridiagonal matrices can be diagonalized exactly [55, 151, 281].
</p>
<p>9.3.2.1 Discretized Second Derivatives
</p>
<p>Discretization of a second derivative involves, under Dirichlet boundary conditions
f (x0)= f (xN+1)= 0, the differentiation matrix (Sect. 18.2)
</p>
<p>M =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>&minus;2 1
1 &minus;2 1
</p>
<p>. . .
. . .
</p>
<p>. . .
</p>
<p>1 &minus;2 1
1 &minus;2
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠
. (9.29)
</p>
<p>Its eigenvectors have the form
</p>
<p>f=
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎝
</p>
<p>f1
...
</p>
<p>fn
...
</p>
<p>fN
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎠
</p>
<p>=
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎝
</p>
<p>sin k
...
</p>
<p>sin(nk)
...
</p>
<p>sin(Nk)
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎠
. (9.30)</p>
<p/>
</div>
<div class="page"><p/>
<p>152 9 Eigenvalue Problems
</p>
<p>This can be seen by inserting (9.30) into the n-th line of the eigenvalue equation
(9.31)
</p>
<p>Mf = λf (9.31)
(Mf)n =
</p>
<p>(
sin
</p>
<p>(
(n&minus; 1)k
</p>
<p>)
+ sin
</p>
<p>(
(n+ 1)k
</p>
<p>)
&minus; 2 sin(nk)
</p>
<p>)
</p>
<p>= 2 sin(nk)
(
cos(k)&minus; 1
</p>
<p>)
= λ(f)n (9.32)
</p>
<p>with the eigenvalue
</p>
<p>λ= 2(cosk &minus; 1)=&minus;4 sin2
(
k
</p>
<p>2
</p>
<p>)
. (9.33)
</p>
<p>The first line of the eigenvalue equation (9.31) reads
</p>
<p>(Mf)1 =
(
&minus;2 sin(k)+ sin(2k)
</p>
<p>)
</p>
<p>= 2 sin(k)
(
cos(k)&minus; 1
</p>
<p>)
= λ(f)n (9.34)
</p>
<p>and from the last line we have
</p>
<p>(Mf)N =
(
&minus;2 sin(Nk)+ sin
</p>
<p>(
[N &minus; 1]k
</p>
<p>))
</p>
<p>= λ(f)N = 2
(
cos(k)&minus; 1
</p>
<p>)
sin(Nk) (9.35)
</p>
<p>which holds if
</p>
<p>sin
(
(N &minus; 1)k
</p>
<p>)
= 2 sin(Nk) cos(k). (9.36)
</p>
<p>This simplifies to
</p>
<p>sin(Nk) cos(k)&minus; cos(Nk) sin(k)= 2 sin(Nk) cos(k)
sin(Nk) cos(k)+ cos(Nk) sin(k)= 0 (9.37)
sin
</p>
<p>(
(N + 1)k
</p>
<p>)
= 0.
</p>
<p>Hence the possible values of k are
</p>
<p>k = π
(N + 1) l with l = 1,2 &middot; &middot; &middot;N (9.38)
</p>
<p>and the eigenvectors are explicitly (Fig. 9.1)
</p>
<p>f=
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎝
</p>
<p>sin( π
N+1 l)
...
</p>
<p>sin( π
N+1 l n)
...
</p>
<p>sin( π
N+1 l N)
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎠
. (9.39)</p>
<p/>
</div>
<div class="page"><p/>
<p>9.3 Tridiagonal Matrices 153
</p>
<p>Fig. 9.1 (Lowest eigenvector
for fixed and open
boundaries) Top: for fixed
boundaries fn = sin(nk)
which is zero at the additional
points x0, xN+1. For open
boundaries
fn = cos((n&minus; 1)k) with
horizontal tangent at x1, xN
due to the boundary
conditions f2 = f0,
fN&minus;1 = fN+1
</p>
<p>For Neumann boundary conditions &part;f
&part;x
(x1) = &part;f&part;x (xN ) = 0 the matrix is slightly
</p>
<p>different (Sect. 18.2)
</p>
<p>M =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>&minus;2 2
1 &minus;2 1
</p>
<p>. . .
. . .
</p>
<p>. . .
</p>
<p>1 &minus;2 1
2 &minus;2
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠
. (9.40)
</p>
<p>Its eigenvalues are also given by the expression (9.33). To obtain the eigenvectors,
we try a more general ansatz with a phase shift
</p>
<p>f=
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎝
</p>
<p>sinΦ1
...
</p>
<p>sin(Φ1 + (n&minus; 1)k)
...
</p>
<p>sin(Φ1 + (N &minus; 1)k)
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎠
. (9.41)
</p>
<p>Obviously
</p>
<p>sin
(
Φ1 + (n&minus; 1)k &minus; k
</p>
<p>)
+ sin
</p>
<p>(
Φ1 + (n&minus; 1)k+ k
</p>
<p>)
&minus; 2 sin
</p>
<p>(
Φ1 + (n&minus; 1)k
</p>
<p>)
</p>
<p>= 2(cosk&minus; 1) sin
(
Φ1 + (n&minus; 1)k
</p>
<p>)
. (9.42)
</p>
<p>The first and last lines of the eigenvalue equation give
</p>
<p>0 = &minus;2 sin(Φ1)+ 2 sin(Φ1 + k)&minus; 2(cosk&minus; 1) sin(Φ1)
= 2 cosΦ1 sink (9.43)
</p>
<p>and</p>
<p/>
</div>
<div class="page"><p/>
<p>154 9 Eigenvalue Problems
</p>
<p>0 = &minus;2 sin
(
Φ1 + (N &minus; 1)k
</p>
<p>)
+ 2 sin
</p>
<p>(
Φ1 + (N &minus; 1)k&minus; k
</p>
<p>)
</p>
<p>&minus; 2(cosk &minus; 1) sin
(
Φ1 + (N &minus; 1)k
</p>
<p>)
</p>
<p>= 2 cos
(
Φ1 + (N &minus; 1)k
</p>
<p>)
sin k (9.44)
</p>
<p>which is solved by
</p>
<p>Φ1 =
π
</p>
<p>2
k = π
</p>
<p>N &minus; 1 l, l = 1,2 &middot; &middot; &middot;N (9.45)
</p>
<p>hence finally the eigenvector is (Fig. 9.1)
</p>
<p>f=
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎝
</p>
<p>1
...
</p>
<p>cos( n&minus;1
N&minus;1πl)
...
</p>
<p>(&minus;1)l
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎠
. (9.46)
</p>
<p>Even simpler is the case of the corresponding cyclic tridiagonal matrix
</p>
<p>M =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>&minus;2 1 1
1 &minus;2 1
</p>
<p>. . .
. . .
</p>
<p>. . .
</p>
<p>1 &minus;2 1
1 1 &minus;2
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠
</p>
<p>(9.47)
</p>
<p>which has eigenvectors
</p>
<p>f=
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎝
</p>
<p>eik
</p>
<p>...
</p>
<p>eink
</p>
<p>...
</p>
<p>eiNk
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎠
</p>
<p>(9.48)
</p>
<p>and eigenvalues
</p>
<p>λ=&minus;2 + e&minus;ik + eik = 2
(
cos(k)&minus; 1
</p>
<p>)
=&minus; sin2
</p>
<p>(
k
</p>
<p>2
</p>
<p>)
(9.49)
</p>
<p>where the possible k &minus; values again follow from the first and last line
</p>
<p>&minus;2eik + ei2k + eiNk =
(
&minus;2 + e&minus;ik + eik
</p>
<p>)
eik (9.50)
</p>
<p>eik + ei(N&minus;1)k &minus; 2eiNk =
(
&minus;2 + e&minus;ik + eik
</p>
<p>)
eiNk (9.51)</p>
<p/>
</div>
<div class="page"><p/>
<p>9.3 Tridiagonal Matrices 155
</p>
<p>which both lead to
</p>
<p>eiNk = 1 (9.52)
</p>
<p>k = 2π
N
</p>
<p>l, l = 0,1 &middot; &middot; &middot;N &minus; 1. (9.53)
</p>
<p>9.3.2.2 Discretized First Derivatives
</p>
<p>Using symmetric differences to discretize a first derivative in one dimension leads
to the matrix2
</p>
<p>D =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>1
&minus;1 1
</p>
<p>. . .
. . .
</p>
<p>. . .
. . .
</p>
<p>&minus;1 1
&minus;1
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>. (9.54)
</p>
<p>The characteristic polynomial of the Hermitian matrix iD is given by the recursion
(9.3.1)
</p>
<p>P0 = 1
P1 =&minus;λ
...
</p>
<p>PN =&minus;λPN&minus;1 &minus; PN&minus;2
</p>
<p>(9.55)
</p>
<p>which after the substitution x = &minus;λ/2 is exactly the recursion for the Chebyshev
polynomial of the second kind UN (x). Hence the eigenvalues of D are given by the
roots xk of UN (x) as
</p>
<p>λD = 2ixk = 2i cos
(
</p>
<p>kπ
</p>
<p>N + 1
</p>
<p>)
k = 1,2 &middot; &middot; &middot;N. (9.56)
</p>
<p>The eigenvalues of the corresponding cyclic tridiagonal matrix
</p>
<p>D =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>1 &minus;1
&minus;1 1
</p>
<p>. . .
. . .
</p>
<p>. . .
. . .
</p>
<p>&minus;1 1
1 &minus;1
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>(9.57)
</p>
<p>2This matrix is skew symmetric, hence iT is Hermitian and has real eigenvalues iλ.</p>
<p/>
</div>
<div class="page"><p/>
<p>156 9 Eigenvalue Problems
</p>
<p>are easy to find. Inserting the ansatz for the eigenvector
</p>
<p>⎛
⎜⎝
</p>
<p>exp ik
...
</p>
<p>exp iNk
</p>
<p>⎞
⎟⎠ (9.58)
</p>
<p>we find the eigenvalues
</p>
<p>ei(m+1)k &minus; ei(m&minus;1)k = λeimk (9.59)
λ = 2i sink (9.60)
</p>
<p>and from the first and last equation
</p>
<p>1 = eiNk (9.61)
eik = ei(N+1)k (9.62)
</p>
<p>the possible k-values
</p>
<p>k = 2π
N
</p>
<p>l, l = 0,1 &middot; &middot; &middot;N &minus; 1. (9.63)
</p>
<p>9.3.3 The QL Algorithm
</p>
<p>Any real matrix A can be decomposed into the product of a lower triangular matrix
L and an orthogonal matrix QT =Q&minus;1 (this is quite similar to the QR factorization
with an upper triangular matrix which is discussed in Sect. 5.2)
</p>
<p>A=QL. (9.64)
</p>
<p>For symmetric tridiagonal matrices this factorization can be efficiently realized by
multiplication with a sequence of rotation matrices which eliminate the off-diagonal
elements in the lower part
</p>
<p>Q=R(N&minus;1,N) &middot; &middot; &middot;R(2,3)R(1,2). (9.65)
</p>
<p>An orthogonal transformation of A is given by
</p>
<p>QTAQ=QTQLQ= LQ. (9.66)
</p>
<p>The QL algorithm is an iterative algorithm. It uses the transformation
</p>
<p>An+1 =QTnAnQn =QTn (An &minus; σn)Qn + σn =QTnQnLnQn + σn
= LnQn + σn (9.67)</p>
<p/>
</div>
<div class="page"><p/>
<p>9.4 Reduction to a Tridiagonal Matrix 157
</p>
<p>where the shift parameter σn was introduced to improve convergence and the QL
factorization is applied to An &minus; σn. This transformation conserves symmetry and
tridiagonal form. Repeated transformation gives a sequence of tridiagonal matrices,
which converge to a diagonal matrix if the shifts σn are properly chosen. A very
popular choice [198] is Wilkinson&rsquo;s shift
</p>
<p>σ = a11 &minus; sign(δ)
a212
</p>
<p>|δ| +
&radic;
δ2 + a212
</p>
<p>δ = a22 &minus; a11
2
</p>
<p>(9.68)
</p>
<p>which is that eigenvalue of the matrix
( a11 a12
a12 a22
</p>
<p>)
which is closer to a11 (the smaller
</p>
<p>one if a11 = a22).
</p>
<p>9.4 Reduction to a Tridiagonal Matrix
</p>
<p>Any symmetric matrix can be transformed to a tridiagonal matrix by a series of
Householder transformations (5.53)
</p>
<p>A&prime; = PAP with P = P T = 1 &minus; 2uu
T
</p>
<p>|u|2 . (9.69)
</p>
<p>The following orthogonal transformation P1 brings the first row and column to tridi-
agonal form. We divide the matrix A according to
</p>
<p>A=
(
a11 α
</p>
<p>T
</p>
<p>α Arest
</p>
<p>)
(9.70)
</p>
<p>with the (N &minus; 1)-dimensional vector
</p>
<p>α =
</p>
<p>⎛
⎜⎝
a12
...
</p>
<p>a1n
</p>
<p>⎞
⎟⎠ .
</p>
<p>Now let
</p>
<p>u=
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>0
a12 + λ
</p>
<p>...
</p>
<p>a1N
</p>
<p>⎞
⎟⎟⎟⎠=
</p>
<p>(
0
α
</p>
<p>)
+ λe(2) with e(2) =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>0
1
0
...
</p>
<p>0
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠
. (9.71)
</p>
<p>Then
</p>
<p>|u|2 = |α|2 + λ2 + 2λa12 (9.72)</p>
<p/>
</div>
<div class="page"><p/>
<p>158 9 Eigenvalue Problems
</p>
<p>and
</p>
<p>uT
(
a11
α
</p>
<p>)
= |α|2 + λa12. (9.73)
</p>
<p>The first row of A is transformed by multiplication with P1 according to
</p>
<p>P1
</p>
<p>(
a11
α
</p>
<p>)
=
(
a11
α
</p>
<p>)
&minus; 2 |α|
</p>
<p>2 + λa12
|α|2 + λ2 + 2λa12
</p>
<p>[(
0
α
</p>
<p>)
+ λe(2)
</p>
<p>]
. (9.74)
</p>
<p>The elements number 3 &middot; &middot; &middot;N are eliminated if we choose3
</p>
<p>λ=&plusmn;|α| (9.75)
</p>
<p>because then
</p>
<p>2
|α|2 + λa12
</p>
<p>|α|2 + λ2 + 2λa12
= 2 |α|
</p>
<p>2 &plusmn; |α|a12
|α|2 + |α|2 &plusmn; 2|α|a12
</p>
<p>= 1 (9.76)
</p>
<p>and
</p>
<p>P1
</p>
<p>(
a11
α
</p>
<p>)
=
(
a11
α
</p>
<p>)
&minus;
(
</p>
<p>0
α
</p>
<p>)
&minus; λe(2) =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>a11
∓|α|
</p>
<p>0
...
</p>
<p>0
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠
. (9.77)
</p>
<p>Finally we have
</p>
<p>A(2) = P1AP1 =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>a11 a
(2)
12 0 &middot; &middot; &middot; 0
</p>
<p>a
(2)
12 a
</p>
<p>(2)
22 a
</p>
<p>(2)
23 &middot; &middot; &middot; a
</p>
<p>(2)
2N
</p>
<p>0 a(2)23
. . . a
</p>
<p>(2)
3N
</p>
<p>...
...
</p>
<p>. . .
...
</p>
<p>0 a(2)2N a
(2)
3N &middot; &middot; &middot; a
</p>
<p>(2)
NN
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>(9.78)
</p>
<p>as desired.
For the next step we choose
</p>
<p>α =
</p>
<p>⎛
⎜⎝
a
(2)
22
...
</p>
<p>a
(2)
2N
</p>
<p>⎞
⎟⎠ , u=
</p>
<p>⎛
⎝
</p>
<p>0
0
α
</p>
<p>⎞
⎠&plusmn; |α|e(3) (9.79)
</p>
<p>3To avoid numerical extinction we choose the sign to be that of A12.</p>
<p/>
</div>
<div class="page"><p/>
<p>9.5 Large Matrices 159
</p>
<p>to eliminate the elements a24 &middot; &middot; &middot;a2N . Note that P2 does not change the first row and
column of A(2) and therefore
</p>
<p>A(3) = P2A(2)P2 =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>a11 a
(2)
12 0 &middot; &middot; &middot; &middot; &middot; &middot; 0
</p>
<p>a
(2)
12 a
</p>
<p>(2)
22 a
</p>
<p>(3)
23 0 &middot; &middot; &middot; 0
</p>
<p>0 a(3)23 a
(3)
33 &middot; &middot; &middot; &middot; &middot; &middot; a
</p>
<p>(3)
3N
</p>
<p>... 0
...
</p>
<p>...
...
</p>
<p>...
...
</p>
<p>...
</p>
<p>0 0 a(3)3N &middot; &middot; &middot; &middot; &middot; &middot; a
(3)
NN
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>. (9.80)
</p>
<p>After N &minus; 1 transformations finally a tridiagonal matrix is obtained.
</p>
<p>9.5 Large Matrices
</p>
<p>Special algorithms are available for matrices of very large dimension to calculate
only some eigenvalues and eigenvectors. The famous Lanczos method [153] diago-
nalizes the matrix in a subspace which is constructed from the vectors
</p>
<p>x0,Ax0,A
2x0 &middot; &middot; &middot;ANx0 (9.81)
</p>
<p>which, starting from an initial normalized guess vector x0 are orthonormalized to
obtain a tridiagonal matrix,
</p>
<p>x1 =
Ax0 &minus; (x0Ax0)x0
|Ax0 &minus; (x0Ax0)x0|
</p>
<p>= Ax0 &minus; a0x0
b0
</p>
<p>x2 =
Ax1 &minus; b0x0 &minus; (x1Ax1)x1
|Ax1 &minus; b0x0 &minus; (x1Ax1)x1|
</p>
<p>= Ax1 &minus; b0x0 &minus; a1x1
b1
</p>
<p>... (9.82)
</p>
<p>xN =
AxN&minus;1 &minus; bN&minus;2xN&minus;2 &minus; (xN&minus;1AxN&minus;1)xN&minus;1
|AxN&minus;1 &minus; bN&minus;2xN&minus;2 &minus; (xN&minus;1AxN&minus;1)xN&minus;1|
</p>
<p>= AxN&minus;1 &minus; bN&minus;2xN&minus;2 &minus; aN&minus;1xN&minus;1
bN&minus;1
</p>
<p>= rN&minus;1
bN&minus;1
</p>
<p>.
</p>
<p>This series is truncated by setting
</p>
<p>aN = (xNAxN ) (9.83)
</p>
<p>and neglecting
</p>
<p>rN =AxN &minus; bN&minus;1xN&minus;1 &minus; aNxN . (9.84)</p>
<p/>
</div>
<div class="page"><p/>
<p>160 9 Eigenvalue Problems
</p>
<p>Within the subspace of the x1 &middot; &middot; &middot;xN the matrix A is represented by the tridiagonal
matrix
</p>
<p>T =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎝
</p>
<p>a0 b0
b0 a1 b1
</p>
<p>. . .
. . .
</p>
<p>. . . aN&minus;1 bN&minus;1
bN&minus;1 aN
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎠
</p>
<p>(9.85)
</p>
<p>which can be diagonalized with standard methods. The whole method can be iter-
ated using an eigenvector of T as the new starting vector and increasing N until the
desired accuracy is achieved. The main advantage of the Lanczos method is that the
matrix A will not be stored in memory. It is sufficient to calculate scalar products
with A.
</p>
<p>9.6 Problems
</p>
<p>Problem 9.1 (Computer experiment: disorder in a tight-binding model) We con-
sider a two-dimensional lattice of interacting particles. Pairs of nearest neighbors
have an interaction V and the diagonal energies are chosen from a Gaussian distri-
bution
</p>
<p>P(E)= 1
�
&radic;
</p>
<p>2π
e&minus;E
</p>
<p>2/2�2 . (9.86)
</p>
<p>The wave function of the system is given by a linear combination
</p>
<p>ψ =
&sum;
</p>
<p>ij
</p>
<p>Cijψij (9.87)
</p>
<p>where on each particle (i, j) one basis function ψij is located. The nonzero elements
of the interaction matrix are given by
</p>
<p>H(ij |ij)=Eij (9.88)
</p>
<p>H(ij |i &plusmn; 1, j)=H(ij |i, j &plusmn; 1)= V. (9.89)
The matrix H is numerically diagonalized and the amplitudes Cij of the lowest
state are shown as circles located at the grid points. As a measure of the degree of
localization the quantity
</p>
<p>&sum;
</p>
<p>ij
</p>
<p>|Cij |4 (9.90)
</p>
<p>is evaluated. Explore the influence of coupling V and disorder �.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 10
</p>
<p>Data Fitting
</p>
<p>Often a set of data points has to be fitted by a continuous function, either to obtain
approximate function values in between the data points or to describe a functional
relationship between two or more variables by a smooth curve, i.e. to fit a certain
model to the data. If uncertainties of the data are negligibly small, an exact fit is
possible, for instance with polynomials, spline functions or trigonometric functions
(Chap. 2). If the uncertainties are considerable, a curve has to be constructed that
fits the data points approximately. Consider a two-dimensional data set
</p>
<p>(xi, yi) i = 1 &middot; &middot; &middot;m (10.1)
</p>
<p>and a model function
</p>
<p>f (x, a1 &middot; &middot; &middot;an) m&ge; n (10.2)
which depends on the variable x and n&le;m additional parameters aj . The errors of
the fitting procedure are given by the residuals
</p>
<p>ri = yi &minus; f (xi, a1 &middot; &middot; &middot;an). (10.3)
</p>
<p>The parameters aj have to be determined such, that the overall error is minimized,
which in most practical cases is measured by the mean square difference1
</p>
<p>Ssd(a1 &middot; &middot; &middot;an)=
1
</p>
<p>m
</p>
<p>m&sum;
</p>
<p>i=1
r2i . (10.4)
</p>
<p>The optimal parameters are determined by solving the system of normal equa-
tions. If the model function depends linearly on the parameters, orthogonalization
offers a numerically more stable method. The dimensionality of a data matrix can be
reduced with the help of singular value decomposition, which allows to approximate
a matrix by another matrix of lower rank and is also useful for linear regression, es-
pecially if the columns of the data matrix are linearly dependent.
</p>
<p>1Minimization of the sum of absolute errors
&sum;
</p>
<p>|ri | is much more complicated.
</p>
<p>P.O.J. Scherer, Computational Physics, Graduate Texts in Physics,
DOI 10.1007/978-3-319-00401-3_10,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>161</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-00401-3_10">http://dx.doi.org/10.1007/978-3-319-00401-3_10</a></div>
</div>
<div class="page"><p/>
<p>162 10 Data Fitting
</p>
<p>10.1 Least Square Fit
</p>
<p>A (local) minimum of (10.4) corresponds to a stationary point with zero gradient.
For n model parameters there are n, generally nonlinear, equations which have to
be solved [275]. From the general condition
</p>
<p>&part;Ssd
</p>
<p>&part;aj
= 0 j = 1 &middot; &middot; &middot;n (10.5)
</p>
<p>we find
m&sum;
</p>
<p>i=1
ri
&part;f (xi, a1 &middot; &middot; &middot;an)
</p>
<p>&part;aj
= 0 (10.6)
</p>
<p>which can be solved with the methods discussed in Chap. 6. For instance, the
Newton-Raphson method starts from a suitable initial guess of parameters
</p>
<p>(
a01 &middot; &middot; &middot;a0n
</p>
<p>)
(10.7)
</p>
<p>and tries to improve the fit iteratively by making small changes to the parameters
</p>
<p>as+1j = a
s
j +�asj . (10.8)
</p>
<p>The changes �asj are determined approximately by expanding the model function
</p>
<p>f
(
xi, a
</p>
<p>s+1
1 &middot; &middot; &middot;a
</p>
<p>s+1
n
</p>
<p>)
= f
</p>
<p>(
xi, a
</p>
<p>s
1 &middot; &middot; &middot;asn
</p>
<p>)
+
</p>
<p>n&sum;
</p>
<p>j=1
</p>
<p>&part;f (xi, a
s
1 &middot; &middot; &middot;asn)
</p>
<p>&part;aj
�asj + &middot; &middot; &middot; (10.9)
</p>
<p>to approximate the new residuals
</p>
<p>rs+1i = r
s
i &minus;
</p>
<p>n&sum;
</p>
<p>j=1
</p>
<p>&part;f (xi, a
s
1 &middot; &middot; &middot;asm)
</p>
<p>&part;aj
�asj (10.10)
</p>
<p>and the derivatives
</p>
<p>&part;rsi
</p>
<p>&part;aj
=&minus;
</p>
<p>&part;f (xi, a
s
1 &middot; &middot; &middot;asm)
</p>
<p>&part;aj
. (10.11)
</p>
<p>Equation (10.6) now becomes
</p>
<p>m&sum;
</p>
<p>i=1
</p>
<p>(
rsi &minus;
</p>
<p>n&sum;
</p>
<p>j=1
</p>
<p>&part;f (xi)
</p>
<p>&part;aj
�asj
</p>
<p>)
&part;f (xi)
</p>
<p>&part;ak
(10.12)
</p>
<p>which is a system of n (usually overdetermined) linear equations for the �aj , the
so-called normal equations:
</p>
<p>m&sum;
</p>
<p>i=1
</p>
<p>n&sum;
</p>
<p>j=1
</p>
<p>&part;f (xi)
</p>
<p>&part;aj
</p>
<p>&part;f (xi)
</p>
<p>&part;ak
�asj =
</p>
<p>m&sum;
</p>
<p>i=1
rsi
&part;f (xi)
</p>
<p>&part;ak
. (10.13)</p>
<p/>
</div>
<div class="page"><p/>
<p>10.1 Least Square Fit 163
</p>
<p>With the definition
</p>
<p>Akj =
1
</p>
<p>m
</p>
<p>m&sum;
</p>
<p>i=1
</p>
<p>&part;f (xi)
</p>
<p>&part;ak
</p>
<p>&part;f (xi)
</p>
<p>&part;aj
(10.14)
</p>
<p>bk =
1
</p>
<p>m
</p>
<p>m&sum;
</p>
<p>i=1
yi
&part;f (xi)
</p>
<p>&part;ak
(10.15)
</p>
<p>the normal equations can be written as
</p>
<p>n&sum;
</p>
<p>j=1
Akj�aj = bk. (10.16)
</p>
<p>10.1.1 Linear Least Square Fit
</p>
<p>Especially important are model functions which depend linearly on all parameters
(Fig. 10.1 shows an example which is discussed in Problem 10.1)
</p>
<p>f (x, a1 &middot; &middot; &middot;an)=
n&sum;
</p>
<p>j=1
ajfj (x). (10.17)
</p>
<p>The derivatives are
</p>
<p>&part;f (xi)
</p>
<p>&part;aj
= fj (xi) (10.18)
</p>
<p>and the minimum of (10.4) is given by the solution of the normal equations
</p>
<p>1
</p>
<p>m
</p>
<p>n&sum;
</p>
<p>j=1
</p>
<p>m&sum;
</p>
<p>i=1
fk(xi)fj (xi)aj =
</p>
<p>1
</p>
<p>m
</p>
<p>m&sum;
</p>
<p>i=1
yifk(xi) (10.19)
</p>
<p>which for a linear fit problem become
</p>
<p>n&sum;
</p>
<p>j=1
Akjaj = bk (10.20)
</p>
<p>with
</p>
<p>Akj =
1
</p>
<p>m
</p>
<p>m&sum;
</p>
<p>i=1
fk(xi)fj (xi) (10.21)
</p>
<p>bk =
1
</p>
<p>m
</p>
<p>m&sum;
</p>
<p>i=1
yifk(xi). (10.22)</p>
<p/>
</div>
<div class="page"><p/>
<p>164 10 Data Fitting
</p>
<p>Fig. 10.1 (Least square fit)
The polynomial
C(T )= aT + bT 3 (full
curve) is fitted to a set of data
points which are distributed
randomly around the &ldquo;exact&rdquo;
values C(T )= a0T + b0T 3
(dashed curve). For more
details see Problem 10.1
</p>
<p>Example (Linear regression) For a linear fit function
</p>
<p>f (x)= a0 + a1x (10.23)
</p>
<p>the mean square difference is
</p>
<p>Ssd =
1
</p>
<p>m
</p>
<p>m&sum;
</p>
<p>i=1
(yi &minus; a0 &minus; a1xi)2 (10.24)
</p>
<p>and we have to solve the equations
</p>
<p>0 = &part;Ssd
&part;a0
</p>
<p>= 1
m
</p>
<p>m&sum;
</p>
<p>i=1
(yi &minus; a0 &minus; a1xi)= y &minus; a0 &minus; a1x
</p>
<p>(10.25)
</p>
<p>0 = &part;Ssd
&part;a1
</p>
<p>= 1
m
</p>
<p>m&sum;
</p>
<p>i=1
(yi &minus; a0 &minus; a1xi)xi = xy &minus; a0x &minus; a1x2
</p>
<p>which can be done here with determinants
</p>
<p>a0 =
</p>
<p>∣∣ y x
xy x2
</p>
<p>∣∣
∣∣ 1 x
x x2
</p>
<p>∣∣ =
y x2 &minus; x xy
x2 &minus; x2
</p>
<p>(10.26)
</p>
<p>a1 =
∣∣ 1 y
x xy
</p>
<p>∣∣
∣∣ 1 x
x x2
</p>
<p>∣∣ =
xy &minus; x y
x2 &minus; x2
</p>
<p>. (10.27)</p>
<p/>
</div>
<div class="page"><p/>
<p>10.1 Least Square Fit 165
</p>
<p>10.1.2 Linear Least Square Fit with Orthogonalization
</p>
<p>With the definitions
</p>
<p>x=
</p>
<p>⎛
⎜⎝
a1
...
</p>
<p>an
</p>
<p>⎞
⎟⎠ b=
</p>
<p>⎛
⎜⎝
</p>
<p>y1
...
</p>
<p>ym
</p>
<p>⎞
⎟⎠ (10.28)
</p>
<p>and the m&times; n matrix
</p>
<p>A=
</p>
<p>⎛
⎜⎝
</p>
<p>a11 &middot; &middot; &middot; a1n
...
</p>
<p>. . .
...
</p>
<p>am1 &middot; &middot; &middot; amn
</p>
<p>⎞
⎟⎠=
</p>
<p>⎛
⎜⎝
</p>
<p>f1(x1) &middot; &middot; &middot; fn(x1)
...
</p>
<p>. . .
...
</p>
<p>f1(xm) &middot; &middot; &middot; fn(xm)
</p>
<p>⎞
⎟⎠ (10.29)
</p>
<p>the linear least square fit problem (10.20) can be formulated as a search for the
minimum of
</p>
<p>|Ax&minus; b| =
&radic;
(Ax&minus; b)T (Ax&minus; b). (10.30)
</p>
<p>In the last section we calculated the gradient
</p>
<p>&part;|Ax&minus; b|2
&part;x
</p>
<p>=AT (Ax&minus; b)+ (Ax&minus; b)TA= 2ATAx&minus; 2AT b (10.31)
</p>
<p>and solved the normal equations
</p>
<p>ATAx=AT b. (10.32)
</p>
<p>This method can become numerically unstable. Alternatively we use orthogonaliza-
tion of the n column vectors ak of A to have
</p>
<p>A= (a1 &middot; &middot; &middot;an)= (q1 &middot; &middot; &middot;qn)
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>r11 r12 &middot; &middot; &middot; r1n
r22 &middot; &middot; &middot; r2n
</p>
<p>. . .
...
</p>
<p>rnn
</p>
<p>⎞
⎟⎟⎟⎠ (10.33)
</p>
<p>where ak and qk are now vectors of dimension m. Since the qk are orthonormal
qTi qk = δik we have
</p>
<p>⎛
⎜⎝
qT1
...
</p>
<p>qTn
</p>
<p>⎞
⎟⎠A=
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>r11 r12 &middot; &middot; &middot; r1n
r22 &middot; &middot; &middot; r2n
</p>
<p>. . .
...
</p>
<p>rnn
</p>
<p>⎞
⎟⎟⎟⎠ . (10.34)
</p>
<p>The qk can be augmented by another (m &minus; n) vectors to provide an orthonormal
basis of Rm. These will not be needed explicitly. They are orthogonal to the first</p>
<p/>
</div>
<div class="page"><p/>
<p>166 10 Data Fitting
</p>
<p>n vectors and hence to the column vectors of A. All vectors qk together form an
orthogonal matrix
</p>
<p>Q=
(
q1 &middot; &middot; &middot; qn qn+1 &middot; &middot; &middot; qm
</p>
<p>)
(10.35)
</p>
<p>and we can define the transformation of the matrix A:
</p>
<p>Ã=
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>qT1
...
</p>
<p>qTn
</p>
<p>qTn+1
...
</p>
<p>qTm
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>(a1 &middot; &middot; &middot;an)=QTA=
(
R
</p>
<p>0
</p>
<p>)
R =
</p>
<p>⎛
⎜⎝
r11 &middot; &middot; &middot; r1n
</p>
<p>. . .
...
</p>
<p>rnn
</p>
<p>⎞
⎟⎠ . (10.36)
</p>
<p>The vector b transforms as
</p>
<p>b̃=QT b=
(
bu
bl
</p>
<p>)
bu =
</p>
<p>⎛
⎜⎝
qT1
...
</p>
<p>qTn
</p>
<p>⎞
⎟⎠b bl =
</p>
<p>⎛
⎜⎝
qTn+1
...
</p>
<p>qTm
</p>
<p>⎞
⎟⎠b. (10.37)
</p>
<p>Since the norm of a vector is not changed by unitary transformations
</p>
<p>|b&minus;Ax| =
&radic;
(bu &minus;Rx)2 + b2l (10.38)
</p>
<p>which is minimized if
</p>
<p>Rx= bu. (10.39)
The error of the fit is given by
</p>
<p>|b&minus;Ax| = |bl |. (10.40)
</p>
<p>Example (Linear regression) Consider again the fit function
</p>
<p>f (x)= a0 + a1x (10.41)
</p>
<p>for the measured data (xi, yi). The fit problem is to determine
∣∣∣∣∣∣∣
</p>
<p>⎛
⎜⎝
</p>
<p>1 x1
...
</p>
<p>...
</p>
<p>1 xm
</p>
<p>⎞
⎟⎠
(
a0
a1
</p>
<p>)
&minus;
</p>
<p>⎛
⎜⎝
</p>
<p>y1
...
</p>
<p>ym
</p>
<p>⎞
⎟⎠
</p>
<p>∣∣∣∣∣∣∣
= min . (10.42)
</p>
<p>Orthogonalization of the column vectors
</p>
<p>a1 =
</p>
<p>⎛
⎜⎝
</p>
<p>1
...
</p>
<p>1
</p>
<p>⎞
⎟⎠ a2 =
</p>
<p>⎛
⎜⎝
</p>
<p>x1
...
</p>
<p>xm
</p>
<p>⎞
⎟⎠ (10.43)</p>
<p/>
</div>
<div class="page"><p/>
<p>10.2 Singular Value Decomposition 167
</p>
<p>with the Schmidt method gives:
</p>
<p>r11 =
&radic;
m (10.44)
</p>
<p>q1 =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>1&radic;
m
</p>
<p>...
1&radic;
m
</p>
<p>⎞
⎟⎟⎠ (10.45)
</p>
<p>r12 =
1&radic;
m
</p>
<p>m&sum;
</p>
<p>i=1
xi =
</p>
<p>&radic;
mx (10.46)
</p>
<p>b2 = (xi &minus; x) (10.47)
</p>
<p>r22 =
&radic;&sum;
</p>
<p>(xi &minus; x)2 =
&radic;
mσx (10.48)
</p>
<p>q2 =
(
xi &minus; x&radic;
mσx
</p>
<p>)
. (10.49)
</p>
<p>Transformation of the right hand side gives
</p>
<p>(
qT1
</p>
<p>qT2
</p>
<p>)⎛
⎜⎝
</p>
<p>y1
...
</p>
<p>ym
</p>
<p>⎞
⎟⎠=
</p>
<p>( &radic;
my
</p>
<p>&radic;
m
</p>
<p>xy&minus;x y
σx
</p>
<p>)
(10.50)
</p>
<p>and we have to solve the system of linear equations
</p>
<p>Rx=
(&radic;
</p>
<p>m
&radic;
mx
</p>
<p>0
&radic;
mσ
</p>
<p>)(
a0
a1
</p>
<p>)
=
( &radic;
</p>
<p>my&radic;
m
</p>
<p>xy&minus;x y
σx
</p>
<p>)
. (10.51)
</p>
<p>The solution
</p>
<p>a1 =
xy &minus; x y
(x &minus; x)2
</p>
<p>(10.52)
</p>
<p>a0 = y &minus; xa1 =
y x2 &minus; x xy
(x &minus; x)2
</p>
<p>(10.53)
</p>
<p>coincides with the earlier results since
</p>
<p>(x &minus; x)2 = x2 &minus; x2. (10.54)
</p>
<p>10.2 Singular Value Decomposition
</p>
<p>Computational physics often has to deal with large amounts of data. Singular value
decomposition is a very useful tool to reduce redundancies and to extract the most</p>
<p/>
</div>
<div class="page"><p/>
<p>168 10 Data Fitting
</p>
<p>important information from data. It has been used for instance for image compres-
sion [216], it is very useful to extract the essential dynamics from molecular dynam-
ics simulations [99, 221] and it is an essential tool of bio-informatics [71].
</p>
<p>10.2.1 Full Singular Value Decomposition
</p>
<p>For m &ge; n,2 any real3 m &times; n matrix A of rank r &le; n can be decomposed into a
product
</p>
<p>A=UΣV T (10.55)
</p>
<p>⎛
⎜⎝
</p>
<p>a11 &middot; &middot; &middot; a1n
...
</p>
<p>. . .
...
</p>
<p>am1 &middot; &middot; &middot; amn
</p>
<p>⎞
⎟⎠=
</p>
<p>⎛
⎜⎝
</p>
<p>u11 &middot; &middot; &middot; u1m
...
</p>
<p>. . .
...
</p>
<p>um1 &middot; &middot; &middot; umm
</p>
<p>⎞
⎟⎠
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>s1
. . .
</p>
<p>sn
0 &middot; &middot; &middot; 0
</p>
<p>⎞
⎟⎟⎟⎠
</p>
<p>⎛
⎜⎝
v11 &middot; &middot; &middot; vn1
...
</p>
<p>. . .
...
</p>
<p>v1n &middot; &middot; &middot; vnn
</p>
<p>⎞
⎟⎠
</p>
<p>(10.56)
where U is an m&times;m orthogonal matrix, Σ is an m&times; n matrix, in which the upper
part is an n&times; n diagonal matrix and V is an orthogonal n&times; n matrix.
</p>
<p>The diagonal elements si are called singular values. Conventionally, they are
sorted in descending order and the last n&minus; r of them are zero. For a square n&times; n
matrix singular value decomposition (10.56) is equivalent to diagonalization
</p>
<p>A=USUT . (10.57)
</p>
<p>10.2.2 Reduced Singular Value Decomposition
</p>
<p>We write
</p>
<p>U = (Un,Um&minus;n) (10.58)
with the m&times; n matrix Un and the m&times; (m&minus; n) matrix Um&minus;n and
</p>
<p>Σ =
(
S
</p>
<p>0
</p>
<p>)
(10.59)
</p>
<p>with the diagonal n&times; n matrix S. The singular value decomposition then becomes
</p>
<p>A= (Un,Um&minus;n)
(
S
</p>
<p>0
</p>
<p>)
V T =UnSV T (10.60)
</p>
<p>2Otherwise consider the transpose matrix.
3Generalization to complex matrices is straightforward.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.2 Singular Value Decomposition 169
</p>
<p>which is known as reduced singular value decomposition. Un (usually simply de-
noted by U ) is not unitary but its column vectors, called the left singular vectors,
are orthonormal
</p>
<p>m&sum;
</p>
<p>i=1
ui,rui,s = δr,s (10.61)
</p>
<p>as well as the column vectors of V which are called the right singular vectors
</p>
<p>n&sum;
</p>
<p>i=1
vi,rvi,s = δr,s . (10.62)
</p>
<p>Hence the products
</p>
<p>UTn Un = V T V =En (10.63)
give the n&times; n unit matrix.
</p>
<p>In principle, U and V can be obtained from diagonalization of ATA and AAT ,
since
</p>
<p>ATA=
(
VΣTUT
</p>
<p>)(
UΣV T
</p>
<p>)
= V (S,0)
</p>
<p>(
S
</p>
<p>0
</p>
<p>)
V T = V S2V T (10.64)
</p>
<p>AAT =
(
UΣV T
</p>
<p>)(
VΣTUT
</p>
<p>)
=U
</p>
<p>(
S
</p>
<p>0
</p>
<p>)
(S,0)UT =UnS2UTn . (10.65)
</p>
<p>However, calculation of U by diagonalization is very inefficient, since usually only
the first n rows are needed (i.e. Un). To perform a reduced singular value decompo-
sition, we first diagonalize
</p>
<p>ATA= VDV T (10.66)
which has positive eigenvalues di &ge; 0, sorted in descending order and obtain the
singular values
</p>
<p>S =D1/2 =
</p>
<p>⎛
⎜⎝
</p>
<p>&radic;
d1
</p>
<p>. . . &radic;
dn
</p>
<p>⎞
⎟⎠ . (10.67)
</p>
<p>Now we determine a matrix U such, that
</p>
<p>A=USV T (10.68)
</p>
<p>or, since V is unitary
</p>
<p>Y =AV =US. (10.69)
The last n&minus; r singular values are zero if r &lt; n. Therefore we partition the matrices
(indices denote the number of rows)
</p>
<p>(
Yr 0n&minus;r
</p>
<p>)
=
(
Ur Un&minus;r
</p>
<p>)(Sr
0n&minus;r
</p>
<p>)
=
(
UrSr 0
</p>
<p>)
. (10.70)</p>
<p/>
</div>
<div class="page"><p/>
<p>170 10 Data Fitting
</p>
<p>We retain only the first r columns and obtain a system of equations
</p>
<p>⎛
⎜⎝
</p>
<p>y11 &middot; &middot; &middot; y1r
...
</p>
<p>. . .
...
</p>
<p>ym1 &middot; &middot; &middot; ymr
</p>
<p>⎞
⎟⎠=
</p>
<p>⎛
⎜⎝
</p>
<p>u11 &middot; &middot; &middot; u1r
...
</p>
<p>. . .
...
</p>
<p>um1 &middot; &middot; &middot; umr
</p>
<p>⎞
⎟⎠
</p>
<p>⎛
⎜⎝
s1
</p>
<p>. . .
</p>
<p>sr
</p>
<p>⎞
⎟⎠ (10.71)
</p>
<p>which can be easily solved to give the first r rows of U
</p>
<p>⎛
⎜⎝
</p>
<p>u11 &middot; &middot; &middot; u1r
...
</p>
<p>. . .
...
</p>
<p>um1 &middot; &middot; &middot; umr
</p>
<p>⎞
⎟⎠=
</p>
<p>⎛
⎜⎝
</p>
<p>y11 &middot; &middot; &middot; y1n
...
</p>
<p>. . .
...
</p>
<p>ym1 &middot; &middot; &middot; ymn
</p>
<p>⎞
⎟⎠
</p>
<p>⎛
⎜⎝
s&minus;11
</p>
<p>. . .
</p>
<p>s&minus;1r
</p>
<p>⎞
⎟⎠ . (10.72)
</p>
<p>The remaining n &minus; r column vectors of U have to be orthogonal to the first r
columns but are otherwise arbitrary. They can be obtained for instance by the Gram
Schmidt method.
</p>
<p>For larger matrices direct decomposition algorithms are available, for instance
[72], which is based on a reduction to bidiagonal form and a variant of the QL
algorithm as first introduced by Golub and Kahan [107].
</p>
<p>10.2.3 Low Rank Matrix Approximation
</p>
<p>Component-wise (10.60) reads
</p>
<p>ai,j =
r&sum;
</p>
<p>k=1
ui,kskvj,k. (10.73)
</p>
<p>Approximations to A of lower rank are obtained by reducing the sum to only the
largest singular values (the smaller singular values are replaced by zero). It can be
shown [243] that the matrix of rank l &le; r
</p>
<p>a
(l)
i,j =
</p>
<p>l&sum;
</p>
<p>k=1
ui,kskvj,k (10.74)
</p>
<p>is the rank-l matrix which minimizes
&sum;
</p>
<p>i,j
</p>
<p>∣∣ai,j &minus; a(l)i,j
∣∣2. (10.75)
</p>
<p>If only the largest singular value is taken into account, A is approximated by the
rank-1 matrix
</p>
<p>a
(1)
i,j = s1ui,1vj,1. (10.76)</p>
<p/>
</div>
<div class="page"><p/>
<p>10.2 Singular Value Decomposition 171
</p>
<p>As an example, consider an m&times; n matrix
</p>
<p>A=
</p>
<p>⎛
⎜⎝
</p>
<p>x1(t1) &middot; &middot; &middot; xn(t1)
...
</p>
<p>...
</p>
<p>x1(tm) &middot; &middot; &middot; xn(tm)
</p>
<p>⎞
⎟⎠ (10.77)
</p>
<p>which contains the values of certain quantities x1 &middot; &middot; &middot;xn observed at different times
t1 &middot; &middot; &middot; tm. For convenience, we assume that the average values have been subtracted,
such that
</p>
<p>&sum;m
j=1 xi = 0. Approximation (10.76) reduces the dimensionality to 1, i.e.
</p>
<p>a linear relation between the data. The i-th row of A,
(
x1(ti) &middot; &middot; &middot; xn(ti)
</p>
<p>)
(10.78)
</p>
<p>is approximated by
</p>
<p>s1 ui,1
(
v1,1 &middot; &middot; &middot; vn,1
</p>
<p>)
(10.79)
</p>
<p>which describes a direct proportionality of different observables
</p>
<p>1
</p>
<p>vj,1
xj (ti)=
</p>
<p>1
</p>
<p>vk,1
xk(ti). (10.80)
</p>
<p>According to (10.75) this linear relation minimizes the mean square distance
between the data points (10.78) and their approximation (10.79).
</p>
<p>Example (Linear approximation [165]) Consider the data matrix
</p>
<p>AT =
(
</p>
<p>1 2 3 4 5
1 2.5 3.9 3.5 4.0
</p>
<p>)
. (10.81)
</p>
<p>First subtract the row averages
</p>
<p>x = 3 y = 2.98 (10.82)
</p>
<p>to obtain
</p>
<p>AT =
(
</p>
<p>&minus;2 &minus;1 0 1 2
&minus;1.98 &minus;0.48 0.92 0.52 1.02
</p>
<p>)
. (10.83)
</p>
<p>Diagonalization of
</p>
<p>ATA=
(
</p>
<p>10.00 7.00
7.00 6.308
</p>
<p>)
(10.84)
</p>
<p>gives the eigenvalues
</p>
<p>d1 = 15.393 d2 = 0.915 (10.85)
and the eigenvectors
</p>
<p>V =
(
</p>
<p>0.792 &minus;0.610
0.610 &minus;0.792
</p>
<p>)
. (10.86)</p>
<p/>
</div>
<div class="page"><p/>
<p>172 10 Data Fitting
</p>
<p>Since there are no zero singular values we find
</p>
<p>U =AVS&minus;1 =
</p>
<p>⎛
⎜⎜⎜⎜⎝
</p>
<p>&minus;0.181 &minus;0.380
&minus;0.070 0.252
0.036 0.797
0.072 &minus;0.217
0.143 &minus;0.451
</p>
<p>⎞
⎟⎟⎟⎟⎠
. (10.87)
</p>
<p>This gives the decomposition4
</p>
<p>A =
(
u1 u2
</p>
<p>)( s1
s2
</p>
<p>)(
vT1
</p>
<p>vT2
</p>
<p>)
= s1u1vT1 + s2u2vT2
</p>
<p>=
</p>
<p>⎛
⎜⎜⎜⎜⎝
</p>
<p>&minus;2.212 &minus;1.704
&minus;0.860 &minus;0.662
0.445 0.343
0.879 0.677
1.748 1.347
</p>
<p>⎞
⎟⎟⎟⎟⎠
+
</p>
<p>⎛
⎜⎜⎜⎜⎝
</p>
<p>0.212 &minus;0.276
&minus;0.140 0.182
&minus;0.445 0.577
0.121 &minus;0.157
0.252 &minus;0.327
</p>
<p>⎞
⎟⎟⎟⎟⎠
. (10.88)
</p>
<p>If we neglect the second contribution corresponding to the small singular value s2
we have an approximation of the data matrix by a rank-1 matrix. The column vectors
of the data matrix, denoted as x and y, are approximated by
</p>
<p>x= s1v11u1 y= s1v21u1 (10.89)
</p>
<p>which describes a proportionality between x and y (Fig. 10.2).
</p>
<p>10.2.4 Linear Least Square Fit with Singular Value Decomposition
</p>
<p>The singular value decomposition can be used for linear regression [165]. Consider
a set of data, which have to be fitted to a linear function
</p>
<p>y = c0 + c1x1 + &middot; &middot; &middot; + cnxn (10.90)
</p>
<p>with the residual
</p>
<p>ri = c0 + c1xi,1 + &middot; &middot; &middot; + cnxi,n &minus; yi . (10.91)
</p>
<p>Let us subtract the averages
</p>
<p>ri &minus; r = c1(xi,1 &minus; x1)+ &middot; &middot; &middot; + cn(xi,n &minus; xn)&minus; (yi &minus; y) (10.92)
</p>
<p>4uiv
T
i is the outer or matrix product of two vectors.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.2 Singular Value Decomposition 173
</p>
<p>Fig. 10.2 (Linear approximation by singular value decomposition) The data set (10.81) is shown
as circles. The linear approximation which is obtained by retaining only the dominant singular
value is shown by the squares and the full line. It minimizes the mean square distance to the data
points. Stars and the dashed line show the approximation by linear regression, which minimizes
the mean square distance in vertical direction
</p>
<p>which we write in matrix notation as
</p>
<p>⎛
⎜⎝
</p>
<p>r1 &minus; r
...
</p>
<p>rm &minus; r
</p>
<p>⎞
⎟⎠=
</p>
<p>⎛
⎜⎝
</p>
<p>x1,1 &minus; x1 &middot; &middot; &middot; x1,n &minus; xn
...
</p>
<p>xm,1 &minus; x1 &middot; &middot; &middot; xm,n &minus; xn
</p>
<p>⎞
⎟⎠
</p>
<p>⎛
⎜⎝
c1
...
</p>
<p>cn
</p>
<p>⎞
⎟⎠&minus;
</p>
<p>⎛
⎜⎝
</p>
<p>y1 &minus; y
...
</p>
<p>ym &minus; y
</p>
<p>⎞
⎟⎠ (10.93)
</p>
<p>or shorter
</p>
<p>r=Xc&minus; y. (10.94)
</p>
<p>Now let us insert the full decomposition of X
</p>
<p>r=UΣV T c&minus; y. (10.95)
</p>
<p>Since U is orthogonal
</p>
<p>UT r=ΣV T c&minus;UT y=Σa&minus; b (10.96)
</p>
<p>where we introduce the abbreviations
</p>
<p>a= V T c b=UT y. (10.97)
</p>
<p>The sum of squared residuals has the form</p>
<p/>
</div>
<div class="page"><p/>
<p>174 10 Data Fitting
</p>
<p>|r|2 =
∣∣UT r
</p>
<p>∣∣2 =
∣∣∣∣
(
Sr 0
0 0
</p>
<p>)(
ar
</p>
<p>an&minus;r
</p>
<p>)
&minus;
(
</p>
<p>br
bn&minus;r
</p>
<p>)∣∣∣∣
2
</p>
<p>= |Srar &minus; br |2 + b2n&minus;r &le; |Srar &minus; br |2. (10.98)
</p>
<p>Hence an&minus;r is arbitrary and one minimum of SD is given by
</p>
<p>ar = S&minus;1r br an&minus;r = 0 (10.99)
</p>
<p>which can be written more compactly as
</p>
<p>a=Σ+b (10.100)
</p>
<p>with the Moore-Penrose pseudoinverse [20] of Σ
</p>
<p>Σ+ =
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>s&minus;11
. . .
</p>
<p>s&minus;1r
0
</p>
<p>⎞
⎟⎟⎟⎠ . (10.101)
</p>
<p>Finally we have
</p>
<p>c= VΣ+UT y=X+y (10.102)
where
</p>
<p>X+ = VΣ+UT (10.103)
is the Moore-Penrose pseudoinverse of X.
</p>
<p>Example The following data matrix has rank 2
</p>
<p>X =
</p>
<p>⎛
⎜⎜⎜⎜⎝
</p>
<p>&minus;3 &minus;4 &minus;5
&minus;2 &minus;3 &minus;4
0 0 0
2 3 4
3 4 5
</p>
<p>⎞
⎟⎟⎟⎟⎠
</p>
<p>y=
</p>
<p>⎛
⎜⎜⎜⎜⎝
</p>
<p>1.0
1.1
0
</p>
<p>&minus;1.0
&minus;1.1
</p>
<p>⎞
⎟⎟⎟⎟⎠
. (10.104)
</p>
<p>A solution to the linear fit problem is given by
</p>
<p>c=X+y =
</p>
<p>⎛
⎝
&minus;0.917 1.167 0 &minus;1.167 0.917
&minus;0.167 0.167 0 &minus;0.167 0.167
0.583 &minus;0.833 0 0.833 &minus;0.583
</p>
<p>⎞
⎠
</p>
<p>⎛
⎜⎜⎜⎜⎝
</p>
<p>1.0
1.1
0
</p>
<p>&minus;1.0
&minus;1.1
</p>
<p>⎞
⎟⎟⎟⎟⎠
</p>
<p>=
</p>
<p>⎛
⎝
</p>
<p>0.525
0.000
&minus;0.525
</p>
<p>⎞
⎠ . (10.105)</p>
<p/>
</div>
<div class="page"><p/>
<p>10.3 Problems 175
</p>
<p>The fit function is
</p>
<p>y = 0.525(x1 &minus; x3) (10.106)
and the residuals are
</p>
<p>Xc&minus; y=
</p>
<p>⎛
⎜⎜⎜⎜⎝
</p>
<p>0.05
&minus;0.05
</p>
<p>0
&minus;0.05
0.05
</p>
<p>⎞
⎟⎟⎟⎟⎠
. (10.107)
</p>
<p>10.3 Problems
</p>
<p>Problem 10.1 (Least square fit) At temperatures far below Debye and Fermi tem-
peratures the specific heat of a metal contains contributions from electrons and lat-
tice vibrations and can be described by
</p>
<p>C(T )= aT + bT 3. (10.108)
</p>
<p>The computer experiment generates data
</p>
<p>Tj = T0 + j�t (10.109)
</p>
<p>Cj =
(
a0Tj + b0T 3j
</p>
<p>)
(1 + εj ) (10.110)
</p>
<p>with relative error
</p>
<p>εj = εξj . (10.111)
Random numbers ξj are taken from a Gaussian normal distribution function
(Sect. 8.2.4.4).
</p>
<p>The fit parameters a, b are determined from minimization of the sum of squares
</p>
<p>S = 1
n
</p>
<p>n&sum;
</p>
<p>j=1
</p>
<p>(
Cj &minus; aTi &minus; bT 3i
</p>
<p>)2
. (10.112)
</p>
<p>Compare the &ldquo;true values&rdquo; a0, b0 with the fitted values a, b.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 11
</p>
<p>Discretization of Differential Equations
</p>
<p>Many processes in science and technology can be described by differential equations
involving the rate of changes in time or space of a continuous variable, the unknown
function. While the simplest differential equations can be solved exactly, a numer-
ical treatment is necessary in most cases and the equations have to be discretized
to turn them into a finite system of equations which can be solved by computers
[6, 155, 200]. In this chapter we discuss different methods to discretize differential
equations. The simplest approach is the method of finite differences, which replaces
the differential quotients by difference quotients (Chap. 3). It is often used for the
discretization of time. Finite difference methods for the space variables work best
on a regular grid. Finite volume methods are very popular in computational fluid dy-
namics. They take averages over small control volumes and can be easily used with
irregular grids. Finite differences and finite volumes belong to the general class of
finite element methods which are prominent in the engineering sciences and use an
expansion in piecewise polynomials with small support. Spectral methods, on the
other hand, expand the solution as a linear combination of global basis functions
like polynomials or trigonometric functions. A general concept for the discretiza-
tion of differential equations is the method of weighted residuals which minimizes
the weighted residual of a numerical solution. Most popular is Galerkin&rsquo;s method
which uses the expansion functions also as weight functions. Simpler are the point
collocation and sub-domain collocation methods which fulfill the differential equa-
tion only at certain points or averaged over certain control volumes. More demand-
ing is the least-squares method which has become popular in computational fluid
dynamics and computational electrodynamics. The least-square integral provides a
measure for the quality of the solution which can be used for adaptive grid size
control.
</p>
<p>If the Green&rsquo;s function is available for a problem, the method of boundary el-
ements is an interesting alternative. It reduces the dimensionality and is, for in-
stance, very popular in chemical physics to solve the Poisson-Boltzmann equa-
tion.
</p>
<p>P.O.J. Scherer, Computational Physics, Graduate Texts in Physics,
DOI 10.1007/978-3-319-00401-3_11,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>177</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-00401-3_11">http://dx.doi.org/10.1007/978-3-319-00401-3_11</a></div>
</div>
<div class="page"><p/>
<p>178 11 Discretization of Differential Equations
</p>
<p>11.1 Classification of Differential Equations
</p>
<p>An ordinary differential equation (ODE) is a differential equation for a function of
one single variable, like Newton&rsquo;s law for the motion of a body under the influence
of a force field
</p>
<p>m
d2
</p>
<p>dt2
x(t)= F(x, t), (11.1)
</p>
<p>a typical initial value problem where the solution in the domain t0 &le; t &le; T is deter-
mined by position and velocity at the initial time
</p>
<p>x(t = t0)= x0
d
</p>
<p>dt
x(t = t0)= v0. (11.2)
</p>
<p>Such equations of motion are discussed in Chap. 12. They also appear if the
spatial derivatives of a partial differential equation have been discretized. Usually
this kind of equation is solved by numerical integration over finite time steps �t =
tn+1 &minus; tn. Boundary value problems, on the other hand, require certain boundary
conditions1 to be fulfilled, for instance the linearized Poisson-Boltzmann equation
in one dimension (Chap. 17)
</p>
<p>d2
</p>
<p>dx2
Φ &minus; κ2Φ =&minus;1
</p>
<p>ε
ρ(x) (11.3)
</p>
<p>where the value of the potential is prescribed on the boundary of the domain x0 &le;
x &le; x1
</p>
<p>Φ(x0)=Φ0 Φ(x1)=Φ1. (11.4)
Partial differential equations (PDE) finally involve partial derivatives with respect
</p>
<p>to at least two different variables, in many cases time and spatial coordinates.
</p>
<p>11.1.1 Linear Second Order PDE
</p>
<p>A very important class are second order linear partial differential equations of the
general form
</p>
<p>[
N&sum;
</p>
<p>i=1
</p>
<p>N&sum;
</p>
<p>j=1
aij
</p>
<p>&part;2
</p>
<p>&part;xi&part;xj
+
</p>
<p>N&sum;
</p>
<p>i=1
bi
</p>
<p>&part;
</p>
<p>&part;xi
+ c
</p>
<p>]
f (x1 . . . xN )+ d = 0 (11.5)
</p>
<p>where the coefficients aij , bi, c, d are functions of the variables x1 . . . xN but do not
depend on the function f itself. The equation is classified according to the eigen-
values of the coefficient matrix aij as [141]
</p>
<p>1Dirichlet b.c. concern the function values, Neumann b.c. the derivative, Robin b.c. a linear com-
bination of both, Cauchy b.c. the function value and the normal derivative and mixed b.c. have
different character on different parts of the boundary.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.1 Classification of Differential Equations 179
</p>
<p>&bull; elliptical if all eigenvalues are positive or all eigenvalues are negative, like for the
Poisson equation (Chap. 17)
</p>
<p>(
&part;2
</p>
<p>&part;x2
+ &part;
</p>
<p>2
</p>
<p>&part;y2
+ &part;
</p>
<p>2
</p>
<p>&part;z2
</p>
<p>)
Φ(x,y, z)=&minus;1
</p>
<p>ε
ρ(x, y, z) (11.6)
</p>
<p>&bull; hyperbolic if one eigenvalue is negative and all the other eigenvalues are positive
or vice versa, for example the wave equation in one spatial dimension (Chap. 18)
</p>
<p>&part;2
</p>
<p>&part;t2
f &minus; c2 &part;
</p>
<p>2
</p>
<p>&part;x2
f = 0 (11.7)
</p>
<p>&bull; parabolic if at least one eigenvalue is zero, like for the diffusion equation
(Chap. 19)
</p>
<p>&part;
</p>
<p>&part;t
f (x, y, z, t)&minus;D
</p>
<p>(
&part;2
</p>
<p>&part;x2
+ &part;
</p>
<p>2
</p>
<p>&part;y2
+ &part;
</p>
<p>2
</p>
<p>&part;z2
</p>
<p>)
f (x, y, z, t)= S(x, y, z, t)
</p>
<p>(11.8)
</p>
<p>&bull; ultra-hyperbolic if there is no zero eigenvalue and more than one positive as well
as more than one negative eigenvalue. Obviously the dimension then must be 4 at
least.
</p>
<p>11.1.2 Conservation Laws
</p>
<p>One of the simplest first order partial differential equations is the advection equation
</p>
<p>&part;
</p>
<p>&part;t
f (x, t)+ u &part;
</p>
<p>&part;x
f (x, t)= 0 (11.9)
</p>
<p>which describes transport of a conserved quantity with density f (for instance mass,
number of particles, charge etc.) in a medium streaming with velocity u. This is a
special case of the class of conservation laws (also called continuity equations)
</p>
<p>&part;
</p>
<p>&part;t
f (x, t)+ divJ(x, t)= g(x, t) (11.10)
</p>
<p>which are very common in physics. Here J describes the corresponding flux and g
is an additional source (or sink) term. For instance the advection-diffusion equation
(also known as convection equation) has this form which describes quite general
transport processes:
</p>
<p>&part;
</p>
<p>&part;t
C = div(D gradC &minus; uC)+ S(x, t)=&minus;divJ+ S(x, t) (11.11)
</p>
<p>where one contribution to the flux
</p>
<p>J=&minus;D gradC + uC (11.12)
is proportional to the gradient of the concentration C (Fick&rsquo;s first law) and the sec-
ond part depends on the velocity field u of a streaming medium. The source term</p>
<p/>
</div>
<div class="page"><p/>
<p>180 11 Discretization of Differential Equations
</p>
<p>S represents the effect of chemical reactions. Equation (11.11) is also similar to the
drift-diffusion equation in semiconductor physics and closely related to the Navier
Stokes equations which are based on the Cauchy momentum equation [1]
</p>
<p>ρ
du
</p>
<p>dt
= ρ
</p>
<p>(
&part;u
</p>
<p>&part;t
+ ugradu
</p>
<p>)
= divσ + f (11.13)
</p>
<p>where σ denotes the stress tensor. Equation (11.10) is the strong or differential form
of the conservation law. The requirements on the smoothness of the solution are re-
duced by using the integral form which is obtained with the help of Gauss&rsquo; theorem
</p>
<p>&int;
</p>
<p>V
</p>
<p>(
&part;
</p>
<p>&part;t
f (x, t)&minus; g(x, t)
</p>
<p>)
dV +
</p>
<p>∮
</p>
<p>&part;V
</p>
<p>J(x, t) dA= 0. (11.14)
</p>
<p>An alternative integral form results from Galerkin&rsquo;s [98] method of weighted
residuals which introduces a weight function w(x) and considers the equation
</p>
<p>&int;
</p>
<p>V
</p>
<p>(
&part;
</p>
<p>&part;t
f (x, t)+ divJ(x, t)&minus; g(x, t)
</p>
<p>)
w(x) dV = 0 (11.15)
</p>
<p>or after applying Gauss&rsquo; theorem
&int;
</p>
<p>V
</p>
<p>{(
&part;
</p>
<p>&part;t
f (x, t)&minus; g(x, t)
</p>
<p>)
w(x)&minus; J(x, t)gradw(x)
</p>
<p>}
dV
</p>
<p>+
∮
</p>
<p>&part;V
</p>
<p>w(x)J(x, t) dA= 0. (11.16)
</p>
<p>The so called weak form of the conservation law states that this equation holds for
arbitrary weight functions w.
</p>
<p>11.2 Finite Differences
</p>
<p>The simplest method to discretize a differential equation is to introduce a grid of
equidistant points and to discretize the differential operators by finite differences
(FDM) as described in Chap. 3. For instance, in one dimension the first and second
derivatives can be discretized by
</p>
<p>x &rarr; xm =m�x m= 1 . . .M (11.17)
f (x)&rarr; fm = f (xm) m= 1 . . .M (11.18)
</p>
<p>&part;f
</p>
<p>&part;x
&rarr;
</p>
<p>(
&part;
</p>
<p>&part;x
f
</p>
<p>)
</p>
<p>m
</p>
<p>= fm+1 &minus; fm
�x
</p>
<p>or
</p>
<p>(
&part;
</p>
<p>&part;x
f
</p>
<p>)
</p>
<p>m
</p>
<p>= fm+1 &minus; fm&minus;1
2�x
</p>
<p>(11.19)
</p>
<p>&part;2f
</p>
<p>&part;x2
&rarr;
</p>
<p>(
&part;2
</p>
<p>&part;x2
f
</p>
<p>)
</p>
<p>m
</p>
<p>= fm+1 + fm&minus;1 &minus; 2fm
�x2
</p>
<p>. (11.20)
</p>
<p>These expressions are not well defined at the boundaries of the grid m= 1,M unless
the boundary conditions are taken into account. For instance, in case of a Dirichlet
problem f0 and fM+1 are given boundary values and</p>
<p/>
</div>
<div class="page"><p/>
<p>11.2 Finite Differences 181
</p>
<p>(
&part;
</p>
<p>&part;x
f
</p>
<p>)
</p>
<p>1
= f2 &minus; f0
</p>
<p>2�x
</p>
<p>(
&part;2
</p>
<p>&part;x2
f
</p>
<p>)
</p>
<p>1
= f2 &minus; 2f1 + f0
</p>
<p>�x2
(11.21)
</p>
<p>(
&part;
</p>
<p>&part;x
f
</p>
<p>)
</p>
<p>M
</p>
<p>= fM+1 &minus; fM
�x
</p>
<p>or
fM+1 &minus; fM&minus;1
</p>
<p>2�x
(
</p>
<p>&part;2
</p>
<p>&part;x2
f
</p>
<p>)
</p>
<p>M
</p>
<p>= fM&minus;1 &minus; 2fM + fM+1
�x2
</p>
<p>. (11.22)
</p>
<p>Other kinds of boundary conditions can be treated in a similar way.
</p>
<p>11.2.1 Finite Differences in Time
</p>
<p>Time derivatives can be treated similarly using an independent time grid
</p>
<p>t &rarr; tn = n�t n= 1 . . .N (11.23)
f (t, x)&rarr; f nm = f (tn, xm) (11.24)
</p>
<p>and finite differences like the first order forward difference quotient
</p>
<p>&part;f
</p>
<p>&part;t
&rarr; f
</p>
<p>n+1
m &minus; f nm
</p>
<p>�t
(11.25)
</p>
<p>or the symmetric difference quotient
</p>
<p>&part;f
</p>
<p>&part;t
&rarr; f
</p>
<p>n+1
m &minus; f n&minus;1m
</p>
<p>2�t
(11.26)
</p>
<p>to obtain a system of equations for the function values at the grid points f nm. For
instance for the diffusion equation in one spatial dimension
</p>
<p>&part;f (x, t)
</p>
<p>&part;t
=D &part;
</p>
<p>2
</p>
<p>&part;x2
f (x, t)+ S(x, t) (11.27)
</p>
<p>the simplest discretization is the FTCS (forward in time, centered in space) scheme
</p>
<p>(
f n+1m &minus; f nm
</p>
<p>)
=D �t
</p>
<p>�x2
</p>
<p>(
f nm+1 + f nm&minus;1 &minus; 2f nm
</p>
<p>)
+ Snm�t (11.28)
</p>
<p>which can be written in matrix notation as
</p>
<p>fn+1 &minus; fn =D
�t
</p>
<p>�x2
Mfn + Sn�t (11.29)
</p>
<p>with
</p>
<p>fn =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>f n1
f n2
f n3
...
</p>
<p>f nM
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠
</p>
<p>and M =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>&minus;2 1
1 &minus;2 1
</p>
<p>1 &minus;2 1
. . .
</p>
<p>. . .
. . .
</p>
<p>1 &minus;2
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠
. (11.30)</p>
<p/>
</div>
<div class="page"><p/>
<p>182 11 Discretization of Differential Equations
</p>
<p>11.2.2 Stability Analysis
</p>
<p>Fully discretized linear differential equations provide an iterative algorithm of the
type2
</p>
<p>fn+1 =Afn + Sn�t (11.31)
</p>
<p>which propagates numerical errors according to
</p>
<p>fn+1 + εn+1 =A(fn + εn)+ Sn�t (11.32)
εj+1 =Aεj . (11.33)
</p>
<p>Errors are amplified exponentially if the absolute value of at least one eigenvalue of
A is larger than one. The algorithm is stable if all eigenvalues of A are smaller than
one in absolute value (Sect. 1.4). If the eigenvalue problem is difficult to solve, the
von Neumann analysis is helpful which decomposes the errors into a Fourier series
and considers the Fourier components individually by setting
</p>
<p>fn = gn(k)
</p>
<p>⎛
⎜⎝
</p>
<p>eik
</p>
<p>...
</p>
<p>eikM
</p>
<p>⎞
⎟⎠ (11.34)
</p>
<p>and calculating the amplification factor
</p>
<p>∣∣∣∣
f n+1m
f nm
</p>
<p>∣∣∣∣=
∣∣g(k)
</p>
<p>∣∣. (11.35)
</p>
<p>The algorithm is stable if |g(k)| &le; 1 for all k.
</p>
<p>Example For the discretized diffusion equation (11.28) we find
</p>
<p>gn+1(k)= gn(k)+ 2D �t
�x2
</p>
<p>gn(k)(cos k &minus; 1) (11.36)
</p>
<p>g(k)= 1 + 2D �t
�x2
</p>
<p>(cosk &minus; 1)= 1 &minus; 4D �t
�x2
</p>
<p>sin2
(
k
</p>
<p>2
</p>
<p>)
(11.37)
</p>
<p>1 &minus; 4D �t
�x2
</p>
<p>&le; g(k)&le; 1 (11.38)
</p>
<p>hence stability requires
</p>
<p>D
�t
</p>
<p>�x2
&le; 1
</p>
<p>2
. (11.39)
</p>
<p>2Differential equations which are higher order in time can be always brought to first order by
introducing the time derivatives as additional variables.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.2 Finite Differences 183
</p>
<p>11.2.3 Method of Lines
</p>
<p>Alternatively time can be considered as a continuous variable. The discrete values
of the function then are functions of time (so called lines)
</p>
<p>fm(t) (11.40)
</p>
<p>and a set of ordinary differential equations has to be solved. For instance for diffu-
sion in one dimension (11.27) the equations
</p>
<p>dfm
dt
</p>
<p>= D
h2
</p>
<p>(fm+1 + fm&minus;1 &minus; 2fm)+ Sm(t) (11.41)
</p>
<p>which can be written in matrix notation as
</p>
<p>d
</p>
<p>dt
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>f1
f1
f2
...
</p>
<p>fM
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠
</p>
<p>= D
�x2
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>&minus;2 1
1 &minus;2 1
</p>
<p>1 &minus;2 1
. . .
</p>
<p>. . .
. . .
</p>
<p>1 &minus;2
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>f1
f2
f3
...
</p>
<p>fM
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠
+
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>S1 + Dh2 f0
S2
S3
...
</p>
<p>SM + Dh2 fM+1
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠
</p>
<p>(11.42)
</p>
<p>or briefly
</p>
<p>d
</p>
<p>dt
f(t)=Af(t)+ S(t). (11.43)
</p>
<p>Several methods to integrate such a semi-discretized equation will be discussed in
Chap. 12. If eigenvectors and eigenvalues of A are easy available, an eigenvector
expansion can be used.
</p>
<p>11.2.4 Eigenvector Expansion
</p>
<p>A homogeneous system
</p>
<p>d
</p>
<p>dt
f(t)=Af(t) (11.44)
</p>
<p>where the matrix A is obtained from discretizing the spatial derivatives, can be
solved by an eigenvector expansion. From the eigenvalue problem
</p>
<p>Af= λf (11.45)
we obtain the eigenvalues λ and eigenvectors fλ which provide the particular solu-
tions:
</p>
<p>f(t)= eλt fλ (11.46)
d
</p>
<p>dt
</p>
<p>(
eλt fλ
</p>
<p>)
= λ
</p>
<p>(
eλt fλ
</p>
<p>)
=A
</p>
<p>(
eλt fλ
</p>
<p>)
. (11.47)</p>
<p/>
</div>
<div class="page"><p/>
<p>184 11 Discretization of Differential Equations
</p>
<p>These can be used to expand the general solution
</p>
<p>f(t)=
&sum;
</p>
<p>λ
</p>
<p>Cλe
λt fλ. (11.48)
</p>
<p>The coefficients Cλ follow from the initial values by solving the linear equations
</p>
<p>f(t = 0)=
&sum;
</p>
<p>λ
</p>
<p>Cλfλ. (11.49)
</p>
<p>If the differential equation is second order in time
</p>
<p>d2
</p>
<p>dt2
f(t)=Af(t) (11.50)
</p>
<p>the particular solutions are
</p>
<p>f(t)= e&plusmn;t
&radic;
λfλ (11.51)
</p>
<p>d2
</p>
<p>dt2
(
e&plusmn;t
</p>
<p>&radic;
λfλ
</p>
<p>)
= λ
</p>
<p>(
e&plusmn;t
</p>
<p>&radic;
λfλ
</p>
<p>)
=A
</p>
<p>(
e&plusmn;t
</p>
<p>&radic;
λfλ
</p>
<p>)
(11.52)
</p>
<p>and the eigenvector expansion is
</p>
<p>f(t)=
&sum;
</p>
<p>λ
</p>
<p>(
Cλ+e
</p>
<p>t
&radic;
λ +Cλ&minus;e&minus;t
</p>
<p>&radic;
λ
)
fλ. (11.53)
</p>
<p>The coefficients Cλ&plusmn; follow from the initial amplitudes and velocities
</p>
<p>f(t = 0)=
&sum;
</p>
<p>λ
</p>
<p>(Cλ+ +Cλ&minus;)fλ
</p>
<p>d
</p>
<p>dt
f(t = 0)=
</p>
<p>&sum;
</p>
<p>λ
</p>
<p>&radic;
λ(Cλ+ &minus;Cλ&minus;)fλ.
</p>
<p>(11.54)
</p>
<p>For a first order inhomogeneous system
</p>
<p>d
</p>
<p>dt
f(t)=Af(t)+ S(t) (11.55)
</p>
<p>the expansion coefficients have to be time dependent
</p>
<p>f(t)=
&sum;
</p>
<p>λ
</p>
<p>Cλ(t)e
λt fλ (11.56)
</p>
<p>and satisfy
</p>
<p>d
</p>
<p>dt
f(t)&minus;Af(t)=
</p>
<p>&sum;
</p>
<p>λ
</p>
<p>dCλ
dt
</p>
<p>eλt fλ = S(t). (11.57)
</p>
<p>After taking the scalar product with fμ3
</p>
<p>dCμ
dt
</p>
<p>= e&minus;μt
(
fμS(t)
</p>
<p>)
(11.58)
</p>
<p>3If A is not Hermitian we have to distinguish left- and right-eigenvectors.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.3 Finite Volumes 185
</p>
<p>can be solved by a simple time integration. For a second order system
</p>
<p>d2
</p>
<p>dt2
f(t)=Af(t)+ S(t) (11.59)
</p>
<p>we introduce the first time derivative as a new variable
</p>
<p>g= d
dt
f (11.60)
</p>
<p>to obtain a first order system of double dimension
</p>
<p>d
</p>
<p>dt
</p>
<p>(
f
</p>
<p>g
</p>
<p>)
=
(
</p>
<p>0 1
A 0
</p>
<p>)(
f
</p>
<p>g
</p>
<p>)
+
(
S
</p>
<p>0
</p>
<p>)
(11.61)
</p>
<p>where eigenvectors and eigenvalues can be found from those of A (11.45)
</p>
<p>(
0 1
A 0
</p>
<p>)(
fλ
</p>
<p>&plusmn;
&radic;
λfλ
</p>
<p>)
=
(
&plusmn;
&radic;
λfλ
</p>
<p>λfλ
</p>
<p>)
=&plusmn;
</p>
<p>&radic;
λ
</p>
<p>(
fλ
</p>
<p>&plusmn;
&radic;
λfλ
</p>
<p>)
(11.62)
</p>
<p>(
&plusmn;
&radic;
λ fTλ f
</p>
<p>T
λ
</p>
<p>)( 0 1
A 0
</p>
<p>)
=
(
λ fTλ &plusmn;
</p>
<p>&radic;
λfTλ
</p>
<p>)
=&plusmn;
</p>
<p>&radic;
λ
(
&plusmn;
&radic;
λ fTλ f
</p>
<p>T
λ
</p>
<p>)
. (11.63)
</p>
<p>Insertion of
</p>
<p>&sum;
</p>
<p>λ
</p>
<p>Cλ+e
&radic;
λt
</p>
<p>(
fλ&radic;
λfλ
</p>
<p>)
+Cλ&minus;e&minus;
</p>
<p>&radic;
λt
</p>
<p>(
fλ
</p>
<p>&minus;
&radic;
λfλ
</p>
<p>)
</p>
<p>gives
</p>
<p>&sum;
</p>
<p>λ
</p>
<p>dCλ+
dt
</p>
<p>e
&radic;
λt
</p>
<p>(
fλ&radic;
λfλ
</p>
<p>)
+ dCλ&minus;
</p>
<p>dt
e
&radic;
λt
</p>
<p>(
fλ
</p>
<p>&minus;
&radic;
λfλ
</p>
<p>)
=
(
S(t)
</p>
<p>0
</p>
<p>)
(11.64)
</p>
<p>and taking the scalar product with one of the left-eigenvectors we end up with
</p>
<p>dCλ+
dt
</p>
<p>= 1
2
</p>
<p>(
fλS(t)
</p>
<p>)
e&minus;
</p>
<p>&radic;
λt (11.65)
</p>
<p>dCλ&minus;
dt
</p>
<p>=&minus;1
2
</p>
<p>(
fλS(t)
</p>
<p>)
e
&radic;
λt . (11.66)
</p>
<p>11.3 Finite Volumes
</p>
<p>Whereas the finite differences method uses function values
</p>
<p>fi,j,k = f (xi, yj , zk) (11.67)
</p>
<p>at the grid points</p>
<p/>
</div>
<div class="page"><p/>
<p>186 11 Discretization of Differential Equations
</p>
<p>Fig. 11.1 (Finite volume method) The domain V is divided into small control volumes Vr , in the
simplest case cubes around the grid points rijk
</p>
<p>rijk = (xi, yj , zk), (11.68)
the finite volume method (FVM) [79] averages function values and derivatives over
small control volumes Vr which are disjoint and span the domain V (Fig. 11.1)
</p>
<p>V =
⋃
</p>
<p>r
</p>
<p>Vr Vr &cap; Vr &prime; = &Oslash; &forall;r �= r &prime;. (11.69)
</p>
<p>The averages are
</p>
<p>f r =
1
</p>
<p>Vr
</p>
<p>&int;
</p>
<p>Vr
</p>
<p>dV f (r) (11.70)
</p>
<p>or in the simple case of cubic control volumes of equal size h3
</p>
<p>f ijk =
1
</p>
<p>h3
</p>
<p>&int; xi+h/2
</p>
<p>xi&minus;h/2
dx
</p>
<p>&int; yj+h/2
</p>
<p>yj&minus;h/2
dy
</p>
<p>&int; zk+h/2
</p>
<p>zk&minus;h/2
dzf (x, y, z). (11.71)
</p>
<p>Such average values have to be related to discrete function values by numerical
integration (Chap. 4). The midpoint rule (4.17), for instance replaces the average by
the central value
</p>
<p>f ijk = f (xi, yj , zk)+O
(
h2
)
</p>
<p>(11.72)
</p>
<p>whereas the trapezoidal rule (4.13) implies the average over the eight corners of the
cube
</p>
<p>f ijk =
1
</p>
<p>8
</p>
<p>&sum;
</p>
<p>m,n,p=&plusmn;1
f (xi+m/2, yj+n/2, zk+p/2)+O
</p>
<p>(
h2
)
. (11.73)
</p>
<p>In (11.73) the function values refer to a dual grid [79] centered around the vertices
of the original grid (11.68) (Fig. 11.2),
</p>
<p>ri+1/2,j+1/2,k+1/2 =
(
xi +
</p>
<p>h
</p>
<p>2
, yj +
</p>
<p>h
</p>
<p>2
, zk +
</p>
<p>h
</p>
<p>2
</p>
<p>)
. (11.74)</p>
<p/>
</div>
<div class="page"><p/>
<p>11.3 Finite Volumes 187
</p>
<p>Fig. 11.2 (Dual grid) The
dual grid (black) is centered
around the vertices of the
original grid (red)
</p>
<p>The average gradient can be rewritten using the generalized Stokes&rsquo; theorem as
</p>
<p>gradfijk =
1
</p>
<p>V
</p>
<p>&int;
</p>
<p>Vijk
</p>
<p>dV gradf (r)=
∮
</p>
<p>&part;Vijk
</p>
<p>f (r) dA. (11.75)
</p>
<p>For a cubic grid we have to integrate over the six faces of the control volume
</p>
<p>gradfijk =
1
</p>
<p>h3
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>&int; zk+h/2
zk&minus;h/2 dz
</p>
<p>&int; yj+h/2
yj&minus;h/2 dy(f (xi +
</p>
<p>h
2 , y, z)&minus; f (xi &minus;
</p>
<p>h
2 , y, z))&int; zk+h/2
</p>
<p>zk&minus;h/2 dz
&int; xi+h/2
xi&minus;h/2 dx(f (xi, y +
</p>
<p>h
2 , z)&minus; f (xi, y &minus;
</p>
<p>h
2 , z))&int; xi+h/2
</p>
<p>xi&minus;h/2 dx
&int; yj+h/2
yj&minus;h/2 dy(f (xi, y, z+
</p>
<p>h
2 )&minus; f (xi, y, z&minus;
</p>
<p>h
2 ))
</p>
<p>⎞
⎟⎟⎟⎠ .
</p>
<p>(11.76)
</p>
<p>The integrals have to be evaluated numerically. Applying as the simplest approxi-
mation the midpoint rule (4.17)
</p>
<p>&int; xi+h/2
</p>
<p>xi&minus;h/2
dx
</p>
<p>&int; yj+h/2
</p>
<p>yj&minus;h/2
dy f (x, y)= h2
</p>
<p>(
f (xi, yj )+O
</p>
<p>(
h2
))
</p>
<p>(11.77)
</p>
<p>this becomes
</p>
<p>gradfijk =
1
</p>
<p>h
</p>
<p>⎛
⎜⎝
f (xi + h2 , yj , zk)&minus; f (xi &minus;
</p>
<p>h
2 , yj , zk)
</p>
<p>f (xi, yj + h2 , zk)&minus; f (xi, yj &minus;
h
2 , zk)
</p>
<p>f (xi, yj , zk + h2 )&minus; f (xi, yj , zk &minus;
h
2 )
</p>
<p>⎞
⎟⎠ (11.78)
</p>
<p>which involves symmetric difference quotients. However, the function values in
(11.78) refer neither to the original nor to the dual grid. Therefore we interpolate
(Fig. 11.3)
</p>
<p>f
</p>
<p>(
xi &plusmn;
</p>
<p>h
</p>
<p>2
, yj , zk
</p>
<p>)
&asymp; 1
</p>
<p>2
</p>
<p>(
f (xi, yj , zk)+ f (xi&plusmn;1, yj , zk)
</p>
<p>)
(11.79)</p>
<p/>
</div>
<div class="page"><p/>
<p>188 11 Discretization of Differential Equations
</p>
<p>Fig. 11.3 (Interpolation
between grid points)
Interpolation is necessary to
relate the averaged gradient
(11.78) to the original or dual
grid
</p>
<p>1
</p>
<p>h
</p>
<p>(
f
</p>
<p>(
xi +
</p>
<p>h
</p>
<p>2
, yj , zk
</p>
<p>)
&minus; f
</p>
<p>(
xi &minus;
</p>
<p>h
</p>
<p>2
, yj , zk
</p>
<p>))
</p>
<p>&asymp; 1
2h
</p>
<p>(
f (xi+1, yj , zk)&minus; f (xi&minus;1, yj , zk)
</p>
<p>)
(11.80)
</p>
<p>or
</p>
<p>f
</p>
<p>(
xi &plusmn;
</p>
<p>h
</p>
<p>2
, yj , zk
</p>
<p>)
&asymp; 1
</p>
<p>4
</p>
<p>&sum;
</p>
<p>m,n=&plusmn;1
f
</p>
<p>(
xi &plusmn;
</p>
<p>h
</p>
<p>2
, yj +m
</p>
<p>h
</p>
<p>2
, zk + n
</p>
<p>h
</p>
<p>2
</p>
<p>)
. (11.81)
</p>
<p>The finite volume method is capable of treating discontinuities and is very flexi-
ble concerning the size and shape of the control volumes.
</p>
<p>11.3.1 Discretization of fluxes
</p>
<p>Integration of (11.10) over a control volume and application of Gauss&rsquo; theorem gives
the integral form of the conservation law
</p>
<p>1
</p>
<p>V
</p>
<p>∮
JdA+ &part;
</p>
<p>&part;t
</p>
<p>1
</p>
<p>V
</p>
<p>&int;
f dV = 1
</p>
<p>V
</p>
<p>&int;
g dV (11.82)
</p>
<p>which involves the flux J of some property like particle concentration, mass, energy
or momentum density or the flux of an electromagnetic field. The total flux through
a control volume is given by the surface integral
</p>
<p>Φ =
∮
</p>
<p>&part;V
</p>
<p>JdA (11.83)
</p>
<p>which in the special case of a cubic volume element of size h3 becomes the sum
over the six faces of the cube (Fig. 11.4)</p>
<p/>
</div>
<div class="page"><p/>
<p>11.3 Finite Volumes 189
</p>
<p>Fig. 11.4 Flux through a
control volume
</p>
<p>Φ =
6&sum;
</p>
<p>r=1
</p>
<p>&int;
</p>
<p>Ar
</p>
<p>JdA
</p>
<p>=
&int; xi+h/2
</p>
<p>xi&minus;h/2
dx
</p>
<p>&int; yj+h/2
</p>
<p>yj&minus;h/2
dy
</p>
<p>(
Jz
</p>
<p>(
x, y, zk +
</p>
<p>h
</p>
<p>2
</p>
<p>)
&minus; Jz
</p>
<p>(
x, y, zk &minus;
</p>
<p>h
</p>
<p>2
</p>
<p>))
</p>
<p>+
&int; xi+h/2
</p>
<p>xi&minus;h/2
dx
</p>
<p>&int; zk+h/2
</p>
<p>zk&minus;h/2
dz
</p>
<p>(
Jy
</p>
<p>(
x, yj +
</p>
<p>h
</p>
<p>2
, z
</p>
<p>)
&minus; Jz
</p>
<p>(
x, yj &minus;
</p>
<p>h
</p>
<p>2
, z
</p>
<p>))
</p>
<p>+
&int; zk+h/2
</p>
<p>zk&minus;h/2
dz
</p>
<p>&int; yj+h/2
</p>
<p>yj&minus;h/2
dy
</p>
<p>(
Jx
</p>
<p>(
xi +
</p>
<p>h
</p>
<p>2
, y, z
</p>
<p>)
&minus; Jz
</p>
<p>(
xi &minus;
</p>
<p>h
</p>
<p>2
, y, z
</p>
<p>))
.
</p>
<p>(11.84)
</p>
<p>The surface integral can be evaluated numerically (Chap. 4). Using the midpoint
approximation (11.77) we obtain
</p>
<p>1
</p>
<p>V
Φ(xi, yj , zk)=
</p>
<p>1
</p>
<p>h
</p>
<p>(
Jz(xi, yj , zk+1/2)&minus; Jz(xi, yj , zk&minus;1/2)
</p>
<p>+ Jy(xi, yj+1/2, zk)&minus; Jy(xi, yj&minus;1/2, zk)
+ Jx(xi+1/2, yj , zk)&minus; Jx(xi&minus;1/2, yj , zk)
</p>
<p>)
. (11.85)
</p>
<p>The trapezoidal rule (4.13) introduces an average over the four corners (Fig. 11.3)
&int; xi+h/2
</p>
<p>xi&minus;h/2
dx
</p>
<p>&int; yj+h/2
</p>
<p>yj&minus;h/2
dy f (x, y)
</p>
<p>= h2
(
</p>
<p>1
</p>
<p>4
</p>
<p>&sum;
</p>
<p>m,n=&plusmn;1
f (xi+m/2, yj+n/2)+O
</p>
<p>(
h2
))
</p>
<p>(11.86)
</p>
<p>which replaces the flux values in (11.85) by the averages
</p>
<p>Jx(xi&plusmn;1/2, yj , zk)=
1
</p>
<p>4
</p>
<p>&sum;
</p>
<p>m=&plusmn;1,n=&plusmn;1
Jz(xi&plusmn;1/2, yj+m/2, zk+n/2) (11.87)
</p>
<p>Jy(xi, yj&plusmn;1/2, zk)=
1
</p>
<p>4
</p>
<p>&sum;
</p>
<p>m=&plusmn;1,n=&plusmn;1
Jz(xi+m/2, yj&plusmn;1/2, zk+n/2) (11.88)</p>
<p/>
</div>
<div class="page"><p/>
<p>190 11 Discretization of Differential Equations
</p>
<p>Jz(xi, yj , zk&plusmn;1/2)=
1
</p>
<p>4
</p>
<p>&sum;
</p>
<p>m=&plusmn;1,n=&plusmn;1
Jz(xi+m/2, yj+n/2, zk&plusmn;1/2). (11.89)
</p>
<p>One advantage of the finite volume method is that the flux is strictly conserved.
</p>
<p>11.4 Weighted Residual Based Methods
</p>
<p>A general method to discretize partial differential equations is to approximate the
solution within a finite dimensional space of trial functions.4 The partial differential
equation is turned into a finite system of equations or a finite system of ordinary
differential equations if time is treated as a continuous variable. This is the basis of
spectral methods which make use of polynomials or Fourier series but also of the
very successful finite element methods. Even finite difference methods and finite
volume methods can be formulated as weighted residual based methods.
</p>
<p>Consider a differential equation5 on the domain V which is written symbolically
with the differential operator T
</p>
<p>T
[
u(r)
</p>
<p>]
= f (r) r &isin; V (11.90)
</p>
<p>and corresponding boundary conditions which are expressed with a boundary oper-
ator B6
</p>
<p>B
[
u(r)
</p>
<p>]
= g(r) r &isin; &part;V . (11.91)
</p>
<p>The basic principle to obtain an approximate solution ũ(r) is to choose a linear
combination of expansion functions Ni(r) i = 1 . . . r as a trial function which fulfills
the boundary conditions7
</p>
<p>ũ=
r&sum;
</p>
<p>i=1
uiNi(r) (11.92)
</p>
<p>B
[
ũ(r)
</p>
<p>]
= g(r). (11.93)
</p>
<p>In general (11.92) is not an exact solution and the residual
</p>
<p>R(r)= T [ũ](r)&minus; f (r) (11.94)
will not be zero throughout the whole domain V . The function ũ has to be deter-
mined such that the residual becomes &ldquo;small&rdquo; in a certain sense. To that end weight
functions8 wj j = 1 . . . r are chosen to define the weighted residuals
</p>
<p>4Also called expansion functions.
5Generalization to systems of equations is straightforward.
6One or more linear differential operators, usually a combination of the function and its first deriva-
tives.
7This requirement can be replaced by additional equations for the ui , for instance with the tau
method [195].
8Also called test functions.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.4 Weighted Residual Based Methods 191
</p>
<p>Rj (u1 . . . ur)=
&int;
</p>
<p>dV wj (r)
(
T [ũ](r)&minus; f (r)
</p>
<p>)
. (11.95)
</p>
<p>The optimal parameters ui are then obtained from the solution of the equations
</p>
<p>Rj (u1 . . . ur)= 0 j = 1 . . . r. (11.96)
In the special case of a linear differential operator these equations are linear
</p>
<p>r&sum;
</p>
<p>i=1
ui
</p>
<p>&int;
dV wj (r)T
</p>
<p>[
Ni(r)
</p>
<p>]
&minus;
&int;
</p>
<p>dV wj (r)f (r)= 0. (11.97)
</p>
<p>Several strategies are available to choose suitable weight functions.
</p>
<p>11.4.1 Point Collocation Method
</p>
<p>The collocation method uses the weight functions wj (r) = δ(r&minus; rj ), with certain
collocation points rj &isin; V . The approximation ũ obeys the differential equation at
the collocation points
</p>
<p>0 =Rj = T [ũ](rj )&minus; f (rj ) (11.98)
and for a linear differential operator
</p>
<p>0 =
r&sum;
</p>
<p>i=1
uiT [Ni](rj )&minus; f (rj ). (11.99)
</p>
<p>The point collocation method is simple to use, especially for nonlinear problems.
Instead of using trial functions satisfying the boundary conditions, extra collocation
points on the boundary can be added (mixed collocation method).
</p>
<p>11.4.2 Sub-domain Method
</p>
<p>This approach uses weight functions which are the characteristic functions of a set
of control volumes Vi which are disjoint and span the whole domain similar as for
the finite volume method
</p>
<p>V =
⋃
</p>
<p>j
</p>
<p>Vj Vj &cap; Vj &prime; = &Oslash; &forall;j �= j &prime; (11.100)
</p>
<p>wj (r)=
{
</p>
<p>1 r &isin; Vj
0 else.
</p>
<p>(11.101)
</p>
<p>The residuals then are integrals over the control volumes and
</p>
<p>0 =Rj =
&int;
</p>
<p>Vj
</p>
<p>dV
(
T [ũ](r)&minus; f (r)
</p>
<p>)
(11.102)</p>
<p/>
</div>
<div class="page"><p/>
<p>192 11 Discretization of Differential Equations
</p>
<p>respectively
</p>
<p>0 =
&sum;
</p>
<p>i
</p>
<p>ui
</p>
<p>&int;
</p>
<p>Vj
</p>
<p>dV T [Ni](r)&minus;
&int;
</p>
<p>Vj
</p>
<p>dV f (r). (11.103)
</p>
<p>11.4.3 Least Squares Method
</p>
<p>Least squares methods have become popular for first order systems of differential
equations in computational fluid dynamics and computational electrodynamics [30,
140].
</p>
<p>The L2-norm of the residual (11.94) is given by the integral
</p>
<p>S =
&int;
</p>
<p>V
</p>
<p>dV R(r)2. (11.104)
</p>
<p>It is minimized by solving the equations
</p>
<p>0 = &part;S
&part;uj
</p>
<p>= 2
&int;
</p>
<p>V
</p>
<p>dV
&part;R
</p>
<p>&part;uj
R(r) (11.105)
</p>
<p>which is equivalent to choosing the weight functions
</p>
<p>wj (r)=
&part;R
</p>
<p>&part;uj
R(r)= &part;
</p>
<p>&part;uj
T
</p>
<p>[&sum;
</p>
<p>i
</p>
<p>uiNi(r)
</p>
<p>]
(11.106)
</p>
<p>or for a linear differential operator simply
</p>
<p>wj (r)= T
[
Nj (r)
</p>
<p>]
. (11.107)
</p>
<p>Advantages of the least squares method are that boundary conditions can be in-
corporated into the residual and that S provides a measure for the quality of the so-
lution which can be used for adaptive grid size control. On the other hand S involves
a differential operator of higher order and therefore much smoother trial functions
are necessary.
</p>
<p>11.4.4 Galerkin Method
</p>
<p>Galerkin&rsquo;s widely used method [87, 98] chooses the basis functions as weight func-
tions
</p>
<p>wj (r)=Nj (r) (11.108)
and solves the following system of equations
</p>
<p>&int;
dV Nj (r)T
</p>
<p>[&sum;
</p>
<p>i
</p>
<p>uiNi(r)
</p>
<p>]
&minus;
&int;
</p>
<p>dV Nj (r)f (r)= 0 (11.109)</p>
<p/>
</div>
<div class="page"><p/>
<p>11.5 Spectral and Pseudo-spectral Methods 193
</p>
<p>or in the simpler linear case
&sum;
</p>
<p>ui
</p>
<p>&int;
</p>
<p>V
</p>
<p>dV Nj (r)T
(
Ni(r)
</p>
<p>)
=
&int;
</p>
<p>V
</p>
<p>dV Nj (r)f (r, t). (11.110)
</p>
<p>11.5 Spectral and Pseudo-spectral Methods
</p>
<p>Spectral methods use basis functions which are nonzero over the whole domain, the
trial functions being mostly polynomials or Fourier sums [35]. They can be used to
solve ordinary as well as partial differential equations. The combination of a spectral
method with the point collocation method is also known as pseudo-spectral method.
</p>
<p>11.5.1 Fourier Pseudo-spectral Methods
</p>
<p>Linear differential operators become diagonal in Fourier space. Combination of
Fourier series expansion and point collocation leads to equations involving a dis-
crete Fourier transformation, which can be performed very efficiently with the Fast
Fourier Transform methods.
</p>
<p>For simplicity we consider only the one-dimensional case. We choose equidistant
collocation points
</p>
<p>xm =m�x m= 0,1 . . .M &minus; 1 (11.111)
and expansion functions
</p>
<p>Nj (x)= eikj x kj =
2π
</p>
<p>M�x
j j = 0,1 . . .M &minus; 1. (11.112)
</p>
<p>For a linear differential operator
</p>
<p>L
[
eikj x
</p>
<p>]
= l(kj )eikj x (11.113)
</p>
<p>and the condition on the residual becomes
</p>
<p>0 =Rm =
M&minus;1&sum;
</p>
<p>j=0
uj l(kj )e
</p>
<p>ikj xm &minus; f (xm) (11.114)
</p>
<p>or
</p>
<p>f (xm)=
M&minus;1&sum;
</p>
<p>j=0
uj l(kj )e
</p>
<p>i2πmj/M (11.115)
</p>
<p>which is nothing but a discrete Fourier back transformation (Sect. 7.2, (7.19)) which
can be inverted to give
</p>
<p>uj l(kj )=
1
</p>
<p>N
</p>
<p>M&minus;1&sum;
</p>
<p>m=0
f (xm)e
</p>
<p>&minus;i2πmj/M . (11.116)</p>
<p/>
</div>
<div class="page"><p/>
<p>194 11 Discretization of Differential Equations
</p>
<p>Instead of exponential expansion functions, sine and cosine functions can be used
to satisfy certain boundary conditions, for instance to solve the Poisson equation
within a cube (Sect. 17.1.2).
</p>
<p>11.5.2 Example: Polynomial Approximation
</p>
<p>Let us consider the initial value problem (Fig. 11.5)
</p>
<p>d
</p>
<p>dx
u(x)&minus; u(x)= 0 u(0)= 1 for 0 &le; x &le; 1 (11.117)
</p>
<p>with the well known solution
</p>
<p>u(x)= ex . (11.118)
</p>
<p>We choose a polynomial trial function with the proper initial value
</p>
<p>ũ(x)= 1 + u1x + u2x2. (11.119)
</p>
<p>The residual is
</p>
<p>R(x)= u1 + 2u2x &minus;
(
1 + u1x + u2x2
</p>
<p>)
</p>
<p>= (u1 &minus; 1)+ (2u2 &minus; u1)x &minus; u2x2. (11.120)
</p>
<p>11.5.2.1 Point Collocation Method
</p>
<p>For our example we need two collocation points to obtain two equations for the two
unknowns u1,2. We choose x1 = 0, x2 = 12 . Then we have to solve the equations
</p>
<p>R(x1)= u1 &minus; 1 = 0 (11.121)
</p>
<p>R(x2)=
1
</p>
<p>2
u1 +
</p>
<p>3
</p>
<p>4
u2 &minus; 1 = 0 (11.122)
</p>
<p>which gives
</p>
<p>u1 = 1 u2 =
2
</p>
<p>3
(11.123)
</p>
<p>uc = 1 + x +
2
</p>
<p>3
x2. (11.124)
</p>
<p>11.5.2.2 Sub-domain Method
</p>
<p>We need two sub-domains to obtain two equations for the two unknowns u1,2. We
choose V1 = {x,0 &lt; x &lt; 12 }, V2 = {x,
</p>
<p>1
2 &lt; x &lt; 1}. Integration gives</p>
<p/>
</div>
<div class="page"><p/>
<p>11.5 Spectral and Pseudo-spectral Methods 195
</p>
<p>Fig. 11.5 (Approximate
solution of a simple
differential equation) The
initial value problem
d
</p>
<p>dx u(x)&minus; u(x)= 0 u(0)= 1
for 0 &le; x &le; 1 is
approximately solved with a
polynomial trial function
ũ(x)= 1 + u1x + u2x2. The
parameters u1,2 are optimized
with the method of weighted
residuals using point
collocation (full curve),
sub-domain collocation
(dotted curve), Galerkin&rsquo;s
method (dashed curve) and
least squares (dash-dotted
curve). The absolute error
ũ(x)&minus; ex (top) and the
residual R(x)= ddx ũ(x)&minus;
ũ(x)= (u1&minus;1)+(2u2&minus;u1)x
&minus; u2x2 both are smallest for
the least squares and
sub-domain collocation
methods
</p>
<p>R1 =
3
</p>
<p>8
u1 +
</p>
<p>5
</p>
<p>24
u2 &minus;
</p>
<p>1
</p>
<p>2
= 0 (11.125)
</p>
<p>R2 =
1
</p>
<p>8
u1 +
</p>
<p>11
</p>
<p>24
u2 &minus;
</p>
<p>1
</p>
<p>2
= 0 (11.126)
</p>
<p>u1 = u2 =
6
</p>
<p>7
(11.127)
</p>
<p>usdc = 1 +
6
</p>
<p>7
x + 6
</p>
<p>7
x2. (11.128)
</p>
<p>11.5.2.3 Galerkin Method
</p>
<p>Galerkin&rsquo;s method uses the weight functions w1(x)= x,w2(x)= x2. The equations
&int; 1
</p>
<p>0
dx w1(x)R(x)=
</p>
<p>1
</p>
<p>6
u1 +
</p>
<p>5
</p>
<p>12
u2 &minus;
</p>
<p>1
</p>
<p>2
= 0 (11.129)
</p>
<p>&int; 1
</p>
<p>0
dx w2(x)R(x)=
</p>
<p>1
</p>
<p>12
u1 +
</p>
<p>3
</p>
<p>10
u2 &minus;
</p>
<p>1
</p>
<p>3
= 0 (11.130)</p>
<p/>
</div>
<div class="page"><p/>
<p>196 11 Discretization of Differential Equations
</p>
<p>have the solution
</p>
<p>u1 =
8
</p>
<p>11
u2 =
</p>
<p>10
</p>
<p>11
(11.131)
</p>
<p>uG = 1 +
8
</p>
<p>11
x + 10
</p>
<p>11
x2. (11.132)
</p>
<p>11.5.2.4 Least Squares Method
</p>
<p>The integral of the squared residual
</p>
<p>S =
&int; 1
</p>
<p>0
dx R(x)2 = 1 &minus; u1 &minus;
</p>
<p>4
</p>
<p>3
u2 +
</p>
<p>1
</p>
<p>3
u21 +
</p>
<p>1
</p>
<p>2
u1u2 +
</p>
<p>8
</p>
<p>15
u22 (11.133)
</p>
<p>is minimized by solving
</p>
<p>&part;S
</p>
<p>&part;u1
= 2
</p>
<p>3
u1 +
</p>
<p>1
</p>
<p>2
u2 &minus; 1 = 0 (11.134)
</p>
<p>&part;S
</p>
<p>&part;u2
= 1
</p>
<p>2
u1 +
</p>
<p>16
</p>
<p>15
u2 &minus;
</p>
<p>4
</p>
<p>3
= 0 (11.135)
</p>
<p>which gives
</p>
<p>u1 =
72
</p>
<p>83
u2 =
</p>
<p>70
</p>
<p>83
(11.136)
</p>
<p>uLS = 1 +
72
</p>
<p>83
x + 70
</p>
<p>83
x2. (11.137)
</p>
<p>11.6 Finite Elements
</p>
<p>The method of finite elements (FEM) is a very flexible method to discretize partial
differential equations [84, 210]. It is rather dominant in a variety of engineering
sciences. Usually the expansion functions Ni are chosen to have compact support.
The integration volume is divided into disjoint sub-volumes
</p>
<p>V =
r⋃
</p>
<p>i=1
Vi Vi &cap; Vi&prime; = &Oslash;&forall;i �= i&prime;. (11.138)
</p>
<p>The Ni(x) are piecewise continuous polynomials which are nonzero only inside Vi
and a few neighbor cells.
</p>
<p>11.6.1 One-Dimensional Elements
</p>
<p>In one dimension the domain is an interval V = {x;a &le; x &le; b} and the sub-volumes
are small sub-intervals Vi = {x;xi &le; x &le; xi+1}. The one-dimensional mesh is the</p>
<p/>
</div>
<div class="page"><p/>
<p>11.6 Finite Elements 197
</p>
<p>Fig. 11.6 (Finite elements in
one dimension) The basis
functions Ni are piecewise
continuous polynomials and
have compact support. In the
simplest case they are
composed of two linear
functions over the
sub-intervals xi&minus;1 &le; x &le; xi
and xi &le; x &le; xi+1
</p>
<p>set of nodes {a = x0, x1 . . . xr = b}. Piecewise linear basis functions (Fig. 11.6) are
in the 1-dimensional case given by
</p>
<p>Ni(x)=
</p>
<p>⎧
⎪⎪⎨
⎪⎪⎩
</p>
<p>xi+1&minus;x
xi+1&minus;xi for xi &lt; x &lt; xi+1
x&minus;xi&minus;1
xi&minus;xi&minus;1 for xi&minus;1 &lt; x &lt; xi
</p>
<p>0 else
</p>
<p>(11.139)
</p>
<p>and the derivatives are (except at the nodes xi )
</p>
<p>N &prime;i (x)=
</p>
<p>⎧
⎪⎪⎨
⎪⎪⎩
</p>
<p>&minus; 1
xi+1&minus;xi for xi &lt; x &lt; xi+1
1
</p>
<p>xi&minus;xi&minus;1 for xi&minus;1 &lt; x &lt; xi
</p>
<p>0 else.
</p>
<p>(11.140)
</p>
<p>11.6.2 Two- and Three-Dimensional Elements
</p>
<p>In two dimensions the mesh is defined by a finite number of points (xi, yi) &isin; V (the
nodes of the mesh). There is considerable freedom in the choice of these points and
they need not be equally spaced.
</p>
<p>11.6.2.1 Triangulation
</p>
<p>The nodes can be regarded as forming the vertices of a triangulation9 of the do-
main V (Fig. 11.7).
</p>
<p>The piecewise linear basis function in one dimension (11.139) can be generalized
to the two-dimensional case by constructing functions Ni(x, y) which are zero at all
nodes except (xi, yi)
</p>
<p>Ni(xj , yj )= δi,j . (11.141)
</p>
<p>9The triangulation is not determined uniquely by the nodes.</p>
<p/>
</div>
<div class="page"><p/>
<p>198 11 Discretization of Differential Equations
</p>
<p>Fig. 11.7 (Triangulation of a
two-dimensional domain)
A two-dimensional mesh is
defined by a set of node
points which can be regarded
to form the vertices of a
triangulation
</p>
<p>These functions are linear over each triangle which contains the vertex i and can
be combined as the sum of small pyramids (Fig. 11.8). Let one of the triangles be
denoted by its three vertices as Tijk .10 The corresponding linear function then is
</p>
<p>nijk(x, y)= α + βx(x &minus; xi)+ βy(y &minus; yi) (11.142)
</p>
<p>where the coefficients follow from the conditions
</p>
<p>nijk(xi, yi)= 1 nijk(xj , yj )= nijk(xk, yk)= 0 (11.143)
</p>
<p>as
</p>
<p>α = 1 βx =
yj &minus; yk
2Aijk
</p>
<p>βy =
xk &minus; xj
2Aijk
</p>
<p>(11.144)
</p>
<p>with
</p>
<p>Aijk =
1
</p>
<p>2
det
</p>
<p>∣∣∣∣
xj &minus; xi xk &minus; xi
yj &minus; yi yk &minus; yi
</p>
<p>∣∣∣∣ (11.145)
</p>
<p>which, apart from sign, is the area of the triangle Tijk . The basis function Ni now is
given by
</p>
<p>Ni(x, y)=
{
nijk(x, y) (x, y) &isin; Tijk
0 else.
</p>
<p>In three dimensions we consider tetrahedrons (Fig. 11.9) instead of triangles. The
corresponding linear function of three arguments has the form
</p>
<p>ni,j,k,l(x, y, z)= α+ βx(x &minus; xi)+ βy(y &minus; yi)+ βz(z&minus; zi) (11.146)
</p>
<p>and from the conditions ni,j,k,l(xi, yi, zi)= 1 and ni,j,k,l = 0 on all other nodes we
find (an algebra program is helpful at that point)
</p>
<p>10The order of the indices does matter.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.6 Finite Elements 199
</p>
<p>Fig. 11.8 (Finite elements in two dimensions) The simplest finite elements in two dimensions are
piecewise linear functions Ni(x, y) which are non-vanishing only at one node (xi , yi) (right side).
They can be constructed from small pyramids built upon one of the triangles that contains this node
(left side)
</p>
<p>Fig. 11.9 (Tetrahedron) The
tetrahedron is the
three-dimensional case of a
Euclidean simplex, i.e. the
simplest polytope
</p>
<p>α = 1
</p>
<p>βx =
1
</p>
<p>6Vijkl
det
</p>
<p>∣∣∣∣
yk &minus; yj yl &minus; yj
zk &minus; zj zl &minus; zj
</p>
<p>∣∣∣∣
</p>
<p>βy =
1
</p>
<p>6Vijkl
det
</p>
<p>∣∣∣∣
zk &minus; zj zl &minus; zj
xk &minus; xj xl &minus; xj
</p>
<p>∣∣∣∣
</p>
<p>βz =
1
</p>
<p>6Vijkl
det
</p>
<p>∣∣∣∣
xk &minus; xj xl &minus; xj
yk &minus; yj yl &minus; yj
</p>
<p>∣∣∣∣ (11.147)
</p>
<p>where Vijkl is, apart from sign, the volume of the tetrahedron
</p>
<p>Vijkl =
1
</p>
<p>6
det
</p>
<p>∣∣∣∣∣∣
</p>
<p>xj &minus; xi xk &minus; xi xl &minus; xi
yj &minus; yi yk &minus; yi yl &minus; yi
zj &minus; zi zk &minus; zi zl &minus; zi
</p>
<p>∣∣∣∣∣∣
. (11.148)
</p>
<p>11.6.2.2 Rectangular Elements
</p>
<p>For a rectangular grid rectangular elements offer a practical alternative to triangles.
Since equations for four nodes have to be fulfilled, the basic element needs four
parameters, which is the case for a bilinear expression. Let us denote one of the
rectangles which contains the vertex i as Ri,j,k,l . The other three edges are</p>
<p/>
</div>
<div class="page"><p/>
<p>200 11 Discretization of Differential Equations
</p>
<p>Fig. 11.10 (Rectangular
elements around one vertex)
The basis function Ni is a
bilinear function on each of
the four rectangles containing
the vertex (xi , yi)
</p>
<p>(xj , yj )= (xi + bx, yi) (xk, yk)= (xi, yi + by) (xl, yl)= (xi + bx, yi + by)
(11.149)
</p>
<p>where bx =&plusmn;hx, by =&plusmn;hy corresponding to the four rectangles with the common
vertex i (Fig. 11.10).
</p>
<p>The bilinear function (Fig. 11.11) corresponding to Rijkl is
</p>
<p>ni,j,k,l(x, y)= α + β(x &minus; xi)+ γ (y &minus; yi)+ η(x &minus; xi)(y &minus; yi). (11.150)
It has to fulfill
</p>
<p>ni,j,k,l(xi, yi)= 1 ni,j,k,l(xj , yj )= ni,j,k,l(xk, yk)= ni,j,k,l(xl, yl)= 0
(11.151)
</p>
<p>from which we find
</p>
<p>α = 1 β =&minus; 1
bx
</p>
<p>γ =&minus; 1
by
</p>
<p>η= 1
bxby
</p>
<p>(11.152)
</p>
<p>ni,j,k,l(x, y)= 1 &minus;
x &minus; xi
bx
</p>
<p>&minus; y &minus; yi
by
</p>
<p>+ (x &minus; xi)
bx
</p>
<p>(y &minus; yi)
by
</p>
<p>. (11.153)
</p>
<p>The basis function centered at node i then is
</p>
<p>Ni(x, y)=
{
ni,j,k,l(x, y) (x, y) &isin;Ri,j,k,l
0 else.
</p>
<p>(11.154)
</p>
<p>Generalization to a three-dimensional grid is straightforward (Fig. 11.12). We
denote one of the eight cuboids containing the node (xi, yi, zi) as Ci,j1...j7 with
(xj1 , yj1 , zj1)= (xi + bx, yi, zi) . . . (xj7 , yj7 , zj7)= (xi + bx, yi + by, zi + bz). The
corresponding trilinear function is
</p>
<p>ni,j1...j7 = 1 &minus;
x &minus; xi
bx
</p>
<p>&minus; y &minus; yi
by
</p>
<p>&minus; z&minus; zi
bz
</p>
<p>+ (x &minus; xi)
bx
</p>
<p>(y &minus; yi)
by
</p>
<p>+ (x &minus; xi)
bx
</p>
<p>(z&minus; zi)
bz
</p>
<p>+ (z&minus; zi)
bz
</p>
<p>(y &minus; yi)
by
</p>
<p>&minus; (x &minus; xi)
bx
</p>
<p>(y &minus; yi)
by
</p>
<p>(z&minus; zi)
bz
</p>
<p>. (11.155)</p>
<p/>
</div>
<div class="page"><p/>
<p>11.6 Finite Elements 201
</p>
<p>Fig. 11.11 (Bilinear elements on a rectangular grid) The basis functions Ni(x, y) on a rectangular
grid (right side) are piecewise bilinear functions (left side), which vanish at all nodes except (xi , yi)
(right side)
</p>
<p>Fig. 11.12
</p>
<p>(Three-dimensional
rectangular grid) The basis
function Ni is trilinear on
each of the eight cuboids
containing the vertex i. It
vanishes on all nodes except
(xi , yi , zi)
</p>
<p>11.6.3 One-Dimensional Galerkin FEM
</p>
<p>As an example we consider the one-dimensional linear differential equation (11.5)
(
a
&part;2
</p>
<p>&part;x2
+ b &part;
</p>
<p>&part;x
+ c
</p>
<p>)
u(x)= f (x) (11.156)
</p>
<p>in the domain 0 &le; x &le; 1 with boundary conditions
</p>
<p>u(0)= u(1)= 0. (11.157)
We use the basis functions from (11.139) on a one-dimensional grid with
</p>
<p>xi+1 &minus; xi = hi (11.158)
and apply the Galerkin method [88]. The boundary conditions require
</p>
<p>u0 = uN&minus;1 = 0. (11.159)
The weighted residual is</p>
<p/>
</div>
<div class="page"><p/>
<p>202 11 Discretization of Differential Equations
</p>
<p>0 =Rj =
&sum;
</p>
<p>i
</p>
<p>ui
</p>
<p>&int; 1
</p>
<p>0
dx Nj (x)
</p>
<p>(
a
&part;2
</p>
<p>&part;x2
+ b &part;
</p>
<p>&part;x
+ c
</p>
<p>)
Ni(x)&minus;
</p>
<p>&int; 1
</p>
<p>0
dx Nj (x)f (x).
</p>
<p>(11.160)
</p>
<p>First we integrate
</p>
<p>&int; 1
</p>
<p>0
Nj (x)Ni(x) dx =
</p>
<p>&int; xi+1
xi&minus;1
</p>
<p>Nj (x)Ni(x) dx =
</p>
<p>⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
</p>
<p>hi+hi&minus;1
3 j = i
</p>
<p>hi
6 j = i + 1
hi&minus;1
</p>
<p>6 j = i &minus; 1
0 |i &minus; j |&gt; 1.
</p>
<p>(11.161)
</p>
<p>Integration of the first derivative gives
</p>
<p>&int; 1
</p>
<p>0
dx Nj (x)N
</p>
<p>&prime;
i (x)=
</p>
<p>⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
</p>
<p>0 j = i
1
2 j = i &minus; 1
&minus; 12 j = i+1
0 else.
</p>
<p>(11.162)
</p>
<p>For the second derivative partial integration gives
&int; 1
</p>
<p>0
dx Nj (x)
</p>
<p>&part;2
</p>
<p>&part;x2
Ni(x)
</p>
<p>=Nj (1)N &prime;i (1 &minus; ε)&minus;Nj (0)N &prime;i (0 + ε)&minus;
&int; 1
</p>
<p>0
dx N &prime;j (x)N
</p>
<p>&prime;
i (x) (11.163)
</p>
<p>where the first two summands are zero due to the boundary conditions. Since
Ni and N &prime;i are nonzero only for xi&minus;1 &lt; x &lt; xi+1 we find
</p>
<p>&int; 1
</p>
<p>0
dx Nj (x)
</p>
<p>&part;2
</p>
<p>&part;x2
Ni(x)=&minus;
</p>
<p>&int; xi+1
xi&minus;1
</p>
<p>dx N &prime;j (x)N
&prime;
i (x)
</p>
<p>=
</p>
<p>⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
</p>
<p>1
hi&minus;1
</p>
<p>j = i &minus; 1
&minus; 1
</p>
<p>hi
&minus; 1
</p>
<p>hi&minus;1
i = j
</p>
<p>1
hi
</p>
<p>j = i + 1
0 else.
</p>
<p>(11.164)
</p>
<p>Integration of the last term in (11.160) gives
&int; 1
</p>
<p>0
dx Nj (x)f (x)=
</p>
<p>&int; xi+1
xi&minus;1
</p>
<p>dx Nj (x)f (x)
</p>
<p>=
&int; xj
xj&minus;1
</p>
<p>dx
x &minus; xj&minus;1
xj &minus; xj&minus;1
</p>
<p>f (x)
</p>
<p>+
&int; xj+1
xj
</p>
<p>dx
xj+1 &minus; x
xj+1 &minus; xj
</p>
<p>f (x). (11.165)</p>
<p/>
</div>
<div class="page"><p/>
<p>11.6 Finite Elements 203
</p>
<p>Applying the trapezoidal rule11 for both integrals we find
&int; xj+1
xj&minus;1
</p>
<p>dx Nj (x)f (x)&asymp; f (xj )
hj + hj&minus;1
</p>
<p>2
. (11.166)
</p>
<p>The discretized equation finally reads
</p>
<p>a
</p>
<p>{
1
</p>
<p>hj&minus;1
uj&minus;1 &minus;
</p>
<p>(
1
</p>
<p>hj
+ 1
</p>
<p>hj&minus;1
</p>
<p>)
uj +
</p>
<p>1
</p>
<p>hj
uj+1
</p>
<p>}
</p>
<p>+ b
{
&minus;1
</p>
<p>2
uj&minus;1 +
</p>
<p>1
</p>
<p>2
uj+1
</p>
<p>}
</p>
<p>+ c
{
hj&minus;1
</p>
<p>6
uj&minus;1 +
</p>
<p>hj + hj&minus;1
3
</p>
<p>uj +
hj
</p>
<p>6
uj+1
</p>
<p>}
</p>
<p>= f (xj )
hj + hj&minus;1
</p>
<p>2
(11.167)
</p>
<p>which can be written in matrix notation as
</p>
<p>Au= Bf (11.168)
</p>
<p>where the matrix A is tridiagonal as a consequence of the compact support of the
basis functions
</p>
<p>A= a
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>&minus; 1
h1
</p>
<p>&minus; 1
h0
, 1
</p>
<p>h1
. . .
1
</p>
<p>hj&minus;1
, &minus; 1
</p>
<p>hj
&minus; 1
</p>
<p>hj&minus;1
, 1
</p>
<p>hj
</p>
<p>. . .
1
</p>
<p>hN&minus;3
, &minus; 1
</p>
<p>hN&minus;2
&minus; 1
</p>
<p>hN&minus;3
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>+ b
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎝
</p>
<p>0, 12
. . .
</p>
<p>&minus; 12 , 0,
1
2
. . .
</p>
<p>&minus; 12 , 0
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎠
</p>
<p>(11.169)
</p>
<p>+ c
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>(h1+h0)
3 ,
</p>
<p>h1
6
. . .
</p>
<p>hj&minus;1
6 ,
</p>
<p>(hj+hj&minus;1)
3 ,
</p>
<p>hj
6
. . .
</p>
<p>hN&minus;3
6 ,
</p>
<p>(hN&minus;2+hN&minus;3)
3
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>11Higher accuracy can be achieved, for instance, by Gaussian integration.</p>
<p/>
</div>
<div class="page"><p/>
<p>204 11 Discretization of Differential Equations
</p>
<p>B =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>h0+h1
2
</p>
<p>. . .
hj&minus;1+hj
</p>
<p>2
. . .
</p>
<p>hN&minus;2+hN&minus;3
2
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎠
.
</p>
<p>For equally spaced nodes hi = hi&minus;1 = h and after division by h (11.167) reduces to
a system of equations where the derivatives are replaced by finite differences (11.20)
</p>
<p>a
</p>
<p>{
1
</p>
<p>h2
uj&minus;1 &minus;
</p>
<p>2
</p>
<p>h2
uj +
</p>
<p>1
</p>
<p>h2
uj+1
</p>
<p>}
</p>
<p>+ b
{
&minus; 1
</p>
<p>2h
uj&minus;1 +
</p>
<p>1
</p>
<p>2h
uj+1
</p>
<p>}
</p>
<p>+ c
{
</p>
<p>1
</p>
<p>6
uj&minus;1 +
</p>
<p>2
</p>
<p>3
uj +
</p>
<p>1
</p>
<p>6
uj+1
</p>
<p>}
</p>
<p>= f (xj ) (11.170)
and the function u is replaced by a certain average
</p>
<p>1
</p>
<p>6
uj&minus;1 +
</p>
<p>2
</p>
<p>3
uj +
</p>
<p>1
</p>
<p>6
uj+1 = uj +
</p>
<p>1
</p>
<p>6
(uj&minus;1 &minus; 2uj + uj+1). (11.171)
</p>
<p>The corresponding matrix in (11.169) is the so called mass matrix. Within the frame-
work of the finite differences method the last expression equals
</p>
<p>uj +
1
</p>
<p>6
(uj&minus;1 &minus; 2uj + uj+1)= uj +
</p>
<p>h2
</p>
<p>6
</p>
<p>(
d2u
</p>
<p>dx2
</p>
<p>)
</p>
<p>j
</p>
<p>+O
(
h4
)
</p>
<p>(11.172)
</p>
<p>hence replacing it by uj (this is called mass lumping) introduces an error of the
order O(h2).
</p>
<p>11.7 Boundary Element Method
</p>
<p>The boundary element method (BEM) [18, 276] is a method for linear partial differ-
ential equations which can be brought into boundary integral form12 like Laplace&rsquo;s
equation (Chap. 17)13
</p>
<p>&minus;�Φ(r)= 0 (11.173)
for which the fundamental solution
</p>
<p>�G
(
r, r&prime;
</p>
<p>)
=&minus;δ
</p>
<p>(
r&minus; r&prime;
</p>
<p>)
</p>
<p>12This is only possible if the fundamental solution or Green&rsquo;s function is available.
13The minus sign is traditionally used.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.7 Boundary Element Method 205
</p>
<p>is given by
</p>
<p>G
(
r&minus; r&prime;
</p>
<p>)
= 1
</p>
<p>4π |r&minus; r&prime;| in three dimensions (11.174)
</p>
<p>G
(
r&minus; r&prime;
</p>
<p>)
= 1
</p>
<p>2π
ln
</p>
<p>1
</p>
<p>|r&minus; r&prime;| in two dimensions. (11.175)
</p>
<p>We apply Gauss&rsquo;s theorem to the expression [277]
</p>
<p>div
[
G
(
r&minus; r&prime;
</p>
<p>)
grad
</p>
<p>(
Φ(r)
</p>
<p>)
&minus;Φ(r)grad
</p>
<p>(
G
(
r&minus; r&prime;
</p>
<p>))]
</p>
<p>=&minus;Φ(r)�
(
G
(
r&minus; r&prime;
</p>
<p>))
. (11.176)
</p>
<p>Integration over a volume V gives
∮
</p>
<p>&part;V
</p>
<p>dA
</p>
<p>(
G
(
r&minus; r&prime;
</p>
<p>) &part;
&part;n
</p>
<p>(
Φ(r)
</p>
<p>)
&minus;Φ(r) &part;
</p>
<p>&part;n
</p>
<p>(
G
(
r&minus; r&prime;
</p>
<p>)))
</p>
<p>=&minus;
&int;
</p>
<p>V
</p>
<p>dV
(
Φ(r)�
</p>
<p>(
G
(
r&minus; r&prime;
</p>
<p>)))
=Φ
</p>
<p>(
r&prime;
)
. (11.177)
</p>
<p>This integral equation determines the potential self-consistently by its value and
normal derivative on the surface of the cavity. It can be solved numerically by di-
viding the surface into a finite number of boundary elements. The resulting system
of linear equations often has smaller dimension than corresponding finite element
approaches. However, the coefficient matrix is in general full and not necessarily
symmetric.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 12
</p>
<p>Equations of Motion
</p>
<p>Simulation of a physical system means to calculate the time evolution of a model
system in many cases. We consider a large class of models which can be described
by a first order initial value problem
</p>
<p>dY
</p>
<p>dt
= f
</p>
<p>(
Y(t), t
</p>
<p>)
Y(t = 0)= Y0 (12.1)
</p>
<p>where Y is the state vector (possibly of very high dimension) which contains all
information about the system. Our goal is to calculate the time evolution of the
state vector Y(t) numerically. For obvious reasons this can be done only for a finite
number of values of t and we have to introduce a grid of discrete times tn which for
simplicity are assumed to be equally spaced:1
</p>
<p>tn+1 = tn +�t. (12.2)
</p>
<p>Advancing time by one step involves the calculation of the integral
</p>
<p>Y(tn+1)&minus; Y(tn)=
&int; tn+1
tn
</p>
<p>f
(
Y
(
t &prime;
)
, t &prime;
</p>
<p>)
dt &prime; (12.3)
</p>
<p>which can be a formidable task since f (Y (t), t) depends on time via the time de-
pendence of all the elements of Y(t). In this chapter we discuss several strategies for
the time integration. The explicit Euler forward difference has low error order but
is useful as a predictor step for implicit methods. A symmetric difference quotient
is much more accurate. It can be used as the corrector step in combination with an
explicit Euler predictor step and is often used for the time integration of partial dif-
ferential equations. Methods with higher error order can be obtained from a Taylor
series expansion, like the Nordsieck and Gear predictor-corrector methods which
have been often applied in molecular dyamics calculations. Runge-Kutta methods
are very important for ordinary differential equations. They are robust and allow an
adaptive control of the step size. Very accurate results can be obtained for ordinary
</p>
<p>1Control of the step width will be discussed later.
</p>
<p>P.O.J. Scherer, Computational Physics, Graduate Texts in Physics,
DOI 10.1007/978-3-319-00401-3_12,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>207</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-00401-3_12">http://dx.doi.org/10.1007/978-3-319-00401-3_12</a></div>
</div>
<div class="page"><p/>
<p>208 12 Equations of Motion
</p>
<p>differential equations with extrapolation methods like the famous Gragg-Bulirsch-
St&ouml;r method. If the solution is smooth enough, multistep methods are applicable,
which use information from several points. Most known are Adams-Bashforth-
Moulton methods and Gear methods (also known as backward differentiation meth-
ods), which are especially useful for stiff problems. The class of Verlet methods has
been developed for molecular dynamics calculations. They are symplectic and time
reversible and conserve energy over long trajectories.
</p>
<p>12.1 The State Vector
</p>
<p>The state of a classical N -particle system is given by the position in phase space, or
equivalently by specifying position and velocity for all the N particles
</p>
<p>Y = (r1,v1, . . . , rN ,vN ). (12.4)
The concept of a state vector is not restricted to a finite number of degrees of
</p>
<p>freedom. For instance a diffusive system can be described by the particle concen-
trations as a function of the coordinate, i.e. the elements of the state vector are now
indexed by the continuous variable x
</p>
<p>Y =
(
c1(x) &middot; &middot; &middot; cM(x)
</p>
<p>)
. (12.5)
</p>
<p>Similarly, a quantum particle moving in an external potential can be described by
the amplitude of the wave function
</p>
<p>Y =
(
Ψ (x)
</p>
<p>)
. (12.6)
</p>
<p>Numerical treatment of continuous systems is not feasible since even the ultimate
high end computer can only handle a finite number of data in finite time. Therefore
discretization is necessary (Chap. 11), by introducing a spatial mesh (Sects. 11.2,
11.3, 11.6), which in the simplest case means a grid of equally spaced points
</p>
<p>xijk = (ih, jh, kh) i = 1 &middot; &middot; &middot; imax, j = 1 &middot; &middot; &middot; jmax, k = 1 &middot; &middot; &middot;kmax (12.7)
Y =
</p>
<p>(
c1(xijk) . . . cM(xijk)
</p>
<p>)
(12.8)
</p>
<p>Y =
(
Ψ (xijk)
</p>
<p>)
(12.9)
</p>
<p>or by expanding the continuous function with respect to a finite set of basis functions
(Sect. 11.5). The elements of the state vector then are the expansion coefficients
</p>
<p>|Ψ 〉 =
N&sum;
</p>
<p>s=1
Cs |Ψs〉 (12.10)
</p>
<p>Y = (C1, . . . ,CN ). (12.11)
If the density matrix formalism is used to take the average over a thermodynamic
ensemble or to trace out the degrees of freedom of a heat bath, the state vector
instead is composed of the elements of the density matrix</p>
<p/>
</div>
<div class="page"><p/>
<p>12.2 Time Evolution of the State Vector 209
</p>
<p>ρ =
N&sum;
</p>
<p>s=1
</p>
<p>N&sum;
</p>
<p>s&prime;=1
ρss&prime; |Ψs〉〈Ψs&prime; | =
</p>
<p>N&sum;
</p>
<p>s=1
</p>
<p>N&sum;
</p>
<p>s&prime;=1
C&lowast;
s&prime;Cs |Ψs〉〈Ψs&prime; | (12.12)
</p>
<p>Y = (ρ11 &middot; &middot; &middot;ρ1N , ρ21 &middot; &middot; &middot;ρ2N , . . . , ρN1 &middot; &middot; &middot;ρNN ). (12.13)
</p>
<p>12.2 Time Evolution of the State Vector
</p>
<p>We assume that all information about the system is included in the state vector. Then
the simplest equation to describe the time evolution of the system gives the change
of the state vector
</p>
<p>dY
</p>
<p>dt
= f (Y, t) (12.14)
</p>
<p>as a function of the state vector (or more generally a functional in the case of a
continuous system). Explicit time dependence has to be considered for instance to
describe the influence of an external time dependent field.
</p>
<p>Some examples will show the universality of this equation of motion:
</p>
<p>&bull; N -particle system
</p>
<p>The motion of N interacting particles is described by
</p>
<p>dY
</p>
<p>dt
= (ṙ1, v̇1 &middot; &middot; &middot;)= (v1,a1 &middot; &middot; &middot;) (12.15)
</p>
<p>where the acceleration of a particle is given by the total force acting upon this parti-
cle and thus depends on all the coordinates and eventually time (velocity dependent
forces could be also considered but are outside the scope of this book)
</p>
<p>ai =
Fi(r1 &middot; &middot; &middot; rN , t)
</p>
<p>mi
. (12.16)
</p>
<p>&bull; Diffusion
</p>
<p>Heat transport and other diffusive processes are described by the diffusion equation
</p>
<p>&part;f
</p>
<p>&part;t
=D�f + S(x, t) (12.17)
</p>
<p>which in its simplest spatially discretized version for 1-dimensional diffusion reads
</p>
<p>&part;f (xi)
</p>
<p>&part;t
= D
</p>
<p>�x2
</p>
<p>(
f (xi+1)+ f (xi&minus;1)&minus; 2f (xi)
</p>
<p>)
+ S(xi, t). (12.18)
</p>
<p>&bull; Waves
</p>
<p>Consider the simple 1-dimensional wave equation
</p>
<p>&part;2f
</p>
<p>&part;t2
= c2 &part;
</p>
<p>2f
</p>
<p>&part;x2
(12.19)</p>
<p/>
</div>
<div class="page"><p/>
<p>210 12 Equations of Motion
</p>
<p>Fig. 12.1 Explicit Euler
method
</p>
<p>which by introducing the velocity g(x)= &part;
&part;t
f (x) as an independent variable can be
</p>
<p>rewritten as
</p>
<p>&part;
</p>
<p>&part;t
</p>
<p>(
f (x), g(x)
</p>
<p>)
=
(
g(x), c2
</p>
<p>&part;2
</p>
<p>&part;x2
f (x)
</p>
<p>)
. (12.20)
</p>
<p>Discretization of space gives
</p>
<p>&part;
</p>
<p>&part;t
</p>
<p>(
f (xi), g(xi)
</p>
<p>)
=
(
g(xi),
</p>
<p>c2
</p>
<p>�x2
</p>
<p>(
f (xi+1)+ f (xi&minus;1)&minus; 2f (xi)
</p>
<p>))
. (12.21)
</p>
<p>&bull; Two-state quantum system
</p>
<p>The Schr&ouml;dinger equation for a two level system (for instance a spin- 12 particle in a
magnetic field) reads
</p>
<p>d
</p>
<p>dt
</p>
<p>(
C1
C2
</p>
<p>)
=
(
H11(t) H12(t)
</p>
<p>H21(t) H22(t)
</p>
<p>)(
C1
C2
</p>
<p>)
. (12.22)
</p>
<p>12.3 Explicit Forward Euler Method
</p>
<p>The simplest method which is often discussed in elementary physics textbooks ap-
proximates the integrand by its value at the lower bound (Fig. 12.1):
</p>
<p>Y(tn+1)&minus; Y(tn)&asymp; f
(
Y(tn), tn
</p>
<p>)
�t. (12.23)
</p>
<p>The truncation error can be estimated from a Taylor series expansion
</p>
<p>Y(tn+1)&minus; Y(tn)=�t
dY
</p>
<p>dt
(tn)+
</p>
<p>�t2
</p>
<p>2
</p>
<p>d2Y
</p>
<p>dt2
(tn)+ &middot; &middot; &middot;
</p>
<p>=�tf
(
Y(tn), tn
</p>
<p>)
+O
</p>
<p>(
�t2
</p>
<p>)
. (12.24)
</p>
<p>The explicit Euler method has several serious drawbacks
</p>
<p>&bull; Low error order</p>
<p/>
</div>
<div class="page"><p/>
<p>12.3 Explicit Forward Euler Method 211
</p>
<p>Fig. 12.2 Systematic errors
of the Euler method
</p>
<p>Suppose you want to integrate from the initial time t0 to the final time t0 + T . For a
time step of �t you have to perform N = T/�t steps. Assuming comparable error
contributions from all steps the global error scales as N�t2 = O(�t). The error
gets smaller as the time step is reduced but it may be necessary to use very small �t
to obtain meaningful results.
</p>
<p>&bull; Loss of orthogonality and normalization
The simple Euler method can produce systematic errors which are very inconvenient
if you want, for instance, to calculate the orbits of a planetary system. This can be
most easily seen from a very simple example. Try to integrate the following equation
of motion (see Example 1.5 on page 12):
</p>
<p>dz
</p>
<p>dt
= iωz. (12.25)
</p>
<p>The exact solution is obviously given by a circular orbit in the complex plane:
</p>
<p>z= z0eiωt (12.26)
|z| = |z0| = const. (12.27)
</p>
<p>Application of the Euler method gives
</p>
<p>z(tn+1)= z(tn)+ iω�tz(tn)= (1 + iω�t)z(tn) (12.28)
</p>
<p>and you find immediately
∣∣z(tn)
</p>
<p>∣∣=
&radic;
</p>
<p>1 +ω2�t2
∣∣z(tn&minus;1)
</p>
<p>∣∣=
(
1 +ω2�t2
</p>
<p>)n/2∣∣z(t0)
∣∣ (12.29)
</p>
<p>which shows that the radius increases continually even for the smallest time step
possible (Fig. 12.2).
</p>
<p>The same kind of error appears if you solve the Schr&ouml;dinger equation for a parti-
cle in an external potential or if you calculate the rotational motion of a rigid body.
For the N -body system it leads to a violation of the conservation of phase space
volume. This can introduce an additional sensitivity of the calculated results to the
initial conditions. Consider a harmonic oscillator with the equation of motion
</p>
<p>d
</p>
<p>dt
</p>
<p>(
x(t)
</p>
<p>v(t)
</p>
<p>)
=
(
</p>
<p>v(t)
</p>
<p>&minus;ω2x(t)
</p>
<p>)
. (12.30)</p>
<p/>
</div>
<div class="page"><p/>
<p>212 12 Equations of Motion
</p>
<p>Fig. 12.3 Time evolution of
the phase space volume
</p>
<p>Fig. 12.4 Implicit backward
Euler method
</p>
<p>Application of the explicit Euler method gives
(
x(t +�t)
v(t +�t)
</p>
<p>)
=
(
x(t)
</p>
<p>v(t)
</p>
<p>)
+
(
</p>
<p>v(t)
</p>
<p>&minus;ω2x(t)
</p>
<p>)
�t. (12.31)
</p>
<p>The change of the phase space volume (Fig. 12.3) is given by the Jacobi determinant
</p>
<p>J =
∣∣∣∣
&part;(x(t +�t), v(t +�t))
</p>
<p>&part;(x(t), v(t))
</p>
<p>∣∣∣∣=
∣∣∣∣
</p>
<p>1 �t
&minus;ω2�t 1
</p>
<p>∣∣∣∣= 1 + (ω�t)2. (12.32)
</p>
<p>In this case the phase space volume increases continuously.
</p>
<p>12.4 Implicit Backward Euler Method
</p>
<p>Alternatively let us make a step backwards in time
</p>
<p>Y(tn)&minus; Y(tn+1)&asymp;&minus;f
(
Y(tn+1), tn+1
</p>
<p>)
�t (12.33)
</p>
<p>which can be written as (Fig. 12.4)
</p>
<p>Y(tn+1)&asymp; Y(tn)+ f
(
Y(tn+1), tn+1
</p>
<p>)
�t. (12.34)</p>
<p/>
</div>
<div class="page"><p/>
<p>12.5 Improved Euler Methods 213
</p>
<p>Fig. 12.5 Improved Euler
method
</p>
<p>Taylor series expansion gives
</p>
<p>Y(tn)= Y(tn+1)&minus;
d
</p>
<p>dt
Y (tn+1)�t +
</p>
<p>d2
</p>
<p>dt2
Y(tn+1)
</p>
<p>�t2
</p>
<p>2
+ &middot; &middot; &middot; (12.35)
</p>
<p>which shows that the error order again is O(�t2). The implicit method is sometimes
used to avoid the inherent instability of the explicit method. For the examples in
Sect. 12.3 it shows the opposite behavior. The radius of the circular orbit as well
as the phase space volume decrease in time. The gradient at future time has to be
estimated before an implicit step can be performed.
</p>
<p>12.5 Improved Euler Methods
</p>
<p>The quality of the approximation can be improved significantly by employing the
midpoint rule (Fig. 12.5)
</p>
<p>Y(tn+1)&minus; Y(tn)&asymp; f
(
Y
</p>
<p>(
t + �t
</p>
<p>2
</p>
<p>)
, tn +
</p>
<p>�t
</p>
<p>2
</p>
<p>)
�t. (12.36)
</p>
<p>The error is smaller by one order of �t :
</p>
<p>Y(tn)+ f
(
Y
</p>
<p>(
t + �t
</p>
<p>2
</p>
<p>)
, tn +
</p>
<p>�t
</p>
<p>2
</p>
<p>)
�t
</p>
<p>= Y(tn)+
(
</p>
<p>dY
</p>
<p>dt
(tn)+
</p>
<p>�t
</p>
<p>2
</p>
<p>d2Y
</p>
<p>dt2
(tn)+ &middot; &middot; &middot;
</p>
<p>)
�t
</p>
<p>= Y(tn +�t)+O
(
�t3
</p>
<p>)
. (12.37)
</p>
<p>The future value Y(t + �t2 ) can be obtained by two different approaches:
</p>
<p>&bull; Predictor-corrector method</p>
<p/>
</div>
<div class="page"><p/>
<p>214 12 Equations of Motion
</p>
<p>Fig. 12.6 Improved polygon
(or Heun) method
</p>
<p>Since f (Y (t + �t2 ), tn +
�t
2 ) is multiplied with �t , it is sufficient to use an approx-
</p>
<p>imation with lower error order. Even the explicit Euler step is sufficient. Together
the following algorithm results:
</p>
<p>predictor step: Y (p) = Y(tn)+
�t
</p>
<p>2
f
(
Y(tn), tn
</p>
<p>)
</p>
<p>corrector step: Y(tn +�t)= Y(tn)+�tf
(
Y (p), tn +
</p>
<p>�t
</p>
<p>2
</p>
<p>)
.
</p>
<p>(12.38)
</p>
<p>&bull; Averaging (Heun method)
The average of f (Y (tn), tn) and f (Y (tn +�t), t +�t) is another approximation to
the midpoint value of comparable quality (Fig. 12.6).
</p>
<p>Expansion around tn +�t/2 gives
1
</p>
<p>2
</p>
<p>(
f
(
Y(tn), tn
</p>
<p>)
+ f
</p>
<p>(
Y(tn +�t), t +�t
</p>
<p>))
</p>
<p>= f
(
Y
</p>
<p>(
tn +
</p>
<p>�t
</p>
<p>2
</p>
<p>)
, tn +
</p>
<p>�t
</p>
<p>2
</p>
<p>)
+O
</p>
<p>(
�t2
</p>
<p>)
. (12.39)
</p>
<p>Inserting the average in (12.36) gives the following algorithm, which is also known
as improved polygon method and corresponds to the trapezoidal rule for the integral
(4.13) or to a combination of explicit and implicit Euler step:
</p>
<p>Y(tn +�t)= Y(tn)+
�t
</p>
<p>2
</p>
<p>(
f
(
Y(tn), tn
</p>
<p>)
+ f
</p>
<p>(
Y(tn +�t), t +�t
</p>
<p>))
. (12.40)
</p>
<p>In the special case of a linear function f (Y (t), t) = FY(t) (for instance rotational
motion or diffusion) this can be solved formally by
</p>
<p>Y(tn +�t)=
(
</p>
<p>1 &minus; �t
2
F
</p>
<p>)&minus;1(
1 + �t
</p>
<p>2
F
</p>
<p>)
Y(tn). (12.41)
</p>
<p>Numerically it is not necessary to perform the matrix inversion. Instead a linear
system of equations is solved:
</p>
<p>(
1 &minus; �t
</p>
<p>2
F
</p>
<p>)
Y(tn +�t)=
</p>
<p>(
1 + �t
</p>
<p>2
F
</p>
<p>)
Y(tn). (12.42)</p>
<p/>
</div>
<div class="page"><p/>
<p>12.6 Taylor Series Methods 215
</p>
<p>In certain cases the Heun method conserves the norm of the state vector, for instance
if F has only imaginary eigenvalues (as for the 1-dimensional Schr&ouml;dinger equation,
see page 393).
</p>
<p>In the general case a predictor step has to be made to estimate the state vector at
tn +�t before the Heun expression (12.40) can be evaluated:
</p>
<p>Y (p) = Y(tn)+�tf
(
Y(tn), tn
</p>
<p>)
. (12.43)
</p>
<p>12.6 Taylor Series Methods
</p>
<p>Higher order methods can be obtained from a Taylor series expansion
</p>
<p>Y(tn +�t)= Y(tn)+�tf
(
Y(tn), tn
</p>
<p>)
+ �t
</p>
<p>2
</p>
<p>2
</p>
<p>df (Y (tn), tn)
</p>
<p>dt
+ &middot; &middot; &middot; . (12.44)
</p>
<p>The total time derivative can be expressed as
</p>
<p>df
</p>
<p>dt
= &part;f
</p>
<p>&part;Y
</p>
<p>dY
</p>
<p>dt
+ &part;f
</p>
<p>&part;t
= f &prime;f + ḟ (12.45)
</p>
<p>where the partial derivatives have been abbreviated in the usual way by &part;f
&part;t
</p>
<p>= ḟ and
&part;f
&part;Y
</p>
<p>= f &prime;. Higher derivatives are given by
</p>
<p>d2f
</p>
<p>dt2
= f &prime;&prime;f 2 + f &prime;2f + 2ḟ &prime;f + f̈ (12.46)
</p>
<p>d3f
</p>
<p>dt3
= &part;
</p>
<p>3f
</p>
<p>&part;t3
+ f &prime;&prime;&prime;f 3 + 3ḟ &prime;&prime;f 2 + f̈ f &prime; + 3f &prime;&prime;ḟ f
</p>
<p>+ 3ḟ &prime; + 4f &prime;&prime;f &prime;f 2 + 5ḟ &prime;f &prime;f + f &prime;3f + f &prime;2ḟ . (12.47)
</p>
<p>12.6.1 Nordsieck Predictor-Corrector Method
</p>
<p>Nordsieck [184] determines an interpolating polynomial of degree m. As variables
he uses the 0th to mth derivatives2 evaluated at the current time t , for instance for
m= 5 he uses the variables
</p>
<p>Y(t) (12.48)
</p>
<p>g(t)= d
dt
Y (t) (12.49)
</p>
<p>a(t)= �t
2
</p>
<p>d2
</p>
<p>dt2
Y(t) (12.50)
</p>
<p>2In fact the derivatives of the interpolating polynomial which exist even if higher derivatives of f
do not exist.</p>
<p/>
</div>
<div class="page"><p/>
<p>216 12 Equations of Motion
</p>
<p>b(t)= �t
2
</p>
<p>6
</p>
<p>d3
</p>
<p>dt3
Y(t) (12.51)
</p>
<p>c(t)= �t
3
</p>
<p>24
</p>
<p>d4
</p>
<p>dt4
Y(t) (12.52)
</p>
<p>d(t)= �t
4
</p>
<p>120
</p>
<p>d5
</p>
<p>dt5
Y(t). (12.53)
</p>
<p>Taylor expansion gives approximate values at t +�t
</p>
<p>Y (t +�t)= Y(t)+�t
[
g(t)+ a(t)+ b(t)+ c(t)+ d(t)+ e(t)
</p>
<p>]
</p>
<p>= Yp(t +�t)+ e(t)�t (12.54)
g(t +�t)= g(t)+ 2a(t)+ 3b(t)+ 4c(t)+ 5d(t)+ 6e(t)
</p>
<p>= gp(t +�T )+ 6e(t) (12.55)
a(t +�t)= a(t)+ 3b(t)+ 6c(t)+ 10d(t)+ 15e(t)
</p>
<p>= ap(t +�t)+ 15e(t) (12.56)
b(t +�t)= b(t)+ 4c(t)+ 10d(t)+ 20e(t)
</p>
<p>= bp(t +�t)+ 20e(t) (12.57)
c(t +�t)= c(t)+ 5d(t)+ 15e(t)= cp(t +�t)+ 15e(t) (12.58)
d(t +�t)= d(t)+ 6e(t)= dp(t +�t)+ 6e(t) (12.59)
</p>
<p>where the next term of the Taylor series e(t)= �t56!
d6
</p>
<p>dt6
Y(t) has been introduced as
</p>
<p>an approximation to the truncation error of the predicted values Yp, gp , etc. It can
be estimated from the second equation
</p>
<p>e= 1
6
</p>
<p>[
f
(
Yp(t +�t), t +�t
</p>
<p>)
&minus; gp(t +�t)
</p>
<p>]
= 1
</p>
<p>6
δf. (12.60)
</p>
<p>This predictor-corrector method turns out to be rather unstable. However, sta-
bility can be achieved by slightly modifying the coefficients of the corrector step.
Nordsieck suggested to use
</p>
<p>Y(t +�t)= Yp(t +�t)+ 95
288
</p>
<p>δf (12.61)
</p>
<p>a(t +�t)= ap(t +�t)+ 25
24
</p>
<p>δf (12.62)
</p>
<p>b(t +�t)= bp(t +�t)+ 35
72
</p>
<p>δf (12.63)
</p>
<p>c(t +�t)= cp(t +�t)+ 5
48
</p>
<p>δf (12.64)
</p>
<p>d(t +�t)= dp(t +�t)+ 1
120
</p>
<p>δf. (12.65)</p>
<p/>
</div>
<div class="page"><p/>
<p>12.7 Runge-Kutta Methods 217
</p>
<p>12.6.2 Gear Predictor-Corrector Methods
</p>
<p>Gear [101] designed special methods for molecular dynamics simulations (Chap. 14)
where Newton&rsquo;s law (12.15) has to be solved numerically. He uses again a truncated
Taylor expansion for the predictor step
</p>
<p>r(t +�t)= r(t)+ v(t)�t + a(t)�t
2
</p>
<p>2
+ ȧ(t)�t
</p>
<p>3
</p>
<p>6
+ ä(t)�t
</p>
<p>4
</p>
<p>24
+ &middot; &middot; &middot; (12.66)
</p>
<p>v(t +�t)= v(t)+ a(t)�t + ȧ(t)�t
2
</p>
<p>2
+ ä(t)�t
</p>
<p>3
</p>
<p>6
+ &middot; &middot; &middot; (12.67)
</p>
<p>a(t +�t)= a(t)+ ȧ(t)�t + ä(t)�t
2
</p>
<p>2
+ &middot; &middot; &middot; (12.68)
</p>
<p>ȧ(t +�t)= ȧ(t)+ ä(t)�t + &middot; &middot; &middot; (12.69)
...
</p>
<p>to calculate new coordinates etc. rpn+1,v
p
</p>
<p>n+1,a
p
</p>
<p>n+1 . . . (Fig. 12.7). The difference be-
tween the predicted acceleration and that calculated using the predicted coordinates
</p>
<p>δan+1 = a
(
rPn+1, t +�t
</p>
<p>)
&minus; apn+1 (12.70)
</p>
<p>is then used as a measure of the error to correct the predicted values according to
</p>
<p>rn+1 = rpn+1 + c1δan+1 (12.71)
vn+1 = vpn+1 + c2δan+1 (12.72)
...
</p>
<p>The coefficients ci were determined to optimize stability and accuracy. For instance
the fourth order Gear corrector reads
</p>
<p>rn+1 = rpn+1 +
�t2
</p>
<p>12
δan+1 (12.73)
</p>
<p>vn+1 = vpn+1 +
5�t
</p>
<p>12
δan+1 (12.74)
</p>
<p>ȧn+1 = ȧn +
1
</p>
<p>�t
δan+1. (12.75)
</p>
<p>Gear methods are generally not time reversible and show systematic energy
drifts. A reversible symplectic predictor-corrector method has been presented re-
cently by Martyna and Tuckerman [169].
</p>
<p>12.7 Runge-Kutta Methods
</p>
<p>If higher derivatives are not so easily available, they can be approximated by numer-
ical differences. f is evaluated at several trial points and the results are combined to
reproduce the Taylor series as close as possible [48].</p>
<p/>
</div>
<div class="page"><p/>
<p>218 12 Equations of Motion
</p>
<p>Fig. 12.7 (Gear
predictor-corrector method)
The difference between
predicted acceleration ap and
acceleration calculated for the
predicted coordinates a(rp) is
used as a measure of the error
to estimate the correction δr
</p>
<p>12.7.1 Second Order Runge-Kutta Method
</p>
<p>Let us begin with two function values. As common in the literature we will denote
the function values as K1,K2, . . . . From the gradient at time tn
</p>
<p>K1 = fn = f
(
Y(tn), tn
</p>
<p>)
(12.76)
</p>
<p>we estimate the state vector at time tn +�t as
Y(tn +�t)&asymp;�t K1. (12.77)
</p>
<p>The gradient at time tn +�t is approximately
K2 = f
</p>
<p>(
Y(tn)+�t K1, tn +�t
</p>
<p>)
(12.78)
</p>
<p>which has the Taylor series expansion
</p>
<p>K2 = fn +
(
ḟn + f &prime;nfn
</p>
<p>)
�t + &middot; &middot; &middot; (12.79)
</p>
<p>and application of the trapezoidal rule (4.13) gives the 2nd order Runge-Kutta
method
</p>
<p>Yn+1 = Yn +
�t
</p>
<p>2
(K1 +K2) (12.80)
</p>
<p>which in fact coincides with the improved Euler or Heun method. Taylor series
expansion shows how the combination of K1 and K2 leads to an expression of higher
error order:
</p>
<p>Yn+1 = Yn +
�t
</p>
<p>2
</p>
<p>(
fn + fn +
</p>
<p>(
ḟn + f &prime;nfn
</p>
<p>)
�t + &middot; &middot; &middot;
</p>
<p>)
</p>
<p>= Yn + fn�t +
dfn
dt
</p>
<p>�t2
</p>
<p>2
+ &middot; &middot; &middot; . (12.81)
</p>
<p>12.7.2 Third Order Runge-Kutta Method
</p>
<p>The accuracy can be further improved by calculating one additional function value
at mid-time. From (12.76) we estimate the gradient at mid-time by</p>
<p/>
</div>
<div class="page"><p/>
<p>12.7 Runge-Kutta Methods 219
</p>
<p>K2 = f
(
Y(t)+ �t
</p>
<p>2
K1, t +
</p>
<p>�t
</p>
<p>2
</p>
<p>)
</p>
<p>= fn +
(
ḟn + f &prime;nfn
</p>
<p>)�t
2
</p>
<p>+
(
f̈n + f &prime;&prime;n f 2n + 2ḟ &prime;nfn
</p>
<p>)�t2
8
</p>
<p>+ &middot; &middot; &middot; . (12.82)
</p>
<p>The gradient at time tn +�t is then estimated as
</p>
<p>K3 = f
(
Y(tn)+�t(2K2 &minus;K1), tn +�t
</p>
<p>)
</p>
<p>= fn + ḟn�t + f &prime;n(2K2 &minus;K1)�t + f̈n
�t2
</p>
<p>2
</p>
<p>+ f &prime;&prime;n
(2K2 &minus;K1)2�t2
</p>
<p>2
+ 2ḟ &prime;n
</p>
<p>(2K2 &minus;K1)�t2
2
</p>
<p>+ &middot; &middot; &middot; . (12.83)
</p>
<p>Inserting the expansion (12.82) gives the leading terms
</p>
<p>K3 = fn +
(
ḟn + f &prime;nfn
</p>
<p>)
�t +
</p>
<p>(
2f &prime;2n fn + f &prime;&prime;n f 2n + f̈n + 2f &prime;nḟn + 2ḟ 2n
</p>
<p>)�t2
2
</p>
<p>+ &middot; &middot; &middot; .
(12.84)
</p>
<p>Applying Simpson&rsquo;s rule (4.14) we combine the three gradients to get the 3rd order
Runge-Kutta method
</p>
<p>Yn+1 = Y(tn)+
�t
</p>
<p>6
(K1 + 4K2 +K3) (12.85)
</p>
<p>where the Taylor series
</p>
<p>Yn+1 = Y(tn)+
�t
</p>
<p>6
</p>
<p>(
6fn + 3
</p>
<p>(
ḟn + fnf &prime;n
</p>
<p>)
�t
</p>
<p>+
(
f &prime;2n fn + f &prime;&prime;n f 2n + 2ḟ &prime;nfn + fn + ḟnf̈ &prime;n
</p>
<p>)
�t2 + &middot; &middot; &middot;
</p>
<p>)
</p>
<p>= Y(tn +�t)+O
(
�t4
</p>
<p>)
(12.86)
</p>
<p>recovers the exact Taylor series (12.44) including terms of order O(�t3).
</p>
<p>12.7.3 Fourth Order Runge-Kutta Method
</p>
<p>The 4th order Runge-Kutta method (RK4) is often used because of its robustness
and accuracy. It uses two different approximations for the midpoint
</p>
<p>K1 = f
(
Y(tn), tn
</p>
<p>)
</p>
<p>K2 = f
(
Y(tn)+
</p>
<p>K1
</p>
<p>2
�t, tn +
</p>
<p>�t
</p>
<p>2
</p>
<p>)
</p>
<p>K3 = f
(
Y(tn)+
</p>
<p>K2
</p>
<p>2
�t, tn +
</p>
<p>�t
</p>
<p>2
</p>
<p>)
</p>
<p>K4 = f
(
Y(tn)+K3�t, tn +�t
</p>
<p>)</p>
<p/>
</div>
<div class="page"><p/>
<p>220 12 Equations of Motion
</p>
<p>Fig. 12.8 Step doubling with
the fourth order Runge-Kutta
method
</p>
<p>and Simpson&rsquo;s rule (4.14) to obtain
</p>
<p>Yn+1 = Y(tn)+
�t
</p>
<p>6
(K1 + 2K2 + 2K3 +K4)= Y(tn +�t)+O
</p>
<p>(
�t5
</p>
<p>)
.
</p>
<p>Expansion of the Taylor series is cumbersome but with the help of an algebra pro-
gram one can easily check that the error is of order �t5.
</p>
<p>12.8 Quality Control and Adaptive Step Size Control
</p>
<p>For practical applications it is necessary to have an estimate for the local error and to
adjust the step size properly. With the Runge-Kutta method this can be achieved by
a step doubling procedure. We calculate yn+2 first by two steps �t and then by one
step 2�t . This needs 11 function evaluations as compared to 8 for the smaller step
size only (Fig. 12.8). For the 4th order method we estimate the following errors:
</p>
<p>�
(
Y
(�t)
n+2
</p>
<p>)
= 2a�t5 (12.87)
</p>
<p>�
(
Y
(2�t)
n+2
</p>
<p>)
= a(2�t)5. (12.88)
</p>
<p>The local error can be estimated from
∣∣Y (�t)n+2 &minus; Y
</p>
<p>(2�t)
n+2
</p>
<p>∣∣= 30|a|�t5
</p>
<p>�
(
Y
(�t)
n+1
</p>
<p>)
= a�t5 =
</p>
<p>|Y (�t)n+2 &minus; Y
(2�t)
n+2 |
</p>
<p>30
.
</p>
<p>The step size �t can now be adjusted to keep the local error within the desired
limits.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.9 Extrapolation Methods 221
</p>
<p>12.9 Extrapolation Methods
</p>
<p>Application of the extrapolation method to calculate the integral
&int; tn+1
tn
</p>
<p>f (t)dt pro-
duces very accurate results but can also be time consuming. The famous Gragg-
Bulirsch-St&ouml;r method [244] starts from an explicit midpoint rule with a special start-
ing procedure. The interval �t is divided into a sequence of N sub-steps
</p>
<p>h= �t
N
</p>
<p>. (12.89)
</p>
<p>First a simple Euler step is performed
</p>
<p>u0 = Y(tn)
u1 = u0 + hf (u0, tn)
</p>
<p>(12.90)
</p>
<p>and then the midpoint rule is applied repeatedly to obtain
</p>
<p>uj+1 = uj&minus;1 + 2hf (uj , tn + jh) j = 1,2, . . . ,N &minus; 1. (12.91)
</p>
<p>Gragg [111] introduced a smoothing procedure to remove oscillations of the leading
error term by defining
</p>
<p>vj =
1
</p>
<p>4
uj&minus;1 +
</p>
<p>1
</p>
<p>2
uj +
</p>
<p>1
</p>
<p>4
uj+1. (12.92)
</p>
<p>He showed that both approximations (12.91, 12.92) have an asymptotic expansion
in powers of h2 and are therefore well suited for an extrapolation method. The mod-
ified midpoint method can be summarized as follows:
</p>
<p>u0 = Y(tn)
u1 = u0 + hf (u0, tn)
uj+1 = uj&minus;1 + 2hf (uj , tn + jh) j = 1,2, . . . ,N &minus; 1
</p>
<p>Y(tn +�t)&asymp;
1
</p>
<p>2
</p>
<p>(
uN + uN&minus;1 + hf (uN , tn +�t)
</p>
<p>)
.
</p>
<p>(12.93)
</p>
<p>The number of sub-steps N is increased according to a sequence like
</p>
<p>N = 2,4,6,8,12,16,24,32,48,64 &middot; &middot; &middot; Nj = 2Nj&minus;2 Bulirsch-St&ouml;r sequence
(12.94)
</p>
<p>or
</p>
<p>N = 2,4,6,8,10,12 &middot; &middot; &middot; Nj = 2j Deuflhard sequence.
</p>
<p>After each successive N is tried, a polynomial extrapolation is attempted. This ex-
trapolation returns both the extrapolated values and an error estimate. If the error is
still too large then N has to be increased further. A more detailed discussion can be
found in [233, 234].</p>
<p/>
</div>
<div class="page"><p/>
<p>222 12 Equations of Motion
</p>
<p>Fig. 12.9 Adams-Bashforth
method
</p>
<p>12.10 Linear Multistep Methods
</p>
<p>All methods discussed so far evaluated one or more values of the gradient f (Y (t), t)
only within the interval tn &middot; &middot; &middot; tn+�t . If the state vector changes sufficiently smooth
then multistep methods can be applied. Linear multistep methods use a combination
of function values Yn and gradients fn from several steps
</p>
<p>Yn+1 =
k&sum;
</p>
<p>j=1
(αjYn&minus;j+1 + βjfn&minus;j+1�t)+ β0fn+1�t (12.95)
</p>
<p>where the coefficients α, β are determined such, that a polynomial of certain or-
der r is integrated exactly. The method is explicit if β0 = 0 and implicit otherwise.
Multistep methods have a small local error and need fewer function evaluations. On
the other hand, they have to be combined with other methods (like Runge-Kutta) to
start and end properly and it can be rather complicated to change the step size during
the calculation. Three families of linear multistep methods are commonly used: ex-
plicit Adams-Bashforth methods, implicit Adams-Moulton methods and backward
differentiation formulas (also known as Gear formulas [102]).
</p>
<p>12.10.1 Adams-Bashforth Methods
</p>
<p>The explicit Adams-Bashforth method of order r uses the gradients from the last
r &minus; 1 steps (Fig. 12.9) to obtain the polynomial
</p>
<p>p(tn)= f (Yn, tn) &middot; &middot; &middot;p(tn&minus;r+1)= f (Yn&minus;r+1, tn&minus;r+1) (12.96)
and to calculate the approximation
</p>
<p>Yn+1 &minus; Yn &asymp;
&int; tn+1
tn
</p>
<p>p(t)dt
</p>
<p>which is generally a linear combination of fn &middot; &middot; &middot;fn&minus;r+1. For example, the Adams-
Bashforth formulas of order 2, 3, 4 are:</p>
<p/>
</div>
<div class="page"><p/>
<p>12.10 Linear Multistep Methods 223
</p>
<p>Fig. 12.10 Adams-Moulton
method
</p>
<p>Yn+1 &minus; Yn =
�t
</p>
<p>2
(3fn &minus; fn&minus;1)+O
</p>
<p>(
�t3
</p>
<p>)
</p>
<p>Yn+1 &minus; Yn =
�t
</p>
<p>12
(23fn &minus; 16fn&minus;1 + 5fm&minus;2)+O
</p>
<p>(
�t4
</p>
<p>)
</p>
<p>Yn+1 &minus; Yn =
�t
</p>
<p>24
(55fn &minus; 59fn&minus;1 + 37fn&minus;2 &minus; 9fn&minus;3)+O
</p>
<p>(
�t5
</p>
<p>)
.
</p>
<p>(12.97)
</p>
<p>12.10.2 Adams-Moulton Methods
</p>
<p>The implicit Adams-Moulton method also uses the yet not known value Yn+1
(Fig. 12.10) to obtain the polynomial
</p>
<p>p(tn+1)= fn+1 &middot; &middot; &middot;p(tn&minus;r+2)= fn&minus;r+2. (12.98)
The corresponding Adams-Moulton formulas of order 2 to 4 are:
</p>
<p>Yn+1 &minus; Yn =
�t
</p>
<p>2
(fn+1 + fn)+O
</p>
<p>(
�t3
</p>
<p>)
</p>
<p>Yn+1 &minus; Yn =
�t
</p>
<p>12
(5fn+1 + 8fn &minus; fn&minus;1)+O
</p>
<p>(
�t4
</p>
<p>)
(12.99)
</p>
<p>Yn+1 &minus; Yn =
�t
</p>
<p>24
(9fn+1 + 19fn &minus; 5fn&minus;1 + fn&minus;2)+O
</p>
<p>(
�t5
</p>
<p>)
. (12.100)
</p>
<p>12.10.3 Backward Differentiation (Gear) Methods
</p>
<p>Gear methods [102] are implicit and usually combined with a modified Newton
method. They make use of previous function values Yn, Yn&minus;1 . . . and the gradient
fn+1 at time t +�t . Only methods of order r &le; 6 are stable and useful. The general
formula (12.95) is
</p>
<p>Yn+1 =
r&sum;
</p>
<p>j=1
αjYn&minus;j+1 + β0fn+1�t. (12.101)</p>
<p/>
</div>
<div class="page"><p/>
<p>224 12 Equations of Motion
</p>
<p>For r = 1 this becomes
Yn+1 = α1Yn + β0f1�t (12.102)
</p>
<p>and all linear polynomials
</p>
<p>p = p0 + p1(t &minus; tn),
dp
</p>
<p>dt
= p1 (12.103)
</p>
<p>are integrated exactly if
</p>
<p>p0 + p1�t = α1p0 + β0p1 (12.104)
which is the case for
</p>
<p>α1 = 1, β0 =�t. (12.105)
Hence the first order Gear method is
</p>
<p>Yn+1 = Yn + fn+1�t +O
(
�t2
</p>
<p>)
(12.106)
</p>
<p>which coincides with the implicit Euler method. The higher order stable Gear meth-
ods are given by
</p>
<p>r = 2: Yn+1 =
4
</p>
<p>3
Yn &minus;
</p>
<p>1
</p>
<p>3
Yn&minus;1 +
</p>
<p>2
</p>
<p>3
fn+1�t +O
</p>
<p>(
�t3
</p>
<p>)
(12.107)
</p>
<p>r = 3: Yn+1 =
18
</p>
<p>11
Yn &minus;
</p>
<p>9
</p>
<p>11
Yn&minus;1 +
</p>
<p>2
</p>
<p>11
Yn&minus;2 +
</p>
<p>6
</p>
<p>11
fn+1�t
</p>
<p>+O
(
�t4
</p>
<p>)
(12.108)
</p>
<p>r = 4: Yn+1 =
48
</p>
<p>25
Yn &minus;
</p>
<p>36
</p>
<p>25
Yn&minus;1 +
</p>
<p>16
</p>
<p>25
Yn&minus;2
</p>
<p>&minus; 3
25
</p>
<p>Yn&minus;3 +
12
</p>
<p>25
fn+1�t +O
</p>
<p>(
�t5
</p>
<p>)
(12.109)
</p>
<p>r = 5: Yn+1 =
300
</p>
<p>137
Yn &minus;
</p>
<p>300
</p>
<p>137
Yn&minus;1 +
</p>
<p>200
</p>
<p>137
Yn&minus;2 &minus;
</p>
<p>75
</p>
<p>137
Yn&minus;3
</p>
<p>+ 12
137
</p>
<p>Yn&minus;4 +
60
</p>
<p>137
fn+1�t +O
</p>
<p>(
�t6
</p>
<p>)
(12.110)
</p>
<p>r = 6: Yn+1 =
120
</p>
<p>49
Yn &minus;
</p>
<p>150
</p>
<p>49
Yn&minus;1 +
</p>
<p>400
</p>
<p>147
Yn&minus;2 &minus;
</p>
<p>75
</p>
<p>49
Yn&minus;3 +
</p>
<p>24
</p>
<p>49
Yn&minus;4
</p>
<p>&minus; 10
147
</p>
<p>Yn&minus;5 +
20
</p>
<p>49
fn+1�t +O
</p>
<p>(
�t7
</p>
<p>)
. (12.111)
</p>
<p>This class of algorithms is useful also for stiff problems (differential equations with
strongly varying eigenvalues).
</p>
<p>12.10.4 Predictor-Corrector Methods
</p>
<p>The Adams-Bashforth-Moulton method combines the explicit method as a predictor
step to calculate an estimate ypn+1 with a corrector step using the implicit method of</p>
<p/>
</div>
<div class="page"><p/>
<p>12.11 Verlet Methods 225
</p>
<p>same order. The general class of linear multistep predictor-corrector methods [100]
uses a predictor step
</p>
<p>Y
(0)
n+1 =
</p>
<p>k&sum;
</p>
<p>j=1
</p>
<p>(
α
(p)
j Yn&minus;j+1 + β
</p>
<p>(p)
j fn&minus;j+1�t
</p>
<p>)
(12.112)
</p>
<p>which is corrected using the formula
</p>
<p>Y
(1)
n+1 =
</p>
<p>k&sum;
</p>
<p>j=1
</p>
<p>(
α
(c)
j Yn&minus;j+1 + β
</p>
<p>(c)
j fn&minus;j+1�t
</p>
<p>)
+ β0f
</p>
<p>(
Y
(0)
n+1, tn+1
</p>
<p>)
�t (12.113)
</p>
<p>and further iterations
</p>
<p>Y
(m+1)
n+1 = Y
</p>
<p>(m)
n+1 &minus; β0
</p>
<p>[
f
(
Y
(m&minus;1)
n+1 , tn+1
</p>
<p>)
&minus; f
</p>
<p>(
Y
(m)
n+1, tn+1
</p>
<p>)]
�t
</p>
<p>m= 1 . . .M &minus; 1 (12.114)
Yn+1 = Y (M)n+1 , Ẏn+1 = f
</p>
<p>(
Y
(M&minus;1)
n+1 , tn+1
</p>
<p>)
. (12.115)
</p>
<p>The coefficients α,β have to be determined to optimize accuracy and stability.
</p>
<p>12.11 Verlet Methods
</p>
<p>For classical molecular dynamics simulations it is necessary to calculate very long
trajectories. Here a family of symplectic methods often is used which conserve the
phase space volume [3, 116, 193, 255, 257, 265]. The equations of motion of a
classical interacting N -body system are
</p>
<p>mi ẍi = Fi (12.116)
</p>
<p>where the force acting on atom i can be calculated once a specific force field is
chosen. Let us write these equations as a system of first order differential equations
</p>
<p>(
ẋi
v̇i
</p>
<p>)
=
(
vi
ai
</p>
<p>)
(12.117)
</p>
<p>where x(t) and v(t) are functions of time and the forces ma(x(t)) are functions of
the time dependent coordinates.
</p>
<p>12.11.1 Liouville Equation
</p>
<p>We rewrite (12.117) as
(
ẋ
</p>
<p>v̇
</p>
<p>)
= L
</p>
<p>(
x
</p>
<p>v
</p>
<p>)
(12.118)</p>
<p/>
</div>
<div class="page"><p/>
<p>226 12 Equations of Motion
</p>
<p>where the Liouville operator L acts on the vector containing all coordinates and
velocities:
</p>
<p>L
</p>
<p>(
x
</p>
<p>v
</p>
<p>)
=
(
v
&part;
</p>
<p>&part;x
+ a &part;
</p>
<p>&part;v
</p>
<p>)(
x
</p>
<p>v
</p>
<p>)
. (12.119)
</p>
<p>The Liouville equation (12.118) can be formally solved by
(
x(t)
</p>
<p>v(t)
</p>
<p>)
= eLt
</p>
<p>(
x(0)
v(0)
</p>
<p>)
. (12.120)
</p>
<p>For a better understanding let us evaluate the first members of the Taylor series of
the exponential:
</p>
<p>L
</p>
<p>(
x
</p>
<p>v
</p>
<p>)
=
(
v
&part;
</p>
<p>&part;x
+ a &part;
</p>
<p>&part;v
</p>
<p>)(
x
</p>
<p>v
</p>
<p>)
=
(
v
</p>
<p>a
</p>
<p>)
(12.121)
</p>
<p>L
2
(
x
</p>
<p>v
</p>
<p>)
=
(
v
&part;
</p>
<p>&part;x
+ a &part;
</p>
<p>&part;v
</p>
<p>)(
v
</p>
<p>a(x)
</p>
<p>)
=
(
</p>
<p>a
</p>
<p>v &part;
&part;x
a
</p>
<p>)
(12.122)
</p>
<p>L
3
(
x
</p>
<p>v
</p>
<p>)
=
(
v
&part;
</p>
<p>&part;x
+ a &part;
</p>
<p>&part;v
</p>
<p>)(
a
</p>
<p>v &part;
&part;x
a
</p>
<p>)
=
(
</p>
<p>v &part;
&part;x
a
</p>
<p>a &part;
&part;x
a+ vv &part;
</p>
<p>&part;x
&part;
&part;x
a
</p>
<p>)
. (12.123)
</p>
<p>But since
</p>
<p>d
</p>
<p>dt
a
(
x(t)
</p>
<p>)
= v &part;
</p>
<p>&part;x
a (12.124)
</p>
<p>d2
</p>
<p>dt2
a
(
x(t)
</p>
<p>)
= d
</p>
<p>dt
</p>
<p>(
v
&part;
</p>
<p>&part;x
a
</p>
<p>)
= a &part;
</p>
<p>&part;x
a+ vv &part;
</p>
<p>&part;x
</p>
<p>&part;
</p>
<p>&part;x
a (12.125)
</p>
<p>we recover
(
</p>
<p>1 + tL+ 1
2
t2L2 + 1
</p>
<p>6
t3L3 + &middot; &middot; &middot;
</p>
<p>)(
x
</p>
<p>v
</p>
<p>)
=
(
x+ vt + 12 t2a+
</p>
<p>1
6 t
</p>
<p>3ȧ+ &middot; &middot; &middot;
v+ at + 12 t2ȧ+
</p>
<p>1
6 t
</p>
<p>3ä+ &middot; &middot; &middot;
</p>
<p>)
.
</p>
<p>(12.126)
</p>
<p>12.11.2 Split-Operator Approximation
</p>
<p>We introduce a small time step �t = t/N and write
</p>
<p>eLt =
(
eL�t
</p>
<p>)N
. (12.127)
</p>
<p>For the small time step �t the split-operator approximation can be used which ap-
proximately factorizes the exponential operator. For example, write the Liouville
operator as the sum of two terms
</p>
<p>LA = v
&part;
</p>
<p>&part;x
LB = a
</p>
<p>&part;
</p>
<p>&part;v
</p>
<p>and make the approximation
</p>
<p>eL�t = eLA�teLB�t + &middot; &middot; &middot; . (12.128)</p>
<p/>
</div>
<div class="page"><p/>
<p>12.11 Verlet Methods 227
</p>
<p>Fig. 12.11 (Position Verlet
method) The exact
integration path is
approximated by two
half-steps with constant
velocities and one step with
constant coordinates
</p>
<p>Each of the two factors simply shifts positions or velocities
</p>
<p>eLA�t
(
x
</p>
<p>v
</p>
<p>)
=
(
x+ v�t
</p>
<p>v
</p>
<p>)
eLB�t
</p>
<p>(
x
</p>
<p>v
</p>
<p>)
=
(
</p>
<p>x
</p>
<p>v+ a�t
</p>
<p>)
(12.129)
</p>
<p>since these two steps correspond to either motion with constant velocities or con-
stant coordinates and forces.
</p>
<p>12.11.3 Position Verlet Method
</p>
<p>Often the following approximation is used which is symmetrical in time
</p>
<p>eL�t = eLA�t/2eLB�teLA�t/2 + &middot; &middot; &middot; . (12.130)
The corresponding algorithm is the so called position Verlet method (Fig. 12.11):
</p>
<p>x
n+ 12
</p>
<p>= xn + vn
�t
</p>
<p>2
(12.131)
</p>
<p>vn+1 = vn + an+ 12 �t = v(tn +�t)+O
(
�t3
</p>
<p>)
(12.132)
</p>
<p>xn+1 = xn+ 12 + vn+1
�t
</p>
<p>2
</p>
<p>= xn +
vn + vn+1
</p>
<p>2
�t = x(tn +�t)+O
</p>
<p>(
�t3
</p>
<p>)
. (12.133)
</p>
<p>12.11.4 Velocity Verlet Method
</p>
<p>If we exchange operators A and B we have
</p>
<p>eL�t = eLB�t/2eLA�teLB�t/2 + &middot; &middot; &middot; (12.134)
</p>
<p>which produces the velocity Verlet algorithm (Fig. 12.12):</p>
<p/>
</div>
<div class="page"><p/>
<p>228 12 Equations of Motion
</p>
<p>Fig. 12.12 (Velocity Verlet
method) The exact
integration path is
approximated by two
half-steps with constant
coordinates and one step with
constant velocities
</p>
<p>v
n+ 12
</p>
<p>= vn + an
�t
</p>
<p>2
(12.135)
</p>
<p>xn+1 = xn + vn+ 12 �t = xn + vn�t + an
�t2
</p>
<p>2
= x(tn +�t)+O
</p>
<p>(
�t3
</p>
<p>)
(12.136)
</p>
<p>vn+1 = vn+ 12 + an+1
�t
</p>
<p>2
= vn +
</p>
<p>an + an+1
2
</p>
<p>�t
</p>
<p>= v(tn +�t)+O
(
�t3
</p>
<p>)
. (12.137)
</p>
<p>12.11.5 St&ouml;rmer-Verlet Method
</p>
<p>The velocity Verlet method is equivalent to St&ouml;rmer&rsquo;s version [208] of the Verlet
method which is a two step method given by
</p>
<p>xn+1 = 2xn &minus; xn&minus;1 + an�t2 (12.138)
</p>
<p>vn =
xn+1 &minus; xn&minus;1
</p>
<p>2�t
. (12.139)
</p>
<p>To show the equivalence we add two consecutive position vectors
</p>
<p>xn+2 + xn+1 = 2xn+1 + 2xn &minus; xn &minus; xn&minus;1 + (an+1 + an)�t2 (12.140)
</p>
<p>which simplifies to
</p>
<p>xn+2 &minus; xn &minus; (xn+1 &minus; xn)= (an+1 + an)�t2. (12.141)
</p>
<p>This can be expressed as the difference of two consecutive velocities:
</p>
<p>2(vn+1 &minus; vn)= (an+1 + an)�t. (12.142)
</p>
<p>Now we substitute
</p>
<p>xn&minus;1 = xn+1 &minus; 2vn�t (12.143)</p>
<p/>
</div>
<div class="page"><p/>
<p>12.11 Verlet Methods 229
</p>
<p>to get
</p>
<p>xn+1 = 2xn &minus; xn+1 + 2vn�t + an�t2 (12.144)
</p>
<p>which simplifies to
</p>
<p>xn+1 = xn + vn�t +
an
</p>
<p>2
�t2. (12.145)
</p>
<p>Thus the equations of the velocity Verlet algorithm have been recovered. However,
since the Verlet method is a 2-step method, the choice of initial values is important.
The St&ouml;rmer-Verlet method starts from two coordinate sets x0, x1. The first step is
</p>
<p>x2 = 2x1 &minus; x0 + a1�t2 (12.146)
</p>
<p>v1 =
x2 &minus; x0
</p>
<p>2�t
= x1 &minus; x0
</p>
<p>�t
+ a1
</p>
<p>2
�t2. (12.147)
</p>
<p>The velocity Verlet method, on the other hand, starts from one set of coordinates
and velocities x1, v1. Here the first step is
</p>
<p>x2 = x1 + v1�t + a1
�t2
</p>
<p>2
(12.148)
</p>
<p>v2 = v1 +
a1 + a2
</p>
<p>2
�t. (12.149)
</p>
<p>The two methods give the same resulting trajectory if we choose
</p>
<p>x0 = x1 &minus; v1�t +
a1
</p>
<p>2
�t2. (12.150)
</p>
<p>If, on the other hand, x0 is known with higher precision, the local error order of
St&ouml;rmer&rsquo;s algorithm changes as can be seen from addition of the two Taylor series
</p>
<p>x(tn +�t)= xn + vn�t +
an
</p>
<p>2
�t2 + ȧn
</p>
<p>6
�t3 + &middot; &middot; &middot; (12.151)
</p>
<p>x(tn &minus;�t)= xn &minus; vn�t +
an
</p>
<p>2
�t2 &minus; ȧn
</p>
<p>6
�t3 + &middot; &middot; &middot; (12.152)
</p>
<p>which gives
</p>
<p>x(tn +�t)= 2x(tn)&minus; x(tn &minus;�t)+ an�t2 +O
(
�t4
</p>
<p>)
(12.153)
</p>
<p>x(tn +�t)&minus; x(tn &minus;�t)
2�t
</p>
<p>= vn +O
(
�t2
</p>
<p>)
. (12.154)
</p>
<p>12.11.6 Error Accumulation for the St&ouml;rmer-Verlet Method
</p>
<p>Equation (12.153) gives only the local error of one single step. Assume the start
values x0 and x1 are exact. The next value x2 has an error with the leading term
�x2 = α�t4. If the trajectory is sufficiently smooth and the time step not too large</p>
<p/>
</div>
<div class="page"><p/>
<p>230 12 Equations of Motion
</p>
<p>the coefficient α will vary only slowly and the error of the next few iterations is
given by
</p>
<p>�x3 = 2�x2 &minus;�x1 = 2α�t4
</p>
<p>�x4 = 2�x3 &minus;�x2 = 3α�t4
...
</p>
<p>�xn+1 = nα�t4.
</p>
<p>(12.155)
</p>
<p>This shows that the effective error order of the St&ouml;rmer-Verlet method is only
O(�t3) similar to the velocity Verlet method.
</p>
<p>12.11.7 Beeman&rsquo;s Method
</p>
<p>Beeman and Schofield [17, 229] introduced a method which is very similar to the
St&ouml;rmer-Verlet method but calculates the velocities with higher precision. This is
important if, for instance, the kinetic energy has to be calculated. Starting from the
Taylor series
</p>
<p>xn+1 = xn + vn�t + an
�t2
</p>
<p>2
+ ȧn
</p>
<p>�t3
</p>
<p>6
+ än
</p>
<p>�t4
</p>
<p>24
+ &middot; &middot; &middot; (12.156)
</p>
<p>the derivative of the acceleration is approximated by a backward difference
</p>
<p>xn+1 = xn + vn�t + an
�t2
</p>
<p>2
+ an &minus; an&minus;1
</p>
<p>�t
</p>
<p>�t3
</p>
<p>6
+O
</p>
<p>(
�t4
</p>
<p>)
</p>
<p>= xn + vn�t +
4an &minus; an&minus;1
</p>
<p>6
�t2 +O
</p>
<p>(
�t4
</p>
<p>)
. (12.157)
</p>
<p>This equation can be used as an explicit step to update the coordinates or as a pre-
dictor step in combination with the implicit corrector step
</p>
<p>xn+1 = xn + vn�t + an
�t2
</p>
<p>2
+ an+1 &minus; an
</p>
<p>�t
</p>
<p>�t3
</p>
<p>6
+O
</p>
<p>(
�t4
</p>
<p>)
</p>
<p>= xn + vn�t +
an+1 + 2an
</p>
<p>6
�t2 +O
</p>
<p>(
�t4
</p>
<p>)
(12.158)
</p>
<p>which can be applied repeatedly (usually two iterations are sufficient). Similarly, the
Taylor series of the velocity is approximated by
</p>
<p>vn+1 = vn + an�t + ȧn
�t2
</p>
<p>2
+ än
</p>
<p>�t3
</p>
<p>6
+ &middot; &middot; &middot;
</p>
<p>= vn + an�t +
(
an+1 &minus; an
</p>
<p>�t
+O(�t)
</p>
<p>)
�t2
</p>
<p>2
+ &middot; &middot; &middot;
</p>
<p>= vn +
an+1 + an
</p>
<p>2
�t +O
</p>
<p>(
�t3
</p>
<p>)
. (12.159)
</p>
<p>Inserting the velocity from (12.158) we obtain the corrector step for the velocity</p>
<p/>
</div>
<div class="page"><p/>
<p>12.11 Verlet Methods 231
</p>
<p>vn+1 =
xn+1 &minus; xn
</p>
<p>�t
&minus; an+1 + 2an
</p>
<p>6
�t + an+1 + an
</p>
<p>2
�t +O
</p>
<p>(
�t3
</p>
<p>)
</p>
<p>= xn+1 &minus; xn
�t
</p>
<p>+ 2an+1 + an
6
</p>
<p>�t +O
(
�t3
</p>
<p>)
. (12.160)
</p>
<p>In combination with (12.157) this can be replaced by
</p>
<p>vn+1 = vn +
4an &minus; an&minus;1
</p>
<p>6
�t + 2an+1 + an
</p>
<p>6
�t +O
</p>
<p>(
�t3
</p>
<p>)
</p>
<p>= vn +
2an+1 + 5an &minus; an&minus;1
</p>
<p>6
�t +O
</p>
<p>(
�t3
</p>
<p>)
. (12.161)
</p>
<p>Together, (12.157) and (12.161) provide an explicit method which is usually un-
derstood as Beeman&rsquo;s method. Inserting the velocity (12.160) from the previous
step
</p>
<p>vn =
xn &minus; xn&minus;1
</p>
<p>�t
+ 2an + an&minus;1
</p>
<p>6
�t +O
</p>
<p>(
�t3
</p>
<p>)
(12.162)
</p>
<p>into (12.157) gives
</p>
<p>xn+1 = 2xn &minus; xn&minus;1 + an�t2 +O
(
�t4
</p>
<p>)
(12.163)
</p>
<p>which coincides with the St&ouml;rmer-Verlet method (12.138). We conclude that Bee-
man&rsquo;s method should produce the same trajectory as the St&ouml;rmer-Verlet method if
numerical errors can be neglected and comparable initial values are used. In fact, the
St&ouml;rmer-Verlet method may suffer from numerical extinction and Beeman&rsquo;s method
provides a numerically more favorable alternative.
</p>
<p>12.11.8 The Leapfrog Method
</p>
<p>Closely related to the Verlet methods is the so called leapfrog method [116]. It uses
the simple decomposition
</p>
<p>eL�t &asymp; eLA�teLB�t (12.164)
but introduces two different time grids for coordinates and velocities which are
shifted by �t/2 (Fig. 12.13).
</p>
<p>The leapfrog algorithm is given by
</p>
<p>v
n+ 12
</p>
<p>= v
n&minus; 12
</p>
<p>+ an�t (12.165)
xn+1 = xn + vn+ 12 �t. (12.166)
</p>
<p>Due to the shifted arguments the order of the method is increased as can be seen
from the Taylor series:
</p>
<p>x(tn)+
(
v(tn)+
</p>
<p>�t
</p>
<p>2
a(tn)+ &middot; &middot; &middot;
</p>
<p>)
�t = x(tn +�t)+O
</p>
<p>(
�t3
</p>
<p>)
(12.167)
</p>
<p>v
</p>
<p>(
tn +
</p>
<p>�t
</p>
<p>2
</p>
<p>)
&minus; v
</p>
<p>(
tn &minus;
</p>
<p>�t
</p>
<p>2
</p>
<p>)
= a(tn)�t +O
</p>
<p>(
�t3
</p>
<p>)
. (12.168)</p>
<p/>
</div>
<div class="page"><p/>
<p>232 12 Equations of Motion
</p>
<p>Fig. 12.13 (Leapfrog
method) The exact
integration path is
approximated by one step
with constant coordinates and
one step with constant
velocities. Two different grids
are used for coordinates and
velocities which are shifted
by �t/2
</p>
<p>One disadvantage of the leapfrog method is that some additional effort is necessary
if the velocities are needed. The simple expression
</p>
<p>v(tn)=
1
</p>
<p>2
</p>
<p>(
v
</p>
<p>(
tn &minus;
</p>
<p>�t
</p>
<p>2
</p>
<p>)
+ v
</p>
<p>(
tn +
</p>
<p>�t
</p>
<p>2
</p>
<p>))
+O
</p>
<p>(
�t2
</p>
<p>)
(12.169)
</p>
<p>is of lower error order than (12.168).
</p>
<p>12.12 Problems
</p>
<p>Problem 12.1 (Circular orbits) In this computer experiment we consider a mass
point moving in a central field. The equation of motion can be written as the follow-
ing system of first order equations:
</p>
<p>⎛
⎜⎜⎝
</p>
<p>ẋ
</p>
<p>ẏ
</p>
<p>v̇x
v̇y
</p>
<p>⎞
⎟⎟⎠=
</p>
<p>⎛
⎜⎜⎜⎜⎝
</p>
<p>0 0 1 0
0 0 0 1
</p>
<p>&minus; 1
(x2+y2)
</p>
<p>3
2
</p>
<p>0 0 0
</p>
<p>0 &minus; 1
(x2+y2)
</p>
<p>3
2
</p>
<p>0 0
</p>
<p>⎞
⎟⎟⎟⎟⎠
</p>
<p>⎛
⎜⎜⎝
</p>
<p>x
</p>
<p>y
</p>
<p>vx
vy
</p>
<p>⎞
⎟⎟⎠ . (12.170)
</p>
<p>For initial values
⎛
⎜⎜⎝
</p>
<p>x
</p>
<p>y
</p>
<p>vx
vy
</p>
<p>⎞
⎟⎟⎠=
</p>
<p>⎛
⎜⎜⎝
</p>
<p>1
0
0
1
</p>
<p>⎞
⎟⎟⎠ (12.171)
</p>
<p>the exact solution is given by
</p>
<p>x = cos t y = sin t. (12.172)
The following methods are used to calculate the position x(t), y(t) and the energy
</p>
<p>Etot =Ekin +Epot =
1
</p>
<p>2
</p>
<p>(
v2x + v2y
</p>
<p>)
&minus; 1&radic;
</p>
<p>x2 + y2
. (12.173)</p>
<p/>
</div>
<div class="page"><p/>
<p>12.12 Problems 233
</p>
<p>&bull; The explicit Euler method (12.3)
x(tn+1)= x(tn)+ vx(tn)�t
y(tn+1)= y(tn)+ vy(tn)�t
</p>
<p>vx(tn+1)= vx(tn)&minus;
x(tn)
</p>
<p>R(tn)3
�t
</p>
<p>vy(tn+1)= vy(tn)&minus;
y(tn)
</p>
<p>R(tn)3
�t.
</p>
<p>(12.174)
</p>
<p>&bull; The 2nd order Runge-Kutta method (12.7.1)
</p>
<p>which consists of the predictor step
</p>
<p>x(tn +�t/2)= x(tn)+
�t
</p>
<p>2
vx(tn) (12.175)
</p>
<p>y(tn +�t/2)= y(tn)+
�t
</p>
<p>2
vy(tn) (12.176)
</p>
<p>vx(tn +�t/2)= vx(tn)&minus;
�t
</p>
<p>2
</p>
<p>x(tn)
</p>
<p>R(tn)3
(12.177)
</p>
<p>vy(tn +�t/2)= vy(tn)&minus;
�t
</p>
<p>2
</p>
<p>y(tn)
</p>
<p>R(tn)3
(12.178)
</p>
<p>and the corrector step
</p>
<p>x(tn+1)= x(tn)+�t vx(tn +�t/2) (12.179)
y(tn+1)= y(tn)+�t vy(tn +�t/2) (12.180)
</p>
<p>vx(tn+1)= vx(tn)&minus;�t
x(tn +�t/2)
R3(tn +�t/2)
</p>
<p>(12.181)
</p>
<p>vy(tn+1)= vy(tn)&minus;�t
y(tn +�t/2)
R3(tn +�t/2)
</p>
<p>. (12.182)
</p>
<p>&bull; The fourth order Runge-Kutta method (12.7.3)
&bull; The Verlet method (12.11.5)
</p>
<p>x(tn+1)= x(tn)+
(
x(tn)&minus; x(tn&minus;1)
</p>
<p>)
&minus;�t x(tn)
</p>
<p>R3(tn)
(12.183)
</p>
<p>y(tn+1)= y(tn)+
(
y(tn)&minus; y(tn&minus;1)
</p>
<p>)
&minus;�t y(tn)
</p>
<p>R3(tn)
(12.184)
</p>
<p>vx(tn)=
x(tn+1)&minus; x(tn&minus;1)
</p>
<p>2�t
= x(tn)&minus; x(tn&minus;1)
</p>
<p>�t
&minus; �t
</p>
<p>2
</p>
<p>x(tn)
</p>
<p>R3(tn)
(12.185)
</p>
<p>vy(tn)=
y(tn+1)&minus; y(tn&minus;1)
</p>
<p>2�t
= y(tn)&minus; y(tn&minus;1)
</p>
<p>�t
&minus; �t
</p>
<p>2
</p>
<p>y(tn)
</p>
<p>R3(tn)
. (12.186)
</p>
<p>To start the Verlet method we need additional coordinates at time &minus;�t which can
be chosen from the exact solution or from the approximation</p>
<p/>
</div>
<div class="page"><p/>
<p>234 12 Equations of Motion
</p>
<p>x(t&minus;1)= x(t0)&minus;�t vx(t0)&minus;
�t2
</p>
<p>2
</p>
<p>x(t0)
</p>
<p>R3(t0)
(12.187)
</p>
<p>y(t&minus;1)= y(t0)&minus;�t vy(t0)&minus;
�t2
</p>
<p>2
</p>
<p>y(t0)
</p>
<p>R3(t0)
. (12.188)
</p>
<p>&bull; The leapfrog method (12.11.8)
</p>
<p>x(tn+1)= x(tn)+ vx(tn+ 12 )�t (12.189)
y(tn+1)= y(tn)+ vy(tn+ 12 )�t (12.190)
</p>
<p>vx(tn+ 12
)= vx(tn&minus; 12 )&minus;
</p>
<p>x(tn)
</p>
<p>R(tn)3
�t (12.191)
</p>
<p>vy(tn+ 12
)= vy(tn&minus; 12 )&minus;
</p>
<p>y(tn)
</p>
<p>R(tn)3
�t (12.192)
</p>
<p>where the velocity at time tn is calculated from
</p>
<p>vx(tn)= vx(tn+ 12 )&minus;
�t
</p>
<p>2
</p>
<p>x(tn+1)
</p>
<p>R3(tn+1)
(12.193)
</p>
<p>vy(tn)= vy(tn+ 12 )&minus;
�t
</p>
<p>2
</p>
<p>y(tn+1)
</p>
<p>R3(tn+1)
. (12.194)
</p>
<p>To start the leapfrog method we need the velocity at time t&minus; 12
which can be taken
</p>
<p>from the exact solution or from
</p>
<p>vx(t&minus; 12
)= vx(t0)&minus;
</p>
<p>�t
</p>
<p>2
</p>
<p>x(t0)
</p>
<p>R3(t0)
(12.195)
</p>
<p>vy(t&minus; 12
)= vy(t0)&minus;
</p>
<p>�t
</p>
<p>2
</p>
<p>y(t0)
</p>
<p>R3(t0)
. (12.196)
</p>
<p>Compare the conservation of energy for the different methods as a function of the
time step �t . Study the influence of the initial values for leapfrog and Verlet meth-
ods.
</p>
<p>Problem 12.2 (N -body system) In this computer experiment we simulate the mo-
tion of three mass points under the influence of gravity. Initial coordinates and ve-
locities as well as the masses can be varied. The equations of motion are solved with
the 4th order Runge-Kutta method with quality control for different step sizes. The
local integration error is estimated using the step doubling method. Try to simulate
a planet with a moon moving round a sun!
</p>
<p>Problem 12.3 (Adams-Bashforth method) In this computer experiment we simu-
late a circular orbit with the Adams-Bashforth method of order 2 &middot; &middot; &middot;7. The absolute
error at time T
</p>
<p>�(T )=
∣∣x(T )&minus; cos(T )
</p>
<p>∣∣+
∣∣y(t)&minus; sin(T )
</p>
<p>∣∣+
∣∣vx(T )+ sin(T )
</p>
<p>∣∣
+
∣∣vy(T )&minus; cos(T )
</p>
<p>∣∣ (12.197)</p>
<p/>
</div>
<div class="page"><p/>
<p>12.12 Problems 235
</p>
<p>is shown as a function of the time step �t in a log-log plot. From the slope
</p>
<p>s = d(log10(�))
d(log10(�t))
</p>
<p>(12.198)
</p>
<p>the leading error order s can be determined. For very small step sizes rounding
errors become dominating which leads to an increase �&sim; (�t)&minus;1.
</p>
<p>Determine maximum precision and optimal step size for different orders of the
method. Compare with the explicit Euler method.</p>
<p/>
</div>
<div class="page"><p/>
<p>Part II
</p>
<p>Simulation of Classical and Quantum
Systems</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 13
</p>
<p>Rotational Motion
</p>
<p>An asymmetric top under the influence of time dependent external forces is a rather
complicated subject in mechanics. Efficient methods to describe the rotational mo-
tion are important as well in astrophysics as in molecular physics. The orientation
of a rigid body relative to the laboratory system can be described by a 3 &times; 3 ma-
trix. Instead of solving nine equations for all its components, the rotation matrix can
be parametrized by the four real components of a quaternion. Euler angles use the
minimum necessary number of three parameters but have numerical disadvantages.
Care has to be taken to conserve the orthogonality of the rotation matrix. Omelyan&rsquo;s
implicit quaternion method is very efficient and conserves orthogonality exactly.
In computer experiments we compare different explicit and implicit methods for a
free rotor, we simulate a rotor in an external field and the collision of two rotating
molecules.
</p>
<p>13.1 Transformation to a Body Fixed Coordinate System
</p>
<p>Let us define a rigid body as a set of mass points mi with fixed relative orientation
(described by distances and angles).
</p>
<p>The position of mi in the laboratory coordinate system CS will be denoted by ri .
The position of the center of mass (COM) of the rigid body is
</p>
<p>R= 1&sum;
i mi
</p>
<p>&sum;
</p>
<p>i
</p>
<p>miri (13.1)
</p>
<p>and the position of mi within the COM coordinate system CSc (Fig. 13.1) is ρi :
</p>
<p>ri =R+ ρi . (13.2)
</p>
<p>Let us define a body fixed coordinate system CScb , where the position ρib of mi is
time independent ddt ρib = 0. ρi and ρib are connected by a linear vector function
</p>
<p>ρi =Aρib (13.3)
</p>
<p>P.O.J. Scherer, Computational Physics, Graduate Texts in Physics,
DOI 10.1007/978-3-319-00401-3_13,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>239</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-00401-3_13">http://dx.doi.org/10.1007/978-3-319-00401-3_13</a></div>
</div>
<div class="page"><p/>
<p>240 13 Rotational Motion
</p>
<p>Fig. 13.1 (Coordinate
systems) Three coordinate
systems will be used: The
laboratory system CS, the
center of mass system CSc
and the body fixed system
CScb
</p>
<p>where A is a 3 &times; 3 matrix
</p>
<p>A=
</p>
<p>⎛
⎝
a11 a12 a13
a21 a22 a23
a31 a32 a33
</p>
<p>⎞
⎠ . (13.4)
</p>
<p>13.2 Properties of the Rotation Matrix
</p>
<p>Rotation conserves the length of ρ:1
</p>
<p>ρT ρ = (Aρ)T (Aρ)= ρTATAρ. (13.5)
</p>
<p>Consider the matrix
</p>
<p>M =ATA&minus; 1 (13.6)
for which
</p>
<p>ρTMρ = 0 (13.7)
holds for all vectors ρ. Let us choose the unit vector in x-direction:
</p>
<p>ρ =
</p>
<p>⎛
⎝
</p>
<p>1
0
0
</p>
<p>⎞
⎠ .
</p>
<p>Then we have
</p>
<p>0 =
(
</p>
<p>1 0 0
)
⎛
⎝
M11 M12 M13
M21 M22 M23
M31 M32 M33
</p>
<p>⎞
⎠
⎛
⎝
</p>
<p>1
0
0
</p>
<p>⎞
⎠=M11. (13.8)
</p>
<p>1ρT ρ denotes the scalar product of two vectors whereas ρρT is the outer or matrix product.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.2 Properties of the Rotation Matrix 241
</p>
<p>Similarly by choosing a unit vector in y or z direction we find M22 =M33 = 0.
Now choose ρ =
</p>
<p>( 1
1
0
</p>
<p>)
:
</p>
<p>0 =
(
</p>
<p>1 1 0
)
⎛
⎝
M11 M12 M13
M21 M22 M23
M31 M32 M33
</p>
<p>⎞
⎠
⎛
⎝
</p>
<p>1
1
0
</p>
<p>⎞
⎠
</p>
<p>=
(
</p>
<p>1 1 0
)
⎛
⎝
M11 +M12
M21 +M22
M31 +M32
</p>
<p>⎞
⎠=M11 +M22 +M12 +M21. (13.9)
</p>
<p>Since the diagonal elements vanish we have M12 =&minus;M21. With
</p>
<p>ρ =
</p>
<p>⎛
⎝
</p>
<p>1
0
1
</p>
<p>⎞
⎠ , ρ =
</p>
<p>⎛
⎝
</p>
<p>0
1
1
</p>
<p>⎞
⎠
</p>
<p>we find M13 =&minus;M31 and M23 =&minus;M32, hence M is skew symmetric and has three
independent components
</p>
<p>M =&minus;MT =
</p>
<p>⎛
⎝
</p>
<p>0 M12 M13
&minus;M12 0 M23
&minus;M13 &minus;M23 0
</p>
<p>⎞
⎠ . (13.10)
</p>
<p>Inserting (13.6) we have
</p>
<p>(
ATA&minus; 1
</p>
<p>)
=&minus;
</p>
<p>(
ATA&minus; 1
</p>
<p>)T =&minus;
(
ATA&minus; 1
</p>
<p>)
(13.11)
</p>
<p>which shows that ATA= 1 or equivalently AT =A&minus;1. Hence (det(A))2 = 1 and A
is an orthogonal matrix. For a pure rotation without reflection only det(A)=+1 is
possible.
</p>
<p>From
</p>
<p>ri =R+Aρib (13.12)
we calculate the velocity
</p>
<p>dri
dt
</p>
<p>= dR
dt
</p>
<p>+ dA
dt
</p>
<p>ρib +A
dρib
dt
</p>
<p>(13.13)
</p>
<p>but since ρib is constant by definition, the last summand vanishes
</p>
<p>ṙi = Ṙ+ Ȧρib = Ṙ+ ȦA&minus;1ρi (13.14)
</p>
<p>and in the center of mass system we have
</p>
<p>d
</p>
<p>dt
ρi = ȦA&minus;1ρi =Wρi (13.15)</p>
<p/>
</div>
<div class="page"><p/>
<p>242 13 Rotational Motion
</p>
<p>Fig. 13.2 Infinitesimal
rotation
</p>
<p>with the matrix
</p>
<p>W = ȦA&minus;1. (13.16)
</p>
<p>13.3 Properties of W , Connection with the Vector of Angular
</p>
<p>Velocity
</p>
<p>Since rotation does not change the length of ρi , we have
</p>
<p>0 = d
dt
|ρi |2 &rarr; 0 = ρi
</p>
<p>d
</p>
<p>dt
ρi = ρi(Wρi) (13.17)
</p>
<p>or in matrix notation
</p>
<p>0 = ρTi Wρi . (13.18)
</p>
<p>This holds for arbitrary ρi . Hence W is skew symmetric and has three independent
components
</p>
<p>W =
</p>
<p>⎛
⎝
</p>
<p>0 W12 W13
&minus;W12 0 W23
&minus;W13 &minus;W23 0
</p>
<p>⎞
⎠ . (13.19)
</p>
<p>Now consider an infinitesimal rotation by the angle dϕ (Fig. 13.2).
Then we have (the index i is suppressed)
</p>
<p>dρ = dρ
dt
</p>
<p>dt =
</p>
<p>⎛
⎝
</p>
<p>0 W12 W13
&minus;W12 0 W23
&minus;W13 &minus;W23 0
</p>
<p>⎞
⎠
⎛
⎝
ρ1
ρ2
ρ3
</p>
<p>⎞
⎠dt =
</p>
<p>⎛
⎝
</p>
<p>W12ρ2 +W13ρ3
&minus;W12ρ1 +W23ρ3
&minus;W13ρ1 &minus;W23ρ2
</p>
<p>⎞
⎠dt
</p>
<p>(13.20)
</p>
<p>which can be written as a cross product:
</p>
<p>dρ = dϕ &times; ρ (13.21)</p>
<p/>
</div>
<div class="page"><p/>
<p>13.3 Properties of W , Connection with the Vector of Angular Velocity 243
</p>
<p>with
</p>
<p>dϕ =
</p>
<p>⎛
⎝
&minus;W23dt
W13dt
</p>
<p>&minus;W12dt
</p>
<p>⎞
⎠ . (13.22)
</p>
<p>But this can be expressed in terms of the angular velocity ω as
</p>
<p>dϕ = ωdt (13.23)
</p>
<p>and finally we have
</p>
<p>dϕ = ωdt =
</p>
<p>⎛
⎝
ω1
ω2
ω3
</p>
<p>⎞
⎠dt W =
</p>
<p>⎛
⎝
</p>
<p>0 &minus;ω3 ω2
ω3 0 &minus;ω1
&minus;ω2 ω1 0
</p>
<p>⎞
⎠ (13.24)
</p>
<p>and the more common form of the equation of motion
</p>
<p>d
</p>
<p>dt
ρ =Wρ = ω&times; ρ. (13.25)
</p>
<p>Example (Rotation around the z-axis) For constant angular velocity ω the equation
of motion
</p>
<p>d
</p>
<p>dt
ρ =Wρ (13.26)
</p>
<p>has the formal solution
</p>
<p>ρ = eWtρ(0)=A(t)ρ(0). (13.27)
The angular velocity vector for rotation around the z-axis is
</p>
<p>ω=
</p>
<p>⎛
⎝
</p>
<p>0
0
ω3
</p>
<p>⎞
⎠ (13.28)
</p>
<p>and
</p>
<p>W =
</p>
<p>⎛
⎝
</p>
<p>0 &minus;ω3 0
ω3 0 0
0 0 0
</p>
<p>⎞
⎠ . (13.29)
</p>
<p>Higher powers of W can be easily calculated since
</p>
<p>W 2 =
</p>
<p>⎛
⎝
&minus;ω23 0 0
</p>
<p>0 &minus;ω23 0
0 0 0
</p>
<p>⎞
⎠ (13.30)
</p>
<p>W 3 =&minus;ω23
</p>
<p>⎛
⎝
</p>
<p>0 &minus;ω3 0
ω3 0 0
0 0 0
</p>
<p>⎞
⎠ (13.31)</p>
<p/>
</div>
<div class="page"><p/>
<p>244 13 Rotational Motion
</p>
<p>etc., and the rotation matrix is obtained from the Taylor series
</p>
<p>A(t) = eWt = 1 +Wt + 1
2
W 2t2 + 1
</p>
<p>6
W 3t3 + &middot; &middot; &middot;
</p>
<p>= 1 +
</p>
<p>⎛
⎝
ω23t
</p>
<p>2 0 0
0 ω23t
</p>
<p>2 0
0 0 0
</p>
<p>⎞
⎠
(
&minus;1
</p>
<p>2
+
</p>
<p>ω23t
2
</p>
<p>24
+ &middot; &middot; &middot;
</p>
<p>)
</p>
<p>+
</p>
<p>⎛
⎝
</p>
<p>0 &minus;ω3t 0
ω3t 0 0
0 0 0
</p>
<p>⎞
⎠
(
</p>
<p>1 &minus;
ω23t
</p>
<p>2
</p>
<p>6
+ &middot; &middot; &middot;
</p>
<p>)
</p>
<p>=
</p>
<p>⎛
⎝
</p>
<p>cos(ω3t) &minus; sin(ω3t)
sin(ω3t) cos(ω3t)
</p>
<p>1
</p>
<p>⎞
⎠ . (13.32)
</p>
<p>13.4 Transformation Properties of the Angular Velocity
</p>
<p>Now imagine we are sitting on the rigid body and observe a mass point moving
outside. Its position in the laboratory system is r1. In the body fixed system we
observe it at
</p>
<p>ρ1b =A&minus;1(r1 &minus;R) (13.33)
</p>
<p>and its velocity in the body fixed system is
</p>
<p>ρ̇1b =A&minus;1(ṙ1 &minus; Ṙ)+
dA&minus;1
</p>
<p>dt
(r1 &minus;R). (13.34)
</p>
<p>The time derivative of the inverse matrix follows from
</p>
<p>0 = d
dt
</p>
<p>(
A&minus;1A
</p>
<p>)
=A&minus;1Ȧ+ dA
</p>
<p>&minus;1
</p>
<p>dt
A (13.35)
</p>
<p>dA&minus;1
</p>
<p>dt
=&minus;A&minus;1ȦA&minus;1 =&minus;A&minus;1W (13.36)
</p>
<p>and hence
</p>
<p>dA&minus;1
</p>
<p>dt
(r1 &minus;R)=&minus;A&minus;1W(r1 &minus;R). (13.37)
</p>
<p>Now we rewrite this using the angular velocity as it is observed in the body fixed
system
</p>
<p>&minus;A&minus;1W(r1 &minus;R)=&minus;WbA&minus;1(r1 &minus;R)=&minus;Wbρ1b =&minus;ωb &times; ρ1b (13.38)</p>
<p/>
</div>
<div class="page"><p/>
<p>13.4 Transformation Properties of the Angular Velocity 245
</p>
<p>where W transforms as like a rank-2 tensor
</p>
<p>Wb =A&minus;1WA. (13.39)
</p>
<p>From this equation the transformation properties of ω can be derived. We consider
only rotation around one axis explicitly, since a general rotation matrix can always
be written as a product of three rotations around different axes. For instance, rotation
around the z-axis gives:
</p>
<p>Wb =
</p>
<p>⎛
⎝
</p>
<p>0 &minus;ωb3 ωb2
ωb3 0 &minus;ωb1
&minus;ωb2 ωb1 0
</p>
<p>⎞
⎠
</p>
<p>=
</p>
<p>⎛
⎝
</p>
<p>cosϕ sinϕ 0
&minus; sinϕ cosϕ 0
</p>
<p>0 0 1
</p>
<p>⎞
⎠
⎛
⎝
</p>
<p>0 &minus;ω3 ω2
ω3 0 &minus;ω1
&minus;ω2 ω1 0
</p>
<p>⎞
⎠
⎛
⎝
</p>
<p>cosϕ &minus; sinϕ 0
sinϕ cosϕ 0
</p>
<p>0 0 1
</p>
<p>⎞
⎠
</p>
<p>=
</p>
<p>⎛
⎝
</p>
<p>0 &minus;ω3 ω2 cosϕ &minus;ω1 sinϕ
ω3 0 &minus;(ω1 cosϕ +ω2 sinϕ)
</p>
<p>&minus;(ω2 cosϕ &minus;ω1 sinϕ) ω1 cosϕ +ω2 sinϕ 0
</p>
<p>⎞
⎠
</p>
<p>(13.40)
</p>
<p>which shows that
⎛
⎝
ω1b
ω2b
ω3b
</p>
<p>⎞
⎠=
</p>
<p>⎛
⎝
</p>
<p>cosϕ sinϕ 0
&minus; sinϕ cosϕ 0
</p>
<p>0 0 1
</p>
<p>⎞
⎠
⎛
⎝
ω1
ω2
ω3
</p>
<p>⎞
⎠=A&minus;1ω (13.41)
</p>
<p>i.e. ω transforms like a vector under rotations. However, there is a subtle difference
considering general coordinate transformations involving reflections. For example,
under reflection at the xy-plane W is transformed according to
</p>
<p>Wb =
</p>
<p>⎛
⎝
</p>
<p>1 0 0
0 1 0
0 0 &minus;1
</p>
<p>⎞
⎠
⎛
⎝
</p>
<p>0 &minus;ω3 ω2
ω3 0 &minus;ω1
&minus;ω2 ω1 0
</p>
<p>⎞
⎠
⎛
⎝
</p>
<p>1 0 0
0 1 0
0 0 &minus;1
</p>
<p>⎞
⎠
</p>
<p>=
</p>
<p>⎛
⎝
</p>
<p>0 &minus;ω3 &minus;ω2
ω3 0 ω1
ω2 &minus;ω1 0
</p>
<p>⎞
⎠ (13.42)
</p>
<p>and the transformed angular velocity vector is
⎛
⎝
ω1b
ω2b
ω3b
</p>
<p>⎞
⎠=&minus;
</p>
<p>⎛
⎝
</p>
<p>1 0 0
0 1 0
0 0 &minus;1
</p>
<p>⎞
⎠
⎛
⎝
ω1
ω2
ω3
</p>
<p>⎞
⎠ . (13.43)
</p>
<p>This is characteristic of a so called axial or pseudo-vector. Under a general coordi-
nate transformation it transforms as
</p>
<p>ωb = det(A)Aω. (13.44)</p>
<p/>
</div>
<div class="page"><p/>
<p>246 13 Rotational Motion
</p>
<p>13.5 Momentum and Angular Momentum
</p>
<p>The total momentum is
</p>
<p>P=
&sum;
</p>
<p>i
</p>
<p>mi ṙi =
&sum;
</p>
<p>i
</p>
<p>miṘ=MṘ (13.45)
</p>
<p>since by definition we have
&sum;
</p>
<p>i miρi = 0.
The total angular momentum can be decomposed into the contribution of the
</p>
<p>center of mass motion and the contribution relative to the center of mass
</p>
<p>L=
&sum;
</p>
<p>i
</p>
<p>miri &times; ṙi =MR&times; Ṙ+
&sum;
</p>
<p>i
</p>
<p>miρi &times; ρ̇i = LCOM +Lint. (13.46)
</p>
<p>The second contribution is
</p>
<p>Lint =
&sum;
</p>
<p>i
</p>
<p>miρi &times; (ω&times; ρi)=
&sum;
</p>
<p>i
</p>
<p>mi
(
ωρ2i &minus; ρi(ρiω)
</p>
<p>)
. (13.47)
</p>
<p>This is a linear vector function of ω, which can be expressed simpler by introducing
the tensor of inertia
</p>
<p>I =
&sum;
</p>
<p>i
</p>
<p>miρ
2
i 1 &minus;miρiρTi (13.48)
</p>
<p>or component-wise
</p>
<p>Im,n =
&sum;
</p>
<p>i
</p>
<p>miρ
2
i δm,n &minus;miρi,mρi,n (13.49)
</p>
<p>as
</p>
<p>Lint = Iω. (13.50)
</p>
<p>13.6 Equations of Motion of a Rigid Body
</p>
<p>Let Fi be an external force acting on mi . Then the equation of motion for the center
of mass is
</p>
<p>d2
</p>
<p>dt2
&sum;
</p>
<p>i
</p>
<p>miri =MR̈=
&sum;
</p>
<p>i
</p>
<p>Fi = Fext. (13.51)
</p>
<p>If there is no total external force Fext, the center of mass moves with constant veloc-
ity
</p>
<p>R=R0 +V(t &minus; t0). (13.52)
The time derivative of the angular momentum equals the total external torque
</p>
<p>d
</p>
<p>dt
L= d
</p>
<p>dt
</p>
<p>&sum;
</p>
<p>i
</p>
<p>miri &times; ṙi =
&sum;
</p>
<p>i
</p>
<p>miri &times; r̈i =
&sum;
</p>
<p>i
</p>
<p>ri &times; Fi =
&sum;
</p>
<p>i
</p>
<p>Ni =Next (13.53)</p>
<p/>
</div>
<div class="page"><p/>
<p>13.7 Moments of Inertia 247
</p>
<p>which can be decomposed into
</p>
<p>Next =R&times; Fext +
&sum;
</p>
<p>i
</p>
<p>ρi &times; Fi . (13.54)
</p>
<p>With the decomposition of the angular momentum
</p>
<p>d
</p>
<p>dt
L= d
</p>
<p>dt
LCOM +
</p>
<p>d
</p>
<p>dt
Lint (13.55)
</p>
<p>we have two separate equations for the two contributions:
</p>
<p>d
</p>
<p>dt
LCOM =
</p>
<p>d
</p>
<p>dt
MR&times; Ṙ=MR&times; R̈=R&times; Fext (13.56)
</p>
<p>d
</p>
<p>dt
Lint =
</p>
<p>&sum;
</p>
<p>i
</p>
<p>ρi &times; Fi =Next &minus;R&times; Fext =Nint. (13.57)
</p>
<p>13.7 Moments of Inertia
</p>
<p>The angular momentum (13.50) is
</p>
<p>Lrot = Iω=AA&minus;1IAA&minus;1ω=AIbωb (13.58)
</p>
<p>where the tensor of inertia in the body fixed system is
</p>
<p>Ib = A&minus;1IA=A&minus;1
(&sum;
</p>
<p>i
</p>
<p>miρ
T
i ρi &minus;miρiρTi
</p>
<p>)
A
</p>
<p>=
&sum;
</p>
<p>i
</p>
<p>miA
T ρTi ρiA&minus;miAT ρiρTi A
</p>
<p>=
&sum;
</p>
<p>i
</p>
<p>miρ
2
ib &minus;miρibρTib. (13.59)
</p>
<p>Since Ib does not depend on time (by definition of the body fixed system) we
will use the principal axes of Ib as the axes of the body fixed system. Then Ib takes
the simple form
</p>
<p>Ib =
</p>
<p>⎛
⎝
I1 0 0
0 I2 0
0 0 I3
</p>
<p>⎞
⎠ (13.60)
</p>
<p>with the principle moments of inertia I1,2,3.</p>
<p/>
</div>
<div class="page"><p/>
<p>248 13 Rotational Motion
</p>
<p>13.8 Equations of Motion for a Rotor
</p>
<p>The following equations describe pure rotation of a rigid body:
</p>
<p>d
</p>
<p>dt
A=WA=AWb (13.61)
</p>
<p>d
</p>
<p>dt
Lint =Nint (13.62)
</p>
<p>W =
</p>
<p>⎛
⎝
</p>
<p>0 &minus;ω3 ω2
ω3 0 &minus;ω1
&minus;ω2 ω1 0
</p>
<p>⎞
⎠ Wij =&minus;εijkωk (13.63)
</p>
<p>Lint =ALint,b = Iω=AIbωb (13.64)
</p>
<p>ωb = I&minus;1b Lint,b =
</p>
<p>⎛
⎝
I&minus;11 0 0
</p>
<p>0 I&minus;12 0
0 0 I&minus;13
</p>
<p>⎞
⎠Lint,b ω=Aωb (13.65)
</p>
<p>Ib = const. (13.66)
</p>
<p>13.9 Explicit Methods
</p>
<p>Equation (13.61) for the rotation matrix and (13.62) for the angular momentum have
to be solved by a suitable algorithm. The simplest integrator is the explicit Euler
method (Fig. 13.3) [241]:
</p>
<p>A(t +�t)=A(t)+A(t)Wb(t)�t +O
(
�t2
</p>
<p>)
(13.67)
</p>
<p>Lint(t +�t)= Lint(t)+Nint(t)�t +O
(
�t2
</p>
<p>)
. (13.68)
</p>
<p>Expanding the Taylor series of A(t) to second order we have the second order ap-
proximation (Fig. 13.3)
</p>
<p>A(t +�t)=A(t)+A(t)Wb(t)�t +
1
</p>
<p>2
</p>
<p>(
A(t)W 2b (t)+A(t)Ẇb(t)
</p>
<p>)
�t2 +O
</p>
<p>(
�t3
</p>
<p>)
.
</p>
<p>(13.69)
A corresponding second order expression for the angular momentum involves
</p>
<p>the time derivative of the forces and is usually not practicable.
The time derivative of W can be expressed via the time derivative of the angular
</p>
<p>velocity which can be calculated as follows:</p>
<p/>
</div>
<div class="page"><p/>
<p>13.9 Explicit Methods 249
</p>
<p>Fig. 13.3 (Global error of the explicit methods) The equations of a free rotor (13.8) are solved
using the explicit first order (full curves) and second order (dashed curves) method. The deviations
|det(A) &minus; 1| (diamonds) and |Ekin &minus; Ekin(0)| (circles) at t = 10 are shown as a function of the
time step �t . Full circles show the energy deviation of the first order method with reorthogonal-
ization. The principal moments of inertia are Ib = diag(1,2,3) and the initial angular momentum
is L= (1,1,1). See also Problem 13.1
</p>
<p>d
</p>
<p>dt
ωb =
</p>
<p>d
</p>
<p>dt
</p>
<p>(
I&minus;1b A
</p>
<p>&minus;1Lint
)
= I&minus;1b
</p>
<p>(
d
</p>
<p>dt
A&minus;1
</p>
<p>)
Lint + I&minus;1b A
</p>
<p>&minus;1Nint
</p>
<p>= I&minus;1b
(
&minus;A&minus;1W
</p>
<p>)
Lint + I&minus;1b A
</p>
<p>&minus;1Nint
</p>
<p>= &minus;I&minus;1b WbLint,b + I
&minus;1
b Nint,b. (13.70)
</p>
<p>Alternatively, in the laboratory system
</p>
<p>d
</p>
<p>dt
ω = d
</p>
<p>dt
(Aωb)=WAωb &minus;AI&minus;1b A
</p>
<p>&minus;1WLint +AI&minus;1b A
&minus;1Nint
</p>
<p>= AI&minus;1b A(Nint &minus;WLint) (13.71)
</p>
<p>where the first summand vanishes due to
</p>
<p>WAωb =AWbωb =Aωb &times;ωb = 0. (13.72)
</p>
<p>Substituting the angular momentum we have
</p>
<p>d
</p>
<p>dt
ωb = I&minus;1b Nint,b &minus; I
</p>
<p>&minus;1
b WbIbωb (13.73)
</p>
<p>which reads in components:</p>
<p/>
</div>
<div class="page"><p/>
<p>250 13 Rotational Motion
</p>
<p>⎛
⎝
ω̇b1
ω̇b2
ω̇b3
</p>
<p>⎞
⎠ =
</p>
<p>⎛
⎝
I&minus;1b1 Nb1
I&minus;1b2 Nb2
I&minus;1b3 Nb3
</p>
<p>⎞
⎠
</p>
<p>&minus;
</p>
<p>⎛
⎝
I&minus;1b1
</p>
<p>I&minus;1b2
I&minus;1b3
</p>
<p>⎞
⎠
⎛
⎝
</p>
<p>0 &minus;ωb3 ωb2
ωb3 0 &minus;ωb1
&minus;ωb2 ωb1 0
</p>
<p>⎞
⎠
⎛
⎝
Ib1ωb1
Ib2ωb2
Ib3ωb3
</p>
<p>⎞
⎠ .
</p>
<p>(13.74)
</p>
<p>Evaluation of the product gives a set of equations which are well known as Euler&rsquo;s
equations:
</p>
<p>ω̇b1 =
Ib2 &minus; Ib3
</p>
<p>Ib1
ωb2ωb3 +
</p>
<p>Nb1
</p>
<p>Ib1
</p>
<p>ω̇b2 =
Ib3 &minus; Ib1
</p>
<p>Ib2
ωb3ωb1 +
</p>
<p>Nb2
</p>
<p>Ib2
(13.75)
</p>
<p>ω̇b3 =
Ib1 &minus; Ib2
</p>
<p>Ib3
ωb1ωb2 +
</p>
<p>Nb3
</p>
<p>Ib3
.
</p>
<p>13.10 Loss of Orthogonality
</p>
<p>The simple methods above do not conserve the orthogonality of A. This is an effect
of higher order but the error can accumulate quickly. Consider the determinant of A.
For the simple explicit Euler scheme we have
</p>
<p>det(A+�A) = det(A+WA�t)= detAdet(1 +W�t)
= detA
</p>
<p>(
1 +ω2�t2
</p>
<p>)
. (13.76)
</p>
<p>The error is of order �t2, but the determinant will continuously increase, i.e. the
rigid body will explode. For the second order integrator we find
</p>
<p>det(A+�A) = det
(
A+WA�t + �t
</p>
<p>2
</p>
<p>2
</p>
<p>(
W 2A+ ẆA
</p>
<p>))
</p>
<p>= detAdet
(
</p>
<p>1 +W�t + �t
2
</p>
<p>2
</p>
<p>(
W 2 + Ẇ
</p>
<p>))
. (13.77)
</p>
<p>This can be simplified to give
</p>
<p>det(A+�A)= detA
(
1 + ω̇ω�t3 + &middot; &middot; &middot;
</p>
<p>)
. (13.78)
</p>
<p>The second order method behaves somewhat better since the product of angular
velocity and acceleration can change in time. To assure that A remains a rotation</p>
<p/>
</div>
<div class="page"><p/>
<p>13.11 Implicit Method 251
</p>
<p>matrix we must introduce constraints or reorthogonalize A at least after some steps
(for instance every time when |det(A)&minus; 1| gets larger than a certain threshold). The
following method with a symmetric correction matrix is a very useful alternative
[127]. The non-singular square matrix A can be decomposed into the product of an
orthonormal matrix Ã and a positive semi-definite matrix S
</p>
<p>A= ÃS (13.79)
</p>
<p>with the positive definite square root of the symmetric matrix ATA
</p>
<p>S =
(
ATA
</p>
<p>)1/2 (13.80)
</p>
<p>and
</p>
<p>Ã=AS&minus;1 =A
(
ATA
</p>
<p>)&minus;1/2 (13.81)
</p>
<p>which is orthonormal as can be seen from
</p>
<p>ÃT Ã=
(
S&minus;1
</p>
<p>)T
ATAS&minus;1 = S&minus;1S2S&minus;1 = 1. (13.82)
</p>
<p>Since the deviation of A from orthogonality is small, we make the approximations
</p>
<p>S = 1 + s (13.83)
</p>
<p>ATA= S2 &asymp; 1 + 2s (13.84)
</p>
<p>s &asymp; A
TA&minus; 1
</p>
<p>2
(13.85)
</p>
<p>S&minus;1 &asymp; 1 &minus; s &asymp; 1 + 1 &minus;A
TA
</p>
<p>2
+ &middot; &middot; &middot; (13.86)
</p>
<p>which can be easily evaluated.
</p>
<p>13.11 Implicit Method
</p>
<p>The quality of the method can be significantly improved by taking the time deriva-
tive at midstep (Fig. 13.4) (12.5):
</p>
<p>A(t +�t)=A(t)+A
(
t + �t
</p>
<p>2
</p>
<p>)
W
</p>
<p>(
t + �t
</p>
<p>2
</p>
<p>)
�t + &middot; &middot; &middot; (13.87)
</p>
<p>Lint(t +�t)= Lint(t)+Nint
(
t + �t
</p>
<p>2
</p>
<p>)
�t + &middot; &middot; &middot; . (13.88)
</p>
<p>Taylor series expansion gives</p>
<p/>
</div>
<div class="page"><p/>
<p>252 13 Rotational Motion
</p>
<p>Fig. 13.4 (Global error of
the implicit method) The
equations of a free rotor
(13.8) are solved using the
implicit method. The
deviations |det(A)&minus; 1|
(diamonds) and
|Ekin &minus;Ekin(0)| (circles) at
t = 10 are shown as a
function of the time step �t .
Initial conditions as in
Fig. 13.3. See also
Problem 13.1
</p>
<p>A
</p>
<p>(
t + �t
</p>
<p>2
</p>
<p>)
W
</p>
<p>(
t + �t
</p>
<p>2
</p>
<p>)
�t
</p>
<p>=A(t)W(t)�t + Ȧ(t)W(t)�t
2
</p>
<p>2
+A(t)Ẇ (t)�t
</p>
<p>2
</p>
<p>2
+O
</p>
<p>(
�t3
</p>
<p>)
(13.89)
</p>
<p>=A(t)W(t)�t +
(
A(t)W 2(t)+A(t)Ẇ (t)
</p>
<p>)�t2
2
</p>
<p>+O
(
�t3
</p>
<p>)
(13.90)
</p>
<p>which has the same error order as the explicit second order method. The matrix
A(t + �t2 ) at mid-time can be approximated by
</p>
<p>1
</p>
<p>2
</p>
<p>(
A(t)+A(t +�t)
</p>
<p>)
= A
</p>
<p>(
t + �t
</p>
<p>2
</p>
<p>)
+ �t
</p>
<p>2
</p>
<p>4
Ä
</p>
<p>(
t + �t
</p>
<p>2
</p>
<p>)
+ &middot; &middot; &middot;
</p>
<p>= A
(
t + �t
</p>
<p>2
</p>
<p>)
+O
</p>
<p>(
�t2
</p>
<p>)
(13.91)
</p>
<p>which does not change the error order of the implicit integrator which now becomes
</p>
<p>A(t +�t)=A(t)+ 1
2
</p>
<p>(
A(t)+A(t +�t)
</p>
<p>)
W
</p>
<p>(
t + �t
</p>
<p>2
</p>
<p>)
�t +O
</p>
<p>(
�t3
</p>
<p>)
. (13.92)
</p>
<p>This equation can be formally solved by
</p>
<p>A(t +�t) = A(t)
(
</p>
<p>1 + �t
2
W
</p>
<p>(
t + �t
</p>
<p>2
</p>
<p>))(
1 &minus; �t
</p>
<p>2
W
</p>
<p>(
t + �t
</p>
<p>2
</p>
<p>))&minus;1
</p>
<p>= A(t)Tb
(
�t
</p>
<p>2
</p>
<p>)
. (13.93)</p>
<p/>
</div>
<div class="page"><p/>
<p>13.11 Implicit Method 253
</p>
<p>Alternatively, using angular velocities in the laboratory system we have the similar
expression
</p>
<p>A(t +�t) =
[
</p>
<p>1 &minus; �t
2
W
</p>
<p>(
t + �t
</p>
<p>2
</p>
<p>)]&minus;1[
1 + �t
</p>
<p>2
W
</p>
<p>(
t + �t
</p>
<p>2
</p>
<p>)]
A(t)
</p>
<p>= T
(
�t
</p>
<p>2
</p>
<p>)
A(t). (13.94)
</p>
<p>The angular velocities at midtime can be calculated with sufficient accuracy from
</p>
<p>W
</p>
<p>(
t + �t
</p>
<p>2
</p>
<p>)
=W(t)+ �t
</p>
<p>2
Ẇ (t)+O
</p>
<p>(
�t2
</p>
<p>)
. (13.95)
</p>
<p>With the help of an algebra program we easily prove that
</p>
<p>det
</p>
<p>(
1 + �t
</p>
<p>2
W
</p>
<p>)
= det
</p>
<p>(
1 &minus; �t
</p>
<p>2
W
</p>
<p>)
= 1 + ω
</p>
<p>2�t2
</p>
<p>4
(13.96)
</p>
<p>and therefore the determinant of the rotation matrix is conserved. The necessary
matrix inversion can be easily done:
</p>
<p>[
1 &minus; �t
</p>
<p>2
W
</p>
<p>]&minus;1
</p>
<p>=
</p>
<p>⎛
⎜⎜⎝
</p>
<p>1 + ω
2
1�t
</p>
<p>2
</p>
<p>4 &minus;ω3
�t
2 +ω1ω2
</p>
<p>�t2
</p>
<p>4 ω2
�t
2 +ω1ω3
</p>
<p>�t2
</p>
<p>4
</p>
<p>ω3
�t
2 +ω1ω2
</p>
<p>�t2
</p>
<p>4 1 +
ω22�t
</p>
<p>2
</p>
<p>4 &minus;ω1
�t
2 +ω2ω3
</p>
<p>�t2
</p>
<p>4
</p>
<p>&minus;ω2 �t2 +ω1ω3
�t2
</p>
<p>4 ω1
�t
2 +ω2ω3
</p>
<p>�t2
</p>
<p>4 1 +
ω23�t
</p>
<p>2
</p>
<p>4
</p>
<p>⎞
⎟⎟⎠
</p>
<p>&times; 1
1 +ω2 �t24
</p>
<p>. (13.97)
</p>
<p>The matrix product is explicitly
</p>
<p>Tb =
[
</p>
<p>1 + �t
2
Wb
</p>
<p>][
1 &minus; �t
</p>
<p>2
Wb
</p>
<p>]&minus;1
</p>
<p>=
</p>
<p>⎛
⎜⎜⎝
</p>
<p>1 + ω
2
b1&minus;ω2b2&minus;ω2b3
</p>
<p>4 �t
2 &minus;ωb3�t +ωb1ωb2 �t
</p>
<p>2
</p>
<p>2 ωb2�t +ωb1ωb3
�t2
</p>
<p>2
</p>
<p>ωb3�t +ωb1ωb2 �t
2
</p>
<p>2 1 +
&minus;ω2b1+ω2b2&minus;ω2b3
</p>
<p>4 �t
2 &minus;ωb1�t +ωb2ωb3 �t
</p>
<p>2
</p>
<p>2
</p>
<p>&minus;ωb2�t +ωb1ωb3 �t
2
</p>
<p>2 ωb1�t +ωb2ωb3
�t2
</p>
<p>2 1 +
&minus;ω2b1&minus;ω2b2+ω2b3
</p>
<p>4 �t
2
</p>
<p>⎞
⎟⎟⎠
</p>
<p>&times; 1
1 +ω2b �t
</p>
<p>2
</p>
<p>4
</p>
<p>. (13.98)
</p>
<p>With the help of an algebra program it can be proved that this matrix is even orthog-
onal
</p>
<p>T Tb Tb = 1 (13.99)</p>
<p/>
</div>
<div class="page"><p/>
<p>254 13 Rotational Motion
</p>
<p>and hence the orthonormality of A is conserved. The approximation for the angular
momentum
</p>
<p>Lint(t)+Nint
(
t + �t
</p>
<p>2
</p>
<p>)
�t
</p>
<p>= Lint(t)+Nint(t)�t + Ṅint(t)
�t2
</p>
<p>2
+ &middot; &middot; &middot; = Lint(t +�t)+O
</p>
<p>(
�t3
</p>
<p>)
</p>
<p>(13.100)
</p>
<p>can be used in an implicit way
</p>
<p>Lint(t +�t)= Lint(t)+
Nint(t +�t)+Nint(t)
</p>
<p>2
�t +O
</p>
<p>(
�t3
</p>
<p>)
. (13.101)
</p>
<p>Alternatively Euler&rsquo;s equations can be used in the form [190, 191]
</p>
<p>ωb1
</p>
<p>(
t + �t
</p>
<p>2
</p>
<p>)
= ωb1
</p>
<p>(
t &minus; �t
</p>
<p>2
</p>
<p>)
+ Ib2 &minus; Ib3
</p>
<p>Ib1
ωb2(t)ωb3(t)�t +
</p>
<p>Nb1
</p>
<p>Ib1
�t etc.
</p>
<p>(13.102)
where the product ωb2(t)ωb3(t) is approximated by
</p>
<p>ωb2(t)ωb3(t)=
1
</p>
<p>2
</p>
<p>[
ωb2
</p>
<p>(
t &minus; �t
</p>
<p>2
</p>
<p>)
ωb3
</p>
<p>(
t &minus; �t
</p>
<p>2
</p>
<p>)
+ωb2
</p>
<p>(
t + �t
</p>
<p>2
</p>
<p>)
ωb3
</p>
<p>(
t + �t
</p>
<p>2
</p>
<p>)]
.
</p>
<p>(13.103)
</p>
<p>ωb1(t + �t2 ) is determined by iterative solution of the last two equations. Starting
with ωb1(t &minus; �t2 ) convergence is achieved after few iterations.
</p>
<p>Example (Free symmetric rotor) For the special case of a free symmetric rotor
(Ib2 = Ib3,Nint = 0) Euler&rsquo;s equations simplify to:
</p>
<p>ω̇b1 = 0 (13.104)
</p>
<p>ω̇b2 =
Ib2(3) &minus; Ib1
</p>
<p>Ib2(3)
ωb1ωb3 = λωb3 (13.105)
</p>
<p>ω̇b3 =
Ib1 &minus; Ib2(3)
</p>
<p>Ib2(3)
ωb1ωb2 =&minus;λωb2 (13.106)
</p>
<p>λ= Ib2(3) &minus; Ib1
Ib2(3)
</p>
<p>ωb1. (13.107)
</p>
<p>Coupled equations of this type appear often in physics. The solution can be found
using a complex quantity
</p>
<p>Ω = ωb2 + iωb3 (13.108)</p>
<p/>
</div>
<div class="page"><p/>
<p>13.12 Kinetic Energy of a Rotor 255
</p>
<p>which obeys the simple differential equation
</p>
<p>Ω̇ = ω̇b2 + iω̇b3 =&minus;i(iλωb3 + λωb2)=&minus;iλΩ (13.109)
</p>
<p>with the solution
</p>
<p>Ω =Ω0e&minus;iλt . (13.110)
Finally
</p>
<p>ωb =
</p>
<p>⎛
⎝
</p>
<p>ωb1(0)
&real;(Ω0e&minus;iλt )
&image;(Ω0e&minus;iλt )
</p>
<p>⎞
⎠=
</p>
<p>⎛
⎝
</p>
<p>ωb1(0)
ωb2(0) cos(λt)+ωb3(0) sin(λt)
ωb3(0) cos(λt)&minus;ωb2(0) sin(λt)
</p>
<p>⎞
⎠ (13.111)
</p>
<p>i.e. ωb rotates around the 1-axis with frequency λ.
</p>
<p>13.12 Kinetic Energy of a Rotor
</p>
<p>The kinetic energy of the rotor is
</p>
<p>Ekin =
&sum;
</p>
<p>i
</p>
<p>mi
</p>
<p>2
ṙ2i =
</p>
<p>&sum;
</p>
<p>i
</p>
<p>mi
</p>
<p>2
(Ṙ+ Ȧρib)2
</p>
<p>=
&sum;
</p>
<p>i
</p>
<p>mi
</p>
<p>2
</p>
<p>(
ṘT + ρTibȦT
</p>
<p>)
(Ṙ + Ȧρib)
</p>
<p>= M
2
Ṙ2 +
</p>
<p>&sum;
</p>
<p>i
</p>
<p>mi
</p>
<p>2
ρTibȦ
</p>
<p>T Ȧρib. (13.112)
</p>
<p>The second part is the contribution of the rotational motion. It can be written as
</p>
<p>Erot =
&sum;
</p>
<p>i
</p>
<p>mi
</p>
<p>2
ρTibW
</p>
<p>T
b A
</p>
<p>TAWbρib =&minus;
&sum;
</p>
<p>i
</p>
<p>mi
</p>
<p>2
ρTibW
</p>
<p>2
b ρib =
</p>
<p>1
</p>
<p>2
ωTb Ibωb (13.113)
</p>
<p>since
</p>
<p>&minus;W 2b =
</p>
<p>⎛
⎝
ω2b3 +ω2b2 &minus;ωb1ωb2 &minus;ωb1ωb3
&minus;ωb1ωb2 ω2b1 +ω2b3 &minus;ωb2ωb3
&minus;ωb1ωb3 &minus;ωb2ωb3 ω2b1 +ω2b2
</p>
<p>⎞
⎠= ω2b &minus;ωbωTb . (13.114)
</p>
<p>13.13 Parametrization by Euler Angles
</p>
<p>So far we had to solve equations for all 9 components of the rotation matrix. But
there are six constraints since the column vectors of the matrix have to be orthonor-
malized. Therefore the matrix can be parametrized with less than 9 variables. In fact</p>
<p/>
</div>
<div class="page"><p/>
<p>256 13 Rotational Motion
</p>
<p>it is sufficient to use only three variables. This can be achieved by splitting the full
rotation into three rotations around different axis. Most common are Euler angles
defined by the orthogonal matrix [106]
(
</p>
<p>cosψ cosφ &minus; cos θ sinφ sinψ &minus; sinψ cosφ &minus; cos θ sinφ cosψ sin θ sinφ
cosψ sinφ + cos θ cosφ sinψ &minus; sinψ sinφ + cos θ cosφ cosψ &minus; sin θ cosφ
</p>
<p>sin θ sinψ sin θ cosψ cos θ
</p>
<p>)
</p>
<p>(13.115)
obeying the equations
</p>
<p>φ̇ = ωx
sinφ cos θ
</p>
<p>sin θ
+ωy
</p>
<p>cosφ cos θ
</p>
<p>sin θ
+ωz (13.116)
</p>
<p>θ̇ = ωx cosφ +ωy sinφ (13.117)
</p>
<p>ψ̇ = ωx
sinφ
</p>
<p>sin θ
&minus;ωy
</p>
<p>cosφ
</p>
<p>sin θ
. (13.118)
</p>
<p>Different versions of Euler angles can be found in the literature, together with
the closely related cardanic angles. For all of them a sin θ appears in the denomi-
nator which causes numerical instabilities at the poles. One possible solution to this
problem is to switch between two different coordinate systems.
</p>
<p>13.14 Cayley-Klein Parameters, Quaternions, Euler Parameters
</p>
<p>There exists another parametrization of the rotation matrix which is very suitable for
numerical calculations. It is connected with the algebra of the so called quaternions.
The vector space of the complex 2&times;2 matrices can be spanned using Pauli matrices
by
</p>
<p>1 =
(
</p>
<p>1 0
0 1
</p>
<p>)
σx =
</p>
<p>(
0 1
1 0
</p>
<p>)
σy =
</p>
<p>(
0 &minus;i
i 0
</p>
<p>)
σz =
</p>
<p>(
1 0
0 &minus;1
</p>
<p>)
. (13.119)
</p>
<p>Any complex 2 &times; 2 matrix can be written as a linear combination
</p>
<p>c01 + cσ . (13.120)
</p>
<p>Accordingly any vector x &isin;R3 can be mapped onto a complex 2 &times; 2 matrix:
</p>
<p>x&rarr; P =
(
</p>
<p>z x &minus; iy
x + iy &minus;z
</p>
<p>)
. (13.121)
</p>
<p>Rotation of the coordinate system leads to the transformation
</p>
<p>P &prime; =QPQ&dagger; (13.122)
</p>
<p>where
</p>
<p>Q=
(
α β
</p>
<p>γ δ
</p>
<p>)
(13.123)</p>
<p/>
</div>
<div class="page"><p/>
<p>13.14 Cayley-Klein Parameters, Quaternions, Euler Parameters 257
</p>
<p>is a complex 2&times;2 rotation matrix. Invariance of the length (|x| =
&radic;
&minus;det(P )) under
</p>
<p>rotation implies that Q must be unitary, i.e. Q&dagger; = Q&minus;1 and its determinant must
be 1. Explicitly
</p>
<p>Q&dagger; =
(
α&lowast; γ &lowast;
</p>
<p>β&lowast; δ&lowast;
</p>
<p>)
=Q&minus;1 = 1
</p>
<p>αδ &minus; βγ
</p>
<p>(
δ &minus;β
&minus;γ α
</p>
<p>)
(13.124)
</p>
<p>and Q has the form
</p>
<p>Q=
(
</p>
<p>α β
</p>
<p>&minus;β&lowast; α&lowast;
)
</p>
<p>with |α|2 + |β|2 = 1. (13.125)
</p>
<p>Setting x&plusmn; = x &plusmn; iy, the transformed matrix has the same form as P :
</p>
<p>QPQ&dagger; =
(
α&lowast;βx+ + β&lowast;αx&minus; + (|α|2 &minus; |β|2)z &minus;β2x+ + α2x&minus; &minus; 2αβz
</p>
<p>α&lowast;2x+ &minus; β&lowast;2x&minus; &minus; 2α&lowast;β&lowast;z &minus;α&lowast;βx+ &minus; αβ&lowast;x&minus; &minus; (|α|2 &minus; |β|2)z
</p>
<p>)
</p>
<p>=
(
</p>
<p>z&prime; x&prime;&minus;
x&prime;+ &minus;z&prime;
</p>
<p>)
. (13.126)
</p>
<p>From comparison we find the transformed vector components:
</p>
<p>x&prime; = 1
2
</p>
<p>(
x&prime;+ + x&prime;&minus;
</p>
<p>)
= 1
</p>
<p>2
</p>
<p>(
α&lowast;2 &minus; β2
</p>
<p>)
x+ +
</p>
<p>1
</p>
<p>2
</p>
<p>(
α2 &minus; β&lowast;2
</p>
<p>)
x&minus; &minus;
</p>
<p>(
αβ + α&lowast;β&lowast;
</p>
<p>)
z
</p>
<p>= α
&lowast;2 + α2 &minus; β&lowast;2 &minus; β2
</p>
<p>2
x + i(α
</p>
<p>&lowast;2 &minus; α2 + β&lowast;2 &minus; β2)
2
</p>
<p>y
</p>
<p>&minus;
(
αβ + α&lowast;β&lowast;
</p>
<p>)
z (13.127)
</p>
<p>y&prime; = 1
2i
</p>
<p>(
x&prime;+ &minus; x&prime;&minus;
</p>
<p>)
= 1
</p>
<p>2i
</p>
<p>(
α&lowast;2 + β2
</p>
<p>)
x+ +
</p>
<p>1
</p>
<p>2i
</p>
<p>(
&minus;β&lowast;2 &minus; α2
</p>
<p>)
x&minus; +
</p>
<p>1
</p>
<p>i
</p>
<p>(
&minus;α&lowast;β&lowast; + αβ
</p>
<p>)
z
</p>
<p>= α
&lowast;2 &minus; α2 &minus; β&lowast;2 + β2
</p>
<p>2i
x + α
</p>
<p>&lowast;2 + α2 + β&lowast;2 + β2
2
</p>
<p>y
</p>
<p>+ i
(
α&lowast;β&lowast; &minus; αβ
</p>
<p>)
z (13.128)
</p>
<p>z&prime; =
(
α&lowast;β + αβ&lowast;
</p>
<p>)
x + i
</p>
<p>(
α&lowast;β &minus; αβ&lowast;
</p>
<p>)
y +
</p>
<p>(
|α|2 &minus; |β|2
</p>
<p>)
z. (13.129)
</p>
<p>This gives us the rotation matrix in terms of the Cayley-Klein parameters α and β:
</p>
<p>A=
</p>
<p>⎛
⎜⎝
</p>
<p>α&lowast;2+α2&minus;β&lowast;2&minus;β2
2
</p>
<p>i(α&lowast;2&minus;α2+β&lowast;2&minus;β2)
2 &minus;(αβ + α&lowast;β&lowast;)
</p>
<p>α&lowast;2&minus;α2&minus;β&lowast;2+β2
2i
</p>
<p>α&lowast;2+α2+β&lowast;2+β2
2
</p>
<p>1
i
(&minus;α&lowast;β&lowast; + αβ)
</p>
<p>(α&lowast;β + αβ&lowast;) i(α&lowast;β &minus; αβ&lowast;) (|α|2 &minus; |β|2)
</p>
<p>⎞
⎟⎠ . (13.130)
</p>
<p>For practical calculations one often prefers to have four real parameters instead of
two complex ones. The so called Euler parameters q0, q1, q2, q3 are defined by
</p>
<p>α = q0 + iq3 β = q2 + iq1. (13.131)</p>
<p/>
</div>
<div class="page"><p/>
<p>258 13 Rotational Motion
</p>
<p>Now the matrix Q
</p>
<p>Q=
(
</p>
<p>q0 + iq3 q2 + iq1
&minus;q2 + iq1 q0 &minus; iq3
</p>
<p>)
= q01 + iq1σx + iq2σy + iq3σz (13.132)
</p>
<p>becomes a so called quaternion which is a linear combination of the four matrices
</p>
<p>U = 1 I = iσz J = iσy K = iσx (13.133)
</p>
<p>which obey the following multiplication rules:
</p>
<p>I 2 = J 2 =K2 =&minus;U
IJ = &minus;JI =K
JK = &minus;KJ = I
KI = &minus;IK = J.
</p>
<p>(13.134)
</p>
<p>In terms of Euler parameters the rotation matrix reads
</p>
<p>A=
</p>
<p>⎛
⎝
q20 + q21 &minus; q22 &minus; q23 2(q1q2 + q0q3) 2(q1q3 &minus; q0q2)
</p>
<p>2(q1q2 &minus; q0q3) q20 &minus; q21 + q22 &minus; q23 2(q2q3 + q0q1)
2(q1q3 + q0q2) 2(q2q3 &minus; q0q1) q20 &minus; q21 &minus; q22 + q23
</p>
<p>⎞
⎠ (13.135)
</p>
<p>and from the equation Ȧ=WA we derive the equation of motion for the quaternion
⎛
⎜⎜⎝
</p>
<p>q̇0
q̇1
q̇2
q̇3
</p>
<p>⎞
⎟⎟⎠=
</p>
<p>1
</p>
<p>2
</p>
<p>⎛
⎜⎜⎝
</p>
<p>0 ω1 ω2 ω3
&minus;ω1 0 &minus;ω3 ω2
&minus;ω2 ω3 0 &minus;ω1
&minus;ω3 &minus;ω2 ω1 0
</p>
<p>⎞
⎟⎟⎠
</p>
<p>⎛
⎜⎜⎝
</p>
<p>q0
q1
q2
q3
</p>
<p>⎞
⎟⎟⎠ (13.136)
</p>
<p>or from Ȧ=AWb the alternative equation
⎛
⎜⎜⎝
</p>
<p>q̇0
q̇1
q̇2
q̇3
</p>
<p>⎞
⎟⎟⎠=
</p>
<p>1
</p>
<p>2
</p>
<p>⎛
⎜⎜⎝
</p>
<p>0 ω1b ω2b ω3b
&minus;ω1b 0 ω3b &minus;ω2b
&minus;ω2b &minus;ω3b 0 ω1b
&minus;ω3b ω2b &minus;ω1b 0
</p>
<p>⎞
⎟⎟⎠
</p>
<p>⎛
⎜⎜⎝
</p>
<p>q0
q1
q2
q3
</p>
<p>⎞
⎟⎟⎠ . (13.137)
</p>
<p>Both of these equations can be written briefly in the form
</p>
<p>q̇= W̃q. (13.138)
</p>
<p>Example (Rotation around the z-axis) Rotation around the z-axis corresponds to the
quaternion with Euler parameters
</p>
<p>q=
</p>
<p>⎛
⎜⎜⎝
</p>
<p>cos ωt2
0
0
</p>
<p>&minus; sin ωt2
</p>
<p>⎞
⎟⎟⎠ (13.139)</p>
<p/>
</div>
<div class="page"><p/>
<p>13.15 Solving the Equations of Motion with Quaternions 259
</p>
<p>as can be seen from the rotation matrix
</p>
<p>A =
</p>
<p>⎛
⎝
(cos ωt2 )
</p>
<p>2 &minus; (sin ωt2 )2 &minus;2 cos
ωt
2 sin
</p>
<p>ωt
2 0
</p>
<p>2 cos ωt2 sin
ωt
2 (cos
</p>
<p>ωt
2 )
</p>
<p>2 &minus; (sin ωt2 )2 0
0 0 (cos ωt2 )
</p>
<p>2 + (sin ωt2 )2
</p>
<p>⎞
⎠
</p>
<p>=
</p>
<p>⎛
⎝
</p>
<p>cosωt &minus; sinωt 0
sinωt cosωt 0
</p>
<p>0 0 1
</p>
<p>⎞
⎠ . (13.140)
</p>
<p>The time derivative of q obeys the equation
</p>
<p>q̇= 1
2
</p>
<p>⎛
⎜⎜⎝
</p>
<p>0 0 0 ω
0 0 &minus;ω 0
0 ω 0 0
&minus;ω 0 0 0
</p>
<p>⎞
⎟⎟⎠
</p>
<p>⎛
⎜⎜⎝
</p>
<p>cos ωt2
0
0
</p>
<p>&minus; sin ωt2
</p>
<p>⎞
⎟⎟⎠=
</p>
<p>⎛
⎜⎜⎝
</p>
<p>&minus;ω2 sinωt
0
0
</p>
<p>&minus;ω2 cosωt
</p>
<p>⎞
⎟⎟⎠ . (13.141)
</p>
<p>After a rotation by 2π the quaternion changes its sign, i.e. q and &minus;q parametrize
the same rotation matrix!
</p>
<p>13.15 Solving the Equations of Motion with Quaternions
</p>
<p>As with the matrix method we can obtain a simple first or second order algorithm
from the Taylor series expansion
</p>
<p>q(t +�t)= q(t)+ W̃ (t)q(t)�t +
( ˙̃W(t)+ W̃ 2(t)
</p>
<p>)
q(t)
</p>
<p>�t2
</p>
<p>2
+ &middot; &middot; &middot; . (13.142)
</p>
<p>Now only one constraint remains, which is the conservation of the norm of the
quaternion. This can be taken into account by rescaling the quaternion whenever its
norm deviates too much from unity.
</p>
<p>It is also possible to use Omelyan&rsquo;s [192] method:
</p>
<p>q(t +�t)= q(t)+ W̃
(
t + �t
</p>
<p>2
</p>
<p>)
1
</p>
<p>2
</p>
<p>(
q(t)+ q(t +�t)
</p>
<p>)
(13.143)
</p>
<p>gives
</p>
<p>q(t +�t)=
(
</p>
<p>1 &minus; �t
2
W̃
</p>
<p>)&minus;1(
1 + �t
</p>
<p>2
W̃
</p>
<p>)
q(t) (13.144)
</p>
<p>where the inverse matrix is
</p>
<p>(
1 &minus; �t
</p>
<p>2
W̃
</p>
<p>)&minus;1
= 1
</p>
<p>1 +ω2 �t216
</p>
<p>(
1 + �t
</p>
<p>2
W̃
</p>
<p>)
(13.145)</p>
<p/>
</div>
<div class="page"><p/>
<p>260 13 Rotational Motion
</p>
<p>Fig. 13.5 Free asymmetric
rotor
</p>
<p>and the matrix product
</p>
<p>(
1 &minus; �t
</p>
<p>2
W̃
</p>
<p>)&minus;1(
1 + �t
</p>
<p>2
W̃
</p>
<p>)
=
</p>
<p>1 &minus;ω2 �t216
1 +ω2 �t216
</p>
<p>+ �t
1 +ω2 �t216
</p>
<p>W̃ . (13.146)
</p>
<p>This method conserves the norm of the quaternion and works quite well.
</p>
<p>13.16 Problems
</p>
<p>Problem 13.1 (Free rotor, Fig. 13.5) In this computer experiment we compare dif-
ferent methods for a free rotor (Sect. 13.8):
</p>
<p>&bull; explicit first order method (13.67)
</p>
<p>A(t +�t)=A(t)+A(t)Wb(t)�t +O
(
�t2
</p>
<p>)
(13.147)
</p>
<p>&bull; explicit second order method (13.69)
</p>
<p>A(t +�t)=A(t)+A(t)Wb(t)�t +
1
</p>
<p>2
</p>
<p>(
A(t)W 2b (t)+A(t)Ẇb(t)
</p>
<p>)
�t2 +O
</p>
<p>(
�t3
</p>
<p>)
</p>
<p>(13.148)
&bull; implicit second order method (13.93)
</p>
<p>A(t +�t)=A(t)
(
</p>
<p>1+ �t
2
W
</p>
<p>(
t + �t
</p>
<p>2
</p>
<p>))(
1&minus; �t
</p>
<p>2
W
</p>
<p>(
t + �t
</p>
<p>2
</p>
<p>))&minus;1
+O
</p>
<p>(
�t3
</p>
<p>)
.
</p>
<p>(13.149)
</p>
<p>The explicit methods can be combined with reorthogonalization according to
(13.79) or with the Gram-Schmidt method. Reorthogonalization threshold and time
step can be varied and the error of kinetic energy and determinant are plotted as a
function of the total simulation time.
</p>
<p>Problem 13.2 (Rotor in a field, Fig. 13.6) In this computer experiment we simulate
a molecule with a permanent dipole moment in a homogeneous electric field E. We
neglect vibrations and describe the molecule as a rigid body consisting of nuclei</p>
<p/>
</div>
<div class="page"><p/>
<p>13.16 Problems 261
</p>
<p>Fig. 13.6 Rotor in an electric
field
</p>
<p>with masses mi and partial charges Qi . The total charge is
&sum;
</p>
<p>i Qi = 0. The dipole
moment is
</p>
<p>p=
&sum;
</p>
<p>i
</p>
<p>Qiri (13.150)
</p>
<p>and external force and torque are
</p>
<p>Fext =
&sum;
</p>
<p>i
</p>
<p>QiE= 0 (13.151)
</p>
<p>Next =
&sum;
</p>
<p>i
</p>
<p>Qiri &times;E= p&times;E. (13.152)
</p>
<p>The angular momentum changes according to
</p>
<p>d
</p>
<p>�t
Lint = p&times;E (13.153)
</p>
<p>where the dipole moment is constant in the body fixed system. We use the implicit
integrator for the rotation matrix (13.93) and the equation
</p>
<p>ω̇b(t)=&minus;I&minus;1b Wb(t)Lint,b(t)+ I
&minus;1
b A
</p>
<p>&minus;1(t)
(
p(t)&times;E
</p>
<p>)
(13.154)
</p>
<p>to solve the equations of motion numerically. Obviously the component of the an-
gular momentum parallel to the field is constant. The potential energy is
</p>
<p>U =&minus;
&sum;
</p>
<p>i
</p>
<p>QiEri =&minus;pE. (13.155)
</p>
<p>Problem 13.3 (Molecular collision) This computer experiment simulates the col-
lision of two rigid methane molecules (Fig. 13.7). The equations of motion are
solved with the implicit quaternion method (13.143) and the velocity Verlet method
(12.11.4). The two molecules interact by a standard 6&ndash;12 Lennard-Jones potential
(14.24) [3]. For comparison the attractive r&minus;6 part can be switched off. The initial</p>
<p/>
</div>
<div class="page"><p/>
<p>262 13 Rotational Motion
</p>
<p>Fig. 13.7 Molecular
collision
</p>
<p>angular momenta as well as the initial velocity v and collision parameter b can be
varied. Total energy and momentum are monitored and the decomposition of the to-
tal energy into translational, rotational and potential energy are plotted as a function
of time.
</p>
<p>Study the exchange of momentum and angular momentum and the transfer of
energy between translational and rotational degrees of freedom.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 14
</p>
<p>Molecular Mechanics
</p>
<p>Classical molecular mechanics simulations have become a very valuable tool for
the investigation of atomic and molecular systems [97, 115, 157, 212, 225], mainly
in the area of materials science and molecular biophysics. Based on the Born-
Oppenheimer separation which assumes that the electrons move much faster than
the nuclei, nuclear motion is described quantum mechanically by the Hamiltonian
</p>
<p>H =
[
T Nuc +U
</p>
<p>(
rNucj
</p>
<p>)]
. (14.1)
</p>
<p>Molecular mechanics uses the corresponding classical energy function
</p>
<p>T Nuc +U
(
rNucj
</p>
<p>)
=
&sum;
</p>
<p>j
</p>
<p>(pNucj )
2
</p>
<p>2mj
+U
</p>
<p>(
rNucj
</p>
<p>)
(14.2)
</p>
<p>which treats the atoms as mass points interacting by classical forces
</p>
<p>Fi =&minus;gradri U
(
rNucj
</p>
<p>)
. (14.3)
</p>
<p>Stable structures, i.e. local minima of the potential energy can be found by the
methods discussed in Chap. 6. Small amplitude motions around an equilibrium ge-
ometry are described by a harmonic normal mode analysis. Molecular dynamics
(MD) simulations solve the classical equations of motion
</p>
<p>mi
d2ri
dt2
</p>
<p>= Fi =&minus;gradri U (14.4)
</p>
<p>numerically.
The potential energy function U(rNucj ) can be calculated with simplified quantum
</p>
<p>methods for not too large systems [50, 152]. Classical MD simulations for larger
molecules use empirical force fields, which approximate the potential energy sur-
face of the electronic ground state. They are able to describe structural and confor-
mational changes but not chemical reactions which usually involve more than one
electronic state. Among the most popular classical force fields are AMBER [63],
CHARMM [163] and GROMOS [58, 262].
</p>
<p>In this chapter we discuss the most important interaction terms, which are con-
veniently expressed in internal coordinates, i.e. bond lengths, bond angles and dihe-
dral angles. We derive expressions for the gradients of the force field with respect
</p>
<p>P.O.J. Scherer, Computational Physics, Graduate Texts in Physics,
DOI 10.1007/978-3-319-00401-3_14,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>263</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-00401-3_14">http://dx.doi.org/10.1007/978-3-319-00401-3_14</a></div>
</div>
<div class="page"><p/>
<p>264 14 Molecular Mechanics
</p>
<p>Fig. 14.1 (Molecular
coordinates) Cartesian
coordinates (left) are used to
solve the equations of motion
whereas the potential energy
is more conveniently
formulated in internal
coordinates (right)
</p>
<p>Fig. 14.2 (Conformation of a
protein) The relative
orientation of two successive
protein residues can be
described by three angles
(Ψ,Φ,ω)
</p>
<p>to Cartesian coordinates. In a computer experiment we simulate a glycine dipeptide
and demonstrate the principles of energy minimization, normal mode analysis and
dynamics simulation.
</p>
<p>14.1 Atomic Coordinates
</p>
<p>The most natural coordinates for the simulation of molecules are the Cartesian co-
ordinates (Fig. 14.1) of the atoms,
</p>
<p>ri = (xi, yi, zi) (14.5)
which can be collected into a 3N -dimensional vector
</p>
<p>(ξ1, ξ2 &middot; &middot; &middot; ξ3N )= (x1, y1, z1, x2 &middot; &middot; &middot;xN , yN , zN ). (14.6)
The second derivatives of the Cartesian coordinates appear directly in the equations
of motion (14.4)
</p>
<p>mr ξ̈r = Fr r = 1 &middot; &middot; &middot;3N. (14.7)
Cartesian coordinates have no direct relation to the structural properties of mole-
cules. For instance a protein is a long chain of atoms (the so called backbone) with
additional side groups (Fig. 14.2).
</p>
<p>The protein structure can be described more intuitively with the help of atomic
distances and angles. Internal coordinates are (Fig. 14.3) distances between two
bonded atoms (bond lengths)
</p>
<p>bij = |rij | = |ri &minus; rj |, (14.8)</p>
<p/>
</div>
<div class="page"><p/>
<p>14.1 Atomic Coordinates 265
</p>
<p>Fig. 14.3 (Internal
coordinates) The structure of
a molecule can be described
by bond lengths, bond angles
and dihedral angles
</p>
<p>Fig. 14.4 Dihedral angle
</p>
<p>angles between two bonds (bond angles)
</p>
<p>φijk = arccos
(
</p>
<p>rijrkj
</p>
<p>|rij ||rkj |
</p>
<p>)
(14.9)
</p>
<p>and dihedral angles which describe the planarity and torsions of the molecule. A di-
hedral angle (Fig. 14.4) is the angle between two planes which are defined by three
bonds
</p>
<p>θijkl = sign(θijkl) arccos(nijknjkl) (14.10)
</p>
<p>nijk =
rij &times; rkj
|rij &times; rkj |
</p>
<p>njkl =
rkj &times; rkl
|rkj &times; rkl |
</p>
<p>(14.11)
</p>
<p>where the conventional sign of the dihedral angle [136] is determined by
</p>
<p>sign θijkl = sign
(
rkj (nijk &times; njkl)
</p>
<p>)
. (14.12)
</p>
<p>Internal coordinates are very convenient for the formulation of a force field. On
the other hand, the kinetic energy (14.2) becomes complicated if expressed in inter-
nal coordinates. Therefore both kinds of coordinates are used in molecular dynamics</p>
<p/>
</div>
<div class="page"><p/>
<p>266 14 Molecular Mechanics
</p>
<p>Fig. 14.5 (Glycine dipeptide
model) The glycine dipeptide
is the simplest model for a
peptide. It is simulated in
Problem 14.1. Optimized
internal coordinates are
shown in Table 14.1
</p>
<p>calculations. The internal coordinates are usually arranged in Z-matrix form. Each
line corresponds to one atom i and shows its position relative to three atoms j , k,
l in terms of the bond length bij , the bond angle φijk and the dihedral angle θijkl
(Fig. 14.5 and Table 14.1).
</p>
<p>14.2 Force Fields
</p>
<p>Classical force fields are usually constructed as an additive combination of many
interaction terms. Generally these can be divided into intramolecular contributions
</p>
<p>Table 14.1 (Z-matrix) The optimized values of the internal coordinates from Problem 14.1 are
shown in Z-matrix form. Except for the first three atoms the position of atom i is given by its
distance bij to atom j , the bond angle φijk and the dihedral angle θijkl
</p>
<p>Number i Label j k l Bond length (&Aring;)
bij
</p>
<p>Bond angle
φijk
</p>
<p>Dihedral
θijkl
</p>
<p>1 N1
</p>
<p>2 C2 1 1.45
</p>
<p>3 C3 2 1 1.53 108.6
</p>
<p>4 N4 3 2 1 1.35 115.0 160.7
</p>
<p>5 C5 4 3 2 1.44 122.3 &minus;152.3
6 C6 5 4 3 1.51 108.7 &minus;153.1
7 O7 3 2 1 1.23 121.4 &minus;26.3
8 O8 6 5 4 1.21 124.4 123.7
</p>
<p>9 O9 6 5 4 1.34 111.5 &minus;56.5
10 H10 1 2 3 1.02 108.7 &minus;67.6
11 H11 1 2 3 1.02 108.7 49.3
</p>
<p>12 H12 2 3 4 1.10 109.4 &minus;76.8
13 H13 2 3 4 1.10 109.4 38.3
</p>
<p>14 H14 4 3 2 1.02 123.1 27.5
</p>
<p>15 H15 5 4 3 1.10 111.2 &minus;32.5
16 H16 5 4 3 1.10 111.1 86.3
</p>
<p>17 H17 9 6 5 0.97 106.9 &minus;147.4</p>
<p/>
</div>
<div class="page"><p/>
<p>14.2 Force Fields 267
</p>
<p>Fig. 14.6 Intramolecular forces
</p>
<p>Ubonded which determine the configuration and motion of a single molecule and
intermolecular contributions Unon-bonded describing interactions between different
atoms or molecules
</p>
<p>U =Ubonded +Unon-bonded. (14.13)
</p>
<p>14.2.1 Intramolecular Forces
</p>
<p>The most important intramolecular forces depend on the deviation of bond lengths,
bond angles and dihedral angles from their equilibrium values. For simplicity a sum
of independent terms is used as for the CHARMM force field [39, 40, 163]
</p>
<p>Uintra =
&sum;
</p>
<p>Ubondij +
&sum;
</p>
<p>U
angle
</p>
<p>ijk +
&sum;
</p>
<p>UUBijk +
&sum;
</p>
<p>Udihedralijkl +
&sum;
</p>
<p>U
improper
</p>
<p>ijkl .
</p>
<p>(14.14)
</p>
<p>The forces are derived from potential functions which are in the simplest case ap-
proximated by harmonic oscillator parabolas (Fig. 14.6), like the bond stretching
energy
</p>
<p>Ubondij =
1
</p>
<p>2
kij
</p>
<p>(
bij &minus; b0ij
</p>
<p>)2 (14.15)</p>
<p/>
</div>
<div class="page"><p/>
<p>268 14 Molecular Mechanics
</p>
<p>Table 14.2 (Atom types of
the glycine dipeptide) Atom
types for glycine
oligopeptides according to
Bautista and Seminario [15].
The atoms are classified by
element and bonding
environment. Atoms of the
same atom type are
considered equivalent
</p>
<p>Atom type Atoms
</p>
<p>C C3,
</p>
<p>C1 C2, C5
</p>
<p>C2 C6
</p>
<p>N N4
</p>
<p>N2 N1
</p>
<p>O O7
</p>
<p>O1 O9
</p>
<p>O2 O8
</p>
<p>H H14
</p>
<p>H1 H12, H13, H15, H16
</p>
<p>H2 H17
</p>
<p>H3 H10, H11
</p>
<p>angle bending terms
</p>
<p>U
angle
</p>
<p>ijk =
1
</p>
<p>2
kijk
</p>
<p>(
φijk &minus; φ0ijk
</p>
<p>)2 (14.16)
</p>
<p>together with the Urey-Bradly correction
</p>
<p>UUBijk =
1
</p>
<p>2
kijk
</p>
<p>(
bik &minus; b0ik
</p>
<p>)2 (14.17)
</p>
<p>and &ldquo;improper dihedral&rdquo; terms which are used to keep planarity
</p>
<p>U
improper
</p>
<p>ijkl =
1
</p>
<p>2
kijkl
</p>
<p>(
θijkl &minus; θ0ijkl
</p>
<p>)2
. (14.18)
</p>
<p>Torsional energy contributions are often described by a cosine function1
</p>
<p>Udihedralijkl = kijkl
(
1 &minus; cos
</p>
<p>(
mθijkl &minus; θ0ijkl
</p>
<p>))
(14.19)
</p>
<p>where m = 1,2,3,4,6 describes the symmetry. For instance m = 3 for the three
equivalent hydrogen atoms of a methyl group. In most cases the phase shift θ0ijkl = 0
or θ0ijkl = π . Then the dihedral potential can be expanded as a polynomial of cos θ ,
for instance
</p>
<p>m= 1: Udihedralijkl = k(1 &plusmn; cos θijkl) (14.20)
m= 2: Udihedralijkl = k&plusmn; k
</p>
<p>(
1 &minus; 2(cos θijkl)2
</p>
<p>)
(14.21)
</p>
<p>m= 3: Udihedralijkl = k
(
1 &plusmn; 3 cos θijkl ∓ 4(cos θijkl)3
</p>
<p>)
. (14.22)
</p>
<p>For more general θ0ijkl the torsional potential can be written as a polynomial of
cos θijkl and sin θijkl .
</p>
<p>1Some force-fields like Desmond [33] or UFF [213] use a more general sum
</p>
<p>k
&sum;M
</p>
<p>m=0 cm cos(mθ &minus; θ0).</p>
<p/>
</div>
<div class="page"><p/>
<p>14.2 Force Fields 269
</p>
<p>Table 14.3 (Bond stretching parameters) Equilibrium bond lengths (&Aring;) and force constants
(kcal mol&minus;1 &Aring;&minus;2) for the glycine dipeptide from [15]
</p>
<p>Bond type b0 k Bonds
</p>
<p>rC,N 1.346 1296.3 C3-N4
</p>
<p>rC1,N 1.438 935.5 N4-C5
</p>
<p>rC1,N2 1.452 887.7 N1-C2
</p>
<p>rC2,C1 1.510 818.9 C5-C6
</p>
<p>rC,C1 1.528 767.9 C2-C3
</p>
<p>rC2,O2 1.211 2154.5 C6-O8
</p>
<p>rC,O 1.229 1945.7 C3-O7
</p>
<p>rC2,O1 1.339 1162.1 C6-O9
</p>
<p>rN,H 1.016 1132.4 N4-H14
</p>
<p>rN2,H3 1.020 1104.5 N1-H10, N1-H11
</p>
<p>rC1,H1 1.098 900.0 C2-H12, C2-H13, C5-H15, C5-H16
</p>
<p>rO1,H2 0.974 1214.6 O9-H17
</p>
<p>The atoms are classified by element and bonding environment. Atoms of the
same atom type are considered equivalent and the parameters transferable (for an
example see Tables 14.2, 14.3, 14.4).
</p>
<p>14.2.2 Intermolecular Interactions
</p>
<p>Interactions between non-bonded atoms
</p>
<p>Unon-bonded =UCoul +UvdW (14.23)
include the Coulomb interaction and the weak attractive van der Waals forces which
are usually combined with a repulsive force at short distances to account for the
Pauli principle. Very often a sum of pairwise Lennard-Jones potentials is used
(Fig. 14.7) [3]
</p>
<p>Uvdw =
&sum;
</p>
<p>A�=B
</p>
<p>&sum;
</p>
<p>i&isin;A,j&isin;B
Uvdwi,j =
</p>
<p>&sum;
</p>
<p>A�=B
</p>
<p>&sum;
</p>
<p>ij
</p>
<p>4εij
</p>
<p>(
σ 12ij
</p>
<p>r12ij
</p>
<p>&minus;
σ 6ij
</p>
<p>r6ij
</p>
<p>)
. (14.24)
</p>
<p>The charge distribution of a molecular system can be described by a set of multi-
poles at the position of the nuclei, the bond centers and further positions (lone pairs
for example). Such distributed multipoles can be calculated quantum chemically for
not too large molecules. In the simplest models only partial charges are taken into
account giving the Coulomb energy as a sum of atom-atom interactions
</p>
<p>UCoul =
&sum;
</p>
<p>A�=B
</p>
<p>&sum;
</p>
<p>i&isin;A,j&isin;B
</p>
<p>qiqj
</p>
<p>4πε0rij
. (14.25)</p>
<p/>
</div>
<div class="page"><p/>
<p>270 14 Molecular Mechanics
</p>
<p>Table 14.4 (Bond angle parameters) Equilibrium bond angles (deg) and force constants
(kcal mol&minus;1 rad&minus;2) for the glycine dipeptide from [15]
</p>
<p>Angle type φ0 k Angles
</p>
<p>φN,C,C1 115.0 160.0 C2-C3-N4
</p>
<p>φC1,N,C 122.3 160.1 C3-N4-C5
</p>
<p>φC1,C2,O1 111.5 156.0 C5-C6-O9
</p>
<p>φC1,C2,O2 124.4 123.8 C5-C6-O8
</p>
<p>φC1,C,O 121.4 127.5 C2-C3-O7
</p>
<p>φO2,C2,O1 124.1 146.5 O8-C6-O9
</p>
<p>φN,C,O 123.2 132.7 N4-C3-O7
</p>
<p>φC,C1,H1 110.1 74.6 H12-C2-C3, H13-C2-C3
</p>
<p>φC2,C1,H1 109.4 69.6 H16-C5-C6, H15-C5-C6
</p>
<p>φC,N,H 123.1 72.0 C3-N4-H14
</p>
<p>φC1,N,H 114.6 68.3 C5-N4-H14
</p>
<p>φC1,N2,H3 108.7 71.7 H10-N1-C2, H11-N1-C2
</p>
<p>φH1,C1,H1 106.6 48.3 H13-C2-H12, H15-C5-H16
</p>
<p>φH3,N2,H3 107.7 45.2 H10-N1-H11
</p>
<p>φC,C1,N2 109.0 139.8 N1-C2-C3
</p>
<p>φC2,C1,N 108.6 129.0 N4-C5-C6
</p>
<p>φC2,O1,H2 106.9 72.0 H17-O9-C6
</p>
<p>φN,C1,H1 111.1 73.3 H15-C5-N4, H16-C5-N4
</p>
<p>φN2,C1,H1 112.6 80.1 H13-C2-N1, H12-C2-N1
</p>
<p>More sophisticated force fields include higher charge multipoles and polarization
effects.
</p>
<p>14.3 Gradients
</p>
<p>The equations of motion are usually solved in Cartesian coordinates and the gradi-
ents of the potential are needed in Cartesian coordinates. Since the potential depends
only on relative position vectors rij , the gradient with respect to a certain atom po-
sition rk can be calculated from
</p>
<p>gradrk =
&sum;
</p>
<p>i&lt;j
</p>
<p>(δik &minus; δjk)gradrij . (14.26)
</p>
<p>Therefore it is sufficient to calculate gradients with respect to the difference vectors.
Numerically efficient methods to calculate first and second derivatives of many force
field terms are given in [156, 259, 260]. The simplest potential terms depend only
on the distance of two atoms. For instance bond stretching terms, Lennard-Jones
and Coulomb energies have the form
</p>
<p>Uij =U(rij )=U
(
|rij |
</p>
<p>)
(14.27)</p>
<p/>
</div>
<div class="page"><p/>
<p>14.3 Gradients 271
</p>
<p>Fig. 14.7 (Lennard-Jones
potential) The 6&ndash;12 potential
(14.24) has its minimum at
rmin = 6
</p>
<p>&radic;
2σ &asymp; 1.12σ with
</p>
<p>Umin =&minus;ǫ
</p>
<p>where the gradient is
</p>
<p>gradrij Uij =
�U
</p>
<p>�r
</p>
<p>rij
</p>
<p>|rij |
. (14.28)
</p>
<p>The most important gradients of this kind are
</p>
<p>gradrij U
bond
i,j = k
</p>
<p>(
rij &minus; b0
</p>
<p>)rij
rij
</p>
<p>= k
(
</p>
<p>1 &minus; b
0
</p>
<p>rij
</p>
<p>)
rij (14.29)
</p>
<p>gradrij U
vdw
i,j = 24εij
</p>
<p>(
&minus;2
</p>
<p>σ 12ij
</p>
<p>r14ij
</p>
<p>+
σ 6ij
</p>
<p>r8ij
</p>
<p>)
rij (14.30)
</p>
<p>gradrij U
Coul
ij =&minus;
</p>
<p>qiqj
</p>
<p>4πε0r3ij
rij . (14.31)
</p>
<p>The gradient of the harmonic bond angle potential is
</p>
<p>gradrU
angle
</p>
<p>i,j,k = k
(
φijk &minus; φ0
</p>
<p>)
gradr φijk (14.32)
</p>
<p>where the gradient of the angle can be calculated from the gradient of its cosine
</p>
<p>gradrij φijk =&minus;
1
</p>
<p>sinφijk
gradrij cosφijk =&minus;
</p>
<p>1
</p>
<p>sinφijk
</p>
<p>(
rkj
</p>
<p>|rij ||rkj |
&minus; rij rkj|rij |3|rkj |
</p>
<p>rij
</p>
<p>)
</p>
<p>=&minus; 1
sinφijk
</p>
<p>(
rkj
</p>
<p>|rij ||rkj |
&minus; cosφijk|rij |2
</p>
<p>rij
</p>
<p>)
(14.33)
</p>
<p>grad rkjφijk =&minus;
1
</p>
<p>sinφijk
</p>
<p>(
rij
</p>
<p>|rij ||rkj |
&minus; cosφijk|rkj |2
</p>
<p>rkj
</p>
<p>)
. (14.34)
</p>
<p>In principle, the sine function in the denominator could lead to numerical prob-
lems which can be avoided by treating angles close to 0 or π separately or using a
function of cosφijk like the trigonometric potential
</p>
<p>U
angle
</p>
<p>ijk =
1
</p>
<p>2
kijk
</p>
<p>(
cosφijk &minus; cosφ0ijk
</p>
<p>)2 (14.35)</p>
<p/>
</div>
<div class="page"><p/>
<p>272 14 Molecular Mechanics
</p>
<p>instead [5, 213, 224]. Alternatively, the gradient of φ can be brought to a form which
is free of singularities by expressing the sine in the denominator by a cosine [59]
</p>
<p>gradrij φijk =&minus;
1&radic;
</p>
<p>r2ij r
2
kj (1 &minus; cos2 φijk)
</p>
<p>(
rkj &minus;
</p>
<p>rijrkj
</p>
<p>r2ij
rij
</p>
<p>)
</p>
<p>=&minus;
r2ij rkj &minus; (rijrkj )rij
</p>
<p>rij
</p>
<p>&radic;
(r2ij rkj &minus; (rij rkj )rij )2
</p>
<p>=&minus; 1
rij
</p>
<p>rij &times; (rkj &times; rij )
|rij &times; (rkj &times; rij )|
</p>
<p>(14.36)
</p>
<p>and similarly
</p>
<p>gradrkj φijk =&minus;
1
</p>
<p>rkj
</p>
<p>rkj &times; (rij &times; rkj )
|rkj &times; (rij &times; rkj )|
</p>
<p>. (14.37)
</p>
<p>Gradients of the dihedral potential are most easily calculated for θ0ijkl = 0 or π .
In that case, the dihedral potential is a polynomial of cos θijkl only (14.20)&ndash;(14.22)
and
</p>
<p>gradrU
dihedral
ijkl =
</p>
<p>dUdihedralijkl
d cos θijkl
</p>
<p>gradr cos θijkl (14.38)
</p>
<p>whereas in the general case 0 &lt; θijkl &lt; π application of the chain rule gives
</p>
<p>gradUdihedralijkl =mkijkl sin
(
m
(
θijkl &minus; θ0
</p>
<p>))
grad θijkl . (14.39)
</p>
<p>If this is evaluated with the help of
</p>
<p>grad θijkl =&minus;
1
</p>
<p>sin θijkl
grad cos θijkl (14.40)
</p>
<p>singularities appear for θ = 0 and π . The same is the case for the gradients of the
harmonic improper potential
</p>
<p>gradU improperijkl = k
(
θijkl &minus; θ0ijkl
</p>
<p>)
grad θijkl . (14.41)
</p>
<p>Again, one possibility which has been often used, is to treat angles close to 0 or π
separately [39]. However, the gradient of the angle θijkl can be calculated directly,
which is much more efficient [19].
</p>
<p>The gradient of the cosine follows from application of the product rule
</p>
<p>grad cos θ = grad
(
</p>
<p>rij &times; rkj
|rij &times; rkj |
</p>
<p>rkj &times; rkl
|rkj &times; rkl |
</p>
<p>)
. (14.42)
</p>
<p>First we derive the differentiation rule
</p>
<p>grada
[
(a&times; b)(c&times; d)
</p>
<p>]
= grada
</p>
<p>[
(ac)(bd)&minus; (ad)(bc)
</p>
<p>]
</p>
<p>= c(bd)&minus; d(bc)= b&times; (c&times; d) (14.43)
which helps us to find</p>
<p/>
</div>
<div class="page"><p/>
<p>14.3 Gradients 273
</p>
<p>gradrij (rij &times; rkj )(rkj &times; rkl)= rkj &times; (rkj &times; rkl) (14.44)
gradrkl (rij &times; rkj )(rkj &times; rkl)= rkj &times; (rkj &times; rij ) (14.45)
gradrkj (rij &times; rkj )(rkj &times; rkl)= rkl &times; (rij &times; rkj )+ rij &times; (rkl &times; rkj ) (14.46)
</p>
<p>and
</p>
<p>gradrij
1
</p>
<p>|rij &times; rkj |
= &minus;rkj &times; (rij &times; rkj )|rij &times; rkj |3
</p>
<p>(14.47)
</p>
<p>gradrkj
1
</p>
<p>|rij &times; rkj |
= &minus;rij &times; (rkj &times; rij )|rij &times; rkj |3
</p>
<p>(14.48)
</p>
<p>gradrkj
1
</p>
<p>|rkj &times; rkl |
= &minus;rkl &times; (rkj &times; rkl)|rkj &times; rkl |3
</p>
<p>(14.49)
</p>
<p>gradrkl
1
</p>
<p>|rkj &times; rkl |
= &minus;rkj &times; (rkl &times; rkj )|rkj &times; rkl |3
</p>
<p>. (14.50)
</p>
<p>Finally we collect terms to obtain the gradients of the cosine [59]
</p>
<p>gradrij cos θijkl =
rkj &times; (rkj &times; rkl)
</p>
<p>|rij &times; rkj ||rkj &times; rkl |
&minus; rkj &times; (rij &times; rkj )|rij &times; rkj |2
</p>
<p>cos θijkl
</p>
<p>= rkj|rij &times; rkj |
&times; (njkl &minus; nijk cos θ)
</p>
<p>= rkj|rij &times; rkj |
&times;
(
njkl &minus; nijk(njklnijk)
</p>
<p>)
</p>
<p>= rkj|rij &times; rkj |
&times;
(
nijk &times; (njkl &times; nijk)
</p>
<p>)
</p>
<p>= rkj|rij &times; rkj |
&times;
(
nijk &times;
</p>
<p>1
</p>
<p>rkj
(&minus;rkj ) sin θ
</p>
<p>)
</p>
<p>= sin θ
rkj
</p>
<p>rkj
</p>
<p>|rij &times; rkj |
&times;
(
nijk &times; (&minus;rkj )
</p>
<p>)
</p>
<p>= sin θ
rkj
</p>
<p>1
</p>
<p>|rij &times; rkj |
(
&minus;nijkr2kj
</p>
<p>)
</p>
<p>=&minus;rkj sin θ
nijk
</p>
<p>|rij &times; rkj |
(14.51)
</p>
<p>gradrkl cos θijkl =
rkj &times; (rkj &times; rij )
</p>
<p>|rij &times; rkj ||rkj &times; rkl |
&minus; rkj &times; (rkl &times; rkj )|rkj &times; rkl |2
</p>
<p>cos θijkl
</p>
<p>= rkj|rkj &times; rkl |
&times; (&minus;nijk + njkl cos θ)
</p>
<p>= rkj|rkj &times; rkl |
&times;
(
&minus;nijk + njkl(nijknjkl)
</p>
<p>)
</p>
<p>=&minus; rkj|rkj &times; rkl |
&times;
(
njkl &times; (nijk &times; njkl)
</p>
<p>)</p>
<p/>
</div>
<div class="page"><p/>
<p>274 14 Molecular Mechanics
</p>
<p>=&minus; rkj|rkj &times; rkl |
&times;
(
njkl &times;
</p>
<p>(
rkj
</p>
<p>rkj
sin θ
</p>
<p>))
</p>
<p>=&minus; sin θ
rkj |rkj &times; rkl |
</p>
<p>rkj &times; (njkl &times; rkj )
</p>
<p>=&minus; rkj sin θ|rkj &times; rkl |
njkl (14.52)
</p>
<p>gradrkj cos θijkl
</p>
<p>= rkl &times; (rij &times; rkj )+ rij &times; (rkl &times; rkj )|rij &times; rkj ||rkj &times; rkl |
</p>
<p>&minus; rij &times; (rkj &times; rij )|rij &times; rkj |2
cos θ &minus; rkl &times; (rkj &times; rkl)|rkj &times; rkl |2
</p>
<p>cos θ
</p>
<p>= rij|rij &times; rkj |
&times; (&minus;njkl + nijk cos θ)+
</p>
<p>rkl
</p>
<p>|rkj &times; rkl |
&times; (nijk &minus; njkl cos θ)
</p>
<p>=&minus; rij|rij &times; rkj |
&times;
(
nijk &times; (njkl &times; nijk)
</p>
<p>)
+ rkl|rkj &times; rkl |
</p>
<p>(
njkl &times; (nijk &times; njkl)
</p>
<p>)
</p>
<p>= rij|rij &times; rkj |
&times;
(
nijk &times;
</p>
<p>(
rkj
</p>
<p>rkj
sin θ
</p>
<p>))
+ rkl|rkj &times; rkl |
</p>
<p>(
njkl &times;
</p>
<p>(
rkj
</p>
<p>rkj
sin θ
</p>
<p>))
</p>
<p>= sin θ
rkj
</p>
<p>1
</p>
<p>|rij &times; rkj |
rij &times; (nijk &times; rkj )+
</p>
<p>sin θ
</p>
<p>rkj
</p>
<p>1
</p>
<p>|rkj &times; rkl |
rkl &times; (njkl &times; rkj )
</p>
<p>= sin θ
rkj
</p>
<p>nijk(rijrkj )
</p>
<p>|rij &times; rkj |
+ sin θ
</p>
<p>rkj
</p>
<p>njkl(rklrkj )
</p>
<p>|rkj &times; rkl |
</p>
<p>= &minus;rijrkj
r2kj
</p>
<p>gradij cos θ &minus;
rklrkj
</p>
<p>r2kj
gradkl cos θ. (14.53)
</p>
<p>14.4 Normal Mode Analysis
</p>
<p>The nuclear motion around an equilibrium configuration can be approximately de-
scribed as the combination of independent harmonic normal modes. Equilibrium
configurations can be found with the methods discussed in Sect. 6.2. The conver-
gence is usually rather slow (Fig. 14.8) except for the full Newton-Raphson method,
which needs the calculation and inversion of the Hessian matrix.
</p>
<p>14.4.1 Harmonic Approximation
</p>
<p>At an equilibrium configuration
</p>
<p>ξi = ξ eqi (14.54)</p>
<p/>
</div>
<div class="page"><p/>
<p>14.4 Normal Mode Analysis 275
</p>
<p>Fig. 14.8 (Convergence of
energy and gradient) The
energy of the glycine
dipeptide is minimized with
the methods of steepest
descent and conjugate
gradients
</p>
<p>the gradient of the potential energy vanishes. For small deviations from the equilib-
rium
</p>
<p>ζi = ξi &minus; ξ eqi (14.55)
approximation by a truncated Taylor series gives
</p>
<p>U(ζ1 &middot; &middot; &middot; ζ3N )=U0 +
1
</p>
<p>2
</p>
<p>&sum;
</p>
<p>i,j
</p>
<p>&part;2U
</p>
<p>&part;ζi&part;ζj
ζiζj + &middot; &middot; &middot; &asymp;U0 +
</p>
<p>1
</p>
<p>2
</p>
<p>&sum;
</p>
<p>i,j
</p>
<p>Hi,j ζiζj (14.56)
</p>
<p>and the equations of motion are approximately
</p>
<p>mi ζ̈i =&minus;
&part;
</p>
<p>&part;ζi
U =&minus;
</p>
<p>&sum;
</p>
<p>j
</p>
<p>Hi,j ζj . (14.57)
</p>
<p>Assuming periodic oscillations
</p>
<p>ζi = ζ 0i eiωt (14.58)
we have
</p>
<p>miω
2ζ 0i =
</p>
<p>&sum;
</p>
<p>j
</p>
<p>Hij ζ
0
j . (14.59)
</p>
<p>If mass weighted coordinates are used, defined as
</p>
<p>τi =
&radic;
miζi (14.60)
</p>
<p>this becomes an ordinary eigenvalue problem
</p>
<p>ω2τ 0i =
&sum;
</p>
<p>j
</p>
<p>Hij&radic;
mimj
</p>
<p>τ 0j . (14.61)
</p>
<p>The eigenvectors ur of the symmetric matrix
</p>
<p>H̃ij =
Hij&radic;
mimj
</p>
<p>(14.62)</p>
<p/>
</div>
<div class="page"><p/>
<p>276 14 Molecular Mechanics
</p>
<p>Fig. 14.9 (Normal mode
distribution for the dipeptide
model) The cumulative
distribution (Sect. 8.1.2) of
normal mode frequencies is
shown for the glycine
dipeptide. Translations and
rotations of the molecule
correspond to the lowest 6
frequencies which are close to
zero. The highest frequencies
between 3100 cm&minus;1 and
3600 cm&minus;1 correspond to the
stretching modes of the 8
hydrogen atoms
</p>
<p>are the solutions of
&sum;
</p>
<p>j
</p>
<p>H̃ijujr = λruir (14.63)
</p>
<p>and satisfy (14.61)
</p>
<p>ω2uir =
&sum;
</p>
<p>H̃ijujr = λruir (14.64)
</p>
<p>with normal mode frequencies
</p>
<p>ωr =
&radic;
λr . (14.65)
</p>
<p>Finally, the Cartesian coordinates are linear combinations of all normal modes
</p>
<p>ζi =
&sum;
</p>
<p>r
</p>
<p>Cr
uir&radic;
mi
</p>
<p>eiωr t . (14.66)
</p>
<p>In a true local energy minimum the Hessian matrix Hij is positive definite and all
frequencies are real valued. The six lowest frequencies are close to zero and corre-
spond to translations and rotations of the whole system (Fig. 14.9).
</p>
<p>14.5 Problems
</p>
<p>Problem 14.1 (Simulation of a glycine dipeptide) In this computer experiment
a glycine dipeptide (Fig. 14.5) is simulated. Parameters for bond stretching (Ta-
ble 14.3) and bond angle (Table 14.4) terms have been derived from quantum cal-
culations by Bautista and Seminario [15].
</p>
<p>&bull; Torsional potential terms (Table 14.5) can be added to make the structure more
rigid. This is especially important for the O9-H17, N4-H14 and N1-H10 bonds,
which rotate almost freely without torsional potentials.</p>
<p/>
</div>
<div class="page"><p/>
<p>14.5 Problems 277
</p>
<p>Table 14.5 (Torsional potential terms) Torsional potential terms Vijkl = kijkl(1 &minus;
cos(θijkl &minus; θ0ijkl)), which can be added to the forcefield. Minimum angles are from the op-
timized structure without torsional terms (14.1). The barrier height of 2kijkl = 2 kcal/mol is only
a guessed value
</p>
<p>i j k l θ0ijkl kijkl Backbone
</p>
<p>10 1 2 3 &minus;67.6 1.0
14 4 3 2 27.5 1.0
</p>
<p>17 9 6 5 &minus;147.4 1.0
4 3 2 1 160.7 1.0 Ψ
</p>
<p>5 4 3 2 &minus;152.3 1.0 ω
6 5 4 3 &minus;153.1 1.0 Φ
8 6 5 4 123.7 1.0
</p>
<p>9 6 5 4 &minus;56.5 1.0
15 5 4 3 &minus;32.5 1.0
16 5 4 3 86.3 1.0
</p>
<p>7 3 2 1 &minus;26.3 1.0
</p>
<p>&bull; The energy can be minimized with the methods of steepest descent or conjugate
gradients.
</p>
<p>&bull; A normal mode analysis can be performed (the Hessian matrix is calculated by
numerical differentiation). The r-th normal mode can be visualized by modulat-
ing the coordinates periodically according to
</p>
<p>ξi = ξ eqi +Cr
uir&radic;
mi
</p>
<p>cosωr t. (14.67)
</p>
<p>&bull; The motion of the atoms can be simulated with the Verlet method. You can stretch
the O9-H17 or N4-H14 bond and observe, how the excitation spreads over the
molecule.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 15
</p>
<p>Thermodynamic Systems
</p>
<p>An important application for computer simulations is the calculation of thermody-
namic averages in an equilibrium system. We discuss two different examples:
</p>
<p>In the first case the classical equations of motion are solved for a system of par-
ticles interacting pairwise by Lennard-Jones forces (Lennard-Jones fluid). The ther-
modynamic average is taken along the trajectory, i.e. over the calculated coordinates
at different times ri(tn). We evaluate the pair distance distribution function
</p>
<p>g(R)= 1
N2 &minus;N
</p>
<p>&lang;&sum;
</p>
<p>i �=j
δ(rij &minus;R)
</p>
<p>&rang;
, (15.1)
</p>
<p>the velocity auto-correlation function
</p>
<p>C(t)=
&lang;
v(t0)v(t)
</p>
<p>&rang;
(15.2)
</p>
<p>and the mean square displacement
</p>
<p>�x2 =
&lang;(
x(t)&minus; x(t0)
</p>
<p>)2&rang;
. (15.3)
</p>
<p>In the second case the Metropolis method is applied to a one- or two-dimensional
system of interacting spins (Ising model). The thermodynamic average is taken over
a set of random configurations q(n). We study the average magnetization
</p>
<p>〈M〉 = μ〈S〉 (15.4)
in a magnetic field and the phase transition to the ferromagnetic state.
</p>
<p>15.1 Simulation of a Lennard-Jones Fluid
</p>
<p>The Lennard-Jones fluid is a simple model of a realistic atomic fluid. It has been
studied by computer simulations since Verlet&rsquo;s early work [118, 265] and serves as
a test case for the theoretical description of liquids [142, 182] and the liquid-gas
[270] and liquid-solid phase transitions [146, 176].
</p>
<p>P.O.J. Scherer, Computational Physics, Graduate Texts in Physics,
DOI 10.1007/978-3-319-00401-3_15,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>279</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-00401-3_15">http://dx.doi.org/10.1007/978-3-319-00401-3_15</a></div>
</div>
<div class="page"><p/>
<p>280 15 Thermodynamic Systems
</p>
<p>In the following we describe a simple computer model of 125 interacting parti-
cles1 without internal degrees of freedom (see problems section). The force on atom
i is given by the gradient of the pairwise Lennard-Jones potential (14.24)
</p>
<p>Fi =
&sum;
</p>
<p>j �=i
Fij =&minus;4ε
</p>
<p>&sum;
</p>
<p>j �=i
▽i
</p>
<p>(
σ 12
</p>
<p>r12ij
</p>
<p>&minus; σ
6
</p>
<p>r6ij
</p>
<p>)
</p>
<p>= 4ε
&sum;
</p>
<p>j �=i
</p>
<p>(
12σ 12
</p>
<p>r14ij
</p>
<p>&minus; 6σ
6
</p>
<p>r8ij
</p>
<p>)
(ri &minus; rj ). (15.5)
</p>
<p>We use argon parameters m = 6.69 &times; 10&minus;26 kg, ε = 1.654 &times; 10&minus;21 J, σ =
3.405 &times; 10&minus;10 m [3]. After introduction of reduced units for length r&lowast; = 1
</p>
<p>σ
r, en-
</p>
<p>ergy E&lowast; = 1
ε
E and time t&lowast; =
</p>
<p>&radic;
ε/mσ 2 t , the potential energy
</p>
<p>U&lowast; =
&sum;
</p>
<p>ij
</p>
<p>4
</p>
<p>(
1
</p>
<p>r&lowast;12ij
&minus; 1
</p>
<p>r&lowast;6ij
</p>
<p>)
(15.6)
</p>
<p>and the equation of motion
</p>
<p>d2
</p>
<p>dt&lowast;2
r&lowast;i = 4
</p>
<p>&sum;
</p>
<p>j �=i
</p>
<p>(
12
</p>
<p>r&lowast;14ij
&minus; 6
</p>
<p>r&lowast;8ij
</p>
<p>)(
r&lowast;i &minus; r&lowast;j
</p>
<p>)
(15.7)
</p>
<p>become universal expressions, i.e. there exists only one universal Lennard-Jones
system. To reduce computer time, usually the 6&ndash;12 potential is modified at larger
distances which can influence the simulation results [239]. In our model a simple
cutoff of potential and forces at rmax = 10 &Aring; is used.
</p>
<p>15.1.1 Integration of the Equations of Motion
</p>
<p>The equations of motion are integrated with the Verlet algorithm (Sect. 12.11.5)
</p>
<p>�ri = ri(t)&minus; ri(t &minus;�t)+
Fi(t)
</p>
<p>m
�t2 (15.8)
</p>
<p>ri(t +�t)= ri(t)+�ri +O
(
�t4
</p>
<p>)
. (15.9)
</p>
<p>We use a higher order expression for the velocities to improve the accuracy of the
calculated kinetic energy
</p>
<p>vi+1 =
�ri
</p>
<p>�t
+ 5Fi(t)&minus; 2Fi(t &minus;�t)
</p>
<p>6m
�t +O
</p>
<p>(
�t3
</p>
<p>)
. (15.10)
</p>
<p>1This small number of particles allows a graphical representation of the system during the simula-
tion.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.1 Simulation of a Lennard-Jones Fluid 281
</p>
<p>Fig. 15.1 Reflecting walls
</p>
<p>15.1.2 Boundary Conditions and Average Pressure
</p>
<p>Molecular dynamics simulations often involve periodic boundary conditions to re-
duce finite size effects. Here we employ an alternative method which simulates a
box with elastic walls. This allows us to calculate explicitly the pressure on the
walls of the box.
</p>
<p>The atoms are kept in the cube by reflecting walls, i.e. whenever an atom passes
a face of the cube, the normal component of the velocity vector is changed in sign
(Fig. 15.1). Thus the kinetic energy is conserved but a momentum of m�v = 2mv&perp;
is transferred to the wall. The average momentum change per time can be interpreted
as a force acting upon the wall
</p>
<p>F&perp; =
&lang;&sum;
</p>
<p>refl. 2mv&perp;
</p>
<p>dt
</p>
<p>&rang;
. (15.11)
</p>
<p>The pressure p is given by
</p>
<p>p = 1
6L2
</p>
<p>&lang;&sum;
walls
</p>
<p>&sum;
refl. 2mv&perp;
</p>
<p>dt
</p>
<p>&rang;
. (15.12)
</p>
<p>With the Verlet algorithm the reflection can be realized by exchanging the values of
the corresponding coordinate at times tn and tn&minus;1.
</p>
<p>15.1.3 Initial Conditions and Average Temperature
</p>
<p>At the very beginning the N = 125 atoms are distributed over equally spaced lattice
points within the cube. Velocities are randomly distributed according to a Gaussian
distribution for each Cartesian component vμ
</p>
<p>f (νμ)=
&radic;
</p>
<p>m
</p>
<p>2πkBT
e&minus;mv
</p>
<p>2
μ/2kBT (15.13)
</p>
<p>corresponding to a Maxwell speed distribution
</p>
<p>f
(
|v|
</p>
<p>)
=
(
</p>
<p>m
</p>
<p>2πkBT
</p>
<p>)3/2
4πv2e&minus;mv
</p>
<p>2/2kBT . (15.14)</p>
<p/>
</div>
<div class="page"><p/>
<p>282 15 Thermodynamic Systems
</p>
<p>Fig. 15.2 (Velocity
distribution) The velocity
distribution is shown for
T = 100 K and T = 500 K
(histograms) and compared to
the Maxwell speed
distribution (solid curves)
</p>
<p>Assuming thermal equilibrium, the effective temperature is calculated from the ki-
netic energy
</p>
<p>kBT =
2
</p>
<p>3N
Ekin. (15.15)
</p>
<p>The desired temperature To is established by the rescaling procedure
</p>
<p>vi &rarr; vi
</p>
<p>&radic;
kBTo
</p>
<p>kBTactual
(15.16)
</p>
<p>which is applied repeatedly during an equilibration run. The velocity distribution
f (|v|) can be monitored. It approaches quickly a stationary Maxwell distribution
(Fig. 15.2).
</p>
<p>A smoother method to control temperature is the Berendsen thermostat algorithm
[21]
</p>
<p>vi &rarr; vi
</p>
<p>&radic;
1 + �t
</p>
<p>τtherm
</p>
<p>kTo &minus; kTactual
kTactual
</p>
<p>(15.17)
</p>
<p>where τtherm is a suitable relaxation time (for instance τtherm = 20�t). This method
can be used also during the simulation. However, it does not generate the trajectory
of a true canonical ensemble. If this is necessary, more complicated methods have
to be used [128].
</p>
<p>15.1.4 Analysis of the Results
</p>
<p>After an initial equilibration phase the system is simulated at constant energy (NVE
simulation) or at constant temperature (NVT) simulation with the Berendsen ther-
mostat method. Several static and dynamic properties can be determined.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.1 Simulation of a Lennard-Jones Fluid 283
</p>
<p>Fig. 15.3 (Inner virial) The
inner virial W (15.20, crosses
and stars) is compared to
pV &minus; kBT (squares and
circles) for two values of the
particle density
N/V = 10&minus;3 &Aring;&minus;1 (a) and
1.95 &times; 10&minus;3 &Aring;&minus;1 (b),
corresponding to reduced
densities n&lowast; = σ 3N/V of
0.040 and 0.077
</p>
<p>15.1.4.1 Deviation from the Ideal Gas Behavior
</p>
<p>A dilute gas is approximately ideal with
</p>
<p>pV =NkBT . (15.18)
For a real gas the interaction between the particles has to be taken into account.
From the equipartition theorem it can be found that2
</p>
<p>pV =NkBT +W (15.19)
with the inner virial (Fig. 15.3)
</p>
<p>W =
&lang;
</p>
<p>1
</p>
<p>3
</p>
<p>&sum;
</p>
<p>i
</p>
<p>riFi
</p>
<p>&rang;
(15.20)
</p>
<p>which can be expanded as a power series of the number density n=N/V [231] to
give
</p>
<p>pV =NkBT
(
</p>
<p>1 + b(T )N
V
</p>
<p>+ c(T )
(
N
</p>
<p>V
</p>
<p>)2
+ &middot; &middot; &middot;
</p>
<p>)
. (15.21)
</p>
<p>The virial coefficient b(T ) can be calculated exactly for the Lennard-Jones gas
[231]:
</p>
<p>b(T )=&minus;2π
3
σ 3
</p>
<p>&infin;&sum;
</p>
<p>j=0
</p>
<p>2j&minus;3/2
</p>
<p>j ! Γ
(
</p>
<p>2j &minus; 1
4
</p>
<p>)(
ε
</p>
<p>kBT
</p>
<p>)(j/2+1/4)
. (15.22)
</p>
<p>For comparison we calculate the quantity
</p>
<p>V
</p>
<p>N
</p>
<p>(
pV
</p>
<p>NkBT
&minus; 1
</p>
<p>)
(15.23)
</p>
<p>which for small values of the particle density n= N/V correlates well (Fig. 15.4)
with expression (15.22).
</p>
<p>2MD simulations with periodic boundary conditions use this equation to calculate the pressure.</p>
<p/>
</div>
<div class="page"><p/>
<p>284 15 Thermodynamic Systems
</p>
<p>Fig. 15.4 (Second virial
coefficient) The value of
V
N
(
pV
kBT
</p>
<p>&minus; 1) is shown for two
values of the particle density
N/V = 10&minus;3 &Aring; (crosses) and
1.95 &times; 10&minus;3 &Aring;&minus;1 (circles)
and compared to the exact
second virial coefficient b
(dashed curve) (15.22)
</p>
<p>Fig. 15.5 (Radial pair
distribution) The normalized
radial distribution function
g(R)/gideal(R) is evaluated
for kT = 35 K, 100 K,
1000 K and a density of
n= 0.025A&minus;3 corresponding
to a reduced density
n&lowast; = σ 3N/V of 1.0. At this
density the Lennard-Jones
system shows a liquid-solid
transition at a temperature of
ca. 180 K [146]
</p>
<p>15.1.4.2 Structural Order
</p>
<p>A convenient measure for structural order [78] is the radial pair distribution function
(Fig. 15.5)
</p>
<p>g(R)=
&lang;
</p>
<p>1
</p>
<p>N(N &minus; 1)
&sum;
</p>
<p>i �=j
δ(rij &minus;R)
</p>
<p>&rang;
= P(R &lt; rij &lt;R + dR)
</p>
<p>dR
(15.24)
</p>
<p>which is usually normalized with respect to an ideal gas, for which
</p>
<p>gideal(R)= 4πnR2dR. (15.25)
</p>
<p>For small distances g(R)/gideal(R) vanishes due to the strong repulsive force.
It peaks at the distance of nearest neighbors and approaches unity at very large
distances. In the condensed phase additional maxima appear showing the degree of
short (liquid) and long range (solid) order.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.1 Simulation of a Lennard-Jones Fluid 285
</p>
<p>Fig. 15.6 (Velocity
auto-correlation function)
The Lennard-Jones system is
simulated for kBT = 200 K
and different values of the
density n&lowast; = 0.12 (a), 0.18
(b), 0.32 (c), 0.62 (d). The
velocity auto-correlation
function (full curves) is
averaged over 20 trajectories
and fitted by an exponential
function (dashed curves)
</p>
<p>Equation (15.25) is not valid for our small model system without periodic bound-
ary conditions. Therefore gideal was calculated numerically to normalize the results
shown in Fig. 15.5.
</p>
<p>15.1.4.3 Ballistic and Diffusive Motion
</p>
<p>The velocity auto-correlation function (Fig. 15.6)
</p>
<p>C(t)=
&lang;
v(t)v(t0)
</p>
<p>&rang;
(15.26)
</p>
<p>decays as a function of the delay time t &minus; t0 due to collisions of the particles. In
a stationary state it does not depend on the initial time t0. Integration leads to the
mean square displacement (Fig. 15.6)
</p>
<p>�x2(t)=
&lang;(
x(t)&minus; x(t0)
</p>
<p>)2&rang;
. (15.27)
</p>
<p>In the absence of collisions the mean square displacement grows with (t &minus; t0)2,
representing a ballistic type of motion. Collisions lead to a diffusive kind of motion
where the mean square displacement grows only linearly with time. The transition
between this two types of motion can be analyzed within the model of Brownian
motion [219] where the collisions are replaced by a fluctuating random force Γ (t)
and a damping constant γ .
</p>
<p>The equation of motion in one dimension is
</p>
<p>v̇+ γ v = Γ (t) (15.28)
with
</p>
<p>&lang;
Γ (t)
</p>
<p>&rang;
= 0 (15.29)
</p>
<p>&lang;
Γ (t)Γ
</p>
<p>(
t &prime;
)&rang;
= 2γ kBT
</p>
<p>m
δ
(
t &minus; t &prime;
</p>
<p>)
. (15.30)</p>
<p/>
</div>
<div class="page"><p/>
<p>286 15 Thermodynamic Systems
</p>
<p>The velocity correlation decays exponentially
</p>
<p>&lang;
v(t)v(t0)
</p>
<p>&rang;
= kBT
</p>
<p>m
e&minus;γ |t&minus;t0| (15.31)
</p>
<p>with the average velocity square given by
</p>
<p>&lang;
v2
&rang;
= C(t0)=
</p>
<p>kBT
</p>
<p>m
= 〈Ekin〉m
</p>
<p>2
</p>
<p>(15.32)
</p>
<p>and the integral of the correlation function equals
&int; &infin;
</p>
<p>t0
</p>
<p>C(t) dt = kBT
γm
</p>
<p>. (15.33)
</p>
<p>The average of �x2 is
</p>
<p>&lang;(
x(t)&minus; x(t0)
</p>
<p>)2&rang;= 2kBT
mγ
</p>
<p>(t &minus; t0)&minus;
2kBT
</p>
<p>mγ 2
</p>
<p>(
1 &minus; e&minus;γ (t&minus;t0)
</p>
<p>)
. (15.34)
</p>
<p>For small time differences t &minus; t0 the motion is ballistic with the thermal velocity
</p>
<p>&lang;(
x(t)&minus; x(t0)
</p>
<p>)2&rang;&asymp; kBT
m
</p>
<p>(t &minus; t0)2 =
&lang;
v2
&rang;
(t &minus; t0)2. (15.35)
</p>
<p>For large time differences diffusive motion emerges with
</p>
<p>&lang;(
x(t)&minus; x(t0)
</p>
<p>)2&rang;&asymp; 2kBT
mγ
</p>
<p>(t &minus; t0)= 2D(t &minus; t0) (15.36)
</p>
<p>with the diffusion constant given by the Einstein relation
</p>
<p>D = kBT
mγ
</p>
<p>. (15.37)
</p>
<p>For a three-dimensional simulation the Cartesian components of the position or ve-
locity vector add up independently. The diffusion coefficient can be determined from
</p>
<p>D = 1
6
</p>
<p>lim
t&rarr;&infin;
</p>
<p>〈(x(t)&minus; x(t0))2〉
t &minus; t0
</p>
<p>(15.38)
</p>
<p>or, alternatively from (15.33) [3]
</p>
<p>D = 1
3
</p>
<p>&int; &infin;
</p>
<p>t0
</p>
<p>&lang;
v(t)v(t0)
</p>
<p>&rang;
dt. (15.39)
</p>
<p>This equation is more generally valid also outside the Brownian limit (Green-
Kubo formula). The Brownian model represents the simulation data quite well at
low particle densities (Figs. 15.6, 15.7). For higher densities the velocity auto-
correlation function shows a very rapid decay followed by a more or less structured
tail [3, 159, 256].</p>
<p/>
</div>
<div class="page"><p/>
<p>15.2 Monte Carlo Simulation 287
</p>
<p>Fig. 15.7 (Mean square
displacement) The
Lennard-Jones system is
simulated for kBT = 200 K
and different values of the
density n&lowast; = 0.12 (a), 0.18
(b), 0.32 (c), 0.62 (d). The
mean square displacement
(full curves) is averaged over
20 trajectories and fitted by a
linear function (dashed lines)
for t &minus; t0 &gt; 1.5 ps
</p>
<p>Fig. 15.8 (Ising model)
N spins can be up or down.
The interaction with the
magnetic field is &minus;μBSi , the
interaction between nearest
neighbors is &minus;JSiSj
</p>
<p>15.2 Monte Carlo Simulation
</p>
<p>The basic principles of Monte Carlo simulations are discussed in Chap. 8. Here we
will apply the Metropolis algorithm to simulate the Ising model in one or two di-
mensions. The Ising model [26, 134] is primarily a model for the phase transition of
a ferromagnetic system. However, it has further applications for instance for a poly-
mer under the influence of an external force or protonation equilibria in proteins.
</p>
<p>15.2.1 One-Dimensional Ising Model
</p>
<p>We consider a chain consisting of N spins which can be either up (Si = 1) or down
(Si =&minus;1). The total energy in a magnetic field is (Fig. 15.8)
</p>
<p>H =&minus;MB =&minus;B
N&sum;
</p>
<p>i=1
μSi (15.40)
</p>
<p>and the average magnetic moment of one spin is
</p>
<p>〈M〉 = μe
μB/kT &minus; e&minus;μB/kT
eμB/kT + e&minus;μB/kT = μ tanh
</p>
<p>(
μB
</p>
<p>kT
</p>
<p>)
. (15.41)</p>
<p/>
</div>
<div class="page"><p/>
<p>288 15 Thermodynamic Systems
</p>
<p>Fig. 15.9 (Numerical
simulation of the
1-dimensional Ising model)
The average magnetization
per spin is calculated from a
MC simulation (circles) and
compared to the exact
solution (15.43). Parameters
are μB =&minus;5 and J =&minus;2
</p>
<p>If interaction between neighboring spins is included the energy of a configuration
(S1 &middot; &middot; &middot;SN ) becomes
</p>
<p>H =&minus;μB
N&sum;
</p>
<p>i=1
Si &minus; J
</p>
<p>N&minus;1&sum;
</p>
<p>i=1
SiSi+1. (15.42)
</p>
<p>The 1-dimensional model can be solved analytically [231]. In the limit N &rarr;&infin; the
magnetization is
</p>
<p>〈M〉 = μ
sinh(μB
</p>
<p>kT
)
</p>
<p>&radic;
sinh2(μB
</p>
<p>kT
)+ e4J/kT
</p>
<p>. (15.43)
</p>
<p>The numerical simulation (Fig. 15.9) starts either with the ordered state Si = 1 or
with a random configuration. New configurations are generated with the Metropolis
method as follows:
</p>
<p>&bull; flip one randomly chosen spin Si3 and calculate the energy change due to the
change �Si = (&minus;Si)&minus; Si =&minus;2Si
</p>
<p>�E =&minus;μB�Si &minus; J�Si(Si+1 + Si&minus;1)= 2μBSi + 2JSi(Si+1 + Si&minus;1)
(15.44)
</p>
<p>&bull; if �E &lt; 0 then accept the flip, otherwise accept it with a probability of P =
e&minus;�E/kT
</p>
<p>As a simple example consider N = 3 spins which have 8 possible configurations.
The probabilities of the trial step Ti&rarr;j are shown in Table 15.1.
</p>
<p>The table is symmetric and all configurations are connected.
</p>
<p>3Or try one spin after the other.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.2 Monte Carlo Simulation 289
</p>
<p>Table 15.1 transition probabilities for a 3-spin system (p = 1/3)
+++ ++&minus; +&minus;+ +&minus;&minus; &minus;++ &minus;+&minus; &minus;&minus;+ &minus;&minus;&minus;
</p>
<p>+++ 0 p p 0 p 0 0 0
++&minus; p 0 0 p 0 p 0 0
+&minus;+ p 0 0 p 0 0 p 0
+&minus;&minus; 0 p p 0 0 0 0 p
&minus;++ p 0 0 0 0 p p 0
&minus;+&minus; 0 p 0 0 p 0 0 p
&minus;&minus;+ 0 0 p 0 p 0 0 p
&minus;&minus;&minus; 0 0 0 p 0 p p 0
</p>
<p>Fig. 15.10 (Numerical
simulation of the
2-dimensional Ising model)
The average magnetization
per spin is calculated for
B = 0 from a MC simulation
(circles) and compared to
(15.46)
</p>
<p>15.2.2 Two-Dimensional Ising Model
</p>
<p>For dimension d &gt; 1 the Ising model behaves qualitatively different as a phase tran-
sition appears. For B = 0 (Fig. 15.10) the 2-dimensional Ising-model with 4 near-
est neighbors can be solved analytically [171, 194]. The magnetization disappears
above the critical temperature Tc, which is given by
</p>
<p>J
</p>
<p>kTc
=&minus;1
</p>
<p>2
ln(
</p>
<p>&radic;
2 &minus; 1)&asymp; 1
</p>
<p>2.27
. (15.45)
</p>
<p>Below Tc the average magnetization is given by
</p>
<p>〈M〉 =
(
</p>
<p>1 &minus; 1
sinh4( 2J
</p>
<p>kT
)
</p>
<p>) 1
8
</p>
<p>. (15.46)</p>
<p/>
</div>
<div class="page"><p/>
<p>290 15 Thermodynamic Systems
</p>
<p>Fig. 15.11 Two state model
</p>
<p>15.3 Problems
</p>
<p>Problem 15.1 (Lennard-Jones fluid) In this computer experiment a Lennard-Jones
fluid is simulated. The pressure is calculated from the average transfer of momentum
(15.12) and compared with expression (15.19).
</p>
<p>&bull; Equilibrate the system and observe how the distribution of squared velocities ap-
proaches a Maxwell distribution.
</p>
<p>&bull; Equilibrate the system for different values of temperature and volume and inves-
tigate the relation between pV/N and kT .
</p>
<p>&bull; observe the radial distribution function for different values of temperature and
densities. Try to locate phase transitions.
</p>
<p>&bull; determine the decay time of the velocity correlation function and compare with
the behavior of the mean square displacement which shows a transition from
ballistic to diffusive motion.
</p>
<p>Problem 15.2 (One-dimensional Ising model) In this computer experiment we sim-
ulate a linear chain of N = 500 spins with periodic boundaries and interaction be-
tween nearest neighbors only. We go along the chain and try to flip one spin after
the other according to the Metropolis method.
</p>
<p>After trying to flip the last spin SN the total magnetization
</p>
<p>M =
N&sum;
</p>
<p>i=1
Si (15.47)
</p>
<p>is calculated. It is averaged over 500 such cycles and then compared graphically
with the analytical solution for the infinite chain (15.43). Temperature and magnetic
field can be varied.
</p>
<p>Problem 15.3 (Two-state model for a polymer) Consider a polymer (Fig. 15.11)
consisting of N units which can be in two states Si = +1 or Si = &minus;1 with corre-
sponding lengths l+ and l&minus;. The interaction between neighboring units takes one of
the values w++,w+&minus;,w&minus;&minus;. Under the influence of an external force κ the energy
of the polymer is
</p>
<p>E =&minus;κ
&sum;
</p>
<p>i
</p>
<p>l(Si)+
&sum;
</p>
<p>i
</p>
<p>w(Si, Si+1). (15.48)
</p>
<p>This model is isomorphic to the one-dimensional Ising model,</p>
<p/>
</div>
<div class="page"><p/>
<p>15.3 Problems 291
</p>
<p>E =&minus;κN l&minus; + l+
2
</p>
<p>&minus; κ l+ &minus; l&minus;
2
</p>
<p>&sum;
Si (15.49)
</p>
<p>+
&sum;(
</p>
<p>w+&minus; +
w++ &minus;w+&minus;
</p>
<p>2
Si +
</p>
<p>w+&minus; &minus;w&minus;&minus;
2
</p>
<p>Si+1
</p>
<p>+ w++ +w&minus;&minus; &minus; 2w+&minus;
2
</p>
<p>SiSi+1
</p>
<p>)
(15.50)
</p>
<p>= κN l&minus; + l+
2
</p>
<p>+Nw+&minus;
</p>
<p>&minus; κ l+ &minus; l&minus;
2
</p>
<p>M + w++ &minus;w&minus;&minus;
2
</p>
<p>M
</p>
<p>+ w++ +w&minus;&minus; &minus; 2w+&minus;
2
</p>
<p>&sum;
SiSi+1. (15.51)
</p>
<p>Comparison with (15.42) shows the correspondence
</p>
<p>&minus;J = w++ +w&minus;&minus; &minus; 2w+&minus;
2
</p>
<p>(15.52)
</p>
<p>&minus;μB =&minus;κ l+ &minus; l&minus;
2
</p>
<p>+ w++ &minus;w&minus;&minus;
2
</p>
<p>(15.53)
</p>
<p>L=
&sum;
</p>
<p>l(Si)=N
l+ + l&minus;
</p>
<p>2
+ l+ &minus; l&minus;
</p>
<p>2
M. (15.54)
</p>
<p>In this computer experiment we simulate a linear chain of N = 20 units with
periodic boundaries and nearest neighbor interaction as in the previous problem.
</p>
<p>The fluctuations of the chain conformation are shown graphically and the mag-
netization of the isomorphic Ising model is compared with the analytical expression
for the infinite system (15.43). Temperature and magnetic field can be varied as well
as the coupling J . For negative J the anti-ferromagnetic state becomes stable at low
magnetic field strengths.
</p>
<p>Problem 15.4 (Two-dimensional Ising model) In this computer experiment a 200&times;
200 square lattice with periodic boundaries and interaction with the 4 nearest neigh-
bors is simulated. The fluctuations of the spins can be observed. At low temperatures
ordered domains with parallel spin appear. The average magnetization is compared
with the analytical expression for the infinite system (15.46).</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 16
</p>
<p>Random Walk and Brownian Motion
</p>
<p>Random walk processes are an important class of stochastic processes. They have
many applications in physics, computer science, ecology, economics and other
fields. A random walk [199] is a sequence of successive random steps. In this chap-
ter we study Markovian [166, 167]1 discrete time2 models. In one dimension the
position of the walker after n steps approaches a Gaussian distribution, which does
not depend on the distribution of the single steps. This follows from the central
limit theorem and can be checked in a computer experiment. A 3-dimensional ran-
dom walk provides a simple statistical model for the configuration of a biopolymer,
the so called freely jointed chain model. In a computer experiment we generate ran-
dom structures and calculate the gyration tensor, an experimentally observable quan-
tity,which gives information on the shape of a polymer. Simulation of the dynamics
is simplified if the fixed length segments of the freely jointed chain are replaced by
Hookean springs. This is utilized in a computer experiment to study the dependence
of the polymer extension on an applied external force (this effect is known as en-
tropic elasticity). The random motion of a heavy particle in a bath of light particles,
known as Brownian motion, can be described by Langevin dynamics, which replace
the collisions with the light particles by an average friction force proportional to the
velocity and a randomly fluctuating force with zero mean and infinitely short cor-
relation time. In a computer experiment we study Brownian motion in a harmonic
potential.
</p>
<p>16.1 Markovian Discrete Time Models
</p>
<p>The time evolution of a system is described in terms of an N -dimensional vector
r(t), which can be for instance the position of a molecule in a liquid, or the price
</p>
<p>1Different steps are independent.
2A special case of the more general continuous time random walk with a waiting time distribution
of P (τ)= δ(τ &minus;�t).
</p>
<p>P.O.J. Scherer, Computational Physics, Graduate Texts in Physics,
DOI 10.1007/978-3-319-00401-3_16,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>293</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-00401-3_16">http://dx.doi.org/10.1007/978-3-319-00401-3_16</a></div>
</div>
<div class="page"><p/>
<p>294 16 Random Walk and Brownian Motion
</p>
<p>Fig. 16.1 Discrete time
random walk
</p>
<p>of a fluctuating stock. At discrete times tn = n�t the position changes suddenly
(Fig. 16.1)
</p>
<p>r(tn+1)= r(tn)+�rn (16.1)
where the steps are distributed according to the probability distribution3
</p>
<p>P(�rn = b)= f (b). (16.2)
The probability of reaching the position R after n+ 1 steps obeys the equation
</p>
<p>Pn+1(R)= P
(
r(tn+1)=R
</p>
<p>)
</p>
<p>=
&int;
</p>
<p>dNbPn(R&minus; b)f (b). (16.3)
</p>
<p>16.2 Random Walk in One Dimension
</p>
<p>Consider a random walk in one dimension. We apply the central limit theorem to
calculate the probability distribution of the position rn after n steps. The first two
moments and the standard deviation of the step distribution are
</p>
<p>b=
&int;
</p>
<p>db bf (b) b2 =
&int;
</p>
<p>db b2f (b) σb =
&radic;
b2 &minus; b2. (16.4)
</p>
<p>Hence the normalized quantity
</p>
<p>ξi =
�xi &minus; b
</p>
<p>σb
(16.5)
</p>
<p>is a random variable with zero average and unit standard deviation. The distribution
function of the new random variable
</p>
<p>3General random walk processes are characterized by a distribution function P (R,R&prime;). Here we
consider only correlated processes for which P (R,R&prime;)= P (R&prime; &minus;R).</p>
<p/>
</div>
<div class="page"><p/>
<p>16.2 Random Walk in One Dimension 295
</p>
<p>Fig. 16.2 (Random walk
with constant step size) The
figure shows the position rn
for three different
1-dimensional random walks
with step size �x =&plusmn;1. The
dashed curves show the width
&plusmn;σ =&plusmn;&radic;n of the Gaussian
approximation (16.8)
</p>
<p>ηn =
ξ1 + ξ2 + &middot; &middot; &middot; + ξn&radic;
</p>
<p>n
= rn &minus; nb
</p>
<p>σb
&radic;
n
</p>
<p>(16.6)
</p>
<p>approaches a normal distribution for large n
</p>
<p>f (ηn)&rarr;
1&radic;
2π
</p>
<p>e&minus;η
2
n/2 (16.7)
</p>
<p>and finally from
</p>
<p>f (rn)drn = f (ηn)dηn = f (ηn)
drn
</p>
<p>σb
&radic;
n
</p>
<p>we have
</p>
<p>f (rn)=
1&radic;
</p>
<p>2πnσb
exp
</p>
<p>{
&minus; (rn &minus; nb)
</p>
<p>2
</p>
<p>2nσ 2b
</p>
<p>}
. (16.8)
</p>
<p>The position of the walker after n steps obeys approximately a Gaussian distribution
centered at rn = nb with a standard deviation of
</p>
<p>σrn =
&radic;
nσb. (16.9)
</p>
<p>16.2.1 Random Walk with Constant Step Size
</p>
<p>In the following we consider the classical example of a 1-dimensional random walk
process with constant step size. At time tn the walker takes a step of length �x to
the left with probability p or to the right with probability q = 1 &minus; p (Figs. 16.2,
16.3).
</p>
<p>The corresponding step size distribution function is
</p>
<p>f (b)= pδ(b+�x)+ qδ(b&minus;�x) (16.10)</p>
<p/>
</div>
<div class="page"><p/>
<p>296 16 Random Walk and Brownian Motion
</p>
<p>Fig. 16.3 Random walk with
constant step size
</p>
<p>with the first two moments
</p>
<p>b= (q &minus; p)�x b2 =�x2. (16.11)
Let the walker start at r(t0)= 0. The probability Pn(m) of reaching position m�x
after n steps obeys the recursion
</p>
<p>Pn+1(m)= pPn(m+ 1)+ qPn(m&minus; 1) (16.12)
which obviously leads to a binomial distribution. From the expansion of
</p>
<p>(p+ q)n =
&sum;( n
</p>
<p>m
</p>
<p>)
pmqn&minus;m (16.13)
</p>
<p>we see that
</p>
<p>Pn(n&minus; 2m)=
(
n
</p>
<p>m
</p>
<p>)
pmqn&minus;m (16.14)
</p>
<p>or after substitution m&prime; = n&minus; 2m=&minus;n,&minus;n+ 2 &middot; &middot; &middot;n&minus; 2, n:
</p>
<p>Pn
(
m&prime;
</p>
<p>)
=
(
</p>
<p>n
</p>
<p>(n&minus;m&prime;)/2
</p>
<p>)
p(n&minus;m
</p>
<p>&prime;)/2q(n+m
&prime;)/2. (16.15)
</p>
<p>Since the steps are uncorrelated we easily find the first two moments
</p>
<p>rn =
n&sum;
</p>
<p>i=1
�xi = nb= n�x(q &minus; p) (16.16)
</p>
<p>and
</p>
<p>r2n =
(
</p>
<p>n&sum;
</p>
<p>i=1
�xi
</p>
<p>)2
=
</p>
<p>n&sum;
</p>
<p>i,j=1
�xi�xj =
</p>
<p>n&sum;
</p>
<p>i=1
(�xi)2 = nb2 = n�x2. (16.17)
</p>
<p>16.3 The Freely Jointed Chain
</p>
<p>We consider a simple statistical model for the conformation of a biopolymer like
DNA or a protein.
</p>
<p>The polymer is modeled by a 3-dimensional chain consisting of M units with
constant bond length and arbitrary relative orientation (Fig. 16.4). The configuration
can be described by a point in a 3(M + 1)-dimensional space which is reached after
M steps �ri = bi of a 3-dimensional random walk with constant step size
</p>
<p>rM = r0 +
M&sum;
</p>
<p>i=1
bi . (16.18)</p>
<p/>
</div>
<div class="page"><p/>
<p>16.3 The Freely Jointed Chain 297
</p>
<p>Fig. 16.4 Freely jointed
chain with constant bond
length b
</p>
<p>16.3.1 Basic Statistic Properties
</p>
<p>The M bond vectors
</p>
<p>bi = ri &minus; ri&minus;1 (16.19)
</p>
<p>have a fixed length |bi | = b and are oriented randomly. The first two moments are
</p>
<p>bi = 0 b2i = b
2. (16.20)
</p>
<p>Since different units are independent
</p>
<p>bibj = δi,jb2. (16.21)
</p>
<p>Obviously the relative position of segment j
</p>
<p>Rj = rj &minus; r0 =
j&sum;
</p>
<p>i=1
bi
</p>
<p>has zero mean
</p>
<p>Rj =
j&sum;
</p>
<p>i=1
bi = 0 (16.22)
</p>
<p>and its second moment is
</p>
<p>R2j =
(
</p>
<p>j&sum;
</p>
<p>i=1
bi
</p>
<p>j&sum;
</p>
<p>k=1
bk
</p>
<p>)
=
</p>
<p>j&sum;
</p>
<p>i,k=1
bibk = jb2. (16.23)
</p>
<p>For the end to end distance (Fig. 16.5)
</p>
<p>RM = rM &minus; r0 =
M&sum;
</p>
<p>i=1
bi (16.24)
</p>
<p>this gives
</p>
<p>RM = 0, R2M =Mb
2. (16.25)
</p>
<p>Let us apply the central limit theorem for large M . For the x coordinate of the end
to end vector we have</p>
<p/>
</div>
<div class="page"><p/>
<p>298 16 Random Walk and Brownian Motion
</p>
<p>Fig. 16.5 (Freely jointed
chain) The figure shows a
random 3-dimensional
structure with 1000 segments
visualized as balls (Molden
graphics [223])
</p>
<p>X =
M&sum;
</p>
<p>i=1
biex = b
</p>
<p>&sum;
</p>
<p>i
</p>
<p>cos θi . (16.26)
</p>
<p>With the help of the averages4
</p>
<p>cos θi =
1
</p>
<p>4π
</p>
<p>&int; 2π
</p>
<p>0
dφ
</p>
<p>&int; π
</p>
<p>0
cos θ sin θ dθ = 0 (16.27)
</p>
<p>(cos θi)2 =
1
</p>
<p>4π
</p>
<p>&int; 2π
</p>
<p>0
dφ
</p>
<p>&int; π
</p>
<p>0
cos2 θ sin θdθ = 1
</p>
<p>3
(16.28)
</p>
<p>we find that the scaled difference
</p>
<p>ξi =
&radic;
</p>
<p>3 cos θi (16.29)
</p>
<p>has zero mean and unit variance and therefore the sum
</p>
<p>X̃ =
&radic;
</p>
<p>3
</p>
<p>b
&radic;
M
</p>
<p>X =
&radic;
</p>
<p>3
</p>
<p>M
</p>
<p>M&sum;
</p>
<p>i=1
cos θi (16.30)
</p>
<p>converges to a normal distribution:
</p>
<p>P(X̃)= 1&radic;
2π
</p>
<p>exp
</p>
<p>{
&minus; X̃
</p>
<p>2
</p>
<p>2
</p>
<p>}
. (16.31)
</p>
<p>4For a 1-dimensional polymer cos θi = 0 and (cos θi)2 = 1. In two dimensions cos θi =
1
π
</p>
<p>&int; π
0 cos θ dθ = 0 and (cos θi)2 =
</p>
<p>1
π
</p>
<p>&int; π
0 cos
</p>
<p>2 θ dθ = 12 . To include these cases the factor 3 in the
exponent of (16.33) should be replaced by the dimension d .</p>
<p/>
</div>
<div class="page"><p/>
<p>16.3 The Freely Jointed Chain 299
</p>
<p>Hence
</p>
<p>P(X)= 1&radic;
2π
</p>
<p>&radic;
3
</p>
<p>b
&radic;
M
</p>
<p>exp
</p>
<p>{
&minus; 3
</p>
<p>2Mb2
X2
</p>
<p>}
(16.32)
</p>
<p>and finally in 3 dimensions
</p>
<p>P(RM)= P(X)P (Y )P (Z)
</p>
<p>=
&radic;
</p>
<p>27
</p>
<p>b3
&radic;
(2πM)3
</p>
<p>exp
</p>
<p>{
&minus; 3
</p>
<p>2Mb2
R2M
</p>
<p>}
. (16.33)
</p>
<p>16.3.2 Gyration Tensor
</p>
<p>For the center of mass
</p>
<p>Rc =
1
</p>
<p>M
</p>
<p>M&sum;
</p>
<p>i=1
Ri
</p>
<p>we find
</p>
<p>Rc = 0 R2c =
1
</p>
<p>M2
</p>
<p>&sum;
</p>
<p>i,j
</p>
<p>RiRj
</p>
<p>and since
</p>
<p>RiRj = min(i, j)b2
</p>
<p>we have
</p>
<p>R2c =
b2
</p>
<p>M2
</p>
<p>(
2
</p>
<p>M&sum;
</p>
<p>i=1
i(M &minus; i + 1)&minus;
</p>
<p>M&sum;
</p>
<p>i=1
i
</p>
<p>)
= b
</p>
<p>2
</p>
<p>M2
</p>
<p>(
M3
</p>
<p>3
+ M
</p>
<p>2
</p>
<p>2
+ M
</p>
<p>6
</p>
<p>)
&asymp; Mb
</p>
<p>2
</p>
<p>3
.
</p>
<p>The gyration radius [170] is generally defined by
</p>
<p>R2g =
1
</p>
<p>M
</p>
<p>M&sum;
</p>
<p>i=1
(Ri &minus;Rc)2
</p>
<p>= 1
M
</p>
<p>M&sum;
</p>
<p>i=1
</p>
<p>(
R2i +R2c &minus; 2
</p>
<p>1
</p>
<p>M
</p>
<p>M&sum;
</p>
<p>j=1
RiRj
</p>
<p>)
= 1
</p>
<p>M
</p>
<p>&sum;
</p>
<p>i
</p>
<p>(
R2i
</p>
<p>)
&minus;R2c
</p>
<p>= b2M + 1
2
</p>
<p>&minus; b
2
</p>
<p>M2
</p>
<p>(
M3
</p>
<p>3
+ M
</p>
<p>2
</p>
<p>2
+ M
</p>
<p>6
</p>
<p>)
= b2
</p>
<p>(
M
</p>
<p>6
&minus; 1
</p>
<p>6M
</p>
<p>)
&asymp; Mb
</p>
<p>2
</p>
<p>6
.
</p>
<p>Rg can be also written as
</p>
<p>R2g =
(
</p>
<p>1
</p>
<p>M
</p>
<p>&sum;
</p>
<p>i
</p>
<p>R2i &minus;
1
</p>
<p>M2
</p>
<p>&sum;
</p>
<p>ij
</p>
<p>RiRj
</p>
<p>)
= 1
</p>
<p>2M2
</p>
<p>M&sum;
</p>
<p>i=1
</p>
<p>M&sum;
</p>
<p>j=1
(Ri &minus;Rj )2</p>
<p/>
</div>
<div class="page"><p/>
<p>300 16 Random Walk and Brownian Motion
</p>
<p>Fig. 16.6 (Gyration tensor) The eigenvalues of the gyration tensor give information on the shape
of the polymer. If the extension is larger (smaller) along one direction than in the perpendicular
plane, one eigenvalue is larger (smaller) than the two other
</p>
<p>Fig. 16.7 Polymer model
with Hookean springs
</p>
<p>and can be experimentally measured with the help of scattering phenomena. It is
related to the gyration tensor which is defined as
</p>
<p>Ωg =
1
</p>
<p>M
</p>
<p>&sum;
</p>
<p>i
</p>
<p>(Ri &minus;Rc)(Ri &minus;Rc)T .
</p>
<p>Its trace is
</p>
<p>tr(Ωg)=R2g
and its eigenvalues give us information about the shape of the polymer (Fig. 16.6).
</p>
<p>16.3.3 Hookean Spring Model
</p>
<p>Simulation of the dynamics of the freely jointed chain is complicated by the con-
straints which are implied by the constant chain length. Much simpler is the simu-
lation of a model which treats the segments as Hookean springs (Fig. 16.7). In the
limit of a large force constant the two models give equivalent results.
</p>
<p>We assume that the segments are independent (self crossing is not avoided). Then
for one segment the energy contribution is
</p>
<p>Ei =
f
</p>
<p>2
</p>
<p>(
|bi | &minus; b
</p>
<p>)2
.</p>
<p/>
</div>
<div class="page"><p/>
<p>16.4 Langevin Dynamics 301
</p>
<p>Fig. 16.8 (Distribution of
bond vectors) The bond
vector distribution for a
1-dimensional chain of
springs has maxima at &plusmn;b.
For large force constants the
width of the two peaks
becomes small and the chain
of springs resembles a freely
jointed chain with constant
bond length
</p>
<p>If the fluctuations are small
∣∣|bi | &minus; b
</p>
<p>∣∣≪ b
then (Fig. 16.8)
</p>
<p>|bi | &asymp; b b2i &asymp; b
2
</p>
<p>and the freely jointed chain model (16.33) gives the entropy as a function of the end
to end vector
</p>
<p>S =&minus;kB ln
(
P(RM)
</p>
<p>)
=&minus;kB ln
</p>
<p>( &radic;
27
</p>
<p>b3
&radic;
(2πM)3
</p>
<p>)
+ 3kB
</p>
<p>2Mb2
RM
</p>
<p>2. (16.34)
</p>
<p>If one end of the polymer is fixed at r0 = 0 and a force κ is applied to the other end,
the free energy is given by
</p>
<p>F = T S &minus; κRM =
3kBT
</p>
<p>2Mb2
R2M &minus; κRM + const. (16.35)
</p>
<p>In thermodynamic equilibrium the free energy is minimal, hence the average exten-
sion is
</p>
<p>RM =
Mb2
</p>
<p>3kBT
κ. (16.36)
</p>
<p>This linear behavior is similar to a Hookean spring with an effective force constant
</p>
<p>feff =
Mb2
</p>
<p>3kBT
(16.37)
</p>
<p>and is only valid for small forces. For large forces the freely jointed chain asymp-
totically reaches its maximum length of RM,max = Mb, whereas for the chain of
springs RM &rarr;M(b+ κ/f ).
</p>
<p>16.4 Langevin Dynamics
</p>
<p>A heavy particle moving in a bath of much smaller and lighter particles (for in-
stance atoms and molecules of the air) shows what is known as Brownian motion</p>
<p/>
</div>
<div class="page"><p/>
<p>302 16 Random Walk and Brownian Motion
</p>
<p>[41, 76, 77]. Due to collisions with the thermally moving bath particles it experi-
ences a fluctuating force which drives the particle into a random motion. The French
physicist Paul Langevin developed a model to describe this motion without includ-
ing the light particles explicitly. The fluctuating force is divided into a macroscopic
friction force proportional to the velocity
</p>
<p>Ffr =&minus;γ v (16.38)
and a randomly fluctuating force with zero mean and infinitely short correlation time
</p>
<p>Frand(t)= 0 Frand(t)Frand
(
t &prime;
)
= F2randδ
</p>
<p>(
t &minus; t &prime;
</p>
<p>)
. (16.39)
</p>
<p>The equations of motion for the heavy particle are
</p>
<p>d
</p>
<p>dt
x= v
</p>
<p>d
</p>
<p>dt
v=&minus;γ v+ 1
</p>
<p>m
Ffr(t)&minus;
</p>
<p>1
</p>
<p>m
&nabla;U(x)
</p>
<p>(16.40)
</p>
<p>with the macroscopic friction coefficient γ and the potential U(x).
The behavior of the random force can be better understood if we introduce a time
</p>
<p>grid tn+1 &minus; tn = �t and take the limit �t &rarr; 0. We assume that the random force
has a constant value during each interval
</p>
<p>Frand(t)= Fn tn &le; t &lt; tn+1 (16.41)
and that the values at different intervals are uncorrelated
</p>
<p>FnFm = δm,nF2n. (16.42)
The auto-correlation function then is given by
</p>
<p>Frand(t)Frand
(
t &prime;
)
=
{
</p>
<p>0 different intervals
F2n same interval.
</p>
<p>(16.43)
</p>
<p>Division by �t gives a sequence of functions which converges to a delta function in
the limit �t &rarr; 0
</p>
<p>1
</p>
<p>�t
Frand(t)Frand
</p>
<p>(
t &prime;
)
&rarr; F2n δ
</p>
<p>(
t &minus; t &prime;
</p>
<p>)
. (16.44)
</p>
<p>Hence we find
</p>
<p>F2n =
1
</p>
<p>�t
F2rand. (16.45)
</p>
<p>Within a short time interval �t &rarr; 0 the velocity changes by
</p>
<p>v(tn +�t)= v&minus; γ v�t &minus;
1
</p>
<p>m
&nabla;U(x)�t + 1
</p>
<p>m
Fn�t + &middot; &middot; &middot; (16.46)
</p>
<p>and taking the square gives
</p>
<p>v2(tn +�t)= v2 &minus; 2γ v2�t &minus;
2
</p>
<p>m
v&nabla;U(x)�t + 2
</p>
<p>m
vFn�t +
</p>
<p>F2n
</p>
<p>m2
(�t)2 + &middot; &middot; &middot; .
</p>
<p>(16.47)</p>
<p/>
</div>
<div class="page"><p/>
<p>16.5 Problems 303
</p>
<p>Hence for the total energy
</p>
<p>E(tn +�t)=
m
</p>
<p>2
v2(tn +�t)+U
</p>
<p>(
x(tn +�t)
</p>
<p>)
</p>
<p>= m
2
v2(tn +�t)+U(x)+ v&nabla;U(x)�t + &middot; &middot; &middot; (16.48)
</p>
<p>we have
</p>
<p>E(tn +�t)=E(tn)&minus;mγ v2�t + vFn�t +
F2n
</p>
<p>2m
(�t)2 + &middot; &middot; &middot; . (16.49)
</p>
<p>On the average the total energy E should be constant and furthermore in d dimen-
sions
</p>
<p>m
</p>
<p>2
v2 = d
</p>
<p>2
kBT . (16.50)
</p>
<p>Therefore we conclude
</p>
<p>mγ v2 = �t
2m
</p>
<p>F2n =
1
</p>
<p>2m
F2rand (16.51)
</p>
<p>from which we obtain finally
</p>
<p>F2n =
2mγd
</p>
<p>�t
kBT . (16.52)
</p>
<p>16.5 Problems
</p>
<p>Problem 16.1 (Random walk in one dimension) This program generates random
walks with (a) fixed step length �x =&plusmn;1 or (b) step length equally distributed over
the interval &minus;
</p>
<p>&radic;
3 &lt;�x &lt;
</p>
<p>&radic;
3. It also shows the variance, which for large number
</p>
<p>of walks approaches σ =&radic;n. See also Fig. 16.2.
</p>
<p>Problem 16.2 (Gyration tensor) The program calculates random walks with M
steps of length b. The bond vectors are generated from M random points ei on the
unit sphere as bi = bei . End to end distance, center of gravity and gyration radius
are calculated and can be averaged over numerous random structures. The gyration
tensor 16.3.2 is diagonalized and the ordered eigenvalues are averaged.
</p>
<p>Problem 16.3 (Brownian motion in a harmonic potential) The program simulates a
particle in a 1-dimensional harmonic potential
</p>
<p>U(x)= f
2
x2 &minus; κx (16.53)
</p>
<p>where κ is an external force. We use the improved Euler method (12.36). First the
coordinate and the velocity at mid time are estimated</p>
<p/>
</div>
<div class="page"><p/>
<p>304 16 Random Walk and Brownian Motion
</p>
<p>x
</p>
<p>(
tn +
</p>
<p>�t
</p>
<p>2
</p>
<p>)
= x(tn)+ v(tn)
</p>
<p>�t
</p>
<p>2
(16.54)
</p>
<p>v
</p>
<p>(
tn +
</p>
<p>�t
</p>
<p>2
</p>
<p>)
= v(tn)&minus; γ v(tn)
</p>
<p>�t
</p>
<p>2
+ Fn
</p>
<p>m
</p>
<p>�t
</p>
<p>2
&minus; f
</p>
<p>m
x(tn)
</p>
<p>�t
</p>
<p>2
(16.55)
</p>
<p>where Fn is a random number obeying (16.52). Then the values at tn+1 are calcu-
lated as
</p>
<p>x(tn +�t)= x(tn)+ v
(
tn +
</p>
<p>�t
</p>
<p>2
</p>
<p>)
�t (16.56)
</p>
<p>v(tn +�t)= v(tn)&minus; γ v
(
tn +
</p>
<p>�t
</p>
<p>2
</p>
<p>)
�t + Fn
</p>
<p>m
�t &minus; f
</p>
<p>m
x
</p>
<p>(
tn +
</p>
<p>�t
</p>
<p>2
</p>
<p>)
�t.
</p>
<p>(16.57)
</p>
<p>Problem 16.4 (Force extension relation) The program simulates a chain of springs
(Sect. 16.3.3) with potential energy
</p>
<p>U = f
2
</p>
<p>&sum;(
|bi | &minus; b
</p>
<p>)2 &minus; κRM . (16.58)
</p>
<p>The force can be varied and the extension along the force direction is averaged over
numerous time steps.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 17
</p>
<p>Electrostatics
</p>
<p>The electrostatic potential Φ(r) of a charge distribution ρ(r) is a solution1 of Pois-
son&rsquo;s equation
</p>
<p>�Φ(r)=&minus;ρ(r) (17.1)
which, for spatially varying dielectric constant ε(r) becomes
</p>
<p>div
(
ε(r)gradΦ(r)
</p>
<p>)
=&minus;ρ(r) (17.2)
</p>
<p>and, if mobile charges are taken into account, like for an electrolyte or semiconduc-
tor, turns into the Poisson-Boltzmann equation
</p>
<p>div
(
ε(r)gradΦ(r)
</p>
<p>)
=&minus;ρfix(r)&minus;
</p>
<p>&sum;
</p>
<p>i
</p>
<p>n0iZie e
&minus;ZieΦ(r)/kBT . (17.3)
</p>
<p>In this chapter we discretize the Poisson and the linearized Poisson-Boltzmann
equation by finite volume methods which are applicable even in case of discontinu-
ous ε. We solve the discretized equations iteratively with the method of successive
over-relaxation. The solvation energy of a charged sphere in a dielectric medium is
calculated to compare the accuracy of several methods. This can be studied also in
a computer experiment.
</p>
<p>Since the Green&rsquo;s function is analytically available for the Poisson and Poisson-
Boltzmann equations, alternatively the method of boundary elements can be applied,
which can reduce the computer time, for instance for solvation models. A computer
experiment simulates a point charge within a spherical cavity and calculates the
solvation energy with the boundary element method.
</p>
<p>17.1 Poisson Equation
</p>
<p>From a combination of the basic equations of electrostatics
</p>
<p>1The solution depends on the boundary conditions, which in the simplest case are given by
lim|r|&rarr;&infin;Φ(r)= 0.
</p>
<p>P.O.J. Scherer, Computational Physics, Graduate Texts in Physics,
DOI 10.1007/978-3-319-00401-3_17,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>305</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-00401-3_17">http://dx.doi.org/10.1007/978-3-319-00401-3_17</a></div>
</div>
<div class="page"><p/>
<p>306 17 Electrostatics
</p>
<p>Fig. 17.1 (Finite volume for
the Poisson equation) The
control volume is a small
cube centered at a grid point
(full circle)
</p>
<p>divD(r)= ρ(r) (17.4)
D(r)= ε(r)E(r) (17.5)
</p>
<p>E(r)=&minus;gradΦ(r) (17.6)
the generalized Poisson equation is obtained
</p>
<p>div
(
ε(r)gradΦ(r)
</p>
<p>)
=&minus;ρ(r) (17.7)
</p>
<p>which can be written in integral form with the help of Gauss&rsquo;s theorem
∮
</p>
<p>&part;V
</p>
<p>dA div
(
ε(r)gradΦ(r)
</p>
<p>)
=
&int;
</p>
<p>V
</p>
<p>dV ε(r)gradΦ(r)=&minus;
&int;
</p>
<p>V
</p>
<p>dV ρ(r). (17.8)
</p>
<p>If ε(r) is continuously differentiable, the product rule for differentiation gives
</p>
<p>ε(r)�Φ(r)+
(
grad ε(r)
</p>
<p>)(
gradΦ(r)
</p>
<p>)
=&minus;ρ(r) (17.9)
</p>
<p>which for constant ε simplifies to the Poisson equation
</p>
<p>�Φ(r)=&minus;ρ(r)
ε
</p>
<p>. (17.10)
</p>
<p>17.1.1 Homogeneous Dielectric Medium
</p>
<p>We begin with the simplest case of a dielectric medium with constant ε and solve
(17.10) numerically. We use a finite volume method (Sect. 11.3) which corresponds
to a finite element method with piecewise constant test functions. The integra-
tion volume is divided into small cubes Vijk which are centered at the grid points
(Fig. 17.1)
</p>
<p>rijk = (hi, hj,hk). (17.11)
Integration of (17.10) over the control volume Vijk around rijk gives
</p>
<p>&int;
</p>
<p>V
</p>
<p>dV div gradΦ =
∮
</p>
<p>&part;V
</p>
<p>gradΦ dA=&minus;1
ε
</p>
<p>&int;
</p>
<p>V
</p>
<p>dV ρ(r)=&minus;Qijk
ε
</p>
<p>. (17.12)
</p>
<p>Qijk is the total charge in the control volume. The flux integral is approximated by
(11.85)</p>
<p/>
</div>
<div class="page"><p/>
<p>17.1 Poisson Equation 307
</p>
<p>∮
</p>
<p>&part;V
</p>
<p>gradΦ dA=&minus;h2
(
&part;Φ
</p>
<p>&part;x
(xi+1/2, yi, zi)&minus;
</p>
<p>&part;Φ
</p>
<p>&part;x
(xi&minus;1/2, yi, zi)
</p>
<p>+ &part;Φ
&part;y
</p>
<p>(xiyi+1/2, zi)&minus;
&part;Φ
</p>
<p>&part;y
(xi, yi&minus;1/2, zi)
</p>
<p>+ &part;Φ
&part;z
</p>
<p>(xi, yi, zi+1/2)&minus;
&part;Φ
</p>
<p>&part;z
(xi, yi, zi&minus;1/2)
</p>
<p>)
. (17.13)
</p>
<p>The derivatives are approximated by symmetric differences
∮
</p>
<p>&part;V
</p>
<p>gradΦ dA=&minus;h
(
Φ(xi+1, yi, zi)&minus;Φ(xi, yi, zi)
</p>
<p>&minus;
(
Φ(xi, yi, zi)&minus;Φ(xi&minus;1, yi, zi)
</p>
<p>)
</p>
<p>+Φ(xi, yi+1, zi)&minus;Φ(xi, yi, zi)
&minus;
(
Φ(xi, yi, zi)&minus;Φ(xi, yi&minus;1, zi)
</p>
<p>)
</p>
<p>+Φ(xi, yi, zi+1)&minus;Φ(xi, yi, zi)
&minus;
(
Φ(xi, yi, zi)&minus;Φ(xi, yi, zi&minus;1)
</p>
<p>))
</p>
<p>=&minus;h
(
Φ(xi&minus;1, yi, zi)+Φ(xi+1, yi, zi)
</p>
<p>+Φ(xi, yi&minus;1, zi)+Φ(xi, yi+1, zi)
+Φ(xi, yi, zi&minus;1)+Φ(xi, yi, zi+1)
&minus; 6Φ(xi, yi, zi1)
</p>
<p>)
(17.14)
</p>
<p>which coincides with the simplest discretization of the second derivatives (3.40).
Finally we obtain the discretized Poisson equation in the more compact form
</p>
<p>6&sum;
</p>
<p>s=1
</p>
<p>(
Φ(rijk + drs)&minus;Φ(rijk)
</p>
<p>)
=&minus;Qijk
</p>
<p>εh
(17.15)
</p>
<p>which involves an average over the 6 neighboring cells
</p>
<p>dr1 = (&minus;h,0,0) . . . dr6 = (0,0, h). (17.16)
</p>
<p>17.1.2 Numerical Methods for the Poisson Equation
</p>
<p>Equation (17.15) is a system of linear equations with very large dimension (for a
grid with 100 &times; 100 &times; 100 points the dimension of the matrix is 106 &times; 106!). Our
computer experiments use the iterative method (5.5)
</p>
<p>Φnew(rijk)=
1
</p>
<p>6
</p>
<p>(&sum;
</p>
<p>s
</p>
<p>Φold(rijk + drs)+
Qijk
</p>
<p>εh
</p>
<p>)
. (17.17)
</p>
<p>Jacobi&rsquo;s method ((5.110) on page 74) makes all the changes in one step whereas
the Gauss-Seidel method ((5.113) on page 74) makes one change after the other. The</p>
<p/>
</div>
<div class="page"><p/>
<p>308 17 Electrostatics
</p>
<p>chessboard (or black red method) divides the grid into two subgrids (with i + j + k
even or odd) which are treated subsequently. The vector drs connects points of
different subgrids. Therefore it is not necessary to store intermediate values like for
the Gauss-Seidel method.
</p>
<p>Convergence can be improved with the method of successive over-relaxation
(SOR, (5.117) on page 75) using a mixture of old and new values
</p>
<p>Φnew(rijk)= (1 &minus;ω)Φold(rijk)+ω
1
</p>
<p>6
</p>
<p>(&sum;
</p>
<p>s
</p>
<p>Φold(rijk + drs)+
Qijk
</p>
<p>εh
</p>
<p>)
</p>
<p>(17.18)
</p>
<p>with the relaxation parameter ω. For 1 &lt;ω &lt; 2 convergence is faster than for ω= 1.
The optimum choice of ω for the Poisson problem in any dimension is discussed in
[279].
</p>
<p>Convergence can be further improved by multigrid methods [120, 283]. Error
components with short wavelengths are strongly damped during a few iterations
whereas it takes a very large number of iterations to remove the long wavelength
components. But here a coarser grid is sufficient and reduces computing time. After
a few iterations a first approximation Φ1 is obtained with the finite residual
</p>
<p>r1 =�Φ1 +
1
</p>
<p>ε
ρ. (17.19)
</p>
<p>Then more iterations on a coarser grid are made to find an approximate solution Φ2
of the equation
</p>
<p>�Φ =&minus;r1 =&minus;
1
</p>
<p>ε
ρ &minus;�Φ1. (17.20)
</p>
<p>The new residual is
</p>
<p>r2 =�Φ2 + r1. (17.21)
Function values of Φ2 on the finer grid are obtained by interpolation and finally the
sum Φ1 +Φ2 provides an improved approximation to the solution since
</p>
<p>�(Φ1 +Φ2)=&minus;
1
</p>
<p>ε
ρ + r1 + (r2 &minus; r1)=&minus;
</p>
<p>1
</p>
<p>ε
ρ + r2. (17.22)
</p>
<p>This method can be extended to a hierarchy of many grids.
Alternatively, the Poisson equation can be solved non-iteratively with pseu-
</p>
<p>dospectral methods [238, 247]. For instance, if the boundary is the surface of a cube,
eigenfunctions of the Laplacian are for homogeneous boundary conditions (Φ = 0)
given by
</p>
<p>Nk(r)= sin(kxx) sin(kyy) sin(kzz) (17.23)
</p>
<p>and for no-flow boundary conditions ( &part;
&part;n
Φ = 0) by
</p>
<p>Nk(r)= cos(kxx) cos(kyy) cos(kzz) (17.24)
which can be used as expansion functions for the potential</p>
<p/>
</div>
<div class="page"><p/>
<p>17.1 Poisson Equation 309
</p>
<p>Φ(r)=
&sum;
</p>
<p>kx ,ky ,kz
</p>
<p>ΦkNk(r). (17.25)
</p>
<p>Introducing collocation points rj the condition on the residual becomes
</p>
<p>0 =�Φ(rj )+
1
</p>
<p>ε
ρ(rj )=
</p>
<p>&sum;
</p>
<p>kx ,ky ,kz
</p>
<p>k2ΦkNk(rj )+
1
</p>
<p>ε
ρ(rj ) (17.26)
</p>
<p>which can be inverted with an inverse discrete sine transformation, (respectively an
inverse discrete cosine transformation for no-flux boundary conditions) to obtain the
Fourier components of the potential. Another discrete sine (or cosine) transforma-
tion gives the potential in real space.
</p>
<p>17.1.3 Charged Sphere
</p>
<p>As a simple example we consider a sphere of radius R with a homogeneous charge
density of
</p>
<p>ρ0 = e &middot;
3
</p>
<p>4πR3
. (17.27)
</p>
<p>The exact potential is given by
</p>
<p>Φ(r)= e
4πε0R
</p>
<p>+ e
8πε0R
</p>
<p>(
1 &minus; r
</p>
<p>2
</p>
<p>R2
</p>
<p>)
for r &lt; R
</p>
<p>Φ(r)= e
4πε0r
</p>
<p>for r &gt; R.
(17.28)
</p>
<p>The charge density (17.27) is discontinuous at the surface of the sphere. Inte-
gration over a control volume smears out this discontinuity which affects the po-
tential values around the boundary (Fig. 17.2). Alternatively we could assign the
value ρ(rijk) which is either ρ0 (17.27) or zero to each control volume which re-
tains a sharp transition but changes the shape of the boundary surface and does not
conserve the total charge. This approach was discussed in the first edition of this
book in connection with a finite differences method. The most precise but also com-
plicated method divides the control volumes at the boundary into two irregularly
shaped parts [188, 189].
</p>
<p>Initial guess as well as boundary values are taken from
</p>
<p>Φ0(r)=
e
</p>
<p>4πε0 max(r, h)
(17.29)
</p>
<p>which provides proper boundary values but is far from the final solution inside the
sphere. The interaction energy is given by (Sect. 17.5)
</p>
<p>Eint =
1
</p>
<p>2
</p>
<p>&int;
</p>
<p>V
</p>
<p>ρ(r)Φ(r) dV = 3
20
</p>
<p>e2
</p>
<p>πε0R
. (17.30)
</p>
<p>Calculated potential (Fig. 17.3) and interaction energy (Figs. 17.4, 17.5) converge
rapidly. The optimum relaxation parameter is around ω&asymp; 1.9.</p>
<p/>
</div>
<div class="page"><p/>
<p>310 17 Electrostatics
</p>
<p>Fig. 17.2 (Discretization of the discontinuous charge density) Left: the most precise method di-
vides the control volumes at the boundary into two irregularly shaped parts. Middle: assigning
either the value ρ0 or zero retains the discontinuity but changes the shape of the boundary. Right:
averaging over a control volume smears out the discontinuous transition
</p>
<p>Fig. 17.3 (Electrostatic
potential of a charged sphere)
A charged sphere is simulated
with radius R = 0.25 &Aring; and a
homogeneous charge density
ρ = e &middot; 3/4πR3. The grid
consists of 2003 points with a
spacing of h= 0.025 &Aring;. The
calculated potential (circles)
is compared to the exact
solution ((17.28), solid
curve), the initial guess is
shown by the dashed line
</p>
<p>Fig. 17.4 (Influence of the
relaxation parameter) The
convergence of the interaction
energy ((17.30), which has a
value of 34.56 eV for this
example) is studied as a
function of the relaxation
parameter ω. The optimum
value is around ω&asymp; 1.9. For
ω &gt; 2 there is no
convergence. The dashed line
shows the exact value</p>
<p/>
</div>
<div class="page"><p/>
<p>17.1 Poisson Equation 311
</p>
<p>Fig. 17.5 (Influence of grid
size) The convergence of the
interaction energy (17.30)
and the central potential value
are studied as a function of
grid size. The dashed lines
show the exact values
</p>
<p>17.1.4 Variable ε
</p>
<p>In the framework of the finite volume method we take the average over a control
volume to discretize ε2 and Φ
</p>
<p>εijk = ε(rijk)=
1
</p>
<p>h3
</p>
<p>&int;
</p>
<p>Vijk
</p>
<p>dV ε(r) (17.31)
</p>
<p>Φijk =Φ(rijk)=
1
</p>
<p>h3
</p>
<p>&int;
</p>
<p>Vijk
</p>
<p>dV Φ(r). (17.32)
</p>
<p>Integration of (17.7) gives
&int;
</p>
<p>V
</p>
<p>dV div
(
ε(r)gradΦ(r)
</p>
<p>)
=
∮
</p>
<p>&part;V
</p>
<p>ε(r)gradΦ dA=&minus;
&int;
</p>
<p>V
</p>
<p>dV ρ(r)=&minus;Qijk.
</p>
<p>(17.33)
</p>
<p>The surface integral is
∮
</p>
<p>&part;V
</p>
<p>dA ε gradΦ =
&sum;
</p>
<p>s&isin;faces
</p>
<p>&int;
</p>
<p>As
</p>
<p>dAε(r)
&part;
</p>
<p>&part;n
Φ. (17.34)
</p>
<p>Applying the midpoint rule (11.77) we find (Fig. 17.6)
</p>
<p>∮
</p>
<p>&part;V
</p>
<p>dAε gradΦ &asymp; h2
6&sum;
</p>
<p>r=1
ε
</p>
<p>(
rijk +
</p>
<p>1
</p>
<p>2
drs
</p>
<p>)
&part;
</p>
<p>&part;n
Φ
</p>
<p>(
rijk +
</p>
<p>1
</p>
<p>2
drs
</p>
<p>)
. (17.35)
</p>
<p>The potential Φ as well as the product ε(r) &part;Φ
&part;n
</p>
<p>are continuous, therefore we make
the approximation [188]
</p>
<p>2But see Sect. 17.1.5 for the case of discontinuous ε.</p>
<p/>
</div>
<div class="page"><p/>
<p>312 17 Electrostatics
</p>
<p>Fig. 17.6 Face center of the
control volume
</p>
<p>ε
</p>
<p>(
rijk +
</p>
<p>1
</p>
<p>2
drs
</p>
<p>)
&part;Φ
</p>
<p>&part;n
</p>
<p>(
rijk +
</p>
<p>1
</p>
<p>2
drs
</p>
<p>)
</p>
<p>= ε(rijk)
Φ(rijk + 12drs)&minus;Φ(rijk)
</p>
<p>h
2
</p>
<p>= ε(rijk + drs)
Φ(rijk + drs)&minus;Φ(rijk + 12drs)
</p>
<p>h
2
</p>
<p>. (17.36)
</p>
<p>From this equation the unknown potential value on the face of the control volume
Φ(rijk + 12drs) (Fig. 17.6) can be calculated
</p>
<p>Φ
</p>
<p>(
rijk +
</p>
<p>1
</p>
<p>2
drs
</p>
<p>)
= ε(rijk)Φ(rijk)+ ε(rijk + drs)Φ(rijk + drs)
</p>
<p>ε(rijk)+ ε(rijk + drs)
(17.37)
</p>
<p>which gives
</p>
<p>ε
</p>
<p>(
rijk +
</p>
<p>1
</p>
<p>2
drs
</p>
<p>)
&part;
</p>
<p>&part;n
Φ
</p>
<p>(
rijk +
</p>
<p>1
</p>
<p>2
drs
</p>
<p>)
</p>
<p>= 2ε(rijk)ε(rijk + drs)
ε(rijk)+ ε(rijk + drs)
</p>
<p>Φ(rijk + drs)&minus;Φ(rijk)
h
</p>
<p>. (17.38)
</p>
<p>Finally we obtain the discretized equation
</p>
<p>&minus;Qijk = h
6&sum;
</p>
<p>s=1
</p>
<p>2ε(rijk + drs)ε(rijk)
ε(rijk + drs)+ ε(rijk)
</p>
<p>(
Φ(rijk + drs)&minus;Φ(rijk)
</p>
<p>)
(17.39)
</p>
<p>which can be solved iteratively according to
</p>
<p>Φnew(rijk)=
&sum; 2ε(rijk+drs)ε(rijk)
</p>
<p>ε(rijk+drs )+ε(rijk)Φ
old(rijk + drs)+ Qijkh
</p>
<p>&sum; 2ε(rijk+drs )ε(rijk)
ε(rijk+drs )+ε(rijk)
</p>
<p>. (17.40)</p>
<p/>
</div>
<div class="page"><p/>
<p>17.1 Poisson Equation 313
</p>
<p>Fig. 17.7 (Transition of ε)
The discontinuous ε(r) (black
line) is averaged over the
control volumes to obtain the
discretized values εijk (full
circles). Equation (17.40)
takes the harmonic average
over two neighbor cells (open
circles) and replaces the
discontinuity by a smooth
transition over a distance of
about h
</p>
<p>Fig. 17.8 Average of ε over
a control volume
</p>
<p>17.1.5 Discontinuous ε
</p>
<p>For practical applications models are often used with piecewise constant ε. A simple
example is the solvation of a charged molecule in a dielectric medium (Fig. 17.9).
Here ε = ε0 within the molecule and ε = ε0ε1 within the medium. At the boundary
ε is discontinuous. In (17.40) the discontinuity is replaced by a smooth transition
between the two values of ε (Fig. 17.7).
</p>
<p>If the discontinuity of ε is inside a control volume Vijk then (17.31) takes the
arithmetic average
</p>
<p>εijk = V (1)ijk ε1 + V
(2)
ijk ε2 (17.41)
</p>
<p>which corresponds to the parallel connection of two capacities (Fig. 17.8). Depend-
ing on geometry, a serial connection may be more appropriate which corresponds to
the weighted harmonic average
</p>
<p>εijk =
1
</p>
<p>V
(1)
ijk ε
</p>
<p>&minus;1
1 + V
</p>
<p>(2)
ijk ε
</p>
<p>&minus;1
2
</p>
<p>. (17.42)</p>
<p/>
</div>
<div class="page"><p/>
<p>314 17 Electrostatics
</p>
<p>Fig. 17.9 (Solvation of a
charged sphere in a dielectric
medium) Charge density and
dielectric constant are
discontinuous at the surface
of the sphere
</p>
<p>17.1.6 Solvation Energy of a Charged Sphere
</p>
<p>We consider again a charged sphere, which is now embedded in a dielectric medium
(Fig. 17.9) with relative dielectric constant ε1.
</p>
<p>For a spherically symmetrical problem (17.7) can be solved by application of
Gauss&rsquo;s theorem
</p>
<p>4πr2ε(r)
dΦ
</p>
<p>dr
=&minus;4π
</p>
<p>&int; r
</p>
<p>0
ρ
(
r &prime;
)
r &prime;2 dr &prime; =&minus;q(r) (17.43)
</p>
<p>Φ(r)=&minus;
&int; r
</p>
<p>0
</p>
<p>q(r)
</p>
<p>4πr2ε(r)
+Φ(0). (17.44)
</p>
<p>For the charged sphere we find
</p>
<p>q(r)=
{
Qr3/R3 for r &lt; R
Q for r &gt; R
</p>
<p>(17.45)
</p>
<p>Φ(r)=&minus; Q
4πε0R3
</p>
<p>r2
</p>
<p>2
+Φ(0) for r &lt; R (17.46)
</p>
<p>Φ(r)=&minus; Q
8πε0R
</p>
<p>+Φ(0)+ Q
4πε0ε1
</p>
<p>(
1
</p>
<p>r
&minus; 1
</p>
<p>R
</p>
<p>)
for r &gt; R. (17.47)
</p>
<p>The constant Φ(0) is chosen to give vanishing potential at infinity
</p>
<p>Φ(0)= Q
4πε0ε1R
</p>
<p>+ Q
8πε0R
</p>
<p>. (17.48)
</p>
<p>The interaction energy is
</p>
<p>Eint =
1
</p>
<p>2
</p>
<p>&int; R
</p>
<p>0
4πr2dr ρΦ(r)= Q
</p>
<p>2(5 + ε1)
40πε0ε1R
</p>
<p>. (17.49)
</p>
<p>Numerical results for ε1 = 4 are shown in Fig. 17.10.
</p>
<p>17.1.7 The Shifted Grid Method
</p>
<p>An alternative approach uses a different grid (Fig. 17.11) for ε which is shifted by
h/2 in all directions [44] or, more generally, a dual grid (11.74),
</p>
<p>εijk = ε(ri+1/2,j+1/2,k+1/2). (17.50)</p>
<p/>
</div>
<div class="page"><p/>
<p>17.2 Poisson-Boltzmann Equation 315
</p>
<p>Fig. 17.10 (Charged sphere
in a dielectric medium)
Numerical results for ε1 = 4
outside the sphere and 2003
</p>
<p>grid points (circles) are
compared to the exact
solution ((17.46), (17.47),
solid curves)
</p>
<p>The value of ε has to be averaged over four neighboring cells to obtain the dis-
cretized equation
</p>
<p>&minus;Qijk
h2
</p>
<p>=
&sum;
</p>
<p>s
</p>
<p>ε(rijk + drs)
&part;Φ
</p>
<p>&part;n
(rijk + drs)
</p>
<p>= Φi,j,k+1 &minus;Φi,j,k
h
</p>
<p>εijk + εi,j&minus;1,k + εi&minus;1,j,k + εi&minus;1,j&minus;1,k
4
</p>
<p>+ Φi,j,k&minus;1 &minus;Φi,j,k
h
</p>
<p>εijk&minus;1 + εi,j&minus;1,k&minus;1 + εi&minus;1,j,k&minus;1 + εi&minus;1,j&minus;1,k&minus;1
4
</p>
<p>+ Φi+1,j,k &minus;Φi,j,k
h
</p>
<p>εijk + εi,j&minus;1,k + εi,j,k&minus;1 + εi,j&minus;1,k&minus;1
4
</p>
<p>+ Φi&minus;1,j,k &minus;Φi,j,k
h
</p>
<p>εi&minus;1jk + εi&minus;1,j&minus;1,k + εi&minus;1,j,k&minus;1 + εi&minus;1,j&minus;1,k&minus;1
4
</p>
<p>+ Φi,j+1,k &minus;Φi,j,k
h
</p>
<p>εijk + εi&minus;1,j,k + εi,j,k&minus;1 + εi&minus;1,j,k&minus;1
4
</p>
<p>+ Φi,j&minus;1,k &minus;Φi,j,k
h
</p>
<p>εij&minus;1k + εi&minus;1,j&minus;1,k + εi,j&minus;1,k&minus;1 + εi&minus;1,j&minus;1,k&minus;1
4
</p>
<p>.
</p>
<p>(17.51)
</p>
<p>The shifted grid method is especially useful if ε changes at planar interfaces. Nu-
merical results of several methods are compared in Fig. 17.12.
</p>
<p>17.2 Poisson-Boltzmann Equation
</p>
<p>Electrostatic interactions are very important in molecular physics. Bio-molecules
are usually embedded in an environment which is polarizable and contains mobile
charges (Na+,K+,Mg++,Cl&minus; &middot; &middot; &middot;).
</p>
<p>We divide the charge density formally into a fixed and a mobile part
</p>
<p>ρ(r)= ρfix(r)+ ρmobile(r). (17.52)</p>
<p/>
</div>
<div class="page"><p/>
<p>316 17 Electrostatics
</p>
<p>Fig. 17.11 (Shifted grid
method) A different grid is
used for the discretization of
ε which is shifted by h/2 in
all directions
</p>
<p>Fig. 17.12 (Comparison of
numerical errors) The
Coulomb interaction of a
charged sphere is calculated
with several methods for 1003
</p>
<p>grid points. Circles: ((17.40),
ε averaged), diamonds:
((17.40), ε&minus;1 averaged),
squares: ((17.51),
ε averaged), triangles:
((17.51), ε&minus;1 averaged), solid
curve: analytical solution
(17.49)
</p>
<p>The fixed part represents, for instance, the charge distribution of a protein
molecule which, neglecting polarization effects, is a given quantity and provides
the inhomogeneity of the equation. The mobile part, on the other hand, represents
the sum of all mobile charges (e is the elementary charge and Zi the charge number
of ion species i)
</p>
<p>ρmobile(r)=
&sum;
</p>
<p>i
</p>
<p>Zieni(r) (17.53)
</p>
<p>which move around until an equilibrium is reached which is determined by the mu-
tual interaction of the ions. The famous Debye-H&uuml;ckel [69] and Gouy-Chapman
models [56, 110] assume that the electrostatic interaction
</p>
<p>U(r)= ZieΦ(r) (17.54)
is dominant and the density of the ions ni is given by a Boltzmann-distribution
</p>
<p>ni(r)= n(0)i e
&minus;ZieΦ(r)/kBT . (17.55)</p>
<p/>
</div>
<div class="page"><p/>
<p>17.2 Poisson-Boltzmann Equation 317
</p>
<p>The potential Φ(r) has to be calculated in a self consistent way together with the
density of mobile charges. The charge density of the free ions is
</p>
<p>ρmobile(r)=
&sum;
</p>
<p>i
</p>
<p>n
(0)
i eZie
</p>
<p>&minus;ZieΦ/kBT (17.56)
</p>
<p>and the Poisson equation (17.7) turns into the Poisson-Boltzmann equation [91]
</p>
<p>div
(
ε(r)gradΦ(r)
</p>
<p>)
+
&sum;
</p>
<p>i
</p>
<p>n
(0)
i eZie
</p>
<p>&minus;ZieΦ/kBT =&minus;ρfix(r). (17.57)
</p>
<p>17.2.1 Linearization of the Poisson-Boltzmann Equation
</p>
<p>For small ion concentrations the exponential can be expanded
</p>
<p>e&minus;ZieΦ/kT &asymp; 1 &minus; ZieΦ
kBT
</p>
<p>+ 1
2
</p>
<p>(
ZieΦ
</p>
<p>kBT
</p>
<p>)2
+ &middot; &middot; &middot; . (17.58)
</p>
<p>For a neutral system
&sum;
</p>
<p>i
</p>
<p>n
(0)
i Zie= 0 (17.59)
</p>
<p>and the linearized Poisson-Boltzmann equation is obtained:
</p>
<p>div
(
ε(r)gradΦ(r)
</p>
<p>)
&minus;
&sum;
</p>
<p>i
</p>
<p>n
(0)
i
</p>
<p>Z2i e
2
</p>
<p>kBT
Φ(r)=&minus;ρfix. (17.60)
</p>
<p>With
</p>
<p>ε(r)= ε0εr(r) (17.61)
</p>
<p>and the definition
</p>
<p>κ(r)2 = e
2
</p>
<p>ε0εr(r)kBT
</p>
<p>&sum;
n
(0)
i Z
</p>
<p>2
i (17.62)
</p>
<p>we have finally
</p>
<p>div
(
εr(r)gradΦ(r)
</p>
<p>)
&minus; εrκ2Φ =&minus;
</p>
<p>1
</p>
<p>ε0
ρ. (17.63)
</p>
<p>For a charged sphere with radius a embedded in a homogeneous medium the solu-
tion of (17.63) is given by
</p>
<p>Φ = A
r
e&minus;κr A= e
</p>
<p>4πε0εr
</p>
<p>eκa
</p>
<p>1 + κa . (17.64)
</p>
<p>The potential is shielded by the ions. Its range is of the order λDebye = 1/κ (the
so-called Debye length).</p>
<p/>
</div>
<div class="page"><p/>
<p>318 17 Electrostatics
</p>
<p>Fig. 17.13 Cavity in a
dielectric medium
</p>
<p>17.2.2 Discretization of the Linearized Poisson-Boltzmann
</p>
<p>Equation
</p>
<p>To solve (17.63) the discrete equation (17.39) is generalized to [181]
&sum;
</p>
<p>s
</p>
<p>2εr(rijk)εr(rijk + drs)
εr(rijk)+ εr (rijk + drs)
</p>
<p>(
Φ(rijk + drs)&minus;Φ(rijk)
</p>
<p>)
</p>
<p>&minus; εr(rijk)κ2(rijk)h2Φ(rijk)=&minus;
Qijk
</p>
<p>hε0
. (17.65)
</p>
<p>If ε is constant then we have to iterate
</p>
<p>Φnew(rijk)=
Qijk
hε0εr
</p>
<p>+
&sum;
</p>
<p>s Φ
old(rijk + drs)
</p>
<p>6 + h2κ2(rijk)
. (17.66)
</p>
<p>17.3 Boundary Element Method for the Poisson Equation
</p>
<p>Often continuum models are used to describe the solvation of a subsystem which is
treated with a high accuracy method. The polarization of the surrounding solvent or
protein is described by its dielectric constant ε and the subsystem is placed inside a
cavity with ε = ε0 (Fig. 17.13). Instead of solving the Poisson equation for a large
solvent volume another kind of method is often used which replaces the polarization
of the medium by a distribution of charges over the boundary surface.
</p>
<p>In the following we consider model systems which are composed of two spatial
regions:
</p>
<p>&bull; the outer region is filled with a dielectric medium (ε1) and contains no free
charges
</p>
<p>&bull; the inner region (&ldquo;Cavity&rdquo;) contains a charge distribution ρ(r) and its dielectric
constant is ε = ε0.
</p>
<p>17.3.1 Integral Equations for the Potential
</p>
<p>Starting from the Poisson equation</p>
<p/>
</div>
<div class="page"><p/>
<p>17.3 Boundary Element Method for the Poisson Equation 319
</p>
<p>div
(
ε(r)gradΦ(r)
</p>
<p>)
=&minus;ρ(r) (17.67)
</p>
<p>we will derive some useful integral equations in the following. First we apply
Gauss&rsquo;s theorem to the expression [277]
</p>
<p>div
[
G
(
r&minus; r&prime;
</p>
<p>)
ε(r)grad
</p>
<p>(
Φ(r)
</p>
<p>)
&minus;Φ(r)ε(r)grad
</p>
<p>(
G
(
r&minus; r&prime;
</p>
<p>))]
</p>
<p>=&minus;ρ(r)G
(
r&minus; r&prime;
</p>
<p>)
&minus;Φ(r)ε(r)div grad
</p>
<p>(
G
(
r&minus; r&prime;
</p>
<p>))
</p>
<p>&minus;Φ(r)grad ε(r)grad
(
G
(
r&minus; r&prime;
</p>
<p>))
(17.68)
</p>
<p>with the yet undetermined function G(r&minus; r&prime;). Integration over a volume V gives
</p>
<p>&minus;
&int;
</p>
<p>V
</p>
<p>dV
(
ρ(r)G
</p>
<p>(
r&minus; r&prime;
</p>
<p>)
+Φ(r)ε(r)div grad
</p>
<p>(
G
(
r&minus; r&prime;
</p>
<p>))
</p>
<p>+Φ(r)grad ε(r)grad
(
G
(
r&minus; r&prime;
</p>
<p>)))
</p>
<p>=
∮
</p>
<p>&part;V
</p>
<p>dA
</p>
<p>(
G
(
r&minus; r&prime;
</p>
<p>)
ε(r)
</p>
<p>&part;
</p>
<p>&part;n
</p>
<p>(
Φ(r)
</p>
<p>)
&minus;Φ(r)ε(r) &part;
</p>
<p>&part;n
</p>
<p>(
G
(
r&minus; r&prime;
</p>
<p>)))
.
</p>
<p>(17.69)
</p>
<p>Now choose G as the fundamental solution of the Poisson equation
</p>
<p>G0
(
r&minus; r&prime;
</p>
<p>)
=&minus; 1
</p>
<p>4π |r&minus; r&prime;| (17.70)
</p>
<p>which obeys
</p>
<p>div gradG0 = δ
(
r&minus; r&prime;
</p>
<p>)
(17.71)
</p>
<p>to obtain the following integral equation for the potential:
</p>
<p>Φ
(
r&prime;
)
ε(r)=
</p>
<p>&int;
</p>
<p>V
</p>
<p>dV
ρ(r)
</p>
<p>4π |r&minus; r&prime;| +
1
</p>
<p>4π
</p>
<p>&int;
</p>
<p>V
</p>
<p>dV Φ(r)grad ε(r)grad
</p>
<p>(
1
</p>
<p>|r&minus; r&prime;|
</p>
<p>)
</p>
<p>&minus; 1
4π
</p>
<p>∮
</p>
<p>&part;V
</p>
<p>dA
</p>
<p>(
1
</p>
<p>|r&minus; r&prime;|ε(r)
&part;
</p>
<p>&part;n
</p>
<p>(
Φ(r)
</p>
<p>)
+Φ(r)ε(r) &part;
</p>
<p>&part;n
</p>
<p>(
1
</p>
<p>|r&minus; r&prime;|
</p>
<p>))
.
</p>
<p>(17.72)
</p>
<p>First consider as the integration volume a sphere with increasing radius. Then the
surface integral vanishes for infinite radius (Φ &rarr; 0 at large distances) [277].
</p>
<p>The gradient of ε(r) is nonzero only on the boundary surface Fig. 17.14 of the
cavity and with the limiting procedure (d &rarr; 0)
</p>
<p>grad ε(r) dV = nε1 &minus; 1
d
</p>
<p>ε0 dV = dAn(ε1 &minus; 1)ε0
</p>
<p>we obtain
</p>
<p>Φ
(
r&prime;
)
= 1
</p>
<p>ε(r&prime;)
</p>
<p>&int;
</p>
<p>cav
</p>
<p>dV
ρ(r)
</p>
<p>4π |r&minus; r&prime;|
</p>
<p>+ (ε1 &minus; 1)ε0
4πε(r&prime;)
</p>
<p>∮
</p>
<p>S
</p>
<p>dAΦ(r)
&part;
</p>
<p>&part;n
</p>
<p>1
</p>
<p>|r&minus; r&prime;| . (17.73)</p>
<p/>
</div>
<div class="page"><p/>
<p>320 17 Electrostatics
</p>
<p>Fig. 17.14 Discontinuity at
the cavity boundary
</p>
<p>This equation allows to calculate the potential inside and outside the cavity from
the given charge density and the potential at the boundary.
</p>
<p>Next we apply (17.72) to the cavity volume (where ε = ε0) and obtain
</p>
<p>Φin
(
r&prime;
)
=
&int;
</p>
<p>V
</p>
<p>dV
ρ(r)
</p>
<p>4π |r&minus; r&prime;|ε0
</p>
<p>&minus; 1
4π
</p>
<p>∮
</p>
<p>S
</p>
<p>dA
</p>
<p>(
Φin(r)
</p>
<p>&part;
</p>
<p>&part;n
</p>
<p>1
</p>
<p>|r&minus; r&prime;| &minus;
1
</p>
<p>|r&minus; r&prime;|
&part;
</p>
<p>&part;n
Φin(r)
</p>
<p>)
. (17.74)
</p>
<p>From comparison with (17.73) we have
∮
</p>
<p>S
</p>
<p>dA
1
</p>
<p>|r&minus; r&prime;|
&part;
</p>
<p>&part;n
Φin(r)= ε1
</p>
<p>∮
</p>
<p>S
</p>
<p>dAΦin(r)
&part;
</p>
<p>&part;n
</p>
<p>1
</p>
<p>|r&minus; r&prime;|
and the potential can be alternatively calculated from the values of its normal gradi-
ent at the boundary
</p>
<p>Φ
(
r&prime;
)
= 1
</p>
<p>ε(r&prime;)
</p>
<p>&int;
</p>
<p>cav
</p>
<p>dV
ρ(r)
</p>
<p>4π |r&minus; r&prime;| +
(1 &minus; 1
</p>
<p>ε1
)ε0
</p>
<p>4πε(r&prime;)
</p>
<p>∮
</p>
<p>S
</p>
<p>dA
1
</p>
<p>|r&minus; r&prime;|
&part;
</p>
<p>&part;n
Φin(r).
</p>
<p>(17.75)
</p>
<p>This equation can be interpreted as the potential generated by the charge density ρ
plus an additional surface charge density
</p>
<p>σ(r)=
(
</p>
<p>1 &minus; 1
ε1
</p>
<p>)
ε0
</p>
<p>&part;
</p>
<p>&part;n
Φin(r). (17.76)
</p>
<p>Integration over the volume outside the cavity (where ε = ε1ε0) gives the following
expression for the potential:
</p>
<p>Φout
(
r&prime;
)
= 1
</p>
<p>4π
</p>
<p>∮
</p>
<p>S
</p>
<p>dA
</p>
<p>(
Φout(r)
</p>
<p>&part;
</p>
<p>&part;n
</p>
<p>1
</p>
<p>|r&minus; r&prime;| &minus;
1
</p>
<p>|r&minus; r&prime;|
&part;
</p>
<p>&part;n
Φout(r)
</p>
<p>)
. (17.77)
</p>
<p>At the boundary the potential is continuous
</p>
<p>Φout(r)=Φin(r) r &isin;A (17.78)
</p>
<p>whereas the normal derivative (hence the normal component of the electric field)
has a discontinuity
</p>
<p>ε1
&part;Φout
</p>
<p>&part;n
= &part;Φin
</p>
<p>&part;n
. (17.79)</p>
<p/>
</div>
<div class="page"><p/>
<p>17.3 Boundary Element Method for the Poisson Equation 321
</p>
<p>Fig. 17.15 Representation of
the boundary by surface
elements
</p>
<p>17.3.2 Calculation of the Boundary Potential
</p>
<p>For a numerical treatment the boundary surface is approximated by a finite set of
small surface elements Si , i = 1 . . .N centered at ri with an area Ai and normal
vector ni (Fig. 17.15). (We assume planar elements in the following, the curvature
leads to higher order corrections.)
</p>
<p>The corresponding values of the potential and its normal derivative are denoted
as Φi = Φ(ri) and &part;Φi&part;n = ni gradΦ(ri). At a point r
</p>
<p>&plusmn;
j close to the element Sj we
</p>
<p>obtain the following approximate equations:
</p>
<p>Φin
(
r&minus;j
</p>
<p>)
=
</p>
<p>&int;
</p>
<p>V
</p>
<p>dV
ρ(r)
</p>
<p>4π |r&minus; r&minus;j |ε0
</p>
<p>&minus; 1
4π
</p>
<p>&sum;
</p>
<p>i
</p>
<p>Φi
</p>
<p>∮
</p>
<p>Si
</p>
<p>dA
&part;
</p>
<p>&part;n
</p>
<p>1
</p>
<p>|r&minus; r&minus;j |
+ 1
</p>
<p>4π
</p>
<p>&sum;
</p>
<p>i
</p>
<p>&part;Φi,in
</p>
<p>&part;n
</p>
<p>∮
</p>
<p>Si
</p>
<p>dA
1
</p>
<p>|r&minus; r&minus;j |
(17.80)
</p>
<p>Φout
(
r+j
</p>
<p>)
= 1
</p>
<p>4π
</p>
<p>&sum;
</p>
<p>i
</p>
<p>Φi
</p>
<p>∮
</p>
<p>Si
</p>
<p>dA
&part;
</p>
<p>&part;n
</p>
<p>1
</p>
<p>|r&minus; r+j |
&minus; 1
</p>
<p>4π
</p>
<p>&sum;
</p>
<p>i
</p>
<p>&part;Φi,out
</p>
<p>&part;n
</p>
<p>∮
</p>
<p>Si
</p>
<p>dA
1
</p>
<p>|r&minus; r+j |
.
</p>
<p>(17.81)
</p>
<p>These two equations can be combined to obtain a system of equations for the
potential values only. To that end we approach the boundary symmetrically with
r&plusmn;i = ri &plusmn; dni . Under this circumstance∮
</p>
<p>Si
</p>
<p>dA
1
</p>
<p>|r&minus; r+j |
=
∮
</p>
<p>Si
</p>
<p>dA
1
</p>
<p>|r&minus; r&minus;j |∮
</p>
<p>Si
</p>
<p>dA
&part;
</p>
<p>&part;n
</p>
<p>1
</p>
<p>|r&minus; r+i |
= &minus;
</p>
<p>∮
</p>
<p>Si
</p>
<p>dA
&part;
</p>
<p>&part;n
</p>
<p>1
</p>
<p>|r&minus; r&minus;i |∮
</p>
<p>Si
</p>
<p>dA
&part;
</p>
<p>&part;n
</p>
<p>1
</p>
<p>|r&minus; r+j |
=
∮
</p>
<p>Si
</p>
<p>dA
&part;
</p>
<p>&part;n
</p>
<p>1
</p>
<p>|r&minus; r&minus;j |
j �= i
</p>
<p>(17.82)
</p>
<p>and we find
</p>
<p>(1 + ε1)Φj =
&int;
</p>
<p>V
</p>
<p>dV
ρ(r)
</p>
<p>4πε0|r&minus; rj |
</p>
<p>&minus; 1
4π
</p>
<p>&sum;
</p>
<p>i �=j
(1 &minus; ε1)Φi
</p>
<p>∮
</p>
<p>Si
</p>
<p>dA
&part;
</p>
<p>&part;n
</p>
<p>1
</p>
<p>|r&minus; r&minus;j |</p>
<p/>
</div>
<div class="page"><p/>
<p>322 17 Electrostatics
</p>
<p>Fig. 17.16 Projection of the
surface element
</p>
<p>&minus; 1
4π
</p>
<p>(1 + ε1)Φj
∮
</p>
<p>Sj
</p>
<p>dA
&part;
</p>
<p>&part;n
</p>
<p>1
</p>
<p>|r&minus; r&minus;j |
. (17.83)
</p>
<p>The integrals for i �= j can be approximated by
∮
</p>
<p>Si
</p>
<p>dA
&part;
</p>
<p>&part;n
</p>
<p>1
</p>
<p>|r&minus; r&minus;j |
=Ai ni gradi
</p>
<p>1
</p>
<p>|ri &minus; rj |
. (17.84)
</p>
<p>The second integral has a simple geometrical interpretation (Fig. 17.16).
Since grad 1|r&minus;r &prime;| =&minus;
</p>
<p>1
|r&minus;r &prime;|2
</p>
<p>r&minus;r &prime;
|r&minus;r &prime;| the area element dA is projected onto a sphere
</p>
<p>with unit radius. The integral
∮
Sj
dAgradr&minus;
</p>
<p>1
|rj&minus;r&minus;j |
</p>
<p>is given by the solid angle of Sj
</p>
<p>with respect to r &prime;. For r &prime; &rarr; rj from inside this is just minus half of the full space
angle of 4π . Thus we have
</p>
<p>(1 + ε1)Φj =
&int;
</p>
<p>V
</p>
<p>dV
ρ(r)
</p>
<p>4π |r&minus; rj |ε0
</p>
<p>&minus; 1
4π
</p>
<p>&sum;
</p>
<p>i �=j
(1 &minus; ε1)ΦiAi
</p>
<p>&part;
</p>
<p>&part;ni
</p>
<p>1
</p>
<p>|ri &minus; rj |
+ 1
</p>
<p>2
(1 + ε1)Φj
</p>
<p>(17.85)
</p>
<p>or
</p>
<p>Φj =
2
</p>
<p>1 + ε1
</p>
<p>&int;
</p>
<p>V
</p>
<p>dV
ρ(r)
</p>
<p>4πε0|r&minus; rj |
+ 1
</p>
<p>2π
</p>
<p>&sum;
</p>
<p>i �=j
</p>
<p>ε1 &minus; 1
ε1 + 1
</p>
<p>ΦiAi
&part;
</p>
<p>&part;ni
</p>
<p>1
</p>
<p>|ri &minus; rj |
.
</p>
<p>(17.86)
</p>
<p>This system of equations can be used to calculate the potential on the boundary. The
potential inside the cavity is then given by (17.73). Numerical stability is improved
by a related method which considers the potential gradient along the boundary. Tak-
ing the normal derivative
</p>
<p>&part;
</p>
<p>&part;nj
= nj gradrj&plusmn; (17.87)
</p>
<p>of (17.80), (17.81) gives</p>
<p/>
</div>
<div class="page"><p/>
<p>17.3 Boundary Element Method for the Poisson Equation 323
</p>
<p>&part;
</p>
<p>&part;nj
Φin
</p>
<p>(
r&minus;j
</p>
<p>)
= &part;
</p>
<p>&part;nj
</p>
<p>&int;
</p>
<p>V
</p>
<p>dV
ρ(r)
</p>
<p>4π |r&minus; r&minus;j |ε0
</p>
<p>&minus; 1
4π
</p>
<p>&sum;
</p>
<p>i
</p>
<p>Φi
</p>
<p>∮
</p>
<p>Si
</p>
<p>dA
&part;2
</p>
<p>&part;n&part;nj
</p>
<p>1
</p>
<p>|r&minus; r&minus;j |
</p>
<p>+ 1
4π
</p>
<p>&sum;
</p>
<p>i
</p>
<p>&part;Φi,in
</p>
<p>&part;n
</p>
<p>∮
</p>
<p>Si
</p>
<p>dA
&part;
</p>
<p>&part;nj
</p>
<p>1
</p>
<p>|r&minus; r&minus;j |
(17.88)
</p>
<p>&part;
</p>
<p>&part;nj
Φout
</p>
<p>(
r+j
</p>
<p>)
= 1
</p>
<p>4π
</p>
<p>&sum;
</p>
<p>i
</p>
<p>Φi
</p>
<p>∮
</p>
<p>Si
</p>
<p>dA
&part;2
</p>
<p>&part;n&part;nj
</p>
<p>1
</p>
<p>|r&minus; r+j |
</p>
<p>&minus; 1
4π
</p>
<p>&sum;
</p>
<p>i
</p>
<p>&part;Φi,out
</p>
<p>&part;n
</p>
<p>∮
</p>
<p>Si
</p>
<p>dA
&part;
</p>
<p>&part;nj
</p>
<p>1
</p>
<p>|r&minus; r+j |
. (17.89)
</p>
<p>In addition to (17.82) we have now
</p>
<p>∮
</p>
<p>Si
</p>
<p>dA
&part;2
</p>
<p>&part;n&part;nj
</p>
<p>1
</p>
<p>|r&minus; r&minus;j |
=
∮
</p>
<p>Si
</p>
<p>dA
&part;2
</p>
<p>&part;n&part;nj
</p>
<p>1
</p>
<p>|r&minus; r+j |
(17.90)
</p>
<p>and the sum of the two equations gives
(
</p>
<p>1 + 1
ε1
</p>
<p>)
&part;
</p>
<p>&part;nj
Φin,j
</p>
<p>= &part;
&part;nj
</p>
<p>(&int;
</p>
<p>V
</p>
<p>dV
ρ(r)
</p>
<p>4πε0|r&minus; rj |
+
</p>
<p>1 &minus; 1
ε1
</p>
<p>4π
</p>
<p>&sum;
</p>
<p>i �=j
Ai
</p>
<p>&part;Φi,in
</p>
<p>&part;n
</p>
<p>1
</p>
<p>|ri &minus; rj |
</p>
<p>)
</p>
<p>+
1 + 1
</p>
<p>ε1
</p>
<p>2π
</p>
<p>&part;Φj,in
</p>
<p>&part;n
(17.91)
</p>
<p>or finally
</p>
<p>&part;
</p>
<p>&part;nj
Φin,j =
</p>
<p>2ε1
ε1 + 1
</p>
<p>&part;
</p>
<p>&part;nj
</p>
<p>&int;
</p>
<p>V
</p>
<p>dV
ρ(r)
</p>
<p>4πε0|r&minus; rj |
</p>
<p>+ 2ε1 &minus; 1
ε1 + 1
</p>
<p>&sum;
</p>
<p>i �=j
Ai
</p>
<p>&part;Φi,in
</p>
<p>&part;n
</p>
<p>&part;
</p>
<p>&part;nj
</p>
<p>1
</p>
<p>|ri &minus; rj |
. (17.92)
</p>
<p>In terms of the surface charge density this reads:
</p>
<p>σ &prime;j = 2ε0
(1 &minus; ε1)
(1 + ε1)
</p>
<p>(
&minus;nj grad
</p>
<p>&int;
dV
</p>
<p>ρ(r)
</p>
<p>4πε0|r&minus; r&prime;|
+ 1
</p>
<p>4πε0
</p>
<p>&sum;
</p>
<p>i �=j
σ &prime;iAi
</p>
<p>nj (rj &minus; ri)
|ri &minus; rj |3
</p>
<p>)
.
</p>
<p>(17.93)
</p>
<p>This system of linear equations can be solved directly or iteratively (a simple damp-
ing scheme σ &prime;m &rarr; ωσ &prime;m+(1&minus;ω)σ &prime;m,old with ω&asymp; 0.6 helps to get rid of oscillations).
From the surface charges σiAi the potential is obtained with the help of (17.75).</p>
<p/>
</div>
<div class="page"><p/>
<p>324 17 Electrostatics
</p>
<p>17.4 Boundary Element Method for the Linearized
</p>
<p>Poisson-Boltzmann Equation
</p>
<p>We consider now a cavity within an electrolyte. The fundamental solution of the
linear Poisson-Boltzmann equation (17.63)
</p>
<p>Gκ
(
r&minus; r&prime;
</p>
<p>)
=&minus; e
</p>
<p>&minus;κ|r&minus;r&prime;|
</p>
<p>4π |r&minus; r&prime;| (17.94)
</p>
<p>obeys
</p>
<p>div gradGκ
(
r&minus; r&prime;
</p>
<p>)
&minus; κ2Gκ
</p>
<p>(
r&minus; r&prime;
</p>
<p>)
= δ
</p>
<p>(
r&minus; r&prime;
</p>
<p>)
. (17.95)
</p>
<p>Inserting into Green&rsquo;s theorem (17.69) we obtain the potential outside the cavity
</p>
<p>Φout
(
r&prime;
)
=&minus;
</p>
<p>∮
</p>
<p>S
</p>
<p>dA
</p>
<p>(
Φout(r)
</p>
<p>&part;
</p>
<p>&part;n
Gκ
</p>
<p>(
r&minus; r&prime;
</p>
<p>)
&minus;Gκ
</p>
<p>(
r&minus; r&prime;
</p>
<p>) &part;
&part;n
</p>
<p>Φout(r)
</p>
<p>)
</p>
<p>(17.96)
</p>
<p>which can be combined with (17.74), (17.79) to give the following equations [32]
</p>
<p>(1 + ε1)Φ
(
r&prime;
)
=
</p>
<p>∮
</p>
<p>S
</p>
<p>dA
</p>
<p>[
Φ(r)
</p>
<p>&part;
</p>
<p>&part;n
(G0 &minus; ε1Gκ)&minus; (G0 &minus;Gκ)
</p>
<p>&part;
</p>
<p>&part;n
Φin(r)
</p>
<p>]
</p>
<p>+
&int;
</p>
<p>cav
</p>
<p>ρ(r)
</p>
<p>4πε0|r&minus; r&prime;|
dV (17.97)
</p>
<p>(1 + ε1)
&part;
</p>
<p>&part;n&prime;
Φin
</p>
<p>(
r&prime;
)
=
</p>
<p>∮
</p>
<p>S
</p>
<p>dAΦ(r)
&part;2
</p>
<p>&part;n&part;n&prime;
(G0 &minus;Gκ)
</p>
<p>&minus;
∮
</p>
<p>S
</p>
<p>dA
&part;
</p>
<p>&part;n
Φin(r)
</p>
<p>&part;
</p>
<p>&part;n&prime;
</p>
<p>(
G0 &minus;
</p>
<p>1
</p>
<p>ε1
Gk
</p>
<p>)
</p>
<p>+ &part;
&part;n&prime;
</p>
<p>&int;
</p>
<p>cav
</p>
<p>ρ(r)
</p>
<p>4πε|r&minus; r&prime;| dV. (17.98)
</p>
<p>For a set of discrete boundary elements the following equations determine the values
of the potential and its normal derivative at the boundary:
</p>
<p>1 + ε1
2
</p>
<p>Φj =
&sum;
</p>
<p>i �=j
Φi
</p>
<p>∮
dA
</p>
<p>&part;
</p>
<p>&part;n
(G0 &minus; ε1Gκ)&minus;
</p>
<p>&sum;
</p>
<p>i �=j
</p>
<p>&part;
</p>
<p>&part;n
Φi,in
</p>
<p>∮
dA(G0 &minus;Gκ)
</p>
<p>+
&int;
</p>
<p>ρ(r)
</p>
<p>4πε0|r&minus; ri |
dV (17.99)
</p>
<p>1 + ε1
2
</p>
<p>&part;
</p>
<p>&part;n&prime;
Φi,in =
</p>
<p>&sum;
</p>
<p>i �=j
Φi
</p>
<p>∮
dA
</p>
<p>&part;2
</p>
<p>&part;n&part;n&prime;
(G0 &minus;Gκ )
</p>
<p>&minus;
&sum;
</p>
<p>i �=j
</p>
<p>&part;
</p>
<p>&part;n
Φi,in
</p>
<p>∮
dA
</p>
<p>&part;
</p>
<p>&part;n&prime;
</p>
<p>(
G0 &minus;
</p>
<p>1
</p>
<p>ε1
Gk
</p>
<p>)
</p>
<p>+ &part;
&part;n&prime;
</p>
<p>&int;
ρ(r)
</p>
<p>4πε|r&minus; ri |
dV. (17.100)</p>
<p/>
</div>
<div class="page"><p/>
<p>17.5 Electrostatic Interaction Energy (Onsager Model) 325
</p>
<p>The situation is much more involved than for the simpler Poisson equation (with
κ = 0) since the calculation of many integrals including such with singularities is
necessary [32, 143].
</p>
<p>17.5 Electrostatic Interaction Energy (Onsager Model)
</p>
<p>A very important quantity in molecular physics is the electrostatic interaction of a
molecule and the surrounding solvent [11, 237]. We calculate it by taking a small
part of the charge distribution from infinite distance (Φ(r &rarr; &infin;) = 0) into the
cavity. The charge distribution thereby changes from λρ(r) to (λ + dλ)ρ(r) with
0 &le; λ&le; 1. The corresponding energy change is
</p>
<p>dE =
&int;
</p>
<p>dλ &middot; ρ(r)Φλ(r) dV
</p>
<p>=
&int;
</p>
<p>dλ &middot; ρ(r)
(&sum;
</p>
<p>n
</p>
<p>σn(λ)An
</p>
<p>4πε0|r &minus; rn|
+
&int;
</p>
<p>λρ(r &prime;)
</p>
<p>4πε0|r &minus; r &prime;|
dV &prime;
</p>
<p>)
dV.
</p>
<p>(17.101)
</p>
<p>Multiplication of (17.93) by a factor of λ shows that the surface charges λσn are
the solution corresponding to the charge density λρ(r). It follows that σn(λ)= λσn
and hence
</p>
<p>dE = λdλ
&int;
</p>
<p>ρ(r)
</p>
<p>(&sum;
</p>
<p>n
</p>
<p>σnAn
</p>
<p>4πε0|r &minus; rn|
+ ρ(r
</p>
<p>&prime;)
</p>
<p>4πε0|r &minus; r &prime;|
dV &prime;
</p>
<p>)
. (17.102)
</p>
<p>The second summand is the self energy of the charge distribution which does not
depend on the medium. The first summand vanishes without a polarizable medium
and gives the interaction energy. Hence we have the final expression
</p>
<p>Eint =
&int;
</p>
<p>dE =
&int; 1
</p>
<p>0
λdλ
</p>
<p>&int;
ρ(r)
</p>
<p>&sum;
</p>
<p>n
</p>
<p>σnAn
</p>
<p>4πε0|r &minus; rn|
dV
</p>
<p>=
&sum;
</p>
<p>n
</p>
<p>σnAn
</p>
<p>&int;
ρ(r)
</p>
<p>8πε0|r &minus; rn|
dV. (17.103)
</p>
<p>For the special case of a spherical cavity with radius a an analytical solution by
a multipole expansion is available [148]
</p>
<p>Eint =&minus;
1
</p>
<p>8πε0
</p>
<p>&sum;
</p>
<p>l
</p>
<p>l&sum;
</p>
<p>m=&minus;l
</p>
<p>(l + 1)(ε1 &minus; 1)
[l + ε1(l + 1)]a2l+1
</p>
<p>Mml M
m
l (17.104)
</p>
<p>with the multipole moments
</p>
<p>Mml =
&int;
</p>
<p>ρ(r, θ,ϕ)
</p>
<p>&radic;
4π
</p>
<p>2l + 1 r
lYml (θ,ϕ) dV . (17.105)
</p>
<p>The first two terms of this series are:</p>
<p/>
</div>
<div class="page"><p/>
<p>326 17 Electrostatics
</p>
<p>Fig. 17.17 Surface charges
</p>
<p>E
(0)
int =&minus;
</p>
<p>1
</p>
<p>8πε0
</p>
<p>ε1 &minus; 1
ε1a
</p>
<p>M00M
0
0 =&minus;
</p>
<p>1
</p>
<p>8πε0
</p>
<p>(
1 &minus; 1
</p>
<p>ε1
</p>
<p>)
Q2
</p>
<p>a
(17.106)
</p>
<p>E
(1)
int =&minus;
</p>
<p>1
</p>
<p>8πε0
</p>
<p>2(ε1 &minus; 1)
(1 + 2ε1)a3
</p>
<p>(
M&minus;11 M
</p>
<p>&minus;1
1 +M
</p>
<p>0
1M
</p>
<p>0
1 +M11M11
</p>
<p>)
</p>
<p>=&minus; 1
8πε0
</p>
<p>2(ε1 &minus; 1)
1 + 2ε1
</p>
<p>μ2
</p>
<p>a3
. (17.107)
</p>
<p>17.5.1 Example: Point Charge in a Spherical Cavity
</p>
<p>Consider a point charge Q in the center of a spherical cavity of radius R (Fig. 17.17).
The dielectric constant is given by
</p>
<p>ε =
{
ε0 r &lt; R
</p>
<p>ε1ε0 r &gt; R.
(17.108)
</p>
<p>Electric field and potential are inside the cavity
</p>
<p>E = Q
4πε0r2
</p>
<p>Φ = Q
4πε0r
</p>
<p>+ Q
4πε0R
</p>
<p>(
1
</p>
<p>ε1
&minus; 1
</p>
<p>)
(17.109)
</p>
<p>and outside
</p>
<p>E = Q
4πε1ε0r2
</p>
<p>Φ = Q
4πε1ε0r
</p>
<p>r &gt; R (17.110)
</p>
<p>which in terms of the surface charge density σ is
</p>
<p>E = Q+ 4πR
2σ
</p>
<p>4πε0r2
r &gt; R (17.111)
</p>
<p>with the total surface charge
</p>
<p>4πR2σ =Q
(
</p>
<p>1
</p>
<p>ε1
&minus; 1
</p>
<p>)
. (17.112)
</p>
<p>The solvation energy (17.103) is given by
</p>
<p>Eint =
Q2
</p>
<p>8πε0
</p>
<p>(
1
</p>
<p>ε1
&minus; 1
</p>
<p>)
(17.113)</p>
<p/>
</div>
<div class="page"><p/>
<p>17.6 Problems 327
</p>
<p>Fig. 17.18 (Solvation energy with the boundary element method) A spherical cavity is simulated
with radius a = 1 &Aring; which contains a point charge in its center. The solvation energy is calculated
with 25 &times; 25 (circles) and 50 &times; 50 (squares) surface elements of equal size. The exact expression
(17.106) is shown by the solid curve
</p>
<p>which is the first term (17.106) of the multipole expansion. Figure 17.18 shows
numerical results.
</p>
<p>17.6 Problems
</p>
<p>Problem 17.1 (Linearized Poisson-Boltzmann equation) This computer experi-
ment simulates a homogeneously charged sphere in a dielectric medium (Fig. 17.19).
The electrostatic potential is calculated from the linearized Poisson-Boltzmann
equation (17.65) on a cubic grid of up to 1003 points. The potential Φ(x) is shown
along a line through the center together with a log-log plot of the maximum change
per iteration
</p>
<p>∣∣Φ(n+1)(r)&minus;Φ(n)(r)
∣∣ (17.114)
</p>
<p>as a measure of convergence.
Explore the dependence of convergence on
</p>
<p>&bull; the initial values which can be chosen either Φ(r) = 0 or from the analytical
solution
</p>
<p>Φ(r)=
{
</p>
<p>Q
8πεε0a
</p>
<p>2+ε(1+κa)
1+κa &minus;
</p>
<p>Q
</p>
<p>8πε0a3
r2 for r &lt; a
</p>
<p>Qe&minus;κ(r&minus;a)
4πε0ε(κa+1)r for r &gt; a
</p>
<p>(17.115)
</p>
<p>&bull; the relaxation parameter ω for different combinations of ε and κ
&bull; the resolution of the grid</p>
<p/>
</div>
<div class="page"><p/>
<p>328 17 Electrostatics
</p>
<p>Fig. 17.19 Charged sphere
in a dielectric medium
</p>
<p>Fig. 17.20 Point charge
inside a spherical cavity
</p>
<p>Problem 17.2 (Boundary element method) In this computer experiment the solva-
tion energy of a point charge within a spherical cavity (Fig. 17.20) is calculated with
the boundary element method (17.93).
</p>
<p>The calculated solvation energy is compared to the analytical value from (17.104)
</p>
<p>Esolv =
Q2
</p>
<p>8πε0R
</p>
<p>&infin;&sum;
</p>
<p>n=1
</p>
<p>s2n
</p>
<p>R2n
</p>
<p>(ε1 &minus; ε2)(n+ 1)
nε1 + (n+ 1)ε2
</p>
<p>(17.116)
</p>
<p>where R is the cavity radius and s is the distance of the charge from the center of
the cavity.
</p>
<p>Explore the dependence of accuracy and convergence on
</p>
<p>&bull; the damping parameter ω
&bull; the number of surface elements (6 &times; 6 &middot; &middot; &middot;42&times; 42) which can be chosen either as
dφ dθ or dφ d cos θ (equal areas)
</p>
<p>&bull; the position of the charge</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 18
</p>
<p>Waves
</p>
<p>Waves are oscillations that move in space and time and are able to transport en-
ergy from one point to another. Quantum mechanical wavefunctions are discussed
in Chap. 21. In this chapter we simulate classical waves which are, for instance,
important in acoustics and electrodynamics. We use the method of finite differences
to discretize the wave equation in one spatial dimension
</p>
<p>&part;2
</p>
<p>&part;t2
f (t, x)= c2 &part;
</p>
<p>2
</p>
<p>&part;x2
f (t, x). (18.1)
</p>
<p>Numerical solutions are obtained by an eigenvector expansion using trigonometric
functions or by time integration. Accuracy and stability of different methods are
compared. The wave function is second order in time and can be integrated directly
with a two-step method. Alternatively, it can be converted into a first order system
of equations of double dimension. Here, the velocity appears explicitly and veloc-
ity dependent damping can be taken into account. Finally, the second order wave
equation can be replaced by two coupled first order equations for two variables (like
velocity and density in case of acoustic waves), which can be solved by quite gen-
eral methods. We compare the leapfrog, Lax-Wendroff and Crank-Nicolson meth-
ods. Only the Crank-Nicolson method is stable for Courant numbers α &gt; 1. It is an
implicit method and can be solved iteratively. In a series of computer experiments
we simulate waves on a string. We study reflection at an open or fixed boundary and
at the interface between two different media. We compare dispersion and damping
for different methods.
</p>
<p>18.1 Classical Waves
</p>
<p>In classical physics there are two main types of waves:
</p>
<p>Electromagnetic waves do not require a medium. They are oscillations of the electro-
magnetic field and propagate also in vacuum. As an example consider a plane wave
</p>
<p>P.O.J. Scherer, Computational Physics, Graduate Texts in Physics,
DOI 10.1007/978-3-319-00401-3_18,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>329</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-00401-3_18">http://dx.doi.org/10.1007/978-3-319-00401-3_18</a></div>
</div>
<div class="page"><p/>
<p>330 18 Waves
</p>
<p>Fig. 18.1 Electromagnetic
wave
</p>
<p>which propagates in x-direction and is linearly polarized (Fig. 18.1). The electric
and magnetic field have the form
</p>
<p>E=
</p>
<p>⎛
⎝
</p>
<p>0
Ey(x, t)
</p>
<p>0
</p>
<p>⎞
⎠ B=
</p>
<p>⎛
⎝
</p>
<p>0
0
</p>
<p>Bz(x, t)
</p>
<p>⎞
⎠ . (18.2)
</p>
<p>Maxwell&rsquo;s equations read in the absence of charges and currents
</p>
<p>divE= divB= 0, rotE=&minus;&part;B
&part;t
</p>
<p>, rotB= μ0ε0
&part;E
</p>
<p>&part;t
. (18.3)
</p>
<p>The fields (18.2) have zero divergence and satisfy the first two equations. Applica-
tion of the third and fourth equation gives
</p>
<p>&part;Ey
</p>
<p>&part;x
=&minus;&part;Bz
</p>
<p>&part;t
&minus; &part;Bz
</p>
<p>&part;x
= μ0ε0
</p>
<p>&part;Ey
</p>
<p>&part;t
(18.4)
</p>
<p>which can be combined to a one-dimensional wave equation
</p>
<p>&part;2Ey
</p>
<p>&part;t2
= c2 &part;
</p>
<p>2Ey
</p>
<p>&part;x2
(18.5)
</p>
<p>with velocity c= (μ0ε0)&minus;1/2.
</p>
<p>Mechanical waves propagate through an elastic medium like air, water or an elastic
solid. The material is subject to external forces deforming it and elastic forces which
try to restore the deformation. As a result the atoms or molecules move around their
equilibrium positions. As an example consider one-dimensional acoustic waves in
an organ pipe (Fig. 18.2):
</p>
<p>A mass element
</p>
<p>dm= ̺ dV = ̺Adx (18.6)
at position x experiences an external force due to the air pressure which, accord-
ing to Newton&rsquo;s law changes the velocity v of the element as described by Euler&rsquo;s
equation1
</p>
<p>1We consider only small deviations from the equilibrium values ̺0,p0, v0 = 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>18.1 Classical Waves 331
</p>
<p>Fig. 18.2 (Acoustic waves in one dimension) A mass element dm = ̺Adx at position x expe-
riences a total force F = F(x) + F(x + dx) = &minus;A &part;p
</p>
<p>&part;x
dx. Due to the conservation of mass the
</p>
<p>change of the density &part;̺
&part;t
</p>
<p>is given by the net flux J = J (x)&minus; J (x + dx)=&minus;̺0A &part;v&part;x dx
</p>
<p>̺0
&part;
</p>
<p>&part;t
v =&minus;&part;p
</p>
<p>&part;x
. (18.7)
</p>
<p>The pressure is a function of the density
</p>
<p>p
</p>
<p>p0
=
(
̺
</p>
<p>̺0
</p>
<p>)n (dp
d̺
</p>
<p>)
</p>
<p>0
= np0
</p>
<p>̺0
= c2 (18.8)
</p>
<p>where n= 1 for an isothermal ideal gas and n&asymp; 1.4 for air under adiabatic condi-
tions (no heat exchange), therefore
</p>
<p>̺0
&part;
</p>
<p>&part;t
v =&minus;c2 &part;̺
</p>
<p>&part;x
. (18.9)
</p>
<p>From the conservation of mass the continuity equation (11.10) follows
</p>
<p>&part;
</p>
<p>&part;t
̺=&minus;̺0
</p>
<p>&part;
</p>
<p>&part;x
v. (18.10)
</p>
<p>Combining the time derivative of (18.10) and the spatial derivative of (18.9) we
obtain again the one-dimensional wave equation
</p>
<p>&part;2
</p>
<p>&part;t2
̺= c2 &part;
</p>
<p>2
</p>
<p>&part;x2
̺. (18.11)
</p>
<p>The wave equation can be factorized as
(
&part;
</p>
<p>&part;t
+ c &part;
</p>
<p>&part;x
</p>
<p>)(
&part;
</p>
<p>&part;t
&minus; c &part;
</p>
<p>&part;x
</p>
<p>)
̺=
</p>
<p>(
&part;
</p>
<p>&part;t
&minus; c &part;
</p>
<p>&part;x
</p>
<p>)(
&part;
</p>
<p>&part;t
+ c &part;
</p>
<p>&part;x
</p>
<p>)
̺= 0 (18.12)
</p>
<p>which shows that solutions of the advection equation
(
&part;
</p>
<p>&part;t
&plusmn; c &part;
</p>
<p>&part;x
</p>
<p>)
̺= 0 (18.13)
</p>
<p>are also solutions of the wave equation, which have the form
</p>
<p>̺= f (x &plusmn; ct). (18.14)
In fact a general solution of the wave equation is given according to d&rsquo;Alembert
</p>
<p>as the sum of two waves running to the left and right side with velocity c and a
constant envelope (Fig. 18.3)</p>
<p/>
</div>
<div class="page"><p/>
<p>332 18 Waves
</p>
<p>Fig. 18.3 d&rsquo;Alembert
solution to the wave equation
</p>
<p>̺= f1(x + ct)+ f2(x &minus; ct). (18.15)
A special solution of this kind is the plane wave solution
</p>
<p>f (x, t)= eiωt&plusmn;ikx
</p>
<p>with the dispersion relation
</p>
<p>ω= ck. (18.16)
</p>
<p>18.2 Spatial Discretization in One Dimension
</p>
<p>We use the simplest finite difference expression for the spatial derivative (Sects. 3.4,
11.2)
</p>
<p>&part;2
</p>
<p>&part;x2
f (x, t)= f (t, x +�x)+ f (t, x &minus;�x)&minus; 2f (t, x)
</p>
<p>�x2
+O
</p>
<p>(
�x2
</p>
<p>)
(18.17)
</p>
<p>and a regular grid
</p>
<p>xm =m�x m= 1,2 . . .M (18.18)
fm = f (xm). (18.19)
</p>
<p>This turns the wave equation into the system of ordinary differential equations
(Sect. 11.2.3)
</p>
<p>d2
</p>
<p>dt2
fm = c2
</p>
<p>fm+1 + fm&minus;1 &minus; 2fm
�x2
</p>
<p>(18.20)
</p>
<p>where f0 and fM+1 have to be specified by suitable boundary conditions (Fig. 18.4).
In matrix notation we have
</p>
<p>f(t)=
</p>
<p>⎛
⎜⎝
</p>
<p>f1(t)
...
</p>
<p>fM(t)
</p>
<p>⎞
⎟⎠ (18.21)
</p>
<p>d2
</p>
<p>dt2
f(t)=Af(t)+ S(t) (18.22)
</p>
<p>where for</p>
<p/>
</div>
<div class="page"><p/>
<p>18.2 Spatial Discretization in One Dimension 333
</p>
<p>Fig. 18.4 (Boundary conditions for 1-dimensional waves) Additional boundary points
x0, xM+1 are used to realize the boundary conditions: (a) fixed boundaries: f (x0) = 0,
&part;2
</p>
<p>&part;x2
f (x1) = 1�x2 (f (x2) &minus; 2f (x1)) or f (xM+1) = 0,
</p>
<p>&part;2
</p>
<p>&part;x2
f (xM ) = 1�x2 (f (xM&minus;1) &minus; 2f (xM )),
</p>
<p>(b) periodic boundary conditions: x0 &equiv; xM , &part;
2
</p>
<p>&part;x2
f (x1) = 1�x2 (f (x2) + f (xM ) &minus; 2f (x1)),
</p>
<p>xM+1 &equiv; x1, &part;
2
</p>
<p>&part;x2
f (xM ) = 1�x2 (f (xM&minus;1) + f (x1) &minus; 2f (xNM )), (c) open bound-
</p>
<p>aries: &part;
&part;x
f (x1) = f (x2)&minus;f (x0)2�x = 0,
</p>
<p>&part;2
</p>
<p>&part;x2
f (x1) = 1�x2 (2f (x2) &minus; 2f (x1)) or
</p>
<p>&part;
&part;x
f (xM ) =
</p>
<p>f (xM+1)&minus;f (xM&minus;1)
2�x = 0,
</p>
<p>&part;2
</p>
<p>&part;x2
f (xM ) = 1�x2 (2f (xM&minus;1) &minus; 2f (xM )), (d) moving boundaries:
</p>
<p>f (x0, t) = ξ0(t), &part;
2
</p>
<p>&part;x2
f (x1) = 1�x2 (f (x2) &minus; 2f (x1) + ξ0(t)) or f (xM+1, t) = ξM+1(t),
</p>
<p>&part;2
</p>
<p>&part;x2
f (xM )= 1�x2 (f (xM&minus;1)&minus; 2f (xM )+ ξN+1(t))
</p>
<p>&bull; fixed boundaries
</p>
<p>A=
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>&minus;2 1
1 &minus;2 1
</p>
<p>1 &minus;2 1
. . .
</p>
<p>. . .
. . .
</p>
<p>1 &minus;2 1
1 &minus;2
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>c2
</p>
<p>�x2
S(t)= 0 (18.23)</p>
<p/>
</div>
<div class="page"><p/>
<p>334 18 Waves
</p>
<p>&bull; periodic boundaries2
</p>
<p>A=
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>&minus;2 1 1
1 &minus;2 1
</p>
<p>1 &minus;2 1
. . .
</p>
<p>. . .
. . .
</p>
<p>1 &minus;2 1
1 1 &minus;2
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>c2
</p>
<p>�x2
S(t)= 0 (18.24)
</p>
<p>&bull; open boundaries
</p>
<p>A=
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>&minus;2 2
1 &minus;2 1
</p>
<p>1 &minus;2 1
. . .
</p>
<p>. . .
. . .
</p>
<p>1 &minus;2 1
2 &minus;2
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>c2
</p>
<p>�x2
S(t)= 0 (18.25)
</p>
<p>&bull; moving boundaries
</p>
<p>A=
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>&minus;2 1
1 &minus;2 1
</p>
<p>1 &minus;2 1
. . .
</p>
<p>. . .
. . .
</p>
<p>1 &minus;2 1
1 &minus;2
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>c2
</p>
<p>�x2
S(t)=
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>ξ0(t)
</p>
<p>0
...
...
</p>
<p>0
ξN+1(t)
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>.
</p>
<p>(18.26)
</p>
<p>A combination of different boundary conditions for both sides is possible.
Equation (18.20) corresponds to a series of mass points which are connected
</p>
<p>by harmonic springs (Fig. 18.5), a model, which is used in solid state physics to
describe longitudinal acoustic waves [129].
</p>
<p>18.3 Solution by an Eigenvector Expansion
</p>
<p>For fixed boundaries (18.20) reads in matrix form
</p>
<p>d2
</p>
<p>dt2
f(t)=Af(t) (18.27)
</p>
<p>with the vector of function values:
</p>
<p>2This corresponds to the boundary condition f0 = f2, &part;&part;x f (x1) = 0. Alternatively we could use
f0 = f1, &part;&part;x f (x1/2)= 0 which replaces the 2 s in the first and last row by 1 s.</p>
<p/>
</div>
<div class="page"><p/>
<p>18.3 Solution by an Eigenvector Expansion 335
</p>
<p>Fig. 18.5 (Atomistic model for longitudinal waves) A set of mass points m is connected by
springs with stiffness K . The elongation of mass point number j from its equilibrium position
xj = j�x is ξj . The equations of motion mξ̈j = &minus;K(ξj &minus; ξj&minus;1)&minus;K(ξj &minus; ξj+1) coincide with
(18.20) with a velocity of c=�x
</p>
<p>&radic;
k�x
m
</p>
<p>f(t)=
</p>
<p>⎛
⎜⎝
</p>
<p>f1(t)
...
</p>
<p>fM(t)
</p>
<p>⎞
⎟⎠ (18.28)
</p>
<p>and the matrix
</p>
<p>A=
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>&minus;2 1
1 &minus;2 1
</p>
<p>1 &minus;2 1
. . .
</p>
<p>. . .
. . .
</p>
<p>1 &minus;2 1
1 &minus;2
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>c2
</p>
<p>�x2
(18.29)
</p>
<p>which can be diagonalized exactly (Sect. 9.3). The two boundary points f (0) = 0
and f ((M + 1)�x)= 0 can be added without any changes. The eigenvalues are
</p>
<p>λ= 2 c
2
</p>
<p>�x2
</p>
<p>(
cos(k�x)&minus; 1
</p>
<p>)
=&minus; 4c
</p>
<p>2
</p>
<p>�x2
sin2
</p>
<p>(
k�x
</p>
<p>2
</p>
<p>)
= (iωk)2
</p>
<p>k�x = πl
(M + 1) , l = 1 . . .M (18.30)
</p>
<p>with the frequencies
</p>
<p>ωk =
2c
</p>
<p>�x
sin
</p>
<p>(
k�x
</p>
<p>2
</p>
<p>)
. (18.31)
</p>
<p>This result deviates from the dispersion relation of the continuous wave equation
(18.11) ωk = ck and approximates it only for k�x ≪ 1 (Fig. 18.6).
</p>
<p>The general solution has the form (Sect. 11.2.4)
</p>
<p>fn(t)=
M&sum;
</p>
<p>l=1
</p>
<p>(
Cl+e
</p>
<p>iωl t +Cl&minus;e&minus;iωl t
)
</p>
<p>sin
</p>
<p>(
m
</p>
<p>πl
</p>
<p>(M + 1)
</p>
<p>)
. (18.32)</p>
<p/>
</div>
<div class="page"><p/>
<p>336 18 Waves
</p>
<p>Fig. 18.6 Dispersion of the
discrete wave equation
</p>
<p>The initial amplitudes and velocities are
</p>
<p>fn(t = 0)=
M&sum;
</p>
<p>l=1
(Cl+ +Cl&minus;) sin
</p>
<p>(
m
</p>
<p>πl
</p>
<p>(M + 1)
</p>
<p>)
= Fm
</p>
<p>d
</p>
<p>dt
fm(t = 0, xm)=
</p>
<p>M&sum;
</p>
<p>l=1
iωl(Cl+ &minus;Cl&minus;) sin
</p>
<p>(
m
</p>
<p>πl
</p>
<p>(M + 1)
</p>
<p>)
=Gm
</p>
<p>(18.33)
</p>
<p>with Fm and Gm given. Different eigenfunctions of a tridiagonal matrix are mutually
orthogonal
</p>
<p>M&sum;
</p>
<p>m=1
sin
</p>
<p>(
m
</p>
<p>πl
</p>
<p>M + 1
</p>
<p>)
sin
</p>
<p>(
m
</p>
<p>πl&prime;
</p>
<p>M + 1
</p>
<p>)
= M
</p>
<p>2
δl,l&prime; (18.34)
</p>
<p>and the coefficients Cl&plusmn; follow from a discrete Fourier transformation:
</p>
<p>F̃l =
1
</p>
<p>M
</p>
<p>M&sum;
</p>
<p>m=1
sin
</p>
<p>(
m
</p>
<p>πl
</p>
<p>N + 1
</p>
<p>)
Fm
</p>
<p>= 1
M
</p>
<p>M&sum;
</p>
<p>m=1
</p>
<p>M&sum;
</p>
<p>l&prime;=1
(Cl&prime;+ +Cl&prime;&minus;) sin
</p>
<p>(
m
</p>
<p>πl&prime;
</p>
<p>M + 1
</p>
<p>)
sin
</p>
<p>(
m
</p>
<p>πl
</p>
<p>M + 1
</p>
<p>)
</p>
<p>= 1
2
(Cl+ +Cl&minus;) (18.35)
</p>
<p>G̃l =
1
</p>
<p>M
</p>
<p>M&sum;
</p>
<p>m=1
sin
</p>
<p>(
m
</p>
<p>πl
</p>
<p>N + 1
</p>
<p>)
Gn
</p>
<p>= 1
M
</p>
<p>M&sum;
</p>
<p>m=1
</p>
<p>NM&sum;
</p>
<p>l&prime;=1
iωl(Cl+ &minus;Cl&minus;) sin
</p>
<p>(
m
</p>
<p>πl&prime;
</p>
<p>M + 1
</p>
<p>)
sin
</p>
<p>(
m
</p>
<p>πl
</p>
<p>M + 1
</p>
<p>)
</p>
<p>= 1
2
iωl(Cl+ &minus;Cl&minus;) (18.36)</p>
<p/>
</div>
<div class="page"><p/>
<p>18.4 Discretization of Space and Time 337
</p>
<p>Cl+ = F̃l +
1
</p>
<p>iωl
G̃l
</p>
<p>Cl&minus; = F̃l &minus;
1
</p>
<p>iωl
G̃l .
</p>
<p>(18.37)
</p>
<p>Finally the explicit solution of the wave equation is
</p>
<p>fm(t)=
M&sum;
</p>
<p>l=1
2
</p>
<p>(
F̃l cos(ωl t)+
</p>
<p>G̃l
</p>
<p>ωl
sin(ωl t)
</p>
<p>)
sin
</p>
<p>(
m
</p>
<p>πl
</p>
<p>M + 1
</p>
<p>)
. (18.38)
</p>
<p>Periodic or open boundaries can be treated similarly as the matrices can be diag-
onalized exactly (Sect. 9.3). For moving boundaries the expansion coefficients are
time dependent (Sect. 11.2.4).
</p>
<p>18.4 Discretization of Space and Time
</p>
<p>Using the finite difference expression also for the second time derivative the fully
discretized wave equation is
</p>
<p>f (t +�t,x)+ f (t &minus;�t,x)&minus; 2f (t, x)
�t2
</p>
<p>= c2 f (t, x +�x)+ f (t, x &minus;�x)&minus; 2f (t, x)
�x2
</p>
<p>+O
(
�x2,�t2
</p>
<p>)
. (18.39)
</p>
<p>For a plane wave
</p>
<p>f = ei(ωt&minus;kx) (18.40)
</p>
<p>we find
</p>
<p>eiω�t + e&minus;iω�t &minus; 2 = c2 �t
2
</p>
<p>�x2
</p>
<p>(
eik�x + e&minus;ik�x &minus; 2
</p>
<p>)
(18.41)
</p>
<p>which can be written as
</p>
<p>sin
ω�t
</p>
<p>2
= α sin k�x
</p>
<p>2
(18.42)
</p>
<p>with the so-called Courant-number [64]
</p>
<p>α = c �t
�x
</p>
<p>. (18.43)
</p>
<p>From (18.42) we see that the dispersion relation is linear only for α = 1. For
α �= 1 not all values of ω and k allowed (Fig. 18.7).</p>
<p/>
</div>
<div class="page"><p/>
<p>338 18 Waves
</p>
<p>Fig. 18.7 (Dispersion of the
discrete wave equation) Only
for α = 1 or for small values
of k�x and ω�t is the
dispersion approximately
linear. For α &lt; 1 only
frequencies
ω &lt; ωmax = 2 arcsin(α)/�t
are allowed whereas for
α &gt; 1 the range of k-values is
bounded by
kmax = 2 arcsin(1/α)/�x
</p>
<p>18.5 Numerical Integration with a Two-Step Method
</p>
<p>We solve the discrete wave equation (18.39) with fixed or open boundaries for
</p>
<p>f (t +�t,x)= 2f (t, x)
(
1 &minus; α2
</p>
<p>)
+ α2
</p>
<p>(
f (t, x +�x)+ f (t, x &minus;�x)
</p>
<p>)
</p>
<p>&minus; f (t &minus;�t,x)+O
(
�t2,�x2
</p>
<p>)
(18.44)
</p>
<p>on the regular grids
</p>
<p>xm =m�x m= 1,2 . . .M (18.45)
tn = n�t n= 1,2 . . .N (18.46)
</p>
<p>fn =
</p>
<p>⎛
⎜⎝
</p>
<p>f n1
...
</p>
<p>f nM
</p>
<p>⎞
⎟⎠=
</p>
<p>⎛
⎜⎝
</p>
<p>f (tn, x1)
...
</p>
<p>f (tn, xM)
</p>
<p>⎞
⎟⎠ (18.47)
</p>
<p>by applying the iteration
</p>
<p>f n+1m = 2
(
1 &minus; α2
</p>
<p>)
f nm + α2f nm+1 + α2f nm&minus;1 &minus; f n&minus;1m . (18.48)
</p>
<p>This is a two-step method which can be rewritten as a one-step method of double
dimension
</p>
<p>(
fn+1
fn
</p>
<p>)
= T
</p>
<p>(
fn
fn&minus;1
</p>
<p>)
=
(
</p>
<p>2 + α2M &minus;1
1 0
</p>
<p>)(
fn
fn&minus;1
</p>
<p>)
(18.49)
</p>
<p>with the tridiagonal matrix
</p>
<p>M =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>&minus;2 a1
1 &minus;2 1
</p>
<p>. . .
. . .
</p>
<p>. . .
</p>
<p>1 &minus;2 1
aN &minus;2
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠
</p>
<p>(18.50)
</p>
<p>where a1 and aN have the values 1 for a fixed or 2 for an open end.</p>
<p/>
</div>
<div class="page"><p/>
<p>18.5 Numerical Integration with a Two-Step Method 339
</p>
<p>The matrix M has eigenvalues (Sect. 9.3)
</p>
<p>λ= 2 cos(k�x)&minus; 2 =&minus;4 sin2
(
k�x
</p>
<p>2
</p>
<p>)
. (18.51)
</p>
<p>To simulate excitation of waves by a moving boundary we add one grid point with
given elongation ξ0(t) and change the first equation into
</p>
<p>f (tn+1, x1)= 2
(
1 &minus; α2
</p>
<p>)
f (tn, x1)+ α2f (tn, x2)+ α2ξ0(tn)&minus; f (tn&minus;1, x1).
</p>
<p>(18.52)
</p>
<p>Repeated iteration gives the series of function values
(
f1
f0
</p>
<p>)
,
</p>
<p>(
f2
f1
</p>
<p>)
= T
</p>
<p>(
f1
f0
</p>
<p>)
,
</p>
<p>(
f3
f2
</p>
<p>)
= T 2
</p>
<p>(
f1
f0
</p>
<p>)
, &middot; &middot; &middot; (18.53)
</p>
<p>A necessary condition for stability is that all eigenvalues of T have absolute values
smaller than one. Otherwise small perturbations would be amplified. The eigenvalue
equation for T is
</p>
<p>(
2 + α2M &minus; σ &minus;1
</p>
<p>1 &minus;σ
</p>
<p>)(
u
</p>
<p>v
</p>
<p>)
=
(
</p>
<p>0
0
</p>
<p>)
. (18.54)
</p>
<p>We substitute the solution of the second equation
</p>
<p>u= σv (18.55)
into the first equation and use the eigenvectors of M (Sect. 9.3) to obtain the eigen-
value equation
</p>
<p>(
2 + α2λ&minus; σ
</p>
<p>)
σv&minus; v = 0. (18.56)
</p>
<p>Hence σ is one of the two roots of
</p>
<p>σ 2 &minus; σ
(
α2λ+ 2
</p>
<p>)
+ 1 = 0 (18.57)
</p>
<p>which are given by (Fig. 18.8)
</p>
<p>σ = 1 + α
2λ
</p>
<p>2
&plusmn;
</p>
<p>&radic;(
α2λ
</p>
<p>2
+ 1
</p>
<p>)2
&minus; 1. (18.58)
</p>
<p>From
</p>
<p>λ=&minus;4 sin2
(
k�x
</p>
<p>2
</p>
<p>)
</p>
<p>we find
</p>
<p>&minus;4 &lt; λ&lt; 0 (18.59)
</p>
<p>1 &minus; 2α2 &lt; α
2λ
</p>
<p>2
+ 1 &lt; 1 (18.60)
</p>
<p>and the square root in (18.58) is imaginary if
</p>
<p>&minus;1 &lt; α
2λ
</p>
<p>2
+ 1 &lt; 1 (18.61)</p>
<p/>
</div>
<div class="page"><p/>
<p>340 18 Waves
</p>
<p>Fig. 18.8 (Stability regions
of the two-step method)
Instabilities appear for
|α|&gt; 1. One of the two
eigenvalues σ becomes
unstable (|σ |&gt; 1) for waves
with large k-values
</p>
<p>which is the case for
</p>
<p>sin2
(
k�x
</p>
<p>2
</p>
<p>)
α2 &lt; 1. (18.62)
</p>
<p>This holds for all k only if
</p>
<p>|α|&lt; 1. (18.63)
But then
</p>
<p>|σ |2 =
(
</p>
<p>1 + α
2λ
</p>
<p>2
</p>
<p>)2
+
(
</p>
<p>1 &minus;
(
α2λ
</p>
<p>2
+ 1
</p>
<p>)2)
= 1 (18.64)
</p>
<p>and the algorithm is (conditionally) stable. If on the other hand |α| &gt; 1 then for
some k-values the square root is real. Here we have
</p>
<p>1 + α
2λ
</p>
<p>2
&lt;&minus;1 (18.65)
</p>
<p>and finally
</p>
<p>1 + α
2λ
</p>
<p>2
&minus;
</p>
<p>&radic;(
1 + α
</p>
<p>2λ
</p>
<p>2
</p>
<p>)2
&minus; 1 &lt;&minus;1 (18.66)
</p>
<p>which shows that instabilities are possible in this case.
</p>
<p>18.6 Reduction to a First Order Differential Equation
</p>
<p>A general method to reduce the order of an ordinary differential equation (or a sys-
tem of such) introduces the time derivatives as additional variables (Chap. 12). The
spatially discretized one-dimensional wave equation (18.22) can be transformed into
a system of double dimension</p>
<p/>
</div>
<div class="page"><p/>
<p>18.6 Reduction to a First Order Differential Equation 341
</p>
<p>d
</p>
<p>dt
f(t)= v(t) (18.67)
</p>
<p>d
</p>
<p>dt
v(t)= c
</p>
<p>2
</p>
<p>�x2
Mf(t)+ S(t). (18.68)
</p>
<p>We use the improved Euler method (Sect. 12.5)
</p>
<p>f(t +�t)= f(t)+ v
(
t + �t
</p>
<p>2
</p>
<p>)
�t +O
</p>
<p>(
�t3
</p>
<p>)
(18.69)
</p>
<p>v(t +�t)= v(t)+
[
</p>
<p>c2
</p>
<p>�x2
Mf
</p>
<p>(
t + �t
</p>
<p>2
</p>
<p>)
+ S
</p>
<p>(
t + �t
</p>
<p>2
</p>
<p>)]
�t +O
</p>
<p>(
�t3
</p>
<p>)
</p>
<p>(18.70)
</p>
<p>and two different time grids
</p>
<p>fn = f(tn) Sn = S(tn) n= 0,1 . . . (18.71)
f(tn+1)= f(tn)+ v(tn+1/2)�t (18.72)
vn = v(tn&minus;1/2) n= 0,1 . . . (18.73)
</p>
<p>v(tn+1/2)= v(tn&minus;1/2)+
[
</p>
<p>c2
</p>
<p>�x2
Mf(tn)+ S(tn)
</p>
<p>]
�t. (18.74)
</p>
<p>We obtain a leapfrog (Fig. 18.9) like algorithm (page 231)
</p>
<p>vn+1 = vn +
[
</p>
<p>c2
</p>
<p>�x2
Mfn + Sn
</p>
<p>]
�t (18.75)
</p>
<p>fn+1 = fn + vn+1�t (18.76)
where the updated velocity (18.75) has to be inserted into (18.76). This can be com-
bined into the iteration
</p>
<p>(
fn+1
vn+1
</p>
<p>)
=
(
fn + vn�t + [ c
</p>
<p>2
</p>
<p>�x2
Mfn + Sn]�t2
</p>
<p>vn + [ c
2
</p>
<p>�x2
Mfn + Sn]�t
</p>
<p>)
</p>
<p>=
(
</p>
<p>1 + c2�t2
�x2
</p>
<p>M �t
</p>
<p>c2�t
�x2
</p>
<p>M 1
</p>
<p>)(
fn
vn
</p>
<p>)
+
(
Sn�t
</p>
<p>2
</p>
<p>Sn�t
</p>
<p>)
. (18.77)
</p>
<p>Since the velocity appears explicitly we can easily add a velocity dependent
damping like
</p>
<p>&minus;γ v(tn, xm) (18.78)
which we approximate by
</p>
<p>&minus;γ v
(
tn &minus;
</p>
<p>�t
</p>
<p>2
, xm
</p>
<p>)
(18.79)
</p>
<p>under the assumption of weak damping
</p>
<p>γ�t ≪ 1. (18.80)</p>
<p/>
</div>
<div class="page"><p/>
<p>342 18 Waves
</p>
<p>Fig. 18.9 Leapfrog method
</p>
<p>To study the stability of this algorithm we consider the homogeneous problem
with fixed boundaries. With the Courant number α = c�t
</p>
<p>�x
(18.77) becomes
</p>
<p>(
fn+1
vn+1
</p>
<p>)
=
(
</p>
<p>1 + α2M �t(1 &minus; γ�t)
α2
</p>
<p>�t
M 1 &minus; γ�t
</p>
<p>)(
fn
vn
</p>
<p>)
. (18.81)
</p>
<p>Using the eigenvectors and eigenvalues of M (Sect. 9.3)
</p>
<p>λ=&minus;4 sin2
(
k�x
</p>
<p>2
</p>
<p>)
(18.82)
</p>
<p>we find the following equation for the eigenvalues σ :
(
1 + α2λ&minus; σ
</p>
<p>)
u+�t(1 &minus; γ�t)v = 0
</p>
<p>α2λu+�t(1 &minus; γ�t &minus; σ)v = 0.
(18.83)
</p>
<p>Solving the second equation for u and substituting into the first equation we have
[(
</p>
<p>1 + α2λ&minus; σ
) �t
&minus;α2λ(1 &minus; γ�t &minus; σ)+�t(1 &minus; γ�t)
</p>
<p>]
= 0 (18.84)
</p>
<p>hence
(
1 + α2λ&minus; σ
</p>
<p>)
(1 &minus; γ�t &minus; σ)&minus; α2λ(1 &minus; γ�t)= 0
</p>
<p>σ 2 &minus; σ
(
2 &minus; γ�t + α2λ
</p>
<p>)
+ (1 &minus; γ�t)= 0
</p>
<p>σ = 1 &minus; γ�t
2
</p>
<p>+ α
2λ
</p>
<p>2
&plusmn;
</p>
<p>&radic;(
1 &minus; γ�t
</p>
<p>2
+ α
</p>
<p>2λ
</p>
<p>2
</p>
<p>)2
&minus; (1 &minus; γ�t).
</p>
<p>(18.85)
</p>
<p>Instabilities are possible if the square root is real and σ &lt;&minus;1 (σ &gt; 1 is not possible).
This is the case for
</p>
<p>&minus;1 + γ�t
2
</p>
<p>&asymp;&minus;
&radic;
</p>
<p>1 &minus; γ�t &lt; 1 &minus; γ�t
2
</p>
<p>+ α
2λ
</p>
<p>2
&lt;
&radic;
</p>
<p>1 &minus; γ�t &asymp; 1 &minus; γ�t
2
(18.86)
</p>
<p>&minus;2 + γ�t &lt; α
2λ
</p>
<p>2
&lt; 0. (18.87)
</p>
<p>The right inequality is satisfied, hence it remains</p>
<p/>
</div>
<div class="page"><p/>
<p>18.7 Two-Variable Method 343
</p>
<p>Fig. 18.10 (Standing waves in an organ pipe) At the closed (left) end the amplitude of the longi-
tudinal velocity is zero whereas the amplitudes of pressure and density changes are extremal. This
is reversed at the open (right) end
</p>
<p>α2 sin2
(
k�x
</p>
<p>2
</p>
<p>)
&lt; 1 &minus; γ�t
</p>
<p>2
. (18.88)
</p>
<p>This holds for all k-values if it holds for the maximum of the sine-function
</p>
<p>α2 &lt; 1 &minus; γ�t
2
</p>
<p>. (18.89)
</p>
<p>This shows that inclusion of the damping term even favors instabilities.
</p>
<p>18.7 Two-Variable Method
</p>
<p>For the 1-dimensional wave equation (18.11) there exists another possibility to re-
duce the order of the time derivative by splitting it up into two first order equations
similar to (18.9), (18.10)
</p>
<p>&part;
</p>
<p>&part;t
f (t, x)= c &part;
</p>
<p>&part;x
g(t, x) (18.90)
</p>
<p>&part;
</p>
<p>&part;t
g(t, x)= c &part;
</p>
<p>&part;x
f (t, x). (18.91)
</p>
<p>Several algorithms can be applied to solve these equations [162]. We discuss only
methods which are second order in space and time and are rather general methods
to solve partial differential equations. The boundary conditions need some special
care. For closed boundaries with f (x0)= 0 obviously &part;f&part;t (x0)= 0 whereas
</p>
<p>&part;f
&part;x
(x0)
</p>
<p>is finite. Hence a closed boundary for f (t, x) is connected with an open bound-
ary for g(t, x) with &part;g
</p>
<p>&part;x
(x0) = 0 and vice versa. This is well known from acoustics
</p>
<p>(Fig. 18.10).
</p>
<p>18.7.1 Leapfrog Scheme
</p>
<p>We use symmetric differences (Sect. 3.2) for the first derivatives
</p>
<p>f (t + �t2 , x)&minus; f (t &minus;
�t
2 , x)
</p>
<p>�t
= c
</p>
<p>g(t, x + �x2 )&minus; g(x &minus;
�x
2 )
</p>
<p>�x
+O
</p>
<p>(
�x2,�t2
</p>
<p>)
</p>
<p>(18.92)
</p>
<p>g(t + �t2 , x)&minus; g(t &minus;
�t
2 , x)
</p>
<p>�t
= c
</p>
<p>f (t, x + �x2 )&minus; f (x &minus;
�x
2 )
</p>
<p>�x
+O
</p>
<p>(
�x2,�t2
</p>
<p>)
</p>
<p>(18.93)</p>
<p/>
</div>
<div class="page"><p/>
<p>344 18 Waves
</p>
<p>Fig. 18.11 (Simulation with
the leapfrog method)
A rectangular pulse is
simulated with the
two-variable leapfrog
method. While for α = 1 the
pulse shape has not changed
after 1000 steps, for smaller
values the short wavelength
components are lost due to
dispersion
</p>
<p>to obtain the following scheme
</p>
<p>g(tn+1/2, xm+1/2)= g(tn&minus;1/2, xm+1/2)+ α
(
f (tn, xm+1)
</p>
<p>)
&minus; f (tn, xm&minus;1)
</p>
<p>(18.94)
</p>
<p>f (tn+1, xm)= f (tn, xm)+ α
(
g(tn+1/2, xm+1/2)&minus; g(tn+1/2, xm&minus;1/2)
</p>
<p>)
.
</p>
<p>(18.95)
</p>
<p>Using different time grids for the two variables
</p>
<p>fn =
</p>
<p>⎛
⎜⎝
</p>
<p>f n1
...
</p>
<p>f nM
</p>
<p>⎞
⎟⎠=
</p>
<p>⎛
⎜⎝
</p>
<p>f (tn, x1)
...
</p>
<p>f (tn, xM)
</p>
<p>⎞
⎟⎠ gn =
</p>
<p>⎛
⎜⎝
</p>
<p>gn1
...
</p>
<p>gnM
</p>
<p>⎞
⎟⎠=
</p>
<p>⎛
⎜⎝
</p>
<p>g(tn&minus;1/2, x1/2)
...
</p>
<p>g(tn&minus;1/2, xM&minus;1/2)
</p>
<p>⎞
⎟⎠
</p>
<p>(18.96)
</p>
<p>this translates into the algorithm (Fig. 18.11)
</p>
<p>gn+1m = gnm + α
(
f nm &minus; f nm&minus;1
</p>
<p>)
(18.97)
</p>
<p>f n+1m = f nm + α
(
gn+1m+1 &minus; g
</p>
<p>n+1
m
</p>
<p>)
</p>
<p>= f nm + α
(
gnm+1 &minus; gnm
</p>
<p>)
+ α2
</p>
<p>(
f nm+1 &minus; 2f nm + f nm&minus;1
</p>
<p>)
. (18.98)
</p>
<p>To analyze the stability we insert
</p>
<p>f nm = ueaneikm�x gnm = veaneikm�x (18.99)
and obtain the equations
</p>
<p>Gv = v+ αu
(
1 &minus; e&minus;ik�x
</p>
<p>)
(18.100)
</p>
<p>Gu= u+ αv
(
eik�x &minus; 1
</p>
<p>)
+ α2u(2 cosk�x &minus; 2) (18.101)
</p>
<p>which in matrix form read
</p>
<p>G
</p>
<p>(
u
</p>
<p>v
</p>
<p>)
=
(
</p>
<p>1 + α2(2 cosk�x &minus; 2) α(eik�x &minus; 1)
α(1 &minus; e&minus;ik�x) 1
</p>
<p>)(
u
</p>
<p>v
</p>
<p>)
. (18.102)</p>
<p/>
</div>
<div class="page"><p/>
<p>18.7 Two-Variable Method 345
</p>
<p>The maximum amplification factor G is given by the largest eigenvalue, which is
one of the roots of
</p>
<p>(
1 &minus; 4α2 sin2
</p>
<p>(
k�x
</p>
<p>2
</p>
<p>)
&minus; σ
</p>
<p>)
(1 &minus; σ)+ 4α2 sin2
</p>
<p>(
k�x
</p>
<p>2
</p>
<p>)
= 0
</p>
<p>(
1 &minus; σ + α2λ2
</p>
<p>)
(1 &minus; σ)&minus; α2λ2 = 0
</p>
<p>(18.103)
</p>
<p>σ = 1 &minus; 2α2 sin2
(
k�x
</p>
<p>2
</p>
<p>)
&plusmn;
</p>
<p>&radic;(
1 &minus; 2α2 sin2
</p>
<p>(
k�x
</p>
<p>2
</p>
<p>))2
&minus; 1. (18.104)
</p>
<p>The eigenvalues coincide with those of the two-step method (18.58).
</p>
<p>18.7.2 Lax-Wendroff Scheme
</p>
<p>The Lax-Wendroff scheme can be derived from the Taylor series expansion
</p>
<p>f (t +�t,x)= f (t, x)+ &part;f (t, x)
&part;t
</p>
<p>�t + 1
2
�t2
</p>
<p>&part;2f (t, x)
</p>
<p>&part;t2
+ &middot; &middot; &middot;
</p>
<p>= f (t, x)+ c�t &part;g(t, x)
&part;x
</p>
<p>+ c
2�t2
</p>
<p>2
</p>
<p>&part;2f (t, x)
</p>
<p>&part;t2
+ &middot; &middot; &middot; (18.105)
</p>
<p>g(t +�t,x)= g(t, x)+ &part;g(t, x)
&part;t
</p>
<p>�t + 1
2
�t2
</p>
<p>&part;2g(t, x)
</p>
<p>&part;t2
+ &middot; &middot; &middot;
</p>
<p>= g(t, x)+ c�t &part;f (t, x)
&part;x
</p>
<p>+ c
2�t2
</p>
<p>2
</p>
<p>&part;2g(t, x)
</p>
<p>&part;t2
+ &middot; &middot; &middot; . (18.106)
</p>
<p>It uses symmetric differences on regular grids (18.45), (18.46) to obtain the iteration
</p>
<p>f n+1m = f nm + c�t
gnm+1 &minus; gnm&minus;1
</p>
<p>2�x
+ c2�t2
</p>
<p>f nm+1 + f nm&minus;1 &minus; 2f nm
2�x2
</p>
<p>(18.107)
</p>
<p>gn+1m = gnm + c�t
f nm+1 &minus; f nm&minus;1
</p>
<p>2�x
+ c2�t2
</p>
<p>gnm+1 + gnm&minus;1 &minus; 2gnm
2�x2
</p>
<p>(18.108)
</p>
<p>(
fn+1
</p>
<p>gn+1
</p>
<p>)
=
(
</p>
<p>1 + α22 M
α
2D
</p>
<p>α
2D 1 +
</p>
<p>α2
</p>
<p>2 M
</p>
<p>)(
fn
</p>
<p>gn
</p>
<p>)
(18.109)
</p>
<p>with the tridiagonal matrix
</p>
<p>D =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>0 1
&minus;1 0 1
</p>
<p>. . .
. . .
</p>
<p>. . .
</p>
<p>&minus;1 0 1
&minus;1 0
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠
. (18.110)
</p>
<p>To analyze the stability we insert
</p>
<p>f nm = ueaneikm�x gnm = veaneikm�x (18.111)</p>
<p/>
</div>
<div class="page"><p/>
<p>346 18 Waves
</p>
<p>Fig. 18.12 (Stability region
of the Lax-Wendroff method)
Instabilities appear for
|α|&gt; 1. In the opposite case
short wavelength modes are
damped
</p>
<p>Fig. 18.13 (Simulation with
the Lax-Wendroff method)
A rectangular pulse is
simulated with the
two-variable Lax-Wendroff
method. While for α = 1 the
pulse shape has not changed
after 2000 steps, for smaller
values the short wavelength
components are lost due to
dispersion and damping
</p>
<p>and calculate the eigenvalues (compare with (18.102)) of
</p>
<p>(
1 + α2(cosk�x &minus; 1) iα sin k�x
</p>
<p>iα sin k�x 1 + α2(cosk�x &minus; 1)
</p>
<p>)
(18.112)
</p>
<p>which are given by
</p>
<p>σ = 1 + α2(cosk�x &minus; 1)&plusmn;
&radic;
α2
</p>
<p>(
cos2 k�x &minus; 1
</p>
<p>)
. (18.113)
</p>
<p>The root is always imaginary and
</p>
<p>|σ |2 = 1 +
(
α4 &minus; α2
</p>
<p>)
(cosk�x &minus; 1)2 &le; 1 + 4
</p>
<p>(
α4 &minus; α2
</p>
<p>)
.
</p>
<p>For α &lt; 1 we find |σ |&lt; 1. The method is stable but there is wavelength dependent
damping (Figs. 18.12, 18.13).</p>
<p/>
</div>
<div class="page"><p/>
<p>18.7 Two-Variable Method 347
</p>
<p>18.7.3 Crank-Nicolson Scheme
</p>
<p>This method takes the average of the explicit and implicit Euler methods
</p>
<p>f (t +�t)= f (t)+ c
2
</p>
<p>(
&part;g
</p>
<p>&part;x
(t, x)+ &part;g
</p>
<p>&part;x
(t +�t,x)
</p>
<p>)
�t (18.114)
</p>
<p>g(t +�t)= g(t)+ c
2
</p>
<p>(
&part;f
</p>
<p>&part;x
(t, x)+ &part;f
</p>
<p>&part;x
(t +�t,x)
</p>
<p>)
�t (18.115)
</p>
<p>and uses symmetric differences on the regular grids (18.45), (18.46) to obtain
</p>
<p>f n+1m = f nm +
α
</p>
<p>4
</p>
<p>(
gnm+1 &minus; gnm&minus;1 + gn+1m+1 &minus; g
</p>
<p>n+1
m&minus;1
</p>
<p>)
(18.116)
</p>
<p>gn+1m = gnm +
α
</p>
<p>4
</p>
<p>(
f nm+1 &minus; f nm&minus;1 + f n+1m+1 &minus; f
</p>
<p>n+1
m&minus;1
</p>
<p>)
(18.117)
</p>
<p>which reads in matrix notation
(
fn+1
gn+1
</p>
<p>)
=
(
</p>
<p>1 α4D
α
4D 1
</p>
<p>)(
fn
gn
</p>
<p>)
+
(
</p>
<p>α
4D
</p>
<p>α
4D
</p>
<p>)(
fn+1
gn+1
</p>
<p>)
. (18.118)
</p>
<p>This equation can be solved formally by collecting terms at time tn+1
(
</p>
<p>1 &minus;α4D
&minus;α4D 1
</p>
<p>)(
fn+1
gn+1
</p>
<p>)
=
(
</p>
<p>1 α4D
α
4D 1
</p>
<p>)(
fn
gn
</p>
<p>)
(18.119)
</p>
<p>and multiplying with the inverse matrix from left
(
fn+1
gn+1
</p>
<p>)
=
(
</p>
<p>1 &minus;α4D
&minus;α4D 1
</p>
<p>)&minus;1 ( 1 α4D
α
4D 1
</p>
<p>)(
fn
gn
</p>
<p>)
. (18.120)
</p>
<p>Now, if u is an eigenvector of D with purely imaginary eigenvalue λ (Sect. 9.3)
(
</p>
<p>1 α4D
α
4D 1
</p>
<p>)(
u
</p>
<p>&plusmn;u
</p>
<p>)
=
(
(1 &plusmn; α4λ)u
(α4λ&plusmn; 1)u
</p>
<p>)
=
(
</p>
<p>1 &plusmn; α
4
λ
</p>
<p>)(
u
</p>
<p>&plusmn;u
</p>
<p>)
(18.121)
</p>
<p>and furthermore
(
</p>
<p>1 &minus;α4D
&minus;α4D 1
</p>
<p>)(
u
</p>
<p>&plusmn;u
</p>
<p>)
=
(
</p>
<p>(1 ∓ α4λ)u
(&minus;α4λ&plusmn; 1)u
</p>
<p>)
=
(
</p>
<p>1 ∓ α
4
λ
</p>
<p>)(
u
</p>
<p>&plusmn;u
</p>
<p>)
.(18.122)
</p>
<p>But, since the eigenvalue of the inverse matrix is the reciprocal of the eigenvalue,
the eigenvalues of
</p>
<p>T =
(
</p>
<p>1 &minus;α4D
&minus;α4D 1
</p>
<p>)&minus;1 ( 1 α4D
α
4D 1
</p>
<p>)
(18.123)
</p>
<p>are given by
</p>
<p>σ =
1 &plusmn; α4λ
1 ∓ α4λ
</p>
<p>. (18.124)
</p>
<p>Since λ is imaginary, we find |σ | = 1. The Crank-Nicolson method is stable and
does not show damping like the Lax-Wendroff method. However, there is consid-</p>
<p/>
</div>
<div class="page"><p/>
<p>348 18 Waves
</p>
<p>Fig. 18.14 (Simulation with
the iterated Crank-Nicolson
method) A rectangular pulse
is simulated with the
two-variable iterated
Crank-Nicolson method.
Only this method is stable for
values α &gt; 1
</p>
<p>erable dispersion. Solution of the linear system (18.119) is complicated and can be
replaced by an iterative predictor-corrector method. Starting from the initial guess
</p>
<p>(
(0)fn+1
(0)gn+1
</p>
<p>)
=
(
</p>
<p>1 α2D
α
2D 1
</p>
<p>)(
fn
gn
</p>
<p>)
(18.125)
</p>
<p>we iterate
(
</p>
<p>(0)fn+1/2
(0)gn+1/2
</p>
<p>)
= 1
</p>
<p>2
</p>
<p>(
(0)fn+1
(0)gn+1
</p>
<p>)
+ 1
</p>
<p>2
</p>
<p>(
fn
gn
</p>
<p>)
=
(
</p>
<p>1 α4D
α
4D 1
</p>
<p>)(
fn
gn
</p>
<p>)
</p>
<p>(
(1)fn+1
(1)gn+1
</p>
<p>)
=
(
fn
gn
</p>
<p>)
+
(
</p>
<p>α
2D
</p>
<p>α
2D
</p>
<p>)(
(0)fn+1/2
(0)gn+1/2
</p>
<p>)
</p>
<p>=
(
</p>
<p>1 α4D
α
4D 1
</p>
<p>)(
fn
gn
</p>
<p>)
+
(
</p>
<p>α
4D
</p>
<p>α
4D
</p>
<p>)(
(0)fn+1
(0)gn+1
</p>
<p>)
(18.126)
</p>
<p>(
(1)fn+1/2
(1)gn+1/2
</p>
<p>)
= 1
</p>
<p>2
</p>
<p>(
fn
gn
</p>
<p>)
+ 1
</p>
<p>2
</p>
<p>(
(1)fn+1
(1)gn+1
</p>
<p>)
</p>
<p>=
(
fn
gn
</p>
<p>)
+
(
</p>
<p>α
4D
</p>
<p>α
4D
</p>
<p>)(
(0)fn+1/2
(0)gn+1/2
</p>
<p>)
(18.127)
</p>
<p>(
(2)fn+1
(2)gn+1
</p>
<p>)
=
(
fn
gn
</p>
<p>)
+
(
</p>
<p>α
2D
</p>
<p>α
2D
</p>
<p>)(
(1)fn+1/2
(1)gn+1/2
</p>
<p>)
</p>
<p>=
(
</p>
<p>1 α4D
α
4D 1
</p>
<p>)(
fn
gn
</p>
<p>)
+
(
</p>
<p>α
4D
</p>
<p>α
4D
</p>
<p>)(
(1)fn+1
(1)gn+1
</p>
<p>)
. (18.128)
</p>
<p>In principle this iteration could be repeated more times, but as Teukolsky showed
[250], two iterations are optimal for hyperbolic equations like the advection or wave
equation. The region of stability is reduced (Figs. 18.14, 18.15) compared to the
implicit Crank-Nicolson method. The eigenvalues are
</p>
<p>(0)σ = 1 &plusmn; iα sin k�x
∣∣(0)σ
</p>
<p>∣∣&gt; 1 (18.129)
</p>
<p>(1)σ = 1 &plusmn; iα sink�x &minus; α
2
</p>
<p>2
sin2 k�x
</p>
<p>∣∣(1)σ
∣∣&gt; 1 (18.130)</p>
<p/>
</div>
<div class="page"><p/>
<p>18.8 Problems 349
</p>
<p>Fig. 18.15 (Simulation of a
triangular pulse) A triangular
pulse is simulated with
different two-variable
methods (full curve: initial
conditions, dashed curve:
leapfrog, dash-dotted:
Lax-Wendroff, dotted:
iterated Crank-Nicolson).
This pulse contains less short
wavelength components than
the square pulse and shows
much less deformation even
after 5000 steps
</p>
<p>(2)σ = 1 &minus; α
2
</p>
<p>2
sin2 k�x &plusmn; i
</p>
<p>(
α sin k�x &minus; α
</p>
<p>3 sin3 k�x
</p>
<p>4
</p>
<p>)
</p>
<p>∣∣(2)σ
∣∣2 = 1 &minus; α
</p>
<p>4 sin4 k�x
</p>
<p>4
+ α
</p>
<p>6 sin6 k�x
</p>
<p>16
&le; 1 for |α| &le; 2.
</p>
<p>(18.131)
</p>
<p>18.8 Problems
</p>
<p>Problem 18.1 (Waves on a damped string) In this computer experiment we simu-
late waves on a string with a moving boundary with the method from Sect. 18.6.
</p>
<p>&bull; Excite the left boundary with a continuous sine function and try to generate stand-
ing waves
</p>
<p>&bull; Increase the velocity until instabilities appear
&bull; Compare reflection at open and fixed right boundary
&bull; Observe the dispersion of pulses with different shape and duration
&bull; The velocity can be changed by a factor n (refractive index) in the region x &gt; 0.
</p>
<p>Observe reflection at the boundary x = 0
</p>
<p>Problem 18.2 (Waves with the Fourier transform method) In this computer exper-
iment we use the method from Sect. 18.3 to simulate waves on a string with fixed
boundaries.
</p>
<p>&bull; Different initial excitations of the string can be selected.
&bull; The dispersion can be switched off by using ωk = ck instead of the proper eigen-
</p>
<p>values (18.31).
</p>
<p>Problem 18.3 (Two-variable methods) In this computer experiment we simulate
waves with periodic boundary conditions. Different initial values (rectangular, trian-
gular or Gaussian pulses of different widths) and methods (leapfrog, Lax-Wendroff,
iterated Crank-Nicolson) can be compared.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 19
</p>
<p>Diffusion
</p>
<p>Diffusion is one of the simplest non-equilibrium processes. It describes the transport
of heat [52, 95] and the time evolution of differences in substance concentrations
[82]. In this chapter, the one-dimensional diffusion equation
</p>
<p>&part;
</p>
<p>&part;t
f (t, x)=D &part;
</p>
<p>2
</p>
<p>&part;x2
f (t, x)+ S(t, x) (19.1)
</p>
<p>is semi-discretized with finite differences. The time integration is performed with
three different Euler methods. The explicit Euler method is conditionally stable only
for small Courant number α = D�t
</p>
<p>�x2
&lt; 1/2, which makes very small time steps nec-
</p>
<p>essary. The fully implicit method is unconditionally stable but its dispersion devi-
ates largely from the exact expression. The Crank-Nicolson method is also uncon-
ditionally stable. However, it is more accurate and its dispersion relation is closer
to the exact one. Extension to more than one dimension is easily possible, but the
numerical effort increases drastically as there is no formulation involving simple
tridiagonal matrices like in one dimension. The split operator approximation uses
the one-dimensional method independently for each dimension. It is very efficient
with almost no loss in accuracy. In a computer experiment the different schemes are
compared for diffusion in two dimensions.
</p>
<p>19.1 Particle Flux and Concentration Changes
</p>
<p>Let f (x, t) denote the concentration of a particle species and J the corresponding
flux of particles. Consider a small cube with volume h3 (Fig. 19.1). The change
of the number of particles within this volume is given by the integral form of the
conservation law (11.10)
</p>
<p>&part;
</p>
<p>&part;t
</p>
<p>&int;
</p>
<p>V
</p>
<p>dV f (r, t)+
∮
</p>
<p>&part;V
</p>
<p>J(r, t) dA=
&int;
</p>
<p>V
</p>
<p>dV S(r, t) (19.2)
</p>
<p>where the source term S(r) accounts for creation or destruction of particles due to
for instance chemical reactions. In Cartesian coordinates we have
</p>
<p>P.O.J. Scherer, Computational Physics, Graduate Texts in Physics,
DOI 10.1007/978-3-319-00401-3_19,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>351</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-00401-3_19">http://dx.doi.org/10.1007/978-3-319-00401-3_19</a></div>
</div>
<div class="page"><p/>
<p>352 19 Diffusion
</p>
<p>Fig. 19.1 Flux through a
volume element
</p>
<p>&int; x+h/2
</p>
<p>x&minus;h/2
dx&prime;
</p>
<p>&int; y+h/2
</p>
<p>y&minus;h/2
dy&prime;
</p>
<p>&int; z+h/2
</p>
<p>z&minus;h/2
dz&prime;
</p>
<p>(
&part;
</p>
<p>&part;t
f
(
x&prime;, y&prime;, z&prime;, t
</p>
<p>)
&minus; S
</p>
<p>(
x&prime;, y&prime;, z&prime;, t
</p>
<p>))
</p>
<p>+
&int; x+h/2
</p>
<p>x&minus;h/2
dx&prime;
</p>
<p>&int; y+h/2
</p>
<p>y&minus;h/2
dy&prime;
</p>
<p>(
Jz
</p>
<p>(
x&prime;, y&prime;, z&minus; h
</p>
<p>2
</p>
<p>)
&minus; Jz
</p>
<p>(
x&prime;, y&prime;, z+ h
</p>
<p>2
</p>
<p>))
</p>
<p>+
&int; x+h/2
</p>
<p>x&minus;h/2
dx&prime;
</p>
<p>&int; z+h/2
</p>
<p>z&minus;h/2
dz&prime;
</p>
<p>(
Jy
</p>
<p>(
x&prime;, y &minus; h
</p>
<p>2
, z&prime;
</p>
<p>)
&minus; Jy
</p>
<p>(
x&prime;, y + h
</p>
<p>2
, z&prime;
</p>
<p>))
</p>
<p>+
&int; z+h/2
</p>
<p>z&minus;h/2
dz&prime;
</p>
<p>&int; y+h/2
</p>
<p>y&minus;h/2
dy&prime;
</p>
<p>(
Jz
</p>
<p>(
x &minus; h
</p>
<p>2
, y&prime;, z&prime;
</p>
<p>)
&minus; Jz
</p>
<p>(
x + h
</p>
<p>2
, y&prime;, z&prime;
</p>
<p>))
= 0.
</p>
<p>(19.3)
</p>
<p>In the limit of small h this turns into the differential form of the conservation law
</p>
<p>h3
(
&part;
</p>
<p>&part;t
f (x, y, z, t)&minus; S(x, y, z, t)
</p>
<p>)
+ h2
</p>
<p>(
h
&part;Jx
</p>
<p>&part;x
+ h&part;Jy
</p>
<p>&part;y
+ h&part;Jz
</p>
<p>&part;z
</p>
<p>)
= 0
</p>
<p>(19.4)
</p>
<p>or after division by h3
</p>
<p>&part;
</p>
<p>&part;t
f (r, t)=&minus;divJ(r, t)+ S(r, t). (19.5)
</p>
<p>Within the framework of linear response theory the flux is proportional to the gradi-
ent of f (Fig. 19.2),
</p>
<p>J=&minus;D gradf. (19.6)
</p>
<p>Together we obtain the diffusion equation
</p>
<p>&part;f
</p>
<p>&part;t
= div(D gradf )+ S (19.7)
</p>
<p>which in the special case of constant D simplifies to
</p>
<p>&part;f
</p>
<p>&part;t
=D�f + S. (19.8)</p>
<p/>
</div>
<div class="page"><p/>
<p>19.2 Diffusion in One Dimension 353
</p>
<p>Fig. 19.2 Diffusion due to a
concentration gradient
</p>
<p>19.2 Diffusion in One Dimension
</p>
<p>We will use the finite differences method which works well if the diffusion constant
D is constant in time and space. We begin with diffusion in one dimension and
use regular grids tn = n�t , xm =m�x, f nm = f (tn, xm) and the discretized second
derivative
</p>
<p>&part;2f
</p>
<p>&part;x2
= f (x +�x)+ f (x &minus;�x)&minus; 2f (x)
</p>
<p>�x2
+O
</p>
<p>(
�x2
</p>
<p>)
(19.9)
</p>
<p>to obtain the semi-discrete diffusion equation
</p>
<p>ḟ (t, xm)=
D
</p>
<p>�x2
</p>
<p>(
f (t, xm+1)+ f (t, xm&minus;1)&minus; 2f (t, xm)
</p>
<p>)
+ S(t, xm) (19.10)
</p>
<p>or in matrix notation
</p>
<p>ḟ(t)= D
�x2
</p>
<p>Mf(t)+ S(t) (19.11)
</p>
<p>with the tridiagonal matrix
</p>
<p>M =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>&minus;2 1
1 &minus;2 1
</p>
<p>. . .
. . .
</p>
<p>. . .
</p>
<p>1 &minus;2 1
1 &minus;2
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠
. (19.12)
</p>
<p>Boundary conditions can be taken into account by introducing extra boundary
points x0, xM+1 (Fig. 19.3).
</p>
<p>19.2.1 Explicit Euler (Forward Time Centered Space) Scheme
</p>
<p>A simple Euler step (Sect. 12.3) makes the approximation
</p>
<p>f n+1m &minus; f nm = ḟ (tn, xm)�t =D
�t
</p>
<p>�x2
</p>
<p>(
f nm+1 + f nm&minus;1 &minus; 2f nm
</p>
<p>)
+ Snm�t.
</p>
<p>(19.13)
</p>
<p>For homogeneous boundary conditions f = 0 this becomes in matrix form</p>
<p/>
</div>
<div class="page"><p/>
<p>354 19 Diffusion
</p>
<p>Fig. 19.3 (Boundary conditions for 1-dimensional diffusion) Additional boundary points
x0, xM+1 are used to realize the boundary conditions. (a) Dirichlet boundary conditions: the func-
</p>
<p>tion values at the boundary are given f (t, x0)= ξ0(t), &part;
2
</p>
<p>&part;x2
f (x1)= 1�x2 (f (x2)&minus; 2f (x1)+ ξ0(t))
</p>
<p>or f (t, xM+1) = ξM+1(t), &part;
2
</p>
<p>&part;x2
f (xM ) = 1�x2 (f (xM&minus;1) &minus; 2f (xM ) + ξM+1(t)). (b) Neumann
</p>
<p>boundary conditions: the flux through the boundary is given, hence the derivative &part;f
&part;x
</p>
<p>at the
</p>
<p>boundary f (t, x0) = f (t, x2) + 2�xD J1(t),
&part;2
</p>
<p>&part;x2
f (x1) = 1�x2 (2f (x2) &minus; 2f (x1) + 2
</p>
<p>�x
D
J1(t)) or
</p>
<p>f (t, xM+1) = f (t, xM&minus;1) &minus; 2�xD JM (t),
&part;2
</p>
<p>&part;x2
f (xM ) = 1�x2 (2f (xM&minus;1) &minus; 2f (xM ) &minus;
</p>
<p>2�x
D
</p>
<p>JM (t)).
(c) No-flow boundary conditions: there is no flux through the boundary, hence the deriva-
</p>
<p>tive &part;f
&part;x
</p>
<p>= 0 at the boundary f (t, x0) = f (t, x2), &part;
2
</p>
<p>&part;x2
f (x1) = 1�x2 (2f (x2) &minus; 2f (x1)) or
</p>
<p>f (t, xM )= f (t, xM&minus;2), &part;
2
</p>
<p>&part;x2
f (xM )= 1�x2 (2f (xM&minus;1)&minus; 2f (xM ))
</p>
<p>⎛
⎜⎝
f n+11
...
</p>
<p>f n+1M
</p>
<p>⎞
⎟⎠=A
</p>
<p>⎛
⎜⎝
</p>
<p>f n1
...
</p>
<p>f nM
</p>
<p>⎞
⎟⎠+
</p>
<p>⎛
⎜⎝
</p>
<p>Sn1�t
...
</p>
<p>SnM�t
</p>
<p>⎞
⎟⎠ (19.14)
</p>
<p>with the tridiagonal matrix
</p>
<p>A=
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎝
</p>
<p>1 &minus; 2D �t
�x2
</p>
<p>D �t
�x2
</p>
<p>D �t
�x2
</p>
<p>1 &minus; 2D �t
�x2
</p>
<p>. . .
. . .
</p>
<p>. . .
</p>
<p>D �t
�x2
</p>
<p>1 &minus; 2D �t
�x2
</p>
<p>D �t
�x2
</p>
<p>D �t
�x2
</p>
<p>1 &minus; 2D �t
�x2
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎠
</p>
<p>= 1 + αM
</p>
<p>(19.15)
</p>
<p>where α is the Courant number for diffusion</p>
<p/>
</div>
<div class="page"><p/>
<p>19.2 Diffusion in One Dimension 355
</p>
<p>α =D �t
�x2
</p>
<p>. (19.16)
</p>
<p>The eigenvalues of M are (compare (18.30))
</p>
<p>λ=&minus;4 sin2
(
k�x
</p>
<p>2
</p>
<p>)
with k�x = π
</p>
<p>M + 1 ,
2π
</p>
<p>M + 1 , . . . ,
Mπ
</p>
<p>M + 1 (19.17)
</p>
<p>and hence the eigenvalues of A are given by
</p>
<p>1 + αλ= 1 &minus; 4α sin2 k�x
2
</p>
<p>. (19.18)
</p>
<p>The algorithm is stable if
</p>
<p>|1 + αλ|&lt; 1 for all λ (19.19)
</p>
<p>which holds if
</p>
<p>&minus;1 &lt; 1 &minus; 4α sin2 k�x
2
</p>
<p>&lt; 1. (19.20)
</p>
<p>The maximum of the sine function is sin( Mπ2(M+1) )&asymp; 1. Hence the right hand inequa-
tion is satisfied and from the left one we have
</p>
<p>&minus;1 &lt; 1 &minus; 4α. (19.21)
</p>
<p>The algorithm is stable for
</p>
<p>α =D �t
�x2
</p>
<p>&lt;
1
</p>
<p>2
. (19.22)
</p>
<p>The dispersion relation follows from inserting a plane wave ansatz
</p>
<p>eiω�t = 1 &minus; 4α sin2
(
k�x
</p>
<p>2
</p>
<p>)
. (19.23)
</p>
<p>For α &gt; 1/4 the right hand side changes sign at
</p>
<p>kc�x = 2 arcsin
&radic;
</p>
<p>1
</p>
<p>4α
. (19.24)
</p>
<p>The imaginary part of ω has a singularity at kc and the real part has a finite value of
π for k &gt; kc (Fig. 19.4 on page 356). Deviations from the exact dispersion
</p>
<p>ω= ik2 (19.25)
</p>
<p>are large, except for very small k.
</p>
<p>19.2.2 Implicit Euler (Backward Time Centered Space) Scheme
</p>
<p>Next we use the backward difference</p>
<p/>
</div>
<div class="page"><p/>
<p>356 19 Diffusion
</p>
<p>Fig. 19.4 (Dispersion of the
explicit Euler method) The
dispersion of the explicit
method is shown for different
values of the Courant number
α and compared to the exact
dispersion (dashed curve).
The imaginary part of ω
shows a singularity for
α &gt; 1/4. Above the
singularity ω is complex
valued
</p>
<p>f n+1m &minus; f nm = ḟ (tn+1, xm)�t
</p>
<p>=D&part;
2f
</p>
<p>&part;x2
(tn+1, xm)�t + S(tn+1, xm)�t (19.26)
</p>
<p>to obtain the implicit method
</p>
<p>f n+1m &minus; α
(
f n+1m+1 + f
</p>
<p>n+1
m&minus;1 &minus; 2f
</p>
<p>n+1
m
</p>
<p>)
= f nm + Sn+1m �t (19.27)
</p>
<p>or in matrix notation
</p>
<p>Afn+1 = fn + Sn+1�t with A= 1 &minus; αM (19.28)
which can be solved formally by
</p>
<p>fn+1 =A&minus;1fn +A&minus;1Sn+1�t. (19.29)
The eigenvalues of A are
</p>
<p>λ(A)= 1 + 4α sin2 k�x
2
</p>
<p>&gt; 1 (19.30)
</p>
<p>and the eigenvalues of A&minus;1
</p>
<p>λ
(
A&minus;1
</p>
<p>)
= λ(A)&minus;1 = 1
</p>
<p>1 + 4α sin2 k�x2
. (19.31)
</p>
<p>The implicit method is unconditionally stable since
∣∣λ
(
A&minus;1
</p>
<p>)∣∣&lt; 1. (19.32)
The dispersion relation of the implicit scheme follows from
</p>
<p>eiω�t = 1
1 + 4α sin2( k�x2 )
</p>
<p>. (19.33)
</p>
<p>There is no singularity and ω is purely imaginary. Still, deviations from the exact
expression are large (Fig. 19.5 on page 357).</p>
<p/>
</div>
<div class="page"><p/>
<p>19.2 Diffusion in One Dimension 357
</p>
<p>Fig. 19.5 (Dispersion of the
implicit Euler method) The
dispersion of the fully
implicit method is shown for
two different values of the
Courant number α and
compared to the exact
dispersion (dashed curve)
</p>
<p>Formally a matrix inversion is necessary. Numerically it is much more efficient
to solve the tridiagonal system of equations (page 69).
</p>
<p>(1 &minus; αM)f (tn+1)= f (tn)+ S(tn+1)�t. (19.34)
</p>
<p>19.2.3 Crank-Nicolson Method
</p>
<p>The Crank-Nicolson method [65] which is often used for diffusion problems, com-
bines implicit and explicit methods. It uses the Heun method (Sect. 12.5) for the
time integration
</p>
<p>f n+1m &minus; f nm =
�t
</p>
<p>2
</p>
<p>(
&part;f
</p>
<p>&part;t
(tn+1, xm)+
</p>
<p>&part;f
</p>
<p>&part;t
(tn, xm)
</p>
<p>)
(19.35)
</p>
<p>=D�t
2
</p>
<p>(
&part;2f
</p>
<p>&part;x2
(tn+1, xm)+
</p>
<p>&part;2f
</p>
<p>&part;x2
(tn, xm)
</p>
<p>)
</p>
<p>+
(
S(tn, xm)+ S(tn+1, xm)
</p>
<p>)�t
2
</p>
<p>(19.36)
</p>
<p>=D�t
2
</p>
<p>(
f nm+1 + f nm&minus;1 &minus; 2f nm
</p>
<p>�x2
+
</p>
<p>f n+1m + f n+1m&minus;1 &minus; 2f n+1m
�x2
</p>
<p>)
</p>
<p>+ S
n
m + Sn+1m
</p>
<p>2
�t. (19.37)
</p>
<p>This approximation is second order both in time and space and becomes in matrix
notation
</p>
<p>(
1 &minus; α
</p>
<p>2
M
</p>
<p>)
fn+1 =
</p>
<p>(
1 + α
</p>
<p>2
M
</p>
<p>)
fn +
</p>
<p>Sn + Sn+1
2
</p>
<p>�t (19.38)
</p>
<p>which can be solved by</p>
<p/>
</div>
<div class="page"><p/>
<p>358 19 Diffusion
</p>
<p>fn+1 =
(
</p>
<p>1 &minus; α
2
M
</p>
<p>)&minus;1(
1 + α
</p>
<p>2
M
</p>
<p>)
fn +
</p>
<p>(
1 &minus; α
</p>
<p>2
M
</p>
<p>)&minus;1
Sn + Sn+1
</p>
<p>2
�t.
</p>
<p>(19.39)
</p>
<p>Again it is numerically much more efficient to solve the tridiagonal system of equa-
tions (19.38) than to calculate the inverse matrix.
</p>
<p>The eigenvalues of this method are
</p>
<p>λ=
1 + α2μ
1 &minus; α2μ
</p>
<p>with μ=&minus;4 sin2 k�x
2
</p>
<p>&isin; [&minus;4,0]. (19.40)
</p>
<p>Since αμ&lt; 0 it follows
</p>
<p>1 + α
2
μ&lt; 1 &minus; α
</p>
<p>2
μ (19.41)
</p>
<p>and hence
</p>
<p>λ &lt; 1. (19.42)
</p>
<p>On the other hand we have
</p>
<p>1 &gt;&minus;1 (19.43)
1 + α
</p>
<p>2
μ&gt;&minus;1 + α
</p>
<p>2
μ (19.44)
</p>
<p>λ &gt;&minus;1. (19.45)
This shows that the Crank-Nicolson method is stable [251]. The dispersion follows
from
</p>
<p>eiω�t =
1 &minus; 2α sin2( k�x2 )
1 + 2α sin2( k�x2 )
</p>
<p>. (19.46)
</p>
<p>For α &gt; 1/2 there is a sign change of the right hand side at
</p>
<p>kc�x = 2arcsin
&radic;
</p>
<p>1
</p>
<p>2α
. (19.47)
</p>
<p>The imaginary part of ω has a singularity at kc and ω is complex valued for k &gt; kc
(Fig. 19.6 on page 359).
</p>
<p>19.2.4 Error Order Analysis
</p>
<p>Taylor series gives for the exact solution
</p>
<p>�fexact =�tḟ (t, x)+
�t2
</p>
<p>2
f̈ (t, x)+ �t
</p>
<p>3
</p>
<p>6
</p>
<p>&part;3
</p>
<p>&part;t3
f (t, x)+ &middot; &middot; &middot;
</p>
<p>=�t
[
Df &prime;&prime;(t, x)+ S(t, x)
</p>
<p>]
</p>
<p>+ �t
2
</p>
<p>2
</p>
<p>[
Dḟ &prime;&prime;(t, x)+ Ṡ(t, x)
</p>
<p>]
+ &middot; &middot; &middot; (19.48)</p>
<p/>
</div>
<div class="page"><p/>
<p>19.2 Diffusion in One Dimension 359
</p>
<p>Fig. 19.6 (Dispersion of the
Crank-Nicolson method) The
dispersion of the
Crank-Nicolson method is
shown for different values of
the Courant number α and
compared to the exact
dispersion (dashed curve).
The imaginary part of ω
shows a singularity for
α &gt; 1/2. Above the
singularity ω is complex
valued. The exact dispersion
is approached quite closely
for α &asymp; 1/2
</p>
<p>whereas for the explicit method
</p>
<p>�fexpl = αMf (t, x)+ S(t, x)�t
</p>
<p>=D �t
�x2
</p>
<p>(
f (t, x +�x)+ f (t, x &minus;�x)&minus; 2f (t, x)
</p>
<p>)
+ S(t, x)�t
</p>
<p>=D �t
�x2
</p>
<p>(
�x2f &prime;&prime;(t, x)+ �x
</p>
<p>4
</p>
<p>12
f &prime;&prime;&prime;&prime;(t, x)+ &middot; &middot; &middot;
</p>
<p>)
+ S(t, x)�t
</p>
<p>=�fexact +
D�t�x2
</p>
<p>12
f &prime;&prime;&prime;&prime;(t, x)&minus; �t
</p>
<p>2
</p>
<p>2
f̈ (t, x)+ &middot; &middot; &middot; (19.49)
</p>
<p>and for the implicit method
</p>
<p>�fimpl = αMf (t +�t,x)+ S(t +�t,x)�t
</p>
<p>=D �t
�x2
</p>
<p>(
f (t +�t,x +�x)+ f (t +�t,x &minus;�x)&minus; 2f (t +�t,x)
</p>
<p>)
</p>
<p>+ S(t +�t,x)�t
</p>
<p>=D �t
�x2
</p>
<p>(
�x2f &prime;&prime;(t, x)+ �x
</p>
<p>4
</p>
<p>12
f &prime;&prime;&prime;&prime;(t, x)+ &middot; &middot; &middot;
</p>
<p>)
</p>
<p>+ S(t, x)�t +D �t
2
</p>
<p>�x2
</p>
<p>(
�x2ḟ &prime;&prime;(t, x)+ �x
</p>
<p>4
</p>
<p>12
ḟ &prime;&prime;&prime;&prime;(t, x)+ &middot; &middot; &middot;
</p>
<p>)
</p>
<p>+ Ṡ(t, x)�t2
</p>
<p>=�fexact +D
�t�x2
</p>
<p>12
f &prime;&prime;&prime;&prime;(t, x)+ 1
</p>
<p>2
�t2f̈ (t, x)+ &middot; &middot; &middot; . (19.50)
</p>
<p>The Crank-Nicolson method has higher accuracy in �t :
</p>
<p>�fCN =
�fexpl +�fimpl
</p>
<p>2
= D�t�x
</p>
<p>2
</p>
<p>12
f &prime;&prime;&prime;&prime;(t, x)&minus; �t
</p>
<p>3
</p>
<p>6
</p>
<p>&part;3f
</p>
<p>&part;t3
+ &middot; &middot; &middot; . (19.51)</p>
<p/>
</div>
<div class="page"><p/>
<p>360 19 Diffusion
</p>
<p>19.2.5 Finite Element Discretization
</p>
<p>In one dimension discretization with finite differences is very similar to dis-
cretization with finite elements, if Galerkin&rsquo;s method is applied on a regular grid
(Chap. 11). The only difference is the non-diagonal form of the mass-matrix which
has to be applied to the time derivative [88]. Implementation of the discretization
scheme (11.170) is straightforward. The semi-discrete diffusion equation becomes
</p>
<p>&part;
</p>
<p>&part;t
</p>
<p>(
1
</p>
<p>6
f (t, xm&minus;1)+
</p>
<p>2
</p>
<p>3
f (t, xm)+
</p>
<p>1
</p>
<p>6
f (t, xm+1)
</p>
<p>)
</p>
<p>= D
�x2
</p>
<p>(
f (t, xm+1)+ f (t, xm&minus;1)&minus; 2f (t, xm)
</p>
<p>)
+ S(t, xm) (19.52)
</p>
<p>or in matrix form
(
</p>
<p>1 + 1
6
M
</p>
<p>)
ḟ(t)= D
</p>
<p>�x2
Mf(t)+ S(t). (19.53)
</p>
<p>This can be combined with the Crank-Nicolson scheme to obtain
(
</p>
<p>1 + 1
6
M
</p>
<p>)
(fn+1 &minus; fn)=
</p>
<p>(
α
</p>
<p>2
Mfn +
</p>
<p>α
</p>
<p>2
Mfn+1
</p>
<p>)
+ �t
</p>
<p>2
(Sn + Sn+1)
</p>
<p>(19.54)
</p>
<p>or
[
</p>
<p>1 +
(
</p>
<p>1
</p>
<p>6
&minus; α
</p>
<p>2
</p>
<p>)
M
</p>
<p>]
fn+1 =
</p>
<p>[
1 +
</p>
<p>(
1
</p>
<p>6
+ α
</p>
<p>2
</p>
<p>)
M
</p>
<p>]
fn +
</p>
<p>�t
</p>
<p>2
(Sn + Sn+1).
</p>
<p>(19.55)
</p>
<p>19.3 Split-Operator Method for Multidimensions
</p>
<p>The simplest discretization of the Laplace operator in 3 dimensions is given by
</p>
<p>�f =
(
</p>
<p>&part;2
</p>
<p>&part;x2
+ &part;
</p>
<p>2
</p>
<p>&part;y2
+ &part;
</p>
<p>2
</p>
<p>&part;z2
</p>
<p>)
f (t, x, y, z)
</p>
<p>= 1
�x2
</p>
<p>(Mx +My +Mz)f (t, x, y, z) (19.56)
</p>
<p>where
</p>
<p>1
</p>
<p>�x2
Mxf (t, x, y, z)=
</p>
<p>f (t, x +�x,y, z)+ f (t, x &minus;�x,y, z)&minus; 2f (t, x, y, z)
�x2
</p>
<p>(19.57)
</p>
<p>etc. denote the discretized second derivatives. Generalization of the Crank-Nicolson
method for the 3-dimensional problem gives</p>
<p/>
</div>
<div class="page"><p/>
<p>19.3 Split-Operator Method for Multidimensions 361
</p>
<p>f (tn+1)=
(
</p>
<p>1 &minus; α
2
Mx &minus;
</p>
<p>α
</p>
<p>2
My &minus;
</p>
<p>α
</p>
<p>2
Mz
</p>
<p>)&minus;1(
1 + α
</p>
<p>2
Mx +
</p>
<p>α
</p>
<p>2
My +
</p>
<p>α
</p>
<p>2
Mz
</p>
<p>)
f (t).
</p>
<p>(19.58)
</p>
<p>But now the matrices representing the operators Mx , My , Mz are not tridiagonal. To
keep the advantages of tridiagonal matrices we use the approximations
</p>
<p>(
1 + α
</p>
<p>2
Mx +
</p>
<p>α
</p>
<p>2
My +
</p>
<p>α
</p>
<p>2
Mz
</p>
<p>)
&asymp;
(
</p>
<p>1 + α
2
Mx
</p>
<p>)(
1 + α
</p>
<p>2
My
</p>
<p>)(
1 + α
</p>
<p>2
Mz
</p>
<p>)
</p>
<p>(19.59)(
1 &minus; α
</p>
<p>2
Mx &minus;
</p>
<p>α
</p>
<p>2
My &minus;
</p>
<p>α
</p>
<p>2
Mz
</p>
<p>)
&asymp;
(
</p>
<p>1 &minus; α
2
Mx
</p>
<p>)(
1 &minus; α
</p>
<p>2
My
</p>
<p>)(
1 &minus; α
</p>
<p>2
Mz
</p>
<p>)
</p>
<p>(19.60)
</p>
<p>and rearrange the factors to obtain
</p>
<p>f (tn+1)=
(
</p>
<p>1 &minus; α
2
Mx
</p>
<p>)&minus;1(
1 + α
</p>
<p>2
Mx
</p>
<p>)(
1 &minus; α
</p>
<p>2
My
</p>
<p>)&minus;1(
1 + α
</p>
<p>2
My
</p>
<p>)
</p>
<p>&times;
(
</p>
<p>1 &minus; α
2
Mz
</p>
<p>)&minus;1(
1 + α
</p>
<p>2
Mz
</p>
<p>)
f (tn) (19.61)
</p>
<p>which represents successive application of the 1-dimensional scheme for the three
directions separately. The last step was possible since the operators Mi and Mj for
different directions i �= j commute. For instance
</p>
<p>MxMyf =Mx
(
f (x, y +�x)+ f (x, y &minus;�x)&minus; 2f (x, y)
</p>
<p>)
</p>
<p>=
(
f (x +�x,y +�y)+ f (x &minus;�x,y +�x)
&minus; 2f (x, y +�x)+ f (x +�x,y &minus;�x)
+ f (x &minus;�x,y &minus;�x)&minus; 2f (x, y &minus;�x)
&minus; 2f (x +�x,y)&minus; 2f (x &minus;�x,y)+ 4f (x, y)
</p>
<p>)
</p>
<p>=MyMxf. (19.62)
</p>
<p>The Taylor series of (19.58) and (19.61) coincide up to second order with respect to
αMi :
(
</p>
<p>1 &minus; α
2
Mx &minus;
</p>
<p>α
</p>
<p>2
My &minus;
</p>
<p>α
</p>
<p>2
Mz
</p>
<p>)&minus;1(
1 + α
</p>
<p>2
Mx +
</p>
<p>α
</p>
<p>2
My +
</p>
<p>α
</p>
<p>2
Mz
</p>
<p>)
</p>
<p>= 1 + α(Mx +My +Mz)+
α2
</p>
<p>2
(Mx +My +Mz)2 +O
</p>
<p>(
α3
</p>
<p>)
(19.63)
</p>
<p>(
1 &minus; α
</p>
<p>2
Mx
</p>
<p>)&minus;1(
1 + α
</p>
<p>2
Mx
</p>
<p>)(
1 &minus; α
</p>
<p>2
My
</p>
<p>)&minus;1(
1 + α
</p>
<p>2
My
</p>
<p>)
</p>
<p>&times;
(
</p>
<p>1 &minus; α
2
Mz
</p>
<p>)&minus;1(
1 + α
</p>
<p>2
Mz
</p>
<p>)</p>
<p/>
</div>
<div class="page"><p/>
<p>362 19 Diffusion
</p>
<p>=
(
</p>
<p>1 + αMx +
α2M2x
</p>
<p>2
</p>
<p>)(
1 + αMy +
</p>
<p>α2M2y
</p>
<p>2
</p>
<p>)(
1 + αMz +
</p>
<p>α2M2z
</p>
<p>2
</p>
<p>)
+O
</p>
<p>(
α3
</p>
<p>)
</p>
<p>= 1 + α(Mx +My +Mz)+
α2
</p>
<p>2
(Mx +My +Mz)2 +O
</p>
<p>(
α3
</p>
<p>)
. (19.64)
</p>
<p>Hence we have
</p>
<p>fn+1 =
(
</p>
<p>1 +D�t
(
�+ �x
</p>
<p>2
</p>
<p>12
�2 + &middot; &middot; &middot;
</p>
<p>)
+ D
</p>
<p>2�t2
</p>
<p>2
</p>
<p>(
�2 + &middot; &middot; &middot;
</p>
<p>))
fn
</p>
<p>+
(
</p>
<p>1 + D�t
2
</p>
<p>�+ &middot; &middot; &middot;
)
Sn+1 + Sn
</p>
<p>2
�t
</p>
<p>= fn +�t(D�fn + Sn)+
�t2
</p>
<p>2
</p>
<p>(
D2�2 +D�Sn + Ṡn
</p>
<p>)
</p>
<p>+O
(
�t�x2,�t3
</p>
<p>)
(19.65)
</p>
<p>and the error order is conserved by the split operator method.
</p>
<p>19.4 Problems
</p>
<p>Problem 19.1 (Diffusion in 2 dimensions) In this computer experiment we solve
the diffusion equation on a two dimensional grid for
</p>
<p>&bull; an initial distribution f (t = 0, x, y)= δx,0δy,0
&bull; a constant source f (t = 0)= 0, S(t, x, y)= δx,0δy,0
Compare implicit, explicit and Crank-Nicolson method.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 20
</p>
<p>Nonlinear Systems
</p>
<p>Nonlinear problems [123, 145] are of interest to physicists, mathematicians and
also engineers. Nonlinear equations are difficult to solve and give rise to interesting
phenomena like indeterministic behavior, multistability or formation of patterns in
time and space. In the following we discuss recurrence relations like an iterated
function [135]
</p>
<p>xn+1 = f (xn) (20.1)
systems of ordinary differential equations like population dynamics models [178,
214, 245]
</p>
<p>ẋ(t)= f (x, y)
ẏ(t)= g(x, y)
</p>
<p>(20.2)
</p>
<p>or partial differential equations like the reaction-diffusion equation [83, 112, 178]
</p>
<p>&part;
</p>
<p>&part;t
c(x, t)=D &part;
</p>
<p>2
</p>
<p>&part;x2
c(x, t)+ f (c) (20.3)
</p>
<p>where f and g are nonlinear in the mathematical sense.1 We discuss fixed points of
the logistic mapping and analyze their stability. A bifurcation diagram visualizes the
appearance of period doubling and chaotic behavior as a function of a control pa-
rameter. The Lyapunov exponent helps to distinguish stable fixed points and periods
from chaotic regions. For continuous-time models, the iterated function is replaced
by a system of differential equations. For stable equilibria all eigenvalues of the Ja-
cobian matrix must have a negative real part. We discuss the Lotka-Volterra model,
which is the simplest model of predator-prey interactions and the Holling-Tanner
model, which incorporates functional response. Finally we allow for spatial inho-
mogeneity and include diffusive terms to obtain reaction-diffusion systems, which
show the phenomena of traveling waves and pattern formation. Computer experi-
ments study orbits and bifurcation diagram of the logistic map, periodic oscillations
</p>
<p>1Linear functions are additive f (x + y)= f (x)+ f (y) and homogeneous f (αx)= αf (x).
</p>
<p>P.O.J. Scherer, Computational Physics, Graduate Texts in Physics,
DOI 10.1007/978-3-319-00401-3_20,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>363</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-00401-3_20">http://dx.doi.org/10.1007/978-3-319-00401-3_20</a></div>
</div>
<div class="page"><p/>
<p>364 20 Nonlinear Systems
</p>
<p>Fig. 20.1 (Orbit of an
iterated function) The
sequence of points
(xi , xi+1), (xi+1, xi+1) is
plotted together with the
curves y = f (x) (dashed)
and y = x (dotted)
</p>
<p>of the Lotka-Volterra model, oscillations and limit cycles of the Holling-Tanner
model and finally pattern formation in the diffusive Lotka-Volterra model.
</p>
<p>20.1 Iterated Functions
</p>
<p>Starting from an initial value x0 a function f is iterated repeatedly
</p>
<p>x1 = f (x0)
x2 = f (x1)
...
</p>
<p>xi+1 = f (xi).
</p>
<p>(20.4)
</p>
<p>The sequence of function values x0, x1 &middot; &middot; &middot; is called the orbit of x0. It can be visual-
ized in a 2-dimensional plot by connecting the points
</p>
<p>(x0, x1)&rarr; (x1, x1)&rarr; (x1, x2)&rarr; (x2, x2)&rarr; &middot;&middot; &middot;&rarr; (xi, xi+1)&rarr; (xi+1, xi+1)
by straight lines (Fig. 20.1).
</p>
<p>20.1.1 Fixed Points and Stability
</p>
<p>If the equation
</p>
<p>x&lowast; = f
(
x&lowast;
</p>
<p>)
(20.5)
</p>
<p>has solutions x&lowast;, then these are called fixed points. Consider a point in the vicinity
of a fixed point
</p>
<p>x = x&lowast; + ε0 (20.6)
and make a Taylor series expansion</p>
<p/>
</div>
<div class="page"><p/>
<p>20.1 Iterated Functions 365
</p>
<p>Fig. 20.2 (Attractive fixed
point) The orbit of an
attractive fixed point
converges to the intersection
of the curves y = x and
y = f (x)
</p>
<p>f (x)= f
(
x&lowast; + ε0
</p>
<p>)
= f
</p>
<p>(
x&lowast;
</p>
<p>)
+ ε0f &prime;
</p>
<p>(
x&lowast;
</p>
<p>)
+ &middot; &middot; &middot; = x&lowast; + ε1 + &middot; &middot; &middot; (20.7)
</p>
<p>with the notation
</p>
<p>ε1 = ε0f &prime;
(
x&lowast;
</p>
<p>)
. (20.8)
</p>
<p>Repeated iteration gives2
</p>
<p>f (2)(x)= f
(
f (x)
</p>
<p>)
= f
</p>
<p>(
x&lowast; + ε1
</p>
<p>)
+ &middot; &middot; &middot; = x&lowast; + ε1f &prime;
</p>
<p>(
x&lowast;
</p>
<p>)
= x&lowast; + ε2
</p>
<p>...
</p>
<p>f (n)
(
x&lowast;
</p>
<p>)
= x&lowast; + εn
</p>
<p>(20.9)
</p>
<p>with the sequence of deviations
</p>
<p>εn = f &prime;
(
x&lowast;
</p>
<p>)
εn&minus;1 = &middot; &middot; &middot; =
</p>
<p>(
f &prime;
(
x&lowast;
</p>
<p>))n
ε0.
</p>
<p>The orbit moves away from the fixed point for arbitrarily small ε0 if |f &prime;(x&lowast;)| &gt; 1
whereas the fixed point is attractive for |f &prime;(x&lowast;)|&lt; 1 (Fig. 20.2).
</p>
<p>Higher order fixed points are defined by iterating f (x) several times. A fixed
point of order n simultaneously solves
</p>
<p>f
(
x&lowast;
</p>
<p>)
�= x&lowast;
</p>
<p>f (2)
(
x&lowast;
</p>
<p>)
�= x&lowast;
</p>
<p>f (n&minus;1)
(
x&lowast;
</p>
<p>)
�= x&lowast;
</p>
<p>f (n)
(
x&lowast;
</p>
<p>)
= x&lowast;.
</p>
<p>(20.10)
</p>
<p>The iterated function values cycle periodically (Fig. 20.3) through
</p>
<p>x&lowast; &rarr; f
(
x&lowast;
</p>
<p>)
&rarr; f (2)
</p>
<p>(
x&lowast;
</p>
<p>)
&middot; &middot; &middot;f (n&minus;1)
</p>
<p>(
x&lowast;
</p>
<p>)
.
</p>
<p>This period is attractive if
∣∣f &prime;
</p>
<p>(
x&lowast;
</p>
<p>)
f &prime;
(
f
(
x&lowast;
</p>
<p>))
f &prime;
(
f (2)
</p>
<p>(
x&lowast;
</p>
<p>))
&middot; &middot; &middot;f &prime;
</p>
<p>(
f (n&minus;1)
</p>
<p>(
x&lowast;
</p>
<p>))∣∣&lt; 1.
</p>
<p>2Here and in the following f (n) denotes an iterated function, not a derivative.</p>
<p/>
</div>
<div class="page"><p/>
<p>366 20 Nonlinear Systems
</p>
<p>Fig. 20.3 (Periodic orbit)
The orbit of an attractive
fourth order fixed point cycles
through the values
x1 = f (x4), x2 = f (x1),
x3 = f (x2), x4 = f (x3)
</p>
<p>20.1.2 The Lyapunov Exponent
</p>
<p>Consider two neighboring orbits with initial values x0 and x0 +ε0. After n iterations
the distance is
</p>
<p>∣∣f
(
f
(
&middot; &middot; &middot;f (x0)
</p>
<p>))
&minus; f
</p>
<p>(
f
(
&middot; &middot; &middot;f (x0 + ε0)
</p>
<p>))∣∣= |ε0|eλn (20.11)
with the so called Lyapunov exponent [161] λ which is useful to characterize the
orbit. The Lyapunov exponent can be determined from
</p>
<p>λ= lim
n&rarr;&infin;
</p>
<p>1
</p>
<p>n
ln
</p>
<p>( |f (n)(x0 + ε0)&minus; f (n)(x0)|
|ε0|
</p>
<p>)
(20.12)
</p>
<p>or numerically easier with the approximation
∣∣f (x0 + ε0)&minus; f (x0)
</p>
<p>∣∣= |ε0|
∣∣f &prime;(x0)
</p>
<p>∣∣
∣∣f
</p>
<p>(
f (x0 + ε0)
</p>
<p>)
&minus; f
</p>
<p>(
f (x0)
</p>
<p>)∣∣=
∣∣(f (x0 + ε0)&minus; f (x0)
</p>
<p>)∣∣∣∣f &prime;(x0 + ε0)
∣∣
</p>
<p>= |ε0|
∣∣f &prime;(x0)
</p>
<p>∣∣∣∣f &prime;(x0 + ε0)
∣∣ (20.13)
</p>
<p>∣∣f (n)(x0 + ε0)&minus; f (n)(x0)
∣∣= |ε0|
</p>
<p>∣∣f &prime;(x0)
∣∣∣∣f &prime;(x1)
</p>
<p>∣∣ &middot; &middot; &middot;
∣∣f &prime;(xn&minus;1)
</p>
<p>∣∣ (20.14)
from
</p>
<p>λ= lim
n&rarr;&infin;
</p>
<p>1
</p>
<p>n
</p>
<p>n&minus;1&sum;
</p>
<p>i=0
ln
∣∣f &prime;(xi)
</p>
<p>∣∣. (20.15)
</p>
<p>For a stable fixed point
</p>
<p>λ&rarr; ln
∣∣f &prime;
</p>
<p>(
x&lowast;
</p>
<p>)∣∣&lt; 0 (20.16)
and for an attractive period
</p>
<p>λ&rarr; ln
∣∣f &prime;
</p>
<p>(
x&lowast;
</p>
<p>)
f &prime;
(
f
(
x&lowast;
</p>
<p>))
. . . f &prime;
</p>
<p>(
f (n&minus;1)
</p>
<p>(
x&lowast;
</p>
<p>))∣∣&lt; 0. (20.17)
Orbits with λ &lt; 0 are attractive fixed points or periods. If, on the other hand, λ &gt; 0,
the orbit is irregular and very sensitive to the initial conditions, hence is chaotic.</p>
<p/>
</div>
<div class="page"><p/>
<p>20.1 Iterated Functions 367
</p>
<p>Fig. 20.4 (Reproduction rate
of the logistic model) At low
densities the growth rate has
its maximum value r0. At
larger densities the growth
rate declines and reaches
r = 0 for N =K . The
parameter K is called
carrying capacity
</p>
<p>20.1.3 The Logistic Map
</p>
<p>A population of animals is observed yearly. The evolution of the population density
N is described in terms of the reproduction rate r by the recurrence relation
</p>
<p>Nn+1 = r Nn (20.18)
where Nn is the population density in year number n. If r is constant, an exponential
increase or decrease of N results.
</p>
<p>The simplest model for the growth of a population which takes into account that
the resources are limited is the logistic model by Verhulst [264]. He assumed that the
reproduction rate r depends on the population density N in a simple way (Fig. 20.4)
</p>
<p>r = r0
(
</p>
<p>1 &minus; N
K
</p>
<p>)
. (20.19)
</p>
<p>The Verhulst model (20.19) leads to the iterated nonlinear function
</p>
<p>Nn+1 = r0Nn &minus;
r0
</p>
<p>K
N2n (20.20)
</p>
<p>with r0 &gt; 0, K &gt; 0. We denote the quotient of population density and carrying
capacity by the new variable
</p>
<p>xn =
1
</p>
<p>K
Nn (20.21)
</p>
<p>and obtain an equation with only one parameter, the so called logistic mapping
</p>
<p>xn+1 =
1
</p>
<p>K
Nn+1 =
</p>
<p>1
</p>
<p>K
r0Nn
</p>
<p>(
1 &minus; Nn
</p>
<p>K
</p>
<p>)
= r0 &middot; xn &middot; (1 &minus; xn). (20.22)
</p>
<p>20.1.4 Fixed Points of the Logistic Map
</p>
<p>Consider an initial point in the interval</p>
<p/>
</div>
<div class="page"><p/>
<p>368 20 Nonlinear Systems
</p>
<p>0 &lt; x0 &lt; 1. (20.23)
</p>
<p>We want to find conditions on r to keep the orbit in this interval. The maximum
value of xn+1 is found from
</p>
<p>dxn+1
dxn
</p>
<p>= r(1 &minus; 2xn)= 0 (20.24)
</p>
<p>which gives xn = 1/2 and max(xn+1)= r/4. If r &gt; 4 then negative xn appear after
some iterations and the orbit is not bound by a finite interval since
</p>
<p>|xn+1|
|xn|
</p>
<p>= |r|
(
1 + |xn|
</p>
<p>)
&gt; 1. (20.25)
</p>
<p>The fixed point equation
</p>
<p>x&lowast; = rx&lowast; &minus; rx&lowast;2 (20.26)
always has the trivial solution
</p>
<p>x&lowast; = 0 (20.27)
and a further solution
</p>
<p>x&lowast; = 1 &minus; 1
r
</p>
<p>(20.28)
</p>
<p>which is only physically reasonable for r &gt; 1, since x should be a positive quantity.
For the logistic mapping the derivative is
</p>
<p>f &prime;(x)= r &minus; 2rx (20.29)
which for the first fixed point x&lowast; = 0 gives |f &prime;(0)| = r . This fixed point is attractive
for 0 &lt; r &lt; 1 and becomes unstable for r &gt; 1. For the second fixed point we have
|f &prime;(1&minus; 1
</p>
<p>r
)| = |2&minus; r|, which is smaller than one in the interval 1 &lt; r &lt; 3. For r &lt; 1
</p>
<p>no such fixed point exists. Finally, for r1 = 3 the first bifurcation appears and higher
order fixed points become stable.
</p>
<p>Consider the fixed point of the double iteration
</p>
<p>x&lowast; = r
(
r
(
x&lowast; &minus; x&lowast;2
</p>
<p>)
&minus; r2
</p>
<p>(
x&lowast; &minus; x&lowast;2
</p>
<p>)2)
. (20.30)
</p>
<p>All roots of this fourth order equation can be found since we already know two of
them. The remaining roots are
</p>
<p>x&lowast;1,2 =
r+1
</p>
<p>2 &plusmn;
&radic;
r2 &minus; 2r &minus; 3
r
</p>
<p>. (20.31)
</p>
<p>They are real valued if
</p>
<p>(r &minus; 1)2 &minus; 4 &gt; 0 &rarr; r &gt; 3 (or r &lt;&minus;1). (20.32)
For r &gt; 3 the orbit oscillates between x&lowast;1 and x
</p>
<p>&lowast;
2 until the next period doubling
</p>
<p>appears for r2 = 1 +
&radic;
</p>
<p>6. With increasing r more and more bifurcations appear and
finally the orbits become chaotic (Fig. 20.5).</p>
<p/>
</div>
<div class="page"><p/>
<p>20.1 Iterated Functions 369
</p>
<p>Fig. 20.5 (Orbits of the logistic map) Left: For 0 &lt; r &lt; 1 the logistic map has the attractive fixed
point x&lowast; = 0. Middle: In the region 1 &lt; r &lt; 3 this fixed point becomes unstable and another stable
fixed point is at x&lowast; = 1 &minus; 1/r . Right: For 3 &lt; r &lt; 1 +
</p>
<p>&radic;
6 the second order fixed point (20.31) is
</p>
<p>stable. For larger values of r more and more bifurcations appear
</p>
<p>Fig. 20.6 (Bifurcation diagram of the logistic map) For different values of r the function is it-
erated 1100 times. The first 1000 iterations are dropped to allow the trajectory to approach stable
fixed points or periods. The iterated function values x1000 &middot; &middot; &middot;x1100 are plotted in an r&ndash;x diagram
together with the estimate (20.15) of the Lyapunov exponent.
The first period doublings appear at r = 3 and r = 1 +
</p>
<p>&radic;
6. For larger values chaotic behavior
</p>
<p>is observed and the estimated Lyapunov exponent becomes positive. In some regions motion is
regular again with negative Lyapunov exponent
</p>
<p>20.1.5 Bifurcation Diagram
</p>
<p>The bifurcation diagram visualizes the appearance of period doubling and chaotic
behavior as a function of the control parameter r (Fig. 20.6).</p>
<p/>
</div>
<div class="page"><p/>
<p>370 20 Nonlinear Systems
</p>
<p>20.2 Population Dynamics
</p>
<p>If time is treated as a continuous variable, the iterated function has to be replaced
by a differential equation
</p>
<p>dN
</p>
<p>dt
= f (N) (20.33)
</p>
<p>or, more generally by a system of equations
</p>
<p>d
</p>
<p>dt
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>N1
N2
...
</p>
<p>Nn
</p>
<p>⎞
⎟⎟⎟⎠=
</p>
<p>⎛
⎜⎜⎝
</p>
<p>f1(N1 &middot; &middot; &middot;Nn)
f2(N1 &middot; &middot; &middot;Nn)
</p>
<p>fn(N1 &middot; &middot; &middot;Nn)
</p>
<p>⎞
⎟⎟⎠ . (20.34)
</p>
<p>20.2.1 Equilibria and Stability
</p>
<p>The role of the fixed points is now taken over by equilibria, which are solutions of
</p>
<p>0 = dN
dt
</p>
<p>= f (Neq) (20.35)
</p>
<p>which means roots of f (N). Let us investigate small deviations from equilibrium
with the help of a Taylor series expansion. Inserting
</p>
<p>N =Neq + ξ (20.36)
we obtain
</p>
<p>dξ
</p>
<p>dt
= f (Neq)+ f &prime;(Neq)ξ + &middot; &middot; &middot; (20.37)
</p>
<p>but since f (Neq)= 0, we have approximately
dξ
</p>
<p>dt
= f &prime;(Neq)ξ (20.38)
</p>
<p>with the solution
</p>
<p>ξ(t)= ξ0 exp
{
f &prime;(Neq)t
</p>
<p>}
. (20.39)
</p>
<p>The equilibrium is only stable if Ref &prime;(Neq) &lt; 0, since then small deviations
disappear exponentially. For Ref &prime;(Neq) &gt; 0 deviations will increase, but the expo-
nential behavior holds only for not too large deviations and saturation may appear.
If the derivative f &prime;(Neq) has a nonzero imaginary part then oscillations will be su-
perimposed. For a system of equations the equilibrium is defined by
</p>
<p>⎛
⎜⎜⎝
</p>
<p>f1(N
eq
</p>
<p>1 &middot; &middot; &middot;N
eq
n )
</p>
<p>f2(N
eq
</p>
<p>1 &middot; &middot; &middot;N
eq
n )
</p>
<p>fN (N
eq
</p>
<p>1 &middot; &middot; &middot;N
eq
n )
</p>
<p>⎞
⎟⎟⎠=
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>0
0
...
</p>
<p>0
</p>
<p>⎞
⎟⎟⎟⎠ (20.40)</p>
<p/>
</div>
<div class="page"><p/>
<p>20.2 Population Dynamics 371
</p>
<p>Fig. 20.7 (Equilibria of the
logistic model) The
equilibrium xeq = 0 is
unstable since an
infinitesimal deviation grows
exponentially in time. The
equilibrium xeq = 1 is stable
since initial deviations
disappear exponentially
</p>
<p>and if such an equilibrium exists, linearization gives
⎛
⎜⎜⎜⎝
</p>
<p>N1
N2
...
</p>
<p>Nn
</p>
<p>⎞
⎟⎟⎟⎠=
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>N
eq
</p>
<p>1
</p>
<p>N
eq
</p>
<p>2
...
</p>
<p>N
eq
n
</p>
<p>⎞
⎟⎟⎟⎠+
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>ξ1
ξ2
...
</p>
<p>ξn
</p>
<p>⎞
⎟⎟⎟⎠ (20.41)
</p>
<p>d
</p>
<p>dt
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>ξ1
ξ2
...
</p>
<p>ξN
</p>
<p>⎞
⎟⎟⎟⎠=
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>&part;f1
&part;N1
</p>
<p>&part;f1
&part;N2
</p>
<p>&middot; &middot; &middot; &part;f1
&part;Nn
</p>
<p>&part;f2
&part;N1
</p>
<p>&part;f2
&part;N2
</p>
<p>&middot; &middot; &middot; &part;f2
&part;Nn
</p>
<p>...
...
</p>
<p>. . .
...
</p>
<p>&part;fn
&part;N1
</p>
<p>&part;fn
&part;N2
</p>
<p>&middot; &middot; &middot; &part;fn
&part;Nn
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>ξ1
ξ2
...
</p>
<p>ξn
</p>
<p>⎞
⎟⎟⎟⎠ . (20.42)
</p>
<p>The equilibrium is stable if all eigenvalues λi of the derivative matrix have a negative
real part.
</p>
<p>20.2.2 The Continuous Logistic Model
</p>
<p>The continuous logistic model describes the evolution by the differential equation
</p>
<p>dx
</p>
<p>dt
= r0x(1 &minus; x). (20.43)
</p>
<p>To find possible equilibria we have to solve
</p>
<p>xeq(1 &minus; xeq)= 0 (20.44)
which has the two roots xeq = 0 and xeq = 1 (Fig. 20.7).
</p>
<p>The derivative f &prime; is
</p>
<p>f &prime;(x)= d
dx
</p>
<p>(
r0x(1 &minus; x)
</p>
<p>)
= r0(1 &minus; 2x). (20.45)
</p>
<p>Since f &prime;(0)= r0 &gt; 0 and f &prime;(1)=&minus;r0 &lt; 0 only the second equilibrium is stable.</p>
<p/>
</div>
<div class="page"><p/>
<p>372 20 Nonlinear Systems
</p>
<p>20.3 Lotka-Volterra Model
</p>
<p>The model by Lotka [160] and Volterra [266] is the simplest model of predator-
prey interactions. It has two variables, the density of prey (H) and the density of
predators (P). The overall reproduction rate of each species is given by the difference
of the birth rate r and the mortality rate m
</p>
<p>dN
</p>
<p>dt
= (r &minus;m)N
</p>
<p>which both may depend on the population densities. The Lotka-Volterra model as-
sumes that the prey mortality depends linearly on the predator density and the preda-
tor birth rate is proportional to the prey density
</p>
<p>mH = aP rP = bH (20.46)
</p>
<p>where a is the predation rate coefficient and b is the reproduction rate of preda-
tors per 1 prey eaten. Together we end up with a system of two coupled nonlinear
differential equations
</p>
<p>dH
</p>
<p>dt
= f (H,P )= rHH &minus; aHP
</p>
<p>dP
</p>
<p>dt
= g(H,P )= bHP &minus;mPP
</p>
<p>(20.47)
</p>
<p>where rH is the intrinsic rate of prey population increase and mP the predator mor-
tality rate.
</p>
<p>20.3.1 Stability Analysis
</p>
<p>To find equilibria we have to solve the system of equations
</p>
<p>f (H,P )= rHH &minus; aHP = 0
g(H,P )= bHP &minus;mPP = 0.
</p>
<p>(20.48)
</p>
<p>The first equation is solved by Heq = 0 or by Peq = rH /a. The second equation is
solved by Peq = 0 or by Heq =mP /b. Hence there are two equilibria, the trivial one
</p>
<p>Peq =Heq = 0 (20.49)
</p>
<p>and a nontrivial one
</p>
<p>Peq =
rH
</p>
<p>a
Heq =
</p>
<p>mP
</p>
<p>b
. (20.50)
</p>
<p>Linearization around the zero equilibrium gives
</p>
<p>dH
</p>
<p>dt
= rHH + &middot; &middot; &middot;
</p>
<p>dP
</p>
<p>dt
=&minus;mPP + &middot; &middot; &middot; . (20.51)</p>
<p/>
</div>
<div class="page"><p/>
<p>20.4 Functional Response 373
</p>
<p>This equilibrium is unstable since a small prey population will increase exponen-
tially. Now expand around the nontrivial equilibrium:
</p>
<p>P = Peq + ξ, H =Heq + η (20.52)
dη
</p>
<p>dt
= &part;f
</p>
<p>&part;H
η+ &part;f
</p>
<p>&part;P
ξ = (rH &minus; aPeq)η&minus; aHeqξ =&minus;
</p>
<p>amP
</p>
<p>b
ξ (20.53)
</p>
<p>dξ
</p>
<p>dt
= &part;g
</p>
<p>&part;H
η+ &part;g
</p>
<p>&part;P
ξ = bPeqη+ (bHeq &minus;mP )ξ =
</p>
<p>brH
</p>
<p>a
η (20.54)
</p>
<p>or in matrix notation
</p>
<p>d
</p>
<p>dt
</p>
<p>(
η
</p>
<p>ξ
</p>
<p>)
=
(
</p>
<p>0 &minus; amP
b
</p>
<p>brH
a
</p>
<p>0
</p>
<p>)(
η
</p>
<p>ξ
</p>
<p>)
. (20.55)
</p>
<p>The eigenvalues are purely imaginary
</p>
<p>λ=&plusmn;i&radic;mH rP =&plusmn;iω (20.56)
and the corresponding eigenvectors are
</p>
<p>(
i
&radic;
mH rP
</p>
<p>brH /a
</p>
<p>)
,
</p>
<p>(
amP /b
</p>
<p>i
&radic;
mH rP
</p>
<p>)
. (20.57)
</p>
<p>The solution of the linearized equations is then given by
</p>
<p>ξ(t)= ξ0 cosωt +
b
</p>
<p>a
</p>
<p>&radic;
rP
</p>
<p>mH
η0 sinωt
</p>
<p>η(t)= η0 cosωt &minus;
a
</p>
<p>b
</p>
<p>&radic;
mH
</p>
<p>rP
ξ0 sinωt
</p>
<p>(20.58)
</p>
<p>which describes an ellipse in the ξ&ndash;η plane (Fig. 20.8). The nonlinear equations
(20.48) have a first integral
</p>
<p>rH lnP(t)&minus; aP (t)&minus; bH(t)+mP lnH(t)= C (20.59)
and therefore the motion in the H&ndash;P plane is on a closed curve around the equilib-
rium which approaches an ellipse for small amplitudes ξ, η.
</p>
<p>20.4 Functional Response
</p>
<p>Holling [124, 125] studied predation of small mammals on pine sawflies. He sug-
gested a very popular model of functional response. Holling assumed that the preda-
tor spends its time on two kinds of activities, searching for prey and prey handling
(chasing, killing, eating, digesting). The total time equals the sum of time spent on
searching and time spent on handling
</p>
<p>T = Tsearch + Thandling. (20.60)
Capturing prey is assumed to be a random process. A predator examines an area
</p>
<p>α per time and captures all prey found there. After spending the time Tsearch the</p>
<p/>
</div>
<div class="page"><p/>
<p>374 20 Nonlinear Systems
</p>
<p>Fig. 20.8 (Lotka-Volterra model) The predator and prey population densities show periodic os-
cillations (right). In the H&ndash;P plane the system moves on a closed curve, which becomes an ellipse
for small deviations from equilibrium (left)
</p>
<p>Fig. 20.9 Functional
response of Holling&rsquo;s model
</p>
<p>predator examined an area of αTsearch and captured HT = HαTsearch prey. Hence
the predation rate is
</p>
<p>a = HT
HT
</p>
<p>= αTsearch
T
</p>
<p>= α 1
1 + Thandling/Tsearch
</p>
<p>. (20.61)
</p>
<p>The handling time is assumed to be proportional to the number of prey captured
</p>
<p>Thandling = ThHαTsearch (20.62)
</p>
<p>where Th is the handling time spent per one prey. The predation rate then is given
by
</p>
<p>a = α
1 + αHTh
</p>
<p>. (20.63)
</p>
<p>At small densities handling time is unimportant and the predation rate is a0 = α
whereas at high prey density handling limits the number of prey captured and the
predation rate approaches a&infin; = 1HTh (Fig. 20.9).</p>
<p/>
</div>
<div class="page"><p/>
<p>20.4 Functional Response 375
</p>
<p>20.4.1 Holling-Tanner Model
</p>
<p>We combine the logistic model with Holling&rsquo;s model for the predation rate [124,
125, 249]
</p>
<p>dH
</p>
<p>dt
= rHH
</p>
<p>(
1 &minus; H
</p>
<p>KH
</p>
<p>)
&minus; aHP
</p>
<p>= rHH
(
</p>
<p>1 &minus; H
KH
</p>
<p>)
&minus; α
</p>
<p>1 + αHTh
HP = f (H,P ) (20.64)
</p>
<p>and assume that the carrying capacity of the predator is proportional to the density
of prey
</p>
<p>dP
</p>
<p>dt
= rPP
</p>
<p>(
1 &minus; P
</p>
<p>KP
</p>
<p>)
= rPP
</p>
<p>(
1 &minus; P
</p>
<p>kH
</p>
<p>)
= g(H,P ). (20.65)
</p>
<p>Obviously there is a trivial equilibrium with Peq =Heq = 0. Linearization gives
dH
</p>
<p>dt
= rHH + &middot; &middot; &middot;
</p>
<p>dP
</p>
<p>dt
= rPP + &middot; &middot; &middot; (20.66)
</p>
<p>which shows that this equilibrium is unstable. There is another trivial equilibrium
with Peq = 0, Heq =KH . Here we find:
</p>
<p>dη
</p>
<p>dt
= rH (KH + η)
</p>
<p>(
1 &minus; KH + η
</p>
<p>KH
</p>
<p>)
&minus; α
</p>
<p>1 + α(KH + η)Th
(KH + η)ξ
</p>
<p>= rHη&minus;
α
</p>
<p>1 + αKHTh
KH ξ + &middot; &middot; &middot; (20.67)
</p>
<p>dξ
</p>
<p>dt
= rP ξ
</p>
<p>(
1 &minus; ξ
</p>
<p>k(KH + η)
</p>
<p>)
= rP ξ + &middot; &middot; &middot; (20.68)
</p>
<p>(
η̇
</p>
<p>ξ̇
</p>
<p>)
=
(
rH &minus; α1+αKH ThKH
0 rP
</p>
<p>)(
η
</p>
<p>ξ
</p>
<p>)
(20.69)
</p>
<p>λ= rH , rP . (20.70)
Let us now look for nontrivial equilibria. The nullclines (Fig. 20.10) are the curves
defined by dHdt = 0 and
</p>
<p>dP
dt = 0, hence by
</p>
<p>P = rH
α
</p>
<p>(
1 &minus; H
</p>
<p>KH
</p>
<p>)
(1 + αHTh) (20.71)
</p>
<p>P = kH. (20.72)
The H -nullcline is a parabola at
</p>
<p>Hm =
αTh &minus;K&minus;1H
2αThK
</p>
<p>&minus;1
H
</p>
<p>Pm =
(αTh +K&minus;1H )2
</p>
<p>4αThK
&minus;1
H
</p>
<p>&gt; 0. (20.73)
</p>
<p>It intersects the H -axis at H =KH and H =&minus;1/αTh and the P -axis at P = rH /α.
There is one intersection of the two nullclines at positive values of H and P which</p>
<p/>
</div>
<div class="page"><p/>
<p>376 20 Nonlinear Systems
</p>
<p>Fig. 20.10 Nullclines of the
predator-prey model
</p>
<p>corresponds to a nontrivial equilibrium. The equilibrium density Heq is the positive
root of
</p>
<p>rHαThH
2
eq + (rH + αkKH &minus; rHKHαTh)Heq &minus; rHKH = 0. (20.74)
</p>
<p>It is explicitly given by
</p>
<p>Heq =&minus;
rH + αkKH &minus; rHKHαTh
</p>
<p>2rHαTh
</p>
<p>+
</p>
<p>&radic;
(rH + αkKH &minus; rHKHαTh)2 + 4r2HαThKH
</p>
<p>2rHαTh
. (20.75)
</p>
<p>The prey density then follows from
</p>
<p>Peq =Heqk. (20.76)
The matrix of derivatives has the elements
</p>
<p>mHP =
&part;f
</p>
<p>&part;P
=&minus; αHeq
</p>
<p>1 + αThHeq
</p>
<p>mHH =
&part;f
</p>
<p>&part;H
= rH
</p>
<p>(
1 &minus; 2Heq
</p>
<p>KH
</p>
<p>)
&minus; αkHeq
</p>
<p>1 + αThH
+
</p>
<p>α2H 2eqkTh
</p>
<p>(1 + αThHeq)2
</p>
<p>mPP =
&part;g
</p>
<p>&part;P
=&minus;rP
</p>
<p>mPH =
&part;g
</p>
<p>&part;H
= rP k
</p>
<p>(20.77)
</p>
<p>from which the eigenvalues are calculated as
</p>
<p>λ= mHH +mPP
2
</p>
<p>&plusmn;
</p>
<p>&radic;
(mHH +mPP)2
</p>
<p>4
&minus; (mHHmPP &minus;mHPmPH). (20.78)
</p>
<p>Oscillations appear, if the squareroot is imaginary (Fig. 20.11).</p>
<p/>
</div>
<div class="page"><p/>
<p>20.4 Functional Response 377
</p>
<p>Fig. 20.11 (Holling-Tanner model) Top: evolution from an unstable equilibrium to a limit cycle,
middle: a stable equilibrium is approached with oscillations, bottom: stable equilibrium without
oscillations</p>
<p/>
</div>
<div class="page"><p/>
<p>378 20 Nonlinear Systems
</p>
<p>20.5 Reaction-Diffusion Systems
</p>
<p>So far we considered spatially homogeneous systems where the density of a popula-
tion or the concentration of a chemical agent depend only on time. If we add spatial
inhomogeneity and diffusive motion, new and interesting phenomena like pattern
formation or traveling excitations can be observed.
</p>
<p>20.5.1 General Properties of Reaction-Diffusion Systems
</p>
<p>Reaction-diffusion systems are described by a diffusion equation3 where the source
term depends nonlinearly on the concentrations
</p>
<p>&part;
</p>
<p>&part;t
</p>
<p>⎛
⎜⎝
</p>
<p>c1
...
</p>
<p>cN
</p>
<p>⎞
⎟⎠=
</p>
<p>⎛
⎜⎝
D1
</p>
<p>. . .
</p>
<p>DN
</p>
<p>⎞
⎟⎠�
</p>
<p>⎛
⎜⎝
</p>
<p>c1
...
</p>
<p>cN
</p>
<p>⎞
⎟⎠+
</p>
<p>⎛
⎜⎝
</p>
<p>F1({c})
...
</p>
<p>FN ({c})
</p>
<p>⎞
⎟⎠ . (20.79)
</p>
<p>20.5.2 Chemical Reactions
</p>
<p>Consider a number of chemical reactions which are described by stoichiometric
equations
</p>
<p>&sum;
</p>
<p>i
</p>
<p>νiAi = 0. (20.80)
</p>
<p>The concentration of agent Ai is
</p>
<p>ci = ci,0 + νix (20.81)
with the reaction variable
</p>
<p>x = ci &minus; ci,0
νi
</p>
<p>(20.82)
</p>
<p>and the reaction rate
</p>
<p>r = dx
dt
</p>
<p>= 1
νi
</p>
<p>dci
dt
</p>
<p>(20.83)
</p>
<p>which, in general is a nonlinear function of all concentrations. The total concentra-
tion change due to diffusion and reactions is given by
</p>
<p>&part;
</p>
<p>&part;t
ck =Dk�ck +
</p>
<p>&sum;
</p>
<p>j
</p>
<p>νkj rj =Dk�ck + Fk
(
{ci}
</p>
<p>)
. (20.84)
</p>
<p>3We consider only the case, that different species diffuse independently and that the diffusion
constants do not depend on direction.</p>
<p/>
</div>
<div class="page"><p/>
<p>20.5 Reaction-Diffusion Systems 379
</p>
<p>20.5.3 Diffusive Population Dynamics
</p>
<p>Combination of population dynamics (20.2) and diffusive motion gives a similar set
of coupled equations for the population densities
</p>
<p>&part;
</p>
<p>&part;t
Nk =Dk�Nk + fk(N1,N2 . . .Nn). (20.85)
</p>
<p>20.5.4 Stability Analysis
</p>
<p>Since a solution of the nonlinear equations is not generally possible we discuss small
deviations from an equilibrium solution Neqk
</p>
<p>4 with
</p>
<p>&part;
</p>
<p>&part;t
Nk =�Nk = 0. (20.86)
</p>
<p>Obviously the equilibrium obeys
</p>
<p>fk(N1 &middot; &middot; &middot;Nn)= 0 k = 1,2 &middot; &middot; &middot;n. (20.87)
</p>
<p>We linearize the equations by setting
</p>
<p>Nk =Neqk + ξk (20.88)
</p>
<p>and expand around the equilibrium
</p>
<p>&part;
</p>
<p>&part;t
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>ξ1
ξ2
...
</p>
<p>ξn
</p>
<p>⎞
⎟⎟⎟⎠=
</p>
<p>⎛
⎜⎝
D1
</p>
<p>. . .
</p>
<p>DN
</p>
<p>⎞
⎟⎠
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>�ξ1
�ξ2
...
</p>
<p>�ξn
</p>
<p>⎞
⎟⎟⎟⎠
</p>
<p>+
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>&part;f1
&part;N1
</p>
<p>&part;f1
&part;N2
</p>
<p>&middot; &middot; &middot; &part;f1
&part;Nn
</p>
<p>&part;f2
&part;N1
</p>
<p>&part;f2
&part;N2
</p>
<p>&middot; &middot; &middot; &part;f2
&part;Nn
</p>
<p>...
...
</p>
<p>. . .
...
</p>
<p>&part;fn
&part;N1
</p>
<p>&part;fn
&part;N2
</p>
<p>&middot; &middot; &middot; &part;fn
&part;Nn
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>ξ1
ξ2
...
</p>
<p>ξn
</p>
<p>⎞
⎟⎟⎟⎠+ &middot; &middot; &middot; . (20.89)
</p>
<p>Plane waves are solutions of the linearized problem.5 Using the ansatz
</p>
<p>ξj = ξj,0ei(ωt&minus;kx) (20.90)
</p>
<p>we obtain
</p>
<p>4We assume tacitly that such a solution exists.
5Strictly this is true only for an infinite or periodic system.</p>
<p/>
</div>
<div class="page"><p/>
<p>380 20 Nonlinear Systems
</p>
<p>iω
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>ξ1
ξ2
...
</p>
<p>ξn
</p>
<p>⎞
⎟⎟⎟⎠=&minus;k
</p>
<p>2D
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>ξ1
ξ2
...
</p>
<p>ξn
</p>
<p>⎞
⎟⎟⎟⎠+M0
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>ξ1
ξ2
...
</p>
<p>ξn
</p>
<p>⎞
⎟⎟⎟⎠ (20.91)
</p>
<p>where M0 denotes the matrix of derivatives and D the matrix of diffusion constants.
For a stable plane wave solution λ= iω is an eigenvalue of
</p>
<p>Mk =M0 &minus; k2D (20.92)
</p>
<p>with
</p>
<p>&real;(λ)&le; 0. (20.93)
</p>
<p>If there are purely imaginary eigenvalues for some k they correspond to stable solu-
tions which are spatially inhomogeneous and lead to formation of certain patterns.
Interestingly, diffusion can lead to instabilities even for a system which is stable in
the absence of diffusion [258].
</p>
<p>20.5.5 Lotka-Volterra Model with Diffusion
</p>
<p>As a simple example we consider again the Lotka-Volterra model. Adding diffusive
terms we obtain the equations
</p>
<p>&part;
</p>
<p>&part;t
</p>
<p>(
H
</p>
<p>P
</p>
<p>)
=
(
rHH &minus; aHP
bHP &minus;mPP
</p>
<p>)
+
(
DH
</p>
<p>DP
</p>
<p>)
�
</p>
<p>(
H
</p>
<p>P
</p>
<p>)
. (20.94)
</p>
<p>There are two equilibria
</p>
<p>Heq = Peq = 0 (20.95)
</p>
<p>and
</p>
<p>Peq =
rH
</p>
<p>a
Heq =
</p>
<p>mP
</p>
<p>b
. (20.96)
</p>
<p>The Jacobian matrix is
</p>
<p>M0 =
&part;
</p>
<p>&part;C
F(C0)=
</p>
<p>(
rH &minus; aPeq &minus;aHeq
</p>
<p>bPeq bHeq &minus;mP
</p>
<p>)
(20.97)
</p>
<p>which gives for the trivial equilibrium
</p>
<p>Mk =
(
rH &minus;DH k2 0
</p>
<p>0 &minus;mP &minus;DP k2
)
. (20.98)
</p>
<p>One eigenvalue λ1 =&minus;mP &minus;DP k2 is negative whereas the second λ2 = rH &minus;DH k2
is positive for k2 &lt; rH /DH . Hence this equilibrium is unstable against fluctuations
with long wavelengths. For the second equilibrium we find:</p>
<p/>
</div>
<div class="page"><p/>
<p>20.5 Reaction-Diffusion Systems 381
</p>
<p>Fig. 20.12 (Lotka-Volterra model with diffusion) The time evolution is calculated for initial ran-
dom fluctuations. Colors indicate the deviation of the predator concentration P (x, y, t) from its
average value (blue: �P &lt;&minus;0.1, green: &minus;0.1 &lt;�P &lt;&minus;0.01, black: &minus;0.01 &lt;�P &lt; 0.01, yel-
low: 0.01 &lt;�P &lt; 0.1, red: �P &gt; 0.1). Parameters as in Fig. 20.13
</p>
<p>Fig. 20.13 (Dispersion of the
diffusive Lotka-Volterra
model) Real (full curve) and
imaginary part (broken line)
of the eigenvalue λ (20.100)
are shown as a function of k.
Parameters are
DH =DP = 1,
mP = rH = a = b= 0.5
</p>
<p>Mk =
(
&minus;DH k2 &minus; amPb
</p>
<p>brH
a
</p>
<p>&minus;DP k2
)
</p>
<p>(20.99)
</p>
<p>tr(Mk)=&minus;(DH +DP )k2
</p>
<p>det(MK)=mP rH +DHDP k4
</p>
<p>λ=&minus;DH +DP
2
</p>
<p>k2 &plusmn; 1
2
</p>
<p>&radic;
(DH &minus;DP )2k4 &minus; 4mP rH .
</p>
<p>(20.100)
</p>
<p>For small k with k2 &lt; 2
&radic;
mP rH /|DH &minus; DP | damped oscillations are expected
</p>
<p>whereas the system is stable against fluctuations with larger k (Figs. 20.12, 20.13,
20.14).</p>
<p/>
</div>
<div class="page"><p/>
<p>382 20 Nonlinear Systems
</p>
<p>Fig. 20.14 (Traveling waves
in the diffusive
Lotka-Volterra model)
Initially P (x, y)= Peq and
H(x,y) is peaked in the
center. This leads to
oscillations and a sharp
wavefront moving away from
the excitation. Color code and
parameters as in Fig. 20.12
</p>
<p>20.6 Problems
</p>
<p>Problem 20.1 (Orbits of the iterated logistic map) This computer example draws
orbits (Fig. 20.5) of the logistic map
</p>
<p>xn+1 = r0 &middot; xn &middot; (1 &minus; xn). (20.101)
</p>
<p>You can select the initial value x0 and the variable r .
</p>
<p>Problem 20.2 (Bifurcation diagram of the logistic map) This computer example
generates a bifurcation diagram of the logistic map (Fig. 20.6). You can select the
range of r .
</p>
<p>Problem 20.3 (Lotka-Volterra model) Equations (20.47) are solved with the im-
proved Euler method (Fig. 20.8). The predictor step uses an explicit Euler step to
calculate the values at t +�t/2
</p>
<p>Hpr
</p>
<p>(
t + �t
</p>
<p>2
</p>
<p>)
=H(t)+
</p>
<p>(
rHH(t)&minus; aH(t)P (t)
</p>
<p>)�t
2
</p>
<p>(20.102)
</p>
<p>Ppr
</p>
<p>(
t + �t
</p>
<p>2
</p>
<p>)
= P(t)+
</p>
<p>(
bH(t)P (t)&minus;mpP(t)
</p>
<p>)�t
2
</p>
<p>(20.103)
</p>
<p>and the corrector step advances time by �t</p>
<p/>
</div>
<div class="page"><p/>
<p>20.6 Problems 383
</p>
<p>H(t +�t)=H(t)+
(
rHHpr
</p>
<p>(
t + �t
</p>
<p>2
</p>
<p>)
&minus; aHpr
</p>
<p>(
t + �t
</p>
<p>2
</p>
<p>)
Ppr
</p>
<p>(
t + �t
</p>
<p>2
</p>
<p>))
�t
</p>
<p>(20.104)
</p>
<p>P(t +�t)= P(t)+
(
bHpr
</p>
<p>(
t + �t
</p>
<p>2
</p>
<p>)
Ppr
</p>
<p>(
t + �t
</p>
<p>2
</p>
<p>)
&minus;mpPpr
</p>
<p>(
t + �t
</p>
<p>2
</p>
<p>))
�t.
</p>
<p>(20.105)
</p>
<p>Problem 20.4 (Holling-Tanner model) The equations of the Holling-Tanner model
(20.64), (20.65) are solved with the improved Euler method (see Fig. 20.11). The
predictor step uses an explicit Euler step to calculate the values at t +�t/2:
</p>
<p>Hpr
</p>
<p>(
t + �t
</p>
<p>2
</p>
<p>)
=H(t)+ f
</p>
<p>(
H(t),P (t)
</p>
<p>)�t
2
</p>
<p>(20.106)
</p>
<p>Ppr
</p>
<p>(
t + �t
</p>
<p>2
</p>
<p>)
= P(t)+ g
</p>
<p>(
H(t),P (t)
</p>
<p>)�t
2
</p>
<p>(20.107)
</p>
<p>and the corrector step advances time by �t :
</p>
<p>H(t +�t)=H(t)+ f
(
Hpr
</p>
<p>(
t + �t
</p>
<p>2
</p>
<p>)
,Ppr
</p>
<p>(
t + �t
</p>
<p>2
</p>
<p>))
�t (20.108)
</p>
<p>P(t +�t)= P(t)+ g
(
Hpr
</p>
<p>(
t + �t
</p>
<p>2
</p>
<p>)
,Ppr
</p>
<p>(
t + �t
</p>
<p>2
</p>
<p>))
�t. (20.109)
</p>
<p>Problem 20.5 (Diffusive Lotka-Volterra model) The Lotka-Volterra model with
diffusion (20.94) is solved in 2 dimensions with an implicit method (19.2.2) for
the diffusive motion (Figs. 20.12, 20.14). The split operator approximation (19.3) is
used to treat diffusion in x and y direction independently. The equations
</p>
<p>(
H(t +�t)
P (t +�t)
</p>
<p>)
=
(
A&minus;1H(t)
A&minus;1P(t)
</p>
<p>)
+
(
A&minus;1f (H(t),P (t))�t
A&minus;1g(H(t),P (t))�t
</p>
<p>)
</p>
<p>&asymp;
(
A&minus;1x A
</p>
<p>&minus;1
y [H(t)+ f (H(t),P (t))�t]
</p>
<p>A&minus;1x A
&minus;1
y [P(t)+ g(H(t),P (t))�t]
</p>
<p>)
(20.110)
</p>
<p>are equivalent to the following systems of linear equations with tridiagonal matrix
(5.3):
</p>
<p>AyU =H(t)+ f
(
H(t),P (t)
</p>
<p>)
�t (20.111)
</p>
<p>U =AxH(t +�t) (20.112)
AyV = P(t)+ g
</p>
<p>(
H(t),P (t)
</p>
<p>)
�t (20.113)
</p>
<p>V =AxP(t +�t). (20.114)
Periodic boundary conditions are implemented with the method described in
Sect. 5.4.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 21
</p>
<p>Simple Quantum Systems
</p>
<p>In this chapter we study simple quantum systems. A particle in a one-dimensional
potential V (x) is described by a wave packet which is a solution of the partial dif-
ferential equation [232]
</p>
<p>i�
&part;
</p>
<p>&part;t
ψ(x)=Hψ(x)=&minus; �
</p>
<p>2
</p>
<p>2m
</p>
<p>&part;2
</p>
<p>&part;x2
ψ(x)+ V (x)ψ(x). (21.1)
</p>
<p>We discuss two approaches to discretize the second derivative. Finite differences
are simple to use but their dispersion deviates largely from the exact relation, except
high order differences are used. Pseudo-spectral methods evaluate the kinetic energy
part in Fourier space and are much more accurate. The time evolution operator can
be approximated by rational expressions like Cauchy&rsquo;s form which corresponds to
the Crank-Nicholson method. These schemes are unitary but involve time consum-
ing matrix inversions. Multistep differencing schemes have comparable accuracy
but are explicit methods. Best known is second order differencing. Split operator
methods approximate the time evolution operator by a product. In combination with
finite differences for the kinetic energy this leads to the method of real-space prod-
uct formula which can applied to wavefunctions with more than one component, for
instance to study transitions between states. In a computer experiment we simulate
a one-dimensional wave packet in a potential with one or two minima.
</p>
<p>Few-state systems are described with a small set of basis states. Especially the
quantum mechanical two-level system is often used as a simple model for the tran-
sition between an initial and a final state due to an external perturbation.1Its wave-
function has two components
</p>
<p>|ψ〉 =
(
C1
C2
</p>
<p>)
(21.2)
</p>
<p>which satisfy two coupled ordinary differential equations for the amplitudes C1,2 of
the two states
</p>
<p>1For instance collisions or the electromagnetic radiation field.
</p>
<p>P.O.J. Scherer, Computational Physics, Graduate Texts in Physics,
DOI 10.1007/978-3-319-00401-3_21,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>385</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-00401-3_21">http://dx.doi.org/10.1007/978-3-319-00401-3_21</a></div>
</div>
<div class="page"><p/>
<p>386 21 Simple Quantum Systems
</p>
<p>i�
d
</p>
<p>dt
</p>
<p>(
C1
C2
</p>
<p>)
=
(
H11 H12
H21 H22
</p>
<p>)(
C1
C2
</p>
<p>)
. (21.3)
</p>
<p>In several computer experiments we study a two-state system in an oscillating
field, a three-state system as a model for superexchange, the Landau-Zener model
for curve-crossing and the ladder model for exponential decay. The density matrix
formalism is used to describe a dissipative two-state system in analogy to the Bloch
equations for nuclear magnetic resonance. In computer experiments we study the
resonance line and the effects of saturation and power broadening. Finally we simu-
late the generation of a coherent superposition state or a spin flip by applying pulses
of suitable duration. This is also discussed in connection with the manipulation of a
qubit represented by a single spin.
</p>
<p>21.1 Pure and Mixed Quantum States
</p>
<p>Whereas pure states of a quantum system are described by a wavefunction, mixed
states are described by a density matrix. Mixed states appear if the exact quantum
state is unknown, for instance for a statistical ensemble of quantum states, a system
with uncertain preparation history, or if the system is entangled with another system.
A mixed state is different from a superposition state. For instance, the superposition
</p>
<p>|ψ〉 = C0|ψ0〉 +C1|ψ1〉 (21.4)
</p>
<p>of the two states |ψ0〉 and |ψ1〉 is a pure state, which can be described by the density
operator
</p>
<p>|ψ〉〈ψ | = |C0|2|ψ0〉〈ψ0| + |C1|2|ψ1〉〈ψ1|
+C0C&lowast;1 |ψ0〉〈ψ1| +C&lowast;0C1|ψ1〉〈ψ0| (21.5)
</p>
<p>whereas the density operator
</p>
<p>ρ = p0|ψ0〉〈ψ0| + p1|ψ1〉〈ψ1| (21.6)
</p>
<p>describes the mixed state of a system which is in the pure state |ψ0〉 with probability
p0 and in the state |ψ1〉 with probability p1 = 1 &minus; p0. The expectation value of an
operator A is in the first case
</p>
<p>〈A〉 = 〈ψ |A|ψ〉 = |C0|2〈ψ0|A|ψ0〉 + |C1|2〈ψ1A|ψ1〉
+C0C&lowast;1 〈ψ1|A|ψ0〉 +C&lowast;0C1〈ψ0|A|ψ1〉 (21.7)
</p>
<p>and in the second case
</p>
<p>〈A〉 = p0〈ψ0|A|ψ0〉 + p1〈ψ1|A|ψ1〉. (21.8)
</p>
<p>Both can be written in the form
</p>
<p>〈A〉 = tr(ρA). (21.9)</p>
<p/>
</div>
<div class="page"><p/>
<p>21.1 Pure and Mixed Quantum States 387
</p>
<p>21.1.1 Wavefunctions
</p>
<p>The time evolution of a quantum system is governed by the time dependent
Schr&ouml;dinger equation [230]
</p>
<p>i�
&part;
</p>
<p>&part;t
|ψ〉 =H |ψ〉 (21.10)
</p>
<p>for the wavefunction ψ . The brackets indicate that |ψ〉 is a vector in an abstract
Hilbert space [122]. Vectors can be added
</p>
<p>|ψ〉 = |ψ1〉 + |ψ2〉 = |ψ1 +ψ2〉 (21.11)
and can be multiplied with a complex number
</p>
<p>|ψ〉 = λ|ψ1〉 = |λψ1〉. (21.12)
Finally a complex valued scalar product of two vectors is defined2
</p>
<p>C = 〈ψ1|ψ2〉 (21.13)
which has the properties
</p>
<p>〈ψ1|ψ2〉 = 〈ψ2|ψ1〉&lowast; (21.14)
〈ψ1|λψ2〉 = λ〈ψ1|ψ2〉 =
</p>
<p>&lang;
λ&lowast;ψ1
</p>
<p>∣∣ψ2
&rang;
</p>
<p>(21.15)
</p>
<p>〈ψ |ψ1 +ψ2〉 = 〈ψ |ψ1〉 + 〈ψ |ψ2〉 (21.16)
〈ψ1 +ψ2|ψ〉 = 〈ψ1|ψ〉 + 〈ψ2|ψ〉. (21.17)
</p>
<p>21.1.2 Density Matrix for an Ensemble of Systems
</p>
<p>Consider a thermal ensemble of systems. Their wave functions are expanded with
respect to basis functions |ψs〉 as
</p>
<p>|ψ〉 =
&sum;
</p>
<p>s
</p>
<p>Cs |ψs〉. (21.18)
</p>
<p>The ensemble average of an operator A is given by
</p>
<p>〈A〉 = 〈ψAψ〉 =
&lang;&sum;
</p>
<p>s,s&prime;
C&lowast;sψsACs&prime;ψs&prime;
</p>
<p>&rang;
(21.19)
</p>
<p>=
&sum;
</p>
<p>s,s&prime;
C&lowast;s Cs&prime;Ass&prime; = tr(ρA) (21.20)
</p>
<p>2If, for instance the wavefunction depends on the coordinates of N particles, the scalar product is
defined by 〈ψn|ψn&prime; 〉 =
</p>
<p>&int;
d3r1 &middot; &middot; &middot;d3rNψ&lowast;n (r1 &middot; &middot; &middot; rN )ψn&prime; (r1 &middot; &middot; &middot; rN ).</p>
<p/>
</div>
<div class="page"><p/>
<p>388 21 Simple Quantum Systems
</p>
<p>with the density matrix
</p>
<p>ρs&prime;s =
&sum;
</p>
<p>s,s&prime;
C&lowast;s Cs&prime; . (21.21)
</p>
<p>The wave function of an N -state system is a linear combination
</p>
<p>|ψ〉 = C1|ψ1〉 +C2|ψ2〉 + &middot; &middot; &middot; +CN |ψN 〉. (21.22)
The diagonal elements of the density matrix are the occupation probabilities
</p>
<p>ρ11 = |C1|2 ρ22 = |C2|2 &middot; &middot; &middot; ρNN = |CN |2 (21.23)
and the nondiagonal elements measure the correlation of two states3
</p>
<p>ρ12 = ρ&lowast;21 = C&lowast;2C1, . . . . (21.24)
</p>
<p>21.1.3 Time Evolution of the Density Matrix
</p>
<p>The expansion coefficients of
</p>
<p>|ψ〉 =
&sum;
</p>
<p>s
</p>
<p>Cs |ψs〉 (21.25)
</p>
<p>can be obtained from the scalar product
</p>
<p>Cs = 〈ψs |ψ〉. (21.26)
Hence we have
</p>
<p>C&lowast;s Cs&prime; = 〈ψ |ψs〉〈ψs&prime; |ψ〉 = 〈ψs&prime; |ψ〉〈ψ |ψs〉 (21.27)
which can be considered to be the s&prime;, s matrix element of the projection operator
|ψ〉〈ψ |
</p>
<p>C&lowast;s Cs&prime; =
(
|ψ〉〈ψ |
</p>
<p>)
s&prime;s . (21.28)
</p>
<p>The thermal average of |ψ〉〈ψ | is the statistical operator
</p>
<p>ρ = |ψ〉〈ψ | (21.29)
which is represented by the density matrix with respect to the basis functions |ψs〉
</p>
<p>ρs&prime;s = |ψ〉〈ψ |s&prime;s = C&lowast;s Cs&prime; . (21.30)
From the Schr&ouml;dinger equation
</p>
<p>i�|ψ̇〉 =H |ψ〉 (21.31)
we find
</p>
<p>3They are often called the &ldquo;coherence&rdquo; of the two states.</p>
<p/>
</div>
<div class="page"><p/>
<p>21.2 Wave Packet Motion in One Dimension 389
</p>
<p>Fig. 21.1 Potential well
</p>
<p>&minus;i�〈ψ̇ | = 〈Hψ | = 〈ψ |H (21.32)
and hence
</p>
<p>i�ρ̇ = i�
(
|ψ̇〉〈ψ | + |ψ〉〈ψ̇ |
</p>
<p>)
= |Hψ〉〈ψ | &minus; |ψ〉〈Hψ |. (21.33)
</p>
<p>Since the Hamiltonian H is identical for all members of the ensemble we end up
with the Liouville-von Neumann equation
</p>
<p>i�ρ̇ =Hρ &minus; ρH = [H,ρ]. (21.34)
With respect to a finite basis this becomes explicitly:
</p>
<p>i�ρ̇ii =
&sum;
</p>
<p>j
</p>
<p>Hijρji &minus; ρijHji =
&sum;
</p>
<p>j �=i
Hijρji &minus; ρijHji (21.35)
</p>
<p>i�ρ̇ik =
&sum;
</p>
<p>j
</p>
<p>Hijρjk &minus; ρijHjk
</p>
<p>= (Hii &minus;Hkk)ρik +Hik(ρkk &minus; ρii)+
&sum;
</p>
<p>j �=i,k
(Hijρjk &minus; ρijHjk).
</p>
<p>(21.36)
</p>
<p>21.2 Wave Packet Motion in One Dimension
</p>
<p>A quantum mechanical particle with mass mp in a one-dimensional potential V (x)
is described by a complex valued wavefunction ψ(x) (Fig. 21.1). We assume that the
wavefunction is negligible outside an interval [a, b]. This is the case for a particle
bound in a potential well i.e. a deep enough minimum of the potential or for a
wave-packet with finite width far from the boundaries. Then the calculation can be
restricted to the finite interval [a, b] by applying the boundary condition
</p>
<p>ψ(x)= 0 for x &le; a or x &ge; b (21.37)
or, if reflections at the boundary should be suppressed, transparent boundary condi-
tions [9].</p>
<p/>
</div>
<div class="page"><p/>
<p>390 21 Simple Quantum Systems
</p>
<p>All observables (quantities which can be measured) of the particle are expecta-
tion values with respect to the wavefunction, for instance its average position is
</p>
<p>〈x〉 =
&lang;
ψ(x)xψ(x)
</p>
<p>&rang;
=
&int; b
</p>
<p>a
</p>
<p>dx ψ&lowast;(x)xψ(x). (21.38)
</p>
<p>The probability of finding the particle at the position x0 is given by
</p>
<p>P(x = x0)=
∣∣ψ(x0)
</p>
<p>∣∣2. (21.39)
For time independent potential V (x) the Schr&ouml;dinger equation
</p>
<p>i�ψ̇ =Hψ =
(
&minus; �
</p>
<p>2
</p>
<p>2mp
</p>
<p>&part;2
</p>
<p>&part;x2
+ V (x)
</p>
<p>)
ψ (21.40)
</p>
<p>can be formally solved by
</p>
<p>ψ(t)=U(t, t0)ψ(t0)= exp
{
&minus; i(t &minus; t0)
</p>
<p>�
H
</p>
<p>}
ψ(t0). (21.41)
</p>
<p>If the potential is time dependent, the more general formal solution is
</p>
<p>ψ(t)=U(t, t0)ψ(t0)= T̂t exp
{
&minus; i
�
</p>
<p>&int; t
</p>
<p>t0
</p>
<p>H(τ)dτ
</p>
<p>}
ψ(t0)
</p>
<p>=
&infin;&sum;
</p>
<p>n=0
</p>
<p>1
</p>
<p>n!
</p>
<p>(&minus;i
�
</p>
<p>)n &int; t
</p>
<p>t0
</p>
<p>dt1
</p>
<p>&int; t
</p>
<p>t0
</p>
<p>dt2 . . .
</p>
<p>&int; t
</p>
<p>t0
</p>
<p>dtn T̂t
{
H(t1)H(t2) . . .H(tn)
</p>
<p>}
</p>
<p>(21.42)
</p>
<p>where T̂t denotes the time ordering operator. The simplest approach is to divide the
time interval 0 . . . t into a sequence of smaller steps
</p>
<p>U(t, t0)=U(t, tN&minus;1) . . .U(t2, t1)U(t1, t0) (21.43)
and to neglect the variation of the Hamiltonian during the small interval �t = tn+1&minus;
tn [158]
</p>
<p>U(tn+1, tn)= exp
{
&minus; i�t
</p>
<p>�
H(tn)
</p>
<p>}
. (21.44)
</p>
<p>21.2.1 Discretization of the Kinetic Energy
</p>
<p>The kinetic energy
</p>
<p>T ψ(x)=&minus; �
2
</p>
<p>2mp
</p>
<p>&part;2
</p>
<p>&part;x2
ψ(x) (21.45)
</p>
<p>is a nonlocal operator in real space. It is most efficiently evaluated in Fourier space
where it becomes diagonal
</p>
<p>F[T ψ](k)= �
2k2
</p>
<p>2mp
F[ψ](k). (21.46)</p>
<p/>
</div>
<div class="page"><p/>
<p>21.2 Wave Packet Motion in One Dimension 391
</p>
<p>21.2.1.1 Pseudo-spectral Methods
</p>
<p>The potential energy is diagonal in real space. Therefore, pseudo-spectral
(Sect. 11.5.1) methods [93] use a Fast Fourier Transform algorithm (Sect. 7.3.2)
to switch between real space and Fourier space. They calculate the action of the
Hamiltonian on the wavefunction according to
</p>
<p>Hψ(x)= V (x)ψ(x)+F&minus;1
[
�
</p>
<p>2k2
</p>
<p>2mp
F[ψ](k)
</p>
<p>]
. (21.47)
</p>
<p>21.2.1.2 Finite Difference Methods
</p>
<p>In real space, the kinetic energy operator can be approximated by finite differences
on a grid, like the simple 3-point expression (3.31)
</p>
<p>&minus; �
2
</p>
<p>2mp
</p>
<p>ψnm+1 +ψnm&minus;1 &minus; 2ψnm
�x2
</p>
<p>+O
(
�x2
</p>
<p>)
(21.48)
</p>
<p>or higher order expressions (3.33)
</p>
<p>&minus; �
2
</p>
<p>2mp
</p>
<p>&minus;ψnm+2 + 16ψnm+1 &minus; 30ψnm + 16ψnm&minus;1 &minus;ψnm&minus;2
12�x2
</p>
<p>+O
(
�x4
</p>
<p>)
(21.49)
</p>
<p>&minus; �
2
</p>
<p>2mp
</p>
<p>1
</p>
<p>�x2
</p>
<p>(
1
</p>
<p>90
ψnm+3 &minus;
</p>
<p>3
</p>
<p>20
ψnm+2 +
</p>
<p>3
</p>
<p>2
ψnm+1 &minus;
</p>
<p>49
</p>
<p>18
ψnm
</p>
<p>+3
2
ψnm&minus;1 &minus;
</p>
<p>3
</p>
<p>20
ψnm&minus;2 +
</p>
<p>1
</p>
<p>90
ψnm&minus;3
</p>
<p>)
+O
</p>
<p>(
�x6
</p>
<p>)
(21.50)
</p>
<p>etc. [94]. However, finite differences inherently lead to deviations of the dispersion
relation from (21.46). Inserting ψm = eikm�x we find
</p>
<p>E(k)= �
2
</p>
<p>2mp
</p>
<p>2(1 &minus; cos(k�x))
�x2
</p>
<p>(21.51)
</p>
<p>for the 3-point expression (21.48),
</p>
<p>E(k)= �
2
</p>
<p>2mp
</p>
<p>15 &minus; 16 cos(k�x)+ cos(2k�x)
6�x2
</p>
<p>(21.52)
</p>
<p>for the 5-point expression (21.49) and
</p>
<p>�
2
</p>
<p>2mp
</p>
<p>1
</p>
<p>�x2
</p>
<p>(
49
</p>
<p>18
&minus; 3 cos(k�x)+ 3
</p>
<p>10
cos(2k�x)&minus; 1
</p>
<p>45
cos(3k�x)
</p>
<p>)
(21.53)
</p>
<p>for the 7-point expression (21.50). Even the 7-point expression shows large devi-
ations for k-values approaching kmax = π/�x (Fig. 21.2). However, it has been
shown that not very high orders are necessary to achieve the numerical accuracy of
the pseudo-spectral Fourier method [113] and that finite difference methods may be
even more efficient in certain applications [114].</p>
<p/>
</div>
<div class="page"><p/>
<p>392 21 Simple Quantum Systems
</p>
<p>Fig. 21.2 (Dispersion of
finite difference expressions)
The dispersion relation of
finite difference expressions
of increasing order ((21.48),
(21.49), (21.50) and the
symmetric 9-point
approximation [94]) are
compared to the exact
dispersion (21.46) of a free
particle (dashed curve)
</p>
<p>21.2.2 Time Evolution
</p>
<p>A number of methods have been proposed [7, 51, 158, 278] to approximate the short
time propagator (21.44). Unitarity is a desirable property since it guaranties stability
and norm conservation even for large time steps. However, depending on the appli-
cation, small deviations from unitarity may be acceptable in return for higher effi-
ciency. The Crank-Nicolson (CN) method [104, 172, 173] is one of the first methods
which have been applied to the time dependent Schr&ouml;dinger equation. It is a unitary
but implicit method and needs the inversion of a matrix which can become cumber-
some in two or more dimensions or if high precision is required. Multistep methods
[131, 132], especially second order [10] differencing (SOD) are explicit but only
conditionally stable and put limits to the time interval �t . Split operator methods
(SPO) approximate the propagator by a unitary product of operators [14, 67, 68].
They are explicit and easy to implement. The real-space split-operator method has
been applied to more complex problems like a molecule in a laser field [61]. Poly-
nomial approximations, especially the Chebishev expansion [53, 248], have very
high accuracy and allow for large time steps, if the Hamiltonian is time independent.
However, they do not provide intermediate results and need many applications of the
Hamiltonian. The short time iterative Lanczos (SIL) method [57, 153, 197] is very
useful also for time dependent Hamiltonians. Even more sophisticated methods us-
ing finite elements and the discrete variable representation are presented for instance
in [4, 226]. In the following we discuss three methods (CN, SOD, SPO) which are
easy to implement and well suited to solve the time dependent Schr&ouml;dinger equation
for a mass point moving in a one-dimensional potential.
</p>
<p>21.2.2.1 Rational Approximation
</p>
<p>Taking the first terms of the Taylor expansion</p>
<p/>
</div>
<div class="page"><p/>
<p>21.2 Wave Packet Motion in One Dimension 393
</p>
<p>U(tn+1, tn)= exp
{
&minus; i�t
</p>
<p>�
H
</p>
<p>}
= 1 &minus; i�t
</p>
<p>�
H + &middot; &middot; &middot; (21.54)
</p>
<p>corresponds to a simple explicit Euler step
</p>
<p>ψ(tn+1)=
(
</p>
<p>1 &minus; i�t
�
</p>
<p>H
</p>
<p>)
ψ(tn). (21.55)
</p>
<p>From the real eigenvalues E of the Hamiltonian we find the eigenvalues of the ex-
plicit method
</p>
<p>λ= 1 &minus; i�t
�
</p>
<p>E (21.56)
</p>
<p>which all have absolute values
</p>
<p>|λ| =
</p>
<p>&radic;
</p>
<p>1 + �t
2E2
</p>
<p>�2
&gt; 1. (21.57)
</p>
<p>Hence the explicit method is not stable.
Expansion of the inverse time evolution operator
</p>
<p>U(tn, tn+1)=U(tn+1, tn)&minus;1 = exp
{
+ i�t
</p>
<p>�
H
</p>
<p>}
= 1 + i�t
</p>
<p>�
H + &middot; &middot; &middot;
</p>
<p>leads to the implicit method
</p>
<p>ψ(tn+1)=ψ(tn)&minus;
i�t
</p>
<p>�
Hψ(tn+1) (21.58)
</p>
<p>which can be rearranged as
</p>
<p>ψ(tn+1)=
(
</p>
<p>1 + i�t
�
</p>
<p>H
</p>
<p>)&minus;1
ψ(tn). (21.59)
</p>
<p>Now all eigenvalues have absolute values &lt; 1. This method is stable but the norm
of the wave function is not conserved. Combination of implicit and explicit method
gives a method [172, 173] similar to the Crank-Nicolson method for the diffusion
equation (Sect. 19.2.3)
</p>
<p>ψ(tn+1)&minus;ψ(tn)=&minus;
i�t
</p>
<p>�
H
</p>
<p>(
ψ(tn+1)
</p>
<p>2
+ ψ(tn)
</p>
<p>2
</p>
<p>)
. (21.60)
</p>
<p>This equation can be solved for the new value of the wavefunction
</p>
<p>ψ(tn+1)=
(
</p>
<p>1 + i�t
2�
</p>
<p>H
</p>
<p>)&minus;1(
1 &minus; i�t
</p>
<p>2�
H
</p>
<p>)
ψ(tn) (21.61)
</p>
<p>which corresponds to a rational approximation4 of the time evolution operator (Cay-
ley&rsquo;s form)
</p>
<p>4The Pad&eacute; approximation (Sect. 2.4.1) of order [1,1].</p>
<p/>
</div>
<div class="page"><p/>
<p>394 21 Simple Quantum Systems
</p>
<p>U(tn+1, tn)&asymp;
1 &minus; i�t2�H
1 + i�t2�H
</p>
<p>. (21.62)
</p>
<p>The eigenvalues of (21.62) all have an absolute value of
</p>
<p>|λ| =
∣∣∣∣
(
</p>
<p>1 + iE�t
2�
</p>
<p>)&minus;1(
1 &minus; iE�t
</p>
<p>2�
</p>
<p>)∣∣∣∣=
</p>
<p>&radic;
1 + E2�t2
</p>
<p>4�2&radic;
1 + E2�t2
</p>
<p>4�2
</p>
<p>= 1. (21.63)
</p>
<p>It is obviously a unitary operator and conserves the norm of the wavefunction since
(
</p>
<p>1 &minus; i�t2�H
1 + i�t2�H
</p>
<p>)&dagger;(1 &minus; i�t2�H
1 + i�t2�H
</p>
<p>)
=
(
</p>
<p>1 + i�t2�H
1 &minus; i�t2�H
</p>
<p>)(
1 &minus; i�t2�H
1 + i�t2�H
</p>
<p>)
= 1 (21.64)
</p>
<p>as H is Hermitian H &dagger; =H and (1+ i�t2�H) and (1&minus; i
�t
2�H) are commuting opera-
</p>
<p>tors. From the Taylor series we find the error order
(
</p>
<p>1 + i�t
2�
</p>
<p>H
</p>
<p>)&minus;1(
1 &minus; i�t
</p>
<p>2�
H
</p>
<p>)
</p>
<p>=
(
</p>
<p>1 &minus; i�t
2�
</p>
<p>H &minus; �t
2
</p>
<p>4�2
H 2 + &middot; &middot; &middot;
</p>
<p>)(
1 &minus; i�t
</p>
<p>2�
H
</p>
<p>)
</p>
<p>= 1 &minus; i�t
�
</p>
<p>H &minus; �t
2
</p>
<p>2�2
H 2 + &middot; &middot; &middot; = exp
</p>
<p>(
&minus; i�t
</p>
<p>�
H
</p>
<p>)
+O
</p>
<p>(
�t3
</p>
<p>)
. (21.65)
</p>
<p>For practical application we rewrite [149]
(
</p>
<p>1 + i�t
2�
</p>
<p>H
</p>
<p>)&minus;1(
1 &minus; i�t
</p>
<p>2�
H
</p>
<p>)
=
(
</p>
<p>1 + i�t
2�
</p>
<p>H
</p>
<p>)&minus;1(
&minus;1 &minus; i�t
</p>
<p>2�
H + 2
</p>
<p>)
</p>
<p>=&minus;1 + 2
(
</p>
<p>1 + i�t
2�
</p>
<p>H
</p>
<p>)&minus;1
(21.66)
</p>
<p>hence
</p>
<p>ψ(tn+1)= 2
(
</p>
<p>1 + i�t
2�
</p>
<p>H
</p>
<p>)&minus;1
ψ(tn)&minus;ψ(tn)= 2χ &minus;ψ(tn). (21.67)
</p>
<p>ψ(tn+1) is obtained in two steps. First we have to solve
(
</p>
<p>1 + i�t
2�
</p>
<p>H
</p>
<p>)
χ =ψ(tn). (21.68)
</p>
<p>Then ψ(tn+1) is given by
</p>
<p>ψ(tn+1)= 2χ &minus;ψ(tn). (21.69)
We use the finite difference method (Sect. 11.2) on the grid
</p>
<p>xm =m�x m= 0 &middot; &middot; &middot;M ψnm =ψ(tn, xm) (21.70)
and approximate the second derivative by</p>
<p/>
</div>
<div class="page"><p/>
<p>21.2 Wave Packet Motion in One Dimension 395
</p>
<p>&part;2
</p>
<p>&part;x2
ψ(tn, xm)=
</p>
<p>ψnm+1 +ψnm&minus;1 &minus; 2ψnm
�x2
</p>
<p>+O
(
�x2
</p>
<p>)
. (21.71)
</p>
<p>Equation (21.68) then becomes a system of linear equations
</p>
<p>A
</p>
<p>⎡
⎢⎢⎢⎣
</p>
<p>χ0
χ1
...
</p>
<p>χM
</p>
<p>⎤
⎥⎥⎥⎦=
</p>
<p>⎡
⎢⎢⎢⎣
</p>
<p>ψn0
ψn1
...
</p>
<p>ψnM
</p>
<p>⎤
⎥⎥⎥⎦ (21.72)
</p>
<p>with a tridiagonal matrix
</p>
<p>A= 1 &minus; i ��t
4mp�x2
</p>
<p>⎛
⎜⎜⎜⎜⎝
</p>
<p>2 &minus;1
</p>
<p>&minus;1 2 . . .
. . .
</p>
<p>. . . &minus;1
&minus;1 2
</p>
<p>⎞
⎟⎟⎟⎟⎠
+ i�t
</p>
<p>2�
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>V0
V1
</p>
<p>. . .
</p>
<p>VM
</p>
<p>⎞
⎟⎟⎟⎠ .
</p>
<p>(21.73)
</p>
<p>The second step (21.69) becomes
</p>
<p>⎡
⎢⎢⎢⎣
</p>
<p>ψn+10
ψn+11
...
</p>
<p>ψn+1M
</p>
<p>⎤
⎥⎥⎥⎦= 2
</p>
<p>⎡
⎢⎢⎢⎣
</p>
<p>χ0
χ1
...
</p>
<p>χM
</p>
<p>⎤
⎥⎥⎥⎦&minus;
</p>
<p>⎡
⎢⎢⎢⎣
</p>
<p>ψn0
ψn1
...
</p>
<p>ψnM
</p>
<p>⎤
⎥⎥⎥⎦ . (21.74)
</p>
<p>Inserting a plane wave
</p>
<p>ψ = ei(kx&minus;ωt) (21.75)
</p>
<p>we obtain the dispersion relation (Fig. 21.3)
</p>
<p>2
</p>
<p>�t
tan(ω�t/2)= �
</p>
<p>2mp
</p>
<p>(
2
</p>
<p>�x
sin
</p>
<p>k�x
</p>
<p>2
</p>
<p>)2
(21.76)
</p>
<p>which we rewrite as
</p>
<p>ω�t = 2 arctan
[
</p>
<p>2α
</p>
<p>π2
sin2
</p>
<p>k�x
</p>
<p>π
</p>
<p>π
</p>
<p>2
</p>
<p>]
(21.77)
</p>
<p>with the dimensionless parameter
</p>
<p>α = π
2
��t
</p>
<p>2mp�x2
. (21.78)
</p>
<p>For time independent potentials the accuracy of this method can be improved
systematically [261] by using higher order finite differences for the spatial derivative
(Sect. 21.2.1.2) and a higher order Pad&eacute; approximation (Sect. 2.4.1) of order [M,M]
for the exponential function</p>
<p/>
</div>
<div class="page"><p/>
<p>396 21 Simple Quantum Systems
</p>
<p>Fig. 21.3 (Dispersion of the Crank-Nicolson method) The dispersion relation of the Crank-Ni-
colson method (21.95) deviates largely from the exact dispersion (21.98), even for small values
of the stability parameter α. The scaled frequency ω�t/α is shown as a function of k�x/π
for α = 0.1,1,2,5,10 (solid curves) and compared with the exact relation of a free particle
ω�t/α = (k�x/π)2 (dashed curve)
</p>
<p>ez =
M&prod;
</p>
<p>s=1
</p>
<p>1 &minus; z/z(M)s
1 + z/z(M)&lowast;s
</p>
<p>+O
(
z2M+1
</p>
<p>)
(21.79)
</p>
<p>to approximate the time evolution operator
</p>
<p>exp
</p>
<p>(
&minus; i�t
</p>
<p>�
H
</p>
<p>)
=
</p>
<p>M&prod;
</p>
<p>s=1
</p>
<p>1 &minus; (i�t H/�)/z(M)s
1 + (i�t H/�)/z&lowast;(M)s
</p>
<p>+O
(
(�t)2M+1
</p>
<p>)
. (21.80)
</p>
<p>However, the matrix inversion can become very time consuming in two or more
dimensions.
</p>
<p>21.2.2.2 Second Order Differencing
</p>
<p>Explicit methods avoid the matrix inversion. The method of second order differenc-
ing [10] takes the difference of forward and backward step
</p>
<p>ψ(tn&minus;1)=U(tn&minus;1, tn)ψ(tn) (21.81)
ψ(tn+1)=U(tn+1, tn)ψ(tn) (21.82)
</p>
<p>to obtain the explicit two-step algorithm
</p>
<p>ψ(tn+1)=ψ(tn&minus;1)+
[
U(tn+1, tn)&minus;U&minus;1(tn, tn&minus;1)
</p>
<p>]
ψ(tn). (21.83)
</p>
<p>The first terms of the Taylor series give the approximation
</p>
<p>ψ(tn+1)=ψ(tn&minus;1)&minus; 2
i�t
</p>
<p>�
Hψ(tn)+O
</p>
<p>(
(�t)3
</p>
<p>)
(21.84)</p>
<p/>
</div>
<div class="page"><p/>
<p>21.2 Wave Packet Motion in One Dimension 397
</p>
<p>which can also be obtained from the second order approximation of the time deriva-
tive [150]
</p>
<p>Hψ = i� &part;
&part;t
</p>
<p>ψ = ψ(t +�t)&minus;ψ(t &minus;�t)
2�t
</p>
<p>. (21.85)
</p>
<p>This two-step algorithm can be formulated as a discrete mapping
(
ψ(tn+1)
ψ(tn)
</p>
<p>)
=
(
&minus;2 i�t
</p>
<p>�
H 1
</p>
<p>1 0
</p>
<p>)(
ψ(tn)
</p>
<p>ψ(tn&minus;1)
</p>
<p>)
(21.86)
</p>
<p>with eigenvalues
</p>
<p>λ=&minus; iEs�t
�
</p>
<p>&plusmn;
</p>
<p>&radic;
</p>
<p>1 &minus; E
2
s�t
</p>
<p>2
</p>
<p>�2
. (21.87)
</p>
<p>For sufficiently small time step [158]
</p>
<p>�t &lt;
�
</p>
<p>max |Es |
(21.88)
</p>
<p>the square root is real,
</p>
<p>|λ|2 = E
2
s�t
</p>
<p>2
</p>
<p>�2
+
(
</p>
<p>1 &minus; E
2
s�t
</p>
<p>2
</p>
<p>�2
</p>
<p>)
= 1 (21.89)
</p>
<p>and the method is conditionally stable and has the same error order as the Crank-
Nicolson method (Sect. 21.2.2.1). Its big advantage is that it is an explicit method
and does not involve matrix inversions. Generalization to higher order multistep dif-
ferencing schemes is straightforward [131]. The method conserves [150] the quan-
tities &real;〈ψ(t +�t)|ψ(t)〉 and &real;〈ψ(t +�t)|H |ψ(t)〉 but is not strictly unitary [10].
Consider a pair of wavefunctions at times t0 and t1 which obey the exact time evo-
lution
</p>
<p>ψ(t1)= exp
{
&minus; i�t
</p>
<p>�
H
</p>
<p>}
ψ(t0) (21.90)
</p>
<p>and apply (21.84) to obtain
</p>
<p>ψ(t2)=
[
</p>
<p>1 &minus; 2 i�t
�
</p>
<p>H exp
</p>
<p>{
&minus; i�t
</p>
<p>�
H
</p>
<p>}]
ψ(t0) (21.91)
</p>
<p>which can be written as
</p>
<p>ψ(t2)= Lψ(t0) (21.92)
where the time evolution operator L obeys
</p>
<p>L
&dagger;
L=
</p>
<p>[
1 + 2 i�t
</p>
<p>�
H exp
</p>
<p>{
+ i�t
</p>
<p>�
H
</p>
<p>}][
1 &minus; 2 i�t
</p>
<p>�
H exp
</p>
<p>{
&minus; i�t
</p>
<p>�
H
</p>
<p>}]
</p>
<p>= 1 &minus; 4�t
�
H sin
</p>
<p>{
�t
</p>
<p>�
H
</p>
<p>}
+ 4
</p>
<p>(
�t
</p>
<p>�
H
</p>
<p>)2
.</p>
<p/>
</div>
<div class="page"><p/>
<p>398 21 Simple Quantum Systems
</p>
<p>Fig. 21.4 (Dispersion of the Fourier method) The dispersion relation of the SOD-Fourier method
(21.95) deviates from the exact dispersion (21.98) only for very high k-values and approaches it
for small values of the stability parameter α. The scaled frequency ω�t/α is shown as a function
of k�x/π for α = 0.5,0.75,1 (solid curves) and compared with the exact relation of a free particle
ω�t/α = (k�x/π)2 (dashed curve)
</p>
<p>Expanding the sine function we find the deviation from unitarity [10]
</p>
<p>L
&dagger;
L&minus; 1 = 2
</p>
<p>3
</p>
<p>(
�t
</p>
<p>�
H
</p>
<p>)4
+ &middot; &middot; &middot; =O
</p>
<p>(
(�t)4
</p>
<p>)
(21.93)
</p>
<p>which is of higher order than the error of the algorithm. Furthermore errors do not
accumulate due to the stability of the algorithm (21.89). This also holds for devia-
tions of the starting values from the condition (21.90).
</p>
<p>The algorithm (21.84) can be combined with the finite differences method
(Sect. 21.2.1.2)
</p>
<p>ψn+1m =ψn&minus;1m &minus; 2
i�t
</p>
<p>�
</p>
<p>[
Vmψ
</p>
<p>n
m &minus;
</p>
<p>�
2
</p>
<p>2mp�x2
(
ψnm+1 +ψnm&minus;1 &minus; 2ψnm
</p>
<p>)]
(21.94)
</p>
<p>or with the pseudo-spectral Fourier method [150]. This combination needs two
Fourier transformations for each step but it avoids the distortion of the dispersion
relation inherent to the finite difference method. Inserting the plane wave (21.75)
into (21.84) we find the dispersion relation (Fig. 21.4) for a free particle (V = 0):
</p>
<p>ω= 1
�t
</p>
<p>arcsin
</p>
<p>(
��tk2
</p>
<p>2mp
</p>
<p>)
= 1
</p>
<p>�t
arcsin
</p>
<p>(
α
</p>
<p>(
k�x
</p>
<p>π
</p>
<p>)2)
. (21.95)
</p>
<p>For a maximum k-value
</p>
<p>kmax =
π
</p>
<p>�x
(21.96)
</p>
<p>the stability condition (21.88) becomes</p>
<p/>
</div>
<div class="page"><p/>
<p>21.2 Wave Packet Motion in One Dimension 399
</p>
<p>Fig. 21.5 (Dispersion of the finite difference method) The dispersion relation of the SOD-FD
method (21.99) deviates largely from the exact dispersion (21.98), even for small values of
the stability parameter α. The scaled frequency ω�t/α is shown as a function of k�x/π for
α = π2/4 &asymp; 2.467, 1.85, 1.23, 0.2 (solid curves) and compared with the exact relation of a free
particle ω�t/α = (k�x/π)2 (dashed curve)
</p>
<p>1 &ge; �t
�
</p>
<p>�
2k2max
</p>
<p>2mp
= α. (21.97)
</p>
<p>For small k the dispersion approximates the exact behavior
</p>
<p>ω= �k
2
</p>
<p>2mP
. (21.98)
</p>
<p>The finite difference method (21.94), on the other hand, has the dispersion relation
(Fig. 21.5)
</p>
<p>ω= 1
�t
</p>
<p>arcsin
</p>
<p>(
4α
</p>
<p>π2
sin2
</p>
<p>(
k�x
</p>
<p>2
</p>
<p>))
(21.99)
</p>
<p>and the stability limit
</p>
<p>1 = �t
�
Emax =
</p>
<p>2��t
</p>
<p>mp�x2
= 4α
</p>
<p>π2
. (21.100)
</p>
<p>The deviation from (21.98) is significant for k�x/π &gt; 0.2 even for small values
of α [150].
</p>
<p>21.2.2.3 Split-Operator Methods
</p>
<p>The split-operator method approximates the exponential short time evolution oper-
ator as a product of exponential operators which are easier tractable. Starting from
the Zassenhaus formula [164]</p>
<p/>
</div>
<div class="page"><p/>
<p>400 21 Simple Quantum Systems
</p>
<p>eλ(A+B) = eλAeλBeλ2C2eλ3C3 . . . (21.101)
</p>
<p>C2 =
1
</p>
<p>2
[B,A] C3 =
</p>
<p>1
</p>
<p>6
[C2,A+ 2B] . . . (21.102)
</p>
<p>approximants of increasing order can be systematically constructed [68, 246]
</p>
<p>eλ(A+B) = eλAeλB +O
(
λ2
)
= eλAeλBeλ2C2 +O
</p>
<p>(
λ3
)
</p>
<p>. . . . (21.103)
</p>
<p>Since these approximants do not conserve time reversibility, often the symmetric
expressions
</p>
<p>eλ(A+B) = eλA/2eλBeλA/2 +O
(
λ3
)
</p>
<p>= eλA/2eλB/2eλ2C3/4eλB/2eλA/2 +O
(
λ5
)
</p>
<p>. . . (21.104)
</p>
<p>are preferred.
</p>
<p>Split-Operator-Fourier Method Dividing the Hamiltonian into its kinetic and
potential parts
</p>
<p>H = T + V =&minus; �
2
</p>
<p>2mP
</p>
<p>&part;2
</p>
<p>&part;x2
+ V (x) (21.105)
</p>
<p>the time evolution operator can be approximated by the time-symmetric expression
</p>
<p>U(�t)= e&minus; i�t2� T e&minus; i�t� V e&minus; i�t2� T +O
(
(�t)3
</p>
<p>)
(21.106)
</p>
<p>where the exponential of the kinetic energy can be easily applied in Fourier space
[80, 150]. Combining several steps (21.106) to integrate over a longer time interval,
consecutive operators can be combined to simplify the algorithm
</p>
<p>U(N�t)=UN (�t)= e&minus; i�t2� T
(
e&minus;
</p>
<p>i�t
�
V e&minus;
</p>
<p>i�t
�
T
)N&minus;1e&minus; i�t� V e&minus; i�t2� T . (21.107)
</p>
<p>Real-Space Product Formulae Using the discretization (21.48) on a regular grid
the time evolution operator becomes the exponential of a matrix
</p>
<p>U(�t)
</p>
<p>= exp
</p>
<p>⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
</p>
<p>&minus;i�t
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>V0
�
+ �
</p>
<p>mP�x
2 &minus; �2mP�x2
</p>
<p>&minus; �
2mP�x2
</p>
<p>V1
�
+ �
</p>
<p>mP�x
2 &minus; �2mP�x2
</p>
<p>. . .
</p>
<p>&minus; �
2mP�x2
</p>
<p>VM
�
</p>
<p>+ �
mP�x
</p>
<p>2
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠
</p>
<p>⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
</p>
<p>= exp
</p>
<p>⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
&minus;i�t
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>γ0 + 2β &minus;β
β γ1 + 2β &minus;β
</p>
<p>. . .
</p>
<p>&minus;β γM + 2β
</p>
<p>⎞
⎟⎟⎟⎠
</p>
<p>⎫
⎪⎪⎪⎬
⎪⎪⎪⎭
</p>
<p>(21.108)
</p>
<p>with the abbreviations</p>
<p/>
</div>
<div class="page"><p/>
<p>21.2 Wave Packet Motion in One Dimension 401
</p>
<p>γm =
1
</p>
<p>�
Vm β =
</p>
<p>�
</p>
<p>2mP�x2
. (21.109)
</p>
<p>The matrix can be decomposed into the sum of two overlapping tridiagonal block
matrices [61, 67]5
</p>
<p>Ho =
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>γ0 + 2β &minus;β
&minus;β 12γ1 + β
</p>
<p>1
2γ2 + β &minus;β
</p>
<p>&minus;β . . .
</p>
<p>⎞
⎟⎟⎟⎠=
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>A1
A3
</p>
<p>. . .
</p>
<p>AM&minus;1
</p>
<p>⎞
⎟⎟⎟⎠
</p>
<p>(21.110)
</p>
<p>He =
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>0 0
0 12γ1 + β &minus;β
</p>
<p>&minus;β 12γ2 + β 0
</p>
<p>0
. . .
</p>
<p>⎞
⎟⎟⎟⎠=
</p>
<p>⎛
⎜⎜⎜⎜⎜⎝
</p>
<p>0
A2
</p>
<p>. . .
</p>
<p>AM&minus;2
0
</p>
<p>⎞
⎟⎟⎟⎟⎟⎠
.(21.111)
</p>
<p>The block structure simplifies the calculation of e&minus;i�tHo and e&minus;i�tHe tremen-
dously since effectively only the exponential functions of 2 &times; 2 matrices
</p>
<p>Bm(τ )= e&minus;iτAm (21.112)
</p>
<p>have to be calculated and the approximation to the time evolution operator
</p>
<p>U(�t)= e&minus;i�tHo/2e&minus;i�tHee&minus;i�tHo/2
</p>
<p>=
</p>
<p>⎛
⎜⎝
B1(
</p>
<p>�t
2 )
</p>
<p>B3(
�t
2 )
</p>
<p>. . .
</p>
<p>⎞
⎟⎠
</p>
<p>⎛
⎜⎝
</p>
<p>1
B2(�t)
</p>
<p>. . .
</p>
<p>⎞
⎟⎠
</p>
<p>&times;
</p>
<p>⎛
⎜⎝
B1(
</p>
<p>�t
2 )
</p>
<p>B3(
�t
2 )
</p>
<p>. . .
</p>
<p>⎞
⎟⎠ (21.113)
</p>
<p>can be applied in real space without any Fourier transformation. To evaluate
(21.112) the real symmetric matrix Am is diagonalized by an orthogonal transfor-
mation (Sect. 9.2)
</p>
<p>A=R&minus;1ÃR =R&minus;1
(
λ1 0
0 λ2
</p>
<p>)
R (21.114)
</p>
<p>and the exponential calculated from
</p>
<p>e&minus;iτA = 1 &minus; iτR&minus;1ÃR + (&minus;iτ)
2
</p>
<p>2
R&minus;1ÃRR&minus;1ÃR + &middot; &middot; &middot;
</p>
<p>5For simplicity only the case of even M is considered.</p>
<p/>
</div>
<div class="page"><p/>
<p>402 21 Simple Quantum Systems
</p>
<p>=R&minus;1
[
</p>
<p>1 &minus; iτ Ã+ (&minus;iτ)
2
</p>
<p>2
ÃR + &middot; &middot; &middot;
</p>
<p>]
R
</p>
<p>=R&minus;1e&minus;iτÃR =R&minus;1
(
e&minus;iτλ1
</p>
<p>e&minus;iτλ2
</p>
<p>)
R. (21.115)
</p>
<p>21.2.3 Example: Free Wave Packet Motion
</p>
<p>We simulate the free motion (V = 0) of a Gaussian wave packet along the x-axis
(see Problem 21.1). To simplify the numerical calculation we set �= 1 and mp = 1
and solve the time dependent Schr&ouml;dinger equation
</p>
<p>i
&part;
</p>
<p>&part;t
ψ =&minus;1
</p>
<p>2
</p>
<p>&part;2
</p>
<p>&part;x2
ψ (21.116)
</p>
<p>for initial values given by a Gaussian wave packet with constant momentum
</p>
<p>ψ0(x)=
(
</p>
<p>2
</p>
<p>aπ
</p>
<p>)1/4
eik0xe&minus;x
</p>
<p>2/a . (21.117)
</p>
<p>The exact solution can be easily found. Fourier transformation of (21.117) gives
</p>
<p>ψ̂k(t = 0)=
1&radic;
2π
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
dx e&minus;ikxψ0(x)
</p>
<p>=
(
</p>
<p>a
</p>
<p>2π
</p>
<p>)1/4
exp
</p>
<p>{
&minus;a
</p>
<p>4
(k &minus; k0)2
</p>
<p>}
. (21.118)
</p>
<p>Time evolution in k-space is rather simple
</p>
<p>i
&part;
</p>
<p>&part;t
ψ̂k =
</p>
<p>k2
</p>
<p>2
ψ̂k (21.119)
</p>
<p>hence
</p>
<p>ψ̂k(t)= e&minus;ik
2t/2ψ̂k(t = 0) (21.120)
</p>
<p>and Fourier back transformation gives the solution of the time dependent Schr&ouml;-
dinger equation in real space
</p>
<p>ψ(t, x)= 1&radic;
2π
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
dx eikxψ̂k(t)
</p>
<p>=
(
</p>
<p>2a
</p>
<p>π
</p>
<p>)1/4 1&radic;
a + 2it
</p>
<p>exp
</p>
<p>{
&minus;
(x &minus; i ak02 )2 +
</p>
<p>ak20
4 (a + 2it)
</p>
<p>a + 2it
</p>
<p>}
.
</p>
<p>(21.121)
</p>
<p>Finally, the probability density is given by a Gaussian
</p>
<p>∣∣ψ(t, x)
∣∣2 =
</p>
<p>&radic;
2a
</p>
<p>π
</p>
<p>1&radic;
a2 + 4t2
</p>
<p>exp
</p>
<p>{
&minus; 2a
a2 + 4t2 (x &minus; k0t)
</p>
<p>2
}
</p>
<p>(21.122)</p>
<p/>
</div>
<div class="page"><p/>
<p>21.3 Few-State Systems 403
</p>
<p>Fig. 21.6 (Conservation of
norm and energy) The free
motion of a Gaussian wave
packet is simulated with the
Crank-Nicolson method (CN)
the second order differences
method (SOD) with 3 point
(21.48) 5 point (21.49) and
7-point (21.50) differences
and with the real-space
split-operator method (SPO).
�t = 10&minus;3, �x = 0.1, a = 1,
k0 = 3.77
</p>
<p>Fig. 21.7 (Free wave-packet
motion) The free motion of a
Gaussian wave packet is
simulated. The probability
density is shown for the initial
Gaussian wave packet and at
later times t = 1,2,3,4.
Results from the second order
differences method with
3 point differences ((21.48),
dash-dotted) and 5 point
differences ((21.49), dashed)
are compared with the exact
solution ((21.122), thin solid
line). �t = 10&minus;3, �x = 0.1,
a = 1, k0 = 3.77
</p>
<p>which moves with constant velocity k0 and kinetic energy
&int; &infin;
</p>
<p>&minus;&infin;
dx ψ&lowast;(x, t)
</p>
<p>(
&minus;�
</p>
<p>2
</p>
<p>2
</p>
<p>&part;2
</p>
<p>&part;x2
</p>
<p>)
ψ(x, t)= 1
</p>
<p>2
</p>
<p>(
k20 +
</p>
<p>1
</p>
<p>a
</p>
<p>)
. (21.123)
</p>
<p>Numerical results are shown in Figs. 21.6, 21.7 and Table 21.1.
</p>
<p>21.3 Few-State Systems
</p>
<p>In the following we discuss simple models which reduce the wavefunction to the
superposition of a few important states, for instance an initial and a final state which
are coupled by a resonant interaction. We approximate the solution of the time de-
pendent Schr&ouml;dinger equation as a linear combination
</p>
<p>∣∣ψ(t)
&rang;
&asymp;
</p>
<p>M&sum;
</p>
<p>j=1
Cj (t)|φj 〉 (21.124)</p>
<p/>
</div>
<div class="page"><p/>
<p>404 21 Simple Quantum Systems
</p>
<p>Table 21.1 (Accuracy of finite differences methods) The relative error of the kinetic energy
(21.123) is shown as calculated with different finite difference methods
</p>
<p>Method Ekin
Ekin&minus;Eexactkin
</p>
<p>Eexactkin
</p>
<p>Crank-Nicolson (CN) with 3 point differences 7.48608 &minus;1.6 &times; 10&minus;2
</p>
<p>Second Order Differences with 3 point differences (SOD3) 7.48646 &minus;1.6 &times; 10&minus;2
</p>
<p>Second Order Differences with 5 point differences (SOD5) 7.60296 &minus;4.6 &times; 10&minus;4
</p>
<p>Second Order Differences with 7 point differences (SOD7) 7.60638 &minus;0.9 &times; 10&minus;5
</p>
<p>Split-Operator method (SOP) with 3 point differences 7.48610 &minus;1.6 &times; 10&minus;2
</p>
<p>exact 7.60645
</p>
<p>of certain basis states |φ1〉 &middot; &middot; &middot; |φM 〉6 which are assumed to satisfy the necessary
boundary conditions and to be orthonormalized
</p>
<p>〈φi |φj 〉 = δij . (21.125)
</p>
<p>Applying the method of weighted residuals (Sect. 11.4) we minimize the residual
</p>
<p>|R〉 = i�
&sum;
</p>
<p>j
</p>
<p>Ċj (t)|φj 〉 &minus;
&sum;
</p>
<p>j
</p>
<p>Cj (t)H |φj 〉 (21.126)
</p>
<p>by choosing the basis functions as weight functions (Sect. 11.4.4) and solving the
system of ordinary differential equations
</p>
<p>0 =Rj = 〈φj |R〉 = i�Ċj &minus;
&sum;
</p>
<p>j &prime;
〈φj |H |φj &prime;〉Cj &prime; (21.127)
</p>
<p>which can be written
</p>
<p>i�Ċi =
M&sum;
</p>
<p>j=1
Hi,jCj (t) (21.128)
</p>
<p>with the matrix elements of the Hamiltonian
</p>
<p>Hi,j = 〈φi |H |φj 〉. (21.129)
</p>
<p>In matrix form (21.128) reads
</p>
<p>i�
</p>
<p>⎛
⎜⎝
</p>
<p>Ċ1(t)
...
</p>
<p>ĊM(t)
</p>
<p>⎞
⎟⎠=
</p>
<p>⎛
⎜⎝
</p>
<p>H1,1 &middot; &middot; &middot; H1,M
...
</p>
<p>. . .
...
</p>
<p>HM,1 &middot; &middot; &middot; HM,M
</p>
<p>⎞
⎟⎠
</p>
<p>⎛
⎜⎝
</p>
<p>C1(t)
...
</p>
<p>CM(t)
</p>
<p>⎞
⎟⎠ (21.130)
</p>
<p>or more symbolically
</p>
<p>i�Ċ(t)=HC(t). (21.131)
</p>
<p>6This basis is usually incomplete.</p>
<p/>
</div>
<div class="page"><p/>
<p>21.3 Few-State Systems 405
</p>
<p>Fig. 21.8 Two-state system
model
</p>
<p>If the Hamilton operator does not depend explicitly on time (H = const.) the
formal solution of (21.131) is given by
</p>
<p>C= exp
{
</p>
<p>t
</p>
<p>i�
H
</p>
<p>}
C(0). (21.132)
</p>
<p>From the solution of the eigenvalue problem
</p>
<p>HCλ = λCλ (21.133)
(eigenvalues λ and corresponding eigenvectors Cλ) we build the linear combination
</p>
<p>C=
&sum;
</p>
<p>λ
</p>
<p>aλCλe
λ
i�
t . (21.134)
</p>
<p>The amplitudes aλ can be calculated from the set of linear equations
</p>
<p>C(0)=
&sum;
</p>
<p>λ
</p>
<p>aλCλ. (21.135)
</p>
<p>In the following we use the 4th order Runge-Kutta method to solve (21.131) nu-
merically whereas the explicit solution (21.132) will be used to obtain approximate
analytical results for special limiting cases.
</p>
<p>A time dependent Hamiltonian H(t) appears in semiclassical models which treat
some of the slow degrees of freedom as classical quantities, for instance an electron
in the Coulomb field of (slowly) moving nuclei
</p>
<p>H(t)= Tel +
&sum;
</p>
<p>j
</p>
<p>&minus;qje
4πε0|r&minus;Rj (t)|
</p>
<p>+
&sum;
</p>
<p>j&lt;j &prime;
</p>
<p>qjqj &prime;
</p>
<p>4πε0|Rj(t)&minus;Rj &prime;(t)|
(21.136)
</p>
<p>or in a time dependent electromagnetic field
</p>
<p>H(t)= Tel + Vel &minus; erE(t). (21.137)
</p>
<p>21.3.1 Two-State System
</p>
<p>The two-state system (also known as two-level system or TLS; Fig. 21.8) is the
simplest model of interacting states and is very often used in physics, for instance in
the context of quantum optics, quantum information, spintronics and quantum dots.
</p>
<p>Its interaction matrix is
</p>
<p>H =
(
E1 V
</p>
<p>V E2
</p>
<p>)
(21.138)</p>
<p/>
</div>
<div class="page"><p/>
<p>406 21 Simple Quantum Systems
</p>
<p>and the equations of motion are
</p>
<p>i�Ċ1 =E1C1 + VC2
i�Ċ2 =E2C2 + VC1.
</p>
<p>(21.139)
</p>
<p>Equations (21.139) can be solved analytically but this involves some lengthy ex-
pressions. Let us therefore concentrate on two limiting cases:
</p>
<p>(a) For E1 =E2 we add and subtract (21.139) to find
</p>
<p>i�
d
</p>
<p>dt
(C1 &plusmn;C2)= (E1 &plusmn; V )(C1 &plusmn;C2) (21.140)
</p>
<p>with the solution
</p>
<p>C1 &plusmn;C2 =
(
C1(0)&plusmn;C2(0)
</p>
<p>)
e&minus;it (E1&plusmn;V )/�. (21.141)
</p>
<p>For initial conditions
</p>
<p>C1(0)= 1 C2(0)= 0 (21.142)
the explicit solution is given by
</p>
<p>C1 = e&minus;itE1/� cos
V t
</p>
<p>�
|C1|2 = cos2
</p>
<p>V t
</p>
<p>�
=
</p>
<p>1 + cos 2V t
�
</p>
<p>2
(21.143)
</p>
<p>C2 =&minus;ie&minus;itE1/� sin
V t
</p>
<p>�
|C2|2 = sin2
</p>
<p>V t
</p>
<p>�
=
</p>
<p>1 &minus; cos 2V t
�
</p>
<p>2
(21.144)
</p>
<p>and the two-state system oscillates between the two states with the period
</p>
<p>T = π�
V
</p>
<p>. (21.145)
</p>
<p>(b) For V ≪ �E = E2 &minus; E17 perturbation theory for the small quantity V/�E
gives the following approximations:
</p>
<p>λ1 &asymp;E1 &minus;
V 2
</p>
<p>�E
</p>
<p>λ2 &asymp;E2 +
V 2
</p>
<p>�E
</p>
<p>(21.146)
</p>
<p>C1 &asymp;
(
</p>
<p>1
V
�E
</p>
<p>)
</p>
<p>C2 &asymp;
( &minus;V
</p>
<p>�E
1
</p>
<p>)
.
</p>
<p>(21.147)
</p>
<p>For initial values C(0)=
( 1
</p>
<p>0
</p>
<p>)
the amplitudes a1,2 are calculated from
</p>
<p>(
1
0
</p>
<p>)
=
(
a1 &minus; a2 V�E
a1
</p>
<p>V
�E
</p>
<p>+ a2
</p>
<p>)
(21.148)
</p>
<p>7We assume E2 &gt;E1.</p>
<p/>
</div>
<div class="page"><p/>
<p>21.3 Few-State Systems 407
</p>
<p>Fig. 21.9 (Numerical simulation of a two-state system) The equations of motion of the two-state
system (21.139) are integrated with the 4th order Runge-Kutta method. For two resonant states the
occupation probability of the initial state shows oscillations with the period (21.145) proportional
to V &minus;1. With increasing energy gap E2 &minus;E1 the amplitude of the oscillations decreases
</p>
<p>which gives in lowest order
</p>
<p>a1 &asymp; 1 &minus;
V 2
</p>
<p>�E2
</p>
<p>a2 &asymp;
V 2
</p>
<p>�E2
.
</p>
<p>(21.149)
</p>
<p>The approximate solution is
</p>
<p>C=
</p>
<p>⎛
⎜⎝
(1 &minus; V 2
</p>
<p>�E2
)e
</p>
<p>1
i�
(E1&minus; V
</p>
<p>2
</p>
<p>�E2
)t + V 2
</p>
<p>�E2
e
</p>
<p>1
i�
(E2+ V
</p>
<p>2
</p>
<p>�E2
)t
</p>
<p>V
�E
</p>
<p>e
1
i�
(E1&minus; V
</p>
<p>2
</p>
<p>�E2
)t &minus; V
</p>
<p>�E
e
</p>
<p>1
i�
(E2+ V
</p>
<p>2
</p>
<p>�E2
)t
</p>
<p>⎞
⎟⎠ (21.150)
</p>
<p>and the occupation probability of the initial state is
</p>
<p>|C1|2 &asymp; 1 &minus; 2
V 2
</p>
<p>�E2
+ 2 V
</p>
<p>2
</p>
<p>�E2
cos
</p>
<p>((
�E + 2 V
</p>
<p>2
</p>
<p>�E
</p>
<p>)
t
</p>
<p>)
. (21.151)
</p>
<p>Numerical results are shown in Fig. 21.9.</p>
<p/>
</div>
<div class="page"><p/>
<p>408 21 Simple Quantum Systems
</p>
<p>Fig. 21.10 Two-state system
in an oscillating field
</p>
<p>21.3.2 Two-State System with Time Dependent Perturbation
</p>
<p>Consider now a 2-state system with an oscillating perturbation (for instance an atom
or molecule in a laser field; Fig. 21.10)
</p>
<p>H =
(
</p>
<p>E1 V (t)
</p>
<p>V (t) E2
</p>
<p>)
V (t)= V0 cosωt. (21.152)
</p>
<p>The equations of motion are
</p>
<p>i�Ċ1 =E1C1 + V (t)C2
i�Ċ2 = V (t)C1 +E2C2.
</p>
<p>(21.153)
</p>
<p>After the substitutions
</p>
<p>C1 = e
E1
i� tu1
</p>
<p>C2 = e
E2
i� tu2
</p>
<p>(21.154)
</p>
<p>ω21 =
E2 &minus;E1
</p>
<p>�
(21.155)
</p>
<p>they become
</p>
<p>i�u̇1 = V (t)e
E2&minus;E1
</p>
<p>i� tu2 =
V0
</p>
<p>2
</p>
<p>(
e&minus;i(ω21&minus;ω)t + e&minus;i(ω21+ω)t
</p>
<p>)
u2
</p>
<p>i�u̇2 = V (t)e
E1&minus;E2
</p>
<p>i� tu1 =
V0
</p>
<p>2
</p>
<p>(
ei(ω21&minus;ω)t + ei(ω21+ω)t
</p>
<p>)
u1.
</p>
<p>(21.156)
</p>
<p>At larger times the system oscillates between the two states.8 Applying the rotating
wave approximation for ω&asymp; ω21 we neglect the fast oscillating perturbation
</p>
<p>i�u̇1 =
V0
</p>
<p>2
e&minus;i(ω21&minus;ω)tu2 (21.157)
</p>
<p>i�u̇2 =
V0
</p>
<p>2
ei(ω21&minus;ω)tu1 (21.158)
</p>
<p>and substitute
</p>
<p>u1 = a1e&minus;i(ω21&minus;ω)t (21.159)
to have
</p>
<p>8So called Rabi oscillations.</p>
<p/>
</div>
<div class="page"><p/>
<p>21.3 Few-State Systems 409
</p>
<p>Fig. 21.11 (Simulation of a two-state system in an oscillating field) The equations of motion
(21.153) are integrated with the 4th order Runge-Kutta method. At resonance the system oscillates
between the two states with the frequency V/�. The dashed curves show the corresponding solution
of a two-state system with constant coupling (Sect. 21.3.1)
</p>
<p>i�
(
ȧ1 &minus; a1i(ω21 &minus;ω)
</p>
<p>)
e&minus;i(ω21&minus;ω)t = V0
</p>
<p>2
e&minus;i(ω21&minus;ω)tu2 (21.160)
</p>
<p>i�u̇2 =
V0
</p>
<p>2
ei(ω21&minus;ω)te&minus;i(ω21&minus;ω)ta1 (21.161)
</p>
<p>or
</p>
<p>i�ȧ1 = �(ω&minus;ω21)a1 +
V0
</p>
<p>2
u2 (21.162)
</p>
<p>i�u̇2 =
V0
</p>
<p>2
a1 (21.163)
</p>
<p>which shows that the system behaves approximately like a two-state system with a
constant interaction V0/2 and an energy gap �(ω21 &minus; ω)= E2 &minus;E1 &minus; �ω (a com-
parison with a full numerical calculation is shown in Fig. 21.11).</p>
<p/>
</div>
<div class="page"><p/>
<p>410 21 Simple Quantum Systems
</p>
<p>Fig. 21.12 Superexchange
model
</p>
<p>21.3.3 Superexchange Model
</p>
<p>The concept of superexchange was originally formulated for magnetic interactions
[8] and later introduced to electron transfer theory [117]. It describes an indirect
interaction through high energy intermediates (Fig. 21.12). In the simplest case, we
have to consider two isoenergetic states i and f which do not interact directly but
via coupling to an intermediate state v.
</p>
<p>The interaction matrix is
</p>
<p>H =
</p>
<p>⎛
⎝
</p>
<p>0 V1 0
V1 E2 V2
0 V2 0
</p>
<p>⎞
⎠ . (21.164)
</p>
<p>For simplification we choose V1 = V2.
Let us first consider the special case of a resonant intermediate state E2 = 0:
</p>
<p>H =
</p>
<p>⎛
⎝
</p>
<p>0 V 0
V 0 V
0 V 0
</p>
<p>⎞
⎠ . (21.165)
</p>
<p>Obviously one eigenvalue is λ1 = 0 and the corresponding eigenvector is
</p>
<p>C1 =
</p>
<p>⎛
⎝
</p>
<p>1
0
&minus;1
</p>
<p>⎞
⎠ . (21.166)
</p>
<p>The two remaining eigenvalues are solutions of
</p>
<p>0 = det
</p>
<p>∣∣∣∣∣∣
</p>
<p>&minus;λ V 0
V &minus;λ V
0 V &minus;λ
</p>
<p>∣∣∣∣∣∣
= λ
</p>
<p>(
&minus;λ2 + 2V 2
</p>
<p>)
(21.167)
</p>
<p>which gives
</p>
<p>λ2,3 =&plusmn;
&radic;
</p>
<p>2V. (21.168)
</p>
<p>The eigenvectors are
</p>
<p>C2,3 =
</p>
<p>⎛
⎝
</p>
<p>1
&plusmn;
&radic;
</p>
<p>2
1
</p>
<p>⎞
⎠ . (21.169)
</p>
<p>From the initial values
</p>
<p>C(0)=
</p>
<p>⎛
⎝
</p>
<p>a1 + a2 + a3&radic;
2a2 &minus;
</p>
<p>&radic;
2a3
</p>
<p>&minus;a1 + a2 + a3
</p>
<p>⎞
⎠=
</p>
<p>⎛
⎝
</p>
<p>1
0
0
</p>
<p>⎞
⎠ (21.170)</p>
<p/>
</div>
<div class="page"><p/>
<p>21.3 Few-State Systems 411
</p>
<p>the amplitudes are calculated as
</p>
<p>a1 =
1
</p>
<p>2
a2 = a3 =
</p>
<p>1
</p>
<p>4
(21.171)
</p>
<p>and finally the solution is
</p>
<p>C= 1
2
</p>
<p>⎛
⎝
</p>
<p>1
0
&minus;1
</p>
<p>⎞
⎠+ 1
</p>
<p>4
</p>
<p>⎛
⎝
</p>
<p>1&radic;
2
</p>
<p>1
</p>
<p>⎞
⎠ e 1i�
</p>
<p>&radic;
2V t + 1
</p>
<p>4
</p>
<p>⎛
⎝
</p>
<p>1
&minus;
&radic;
</p>
<p>2
1
</p>
<p>⎞
⎠ e&minus; 1i�
</p>
<p>&radic;
2V t
</p>
<p>=
</p>
<p>⎛
⎜⎜⎝
</p>
<p>1
2 +
</p>
<p>1
2 cos
</p>
<p>&radic;
2V
�
</p>
<p>t
&radic;
</p>
<p>2
2 i sin
</p>
<p>&radic;
2V
�
</p>
<p>t
</p>
<p>&minus; 12 +
1
2 cos
</p>
<p>&radic;
2V
�
</p>
<p>t
</p>
<p>⎞
⎟⎟⎠ . (21.172)
</p>
<p>Let us now consider the case of a distant intermediate state V ≪ |E2|. λ1 = 0 and
the corresponding eigenvector still provide one solution. The two other eigenvalues
are approximately given by
</p>
<p>λ2,3 =&plusmn;
</p>
<p>&radic;
E22
</p>
<p>4
+ 2V 2 + E2
</p>
<p>2
&asymp; E2
</p>
<p>2
&plusmn; E2
</p>
<p>2
</p>
<p>(
1 + 4V
</p>
<p>2
</p>
<p>E22
</p>
<p>)
(21.173)
</p>
<p>λ2 &asymp;E2 +
2V 2
</p>
<p>E2
λ3 &asymp;&minus;
</p>
<p>2V 2
</p>
<p>E2
(21.174)
</p>
<p>and the eigenvectors by
</p>
<p>C2 &asymp;
</p>
<p>⎛
⎝
</p>
<p>1
E2
V
</p>
<p>+ 2V
E2
</p>
<p>1
</p>
<p>⎞
⎠ C3 &asymp;
</p>
<p>⎛
⎝
</p>
<p>1
&minus; 2V
</p>
<p>E2
1
</p>
<p>⎞
⎠ . (21.175)
</p>
<p>From the initial values
</p>
<p>C(0)=
</p>
<p>⎛
⎝
</p>
<p>1
0
0
</p>
<p>⎞
⎠=
</p>
<p>⎛
⎝
</p>
<p>a1 + a2 + a3
a2λ2 + a3λ3
&minus;a1 + a2 + a3
</p>
<p>⎞
⎠ (21.176)
</p>
<p>we calculate the amplitudes
</p>
<p>a1 =
1
</p>
<p>2
a2 &asymp;
</p>
<p>V 2
</p>
<p>E22
</p>
<p>a3 &asymp;
1
</p>
<p>2
</p>
<p>(
1 &minus; 2V
</p>
<p>2
</p>
<p>E22
</p>
<p>)
(21.177)
</p>
<p>and finally the solution
</p>
<p>C&asymp;
</p>
<p>⎛
⎜⎜⎜⎜⎝
</p>
<p>1
2 (1 + e
</p>
<p>&minus; 1
i�
</p>
<p>2V 2
E2
</p>
<p>t
)
</p>
<p>V
E2
</p>
<p>e
1
i�
E2t &minus; 2V
</p>
<p>E2
e
&minus; 1
</p>
<p>i�
2V 2
E2
</p>
<p>t
</p>
<p>1
2 (&minus;1 + e
</p>
<p>&minus; 1
i�
</p>
<p>2V 2
E2
</p>
<p>t
)
</p>
<p>⎞
⎟⎟⎟⎟⎠
. (21.178)
</p>
<p>The occupation probability of the initial state is</p>
<p/>
</div>
<div class="page"><p/>
<p>412 21 Simple Quantum Systems
</p>
<p>Fig. 21.13 (Numerical simulation of the superexchange model) The equations of motion for the
model (21.164) are solved numerically with the 4th order Runge-Kutta method. The energy gap
is varied to study the transition from the simple oscillation with ω =
</p>
<p>&radic;
2V/� (21.172) to the ef-
</p>
<p>fective two-state system with ω = Veff /� (21.179). Parameters are V1 = V2 = 1, E1 = E3 = 0,
E2 = 0,1,5,20. The occupation probability of the initial (solid curves), virtual intermediate
(dashed curves) and final (dash-dotted curves) state are shown
</p>
<p>|C1|2 =
1
</p>
<p>4
</p>
<p>∣∣∣1 + e&minus;
1
i�
</p>
<p>2V 2
E2
</p>
<p>t
∣∣∣
2
= cos2
</p>
<p>(
V 2
</p>
<p>�E2
t
</p>
<p>)
(21.179)
</p>
<p>which shows that the system behaves like a 2-state system with an effective interac-
tion of
</p>
<p>Veff =
V 2
</p>
<p>E2
. (21.180)
</p>
<p>Numerical results are shown in Fig. 21.13.
</p>
<p>21.3.4 Ladder Model for Exponential Decay
</p>
<p>For time independent Hamiltonian the solution (21.132) of the Schr&ouml;dinger equation
is a sum of oscillating terms and the quantum recurrence theorem [29] states that the
system returns to the initial state arbitrarily closely after a certain time Tr . However,
if the initial state is coupled to a larger number of final states, the recurrence time can
become very long and an exponential decay observed over a large period. The ladder</p>
<p/>
</div>
<div class="page"><p/>
<p>21.3 Few-State Systems 413
</p>
<p>Fig. 21.14 Ladder model
</p>
<p>model [27, 242] considers an initial state |0〉 interacting with a manifold of states
|1〉 &middot; &middot; &middot; |n〉, which do not interact with each other and are equally spaced (Fig. 21.14)
</p>
<p>H =
</p>
<p>⎛
⎜⎜⎜⎝
</p>
<p>0 V &middot; &middot; &middot; V
V E1
...
</p>
<p>. . .
</p>
<p>V En
</p>
<p>⎞
⎟⎟⎟⎠ Ej =E1 + (j &minus; 1)�E. (21.181)
</p>
<p>The equations of motion are
</p>
<p>i�Ċ0 = V
n&sum;
</p>
<p>j=1
Cj
</p>
<p>i�Ċj =EjCj + VC0.
(21.182)
</p>
<p>For the special case �E = 0 we simply have
</p>
<p>C̈0 =&minus;
V 2
</p>
<p>�2
nC0 (21.183)
</p>
<p>with an oscillating solution
</p>
<p>C0 &sim; cos
(
V
&radic;
n
</p>
<p>�
t
</p>
<p>)
. (21.184)
</p>
<p>Here the n states act like one state with an effective coupling of V
&radic;
n.
</p>
<p>For the general case �E �= 0 we substitute
</p>
<p>Cj = uje
Ej
i� t (21.185)
</p>
<p>and have
</p>
<p>i�u̇j e
Ej
i� t = VC0. (21.186)
</p>
<p>Integration gives
</p>
<p>uj =
V
</p>
<p>i�
</p>
<p>&int; t
</p>
<p>t0
</p>
<p>e&minus;
Ej
i� t
</p>
<p>&prime;
C0
</p>
<p>(
t &prime;
)
dt &prime; (21.187)
</p>
<p>and therefore
</p>
<p>Cj =
V
</p>
<p>i�
</p>
<p>&int; t
</p>
<p>t0
</p>
<p>ei
Ej
�
(t &prime;&minus;t)C0
</p>
<p>(
t &prime;
)
dt &prime;. (21.188)
</p>
<p>With the definition
</p>
<p>Ej = j &lowast; ��ω (21.189)</p>
<p/>
</div>
<div class="page"><p/>
<p>414 21 Simple Quantum Systems
</p>
<p>Fig. 21.15 (Numerical
solution of the ladder model)
The time evolution of the
ladder model (21.182) is
calculated with the 4th order
Runge-Kutta method for
N = 50 states and different
values of the coupling V
</p>
<p>we have
</p>
<p>Ċ0 =
V
</p>
<p>i�
</p>
<p>n&sum;
</p>
<p>j=1
Cj =&minus;
</p>
<p>V 2
</p>
<p>�2
</p>
<p>&sum;
</p>
<p>j
</p>
<p>&int; t
</p>
<p>t0
</p>
<p>eij�ω(t
&prime;&minus;t)C0
</p>
<p>(
t &prime;
)
dt &prime;. (21.190)
</p>
<p>We replace the sum by an integral over the continuous variable
</p>
<p>ω= j�ω (21.191)
and extend the integration range to &minus;&infin;&middot; &middot; &middot;&infin;. Then the sum becomes approxi-
mately a delta function
</p>
<p>&infin;&sum;
</p>
<p>j=&minus;&infin;
eij�ω(t
</p>
<p>&prime;&minus;t)�j &rarr;
&int; &infin;
</p>
<p>&minus;&infin;
eiω(t
</p>
<p>&prime;&minus;t) dω
</p>
<p>�ω
= 2π
</p>
<p>�ω
δ
(
t &minus; t &prime;
</p>
<p>)
(21.192)
</p>
<p>and the final result is an exponential decay law
</p>
<p>Ċ0 =&minus;
2πV 2
</p>
<p>�2�ω
C0 =&minus;
</p>
<p>2πV 2
</p>
<p>�
ρ(E)C0 (21.193)
</p>
<p>with the density of final states
</p>
<p>ρ(E)= 1
��ω
</p>
<p>= 1
�E
</p>
<p>. (21.194)
</p>
<p>Numerical results are shown in Fig. 21.15.
</p>
<p>21.3.5 Landau-Zener Model
</p>
<p>This model describes crossing of two states (Fig. 21.16), for instance for colliding
atoms or molecules [154, 282]. It is assumed that the interaction V is constant near
the crossing point and that the nuclei move classically with constant velocity</p>
<p/>
</div>
<div class="page"><p/>
<p>21.3 Few-State Systems 415
</p>
<p>Fig. 21.16 Slow atomic
collision
</p>
<p>Fig. 21.17 Multiple passage
of the interaction region
</p>
<p>H =
(
</p>
<p>0 V
V �E(t)
</p>
<p>)
�E(t)=�E0 + vt. (21.195)
</p>
<p>For small interaction V or large velocity &part;
&part;t
�E = Q̇ &part;
</p>
<p>&part;Q
�E the transition probabil-
</p>
<p>ity can be calculated with perturbation theory to give
</p>
<p>P = 2πV
2
</p>
<p>�
&part;
&part;t
�E
</p>
<p>. (21.196)
</p>
<p>This expression becomes invalid for small velocities. Here the system stays on the
adiabatic potential surface, i.e. P &rarr; 1. Landau and Zener found the following ex-
pression which is valid in both limits:
</p>
<p>PLZ = 1 &minus; exp
(
&minus; 2πV
</p>
<p>2
</p>
<p>�
&part;
&part;t
�E
</p>
<p>)
. (21.197)
</p>
<p>In case of collisions multiple crossing of the interaction region has to be taken into
account (Fig. 21.17). Numerical results are shown in Fig. 21.18.</p>
<p/>
</div>
<div class="page"><p/>
<p>416 21 Simple Quantum Systems
</p>
<p>Fig. 21.18 (Numerical solution of the Landau-Zener model) Numerical calculations (solid
curves) are compared with the Landau-Zener probability ((21.197), dashed lines) and the approxi-
mation ((21.196), dotted lines). The velocity is d�E/dt = 1
</p>
<p>21.4 The Dissipative Two-State System
</p>
<p>A two-state quantum system coupled to a thermal bath serves as a model for mag-
netic resonance phenomena, coherent optical excitations [263, 280] and, quite re-
cently, for a qubit, the basic element of a future quantum computer [73, 183]. Its
quantum state can not be described by a single wavefunction. Instead mixed quan-
tum states have to be considered which can be conveniently described within the
density matrix formalism [232].
</p>
<p>21.4.1 Equations of Motion for a Two-State System
</p>
<p>The equations of motion for a two-state system are
</p>
<p>i�ρ̇11 =H12ρ21 &minus; ρ12H21 (21.198)
i�ρ̇22 =H21ρ12 &minus; ρ21H12 (21.199)
i�ρ̇12 = (H11 &minus;H22)ρ12 +H12(ρ22 &minus; ρ11) (21.200)
</p>
<p>&minus;i�ρ̇21 = (H11 &minus;H22)ρ21 +H21(ρ22 &minus; ρ11) (21.201)</p>
<p/>
</div>
<div class="page"><p/>
<p>21.4 The Dissipative Two-State System 417
</p>
<p>which can be arranged as a system of linear equations9
</p>
<p>i�
</p>
<p>⎛
⎜⎜⎝
</p>
<p>ρ̇11
ρ̇22
ρ̇12
ρ̇21
</p>
<p>⎞
⎟⎟⎠=
</p>
<p>⎛
⎜⎜⎝
</p>
<p>0 0 &minus;H21 H12
0 0 H21 &minus;H12
</p>
<p>&minus;H12 H12 H11 &minus;H22 0
H21 &minus;H21 0 H22 &minus;H11
</p>
<p>⎞
⎟⎟⎠
</p>
<p>⎛
⎜⎜⎝
</p>
<p>ρ11
ρ22
ρ12
ρ21
</p>
<p>⎞
⎟⎟⎠ .
</p>
<p>(21.202)
</p>
<p>21.4.2 The Vector Model
</p>
<p>The density matrix is Hermitian
</p>
<p>ρij = ρ&lowast;ji (21.203)
its diagonal elements are real valued and due to conservation of probability
</p>
<p>ρ11 + ρ22 = const. (21.204)
Therefore the four elements of the density matrix can be specified by three real
parameters, which are usually chosen as
</p>
<p>x = 2&real;ρ21 (21.205)
y = 2&image;ρ21 (21.206)
z= ρ11 &minus; ρ22 (21.207)
</p>
<p>and satisfy the equations
</p>
<p>d
</p>
<p>dt
2&real;(ρ21)=&minus;
</p>
<p>1
</p>
<p>�
</p>
<p>(
(H11 &minus;H22)2&image;(ρ21)+ 2&image;(H12)(ρ11 &minus; ρ22)
</p>
<p>)
(21.208)
</p>
<p>d
</p>
<p>dt
2&image;(ρ21)=
</p>
<p>1
</p>
<p>�
</p>
<p>(
(H11 &minus;H22)2&real;(ρ21)&minus; 2&real;(H12)(ρ11 &minus; ρ22)
</p>
<p>)
(21.209)
</p>
<p>d
</p>
<p>dt
(ρ11 &minus; ρ22)=
</p>
<p>2
</p>
<p>�
</p>
<p>(
&image;(H12)2&real;(ρ21)+&real;H122&image;(ρ21)
</p>
<p>)
. (21.210)
</p>
<p>Together they form the Bloch vector
</p>
<p>r=
</p>
<p>⎛
⎝
x
</p>
<p>y
</p>
<p>z
</p>
<p>⎞
⎠ (21.211)
</p>
<p>which is often used to visualize the time evolution of a two-state system [81]. In
terms of the Bloch vector the density matrix is given by
</p>
<p>(
ρ11 ρ12
ρ21 ρ22
</p>
<p>)
=
(
</p>
<p>1+z
2
</p>
<p>x&minus;iy
2
</p>
<p>x+iy
2
</p>
<p>1&minus;z
2
</p>
<p>)
= 1
</p>
<p>2
(1 + rσ ) (21.212)
</p>
<p>9The matrix of this system corresponds to the Liouville operator.</p>
<p/>
</div>
<div class="page"><p/>
<p>418 21 Simple Quantum Systems
</p>
<p>with the Pauli matrices
</p>
<p>σx =
(
</p>
<p>0 1
1 0
</p>
<p>)
, σy =
</p>
<p>(
0 &minus;i
i 0
</p>
<p>)
, σz =
</p>
<p>(
1
</p>
<p>&minus;1
</p>
<p>)
. (21.213)
</p>
<p>From (21.208), (21.209), (21.210) we obtain the equation of motion
</p>
<p>d
</p>
<p>dt
</p>
<p>⎛
⎝
x
</p>
<p>y
</p>
<p>z
</p>
<p>⎞
⎠=
</p>
<p>⎛
⎜⎝
&minus;y H11&minus;H22
</p>
<p>�
&minus; z 2&image;(H12)
</p>
<p>�
</p>
<p>x H11&minus;H22
�
</p>
<p>&minus; z 2&real;(H12)
�
</p>
<p>x
2&image;(H12)
</p>
<p>�
+ y 2&real;(H12)
</p>
<p>�
</p>
<p>⎞
⎟⎠ (21.214)
</p>
<p>which can be written as a cross product
</p>
<p>d
</p>
<p>dt
r= ω&times; r (21.215)
</p>
<p>with
</p>
<p>ω=
</p>
<p>⎛
⎜⎝
</p>
<p>2
�
&real;H12
</p>
<p>&minus; 2
�
&image;H12
</p>
<p>1
�
(H11 &minus;H22)
</p>
<p>⎞
⎟⎠ . (21.216)
</p>
<p>Any normalized pure quantum state of the two-state system can be written as [75]
</p>
<p>|ψ〉 =
(
C1
C2
</p>
<p>)
= cos θ
</p>
<p>2
</p>
<p>(
1
0
</p>
<p>)
+ eiφ sin θ
</p>
<p>2
</p>
<p>(
0
1
</p>
<p>)
(21.217)
</p>
<p>corresponding to the density matrix
</p>
<p>ρ =
(
</p>
<p>cos2 θ2 e
&minus;iφ sin θ2 cos
</p>
<p>θ
2
</p>
<p>eiφ sin θ2 cos
θ
2 sin
</p>
<p>2 θ
2
</p>
<p>)
. (21.218)
</p>
<p>The Bloch vector
</p>
<p>r=
</p>
<p>⎛
⎝
</p>
<p>cosφ sin θ
sinφ sin θ
</p>
<p>cos θ
</p>
<p>⎞
⎠ (21.219)
</p>
<p>represents a point on the unit sphere (the Bloch sphere, Fig. 21.19). Mixed states cor-
respond to the interior of the Bloch sphere with the fully mixed state ρ =
</p>
<p>( 1/2 0
0 1/2
</p>
<p>)
</p>
<p>represented by the center of the sphere (Fig. 21.19).
</p>
<p>21.4.3 The Spin-1
2
System
</p>
<p>An important example of a two-state system is a particle with spin 12 . Its quantum
state can be described by a two-component vector
</p>
<p>(
C1
C2
</p>
<p>)
= C1
</p>
<p>(
1
0
</p>
<p>)
+C2
</p>
<p>(
0
1
</p>
<p>)
(21.220)</p>
<p/>
</div>
<div class="page"><p/>
<p>21.4 The Dissipative Two-State System 419
</p>
<p>Fig. 21.19 (Bloch sphere) Left: Any pure quantum state of a two-state system can be represented
by a point on the Bloch sphere. Right: The poles represent the basis states. Mixed quantum states
correspond to the interior of the sphere, the center represents the fully mixed state
</p>
<p>where the two unit vectors are eigenvectors of the spin component in z-direction
corresponding to the eigenvalues sz =&plusmn;�2 . The components of the spin operator are
given by the Pauli matrices
</p>
<p>Si =
�
</p>
<p>2
σi (21.221)
</p>
<p>and have expectation values
</p>
<p>〈S〉 = �
2
</p>
<p>(
C&lowast;1 C
</p>
<p>&lowast;
2
</p>
<p>)
⎛
⎝
σx
σy
σz
</p>
<p>⎞
⎠
(
C1
C2
</p>
<p>)
= �
</p>
<p>⎛
⎜⎜⎝
</p>
<p>C&lowast;1C2+C&lowast;2C1
2
</p>
<p>C&lowast;1C2&minus;C&lowast;2C1
2i
</p>
<p>|C1|2&minus;|C2|2
2
</p>
<p>⎞
⎟⎟⎠ . (21.222)
</p>
<p>The ensemble average for a system of spin- 12 particles is given by the Bloch vector
</p>
<p>〈S〉 = �
</p>
<p>⎛
⎜⎝
</p>
<p>ρ21+ρ12
2
</p>
<p>ρ21&minus;ρ12
2i
</p>
<p>ρ11&minus;ρ22
2
</p>
<p>⎞
⎟⎠=
</p>
<p>�
</p>
<p>2
r. (21.223)
</p>
<p>The Hamiltonian of a spin- 12 particle in a magnetic field B is
</p>
<p>H =&minus;γ �
2
σB=&minus;γ �
</p>
<p>2
</p>
<p>(
Bz Bx &minus; iBy
</p>
<p>Bx + iBy &minus;Bz
</p>
<p>)
(21.224)
</p>
<p>from which the following relations are obtained
</p>
<p>γBx =&minus;
2
</p>
<p>�
&real;H12 (21.225)
</p>
<p>γBy =
2
</p>
<p>�
&image;H12 (21.226)</p>
<p/>
</div>
<div class="page"><p/>
<p>420 21 Simple Quantum Systems
</p>
<p>γBz =&minus;
H11 &minus;H22
</p>
<p>�
(21.227)
</p>
<p>ω=&minus;γB. (21.228)
The average magnetization
</p>
<p>m= γ 〈S〉 = γ �
2
r (21.229)
</p>
<p>obeys the equation of motion
</p>
<p>d
</p>
<p>dt
m=&minus;γB&times;m. (21.230)
</p>
<p>21.4.4 Relaxation Processes&mdash;The Bloch Equations
</p>
<p>Relaxation of the nuclear magnetization due to interaction with the environment
was first described phenomenologically by Bloch in 1946 [28]. A more rigorous
description was given later [201, 269] and also applied to optical transitions [179].
Recently electron spin relaxation has attracted much interest in the new field of
spintronics [284] and the dissipative two-state system has been used to describe the
decoherence of a qubit [46].
</p>
<p>21.4.4.1 Phenomenological Description
</p>
<p>In thermal equilibrium the density matrix is given by a canonical distribution
</p>
<p>ρeq = e
&minus;βH
</p>
<p>tr(e&minus;βH )
(21.231)
</p>
<p>which for a two-state system without perturbation
</p>
<p>H0 =
(
</p>
<p>�
2
</p>
<p>&minus;�2
</p>
<p>)
(21.232)
</p>
<p>becomes
</p>
<p>ρeq =
(
</p>
<p>e&minus;β�/2
</p>
<p>eβ�/2+e&minus;β�/2
eβ�/2
</p>
<p>eβ�/2+e&minus;β�/2
</p>
<p>)
(21.233)
</p>
<p>where, as usually β = 1/kBT . If the energy gap is very large �≫ kBT like for an
optical excitation, the equilibrium state is the state with lower energy10
</p>
<p>ρeq =
(
</p>
<p>0 0
0 1
</p>
<p>)
. (21.234)
</p>
<p>10We assume �&ge; 0, such that the equilibrium value of z= ρ11 &minus; ρ22 is negative. Eventually, the
two states have to be exchanged.</p>
<p/>
</div>
<div class="page"><p/>
<p>21.4 The Dissipative Two-State System 421
</p>
<p>The phenomenological model assumes that deviations of the occupation difference
from its equilibrium value
</p>
<p>ρ
eq
</p>
<p>11 &minus; ρ
eq
</p>
<p>22 =&minus; tanh
(
</p>
<p>�
</p>
<p>2kBT
</p>
<p>)
(21.235)
</p>
<p>decay exponentially with a time constant T1 (for NMR this is the spin-lattice relax-
ation time)
</p>
<p>d
</p>
<p>dt |Rel
(ρ11 &minus; ρ22)=&minus;
</p>
<p>1
</p>
<p>T1
</p>
<p>[
(ρ11 &minus; ρ22)&minus;
</p>
<p>(
ρ
</p>
<p>eq
</p>
<p>11 &minus; ρ
eq
</p>
<p>22
</p>
<p>)]
. (21.236)
</p>
<p>The coherence of the two states decays exponentially with a time constant T2
which is closely related to T1 in certain cases11 but can be much smaller than T1 if
there are additional dephasing mechanisms. The equation
</p>
<p>d
</p>
<p>dt |Rel
ρ12 =&minus;
</p>
<p>1
</p>
<p>T2
ρ12 (21.237)
</p>
<p>describes the decay of the transversal polarization due to spatial and temporal dif-
ferences of different spins (spin-spin relaxation), whereas for an optical excitation
or a qubit it describes the loss of coherence of a single two-state system due to
interaction with its environment.
</p>
<p>The combination of (21.230) and the relaxation terms (21.236), (21.237) gives
the Bloch equations [28] which were originally formulated to describe the time evo-
lution of the macroscopic polarization
</p>
<p>dm
</p>
<p>dt
=&minus;γB&times;m&minus;R(m&minus;meq) R =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>1
T2
</p>
<p>0 0
</p>
<p>0 1
T2
</p>
<p>0
</p>
<p>0 0 1
T1
</p>
<p>⎞
⎟⎟⎠ . (21.238)
</p>
<p>For the components of the Bloch vector they read explicitly
</p>
<p>d
</p>
<p>dt
</p>
<p>⎛
⎝
x
</p>
<p>y
</p>
<p>z
</p>
<p>⎞
⎠=
</p>
<p>⎛
⎜⎝
</p>
<p>&minus;1/T2 &minus; 1� (H11 &minus;H22) &minus;
2
�
&image;H12
</p>
<p>1
�
(H11 &minus;H22) &minus;1/T2 &minus; 2�&real;H12
</p>
<p>2
�
&image;H12 2�&real;H12 &minus;1/T1
</p>
<p>⎞
⎟⎠
</p>
<p>⎛
⎝
x
</p>
<p>y
</p>
<p>z
</p>
<p>⎞
⎠
</p>
<p>+
</p>
<p>⎛
⎝
</p>
<p>0
0
</p>
<p>zeq/T1
</p>
<p>⎞
⎠ . (21.239)
</p>
<p>21.4.5 The Driven Two-State System
</p>
<p>The Hamiltonian of a two-state system (for instance an atom or molecule) in an os-
cillating electric field Ee&minus;iωf t with energy splitting � and transition dipole moment
μ is
</p>
<p>11For instance T2 = 2T1 for pure radiative damping.</p>
<p/>
</div>
<div class="page"><p/>
<p>422 21 Simple Quantum Systems
</p>
<p>H =
(
</p>
<p>�
2 &minus;μEe&minus;iωf t
</p>
<p>&minus;μEeiωf t &minus;�2
</p>
<p>)
. (21.240)
</p>
<p>The corresponding magnetic field
</p>
<p>Bx =
2
</p>
<p>γ�
μE cosωf t (21.241)
</p>
<p>By =
2
</p>
<p>γ�
μE sinωf t (21.242)
</p>
<p>Bz =&minus;
�
</p>
<p>γ�
(21.243)
</p>
<p>is that of a typical NMR experiment with a constant component along the z-axis and
a rotating component in the xy-plane.
</p>
<p>21.4.5.1 Free Precession
</p>
<p>Consider the special case Bz = const., Bx = By = 0. The corresponding Hamilto-
nian matrix is diagonal
</p>
<p>H =
(
</p>
<p>�Ω0
2 0
</p>
<p>0 &minus;�Ω02
</p>
<p>)
(21.244)
</p>
<p>with the Larmor-frequency
</p>
<p>Ω0 =
�
</p>
<p>�
=&minus;γB0. (21.245)
</p>
<p>The equations of motion for the density matrix are
</p>
<p>&part;
</p>
<p>&part;t
(ρ11 &minus; ρ22)=&minus;
</p>
<p>(ρ11 &minus; ρ22)&minus; (ρeq11 &minus; ρ
eq
</p>
<p>22)
</p>
<p>T1
(21.246)
</p>
<p>i�
&part;
</p>
<p>&part;t
ρ12 = �Ω0ρ12 &minus; i�
</p>
<p>1
</p>
<p>T2
ρ12 (21.247)
</p>
<p>with the solution
</p>
<p>(ρ11 &minus; ρ22)=
(
ρ
</p>
<p>eq
</p>
<p>11 &minus; ρ
eq
</p>
<p>22
</p>
<p>)
+
[(
ρ11(0)&minus; ρ22(0)
</p>
<p>)
&minus;
(
ρ
</p>
<p>eq
</p>
<p>11 &minus; ρ
eq
</p>
<p>22
</p>
<p>)]
e&minus;t/T1
</p>
<p>(21.248)
</p>
<p>ρ12 = ρ12(0)e&minus;iΩ0t&minus;t/T2 . (21.249)
The Bloch vector
</p>
<p>r=
</p>
<p>⎛
⎝
(x0 cosΩ0t &minus; y0 sinΩ0t)e&minus;t/T2
(y0 cosΩ0t + x0 sinΩ0t)e&minus;t/T2
</p>
<p>zeq + (z0 &minus; zeq)e&minus;t/T1
</p>
<p>⎞
⎠ (21.250)
</p>
<p>is subject to damped precession around the z-axis with the Larmor frequency
(Fig. 21.20).</p>
<p/>
</div>
<div class="page"><p/>
<p>21.4 The Dissipative Two-State System 423
</p>
<p>Fig. 21.20 (Free precession) The Bloch equations (21.239) are numerically solved with the 4th
order Runge-Kutta method. After excitation with a short resonant pulse the free precession is ob-
served. Left: The occupation difference z= ρ11&minus;ρ22 decays exponentially to its equilibrium value.
Right: In the xy-plane the Bloch vector moves on a spiral towards the equilibrium position (x = 0,
y = 0)
</p>
<p>21.4.5.2 Stationary Solution for Monochromatic Excitation
</p>
<p>For the two-state system (21.240) with
</p>
<p>H11 &minus;H22 =�= �Ω0 (21.251)
H12 = V0(cosωf t &minus; i sinωf t) (21.252)
</p>
<p>the solution of the Bloch equations (21.238)
</p>
<p>d
</p>
<p>dt
</p>
<p>⎛
⎝
x
</p>
<p>y
</p>
<p>z
</p>
<p>⎞
⎠=
</p>
<p>⎛
⎜⎝
</p>
<p>&minus;1/T2 &minus;Ω0 2V0� sinωf t
Ω0 &minus;1/T2 &minus; 2V0� cosωf t
</p>
<p>&minus; 2V0
�
</p>
<p>sinωf t
2V0
�
</p>
<p>cosωf t &minus;1/T1
</p>
<p>⎞
⎟⎠
</p>
<p>⎛
⎝
x
</p>
<p>y
</p>
<p>z
</p>
<p>⎞
⎠+
</p>
<p>⎛
⎝
</p>
<p>0
0
</p>
<p>zeq/T1
</p>
<p>⎞
⎠
</p>
<p>(21.253)
</p>
<p>can be found explicitly [280]. We transform to a coordinate system which rotates
around the z-axis ((13.3) on page 243) with angular velocity ωf
</p>
<p>⎛
⎝
x&prime;
</p>
<p>y&prime;
</p>
<p>z&prime;
</p>
<p>⎞
⎠=
</p>
<p>⎛
⎝
</p>
<p>cos(ωf t) sin(ωf t) 0
&minus; sin(ωf t) cos(ωf t) 0
</p>
<p>0 0 1
</p>
<p>⎞
⎠
⎛
⎝
x
</p>
<p>y
</p>
<p>z
</p>
<p>⎞
⎠=A(t)
</p>
<p>⎛
⎝
x
</p>
<p>y
</p>
<p>z
</p>
<p>⎞
⎠ . (21.254)
</p>
<p>Then
</p>
<p>d
</p>
<p>dt
</p>
<p>⎛
⎝
x&prime;
</p>
<p>y&prime;
</p>
<p>z&prime;
</p>
<p>⎞
⎠= Ȧ
</p>
<p>⎛
⎝
x
</p>
<p>y
</p>
<p>z
</p>
<p>⎞
⎠+A d
</p>
<p>dt
</p>
<p>⎛
⎝
x
</p>
<p>y
</p>
<p>z
</p>
<p>⎞
⎠=
</p>
<p>(
ȦA&minus;1 +AKA&minus;1
</p>
<p>)
⎛
⎝
x&prime;
</p>
<p>y&prime;
</p>
<p>z&prime;
</p>
<p>⎞
⎠+A
</p>
<p>⎛
⎝
</p>
<p>0
0
zeq
T1
</p>
<p>⎞
⎠
</p>
<p>(21.255)
</p>
<p>with</p>
<p/>
</div>
<div class="page"><p/>
<p>424 21 Simple Quantum Systems
</p>
<p>K =
</p>
<p>⎛
⎜⎝
</p>
<p>&minus;1/T2 &minus;Ω0 2V0� sinωf t
Ω0 &minus;1/T2 &minus; 2V0� cosωf t
</p>
<p>&minus; 2V0
�
</p>
<p>sinωf t
2V0
�
</p>
<p>cosωf t &minus;1/T1
</p>
<p>⎞
⎟⎠ . (21.256)
</p>
<p>The matrix products are
</p>
<p>ȦA&minus;1 =W =
</p>
<p>⎛
⎝
</p>
<p>0 ωf 0
&minus;ωf 0 0
</p>
<p>0 0 0
</p>
<p>⎞
⎠ AKA&minus;1 =
</p>
<p>⎛
⎝
&minus;1/T2 &minus;Ω0 0
Ω0 &minus;1/T2 &minus; 2V0�
0 2V0
</p>
<p>�
&minus;1/T1
</p>
<p>⎞
⎠
</p>
<p>(21.257)
</p>
<p>and the equation of motion simplifies to
</p>
<p>⎛
⎝
ẋ&prime;
</p>
<p>ẏ&prime;
</p>
<p>ż&prime;
</p>
<p>⎞
⎠=
</p>
<p>⎛
⎜⎝
</p>
<p>&minus; 1
T2
</p>
<p>ωf &minus;Ω0 0
Ω0 &minus;ωf &minus; 1T2 &minus;
</p>
<p>2V0
�
</p>
<p>0 2V0
�
</p>
<p>&minus; 1
T1
</p>
<p>⎞
⎟⎠
</p>
<p>⎛
⎝
x&prime;
</p>
<p>y&prime;
</p>
<p>z&prime;
</p>
<p>⎞
⎠+
</p>
<p>⎛
⎝
</p>
<p>0
0
zeq
</p>
<p>T1
</p>
<p>⎞
⎠ . (21.258)
</p>
<p>For times short compared to the relaxation times the solution is approximately given
by harmonic oscillations. The generalized Rabi frequency ΩR follows from [96]
</p>
<p>iΩRx
&prime; = (ωf &minus;Ω0)y&prime; (21.259)
</p>
<p>iΩRy
&prime; = (Ω0 &minus;ωf )x&prime; &minus;
</p>
<p>2V0
�
</p>
<p>z&prime; (21.260)
</p>
<p>iΩRz
&prime; = 2V0
</p>
<p>�
y&prime; (21.261)
</p>
<p>as
</p>
<p>ΩR =
</p>
<p>&radic;
</p>
<p>(Ω0 &minus;ωf )2 +
(
</p>
<p>2V0
�
</p>
<p>)2
. (21.262)
</p>
<p>At larger times these oscillations are damped and the stationary solution is ap-
proached (Fig. 21.21) which is given by
</p>
<p>zeq
</p>
<p>1 + 4V
2
0
</p>
<p>�2
T1T2 + T 22 (ωf &minus;Ω0)2
</p>
<p>⎛
⎜⎝
</p>
<p>2T 22
V0
�
(Ω0 &minus;ωf )
</p>
<p>&minus;2T2 V0�
1 + T 22 (ωf &minus;Ω0)2
</p>
<p>⎞
⎟⎠ . (21.263)
</p>
<p>The occupation difference
</p>
<p>z= ρ11 &minus; ρ22 = zeq
(
</p>
<p>1 &minus;
4
V 20
�2
</p>
<p>T1T2
</p>
<p>1 + 4V
2
0
</p>
<p>�2
T1T2 + T 22 (ωf &minus;Ω0)2
</p>
<p>)
(21.264)
</p>
<p>has the form of a Lorentzian. The line width increases for higher intensities (power
broadening)
</p>
<p>�ω= 1
T2
</p>
<p>&radic;
</p>
<p>1 + 4
V 20
</p>
<p>�2
T1T2 (21.265)</p>
<p/>
</div>
<div class="page"><p/>
<p>21.4 The Dissipative Two-State System 425
</p>
<p>Fig. 21.21 (Monochromatic Excitation) The Bloch equations are solved numerically with the 4th
order Runge-Kutta method for a monochromatic perturbation with ω = 4, V0 = 0.5. Parameters
of the two-state system are ω0 = 5, zeq = &minus;1.0 and T1 = T2 = 5.0. The occupation difference
z= ρ11 &minus;ρ22 initially shows Rabi oscillations which disappear at larger times where the stationary
value z=&minus;0.51 is reached
</p>
<p>Fig. 21.22 (Resonance line)
The equations of motion of
the two-state system
including relaxation terms are
integrated with the 4th order
Runge-Kutta method until a
steady state is reached.
Parameters are ω0 = 5,
zeq =&minus;0.8, V = 0.01 and
T1 = T2 = 3.0, 6.9. The
change of the occupation
difference is shown as a
function of frequency
(circles) and compared with
the steady state solution
(21.263)
</p>
<p>and the maximum
</p>
<p>z(Ω0)
</p>
<p>zeq
= 1
</p>
<p>1 + 4V
2
0
</p>
<p>�2
T1T2
</p>
<p>(21.266)
</p>
<p>approaches zero (saturation), Figs. 21.22, 21.23.
</p>
<p>21.4.5.3 Excitation by a Resonant Pulse
</p>
<p>For a resonant pulse with real valued envelope V0(t) and initial phase angle Φ0</p>
<p/>
</div>
<div class="page"><p/>
<p>426 21 Simple Quantum Systems
</p>
<p>Fig. 21.23 (Power saturation
and broadening) The
resonance line is investigated
as a function of the coupling
strength V and compared
with the stationary solution
(21.263) to observe the
broadening of the line width
(21.265). Parameters are
ω0 = 5, zeq =&minus;1.0,
T1 = T2 = 100 and V = 0.5,
0.25, 0.125, 0.0625, 0.03125
</p>
<p>H12 = V0(t)e&minus;i(Ω0t+Φ0)
</p>
<p>the equation of motion in the rotating system is
</p>
<p>⎛
⎝
ẋ&prime;
</p>
<p>ẏ&prime;
</p>
<p>ż&prime;
</p>
<p>⎞
⎠=
</p>
<p>⎛
⎜⎜⎝
</p>
<p>&minus; 1
T2
</p>
<p>0 &minus; 2V0(t)
�
</p>
<p>sinΦ0
</p>
<p>0 &minus; 1
T2
</p>
<p>&minus; 2V0(t)
�
</p>
<p>cosΦ0
2V0(t)
</p>
<p>�
sinΦ0
</p>
<p>2V0(t)
�
</p>
<p>cosΦ0 &minus; 1T1
</p>
<p>⎞
⎟⎟⎠
</p>
<p>⎛
⎝
x&prime;
</p>
<p>y&prime;
</p>
<p>z&prime;
</p>
<p>⎞
⎠+
</p>
<p>⎛
⎝
</p>
<p>0
0
zeq
</p>
<p>T1
</p>
<p>⎞
⎠ .
</p>
<p>(21.267)
</p>
<p>If the relaxation times are large compared to the pulse duration this describes ap-
proximately a rotation around an axis in the xy-plane (compare with (13.24))
</p>
<p>d
</p>
<p>dt
r&prime; &asymp;W(t)r&prime; = 2V0(t)
</p>
<p>�
W0r
</p>
<p>&prime; (21.268)
</p>
<p>W0 =
</p>
<p>⎛
⎝
</p>
<p>0 0 &minus; sinΦ0
0 0 &minus; cosΦ0
</p>
<p>sinΦ0 cosΦ0 0
</p>
<p>⎞
⎠ . (21.269)
</p>
<p>Since the axis is time independent, a formal solution is given by
</p>
<p>r&prime;(t)= eW
&int; t
t0
</p>
<p>2V0(t
&prime;)
</p>
<p>�
dt &prime;
r&prime;(0)= eW0Φ(t)r&prime;(0) (21.270)
</p>
<p>with the phase angle
</p>
<p>Φ(t)=
&int; t
</p>
<p>t0
</p>
<p>2V0(t &prime;)
</p>
<p>�
dt &prime;. (21.271)
</p>
<p>Now, since
</p>
<p>W 20 =
</p>
<p>⎛
⎝
</p>
<p>&minus; sin2 Φ0 &minus; sinΦ0 cosΦ0 0
&minus; sinΦ0 cosΦ0 &minus; cos2 Φ0 0
</p>
<p>0 0 &minus;1
</p>
<p>⎞
⎠ (21.272)
</p>
<p>W 30 =&minus;W0 (21.273)
W 40 =&minus;W 20 (21.274)</p>
<p/>
</div>
<div class="page"><p/>
<p>21.4 The Dissipative Two-State System 427
</p>
<p>Fig. 21.24 (Rotation of the
Bloch vector by a resonant
pulse) A resonant pulse
rotates the Bloch vector by
the angle Φ around an axis in
the x&prime;y&prime;-plane
</p>
<p>the Taylor series of the exponential function in (21.270) can be summed up
</p>
<p>eW0Φ
</p>
<p>= 1 +ΦW0 +
1
</p>
<p>2
Φ2W 20 +
</p>
<p>1
</p>
<p>3!Φ
3W 30 + &middot; &middot; &middot;
</p>
<p>= 1 +W 20
(
Φ2
</p>
<p>2
&minus; Φ
</p>
<p>4
</p>
<p>4! + &middot; &middot; &middot;
)
+W0
</p>
<p>(
Φ &minus; Φ
</p>
<p>3
</p>
<p>3! + &middot; &middot; &middot;
)
</p>
<p>= 1 +W 20 (1 &minus; cosΦ)+W0 sinΦ
</p>
<p>=
</p>
<p>⎛
⎝
</p>
<p>1 &minus; sin2 Φ0(1 &minus; cosΦ) &minus; sinΦ0 cosΦ0(1 &minus; cosΦ) &minus; sinΦ0 sinΦ
&minus; sinΦ0 cosΦ0(1 &minus; cosΦ) 1 &minus; cos2 Φ0(1 &minus; cosΦ) &minus; cosΦ0 sinΦ
</p>
<p>sinΦ0 sinΦ cosΦ0 sinΦ cosΦ
</p>
<p>⎞
⎠
</p>
<p>=
</p>
<p>⎛
⎝
</p>
<p>cosΦ0 sinΦ0 0
&minus; sinΦ0 cosΦ0 0
</p>
<p>0 0 1
</p>
<p>⎞
⎠
⎛
⎝
</p>
<p>1 0 0
0 cosΦ &minus; sinΦ
0 sinΦ cosΦ
</p>
<p>⎞
⎠
</p>
<p>&times;
</p>
<p>⎛
⎝
</p>
<p>cosΦ0 &minus; sinΦ0 0
sinΦ0 cosΦ0 0
</p>
<p>0 0 1
</p>
<p>⎞
⎠ . (21.275)
</p>
<p>The result is a rotation about the angle Φ around an axis in the xy-plane deter-
mined by Φ0 (Fig. 21.24), especially around the x-axis for Φ0 = 0 and around the
y-axis for Φ0 = π2 .
</p>
<p>After a π -pulse (Φ = π ) the z-component changes its sign
</p>
<p>r&prime; =
</p>
<p>⎛
⎝
</p>
<p>cos(2Φ0) &minus; sin(2Φ0) 0
&minus; sin(2Φ0) &minus; cos(2Φ0) 0
</p>
<p>0 0 &minus;1
</p>
<p>⎞
⎠ r(0). (21.276)
</p>
<p>The transition between the two basis states z=&minus;1 and z= 1 corresponds to a spin
flip (Fig. 21.25). On the other hand, a π/2-pulse transforms the basis states into a
coherent mixture
</p>
<p>r&prime; =
</p>
<p>⎛
⎝
</p>
<p>1 &minus; sin2 Φ0 &minus; sinΦ0 cosΦ0 &minus; sinΦ0
&minus; sinΦ0 cosΦ0 1 &minus; cos2 Φ0 &minus; cosΦ0
</p>
<p>sinΦ0 cosΦ0 0
</p>
<p>⎞
⎠ r(0). (21.277)</p>
<p/>
</div>
<div class="page"><p/>
<p>428 21 Simple Quantum Systems
</p>
<p>Fig. 21.25 (Spin flip by a π -pulse) The equations of motion of the Bloch vector (21.253) are
solved with the 4th order Runge-Kutta method for an interaction pulse with a Gaussian shape. The
pulse is adjusted to obtain a spin flip. The influence of dephasing processes is studied. T1 = 1000,
tp = 1.8, V0 = 0.25. The occupation difference ρ11 &minus; ρ22 = z (solid curves) and the coherence
|ρ12| = 12
</p>
<p>&radic;
x2 + y2 (broken curves) are shown for several values of the dephasing time T2 = 5, 10,
</p>
<p>100, 1000
</p>
<p>21.4.6 Elementary Qubit Manipulation
</p>
<p>Whereas a classical bit can be only in one of two states
</p>
<p>either
</p>
<p>(
1
0
</p>
<p>)
or
</p>
<p>(
0
1
</p>
<p>)
(21.278)
</p>
<p>the state of a qubit is a quantum mechanical superposition
</p>
<p>|ψ〉 = C0
(
</p>
<p>1
0
</p>
<p>)
+C1
</p>
<p>(
0
1
</p>
<p>)
. (21.279)
</p>
<p>The time evolution of the qubit is described by a unitary transformation
</p>
<p>|ψ〉&rarr;U |ψ〉 (21.280)
which is represented by a complex 2 &times; 2 unitary matrix that has the general form
(see also Sect. 13.14)
</p>
<p>U =
(
</p>
<p>α β
</p>
<p>&minus;eiϕβ&lowast; eiϕα&lowast;
)
</p>
<p>|α|2 + |β|2 = 1, detU = eiϕ . (21.281)
</p>
<p>The Bloch vector is transformed with an orthogonal matrix A, which can be found
from (21.212) and the transformed density matrix UρU&minus;1
</p>
<p>r&rarr;Ar A=
</p>
<p>⎛
⎝
&real;((α2 &minus; β2)e&minus;iϕ) &image;((α2 + β2)e&minus;iϕ) &minus;2&real;(αβe&minus;iϕ)
&image;((β2 &minus; α2)e&minus;iϕ) &real;((α2 + β2)e&minus;iϕ) 2&image;(αβe&minus;iϕ)
</p>
<p>2&real;(α&lowast;β) 2&image;(α&lowast;β) (|α|2 &minus; |β|2)
</p>
<p>⎞
⎠ .
</p>
<p>(21.282)</p>
<p/>
</div>
<div class="page"><p/>
<p>21.4 The Dissipative Two-State System 429
</p>
<p>Any single qubit transformation can be realized as a sequence of rotations around
just two axes [126, 183, 263]. In the following we consider some simple transfor-
mations, so called quantum gates [274].
</p>
<p>21.4.6.1 Pauli-Gates
</p>
<p>Of special interest are the gates represented by the Pauli matrices U = σi since any
complex 2 &times; 2 matrix can be obtained as a linear combination of the Pauli matrices
and the unit matrix (Sect. 13.14). For all three of them detU =&minus;1 and ϕ = π .
</p>
<p>The X-gate
</p>
<p>UX = σx =
(
</p>
<p>0 1
1 0
</p>
<p>)
(21.283)
</p>
<p>corresponds to rotation by π radians around the x-axis ((21.276) with Φ0 = 0)
</p>
<p>AX =
</p>
<p>⎛
⎝
</p>
<p>1 0 0
0 &minus;1 0
0 0 &minus;1
</p>
<p>⎞
⎠ . (21.284)
</p>
<p>It is also known as NOT-gate since it exchanges the two basis states. Similarly, the
Y -gate
</p>
<p>UY = σy =
(
</p>
<p>0 &minus;i
i 0
</p>
<p>)
</p>
<p>rotates the Bloch vector by π radians around the y-axis (21.276 with Φ0 = π/2)
</p>
<p>AY =
</p>
<p>⎛
⎝
&minus;1 0 0
0 1 0
0 0 &minus;1
</p>
<p>⎞
⎠ (21.285)
</p>
<p>and the Z-gate
</p>
<p>UZ = σz =
(
</p>
<p>1 0
0 &minus;1
</p>
<p>)
(21.286)
</p>
<p>by π radians around the z-axis
</p>
<p>AZ =
</p>
<p>⎛
⎝
&minus;1 0 0
0 &minus;1 0
0 0 1
</p>
<p>⎞
⎠ . (21.287)
</p>
<p>This rotation can be replaced by two successive rotations in the xy-plane
</p>
<p>AZ =AXAY . (21.288)
</p>
<p>The corresponding transformation of the wavefunction produces an overall phase
shift of π/2 since the product of the Pauli matrices is σxσy = iσz, which is not
relevant for observable quantities.</p>
<p/>
</div>
<div class="page"><p/>
<p>430 21 Simple Quantum Systems
</p>
<p>21.4.6.2 Hadamard Gate
</p>
<p>The Hadamard gate is a very important ingredient for quantum computation. It trans-
forms the basis states into coherent superpositions and vice versa. It is described by
the matrix
</p>
<p>UH =
</p>
<p>⎛
⎝
</p>
<p>1&radic;
2
</p>
<p>1&radic;
2
</p>
<p>1&radic;
2
</p>
<p>&minus; 1&radic;
2
</p>
<p>⎞
⎠ (21.289)
</p>
<p>with detUH =&minus;1 and
</p>
<p>AH =
</p>
<p>⎛
⎝
</p>
<p>0 0 1
0 &minus;1 0
1 0 0
</p>
<p>⎞
⎠ (21.290)
</p>
<p>which can be obtained as the product
</p>
<p>AH =
</p>
<p>⎛
⎝
</p>
<p>0 0 &minus;1
0 1 0
1 0 0
</p>
<p>⎞
⎠
⎛
⎝
</p>
<p>1 0 0
0 &minus;1 0
0 0 &minus;1
</p>
<p>⎞
⎠ (21.291)
</p>
<p>of a rotation by π radians around the x-axis and a second rotation by π/2 radians
around the y-axis. The first rotation corresponds to the X-gate and the second to
(21.277) with Φ0 = π/2
</p>
<p>U =
</p>
<p>⎛
⎝
</p>
<p>1&radic;
2
</p>
<p>1&radic;
2
</p>
<p>&minus; 1&radic;
2
</p>
<p>1&radic;
2
</p>
<p>⎞
⎠ . (21.292)
</p>
<p>21.5 Problems
</p>
<p>Problem 21.1 (Wave packet motion) In this computer experiment we solve the
Schr&ouml;dinger equation for a particle in the potential V (x) for an initially localized
Gaussian wave packet ψ(t = 0, x) &sim; exp(&minus;a(x &minus; x0)2). The potential is a box,
a harmonic parabola or a fourth order double well. Initial width and position of the
wave packet can be varied.
</p>
<p>&bull; Try to generate the time independent ground state wave function for the harmonic
oscillator.
</p>
<p>&bull; Observe the dispersion of the wave packet for different conditions and try to gen-
erate a moving wave packet with little dispersion.
</p>
<p>&bull; Try to observe tunneling in the double well potential.
</p>
<p>Problem 21.2 (Two-state system) In this computer experiment a two-state system
is simulated. Amplitude and frequency of an external field can be varied as well as
the energy gap between the two states (see Fig. 21.9).</p>
<p/>
</div>
<div class="page"><p/>
<p>21.5 Problems 431
</p>
<p>Fig. 21.26 (Generation of a coherent mixture by a π/2-pulse) The equations of motion of the
Bloch vector (21.253) are solved with the 4th order Runge-Kutta method for an interaction pulse
with a Gaussian shape. The pulse is adjusted to obtain a coherent mixture. The influence of dephas-
ing processes is studied. T1 = 1000, tp = 0.9, V0 = 0.25. The occupation difference ρ11 &minus; ρ22 = z
(solid curves) and the coherence |ρ12| = 12
</p>
<p>&radic;
x2 + y2 (broken curves) are shown for several values
</p>
<p>of the dephasing time T2 = 5, 10, 100, 1000
</p>
<p>&bull; Compare the time evolution at resonance and away from it.
</p>
<p>Problem 21.3 (Three-state system) In this computer experiment a three-state sys-
tem is simulated.
</p>
<p>&bull; Verify that the system behaves like an effective two-state system if the intermedi-
ate state is higher in energy than initial and final states (see Fig. 21.13).
</p>
<p>Problem 21.4 (Ladder model) In this computer experiment the ladder model is sim-
ulated. The coupling strength and the spacing of the final states can be varied.
</p>
<p>&bull; Check the validity of the exponential decay approximation (see Fig. 21.15).
</p>
<p>Problem 21.5 (Landau-Zener model) This computer experiment simulates the Lan-
dau Zener model. The coupling strength and the nuclear velocity can be varied (see
Fig. 21.18).
</p>
<p>&bull; Try to find parameters for an efficient crossing of the states.
</p>
<p>Problem 21.6 (Resonance line) In this computer experiment a two-state system
with damping is simulated. The resonance curve is calculated from the steady state
occupation probabilities (see Figs. 21.22, 21.23).
</p>
<p>&bull; Study the dependence of the line width on the intensity (power broadening).
</p>
<p>Problem 21.7 (Spin flip) The damped two-state system is now subject to an exter-
nal pulsed field (see Figs. 21.25, 21.26, 21.27).</p>
<p/>
</div>
<div class="page"><p/>
<p>432 21 Simple Quantum Systems
</p>
<p>Fig. 21.27 (Motion of the
Bloch vector during π2 and π
pulses) The trace of the
Bloch vector is shown in the
laboratory system. Left:
π
2 -pulse as in Fig. 21.26 with
T2 = 1000. Right: π -pulse as
in Fig. 21.25 with T2 = 1000
</p>
<p>&bull; Try to produce a coherent superposition state (π/2 pulse) or a spin flip (π pulse).
&bull; Investigate the influence of decoherence.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix I
</p>
<p>Performing the Computer Experiments
</p>
<p>The computer experiments are realized as Java-applets which can be run in any
browser that has the Java plug-in installed without installing anything else. They are
written in a C-like fashion which improves the readability for readers who are not
so familiar with object oriented programming. The source code can be studied most
conveniently with the netbeans environment which is open source and allows quick
generation of graphical user interfaces.
</p>
<p>After downloading and unzipping the zipped file from extras.springer.com you
have two options:
</p>
<p>P.O.J. Scherer, Computational Physics, Graduate Texts in Physics,
DOI 10.1007/978-3-319-00401-3,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>433</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-00401-3">http://dx.doi.org/10.1007/978-3-319-00401-3</a></div>
</div>
<div class="page"><p/>
<p>434 I Performing the Computer Experiments
</p>
<p>Run a program in your Browser
</p>
<p>Open the file CP-examples.html in your browser. If the Java plug-in is installed
properly you can start any one of the programs by simply clicking its number in the
left hand frame.
</p>
<p>Open a program with the netbeans environment
</p>
<p>If you have the netbeans environment installed, you can import any of the pro-
grams as a separate project by opening the corresponding folder in the directory
HTML/code/. You may have a look at the source code and compile and run it</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix II
</p>
<p>Methods and Algorithms
</p>
<p>Purpose Method Comments Pages
</p>
<p>Interpolation Lagrange polynomial explicit form, easy to evaluate 17
</p>
<p>Barycentric Lagrange
polynomial
</p>
<p>for evaluation at many points 17
</p>
<p>Newton&rsquo;s divided
differences
</p>
<p>new points added easily 18
</p>
<p>Neville method for evaluation at one point 20
</p>
<p>Spline interpolation smoother, less oscillatory 22
</p>
<p>Rational interpolation smoother, less oscillatory, often less
coefficients necessary
</p>
<p>25, 28
</p>
<p>Pad&eacute; approximation often better than Taylor series 25
</p>
<p>Barycentric rational
interpolation
</p>
<p>easy to evaluate 27
</p>
<p>Rational interpolation
without poles
</p>
<p>alternative to splines, analytical 31
</p>
<p>Multivariate
interpolation
</p>
<p>multidimensional 32
</p>
<p>Trigonometric
interpolation
</p>
<p>periodic functions 116
</p>
<p>Differentiation One-sided difference
quotient
</p>
<p>low error order 37
</p>
<p>Central difference
quotient
</p>
<p>higher error order 38
</p>
<p>Extrapolation high accuracy 221
</p>
<p>Higher derivatives finite difference methods 41
</p>
<p>Partial derivatives finite difference methods 42
</p>
<p>P.O.J. Scherer, Computational Physics, Graduate Texts in Physics,
DOI 10.1007/978-3-319-00401-3,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>435</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-00401-3">http://dx.doi.org/10.1007/978-3-319-00401-3</a></div>
</div>
<div class="page"><p/>
<p>436 II Methods and Algorithms
</p>
<p>Purpose Method Comments Pages
</p>
<p>Integration Newton-Cotes formulae equally spaced points 46
Trapezoidal rule simple, closed interval 46
Midpoint rule simple, open interval 48
Simpson&rsquo;s rule more accurate 46
Composite
Newton-Cotes rules
</p>
<p>for larger intervals 48
</p>
<p>Extrapolation
(Romberg)
</p>
<p>high accuracy 49
</p>
<p>Clenshaw-Curtis
expressions
</p>
<p>suitable for adaptive and
multidimensional quadrature
</p>
<p>50
</p>
<p>Gaussian integration high accuracy if polynomial
approximation possible
</p>
<p>52
</p>
<p>Monte Carlo integration high dimensional integrals 139
</p>
<p>Linear equations Gaussian elimination
(LU reduction)
</p>
<p>standard method for linear equations
and matrix inversion
</p>
<p>60
</p>
<p>QR decomposition numerically more stable 64
Iterative solution large sparse systems 73
Jacobi relaxation converges for diagonally dominant
</p>
<p>matrices, parallel computation
possible
</p>
<p>73
</p>
<p>Gauss-Seidel relaxation converges for symmetric positive
definite or diagonal dominant
matrices, no extra storage
</p>
<p>74
</p>
<p>Chessboard (black-red) two independent subgrids, especially
for Poisson equation
</p>
<p>307
</p>
<p>Damping and Successive
over-relaxation
</p>
<p>speeds up convergence for proper
relaxation parameter
</p>
<p>75
</p>
<p>Multigrid fast convergence but more
complicated
</p>
<p>307
</p>
<p>Conjugate gradients for symmetric positive definite
matrices, preconditioning often
necessary
</p>
<p>76
</p>
<p>Special LU
decomposition
</p>
<p>Tridiagonal linear equations 69
</p>
<p>Sherman-Morrison
formula
</p>
<p>Cyclic tridiagonal systems 71
</p>
<p>Root finding Bisection reliable but slow continuous functions 84
Regula falsi (false
position)
</p>
<p>speed and robustness between
bisection and interpolation
</p>
<p>85
</p>
<p>Newton-Raphson continuous derivative necessary,
converges fast if starting point is close
to a root
</p>
<p>85
</p>
<p>Interpolation (secant) no derivative necessary, but slower
than Newton
</p>
<p>87
</p>
<p>Inverse interpolation mainly used by combined methods 88
Dekker&rsquo;s combined
method
</p>
<p>Combination of bisection and secant
method
</p>
<p>91
</p>
<p>Brent&rsquo;s combined
method
</p>
<p>Combination of bisection, secant, and
quadratic inverse interpolation
methods, very popular
</p>
<p>92
</p>
<p>Chandrupatla&rsquo;s
combined method
</p>
<p>Uses quadratic interpolation whenever
possible, faster than Brent&rsquo;s method,
especially for higher order roots
</p>
<p>95</p>
<p/>
</div>
<div class="page"><p/>
<p>II Methods and Algorithms 437
</p>
<p>Purpose Method Comments Pages
</p>
<p>Multidimensional
root finding
</p>
<p>Newton-Raphson Needs full Hessian 97
Quasi-Newton
(Broyden)
</p>
<p>Hessian not needed, no matrix
inversion
</p>
<p>98
</p>
<p>Function
minimization
</p>
<p>Ternary search no gradient needed, very simple, for
unimodal functions
</p>
<p>99
</p>
<p>Golden section search
(Brent)
</p>
<p>faster than ternary search but more
complicated
</p>
<p>101
</p>
<p>Multidimensional
minimization
</p>
<p>Steepest descent simple but slow 106
Conjugate gradients faster than steepest descent 107
Newton-Raphson fast, if starting point close to
</p>
<p>minimum, needs full Hessian
107
</p>
<p>Quasi-Newton
(BFGS,DFP)
</p>
<p>Hessian not needed, very popular 108
</p>
<p>Fourier
transformation
</p>
<p>G&ouml;rtzel&rsquo;s algorithm efficient if only some Fourier
components are needed
</p>
<p>120
</p>
<p>Fast Fourier transform much faster than direct discrete
Fourier transform
</p>
<p>121
</p>
<p>Random numbers Linear congruent
mapping
</p>
<p>simple pseudo-random number
generator
</p>
<p>135
</p>
<p>Marsaglia-Zamann higher quality random numbers but
more complicated
</p>
<p>135
</p>
<p>RN with given
distribution
</p>
<p>inverse of cumulative distribution
function needed
</p>
<p>136
</p>
<p>Random points on unit
sphere
</p>
<p>random directions 137
</p>
<p>Gaussian RN
(Box-Muller)
</p>
<p>Gaussian random numbers 138
</p>
<p>Thermodynamic
average
</p>
<p>Simple sampling inefficient 141
Importance sampling samples preferentially important
</p>
<p>configurations
142
</p>
<p>Metropolis algorithm generates configurations according to
a canonical distribution
</p>
<p>142
</p>
<p>Eigenvalue
problems
</p>
<p>Direct solution only for very small dimension 148
Tridiagonal matrices explicit solutions for some special
</p>
<p>tridiagonal matrices
150
</p>
<p>Jacobi simple but not very efficient 148
QL efficient method for not too large
</p>
<p>matrices, especially in combination
with tridiagonalization by
Householder transformations
</p>
<p>156
</p>
<p>Lanczos iterative method for very large
matrices or if only a few eigenvalues
are needed
</p>
<p>159
</p>
<p>Singular value
decomposition (SVD)
</p>
<p>Generalization for arbitrary matrices 167</p>
<p/>
</div>
<div class="page"><p/>
<p>438 II Methods and Algorithms
</p>
<p>Purpose Method Comments Pages
</p>
<p>Data fitting Least square fit fit a model function to a set of data 162
Linear least square fit
with normal equations
</p>
<p>simple but less accurate 163
</p>
<p>Linear fit with
orthogonalization
</p>
<p>better numerical stability 165
</p>
<p>Linear fit with SVD expensive but more reliable, also for
rank deficient matrices
</p>
<p>172
</p>
<p>Low rank matrix
approximation
</p>
<p>data compression, total linear least
squares
</p>
<p>170
</p>
<p>Discretization Method of lines continuous time, discretized space 183
Eigenvector expansion
Finite differences simplest discretization, uniform grids 180
Finite volumes partial differential equations with a
</p>
<p>divergence term ( conservation laws),
flux conservative, allows unstructured
meshes and discontinuous material
parameters
</p>
<p>185
</p>
<p>Finite elements very flexible and general
discretization method but also more
complicated
</p>
<p>196
</p>
<p>Spectral methods expansion with global basis functions,
mostly polynomials and Fourier sums,
less expensive than finite elements but
not as accurate for discontinuous
material parameters and complicated
geometries
</p>
<p>193
</p>
<p>Dual grid for finite volumes 185, 314
Weighted residuals general method to determine the
</p>
<p>expansion coefficients
190
</p>
<p>Point collocation simplest criterion, often used for
nonlinear problems and spectral
methods
</p>
<p>191
</p>
<p>Sub-domains more general than finite volumes 191
Least square popular for computational fluid
</p>
<p>dynamics and electrodynamics
192
</p>
<p>Galerkin most widely used criterion, leads
often to symmetric matrices
</p>
<p>192
</p>
<p>Fourier pseudo-spectral
method
</p>
<p>very useful whenever a Laplacian is
involved, reduces dispersion
</p>
<p>193
</p>
<p>Boundary elements if the Green&rsquo;s function is available 204
</p>
<p>Time evolution Explicit forward Euler low error order and unstable, mainly
used as predictor step
</p>
<p>210
</p>
<p>Implicit backward Euler low error order but stable, used for
stiff problems and as corrector step
</p>
<p>212
</p>
<p>Improved Euler (Heun,
predictor-corrector)
</p>
<p>higher error order 213
</p>
<p>Nordsieck
predictor-corrector
</p>
<p>implicit method, has been used for
molecular dynamics
</p>
<p>215
</p>
<p>Gear predictor-corrector optimized for molecular dynamics 217
Explicit Runge Kutta
(2nd, 3rd, 4th)
</p>
<p>general and robust methods, easy step
size and quality control
</p>
<p>217</p>
<p/>
</div>
<div class="page"><p/>
<p>II Methods and Algorithms 439
</p>
<p>Purpose Method Comments Pages
</p>
<p>Extrapolation
(Gragg-Bulirsch-St&ouml;r)
</p>
<p>very accurate and very slow 221
</p>
<p>Explicit
Adams-Bashforth
</p>
<p>high error order but not self-starting,
for smooth functions, can be used as
predictor
</p>
<p>222
</p>
<p>Implicit Adams-Moulton better stability than explicit method,
can be used as corrector
</p>
<p>223
</p>
<p>Backward differentiation
(Gear)
</p>
<p>implicit, especially for stiff problems 223
</p>
<p>Linear multistep
predictor-corrector
</p>
<p>General class, includes
Adams-Bashforth-Moulton and Gear
methods
</p>
<p>224
</p>
<p>Verlet integration symplectic, time reversible, for
molecular dynamics
</p>
<p>225
</p>
<p>Position Verlet less popular 227
Velocity Verlet often used 227
St&ouml;rmer-Verlet if velocities are not needed 228
Beeman&rsquo;s method velocities more accurate than for
</p>
<p>St&ouml;rmer-Verlet
230
</p>
<p>Leapfrog simple but two different grids 231, 231, 343
Crank-Nicolson implicit, stable, diffusion and
</p>
<p>Schr&ouml;dinger equation
357, 347
</p>
<p>Lax-Wendroff hyperbolic differential equations 345
Two-step differential equation with second
</p>
<p>order time derivative
338
</p>
<p>Reduction to a first order
equation
</p>
<p>Derivatives treated as additional
variables
</p>
<p>340
</p>
<p>Two-variable transforms wave equation into a
system of two first order equations
</p>
<p>343
</p>
<p>Split operator approximates an operator by a product 360, 226, 399
</p>
<p>Unitary time
evolution
</p>
<p>Rational approximation implicit,unitary 392
Second order
differencing
</p>
<p>explicit, not exactly unitary 396
</p>
<p>Split operator Fourier low dispersion, needs fast Fourier
transformation
</p>
<p>399
</p>
<p>Real space product
formula
</p>
<p>fast but less accurate, useful for
wavepackets in coupled states
</p>
<p>399
</p>
<p>Rotation Reorthogonalization restore orthogonality of rotation
matrix
</p>
<p>250
</p>
<p>Quaternions optimum parametrization of the
rotation matrix
</p>
<p>256
</p>
<p>Euler angles numerical singularities 255
Explicit method low accuracy, reorthogonalization
</p>
<p>needed
250
</p>
<p>Implicit method higher accuracy, orthogonal
transformation
</p>
<p>251
</p>
<p>Molecular
dynamics
</p>
<p>Force field gradients needed for molecular dynamics 270
Normal mode analysis small amplitude motion around an
</p>
<p>equilibrium
274
</p>
<p>Behrendsen thermostat simple method to control temperature 281
Langevin dynamics Brownian motion 301</p>
<p/>
</div>
<div class="page"><p/>
<p>References
</p>
<p>1. D.J. Acheson, Elementary Fluid Dynamics (Oxford University Press, London, 1990)
2. N. Ahmed, T. Natarajan, K.R. Rao, IEEE Trans. Comput. 23, 90 (1974)
3. M.P. Allen, D.J. Tildesley, Computer Simulation of Liquids (Oxford University Press, Lon-
</p>
<p>don, 1989). ISBN 0-19-855645-4
4. I. Allonso-Mallo, N. Reguera, J. Comp. Physiol. 220, 409 (2006)
5. V.S. Allured, C.M. Kelly, C.R. Landis, J. Am. Chem. Soc. 113, 1 (1991)
6. W. Ames, Numerical Methods for Partial Differential Equations (Academic Press, San
</p>
<p>Diego, 1992)
7. Z.A. Anastassi, T.E. Simos, Phys. Rep. 482&ndash;483, 1 (2009)
8. P.W. Anderson, Phys. Rev. 79, 350 (1950)
9. X. Antoine, A. Arnold, C. Besse, M. Erhardt, A. Sch&auml;dle, Commun. Comput. Phys. 4, 729
</p>
<p>(2008)
10. A. Askar, A.S. Cakmak, J. Chem. Phys. 68, 2794 (1978)
11. J.S. Bader et al., J. Chem. Phys. 106, 2372 (1997)
12. C.T.H. Baker, Numer. Math. 15, 315 (1970)
13. G.A. Baker Jr., P. Graves-Morris, Pad&eacute; Approximants (Cambridge University Press, New
</p>
<p>York, 1996)
14. A.D. Bandrauk, H. Shen, Chem. Phys. Lett. 176, 428 (1991)
15. E.J. Bautista, J.M. Seminario, Int. J. Quant. Chem. 108, 180 (2008)
16. B. Beckermann, Numer. Math. 85, 553 (2000)
17. D. Beeman, J. Comp. Physiol. 20, 130 (1976)
18. G. Beer, I. Smith, C. Duenser, The Boundary Element Method with Programming: For En-
</p>
<p>gineers and Scientists (Springer, Berlin, 2008)
19. H. Bekker, H.J.C. Berendsen, W.F. van Gunsteren, J. Comput. Chem. 16, 527 (1995)
20. A. Ben-Israel, T.N.E. Greville, Generalized Inverses (Springer, Berlin, 2003). ISBN 0-387-
</p>
<p>00293-6
21. H.J.C. Berendsen, J.P.M. Postma, W.F. van Gunsteren, A. DiNola, J.R. Haak, J. Chem. Phys.
</p>
<p>81, 3684 (1984)
22. A. Bermann, S. Gueron, Math. Gaz. 86, 274 (2002)
23. J.P. Berrut, Comput. Math. Appl. 14, 1 (1988)
24. J.-P. Berrut, L.N. Trefethen, Barycentric Lagrange interpolation. SIAM Rev. 46(3), 501&ndash;517
</p>
<p>(2004)
25. J.P. Berrut, R. Baltensperger, H.D. Mittelmann, Int. Ser. Numer. Math. 151, 27 (2005)
26. K. Binder, Ising model, in Encyclopedia of Mathematics, Suppl. vol. 2, ed. by R. Hoksbergen
</p>
<p>(Kluwer Academic, Dordrecht, 2000), pp. 279&ndash;281
27. M. Bixon, J. Jortner, J. Chem. Phys. 48, 715 (1986)
28. F. Bloch, Nuclear induction. Phys. Rev. 70, 460 (1946)
</p>
<p>P.O.J. Scherer, Computational Physics, Graduate Texts in Physics,
DOI 10.1007/978-3-319-00401-3,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>441</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-00401-3">http://dx.doi.org/10.1007/978-3-319-00401-3</a></div>
</div>
<div class="page"><p/>
<p>442 References
</p>
<p>29. P. Bocchieri, A. Loinger, Phys. Rev. 107, 337 (1957)
30. P. Bochev, M. Gunzburger, Least Squares Finite Element Methods (Springer, Berlin, 2009)
31. E. Bonomi, J.-L. Lutton, SIAM Rev. 26, 551 (1984)
32. A.H. Boschitsch, M.O. Fenley, H.X. Zhou, J. Phys. Chem. B 106, 2741 (2002)
33. K.J. Bowers, E. Chow, H. Xu, R.U. Dror, M.P. Eastwood, B.A. Gregersen, J.L. Klepeis, I.
</p>
<p>Kolossvary, M.A. Moraes, F.D. Sacerdoti, J.K. Salmon, Y. Shan, D.E. Shaw, Scalable algo-
rithms for molecular dynamics simulations on commodity clusters, in SC 2006 Conference.
Proceedings of the ACM/IEEE (2006), p. 43
</p>
<p>34. G.E.P. Box, M.E. Muller, Ann. Math. Stat. 29, 610 (1958)
35. J.P. Boyd, Chebyshev and Fourier Spectral Methods (Dover, New York, 2001)
36. R.P. Brent, Comput. J. 14, 422 (1971)
37. R.P. Brent, Algorithms for Minimization without Derivatives (Prentice Hall, Englewood
</p>
<p>Cliffs, 1973), Chap. 4
38. I.N. Bronshtein, K.A. Semendyayev, Handbook of Mathematics, 3rd edn. (Springer, New
</p>
<p>York, 1997), p. 892
39. B.R. Brooks, R.E. Bruccoleri, B.D. Olafson, D.J. States, S. Swaminathan, M. Karplus,
</p>
<p>J. Comput. Chem. 4, 187 (1983)
40. B.R. Brooks, C.L. Brooks III, A.D. Mackerell Jr., L. Nilsson, R.J. Petrella, B. Roux, Y. Won,
</p>
<p>G. Archontis, C. Bartels, S. Boresch, A. Caflisch, L. Caves, Q. Cui, A.R. Dinner, M. Feig, S.
Fischer, J. Gao, M. Hodoscek, W. Im, K. Kuzcera, T. Lazaridis, J. Ma, V. Ovchinnikov, E.
Paci, R.W. Pastor, C.B. Post, J.Z. Pu, M. Schaefer, B. Tidor, R.M. Venable, H.L. Woodcock,
X. Wu, W. Yang, D.M. York, M. Karplus, J. Comput. Chem. 30, 1545 (2009)
</p>
<p>41. R. Brown, Philos. Mag. 4, 161 (1828)
42. C.G. Broyden, Math. Comput. 19, 577 (1965)
43. C.G. Broyden, J. Inst. Math. Appl. 6, 76 (1970)
44. R.E. Bruccoleri, J. Novotny, M.E. Davis, K.A. Sharp, J. Comput. Chem. 18, 268 (1997)
45. R.L. Burden, J.D. Faires, Numerical Analysis (Brooks Cole, Boston, 2010), Chap. 2
46. G. Burkard, R.H. Koch, D.P. DiVincenzo, Phys. Rev. B 69, 64503 (2004)
47. J.C.P. Bus, T.J. Dekker, ACM Trans. Math. Softw. 1, 330 (1975)
48. J.C. Butcher, The Numerical Analysis of Ordinary Differential Equations: Runge-Kutta and
</p>
<p>General Linear Methods (Wiley, New York, 1987)
49. R.E. Caflisch, Monte Carlo and quasi-Monte Carlo methods. Acta Numer. 7, 1&ndash;49 (1998)
50. R. Car, M. Parrinello, Phys. Rev. Lett. 55, 2471 (1985)
51. A. Castro, M.A.L. Marques, A. Rubio, J. Chem. Phys. 121, 3425 (2004)
52. Y.A. Cengel, Heat transfer&mdash;A Practical Approach, 2nd edn. (McGraw-Hill, New York,
</p>
<p>2003), p. 26. ISBN 0072458933, 9780072458930
53. C. Cerjan, K.C. Kulander, Comput. Phys. Commun. 63, 529 (1991)
54. T.R. Chandrupatla, Adv. Eng. Softw. 28, 145 (1997)
55. H.W. Chang, S.E. Liu, R. Burridge, Linear Algebra Appl. 430, 999 (2009)
56. D.L. Chapman, Philos. Mag. 25, 475 (1913)
57. R. Chen, H. Guo, J. Chem. Phys. 111, 9944 (1999)
58. M. Christen, P.H. H&uuml;nenberger, D. Bakowies, R. Baron, R. B&uuml;rgi, D.P. Geerke, T.N. Heinz,
</p>
<p>M.A. Kastenholz, V. Kr&auml;utler, C. Oostenbrink, C. Peter, D. Trzesniak, W.F. van Gunsteren,
J. Comput. Chem. 26, 1719 (2005)
</p>
<p>59. S. Chynoweth, U.C. Klomp, L.E. Scales, Comput. Phys. Commun. 62, 297 (1991)
60. C.W. Clenshaw, A.R. Curtis, Numer. Math. 2, 197 (1960)
61. L.A. Collins, J.D. Kress, R.B. Walker, Comput. Phys. Commun. 114, 15 (1998)
62. J.W. Cooley, J.W. Tukey, Math. Comput. 19, 297 (1965)
63. W.D. Cornell, P. Cieplak, C.I. Bayly, I.R. Gould, K.M. Merz Jr., D.M. Ferguson, D.C.
</p>
<p>Spellmeyer, T. Fox, J.W. Caldwell, P.A. Kollman, J. Am. Chem. Soc. 117, 5179 (1995)
64. R. Courant, K. Friedrichs, H. Lewy, Math. Ann. 100, 32 (1928)
65. J. Crank, P. Nicolson, Proc. Camb. Philol. Soc. 43, 50 (1947)
66. J.W. Daniel, W.B. Gragg, L. Kaufmann, G.W. Stewart, Math. Comput. 30, 772 (1976)
67. H. De Raedt, Comput. Phys. Rep. 7, 1 (1987)</p>
<p/>
</div>
<div class="page"><p/>
<p>References 443
</p>
<p>68. H. De Raedt, B. De Raedt, Phys. Rev. A 28, 3575 (1983)
69. P. Debye, E. H&uuml;ckel, Phys. Z. 24, 185 (1923)
70. T.J. Dekker, Finding a zero by means of successive linear interpolation, in Constructive
</p>
<p>Aspects of the Fundamental Theorem of Algebra, ed. by B. Dejon, P. Henrici (Wiley-
Interscience, London, 1969)
</p>
<p>71. D.P. Derrarr et al., in A Practical Approach to Microarray Data Analysis (Kluwer Academic,
Norwell, 2003), p. 91
</p>
<p>72. DGESVD routine from the freely available software package LAPACK, http://www.netlib.
org/lapack
</p>
<p>73. L. Diosi, A Short Course in Quantum Information Theory (Springer, Berlin, 2007)
74. P. Duhamel, M. Vetterli, Signal Process. 19, 259 (1990)
75. S.E. Economou, T.L. Reinecke, in Optical Generation and Control of Quantum Coherence in
</p>
<p>Semiconductor Nanostructures, ed. by G. Slavcheva, P. Roussignol (Springer, Berlin, 2010),
p. 62
</p>
<p>76. A. Einstein, Ann. Phys. 17, 549 (1905)
77. A. Einstein, Investigations on the Theory of Brownian Movement (Dover, New York, 1956)
78. J.R. Errington, P.G. Debenedetti, S. Torquato, J. Chem. Phys. 118, 2256 (2003)
79. R. Eymard, T. Galloue, R. Herbin, Finite volume methods, in Handbook of Numerical Anal-
</p>
<p>ysis, vol. 7, ed. by P.G. Ciarlet, J.L. Lions (Elsevier, Amsterdam, 2000), pp. 713&ndash;1020
80. M.D. Feit, J.A. Fleck Jr., A. Steiger, J. Comp. Physiol. 47, 412 (1982)
81. R.P. Feynman, F.L. Vernon, R.W. Hellwarth, J. Appl. Phys. 28, 49 (1957)
82. A. Fick, Philos. Mag. 10, 30 (1855)
83. P.C. Fife, Mathematical Aspects of Reacting and Diffusing Systems (Springer, Berlin, 1979)
84. J. Fish, T. Belytschko, A First Course in Finite Elements (Wiley, New York, 2007)
85. G.S. Fishman, Monte Carlo: Concepts, Algorithms, and Applications (Springer, New York,
</p>
<p>1995). ISBN 038794527X
86. R. Fletcher, Comput. J. 13, 317 (1970)
87. C.A.J. Fletcher, Computational Galerkin Methods (Springer, Berlin, 1984)
88. C.A.J. Fletcher, Computational Techniques for Fluid Dynamics, vol. I, 2nd edn. (Springer,
</p>
<p>Berlin, 1991)
89. R. Fletcher, C. Reeves, Comput. J. 7, 149 (1964)
90. M.S. Floater, K. Hormann, Numer. Math. 107, 315 (2007)
91. F. Fogolari, A. Brigo, H. Molinari, J. Mol. Recognit. 15, 377 (2002)
92. J.A. Ford, Improved Algorithms of Illinois-Type for the Numerical Solution of Nonlinear
</p>
<p>Equations (University of Essex Press, Essex, 1995), Technical Report CSM-257
93. B. Fornberg, Geophysics 52, 4 (1987)
94. B. Fornberg, Math. Comput. 51, 699 (1988)
95. J. Fourier, The Analytical Theory of Heat (Cambridge University Press, Cambridge, 1878),
</p>
<p>reissued by Cambridge University Press, 2009. ISBN 978-1-108-00178-6
96. M. Fox, Quantum Optics: An Introduction (Oxford University Press, London, 2006)
97. D. Frenkel, B. Smit, Understanding Molecular Simulation: From Algorithms to Applications
</p>
<p>(Academic Press, San Diego, 2002). ISBN 0-12-267351-4
98. B.G. Galerkin, On electrical circuits for the approximate solution of the Laplace equation.
</p>
<p>Vestnik Inzh. 19, 897&ndash;908 (1915)
99. A.E. Garcia, Phys. Rev. Lett. 86, 2696 (1992)
</p>
<p>100. C.W. Gear, Math. Comput. 21, 146 (1967)
101. C.W. Gear, Numerical Initial Value Problems in Ordinary Differential Equations (Prentice
</p>
<p>Hall, Englewood Cliffs, 1971)
102. C.W. Gear, IEEE Trans. Circuit Theory 18, 89&ndash;95 (1971)
103. G. Goertzel, Am. Math. Mon. 65, 34 (1958)
104. A. Goldberg, H.M. Schey, J.L. Schwartz, Am. J. Phys. 35, 177 (1967)
105. D. Goldfarb, Math. Comput. 24, 23 (1970)
106. H. Goldstein, Klassische Mechanik (Akademische Verlagsgesellschaft, Frankfurt am Main,
</p>
<p>1974)</p>
<p/>
<div class="annotation"><a href="http://www.netlib.org/lapack">http://www.netlib.org/lapack</a></div>
<div class="annotation"><a href="http://www.netlib.org/lapack">http://www.netlib.org/lapack</a></div>
</div>
<div class="page"><p/>
<p>444 References
</p>
<p>107. G. Golub, W. Kahan, J. Soc. Ind. Appl. Math., Ser. B Numer. Anal. 2, 205 (1965)
108. G.H. Golub, C.F. Van Loan, Matrix Computations, 3rd edn. (Johns Hopkins University Press,
</p>
<p>Baltimore, 1976). ISBN 978-0-8018-5414-9
109. G.H. Golub, J.H. Welsch, Math. Comput. 23, 221&ndash;230 (1969)
110. G.L. Gouy, J. Phys. 9, 457 (1910)
111. W.B. Gragg, SIAM J. Numer. Anal. 2, 384 (1965)
112. P. Grindrod, Patterns and Waves: The Theory and Applications of Reaction-Diffusion Equa-
</p>
<p>tions (Clarendon Press, Oxford, 1991)
113. R. Guantes, S.C. Farantos, J. Chem. Phys. 111, 10827 (1999)
114. R. Guantes, A. Nezis, S.C. Farantos, J. Chem. Phys. 111, 10836 (1999)
115. J.M. Haile, Molecular Dynamics Simulation: Elementary Methods (Wiley, New York, 2001).
</p>
<p>ISBN 0-471-18439-X
116. E. Hairer, C. Lubich, G. Wanner, Acta Numer. 12, 399 (2003)
117. J. Halpern, L.E. Orgel, Discuss. Faraday Soc. 29, 32 (1960)
118. J.-P. Hansen, L. Verlet, Phys. Rev. 184, 151 (1969)
119. F.j. Harris, Proc. IEEE 66, 51 (1978)
120. M.T. Heath, Multigrid methods, in Scientific Computing: An Introductory Survey (McGraw-
</p>
<p>Hill, New York, 2002), p. 478 ff. &sect;11.5.7, Higher Education
121. M.R. Hestenes, E. Stiefel, J. Res. Natl. Bur. Stand. 49, 435 (1952)
122. D. Hilbert, L. Nordheim, J. von Neumann, Math. Ann. 98, 1 (1927)
123. D. Hinrichsen, A.J. Pritchard, Mathematical Systems Theory I&mdash;Modelling, State Space Anal-
</p>
<p>ysis, Stability and Robustness (Springer, Berlin, 2005). ISBN 0-978-3-540-441250
124. C.S. Holling, Can. Entomol. 91, 293 (1959)
125. C.S. Holling, Can. Entomol. 91, 385 (1959)
126. F.-Y. Hong, S.-J. Xiong, Chin. J. Phys. 46, 379 (2008)
127. B.K.P. Horn, H.M. Hilden, S. Negahdaripour, J. Opt. Soc. Am. A 5, 1127 (1988)
128. P.H. Huenenberger, Adv. Polym. Sci. 173, 105 (2005)
129. H. Ibach, H. L&uuml;th, Solid-State Physics: An Introduction to Principles of Materials Science.
</p>
<p>Advanced Texts in Physics (Springer, Berlin, 2003)
130. IEEE 754-2008, Standard for Floating-Point Arithmetic, IEEE Standards Association, 2008
131. T. Iitaka, Phys. Rev. E 49, 4684 (1994)
132. T. Iitaka, N. Carjan, T. Strottman, Comput. Phys. Commun. 90, 251 (1995)
133. R.Z. Iqbal, Master thesis, School of Mathematics, University of Birmingham, 2008
134. E. Ising, Beitrag zur Theorie des Ferromagnetismus. Z. Phys. 31, 253&ndash;258 (1925).
</p>
<p>doi:10.1007/BF02980577
135. V.I. Istratescu, Fixed Point Theory: An Introduction (Reidel, Dordrecht 1981). ISBN 90-277-
</p>
<p>1224-7
136. IUPAC-IUB Commission on Biochemical Nomenclature, Biochemistry 9, 3471 (1970)
137. H. Jeffreys, B.S. Jeffreys, Lagrange&rsquo;s interpolation formula, in Methods of Mathematical
</p>
<p>Physics, 3rd edn. (Cambridge University Press, Cambridge, 1988), p. 260, &sect; 9.011
138. H. Jeffreys, B.S. Jeffreys, Divided differences, in Methods of Mathematical Physics, 3rd edn.
</p>
<p>(Cambridge University Press, Cambridge, 1988), pp. 260&ndash;264, &sect; 9.012
139. H. Jeffreys, B.S. Jeffreys, Methods of Mathematical Physics, 3rd edn. (Cambridge University
</p>
<p>Press, Cambridge, 1988), pp. 305&ndash;306
140. B.-n. Jiang, The Least-Squares Finite Element Method (Springer, Berlin, 1998)
141. F. John, Duke Math. J. 4, 300 (1938)
142. J.K. Johnson, J.A. Zollweg, K.E. Gubbins, Mol. Phys. 78, 591 (1993)
143. A.H. Juffer et al., J. Phys. Chem. B 101, 7664 (1997)
144. E.I. Jury, Theory and Application of the Z-Transform Method (Krieger, Melbourne, 1973).
</p>
<p>ISBN 0-88275-122-0
145. K. Khalil Hassan, Nonlinear Systems (Prentice Hall, Englewood Cliffs, 2001). ISBN 0-13-
</p>
<p>067389-7
146. S.A. Khrapak, M. Chaudhuri, G.E. Morfill, Phys. Rev. B 82, 052101 (2010)
147. J. Kiefer, Proc. Am. Math. Soc. 4, 502&ndash;506 (1953)</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/BF02980577">http://dx.doi.org/10.1007/BF02980577</a></div>
</div>
<div class="page"><p/>
<p>References 445
</p>
<p>148. J.G. Kirkwood, J. Chem. Phys. 2, 351 (1934)
149. S.E. Koonin, C.M. Dawn, Computational Physics (Perseus Books, New York, 1990). ISBN
</p>
<p>978-0201127799
150. D. Kosloff, R. Kosloff, J. Comp. Physiol. 52, 35 (1983)
151. S. Kouachi, Electron. J. Linear Algebra 15, 115 (2006)
152. T. K&uuml;hne, M. Krack, F. Mohamed, M. Parrinello, Phys. Rev. Lett. 98, 066401 (2007)
153. C. Lanczos, J. Res. Natl. Bur. Stand. 45, 255 (1951)
154. L. Landau, Phys. Sov. Union 2, 46&ndash;51 (1932)
155. H.P. Langtangen, Computational Partial Differential Equations: Numerical Methods and
</p>
<p>Diffpack Programming (Springer, Berlin, 2003)
156. C. Lavor, Physica D 227, 135 (2007)
157. A. Leach, Molecular Modelling: Principles and Applications, 2nd edn. (Prentice Hall, Har-
</p>
<p>low, 2001). ISBN 978-0582382107
158. C. Leforestier, R.H. Bisseling, C. Cerjan, M.D. Feit, R. Friesner, A. Guldberg, A. Ham-
</p>
<p>merich, G. Jolicard, W. Karrlein, H.-D. Meyer, N. Lipkin, O. Roncero, R. Kosloff, J. Comp.
Physiol. 94, 59 (1991)
</p>
<p>159. D. Levesque, L. Verlet, Phys. Rev. A 2, 2514 (1970)
160. A.J. Lotka, Elements of Physical Biology (Williams and Wilkins, Baltimore, 1925)
161. A.M. Lyapunov, Stability of Motion (Academic Press, New York, 1966)
162. L. Lynch, Numerical integration of linear and nonlinear wave equations. Bachelor thesis,
</p>
<p>Florida Atlantic University, Digital Commons@University of Nebraska-Lincoln, 2004
163. A.D. MacKerell Jr., B. Brooks, C.L. Brooks III, L. Nilsson, B. Roux, Y. Won, M. Karplus,
</p>
<p>CHARMM: The energy function and its parameterization with an overview of the program,
in The Encyclopedia of Computational Chemistry, vol. 1, ed. by P.v.R. Schleyer et al. (Wiley,
Chichester, 1998), pp. 271&ndash; 277
</p>
<p>164. W. Magnus, Commun. Appl. Math. 7, 649 (1954)
165. J. Mandel, Am. Stat. 36, 15 (1982)
166. A.A. Markov, Theory of Algorithms [Translated by Jacques J. Schorr-Kon and PST staff]
</p>
<p>Imprint Moscow, Academy of Sciences of the USSR, 1954 [Jerusalem, Israel Program for
Scientific Translations, 1961; available from Office of Technical Services, United States
Department of Commerce] Added t.p. in Russian Translation of Works of the Mathemati-
cal Institute, Academy of Sciences of the USSR, v. 42. Original title: Teoriya algorifmov.
[QA248.M2943 Dartmouth College library. U.S. Dept. of Commerce, Office of Technical
Services, number OTS 60-51085] (1954)
</p>
<p>167. A.A. Markov, Extension of the limit theorems of probability theory to a sum of variables
connected in a chain, in Dynamic Probabilistic Systems, vol. 1: Markov Chains (Wiley, New
York, 1971), reprinted in Appendix B of Howard, R.
</p>
<p>168. G. Marsaglia, A. Zaman, Ann. Appl. Probab. 1, 462 (1991)
169. G.J. Martyna, M.E. Tuckerman, J. Chem. Phys. 102, 8071 (1995)
170. W.L. Mattice, U.W. Suter, Conformational Theory of Large Molecules (Wiley-Interscience,
</p>
<p>New York, 1994). ISBN 0-471-84338-5
171. B.M. McCoy, T.T. Wu, The Two-Dimensional Ising Model (Harvard University Press, Cam-
</p>
<p>bridge, 1973). ISBN 0674914406
172. E.A. McCullough Jr., R.E. Wyatt, J. Chem. Phys. 51, 1253 (1969)
173. E.A. McCullough Jr., R.E. Wyatt, J. Chem. Phys. 54, 3592 (1971)
174. N. Metropolis, S. Ulam, J. Am. Stat. Assoc. 44, 335 (1949)
175. N. Metropolis, A. Rosenbluth, M. Rosenbluth, A. Teller, E. Teller, J. Chem. Phys. 21, 1087
</p>
<p>(1953)
176. C. Muguruma, Y. Okamoto, M. Mikami, Croat. Chem. Acta 80, 203 (2007)
177. D.E. Muller, Math. Tables Other Aids Comput. 10, 208 (1956)
178. J.D. Murray, Mathematical Biology: I. An Introduction, vol. 2, 3rd edn. (Springer, Berlin,
</p>
<p>2002). ISBN 0-387-95223-3
179. S.J. Nettel, A. Kempicki, Am. J. Phys. 47, 987 (1979)
180. E.H. Neville, J. Indian Math. Soc. 20, 87 (1933)</p>
<p/>
</div>
<div class="page"><p/>
<p>446 References
</p>
<p>181. A. Nicholls, B. Honig, J. Comput. Chem. 12, 435 (1990)
182. J.J. Nicolas, K.E. Gubbins, W.B. Streett, D.J. Tildesley, Mol. Phys. 37, 1429 (1979)
183. M. Nielsen, I. Chuang, Quantum Computation and Quantum Information (Cambridge Uni-
</p>
<p>versity Press, Cambridge, 2000)
184. A. Nordsieck, Math. Comput. 16, 22 (1962)
185. M. Novelinkova, in: WDS&rsquo;11, Proceedings of Contributed Papers, Part I (2011), p. 67
186. G. N&uuml;rnberger, Approximation by Spline Functions (Springer, Berlin, 1989). ISBN 3-540-
</p>
<p>51618-2
187. H.J. Nussbaumer, Fast Fourier Transform and Convolution Algorithms (Springer, Berlin,
</p>
<p>1990)
188. M. Oevermann, R. Klein, J. Comp. Physiol. 219, 749 (2006)
189. M. Oevermann, C. Scharfenberg, R. Klein, J. Comput. Phys. 228(14), 5184&ndash;5206 (2009).
</p>
<p>doi:10.1016/j.jcp.2009.04.018
190. I.P. Omelyan, Phys. Rev. 58, 1169 (1998)
191. I.P. Omelyan, Comput. Phys. Commun. 109, 171 (1998)
192. I.P. Omelyan, Comput. Phys. 12, 97 (1998)
193. I.P. Omelyan, I.M. Mryglod, R. Folk, Comput. Phys. Commun. 151, 272 (2003)
194. L. Onsager, Phys. Rev. 65, 117 (1944)
195. E.L. Ortiz, SIAM J. Numer. Anal. 6, 480 (1969)
196. J.M. Papakonstantinou, A historical development of the (n+ 1)-point secant method. M.A.
</p>
<p>thesis, Rice University Electronic Theses and Dissertations, 2007
197. T.J. Park, J.C. Light, J. Chem. Phys. 85, 5870 (1986)
198. B.N. Parlett, The Symmetric Eigenvalue Problem (Society for Industrial and Applied Mathe-
</p>
<p>matics, Philadelphia, 1998)
199. K. Pearson, The problem of the random walk. Nature 72, 294 (1905)
200. J. Peiro, S. Sherwin, in Handbook of Materials Modeling, vol. 1, ed. by S. Yip (Springer,
</p>
<p>Berlin, 2005), pp. 1&ndash;32
201. D. Pines, C.P. Slichter, Phys. Rev. 100, 1014 (1955)
202. H. Pollard, Q. J. Pure Appl. Math. 49, 1 (1920)
203. W.H. Press, S.A. Teukolsky, W.T. Vetterling, B.P. Flannery, LU decomposition and its appli-
</p>
<p>cations, in Numerical Recipes: The Art of Scientific Computing, 3rd edn. (Cambridge Uni-
versity Press, Cambridge, 2007), pp. 48&ndash;55
</p>
<p>204. W.H. Press, S.A. Teukolsky, W.T. Vetterling, B.P. Flannery, Cholesky decomposition, in Nu-
merical Recipes: The Art of Scientific Computing, 3rd edn. (Cambridge University Press,
Cambridge, 2007), pp. 100&ndash;101
</p>
<p>205. W.H. Press, S.A. Teukolsky, W.T. Vetterling, B.P. Flannery, Cyclic tridiagonal systems, in
Numerical Recipes: The Art of Scientific Computing, 3rd edn. (Cambridge University Press,
Cambridge, 2007), p. 79
</p>
<p>206. W.H. Press, S.A. Teukolsky, W.T. Vetterling, B.P. Flannery, Relaxation mehods for boundary
value problems, in Numerical Recipes: The Art of Scientific Computing, 3rd edn. (Cambridge
University Press, Cambridge, 2007), pp. 1059&ndash;1065
</p>
<p>207. W.H. Press, S.A. Teukolsky, W.T. Vetterling, B.P. Flannery, Golden section search in one
dimension, in Numerical Recipes: The Art of Scientific Computing, 3rd edn. (Cambridge
University Press, New York, 2007). ISBN 978-0-521-88068-8, Sect. 10.2
</p>
<p>208. W.H. Press, S.A. Teukolsky, W.T. Vetterling, B.P. Flannery, Second-order conservative equa-
tions, in Numerical Recipes: The Art of Scientific Computing, 3rd edn. (Cambridge University
Press, Cambridge, 2007), Sect. 17.4
</p>
<p>209. A. Quarteroni, R. Sacco, F. Saleri, Numerical Mathematics, 2nd edn. (Springer, Berlin, 2007)
210. S.S. Rao, The Finite Element Method in Engineering (Elsevier, Amsterdam, 2011)
211. K.R. Rao, P. Yip, Discrete Cosine Transform: Algorithms, Advantages, Applications (Aca-
</p>
<p>demic Press, Boston, 1990)
212. D.C. Rapaport, The Art of Molecular Dynamics Simulation (Cambridge University Press,
</p>
<p>Cambridge, 2004). ISBN 0-521-44561-2</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1016/j.jcp.2009.04.018">http://dx.doi.org/10.1016/j.jcp.2009.04.018</a></div>
</div>
<div class="page"><p/>
<p>References 447
</p>
<p>213. A.K. Rappe, C.J. Casewit, K.S. Colwell, W.A. Goddar, W.M. Skiff, J. Am. Chem. Soc. 115,
10024 (1992)
</p>
<p>214. E. Renshaw, Modelling Biological Populations in Space and Time (Cambridge University
Press, Cambridge, 1991). ISBN 0-521-44855-7
</p>
<p>215. J. Rice, Mathematical Statistics and Data Analysis, 2nd edn. (Duxbury Press, N. Scituate,
1995). ISBN 0-534-20934-3
</p>
<p>216. J.A. Richards, Remote Sensing Digital Image Analysis (Springer, Berlin, 1993)
217. L.F. Richardson, Philos. Trans. R. Soc. Lond. Ser. A 210, 307&ndash;357 (1911)
218. R.D. Richtmyer, Principles of Modern Mathematical Physics I (Springer, New York, 1978)
219. H. Risken, The Fokker-Planck Equation (Springer, Berlin, 1989)
220. C.P. Robert, G. Casella, Monte Carlo Statistical Methods, 2nd edn. (Springer, New York,
</p>
<p>2004). ISBN 0387212396
221. T.D. Romo et al., Proteins 22, 311 (1995)
222. I.R. Savage, E. Lukacs, in Contributions to the Solution of Systems of Linear Equations and
</p>
<p>the Determination of Eigenvalues, ed. by O. Taussky. National Bureau of Standards, Applied
Mathematics Series, vol. 39 (1954)
</p>
<p>223. G. Schaftenaar, J.H. Noordik, J. Comput.-Aided Mol. Des. 14, 123 (2000)
224. T. Schlick, J. Comput. Chem. 10, 951 (1989)
225. T. Schlick, Molecular Modeling and Simulation (Springer, Berlin, 2002). ISBN 0-387-
</p>
<p>95404-X
226. B.I. Schneider, J.-A. Collins, J. Non-Cryst. Solids 351, 1551 (2005)
227. C. Schneider, W. Werner, Math. Comput. 47, 285 (1986)
228. I.J. Schoenberg, Q. Appl. Math. 4, 45&ndash;99 and 112&ndash;141 (1946)
229. P. Schofield, Comput. Phys. Commun. 5, 17 (1973)
230. E. Schr&ouml;dinger, Phys. Rev. 28, 1049 (1926)
231. F. Schwabl, Statistical Mechanics (Springer, Berlin, 2003)
232. F. Schwabl, Quantum Mechanics, 4th edn. (Springer, Berlin, 2007)
233. L.F. Shampine, IMA J. Numer. Anal. 3, 383 (1983)
234. L.F. Shampine, L.S. Baca, Numer. Math. 41, 165 (1983)
235. D.F. Shanno, Math. Comput. 24, 647 (1970)
236. J. Sherman, W.J. Morrison, Ann. Math. Stat. 20, 621 (1949)
237. T. Simonson, Rep. Prog. Phys. 66, 737 (2003)
238. G. Skollermo, Math. Comput. 29, 697 (1975)
239. B. Smit, J. Chem. Phys. 96, 8639 (1992)
240. A. Sommariva, Comput. Math. Appl. 65(4), 682&ndash;693 (2013). doi:10.1016/j.camwa.2012.12.
</p>
<p>004 [MATLAB CODES (zip file)]
241. R. Sonnenschein, A. Laaksonen, E. Clementi, J. Comput. Chem. 7, 645 (1986)
242. B.I. Stepanov, V.P. Grobkovskii, Theory of Luminescence (Butterworth, London, 1986)
243. G.W. Stewart, SIAM Rev. 35, 551 (1993)
244. J. St&ouml;r, R. Bulirsch, Introduction to Numerical Analysis, 3rd revised edn. (Springer, New
</p>
<p>York, 2010). ISBN 978-1441930064
245. S.H. Strogatz, Nonlinear Dynamics and Chaos: Applications to Physics, Biology, Chemistry,
</p>
<p>and Engineering (Perseus Books, Reading, 2001). ISBN 0-7382-0453-6
246. M. Suzuki, Commun. Math. Phys. 51, 183 (1976)
247. P.N. Swarztrauber, R.A. Sweet, in Handbook of Fluid Dynamics and Fluid Machinery, ed.
</p>
<p>by J.A. Schetz, A.E. Fuhs (Wiley, New York, 1996)
248. H. Tal-Ezer, R. Kosloff, J. Chem. Phys. 81, 3967 (1984)
249. J.T. Tanner, Ecology 56, 855 (1975)
250. S.A. Teukolsky, Phys. Rev. D 61(8), 087501 (2000). doi:10.1103/PhysRevD.61.087501
251. J.W. Thomas, Numerical Partial Differential Equations: Finite Difference Methods. Texts in
</p>
<p>Applied Mathematics, vol. 22 (Springer, Berlin, 1995)
252. J.Y. Tjalling, Historical development of the Newton-Raphson method. SIAM Rev. 37, 531
</p>
<p>(1995)
253. L.N. Trefethen, SIAM Rev. 50, 67&ndash;87 (2008)</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1016/j.camwa.2012.12.004">http://dx.doi.org/10.1016/j.camwa.2012.12.004</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1016/j.camwa.2012.12.004">http://dx.doi.org/10.1016/j.camwa.2012.12.004</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1103/PhysRevD.61.087501">http://dx.doi.org/10.1103/PhysRevD.61.087501</a></div>
</div>
<div class="page"><p/>
<p>448 References
</p>
<p>254. L.N. Trefethen, D. Bau III, Numerical Linear Algebra (Society for Industrial and Applied
Mathematics, Philadelphia, 1997), p. 1
</p>
<p>255. S.-H. Tsai et al., Braz. J. Phys. 34, 384 (2004)
256. T. Tsang, H. Tang, Phys. Rev. A 15, 1696 (1977)
257. M. Tuckerman, B.J. Berne, J. Chem. Phys. 97, 1990 (1992)
258. A.M. Turing, Philos. Trans. R. Soc. Lond. B 237, 37 (1952)
259. R.E. Tuzun, D.W. Noid, B.G. Sumpter, Macromol. Theory Simul. 5, 771 (1996)
260. R.E. Tuzun, D.W. Noid, B.G. Sumpter, J. Comput. Chem. 18, 1804 (1997)
261. W. van Dijk, F.M. Toyama, Phys. Rev. E 75, 036707 (2007)
262. W.F. van Gunsteren, S.R. Billeter, A.A. Eising, P.H. H&uuml;nenberger, P. Kr&uuml;ger, A.E. Mark,
</p>
<p>W.R.P. Scott, I.G. Tironi, Biomolecular Simulation: The GROMOS96 Manual and User
Guide (vdf Hochschulverlag AG an der ETH Z&uuml;rich and BIOMOS b.v., Z&uuml;rich, Groningen,
1996)
</p>
<p>263. L.M.K. Vandersypen, I.L. Chuang, Rev. Mod. Phys. 76, 1037 (2004)
264. P.F. Verhulst, Mem. Acad. R. Sci. Belles Lettres Bruxelles 18, 1&ndash;42 (1845)
265. L. Verlet, Phys. Rev. 159, 98 (1967)
266. V. Volterra, Mem. R. Accad. Naz. Lincei 2, 31 (1926)
267. J. Waldvogel, BIT Numer. Math. 46, 195 (2006)
268. Z. Wang, B.R. Hunt, Appl. Math. Comput. 16, 19 (1985)
269. R.K. Wangsness, F. Bloch, Phys. Rev. 89, 728 (1953)
270. H. Watanabe, N. Ito, C.K. Hu, J. Chem. Phys. 136, 204102 (2012)
271. W. Werner, Math. Comput. 43, 205 (1984)
272. H. Wilf, Mathematics for the Physical Sciences (Wiley, New York, 1962)
273. J.H. Wilkinson, J. ACM 8, 281 (1961)
274. C.P. Williams, Explorations in Quantum Computing (Springer, Berlin, 2011)
275. J. Wolberg, Data Analysis Using the Method of Least Squares: Extracting the Most Informa-
</p>
<p>tion from Experiments (Springer, Berlin, 2005)
276. L.C. Wrobel, M.H. Aliabadi, The Boundary Element Method (Wiley, New York, 2002)
277. G. Wunsch, Feldtheorie (VEB Technik, Berlin, 1973)
278. K. Yabana, G.F. Bertsch, Phys. Rev. B 54, 4484 (1996)
279. S. Yang, M.K. Gobbert, Appl. Math. Lett. 22, 325 (2009)
280. A. Yariv, Quantum Electronics (Wiley, New York, 1975)
281. W.C. Yueh, Appl. Math. E-Notes 5, 66 (2005)
282. C. Zener, Proc. R. Soc. Lond. A 137(6), 696&ndash;702 (1932)
283. Y. Zhu, A.C. Cangellaris, Multigrid Finite Element Methods for Electromagnetic Field Mod-
</p>
<p>eling (Wiley, New York, 2006), p. 132 ff. ISBN 0471741108
284. I. Zutic, J. Fabian, S. Das Sarma, Rev. Mod. Phys. 76, 323 (2004)</p>
<p/>
</div>
<div class="page"><p/>
<p>Index
</p>
<p>A
</p>
<p>Adams-Bashforth, 222, 234
Adams-Moulton, 223
Amplification factor, 345
Angular momentum, 246&ndash;248, 254
Angular velocity, 242&ndash;244
Attractive fixed point, 365
Auto-correlation, 302
Average extension, 301
Average of measurements, 134
</p>
<p>B
</p>
<p>Backward difference, 37
Backward differentiation, 223
Backward substitution, 61
Ballistic motion, 285
Beeman, 230
BFGS, 110
Bicubic spline interpolation, 35
Bifurcation, 368
Bifurcation diagram, 369
Bilinear interpolation, 32, 35
Binomial distribution, 133
Bio-molecules, 315
Biopolymer, 296
Birth rate, 372
Bisection, 84
Bloch equations, 420, 421
Bloch vector, 417
Bond angle, 265
Bond length, 264
Boundary conditions, 178
Boundary element, 318, 324, 327
Boundary element method, 204
Boundary potential, 321
Boundary value problems, 178
Box Muller, 138
</p>
<p>Brent, 92
Brownian motion, 285, 293, 301, 303
Broyden, 99
</p>
<p>C
</p>
<p>Calculation of π , 138
Carrying capacity, 367, 375
Cartesian coordinates, 264
Cavity, 318, 322, 325, 326
Cayley-Klein, 256, 257
Central difference quotient, 38
Central limit theorem, 133, 144, 294, 297
Chain, 296
Chandrupatla, 95
Chaotic behavior, 369
Characteristic polynomial, 151
Charged sphere, 309, 314, 317
Chebyshev, 51
Chemical reactions, 378
Chessboard method, 308
Circular orbit, 211, 232
Clenshaw-Curtis, 50
Coin, 133
Collisions, 285, 302, 415
Composite midpoint rule, 48
Composite Newton-Cotes formulas, 48
Composite Simpson&rsquo;s rule, 48
Composite trapezoidal rule, 48
Computer experiments, 433
Concentration, 351
Condition number, 77
Configuration integral, 141
Conjugate gradients, 76, 107
Conservation laws, 179
Continuous logistic model, 371
Control parameter, 369
Control volumes, 186
</p>
<p>P.O.J. Scherer, Computational Physics, Graduate Texts in Physics,
DOI 10.1007/978-3-319-00401-3,
&copy; Springer International Publishing Switzerland 2013
</p>
<p>449</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-00401-3">http://dx.doi.org/10.1007/978-3-319-00401-3</a></div>
</div>
<div class="page"><p/>
<p>450 Index
</p>
<p>Coordinate system, 239
Correlation coefficient, 132
Coulomb interaction, 269
Courant number, 337
Covariance matrix, 132
Crank-Nicolson, 347, 357, 396
Critical temperature, 289
Crossing point, 414
Cubic spline, 22, 34
Cumulative probability distribution, 127
Cyclic tridiagonal, 71, 154
</p>
<p>D
</p>
<p>Damped string, 349
Damping, 285, 341, 431
Data fitting, 161
Debye length, 317
Dekker, 91
Density matrix, 208, 386, 416
Density of states, 414
Detailed balance, 143
Determinant, 250
Dielectric medium, 306, 314
Differential equations, 177
Differentiation, 37
Differentiation matrix, 151
Diffusion equation, 362
Diffusive motion, 285
Diffusive population dynamics, 379
Dihedral angle, 265
Direction set, 106
Discontinuity, 320
Discontinuous ε, 313
Discrete Fourier transformation, 114, 125, 193
Discretization, 177
Disorder, 160
Dispersion, 332, 336, 337
Divided differences, 18
Dual grid, 186
</p>
<p>E
</p>
<p>Effective coupling, 413
Effective force constant, 301
Eigenvalue, 147
Eigenvector expansion, 183, 334
Electric field, 260
Electrolyte, 315
Electrostatics, 305
Elliptical differential equation, 179
Elongation, 339
End to end distance, 297
Energy function, 141, 145
Ensemble average, 387
Equations of motion, 207
</p>
<p>Equilibria, 370
Error accumulation, 229
Error analysis, 3
Error function, 131
Error of addition, 8
Error of multiplication, 9
Error propagation, 9
Euler angles, 255
Euler parameters, 257
Euler-McLaurin expansion, 49
Euler&rsquo;s equations, 250, 254
Expectation value, 129
Explicit Euler method, 210, 212, 248, 250,
</p>
<p>353, 393
Exponent overflow, 5
Exponent underflow, 5
Exponential decay, 412, 414, 431
Exponential distribution, 137
Extrapolation, 39, 49, 221
</p>
<p>F
</p>
<p>Fair die, 130, 136
Fast Fourier transformation, 121
Few-state systems, 403
Filter function, 120
Finite differences, 37, 180
Finite elements, 196
Finite volumes, 185
Fixed point, 364
Fixed point equation, 368
Fletcher-Rieves, 107
Floating point numbers, 3
Floating point operations, 6
Fluctuating force, 302
Flux, 351, 188
Force, 301, 303
Force extension relation, 304
Force field, 263, 266
Forward difference, 37
Fourier transformation, 113, 336
Free energy, 301
Free precession, 422
Free rotor, 254
Freely jointed chain, 296, 300
Friction coefficient, 302
Friction force, 302
Frobenius matrix, 60
FTCS, 181
Functional response, 373
</p>
<p>G
</p>
<p>Galerkin, 192, 201
Gauss-Legendre, 53
Gauss-Seidel, 74, 307</p>
<p/>
</div>
<div class="page"><p/>
<p>Index 451
</p>
<p>Gaussian distribution, 131, 138, 295
Gaussian elimination, 60
Gaussian integral rules, 54
Gaussian integration, 52
Gauss&rsquo;s theorem, 205, 314, 319
Gear, 217, 223
Givens, 66
Global truncation error, 13
Glycine dipeptide, 266
Goertzel, 120
Golden section search, 101
Gradient vector, 106
Gradients, 270
Gram-Schmidt, 64
Green&rsquo;s theorem, 324
Grid, 208
Gyration radius, 299
Gyration tensor, 299, 303
</p>
<p>H
</p>
<p>Hadamard gate, 430
Hamilton operator, 405
Harmonic approximation, 274
Harmonic potential, 303
Hessian, 106, 107, 276
Heun, 214, 218
Higher derivatives, 41
Hilbert matrix, 80
Hilbert space, 387
Histogram, 128
Holling, 373
Holling-Tanner model, 375
Hookean spring, 300, 301, 304
Householder, 66, 157
Hyperbolic differential equation, 179
</p>
<p>I
</p>
<p>Implicit Euler method, 212
Implicit method, 356
Importance sampling, 142
Improved Euler method, 213, 303
Inertia, 247
Inevitable error, 11
Inhomogeneity, 378
Initial value problem, 178
Integers, 14
Integral equations, 318
Integral form, 180
Integration, 45
Interacting states, 405
Interaction energy, 309, 325
Intermediate state, 410
Intermolecular interactions, 269
Internal coordinates, 264
</p>
<p>Interpolating function, 15, 117
Interpolating polynomial, 17, 19, 20, 42
Interpolation, 15, 87
Interpolation error, 21
Intramolecular forces, 267
Inverse interpolation, 88
Ising model, 287, 289, 290
Iterated functions, 364
Iterative algorithms, 11
Iterative method, 307
Iterative solution, 73
</p>
<p>J
</p>
<p>Jacobi, 73, 148, 307
Jacobi determinant, 212
Jacobian, 97
</p>
<p>K
</p>
<p>Kinetic energy, 255, 390
</p>
<p>L
</p>
<p>Ladder model, 414, 431
Lagrange, 17, 42, 46
Lanczos, 159
Landau-Zener model, 414, 431
Langevin dynamics, 301
Laplace operator, 43, 360
Larmor-frequency, 422
Laser field, 408
Lax-Wendroff method, 345
Leapfrog, 231, 341&ndash;343
Least square, 192
Least square fit, 162, 175
Legendre polynomials, 53
Lennard-Jones, 269, 279, 280
Lennard-Jones system, 290
Linear approximation, 171
Linear equations, 59
Linear fit function, 164
Linear least square fit, 163, 172
Linear regression, 164, 166
Liouville, 225, 389
Local truncation error, 13
Logistic map, 367
Lotka-Volterra model, 372, 380
Low rank matrix approximation, 170
Lower triangular matrix, 62
LU decomposition, 63, 70
Lyapunov exponent, 366, 369
</p>
<p>M
</p>
<p>Machine numbers, 3, 6
Machine precision, 14
Magnetization, 289, 420
Markov chain, 142</p>
<p/>
</div>
<div class="page"><p/>
<p>452 Index
</p>
<p>Marsaglia, 135
Matrix elements, 404
Matrix inversion, 77
Mean square displacement, 285
Mesh, 197
Method of lines, 183
Metropolis, 142, 287
Midpoint rule, 48, 213
Milne rule, 47
Minimization, 99
Mixed states, 386
Mobile charges, 315
Modified midpoint method, 221
Molecular collision, 261
Molecular dynamics, 263
Moments, 129
Moments of inertia, 247
Monochromatic excitation, 423
Monte Carlo, 127, 138, 287
Mortality rate, 372
Multigrid, 308
Multipole expansion, 325
Multistep, 222
Multivariate distribution, 132
Multivariate interpolation, 32
</p>
<p>N
</p>
<p>N -body system, 234
Neumann, 389
Neville, 20, 40
Newton, 18
Newton-Cotes, 46
Newton-Raphson, 85, 97, 107
NMR, 422
Nodes, 197
Noise filter, 125
Nonlinear optimization, 145
Nonlinear systems, 363
Nordsieck, 215
Normal distribution, 131, 133
Normal equations, 162, 163
Normal modes, 274
Nullclines, 376
Numerical errors, 6
Numerical extinction, 7, 38
Numerical integration, 139
</p>
<p>O
</p>
<p>Observables, 390
Occupation probability, 407, 411
Omelyan, 259
One-sided difference, 37
Onsager, 325
Open interval, 48
</p>
<p>Optimized sample points, 50
Orbit, 364
Orthogonality, 250
Orthogonalization, 64
Oscillating perturbation, 408
</p>
<p>P
</p>
<p>Pair distance distribution, 284
Parabolic differential equations, 179
Partition function, 141
Pattern formation, 378
Pauli matrices, 256, 419
Pauli-gates, 429
Period, 365
Period doubling, 369
Periodic orbit, 366
Phase angle, 426
Phase space, 208, 211, 225
Phase transition, 289
Pivoting, 63
Plane wave, 332, 337, 379
Point collocation method, 191
Poisson equation, 305, 318
Poisson-Boltzmann equation, 315
Polarization, 318
Polymer, 290
Polynomial, 17, 19, 20, 42, 148
Polynomial extrapolation, 221
Polynomial interpolation, 16, 33
Population, 367
Population dynamics, 370
Potential energy, 263
Predation, 372
Predator, 372
Predictor-corrector, 213, 215, 217, 224
Pressure, 281
Prey, 372
Principal axes, 247
Probability density, 127
Pseudo random numbers, 135
Pseudo-spectral, 391
Pseudo-spectral method, 193
Pseudoinverse, 174
Pure states, 386
</p>
<p>Q
</p>
<p>QL algorithm, 156
QR decomposition, 64
Quality control, 220
Quantum systems, 385
Quasi-Newton condition, 98, 109
Quasi-Newton methods, 98, 108
Quaternion, 256, 258, 259</p>
<p/>
</div>
<div class="page"><p/>
<p>Index 453
</p>
<p>Qubit, 428
Qubit manipulation, 428
</p>
<p>R
</p>
<p>Rabi oscillations, 408
Random motion, 302
Random numbers, 127, 135, 136
Random points, 137
Random walk, 293, 303
Rational approximation, 392
Reaction-diffusion systems, 378
Real space product formulae, 400
Rectangular elements, 199
Recurrence, 367
Reflecting walls, 281
Regula falsi method, 85
Relaxation, 420
Relaxation parameter, 308
Reproduction rate, 367
Residual, 308
Resonance curve, 431
Resonant pulse, 425
Rigid body, 246, 248
Romberg, 49, 50
Romberg integration, 56
Root finding, 83
Roots, 83
Rosenbrock, 108, 111
Rotation in the complex plane, 12
Rotation matrix, 240, 248
Rotational motion, 239
Rotor, 248
Rotor in a field, 260
Rounding errors, 3
Runge-Kutta, 217, 405
</p>
<p>S
</p>
<p>Sampling theorem, 117
Schr&ouml;dinger equation, 387, 388, 390, 430
Secant method, 86
Second order differencing, 396
Self energy, 325
Semi-discretized, 183
Sherman-Morrison formula, 71
Shifted grid, 314
Simple sampling, 141
Simpson&rsquo;s rule, 47, 219
Simulated annealing, 145
Singular values, 167, 168
Solvation, 313, 314, 318, 327
Solvation energy, 326
Solvent, 325
Specific heat, 175
Spectral methods, 193
</p>
<p>Spin, 287
Spin flip, 427
Spin vector, 419
Spline interpolation, 22
Split operator, 226, 360, 399
Stability analysis, 11, 182
Standard deviation, 130
Statistical operator, 388
Steepest descent, 106
Step size control, 220
St&ouml;rmer-Verlet method, 228
Sub-domain method, 191
Successive over-relaxation, 75
Superexchange, 410
Superposition, 386
Surface charge, 323, 325, 326
Surface element, 137, 322
Symmetric difference quotient, 38
</p>
<p>T
</p>
<p>Taylor series method, 215
Ternary search, 99
Tetrahedrons, 198
Thermal average, 388
Thermal equilibrium, 143
Thermodynamic averages, 141
Thermodynamic systems, 279
Three-state system, 431
Tight-binding model, 160
Time derivatives, 181
Time evolution, 209
Transmission function, 121
Trapezoidal rule, 47, 119
Trial step, 144
Triangulation, 197
Tridiagonal, 69, 150, 338, 345, 354, 395
Trigonometric interpolation, 116
Truncation error, 13
Two-state system, 210, 405, 407, 408, 416, 430
Two-step method, 338
Two-variable method, 343
</p>
<p>U
</p>
<p>Ultra-hyperbolic differential equation, 179
Unimodal, 99
Unitary transformation, 66
Update matrix, 98
Upper triangular matrix, 61
</p>
<p>V
</p>
<p>Van der Waals, 269
Variable ε, 311
Variance, 129
Vector model, 417
Verhulst, 367</p>
<p/>
</div>
<div class="page"><p/>
<p>454 Index
</p>
<p>Verlet, 225, 227, 228, 280
Vertex, 186, 198
Virial, 283
Virial coefficient, 283
</p>
<p>W
</p>
<p>W -matrix, 242
Wave, 329
Wave equation, 332
Wave function, 387, 389
Wave packet, 402, 430
</p>
<p>Weak form, 180
Weddle rule, 47
Weight function, 180
Weighted residuals, 190, 404
Windowing function, 119
</p>
<p>Z
</p>
<p>Z-matrix, 266
Z-transform, 120
Zamann, 135</p>
<p/>
</div>
<ul>	<li>Computational Physics</li>
<ul>	<li>Preface to the Second Edition</li>
	<li>Preface to the First Edition</li>
	<li>Contents</li>
</ul>
	<li>Part I: Numerical Methods</li>
<ul>	<li>Chapter 1: Error Analysis</li>
<ul>	<li>1.1 Machine Numbers and Rounding Errors</li>
	<li>1.2 Numerical Errors of Elementary Floating Point Operations</li>
<ul>	<li>1.2.1 Numerical Extinction</li>
	<li>1.2.2 Addition</li>
	<li>1.2.3 Multiplication</li>
</ul>
	<li>1.3 Error Propagation</li>
	<li>1.4 Stability of Iterative Algorithms</li>
	<li>1.5 Example: Rotation</li>
	<li>1.6 Truncation Error</li>
	<li>1.7 Problems</li>
</ul>
	<li>Chapter 2: Interpolation</li>
<ul>	<li>2.1 Interpolating Functions</li>
	<li>2.2 Polynomial Interpolation</li>
<ul>	<li>2.2.1 Lagrange Polynomials</li>
	<li>2.2.2 Barycentric Lagrange Interpolation</li>
	<li>2.2.3 Newton's Divided Differences</li>
	<li>2.2.4 Neville Method</li>
	<li>2.2.5 Error of Polynomial Interpolation</li>
</ul>
	<li>2.3 Spline Interpolation</li>
	<li>2.4 Rational Interpolation</li>
<ul>	<li>2.4.1 Pad&eacute; Approximant</li>
	<li>2.4.2 Barycentric Rational Interpolation</li>
<ul>	<li>2.4.2.1 Rational Interpolation of Order [M,N]</li>
	<li>2.4.2.2 Rational Interpolation Without Poles</li>
</ul>
</ul>
	<li>2.5 Multivariate Interpolation</li>
	<li>2.6 Problems</li>
</ul>
	<li>Chapter 3: Numerical Differentiation</li>
<ul>	<li>3.1 One-Sided Difference Quotient</li>
	<li>3.2 Central Difference Quotient</li>
	<li>3.3 Extrapolation Methods</li>
	<li>3.4 Higher Derivatives</li>
	<li>3.5 Partial Derivatives of Multivariate Functions</li>
	<li>3.6 Problems</li>
</ul>
	<li>Chapter 4: Numerical Integration</li>
<ul>	<li>4.1 Equidistant Sample Points</li>
<ul>	<li>4.1.1 Closed Newton-Cotes Formulae</li>
	<li>4.1.2 Open Newton-Cotes Formulae</li>
	<li>4.1.3 Composite Newton-Cotes Rules</li>
	<li>4.1.4 Extrapolation Method (Romberg Integration)</li>
</ul>
	<li>4.2 Optimized Sample Points</li>
<ul>	<li>4.2.1 Clenshaw-Curtis Expressions</li>
	<li>4.2.2 Gaussian Integration</li>
<ul>	<li>4.2.2.1 Gauss-Legendre Integration</li>
	<li>4.2.2.2 Other Types of Gaussian Integration</li>
	<li>4.2.2.3 Connection with an Eigenvalue Problem</li>
</ul>
</ul>
	<li>4.3 Problems</li>
</ul>
	<li>Chapter 5: Systems of Inhomogeneous Linear Equations</li>
<ul>	<li>5.1 Gaussian Elimination Method</li>
<ul>	<li>5.1.1 Pivoting</li>
	<li>5.1.2 Direct LU Decomposition</li>
</ul>
	<li>5.2 QR Decomposition</li>
<ul>	<li>5.2.1 QR Decomposition by Orthogonalization</li>
	<li>5.2.2 QR Decomposition by Householder Reﬂections</li>
</ul>
	<li>5.3 Linear Equations with Tridiagonal Matrix</li>
	<li>5.4 Cyclic Tridiagonal Systems</li>
	<li>5.5 Iterative Solution of Inhomogeneous Linear Equations</li>
<ul>	<li>5.5.1 General Relaxation Method</li>
	<li>5.5.2 Jacobi Method</li>
	<li>5.5.3 Gauss-Seidel Method</li>
	<li>5.5.4 Damping and Successive Over-Relaxation</li>
</ul>
	<li>5.6 Conjugate Gradients</li>
	<li>5.7 Matrix Inversion</li>
	<li>5.8 Problems</li>
</ul>
	<li>Chapter 6: Roots and Extremal Points</li>
<ul>	<li>6.1 Root Finding</li>
<ul>	<li>6.1.1 Bisection</li>
	<li>6.1.2 Regula Falsi (False Position) Method</li>
	<li>6.1.3 Newton-Raphson Method</li>
	<li>6.1.4 Secant Method</li>
	<li>6.1.5 Interpolation</li>
	<li>6.1.6 Inverse Interpolation</li>
	<li>6.1.7 Combined Methods</li>
<ul>	<li>6.1.7.1 Dekker's Method</li>
	<li>6.1.7.2 Brent's Method</li>
	<li>6.1.7.3 Chandrupatla's method</li>
</ul>
	<li>6.1.8 Multidimensional Root Finding</li>
	<li>6.1.9 Quasi-Newton Methods</li>
</ul>
	<li>6.2 Function Minimization</li>
<ul>	<li>6.2.1 The Ternary Search Method</li>
	<li>6.2.2 The Golden Section Search Method (Brent's Method)</li>
	<li>6.2.3 Minimization in Multidimensions</li>
	<li>6.2.4 Steepest Descent Method</li>
	<li>6.2.5 Conjugate Gradient Method</li>
	<li>6.2.6 Newton-Raphson Method</li>
	<li>6.2.7 Quasi-Newton Methods</li>
</ul>
	<li>6.3 Problems</li>
</ul>
	<li>Chapter 7: Fourier Transformation</li>
<ul>	<li>7.1 Fourier Integral and Fourier Series</li>
	<li>7.2 Discrete Fourier Transformation</li>
<ul>	<li>7.2.1 Trigonometric Interpolation</li>
	<li>7.2.2 Real Valued Functions</li>
	<li>7.2.3 Approximate Continuous Fourier Transformation</li>
</ul>
	<li>7.3 Fourier Transform Algorithms</li>
<ul>	<li>7.3.1 Goertzel's Algorithm</li>
	<li>7.3.2 Fast Fourier Transformation</li>
</ul>
	<li>7.4 Problems</li>
</ul>
	<li>Chapter 8: Random Numbers and Monte Carlo Methods</li>
<ul>	<li>8.1 Some Basic Statistics</li>
<ul>	<li>8.1.1 Probability Density and Cumulative Probability Distribution</li>
	<li>8.1.2 Histogram</li>
	<li>8.1.3 Expectation Values and Moments</li>
	<li>8.1.4 Example: Fair Die</li>
	<li>8.1.5 Normal Distribution</li>
	<li>8.1.6 Multivariate Distributions</li>
	<li>8.1.7 Central Limit Theorem</li>
	<li>8.1.8 Example: Binomial Distribution</li>
	<li>8.1.9 Average of Repeated Measurements</li>
</ul>
	<li>8.2 Random Numbers</li>
<ul>	<li>8.2.1 Linear Congruent Mapping</li>
	<li>8.2.2 Marsaglia-Zamann Method</li>
	<li>8.2.3 Random Numbers with Given Distribution</li>
	<li>8.2.4 Examples</li>
<ul>	<li>8.2.4.1 Fair Die</li>
	<li>8.2.4.2 Exponential Distribution</li>
	<li>8.2.4.3 Random Points on the Unit Sphere</li>
	<li>8.2.4.4 Gaussian Distribution (Box Muller)</li>
</ul>
</ul>
	<li>8.3 Monte Carlo Integration</li>
<ul>	<li>8.3.1 Numerical Calculation of pi</li>
	<li>8.3.2 Calculation of an Integral</li>
	<li>8.3.3 More General Random Numbers</li>
</ul>
	<li>8.4 Monte Carlo Method for Thermodynamic Averages</li>
<ul>	<li>8.4.1 Simple Sampling</li>
	<li>8.4.2 Importance Sampling</li>
	<li>8.4.3 Metropolis Algorithm</li>
</ul>
	<li>8.5 Problems</li>
</ul>
	<li>Chapter 9: Eigenvalue Problems</li>
<ul>	<li>9.1 Direct Solution</li>
	<li>9.2 Jacobi Method</li>
	<li>9.3 Tridiagonal Matrices</li>
<ul>	<li>9.3.1 Characteristic Polynomial of a Tridiagonal Matrix</li>
	<li>9.3.2 Special Tridiagonal Matrices</li>
<ul>	<li>9.3.2.1 Discretized Second Derivatives</li>
	<li>9.3.2.2 Discretized First Derivatives</li>
</ul>
	<li>9.3.3 The QL Algorithm</li>
</ul>
	<li>9.4 Reduction to a Tridiagonal Matrix</li>
	<li>9.5 Large Matrices</li>
	<li>9.6 Problems</li>
</ul>
	<li>Chapter 10: Data Fitting</li>
<ul>	<li>10.1 Least Square Fit</li>
<ul>	<li>10.1.1 Linear Least Square Fit</li>
	<li>10.1.2 Linear Least Square Fit with Orthogonalization</li>
</ul>
	<li>10.2 Singular Value Decomposition</li>
<ul>	<li>10.2.1 Full Singular Value Decomposition</li>
	<li>10.2.2 Reduced Singular Value Decomposition</li>
	<li>10.2.3 Low Rank Matrix Approximation</li>
	<li>10.2.4 Linear Least Square Fit with Singular Value Decomposition</li>
</ul>
	<li>10.3 Problems</li>
</ul>
	<li>Chapter 11: Discretization of Differential Equations</li>
<ul>	<li>11.1 Classiﬁcation of Differential Equations</li>
<ul>	<li>11.1.1 Linear Second Order PDE</li>
	<li>11.1.2 Conservation Laws</li>
</ul>
	<li>11.2 Finite Differences</li>
<ul>	<li>11.2.1 Finite Differences in Time</li>
	<li>11.2.2 Stability Analysis</li>
	<li>11.2.3 Method of Lines</li>
	<li>11.2.4 Eigenvector Expansion</li>
</ul>
	<li>11.3 Finite Volumes</li>
<ul>	<li>11.3.1 Discretization of ﬂuxes</li>
</ul>
	<li>11.4 Weighted Residual Based Methods</li>
<ul>	<li>11.4.1 Point Collocation Method</li>
	<li>11.4.2 Sub-domain Method</li>
	<li>11.4.3 Least Squares Method</li>
	<li>11.4.4 Galerkin Method</li>
</ul>
	<li>11.5 Spectral and Pseudo-spectral Methods</li>
<ul>	<li>11.5.1 Fourier Pseudo-spectral Methods</li>
	<li>11.5.2 Example: Polynomial Approximation</li>
<ul>	<li>11.5.2.1 Point Collocation Method</li>
	<li>11.5.2.2 Sub-domain Method</li>
	<li>11.5.2.3 Galerkin Method</li>
	<li>11.5.2.4 Least Squares Method</li>
</ul>
</ul>
	<li>11.6 Finite Elements</li>
<ul>	<li>11.6.1 One-Dimensional Elements</li>
	<li>11.6.2 Two- and Three-Dimensional Elements</li>
<ul>	<li>11.6.2.1 Triangulation</li>
	<li>11.6.2.2 Rectangular Elements</li>
</ul>
	<li>11.6.3 One-Dimensional Galerkin FEM</li>
</ul>
	<li>11.7 Boundary Element Method</li>
</ul>
	<li>Chapter 12: Equations of Motion</li>
<ul>	<li>12.1 The State Vector</li>
	<li>12.2 Time Evolution of the State Vector</li>
	<li>12.3 Explicit Forward Euler Method</li>
	<li>12.4 Implicit Backward Euler Method</li>
	<li>12.5 Improved Euler Methods</li>
	<li>12.6 Taylor Series Methods</li>
<ul>	<li>12.6.1 Nordsieck Predictor-Corrector Method</li>
	<li>12.6.2 Gear Predictor-Corrector Methods</li>
</ul>
	<li>12.7 Runge-Kutta Methods</li>
<ul>	<li>12.7.1 Second Order Runge-Kutta Method</li>
	<li>12.7.2 Third Order Runge-Kutta Method</li>
	<li>12.7.3 Fourth Order Runge-Kutta Method</li>
</ul>
	<li>12.8 Quality Control and Adaptive Step Size Control</li>
	<li>12.9 Extrapolation Methods</li>
	<li>12.10 Linear Multistep Methods</li>
<ul>	<li>12.10.1 Adams-Bashforth Methods</li>
	<li>12.10.2 Adams-Moulton Methods</li>
	<li>12.10.3 Backward Differentiation (Gear) Methods</li>
	<li>12.10.4 Predictor-Corrector Methods</li>
</ul>
	<li>12.11 Verlet Methods</li>
<ul>	<li>12.11.1 Liouville Equation</li>
	<li>12.11.2 Split-Operator Approximation</li>
	<li>12.11.3 Position Verlet Method</li>
	<li>12.11.4 Velocity Verlet Method</li>
	<li>12.11.5 St&ouml;rmer-Verlet Method</li>
	<li>12.11.6 Error Accumulation for the St&ouml;rmer-Verlet Method</li>
	<li>12.11.7 Beeman's Method</li>
	<li>12.11.8 The Leapfrog Method</li>
</ul>
	<li>12.12 Problems</li>
</ul>
</ul>
	<li>Part II: Simulation of Classical and Quantum Systems</li>
<ul>	<li>Chapter 13: Rotational Motion</li>
<ul>	<li>13.1 Transformation to a Body Fixed Coordinate System</li>
	<li>13.2 Properties of the Rotation Matrix</li>
	<li>13.3 Properties of W, Connection with the Vector of Angular Velocity</li>
	<li>13.4 Transformation Properties of the Angular Velocity</li>
	<li>13.5 Momentum and Angular Momentum</li>
	<li>13.6 Equations of Motion of a Rigid Body</li>
	<li>13.7 Moments of Inertia</li>
	<li>13.8 Equations of Motion for a Rotor</li>
	<li>13.9 Explicit Methods</li>
	<li>13.10 Loss of Orthogonality</li>
	<li>13.11 Implicit Method</li>
	<li>13.12 Kinetic Energy of a Rotor</li>
	<li>13.13 Parametrization by Euler Angles</li>
	<li>13.14 Cayley-Klein Parameters, Quaternions, Euler Parameters</li>
	<li>13.15 Solving the Equations of Motion with Quaternions</li>
	<li>13.16 Problems</li>
</ul>
	<li>Chapter 14: Molecular Mechanics</li>
<ul>	<li>14.1 Atomic Coordinates</li>
	<li>14.2 Force Fields</li>
<ul>	<li>14.2.1 Intramolecular Forces</li>
	<li>14.2.2 Intermolecular Interactions</li>
</ul>
	<li>14.3 Gradients</li>
	<li>14.4 Normal Mode Analysis</li>
<ul>	<li>14.4.1 Harmonic Approximation</li>
</ul>
	<li>14.5 Problems</li>
</ul>
	<li>Chapter 15: Thermodynamic Systems</li>
<ul>	<li>15.1 Simulation of a Lennard-Jones Fluid</li>
<ul>	<li>15.1.1 Integration of the Equations of Motion</li>
	<li>15.1.2 Boundary Conditions and Average Pressure</li>
	<li>15.1.3 Initial Conditions and Average Temperature</li>
	<li>15.1.4 Analysis of the Results</li>
<ul>	<li>15.1.4.1 Deviation from the Ideal Gas Behavior</li>
	<li>15.1.4.2 Structural Order</li>
	<li>15.1.4.3 Ballistic and Diffusive Motion</li>
</ul>
</ul>
	<li>15.2 Monte Carlo Simulation</li>
<ul>	<li>15.2.1 One-Dimensional Ising Model</li>
	<li>15.2.2 Two-Dimensional Ising Model</li>
</ul>
	<li>15.3 Problems</li>
</ul>
	<li>Chapter 16: Random Walk and Brownian Motion</li>
<ul>	<li>16.1 Markovian Discrete Time Models</li>
	<li>16.2 Random Walk in One Dimension</li>
<ul>	<li>16.2.1 Random Walk with Constant Step Size</li>
</ul>
	<li>16.3 The Freely Jointed Chain</li>
<ul>	<li>16.3.1 Basic Statistic Properties</li>
	<li>16.3.2 Gyration Tensor</li>
	<li>16.3.3 Hookean Spring Model</li>
</ul>
	<li>16.4 Langevin Dynamics</li>
	<li>16.5 Problems</li>
</ul>
	<li>Chapter 17: Electrostatics</li>
<ul>	<li>17.1 Poisson Equation</li>
<ul>	<li>17.1.1 Homogeneous Dielectric Medium</li>
	<li>17.1.2 Numerical Methods for the Poisson Equation</li>
	<li>17.1.3 Charged Sphere</li>
	<li>17.1.4 Variable epsilon</li>
	<li>17.1.5 Discontinuous epsilon</li>
	<li>17.1.6 Solvation Energy of a Charged Sphere</li>
	<li>17.1.7 The Shifted Grid Method</li>
</ul>
	<li>17.2 Poisson-Boltzmann Equation</li>
<ul>	<li>17.2.1 Linearization of the Poisson-Boltzmann Equation</li>
	<li>17.2.2 Discretization of the Linearized Poisson-Boltzmann Equation</li>
</ul>
	<li>17.3 Boundary Element Method for the Poisson Equation</li>
<ul>	<li>17.3.1 Integral Equations for the Potential</li>
	<li>17.3.2 Calculation of the Boundary Potential</li>
</ul>
	<li>17.4 Boundary Element Method for the Linearized Poisson-Boltzmann Equation</li>
	<li>17.5 Electrostatic Interaction Energy (Onsager Model)</li>
<ul>	<li>17.5.1 Example: Point Charge in a Spherical Cavity</li>
</ul>
	<li>17.6 Problems</li>
</ul>
	<li>Chapter 18: Waves</li>
<ul>	<li>18.1 Classical Waves</li>
	<li>18.2 Spatial Discretization in One Dimension</li>
	<li>18.3 Solution by an Eigenvector Expansion</li>
	<li>18.4 Discretization of Space and Time</li>
	<li>18.5 Numerical Integration with a Two-Step Method</li>
	<li>18.6 Reduction to a First Order Differential Equation</li>
	<li>18.7 Two-Variable Method</li>
<ul>	<li>18.7.1 Leapfrog Scheme</li>
	<li>18.7.2 Lax-Wendroff Scheme</li>
	<li>18.7.3 Crank-Nicolson Scheme</li>
</ul>
	<li>18.8 Problems</li>
</ul>
	<li>Chapter 19: Diffusion</li>
<ul>	<li>19.1 Particle Flux and Concentration Changes</li>
	<li>19.2 Diffusion in One Dimension</li>
<ul>	<li>19.2.1 Explicit Euler (Forward Time Centered Space) Scheme</li>
	<li>19.2.2 Implicit Euler (Backward Time Centered Space) Scheme</li>
	<li>19.2.3 Crank-Nicolson Method</li>
	<li>19.2.4 Error Order Analysis</li>
	<li>19.2.5 Finite Element Discretization</li>
</ul>
	<li>19.3 Split-Operator Method for Multidimensions</li>
	<li>19.4 Problems</li>
</ul>
	<li>Chapter 20: Nonlinear Systems</li>
<ul>	<li>20.1 Iterated Functions</li>
<ul>	<li>20.1.1 Fixed Points and Stability</li>
	<li>20.1.2 The Lyapunov Exponent</li>
	<li>20.1.3 The Logistic Map</li>
	<li>20.1.4 Fixed Points of the Logistic Map</li>
	<li>20.1.5 Bifurcation Diagram</li>
</ul>
	<li>20.2 Population Dynamics</li>
<ul>	<li>20.2.1 Equilibria and Stability</li>
	<li>20.2.2 The Continuous Logistic Model</li>
</ul>
	<li>20.3 Lotka-Volterra Model</li>
<ul>	<li>20.3.1 Stability Analysis</li>
</ul>
	<li>20.4 Functional Response</li>
<ul>	<li>20.4.1 Holling-Tanner Model</li>
</ul>
	<li>20.5 Reaction-Diffusion Systems</li>
<ul>	<li>20.5.1 General Properties of Reaction-Diffusion Systems</li>
	<li>20.5.2 Chemical Reactions</li>
	<li>20.5.3 Diffusive Population Dynamics</li>
	<li>20.5.4 Stability Analysis</li>
	<li>20.5.5 Lotka-Volterra Model with Diffusion</li>
</ul>
	<li>20.6 Problems</li>
</ul>
	<li>Chapter 21: Simple Quantum Systems</li>
<ul>	<li>21.1 Pure and Mixed Quantum States</li>
<ul>	<li>21.1.1 Wavefunctions</li>
	<li>21.1.2 Density Matrix for an Ensemble of Systems</li>
	<li>21.1.3 Time Evolution of the Density Matrix</li>
</ul>
	<li>21.2 Wave Packet Motion in One Dimension</li>
<ul>	<li>21.2.1 Discretization of the Kinetic Energy</li>
<ul>	<li>21.2.1.1 Pseudo-spectral Methods</li>
	<li>21.2.1.2 Finite Difference Methods</li>
</ul>
	<li>21.2.2 Time Evolution</li>
<ul>	<li>21.2.2.1 Rational Approximation</li>
	<li>21.2.2.2 Second Order Differencing</li>
	<li>21.2.2.3 Split-Operator Methods</li>
<ul>	<li>Split-Operator-Fourier Method</li>
	<li>Real-Space Product Formulae</li>
</ul>
</ul>
	<li>21.2.3 Example: Free Wave Packet Motion</li>
</ul>
	<li>21.3 Few-State Systems</li>
<ul>	<li>21.3.1 Two-State System</li>
	<li>21.3.2 Two-State System with Time Dependent Perturbation</li>
	<li>21.3.3 Superexchange Model</li>
	<li>21.3.4 Ladder Model for Exponential Decay</li>
	<li>21.3.5 Landau-Zener Model</li>
</ul>
	<li>21.4 The Dissipative Two-State System</li>
<ul>	<li>21.4.1 Equations of Motion for a Two-State System</li>
	<li>21.4.2 The Vector Model</li>
	<li>21.4.3 The Spin-1/2 System</li>
	<li>21.4.4 Relaxation Processes-The Bloch Equations</li>
<ul>	<li>21.4.4.1 Phenomenological Description</li>
</ul>
	<li>21.4.5 The Driven Two-State System</li>
<ul>	<li>21.4.5.1 Free Precession</li>
	<li>21.4.5.2 Stationary Solution for Monochromatic Excitation</li>
	<li>21.4.5.3 Excitation by a Resonant Pulse</li>
</ul>
	<li>21.4.6 Elementary Qubit Manipulation</li>
<ul>	<li>21.4.6.1 Pauli-Gates</li>
	<li>21.4.6.2 Hadamard Gate</li>
</ul>
</ul>
	<li>21.5 Problems</li>
</ul>
</ul>
	<li>Appendix I: Performing the Computer Experiments</li>
	<li>Appendix II: Methods and Algorithms</li>
	<li>References</li>
	<li>Index</li>
</ul>
</body></html>